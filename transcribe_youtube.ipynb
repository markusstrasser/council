{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.3\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install -r requirements.txt\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VMj-3S1tku0', 'PaCmpygFfXo', 'TCH_1BHY58I', 'P6sfmUTpUmc', 'q8SA3rM6ckI']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "karpathy_playlist = \"https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ\"\n",
    "\n",
    "karpathy_urls = get_urls_from_youtube_playlist(karpathy_playlist)\n",
    "karpathy_playlist_id = extract.playlist_id(karpathy_playlist)\n",
    "\n",
    "karpathy_playlist_vid_ids = [extract.video_id(url) for url in karpathy_urls]\n",
    "\n",
    "karpathy_playlist_vid_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git (from -r requirements.txt (line 1))\n",
      "  Cloning https://github.com/openai/whisper.git to /private/var/folders/tc/q_js7srx33sc183v2vl5lfr00000gn/T/pip-req-build-vzt8jss8\n",
      "  Running command git clone -q https://github.com/openai/whisper.git /private/var/folders/tc/q_js7srx33sc183v2vl5lfr00000gn/T/pip-req-build-vzt8jss8\n",
      "  Resolved https://github.com/openai/whisper.git to commit 9f70a352f9f8630ab3aa0d06af5cb9532bd8c21d\n",
      "Collecting torch\n",
      "  Downloading torch-1.13.0-cp37-none-macosx_10_9_x86_64.whl (137.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 137.5 MB 2.7 MB/s eta 0:00:011MB/s eta 0:00:0726.3 MB/s eta 0:00:04| 53.9 MB 26.3 MB/s eta 0:00:042 MB 26.3 MB/s eta 0:00:03��████▊             | 80.7 MB 26.3 MB/s eta 0:00:03eta 0:00:02███████████████████████▏      | 108.1 MB 44.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.14.0-cp37-cp37m-macosx_10_9_x86_64.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 12.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading torchaudio-0.13.0-cp37-cp37m-macosx_10_9_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 112.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting farm-haystack[all]\n",
      "  Using cached farm_haystack-1.10.0-py3-none-any.whl (736 kB)\n",
      "Requirement already satisfied: pandas in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (1.3.4)\n",
      "Requirement already satisfied: numpy in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (1.20.3)\n",
      "Requirement already satisfied: jupyter in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: tqdm in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from whisper==1.0->-r requirements.txt (line 1)) (4.62.3)\n",
      "Requirement already satisfied: more-itertools in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from whisper==1.0->-r requirements.txt (line 1)) (8.10.0)\n",
      "Collecting transformers>=4.19.0\n",
      "  Using cached transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
      "Collecting ffmpeg-python==0.2.0\n",
      "  Using cached ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Collecting future\n",
      "  Using cached future-0.18.2.tar.gz (829 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from torch->-r requirements.txt (line 2)) (3.10.0.2)\n",
      "Requirement already satisfied: requests in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from torchvision->-r requirements.txt (line 3)) (2.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from torchvision->-r requirements.txt (line 3)) (8.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (4.8.1)\n",
      "Collecting mlflow\n",
      "  Using cached mlflow-1.30.0-py3-none-any.whl (17.0 MB)\n",
      "Collecting quantulum3\n",
      "  Using cached quantulum3-0.7.11-py3-none-any.whl (10.7 MB)\n",
      "Requirement already satisfied: networkx in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (2.6.3)\n",
      "Collecting tika\n",
      "  Using cached tika-1.24.tar.gz (28 kB)\n",
      "Collecting azure-ai-formrecognizer>=3.2.0b2\n",
      "  Using cached azure_ai_formrecognizer-3.2.0-py3-none-any.whl (228 kB)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (1.7.1)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting rapidfuzz<2.8.0,>=2.0.15\n",
      "  Downloading rapidfuzz-2.7.0-cp37-cp37m-macosx_10_9_x86_64.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 101.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jsonschema in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (3.2.0)\n",
      "Collecting elasticsearch<8,>=7.7\n",
      "  Using cached elasticsearch-7.17.7-py2.py3-none-any.whl (385 kB)\n",
      "Collecting mmh3\n",
      "  Downloading mmh3-3.0.0-cp37-cp37m-macosx_10_9_x86_64.whl (12 kB)\n",
      "Requirement already satisfied: nltk in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (3.6.5)\n",
      "Collecting transformers>=4.19.0\n",
      "  Using cached transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
      "Collecting sentence-transformers>=2.2.0\n",
      "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "Collecting python-docx\n",
      "  Using cached python-docx-0.8.11.tar.gz (5.6 MB)\n",
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9.tar.gz (981 kB)\n",
      "Collecting farm-haystack[all]\n",
      "  Using cached farm_haystack-1.9.1-py3-none-any.whl (733 kB)\n",
      "Collecting elasticsearch<7.11,>=7.7\n",
      "  Using cached elasticsearch-7.10.1-py2.py3-none-any.whl (322 kB)\n",
      "Collecting elastic-apm\n",
      "  Using cached elastic-apm-6.13.0.tar.gz (153 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting farm-haystack[all]\n",
      "  Using cached farm_haystack-1.9.0-py3-none-any.whl (712 kB)\n",
      "  Using cached farm_haystack-1.8.0-py3-none-any.whl (666 kB)\n",
      "Collecting azure-ai-formrecognizer==3.2.0b2\n",
      "  Using cached azure_ai_formrecognizer-3.2.0b2-py2.py3-none-any.whl (219 kB)\n",
      "Collecting farm-haystack[all]\n",
      "  Using cached farm_haystack-1.7.1-py3-none-any.whl (629 kB)\n",
      "  Using cached farm_haystack-1.7.0-py3-none-any.whl (641 kB)\n",
      "  Using cached farm_haystack-1.6.0-py3-none-any.whl (596 kB)\n",
      "  Using cached farm_haystack-1.5.0-py3-none-any.whl (570 kB)\n",
      "Collecting rapidfuzz\n",
      "  Downloading rapidfuzz-2.13.0-cp37-cp37m-macosx_10_9_x86_64.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 33.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn>=1.0.0\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-macosx_10_13_x86_64.whl (7.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.8 MB 19.1 MB/s eta 0:00:01�███████████▋       | 6.0 MB 19.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting seqeval\n",
      "  Using cached seqeval-1.2.2.tar.gz (43 kB)\n",
      "Collecting azure-core<1.23\n",
      "  Using cached azure_core-1.22.1-py3-none-any.whl (178 kB)\n",
      "Collecting farm-haystack[all]\n",
      "  Using cached farm_haystack-1.4.0-py3-none-any.whl (524 kB)\n",
      "  Using cached farm_haystack-1.3.0-py3-none-any.whl (475 kB)\n",
      "Collecting mlflow<=1.13.1\n",
      "  Using cached mlflow-1.13.1-py3-none-any.whl (14.1 MB)\n",
      "Collecting farm-haystack[all]\n",
      "  Using cached farm_haystack-1.2.0-py3-none-any.whl (430 kB)\n",
      "  Using cached farm_haystack-1.1.0-py3-none-any.whl (392 kB)\n",
      "\u001b[33mWARNING: farm-haystack 1.1.0 does not provide the extra 'all'\u001b[0m\n",
      "Collecting weaviate-client==2.5.0\n",
      "  Using cached weaviate_client-2.5.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: psutil in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (5.8.0)\n",
      "Collecting dataclasses-json\n",
      "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: wheel in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (0.37.0)\n",
      "Requirement already satisfied: httptools in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (58.0.4)\n",
      "Collecting pdf2image==1.14.0\n",
      "  Using cached pdf2image-1.14.0-py3-none-any.whl (10 kB)\n",
      "Collecting SPARQLWrapper\n",
      "  Using cached SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: uvicorn in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (0.13.4)\n",
      "Collecting uvloop==0.14\n",
      "  Downloading uvloop-0.14.0-cp37-cp37m-macosx_10_11_x86_64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 98.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting coverage\n",
      "  Downloading coverage-6.5.0-cp37-cp37m-macosx_10_9_x86_64.whl (185 kB)\n",
      "\u001b[K     |████████████████████████████████| 185 kB 107.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fastapi in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (0.63.0)\n",
      "Collecting faiss-cpu>=1.6.3\n",
      "  Downloading faiss_cpu-1.7.2-cp37-cp37m-macosx_10_9_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 114.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tox\n",
      "  Using cached tox-3.27.0-py2.py3-none-any.whl (86 kB)\n",
      "Collecting gunicorn\n",
      "  Using cached gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "Collecting farm-haystack[all]\n",
      "  Using cached farm_haystack-1.0.0-py3-none-any.whl (375 kB)\n",
      "\u001b[33mWARNING: farm-haystack 1.0.0 does not provide the extra 'all'\u001b[0m\n",
      "Collecting ray==1.5.0\n",
      "  Downloading ray-1.5.0-cp37-cp37m-macosx_10_13_intel.whl (53.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 53.5 MB 6.5 MB/s eta 0:00:011█████████████▏             | 30.5 MB 49.3 MB/s eta 0:00:01██████████████▊       | 41.4 MB 49.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting farm-haystack[all]\n",
      "  Using cached farm_haystack-0.10.0-py3-none-any.whl (200 kB)\n",
      "\u001b[33mWARNING: farm-haystack 0.10.0 does not provide the extra 'all'\u001b[0m\n",
      "  Using cached farm_haystack-0.9.0-py3-none-any.whl (180 kB)\n",
      "\u001b[33mWARNING: farm-haystack 0.9.0 does not provide the extra 'all'\u001b[0m\n",
      "Collecting weaviate-client\n",
      "  Using cached weaviate_client-3.8.0-py3-none-any.whl (73 kB)\n",
      "Collecting farm==0.8.0\n",
      "  Using cached farm-0.8.0-py3-none-any.whl (204 kB)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (1.4.22)\n",
      "Collecting pymilvus\n",
      "  Using cached pymilvus-2.1.3-py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: python-multipart in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (0.0.5)\n",
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.5-cp37-cp37m-macosx_10_15_x86_64.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 101.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sqlalchemy-utils\n",
      "  Using cached SQLAlchemy_Utils-0.38.3-py3-none-any.whl (100 kB)\n",
      "Collecting elasticsearch<=7.10,>=7.7\n",
      "  Using cached elasticsearch-7.10.0-py2.py3-none-any.whl (321 kB)\n",
      "INFO: pip is looking at multiple versions of farm-haystack[all] to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[33mWARNING: farm-haystack 1.1.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 1.0.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 0.10.0 does not provide the extra 'all'\u001b[0m\n",
      "Collecting farm-haystack[all]\n",
      "  Using cached farm-haystack-0.8.0.tar.gz (152 kB)\n",
      "\u001b[33mWARNING: farm-haystack 0.8.0 does not provide the extra 'all'\u001b[0m\n",
      "Collecting farm==0.7.1\n",
      "  Using cached farm-0.7.1-py3-none-any.whl (203 kB)\n",
      "Collecting dotmap\n",
      "  Downloading dotmap-1.3.30-py3-none-any.whl (11 kB)\n",
      "\u001b[33mWARNING: farm-haystack 1.1.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 1.0.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 0.10.0 does not provide the extra 'all'\u001b[0m\n",
      "Collecting farm-haystack[all]\n",
      "  Using cached farm_haystack-0.7.0-py3-none-any.whl (114 kB)\n",
      "\u001b[33mWARNING: farm-haystack 0.7.0 does not provide the extra 'all'\u001b[0m\n",
      "Collecting farm==0.6.2\n",
      "  Using cached farm-0.6.2-py3-none-any.whl (207 kB)\n",
      "Requirement already satisfied: uvloop in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (0.16.0)\n",
      "Collecting faiss-cpu==1.6.3\n",
      "  Downloading faiss_cpu-1.6.3-cp37-cp37m-macosx_10_9_x86_64.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 103.5 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of faiss-cpu to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[33mWARNING: farm-haystack 1.1.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 1.0.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 0.10.0 does not provide the extra 'all'\u001b[0m\n",
      "Collecting farm-haystack[all]\n",
      "  Using cached farm_haystack-0.6.0-py3-none-any.whl (104 kB)\n",
      "\u001b[33mWARNING: farm-haystack 0.6.0 does not provide the extra 'all'\u001b[0m\n",
      "Collecting farm==0.5.0\n",
      "  Using cached farm-0.5.0-py3-none-any.whl (207 kB)\n",
      "Collecting Werkzeug==0.16.1\n",
      "  Using cached Werkzeug-0.16.1-py2.py3-none-any.whl (327 kB)\n",
      "Collecting dotmap==1.3.0\n",
      "  Downloading dotmap-1.3.0-py3-none-any.whl (8.9 kB)\n",
      "Collecting mlflow==1.0.0\n",
      "  Using cached mlflow-1.0.0-py3-none-any.whl (47.7 MB)\n",
      "\u001b[33mWARNING: farm-haystack 1.1.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 1.0.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 0.10.0 does not provide the extra 'all'\u001b[0m\n",
      "Collecting farm-haystack[all]\n",
      "  Using cached farm_haystack-0.5.0-py3-none-any.whl (94 kB)\n",
      "\u001b[33mWARNING: farm-haystack 0.5.0 does not provide the extra 'all'\u001b[0m\n",
      "Collecting elasticsearch\n",
      "  Using cached elasticsearch-8.4.3-py3-none-any.whl (384 kB)\n",
      "\u001b[33mWARNING: farm-haystack 1.1.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 1.0.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 0.10.0 does not provide the extra 'all'\u001b[0m\n",
      "Collecting farm-haystack[all]\n",
      "  Using cached farm_haystack-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[33mWARNING: farm-haystack 0.4.0 does not provide the extra 'all'\u001b[0m\n",
      "Collecting farm==0.4.9\n",
      "  Using cached farm-0.4.9-py3-none-any.whl (190 kB)\n",
      "Collecting wget\n",
      "  Using cached wget-3.2.zip (10 kB)\n",
      "\u001b[33mWARNING: farm-haystack 1.1.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 1.0.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 0.10.0 does not provide the extra 'all'\u001b[0m\n",
      "Collecting farm-haystack[all]\n",
      "  Using cached farm_haystack-0.3.0-py3-none-any.whl (82 kB)\n",
      "\u001b[33mWARNING: farm-haystack 0.3.0 does not provide the extra 'all'\u001b[0m\n",
      "Collecting farm==0.4.6\n",
      "  Using cached farm-0.4.6-py3-none-any.whl (184 kB)\n",
      "\u001b[33mWARNING: farm-haystack 1.1.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 1.0.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 0.10.0 does not provide the extra 'all'\u001b[0m\n",
      "Collecting farm-haystack[all]\n",
      "  Using cached farm-haystack-0.2.1.tar.gz (30 kB)\n",
      "\u001b[33mWARNING: farm-haystack 0.2.1 does not provide the extra 'all'\u001b[0m\n",
      "Collecting farm==0.4.3\n",
      "  Using cached farm-0.4.3.tar.gz (153 kB)\n",
      "INFO: pip is looking at multiple versions of farm-haystack[all] to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[33mWARNING: farm-haystack 1.1.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 1.0.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 0.10.0 does not provide the extra 'all'\u001b[0m\n",
      "Collecting farm-haystack[all]\n",
      "  Using cached farm-haystack-0.2.0.post1.tar.gz (30 kB)\n",
      "\u001b[33mWARNING: farm-haystack 0.2.0.post1 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 1.1.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 1.0.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 0.10.0 does not provide the extra 'all'\u001b[0m\n",
      "  Using cached farm-haystack-0.1.0.post2.tar.gz (14 kB)\n",
      "\u001b[33mWARNING: farm-haystack 0.1.0.post2 does not provide the extra 'all'\u001b[0m\n",
      "Collecting farm==0.3.2\n",
      "  Using cached farm-0.3.2.tar.gz (94 kB)\n",
      "Requirement already satisfied: flask in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (1.1.2)\n",
      "Collecting flask_cors\n",
      "  Using cached Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
      "Collecting flask_restplus\n",
      "  Using cached flask_restplus-0.13.0-py2.py3-none-any.whl (2.5 MB)\n",
      "Collecting flask_sqlalchemy\n",
      "  Using cached Flask_SQLAlchemy-3.0.2-py3-none-any.whl (24 kB)\n",
      "Collecting boto3\n",
      "  Using cached boto3-1.25.5-py3-none-any.whl (132 kB)\n",
      "\u001b[33mWARNING: farm-haystack 1.1.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 1.0.0 does not provide the extra 'all'\u001b[0m\n",
      "\u001b[33mWARNING: farm-haystack 0.10.0 does not provide the extra 'all'\u001b[0m\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-0.12.1-cp37-cp37m-macosx_10_9_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 109.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading torchaudio-0.12.0-cp37-cp37m-macosx_10_15_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 50.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading torchaudio-0.11.0-cp37-cp37m-macosx_10_15_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 13.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading torchaudio-0.10.2-cp37-cp37m-macosx_10_9_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 114.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading torchaudio-0.10.1-cp37-cp37m-macosx_10_9_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 109.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading torchaudio-0.10.0-cp37-cp37m-macosx_10_9_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 563 kB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading torchaudio-0.9.1-cp37-cp37m-macosx_10_9_x86_64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 53.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading torchaudio-0.9.0-cp37-cp37m-macosx_10_9_x86_64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 27.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading torchaudio-0.8.1-cp37-cp37m-macosx_10_9_x86_64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 12.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading torchaudio-0.8.0-cp37-cp37m-macosx_10_9_x86_64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 112.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading torchaudio-0.7.2-cp37-cp37m-macosx_10_9_x86_64.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 117.4 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading torchaudio-0.7.0-cp37-cp37m-macosx_10_9_x86_64.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 20.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading torchaudio-0.6.0-cp37-cp37m-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 11.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading torchaudio-0.5.1-cp37-cp37m-macosx_10_9_x86_64.whl (765 kB)\n",
      "\u001b[K     |████████████████████████████████| 765 kB 118.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading torchaudio-0.5.0-cp37-cp37m-macosx_10_9_x86_64.whl (765 kB)\n",
      "\u001b[K     |████████████████████████████████| 765 kB 105.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading torchaudio-0.4.0-cp37-cp37m-macosx_10_9_x86_64.whl (759 kB)\n",
      "\u001b[K     |████████████████████████████████| 759 kB 114.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading torchaudio-0.3.2-cp37-cp37m-macosx_10_7_x86_64.whl (750 kB)\n",
      "\u001b[K     |████████████████████████████████| 750 kB 108.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading torchaudio-0.3.1-cp37-cp37m-macosx_10_9_x86_64.whl (750 kB)\n",
      "\u001b[K     |████████████████████████████████| 750 kB 118.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading torchaudio-0.3.0-cp37-cp37m-macosx_10_7_x86_64.whl (743 kB)\n",
      "\u001b[K     |████████████████████████████████| 743 kB 119.4 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.13.1-cp37-cp37m-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 117.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading torch-1.12.1-cp37-none-macosx_10_9_x86_64.whl (137.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 137.7 MB 466 kB/s eta 0:00:011          | 17.6 MB 103.3 MB/s eta 0:00:02                        | 32.7 MB 103.3 MB/s eta 0:00:02B 4.4 MB/s eta 0:00:23�████████▊                  | 59.2 MB 4.4 MB/s eta 0:00:18██████████▍             | 79.1 MB 723 kB/s eta 0:01:22███████████             | 82.2 MB 723 kB/s eta 0:01:17     |████████████████████            | 86.4 MB 723 kB/s eta 0:01:11        | 96.4 MB 723 kB/s eta 0:00:58��██████████████▎  | 126.2 MB 466 kB/s eta 0:00:25�█▊  | 127.9 MB 466 kB/s eta 0:00:22��████████████▌| 135.6 MB 466 kB/s eta 0:00:05\n",
      "\u001b[?25hCollecting huggingface-hub>=0.5.0\n",
      "  Using cached huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "Requirement already satisfied: pydantic in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (1.9.0)\n",
      "Collecting posthog\n",
      "  Using cached posthog-2.1.2-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from transformers>=4.19.0->whisper==1.0->-r requirements.txt (line 1)) (2021.8.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from transformers>=4.19.0->whisper==1.0->-r requirements.txt (line 1)) (21.0)\n",
      "Requirement already satisfied: filelock in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from transformers>=4.19.0->whisper==1.0->-r requirements.txt (line 1)) (3.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from transformers>=4.19.0->whisper==1.0->-r requirements.txt (line 1)) (5.4.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-macosx_10_11_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 106.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 6)) (2021.3)\n",
      "Requirement already satisfied: notebook in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from jupyter->-r requirements.txt (line 8)) (6.4.5)\n",
      "Requirement already satisfied: ipywidgets in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from jupyter->-r requirements.txt (line 8)) (7.6.5)\n",
      "Requirement already satisfied: ipykernel in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from jupyter->-r requirements.txt (line 8)) (6.4.1)\n",
      "Requirement already satisfied: nbconvert in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from jupyter->-r requirements.txt (line 8)) (6.1.0)\n",
      "Requirement already satisfied: qtconsole in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from jupyter->-r requirements.txt (line 8)) (5.1.1)\n",
      "Requirement already satisfied: jupyter-console in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from jupyter->-r requirements.txt (line 8)) (6.4.0)\n",
      "Collecting azure-core<2.0.0,>=1.23.0\n",
      "  Using cached azure_core-1.26.0-py3-none-any.whl (178 kB)\n",
      "Collecting azure-common~=1.1\n",
      "  Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Collecting msrest>=0.6.21\n",
      "  Using cached msrest-0.7.1-py3-none-any.whl (85 kB)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six>=1.11.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from azure-core<2.0.0,>=1.23.0->azure-ai-formrecognizer>=3.2.0b2->farm-haystack[all]->-r requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from elasticsearch<8,>=7.7->farm-haystack[all]->-r requirements.txt (line 5)) (1.26.7)\n",
      "Requirement already satisfied: certifi in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from elasticsearch<8,>=7.7->farm-haystack[all]->-r requirements.txt (line 5)) (2021.10.8)\n",
      "Collecting requests-oauthlib>=0.5.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting isodate>=0.6.0\n",
      "  Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from packaging>=20.0->transformers>=4.19.0->whisper==1.0->-r requirements.txt (line 1)) (3.0.4)\n",
      "Collecting jarowinkler<2.0.0,>=1.2.0\n",
      "  Downloading jarowinkler-1.2.3-cp37-cp37m-macosx_10_9_x86_64.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 2.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from requests->torchvision->-r requirements.txt (line 3)) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from requests->torchvision->-r requirements.txt (line 3)) (2.0.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from scikit-learn>=1.0.0->farm-haystack[all]->-r requirements.txt (line 5)) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from scikit-learn>=1.0.0->farm-haystack[all]->-r requirements.txt (line 5)) (1.1.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp37-cp37m-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 29.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting black[jupyter]==22.6.0\n",
      "  Downloading black-22.6.0-cp37-cp37m-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 118.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytest-custom-exit-code\n",
      "  Using cached pytest_custom_exit_code-0.3.0-py3-none-any.whl (4.1 kB)\n",
      "Collecting mkdocs\n",
      "  Using cached mkdocs-1.4.1-py3-none-any.whl (3.6 MB)\n",
      "Collecting mypy\n",
      "  Downloading mypy-0.982-cp37-cp37m-macosx_10_9_x86_64.whl (10.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.4 MB 8.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytest in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (6.2.4)\n",
      "Requirement already satisfied: pylint in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (2.9.6)\n",
      "Collecting pydoc-markdown\n",
      "  Using cached pydoc_markdown-4.6.3-py3-none-any.whl (64 kB)\n",
      "Collecting responses\n",
      "  Using cached responses-0.22.0-py3-none-any.whl (51 kB)\n",
      "Collecting jupytercontrib\n",
      "  Using cached jupytercontrib-0.0.7-py2.py3-none-any.whl (8.8 kB)\n",
      "Collecting requests-cache\n",
      "  Using cached requests_cache-0.9.7-py3-none-any.whl (48 kB)\n",
      "Collecting pre-commit\n",
      "  Using cached pre_commit-2.20.0-py2.py3-none-any.whl (199 kB)\n",
      "Requirement already satisfied: watchdog in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (2.1.3)\n",
      "Collecting beir\n",
      "  Using cached beir-1.0.1.tar.gz (50 kB)\n",
      "Collecting espnet\n",
      "  Using cached espnet-202209-py3-none-any.whl (1.2 MB)\n",
      "Collecting espnet-model-zoo\n",
      "  Using cached espnet_model_zoo-0.1.7-py3-none-any.whl (19 kB)\n",
      "Collecting pyworld<=0.2.12\n",
      "  Using cached pyworld-0.2.12.tar.gz (222 kB)\n",
      "Collecting pydub\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting python-magic\n",
      "  Using cached python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Collecting markdown\n",
      "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from farm-haystack[all]->-r requirements.txt (line 5)) (4.10.0)\n",
      "Collecting pdf2image>1.14\n",
      "  Using cached pdf2image-1.16.0-py3-none-any.whl (10 kB)\n",
      "Collecting pytesseract>0.3.7\n",
      "  Using cached pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
      "Collecting ray<2,>=1.9.1\n",
      "  Downloading ray-1.13.0-cp37-cp37m-macosx_10_15_intel.whl (57.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 57.1 MB 1.1 MB/s  eta 0:00:01  |███████████████████▉            | 35.3 MB 66.6 MB/s eta 0:00:01████████ | 55.1 MB 66.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiorwlock<2,>=1.3.0\n",
      "  Using cached aiorwlock-1.3.0-py3-none-any.whl (10.0 kB)\n",
      "Collecting webdriver-manager\n",
      "  Using cached webdriver_manager-3.8.4-py2.py3-none-any.whl (27 kB)\n",
      "Collecting selenium!=4.1.4,>=4.0.0\n",
      "  Using cached selenium-4.5.0-py3-none-any.whl (995 kB)\n",
      "Collecting onnxruntime-tools\n",
      "  Using cached onnxruntime_tools-1.7.0-py3-none-any.whl (212 kB)\n",
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.13.1-cp37-cp37m-macosx_10_15_x86_64.whl (6.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.1 MB 105.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click>=8.0.0\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting platformdirs>=2\n",
      "  Using cached platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
      "Collecting pathspec>=0.9.0\n",
      "  Using cached pathspec-0.10.1-py3-none-any.whl (27 kB)\n",
      "Collecting tomli>=1.1.0\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typed-ast>=1.4.2 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from black[jupyter]==22.6.0->farm-haystack[all]->-r requirements.txt (line 5)) (1.4.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from black[jupyter]==22.6.0->farm-haystack[all]->-r requirements.txt (line 5)) (0.4.3)\n",
      "Collecting tokenize-rt>=3.2.0\n",
      "  Using cached tokenize_rt-5.0.0-py2.py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: ipython>=7.8.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from black[jupyter]==22.6.0->farm-haystack[all]->-r requirements.txt (line 5)) (7.29.0)\n",
      "Requirement already satisfied: backcall in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ipython>=7.8.0->black[jupyter]==22.6.0->farm-haystack[all]->-r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ipython>=7.8.0->black[jupyter]==22.6.0->farm-haystack[all]->-r requirements.txt (line 5)) (0.18.0)\n",
      "Requirement already satisfied: pickleshare in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ipython>=7.8.0->black[jupyter]==22.6.0->farm-haystack[all]->-r requirements.txt (line 5)) (0.7.5)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ipython>=7.8.0->black[jupyter]==22.6.0->farm-haystack[all]->-r requirements.txt (line 5)) (0.1.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ipython>=7.8.0->black[jupyter]==22.6.0->farm-haystack[all]->-r requirements.txt (line 5)) (3.0.20)\n",
      "Requirement already satisfied: pygments in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ipython>=7.8.0->black[jupyter]==22.6.0->farm-haystack[all]->-r requirements.txt (line 5)) (2.10.0)\n",
      "Requirement already satisfied: appnope in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ipython>=7.8.0->black[jupyter]==22.6.0->farm-haystack[all]->-r requirements.txt (line 5)) (0.1.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ipython>=7.8.0->black[jupyter]==22.6.0->farm-haystack[all]->-r requirements.txt (line 5)) (5.1.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ipython>=7.8.0->black[jupyter]==22.6.0->farm-haystack[all]->-r requirements.txt (line 5)) (4.8.0)\n",
      "Requirement already satisfied: decorator in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ipython>=7.8.0->black[jupyter]==22.6.0->farm-haystack[all]->-r requirements.txt (line 5)) (5.1.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from jedi>=0.16->ipython>=7.8.0->black[jupyter]==22.6.0->farm-haystack[all]->-r requirements.txt (line 5)) (0.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from pexpect>4.3->ipython>=7.8.0->black[jupyter]==22.6.0->farm-haystack[all]->-r requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.8.0->black[jupyter]==22.6.0->farm-haystack[all]->-r requirements.txt (line 5)) (0.2.5)\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: cython>=0.24.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from pyworld<=0.2.12->farm-haystack[all]->-r requirements.txt (line 5)) (0.29.24)\n",
      "Collecting grpcio<=1.43.0,>=1.28.1\n",
      "  Downloading grpcio-1.43.0-cp37-cp37m-macosx_10_10_x86_64.whl (4.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1 MB 112.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf<4.0.0,>=3.15.3\n",
      "  Downloading protobuf-3.20.3-cp37-cp37m-macosx_10_9_x86_64.whl (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 106.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click>=8.0.0\n",
      "  Using cached click-8.0.4-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ray<2,>=1.9.1->farm-haystack[all]->-r requirements.txt (line 5)) (1.0.2)\n",
      "Requirement already satisfied: attrs in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ray<2,>=1.9.1->farm-haystack[all]->-r requirements.txt (line 5)) (21.2.0)\n",
      "Collecting aiosignal\n",
      "  Using cached aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting virtualenv\n",
      "  Using cached virtualenv-20.16.6-py3-none-any.whl (8.8 MB)\n",
      "Collecting frozenlist\n",
      "  Downloading frozenlist-1.3.1-cp37-cp37m-macosx_10_9_x86_64.whl (36 kB)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Using cached trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Collecting trio~=0.17\n",
      "  Using cached trio-0.22.0-py3-none-any.whl (384 kB)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Using cached exceptiongroup-1.0.0-py3-none-any.whl (12 kB)\n",
      "Collecting outcome\n",
      "  Using cached outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: async-generator>=1.9 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from trio~=0.17->selenium!=4.1.4,>=4.0.0->farm-haystack[all]->-r requirements.txt (line 5)) (1.10)\n",
      "Requirement already satisfied: sniffio in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from trio~=0.17->selenium!=4.1.4,>=4.0.0->farm-haystack[all]->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from trio~=0.17->selenium!=4.1.4,>=4.0.0->farm-haystack[all]->-r requirements.txt (line 5)) (2.4.0)\n",
      "Collecting wsproto>=0.14\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from urllib3<2,>=1.21.1->elasticsearch<8,>=7.7->farm-haystack[all]->-r requirements.txt (line 5)) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium!=4.1.4,>=4.0.0->farm-haystack[all]->-r requirements.txt (line 5)) (0.13.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from beautifulsoup4->farm-haystack[all]->-r requirements.txt (line 5)) (2.2.1)\n",
      "Collecting pytrec_eval\n",
      "  Using cached pytrec_eval-0.5.tar.gz (15 kB)\n",
      "Collecting elasticsearch<8,>=7.7\n",
      "  Using cached elasticsearch-7.9.1-py2.py3-none-any.whl (219 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.6.1-py3-none-any.whl (441 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.1.0-cp37-cp37m-macosx_10_9_x86_64.whl (34 kB)\n",
      "Collecting responses\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-10.0.0-cp37-cp37m-macosx_10_14_x86_64.whl (24.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.6 MB 14.5 MB/s eta 0:00:01  |███████████████████▋            | 15.0 MB 14.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2022.10.0-py3-none-any.whl (138 kB)\n",
      "\u001b[K     |████████████████████████████████| 138 kB 111.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.3-cp37-cp37m-macosx_10_9_x86_64.whl (355 kB)\n",
      "\u001b[K     |████████████████████████████████| 355 kB 24.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill\n",
      "  Using cached dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
      "\u001b[K     |████████████████████████████████| 115 kB 120.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp37-cp37m-macosx_10_9_x86_64.whl (28 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp37-cp37m-macosx_10_9_x86_64.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 25.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting asynctest==0.13.0\n",
      "  Using cached asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting configargparse>=1.2.1\n",
      "  Using cached ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n",
      "Collecting soundfile>=0.10.2\n",
      "  Downloading soundfile-0.11.0-py2.py3-none-macosx_10_9_x86_64.macosx_11_0_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting espnet-tts-frontend\n",
      "  Using cached espnet_tts_frontend-0.0.3-py3-none-any.whl (11 kB)\n",
      "Collecting pytorch-wpe\n",
      "  Using cached pytorch_wpe-0.0.1-py3-none-any.whl (8.1 kB)\n",
      "Collecting humanfriendly\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Collecting kaldiio>=2.17.0\n",
      "  Using cached kaldiio-2.17.2.tar.gz (24 kB)\n",
      "Requirement already satisfied: h5py>=2.10.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from espnet->farm-haystack[all]->-r requirements.txt (line 5)) (2.10.0)\n",
      "Collecting typeguard>=2.7.0\n",
      "  Using cached typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Collecting ci-sdr\n",
      "  Using cached ci_sdr-0.0.2.tar.gz (15 kB)\n",
      "Collecting torch-complex\n",
      "  Using cached torch_complex-0.4.3-py3-none-any.whl (9.1 kB)\n",
      "Collecting pypinyin<=0.44.0\n",
      "  Using cached pypinyin-0.44.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Collecting librosa>=0.8.0\n",
      "  Using cached librosa-0.9.2-py3-none-any.whl (214 kB)\n",
      "Collecting ctc-segmentation>=1.6.6\n",
      "  Using cached ctc_segmentation-1.7.4.tar.gz (73 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jamo==0.4.1\n",
      "  Using cached jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
      "Collecting fast-bss-eval==0.1.3\n",
      "  Using cached fast_bss_eval-0.1.3.tar.gz (33 kB)\n",
      "Collecting protobuf<4.0.0,>=3.15.3\n",
      "  Downloading protobuf-3.20.1-cp37-cp37m-macosx_10_9_x86_64.whl (961 kB)\n",
      "\u001b[K     |████████████████████████████████| 961 kB 92.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from importlib-metadata->farm-haystack[all]->-r requirements.txt (line 5)) (3.6.0)\n",
      "Collecting pooch>=1.0\n",
      "  Using cached pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "Collecting resampy>=0.2.2\n",
      "  Using cached resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
      "Requirement already satisfied: numba>=0.45.1 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from librosa>=0.8.0->espnet->farm-haystack[all]->-r requirements.txt (line 5)) (0.54.1)\n",
      "Collecting audioread>=2.1.9\n",
      "  Using cached audioread-3.0.0.tar.gz (377 kB)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from numba>=0.45.1->librosa>=0.8.0->espnet->farm-haystack[all]->-r requirements.txt (line 5)) (0.37.0)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from pooch>=1.0->librosa>=0.8.0->espnet->farm-haystack[all]->-r requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: cffi>=1.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from soundfile>=0.10.2->espnet->farm-haystack[all]->-r requirements.txt (line 5)) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.10.2->espnet->farm-haystack[all]->-r requirements.txt (line 5)) (2.20)\n",
      "Collecting einops\n",
      "  Using cached einops-0.5.0-py3-none-any.whl (36 kB)\n",
      "Collecting inflect>=1.0.0\n",
      "  Using cached inflect-6.0.2-py3-none-any.whl (34 kB)\n",
      "Collecting g2p-en\n",
      "  Using cached g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
      "Requirement already satisfied: unidecode>=1.0.22 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from espnet-tts-frontend->espnet->farm-haystack[all]->-r requirements.txt (line 5)) (1.2.0)\n",
      "Collecting jaconv\n",
      "  Using cached jaconv-0.3.tar.gz (15 kB)\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-1.10.2-cp37-cp37m-macosx_10_9_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 101.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opensearch-py>=2\n",
      "  Using cached opensearch_py-2.0.0-py2.py3-none-any.whl (204 kB)\n",
      "Collecting weaviate-client==3.6.0\n",
      "  Using cached weaviate_client-3.6.0-py3-none-any.whl (67 kB)\n",
      "Collecting validators<0.19.0,>=0.18.2\n",
      "  Using cached validators-0.18.2-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from sqlalchemy>=1.4.2->farm-haystack[all]->-r requirements.txt (line 5)) (1.1.1)\n",
      "Requirement already satisfied: ujson<=5.4.0,>=2.0.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from pymilvus->farm-haystack[all]->-r requirements.txt (line 5)) (3.2.0)\n",
      "Collecting pymilvus<3,>=2.0.0\n",
      "  Using cached pymilvus-2.1.2-py3-none-any.whl (118 kB)\n",
      "  Using cached pymilvus-2.1.1-py3-none-any.whl (117 kB)\n",
      "  Using cached pymilvus-2.1.0-py3-none-any.whl (132 kB)\n",
      "  Using cached pymilvus-2.0.2-py3-none-any.whl (119 kB)\n",
      "Collecting grpcio-tools==1.37.1\n",
      "  Downloading grpcio_tools-1.37.1-cp37-cp37m-macosx_10_10_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 65.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio<=1.43.0,>=1.28.1\n",
      "  Downloading grpcio-1.37.1-cp37-cp37m-macosx_10_10_x86_64.whl (3.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9 MB 108.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pinecone-client<3,>=2.0.11\n",
      "  Using cached pinecone_client-2.0.13-py3-none-any.whl (175 kB)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from pinecone-client<3,>=2.0.11->farm-haystack[all]->-r requirements.txt (line 5)) (2.2.1)\n",
      "Collecting loguru>=0.5.0\n",
      "  Using cached loguru-0.6.0-py3-none-any.whl (58 kB)\n",
      "Collecting distance>=0.1.3\n",
      "  Using cached Distance-0.1.3.tar.gz (180 kB)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ipykernel->jupyter->-r requirements.txt (line 8)) (6.1.12)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ipykernel->jupyter->-r requirements.txt (line 8)) (1.4.1)\n",
      "Requirement already satisfied: ipython-genutils in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ipykernel->jupyter->-r requirements.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ipykernel->jupyter->-r requirements.txt (line 8)) (6.1)\n",
      "Requirement already satisfied: argcomplete>=1.12.3 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ipykernel->jupyter->-r requirements.txt (line 8)) (1.12.3)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel->jupyter->-r requirements.txt (line 8)) (4.8.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel->jupyter->-r requirements.txt (line 8)) (22.2.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ipywidgets->jupyter->-r requirements.txt (line 8)) (5.1.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ipywidgets->jupyter->-r requirements.txt (line 8)) (3.5.1)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from ipywidgets->jupyter->-r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from jsonschema->farm-haystack[all]->-r requirements.txt (line 5)) (0.18.0)\n",
      "Requirement already satisfied: prometheus-client in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from notebook->jupyter->-r requirements.txt (line 8)) (0.11.0)\n",
      "Requirement already satisfied: argon2-cffi in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from notebook->jupyter->-r requirements.txt (line 8)) (20.1.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from notebook->jupyter->-r requirements.txt (line 8)) (1.8.0)\n",
      "Requirement already satisfied: jinja2 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from notebook->jupyter->-r requirements.txt (line 8)) (2.11.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from notebook->jupyter->-r requirements.txt (line 8)) (0.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from jinja2->notebook->jupyter->-r requirements.txt (line 8)) (1.1.1)\n",
      "Collecting markdown\n",
      "  Using cached Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "Collecting mergedeep>=1.3.4\n",
      "  Using cached mergedeep-1.3.4-py3-none-any.whl (6.4 kB)\n",
      "Collecting pyyaml-env-tag>=0.1\n",
      "  Using cached pyyaml_env_tag-0.1-py3-none-any.whl (3.9 kB)\n",
      "Collecting ghp-import>=1.0\n",
      "  Using cached ghp_import-2.1.0-py3-none-any.whl (11 kB)\n",
      "Collecting sqlparse<1,>=0.4.0\n",
      "  Using cached sqlparse-0.4.3-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: cloudpickle<3 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from mlflow->farm-haystack[all]->-r requirements.txt (line 5)) (2.0.0)\n",
      "Collecting prometheus-flask-exporter<1\n",
      "  Using cached prometheus_flask_exporter-0.20.3-py3-none-any.whl (18 kB)\n",
      "Collecting docker<7,>=4.0.0\n",
      "  Using cached docker-6.0.0-py3-none-any.whl (147 kB)\n",
      "Collecting databricks-cli<1,>=0.8.7\n",
      "  Using cached databricks-cli-0.17.3.tar.gz (77 kB)\n",
      "Collecting querystring-parser<2\n",
      "  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting gitpython<4,>=2.1.0\n",
      "  Using cached GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: entrypoints<1 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from mlflow->farm-haystack[all]->-r requirements.txt (line 5)) (0.3)\n",
      "Collecting alembic<2\n",
      "  Using cached alembic-1.8.1-py3-none-any.whl (209 kB)\n",
      "Collecting Mako\n",
      "  Using cached Mako-1.2.3-py3-none-any.whl (78 kB)\n",
      "Collecting importlib-resources\n",
      "  Using cached importlib_resources-5.10.0-py3-none-any.whl (34 kB)\n",
      "Collecting pyjwt>=1.7.0\n",
      "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
      "Collecting tabulate>=0.7.7\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Using cached websocket_client-1.4.1-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from flask->farm-haystack[all]->-r requirements.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from flask->farm-haystack[all]->-r requirements.txt (line 5)) (2.0.2)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
      "\u001b[K     |████████████████████████████████| 115 kB 100.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: mistune<2,>=0.8.1 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.8.4)\n",
      "Requirement already satisfied: testpath in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.1.2)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.5.3)\n",
      "Requirement already satisfied: defusedxml in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.7.1)\n",
      "Requirement already satisfied: bleach in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (4.0.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (1.4.3)\n",
      "Requirement already satisfied: nest-asyncio in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->jupyter->-r requirements.txt (line 8)) (1.5.1)\n",
      "Requirement already satisfied: webencodings in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from bleach->nbconvert->jupyter->-r requirements.txt (line 8)) (0.5.1)\n",
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.12.1-cp37-cp37m-macosx_10_15_x86_64.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading onnxruntime-1.12.0-cp37-cp37m-macosx_10_15_x86_64.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 18.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading onnxruntime-1.11.1-cp37-cp37m-macosx_10_14_x86_64.whl (5.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.7 MB 110.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading onnxruntime-1.11.0-cp37-cp37m-macosx_10_14_x86_64.whl (5.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.7 MB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading onnxruntime-1.10.0-cp37-cp37m-macosx_10_14_x86_64.whl (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 117.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers\n",
      "  Using cached flatbuffers-22.10.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting py-cpuinfo\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting py3nvml\n",
      "  Using cached py3nvml-0.2.7-py3-none-any.whl (55 kB)\n",
      "Collecting onnx\n",
      "  Downloading onnx-1.12.0-cp37-cp37m-macosx_10_12_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 54.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting coloredlogs\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Collecting monotonic>=1.5\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting backoff<2.0.0,>=1.10.0\n",
      "  Using cached backoff-1.11.1-py2.py3-none-any.whl (13 kB)\n",
      "Collecting identify>=1.0.0\n",
      "  Using cached identify-2.5.8-py2.py3-none-any.whl (98 kB)\n",
      "Collecting nodeenv>=0.11.1\n",
      "  Using cached nodeenv-1.7.0-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: toml in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from pre-commit->farm-haystack[all]->-r requirements.txt (line 5)) (0.10.2)\n",
      "Collecting cfgv>=2.0.0\n",
      "  Using cached cfgv-3.3.1-py2.py3-none-any.whl (7.3 kB)\n",
      "Collecting importlib-metadata\n",
      "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
      "Collecting distlib<1,>=0.3.6\n",
      "  Using cached distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Collecting xmltodict\n",
      "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
      "Collecting nr.util<1.0.0,>=0.7.5\n",
      "  Using cached nr.util-0.8.12-py3-none-any.whl (90 kB)\n",
      "Collecting docspec-python<3.0.0,>=2.0.0a1\n",
      "  Using cached docspec_python-2.0.2-py3-none-any.whl (13 kB)\n",
      "Collecting databind<2.0.0,>=1.5.0\n",
      "  Using cached databind-1.5.3-py3-none-any.whl (1.6 kB)\n",
      "Collecting docstring-parser<0.12,>=0.11\n",
      "  Using cached docstring_parser-0.11.tar.gz (22 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: yapf>=0.30.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from pydoc-markdown->farm-haystack[all]->-r requirements.txt (line 5)) (0.31.0)\n",
      "Collecting tomli_w<2.0.0,>=1.0.0\n",
      "  Using cached tomli_w-1.0.0-py3-none-any.whl (6.0 kB)\n",
      "Collecting docspec<3.0.0,>=2.0.0a1\n",
      "  Using cached docspec-2.0.2-py3-none-any.whl (9.5 kB)\n",
      "Collecting databind.json<2.0.0,>=1.5.3\n",
      "  Using cached databind.json-1.5.3-py3-none-any.whl (14 kB)\n",
      "Collecting databind.core<2.0.0,>=1.5.3\n",
      "  Using cached databind.core-1.5.3-py3-none-any.whl (35 kB)\n",
      "Collecting Deprecated<2.0.0,>=1.2.12\n",
      "  Using cached Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from Deprecated<2.0.0,>=1.2.12->databind.core<2.0.0,>=1.5.3->databind<2.0.0,>=1.5.0->pydoc-markdown->farm-haystack[all]->-r requirements.txt (line 5)) (1.12.1)\n",
      "Collecting MarkupSafe>=0.23\n",
      "  Downloading MarkupSafe-2.1.1-cp37-cp37m-macosx_10_9_x86_64.whl (13 kB)\n",
      "Requirement already satisfied: isort<6,>=4.2.5 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from pylint->farm-haystack[all]->-r requirements.txt (line 5)) (5.9.3)\n",
      "Requirement already satisfied: mccabe<0.7,>=0.6 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from pylint->farm-haystack[all]->-r requirements.txt (line 5)) (0.6.1)\n",
      "Requirement already satisfied: astroid<2.7,>=2.6.5 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from pylint->farm-haystack[all]->-r requirements.txt (line 5)) (2.6.6)\n",
      "Requirement already satisfied: lazy-object-proxy>=1.4.0 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from astroid<2.7,>=2.6.5->pylint->farm-haystack[all]->-r requirements.txt (line 5)) (1.6.0)\n",
      "Requirement already satisfied: iniconfig in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from pytest->farm-haystack[all]->-r requirements.txt (line 5)) (1.1.1)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from pytest->farm-haystack[all]->-r requirements.txt (line 5)) (0.13.1)\n",
      "Requirement already satisfied: py>=1.8.2 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from pytest->farm-haystack[all]->-r requirements.txt (line 5)) (1.10.0)\n",
      "Requirement already satisfied: lxml>=2.3.2 in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from python-docx->farm-haystack[all]->-r requirements.txt (line 5)) (4.6.3)\n",
      "Requirement already satisfied: qtpy in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from qtconsole->jupyter->-r requirements.txt (line 8)) (1.10.0)\n",
      "Collecting num2words\n",
      "  Using cached num2words-0.5.12-py3-none-any.whl (125 kB)\n",
      "Collecting docopt>=0.6.2\n",
      "  Using cached docopt-0.6.2.tar.gz (25 kB)\n",
      "Collecting url-normalize>=1.4\n",
      "  Using cached url_normalize-1.4.3-py2.py3-none-any.whl (6.8 kB)\n",
      "Collecting cattrs>=22.2\n",
      "  Using cached cattrs-22.2.0-py3-none-any.whl (35 kB)\n",
      "Collecting rdflib>=6.1.1\n",
      "  Using cached rdflib-6.2.0-py3-none-any.whl (500 kB)\n",
      "Requirement already satisfied: python-dotenv in /Users/markus/opt/anaconda3/envs/wispr/lib/python3.7/site-packages (from webdriver-manager->farm-haystack[all]->-r requirements.txt (line 5)) (0.20.0)\n",
      "Building wheels for collected packages: whisper, sentence-transformers, pyworld, beir, fast-bss-eval, ctc-segmentation, kaldiio, audioread, ci-sdr, future, distance, jaconv, langdetect, databricks-cli, docstring-parser, python-docx, pytrec-eval, docopt, seqeval, tika\n",
      "  Building wheel for whisper (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for whisper: filename=whisper-1.0-py3-none-any.whl size=1175217 sha256=839e4db1fcde06d243b05c545f5c5b3216e1f1c0cb8e3684e8f50096089c8f5e\n",
      "  Stored in directory: /private/var/folders/tc/q_js7srx33sc183v2vl5lfr00000gn/T/pip-ephem-wheel-cache-ia1kmlo7/wheels/16/15/89/1c7bb31bd0006793a95549d04785121a8a36daad9158e1e43a\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=072d2375182f99f76c1f58e5ef03344e8322f8cef67870777a69ae838116b040\n",
      "  Stored in directory: /Users/markus/Library/Caches/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
      "  Building wheel for pyworld (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyworld: filename=pyworld-0.2.12-cp37-cp37m-macosx_10_9_x86_64.whl size=212080 sha256=db7e4854f27629c4e549ee873180307c0f9ef4a8e8396624e8c69f5806b710ab\n",
      "  Stored in directory: /Users/markus/Library/Caches/pip/wheels/49/08/ca/d754439cfd565f7a44e5b0a315d163a3cff19d756a49db1857\n",
      "  Building wheel for beir (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for beir: filename=beir-1.0.1-py3-none-any.whl size=62516 sha256=bae9156a9288e40647ded787587c888e56bd40a7ee7862f1b448570309e39e78\n",
      "  Stored in directory: /Users/markus/Library/Caches/pip/wheels/49/e8/58/18d0218f6a1eb9b343b100181915dccc668d7fe688704be89c\n",
      "  Building wheel for fast-bss-eval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fast-bss-eval: filename=fast_bss_eval-0.1.3-py3-none-any.whl size=44260 sha256=93d56875c264c4355fcf7f8706bcd908298e2bb6422fbe92c19af100a9fd87ec\n",
      "  Stored in directory: /Users/markus/Library/Caches/pip/wheels/45/38/82/6507c8f1765284b45fac38f334621435b633499f27c813e3f1\n",
      "  Building wheel for ctc-segmentation (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ctc-segmentation: filename=ctc_segmentation-1.7.4-cp37-cp37m-macosx_10_9_x86_64.whl size=37866 sha256=02f1f255248303445a32ac8e61075c06c3e39ae8878a72e473c61f59ba347d2c\n",
      "  Stored in directory: /Users/markus/Library/Caches/pip/wheels/e5/95/9c/8ece861dba688bc491c46c9c4b57c49c5837bf3d371b706acb\n",
      "  Building wheel for kaldiio (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaldiio: filename=kaldiio-2.17.2-py3-none-any.whl size=24472 sha256=3bec895ff289298cef5b1f28d3898a781cfe91c006f9d490780e262d913b423e\n",
      "  Stored in directory: /Users/markus/Library/Caches/pip/wheels/04/07/e8/45641287c59bf6ce41e22259f8680b521c31e6306cb88392ac\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23706 sha256=5c711c3c8cf1f8b43d2691888ec5e64c071051ca70eef493653b9d720cf580ce\n",
      "  Stored in directory: /Users/markus/Library/Caches/pip/wheels/71/a4/fa/24175dada88ca37d7fd22ffec10b33cb0a4909d7d07f04101f\n",
      "  Building wheel for ci-sdr (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ci-sdr: filename=ci_sdr-0.0.2-py3-none-any.whl size=15276 sha256=ce194190b84d495a7e21cff8854d74c92cf35097910ec49bbc53259b8804995f\n",
      "  Stored in directory: /Users/markus/Library/Caches/pip/wheels/b4/ca/94/53aa51e44584908b10dcc98dc57628cabe7a0ed324746eea2e\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=c97390b063349e533f7f47a9ec554157ebf919f108ee2f0e4a9f320bfcec2555\n",
      "  Stored in directory: /Users/markus/Library/Caches/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "  Building wheel for distance (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16275 sha256=a5660fd4b4bd959858599986a7c6aa602e6cfe32875b1c29adc76f9610d1e455\n",
      "  Stored in directory: /Users/markus/Library/Caches/pip/wheels/b2/10/1b/96fca621a1be378e2fe104cfb0d160bb6cdf3d04a3d35266cc\n",
      "  Building wheel for jaconv (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jaconv: filename=jaconv-0.3-py3-none-any.whl size=15565 sha256=784abb6b52a2679204847f787d111f95fa4616a835646a998c247445b9a3376a\n",
      "  Stored in directory: /Users/markus/Library/Caches/pip/wheels/8f/4f/c2/a2a3b14d0e94f855f4aa8887bf0267bee9ecfb8e62a9ee2d92\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=7c6b0f1174ee5ff7e449d7117cae2a7915f6c53203a54182defb2df5835cc2bd\n",
      "  Stored in directory: /Users/markus/Library/Caches/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
      "  Building wheel for databricks-cli (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for databricks-cli: filename=databricks_cli-0.17.3-py3-none-any.whl size=139103 sha256=07c44069864db925904cdf93113cb298e66e61ef7cc751c9e17298682c6a2d86\n",
      "  Stored in directory: /Users/markus/Library/Caches/pip/wheels/3f/73/87/c1e4b2145eb6049bb6c9aaf7ea1e38302b77ca219b6fef5d5c\n",
      "  Building wheel for docstring-parser (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docstring-parser: filename=docstring_parser-0.11-py3-none-any.whl size=31514 sha256=2d4472dddcdf63f6e57b11004bcc5bc46acf298d7eede48aa87295886b1a6601\n",
      "  Stored in directory: /Users/markus/Library/Caches/pip/wheels/8d/ba/2a/1376f9ea0b3f20a9700b4f1b4ee3cefe69b4a96e26a28e0240\n",
      "  Building wheel for python-docx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184508 sha256=a9d2ba85feeee14c54caa30ad93c8b2b4e31d4d2705033d1246b1970588a7833\n",
      "  Stored in directory: /Users/markus/Library/Caches/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
      "  Building wheel for pytrec-eval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytrec-eval: filename=pytrec_eval-0.5-cp37-cp37m-macosx_10_9_x86_64.whl size=73781 sha256=181294e6f61f1a6a761aee897cd5be7cb7919d70912b497ab402873544618188\n",
      "  Stored in directory: /Users/markus/Library/Caches/pip/wheels/42/96/77/0829b8b2606f90f61ba10a51277629d2b615604e122ee932f4\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=71ba3c66a5973e878f31ec19ccbb2fc6a3695b411a0672963d8de8b346dc9dd3\n",
      "  Stored in directory: /Users/markus/Library/Caches/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=92dff3d3a263223ff3a38ede09361b5a19bdcb0199a335b8c36e567a13eea4ae\n",
      "  Stored in directory: /Users/markus/Library/Caches/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
      "  Building wheel for tika (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32891 sha256=dfcf5e3b623fbc8c14a5cc3f25d8cd06b3bc922db1c07deb9680e16abd308065\n",
      "  Stored in directory: /Users/markus/Library/Caches/pip/wheels/ec/2b/38/58ff05467a742e32f67f5d0de048fa046e764e2fbb25ac93f3\n",
      "Successfully built whisper sentence-transformers pyworld beir fast-bss-eval ctc-segmentation kaldiio audioread ci-sdr future distance jaconv langdetect databricks-cli docstring-parser python-docx pytrec-eval docopt seqeval tika\n",
      "Installing collected packages: typing-extensions, MarkupSafe, importlib-metadata, Deprecated, smmap, packaging, oauthlib, nr.util, jinja2, filelock, click, websocket-client, torch, tokenizers, tabulate, requests-oauthlib, pyjwt, pydantic, multidict, Mako, isodate, importlib-resources, huggingface-hub, gitdb, frozenlist, docopt, databind.core, azure-core, yarl, transformers, torchvision, sqlparse, sentencepiece, scikit-learn, querystring-parser, protobuf, prometheus-flask-exporter, num2words, msrest, monotonic, jarowinkler, inflect, gunicorn, grpcio, gitpython, docker, distance, databricks-cli, databind.json, backoff, azure-common, asynctest, async-timeout, alembic, aiosignal, tika, soundfile, seqeval, sentence-transformers, resampy, rapidfuzz, quantulum3, python-docx, pypinyin, posthog, pooch, outcome, mmh3, mlflow, loguru, langdetect, jaconv, grpcio-tools, g2p-en, fsspec, exceptiongroup, elasticsearch, einops, dill, databind, azure-ai-formrecognizer, audioread, aiohttp, xxhash, xmltodict, wsproto, validators, typeguard, trio, torch-complex, tomli, sqlalchemy-utils, responses, rdflib, pyworld, pytorch-wpe, pymilvus, pyarrow, psycopg2-binary, platformdirs, pinecone-client, pathspec, multiprocess, librosa, kaldiio, jamo, humanfriendly, fast-bss-eval, farm-haystack, faiss-cpu, espnet-tts-frontend, docspec, distlib, ctc-segmentation, configargparse, ci-sdr, weaviate-client, virtualenv, url-normalize, trio-websocket, tomli-w, tokenize-rt, SPARQLWrapper, pyyaml-env-tag, pytrec-eval, py3nvml, py-cpuinfo, opensearch-py, onnx, nodeenv, mergedeep, markdown, identify, ghp-import, flatbuffers, espnet, docstring-parser, docspec-python, datasets, coloredlogs, cfgv, cattrs, black, webdriver-manager, tox, selenium, requests-cache, ray, python-magic, pytest-custom-exit-code, pytesseract, pydub, pydoc-markdown, pre-commit, pdf2image, onnxruntime-tools, onnxruntime, mypy, mkdocs, jupytercontrib, future, espnet-model-zoo, coverage, beir, aiorwlock, ffmpeg-python, whisper, torchaudio\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 1.1.1\n",
      "    Uninstalling MarkupSafe-1.1.1:\n",
      "      Successfully uninstalled MarkupSafe-1.1.1\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.8.1\n",
      "    Uninstalling importlib-metadata-4.8.1:\n",
      "      Successfully uninstalled importlib-metadata-4.8.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.0\n",
      "    Uninstalling packaging-21.0:\n",
      "      Successfully uninstalled packaging-21.0\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 2.11.3\n",
      "    Uninstalling Jinja2-2.11.3:\n",
      "      Successfully uninstalled Jinja2-2.11.3\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.3.1\n",
      "    Uninstalling filelock-3.3.1:\n",
      "      Successfully uninstalled filelock-3.3.1\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 7.1.2\n",
      "    Uninstalling click-7.1.2:\n",
      "      Successfully uninstalled click-7.1.2\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.9.0\n",
      "    Uninstalling pydantic-1.9.0:\n",
      "      Successfully uninstalled pydantic-1.9.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.2\n",
      "    Uninstalling scikit-learn-0.24.2:\n",
      "      Successfully uninstalled scikit-learn-0.24.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2021.8.1\n",
      "    Uninstalling fsspec-2021.8.1:\n",
      "      Successfully uninstalled fsspec-2021.8.1\n",
      "  Attempting uninstall: pathspec\n",
      "    Found existing installation: pathspec 0.7.0\n",
      "    Uninstalling pathspec-0.7.0:\n",
      "      Successfully uninstalled pathspec-0.7.0\n",
      "  Attempting uninstall: black\n",
      "    Found existing installation: black 19.10b0\n",
      "    Uninstalling black-19.10b0:\n",
      "      Successfully uninstalled black-19.10b0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.10.1 requires ruamel-yaml, which is not installed.\n",
      "uvicorn 0.13.4 requires click==7.*, but you have click 8.0.4 which is incompatible.\n",
      "cookiecutter 1.7.2 requires Jinja2<3.0.0, but you have jinja2 3.1.2 which is incompatible.\n",
      "cookiecutter 1.7.2 requires MarkupSafe<2.0.0, but you have markupsafe 2.1.1 which is incompatible.\u001b[0m\n",
      "Successfully installed Deprecated-1.2.13 Mako-1.2.3 MarkupSafe-2.1.1 SPARQLWrapper-2.0.0 aiohttp-3.8.3 aiorwlock-1.3.0 aiosignal-1.2.0 alembic-1.8.1 async-timeout-4.0.2 asynctest-0.13.0 audioread-3.0.0 azure-ai-formrecognizer-3.2.0 azure-common-1.1.28 azure-core-1.26.0 backoff-1.11.1 beir-1.0.1 black-22.6.0 cattrs-22.2.0 cfgv-3.3.1 ci-sdr-0.0.2 click-8.0.4 coloredlogs-15.0.1 configargparse-1.5.3 coverage-6.5.0 ctc-segmentation-1.7.4 databind-1.5.3 databind.core-1.5.3 databind.json-1.5.3 databricks-cli-0.17.3 datasets-2.6.1 dill-0.3.5.1 distance-0.1.3 distlib-0.3.6 docker-6.0.0 docopt-0.6.2 docspec-2.0.2 docspec-python-2.0.2 docstring-parser-0.11 einops-0.5.0 elasticsearch-7.9.1 espnet-202209 espnet-model-zoo-0.1.7 espnet-tts-frontend-0.0.3 exceptiongroup-1.0.0 faiss-cpu-1.7.2 farm-haystack-1.10.0 fast-bss-eval-0.1.3 ffmpeg-python-0.2.0 filelock-3.8.0 flatbuffers-22.10.26 frozenlist-1.3.1 fsspec-2022.10.0 future-0.18.2 g2p-en-2.1.0 ghp-import-2.1.0 gitdb-4.0.9 gitpython-3.1.29 grpcio-1.37.1 grpcio-tools-1.37.1 gunicorn-20.1.0 huggingface-hub-0.10.1 humanfriendly-10.0 identify-2.5.8 importlib-metadata-4.13.0 importlib-resources-5.10.0 inflect-6.0.2 isodate-0.6.1 jaconv-0.3 jamo-0.4.1 jarowinkler-1.2.3 jinja2-3.1.2 jupytercontrib-0.0.7 kaldiio-2.17.2 langdetect-1.0.9 librosa-0.9.2 loguru-0.6.0 markdown-3.3.7 mergedeep-1.3.4 mkdocs-1.4.1 mlflow-1.30.0 mmh3-3.0.0 monotonic-1.6 msrest-0.7.1 multidict-6.0.2 multiprocess-0.70.13 mypy-0.982 nodeenv-1.7.0 nr.util-0.8.12 num2words-0.5.12 oauthlib-3.2.2 onnx-1.12.0 onnxruntime-1.10.0 onnxruntime-tools-1.7.0 opensearch-py-2.0.0 outcome-1.2.0 packaging-21.3 pathspec-0.10.1 pdf2image-1.16.0 pinecone-client-2.0.13 platformdirs-2.5.2 pooch-1.6.0 posthog-2.1.2 pre-commit-2.20.0 prometheus-flask-exporter-0.20.3 protobuf-3.20.1 psycopg2-binary-2.9.5 py-cpuinfo-9.0.0 py3nvml-0.2.7 pyarrow-10.0.0 pydantic-1.10.2 pydoc-markdown-4.6.3 pydub-0.25.1 pyjwt-2.6.0 pymilvus-2.0.2 pypinyin-0.44.0 pytesseract-0.3.10 pytest-custom-exit-code-0.3.0 python-docx-0.8.11 python-magic-0.4.27 pytorch-wpe-0.0.1 pytrec-eval-0.5 pyworld-0.2.12 pyyaml-env-tag-0.1 quantulum3-0.7.11 querystring-parser-1.2.4 rapidfuzz-2.7.0 ray-1.13.0 rdflib-6.2.0 requests-cache-0.9.7 requests-oauthlib-1.3.1 resampy-0.4.2 responses-0.18.0 scikit-learn-1.0.2 selenium-4.5.0 sentence-transformers-2.2.2 sentencepiece-0.1.97 seqeval-1.2.2 smmap-5.0.0 soundfile-0.11.0 sqlalchemy-utils-0.38.3 sqlparse-0.4.3 tabulate-0.9.0 tika-1.24 tokenize-rt-5.0.0 tokenizers-0.12.1 tomli-2.0.1 tomli-w-1.0.0 torch-1.12.1 torch-complex-0.4.3 torchaudio-0.12.1 torchvision-0.13.1 tox-3.27.0 transformers-4.21.2 trio-0.22.0 trio-websocket-0.9.2 typeguard-2.13.3 typing-extensions-4.4.0 url-normalize-1.4.3 validators-0.18.2 virtualenv-20.16.6 weaviate-client-3.6.0 webdriver-manager-3.8.4 websocket-client-1.4.1 whisper-1.0 wsproto-1.2.0 xmltodict-0.13.0 xxhash-3.1.0 yarl-1.8.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import datetime\n",
    "import subprocess\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from haystack.nodes import EmbeddingRetriever, FARMReader\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts=\"\"\n",
    "df = pd.read_json(transcripts)\n",
    "ds = df.to_dict(\"records\")\n",
    "\n",
    "# document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\")\n",
    "# document_store = ElasticsearchDocumentStore(similarity=\"cosine\", embedding_dim=384)\n",
    "\n",
    "\n",
    "dicts = [{\"content\": d[\"text\"], \"meta\": {i:d[i] for i in d if i!='text'}} for d in ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of documents present in the SQL database (1) does not match the number of embeddings in FAISS (0). Make sure your FAISS configuration file correctly points to the same database that was used when creating the original index.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tc/q_js7srx33sc183v2vl5lfr00000gn/T/ipykernel_68447/2397564457.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_stores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInMemoryDocumentStore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFAISSDocumentStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdocument_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISSDocumentStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaiss_index_factory_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Flat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# document_store = InMemoryDocumentStore(embedding_dim=384, similarity=\"cosine\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/wispr/lib/python3.7/site-packages/haystack/nodes/base.py\u001b[0m in \u001b[0;36mwrapper_exportable_to_yaml\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Call the actuall __init__ function with all the arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper_exportable_to_yaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/wispr/lib/python3.7/site-packages/haystack/document_stores/faiss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sql_url, vector_dim, embedding_dim, faiss_index_factory_str, faiss_index, return_embedding, index, similarity, embedding_field, progress_bar, duplicate_documents, faiss_index_path, faiss_config_path, isolation_level, n_links, ef_search, ef_construction, validate_index_sync)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidate_index_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_index_sync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_params_load_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/wispr/lib/python3.7/site-packages/haystack/document_stores/faiss.py\u001b[0m in \u001b[0;36m_validate_index_sync\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             raise ValueError(\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0;34mf\"The number of documents present in the SQL database ({self.get_document_count()}) does not \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0;34mf\"match the number of embeddings in FAISS ({self.get_embedding_count()}). Make sure your FAISS \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;34m\"configuration file correctly points to the same database that \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The number of documents present in the SQL database (1) does not match the number of embeddings in FAISS (0). Make sure your FAISS configuration file correctly points to the same database that was used when creating the original index."
     ]
    }
   ],
   "source": [
    "# In-Memory Document Store\n",
    "from haystack.document_stores import InMemoryDocumentStore, FAISSDocumentStore\n",
    "\n",
    "# document_store = FAISSDocumentStore(faiss_index_factory_str=\"Flat\", embedding_dim=384, similarity=\"cosine\")\n",
    "\n",
    "document_store = InMemoryDocumentStore(embedding_dim=384, similarity=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing Documents:   0%|          | 0/155 [00:00<?, ?it/s]ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "ERROR:haystack.document_stores.sql:Document c2ae3bae6d60ff5442a064aa2c9bfbc3 - Discarded metadata 'segments', since it has invalid type: dict.\n",
      "SQLDocumentStore can accept and cast to string only the following types: str, int, float, bool, bytes, bytearray, NoneType\n",
      "Writing Documents: 10000it [00:01, 7368.01it/s]           \n",
      "WARNING:haystack.nodes.retriever._embedding_encoder:You are using a Sentence Transformer with the dot_product function. We recommend using cosine instead. This can be set when initializing the DocumentStore\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.39s/it]ocs/s]\n",
      "Updating Embedding:   0%|          | 0/1 [00:02<?, ? docs/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Embedding dimensions of the model (384) don't match the embedding dimensions of the document store (768). Initiate FAISSDocumentStore again with arg embedding_dim=384.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tc/q_js7srx33sc183v2vl5lfr00000gn/T/ipykernel_68447/2392501609.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingRetriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_store\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocument_store\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sentence-transformers/all-MiniLM-L6-v2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdocument_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/wispr/lib/python3.7/site-packages/haystack/document_stores/faiss.py\u001b[0m in \u001b[0;36mupdate_embeddings\u001b[0;34m(self, retriever, index, update_existing_embeddings, filters, batch_size)\u001b[0m\n\u001b[1;32m    359\u001b[0m                 \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                 self._validate_embeddings_shape(\n\u001b[0;32m--> 361\u001b[0;31m                     \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m                 )\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/wispr/lib/python3.7/site-packages/haystack/document_stores/base.py\u001b[0m in \u001b[0;36m_validate_embeddings_shape\u001b[0;34m(cls, embeddings, num_documents, embedding_dim)\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0membedding_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             raise RuntimeError(\n\u001b[0;32m--> 676\u001b[0;31m                 \u001b[0;34mf\"Embedding dimensions of the model ({embedding_size}) don't match the embedding dimensions of the document store ({embedding_dim}). \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m                 \u001b[0;34mf\"Initiate {cls.__name__} again with arg embedding_dim={embedding_size}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Embedding dimensions of the model (384) don't match the embedding dimensions of the document store (768). Initiate FAISSDocumentStore again with arg embedding_dim=384."
     ]
    }
   ],
   "source": [
    "document_store.write_documents(dicts)\n",
    "\n",
    "retriever = EmbeddingRetriever(document_store=document_store, embedding_model='sentence-transformers/all-MiniLM-L6-v2', use_gpu=True, top_k=10)\n",
    "\n",
    "document_store.update_embeddings(retriever)\n",
    "\n",
    "\n",
    "\n",
    "reader = FARMReader(model_name_or_path='deepset/roberta-base-squad2', use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.81s/ Batches]\n"
     ]
    }
   ],
   "source": [
    "pipe = ExtractiveQAPipeline(reader, retriever)\n",
    "# You can configure how many candidates the reader and retriever shall return\n",
    "# The higher top_k_retriever, the better (but also the slower) your answers. \n",
    "prediction = pipe.run(\n",
    "    query=\"What is self-supervised learning\", params={\"Retriever\": {\"top_k\": 100}, \"Reader\": {\"top_k\": 15}}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is self-supervised learning',\n",
       " 'no_ans_gap': 8.132203578948975,\n",
       " 'answers': [<Answer {'answer': 'a setting', 'type': 'extractive', 'score': 0.834409236907959, 'context': \"0 bits of information per sample. But self-supervised learning here is a setting. You, ideally, we don't know how to do this yet, but ideally you woul\", 'offsets_in_document': [{'start': 5337, 'end': 5346}], 'offsets_in_context': [{'start': 71, 'end': 80}], 'document_id': 'c2ae3bae6d60ff5442a064aa2c9bfbc3', 'meta': {'segments': {'id': 0, 'seek': 0, 'start': 0.0, 'end': 8.44, 'text': ' What is self-supervised learning and why is it the dark matter of intelligence?', 'tokens': [50364, 708, 307, 2698, 12, 48172, 24420, 2539, 293, 983, 307, 309, 264, 2877, 1871, 295, 7599, 30, 50786, 50786, 286, 603, 722, 538, 264, 2877, 1871, 644, 13, 50970, 50970, 821, 307, 2745, 257, 733, 295, 2539, 300, 6255, 293, 4882, 366, 884, 300, 321, 4362, 51340, 51340, 366, 406, 11408, 2175, 6108, 365, 8379, 420, 365, 7318, 13, 51528, 51528, 407, 264, 881, 3743, 11587, 281, 3479, 2539, 965, 366, 11, 420, 13480, 328, 2592, 286, 820, 51756, 51756], 'temperature': 0.0, 'avg_logprob': -0.153848555029892, 'compression_ratio': 1.607476635514018, 'no_speech_prob': 0.001889239880256}, 'language': 'en'}}>,\n",
       "  <Answer {'answer': 'one instance or one attempt at trying to reproduce this kind of learning', 'type': 'extractive', 'score': 0.5320419669151306, 'context': \"chines? So self-supervised learning is one instance or one attempt at trying to reproduce this kind of learning. Okay. So you're looking at just obser\", 'offsets_in_document': [{'start': 2129, 'end': 2201}], 'offsets_in_context': [{'start': 39, 'end': 111}], 'document_id': 'c2ae3bae6d60ff5442a064aa2c9bfbc3', 'meta': {'segments': {'id': 0, 'seek': 0, 'start': 0.0, 'end': 8.44, 'text': ' What is self-supervised learning and why is it the dark matter of intelligence?', 'tokens': [50364, 708, 307, 2698, 12, 48172, 24420, 2539, 293, 983, 307, 309, 264, 2877, 1871, 295, 7599, 30, 50786, 50786, 286, 603, 722, 538, 264, 2877, 1871, 644, 13, 50970, 50970, 821, 307, 2745, 257, 733, 295, 2539, 300, 6255, 293, 4882, 366, 884, 300, 321, 4362, 51340, 51340, 366, 406, 11408, 2175, 6108, 365, 8379, 420, 365, 7318, 13, 51528, 51528, 407, 264, 881, 3743, 11587, 281, 3479, 2539, 965, 366, 11, 420, 13480, 328, 2592, 286, 820, 51756, 51756], 'temperature': 0.0, 'avg_logprob': -0.153848555029892, 'compression_ratio': 1.607476635514018, 'no_speech_prob': 0.001889239880256}, 'language': 'en'}}>,\n",
       "  <Answer {'answer': 'some source of truth being told to it by somebody', 'type': 'extractive', 'score': 0.27946487069129944, 'context': 'ter. So self-supervised learning still has to have some source of truth being told to it by somebody. So you have to figure out a way without human as', 'offsets_in_document': [{'start': 3853, 'end': 3902}], 'offsets_in_context': [{'start': 51, 'end': 100}], 'document_id': 'c2ae3bae6d60ff5442a064aa2c9bfbc3', 'meta': {'segments': {'id': 0, 'seek': 0, 'start': 0.0, 'end': 8.44, 'text': ' What is self-supervised learning and why is it the dark matter of intelligence?', 'tokens': [50364, 708, 307, 2698, 12, 48172, 24420, 2539, 293, 983, 307, 309, 264, 2877, 1871, 295, 7599, 30, 50786, 50786, 286, 603, 722, 538, 264, 2877, 1871, 644, 13, 50970, 50970, 821, 307, 2745, 257, 733, 295, 2539, 300, 6255, 293, 4882, 366, 884, 300, 321, 4362, 51340, 51340, 366, 406, 11408, 2175, 6108, 365, 8379, 420, 365, 7318, 13, 51528, 51528, 407, 264, 881, 3743, 11587, 281, 3479, 2539, 965, 366, 11, 420, 13480, 328, 2592, 286, 820, 51756, 51756], 'temperature': 0.0, 'avg_logprob': -0.153848555029892, 'compression_ratio': 1.607476635514018, 'no_speech_prob': 0.001889239880256}, 'language': 'en'}}>],\n",
       " 'documents': [<Document: {'content': \" What is self-supervised learning and why is it the dark matter of intelligence? I'll start by the dark matter part. There is obviously a kind of learning that humans and animals are doing that we currently are not reproducing properly with machines or with AI. So the most popular approaches to machine learning today are, or paradigms I should say, are supervised learning and reinforcement learning. And they are extremely inefficient. Supervised learning requires many samples for learning anything. And reinforcement learning requires a ridiculously large number of trials and errors for a system to learn anything. And that's why we don't have self-driving cars. That's a big leap from one to the other. So to solve difficult problems, you have to have a lot of human annotation for supervised learning to work. And to solve those difficult problems with reinforcement learning, you have to have some way to maybe simulate that problem such that you can do that large scale kind of learning that reinforcement learning requires. Right. So how is it that most teenagers can learn to drive a car in about 20 hours of practice? Whereas even with millions of hours of simulated practice, a self-driving car can't actually learn to drive itself properly. And so obviously we're missing something. And it's quite obvious for a lot of people that the immediate response you get from many people is, well, humans use their background knowledge to learn faster. And they're right. Now, how was that background knowledge acquired? And that's the big question. So now you have to ask, how do babies in the first few months of life learn how the world works? Mostly by observation because they can hardly act in the world. And they learn an enormous amount of background knowledge about the world. That may be the basis of what we call common sense. This type of learning is not learning a task. It's not being reinforced for anything. It's just observing the world and figuring out how it works. Learning world models, learning world models. How do we do this? And how do we reproduce this in machines? So self-supervised learning is one instance or one attempt at trying to reproduce this kind of learning. Okay. So you're looking at just observation. So not even the interacting part of a child. It's just sitting there watching mom and dad walk around, pick up stuff, all of that. That's what we mean by background knowledge. Perhaps. Watching mom and dad just watching the world go by. Just having eyes open or having eyes closed or the very act of opening and closing eyes that the world appears and disappears. All that basic information. And you're saying in order to learn to drive, like the reason humans are able to learn to drive quickly, some faster than others, is because of the background knowledge. They were able to watch cars operate in the world in the many years leading up to it, the physics of basics, objects, and all that kind of stuff. That's right. I mean, the basic physics of objects, you don't even need to know how a car works, right? Because that you can learn fairly quickly. I mean, the example I use very often is you're driving next to a cliff and you know in advance because of your understanding of intuitive physics that if you turn the wheel to the right, the car will veer to the right, will run off the cliff, fall off the cliff, and nothing good will come out of this, right? But if you are a sort of tabularized reinforcement learning system that doesn't have a model of the world, you have to repeat falling off this cliff thousands of times before you figure out it's a bad idea. And then a few more thousand times before you figure out how to not do it. And then a few more million times before you figure out how to not do it in every situation you ever encounter. So self-supervised learning still has to have some source of truth being told to it by somebody. So you have to figure out a way without human assistance or without significant amount of human assistance to get that truth from the world. So the mystery there is how much signal is there? How much truth is there that the world gives you? Whether it's the human world, like you watch YouTube or something like that, or it's the more natural world. So how much signal is there? So here's the trick, there is way more signal in sort of a self-supervised setting than there is in either a supervised or reinforcement setting. And this is going to my analogy of the cake. Le cake, as someone has called it, where when you try to figure out how much information you ask the machine to predict and how much feedback you give the machine at every trial, in reinforcement learning, you give the machine a single scalar. You tell the machine you did good, you did bad. And you only tell this to the machine once in a while. When I say you, it could be the universe telling the machine, right? But it's just one scalar. So as a consequence, you cannot possibly learn something very complicated without many, many, many trials where you get many, many feedbacks of this type. Supervised learning, you give a few bits to the machine at every sample. And say you're training a system on recognizing images on ImageNet, there is 1,000 categories, that's a little less than 10 bits of information per sample. But self-supervised learning here is a setting. You, ideally, we don't know how to do this yet, but ideally you would show a machine a segment of video and then stop the video and ask the machine to predict what's going to happen next. So you let the machine predict and then you let time go by and show the machine what actually happened, and hope the machine will learn to do a better job at predicting next time around. There's a huge amount of information you give the machine because it's an entire video clip of the future after the video clip you fed it in the first place. So both for language and for vision, there's a subtle, seemingly trivial construction, but maybe that's representative of what is required to create intelligence, which is filling the gap. So it sounds dumb, but it is possible you can solve all of intelligence in this way. Just for both language, just give a sentence and continue it. Or give a sentence and there's a gap in it, some words blanked out and you fill in what words go there. For vision, you give a sequence of images and predict what's going to happen next, or you fill in what happened in between. Do you think it's possible that formulation alone as a signal for self-supervised learning can solve intelligence for vision and language? I think that's our best shot at the moment. So whether this will take us all the way to human level intelligence or something, or just cat level intelligence is not clear, but among all the possible approaches that people have proposed, I think it's our best shot. So I think this idea of an intelligent system filling in the blanks, either predicting the future, inferring the past, filling in missing information. I'm currently filling the blank of what is behind your head and what your head looks like from the back, because I have basic knowledge about how humans are made. And I don't know what you're going to say, at which point you're going to speak, whether you're going to move your head this way or that way, which way you're going to look. But I know you're not going to just dematerialize and reappear three meters down the hall, because I know what's possible and what's impossible according to intuitive physics. You have a model of what's possible and what's impossible, and then you'd be very surprised if it happens and then you'll have to reconstruct your model. Right. So that's the model of the world. It's what tells you what fills in the blanks. So given your partial information about the state of the world, given by your perception, your model of the world fills in the missing information, and that includes predicting the future, re-predicting the past, filling in things you don't immediately perceive. And that doesn't have to be purely generic vision or visual information or generic language you can go to specifics like predicting what control decision you make when you're driving in a lane. You have a sequence of images from a vehicle, and then you have information if you record it on video where the car ended up going. So you can go back in time and predict where the car went based on the visual information. That's very specific, domain specific. Right, but the question is whether we can come up with sort of a generic method for training machines to do this kind of prediction or filling in the blanks. So right now, this type of approach has been unbelievably successful in the context of natural language processing. Every modern natural language processing is pre-trained in self-supervised manner to fill in the blanks. You show it a sequence of words, you remove 10% of them, and then you train some gigantic neural net to predict the words that are missing. And once you've pre-trained that network, you can use the internal representation learned by it as input to something that you train supervised or whatever. That's been incredibly successful. Not so successful in images, although it's making progress. And it's based on sort of manual data augmentation. We can go into this later. But what has not been successful yet is training from video. So getting a machine to learn to represent the visual world, for example, by just watching video. We succeeded in doing this.\", 'content_type': 'text', 'score': 0.7794099152088165, 'meta': {'segments': {'id': 0, 'seek': 0, 'start': 0.0, 'end': 8.44, 'text': ' What is self-supervised learning and why is it the dark matter of intelligence?', 'tokens': [50364, 708, 307, 2698, 12, 48172, 24420, 2539, 293, 983, 307, 309, 264, 2877, 1871, 295, 7599, 30, 50786, 50786, 286, 603, 722, 538, 264, 2877, 1871, 644, 13, 50970, 50970, 821, 307, 2745, 257, 733, 295, 2539, 300, 6255, 293, 4882, 366, 884, 300, 321, 4362, 51340, 51340, 366, 406, 11408, 2175, 6108, 365, 8379, 420, 365, 7318, 13, 51528, 51528, 407, 264, 881, 3743, 11587, 281, 3479, 2539, 965, 366, 11, 420, 13480, 328, 2592, 286, 820, 51756, 51756], 'temperature': 0.0, 'avg_logprob': -0.153848555029892, 'compression_ratio': 1.607476635514018, 'no_speech_prob': 0.001889239880256}, 'language': 'en'}, 'embedding': None, 'id': 'c2ae3bae6d60ff5442a064aa2c9bfbc3'}>],\n",
       " 'root_node': 'Query',\n",
       " 'params': {'Retriever': {'top_k': 100}, 'Reader': {'top_k': 15}},\n",
       " 'node_id': 'Reader'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO find timestamp for answer (approximate through start tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('3.7.3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5789f8957a6146cbcff8af737daa2808d842dd4a2072828d297a928ebcd7d24d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
