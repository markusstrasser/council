{
  "https://www.youtube.com/watch?v=DokLw1tILlw": {
    "current_person": "Yann LeCun",
    "video_id": "DokLw1tILlw",
    "url": "https://www.youtube.com/watch?v=DokLw1tILlw",
    "title": "Yann LeCun: \"A Path Towards Autonomous AI\", Baidu 2022-02-22",
    "description": "Technical talk by Yann LeCun:\n\"A Path Towards Autonomous AI\"\nHosted virtually by Baidu on 2022-02-22.\n\nVideo of Q&A sessions here: https://youtu.be/Qgh2IU_fRMs\n \nTL;DR: \n- autonomous AI requires predictive world models\n- world models must be able to perform multimodal predictions\n- solution: Joint Embedding Predictive Architecture (JEPA)\n- JEPA makes prediction in representation space, and can choose to ignore irrelevant or hard-to-predict details.\n- JEPA can be trained non-contrastively by (1) making the representations of input maximally informative, (2) making the representations predictable from each other, (3) regularizing latent variables necessary for prediction.\n- JEPAs can be stacked to make long-term/long-range predictions in more abstract representation spaces.\n- Hierarchical JEPAs can be used for hierarchical planning.\n\nExplanatory blog post: https://ai.facebook.com/blog/yann-lecun-advances-in-ai-research/\n\nTopics: \n- How to get machines to learn like humans and animals?\n- Challenges in AI: self-supervised learning, reasoning, hierarchical planning\n- Learning models of the world\n- architecture for autonomous AI: world model, cost, actor, perception, configurator, short-term memory.\n- perception-action cycle: Mode-1 (reactive) and Mode-2 (planning)\n- Intrinsic Cost and Trainable Cost modules\n- building and training a world model\n- self-supervised learning (SSL)\n- Energy-Based Models: \n- contrastive and regularized training methods\n- EBM architectures: Joint Embedding Predictive Architecture (JEPA)\n- contrastive methods for training JEPA (bad)\n- regularized (non-contrastive) methods for training JEPA (good)\n- VICReg: Variance Invariance Covariance Regularization\n- hierarchical JEPA for world models.\n- hierarchical planning under uncertainty with hierarchical JEPA",
    "author": "Yann LeCun",
    "keywords": [],
    "channel_url": "https://www.youtube.com/channel/UCMU7l2bIv6MXlgJR3-E33Dw",
    "length": 3739,
    "views": 34801,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=VRzvpV9DZ8Y": {
    "current_person": "Yann LeCun",
    "video_id": "VRzvpV9DZ8Y",
    "url": "https://www.youtube.com/watch?v=VRzvpV9DZ8Y",
    "title": "Yann LeCun: From Machine Learning to Autonomous Intelligence",
    "description": "EECS Colloquium \nWednesday, September 27, 2022\nBanatao Auditorium, 310 Sutardja Dai Hall\n4-5p\n\nCaption available upon request",
    "author": "UC Berkeley EECS Events",
    "keywords": [],
    "channel_url": "https://www.youtube.com/channel/UCYN8R_LpgxCkeVTO_YdVJoA",
    "length": 4085,
    "views": 67744,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=a0-mULz6nhI": {
    "current_person": "Yann LeCun",
    "video_id": "a0-mULz6nhI",
    "url": "https://www.youtube.com/watch?v=a0-mULz6nhI",
    "title": "Yann LeCun - The Present and Future of Artificial Intelligence",
    "description": "Yann LeCun (Meta AI & New York University)",
    "author": "Institut des Hautes \u00c9tudes Scientifiques (IH\u00c9S)",
    "keywords": [
      "Artificial Intelligence",
      "Computer Science",
      "General Public",
      "Research talk",
      "Standard"
    ],
    "channel_url": "https://www.youtube.com/channel/UC4R1IsRVKs_qlWKTm9pT82Q",
    "length": 5907,
    "views": 7801,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=9r2lqrtQXFQ": {
    "current_person": "Yann LeCun",
    "video_id": "9r2lqrtQXFQ",
    "url": "https://www.youtube.com/watch?v=9r2lqrtQXFQ",
    "title": "Yann LeCun on the future of deep learning hardware",
    "description": "The progress of AI research will be closely tied to innovations in hardware.\nFacebook's Chief AI Scientist Yann LeCun describes how advances in deep learning (DL) research will influence the hardware architecture of the future.\nLeCun says the demand for DL-specific hardware will likely only increase. New architectural concepts such as dynamic networks, associative-memory structures, and sparse activations will affect the type of hardware architecture that will be required in the future.",
    "author": "Meta AI",
    "keywords": [],
    "channel_url": "https://www.youtube.com/channel/UC5qxlwEKM7-5YZudb24l0bg",
    "length": 383,
    "views": 13129,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=gG5NCkMerHU": {
    "current_person": "Yann LeCun",
    "video_id": "gG5NCkMerHU",
    "url": "https://www.youtube.com/watch?v=gG5NCkMerHU",
    "title": "The Epistemology of Deep Learning - Yann LeCun",
    "description": "Deep Learning: Alchemy or Science?\n\nTopic: The Epistemology of Deep Learning\nSpeaker: Yann LeCun\nAffiliation: Facebook AI Research/New York University\nDate: February 22, 2019\n\nFor more video please visit http://video.ias.edu",
    "author": "Institute for Advanced Study",
    "keywords": [],
    "channel_url": "https://www.youtube.com/channel/UC8aRaZ6_0weiS50pvCmo0pw",
    "length": 4038,
    "views": 28468,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=0bMe_vCZo30": {
    "current_person": "Yann LeCun",
    "video_id": "0bMe_vCZo30",
    "url": "https://www.youtube.com/watch?v=0bMe_vCZo30",
    "title": "Week 1 \u2013 Lecture: History, motivation, and evolution of Deep Learning",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Yann LeCun\nWeek 1: http://bit.ly/pDL-en-01\n\n0:00:00 \u2013 Week 1 \u2013 Lecture\n\nLECTURE Part A: http://bit.ly/pDL-en-01-1\nWe discuss the motivation behind deep learning. We begin with the history and inspiration of deep learning. Then we discuss the history of pattern recognition and introduce gradient descent and its computation by backpropagation. Finally, we discuss the hierarchical representation of the visual cortex.\n0:03:37 \u2013 Inspiration of Deep Learning and Its History, Supervised Learning\n0:24:21 \u2013 History of Pattern Recognition and Introduction to Gradient Descent\n0:38:56 \u2013 Computing Gradients by Backpropagation, Hierarchical Representation of the Visual Cortex\n\nLECTURE Part B: http://bit.ly/pDL-en-01-2\nWe first discuss the evolution of CNNs, from Fukushima to LeCun to Alexnet. We then discuss some applications of CNN's, such as image segmentation, autonomous vehicles, and medical image analysis. We discuss the hierarchical nature of deep networks and the attributes of deep networks that make them advantageous. We conclude with a discussion of generating and learning features/representations.\n0:49:25 \u2013 Evolution of CNNs\n1:05:55 \u2013 Deep Learning & Feature Extraction\n1:19:27 \u2013 Learning Representations",
    "author": "Alfredo Canziani",
    "keywords": [
      "Deep Learning",
      "Yann LeCun",
      "NYU",
      "PyTorch"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 5936,
    "views": 187830,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=5_qrxVq1kvc": {
    "current_person": "Yann LeCun",
    "video_id": "5_qrxVq1kvc",
    "url": "https://www.youtube.com/watch?v=5_qrxVq1kvc",
    "title": "Week 1 \u2013 Practicum: Classification, linear algebra, and visualisation",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Alfredo Canziani\nWeek 1: http://bit.ly/pDL-en-01\n\n0:00:00 \u2013 Week 1 \u2013 Practicum\n\nPRACTICUM: http://bit.ly/pDL-en-01-3\nWe discuss the motivation for applying transformations to data points visualized in space. We talk about Linear Algebra and the application of linear and non-linear transformations. We discuss the use of visualization to understand the function and effects of these transformations. We walk through examples in a Jupyter Notebook and conclude with a discussion of functions represented by Neural Networks. \n0:03:53 \u2013 Problem Motivation and Linear Algebra\n0:13:18 \u2013 Data Visualization - Separating Points by Color Using a Network\n0:28:13 \u2013 Random Projections - Jupyter Notebook",
    "author": "Alfredo Canziani",
    "keywords": [
      "Deep Learning",
      "Yann LeCun",
      "NYU",
      "PyTorch"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 3150,
    "views": 46815,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=d9vdh3b787Y": {
    "current_person": "Yann LeCun",
    "video_id": "d9vdh3b787Y",
    "url": "https://www.youtube.com/watch?v=d9vdh3b787Y",
    "title": "Week 2 \u2013 Lecture: Stochastic gradient descent and backpropagation",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Yann LeCun\nWeek 2: http://bit.ly/pDL-en-02\n\n0:00:00 \u2013 Week 2 \u2013 Lecture\n\nLECTURE Part A: http://bit.ly/pDL-en-02-1\nWe start by understanding what parameterized models are and then discuss what a loss function is. We then look at gradient-based methods and how it's used in the backpropagation algorithm in a traditional neural network. We conclude this section by learning how to implement a neural network in PyTorch followed by a discussion on a more generalized form of backpropagation.\n0:00:29 \u2013 Gradient Descent Optimization Algorithm\n0:17:16 \u2013 Advantages of SGD, Backpropagation for Traditional Neural Net\n0:38:08 \u2013 PyTorch implementation of Neural Network and a Generalized Backprop Algorithm\n\nLECTURE Part B: http://bit.ly/pDL-en-02-2\nWe begin with a concrete example of backpropagation and discuss the dimensions of Jacobian matrices. We then look at various basic neural net modules and compute their gradients, followed by a brief discussion on softmax and logsoftmax. The other topic of discussion in this part is Practical Tricks for Backpropagation.\n0:49:49 \u2013 Basic Modules - LogSoftMax\n1:05:53 \u2013 Practical Tricks for Backpropagation\n1:21:31 \u2013 Computing gradients for NN modules and Practical tricks for Back Propagation",
    "author": "Alfredo Canziani",
    "keywords": [
      "Deep Learning",
      "Yann LeCun",
      "NYU",
      "PyTorch",
      "stochastic gradient descent",
      "SGD",
      "backpropagation",
      "backprop"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 6196,
    "views": 43408,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=WAn6lip5oWk": {
    "current_person": "Yann LeCun",
    "video_id": "WAn6lip5oWk",
    "url": "https://www.youtube.com/watch?v=WAn6lip5oWk",
    "title": "Week 2 \u2013 Practicum: Training a neural network",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Alfredo Canziani\nWeek 2: http://bit.ly/pDL-en-02\n\n0:00:00 \u2013 Week 2 \u2013 Practicum\n\nPRACTICUM: http://bit.ly/pDL-en-02-3\nWe give a brief introduction to supervised learning using artificial neural networks. We expound on the problem formulation and conventions of data used to train these networks. We also discuss how to train a neural network for multi class classification, and how to perform inference once the network is trained.\n0:02:30 \u2013 An Introduction to Supervised Learning using ANNs\n0:13:42 \u2013 Neural Network Inference and Neural Network Training (I)\n0:36:28 \u2013 Neural Network Training (II) and a Classification Example",
    "author": "Alfredo Canziani",
    "keywords": [
      "Deep Learning",
      "Yann LeCun",
      "NYU",
      "PyTorch",
      "Neural Network",
      "Classification",
      "Regression"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 3420,
    "views": 20667,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=FW5gFiJb-ig": {
    "current_person": "Yann LeCun",
    "video_id": "FW5gFiJb-ig",
    "url": "https://www.youtube.com/watch?v=FW5gFiJb-ig",
    "title": "Week 3 \u2013 Lecture: Convolutional neural networks",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Yann LeCun\nWeek 3: http://bit.ly/pDL-en-03\n\n0:00:00 \u2013 Week 3 \u2013 Lecture\n\nLECTURE Part A: http://bit.ly/pDL-en-03-1\nWe first see a visualization of 6-layer neural network. Next we begin to the topic of Convolution and Convolution Neural Networks (CNN). We review several types of parameter transformation in CNN and introduce the idea of a kernel, used to learn features in a hierarchical manner, and to classify our input data is the basic idea of a CNN.\n0:00:05 \u2013 Visualization of Neural Networks\n0:07:57 \u2013 Parameter Transformations, the Convolution Operator, and Deep Convolutional Neural Networks\n0:37:34 \u2013 Inspirations from Biology\n\nLECTURE Part B: http://bit.ly/pDL-en-03-2\nWe give an introduction on CNN evolutions. We discuss in detail on architectures of CNN with modern implementation of LeNet5, exemplified by the task of digit recognition on MNIST. Based on its design principles, we expand on the advantages of CNN which fully explores compositionality, stationarity, and locality features of natural images.\n0:49:09 \u2013 The First ConvNets\n1:03:50 \u2013 LeNet5 and digit recognition\n1:20:27 \u2013 Feature Binding and What are ConvNets Good for?",
    "author": "Alfredo Canziani",
    "keywords": [
      "Deep Learning",
      "Yann LeCun",
      "NYU",
      "PyTorch",
      "convolutional neural network",
      "CNN",
      "ConvNet",
      "biology inspired"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 5895,
    "views": 23091,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=kwPWpVverkw": {
    "current_person": "Yann LeCun",
    "video_id": "kwPWpVverkw",
    "url": "https://www.youtube.com/watch?v=kwPWpVverkw",
    "title": "Week 3 \u2013 Practicum: Natural signals properties and CNNs",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Alfredo Canziani\nWeek 3: http://bit.ly/pDL-en-03\n\n0:00:00 \u2013 Week 3 \u2013 Practicum\n\nPRACTICUM: http://bit.ly/pDL-en-03-3\nProperties of signals that are most relevant to CNNs are discussed, namely:- Locality, Stationarity, and Compositionality. How a kernel exploits these features by using Sparsity, Weight sharing and Stacking of layers is explored next, along with the concepts of padding and pooling. A performance comparison between FCN and CNN for different data modalities was also made.\n0:00:26 \u2013 Properties of natural signals\n0:17:54 \u2013 Exploiting Properties of Natural Signals to Build Standard Spatial CNN\n0:39:36 \u2013 Pooling and Covnet - Jupyter Notebook",
    "author": "Alfredo Canziani",
    "keywords": [
      "Deep Learning",
      "Yann LeCun",
      "NYU",
      "PyTorch",
      "convolutional neural networks",
      "CNN",
      "ConvNet",
      "locality",
      "stationarity",
      "compositionality",
      "sparsity",
      "parameter sharing",
      "hierarchical",
      "receptive field",
      "kernels",
      "padding",
      "pooling"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 2901,
    "views": 11235,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=OrBEon3VlQg": {
    "current_person": "Yann LeCun",
    "video_id": "OrBEon3VlQg",
    "url": "https://www.youtube.com/watch?v=OrBEon3VlQg",
    "title": "Week 4 \u2013 Practicum: Listening to convolutions",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Alfredo Canziani\nWeek 4: http://bit.ly/pDL-en-04\n\n0:00:00 \u2013 Week 4 \u2013 Practicum\n\nPRACTICUM: http://bit.ly/pDL-en-04-3\nWe start with a brief review of linear algebra and then extend the topic to convolutions using audio data as an example. key concepts like locality, stationarity and Toeplitz matrix are discussed. Then we give a live demo of convolution performance in pitch analysis. Finally, there is a short digression about the dimensionality of different data.\n0:01:08 \u2013 Linear Algebra Review\n0:17:10 \u2013 Extend Linear Algebra to Convolution\n0:28:29 \u2013 Listening to Convolutions",
    "author": "Alfredo Canziani",
    "keywords": [
      "Deep Learning",
      "Yann LeCun",
      "NYU",
      "PyTorch",
      "convolutional neural networks",
      "CNN",
      "ConvNet",
      "kernels",
      "linear algebra",
      "prank"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 3062,
    "views": 10354,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=--NZb480zlg": {
    "current_person": "Yann LeCun",
    "video_id": "--NZb480zlg",
    "url": "https://www.youtube.com/watch?v=--NZb480zlg",
    "title": "Week 5 \u2013 Lecture: Optimisation",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Aaron DeFazio\nWeek 5: http://bit.ly/pDL-en-05\n\n0:00:00 \u2013 Week 5 \u2013 Lecture\n\nLECTURE Part A: http://bit.ly/pDL-en-05-1\nWe begin by introducing Gradient Descent. We discuss the intuition and also talk about how step sizes play an important role in reaching the solution. Then we move on to SGD and its performance in comparison to Full Batch GD. Finally we talk about Momentum Updates, specifically the two update rules, the intuition behind momentum and its effect on convergence.\n0:01:28 \u2013 Gradient Descent\n0:14:58 \u2013 Stochastic Gradient Descent\n0:27:52 \u2013 Momentum\n\nLECTURE Part B: http://bit.ly/pDL-en-05-2\nWe discuss adaptive methods for SGD such as RMSprop and ADAM. We also talk about normalization layers and their effects on the neural network training process. Finally, we discuss a real-world example of neural nets being used in industry to make MRI scans faster and more efficient.\n0:44:35 \u2013 Adaptive Methods\n1:05:07 \u2013 Normalization Layers\n1:20:17 \u2013 The Death of Optimization",
    "author": "Alfredo Canziani",
    "keywords": [
      "Deep Learning",
      "Yann LeCun",
      "NYU",
      "PyTorch",
      "optimisation",
      "SGD",
      "stochastic gradient descent"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 5345,
    "views": 14050,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=ycbMGyCPzvE": {
    "current_person": "Yann LeCun",
    "video_id": "ycbMGyCPzvE",
    "url": "https://www.youtube.com/watch?v=ycbMGyCPzvE",
    "title": "Week 6 \u2013 Lecture: CNN applications, RNN, and attention",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Yann LeCun\nWeek 6: http://bit.ly/pDL-en-06\n\n0:00:00 \u2013 Week 6 \u2013 Lecture\n\nLECTURE Part A: http://bit.ly/pDL-en-06-1\nWe discussed three applications of convolutional neural networks. We started with digit recognition and the application to a 5-digit zip code recognition. In object detection, we talk about how to use multi-scale architecture in a face detection setting. Lastly, we saw how ConvNets are used in semantic segmentation tasks with concrete examples in a robotic vision system and object segmentation in an urban environment.\n0:00:43 \u2013 Word-level training with minimal supervision\n0:20:41 \u2013 Face Detection and Semantic Segmentation\n0:27:49 \u2013 ConvNet for Long Range Adaptive Robot Vision and Scene Parsing\n\nLECTURE Part B: http://bit.ly/pDL-en-06-2\nWe examine Recurrent Neural Networks, their problems, and common techniques for mitigating these issues. We then review a variety of modules developed to resolve RNN model issues including Attention, GRUs (Gated Recurrent Unit), LSTMs (Long Short-Term Memory), and Seq2Seq.\n0:43:40 \u2013 Recurrent Neural Networks and Attention Mechanisms\n0:59:09 \u2013 GRUs, LSTMs, and Seq2Seq Models\n1:16:15 \u2013 Memory Networks",
    "author": "Alfredo Canziani",
    "keywords": [
      "CNN",
      "Yann LeCun",
      "Deep Learning",
      "RNN",
      "LSTM",
      "Attention",
      "PyTorch",
      "NYU"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 5328,
    "views": 14852,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=tVwV14YkbYs": {
    "current_person": "Yann LeCun",
    "video_id": "tVwV14YkbYs",
    "url": "https://www.youtube.com/watch?v=tVwV14YkbYs",
    "title": "Week 7 \u2013 Lecture: Energy based models and self-supervised learning",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Yann LeCun\nWeek 7: http://bit.ly/pDL-en-07\n\n0:00:00 \u2013 Week 7 \u2013 Lecture\n\nLECTURE Part A: http://bit.ly/pDL-en-07-1\nWe introduced the concept of the energy-based models and the intention for different approaches other than feed-forward networks. To solve the difficulty of the inference in EBM, latent variables are used to provide auxiliary information and enable multiple possible predictions. Finally, the EBM can generalize to probabilistic model with more flexible scoring functions.\n0:01:04 \u2013 Energy-based model concept\n0:15:04 \u2013 Latent-variable EBM: inference \n0:28:23 \u2013 EBM vs. probabilistic models\n\nLECTURE Part B: http://bit.ly/pDL-en-07-2\nWe discussed self-supervised learning, introduced how to train an Energy-based models, discussed Latent Variable EBM, specifically with an explained K-means example. We also introduced Contrastive Methods, explained a denoising autoencoder with a topographic map, the training process, and how it can be used, followed by an introduction to BERT. Finally, we talked about Contrastive Divergence, also explained using a topographic map.\n0:44:43 \u2013 Self-supervised learning\n1:05:57 \u2013 Training an Energy-Based Model\n1:19:27 \u2013 Latent Variable EBM, K-means example, Contrastive Methods",
    "author": "Alfredo Canziani",
    "keywords": [
      "Yann LeCun",
      "Deep Learning",
      "PyTorch",
      "NYU",
      "EBM",
      "Energy Based Models",
      "SSL",
      "Semi Supervised Learning",
      "LV",
      "Latent Variable"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 5839,
    "views": 25366,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=bggWQ14DD9M": {
    "current_person": "Yann LeCun",
    "video_id": "bggWQ14DD9M",
    "url": "https://www.youtube.com/watch?v=bggWQ14DD9M",
    "title": "Week 7 \u2013 Practicum: Under- and over-complete autoencoders",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Alfredo Canziani\nWeek 7: http://bit.ly/pDL-en-07\n\n0:00:00 \u2013 Week 7 \u2013 Practicum\n\nPRACTICUM: http://bit.ly/pDL-en-07-3\nWe discussed some applications of Autoencoders and talked about why we want to use them. Then we talked about different architectures of Autoencoders (under or over complete hidden layer), how to avoid overfitting issues and the loss functions we should use. Finally we implemented a standard Autoencoder and a denoising Autoencoder.\n0:00:55 \u2013 Application of Autoencoders\n0:14:39 \u2013 Architecture and loss function in Autoencoders\n0:41:31 \u2013 Notebook example for different types of Autoencoders",
    "author": "Alfredo Canziani",
    "keywords": [
      "Deep Learning",
      "Yann LeCun",
      "autoencoder",
      "denoising autoencoder",
      "contractive autoencoder",
      "under-complete",
      "over-complete",
      "inpainting",
      "generative",
      "restoration",
      "PyTorch"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 3304,
    "views": 8582,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=ZaVP2SY23nc": {
    "current_person": "Yann LeCun",
    "video_id": "ZaVP2SY23nc",
    "url": "https://www.youtube.com/watch?v=ZaVP2SY23nc",
    "title": "Week 8 \u2013 Lecture: Contrastive methods and regularised latent variable models",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Yann LeCun\nWeek 8: http://bit.ly/pDL-en-08\n\n0:00:00 \u2013 Week 8 \u2013 Lecture\n\nLECTURE Part A: http://bit.ly/pDL-en-08-1\nIn this section, we focused on the introduction of contrastive methods in Energy-Based Models in several aspects. First, we discuss the advantage brought by applying contrastive methods in self-supervised learning. Second, we discussed the architecture of denoising autoencoders and their weakness in image reconstruction tasks. We also talked about other contrastive methods, like contrastive divergence and persistent contrastive divergence.\n0:00:05 \u2013 Recap on EBM and Characteristics of Different Contrastive Methods\n0:10:13 \u2013 Contrastive Methods in Self-Supervised Learning\n0:23:04 \u2013 Denoising Autoencoder and other Contrastive methods\n\nLECTURE Part B: http://bit.ly/pDL-en-08-2\nIn this section, we discussed regularized latent variable EBMs in detail covering concepts of conditional and unconditional versions of these models. We then discussed the algorithms of ISTA, FISTA and LISTA and look at examples of sparse coding and filters learned from convolutional sparse encoders. Finally we talked about Variational Auto-Encoders and the underlying concepts involved.\n0:37:13 \u2013 Overview of Regularized Latent Variable Energy Based Models and Sparse Coding\n1:07:46 \u2013 Convolutional Sparse Auto-Encoders\n1:12:51 \u2013 Variational Auto-Encoders",
    "author": "Alfredo Canziani",
    "keywords": [
      "Yann LeCun",
      "Deep Learning",
      "PyTorch",
      "NYU",
      "EBM",
      "Energy Based Models",
      "SSL",
      "Semi Supervised Learning",
      "LV",
      "Latent Variable",
      "contrastive methods",
      "Regularised Latent Variables"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 5966,
    "views": 11872,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=7Rb4s9wNOmc": {
    "current_person": "Yann LeCun",
    "video_id": "7Rb4s9wNOmc",
    "url": "https://www.youtube.com/watch?v=7Rb4s9wNOmc",
    "title": "Week 8 \u2013 Practicum: Variational autoencoders",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Alfredo Canziani\nWeek 8: http://bit.ly/pDL-en-08\n\n0:00:00 \u2013 Week 8 \u2013 Practicum\n\nPRACTICUM: http://bit.ly/pDL-en-08-3\nIn this section, we discussed a specific type of generative model called Variational Autoencoders and compared their functionalities and advantages over Classic Autoencoders. We explored the objective function of VAE in detail, understanding how it enforced some structure in the latent space. Finally, we implemented and trained a VAE on the MNIST dataset and used it to generate new samples.\n0:02:35 \u2013 Autoencoders (AEs) vs. variational autoencoders (VAEs)\n0:16:37 \u2013 Understanding the VAE objective function\n0:31:33 \u2013 Notebook example for variational autoencoder",
    "author": "Alfredo Canziani",
    "keywords": [
      "Deep Learning",
      "Yann LeCun",
      "autoencoder",
      "over-complete",
      "generative",
      "variational autoencoder",
      "posterior",
      "prior",
      "KL divergence",
      "relative entropy",
      "PyTorch"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 3485,
    "views": 18364,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=Pgct8PKV7iw": {
    "current_person": "Yann LeCun",
    "video_id": "Pgct8PKV7iw",
    "url": "https://www.youtube.com/watch?v=Pgct8PKV7iw",
    "title": "Week 9 \u2013 Lecture: Group sparsity, world model, and generative adversarial networks (GANs)",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Yann LeCun\nWeek 9: http://bit.ly/pDL-en-09\n\n0:00:00 \u2013 Week 9 \u2013 Lecture\n\nLECTURE Part A: http://bit.ly/pDL-en-09-1\nWe discussed discriminative recurrent sparse auto-encoders and group sparsity. The main idea was how to combine sparse coding with discriminative training. We went through how to structure a network with a recurrent autoencoder similar to LISTA and a decoder. Then we discussed how to use group sparsity to extract invariant features.\n0:00:35 \u2013 Discriminative Recurrent Sparse Auto-Encoder and Group Sparsity\n0:15:18 \u2013 AE With Group Sparsity: Questions and Clarification\n0:30:34 \u2013 Convolutional RELU with Group Sparsity\n\nLECTURE Part B: http://bit.ly/pDL-en-09-2\nIn this section, we talked about the World Models for autonomous control including the neural network architecture and training schema. Then, we discussed the difference between World Models and Reinforcement Learning (RL). Finally, we studied Generative Adversarial Networks (GANs) in terms of energy-based model with the contrastive method.\n0:42:06 \u2013 Learning World Models for Autonomous Control\n1:06:33 \u2013 Reinforcement Learning\n1:30:30 \u2013 Generative Adversarial Network",
    "author": "Alfredo Canziani",
    "keywords": [
      "Yann LeCun",
      "Deep Learning",
      "PyTorch",
      "NYU",
      "EBM",
      "Energy Based Models",
      "contrastive methods",
      "Regularised Latent Variables",
      "Generative Adversarial Network",
      "GAN",
      "World model",
      "control",
      "Reinforcement Learning",
      "RL",
      "sparsity"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 7104,
    "views": 8101,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=xYc11zyZ26M": {
    "current_person": "Yann LeCun",
    "video_id": "xYc11zyZ26M",
    "url": "https://www.youtube.com/watch?v=xYc11zyZ26M",
    "title": "Week 9 \u2013 Practicum: (Energy-based) Generative adversarial networks",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Alfredo Canziani\nWeek 9: http://bit.ly/pDL-en-09\n\n0:00:00 \u2013 Week 9 \u2013 Practicum\n\nPRACTICUM: http://bit.ly/pDL-en-09-3\nDuring this week\u2019s practicum, we explored Generative Adversarial Networks (GANs) and how they can produce realistic generative models. We then compared GANs with VAEs from week 8 to highlight key differences between two networks. Next, we discussed several model limitations of GANs. Finally, we looked at the source code for the PyTorch example Deep Convolutional Generative Adversarial Networks (DCGAN).\n0:00:57 \u2013 Intro to GANs\n0:30:44 \u2013 Difference between GANs and VAEs and major pitfalls in GANs\n0:48:31 \u2013 DCGAN source code",
    "author": "Alfredo Canziani",
    "keywords": [
      "Deep Learning",
      "Yann LeCun",
      "generative",
      "variational autoencoder",
      "PyTorch",
      "GAN",
      "generative adversarial network",
      "energy model",
      "EBGAN",
      "generator",
      "discriminator",
      "cost",
      "adversarial",
      "minmax",
      "NYU"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 4512,
    "views": 6064,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=0KeR6i1_56g": {
    "current_person": "Yann LeCun",
    "video_id": "0KeR6i1_56g",
    "url": "https://www.youtube.com/watch?v=0KeR6i1_56g",
    "title": "Week 10 \u2013 Lecture: Self-supervised learning (SSL) in computer vision (CV)",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Ishan Misra\nWeek 10: http://bit.ly/pDL-en-10\n\n0:00:00 \u2013 Week 10 \u2013 Lecture\n\nLECTURE Part A: http://bit.ly/pDL-en-10-1\nIn this section, we understand the motivation behind Self-Supervised Learning (SSL), define what it is and see some of its applications in NLP and Computer Vision. We understand how pretext tasks aid with SSL and see some example pretext tasks in images, videos and videos with sound. Finally, we try to get an intuition behind the representation learned by pretext tasks.\n0:01:15 \u2013 Challenges of supervised learning and how self supervised learning differs from supervised and unsupervised, with examples in NLP and Relative positions for vision\n0:12:39 \u2013 Examples of pretext tasks in images, videos and videos with sound\n0:40:26 \u2013 Understanding what the \"pretext\" task learns\n\nLECTURE Part B: http://bit.ly/pDL-en-10-2\nIn this section, we discuss the shortcomings of pretext tasks, define characteristics that make a good pretrained feature, and how we can achieve this using Clustering and Contrastive Learning. We then learn about ClusterFit, its steps and performance. We further dive into a specific simple framework for Contrastive Learning known as PIRL. We discuss its working as well as its evaluation in different contexts.\n1:01:50 \u2013 Generalization of pretext task and ClusterFit\n1:19:08 \u2013 Basic idea of PIRL\n1:38:09 \u2013 Evaluating PIRL on different tasks and questions",
    "author": "Alfredo Canziani",
    "keywords": [
      "Yann LeCun",
      "Deep Learning",
      "PyTorch",
      "NYU",
      "contrastive methods",
      "SSL",
      "unsupervised learning",
      "representation learning",
      "CV",
      "triplet loss",
      "pretext task"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 7242,
    "views": 15323,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=A3klBqEWR-I": {
    "current_person": "Yann LeCun",
    "video_id": "A3klBqEWR-I",
    "url": "https://www.youtube.com/watch?v=A3klBqEWR-I",
    "title": "Week 10 \u2013 Practicum: The Truck Backer-Upper",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Alfredo Canziani\nWeek 10: http://bit.ly/pDL-en-10\n\n0:00:00 \u2013 Week 10 \u2013 Practicum\n\nPRACTICUM: http://bit.ly/pDL-en-10-3\nDuring this week\u2019s practicum, we explore the Truck Backer-Upper (Nguyen & Widrow, \u201890). This problem shows how to solve an non-linear control problem using neural networks. We learn a model of a truck\u2019s kinematics, and optimize a controller through this learned model, finding that the controller is able to learn complex behaviors through purely observational data.\n0:00:59 \u2013 Set up and visualization of the self-learning problem \"The Truck Backer-Upper\"\n0:19:44 \u2013 Training the Neural-nets Model for Emulator and Controller\n0:38:48 \u2013 Understanding of the PyTorch code",
    "author": "Alfredo Canziani",
    "keywords": [
      "Deep Learning",
      "Yann LeCun",
      "PyTorch",
      "NYU",
      "model predictive control",
      "RL",
      "Reinforcement Learning",
      "controller",
      "policy",
      "world model",
      "predictive model",
      "forward model",
      "model based RL"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 3625,
    "views": 3890,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=bj1fh3BvqSU": {
    "current_person": "Yann LeCun",
    "video_id": "bj1fh3BvqSU",
    "url": "https://www.youtube.com/watch?v=bj1fh3BvqSU",
    "title": "Week 11 \u2013 Lecture: PyTorch activation and loss functions",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Yann LeCun\nWeek 11: http://bit.ly/pDL-en-11\n\n0:00:00 \u2013 Week 11 \u2013 Lecture\n\nLECTURE Part A: http://bit.ly/pDL-en-11-1\nIn this section, we discussed about the common activation functions in Pytorch. In particular, we compared activations with kink(s) versus smooth activations - the former is preferred in a deep neural network as the latter might suffer with gradient vanishing problem. We then learned about the common loss functions in Pytorch.\n0:00:15 \u2013 Activation Functions\n0:14:21 \u2013 Q&A of activation\n0:33:10 \u2013 Loss Functions (until AdaptiveLogSoftMax)\n\nLECTURE Part B: http://bit.ly/pDL-en-11-2\nIn this section, we continued to learn about loss functions - in particular, margin-based losses and their applications. We then discussed how to design a good loss function for EBMs as well as examples of well-known EBM loss functions. We gave particular attention to margin-based loss function here, as well as explaining the idea of \u201cmost offending incorrect answer.\n0:53:27 \u2013 Loss Functions (until CosineEmbeddingLoss)\n1:08:23 \u2013 Loss Functions and Loss Functions for Energy Based Models\n1:23:18 \u2013 Loss Functions for Energy Based Models",
    "author": "Alfredo Canziani",
    "keywords": [
      "Yann LeCun",
      "Deep Learning",
      "PyTorch",
      "NYU",
      "activation functions",
      "loss functions"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 6824,
    "views": 10548,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=6D4EWKJgNn0": {
    "current_person": "Yann LeCun",
    "video_id": "6D4EWKJgNn0",
    "url": "https://www.youtube.com/watch?v=6D4EWKJgNn0",
    "title": "Week 12 \u2013 Lecture: Deep Learning for Natural Language Processing (NLP)",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Mike Lewis\nWeek 12: http://bit.ly/pDL-en-12\n\n0:00:00 \u2013 Week 12 \u2013 Lecture\n\nLECTURE Part A: http://bit.ly/pDL-en-12-1\nIn this section we discuss the various architectures used in NLP applications, beginning with CNNs, RNNs, and eventually covering the state of-the art architecture, transformers. We then discuss the various modules that comprise transformers and how they make transformers advantageous for NLP tasks. Finally, we discuss tricks that allow transformers to be trained effectively.\n0:00:44 \u2013 Introduction to deep learning in NLP and language models\n0:13:48 \u2013 Transformer language model structure and intuition\n0:32:55 \u2013 Some tricks and facts of Transformer Language Models and decoding Language Models\n\nLECTURE Part B: http://bit.ly/pDL-en-12-2\nIn this section we introduce beam search as a middle ground between greedy decoding and exhaustive search. We consider the case of wanting to sample from the generative distribution (i.e. when generating text) and introduce \u201ctop-k\u201d sampling. Subsequently, we introduce sequence to sequence models (with a transformer variant) and back-translation. We then introduce unsupervised learning approaches for learning embeddings and discuss word2vec, GPT, and BERT.\n0:45:32 \u2013 Beam Search, Sampling and Text Generation\n1:03:31 \u2013 Back-translation, word2vec and BERT's\n1:22:43 \u2013 Pre-training for NLP and Next Steps",
    "author": "Alfredo Canziani",
    "keywords": [
      "Yann LeCun",
      "Deep Learning",
      "PyTorch",
      "NYU",
      "NLP",
      "Natural Language Processing",
      "BERT",
      "GPT",
      "GPT-2",
      "GPT-3",
      "transformer",
      "attention"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 6056,
    "views": 7807,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=f01J0Dri-6k": {
    "current_person": "Yann LeCun",
    "video_id": "f01J0Dri-6k",
    "url": "https://www.youtube.com/watch?v=f01J0Dri-6k",
    "title": "Week 12 \u2013 Practicum: Attention and the Transformer",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Alfredo Canziani\nWeek 12: http://bit.ly/pDL-en-12\n\n0:00:00 \u2013 Week 12 \u2013 Practicum\n\nPRACTICUM: http://bit.ly/pDL-en-12-3\nWe introduce attention, focusing on self-attention and its hidden layer representations of the inputs. Then, we introduce the key-value store paradigm and discuss how to represent queries, keys, and values as rotations of an input. Finally, we use attention to interpret the transformer architecture, taking a forward pass through a basic transformer, and comparing the encoder-decoder paradigm to sequential architectures.\n0:01:09 \u2013 Attention\n0:17:36 \u2013 Key-value store\n0:35:14 \u2013 Transformer and PyTorch implementation\n0:54:00 \u2013 Q&A",
    "author": "Alfredo Canziani",
    "keywords": [
      "Deep Learning",
      "Yann LeCun",
      "PyTorch",
      "NYU",
      "Neural Machine Translation",
      "NMT",
      "Natural Language Processing",
      "NLP",
      "attention",
      "transformer",
      "BERT",
      "OpenAI",
      "GTP2",
      "GTP3",
      "self-attention",
      "cross-attention",
      "encoder-decoder",
      "RNN",
      "autoregressive",
      "softmax"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 4682,
    "views": 9696,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=Iiv9R6BjxHM": {
    "current_person": "Yann LeCun",
    "video_id": "Iiv9R6BjxHM",
    "url": "https://www.youtube.com/watch?v=Iiv9R6BjxHM",
    "title": "Week 13 \u2013 Lecture: Graph Convolutional Networks (GCNs)",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Xavier Bresson\nWeek 13: http://bit.ly/pDL-en-13\n\n0:00:00 \u2013 Week 13 \u2013 Lecture\n\nLECTURE Part A: http://bit.ly/pDL-en-13-1\nIn this section, we discuss the architecture and convolution of traditional convolutional neural networks. Then we extend to the graph domain. We understand the characteristics of graph and define the graph convolution. Finally, we introduce spectral graph convolutional neural networks and discuss how to perform spectral convolution.\n0:00:50 \u2013 Architecture of Traditional ConvNets\n0:13:11 \u2013 Convolution of Traditional ConvNets\n0:25:29 \u2013 Spectral Convolution\n\nLECTURE Part B: http://bit.ly/pDL-en-13-2\nThis section covers the complete spectrum of Graph Convolutional Networks (GCNs), starting with the implementation of Spectral Convolution through Spectral Networks. It then provides insights on applicability of the other convolutional definition of Template Matching to graphs, leading to Spatial networks. Various architectures employing the two approaches are detailed out with their corresponding pros & cons, experiments, benchmarks and applications.\n0:44:30 \u2013 Spectral GCNs\n1:06:04 \u2013 Template Matching, Isotropic GCNs and Benchmarking GNNs\n1:33:06 \u2013 Anisotropic GCNs and Conclusion",
    "author": "Alfredo Canziani",
    "keywords": [
      "Yann LeCun",
      "Deep Learning",
      "PyTorch",
      "NYU",
      "transformer",
      "attention",
      "Xavier Bresson",
      "Graph Convolutional Networks",
      "graph",
      "Graph Laplacian",
      "Spectral Graph ConvNets",
      "SplineGCNs",
      "LapGCNs",
      "ChebNets",
      "Isotropic GCNs",
      "GraphSage",
      "Anisotropic GCNs",
      "MoNets",
      "Graph Attention Networks",
      "GAT",
      "Gated Graph ConvNets",
      "Graph Transformers",
      "Benchmarking GNNs"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 7222,
    "views": 43961,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=gYayCG6YyO8": {
    "current_person": "Yann LeCun",
    "video_id": "gYayCG6YyO8",
    "url": "https://www.youtube.com/watch?v=gYayCG6YyO8",
    "title": "Week 14 \u2013 Lecture: Structured prediction with energy based models",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Yann LeCun\nWeek 14: http://bit.ly/pDL-en-14\n\n0:00:00 \u2013 Week 14 \u2013 Lecture\n\nLECTURE Part A: http://bit.ly/pDL-en-14-1\nIn this section, we discussed the structured prediction. We first introduced the Energy-Based factor graph and efficient inference for it. Then we gave some examples for simple Energy-Based factor graphs with \u201cshallow\u201d factors. Finally, we discussed the Graph Transformer Net.\n0:00:25 \u2013 Structured Prediction, Energy based factor graphs, Sequence Labeling\n0:18:06 \u2013 Efficient Inference for Energy-Based Factor Graph and Some Simple Energy-Based Factor Graphs\n0:43:30 \u2013 Graph Transformer Net\n\nLECTURE Part B: http://bit.ly/pDL-en-14-2\nThe second leg of the lecture further discusses the application of graphical model methods to energy-based models. After spending some time comparing different loss functions, we discuss the application of the Viterbi algorithm and forward algorithm to graphical transformer networks. We then transition to discussing the Lagrangian formulation of backpropagation and then variational inference for energy-based models.\n1:00:22 \u2013 Comparing Losses and the start of language models as graphs\n1:15:18 \u2013 Forward algorithm in Graph Transformer Networks\n1:32:53 \u2013 Lagrangian formulation of back prop and neural ODE\n1:48:42 \u2013 Variational Inference in terms of Energy",
    "author": "Alfredo Canziani",
    "keywords": [
      "Yann LeCun",
      "Deep Learning",
      "PyTorch",
      "NYU",
      "Structured Prediction",
      "Energy based factor graphs",
      "Sequence Labeling",
      "Conditional Random Field",
      "Max Margin Markov Nets",
      "Latent SVM",
      "Graph Transformer Net"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 7650,
    "views": 7605,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=sbhr2wjU1-I": {
    "current_person": "Yann LeCun",
    "video_id": "sbhr2wjU1-I",
    "url": "https://www.youtube.com/watch?v=sbhr2wjU1-I",
    "title": "Week 15 \u2013 Practicum part A: Inference for latent variable energy based models (EBMs)",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Alfredo Canziani\nWeek 15: http://bit.ly/pDL-en-15\n\n0:00:00 \u2013 Week 15 \u2013 Practicum part A\n\nPRACTICUM: http://bit.ly/pDL-en-15-1\nWhen encountering the data with multiple outputs for a single input, feed-forward networks cannot capture such implicit dependencies. Instead, latent-variable energy-based models (EBMs) come to the rescue. We developed a toy ellipse example with a fixed input and the optimal model formulation. Then, we applied latent-variable EBMs to inference the best latent variables that can learn the implicit relationships.\n0:00:46 \u2013 Training data and model definition\n0:18:08 \u2013 Energy and free energy for two training samples\n0:37:21 \u2013 Free energy dense estimation",
    "author": "Alfredo Canziani",
    "keywords": [
      "Deep Learning",
      "Yann LeCun",
      "PyTorch",
      "NYU",
      "EBM",
      "latent variable",
      "free energy",
      "energy based model",
      "inference",
      "multimodal"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 3544,
    "views": 3803,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=XLSb1Cs1Jao": {
    "current_person": "Yann LeCun",
    "video_id": "XLSb1Cs1Jao",
    "url": "https://www.youtube.com/watch?v=XLSb1Cs1Jao",
    "title": "Week 15 \u2013 Practicum part B: Training latent variable energy based models (EBMs)",
    "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Alfredo Canziani\nWeek 15: http://bit.ly/pDL-en-15\n\n0:00:00 \u2013 Week 15 \u2013 Practicum part B\n\nPRACTICUM: http://bit.ly/pDL-en-15-2\nThis section starts from introducing a relaxed version of free energy by modifying the \"temperature\" to smooth the energy function. Then we demonstrate how to train EBMs by minimizing loss functionals with several examples. Finally we give a concrete example of self-supervised learning, where we train a EBM to learn a horn-like data manifold.\n0:00:11 \u2013 Free Energy, zero temperature limit and relaxation\n0:27:11 \u2013 Training an EBM\n0:42:57 \u2013 Conditional / self-supervised",
    "author": "Alfredo Canziani",
    "keywords": [
      "Deep Learning",
      "Yann LeCun",
      "PyTorch",
      "NYU",
      "EBM",
      "latent variable",
      "free energy",
      "energy based model",
      "inference",
      "multimodal"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 3536,
    "views": 2946,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=d2GixptaHjk": {
    "current_person": "Yann LeCun",
    "video_id": "d2GixptaHjk",
    "url": "https://www.youtube.com/watch?v=d2GixptaHjk",
    "title": "Matrix multiplication, signals, and convolutions",
    "description": "Spring 2020 website: http://bit.ly/pDL-home\nSpring 2020 playlist: http://bit.ly/pDL-YouTube\nSpeaker: Alfredo Canziani\nFrom NYU Deep Learning, Fall 2020 course.\n\n00:00:00 \u2013 Week 5 \u2013 Practicum\n\n00:00:28 \u2013 Table of contents\n00:01:13 \u2013 Matrix multiplication recap\n00:15:13 \u2013 Natural signals\n00:21:17 \u2013 Extension to natural signals\n00:36:49 \u2013 Playing with IPython and PyTorch",
    "author": "Alfredo Canziani",
    "keywords": [
      "Deep Learning",
      "Yann LeCun",
      "NYU",
      "PyTorch",
      "convolutional neural networks",
      "CNN",
      "ConvNet",
      "locality",
      "stationarity",
      "sparsity",
      "parameter sharing",
      "kernels"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 2822,
    "views": 4044,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=nCq_vy9qE-k": {
    "current_person": "Yann LeCun",
    "video_id": "nCq_vy9qE-k",
    "url": "https://www.youtube.com/watch?v=nCq_vy9qE-k",
    "title": "Supervised and self-supervised transfer learning (with PyTorch Lightning)",
    "description": "Spring 2020 website: http://bit.ly/pDL-home\nSpring 2020 playlist: http://bit.ly/pDL-YouTube\nSpeaker: William Falcon & Alfredo Canziani\nFrom NYU Deep Learning, Fall 2020 course.\n\n00:00:00 \u2013 Week 10 \u2013 Practicum\n\n00:06:00 \u2013 Supervised and self-supervised\rtransfer learning\n00:18:43 \u2013 Supervised transfer learning\rwith PyTorch\n00:22:31 \u2013 Supervised transfer learning\rwith Lightning \u2013 implementation\n00:30:59 \u2013 Supervised transfer learning\rwith Lightning \u2013 training\n00:41:01 \u2013 Self-supervised transfer learning\rwith Lightning\n00:53:09 \u2013 Generalisation comparison\n01:08:34 \u2013 Summary",
    "author": "Alfredo Canziani",
    "keywords": [
      "Deep Learning",
      "Yann LeCun",
      "NYU",
      "PyTorch",
      "Lightning",
      "William Falcon",
      "Boltz",
      "transfer learning",
      "self-supervised",
      "supervised",
      "back-bone",
      "ResNet",
      "fine tuning"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 4284,
    "views": 10625,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=nTlCqaL7fCY": {
    "current_person": "Yann LeCun",
    "video_id": "nTlCqaL7fCY",
    "url": "https://www.youtube.com/watch?v=nTlCqaL7fCY",
    "title": "01L \u2013 Gradient descent and the backpropagation algorithm",
    "description": "Course website: http://bit.ly/DLSP21-home\nPlaylist: http://bit.ly/DLSP21-YouTube\nSpeaker: Yann LeCun\n\nChapters\n00:00:00 \u2013 Supervised learning\n00:03:43 \u2013 Parametrised models\n00:07:23 \u2013 Block diagram\n00:08:55 \u2013 Loss function, average loss\n00:12:23 \u2013 Gradient descent\n00:30:47 \u2013 Traditional neural nets\n00:35:07 \u2013 Backprop through a non-linear function\n00:40:41 \u2013 Backprop through a weighted sum\n00:50:55 \u2013 PyTorch implementation\n00:57:18 \u2013 Backprop through a functional module\n01:05:08 \u2013 Backprop through a functional module\n01:12:15 \u2013 Backprop in practice\n01:33:15 \u2013 Learning representations\n01:42:14 \u2013 Shallow networks are universal approximators!\n01:47:25 \u2013 Multilayer architectures == compositional structure of data",
    "author": "Alfredo Canziani",
    "keywords": [
      "PyTorch",
      "NYU",
      "Yann LeCun",
      "Deep Learning",
      "neural networks"
    ],
    "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw",
    "length": 6664,
    "views": 33027,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=cWzi38-vDbE": {
    "current_person": "Yann LeCun",
    "video_id": "cWzi38-vDbE",
    "url": "https://www.youtube.com/watch?v=cWzi38-vDbE",
    "title": "Yann LeCun - How does the brain learn so much so quickly? (CCN 2017)",
    "description": "Presented at Cognitive Computational Neuroscience (CCN) 2017 (http://www.ccneuro.org) held September 6-8, 2017.",
    "author": "Kendrick Kay",
    "keywords": [],
    "channel_url": "https://www.youtube.com/channel/UCrqzM2TcRD5PFQAJIrrHazw",
    "length": 2610,
    "views": 38467,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=A7AnCvYDQrU": {
    "current_person": "Yann LeCun",
    "video_id": "A7AnCvYDQrU",
    "url": "https://www.youtube.com/watch?v=A7AnCvYDQrU",
    "title": "Yann LeCun: \"Energy-Based Self-Supervised Learning\"",
    "description": "Machine Learning for Physics and the Physics of Learning 2019\r\nWorkshop IV: Using Physical Insights for Machine Learning\n\r\n\"Energy-Based Self-Supervised Learning\"\nYann LeCun - Courant Institute of Mathematical Sciences, New York University & Facebook AI Research\n\nInstitute for Pure and Applied Mathematics, UCLA\r\nNovember 18, 2019\r\n\nFor more information: http://www.ipam.ucla.edu/mlpws4",
    "author": "Institute for Pure & Applied Mathematics (IPAM)",
    "keywords": [
      "ipam",
      "math",
      "mathematics",
      "ucla",
      "yann lecun",
      "machine learning",
      "supervised learning",
      "AI",
      "artificial intelligence",
      "self-supervised learning"
    ],
    "channel_url": "https://www.youtube.com/channel/UCGzuiiLdQZu9wxDNJHO_JnA",
    "length": 3610,
    "views": 31386,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=YzD7Z2yRL7Y": {
    "current_person": "Yann LeCun",
    "video_id": "YzD7Z2yRL7Y",
    "url": "https://www.youtube.com/watch?v=YzD7Z2yRL7Y",
    "title": "ISSCC 2019: Deep Learning Hardware: Past, Present, and Future - Yann LeCun",
    "description": "Yann LeCun, Facebook AI Research & New York University, New York, NY\n\nDeep learning has caused revolutions in computer understanding of images, audio, and text,\nenabling new applications such as information search and filtering, autonomous driving,\nradiology screening, real-time language translation, and virtual assistants. But almost all these\nsuccesses largely use supervised learning, which requires human-annotated data, or\nreinforcement learning, which requires too many trials to be practical in most real-world\nsituations. In contrast, animals and humans seem to learn vast amounts of background\nknowledge about the world through mere observation and occasional actions in a selfsupervised manner. Making progress in self-supervised learning is the main challenge of AI\nfor the next decade. Success may result in machines with some level of common sense. But\nthey will be built around deep learning architectures that are considerably larger than current\nones, requiring vastly more powerful hardware than what we have today.",
    "author": "ISSCC Videos",
    "keywords": [
      "Yann LeCun",
      "ISSCC",
      "ISSCC2019",
      "Deep Learning"
    ],
    "channel_url": "https://www.youtube.com/channel/UCsE9jUU5dOqjylXEYBj-BjA",
    "length": 2150,
    "views": 16707,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=_IqJCWsSo6M": {
    "current_person": "Yann LeCun",
    "video_id": "_IqJCWsSo6M",
    "url": "https://www.youtube.com/watch?v=_IqJCWsSo6M",
    "title": "Yann LeCun - Lecture 1",
    "description": "",
    "author": "LesHouches2022",
    "keywords": [],
    "channel_url": "https://www.youtube.com/channel/UCNvC8umtUuZSpcWDmbiWy2Q",
    "length": 5409,
    "views": 3438,
    "whisper_result": ""
  }
}