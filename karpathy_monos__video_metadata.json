{
  "https://www.youtube.com/watch?v=VMj-3S1tku0": {
    "current_person": "Andrej Karpathy",
    "video_id": "VMj-3S1tku0",
    "url": "https://www.youtube.com/watch?v=VMj-3S1tku0",
    "title": "The spelled-out intro to neural networks and backpropagation: building micrograd",
    "description": "This is the most step-by-step spelled-out explanation of backpropagation and training of neural networks. It only assumes basic knowledge of Python and a vague recollection of calculus from high school.\n\nLinks:\n- micrograd on github: https://github.com/karpathy/micrograd\n- jupyter notebooks I built in this video: https://github.com/karpathy/nn-zero-to-hero/tree/master/lectures/micrograd\n- my website: https://karpathy.ai\n- my twitter: https://twitter.com/karpathy\n- \"discussion forum\": nvm, use youtube comments below for now :)\n- (new) Neural Networks: Zero to Hero series Discord channel: https://discord.gg/Hp2m3kheJn , for people who'd like to chat more and go beyond youtube comments\n\nExercises:\nyou should now be able to complete the following google collab, good luck!:\nhttps://colab.research.google.com/drive/1FPTx1RXtBfc4MaTkf7viZZD4U2F9gtKN?usp=sharing\n\nChapters:\n00:00:00 intro\n00:00:25 micrograd overview\n00:08:08 derivative of a simple function with one input\n00:14:12 derivative of a function with multiple inputs\n00:19:09 starting the core Value object of micrograd and its visualization\n00:32:10 manual backpropagation example #1: simple expression\n00:51:10 preview of a single optimization step\n00:52:52 manual backpropagation example #2: a neuron\n01:09:02 implementing the backward function for each operation\n01:17:32 implementing the backward function for a whole expression graph\n01:22:28 fixing a backprop bug when one node is used multiple times\n01:27:05 breaking up a tanh, exercising with more operations\n01:39:31 doing the same thing but in PyTorch: comparison\n01:43:55 building out a neural net library (multi-layer perceptron) in micrograd\n01:51:04 creating a tiny dataset, writing the loss function\n01:57:56 collecting all of the parameters of the neural net\n02:01:12 doing gradient descent optimization manually, training the network\n02:14:03 summary of what we learned, how to go towards modern neural nets\n02:16:46 walkthrough of the full code of micrograd on github\n02:21:10 real stuff: diving into PyTorch, finding their backward pass for tanh\n02:24:39 conclusion\n02:25:20 outtakes :)",
    "author": "Andrej Karpathy",
    "keywords": [
      "neural",
      "network",
      "backpropagation",
      "lecture"
    ],
    "channel_url": "https://www.youtube.com/channel/UCXUPKJO5MZQN11PqgIvyuvQ",
    "length": 8752,
    "views": 186184,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=3SypMvnQT_s": {
    "current_person": "Andrej Karpathy",
    "video_id": "3SypMvnQT_s",
    "url": "https://www.youtube.com/watch?v=3SypMvnQT_s",
    "title": "Tesla Full Self Driving explained by Andrej Karpathy",
    "description": "Head of Tesla Full Self Driving Andrej Karpathy gives a very technical and in-depth presentation at Tesla AI Day on August 19 2021\n\n#Tesla #FSD #Autonomy\n\nGet 1000Miles/1500Kms of free Supercharging when you purchase a new Tesla! Click this link before placing your order!\nhttp://ts.la/trevor41818\n\n\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Our FREE Forum \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nhttps://teslaownersonline.com/\n\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Support Channel \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nWant to support us? http://patreon.com/teslaownersonline\n15% off at Teslabros.com https://teslabros.com/?ref=TeslaOwnersOnline\n$5 off EVAnnex use coupon code HIGH5 https://evannex.com\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Social Media \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nTwitter https://twitter.com/Model3Owners\nYouTube https://www.youtube.com/model3ownersclubdotcom\nFaceBook https://www.facebook.com/groups/TeslaOwnersOnline/\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Our Gear \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n (As an Amazon Associate I earn from qualifying purchases):\nMY MAIN CAMERA: Canon EOS R Mirrorless Camera https://amzn.to/2YJdDcB\nMY MAIN LENS: Canon RF 15-35mm F2.8 https://amzn.to/3iaOijo\nMY BACKUP CAMERA: Canon EOS M50 https://amzn.to/32y9Sr8\nMAIN FAVE MIC: Deity V-Mic D3 Pro Microphone https://amzn.to/3eNATLW\nBEST DRONE: DJI MAvic Air 2 https://amzn.to/32sdDhy\nI RECORD WITH: Atomos Ninja V Recorder https://amzn.to/2VsCuiA\nMAIN LIGHT: Aputure LS 300x Light https://amzn.to/2NFh7q3\nFAVE ACTION CAM: DJI Osmo Action Camera https://amzn.to/2WNN8ls\nMY GIMBAL: Zhiyun Weebill S Gimbal https://amzn.to/3dt3iqw\nI STORE MY VIDEOS ON: Synology DS1819+ RAID https://amzn.to/2Ufk9Fy\nMY MAIN TRIPOD: Vanguard Alta Pro 263AB Tripod https://amzn.to/2PWzVl9\nFAVE SLIDER: Neewer 31.5 inch Slider https://amzn.to/2Vyukre\nWIRELESS MIC: RODE Wireless GO Microphones https://amzn.to/2Q27TVu\nSMALL MIC: RODE VideoMic http://amzn.to/2F1R1Zt\nMAIN RECORDER FOR PODCASTS: Zoom H6 Audio recorder http://amzn.to/2ErgMFy\n\nNOTE: Federal law allow citizens to reproduce, distribute , or exhibit portions of copyrighted material. This is called fair use and is allowed for the purpose of criticism, news reporting, teaching, and parody which doesn't infringe of copyright under 17 USC 107.",
    "author": "Tesla Owners Online",
    "keywords": [
      "Tesla",
      "Model S",
      "Model 3",
      "Tesla Model 3",
      "Model X",
      "Supercharging",
      "Gigafactory",
      "Review",
      "News",
      "Elon Musk",
      "Full Self Driving",
      "FSD",
      "Andrej Karpathy"
    ],
    "channel_url": "https://www.youtube.com/channel/UCry4jW5bcj9DIs7ZwA95Ylw",
    "length": 1555,
    "views": 16421,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=a510m7s_SVI": {
    "current_person": "Andrej Karpathy",
    "video_id": "a510m7s_SVI",
    "url": "https://www.youtube.com/watch?v=a510m7s_SVI",
    "title": "FULL Andrej Karpathy Tesla Autonomous Driving Talk CVPR 2021",
    "description": "",
    "author": "Tesla News",
    "keywords": [],
    "channel_url": "https://www.youtube.com/channel/UCsr8J0yROgEtjfGb8fBwlpg",
    "length": 2214,
    "views": 30011,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=q8SA3rM6ckI": {
    "current_person": "Andrej Karpathy",
    "video_id": "q8SA3rM6ckI",
    "url": "https://www.youtube.com/watch?v=q8SA3rM6ckI",
    "title": "Building makemore Part 4: Becoming a Backprop Ninja",
    "description": "We take the 2-layer MLP (with BatchNorm) from the previous video and backpropagate through it manually without using PyTorch autograd's loss.backward(): through the cross entropy loss, 2nd linear layer, tanh, batchnorm, 1st linear layer, and the embedding table. Along the way, we get a strong intuitive understanding about how gradients flow backwards through the compute graph and on the level of efficient Tensors, not just individual scalars like in micrograd. This helps build competence and intuition around how neural nets are optimized and sets you up to more confidently innovate on and debug modern neural networks.\n\n!!!!!!!!!!!!\nI recommend you work through the exercise yourself but work with it in tandem and whenever you are stuck unpause the video and see me give away the answer. This video is not super intended to be simply watched. The exercise is here:\nhttps://colab.research.google.com/drive/1WV2oi2fh9XXyldh02wupFQX0wh5ZC-z-?usp=sharing\n!!!!!!!!!!!!\n\nLinks:\n- makemore on github: https://github.com/karpathy/makemore\n- jupyter notebook I built in this video: https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part4_backprop.ipynb\n- collab notebook: https://colab.research.google.com/drive/1WV2oi2fh9XXyldh02wupFQX0wh5ZC-z-?usp=sharing\n- my website: https://karpathy.ai\n- my twitter: https://twitter.com/karpathy\n- our Discord channel: https://discord.gg/Hp2m3kheJn\n\nSupplementary links:\n- Yes you should understand backprop: https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b\n- BatchNorm paper: https://arxiv.org/abs/1502.03167\n- Bessel\u2019s Correction: http://math.oxford.emory.edu/site/math117/besselCorrection/\n- Bengio et al. 2003 MLP LM https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf \n\nChapters:\n00:00:00 intro: why you should care & fun history\n00:07:26 starter code\n00:13:01 exercise 1: backproping the atomic compute graph\n01:05:17 brief digression: bessel\u2019s correction in batchnorm\n01:26:31 exercise 2: cross entropy loss backward pass\n01:36:37 exercise 3: batch norm layer backward pass\n01:50:02 exercise 4: putting it all together\n01:54:24 outro",
    "author": "Andrej Karpathy",
    "keywords": [
      "deep learning",
      "backpropagation",
      "neural network",
      "language model",
      "chain rule",
      "tensors"
    ],
    "channel_url": "https://www.youtube.com/channel/UCXUPKJO5MZQN11PqgIvyuvQ",
    "length": 6924,
    "views": 15104,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=PaCmpygFfXo": {
    "current_person": "Andrej Karpathy",
    "video_id": "PaCmpygFfXo",
    "url": "https://www.youtube.com/watch?v=PaCmpygFfXo",
    "title": "The spelled-out intro to language modeling: building makemore",
    "description": "We implement a bigram character-level language model, which we will further complexify in followup videos into a modern Transformer language model, like GPT. In this video, the focus is on (1) introducing torch.Tensor and its subtleties and use in efficiently evaluating neural networks and (2) the overall framework of language modeling that includes model training, sampling, and the evaluation of a loss (e.g. the negative log likelihood for classification).\n\nLinks:\n- makemore on github: https://github.com/karpathy/makemore\n- jupyter notebook I built in this video: https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part1_bigrams.ipynb\n- my website: https://karpathy.ai\n- my twitter: https://twitter.com/karpathy\n- (new) Neural Networks: Zero to Hero series Discord channel: https://discord.gg/Hp2m3kheJn , for people who'd like to chat more and go beyond youtube comments\n\nUseful links for practice:\n- Python + Numpy tutorial from CS231n https://cs231n.github.io/python-numpy-tutorial/ . We use torch.tensor instead of numpy.array in this video. Their design (e.g. broadcasting, data types, etc.) is so similar that practicing one is basically practicing the other, just be careful with some of the APIs - how various functions are named, what arguments they take, etc. - these details can vary.\n- PyTorch tutorial on Tensor https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html\n- Another PyTorch intro to Tensor https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html\n\nExercises:\nE01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\nE02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\nE03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\nE04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\nE05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?\nE06: meta-exercise! Think of a fun/interesting exercise and complete it.\n\nChapters:\n00:00:00 intro\n00:03:03 reading and exploring the dataset\n00:06:24 exploring the bigrams in the dataset\n00:09:24 counting bigrams in a python dictionary\n00:12:45 counting bigrams in a 2D torch tensor (\"training the model\")\n00:18:19 visualizing the bigram tensor\n00:20:54 deleting spurious (S) and (E) tokens in favor of a single . token\n00:24:02 sampling from the model\n00:36:17 efficiency! vectorized normalization of the rows, tensor broadcasting \n00:50:14 loss function (the negative log likelihood of the data under our model)\n01:00:50 model smoothing with fake counts\n01:02:57 PART 2: the neural network approach: intro\n01:05:26 creating the bigram dataset for the neural net\n01:10:01 feeding integers into neural nets? one-hot encodings\n01:13:53 the \"neural net\": one linear layer of neurons implemented with matrix multiplication\n01:18:46 transforming neural net outputs into probabilities: the softmax\n01:26:17 summary, preview to next steps, reference to micrograd\n01:35:49 vectorized loss\n01:38:36 backward and update, in PyTorch\n01:42:55 putting everything together\n01:47:49 note 1: one-hot encoding really just selects a row of the next Linear layer's weight matrix\n01:50:18 note 2: model smoothing as regularization loss\n01:54:31 sampling from the neural net\n01:56:16 conclusion",
    "author": "Andrej Karpathy",
    "keywords": [
      "deep learning",
      "language model",
      "gpt",
      "bigram",
      "neural network",
      "pytorch",
      "torch",
      "tensor"
    ],
    "channel_url": "https://www.youtube.com/channel/UCXUPKJO5MZQN11PqgIvyuvQ",
    "length": 7065,
    "views": 52909,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=TCH_1BHY58I": {
    "current_person": "Andrej Karpathy",
    "video_id": "TCH_1BHY58I",
    "url": "https://www.youtube.com/watch?v=TCH_1BHY58I",
    "title": "Building makemore Part 2: MLP",
    "description": "We implement a multilayer perceptron (MLP) character-level language model. In this video we also introduce many basics of machine learning (e.g. model training, learning rate tuning, hyperparameters, evaluation, train/dev/test splits, under/overfitting, etc.).\n\nLinks:\n- makemore on github: https://github.com/karpathy/makemore\n- jupyter notebook I built in this video: https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part2_mlp.ipynb\n- collab notebook (new)!!!: https://colab.research.google.com/drive/1YIfmkftLrz6MPTOO9Vwqrop2Q5llHIGK?usp=sharing\n- Bengio et al. 2003 MLP language model paper (pdf): https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n- my website: https://karpathy.ai\n- my twitter: https://twitter.com/karpathy\n- (new) Neural Networks: Zero to Hero series Discord channel: https://discord.gg/Hp2m3kheJn , for people who'd like to chat more and go beyond youtube comments\n\nUseful links:\n- PyTorch internals ref http://blog.ezyang.com/2019/05/pytorch-internals/\n\nExercises:\n- E01: Tune the hyperparameters of the training to beat my best validation loss of 2.2\n- E02: I was not careful with the intialization of the network in this video. (1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? (2) Can you tune the initialization to get a starting loss that is much more similar to (1)?\n- E03: Read the Bengio et al 2003 paper (link above), implement and try any idea from the paper. Did it work?\n\nChapters:\n00:00:00 intro\n00:01:48 Bengio et al. 2003 (MLP language model) paper walkthrough\n00:09:03 (re-)building our training dataset\n00:12:19 implementing the embedding lookup table\n00:18:35 implementing the hidden layer + internals of torch.Tensor: storage, views\n00:29:15 implementing the output layer\n00:29:53 implementing the negative log likelihood loss\n00:32:17 summary of the full network\n00:32:49 introducing F.cross_entropy and why\n00:37:56 implementing the training loop, overfitting one batch\n00:41:25 training on the full dataset, minibatches\n00:45:40 finding a good initial learning rate\n00:53:20 splitting up the dataset into train/val/test splits and why\n01:00:49 experiment: larger hidden layer\n01:05:27 visualizing the character embeddings\n01:07:16 experiment: larger embedding size\n01:11:46 summary of our final code, conclusion\n01:13:24 sampling from the model\n01:14:55 google collab (new!!) notebook advertisement",
    "author": "Andrej Karpathy",
    "keywords": [
      "deep learning",
      "neural network",
      "multilayer perceptron",
      "nlp",
      "language model"
    ],
    "channel_url": "https://www.youtube.com/channel/UCXUPKJO5MZQN11PqgIvyuvQ",
    "length": 4539,
    "views": 29648,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=P6sfmUTpUmc": {
    "current_person": "Andrej Karpathy",
    "video_id": "P6sfmUTpUmc",
    "url": "https://www.youtube.com/watch?v=P6sfmUTpUmc",
    "title": "Building makemore Part 3: Activations & Gradients, BatchNorm",
    "description": "We dive into some of the internals of MLPs with multiple layers and scrutinize the statistics of the forward pass activations, backward pass gradients, and some of the pitfalls when they are improperly scaled. We also look at the typical diagnostic tools and visualizations you'd want to use to understand the health of your deep network. We learn why training deep neural nets can be fragile and introduce the first modern innovation that made doing so much easier: Batch Normalization. Residual connections and the Adam optimizer remain notable todos for later video.\n\nLinks:\n- makemore on github: https://github.com/karpathy/makemore\n- jupyter notebook I built in this video: https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part3_bn.ipynb\n- collab notebook: https://colab.research.google.com/drive/1H5CSy-OnisagUgDUXhHwo1ng2pjKHYSN?usp=sharing\n- my website: https://karpathy.ai\n- my twitter: https://twitter.com/karpathy\n- Discord channel: https://discord.gg/Hp2m3kheJn\n\nUseful links:\n- \"Kaiming init\" paper: https://arxiv.org/abs/1502.01852\n- BatchNorm paper: https://arxiv.org/abs/1502.03167\n- Bengio et al. 2003 MLP language model paper (pdf): https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n\nExercises:\n- E01: I did not get around to seeing what happens when you initialize all weights and biases to zero. Try this and train the neural net. You might think either that 1) the network trains just fine or 2) the network doesn't train at all, but actually it is 3) the network trains but only partially, and achieves a pretty bad final performance. Inspect the gradients and activations to figure out what is happening and why the network is only partially training, and what part is being trained exactly.\n- E02: BatchNorm, unlike other normalization layers like LayerNorm/GroupNorm etc. has the big advantage that after training, the batchnorm gamma/beta can be \"folded into\" the weights of the preceeding Linear layers, effectively erasing the need to forward it at test time. Set up a small 3-layer MLP with batchnorms, train the network, then \"fold\" the batchnorm gamma/beta into the preceeding Linear layer's W,b by creating a new W2, b2 and erasing the batch norm. Verify that this gives the same forward pass during inference. i.e. we see that the batchnorm is there just for stabilizing the training, and can be thrown out after training is done! pretty cool.\n\nChapters:\n00:00:00 intro\n00:01:22 starter code\n00:04:19 fixing the initial loss \n00:12:59 fixing the saturated tanh\n00:27:53 calculating the init scale: \u201cKaiming init\u201d\n00:40:40 batch normalization\n01:03:07 batch normalization: summary\n01:04:50 real example: resnet50 walkthrough\n01:14:10 summary of the lecture\n01:18:35 just kidding: part2: PyTorch-ifying the code\n01:26:51 viz #1: forward pass activations statistics\n01:30:54 viz #2: backward pass gradient statistics\n01:32:07 the fully linear case of no non-linearities\n01:36:15 viz #3: parameter activation and gradient statistics\n01:39:55 viz #4: update:data ratio over time\n01:46:04 bringing back batchnorm, looking at the visualizations\n01:51:34 summary of the lecture for real this time",
    "author": "Andrej Karpathy",
    "keywords": [
      "neural network",
      "deep learning",
      "makemore",
      "batchnorm",
      "batch normalization",
      "pytorch"
    ],
    "channel_url": "https://www.youtube.com/channel/UCXUPKJO5MZQN11PqgIvyuvQ",
    "length": 6957,
    "views": 29110,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=oBklltKXtDE": {
    "current_person": "Andrej Karpathy",
    "video_id": "oBklltKXtDE",
    "url": "https://www.youtube.com/watch?v=oBklltKXtDE",
    "title": "PyTorch at Tesla - Andrej Karpathy, Tesla",
    "description": "Hear from Andrej Karpathy on how Tesla is using PyTorch to develop full self-driving capabilities for its vehicles, including AutoPilot and Smart Summon.",
    "author": "PyTorch",
    "keywords": [
      "AI",
      "Artificial Intelligence",
      "Machine Learning",
      "ML",
      "Facebook",
      "PyTorch",
      "PyTorch 1.3",
      "PyTorch Developer Conference",
      "PTDC19",
      "PTDC",
      "Developer Conference",
      "Tesla",
      "PyTorch at Tesla",
      "Andrej Karpathy",
      "Autonomous driving cars",
      "Self-driving cars",
      "Autopilot",
      "smart Summon"
    ],
    "channel_url": "https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw",
    "length": 671,
    "views": 437703,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=YNvwfMhAD8U": {
    "current_person": "Andrej Karpathy",
    "video_id": "YNvwfMhAD8U",
    "url": "https://www.youtube.com/watch?v=YNvwfMhAD8U",
    "title": "Andrej Karpathy on Machine Consciousness",
    "description": "At the NeoGenesis salon, Andrej gave a talk on consciousness - (self-awareness / inner dialogue).\n\nRead his short story (Forward Pass) for a narrativized argument at:\nkarpathy.github.io/2021/03/27/forward-pass",
    "author": "NeoGenesis House",
    "keywords": [],
    "channel_url": "https://www.youtube.com/channel/UCW_GCaZNXPr6gQJiKvbXOrQ",
    "length": 543,
    "views": 1737,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=u6aEYuemt0M": {
    "current_person": "Andrej Karpathy",
    "video_id": "u6aEYuemt0M",
    "url": "https://www.youtube.com/watch?v=u6aEYuemt0M",
    "title": "Deep Learning for Computer Vision (Andrej Karpathy, OpenAI)",
    "description": "The talks at the Deep Learning School on September 24/25, 2016 were amazing. I clipped out individual talks  from the full live streams and provided links to each below in case that's useful for people who want to watch specific talks several times (like I do). Please check out the official website (http://www.bayareadlschool.org) and full live streams below.\n\nHaving read, watched, and presented deep learning material over the past few years, I have to say that this is one of the best collection of introductory deep learning talks I've yet encountered. Here are links to the individual talks and the full live streams for the two days:\n\n1. Foundations of Deep Learning (Hugo Larochelle, Twitter) - https://youtu.be/zij_FTbJHsk\n2. Deep Learning for Computer Vision (Andrej Karpathy, OpenAI) - https://youtu.be/u6aEYuemt0M\n3. Deep Learning for Natural Language Processing (Richard Socher, Salesforce) - https://youtu.be/oGk1v1jQITw\n4. TensorFlow Tutorial (Sherry Moore, Google Brain) - https://youtu.be/Ejec3ID_h0w\n5. Foundations of Unsupervised Deep Learning (Ruslan Salakhutdinov, CMU) - https://youtu.be/rK6bchqeaN8\n6. Nuts and Bolts of Applying Deep Learning (Andrew Ng) - https://youtu.be/F1ka6a13S9I\n7. Deep Reinforcement Learning (John Schulman, OpenAI) - https://youtu.be/PtAIh9KSnjo\n8. Theano Tutorial (Pascal Lamblin, MILA) - https://youtu.be/OU8I1oJ9HhI\n9. Deep Learning for Speech Recognition (Adam Coates, Baidu) - https://youtu.be/g-sndkf7mCs\n10. Torch Tutorial (Alex Wiltschko, Twitter) - https://youtu.be/L1sHcj3qDNc\n11. Sequence to Sequence Deep Learning (Quoc Le, Google) - https://youtu.be/G5RY_SUJih4\n12. Foundations and Challenges of Deep Learning (Yoshua Bengio) - https://youtu.be/11rsu_WwZTc\n\nFull Day Live Streams:\nDay 1: https://youtu.be/eyovmAtoUx0\nDay 2: https://youtu.be/9dXiAecyJrY\n\nGo to http://www.bayareadlschool.org for more information on the event, speaker bios, slides, etc. Huge thanks to the organizers (Shubho Sengupta et al) for making this event happen.\n\nCONNECT:\n- If you enjoyed this video, please subscribe to this channel.\n- AI Podcast: https://lexfridman.com/ai/\n- Show your support: https://www.patreon.com/lexfridman\n- LinkedIn: https://www.linkedin.com/in/lexfridman\n- Twitter: https://twitter.com/lexfridman\n- Facebook: https://www.facebook.com/lexfridman\n- Instagram: https://www.instagram.com/lexfridman\n- Slack: https://deep-mit-slack.herokuapp.com",
    "author": "Lex Fridman",
    "keywords": [
      "deep learning"
    ],
    "channel_url": "https://www.youtube.com/channel/UCSHZKyawb77ixDdsGog4iWA",
    "length": 5116,
    "views": 152649,
    "whisper_result": ""
  },
  "https://www.youtube.com/watch?v=IHH47nZ7FZU": {
    "current_person": "Andrej Karpathy",
    "video_id": "IHH47nZ7FZU",
    "url": "https://www.youtube.com/watch?v=IHH47nZ7FZU",
    "title": "Andrej Karpathy: Tesla Autopilot and Multi-Task Learning for Perception and Prediction",
    "description": "Clips from Andrej Karpathy's talk at ICML (June 2019). I think multi-task learning is one of the most important (and understudied) subfields of machine learning. Most real world problems are multi-task. I especially find the discussion on team workflow fascinating (see 18:25). I've been thinking and working on this topic a lot lately, and will probably give a lecture on it. Here's the outline:\n0:00 - Sensors\n0:29 - Single-task learning challenges\n4:35 - Multi-task neural network architecture\n11:50 - Loss function considerations\n14:34 - Training dynamics\n18:25 - Team workflow\n\nFull talk:\nhttps://slideslive.com/38917690/multitask-learning-in-the-wilderness",
    "author": "Lex Clips",
    "keywords": [
      "tesla",
      "tesla autopilot",
      "deep learning",
      "machine learning",
      "multi-task learning",
      "karpathy",
      "elon musk",
      "autopilot",
      "artificial intelligence",
      "ai",
      "lex autopilot",
      "lex clips",
      "andrej karpathy",
      "self driving",
      "neural network"
    ],
    "channel_url": "https://www.youtube.com/channel/UCJIfeSCssxSC_Dhc5s7woww",
    "length": 1436,
    "views": 61501,
    "whisper_result": ""
  }
}