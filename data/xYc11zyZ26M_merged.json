{"video_id": "xYc11zyZ26M", "title": "Week 9 \u2013 Practicum: (Energy-based) Generative adversarial networks", "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Alfredo Canziani\nWeek 9: http://bit.ly/pDL-en-09\n\n0:00:00 \u2013 Week 9 \u2013 Practicum\n\nPRACTICUM: http://bit.ly/pDL-en-09-3\nDuring this week\u2019s practicum, we explored Generative Adversarial Networks (GANs) and how they can produce realistic generative models. We then compared GANs with VAEs from week 8 to highlight key differences between two networks. Next, we discussed several model limitations of GANs. Finally, we looked at the source code for the PyTorch example Deep Convolutional Generative Adversarial Networks (DCGAN).\n0:00:57 \u2013 Intro to GANs\n0:30:44 \u2013 Difference between GANs and VAEs and major pitfalls in GANs\n0:48:31 \u2013 DCGAN source code", "author": "Alfredo Canziani", "keywords": ["Deep Learning", "Yann LeCun", "generative", "variational autoencoder", "PyTorch", "GAN", "generative adversarial network", "energy model", "EBGAN", "generator", "discriminator", "cost", "adversarial", "minmax", "NYU"], "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw", "length": 4512, "views": 6065, "publish_date": "11/02/2022", "timestamp": 1591574400, "entity": "Yann LeCun", "transcript": {"text": " Today we're going to be talking about generative adversarial networks, or how to actually have them properly made. All right, so generative adversarial networks, unsupervised learning, generative models. So generative models, again, are models that allow you to get something that is in the input space. Most of the time, what is happening in this field is that we assume there is a probability distribution over these samples, but it doesn't have to. For example, a decoder in a classic route encoder can be thought of as a generative model, in my opinion, and also for IAN. Many will disagree, and they say generative model has to have an input which follows a specific distribution. We are in the realm of unsupervised learning where we don't have labels. So let's get started with generative adversarial networks. So what is this stuff? You should know, right? So this is a variational out encoder. The variational out encoder is basically like a normal out encoder where the encoder, in this case, provides us the parameters for a distribution from where we sample our latent input to z. So the only difference between the normal one is the sampler, which is going to pick a random sample. So instead of having one simple code, which is like one point, you have one input here, you have one code here. Instead, now you're going to have some volume. And therefore, each point within this volume will be mapped back to the original point. That's a very important part about the variational out encoder. So let's see how this generative adversarial net looks like. So we have this stuff, which is actually the same, right? So what's going on here? We have the same generator and the same sampler. And then what else do we have? OK, we have another input there. So the input before it was on the left hand side on the bottom. Now the input is halfway through. And the output is actually also halfway through. Finally, we get that kind of switch. And then on top of that switch, we're going to have a cost network. Usually in the classical definition, in the classical formulation of a gun, there we have like a discriminator. Discriminators are just a plain wrong option, at least following Jan's suggestions, which I agree with, because we'll see soon why. We'll see that in a bit. Right now, let's focus on the fact that we have this cost network. So we have basically similar modules, right? Sampler on the right hand side. There's a sampler on the left hand side. We have a decoder on the left hand side, which is basically generating something. Since z is considered a code, then we have a decoding step. Whereas on the right hand side, since z was not a code, but was simply an input, then we have a generator. And that z is simply, for example, a sample from a Gaussian distribution, with normal distribution. And then that x hat will be generated by this initially untrained network. The cost network instead has to figure out, it has to be a high cost if we feed that x hat, the blue one, because we want to give, like we want to say, oh, this is a bad sample. Or instead, if we sample the pink one, if you get the switch to select the pink one, we should have a low cost, because that would allow us to figure out that we are actually doing a bad sample. That we are actually doing, we have actually a true sample, a good sample. So summarizing the sequence of operations, we have that the generator maps my latent input to z into this Rn, which is the space of the input space. So we have the latent input, the orange one that is mapped into the original input. So we have that orange z mapped to x hat in blue. The top one is that it's a cost network in this case, which maps the input, which can be the pink x or the blue hat, the blue x hat, which is mapped to my cost. So in this case, this cost, it is, yeah, this cost module is actually a cost, like in Jan's diagram, it's going to be a square, okay, which outputs a scalar. This scalar will be a high value, a large number, positive large number, if the input is a fake input. And it should be a low number, probably up to zero, if we actually have the input coming from the pink side, the real side. Okay. And then how do we train this system? So the system will be trained with different gradients. So the cost network will be trained in order to have low cost for inputs that are pink and a high cost for inputs that are blue. Okay. So for example, you can think about this, if you would have like a discriminator in this case, you may have, you may think about this as a, you know, two classes classification problem, you try to get zero for x pink, pink x, and a one for the blue x. We talk a bit about why that's bad to use this zero one output in a second, in a second. But otherwise, we just want this network to learn this cost. Okay. So let's figure out how this works in a diagram. Do you remember how we were starting with a variational out encoder? With a variational out encoder, we were starting from the left hand side, right? We were picking a input and then we were, so we were taking the input, we were moving to the latent space. We were moving this point because we are adding some noise. And then we were getting back to the original point. Then we were trying to get those points close together by using the reconstruction laws. And then we were trying to set some structure in the latent space by using that relative entropy there. Okay. Instead for the gun, the generative other side of your network, we're going to be starting from the right hand side. So we pick a sample, a random number, let's say 42. We feed that through a generator and we get that blue x hat over there. Then we're going to be training in another network in order to be coming up with a high value for that blue sample. Then we're going to pick another X, let's say a pink X in this case on the bottom right of the spiral, which is going to be enforced now to have a low cost. So this is pretty much like a first initial big picture about how this system works. So let me try to give you two more interpretations. So this is like the kind of definitions and visual interpret, like mathematical definition, and then the visual definition. Now I'm going to be trying to give you a few interpretations, which I pretty like, and are going to make me sound like a fool, but I am a fool. So, you know, I just go for it. So you can think about the generator as being a Italian and therefore I will be using some proper Italian accent. Okay. So I'm a proper Italian now and I am in the south of Italy and I'm going to be trying to make some fake money. Okay. Because we are very good at that. So we make a fake money and then we go to Germany to get some, to purchase something. Okay. We go to Germany with this fake money and then there is this German people look at us. It's like, Oh, fucking Italian. This is fake money. And so we can't really manage to, we can't really manage to buy anything, but since we are Italian, we have Spikes. We have spies in the, okay. There are questions. Hold on. Maybe I'm offending people now chat. What's going on? Oh, okay. You're enjoying the thing. Cool. Okay. So I was not offending anyone. Fantastic. Okay. So, uh, we have a spy back in Germany and the spy is like calling back, uh, home. Hey, mama Mia, you gave us the wrong money. Like it was so fucked up. But it was just, you know, not proper. Okay. Okay. So yeah, chill, chill down. Right. We are like back again, home. Uh, what movie is this? That's just my own movie. So we are back in Italy. Um, you know, we are, we are making, you're able to make such nice, uh, art and everything. So we must be able to make better money. Right. So we try now to fix the things that our spy told us. So we are fixing the things that our spy told us. So we make a better money. We go back to Germany and try to buy other things. And Germans are like, huh, it's better. It's fake. Okay. Then again, you had a spy calling back down to Italy and it's like, Oh, what are you doing? Uh, and then we're like, Oh, I understand. I can be, you know, and we are fixing it, the money, you know, we are making several iterations of that. Like, so we try to make better and better, uh, versions of the money. Finally, we go back to Germany and this case, Germany, because they have money, right? That we had, they have things we can buy. So we go back there and they are like, huh, it looks very good. No, I don't know how to make German accent. I'm sorry. And so they accept the money, right? Okay. And this is how pretty much these, uh, uh, generative other side of the network works. We have like a generator, which are the Italian dudes in the South, which are making fake money. And we are trying to purchase something in Germany and Germany is the discriminator and they are very strict and very, um, you know, they're German. Okay. Um, politically correct. I'm not so whatever, but then we do have a spy, right? And what is the spy? Can anyone figure out what's the spy analogy here? We haven't mentioned that so far. So the last function, back prop discriminator, okay. Uh, some, some feedback, okay. It's feedback. And how is the feedback coming from? So whenever we train the, uh, whenever we train the discriminator or the cost network, right, we have some gradient, that gradient allow me to do two things, right? I can either lower the, uh, I can either lower the final value. And so I can tune my parameters of the cost function. Let me go back to the cost function. So we have some gradients of the final cost, right? And so, uh, we have, yeah, finally some gradient. So the final cost with respect to the parameters of the network. And, and so usually, usually when I train the network, the cost network, I will try to tune the parameters such that, uh, I will have a final lower loss, right? This is a cost network and there is a loss on top of the cost network, right? It's a bit confusing. So, um, we're going to be trying to optimize the parameters of the cost network in order to perform well and therefore having a very low loss on the same way we can use those same gradients that are computed with respect to this network. And you'll see my mouse. Uh, so I have, you know, my final loss on top of here. We come down with the gradients and then you have here some gradients and now these gradients, you know, how, if you change, uh, these X hat, you're going to know how these final loss will change, right? Therefore you can train now this generator with this gradient in order to increase this final loss. Okay. So when we train this cost network, we'd like to minimize the final loss given that we input these two different inputs, right? But also we'd like to increase this final loss. So we'd like to make this final network perform worse by, you know, improving the generator. Okay. And so this information that comes down here and down this way, which is the backward pass, right? The input gradient will be used for tuning the parameter of the generator such that it managed to fool the cost network. Okay. And so this is the analogy with the spy in the German, uh, in Germany. Okay. Uh, is the distribution of Z fixed? Uh, sorry, fixed. Uh, so yeah, so Z, uh, Z actually comes from, let's say a normal distribution. I actually don't really, um, have anything about, uh, anything to say about this distribution. As long as you pick your distribution, you know, the generator will map that distribution into some X hat distribution, which will hopefully match what is the pink, uh, distribution of the axis. Okay. So even, even though Z, the distribution of Z is fixed, we, we can be sure that we can change the generator in such a way that, um, we can minimize the cost function. Right. So a loss at distribution is fixed. Uh, the generator will, uh, uh, how do you say, ply P L a Y, I think you will apply this kind of distribution such that you're going to be basically like reflowing into something that looks alike the X, uh, in the pink X, uh, hopefully, okay. I haven't told you about the pitfalls of this, uh, system. Okay. But hopefully we'd like to manage to get a distribution out of those blue Xs X hats such that they resemble the original distribution on the left-hand side in the pink one. Okay. Did I answer your question? Yeah, that makes sense. Okay. Wouldn't the X produced by the generator be the new improved money? Um, the blue one. Okay. Yeah. Thank you. Uh, I actually didn't finish that one. So the, uh, pink one are the true euros we are using in Europe and the blue, the blue X hat are the money that we make in Italy. Okay. Oh, my, yeah. Okay. Other questions. I thought the generator was supposed to give negative samples. Um, so negative samples. Okay. Uh, so there are two steps here. Uh, we provide negative samples that are these X hats to the cost network. So the cost network is trained in order to have low values on the pink input and higher values on the blue input. Okay. And so if the network, the cost network performs well, then the final loss here on top will be very zero, very low. Okay. So if the cost network is very, is performing very well, then you're going to have a final low loss here. Nevertheless, the generator will be trained in order to increase that loss, because we'd like to fool these Germans. That doesn't make sense. Can you just clarify what the spy is in this analogy? Yeah. The spy is the input gradient. So whenever here I have my cost network and to train this cost network, I'm going to have a final layer here on top, right? Uh, let's say this is an MSC, uh, for example, uh, an MSC with, you know, zero for whenever I, whenever I have an input, uh, X pink or some, you know, value, let's say plus 10 in this case, it's a, I will try a number for the moment. Um, uh, value plus 10 for the blue guys, right? So my cost network is a regression regression network. You can think about this as just one single linear layer. So it's like an affine transformation of the input. And then these basically a final value, I set it to be zero for the pink input. I have an MSC between the output of the network and zero for whenever I input the pink input. And instead, let's say I choose an arbitrary value of 10 to be reflecting that the input is the blue one, right? So we have cost network, which is a network that is outputting a single scalar value. And this scalar value will go inside the MSC module here on top. Let me write maybe so we can all see what's going on. So I have here my MSC. Uh, this is my loss function, right? So don't get confused between loss and cost. There are two different things. So I have my MSC here. Uh, and if I have this guy here, my target is going to be zero. It's going to be my Y. Y, okay. For this one. And instead, if I input this guy here to the cost network, I expect to have, let's say, uh, in this case, an arbitrary plus 10. So my MSC in this case is going to be, uh, you know, mean square error between the output of the cost network and zero. In the other case, I'm going to have the MSC between the output of the network and 10. Um, so the network, if I just train, let's say we forget about all this stuff, right? We have just a few samples with, with, we think for the moment that the generator is not improving. So we have several pink samples and several blue samples. And then you train a network such that if I put the input, the pink one, you're going to get a zero in the output. And then if you put the blue one instead, you're going to be forcing the network to learn number 10. Okay. So you do some steps in gradient descent in the parameter space, such that in one case you get zero. The other case you get 10 whenever you provide several, several samples, right? Now that we have this network, this cost network, you can think about, uh, having the cost network, uh, to be actually the loss for the generator. Okay. And so if I have my generator input, uh, outputting something and these cost network, we say, oh, it's a very high cost. Then by trying to minimize this cost, you will try to basically generate something that was initially making the cost network providing you a low value. Okay. Is it making sense? Could you just quickly clarify the difference between costs and loss? Uh, the loss is what we use in order to train something. Okay. So my loss in this case is the MSC loss. This is my loss. So in order to train my cost network, I will have a loss function, which is the MSC loss function by minimizing the MSC loss function. I will be training the cost network. Now the fucked up part comes and I'm going to say that for my generator, the loss function that I want to minimize is the cost network. So for these generator, the loss is the cost and I try to minimize this guy output. Okay. So this is also, um, relative to what the honest teaching with the energy based models, you have energies and we try to have low energies through minimization of a loss function. So the loss function is what you use in order to train the parameters of a network. Okay. So that's the difference. Um, so if the network, okay. So another, uh, additional point is that a cost is like a evaluation of some network performance. So if my generator outputs a bad X, like, which is not pretty good looking, then you're going to have a high cost. Okay. It's like a high energy, but in order to minimize this energy, usually you have to minimize these losses. Okay. So, but again, the definition, what we like to use is that the loss is what you minimize in order to train the parameters of a network. So instead, like a cost can be thought as, you know, I take an action and then I have an action, I have a cost for taking that specific action. Okay. So you take an action, which is like writing an email about changing things. And then the cost is going to be having everyone piece at you. Hmm. Makes sense. Right. You always learn something new. Okay. Other questions so far. Um, sorry, I'm still a bit confused about the cost and generator. So for generator that generates the blue X, we want to increase the cost, but you just mentioned that we want to minimize the cost is like the loss function. So the cost is like the loss function for the generator and we want to minimize the loss. So we want to increase the costs or we want to decrease the cost for the generator. For the generator, you want to minimize the cost. So we train the generator through minimization of the cost network value. Okay. So there is two parts of this thing. Let me change color. So first part is going to be the training of this guy here. And the training of the cost network is made through the minimization of the MSC on top of here. So this is the loss for the cost network. So the cost, the MSC here is made between zero. Whenever I input a pink input. And then let's say for this example, like for sake of example, I like to have an MSC against 10 whenever I input a blue sample. Okay. So now we perform several steps of gradient descent in the parameter space of the cost network such that we minimize this loss. Okay. So now we have a network here, which is going to be outputting zero. If I put a pink input and input and output a 10, if I input a blue input so far, are you with me? Yeah. So it's like cost, the network costs will generate a high value for blue X, right? Yeah. That's what we train this cost to do. Okay. So this cost network will have to generate some large value in this case, 10 if I input a blue guy and we'll have to generate a small zero output if I put a zero, if I put a pink input. And in order to do that, we do this by minimization of MSC loss. Okay. This is first part. So far you're with me, right? Yeah. Okay. Fantastic. Now we have the second part, which is the cute version, the version that Jan likes, the different version that you don't find online, which is the following. So this cost network now will give you values that are close to zero whenever you input something that looks like proper. Okay. Otherwise it will put a high output, let's say a value around 10, if you put inside crappy input. So now, finally, how do we train this generator? Well, the generator now will be trained through the minimization of the cost network, right? So the cost network will say 10 here. So this output blue guy here, it's bad guy, right? So if the generator now switches slightly this X to make something that looks like this guy over here, then you get that from 10, we went down to zero, right? And therefore you got to minimize this cost network output value. And so we are using the cost network as the loss for training the generator. Okay. What do you mean by like, getting blue X closer to pink X? Right. So right now my generator outputs this blue, blue X. Okay. And this is like some image that looks bad or it's, you know, money that really looks fake. Now, how do you make better money? Well, the cost network is going to give you a scalar value for each output your generator makes. Therefore you can compute the partial derivative. You can compute the gradient, you know, of that cost value. I like to compute the partial derivative of these lowercase c. So dc over dx hat, right? So here I have the partial and this is awful writing. Sorry. All right. I can't write. Okay. E, no, that was C. Oh my God. Okay. This was a lowercase c. So it's like that. All right. Cool. So I compute the partial derivative of my lowercase c with respect to the X hat. Right. So now I have a gradient. This gradient allows me to move around and I figure out whether the cost is going to increase or decrease. Right. So this is kind of maybe, you know, a little bit not standard as in also yesterday Jan was talking about this. You know, you have some inputs in to your network. You can decide to do gradient descent in the input space. I can decide, for example, there is a architecture which doesn't have a generator at all. You start with a sample here and then you perform gradient descent in this sample space. And then you move the samples such that you get a lower, lower value for the cost network. And this way you can, you know, get an input that looks like resembling a good input, right? The pink one. Does it make that, that I kind of explain myself or is it still weird? Oh, it's much clearer. Thank you. You sure? Yeah. Yeah. It's like taking gradients in the input space and make it move towards like, and then decrease the cost. So that means the input actually gets better, like gets better money or better image. Right. Right. Right. And then you can also use this one as your gradient here coming down here. Right. And so now you can compute with a chain rule also on the partial derivative of this lowercase c with respect to the parameters w of the generator. Okay. So in this case, then I can train the generator, right? I had the partial of the cost over the parameters and therefore I can change now the values of the parameters on the generator in order to, you know, improve the network. Okay. Got it. It totally makes sense. Thank you. Of course. Yeah. Are they trained simultaneously or are they trained first? The cost network or the generator network? Right. People try both. They say it's better sometimes to keep one fixed while you're changing the other because otherwise you'll have always a moving target. Then there are contradictory evidence. We are actually going to be reading now some source code after we cover the major pitfalls, but I'm going to get back to your question in a few minutes. We don't need a regularization like KLD for Zedding gun because we sample from normal, yeah, direct directory, direct, yeah, directly. You sample the orange guy here from a normal distribution. So that's it, right? You have a sample, like a random number, and then you send these random numbers through the generator. That's it. And my Google home just came back to life. Okay. I answered your question. I think more questions. Then we have pitfalls and then we actually are going to be looking at source code. Yeah. So it seems like we are replacing the reconstruction loss with the differentiator network. How does that help exactly? Why can't, like, how is it bad to just use the reconstruction loss? Okay. Okay. Okay. Okay. Okay. This is, this is a very, very good question. I mean, I, it's something I forgot completely to say. So on the original encoder, we were always starting from some point. Then we were getting back to the space. We were moving a little bit that point such that we could cover some area and then go back to the other side. And then you try to make those two close, right? But in our case, right now in this generative other cellular net, we actually starting from the right hand side. So in the generative other cellular net, you start from the right hand side. So you start from the net. We actually starting from the right hand side. So in the generative other cellular net, you start from the right. There is no whatsoever connection between this guy here and this guy here. All you have is a cost network, which is telling you whether you are on this kind of thing here, right? I can't, it's going to be ugly, but okay. There's a cost network and it's going to tell you in this case, plus 10 here. And then it's going to tell you, let's say zero here. Okay. And the other case, you have a generative network here, which is mapping this input here down to here. Right? So one is trained in order to have the low values around the manifold and then larger values outside. And then you some, you would like something that is like, you know, you may want some curve levels, right? Like that such that as you move further away, and this stuff keeps increasing. If you have a discriminator, they will force to have zero here and one outside exactly in this manifold, like very, very close by. Right. And so that creates many problems. So, okay, let me try another analogy. There is another analogy. So, oh, there are questions, more questions. Let me go with the analogy. Then let's see whether this makes more sense. Let me actually see myself such that I can, okay. I can see now myself. All right. So you have like some true data points here. Okay. And then you have some generated data points over here that have been generated by the generator. Right. So points here, points down there. Let's assume now we are talking about this discriminator. Okay. So that I can illustrate what are the problems there. So you have a discriminator, which has these two kind of data. You have true data down here, fake data over here. And so what does the discriminator do? The discriminator decision boundary is going to be just a line here, right? That is cutting this stuff in half. Right. So far. Yeah. Right. Yes. Okay. Cool. So now you turn on the second step. Second step is going to be you turn on gravity on this decision boundary. So these points that are here will be boom, falling down here. Okay. The point here gets attracted by the decision boundary. So we train first the discriminator. We had this kind of decision boundary. And then we train the generator. You have these guys collapsing down here. So then you're going to be in a new situation. You have true data here, fake data here. You train again the discriminator in this case, you're going to have a decision boundary, which is going to be halfway here. Right. Then you turn on gravity such that these points here will collapse here. Right. And then you keep iterating this stuff, right? This stuff will be getting closer and closer and closer and closer and closer to the true data. Right. So you have these points that are like approaching and arriving to the real data location. So let's say now you're using your discriminator. You have those binary cross entropy laws for training the discriminator. What is now the main issue? Let's say I do a shifting. I bring my true data here such that we can see better what happens. So you have true data here. You have generated data here. Right. They are overlapping. And now you have a discriminator cutting here. So you're going to have overlap of these samples and this discriminator has no idea what to do. Right. So first of all, you're going to get misclassifications just because you thought you converged. We actually converge. Right. If you think about that, my true data is here. My generated data is here. They are overlapping. So I actually managed to reach convergence. And now my discriminator has no whatsoever clue how to split these things apart. So we don't converge. Or when we converge, we get issues. Right. The discriminator, I think the discriminator just tells apart two classes. Well, the discriminator cannot tell apart the two classes because these inputs are no more separated. Right. They are going to be like, if you actually manage to get the generator to perform very, very good samples, then these good samples are, you cannot tell them apart from the actual real samples. Right. And now the discriminator has no whatsoever clue about how to basically tell them apart. So whenever the generator works, the discriminator will not work. How nice is that. Okay. One other problem. Let's say again, you have fake data here, true data over here, and now you have a perfect, amazing, awesome discriminator such that here is absolutely zero. And then here is absolutely one. Okay. So you have like a basically like a step function. You don't have a sigmoid. What's going to be now the gradient. It's saturated, right. Or it's zero or it's one. There is no more gradient. These points will never move. Right. So the gravity that I was showing you before that was attracting these generated data through onto the decision boundary was basically the gradients that I saw the gradient of the final of the output of the discriminator or the cost network with respect to the, you know, samples generated by the generator. Right. But now if these discriminator has a perfect, is a perfect discriminator, zero here, one here, well, it's completely flat, right. If it's like that, there is no whatsoever gradient here. Right. And therefore if you're over here, so let's say we have data in one, one X, right. In one, one dimension, you have zero, zero, zero. Then you have one, one, one, one, one. But then if there is just, you know, there is no gradient, these points will never know they had to go in that direction. They will see, oh, we are bad guys. We have a bad value, but then we don't know in which direction to move because there is no whatsoever direction. The gradient is zero. It's a flat region. Right. So this is a very big issue, right. So whenever we train this generator, the cellular network, you want to make sure that this cost gradually increases as you move away from your region of the true data. Okay. Such that if there is a smooth or it's like a, you know, a convex thing, right. So if it keeps going up, up, up, up, up, you always know in which direction to fall down in order to arrive at the location where your true data is. Okay. And my Google home keeps rebooting. I'm like turning this shit off. There you go. Is it clear so far? Yeah. Yeah. Yeah. One final issue was that if we get a generator, which gets every point here mapped into this point over here, you know, all weights are zero. You have the final bias be exactly this value over here. Then that's finished because the discriminator or the cost function will say, you've done a very good job and the generator say, yay. And then the generator just outputs one image, right. And this is called mode collapse. Meaning that all points are mapped into just one point and you can't do anything about it. So the actual full story is that if every point here gets mapped to this point here, then the discriminator will tell that, oh, this is a fake point, right. And therefore the generator will switch and we'll say, this is the real output, right. And then you train the discriminator, discriminator say, oh, this is fake. Okay. So the generator will say, this is the real one, right. Okay. So you basically have a network that is just jumping through the samples and you can't fix that unless you introduce some, you know, penalty for not having some kind of diversity in the output of the generator. Finished ingredients. Well, whenever you have like saturated discriminators and we don't like discriminators, we prefer to learn this kind of smooth loss, cost, right. Cost network. Mode collapse. The thing that I just described right now, we just fall on one specific point. Unstable convergence. Yeah. And the point is that whenever you get a very good generator, you know, the discriminator will have no idea what's going on. You may have like a very big, a very big loss because you may get, you know, this point should be classified as this one. Instead it's completely classified as something else. You get some very, very large gradient. The discriminator will jump away and then the generator will jump away and the decision boundary will go, you know, bunkers, bunkers. And then you're going to have the generator trying to run after these, you know, running away decision boundary. Okay. And so there is no convergence. There is equilibrium. So it's an unstable equilibrium point, which is very, very tricky to catch. Yeah. So I understand we have some sort of minimax problem here with our generator and our cost. But in general, when you optimize this, I don't know really any straightforward ways to make sure you converge really to the right point. Right. I am not sure how you figure out whether you converge to a good point, but through visual inspection of your outputs of the generator. Or you can train several, you can train several guns and then you train a discriminator on some image data set. And then you classify, you evaluate the quality of the, of the image. Right. So this is like some kind of not good metric we don't like, but that's what has been done. It's called inception score. And then you can call it inception score. So you train a network, let's say the inception network. That's why it's called inception score on, you know, an image data set. And then you can try to see whether these generators are giving you images that look like something from, you know, from, from it is training data set. Again, it's not really a good metric, but someone tried to use this for a way to evaluate generative, to evaluate generative models. Yeah. Before starting, before going to the notebooks, let's have a look to actually a practical example of training loss for these two networks we have just seen now. Okay. So the loss function for my cost network, given the input X and the latent input Z in orange, can be the following. So it can be equal my cost C given my pink input X and then plus this part here, which is the positive part of a margin M minus the cost I'm going to give to a generated input, which is, is outputted by my generator, which is fed with the input, latent input, a random number. Okay. So G of Z gives me a fake input, then C will have to give me a cost. And as long as this cost will be lower than M, this part here will be positive part. As soon as C, the cost network gives me a cost for this generated input, which is larger than M, then this part here, M minus some number larger than M is going to be a negative number. And then since I take the positive part, this goes to zero. And so this part of the loss goes to zero. Whenever the cost network gives me a output that is larger than M for a input that is being provided by my generator. On the other side here, we have simply the cost associated to the correct pink input. Right. And so in order to squish this down to zero, you just have to have your cost network outputting a zero whenever the input is the good one. Okay. So in the example, in the example I was making before, I was saying that M is 10 and therefore the network is encouraged to output a scalar of 10, at least 10, at least 10, right? For inputs that are coming from the generator and said cost that is equal to zero is promoted by this term over here. So this is a example of possible loss we can use for training the cost network. Now this is done in this paper here by Jake, Michael and Jan from 2016. Then how do we train the generator? Well, that's quite straightforward because you simply have the loss for training the generator being equal the cost that the network, the cost network gives me for a given generated sample. Right. And so my generator will simply try to get a low cost. And that's so pretty. All right. Okay. Again, can we, can we be more specific? No. What is this cost network? I haven't told you yet a specific choice you can make for creating a network that is giving you this scalar based on the input, but I think you may already have some ideas how this network can be made. And so a possible choice for for this network is going to be the following. It's going to be the MSE, the quadratic difference between the decoding of the encoding of the specific input. So this is the reconstruction of a autoencoder minus the input itself. Square, right? The norm square. So how does this work? Well, like if the autoencoder is being trained only on pink samples, it will be able to reconstruct pink samples only. Right. And therefore the distance between my input, the pink input and the reconstruction of the autoencoder when I provide the pink input will be very small, hopefully, right. If we train this nicely instead, what happens now if I put an input here that is far from anything that is on the data manifold. Well, my autoencoder has been trained to output things that stays on the data manifold. And therefore there will be a substantial difference between my actual input and what my autoencoder can give you. Right. The nice part of this specific choice of cost network is that you can train this autoencoder without the generator. Right. You can simply train an autoencoder, whatever you can have like an under complete hidden layer, over complete, and you use some kind of regularization and information restriction, bottleneck. But nevertheless, you can actually train this guy without having a generator. Right. And this one, you will simply learn what is the train, the data manifold. And then you can use this as a proxy to establish the difference, the distance between your current input and what the network thinks the closest input on the training manifold could be. Okay. All right. Let's move on. In the last five minutes, if there are no questions, we are going to be reading the source code from PyTorch examples together. And this, I think, is going to be the first case where we are actually reading some programmer developer code. I'm not a programmer. So whatever you've been consuming so far were my notebooks, which were some kind of pedagogical and educational content, which is kind of massaged such that it looks nice and pretty and has nice looking output. Right now, we're going to be reading actually nice code written by people that do this as their job. Right. So we go GitHub. We don't go on PyTorch deep learning. We're going to PyTorch. PyTorch? No. PyTorch examples. Examples. Okay. So let's zoom a little bit. Okay. So here we have the DC gun and the swim width here. Okay. So we can just go through this code main things, right? So we start with importing a bunch of crappy things. As usual, you have an argument parser such that you can send some specific commands, specific parameters in the command line. This printouts all the options for the current setup. This one tries to make a directory. Otherwise, whatever. This is, if you choose a manual seed, then you're going to be actually setting a manual seed in such a way you're going to have reproducible results. CUDA benchmark equal true, I think speeds up the, yeah, this one allows you to have faster GPU routines, kernels. If you don't have CUDA, you're going to be taking forever to train this stuff. Data root, whatever. Data set. So you're going to be loading here ImageNet folders or LFW data set. So this is all things that we already know. Okay. So NGPU is going to be the number of GPU and Z is going to be the size of the latent variable. NGF and NDF is going to be, let's see, NGF, NDF. The number, I think, on the generative features and the number of discriminative features. And okay, we have some specific weight initialization, which really helps getting some proper training starting. And then let's actually have a look to this generator. Okay. So this is a classical and then subclass generator. You don't need this stuff if you're using Python 3. So let's see. So we have a sequential, right? The generator will be up sampling, such that, as you have seen from the last homework, you want to go from a small dimension to allow dimension to a larger dimension, you're going to use this module. They have batch norm, relu, and so on, right? And transpose convolution, batch norm, relu, and keep going. And finally, we have a tanh. We have a tanh because the output in this case is going to be lying within minus one to plus one. Forward is simply you send the input through the main. And the main was this one. The main mode, right? This is for using data parallel if you want to use several GPUs. And then here is how do you initialize with the specific initialization you define above. So simply just to put it in short, right? What does this thing do? You input something here that has nz size, right? And nz is the size of the latent, which is nz, nz, 100. So you input a vector of size 100. So it's a tensor, a one-dimensional tensor with 100 size. The size is 100. And so whenever you input this 100 vector, the output is going to be something like 64 by 64 times the number of channels in case you have color image or not, right? And nc, nc being the number of channels of the input image, right? Okay. It should be clear so far, right? No crazy things going on. Let's see the last part. And then let's see how they train. So the discriminator is the same stuff. You have a sequential. In this case, we feed this whatever number of channels times 64 times 64. And then you go down with leaky-reel-o. Oh, this is important. So leaky-reel-o in the discriminator, make sure you're not going to be killing the gradient if you are in the region, in a negative region, right? This is really, really important. If you don't have gradients here, then you can't train the generator. So you keep going down like that. And then finally, they use a sigmoid because they train this stuff as like a discriminator, like a classifier between two classes. And the forward is simply you send stuff through the main branch and they initialize this network. So we have netd and netg. So this implementation is slightly different from what you were going over before, right? Because the discriminator is just one. It outputs like the sigmoid. The only difference is this line here. Right. So far. So in the things we were talking in the lecture just before, we don't have the sigmoid. We just have this final convolution layer. Okay. Okay, gotcha. Second, of course, second difference is that we would not be using a binary cross entropy loss. This is the source of all evils, right? BCE plus this sigmoid, it's wrong way of training a generative adversarial network, a generator. Okay. So nevertheless, we go with the main formulation here. So let's see how it works. Fixed noise, you just create some random stuff with the batch size and the correct size here. We have two optimizers, one optimizer for the discriminator, one optimizer for the generator. And let's see what are the five steps that you should all know, right? So let's figure out. First of all, we zero the gradients of the discriminator. Okay. So now we have the real data is going to be the data zero that comes from the data loader. Right. So we have real data here. And then we are going to be having a set of labels, which are going to be the real labels. Okay. So then we have the network, the discriminator is going to be fed with the real input. And then we have some real output. Right. And then you're going to be computing the first part, which is going to be the criterion, which is the binary cross entropy between the output for whenever we put the real input and the real label. Okay. And then we perform the first step. So here we perform backward. In these criterion, which is computing the partial derivative of these binary cross entropy with respect to the weights of the discriminator. When we fed the real data to the discriminator, and we output, we try to match the labels, which are the real labels. Okay. This first point, number one. Okay. Keep in mind. Second part. Second part is going to be you get noise, and therefore you get your network, your generator, you feed some noise inside the generator. Therefore you get some fake output. Here, I'm going to be having my labels. Now are filled with the fake label. Okay. Therefore you feed the stuff inside the discriminator. We feed the fake data, but we detach, right? This is the important part. So right now we fed the fake data, but we detach it from the generator. And then we train again. So we have the criterion, we compute the loss between the output of the discriminator with the labels for the fake class. Okay. And then we perform another step of backward. So here we have the fake data. So we have the labels for the fake class. Okay. And then we perform another step of backward. So now we have two backward, right? So we have backward here, backward here, and we have computed the partial derivative of these criterion in the case where we were inputting real data. And in the case where we were inputting fake data. Okay. And so you compute backward here, backward here. There is no gradient, right? This is important part. So we only called clear the gradient at the beginning and we compute first the gradients with the real data and then the gradients for the fake data. Now you have that we can compute this one, right? So we step in the optimizer. So we computed the backward, the partial derivatives. We computed the other partial derivatives. Now we step. Finally, we train the generator and then we are done. So how do we train the generator? Now you fill the labels with the real labels. Okay. But you still feed the discriminator and the fake data, the one that was generated by my generator. This discriminator should say, oh, this is fake data, but we say, no, no, this is real data. And therefore you basically swap the thing. Right? So now you have the, when we compute these back propagation, we have these gradients, which are going in the opposite direction. These are trying to make your network perform worse. Okay. But then we are going to be just stepping with the generator. Right? So this one computes the partial derivative for everyone, right? Partial derivative of the criterion with respect to the weights of the discriminator and the weights of the data. So you can see the weights of the generator, but then we are going to be stepping only with the generator. So the generator will try to make lower criterion and the criterion has the label swapped, right? These are real label for whenever we feed the discriminator fake data. And so this one is actually working against the discriminator. And that was it. So you had one backward here. You have another backward here and you have another backward here and other questions right now. Wait, what's the difference between the first two backwards? Because they're both on the same objective. Right, right. Okay. So the first backward here, it's computed when the network, the discriminator, the cost network has been fed with the real data and the label here are, will fill, are filled with the real label. Okay. So this is the first part of the backward. So you have class, true class, and then you have class of the fake class, right? In this case, I generate my fake data through the generator, which was fed noise. And then I feed my discriminator with the fake data, but I stopped the gradients to go backwards in the generator. And this criterion still tries to make the output of the discriminator has been close to the label. And the label in this case are the one, the fake label, the one that are associated to the noise. So more than noise, maybe we can call this noise label or maybe, okay, it's fake label is fine too, right? Fake is the data and then the blue X hat that is generated by my generator network. And then when I put this X hat here inside the, sorry, the discriminator, I will tell the discriminator, hey, this one should be labeled as fake labels, right? And so you have this criteria here. So in this backward, you're going to be getting those partial derivative of the loss function with respect to the parameters. In the case when we have fed the fake data and we are trying to label them as fake labels, right? We have fake targets, fake labels. In the other part here, we actually were inputting inside the discriminator real data. And then we tell the network, you have a loss between your output and the labels, which are supposed to be real label. So the first part, you try to get, you get the partial derivatives corresponding to the loss that has been computed when real data was fed to the discriminator. In the second part is that you have the loss of, with respect, the loss of your output of the network when we fed fake data, right? And so here we simply do, again, another backward. So in this case, this backward, this line here and this line here, we'll give you, they will accumulate, right? Because PyTorch by default will accumulate every time you perform backward. So first part, you accumulate for the first half of the batch. And then second time you accumulate it, basically, you have the partial derivative for the second part of the batch. The first part of the batch is the real data. Second part of the batch is the fake data. Overall, you're going to have the partial derivatives of the fake, the real data and the fake data. And then we use this gradient in order to tune, to change the parameters of the network, the discriminator, right? Does it make sense so far? Yeah, that makes sense. But one of them is increasing it and the other one is decreasing. So these ones so far are both trying to decrease the criteria. So the first one is trying to decrease the criterion. So this is, you can see here in this criterion here, has the output, which is fed of the discriminator when it was fed with the real CPU data. So you have real data and real labels. So the criterion here is trying to match, to pair real data and real labels. Okay, so far? Yes. Okay. Second part, you try to have the network here, try to match fake data with fake label. Okay. Because the output comes from this discriminator, which was input with fake data. And then this, you know, you should force the network to say, oh, these are fake labels, right? And so first one, you had this criterion here acting on true data with labels that are telling you these are true data. And then you train, you have the loss for the network, which is going to be saying that these outputs that should be labeled as fake data, right? So this is still trying to minimize these criteria. Therefore, whenever you perform the optimizer step, the optimizer step will try to lower both this one and this one. Okay. Another way to do this one would be to have the summation between this one plus this one. You perform only one gradient the same step. Okay. The alternative, if you understand what I said, would be, let me try to open it, open item, item, this line here, right? So at 226. And then the other one was down to 235, right? 235. So we perform this one dot backward, and we did this one dot back, right? But otherwise, we could have done 226 plus the other one 235. And then we just perform backward here. Okay. So this was an alternative, which is actually exactly the same as right now. If you perform twice backward on the two different criterions is exactly as summing the two criterions and then performing backward only once. Okay. And then below, whenever we train the generator, here we swap the labels, right? In this case, we're going to be training the, so we step in with the generator optimizer such that we try to induce the network to output labels that are real labels when we provide data that is fake data, right? So this step in here, it will not try to untrain the discriminator, but it will train the generator such that it tries to make the discriminator performing poorly. So our generator, if it's generating our fake data, don't we want to be able to tell that apart? So don't we want to take a step in the other direction for that? Yeah. So you want to take a step in the other direction for the generator, right? You said no for the fake data, we want to be able to tell it's fake. Yeah. And that's, that's, that's where you do that here. If you have fake data, when you put fake data inside the discriminator, you also say that these labels are fake labels, right? So, okay. Fake label doesn't mean they are fake. These are the labels for the fake data. Maybe this is weird. So these are the true label. They are not fake label. They are true label for the fake data. Okay. This, I guess it's, see, that's what I dislike from other people writing code. That doesn't make sense. In this case, before this, for the generator, for the discriminator, we try to lower this criterion and we put this criterion. So these two lines are trying to match real data, the true data with a true label, right? And in this case, you have trying to match the, you know, generated data with a generated label, right? So, both of these two parts are trying to train the discriminator such that it can tell apart the two things. Wait, so just to clarify, so for example, like if we're trying to produce cat images, then like the generator would produce like, oh, I tried to make a cat image here and here's the label that's saying that it should be a cat versus this image. I didn't try to make a cat. So the label is zero for I didn't try to make a cat. Okay. So let me go with cat. So, what is it? So here we're going to have a real data. These are very nice, cute pictures of cats, right? And so we're going to say, oh, this output should be named as cat, right? Because it's very nice and looking cute. Then I'm going to be feeding some garbage, some noise to the generator. This looks like a monster. Okay. Ugly cat. Okay. So, let's try to make a cat image. So, this looks like a monster. Okay. Ugly cat. So then we provide this monster looking like images to the discriminator. And then we are going to be feeding these laws with the, you know, verdict, then whatever the discriminator says and with the label, they say these are monsters. And so here you perform backward again, and then step such that you're going to be training the discriminator such that they can tell apart cats from monsters. First part, second part below, we feed the monsters. In this case, we still have, we have the gradients, right? In this case, we cut off the gradient. Pay attention to this part. Here we cut off the gradient. So gradients don't go down the generator. In this case, we actually input the fake data, the monster looking images inside the discriminator. The discriminator say, oh, monsters, monsters. But in this case, we say, no, these are cute cats pictures. And so now we train the, we perform backward, which is computing the partial derivatives with respect to everything. And then we step for the generator such that the monsters that the generator was making now they look more cute. Okay. I can't be more cute than this. Sorry. Why don't we send the gradient of the fake data to the discriminator? We do in the second case, right? So let me answer the thing. So in this case here, we, when we send, when we send the gradients backwards, but back to the, you know, to the generator, we actually swap the correct labels with the incorrect labels. In this case, we input monsters. The discriminator says these are monsters and we say, oh, these are good looking cats. And then we train the generator such that these monsters will look like more nice looking cats. In this case, you don't want to send the gradients through because in this case you try to minimize the correct classification part. Right? So if you would send gradients backward, you would basically get a worse performing generator, right? Because you don't want to minimize this criterion. You want to maximize this criteria. Right? So that's why we don't have gradients in this first case, but we do have gradients in this case, because we absolutely want to compute the gradients with respect to the generator of this criteria. Is the combination of BC loss and sigmoid because, I mean, as a problem because of the underflow? So the problem with the, the BC thing here is the probabilistic approach, right? So this sigmoid, if you train this network very well, the sigmoid will be giving you zero gradients. And because if you saturate, you know, you're going to have, you're in the two, if you're not exactly on the middle way, if you are just away from the decision boundary, you're going to have basically or one, so it's still going to have zero gradient, or it's going to be the other side here, still all zero, but there is no gradient. So if you're over here, you don't know where to go, how to go down the hill, right? Because there is no hill. There is like a plateau. So this is the first problem. Second problem is that if you want to really have a very vertical, like a very vertical edge here, you will need very, very, very, very large weights. Okay. Such that if you, you know, the larger the weight, the larger is going to be the final value inside the sigmoid. And if you want to get like a saturated sigmoid, you're going to have like pretty large weights leading to that module. And this one creates some, you know, it's going to make your weights and everything kind of explode. So that's why people want to do several things like they want to limit the norm of the weights, then you want to limit the norm of the gradients. And there are many, many ways to patch this architecture, but that's, that's the patching, right? We don't want patching. We'd like to know what is proper and what is proper is going to be basically, and using a autoencoder, for example, for your final cost network. So if you consider the reconstruction error of an autoencoder, the reconstruction error of an autoencoder will be zero or small. If you provide a data that is coming from the training distribution, if you provide a sample that is away from the training distribution, remember the manifold from last time, then the autoencoder will do a poor job at the reconstruction and therefore the reconstruction error will be larger. So instead of using a discriminator, you can use a autoencoder reconstruction error. How can you get more out of this course overall? So let me give you a few suggestions. First, comprehension. If something was still not clear, just ask me the question section below the video. I will answer it. So let's say that you have a video. I will answer every question. So you will get it eventually. If you'd like to get more news about the field, things I do in terms of educational content and things I find interesting, you can follow up on Twitter and there you have my handle, AlfCNZ. If you'd like to have updates about newer videos, don't forget to subscribe to the channel and activate the notification bell. If you actually like this video, don't forget to put a thumb up. It helps as well recommending this video to other people. If you'd like to search the content of this lesson, we have an English transcription which is connected directly to this video. So every title in the transcription is clickable. If you click on the title, you get the right director to the correct location in the video. In the same way, each section of the video is the same title as in the transcription. So you can go back and forth. Maybe English is not your first language. Parli italiano, hablessa spagnol, nishuopo, don't quama, speak Korean. I have no idea how to speak Korean. Well, we have several translations of this material available on the website. So, and we are also looking for more translations if you can help as well. It's really important that you actually try to do some of the exercises and you play around with the notebooks and the source code we provided in order to internalize and understand better the concepts we explain during the lessons. Contribute, this is really giving you the opportunity to show your contribution. For example, you find some typos in the write-up, so you find some bugs in the notebooks. You can fix those and, you know, be part of this whole project by sending me a pull request on GitHub or letting me know otherwise. And that was it. So, see you next time. Bye bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 2.96, "text": " Today we're going to be talking about generative adversarial networks,", "tokens": [50364, 2692, 321, 434, 516, 281, 312, 1417, 466, 1337, 1166, 17641, 44745, 9590, 11, 50512, 50612, 420, 577, 281, 767, 362, 552, 6108, 1027, 13, 50848, 50900, 1057, 558, 11, 370, 1337, 1166, 17641, 44745, 9590, 11, 2693, 12879, 24420, 2539, 11, 1337, 1166, 5245, 13, 51116, 51116, 407, 1337, 1166, 5245, 11, 797, 11, 366, 5245, 300, 2089, 291, 281, 483, 746, 300, 307, 294, 264, 4846, 1901, 13, 51432, 51464, 4534, 295, 264, 565, 11, 437, 307, 2737, 294, 341, 2519, 307, 300, 321, 6552, 456, 307, 257, 51808, 51848], "temperature": 0.0, "avg_logprob": -0.15952097101414459, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.0006442471058107913}, {"id": 1, "seek": 0, "start": 4.96, "end": 9.68, "text": " or how to actually have them properly made.", "tokens": [50364, 2692, 321, 434, 516, 281, 312, 1417, 466, 1337, 1166, 17641, 44745, 9590, 11, 50512, 50612, 420, 577, 281, 767, 362, 552, 6108, 1027, 13, 50848, 50900, 1057, 558, 11, 370, 1337, 1166, 17641, 44745, 9590, 11, 2693, 12879, 24420, 2539, 11, 1337, 1166, 5245, 13, 51116, 51116, 407, 1337, 1166, 5245, 11, 797, 11, 366, 5245, 300, 2089, 291, 281, 483, 746, 300, 307, 294, 264, 4846, 1901, 13, 51432, 51464, 4534, 295, 264, 565, 11, 437, 307, 2737, 294, 341, 2519, 307, 300, 321, 6552, 456, 307, 257, 51808, 51848], "temperature": 0.0, "avg_logprob": -0.15952097101414459, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.0006442471058107913}, {"id": 2, "seek": 0, "start": 10.72, "end": 15.040000000000001, "text": " All right, so generative adversarial networks, unsupervised learning, generative models.", "tokens": [50364, 2692, 321, 434, 516, 281, 312, 1417, 466, 1337, 1166, 17641, 44745, 9590, 11, 50512, 50612, 420, 577, 281, 767, 362, 552, 6108, 1027, 13, 50848, 50900, 1057, 558, 11, 370, 1337, 1166, 17641, 44745, 9590, 11, 2693, 12879, 24420, 2539, 11, 1337, 1166, 5245, 13, 51116, 51116, 407, 1337, 1166, 5245, 11, 797, 11, 366, 5245, 300, 2089, 291, 281, 483, 746, 300, 307, 294, 264, 4846, 1901, 13, 51432, 51464, 4534, 295, 264, 565, 11, 437, 307, 2737, 294, 341, 2519, 307, 300, 321, 6552, 456, 307, 257, 51808, 51848], "temperature": 0.0, "avg_logprob": -0.15952097101414459, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.0006442471058107913}, {"id": 3, "seek": 0, "start": 15.040000000000001, "end": 21.36, "text": " So generative models, again, are models that allow you to get something that is in the input space.", "tokens": [50364, 2692, 321, 434, 516, 281, 312, 1417, 466, 1337, 1166, 17641, 44745, 9590, 11, 50512, 50612, 420, 577, 281, 767, 362, 552, 6108, 1027, 13, 50848, 50900, 1057, 558, 11, 370, 1337, 1166, 17641, 44745, 9590, 11, 2693, 12879, 24420, 2539, 11, 1337, 1166, 5245, 13, 51116, 51116, 407, 1337, 1166, 5245, 11, 797, 11, 366, 5245, 300, 2089, 291, 281, 483, 746, 300, 307, 294, 264, 4846, 1901, 13, 51432, 51464, 4534, 295, 264, 565, 11, 437, 307, 2737, 294, 341, 2519, 307, 300, 321, 6552, 456, 307, 257, 51808, 51848], "temperature": 0.0, "avg_logprob": -0.15952097101414459, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.0006442471058107913}, {"id": 4, "seek": 0, "start": 22.0, "end": 28.88, "text": " Most of the time, what is happening in this field is that we assume there is a", "tokens": [50364, 2692, 321, 434, 516, 281, 312, 1417, 466, 1337, 1166, 17641, 44745, 9590, 11, 50512, 50612, 420, 577, 281, 767, 362, 552, 6108, 1027, 13, 50848, 50900, 1057, 558, 11, 370, 1337, 1166, 17641, 44745, 9590, 11, 2693, 12879, 24420, 2539, 11, 1337, 1166, 5245, 13, 51116, 51116, 407, 1337, 1166, 5245, 11, 797, 11, 366, 5245, 300, 2089, 291, 281, 483, 746, 300, 307, 294, 264, 4846, 1901, 13, 51432, 51464, 4534, 295, 264, 565, 11, 437, 307, 2737, 294, 341, 2519, 307, 300, 321, 6552, 456, 307, 257, 51808, 51848], "temperature": 0.0, "avg_logprob": -0.15952097101414459, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.0006442471058107913}, {"id": 5, "seek": 2888, "start": 28.88, "end": 34.16, "text": " probability distribution over these samples, but it doesn't have to.", "tokens": [50364, 8482, 7316, 670, 613, 10938, 11, 457, 309, 1177, 380, 362, 281, 13, 50628, 50664, 1171, 1365, 11, 257, 979, 19866, 294, 257, 7230, 7955, 2058, 19866, 393, 312, 1194, 295, 382, 257, 1337, 1166, 2316, 11, 51028, 51028, 294, 452, 4800, 11, 293, 611, 337, 286, 1770, 13, 5126, 486, 14091, 11, 293, 436, 584, 1337, 1166, 2316, 575, 51292, 51292, 281, 362, 364, 4846, 597, 10002, 257, 2685, 7316, 13, 492, 366, 294, 264, 15355, 295, 2693, 12879, 24420, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.20883403104894302, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.00017841624503489584}, {"id": 6, "seek": 2888, "start": 34.879999999999995, "end": 42.16, "text": " For example, a decoder in a classic route encoder can be thought of as a generative model,", "tokens": [50364, 8482, 7316, 670, 613, 10938, 11, 457, 309, 1177, 380, 362, 281, 13, 50628, 50664, 1171, 1365, 11, 257, 979, 19866, 294, 257, 7230, 7955, 2058, 19866, 393, 312, 1194, 295, 382, 257, 1337, 1166, 2316, 11, 51028, 51028, 294, 452, 4800, 11, 293, 611, 337, 286, 1770, 13, 5126, 486, 14091, 11, 293, 436, 584, 1337, 1166, 2316, 575, 51292, 51292, 281, 362, 364, 4846, 597, 10002, 257, 2685, 7316, 13, 492, 366, 294, 264, 15355, 295, 2693, 12879, 24420, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.20883403104894302, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.00017841624503489584}, {"id": 7, "seek": 2888, "start": 42.16, "end": 47.44, "text": " in my opinion, and also for IAN. Many will disagree, and they say generative model has", "tokens": [50364, 8482, 7316, 670, 613, 10938, 11, 457, 309, 1177, 380, 362, 281, 13, 50628, 50664, 1171, 1365, 11, 257, 979, 19866, 294, 257, 7230, 7955, 2058, 19866, 393, 312, 1194, 295, 382, 257, 1337, 1166, 2316, 11, 51028, 51028, 294, 452, 4800, 11, 293, 611, 337, 286, 1770, 13, 5126, 486, 14091, 11, 293, 436, 584, 1337, 1166, 2316, 575, 51292, 51292, 281, 362, 364, 4846, 597, 10002, 257, 2685, 7316, 13, 492, 366, 294, 264, 15355, 295, 2693, 12879, 24420, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.20883403104894302, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.00017841624503489584}, {"id": 8, "seek": 2888, "start": 47.44, "end": 54.879999999999995, "text": " to have an input which follows a specific distribution. We are in the realm of unsupervised", "tokens": [50364, 8482, 7316, 670, 613, 10938, 11, 457, 309, 1177, 380, 362, 281, 13, 50628, 50664, 1171, 1365, 11, 257, 979, 19866, 294, 257, 7230, 7955, 2058, 19866, 393, 312, 1194, 295, 382, 257, 1337, 1166, 2316, 11, 51028, 51028, 294, 452, 4800, 11, 293, 611, 337, 286, 1770, 13, 5126, 486, 14091, 11, 293, 436, 584, 1337, 1166, 2316, 575, 51292, 51292, 281, 362, 364, 4846, 597, 10002, 257, 2685, 7316, 13, 492, 366, 294, 264, 15355, 295, 2693, 12879, 24420, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.20883403104894302, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.00017841624503489584}, {"id": 9, "seek": 5488, "start": 54.88, "end": 61.36, "text": " learning where we don't have labels. So let's get started with generative adversarial networks.", "tokens": [50364, 2539, 689, 321, 500, 380, 362, 16949, 13, 407, 718, 311, 483, 1409, 365, 1337, 1166, 17641, 44745, 9590, 13, 50688, 50924, 407, 437, 307, 341, 1507, 30, 509, 820, 458, 11, 558, 30, 407, 341, 307, 257, 3034, 1478, 484, 2058, 19866, 13, 51204, 51204, 440, 3034, 1478, 484, 2058, 19866, 307, 1936, 411, 257, 2710, 484, 2058, 19866, 689, 264, 2058, 19866, 11, 51456, 51456, 294, 341, 1389, 11, 6417, 505, 264, 9834, 337, 257, 7316, 490, 689, 321, 6889, 527, 48994, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.1254864280874079, "compression_ratio": 1.6422018348623852, "no_speech_prob": 3.1543971999781206e-05}, {"id": 10, "seek": 5488, "start": 66.08, "end": 71.68, "text": " So what is this stuff? You should know, right? So this is a variational out encoder.", "tokens": [50364, 2539, 689, 321, 500, 380, 362, 16949, 13, 407, 718, 311, 483, 1409, 365, 1337, 1166, 17641, 44745, 9590, 13, 50688, 50924, 407, 437, 307, 341, 1507, 30, 509, 820, 458, 11, 558, 30, 407, 341, 307, 257, 3034, 1478, 484, 2058, 19866, 13, 51204, 51204, 440, 3034, 1478, 484, 2058, 19866, 307, 1936, 411, 257, 2710, 484, 2058, 19866, 689, 264, 2058, 19866, 11, 51456, 51456, 294, 341, 1389, 11, 6417, 505, 264, 9834, 337, 257, 7316, 490, 689, 321, 6889, 527, 48994, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.1254864280874079, "compression_ratio": 1.6422018348623852, "no_speech_prob": 3.1543971999781206e-05}, {"id": 11, "seek": 5488, "start": 71.68, "end": 76.72, "text": " The variational out encoder is basically like a normal out encoder where the encoder,", "tokens": [50364, 2539, 689, 321, 500, 380, 362, 16949, 13, 407, 718, 311, 483, 1409, 365, 1337, 1166, 17641, 44745, 9590, 13, 50688, 50924, 407, 437, 307, 341, 1507, 30, 509, 820, 458, 11, 558, 30, 407, 341, 307, 257, 3034, 1478, 484, 2058, 19866, 13, 51204, 51204, 440, 3034, 1478, 484, 2058, 19866, 307, 1936, 411, 257, 2710, 484, 2058, 19866, 689, 264, 2058, 19866, 11, 51456, 51456, 294, 341, 1389, 11, 6417, 505, 264, 9834, 337, 257, 7316, 490, 689, 321, 6889, 527, 48994, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.1254864280874079, "compression_ratio": 1.6422018348623852, "no_speech_prob": 3.1543971999781206e-05}, {"id": 12, "seek": 5488, "start": 76.72, "end": 82.4, "text": " in this case, provides us the parameters for a distribution from where we sample our latent", "tokens": [50364, 2539, 689, 321, 500, 380, 362, 16949, 13, 407, 718, 311, 483, 1409, 365, 1337, 1166, 17641, 44745, 9590, 13, 50688, 50924, 407, 437, 307, 341, 1507, 30, 509, 820, 458, 11, 558, 30, 407, 341, 307, 257, 3034, 1478, 484, 2058, 19866, 13, 51204, 51204, 440, 3034, 1478, 484, 2058, 19866, 307, 1936, 411, 257, 2710, 484, 2058, 19866, 689, 264, 2058, 19866, 11, 51456, 51456, 294, 341, 1389, 11, 6417, 505, 264, 9834, 337, 257, 7316, 490, 689, 321, 6889, 527, 48994, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.1254864280874079, "compression_ratio": 1.6422018348623852, "no_speech_prob": 3.1543971999781206e-05}, {"id": 13, "seek": 8240, "start": 82.4, "end": 91.2, "text": " input to z. So the only difference between the normal one is the sampler, which is going to pick", "tokens": [50364, 4846, 281, 710, 13, 407, 264, 787, 2649, 1296, 264, 2710, 472, 307, 264, 3247, 22732, 11, 597, 307, 516, 281, 1888, 50804, 50804, 257, 4974, 6889, 13, 407, 2602, 295, 1419, 472, 2199, 3089, 11, 597, 307, 411, 472, 935, 11, 291, 362, 472, 4846, 51112, 51112, 510, 11, 291, 362, 472, 3089, 510, 13, 7156, 11, 586, 291, 434, 516, 281, 362, 512, 5523, 13, 400, 4412, 11, 1184, 51436, 51472, 935, 1951, 341, 5523, 486, 312, 33318, 646, 281, 264, 3380, 935, 13, 663, 311, 257, 588, 1021, 644, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.12803111473719278, "compression_ratio": 1.6986899563318778, "no_speech_prob": 4.589021045831032e-05}, {"id": 14, "seek": 8240, "start": 91.2, "end": 97.36000000000001, "text": " a random sample. So instead of having one simple code, which is like one point, you have one input", "tokens": [50364, 4846, 281, 710, 13, 407, 264, 787, 2649, 1296, 264, 2710, 472, 307, 264, 3247, 22732, 11, 597, 307, 516, 281, 1888, 50804, 50804, 257, 4974, 6889, 13, 407, 2602, 295, 1419, 472, 2199, 3089, 11, 597, 307, 411, 472, 935, 11, 291, 362, 472, 4846, 51112, 51112, 510, 11, 291, 362, 472, 3089, 510, 13, 7156, 11, 586, 291, 434, 516, 281, 362, 512, 5523, 13, 400, 4412, 11, 1184, 51436, 51472, 935, 1951, 341, 5523, 486, 312, 33318, 646, 281, 264, 3380, 935, 13, 663, 311, 257, 588, 1021, 644, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.12803111473719278, "compression_ratio": 1.6986899563318778, "no_speech_prob": 4.589021045831032e-05}, {"id": 15, "seek": 8240, "start": 97.36000000000001, "end": 103.84, "text": " here, you have one code here. Instead, now you're going to have some volume. And therefore, each", "tokens": [50364, 4846, 281, 710, 13, 407, 264, 787, 2649, 1296, 264, 2710, 472, 307, 264, 3247, 22732, 11, 597, 307, 516, 281, 1888, 50804, 50804, 257, 4974, 6889, 13, 407, 2602, 295, 1419, 472, 2199, 3089, 11, 597, 307, 411, 472, 935, 11, 291, 362, 472, 4846, 51112, 51112, 510, 11, 291, 362, 472, 3089, 510, 13, 7156, 11, 586, 291, 434, 516, 281, 362, 512, 5523, 13, 400, 4412, 11, 1184, 51436, 51472, 935, 1951, 341, 5523, 486, 312, 33318, 646, 281, 264, 3380, 935, 13, 663, 311, 257, 588, 1021, 644, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.12803111473719278, "compression_ratio": 1.6986899563318778, "no_speech_prob": 4.589021045831032e-05}, {"id": 16, "seek": 8240, "start": 104.56, "end": 112.16000000000001, "text": " point within this volume will be mapped back to the original point. That's a very important part", "tokens": [50364, 4846, 281, 710, 13, 407, 264, 787, 2649, 1296, 264, 2710, 472, 307, 264, 3247, 22732, 11, 597, 307, 516, 281, 1888, 50804, 50804, 257, 4974, 6889, 13, 407, 2602, 295, 1419, 472, 2199, 3089, 11, 597, 307, 411, 472, 935, 11, 291, 362, 472, 4846, 51112, 51112, 510, 11, 291, 362, 472, 3089, 510, 13, 7156, 11, 586, 291, 434, 516, 281, 362, 512, 5523, 13, 400, 4412, 11, 1184, 51436, 51472, 935, 1951, 341, 5523, 486, 312, 33318, 646, 281, 264, 3380, 935, 13, 663, 311, 257, 588, 1021, 644, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.12803111473719278, "compression_ratio": 1.6986899563318778, "no_speech_prob": 4.589021045831032e-05}, {"id": 17, "seek": 11216, "start": 112.16, "end": 118.96, "text": " about the variational out encoder. So let's see how this generative adversarial net looks like.", "tokens": [50364, 466, 264, 3034, 1478, 484, 2058, 19866, 13, 407, 718, 311, 536, 577, 341, 1337, 1166, 17641, 44745, 2533, 1542, 411, 13, 50704, 50760, 407, 321, 362, 341, 1507, 11, 597, 307, 767, 264, 912, 11, 558, 30, 407, 437, 311, 516, 322, 510, 30, 492, 362, 51236, 51236, 264, 912, 19265, 293, 264, 912, 3247, 22732, 13, 400, 550, 437, 1646, 360, 321, 362, 30, 2264, 11, 321, 362, 1071, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.11395309448242187, "compression_ratio": 1.5706214689265536, "no_speech_prob": 4.1935469198506325e-05}, {"id": 18, "seek": 11216, "start": 120.08, "end": 129.6, "text": " So we have this stuff, which is actually the same, right? So what's going on here? We have", "tokens": [50364, 466, 264, 3034, 1478, 484, 2058, 19866, 13, 407, 718, 311, 536, 577, 341, 1337, 1166, 17641, 44745, 2533, 1542, 411, 13, 50704, 50760, 407, 321, 362, 341, 1507, 11, 597, 307, 767, 264, 912, 11, 558, 30, 407, 437, 311, 516, 322, 510, 30, 492, 362, 51236, 51236, 264, 912, 19265, 293, 264, 912, 3247, 22732, 13, 400, 550, 437, 1646, 360, 321, 362, 30, 2264, 11, 321, 362, 1071, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.11395309448242187, "compression_ratio": 1.5706214689265536, "no_speech_prob": 4.1935469198506325e-05}, {"id": 19, "seek": 11216, "start": 129.6, "end": 138.07999999999998, "text": " the same generator and the same sampler. And then what else do we have? OK, we have another", "tokens": [50364, 466, 264, 3034, 1478, 484, 2058, 19866, 13, 407, 718, 311, 536, 577, 341, 1337, 1166, 17641, 44745, 2533, 1542, 411, 13, 50704, 50760, 407, 321, 362, 341, 1507, 11, 597, 307, 767, 264, 912, 11, 558, 30, 407, 437, 311, 516, 322, 510, 30, 492, 362, 51236, 51236, 264, 912, 19265, 293, 264, 912, 3247, 22732, 13, 400, 550, 437, 1646, 360, 321, 362, 30, 2264, 11, 321, 362, 1071, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.11395309448242187, "compression_ratio": 1.5706214689265536, "no_speech_prob": 4.1935469198506325e-05}, {"id": 20, "seek": 13808, "start": 138.08, "end": 143.36, "text": " input there. So the input before it was on the left hand side on the bottom. Now the input is", "tokens": [50364, 4846, 456, 13, 407, 264, 4846, 949, 309, 390, 322, 264, 1411, 1011, 1252, 322, 264, 2767, 13, 823, 264, 4846, 307, 50628, 50628, 15461, 807, 13, 400, 264, 5598, 307, 767, 611, 15461, 807, 13, 6288, 11, 321, 483, 300, 733, 295, 50984, 50984, 3679, 13, 400, 550, 322, 1192, 295, 300, 3679, 11, 321, 434, 516, 281, 362, 257, 2063, 3209, 13, 11419, 294, 264, 51256, 51256, 13735, 7123, 11, 294, 264, 13735, 37642, 295, 257, 3874, 11, 456, 321, 362, 411, 257, 20828, 1639, 13, 51624, 51712], "temperature": 0.0, "avg_logprob": -0.11771620874819548, "compression_ratio": 1.7407407407407407, "no_speech_prob": 2.5839171939878725e-05}, {"id": 21, "seek": 13808, "start": 143.36, "end": 150.48000000000002, "text": " halfway through. And the output is actually also halfway through. Finally, we get that kind of", "tokens": [50364, 4846, 456, 13, 407, 264, 4846, 949, 309, 390, 322, 264, 1411, 1011, 1252, 322, 264, 2767, 13, 823, 264, 4846, 307, 50628, 50628, 15461, 807, 13, 400, 264, 5598, 307, 767, 611, 15461, 807, 13, 6288, 11, 321, 483, 300, 733, 295, 50984, 50984, 3679, 13, 400, 550, 322, 1192, 295, 300, 3679, 11, 321, 434, 516, 281, 362, 257, 2063, 3209, 13, 11419, 294, 264, 51256, 51256, 13735, 7123, 11, 294, 264, 13735, 37642, 295, 257, 3874, 11, 456, 321, 362, 411, 257, 20828, 1639, 13, 51624, 51712], "temperature": 0.0, "avg_logprob": -0.11771620874819548, "compression_ratio": 1.7407407407407407, "no_speech_prob": 2.5839171939878725e-05}, {"id": 22, "seek": 13808, "start": 150.48000000000002, "end": 155.92000000000002, "text": " switch. And then on top of that switch, we're going to have a cost network. Usually in the", "tokens": [50364, 4846, 456, 13, 407, 264, 4846, 949, 309, 390, 322, 264, 1411, 1011, 1252, 322, 264, 2767, 13, 823, 264, 4846, 307, 50628, 50628, 15461, 807, 13, 400, 264, 5598, 307, 767, 611, 15461, 807, 13, 6288, 11, 321, 483, 300, 733, 295, 50984, 50984, 3679, 13, 400, 550, 322, 1192, 295, 300, 3679, 11, 321, 434, 516, 281, 362, 257, 2063, 3209, 13, 11419, 294, 264, 51256, 51256, 13735, 7123, 11, 294, 264, 13735, 37642, 295, 257, 3874, 11, 456, 321, 362, 411, 257, 20828, 1639, 13, 51624, 51712], "temperature": 0.0, "avg_logprob": -0.11771620874819548, "compression_ratio": 1.7407407407407407, "no_speech_prob": 2.5839171939878725e-05}, {"id": 23, "seek": 13808, "start": 155.92000000000002, "end": 163.28, "text": " classical definition, in the classical formulation of a gun, there we have like a discriminator.", "tokens": [50364, 4846, 456, 13, 407, 264, 4846, 949, 309, 390, 322, 264, 1411, 1011, 1252, 322, 264, 2767, 13, 823, 264, 4846, 307, 50628, 50628, 15461, 807, 13, 400, 264, 5598, 307, 767, 611, 15461, 807, 13, 6288, 11, 321, 483, 300, 733, 295, 50984, 50984, 3679, 13, 400, 550, 322, 1192, 295, 300, 3679, 11, 321, 434, 516, 281, 362, 257, 2063, 3209, 13, 11419, 294, 264, 51256, 51256, 13735, 7123, 11, 294, 264, 13735, 37642, 295, 257, 3874, 11, 456, 321, 362, 411, 257, 20828, 1639, 13, 51624, 51712], "temperature": 0.0, "avg_logprob": -0.11771620874819548, "compression_ratio": 1.7407407407407407, "no_speech_prob": 2.5839171939878725e-05}, {"id": 24, "seek": 16328, "start": 163.28, "end": 170.24, "text": " Discriminators are just a plain wrong option, at least following Jan's suggestions, which I agree", "tokens": [50364, 19839, 16796, 3391, 366, 445, 257, 11121, 2085, 3614, 11, 412, 1935, 3480, 4956, 311, 13396, 11, 597, 286, 3986, 50712, 50712, 365, 11, 570, 321, 603, 536, 2321, 983, 13, 492, 603, 536, 300, 294, 257, 857, 13, 1779, 586, 11, 718, 311, 1879, 322, 264, 1186, 300, 51164, 51212, 321, 362, 341, 2063, 3209, 13, 407, 321, 362, 1936, 2531, 16679, 11, 558, 30, 4832, 22732, 322, 264, 558, 51560, 51560, 1011, 1252, 13, 821, 311, 257, 3247, 22732, 322, 264, 1411, 1011, 1252, 13, 492, 362, 257, 979, 19866, 322, 264, 1411, 1011, 1252, 11, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.19754733291326784, "compression_ratio": 1.6754385964912282, "no_speech_prob": 7.179885869845748e-06}, {"id": 25, "seek": 16328, "start": 170.24, "end": 179.28, "text": " with, because we'll see soon why. We'll see that in a bit. Right now, let's focus on the fact that", "tokens": [50364, 19839, 16796, 3391, 366, 445, 257, 11121, 2085, 3614, 11, 412, 1935, 3480, 4956, 311, 13396, 11, 597, 286, 3986, 50712, 50712, 365, 11, 570, 321, 603, 536, 2321, 983, 13, 492, 603, 536, 300, 294, 257, 857, 13, 1779, 586, 11, 718, 311, 1879, 322, 264, 1186, 300, 51164, 51212, 321, 362, 341, 2063, 3209, 13, 407, 321, 362, 1936, 2531, 16679, 11, 558, 30, 4832, 22732, 322, 264, 558, 51560, 51560, 1011, 1252, 13, 821, 311, 257, 3247, 22732, 322, 264, 1411, 1011, 1252, 13, 492, 362, 257, 979, 19866, 322, 264, 1411, 1011, 1252, 11, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.19754733291326784, "compression_ratio": 1.6754385964912282, "no_speech_prob": 7.179885869845748e-06}, {"id": 26, "seek": 16328, "start": 180.24, "end": 187.2, "text": " we have this cost network. So we have basically similar modules, right? Sampler on the right", "tokens": [50364, 19839, 16796, 3391, 366, 445, 257, 11121, 2085, 3614, 11, 412, 1935, 3480, 4956, 311, 13396, 11, 597, 286, 3986, 50712, 50712, 365, 11, 570, 321, 603, 536, 2321, 983, 13, 492, 603, 536, 300, 294, 257, 857, 13, 1779, 586, 11, 718, 311, 1879, 322, 264, 1186, 300, 51164, 51212, 321, 362, 341, 2063, 3209, 13, 407, 321, 362, 1936, 2531, 16679, 11, 558, 30, 4832, 22732, 322, 264, 558, 51560, 51560, 1011, 1252, 13, 821, 311, 257, 3247, 22732, 322, 264, 1411, 1011, 1252, 13, 492, 362, 257, 979, 19866, 322, 264, 1411, 1011, 1252, 11, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.19754733291326784, "compression_ratio": 1.6754385964912282, "no_speech_prob": 7.179885869845748e-06}, {"id": 27, "seek": 16328, "start": 187.2, "end": 191.44, "text": " hand side. There's a sampler on the left hand side. We have a decoder on the left hand side,", "tokens": [50364, 19839, 16796, 3391, 366, 445, 257, 11121, 2085, 3614, 11, 412, 1935, 3480, 4956, 311, 13396, 11, 597, 286, 3986, 50712, 50712, 365, 11, 570, 321, 603, 536, 2321, 983, 13, 492, 603, 536, 300, 294, 257, 857, 13, 1779, 586, 11, 718, 311, 1879, 322, 264, 1186, 300, 51164, 51212, 321, 362, 341, 2063, 3209, 13, 407, 321, 362, 1936, 2531, 16679, 11, 558, 30, 4832, 22732, 322, 264, 558, 51560, 51560, 1011, 1252, 13, 821, 311, 257, 3247, 22732, 322, 264, 1411, 1011, 1252, 13, 492, 362, 257, 979, 19866, 322, 264, 1411, 1011, 1252, 11, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.19754733291326784, "compression_ratio": 1.6754385964912282, "no_speech_prob": 7.179885869845748e-06}, {"id": 28, "seek": 19144, "start": 191.44, "end": 198.8, "text": " which is basically generating something. Since z is considered a code, then we have a decoding step.", "tokens": [50364, 597, 307, 1936, 17746, 746, 13, 4162, 710, 307, 4888, 257, 3089, 11, 550, 321, 362, 257, 979, 8616, 1823, 13, 50732, 50776, 13813, 322, 264, 558, 1011, 1252, 11, 1670, 710, 390, 406, 257, 3089, 11, 457, 390, 2935, 364, 4846, 11, 50984, 50984, 550, 321, 362, 257, 19265, 13, 400, 300, 710, 307, 2935, 11, 337, 1365, 11, 257, 6889, 490, 257, 39148, 7316, 11, 51308, 51308, 365, 2710, 7316, 13, 400, 550, 300, 2031, 2385, 486, 312, 10833, 538, 341, 9105, 1701, 31774, 3209, 13, 51684, 51760], "temperature": 0.0, "avg_logprob": -0.2008070531098739, "compression_ratio": 1.6977777777777778, "no_speech_prob": 3.7820133002242073e-05}, {"id": 29, "seek": 19144, "start": 199.68, "end": 203.84, "text": " Whereas on the right hand side, since z was not a code, but was simply an input,", "tokens": [50364, 597, 307, 1936, 17746, 746, 13, 4162, 710, 307, 4888, 257, 3089, 11, 550, 321, 362, 257, 979, 8616, 1823, 13, 50732, 50776, 13813, 322, 264, 558, 1011, 1252, 11, 1670, 710, 390, 406, 257, 3089, 11, 457, 390, 2935, 364, 4846, 11, 50984, 50984, 550, 321, 362, 257, 19265, 13, 400, 300, 710, 307, 2935, 11, 337, 1365, 11, 257, 6889, 490, 257, 39148, 7316, 11, 51308, 51308, 365, 2710, 7316, 13, 400, 550, 300, 2031, 2385, 486, 312, 10833, 538, 341, 9105, 1701, 31774, 3209, 13, 51684, 51760], "temperature": 0.0, "avg_logprob": -0.2008070531098739, "compression_ratio": 1.6977777777777778, "no_speech_prob": 3.7820133002242073e-05}, {"id": 30, "seek": 19144, "start": 203.84, "end": 210.32, "text": " then we have a generator. And that z is simply, for example, a sample from a Gaussian distribution,", "tokens": [50364, 597, 307, 1936, 17746, 746, 13, 4162, 710, 307, 4888, 257, 3089, 11, 550, 321, 362, 257, 979, 8616, 1823, 13, 50732, 50776, 13813, 322, 264, 558, 1011, 1252, 11, 1670, 710, 390, 406, 257, 3089, 11, 457, 390, 2935, 364, 4846, 11, 50984, 50984, 550, 321, 362, 257, 19265, 13, 400, 300, 710, 307, 2935, 11, 337, 1365, 11, 257, 6889, 490, 257, 39148, 7316, 11, 51308, 51308, 365, 2710, 7316, 13, 400, 550, 300, 2031, 2385, 486, 312, 10833, 538, 341, 9105, 1701, 31774, 3209, 13, 51684, 51760], "temperature": 0.0, "avg_logprob": -0.2008070531098739, "compression_ratio": 1.6977777777777778, "no_speech_prob": 3.7820133002242073e-05}, {"id": 31, "seek": 19144, "start": 210.32, "end": 217.84, "text": " with normal distribution. And then that x hat will be generated by this initially untrained network.", "tokens": [50364, 597, 307, 1936, 17746, 746, 13, 4162, 710, 307, 4888, 257, 3089, 11, 550, 321, 362, 257, 979, 8616, 1823, 13, 50732, 50776, 13813, 322, 264, 558, 1011, 1252, 11, 1670, 710, 390, 406, 257, 3089, 11, 457, 390, 2935, 364, 4846, 11, 50984, 50984, 550, 321, 362, 257, 19265, 13, 400, 300, 710, 307, 2935, 11, 337, 1365, 11, 257, 6889, 490, 257, 39148, 7316, 11, 51308, 51308, 365, 2710, 7316, 13, 400, 550, 300, 2031, 2385, 486, 312, 10833, 538, 341, 9105, 1701, 31774, 3209, 13, 51684, 51760], "temperature": 0.0, "avg_logprob": -0.2008070531098739, "compression_ratio": 1.6977777777777778, "no_speech_prob": 3.7820133002242073e-05}, {"id": 32, "seek": 21784, "start": 217.84, "end": 226.24, "text": " The cost network instead has to figure out, it has to be a high cost if we feed that x hat,", "tokens": [50364, 440, 2063, 3209, 2602, 575, 281, 2573, 484, 11, 309, 575, 281, 312, 257, 1090, 2063, 498, 321, 3154, 300, 2031, 2385, 11, 50784, 50812, 264, 3344, 472, 11, 570, 321, 528, 281, 976, 11, 411, 321, 528, 281, 584, 11, 1954, 11, 341, 307, 257, 1578, 6889, 13, 1610, 2602, 11, 51184, 51184, 498, 321, 6889, 264, 7022, 472, 11, 498, 291, 483, 264, 3679, 281, 3048, 264, 7022, 472, 11, 321, 820, 362, 257, 2295, 2063, 11, 51532, 51532, 570, 300, 576, 2089, 505, 281, 2573, 484, 300, 321, 366, 767, 884, 257, 1578, 6889, 13, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.22683065077837775, "compression_ratio": 1.8507462686567164, "no_speech_prob": 2.134488931915257e-05}, {"id": 33, "seek": 21784, "start": 226.8, "end": 234.24, "text": " the blue one, because we want to give, like we want to say, oh, this is a bad sample. Or instead,", "tokens": [50364, 440, 2063, 3209, 2602, 575, 281, 2573, 484, 11, 309, 575, 281, 312, 257, 1090, 2063, 498, 321, 3154, 300, 2031, 2385, 11, 50784, 50812, 264, 3344, 472, 11, 570, 321, 528, 281, 976, 11, 411, 321, 528, 281, 584, 11, 1954, 11, 341, 307, 257, 1578, 6889, 13, 1610, 2602, 11, 51184, 51184, 498, 321, 6889, 264, 7022, 472, 11, 498, 291, 483, 264, 3679, 281, 3048, 264, 7022, 472, 11, 321, 820, 362, 257, 2295, 2063, 11, 51532, 51532, 570, 300, 576, 2089, 505, 281, 2573, 484, 300, 321, 366, 767, 884, 257, 1578, 6889, 13, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.22683065077837775, "compression_ratio": 1.8507462686567164, "no_speech_prob": 2.134488931915257e-05}, {"id": 34, "seek": 21784, "start": 234.24, "end": 241.2, "text": " if we sample the pink one, if you get the switch to select the pink one, we should have a low cost,", "tokens": [50364, 440, 2063, 3209, 2602, 575, 281, 2573, 484, 11, 309, 575, 281, 312, 257, 1090, 2063, 498, 321, 3154, 300, 2031, 2385, 11, 50784, 50812, 264, 3344, 472, 11, 570, 321, 528, 281, 976, 11, 411, 321, 528, 281, 584, 11, 1954, 11, 341, 307, 257, 1578, 6889, 13, 1610, 2602, 11, 51184, 51184, 498, 321, 6889, 264, 7022, 472, 11, 498, 291, 483, 264, 3679, 281, 3048, 264, 7022, 472, 11, 321, 820, 362, 257, 2295, 2063, 11, 51532, 51532, 570, 300, 576, 2089, 505, 281, 2573, 484, 300, 321, 366, 767, 884, 257, 1578, 6889, 13, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.22683065077837775, "compression_ratio": 1.8507462686567164, "no_speech_prob": 2.134488931915257e-05}, {"id": 35, "seek": 21784, "start": 241.2, "end": 246.48000000000002, "text": " because that would allow us to figure out that we are actually doing a bad sample.", "tokens": [50364, 440, 2063, 3209, 2602, 575, 281, 2573, 484, 11, 309, 575, 281, 312, 257, 1090, 2063, 498, 321, 3154, 300, 2031, 2385, 11, 50784, 50812, 264, 3344, 472, 11, 570, 321, 528, 281, 976, 11, 411, 321, 528, 281, 584, 11, 1954, 11, 341, 307, 257, 1578, 6889, 13, 1610, 2602, 11, 51184, 51184, 498, 321, 6889, 264, 7022, 472, 11, 498, 291, 483, 264, 3679, 281, 3048, 264, 7022, 472, 11, 321, 820, 362, 257, 2295, 2063, 11, 51532, 51532, 570, 300, 576, 2089, 505, 281, 2573, 484, 300, 321, 366, 767, 884, 257, 1578, 6889, 13, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.22683065077837775, "compression_ratio": 1.8507462686567164, "no_speech_prob": 2.134488931915257e-05}, {"id": 36, "seek": 24648, "start": 246.48, "end": 250.64, "text": " That we are actually doing, we have actually a true sample, a good sample.", "tokens": [50364, 663, 321, 366, 767, 884, 11, 321, 362, 767, 257, 2074, 6889, 11, 257, 665, 6889, 13, 50572, 50624, 407, 14611, 3319, 264, 8310, 295, 7705, 11, 321, 362, 300, 264, 19265, 11317, 452, 48994, 4846, 281, 710, 50916, 50948, 666, 341, 497, 77, 11, 597, 307, 264, 1901, 295, 264, 4846, 1901, 13, 407, 321, 362, 264, 48994, 4846, 11, 51180, 51224, 264, 7671, 472, 300, 307, 33318, 666, 264, 3380, 4846, 13, 407, 321, 362, 300, 7671, 710, 33318, 281, 2031, 2385, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.1291712522506714, "compression_ratio": 1.86096256684492, "no_speech_prob": 2.8300108169787563e-05}, {"id": 37, "seek": 24648, "start": 251.67999999999998, "end": 257.52, "text": " So summarizing the sequence of operations, we have that the generator maps my latent input to z", "tokens": [50364, 663, 321, 366, 767, 884, 11, 321, 362, 767, 257, 2074, 6889, 11, 257, 665, 6889, 13, 50572, 50624, 407, 14611, 3319, 264, 8310, 295, 7705, 11, 321, 362, 300, 264, 19265, 11317, 452, 48994, 4846, 281, 710, 50916, 50948, 666, 341, 497, 77, 11, 597, 307, 264, 1901, 295, 264, 4846, 1901, 13, 407, 321, 362, 264, 48994, 4846, 11, 51180, 51224, 264, 7671, 472, 300, 307, 33318, 666, 264, 3380, 4846, 13, 407, 321, 362, 300, 7671, 710, 33318, 281, 2031, 2385, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.1291712522506714, "compression_ratio": 1.86096256684492, "no_speech_prob": 2.8300108169787563e-05}, {"id": 38, "seek": 24648, "start": 258.15999999999997, "end": 262.8, "text": " into this Rn, which is the space of the input space. So we have the latent input,", "tokens": [50364, 663, 321, 366, 767, 884, 11, 321, 362, 767, 257, 2074, 6889, 11, 257, 665, 6889, 13, 50572, 50624, 407, 14611, 3319, 264, 8310, 295, 7705, 11, 321, 362, 300, 264, 19265, 11317, 452, 48994, 4846, 281, 710, 50916, 50948, 666, 341, 497, 77, 11, 597, 307, 264, 1901, 295, 264, 4846, 1901, 13, 407, 321, 362, 264, 48994, 4846, 11, 51180, 51224, 264, 7671, 472, 300, 307, 33318, 666, 264, 3380, 4846, 13, 407, 321, 362, 300, 7671, 710, 33318, 281, 2031, 2385, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.1291712522506714, "compression_ratio": 1.86096256684492, "no_speech_prob": 2.8300108169787563e-05}, {"id": 39, "seek": 24648, "start": 263.68, "end": 270.0, "text": " the orange one that is mapped into the original input. So we have that orange z mapped to x hat", "tokens": [50364, 663, 321, 366, 767, 884, 11, 321, 362, 767, 257, 2074, 6889, 11, 257, 665, 6889, 13, 50572, 50624, 407, 14611, 3319, 264, 8310, 295, 7705, 11, 321, 362, 300, 264, 19265, 11317, 452, 48994, 4846, 281, 710, 50916, 50948, 666, 341, 497, 77, 11, 597, 307, 264, 1901, 295, 264, 4846, 1901, 13, 407, 321, 362, 264, 48994, 4846, 11, 51180, 51224, 264, 7671, 472, 300, 307, 33318, 666, 264, 3380, 4846, 13, 407, 321, 362, 300, 7671, 710, 33318, 281, 2031, 2385, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.1291712522506714, "compression_ratio": 1.86096256684492, "no_speech_prob": 2.8300108169787563e-05}, {"id": 40, "seek": 27000, "start": 270.0, "end": 278.48, "text": " in blue. The top one is that it's a cost network in this case, which maps the input, which can be", "tokens": [50364, 294, 3344, 13, 440, 1192, 472, 307, 300, 309, 311, 257, 2063, 3209, 294, 341, 1389, 11, 597, 11317, 264, 4846, 11, 597, 393, 312, 50788, 50788, 264, 7022, 2031, 420, 264, 3344, 2385, 11, 264, 3344, 2031, 2385, 11, 597, 307, 33318, 281, 452, 2063, 13, 407, 294, 341, 1389, 11, 341, 2063, 11, 51212, 51240, 309, 307, 11, 1338, 11, 341, 2063, 10088, 307, 767, 257, 2063, 11, 411, 294, 4956, 311, 10686, 11, 309, 311, 516, 281, 312, 257, 3732, 11, 51560, 51636], "temperature": 0.0, "avg_logprob": -0.17161090807481247, "compression_ratio": 1.6875, "no_speech_prob": 3.786172965192236e-05}, {"id": 41, "seek": 27000, "start": 278.48, "end": 286.96, "text": " the pink x or the blue hat, the blue x hat, which is mapped to my cost. So in this case, this cost,", "tokens": [50364, 294, 3344, 13, 440, 1192, 472, 307, 300, 309, 311, 257, 2063, 3209, 294, 341, 1389, 11, 597, 11317, 264, 4846, 11, 597, 393, 312, 50788, 50788, 264, 7022, 2031, 420, 264, 3344, 2385, 11, 264, 3344, 2031, 2385, 11, 597, 307, 33318, 281, 452, 2063, 13, 407, 294, 341, 1389, 11, 341, 2063, 11, 51212, 51240, 309, 307, 11, 1338, 11, 341, 2063, 10088, 307, 767, 257, 2063, 11, 411, 294, 4956, 311, 10686, 11, 309, 311, 516, 281, 312, 257, 3732, 11, 51560, 51636], "temperature": 0.0, "avg_logprob": -0.17161090807481247, "compression_ratio": 1.6875, "no_speech_prob": 3.786172965192236e-05}, {"id": 42, "seek": 27000, "start": 287.52, "end": 293.92, "text": " it is, yeah, this cost module is actually a cost, like in Jan's diagram, it's going to be a square,", "tokens": [50364, 294, 3344, 13, 440, 1192, 472, 307, 300, 309, 311, 257, 2063, 3209, 294, 341, 1389, 11, 597, 11317, 264, 4846, 11, 597, 393, 312, 50788, 50788, 264, 7022, 2031, 420, 264, 3344, 2385, 11, 264, 3344, 2031, 2385, 11, 597, 307, 33318, 281, 452, 2063, 13, 407, 294, 341, 1389, 11, 341, 2063, 11, 51212, 51240, 309, 307, 11, 1338, 11, 341, 2063, 10088, 307, 767, 257, 2063, 11, 411, 294, 4956, 311, 10686, 11, 309, 311, 516, 281, 312, 257, 3732, 11, 51560, 51636], "temperature": 0.0, "avg_logprob": -0.17161090807481247, "compression_ratio": 1.6875, "no_speech_prob": 3.786172965192236e-05}, {"id": 43, "seek": 29392, "start": 293.92, "end": 302.24, "text": " okay, which outputs a scalar. This scalar will be a high value, a large number, positive large number,", "tokens": [50364, 1392, 11, 597, 23930, 257, 39684, 13, 639, 39684, 486, 312, 257, 1090, 2158, 11, 257, 2416, 1230, 11, 3353, 2416, 1230, 11, 50780, 50780, 498, 264, 4846, 307, 257, 7592, 4846, 13, 400, 309, 820, 312, 257, 2295, 1230, 11, 1391, 493, 281, 4018, 11, 498, 321, 767, 51148, 51148, 362, 264, 4846, 1348, 490, 264, 7022, 1252, 11, 264, 957, 1252, 13, 1033, 13, 400, 550, 577, 360, 321, 3847, 341, 1185, 30, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.1359595165976995, "compression_ratio": 1.5873015873015872, "no_speech_prob": 7.342474418692291e-05}, {"id": 44, "seek": 29392, "start": 302.24, "end": 309.6, "text": " if the input is a fake input. And it should be a low number, probably up to zero, if we actually", "tokens": [50364, 1392, 11, 597, 23930, 257, 39684, 13, 639, 39684, 486, 312, 257, 1090, 2158, 11, 257, 2416, 1230, 11, 3353, 2416, 1230, 11, 50780, 50780, 498, 264, 4846, 307, 257, 7592, 4846, 13, 400, 309, 820, 312, 257, 2295, 1230, 11, 1391, 493, 281, 4018, 11, 498, 321, 767, 51148, 51148, 362, 264, 4846, 1348, 490, 264, 7022, 1252, 11, 264, 957, 1252, 13, 1033, 13, 400, 550, 577, 360, 321, 3847, 341, 1185, 30, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.1359595165976995, "compression_ratio": 1.5873015873015872, "no_speech_prob": 7.342474418692291e-05}, {"id": 45, "seek": 29392, "start": 309.6, "end": 318.32, "text": " have the input coming from the pink side, the real side. Okay. And then how do we train this system?", "tokens": [50364, 1392, 11, 597, 23930, 257, 39684, 13, 639, 39684, 486, 312, 257, 1090, 2158, 11, 257, 2416, 1230, 11, 3353, 2416, 1230, 11, 50780, 50780, 498, 264, 4846, 307, 257, 7592, 4846, 13, 400, 309, 820, 312, 257, 2295, 1230, 11, 1391, 493, 281, 4018, 11, 498, 321, 767, 51148, 51148, 362, 264, 4846, 1348, 490, 264, 7022, 1252, 11, 264, 957, 1252, 13, 1033, 13, 400, 550, 577, 360, 321, 3847, 341, 1185, 30, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.1359595165976995, "compression_ratio": 1.5873015873015872, "no_speech_prob": 7.342474418692291e-05}, {"id": 46, "seek": 31832, "start": 318.32, "end": 326.48, "text": " So the system will be trained with different gradients. So the cost network will be trained", "tokens": [50364, 407, 264, 1185, 486, 312, 8895, 365, 819, 2771, 2448, 13, 407, 264, 2063, 3209, 486, 312, 8895, 50772, 50804, 294, 1668, 281, 362, 2295, 2063, 337, 15743, 300, 366, 7022, 293, 257, 1090, 2063, 337, 15743, 300, 366, 3344, 13, 1033, 13, 51172, 51200, 407, 337, 1365, 11, 291, 393, 519, 466, 341, 11, 498, 291, 576, 362, 411, 257, 20828, 1639, 294, 341, 1389, 11, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.15124910650118975, "compression_ratio": 1.6764705882352942, "no_speech_prob": 2.811543026837171e-06}, {"id": 47, "seek": 31832, "start": 327.12, "end": 334.48, "text": " in order to have low cost for inputs that are pink and a high cost for inputs that are blue. Okay.", "tokens": [50364, 407, 264, 1185, 486, 312, 8895, 365, 819, 2771, 2448, 13, 407, 264, 2063, 3209, 486, 312, 8895, 50772, 50804, 294, 1668, 281, 362, 2295, 2063, 337, 15743, 300, 366, 7022, 293, 257, 1090, 2063, 337, 15743, 300, 366, 3344, 13, 1033, 13, 51172, 51200, 407, 337, 1365, 11, 291, 393, 519, 466, 341, 11, 498, 291, 576, 362, 411, 257, 20828, 1639, 294, 341, 1389, 11, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.15124910650118975, "compression_ratio": 1.6764705882352942, "no_speech_prob": 2.811543026837171e-06}, {"id": 48, "seek": 31832, "start": 335.03999999999996, "end": 341.76, "text": " So for example, you can think about this, if you would have like a discriminator in this case,", "tokens": [50364, 407, 264, 1185, 486, 312, 8895, 365, 819, 2771, 2448, 13, 407, 264, 2063, 3209, 486, 312, 8895, 50772, 50804, 294, 1668, 281, 362, 2295, 2063, 337, 15743, 300, 366, 7022, 293, 257, 1090, 2063, 337, 15743, 300, 366, 3344, 13, 1033, 13, 51172, 51200, 407, 337, 1365, 11, 291, 393, 519, 466, 341, 11, 498, 291, 576, 362, 411, 257, 20828, 1639, 294, 341, 1389, 11, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.15124910650118975, "compression_ratio": 1.6764705882352942, "no_speech_prob": 2.811543026837171e-06}, {"id": 49, "seek": 34176, "start": 341.76, "end": 348.48, "text": " you may have, you may think about this as a, you know, two classes classification problem, you try", "tokens": [50364, 291, 815, 362, 11, 291, 815, 519, 466, 341, 382, 257, 11, 291, 458, 11, 732, 5359, 21538, 1154, 11, 291, 853, 50700, 50700, 281, 483, 4018, 337, 2031, 7022, 11, 7022, 2031, 11, 293, 257, 472, 337, 264, 3344, 2031, 13, 492, 751, 257, 857, 466, 983, 300, 311, 1578, 281, 764, 51192, 51192, 341, 4018, 472, 5598, 294, 257, 1150, 11, 294, 257, 1150, 13, 583, 5911, 11, 321, 445, 528, 341, 3209, 281, 1466, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.14036662490279586, "compression_ratio": 1.5775401069518717, "no_speech_prob": 7.88073884905316e-06}, {"id": 50, "seek": 34176, "start": 348.48, "end": 358.32, "text": " to get zero for x pink, pink x, and a one for the blue x. We talk a bit about why that's bad to use", "tokens": [50364, 291, 815, 362, 11, 291, 815, 519, 466, 341, 382, 257, 11, 291, 458, 11, 732, 5359, 21538, 1154, 11, 291, 853, 50700, 50700, 281, 483, 4018, 337, 2031, 7022, 11, 7022, 2031, 11, 293, 257, 472, 337, 264, 3344, 2031, 13, 492, 751, 257, 857, 466, 983, 300, 311, 1578, 281, 764, 51192, 51192, 341, 4018, 472, 5598, 294, 257, 1150, 11, 294, 257, 1150, 13, 583, 5911, 11, 321, 445, 528, 341, 3209, 281, 1466, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.14036662490279586, "compression_ratio": 1.5775401069518717, "no_speech_prob": 7.88073884905316e-06}, {"id": 51, "seek": 34176, "start": 358.32, "end": 366.32, "text": " this zero one output in a second, in a second. But otherwise, we just want this network to learn", "tokens": [50364, 291, 815, 362, 11, 291, 815, 519, 466, 341, 382, 257, 11, 291, 458, 11, 732, 5359, 21538, 1154, 11, 291, 853, 50700, 50700, 281, 483, 4018, 337, 2031, 7022, 11, 7022, 2031, 11, 293, 257, 472, 337, 264, 3344, 2031, 13, 492, 751, 257, 857, 466, 983, 300, 311, 1578, 281, 764, 51192, 51192, 341, 4018, 472, 5598, 294, 257, 1150, 11, 294, 257, 1150, 13, 583, 5911, 11, 321, 445, 528, 341, 3209, 281, 1466, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.14036662490279586, "compression_ratio": 1.5775401069518717, "no_speech_prob": 7.88073884905316e-06}, {"id": 52, "seek": 36632, "start": 366.32, "end": 372.96, "text": " this cost. Okay. So let's figure out how this works in a diagram. Do you remember how we were", "tokens": [50364, 341, 2063, 13, 1033, 13, 407, 718, 311, 2573, 484, 577, 341, 1985, 294, 257, 10686, 13, 1144, 291, 1604, 577, 321, 645, 50696, 50696, 2891, 365, 257, 3034, 1478, 484, 2058, 19866, 30, 2022, 257, 3034, 1478, 484, 2058, 19866, 11, 321, 645, 2891, 490, 50900, 50900, 264, 1411, 1011, 1252, 11, 558, 30, 492, 645, 8867, 257, 4846, 293, 550, 321, 645, 11, 370, 321, 645, 1940, 264, 4846, 11, 51164, 51164, 321, 645, 2684, 281, 264, 48994, 1901, 13, 492, 645, 2684, 341, 935, 570, 321, 366, 5127, 512, 5658, 13, 51472, 51472, 400, 550, 321, 645, 1242, 646, 281, 264, 3380, 935, 13, 1396, 321, 645, 1382, 281, 483, 729, 2793, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.08715046954756024, "compression_ratio": 1.9153225806451613, "no_speech_prob": 1.2026695912936702e-05}, {"id": 53, "seek": 36632, "start": 372.96, "end": 377.04, "text": " starting with a variational out encoder? With a variational out encoder, we were starting from", "tokens": [50364, 341, 2063, 13, 1033, 13, 407, 718, 311, 2573, 484, 577, 341, 1985, 294, 257, 10686, 13, 1144, 291, 1604, 577, 321, 645, 50696, 50696, 2891, 365, 257, 3034, 1478, 484, 2058, 19866, 30, 2022, 257, 3034, 1478, 484, 2058, 19866, 11, 321, 645, 2891, 490, 50900, 50900, 264, 1411, 1011, 1252, 11, 558, 30, 492, 645, 8867, 257, 4846, 293, 550, 321, 645, 11, 370, 321, 645, 1940, 264, 4846, 11, 51164, 51164, 321, 645, 2684, 281, 264, 48994, 1901, 13, 492, 645, 2684, 341, 935, 570, 321, 366, 5127, 512, 5658, 13, 51472, 51472, 400, 550, 321, 645, 1242, 646, 281, 264, 3380, 935, 13, 1396, 321, 645, 1382, 281, 483, 729, 2793, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.08715046954756024, "compression_ratio": 1.9153225806451613, "no_speech_prob": 1.2026695912936702e-05}, {"id": 54, "seek": 36632, "start": 377.04, "end": 382.32, "text": " the left hand side, right? We were picking a input and then we were, so we were taking the input,", "tokens": [50364, 341, 2063, 13, 1033, 13, 407, 718, 311, 2573, 484, 577, 341, 1985, 294, 257, 10686, 13, 1144, 291, 1604, 577, 321, 645, 50696, 50696, 2891, 365, 257, 3034, 1478, 484, 2058, 19866, 30, 2022, 257, 3034, 1478, 484, 2058, 19866, 11, 321, 645, 2891, 490, 50900, 50900, 264, 1411, 1011, 1252, 11, 558, 30, 492, 645, 8867, 257, 4846, 293, 550, 321, 645, 11, 370, 321, 645, 1940, 264, 4846, 11, 51164, 51164, 321, 645, 2684, 281, 264, 48994, 1901, 13, 492, 645, 2684, 341, 935, 570, 321, 366, 5127, 512, 5658, 13, 51472, 51472, 400, 550, 321, 645, 1242, 646, 281, 264, 3380, 935, 13, 1396, 321, 645, 1382, 281, 483, 729, 2793, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.08715046954756024, "compression_ratio": 1.9153225806451613, "no_speech_prob": 1.2026695912936702e-05}, {"id": 55, "seek": 36632, "start": 382.32, "end": 388.48, "text": " we were moving to the latent space. We were moving this point because we are adding some noise.", "tokens": [50364, 341, 2063, 13, 1033, 13, 407, 718, 311, 2573, 484, 577, 341, 1985, 294, 257, 10686, 13, 1144, 291, 1604, 577, 321, 645, 50696, 50696, 2891, 365, 257, 3034, 1478, 484, 2058, 19866, 30, 2022, 257, 3034, 1478, 484, 2058, 19866, 11, 321, 645, 2891, 490, 50900, 50900, 264, 1411, 1011, 1252, 11, 558, 30, 492, 645, 8867, 257, 4846, 293, 550, 321, 645, 11, 370, 321, 645, 1940, 264, 4846, 11, 51164, 51164, 321, 645, 2684, 281, 264, 48994, 1901, 13, 492, 645, 2684, 341, 935, 570, 321, 366, 5127, 512, 5658, 13, 51472, 51472, 400, 550, 321, 645, 1242, 646, 281, 264, 3380, 935, 13, 1396, 321, 645, 1382, 281, 483, 729, 2793, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.08715046954756024, "compression_ratio": 1.9153225806451613, "no_speech_prob": 1.2026695912936702e-05}, {"id": 56, "seek": 36632, "start": 388.48, "end": 392.56, "text": " And then we were getting back to the original point. Then we were trying to get those points", "tokens": [50364, 341, 2063, 13, 1033, 13, 407, 718, 311, 2573, 484, 577, 341, 1985, 294, 257, 10686, 13, 1144, 291, 1604, 577, 321, 645, 50696, 50696, 2891, 365, 257, 3034, 1478, 484, 2058, 19866, 30, 2022, 257, 3034, 1478, 484, 2058, 19866, 11, 321, 645, 2891, 490, 50900, 50900, 264, 1411, 1011, 1252, 11, 558, 30, 492, 645, 8867, 257, 4846, 293, 550, 321, 645, 11, 370, 321, 645, 1940, 264, 4846, 11, 51164, 51164, 321, 645, 2684, 281, 264, 48994, 1901, 13, 492, 645, 2684, 341, 935, 570, 321, 366, 5127, 512, 5658, 13, 51472, 51472, 400, 550, 321, 645, 1242, 646, 281, 264, 3380, 935, 13, 1396, 321, 645, 1382, 281, 483, 729, 2793, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.08715046954756024, "compression_ratio": 1.9153225806451613, "no_speech_prob": 1.2026695912936702e-05}, {"id": 57, "seek": 39256, "start": 392.56, "end": 398.8, "text": " close together by using the reconstruction laws. And then we were trying to set some structure in", "tokens": [50364, 1998, 1214, 538, 1228, 264, 31565, 6064, 13, 400, 550, 321, 645, 1382, 281, 992, 512, 3877, 294, 50676, 50676, 264, 48994, 1901, 538, 1228, 300, 4972, 30867, 456, 13, 1033, 13, 7156, 337, 264, 3874, 11, 264, 1337, 1166, 51060, 51060, 661, 1252, 295, 428, 3209, 11, 321, 434, 516, 281, 312, 2891, 490, 264, 558, 1011, 1252, 13, 407, 321, 1888, 51288, 51288, 257, 6889, 11, 257, 4974, 1230, 11, 718, 311, 584, 14034, 13, 492, 3154, 300, 807, 257, 19265, 293, 321, 483, 300, 3344, 2031, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.1375435347198158, "compression_ratio": 1.6092436974789917, "no_speech_prob": 1.3782359928882215e-05}, {"id": 58, "seek": 39256, "start": 398.8, "end": 406.48, "text": " the latent space by using that relative entropy there. Okay. Instead for the gun, the generative", "tokens": [50364, 1998, 1214, 538, 1228, 264, 31565, 6064, 13, 400, 550, 321, 645, 1382, 281, 992, 512, 3877, 294, 50676, 50676, 264, 48994, 1901, 538, 1228, 300, 4972, 30867, 456, 13, 1033, 13, 7156, 337, 264, 3874, 11, 264, 1337, 1166, 51060, 51060, 661, 1252, 295, 428, 3209, 11, 321, 434, 516, 281, 312, 2891, 490, 264, 558, 1011, 1252, 13, 407, 321, 1888, 51288, 51288, 257, 6889, 11, 257, 4974, 1230, 11, 718, 311, 584, 14034, 13, 492, 3154, 300, 807, 257, 19265, 293, 321, 483, 300, 3344, 2031, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.1375435347198158, "compression_ratio": 1.6092436974789917, "no_speech_prob": 1.3782359928882215e-05}, {"id": 59, "seek": 39256, "start": 406.48, "end": 411.04, "text": " other side of your network, we're going to be starting from the right hand side. So we pick", "tokens": [50364, 1998, 1214, 538, 1228, 264, 31565, 6064, 13, 400, 550, 321, 645, 1382, 281, 992, 512, 3877, 294, 50676, 50676, 264, 48994, 1901, 538, 1228, 300, 4972, 30867, 456, 13, 1033, 13, 7156, 337, 264, 3874, 11, 264, 1337, 1166, 51060, 51060, 661, 1252, 295, 428, 3209, 11, 321, 434, 516, 281, 312, 2891, 490, 264, 558, 1011, 1252, 13, 407, 321, 1888, 51288, 51288, 257, 6889, 11, 257, 4974, 1230, 11, 718, 311, 584, 14034, 13, 492, 3154, 300, 807, 257, 19265, 293, 321, 483, 300, 3344, 2031, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.1375435347198158, "compression_ratio": 1.6092436974789917, "no_speech_prob": 1.3782359928882215e-05}, {"id": 60, "seek": 39256, "start": 411.04, "end": 419.2, "text": " a sample, a random number, let's say 42. We feed that through a generator and we get that blue x", "tokens": [50364, 1998, 1214, 538, 1228, 264, 31565, 6064, 13, 400, 550, 321, 645, 1382, 281, 992, 512, 3877, 294, 50676, 50676, 264, 48994, 1901, 538, 1228, 300, 4972, 30867, 456, 13, 1033, 13, 7156, 337, 264, 3874, 11, 264, 1337, 1166, 51060, 51060, 661, 1252, 295, 428, 3209, 11, 321, 434, 516, 281, 312, 2891, 490, 264, 558, 1011, 1252, 13, 407, 321, 1888, 51288, 51288, 257, 6889, 11, 257, 4974, 1230, 11, 718, 311, 584, 14034, 13, 492, 3154, 300, 807, 257, 19265, 293, 321, 483, 300, 3344, 2031, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.1375435347198158, "compression_ratio": 1.6092436974789917, "no_speech_prob": 1.3782359928882215e-05}, {"id": 61, "seek": 41920, "start": 419.2, "end": 428.71999999999997, "text": " hat over there. Then we're going to be training in another network in order to be coming up with a", "tokens": [50364, 2385, 670, 456, 13, 1396, 321, 434, 516, 281, 312, 3097, 294, 1071, 3209, 294, 1668, 281, 312, 1348, 493, 365, 257, 50840, 50840, 1090, 2158, 337, 300, 3344, 6889, 13, 1396, 321, 434, 516, 281, 1888, 1071, 1783, 11, 718, 311, 584, 257, 7022, 1783, 294, 341, 1389, 51184, 51184, 322, 264, 2767, 558, 295, 264, 25165, 11, 597, 307, 516, 281, 312, 40953, 586, 281, 362, 257, 2295, 2063, 13, 51452, 51524], "temperature": 0.0, "avg_logprob": -0.093513990703382, "compression_ratio": 1.6457142857142857, "no_speech_prob": 1.3395284440775868e-05}, {"id": 62, "seek": 41920, "start": 428.71999999999997, "end": 435.59999999999997, "text": " high value for that blue sample. Then we're going to pick another X, let's say a pink X in this case", "tokens": [50364, 2385, 670, 456, 13, 1396, 321, 434, 516, 281, 312, 3097, 294, 1071, 3209, 294, 1668, 281, 312, 1348, 493, 365, 257, 50840, 50840, 1090, 2158, 337, 300, 3344, 6889, 13, 1396, 321, 434, 516, 281, 1888, 1071, 1783, 11, 718, 311, 584, 257, 7022, 1783, 294, 341, 1389, 51184, 51184, 322, 264, 2767, 558, 295, 264, 25165, 11, 597, 307, 516, 281, 312, 40953, 586, 281, 362, 257, 2295, 2063, 13, 51452, 51524], "temperature": 0.0, "avg_logprob": -0.093513990703382, "compression_ratio": 1.6457142857142857, "no_speech_prob": 1.3395284440775868e-05}, {"id": 63, "seek": 41920, "start": 435.59999999999997, "end": 440.96, "text": " on the bottom right of the spiral, which is going to be enforced now to have a low cost.", "tokens": [50364, 2385, 670, 456, 13, 1396, 321, 434, 516, 281, 312, 3097, 294, 1071, 3209, 294, 1668, 281, 312, 1348, 493, 365, 257, 50840, 50840, 1090, 2158, 337, 300, 3344, 6889, 13, 1396, 321, 434, 516, 281, 1888, 1071, 1783, 11, 718, 311, 584, 257, 7022, 1783, 294, 341, 1389, 51184, 51184, 322, 264, 2767, 558, 295, 264, 25165, 11, 597, 307, 516, 281, 312, 40953, 586, 281, 362, 257, 2295, 2063, 13, 51452, 51524], "temperature": 0.0, "avg_logprob": -0.093513990703382, "compression_ratio": 1.6457142857142857, "no_speech_prob": 1.3395284440775868e-05}, {"id": 64, "seek": 44096, "start": 440.96, "end": 450.08, "text": " So this is pretty much like a first initial big picture about how this system works. So let me", "tokens": [50364, 407, 341, 307, 1238, 709, 411, 257, 700, 5883, 955, 3036, 466, 577, 341, 1185, 1985, 13, 407, 718, 385, 50820, 50820, 853, 281, 976, 291, 732, 544, 37547, 13, 407, 341, 307, 411, 264, 733, 295, 21988, 293, 5056, 51192, 51192, 7302, 11, 411, 18894, 7123, 11, 293, 550, 264, 5056, 7123, 13, 823, 286, 478, 516, 281, 312, 1382, 51528, 51528], "temperature": 0.0, "avg_logprob": -0.19905933967003456, "compression_ratio": 1.6306818181818181, "no_speech_prob": 7.065310910547851e-06}, {"id": 65, "seek": 44096, "start": 450.08, "end": 457.52, "text": " try to give you two more interpretations. So this is like the kind of definitions and visual", "tokens": [50364, 407, 341, 307, 1238, 709, 411, 257, 700, 5883, 955, 3036, 466, 577, 341, 1185, 1985, 13, 407, 718, 385, 50820, 50820, 853, 281, 976, 291, 732, 544, 37547, 13, 407, 341, 307, 411, 264, 733, 295, 21988, 293, 5056, 51192, 51192, 7302, 11, 411, 18894, 7123, 11, 293, 550, 264, 5056, 7123, 13, 823, 286, 478, 516, 281, 312, 1382, 51528, 51528], "temperature": 0.0, "avg_logprob": -0.19905933967003456, "compression_ratio": 1.6306818181818181, "no_speech_prob": 7.065310910547851e-06}, {"id": 66, "seek": 44096, "start": 457.52, "end": 464.24, "text": " interpret, like mathematical definition, and then the visual definition. Now I'm going to be trying", "tokens": [50364, 407, 341, 307, 1238, 709, 411, 257, 700, 5883, 955, 3036, 466, 577, 341, 1185, 1985, 13, 407, 718, 385, 50820, 50820, 853, 281, 976, 291, 732, 544, 37547, 13, 407, 341, 307, 411, 264, 733, 295, 21988, 293, 5056, 51192, 51192, 7302, 11, 411, 18894, 7123, 11, 293, 550, 264, 5056, 7123, 13, 823, 286, 478, 516, 281, 312, 1382, 51528, 51528], "temperature": 0.0, "avg_logprob": -0.19905933967003456, "compression_ratio": 1.6306818181818181, "no_speech_prob": 7.065310910547851e-06}, {"id": 67, "seek": 46424, "start": 464.24, "end": 470.96000000000004, "text": " to give you a few interpretations, which I pretty like, and are going to make me sound like a fool,", "tokens": [50364, 281, 976, 291, 257, 1326, 37547, 11, 597, 286, 1238, 411, 11, 293, 366, 516, 281, 652, 385, 1626, 411, 257, 7979, 11, 50700, 50700, 457, 286, 669, 257, 7979, 13, 407, 11, 291, 458, 11, 286, 445, 352, 337, 309, 13, 407, 291, 393, 519, 466, 264, 19265, 382, 885, 257, 51072, 51072, 10003, 293, 4412, 286, 486, 312, 1228, 512, 2296, 10003, 11982, 13, 1033, 13, 407, 286, 478, 257, 2296, 10003, 586, 51460, 51460, 293, 286, 669, 294, 264, 7377, 295, 10705, 293, 286, 478, 516, 281, 312, 1382, 281, 652, 512, 7592, 1460, 13, 1033, 13, 1436, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.14999484107607886, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.0004021005006507039}, {"id": 68, "seek": 46424, "start": 470.96000000000004, "end": 478.40000000000003, "text": " but I am a fool. So, you know, I just go for it. So you can think about the generator as being a", "tokens": [50364, 281, 976, 291, 257, 1326, 37547, 11, 597, 286, 1238, 411, 11, 293, 366, 516, 281, 652, 385, 1626, 411, 257, 7979, 11, 50700, 50700, 457, 286, 669, 257, 7979, 13, 407, 11, 291, 458, 11, 286, 445, 352, 337, 309, 13, 407, 291, 393, 519, 466, 264, 19265, 382, 885, 257, 51072, 51072, 10003, 293, 4412, 286, 486, 312, 1228, 512, 2296, 10003, 11982, 13, 1033, 13, 407, 286, 478, 257, 2296, 10003, 586, 51460, 51460, 293, 286, 669, 294, 264, 7377, 295, 10705, 293, 286, 478, 516, 281, 312, 1382, 281, 652, 512, 7592, 1460, 13, 1033, 13, 1436, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.14999484107607886, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.0004021005006507039}, {"id": 69, "seek": 46424, "start": 478.40000000000003, "end": 486.16, "text": " Italian and therefore I will be using some proper Italian accent. Okay. So I'm a proper Italian now", "tokens": [50364, 281, 976, 291, 257, 1326, 37547, 11, 597, 286, 1238, 411, 11, 293, 366, 516, 281, 652, 385, 1626, 411, 257, 7979, 11, 50700, 50700, 457, 286, 669, 257, 7979, 13, 407, 11, 291, 458, 11, 286, 445, 352, 337, 309, 13, 407, 291, 393, 519, 466, 264, 19265, 382, 885, 257, 51072, 51072, 10003, 293, 4412, 286, 486, 312, 1228, 512, 2296, 10003, 11982, 13, 1033, 13, 407, 286, 478, 257, 2296, 10003, 586, 51460, 51460, 293, 286, 669, 294, 264, 7377, 295, 10705, 293, 286, 478, 516, 281, 312, 1382, 281, 652, 512, 7592, 1460, 13, 1033, 13, 1436, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.14999484107607886, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.0004021005006507039}, {"id": 70, "seek": 46424, "start": 486.16, "end": 492.56, "text": " and I am in the south of Italy and I'm going to be trying to make some fake money. Okay. Because", "tokens": [50364, 281, 976, 291, 257, 1326, 37547, 11, 597, 286, 1238, 411, 11, 293, 366, 516, 281, 652, 385, 1626, 411, 257, 7979, 11, 50700, 50700, 457, 286, 669, 257, 7979, 13, 407, 11, 291, 458, 11, 286, 445, 352, 337, 309, 13, 407, 291, 393, 519, 466, 264, 19265, 382, 885, 257, 51072, 51072, 10003, 293, 4412, 286, 486, 312, 1228, 512, 2296, 10003, 11982, 13, 1033, 13, 407, 286, 478, 257, 2296, 10003, 586, 51460, 51460, 293, 286, 669, 294, 264, 7377, 295, 10705, 293, 286, 478, 516, 281, 312, 1382, 281, 652, 512, 7592, 1460, 13, 1033, 13, 1436, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.14999484107607886, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.0004021005006507039}, {"id": 71, "seek": 49256, "start": 492.56, "end": 499.52, "text": " we are very good at that. So we make a fake money and then we go to Germany to get some,", "tokens": [50364, 321, 366, 588, 665, 412, 300, 13, 407, 321, 652, 257, 7592, 1460, 293, 550, 321, 352, 281, 7244, 281, 483, 512, 11, 50712, 50744, 281, 8110, 746, 13, 1033, 13, 492, 352, 281, 7244, 365, 341, 7592, 1460, 293, 550, 456, 307, 341, 50940, 50976, 6521, 561, 574, 412, 505, 13, 467, 311, 411, 11, 876, 11, 5546, 10003, 13, 639, 307, 7592, 1460, 13, 400, 370, 321, 393, 380, 51380, 51380, 534, 3067, 281, 11, 321, 393, 380, 534, 3067, 281, 2256, 1340, 11, 457, 1670, 321, 366, 10003, 11, 321, 362, 1738, 8916, 13, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.2749346364842783, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0003675056796055287}, {"id": 72, "seek": 49256, "start": 500.16, "end": 504.08, "text": " to purchase something. Okay. We go to Germany with this fake money and then there is this", "tokens": [50364, 321, 366, 588, 665, 412, 300, 13, 407, 321, 652, 257, 7592, 1460, 293, 550, 321, 352, 281, 7244, 281, 483, 512, 11, 50712, 50744, 281, 8110, 746, 13, 1033, 13, 492, 352, 281, 7244, 365, 341, 7592, 1460, 293, 550, 456, 307, 341, 50940, 50976, 6521, 561, 574, 412, 505, 13, 467, 311, 411, 11, 876, 11, 5546, 10003, 13, 639, 307, 7592, 1460, 13, 400, 370, 321, 393, 380, 51380, 51380, 534, 3067, 281, 11, 321, 393, 380, 534, 3067, 281, 2256, 1340, 11, 457, 1670, 321, 366, 10003, 11, 321, 362, 1738, 8916, 13, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.2749346364842783, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0003675056796055287}, {"id": 73, "seek": 49256, "start": 504.8, "end": 512.88, "text": " German people look at us. It's like, Oh, fucking Italian. This is fake money. And so we can't", "tokens": [50364, 321, 366, 588, 665, 412, 300, 13, 407, 321, 652, 257, 7592, 1460, 293, 550, 321, 352, 281, 7244, 281, 483, 512, 11, 50712, 50744, 281, 8110, 746, 13, 1033, 13, 492, 352, 281, 7244, 365, 341, 7592, 1460, 293, 550, 456, 307, 341, 50940, 50976, 6521, 561, 574, 412, 505, 13, 467, 311, 411, 11, 876, 11, 5546, 10003, 13, 639, 307, 7592, 1460, 13, 400, 370, 321, 393, 380, 51380, 51380, 534, 3067, 281, 11, 321, 393, 380, 534, 3067, 281, 2256, 1340, 11, 457, 1670, 321, 366, 10003, 11, 321, 362, 1738, 8916, 13, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.2749346364842783, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0003675056796055287}, {"id": 74, "seek": 49256, "start": 512.88, "end": 521.12, "text": " really manage to, we can't really manage to buy anything, but since we are Italian, we have Spikes.", "tokens": [50364, 321, 366, 588, 665, 412, 300, 13, 407, 321, 652, 257, 7592, 1460, 293, 550, 321, 352, 281, 7244, 281, 483, 512, 11, 50712, 50744, 281, 8110, 746, 13, 1033, 13, 492, 352, 281, 7244, 365, 341, 7592, 1460, 293, 550, 456, 307, 341, 50940, 50976, 6521, 561, 574, 412, 505, 13, 467, 311, 411, 11, 876, 11, 5546, 10003, 13, 639, 307, 7592, 1460, 13, 400, 370, 321, 393, 380, 51380, 51380, 534, 3067, 281, 11, 321, 393, 380, 534, 3067, 281, 2256, 1340, 11, 457, 1670, 321, 366, 10003, 11, 321, 362, 1738, 8916, 13, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.2749346364842783, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0003675056796055287}, {"id": 75, "seek": 52112, "start": 521.12, "end": 527.12, "text": " We have spies in the, okay. There are questions. Hold on. Maybe I'm offending people now chat.", "tokens": [50364, 492, 362, 45858, 294, 264, 11, 1392, 13, 821, 366, 1651, 13, 6962, 322, 13, 2704, 286, 478, 766, 2029, 561, 586, 5081, 13, 50664, 50664, 708, 311, 516, 322, 30, 876, 11, 1392, 13, 509, 434, 9929, 264, 551, 13, 8561, 13, 1033, 13, 407, 286, 390, 406, 766, 2029, 2878, 13, 51044, 51044, 21320, 13, 1033, 13, 407, 11, 2232, 11, 321, 362, 257, 20752, 646, 294, 7244, 293, 264, 20752, 307, 411, 5141, 646, 11, 2232, 11, 51444, 51444, 1280, 13, 1911, 11, 18775, 28545, 11, 291, 2729, 505, 264, 2085, 1460, 13, 1743, 309, 390, 370, 22518, 493, 13, 583, 309, 390, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.23636861280961471, "compression_ratio": 1.5267489711934157, "no_speech_prob": 0.0009774892823770642}, {"id": 76, "seek": 52112, "start": 527.12, "end": 534.72, "text": " What's going on? Oh, okay. You're enjoying the thing. Cool. Okay. So I was not offending anyone.", "tokens": [50364, 492, 362, 45858, 294, 264, 11, 1392, 13, 821, 366, 1651, 13, 6962, 322, 13, 2704, 286, 478, 766, 2029, 561, 586, 5081, 13, 50664, 50664, 708, 311, 516, 322, 30, 876, 11, 1392, 13, 509, 434, 9929, 264, 551, 13, 8561, 13, 1033, 13, 407, 286, 390, 406, 766, 2029, 2878, 13, 51044, 51044, 21320, 13, 1033, 13, 407, 11, 2232, 11, 321, 362, 257, 20752, 646, 294, 7244, 293, 264, 20752, 307, 411, 5141, 646, 11, 2232, 11, 51444, 51444, 1280, 13, 1911, 11, 18775, 28545, 11, 291, 2729, 505, 264, 2085, 1460, 13, 1743, 309, 390, 370, 22518, 493, 13, 583, 309, 390, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.23636861280961471, "compression_ratio": 1.5267489711934157, "no_speech_prob": 0.0009774892823770642}, {"id": 77, "seek": 52112, "start": 534.72, "end": 542.72, "text": " Fantastic. Okay. So, uh, we have a spy back in Germany and the spy is like calling back, uh,", "tokens": [50364, 492, 362, 45858, 294, 264, 11, 1392, 13, 821, 366, 1651, 13, 6962, 322, 13, 2704, 286, 478, 766, 2029, 561, 586, 5081, 13, 50664, 50664, 708, 311, 516, 322, 30, 876, 11, 1392, 13, 509, 434, 9929, 264, 551, 13, 8561, 13, 1033, 13, 407, 286, 390, 406, 766, 2029, 2878, 13, 51044, 51044, 21320, 13, 1033, 13, 407, 11, 2232, 11, 321, 362, 257, 20752, 646, 294, 7244, 293, 264, 20752, 307, 411, 5141, 646, 11, 2232, 11, 51444, 51444, 1280, 13, 1911, 11, 18775, 28545, 11, 291, 2729, 505, 264, 2085, 1460, 13, 1743, 309, 390, 370, 22518, 493, 13, 583, 309, 390, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.23636861280961471, "compression_ratio": 1.5267489711934157, "no_speech_prob": 0.0009774892823770642}, {"id": 78, "seek": 52112, "start": 542.72, "end": 549.12, "text": " home. Hey, mama Mia, you gave us the wrong money. Like it was so fucked up. But it was", "tokens": [50364, 492, 362, 45858, 294, 264, 11, 1392, 13, 821, 366, 1651, 13, 6962, 322, 13, 2704, 286, 478, 766, 2029, 561, 586, 5081, 13, 50664, 50664, 708, 311, 516, 322, 30, 876, 11, 1392, 13, 509, 434, 9929, 264, 551, 13, 8561, 13, 1033, 13, 407, 286, 390, 406, 766, 2029, 2878, 13, 51044, 51044, 21320, 13, 1033, 13, 407, 11, 2232, 11, 321, 362, 257, 20752, 646, 294, 7244, 293, 264, 20752, 307, 411, 5141, 646, 11, 2232, 11, 51444, 51444, 1280, 13, 1911, 11, 18775, 28545, 11, 291, 2729, 505, 264, 2085, 1460, 13, 1743, 309, 390, 370, 22518, 493, 13, 583, 309, 390, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.23636861280961471, "compression_ratio": 1.5267489711934157, "no_speech_prob": 0.0009774892823770642}, {"id": 79, "seek": 54912, "start": 549.12, "end": 556.24, "text": " just, you know, not proper. Okay. Okay. So yeah, chill, chill down. Right. We are like back again,", "tokens": [50364, 445, 11, 291, 458, 11, 406, 2296, 13, 1033, 13, 1033, 13, 407, 1338, 11, 11355, 11, 11355, 760, 13, 1779, 13, 492, 366, 411, 646, 797, 11, 50720, 50720, 1280, 13, 4019, 11, 437, 3169, 307, 341, 30, 663, 311, 445, 452, 1065, 3169, 13, 407, 321, 366, 646, 294, 10705, 13, 3301, 11, 291, 458, 11, 51096, 51096, 321, 366, 11, 321, 366, 1455, 11, 291, 434, 1075, 281, 652, 1270, 1481, 11, 2232, 11, 1523, 293, 1203, 13, 407, 321, 1633, 312, 1075, 51444, 51444, 281, 652, 1101, 1460, 13, 1779, 13, 407, 321, 853, 586, 281, 3191, 264, 721, 300, 527, 20752, 1907, 505, 13, 407, 321, 366, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.2232183259108971, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.0002638477017171681}, {"id": 80, "seek": 54912, "start": 556.24, "end": 563.76, "text": " home. Uh, what movie is this? That's just my own movie. So we are back in Italy. Um, you know,", "tokens": [50364, 445, 11, 291, 458, 11, 406, 2296, 13, 1033, 13, 1033, 13, 407, 1338, 11, 11355, 11, 11355, 760, 13, 1779, 13, 492, 366, 411, 646, 797, 11, 50720, 50720, 1280, 13, 4019, 11, 437, 3169, 307, 341, 30, 663, 311, 445, 452, 1065, 3169, 13, 407, 321, 366, 646, 294, 10705, 13, 3301, 11, 291, 458, 11, 51096, 51096, 321, 366, 11, 321, 366, 1455, 11, 291, 434, 1075, 281, 652, 1270, 1481, 11, 2232, 11, 1523, 293, 1203, 13, 407, 321, 1633, 312, 1075, 51444, 51444, 281, 652, 1101, 1460, 13, 1779, 13, 407, 321, 853, 586, 281, 3191, 264, 721, 300, 527, 20752, 1907, 505, 13, 407, 321, 366, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.2232183259108971, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.0002638477017171681}, {"id": 81, "seek": 54912, "start": 563.76, "end": 570.72, "text": " we are, we are making, you're able to make such nice, uh, art and everything. So we must be able", "tokens": [50364, 445, 11, 291, 458, 11, 406, 2296, 13, 1033, 13, 1033, 13, 407, 1338, 11, 11355, 11, 11355, 760, 13, 1779, 13, 492, 366, 411, 646, 797, 11, 50720, 50720, 1280, 13, 4019, 11, 437, 3169, 307, 341, 30, 663, 311, 445, 452, 1065, 3169, 13, 407, 321, 366, 646, 294, 10705, 13, 3301, 11, 291, 458, 11, 51096, 51096, 321, 366, 11, 321, 366, 1455, 11, 291, 434, 1075, 281, 652, 1270, 1481, 11, 2232, 11, 1523, 293, 1203, 13, 407, 321, 1633, 312, 1075, 51444, 51444, 281, 652, 1101, 1460, 13, 1779, 13, 407, 321, 853, 586, 281, 3191, 264, 721, 300, 527, 20752, 1907, 505, 13, 407, 321, 366, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.2232183259108971, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.0002638477017171681}, {"id": 82, "seek": 54912, "start": 570.72, "end": 576.88, "text": " to make better money. Right. So we try now to fix the things that our spy told us. So we are", "tokens": [50364, 445, 11, 291, 458, 11, 406, 2296, 13, 1033, 13, 1033, 13, 407, 1338, 11, 11355, 11, 11355, 760, 13, 1779, 13, 492, 366, 411, 646, 797, 11, 50720, 50720, 1280, 13, 4019, 11, 437, 3169, 307, 341, 30, 663, 311, 445, 452, 1065, 3169, 13, 407, 321, 366, 646, 294, 10705, 13, 3301, 11, 291, 458, 11, 51096, 51096, 321, 366, 11, 321, 366, 1455, 11, 291, 434, 1075, 281, 652, 1270, 1481, 11, 2232, 11, 1523, 293, 1203, 13, 407, 321, 1633, 312, 1075, 51444, 51444, 281, 652, 1101, 1460, 13, 1779, 13, 407, 321, 853, 586, 281, 3191, 264, 721, 300, 527, 20752, 1907, 505, 13, 407, 321, 366, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.2232183259108971, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.0002638477017171681}, {"id": 83, "seek": 57688, "start": 576.88, "end": 582.4, "text": " fixing the things that our spy told us. So we make a better money. We go back to Germany and try to", "tokens": [50364, 19442, 264, 721, 300, 527, 20752, 1907, 505, 13, 407, 321, 652, 257, 1101, 1460, 13, 492, 352, 646, 281, 7244, 293, 853, 281, 50640, 50640, 2256, 661, 721, 13, 400, 18116, 366, 411, 11, 7020, 11, 309, 311, 1101, 13, 467, 311, 7592, 13, 1033, 13, 1396, 797, 11, 291, 632, 257, 20752, 51120, 51120, 5141, 646, 760, 281, 10705, 293, 309, 311, 411, 11, 876, 11, 437, 366, 291, 884, 30, 4019, 11, 293, 550, 321, 434, 411, 11, 876, 11, 286, 51408, 51408, 1223, 13, 286, 393, 312, 11, 291, 458, 11, 293, 321, 366, 19442, 309, 11, 264, 1460, 11, 291, 458, 11, 321, 366, 1455, 2940, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.1760839213495669, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.00013250343909021467}, {"id": 84, "seek": 57688, "start": 582.4, "end": 592.0, "text": " buy other things. And Germans are like, huh, it's better. It's fake. Okay. Then again, you had a spy", "tokens": [50364, 19442, 264, 721, 300, 527, 20752, 1907, 505, 13, 407, 321, 652, 257, 1101, 1460, 13, 492, 352, 646, 281, 7244, 293, 853, 281, 50640, 50640, 2256, 661, 721, 13, 400, 18116, 366, 411, 11, 7020, 11, 309, 311, 1101, 13, 467, 311, 7592, 13, 1033, 13, 1396, 797, 11, 291, 632, 257, 20752, 51120, 51120, 5141, 646, 760, 281, 10705, 293, 309, 311, 411, 11, 876, 11, 437, 366, 291, 884, 30, 4019, 11, 293, 550, 321, 434, 411, 11, 876, 11, 286, 51408, 51408, 1223, 13, 286, 393, 312, 11, 291, 458, 11, 293, 321, 366, 19442, 309, 11, 264, 1460, 11, 291, 458, 11, 321, 366, 1455, 2940, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.1760839213495669, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.00013250343909021467}, {"id": 85, "seek": 57688, "start": 592.0, "end": 597.76, "text": " calling back down to Italy and it's like, Oh, what are you doing? Uh, and then we're like, Oh, I", "tokens": [50364, 19442, 264, 721, 300, 527, 20752, 1907, 505, 13, 407, 321, 652, 257, 1101, 1460, 13, 492, 352, 646, 281, 7244, 293, 853, 281, 50640, 50640, 2256, 661, 721, 13, 400, 18116, 366, 411, 11, 7020, 11, 309, 311, 1101, 13, 467, 311, 7592, 13, 1033, 13, 1396, 797, 11, 291, 632, 257, 20752, 51120, 51120, 5141, 646, 760, 281, 10705, 293, 309, 311, 411, 11, 876, 11, 437, 366, 291, 884, 30, 4019, 11, 293, 550, 321, 434, 411, 11, 876, 11, 286, 51408, 51408, 1223, 13, 286, 393, 312, 11, 291, 458, 11, 293, 321, 366, 19442, 309, 11, 264, 1460, 11, 291, 458, 11, 321, 366, 1455, 2940, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.1760839213495669, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.00013250343909021467}, {"id": 86, "seek": 57688, "start": 597.76, "end": 602.64, "text": " understand. I can be, you know, and we are fixing it, the money, you know, we are making several", "tokens": [50364, 19442, 264, 721, 300, 527, 20752, 1907, 505, 13, 407, 321, 652, 257, 1101, 1460, 13, 492, 352, 646, 281, 7244, 293, 853, 281, 50640, 50640, 2256, 661, 721, 13, 400, 18116, 366, 411, 11, 7020, 11, 309, 311, 1101, 13, 467, 311, 7592, 13, 1033, 13, 1396, 797, 11, 291, 632, 257, 20752, 51120, 51120, 5141, 646, 760, 281, 10705, 293, 309, 311, 411, 11, 876, 11, 437, 366, 291, 884, 30, 4019, 11, 293, 550, 321, 434, 411, 11, 876, 11, 286, 51408, 51408, 1223, 13, 286, 393, 312, 11, 291, 458, 11, 293, 321, 366, 19442, 309, 11, 264, 1460, 11, 291, 458, 11, 321, 366, 1455, 2940, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.1760839213495669, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.00013250343909021467}, {"id": 87, "seek": 60264, "start": 602.64, "end": 607.36, "text": " iterations of that. Like, so we try to make better and better, uh, versions of the money. Finally,", "tokens": [50364, 36540, 295, 300, 13, 1743, 11, 370, 321, 853, 281, 652, 1101, 293, 1101, 11, 2232, 11, 9606, 295, 264, 1460, 13, 6288, 11, 50600, 50676, 321, 352, 646, 281, 7244, 293, 341, 1389, 11, 7244, 11, 570, 436, 362, 1460, 11, 558, 30, 663, 321, 632, 11, 50884, 50884, 436, 362, 721, 321, 393, 2256, 13, 407, 321, 352, 646, 456, 293, 436, 366, 411, 11, 7020, 11, 51096, 51188, 309, 1542, 588, 665, 13, 883, 11, 286, 500, 380, 458, 577, 281, 652, 6521, 11982, 13, 286, 478, 2597, 13, 51380, 51448, 400, 370, 436, 3241, 264, 1460, 11, 558, 30, 1033, 13, 400, 341, 307, 577, 1238, 709, 613, 11, 2232, 11, 51676, 51716], "temperature": 0.0, "avg_logprob": -0.12063018414152771, "compression_ratio": 1.6827309236947792, "no_speech_prob": 1.496629192843102e-05}, {"id": 88, "seek": 60264, "start": 608.88, "end": 613.04, "text": " we go back to Germany and this case, Germany, because they have money, right? That we had,", "tokens": [50364, 36540, 295, 300, 13, 1743, 11, 370, 321, 853, 281, 652, 1101, 293, 1101, 11, 2232, 11, 9606, 295, 264, 1460, 13, 6288, 11, 50600, 50676, 321, 352, 646, 281, 7244, 293, 341, 1389, 11, 7244, 11, 570, 436, 362, 1460, 11, 558, 30, 663, 321, 632, 11, 50884, 50884, 436, 362, 721, 321, 393, 2256, 13, 407, 321, 352, 646, 456, 293, 436, 366, 411, 11, 7020, 11, 51096, 51188, 309, 1542, 588, 665, 13, 883, 11, 286, 500, 380, 458, 577, 281, 652, 6521, 11982, 13, 286, 478, 2597, 13, 51380, 51448, 400, 370, 436, 3241, 264, 1460, 11, 558, 30, 1033, 13, 400, 341, 307, 577, 1238, 709, 613, 11, 2232, 11, 51676, 51716], "temperature": 0.0, "avg_logprob": -0.12063018414152771, "compression_ratio": 1.6827309236947792, "no_speech_prob": 1.496629192843102e-05}, {"id": 89, "seek": 60264, "start": 613.04, "end": 617.28, "text": " they have things we can buy. So we go back there and they are like, huh,", "tokens": [50364, 36540, 295, 300, 13, 1743, 11, 370, 321, 853, 281, 652, 1101, 293, 1101, 11, 2232, 11, 9606, 295, 264, 1460, 13, 6288, 11, 50600, 50676, 321, 352, 646, 281, 7244, 293, 341, 1389, 11, 7244, 11, 570, 436, 362, 1460, 11, 558, 30, 663, 321, 632, 11, 50884, 50884, 436, 362, 721, 321, 393, 2256, 13, 407, 321, 352, 646, 456, 293, 436, 366, 411, 11, 7020, 11, 51096, 51188, 309, 1542, 588, 665, 13, 883, 11, 286, 500, 380, 458, 577, 281, 652, 6521, 11982, 13, 286, 478, 2597, 13, 51380, 51448, 400, 370, 436, 3241, 264, 1460, 11, 558, 30, 1033, 13, 400, 341, 307, 577, 1238, 709, 613, 11, 2232, 11, 51676, 51716], "temperature": 0.0, "avg_logprob": -0.12063018414152771, "compression_ratio": 1.6827309236947792, "no_speech_prob": 1.496629192843102e-05}, {"id": 90, "seek": 60264, "start": 619.12, "end": 622.96, "text": " it looks very good. No, I don't know how to make German accent. I'm sorry.", "tokens": [50364, 36540, 295, 300, 13, 1743, 11, 370, 321, 853, 281, 652, 1101, 293, 1101, 11, 2232, 11, 9606, 295, 264, 1460, 13, 6288, 11, 50600, 50676, 321, 352, 646, 281, 7244, 293, 341, 1389, 11, 7244, 11, 570, 436, 362, 1460, 11, 558, 30, 663, 321, 632, 11, 50884, 50884, 436, 362, 721, 321, 393, 2256, 13, 407, 321, 352, 646, 456, 293, 436, 366, 411, 11, 7020, 11, 51096, 51188, 309, 1542, 588, 665, 13, 883, 11, 286, 500, 380, 458, 577, 281, 652, 6521, 11982, 13, 286, 478, 2597, 13, 51380, 51448, 400, 370, 436, 3241, 264, 1460, 11, 558, 30, 1033, 13, 400, 341, 307, 577, 1238, 709, 613, 11, 2232, 11, 51676, 51716], "temperature": 0.0, "avg_logprob": -0.12063018414152771, "compression_ratio": 1.6827309236947792, "no_speech_prob": 1.496629192843102e-05}, {"id": 91, "seek": 60264, "start": 624.3199999999999, "end": 628.88, "text": " And so they accept the money, right? Okay. And this is how pretty much these, uh,", "tokens": [50364, 36540, 295, 300, 13, 1743, 11, 370, 321, 853, 281, 652, 1101, 293, 1101, 11, 2232, 11, 9606, 295, 264, 1460, 13, 6288, 11, 50600, 50676, 321, 352, 646, 281, 7244, 293, 341, 1389, 11, 7244, 11, 570, 436, 362, 1460, 11, 558, 30, 663, 321, 632, 11, 50884, 50884, 436, 362, 721, 321, 393, 2256, 13, 407, 321, 352, 646, 456, 293, 436, 366, 411, 11, 7020, 11, 51096, 51188, 309, 1542, 588, 665, 13, 883, 11, 286, 500, 380, 458, 577, 281, 652, 6521, 11982, 13, 286, 478, 2597, 13, 51380, 51448, 400, 370, 436, 3241, 264, 1460, 11, 558, 30, 1033, 13, 400, 341, 307, 577, 1238, 709, 613, 11, 2232, 11, 51676, 51716], "temperature": 0.0, "avg_logprob": -0.12063018414152771, "compression_ratio": 1.6827309236947792, "no_speech_prob": 1.496629192843102e-05}, {"id": 92, "seek": 62888, "start": 628.88, "end": 634.4, "text": " uh, generative other side of the network works. We have like a generator, which are the Italian", "tokens": [50364, 2232, 11, 1337, 1166, 661, 1252, 295, 264, 3209, 1985, 13, 492, 362, 411, 257, 19265, 11, 597, 366, 264, 10003, 50640, 50640, 27717, 294, 264, 4242, 11, 597, 366, 1455, 7592, 1460, 13, 400, 321, 366, 1382, 281, 8110, 746, 294, 7244, 50876, 50876, 293, 7244, 307, 264, 20828, 1639, 293, 436, 366, 588, 10910, 293, 588, 11, 1105, 11, 291, 458, 11, 436, 434, 6521, 13, 51204, 51280, 1033, 13, 3301, 11, 21154, 3006, 13, 286, 478, 406, 370, 2035, 11, 457, 550, 321, 360, 362, 257, 20752, 11, 558, 30, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.13881061003380216, "compression_ratio": 1.5791666666666666, "no_speech_prob": 5.824309482704848e-05}, {"id": 93, "seek": 62888, "start": 634.4, "end": 639.12, "text": " dudes in the South, which are making fake money. And we are trying to purchase something in Germany", "tokens": [50364, 2232, 11, 1337, 1166, 661, 1252, 295, 264, 3209, 1985, 13, 492, 362, 411, 257, 19265, 11, 597, 366, 264, 10003, 50640, 50640, 27717, 294, 264, 4242, 11, 597, 366, 1455, 7592, 1460, 13, 400, 321, 366, 1382, 281, 8110, 746, 294, 7244, 50876, 50876, 293, 7244, 307, 264, 20828, 1639, 293, 436, 366, 588, 10910, 293, 588, 11, 1105, 11, 291, 458, 11, 436, 434, 6521, 13, 51204, 51280, 1033, 13, 3301, 11, 21154, 3006, 13, 286, 478, 406, 370, 2035, 11, 457, 550, 321, 360, 362, 257, 20752, 11, 558, 30, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.13881061003380216, "compression_ratio": 1.5791666666666666, "no_speech_prob": 5.824309482704848e-05}, {"id": 94, "seek": 62888, "start": 639.12, "end": 645.68, "text": " and Germany is the discriminator and they are very strict and very, um, you know, they're German.", "tokens": [50364, 2232, 11, 1337, 1166, 661, 1252, 295, 264, 3209, 1985, 13, 492, 362, 411, 257, 19265, 11, 597, 366, 264, 10003, 50640, 50640, 27717, 294, 264, 4242, 11, 597, 366, 1455, 7592, 1460, 13, 400, 321, 366, 1382, 281, 8110, 746, 294, 7244, 50876, 50876, 293, 7244, 307, 264, 20828, 1639, 293, 436, 366, 588, 10910, 293, 588, 11, 1105, 11, 291, 458, 11, 436, 434, 6521, 13, 51204, 51280, 1033, 13, 3301, 11, 21154, 3006, 13, 286, 478, 406, 370, 2035, 11, 457, 550, 321, 360, 362, 257, 20752, 11, 558, 30, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.13881061003380216, "compression_ratio": 1.5791666666666666, "no_speech_prob": 5.824309482704848e-05}, {"id": 95, "seek": 62888, "start": 647.2, "end": 653.2, "text": " Okay. Um, politically correct. I'm not so whatever, but then we do have a spy, right?", "tokens": [50364, 2232, 11, 1337, 1166, 661, 1252, 295, 264, 3209, 1985, 13, 492, 362, 411, 257, 19265, 11, 597, 366, 264, 10003, 50640, 50640, 27717, 294, 264, 4242, 11, 597, 366, 1455, 7592, 1460, 13, 400, 321, 366, 1382, 281, 8110, 746, 294, 7244, 50876, 50876, 293, 7244, 307, 264, 20828, 1639, 293, 436, 366, 588, 10910, 293, 588, 11, 1105, 11, 291, 458, 11, 436, 434, 6521, 13, 51204, 51280, 1033, 13, 3301, 11, 21154, 3006, 13, 286, 478, 406, 370, 2035, 11, 457, 550, 321, 360, 362, 257, 20752, 11, 558, 30, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.13881061003380216, "compression_ratio": 1.5791666666666666, "no_speech_prob": 5.824309482704848e-05}, {"id": 96, "seek": 65320, "start": 653.2, "end": 659.2, "text": " And what is the spy? Can anyone figure out what's the spy analogy here? We haven't mentioned that", "tokens": [50364, 400, 437, 307, 264, 20752, 30, 1664, 2878, 2573, 484, 437, 311, 264, 20752, 21663, 510, 30, 492, 2378, 380, 2835, 300, 50664, 50664, 370, 1400, 13, 407, 264, 1036, 2445, 11, 646, 2365, 20828, 1639, 11, 1392, 13, 4019, 11, 512, 11, 512, 5824, 11, 51060, 51060, 1392, 13, 467, 311, 5824, 13, 400, 577, 307, 264, 5824, 1348, 490, 30, 407, 5699, 321, 3847, 264, 11, 2232, 11, 51464, 51464, 5699, 321, 3847, 264, 20828, 1639, 420, 264, 2063, 3209, 11, 558, 11, 321, 362, 512, 16235, 11, 51696, 51744], "temperature": 0.0, "avg_logprob": -0.13168366411899, "compression_ratio": 1.6651162790697673, "no_speech_prob": 7.4060130828002e-06}, {"id": 97, "seek": 65320, "start": 659.2, "end": 667.12, "text": " so far. So the last function, back prop discriminator, okay. Uh, some, some feedback,", "tokens": [50364, 400, 437, 307, 264, 20752, 30, 1664, 2878, 2573, 484, 437, 311, 264, 20752, 21663, 510, 30, 492, 2378, 380, 2835, 300, 50664, 50664, 370, 1400, 13, 407, 264, 1036, 2445, 11, 646, 2365, 20828, 1639, 11, 1392, 13, 4019, 11, 512, 11, 512, 5824, 11, 51060, 51060, 1392, 13, 467, 311, 5824, 13, 400, 577, 307, 264, 5824, 1348, 490, 30, 407, 5699, 321, 3847, 264, 11, 2232, 11, 51464, 51464, 5699, 321, 3847, 264, 20828, 1639, 420, 264, 2063, 3209, 11, 558, 11, 321, 362, 512, 16235, 11, 51696, 51744], "temperature": 0.0, "avg_logprob": -0.13168366411899, "compression_ratio": 1.6651162790697673, "no_speech_prob": 7.4060130828002e-06}, {"id": 98, "seek": 65320, "start": 667.12, "end": 675.2, "text": " okay. It's feedback. And how is the feedback coming from? So whenever we train the, uh,", "tokens": [50364, 400, 437, 307, 264, 20752, 30, 1664, 2878, 2573, 484, 437, 311, 264, 20752, 21663, 510, 30, 492, 2378, 380, 2835, 300, 50664, 50664, 370, 1400, 13, 407, 264, 1036, 2445, 11, 646, 2365, 20828, 1639, 11, 1392, 13, 4019, 11, 512, 11, 512, 5824, 11, 51060, 51060, 1392, 13, 467, 311, 5824, 13, 400, 577, 307, 264, 5824, 1348, 490, 30, 407, 5699, 321, 3847, 264, 11, 2232, 11, 51464, 51464, 5699, 321, 3847, 264, 20828, 1639, 420, 264, 2063, 3209, 11, 558, 11, 321, 362, 512, 16235, 11, 51696, 51744], "temperature": 0.0, "avg_logprob": -0.13168366411899, "compression_ratio": 1.6651162790697673, "no_speech_prob": 7.4060130828002e-06}, {"id": 99, "seek": 65320, "start": 675.2, "end": 679.84, "text": " whenever we train the discriminator or the cost network, right, we have some gradient,", "tokens": [50364, 400, 437, 307, 264, 20752, 30, 1664, 2878, 2573, 484, 437, 311, 264, 20752, 21663, 510, 30, 492, 2378, 380, 2835, 300, 50664, 50664, 370, 1400, 13, 407, 264, 1036, 2445, 11, 646, 2365, 20828, 1639, 11, 1392, 13, 4019, 11, 512, 11, 512, 5824, 11, 51060, 51060, 1392, 13, 467, 311, 5824, 13, 400, 577, 307, 264, 5824, 1348, 490, 30, 407, 5699, 321, 3847, 264, 11, 2232, 11, 51464, 51464, 5699, 321, 3847, 264, 20828, 1639, 420, 264, 2063, 3209, 11, 558, 11, 321, 362, 512, 16235, 11, 51696, 51744], "temperature": 0.0, "avg_logprob": -0.13168366411899, "compression_ratio": 1.6651162790697673, "no_speech_prob": 7.4060130828002e-06}, {"id": 100, "seek": 67984, "start": 679.84, "end": 689.44, "text": " that gradient allow me to do two things, right? I can either lower the, uh, I can either lower the", "tokens": [50364, 300, 16235, 2089, 385, 281, 360, 732, 721, 11, 558, 30, 286, 393, 2139, 3126, 264, 11, 2232, 11, 286, 393, 2139, 3126, 264, 50844, 50844, 2572, 2158, 13, 400, 370, 286, 393, 10864, 452, 9834, 295, 264, 2063, 2445, 13, 961, 385, 352, 646, 281, 264, 2063, 51180, 51180, 2445, 13, 407, 321, 362, 512, 2771, 2448, 295, 264, 2572, 2063, 11, 558, 30, 400, 370, 11, 2232, 11, 321, 362, 11, 1338, 11, 51484, 51484, 2721, 512, 16235, 13, 407, 264, 2572, 2063, 365, 3104, 281, 264, 9834, 295, 264, 3209, 13, 51676, 51712], "temperature": 0.0, "avg_logprob": -0.18392586221500318, "compression_ratio": 1.921875, "no_speech_prob": 1.1114791050204076e-05}, {"id": 101, "seek": 67984, "start": 689.44, "end": 696.1600000000001, "text": " final value. And so I can tune my parameters of the cost function. Let me go back to the cost", "tokens": [50364, 300, 16235, 2089, 385, 281, 360, 732, 721, 11, 558, 30, 286, 393, 2139, 3126, 264, 11, 2232, 11, 286, 393, 2139, 3126, 264, 50844, 50844, 2572, 2158, 13, 400, 370, 286, 393, 10864, 452, 9834, 295, 264, 2063, 2445, 13, 961, 385, 352, 646, 281, 264, 2063, 51180, 51180, 2445, 13, 407, 321, 362, 512, 2771, 2448, 295, 264, 2572, 2063, 11, 558, 30, 400, 370, 11, 2232, 11, 321, 362, 11, 1338, 11, 51484, 51484, 2721, 512, 16235, 13, 407, 264, 2572, 2063, 365, 3104, 281, 264, 9834, 295, 264, 3209, 13, 51676, 51712], "temperature": 0.0, "avg_logprob": -0.18392586221500318, "compression_ratio": 1.921875, "no_speech_prob": 1.1114791050204076e-05}, {"id": 102, "seek": 67984, "start": 696.1600000000001, "end": 702.24, "text": " function. So we have some gradients of the final cost, right? And so, uh, we have, yeah,", "tokens": [50364, 300, 16235, 2089, 385, 281, 360, 732, 721, 11, 558, 30, 286, 393, 2139, 3126, 264, 11, 2232, 11, 286, 393, 2139, 3126, 264, 50844, 50844, 2572, 2158, 13, 400, 370, 286, 393, 10864, 452, 9834, 295, 264, 2063, 2445, 13, 961, 385, 352, 646, 281, 264, 2063, 51180, 51180, 2445, 13, 407, 321, 362, 512, 2771, 2448, 295, 264, 2572, 2063, 11, 558, 30, 400, 370, 11, 2232, 11, 321, 362, 11, 1338, 11, 51484, 51484, 2721, 512, 16235, 13, 407, 264, 2572, 2063, 365, 3104, 281, 264, 9834, 295, 264, 3209, 13, 51676, 51712], "temperature": 0.0, "avg_logprob": -0.18392586221500318, "compression_ratio": 1.921875, "no_speech_prob": 1.1114791050204076e-05}, {"id": 103, "seek": 67984, "start": 702.24, "end": 706.08, "text": " finally some gradient. So the final cost with respect to the parameters of the network.", "tokens": [50364, 300, 16235, 2089, 385, 281, 360, 732, 721, 11, 558, 30, 286, 393, 2139, 3126, 264, 11, 2232, 11, 286, 393, 2139, 3126, 264, 50844, 50844, 2572, 2158, 13, 400, 370, 286, 393, 10864, 452, 9834, 295, 264, 2063, 2445, 13, 961, 385, 352, 646, 281, 264, 2063, 51180, 51180, 2445, 13, 407, 321, 362, 512, 2771, 2448, 295, 264, 2572, 2063, 11, 558, 30, 400, 370, 11, 2232, 11, 321, 362, 11, 1338, 11, 51484, 51484, 2721, 512, 16235, 13, 407, 264, 2572, 2063, 365, 3104, 281, 264, 9834, 295, 264, 3209, 13, 51676, 51712], "temperature": 0.0, "avg_logprob": -0.18392586221500318, "compression_ratio": 1.921875, "no_speech_prob": 1.1114791050204076e-05}, {"id": 104, "seek": 70608, "start": 706.08, "end": 711.6800000000001, "text": " And, and so usually, usually when I train the network, the cost network, I will try to", "tokens": [50364, 400, 11, 293, 370, 2673, 11, 2673, 562, 286, 3847, 264, 3209, 11, 264, 2063, 3209, 11, 286, 486, 853, 281, 50644, 50668, 10864, 264, 9834, 1270, 300, 11, 2232, 11, 286, 486, 362, 257, 2572, 3126, 4470, 11, 558, 30, 639, 307, 257, 2063, 3209, 51008, 51008, 293, 456, 307, 257, 4470, 322, 1192, 295, 264, 2063, 3209, 11, 558, 30, 467, 311, 257, 857, 13181, 13, 407, 11, 1105, 11, 321, 434, 516, 51368, 51368, 281, 312, 1382, 281, 19719, 264, 9834, 295, 264, 2063, 3209, 294, 1668, 281, 2042, 731, 293, 4412, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.16474427117241752, "compression_ratio": 1.7962085308056872, "no_speech_prob": 1.318198428634787e-05}, {"id": 105, "seek": 70608, "start": 712.1600000000001, "end": 718.96, "text": " tune the parameters such that, uh, I will have a final lower loss, right? This is a cost network", "tokens": [50364, 400, 11, 293, 370, 2673, 11, 2673, 562, 286, 3847, 264, 3209, 11, 264, 2063, 3209, 11, 286, 486, 853, 281, 50644, 50668, 10864, 264, 9834, 1270, 300, 11, 2232, 11, 286, 486, 362, 257, 2572, 3126, 4470, 11, 558, 30, 639, 307, 257, 2063, 3209, 51008, 51008, 293, 456, 307, 257, 4470, 322, 1192, 295, 264, 2063, 3209, 11, 558, 30, 467, 311, 257, 857, 13181, 13, 407, 11, 1105, 11, 321, 434, 516, 51368, 51368, 281, 312, 1382, 281, 19719, 264, 9834, 295, 264, 2063, 3209, 294, 1668, 281, 2042, 731, 293, 4412, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.16474427117241752, "compression_ratio": 1.7962085308056872, "no_speech_prob": 1.318198428634787e-05}, {"id": 106, "seek": 70608, "start": 718.96, "end": 726.1600000000001, "text": " and there is a loss on top of the cost network, right? It's a bit confusing. So, um, we're going", "tokens": [50364, 400, 11, 293, 370, 2673, 11, 2673, 562, 286, 3847, 264, 3209, 11, 264, 2063, 3209, 11, 286, 486, 853, 281, 50644, 50668, 10864, 264, 9834, 1270, 300, 11, 2232, 11, 286, 486, 362, 257, 2572, 3126, 4470, 11, 558, 30, 639, 307, 257, 2063, 3209, 51008, 51008, 293, 456, 307, 257, 4470, 322, 1192, 295, 264, 2063, 3209, 11, 558, 30, 467, 311, 257, 857, 13181, 13, 407, 11, 1105, 11, 321, 434, 516, 51368, 51368, 281, 312, 1382, 281, 19719, 264, 9834, 295, 264, 2063, 3209, 294, 1668, 281, 2042, 731, 293, 4412, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.16474427117241752, "compression_ratio": 1.7962085308056872, "no_speech_prob": 1.318198428634787e-05}, {"id": 107, "seek": 70608, "start": 726.1600000000001, "end": 732.4000000000001, "text": " to be trying to optimize the parameters of the cost network in order to perform well and therefore", "tokens": [50364, 400, 11, 293, 370, 2673, 11, 2673, 562, 286, 3847, 264, 3209, 11, 264, 2063, 3209, 11, 286, 486, 853, 281, 50644, 50668, 10864, 264, 9834, 1270, 300, 11, 2232, 11, 286, 486, 362, 257, 2572, 3126, 4470, 11, 558, 30, 639, 307, 257, 2063, 3209, 51008, 51008, 293, 456, 307, 257, 4470, 322, 1192, 295, 264, 2063, 3209, 11, 558, 30, 467, 311, 257, 857, 13181, 13, 407, 11, 1105, 11, 321, 434, 516, 51368, 51368, 281, 312, 1382, 281, 19719, 264, 9834, 295, 264, 2063, 3209, 294, 1668, 281, 2042, 731, 293, 4412, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.16474427117241752, "compression_ratio": 1.7962085308056872, "no_speech_prob": 1.318198428634787e-05}, {"id": 108, "seek": 73240, "start": 732.4, "end": 741.04, "text": " having a very low loss on the same way we can use those same gradients that are computed with respect", "tokens": [50364, 1419, 257, 588, 2295, 4470, 322, 264, 912, 636, 321, 393, 764, 729, 912, 2771, 2448, 300, 366, 40610, 365, 3104, 50796, 50796, 281, 341, 3209, 13, 400, 291, 603, 536, 452, 9719, 13, 4019, 11, 370, 286, 362, 11, 291, 458, 11, 452, 2572, 4470, 322, 1192, 295, 510, 13, 51116, 51116, 492, 808, 760, 365, 264, 2771, 2448, 293, 550, 291, 362, 510, 512, 2771, 2448, 293, 586, 613, 2771, 2448, 11, 51412, 51440, 291, 458, 11, 577, 11, 498, 291, 1319, 11, 2232, 11, 613, 1783, 2385, 11, 291, 434, 516, 281, 458, 577, 613, 2572, 4470, 486, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.18656699770972843, "compression_ratio": 1.7671232876712328, "no_speech_prob": 3.473995820968412e-05}, {"id": 109, "seek": 73240, "start": 741.04, "end": 747.4399999999999, "text": " to this network. And you'll see my mouse. Uh, so I have, you know, my final loss on top of here.", "tokens": [50364, 1419, 257, 588, 2295, 4470, 322, 264, 912, 636, 321, 393, 764, 729, 912, 2771, 2448, 300, 366, 40610, 365, 3104, 50796, 50796, 281, 341, 3209, 13, 400, 291, 603, 536, 452, 9719, 13, 4019, 11, 370, 286, 362, 11, 291, 458, 11, 452, 2572, 4470, 322, 1192, 295, 510, 13, 51116, 51116, 492, 808, 760, 365, 264, 2771, 2448, 293, 550, 291, 362, 510, 512, 2771, 2448, 293, 586, 613, 2771, 2448, 11, 51412, 51440, 291, 458, 11, 577, 11, 498, 291, 1319, 11, 2232, 11, 613, 1783, 2385, 11, 291, 434, 516, 281, 458, 577, 613, 2572, 4470, 486, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.18656699770972843, "compression_ratio": 1.7671232876712328, "no_speech_prob": 3.473995820968412e-05}, {"id": 110, "seek": 73240, "start": 747.4399999999999, "end": 753.36, "text": " We come down with the gradients and then you have here some gradients and now these gradients,", "tokens": [50364, 1419, 257, 588, 2295, 4470, 322, 264, 912, 636, 321, 393, 764, 729, 912, 2771, 2448, 300, 366, 40610, 365, 3104, 50796, 50796, 281, 341, 3209, 13, 400, 291, 603, 536, 452, 9719, 13, 4019, 11, 370, 286, 362, 11, 291, 458, 11, 452, 2572, 4470, 322, 1192, 295, 510, 13, 51116, 51116, 492, 808, 760, 365, 264, 2771, 2448, 293, 550, 291, 362, 510, 512, 2771, 2448, 293, 586, 613, 2771, 2448, 11, 51412, 51440, 291, 458, 11, 577, 11, 498, 291, 1319, 11, 2232, 11, 613, 1783, 2385, 11, 291, 434, 516, 281, 458, 577, 613, 2572, 4470, 486, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.18656699770972843, "compression_ratio": 1.7671232876712328, "no_speech_prob": 3.473995820968412e-05}, {"id": 111, "seek": 73240, "start": 753.92, "end": 760.0799999999999, "text": " you know, how, if you change, uh, these X hat, you're going to know how these final loss will", "tokens": [50364, 1419, 257, 588, 2295, 4470, 322, 264, 912, 636, 321, 393, 764, 729, 912, 2771, 2448, 300, 366, 40610, 365, 3104, 50796, 50796, 281, 341, 3209, 13, 400, 291, 603, 536, 452, 9719, 13, 4019, 11, 370, 286, 362, 11, 291, 458, 11, 452, 2572, 4470, 322, 1192, 295, 510, 13, 51116, 51116, 492, 808, 760, 365, 264, 2771, 2448, 293, 550, 291, 362, 510, 512, 2771, 2448, 293, 586, 613, 2771, 2448, 11, 51412, 51440, 291, 458, 11, 577, 11, 498, 291, 1319, 11, 2232, 11, 613, 1783, 2385, 11, 291, 434, 516, 281, 458, 577, 613, 2572, 4470, 486, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.18656699770972843, "compression_ratio": 1.7671232876712328, "no_speech_prob": 3.473995820968412e-05}, {"id": 112, "seek": 76008, "start": 760.08, "end": 767.44, "text": " change, right? Therefore you can train now this generator with this gradient in order to increase", "tokens": [50364, 1319, 11, 558, 30, 7504, 291, 393, 3847, 586, 341, 19265, 365, 341, 16235, 294, 1668, 281, 3488, 50732, 50732, 341, 2572, 4470, 13, 1033, 13, 407, 562, 321, 3847, 341, 2063, 3209, 11, 321, 1116, 411, 281, 17522, 264, 2572, 4470, 51060, 51108, 2212, 300, 321, 4846, 613, 732, 819, 15743, 11, 558, 30, 583, 611, 321, 1116, 411, 281, 3488, 341, 2572, 51436, 51436, 4470, 13, 407, 321, 1116, 411, 281, 652, 341, 2572, 3209, 2042, 5324, 538, 11, 291, 458, 11, 11470, 264, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.1322496838039822, "compression_ratio": 1.766355140186916, "no_speech_prob": 9.632043656893075e-06}, {"id": 113, "seek": 76008, "start": 767.44, "end": 774.0, "text": " this final loss. Okay. So when we train this cost network, we'd like to minimize the final loss", "tokens": [50364, 1319, 11, 558, 30, 7504, 291, 393, 3847, 586, 341, 19265, 365, 341, 16235, 294, 1668, 281, 3488, 50732, 50732, 341, 2572, 4470, 13, 1033, 13, 407, 562, 321, 3847, 341, 2063, 3209, 11, 321, 1116, 411, 281, 17522, 264, 2572, 4470, 51060, 51108, 2212, 300, 321, 4846, 613, 732, 819, 15743, 11, 558, 30, 583, 611, 321, 1116, 411, 281, 3488, 341, 2572, 51436, 51436, 4470, 13, 407, 321, 1116, 411, 281, 652, 341, 2572, 3209, 2042, 5324, 538, 11, 291, 458, 11, 11470, 264, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.1322496838039822, "compression_ratio": 1.766355140186916, "no_speech_prob": 9.632043656893075e-06}, {"id": 114, "seek": 76008, "start": 774.96, "end": 781.5200000000001, "text": " given that we input these two different inputs, right? But also we'd like to increase this final", "tokens": [50364, 1319, 11, 558, 30, 7504, 291, 393, 3847, 586, 341, 19265, 365, 341, 16235, 294, 1668, 281, 3488, 50732, 50732, 341, 2572, 4470, 13, 1033, 13, 407, 562, 321, 3847, 341, 2063, 3209, 11, 321, 1116, 411, 281, 17522, 264, 2572, 4470, 51060, 51108, 2212, 300, 321, 4846, 613, 732, 819, 15743, 11, 558, 30, 583, 611, 321, 1116, 411, 281, 3488, 341, 2572, 51436, 51436, 4470, 13, 407, 321, 1116, 411, 281, 652, 341, 2572, 3209, 2042, 5324, 538, 11, 291, 458, 11, 11470, 264, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.1322496838039822, "compression_ratio": 1.766355140186916, "no_speech_prob": 9.632043656893075e-06}, {"id": 115, "seek": 76008, "start": 781.5200000000001, "end": 788.4000000000001, "text": " loss. So we'd like to make this final network perform worse by, you know, improving the", "tokens": [50364, 1319, 11, 558, 30, 7504, 291, 393, 3847, 586, 341, 19265, 365, 341, 16235, 294, 1668, 281, 3488, 50732, 50732, 341, 2572, 4470, 13, 1033, 13, 407, 562, 321, 3847, 341, 2063, 3209, 11, 321, 1116, 411, 281, 17522, 264, 2572, 4470, 51060, 51108, 2212, 300, 321, 4846, 613, 732, 819, 15743, 11, 558, 30, 583, 611, 321, 1116, 411, 281, 3488, 341, 2572, 51436, 51436, 4470, 13, 407, 321, 1116, 411, 281, 652, 341, 2572, 3209, 2042, 5324, 538, 11, 291, 458, 11, 11470, 264, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.1322496838039822, "compression_ratio": 1.766355140186916, "no_speech_prob": 9.632043656893075e-06}, {"id": 116, "seek": 78840, "start": 788.4, "end": 794.0799999999999, "text": " generator. Okay. And so this information that comes down here and down this way, which is the", "tokens": [50364, 19265, 13, 1033, 13, 400, 370, 341, 1589, 300, 1487, 760, 510, 293, 760, 341, 636, 11, 597, 307, 264, 50648, 50684, 23897, 1320, 11, 558, 30, 440, 4846, 16235, 486, 312, 1143, 337, 15164, 264, 13075, 295, 264, 19265, 51020, 51020, 1270, 300, 309, 6453, 281, 7979, 264, 2063, 3209, 13, 1033, 13, 400, 370, 341, 307, 264, 21663, 365, 264, 20752, 51384, 51384, 294, 264, 6521, 11, 2232, 11, 294, 7244, 13, 1033, 13, 4019, 11, 307, 264, 7316, 295, 1176, 6806, 30, 4019, 11, 2597, 11, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.2320960670389155, "compression_ratio": 1.6339285714285714, "no_speech_prob": 1.539976437925361e-05}, {"id": 117, "seek": 78840, "start": 794.8, "end": 801.52, "text": " backward pass, right? The input gradient will be used for tuning the parameter of the generator", "tokens": [50364, 19265, 13, 1033, 13, 400, 370, 341, 1589, 300, 1487, 760, 510, 293, 760, 341, 636, 11, 597, 307, 264, 50648, 50684, 23897, 1320, 11, 558, 30, 440, 4846, 16235, 486, 312, 1143, 337, 15164, 264, 13075, 295, 264, 19265, 51020, 51020, 1270, 300, 309, 6453, 281, 7979, 264, 2063, 3209, 13, 1033, 13, 400, 370, 341, 307, 264, 21663, 365, 264, 20752, 51384, 51384, 294, 264, 6521, 11, 2232, 11, 294, 7244, 13, 1033, 13, 4019, 11, 307, 264, 7316, 295, 1176, 6806, 30, 4019, 11, 2597, 11, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.2320960670389155, "compression_ratio": 1.6339285714285714, "no_speech_prob": 1.539976437925361e-05}, {"id": 118, "seek": 78840, "start": 801.52, "end": 808.8, "text": " such that it managed to fool the cost network. Okay. And so this is the analogy with the spy", "tokens": [50364, 19265, 13, 1033, 13, 400, 370, 341, 1589, 300, 1487, 760, 510, 293, 760, 341, 636, 11, 597, 307, 264, 50648, 50684, 23897, 1320, 11, 558, 30, 440, 4846, 16235, 486, 312, 1143, 337, 15164, 264, 13075, 295, 264, 19265, 51020, 51020, 1270, 300, 309, 6453, 281, 7979, 264, 2063, 3209, 13, 1033, 13, 400, 370, 341, 307, 264, 21663, 365, 264, 20752, 51384, 51384, 294, 264, 6521, 11, 2232, 11, 294, 7244, 13, 1033, 13, 4019, 11, 307, 264, 7316, 295, 1176, 6806, 30, 4019, 11, 2597, 11, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.2320960670389155, "compression_ratio": 1.6339285714285714, "no_speech_prob": 1.539976437925361e-05}, {"id": 119, "seek": 78840, "start": 808.8, "end": 816.72, "text": " in the German, uh, in Germany. Okay. Uh, is the distribution of Z fixed? Uh, sorry,", "tokens": [50364, 19265, 13, 1033, 13, 400, 370, 341, 1589, 300, 1487, 760, 510, 293, 760, 341, 636, 11, 597, 307, 264, 50648, 50684, 23897, 1320, 11, 558, 30, 440, 4846, 16235, 486, 312, 1143, 337, 15164, 264, 13075, 295, 264, 19265, 51020, 51020, 1270, 300, 309, 6453, 281, 7979, 264, 2063, 3209, 13, 1033, 13, 400, 370, 341, 307, 264, 21663, 365, 264, 20752, 51384, 51384, 294, 264, 6521, 11, 2232, 11, 294, 7244, 13, 1033, 13, 4019, 11, 307, 264, 7316, 295, 1176, 6806, 30, 4019, 11, 2597, 11, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.2320960670389155, "compression_ratio": 1.6339285714285714, "no_speech_prob": 1.539976437925361e-05}, {"id": 120, "seek": 81672, "start": 816.72, "end": 823.84, "text": " fixed. Uh, so yeah, so Z, uh, Z actually comes from, let's say a normal distribution. I actually", "tokens": [50364, 6806, 13, 4019, 11, 370, 1338, 11, 370, 1176, 11, 2232, 11, 1176, 767, 1487, 490, 11, 718, 311, 584, 257, 2710, 7316, 13, 286, 767, 50720, 50720, 500, 380, 534, 11, 1105, 11, 362, 1340, 466, 11, 2232, 11, 1340, 281, 584, 466, 341, 7316, 13, 1018, 938, 382, 291, 51060, 51128, 1888, 428, 7316, 11, 291, 458, 11, 264, 19265, 486, 4471, 300, 7316, 666, 512, 1783, 2385, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.09849846040880358, "compression_ratio": 1.6436781609195403, "no_speech_prob": 5.619897274300456e-05}, {"id": 121, "seek": 81672, "start": 823.84, "end": 830.64, "text": " don't really, um, have anything about, uh, anything to say about this distribution. As long as you", "tokens": [50364, 6806, 13, 4019, 11, 370, 1338, 11, 370, 1176, 11, 2232, 11, 1176, 767, 1487, 490, 11, 718, 311, 584, 257, 2710, 7316, 13, 286, 767, 50720, 50720, 500, 380, 534, 11, 1105, 11, 362, 1340, 466, 11, 2232, 11, 1340, 281, 584, 466, 341, 7316, 13, 1018, 938, 382, 291, 51060, 51128, 1888, 428, 7316, 11, 291, 458, 11, 264, 19265, 486, 4471, 300, 7316, 666, 512, 1783, 2385, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.09849846040880358, "compression_ratio": 1.6436781609195403, "no_speech_prob": 5.619897274300456e-05}, {"id": 122, "seek": 81672, "start": 832.0, "end": 839.9200000000001, "text": " pick your distribution, you know, the generator will map that distribution into some X hat", "tokens": [50364, 6806, 13, 4019, 11, 370, 1338, 11, 370, 1176, 11, 2232, 11, 1176, 767, 1487, 490, 11, 718, 311, 584, 257, 2710, 7316, 13, 286, 767, 50720, 50720, 500, 380, 534, 11, 1105, 11, 362, 1340, 466, 11, 2232, 11, 1340, 281, 584, 466, 341, 7316, 13, 1018, 938, 382, 291, 51060, 51128, 1888, 428, 7316, 11, 291, 458, 11, 264, 19265, 486, 4471, 300, 7316, 666, 512, 1783, 2385, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.09849846040880358, "compression_ratio": 1.6436781609195403, "no_speech_prob": 5.619897274300456e-05}, {"id": 123, "seek": 83992, "start": 839.92, "end": 847.92, "text": " distribution, which will hopefully match what is the pink, uh, distribution of the axis. Okay.", "tokens": [50364, 7316, 11, 597, 486, 4696, 2995, 437, 307, 264, 7022, 11, 2232, 11, 7316, 295, 264, 10298, 13, 1033, 13, 50764, 50792, 407, 754, 11, 754, 1673, 1176, 11, 264, 7316, 295, 1176, 307, 6806, 11, 321, 11, 321, 393, 312, 988, 300, 321, 393, 1319, 51092, 51092, 264, 19265, 294, 1270, 257, 636, 300, 11, 1105, 11, 321, 393, 17522, 264, 2063, 2445, 13, 1779, 13, 407, 257, 4470, 51404, 51404, 412, 7316, 307, 6806, 13, 4019, 11, 264, 19265, 486, 11, 2232, 11, 2232, 11, 577, 360, 291, 584, 11, 35318, 430, 441, 257, 398, 11, 286, 519, 291, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.16762480962844123, "compression_ratio": 1.6905829596412556, "no_speech_prob": 7.48267411836423e-05}, {"id": 124, "seek": 83992, "start": 848.4799999999999, "end": 854.4799999999999, "text": " So even, even though Z, the distribution of Z is fixed, we, we can be sure that we can change", "tokens": [50364, 7316, 11, 597, 486, 4696, 2995, 437, 307, 264, 7022, 11, 2232, 11, 7316, 295, 264, 10298, 13, 1033, 13, 50764, 50792, 407, 754, 11, 754, 1673, 1176, 11, 264, 7316, 295, 1176, 307, 6806, 11, 321, 11, 321, 393, 312, 988, 300, 321, 393, 1319, 51092, 51092, 264, 19265, 294, 1270, 257, 636, 300, 11, 1105, 11, 321, 393, 17522, 264, 2063, 2445, 13, 1779, 13, 407, 257, 4470, 51404, 51404, 412, 7316, 307, 6806, 13, 4019, 11, 264, 19265, 486, 11, 2232, 11, 2232, 11, 577, 360, 291, 584, 11, 35318, 430, 441, 257, 398, 11, 286, 519, 291, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.16762480962844123, "compression_ratio": 1.6905829596412556, "no_speech_prob": 7.48267411836423e-05}, {"id": 125, "seek": 83992, "start": 854.4799999999999, "end": 860.7199999999999, "text": " the generator in such a way that, um, we can minimize the cost function. Right. So a loss", "tokens": [50364, 7316, 11, 597, 486, 4696, 2995, 437, 307, 264, 7022, 11, 2232, 11, 7316, 295, 264, 10298, 13, 1033, 13, 50764, 50792, 407, 754, 11, 754, 1673, 1176, 11, 264, 7316, 295, 1176, 307, 6806, 11, 321, 11, 321, 393, 312, 988, 300, 321, 393, 1319, 51092, 51092, 264, 19265, 294, 1270, 257, 636, 300, 11, 1105, 11, 321, 393, 17522, 264, 2063, 2445, 13, 1779, 13, 407, 257, 4470, 51404, 51404, 412, 7316, 307, 6806, 13, 4019, 11, 264, 19265, 486, 11, 2232, 11, 2232, 11, 577, 360, 291, 584, 11, 35318, 430, 441, 257, 398, 11, 286, 519, 291, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.16762480962844123, "compression_ratio": 1.6905829596412556, "no_speech_prob": 7.48267411836423e-05}, {"id": 126, "seek": 83992, "start": 860.7199999999999, "end": 868.88, "text": " at distribution is fixed. Uh, the generator will, uh, uh, how do you say, ply P L a Y, I think you", "tokens": [50364, 7316, 11, 597, 486, 4696, 2995, 437, 307, 264, 7022, 11, 2232, 11, 7316, 295, 264, 10298, 13, 1033, 13, 50764, 50792, 407, 754, 11, 754, 1673, 1176, 11, 264, 7316, 295, 1176, 307, 6806, 11, 321, 11, 321, 393, 312, 988, 300, 321, 393, 1319, 51092, 51092, 264, 19265, 294, 1270, 257, 636, 300, 11, 1105, 11, 321, 393, 17522, 264, 2063, 2445, 13, 1779, 13, 407, 257, 4470, 51404, 51404, 412, 7316, 307, 6806, 13, 4019, 11, 264, 19265, 486, 11, 2232, 11, 2232, 11, 577, 360, 291, 584, 11, 35318, 430, 441, 257, 398, 11, 286, 519, 291, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.16762480962844123, "compression_ratio": 1.6905829596412556, "no_speech_prob": 7.48267411836423e-05}, {"id": 127, "seek": 86888, "start": 868.88, "end": 873.84, "text": " will apply this kind of distribution such that you're going to be basically like reflowing into", "tokens": [50364, 486, 3079, 341, 733, 295, 7316, 1270, 300, 291, 434, 516, 281, 312, 1936, 411, 1895, 75, 9637, 666, 50612, 50612, 746, 300, 1542, 20025, 264, 1783, 11, 2232, 11, 294, 264, 7022, 1783, 11, 2232, 11, 4696, 11, 1392, 13, 286, 2378, 380, 1907, 291, 466, 51000, 51000, 264, 10147, 18542, 295, 341, 11, 2232, 11, 1185, 13, 1033, 13, 583, 4696, 321, 1116, 411, 281, 3067, 281, 483, 257, 7316, 51368, 51368, 484, 295, 729, 3344, 1783, 82, 1783, 20549, 1270, 300, 436, 36870, 264, 3380, 7316, 322, 264, 1411, 12, 5543, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.12791257001915757, "compression_ratio": 1.6885964912280702, "no_speech_prob": 3.698285945574753e-05}, {"id": 128, "seek": 86888, "start": 873.84, "end": 881.6, "text": " something that looks alike the X, uh, in the pink X, uh, hopefully, okay. I haven't told you about", "tokens": [50364, 486, 3079, 341, 733, 295, 7316, 1270, 300, 291, 434, 516, 281, 312, 1936, 411, 1895, 75, 9637, 666, 50612, 50612, 746, 300, 1542, 20025, 264, 1783, 11, 2232, 11, 294, 264, 7022, 1783, 11, 2232, 11, 4696, 11, 1392, 13, 286, 2378, 380, 1907, 291, 466, 51000, 51000, 264, 10147, 18542, 295, 341, 11, 2232, 11, 1185, 13, 1033, 13, 583, 4696, 321, 1116, 411, 281, 3067, 281, 483, 257, 7316, 51368, 51368, 484, 295, 729, 3344, 1783, 82, 1783, 20549, 1270, 300, 436, 36870, 264, 3380, 7316, 322, 264, 1411, 12, 5543, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.12791257001915757, "compression_ratio": 1.6885964912280702, "no_speech_prob": 3.698285945574753e-05}, {"id": 129, "seek": 86888, "start": 881.6, "end": 888.96, "text": " the pitfalls of this, uh, system. Okay. But hopefully we'd like to manage to get a distribution", "tokens": [50364, 486, 3079, 341, 733, 295, 7316, 1270, 300, 291, 434, 516, 281, 312, 1936, 411, 1895, 75, 9637, 666, 50612, 50612, 746, 300, 1542, 20025, 264, 1783, 11, 2232, 11, 294, 264, 7022, 1783, 11, 2232, 11, 4696, 11, 1392, 13, 286, 2378, 380, 1907, 291, 466, 51000, 51000, 264, 10147, 18542, 295, 341, 11, 2232, 11, 1185, 13, 1033, 13, 583, 4696, 321, 1116, 411, 281, 3067, 281, 483, 257, 7316, 51368, 51368, 484, 295, 729, 3344, 1783, 82, 1783, 20549, 1270, 300, 436, 36870, 264, 3380, 7316, 322, 264, 1411, 12, 5543, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.12791257001915757, "compression_ratio": 1.6885964912280702, "no_speech_prob": 3.698285945574753e-05}, {"id": 130, "seek": 86888, "start": 888.96, "end": 895.68, "text": " out of those blue Xs X hats such that they resemble the original distribution on the left-hand", "tokens": [50364, 486, 3079, 341, 733, 295, 7316, 1270, 300, 291, 434, 516, 281, 312, 1936, 411, 1895, 75, 9637, 666, 50612, 50612, 746, 300, 1542, 20025, 264, 1783, 11, 2232, 11, 294, 264, 7022, 1783, 11, 2232, 11, 4696, 11, 1392, 13, 286, 2378, 380, 1907, 291, 466, 51000, 51000, 264, 10147, 18542, 295, 341, 11, 2232, 11, 1185, 13, 1033, 13, 583, 4696, 321, 1116, 411, 281, 3067, 281, 483, 257, 7316, 51368, 51368, 484, 295, 729, 3344, 1783, 82, 1783, 20549, 1270, 300, 436, 36870, 264, 3380, 7316, 322, 264, 1411, 12, 5543, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.12791257001915757, "compression_ratio": 1.6885964912280702, "no_speech_prob": 3.698285945574753e-05}, {"id": 131, "seek": 89568, "start": 895.68, "end": 902.4799999999999, "text": " side in the pink one. Okay. Did I answer your question? Yeah, that makes sense. Okay. Wouldn't", "tokens": [50364, 1252, 294, 264, 7022, 472, 13, 1033, 13, 2589, 286, 1867, 428, 1168, 30, 865, 11, 300, 1669, 2020, 13, 1033, 13, 26291, 380, 50704, 50704, 264, 1783, 7126, 538, 264, 19265, 312, 264, 777, 9689, 1460, 30, 3301, 11, 264, 3344, 472, 13, 1033, 13, 865, 13, 51148, 51148, 1044, 291, 13, 4019, 11, 286, 767, 994, 380, 2413, 300, 472, 13, 407, 264, 11, 2232, 11, 7022, 472, 366, 264, 2074, 14160, 51420, 51420, 321, 366, 1228, 294, 3315, 293, 264, 3344, 11, 264, 3344, 1783, 2385, 366, 264, 1460, 300, 321, 652, 294, 10705, 13, 1033, 13, 51756, 51824], "temperature": 0.0, "avg_logprob": -0.0755367508301368, "compression_ratio": 1.6576576576576576, "no_speech_prob": 0.0006442104931920767}, {"id": 132, "seek": 89568, "start": 902.4799999999999, "end": 911.3599999999999, "text": " the X produced by the generator be the new improved money? Um, the blue one. Okay. Yeah.", "tokens": [50364, 1252, 294, 264, 7022, 472, 13, 1033, 13, 2589, 286, 1867, 428, 1168, 30, 865, 11, 300, 1669, 2020, 13, 1033, 13, 26291, 380, 50704, 50704, 264, 1783, 7126, 538, 264, 19265, 312, 264, 777, 9689, 1460, 30, 3301, 11, 264, 3344, 472, 13, 1033, 13, 865, 13, 51148, 51148, 1044, 291, 13, 4019, 11, 286, 767, 994, 380, 2413, 300, 472, 13, 407, 264, 11, 2232, 11, 7022, 472, 366, 264, 2074, 14160, 51420, 51420, 321, 366, 1228, 294, 3315, 293, 264, 3344, 11, 264, 3344, 1783, 2385, 366, 264, 1460, 300, 321, 652, 294, 10705, 13, 1033, 13, 51756, 51824], "temperature": 0.0, "avg_logprob": -0.0755367508301368, "compression_ratio": 1.6576576576576576, "no_speech_prob": 0.0006442104931920767}, {"id": 133, "seek": 89568, "start": 911.3599999999999, "end": 916.8, "text": " Thank you. Uh, I actually didn't finish that one. So the, uh, pink one are the true euros", "tokens": [50364, 1252, 294, 264, 7022, 472, 13, 1033, 13, 2589, 286, 1867, 428, 1168, 30, 865, 11, 300, 1669, 2020, 13, 1033, 13, 26291, 380, 50704, 50704, 264, 1783, 7126, 538, 264, 19265, 312, 264, 777, 9689, 1460, 30, 3301, 11, 264, 3344, 472, 13, 1033, 13, 865, 13, 51148, 51148, 1044, 291, 13, 4019, 11, 286, 767, 994, 380, 2413, 300, 472, 13, 407, 264, 11, 2232, 11, 7022, 472, 366, 264, 2074, 14160, 51420, 51420, 321, 366, 1228, 294, 3315, 293, 264, 3344, 11, 264, 3344, 1783, 2385, 366, 264, 1460, 300, 321, 652, 294, 10705, 13, 1033, 13, 51756, 51824], "temperature": 0.0, "avg_logprob": -0.0755367508301368, "compression_ratio": 1.6576576576576576, "no_speech_prob": 0.0006442104931920767}, {"id": 134, "seek": 89568, "start": 916.8, "end": 923.52, "text": " we are using in Europe and the blue, the blue X hat are the money that we make in Italy. Okay.", "tokens": [50364, 1252, 294, 264, 7022, 472, 13, 1033, 13, 2589, 286, 1867, 428, 1168, 30, 865, 11, 300, 1669, 2020, 13, 1033, 13, 26291, 380, 50704, 50704, 264, 1783, 7126, 538, 264, 19265, 312, 264, 777, 9689, 1460, 30, 3301, 11, 264, 3344, 472, 13, 1033, 13, 865, 13, 51148, 51148, 1044, 291, 13, 4019, 11, 286, 767, 994, 380, 2413, 300, 472, 13, 407, 264, 11, 2232, 11, 7022, 472, 366, 264, 2074, 14160, 51420, 51420, 321, 366, 1228, 294, 3315, 293, 264, 3344, 11, 264, 3344, 1783, 2385, 366, 264, 1460, 300, 321, 652, 294, 10705, 13, 1033, 13, 51756, 51824], "temperature": 0.0, "avg_logprob": -0.0755367508301368, "compression_ratio": 1.6576576576576576, "no_speech_prob": 0.0006442104931920767}, {"id": 135, "seek": 92352, "start": 923.52, "end": 930.8, "text": " Oh, my, yeah. Okay. Other questions. I thought the generator was supposed to give negative samples.", "tokens": [50364, 876, 11, 452, 11, 1338, 13, 1033, 13, 5358, 1651, 13, 286, 1194, 264, 19265, 390, 3442, 281, 976, 3671, 10938, 13, 50728, 50828, 3301, 11, 370, 3671, 10938, 13, 1033, 13, 4019, 11, 370, 456, 366, 732, 4439, 510, 13, 4019, 11, 321, 2893, 3671, 10938, 51212, 51212, 300, 366, 613, 1783, 20549, 281, 264, 2063, 3209, 13, 407, 264, 2063, 3209, 307, 8895, 294, 1668, 281, 362, 2295, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.22651888873126055, "compression_ratio": 1.686046511627907, "no_speech_prob": 3.87730251532048e-05}, {"id": 136, "seek": 92352, "start": 932.8, "end": 940.48, "text": " Um, so negative samples. Okay. Uh, so there are two steps here. Uh, we provide negative samples", "tokens": [50364, 876, 11, 452, 11, 1338, 13, 1033, 13, 5358, 1651, 13, 286, 1194, 264, 19265, 390, 3442, 281, 976, 3671, 10938, 13, 50728, 50828, 3301, 11, 370, 3671, 10938, 13, 1033, 13, 4019, 11, 370, 456, 366, 732, 4439, 510, 13, 4019, 11, 321, 2893, 3671, 10938, 51212, 51212, 300, 366, 613, 1783, 20549, 281, 264, 2063, 3209, 13, 407, 264, 2063, 3209, 307, 8895, 294, 1668, 281, 362, 2295, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.22651888873126055, "compression_ratio": 1.686046511627907, "no_speech_prob": 3.87730251532048e-05}, {"id": 137, "seek": 92352, "start": 940.48, "end": 947.4399999999999, "text": " that are these X hats to the cost network. So the cost network is trained in order to have low", "tokens": [50364, 876, 11, 452, 11, 1338, 13, 1033, 13, 5358, 1651, 13, 286, 1194, 264, 19265, 390, 3442, 281, 976, 3671, 10938, 13, 50728, 50828, 3301, 11, 370, 3671, 10938, 13, 1033, 13, 4019, 11, 370, 456, 366, 732, 4439, 510, 13, 4019, 11, 321, 2893, 3671, 10938, 51212, 51212, 300, 366, 613, 1783, 20549, 281, 264, 2063, 3209, 13, 407, 264, 2063, 3209, 307, 8895, 294, 1668, 281, 362, 2295, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.22651888873126055, "compression_ratio": 1.686046511627907, "no_speech_prob": 3.87730251532048e-05}, {"id": 138, "seek": 94744, "start": 947.44, "end": 953.9200000000001, "text": " values on the pink input and higher values on the blue input. Okay. And so if the network,", "tokens": [50364, 4190, 322, 264, 7022, 4846, 293, 2946, 4190, 322, 264, 3344, 4846, 13, 1033, 13, 400, 370, 498, 264, 3209, 11, 50688, 50688, 264, 2063, 3209, 26213, 731, 11, 550, 264, 2572, 4470, 510, 322, 1192, 486, 312, 588, 4018, 11, 588, 2295, 13, 51032, 51032, 1033, 13, 407, 498, 264, 2063, 3209, 307, 588, 11, 307, 10205, 588, 731, 11, 550, 291, 434, 516, 281, 362, 257, 2572, 51316, 51316, 2295, 4470, 510, 13, 26554, 11, 264, 19265, 486, 312, 8895, 294, 1668, 281, 3488, 300, 4470, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.10679464442755586, "compression_ratio": 1.7922705314009661, "no_speech_prob": 3.876318805851042e-05}, {"id": 139, "seek": 94744, "start": 953.9200000000001, "end": 960.8000000000001, "text": " the cost network performs well, then the final loss here on top will be very zero, very low.", "tokens": [50364, 4190, 322, 264, 7022, 4846, 293, 2946, 4190, 322, 264, 3344, 4846, 13, 1033, 13, 400, 370, 498, 264, 3209, 11, 50688, 50688, 264, 2063, 3209, 26213, 731, 11, 550, 264, 2572, 4470, 510, 322, 1192, 486, 312, 588, 4018, 11, 588, 2295, 13, 51032, 51032, 1033, 13, 407, 498, 264, 2063, 3209, 307, 588, 11, 307, 10205, 588, 731, 11, 550, 291, 434, 516, 281, 362, 257, 2572, 51316, 51316, 2295, 4470, 510, 13, 26554, 11, 264, 19265, 486, 312, 8895, 294, 1668, 281, 3488, 300, 4470, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.10679464442755586, "compression_ratio": 1.7922705314009661, "no_speech_prob": 3.876318805851042e-05}, {"id": 140, "seek": 94744, "start": 960.8000000000001, "end": 966.48, "text": " Okay. So if the cost network is very, is performing very well, then you're going to have a final", "tokens": [50364, 4190, 322, 264, 7022, 4846, 293, 2946, 4190, 322, 264, 3344, 4846, 13, 1033, 13, 400, 370, 498, 264, 3209, 11, 50688, 50688, 264, 2063, 3209, 26213, 731, 11, 550, 264, 2572, 4470, 510, 322, 1192, 486, 312, 588, 4018, 11, 588, 2295, 13, 51032, 51032, 1033, 13, 407, 498, 264, 2063, 3209, 307, 588, 11, 307, 10205, 588, 731, 11, 550, 291, 434, 516, 281, 362, 257, 2572, 51316, 51316, 2295, 4470, 510, 13, 26554, 11, 264, 19265, 486, 312, 8895, 294, 1668, 281, 3488, 300, 4470, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.10679464442755586, "compression_ratio": 1.7922705314009661, "no_speech_prob": 3.876318805851042e-05}, {"id": 141, "seek": 94744, "start": 966.48, "end": 974.0, "text": " low loss here. Nevertheless, the generator will be trained in order to increase that loss,", "tokens": [50364, 4190, 322, 264, 7022, 4846, 293, 2946, 4190, 322, 264, 3344, 4846, 13, 1033, 13, 400, 370, 498, 264, 3209, 11, 50688, 50688, 264, 2063, 3209, 26213, 731, 11, 550, 264, 2572, 4470, 510, 322, 1192, 486, 312, 588, 4018, 11, 588, 2295, 13, 51032, 51032, 1033, 13, 407, 498, 264, 2063, 3209, 307, 588, 11, 307, 10205, 588, 731, 11, 550, 291, 434, 516, 281, 362, 257, 2572, 51316, 51316, 2295, 4470, 510, 13, 26554, 11, 264, 19265, 486, 312, 8895, 294, 1668, 281, 3488, 300, 4470, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.10679464442755586, "compression_ratio": 1.7922705314009661, "no_speech_prob": 3.876318805851042e-05}, {"id": 142, "seek": 97400, "start": 974.0, "end": 982.64, "text": " because we'd like to fool these Germans. That doesn't make sense. Can you just clarify what", "tokens": [50364, 570, 321, 1116, 411, 281, 7979, 613, 18116, 13, 663, 1177, 380, 652, 2020, 13, 1664, 291, 445, 17594, 437, 50796, 50796, 264, 20752, 307, 294, 341, 21663, 30, 865, 13, 440, 20752, 307, 264, 4846, 16235, 13, 407, 5699, 510, 286, 362, 452, 2063, 51108, 51108, 3209, 293, 281, 3847, 341, 2063, 3209, 11, 286, 478, 516, 281, 362, 257, 2572, 4583, 510, 322, 1192, 11, 558, 30, 4019, 11, 51380, 51380, 718, 311, 584, 341, 307, 364, 7395, 34, 11, 2232, 11, 337, 1365, 11, 2232, 11, 364, 7395, 34, 365, 11, 291, 458, 11, 4018, 337, 5699, 286, 11, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.15146298678416126, "compression_ratio": 1.5625, "no_speech_prob": 2.465964644216001e-05}, {"id": 143, "seek": 97400, "start": 982.64, "end": 988.88, "text": " the spy is in this analogy? Yeah. The spy is the input gradient. So whenever here I have my cost", "tokens": [50364, 570, 321, 1116, 411, 281, 7979, 613, 18116, 13, 663, 1177, 380, 652, 2020, 13, 1664, 291, 445, 17594, 437, 50796, 50796, 264, 20752, 307, 294, 341, 21663, 30, 865, 13, 440, 20752, 307, 264, 4846, 16235, 13, 407, 5699, 510, 286, 362, 452, 2063, 51108, 51108, 3209, 293, 281, 3847, 341, 2063, 3209, 11, 286, 478, 516, 281, 362, 257, 2572, 4583, 510, 322, 1192, 11, 558, 30, 4019, 11, 51380, 51380, 718, 311, 584, 341, 307, 364, 7395, 34, 11, 2232, 11, 337, 1365, 11, 2232, 11, 364, 7395, 34, 365, 11, 291, 458, 11, 4018, 337, 5699, 286, 11, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.15146298678416126, "compression_ratio": 1.5625, "no_speech_prob": 2.465964644216001e-05}, {"id": 144, "seek": 97400, "start": 988.88, "end": 994.32, "text": " network and to train this cost network, I'm going to have a final layer here on top, right? Uh,", "tokens": [50364, 570, 321, 1116, 411, 281, 7979, 613, 18116, 13, 663, 1177, 380, 652, 2020, 13, 1664, 291, 445, 17594, 437, 50796, 50796, 264, 20752, 307, 294, 341, 21663, 30, 865, 13, 440, 20752, 307, 264, 4846, 16235, 13, 407, 5699, 510, 286, 362, 452, 2063, 51108, 51108, 3209, 293, 281, 3847, 341, 2063, 3209, 11, 286, 478, 516, 281, 362, 257, 2572, 4583, 510, 322, 1192, 11, 558, 30, 4019, 11, 51380, 51380, 718, 311, 584, 341, 307, 364, 7395, 34, 11, 2232, 11, 337, 1365, 11, 2232, 11, 364, 7395, 34, 365, 11, 291, 458, 11, 4018, 337, 5699, 286, 11, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.15146298678416126, "compression_ratio": 1.5625, "no_speech_prob": 2.465964644216001e-05}, {"id": 145, "seek": 97400, "start": 994.32, "end": 1001.44, "text": " let's say this is an MSC, uh, for example, uh, an MSC with, you know, zero for whenever I,", "tokens": [50364, 570, 321, 1116, 411, 281, 7979, 613, 18116, 13, 663, 1177, 380, 652, 2020, 13, 1664, 291, 445, 17594, 437, 50796, 50796, 264, 20752, 307, 294, 341, 21663, 30, 865, 13, 440, 20752, 307, 264, 4846, 16235, 13, 407, 5699, 510, 286, 362, 452, 2063, 51108, 51108, 3209, 293, 281, 3847, 341, 2063, 3209, 11, 286, 478, 516, 281, 362, 257, 2572, 4583, 510, 322, 1192, 11, 558, 30, 4019, 11, 51380, 51380, 718, 311, 584, 341, 307, 364, 7395, 34, 11, 2232, 11, 337, 1365, 11, 2232, 11, 364, 7395, 34, 365, 11, 291, 458, 11, 4018, 337, 5699, 286, 11, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.15146298678416126, "compression_ratio": 1.5625, "no_speech_prob": 2.465964644216001e-05}, {"id": 146, "seek": 100144, "start": 1001.44, "end": 1008.6400000000001, "text": " whenever I have an input, uh, X pink or some, you know, value, let's say plus 10 in this case,", "tokens": [50364, 5699, 286, 362, 364, 4846, 11, 2232, 11, 1783, 7022, 420, 512, 11, 291, 458, 11, 2158, 11, 718, 311, 584, 1804, 1266, 294, 341, 1389, 11, 50724, 50776, 309, 311, 257, 11, 286, 486, 853, 257, 1230, 337, 264, 1623, 13, 3301, 11, 2232, 11, 2158, 1804, 1266, 337, 264, 3344, 1074, 11, 558, 30, 407, 51048, 51048, 452, 2063, 3209, 307, 257, 24590, 24590, 3209, 13, 509, 393, 519, 466, 341, 382, 445, 472, 2167, 8213, 51376, 51412, 4583, 13, 407, 309, 311, 411, 364, 2096, 533, 9887, 295, 264, 4846, 13, 400, 550, 613, 1936, 257, 2572, 2158, 11, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.1964532564271171, "compression_ratio": 1.6024590163934427, "no_speech_prob": 2.122585465258453e-05}, {"id": 147, "seek": 100144, "start": 1009.6800000000001, "end": 1015.12, "text": " it's a, I will try a number for the moment. Um, uh, value plus 10 for the blue guys, right? So", "tokens": [50364, 5699, 286, 362, 364, 4846, 11, 2232, 11, 1783, 7022, 420, 512, 11, 291, 458, 11, 2158, 11, 718, 311, 584, 1804, 1266, 294, 341, 1389, 11, 50724, 50776, 309, 311, 257, 11, 286, 486, 853, 257, 1230, 337, 264, 1623, 13, 3301, 11, 2232, 11, 2158, 1804, 1266, 337, 264, 3344, 1074, 11, 558, 30, 407, 51048, 51048, 452, 2063, 3209, 307, 257, 24590, 24590, 3209, 13, 509, 393, 519, 466, 341, 382, 445, 472, 2167, 8213, 51376, 51412, 4583, 13, 407, 309, 311, 411, 364, 2096, 533, 9887, 295, 264, 4846, 13, 400, 550, 613, 1936, 257, 2572, 2158, 11, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.1964532564271171, "compression_ratio": 1.6024590163934427, "no_speech_prob": 2.122585465258453e-05}, {"id": 148, "seek": 100144, "start": 1015.12, "end": 1021.6800000000001, "text": " my cost network is a regression regression network. You can think about this as just one single linear", "tokens": [50364, 5699, 286, 362, 364, 4846, 11, 2232, 11, 1783, 7022, 420, 512, 11, 291, 458, 11, 2158, 11, 718, 311, 584, 1804, 1266, 294, 341, 1389, 11, 50724, 50776, 309, 311, 257, 11, 286, 486, 853, 257, 1230, 337, 264, 1623, 13, 3301, 11, 2232, 11, 2158, 1804, 1266, 337, 264, 3344, 1074, 11, 558, 30, 407, 51048, 51048, 452, 2063, 3209, 307, 257, 24590, 24590, 3209, 13, 509, 393, 519, 466, 341, 382, 445, 472, 2167, 8213, 51376, 51412, 4583, 13, 407, 309, 311, 411, 364, 2096, 533, 9887, 295, 264, 4846, 13, 400, 550, 613, 1936, 257, 2572, 2158, 11, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.1964532564271171, "compression_ratio": 1.6024590163934427, "no_speech_prob": 2.122585465258453e-05}, {"id": 149, "seek": 100144, "start": 1022.4000000000001, "end": 1029.04, "text": " layer. So it's like an affine transformation of the input. And then these basically a final value,", "tokens": [50364, 5699, 286, 362, 364, 4846, 11, 2232, 11, 1783, 7022, 420, 512, 11, 291, 458, 11, 2158, 11, 718, 311, 584, 1804, 1266, 294, 341, 1389, 11, 50724, 50776, 309, 311, 257, 11, 286, 486, 853, 257, 1230, 337, 264, 1623, 13, 3301, 11, 2232, 11, 2158, 1804, 1266, 337, 264, 3344, 1074, 11, 558, 30, 407, 51048, 51048, 452, 2063, 3209, 307, 257, 24590, 24590, 3209, 13, 509, 393, 519, 466, 341, 382, 445, 472, 2167, 8213, 51376, 51412, 4583, 13, 407, 309, 311, 411, 364, 2096, 533, 9887, 295, 264, 4846, 13, 400, 550, 613, 1936, 257, 2572, 2158, 11, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.1964532564271171, "compression_ratio": 1.6024590163934427, "no_speech_prob": 2.122585465258453e-05}, {"id": 150, "seek": 102904, "start": 1029.04, "end": 1036.08, "text": " I set it to be zero for the pink input. I have an MSC between the output of the network and zero", "tokens": [50364, 286, 992, 309, 281, 312, 4018, 337, 264, 7022, 4846, 13, 286, 362, 364, 7395, 34, 1296, 264, 5598, 295, 264, 3209, 293, 4018, 50716, 50716, 337, 5699, 286, 4846, 264, 7022, 4846, 13, 400, 2602, 11, 718, 311, 584, 286, 2826, 364, 23211, 2158, 295, 1266, 51044, 51080, 281, 312, 23543, 300, 264, 4846, 307, 264, 3344, 472, 11, 558, 30, 407, 321, 362, 2063, 3209, 11, 597, 307, 257, 3209, 51440, 51440, 300, 307, 5598, 783, 257, 2167, 39684, 2158, 13, 400, 341, 39684, 2158, 486, 352, 1854, 264, 7395, 34, 10088, 510, 322, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.12309101104736328, "compression_ratio": 1.7117903930131004, "no_speech_prob": 3.0838553357170895e-05}, {"id": 151, "seek": 102904, "start": 1036.08, "end": 1042.6399999999999, "text": " for whenever I input the pink input. And instead, let's say I choose an arbitrary value of 10", "tokens": [50364, 286, 992, 309, 281, 312, 4018, 337, 264, 7022, 4846, 13, 286, 362, 364, 7395, 34, 1296, 264, 5598, 295, 264, 3209, 293, 4018, 50716, 50716, 337, 5699, 286, 4846, 264, 7022, 4846, 13, 400, 2602, 11, 718, 311, 584, 286, 2826, 364, 23211, 2158, 295, 1266, 51044, 51080, 281, 312, 23543, 300, 264, 4846, 307, 264, 3344, 472, 11, 558, 30, 407, 321, 362, 2063, 3209, 11, 597, 307, 257, 3209, 51440, 51440, 300, 307, 5598, 783, 257, 2167, 39684, 2158, 13, 400, 341, 39684, 2158, 486, 352, 1854, 264, 7395, 34, 10088, 510, 322, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.12309101104736328, "compression_ratio": 1.7117903930131004, "no_speech_prob": 3.0838553357170895e-05}, {"id": 152, "seek": 102904, "start": 1043.36, "end": 1050.56, "text": " to be reflecting that the input is the blue one, right? So we have cost network, which is a network", "tokens": [50364, 286, 992, 309, 281, 312, 4018, 337, 264, 7022, 4846, 13, 286, 362, 364, 7395, 34, 1296, 264, 5598, 295, 264, 3209, 293, 4018, 50716, 50716, 337, 5699, 286, 4846, 264, 7022, 4846, 13, 400, 2602, 11, 718, 311, 584, 286, 2826, 364, 23211, 2158, 295, 1266, 51044, 51080, 281, 312, 23543, 300, 264, 4846, 307, 264, 3344, 472, 11, 558, 30, 407, 321, 362, 2063, 3209, 11, 597, 307, 257, 3209, 51440, 51440, 300, 307, 5598, 783, 257, 2167, 39684, 2158, 13, 400, 341, 39684, 2158, 486, 352, 1854, 264, 7395, 34, 10088, 510, 322, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.12309101104736328, "compression_ratio": 1.7117903930131004, "no_speech_prob": 3.0838553357170895e-05}, {"id": 153, "seek": 102904, "start": 1050.56, "end": 1057.76, "text": " that is outputting a single scalar value. And this scalar value will go inside the MSC module here on", "tokens": [50364, 286, 992, 309, 281, 312, 4018, 337, 264, 7022, 4846, 13, 286, 362, 364, 7395, 34, 1296, 264, 5598, 295, 264, 3209, 293, 4018, 50716, 50716, 337, 5699, 286, 4846, 264, 7022, 4846, 13, 400, 2602, 11, 718, 311, 584, 286, 2826, 364, 23211, 2158, 295, 1266, 51044, 51080, 281, 312, 23543, 300, 264, 4846, 307, 264, 3344, 472, 11, 558, 30, 407, 321, 362, 2063, 3209, 11, 597, 307, 257, 3209, 51440, 51440, 300, 307, 5598, 783, 257, 2167, 39684, 2158, 13, 400, 341, 39684, 2158, 486, 352, 1854, 264, 7395, 34, 10088, 510, 322, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.12309101104736328, "compression_ratio": 1.7117903930131004, "no_speech_prob": 3.0838553357170895e-05}, {"id": 154, "seek": 105776, "start": 1057.76, "end": 1069.44, "text": " top. Let me write maybe so we can all see what's going on. So I have here my MSC. Uh, this is my", "tokens": [50364, 1192, 13, 961, 385, 2464, 1310, 370, 321, 393, 439, 536, 437, 311, 516, 322, 13, 407, 286, 362, 510, 452, 7395, 34, 13, 4019, 11, 341, 307, 452, 50948, 50948, 4470, 2445, 11, 558, 30, 407, 500, 380, 483, 9019, 1296, 4470, 293, 2063, 13, 821, 366, 732, 819, 721, 13, 51204, 51244, 407, 286, 362, 452, 7395, 34, 510, 13, 4019, 11, 293, 498, 286, 362, 341, 2146, 510, 11, 452, 3779, 307, 516, 281, 312, 4018, 13, 467, 311, 516, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.08676047708796358, "compression_ratio": 1.5340314136125655, "no_speech_prob": 1.7222599126398563e-05}, {"id": 155, "seek": 105776, "start": 1069.44, "end": 1074.56, "text": " loss function, right? So don't get confused between loss and cost. There are two different things.", "tokens": [50364, 1192, 13, 961, 385, 2464, 1310, 370, 321, 393, 439, 536, 437, 311, 516, 322, 13, 407, 286, 362, 510, 452, 7395, 34, 13, 4019, 11, 341, 307, 452, 50948, 50948, 4470, 2445, 11, 558, 30, 407, 500, 380, 483, 9019, 1296, 4470, 293, 2063, 13, 821, 366, 732, 819, 721, 13, 51204, 51244, 407, 286, 362, 452, 7395, 34, 510, 13, 4019, 11, 293, 498, 286, 362, 341, 2146, 510, 11, 452, 3779, 307, 516, 281, 312, 4018, 13, 467, 311, 516, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.08676047708796358, "compression_ratio": 1.5340314136125655, "no_speech_prob": 1.7222599126398563e-05}, {"id": 156, "seek": 105776, "start": 1075.36, "end": 1084.56, "text": " So I have my MSC here. Uh, and if I have this guy here, my target is going to be zero. It's going", "tokens": [50364, 1192, 13, 961, 385, 2464, 1310, 370, 321, 393, 439, 536, 437, 311, 516, 322, 13, 407, 286, 362, 510, 452, 7395, 34, 13, 4019, 11, 341, 307, 452, 50948, 50948, 4470, 2445, 11, 558, 30, 407, 500, 380, 483, 9019, 1296, 4470, 293, 2063, 13, 821, 366, 732, 819, 721, 13, 51204, 51244, 407, 286, 362, 452, 7395, 34, 510, 13, 4019, 11, 293, 498, 286, 362, 341, 2146, 510, 11, 452, 3779, 307, 516, 281, 312, 4018, 13, 467, 311, 516, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.08676047708796358, "compression_ratio": 1.5340314136125655, "no_speech_prob": 1.7222599126398563e-05}, {"id": 157, "seek": 108456, "start": 1084.56, "end": 1094.3999999999999, "text": " to be my Y. Y, okay. For this one. And instead, if I input this guy here to the cost network,", "tokens": [50364, 281, 312, 452, 398, 13, 398, 11, 1392, 13, 1171, 341, 472, 13, 400, 2602, 11, 498, 286, 4846, 341, 2146, 510, 281, 264, 2063, 3209, 11, 50856, 50856, 286, 2066, 281, 362, 11, 718, 311, 584, 11, 2232, 11, 294, 341, 1389, 11, 364, 23211, 1804, 1266, 13, 407, 452, 7395, 34, 294, 341, 1389, 307, 516, 51204, 51204, 281, 312, 11, 2232, 11, 291, 458, 11, 914, 3732, 6713, 1296, 264, 5598, 295, 264, 2063, 3209, 293, 4018, 13, 682, 264, 661, 51508, 51508, 1389, 11, 286, 478, 516, 281, 362, 264, 7395, 34, 1296, 264, 5598, 295, 264, 3209, 293, 1266, 13, 3301, 11, 370, 264, 3209, 11, 498, 286, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.08286529932266627, "compression_ratio": 1.7433628318584071, "no_speech_prob": 1.239997163793305e-05}, {"id": 158, "seek": 108456, "start": 1094.3999999999999, "end": 1101.36, "text": " I expect to have, let's say, uh, in this case, an arbitrary plus 10. So my MSC in this case is going", "tokens": [50364, 281, 312, 452, 398, 13, 398, 11, 1392, 13, 1171, 341, 472, 13, 400, 2602, 11, 498, 286, 4846, 341, 2146, 510, 281, 264, 2063, 3209, 11, 50856, 50856, 286, 2066, 281, 362, 11, 718, 311, 584, 11, 2232, 11, 294, 341, 1389, 11, 364, 23211, 1804, 1266, 13, 407, 452, 7395, 34, 294, 341, 1389, 307, 516, 51204, 51204, 281, 312, 11, 2232, 11, 291, 458, 11, 914, 3732, 6713, 1296, 264, 5598, 295, 264, 2063, 3209, 293, 4018, 13, 682, 264, 661, 51508, 51508, 1389, 11, 286, 478, 516, 281, 362, 264, 7395, 34, 1296, 264, 5598, 295, 264, 3209, 293, 1266, 13, 3301, 11, 370, 264, 3209, 11, 498, 286, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.08286529932266627, "compression_ratio": 1.7433628318584071, "no_speech_prob": 1.239997163793305e-05}, {"id": 159, "seek": 108456, "start": 1101.36, "end": 1107.44, "text": " to be, uh, you know, mean square error between the output of the cost network and zero. In the other", "tokens": [50364, 281, 312, 452, 398, 13, 398, 11, 1392, 13, 1171, 341, 472, 13, 400, 2602, 11, 498, 286, 4846, 341, 2146, 510, 281, 264, 2063, 3209, 11, 50856, 50856, 286, 2066, 281, 362, 11, 718, 311, 584, 11, 2232, 11, 294, 341, 1389, 11, 364, 23211, 1804, 1266, 13, 407, 452, 7395, 34, 294, 341, 1389, 307, 516, 51204, 51204, 281, 312, 11, 2232, 11, 291, 458, 11, 914, 3732, 6713, 1296, 264, 5598, 295, 264, 2063, 3209, 293, 4018, 13, 682, 264, 661, 51508, 51508, 1389, 11, 286, 478, 516, 281, 362, 264, 7395, 34, 1296, 264, 5598, 295, 264, 3209, 293, 1266, 13, 3301, 11, 370, 264, 3209, 11, 498, 286, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.08286529932266627, "compression_ratio": 1.7433628318584071, "no_speech_prob": 1.239997163793305e-05}, {"id": 160, "seek": 108456, "start": 1107.44, "end": 1113.84, "text": " case, I'm going to have the MSC between the output of the network and 10. Um, so the network, if I", "tokens": [50364, 281, 312, 452, 398, 13, 398, 11, 1392, 13, 1171, 341, 472, 13, 400, 2602, 11, 498, 286, 4846, 341, 2146, 510, 281, 264, 2063, 3209, 11, 50856, 50856, 286, 2066, 281, 362, 11, 718, 311, 584, 11, 2232, 11, 294, 341, 1389, 11, 364, 23211, 1804, 1266, 13, 407, 452, 7395, 34, 294, 341, 1389, 307, 516, 51204, 51204, 281, 312, 11, 2232, 11, 291, 458, 11, 914, 3732, 6713, 1296, 264, 5598, 295, 264, 2063, 3209, 293, 4018, 13, 682, 264, 661, 51508, 51508, 1389, 11, 286, 478, 516, 281, 362, 264, 7395, 34, 1296, 264, 5598, 295, 264, 3209, 293, 1266, 13, 3301, 11, 370, 264, 3209, 11, 498, 286, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.08286529932266627, "compression_ratio": 1.7433628318584071, "no_speech_prob": 1.239997163793305e-05}, {"id": 161, "seek": 111384, "start": 1113.84, "end": 1119.28, "text": " just train, let's say we forget about all this stuff, right? We have just a few samples with,", "tokens": [50364, 445, 3847, 11, 718, 311, 584, 321, 2870, 466, 439, 341, 1507, 11, 558, 30, 492, 362, 445, 257, 1326, 10938, 365, 11, 50636, 50636, 365, 11, 321, 519, 337, 264, 1623, 300, 264, 19265, 307, 406, 11470, 13, 407, 321, 362, 2940, 7022, 10938, 50968, 50968, 293, 2940, 3344, 10938, 13, 400, 550, 291, 3847, 257, 3209, 1270, 300, 498, 286, 829, 264, 4846, 11, 264, 7022, 51272, 51272, 472, 11, 291, 434, 516, 281, 483, 257, 4018, 294, 264, 5598, 13, 400, 550, 498, 291, 829, 264, 3344, 472, 2602, 11, 291, 434, 51512, 51512, 516, 281, 312, 19030, 264, 3209, 281, 1466, 1230, 1266, 13, 1033, 13, 407, 291, 360, 512, 4439, 294, 16235, 23475, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.07898030515576972, "compression_ratio": 1.749090909090909, "no_speech_prob": 2.583044442872051e-05}, {"id": 162, "seek": 111384, "start": 1119.28, "end": 1125.9199999999998, "text": " with, we think for the moment that the generator is not improving. So we have several pink samples", "tokens": [50364, 445, 3847, 11, 718, 311, 584, 321, 2870, 466, 439, 341, 1507, 11, 558, 30, 492, 362, 445, 257, 1326, 10938, 365, 11, 50636, 50636, 365, 11, 321, 519, 337, 264, 1623, 300, 264, 19265, 307, 406, 11470, 13, 407, 321, 362, 2940, 7022, 10938, 50968, 50968, 293, 2940, 3344, 10938, 13, 400, 550, 291, 3847, 257, 3209, 1270, 300, 498, 286, 829, 264, 4846, 11, 264, 7022, 51272, 51272, 472, 11, 291, 434, 516, 281, 483, 257, 4018, 294, 264, 5598, 13, 400, 550, 498, 291, 829, 264, 3344, 472, 2602, 11, 291, 434, 51512, 51512, 516, 281, 312, 19030, 264, 3209, 281, 1466, 1230, 1266, 13, 1033, 13, 407, 291, 360, 512, 4439, 294, 16235, 23475, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.07898030515576972, "compression_ratio": 1.749090909090909, "no_speech_prob": 2.583044442872051e-05}, {"id": 163, "seek": 111384, "start": 1125.9199999999998, "end": 1132.0, "text": " and several blue samples. And then you train a network such that if I put the input, the pink", "tokens": [50364, 445, 3847, 11, 718, 311, 584, 321, 2870, 466, 439, 341, 1507, 11, 558, 30, 492, 362, 445, 257, 1326, 10938, 365, 11, 50636, 50636, 365, 11, 321, 519, 337, 264, 1623, 300, 264, 19265, 307, 406, 11470, 13, 407, 321, 362, 2940, 7022, 10938, 50968, 50968, 293, 2940, 3344, 10938, 13, 400, 550, 291, 3847, 257, 3209, 1270, 300, 498, 286, 829, 264, 4846, 11, 264, 7022, 51272, 51272, 472, 11, 291, 434, 516, 281, 483, 257, 4018, 294, 264, 5598, 13, 400, 550, 498, 291, 829, 264, 3344, 472, 2602, 11, 291, 434, 51512, 51512, 516, 281, 312, 19030, 264, 3209, 281, 1466, 1230, 1266, 13, 1033, 13, 407, 291, 360, 512, 4439, 294, 16235, 23475, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.07898030515576972, "compression_ratio": 1.749090909090909, "no_speech_prob": 2.583044442872051e-05}, {"id": 164, "seek": 111384, "start": 1132.0, "end": 1136.8, "text": " one, you're going to get a zero in the output. And then if you put the blue one instead, you're", "tokens": [50364, 445, 3847, 11, 718, 311, 584, 321, 2870, 466, 439, 341, 1507, 11, 558, 30, 492, 362, 445, 257, 1326, 10938, 365, 11, 50636, 50636, 365, 11, 321, 519, 337, 264, 1623, 300, 264, 19265, 307, 406, 11470, 13, 407, 321, 362, 2940, 7022, 10938, 50968, 50968, 293, 2940, 3344, 10938, 13, 400, 550, 291, 3847, 257, 3209, 1270, 300, 498, 286, 829, 264, 4846, 11, 264, 7022, 51272, 51272, 472, 11, 291, 434, 516, 281, 483, 257, 4018, 294, 264, 5598, 13, 400, 550, 498, 291, 829, 264, 3344, 472, 2602, 11, 291, 434, 51512, 51512, 516, 281, 312, 19030, 264, 3209, 281, 1466, 1230, 1266, 13, 1033, 13, 407, 291, 360, 512, 4439, 294, 16235, 23475, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.07898030515576972, "compression_ratio": 1.749090909090909, "no_speech_prob": 2.583044442872051e-05}, {"id": 165, "seek": 111384, "start": 1136.8, "end": 1142.08, "text": " going to be forcing the network to learn number 10. Okay. So you do some steps in gradient descent", "tokens": [50364, 445, 3847, 11, 718, 311, 584, 321, 2870, 466, 439, 341, 1507, 11, 558, 30, 492, 362, 445, 257, 1326, 10938, 365, 11, 50636, 50636, 365, 11, 321, 519, 337, 264, 1623, 300, 264, 19265, 307, 406, 11470, 13, 407, 321, 362, 2940, 7022, 10938, 50968, 50968, 293, 2940, 3344, 10938, 13, 400, 550, 291, 3847, 257, 3209, 1270, 300, 498, 286, 829, 264, 4846, 11, 264, 7022, 51272, 51272, 472, 11, 291, 434, 516, 281, 483, 257, 4018, 294, 264, 5598, 13, 400, 550, 498, 291, 829, 264, 3344, 472, 2602, 11, 291, 434, 51512, 51512, 516, 281, 312, 19030, 264, 3209, 281, 1466, 1230, 1266, 13, 1033, 13, 407, 291, 360, 512, 4439, 294, 16235, 23475, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.07898030515576972, "compression_ratio": 1.749090909090909, "no_speech_prob": 2.583044442872051e-05}, {"id": 166, "seek": 114208, "start": 1142.08, "end": 1147.84, "text": " in the parameter space, such that in one case you get zero. The other case you get 10 whenever you", "tokens": [50364, 294, 264, 13075, 1901, 11, 1270, 300, 294, 472, 1389, 291, 483, 4018, 13, 440, 661, 1389, 291, 483, 1266, 5699, 291, 50652, 50652, 2893, 2940, 11, 2940, 10938, 11, 558, 30, 823, 300, 321, 362, 341, 3209, 11, 341, 2063, 3209, 11, 50980, 51032, 291, 393, 519, 466, 11, 2232, 11, 1419, 264, 2063, 3209, 11, 2232, 11, 281, 312, 767, 264, 4470, 337, 264, 19265, 13, 51440, 51440, 1033, 13, 400, 370, 498, 286, 362, 452, 19265, 4846, 11, 2232, 11, 5598, 783, 746, 293, 613, 2063, 3209, 11, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.13727223245721115, "compression_ratio": 1.726027397260274, "no_speech_prob": 9.504111403657589e-06}, {"id": 167, "seek": 114208, "start": 1147.84, "end": 1154.3999999999999, "text": " provide several, several samples, right? Now that we have this network, this cost network,", "tokens": [50364, 294, 264, 13075, 1901, 11, 1270, 300, 294, 472, 1389, 291, 483, 4018, 13, 440, 661, 1389, 291, 483, 1266, 5699, 291, 50652, 50652, 2893, 2940, 11, 2940, 10938, 11, 558, 30, 823, 300, 321, 362, 341, 3209, 11, 341, 2063, 3209, 11, 50980, 51032, 291, 393, 519, 466, 11, 2232, 11, 1419, 264, 2063, 3209, 11, 2232, 11, 281, 312, 767, 264, 4470, 337, 264, 19265, 13, 51440, 51440, 1033, 13, 400, 370, 498, 286, 362, 452, 19265, 4846, 11, 2232, 11, 5598, 783, 746, 293, 613, 2063, 3209, 11, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.13727223245721115, "compression_ratio": 1.726027397260274, "no_speech_prob": 9.504111403657589e-06}, {"id": 168, "seek": 114208, "start": 1155.4399999999998, "end": 1163.6, "text": " you can think about, uh, having the cost network, uh, to be actually the loss for the generator.", "tokens": [50364, 294, 264, 13075, 1901, 11, 1270, 300, 294, 472, 1389, 291, 483, 4018, 13, 440, 661, 1389, 291, 483, 1266, 5699, 291, 50652, 50652, 2893, 2940, 11, 2940, 10938, 11, 558, 30, 823, 300, 321, 362, 341, 3209, 11, 341, 2063, 3209, 11, 50980, 51032, 291, 393, 519, 466, 11, 2232, 11, 1419, 264, 2063, 3209, 11, 2232, 11, 281, 312, 767, 264, 4470, 337, 264, 19265, 13, 51440, 51440, 1033, 13, 400, 370, 498, 286, 362, 452, 19265, 4846, 11, 2232, 11, 5598, 783, 746, 293, 613, 2063, 3209, 11, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.13727223245721115, "compression_ratio": 1.726027397260274, "no_speech_prob": 9.504111403657589e-06}, {"id": 169, "seek": 114208, "start": 1163.6, "end": 1169.6799999999998, "text": " Okay. And so if I have my generator input, uh, outputting something and these cost network,", "tokens": [50364, 294, 264, 13075, 1901, 11, 1270, 300, 294, 472, 1389, 291, 483, 4018, 13, 440, 661, 1389, 291, 483, 1266, 5699, 291, 50652, 50652, 2893, 2940, 11, 2940, 10938, 11, 558, 30, 823, 300, 321, 362, 341, 3209, 11, 341, 2063, 3209, 11, 50980, 51032, 291, 393, 519, 466, 11, 2232, 11, 1419, 264, 2063, 3209, 11, 2232, 11, 281, 312, 767, 264, 4470, 337, 264, 19265, 13, 51440, 51440, 1033, 13, 400, 370, 498, 286, 362, 452, 19265, 4846, 11, 2232, 11, 5598, 783, 746, 293, 613, 2063, 3209, 11, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.13727223245721115, "compression_ratio": 1.726027397260274, "no_speech_prob": 9.504111403657589e-06}, {"id": 170, "seek": 116968, "start": 1169.68, "end": 1177.04, "text": " we say, oh, it's a very high cost. Then by trying to minimize this cost, you will try to basically", "tokens": [50364, 321, 584, 11, 1954, 11, 309, 311, 257, 588, 1090, 2063, 13, 1396, 538, 1382, 281, 17522, 341, 2063, 11, 291, 486, 853, 281, 1936, 50732, 50732, 8460, 746, 300, 390, 9105, 1455, 264, 2063, 3209, 6530, 291, 257, 2295, 2158, 13, 1033, 13, 51044, 51104, 1119, 309, 1455, 2020, 30, 51168, 51264, 7497, 291, 445, 2661, 17594, 264, 2649, 1296, 5497, 293, 4470, 30, 51404, 51476], "temperature": 0.0, "avg_logprob": -0.10828193028767903, "compression_ratio": 1.4512820512820512, "no_speech_prob": 1.921840157592669e-05}, {"id": 171, "seek": 116968, "start": 1177.04, "end": 1183.28, "text": " generate something that was initially making the cost network providing you a low value. Okay.", "tokens": [50364, 321, 584, 11, 1954, 11, 309, 311, 257, 588, 1090, 2063, 13, 1396, 538, 1382, 281, 17522, 341, 2063, 11, 291, 486, 853, 281, 1936, 50732, 50732, 8460, 746, 300, 390, 9105, 1455, 264, 2063, 3209, 6530, 291, 257, 2295, 2158, 13, 1033, 13, 51044, 51104, 1119, 309, 1455, 2020, 30, 51168, 51264, 7497, 291, 445, 2661, 17594, 264, 2649, 1296, 5497, 293, 4470, 30, 51404, 51476], "temperature": 0.0, "avg_logprob": -0.10828193028767903, "compression_ratio": 1.4512820512820512, "no_speech_prob": 1.921840157592669e-05}, {"id": 172, "seek": 116968, "start": 1184.48, "end": 1185.76, "text": " Is it making sense?", "tokens": [50364, 321, 584, 11, 1954, 11, 309, 311, 257, 588, 1090, 2063, 13, 1396, 538, 1382, 281, 17522, 341, 2063, 11, 291, 486, 853, 281, 1936, 50732, 50732, 8460, 746, 300, 390, 9105, 1455, 264, 2063, 3209, 6530, 291, 257, 2295, 2158, 13, 1033, 13, 51044, 51104, 1119, 309, 1455, 2020, 30, 51168, 51264, 7497, 291, 445, 2661, 17594, 264, 2649, 1296, 5497, 293, 4470, 30, 51404, 51476], "temperature": 0.0, "avg_logprob": -0.10828193028767903, "compression_ratio": 1.4512820512820512, "no_speech_prob": 1.921840157592669e-05}, {"id": 173, "seek": 116968, "start": 1187.68, "end": 1190.48, "text": " Could you just quickly clarify the difference between costs and loss?", "tokens": [50364, 321, 584, 11, 1954, 11, 309, 311, 257, 588, 1090, 2063, 13, 1396, 538, 1382, 281, 17522, 341, 2063, 11, 291, 486, 853, 281, 1936, 50732, 50732, 8460, 746, 300, 390, 9105, 1455, 264, 2063, 3209, 6530, 291, 257, 2295, 2158, 13, 1033, 13, 51044, 51104, 1119, 309, 1455, 2020, 30, 51168, 51264, 7497, 291, 445, 2661, 17594, 264, 2649, 1296, 5497, 293, 4470, 30, 51404, 51476], "temperature": 0.0, "avg_logprob": -0.10828193028767903, "compression_ratio": 1.4512820512820512, "no_speech_prob": 1.921840157592669e-05}, {"id": 174, "seek": 119048, "start": 1190.48, "end": 1199.04, "text": " Uh, the loss is what we use in order to train something. Okay. So my loss in this case is the", "tokens": [50364, 4019, 11, 264, 4470, 307, 437, 321, 764, 294, 1668, 281, 3847, 746, 13, 1033, 13, 407, 452, 4470, 294, 341, 1389, 307, 264, 50792, 50792, 7395, 34, 4470, 13, 639, 307, 452, 4470, 13, 407, 294, 1668, 281, 3847, 452, 2063, 3209, 11, 286, 486, 362, 257, 4470, 2445, 11, 51444, 51472, 597, 307, 264, 7395, 34, 4470, 2445, 538, 46608, 264, 7395, 34, 4470, 2445, 13, 286, 486, 312, 3097, 264, 2063, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.12222716747186123, "compression_ratio": 1.7577639751552796, "no_speech_prob": 2.5846462449408136e-05}, {"id": 175, "seek": 119048, "start": 1199.04, "end": 1212.08, "text": " MSC loss. This is my loss. So in order to train my cost network, I will have a loss function,", "tokens": [50364, 4019, 11, 264, 4470, 307, 437, 321, 764, 294, 1668, 281, 3847, 746, 13, 1033, 13, 407, 452, 4470, 294, 341, 1389, 307, 264, 50792, 50792, 7395, 34, 4470, 13, 639, 307, 452, 4470, 13, 407, 294, 1668, 281, 3847, 452, 2063, 3209, 11, 286, 486, 362, 257, 4470, 2445, 11, 51444, 51472, 597, 307, 264, 7395, 34, 4470, 2445, 538, 46608, 264, 7395, 34, 4470, 2445, 13, 286, 486, 312, 3097, 264, 2063, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.12222716747186123, "compression_ratio": 1.7577639751552796, "no_speech_prob": 2.5846462449408136e-05}, {"id": 176, "seek": 119048, "start": 1212.64, "end": 1218.8, "text": " which is the MSC loss function by minimizing the MSC loss function. I will be training the cost", "tokens": [50364, 4019, 11, 264, 4470, 307, 437, 321, 764, 294, 1668, 281, 3847, 746, 13, 1033, 13, 407, 452, 4470, 294, 341, 1389, 307, 264, 50792, 50792, 7395, 34, 4470, 13, 639, 307, 452, 4470, 13, 407, 294, 1668, 281, 3847, 452, 2063, 3209, 11, 286, 486, 362, 257, 4470, 2445, 11, 51444, 51472, 597, 307, 264, 7395, 34, 4470, 2445, 538, 46608, 264, 7395, 34, 4470, 2445, 13, 286, 486, 312, 3097, 264, 2063, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.12222716747186123, "compression_ratio": 1.7577639751552796, "no_speech_prob": 2.5846462449408136e-05}, {"id": 177, "seek": 121880, "start": 1218.8, "end": 1227.36, "text": " network. Now the fucked up part comes and I'm going to say that for my generator, the loss", "tokens": [50364, 3209, 13, 823, 264, 22518, 493, 644, 1487, 293, 286, 478, 516, 281, 584, 300, 337, 452, 19265, 11, 264, 4470, 50792, 50792, 2445, 300, 286, 528, 281, 17522, 307, 264, 2063, 3209, 13, 407, 337, 613, 19265, 11, 264, 4470, 307, 51188, 51188, 264, 2063, 293, 286, 853, 281, 17522, 341, 2146, 5598, 13, 1033, 13, 407, 341, 307, 611, 11, 1105, 11, 4972, 281, 437, 264, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.16212615701887342, "compression_ratio": 1.6331360946745561, "no_speech_prob": 1.0875828593270853e-05}, {"id": 178, "seek": 121880, "start": 1227.36, "end": 1235.28, "text": " function that I want to minimize is the cost network. So for these generator, the loss is", "tokens": [50364, 3209, 13, 823, 264, 22518, 493, 644, 1487, 293, 286, 478, 516, 281, 584, 300, 337, 452, 19265, 11, 264, 4470, 50792, 50792, 2445, 300, 286, 528, 281, 17522, 307, 264, 2063, 3209, 13, 407, 337, 613, 19265, 11, 264, 4470, 307, 51188, 51188, 264, 2063, 293, 286, 853, 281, 17522, 341, 2146, 5598, 13, 1033, 13, 407, 341, 307, 611, 11, 1105, 11, 4972, 281, 437, 264, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.16212615701887342, "compression_ratio": 1.6331360946745561, "no_speech_prob": 1.0875828593270853e-05}, {"id": 179, "seek": 121880, "start": 1235.28, "end": 1243.76, "text": " the cost and I try to minimize this guy output. Okay. So this is also, um, relative to what the", "tokens": [50364, 3209, 13, 823, 264, 22518, 493, 644, 1487, 293, 286, 478, 516, 281, 584, 300, 337, 452, 19265, 11, 264, 4470, 50792, 50792, 2445, 300, 286, 528, 281, 17522, 307, 264, 2063, 3209, 13, 407, 337, 613, 19265, 11, 264, 4470, 307, 51188, 51188, 264, 2063, 293, 286, 853, 281, 17522, 341, 2146, 5598, 13, 1033, 13, 407, 341, 307, 611, 11, 1105, 11, 4972, 281, 437, 264, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.16212615701887342, "compression_ratio": 1.6331360946745561, "no_speech_prob": 1.0875828593270853e-05}, {"id": 180, "seek": 124376, "start": 1243.76, "end": 1249.12, "text": " honest teaching with the energy based models, you have energies and we try to have low energies", "tokens": [50364, 3245, 4571, 365, 264, 2281, 2361, 5245, 11, 291, 362, 25737, 293, 321, 853, 281, 362, 2295, 25737, 50632, 50640, 807, 4464, 2144, 295, 257, 4470, 2445, 13, 407, 264, 4470, 2445, 307, 437, 291, 764, 294, 1668, 281, 50944, 50984, 3847, 264, 9834, 295, 257, 3209, 13, 1033, 13, 407, 300, 311, 264, 2649, 13, 51164, 51288, 3301, 11, 370, 498, 264, 3209, 11, 1392, 13, 407, 1071, 11, 2232, 11, 4497, 935, 307, 300, 257, 2063, 307, 411, 257, 51704, 51744], "temperature": 0.0, "avg_logprob": -0.26507702995749083, "compression_ratio": 1.6699507389162562, "no_speech_prob": 4.677114702644758e-05}, {"id": 181, "seek": 124376, "start": 1249.28, "end": 1255.36, "text": " through minimization of a loss function. So the loss function is what you use in order to", "tokens": [50364, 3245, 4571, 365, 264, 2281, 2361, 5245, 11, 291, 362, 25737, 293, 321, 853, 281, 362, 2295, 25737, 50632, 50640, 807, 4464, 2144, 295, 257, 4470, 2445, 13, 407, 264, 4470, 2445, 307, 437, 291, 764, 294, 1668, 281, 50944, 50984, 3847, 264, 9834, 295, 257, 3209, 13, 1033, 13, 407, 300, 311, 264, 2649, 13, 51164, 51288, 3301, 11, 370, 498, 264, 3209, 11, 1392, 13, 407, 1071, 11, 2232, 11, 4497, 935, 307, 300, 257, 2063, 307, 411, 257, 51704, 51744], "temperature": 0.0, "avg_logprob": -0.26507702995749083, "compression_ratio": 1.6699507389162562, "no_speech_prob": 4.677114702644758e-05}, {"id": 182, "seek": 124376, "start": 1256.16, "end": 1259.76, "text": " train the parameters of a network. Okay. So that's the difference.", "tokens": [50364, 3245, 4571, 365, 264, 2281, 2361, 5245, 11, 291, 362, 25737, 293, 321, 853, 281, 362, 2295, 25737, 50632, 50640, 807, 4464, 2144, 295, 257, 4470, 2445, 13, 407, 264, 4470, 2445, 307, 437, 291, 764, 294, 1668, 281, 50944, 50984, 3847, 264, 9834, 295, 257, 3209, 13, 1033, 13, 407, 300, 311, 264, 2649, 13, 51164, 51288, 3301, 11, 370, 498, 264, 3209, 11, 1392, 13, 407, 1071, 11, 2232, 11, 4497, 935, 307, 300, 257, 2063, 307, 411, 257, 51704, 51744], "temperature": 0.0, "avg_logprob": -0.26507702995749083, "compression_ratio": 1.6699507389162562, "no_speech_prob": 4.677114702644758e-05}, {"id": 183, "seek": 124376, "start": 1262.24, "end": 1270.56, "text": " Um, so if the network, okay. So another, uh, additional point is that a cost is like a", "tokens": [50364, 3245, 4571, 365, 264, 2281, 2361, 5245, 11, 291, 362, 25737, 293, 321, 853, 281, 362, 2295, 25737, 50632, 50640, 807, 4464, 2144, 295, 257, 4470, 2445, 13, 407, 264, 4470, 2445, 307, 437, 291, 764, 294, 1668, 281, 50944, 50984, 3847, 264, 9834, 295, 257, 3209, 13, 1033, 13, 407, 300, 311, 264, 2649, 13, 51164, 51288, 3301, 11, 370, 498, 264, 3209, 11, 1392, 13, 407, 1071, 11, 2232, 11, 4497, 935, 307, 300, 257, 2063, 307, 411, 257, 51704, 51744], "temperature": 0.0, "avg_logprob": -0.26507702995749083, "compression_ratio": 1.6699507389162562, "no_speech_prob": 4.677114702644758e-05}, {"id": 184, "seek": 127056, "start": 1270.56, "end": 1279.12, "text": " evaluation of some network performance. So if my generator outputs a bad X, like, which is not", "tokens": [50364, 13344, 295, 512, 3209, 3389, 13, 407, 498, 452, 19265, 23930, 257, 1578, 1783, 11, 411, 11, 597, 307, 406, 50792, 50792, 1238, 665, 1237, 11, 550, 291, 434, 516, 281, 362, 257, 1090, 2063, 13, 1033, 13, 467, 311, 411, 257, 1090, 2281, 11, 51108, 51176, 457, 294, 1668, 281, 17522, 341, 2281, 11, 2673, 291, 362, 281, 17522, 613, 15352, 13, 1033, 13, 407, 11, 51420, 51420, 457, 797, 11, 264, 7123, 11, 437, 321, 411, 281, 764, 307, 300, 264, 4470, 307, 437, 291, 17522, 294, 1668, 281, 51704, 51728], "temperature": 0.0, "avg_logprob": -0.178549535650956, "compression_ratio": 1.6801801801801801, "no_speech_prob": 3.645355172920972e-05}, {"id": 185, "seek": 127056, "start": 1279.12, "end": 1285.44, "text": " pretty good looking, then you're going to have a high cost. Okay. It's like a high energy,", "tokens": [50364, 13344, 295, 512, 3209, 3389, 13, 407, 498, 452, 19265, 23930, 257, 1578, 1783, 11, 411, 11, 597, 307, 406, 50792, 50792, 1238, 665, 1237, 11, 550, 291, 434, 516, 281, 362, 257, 1090, 2063, 13, 1033, 13, 467, 311, 411, 257, 1090, 2281, 11, 51108, 51176, 457, 294, 1668, 281, 17522, 341, 2281, 11, 2673, 291, 362, 281, 17522, 613, 15352, 13, 1033, 13, 407, 11, 51420, 51420, 457, 797, 11, 264, 7123, 11, 437, 321, 411, 281, 764, 307, 300, 264, 4470, 307, 437, 291, 17522, 294, 1668, 281, 51704, 51728], "temperature": 0.0, "avg_logprob": -0.178549535650956, "compression_ratio": 1.6801801801801801, "no_speech_prob": 3.645355172920972e-05}, {"id": 186, "seek": 127056, "start": 1286.8, "end": 1291.6799999999998, "text": " but in order to minimize this energy, usually you have to minimize these losses. Okay. So,", "tokens": [50364, 13344, 295, 512, 3209, 3389, 13, 407, 498, 452, 19265, 23930, 257, 1578, 1783, 11, 411, 11, 597, 307, 406, 50792, 50792, 1238, 665, 1237, 11, 550, 291, 434, 516, 281, 362, 257, 1090, 2063, 13, 1033, 13, 467, 311, 411, 257, 1090, 2281, 11, 51108, 51176, 457, 294, 1668, 281, 17522, 341, 2281, 11, 2673, 291, 362, 281, 17522, 613, 15352, 13, 1033, 13, 407, 11, 51420, 51420, 457, 797, 11, 264, 7123, 11, 437, 321, 411, 281, 764, 307, 300, 264, 4470, 307, 437, 291, 17522, 294, 1668, 281, 51704, 51728], "temperature": 0.0, "avg_logprob": -0.178549535650956, "compression_ratio": 1.6801801801801801, "no_speech_prob": 3.645355172920972e-05}, {"id": 187, "seek": 127056, "start": 1291.6799999999998, "end": 1297.36, "text": " but again, the definition, what we like to use is that the loss is what you minimize in order to", "tokens": [50364, 13344, 295, 512, 3209, 3389, 13, 407, 498, 452, 19265, 23930, 257, 1578, 1783, 11, 411, 11, 597, 307, 406, 50792, 50792, 1238, 665, 1237, 11, 550, 291, 434, 516, 281, 362, 257, 1090, 2063, 13, 1033, 13, 467, 311, 411, 257, 1090, 2281, 11, 51108, 51176, 457, 294, 1668, 281, 17522, 341, 2281, 11, 2673, 291, 362, 281, 17522, 613, 15352, 13, 1033, 13, 407, 11, 51420, 51420, 457, 797, 11, 264, 7123, 11, 437, 321, 411, 281, 764, 307, 300, 264, 4470, 307, 437, 291, 17522, 294, 1668, 281, 51704, 51728], "temperature": 0.0, "avg_logprob": -0.178549535650956, "compression_ratio": 1.6801801801801801, "no_speech_prob": 3.645355172920972e-05}, {"id": 188, "seek": 129736, "start": 1297.36, "end": 1306.3999999999999, "text": " train the parameters of a network. So instead, like a cost can be thought as, you know, I take an", "tokens": [50364, 3847, 264, 9834, 295, 257, 3209, 13, 407, 2602, 11, 411, 257, 2063, 393, 312, 1194, 382, 11, 291, 458, 11, 286, 747, 364, 50816, 50816, 3069, 293, 550, 286, 362, 364, 3069, 11, 286, 362, 257, 2063, 337, 1940, 300, 2685, 3069, 13, 1033, 13, 407, 291, 51200, 51208, 747, 364, 3069, 11, 597, 307, 411, 3579, 364, 3796, 466, 4473, 721, 13, 400, 550, 264, 2063, 307, 516, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.1755253688709156, "compression_ratio": 1.6179775280898876, "no_speech_prob": 2.7038979169446975e-05}, {"id": 189, "seek": 129736, "start": 1306.3999999999999, "end": 1314.08, "text": " action and then I have an action, I have a cost for taking that specific action. Okay. So you", "tokens": [50364, 3847, 264, 9834, 295, 257, 3209, 13, 407, 2602, 11, 411, 257, 2063, 393, 312, 1194, 382, 11, 291, 458, 11, 286, 747, 364, 50816, 50816, 3069, 293, 550, 286, 362, 364, 3069, 11, 286, 362, 257, 2063, 337, 1940, 300, 2685, 3069, 13, 1033, 13, 407, 291, 51200, 51208, 747, 364, 3069, 11, 597, 307, 411, 3579, 364, 3796, 466, 4473, 721, 13, 400, 550, 264, 2063, 307, 516, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.1755253688709156, "compression_ratio": 1.6179775280898876, "no_speech_prob": 2.7038979169446975e-05}, {"id": 190, "seek": 129736, "start": 1314.24, "end": 1320.8, "text": " take an action, which is like writing an email about changing things. And then the cost is going", "tokens": [50364, 3847, 264, 9834, 295, 257, 3209, 13, 407, 2602, 11, 411, 257, 2063, 393, 312, 1194, 382, 11, 291, 458, 11, 286, 747, 364, 50816, 50816, 3069, 293, 550, 286, 362, 364, 3069, 11, 286, 362, 257, 2063, 337, 1940, 300, 2685, 3069, 13, 1033, 13, 407, 291, 51200, 51208, 747, 364, 3069, 11, 597, 307, 411, 3579, 364, 3796, 466, 4473, 721, 13, 400, 550, 264, 2063, 307, 516, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.1755253688709156, "compression_ratio": 1.6179775280898876, "no_speech_prob": 2.7038979169446975e-05}, {"id": 191, "seek": 132080, "start": 1320.8, "end": 1327.76, "text": " to be having everyone piece at you. Hmm. Makes sense. Right. You always learn something new.", "tokens": [50364, 281, 312, 1419, 1518, 2522, 412, 291, 13, 8239, 13, 25245, 2020, 13, 1779, 13, 509, 1009, 1466, 746, 777, 13, 50712, 50784, 1033, 13, 5358, 1651, 370, 1400, 13, 50860, 50964, 3301, 11, 2597, 11, 286, 478, 920, 257, 857, 9019, 466, 264, 2063, 293, 19265, 13, 407, 337, 51228, 51228, 19265, 300, 23815, 264, 3344, 1783, 11, 321, 528, 281, 3488, 264, 2063, 11, 457, 291, 445, 2835, 51528, 51528, 300, 321, 528, 281, 17522, 264, 2063, 307, 411, 264, 4470, 2445, 13, 407, 264, 2063, 307, 411, 264, 4470, 2445, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.37413465362234216, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.9222270566388033e-05}, {"id": 192, "seek": 132080, "start": 1329.2, "end": 1330.72, "text": " Okay. Other questions so far.", "tokens": [50364, 281, 312, 1419, 1518, 2522, 412, 291, 13, 8239, 13, 25245, 2020, 13, 1779, 13, 509, 1009, 1466, 746, 777, 13, 50712, 50784, 1033, 13, 5358, 1651, 370, 1400, 13, 50860, 50964, 3301, 11, 2597, 11, 286, 478, 920, 257, 857, 9019, 466, 264, 2063, 293, 19265, 13, 407, 337, 51228, 51228, 19265, 300, 23815, 264, 3344, 1783, 11, 321, 528, 281, 3488, 264, 2063, 11, 457, 291, 445, 2835, 51528, 51528, 300, 321, 528, 281, 17522, 264, 2063, 307, 411, 264, 4470, 2445, 13, 407, 264, 2063, 307, 411, 264, 4470, 2445, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.37413465362234216, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.9222270566388033e-05}, {"id": 193, "seek": 132080, "start": 1332.8, "end": 1338.08, "text": " Um, sorry, I'm still a bit confused about the cost and generator. So for", "tokens": [50364, 281, 312, 1419, 1518, 2522, 412, 291, 13, 8239, 13, 25245, 2020, 13, 1779, 13, 509, 1009, 1466, 746, 777, 13, 50712, 50784, 1033, 13, 5358, 1651, 370, 1400, 13, 50860, 50964, 3301, 11, 2597, 11, 286, 478, 920, 257, 857, 9019, 466, 264, 2063, 293, 19265, 13, 407, 337, 51228, 51228, 19265, 300, 23815, 264, 3344, 1783, 11, 321, 528, 281, 3488, 264, 2063, 11, 457, 291, 445, 2835, 51528, 51528, 300, 321, 528, 281, 17522, 264, 2063, 307, 411, 264, 4470, 2445, 13, 407, 264, 2063, 307, 411, 264, 4470, 2445, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.37413465362234216, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.9222270566388033e-05}, {"id": 194, "seek": 132080, "start": 1338.08, "end": 1344.08, "text": " generator that generates the blue X, we want to increase the cost, but you just mentioned", "tokens": [50364, 281, 312, 1419, 1518, 2522, 412, 291, 13, 8239, 13, 25245, 2020, 13, 1779, 13, 509, 1009, 1466, 746, 777, 13, 50712, 50784, 1033, 13, 5358, 1651, 370, 1400, 13, 50860, 50964, 3301, 11, 2597, 11, 286, 478, 920, 257, 857, 9019, 466, 264, 2063, 293, 19265, 13, 407, 337, 51228, 51228, 19265, 300, 23815, 264, 3344, 1783, 11, 321, 528, 281, 3488, 264, 2063, 11, 457, 291, 445, 2835, 51528, 51528, 300, 321, 528, 281, 17522, 264, 2063, 307, 411, 264, 4470, 2445, 13, 407, 264, 2063, 307, 411, 264, 4470, 2445, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.37413465362234216, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.9222270566388033e-05}, {"id": 195, "seek": 132080, "start": 1344.08, "end": 1350.72, "text": " that we want to minimize the cost is like the loss function. So the cost is like the loss function", "tokens": [50364, 281, 312, 1419, 1518, 2522, 412, 291, 13, 8239, 13, 25245, 2020, 13, 1779, 13, 509, 1009, 1466, 746, 777, 13, 50712, 50784, 1033, 13, 5358, 1651, 370, 1400, 13, 50860, 50964, 3301, 11, 2597, 11, 286, 478, 920, 257, 857, 9019, 466, 264, 2063, 293, 19265, 13, 407, 337, 51228, 51228, 19265, 300, 23815, 264, 3344, 1783, 11, 321, 528, 281, 3488, 264, 2063, 11, 457, 291, 445, 2835, 51528, 51528, 300, 321, 528, 281, 17522, 264, 2063, 307, 411, 264, 4470, 2445, 13, 407, 264, 2063, 307, 411, 264, 4470, 2445, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.37413465362234216, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.9222270566388033e-05}, {"id": 196, "seek": 135072, "start": 1350.72, "end": 1356.88, "text": " for the generator and we want to minimize the loss. So we want to increase the costs or we want to", "tokens": [50364, 337, 264, 19265, 293, 321, 528, 281, 17522, 264, 4470, 13, 407, 321, 528, 281, 3488, 264, 5497, 420, 321, 528, 281, 50672, 50672, 11514, 264, 2063, 337, 264, 19265, 13, 1171, 264, 19265, 11, 291, 528, 281, 17522, 264, 2063, 13, 407, 321, 51028, 51028, 3847, 264, 19265, 807, 4464, 2144, 295, 264, 2063, 3209, 2158, 13, 1033, 13, 407, 456, 307, 732, 3166, 51536, 51536, 295, 341, 551, 13, 961, 385, 1319, 2017, 13, 407, 700, 644, 307, 516, 281, 312, 264, 3097, 295, 341, 2146, 510, 13, 51788, 51840], "temperature": 0.0, "avg_logprob": -0.12271513837449094, "compression_ratio": 2.005235602094241, "no_speech_prob": 0.00017670357192400843}, {"id": 197, "seek": 135072, "start": 1356.88, "end": 1364.0, "text": " decrease the cost for the generator. For the generator, you want to minimize the cost. So we", "tokens": [50364, 337, 264, 19265, 293, 321, 528, 281, 17522, 264, 4470, 13, 407, 321, 528, 281, 3488, 264, 5497, 420, 321, 528, 281, 50672, 50672, 11514, 264, 2063, 337, 264, 19265, 13, 1171, 264, 19265, 11, 291, 528, 281, 17522, 264, 2063, 13, 407, 321, 51028, 51028, 3847, 264, 19265, 807, 4464, 2144, 295, 264, 2063, 3209, 2158, 13, 1033, 13, 407, 456, 307, 732, 3166, 51536, 51536, 295, 341, 551, 13, 961, 385, 1319, 2017, 13, 407, 700, 644, 307, 516, 281, 312, 264, 3097, 295, 341, 2146, 510, 13, 51788, 51840], "temperature": 0.0, "avg_logprob": -0.12271513837449094, "compression_ratio": 2.005235602094241, "no_speech_prob": 0.00017670357192400843}, {"id": 198, "seek": 135072, "start": 1364.0, "end": 1374.16, "text": " train the generator through minimization of the cost network value. Okay. So there is two parts", "tokens": [50364, 337, 264, 19265, 293, 321, 528, 281, 17522, 264, 4470, 13, 407, 321, 528, 281, 3488, 264, 5497, 420, 321, 528, 281, 50672, 50672, 11514, 264, 2063, 337, 264, 19265, 13, 1171, 264, 19265, 11, 291, 528, 281, 17522, 264, 2063, 13, 407, 321, 51028, 51028, 3847, 264, 19265, 807, 4464, 2144, 295, 264, 2063, 3209, 2158, 13, 1033, 13, 407, 456, 307, 732, 3166, 51536, 51536, 295, 341, 551, 13, 961, 385, 1319, 2017, 13, 407, 700, 644, 307, 516, 281, 312, 264, 3097, 295, 341, 2146, 510, 13, 51788, 51840], "temperature": 0.0, "avg_logprob": -0.12271513837449094, "compression_ratio": 2.005235602094241, "no_speech_prob": 0.00017670357192400843}, {"id": 199, "seek": 135072, "start": 1374.16, "end": 1379.2, "text": " of this thing. Let me change color. So first part is going to be the training of this guy here.", "tokens": [50364, 337, 264, 19265, 293, 321, 528, 281, 17522, 264, 4470, 13, 407, 321, 528, 281, 3488, 264, 5497, 420, 321, 528, 281, 50672, 50672, 11514, 264, 2063, 337, 264, 19265, 13, 1171, 264, 19265, 11, 291, 528, 281, 17522, 264, 2063, 13, 407, 321, 51028, 51028, 3847, 264, 19265, 807, 4464, 2144, 295, 264, 2063, 3209, 2158, 13, 1033, 13, 407, 456, 307, 732, 3166, 51536, 51536, 295, 341, 551, 13, 961, 385, 1319, 2017, 13, 407, 700, 644, 307, 516, 281, 312, 264, 3097, 295, 341, 2146, 510, 13, 51788, 51840], "temperature": 0.0, "avg_logprob": -0.12271513837449094, "compression_ratio": 2.005235602094241, "no_speech_prob": 0.00017670357192400843}, {"id": 200, "seek": 137920, "start": 1379.2, "end": 1387.04, "text": " And the training of the cost network is made through the minimization of the MSC on top of here.", "tokens": [50364, 400, 264, 3097, 295, 264, 2063, 3209, 307, 1027, 807, 264, 4464, 2144, 295, 264, 7395, 34, 322, 1192, 295, 510, 13, 50756, 50756, 407, 341, 307, 264, 4470, 337, 264, 2063, 3209, 13, 407, 264, 2063, 11, 264, 7395, 34, 510, 307, 1027, 1296, 4018, 13, 51176, 51224, 14159, 286, 4846, 257, 7022, 4846, 13, 400, 550, 718, 311, 584, 337, 341, 1365, 11, 411, 337, 9717, 295, 1365, 11, 51572, 51600], "temperature": 0.0, "avg_logprob": -0.12167856852213542, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.00013699146802537143}, {"id": 201, "seek": 137920, "start": 1387.04, "end": 1395.44, "text": " So this is the loss for the cost network. So the cost, the MSC here is made between zero.", "tokens": [50364, 400, 264, 3097, 295, 264, 2063, 3209, 307, 1027, 807, 264, 4464, 2144, 295, 264, 7395, 34, 322, 1192, 295, 510, 13, 50756, 50756, 407, 341, 307, 264, 4470, 337, 264, 2063, 3209, 13, 407, 264, 2063, 11, 264, 7395, 34, 510, 307, 1027, 1296, 4018, 13, 51176, 51224, 14159, 286, 4846, 257, 7022, 4846, 13, 400, 550, 718, 311, 584, 337, 341, 1365, 11, 411, 337, 9717, 295, 1365, 11, 51572, 51600], "temperature": 0.0, "avg_logprob": -0.12167856852213542, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.00013699146802537143}, {"id": 202, "seek": 137920, "start": 1396.4, "end": 1403.3600000000001, "text": " Whenever I input a pink input. And then let's say for this example, like for sake of example,", "tokens": [50364, 400, 264, 3097, 295, 264, 2063, 3209, 307, 1027, 807, 264, 4464, 2144, 295, 264, 7395, 34, 322, 1192, 295, 510, 13, 50756, 50756, 407, 341, 307, 264, 4470, 337, 264, 2063, 3209, 13, 407, 264, 2063, 11, 264, 7395, 34, 510, 307, 1027, 1296, 4018, 13, 51176, 51224, 14159, 286, 4846, 257, 7022, 4846, 13, 400, 550, 718, 311, 584, 337, 341, 1365, 11, 411, 337, 9717, 295, 1365, 11, 51572, 51600], "temperature": 0.0, "avg_logprob": -0.12167856852213542, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.00013699146802537143}, {"id": 203, "seek": 140336, "start": 1403.36, "end": 1410.9599999999998, "text": " I like to have an MSC against 10 whenever I input a blue sample. Okay. So now we perform several", "tokens": [50364, 286, 411, 281, 362, 364, 7395, 34, 1970, 1266, 5699, 286, 4846, 257, 3344, 6889, 13, 1033, 13, 407, 586, 321, 2042, 2940, 50744, 50744, 4439, 295, 16235, 23475, 294, 264, 13075, 1901, 295, 264, 2063, 3209, 1270, 300, 321, 17522, 341, 51108, 51108, 4470, 13, 1033, 13, 407, 586, 321, 362, 257, 3209, 510, 11, 597, 307, 516, 281, 312, 5598, 783, 4018, 13, 759, 286, 829, 257, 7022, 51440, 51440], "temperature": 0.0, "avg_logprob": -0.12260266896840688, "compression_ratio": 1.4896907216494846, "no_speech_prob": 0.00016157605568878353}, {"id": 204, "seek": 140336, "start": 1410.9599999999998, "end": 1418.24, "text": " steps of gradient descent in the parameter space of the cost network such that we minimize this", "tokens": [50364, 286, 411, 281, 362, 364, 7395, 34, 1970, 1266, 5699, 286, 4846, 257, 3344, 6889, 13, 1033, 13, 407, 586, 321, 2042, 2940, 50744, 50744, 4439, 295, 16235, 23475, 294, 264, 13075, 1901, 295, 264, 2063, 3209, 1270, 300, 321, 17522, 341, 51108, 51108, 4470, 13, 1033, 13, 407, 586, 321, 362, 257, 3209, 510, 11, 597, 307, 516, 281, 312, 5598, 783, 4018, 13, 759, 286, 829, 257, 7022, 51440, 51440], "temperature": 0.0, "avg_logprob": -0.12260266896840688, "compression_ratio": 1.4896907216494846, "no_speech_prob": 0.00016157605568878353}, {"id": 205, "seek": 140336, "start": 1418.24, "end": 1424.8799999999999, "text": " loss. Okay. So now we have a network here, which is going to be outputting zero. If I put a pink", "tokens": [50364, 286, 411, 281, 362, 364, 7395, 34, 1970, 1266, 5699, 286, 4846, 257, 3344, 6889, 13, 1033, 13, 407, 586, 321, 2042, 2940, 50744, 50744, 4439, 295, 16235, 23475, 294, 264, 13075, 1901, 295, 264, 2063, 3209, 1270, 300, 321, 17522, 341, 51108, 51108, 4470, 13, 1033, 13, 407, 586, 321, 362, 257, 3209, 510, 11, 597, 307, 516, 281, 312, 5598, 783, 4018, 13, 759, 286, 829, 257, 7022, 51440, 51440], "temperature": 0.0, "avg_logprob": -0.12260266896840688, "compression_ratio": 1.4896907216494846, "no_speech_prob": 0.00016157605568878353}, {"id": 206, "seek": 142488, "start": 1424.88, "end": 1434.16, "text": " input and input and output a 10, if I input a blue input so far, are you with me? Yeah. So it's like", "tokens": [50364, 4846, 293, 4846, 293, 5598, 257, 1266, 11, 498, 286, 4846, 257, 3344, 4846, 370, 1400, 11, 366, 291, 365, 385, 30, 865, 13, 407, 309, 311, 411, 50828, 50828, 2063, 11, 264, 3209, 5497, 486, 8460, 257, 1090, 2158, 337, 3344, 1783, 11, 558, 30, 865, 13, 663, 311, 437, 321, 3847, 51144, 51144, 341, 2063, 281, 360, 13, 1033, 13, 407, 341, 2063, 3209, 486, 362, 281, 8460, 512, 2416, 2158, 294, 341, 1389, 11, 51476, 51476], "temperature": 0.0, "avg_logprob": -0.20929298871829186, "compression_ratio": 1.6704545454545454, "no_speech_prob": 9.886929910862818e-05}, {"id": 207, "seek": 142488, "start": 1434.16, "end": 1440.48, "text": " cost, the network costs will generate a high value for blue X, right? Yeah. That's what we train", "tokens": [50364, 4846, 293, 4846, 293, 5598, 257, 1266, 11, 498, 286, 4846, 257, 3344, 4846, 370, 1400, 11, 366, 291, 365, 385, 30, 865, 13, 407, 309, 311, 411, 50828, 50828, 2063, 11, 264, 3209, 5497, 486, 8460, 257, 1090, 2158, 337, 3344, 1783, 11, 558, 30, 865, 13, 663, 311, 437, 321, 3847, 51144, 51144, 341, 2063, 281, 360, 13, 1033, 13, 407, 341, 2063, 3209, 486, 362, 281, 8460, 512, 2416, 2158, 294, 341, 1389, 11, 51476, 51476], "temperature": 0.0, "avg_logprob": -0.20929298871829186, "compression_ratio": 1.6704545454545454, "no_speech_prob": 9.886929910862818e-05}, {"id": 208, "seek": 142488, "start": 1440.48, "end": 1447.1200000000001, "text": " this cost to do. Okay. So this cost network will have to generate some large value in this case,", "tokens": [50364, 4846, 293, 4846, 293, 5598, 257, 1266, 11, 498, 286, 4846, 257, 3344, 4846, 370, 1400, 11, 366, 291, 365, 385, 30, 865, 13, 407, 309, 311, 411, 50828, 50828, 2063, 11, 264, 3209, 5497, 486, 8460, 257, 1090, 2158, 337, 3344, 1783, 11, 558, 30, 865, 13, 663, 311, 437, 321, 3847, 51144, 51144, 341, 2063, 281, 360, 13, 1033, 13, 407, 341, 2063, 3209, 486, 362, 281, 8460, 512, 2416, 2158, 294, 341, 1389, 11, 51476, 51476], "temperature": 0.0, "avg_logprob": -0.20929298871829186, "compression_ratio": 1.6704545454545454, "no_speech_prob": 9.886929910862818e-05}, {"id": 209, "seek": 144712, "start": 1447.12, "end": 1455.12, "text": " 10 if I input a blue guy and we'll have to generate a small zero output if I put a zero,", "tokens": [50364, 1266, 498, 286, 4846, 257, 3344, 2146, 293, 321, 603, 362, 281, 8460, 257, 1359, 4018, 5598, 498, 286, 829, 257, 4018, 11, 50764, 50792, 498, 286, 829, 257, 7022, 4846, 13, 400, 294, 1668, 281, 360, 300, 11, 321, 360, 341, 538, 4464, 2144, 295, 7395, 34, 4470, 13, 1033, 13, 51136, 51184, 639, 307, 700, 644, 13, 407, 1400, 291, 434, 365, 385, 11, 558, 30, 865, 13, 1033, 13, 21320, 13, 823, 321, 362, 264, 1150, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.19623216376247177, "compression_ratio": 1.4183673469387754, "no_speech_prob": 0.00030382967088371515}, {"id": 210, "seek": 144712, "start": 1455.6799999999998, "end": 1462.56, "text": " if I put a pink input. And in order to do that, we do this by minimization of MSC loss. Okay.", "tokens": [50364, 1266, 498, 286, 4846, 257, 3344, 2146, 293, 321, 603, 362, 281, 8460, 257, 1359, 4018, 5598, 498, 286, 829, 257, 4018, 11, 50764, 50792, 498, 286, 829, 257, 7022, 4846, 13, 400, 294, 1668, 281, 360, 300, 11, 321, 360, 341, 538, 4464, 2144, 295, 7395, 34, 4470, 13, 1033, 13, 51136, 51184, 639, 307, 700, 644, 13, 407, 1400, 291, 434, 365, 385, 11, 558, 30, 865, 13, 1033, 13, 21320, 13, 823, 321, 362, 264, 1150, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.19623216376247177, "compression_ratio": 1.4183673469387754, "no_speech_prob": 0.00030382967088371515}, {"id": 211, "seek": 144712, "start": 1463.52, "end": 1471.28, "text": " This is first part. So far you're with me, right? Yeah. Okay. Fantastic. Now we have the second", "tokens": [50364, 1266, 498, 286, 4846, 257, 3344, 2146, 293, 321, 603, 362, 281, 8460, 257, 1359, 4018, 5598, 498, 286, 829, 257, 4018, 11, 50764, 50792, 498, 286, 829, 257, 7022, 4846, 13, 400, 294, 1668, 281, 360, 300, 11, 321, 360, 341, 538, 4464, 2144, 295, 7395, 34, 4470, 13, 1033, 13, 51136, 51184, 639, 307, 700, 644, 13, 407, 1400, 291, 434, 365, 385, 11, 558, 30, 865, 13, 1033, 13, 21320, 13, 823, 321, 362, 264, 1150, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.19623216376247177, "compression_ratio": 1.4183673469387754, "no_speech_prob": 0.00030382967088371515}, {"id": 212, "seek": 147128, "start": 1471.28, "end": 1477.44, "text": " part, which is the cute version, the version that Jan likes, the different version that you", "tokens": [50364, 644, 11, 597, 307, 264, 4052, 3037, 11, 264, 3037, 300, 4956, 5902, 11, 264, 819, 3037, 300, 291, 50672, 50672, 500, 380, 915, 2950, 11, 597, 307, 264, 3480, 13, 407, 341, 2063, 3209, 586, 486, 976, 291, 4190, 300, 51052, 51052, 366, 1998, 281, 4018, 5699, 291, 4846, 746, 300, 1542, 411, 2296, 13, 1033, 13, 10328, 309, 486, 51488, 51488], "temperature": 0.0, "avg_logprob": -0.16791226313664362, "compression_ratio": 1.5909090909090908, "no_speech_prob": 5.0568913138704374e-05}, {"id": 213, "seek": 147128, "start": 1477.44, "end": 1485.04, "text": " don't find online, which is the following. So this cost network now will give you values that", "tokens": [50364, 644, 11, 597, 307, 264, 4052, 3037, 11, 264, 3037, 300, 4956, 5902, 11, 264, 819, 3037, 300, 291, 50672, 50672, 500, 380, 915, 2950, 11, 597, 307, 264, 3480, 13, 407, 341, 2063, 3209, 586, 486, 976, 291, 4190, 300, 51052, 51052, 366, 1998, 281, 4018, 5699, 291, 4846, 746, 300, 1542, 411, 2296, 13, 1033, 13, 10328, 309, 486, 51488, 51488], "temperature": 0.0, "avg_logprob": -0.16791226313664362, "compression_ratio": 1.5909090909090908, "no_speech_prob": 5.0568913138704374e-05}, {"id": 214, "seek": 147128, "start": 1485.04, "end": 1493.76, "text": " are close to zero whenever you input something that looks like proper. Okay. Otherwise it will", "tokens": [50364, 644, 11, 597, 307, 264, 4052, 3037, 11, 264, 3037, 300, 4956, 5902, 11, 264, 819, 3037, 300, 291, 50672, 50672, 500, 380, 915, 2950, 11, 597, 307, 264, 3480, 13, 407, 341, 2063, 3209, 586, 486, 976, 291, 4190, 300, 51052, 51052, 366, 1998, 281, 4018, 5699, 291, 4846, 746, 300, 1542, 411, 2296, 13, 1033, 13, 10328, 309, 486, 51488, 51488], "temperature": 0.0, "avg_logprob": -0.16791226313664362, "compression_ratio": 1.5909090909090908, "no_speech_prob": 5.0568913138704374e-05}, {"id": 215, "seek": 149376, "start": 1493.76, "end": 1504.08, "text": " put a high output, let's say a value around 10, if you put inside crappy input. So now, finally,", "tokens": [50364, 829, 257, 1090, 5598, 11, 718, 311, 584, 257, 2158, 926, 1266, 11, 498, 291, 829, 1854, 36531, 4846, 13, 407, 586, 11, 2721, 11, 50880, 50880, 577, 360, 321, 3847, 341, 19265, 30, 1042, 11, 264, 19265, 586, 486, 312, 8895, 807, 264, 4464, 2144, 51200, 51240, 295, 264, 2063, 3209, 11, 558, 30, 407, 264, 2063, 3209, 486, 584, 1266, 510, 13, 407, 341, 5598, 3344, 2146, 510, 11, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.100978151957194, "compression_ratio": 1.529100529100529, "no_speech_prob": 3.699852095451206e-05}, {"id": 216, "seek": 149376, "start": 1504.08, "end": 1510.48, "text": " how do we train this generator? Well, the generator now will be trained through the minimization", "tokens": [50364, 829, 257, 1090, 5598, 11, 718, 311, 584, 257, 2158, 926, 1266, 11, 498, 291, 829, 1854, 36531, 4846, 13, 407, 586, 11, 2721, 11, 50880, 50880, 577, 360, 321, 3847, 341, 19265, 30, 1042, 11, 264, 19265, 586, 486, 312, 8895, 807, 264, 4464, 2144, 51200, 51240, 295, 264, 2063, 3209, 11, 558, 30, 407, 264, 2063, 3209, 486, 584, 1266, 510, 13, 407, 341, 5598, 3344, 2146, 510, 11, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.100978151957194, "compression_ratio": 1.529100529100529, "no_speech_prob": 3.699852095451206e-05}, {"id": 217, "seek": 149376, "start": 1511.28, "end": 1519.52, "text": " of the cost network, right? So the cost network will say 10 here. So this output blue guy here,", "tokens": [50364, 829, 257, 1090, 5598, 11, 718, 311, 584, 257, 2158, 926, 1266, 11, 498, 291, 829, 1854, 36531, 4846, 13, 407, 586, 11, 2721, 11, 50880, 50880, 577, 360, 321, 3847, 341, 19265, 30, 1042, 11, 264, 19265, 586, 486, 312, 8895, 807, 264, 4464, 2144, 51200, 51240, 295, 264, 2063, 3209, 11, 558, 30, 407, 264, 2063, 3209, 486, 584, 1266, 510, 13, 407, 341, 5598, 3344, 2146, 510, 11, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.100978151957194, "compression_ratio": 1.529100529100529, "no_speech_prob": 3.699852095451206e-05}, {"id": 218, "seek": 151952, "start": 1519.52, "end": 1527.12, "text": " it's bad guy, right? So if the generator now switches slightly this X to make something", "tokens": [50364, 309, 311, 1578, 2146, 11, 558, 30, 407, 498, 264, 19265, 586, 19458, 4748, 341, 1783, 281, 652, 746, 50744, 50744, 300, 1542, 411, 341, 2146, 670, 510, 11, 550, 291, 483, 300, 490, 1266, 11, 321, 1437, 760, 281, 4018, 11, 558, 30, 51068, 51096, 400, 4412, 291, 658, 281, 17522, 341, 2063, 3209, 5598, 2158, 13, 400, 370, 321, 366, 1228, 264, 51460, 51460], "temperature": 0.0, "avg_logprob": -0.09727172290577608, "compression_ratio": 1.478021978021978, "no_speech_prob": 3.2177191314985976e-05}, {"id": 219, "seek": 151952, "start": 1527.12, "end": 1533.6, "text": " that looks like this guy over here, then you get that from 10, we went down to zero, right?", "tokens": [50364, 309, 311, 1578, 2146, 11, 558, 30, 407, 498, 264, 19265, 586, 19458, 4748, 341, 1783, 281, 652, 746, 50744, 50744, 300, 1542, 411, 341, 2146, 670, 510, 11, 550, 291, 483, 300, 490, 1266, 11, 321, 1437, 760, 281, 4018, 11, 558, 30, 51068, 51096, 400, 4412, 291, 658, 281, 17522, 341, 2063, 3209, 5598, 2158, 13, 400, 370, 321, 366, 1228, 264, 51460, 51460], "temperature": 0.0, "avg_logprob": -0.09727172290577608, "compression_ratio": 1.478021978021978, "no_speech_prob": 3.2177191314985976e-05}, {"id": 220, "seek": 151952, "start": 1534.16, "end": 1541.44, "text": " And therefore you got to minimize this cost network output value. And so we are using the", "tokens": [50364, 309, 311, 1578, 2146, 11, 558, 30, 407, 498, 264, 19265, 586, 19458, 4748, 341, 1783, 281, 652, 746, 50744, 50744, 300, 1542, 411, 341, 2146, 670, 510, 11, 550, 291, 483, 300, 490, 1266, 11, 321, 1437, 760, 281, 4018, 11, 558, 30, 51068, 51096, 400, 4412, 291, 658, 281, 17522, 341, 2063, 3209, 5598, 2158, 13, 400, 370, 321, 366, 1228, 264, 51460, 51460], "temperature": 0.0, "avg_logprob": -0.09727172290577608, "compression_ratio": 1.478021978021978, "no_speech_prob": 3.2177191314985976e-05}, {"id": 221, "seek": 154144, "start": 1541.44, "end": 1549.68, "text": " cost network as the loss for training the generator. Okay. What do you mean by like,", "tokens": [50364, 2063, 3209, 382, 264, 4470, 337, 3097, 264, 19265, 13, 1033, 13, 708, 360, 291, 914, 538, 411, 11, 50776, 50816, 1242, 3344, 1783, 4966, 281, 7022, 1783, 30, 1779, 13, 407, 558, 586, 452, 19265, 23930, 341, 51160, 51200, 3344, 11, 3344, 1783, 13, 1033, 13, 400, 341, 307, 411, 512, 3256, 300, 1542, 1578, 420, 309, 311, 11, 291, 458, 11, 51488, 51524], "temperature": 0.0, "avg_logprob": -0.14457917569288567, "compression_ratio": 1.4411764705882353, "no_speech_prob": 1.7773319996194914e-05}, {"id": 222, "seek": 154144, "start": 1550.48, "end": 1557.3600000000001, "text": " getting blue X closer to pink X? Right. So right now my generator outputs this", "tokens": [50364, 2063, 3209, 382, 264, 4470, 337, 3097, 264, 19265, 13, 1033, 13, 708, 360, 291, 914, 538, 411, 11, 50776, 50816, 1242, 3344, 1783, 4966, 281, 7022, 1783, 30, 1779, 13, 407, 558, 586, 452, 19265, 23930, 341, 51160, 51200, 3344, 11, 3344, 1783, 13, 1033, 13, 400, 341, 307, 411, 512, 3256, 300, 1542, 1578, 420, 309, 311, 11, 291, 458, 11, 51488, 51524], "temperature": 0.0, "avg_logprob": -0.14457917569288567, "compression_ratio": 1.4411764705882353, "no_speech_prob": 1.7773319996194914e-05}, {"id": 223, "seek": 154144, "start": 1558.16, "end": 1563.92, "text": " blue, blue X. Okay. And this is like some image that looks bad or it's, you know,", "tokens": [50364, 2063, 3209, 382, 264, 4470, 337, 3097, 264, 19265, 13, 1033, 13, 708, 360, 291, 914, 538, 411, 11, 50776, 50816, 1242, 3344, 1783, 4966, 281, 7022, 1783, 30, 1779, 13, 407, 558, 586, 452, 19265, 23930, 341, 51160, 51200, 3344, 11, 3344, 1783, 13, 1033, 13, 400, 341, 307, 411, 512, 3256, 300, 1542, 1578, 420, 309, 311, 11, 291, 458, 11, 51488, 51524], "temperature": 0.0, "avg_logprob": -0.14457917569288567, "compression_ratio": 1.4411764705882353, "no_speech_prob": 1.7773319996194914e-05}, {"id": 224, "seek": 156392, "start": 1563.92, "end": 1573.04, "text": " money that really looks fake. Now, how do you make better money? Well, the cost network is going to", "tokens": [50364, 1460, 300, 534, 1542, 7592, 13, 823, 11, 577, 360, 291, 652, 1101, 1460, 30, 1042, 11, 264, 2063, 3209, 307, 516, 281, 50820, 50820, 976, 291, 257, 39684, 2158, 337, 1184, 5598, 428, 19265, 1669, 13, 7504, 291, 393, 14722, 264, 51240, 51240, 14641, 13760, 13, 509, 393, 14722, 264, 16235, 11, 291, 458, 11, 295, 300, 2063, 2158, 13, 286, 411, 281, 14722, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.13259392890377322, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.442441589460941e-05}, {"id": 225, "seek": 156392, "start": 1573.04, "end": 1581.44, "text": " give you a scalar value for each output your generator makes. Therefore you can compute the", "tokens": [50364, 1460, 300, 534, 1542, 7592, 13, 823, 11, 577, 360, 291, 652, 1101, 1460, 30, 1042, 11, 264, 2063, 3209, 307, 516, 281, 50820, 50820, 976, 291, 257, 39684, 2158, 337, 1184, 5598, 428, 19265, 1669, 13, 7504, 291, 393, 14722, 264, 51240, 51240, 14641, 13760, 13, 509, 393, 14722, 264, 16235, 11, 291, 458, 11, 295, 300, 2063, 2158, 13, 286, 411, 281, 14722, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.13259392890377322, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.442441589460941e-05}, {"id": 226, "seek": 156392, "start": 1581.44, "end": 1587.68, "text": " partial derivative. You can compute the gradient, you know, of that cost value. I like to compute", "tokens": [50364, 1460, 300, 534, 1542, 7592, 13, 823, 11, 577, 360, 291, 652, 1101, 1460, 30, 1042, 11, 264, 2063, 3209, 307, 516, 281, 50820, 50820, 976, 291, 257, 39684, 2158, 337, 1184, 5598, 428, 19265, 1669, 13, 7504, 291, 393, 14722, 264, 51240, 51240, 14641, 13760, 13, 509, 393, 14722, 264, 16235, 11, 291, 458, 11, 295, 300, 2063, 2158, 13, 286, 411, 281, 14722, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.13259392890377322, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.442441589460941e-05}, {"id": 227, "seek": 158768, "start": 1587.68, "end": 1603.8400000000001, "text": " the partial derivative of these lowercase c. So dc over dx hat, right? So here I have the partial", "tokens": [50364, 264, 14641, 13760, 295, 613, 3126, 9765, 269, 13, 407, 274, 66, 670, 30017, 2385, 11, 558, 30, 407, 510, 286, 362, 264, 14641, 51172, 51172, 293, 341, 307, 11232, 3579, 13, 4919, 13, 1057, 558, 13, 286, 393, 380, 2464, 13, 1033, 13, 462, 11, 572, 11, 300, 390, 383, 13, 876, 452, 1265, 13, 51536, 51608], "temperature": 0.0, "avg_logprob": -0.3042462348937988, "compression_ratio": 1.3310344827586207, "no_speech_prob": 0.0002766464604064822}, {"id": 228, "seek": 158768, "start": 1603.8400000000001, "end": 1611.1200000000001, "text": " and this is awful writing. Sorry. All right. I can't write. Okay. E, no, that was C. Oh my God.", "tokens": [50364, 264, 14641, 13760, 295, 613, 3126, 9765, 269, 13, 407, 274, 66, 670, 30017, 2385, 11, 558, 30, 407, 510, 286, 362, 264, 14641, 51172, 51172, 293, 341, 307, 11232, 3579, 13, 4919, 13, 1057, 558, 13, 286, 393, 380, 2464, 13, 1033, 13, 462, 11, 572, 11, 300, 390, 383, 13, 876, 452, 1265, 13, 51536, 51608], "temperature": 0.0, "avg_logprob": -0.3042462348937988, "compression_ratio": 1.3310344827586207, "no_speech_prob": 0.0002766464604064822}, {"id": 229, "seek": 161112, "start": 1611.12, "end": 1619.9199999999998, "text": " Okay. This was a lowercase c. So it's like that. All right. Cool. So I compute the partial derivative", "tokens": [50364, 1033, 13, 639, 390, 257, 3126, 9765, 269, 13, 407, 309, 311, 411, 300, 13, 1057, 558, 13, 8561, 13, 407, 286, 14722, 264, 14641, 13760, 50804, 50804, 295, 452, 3126, 9765, 269, 365, 3104, 281, 264, 1783, 2385, 13, 1779, 13, 407, 586, 286, 362, 257, 16235, 13, 639, 16235, 4045, 385, 51172, 51232, 281, 1286, 926, 293, 286, 2573, 484, 1968, 264, 2063, 307, 516, 281, 3488, 420, 11514, 13, 1779, 13, 407, 341, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.16835167407989501, "compression_ratio": 1.5677083333333333, "no_speech_prob": 1.4967930837883614e-05}, {"id": 230, "seek": 161112, "start": 1619.9199999999998, "end": 1627.28, "text": " of my lowercase c with respect to the X hat. Right. So now I have a gradient. This gradient allows me", "tokens": [50364, 1033, 13, 639, 390, 257, 3126, 9765, 269, 13, 407, 309, 311, 411, 300, 13, 1057, 558, 13, 8561, 13, 407, 286, 14722, 264, 14641, 13760, 50804, 50804, 295, 452, 3126, 9765, 269, 365, 3104, 281, 264, 1783, 2385, 13, 1779, 13, 407, 586, 286, 362, 257, 16235, 13, 639, 16235, 4045, 385, 51172, 51232, 281, 1286, 926, 293, 286, 2573, 484, 1968, 264, 2063, 307, 516, 281, 3488, 420, 11514, 13, 1779, 13, 407, 341, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.16835167407989501, "compression_ratio": 1.5677083333333333, "no_speech_prob": 1.4967930837883614e-05}, {"id": 231, "seek": 161112, "start": 1628.4799999999998, "end": 1638.32, "text": " to move around and I figure out whether the cost is going to increase or decrease. Right. So this", "tokens": [50364, 1033, 13, 639, 390, 257, 3126, 9765, 269, 13, 407, 309, 311, 411, 300, 13, 1057, 558, 13, 8561, 13, 407, 286, 14722, 264, 14641, 13760, 50804, 50804, 295, 452, 3126, 9765, 269, 365, 3104, 281, 264, 1783, 2385, 13, 1779, 13, 407, 586, 286, 362, 257, 16235, 13, 639, 16235, 4045, 385, 51172, 51232, 281, 1286, 926, 293, 286, 2573, 484, 1968, 264, 2063, 307, 516, 281, 3488, 420, 11514, 13, 1779, 13, 407, 341, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.16835167407989501, "compression_ratio": 1.5677083333333333, "no_speech_prob": 1.4967930837883614e-05}, {"id": 232, "seek": 163832, "start": 1638.32, "end": 1646.8799999999999, "text": " is kind of maybe, you know, a little bit not standard as in also yesterday Jan was talking", "tokens": [50364, 307, 733, 295, 1310, 11, 291, 458, 11, 257, 707, 857, 406, 3832, 382, 294, 611, 5186, 4956, 390, 1417, 50792, 50792, 466, 341, 13, 509, 458, 11, 291, 362, 512, 15743, 294, 281, 428, 3209, 13, 509, 393, 4536, 281, 360, 16235, 23475, 51092, 51128, 294, 264, 4846, 1901, 13, 286, 393, 4536, 11, 337, 1365, 11, 456, 307, 257, 9482, 597, 1177, 380, 362, 257, 51428, 51428, 19265, 412, 439, 13, 509, 722, 365, 257, 6889, 510, 293, 550, 291, 2042, 16235, 23475, 294, 341, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.09795588189428979, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.748908107401803e-05}, {"id": 233, "seek": 163832, "start": 1646.8799999999999, "end": 1652.8799999999999, "text": " about this. You know, you have some inputs in to your network. You can decide to do gradient descent", "tokens": [50364, 307, 733, 295, 1310, 11, 291, 458, 11, 257, 707, 857, 406, 3832, 382, 294, 611, 5186, 4956, 390, 1417, 50792, 50792, 466, 341, 13, 509, 458, 11, 291, 362, 512, 15743, 294, 281, 428, 3209, 13, 509, 393, 4536, 281, 360, 16235, 23475, 51092, 51128, 294, 264, 4846, 1901, 13, 286, 393, 4536, 11, 337, 1365, 11, 456, 307, 257, 9482, 597, 1177, 380, 362, 257, 51428, 51428, 19265, 412, 439, 13, 509, 722, 365, 257, 6889, 510, 293, 550, 291, 2042, 16235, 23475, 294, 341, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.09795588189428979, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.748908107401803e-05}, {"id": 234, "seek": 163832, "start": 1653.6, "end": 1659.6, "text": " in the input space. I can decide, for example, there is a architecture which doesn't have a", "tokens": [50364, 307, 733, 295, 1310, 11, 291, 458, 11, 257, 707, 857, 406, 3832, 382, 294, 611, 5186, 4956, 390, 1417, 50792, 50792, 466, 341, 13, 509, 458, 11, 291, 362, 512, 15743, 294, 281, 428, 3209, 13, 509, 393, 4536, 281, 360, 16235, 23475, 51092, 51128, 294, 264, 4846, 1901, 13, 286, 393, 4536, 11, 337, 1365, 11, 456, 307, 257, 9482, 597, 1177, 380, 362, 257, 51428, 51428, 19265, 412, 439, 13, 509, 722, 365, 257, 6889, 510, 293, 550, 291, 2042, 16235, 23475, 294, 341, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.09795588189428979, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.748908107401803e-05}, {"id": 235, "seek": 163832, "start": 1659.6, "end": 1665.4399999999998, "text": " generator at all. You start with a sample here and then you perform gradient descent in this", "tokens": [50364, 307, 733, 295, 1310, 11, 291, 458, 11, 257, 707, 857, 406, 3832, 382, 294, 611, 5186, 4956, 390, 1417, 50792, 50792, 466, 341, 13, 509, 458, 11, 291, 362, 512, 15743, 294, 281, 428, 3209, 13, 509, 393, 4536, 281, 360, 16235, 23475, 51092, 51128, 294, 264, 4846, 1901, 13, 286, 393, 4536, 11, 337, 1365, 11, 456, 307, 257, 9482, 597, 1177, 380, 362, 257, 51428, 51428, 19265, 412, 439, 13, 509, 722, 365, 257, 6889, 510, 293, 550, 291, 2042, 16235, 23475, 294, 341, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.09795588189428979, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.748908107401803e-05}, {"id": 236, "seek": 166544, "start": 1665.44, "end": 1671.1200000000001, "text": " sample space. And then you move the samples such that you get a lower, lower value for the cost", "tokens": [50364, 6889, 1901, 13, 400, 550, 291, 1286, 264, 10938, 1270, 300, 291, 483, 257, 3126, 11, 3126, 2158, 337, 264, 2063, 50648, 50648, 3209, 13, 400, 341, 636, 291, 393, 11, 291, 458, 11, 483, 364, 4846, 300, 1542, 411, 20695, 1688, 257, 665, 4846, 11, 51104, 51104, 558, 30, 440, 7022, 472, 13, 4402, 309, 652, 300, 11, 300, 286, 733, 295, 2903, 2059, 420, 307, 309, 920, 3657, 30, 51440, 51484, 876, 11, 309, 311, 709, 26131, 13, 1044, 291, 13, 509, 988, 30, 865, 13, 865, 13, 467, 311, 411, 1940, 2771, 2448, 294, 264, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.13666631661209405, "compression_ratio": 1.5564853556485356, "no_speech_prob": 2.4670582206454128e-05}, {"id": 237, "seek": 166544, "start": 1671.1200000000001, "end": 1680.24, "text": " network. And this way you can, you know, get an input that looks like resembling a good input,", "tokens": [50364, 6889, 1901, 13, 400, 550, 291, 1286, 264, 10938, 1270, 300, 291, 483, 257, 3126, 11, 3126, 2158, 337, 264, 2063, 50648, 50648, 3209, 13, 400, 341, 636, 291, 393, 11, 291, 458, 11, 483, 364, 4846, 300, 1542, 411, 20695, 1688, 257, 665, 4846, 11, 51104, 51104, 558, 30, 440, 7022, 472, 13, 4402, 309, 652, 300, 11, 300, 286, 733, 295, 2903, 2059, 420, 307, 309, 920, 3657, 30, 51440, 51484, 876, 11, 309, 311, 709, 26131, 13, 1044, 291, 13, 509, 988, 30, 865, 13, 865, 13, 467, 311, 411, 1940, 2771, 2448, 294, 264, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.13666631661209405, "compression_ratio": 1.5564853556485356, "no_speech_prob": 2.4670582206454128e-05}, {"id": 238, "seek": 166544, "start": 1680.24, "end": 1686.96, "text": " right? The pink one. Does it make that, that I kind of explain myself or is it still weird?", "tokens": [50364, 6889, 1901, 13, 400, 550, 291, 1286, 264, 10938, 1270, 300, 291, 483, 257, 3126, 11, 3126, 2158, 337, 264, 2063, 50648, 50648, 3209, 13, 400, 341, 636, 291, 393, 11, 291, 458, 11, 483, 364, 4846, 300, 1542, 411, 20695, 1688, 257, 665, 4846, 11, 51104, 51104, 558, 30, 440, 7022, 472, 13, 4402, 309, 652, 300, 11, 300, 286, 733, 295, 2903, 2059, 420, 307, 309, 920, 3657, 30, 51440, 51484, 876, 11, 309, 311, 709, 26131, 13, 1044, 291, 13, 509, 988, 30, 865, 13, 865, 13, 467, 311, 411, 1940, 2771, 2448, 294, 264, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.13666631661209405, "compression_ratio": 1.5564853556485356, "no_speech_prob": 2.4670582206454128e-05}, {"id": 239, "seek": 166544, "start": 1687.8400000000001, "end": 1693.76, "text": " Oh, it's much clearer. Thank you. You sure? Yeah. Yeah. It's like taking gradients in the", "tokens": [50364, 6889, 1901, 13, 400, 550, 291, 1286, 264, 10938, 1270, 300, 291, 483, 257, 3126, 11, 3126, 2158, 337, 264, 2063, 50648, 50648, 3209, 13, 400, 341, 636, 291, 393, 11, 291, 458, 11, 483, 364, 4846, 300, 1542, 411, 20695, 1688, 257, 665, 4846, 11, 51104, 51104, 558, 30, 440, 7022, 472, 13, 4402, 309, 652, 300, 11, 300, 286, 733, 295, 2903, 2059, 420, 307, 309, 920, 3657, 30, 51440, 51484, 876, 11, 309, 311, 709, 26131, 13, 1044, 291, 13, 509, 988, 30, 865, 13, 865, 13, 467, 311, 411, 1940, 2771, 2448, 294, 264, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.13666631661209405, "compression_ratio": 1.5564853556485356, "no_speech_prob": 2.4670582206454128e-05}, {"id": 240, "seek": 169376, "start": 1693.76, "end": 1700.72, "text": " input space and make it move towards like, and then decrease the cost. So that means the input", "tokens": [50364, 4846, 1901, 293, 652, 309, 1286, 3030, 411, 11, 293, 550, 11514, 264, 2063, 13, 407, 300, 1355, 264, 4846, 50712, 50712, 767, 2170, 1101, 11, 411, 2170, 1101, 1460, 420, 1101, 3256, 13, 1779, 13, 1779, 13, 1779, 13, 400, 550, 291, 51032, 51032, 393, 611, 764, 341, 472, 382, 428, 16235, 510, 1348, 760, 510, 13, 1779, 13, 400, 370, 586, 291, 393, 14722, 365, 51368, 51368, 257, 5021, 4978, 611, 322, 264, 14641, 13760, 295, 341, 3126, 9765, 269, 365, 3104, 281, 264, 9834, 51780, 51812], "temperature": 0.0, "avg_logprob": -0.13415968549120558, "compression_ratio": 1.7625570776255708, "no_speech_prob": 3.822399230557494e-05}, {"id": 241, "seek": 169376, "start": 1700.72, "end": 1707.12, "text": " actually gets better, like gets better money or better image. Right. Right. Right. And then you", "tokens": [50364, 4846, 1901, 293, 652, 309, 1286, 3030, 411, 11, 293, 550, 11514, 264, 2063, 13, 407, 300, 1355, 264, 4846, 50712, 50712, 767, 2170, 1101, 11, 411, 2170, 1101, 1460, 420, 1101, 3256, 13, 1779, 13, 1779, 13, 1779, 13, 400, 550, 291, 51032, 51032, 393, 611, 764, 341, 472, 382, 428, 16235, 510, 1348, 760, 510, 13, 1779, 13, 400, 370, 586, 291, 393, 14722, 365, 51368, 51368, 257, 5021, 4978, 611, 322, 264, 14641, 13760, 295, 341, 3126, 9765, 269, 365, 3104, 281, 264, 9834, 51780, 51812], "temperature": 0.0, "avg_logprob": -0.13415968549120558, "compression_ratio": 1.7625570776255708, "no_speech_prob": 3.822399230557494e-05}, {"id": 242, "seek": 169376, "start": 1707.12, "end": 1713.84, "text": " can also use this one as your gradient here coming down here. Right. And so now you can compute with", "tokens": [50364, 4846, 1901, 293, 652, 309, 1286, 3030, 411, 11, 293, 550, 11514, 264, 2063, 13, 407, 300, 1355, 264, 4846, 50712, 50712, 767, 2170, 1101, 11, 411, 2170, 1101, 1460, 420, 1101, 3256, 13, 1779, 13, 1779, 13, 1779, 13, 400, 550, 291, 51032, 51032, 393, 611, 764, 341, 472, 382, 428, 16235, 510, 1348, 760, 510, 13, 1779, 13, 400, 370, 586, 291, 393, 14722, 365, 51368, 51368, 257, 5021, 4978, 611, 322, 264, 14641, 13760, 295, 341, 3126, 9765, 269, 365, 3104, 281, 264, 9834, 51780, 51812], "temperature": 0.0, "avg_logprob": -0.13415968549120558, "compression_ratio": 1.7625570776255708, "no_speech_prob": 3.822399230557494e-05}, {"id": 243, "seek": 169376, "start": 1713.84, "end": 1722.08, "text": " a chain rule also on the partial derivative of this lowercase c with respect to the parameters", "tokens": [50364, 4846, 1901, 293, 652, 309, 1286, 3030, 411, 11, 293, 550, 11514, 264, 2063, 13, 407, 300, 1355, 264, 4846, 50712, 50712, 767, 2170, 1101, 11, 411, 2170, 1101, 1460, 420, 1101, 3256, 13, 1779, 13, 1779, 13, 1779, 13, 400, 550, 291, 51032, 51032, 393, 611, 764, 341, 472, 382, 428, 16235, 510, 1348, 760, 510, 13, 1779, 13, 400, 370, 586, 291, 393, 14722, 365, 51368, 51368, 257, 5021, 4978, 611, 322, 264, 14641, 13760, 295, 341, 3126, 9765, 269, 365, 3104, 281, 264, 9834, 51780, 51812], "temperature": 0.0, "avg_logprob": -0.13415968549120558, "compression_ratio": 1.7625570776255708, "no_speech_prob": 3.822399230557494e-05}, {"id": 244, "seek": 172208, "start": 1722.08, "end": 1730.96, "text": " w of the generator. Okay. So in this case, then I can train the generator, right? I had the partial", "tokens": [50364, 261, 295, 264, 19265, 13, 1033, 13, 407, 294, 341, 1389, 11, 550, 286, 393, 3847, 264, 19265, 11, 558, 30, 286, 632, 264, 14641, 50808, 50808, 295, 264, 2063, 670, 264, 9834, 293, 4412, 286, 393, 1319, 586, 264, 4190, 295, 264, 9834, 322, 264, 51236, 51236, 19265, 294, 1668, 281, 11, 291, 458, 11, 3470, 264, 3209, 13, 1033, 13, 5803, 309, 13, 467, 3879, 1669, 2020, 13, 51556, 51556], "temperature": 0.0, "avg_logprob": -0.1866460877495843, "compression_ratio": 1.676300578034682, "no_speech_prob": 8.219398296205327e-05}, {"id": 245, "seek": 172208, "start": 1730.96, "end": 1739.52, "text": " of the cost over the parameters and therefore I can change now the values of the parameters on the", "tokens": [50364, 261, 295, 264, 19265, 13, 1033, 13, 407, 294, 341, 1389, 11, 550, 286, 393, 3847, 264, 19265, 11, 558, 30, 286, 632, 264, 14641, 50808, 50808, 295, 264, 2063, 670, 264, 9834, 293, 4412, 286, 393, 1319, 586, 264, 4190, 295, 264, 9834, 322, 264, 51236, 51236, 19265, 294, 1668, 281, 11, 291, 458, 11, 3470, 264, 3209, 13, 1033, 13, 5803, 309, 13, 467, 3879, 1669, 2020, 13, 51556, 51556], "temperature": 0.0, "avg_logprob": -0.1866460877495843, "compression_ratio": 1.676300578034682, "no_speech_prob": 8.219398296205327e-05}, {"id": 246, "seek": 172208, "start": 1739.52, "end": 1745.9199999999998, "text": " generator in order to, you know, improve the network. Okay. Got it. It totally makes sense.", "tokens": [50364, 261, 295, 264, 19265, 13, 1033, 13, 407, 294, 341, 1389, 11, 550, 286, 393, 3847, 264, 19265, 11, 558, 30, 286, 632, 264, 14641, 50808, 50808, 295, 264, 2063, 670, 264, 9834, 293, 4412, 286, 393, 1319, 586, 264, 4190, 295, 264, 9834, 322, 264, 51236, 51236, 19265, 294, 1668, 281, 11, 291, 458, 11, 3470, 264, 3209, 13, 1033, 13, 5803, 309, 13, 467, 3879, 1669, 2020, 13, 51556, 51556], "temperature": 0.0, "avg_logprob": -0.1866460877495843, "compression_ratio": 1.676300578034682, "no_speech_prob": 8.219398296205327e-05}, {"id": 247, "seek": 174592, "start": 1745.92, "end": 1753.68, "text": " Thank you. Of course. Yeah. Are they trained simultaneously or are they trained first?", "tokens": [50364, 1044, 291, 13, 2720, 1164, 13, 865, 13, 2014, 436, 8895, 16561, 420, 366, 436, 8895, 700, 30, 50752, 50808, 440, 2063, 3209, 420, 264, 19265, 3209, 30, 1779, 13, 3432, 853, 1293, 13, 814, 584, 309, 311, 1101, 2171, 281, 51256, 51256, 1066, 472, 6806, 1339, 291, 434, 4473, 264, 661, 570, 5911, 291, 603, 362, 1009, 257, 2684, 51516, 51516, 3779, 13, 1396, 456, 366, 49555, 4467, 13, 492, 366, 767, 516, 281, 312, 3760, 586, 512, 4009, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.19141347067696707, "compression_ratio": 1.5661157024793388, "no_speech_prob": 2.6268498913850635e-05}, {"id": 248, "seek": 174592, "start": 1754.8000000000002, "end": 1763.76, "text": " The cost network or the generator network? Right. People try both. They say it's better sometimes to", "tokens": [50364, 1044, 291, 13, 2720, 1164, 13, 865, 13, 2014, 436, 8895, 16561, 420, 366, 436, 8895, 700, 30, 50752, 50808, 440, 2063, 3209, 420, 264, 19265, 3209, 30, 1779, 13, 3432, 853, 1293, 13, 814, 584, 309, 311, 1101, 2171, 281, 51256, 51256, 1066, 472, 6806, 1339, 291, 434, 4473, 264, 661, 570, 5911, 291, 603, 362, 1009, 257, 2684, 51516, 51516, 3779, 13, 1396, 456, 366, 49555, 4467, 13, 492, 366, 767, 516, 281, 312, 3760, 586, 512, 4009, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.19141347067696707, "compression_ratio": 1.5661157024793388, "no_speech_prob": 2.6268498913850635e-05}, {"id": 249, "seek": 174592, "start": 1763.76, "end": 1768.96, "text": " keep one fixed while you're changing the other because otherwise you'll have always a moving", "tokens": [50364, 1044, 291, 13, 2720, 1164, 13, 865, 13, 2014, 436, 8895, 16561, 420, 366, 436, 8895, 700, 30, 50752, 50808, 440, 2063, 3209, 420, 264, 19265, 3209, 30, 1779, 13, 3432, 853, 1293, 13, 814, 584, 309, 311, 1101, 2171, 281, 51256, 51256, 1066, 472, 6806, 1339, 291, 434, 4473, 264, 661, 570, 5911, 291, 603, 362, 1009, 257, 2684, 51516, 51516, 3779, 13, 1396, 456, 366, 49555, 4467, 13, 492, 366, 767, 516, 281, 312, 3760, 586, 512, 4009, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.19141347067696707, "compression_ratio": 1.5661157024793388, "no_speech_prob": 2.6268498913850635e-05}, {"id": 250, "seek": 174592, "start": 1768.96, "end": 1775.52, "text": " target. Then there are contradictory evidence. We are actually going to be reading now some source", "tokens": [50364, 1044, 291, 13, 2720, 1164, 13, 865, 13, 2014, 436, 8895, 16561, 420, 366, 436, 8895, 700, 30, 50752, 50808, 440, 2063, 3209, 420, 264, 19265, 3209, 30, 1779, 13, 3432, 853, 1293, 13, 814, 584, 309, 311, 1101, 2171, 281, 51256, 51256, 1066, 472, 6806, 1339, 291, 434, 4473, 264, 661, 570, 5911, 291, 603, 362, 1009, 257, 2684, 51516, 51516, 3779, 13, 1396, 456, 366, 49555, 4467, 13, 492, 366, 767, 516, 281, 312, 3760, 586, 512, 4009, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.19141347067696707, "compression_ratio": 1.5661157024793388, "no_speech_prob": 2.6268498913850635e-05}, {"id": 251, "seek": 177552, "start": 1775.52, "end": 1781.92, "text": " code after we cover the major pitfalls, but I'm going to get back to your question in a few minutes.", "tokens": [50364, 3089, 934, 321, 2060, 264, 2563, 10147, 18542, 11, 457, 286, 478, 516, 281, 483, 646, 281, 428, 1168, 294, 257, 1326, 2077, 13, 50684, 50720, 492, 500, 380, 643, 257, 3890, 2144, 411, 47991, 35, 337, 1176, 292, 3584, 3874, 570, 321, 6889, 490, 2710, 11, 51092, 51132, 1338, 11, 2047, 21120, 11, 2047, 11, 1338, 11, 3838, 13, 509, 6889, 264, 7671, 2146, 510, 490, 51560, 51624], "temperature": 0.0, "avg_logprob": -0.28120212823572294, "compression_ratio": 1.4648648648648648, "no_speech_prob": 9.16213757591322e-05}, {"id": 252, "seek": 177552, "start": 1782.6399999999999, "end": 1790.08, "text": " We don't need a regularization like KLD for Zedding gun because we sample from normal,", "tokens": [50364, 3089, 934, 321, 2060, 264, 2563, 10147, 18542, 11, 457, 286, 478, 516, 281, 483, 646, 281, 428, 1168, 294, 257, 1326, 2077, 13, 50684, 50720, 492, 500, 380, 643, 257, 3890, 2144, 411, 47991, 35, 337, 1176, 292, 3584, 3874, 570, 321, 6889, 490, 2710, 11, 51092, 51132, 1338, 11, 2047, 21120, 11, 2047, 11, 1338, 11, 3838, 13, 509, 6889, 264, 7671, 2146, 510, 490, 51560, 51624], "temperature": 0.0, "avg_logprob": -0.28120212823572294, "compression_ratio": 1.4648648648648648, "no_speech_prob": 9.16213757591322e-05}, {"id": 253, "seek": 177552, "start": 1790.8799999999999, "end": 1799.44, "text": " yeah, direct directory, direct, yeah, directly. You sample the orange guy here from", "tokens": [50364, 3089, 934, 321, 2060, 264, 2563, 10147, 18542, 11, 457, 286, 478, 516, 281, 483, 646, 281, 428, 1168, 294, 257, 1326, 2077, 13, 50684, 50720, 492, 500, 380, 643, 257, 3890, 2144, 411, 47991, 35, 337, 1176, 292, 3584, 3874, 570, 321, 6889, 490, 2710, 11, 51092, 51132, 1338, 11, 2047, 21120, 11, 2047, 11, 1338, 11, 3838, 13, 509, 6889, 264, 7671, 2146, 510, 490, 51560, 51624], "temperature": 0.0, "avg_logprob": -0.28120212823572294, "compression_ratio": 1.4648648648648648, "no_speech_prob": 9.16213757591322e-05}, {"id": 254, "seek": 179944, "start": 1799.44, "end": 1804.72, "text": " a normal distribution. So that's it, right? You have a sample, like a random number,", "tokens": [50364, 257, 2710, 7316, 13, 407, 300, 311, 309, 11, 558, 30, 509, 362, 257, 6889, 11, 411, 257, 4974, 1230, 11, 50628, 50656, 293, 550, 291, 2845, 613, 4974, 3547, 807, 264, 19265, 13, 663, 311, 309, 13, 400, 452, 3329, 1280, 445, 50896, 50896, 1361, 646, 281, 993, 13, 1033, 13, 286, 10103, 428, 1168, 13, 286, 519, 544, 1651, 13, 1396, 321, 362, 10147, 18542, 293, 51260, 51260, 550, 321, 767, 366, 516, 281, 312, 1237, 412, 4009, 3089, 13, 865, 13, 407, 309, 2544, 411, 321, 366, 19139, 264, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.2252136468887329, "compression_ratio": 1.5892116182572613, "no_speech_prob": 1.593400702404324e-05}, {"id": 255, "seek": 179944, "start": 1805.28, "end": 1810.0800000000002, "text": " and then you send these random numbers through the generator. That's it. And my Google home just", "tokens": [50364, 257, 2710, 7316, 13, 407, 300, 311, 309, 11, 558, 30, 509, 362, 257, 6889, 11, 411, 257, 4974, 1230, 11, 50628, 50656, 293, 550, 291, 2845, 613, 4974, 3547, 807, 264, 19265, 13, 663, 311, 309, 13, 400, 452, 3329, 1280, 445, 50896, 50896, 1361, 646, 281, 993, 13, 1033, 13, 286, 10103, 428, 1168, 13, 286, 519, 544, 1651, 13, 1396, 321, 362, 10147, 18542, 293, 51260, 51260, 550, 321, 767, 366, 516, 281, 312, 1237, 412, 4009, 3089, 13, 865, 13, 407, 309, 2544, 411, 321, 366, 19139, 264, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.2252136468887329, "compression_ratio": 1.5892116182572613, "no_speech_prob": 1.593400702404324e-05}, {"id": 256, "seek": 179944, "start": 1810.0800000000002, "end": 1817.3600000000001, "text": " came back to life. Okay. I answered your question. I think more questions. Then we have pitfalls and", "tokens": [50364, 257, 2710, 7316, 13, 407, 300, 311, 309, 11, 558, 30, 509, 362, 257, 6889, 11, 411, 257, 4974, 1230, 11, 50628, 50656, 293, 550, 291, 2845, 613, 4974, 3547, 807, 264, 19265, 13, 663, 311, 309, 13, 400, 452, 3329, 1280, 445, 50896, 50896, 1361, 646, 281, 993, 13, 1033, 13, 286, 10103, 428, 1168, 13, 286, 519, 544, 1651, 13, 1396, 321, 362, 10147, 18542, 293, 51260, 51260, 550, 321, 767, 366, 516, 281, 312, 1237, 412, 4009, 3089, 13, 865, 13, 407, 309, 2544, 411, 321, 366, 19139, 264, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.2252136468887329, "compression_ratio": 1.5892116182572613, "no_speech_prob": 1.593400702404324e-05}, {"id": 257, "seek": 179944, "start": 1817.3600000000001, "end": 1822.96, "text": " then we actually are going to be looking at source code. Yeah. So it seems like we are replacing the", "tokens": [50364, 257, 2710, 7316, 13, 407, 300, 311, 309, 11, 558, 30, 509, 362, 257, 6889, 11, 411, 257, 4974, 1230, 11, 50628, 50656, 293, 550, 291, 2845, 613, 4974, 3547, 807, 264, 19265, 13, 663, 311, 309, 13, 400, 452, 3329, 1280, 445, 50896, 50896, 1361, 646, 281, 993, 13, 1033, 13, 286, 10103, 428, 1168, 13, 286, 519, 544, 1651, 13, 1396, 321, 362, 10147, 18542, 293, 51260, 51260, 550, 321, 767, 366, 516, 281, 312, 1237, 412, 4009, 3089, 13, 865, 13, 407, 309, 2544, 411, 321, 366, 19139, 264, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.2252136468887329, "compression_ratio": 1.5892116182572613, "no_speech_prob": 1.593400702404324e-05}, {"id": 258, "seek": 182296, "start": 1822.96, "end": 1832.96, "text": " reconstruction loss with the differentiator network. How does that help exactly? Why can't, like,", "tokens": [50364, 31565, 4470, 365, 264, 27372, 1639, 3209, 13, 1012, 775, 300, 854, 2293, 30, 1545, 393, 380, 11, 411, 11, 50864, 50864, 577, 307, 309, 1578, 281, 445, 764, 264, 31565, 4470, 30, 1033, 13, 1033, 13, 1033, 13, 1033, 13, 1033, 13, 639, 307, 11, 51144, 51144, 341, 307, 257, 588, 11, 588, 665, 1168, 13, 286, 914, 11, 286, 11, 309, 311, 746, 286, 5298, 2584, 281, 584, 13, 51368, 51748], "temperature": 0.0, "avg_logprob": -0.281155268351237, "compression_ratio": 1.547486033519553, "no_speech_prob": 1.0615437531669158e-05}, {"id": 259, "seek": 182296, "start": 1832.96, "end": 1838.56, "text": " how is it bad to just use the reconstruction loss? Okay. Okay. Okay. Okay. Okay. This is,", "tokens": [50364, 31565, 4470, 365, 264, 27372, 1639, 3209, 13, 1012, 775, 300, 854, 2293, 30, 1545, 393, 380, 11, 411, 11, 50864, 50864, 577, 307, 309, 1578, 281, 445, 764, 264, 31565, 4470, 30, 1033, 13, 1033, 13, 1033, 13, 1033, 13, 1033, 13, 639, 307, 11, 51144, 51144, 341, 307, 257, 588, 11, 588, 665, 1168, 13, 286, 914, 11, 286, 11, 309, 311, 746, 286, 5298, 2584, 281, 584, 13, 51368, 51748], "temperature": 0.0, "avg_logprob": -0.281155268351237, "compression_ratio": 1.547486033519553, "no_speech_prob": 1.0615437531669158e-05}, {"id": 260, "seek": 182296, "start": 1838.56, "end": 1843.04, "text": " this is a very, very good question. I mean, I, it's something I forgot completely to say.", "tokens": [50364, 31565, 4470, 365, 264, 27372, 1639, 3209, 13, 1012, 775, 300, 854, 2293, 30, 1545, 393, 380, 11, 411, 11, 50864, 50864, 577, 307, 309, 1578, 281, 445, 764, 264, 31565, 4470, 30, 1033, 13, 1033, 13, 1033, 13, 1033, 13, 1033, 13, 639, 307, 11, 51144, 51144, 341, 307, 257, 588, 11, 588, 665, 1168, 13, 286, 914, 11, 286, 11, 309, 311, 746, 286, 5298, 2584, 281, 584, 13, 51368, 51748], "temperature": 0.0, "avg_logprob": -0.281155268351237, "compression_ratio": 1.547486033519553, "no_speech_prob": 1.0615437531669158e-05}, {"id": 261, "seek": 184304, "start": 1843.04, "end": 1847.28, "text": " So on the original encoder, we were always starting from some point. Then we were getting", "tokens": [50364, 407, 322, 264, 3380, 2058, 19866, 11, 321, 645, 1009, 2891, 490, 512, 935, 13, 1396, 321, 645, 1242, 50576, 50576, 646, 281, 264, 1901, 13, 492, 645, 2684, 257, 707, 857, 300, 935, 1270, 300, 321, 727, 2060, 512, 1859, 50800, 50800, 293, 550, 352, 646, 281, 264, 661, 1252, 13, 400, 550, 291, 853, 281, 652, 729, 732, 1998, 11, 558, 30, 583, 294, 527, 1389, 11, 51072, 51072, 558, 586, 294, 341, 1337, 1166, 661, 29267, 2533, 11, 321, 767, 2891, 490, 264, 558, 1011, 1252, 13, 51268, 51312, 407, 294, 264, 1337, 1166, 661, 29267, 2533, 11, 291, 722, 490, 264, 558, 1011, 1252, 13, 407, 291, 722, 490, 264, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.31485913163524565, "compression_ratio": 2.0296610169491527, "no_speech_prob": 3.363309588166885e-05}, {"id": 262, "seek": 184304, "start": 1847.28, "end": 1851.76, "text": " back to the space. We were moving a little bit that point such that we could cover some area", "tokens": [50364, 407, 322, 264, 3380, 2058, 19866, 11, 321, 645, 1009, 2891, 490, 512, 935, 13, 1396, 321, 645, 1242, 50576, 50576, 646, 281, 264, 1901, 13, 492, 645, 2684, 257, 707, 857, 300, 935, 1270, 300, 321, 727, 2060, 512, 1859, 50800, 50800, 293, 550, 352, 646, 281, 264, 661, 1252, 13, 400, 550, 291, 853, 281, 652, 729, 732, 1998, 11, 558, 30, 583, 294, 527, 1389, 11, 51072, 51072, 558, 586, 294, 341, 1337, 1166, 661, 29267, 2533, 11, 321, 767, 2891, 490, 264, 558, 1011, 1252, 13, 51268, 51312, 407, 294, 264, 1337, 1166, 661, 29267, 2533, 11, 291, 722, 490, 264, 558, 1011, 1252, 13, 407, 291, 722, 490, 264, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.31485913163524565, "compression_ratio": 2.0296610169491527, "no_speech_prob": 3.363309588166885e-05}, {"id": 263, "seek": 184304, "start": 1851.76, "end": 1857.2, "text": " and then go back to the other side. And then you try to make those two close, right? But in our case,", "tokens": [50364, 407, 322, 264, 3380, 2058, 19866, 11, 321, 645, 1009, 2891, 490, 512, 935, 13, 1396, 321, 645, 1242, 50576, 50576, 646, 281, 264, 1901, 13, 492, 645, 2684, 257, 707, 857, 300, 935, 1270, 300, 321, 727, 2060, 512, 1859, 50800, 50800, 293, 550, 352, 646, 281, 264, 661, 1252, 13, 400, 550, 291, 853, 281, 652, 729, 732, 1998, 11, 558, 30, 583, 294, 527, 1389, 11, 51072, 51072, 558, 586, 294, 341, 1337, 1166, 661, 29267, 2533, 11, 321, 767, 2891, 490, 264, 558, 1011, 1252, 13, 51268, 51312, 407, 294, 264, 1337, 1166, 661, 29267, 2533, 11, 291, 722, 490, 264, 558, 1011, 1252, 13, 407, 291, 722, 490, 264, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.31485913163524565, "compression_ratio": 2.0296610169491527, "no_speech_prob": 3.363309588166885e-05}, {"id": 264, "seek": 184304, "start": 1857.2, "end": 1861.12, "text": " right now in this generative other cellular net, we actually starting from the right hand side.", "tokens": [50364, 407, 322, 264, 3380, 2058, 19866, 11, 321, 645, 1009, 2891, 490, 512, 935, 13, 1396, 321, 645, 1242, 50576, 50576, 646, 281, 264, 1901, 13, 492, 645, 2684, 257, 707, 857, 300, 935, 1270, 300, 321, 727, 2060, 512, 1859, 50800, 50800, 293, 550, 352, 646, 281, 264, 661, 1252, 13, 400, 550, 291, 853, 281, 652, 729, 732, 1998, 11, 558, 30, 583, 294, 527, 1389, 11, 51072, 51072, 558, 586, 294, 341, 1337, 1166, 661, 29267, 2533, 11, 321, 767, 2891, 490, 264, 558, 1011, 1252, 13, 51268, 51312, 407, 294, 264, 1337, 1166, 661, 29267, 2533, 11, 291, 722, 490, 264, 558, 1011, 1252, 13, 407, 291, 722, 490, 264, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.31485913163524565, "compression_ratio": 2.0296610169491527, "no_speech_prob": 3.363309588166885e-05}, {"id": 265, "seek": 184304, "start": 1862.0, "end": 1868.1599999999999, "text": " So in the generative other cellular net, you start from the right hand side. So you start from the", "tokens": [50364, 407, 322, 264, 3380, 2058, 19866, 11, 321, 645, 1009, 2891, 490, 512, 935, 13, 1396, 321, 645, 1242, 50576, 50576, 646, 281, 264, 1901, 13, 492, 645, 2684, 257, 707, 857, 300, 935, 1270, 300, 321, 727, 2060, 512, 1859, 50800, 50800, 293, 550, 352, 646, 281, 264, 661, 1252, 13, 400, 550, 291, 853, 281, 652, 729, 732, 1998, 11, 558, 30, 583, 294, 527, 1389, 11, 51072, 51072, 558, 586, 294, 341, 1337, 1166, 661, 29267, 2533, 11, 321, 767, 2891, 490, 264, 558, 1011, 1252, 13, 51268, 51312, 407, 294, 264, 1337, 1166, 661, 29267, 2533, 11, 291, 722, 490, 264, 558, 1011, 1252, 13, 407, 291, 722, 490, 264, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.31485913163524565, "compression_ratio": 2.0296610169491527, "no_speech_prob": 3.363309588166885e-05}, {"id": 266, "seek": 186816, "start": 1868.16, "end": 1873.2, "text": " net. We actually starting from the right hand side. So in the generative other cellular net, you start", "tokens": [50364, 2533, 13, 492, 767, 2891, 490, 264, 558, 1011, 1252, 13, 407, 294, 264, 1337, 1166, 661, 29267, 2533, 11, 291, 722, 50616, 50616, 490, 264, 558, 13, 821, 307, 572, 17076, 4984, 1296, 341, 2146, 510, 293, 341, 2146, 510, 13, 1057, 291, 50992, 50992, 362, 307, 257, 2063, 3209, 11, 597, 307, 3585, 291, 1968, 291, 366, 322, 341, 733, 295, 551, 510, 11, 558, 30, 286, 393, 380, 11, 51404, 51448, 309, 311, 516, 281, 312, 12246, 11, 457, 1392, 13, 821, 311, 257, 2063, 3209, 293, 309, 311, 516, 281, 980, 291, 294, 341, 1389, 11, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.13345622099362886, "compression_ratio": 1.7831858407079646, "no_speech_prob": 0.00013093494635540992}, {"id": 267, "seek": 186816, "start": 1873.2, "end": 1880.72, "text": " from the right. There is no whatsoever connection between this guy here and this guy here. All you", "tokens": [50364, 2533, 13, 492, 767, 2891, 490, 264, 558, 1011, 1252, 13, 407, 294, 264, 1337, 1166, 661, 29267, 2533, 11, 291, 722, 50616, 50616, 490, 264, 558, 13, 821, 307, 572, 17076, 4984, 1296, 341, 2146, 510, 293, 341, 2146, 510, 13, 1057, 291, 50992, 50992, 362, 307, 257, 2063, 3209, 11, 597, 307, 3585, 291, 1968, 291, 366, 322, 341, 733, 295, 551, 510, 11, 558, 30, 286, 393, 380, 11, 51404, 51448, 309, 311, 516, 281, 312, 12246, 11, 457, 1392, 13, 821, 311, 257, 2063, 3209, 293, 309, 311, 516, 281, 980, 291, 294, 341, 1389, 11, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.13345622099362886, "compression_ratio": 1.7831858407079646, "no_speech_prob": 0.00013093494635540992}, {"id": 268, "seek": 186816, "start": 1880.72, "end": 1888.96, "text": " have is a cost network, which is telling you whether you are on this kind of thing here, right? I can't,", "tokens": [50364, 2533, 13, 492, 767, 2891, 490, 264, 558, 1011, 1252, 13, 407, 294, 264, 1337, 1166, 661, 29267, 2533, 11, 291, 722, 50616, 50616, 490, 264, 558, 13, 821, 307, 572, 17076, 4984, 1296, 341, 2146, 510, 293, 341, 2146, 510, 13, 1057, 291, 50992, 50992, 362, 307, 257, 2063, 3209, 11, 597, 307, 3585, 291, 1968, 291, 366, 322, 341, 733, 295, 551, 510, 11, 558, 30, 286, 393, 380, 11, 51404, 51448, 309, 311, 516, 281, 312, 12246, 11, 457, 1392, 13, 821, 311, 257, 2063, 3209, 293, 309, 311, 516, 281, 980, 291, 294, 341, 1389, 11, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.13345622099362886, "compression_ratio": 1.7831858407079646, "no_speech_prob": 0.00013093494635540992}, {"id": 269, "seek": 186816, "start": 1889.8400000000001, "end": 1894.5600000000002, "text": " it's going to be ugly, but okay. There's a cost network and it's going to tell you in this case,", "tokens": [50364, 2533, 13, 492, 767, 2891, 490, 264, 558, 1011, 1252, 13, 407, 294, 264, 1337, 1166, 661, 29267, 2533, 11, 291, 722, 50616, 50616, 490, 264, 558, 13, 821, 307, 572, 17076, 4984, 1296, 341, 2146, 510, 293, 341, 2146, 510, 13, 1057, 291, 50992, 50992, 362, 307, 257, 2063, 3209, 11, 597, 307, 3585, 291, 1968, 291, 366, 322, 341, 733, 295, 551, 510, 11, 558, 30, 286, 393, 380, 11, 51404, 51448, 309, 311, 516, 281, 312, 12246, 11, 457, 1392, 13, 821, 311, 257, 2063, 3209, 293, 309, 311, 516, 281, 980, 291, 294, 341, 1389, 11, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.13345622099362886, "compression_ratio": 1.7831858407079646, "no_speech_prob": 0.00013093494635540992}, {"id": 270, "seek": 189456, "start": 1894.56, "end": 1902.0, "text": " plus 10 here. And then it's going to tell you, let's say zero here. Okay. And the other case,", "tokens": [50364, 1804, 1266, 510, 13, 400, 550, 309, 311, 516, 281, 980, 291, 11, 718, 311, 584, 4018, 510, 13, 1033, 13, 400, 264, 661, 1389, 11, 50736, 50736, 291, 362, 257, 1337, 1166, 3209, 510, 11, 597, 307, 18350, 341, 4846, 510, 760, 281, 510, 13, 1779, 30, 51016, 51056, 407, 472, 307, 8895, 294, 1668, 281, 362, 264, 2295, 4190, 926, 264, 47138, 293, 550, 4833, 4190, 2380, 13, 51376, 51376, 400, 550, 291, 512, 11, 291, 576, 411, 746, 300, 307, 411, 11, 291, 458, 11, 291, 815, 528, 512, 7605, 4358, 11, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.12118554356122258, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.7463887565536425e-05}, {"id": 271, "seek": 189456, "start": 1902.0, "end": 1907.6, "text": " you have a generative network here, which is mapping this input here down to here. Right?", "tokens": [50364, 1804, 1266, 510, 13, 400, 550, 309, 311, 516, 281, 980, 291, 11, 718, 311, 584, 4018, 510, 13, 1033, 13, 400, 264, 661, 1389, 11, 50736, 50736, 291, 362, 257, 1337, 1166, 3209, 510, 11, 597, 307, 18350, 341, 4846, 510, 760, 281, 510, 13, 1779, 30, 51016, 51056, 407, 472, 307, 8895, 294, 1668, 281, 362, 264, 2295, 4190, 926, 264, 47138, 293, 550, 4833, 4190, 2380, 13, 51376, 51376, 400, 550, 291, 512, 11, 291, 576, 411, 746, 300, 307, 411, 11, 291, 458, 11, 291, 815, 528, 512, 7605, 4358, 11, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.12118554356122258, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.7463887565536425e-05}, {"id": 272, "seek": 189456, "start": 1908.3999999999999, "end": 1914.8, "text": " So one is trained in order to have the low values around the manifold and then larger values outside.", "tokens": [50364, 1804, 1266, 510, 13, 400, 550, 309, 311, 516, 281, 980, 291, 11, 718, 311, 584, 4018, 510, 13, 1033, 13, 400, 264, 661, 1389, 11, 50736, 50736, 291, 362, 257, 1337, 1166, 3209, 510, 11, 597, 307, 18350, 341, 4846, 510, 760, 281, 510, 13, 1779, 30, 51016, 51056, 407, 472, 307, 8895, 294, 1668, 281, 362, 264, 2295, 4190, 926, 264, 47138, 293, 550, 4833, 4190, 2380, 13, 51376, 51376, 400, 550, 291, 512, 11, 291, 576, 411, 746, 300, 307, 411, 11, 291, 458, 11, 291, 815, 528, 512, 7605, 4358, 11, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.12118554356122258, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.7463887565536425e-05}, {"id": 273, "seek": 189456, "start": 1914.8, "end": 1920.6399999999999, "text": " And then you some, you would like something that is like, you know, you may want some curve levels,", "tokens": [50364, 1804, 1266, 510, 13, 400, 550, 309, 311, 516, 281, 980, 291, 11, 718, 311, 584, 4018, 510, 13, 1033, 13, 400, 264, 661, 1389, 11, 50736, 50736, 291, 362, 257, 1337, 1166, 3209, 510, 11, 597, 307, 18350, 341, 4846, 510, 760, 281, 510, 13, 1779, 30, 51016, 51056, 407, 472, 307, 8895, 294, 1668, 281, 362, 264, 2295, 4190, 926, 264, 47138, 293, 550, 4833, 4190, 2380, 13, 51376, 51376, 400, 550, 291, 512, 11, 291, 576, 411, 746, 300, 307, 411, 11, 291, 458, 11, 291, 815, 528, 512, 7605, 4358, 11, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.12118554356122258, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.7463887565536425e-05}, {"id": 274, "seek": 192064, "start": 1920.64, "end": 1925.2, "text": " right? Like that such that as you move further away, and this stuff keeps increasing.", "tokens": [50364, 558, 30, 1743, 300, 1270, 300, 382, 291, 1286, 3052, 1314, 11, 293, 341, 1507, 5965, 5662, 13, 50592, 50648, 759, 291, 362, 257, 20828, 1639, 11, 436, 486, 3464, 281, 362, 4018, 510, 293, 472, 2380, 2293, 294, 341, 51024, 51052, 47138, 11, 411, 588, 11, 588, 1998, 538, 13, 1779, 13, 400, 370, 300, 7829, 867, 2740, 13, 407, 11, 1392, 11, 718, 385, 51468, 51468, 853, 1071, 21663, 13, 821, 307, 1071, 21663, 13, 407, 11, 1954, 11, 456, 366, 1651, 11, 544, 1651, 13, 961, 385, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.21174144744873047, "compression_ratio": 1.6051502145922747, "no_speech_prob": 1.718480598356109e-05}, {"id": 275, "seek": 192064, "start": 1926.3200000000002, "end": 1933.8400000000001, "text": " If you have a discriminator, they will force to have zero here and one outside exactly in this", "tokens": [50364, 558, 30, 1743, 300, 1270, 300, 382, 291, 1286, 3052, 1314, 11, 293, 341, 1507, 5965, 5662, 13, 50592, 50648, 759, 291, 362, 257, 20828, 1639, 11, 436, 486, 3464, 281, 362, 4018, 510, 293, 472, 2380, 2293, 294, 341, 51024, 51052, 47138, 11, 411, 588, 11, 588, 1998, 538, 13, 1779, 13, 400, 370, 300, 7829, 867, 2740, 13, 407, 11, 1392, 11, 718, 385, 51468, 51468, 853, 1071, 21663, 13, 821, 307, 1071, 21663, 13, 407, 11, 1954, 11, 456, 366, 1651, 11, 544, 1651, 13, 961, 385, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.21174144744873047, "compression_ratio": 1.6051502145922747, "no_speech_prob": 1.718480598356109e-05}, {"id": 276, "seek": 192064, "start": 1934.4, "end": 1942.72, "text": " manifold, like very, very close by. Right. And so that creates many problems. So, okay, let me", "tokens": [50364, 558, 30, 1743, 300, 1270, 300, 382, 291, 1286, 3052, 1314, 11, 293, 341, 1507, 5965, 5662, 13, 50592, 50648, 759, 291, 362, 257, 20828, 1639, 11, 436, 486, 3464, 281, 362, 4018, 510, 293, 472, 2380, 2293, 294, 341, 51024, 51052, 47138, 11, 411, 588, 11, 588, 1998, 538, 13, 1779, 13, 400, 370, 300, 7829, 867, 2740, 13, 407, 11, 1392, 11, 718, 385, 51468, 51468, 853, 1071, 21663, 13, 821, 307, 1071, 21663, 13, 407, 11, 1954, 11, 456, 366, 1651, 11, 544, 1651, 13, 961, 385, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.21174144744873047, "compression_ratio": 1.6051502145922747, "no_speech_prob": 1.718480598356109e-05}, {"id": 277, "seek": 192064, "start": 1942.72, "end": 1949.44, "text": " try another analogy. There is another analogy. So, oh, there are questions, more questions. Let me", "tokens": [50364, 558, 30, 1743, 300, 1270, 300, 382, 291, 1286, 3052, 1314, 11, 293, 341, 1507, 5965, 5662, 13, 50592, 50648, 759, 291, 362, 257, 20828, 1639, 11, 436, 486, 3464, 281, 362, 4018, 510, 293, 472, 2380, 2293, 294, 341, 51024, 51052, 47138, 11, 411, 588, 11, 588, 1998, 538, 13, 1779, 13, 400, 370, 300, 7829, 867, 2740, 13, 407, 11, 1392, 11, 718, 385, 51468, 51468, 853, 1071, 21663, 13, 821, 307, 1071, 21663, 13, 407, 11, 1954, 11, 456, 366, 1651, 11, 544, 1651, 13, 961, 385, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.21174144744873047, "compression_ratio": 1.6051502145922747, "no_speech_prob": 1.718480598356109e-05}, {"id": 278, "seek": 194944, "start": 1949.44, "end": 1956.3200000000002, "text": " go with the analogy. Then let's see whether this makes more sense. Let me actually see myself such", "tokens": [50364, 352, 365, 264, 21663, 13, 1396, 718, 311, 536, 1968, 341, 1669, 544, 2020, 13, 961, 385, 767, 536, 2059, 1270, 50708, 50708, 300, 286, 393, 11, 1392, 13, 286, 393, 536, 586, 2059, 13, 1057, 558, 13, 407, 291, 362, 411, 512, 2074, 1412, 2793, 510, 13, 1033, 13, 51184, 51184, 400, 550, 291, 362, 512, 10833, 1412, 2793, 670, 510, 300, 362, 668, 10833, 538, 264, 19265, 13, 51504, 51504, 1779, 13, 407, 2793, 510, 11, 2793, 760, 456, 13, 961, 311, 6552, 586, 321, 366, 1417, 466, 341, 20828, 1639, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.10781200564637476, "compression_ratio": 1.7081545064377683, "no_speech_prob": 9.079987648874521e-06}, {"id": 279, "seek": 194944, "start": 1956.3200000000002, "end": 1965.8400000000001, "text": " that I can, okay. I can see now myself. All right. So you have like some true data points here. Okay.", "tokens": [50364, 352, 365, 264, 21663, 13, 1396, 718, 311, 536, 1968, 341, 1669, 544, 2020, 13, 961, 385, 767, 536, 2059, 1270, 50708, 50708, 300, 286, 393, 11, 1392, 13, 286, 393, 536, 586, 2059, 13, 1057, 558, 13, 407, 291, 362, 411, 512, 2074, 1412, 2793, 510, 13, 1033, 13, 51184, 51184, 400, 550, 291, 362, 512, 10833, 1412, 2793, 670, 510, 300, 362, 668, 10833, 538, 264, 19265, 13, 51504, 51504, 1779, 13, 407, 2793, 510, 11, 2793, 760, 456, 13, 961, 311, 6552, 586, 321, 366, 1417, 466, 341, 20828, 1639, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.10781200564637476, "compression_ratio": 1.7081545064377683, "no_speech_prob": 9.079987648874521e-06}, {"id": 280, "seek": 194944, "start": 1965.8400000000001, "end": 1972.24, "text": " And then you have some generated data points over here that have been generated by the generator.", "tokens": [50364, 352, 365, 264, 21663, 13, 1396, 718, 311, 536, 1968, 341, 1669, 544, 2020, 13, 961, 385, 767, 536, 2059, 1270, 50708, 50708, 300, 286, 393, 11, 1392, 13, 286, 393, 536, 586, 2059, 13, 1057, 558, 13, 407, 291, 362, 411, 512, 2074, 1412, 2793, 510, 13, 1033, 13, 51184, 51184, 400, 550, 291, 362, 512, 10833, 1412, 2793, 670, 510, 300, 362, 668, 10833, 538, 264, 19265, 13, 51504, 51504, 1779, 13, 407, 2793, 510, 11, 2793, 760, 456, 13, 961, 311, 6552, 586, 321, 366, 1417, 466, 341, 20828, 1639, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.10781200564637476, "compression_ratio": 1.7081545064377683, "no_speech_prob": 9.079987648874521e-06}, {"id": 281, "seek": 194944, "start": 1972.24, "end": 1978.72, "text": " Right. So points here, points down there. Let's assume now we are talking about this discriminator.", "tokens": [50364, 352, 365, 264, 21663, 13, 1396, 718, 311, 536, 1968, 341, 1669, 544, 2020, 13, 961, 385, 767, 536, 2059, 1270, 50708, 50708, 300, 286, 393, 11, 1392, 13, 286, 393, 536, 586, 2059, 13, 1057, 558, 13, 407, 291, 362, 411, 512, 2074, 1412, 2793, 510, 13, 1033, 13, 51184, 51184, 400, 550, 291, 362, 512, 10833, 1412, 2793, 670, 510, 300, 362, 668, 10833, 538, 264, 19265, 13, 51504, 51504, 1779, 13, 407, 2793, 510, 11, 2793, 760, 456, 13, 961, 311, 6552, 586, 321, 366, 1417, 466, 341, 20828, 1639, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.10781200564637476, "compression_ratio": 1.7081545064377683, "no_speech_prob": 9.079987648874521e-06}, {"id": 282, "seek": 197872, "start": 1978.72, "end": 1984.48, "text": " Okay. So that I can illustrate what are the problems there. So you have a discriminator,", "tokens": [50364, 1033, 13, 407, 300, 286, 393, 23221, 437, 366, 264, 2740, 456, 13, 407, 291, 362, 257, 20828, 1639, 11, 50652, 50652, 597, 575, 613, 732, 733, 295, 1412, 13, 509, 362, 2074, 1412, 760, 510, 11, 7592, 1412, 670, 510, 13, 400, 370, 437, 51020, 51020, 775, 264, 20828, 1639, 360, 30, 440, 20828, 1639, 3537, 12866, 307, 516, 281, 312, 445, 257, 1622, 51240, 51284, 510, 11, 558, 30, 663, 307, 6492, 341, 1507, 294, 1922, 13, 1779, 13, 407, 1400, 13, 865, 13, 1779, 13, 1079, 13, 1033, 13, 8561, 13, 407, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.1263203283753058, "compression_ratio": 1.6681614349775784, "no_speech_prob": 1.3843298802385107e-05}, {"id": 283, "seek": 197872, "start": 1984.48, "end": 1991.84, "text": " which has these two kind of data. You have true data down here, fake data over here. And so what", "tokens": [50364, 1033, 13, 407, 300, 286, 393, 23221, 437, 366, 264, 2740, 456, 13, 407, 291, 362, 257, 20828, 1639, 11, 50652, 50652, 597, 575, 613, 732, 733, 295, 1412, 13, 509, 362, 2074, 1412, 760, 510, 11, 7592, 1412, 670, 510, 13, 400, 370, 437, 51020, 51020, 775, 264, 20828, 1639, 360, 30, 440, 20828, 1639, 3537, 12866, 307, 516, 281, 312, 445, 257, 1622, 51240, 51284, 510, 11, 558, 30, 663, 307, 6492, 341, 1507, 294, 1922, 13, 1779, 13, 407, 1400, 13, 865, 13, 1779, 13, 1079, 13, 1033, 13, 8561, 13, 407, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.1263203283753058, "compression_ratio": 1.6681614349775784, "no_speech_prob": 1.3843298802385107e-05}, {"id": 284, "seek": 197872, "start": 1991.84, "end": 1996.24, "text": " does the discriminator do? The discriminator decision boundary is going to be just a line", "tokens": [50364, 1033, 13, 407, 300, 286, 393, 23221, 437, 366, 264, 2740, 456, 13, 407, 291, 362, 257, 20828, 1639, 11, 50652, 50652, 597, 575, 613, 732, 733, 295, 1412, 13, 509, 362, 2074, 1412, 760, 510, 11, 7592, 1412, 670, 510, 13, 400, 370, 437, 51020, 51020, 775, 264, 20828, 1639, 360, 30, 440, 20828, 1639, 3537, 12866, 307, 516, 281, 312, 445, 257, 1622, 51240, 51284, 510, 11, 558, 30, 663, 307, 6492, 341, 1507, 294, 1922, 13, 1779, 13, 407, 1400, 13, 865, 13, 1779, 13, 1079, 13, 1033, 13, 8561, 13, 407, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.1263203283753058, "compression_ratio": 1.6681614349775784, "no_speech_prob": 1.3843298802385107e-05}, {"id": 285, "seek": 197872, "start": 1997.1200000000001, "end": 2004.88, "text": " here, right? That is cutting this stuff in half. Right. So far. Yeah. Right. Yes. Okay. Cool. So", "tokens": [50364, 1033, 13, 407, 300, 286, 393, 23221, 437, 366, 264, 2740, 456, 13, 407, 291, 362, 257, 20828, 1639, 11, 50652, 50652, 597, 575, 613, 732, 733, 295, 1412, 13, 509, 362, 2074, 1412, 760, 510, 11, 7592, 1412, 670, 510, 13, 400, 370, 437, 51020, 51020, 775, 264, 20828, 1639, 360, 30, 440, 20828, 1639, 3537, 12866, 307, 516, 281, 312, 445, 257, 1622, 51240, 51284, 510, 11, 558, 30, 663, 307, 6492, 341, 1507, 294, 1922, 13, 1779, 13, 407, 1400, 13, 865, 13, 1779, 13, 1079, 13, 1033, 13, 8561, 13, 407, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.1263203283753058, "compression_ratio": 1.6681614349775784, "no_speech_prob": 1.3843298802385107e-05}, {"id": 286, "seek": 200488, "start": 2004.88, "end": 2010.5600000000002, "text": " now you turn on the second step. Second step is going to be you turn on gravity on this decision", "tokens": [50364, 586, 291, 1261, 322, 264, 1150, 1823, 13, 5736, 1823, 307, 516, 281, 312, 291, 1261, 322, 12110, 322, 341, 3537, 50648, 50648, 12866, 13, 407, 613, 2793, 300, 366, 510, 486, 312, 9351, 11, 7440, 760, 510, 13, 1033, 13, 440, 935, 510, 50964, 50964, 2170, 15912, 538, 264, 3537, 12866, 13, 407, 321, 3847, 700, 264, 20828, 1639, 13, 492, 632, 341, 733, 295, 51232, 51304, 3537, 12866, 13, 400, 550, 321, 3847, 264, 19265, 13, 509, 362, 613, 1074, 45339, 760, 510, 13, 51572, 51620], "temperature": 0.0, "avg_logprob": -0.11910188462999131, "compression_ratio": 1.819047619047619, "no_speech_prob": 1.8616174202179536e-05}, {"id": 287, "seek": 200488, "start": 2010.5600000000002, "end": 2016.88, "text": " boundary. So these points that are here will be boom, falling down here. Okay. The point here", "tokens": [50364, 586, 291, 1261, 322, 264, 1150, 1823, 13, 5736, 1823, 307, 516, 281, 312, 291, 1261, 322, 12110, 322, 341, 3537, 50648, 50648, 12866, 13, 407, 613, 2793, 300, 366, 510, 486, 312, 9351, 11, 7440, 760, 510, 13, 1033, 13, 440, 935, 510, 50964, 50964, 2170, 15912, 538, 264, 3537, 12866, 13, 407, 321, 3847, 700, 264, 20828, 1639, 13, 492, 632, 341, 733, 295, 51232, 51304, 3537, 12866, 13, 400, 550, 321, 3847, 264, 19265, 13, 509, 362, 613, 1074, 45339, 760, 510, 13, 51572, 51620], "temperature": 0.0, "avg_logprob": -0.11910188462999131, "compression_ratio": 1.819047619047619, "no_speech_prob": 1.8616174202179536e-05}, {"id": 288, "seek": 200488, "start": 2016.88, "end": 2022.24, "text": " gets attracted by the decision boundary. So we train first the discriminator. We had this kind of", "tokens": [50364, 586, 291, 1261, 322, 264, 1150, 1823, 13, 5736, 1823, 307, 516, 281, 312, 291, 1261, 322, 12110, 322, 341, 3537, 50648, 50648, 12866, 13, 407, 613, 2793, 300, 366, 510, 486, 312, 9351, 11, 7440, 760, 510, 13, 1033, 13, 440, 935, 510, 50964, 50964, 2170, 15912, 538, 264, 3537, 12866, 13, 407, 321, 3847, 700, 264, 20828, 1639, 13, 492, 632, 341, 733, 295, 51232, 51304, 3537, 12866, 13, 400, 550, 321, 3847, 264, 19265, 13, 509, 362, 613, 1074, 45339, 760, 510, 13, 51572, 51620], "temperature": 0.0, "avg_logprob": -0.11910188462999131, "compression_ratio": 1.819047619047619, "no_speech_prob": 1.8616174202179536e-05}, {"id": 289, "seek": 200488, "start": 2023.68, "end": 2029.0400000000002, "text": " decision boundary. And then we train the generator. You have these guys collapsing down here.", "tokens": [50364, 586, 291, 1261, 322, 264, 1150, 1823, 13, 5736, 1823, 307, 516, 281, 312, 291, 1261, 322, 12110, 322, 341, 3537, 50648, 50648, 12866, 13, 407, 613, 2793, 300, 366, 510, 486, 312, 9351, 11, 7440, 760, 510, 13, 1033, 13, 440, 935, 510, 50964, 50964, 2170, 15912, 538, 264, 3537, 12866, 13, 407, 321, 3847, 700, 264, 20828, 1639, 13, 492, 632, 341, 733, 295, 51232, 51304, 3537, 12866, 13, 400, 550, 321, 3847, 264, 19265, 13, 509, 362, 613, 1074, 45339, 760, 510, 13, 51572, 51620], "temperature": 0.0, "avg_logprob": -0.11910188462999131, "compression_ratio": 1.819047619047619, "no_speech_prob": 1.8616174202179536e-05}, {"id": 290, "seek": 202904, "start": 2029.04, "end": 2035.84, "text": " So then you're going to be in a new situation. You have true data here, fake data here. You train", "tokens": [50364, 407, 550, 291, 434, 516, 281, 312, 294, 257, 777, 2590, 13, 509, 362, 2074, 1412, 510, 11, 7592, 1412, 510, 13, 509, 3847, 50704, 50704, 797, 264, 20828, 1639, 294, 341, 1389, 11, 291, 434, 516, 281, 362, 257, 3537, 12866, 11, 597, 307, 516, 281, 312, 50900, 50900, 15461, 510, 13, 1779, 13, 1396, 291, 1261, 322, 12110, 1270, 300, 613, 2793, 510, 486, 15584, 510, 13, 1779, 13, 51232, 51232, 400, 550, 291, 1066, 17138, 990, 341, 1507, 11, 558, 30, 639, 1507, 486, 312, 1242, 4966, 293, 4966, 293, 51444, 51444, 4966, 293, 4966, 293, 4966, 281, 264, 2074, 1412, 13, 1779, 13, 407, 291, 362, 613, 2793, 300, 366, 411, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.18853049719033121, "compression_ratio": 1.9598393574297188, "no_speech_prob": 1.4967245988373179e-05}, {"id": 291, "seek": 202904, "start": 2035.84, "end": 2039.76, "text": " again the discriminator in this case, you're going to have a decision boundary, which is going to be", "tokens": [50364, 407, 550, 291, 434, 516, 281, 312, 294, 257, 777, 2590, 13, 509, 362, 2074, 1412, 510, 11, 7592, 1412, 510, 13, 509, 3847, 50704, 50704, 797, 264, 20828, 1639, 294, 341, 1389, 11, 291, 434, 516, 281, 362, 257, 3537, 12866, 11, 597, 307, 516, 281, 312, 50900, 50900, 15461, 510, 13, 1779, 13, 1396, 291, 1261, 322, 12110, 1270, 300, 613, 2793, 510, 486, 15584, 510, 13, 1779, 13, 51232, 51232, 400, 550, 291, 1066, 17138, 990, 341, 1507, 11, 558, 30, 639, 1507, 486, 312, 1242, 4966, 293, 4966, 293, 51444, 51444, 4966, 293, 4966, 293, 4966, 281, 264, 2074, 1412, 13, 1779, 13, 407, 291, 362, 613, 2793, 300, 366, 411, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.18853049719033121, "compression_ratio": 1.9598393574297188, "no_speech_prob": 1.4967245988373179e-05}, {"id": 292, "seek": 202904, "start": 2039.76, "end": 2046.3999999999999, "text": " halfway here. Right. Then you turn on gravity such that these points here will collapse here. Right.", "tokens": [50364, 407, 550, 291, 434, 516, 281, 312, 294, 257, 777, 2590, 13, 509, 362, 2074, 1412, 510, 11, 7592, 1412, 510, 13, 509, 3847, 50704, 50704, 797, 264, 20828, 1639, 294, 341, 1389, 11, 291, 434, 516, 281, 362, 257, 3537, 12866, 11, 597, 307, 516, 281, 312, 50900, 50900, 15461, 510, 13, 1779, 13, 1396, 291, 1261, 322, 12110, 1270, 300, 613, 2793, 510, 486, 15584, 510, 13, 1779, 13, 51232, 51232, 400, 550, 291, 1066, 17138, 990, 341, 1507, 11, 558, 30, 639, 1507, 486, 312, 1242, 4966, 293, 4966, 293, 51444, 51444, 4966, 293, 4966, 293, 4966, 281, 264, 2074, 1412, 13, 1779, 13, 407, 291, 362, 613, 2793, 300, 366, 411, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.18853049719033121, "compression_ratio": 1.9598393574297188, "no_speech_prob": 1.4967245988373179e-05}, {"id": 293, "seek": 202904, "start": 2046.3999999999999, "end": 2050.64, "text": " And then you keep iterating this stuff, right? This stuff will be getting closer and closer and", "tokens": [50364, 407, 550, 291, 434, 516, 281, 312, 294, 257, 777, 2590, 13, 509, 362, 2074, 1412, 510, 11, 7592, 1412, 510, 13, 509, 3847, 50704, 50704, 797, 264, 20828, 1639, 294, 341, 1389, 11, 291, 434, 516, 281, 362, 257, 3537, 12866, 11, 597, 307, 516, 281, 312, 50900, 50900, 15461, 510, 13, 1779, 13, 1396, 291, 1261, 322, 12110, 1270, 300, 613, 2793, 510, 486, 15584, 510, 13, 1779, 13, 51232, 51232, 400, 550, 291, 1066, 17138, 990, 341, 1507, 11, 558, 30, 639, 1507, 486, 312, 1242, 4966, 293, 4966, 293, 51444, 51444, 4966, 293, 4966, 293, 4966, 281, 264, 2074, 1412, 13, 1779, 13, 407, 291, 362, 613, 2793, 300, 366, 411, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.18853049719033121, "compression_ratio": 1.9598393574297188, "no_speech_prob": 1.4967245988373179e-05}, {"id": 294, "seek": 202904, "start": 2050.64, "end": 2056.48, "text": " closer and closer and closer to the true data. Right. So you have these points that are like", "tokens": [50364, 407, 550, 291, 434, 516, 281, 312, 294, 257, 777, 2590, 13, 509, 362, 2074, 1412, 510, 11, 7592, 1412, 510, 13, 509, 3847, 50704, 50704, 797, 264, 20828, 1639, 294, 341, 1389, 11, 291, 434, 516, 281, 362, 257, 3537, 12866, 11, 597, 307, 516, 281, 312, 50900, 50900, 15461, 510, 13, 1779, 13, 1396, 291, 1261, 322, 12110, 1270, 300, 613, 2793, 510, 486, 15584, 510, 13, 1779, 13, 51232, 51232, 400, 550, 291, 1066, 17138, 990, 341, 1507, 11, 558, 30, 639, 1507, 486, 312, 1242, 4966, 293, 4966, 293, 51444, 51444, 4966, 293, 4966, 293, 4966, 281, 264, 2074, 1412, 13, 1779, 13, 407, 291, 362, 613, 2793, 300, 366, 411, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.18853049719033121, "compression_ratio": 1.9598393574297188, "no_speech_prob": 1.4967245988373179e-05}, {"id": 295, "seek": 205648, "start": 2056.48, "end": 2069.12, "text": " approaching and arriving to the real data location. So let's say now you're using your discriminator.", "tokens": [50364, 14908, 293, 22436, 281, 264, 957, 1412, 4914, 13, 407, 718, 311, 584, 586, 291, 434, 1228, 428, 20828, 1639, 13, 50996, 50996, 509, 362, 729, 17434, 3278, 30867, 6064, 337, 3097, 264, 20828, 1639, 13, 708, 307, 586, 264, 2135, 51396, 51396, 2734, 30, 961, 311, 584, 286, 360, 257, 17573, 13, 286, 1565, 452, 2074, 1412, 510, 1270, 300, 321, 393, 536, 300, 264, 51828], "temperature": 0.0, "avg_logprob": -0.21430229795151862, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.5682209777878597e-05}, {"id": 296, "seek": 205648, "start": 2069.12, "end": 2077.12, "text": " You have those binary cross entropy laws for training the discriminator. What is now the main", "tokens": [50364, 14908, 293, 22436, 281, 264, 957, 1412, 4914, 13, 407, 718, 311, 584, 586, 291, 434, 1228, 428, 20828, 1639, 13, 50996, 50996, 509, 362, 729, 17434, 3278, 30867, 6064, 337, 3097, 264, 20828, 1639, 13, 708, 307, 586, 264, 2135, 51396, 51396, 2734, 30, 961, 311, 584, 286, 360, 257, 17573, 13, 286, 1565, 452, 2074, 1412, 510, 1270, 300, 321, 393, 536, 300, 264, 51828], "temperature": 0.0, "avg_logprob": -0.21430229795151862, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.5682209777878597e-05}, {"id": 297, "seek": 207712, "start": 2077.12, "end": 2087.52, "text": " issue? Let's say I do a shifting. I bring my true data here such that we can see better what happens.", "tokens": [50364, 2734, 30, 961, 311, 584, 286, 360, 257, 17573, 13, 286, 1565, 452, 2074, 1412, 510, 1270, 300, 321, 393, 536, 1101, 437, 2314, 13, 50884, 50884, 407, 291, 362, 2074, 1412, 510, 13, 509, 362, 10833, 1412, 510, 13, 1779, 13, 814, 366, 33535, 13, 400, 586, 51212, 51212, 291, 362, 257, 20828, 1639, 6492, 510, 13, 407, 291, 434, 516, 281, 362, 19959, 295, 613, 10938, 293, 341, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.13663950481930295, "compression_ratio": 1.575268817204301, "no_speech_prob": 6.5397498474339955e-06}, {"id": 298, "seek": 207712, "start": 2087.52, "end": 2094.08, "text": " So you have true data here. You have generated data here. Right. They are overlapping. And now", "tokens": [50364, 2734, 30, 961, 311, 584, 286, 360, 257, 17573, 13, 286, 1565, 452, 2074, 1412, 510, 1270, 300, 321, 393, 536, 1101, 437, 2314, 13, 50884, 50884, 407, 291, 362, 2074, 1412, 510, 13, 509, 362, 10833, 1412, 510, 13, 1779, 13, 814, 366, 33535, 13, 400, 586, 51212, 51212, 291, 362, 257, 20828, 1639, 6492, 510, 13, 407, 291, 434, 516, 281, 362, 19959, 295, 613, 10938, 293, 341, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.13663950481930295, "compression_ratio": 1.575268817204301, "no_speech_prob": 6.5397498474339955e-06}, {"id": 299, "seek": 207712, "start": 2094.08, "end": 2101.04, "text": " you have a discriminator cutting here. So you're going to have overlap of these samples and this", "tokens": [50364, 2734, 30, 961, 311, 584, 286, 360, 257, 17573, 13, 286, 1565, 452, 2074, 1412, 510, 1270, 300, 321, 393, 536, 1101, 437, 2314, 13, 50884, 50884, 407, 291, 362, 2074, 1412, 510, 13, 509, 362, 10833, 1412, 510, 13, 1779, 13, 814, 366, 33535, 13, 400, 586, 51212, 51212, 291, 362, 257, 20828, 1639, 6492, 510, 13, 407, 291, 434, 516, 281, 362, 19959, 295, 613, 10938, 293, 341, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.13663950481930295, "compression_ratio": 1.575268817204301, "no_speech_prob": 6.5397498474339955e-06}, {"id": 300, "seek": 210104, "start": 2101.04, "end": 2107.7599999999998, "text": " discriminator has no idea what to do. Right. So first of all, you're going to get misclassifications", "tokens": [50364, 20828, 1639, 575, 572, 1558, 437, 281, 360, 13, 1779, 13, 407, 700, 295, 439, 11, 291, 434, 516, 281, 483, 3346, 11665, 7833, 50700, 50700, 445, 570, 291, 1194, 291, 9652, 3004, 13, 492, 767, 41881, 13, 1779, 13, 759, 291, 519, 466, 300, 11, 51020, 51020, 452, 2074, 1412, 307, 510, 13, 1222, 10833, 1412, 307, 510, 13, 814, 366, 33535, 13, 51228, 51328, 407, 286, 767, 6453, 281, 2524, 32181, 13, 400, 586, 452, 20828, 1639, 575, 572, 17076, 13602, 577, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.1005572188984264, "compression_ratio": 1.6409090909090909, "no_speech_prob": 4.783576059708139e-06}, {"id": 301, "seek": 210104, "start": 2107.7599999999998, "end": 2114.16, "text": " just because you thought you converged. We actually converge. Right. If you think about that,", "tokens": [50364, 20828, 1639, 575, 572, 1558, 437, 281, 360, 13, 1779, 13, 407, 700, 295, 439, 11, 291, 434, 516, 281, 483, 3346, 11665, 7833, 50700, 50700, 445, 570, 291, 1194, 291, 9652, 3004, 13, 492, 767, 41881, 13, 1779, 13, 759, 291, 519, 466, 300, 11, 51020, 51020, 452, 2074, 1412, 307, 510, 13, 1222, 10833, 1412, 307, 510, 13, 814, 366, 33535, 13, 51228, 51328, 407, 286, 767, 6453, 281, 2524, 32181, 13, 400, 586, 452, 20828, 1639, 575, 572, 17076, 13602, 577, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.1005572188984264, "compression_ratio": 1.6409090909090909, "no_speech_prob": 4.783576059708139e-06}, {"id": 302, "seek": 210104, "start": 2114.16, "end": 2118.32, "text": " my true data is here. My generated data is here. They are overlapping.", "tokens": [50364, 20828, 1639, 575, 572, 1558, 437, 281, 360, 13, 1779, 13, 407, 700, 295, 439, 11, 291, 434, 516, 281, 483, 3346, 11665, 7833, 50700, 50700, 445, 570, 291, 1194, 291, 9652, 3004, 13, 492, 767, 41881, 13, 1779, 13, 759, 291, 519, 466, 300, 11, 51020, 51020, 452, 2074, 1412, 307, 510, 13, 1222, 10833, 1412, 307, 510, 13, 814, 366, 33535, 13, 51228, 51328, 407, 286, 767, 6453, 281, 2524, 32181, 13, 400, 586, 452, 20828, 1639, 575, 572, 17076, 13602, 577, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.1005572188984264, "compression_ratio": 1.6409090909090909, "no_speech_prob": 4.783576059708139e-06}, {"id": 303, "seek": 210104, "start": 2120.32, "end": 2129.7599999999998, "text": " So I actually managed to reach convergence. And now my discriminator has no whatsoever clue how", "tokens": [50364, 20828, 1639, 575, 572, 1558, 437, 281, 360, 13, 1779, 13, 407, 700, 295, 439, 11, 291, 434, 516, 281, 483, 3346, 11665, 7833, 50700, 50700, 445, 570, 291, 1194, 291, 9652, 3004, 13, 492, 767, 41881, 13, 1779, 13, 759, 291, 519, 466, 300, 11, 51020, 51020, 452, 2074, 1412, 307, 510, 13, 1222, 10833, 1412, 307, 510, 13, 814, 366, 33535, 13, 51228, 51328, 407, 286, 767, 6453, 281, 2524, 32181, 13, 400, 586, 452, 20828, 1639, 575, 572, 17076, 13602, 577, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.1005572188984264, "compression_ratio": 1.6409090909090909, "no_speech_prob": 4.783576059708139e-06}, {"id": 304, "seek": 212976, "start": 2129.76, "end": 2142.4, "text": " to split these things apart. So we don't converge. Or when we converge, we get issues. Right.", "tokens": [50364, 281, 7472, 613, 721, 4936, 13, 407, 321, 500, 380, 41881, 13, 1610, 562, 321, 41881, 11, 321, 483, 2663, 13, 1779, 13, 50996, 51180, 440, 20828, 1639, 11, 286, 519, 264, 20828, 1639, 445, 5112, 4936, 732, 5359, 13, 1042, 11, 264, 20828, 1639, 51400, 51400, 2644, 980, 4936, 264, 732, 5359, 570, 613, 15743, 366, 572, 544, 12005, 13, 1779, 13, 51736, 51776], "temperature": 0.0, "avg_logprob": -0.13633964310831098, "compression_ratio": 1.6686746987951808, "no_speech_prob": 1.2802142919099424e-05}, {"id": 305, "seek": 212976, "start": 2146.0800000000004, "end": 2150.48, "text": " The discriminator, I think the discriminator just tells apart two classes. Well, the discriminator", "tokens": [50364, 281, 7472, 613, 721, 4936, 13, 407, 321, 500, 380, 41881, 13, 1610, 562, 321, 41881, 11, 321, 483, 2663, 13, 1779, 13, 50996, 51180, 440, 20828, 1639, 11, 286, 519, 264, 20828, 1639, 445, 5112, 4936, 732, 5359, 13, 1042, 11, 264, 20828, 1639, 51400, 51400, 2644, 980, 4936, 264, 732, 5359, 570, 613, 15743, 366, 572, 544, 12005, 13, 1779, 13, 51736, 51776], "temperature": 0.0, "avg_logprob": -0.13633964310831098, "compression_ratio": 1.6686746987951808, "no_speech_prob": 1.2802142919099424e-05}, {"id": 306, "seek": 212976, "start": 2150.48, "end": 2157.2000000000003, "text": " cannot tell apart the two classes because these inputs are no more separated. Right.", "tokens": [50364, 281, 7472, 613, 721, 4936, 13, 407, 321, 500, 380, 41881, 13, 1610, 562, 321, 41881, 11, 321, 483, 2663, 13, 1779, 13, 50996, 51180, 440, 20828, 1639, 11, 286, 519, 264, 20828, 1639, 445, 5112, 4936, 732, 5359, 13, 1042, 11, 264, 20828, 1639, 51400, 51400, 2644, 980, 4936, 264, 732, 5359, 570, 613, 15743, 366, 572, 544, 12005, 13, 1779, 13, 51736, 51776], "temperature": 0.0, "avg_logprob": -0.13633964310831098, "compression_ratio": 1.6686746987951808, "no_speech_prob": 1.2802142919099424e-05}, {"id": 307, "seek": 215720, "start": 2157.2, "end": 2162.16, "text": " They are going to be like, if you actually manage to get the generator to perform very,", "tokens": [50364, 814, 366, 516, 281, 312, 411, 11, 498, 291, 767, 3067, 281, 483, 264, 19265, 281, 2042, 588, 11, 50612, 50612, 588, 665, 10938, 11, 550, 613, 665, 10938, 366, 11, 291, 2644, 980, 552, 4936, 490, 264, 3539, 957, 50916, 50916, 10938, 13, 1779, 13, 400, 586, 264, 20828, 1639, 575, 572, 17076, 13602, 466, 577, 281, 1936, 980, 552, 51348, 51348, 4936, 13, 407, 5699, 264, 19265, 1985, 11, 264, 20828, 1639, 486, 406, 589, 13, 1012, 1481, 307, 300, 13, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.16490867220122238, "compression_ratio": 1.7630331753554502, "no_speech_prob": 4.221054496156285e-06}, {"id": 308, "seek": 215720, "start": 2162.16, "end": 2168.24, "text": " very good samples, then these good samples are, you cannot tell them apart from the actual real", "tokens": [50364, 814, 366, 516, 281, 312, 411, 11, 498, 291, 767, 3067, 281, 483, 264, 19265, 281, 2042, 588, 11, 50612, 50612, 588, 665, 10938, 11, 550, 613, 665, 10938, 366, 11, 291, 2644, 980, 552, 4936, 490, 264, 3539, 957, 50916, 50916, 10938, 13, 1779, 13, 400, 586, 264, 20828, 1639, 575, 572, 17076, 13602, 466, 577, 281, 1936, 980, 552, 51348, 51348, 4936, 13, 407, 5699, 264, 19265, 1985, 11, 264, 20828, 1639, 486, 406, 589, 13, 1012, 1481, 307, 300, 13, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.16490867220122238, "compression_ratio": 1.7630331753554502, "no_speech_prob": 4.221054496156285e-06}, {"id": 309, "seek": 215720, "start": 2168.24, "end": 2176.8799999999997, "text": " samples. Right. And now the discriminator has no whatsoever clue about how to basically tell them", "tokens": [50364, 814, 366, 516, 281, 312, 411, 11, 498, 291, 767, 3067, 281, 483, 264, 19265, 281, 2042, 588, 11, 50612, 50612, 588, 665, 10938, 11, 550, 613, 665, 10938, 366, 11, 291, 2644, 980, 552, 4936, 490, 264, 3539, 957, 50916, 50916, 10938, 13, 1779, 13, 400, 586, 264, 20828, 1639, 575, 572, 17076, 13602, 466, 577, 281, 1936, 980, 552, 51348, 51348, 4936, 13, 407, 5699, 264, 19265, 1985, 11, 264, 20828, 1639, 486, 406, 589, 13, 1012, 1481, 307, 300, 13, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.16490867220122238, "compression_ratio": 1.7630331753554502, "no_speech_prob": 4.221054496156285e-06}, {"id": 310, "seek": 215720, "start": 2176.8799999999997, "end": 2184.3199999999997, "text": " apart. So whenever the generator works, the discriminator will not work. How nice is that.", "tokens": [50364, 814, 366, 516, 281, 312, 411, 11, 498, 291, 767, 3067, 281, 483, 264, 19265, 281, 2042, 588, 11, 50612, 50612, 588, 665, 10938, 11, 550, 613, 665, 10938, 366, 11, 291, 2644, 980, 552, 4936, 490, 264, 3539, 957, 50916, 50916, 10938, 13, 1779, 13, 400, 586, 264, 20828, 1639, 575, 572, 17076, 13602, 466, 577, 281, 1936, 980, 552, 51348, 51348, 4936, 13, 407, 5699, 264, 19265, 1985, 11, 264, 20828, 1639, 486, 406, 589, 13, 1012, 1481, 307, 300, 13, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.16490867220122238, "compression_ratio": 1.7630331753554502, "no_speech_prob": 4.221054496156285e-06}, {"id": 311, "seek": 218432, "start": 2184.32, "end": 2193.2000000000003, "text": " Okay. One other problem. Let's say again, you have fake data here, true data over here,", "tokens": [50364, 1033, 13, 1485, 661, 1154, 13, 961, 311, 584, 797, 11, 291, 362, 7592, 1412, 510, 11, 2074, 1412, 670, 510, 11, 50808, 50808, 293, 586, 291, 362, 257, 2176, 11, 2243, 11, 3476, 20828, 1639, 1270, 300, 510, 307, 3122, 4018, 13, 51196, 51196, 400, 550, 510, 307, 3122, 472, 13, 1033, 13, 407, 291, 362, 411, 257, 1936, 411, 257, 1823, 2445, 13, 509, 51452, 51452], "temperature": 0.0, "avg_logprob": -0.162968989780971, "compression_ratio": 1.5363128491620113, "no_speech_prob": 7.766052476654295e-06}, {"id": 312, "seek": 218432, "start": 2193.2000000000003, "end": 2200.96, "text": " and now you have a perfect, amazing, awesome discriminator such that here is absolutely zero.", "tokens": [50364, 1033, 13, 1485, 661, 1154, 13, 961, 311, 584, 797, 11, 291, 362, 7592, 1412, 510, 11, 2074, 1412, 670, 510, 11, 50808, 50808, 293, 586, 291, 362, 257, 2176, 11, 2243, 11, 3476, 20828, 1639, 1270, 300, 510, 307, 3122, 4018, 13, 51196, 51196, 400, 550, 510, 307, 3122, 472, 13, 1033, 13, 407, 291, 362, 411, 257, 1936, 411, 257, 1823, 2445, 13, 509, 51452, 51452], "temperature": 0.0, "avg_logprob": -0.162968989780971, "compression_ratio": 1.5363128491620113, "no_speech_prob": 7.766052476654295e-06}, {"id": 313, "seek": 218432, "start": 2200.96, "end": 2206.0800000000004, "text": " And then here is absolutely one. Okay. So you have like a basically like a step function. You", "tokens": [50364, 1033, 13, 1485, 661, 1154, 13, 961, 311, 584, 797, 11, 291, 362, 7592, 1412, 510, 11, 2074, 1412, 670, 510, 11, 50808, 50808, 293, 586, 291, 362, 257, 2176, 11, 2243, 11, 3476, 20828, 1639, 1270, 300, 510, 307, 3122, 4018, 13, 51196, 51196, 400, 550, 510, 307, 3122, 472, 13, 1033, 13, 407, 291, 362, 411, 257, 1936, 411, 257, 1823, 2445, 13, 509, 51452, 51452], "temperature": 0.0, "avg_logprob": -0.162968989780971, "compression_ratio": 1.5363128491620113, "no_speech_prob": 7.766052476654295e-06}, {"id": 314, "seek": 220608, "start": 2206.08, "end": 2214.7999999999997, "text": " don't have a sigmoid. What's going to be now the gradient. It's saturated, right. Or it's zero or", "tokens": [50364, 500, 380, 362, 257, 4556, 3280, 327, 13, 708, 311, 516, 281, 312, 586, 264, 16235, 13, 467, 311, 25408, 11, 558, 13, 1610, 309, 311, 4018, 420, 50800, 50800, 309, 311, 472, 13, 821, 307, 572, 544, 16235, 13, 1981, 2793, 486, 1128, 1286, 13, 1779, 13, 407, 264, 12110, 300, 286, 390, 51136, 51136, 4099, 291, 949, 300, 390, 36594, 613, 10833, 1412, 807, 3911, 264, 3537, 12866, 51464, 51464, 390, 1936, 264, 2771, 2448, 300, 286, 1866, 264, 16235, 295, 264, 2572, 295, 264, 5598, 295, 264, 51780, 51812], "temperature": 0.0, "avg_logprob": -0.12347043828761324, "compression_ratio": 1.7104072398190044, "no_speech_prob": 4.494880158745218e-06}, {"id": 315, "seek": 220608, "start": 2214.7999999999997, "end": 2221.52, "text": " it's one. There is no more gradient. These points will never move. Right. So the gravity that I was", "tokens": [50364, 500, 380, 362, 257, 4556, 3280, 327, 13, 708, 311, 516, 281, 312, 586, 264, 16235, 13, 467, 311, 25408, 11, 558, 13, 1610, 309, 311, 4018, 420, 50800, 50800, 309, 311, 472, 13, 821, 307, 572, 544, 16235, 13, 1981, 2793, 486, 1128, 1286, 13, 1779, 13, 407, 264, 12110, 300, 286, 390, 51136, 51136, 4099, 291, 949, 300, 390, 36594, 613, 10833, 1412, 807, 3911, 264, 3537, 12866, 51464, 51464, 390, 1936, 264, 2771, 2448, 300, 286, 1866, 264, 16235, 295, 264, 2572, 295, 264, 5598, 295, 264, 51780, 51812], "temperature": 0.0, "avg_logprob": -0.12347043828761324, "compression_ratio": 1.7104072398190044, "no_speech_prob": 4.494880158745218e-06}, {"id": 316, "seek": 220608, "start": 2221.52, "end": 2228.08, "text": " showing you before that was attracting these generated data through onto the decision boundary", "tokens": [50364, 500, 380, 362, 257, 4556, 3280, 327, 13, 708, 311, 516, 281, 312, 586, 264, 16235, 13, 467, 311, 25408, 11, 558, 13, 1610, 309, 311, 4018, 420, 50800, 50800, 309, 311, 472, 13, 821, 307, 572, 544, 16235, 13, 1981, 2793, 486, 1128, 1286, 13, 1779, 13, 407, 264, 12110, 300, 286, 390, 51136, 51136, 4099, 291, 949, 300, 390, 36594, 613, 10833, 1412, 807, 3911, 264, 3537, 12866, 51464, 51464, 390, 1936, 264, 2771, 2448, 300, 286, 1866, 264, 16235, 295, 264, 2572, 295, 264, 5598, 295, 264, 51780, 51812], "temperature": 0.0, "avg_logprob": -0.12347043828761324, "compression_ratio": 1.7104072398190044, "no_speech_prob": 4.494880158745218e-06}, {"id": 317, "seek": 220608, "start": 2228.08, "end": 2234.4, "text": " was basically the gradients that I saw the gradient of the final of the output of the", "tokens": [50364, 500, 380, 362, 257, 4556, 3280, 327, 13, 708, 311, 516, 281, 312, 586, 264, 16235, 13, 467, 311, 25408, 11, 558, 13, 1610, 309, 311, 4018, 420, 50800, 50800, 309, 311, 472, 13, 821, 307, 572, 544, 16235, 13, 1981, 2793, 486, 1128, 1286, 13, 1779, 13, 407, 264, 12110, 300, 286, 390, 51136, 51136, 4099, 291, 949, 300, 390, 36594, 613, 10833, 1412, 807, 3911, 264, 3537, 12866, 51464, 51464, 390, 1936, 264, 2771, 2448, 300, 286, 1866, 264, 16235, 295, 264, 2572, 295, 264, 5598, 295, 264, 51780, 51812], "temperature": 0.0, "avg_logprob": -0.12347043828761324, "compression_ratio": 1.7104072398190044, "no_speech_prob": 4.494880158745218e-06}, {"id": 318, "seek": 223440, "start": 2234.4, "end": 2241.04, "text": " discriminator or the cost network with respect to the, you know, samples generated by the generator.", "tokens": [50364, 20828, 1639, 420, 264, 2063, 3209, 365, 3104, 281, 264, 11, 291, 458, 11, 10938, 10833, 538, 264, 19265, 13, 50696, 50696, 1779, 13, 583, 586, 498, 613, 20828, 1639, 575, 257, 2176, 11, 307, 257, 2176, 20828, 1639, 11, 4018, 510, 11, 51020, 51020, 472, 510, 11, 731, 11, 309, 311, 2584, 4962, 11, 558, 13, 759, 309, 311, 411, 300, 11, 456, 307, 572, 17076, 51272, 51372, 16235, 510, 13, 1779, 13, 400, 4412, 498, 291, 434, 670, 510, 11, 370, 718, 311, 584, 321, 362, 1412, 294, 472, 11, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.22410476207733154, "compression_ratio": 1.6972477064220184, "no_speech_prob": 2.769231514321291e-06}, {"id": 319, "seek": 223440, "start": 2241.04, "end": 2247.52, "text": " Right. But now if these discriminator has a perfect, is a perfect discriminator, zero here,", "tokens": [50364, 20828, 1639, 420, 264, 2063, 3209, 365, 3104, 281, 264, 11, 291, 458, 11, 10938, 10833, 538, 264, 19265, 13, 50696, 50696, 1779, 13, 583, 586, 498, 613, 20828, 1639, 575, 257, 2176, 11, 307, 257, 2176, 20828, 1639, 11, 4018, 510, 11, 51020, 51020, 472, 510, 11, 731, 11, 309, 311, 2584, 4962, 11, 558, 13, 759, 309, 311, 411, 300, 11, 456, 307, 572, 17076, 51272, 51372, 16235, 510, 13, 1779, 13, 400, 4412, 498, 291, 434, 670, 510, 11, 370, 718, 311, 584, 321, 362, 1412, 294, 472, 11, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.22410476207733154, "compression_ratio": 1.6972477064220184, "no_speech_prob": 2.769231514321291e-06}, {"id": 320, "seek": 223440, "start": 2247.52, "end": 2252.56, "text": " one here, well, it's completely flat, right. If it's like that, there is no whatsoever", "tokens": [50364, 20828, 1639, 420, 264, 2063, 3209, 365, 3104, 281, 264, 11, 291, 458, 11, 10938, 10833, 538, 264, 19265, 13, 50696, 50696, 1779, 13, 583, 586, 498, 613, 20828, 1639, 575, 257, 2176, 11, 307, 257, 2176, 20828, 1639, 11, 4018, 510, 11, 51020, 51020, 472, 510, 11, 731, 11, 309, 311, 2584, 4962, 11, 558, 13, 759, 309, 311, 411, 300, 11, 456, 307, 572, 17076, 51272, 51372, 16235, 510, 13, 1779, 13, 400, 4412, 498, 291, 434, 670, 510, 11, 370, 718, 311, 584, 321, 362, 1412, 294, 472, 11, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.22410476207733154, "compression_ratio": 1.6972477064220184, "no_speech_prob": 2.769231514321291e-06}, {"id": 321, "seek": 223440, "start": 2254.56, "end": 2260.0, "text": " gradient here. Right. And therefore if you're over here, so let's say we have data in one,", "tokens": [50364, 20828, 1639, 420, 264, 2063, 3209, 365, 3104, 281, 264, 11, 291, 458, 11, 10938, 10833, 538, 264, 19265, 13, 50696, 50696, 1779, 13, 583, 586, 498, 613, 20828, 1639, 575, 257, 2176, 11, 307, 257, 2176, 20828, 1639, 11, 4018, 510, 11, 51020, 51020, 472, 510, 11, 731, 11, 309, 311, 2584, 4962, 11, 558, 13, 759, 309, 311, 411, 300, 11, 456, 307, 572, 17076, 51272, 51372, 16235, 510, 13, 1779, 13, 400, 4412, 498, 291, 434, 670, 510, 11, 370, 718, 311, 584, 321, 362, 1412, 294, 472, 11, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.22410476207733154, "compression_ratio": 1.6972477064220184, "no_speech_prob": 2.769231514321291e-06}, {"id": 322, "seek": 226000, "start": 2260.0, "end": 2265.04, "text": " one X, right. In one, one dimension, you have zero, zero, zero. Then you have one, one, one, one, one.", "tokens": [50364, 472, 1783, 11, 558, 13, 682, 472, 11, 472, 10139, 11, 291, 362, 4018, 11, 4018, 11, 4018, 13, 1396, 291, 362, 472, 11, 472, 11, 472, 11, 472, 11, 472, 13, 50616, 50664, 583, 550, 498, 456, 307, 445, 11, 291, 458, 11, 456, 307, 572, 16235, 11, 613, 2793, 486, 1128, 458, 436, 632, 50884, 50884, 281, 352, 294, 300, 3513, 13, 814, 486, 536, 11, 1954, 11, 321, 366, 1578, 1074, 13, 492, 362, 257, 1578, 2158, 11, 457, 550, 51216, 51244, 321, 500, 380, 458, 294, 597, 3513, 281, 1286, 570, 456, 307, 572, 17076, 3513, 13, 51424, 51424, 440, 16235, 307, 4018, 13, 467, 311, 257, 4962, 4458, 13, 1779, 13, 407, 341, 307, 257, 588, 955, 2734, 11, 558, 13, 407, 5699, 321, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.12897596144138423, "compression_ratio": 1.876984126984127, "no_speech_prob": 9.817330465011764e-06}, {"id": 323, "seek": 226000, "start": 2266.0, "end": 2270.4, "text": " But then if there is just, you know, there is no gradient, these points will never know they had", "tokens": [50364, 472, 1783, 11, 558, 13, 682, 472, 11, 472, 10139, 11, 291, 362, 4018, 11, 4018, 11, 4018, 13, 1396, 291, 362, 472, 11, 472, 11, 472, 11, 472, 11, 472, 13, 50616, 50664, 583, 550, 498, 456, 307, 445, 11, 291, 458, 11, 456, 307, 572, 16235, 11, 613, 2793, 486, 1128, 458, 436, 632, 50884, 50884, 281, 352, 294, 300, 3513, 13, 814, 486, 536, 11, 1954, 11, 321, 366, 1578, 1074, 13, 492, 362, 257, 1578, 2158, 11, 457, 550, 51216, 51244, 321, 500, 380, 458, 294, 597, 3513, 281, 1286, 570, 456, 307, 572, 17076, 3513, 13, 51424, 51424, 440, 16235, 307, 4018, 13, 467, 311, 257, 4962, 4458, 13, 1779, 13, 407, 341, 307, 257, 588, 955, 2734, 11, 558, 13, 407, 5699, 321, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.12897596144138423, "compression_ratio": 1.876984126984127, "no_speech_prob": 9.817330465011764e-06}, {"id": 324, "seek": 226000, "start": 2270.4, "end": 2277.04, "text": " to go in that direction. They will see, oh, we are bad guys. We have a bad value, but then", "tokens": [50364, 472, 1783, 11, 558, 13, 682, 472, 11, 472, 10139, 11, 291, 362, 4018, 11, 4018, 11, 4018, 13, 1396, 291, 362, 472, 11, 472, 11, 472, 11, 472, 11, 472, 13, 50616, 50664, 583, 550, 498, 456, 307, 445, 11, 291, 458, 11, 456, 307, 572, 16235, 11, 613, 2793, 486, 1128, 458, 436, 632, 50884, 50884, 281, 352, 294, 300, 3513, 13, 814, 486, 536, 11, 1954, 11, 321, 366, 1578, 1074, 13, 492, 362, 257, 1578, 2158, 11, 457, 550, 51216, 51244, 321, 500, 380, 458, 294, 597, 3513, 281, 1286, 570, 456, 307, 572, 17076, 3513, 13, 51424, 51424, 440, 16235, 307, 4018, 13, 467, 311, 257, 4962, 4458, 13, 1779, 13, 407, 341, 307, 257, 588, 955, 2734, 11, 558, 13, 407, 5699, 321, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.12897596144138423, "compression_ratio": 1.876984126984127, "no_speech_prob": 9.817330465011764e-06}, {"id": 325, "seek": 226000, "start": 2277.6, "end": 2281.2, "text": " we don't know in which direction to move because there is no whatsoever direction.", "tokens": [50364, 472, 1783, 11, 558, 13, 682, 472, 11, 472, 10139, 11, 291, 362, 4018, 11, 4018, 11, 4018, 13, 1396, 291, 362, 472, 11, 472, 11, 472, 11, 472, 11, 472, 13, 50616, 50664, 583, 550, 498, 456, 307, 445, 11, 291, 458, 11, 456, 307, 572, 16235, 11, 613, 2793, 486, 1128, 458, 436, 632, 50884, 50884, 281, 352, 294, 300, 3513, 13, 814, 486, 536, 11, 1954, 11, 321, 366, 1578, 1074, 13, 492, 362, 257, 1578, 2158, 11, 457, 550, 51216, 51244, 321, 500, 380, 458, 294, 597, 3513, 281, 1286, 570, 456, 307, 572, 17076, 3513, 13, 51424, 51424, 440, 16235, 307, 4018, 13, 467, 311, 257, 4962, 4458, 13, 1779, 13, 407, 341, 307, 257, 588, 955, 2734, 11, 558, 13, 407, 5699, 321, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.12897596144138423, "compression_ratio": 1.876984126984127, "no_speech_prob": 9.817330465011764e-06}, {"id": 326, "seek": 226000, "start": 2281.2, "end": 2287.2, "text": " The gradient is zero. It's a flat region. Right. So this is a very big issue, right. So whenever we", "tokens": [50364, 472, 1783, 11, 558, 13, 682, 472, 11, 472, 10139, 11, 291, 362, 4018, 11, 4018, 11, 4018, 13, 1396, 291, 362, 472, 11, 472, 11, 472, 11, 472, 11, 472, 13, 50616, 50664, 583, 550, 498, 456, 307, 445, 11, 291, 458, 11, 456, 307, 572, 16235, 11, 613, 2793, 486, 1128, 458, 436, 632, 50884, 50884, 281, 352, 294, 300, 3513, 13, 814, 486, 536, 11, 1954, 11, 321, 366, 1578, 1074, 13, 492, 362, 257, 1578, 2158, 11, 457, 550, 51216, 51244, 321, 500, 380, 458, 294, 597, 3513, 281, 1286, 570, 456, 307, 572, 17076, 3513, 13, 51424, 51424, 440, 16235, 307, 4018, 13, 467, 311, 257, 4962, 4458, 13, 1779, 13, 407, 341, 307, 257, 588, 955, 2734, 11, 558, 13, 407, 5699, 321, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.12897596144138423, "compression_ratio": 1.876984126984127, "no_speech_prob": 9.817330465011764e-06}, {"id": 327, "seek": 228720, "start": 2287.2, "end": 2295.68, "text": " train this generator, the cellular network, you want to make sure that this cost gradually increases", "tokens": [50364, 3847, 341, 19265, 11, 264, 29267, 3209, 11, 291, 528, 281, 652, 988, 300, 341, 2063, 13145, 8637, 50788, 50788, 382, 291, 1286, 1314, 490, 428, 4458, 295, 264, 2074, 1412, 13, 1033, 13, 9653, 300, 498, 456, 307, 257, 5508, 420, 309, 311, 411, 51160, 51160, 257, 11, 291, 458, 11, 257, 42432, 551, 11, 558, 13, 407, 498, 309, 5965, 516, 493, 11, 493, 11, 493, 11, 493, 11, 493, 11, 291, 1009, 458, 294, 597, 51380, 51380, 3513, 281, 2100, 760, 294, 1668, 281, 8881, 412, 264, 4914, 689, 428, 2074, 1412, 307, 13, 1033, 13, 51836], "temperature": 0.0, "avg_logprob": -0.11957292930752624, "compression_ratio": 1.672340425531915, "no_speech_prob": 7.76457727624802e-06}, {"id": 328, "seek": 228720, "start": 2295.68, "end": 2303.12, "text": " as you move away from your region of the true data. Okay. Such that if there is a smooth or it's like", "tokens": [50364, 3847, 341, 19265, 11, 264, 29267, 3209, 11, 291, 528, 281, 652, 988, 300, 341, 2063, 13145, 8637, 50788, 50788, 382, 291, 1286, 1314, 490, 428, 4458, 295, 264, 2074, 1412, 13, 1033, 13, 9653, 300, 498, 456, 307, 257, 5508, 420, 309, 311, 411, 51160, 51160, 257, 11, 291, 458, 11, 257, 42432, 551, 11, 558, 13, 407, 498, 309, 5965, 516, 493, 11, 493, 11, 493, 11, 493, 11, 493, 11, 291, 1009, 458, 294, 597, 51380, 51380, 3513, 281, 2100, 760, 294, 1668, 281, 8881, 412, 264, 4914, 689, 428, 2074, 1412, 307, 13, 1033, 13, 51836], "temperature": 0.0, "avg_logprob": -0.11957292930752624, "compression_ratio": 1.672340425531915, "no_speech_prob": 7.76457727624802e-06}, {"id": 329, "seek": 228720, "start": 2303.12, "end": 2307.52, "text": " a, you know, a convex thing, right. So if it keeps going up, up, up, up, up, you always know in which", "tokens": [50364, 3847, 341, 19265, 11, 264, 29267, 3209, 11, 291, 528, 281, 652, 988, 300, 341, 2063, 13145, 8637, 50788, 50788, 382, 291, 1286, 1314, 490, 428, 4458, 295, 264, 2074, 1412, 13, 1033, 13, 9653, 300, 498, 456, 307, 257, 5508, 420, 309, 311, 411, 51160, 51160, 257, 11, 291, 458, 11, 257, 42432, 551, 11, 558, 13, 407, 498, 309, 5965, 516, 493, 11, 493, 11, 493, 11, 493, 11, 493, 11, 291, 1009, 458, 294, 597, 51380, 51380, 3513, 281, 2100, 760, 294, 1668, 281, 8881, 412, 264, 4914, 689, 428, 2074, 1412, 307, 13, 1033, 13, 51836], "temperature": 0.0, "avg_logprob": -0.11957292930752624, "compression_ratio": 1.672340425531915, "no_speech_prob": 7.76457727624802e-06}, {"id": 330, "seek": 230752, "start": 2307.52, "end": 2318.4, "text": " direction to fall down in order to arrive at the location where your true data is. Okay. And my", "tokens": [50364, 3513, 281, 2100, 760, 294, 1668, 281, 8881, 412, 264, 4914, 689, 428, 2074, 1412, 307, 13, 1033, 13, 400, 452, 50908, 50908, 3329, 1280, 5965, 26802, 17001, 13, 286, 478, 411, 6246, 341, 4611, 766, 13, 821, 291, 352, 13, 1119, 309, 1850, 370, 1400, 30, 51344, 51428], "temperature": 0.0, "avg_logprob": -0.09080014509313247, "compression_ratio": 1.3286713286713288, "no_speech_prob": 5.971730570308864e-05}, {"id": 331, "seek": 230752, "start": 2318.4, "end": 2327.12, "text": " Google home keeps rebooting. I'm like turning this shit off. There you go. Is it clear so far?", "tokens": [50364, 3513, 281, 2100, 760, 294, 1668, 281, 8881, 412, 264, 4914, 689, 428, 2074, 1412, 307, 13, 1033, 13, 400, 452, 50908, 50908, 3329, 1280, 5965, 26802, 17001, 13, 286, 478, 411, 6246, 341, 4611, 766, 13, 821, 291, 352, 13, 1119, 309, 1850, 370, 1400, 30, 51344, 51428], "temperature": 0.0, "avg_logprob": -0.09080014509313247, "compression_ratio": 1.3286713286713288, "no_speech_prob": 5.971730570308864e-05}, {"id": 332, "seek": 232712, "start": 2327.12, "end": 2341.2799999999997, "text": " Yeah. Yeah. Yeah. One final issue was that if we get a generator, which gets every point here", "tokens": [50364, 865, 13, 865, 13, 865, 13, 1485, 2572, 2734, 390, 300, 498, 321, 483, 257, 19265, 11, 597, 2170, 633, 935, 510, 51072, 51128, 33318, 666, 341, 935, 670, 510, 11, 51344, 51540, 291, 458, 11, 439, 17443, 366, 4018, 13, 509, 362, 264, 2572, 12577, 312, 2293, 341, 2158, 670, 510, 13, 51764, 51796], "temperature": 0.0, "avg_logprob": -0.16031531283729955, "compression_ratio": 1.4896551724137932, "no_speech_prob": 1.380311823595548e-05}, {"id": 333, "seek": 232712, "start": 2342.4, "end": 2346.72, "text": " mapped into this point over here,", "tokens": [50364, 865, 13, 865, 13, 865, 13, 1485, 2572, 2734, 390, 300, 498, 321, 483, 257, 19265, 11, 597, 2170, 633, 935, 510, 51072, 51128, 33318, 666, 341, 935, 670, 510, 11, 51344, 51540, 291, 458, 11, 439, 17443, 366, 4018, 13, 509, 362, 264, 2572, 12577, 312, 2293, 341, 2158, 670, 510, 13, 51764, 51796], "temperature": 0.0, "avg_logprob": -0.16031531283729955, "compression_ratio": 1.4896551724137932, "no_speech_prob": 1.380311823595548e-05}, {"id": 334, "seek": 232712, "start": 2350.64, "end": 2355.12, "text": " you know, all weights are zero. You have the final bias be exactly this value over here.", "tokens": [50364, 865, 13, 865, 13, 865, 13, 1485, 2572, 2734, 390, 300, 498, 321, 483, 257, 19265, 11, 597, 2170, 633, 935, 510, 51072, 51128, 33318, 666, 341, 935, 670, 510, 11, 51344, 51540, 291, 458, 11, 439, 17443, 366, 4018, 13, 509, 362, 264, 2572, 12577, 312, 2293, 341, 2158, 670, 510, 13, 51764, 51796], "temperature": 0.0, "avg_logprob": -0.16031531283729955, "compression_ratio": 1.4896551724137932, "no_speech_prob": 1.380311823595548e-05}, {"id": 335, "seek": 235512, "start": 2355.12, "end": 2361.3599999999997, "text": " Then that's finished because the discriminator or the cost function will say, you've done a very good", "tokens": [50364, 1396, 300, 311, 4335, 570, 264, 20828, 1639, 420, 264, 2063, 2445, 486, 584, 11, 291, 600, 1096, 257, 588, 665, 50676, 50676, 1691, 293, 264, 19265, 584, 11, 23986, 13, 400, 550, 264, 19265, 445, 23930, 472, 3256, 11, 558, 13, 400, 341, 51076, 51076, 307, 1219, 4391, 15584, 13, 19948, 300, 439, 2793, 366, 33318, 666, 445, 472, 935, 293, 291, 393, 380, 51364, 51404, 360, 1340, 466, 309, 13, 407, 264, 3539, 1577, 1657, 307, 300, 498, 633, 935, 510, 2170, 33318, 281, 341, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.19743349263956259, "compression_ratio": 1.6523605150214593, "no_speech_prob": 7.761660526739433e-06}, {"id": 336, "seek": 235512, "start": 2361.3599999999997, "end": 2369.3599999999997, "text": " job and the generator say, yay. And then the generator just outputs one image, right. And this", "tokens": [50364, 1396, 300, 311, 4335, 570, 264, 20828, 1639, 420, 264, 2063, 2445, 486, 584, 11, 291, 600, 1096, 257, 588, 665, 50676, 50676, 1691, 293, 264, 19265, 584, 11, 23986, 13, 400, 550, 264, 19265, 445, 23930, 472, 3256, 11, 558, 13, 400, 341, 51076, 51076, 307, 1219, 4391, 15584, 13, 19948, 300, 439, 2793, 366, 33318, 666, 445, 472, 935, 293, 291, 393, 380, 51364, 51404, 360, 1340, 466, 309, 13, 407, 264, 3539, 1577, 1657, 307, 300, 498, 633, 935, 510, 2170, 33318, 281, 341, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.19743349263956259, "compression_ratio": 1.6523605150214593, "no_speech_prob": 7.761660526739433e-06}, {"id": 337, "seek": 235512, "start": 2369.3599999999997, "end": 2375.12, "text": " is called mode collapse. Meaning that all points are mapped into just one point and you can't", "tokens": [50364, 1396, 300, 311, 4335, 570, 264, 20828, 1639, 420, 264, 2063, 2445, 486, 584, 11, 291, 600, 1096, 257, 588, 665, 50676, 50676, 1691, 293, 264, 19265, 584, 11, 23986, 13, 400, 550, 264, 19265, 445, 23930, 472, 3256, 11, 558, 13, 400, 341, 51076, 51076, 307, 1219, 4391, 15584, 13, 19948, 300, 439, 2793, 366, 33318, 666, 445, 472, 935, 293, 291, 393, 380, 51364, 51404, 360, 1340, 466, 309, 13, 407, 264, 3539, 1577, 1657, 307, 300, 498, 633, 935, 510, 2170, 33318, 281, 341, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.19743349263956259, "compression_ratio": 1.6523605150214593, "no_speech_prob": 7.761660526739433e-06}, {"id": 338, "seek": 235512, "start": 2375.92, "end": 2384.48, "text": " do anything about it. So the actual full story is that if every point here gets mapped to this", "tokens": [50364, 1396, 300, 311, 4335, 570, 264, 20828, 1639, 420, 264, 2063, 2445, 486, 584, 11, 291, 600, 1096, 257, 588, 665, 50676, 50676, 1691, 293, 264, 19265, 584, 11, 23986, 13, 400, 550, 264, 19265, 445, 23930, 472, 3256, 11, 558, 13, 400, 341, 51076, 51076, 307, 1219, 4391, 15584, 13, 19948, 300, 439, 2793, 366, 33318, 666, 445, 472, 935, 293, 291, 393, 380, 51364, 51404, 360, 1340, 466, 309, 13, 407, 264, 3539, 1577, 1657, 307, 300, 498, 633, 935, 510, 2170, 33318, 281, 341, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.19743349263956259, "compression_ratio": 1.6523605150214593, "no_speech_prob": 7.761660526739433e-06}, {"id": 339, "seek": 238448, "start": 2384.48, "end": 2390.88, "text": " point here, then the discriminator will tell that, oh, this is a fake point, right. And therefore the", "tokens": [50364, 935, 510, 11, 550, 264, 20828, 1639, 486, 980, 300, 11, 1954, 11, 341, 307, 257, 7592, 935, 11, 558, 13, 400, 4412, 264, 50684, 50684, 19265, 486, 3679, 293, 321, 603, 584, 11, 341, 307, 264, 957, 5598, 11, 558, 13, 400, 550, 291, 3847, 264, 50936, 50936, 20828, 1639, 11, 20828, 1639, 584, 11, 1954, 11, 341, 307, 7592, 13, 1033, 13, 407, 264, 19265, 486, 584, 11, 341, 307, 264, 957, 51172, 51172, 472, 11, 558, 13, 1033, 13, 407, 291, 1936, 362, 257, 3209, 300, 307, 445, 11233, 807, 264, 10938, 293, 291, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.14267157564068786, "compression_ratio": 1.9898989898989898, "no_speech_prob": 1.5381974662886932e-05}, {"id": 340, "seek": 238448, "start": 2390.88, "end": 2395.92, "text": " generator will switch and we'll say, this is the real output, right. And then you train the", "tokens": [50364, 935, 510, 11, 550, 264, 20828, 1639, 486, 980, 300, 11, 1954, 11, 341, 307, 257, 7592, 935, 11, 558, 13, 400, 4412, 264, 50684, 50684, 19265, 486, 3679, 293, 321, 603, 584, 11, 341, 307, 264, 957, 5598, 11, 558, 13, 400, 550, 291, 3847, 264, 50936, 50936, 20828, 1639, 11, 20828, 1639, 584, 11, 1954, 11, 341, 307, 7592, 13, 1033, 13, 407, 264, 19265, 486, 584, 11, 341, 307, 264, 957, 51172, 51172, 472, 11, 558, 13, 1033, 13, 407, 291, 1936, 362, 257, 3209, 300, 307, 445, 11233, 807, 264, 10938, 293, 291, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.14267157564068786, "compression_ratio": 1.9898989898989898, "no_speech_prob": 1.5381974662886932e-05}, {"id": 341, "seek": 238448, "start": 2395.92, "end": 2400.64, "text": " discriminator, discriminator say, oh, this is fake. Okay. So the generator will say, this is the real", "tokens": [50364, 935, 510, 11, 550, 264, 20828, 1639, 486, 980, 300, 11, 1954, 11, 341, 307, 257, 7592, 935, 11, 558, 13, 400, 4412, 264, 50684, 50684, 19265, 486, 3679, 293, 321, 603, 584, 11, 341, 307, 264, 957, 5598, 11, 558, 13, 400, 550, 291, 3847, 264, 50936, 50936, 20828, 1639, 11, 20828, 1639, 584, 11, 1954, 11, 341, 307, 7592, 13, 1033, 13, 407, 264, 19265, 486, 584, 11, 341, 307, 264, 957, 51172, 51172, 472, 11, 558, 13, 1033, 13, 407, 291, 1936, 362, 257, 3209, 300, 307, 445, 11233, 807, 264, 10938, 293, 291, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.14267157564068786, "compression_ratio": 1.9898989898989898, "no_speech_prob": 1.5381974662886932e-05}, {"id": 342, "seek": 238448, "start": 2400.64, "end": 2408.72, "text": " one, right. Okay. So you basically have a network that is just jumping through the samples and you", "tokens": [50364, 935, 510, 11, 550, 264, 20828, 1639, 486, 980, 300, 11, 1954, 11, 341, 307, 257, 7592, 935, 11, 558, 13, 400, 4412, 264, 50684, 50684, 19265, 486, 3679, 293, 321, 603, 584, 11, 341, 307, 264, 957, 5598, 11, 558, 13, 400, 550, 291, 3847, 264, 50936, 50936, 20828, 1639, 11, 20828, 1639, 584, 11, 1954, 11, 341, 307, 7592, 13, 1033, 13, 407, 264, 19265, 486, 584, 11, 341, 307, 264, 957, 51172, 51172, 472, 11, 558, 13, 1033, 13, 407, 291, 1936, 362, 257, 3209, 300, 307, 445, 11233, 807, 264, 10938, 293, 291, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.14267157564068786, "compression_ratio": 1.9898989898989898, "no_speech_prob": 1.5381974662886932e-05}, {"id": 343, "seek": 240872, "start": 2408.72, "end": 2415.2799999999997, "text": " can't fix that unless you introduce some, you know, penalty for not having some kind of diversity in", "tokens": [50364, 393, 380, 3191, 300, 5969, 291, 5366, 512, 11, 291, 458, 11, 16263, 337, 406, 1419, 512, 733, 295, 8811, 294, 50692, 50692, 264, 5598, 295, 264, 19265, 13, 48188, 6952, 13, 1042, 11, 5699, 291, 362, 411, 25408, 20828, 3391, 51052, 51052, 293, 321, 500, 380, 411, 20828, 3391, 11, 321, 4382, 281, 1466, 341, 733, 295, 5508, 4470, 11, 2063, 11, 558, 13, 51348, 51384, 20863, 3209, 13, 20500, 15584, 13, 440, 551, 300, 286, 445, 7619, 558, 586, 11, 321, 445, 2100, 322, 472, 2685, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.209750859633736, "compression_ratio": 1.5793650793650793, "no_speech_prob": 7.515137895097723e-06}, {"id": 344, "seek": 240872, "start": 2415.2799999999997, "end": 2422.48, "text": " the output of the generator. Finished ingredients. Well, whenever you have like saturated discriminators", "tokens": [50364, 393, 380, 3191, 300, 5969, 291, 5366, 512, 11, 291, 458, 11, 16263, 337, 406, 1419, 512, 733, 295, 8811, 294, 50692, 50692, 264, 5598, 295, 264, 19265, 13, 48188, 6952, 13, 1042, 11, 5699, 291, 362, 411, 25408, 20828, 3391, 51052, 51052, 293, 321, 500, 380, 411, 20828, 3391, 11, 321, 4382, 281, 1466, 341, 733, 295, 5508, 4470, 11, 2063, 11, 558, 13, 51348, 51384, 20863, 3209, 13, 20500, 15584, 13, 440, 551, 300, 286, 445, 7619, 558, 586, 11, 321, 445, 2100, 322, 472, 2685, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.209750859633736, "compression_ratio": 1.5793650793650793, "no_speech_prob": 7.515137895097723e-06}, {"id": 345, "seek": 240872, "start": 2422.48, "end": 2428.3999999999996, "text": " and we don't like discriminators, we prefer to learn this kind of smooth loss, cost, right.", "tokens": [50364, 393, 380, 3191, 300, 5969, 291, 5366, 512, 11, 291, 458, 11, 16263, 337, 406, 1419, 512, 733, 295, 8811, 294, 50692, 50692, 264, 5598, 295, 264, 19265, 13, 48188, 6952, 13, 1042, 11, 5699, 291, 362, 411, 25408, 20828, 3391, 51052, 51052, 293, 321, 500, 380, 411, 20828, 3391, 11, 321, 4382, 281, 1466, 341, 733, 295, 5508, 4470, 11, 2063, 11, 558, 13, 51348, 51384, 20863, 3209, 13, 20500, 15584, 13, 440, 551, 300, 286, 445, 7619, 558, 586, 11, 321, 445, 2100, 322, 472, 2685, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.209750859633736, "compression_ratio": 1.5793650793650793, "no_speech_prob": 7.515137895097723e-06}, {"id": 346, "seek": 240872, "start": 2429.12, "end": 2435.12, "text": " Cost network. Mode collapse. The thing that I just described right now, we just fall on one specific", "tokens": [50364, 393, 380, 3191, 300, 5969, 291, 5366, 512, 11, 291, 458, 11, 16263, 337, 406, 1419, 512, 733, 295, 8811, 294, 50692, 50692, 264, 5598, 295, 264, 19265, 13, 48188, 6952, 13, 1042, 11, 5699, 291, 362, 411, 25408, 20828, 3391, 51052, 51052, 293, 321, 500, 380, 411, 20828, 3391, 11, 321, 4382, 281, 1466, 341, 733, 295, 5508, 4470, 11, 2063, 11, 558, 13, 51348, 51384, 20863, 3209, 13, 20500, 15584, 13, 440, 551, 300, 286, 445, 7619, 558, 586, 11, 321, 445, 2100, 322, 472, 2685, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.209750859633736, "compression_ratio": 1.5793650793650793, "no_speech_prob": 7.515137895097723e-06}, {"id": 347, "seek": 243512, "start": 2435.12, "end": 2442.3199999999997, "text": " point. Unstable convergence. Yeah. And the point is that whenever you get a very good generator,", "tokens": [50364, 935, 13, 1156, 372, 712, 32181, 13, 865, 13, 400, 264, 935, 307, 300, 5699, 291, 483, 257, 588, 665, 19265, 11, 50724, 50800, 291, 458, 11, 264, 20828, 1639, 486, 362, 572, 1558, 437, 311, 516, 322, 13, 509, 815, 362, 411, 257, 588, 955, 11, 51068, 51104, 257, 588, 955, 4470, 570, 291, 815, 483, 11, 291, 458, 11, 341, 935, 820, 312, 20627, 382, 341, 472, 13, 51320, 51364, 7156, 309, 311, 2584, 20627, 382, 746, 1646, 13, 509, 483, 512, 588, 11, 588, 2416, 16235, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.14241454956379343, "compression_ratio": 1.6816143497757847, "no_speech_prob": 6.072058386052959e-05}, {"id": 348, "seek": 243512, "start": 2443.8399999999997, "end": 2449.2, "text": " you know, the discriminator will have no idea what's going on. You may have like a very big,", "tokens": [50364, 935, 13, 1156, 372, 712, 32181, 13, 865, 13, 400, 264, 935, 307, 300, 5699, 291, 483, 257, 588, 665, 19265, 11, 50724, 50800, 291, 458, 11, 264, 20828, 1639, 486, 362, 572, 1558, 437, 311, 516, 322, 13, 509, 815, 362, 411, 257, 588, 955, 11, 51068, 51104, 257, 588, 955, 4470, 570, 291, 815, 483, 11, 291, 458, 11, 341, 935, 820, 312, 20627, 382, 341, 472, 13, 51320, 51364, 7156, 309, 311, 2584, 20627, 382, 746, 1646, 13, 509, 483, 512, 588, 11, 588, 2416, 16235, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.14241454956379343, "compression_ratio": 1.6816143497757847, "no_speech_prob": 6.072058386052959e-05}, {"id": 349, "seek": 243512, "start": 2449.92, "end": 2454.24, "text": " a very big loss because you may get, you know, this point should be classified as this one.", "tokens": [50364, 935, 13, 1156, 372, 712, 32181, 13, 865, 13, 400, 264, 935, 307, 300, 5699, 291, 483, 257, 588, 665, 19265, 11, 50724, 50800, 291, 458, 11, 264, 20828, 1639, 486, 362, 572, 1558, 437, 311, 516, 322, 13, 509, 815, 362, 411, 257, 588, 955, 11, 51068, 51104, 257, 588, 955, 4470, 570, 291, 815, 483, 11, 291, 458, 11, 341, 935, 820, 312, 20627, 382, 341, 472, 13, 51320, 51364, 7156, 309, 311, 2584, 20627, 382, 746, 1646, 13, 509, 483, 512, 588, 11, 588, 2416, 16235, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.14241454956379343, "compression_ratio": 1.6816143497757847, "no_speech_prob": 6.072058386052959e-05}, {"id": 350, "seek": 243512, "start": 2455.12, "end": 2459.3599999999997, "text": " Instead it's completely classified as something else. You get some very, very large gradient.", "tokens": [50364, 935, 13, 1156, 372, 712, 32181, 13, 865, 13, 400, 264, 935, 307, 300, 5699, 291, 483, 257, 588, 665, 19265, 11, 50724, 50800, 291, 458, 11, 264, 20828, 1639, 486, 362, 572, 1558, 437, 311, 516, 322, 13, 509, 815, 362, 411, 257, 588, 955, 11, 51068, 51104, 257, 588, 955, 4470, 570, 291, 815, 483, 11, 291, 458, 11, 341, 935, 820, 312, 20627, 382, 341, 472, 13, 51320, 51364, 7156, 309, 311, 2584, 20627, 382, 746, 1646, 13, 509, 483, 512, 588, 11, 588, 2416, 16235, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.14241454956379343, "compression_ratio": 1.6816143497757847, "no_speech_prob": 6.072058386052959e-05}, {"id": 351, "seek": 245936, "start": 2459.36, "end": 2466.6400000000003, "text": " The discriminator will jump away and then the generator will jump away and the decision boundary", "tokens": [50364, 440, 20828, 1639, 486, 3012, 1314, 293, 550, 264, 19265, 486, 3012, 1314, 293, 264, 3537, 12866, 50728, 50728, 486, 352, 11, 291, 458, 11, 25125, 433, 11, 25125, 433, 13, 400, 550, 291, 434, 516, 281, 362, 264, 19265, 1382, 281, 50912, 50948, 1190, 934, 613, 11, 291, 458, 11, 2614, 1314, 3537, 12866, 13, 1033, 13, 400, 370, 456, 307, 572, 32181, 13, 51360, 51360, 821, 307, 15625, 13, 407, 309, 311, 364, 23742, 15625, 935, 11, 597, 307, 588, 11, 588, 12414, 281, 51676, 51740], "temperature": 0.0, "avg_logprob": -0.15634045071072047, "compression_ratio": 1.8115942028985508, "no_speech_prob": 4.0605325921205804e-05}, {"id": 352, "seek": 245936, "start": 2466.6400000000003, "end": 2470.32, "text": " will go, you know, bunkers, bunkers. And then you're going to have the generator trying to", "tokens": [50364, 440, 20828, 1639, 486, 3012, 1314, 293, 550, 264, 19265, 486, 3012, 1314, 293, 264, 3537, 12866, 50728, 50728, 486, 352, 11, 291, 458, 11, 25125, 433, 11, 25125, 433, 13, 400, 550, 291, 434, 516, 281, 362, 264, 19265, 1382, 281, 50912, 50948, 1190, 934, 613, 11, 291, 458, 11, 2614, 1314, 3537, 12866, 13, 1033, 13, 400, 370, 456, 307, 572, 32181, 13, 51360, 51360, 821, 307, 15625, 13, 407, 309, 311, 364, 23742, 15625, 935, 11, 597, 307, 588, 11, 588, 12414, 281, 51676, 51740], "temperature": 0.0, "avg_logprob": -0.15634045071072047, "compression_ratio": 1.8115942028985508, "no_speech_prob": 4.0605325921205804e-05}, {"id": 353, "seek": 245936, "start": 2471.04, "end": 2479.28, "text": " run after these, you know, running away decision boundary. Okay. And so there is no convergence.", "tokens": [50364, 440, 20828, 1639, 486, 3012, 1314, 293, 550, 264, 19265, 486, 3012, 1314, 293, 264, 3537, 12866, 50728, 50728, 486, 352, 11, 291, 458, 11, 25125, 433, 11, 25125, 433, 13, 400, 550, 291, 434, 516, 281, 362, 264, 19265, 1382, 281, 50912, 50948, 1190, 934, 613, 11, 291, 458, 11, 2614, 1314, 3537, 12866, 13, 1033, 13, 400, 370, 456, 307, 572, 32181, 13, 51360, 51360, 821, 307, 15625, 13, 407, 309, 311, 364, 23742, 15625, 935, 11, 597, 307, 588, 11, 588, 12414, 281, 51676, 51740], "temperature": 0.0, "avg_logprob": -0.15634045071072047, "compression_ratio": 1.8115942028985508, "no_speech_prob": 4.0605325921205804e-05}, {"id": 354, "seek": 245936, "start": 2479.28, "end": 2485.6, "text": " There is equilibrium. So it's an unstable equilibrium point, which is very, very tricky to", "tokens": [50364, 440, 20828, 1639, 486, 3012, 1314, 293, 550, 264, 19265, 486, 3012, 1314, 293, 264, 3537, 12866, 50728, 50728, 486, 352, 11, 291, 458, 11, 25125, 433, 11, 25125, 433, 13, 400, 550, 291, 434, 516, 281, 362, 264, 19265, 1382, 281, 50912, 50948, 1190, 934, 613, 11, 291, 458, 11, 2614, 1314, 3537, 12866, 13, 1033, 13, 400, 370, 456, 307, 572, 32181, 13, 51360, 51360, 821, 307, 15625, 13, 407, 309, 311, 364, 23742, 15625, 935, 11, 597, 307, 588, 11, 588, 12414, 281, 51676, 51740], "temperature": 0.0, "avg_logprob": -0.15634045071072047, "compression_ratio": 1.8115942028985508, "no_speech_prob": 4.0605325921205804e-05}, {"id": 355, "seek": 248560, "start": 2485.6, "end": 2491.92, "text": " catch. Yeah. So I understand we have some sort of minimax problem here with our generator and our", "tokens": [50364, 3745, 13, 865, 13, 407, 286, 1223, 321, 362, 512, 1333, 295, 4464, 2797, 1154, 510, 365, 527, 19265, 293, 527, 50680, 50680, 2063, 13, 583, 294, 2674, 11, 562, 291, 19719, 341, 11, 286, 500, 380, 458, 534, 604, 15325, 2098, 281, 652, 50996, 50996, 988, 291, 41881, 534, 281, 264, 558, 935, 13, 1779, 13, 286, 669, 406, 988, 577, 291, 2573, 484, 1968, 51272, 51272, 291, 41881, 281, 257, 665, 935, 11, 457, 807, 5056, 22085, 295, 428, 23930, 295, 264, 19265, 13, 51536, 51588], "temperature": 0.0, "avg_logprob": -0.18635677761501737, "compression_ratio": 1.5892116182572613, "no_speech_prob": 3.645565811893903e-05}, {"id": 356, "seek": 248560, "start": 2491.92, "end": 2498.24, "text": " cost. But in general, when you optimize this, I don't know really any straightforward ways to make", "tokens": [50364, 3745, 13, 865, 13, 407, 286, 1223, 321, 362, 512, 1333, 295, 4464, 2797, 1154, 510, 365, 527, 19265, 293, 527, 50680, 50680, 2063, 13, 583, 294, 2674, 11, 562, 291, 19719, 341, 11, 286, 500, 380, 458, 534, 604, 15325, 2098, 281, 652, 50996, 50996, 988, 291, 41881, 534, 281, 264, 558, 935, 13, 1779, 13, 286, 669, 406, 988, 577, 291, 2573, 484, 1968, 51272, 51272, 291, 41881, 281, 257, 665, 935, 11, 457, 807, 5056, 22085, 295, 428, 23930, 295, 264, 19265, 13, 51536, 51588], "temperature": 0.0, "avg_logprob": -0.18635677761501737, "compression_ratio": 1.5892116182572613, "no_speech_prob": 3.645565811893903e-05}, {"id": 357, "seek": 248560, "start": 2498.24, "end": 2503.7599999999998, "text": " sure you converge really to the right point. Right. I am not sure how you figure out whether", "tokens": [50364, 3745, 13, 865, 13, 407, 286, 1223, 321, 362, 512, 1333, 295, 4464, 2797, 1154, 510, 365, 527, 19265, 293, 527, 50680, 50680, 2063, 13, 583, 294, 2674, 11, 562, 291, 19719, 341, 11, 286, 500, 380, 458, 534, 604, 15325, 2098, 281, 652, 50996, 50996, 988, 291, 41881, 534, 281, 264, 558, 935, 13, 1779, 13, 286, 669, 406, 988, 577, 291, 2573, 484, 1968, 51272, 51272, 291, 41881, 281, 257, 665, 935, 11, 457, 807, 5056, 22085, 295, 428, 23930, 295, 264, 19265, 13, 51536, 51588], "temperature": 0.0, "avg_logprob": -0.18635677761501737, "compression_ratio": 1.5892116182572613, "no_speech_prob": 3.645565811893903e-05}, {"id": 358, "seek": 248560, "start": 2503.7599999999998, "end": 2509.04, "text": " you converge to a good point, but through visual inspection of your outputs of the generator.", "tokens": [50364, 3745, 13, 865, 13, 407, 286, 1223, 321, 362, 512, 1333, 295, 4464, 2797, 1154, 510, 365, 527, 19265, 293, 527, 50680, 50680, 2063, 13, 583, 294, 2674, 11, 562, 291, 19719, 341, 11, 286, 500, 380, 458, 534, 604, 15325, 2098, 281, 652, 50996, 50996, 988, 291, 41881, 534, 281, 264, 558, 935, 13, 1779, 13, 286, 669, 406, 988, 577, 291, 2573, 484, 1968, 51272, 51272, 291, 41881, 281, 257, 665, 935, 11, 457, 807, 5056, 22085, 295, 428, 23930, 295, 264, 19265, 13, 51536, 51588], "temperature": 0.0, "avg_logprob": -0.18635677761501737, "compression_ratio": 1.5892116182572613, "no_speech_prob": 3.645565811893903e-05}, {"id": 359, "seek": 250904, "start": 2509.04, "end": 2515.84, "text": " Or you can train several, you can train several guns and then you train a discriminator", "tokens": [50364, 1610, 291, 393, 3847, 2940, 11, 291, 393, 3847, 2940, 10153, 293, 550, 291, 3847, 257, 20828, 1639, 50704, 50748, 322, 512, 3256, 1412, 992, 13, 400, 550, 291, 33872, 11, 291, 13059, 264, 3125, 295, 264, 11, 51148, 51240, 295, 264, 3256, 13, 1779, 13, 407, 341, 307, 411, 512, 733, 295, 406, 665, 20678, 321, 500, 380, 411, 11, 51548, 51548, 457, 300, 311, 437, 575, 668, 1096, 13, 467, 311, 1219, 49834, 6175, 13, 400, 550, 291, 393, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.32380252164952894, "compression_ratio": 1.675257731958763, "no_speech_prob": 8.3396393165458e-05}, {"id": 360, "seek": 250904, "start": 2516.72, "end": 2524.72, "text": " on some image data set. And then you classify, you evaluate the quality of the,", "tokens": [50364, 1610, 291, 393, 3847, 2940, 11, 291, 393, 3847, 2940, 10153, 293, 550, 291, 3847, 257, 20828, 1639, 50704, 50748, 322, 512, 3256, 1412, 992, 13, 400, 550, 291, 33872, 11, 291, 13059, 264, 3125, 295, 264, 11, 51148, 51240, 295, 264, 3256, 13, 1779, 13, 407, 341, 307, 411, 512, 733, 295, 406, 665, 20678, 321, 500, 380, 411, 11, 51548, 51548, 457, 300, 311, 437, 575, 668, 1096, 13, 467, 311, 1219, 49834, 6175, 13, 400, 550, 291, 393, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.32380252164952894, "compression_ratio": 1.675257731958763, "no_speech_prob": 8.3396393165458e-05}, {"id": 361, "seek": 250904, "start": 2526.56, "end": 2532.72, "text": " of the image. Right. So this is like some kind of not good metric we don't like,", "tokens": [50364, 1610, 291, 393, 3847, 2940, 11, 291, 393, 3847, 2940, 10153, 293, 550, 291, 3847, 257, 20828, 1639, 50704, 50748, 322, 512, 3256, 1412, 992, 13, 400, 550, 291, 33872, 11, 291, 13059, 264, 3125, 295, 264, 11, 51148, 51240, 295, 264, 3256, 13, 1779, 13, 407, 341, 307, 411, 512, 733, 295, 406, 665, 20678, 321, 500, 380, 411, 11, 51548, 51548, 457, 300, 311, 437, 575, 668, 1096, 13, 467, 311, 1219, 49834, 6175, 13, 400, 550, 291, 393, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.32380252164952894, "compression_ratio": 1.675257731958763, "no_speech_prob": 8.3396393165458e-05}, {"id": 362, "seek": 250904, "start": 2532.72, "end": 2537.68, "text": " but that's what has been done. It's called inception score. And then you can", "tokens": [50364, 1610, 291, 393, 3847, 2940, 11, 291, 393, 3847, 2940, 10153, 293, 550, 291, 3847, 257, 20828, 1639, 50704, 50748, 322, 512, 3256, 1412, 992, 13, 400, 550, 291, 33872, 11, 291, 13059, 264, 3125, 295, 264, 11, 51148, 51240, 295, 264, 3256, 13, 1779, 13, 407, 341, 307, 411, 512, 733, 295, 406, 665, 20678, 321, 500, 380, 411, 11, 51548, 51548, 457, 300, 311, 437, 575, 668, 1096, 13, 467, 311, 1219, 49834, 6175, 13, 400, 550, 291, 393, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.32380252164952894, "compression_ratio": 1.675257731958763, "no_speech_prob": 8.3396393165458e-05}, {"id": 363, "seek": 253768, "start": 2537.68, "end": 2542.96, "text": " call it inception score. So you train a network, let's say the inception network. That's why it's", "tokens": [50364, 818, 309, 49834, 6175, 13, 407, 291, 3847, 257, 3209, 11, 718, 311, 584, 264, 49834, 3209, 13, 663, 311, 983, 309, 311, 50628, 50628, 1219, 49834, 6175, 322, 11, 291, 458, 11, 364, 3256, 1412, 992, 13, 400, 550, 291, 393, 853, 281, 536, 1968, 50996, 51028, 613, 38662, 366, 2902, 291, 5267, 300, 574, 411, 746, 490, 11, 291, 458, 11, 51340, 51380, 490, 11, 490, 309, 307, 3097, 1412, 992, 13, 3764, 11, 309, 311, 406, 534, 257, 665, 20678, 11, 457, 1580, 3031, 281, 764, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.17319691565728956, "compression_ratio": 1.727699530516432, "no_speech_prob": 8.314950150634104e-07}, {"id": 364, "seek": 253768, "start": 2542.96, "end": 2550.3199999999997, "text": " called inception score on, you know, an image data set. And then you can try to see whether", "tokens": [50364, 818, 309, 49834, 6175, 13, 407, 291, 3847, 257, 3209, 11, 718, 311, 584, 264, 49834, 3209, 13, 663, 311, 983, 309, 311, 50628, 50628, 1219, 49834, 6175, 322, 11, 291, 458, 11, 364, 3256, 1412, 992, 13, 400, 550, 291, 393, 853, 281, 536, 1968, 50996, 51028, 613, 38662, 366, 2902, 291, 5267, 300, 574, 411, 746, 490, 11, 291, 458, 11, 51340, 51380, 490, 11, 490, 309, 307, 3097, 1412, 992, 13, 3764, 11, 309, 311, 406, 534, 257, 665, 20678, 11, 457, 1580, 3031, 281, 764, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.17319691565728956, "compression_ratio": 1.727699530516432, "no_speech_prob": 8.314950150634104e-07}, {"id": 365, "seek": 253768, "start": 2550.96, "end": 2557.2, "text": " these generators are giving you images that look like something from, you know,", "tokens": [50364, 818, 309, 49834, 6175, 13, 407, 291, 3847, 257, 3209, 11, 718, 311, 584, 264, 49834, 3209, 13, 663, 311, 983, 309, 311, 50628, 50628, 1219, 49834, 6175, 322, 11, 291, 458, 11, 364, 3256, 1412, 992, 13, 400, 550, 291, 393, 853, 281, 536, 1968, 50996, 51028, 613, 38662, 366, 2902, 291, 5267, 300, 574, 411, 746, 490, 11, 291, 458, 11, 51340, 51380, 490, 11, 490, 309, 307, 3097, 1412, 992, 13, 3764, 11, 309, 311, 406, 534, 257, 665, 20678, 11, 457, 1580, 3031, 281, 764, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.17319691565728956, "compression_ratio": 1.727699530516432, "no_speech_prob": 8.314950150634104e-07}, {"id": 366, "seek": 253768, "start": 2558.0, "end": 2565.7599999999998, "text": " from, from it is training data set. Again, it's not really a good metric, but someone tried to use", "tokens": [50364, 818, 309, 49834, 6175, 13, 407, 291, 3847, 257, 3209, 11, 718, 311, 584, 264, 49834, 3209, 13, 663, 311, 983, 309, 311, 50628, 50628, 1219, 49834, 6175, 322, 11, 291, 458, 11, 364, 3256, 1412, 992, 13, 400, 550, 291, 393, 853, 281, 536, 1968, 50996, 51028, 613, 38662, 366, 2902, 291, 5267, 300, 574, 411, 746, 490, 11, 291, 458, 11, 51340, 51380, 490, 11, 490, 309, 307, 3097, 1412, 992, 13, 3764, 11, 309, 311, 406, 534, 257, 665, 20678, 11, 457, 1580, 3031, 281, 764, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.17319691565728956, "compression_ratio": 1.727699530516432, "no_speech_prob": 8.314950150634104e-07}, {"id": 367, "seek": 256576, "start": 2565.76, "end": 2571.36, "text": " this for a way to evaluate generative, to evaluate generative models. Yeah.", "tokens": [50364, 341, 337, 257, 636, 281, 13059, 1337, 1166, 11, 281, 13059, 1337, 1166, 5245, 13, 865, 13, 50644, 50716, 4546, 2891, 11, 949, 516, 281, 264, 43782, 11, 718, 311, 362, 257, 574, 281, 767, 257, 8496, 1365, 51056, 51056, 295, 3097, 4470, 337, 613, 732, 9590, 321, 362, 445, 1612, 586, 13, 1033, 13, 407, 264, 4470, 2445, 337, 51460, 51460], "temperature": 0.0, "avg_logprob": -0.10433212667703629, "compression_ratio": 1.552325581395349, "no_speech_prob": 1.5630210327799432e-05}, {"id": 368, "seek": 256576, "start": 2572.8, "end": 2579.6000000000004, "text": " Before starting, before going to the notebooks, let's have a look to actually a practical example", "tokens": [50364, 341, 337, 257, 636, 281, 13059, 1337, 1166, 11, 281, 13059, 1337, 1166, 5245, 13, 865, 13, 50644, 50716, 4546, 2891, 11, 949, 516, 281, 264, 43782, 11, 718, 311, 362, 257, 574, 281, 767, 257, 8496, 1365, 51056, 51056, 295, 3097, 4470, 337, 613, 732, 9590, 321, 362, 445, 1612, 586, 13, 1033, 13, 407, 264, 4470, 2445, 337, 51460, 51460], "temperature": 0.0, "avg_logprob": -0.10433212667703629, "compression_ratio": 1.552325581395349, "no_speech_prob": 1.5630210327799432e-05}, {"id": 369, "seek": 256576, "start": 2579.6000000000004, "end": 2587.6800000000003, "text": " of training loss for these two networks we have just seen now. Okay. So the loss function for", "tokens": [50364, 341, 337, 257, 636, 281, 13059, 1337, 1166, 11, 281, 13059, 1337, 1166, 5245, 13, 865, 13, 50644, 50716, 4546, 2891, 11, 949, 516, 281, 264, 43782, 11, 718, 311, 362, 257, 574, 281, 767, 257, 8496, 1365, 51056, 51056, 295, 3097, 4470, 337, 613, 732, 9590, 321, 362, 445, 1612, 586, 13, 1033, 13, 407, 264, 4470, 2445, 337, 51460, 51460], "temperature": 0.0, "avg_logprob": -0.10433212667703629, "compression_ratio": 1.552325581395349, "no_speech_prob": 1.5630210327799432e-05}, {"id": 370, "seek": 258768, "start": 2587.68, "end": 2598.24, "text": " my cost network, given the input X and the latent input Z in orange, can be the following. So it can", "tokens": [50364, 452, 2063, 3209, 11, 2212, 264, 4846, 1783, 293, 264, 48994, 4846, 1176, 294, 7671, 11, 393, 312, 264, 3480, 13, 407, 309, 393, 50892, 50892, 312, 2681, 452, 2063, 383, 2212, 452, 7022, 4846, 1783, 293, 550, 1804, 341, 644, 510, 11, 597, 307, 264, 3353, 644, 51472, 51540], "temperature": 0.0, "avg_logprob": -0.16136848009549654, "compression_ratio": 1.5114503816793894, "no_speech_prob": 2.11040551221231e-05}, {"id": 371, "seek": 258768, "start": 2598.24, "end": 2609.8399999999997, "text": " be equal my cost C given my pink input X and then plus this part here, which is the positive part", "tokens": [50364, 452, 2063, 3209, 11, 2212, 264, 4846, 1783, 293, 264, 48994, 4846, 1176, 294, 7671, 11, 393, 312, 264, 3480, 13, 407, 309, 393, 50892, 50892, 312, 2681, 452, 2063, 383, 2212, 452, 7022, 4846, 1783, 293, 550, 1804, 341, 644, 510, 11, 597, 307, 264, 3353, 644, 51472, 51540], "temperature": 0.0, "avg_logprob": -0.16136848009549654, "compression_ratio": 1.5114503816793894, "no_speech_prob": 2.11040551221231e-05}, {"id": 372, "seek": 260984, "start": 2609.84, "end": 2619.84, "text": " of a margin M minus the cost I'm going to give to a generated input, which is, is outputted by my", "tokens": [50364, 295, 257, 10270, 376, 3175, 264, 2063, 286, 478, 516, 281, 976, 281, 257, 10833, 4846, 11, 597, 307, 11, 307, 5598, 14727, 538, 452, 50864, 50864, 19265, 11, 597, 307, 4636, 365, 264, 4846, 11, 48994, 4846, 11, 257, 4974, 1230, 13, 1033, 13, 407, 460, 295, 1176, 2709, 51240, 51240, 385, 257, 7592, 4846, 11, 550, 383, 486, 362, 281, 976, 385, 257, 2063, 13, 400, 382, 938, 382, 341, 2063, 486, 312, 3126, 51684, 51704], "temperature": 0.0, "avg_logprob": -0.16571723222732543, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.00010718309204094112}, {"id": 373, "seek": 260984, "start": 2619.84, "end": 2627.36, "text": " generator, which is fed with the input, latent input, a random number. Okay. So G of Z gives", "tokens": [50364, 295, 257, 10270, 376, 3175, 264, 2063, 286, 478, 516, 281, 976, 281, 257, 10833, 4846, 11, 597, 307, 11, 307, 5598, 14727, 538, 452, 50864, 50864, 19265, 11, 597, 307, 4636, 365, 264, 4846, 11, 48994, 4846, 11, 257, 4974, 1230, 13, 1033, 13, 407, 460, 295, 1176, 2709, 51240, 51240, 385, 257, 7592, 4846, 11, 550, 383, 486, 362, 281, 976, 385, 257, 2063, 13, 400, 382, 938, 382, 341, 2063, 486, 312, 3126, 51684, 51704], "temperature": 0.0, "avg_logprob": -0.16571723222732543, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.00010718309204094112}, {"id": 374, "seek": 260984, "start": 2627.36, "end": 2636.2400000000002, "text": " me a fake input, then C will have to give me a cost. And as long as this cost will be lower", "tokens": [50364, 295, 257, 10270, 376, 3175, 264, 2063, 286, 478, 516, 281, 976, 281, 257, 10833, 4846, 11, 597, 307, 11, 307, 5598, 14727, 538, 452, 50864, 50864, 19265, 11, 597, 307, 4636, 365, 264, 4846, 11, 48994, 4846, 11, 257, 4974, 1230, 13, 1033, 13, 407, 460, 295, 1176, 2709, 51240, 51240, 385, 257, 7592, 4846, 11, 550, 383, 486, 362, 281, 976, 385, 257, 2063, 13, 400, 382, 938, 382, 341, 2063, 486, 312, 3126, 51684, 51704], "temperature": 0.0, "avg_logprob": -0.16571723222732543, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.00010718309204094112}, {"id": 375, "seek": 263624, "start": 2636.24, "end": 2645.2, "text": " than M, this part here will be positive part. As soon as C, the cost network gives me a cost for", "tokens": [50364, 813, 376, 11, 341, 644, 510, 486, 312, 3353, 644, 13, 1018, 2321, 382, 383, 11, 264, 2063, 3209, 2709, 385, 257, 2063, 337, 50812, 50812, 341, 10833, 4846, 11, 597, 307, 4833, 813, 376, 11, 550, 341, 644, 510, 11, 376, 3175, 512, 1230, 4833, 813, 376, 51236, 51236, 307, 516, 281, 312, 257, 3671, 1230, 13, 400, 550, 1670, 286, 747, 264, 3353, 644, 11, 341, 1709, 281, 4018, 13, 51464, 51464, 400, 370, 341, 644, 295, 264, 4470, 1709, 281, 4018, 13, 14159, 264, 2063, 3209, 2709, 385, 257, 5598, 300, 307, 4833, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.17119478225708007, "compression_ratio": 1.9170731707317072, "no_speech_prob": 8.34354359540157e-05}, {"id": 376, "seek": 263624, "start": 2645.2, "end": 2653.68, "text": " this generated input, which is larger than M, then this part here, M minus some number larger than M", "tokens": [50364, 813, 376, 11, 341, 644, 510, 486, 312, 3353, 644, 13, 1018, 2321, 382, 383, 11, 264, 2063, 3209, 2709, 385, 257, 2063, 337, 50812, 50812, 341, 10833, 4846, 11, 597, 307, 4833, 813, 376, 11, 550, 341, 644, 510, 11, 376, 3175, 512, 1230, 4833, 813, 376, 51236, 51236, 307, 516, 281, 312, 257, 3671, 1230, 13, 400, 550, 1670, 286, 747, 264, 3353, 644, 11, 341, 1709, 281, 4018, 13, 51464, 51464, 400, 370, 341, 644, 295, 264, 4470, 1709, 281, 4018, 13, 14159, 264, 2063, 3209, 2709, 385, 257, 5598, 300, 307, 4833, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.17119478225708007, "compression_ratio": 1.9170731707317072, "no_speech_prob": 8.34354359540157e-05}, {"id": 377, "seek": 263624, "start": 2653.68, "end": 2658.24, "text": " is going to be a negative number. And then since I take the positive part, this goes to zero.", "tokens": [50364, 813, 376, 11, 341, 644, 510, 486, 312, 3353, 644, 13, 1018, 2321, 382, 383, 11, 264, 2063, 3209, 2709, 385, 257, 2063, 337, 50812, 50812, 341, 10833, 4846, 11, 597, 307, 4833, 813, 376, 11, 550, 341, 644, 510, 11, 376, 3175, 512, 1230, 4833, 813, 376, 51236, 51236, 307, 516, 281, 312, 257, 3671, 1230, 13, 400, 550, 1670, 286, 747, 264, 3353, 644, 11, 341, 1709, 281, 4018, 13, 51464, 51464, 400, 370, 341, 644, 295, 264, 4470, 1709, 281, 4018, 13, 14159, 264, 2063, 3209, 2709, 385, 257, 5598, 300, 307, 4833, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.17119478225708007, "compression_ratio": 1.9170731707317072, "no_speech_prob": 8.34354359540157e-05}, {"id": 378, "seek": 263624, "start": 2658.24, "end": 2664.8799999999997, "text": " And so this part of the loss goes to zero. Whenever the cost network gives me a output that is larger", "tokens": [50364, 813, 376, 11, 341, 644, 510, 486, 312, 3353, 644, 13, 1018, 2321, 382, 383, 11, 264, 2063, 3209, 2709, 385, 257, 2063, 337, 50812, 50812, 341, 10833, 4846, 11, 597, 307, 4833, 813, 376, 11, 550, 341, 644, 510, 11, 376, 3175, 512, 1230, 4833, 813, 376, 51236, 51236, 307, 516, 281, 312, 257, 3671, 1230, 13, 400, 550, 1670, 286, 747, 264, 3353, 644, 11, 341, 1709, 281, 4018, 13, 51464, 51464, 400, 370, 341, 644, 295, 264, 4470, 1709, 281, 4018, 13, 14159, 264, 2063, 3209, 2709, 385, 257, 5598, 300, 307, 4833, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.17119478225708007, "compression_ratio": 1.9170731707317072, "no_speech_prob": 8.34354359540157e-05}, {"id": 379, "seek": 266488, "start": 2664.88, "end": 2672.8, "text": " than M for a input that is being provided by my generator. On the other side here, we have simply", "tokens": [50364, 813, 376, 337, 257, 4846, 300, 307, 885, 5649, 538, 452, 19265, 13, 1282, 264, 661, 1252, 510, 11, 321, 362, 2935, 50760, 50760, 264, 2063, 6615, 281, 264, 3006, 7022, 4846, 13, 1779, 13, 400, 370, 294, 1668, 281, 31379, 341, 760, 281, 4018, 11, 51124, 51124, 291, 445, 362, 281, 362, 428, 2063, 3209, 5598, 783, 257, 4018, 5699, 264, 4846, 307, 264, 665, 472, 13, 1033, 13, 51512, 51512], "temperature": 0.0, "avg_logprob": -0.1640909298046215, "compression_ratio": 1.5336787564766838, "no_speech_prob": 3.903053584508598e-05}, {"id": 380, "seek": 266488, "start": 2672.8, "end": 2680.08, "text": " the cost associated to the correct pink input. Right. And so in order to squish this down to zero,", "tokens": [50364, 813, 376, 337, 257, 4846, 300, 307, 885, 5649, 538, 452, 19265, 13, 1282, 264, 661, 1252, 510, 11, 321, 362, 2935, 50760, 50760, 264, 2063, 6615, 281, 264, 3006, 7022, 4846, 13, 1779, 13, 400, 370, 294, 1668, 281, 31379, 341, 760, 281, 4018, 11, 51124, 51124, 291, 445, 362, 281, 362, 428, 2063, 3209, 5598, 783, 257, 4018, 5699, 264, 4846, 307, 264, 665, 472, 13, 1033, 13, 51512, 51512], "temperature": 0.0, "avg_logprob": -0.1640909298046215, "compression_ratio": 1.5336787564766838, "no_speech_prob": 3.903053584508598e-05}, {"id": 381, "seek": 266488, "start": 2680.08, "end": 2687.84, "text": " you just have to have your cost network outputting a zero whenever the input is the good one. Okay.", "tokens": [50364, 813, 376, 337, 257, 4846, 300, 307, 885, 5649, 538, 452, 19265, 13, 1282, 264, 661, 1252, 510, 11, 321, 362, 2935, 50760, 50760, 264, 2063, 6615, 281, 264, 3006, 7022, 4846, 13, 1779, 13, 400, 370, 294, 1668, 281, 31379, 341, 760, 281, 4018, 11, 51124, 51124, 291, 445, 362, 281, 362, 428, 2063, 3209, 5598, 783, 257, 4018, 5699, 264, 4846, 307, 264, 665, 472, 13, 1033, 13, 51512, 51512], "temperature": 0.0, "avg_logprob": -0.1640909298046215, "compression_ratio": 1.5336787564766838, "no_speech_prob": 3.903053584508598e-05}, {"id": 382, "seek": 268784, "start": 2687.84, "end": 2695.76, "text": " So in the example, in the example I was making before, I was saying that M is 10 and therefore", "tokens": [50364, 407, 294, 264, 1365, 11, 294, 264, 1365, 286, 390, 1455, 949, 11, 286, 390, 1566, 300, 376, 307, 1266, 293, 4412, 50760, 50760, 264, 3209, 307, 14658, 281, 5598, 257, 39684, 295, 1266, 11, 412, 1935, 1266, 11, 412, 1935, 1266, 11, 558, 30, 1171, 15743, 51176, 51176, 300, 366, 1348, 490, 264, 19265, 293, 848, 2063, 300, 307, 2681, 281, 4018, 307, 21162, 538, 341, 1433, 51452, 51452, 670, 510, 13, 407, 341, 307, 257, 1365, 295, 1944, 4470, 321, 393, 764, 337, 3097, 264, 2063, 3209, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.16235778686848093, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.00010299096902599558}, {"id": 383, "seek": 268784, "start": 2695.76, "end": 2704.08, "text": " the network is encouraged to output a scalar of 10, at least 10, at least 10, right? For inputs", "tokens": [50364, 407, 294, 264, 1365, 11, 294, 264, 1365, 286, 390, 1455, 949, 11, 286, 390, 1566, 300, 376, 307, 1266, 293, 4412, 50760, 50760, 264, 3209, 307, 14658, 281, 5598, 257, 39684, 295, 1266, 11, 412, 1935, 1266, 11, 412, 1935, 1266, 11, 558, 30, 1171, 15743, 51176, 51176, 300, 366, 1348, 490, 264, 19265, 293, 848, 2063, 300, 307, 2681, 281, 4018, 307, 21162, 538, 341, 1433, 51452, 51452, 670, 510, 13, 407, 341, 307, 257, 1365, 295, 1944, 4470, 321, 393, 764, 337, 3097, 264, 2063, 3209, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.16235778686848093, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.00010299096902599558}, {"id": 384, "seek": 268784, "start": 2704.08, "end": 2709.6000000000004, "text": " that are coming from the generator and said cost that is equal to zero is promoted by this term", "tokens": [50364, 407, 294, 264, 1365, 11, 294, 264, 1365, 286, 390, 1455, 949, 11, 286, 390, 1566, 300, 376, 307, 1266, 293, 4412, 50760, 50760, 264, 3209, 307, 14658, 281, 5598, 257, 39684, 295, 1266, 11, 412, 1935, 1266, 11, 412, 1935, 1266, 11, 558, 30, 1171, 15743, 51176, 51176, 300, 366, 1348, 490, 264, 19265, 293, 848, 2063, 300, 307, 2681, 281, 4018, 307, 21162, 538, 341, 1433, 51452, 51452, 670, 510, 13, 407, 341, 307, 257, 1365, 295, 1944, 4470, 321, 393, 764, 337, 3097, 264, 2063, 3209, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.16235778686848093, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.00010299096902599558}, {"id": 385, "seek": 268784, "start": 2709.6000000000004, "end": 2716.88, "text": " over here. So this is a example of possible loss we can use for training the cost network.", "tokens": [50364, 407, 294, 264, 1365, 11, 294, 264, 1365, 286, 390, 1455, 949, 11, 286, 390, 1566, 300, 376, 307, 1266, 293, 4412, 50760, 50760, 264, 3209, 307, 14658, 281, 5598, 257, 39684, 295, 1266, 11, 412, 1935, 1266, 11, 412, 1935, 1266, 11, 558, 30, 1171, 15743, 51176, 51176, 300, 366, 1348, 490, 264, 19265, 293, 848, 2063, 300, 307, 2681, 281, 4018, 307, 21162, 538, 341, 1433, 51452, 51452, 670, 510, 13, 407, 341, 307, 257, 1365, 295, 1944, 4470, 321, 393, 764, 337, 3097, 264, 2063, 3209, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.16235778686848093, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.00010299096902599558}, {"id": 386, "seek": 271688, "start": 2716.88, "end": 2724.8, "text": " Now this is done in this paper here by Jake, Michael and Jan from 2016.", "tokens": [50364, 823, 341, 307, 1096, 294, 341, 3035, 510, 538, 15822, 11, 5116, 293, 4956, 490, 6549, 13, 50760, 50760, 1396, 577, 360, 321, 3847, 264, 19265, 30, 1042, 11, 300, 311, 1596, 15325, 570, 291, 2935, 51080, 51108, 362, 264, 4470, 337, 3097, 264, 19265, 885, 2681, 264, 2063, 300, 264, 3209, 11, 264, 2063, 3209, 51556, 51556], "temperature": 0.0, "avg_logprob": -0.13724530537923177, "compression_ratio": 1.5, "no_speech_prob": 3.1656905775889754e-05}, {"id": 387, "seek": 271688, "start": 2724.8, "end": 2731.2000000000003, "text": " Then how do we train the generator? Well, that's quite straightforward because you simply", "tokens": [50364, 823, 341, 307, 1096, 294, 341, 3035, 510, 538, 15822, 11, 5116, 293, 4956, 490, 6549, 13, 50760, 50760, 1396, 577, 360, 321, 3847, 264, 19265, 30, 1042, 11, 300, 311, 1596, 15325, 570, 291, 2935, 51080, 51108, 362, 264, 4470, 337, 3097, 264, 19265, 885, 2681, 264, 2063, 300, 264, 3209, 11, 264, 2063, 3209, 51556, 51556], "temperature": 0.0, "avg_logprob": -0.13724530537923177, "compression_ratio": 1.5, "no_speech_prob": 3.1656905775889754e-05}, {"id": 388, "seek": 271688, "start": 2731.76, "end": 2740.7200000000003, "text": " have the loss for training the generator being equal the cost that the network, the cost network", "tokens": [50364, 823, 341, 307, 1096, 294, 341, 3035, 510, 538, 15822, 11, 5116, 293, 4956, 490, 6549, 13, 50760, 50760, 1396, 577, 360, 321, 3847, 264, 19265, 30, 1042, 11, 300, 311, 1596, 15325, 570, 291, 2935, 51080, 51108, 362, 264, 4470, 337, 3097, 264, 19265, 885, 2681, 264, 2063, 300, 264, 3209, 11, 264, 2063, 3209, 51556, 51556], "temperature": 0.0, "avg_logprob": -0.13724530537923177, "compression_ratio": 1.5, "no_speech_prob": 3.1656905775889754e-05}, {"id": 389, "seek": 274072, "start": 2740.72, "end": 2750.16, "text": " gives me for a given generated sample. Right. And so my generator will simply try to get a low cost.", "tokens": [50364, 2709, 385, 337, 257, 2212, 10833, 6889, 13, 1779, 13, 400, 370, 452, 19265, 486, 2935, 853, 281, 483, 257, 2295, 2063, 13, 50836, 50880, 400, 300, 311, 370, 1238, 13, 1057, 558, 13, 1033, 13, 3764, 11, 393, 321, 11, 393, 321, 312, 544, 2685, 30, 883, 13, 708, 307, 51356, 51356, 341, 2063, 3209, 30, 286, 2378, 380, 1907, 291, 1939, 257, 2685, 3922, 291, 393, 652, 337, 4084, 257, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.11900344647859272, "compression_ratio": 1.516304347826087, "no_speech_prob": 1.2788103958882857e-05}, {"id": 390, "seek": 274072, "start": 2751.04, "end": 2760.56, "text": " And that's so pretty. All right. Okay. Again, can we, can we be more specific? No. What is", "tokens": [50364, 2709, 385, 337, 257, 2212, 10833, 6889, 13, 1779, 13, 400, 370, 452, 19265, 486, 2935, 853, 281, 483, 257, 2295, 2063, 13, 50836, 50880, 400, 300, 311, 370, 1238, 13, 1057, 558, 13, 1033, 13, 3764, 11, 393, 321, 11, 393, 321, 312, 544, 2685, 30, 883, 13, 708, 307, 51356, 51356, 341, 2063, 3209, 30, 286, 2378, 380, 1907, 291, 1939, 257, 2685, 3922, 291, 393, 652, 337, 4084, 257, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.11900344647859272, "compression_ratio": 1.516304347826087, "no_speech_prob": 1.2788103958882857e-05}, {"id": 391, "seek": 274072, "start": 2760.56, "end": 2767.2, "text": " this cost network? I haven't told you yet a specific choice you can make for creating a", "tokens": [50364, 2709, 385, 337, 257, 2212, 10833, 6889, 13, 1779, 13, 400, 370, 452, 19265, 486, 2935, 853, 281, 483, 257, 2295, 2063, 13, 50836, 50880, 400, 300, 311, 370, 1238, 13, 1057, 558, 13, 1033, 13, 3764, 11, 393, 321, 11, 393, 321, 312, 544, 2685, 30, 883, 13, 708, 307, 51356, 51356, 341, 2063, 3209, 30, 286, 2378, 380, 1907, 291, 1939, 257, 2685, 3922, 291, 393, 652, 337, 4084, 257, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.11900344647859272, "compression_ratio": 1.516304347826087, "no_speech_prob": 1.2788103958882857e-05}, {"id": 392, "seek": 276720, "start": 2767.2, "end": 2773.68, "text": " network that is giving you this scalar based on the input, but I think you may already have some", "tokens": [50364, 3209, 300, 307, 2902, 291, 341, 39684, 2361, 322, 264, 4846, 11, 457, 286, 519, 291, 815, 1217, 362, 512, 50688, 50748, 3487, 577, 341, 3209, 393, 312, 1027, 13, 400, 370, 257, 1944, 3922, 337, 51056, 51148, 337, 341, 3209, 307, 516, 281, 312, 264, 3480, 13, 467, 311, 516, 281, 312, 264, 376, 5879, 11, 264, 37262, 2649, 51544, 51584], "temperature": 0.0, "avg_logprob": -0.08901692926883698, "compression_ratio": 1.505813953488372, "no_speech_prob": 4.1225997847504914e-05}, {"id": 393, "seek": 276720, "start": 2774.8799999999997, "end": 2781.04, "text": " ideas how this network can be made. And so a possible choice for", "tokens": [50364, 3209, 300, 307, 2902, 291, 341, 39684, 2361, 322, 264, 4846, 11, 457, 286, 519, 291, 815, 1217, 362, 512, 50688, 50748, 3487, 577, 341, 3209, 393, 312, 1027, 13, 400, 370, 257, 1944, 3922, 337, 51056, 51148, 337, 341, 3209, 307, 516, 281, 312, 264, 3480, 13, 467, 311, 516, 281, 312, 264, 376, 5879, 11, 264, 37262, 2649, 51544, 51584], "temperature": 0.0, "avg_logprob": -0.08901692926883698, "compression_ratio": 1.505813953488372, "no_speech_prob": 4.1225997847504914e-05}, {"id": 394, "seek": 276720, "start": 2782.8799999999997, "end": 2790.7999999999997, "text": " for this network is going to be the following. It's going to be the MSE, the quadratic difference", "tokens": [50364, 3209, 300, 307, 2902, 291, 341, 39684, 2361, 322, 264, 4846, 11, 457, 286, 519, 291, 815, 1217, 362, 512, 50688, 50748, 3487, 577, 341, 3209, 393, 312, 1027, 13, 400, 370, 257, 1944, 3922, 337, 51056, 51148, 337, 341, 3209, 307, 516, 281, 312, 264, 3480, 13, 467, 311, 516, 281, 312, 264, 376, 5879, 11, 264, 37262, 2649, 51544, 51584], "temperature": 0.0, "avg_logprob": -0.08901692926883698, "compression_ratio": 1.505813953488372, "no_speech_prob": 4.1225997847504914e-05}, {"id": 395, "seek": 279080, "start": 2790.8, "end": 2799.36, "text": " between the decoding of the encoding of the specific input. So this is the reconstruction", "tokens": [50364, 1296, 264, 979, 8616, 295, 264, 43430, 295, 264, 2685, 4846, 13, 407, 341, 307, 264, 31565, 50792, 50792, 295, 257, 8399, 22660, 19866, 3175, 264, 4846, 2564, 13, 16463, 11, 558, 30, 440, 2026, 3732, 13, 407, 577, 775, 341, 589, 30, 51268, 51268, 1042, 11, 411, 498, 264, 8399, 22660, 19866, 307, 885, 8895, 787, 322, 7022, 10938, 11, 309, 486, 312, 1075, 281, 31499, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.15439367965913156, "compression_ratio": 1.6379310344827587, "no_speech_prob": 3.703900802065618e-05}, {"id": 396, "seek": 279080, "start": 2799.36, "end": 2808.88, "text": " of a autoencoder minus the input itself. Square, right? The norm square. So how does this work?", "tokens": [50364, 1296, 264, 979, 8616, 295, 264, 43430, 295, 264, 2685, 4846, 13, 407, 341, 307, 264, 31565, 50792, 50792, 295, 257, 8399, 22660, 19866, 3175, 264, 4846, 2564, 13, 16463, 11, 558, 30, 440, 2026, 3732, 13, 407, 577, 775, 341, 589, 30, 51268, 51268, 1042, 11, 411, 498, 264, 8399, 22660, 19866, 307, 885, 8895, 787, 322, 7022, 10938, 11, 309, 486, 312, 1075, 281, 31499, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.15439367965913156, "compression_ratio": 1.6379310344827587, "no_speech_prob": 3.703900802065618e-05}, {"id": 397, "seek": 279080, "start": 2808.88, "end": 2818.32, "text": " Well, like if the autoencoder is being trained only on pink samples, it will be able to reconstruct", "tokens": [50364, 1296, 264, 979, 8616, 295, 264, 43430, 295, 264, 2685, 4846, 13, 407, 341, 307, 264, 31565, 50792, 50792, 295, 257, 8399, 22660, 19866, 3175, 264, 4846, 2564, 13, 16463, 11, 558, 30, 440, 2026, 3732, 13, 407, 577, 775, 341, 589, 30, 51268, 51268, 1042, 11, 411, 498, 264, 8399, 22660, 19866, 307, 885, 8895, 787, 322, 7022, 10938, 11, 309, 486, 312, 1075, 281, 31499, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.15439367965913156, "compression_ratio": 1.6379310344827587, "no_speech_prob": 3.703900802065618e-05}, {"id": 398, "seek": 281832, "start": 2818.32, "end": 2825.1200000000003, "text": " pink samples only. Right. And therefore the distance between my input, the pink input and", "tokens": [50364, 7022, 10938, 787, 13, 1779, 13, 400, 4412, 264, 4560, 1296, 452, 4846, 11, 264, 7022, 4846, 293, 50704, 50704, 264, 31565, 295, 264, 8399, 22660, 19866, 562, 286, 2893, 264, 7022, 4846, 486, 312, 588, 1359, 11, 51028, 51028, 4696, 11, 558, 13, 759, 321, 3847, 341, 9594, 2602, 11, 437, 2314, 586, 498, 286, 829, 364, 4846, 510, 300, 307, 51376, 51376, 1400, 490, 1340, 300, 307, 322, 264, 1412, 47138, 13, 1042, 11, 452, 8399, 22660, 19866, 575, 668, 8895, 281, 5598, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.1837508598070466, "compression_ratio": 1.6533333333333333, "no_speech_prob": 7.0711594162276015e-06}, {"id": 399, "seek": 281832, "start": 2825.1200000000003, "end": 2831.6000000000004, "text": " the reconstruction of the autoencoder when I provide the pink input will be very small,", "tokens": [50364, 7022, 10938, 787, 13, 1779, 13, 400, 4412, 264, 4560, 1296, 452, 4846, 11, 264, 7022, 4846, 293, 50704, 50704, 264, 31565, 295, 264, 8399, 22660, 19866, 562, 286, 2893, 264, 7022, 4846, 486, 312, 588, 1359, 11, 51028, 51028, 4696, 11, 558, 13, 759, 321, 3847, 341, 9594, 2602, 11, 437, 2314, 586, 498, 286, 829, 364, 4846, 510, 300, 307, 51376, 51376, 1400, 490, 1340, 300, 307, 322, 264, 1412, 47138, 13, 1042, 11, 452, 8399, 22660, 19866, 575, 668, 8895, 281, 5598, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.1837508598070466, "compression_ratio": 1.6533333333333333, "no_speech_prob": 7.0711594162276015e-06}, {"id": 400, "seek": 281832, "start": 2831.6000000000004, "end": 2838.56, "text": " hopefully, right. If we train this nicely instead, what happens now if I put an input here that is", "tokens": [50364, 7022, 10938, 787, 13, 1779, 13, 400, 4412, 264, 4560, 1296, 452, 4846, 11, 264, 7022, 4846, 293, 50704, 50704, 264, 31565, 295, 264, 8399, 22660, 19866, 562, 286, 2893, 264, 7022, 4846, 486, 312, 588, 1359, 11, 51028, 51028, 4696, 11, 558, 13, 759, 321, 3847, 341, 9594, 2602, 11, 437, 2314, 586, 498, 286, 829, 364, 4846, 510, 300, 307, 51376, 51376, 1400, 490, 1340, 300, 307, 322, 264, 1412, 47138, 13, 1042, 11, 452, 8399, 22660, 19866, 575, 668, 8895, 281, 5598, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.1837508598070466, "compression_ratio": 1.6533333333333333, "no_speech_prob": 7.0711594162276015e-06}, {"id": 401, "seek": 281832, "start": 2838.56, "end": 2844.1600000000003, "text": " far from anything that is on the data manifold. Well, my autoencoder has been trained to output", "tokens": [50364, 7022, 10938, 787, 13, 1779, 13, 400, 4412, 264, 4560, 1296, 452, 4846, 11, 264, 7022, 4846, 293, 50704, 50704, 264, 31565, 295, 264, 8399, 22660, 19866, 562, 286, 2893, 264, 7022, 4846, 486, 312, 588, 1359, 11, 51028, 51028, 4696, 11, 558, 13, 759, 321, 3847, 341, 9594, 2602, 11, 437, 2314, 586, 498, 286, 829, 364, 4846, 510, 300, 307, 51376, 51376, 1400, 490, 1340, 300, 307, 322, 264, 1412, 47138, 13, 1042, 11, 452, 8399, 22660, 19866, 575, 668, 8895, 281, 5598, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.1837508598070466, "compression_ratio": 1.6533333333333333, "no_speech_prob": 7.0711594162276015e-06}, {"id": 402, "seek": 284416, "start": 2844.16, "end": 2852.16, "text": " things that stays on the data manifold. And therefore there will be a substantial", "tokens": [50364, 721, 300, 10834, 322, 264, 1412, 47138, 13, 400, 4412, 456, 486, 312, 257, 16726, 50764, 50764, 2649, 1296, 452, 3539, 4846, 293, 437, 452, 8399, 22660, 19866, 393, 976, 291, 13, 1779, 13, 440, 1481, 644, 295, 51160, 51160, 341, 2685, 3922, 295, 2063, 3209, 307, 300, 291, 393, 3847, 341, 8399, 22660, 19866, 1553, 264, 19265, 13, 51532, 51532, 1779, 13, 509, 393, 2935, 3847, 364, 8399, 22660, 19866, 11, 2035, 291, 393, 362, 411, 364, 833, 3566, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.14010762033008395, "compression_ratio": 1.6788990825688073, "no_speech_prob": 5.8610485211829655e-06}, {"id": 403, "seek": 284416, "start": 2852.16, "end": 2860.08, "text": " difference between my actual input and what my autoencoder can give you. Right. The nice part of", "tokens": [50364, 721, 300, 10834, 322, 264, 1412, 47138, 13, 400, 4412, 456, 486, 312, 257, 16726, 50764, 50764, 2649, 1296, 452, 3539, 4846, 293, 437, 452, 8399, 22660, 19866, 393, 976, 291, 13, 1779, 13, 440, 1481, 644, 295, 51160, 51160, 341, 2685, 3922, 295, 2063, 3209, 307, 300, 291, 393, 3847, 341, 8399, 22660, 19866, 1553, 264, 19265, 13, 51532, 51532, 1779, 13, 509, 393, 2935, 3847, 364, 8399, 22660, 19866, 11, 2035, 291, 393, 362, 411, 364, 833, 3566, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.14010762033008395, "compression_ratio": 1.6788990825688073, "no_speech_prob": 5.8610485211829655e-06}, {"id": 404, "seek": 284416, "start": 2860.08, "end": 2867.52, "text": " this specific choice of cost network is that you can train this autoencoder without the generator.", "tokens": [50364, 721, 300, 10834, 322, 264, 1412, 47138, 13, 400, 4412, 456, 486, 312, 257, 16726, 50764, 50764, 2649, 1296, 452, 3539, 4846, 293, 437, 452, 8399, 22660, 19866, 393, 976, 291, 13, 1779, 13, 440, 1481, 644, 295, 51160, 51160, 341, 2685, 3922, 295, 2063, 3209, 307, 300, 291, 393, 3847, 341, 8399, 22660, 19866, 1553, 264, 19265, 13, 51532, 51532, 1779, 13, 509, 393, 2935, 3847, 364, 8399, 22660, 19866, 11, 2035, 291, 393, 362, 411, 364, 833, 3566, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.14010762033008395, "compression_ratio": 1.6788990825688073, "no_speech_prob": 5.8610485211829655e-06}, {"id": 405, "seek": 284416, "start": 2867.52, "end": 2872.7999999999997, "text": " Right. You can simply train an autoencoder, whatever you can have like an under complete", "tokens": [50364, 721, 300, 10834, 322, 264, 1412, 47138, 13, 400, 4412, 456, 486, 312, 257, 16726, 50764, 50764, 2649, 1296, 452, 3539, 4846, 293, 437, 452, 8399, 22660, 19866, 393, 976, 291, 13, 1779, 13, 440, 1481, 644, 295, 51160, 51160, 341, 2685, 3922, 295, 2063, 3209, 307, 300, 291, 393, 3847, 341, 8399, 22660, 19866, 1553, 264, 19265, 13, 51532, 51532, 1779, 13, 509, 393, 2935, 3847, 364, 8399, 22660, 19866, 11, 2035, 291, 393, 362, 411, 364, 833, 3566, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.14010762033008395, "compression_ratio": 1.6788990825688073, "no_speech_prob": 5.8610485211829655e-06}, {"id": 406, "seek": 287280, "start": 2872.8, "end": 2879.84, "text": " hidden layer, over complete, and you use some kind of regularization and information restriction,", "tokens": [50364, 7633, 4583, 11, 670, 3566, 11, 293, 291, 764, 512, 733, 295, 3890, 2144, 293, 1589, 29529, 11, 50716, 50716, 44641, 547, 13, 583, 26924, 11, 291, 393, 767, 3847, 341, 2146, 1553, 1419, 257, 19265, 13, 1779, 13, 51044, 51044, 400, 341, 472, 11, 291, 486, 2935, 1466, 437, 307, 264, 3847, 11, 264, 1412, 47138, 13, 400, 550, 291, 393, 764, 51324, 51324, 341, 382, 257, 29690, 281, 8327, 264, 2649, 11, 264, 4560, 1296, 428, 2190, 4846, 293, 437, 264, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.11455472310384114, "compression_ratio": 1.6192468619246863, "no_speech_prob": 3.474190816632472e-05}, {"id": 407, "seek": 287280, "start": 2879.84, "end": 2886.4, "text": " bottleneck. But nevertheless, you can actually train this guy without having a generator. Right.", "tokens": [50364, 7633, 4583, 11, 670, 3566, 11, 293, 291, 764, 512, 733, 295, 3890, 2144, 293, 1589, 29529, 11, 50716, 50716, 44641, 547, 13, 583, 26924, 11, 291, 393, 767, 3847, 341, 2146, 1553, 1419, 257, 19265, 13, 1779, 13, 51044, 51044, 400, 341, 472, 11, 291, 486, 2935, 1466, 437, 307, 264, 3847, 11, 264, 1412, 47138, 13, 400, 550, 291, 393, 764, 51324, 51324, 341, 382, 257, 29690, 281, 8327, 264, 2649, 11, 264, 4560, 1296, 428, 2190, 4846, 293, 437, 264, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.11455472310384114, "compression_ratio": 1.6192468619246863, "no_speech_prob": 3.474190816632472e-05}, {"id": 408, "seek": 287280, "start": 2886.4, "end": 2892.0, "text": " And this one, you will simply learn what is the train, the data manifold. And then you can use", "tokens": [50364, 7633, 4583, 11, 670, 3566, 11, 293, 291, 764, 512, 733, 295, 3890, 2144, 293, 1589, 29529, 11, 50716, 50716, 44641, 547, 13, 583, 26924, 11, 291, 393, 767, 3847, 341, 2146, 1553, 1419, 257, 19265, 13, 1779, 13, 51044, 51044, 400, 341, 472, 11, 291, 486, 2935, 1466, 437, 307, 264, 3847, 11, 264, 1412, 47138, 13, 400, 550, 291, 393, 764, 51324, 51324, 341, 382, 257, 29690, 281, 8327, 264, 2649, 11, 264, 4560, 1296, 428, 2190, 4846, 293, 437, 264, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.11455472310384114, "compression_ratio": 1.6192468619246863, "no_speech_prob": 3.474190816632472e-05}, {"id": 409, "seek": 287280, "start": 2892.0, "end": 2898.96, "text": " this as a proxy to establish the difference, the distance between your current input and what the", "tokens": [50364, 7633, 4583, 11, 670, 3566, 11, 293, 291, 764, 512, 733, 295, 3890, 2144, 293, 1589, 29529, 11, 50716, 50716, 44641, 547, 13, 583, 26924, 11, 291, 393, 767, 3847, 341, 2146, 1553, 1419, 257, 19265, 13, 1779, 13, 51044, 51044, 400, 341, 472, 11, 291, 486, 2935, 1466, 437, 307, 264, 3847, 11, 264, 1412, 47138, 13, 400, 550, 291, 393, 764, 51324, 51324, 341, 382, 257, 29690, 281, 8327, 264, 2649, 11, 264, 4560, 1296, 428, 2190, 4846, 293, 437, 264, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.11455472310384114, "compression_ratio": 1.6192468619246863, "no_speech_prob": 3.474190816632472e-05}, {"id": 410, "seek": 289896, "start": 2898.96, "end": 2907.28, "text": " network thinks the closest input on the training manifold could be. Okay. All right. Let's move on.", "tokens": [50364, 3209, 7309, 264, 13699, 4846, 322, 264, 3097, 47138, 727, 312, 13, 1033, 13, 1057, 558, 13, 961, 311, 1286, 322, 13, 50780, 50780, 682, 264, 1036, 1732, 2077, 11, 498, 456, 366, 572, 1651, 11, 321, 366, 516, 281, 312, 3760, 264, 51076, 51076, 4009, 3089, 490, 9953, 51, 284, 339, 5110, 1214, 13, 400, 341, 11, 286, 519, 11, 307, 516, 281, 312, 264, 700, 1389, 689, 51408, 51408, 321, 366, 767, 3760, 512, 32116, 10754, 3089, 13, 286, 478, 406, 257, 32116, 13, 407, 2035, 291, 600, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10623931884765625, "compression_ratio": 1.5857740585774058, "no_speech_prob": 2.0448000213946216e-05}, {"id": 411, "seek": 289896, "start": 2907.28, "end": 2913.2, "text": " In the last five minutes, if there are no questions, we are going to be reading the", "tokens": [50364, 3209, 7309, 264, 13699, 4846, 322, 264, 3097, 47138, 727, 312, 13, 1033, 13, 1057, 558, 13, 961, 311, 1286, 322, 13, 50780, 50780, 682, 264, 1036, 1732, 2077, 11, 498, 456, 366, 572, 1651, 11, 321, 366, 516, 281, 312, 3760, 264, 51076, 51076, 4009, 3089, 490, 9953, 51, 284, 339, 5110, 1214, 13, 400, 341, 11, 286, 519, 11, 307, 516, 281, 312, 264, 700, 1389, 689, 51408, 51408, 321, 366, 767, 3760, 512, 32116, 10754, 3089, 13, 286, 478, 406, 257, 32116, 13, 407, 2035, 291, 600, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10623931884765625, "compression_ratio": 1.5857740585774058, "no_speech_prob": 2.0448000213946216e-05}, {"id": 412, "seek": 289896, "start": 2913.2, "end": 2919.84, "text": " source code from PyTorch examples together. And this, I think, is going to be the first case where", "tokens": [50364, 3209, 7309, 264, 13699, 4846, 322, 264, 3097, 47138, 727, 312, 13, 1033, 13, 1057, 558, 13, 961, 311, 1286, 322, 13, 50780, 50780, 682, 264, 1036, 1732, 2077, 11, 498, 456, 366, 572, 1651, 11, 321, 366, 516, 281, 312, 3760, 264, 51076, 51076, 4009, 3089, 490, 9953, 51, 284, 339, 5110, 1214, 13, 400, 341, 11, 286, 519, 11, 307, 516, 281, 312, 264, 700, 1389, 689, 51408, 51408, 321, 366, 767, 3760, 512, 32116, 10754, 3089, 13, 286, 478, 406, 257, 32116, 13, 407, 2035, 291, 600, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10623931884765625, "compression_ratio": 1.5857740585774058, "no_speech_prob": 2.0448000213946216e-05}, {"id": 413, "seek": 289896, "start": 2919.84, "end": 2926.96, "text": " we are actually reading some programmer developer code. I'm not a programmer. So whatever you've", "tokens": [50364, 3209, 7309, 264, 13699, 4846, 322, 264, 3097, 47138, 727, 312, 13, 1033, 13, 1057, 558, 13, 961, 311, 1286, 322, 13, 50780, 50780, 682, 264, 1036, 1732, 2077, 11, 498, 456, 366, 572, 1651, 11, 321, 366, 516, 281, 312, 3760, 264, 51076, 51076, 4009, 3089, 490, 9953, 51, 284, 339, 5110, 1214, 13, 400, 341, 11, 286, 519, 11, 307, 516, 281, 312, 264, 700, 1389, 689, 51408, 51408, 321, 366, 767, 3760, 512, 32116, 10754, 3089, 13, 286, 478, 406, 257, 32116, 13, 407, 2035, 291, 600, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10623931884765625, "compression_ratio": 1.5857740585774058, "no_speech_prob": 2.0448000213946216e-05}, {"id": 414, "seek": 292696, "start": 2926.96, "end": 2933.84, "text": " been consuming so far were my notebooks, which were some kind of pedagogical and educational", "tokens": [50364, 668, 19867, 370, 1400, 645, 452, 43782, 11, 597, 645, 512, 733, 295, 5670, 31599, 804, 293, 10189, 50708, 50708, 2701, 11, 597, 307, 733, 295, 2758, 2980, 1270, 300, 309, 1542, 1481, 293, 1238, 293, 575, 1481, 1237, 5598, 13, 51072, 51072, 1779, 586, 11, 321, 434, 516, 281, 312, 3760, 767, 1481, 3089, 3720, 538, 561, 300, 360, 341, 382, 641, 1691, 13, 51356, 51356, 1779, 13, 407, 321, 352, 23331, 13, 492, 500, 380, 352, 322, 9953, 51, 284, 339, 2452, 2539, 13, 492, 434, 516, 281, 9953, 51, 284, 339, 13, 9953, 51, 284, 339, 30, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.15708182408259466, "compression_ratio": 1.6329113924050633, "no_speech_prob": 1.3832673175784294e-05}, {"id": 415, "seek": 292696, "start": 2933.84, "end": 2941.12, "text": " content, which is kind of massaged such that it looks nice and pretty and has nice looking output.", "tokens": [50364, 668, 19867, 370, 1400, 645, 452, 43782, 11, 597, 645, 512, 733, 295, 5670, 31599, 804, 293, 10189, 50708, 50708, 2701, 11, 597, 307, 733, 295, 2758, 2980, 1270, 300, 309, 1542, 1481, 293, 1238, 293, 575, 1481, 1237, 5598, 13, 51072, 51072, 1779, 586, 11, 321, 434, 516, 281, 312, 3760, 767, 1481, 3089, 3720, 538, 561, 300, 360, 341, 382, 641, 1691, 13, 51356, 51356, 1779, 13, 407, 321, 352, 23331, 13, 492, 500, 380, 352, 322, 9953, 51, 284, 339, 2452, 2539, 13, 492, 434, 516, 281, 9953, 51, 284, 339, 13, 9953, 51, 284, 339, 30, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.15708182408259466, "compression_ratio": 1.6329113924050633, "no_speech_prob": 1.3832673175784294e-05}, {"id": 416, "seek": 292696, "start": 2941.12, "end": 2946.8, "text": " Right now, we're going to be reading actually nice code written by people that do this as their job.", "tokens": [50364, 668, 19867, 370, 1400, 645, 452, 43782, 11, 597, 645, 512, 733, 295, 5670, 31599, 804, 293, 10189, 50708, 50708, 2701, 11, 597, 307, 733, 295, 2758, 2980, 1270, 300, 309, 1542, 1481, 293, 1238, 293, 575, 1481, 1237, 5598, 13, 51072, 51072, 1779, 586, 11, 321, 434, 516, 281, 312, 3760, 767, 1481, 3089, 3720, 538, 561, 300, 360, 341, 382, 641, 1691, 13, 51356, 51356, 1779, 13, 407, 321, 352, 23331, 13, 492, 500, 380, 352, 322, 9953, 51, 284, 339, 2452, 2539, 13, 492, 434, 516, 281, 9953, 51, 284, 339, 13, 9953, 51, 284, 339, 30, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.15708182408259466, "compression_ratio": 1.6329113924050633, "no_speech_prob": 1.3832673175784294e-05}, {"id": 417, "seek": 292696, "start": 2946.8, "end": 2956.0, "text": " Right. So we go GitHub. We don't go on PyTorch deep learning. We're going to PyTorch. PyTorch?", "tokens": [50364, 668, 19867, 370, 1400, 645, 452, 43782, 11, 597, 645, 512, 733, 295, 5670, 31599, 804, 293, 10189, 50708, 50708, 2701, 11, 597, 307, 733, 295, 2758, 2980, 1270, 300, 309, 1542, 1481, 293, 1238, 293, 575, 1481, 1237, 5598, 13, 51072, 51072, 1779, 586, 11, 321, 434, 516, 281, 312, 3760, 767, 1481, 3089, 3720, 538, 561, 300, 360, 341, 382, 641, 1691, 13, 51356, 51356, 1779, 13, 407, 321, 352, 23331, 13, 492, 500, 380, 352, 322, 9953, 51, 284, 339, 2452, 2539, 13, 492, 434, 516, 281, 9953, 51, 284, 339, 13, 9953, 51, 284, 339, 30, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.15708182408259466, "compression_ratio": 1.6329113924050633, "no_speech_prob": 1.3832673175784294e-05}, {"id": 418, "seek": 295600, "start": 2956.0, "end": 2967.92, "text": " No. PyTorch examples. Examples. Okay. So let's zoom a little bit.", "tokens": [50364, 883, 13, 9953, 51, 284, 339, 5110, 13, 48591, 13, 1033, 13, 407, 718, 311, 8863, 257, 707, 857, 13, 50960, 51088, 1033, 13, 407, 510, 321, 362, 264, 9114, 3874, 293, 264, 7110, 11402, 510, 13, 1033, 13, 407, 321, 393, 445, 352, 807, 341, 3089, 51628, 51628], "temperature": 0.0, "avg_logprob": -0.19546222686767578, "compression_ratio": 1.3524590163934427, "no_speech_prob": 1.608945785847027e-05}, {"id": 419, "seek": 295600, "start": 2970.48, "end": 2981.28, "text": " Okay. So here we have the DC gun and the swim width here. Okay. So we can just go through this code", "tokens": [50364, 883, 13, 9953, 51, 284, 339, 5110, 13, 48591, 13, 1033, 13, 407, 718, 311, 8863, 257, 707, 857, 13, 50960, 51088, 1033, 13, 407, 510, 321, 362, 264, 9114, 3874, 293, 264, 7110, 11402, 510, 13, 1033, 13, 407, 321, 393, 445, 352, 807, 341, 3089, 51628, 51628], "temperature": 0.0, "avg_logprob": -0.19546222686767578, "compression_ratio": 1.3524590163934427, "no_speech_prob": 1.608945785847027e-05}, {"id": 420, "seek": 298128, "start": 2981.28, "end": 2986.8, "text": " main things, right? So we start with importing a bunch of crappy things. As usual,", "tokens": [50364, 2135, 721, 11, 558, 30, 407, 321, 722, 365, 43866, 257, 3840, 295, 36531, 721, 13, 1018, 7713, 11, 50640, 50640, 291, 362, 364, 6770, 21156, 260, 1270, 300, 291, 393, 2845, 512, 2685, 16901, 11, 2685, 9834, 50988, 50988, 294, 264, 5622, 1622, 13, 639, 4482, 7711, 439, 264, 3956, 337, 264, 2190, 8657, 13, 639, 472, 9898, 51292, 51292, 281, 652, 257, 21120, 13, 10328, 11, 2035, 13, 639, 307, 11, 498, 291, 2826, 257, 9688, 8871, 11, 550, 291, 434, 516, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.12473862821405585, "compression_ratio": 1.5938864628820961, "no_speech_prob": 7.887174433562905e-06}, {"id": 421, "seek": 298128, "start": 2986.8, "end": 2993.76, "text": " you have an argument parser such that you can send some specific commands, specific parameters", "tokens": [50364, 2135, 721, 11, 558, 30, 407, 321, 722, 365, 43866, 257, 3840, 295, 36531, 721, 13, 1018, 7713, 11, 50640, 50640, 291, 362, 364, 6770, 21156, 260, 1270, 300, 291, 393, 2845, 512, 2685, 16901, 11, 2685, 9834, 50988, 50988, 294, 264, 5622, 1622, 13, 639, 4482, 7711, 439, 264, 3956, 337, 264, 2190, 8657, 13, 639, 472, 9898, 51292, 51292, 281, 652, 257, 21120, 13, 10328, 11, 2035, 13, 639, 307, 11, 498, 291, 2826, 257, 9688, 8871, 11, 550, 291, 434, 516, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.12473862821405585, "compression_ratio": 1.5938864628820961, "no_speech_prob": 7.887174433562905e-06}, {"id": 422, "seek": 298128, "start": 2993.76, "end": 2999.84, "text": " in the command line. This printouts all the options for the current setup. This one tries", "tokens": [50364, 2135, 721, 11, 558, 30, 407, 321, 722, 365, 43866, 257, 3840, 295, 36531, 721, 13, 1018, 7713, 11, 50640, 50640, 291, 362, 364, 6770, 21156, 260, 1270, 300, 291, 393, 2845, 512, 2685, 16901, 11, 2685, 9834, 50988, 50988, 294, 264, 5622, 1622, 13, 639, 4482, 7711, 439, 264, 3956, 337, 264, 2190, 8657, 13, 639, 472, 9898, 51292, 51292, 281, 652, 257, 21120, 13, 10328, 11, 2035, 13, 639, 307, 11, 498, 291, 2826, 257, 9688, 8871, 11, 550, 291, 434, 516, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.12473862821405585, "compression_ratio": 1.5938864628820961, "no_speech_prob": 7.887174433562905e-06}, {"id": 423, "seek": 298128, "start": 2999.84, "end": 3005.92, "text": " to make a directory. Otherwise, whatever. This is, if you choose a manual seed, then you're going", "tokens": [50364, 2135, 721, 11, 558, 30, 407, 321, 722, 365, 43866, 257, 3840, 295, 36531, 721, 13, 1018, 7713, 11, 50640, 50640, 291, 362, 364, 6770, 21156, 260, 1270, 300, 291, 393, 2845, 512, 2685, 16901, 11, 2685, 9834, 50988, 50988, 294, 264, 5622, 1622, 13, 639, 4482, 7711, 439, 264, 3956, 337, 264, 2190, 8657, 13, 639, 472, 9898, 51292, 51292, 281, 652, 257, 21120, 13, 10328, 11, 2035, 13, 639, 307, 11, 498, 291, 2826, 257, 9688, 8871, 11, 550, 291, 434, 516, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.12473862821405585, "compression_ratio": 1.5938864628820961, "no_speech_prob": 7.887174433562905e-06}, {"id": 424, "seek": 300592, "start": 3005.92, "end": 3011.12, "text": " to be actually setting a manual seed in such a way you're going to have reproducible results.", "tokens": [50364, 281, 312, 767, 3287, 257, 9688, 8871, 294, 1270, 257, 636, 291, 434, 516, 281, 362, 11408, 32128, 3542, 13, 50624, 50752, 29777, 7509, 18927, 2681, 2074, 11, 286, 519, 16411, 493, 264, 11, 1338, 11, 341, 472, 4045, 291, 281, 362, 4663, 51084, 51116, 18407, 33827, 11, 23434, 1625, 13, 759, 291, 500, 380, 362, 29777, 7509, 11, 291, 434, 516, 281, 312, 1940, 5680, 281, 3847, 341, 1507, 13, 51520, 51608], "temperature": 0.0, "avg_logprob": -0.1366406758626302, "compression_ratio": 1.4895833333333333, "no_speech_prob": 2.5852084945654497e-05}, {"id": 425, "seek": 300592, "start": 3013.6800000000003, "end": 3020.32, "text": " CUDA benchmark equal true, I think speeds up the, yeah, this one allows you to have faster", "tokens": [50364, 281, 312, 767, 3287, 257, 9688, 8871, 294, 1270, 257, 636, 291, 434, 516, 281, 362, 11408, 32128, 3542, 13, 50624, 50752, 29777, 7509, 18927, 2681, 2074, 11, 286, 519, 16411, 493, 264, 11, 1338, 11, 341, 472, 4045, 291, 281, 362, 4663, 51084, 51116, 18407, 33827, 11, 23434, 1625, 13, 759, 291, 500, 380, 362, 29777, 7509, 11, 291, 434, 516, 281, 312, 1940, 5680, 281, 3847, 341, 1507, 13, 51520, 51608], "temperature": 0.0, "avg_logprob": -0.1366406758626302, "compression_ratio": 1.4895833333333333, "no_speech_prob": 2.5852084945654497e-05}, {"id": 426, "seek": 300592, "start": 3020.96, "end": 3029.04, "text": " GPU routines, kernels. If you don't have CUDA, you're going to be taking forever to train this stuff.", "tokens": [50364, 281, 312, 767, 3287, 257, 9688, 8871, 294, 1270, 257, 636, 291, 434, 516, 281, 362, 11408, 32128, 3542, 13, 50624, 50752, 29777, 7509, 18927, 2681, 2074, 11, 286, 519, 16411, 493, 264, 11, 1338, 11, 341, 472, 4045, 291, 281, 362, 4663, 51084, 51116, 18407, 33827, 11, 23434, 1625, 13, 759, 291, 500, 380, 362, 29777, 7509, 11, 291, 434, 516, 281, 312, 1940, 5680, 281, 3847, 341, 1507, 13, 51520, 51608], "temperature": 0.0, "avg_logprob": -0.1366406758626302, "compression_ratio": 1.4895833333333333, "no_speech_prob": 2.5852084945654497e-05}, {"id": 427, "seek": 302904, "start": 3029.04, "end": 3036.64, "text": " Data root, whatever. Data set. So you're going to be loading here ImageNet folders or LFW data set.", "tokens": [50364, 11888, 5593, 11, 2035, 13, 11888, 992, 13, 407, 291, 434, 516, 281, 312, 15114, 510, 29903, 31890, 31082, 420, 441, 37, 54, 1412, 992, 13, 50744, 50780, 407, 341, 307, 439, 721, 300, 321, 1217, 458, 13, 1033, 13, 407, 426, 38, 8115, 307, 516, 281, 312, 264, 1230, 295, 18407, 293, 1176, 307, 51280, 51280], "temperature": 0.0, "avg_logprob": -0.3210957737292274, "compression_ratio": 1.3724137931034484, "no_speech_prob": 2.5845933123491704e-05}, {"id": 428, "seek": 302904, "start": 3037.36, "end": 3047.36, "text": " So this is all things that we already know. Okay. So NGPU is going to be the number of GPU and Z is", "tokens": [50364, 11888, 5593, 11, 2035, 13, 11888, 992, 13, 407, 291, 434, 516, 281, 312, 15114, 510, 29903, 31890, 31082, 420, 441, 37, 54, 1412, 992, 13, 50744, 50780, 407, 341, 307, 439, 721, 300, 321, 1217, 458, 13, 1033, 13, 407, 426, 38, 8115, 307, 516, 281, 312, 264, 1230, 295, 18407, 293, 1176, 307, 51280, 51280], "temperature": 0.0, "avg_logprob": -0.3210957737292274, "compression_ratio": 1.3724137931034484, "no_speech_prob": 2.5845933123491704e-05}, {"id": 429, "seek": 304736, "start": 3047.36, "end": 3059.52, "text": " going to be the size of the latent variable. NGF and NDF is going to be, let's see, NGF, NDF.", "tokens": [50364, 516, 281, 312, 264, 2744, 295, 264, 48994, 7006, 13, 426, 38, 37, 293, 40709, 37, 307, 516, 281, 312, 11, 718, 311, 536, 11, 426, 38, 37, 11, 40709, 37, 13, 50972, 51088, 440, 1230, 11, 286, 519, 11, 322, 264, 1337, 1166, 4122, 293, 264, 1230, 295, 20828, 1166, 4122, 13, 51284, 51376, 400, 1392, 11, 321, 362, 512, 2685, 3364, 5883, 2144, 11, 597, 534, 3665, 51596, 51628], "temperature": 0.0, "avg_logprob": -0.15483867958800435, "compression_ratio": 1.4576271186440677, "no_speech_prob": 1.6683226931490935e-05}, {"id": 430, "seek": 304736, "start": 3061.84, "end": 3065.76, "text": " The number, I think, on the generative features and the number of discriminative features.", "tokens": [50364, 516, 281, 312, 264, 2744, 295, 264, 48994, 7006, 13, 426, 38, 37, 293, 40709, 37, 307, 516, 281, 312, 11, 718, 311, 536, 11, 426, 38, 37, 11, 40709, 37, 13, 50972, 51088, 440, 1230, 11, 286, 519, 11, 322, 264, 1337, 1166, 4122, 293, 264, 1230, 295, 20828, 1166, 4122, 13, 51284, 51376, 400, 1392, 11, 321, 362, 512, 2685, 3364, 5883, 2144, 11, 597, 534, 3665, 51596, 51628], "temperature": 0.0, "avg_logprob": -0.15483867958800435, "compression_ratio": 1.4576271186440677, "no_speech_prob": 1.6683226931490935e-05}, {"id": 431, "seek": 304736, "start": 3067.6, "end": 3072.0, "text": " And okay, we have some specific weight initialization, which really helps", "tokens": [50364, 516, 281, 312, 264, 2744, 295, 264, 48994, 7006, 13, 426, 38, 37, 293, 40709, 37, 307, 516, 281, 312, 11, 718, 311, 536, 11, 426, 38, 37, 11, 40709, 37, 13, 50972, 51088, 440, 1230, 11, 286, 519, 11, 322, 264, 1337, 1166, 4122, 293, 264, 1230, 295, 20828, 1166, 4122, 13, 51284, 51376, 400, 1392, 11, 321, 362, 512, 2685, 3364, 5883, 2144, 11, 597, 534, 3665, 51596, 51628], "temperature": 0.0, "avg_logprob": -0.15483867958800435, "compression_ratio": 1.4576271186440677, "no_speech_prob": 1.6683226931490935e-05}, {"id": 432, "seek": 307200, "start": 3072.0, "end": 3078.24, "text": " getting some proper training starting. And then let's actually have a look to this generator.", "tokens": [50364, 1242, 512, 2296, 3097, 2891, 13, 400, 550, 718, 311, 767, 362, 257, 574, 281, 341, 19265, 13, 50676, 50744, 1033, 13, 407, 341, 307, 257, 13735, 293, 550, 1422, 11665, 19265, 13, 509, 500, 380, 643, 341, 1507, 498, 291, 434, 51104, 51104, 1228, 15329, 805, 13, 407, 718, 311, 536, 13, 407, 321, 362, 257, 42881, 11, 558, 30, 440, 19265, 486, 312, 493, 21179, 11, 51548, 51548, 1270, 300, 11, 382, 291, 362, 1612, 490, 264, 1036, 14578, 11, 291, 528, 281, 352, 490, 257, 1359, 10139, 281, 2089, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.21224788824717203, "compression_ratio": 1.602510460251046, "no_speech_prob": 1.917092959047295e-05}, {"id": 433, "seek": 307200, "start": 3079.6, "end": 3086.8, "text": " Okay. So this is a classical and then subclass generator. You don't need this stuff if you're", "tokens": [50364, 1242, 512, 2296, 3097, 2891, 13, 400, 550, 718, 311, 767, 362, 257, 574, 281, 341, 19265, 13, 50676, 50744, 1033, 13, 407, 341, 307, 257, 13735, 293, 550, 1422, 11665, 19265, 13, 509, 500, 380, 643, 341, 1507, 498, 291, 434, 51104, 51104, 1228, 15329, 805, 13, 407, 718, 311, 536, 13, 407, 321, 362, 257, 42881, 11, 558, 30, 440, 19265, 486, 312, 493, 21179, 11, 51548, 51548, 1270, 300, 11, 382, 291, 362, 1612, 490, 264, 1036, 14578, 11, 291, 528, 281, 352, 490, 257, 1359, 10139, 281, 2089, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.21224788824717203, "compression_ratio": 1.602510460251046, "no_speech_prob": 1.917092959047295e-05}, {"id": 434, "seek": 307200, "start": 3086.8, "end": 3095.68, "text": " using Python 3. So let's see. So we have a sequential, right? The generator will be up sampling,", "tokens": [50364, 1242, 512, 2296, 3097, 2891, 13, 400, 550, 718, 311, 767, 362, 257, 574, 281, 341, 19265, 13, 50676, 50744, 1033, 13, 407, 341, 307, 257, 13735, 293, 550, 1422, 11665, 19265, 13, 509, 500, 380, 643, 341, 1507, 498, 291, 434, 51104, 51104, 1228, 15329, 805, 13, 407, 718, 311, 536, 13, 407, 321, 362, 257, 42881, 11, 558, 30, 440, 19265, 486, 312, 493, 21179, 11, 51548, 51548, 1270, 300, 11, 382, 291, 362, 1612, 490, 264, 1036, 14578, 11, 291, 528, 281, 352, 490, 257, 1359, 10139, 281, 2089, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.21224788824717203, "compression_ratio": 1.602510460251046, "no_speech_prob": 1.917092959047295e-05}, {"id": 435, "seek": 307200, "start": 3095.68, "end": 3101.28, "text": " such that, as you have seen from the last homework, you want to go from a small dimension to allow", "tokens": [50364, 1242, 512, 2296, 3097, 2891, 13, 400, 550, 718, 311, 767, 362, 257, 574, 281, 341, 19265, 13, 50676, 50744, 1033, 13, 407, 341, 307, 257, 13735, 293, 550, 1422, 11665, 19265, 13, 509, 500, 380, 643, 341, 1507, 498, 291, 434, 51104, 51104, 1228, 15329, 805, 13, 407, 718, 311, 536, 13, 407, 321, 362, 257, 42881, 11, 558, 30, 440, 19265, 486, 312, 493, 21179, 11, 51548, 51548, 1270, 300, 11, 382, 291, 362, 1612, 490, 264, 1036, 14578, 11, 291, 528, 281, 352, 490, 257, 1359, 10139, 281, 2089, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.21224788824717203, "compression_ratio": 1.602510460251046, "no_speech_prob": 1.917092959047295e-05}, {"id": 436, "seek": 310128, "start": 3101.28, "end": 3106.1600000000003, "text": " dimension to a larger dimension, you're going to use this module. They have batch norm, relu,", "tokens": [50364, 10139, 281, 257, 4833, 10139, 11, 291, 434, 516, 281, 764, 341, 10088, 13, 814, 362, 15245, 2026, 11, 1039, 84, 11, 50608, 50608, 293, 370, 322, 11, 558, 30, 400, 25167, 45216, 11, 15245, 2026, 11, 1039, 84, 11, 293, 1066, 516, 13, 400, 2721, 11, 50932, 50932, 321, 362, 257, 7603, 71, 13, 492, 362, 257, 7603, 71, 570, 264, 5598, 294, 341, 1389, 307, 516, 281, 312, 8493, 1951, 3175, 472, 51364, 51364, 281, 1804, 472, 13, 35524, 307, 2935, 291, 2845, 264, 4846, 807, 264, 2135, 13, 400, 264, 2135, 390, 341, 472, 13, 51804, 51832], "temperature": 0.0, "avg_logprob": -0.14880884394926183, "compression_ratio": 1.731818181818182, "no_speech_prob": 6.319341628113762e-05}, {"id": 437, "seek": 310128, "start": 3106.1600000000003, "end": 3112.6400000000003, "text": " and so on, right? And transpose convolution, batch norm, relu, and keep going. And finally,", "tokens": [50364, 10139, 281, 257, 4833, 10139, 11, 291, 434, 516, 281, 764, 341, 10088, 13, 814, 362, 15245, 2026, 11, 1039, 84, 11, 50608, 50608, 293, 370, 322, 11, 558, 30, 400, 25167, 45216, 11, 15245, 2026, 11, 1039, 84, 11, 293, 1066, 516, 13, 400, 2721, 11, 50932, 50932, 321, 362, 257, 7603, 71, 13, 492, 362, 257, 7603, 71, 570, 264, 5598, 294, 341, 1389, 307, 516, 281, 312, 8493, 1951, 3175, 472, 51364, 51364, 281, 1804, 472, 13, 35524, 307, 2935, 291, 2845, 264, 4846, 807, 264, 2135, 13, 400, 264, 2135, 390, 341, 472, 13, 51804, 51832], "temperature": 0.0, "avg_logprob": -0.14880884394926183, "compression_ratio": 1.731818181818182, "no_speech_prob": 6.319341628113762e-05}, {"id": 438, "seek": 310128, "start": 3112.6400000000003, "end": 3121.28, "text": " we have a tanh. We have a tanh because the output in this case is going to be lying within minus one", "tokens": [50364, 10139, 281, 257, 4833, 10139, 11, 291, 434, 516, 281, 764, 341, 10088, 13, 814, 362, 15245, 2026, 11, 1039, 84, 11, 50608, 50608, 293, 370, 322, 11, 558, 30, 400, 25167, 45216, 11, 15245, 2026, 11, 1039, 84, 11, 293, 1066, 516, 13, 400, 2721, 11, 50932, 50932, 321, 362, 257, 7603, 71, 13, 492, 362, 257, 7603, 71, 570, 264, 5598, 294, 341, 1389, 307, 516, 281, 312, 8493, 1951, 3175, 472, 51364, 51364, 281, 1804, 472, 13, 35524, 307, 2935, 291, 2845, 264, 4846, 807, 264, 2135, 13, 400, 264, 2135, 390, 341, 472, 13, 51804, 51832], "temperature": 0.0, "avg_logprob": -0.14880884394926183, "compression_ratio": 1.731818181818182, "no_speech_prob": 6.319341628113762e-05}, {"id": 439, "seek": 310128, "start": 3121.28, "end": 3130.0800000000004, "text": " to plus one. Forward is simply you send the input through the main. And the main was this one.", "tokens": [50364, 10139, 281, 257, 4833, 10139, 11, 291, 434, 516, 281, 764, 341, 10088, 13, 814, 362, 15245, 2026, 11, 1039, 84, 11, 50608, 50608, 293, 370, 322, 11, 558, 30, 400, 25167, 45216, 11, 15245, 2026, 11, 1039, 84, 11, 293, 1066, 516, 13, 400, 2721, 11, 50932, 50932, 321, 362, 257, 7603, 71, 13, 492, 362, 257, 7603, 71, 570, 264, 5598, 294, 341, 1389, 307, 516, 281, 312, 8493, 1951, 3175, 472, 51364, 51364, 281, 1804, 472, 13, 35524, 307, 2935, 291, 2845, 264, 4846, 807, 264, 2135, 13, 400, 264, 2135, 390, 341, 472, 13, 51804, 51832], "temperature": 0.0, "avg_logprob": -0.14880884394926183, "compression_ratio": 1.731818181818182, "no_speech_prob": 6.319341628113762e-05}, {"id": 440, "seek": 313008, "start": 3130.08, "end": 3137.2, "text": " The main mode, right? This is for using data parallel if you want to use several GPUs.", "tokens": [50364, 440, 2135, 4391, 11, 558, 30, 639, 307, 337, 1228, 1412, 8952, 498, 291, 528, 281, 764, 2940, 18407, 82, 13, 50720, 50812, 400, 550, 510, 307, 577, 360, 291, 5883, 1125, 365, 264, 2685, 5883, 2144, 291, 6964, 3673, 13, 51084, 51148, 407, 2935, 445, 281, 829, 309, 294, 2099, 11, 558, 30, 708, 775, 341, 551, 360, 30, 509, 4846, 746, 510, 300, 51460, 51460], "temperature": 0.0, "avg_logprob": -0.17573038736979166, "compression_ratio": 1.483695652173913, "no_speech_prob": 2.3542752387584187e-05}, {"id": 441, "seek": 313008, "start": 3139.04, "end": 3144.48, "text": " And then here is how do you initialize with the specific initialization you define above.", "tokens": [50364, 440, 2135, 4391, 11, 558, 30, 639, 307, 337, 1228, 1412, 8952, 498, 291, 528, 281, 764, 2940, 18407, 82, 13, 50720, 50812, 400, 550, 510, 307, 577, 360, 291, 5883, 1125, 365, 264, 2685, 5883, 2144, 291, 6964, 3673, 13, 51084, 51148, 407, 2935, 445, 281, 829, 309, 294, 2099, 11, 558, 30, 708, 775, 341, 551, 360, 30, 509, 4846, 746, 510, 300, 51460, 51460], "temperature": 0.0, "avg_logprob": -0.17573038736979166, "compression_ratio": 1.483695652173913, "no_speech_prob": 2.3542752387584187e-05}, {"id": 442, "seek": 313008, "start": 3145.7599999999998, "end": 3152.0, "text": " So simply just to put it in short, right? What does this thing do? You input something here that", "tokens": [50364, 440, 2135, 4391, 11, 558, 30, 639, 307, 337, 1228, 1412, 8952, 498, 291, 528, 281, 764, 2940, 18407, 82, 13, 50720, 50812, 400, 550, 510, 307, 577, 360, 291, 5883, 1125, 365, 264, 2685, 5883, 2144, 291, 6964, 3673, 13, 51084, 51148, 407, 2935, 445, 281, 829, 309, 294, 2099, 11, 558, 30, 708, 775, 341, 551, 360, 30, 509, 4846, 746, 510, 300, 51460, 51460], "temperature": 0.0, "avg_logprob": -0.17573038736979166, "compression_ratio": 1.483695652173913, "no_speech_prob": 2.3542752387584187e-05}, {"id": 443, "seek": 315200, "start": 3152.0, "end": 3160.64, "text": " has nz size, right? And nz is the size of the latent, which is nz, nz, 100. So you input a", "tokens": [50364, 575, 297, 89, 2744, 11, 558, 30, 400, 297, 89, 307, 264, 2744, 295, 264, 48994, 11, 597, 307, 297, 89, 11, 297, 89, 11, 2319, 13, 407, 291, 4846, 257, 50796, 50796, 8062, 295, 2744, 2319, 13, 407, 309, 311, 257, 40863, 11, 257, 472, 12, 18759, 40863, 365, 2319, 2744, 13, 440, 51192, 51192, 2744, 307, 2319, 13, 400, 370, 5699, 291, 4846, 341, 2319, 8062, 11, 264, 5598, 307, 516, 281, 312, 746, 411, 51572, 51600], "temperature": 0.0, "avg_logprob": -0.0887761174896617, "compression_ratio": 1.5917159763313609, "no_speech_prob": 2.546281393733807e-05}, {"id": 444, "seek": 315200, "start": 3160.64, "end": 3168.56, "text": " vector of size 100. So it's a tensor, a one-dimensional tensor with 100 size. The", "tokens": [50364, 575, 297, 89, 2744, 11, 558, 30, 400, 297, 89, 307, 264, 2744, 295, 264, 48994, 11, 597, 307, 297, 89, 11, 297, 89, 11, 2319, 13, 407, 291, 4846, 257, 50796, 50796, 8062, 295, 2744, 2319, 13, 407, 309, 311, 257, 40863, 11, 257, 472, 12, 18759, 40863, 365, 2319, 2744, 13, 440, 51192, 51192, 2744, 307, 2319, 13, 400, 370, 5699, 291, 4846, 341, 2319, 8062, 11, 264, 5598, 307, 516, 281, 312, 746, 411, 51572, 51600], "temperature": 0.0, "avg_logprob": -0.0887761174896617, "compression_ratio": 1.5917159763313609, "no_speech_prob": 2.546281393733807e-05}, {"id": 445, "seek": 315200, "start": 3168.56, "end": 3176.16, "text": " size is 100. And so whenever you input this 100 vector, the output is going to be something like", "tokens": [50364, 575, 297, 89, 2744, 11, 558, 30, 400, 297, 89, 307, 264, 2744, 295, 264, 48994, 11, 597, 307, 297, 89, 11, 297, 89, 11, 2319, 13, 407, 291, 4846, 257, 50796, 50796, 8062, 295, 2744, 2319, 13, 407, 309, 311, 257, 40863, 11, 257, 472, 12, 18759, 40863, 365, 2319, 2744, 13, 440, 51192, 51192, 2744, 307, 2319, 13, 400, 370, 5699, 291, 4846, 341, 2319, 8062, 11, 264, 5598, 307, 516, 281, 312, 746, 411, 51572, 51600], "temperature": 0.0, "avg_logprob": -0.0887761174896617, "compression_ratio": 1.5917159763313609, "no_speech_prob": 2.546281393733807e-05}, {"id": 446, "seek": 317616, "start": 3176.16, "end": 3182.3999999999996, "text": " 64 by 64 times the number of channels in case you have color image or not, right?", "tokens": [50364, 12145, 538, 12145, 1413, 264, 1230, 295, 9235, 294, 1389, 291, 362, 2017, 3256, 420, 406, 11, 558, 30, 50676, 50796, 400, 297, 66, 11, 297, 66, 885, 264, 1230, 295, 9235, 295, 264, 4846, 3256, 11, 558, 30, 1033, 13, 467, 820, 312, 1850, 370, 51216, 51216, 1400, 11, 558, 30, 883, 3219, 721, 516, 322, 13, 961, 311, 536, 264, 1036, 644, 13, 400, 550, 718, 311, 536, 577, 436, 3847, 13, 51540, 51572, 407, 264, 20828, 1639, 307, 264, 912, 1507, 13, 509, 362, 257, 42881, 13, 682, 341, 1389, 11, 321, 3154, 51824], "temperature": 0.0, "avg_logprob": -0.20952881707085502, "compression_ratio": 1.5955555555555556, "no_speech_prob": 1.615237124497071e-05}, {"id": 447, "seek": 317616, "start": 3184.7999999999997, "end": 3193.2, "text": " And nc, nc being the number of channels of the input image, right? Okay. It should be clear so", "tokens": [50364, 12145, 538, 12145, 1413, 264, 1230, 295, 9235, 294, 1389, 291, 362, 2017, 3256, 420, 406, 11, 558, 30, 50676, 50796, 400, 297, 66, 11, 297, 66, 885, 264, 1230, 295, 9235, 295, 264, 4846, 3256, 11, 558, 30, 1033, 13, 467, 820, 312, 1850, 370, 51216, 51216, 1400, 11, 558, 30, 883, 3219, 721, 516, 322, 13, 961, 311, 536, 264, 1036, 644, 13, 400, 550, 718, 311, 536, 577, 436, 3847, 13, 51540, 51572, 407, 264, 20828, 1639, 307, 264, 912, 1507, 13, 509, 362, 257, 42881, 13, 682, 341, 1389, 11, 321, 3154, 51824], "temperature": 0.0, "avg_logprob": -0.20952881707085502, "compression_ratio": 1.5955555555555556, "no_speech_prob": 1.615237124497071e-05}, {"id": 448, "seek": 317616, "start": 3193.2, "end": 3199.68, "text": " far, right? No crazy things going on. Let's see the last part. And then let's see how they train.", "tokens": [50364, 12145, 538, 12145, 1413, 264, 1230, 295, 9235, 294, 1389, 291, 362, 2017, 3256, 420, 406, 11, 558, 30, 50676, 50796, 400, 297, 66, 11, 297, 66, 885, 264, 1230, 295, 9235, 295, 264, 4846, 3256, 11, 558, 30, 1033, 13, 467, 820, 312, 1850, 370, 51216, 51216, 1400, 11, 558, 30, 883, 3219, 721, 516, 322, 13, 961, 311, 536, 264, 1036, 644, 13, 400, 550, 718, 311, 536, 577, 436, 3847, 13, 51540, 51572, 407, 264, 20828, 1639, 307, 264, 912, 1507, 13, 509, 362, 257, 42881, 13, 682, 341, 1389, 11, 321, 3154, 51824], "temperature": 0.0, "avg_logprob": -0.20952881707085502, "compression_ratio": 1.5955555555555556, "no_speech_prob": 1.615237124497071e-05}, {"id": 449, "seek": 319968, "start": 3199.68, "end": 3206.16, "text": " So the discriminator is the same stuff. You have a sequential. In this case, we feed this whatever", "tokens": [50364, 407, 264, 20828, 1639, 307, 264, 912, 1507, 13, 509, 362, 257, 42881, 13, 682, 341, 1389, 11, 321, 3154, 341, 2035, 50688, 50688, 1230, 295, 9235, 1413, 12145, 1413, 12145, 13, 400, 550, 291, 352, 760, 365, 476, 15681, 12, 265, 338, 12, 78, 13, 876, 11, 341, 307, 1021, 13, 51000, 51000, 407, 476, 15681, 12, 265, 338, 12, 78, 294, 264, 20828, 1639, 11, 652, 988, 291, 434, 406, 516, 281, 312, 8011, 264, 16235, 51252, 51252, 498, 291, 366, 294, 264, 4458, 11, 294, 257, 3671, 4458, 11, 558, 30, 639, 307, 534, 11, 534, 1021, 13, 51472, 51472, 759, 291, 500, 380, 362, 2771, 2448, 510, 11, 550, 291, 393, 380, 3847, 264, 19265, 13, 407, 291, 1066, 516, 760, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.20599748182666394, "compression_ratio": 1.7649253731343284, "no_speech_prob": 6.687200220767409e-05}, {"id": 450, "seek": 319968, "start": 3206.16, "end": 3212.3999999999996, "text": " number of channels times 64 times 64. And then you go down with leaky-reel-o. Oh, this is important.", "tokens": [50364, 407, 264, 20828, 1639, 307, 264, 912, 1507, 13, 509, 362, 257, 42881, 13, 682, 341, 1389, 11, 321, 3154, 341, 2035, 50688, 50688, 1230, 295, 9235, 1413, 12145, 1413, 12145, 13, 400, 550, 291, 352, 760, 365, 476, 15681, 12, 265, 338, 12, 78, 13, 876, 11, 341, 307, 1021, 13, 51000, 51000, 407, 476, 15681, 12, 265, 338, 12, 78, 294, 264, 20828, 1639, 11, 652, 988, 291, 434, 406, 516, 281, 312, 8011, 264, 16235, 51252, 51252, 498, 291, 366, 294, 264, 4458, 11, 294, 257, 3671, 4458, 11, 558, 30, 639, 307, 534, 11, 534, 1021, 13, 51472, 51472, 759, 291, 500, 380, 362, 2771, 2448, 510, 11, 550, 291, 393, 380, 3847, 264, 19265, 13, 407, 291, 1066, 516, 760, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.20599748182666394, "compression_ratio": 1.7649253731343284, "no_speech_prob": 6.687200220767409e-05}, {"id": 451, "seek": 319968, "start": 3212.3999999999996, "end": 3217.44, "text": " So leaky-reel-o in the discriminator, make sure you're not going to be killing the gradient", "tokens": [50364, 407, 264, 20828, 1639, 307, 264, 912, 1507, 13, 509, 362, 257, 42881, 13, 682, 341, 1389, 11, 321, 3154, 341, 2035, 50688, 50688, 1230, 295, 9235, 1413, 12145, 1413, 12145, 13, 400, 550, 291, 352, 760, 365, 476, 15681, 12, 265, 338, 12, 78, 13, 876, 11, 341, 307, 1021, 13, 51000, 51000, 407, 476, 15681, 12, 265, 338, 12, 78, 294, 264, 20828, 1639, 11, 652, 988, 291, 434, 406, 516, 281, 312, 8011, 264, 16235, 51252, 51252, 498, 291, 366, 294, 264, 4458, 11, 294, 257, 3671, 4458, 11, 558, 30, 639, 307, 534, 11, 534, 1021, 13, 51472, 51472, 759, 291, 500, 380, 362, 2771, 2448, 510, 11, 550, 291, 393, 380, 3847, 264, 19265, 13, 407, 291, 1066, 516, 760, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.20599748182666394, "compression_ratio": 1.7649253731343284, "no_speech_prob": 6.687200220767409e-05}, {"id": 452, "seek": 319968, "start": 3217.44, "end": 3221.8399999999997, "text": " if you are in the region, in a negative region, right? This is really, really important.", "tokens": [50364, 407, 264, 20828, 1639, 307, 264, 912, 1507, 13, 509, 362, 257, 42881, 13, 682, 341, 1389, 11, 321, 3154, 341, 2035, 50688, 50688, 1230, 295, 9235, 1413, 12145, 1413, 12145, 13, 400, 550, 291, 352, 760, 365, 476, 15681, 12, 265, 338, 12, 78, 13, 876, 11, 341, 307, 1021, 13, 51000, 51000, 407, 476, 15681, 12, 265, 338, 12, 78, 294, 264, 20828, 1639, 11, 652, 988, 291, 434, 406, 516, 281, 312, 8011, 264, 16235, 51252, 51252, 498, 291, 366, 294, 264, 4458, 11, 294, 257, 3671, 4458, 11, 558, 30, 639, 307, 534, 11, 534, 1021, 13, 51472, 51472, 759, 291, 500, 380, 362, 2771, 2448, 510, 11, 550, 291, 393, 380, 3847, 264, 19265, 13, 407, 291, 1066, 516, 760, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.20599748182666394, "compression_ratio": 1.7649253731343284, "no_speech_prob": 6.687200220767409e-05}, {"id": 453, "seek": 319968, "start": 3221.8399999999997, "end": 3228.7999999999997, "text": " If you don't have gradients here, then you can't train the generator. So you keep going down", "tokens": [50364, 407, 264, 20828, 1639, 307, 264, 912, 1507, 13, 509, 362, 257, 42881, 13, 682, 341, 1389, 11, 321, 3154, 341, 2035, 50688, 50688, 1230, 295, 9235, 1413, 12145, 1413, 12145, 13, 400, 550, 291, 352, 760, 365, 476, 15681, 12, 265, 338, 12, 78, 13, 876, 11, 341, 307, 1021, 13, 51000, 51000, 407, 476, 15681, 12, 265, 338, 12, 78, 294, 264, 20828, 1639, 11, 652, 988, 291, 434, 406, 516, 281, 312, 8011, 264, 16235, 51252, 51252, 498, 291, 366, 294, 264, 4458, 11, 294, 257, 3671, 4458, 11, 558, 30, 639, 307, 534, 11, 534, 1021, 13, 51472, 51472, 759, 291, 500, 380, 362, 2771, 2448, 510, 11, 550, 291, 393, 380, 3847, 264, 19265, 13, 407, 291, 1066, 516, 760, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.20599748182666394, "compression_ratio": 1.7649253731343284, "no_speech_prob": 6.687200220767409e-05}, {"id": 454, "seek": 322880, "start": 3228.8, "end": 3233.6000000000004, "text": " like that. And then finally, they use a sigmoid because they train this stuff as like a", "tokens": [50364, 411, 300, 13, 400, 550, 2721, 11, 436, 764, 257, 4556, 3280, 327, 570, 436, 3847, 341, 1507, 382, 411, 257, 50604, 50604, 20828, 1639, 11, 411, 257, 1508, 9902, 1296, 732, 5359, 13, 400, 264, 2128, 307, 2935, 291, 2845, 1507, 50952, 50952, 807, 264, 2135, 9819, 293, 436, 5883, 1125, 341, 3209, 13, 407, 321, 362, 2533, 67, 293, 2533, 70, 13, 51352, 51400, 407, 341, 11420, 307, 4748, 819, 490, 437, 291, 645, 516, 670, 949, 11, 558, 30, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.23934958701909975, "compression_ratio": 1.640552995391705, "no_speech_prob": 7.644456673006061e-06}, {"id": 455, "seek": 322880, "start": 3233.6000000000004, "end": 3240.5600000000004, "text": " discriminator, like a classifier between two classes. And the forward is simply you send stuff", "tokens": [50364, 411, 300, 13, 400, 550, 2721, 11, 436, 764, 257, 4556, 3280, 327, 570, 436, 3847, 341, 1507, 382, 411, 257, 50604, 50604, 20828, 1639, 11, 411, 257, 1508, 9902, 1296, 732, 5359, 13, 400, 264, 2128, 307, 2935, 291, 2845, 1507, 50952, 50952, 807, 264, 2135, 9819, 293, 436, 5883, 1125, 341, 3209, 13, 407, 321, 362, 2533, 67, 293, 2533, 70, 13, 51352, 51400, 407, 341, 11420, 307, 4748, 819, 490, 437, 291, 645, 516, 670, 949, 11, 558, 30, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.23934958701909975, "compression_ratio": 1.640552995391705, "no_speech_prob": 7.644456673006061e-06}, {"id": 456, "seek": 322880, "start": 3240.5600000000004, "end": 3248.5600000000004, "text": " through the main branch and they initialize this network. So we have netd and netg.", "tokens": [50364, 411, 300, 13, 400, 550, 2721, 11, 436, 764, 257, 4556, 3280, 327, 570, 436, 3847, 341, 1507, 382, 411, 257, 50604, 50604, 20828, 1639, 11, 411, 257, 1508, 9902, 1296, 732, 5359, 13, 400, 264, 2128, 307, 2935, 291, 2845, 1507, 50952, 50952, 807, 264, 2135, 9819, 293, 436, 5883, 1125, 341, 3209, 13, 407, 321, 362, 2533, 67, 293, 2533, 70, 13, 51352, 51400, 407, 341, 11420, 307, 4748, 819, 490, 437, 291, 645, 516, 670, 949, 11, 558, 30, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.23934958701909975, "compression_ratio": 1.640552995391705, "no_speech_prob": 7.644456673006061e-06}, {"id": 457, "seek": 322880, "start": 3249.52, "end": 3255.92, "text": " So this implementation is slightly different from what you were going over before, right?", "tokens": [50364, 411, 300, 13, 400, 550, 2721, 11, 436, 764, 257, 4556, 3280, 327, 570, 436, 3847, 341, 1507, 382, 411, 257, 50604, 50604, 20828, 1639, 11, 411, 257, 1508, 9902, 1296, 732, 5359, 13, 400, 264, 2128, 307, 2935, 291, 2845, 1507, 50952, 50952, 807, 264, 2135, 9819, 293, 436, 5883, 1125, 341, 3209, 13, 407, 321, 362, 2533, 67, 293, 2533, 70, 13, 51352, 51400, 407, 341, 11420, 307, 4748, 819, 490, 437, 291, 645, 516, 670, 949, 11, 558, 30, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.23934958701909975, "compression_ratio": 1.640552995391705, "no_speech_prob": 7.644456673006061e-06}, {"id": 458, "seek": 325592, "start": 3255.92, "end": 3262.16, "text": " Because the discriminator is just one. It outputs like the sigmoid.", "tokens": [50364, 1436, 264, 20828, 1639, 307, 445, 472, 13, 467, 23930, 411, 264, 4556, 3280, 327, 13, 50676, 50728, 440, 787, 2649, 307, 341, 1622, 510, 13, 1779, 13, 407, 1400, 13, 407, 294, 264, 721, 321, 645, 1417, 294, 264, 51196, 51196, 7991, 445, 949, 11, 321, 500, 380, 362, 264, 4556, 3280, 327, 13, 492, 445, 362, 341, 2572, 45216, 4583, 13, 1033, 13, 51500, 51536, 1033, 11, 658, 4413, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.30689631010356705, "compression_ratio": 1.4945054945054945, "no_speech_prob": 1.1658395123959053e-05}, {"id": 459, "seek": 325592, "start": 3263.2000000000003, "end": 3272.56, "text": " The only difference is this line here. Right. So far. So in the things we were talking in the", "tokens": [50364, 1436, 264, 20828, 1639, 307, 445, 472, 13, 467, 23930, 411, 264, 4556, 3280, 327, 13, 50676, 50728, 440, 787, 2649, 307, 341, 1622, 510, 13, 1779, 13, 407, 1400, 13, 407, 294, 264, 721, 321, 645, 1417, 294, 264, 51196, 51196, 7991, 445, 949, 11, 321, 500, 380, 362, 264, 4556, 3280, 327, 13, 492, 445, 362, 341, 2572, 45216, 4583, 13, 1033, 13, 51500, 51536, 1033, 11, 658, 4413, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.30689631010356705, "compression_ratio": 1.4945054945054945, "no_speech_prob": 1.1658395123959053e-05}, {"id": 460, "seek": 325592, "start": 3272.56, "end": 3278.64, "text": " lecture just before, we don't have the sigmoid. We just have this final convolution layer. Okay.", "tokens": [50364, 1436, 264, 20828, 1639, 307, 445, 472, 13, 467, 23930, 411, 264, 4556, 3280, 327, 13, 50676, 50728, 440, 787, 2649, 307, 341, 1622, 510, 13, 1779, 13, 407, 1400, 13, 407, 294, 264, 721, 321, 645, 1417, 294, 264, 51196, 51196, 7991, 445, 949, 11, 321, 500, 380, 362, 264, 4556, 3280, 327, 13, 492, 445, 362, 341, 2572, 45216, 4583, 13, 1033, 13, 51500, 51536, 1033, 11, 658, 4413, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.30689631010356705, "compression_ratio": 1.4945054945054945, "no_speech_prob": 1.1658395123959053e-05}, {"id": 461, "seek": 325592, "start": 3279.36, "end": 3280.16, "text": " Okay, gotcha.", "tokens": [50364, 1436, 264, 20828, 1639, 307, 445, 472, 13, 467, 23930, 411, 264, 4556, 3280, 327, 13, 50676, 50728, 440, 787, 2649, 307, 341, 1622, 510, 13, 1779, 13, 407, 1400, 13, 407, 294, 264, 721, 321, 645, 1417, 294, 264, 51196, 51196, 7991, 445, 949, 11, 321, 500, 380, 362, 264, 4556, 3280, 327, 13, 492, 445, 362, 341, 2572, 45216, 4583, 13, 1033, 13, 51500, 51536, 1033, 11, 658, 4413, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.30689631010356705, "compression_ratio": 1.4945054945054945, "no_speech_prob": 1.1658395123959053e-05}, {"id": 462, "seek": 328016, "start": 3280.16, "end": 3288.0, "text": " Second, of course, second difference is that we would not be using a binary cross entropy loss.", "tokens": [50364, 5736, 11, 295, 1164, 11, 1150, 2649, 307, 300, 321, 576, 406, 312, 1228, 257, 17434, 3278, 30867, 4470, 13, 50756, 50756, 639, 307, 264, 4009, 295, 439, 1073, 4174, 11, 558, 30, 49369, 1804, 341, 4556, 3280, 327, 11, 309, 311, 2085, 636, 295, 3097, 257, 51240, 51336, 1337, 1166, 17641, 44745, 3209, 11, 257, 19265, 13, 1033, 13, 51444, 51512, 407, 26924, 11, 321, 352, 365, 264, 2135, 37642, 510, 13, 407, 718, 311, 536, 577, 309, 1985, 13, 51712, 51760], "temperature": 0.0, "avg_logprob": -0.1472447451423196, "compression_ratio": 1.4814814814814814, "no_speech_prob": 1.6955938917817548e-05}, {"id": 463, "seek": 328016, "start": 3288.0, "end": 3297.68, "text": " This is the source of all evils, right? BCE plus this sigmoid, it's wrong way of training a", "tokens": [50364, 5736, 11, 295, 1164, 11, 1150, 2649, 307, 300, 321, 576, 406, 312, 1228, 257, 17434, 3278, 30867, 4470, 13, 50756, 50756, 639, 307, 264, 4009, 295, 439, 1073, 4174, 11, 558, 30, 49369, 1804, 341, 4556, 3280, 327, 11, 309, 311, 2085, 636, 295, 3097, 257, 51240, 51336, 1337, 1166, 17641, 44745, 3209, 11, 257, 19265, 13, 1033, 13, 51444, 51512, 407, 26924, 11, 321, 352, 365, 264, 2135, 37642, 510, 13, 407, 718, 311, 536, 577, 309, 1985, 13, 51712, 51760], "temperature": 0.0, "avg_logprob": -0.1472447451423196, "compression_ratio": 1.4814814814814814, "no_speech_prob": 1.6955938917817548e-05}, {"id": 464, "seek": 328016, "start": 3299.6, "end": 3301.7599999999998, "text": " generative adversarial network, a generator. Okay.", "tokens": [50364, 5736, 11, 295, 1164, 11, 1150, 2649, 307, 300, 321, 576, 406, 312, 1228, 257, 17434, 3278, 30867, 4470, 13, 50756, 50756, 639, 307, 264, 4009, 295, 439, 1073, 4174, 11, 558, 30, 49369, 1804, 341, 4556, 3280, 327, 11, 309, 311, 2085, 636, 295, 3097, 257, 51240, 51336, 1337, 1166, 17641, 44745, 3209, 11, 257, 19265, 13, 1033, 13, 51444, 51512, 407, 26924, 11, 321, 352, 365, 264, 2135, 37642, 510, 13, 407, 718, 311, 536, 577, 309, 1985, 13, 51712, 51760], "temperature": 0.0, "avg_logprob": -0.1472447451423196, "compression_ratio": 1.4814814814814814, "no_speech_prob": 1.6955938917817548e-05}, {"id": 465, "seek": 328016, "start": 3303.12, "end": 3307.12, "text": " So nevertheless, we go with the main formulation here. So let's see how it works.", "tokens": [50364, 5736, 11, 295, 1164, 11, 1150, 2649, 307, 300, 321, 576, 406, 312, 1228, 257, 17434, 3278, 30867, 4470, 13, 50756, 50756, 639, 307, 264, 4009, 295, 439, 1073, 4174, 11, 558, 30, 49369, 1804, 341, 4556, 3280, 327, 11, 309, 311, 2085, 636, 295, 3097, 257, 51240, 51336, 1337, 1166, 17641, 44745, 3209, 11, 257, 19265, 13, 1033, 13, 51444, 51512, 407, 26924, 11, 321, 352, 365, 264, 2135, 37642, 510, 13, 407, 718, 311, 536, 577, 309, 1985, 13, 51712, 51760], "temperature": 0.0, "avg_logprob": -0.1472447451423196, "compression_ratio": 1.4814814814814814, "no_speech_prob": 1.6955938917817548e-05}, {"id": 466, "seek": 330712, "start": 3307.12, "end": 3313.92, "text": " Fixed noise, you just create some random stuff with the batch size and the correct size here.", "tokens": [50364, 25538, 292, 5658, 11, 291, 445, 1884, 512, 4974, 1507, 365, 264, 15245, 2744, 293, 264, 3006, 2744, 510, 13, 50704, 50744, 492, 362, 732, 5028, 22525, 11, 472, 5028, 6545, 337, 264, 20828, 1639, 11, 472, 5028, 6545, 337, 264, 19265, 13, 51032, 51116, 400, 718, 311, 536, 437, 366, 264, 1732, 4439, 300, 291, 820, 439, 458, 11, 558, 30, 407, 718, 311, 2573, 484, 13, 51464, 51512, 2386, 295, 439, 11, 321, 4018, 264, 2771, 2448, 295, 264, 20828, 1639, 13, 51668, 51728], "temperature": 0.0, "avg_logprob": -0.21926481073552911, "compression_ratio": 1.6519607843137254, "no_speech_prob": 3.4527245588833466e-05}, {"id": 467, "seek": 330712, "start": 3314.72, "end": 3320.48, "text": " We have two optimizers, one optimizer for the discriminator, one optimizer for the generator.", "tokens": [50364, 25538, 292, 5658, 11, 291, 445, 1884, 512, 4974, 1507, 365, 264, 15245, 2744, 293, 264, 3006, 2744, 510, 13, 50704, 50744, 492, 362, 732, 5028, 22525, 11, 472, 5028, 6545, 337, 264, 20828, 1639, 11, 472, 5028, 6545, 337, 264, 19265, 13, 51032, 51116, 400, 718, 311, 536, 437, 366, 264, 1732, 4439, 300, 291, 820, 439, 458, 11, 558, 30, 407, 718, 311, 2573, 484, 13, 51464, 51512, 2386, 295, 439, 11, 321, 4018, 264, 2771, 2448, 295, 264, 20828, 1639, 13, 51668, 51728], "temperature": 0.0, "avg_logprob": -0.21926481073552911, "compression_ratio": 1.6519607843137254, "no_speech_prob": 3.4527245588833466e-05}, {"id": 468, "seek": 330712, "start": 3322.16, "end": 3329.12, "text": " And let's see what are the five steps that you should all know, right? So let's figure out.", "tokens": [50364, 25538, 292, 5658, 11, 291, 445, 1884, 512, 4974, 1507, 365, 264, 15245, 2744, 293, 264, 3006, 2744, 510, 13, 50704, 50744, 492, 362, 732, 5028, 22525, 11, 472, 5028, 6545, 337, 264, 20828, 1639, 11, 472, 5028, 6545, 337, 264, 19265, 13, 51032, 51116, 400, 718, 311, 536, 437, 366, 264, 1732, 4439, 300, 291, 820, 439, 458, 11, 558, 30, 407, 718, 311, 2573, 484, 13, 51464, 51512, 2386, 295, 439, 11, 321, 4018, 264, 2771, 2448, 295, 264, 20828, 1639, 13, 51668, 51728], "temperature": 0.0, "avg_logprob": -0.21926481073552911, "compression_ratio": 1.6519607843137254, "no_speech_prob": 3.4527245588833466e-05}, {"id": 469, "seek": 330712, "start": 3330.08, "end": 3333.2, "text": " First of all, we zero the gradients of the discriminator.", "tokens": [50364, 25538, 292, 5658, 11, 291, 445, 1884, 512, 4974, 1507, 365, 264, 15245, 2744, 293, 264, 3006, 2744, 510, 13, 50704, 50744, 492, 362, 732, 5028, 22525, 11, 472, 5028, 6545, 337, 264, 20828, 1639, 11, 472, 5028, 6545, 337, 264, 19265, 13, 51032, 51116, 400, 718, 311, 536, 437, 366, 264, 1732, 4439, 300, 291, 820, 439, 458, 11, 558, 30, 407, 718, 311, 2573, 484, 13, 51464, 51512, 2386, 295, 439, 11, 321, 4018, 264, 2771, 2448, 295, 264, 20828, 1639, 13, 51668, 51728], "temperature": 0.0, "avg_logprob": -0.21926481073552911, "compression_ratio": 1.6519607843137254, "no_speech_prob": 3.4527245588833466e-05}, {"id": 470, "seek": 333320, "start": 3333.2, "end": 3342.7999999999997, "text": " Okay. So now we have the real data is going to be the data zero that comes from the data loader.", "tokens": [50364, 1033, 13, 407, 586, 321, 362, 264, 957, 1412, 307, 516, 281, 312, 264, 1412, 4018, 300, 1487, 490, 264, 1412, 3677, 260, 13, 50844, 50844, 1779, 13, 407, 321, 362, 957, 1412, 510, 13, 400, 550, 321, 366, 516, 281, 312, 1419, 51136, 51212, 257, 992, 295, 16949, 11, 597, 366, 516, 281, 312, 264, 957, 16949, 13, 1033, 13, 51412, 51496, 407, 550, 321, 362, 264, 3209, 11, 264, 51600, 51644], "temperature": 0.0, "avg_logprob": -0.35033437093098957, "compression_ratio": 1.793103448275862, "no_speech_prob": 1.5285108020179905e-05}, {"id": 471, "seek": 333320, "start": 3342.7999999999997, "end": 3348.64, "text": " Right. So we have real data here. And then we are going to be having", "tokens": [50364, 1033, 13, 407, 586, 321, 362, 264, 957, 1412, 307, 516, 281, 312, 264, 1412, 4018, 300, 1487, 490, 264, 1412, 3677, 260, 13, 50844, 50844, 1779, 13, 407, 321, 362, 957, 1412, 510, 13, 400, 550, 321, 366, 516, 281, 312, 1419, 51136, 51212, 257, 992, 295, 16949, 11, 597, 366, 516, 281, 312, 264, 957, 16949, 13, 1033, 13, 51412, 51496, 407, 550, 321, 362, 264, 3209, 11, 264, 51600, 51644], "temperature": 0.0, "avg_logprob": -0.35033437093098957, "compression_ratio": 1.793103448275862, "no_speech_prob": 1.5285108020179905e-05}, {"id": 472, "seek": 333320, "start": 3350.16, "end": 3354.16, "text": " a set of labels, which are going to be the real labels. Okay.", "tokens": [50364, 1033, 13, 407, 586, 321, 362, 264, 957, 1412, 307, 516, 281, 312, 264, 1412, 4018, 300, 1487, 490, 264, 1412, 3677, 260, 13, 50844, 50844, 1779, 13, 407, 321, 362, 957, 1412, 510, 13, 400, 550, 321, 366, 516, 281, 312, 1419, 51136, 51212, 257, 992, 295, 16949, 11, 597, 366, 516, 281, 312, 264, 957, 16949, 13, 1033, 13, 51412, 51496, 407, 550, 321, 362, 264, 3209, 11, 264, 51600, 51644], "temperature": 0.0, "avg_logprob": -0.35033437093098957, "compression_ratio": 1.793103448275862, "no_speech_prob": 1.5285108020179905e-05}, {"id": 473, "seek": 333320, "start": 3355.8399999999997, "end": 3357.9199999999996, "text": " So then we have the network, the", "tokens": [50364, 1033, 13, 407, 586, 321, 362, 264, 957, 1412, 307, 516, 281, 312, 264, 1412, 4018, 300, 1487, 490, 264, 1412, 3677, 260, 13, 50844, 50844, 1779, 13, 407, 321, 362, 957, 1412, 510, 13, 400, 550, 321, 366, 516, 281, 312, 1419, 51136, 51212, 257, 992, 295, 16949, 11, 597, 366, 516, 281, 312, 264, 957, 16949, 13, 1033, 13, 51412, 51496, 407, 550, 321, 362, 264, 3209, 11, 264, 51600, 51644], "temperature": 0.0, "avg_logprob": -0.35033437093098957, "compression_ratio": 1.793103448275862, "no_speech_prob": 1.5285108020179905e-05}, {"id": 474, "seek": 335792, "start": 3357.92, "end": 3362.96, "text": " discriminator is going to be fed with the real input. And then we have some real output. Right.", "tokens": [50364, 20828, 1639, 307, 516, 281, 312, 4636, 365, 264, 957, 4846, 13, 400, 550, 321, 362, 512, 957, 5598, 13, 1779, 13, 50616, 50616, 400, 550, 291, 434, 516, 281, 312, 15866, 264, 700, 644, 11, 597, 307, 516, 281, 312, 264, 46691, 11, 50804, 50804, 597, 307, 264, 17434, 3278, 30867, 1296, 264, 5598, 337, 5699, 321, 829, 264, 957, 4846, 293, 51164, 51164, 264, 957, 7645, 13, 1033, 13, 400, 550, 321, 2042, 264, 700, 1823, 13, 407, 510, 321, 2042, 23897, 13, 51532, 51600], "temperature": 0.0, "avg_logprob": -0.203808752338538, "compression_ratio": 1.8477157360406091, "no_speech_prob": 5.715095903724432e-05}, {"id": 475, "seek": 335792, "start": 3362.96, "end": 3366.7200000000003, "text": " And then you're going to be computing the first part, which is going to be the criterion,", "tokens": [50364, 20828, 1639, 307, 516, 281, 312, 4636, 365, 264, 957, 4846, 13, 400, 550, 321, 362, 512, 957, 5598, 13, 1779, 13, 50616, 50616, 400, 550, 291, 434, 516, 281, 312, 15866, 264, 700, 644, 11, 597, 307, 516, 281, 312, 264, 46691, 11, 50804, 50804, 597, 307, 264, 17434, 3278, 30867, 1296, 264, 5598, 337, 5699, 321, 829, 264, 957, 4846, 293, 51164, 51164, 264, 957, 7645, 13, 1033, 13, 400, 550, 321, 2042, 264, 700, 1823, 13, 407, 510, 321, 2042, 23897, 13, 51532, 51600], "temperature": 0.0, "avg_logprob": -0.203808752338538, "compression_ratio": 1.8477157360406091, "no_speech_prob": 5.715095903724432e-05}, {"id": 476, "seek": 335792, "start": 3366.7200000000003, "end": 3373.92, "text": " which is the binary cross entropy between the output for whenever we put the real input and", "tokens": [50364, 20828, 1639, 307, 516, 281, 312, 4636, 365, 264, 957, 4846, 13, 400, 550, 321, 362, 512, 957, 5598, 13, 1779, 13, 50616, 50616, 400, 550, 291, 434, 516, 281, 312, 15866, 264, 700, 644, 11, 597, 307, 516, 281, 312, 264, 46691, 11, 50804, 50804, 597, 307, 264, 17434, 3278, 30867, 1296, 264, 5598, 337, 5699, 321, 829, 264, 957, 4846, 293, 51164, 51164, 264, 957, 7645, 13, 1033, 13, 400, 550, 321, 2042, 264, 700, 1823, 13, 407, 510, 321, 2042, 23897, 13, 51532, 51600], "temperature": 0.0, "avg_logprob": -0.203808752338538, "compression_ratio": 1.8477157360406091, "no_speech_prob": 5.715095903724432e-05}, {"id": 477, "seek": 335792, "start": 3373.92, "end": 3381.28, "text": " the real label. Okay. And then we perform the first step. So here we perform backward.", "tokens": [50364, 20828, 1639, 307, 516, 281, 312, 4636, 365, 264, 957, 4846, 13, 400, 550, 321, 362, 512, 957, 5598, 13, 1779, 13, 50616, 50616, 400, 550, 291, 434, 516, 281, 312, 15866, 264, 700, 644, 11, 597, 307, 516, 281, 312, 264, 46691, 11, 50804, 50804, 597, 307, 264, 17434, 3278, 30867, 1296, 264, 5598, 337, 5699, 321, 829, 264, 957, 4846, 293, 51164, 51164, 264, 957, 7645, 13, 1033, 13, 400, 550, 321, 2042, 264, 700, 1823, 13, 407, 510, 321, 2042, 23897, 13, 51532, 51600], "temperature": 0.0, "avg_logprob": -0.203808752338538, "compression_ratio": 1.8477157360406091, "no_speech_prob": 5.715095903724432e-05}, {"id": 478, "seek": 338128, "start": 3381.28, "end": 3386.6400000000003, "text": " In these criterion, which is computing the partial derivative of these binary cross entropy", "tokens": [50364, 682, 613, 46691, 11, 597, 307, 15866, 264, 14641, 13760, 295, 613, 17434, 3278, 30867, 50632, 50632, 365, 3104, 281, 264, 17443, 295, 264, 20828, 1639, 13, 1133, 321, 4636, 264, 957, 1412, 281, 264, 20828, 1639, 11, 50936, 50936, 293, 321, 5598, 11, 321, 853, 281, 2995, 264, 16949, 11, 597, 366, 264, 957, 16949, 13, 1033, 13, 639, 700, 935, 11, 51240, 51240, 1230, 472, 13, 1033, 13, 5527, 294, 1575, 13, 5736, 644, 13, 5736, 644, 307, 516, 281, 312, 291, 483, 5658, 11, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.23520289410601605, "compression_ratio": 1.6818181818181819, "no_speech_prob": 7.255133823491633e-05}, {"id": 479, "seek": 338128, "start": 3386.6400000000003, "end": 3392.7200000000003, "text": " with respect to the weights of the discriminator. When we fed the real data to the discriminator,", "tokens": [50364, 682, 613, 46691, 11, 597, 307, 15866, 264, 14641, 13760, 295, 613, 17434, 3278, 30867, 50632, 50632, 365, 3104, 281, 264, 17443, 295, 264, 20828, 1639, 13, 1133, 321, 4636, 264, 957, 1412, 281, 264, 20828, 1639, 11, 50936, 50936, 293, 321, 5598, 11, 321, 853, 281, 2995, 264, 16949, 11, 597, 366, 264, 957, 16949, 13, 1033, 13, 639, 700, 935, 11, 51240, 51240, 1230, 472, 13, 1033, 13, 5527, 294, 1575, 13, 5736, 644, 13, 5736, 644, 307, 516, 281, 312, 291, 483, 5658, 11, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.23520289410601605, "compression_ratio": 1.6818181818181819, "no_speech_prob": 7.255133823491633e-05}, {"id": 480, "seek": 338128, "start": 3392.7200000000003, "end": 3398.8, "text": " and we output, we try to match the labels, which are the real labels. Okay. This first point,", "tokens": [50364, 682, 613, 46691, 11, 597, 307, 15866, 264, 14641, 13760, 295, 613, 17434, 3278, 30867, 50632, 50632, 365, 3104, 281, 264, 17443, 295, 264, 20828, 1639, 13, 1133, 321, 4636, 264, 957, 1412, 281, 264, 20828, 1639, 11, 50936, 50936, 293, 321, 5598, 11, 321, 853, 281, 2995, 264, 16949, 11, 597, 366, 264, 957, 16949, 13, 1033, 13, 639, 700, 935, 11, 51240, 51240, 1230, 472, 13, 1033, 13, 5527, 294, 1575, 13, 5736, 644, 13, 5736, 644, 307, 516, 281, 312, 291, 483, 5658, 11, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.23520289410601605, "compression_ratio": 1.6818181818181819, "no_speech_prob": 7.255133823491633e-05}, {"id": 481, "seek": 338128, "start": 3398.8, "end": 3404.8, "text": " number one. Okay. Keep in mind. Second part. Second part is going to be you get noise,", "tokens": [50364, 682, 613, 46691, 11, 597, 307, 15866, 264, 14641, 13760, 295, 613, 17434, 3278, 30867, 50632, 50632, 365, 3104, 281, 264, 17443, 295, 264, 20828, 1639, 13, 1133, 321, 4636, 264, 957, 1412, 281, 264, 20828, 1639, 11, 50936, 50936, 293, 321, 5598, 11, 321, 853, 281, 2995, 264, 16949, 11, 597, 366, 264, 957, 16949, 13, 1033, 13, 639, 700, 935, 11, 51240, 51240, 1230, 472, 13, 1033, 13, 5527, 294, 1575, 13, 5736, 644, 13, 5736, 644, 307, 516, 281, 312, 291, 483, 5658, 11, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.23520289410601605, "compression_ratio": 1.6818181818181819, "no_speech_prob": 7.255133823491633e-05}, {"id": 482, "seek": 340480, "start": 3404.8, "end": 3412.8, "text": " and therefore you get your network, your generator, you feed some noise inside the generator.", "tokens": [50364, 293, 4412, 291, 483, 428, 3209, 11, 428, 19265, 11, 291, 3154, 512, 5658, 1854, 264, 19265, 13, 50764, 50804, 7504, 291, 483, 512, 7592, 5598, 13, 1692, 11, 286, 478, 516, 281, 312, 1419, 452, 16949, 13, 823, 366, 6412, 365, 51100, 51100, 264, 7592, 7645, 13, 1033, 13, 7504, 291, 3154, 264, 1507, 1854, 264, 20828, 1639, 13, 492, 3154, 264, 7592, 51508, 51508], "temperature": 0.0, "avg_logprob": -0.22593369203455307, "compression_ratio": 1.7048192771084338, "no_speech_prob": 5.375852560973726e-05}, {"id": 483, "seek": 340480, "start": 3413.6000000000004, "end": 3419.52, "text": " Therefore you get some fake output. Here, I'm going to be having my labels. Now are filled with", "tokens": [50364, 293, 4412, 291, 483, 428, 3209, 11, 428, 19265, 11, 291, 3154, 512, 5658, 1854, 264, 19265, 13, 50764, 50804, 7504, 291, 483, 512, 7592, 5598, 13, 1692, 11, 286, 478, 516, 281, 312, 1419, 452, 16949, 13, 823, 366, 6412, 365, 51100, 51100, 264, 7592, 7645, 13, 1033, 13, 7504, 291, 3154, 264, 1507, 1854, 264, 20828, 1639, 13, 492, 3154, 264, 7592, 51508, 51508], "temperature": 0.0, "avg_logprob": -0.22593369203455307, "compression_ratio": 1.7048192771084338, "no_speech_prob": 5.375852560973726e-05}, {"id": 484, "seek": 340480, "start": 3419.52, "end": 3427.6800000000003, "text": " the fake label. Okay. Therefore you feed the stuff inside the discriminator. We feed the fake", "tokens": [50364, 293, 4412, 291, 483, 428, 3209, 11, 428, 19265, 11, 291, 3154, 512, 5658, 1854, 264, 19265, 13, 50764, 50804, 7504, 291, 483, 512, 7592, 5598, 13, 1692, 11, 286, 478, 516, 281, 312, 1419, 452, 16949, 13, 823, 366, 6412, 365, 51100, 51100, 264, 7592, 7645, 13, 1033, 13, 7504, 291, 3154, 264, 1507, 1854, 264, 20828, 1639, 13, 492, 3154, 264, 7592, 51508, 51508], "temperature": 0.0, "avg_logprob": -0.22593369203455307, "compression_ratio": 1.7048192771084338, "no_speech_prob": 5.375852560973726e-05}, {"id": 485, "seek": 342768, "start": 3427.68, "end": 3435.2, "text": " data, but we detach, right? This is the important part. So right now we fed the fake data, but we", "tokens": [50364, 1412, 11, 457, 321, 43245, 11, 558, 30, 639, 307, 264, 1021, 644, 13, 407, 558, 586, 321, 4636, 264, 7592, 1412, 11, 457, 321, 50740, 50740, 43245, 309, 490, 264, 19265, 13, 400, 550, 321, 3847, 797, 13, 407, 321, 362, 264, 46691, 11, 321, 14722, 264, 51032, 51032, 4470, 1296, 264, 5598, 295, 264, 20828, 1639, 365, 264, 16949, 337, 264, 7592, 1508, 13, 1033, 13, 51292, 51336, 400, 550, 321, 2042, 1071, 1823, 295, 23897, 13, 407, 510, 321, 362, 264, 7592, 1412, 13, 407, 321, 362, 264, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.35393178839432565, "compression_ratio": 1.7971014492753623, "no_speech_prob": 1.8508681023376994e-05}, {"id": 486, "seek": 342768, "start": 3435.2, "end": 3441.04, "text": " detach it from the generator. And then we train again. So we have the criterion, we compute the", "tokens": [50364, 1412, 11, 457, 321, 43245, 11, 558, 30, 639, 307, 264, 1021, 644, 13, 407, 558, 586, 321, 4636, 264, 7592, 1412, 11, 457, 321, 50740, 50740, 43245, 309, 490, 264, 19265, 13, 400, 550, 321, 3847, 797, 13, 407, 321, 362, 264, 46691, 11, 321, 14722, 264, 51032, 51032, 4470, 1296, 264, 5598, 295, 264, 20828, 1639, 365, 264, 16949, 337, 264, 7592, 1508, 13, 1033, 13, 51292, 51336, 400, 550, 321, 2042, 1071, 1823, 295, 23897, 13, 407, 510, 321, 362, 264, 7592, 1412, 13, 407, 321, 362, 264, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.35393178839432565, "compression_ratio": 1.7971014492753623, "no_speech_prob": 1.8508681023376994e-05}, {"id": 487, "seek": 342768, "start": 3441.04, "end": 3446.24, "text": " loss between the output of the discriminator with the labels for the fake class. Okay.", "tokens": [50364, 1412, 11, 457, 321, 43245, 11, 558, 30, 639, 307, 264, 1021, 644, 13, 407, 558, 586, 321, 4636, 264, 7592, 1412, 11, 457, 321, 50740, 50740, 43245, 309, 490, 264, 19265, 13, 400, 550, 321, 3847, 797, 13, 407, 321, 362, 264, 46691, 11, 321, 14722, 264, 51032, 51032, 4470, 1296, 264, 5598, 295, 264, 20828, 1639, 365, 264, 16949, 337, 264, 7592, 1508, 13, 1033, 13, 51292, 51336, 400, 550, 321, 2042, 1071, 1823, 295, 23897, 13, 407, 510, 321, 362, 264, 7592, 1412, 13, 407, 321, 362, 264, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.35393178839432565, "compression_ratio": 1.7971014492753623, "no_speech_prob": 1.8508681023376994e-05}, {"id": 488, "seek": 342768, "start": 3447.12, "end": 3453.12, "text": " And then we perform another step of backward. So here we have the fake data. So we have the", "tokens": [50364, 1412, 11, 457, 321, 43245, 11, 558, 30, 639, 307, 264, 1021, 644, 13, 407, 558, 586, 321, 4636, 264, 7592, 1412, 11, 457, 321, 50740, 50740, 43245, 309, 490, 264, 19265, 13, 400, 550, 321, 3847, 797, 13, 407, 321, 362, 264, 46691, 11, 321, 14722, 264, 51032, 51032, 4470, 1296, 264, 5598, 295, 264, 20828, 1639, 365, 264, 16949, 337, 264, 7592, 1508, 13, 1033, 13, 51292, 51336, 400, 550, 321, 2042, 1071, 1823, 295, 23897, 13, 407, 510, 321, 362, 264, 7592, 1412, 13, 407, 321, 362, 264, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.35393178839432565, "compression_ratio": 1.7971014492753623, "no_speech_prob": 1.8508681023376994e-05}, {"id": 489, "seek": 345312, "start": 3453.12, "end": 3459.68, "text": " labels for the fake class. Okay. And then we perform another step of backward. So now we have", "tokens": [50364, 16949, 337, 264, 7592, 1508, 13, 1033, 13, 400, 550, 321, 2042, 1071, 1823, 295, 23897, 13, 407, 586, 321, 362, 50692, 50692, 732, 23897, 11, 558, 30, 407, 321, 362, 23897, 510, 11, 23897, 510, 11, 293, 321, 362, 40610, 264, 14641, 50972, 50972, 13760, 295, 613, 46691, 294, 264, 1389, 689, 321, 645, 4846, 783, 957, 1412, 13, 400, 294, 264, 1389, 689, 51372, 51372, 321, 645, 4846, 783, 7592, 1412, 13, 1033, 13, 400, 370, 291, 14722, 23897, 510, 11, 23897, 510, 13, 821, 307, 572, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.10833244939004222, "compression_ratio": 1.9844559585492227, "no_speech_prob": 1.061373041011393e-05}, {"id": 490, "seek": 345312, "start": 3459.68, "end": 3465.2799999999997, "text": " two backward, right? So we have backward here, backward here, and we have computed the partial", "tokens": [50364, 16949, 337, 264, 7592, 1508, 13, 1033, 13, 400, 550, 321, 2042, 1071, 1823, 295, 23897, 13, 407, 586, 321, 362, 50692, 50692, 732, 23897, 11, 558, 30, 407, 321, 362, 23897, 510, 11, 23897, 510, 11, 293, 321, 362, 40610, 264, 14641, 50972, 50972, 13760, 295, 613, 46691, 294, 264, 1389, 689, 321, 645, 4846, 783, 957, 1412, 13, 400, 294, 264, 1389, 689, 51372, 51372, 321, 645, 4846, 783, 7592, 1412, 13, 1033, 13, 400, 370, 291, 14722, 23897, 510, 11, 23897, 510, 13, 821, 307, 572, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.10833244939004222, "compression_ratio": 1.9844559585492227, "no_speech_prob": 1.061373041011393e-05}, {"id": 491, "seek": 345312, "start": 3465.2799999999997, "end": 3473.2799999999997, "text": " derivative of these criterion in the case where we were inputting real data. And in the case where", "tokens": [50364, 16949, 337, 264, 7592, 1508, 13, 1033, 13, 400, 550, 321, 2042, 1071, 1823, 295, 23897, 13, 407, 586, 321, 362, 50692, 50692, 732, 23897, 11, 558, 30, 407, 321, 362, 23897, 510, 11, 23897, 510, 11, 293, 321, 362, 40610, 264, 14641, 50972, 50972, 13760, 295, 613, 46691, 294, 264, 1389, 689, 321, 645, 4846, 783, 957, 1412, 13, 400, 294, 264, 1389, 689, 51372, 51372, 321, 645, 4846, 783, 7592, 1412, 13, 1033, 13, 400, 370, 291, 14722, 23897, 510, 11, 23897, 510, 13, 821, 307, 572, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.10833244939004222, "compression_ratio": 1.9844559585492227, "no_speech_prob": 1.061373041011393e-05}, {"id": 492, "seek": 345312, "start": 3473.2799999999997, "end": 3479.6, "text": " we were inputting fake data. Okay. And so you compute backward here, backward here. There is no", "tokens": [50364, 16949, 337, 264, 7592, 1508, 13, 1033, 13, 400, 550, 321, 2042, 1071, 1823, 295, 23897, 13, 407, 586, 321, 362, 50692, 50692, 732, 23897, 11, 558, 30, 407, 321, 362, 23897, 510, 11, 23897, 510, 11, 293, 321, 362, 40610, 264, 14641, 50972, 50972, 13760, 295, 613, 46691, 294, 264, 1389, 689, 321, 645, 4846, 783, 957, 1412, 13, 400, 294, 264, 1389, 689, 51372, 51372, 321, 645, 4846, 783, 7592, 1412, 13, 1033, 13, 400, 370, 291, 14722, 23897, 510, 11, 23897, 510, 13, 821, 307, 572, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.10833244939004222, "compression_ratio": 1.9844559585492227, "no_speech_prob": 1.061373041011393e-05}, {"id": 493, "seek": 347960, "start": 3479.6, "end": 3484.64, "text": " gradient, right? This is important part. So we only called clear the gradient at the beginning", "tokens": [50364, 16235, 11, 558, 30, 639, 307, 1021, 644, 13, 407, 321, 787, 1219, 1850, 264, 16235, 412, 264, 2863, 50616, 50616, 293, 321, 14722, 700, 264, 2771, 2448, 365, 264, 957, 1412, 293, 550, 264, 2771, 2448, 337, 264, 7592, 1412, 13, 50904, 50956, 823, 291, 362, 300, 321, 393, 14722, 341, 472, 11, 558, 30, 407, 321, 1823, 294, 264, 5028, 6545, 13, 407, 321, 40610, 264, 51288, 51288, 23897, 11, 264, 14641, 33733, 13, 492, 40610, 264, 661, 14641, 33733, 13, 823, 321, 1823, 13, 51536, 51644, 6288, 11, 321, 3847, 264, 19265, 293, 550, 321, 366, 1096, 13, 407, 577, 360, 321, 3847, 264, 19265, 30, 51856], "temperature": 0.0, "avg_logprob": -0.11439296177455358, "compression_ratio": 1.9539748953974896, "no_speech_prob": 5.0950398872373626e-05}, {"id": 494, "seek": 347960, "start": 3484.64, "end": 3490.4, "text": " and we compute first the gradients with the real data and then the gradients for the fake data.", "tokens": [50364, 16235, 11, 558, 30, 639, 307, 1021, 644, 13, 407, 321, 787, 1219, 1850, 264, 16235, 412, 264, 2863, 50616, 50616, 293, 321, 14722, 700, 264, 2771, 2448, 365, 264, 957, 1412, 293, 550, 264, 2771, 2448, 337, 264, 7592, 1412, 13, 50904, 50956, 823, 291, 362, 300, 321, 393, 14722, 341, 472, 11, 558, 30, 407, 321, 1823, 294, 264, 5028, 6545, 13, 407, 321, 40610, 264, 51288, 51288, 23897, 11, 264, 14641, 33733, 13, 492, 40610, 264, 661, 14641, 33733, 13, 823, 321, 1823, 13, 51536, 51644, 6288, 11, 321, 3847, 264, 19265, 293, 550, 321, 366, 1096, 13, 407, 577, 360, 321, 3847, 264, 19265, 30, 51856], "temperature": 0.0, "avg_logprob": -0.11439296177455358, "compression_ratio": 1.9539748953974896, "no_speech_prob": 5.0950398872373626e-05}, {"id": 495, "seek": 347960, "start": 3491.44, "end": 3498.08, "text": " Now you have that we can compute this one, right? So we step in the optimizer. So we computed the", "tokens": [50364, 16235, 11, 558, 30, 639, 307, 1021, 644, 13, 407, 321, 787, 1219, 1850, 264, 16235, 412, 264, 2863, 50616, 50616, 293, 321, 14722, 700, 264, 2771, 2448, 365, 264, 957, 1412, 293, 550, 264, 2771, 2448, 337, 264, 7592, 1412, 13, 50904, 50956, 823, 291, 362, 300, 321, 393, 14722, 341, 472, 11, 558, 30, 407, 321, 1823, 294, 264, 5028, 6545, 13, 407, 321, 40610, 264, 51288, 51288, 23897, 11, 264, 14641, 33733, 13, 492, 40610, 264, 661, 14641, 33733, 13, 823, 321, 1823, 13, 51536, 51644, 6288, 11, 321, 3847, 264, 19265, 293, 550, 321, 366, 1096, 13, 407, 577, 360, 321, 3847, 264, 19265, 30, 51856], "temperature": 0.0, "avg_logprob": -0.11439296177455358, "compression_ratio": 1.9539748953974896, "no_speech_prob": 5.0950398872373626e-05}, {"id": 496, "seek": 347960, "start": 3498.08, "end": 3503.04, "text": " backward, the partial derivatives. We computed the other partial derivatives. Now we step.", "tokens": [50364, 16235, 11, 558, 30, 639, 307, 1021, 644, 13, 407, 321, 787, 1219, 1850, 264, 16235, 412, 264, 2863, 50616, 50616, 293, 321, 14722, 700, 264, 2771, 2448, 365, 264, 957, 1412, 293, 550, 264, 2771, 2448, 337, 264, 7592, 1412, 13, 50904, 50956, 823, 291, 362, 300, 321, 393, 14722, 341, 472, 11, 558, 30, 407, 321, 1823, 294, 264, 5028, 6545, 13, 407, 321, 40610, 264, 51288, 51288, 23897, 11, 264, 14641, 33733, 13, 492, 40610, 264, 661, 14641, 33733, 13, 823, 321, 1823, 13, 51536, 51644, 6288, 11, 321, 3847, 264, 19265, 293, 550, 321, 366, 1096, 13, 407, 577, 360, 321, 3847, 264, 19265, 30, 51856], "temperature": 0.0, "avg_logprob": -0.11439296177455358, "compression_ratio": 1.9539748953974896, "no_speech_prob": 5.0950398872373626e-05}, {"id": 497, "seek": 350304, "start": 3503.04, "end": 3507.6, "text": " Finally, we train the generator and then we are done. So how do we train the generator?", "tokens": [50364, 6288, 11, 321, 3847, 264, 19265, 293, 550, 321, 366, 1096, 13, 407, 577, 360, 321, 3847, 264, 19265, 30, 50592, 50628, 823, 291, 2836, 264, 16949, 365, 264, 957, 16949, 13, 1033, 13, 583, 291, 920, 3154, 264, 20828, 1639, 293, 51060, 51060, 264, 7592, 1412, 11, 264, 472, 300, 390, 10833, 538, 452, 19265, 13, 639, 20828, 1639, 820, 584, 11, 51348, 51348, 1954, 11, 341, 307, 7592, 1412, 11, 457, 321, 584, 11, 572, 11, 572, 11, 341, 307, 957, 1412, 13, 400, 4412, 291, 1936, 18135, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.15787119561053337, "compression_ratio": 1.7718446601941749, "no_speech_prob": 1.8556944269221276e-05}, {"id": 498, "seek": 350304, "start": 3508.32, "end": 3516.96, "text": " Now you fill the labels with the real labels. Okay. But you still feed the discriminator and", "tokens": [50364, 6288, 11, 321, 3847, 264, 19265, 293, 550, 321, 366, 1096, 13, 407, 577, 360, 321, 3847, 264, 19265, 30, 50592, 50628, 823, 291, 2836, 264, 16949, 365, 264, 957, 16949, 13, 1033, 13, 583, 291, 920, 3154, 264, 20828, 1639, 293, 51060, 51060, 264, 7592, 1412, 11, 264, 472, 300, 390, 10833, 538, 452, 19265, 13, 639, 20828, 1639, 820, 584, 11, 51348, 51348, 1954, 11, 341, 307, 7592, 1412, 11, 457, 321, 584, 11, 572, 11, 572, 11, 341, 307, 957, 1412, 13, 400, 4412, 291, 1936, 18135, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.15787119561053337, "compression_ratio": 1.7718446601941749, "no_speech_prob": 1.8556944269221276e-05}, {"id": 499, "seek": 350304, "start": 3516.96, "end": 3522.72, "text": " the fake data, the one that was generated by my generator. This discriminator should say,", "tokens": [50364, 6288, 11, 321, 3847, 264, 19265, 293, 550, 321, 366, 1096, 13, 407, 577, 360, 321, 3847, 264, 19265, 30, 50592, 50628, 823, 291, 2836, 264, 16949, 365, 264, 957, 16949, 13, 1033, 13, 583, 291, 920, 3154, 264, 20828, 1639, 293, 51060, 51060, 264, 7592, 1412, 11, 264, 472, 300, 390, 10833, 538, 452, 19265, 13, 639, 20828, 1639, 820, 584, 11, 51348, 51348, 1954, 11, 341, 307, 7592, 1412, 11, 457, 321, 584, 11, 572, 11, 572, 11, 341, 307, 957, 1412, 13, 400, 4412, 291, 1936, 18135, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.15787119561053337, "compression_ratio": 1.7718446601941749, "no_speech_prob": 1.8556944269221276e-05}, {"id": 500, "seek": 350304, "start": 3522.72, "end": 3528.72, "text": " oh, this is fake data, but we say, no, no, this is real data. And therefore you basically swap", "tokens": [50364, 6288, 11, 321, 3847, 264, 19265, 293, 550, 321, 366, 1096, 13, 407, 577, 360, 321, 3847, 264, 19265, 30, 50592, 50628, 823, 291, 2836, 264, 16949, 365, 264, 957, 16949, 13, 1033, 13, 583, 291, 920, 3154, 264, 20828, 1639, 293, 51060, 51060, 264, 7592, 1412, 11, 264, 472, 300, 390, 10833, 538, 452, 19265, 13, 639, 20828, 1639, 820, 584, 11, 51348, 51348, 1954, 11, 341, 307, 7592, 1412, 11, 457, 321, 584, 11, 572, 11, 572, 11, 341, 307, 957, 1412, 13, 400, 4412, 291, 1936, 18135, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.15787119561053337, "compression_ratio": 1.7718446601941749, "no_speech_prob": 1.8556944269221276e-05}, {"id": 501, "seek": 352872, "start": 3528.72, "end": 3534.8799999999997, "text": " the thing. Right? So now you have the, when we compute these back propagation, we have these", "tokens": [50364, 264, 551, 13, 1779, 30, 407, 586, 291, 362, 264, 11, 562, 321, 14722, 613, 646, 38377, 11, 321, 362, 613, 50672, 50672, 2771, 2448, 11, 597, 366, 516, 294, 264, 6182, 3513, 13, 1981, 366, 1382, 281, 652, 428, 3209, 50932, 50932, 2042, 5324, 13, 1033, 13, 583, 550, 321, 366, 516, 281, 312, 445, 16821, 365, 264, 19265, 13, 1779, 30, 407, 341, 51272, 51272, 472, 715, 1819, 264, 14641, 13760, 337, 1518, 11, 558, 30, 4100, 831, 13760, 295, 264, 46691, 365, 51552, 51552, 3104, 281, 264, 17443, 295, 264, 20828, 1639, 293, 264, 17443, 295, 264, 1412, 13, 407, 291, 393, 536, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.2893030340021307, "compression_ratio": 1.8, "no_speech_prob": 9.803111424844246e-06}, {"id": 502, "seek": 352872, "start": 3534.8799999999997, "end": 3540.08, "text": " gradients, which are going in the opposite direction. These are trying to make your network", "tokens": [50364, 264, 551, 13, 1779, 30, 407, 586, 291, 362, 264, 11, 562, 321, 14722, 613, 646, 38377, 11, 321, 362, 613, 50672, 50672, 2771, 2448, 11, 597, 366, 516, 294, 264, 6182, 3513, 13, 1981, 366, 1382, 281, 652, 428, 3209, 50932, 50932, 2042, 5324, 13, 1033, 13, 583, 550, 321, 366, 516, 281, 312, 445, 16821, 365, 264, 19265, 13, 1779, 30, 407, 341, 51272, 51272, 472, 715, 1819, 264, 14641, 13760, 337, 1518, 11, 558, 30, 4100, 831, 13760, 295, 264, 46691, 365, 51552, 51552, 3104, 281, 264, 17443, 295, 264, 20828, 1639, 293, 264, 17443, 295, 264, 1412, 13, 407, 291, 393, 536, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.2893030340021307, "compression_ratio": 1.8, "no_speech_prob": 9.803111424844246e-06}, {"id": 503, "seek": 352872, "start": 3540.08, "end": 3546.8799999999997, "text": " perform worse. Okay. But then we are going to be just stepping with the generator. Right? So this", "tokens": [50364, 264, 551, 13, 1779, 30, 407, 586, 291, 362, 264, 11, 562, 321, 14722, 613, 646, 38377, 11, 321, 362, 613, 50672, 50672, 2771, 2448, 11, 597, 366, 516, 294, 264, 6182, 3513, 13, 1981, 366, 1382, 281, 652, 428, 3209, 50932, 50932, 2042, 5324, 13, 1033, 13, 583, 550, 321, 366, 516, 281, 312, 445, 16821, 365, 264, 19265, 13, 1779, 30, 407, 341, 51272, 51272, 472, 715, 1819, 264, 14641, 13760, 337, 1518, 11, 558, 30, 4100, 831, 13760, 295, 264, 46691, 365, 51552, 51552, 3104, 281, 264, 17443, 295, 264, 20828, 1639, 293, 264, 17443, 295, 264, 1412, 13, 407, 291, 393, 536, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.2893030340021307, "compression_ratio": 1.8, "no_speech_prob": 9.803111424844246e-06}, {"id": 504, "seek": 352872, "start": 3546.8799999999997, "end": 3552.48, "text": " one computes the partial derivative for everyone, right? Partial derivative of the criterion with", "tokens": [50364, 264, 551, 13, 1779, 30, 407, 586, 291, 362, 264, 11, 562, 321, 14722, 613, 646, 38377, 11, 321, 362, 613, 50672, 50672, 2771, 2448, 11, 597, 366, 516, 294, 264, 6182, 3513, 13, 1981, 366, 1382, 281, 652, 428, 3209, 50932, 50932, 2042, 5324, 13, 1033, 13, 583, 550, 321, 366, 516, 281, 312, 445, 16821, 365, 264, 19265, 13, 1779, 30, 407, 341, 51272, 51272, 472, 715, 1819, 264, 14641, 13760, 337, 1518, 11, 558, 30, 4100, 831, 13760, 295, 264, 46691, 365, 51552, 51552, 3104, 281, 264, 17443, 295, 264, 20828, 1639, 293, 264, 17443, 295, 264, 1412, 13, 407, 291, 393, 536, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.2893030340021307, "compression_ratio": 1.8, "no_speech_prob": 9.803111424844246e-06}, {"id": 505, "seek": 352872, "start": 3552.48, "end": 3558.08, "text": " respect to the weights of the discriminator and the weights of the data. So you can see", "tokens": [50364, 264, 551, 13, 1779, 30, 407, 586, 291, 362, 264, 11, 562, 321, 14722, 613, 646, 38377, 11, 321, 362, 613, 50672, 50672, 2771, 2448, 11, 597, 366, 516, 294, 264, 6182, 3513, 13, 1981, 366, 1382, 281, 652, 428, 3209, 50932, 50932, 2042, 5324, 13, 1033, 13, 583, 550, 321, 366, 516, 281, 312, 445, 16821, 365, 264, 19265, 13, 1779, 30, 407, 341, 51272, 51272, 472, 715, 1819, 264, 14641, 13760, 337, 1518, 11, 558, 30, 4100, 831, 13760, 295, 264, 46691, 365, 51552, 51552, 3104, 281, 264, 17443, 295, 264, 20828, 1639, 293, 264, 17443, 295, 264, 1412, 13, 407, 291, 393, 536, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.2893030340021307, "compression_ratio": 1.8, "no_speech_prob": 9.803111424844246e-06}, {"id": 506, "seek": 355808, "start": 3558.08, "end": 3563.92, "text": " the weights of the generator, but then we are going to be stepping only with the generator.", "tokens": [50364, 264, 17443, 295, 264, 19265, 11, 457, 550, 321, 366, 516, 281, 312, 16821, 787, 365, 264, 19265, 13, 50656, 50656, 407, 264, 19265, 486, 853, 281, 652, 3126, 46691, 293, 264, 46691, 575, 264, 7645, 50011, 11, 558, 30, 51068, 51068, 1981, 366, 957, 7645, 337, 5699, 321, 3154, 264, 20828, 1639, 7592, 1412, 13, 400, 370, 341, 472, 307, 51380, 51380, 767, 1364, 1970, 264, 20828, 1639, 13, 400, 300, 390, 309, 13, 407, 291, 632, 472, 23897, 510, 13, 51732, 51796], "temperature": 0.0, "avg_logprob": -0.09577920824982399, "compression_ratio": 1.7289719626168225, "no_speech_prob": 1.9489363694447093e-05}, {"id": 507, "seek": 355808, "start": 3563.92, "end": 3572.16, "text": " So the generator will try to make lower criterion and the criterion has the label swapped, right?", "tokens": [50364, 264, 17443, 295, 264, 19265, 11, 457, 550, 321, 366, 516, 281, 312, 16821, 787, 365, 264, 19265, 13, 50656, 50656, 407, 264, 19265, 486, 853, 281, 652, 3126, 46691, 293, 264, 46691, 575, 264, 7645, 50011, 11, 558, 30, 51068, 51068, 1981, 366, 957, 7645, 337, 5699, 321, 3154, 264, 20828, 1639, 7592, 1412, 13, 400, 370, 341, 472, 307, 51380, 51380, 767, 1364, 1970, 264, 20828, 1639, 13, 400, 300, 390, 309, 13, 407, 291, 632, 472, 23897, 510, 13, 51732, 51796], "temperature": 0.0, "avg_logprob": -0.09577920824982399, "compression_ratio": 1.7289719626168225, "no_speech_prob": 1.9489363694447093e-05}, {"id": 508, "seek": 355808, "start": 3572.16, "end": 3578.4, "text": " These are real label for whenever we feed the discriminator fake data. And so this one is", "tokens": [50364, 264, 17443, 295, 264, 19265, 11, 457, 550, 321, 366, 516, 281, 312, 16821, 787, 365, 264, 19265, 13, 50656, 50656, 407, 264, 19265, 486, 853, 281, 652, 3126, 46691, 293, 264, 46691, 575, 264, 7645, 50011, 11, 558, 30, 51068, 51068, 1981, 366, 957, 7645, 337, 5699, 321, 3154, 264, 20828, 1639, 7592, 1412, 13, 400, 370, 341, 472, 307, 51380, 51380, 767, 1364, 1970, 264, 20828, 1639, 13, 400, 300, 390, 309, 13, 407, 291, 632, 472, 23897, 510, 13, 51732, 51796], "temperature": 0.0, "avg_logprob": -0.09577920824982399, "compression_ratio": 1.7289719626168225, "no_speech_prob": 1.9489363694447093e-05}, {"id": 509, "seek": 355808, "start": 3578.4, "end": 3585.44, "text": " actually working against the discriminator. And that was it. So you had one backward here.", "tokens": [50364, 264, 17443, 295, 264, 19265, 11, 457, 550, 321, 366, 516, 281, 312, 16821, 787, 365, 264, 19265, 13, 50656, 50656, 407, 264, 19265, 486, 853, 281, 652, 3126, 46691, 293, 264, 46691, 575, 264, 7645, 50011, 11, 558, 30, 51068, 51068, 1981, 366, 957, 7645, 337, 5699, 321, 3154, 264, 20828, 1639, 7592, 1412, 13, 400, 370, 341, 472, 307, 51380, 51380, 767, 1364, 1970, 264, 20828, 1639, 13, 400, 300, 390, 309, 13, 407, 291, 632, 472, 23897, 510, 13, 51732, 51796], "temperature": 0.0, "avg_logprob": -0.09577920824982399, "compression_ratio": 1.7289719626168225, "no_speech_prob": 1.9489363694447093e-05}, {"id": 510, "seek": 358544, "start": 3585.44, "end": 3592.2400000000002, "text": " You have another backward here and you have another backward here and other questions right now.", "tokens": [50364, 509, 362, 1071, 23897, 510, 293, 291, 362, 1071, 23897, 510, 293, 661, 1651, 558, 586, 13, 50704, 50704, 3802, 11, 437, 311, 264, 2649, 1296, 264, 700, 732, 12204, 30, 1436, 436, 434, 1293, 322, 264, 912, 50960, 50960, 10024, 13, 1779, 11, 558, 13, 1033, 13, 407, 264, 700, 23897, 510, 11, 309, 311, 40610, 562, 264, 3209, 11, 51316, 51316, 264, 20828, 1639, 11, 264, 2063, 3209, 575, 668, 4636, 365, 264, 957, 1412, 293, 264, 7645, 510, 366, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.22690049533186288, "compression_ratio": 1.7511737089201878, "no_speech_prob": 1.4965161426516715e-05}, {"id": 511, "seek": 358544, "start": 3592.2400000000002, "end": 3597.36, "text": " Wait, what's the difference between the first two backwards? Because they're both on the same", "tokens": [50364, 509, 362, 1071, 23897, 510, 293, 291, 362, 1071, 23897, 510, 293, 661, 1651, 558, 586, 13, 50704, 50704, 3802, 11, 437, 311, 264, 2649, 1296, 264, 700, 732, 12204, 30, 1436, 436, 434, 1293, 322, 264, 912, 50960, 50960, 10024, 13, 1779, 11, 558, 13, 1033, 13, 407, 264, 700, 23897, 510, 11, 309, 311, 40610, 562, 264, 3209, 11, 51316, 51316, 264, 20828, 1639, 11, 264, 2063, 3209, 575, 668, 4636, 365, 264, 957, 1412, 293, 264, 7645, 510, 366, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.22690049533186288, "compression_ratio": 1.7511737089201878, "no_speech_prob": 1.4965161426516715e-05}, {"id": 512, "seek": 358544, "start": 3597.36, "end": 3604.48, "text": " objective. Right, right. Okay. So the first backward here, it's computed when the network,", "tokens": [50364, 509, 362, 1071, 23897, 510, 293, 291, 362, 1071, 23897, 510, 293, 661, 1651, 558, 586, 13, 50704, 50704, 3802, 11, 437, 311, 264, 2649, 1296, 264, 700, 732, 12204, 30, 1436, 436, 434, 1293, 322, 264, 912, 50960, 50960, 10024, 13, 1779, 11, 558, 13, 1033, 13, 407, 264, 700, 23897, 510, 11, 309, 311, 40610, 562, 264, 3209, 11, 51316, 51316, 264, 20828, 1639, 11, 264, 2063, 3209, 575, 668, 4636, 365, 264, 957, 1412, 293, 264, 7645, 510, 366, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.22690049533186288, "compression_ratio": 1.7511737089201878, "no_speech_prob": 1.4965161426516715e-05}, {"id": 513, "seek": 358544, "start": 3604.48, "end": 3612.0, "text": " the discriminator, the cost network has been fed with the real data and the label here are,", "tokens": [50364, 509, 362, 1071, 23897, 510, 293, 291, 362, 1071, 23897, 510, 293, 661, 1651, 558, 586, 13, 50704, 50704, 3802, 11, 437, 311, 264, 2649, 1296, 264, 700, 732, 12204, 30, 1436, 436, 434, 1293, 322, 264, 912, 50960, 50960, 10024, 13, 1779, 11, 558, 13, 1033, 13, 407, 264, 700, 23897, 510, 11, 309, 311, 40610, 562, 264, 3209, 11, 51316, 51316, 264, 20828, 1639, 11, 264, 2063, 3209, 575, 668, 4636, 365, 264, 957, 1412, 293, 264, 7645, 510, 366, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.22690049533186288, "compression_ratio": 1.7511737089201878, "no_speech_prob": 1.4965161426516715e-05}, {"id": 514, "seek": 361200, "start": 3612.0, "end": 3618.16, "text": " will fill, are filled with the real label. Okay. So this is the first part of the backward. So you", "tokens": [50364, 486, 2836, 11, 366, 6412, 365, 264, 957, 7645, 13, 1033, 13, 407, 341, 307, 264, 700, 644, 295, 264, 23897, 13, 407, 291, 50672, 50672, 362, 1508, 11, 2074, 1508, 11, 293, 550, 291, 362, 1508, 295, 264, 7592, 1508, 11, 558, 30, 682, 341, 1389, 11, 286, 8460, 51016, 51016, 452, 7592, 1412, 807, 264, 19265, 11, 597, 390, 4636, 5658, 13, 400, 550, 286, 3154, 452, 20828, 1639, 365, 264, 51336, 51336, 7592, 1412, 11, 457, 286, 5936, 264, 2771, 2448, 281, 352, 12204, 294, 264, 19265, 13, 400, 341, 46691, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.16084418004872847, "compression_ratio": 1.7798165137614679, "no_speech_prob": 1.773425901774317e-05}, {"id": 515, "seek": 361200, "start": 3618.16, "end": 3625.04, "text": " have class, true class, and then you have class of the fake class, right? In this case, I generate", "tokens": [50364, 486, 2836, 11, 366, 6412, 365, 264, 957, 7645, 13, 1033, 13, 407, 341, 307, 264, 700, 644, 295, 264, 23897, 13, 407, 291, 50672, 50672, 362, 1508, 11, 2074, 1508, 11, 293, 550, 291, 362, 1508, 295, 264, 7592, 1508, 11, 558, 30, 682, 341, 1389, 11, 286, 8460, 51016, 51016, 452, 7592, 1412, 807, 264, 19265, 11, 597, 390, 4636, 5658, 13, 400, 550, 286, 3154, 452, 20828, 1639, 365, 264, 51336, 51336, 7592, 1412, 11, 457, 286, 5936, 264, 2771, 2448, 281, 352, 12204, 294, 264, 19265, 13, 400, 341, 46691, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.16084418004872847, "compression_ratio": 1.7798165137614679, "no_speech_prob": 1.773425901774317e-05}, {"id": 516, "seek": 361200, "start": 3625.04, "end": 3631.44, "text": " my fake data through the generator, which was fed noise. And then I feed my discriminator with the", "tokens": [50364, 486, 2836, 11, 366, 6412, 365, 264, 957, 7645, 13, 1033, 13, 407, 341, 307, 264, 700, 644, 295, 264, 23897, 13, 407, 291, 50672, 50672, 362, 1508, 11, 2074, 1508, 11, 293, 550, 291, 362, 1508, 295, 264, 7592, 1508, 11, 558, 30, 682, 341, 1389, 11, 286, 8460, 51016, 51016, 452, 7592, 1412, 807, 264, 19265, 11, 597, 390, 4636, 5658, 13, 400, 550, 286, 3154, 452, 20828, 1639, 365, 264, 51336, 51336, 7592, 1412, 11, 457, 286, 5936, 264, 2771, 2448, 281, 352, 12204, 294, 264, 19265, 13, 400, 341, 46691, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.16084418004872847, "compression_ratio": 1.7798165137614679, "no_speech_prob": 1.773425901774317e-05}, {"id": 517, "seek": 361200, "start": 3631.44, "end": 3639.36, "text": " fake data, but I stopped the gradients to go backwards in the generator. And this criterion", "tokens": [50364, 486, 2836, 11, 366, 6412, 365, 264, 957, 7645, 13, 1033, 13, 407, 341, 307, 264, 700, 644, 295, 264, 23897, 13, 407, 291, 50672, 50672, 362, 1508, 11, 2074, 1508, 11, 293, 550, 291, 362, 1508, 295, 264, 7592, 1508, 11, 558, 30, 682, 341, 1389, 11, 286, 8460, 51016, 51016, 452, 7592, 1412, 807, 264, 19265, 11, 597, 390, 4636, 5658, 13, 400, 550, 286, 3154, 452, 20828, 1639, 365, 264, 51336, 51336, 7592, 1412, 11, 457, 286, 5936, 264, 2771, 2448, 281, 352, 12204, 294, 264, 19265, 13, 400, 341, 46691, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.16084418004872847, "compression_ratio": 1.7798165137614679, "no_speech_prob": 1.773425901774317e-05}, {"id": 518, "seek": 363936, "start": 3639.36, "end": 3647.28, "text": " still tries to make the output of the discriminator has been close to the label. And the label in this", "tokens": [50364, 920, 9898, 281, 652, 264, 5598, 295, 264, 20828, 1639, 575, 668, 1998, 281, 264, 7645, 13, 400, 264, 7645, 294, 341, 50760, 50760, 1389, 366, 264, 472, 11, 264, 7592, 7645, 11, 264, 472, 300, 366, 6615, 281, 264, 5658, 13, 407, 544, 813, 5658, 11, 51052, 51052, 1310, 321, 393, 818, 341, 5658, 7645, 420, 1310, 11, 1392, 11, 309, 311, 7592, 7645, 307, 2489, 886, 11, 558, 30, 40469, 307, 264, 51348, 51348, 1412, 293, 550, 264, 3344, 1783, 2385, 300, 307, 10833, 538, 452, 19265, 3209, 13, 400, 550, 562, 286, 829, 341, 1783, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.23340367335899204, "compression_ratio": 1.76, "no_speech_prob": 4.110985537408851e-05}, {"id": 519, "seek": 363936, "start": 3647.28, "end": 3653.1200000000003, "text": " case are the one, the fake label, the one that are associated to the noise. So more than noise,", "tokens": [50364, 920, 9898, 281, 652, 264, 5598, 295, 264, 20828, 1639, 575, 668, 1998, 281, 264, 7645, 13, 400, 264, 7645, 294, 341, 50760, 50760, 1389, 366, 264, 472, 11, 264, 7592, 7645, 11, 264, 472, 300, 366, 6615, 281, 264, 5658, 13, 407, 544, 813, 5658, 11, 51052, 51052, 1310, 321, 393, 818, 341, 5658, 7645, 420, 1310, 11, 1392, 11, 309, 311, 7592, 7645, 307, 2489, 886, 11, 558, 30, 40469, 307, 264, 51348, 51348, 1412, 293, 550, 264, 3344, 1783, 2385, 300, 307, 10833, 538, 452, 19265, 3209, 13, 400, 550, 562, 286, 829, 341, 1783, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.23340367335899204, "compression_ratio": 1.76, "no_speech_prob": 4.110985537408851e-05}, {"id": 520, "seek": 363936, "start": 3653.1200000000003, "end": 3659.04, "text": " maybe we can call this noise label or maybe, okay, it's fake label is fine too, right? Fake is the", "tokens": [50364, 920, 9898, 281, 652, 264, 5598, 295, 264, 20828, 1639, 575, 668, 1998, 281, 264, 7645, 13, 400, 264, 7645, 294, 341, 50760, 50760, 1389, 366, 264, 472, 11, 264, 7592, 7645, 11, 264, 472, 300, 366, 6615, 281, 264, 5658, 13, 407, 544, 813, 5658, 11, 51052, 51052, 1310, 321, 393, 818, 341, 5658, 7645, 420, 1310, 11, 1392, 11, 309, 311, 7592, 7645, 307, 2489, 886, 11, 558, 30, 40469, 307, 264, 51348, 51348, 1412, 293, 550, 264, 3344, 1783, 2385, 300, 307, 10833, 538, 452, 19265, 3209, 13, 400, 550, 562, 286, 829, 341, 1783, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.23340367335899204, "compression_ratio": 1.76, "no_speech_prob": 4.110985537408851e-05}, {"id": 521, "seek": 363936, "start": 3659.04, "end": 3666.4, "text": " data and then the blue X hat that is generated by my generator network. And then when I put this X", "tokens": [50364, 920, 9898, 281, 652, 264, 5598, 295, 264, 20828, 1639, 575, 668, 1998, 281, 264, 7645, 13, 400, 264, 7645, 294, 341, 50760, 50760, 1389, 366, 264, 472, 11, 264, 7592, 7645, 11, 264, 472, 300, 366, 6615, 281, 264, 5658, 13, 407, 544, 813, 5658, 11, 51052, 51052, 1310, 321, 393, 818, 341, 5658, 7645, 420, 1310, 11, 1392, 11, 309, 311, 7592, 7645, 307, 2489, 886, 11, 558, 30, 40469, 307, 264, 51348, 51348, 1412, 293, 550, 264, 3344, 1783, 2385, 300, 307, 10833, 538, 452, 19265, 3209, 13, 400, 550, 562, 286, 829, 341, 1783, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.23340367335899204, "compression_ratio": 1.76, "no_speech_prob": 4.110985537408851e-05}, {"id": 522, "seek": 366640, "start": 3666.4, "end": 3674.32, "text": " hat here inside the, sorry, the discriminator, I will tell the discriminator, hey, this one should", "tokens": [50364, 2385, 510, 1854, 264, 11, 2597, 11, 264, 20828, 1639, 11, 286, 486, 980, 264, 20828, 1639, 11, 4177, 11, 341, 472, 820, 50760, 50760, 312, 21335, 382, 7592, 16949, 11, 558, 30, 400, 370, 291, 362, 341, 11101, 510, 13, 407, 294, 341, 23897, 11, 291, 434, 51140, 51140, 516, 281, 312, 1242, 729, 14641, 13760, 295, 264, 4470, 2445, 365, 3104, 281, 264, 9834, 13, 51436, 51496], "temperature": 0.0, "avg_logprob": -0.1707180184377751, "compression_ratio": 1.5555555555555556, "no_speech_prob": 3.1598327041137964e-05}, {"id": 523, "seek": 366640, "start": 3674.32, "end": 3681.92, "text": " be labeled as fake labels, right? And so you have this criteria here. So in this backward, you're", "tokens": [50364, 2385, 510, 1854, 264, 11, 2597, 11, 264, 20828, 1639, 11, 286, 486, 980, 264, 20828, 1639, 11, 4177, 11, 341, 472, 820, 50760, 50760, 312, 21335, 382, 7592, 16949, 11, 558, 30, 400, 370, 291, 362, 341, 11101, 510, 13, 407, 294, 341, 23897, 11, 291, 434, 51140, 51140, 516, 281, 312, 1242, 729, 14641, 13760, 295, 264, 4470, 2445, 365, 3104, 281, 264, 9834, 13, 51436, 51496], "temperature": 0.0, "avg_logprob": -0.1707180184377751, "compression_ratio": 1.5555555555555556, "no_speech_prob": 3.1598327041137964e-05}, {"id": 524, "seek": 366640, "start": 3681.92, "end": 3687.84, "text": " going to be getting those partial derivative of the loss function with respect to the parameters.", "tokens": [50364, 2385, 510, 1854, 264, 11, 2597, 11, 264, 20828, 1639, 11, 286, 486, 980, 264, 20828, 1639, 11, 4177, 11, 341, 472, 820, 50760, 50760, 312, 21335, 382, 7592, 16949, 11, 558, 30, 400, 370, 291, 362, 341, 11101, 510, 13, 407, 294, 341, 23897, 11, 291, 434, 51140, 51140, 516, 281, 312, 1242, 729, 14641, 13760, 295, 264, 4470, 2445, 365, 3104, 281, 264, 9834, 13, 51436, 51496], "temperature": 0.0, "avg_logprob": -0.1707180184377751, "compression_ratio": 1.5555555555555556, "no_speech_prob": 3.1598327041137964e-05}, {"id": 525, "seek": 368784, "start": 3687.84, "end": 3697.44, "text": " In the case when we have fed the fake data and we are trying to label them as fake labels,", "tokens": [50364, 682, 264, 1389, 562, 321, 362, 4636, 264, 7592, 1412, 293, 321, 366, 1382, 281, 7645, 552, 382, 7592, 16949, 11, 50844, 50844, 558, 30, 492, 362, 7592, 12911, 11, 7592, 16949, 13, 682, 264, 661, 644, 510, 11, 321, 767, 645, 4846, 783, 1854, 51260, 51260, 264, 20828, 1639, 957, 1412, 13, 400, 550, 321, 980, 264, 3209, 11, 291, 362, 257, 4470, 1296, 428, 5598, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.12648869903994278, "compression_ratio": 1.5921787709497206, "no_speech_prob": 2.3899583538877778e-05}, {"id": 526, "seek": 368784, "start": 3697.44, "end": 3705.76, "text": " right? We have fake targets, fake labels. In the other part here, we actually were inputting inside", "tokens": [50364, 682, 264, 1389, 562, 321, 362, 4636, 264, 7592, 1412, 293, 321, 366, 1382, 281, 7645, 552, 382, 7592, 16949, 11, 50844, 50844, 558, 30, 492, 362, 7592, 12911, 11, 7592, 16949, 13, 682, 264, 661, 644, 510, 11, 321, 767, 645, 4846, 783, 1854, 51260, 51260, 264, 20828, 1639, 957, 1412, 13, 400, 550, 321, 980, 264, 3209, 11, 291, 362, 257, 4470, 1296, 428, 5598, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.12648869903994278, "compression_ratio": 1.5921787709497206, "no_speech_prob": 2.3899583538877778e-05}, {"id": 527, "seek": 368784, "start": 3705.76, "end": 3712.6400000000003, "text": " the discriminator real data. And then we tell the network, you have a loss between your output", "tokens": [50364, 682, 264, 1389, 562, 321, 362, 4636, 264, 7592, 1412, 293, 321, 366, 1382, 281, 7645, 552, 382, 7592, 16949, 11, 50844, 50844, 558, 30, 492, 362, 7592, 12911, 11, 7592, 16949, 13, 682, 264, 661, 644, 510, 11, 321, 767, 645, 4846, 783, 1854, 51260, 51260, 264, 20828, 1639, 957, 1412, 13, 400, 550, 321, 980, 264, 3209, 11, 291, 362, 257, 4470, 1296, 428, 5598, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.12648869903994278, "compression_ratio": 1.5921787709497206, "no_speech_prob": 2.3899583538877778e-05}, {"id": 528, "seek": 371264, "start": 3712.64, "end": 3718.08, "text": " and the labels, which are supposed to be real label. So the first part, you try to get,", "tokens": [50364, 293, 264, 16949, 11, 597, 366, 3442, 281, 312, 957, 7645, 13, 407, 264, 700, 644, 11, 291, 853, 281, 483, 11, 50636, 50668, 291, 483, 264, 14641, 33733, 11760, 281, 264, 4470, 300, 575, 668, 40610, 562, 957, 1412, 50972, 50972, 390, 4636, 281, 264, 20828, 1639, 13, 682, 264, 1150, 644, 307, 300, 291, 362, 264, 4470, 295, 11, 365, 3104, 11, 51316, 51316, 264, 4470, 295, 428, 5598, 295, 264, 3209, 562, 321, 4636, 7592, 1412, 11, 558, 30, 400, 370, 510, 321, 2935, 360, 11, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.15775879480505503, "compression_ratio": 1.6787330316742082, "no_speech_prob": 4.221362360112835e-06}, {"id": 529, "seek": 371264, "start": 3718.72, "end": 3724.7999999999997, "text": " you get the partial derivatives corresponding to the loss that has been computed when real data", "tokens": [50364, 293, 264, 16949, 11, 597, 366, 3442, 281, 312, 957, 7645, 13, 407, 264, 700, 644, 11, 291, 853, 281, 483, 11, 50636, 50668, 291, 483, 264, 14641, 33733, 11760, 281, 264, 4470, 300, 575, 668, 40610, 562, 957, 1412, 50972, 50972, 390, 4636, 281, 264, 20828, 1639, 13, 682, 264, 1150, 644, 307, 300, 291, 362, 264, 4470, 295, 11, 365, 3104, 11, 51316, 51316, 264, 4470, 295, 428, 5598, 295, 264, 3209, 562, 321, 4636, 7592, 1412, 11, 558, 30, 400, 370, 510, 321, 2935, 360, 11, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.15775879480505503, "compression_ratio": 1.6787330316742082, "no_speech_prob": 4.221362360112835e-06}, {"id": 530, "seek": 371264, "start": 3724.7999999999997, "end": 3731.68, "text": " was fed to the discriminator. In the second part is that you have the loss of, with respect,", "tokens": [50364, 293, 264, 16949, 11, 597, 366, 3442, 281, 312, 957, 7645, 13, 407, 264, 700, 644, 11, 291, 853, 281, 483, 11, 50636, 50668, 291, 483, 264, 14641, 33733, 11760, 281, 264, 4470, 300, 575, 668, 40610, 562, 957, 1412, 50972, 50972, 390, 4636, 281, 264, 20828, 1639, 13, 682, 264, 1150, 644, 307, 300, 291, 362, 264, 4470, 295, 11, 365, 3104, 11, 51316, 51316, 264, 4470, 295, 428, 5598, 295, 264, 3209, 562, 321, 4636, 7592, 1412, 11, 558, 30, 400, 370, 510, 321, 2935, 360, 11, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.15775879480505503, "compression_ratio": 1.6787330316742082, "no_speech_prob": 4.221362360112835e-06}, {"id": 531, "seek": 371264, "start": 3731.68, "end": 3739.04, "text": " the loss of your output of the network when we fed fake data, right? And so here we simply do,", "tokens": [50364, 293, 264, 16949, 11, 597, 366, 3442, 281, 312, 957, 7645, 13, 407, 264, 700, 644, 11, 291, 853, 281, 483, 11, 50636, 50668, 291, 483, 264, 14641, 33733, 11760, 281, 264, 4470, 300, 575, 668, 40610, 562, 957, 1412, 50972, 50972, 390, 4636, 281, 264, 20828, 1639, 13, 682, 264, 1150, 644, 307, 300, 291, 362, 264, 4470, 295, 11, 365, 3104, 11, 51316, 51316, 264, 4470, 295, 428, 5598, 295, 264, 3209, 562, 321, 4636, 7592, 1412, 11, 558, 30, 400, 370, 510, 321, 2935, 360, 11, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.15775879480505503, "compression_ratio": 1.6787330316742082, "no_speech_prob": 4.221362360112835e-06}, {"id": 532, "seek": 373904, "start": 3739.04, "end": 3743.6, "text": " again, another backward. So in this case, this backward, this line here and this line here,", "tokens": [50364, 797, 11, 1071, 23897, 13, 407, 294, 341, 1389, 11, 341, 23897, 11, 341, 1622, 510, 293, 341, 1622, 510, 11, 50592, 50620, 321, 603, 976, 291, 11, 436, 486, 33384, 11, 558, 30, 1436, 9953, 51, 284, 339, 538, 7576, 486, 33384, 50820, 50820, 633, 565, 291, 2042, 23897, 13, 407, 700, 644, 11, 291, 33384, 337, 264, 700, 1922, 295, 264, 15245, 13, 51084, 51084, 400, 550, 1150, 565, 291, 33384, 309, 11, 1936, 11, 291, 362, 264, 14641, 13760, 337, 264, 1150, 51332, 51332, 644, 295, 264, 15245, 13, 440, 700, 644, 295, 264, 15245, 307, 264, 957, 1412, 13, 5736, 644, 295, 264, 15245, 307, 264, 7592, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.17716230309527853, "compression_ratio": 2.03862660944206, "no_speech_prob": 1.239840185007779e-05}, {"id": 533, "seek": 373904, "start": 3744.16, "end": 3748.16, "text": " we'll give you, they will accumulate, right? Because PyTorch by default will accumulate", "tokens": [50364, 797, 11, 1071, 23897, 13, 407, 294, 341, 1389, 11, 341, 23897, 11, 341, 1622, 510, 293, 341, 1622, 510, 11, 50592, 50620, 321, 603, 976, 291, 11, 436, 486, 33384, 11, 558, 30, 1436, 9953, 51, 284, 339, 538, 7576, 486, 33384, 50820, 50820, 633, 565, 291, 2042, 23897, 13, 407, 700, 644, 11, 291, 33384, 337, 264, 700, 1922, 295, 264, 15245, 13, 51084, 51084, 400, 550, 1150, 565, 291, 33384, 309, 11, 1936, 11, 291, 362, 264, 14641, 13760, 337, 264, 1150, 51332, 51332, 644, 295, 264, 15245, 13, 440, 700, 644, 295, 264, 15245, 307, 264, 957, 1412, 13, 5736, 644, 295, 264, 15245, 307, 264, 7592, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.17716230309527853, "compression_ratio": 2.03862660944206, "no_speech_prob": 1.239840185007779e-05}, {"id": 534, "seek": 373904, "start": 3748.16, "end": 3753.44, "text": " every time you perform backward. So first part, you accumulate for the first half of the batch.", "tokens": [50364, 797, 11, 1071, 23897, 13, 407, 294, 341, 1389, 11, 341, 23897, 11, 341, 1622, 510, 293, 341, 1622, 510, 11, 50592, 50620, 321, 603, 976, 291, 11, 436, 486, 33384, 11, 558, 30, 1436, 9953, 51, 284, 339, 538, 7576, 486, 33384, 50820, 50820, 633, 565, 291, 2042, 23897, 13, 407, 700, 644, 11, 291, 33384, 337, 264, 700, 1922, 295, 264, 15245, 13, 51084, 51084, 400, 550, 1150, 565, 291, 33384, 309, 11, 1936, 11, 291, 362, 264, 14641, 13760, 337, 264, 1150, 51332, 51332, 644, 295, 264, 15245, 13, 440, 700, 644, 295, 264, 15245, 307, 264, 957, 1412, 13, 5736, 644, 295, 264, 15245, 307, 264, 7592, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.17716230309527853, "compression_ratio": 2.03862660944206, "no_speech_prob": 1.239840185007779e-05}, {"id": 535, "seek": 373904, "start": 3753.44, "end": 3758.4, "text": " And then second time you accumulate it, basically, you have the partial derivative for the second", "tokens": [50364, 797, 11, 1071, 23897, 13, 407, 294, 341, 1389, 11, 341, 23897, 11, 341, 1622, 510, 293, 341, 1622, 510, 11, 50592, 50620, 321, 603, 976, 291, 11, 436, 486, 33384, 11, 558, 30, 1436, 9953, 51, 284, 339, 538, 7576, 486, 33384, 50820, 50820, 633, 565, 291, 2042, 23897, 13, 407, 700, 644, 11, 291, 33384, 337, 264, 700, 1922, 295, 264, 15245, 13, 51084, 51084, 400, 550, 1150, 565, 291, 33384, 309, 11, 1936, 11, 291, 362, 264, 14641, 13760, 337, 264, 1150, 51332, 51332, 644, 295, 264, 15245, 13, 440, 700, 644, 295, 264, 15245, 307, 264, 957, 1412, 13, 5736, 644, 295, 264, 15245, 307, 264, 7592, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.17716230309527853, "compression_ratio": 2.03862660944206, "no_speech_prob": 1.239840185007779e-05}, {"id": 536, "seek": 373904, "start": 3758.4, "end": 3763.68, "text": " part of the batch. The first part of the batch is the real data. Second part of the batch is the fake", "tokens": [50364, 797, 11, 1071, 23897, 13, 407, 294, 341, 1389, 11, 341, 23897, 11, 341, 1622, 510, 293, 341, 1622, 510, 11, 50592, 50620, 321, 603, 976, 291, 11, 436, 486, 33384, 11, 558, 30, 1436, 9953, 51, 284, 339, 538, 7576, 486, 33384, 50820, 50820, 633, 565, 291, 2042, 23897, 13, 407, 700, 644, 11, 291, 33384, 337, 264, 700, 1922, 295, 264, 15245, 13, 51084, 51084, 400, 550, 1150, 565, 291, 33384, 309, 11, 1936, 11, 291, 362, 264, 14641, 13760, 337, 264, 1150, 51332, 51332, 644, 295, 264, 15245, 13, 440, 700, 644, 295, 264, 15245, 307, 264, 957, 1412, 13, 5736, 644, 295, 264, 15245, 307, 264, 7592, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.17716230309527853, "compression_ratio": 2.03862660944206, "no_speech_prob": 1.239840185007779e-05}, {"id": 537, "seek": 376368, "start": 3763.68, "end": 3769.6, "text": " data. Overall, you're going to have the partial derivatives of the fake, the real data and the", "tokens": [50364, 1412, 13, 18420, 11, 291, 434, 516, 281, 362, 264, 14641, 33733, 295, 264, 7592, 11, 264, 957, 1412, 293, 264, 50660, 50660, 7592, 1412, 13, 400, 550, 321, 764, 341, 16235, 294, 1668, 281, 10864, 11, 281, 1319, 264, 9834, 50980, 51044, 295, 264, 3209, 11, 264, 20828, 1639, 11, 558, 30, 4402, 309, 652, 2020, 370, 1400, 30, 51252, 51308, 865, 11, 300, 1669, 2020, 13, 583, 472, 295, 552, 307, 5662, 309, 293, 264, 661, 472, 307, 23223, 13, 51484, 51484, 407, 613, 2306, 370, 1400, 366, 1293, 1382, 281, 11514, 264, 11101, 13, 407, 264, 700, 472, 307, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.27704865077756485, "compression_ratio": 1.7796610169491525, "no_speech_prob": 7.794749399181455e-05}, {"id": 538, "seek": 376368, "start": 3769.6, "end": 3776.0, "text": " fake data. And then we use this gradient in order to tune, to change the parameters", "tokens": [50364, 1412, 13, 18420, 11, 291, 434, 516, 281, 362, 264, 14641, 33733, 295, 264, 7592, 11, 264, 957, 1412, 293, 264, 50660, 50660, 7592, 1412, 13, 400, 550, 321, 764, 341, 16235, 294, 1668, 281, 10864, 11, 281, 1319, 264, 9834, 50980, 51044, 295, 264, 3209, 11, 264, 20828, 1639, 11, 558, 30, 4402, 309, 652, 2020, 370, 1400, 30, 51252, 51308, 865, 11, 300, 1669, 2020, 13, 583, 472, 295, 552, 307, 5662, 309, 293, 264, 661, 472, 307, 23223, 13, 51484, 51484, 407, 613, 2306, 370, 1400, 366, 1293, 1382, 281, 11514, 264, 11101, 13, 407, 264, 700, 472, 307, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.27704865077756485, "compression_ratio": 1.7796610169491525, "no_speech_prob": 7.794749399181455e-05}, {"id": 539, "seek": 376368, "start": 3777.2799999999997, "end": 3781.44, "text": " of the network, the discriminator, right? Does it make sense so far?", "tokens": [50364, 1412, 13, 18420, 11, 291, 434, 516, 281, 362, 264, 14641, 33733, 295, 264, 7592, 11, 264, 957, 1412, 293, 264, 50660, 50660, 7592, 1412, 13, 400, 550, 321, 764, 341, 16235, 294, 1668, 281, 10864, 11, 281, 1319, 264, 9834, 50980, 51044, 295, 264, 3209, 11, 264, 20828, 1639, 11, 558, 30, 4402, 309, 652, 2020, 370, 1400, 30, 51252, 51308, 865, 11, 300, 1669, 2020, 13, 583, 472, 295, 552, 307, 5662, 309, 293, 264, 661, 472, 307, 23223, 13, 51484, 51484, 407, 613, 2306, 370, 1400, 366, 1293, 1382, 281, 11514, 264, 11101, 13, 407, 264, 700, 472, 307, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.27704865077756485, "compression_ratio": 1.7796610169491525, "no_speech_prob": 7.794749399181455e-05}, {"id": 540, "seek": 376368, "start": 3782.56, "end": 3786.08, "text": " Yeah, that makes sense. But one of them is increasing it and the other one is decreasing.", "tokens": [50364, 1412, 13, 18420, 11, 291, 434, 516, 281, 362, 264, 14641, 33733, 295, 264, 7592, 11, 264, 957, 1412, 293, 264, 50660, 50660, 7592, 1412, 13, 400, 550, 321, 764, 341, 16235, 294, 1668, 281, 10864, 11, 281, 1319, 264, 9834, 50980, 51044, 295, 264, 3209, 11, 264, 20828, 1639, 11, 558, 30, 4402, 309, 652, 2020, 370, 1400, 30, 51252, 51308, 865, 11, 300, 1669, 2020, 13, 583, 472, 295, 552, 307, 5662, 309, 293, 264, 661, 472, 307, 23223, 13, 51484, 51484, 407, 613, 2306, 370, 1400, 366, 1293, 1382, 281, 11514, 264, 11101, 13, 407, 264, 700, 472, 307, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.27704865077756485, "compression_ratio": 1.7796610169491525, "no_speech_prob": 7.794749399181455e-05}, {"id": 541, "seek": 376368, "start": 3786.08, "end": 3791.7599999999998, "text": " So these ones so far are both trying to decrease the criteria. So the first one is", "tokens": [50364, 1412, 13, 18420, 11, 291, 434, 516, 281, 362, 264, 14641, 33733, 295, 264, 7592, 11, 264, 957, 1412, 293, 264, 50660, 50660, 7592, 1412, 13, 400, 550, 321, 764, 341, 16235, 294, 1668, 281, 10864, 11, 281, 1319, 264, 9834, 50980, 51044, 295, 264, 3209, 11, 264, 20828, 1639, 11, 558, 30, 4402, 309, 652, 2020, 370, 1400, 30, 51252, 51308, 865, 11, 300, 1669, 2020, 13, 583, 472, 295, 552, 307, 5662, 309, 293, 264, 661, 472, 307, 23223, 13, 51484, 51484, 407, 613, 2306, 370, 1400, 366, 1293, 1382, 281, 11514, 264, 11101, 13, 407, 264, 700, 472, 307, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.27704865077756485, "compression_ratio": 1.7796610169491525, "no_speech_prob": 7.794749399181455e-05}, {"id": 542, "seek": 379176, "start": 3791.76, "end": 3797.6800000000003, "text": " trying to decrease the criterion. So this is, you can see here in this criterion here,", "tokens": [50364, 1382, 281, 11514, 264, 46691, 13, 407, 341, 307, 11, 291, 393, 536, 510, 294, 341, 46691, 510, 11, 50660, 50696, 575, 264, 5598, 11, 597, 307, 4636, 295, 264, 20828, 1639, 562, 309, 390, 4636, 365, 264, 957, 13199, 1412, 13, 407, 291, 51048, 51048, 362, 957, 1412, 293, 957, 16949, 13, 407, 264, 46691, 510, 307, 1382, 281, 2995, 11, 281, 6119, 957, 1412, 293, 51472, 51472], "temperature": 0.0, "avg_logprob": -0.11926708758716852, "compression_ratio": 1.701219512195122, "no_speech_prob": 7.92039354564622e-05}, {"id": 543, "seek": 379176, "start": 3798.4, "end": 3805.44, "text": " has the output, which is fed of the discriminator when it was fed with the real CPU data. So you", "tokens": [50364, 1382, 281, 11514, 264, 46691, 13, 407, 341, 307, 11, 291, 393, 536, 510, 294, 341, 46691, 510, 11, 50660, 50696, 575, 264, 5598, 11, 597, 307, 4636, 295, 264, 20828, 1639, 562, 309, 390, 4636, 365, 264, 957, 13199, 1412, 13, 407, 291, 51048, 51048, 362, 957, 1412, 293, 957, 16949, 13, 407, 264, 46691, 510, 307, 1382, 281, 2995, 11, 281, 6119, 957, 1412, 293, 51472, 51472], "temperature": 0.0, "avg_logprob": -0.11926708758716852, "compression_ratio": 1.701219512195122, "no_speech_prob": 7.92039354564622e-05}, {"id": 544, "seek": 379176, "start": 3805.44, "end": 3813.92, "text": " have real data and real labels. So the criterion here is trying to match, to pair real data and", "tokens": [50364, 1382, 281, 11514, 264, 46691, 13, 407, 341, 307, 11, 291, 393, 536, 510, 294, 341, 46691, 510, 11, 50660, 50696, 575, 264, 5598, 11, 597, 307, 4636, 295, 264, 20828, 1639, 562, 309, 390, 4636, 365, 264, 957, 13199, 1412, 13, 407, 291, 51048, 51048, 362, 957, 1412, 293, 957, 16949, 13, 407, 264, 46691, 510, 307, 1382, 281, 2995, 11, 281, 6119, 957, 1412, 293, 51472, 51472], "temperature": 0.0, "avg_logprob": -0.11926708758716852, "compression_ratio": 1.701219512195122, "no_speech_prob": 7.92039354564622e-05}, {"id": 545, "seek": 381392, "start": 3813.92, "end": 3823.92, "text": " real labels. Okay, so far? Yes. Okay. Second part, you try to have the network here, try to match", "tokens": [50364, 957, 16949, 13, 1033, 11, 370, 1400, 30, 1079, 13, 1033, 13, 5736, 644, 11, 291, 853, 281, 362, 264, 3209, 510, 11, 853, 281, 2995, 50864, 50864, 7592, 1412, 365, 7592, 7645, 13, 1033, 13, 1436, 264, 5598, 1487, 490, 341, 20828, 1639, 11, 597, 390, 4846, 51216, 51216, 365, 7592, 1412, 13, 400, 550, 341, 11, 291, 458, 11, 291, 820, 3464, 264, 3209, 281, 584, 11, 1954, 11, 613, 366, 7592, 16949, 11, 51544, 51544], "temperature": 0.0, "avg_logprob": -0.12580009698867797, "compression_ratio": 1.627027027027027, "no_speech_prob": 7.524344709963771e-06}, {"id": 546, "seek": 381392, "start": 3823.92, "end": 3830.96, "text": " fake data with fake label. Okay. Because the output comes from this discriminator, which was input", "tokens": [50364, 957, 16949, 13, 1033, 11, 370, 1400, 30, 1079, 13, 1033, 13, 5736, 644, 11, 291, 853, 281, 362, 264, 3209, 510, 11, 853, 281, 2995, 50864, 50864, 7592, 1412, 365, 7592, 7645, 13, 1033, 13, 1436, 264, 5598, 1487, 490, 341, 20828, 1639, 11, 597, 390, 4846, 51216, 51216, 365, 7592, 1412, 13, 400, 550, 341, 11, 291, 458, 11, 291, 820, 3464, 264, 3209, 281, 584, 11, 1954, 11, 613, 366, 7592, 16949, 11, 51544, 51544], "temperature": 0.0, "avg_logprob": -0.12580009698867797, "compression_ratio": 1.627027027027027, "no_speech_prob": 7.524344709963771e-06}, {"id": 547, "seek": 381392, "start": 3830.96, "end": 3837.52, "text": " with fake data. And then this, you know, you should force the network to say, oh, these are fake labels,", "tokens": [50364, 957, 16949, 13, 1033, 11, 370, 1400, 30, 1079, 13, 1033, 13, 5736, 644, 11, 291, 853, 281, 362, 264, 3209, 510, 11, 853, 281, 2995, 50864, 50864, 7592, 1412, 365, 7592, 7645, 13, 1033, 13, 1436, 264, 5598, 1487, 490, 341, 20828, 1639, 11, 597, 390, 4846, 51216, 51216, 365, 7592, 1412, 13, 400, 550, 341, 11, 291, 458, 11, 291, 820, 3464, 264, 3209, 281, 584, 11, 1954, 11, 613, 366, 7592, 16949, 11, 51544, 51544], "temperature": 0.0, "avg_logprob": -0.12580009698867797, "compression_ratio": 1.627027027027027, "no_speech_prob": 7.524344709963771e-06}, {"id": 548, "seek": 383752, "start": 3837.52, "end": 3847.12, "text": " right? And so first one, you had this criterion here acting on true data with labels that are", "tokens": [50364, 558, 30, 400, 370, 700, 472, 11, 291, 632, 341, 46691, 510, 6577, 322, 2074, 1412, 365, 16949, 300, 366, 50844, 50876, 3585, 291, 613, 366, 2074, 1412, 13, 400, 550, 291, 3847, 11, 291, 362, 264, 4470, 337, 264, 3209, 11, 51212, 51212, 597, 307, 516, 281, 312, 1566, 300, 613, 23930, 300, 820, 312, 21335, 382, 7592, 1412, 11, 558, 30, 407, 51532, 51532, 341, 307, 920, 1382, 281, 17522, 613, 11101, 13, 7504, 11, 5699, 291, 2042, 264, 5028, 6545, 1823, 11, 51812, 51856], "temperature": 0.0, "avg_logprob": -0.1270447527424673, "compression_ratio": 1.7013574660633484, "no_speech_prob": 1.4052203368919436e-05}, {"id": 549, "seek": 383752, "start": 3847.7599999999998, "end": 3854.48, "text": " telling you these are true data. And then you train, you have the loss for the network,", "tokens": [50364, 558, 30, 400, 370, 700, 472, 11, 291, 632, 341, 46691, 510, 6577, 322, 2074, 1412, 365, 16949, 300, 366, 50844, 50876, 3585, 291, 613, 366, 2074, 1412, 13, 400, 550, 291, 3847, 11, 291, 362, 264, 4470, 337, 264, 3209, 11, 51212, 51212, 597, 307, 516, 281, 312, 1566, 300, 613, 23930, 300, 820, 312, 21335, 382, 7592, 1412, 11, 558, 30, 407, 51532, 51532, 341, 307, 920, 1382, 281, 17522, 613, 11101, 13, 7504, 11, 5699, 291, 2042, 264, 5028, 6545, 1823, 11, 51812, 51856], "temperature": 0.0, "avg_logprob": -0.1270447527424673, "compression_ratio": 1.7013574660633484, "no_speech_prob": 1.4052203368919436e-05}, {"id": 550, "seek": 383752, "start": 3854.48, "end": 3860.88, "text": " which is going to be saying that these outputs that should be labeled as fake data, right? So", "tokens": [50364, 558, 30, 400, 370, 700, 472, 11, 291, 632, 341, 46691, 510, 6577, 322, 2074, 1412, 365, 16949, 300, 366, 50844, 50876, 3585, 291, 613, 366, 2074, 1412, 13, 400, 550, 291, 3847, 11, 291, 362, 264, 4470, 337, 264, 3209, 11, 51212, 51212, 597, 307, 516, 281, 312, 1566, 300, 613, 23930, 300, 820, 312, 21335, 382, 7592, 1412, 11, 558, 30, 407, 51532, 51532, 341, 307, 920, 1382, 281, 17522, 613, 11101, 13, 7504, 11, 5699, 291, 2042, 264, 5028, 6545, 1823, 11, 51812, 51856], "temperature": 0.0, "avg_logprob": -0.1270447527424673, "compression_ratio": 1.7013574660633484, "no_speech_prob": 1.4052203368919436e-05}, {"id": 551, "seek": 383752, "start": 3860.88, "end": 3866.48, "text": " this is still trying to minimize these criteria. Therefore, whenever you perform the optimizer step,", "tokens": [50364, 558, 30, 400, 370, 700, 472, 11, 291, 632, 341, 46691, 510, 6577, 322, 2074, 1412, 365, 16949, 300, 366, 50844, 50876, 3585, 291, 613, 366, 2074, 1412, 13, 400, 550, 291, 3847, 11, 291, 362, 264, 4470, 337, 264, 3209, 11, 51212, 51212, 597, 307, 516, 281, 312, 1566, 300, 613, 23930, 300, 820, 312, 21335, 382, 7592, 1412, 11, 558, 30, 407, 51532, 51532, 341, 307, 920, 1382, 281, 17522, 613, 11101, 13, 7504, 11, 5699, 291, 2042, 264, 5028, 6545, 1823, 11, 51812, 51856], "temperature": 0.0, "avg_logprob": -0.1270447527424673, "compression_ratio": 1.7013574660633484, "no_speech_prob": 1.4052203368919436e-05}, {"id": 552, "seek": 386648, "start": 3866.48, "end": 3873.2, "text": " the optimizer step will try to lower both this one and this one. Okay. Another way to do this one", "tokens": [50364, 264, 5028, 6545, 1823, 486, 853, 281, 3126, 1293, 341, 472, 293, 341, 472, 13, 1033, 13, 3996, 636, 281, 360, 341, 472, 50700, 50700, 576, 312, 281, 362, 264, 28811, 1296, 341, 472, 1804, 341, 472, 13, 509, 2042, 787, 472, 16235, 264, 51032, 51032, 912, 1823, 13, 1033, 13, 440, 8535, 11, 498, 291, 1223, 437, 286, 848, 11, 576, 312, 11, 718, 385, 853, 281, 1269, 309, 11, 51320, 51320], "temperature": 0.0, "avg_logprob": -0.2248296864827474, "compression_ratio": 1.5956284153005464, "no_speech_prob": 1.9773966414504685e-05}, {"id": 553, "seek": 386648, "start": 3873.2, "end": 3879.84, "text": " would be to have the summation between this one plus this one. You perform only one gradient the", "tokens": [50364, 264, 5028, 6545, 1823, 486, 853, 281, 3126, 1293, 341, 472, 293, 341, 472, 13, 1033, 13, 3996, 636, 281, 360, 341, 472, 50700, 50700, 576, 312, 281, 362, 264, 28811, 1296, 341, 472, 1804, 341, 472, 13, 509, 2042, 787, 472, 16235, 264, 51032, 51032, 912, 1823, 13, 1033, 13, 440, 8535, 11, 498, 291, 1223, 437, 286, 848, 11, 576, 312, 11, 718, 385, 853, 281, 1269, 309, 11, 51320, 51320], "temperature": 0.0, "avg_logprob": -0.2248296864827474, "compression_ratio": 1.5956284153005464, "no_speech_prob": 1.9773966414504685e-05}, {"id": 554, "seek": 386648, "start": 3879.84, "end": 3885.6, "text": " same step. Okay. The alternative, if you understand what I said, would be, let me try to open it,", "tokens": [50364, 264, 5028, 6545, 1823, 486, 853, 281, 3126, 1293, 341, 472, 293, 341, 472, 13, 1033, 13, 3996, 636, 281, 360, 341, 472, 50700, 50700, 576, 312, 281, 362, 264, 28811, 1296, 341, 472, 1804, 341, 472, 13, 509, 2042, 787, 472, 16235, 264, 51032, 51032, 912, 1823, 13, 1033, 13, 440, 8535, 11, 498, 291, 1223, 437, 286, 848, 11, 576, 312, 11, 718, 385, 853, 281, 1269, 309, 11, 51320, 51320], "temperature": 0.0, "avg_logprob": -0.2248296864827474, "compression_ratio": 1.5956284153005464, "no_speech_prob": 1.9773966414504685e-05}, {"id": 555, "seek": 388560, "start": 3885.6, "end": 3898.48, "text": " open item, item, this line here, right? So at 226. And then the other one was down to 235, right?", "tokens": [50364, 1269, 3174, 11, 3174, 11, 341, 1622, 510, 11, 558, 30, 407, 412, 5853, 21, 13, 400, 550, 264, 661, 472, 390, 760, 281, 6673, 20, 11, 558, 30, 51008, 51044, 6673, 20, 13, 407, 321, 2042, 341, 472, 5893, 23897, 11, 293, 321, 630, 341, 472, 5893, 646, 11, 558, 30, 583, 5911, 11, 51432, 51432], "temperature": 0.0, "avg_logprob": -0.24064854444083522, "compression_ratio": 1.4921875, "no_speech_prob": 1.7058664525393397e-06}, {"id": 556, "seek": 388560, "start": 3899.2, "end": 3906.96, "text": " 235. So we perform this one dot backward, and we did this one dot back, right? But otherwise,", "tokens": [50364, 1269, 3174, 11, 3174, 11, 341, 1622, 510, 11, 558, 30, 407, 412, 5853, 21, 13, 400, 550, 264, 661, 472, 390, 760, 281, 6673, 20, 11, 558, 30, 51008, 51044, 6673, 20, 13, 407, 321, 2042, 341, 472, 5893, 23897, 11, 293, 321, 630, 341, 472, 5893, 646, 11, 558, 30, 583, 5911, 11, 51432, 51432], "temperature": 0.0, "avg_logprob": -0.24064854444083522, "compression_ratio": 1.4921875, "no_speech_prob": 1.7058664525393397e-06}, {"id": 557, "seek": 390696, "start": 3906.96, "end": 3916.88, "text": " we could have done 226 plus the other one 235. And then we just perform backward here. Okay.", "tokens": [50364, 321, 727, 362, 1096, 5853, 21, 1804, 264, 661, 472, 6673, 20, 13, 400, 550, 321, 445, 2042, 23897, 510, 13, 1033, 13, 50860, 50900, 407, 341, 390, 364, 8535, 11, 597, 307, 767, 2293, 264, 912, 382, 558, 586, 13, 759, 291, 2042, 51144, 51144, 6091, 23897, 322, 264, 732, 819, 9912, 626, 307, 2293, 382, 2408, 2810, 264, 732, 9912, 626, 293, 550, 51444, 51444, 10205, 23897, 787, 1564, 13, 1033, 13, 400, 550, 2507, 11, 5699, 321, 3847, 264, 19265, 11, 51740, 51776], "temperature": 0.0, "avg_logprob": -0.11208351091905074, "compression_ratio": 1.660633484162896, "no_speech_prob": 6.239993126655463e-06}, {"id": 558, "seek": 390696, "start": 3917.68, "end": 3922.56, "text": " So this was an alternative, which is actually exactly the same as right now. If you perform", "tokens": [50364, 321, 727, 362, 1096, 5853, 21, 1804, 264, 661, 472, 6673, 20, 13, 400, 550, 321, 445, 2042, 23897, 510, 13, 1033, 13, 50860, 50900, 407, 341, 390, 364, 8535, 11, 597, 307, 767, 2293, 264, 912, 382, 558, 586, 13, 759, 291, 2042, 51144, 51144, 6091, 23897, 322, 264, 732, 819, 9912, 626, 307, 2293, 382, 2408, 2810, 264, 732, 9912, 626, 293, 550, 51444, 51444, 10205, 23897, 787, 1564, 13, 1033, 13, 400, 550, 2507, 11, 5699, 321, 3847, 264, 19265, 11, 51740, 51776], "temperature": 0.0, "avg_logprob": -0.11208351091905074, "compression_ratio": 1.660633484162896, "no_speech_prob": 6.239993126655463e-06}, {"id": 559, "seek": 390696, "start": 3922.56, "end": 3928.56, "text": " twice backward on the two different criterions is exactly as summing the two criterions and then", "tokens": [50364, 321, 727, 362, 1096, 5853, 21, 1804, 264, 661, 472, 6673, 20, 13, 400, 550, 321, 445, 2042, 23897, 510, 13, 1033, 13, 50860, 50900, 407, 341, 390, 364, 8535, 11, 597, 307, 767, 2293, 264, 912, 382, 558, 586, 13, 759, 291, 2042, 51144, 51144, 6091, 23897, 322, 264, 732, 819, 9912, 626, 307, 2293, 382, 2408, 2810, 264, 732, 9912, 626, 293, 550, 51444, 51444, 10205, 23897, 787, 1564, 13, 1033, 13, 400, 550, 2507, 11, 5699, 321, 3847, 264, 19265, 11, 51740, 51776], "temperature": 0.0, "avg_logprob": -0.11208351091905074, "compression_ratio": 1.660633484162896, "no_speech_prob": 6.239993126655463e-06}, {"id": 560, "seek": 390696, "start": 3928.56, "end": 3934.48, "text": " performing backward only once. Okay. And then below, whenever we train the generator,", "tokens": [50364, 321, 727, 362, 1096, 5853, 21, 1804, 264, 661, 472, 6673, 20, 13, 400, 550, 321, 445, 2042, 23897, 510, 13, 1033, 13, 50860, 50900, 407, 341, 390, 364, 8535, 11, 597, 307, 767, 2293, 264, 912, 382, 558, 586, 13, 759, 291, 2042, 51144, 51144, 6091, 23897, 322, 264, 732, 819, 9912, 626, 307, 2293, 382, 2408, 2810, 264, 732, 9912, 626, 293, 550, 51444, 51444, 10205, 23897, 787, 1564, 13, 1033, 13, 400, 550, 2507, 11, 5699, 321, 3847, 264, 19265, 11, 51740, 51776], "temperature": 0.0, "avg_logprob": -0.11208351091905074, "compression_ratio": 1.660633484162896, "no_speech_prob": 6.239993126655463e-06}, {"id": 561, "seek": 393448, "start": 3934.48, "end": 3941.6, "text": " here we swap the labels, right? In this case, we're going to be training the,", "tokens": [50364, 510, 321, 18135, 264, 16949, 11, 558, 30, 682, 341, 1389, 11, 321, 434, 516, 281, 312, 3097, 264, 11, 50720, 50756, 370, 321, 1823, 294, 365, 264, 19265, 5028, 6545, 1270, 300, 321, 853, 281, 41263, 264, 3209, 281, 5598, 16949, 51188, 51188, 300, 366, 957, 16949, 562, 321, 2893, 1412, 300, 307, 7592, 1412, 11, 558, 30, 407, 341, 1823, 294, 510, 11, 51492, 51508, 309, 486, 406, 853, 281, 1701, 7146, 264, 20828, 1639, 11, 457, 309, 486, 3847, 264, 19265, 1270, 300, 309, 9898, 281, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.17711395345708375, "compression_ratio": 1.7853658536585366, "no_speech_prob": 1.5179150068433955e-05}, {"id": 562, "seek": 393448, "start": 3942.32, "end": 3950.96, "text": " so we step in with the generator optimizer such that we try to induce the network to output labels", "tokens": [50364, 510, 321, 18135, 264, 16949, 11, 558, 30, 682, 341, 1389, 11, 321, 434, 516, 281, 312, 3097, 264, 11, 50720, 50756, 370, 321, 1823, 294, 365, 264, 19265, 5028, 6545, 1270, 300, 321, 853, 281, 41263, 264, 3209, 281, 5598, 16949, 51188, 51188, 300, 366, 957, 16949, 562, 321, 2893, 1412, 300, 307, 7592, 1412, 11, 558, 30, 407, 341, 1823, 294, 510, 11, 51492, 51508, 309, 486, 406, 853, 281, 1701, 7146, 264, 20828, 1639, 11, 457, 309, 486, 3847, 264, 19265, 1270, 300, 309, 9898, 281, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.17711395345708375, "compression_ratio": 1.7853658536585366, "no_speech_prob": 1.5179150068433955e-05}, {"id": 563, "seek": 393448, "start": 3950.96, "end": 3957.04, "text": " that are real labels when we provide data that is fake data, right? So this step in here,", "tokens": [50364, 510, 321, 18135, 264, 16949, 11, 558, 30, 682, 341, 1389, 11, 321, 434, 516, 281, 312, 3097, 264, 11, 50720, 50756, 370, 321, 1823, 294, 365, 264, 19265, 5028, 6545, 1270, 300, 321, 853, 281, 41263, 264, 3209, 281, 5598, 16949, 51188, 51188, 300, 366, 957, 16949, 562, 321, 2893, 1412, 300, 307, 7592, 1412, 11, 558, 30, 407, 341, 1823, 294, 510, 11, 51492, 51508, 309, 486, 406, 853, 281, 1701, 7146, 264, 20828, 1639, 11, 457, 309, 486, 3847, 264, 19265, 1270, 300, 309, 9898, 281, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.17711395345708375, "compression_ratio": 1.7853658536585366, "no_speech_prob": 1.5179150068433955e-05}, {"id": 564, "seek": 393448, "start": 3957.36, "end": 3963.6, "text": " it will not try to untrain the discriminator, but it will train the generator such that it tries to", "tokens": [50364, 510, 321, 18135, 264, 16949, 11, 558, 30, 682, 341, 1389, 11, 321, 434, 516, 281, 312, 3097, 264, 11, 50720, 50756, 370, 321, 1823, 294, 365, 264, 19265, 5028, 6545, 1270, 300, 321, 853, 281, 41263, 264, 3209, 281, 5598, 16949, 51188, 51188, 300, 366, 957, 16949, 562, 321, 2893, 1412, 300, 307, 7592, 1412, 11, 558, 30, 407, 341, 1823, 294, 510, 11, 51492, 51508, 309, 486, 406, 853, 281, 1701, 7146, 264, 20828, 1639, 11, 457, 309, 486, 3847, 264, 19265, 1270, 300, 309, 9898, 281, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.17711395345708375, "compression_ratio": 1.7853658536585366, "no_speech_prob": 1.5179150068433955e-05}, {"id": 565, "seek": 396360, "start": 3963.6, "end": 3971.44, "text": " make the discriminator performing poorly. So our generator, if it's generating our fake data,", "tokens": [50364, 652, 264, 20828, 1639, 10205, 22271, 13, 407, 527, 19265, 11, 498, 309, 311, 17746, 527, 7592, 1412, 11, 50756, 50800, 500, 380, 321, 528, 281, 312, 1075, 281, 980, 300, 4936, 30, 407, 500, 380, 321, 528, 281, 747, 257, 1823, 294, 264, 661, 50968, 50968, 3513, 337, 300, 30, 865, 13, 407, 291, 528, 281, 747, 257, 1823, 294, 264, 661, 3513, 337, 264, 19265, 11, 51176, 51176, 558, 30, 509, 848, 572, 337, 264, 7592, 1412, 11, 321, 528, 281, 312, 1075, 281, 980, 309, 311, 7592, 13, 865, 13, 400, 300, 311, 11, 51488, 51488, 300, 311, 11, 300, 311, 689, 291, 360, 300, 510, 13, 759, 291, 362, 7592, 1412, 11, 51652, 51724], "temperature": 0.0, "avg_logprob": -0.19461531639099122, "compression_ratio": 2.0966183574879227, "no_speech_prob": 2.0143195797572844e-05}, {"id": 566, "seek": 396360, "start": 3972.3199999999997, "end": 3975.68, "text": " don't we want to be able to tell that apart? So don't we want to take a step in the other", "tokens": [50364, 652, 264, 20828, 1639, 10205, 22271, 13, 407, 527, 19265, 11, 498, 309, 311, 17746, 527, 7592, 1412, 11, 50756, 50800, 500, 380, 321, 528, 281, 312, 1075, 281, 980, 300, 4936, 30, 407, 500, 380, 321, 528, 281, 747, 257, 1823, 294, 264, 661, 50968, 50968, 3513, 337, 300, 30, 865, 13, 407, 291, 528, 281, 747, 257, 1823, 294, 264, 661, 3513, 337, 264, 19265, 11, 51176, 51176, 558, 30, 509, 848, 572, 337, 264, 7592, 1412, 11, 321, 528, 281, 312, 1075, 281, 980, 309, 311, 7592, 13, 865, 13, 400, 300, 311, 11, 51488, 51488, 300, 311, 11, 300, 311, 689, 291, 360, 300, 510, 13, 759, 291, 362, 7592, 1412, 11, 51652, 51724], "temperature": 0.0, "avg_logprob": -0.19461531639099122, "compression_ratio": 2.0966183574879227, "no_speech_prob": 2.0143195797572844e-05}, {"id": 567, "seek": 396360, "start": 3975.68, "end": 3979.8399999999997, "text": " direction for that? Yeah. So you want to take a step in the other direction for the generator,", "tokens": [50364, 652, 264, 20828, 1639, 10205, 22271, 13, 407, 527, 19265, 11, 498, 309, 311, 17746, 527, 7592, 1412, 11, 50756, 50800, 500, 380, 321, 528, 281, 312, 1075, 281, 980, 300, 4936, 30, 407, 500, 380, 321, 528, 281, 747, 257, 1823, 294, 264, 661, 50968, 50968, 3513, 337, 300, 30, 865, 13, 407, 291, 528, 281, 747, 257, 1823, 294, 264, 661, 3513, 337, 264, 19265, 11, 51176, 51176, 558, 30, 509, 848, 572, 337, 264, 7592, 1412, 11, 321, 528, 281, 312, 1075, 281, 980, 309, 311, 7592, 13, 865, 13, 400, 300, 311, 11, 51488, 51488, 300, 311, 11, 300, 311, 689, 291, 360, 300, 510, 13, 759, 291, 362, 7592, 1412, 11, 51652, 51724], "temperature": 0.0, "avg_logprob": -0.19461531639099122, "compression_ratio": 2.0966183574879227, "no_speech_prob": 2.0143195797572844e-05}, {"id": 568, "seek": 396360, "start": 3979.8399999999997, "end": 3986.08, "text": " right? You said no for the fake data, we want to be able to tell it's fake. Yeah. And that's,", "tokens": [50364, 652, 264, 20828, 1639, 10205, 22271, 13, 407, 527, 19265, 11, 498, 309, 311, 17746, 527, 7592, 1412, 11, 50756, 50800, 500, 380, 321, 528, 281, 312, 1075, 281, 980, 300, 4936, 30, 407, 500, 380, 321, 528, 281, 747, 257, 1823, 294, 264, 661, 50968, 50968, 3513, 337, 300, 30, 865, 13, 407, 291, 528, 281, 747, 257, 1823, 294, 264, 661, 3513, 337, 264, 19265, 11, 51176, 51176, 558, 30, 509, 848, 572, 337, 264, 7592, 1412, 11, 321, 528, 281, 312, 1075, 281, 980, 309, 311, 7592, 13, 865, 13, 400, 300, 311, 11, 51488, 51488, 300, 311, 11, 300, 311, 689, 291, 360, 300, 510, 13, 759, 291, 362, 7592, 1412, 11, 51652, 51724], "temperature": 0.0, "avg_logprob": -0.19461531639099122, "compression_ratio": 2.0966183574879227, "no_speech_prob": 2.0143195797572844e-05}, {"id": 569, "seek": 396360, "start": 3986.08, "end": 3989.36, "text": " that's, that's where you do that here. If you have fake data,", "tokens": [50364, 652, 264, 20828, 1639, 10205, 22271, 13, 407, 527, 19265, 11, 498, 309, 311, 17746, 527, 7592, 1412, 11, 50756, 50800, 500, 380, 321, 528, 281, 312, 1075, 281, 980, 300, 4936, 30, 407, 500, 380, 321, 528, 281, 747, 257, 1823, 294, 264, 661, 50968, 50968, 3513, 337, 300, 30, 865, 13, 407, 291, 528, 281, 747, 257, 1823, 294, 264, 661, 3513, 337, 264, 19265, 11, 51176, 51176, 558, 30, 509, 848, 572, 337, 264, 7592, 1412, 11, 321, 528, 281, 312, 1075, 281, 980, 309, 311, 7592, 13, 865, 13, 400, 300, 311, 11, 51488, 51488, 300, 311, 11, 300, 311, 689, 291, 360, 300, 510, 13, 759, 291, 362, 7592, 1412, 11, 51652, 51724], "temperature": 0.0, "avg_logprob": -0.19461531639099122, "compression_ratio": 2.0966183574879227, "no_speech_prob": 2.0143195797572844e-05}, {"id": 570, "seek": 398936, "start": 3989.36, "end": 3994.1600000000003, "text": " when you put fake data inside the discriminator, you also say that these labels are fake labels,", "tokens": [50364, 562, 291, 829, 7592, 1412, 1854, 264, 20828, 1639, 11, 291, 611, 584, 300, 613, 16949, 366, 7592, 16949, 11, 50604, 50604, 558, 30, 407, 11, 1392, 13, 40469, 7645, 1177, 380, 914, 436, 366, 7592, 13, 1981, 366, 264, 16949, 337, 264, 7592, 1412, 13, 50896, 50992, 2704, 341, 307, 3657, 13, 407, 613, 366, 264, 2074, 7645, 13, 814, 366, 406, 7592, 7645, 13, 814, 366, 2074, 7645, 337, 51328, 51328, 264, 7592, 1412, 13, 1033, 13, 639, 11, 286, 2041, 309, 311, 11, 536, 11, 300, 311, 437, 286, 26006, 490, 661, 561, 3579, 3089, 13, 51704, 51756], "temperature": 0.0, "avg_logprob": -0.26427541195767595, "compression_ratio": 1.7853881278538812, "no_speech_prob": 2.8843442123616114e-05}, {"id": 571, "seek": 398936, "start": 3994.1600000000003, "end": 4000.0, "text": " right? So, okay. Fake label doesn't mean they are fake. These are the labels for the fake data.", "tokens": [50364, 562, 291, 829, 7592, 1412, 1854, 264, 20828, 1639, 11, 291, 611, 584, 300, 613, 16949, 366, 7592, 16949, 11, 50604, 50604, 558, 30, 407, 11, 1392, 13, 40469, 7645, 1177, 380, 914, 436, 366, 7592, 13, 1981, 366, 264, 16949, 337, 264, 7592, 1412, 13, 50896, 50992, 2704, 341, 307, 3657, 13, 407, 613, 366, 264, 2074, 7645, 13, 814, 366, 406, 7592, 7645, 13, 814, 366, 2074, 7645, 337, 51328, 51328, 264, 7592, 1412, 13, 1033, 13, 639, 11, 286, 2041, 309, 311, 11, 536, 11, 300, 311, 437, 286, 26006, 490, 661, 561, 3579, 3089, 13, 51704, 51756], "temperature": 0.0, "avg_logprob": -0.26427541195767595, "compression_ratio": 1.7853881278538812, "no_speech_prob": 2.8843442123616114e-05}, {"id": 572, "seek": 398936, "start": 4001.92, "end": 4008.6400000000003, "text": " Maybe this is weird. So these are the true label. They are not fake label. They are true label for", "tokens": [50364, 562, 291, 829, 7592, 1412, 1854, 264, 20828, 1639, 11, 291, 611, 584, 300, 613, 16949, 366, 7592, 16949, 11, 50604, 50604, 558, 30, 407, 11, 1392, 13, 40469, 7645, 1177, 380, 914, 436, 366, 7592, 13, 1981, 366, 264, 16949, 337, 264, 7592, 1412, 13, 50896, 50992, 2704, 341, 307, 3657, 13, 407, 613, 366, 264, 2074, 7645, 13, 814, 366, 406, 7592, 7645, 13, 814, 366, 2074, 7645, 337, 51328, 51328, 264, 7592, 1412, 13, 1033, 13, 639, 11, 286, 2041, 309, 311, 11, 536, 11, 300, 311, 437, 286, 26006, 490, 661, 561, 3579, 3089, 13, 51704, 51756], "temperature": 0.0, "avg_logprob": -0.26427541195767595, "compression_ratio": 1.7853881278538812, "no_speech_prob": 2.8843442123616114e-05}, {"id": 573, "seek": 398936, "start": 4008.6400000000003, "end": 4016.1600000000003, "text": " the fake data. Okay. This, I guess it's, see, that's what I dislike from other people writing code.", "tokens": [50364, 562, 291, 829, 7592, 1412, 1854, 264, 20828, 1639, 11, 291, 611, 584, 300, 613, 16949, 366, 7592, 16949, 11, 50604, 50604, 558, 30, 407, 11, 1392, 13, 40469, 7645, 1177, 380, 914, 436, 366, 7592, 13, 1981, 366, 264, 16949, 337, 264, 7592, 1412, 13, 50896, 50992, 2704, 341, 307, 3657, 13, 407, 613, 366, 264, 2074, 7645, 13, 814, 366, 406, 7592, 7645, 13, 814, 366, 2074, 7645, 337, 51328, 51328, 264, 7592, 1412, 13, 1033, 13, 639, 11, 286, 2041, 309, 311, 11, 536, 11, 300, 311, 437, 286, 26006, 490, 661, 561, 3579, 3089, 13, 51704, 51756], "temperature": 0.0, "avg_logprob": -0.26427541195767595, "compression_ratio": 1.7853881278538812, "no_speech_prob": 2.8843442123616114e-05}, {"id": 574, "seek": 401616, "start": 4016.16, "end": 4021.44, "text": " That doesn't make sense. In this case, before this, for the generator, for the discriminator,", "tokens": [50364, 663, 1177, 380, 652, 2020, 13, 682, 341, 1389, 11, 949, 341, 11, 337, 264, 19265, 11, 337, 264, 20828, 1639, 11, 50628, 50628, 321, 853, 281, 3126, 341, 46691, 293, 321, 829, 341, 46691, 13, 407, 613, 732, 3876, 366, 1382, 281, 2995, 50984, 51000, 957, 1412, 11, 264, 2074, 1412, 365, 257, 2074, 7645, 11, 558, 30, 400, 294, 341, 1389, 11, 291, 362, 1382, 281, 2995, 51356, 51396, 264, 11, 291, 458, 11, 10833, 1412, 365, 257, 10833, 7645, 11, 558, 30, 407, 11, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.29019429133488583, "compression_ratio": 1.8465608465608465, "no_speech_prob": 1.0607755939417984e-05}, {"id": 575, "seek": 401616, "start": 4021.44, "end": 4028.56, "text": " we try to lower this criterion and we put this criterion. So these two lines are trying to match", "tokens": [50364, 663, 1177, 380, 652, 2020, 13, 682, 341, 1389, 11, 949, 341, 11, 337, 264, 19265, 11, 337, 264, 20828, 1639, 11, 50628, 50628, 321, 853, 281, 3126, 341, 46691, 293, 321, 829, 341, 46691, 13, 407, 613, 732, 3876, 366, 1382, 281, 2995, 50984, 51000, 957, 1412, 11, 264, 2074, 1412, 365, 257, 2074, 7645, 11, 558, 30, 400, 294, 341, 1389, 11, 291, 362, 1382, 281, 2995, 51356, 51396, 264, 11, 291, 458, 11, 10833, 1412, 365, 257, 10833, 7645, 11, 558, 30, 407, 11, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.29019429133488583, "compression_ratio": 1.8465608465608465, "no_speech_prob": 1.0607755939417984e-05}, {"id": 576, "seek": 401616, "start": 4028.8799999999997, "end": 4036.0, "text": " real data, the true data with a true label, right? And in this case, you have trying to match", "tokens": [50364, 663, 1177, 380, 652, 2020, 13, 682, 341, 1389, 11, 949, 341, 11, 337, 264, 19265, 11, 337, 264, 20828, 1639, 11, 50628, 50628, 321, 853, 281, 3126, 341, 46691, 293, 321, 829, 341, 46691, 13, 407, 613, 732, 3876, 366, 1382, 281, 2995, 50984, 51000, 957, 1412, 11, 264, 2074, 1412, 365, 257, 2074, 7645, 11, 558, 30, 400, 294, 341, 1389, 11, 291, 362, 1382, 281, 2995, 51356, 51396, 264, 11, 291, 458, 11, 10833, 1412, 365, 257, 10833, 7645, 11, 558, 30, 407, 11, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.29019429133488583, "compression_ratio": 1.8465608465608465, "no_speech_prob": 1.0607755939417984e-05}, {"id": 577, "seek": 401616, "start": 4036.7999999999997, "end": 4042.24, "text": " the, you know, generated data with a generated label, right? So,", "tokens": [50364, 663, 1177, 380, 652, 2020, 13, 682, 341, 1389, 11, 949, 341, 11, 337, 264, 19265, 11, 337, 264, 20828, 1639, 11, 50628, 50628, 321, 853, 281, 3126, 341, 46691, 293, 321, 829, 341, 46691, 13, 407, 613, 732, 3876, 366, 1382, 281, 2995, 50984, 51000, 957, 1412, 11, 264, 2074, 1412, 365, 257, 2074, 7645, 11, 558, 30, 400, 294, 341, 1389, 11, 291, 362, 1382, 281, 2995, 51356, 51396, 264, 11, 291, 458, 11, 10833, 1412, 365, 257, 10833, 7645, 11, 558, 30, 407, 11, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.29019429133488583, "compression_ratio": 1.8465608465608465, "no_speech_prob": 1.0607755939417984e-05}, {"id": 578, "seek": 404224, "start": 4042.24, "end": 4048.7999999999997, "text": " both of these two parts are trying to train the discriminator such that it can tell apart the two", "tokens": [50364, 1293, 295, 613, 732, 3166, 366, 1382, 281, 3847, 264, 20828, 1639, 1270, 300, 309, 393, 980, 4936, 264, 732, 50692, 50692, 721, 13, 3802, 11, 370, 445, 281, 17594, 11, 370, 337, 1365, 11, 411, 498, 321, 434, 1382, 281, 5258, 3857, 5267, 11, 50916, 50916, 550, 411, 264, 19265, 576, 5258, 411, 11, 1954, 11, 286, 3031, 281, 652, 257, 3857, 3256, 510, 293, 510, 311, 264, 51156, 51156, 7645, 300, 311, 1566, 300, 309, 820, 312, 257, 3857, 5717, 341, 3256, 13, 286, 994, 380, 853, 281, 652, 257, 3857, 13, 407, 264, 51396, 51396, 7645, 307, 4018, 337, 286, 994, 380, 853, 281, 652, 257, 3857, 13, 1033, 13, 407, 718, 385, 352, 365, 3857, 13, 407, 11, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.2226803946116614, "compression_ratio": 1.8634538152610443, "no_speech_prob": 1.1124159755127039e-05}, {"id": 579, "seek": 404224, "start": 4048.7999999999997, "end": 4053.2799999999997, "text": " things. Wait, so just to clarify, so for example, like if we're trying to produce cat images,", "tokens": [50364, 1293, 295, 613, 732, 3166, 366, 1382, 281, 3847, 264, 20828, 1639, 1270, 300, 309, 393, 980, 4936, 264, 732, 50692, 50692, 721, 13, 3802, 11, 370, 445, 281, 17594, 11, 370, 337, 1365, 11, 411, 498, 321, 434, 1382, 281, 5258, 3857, 5267, 11, 50916, 50916, 550, 411, 264, 19265, 576, 5258, 411, 11, 1954, 11, 286, 3031, 281, 652, 257, 3857, 3256, 510, 293, 510, 311, 264, 51156, 51156, 7645, 300, 311, 1566, 300, 309, 820, 312, 257, 3857, 5717, 341, 3256, 13, 286, 994, 380, 853, 281, 652, 257, 3857, 13, 407, 264, 51396, 51396, 7645, 307, 4018, 337, 286, 994, 380, 853, 281, 652, 257, 3857, 13, 1033, 13, 407, 718, 385, 352, 365, 3857, 13, 407, 11, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.2226803946116614, "compression_ratio": 1.8634538152610443, "no_speech_prob": 1.1124159755127039e-05}, {"id": 580, "seek": 404224, "start": 4053.2799999999997, "end": 4058.08, "text": " then like the generator would produce like, oh, I tried to make a cat image here and here's the", "tokens": [50364, 1293, 295, 613, 732, 3166, 366, 1382, 281, 3847, 264, 20828, 1639, 1270, 300, 309, 393, 980, 4936, 264, 732, 50692, 50692, 721, 13, 3802, 11, 370, 445, 281, 17594, 11, 370, 337, 1365, 11, 411, 498, 321, 434, 1382, 281, 5258, 3857, 5267, 11, 50916, 50916, 550, 411, 264, 19265, 576, 5258, 411, 11, 1954, 11, 286, 3031, 281, 652, 257, 3857, 3256, 510, 293, 510, 311, 264, 51156, 51156, 7645, 300, 311, 1566, 300, 309, 820, 312, 257, 3857, 5717, 341, 3256, 13, 286, 994, 380, 853, 281, 652, 257, 3857, 13, 407, 264, 51396, 51396, 7645, 307, 4018, 337, 286, 994, 380, 853, 281, 652, 257, 3857, 13, 1033, 13, 407, 718, 385, 352, 365, 3857, 13, 407, 11, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.2226803946116614, "compression_ratio": 1.8634538152610443, "no_speech_prob": 1.1124159755127039e-05}, {"id": 581, "seek": 404224, "start": 4058.08, "end": 4062.8799999999997, "text": " label that's saying that it should be a cat versus this image. I didn't try to make a cat. So the", "tokens": [50364, 1293, 295, 613, 732, 3166, 366, 1382, 281, 3847, 264, 20828, 1639, 1270, 300, 309, 393, 980, 4936, 264, 732, 50692, 50692, 721, 13, 3802, 11, 370, 445, 281, 17594, 11, 370, 337, 1365, 11, 411, 498, 321, 434, 1382, 281, 5258, 3857, 5267, 11, 50916, 50916, 550, 411, 264, 19265, 576, 5258, 411, 11, 1954, 11, 286, 3031, 281, 652, 257, 3857, 3256, 510, 293, 510, 311, 264, 51156, 51156, 7645, 300, 311, 1566, 300, 309, 820, 312, 257, 3857, 5717, 341, 3256, 13, 286, 994, 380, 853, 281, 652, 257, 3857, 13, 407, 264, 51396, 51396, 7645, 307, 4018, 337, 286, 994, 380, 853, 281, 652, 257, 3857, 13, 1033, 13, 407, 718, 385, 352, 365, 3857, 13, 407, 11, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.2226803946116614, "compression_ratio": 1.8634538152610443, "no_speech_prob": 1.1124159755127039e-05}, {"id": 582, "seek": 404224, "start": 4062.8799999999997, "end": 4067.68, "text": " label is zero for I didn't try to make a cat. Okay. So let me go with cat. So,", "tokens": [50364, 1293, 295, 613, 732, 3166, 366, 1382, 281, 3847, 264, 20828, 1639, 1270, 300, 309, 393, 980, 4936, 264, 732, 50692, 50692, 721, 13, 3802, 11, 370, 445, 281, 17594, 11, 370, 337, 1365, 11, 411, 498, 321, 434, 1382, 281, 5258, 3857, 5267, 11, 50916, 50916, 550, 411, 264, 19265, 576, 5258, 411, 11, 1954, 11, 286, 3031, 281, 652, 257, 3857, 3256, 510, 293, 510, 311, 264, 51156, 51156, 7645, 300, 311, 1566, 300, 309, 820, 312, 257, 3857, 5717, 341, 3256, 13, 286, 994, 380, 853, 281, 652, 257, 3857, 13, 407, 264, 51396, 51396, 7645, 307, 4018, 337, 286, 994, 380, 853, 281, 652, 257, 3857, 13, 1033, 13, 407, 718, 385, 352, 365, 3857, 13, 407, 11, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.2226803946116614, "compression_ratio": 1.8634538152610443, "no_speech_prob": 1.1124159755127039e-05}, {"id": 583, "seek": 406768, "start": 4067.68, "end": 4073.9199999999996, "text": " what is it? So here we're going to have a real data. These are very nice, cute pictures of cats,", "tokens": [50364, 437, 307, 309, 30, 407, 510, 321, 434, 516, 281, 362, 257, 957, 1412, 13, 1981, 366, 588, 1481, 11, 4052, 5242, 295, 11111, 11, 50676, 50676, 558, 30, 400, 370, 321, 434, 516, 281, 584, 11, 1954, 11, 341, 5598, 820, 312, 4926, 382, 3857, 11, 558, 30, 1436, 309, 311, 588, 50960, 50960, 1481, 293, 1237, 4052, 13, 1396, 286, 478, 516, 281, 312, 12919, 512, 14150, 11, 512, 5658, 281, 264, 19265, 13, 639, 51296, 51296, 1542, 411, 257, 10090, 13, 1033, 13, 28690, 356, 3857, 13, 1033, 13, 407, 11, 718, 311, 853, 281, 652, 257, 3857, 3256, 13, 407, 11, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.32953594802716457, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.0001038792179315351}, {"id": 584, "seek": 406768, "start": 4073.9199999999996, "end": 4079.6, "text": " right? And so we're going to say, oh, this output should be named as cat, right? Because it's very", "tokens": [50364, 437, 307, 309, 30, 407, 510, 321, 434, 516, 281, 362, 257, 957, 1412, 13, 1981, 366, 588, 1481, 11, 4052, 5242, 295, 11111, 11, 50676, 50676, 558, 30, 400, 370, 321, 434, 516, 281, 584, 11, 1954, 11, 341, 5598, 820, 312, 4926, 382, 3857, 11, 558, 30, 1436, 309, 311, 588, 50960, 50960, 1481, 293, 1237, 4052, 13, 1396, 286, 478, 516, 281, 312, 12919, 512, 14150, 11, 512, 5658, 281, 264, 19265, 13, 639, 51296, 51296, 1542, 411, 257, 10090, 13, 1033, 13, 28690, 356, 3857, 13, 1033, 13, 407, 11, 718, 311, 853, 281, 652, 257, 3857, 3256, 13, 407, 11, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.32953594802716457, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.0001038792179315351}, {"id": 585, "seek": 406768, "start": 4079.6, "end": 4086.3199999999997, "text": " nice and looking cute. Then I'm going to be feeding some garbage, some noise to the generator. This", "tokens": [50364, 437, 307, 309, 30, 407, 510, 321, 434, 516, 281, 362, 257, 957, 1412, 13, 1981, 366, 588, 1481, 11, 4052, 5242, 295, 11111, 11, 50676, 50676, 558, 30, 400, 370, 321, 434, 516, 281, 584, 11, 1954, 11, 341, 5598, 820, 312, 4926, 382, 3857, 11, 558, 30, 1436, 309, 311, 588, 50960, 50960, 1481, 293, 1237, 4052, 13, 1396, 286, 478, 516, 281, 312, 12919, 512, 14150, 11, 512, 5658, 281, 264, 19265, 13, 639, 51296, 51296, 1542, 411, 257, 10090, 13, 1033, 13, 28690, 356, 3857, 13, 1033, 13, 407, 11, 718, 311, 853, 281, 652, 257, 3857, 3256, 13, 407, 11, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.32953594802716457, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.0001038792179315351}, {"id": 586, "seek": 406768, "start": 4086.3199999999997, "end": 4093.9199999999996, "text": " looks like a monster. Okay. Ugly cat. Okay. So, let's try to make a cat image. So,", "tokens": [50364, 437, 307, 309, 30, 407, 510, 321, 434, 516, 281, 362, 257, 957, 1412, 13, 1981, 366, 588, 1481, 11, 4052, 5242, 295, 11111, 11, 50676, 50676, 558, 30, 400, 370, 321, 434, 516, 281, 584, 11, 1954, 11, 341, 5598, 820, 312, 4926, 382, 3857, 11, 558, 30, 1436, 309, 311, 588, 50960, 50960, 1481, 293, 1237, 4052, 13, 1396, 286, 478, 516, 281, 312, 12919, 512, 14150, 11, 512, 5658, 281, 264, 19265, 13, 639, 51296, 51296, 1542, 411, 257, 10090, 13, 1033, 13, 28690, 356, 3857, 13, 1033, 13, 407, 11, 718, 311, 853, 281, 652, 257, 3857, 3256, 13, 407, 11, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.32953594802716457, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.0001038792179315351}, {"id": 587, "seek": 409392, "start": 4093.92, "end": 4102.24, "text": " this looks like a monster. Okay. Ugly cat. So then we provide this monster looking like images to", "tokens": [50364, 341, 1542, 411, 257, 10090, 13, 1033, 13, 28690, 356, 3857, 13, 407, 550, 321, 2893, 341, 10090, 1237, 411, 5267, 281, 50780, 50780, 264, 20828, 1639, 13, 400, 550, 321, 366, 516, 281, 312, 12919, 613, 6064, 365, 264, 11, 291, 458, 11, 33957, 11, 51076, 51076, 550, 2035, 264, 20828, 1639, 1619, 293, 365, 264, 7645, 11, 436, 584, 613, 366, 15785, 13, 400, 370, 510, 51348, 51348, 291, 2042, 23897, 797, 11, 293, 550, 1823, 1270, 300, 291, 434, 516, 281, 312, 3097, 264, 20828, 1639, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1354446821315314, "compression_ratio": 1.7880184331797235, "no_speech_prob": 2.9284738047863357e-05}, {"id": 588, "seek": 409392, "start": 4102.24, "end": 4108.16, "text": " the discriminator. And then we are going to be feeding these laws with the, you know, verdict,", "tokens": [50364, 341, 1542, 411, 257, 10090, 13, 1033, 13, 28690, 356, 3857, 13, 407, 550, 321, 2893, 341, 10090, 1237, 411, 5267, 281, 50780, 50780, 264, 20828, 1639, 13, 400, 550, 321, 366, 516, 281, 312, 12919, 613, 6064, 365, 264, 11, 291, 458, 11, 33957, 11, 51076, 51076, 550, 2035, 264, 20828, 1639, 1619, 293, 365, 264, 7645, 11, 436, 584, 613, 366, 15785, 13, 400, 370, 510, 51348, 51348, 291, 2042, 23897, 797, 11, 293, 550, 1823, 1270, 300, 291, 434, 516, 281, 312, 3097, 264, 20828, 1639, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1354446821315314, "compression_ratio": 1.7880184331797235, "no_speech_prob": 2.9284738047863357e-05}, {"id": 589, "seek": 409392, "start": 4108.16, "end": 4113.6, "text": " then whatever the discriminator says and with the label, they say these are monsters. And so here", "tokens": [50364, 341, 1542, 411, 257, 10090, 13, 1033, 13, 28690, 356, 3857, 13, 407, 550, 321, 2893, 341, 10090, 1237, 411, 5267, 281, 50780, 50780, 264, 20828, 1639, 13, 400, 550, 321, 366, 516, 281, 312, 12919, 613, 6064, 365, 264, 11, 291, 458, 11, 33957, 11, 51076, 51076, 550, 2035, 264, 20828, 1639, 1619, 293, 365, 264, 7645, 11, 436, 584, 613, 366, 15785, 13, 400, 370, 510, 51348, 51348, 291, 2042, 23897, 797, 11, 293, 550, 1823, 1270, 300, 291, 434, 516, 281, 312, 3097, 264, 20828, 1639, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1354446821315314, "compression_ratio": 1.7880184331797235, "no_speech_prob": 2.9284738047863357e-05}, {"id": 590, "seek": 409392, "start": 4113.6, "end": 4120.0, "text": " you perform backward again, and then step such that you're going to be training the discriminator", "tokens": [50364, 341, 1542, 411, 257, 10090, 13, 1033, 13, 28690, 356, 3857, 13, 407, 550, 321, 2893, 341, 10090, 1237, 411, 5267, 281, 50780, 50780, 264, 20828, 1639, 13, 400, 550, 321, 366, 516, 281, 312, 12919, 613, 6064, 365, 264, 11, 291, 458, 11, 33957, 11, 51076, 51076, 550, 2035, 264, 20828, 1639, 1619, 293, 365, 264, 7645, 11, 436, 584, 613, 366, 15785, 13, 400, 370, 510, 51348, 51348, 291, 2042, 23897, 797, 11, 293, 550, 1823, 1270, 300, 291, 434, 516, 281, 312, 3097, 264, 20828, 1639, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1354446821315314, "compression_ratio": 1.7880184331797235, "no_speech_prob": 2.9284738047863357e-05}, {"id": 591, "seek": 412000, "start": 4120.0, "end": 4127.76, "text": " such that they can tell apart cats from monsters. First part, second part below, we feed the monsters.", "tokens": [50364, 1270, 300, 436, 393, 980, 4936, 11111, 490, 15785, 13, 2386, 644, 11, 1150, 644, 2507, 11, 321, 3154, 264, 15785, 13, 50752, 50752, 682, 341, 1389, 11, 321, 920, 362, 11, 321, 362, 264, 2771, 2448, 11, 558, 30, 682, 341, 1389, 11, 321, 1723, 766, 264, 16235, 13, 11431, 50980, 50980, 3202, 281, 341, 644, 13, 1692, 321, 1723, 766, 264, 16235, 13, 407, 2771, 2448, 500, 380, 352, 760, 264, 19265, 13, 51228, 51268, 682, 341, 1389, 11, 321, 767, 4846, 264, 7592, 1412, 11, 264, 10090, 1237, 5267, 1854, 264, 20828, 1639, 13, 51588, 51588, 440, 20828, 1639, 584, 11, 1954, 11, 15785, 11, 15785, 13, 583, 294, 341, 1389, 11, 321, 584, 11, 572, 11, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.09956201430289977, "compression_ratio": 1.99581589958159, "no_speech_prob": 3.0656246963189915e-05}, {"id": 592, "seek": 412000, "start": 4127.76, "end": 4132.32, "text": " In this case, we still have, we have the gradients, right? In this case, we cut off the gradient. Pay", "tokens": [50364, 1270, 300, 436, 393, 980, 4936, 11111, 490, 15785, 13, 2386, 644, 11, 1150, 644, 2507, 11, 321, 3154, 264, 15785, 13, 50752, 50752, 682, 341, 1389, 11, 321, 920, 362, 11, 321, 362, 264, 2771, 2448, 11, 558, 30, 682, 341, 1389, 11, 321, 1723, 766, 264, 16235, 13, 11431, 50980, 50980, 3202, 281, 341, 644, 13, 1692, 321, 1723, 766, 264, 16235, 13, 407, 2771, 2448, 500, 380, 352, 760, 264, 19265, 13, 51228, 51268, 682, 341, 1389, 11, 321, 767, 4846, 264, 7592, 1412, 11, 264, 10090, 1237, 5267, 1854, 264, 20828, 1639, 13, 51588, 51588, 440, 20828, 1639, 584, 11, 1954, 11, 15785, 11, 15785, 13, 583, 294, 341, 1389, 11, 321, 584, 11, 572, 11, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.09956201430289977, "compression_ratio": 1.99581589958159, "no_speech_prob": 3.0656246963189915e-05}, {"id": 593, "seek": 412000, "start": 4132.32, "end": 4137.28, "text": " attention to this part. Here we cut off the gradient. So gradients don't go down the generator.", "tokens": [50364, 1270, 300, 436, 393, 980, 4936, 11111, 490, 15785, 13, 2386, 644, 11, 1150, 644, 2507, 11, 321, 3154, 264, 15785, 13, 50752, 50752, 682, 341, 1389, 11, 321, 920, 362, 11, 321, 362, 264, 2771, 2448, 11, 558, 30, 682, 341, 1389, 11, 321, 1723, 766, 264, 16235, 13, 11431, 50980, 50980, 3202, 281, 341, 644, 13, 1692, 321, 1723, 766, 264, 16235, 13, 407, 2771, 2448, 500, 380, 352, 760, 264, 19265, 13, 51228, 51268, 682, 341, 1389, 11, 321, 767, 4846, 264, 7592, 1412, 11, 264, 10090, 1237, 5267, 1854, 264, 20828, 1639, 13, 51588, 51588, 440, 20828, 1639, 584, 11, 1954, 11, 15785, 11, 15785, 13, 583, 294, 341, 1389, 11, 321, 584, 11, 572, 11, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.09956201430289977, "compression_ratio": 1.99581589958159, "no_speech_prob": 3.0656246963189915e-05}, {"id": 594, "seek": 412000, "start": 4138.08, "end": 4144.48, "text": " In this case, we actually input the fake data, the monster looking images inside the discriminator.", "tokens": [50364, 1270, 300, 436, 393, 980, 4936, 11111, 490, 15785, 13, 2386, 644, 11, 1150, 644, 2507, 11, 321, 3154, 264, 15785, 13, 50752, 50752, 682, 341, 1389, 11, 321, 920, 362, 11, 321, 362, 264, 2771, 2448, 11, 558, 30, 682, 341, 1389, 11, 321, 1723, 766, 264, 16235, 13, 11431, 50980, 50980, 3202, 281, 341, 644, 13, 1692, 321, 1723, 766, 264, 16235, 13, 407, 2771, 2448, 500, 380, 352, 760, 264, 19265, 13, 51228, 51268, 682, 341, 1389, 11, 321, 767, 4846, 264, 7592, 1412, 11, 264, 10090, 1237, 5267, 1854, 264, 20828, 1639, 13, 51588, 51588, 440, 20828, 1639, 584, 11, 1954, 11, 15785, 11, 15785, 13, 583, 294, 341, 1389, 11, 321, 584, 11, 572, 11, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.09956201430289977, "compression_ratio": 1.99581589958159, "no_speech_prob": 3.0656246963189915e-05}, {"id": 595, "seek": 412000, "start": 4144.48, "end": 4148.8, "text": " The discriminator say, oh, monsters, monsters. But in this case, we say, no,", "tokens": [50364, 1270, 300, 436, 393, 980, 4936, 11111, 490, 15785, 13, 2386, 644, 11, 1150, 644, 2507, 11, 321, 3154, 264, 15785, 13, 50752, 50752, 682, 341, 1389, 11, 321, 920, 362, 11, 321, 362, 264, 2771, 2448, 11, 558, 30, 682, 341, 1389, 11, 321, 1723, 766, 264, 16235, 13, 11431, 50980, 50980, 3202, 281, 341, 644, 13, 1692, 321, 1723, 766, 264, 16235, 13, 407, 2771, 2448, 500, 380, 352, 760, 264, 19265, 13, 51228, 51268, 682, 341, 1389, 11, 321, 767, 4846, 264, 7592, 1412, 11, 264, 10090, 1237, 5267, 1854, 264, 20828, 1639, 13, 51588, 51588, 440, 20828, 1639, 584, 11, 1954, 11, 15785, 11, 15785, 13, 583, 294, 341, 1389, 11, 321, 584, 11, 572, 11, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.09956201430289977, "compression_ratio": 1.99581589958159, "no_speech_prob": 3.0656246963189915e-05}, {"id": 596, "seek": 414880, "start": 4148.8, "end": 4156.400000000001, "text": " these are cute cats pictures. And so now we train the, we perform backward, which is computing the", "tokens": [50364, 613, 366, 4052, 11111, 5242, 13, 400, 370, 586, 321, 3847, 264, 11, 321, 2042, 23897, 11, 597, 307, 15866, 264, 50744, 50744, 14641, 33733, 365, 3104, 281, 1203, 13, 400, 550, 321, 1823, 337, 264, 19265, 1270, 300, 264, 51056, 51056, 15785, 300, 264, 19265, 390, 1455, 586, 436, 574, 544, 4052, 13, 1033, 13, 286, 393, 380, 312, 544, 4052, 813, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1376066777243543, "compression_ratio": 1.564516129032258, "no_speech_prob": 7.7633876571781e-06}, {"id": 597, "seek": 414880, "start": 4156.400000000001, "end": 4162.64, "text": " partial derivatives with respect to everything. And then we step for the generator such that the", "tokens": [50364, 613, 366, 4052, 11111, 5242, 13, 400, 370, 586, 321, 3847, 264, 11, 321, 2042, 23897, 11, 597, 307, 15866, 264, 50744, 50744, 14641, 33733, 365, 3104, 281, 1203, 13, 400, 550, 321, 1823, 337, 264, 19265, 1270, 300, 264, 51056, 51056, 15785, 300, 264, 19265, 390, 1455, 586, 436, 574, 544, 4052, 13, 1033, 13, 286, 393, 380, 312, 544, 4052, 813, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1376066777243543, "compression_ratio": 1.564516129032258, "no_speech_prob": 7.7633876571781e-06}, {"id": 598, "seek": 414880, "start": 4162.64, "end": 4172.8, "text": " monsters that the generator was making now they look more cute. Okay. I can't be more cute than", "tokens": [50364, 613, 366, 4052, 11111, 5242, 13, 400, 370, 586, 321, 3847, 264, 11, 321, 2042, 23897, 11, 597, 307, 15866, 264, 50744, 50744, 14641, 33733, 365, 3104, 281, 1203, 13, 400, 550, 321, 1823, 337, 264, 19265, 1270, 300, 264, 51056, 51056, 15785, 300, 264, 19265, 390, 1455, 586, 436, 574, 544, 4052, 13, 1033, 13, 286, 393, 380, 312, 544, 4052, 813, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1376066777243543, "compression_ratio": 1.564516129032258, "no_speech_prob": 7.7633876571781e-06}, {"id": 599, "seek": 417280, "start": 4172.8, "end": 4181.68, "text": " this. Sorry. Why don't we send the gradient of the fake data to the discriminator? We do in the second", "tokens": [50364, 341, 13, 4919, 13, 1545, 500, 380, 321, 2845, 264, 16235, 295, 264, 7592, 1412, 281, 264, 20828, 1639, 30, 492, 360, 294, 264, 1150, 50808, 50808, 1389, 11, 558, 30, 407, 718, 385, 1867, 264, 551, 13, 407, 294, 341, 1389, 510, 11, 321, 11, 562, 321, 2845, 11, 562, 321, 2845, 264, 51352, 51352, 2771, 2448, 12204, 11, 457, 646, 281, 264, 11, 291, 458, 11, 281, 264, 19265, 11, 321, 767, 18135, 264, 3006, 16949, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.08190586508774175, "compression_ratio": 1.6236559139784945, "no_speech_prob": 1.278285253647482e-05}, {"id": 600, "seek": 417280, "start": 4181.68, "end": 4192.56, "text": " case, right? So let me answer the thing. So in this case here, we, when we send, when we send the", "tokens": [50364, 341, 13, 4919, 13, 1545, 500, 380, 321, 2845, 264, 16235, 295, 264, 7592, 1412, 281, 264, 20828, 1639, 30, 492, 360, 294, 264, 1150, 50808, 50808, 1389, 11, 558, 30, 407, 718, 385, 1867, 264, 551, 13, 407, 294, 341, 1389, 510, 11, 321, 11, 562, 321, 2845, 11, 562, 321, 2845, 264, 51352, 51352, 2771, 2448, 12204, 11, 457, 646, 281, 264, 11, 291, 458, 11, 281, 264, 19265, 11, 321, 767, 18135, 264, 3006, 16949, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.08190586508774175, "compression_ratio": 1.6236559139784945, "no_speech_prob": 1.278285253647482e-05}, {"id": 601, "seek": 417280, "start": 4192.56, "end": 4199.04, "text": " gradients backwards, but back to the, you know, to the generator, we actually swap the correct labels", "tokens": [50364, 341, 13, 4919, 13, 1545, 500, 380, 321, 2845, 264, 16235, 295, 264, 7592, 1412, 281, 264, 20828, 1639, 30, 492, 360, 294, 264, 1150, 50808, 50808, 1389, 11, 558, 30, 407, 718, 385, 1867, 264, 551, 13, 407, 294, 341, 1389, 510, 11, 321, 11, 562, 321, 2845, 11, 562, 321, 2845, 264, 51352, 51352, 2771, 2448, 12204, 11, 457, 646, 281, 264, 11, 291, 458, 11, 281, 264, 19265, 11, 321, 767, 18135, 264, 3006, 16949, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.08190586508774175, "compression_ratio": 1.6236559139784945, "no_speech_prob": 1.278285253647482e-05}, {"id": 602, "seek": 419904, "start": 4199.04, "end": 4207.44, "text": " with the incorrect labels. In this case, we input monsters. The discriminator says these are monsters", "tokens": [50364, 365, 264, 18424, 16949, 13, 682, 341, 1389, 11, 321, 4846, 15785, 13, 440, 20828, 1639, 1619, 613, 366, 15785, 50784, 50784, 293, 321, 584, 11, 1954, 11, 613, 366, 665, 1237, 11111, 13, 400, 550, 321, 3847, 264, 19265, 1270, 300, 613, 15785, 51164, 51164, 486, 574, 411, 544, 1481, 1237, 11111, 13, 682, 341, 1389, 11, 291, 500, 380, 528, 281, 2845, 264, 2771, 2448, 807, 51480, 51480, 570, 294, 341, 1389, 291, 853, 281, 17522, 264, 3006, 21538, 644, 13, 1779, 30, 407, 498, 291, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.09949608471082605, "compression_ratio": 1.7422222222222221, "no_speech_prob": 1.1803628694906365e-05}, {"id": 603, "seek": 419904, "start": 4207.44, "end": 4215.04, "text": " and we say, oh, these are good looking cats. And then we train the generator such that these monsters", "tokens": [50364, 365, 264, 18424, 16949, 13, 682, 341, 1389, 11, 321, 4846, 15785, 13, 440, 20828, 1639, 1619, 613, 366, 15785, 50784, 50784, 293, 321, 584, 11, 1954, 11, 613, 366, 665, 1237, 11111, 13, 400, 550, 321, 3847, 264, 19265, 1270, 300, 613, 15785, 51164, 51164, 486, 574, 411, 544, 1481, 1237, 11111, 13, 682, 341, 1389, 11, 291, 500, 380, 528, 281, 2845, 264, 2771, 2448, 807, 51480, 51480, 570, 294, 341, 1389, 291, 853, 281, 17522, 264, 3006, 21538, 644, 13, 1779, 30, 407, 498, 291, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.09949608471082605, "compression_ratio": 1.7422222222222221, "no_speech_prob": 1.1803628694906365e-05}, {"id": 604, "seek": 419904, "start": 4215.04, "end": 4221.36, "text": " will look like more nice looking cats. In this case, you don't want to send the gradients through", "tokens": [50364, 365, 264, 18424, 16949, 13, 682, 341, 1389, 11, 321, 4846, 15785, 13, 440, 20828, 1639, 1619, 613, 366, 15785, 50784, 50784, 293, 321, 584, 11, 1954, 11, 613, 366, 665, 1237, 11111, 13, 400, 550, 321, 3847, 264, 19265, 1270, 300, 613, 15785, 51164, 51164, 486, 574, 411, 544, 1481, 1237, 11111, 13, 682, 341, 1389, 11, 291, 500, 380, 528, 281, 2845, 264, 2771, 2448, 807, 51480, 51480, 570, 294, 341, 1389, 291, 853, 281, 17522, 264, 3006, 21538, 644, 13, 1779, 30, 407, 498, 291, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.09949608471082605, "compression_ratio": 1.7422222222222221, "no_speech_prob": 1.1803628694906365e-05}, {"id": 605, "seek": 419904, "start": 4221.36, "end": 4227.12, "text": " because in this case you try to minimize the correct classification part. Right? So if you", "tokens": [50364, 365, 264, 18424, 16949, 13, 682, 341, 1389, 11, 321, 4846, 15785, 13, 440, 20828, 1639, 1619, 613, 366, 15785, 50784, 50784, 293, 321, 584, 11, 1954, 11, 613, 366, 665, 1237, 11111, 13, 400, 550, 321, 3847, 264, 19265, 1270, 300, 613, 15785, 51164, 51164, 486, 574, 411, 544, 1481, 1237, 11111, 13, 682, 341, 1389, 11, 291, 500, 380, 528, 281, 2845, 264, 2771, 2448, 807, 51480, 51480, 570, 294, 341, 1389, 291, 853, 281, 17522, 264, 3006, 21538, 644, 13, 1779, 30, 407, 498, 291, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.09949608471082605, "compression_ratio": 1.7422222222222221, "no_speech_prob": 1.1803628694906365e-05}, {"id": 606, "seek": 422712, "start": 4227.12, "end": 4233.92, "text": " would send gradients backward, you would basically get a worse performing generator, right? Because", "tokens": [50364, 576, 2845, 2771, 2448, 23897, 11, 291, 576, 1936, 483, 257, 5324, 10205, 19265, 11, 558, 30, 1436, 50704, 50704, 291, 500, 380, 528, 281, 17522, 341, 46691, 13, 509, 528, 281, 19874, 341, 11101, 13, 1779, 30, 407, 300, 311, 51072, 51072, 983, 321, 500, 380, 362, 2771, 2448, 294, 341, 700, 1389, 11, 457, 321, 360, 362, 2771, 2448, 294, 341, 1389, 11, 570, 321, 51300, 51300, 3122, 528, 281, 14722, 264, 2771, 2448, 365, 3104, 281, 264, 19265, 295, 341, 11101, 13, 51596, 51688], "temperature": 0.0, "avg_logprob": -0.08930028422494952, "compression_ratio": 1.8365384615384615, "no_speech_prob": 8.657174475956708e-06}, {"id": 607, "seek": 422712, "start": 4233.92, "end": 4241.28, "text": " you don't want to minimize this criterion. You want to maximize this criteria. Right? So that's", "tokens": [50364, 576, 2845, 2771, 2448, 23897, 11, 291, 576, 1936, 483, 257, 5324, 10205, 19265, 11, 558, 30, 1436, 50704, 50704, 291, 500, 380, 528, 281, 17522, 341, 46691, 13, 509, 528, 281, 19874, 341, 11101, 13, 1779, 30, 407, 300, 311, 51072, 51072, 983, 321, 500, 380, 362, 2771, 2448, 294, 341, 700, 1389, 11, 457, 321, 360, 362, 2771, 2448, 294, 341, 1389, 11, 570, 321, 51300, 51300, 3122, 528, 281, 14722, 264, 2771, 2448, 365, 3104, 281, 264, 19265, 295, 341, 11101, 13, 51596, 51688], "temperature": 0.0, "avg_logprob": -0.08930028422494952, "compression_ratio": 1.8365384615384615, "no_speech_prob": 8.657174475956708e-06}, {"id": 608, "seek": 422712, "start": 4241.28, "end": 4245.84, "text": " why we don't have gradients in this first case, but we do have gradients in this case, because we", "tokens": [50364, 576, 2845, 2771, 2448, 23897, 11, 291, 576, 1936, 483, 257, 5324, 10205, 19265, 11, 558, 30, 1436, 50704, 50704, 291, 500, 380, 528, 281, 17522, 341, 46691, 13, 509, 528, 281, 19874, 341, 11101, 13, 1779, 30, 407, 300, 311, 51072, 51072, 983, 321, 500, 380, 362, 2771, 2448, 294, 341, 700, 1389, 11, 457, 321, 360, 362, 2771, 2448, 294, 341, 1389, 11, 570, 321, 51300, 51300, 3122, 528, 281, 14722, 264, 2771, 2448, 365, 3104, 281, 264, 19265, 295, 341, 11101, 13, 51596, 51688], "temperature": 0.0, "avg_logprob": -0.08930028422494952, "compression_ratio": 1.8365384615384615, "no_speech_prob": 8.657174475956708e-06}, {"id": 609, "seek": 422712, "start": 4245.84, "end": 4251.76, "text": " absolutely want to compute the gradients with respect to the generator of this criteria.", "tokens": [50364, 576, 2845, 2771, 2448, 23897, 11, 291, 576, 1936, 483, 257, 5324, 10205, 19265, 11, 558, 30, 1436, 50704, 50704, 291, 500, 380, 528, 281, 17522, 341, 46691, 13, 509, 528, 281, 19874, 341, 11101, 13, 1779, 30, 407, 300, 311, 51072, 51072, 983, 321, 500, 380, 362, 2771, 2448, 294, 341, 700, 1389, 11, 457, 321, 360, 362, 2771, 2448, 294, 341, 1389, 11, 570, 321, 51300, 51300, 3122, 528, 281, 14722, 264, 2771, 2448, 365, 3104, 281, 264, 19265, 295, 341, 11101, 13, 51596, 51688], "temperature": 0.0, "avg_logprob": -0.08930028422494952, "compression_ratio": 1.8365384615384615, "no_speech_prob": 8.657174475956708e-06}, {"id": 610, "seek": 425176, "start": 4251.76, "end": 4261.12, "text": " Is the combination of BC loss and sigmoid because, I mean, as a problem because of the underflow?", "tokens": [50364, 1119, 264, 6562, 295, 14359, 4470, 293, 4556, 3280, 327, 570, 11, 286, 914, 11, 382, 257, 1154, 570, 295, 264, 833, 10565, 30, 50832, 50952, 407, 264, 1154, 365, 264, 11, 264, 14359, 551, 510, 307, 264, 31959, 3142, 3109, 11, 558, 30, 407, 341, 4556, 3280, 327, 11, 51364, 51364, 498, 291, 3847, 341, 3209, 588, 731, 11, 264, 4556, 3280, 327, 486, 312, 2902, 291, 4018, 2771, 2448, 13, 51604, 51716], "temperature": 0.0, "avg_logprob": -0.20761915257102564, "compression_ratio": 1.576271186440678, "no_speech_prob": 9.971045074053109e-06}, {"id": 611, "seek": 425176, "start": 4263.52, "end": 4271.76, "text": " So the problem with the, the BC thing here is the probabilistic approach, right? So this sigmoid,", "tokens": [50364, 1119, 264, 6562, 295, 14359, 4470, 293, 4556, 3280, 327, 570, 11, 286, 914, 11, 382, 257, 1154, 570, 295, 264, 833, 10565, 30, 50832, 50952, 407, 264, 1154, 365, 264, 11, 264, 14359, 551, 510, 307, 264, 31959, 3142, 3109, 11, 558, 30, 407, 341, 4556, 3280, 327, 11, 51364, 51364, 498, 291, 3847, 341, 3209, 588, 731, 11, 264, 4556, 3280, 327, 486, 312, 2902, 291, 4018, 2771, 2448, 13, 51604, 51716], "temperature": 0.0, "avg_logprob": -0.20761915257102564, "compression_ratio": 1.576271186440678, "no_speech_prob": 9.971045074053109e-06}, {"id": 612, "seek": 425176, "start": 4271.76, "end": 4276.56, "text": " if you train this network very well, the sigmoid will be giving you zero gradients.", "tokens": [50364, 1119, 264, 6562, 295, 14359, 4470, 293, 4556, 3280, 327, 570, 11, 286, 914, 11, 382, 257, 1154, 570, 295, 264, 833, 10565, 30, 50832, 50952, 407, 264, 1154, 365, 264, 11, 264, 14359, 551, 510, 307, 264, 31959, 3142, 3109, 11, 558, 30, 407, 341, 4556, 3280, 327, 11, 51364, 51364, 498, 291, 3847, 341, 3209, 588, 731, 11, 264, 4556, 3280, 327, 486, 312, 2902, 291, 4018, 2771, 2448, 13, 51604, 51716], "temperature": 0.0, "avg_logprob": -0.20761915257102564, "compression_ratio": 1.576271186440678, "no_speech_prob": 9.971045074053109e-06}, {"id": 613, "seek": 427656, "start": 4276.56, "end": 4281.200000000001, "text": " And because if you saturate, you know, you're going to have, you're in the two, if you're not", "tokens": [50364, 400, 570, 498, 291, 21160, 473, 11, 291, 458, 11, 291, 434, 516, 281, 362, 11, 291, 434, 294, 264, 732, 11, 498, 291, 434, 406, 50596, 50596, 2293, 322, 264, 2808, 636, 11, 498, 291, 366, 445, 1314, 490, 264, 3537, 12866, 11, 291, 434, 516, 281, 362, 50872, 50872, 1936, 420, 472, 11, 370, 309, 311, 920, 516, 281, 362, 4018, 16235, 11, 420, 309, 311, 516, 281, 312, 264, 661, 1252, 510, 11, 51164, 51164, 920, 439, 4018, 11, 457, 456, 307, 572, 16235, 13, 407, 498, 291, 434, 670, 510, 11, 291, 500, 380, 458, 689, 281, 352, 11, 51412, 51456, 577, 281, 352, 760, 264, 10997, 11, 558, 30, 1436, 456, 307, 572, 10997, 13, 821, 307, 411, 257, 39885, 13, 51620, 51648], "temperature": 0.0, "avg_logprob": -0.1893522702730619, "compression_ratio": 1.8685258964143425, "no_speech_prob": 2.6266478016623296e-05}, {"id": 614, "seek": 427656, "start": 4281.200000000001, "end": 4286.72, "text": " exactly on the middle way, if you are just away from the decision boundary, you're going to have", "tokens": [50364, 400, 570, 498, 291, 21160, 473, 11, 291, 458, 11, 291, 434, 516, 281, 362, 11, 291, 434, 294, 264, 732, 11, 498, 291, 434, 406, 50596, 50596, 2293, 322, 264, 2808, 636, 11, 498, 291, 366, 445, 1314, 490, 264, 3537, 12866, 11, 291, 434, 516, 281, 362, 50872, 50872, 1936, 420, 472, 11, 370, 309, 311, 920, 516, 281, 362, 4018, 16235, 11, 420, 309, 311, 516, 281, 312, 264, 661, 1252, 510, 11, 51164, 51164, 920, 439, 4018, 11, 457, 456, 307, 572, 16235, 13, 407, 498, 291, 434, 670, 510, 11, 291, 500, 380, 458, 689, 281, 352, 11, 51412, 51456, 577, 281, 352, 760, 264, 10997, 11, 558, 30, 1436, 456, 307, 572, 10997, 13, 821, 307, 411, 257, 39885, 13, 51620, 51648], "temperature": 0.0, "avg_logprob": -0.1893522702730619, "compression_ratio": 1.8685258964143425, "no_speech_prob": 2.6266478016623296e-05}, {"id": 615, "seek": 427656, "start": 4286.72, "end": 4292.56, "text": " basically or one, so it's still going to have zero gradient, or it's going to be the other side here,", "tokens": [50364, 400, 570, 498, 291, 21160, 473, 11, 291, 458, 11, 291, 434, 516, 281, 362, 11, 291, 434, 294, 264, 732, 11, 498, 291, 434, 406, 50596, 50596, 2293, 322, 264, 2808, 636, 11, 498, 291, 366, 445, 1314, 490, 264, 3537, 12866, 11, 291, 434, 516, 281, 362, 50872, 50872, 1936, 420, 472, 11, 370, 309, 311, 920, 516, 281, 362, 4018, 16235, 11, 420, 309, 311, 516, 281, 312, 264, 661, 1252, 510, 11, 51164, 51164, 920, 439, 4018, 11, 457, 456, 307, 572, 16235, 13, 407, 498, 291, 434, 670, 510, 11, 291, 500, 380, 458, 689, 281, 352, 11, 51412, 51456, 577, 281, 352, 760, 264, 10997, 11, 558, 30, 1436, 456, 307, 572, 10997, 13, 821, 307, 411, 257, 39885, 13, 51620, 51648], "temperature": 0.0, "avg_logprob": -0.1893522702730619, "compression_ratio": 1.8685258964143425, "no_speech_prob": 2.6266478016623296e-05}, {"id": 616, "seek": 427656, "start": 4292.56, "end": 4297.52, "text": " still all zero, but there is no gradient. So if you're over here, you don't know where to go,", "tokens": [50364, 400, 570, 498, 291, 21160, 473, 11, 291, 458, 11, 291, 434, 516, 281, 362, 11, 291, 434, 294, 264, 732, 11, 498, 291, 434, 406, 50596, 50596, 2293, 322, 264, 2808, 636, 11, 498, 291, 366, 445, 1314, 490, 264, 3537, 12866, 11, 291, 434, 516, 281, 362, 50872, 50872, 1936, 420, 472, 11, 370, 309, 311, 920, 516, 281, 362, 4018, 16235, 11, 420, 309, 311, 516, 281, 312, 264, 661, 1252, 510, 11, 51164, 51164, 920, 439, 4018, 11, 457, 456, 307, 572, 16235, 13, 407, 498, 291, 434, 670, 510, 11, 291, 500, 380, 458, 689, 281, 352, 11, 51412, 51456, 577, 281, 352, 760, 264, 10997, 11, 558, 30, 1436, 456, 307, 572, 10997, 13, 821, 307, 411, 257, 39885, 13, 51620, 51648], "temperature": 0.0, "avg_logprob": -0.1893522702730619, "compression_ratio": 1.8685258964143425, "no_speech_prob": 2.6266478016623296e-05}, {"id": 617, "seek": 427656, "start": 4298.400000000001, "end": 4301.68, "text": " how to go down the hill, right? Because there is no hill. There is like a plateau.", "tokens": [50364, 400, 570, 498, 291, 21160, 473, 11, 291, 458, 11, 291, 434, 516, 281, 362, 11, 291, 434, 294, 264, 732, 11, 498, 291, 434, 406, 50596, 50596, 2293, 322, 264, 2808, 636, 11, 498, 291, 366, 445, 1314, 490, 264, 3537, 12866, 11, 291, 434, 516, 281, 362, 50872, 50872, 1936, 420, 472, 11, 370, 309, 311, 920, 516, 281, 362, 4018, 16235, 11, 420, 309, 311, 516, 281, 312, 264, 661, 1252, 510, 11, 51164, 51164, 920, 439, 4018, 11, 457, 456, 307, 572, 16235, 13, 407, 498, 291, 434, 670, 510, 11, 291, 500, 380, 458, 689, 281, 352, 11, 51412, 51456, 577, 281, 352, 760, 264, 10997, 11, 558, 30, 1436, 456, 307, 572, 10997, 13, 821, 307, 411, 257, 39885, 13, 51620, 51648], "temperature": 0.0, "avg_logprob": -0.1893522702730619, "compression_ratio": 1.8685258964143425, "no_speech_prob": 2.6266478016623296e-05}, {"id": 618, "seek": 430168, "start": 4301.68, "end": 4307.68, "text": " So this is the first problem. Second problem is that if you want to really have a very vertical,", "tokens": [50364, 407, 341, 307, 264, 700, 1154, 13, 5736, 1154, 307, 300, 498, 291, 528, 281, 534, 362, 257, 588, 9429, 11, 50664, 50724, 411, 257, 588, 9429, 4691, 510, 11, 291, 486, 643, 588, 11, 588, 11, 588, 11, 588, 2416, 17443, 13, 1033, 13, 51036, 51076, 9653, 300, 498, 291, 11, 291, 458, 11, 264, 4833, 264, 3364, 11, 264, 4833, 307, 516, 281, 312, 264, 2572, 51276, 51276, 2158, 1854, 264, 4556, 3280, 327, 13, 400, 498, 291, 528, 281, 483, 411, 257, 25408, 4556, 3280, 327, 11, 291, 434, 516, 281, 362, 411, 51500, 51548], "temperature": 0.0, "avg_logprob": -0.18210792541503906, "compression_ratio": 1.87, "no_speech_prob": 3.215492324670777e-05}, {"id": 619, "seek": 430168, "start": 4308.88, "end": 4315.12, "text": " like a very vertical edge here, you will need very, very, very, very large weights. Okay.", "tokens": [50364, 407, 341, 307, 264, 700, 1154, 13, 5736, 1154, 307, 300, 498, 291, 528, 281, 534, 362, 257, 588, 9429, 11, 50664, 50724, 411, 257, 588, 9429, 4691, 510, 11, 291, 486, 643, 588, 11, 588, 11, 588, 11, 588, 2416, 17443, 13, 1033, 13, 51036, 51076, 9653, 300, 498, 291, 11, 291, 458, 11, 264, 4833, 264, 3364, 11, 264, 4833, 307, 516, 281, 312, 264, 2572, 51276, 51276, 2158, 1854, 264, 4556, 3280, 327, 13, 400, 498, 291, 528, 281, 483, 411, 257, 25408, 4556, 3280, 327, 11, 291, 434, 516, 281, 362, 411, 51500, 51548], "temperature": 0.0, "avg_logprob": -0.18210792541503906, "compression_ratio": 1.87, "no_speech_prob": 3.215492324670777e-05}, {"id": 620, "seek": 430168, "start": 4315.92, "end": 4319.92, "text": " Such that if you, you know, the larger the weight, the larger is going to be the final", "tokens": [50364, 407, 341, 307, 264, 700, 1154, 13, 5736, 1154, 307, 300, 498, 291, 528, 281, 534, 362, 257, 588, 9429, 11, 50664, 50724, 411, 257, 588, 9429, 4691, 510, 11, 291, 486, 643, 588, 11, 588, 11, 588, 11, 588, 2416, 17443, 13, 1033, 13, 51036, 51076, 9653, 300, 498, 291, 11, 291, 458, 11, 264, 4833, 264, 3364, 11, 264, 4833, 307, 516, 281, 312, 264, 2572, 51276, 51276, 2158, 1854, 264, 4556, 3280, 327, 13, 400, 498, 291, 528, 281, 483, 411, 257, 25408, 4556, 3280, 327, 11, 291, 434, 516, 281, 362, 411, 51500, 51548], "temperature": 0.0, "avg_logprob": -0.18210792541503906, "compression_ratio": 1.87, "no_speech_prob": 3.215492324670777e-05}, {"id": 621, "seek": 430168, "start": 4319.92, "end": 4324.400000000001, "text": " value inside the sigmoid. And if you want to get like a saturated sigmoid, you're going to have like", "tokens": [50364, 407, 341, 307, 264, 700, 1154, 13, 5736, 1154, 307, 300, 498, 291, 528, 281, 534, 362, 257, 588, 9429, 11, 50664, 50724, 411, 257, 588, 9429, 4691, 510, 11, 291, 486, 643, 588, 11, 588, 11, 588, 11, 588, 2416, 17443, 13, 1033, 13, 51036, 51076, 9653, 300, 498, 291, 11, 291, 458, 11, 264, 4833, 264, 3364, 11, 264, 4833, 307, 516, 281, 312, 264, 2572, 51276, 51276, 2158, 1854, 264, 4556, 3280, 327, 13, 400, 498, 291, 528, 281, 483, 411, 257, 25408, 4556, 3280, 327, 11, 291, 434, 516, 281, 362, 411, 51500, 51548], "temperature": 0.0, "avg_logprob": -0.18210792541503906, "compression_ratio": 1.87, "no_speech_prob": 3.215492324670777e-05}, {"id": 622, "seek": 432440, "start": 4324.4, "end": 4331.04, "text": " pretty large weights leading to that module. And this one creates some, you know,", "tokens": [50364, 1238, 2416, 17443, 5775, 281, 300, 10088, 13, 400, 341, 472, 7829, 512, 11, 291, 458, 11, 50696, 50696, 309, 311, 516, 281, 652, 428, 17443, 293, 1203, 733, 295, 21411, 13, 407, 300, 311, 983, 561, 528, 281, 360, 50960, 50960, 2940, 721, 411, 436, 528, 281, 4948, 264, 2026, 295, 264, 17443, 11, 550, 291, 528, 281, 4948, 264, 2026, 295, 51264, 51264, 264, 2771, 2448, 13, 400, 456, 366, 867, 11, 867, 2098, 281, 9972, 341, 9482, 11, 457, 300, 311, 11, 300, 311, 264, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.22305559075396994, "compression_ratio": 1.7547169811320755, "no_speech_prob": 6.690547888865694e-05}, {"id": 623, "seek": 432440, "start": 4331.04, "end": 4336.32, "text": " it's going to make your weights and everything kind of explode. So that's why people want to do", "tokens": [50364, 1238, 2416, 17443, 5775, 281, 300, 10088, 13, 400, 341, 472, 7829, 512, 11, 291, 458, 11, 50696, 50696, 309, 311, 516, 281, 652, 428, 17443, 293, 1203, 733, 295, 21411, 13, 407, 300, 311, 983, 561, 528, 281, 360, 50960, 50960, 2940, 721, 411, 436, 528, 281, 4948, 264, 2026, 295, 264, 17443, 11, 550, 291, 528, 281, 4948, 264, 2026, 295, 51264, 51264, 264, 2771, 2448, 13, 400, 456, 366, 867, 11, 867, 2098, 281, 9972, 341, 9482, 11, 457, 300, 311, 11, 300, 311, 264, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.22305559075396994, "compression_ratio": 1.7547169811320755, "no_speech_prob": 6.690547888865694e-05}, {"id": 624, "seek": 432440, "start": 4336.32, "end": 4342.4, "text": " several things like they want to limit the norm of the weights, then you want to limit the norm of", "tokens": [50364, 1238, 2416, 17443, 5775, 281, 300, 10088, 13, 400, 341, 472, 7829, 512, 11, 291, 458, 11, 50696, 50696, 309, 311, 516, 281, 652, 428, 17443, 293, 1203, 733, 295, 21411, 13, 407, 300, 311, 983, 561, 528, 281, 360, 50960, 50960, 2940, 721, 411, 436, 528, 281, 4948, 264, 2026, 295, 264, 17443, 11, 550, 291, 528, 281, 4948, 264, 2026, 295, 51264, 51264, 264, 2771, 2448, 13, 400, 456, 366, 867, 11, 867, 2098, 281, 9972, 341, 9482, 11, 457, 300, 311, 11, 300, 311, 264, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.22305559075396994, "compression_ratio": 1.7547169811320755, "no_speech_prob": 6.690547888865694e-05}, {"id": 625, "seek": 432440, "start": 4342.4, "end": 4350.96, "text": " the gradients. And there are many, many ways to patch this architecture, but that's, that's the", "tokens": [50364, 1238, 2416, 17443, 5775, 281, 300, 10088, 13, 400, 341, 472, 7829, 512, 11, 291, 458, 11, 50696, 50696, 309, 311, 516, 281, 652, 428, 17443, 293, 1203, 733, 295, 21411, 13, 407, 300, 311, 983, 561, 528, 281, 360, 50960, 50960, 2940, 721, 411, 436, 528, 281, 4948, 264, 2026, 295, 264, 17443, 11, 550, 291, 528, 281, 4948, 264, 2026, 295, 51264, 51264, 264, 2771, 2448, 13, 400, 456, 366, 867, 11, 867, 2098, 281, 9972, 341, 9482, 11, 457, 300, 311, 11, 300, 311, 264, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.22305559075396994, "compression_ratio": 1.7547169811320755, "no_speech_prob": 6.690547888865694e-05}, {"id": 626, "seek": 435096, "start": 4350.96, "end": 4356.4, "text": " patching, right? We don't want patching. We'd like to know what is proper and what is proper is going", "tokens": [50364, 9972, 278, 11, 558, 30, 492, 500, 380, 528, 9972, 278, 13, 492, 1116, 411, 281, 458, 437, 307, 2296, 293, 437, 307, 2296, 307, 516, 50636, 50636, 281, 312, 1936, 11, 293, 1228, 257, 8399, 22660, 19866, 11, 337, 1365, 11, 337, 428, 2572, 2063, 3209, 13, 51008, 51048, 407, 498, 291, 1949, 264, 31565, 6713, 295, 364, 8399, 22660, 19866, 11, 264, 31565, 6713, 295, 364, 51280, 51280, 8399, 22660, 19866, 486, 312, 4018, 420, 1359, 13, 759, 291, 2893, 257, 1412, 300, 307, 1348, 490, 264, 3097, 7316, 11, 51548, 51580, 498, 291, 2893, 257, 6889, 300, 307, 1314, 490, 264, 3097, 7316, 11, 51716, 51744], "temperature": 0.0, "avg_logprob": -0.2574961894267314, "compression_ratio": 1.9567099567099566, "no_speech_prob": 2.7097725251223892e-05}, {"id": 627, "seek": 435096, "start": 4356.4, "end": 4363.84, "text": " to be basically, and using a autoencoder, for example, for your final cost network.", "tokens": [50364, 9972, 278, 11, 558, 30, 492, 500, 380, 528, 9972, 278, 13, 492, 1116, 411, 281, 458, 437, 307, 2296, 293, 437, 307, 2296, 307, 516, 50636, 50636, 281, 312, 1936, 11, 293, 1228, 257, 8399, 22660, 19866, 11, 337, 1365, 11, 337, 428, 2572, 2063, 3209, 13, 51008, 51048, 407, 498, 291, 1949, 264, 31565, 6713, 295, 364, 8399, 22660, 19866, 11, 264, 31565, 6713, 295, 364, 51280, 51280, 8399, 22660, 19866, 486, 312, 4018, 420, 1359, 13, 759, 291, 2893, 257, 1412, 300, 307, 1348, 490, 264, 3097, 7316, 11, 51548, 51580, 498, 291, 2893, 257, 6889, 300, 307, 1314, 490, 264, 3097, 7316, 11, 51716, 51744], "temperature": 0.0, "avg_logprob": -0.2574961894267314, "compression_ratio": 1.9567099567099566, "no_speech_prob": 2.7097725251223892e-05}, {"id": 628, "seek": 435096, "start": 4364.64, "end": 4369.28, "text": " So if you consider the reconstruction error of an autoencoder, the reconstruction error of an", "tokens": [50364, 9972, 278, 11, 558, 30, 492, 500, 380, 528, 9972, 278, 13, 492, 1116, 411, 281, 458, 437, 307, 2296, 293, 437, 307, 2296, 307, 516, 50636, 50636, 281, 312, 1936, 11, 293, 1228, 257, 8399, 22660, 19866, 11, 337, 1365, 11, 337, 428, 2572, 2063, 3209, 13, 51008, 51048, 407, 498, 291, 1949, 264, 31565, 6713, 295, 364, 8399, 22660, 19866, 11, 264, 31565, 6713, 295, 364, 51280, 51280, 8399, 22660, 19866, 486, 312, 4018, 420, 1359, 13, 759, 291, 2893, 257, 1412, 300, 307, 1348, 490, 264, 3097, 7316, 11, 51548, 51580, 498, 291, 2893, 257, 6889, 300, 307, 1314, 490, 264, 3097, 7316, 11, 51716, 51744], "temperature": 0.0, "avg_logprob": -0.2574961894267314, "compression_ratio": 1.9567099567099566, "no_speech_prob": 2.7097725251223892e-05}, {"id": 629, "seek": 435096, "start": 4369.28, "end": 4374.64, "text": " autoencoder will be zero or small. If you provide a data that is coming from the training distribution,", "tokens": [50364, 9972, 278, 11, 558, 30, 492, 500, 380, 528, 9972, 278, 13, 492, 1116, 411, 281, 458, 437, 307, 2296, 293, 437, 307, 2296, 307, 516, 50636, 50636, 281, 312, 1936, 11, 293, 1228, 257, 8399, 22660, 19866, 11, 337, 1365, 11, 337, 428, 2572, 2063, 3209, 13, 51008, 51048, 407, 498, 291, 1949, 264, 31565, 6713, 295, 364, 8399, 22660, 19866, 11, 264, 31565, 6713, 295, 364, 51280, 51280, 8399, 22660, 19866, 486, 312, 4018, 420, 1359, 13, 759, 291, 2893, 257, 1412, 300, 307, 1348, 490, 264, 3097, 7316, 11, 51548, 51580, 498, 291, 2893, 257, 6889, 300, 307, 1314, 490, 264, 3097, 7316, 11, 51716, 51744], "temperature": 0.0, "avg_logprob": -0.2574961894267314, "compression_ratio": 1.9567099567099566, "no_speech_prob": 2.7097725251223892e-05}, {"id": 630, "seek": 435096, "start": 4375.28, "end": 4378.0, "text": " if you provide a sample that is away from the training distribution,", "tokens": [50364, 9972, 278, 11, 558, 30, 492, 500, 380, 528, 9972, 278, 13, 492, 1116, 411, 281, 458, 437, 307, 2296, 293, 437, 307, 2296, 307, 516, 50636, 50636, 281, 312, 1936, 11, 293, 1228, 257, 8399, 22660, 19866, 11, 337, 1365, 11, 337, 428, 2572, 2063, 3209, 13, 51008, 51048, 407, 498, 291, 1949, 264, 31565, 6713, 295, 364, 8399, 22660, 19866, 11, 264, 31565, 6713, 295, 364, 51280, 51280, 8399, 22660, 19866, 486, 312, 4018, 420, 1359, 13, 759, 291, 2893, 257, 1412, 300, 307, 1348, 490, 264, 3097, 7316, 11, 51548, 51580, 498, 291, 2893, 257, 6889, 300, 307, 1314, 490, 264, 3097, 7316, 11, 51716, 51744], "temperature": 0.0, "avg_logprob": -0.2574961894267314, "compression_ratio": 1.9567099567099566, "no_speech_prob": 2.7097725251223892e-05}, {"id": 631, "seek": 437800, "start": 4378.0, "end": 4383.6, "text": " remember the manifold from last time, then the autoencoder will do a poor job at the reconstruction", "tokens": [50364, 1604, 264, 47138, 490, 1036, 565, 11, 550, 264, 8399, 22660, 19866, 486, 360, 257, 4716, 1691, 412, 264, 31565, 50644, 50644, 293, 4412, 264, 31565, 6713, 486, 312, 4833, 13, 407, 2602, 295, 1228, 257, 20828, 1639, 11, 50896, 50896, 291, 393, 764, 257, 8399, 22660, 19866, 31565, 6713, 13, 1012, 393, 291, 483, 544, 484, 295, 341, 1164, 4787, 30, 51308, 51308, 407, 718, 385, 976, 291, 257, 1326, 13396, 13, 2386, 11, 44991, 13, 759, 746, 390, 920, 406, 1850, 11, 51536, 51536, 445, 1029, 385, 264, 1168, 3541, 2507, 264, 960, 13, 286, 486, 1867, 309, 13, 407, 718, 311, 584, 300, 291, 362, 257, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.30563351116349213, "compression_ratio": 1.7418181818181817, "no_speech_prob": 2.5040673790499568e-05}, {"id": 632, "seek": 437800, "start": 4383.6, "end": 4388.64, "text": " and therefore the reconstruction error will be larger. So instead of using a discriminator,", "tokens": [50364, 1604, 264, 47138, 490, 1036, 565, 11, 550, 264, 8399, 22660, 19866, 486, 360, 257, 4716, 1691, 412, 264, 31565, 50644, 50644, 293, 4412, 264, 31565, 6713, 486, 312, 4833, 13, 407, 2602, 295, 1228, 257, 20828, 1639, 11, 50896, 50896, 291, 393, 764, 257, 8399, 22660, 19866, 31565, 6713, 13, 1012, 393, 291, 483, 544, 484, 295, 341, 1164, 4787, 30, 51308, 51308, 407, 718, 385, 976, 291, 257, 1326, 13396, 13, 2386, 11, 44991, 13, 759, 746, 390, 920, 406, 1850, 11, 51536, 51536, 445, 1029, 385, 264, 1168, 3541, 2507, 264, 960, 13, 286, 486, 1867, 309, 13, 407, 718, 311, 584, 300, 291, 362, 257, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.30563351116349213, "compression_ratio": 1.7418181818181817, "no_speech_prob": 2.5040673790499568e-05}, {"id": 633, "seek": 437800, "start": 4388.64, "end": 4396.88, "text": " you can use a autoencoder reconstruction error. How can you get more out of this course overall?", "tokens": [50364, 1604, 264, 47138, 490, 1036, 565, 11, 550, 264, 8399, 22660, 19866, 486, 360, 257, 4716, 1691, 412, 264, 31565, 50644, 50644, 293, 4412, 264, 31565, 6713, 486, 312, 4833, 13, 407, 2602, 295, 1228, 257, 20828, 1639, 11, 50896, 50896, 291, 393, 764, 257, 8399, 22660, 19866, 31565, 6713, 13, 1012, 393, 291, 483, 544, 484, 295, 341, 1164, 4787, 30, 51308, 51308, 407, 718, 385, 976, 291, 257, 1326, 13396, 13, 2386, 11, 44991, 13, 759, 746, 390, 920, 406, 1850, 11, 51536, 51536, 445, 1029, 385, 264, 1168, 3541, 2507, 264, 960, 13, 286, 486, 1867, 309, 13, 407, 718, 311, 584, 300, 291, 362, 257, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.30563351116349213, "compression_ratio": 1.7418181818181817, "no_speech_prob": 2.5040673790499568e-05}, {"id": 634, "seek": 437800, "start": 4396.88, "end": 4401.44, "text": " So let me give you a few suggestions. First, comprehension. If something was still not clear,", "tokens": [50364, 1604, 264, 47138, 490, 1036, 565, 11, 550, 264, 8399, 22660, 19866, 486, 360, 257, 4716, 1691, 412, 264, 31565, 50644, 50644, 293, 4412, 264, 31565, 6713, 486, 312, 4833, 13, 407, 2602, 295, 1228, 257, 20828, 1639, 11, 50896, 50896, 291, 393, 764, 257, 8399, 22660, 19866, 31565, 6713, 13, 1012, 393, 291, 483, 544, 484, 295, 341, 1164, 4787, 30, 51308, 51308, 407, 718, 385, 976, 291, 257, 1326, 13396, 13, 2386, 11, 44991, 13, 759, 746, 390, 920, 406, 1850, 11, 51536, 51536, 445, 1029, 385, 264, 1168, 3541, 2507, 264, 960, 13, 286, 486, 1867, 309, 13, 407, 718, 311, 584, 300, 291, 362, 257, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.30563351116349213, "compression_ratio": 1.7418181818181817, "no_speech_prob": 2.5040673790499568e-05}, {"id": 635, "seek": 437800, "start": 4401.44, "end": 4407.44, "text": " just ask me the question section below the video. I will answer it. So let's say that you have a", "tokens": [50364, 1604, 264, 47138, 490, 1036, 565, 11, 550, 264, 8399, 22660, 19866, 486, 360, 257, 4716, 1691, 412, 264, 31565, 50644, 50644, 293, 4412, 264, 31565, 6713, 486, 312, 4833, 13, 407, 2602, 295, 1228, 257, 20828, 1639, 11, 50896, 50896, 291, 393, 764, 257, 8399, 22660, 19866, 31565, 6713, 13, 1012, 393, 291, 483, 544, 484, 295, 341, 1164, 4787, 30, 51308, 51308, 407, 718, 385, 976, 291, 257, 1326, 13396, 13, 2386, 11, 44991, 13, 759, 746, 390, 920, 406, 1850, 11, 51536, 51536, 445, 1029, 385, 264, 1168, 3541, 2507, 264, 960, 13, 286, 486, 1867, 309, 13, 407, 718, 311, 584, 300, 291, 362, 257, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.30563351116349213, "compression_ratio": 1.7418181818181817, "no_speech_prob": 2.5040673790499568e-05}, {"id": 636, "seek": 440744, "start": 4407.44, "end": 4413.36, "text": " video. I will answer every question. So you will get it eventually. If you'd like to get more news", "tokens": [50364, 960, 13, 286, 486, 1867, 633, 1168, 13, 407, 291, 486, 483, 309, 4728, 13, 759, 291, 1116, 411, 281, 483, 544, 2583, 50660, 50660, 466, 264, 2519, 11, 721, 286, 360, 294, 2115, 295, 10189, 2701, 293, 721, 286, 915, 1880, 11, 50948, 50948, 291, 393, 1524, 493, 322, 5794, 293, 456, 291, 362, 452, 4813, 11, 21996, 34, 45, 57, 13, 759, 291, 1116, 411, 281, 362, 9205, 51236, 51236, 466, 17628, 2145, 11, 500, 380, 2870, 281, 3022, 281, 264, 2269, 293, 13615, 264, 11554, 4549, 13, 51488, 51488, 759, 291, 767, 411, 341, 960, 11, 500, 380, 2870, 281, 829, 257, 9298, 493, 13, 467, 3665, 382, 731, 30559, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.08041101439386351, "compression_ratio": 1.7418181818181817, "no_speech_prob": 0.0008018818916752934}, {"id": 637, "seek": 440744, "start": 4413.36, "end": 4419.12, "text": " about the field, things I do in terms of educational content and things I find interesting,", "tokens": [50364, 960, 13, 286, 486, 1867, 633, 1168, 13, 407, 291, 486, 483, 309, 4728, 13, 759, 291, 1116, 411, 281, 483, 544, 2583, 50660, 50660, 466, 264, 2519, 11, 721, 286, 360, 294, 2115, 295, 10189, 2701, 293, 721, 286, 915, 1880, 11, 50948, 50948, 291, 393, 1524, 493, 322, 5794, 293, 456, 291, 362, 452, 4813, 11, 21996, 34, 45, 57, 13, 759, 291, 1116, 411, 281, 362, 9205, 51236, 51236, 466, 17628, 2145, 11, 500, 380, 2870, 281, 3022, 281, 264, 2269, 293, 13615, 264, 11554, 4549, 13, 51488, 51488, 759, 291, 767, 411, 341, 960, 11, 500, 380, 2870, 281, 829, 257, 9298, 493, 13, 467, 3665, 382, 731, 30559, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.08041101439386351, "compression_ratio": 1.7418181818181817, "no_speech_prob": 0.0008018818916752934}, {"id": 638, "seek": 440744, "start": 4419.12, "end": 4424.879999999999, "text": " you can follow up on Twitter and there you have my handle, AlfCNZ. If you'd like to have updates", "tokens": [50364, 960, 13, 286, 486, 1867, 633, 1168, 13, 407, 291, 486, 483, 309, 4728, 13, 759, 291, 1116, 411, 281, 483, 544, 2583, 50660, 50660, 466, 264, 2519, 11, 721, 286, 360, 294, 2115, 295, 10189, 2701, 293, 721, 286, 915, 1880, 11, 50948, 50948, 291, 393, 1524, 493, 322, 5794, 293, 456, 291, 362, 452, 4813, 11, 21996, 34, 45, 57, 13, 759, 291, 1116, 411, 281, 362, 9205, 51236, 51236, 466, 17628, 2145, 11, 500, 380, 2870, 281, 3022, 281, 264, 2269, 293, 13615, 264, 11554, 4549, 13, 51488, 51488, 759, 291, 767, 411, 341, 960, 11, 500, 380, 2870, 281, 829, 257, 9298, 493, 13, 467, 3665, 382, 731, 30559, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.08041101439386351, "compression_ratio": 1.7418181818181817, "no_speech_prob": 0.0008018818916752934}, {"id": 639, "seek": 440744, "start": 4424.879999999999, "end": 4429.919999999999, "text": " about newer videos, don't forget to subscribe to the channel and activate the notification bell.", "tokens": [50364, 960, 13, 286, 486, 1867, 633, 1168, 13, 407, 291, 486, 483, 309, 4728, 13, 759, 291, 1116, 411, 281, 483, 544, 2583, 50660, 50660, 466, 264, 2519, 11, 721, 286, 360, 294, 2115, 295, 10189, 2701, 293, 721, 286, 915, 1880, 11, 50948, 50948, 291, 393, 1524, 493, 322, 5794, 293, 456, 291, 362, 452, 4813, 11, 21996, 34, 45, 57, 13, 759, 291, 1116, 411, 281, 362, 9205, 51236, 51236, 466, 17628, 2145, 11, 500, 380, 2870, 281, 3022, 281, 264, 2269, 293, 13615, 264, 11554, 4549, 13, 51488, 51488, 759, 291, 767, 411, 341, 960, 11, 500, 380, 2870, 281, 829, 257, 9298, 493, 13, 467, 3665, 382, 731, 30559, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.08041101439386351, "compression_ratio": 1.7418181818181817, "no_speech_prob": 0.0008018818916752934}, {"id": 640, "seek": 440744, "start": 4429.919999999999, "end": 4435.599999999999, "text": " If you actually like this video, don't forget to put a thumb up. It helps as well recommending", "tokens": [50364, 960, 13, 286, 486, 1867, 633, 1168, 13, 407, 291, 486, 483, 309, 4728, 13, 759, 291, 1116, 411, 281, 483, 544, 2583, 50660, 50660, 466, 264, 2519, 11, 721, 286, 360, 294, 2115, 295, 10189, 2701, 293, 721, 286, 915, 1880, 11, 50948, 50948, 291, 393, 1524, 493, 322, 5794, 293, 456, 291, 362, 452, 4813, 11, 21996, 34, 45, 57, 13, 759, 291, 1116, 411, 281, 362, 9205, 51236, 51236, 466, 17628, 2145, 11, 500, 380, 2870, 281, 3022, 281, 264, 2269, 293, 13615, 264, 11554, 4549, 13, 51488, 51488, 759, 291, 767, 411, 341, 960, 11, 500, 380, 2870, 281, 829, 257, 9298, 493, 13, 467, 3665, 382, 731, 30559, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.08041101439386351, "compression_ratio": 1.7418181818181817, "no_speech_prob": 0.0008018818916752934}, {"id": 641, "seek": 443560, "start": 4435.6, "end": 4440.320000000001, "text": " this video to other people. If you'd like to search the content of this lesson, we have an", "tokens": [50364, 341, 960, 281, 661, 561, 13, 759, 291, 1116, 411, 281, 3164, 264, 2701, 295, 341, 6898, 11, 321, 362, 364, 50600, 50600, 3669, 35288, 597, 307, 4582, 3838, 281, 341, 960, 13, 407, 633, 4876, 294, 264, 35288, 50876, 50876, 307, 2052, 712, 13, 759, 291, 2052, 322, 264, 4876, 11, 291, 483, 264, 558, 5391, 281, 264, 3006, 4914, 294, 264, 51096, 51096, 960, 13, 682, 264, 912, 636, 11, 1184, 3541, 295, 264, 960, 307, 264, 912, 4876, 382, 294, 264, 35288, 13, 407, 291, 51308, 51308, 393, 352, 646, 293, 5220, 13, 2704, 3669, 307, 406, 428, 700, 2856, 13, 3457, 2081, 48486, 11, 3025, 1832, 64, 637, 4535, 401, 11, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.14672163955303802, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.005878360942006111}, {"id": 642, "seek": 443560, "start": 4440.320000000001, "end": 4445.84, "text": " English transcription which is connected directly to this video. So every title in the transcription", "tokens": [50364, 341, 960, 281, 661, 561, 13, 759, 291, 1116, 411, 281, 3164, 264, 2701, 295, 341, 6898, 11, 321, 362, 364, 50600, 50600, 3669, 35288, 597, 307, 4582, 3838, 281, 341, 960, 13, 407, 633, 4876, 294, 264, 35288, 50876, 50876, 307, 2052, 712, 13, 759, 291, 2052, 322, 264, 4876, 11, 291, 483, 264, 558, 5391, 281, 264, 3006, 4914, 294, 264, 51096, 51096, 960, 13, 682, 264, 912, 636, 11, 1184, 3541, 295, 264, 960, 307, 264, 912, 4876, 382, 294, 264, 35288, 13, 407, 291, 51308, 51308, 393, 352, 646, 293, 5220, 13, 2704, 3669, 307, 406, 428, 700, 2856, 13, 3457, 2081, 48486, 11, 3025, 1832, 64, 637, 4535, 401, 11, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.14672163955303802, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.005878360942006111}, {"id": 643, "seek": 443560, "start": 4445.84, "end": 4450.240000000001, "text": " is clickable. If you click on the title, you get the right director to the correct location in the", "tokens": [50364, 341, 960, 281, 661, 561, 13, 759, 291, 1116, 411, 281, 3164, 264, 2701, 295, 341, 6898, 11, 321, 362, 364, 50600, 50600, 3669, 35288, 597, 307, 4582, 3838, 281, 341, 960, 13, 407, 633, 4876, 294, 264, 35288, 50876, 50876, 307, 2052, 712, 13, 759, 291, 2052, 322, 264, 4876, 11, 291, 483, 264, 558, 5391, 281, 264, 3006, 4914, 294, 264, 51096, 51096, 960, 13, 682, 264, 912, 636, 11, 1184, 3541, 295, 264, 960, 307, 264, 912, 4876, 382, 294, 264, 35288, 13, 407, 291, 51308, 51308, 393, 352, 646, 293, 5220, 13, 2704, 3669, 307, 406, 428, 700, 2856, 13, 3457, 2081, 48486, 11, 3025, 1832, 64, 637, 4535, 401, 11, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.14672163955303802, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.005878360942006111}, {"id": 644, "seek": 443560, "start": 4450.240000000001, "end": 4454.4800000000005, "text": " video. In the same way, each section of the video is the same title as in the transcription. So you", "tokens": [50364, 341, 960, 281, 661, 561, 13, 759, 291, 1116, 411, 281, 3164, 264, 2701, 295, 341, 6898, 11, 321, 362, 364, 50600, 50600, 3669, 35288, 597, 307, 4582, 3838, 281, 341, 960, 13, 407, 633, 4876, 294, 264, 35288, 50876, 50876, 307, 2052, 712, 13, 759, 291, 2052, 322, 264, 4876, 11, 291, 483, 264, 558, 5391, 281, 264, 3006, 4914, 294, 264, 51096, 51096, 960, 13, 682, 264, 912, 636, 11, 1184, 3541, 295, 264, 960, 307, 264, 912, 4876, 382, 294, 264, 35288, 13, 407, 291, 51308, 51308, 393, 352, 646, 293, 5220, 13, 2704, 3669, 307, 406, 428, 700, 2856, 13, 3457, 2081, 48486, 11, 3025, 1832, 64, 637, 4535, 401, 11, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.14672163955303802, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.005878360942006111}, {"id": 645, "seek": 443560, "start": 4454.4800000000005, "end": 4461.360000000001, "text": " can go back and forth. Maybe English is not your first language. Parli italiano, hablessa spagnol,", "tokens": [50364, 341, 960, 281, 661, 561, 13, 759, 291, 1116, 411, 281, 3164, 264, 2701, 295, 341, 6898, 11, 321, 362, 364, 50600, 50600, 3669, 35288, 597, 307, 4582, 3838, 281, 341, 960, 13, 407, 633, 4876, 294, 264, 35288, 50876, 50876, 307, 2052, 712, 13, 759, 291, 2052, 322, 264, 4876, 11, 291, 483, 264, 558, 5391, 281, 264, 3006, 4914, 294, 264, 51096, 51096, 960, 13, 682, 264, 912, 636, 11, 1184, 3541, 295, 264, 960, 307, 264, 912, 4876, 382, 294, 264, 35288, 13, 407, 291, 51308, 51308, 393, 352, 646, 293, 5220, 13, 2704, 3669, 307, 406, 428, 700, 2856, 13, 3457, 2081, 48486, 11, 3025, 1832, 64, 637, 4535, 401, 11, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.14672163955303802, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.005878360942006111}, {"id": 646, "seek": 446136, "start": 4461.36, "end": 4466.32, "text": " nishuopo, don't quama, speak Korean. I have no idea how to speak Korean. Well, we have several", "tokens": [50364, 297, 742, 84, 404, 78, 11, 500, 380, 421, 2404, 11, 1710, 6933, 13, 286, 362, 572, 1558, 577, 281, 1710, 6933, 13, 1042, 11, 321, 362, 2940, 50612, 50612, 37578, 295, 341, 2527, 2435, 322, 264, 3144, 13, 407, 11, 293, 321, 366, 611, 1237, 337, 544, 50912, 50912, 37578, 498, 291, 393, 854, 382, 731, 13, 467, 311, 534, 1021, 300, 291, 767, 853, 281, 360, 512, 295, 264, 51204, 51204, 11900, 293, 291, 862, 926, 365, 264, 43782, 293, 264, 4009, 3089, 321, 5649, 294, 1668, 281, 51480, 51480, 6920, 1125, 293, 1223, 1101, 264, 10392, 321, 2903, 1830, 264, 8820, 13, 4839, 2024, 1169, 11, 341, 307, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.16410757977029553, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0018643307266756892}, {"id": 647, "seek": 446136, "start": 4466.32, "end": 4472.32, "text": " translations of this material available on the website. So, and we are also looking for more", "tokens": [50364, 297, 742, 84, 404, 78, 11, 500, 380, 421, 2404, 11, 1710, 6933, 13, 286, 362, 572, 1558, 577, 281, 1710, 6933, 13, 1042, 11, 321, 362, 2940, 50612, 50612, 37578, 295, 341, 2527, 2435, 322, 264, 3144, 13, 407, 11, 293, 321, 366, 611, 1237, 337, 544, 50912, 50912, 37578, 498, 291, 393, 854, 382, 731, 13, 467, 311, 534, 1021, 300, 291, 767, 853, 281, 360, 512, 295, 264, 51204, 51204, 11900, 293, 291, 862, 926, 365, 264, 43782, 293, 264, 4009, 3089, 321, 5649, 294, 1668, 281, 51480, 51480, 6920, 1125, 293, 1223, 1101, 264, 10392, 321, 2903, 1830, 264, 8820, 13, 4839, 2024, 1169, 11, 341, 307, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.16410757977029553, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0018643307266756892}, {"id": 648, "seek": 446136, "start": 4472.32, "end": 4478.16, "text": " translations if you can help as well. It's really important that you actually try to do some of the", "tokens": [50364, 297, 742, 84, 404, 78, 11, 500, 380, 421, 2404, 11, 1710, 6933, 13, 286, 362, 572, 1558, 577, 281, 1710, 6933, 13, 1042, 11, 321, 362, 2940, 50612, 50612, 37578, 295, 341, 2527, 2435, 322, 264, 3144, 13, 407, 11, 293, 321, 366, 611, 1237, 337, 544, 50912, 50912, 37578, 498, 291, 393, 854, 382, 731, 13, 467, 311, 534, 1021, 300, 291, 767, 853, 281, 360, 512, 295, 264, 51204, 51204, 11900, 293, 291, 862, 926, 365, 264, 43782, 293, 264, 4009, 3089, 321, 5649, 294, 1668, 281, 51480, 51480, 6920, 1125, 293, 1223, 1101, 264, 10392, 321, 2903, 1830, 264, 8820, 13, 4839, 2024, 1169, 11, 341, 307, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.16410757977029553, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0018643307266756892}, {"id": 649, "seek": 446136, "start": 4478.16, "end": 4483.679999999999, "text": " exercises and you play around with the notebooks and the source code we provided in order to", "tokens": [50364, 297, 742, 84, 404, 78, 11, 500, 380, 421, 2404, 11, 1710, 6933, 13, 286, 362, 572, 1558, 577, 281, 1710, 6933, 13, 1042, 11, 321, 362, 2940, 50612, 50612, 37578, 295, 341, 2527, 2435, 322, 264, 3144, 13, 407, 11, 293, 321, 366, 611, 1237, 337, 544, 50912, 50912, 37578, 498, 291, 393, 854, 382, 731, 13, 467, 311, 534, 1021, 300, 291, 767, 853, 281, 360, 512, 295, 264, 51204, 51204, 11900, 293, 291, 862, 926, 365, 264, 43782, 293, 264, 4009, 3089, 321, 5649, 294, 1668, 281, 51480, 51480, 6920, 1125, 293, 1223, 1101, 264, 10392, 321, 2903, 1830, 264, 8820, 13, 4839, 2024, 1169, 11, 341, 307, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.16410757977029553, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0018643307266756892}, {"id": 650, "seek": 446136, "start": 4483.679999999999, "end": 4490.4, "text": " internalize and understand better the concepts we explain during the lessons. Contribute, this is", "tokens": [50364, 297, 742, 84, 404, 78, 11, 500, 380, 421, 2404, 11, 1710, 6933, 13, 286, 362, 572, 1558, 577, 281, 1710, 6933, 13, 1042, 11, 321, 362, 2940, 50612, 50612, 37578, 295, 341, 2527, 2435, 322, 264, 3144, 13, 407, 11, 293, 321, 366, 611, 1237, 337, 544, 50912, 50912, 37578, 498, 291, 393, 854, 382, 731, 13, 467, 311, 534, 1021, 300, 291, 767, 853, 281, 360, 512, 295, 264, 51204, 51204, 11900, 293, 291, 862, 926, 365, 264, 43782, 293, 264, 4009, 3089, 321, 5649, 294, 1668, 281, 51480, 51480, 6920, 1125, 293, 1223, 1101, 264, 10392, 321, 2903, 1830, 264, 8820, 13, 4839, 2024, 1169, 11, 341, 307, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.16410757977029553, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0018643307266756892}, {"id": 651, "seek": 449040, "start": 4490.4, "end": 4496.08, "text": " really giving you the opportunity to show your contribution. For example, you find some typos", "tokens": [50364, 534, 2902, 291, 264, 2650, 281, 855, 428, 13150, 13, 1171, 1365, 11, 291, 915, 512, 2125, 329, 50648, 50648, 294, 264, 2464, 12, 1010, 11, 370, 291, 915, 512, 15120, 294, 264, 43782, 13, 509, 393, 3191, 729, 293, 11, 291, 458, 11, 50896, 50896, 312, 644, 295, 341, 1379, 1716, 538, 7750, 385, 257, 2235, 5308, 322, 23331, 420, 8295, 385, 458, 5911, 13, 51216, 51216, 400, 300, 390, 309, 13, 407, 536, 291, 958, 565, 13, 4621, 6543, 13, 51416], "temperature": 0.0, "avg_logprob": -0.1637370838838465, "compression_ratio": 1.5348837209302326, "no_speech_prob": 0.0003972435079049319}, {"id": 652, "seek": 449040, "start": 4496.08, "end": 4501.04, "text": " in the write-up, so you find some bugs in the notebooks. You can fix those and, you know,", "tokens": [50364, 534, 2902, 291, 264, 2650, 281, 855, 428, 13150, 13, 1171, 1365, 11, 291, 915, 512, 2125, 329, 50648, 50648, 294, 264, 2464, 12, 1010, 11, 370, 291, 915, 512, 15120, 294, 264, 43782, 13, 509, 393, 3191, 729, 293, 11, 291, 458, 11, 50896, 50896, 312, 644, 295, 341, 1379, 1716, 538, 7750, 385, 257, 2235, 5308, 322, 23331, 420, 8295, 385, 458, 5911, 13, 51216, 51216, 400, 300, 390, 309, 13, 407, 536, 291, 958, 565, 13, 4621, 6543, 13, 51416], "temperature": 0.0, "avg_logprob": -0.1637370838838465, "compression_ratio": 1.5348837209302326, "no_speech_prob": 0.0003972435079049319}, {"id": 653, "seek": 449040, "start": 4501.04, "end": 4507.44, "text": " be part of this whole project by sending me a pull request on GitHub or letting me know otherwise.", "tokens": [50364, 534, 2902, 291, 264, 2650, 281, 855, 428, 13150, 13, 1171, 1365, 11, 291, 915, 512, 2125, 329, 50648, 50648, 294, 264, 2464, 12, 1010, 11, 370, 291, 915, 512, 15120, 294, 264, 43782, 13, 509, 393, 3191, 729, 293, 11, 291, 458, 11, 50896, 50896, 312, 644, 295, 341, 1379, 1716, 538, 7750, 385, 257, 2235, 5308, 322, 23331, 420, 8295, 385, 458, 5911, 13, 51216, 51216, 400, 300, 390, 309, 13, 407, 536, 291, 958, 565, 13, 4621, 6543, 13, 51416], "temperature": 0.0, "avg_logprob": -0.1637370838838465, "compression_ratio": 1.5348837209302326, "no_speech_prob": 0.0003972435079049319}, {"id": 654, "seek": 450744, "start": 4507.44, "end": 4520.96, "text": " And that was it. So, see you next time. Bye bye.", "tokens": [50364, 400, 300, 390, 309, 13, 407, 11, 536, 291, 958, 565, 13, 4621, 6543, 13, 51040], "temperature": 0.0, "avg_logprob": -0.341030306286282, "compression_ratio": 0.8571428571428571, "no_speech_prob": 0.001036508590914309}], "language": "en", "video_id": "xYc11zyZ26M", "entity": "Yann LeCun"}}