{"video_id": "YB61HDL7EzE", "title": "6.4 Bias and variance | Diagnosing bias and variance -[Machine Learning | Andrew Ng]", "description": "Second Course:\nAdvanced Learning Algorithms.\n\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 672, "views": 107, "publish_date": "11/04/2022", "timestamp": 1661817600, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " The typical workflow of developing a machine learning system is that you have an idea and you train a model and you almost always find that it doesn't work as well as you wish yet. When I'm training machine learning model, it pretty much never works that well the first time. And so key to the process of building machine learning system is how to decide what to do next in order to improve this performance. I've found across many different applications that looking at the bias and variance of a learning algorithm gives you very good guidance on what to try next. Let's take a look at what this means. You might remember this example from the first course on linear regression, where given this data set, if you were to fill a straight line to it, it doesn't do that well. And we said that this algorithm has high bias or that it underfits this data set. Or if you were to fit a four-folder polynomial, then it has high variance or it overfits. And in the middle, if you fit a quadratic polynomial, then it looks pretty good. And I said that was just right. Because this is a problem with just a single feature x, we could plot the function f and look at it like this. But if you had more features, you can't plot f and visualize whether it's doing well as easily. So instead of trying to look at plots like this, a more systematic way to diagnose or to find out if your algorithm has high bias or high variance will be to look at the performance of your algorithm on the training set and on the cross validation set. In particular, let's look at the example on the left. If you were to compute J train, how well does the algorithm do on the training set? Not that well. So let's say J train here would be high because there are actually pretty large errors between the examples and the actual predictions of the model. And how about Jcv? So Jcv would be if you had a few new examples, maybe examples like that, that the algorithm had not previously seen. And here, the algorithm also doesn't do that well on examples that it had not previously seen. So Jcv would also be high. And one characteristic of an algorithm with high bias, something that is underfitting is that it's not even doing that well on the training set. And so when J train is high, that gives you a strong indicator that this algorithm has high bias. Let's now look at the example on the right. If you were to compute J train, how well is this doing on the training set? Well, it's actually doing great on the training set, fits the training data really well. So J train here will be low. But if you were to evaluate this model on other houses not in the training set, then you find that Jcv, the cross validation error will be quite high. And so a characteristic signature or characteristic cue that your algorithm has high variance will be of Jcv is much higher than J train. In other words, it does much better on data it has seen than on data it has not seen. And this turns out to be a strong indicator that your algorithm has high variance. And again, the point of what we're doing is that by computing J train and Jcv and seeing if J train is high, or if Jcv is much higher than J train, this gives you a sense even if you can't plot to function f of whether your algorithm has high bias or high variance. And finally, the case in the middle, if you look at J train is pretty low since it's doing quite well on the training set. And if you were to look at a few new examples like those from say your cross validation set, you find that Jcv is also pretty low. And so J train not being too high, indicates this doesn't have a high bias problem. And Jcv not being much worse than J train, this indicates that it doesn't have a high variance problem either, which is why this model, the quadratic model seems to be a pretty good one for this application. Let me share with you another view of bias and variance. So to summarize when d equals one for a linear polynomial, J train was high and Jcv was high. When d equals four, J train was low, but Jcv is high. And when d equals two, both were pretty low. Let's now take a different view on bias and variance. And in particular, on the next slide, I'd like to show you how J train and Jcv vary as a function of the degree of the polynomial you're fitting. So let me draw a figure where the horizontal axis of this figure will be the degree of polynomial that we're fitting to the data. Over on the left will correspond to a small value of d, like d equals one, which corresponds to a fitting straight line. And over to the right will correspond to say d equals four or even higher values of d, where we're fitting this high order polynomial. So if you were to plot J train of Wb as a function of degree of polynomial, what you find is that as you fit a higher and higher degree polynomial, here I'm assuming we're not using regularization, but as you fit a higher and higher order polynomial, the training error will tend to go down because when you have a very simple linear function, it doesn't fit the training data that well. When you fit a quadratic function or a third order polynomial or a fourth order polynomial, it fits the training data better and better. So as the degree of polynomial increases, J train will typically go down. Next, let's look at Jcv, which is how well does it do on data that it did not get to fit to? What we saw was when d equals one, when the degree of polynomial was very low, Jcv was pretty high because it underfit, so it didn't do well on the cross validation set. And here on the right as well, when the degree of polynomial is very large, say four, it doesn't do well on the cross validation set either and so is also high. But if d was in between, say a second order polynomial, then it actually did much better. And so if you were to vary the degree of polynomial, you'd actually get a curve that looks like this, which comes down and then goes back up. Where if the degree of polynomial is too low, it underfits and so doesn't do well on the cross validation set. If it is too high, it overfits and also doesn't do well on the cross validation set. And there's only if it's somewhere in the middle that is just right, which is why the second order polynomial in our example ends up with a lower cross validation error and neither high bias nor high variance. So to summarize, how do you diagnose bias and variance in your learning algorithm? If your learning algorithm has high bias or has underfit data, the key indicator will be if Jtrain is high. And so that corresponds to this leftmost portion of the curve, which is where Jtrain is high. And usually you have Jtrain and JCV will be close to each other. And how do you diagnose if you have high variance? Well the key indicator for high variance will be if JCV is much greater than Jtrain. This double greater than sign in math refers to much greater than. So this is greater and this means much greater. And this rightmost portion of the plot is where JCV is much greater than Jtrain. And usually Jtrain will be pretty low, but the key indicator is whether JCV is much greater than Jtrain. And that's what happens when we had fit a very high order polynomial to this small dataset. And even though we've just seen bias and variance, it turns out in some cases it's possible to simultaneously have high bias and have high variance. You won't see this happen that much for linear regression, but it turns out that if you're training a neural network, there are some applications where unfortunately you have high bias and high variance. And one way to recognize that situation will be if Jtrain is high, so you're not doing that well on the training set, but even worse the cross validation error is again even much larger than the training set. The notion of high bias and high variance, it doesn't really happen for linear models applied to 1D, but to give intuition about what it looks like, it would be as if for part of the input you had a very complicated model that overfit, so it overfits to part of the input. But then for some reason for other parts of the input, it doesn't even fit the training data well, and so it underfits for part of the input. In this example, which looks artificial because it's a single feature input, we fit the training set really well and we overfit in part of the input, and we don't even fit the training data well and we underfit in part of the input. And that's how in some applications you can unfortunately end up with both high bias and high variance. And the indicator for that will be if the algorithm does poorly on the training set and it even does much worse than on the training set. For most learning applications, you probably have primarily a high bias or a high variance problem rather than both at the same time, but it is possible sometimes that both are the same time. So I know that there's a lot to process, there are a lot of concepts on the slides, but the key takeaways are high bias means it's not even doing well on the training set, and high variance means it does much worse on the cross-validation set than the training set. Whenever I'm training a machine learning algorithm, I will almost always try to figure out to what extent the algorithm has a high bias or underfitting versus a high variance or an overfitting problem. And this will give good guidance, as we'll see later this week, on how you can improve the performance of the algorithm. But first, let's take a look at how regularization affects the bias and variance of a learning algorithm, because that will help you better understand when you should use regularization. Let's take a look at that in the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.2, "text": " The typical workflow of developing a machine learning system is that you have an idea and", "tokens": [50364, 440, 7476, 20993, 295, 6416, 257, 3479, 2539, 1185, 307, 300, 291, 362, 364, 1558, 293, 50774, 50774, 291, 3847, 257, 2316, 293, 291, 1920, 1009, 915, 300, 309, 1177, 380, 589, 382, 731, 382, 291, 3172, 1939, 13, 51074, 51074, 1133, 286, 478, 3097, 3479, 2539, 2316, 11, 309, 1238, 709, 1128, 1985, 300, 731, 264, 700, 51260, 51260, 565, 13, 51310, 51310, 400, 370, 2141, 281, 264, 1399, 295, 2390, 3479, 2539, 1185, 307, 577, 281, 4536, 437, 281, 360, 51540, 51540, 958, 294, 1668, 281, 3470, 341, 3389, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.15597050388654074, "compression_ratio": 1.7215189873417722, "no_speech_prob": 0.014480539597570896}, {"id": 1, "seek": 0, "start": 8.2, "end": 14.200000000000001, "text": " you train a model and you almost always find that it doesn't work as well as you wish yet.", "tokens": [50364, 440, 7476, 20993, 295, 6416, 257, 3479, 2539, 1185, 307, 300, 291, 362, 364, 1558, 293, 50774, 50774, 291, 3847, 257, 2316, 293, 291, 1920, 1009, 915, 300, 309, 1177, 380, 589, 382, 731, 382, 291, 3172, 1939, 13, 51074, 51074, 1133, 286, 478, 3097, 3479, 2539, 2316, 11, 309, 1238, 709, 1128, 1985, 300, 731, 264, 700, 51260, 51260, 565, 13, 51310, 51310, 400, 370, 2141, 281, 264, 1399, 295, 2390, 3479, 2539, 1185, 307, 577, 281, 4536, 437, 281, 360, 51540, 51540, 958, 294, 1668, 281, 3470, 341, 3389, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.15597050388654074, "compression_ratio": 1.7215189873417722, "no_speech_prob": 0.014480539597570896}, {"id": 2, "seek": 0, "start": 14.200000000000001, "end": 17.92, "text": " When I'm training machine learning model, it pretty much never works that well the first", "tokens": [50364, 440, 7476, 20993, 295, 6416, 257, 3479, 2539, 1185, 307, 300, 291, 362, 364, 1558, 293, 50774, 50774, 291, 3847, 257, 2316, 293, 291, 1920, 1009, 915, 300, 309, 1177, 380, 589, 382, 731, 382, 291, 3172, 1939, 13, 51074, 51074, 1133, 286, 478, 3097, 3479, 2539, 2316, 11, 309, 1238, 709, 1128, 1985, 300, 731, 264, 700, 51260, 51260, 565, 13, 51310, 51310, 400, 370, 2141, 281, 264, 1399, 295, 2390, 3479, 2539, 1185, 307, 577, 281, 4536, 437, 281, 360, 51540, 51540, 958, 294, 1668, 281, 3470, 341, 3389, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.15597050388654074, "compression_ratio": 1.7215189873417722, "no_speech_prob": 0.014480539597570896}, {"id": 3, "seek": 0, "start": 17.92, "end": 18.92, "text": " time.", "tokens": [50364, 440, 7476, 20993, 295, 6416, 257, 3479, 2539, 1185, 307, 300, 291, 362, 364, 1558, 293, 50774, 50774, 291, 3847, 257, 2316, 293, 291, 1920, 1009, 915, 300, 309, 1177, 380, 589, 382, 731, 382, 291, 3172, 1939, 13, 51074, 51074, 1133, 286, 478, 3097, 3479, 2539, 2316, 11, 309, 1238, 709, 1128, 1985, 300, 731, 264, 700, 51260, 51260, 565, 13, 51310, 51310, 400, 370, 2141, 281, 264, 1399, 295, 2390, 3479, 2539, 1185, 307, 577, 281, 4536, 437, 281, 360, 51540, 51540, 958, 294, 1668, 281, 3470, 341, 3389, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.15597050388654074, "compression_ratio": 1.7215189873417722, "no_speech_prob": 0.014480539597570896}, {"id": 4, "seek": 0, "start": 18.92, "end": 23.52, "text": " And so key to the process of building machine learning system is how to decide what to do", "tokens": [50364, 440, 7476, 20993, 295, 6416, 257, 3479, 2539, 1185, 307, 300, 291, 362, 364, 1558, 293, 50774, 50774, 291, 3847, 257, 2316, 293, 291, 1920, 1009, 915, 300, 309, 1177, 380, 589, 382, 731, 382, 291, 3172, 1939, 13, 51074, 51074, 1133, 286, 478, 3097, 3479, 2539, 2316, 11, 309, 1238, 709, 1128, 1985, 300, 731, 264, 700, 51260, 51260, 565, 13, 51310, 51310, 400, 370, 2141, 281, 264, 1399, 295, 2390, 3479, 2539, 1185, 307, 577, 281, 4536, 437, 281, 360, 51540, 51540, 958, 294, 1668, 281, 3470, 341, 3389, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.15597050388654074, "compression_ratio": 1.7215189873417722, "no_speech_prob": 0.014480539597570896}, {"id": 5, "seek": 0, "start": 23.52, "end": 26.12, "text": " next in order to improve this performance.", "tokens": [50364, 440, 7476, 20993, 295, 6416, 257, 3479, 2539, 1185, 307, 300, 291, 362, 364, 1558, 293, 50774, 50774, 291, 3847, 257, 2316, 293, 291, 1920, 1009, 915, 300, 309, 1177, 380, 589, 382, 731, 382, 291, 3172, 1939, 13, 51074, 51074, 1133, 286, 478, 3097, 3479, 2539, 2316, 11, 309, 1238, 709, 1128, 1985, 300, 731, 264, 700, 51260, 51260, 565, 13, 51310, 51310, 400, 370, 2141, 281, 264, 1399, 295, 2390, 3479, 2539, 1185, 307, 577, 281, 4536, 437, 281, 360, 51540, 51540, 958, 294, 1668, 281, 3470, 341, 3389, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.15597050388654074, "compression_ratio": 1.7215189873417722, "no_speech_prob": 0.014480539597570896}, {"id": 6, "seek": 2612, "start": 26.12, "end": 31.04, "text": " I've found across many different applications that looking at the bias and variance of a", "tokens": [50364, 286, 600, 1352, 2108, 867, 819, 5821, 300, 1237, 412, 264, 12577, 293, 21977, 295, 257, 50610, 50610, 2539, 9284, 2709, 291, 588, 665, 10056, 322, 437, 281, 853, 958, 13, 50826, 50826, 961, 311, 747, 257, 574, 412, 437, 341, 1355, 13, 50964, 50964, 509, 1062, 1604, 341, 1365, 490, 264, 700, 1164, 322, 8213, 24590, 11, 689, 2212, 341, 51334, 51334, 1412, 992, 11, 498, 291, 645, 281, 2836, 257, 2997, 1622, 281, 309, 11, 309, 1177, 380, 360, 300, 731, 13, 51541, 51541, 400, 321, 848, 300, 341, 9284, 575, 1090, 12577, 420, 300, 309, 833, 13979, 341, 1412, 992, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.143568339171233, "compression_ratio": 1.6654275092936803, "no_speech_prob": 1.8054797692457214e-05}, {"id": 7, "seek": 2612, "start": 31.04, "end": 35.36, "text": " learning algorithm gives you very good guidance on what to try next.", "tokens": [50364, 286, 600, 1352, 2108, 867, 819, 5821, 300, 1237, 412, 264, 12577, 293, 21977, 295, 257, 50610, 50610, 2539, 9284, 2709, 291, 588, 665, 10056, 322, 437, 281, 853, 958, 13, 50826, 50826, 961, 311, 747, 257, 574, 412, 437, 341, 1355, 13, 50964, 50964, 509, 1062, 1604, 341, 1365, 490, 264, 700, 1164, 322, 8213, 24590, 11, 689, 2212, 341, 51334, 51334, 1412, 992, 11, 498, 291, 645, 281, 2836, 257, 2997, 1622, 281, 309, 11, 309, 1177, 380, 360, 300, 731, 13, 51541, 51541, 400, 321, 848, 300, 341, 9284, 575, 1090, 12577, 420, 300, 309, 833, 13979, 341, 1412, 992, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.143568339171233, "compression_ratio": 1.6654275092936803, "no_speech_prob": 1.8054797692457214e-05}, {"id": 8, "seek": 2612, "start": 35.36, "end": 38.120000000000005, "text": " Let's take a look at what this means.", "tokens": [50364, 286, 600, 1352, 2108, 867, 819, 5821, 300, 1237, 412, 264, 12577, 293, 21977, 295, 257, 50610, 50610, 2539, 9284, 2709, 291, 588, 665, 10056, 322, 437, 281, 853, 958, 13, 50826, 50826, 961, 311, 747, 257, 574, 412, 437, 341, 1355, 13, 50964, 50964, 509, 1062, 1604, 341, 1365, 490, 264, 700, 1164, 322, 8213, 24590, 11, 689, 2212, 341, 51334, 51334, 1412, 992, 11, 498, 291, 645, 281, 2836, 257, 2997, 1622, 281, 309, 11, 309, 1177, 380, 360, 300, 731, 13, 51541, 51541, 400, 321, 848, 300, 341, 9284, 575, 1090, 12577, 420, 300, 309, 833, 13979, 341, 1412, 992, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.143568339171233, "compression_ratio": 1.6654275092936803, "no_speech_prob": 1.8054797692457214e-05}, {"id": 9, "seek": 2612, "start": 38.120000000000005, "end": 45.52, "text": " You might remember this example from the first course on linear regression, where given this", "tokens": [50364, 286, 600, 1352, 2108, 867, 819, 5821, 300, 1237, 412, 264, 12577, 293, 21977, 295, 257, 50610, 50610, 2539, 9284, 2709, 291, 588, 665, 10056, 322, 437, 281, 853, 958, 13, 50826, 50826, 961, 311, 747, 257, 574, 412, 437, 341, 1355, 13, 50964, 50964, 509, 1062, 1604, 341, 1365, 490, 264, 700, 1164, 322, 8213, 24590, 11, 689, 2212, 341, 51334, 51334, 1412, 992, 11, 498, 291, 645, 281, 2836, 257, 2997, 1622, 281, 309, 11, 309, 1177, 380, 360, 300, 731, 13, 51541, 51541, 400, 321, 848, 300, 341, 9284, 575, 1090, 12577, 420, 300, 309, 833, 13979, 341, 1412, 992, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.143568339171233, "compression_ratio": 1.6654275092936803, "no_speech_prob": 1.8054797692457214e-05}, {"id": 10, "seek": 2612, "start": 45.52, "end": 49.66, "text": " data set, if you were to fill a straight line to it, it doesn't do that well.", "tokens": [50364, 286, 600, 1352, 2108, 867, 819, 5821, 300, 1237, 412, 264, 12577, 293, 21977, 295, 257, 50610, 50610, 2539, 9284, 2709, 291, 588, 665, 10056, 322, 437, 281, 853, 958, 13, 50826, 50826, 961, 311, 747, 257, 574, 412, 437, 341, 1355, 13, 50964, 50964, 509, 1062, 1604, 341, 1365, 490, 264, 700, 1164, 322, 8213, 24590, 11, 689, 2212, 341, 51334, 51334, 1412, 992, 11, 498, 291, 645, 281, 2836, 257, 2997, 1622, 281, 309, 11, 309, 1177, 380, 360, 300, 731, 13, 51541, 51541, 400, 321, 848, 300, 341, 9284, 575, 1090, 12577, 420, 300, 309, 833, 13979, 341, 1412, 992, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.143568339171233, "compression_ratio": 1.6654275092936803, "no_speech_prob": 1.8054797692457214e-05}, {"id": 11, "seek": 2612, "start": 49.66, "end": 56.08, "text": " And we said that this algorithm has high bias or that it underfits this data set.", "tokens": [50364, 286, 600, 1352, 2108, 867, 819, 5821, 300, 1237, 412, 264, 12577, 293, 21977, 295, 257, 50610, 50610, 2539, 9284, 2709, 291, 588, 665, 10056, 322, 437, 281, 853, 958, 13, 50826, 50826, 961, 311, 747, 257, 574, 412, 437, 341, 1355, 13, 50964, 50964, 509, 1062, 1604, 341, 1365, 490, 264, 700, 1164, 322, 8213, 24590, 11, 689, 2212, 341, 51334, 51334, 1412, 992, 11, 498, 291, 645, 281, 2836, 257, 2997, 1622, 281, 309, 11, 309, 1177, 380, 360, 300, 731, 13, 51541, 51541, 400, 321, 848, 300, 341, 9284, 575, 1090, 12577, 420, 300, 309, 833, 13979, 341, 1412, 992, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.143568339171233, "compression_ratio": 1.6654275092936803, "no_speech_prob": 1.8054797692457214e-05}, {"id": 12, "seek": 5608, "start": 56.08, "end": 64.2, "text": " Or if you were to fit a four-folder polynomial, then it has high variance or it overfits.", "tokens": [50364, 1610, 498, 291, 645, 281, 3318, 257, 1451, 12, 18353, 260, 26110, 11, 550, 309, 575, 1090, 21977, 420, 309, 670, 13979, 13, 50770, 50770, 400, 294, 264, 2808, 11, 498, 291, 3318, 257, 37262, 26110, 11, 550, 309, 1542, 1238, 665, 13, 51004, 51004, 400, 286, 848, 300, 390, 445, 558, 13, 51133, 51133, 1436, 341, 307, 257, 1154, 365, 445, 257, 2167, 4111, 2031, 11, 321, 727, 7542, 264, 2445, 283, 293, 51366, 51366, 574, 412, 309, 411, 341, 13, 51440, 51440, 583, 498, 291, 632, 544, 4122, 11, 291, 393, 380, 7542, 283, 293, 23273, 1968, 309, 311, 884, 731, 382, 51712, 51712, 3612, 13, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.14683106967381068, "compression_ratio": 1.6558704453441295, "no_speech_prob": 1.3006584595132153e-05}, {"id": 13, "seek": 5608, "start": 64.2, "end": 68.88, "text": " And in the middle, if you fit a quadratic polynomial, then it looks pretty good.", "tokens": [50364, 1610, 498, 291, 645, 281, 3318, 257, 1451, 12, 18353, 260, 26110, 11, 550, 309, 575, 1090, 21977, 420, 309, 670, 13979, 13, 50770, 50770, 400, 294, 264, 2808, 11, 498, 291, 3318, 257, 37262, 26110, 11, 550, 309, 1542, 1238, 665, 13, 51004, 51004, 400, 286, 848, 300, 390, 445, 558, 13, 51133, 51133, 1436, 341, 307, 257, 1154, 365, 445, 257, 2167, 4111, 2031, 11, 321, 727, 7542, 264, 2445, 283, 293, 51366, 51366, 574, 412, 309, 411, 341, 13, 51440, 51440, 583, 498, 291, 632, 544, 4122, 11, 291, 393, 380, 7542, 283, 293, 23273, 1968, 309, 311, 884, 731, 382, 51712, 51712, 3612, 13, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.14683106967381068, "compression_ratio": 1.6558704453441295, "no_speech_prob": 1.3006584595132153e-05}, {"id": 14, "seek": 5608, "start": 68.88, "end": 71.46, "text": " And I said that was just right.", "tokens": [50364, 1610, 498, 291, 645, 281, 3318, 257, 1451, 12, 18353, 260, 26110, 11, 550, 309, 575, 1090, 21977, 420, 309, 670, 13979, 13, 50770, 50770, 400, 294, 264, 2808, 11, 498, 291, 3318, 257, 37262, 26110, 11, 550, 309, 1542, 1238, 665, 13, 51004, 51004, 400, 286, 848, 300, 390, 445, 558, 13, 51133, 51133, 1436, 341, 307, 257, 1154, 365, 445, 257, 2167, 4111, 2031, 11, 321, 727, 7542, 264, 2445, 283, 293, 51366, 51366, 574, 412, 309, 411, 341, 13, 51440, 51440, 583, 498, 291, 632, 544, 4122, 11, 291, 393, 380, 7542, 283, 293, 23273, 1968, 309, 311, 884, 731, 382, 51712, 51712, 3612, 13, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.14683106967381068, "compression_ratio": 1.6558704453441295, "no_speech_prob": 1.3006584595132153e-05}, {"id": 15, "seek": 5608, "start": 71.46, "end": 76.12, "text": " Because this is a problem with just a single feature x, we could plot the function f and", "tokens": [50364, 1610, 498, 291, 645, 281, 3318, 257, 1451, 12, 18353, 260, 26110, 11, 550, 309, 575, 1090, 21977, 420, 309, 670, 13979, 13, 50770, 50770, 400, 294, 264, 2808, 11, 498, 291, 3318, 257, 37262, 26110, 11, 550, 309, 1542, 1238, 665, 13, 51004, 51004, 400, 286, 848, 300, 390, 445, 558, 13, 51133, 51133, 1436, 341, 307, 257, 1154, 365, 445, 257, 2167, 4111, 2031, 11, 321, 727, 7542, 264, 2445, 283, 293, 51366, 51366, 574, 412, 309, 411, 341, 13, 51440, 51440, 583, 498, 291, 632, 544, 4122, 11, 291, 393, 380, 7542, 283, 293, 23273, 1968, 309, 311, 884, 731, 382, 51712, 51712, 3612, 13, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.14683106967381068, "compression_ratio": 1.6558704453441295, "no_speech_prob": 1.3006584595132153e-05}, {"id": 16, "seek": 5608, "start": 76.12, "end": 77.6, "text": " look at it like this.", "tokens": [50364, 1610, 498, 291, 645, 281, 3318, 257, 1451, 12, 18353, 260, 26110, 11, 550, 309, 575, 1090, 21977, 420, 309, 670, 13979, 13, 50770, 50770, 400, 294, 264, 2808, 11, 498, 291, 3318, 257, 37262, 26110, 11, 550, 309, 1542, 1238, 665, 13, 51004, 51004, 400, 286, 848, 300, 390, 445, 558, 13, 51133, 51133, 1436, 341, 307, 257, 1154, 365, 445, 257, 2167, 4111, 2031, 11, 321, 727, 7542, 264, 2445, 283, 293, 51366, 51366, 574, 412, 309, 411, 341, 13, 51440, 51440, 583, 498, 291, 632, 544, 4122, 11, 291, 393, 380, 7542, 283, 293, 23273, 1968, 309, 311, 884, 731, 382, 51712, 51712, 3612, 13, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.14683106967381068, "compression_ratio": 1.6558704453441295, "no_speech_prob": 1.3006584595132153e-05}, {"id": 17, "seek": 5608, "start": 77.6, "end": 83.03999999999999, "text": " But if you had more features, you can't plot f and visualize whether it's doing well as", "tokens": [50364, 1610, 498, 291, 645, 281, 3318, 257, 1451, 12, 18353, 260, 26110, 11, 550, 309, 575, 1090, 21977, 420, 309, 670, 13979, 13, 50770, 50770, 400, 294, 264, 2808, 11, 498, 291, 3318, 257, 37262, 26110, 11, 550, 309, 1542, 1238, 665, 13, 51004, 51004, 400, 286, 848, 300, 390, 445, 558, 13, 51133, 51133, 1436, 341, 307, 257, 1154, 365, 445, 257, 2167, 4111, 2031, 11, 321, 727, 7542, 264, 2445, 283, 293, 51366, 51366, 574, 412, 309, 411, 341, 13, 51440, 51440, 583, 498, 291, 632, 544, 4122, 11, 291, 393, 380, 7542, 283, 293, 23273, 1968, 309, 311, 884, 731, 382, 51712, 51712, 3612, 13, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.14683106967381068, "compression_ratio": 1.6558704453441295, "no_speech_prob": 1.3006584595132153e-05}, {"id": 18, "seek": 5608, "start": 83.03999999999999, "end": 84.8, "text": " easily.", "tokens": [50364, 1610, 498, 291, 645, 281, 3318, 257, 1451, 12, 18353, 260, 26110, 11, 550, 309, 575, 1090, 21977, 420, 309, 670, 13979, 13, 50770, 50770, 400, 294, 264, 2808, 11, 498, 291, 3318, 257, 37262, 26110, 11, 550, 309, 1542, 1238, 665, 13, 51004, 51004, 400, 286, 848, 300, 390, 445, 558, 13, 51133, 51133, 1436, 341, 307, 257, 1154, 365, 445, 257, 2167, 4111, 2031, 11, 321, 727, 7542, 264, 2445, 283, 293, 51366, 51366, 574, 412, 309, 411, 341, 13, 51440, 51440, 583, 498, 291, 632, 544, 4122, 11, 291, 393, 380, 7542, 283, 293, 23273, 1968, 309, 311, 884, 731, 382, 51712, 51712, 3612, 13, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.14683106967381068, "compression_ratio": 1.6558704453441295, "no_speech_prob": 1.3006584595132153e-05}, {"id": 19, "seek": 8480, "start": 84.8, "end": 91.47999999999999, "text": " So instead of trying to look at plots like this, a more systematic way to diagnose or", "tokens": [50364, 407, 2602, 295, 1382, 281, 574, 412, 28609, 411, 341, 11, 257, 544, 27249, 636, 281, 36238, 420, 50698, 50698, 281, 915, 484, 498, 428, 9284, 575, 1090, 12577, 420, 1090, 21977, 486, 312, 281, 574, 412, 264, 3389, 51000, 51000, 295, 428, 9284, 322, 264, 3097, 992, 293, 322, 264, 3278, 24071, 992, 13, 51262, 51262, 682, 1729, 11, 718, 311, 574, 412, 264, 1365, 322, 264, 1411, 13, 51403, 51403, 759, 291, 645, 281, 14722, 508, 3847, 11, 577, 731, 775, 264, 9284, 360, 322, 264, 3097, 992, 30, 51732, 51732, 1726, 300, 731, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.08540758756127688, "compression_ratio": 1.7763157894736843, "no_speech_prob": 2.3687441625952488e-06}, {"id": 20, "seek": 8480, "start": 91.47999999999999, "end": 97.52, "text": " to find out if your algorithm has high bias or high variance will be to look at the performance", "tokens": [50364, 407, 2602, 295, 1382, 281, 574, 412, 28609, 411, 341, 11, 257, 544, 27249, 636, 281, 36238, 420, 50698, 50698, 281, 915, 484, 498, 428, 9284, 575, 1090, 12577, 420, 1090, 21977, 486, 312, 281, 574, 412, 264, 3389, 51000, 51000, 295, 428, 9284, 322, 264, 3097, 992, 293, 322, 264, 3278, 24071, 992, 13, 51262, 51262, 682, 1729, 11, 718, 311, 574, 412, 264, 1365, 322, 264, 1411, 13, 51403, 51403, 759, 291, 645, 281, 14722, 508, 3847, 11, 577, 731, 775, 264, 9284, 360, 322, 264, 3097, 992, 30, 51732, 51732, 1726, 300, 731, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.08540758756127688, "compression_ratio": 1.7763157894736843, "no_speech_prob": 2.3687441625952488e-06}, {"id": 21, "seek": 8480, "start": 97.52, "end": 102.75999999999999, "text": " of your algorithm on the training set and on the cross validation set.", "tokens": [50364, 407, 2602, 295, 1382, 281, 574, 412, 28609, 411, 341, 11, 257, 544, 27249, 636, 281, 36238, 420, 50698, 50698, 281, 915, 484, 498, 428, 9284, 575, 1090, 12577, 420, 1090, 21977, 486, 312, 281, 574, 412, 264, 3389, 51000, 51000, 295, 428, 9284, 322, 264, 3097, 992, 293, 322, 264, 3278, 24071, 992, 13, 51262, 51262, 682, 1729, 11, 718, 311, 574, 412, 264, 1365, 322, 264, 1411, 13, 51403, 51403, 759, 291, 645, 281, 14722, 508, 3847, 11, 577, 731, 775, 264, 9284, 360, 322, 264, 3097, 992, 30, 51732, 51732, 1726, 300, 731, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.08540758756127688, "compression_ratio": 1.7763157894736843, "no_speech_prob": 2.3687441625952488e-06}, {"id": 22, "seek": 8480, "start": 102.75999999999999, "end": 105.58, "text": " In particular, let's look at the example on the left.", "tokens": [50364, 407, 2602, 295, 1382, 281, 574, 412, 28609, 411, 341, 11, 257, 544, 27249, 636, 281, 36238, 420, 50698, 50698, 281, 915, 484, 498, 428, 9284, 575, 1090, 12577, 420, 1090, 21977, 486, 312, 281, 574, 412, 264, 3389, 51000, 51000, 295, 428, 9284, 322, 264, 3097, 992, 293, 322, 264, 3278, 24071, 992, 13, 51262, 51262, 682, 1729, 11, 718, 311, 574, 412, 264, 1365, 322, 264, 1411, 13, 51403, 51403, 759, 291, 645, 281, 14722, 508, 3847, 11, 577, 731, 775, 264, 9284, 360, 322, 264, 3097, 992, 30, 51732, 51732, 1726, 300, 731, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.08540758756127688, "compression_ratio": 1.7763157894736843, "no_speech_prob": 2.3687441625952488e-06}, {"id": 23, "seek": 8480, "start": 105.58, "end": 112.16, "text": " If you were to compute J train, how well does the algorithm do on the training set?", "tokens": [50364, 407, 2602, 295, 1382, 281, 574, 412, 28609, 411, 341, 11, 257, 544, 27249, 636, 281, 36238, 420, 50698, 50698, 281, 915, 484, 498, 428, 9284, 575, 1090, 12577, 420, 1090, 21977, 486, 312, 281, 574, 412, 264, 3389, 51000, 51000, 295, 428, 9284, 322, 264, 3097, 992, 293, 322, 264, 3278, 24071, 992, 13, 51262, 51262, 682, 1729, 11, 718, 311, 574, 412, 264, 1365, 322, 264, 1411, 13, 51403, 51403, 759, 291, 645, 281, 14722, 508, 3847, 11, 577, 731, 775, 264, 9284, 360, 322, 264, 3097, 992, 30, 51732, 51732, 1726, 300, 731, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.08540758756127688, "compression_ratio": 1.7763157894736843, "no_speech_prob": 2.3687441625952488e-06}, {"id": 24, "seek": 8480, "start": 112.16, "end": 113.16, "text": " Not that well.", "tokens": [50364, 407, 2602, 295, 1382, 281, 574, 412, 28609, 411, 341, 11, 257, 544, 27249, 636, 281, 36238, 420, 50698, 50698, 281, 915, 484, 498, 428, 9284, 575, 1090, 12577, 420, 1090, 21977, 486, 312, 281, 574, 412, 264, 3389, 51000, 51000, 295, 428, 9284, 322, 264, 3097, 992, 293, 322, 264, 3278, 24071, 992, 13, 51262, 51262, 682, 1729, 11, 718, 311, 574, 412, 264, 1365, 322, 264, 1411, 13, 51403, 51403, 759, 291, 645, 281, 14722, 508, 3847, 11, 577, 731, 775, 264, 9284, 360, 322, 264, 3097, 992, 30, 51732, 51732, 1726, 300, 731, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.08540758756127688, "compression_ratio": 1.7763157894736843, "no_speech_prob": 2.3687441625952488e-06}, {"id": 25, "seek": 11316, "start": 113.16, "end": 118.64, "text": " So let's say J train here would be high because there are actually pretty large errors between", "tokens": [50364, 407, 718, 311, 584, 508, 3847, 510, 576, 312, 1090, 570, 456, 366, 767, 1238, 2416, 13603, 1296, 50638, 50638, 264, 5110, 293, 264, 3539, 21264, 295, 264, 2316, 13, 50866, 50866, 400, 577, 466, 508, 66, 85, 30, 50952, 50952, 407, 508, 66, 85, 576, 312, 498, 291, 632, 257, 1326, 777, 5110, 11, 1310, 5110, 411, 300, 11, 300, 264, 9284, 51376, 51376, 632, 406, 8046, 1612, 13, 51514, 51514, 400, 510, 11, 264, 9284, 611, 1177, 380, 360, 300, 731, 322, 5110, 300, 309, 632, 406, 8046, 51778, 51778, 1612, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.17581521248330875, "compression_ratio": 1.7877358490566038, "no_speech_prob": 1.963790964509826e-06}, {"id": 26, "seek": 11316, "start": 118.64, "end": 123.2, "text": " the examples and the actual predictions of the model.", "tokens": [50364, 407, 718, 311, 584, 508, 3847, 510, 576, 312, 1090, 570, 456, 366, 767, 1238, 2416, 13603, 1296, 50638, 50638, 264, 5110, 293, 264, 3539, 21264, 295, 264, 2316, 13, 50866, 50866, 400, 577, 466, 508, 66, 85, 30, 50952, 50952, 407, 508, 66, 85, 576, 312, 498, 291, 632, 257, 1326, 777, 5110, 11, 1310, 5110, 411, 300, 11, 300, 264, 9284, 51376, 51376, 632, 406, 8046, 1612, 13, 51514, 51514, 400, 510, 11, 264, 9284, 611, 1177, 380, 360, 300, 731, 322, 5110, 300, 309, 632, 406, 8046, 51778, 51778, 1612, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.17581521248330875, "compression_ratio": 1.7877358490566038, "no_speech_prob": 1.963790964509826e-06}, {"id": 27, "seek": 11316, "start": 123.2, "end": 124.92, "text": " And how about Jcv?", "tokens": [50364, 407, 718, 311, 584, 508, 3847, 510, 576, 312, 1090, 570, 456, 366, 767, 1238, 2416, 13603, 1296, 50638, 50638, 264, 5110, 293, 264, 3539, 21264, 295, 264, 2316, 13, 50866, 50866, 400, 577, 466, 508, 66, 85, 30, 50952, 50952, 407, 508, 66, 85, 576, 312, 498, 291, 632, 257, 1326, 777, 5110, 11, 1310, 5110, 411, 300, 11, 300, 264, 9284, 51376, 51376, 632, 406, 8046, 1612, 13, 51514, 51514, 400, 510, 11, 264, 9284, 611, 1177, 380, 360, 300, 731, 322, 5110, 300, 309, 632, 406, 8046, 51778, 51778, 1612, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.17581521248330875, "compression_ratio": 1.7877358490566038, "no_speech_prob": 1.963790964509826e-06}, {"id": 28, "seek": 11316, "start": 124.92, "end": 133.4, "text": " So Jcv would be if you had a few new examples, maybe examples like that, that the algorithm", "tokens": [50364, 407, 718, 311, 584, 508, 3847, 510, 576, 312, 1090, 570, 456, 366, 767, 1238, 2416, 13603, 1296, 50638, 50638, 264, 5110, 293, 264, 3539, 21264, 295, 264, 2316, 13, 50866, 50866, 400, 577, 466, 508, 66, 85, 30, 50952, 50952, 407, 508, 66, 85, 576, 312, 498, 291, 632, 257, 1326, 777, 5110, 11, 1310, 5110, 411, 300, 11, 300, 264, 9284, 51376, 51376, 632, 406, 8046, 1612, 13, 51514, 51514, 400, 510, 11, 264, 9284, 611, 1177, 380, 360, 300, 731, 322, 5110, 300, 309, 632, 406, 8046, 51778, 51778, 1612, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.17581521248330875, "compression_ratio": 1.7877358490566038, "no_speech_prob": 1.963790964509826e-06}, {"id": 29, "seek": 11316, "start": 133.4, "end": 136.16, "text": " had not previously seen.", "tokens": [50364, 407, 718, 311, 584, 508, 3847, 510, 576, 312, 1090, 570, 456, 366, 767, 1238, 2416, 13603, 1296, 50638, 50638, 264, 5110, 293, 264, 3539, 21264, 295, 264, 2316, 13, 50866, 50866, 400, 577, 466, 508, 66, 85, 30, 50952, 50952, 407, 508, 66, 85, 576, 312, 498, 291, 632, 257, 1326, 777, 5110, 11, 1310, 5110, 411, 300, 11, 300, 264, 9284, 51376, 51376, 632, 406, 8046, 1612, 13, 51514, 51514, 400, 510, 11, 264, 9284, 611, 1177, 380, 360, 300, 731, 322, 5110, 300, 309, 632, 406, 8046, 51778, 51778, 1612, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.17581521248330875, "compression_ratio": 1.7877358490566038, "no_speech_prob": 1.963790964509826e-06}, {"id": 30, "seek": 11316, "start": 136.16, "end": 141.44, "text": " And here, the algorithm also doesn't do that well on examples that it had not previously", "tokens": [50364, 407, 718, 311, 584, 508, 3847, 510, 576, 312, 1090, 570, 456, 366, 767, 1238, 2416, 13603, 1296, 50638, 50638, 264, 5110, 293, 264, 3539, 21264, 295, 264, 2316, 13, 50866, 50866, 400, 577, 466, 508, 66, 85, 30, 50952, 50952, 407, 508, 66, 85, 576, 312, 498, 291, 632, 257, 1326, 777, 5110, 11, 1310, 5110, 411, 300, 11, 300, 264, 9284, 51376, 51376, 632, 406, 8046, 1612, 13, 51514, 51514, 400, 510, 11, 264, 9284, 611, 1177, 380, 360, 300, 731, 322, 5110, 300, 309, 632, 406, 8046, 51778, 51778, 1612, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.17581521248330875, "compression_ratio": 1.7877358490566038, "no_speech_prob": 1.963790964509826e-06}, {"id": 31, "seek": 11316, "start": 141.44, "end": 142.44, "text": " seen.", "tokens": [50364, 407, 718, 311, 584, 508, 3847, 510, 576, 312, 1090, 570, 456, 366, 767, 1238, 2416, 13603, 1296, 50638, 50638, 264, 5110, 293, 264, 3539, 21264, 295, 264, 2316, 13, 50866, 50866, 400, 577, 466, 508, 66, 85, 30, 50952, 50952, 407, 508, 66, 85, 576, 312, 498, 291, 632, 257, 1326, 777, 5110, 11, 1310, 5110, 411, 300, 11, 300, 264, 9284, 51376, 51376, 632, 406, 8046, 1612, 13, 51514, 51514, 400, 510, 11, 264, 9284, 611, 1177, 380, 360, 300, 731, 322, 5110, 300, 309, 632, 406, 8046, 51778, 51778, 1612, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.17581521248330875, "compression_ratio": 1.7877358490566038, "no_speech_prob": 1.963790964509826e-06}, {"id": 32, "seek": 14244, "start": 142.44, "end": 145.88, "text": " So Jcv would also be high.", "tokens": [50364, 407, 508, 66, 85, 576, 611, 312, 1090, 13, 50536, 50536, 400, 472, 16282, 295, 364, 9284, 365, 1090, 12577, 11, 746, 300, 307, 833, 69, 2414, 50762, 50762, 307, 300, 309, 311, 406, 754, 884, 300, 731, 322, 264, 3097, 992, 13, 50950, 50950, 400, 370, 562, 508, 3847, 307, 1090, 11, 300, 2709, 291, 257, 2068, 16961, 300, 341, 9284, 575, 51192, 51192, 1090, 12577, 13, 51268, 51268, 961, 311, 586, 574, 412, 264, 1365, 322, 264, 558, 13, 51402, 51402, 759, 291, 645, 281, 14722, 508, 3847, 11, 577, 731, 307, 341, 884, 322, 264, 3097, 992, 30, 51614, 51614, 1042, 11, 309, 311, 767, 884, 869, 322, 264, 3097, 992, 11, 9001, 264, 3097, 1412, 534, 731, 13, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10308125662425208, "compression_ratio": 1.8527131782945736, "no_speech_prob": 7.071670552249998e-06}, {"id": 33, "seek": 14244, "start": 145.88, "end": 150.4, "text": " And one characteristic of an algorithm with high bias, something that is underfitting", "tokens": [50364, 407, 508, 66, 85, 576, 611, 312, 1090, 13, 50536, 50536, 400, 472, 16282, 295, 364, 9284, 365, 1090, 12577, 11, 746, 300, 307, 833, 69, 2414, 50762, 50762, 307, 300, 309, 311, 406, 754, 884, 300, 731, 322, 264, 3097, 992, 13, 50950, 50950, 400, 370, 562, 508, 3847, 307, 1090, 11, 300, 2709, 291, 257, 2068, 16961, 300, 341, 9284, 575, 51192, 51192, 1090, 12577, 13, 51268, 51268, 961, 311, 586, 574, 412, 264, 1365, 322, 264, 558, 13, 51402, 51402, 759, 291, 645, 281, 14722, 508, 3847, 11, 577, 731, 307, 341, 884, 322, 264, 3097, 992, 30, 51614, 51614, 1042, 11, 309, 311, 767, 884, 869, 322, 264, 3097, 992, 11, 9001, 264, 3097, 1412, 534, 731, 13, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10308125662425208, "compression_ratio": 1.8527131782945736, "no_speech_prob": 7.071670552249998e-06}, {"id": 34, "seek": 14244, "start": 150.4, "end": 154.16, "text": " is that it's not even doing that well on the training set.", "tokens": [50364, 407, 508, 66, 85, 576, 611, 312, 1090, 13, 50536, 50536, 400, 472, 16282, 295, 364, 9284, 365, 1090, 12577, 11, 746, 300, 307, 833, 69, 2414, 50762, 50762, 307, 300, 309, 311, 406, 754, 884, 300, 731, 322, 264, 3097, 992, 13, 50950, 50950, 400, 370, 562, 508, 3847, 307, 1090, 11, 300, 2709, 291, 257, 2068, 16961, 300, 341, 9284, 575, 51192, 51192, 1090, 12577, 13, 51268, 51268, 961, 311, 586, 574, 412, 264, 1365, 322, 264, 558, 13, 51402, 51402, 759, 291, 645, 281, 14722, 508, 3847, 11, 577, 731, 307, 341, 884, 322, 264, 3097, 992, 30, 51614, 51614, 1042, 11, 309, 311, 767, 884, 869, 322, 264, 3097, 992, 11, 9001, 264, 3097, 1412, 534, 731, 13, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10308125662425208, "compression_ratio": 1.8527131782945736, "no_speech_prob": 7.071670552249998e-06}, {"id": 35, "seek": 14244, "start": 154.16, "end": 159.0, "text": " And so when J train is high, that gives you a strong indicator that this algorithm has", "tokens": [50364, 407, 508, 66, 85, 576, 611, 312, 1090, 13, 50536, 50536, 400, 472, 16282, 295, 364, 9284, 365, 1090, 12577, 11, 746, 300, 307, 833, 69, 2414, 50762, 50762, 307, 300, 309, 311, 406, 754, 884, 300, 731, 322, 264, 3097, 992, 13, 50950, 50950, 400, 370, 562, 508, 3847, 307, 1090, 11, 300, 2709, 291, 257, 2068, 16961, 300, 341, 9284, 575, 51192, 51192, 1090, 12577, 13, 51268, 51268, 961, 311, 586, 574, 412, 264, 1365, 322, 264, 558, 13, 51402, 51402, 759, 291, 645, 281, 14722, 508, 3847, 11, 577, 731, 307, 341, 884, 322, 264, 3097, 992, 30, 51614, 51614, 1042, 11, 309, 311, 767, 884, 869, 322, 264, 3097, 992, 11, 9001, 264, 3097, 1412, 534, 731, 13, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10308125662425208, "compression_ratio": 1.8527131782945736, "no_speech_prob": 7.071670552249998e-06}, {"id": 36, "seek": 14244, "start": 159.0, "end": 160.52, "text": " high bias.", "tokens": [50364, 407, 508, 66, 85, 576, 611, 312, 1090, 13, 50536, 50536, 400, 472, 16282, 295, 364, 9284, 365, 1090, 12577, 11, 746, 300, 307, 833, 69, 2414, 50762, 50762, 307, 300, 309, 311, 406, 754, 884, 300, 731, 322, 264, 3097, 992, 13, 50950, 50950, 400, 370, 562, 508, 3847, 307, 1090, 11, 300, 2709, 291, 257, 2068, 16961, 300, 341, 9284, 575, 51192, 51192, 1090, 12577, 13, 51268, 51268, 961, 311, 586, 574, 412, 264, 1365, 322, 264, 558, 13, 51402, 51402, 759, 291, 645, 281, 14722, 508, 3847, 11, 577, 731, 307, 341, 884, 322, 264, 3097, 992, 30, 51614, 51614, 1042, 11, 309, 311, 767, 884, 869, 322, 264, 3097, 992, 11, 9001, 264, 3097, 1412, 534, 731, 13, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10308125662425208, "compression_ratio": 1.8527131782945736, "no_speech_prob": 7.071670552249998e-06}, {"id": 37, "seek": 14244, "start": 160.52, "end": 163.2, "text": " Let's now look at the example on the right.", "tokens": [50364, 407, 508, 66, 85, 576, 611, 312, 1090, 13, 50536, 50536, 400, 472, 16282, 295, 364, 9284, 365, 1090, 12577, 11, 746, 300, 307, 833, 69, 2414, 50762, 50762, 307, 300, 309, 311, 406, 754, 884, 300, 731, 322, 264, 3097, 992, 13, 50950, 50950, 400, 370, 562, 508, 3847, 307, 1090, 11, 300, 2709, 291, 257, 2068, 16961, 300, 341, 9284, 575, 51192, 51192, 1090, 12577, 13, 51268, 51268, 961, 311, 586, 574, 412, 264, 1365, 322, 264, 558, 13, 51402, 51402, 759, 291, 645, 281, 14722, 508, 3847, 11, 577, 731, 307, 341, 884, 322, 264, 3097, 992, 30, 51614, 51614, 1042, 11, 309, 311, 767, 884, 869, 322, 264, 3097, 992, 11, 9001, 264, 3097, 1412, 534, 731, 13, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10308125662425208, "compression_ratio": 1.8527131782945736, "no_speech_prob": 7.071670552249998e-06}, {"id": 38, "seek": 14244, "start": 163.2, "end": 167.44, "text": " If you were to compute J train, how well is this doing on the training set?", "tokens": [50364, 407, 508, 66, 85, 576, 611, 312, 1090, 13, 50536, 50536, 400, 472, 16282, 295, 364, 9284, 365, 1090, 12577, 11, 746, 300, 307, 833, 69, 2414, 50762, 50762, 307, 300, 309, 311, 406, 754, 884, 300, 731, 322, 264, 3097, 992, 13, 50950, 50950, 400, 370, 562, 508, 3847, 307, 1090, 11, 300, 2709, 291, 257, 2068, 16961, 300, 341, 9284, 575, 51192, 51192, 1090, 12577, 13, 51268, 51268, 961, 311, 586, 574, 412, 264, 1365, 322, 264, 558, 13, 51402, 51402, 759, 291, 645, 281, 14722, 508, 3847, 11, 577, 731, 307, 341, 884, 322, 264, 3097, 992, 30, 51614, 51614, 1042, 11, 309, 311, 767, 884, 869, 322, 264, 3097, 992, 11, 9001, 264, 3097, 1412, 534, 731, 13, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10308125662425208, "compression_ratio": 1.8527131782945736, "no_speech_prob": 7.071670552249998e-06}, {"id": 39, "seek": 14244, "start": 167.44, "end": 172.2, "text": " Well, it's actually doing great on the training set, fits the training data really well.", "tokens": [50364, 407, 508, 66, 85, 576, 611, 312, 1090, 13, 50536, 50536, 400, 472, 16282, 295, 364, 9284, 365, 1090, 12577, 11, 746, 300, 307, 833, 69, 2414, 50762, 50762, 307, 300, 309, 311, 406, 754, 884, 300, 731, 322, 264, 3097, 992, 13, 50950, 50950, 400, 370, 562, 508, 3847, 307, 1090, 11, 300, 2709, 291, 257, 2068, 16961, 300, 341, 9284, 575, 51192, 51192, 1090, 12577, 13, 51268, 51268, 961, 311, 586, 574, 412, 264, 1365, 322, 264, 558, 13, 51402, 51402, 759, 291, 645, 281, 14722, 508, 3847, 11, 577, 731, 307, 341, 884, 322, 264, 3097, 992, 30, 51614, 51614, 1042, 11, 309, 311, 767, 884, 869, 322, 264, 3097, 992, 11, 9001, 264, 3097, 1412, 534, 731, 13, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10308125662425208, "compression_ratio": 1.8527131782945736, "no_speech_prob": 7.071670552249998e-06}, {"id": 40, "seek": 17220, "start": 172.2, "end": 174.64, "text": " So J train here will be low.", "tokens": [50364, 407, 508, 3847, 510, 486, 312, 2295, 13, 50486, 50486, 583, 498, 291, 645, 281, 13059, 341, 2316, 322, 661, 8078, 406, 294, 264, 3097, 992, 11, 550, 50828, 50828, 291, 915, 300, 508, 66, 85, 11, 264, 3278, 24071, 6713, 486, 312, 1596, 1090, 13, 51151, 51151, 400, 370, 257, 16282, 13397, 420, 16282, 22656, 300, 428, 9284, 575, 1090, 21977, 51442, 51442, 486, 312, 295, 508, 66, 85, 307, 709, 2946, 813, 508, 3847, 13, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.1194054874373071, "compression_ratio": 1.6256410256410256, "no_speech_prob": 2.9944073958176887e-06}, {"id": 41, "seek": 17220, "start": 174.64, "end": 181.48, "text": " But if you were to evaluate this model on other houses not in the training set, then", "tokens": [50364, 407, 508, 3847, 510, 486, 312, 2295, 13, 50486, 50486, 583, 498, 291, 645, 281, 13059, 341, 2316, 322, 661, 8078, 406, 294, 264, 3097, 992, 11, 550, 50828, 50828, 291, 915, 300, 508, 66, 85, 11, 264, 3278, 24071, 6713, 486, 312, 1596, 1090, 13, 51151, 51151, 400, 370, 257, 16282, 13397, 420, 16282, 22656, 300, 428, 9284, 575, 1090, 21977, 51442, 51442, 486, 312, 295, 508, 66, 85, 307, 709, 2946, 813, 508, 3847, 13, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.1194054874373071, "compression_ratio": 1.6256410256410256, "no_speech_prob": 2.9944073958176887e-06}, {"id": 42, "seek": 17220, "start": 181.48, "end": 187.94, "text": " you find that Jcv, the cross validation error will be quite high.", "tokens": [50364, 407, 508, 3847, 510, 486, 312, 2295, 13, 50486, 50486, 583, 498, 291, 645, 281, 13059, 341, 2316, 322, 661, 8078, 406, 294, 264, 3097, 992, 11, 550, 50828, 50828, 291, 915, 300, 508, 66, 85, 11, 264, 3278, 24071, 6713, 486, 312, 1596, 1090, 13, 51151, 51151, 400, 370, 257, 16282, 13397, 420, 16282, 22656, 300, 428, 9284, 575, 1090, 21977, 51442, 51442, 486, 312, 295, 508, 66, 85, 307, 709, 2946, 813, 508, 3847, 13, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.1194054874373071, "compression_ratio": 1.6256410256410256, "no_speech_prob": 2.9944073958176887e-06}, {"id": 43, "seek": 17220, "start": 187.94, "end": 193.76, "text": " And so a characteristic signature or characteristic cue that your algorithm has high variance", "tokens": [50364, 407, 508, 3847, 510, 486, 312, 2295, 13, 50486, 50486, 583, 498, 291, 645, 281, 13059, 341, 2316, 322, 661, 8078, 406, 294, 264, 3097, 992, 11, 550, 50828, 50828, 291, 915, 300, 508, 66, 85, 11, 264, 3278, 24071, 6713, 486, 312, 1596, 1090, 13, 51151, 51151, 400, 370, 257, 16282, 13397, 420, 16282, 22656, 300, 428, 9284, 575, 1090, 21977, 51442, 51442, 486, 312, 295, 508, 66, 85, 307, 709, 2946, 813, 508, 3847, 13, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.1194054874373071, "compression_ratio": 1.6256410256410256, "no_speech_prob": 2.9944073958176887e-06}, {"id": 44, "seek": 17220, "start": 193.76, "end": 198.67999999999998, "text": " will be of Jcv is much higher than J train.", "tokens": [50364, 407, 508, 3847, 510, 486, 312, 2295, 13, 50486, 50486, 583, 498, 291, 645, 281, 13059, 341, 2316, 322, 661, 8078, 406, 294, 264, 3097, 992, 11, 550, 50828, 50828, 291, 915, 300, 508, 66, 85, 11, 264, 3278, 24071, 6713, 486, 312, 1596, 1090, 13, 51151, 51151, 400, 370, 257, 16282, 13397, 420, 16282, 22656, 300, 428, 9284, 575, 1090, 21977, 51442, 51442, 486, 312, 295, 508, 66, 85, 307, 709, 2946, 813, 508, 3847, 13, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.1194054874373071, "compression_ratio": 1.6256410256410256, "no_speech_prob": 2.9944073958176887e-06}, {"id": 45, "seek": 19868, "start": 198.68, "end": 203.28, "text": " In other words, it does much better on data it has seen than on data it has not seen.", "tokens": [50364, 682, 661, 2283, 11, 309, 775, 709, 1101, 322, 1412, 309, 575, 1612, 813, 322, 1412, 309, 575, 406, 1612, 13, 50594, 50594, 400, 341, 4523, 484, 281, 312, 257, 2068, 16961, 300, 428, 9284, 575, 1090, 21977, 13, 50858, 50858, 400, 797, 11, 264, 935, 295, 437, 321, 434, 884, 307, 300, 538, 15866, 508, 3847, 293, 508, 66, 85, 293, 2577, 51196, 51196, 498, 508, 3847, 307, 1090, 11, 420, 498, 508, 66, 85, 307, 709, 2946, 813, 508, 3847, 11, 341, 2709, 291, 257, 2020, 754, 51518, 51518], "temperature": 0.0, "avg_logprob": -0.08885016492618028, "compression_ratio": 1.6780487804878048, "no_speech_prob": 2.3687357497692574e-06}, {"id": 46, "seek": 19868, "start": 203.28, "end": 208.56, "text": " And this turns out to be a strong indicator that your algorithm has high variance.", "tokens": [50364, 682, 661, 2283, 11, 309, 775, 709, 1101, 322, 1412, 309, 575, 1612, 813, 322, 1412, 309, 575, 406, 1612, 13, 50594, 50594, 400, 341, 4523, 484, 281, 312, 257, 2068, 16961, 300, 428, 9284, 575, 1090, 21977, 13, 50858, 50858, 400, 797, 11, 264, 935, 295, 437, 321, 434, 884, 307, 300, 538, 15866, 508, 3847, 293, 508, 66, 85, 293, 2577, 51196, 51196, 498, 508, 3847, 307, 1090, 11, 420, 498, 508, 66, 85, 307, 709, 2946, 813, 508, 3847, 11, 341, 2709, 291, 257, 2020, 754, 51518, 51518], "temperature": 0.0, "avg_logprob": -0.08885016492618028, "compression_ratio": 1.6780487804878048, "no_speech_prob": 2.3687357497692574e-06}, {"id": 47, "seek": 19868, "start": 208.56, "end": 215.32, "text": " And again, the point of what we're doing is that by computing J train and Jcv and seeing", "tokens": [50364, 682, 661, 2283, 11, 309, 775, 709, 1101, 322, 1412, 309, 575, 1612, 813, 322, 1412, 309, 575, 406, 1612, 13, 50594, 50594, 400, 341, 4523, 484, 281, 312, 257, 2068, 16961, 300, 428, 9284, 575, 1090, 21977, 13, 50858, 50858, 400, 797, 11, 264, 935, 295, 437, 321, 434, 884, 307, 300, 538, 15866, 508, 3847, 293, 508, 66, 85, 293, 2577, 51196, 51196, 498, 508, 3847, 307, 1090, 11, 420, 498, 508, 66, 85, 307, 709, 2946, 813, 508, 3847, 11, 341, 2709, 291, 257, 2020, 754, 51518, 51518], "temperature": 0.0, "avg_logprob": -0.08885016492618028, "compression_ratio": 1.6780487804878048, "no_speech_prob": 2.3687357497692574e-06}, {"id": 48, "seek": 19868, "start": 215.32, "end": 221.76000000000002, "text": " if J train is high, or if Jcv is much higher than J train, this gives you a sense even", "tokens": [50364, 682, 661, 2283, 11, 309, 775, 709, 1101, 322, 1412, 309, 575, 1612, 813, 322, 1412, 309, 575, 406, 1612, 13, 50594, 50594, 400, 341, 4523, 484, 281, 312, 257, 2068, 16961, 300, 428, 9284, 575, 1090, 21977, 13, 50858, 50858, 400, 797, 11, 264, 935, 295, 437, 321, 434, 884, 307, 300, 538, 15866, 508, 3847, 293, 508, 66, 85, 293, 2577, 51196, 51196, 498, 508, 3847, 307, 1090, 11, 420, 498, 508, 66, 85, 307, 709, 2946, 813, 508, 3847, 11, 341, 2709, 291, 257, 2020, 754, 51518, 51518], "temperature": 0.0, "avg_logprob": -0.08885016492618028, "compression_ratio": 1.6780487804878048, "no_speech_prob": 2.3687357497692574e-06}, {"id": 49, "seek": 22176, "start": 221.76, "end": 229.32, "text": " if you can't plot to function f of whether your algorithm has high bias or high variance.", "tokens": [50364, 498, 291, 393, 380, 7542, 281, 2445, 283, 295, 1968, 428, 9284, 575, 1090, 12577, 420, 1090, 21977, 13, 50742, 50742, 400, 2721, 11, 264, 1389, 294, 264, 2808, 11, 498, 291, 574, 412, 508, 3847, 307, 1238, 2295, 1670, 309, 311, 884, 51110, 51110, 1596, 731, 322, 264, 3097, 992, 13, 51240, 51240, 400, 498, 291, 645, 281, 574, 412, 257, 1326, 777, 5110, 411, 729, 490, 584, 428, 3278, 24071, 51548, 51548, 992, 11, 291, 915, 300, 508, 66, 85, 307, 611, 1238, 2295, 13, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.110934812944014, "compression_ratio": 1.5953488372093023, "no_speech_prob": 6.475825102825183e-07}, {"id": 50, "seek": 22176, "start": 229.32, "end": 236.67999999999998, "text": " And finally, the case in the middle, if you look at J train is pretty low since it's doing", "tokens": [50364, 498, 291, 393, 380, 7542, 281, 2445, 283, 295, 1968, 428, 9284, 575, 1090, 12577, 420, 1090, 21977, 13, 50742, 50742, 400, 2721, 11, 264, 1389, 294, 264, 2808, 11, 498, 291, 574, 412, 508, 3847, 307, 1238, 2295, 1670, 309, 311, 884, 51110, 51110, 1596, 731, 322, 264, 3097, 992, 13, 51240, 51240, 400, 498, 291, 645, 281, 574, 412, 257, 1326, 777, 5110, 411, 729, 490, 584, 428, 3278, 24071, 51548, 51548, 992, 11, 291, 915, 300, 508, 66, 85, 307, 611, 1238, 2295, 13, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.110934812944014, "compression_ratio": 1.5953488372093023, "no_speech_prob": 6.475825102825183e-07}, {"id": 51, "seek": 22176, "start": 236.67999999999998, "end": 239.28, "text": " quite well on the training set.", "tokens": [50364, 498, 291, 393, 380, 7542, 281, 2445, 283, 295, 1968, 428, 9284, 575, 1090, 12577, 420, 1090, 21977, 13, 50742, 50742, 400, 2721, 11, 264, 1389, 294, 264, 2808, 11, 498, 291, 574, 412, 508, 3847, 307, 1238, 2295, 1670, 309, 311, 884, 51110, 51110, 1596, 731, 322, 264, 3097, 992, 13, 51240, 51240, 400, 498, 291, 645, 281, 574, 412, 257, 1326, 777, 5110, 411, 729, 490, 584, 428, 3278, 24071, 51548, 51548, 992, 11, 291, 915, 300, 508, 66, 85, 307, 611, 1238, 2295, 13, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.110934812944014, "compression_ratio": 1.5953488372093023, "no_speech_prob": 6.475825102825183e-07}, {"id": 52, "seek": 22176, "start": 239.28, "end": 245.44, "text": " And if you were to look at a few new examples like those from say your cross validation", "tokens": [50364, 498, 291, 393, 380, 7542, 281, 2445, 283, 295, 1968, 428, 9284, 575, 1090, 12577, 420, 1090, 21977, 13, 50742, 50742, 400, 2721, 11, 264, 1389, 294, 264, 2808, 11, 498, 291, 574, 412, 508, 3847, 307, 1238, 2295, 1670, 309, 311, 884, 51110, 51110, 1596, 731, 322, 264, 3097, 992, 13, 51240, 51240, 400, 498, 291, 645, 281, 574, 412, 257, 1326, 777, 5110, 411, 729, 490, 584, 428, 3278, 24071, 51548, 51548, 992, 11, 291, 915, 300, 508, 66, 85, 307, 611, 1238, 2295, 13, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.110934812944014, "compression_ratio": 1.5953488372093023, "no_speech_prob": 6.475825102825183e-07}, {"id": 53, "seek": 22176, "start": 245.44, "end": 249.6, "text": " set, you find that Jcv is also pretty low.", "tokens": [50364, 498, 291, 393, 380, 7542, 281, 2445, 283, 295, 1968, 428, 9284, 575, 1090, 12577, 420, 1090, 21977, 13, 50742, 50742, 400, 2721, 11, 264, 1389, 294, 264, 2808, 11, 498, 291, 574, 412, 508, 3847, 307, 1238, 2295, 1670, 309, 311, 884, 51110, 51110, 1596, 731, 322, 264, 3097, 992, 13, 51240, 51240, 400, 498, 291, 645, 281, 574, 412, 257, 1326, 777, 5110, 411, 729, 490, 584, 428, 3278, 24071, 51548, 51548, 992, 11, 291, 915, 300, 508, 66, 85, 307, 611, 1238, 2295, 13, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.110934812944014, "compression_ratio": 1.5953488372093023, "no_speech_prob": 6.475825102825183e-07}, {"id": 54, "seek": 24960, "start": 249.6, "end": 257.64, "text": " And so J train not being too high, indicates this doesn't have a high bias problem.", "tokens": [50364, 400, 370, 508, 3847, 406, 885, 886, 1090, 11, 16203, 341, 1177, 380, 362, 257, 1090, 12577, 1154, 13, 50766, 50766, 400, 508, 66, 85, 406, 885, 709, 5324, 813, 508, 3847, 11, 341, 16203, 300, 309, 1177, 380, 362, 257, 1090, 51022, 51022, 21977, 1154, 2139, 11, 597, 307, 983, 341, 2316, 11, 264, 37262, 2316, 2544, 281, 312, 257, 1238, 51332, 51332, 665, 472, 337, 341, 3861, 13, 51466, 51466, 961, 385, 2073, 365, 291, 1071, 1910, 295, 12577, 293, 21977, 13, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.11561078375036066, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.42540704170824e-06}, {"id": 55, "seek": 24960, "start": 257.64, "end": 262.76, "text": " And Jcv not being much worse than J train, this indicates that it doesn't have a high", "tokens": [50364, 400, 370, 508, 3847, 406, 885, 886, 1090, 11, 16203, 341, 1177, 380, 362, 257, 1090, 12577, 1154, 13, 50766, 50766, 400, 508, 66, 85, 406, 885, 709, 5324, 813, 508, 3847, 11, 341, 16203, 300, 309, 1177, 380, 362, 257, 1090, 51022, 51022, 21977, 1154, 2139, 11, 597, 307, 983, 341, 2316, 11, 264, 37262, 2316, 2544, 281, 312, 257, 1238, 51332, 51332, 665, 472, 337, 341, 3861, 13, 51466, 51466, 961, 385, 2073, 365, 291, 1071, 1910, 295, 12577, 293, 21977, 13, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.11561078375036066, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.42540704170824e-06}, {"id": 56, "seek": 24960, "start": 262.76, "end": 268.96, "text": " variance problem either, which is why this model, the quadratic model seems to be a pretty", "tokens": [50364, 400, 370, 508, 3847, 406, 885, 886, 1090, 11, 16203, 341, 1177, 380, 362, 257, 1090, 12577, 1154, 13, 50766, 50766, 400, 508, 66, 85, 406, 885, 709, 5324, 813, 508, 3847, 11, 341, 16203, 300, 309, 1177, 380, 362, 257, 1090, 51022, 51022, 21977, 1154, 2139, 11, 597, 307, 983, 341, 2316, 11, 264, 37262, 2316, 2544, 281, 312, 257, 1238, 51332, 51332, 665, 472, 337, 341, 3861, 13, 51466, 51466, 961, 385, 2073, 365, 291, 1071, 1910, 295, 12577, 293, 21977, 13, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.11561078375036066, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.42540704170824e-06}, {"id": 57, "seek": 24960, "start": 268.96, "end": 271.64, "text": " good one for this application.", "tokens": [50364, 400, 370, 508, 3847, 406, 885, 886, 1090, 11, 16203, 341, 1177, 380, 362, 257, 1090, 12577, 1154, 13, 50766, 50766, 400, 508, 66, 85, 406, 885, 709, 5324, 813, 508, 3847, 11, 341, 16203, 300, 309, 1177, 380, 362, 257, 1090, 51022, 51022, 21977, 1154, 2139, 11, 597, 307, 983, 341, 2316, 11, 264, 37262, 2316, 2544, 281, 312, 257, 1238, 51332, 51332, 665, 472, 337, 341, 3861, 13, 51466, 51466, 961, 385, 2073, 365, 291, 1071, 1910, 295, 12577, 293, 21977, 13, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.11561078375036066, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.42540704170824e-06}, {"id": 58, "seek": 24960, "start": 271.64, "end": 276.36, "text": " Let me share with you another view of bias and variance.", "tokens": [50364, 400, 370, 508, 3847, 406, 885, 886, 1090, 11, 16203, 341, 1177, 380, 362, 257, 1090, 12577, 1154, 13, 50766, 50766, 400, 508, 66, 85, 406, 885, 709, 5324, 813, 508, 3847, 11, 341, 16203, 300, 309, 1177, 380, 362, 257, 1090, 51022, 51022, 21977, 1154, 2139, 11, 597, 307, 983, 341, 2316, 11, 264, 37262, 2316, 2544, 281, 312, 257, 1238, 51332, 51332, 665, 472, 337, 341, 3861, 13, 51466, 51466, 961, 385, 2073, 365, 291, 1071, 1910, 295, 12577, 293, 21977, 13, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.11561078375036066, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.42540704170824e-06}, {"id": 59, "seek": 27636, "start": 276.36, "end": 284.66, "text": " So to summarize when d equals one for a linear polynomial, J train was high and Jcv was high.", "tokens": [50364, 407, 281, 20858, 562, 274, 6915, 472, 337, 257, 8213, 26110, 11, 508, 3847, 390, 1090, 293, 508, 66, 85, 390, 1090, 13, 50779, 50779, 1133, 274, 6915, 1451, 11, 508, 3847, 390, 2295, 11, 457, 508, 66, 85, 307, 1090, 13, 51029, 51029, 400, 562, 274, 6915, 732, 11, 1293, 645, 1238, 2295, 13, 51214, 51214, 961, 311, 586, 747, 257, 819, 1910, 322, 12577, 293, 21977, 13, 51356, 51356, 400, 294, 1729, 11, 322, 264, 958, 4137, 11, 286, 1116, 411, 281, 855, 291, 577, 508, 3847, 293, 508, 66, 85, 10559, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.13697183375455896, "compression_ratio": 1.625615763546798, "no_speech_prob": 5.7718179959920235e-06}, {"id": 60, "seek": 27636, "start": 284.66, "end": 289.66, "text": " When d equals four, J train was low, but Jcv is high.", "tokens": [50364, 407, 281, 20858, 562, 274, 6915, 472, 337, 257, 8213, 26110, 11, 508, 3847, 390, 1090, 293, 508, 66, 85, 390, 1090, 13, 50779, 50779, 1133, 274, 6915, 1451, 11, 508, 3847, 390, 2295, 11, 457, 508, 66, 85, 307, 1090, 13, 51029, 51029, 400, 562, 274, 6915, 732, 11, 1293, 645, 1238, 2295, 13, 51214, 51214, 961, 311, 586, 747, 257, 819, 1910, 322, 12577, 293, 21977, 13, 51356, 51356, 400, 294, 1729, 11, 322, 264, 958, 4137, 11, 286, 1116, 411, 281, 855, 291, 577, 508, 3847, 293, 508, 66, 85, 10559, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.13697183375455896, "compression_ratio": 1.625615763546798, "no_speech_prob": 5.7718179959920235e-06}, {"id": 61, "seek": 27636, "start": 289.66, "end": 293.36, "text": " And when d equals two, both were pretty low.", "tokens": [50364, 407, 281, 20858, 562, 274, 6915, 472, 337, 257, 8213, 26110, 11, 508, 3847, 390, 1090, 293, 508, 66, 85, 390, 1090, 13, 50779, 50779, 1133, 274, 6915, 1451, 11, 508, 3847, 390, 2295, 11, 457, 508, 66, 85, 307, 1090, 13, 51029, 51029, 400, 562, 274, 6915, 732, 11, 1293, 645, 1238, 2295, 13, 51214, 51214, 961, 311, 586, 747, 257, 819, 1910, 322, 12577, 293, 21977, 13, 51356, 51356, 400, 294, 1729, 11, 322, 264, 958, 4137, 11, 286, 1116, 411, 281, 855, 291, 577, 508, 3847, 293, 508, 66, 85, 10559, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.13697183375455896, "compression_ratio": 1.625615763546798, "no_speech_prob": 5.7718179959920235e-06}, {"id": 62, "seek": 27636, "start": 293.36, "end": 296.2, "text": " Let's now take a different view on bias and variance.", "tokens": [50364, 407, 281, 20858, 562, 274, 6915, 472, 337, 257, 8213, 26110, 11, 508, 3847, 390, 1090, 293, 508, 66, 85, 390, 1090, 13, 50779, 50779, 1133, 274, 6915, 1451, 11, 508, 3847, 390, 2295, 11, 457, 508, 66, 85, 307, 1090, 13, 51029, 51029, 400, 562, 274, 6915, 732, 11, 1293, 645, 1238, 2295, 13, 51214, 51214, 961, 311, 586, 747, 257, 819, 1910, 322, 12577, 293, 21977, 13, 51356, 51356, 400, 294, 1729, 11, 322, 264, 958, 4137, 11, 286, 1116, 411, 281, 855, 291, 577, 508, 3847, 293, 508, 66, 85, 10559, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.13697183375455896, "compression_ratio": 1.625615763546798, "no_speech_prob": 5.7718179959920235e-06}, {"id": 63, "seek": 27636, "start": 296.2, "end": 302.52000000000004, "text": " And in particular, on the next slide, I'd like to show you how J train and Jcv vary", "tokens": [50364, 407, 281, 20858, 562, 274, 6915, 472, 337, 257, 8213, 26110, 11, 508, 3847, 390, 1090, 293, 508, 66, 85, 390, 1090, 13, 50779, 50779, 1133, 274, 6915, 1451, 11, 508, 3847, 390, 2295, 11, 457, 508, 66, 85, 307, 1090, 13, 51029, 51029, 400, 562, 274, 6915, 732, 11, 1293, 645, 1238, 2295, 13, 51214, 51214, 961, 311, 586, 747, 257, 819, 1910, 322, 12577, 293, 21977, 13, 51356, 51356, 400, 294, 1729, 11, 322, 264, 958, 4137, 11, 286, 1116, 411, 281, 855, 291, 577, 508, 3847, 293, 508, 66, 85, 10559, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.13697183375455896, "compression_ratio": 1.625615763546798, "no_speech_prob": 5.7718179959920235e-06}, {"id": 64, "seek": 30252, "start": 302.52, "end": 307.28, "text": " as a function of the degree of the polynomial you're fitting.", "tokens": [50364, 382, 257, 2445, 295, 264, 4314, 295, 264, 26110, 291, 434, 15669, 13, 50602, 50602, 407, 718, 385, 2642, 257, 2573, 689, 264, 12750, 10298, 295, 341, 2573, 486, 312, 264, 4314, 295, 50820, 50820, 26110, 300, 321, 434, 15669, 281, 264, 1412, 13, 51052, 51052, 4886, 322, 264, 1411, 486, 6805, 281, 257, 1359, 2158, 295, 274, 11, 411, 274, 6915, 472, 11, 597, 23249, 51318, 51318, 281, 257, 15669, 2997, 1622, 13, 51410, 51410, 400, 670, 281, 264, 558, 486, 6805, 281, 584, 274, 6915, 1451, 420, 754, 2946, 4190, 295, 274, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.12270270453559028, "compression_ratio": 1.8387096774193548, "no_speech_prob": 6.339134415611625e-06}, {"id": 65, "seek": 30252, "start": 307.28, "end": 311.64, "text": " So let me draw a figure where the horizontal axis of this figure will be the degree of", "tokens": [50364, 382, 257, 2445, 295, 264, 4314, 295, 264, 26110, 291, 434, 15669, 13, 50602, 50602, 407, 718, 385, 2642, 257, 2573, 689, 264, 12750, 10298, 295, 341, 2573, 486, 312, 264, 4314, 295, 50820, 50820, 26110, 300, 321, 434, 15669, 281, 264, 1412, 13, 51052, 51052, 4886, 322, 264, 1411, 486, 6805, 281, 257, 1359, 2158, 295, 274, 11, 411, 274, 6915, 472, 11, 597, 23249, 51318, 51318, 281, 257, 15669, 2997, 1622, 13, 51410, 51410, 400, 670, 281, 264, 558, 486, 6805, 281, 584, 274, 6915, 1451, 420, 754, 2946, 4190, 295, 274, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.12270270453559028, "compression_ratio": 1.8387096774193548, "no_speech_prob": 6.339134415611625e-06}, {"id": 66, "seek": 30252, "start": 311.64, "end": 316.28, "text": " polynomial that we're fitting to the data.", "tokens": [50364, 382, 257, 2445, 295, 264, 4314, 295, 264, 26110, 291, 434, 15669, 13, 50602, 50602, 407, 718, 385, 2642, 257, 2573, 689, 264, 12750, 10298, 295, 341, 2573, 486, 312, 264, 4314, 295, 50820, 50820, 26110, 300, 321, 434, 15669, 281, 264, 1412, 13, 51052, 51052, 4886, 322, 264, 1411, 486, 6805, 281, 257, 1359, 2158, 295, 274, 11, 411, 274, 6915, 472, 11, 597, 23249, 51318, 51318, 281, 257, 15669, 2997, 1622, 13, 51410, 51410, 400, 670, 281, 264, 558, 486, 6805, 281, 584, 274, 6915, 1451, 420, 754, 2946, 4190, 295, 274, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.12270270453559028, "compression_ratio": 1.8387096774193548, "no_speech_prob": 6.339134415611625e-06}, {"id": 67, "seek": 30252, "start": 316.28, "end": 321.59999999999997, "text": " Over on the left will correspond to a small value of d, like d equals one, which corresponds", "tokens": [50364, 382, 257, 2445, 295, 264, 4314, 295, 264, 26110, 291, 434, 15669, 13, 50602, 50602, 407, 718, 385, 2642, 257, 2573, 689, 264, 12750, 10298, 295, 341, 2573, 486, 312, 264, 4314, 295, 50820, 50820, 26110, 300, 321, 434, 15669, 281, 264, 1412, 13, 51052, 51052, 4886, 322, 264, 1411, 486, 6805, 281, 257, 1359, 2158, 295, 274, 11, 411, 274, 6915, 472, 11, 597, 23249, 51318, 51318, 281, 257, 15669, 2997, 1622, 13, 51410, 51410, 400, 670, 281, 264, 558, 486, 6805, 281, 584, 274, 6915, 1451, 420, 754, 2946, 4190, 295, 274, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.12270270453559028, "compression_ratio": 1.8387096774193548, "no_speech_prob": 6.339134415611625e-06}, {"id": 68, "seek": 30252, "start": 321.59999999999997, "end": 323.44, "text": " to a fitting straight line.", "tokens": [50364, 382, 257, 2445, 295, 264, 4314, 295, 264, 26110, 291, 434, 15669, 13, 50602, 50602, 407, 718, 385, 2642, 257, 2573, 689, 264, 12750, 10298, 295, 341, 2573, 486, 312, 264, 4314, 295, 50820, 50820, 26110, 300, 321, 434, 15669, 281, 264, 1412, 13, 51052, 51052, 4886, 322, 264, 1411, 486, 6805, 281, 257, 1359, 2158, 295, 274, 11, 411, 274, 6915, 472, 11, 597, 23249, 51318, 51318, 281, 257, 15669, 2997, 1622, 13, 51410, 51410, 400, 670, 281, 264, 558, 486, 6805, 281, 584, 274, 6915, 1451, 420, 754, 2946, 4190, 295, 274, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.12270270453559028, "compression_ratio": 1.8387096774193548, "no_speech_prob": 6.339134415611625e-06}, {"id": 69, "seek": 30252, "start": 323.44, "end": 329.91999999999996, "text": " And over to the right will correspond to say d equals four or even higher values of d,", "tokens": [50364, 382, 257, 2445, 295, 264, 4314, 295, 264, 26110, 291, 434, 15669, 13, 50602, 50602, 407, 718, 385, 2642, 257, 2573, 689, 264, 12750, 10298, 295, 341, 2573, 486, 312, 264, 4314, 295, 50820, 50820, 26110, 300, 321, 434, 15669, 281, 264, 1412, 13, 51052, 51052, 4886, 322, 264, 1411, 486, 6805, 281, 257, 1359, 2158, 295, 274, 11, 411, 274, 6915, 472, 11, 597, 23249, 51318, 51318, 281, 257, 15669, 2997, 1622, 13, 51410, 51410, 400, 670, 281, 264, 558, 486, 6805, 281, 584, 274, 6915, 1451, 420, 754, 2946, 4190, 295, 274, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.12270270453559028, "compression_ratio": 1.8387096774193548, "no_speech_prob": 6.339134415611625e-06}, {"id": 70, "seek": 32992, "start": 329.92, "end": 333.72, "text": " where we're fitting this high order polynomial.", "tokens": [50364, 689, 321, 434, 15669, 341, 1090, 1668, 26110, 13, 50554, 50554, 407, 498, 291, 645, 281, 7542, 508, 3847, 295, 343, 65, 382, 257, 2445, 295, 4314, 295, 26110, 11, 437, 291, 51000, 51000, 915, 307, 300, 382, 291, 3318, 257, 2946, 293, 2946, 4314, 26110, 11, 510, 286, 478, 11926, 321, 434, 51324, 51324, 406, 1228, 3890, 2144, 11, 457, 382, 291, 3318, 257, 2946, 293, 2946, 1668, 26110, 11, 264, 3097, 51592, 51592, 6713, 486, 3928, 281, 352, 760, 570, 562, 291, 362, 257, 588, 2199, 8213, 2445, 11, 309, 1177, 380, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.11072417667933873, "compression_ratio": 1.8108108108108107, "no_speech_prob": 4.637809979612939e-06}, {"id": 71, "seek": 32992, "start": 333.72, "end": 342.64000000000004, "text": " So if you were to plot J train of Wb as a function of degree of polynomial, what you", "tokens": [50364, 689, 321, 434, 15669, 341, 1090, 1668, 26110, 13, 50554, 50554, 407, 498, 291, 645, 281, 7542, 508, 3847, 295, 343, 65, 382, 257, 2445, 295, 4314, 295, 26110, 11, 437, 291, 51000, 51000, 915, 307, 300, 382, 291, 3318, 257, 2946, 293, 2946, 4314, 26110, 11, 510, 286, 478, 11926, 321, 434, 51324, 51324, 406, 1228, 3890, 2144, 11, 457, 382, 291, 3318, 257, 2946, 293, 2946, 1668, 26110, 11, 264, 3097, 51592, 51592, 6713, 486, 3928, 281, 352, 760, 570, 562, 291, 362, 257, 588, 2199, 8213, 2445, 11, 309, 1177, 380, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.11072417667933873, "compression_ratio": 1.8108108108108107, "no_speech_prob": 4.637809979612939e-06}, {"id": 72, "seek": 32992, "start": 342.64000000000004, "end": 349.12, "text": " find is that as you fit a higher and higher degree polynomial, here I'm assuming we're", "tokens": [50364, 689, 321, 434, 15669, 341, 1090, 1668, 26110, 13, 50554, 50554, 407, 498, 291, 645, 281, 7542, 508, 3847, 295, 343, 65, 382, 257, 2445, 295, 4314, 295, 26110, 11, 437, 291, 51000, 51000, 915, 307, 300, 382, 291, 3318, 257, 2946, 293, 2946, 4314, 26110, 11, 510, 286, 478, 11926, 321, 434, 51324, 51324, 406, 1228, 3890, 2144, 11, 457, 382, 291, 3318, 257, 2946, 293, 2946, 1668, 26110, 11, 264, 3097, 51592, 51592, 6713, 486, 3928, 281, 352, 760, 570, 562, 291, 362, 257, 588, 2199, 8213, 2445, 11, 309, 1177, 380, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.11072417667933873, "compression_ratio": 1.8108108108108107, "no_speech_prob": 4.637809979612939e-06}, {"id": 73, "seek": 32992, "start": 349.12, "end": 354.48, "text": " not using regularization, but as you fit a higher and higher order polynomial, the training", "tokens": [50364, 689, 321, 434, 15669, 341, 1090, 1668, 26110, 13, 50554, 50554, 407, 498, 291, 645, 281, 7542, 508, 3847, 295, 343, 65, 382, 257, 2445, 295, 4314, 295, 26110, 11, 437, 291, 51000, 51000, 915, 307, 300, 382, 291, 3318, 257, 2946, 293, 2946, 4314, 26110, 11, 510, 286, 478, 11926, 321, 434, 51324, 51324, 406, 1228, 3890, 2144, 11, 457, 382, 291, 3318, 257, 2946, 293, 2946, 1668, 26110, 11, 264, 3097, 51592, 51592, 6713, 486, 3928, 281, 352, 760, 570, 562, 291, 362, 257, 588, 2199, 8213, 2445, 11, 309, 1177, 380, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.11072417667933873, "compression_ratio": 1.8108108108108107, "no_speech_prob": 4.637809979612939e-06}, {"id": 74, "seek": 32992, "start": 354.48, "end": 359.64, "text": " error will tend to go down because when you have a very simple linear function, it doesn't", "tokens": [50364, 689, 321, 434, 15669, 341, 1090, 1668, 26110, 13, 50554, 50554, 407, 498, 291, 645, 281, 7542, 508, 3847, 295, 343, 65, 382, 257, 2445, 295, 4314, 295, 26110, 11, 437, 291, 51000, 51000, 915, 307, 300, 382, 291, 3318, 257, 2946, 293, 2946, 4314, 26110, 11, 510, 286, 478, 11926, 321, 434, 51324, 51324, 406, 1228, 3890, 2144, 11, 457, 382, 291, 3318, 257, 2946, 293, 2946, 1668, 26110, 11, 264, 3097, 51592, 51592, 6713, 486, 3928, 281, 352, 760, 570, 562, 291, 362, 257, 588, 2199, 8213, 2445, 11, 309, 1177, 380, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.11072417667933873, "compression_ratio": 1.8108108108108107, "no_speech_prob": 4.637809979612939e-06}, {"id": 75, "seek": 35964, "start": 359.64, "end": 361.71999999999997, "text": " fit the training data that well.", "tokens": [50364, 3318, 264, 3097, 1412, 300, 731, 13, 50468, 50468, 1133, 291, 3318, 257, 37262, 2445, 420, 257, 2636, 1668, 26110, 420, 257, 6409, 1668, 26110, 11, 50758, 50758, 309, 9001, 264, 3097, 1412, 1101, 293, 1101, 13, 50904, 50904, 407, 382, 264, 4314, 295, 26110, 8637, 11, 508, 3847, 486, 5850, 352, 760, 13, 51230, 51230, 3087, 11, 718, 311, 574, 412, 508, 66, 85, 11, 597, 307, 577, 731, 775, 309, 360, 322, 1412, 300, 309, 630, 406, 483, 281, 51730, 51730, 3318, 281, 30, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.12061057620578342, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.0782977369672153e-05}, {"id": 76, "seek": 35964, "start": 361.71999999999997, "end": 367.52, "text": " When you fit a quadratic function or a third order polynomial or a fourth order polynomial,", "tokens": [50364, 3318, 264, 3097, 1412, 300, 731, 13, 50468, 50468, 1133, 291, 3318, 257, 37262, 2445, 420, 257, 2636, 1668, 26110, 420, 257, 6409, 1668, 26110, 11, 50758, 50758, 309, 9001, 264, 3097, 1412, 1101, 293, 1101, 13, 50904, 50904, 407, 382, 264, 4314, 295, 26110, 8637, 11, 508, 3847, 486, 5850, 352, 760, 13, 51230, 51230, 3087, 11, 718, 311, 574, 412, 508, 66, 85, 11, 597, 307, 577, 731, 775, 309, 360, 322, 1412, 300, 309, 630, 406, 483, 281, 51730, 51730, 3318, 281, 30, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.12061057620578342, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.0782977369672153e-05}, {"id": 77, "seek": 35964, "start": 367.52, "end": 370.44, "text": " it fits the training data better and better.", "tokens": [50364, 3318, 264, 3097, 1412, 300, 731, 13, 50468, 50468, 1133, 291, 3318, 257, 37262, 2445, 420, 257, 2636, 1668, 26110, 420, 257, 6409, 1668, 26110, 11, 50758, 50758, 309, 9001, 264, 3097, 1412, 1101, 293, 1101, 13, 50904, 50904, 407, 382, 264, 4314, 295, 26110, 8637, 11, 508, 3847, 486, 5850, 352, 760, 13, 51230, 51230, 3087, 11, 718, 311, 574, 412, 508, 66, 85, 11, 597, 307, 577, 731, 775, 309, 360, 322, 1412, 300, 309, 630, 406, 483, 281, 51730, 51730, 3318, 281, 30, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.12061057620578342, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.0782977369672153e-05}, {"id": 78, "seek": 35964, "start": 370.44, "end": 376.96, "text": " So as the degree of polynomial increases, J train will typically go down.", "tokens": [50364, 3318, 264, 3097, 1412, 300, 731, 13, 50468, 50468, 1133, 291, 3318, 257, 37262, 2445, 420, 257, 2636, 1668, 26110, 420, 257, 6409, 1668, 26110, 11, 50758, 50758, 309, 9001, 264, 3097, 1412, 1101, 293, 1101, 13, 50904, 50904, 407, 382, 264, 4314, 295, 26110, 8637, 11, 508, 3847, 486, 5850, 352, 760, 13, 51230, 51230, 3087, 11, 718, 311, 574, 412, 508, 66, 85, 11, 597, 307, 577, 731, 775, 309, 360, 322, 1412, 300, 309, 630, 406, 483, 281, 51730, 51730, 3318, 281, 30, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.12061057620578342, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.0782977369672153e-05}, {"id": 79, "seek": 35964, "start": 376.96, "end": 386.96, "text": " Next, let's look at Jcv, which is how well does it do on data that it did not get to", "tokens": [50364, 3318, 264, 3097, 1412, 300, 731, 13, 50468, 50468, 1133, 291, 3318, 257, 37262, 2445, 420, 257, 2636, 1668, 26110, 420, 257, 6409, 1668, 26110, 11, 50758, 50758, 309, 9001, 264, 3097, 1412, 1101, 293, 1101, 13, 50904, 50904, 407, 382, 264, 4314, 295, 26110, 8637, 11, 508, 3847, 486, 5850, 352, 760, 13, 51230, 51230, 3087, 11, 718, 311, 574, 412, 508, 66, 85, 11, 597, 307, 577, 731, 775, 309, 360, 322, 1412, 300, 309, 630, 406, 483, 281, 51730, 51730, 3318, 281, 30, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.12061057620578342, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.0782977369672153e-05}, {"id": 80, "seek": 35964, "start": 386.96, "end": 388.15999999999997, "text": " fit to?", "tokens": [50364, 3318, 264, 3097, 1412, 300, 731, 13, 50468, 50468, 1133, 291, 3318, 257, 37262, 2445, 420, 257, 2636, 1668, 26110, 420, 257, 6409, 1668, 26110, 11, 50758, 50758, 309, 9001, 264, 3097, 1412, 1101, 293, 1101, 13, 50904, 50904, 407, 382, 264, 4314, 295, 26110, 8637, 11, 508, 3847, 486, 5850, 352, 760, 13, 51230, 51230, 3087, 11, 718, 311, 574, 412, 508, 66, 85, 11, 597, 307, 577, 731, 775, 309, 360, 322, 1412, 300, 309, 630, 406, 483, 281, 51730, 51730, 3318, 281, 30, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.12061057620578342, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.0782977369672153e-05}, {"id": 81, "seek": 38816, "start": 388.16, "end": 394.64000000000004, "text": " What we saw was when d equals one, when the degree of polynomial was very low, Jcv was", "tokens": [50364, 708, 321, 1866, 390, 562, 274, 6915, 472, 11, 562, 264, 4314, 295, 26110, 390, 588, 2295, 11, 508, 66, 85, 390, 50688, 50688, 1238, 1090, 570, 309, 833, 6845, 11, 370, 309, 994, 380, 360, 731, 322, 264, 3278, 24071, 992, 13, 50976, 50976, 400, 510, 322, 264, 558, 382, 731, 11, 562, 264, 4314, 295, 26110, 307, 588, 2416, 11, 584, 1451, 11, 309, 51294, 51294, 1177, 380, 360, 731, 322, 264, 3278, 24071, 992, 2139, 293, 370, 307, 611, 1090, 13, 51583, 51583, 583, 498, 274, 390, 294, 1296, 11, 584, 257, 1150, 1668, 26110, 11, 550, 309, 767, 630, 709, 1101, 13, 51863, 51863], "temperature": 0.0, "avg_logprob": -0.11658307855779475, "compression_ratio": 1.8421052631578947, "no_speech_prob": 7.889043445175048e-06}, {"id": 82, "seek": 38816, "start": 394.64000000000004, "end": 400.40000000000003, "text": " pretty high because it underfit, so it didn't do well on the cross validation set.", "tokens": [50364, 708, 321, 1866, 390, 562, 274, 6915, 472, 11, 562, 264, 4314, 295, 26110, 390, 588, 2295, 11, 508, 66, 85, 390, 50688, 50688, 1238, 1090, 570, 309, 833, 6845, 11, 370, 309, 994, 380, 360, 731, 322, 264, 3278, 24071, 992, 13, 50976, 50976, 400, 510, 322, 264, 558, 382, 731, 11, 562, 264, 4314, 295, 26110, 307, 588, 2416, 11, 584, 1451, 11, 309, 51294, 51294, 1177, 380, 360, 731, 322, 264, 3278, 24071, 992, 2139, 293, 370, 307, 611, 1090, 13, 51583, 51583, 583, 498, 274, 390, 294, 1296, 11, 584, 257, 1150, 1668, 26110, 11, 550, 309, 767, 630, 709, 1101, 13, 51863, 51863], "temperature": 0.0, "avg_logprob": -0.11658307855779475, "compression_ratio": 1.8421052631578947, "no_speech_prob": 7.889043445175048e-06}, {"id": 83, "seek": 38816, "start": 400.40000000000003, "end": 406.76000000000005, "text": " And here on the right as well, when the degree of polynomial is very large, say four, it", "tokens": [50364, 708, 321, 1866, 390, 562, 274, 6915, 472, 11, 562, 264, 4314, 295, 26110, 390, 588, 2295, 11, 508, 66, 85, 390, 50688, 50688, 1238, 1090, 570, 309, 833, 6845, 11, 370, 309, 994, 380, 360, 731, 322, 264, 3278, 24071, 992, 13, 50976, 50976, 400, 510, 322, 264, 558, 382, 731, 11, 562, 264, 4314, 295, 26110, 307, 588, 2416, 11, 584, 1451, 11, 309, 51294, 51294, 1177, 380, 360, 731, 322, 264, 3278, 24071, 992, 2139, 293, 370, 307, 611, 1090, 13, 51583, 51583, 583, 498, 274, 390, 294, 1296, 11, 584, 257, 1150, 1668, 26110, 11, 550, 309, 767, 630, 709, 1101, 13, 51863, 51863], "temperature": 0.0, "avg_logprob": -0.11658307855779475, "compression_ratio": 1.8421052631578947, "no_speech_prob": 7.889043445175048e-06}, {"id": 84, "seek": 38816, "start": 406.76000000000005, "end": 412.54, "text": " doesn't do well on the cross validation set either and so is also high.", "tokens": [50364, 708, 321, 1866, 390, 562, 274, 6915, 472, 11, 562, 264, 4314, 295, 26110, 390, 588, 2295, 11, 508, 66, 85, 390, 50688, 50688, 1238, 1090, 570, 309, 833, 6845, 11, 370, 309, 994, 380, 360, 731, 322, 264, 3278, 24071, 992, 13, 50976, 50976, 400, 510, 322, 264, 558, 382, 731, 11, 562, 264, 4314, 295, 26110, 307, 588, 2416, 11, 584, 1451, 11, 309, 51294, 51294, 1177, 380, 360, 731, 322, 264, 3278, 24071, 992, 2139, 293, 370, 307, 611, 1090, 13, 51583, 51583, 583, 498, 274, 390, 294, 1296, 11, 584, 257, 1150, 1668, 26110, 11, 550, 309, 767, 630, 709, 1101, 13, 51863, 51863], "temperature": 0.0, "avg_logprob": -0.11658307855779475, "compression_ratio": 1.8421052631578947, "no_speech_prob": 7.889043445175048e-06}, {"id": 85, "seek": 38816, "start": 412.54, "end": 418.14000000000004, "text": " But if d was in between, say a second order polynomial, then it actually did much better.", "tokens": [50364, 708, 321, 1866, 390, 562, 274, 6915, 472, 11, 562, 264, 4314, 295, 26110, 390, 588, 2295, 11, 508, 66, 85, 390, 50688, 50688, 1238, 1090, 570, 309, 833, 6845, 11, 370, 309, 994, 380, 360, 731, 322, 264, 3278, 24071, 992, 13, 50976, 50976, 400, 510, 322, 264, 558, 382, 731, 11, 562, 264, 4314, 295, 26110, 307, 588, 2416, 11, 584, 1451, 11, 309, 51294, 51294, 1177, 380, 360, 731, 322, 264, 3278, 24071, 992, 2139, 293, 370, 307, 611, 1090, 13, 51583, 51583, 583, 498, 274, 390, 294, 1296, 11, 584, 257, 1150, 1668, 26110, 11, 550, 309, 767, 630, 709, 1101, 13, 51863, 51863], "temperature": 0.0, "avg_logprob": -0.11658307855779475, "compression_ratio": 1.8421052631578947, "no_speech_prob": 7.889043445175048e-06}, {"id": 86, "seek": 41814, "start": 418.14, "end": 423.47999999999996, "text": " And so if you were to vary the degree of polynomial, you'd actually get a curve that looks like", "tokens": [50364, 400, 370, 498, 291, 645, 281, 10559, 264, 4314, 295, 26110, 11, 291, 1116, 767, 483, 257, 7605, 300, 1542, 411, 50631, 50631, 341, 11, 597, 1487, 760, 293, 550, 1709, 646, 493, 13, 50815, 50815, 2305, 498, 264, 4314, 295, 26110, 307, 886, 2295, 11, 309, 833, 13979, 293, 370, 1177, 380, 360, 731, 322, 264, 51111, 51111, 3278, 24071, 992, 13, 51173, 51173, 759, 309, 307, 886, 1090, 11, 309, 670, 13979, 293, 611, 1177, 380, 360, 731, 322, 264, 3278, 24071, 992, 13, 51477, 51477, 400, 456, 311, 787, 498, 309, 311, 4079, 294, 264, 2808, 300, 307, 445, 558, 11, 597, 307, 983, 264, 51727, 51727], "temperature": 0.0, "avg_logprob": -0.12294795683452062, "compression_ratio": 1.8311688311688312, "no_speech_prob": 1.1659198207780719e-05}, {"id": 87, "seek": 41814, "start": 423.47999999999996, "end": 427.15999999999997, "text": " this, which comes down and then goes back up.", "tokens": [50364, 400, 370, 498, 291, 645, 281, 10559, 264, 4314, 295, 26110, 11, 291, 1116, 767, 483, 257, 7605, 300, 1542, 411, 50631, 50631, 341, 11, 597, 1487, 760, 293, 550, 1709, 646, 493, 13, 50815, 50815, 2305, 498, 264, 4314, 295, 26110, 307, 886, 2295, 11, 309, 833, 13979, 293, 370, 1177, 380, 360, 731, 322, 264, 51111, 51111, 3278, 24071, 992, 13, 51173, 51173, 759, 309, 307, 886, 1090, 11, 309, 670, 13979, 293, 611, 1177, 380, 360, 731, 322, 264, 3278, 24071, 992, 13, 51477, 51477, 400, 456, 311, 787, 498, 309, 311, 4079, 294, 264, 2808, 300, 307, 445, 558, 11, 597, 307, 983, 264, 51727, 51727], "temperature": 0.0, "avg_logprob": -0.12294795683452062, "compression_ratio": 1.8311688311688312, "no_speech_prob": 1.1659198207780719e-05}, {"id": 88, "seek": 41814, "start": 427.15999999999997, "end": 433.08, "text": " Where if the degree of polynomial is too low, it underfits and so doesn't do well on the", "tokens": [50364, 400, 370, 498, 291, 645, 281, 10559, 264, 4314, 295, 26110, 11, 291, 1116, 767, 483, 257, 7605, 300, 1542, 411, 50631, 50631, 341, 11, 597, 1487, 760, 293, 550, 1709, 646, 493, 13, 50815, 50815, 2305, 498, 264, 4314, 295, 26110, 307, 886, 2295, 11, 309, 833, 13979, 293, 370, 1177, 380, 360, 731, 322, 264, 51111, 51111, 3278, 24071, 992, 13, 51173, 51173, 759, 309, 307, 886, 1090, 11, 309, 670, 13979, 293, 611, 1177, 380, 360, 731, 322, 264, 3278, 24071, 992, 13, 51477, 51477, 400, 456, 311, 787, 498, 309, 311, 4079, 294, 264, 2808, 300, 307, 445, 558, 11, 597, 307, 983, 264, 51727, 51727], "temperature": 0.0, "avg_logprob": -0.12294795683452062, "compression_ratio": 1.8311688311688312, "no_speech_prob": 1.1659198207780719e-05}, {"id": 89, "seek": 41814, "start": 433.08, "end": 434.32, "text": " cross validation set.", "tokens": [50364, 400, 370, 498, 291, 645, 281, 10559, 264, 4314, 295, 26110, 11, 291, 1116, 767, 483, 257, 7605, 300, 1542, 411, 50631, 50631, 341, 11, 597, 1487, 760, 293, 550, 1709, 646, 493, 13, 50815, 50815, 2305, 498, 264, 4314, 295, 26110, 307, 886, 2295, 11, 309, 833, 13979, 293, 370, 1177, 380, 360, 731, 322, 264, 51111, 51111, 3278, 24071, 992, 13, 51173, 51173, 759, 309, 307, 886, 1090, 11, 309, 670, 13979, 293, 611, 1177, 380, 360, 731, 322, 264, 3278, 24071, 992, 13, 51477, 51477, 400, 456, 311, 787, 498, 309, 311, 4079, 294, 264, 2808, 300, 307, 445, 558, 11, 597, 307, 983, 264, 51727, 51727], "temperature": 0.0, "avg_logprob": -0.12294795683452062, "compression_ratio": 1.8311688311688312, "no_speech_prob": 1.1659198207780719e-05}, {"id": 90, "seek": 41814, "start": 434.32, "end": 440.4, "text": " If it is too high, it overfits and also doesn't do well on the cross validation set.", "tokens": [50364, 400, 370, 498, 291, 645, 281, 10559, 264, 4314, 295, 26110, 11, 291, 1116, 767, 483, 257, 7605, 300, 1542, 411, 50631, 50631, 341, 11, 597, 1487, 760, 293, 550, 1709, 646, 493, 13, 50815, 50815, 2305, 498, 264, 4314, 295, 26110, 307, 886, 2295, 11, 309, 833, 13979, 293, 370, 1177, 380, 360, 731, 322, 264, 51111, 51111, 3278, 24071, 992, 13, 51173, 51173, 759, 309, 307, 886, 1090, 11, 309, 670, 13979, 293, 611, 1177, 380, 360, 731, 322, 264, 3278, 24071, 992, 13, 51477, 51477, 400, 456, 311, 787, 498, 309, 311, 4079, 294, 264, 2808, 300, 307, 445, 558, 11, 597, 307, 983, 264, 51727, 51727], "temperature": 0.0, "avg_logprob": -0.12294795683452062, "compression_ratio": 1.8311688311688312, "no_speech_prob": 1.1659198207780719e-05}, {"id": 91, "seek": 41814, "start": 440.4, "end": 445.4, "text": " And there's only if it's somewhere in the middle that is just right, which is why the", "tokens": [50364, 400, 370, 498, 291, 645, 281, 10559, 264, 4314, 295, 26110, 11, 291, 1116, 767, 483, 257, 7605, 300, 1542, 411, 50631, 50631, 341, 11, 597, 1487, 760, 293, 550, 1709, 646, 493, 13, 50815, 50815, 2305, 498, 264, 4314, 295, 26110, 307, 886, 2295, 11, 309, 833, 13979, 293, 370, 1177, 380, 360, 731, 322, 264, 51111, 51111, 3278, 24071, 992, 13, 51173, 51173, 759, 309, 307, 886, 1090, 11, 309, 670, 13979, 293, 611, 1177, 380, 360, 731, 322, 264, 3278, 24071, 992, 13, 51477, 51477, 400, 456, 311, 787, 498, 309, 311, 4079, 294, 264, 2808, 300, 307, 445, 558, 11, 597, 307, 983, 264, 51727, 51727], "temperature": 0.0, "avg_logprob": -0.12294795683452062, "compression_ratio": 1.8311688311688312, "no_speech_prob": 1.1659198207780719e-05}, {"id": 92, "seek": 44540, "start": 445.4, "end": 450.67999999999995, "text": " second order polynomial in our example ends up with a lower cross validation error and", "tokens": [50364, 1150, 1668, 26110, 294, 527, 1365, 5314, 493, 365, 257, 3126, 3278, 24071, 6713, 293, 50628, 50628, 9662, 1090, 12577, 6051, 1090, 21977, 13, 50812, 50812, 407, 281, 20858, 11, 577, 360, 291, 36238, 12577, 293, 21977, 294, 428, 2539, 9284, 30, 51143, 51143, 759, 428, 2539, 9284, 575, 1090, 12577, 420, 575, 833, 6845, 1412, 11, 264, 2141, 16961, 486, 51394, 51394, 312, 498, 508, 83, 7146, 307, 1090, 13, 51552, 51552, 400, 370, 300, 23249, 281, 341, 1411, 1761, 8044, 295, 264, 7605, 11, 597, 307, 689, 508, 83, 7146, 307, 1090, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.1087634154040404, "compression_ratio": 1.6859504132231404, "no_speech_prob": 2.0904294615320396e-06}, {"id": 93, "seek": 44540, "start": 450.67999999999995, "end": 454.35999999999996, "text": " neither high bias nor high variance.", "tokens": [50364, 1150, 1668, 26110, 294, 527, 1365, 5314, 493, 365, 257, 3126, 3278, 24071, 6713, 293, 50628, 50628, 9662, 1090, 12577, 6051, 1090, 21977, 13, 50812, 50812, 407, 281, 20858, 11, 577, 360, 291, 36238, 12577, 293, 21977, 294, 428, 2539, 9284, 30, 51143, 51143, 759, 428, 2539, 9284, 575, 1090, 12577, 420, 575, 833, 6845, 1412, 11, 264, 2141, 16961, 486, 51394, 51394, 312, 498, 508, 83, 7146, 307, 1090, 13, 51552, 51552, 400, 370, 300, 23249, 281, 341, 1411, 1761, 8044, 295, 264, 7605, 11, 597, 307, 689, 508, 83, 7146, 307, 1090, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.1087634154040404, "compression_ratio": 1.6859504132231404, "no_speech_prob": 2.0904294615320396e-06}, {"id": 94, "seek": 44540, "start": 454.35999999999996, "end": 460.97999999999996, "text": " So to summarize, how do you diagnose bias and variance in your learning algorithm?", "tokens": [50364, 1150, 1668, 26110, 294, 527, 1365, 5314, 493, 365, 257, 3126, 3278, 24071, 6713, 293, 50628, 50628, 9662, 1090, 12577, 6051, 1090, 21977, 13, 50812, 50812, 407, 281, 20858, 11, 577, 360, 291, 36238, 12577, 293, 21977, 294, 428, 2539, 9284, 30, 51143, 51143, 759, 428, 2539, 9284, 575, 1090, 12577, 420, 575, 833, 6845, 1412, 11, 264, 2141, 16961, 486, 51394, 51394, 312, 498, 508, 83, 7146, 307, 1090, 13, 51552, 51552, 400, 370, 300, 23249, 281, 341, 1411, 1761, 8044, 295, 264, 7605, 11, 597, 307, 689, 508, 83, 7146, 307, 1090, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.1087634154040404, "compression_ratio": 1.6859504132231404, "no_speech_prob": 2.0904294615320396e-06}, {"id": 95, "seek": 44540, "start": 460.97999999999996, "end": 466.0, "text": " If your learning algorithm has high bias or has underfit data, the key indicator will", "tokens": [50364, 1150, 1668, 26110, 294, 527, 1365, 5314, 493, 365, 257, 3126, 3278, 24071, 6713, 293, 50628, 50628, 9662, 1090, 12577, 6051, 1090, 21977, 13, 50812, 50812, 407, 281, 20858, 11, 577, 360, 291, 36238, 12577, 293, 21977, 294, 428, 2539, 9284, 30, 51143, 51143, 759, 428, 2539, 9284, 575, 1090, 12577, 420, 575, 833, 6845, 1412, 11, 264, 2141, 16961, 486, 51394, 51394, 312, 498, 508, 83, 7146, 307, 1090, 13, 51552, 51552, 400, 370, 300, 23249, 281, 341, 1411, 1761, 8044, 295, 264, 7605, 11, 597, 307, 689, 508, 83, 7146, 307, 1090, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.1087634154040404, "compression_ratio": 1.6859504132231404, "no_speech_prob": 2.0904294615320396e-06}, {"id": 96, "seek": 44540, "start": 466.0, "end": 469.15999999999997, "text": " be if Jtrain is high.", "tokens": [50364, 1150, 1668, 26110, 294, 527, 1365, 5314, 493, 365, 257, 3126, 3278, 24071, 6713, 293, 50628, 50628, 9662, 1090, 12577, 6051, 1090, 21977, 13, 50812, 50812, 407, 281, 20858, 11, 577, 360, 291, 36238, 12577, 293, 21977, 294, 428, 2539, 9284, 30, 51143, 51143, 759, 428, 2539, 9284, 575, 1090, 12577, 420, 575, 833, 6845, 1412, 11, 264, 2141, 16961, 486, 51394, 51394, 312, 498, 508, 83, 7146, 307, 1090, 13, 51552, 51552, 400, 370, 300, 23249, 281, 341, 1411, 1761, 8044, 295, 264, 7605, 11, 597, 307, 689, 508, 83, 7146, 307, 1090, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.1087634154040404, "compression_ratio": 1.6859504132231404, "no_speech_prob": 2.0904294615320396e-06}, {"id": 97, "seek": 44540, "start": 469.15999999999997, "end": 474.52, "text": " And so that corresponds to this leftmost portion of the curve, which is where Jtrain is high.", "tokens": [50364, 1150, 1668, 26110, 294, 527, 1365, 5314, 493, 365, 257, 3126, 3278, 24071, 6713, 293, 50628, 50628, 9662, 1090, 12577, 6051, 1090, 21977, 13, 50812, 50812, 407, 281, 20858, 11, 577, 360, 291, 36238, 12577, 293, 21977, 294, 428, 2539, 9284, 30, 51143, 51143, 759, 428, 2539, 9284, 575, 1090, 12577, 420, 575, 833, 6845, 1412, 11, 264, 2141, 16961, 486, 51394, 51394, 312, 498, 508, 83, 7146, 307, 1090, 13, 51552, 51552, 400, 370, 300, 23249, 281, 341, 1411, 1761, 8044, 295, 264, 7605, 11, 597, 307, 689, 508, 83, 7146, 307, 1090, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.1087634154040404, "compression_ratio": 1.6859504132231404, "no_speech_prob": 2.0904294615320396e-06}, {"id": 98, "seek": 47452, "start": 474.52, "end": 479.71999999999997, "text": " And usually you have Jtrain and JCV will be close to each other.", "tokens": [50364, 400, 2673, 291, 362, 508, 83, 7146, 293, 49802, 53, 486, 312, 1998, 281, 1184, 661, 13, 50624, 50624, 400, 577, 360, 291, 36238, 498, 291, 362, 1090, 21977, 30, 50774, 50774, 1042, 264, 2141, 16961, 337, 1090, 21977, 486, 312, 498, 49802, 53, 307, 709, 5044, 813, 508, 83, 7146, 13, 51090, 51090, 639, 3834, 5044, 813, 1465, 294, 5221, 14942, 281, 709, 5044, 813, 13, 51278, 51278, 407, 341, 307, 5044, 293, 341, 1355, 709, 5044, 13, 51486, 51486, 400, 341, 558, 1761, 8044, 295, 264, 7542, 307, 689, 49802, 53, 307, 709, 5044, 813, 508, 83, 7146, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10480343954903738, "compression_ratio": 1.8591549295774648, "no_speech_prob": 4.222763436700916e-06}, {"id": 99, "seek": 47452, "start": 479.71999999999997, "end": 482.71999999999997, "text": " And how do you diagnose if you have high variance?", "tokens": [50364, 400, 2673, 291, 362, 508, 83, 7146, 293, 49802, 53, 486, 312, 1998, 281, 1184, 661, 13, 50624, 50624, 400, 577, 360, 291, 36238, 498, 291, 362, 1090, 21977, 30, 50774, 50774, 1042, 264, 2141, 16961, 337, 1090, 21977, 486, 312, 498, 49802, 53, 307, 709, 5044, 813, 508, 83, 7146, 13, 51090, 51090, 639, 3834, 5044, 813, 1465, 294, 5221, 14942, 281, 709, 5044, 813, 13, 51278, 51278, 407, 341, 307, 5044, 293, 341, 1355, 709, 5044, 13, 51486, 51486, 400, 341, 558, 1761, 8044, 295, 264, 7542, 307, 689, 49802, 53, 307, 709, 5044, 813, 508, 83, 7146, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10480343954903738, "compression_ratio": 1.8591549295774648, "no_speech_prob": 4.222763436700916e-06}, {"id": 100, "seek": 47452, "start": 482.71999999999997, "end": 489.03999999999996, "text": " Well the key indicator for high variance will be if JCV is much greater than Jtrain.", "tokens": [50364, 400, 2673, 291, 362, 508, 83, 7146, 293, 49802, 53, 486, 312, 1998, 281, 1184, 661, 13, 50624, 50624, 400, 577, 360, 291, 36238, 498, 291, 362, 1090, 21977, 30, 50774, 50774, 1042, 264, 2141, 16961, 337, 1090, 21977, 486, 312, 498, 49802, 53, 307, 709, 5044, 813, 508, 83, 7146, 13, 51090, 51090, 639, 3834, 5044, 813, 1465, 294, 5221, 14942, 281, 709, 5044, 813, 13, 51278, 51278, 407, 341, 307, 5044, 293, 341, 1355, 709, 5044, 13, 51486, 51486, 400, 341, 558, 1761, 8044, 295, 264, 7542, 307, 689, 49802, 53, 307, 709, 5044, 813, 508, 83, 7146, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10480343954903738, "compression_ratio": 1.8591549295774648, "no_speech_prob": 4.222763436700916e-06}, {"id": 101, "seek": 47452, "start": 489.03999999999996, "end": 492.79999999999995, "text": " This double greater than sign in math refers to much greater than.", "tokens": [50364, 400, 2673, 291, 362, 508, 83, 7146, 293, 49802, 53, 486, 312, 1998, 281, 1184, 661, 13, 50624, 50624, 400, 577, 360, 291, 36238, 498, 291, 362, 1090, 21977, 30, 50774, 50774, 1042, 264, 2141, 16961, 337, 1090, 21977, 486, 312, 498, 49802, 53, 307, 709, 5044, 813, 508, 83, 7146, 13, 51090, 51090, 639, 3834, 5044, 813, 1465, 294, 5221, 14942, 281, 709, 5044, 813, 13, 51278, 51278, 407, 341, 307, 5044, 293, 341, 1355, 709, 5044, 13, 51486, 51486, 400, 341, 558, 1761, 8044, 295, 264, 7542, 307, 689, 49802, 53, 307, 709, 5044, 813, 508, 83, 7146, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10480343954903738, "compression_ratio": 1.8591549295774648, "no_speech_prob": 4.222763436700916e-06}, {"id": 102, "seek": 47452, "start": 492.79999999999995, "end": 496.96, "text": " So this is greater and this means much greater.", "tokens": [50364, 400, 2673, 291, 362, 508, 83, 7146, 293, 49802, 53, 486, 312, 1998, 281, 1184, 661, 13, 50624, 50624, 400, 577, 360, 291, 36238, 498, 291, 362, 1090, 21977, 30, 50774, 50774, 1042, 264, 2141, 16961, 337, 1090, 21977, 486, 312, 498, 49802, 53, 307, 709, 5044, 813, 508, 83, 7146, 13, 51090, 51090, 639, 3834, 5044, 813, 1465, 294, 5221, 14942, 281, 709, 5044, 813, 13, 51278, 51278, 407, 341, 307, 5044, 293, 341, 1355, 709, 5044, 13, 51486, 51486, 400, 341, 558, 1761, 8044, 295, 264, 7542, 307, 689, 49802, 53, 307, 709, 5044, 813, 508, 83, 7146, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10480343954903738, "compression_ratio": 1.8591549295774648, "no_speech_prob": 4.222763436700916e-06}, {"id": 103, "seek": 47452, "start": 496.96, "end": 503.47999999999996, "text": " And this rightmost portion of the plot is where JCV is much greater than Jtrain.", "tokens": [50364, 400, 2673, 291, 362, 508, 83, 7146, 293, 49802, 53, 486, 312, 1998, 281, 1184, 661, 13, 50624, 50624, 400, 577, 360, 291, 36238, 498, 291, 362, 1090, 21977, 30, 50774, 50774, 1042, 264, 2141, 16961, 337, 1090, 21977, 486, 312, 498, 49802, 53, 307, 709, 5044, 813, 508, 83, 7146, 13, 51090, 51090, 639, 3834, 5044, 813, 1465, 294, 5221, 14942, 281, 709, 5044, 813, 13, 51278, 51278, 407, 341, 307, 5044, 293, 341, 1355, 709, 5044, 13, 51486, 51486, 400, 341, 558, 1761, 8044, 295, 264, 7542, 307, 689, 49802, 53, 307, 709, 5044, 813, 508, 83, 7146, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10480343954903738, "compression_ratio": 1.8591549295774648, "no_speech_prob": 4.222763436700916e-06}, {"id": 104, "seek": 50348, "start": 503.48, "end": 510.0, "text": " And usually Jtrain will be pretty low, but the key indicator is whether JCV is much greater", "tokens": [50364, 400, 2673, 508, 83, 7146, 486, 312, 1238, 2295, 11, 457, 264, 2141, 16961, 307, 1968, 49802, 53, 307, 709, 5044, 50690, 50690, 813, 508, 83, 7146, 13, 50772, 50772, 400, 300, 311, 437, 2314, 562, 321, 632, 3318, 257, 588, 1090, 1668, 26110, 281, 341, 1359, 28872, 13, 51076, 51076, 400, 754, 1673, 321, 600, 445, 1612, 12577, 293, 21977, 11, 309, 4523, 484, 294, 512, 3331, 309, 311, 1944, 281, 51387, 51387, 16561, 362, 1090, 12577, 293, 362, 1090, 21977, 13, 51620, 51620, 509, 1582, 380, 536, 341, 1051, 300, 709, 337, 8213, 24590, 11, 457, 309, 4523, 484, 300, 498, 291, 434, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.10078468672726132, "compression_ratio": 1.6577946768060836, "no_speech_prob": 2.9479872409865493e-06}, {"id": 105, "seek": 50348, "start": 510.0, "end": 511.64000000000004, "text": " than Jtrain.", "tokens": [50364, 400, 2673, 508, 83, 7146, 486, 312, 1238, 2295, 11, 457, 264, 2141, 16961, 307, 1968, 49802, 53, 307, 709, 5044, 50690, 50690, 813, 508, 83, 7146, 13, 50772, 50772, 400, 300, 311, 437, 2314, 562, 321, 632, 3318, 257, 588, 1090, 1668, 26110, 281, 341, 1359, 28872, 13, 51076, 51076, 400, 754, 1673, 321, 600, 445, 1612, 12577, 293, 21977, 11, 309, 4523, 484, 294, 512, 3331, 309, 311, 1944, 281, 51387, 51387, 16561, 362, 1090, 12577, 293, 362, 1090, 21977, 13, 51620, 51620, 509, 1582, 380, 536, 341, 1051, 300, 709, 337, 8213, 24590, 11, 457, 309, 4523, 484, 300, 498, 291, 434, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.10078468672726132, "compression_ratio": 1.6577946768060836, "no_speech_prob": 2.9479872409865493e-06}, {"id": 106, "seek": 50348, "start": 511.64000000000004, "end": 517.72, "text": " And that's what happens when we had fit a very high order polynomial to this small dataset.", "tokens": [50364, 400, 2673, 508, 83, 7146, 486, 312, 1238, 2295, 11, 457, 264, 2141, 16961, 307, 1968, 49802, 53, 307, 709, 5044, 50690, 50690, 813, 508, 83, 7146, 13, 50772, 50772, 400, 300, 311, 437, 2314, 562, 321, 632, 3318, 257, 588, 1090, 1668, 26110, 281, 341, 1359, 28872, 13, 51076, 51076, 400, 754, 1673, 321, 600, 445, 1612, 12577, 293, 21977, 11, 309, 4523, 484, 294, 512, 3331, 309, 311, 1944, 281, 51387, 51387, 16561, 362, 1090, 12577, 293, 362, 1090, 21977, 13, 51620, 51620, 509, 1582, 380, 536, 341, 1051, 300, 709, 337, 8213, 24590, 11, 457, 309, 4523, 484, 300, 498, 291, 434, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.10078468672726132, "compression_ratio": 1.6577946768060836, "no_speech_prob": 2.9479872409865493e-06}, {"id": 107, "seek": 50348, "start": 517.72, "end": 523.94, "text": " And even though we've just seen bias and variance, it turns out in some cases it's possible to", "tokens": [50364, 400, 2673, 508, 83, 7146, 486, 312, 1238, 2295, 11, 457, 264, 2141, 16961, 307, 1968, 49802, 53, 307, 709, 5044, 50690, 50690, 813, 508, 83, 7146, 13, 50772, 50772, 400, 300, 311, 437, 2314, 562, 321, 632, 3318, 257, 588, 1090, 1668, 26110, 281, 341, 1359, 28872, 13, 51076, 51076, 400, 754, 1673, 321, 600, 445, 1612, 12577, 293, 21977, 11, 309, 4523, 484, 294, 512, 3331, 309, 311, 1944, 281, 51387, 51387, 16561, 362, 1090, 12577, 293, 362, 1090, 21977, 13, 51620, 51620, 509, 1582, 380, 536, 341, 1051, 300, 709, 337, 8213, 24590, 11, 457, 309, 4523, 484, 300, 498, 291, 434, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.10078468672726132, "compression_ratio": 1.6577946768060836, "no_speech_prob": 2.9479872409865493e-06}, {"id": 108, "seek": 50348, "start": 523.94, "end": 528.6, "text": " simultaneously have high bias and have high variance.", "tokens": [50364, 400, 2673, 508, 83, 7146, 486, 312, 1238, 2295, 11, 457, 264, 2141, 16961, 307, 1968, 49802, 53, 307, 709, 5044, 50690, 50690, 813, 508, 83, 7146, 13, 50772, 50772, 400, 300, 311, 437, 2314, 562, 321, 632, 3318, 257, 588, 1090, 1668, 26110, 281, 341, 1359, 28872, 13, 51076, 51076, 400, 754, 1673, 321, 600, 445, 1612, 12577, 293, 21977, 11, 309, 4523, 484, 294, 512, 3331, 309, 311, 1944, 281, 51387, 51387, 16561, 362, 1090, 12577, 293, 362, 1090, 21977, 13, 51620, 51620, 509, 1582, 380, 536, 341, 1051, 300, 709, 337, 8213, 24590, 11, 457, 309, 4523, 484, 300, 498, 291, 434, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.10078468672726132, "compression_ratio": 1.6577946768060836, "no_speech_prob": 2.9479872409865493e-06}, {"id": 109, "seek": 50348, "start": 528.6, "end": 533.0, "text": " You won't see this happen that much for linear regression, but it turns out that if you're", "tokens": [50364, 400, 2673, 508, 83, 7146, 486, 312, 1238, 2295, 11, 457, 264, 2141, 16961, 307, 1968, 49802, 53, 307, 709, 5044, 50690, 50690, 813, 508, 83, 7146, 13, 50772, 50772, 400, 300, 311, 437, 2314, 562, 321, 632, 3318, 257, 588, 1090, 1668, 26110, 281, 341, 1359, 28872, 13, 51076, 51076, 400, 754, 1673, 321, 600, 445, 1612, 12577, 293, 21977, 11, 309, 4523, 484, 294, 512, 3331, 309, 311, 1944, 281, 51387, 51387, 16561, 362, 1090, 12577, 293, 362, 1090, 21977, 13, 51620, 51620, 509, 1582, 380, 536, 341, 1051, 300, 709, 337, 8213, 24590, 11, 457, 309, 4523, 484, 300, 498, 291, 434, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.10078468672726132, "compression_ratio": 1.6577946768060836, "no_speech_prob": 2.9479872409865493e-06}, {"id": 110, "seek": 53300, "start": 533.0, "end": 536.96, "text": " training a neural network, there are some applications where unfortunately you have", "tokens": [50364, 3097, 257, 18161, 3209, 11, 456, 366, 512, 5821, 689, 7015, 291, 362, 50562, 50562, 1090, 12577, 293, 1090, 21977, 13, 50702, 50702, 400, 472, 636, 281, 5521, 300, 2590, 486, 312, 498, 508, 83, 7146, 307, 1090, 11, 370, 291, 434, 406, 884, 50988, 50988, 300, 731, 322, 264, 3097, 992, 11, 457, 754, 5324, 264, 3278, 24071, 6713, 307, 797, 754, 709, 51258, 51258, 4833, 813, 264, 3097, 992, 13, 51414, 51414, 440, 10710, 295, 1090, 12577, 293, 1090, 21977, 11, 309, 1177, 380, 534, 1051, 337, 8213, 5245, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.10038395931846217, "compression_ratio": 1.7172995780590716, "no_speech_prob": 3.6898242683491844e-07}, {"id": 111, "seek": 53300, "start": 536.96, "end": 539.76, "text": " high bias and high variance.", "tokens": [50364, 3097, 257, 18161, 3209, 11, 456, 366, 512, 5821, 689, 7015, 291, 362, 50562, 50562, 1090, 12577, 293, 1090, 21977, 13, 50702, 50702, 400, 472, 636, 281, 5521, 300, 2590, 486, 312, 498, 508, 83, 7146, 307, 1090, 11, 370, 291, 434, 406, 884, 50988, 50988, 300, 731, 322, 264, 3097, 992, 11, 457, 754, 5324, 264, 3278, 24071, 6713, 307, 797, 754, 709, 51258, 51258, 4833, 813, 264, 3097, 992, 13, 51414, 51414, 440, 10710, 295, 1090, 12577, 293, 1090, 21977, 11, 309, 1177, 380, 534, 1051, 337, 8213, 5245, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.10038395931846217, "compression_ratio": 1.7172995780590716, "no_speech_prob": 3.6898242683491844e-07}, {"id": 112, "seek": 53300, "start": 539.76, "end": 545.48, "text": " And one way to recognize that situation will be if Jtrain is high, so you're not doing", "tokens": [50364, 3097, 257, 18161, 3209, 11, 456, 366, 512, 5821, 689, 7015, 291, 362, 50562, 50562, 1090, 12577, 293, 1090, 21977, 13, 50702, 50702, 400, 472, 636, 281, 5521, 300, 2590, 486, 312, 498, 508, 83, 7146, 307, 1090, 11, 370, 291, 434, 406, 884, 50988, 50988, 300, 731, 322, 264, 3097, 992, 11, 457, 754, 5324, 264, 3278, 24071, 6713, 307, 797, 754, 709, 51258, 51258, 4833, 813, 264, 3097, 992, 13, 51414, 51414, 440, 10710, 295, 1090, 12577, 293, 1090, 21977, 11, 309, 1177, 380, 534, 1051, 337, 8213, 5245, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.10038395931846217, "compression_ratio": 1.7172995780590716, "no_speech_prob": 3.6898242683491844e-07}, {"id": 113, "seek": 53300, "start": 545.48, "end": 550.88, "text": " that well on the training set, but even worse the cross validation error is again even much", "tokens": [50364, 3097, 257, 18161, 3209, 11, 456, 366, 512, 5821, 689, 7015, 291, 362, 50562, 50562, 1090, 12577, 293, 1090, 21977, 13, 50702, 50702, 400, 472, 636, 281, 5521, 300, 2590, 486, 312, 498, 508, 83, 7146, 307, 1090, 11, 370, 291, 434, 406, 884, 50988, 50988, 300, 731, 322, 264, 3097, 992, 11, 457, 754, 5324, 264, 3278, 24071, 6713, 307, 797, 754, 709, 51258, 51258, 4833, 813, 264, 3097, 992, 13, 51414, 51414, 440, 10710, 295, 1090, 12577, 293, 1090, 21977, 11, 309, 1177, 380, 534, 1051, 337, 8213, 5245, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.10038395931846217, "compression_ratio": 1.7172995780590716, "no_speech_prob": 3.6898242683491844e-07}, {"id": 114, "seek": 53300, "start": 550.88, "end": 554.0, "text": " larger than the training set.", "tokens": [50364, 3097, 257, 18161, 3209, 11, 456, 366, 512, 5821, 689, 7015, 291, 362, 50562, 50562, 1090, 12577, 293, 1090, 21977, 13, 50702, 50702, 400, 472, 636, 281, 5521, 300, 2590, 486, 312, 498, 508, 83, 7146, 307, 1090, 11, 370, 291, 434, 406, 884, 50988, 50988, 300, 731, 322, 264, 3097, 992, 11, 457, 754, 5324, 264, 3278, 24071, 6713, 307, 797, 754, 709, 51258, 51258, 4833, 813, 264, 3097, 992, 13, 51414, 51414, 440, 10710, 295, 1090, 12577, 293, 1090, 21977, 11, 309, 1177, 380, 534, 1051, 337, 8213, 5245, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.10038395931846217, "compression_ratio": 1.7172995780590716, "no_speech_prob": 3.6898242683491844e-07}, {"id": 115, "seek": 53300, "start": 554.0, "end": 559.16, "text": " The notion of high bias and high variance, it doesn't really happen for linear models", "tokens": [50364, 3097, 257, 18161, 3209, 11, 456, 366, 512, 5821, 689, 7015, 291, 362, 50562, 50562, 1090, 12577, 293, 1090, 21977, 13, 50702, 50702, 400, 472, 636, 281, 5521, 300, 2590, 486, 312, 498, 508, 83, 7146, 307, 1090, 11, 370, 291, 434, 406, 884, 50988, 50988, 300, 731, 322, 264, 3097, 992, 11, 457, 754, 5324, 264, 3278, 24071, 6713, 307, 797, 754, 709, 51258, 51258, 4833, 813, 264, 3097, 992, 13, 51414, 51414, 440, 10710, 295, 1090, 12577, 293, 1090, 21977, 11, 309, 1177, 380, 534, 1051, 337, 8213, 5245, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.10038395931846217, "compression_ratio": 1.7172995780590716, "no_speech_prob": 3.6898242683491844e-07}, {"id": 116, "seek": 55916, "start": 559.16, "end": 566.0, "text": " applied to 1D, but to give intuition about what it looks like, it would be as if for", "tokens": [50364, 6456, 281, 502, 35, 11, 457, 281, 976, 24002, 466, 437, 309, 1542, 411, 11, 309, 576, 312, 382, 498, 337, 50706, 50706, 644, 295, 264, 4846, 291, 632, 257, 588, 6179, 2316, 300, 670, 6845, 11, 370, 309, 670, 13979, 281, 644, 51078, 51078, 295, 264, 4846, 13, 51134, 51134, 583, 550, 337, 512, 1778, 337, 661, 3166, 295, 264, 4846, 11, 309, 1177, 380, 754, 3318, 264, 3097, 51390, 51390, 1412, 731, 11, 293, 370, 309, 833, 13979, 337, 644, 295, 264, 4846, 13, 51582, 51582, 682, 341, 1365, 11, 597, 1542, 11677, 570, 309, 311, 257, 2167, 4111, 4846, 11, 321, 3318, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.1285744927146218, "compression_ratio": 1.7627118644067796, "no_speech_prob": 6.375432803906733e-07}, {"id": 117, "seek": 55916, "start": 566.0, "end": 573.4399999999999, "text": " part of the input you had a very complicated model that overfit, so it overfits to part", "tokens": [50364, 6456, 281, 502, 35, 11, 457, 281, 976, 24002, 466, 437, 309, 1542, 411, 11, 309, 576, 312, 382, 498, 337, 50706, 50706, 644, 295, 264, 4846, 291, 632, 257, 588, 6179, 2316, 300, 670, 6845, 11, 370, 309, 670, 13979, 281, 644, 51078, 51078, 295, 264, 4846, 13, 51134, 51134, 583, 550, 337, 512, 1778, 337, 661, 3166, 295, 264, 4846, 11, 309, 1177, 380, 754, 3318, 264, 3097, 51390, 51390, 1412, 731, 11, 293, 370, 309, 833, 13979, 337, 644, 295, 264, 4846, 13, 51582, 51582, 682, 341, 1365, 11, 597, 1542, 11677, 570, 309, 311, 257, 2167, 4111, 4846, 11, 321, 3318, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.1285744927146218, "compression_ratio": 1.7627118644067796, "no_speech_prob": 6.375432803906733e-07}, {"id": 118, "seek": 55916, "start": 573.4399999999999, "end": 574.56, "text": " of the input.", "tokens": [50364, 6456, 281, 502, 35, 11, 457, 281, 976, 24002, 466, 437, 309, 1542, 411, 11, 309, 576, 312, 382, 498, 337, 50706, 50706, 644, 295, 264, 4846, 291, 632, 257, 588, 6179, 2316, 300, 670, 6845, 11, 370, 309, 670, 13979, 281, 644, 51078, 51078, 295, 264, 4846, 13, 51134, 51134, 583, 550, 337, 512, 1778, 337, 661, 3166, 295, 264, 4846, 11, 309, 1177, 380, 754, 3318, 264, 3097, 51390, 51390, 1412, 731, 11, 293, 370, 309, 833, 13979, 337, 644, 295, 264, 4846, 13, 51582, 51582, 682, 341, 1365, 11, 597, 1542, 11677, 570, 309, 311, 257, 2167, 4111, 4846, 11, 321, 3318, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.1285744927146218, "compression_ratio": 1.7627118644067796, "no_speech_prob": 6.375432803906733e-07}, {"id": 119, "seek": 55916, "start": 574.56, "end": 579.68, "text": " But then for some reason for other parts of the input, it doesn't even fit the training", "tokens": [50364, 6456, 281, 502, 35, 11, 457, 281, 976, 24002, 466, 437, 309, 1542, 411, 11, 309, 576, 312, 382, 498, 337, 50706, 50706, 644, 295, 264, 4846, 291, 632, 257, 588, 6179, 2316, 300, 670, 6845, 11, 370, 309, 670, 13979, 281, 644, 51078, 51078, 295, 264, 4846, 13, 51134, 51134, 583, 550, 337, 512, 1778, 337, 661, 3166, 295, 264, 4846, 11, 309, 1177, 380, 754, 3318, 264, 3097, 51390, 51390, 1412, 731, 11, 293, 370, 309, 833, 13979, 337, 644, 295, 264, 4846, 13, 51582, 51582, 682, 341, 1365, 11, 597, 1542, 11677, 570, 309, 311, 257, 2167, 4111, 4846, 11, 321, 3318, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.1285744927146218, "compression_ratio": 1.7627118644067796, "no_speech_prob": 6.375432803906733e-07}, {"id": 120, "seek": 55916, "start": 579.68, "end": 583.52, "text": " data well, and so it underfits for part of the input.", "tokens": [50364, 6456, 281, 502, 35, 11, 457, 281, 976, 24002, 466, 437, 309, 1542, 411, 11, 309, 576, 312, 382, 498, 337, 50706, 50706, 644, 295, 264, 4846, 291, 632, 257, 588, 6179, 2316, 300, 670, 6845, 11, 370, 309, 670, 13979, 281, 644, 51078, 51078, 295, 264, 4846, 13, 51134, 51134, 583, 550, 337, 512, 1778, 337, 661, 3166, 295, 264, 4846, 11, 309, 1177, 380, 754, 3318, 264, 3097, 51390, 51390, 1412, 731, 11, 293, 370, 309, 833, 13979, 337, 644, 295, 264, 4846, 13, 51582, 51582, 682, 341, 1365, 11, 597, 1542, 11677, 570, 309, 311, 257, 2167, 4111, 4846, 11, 321, 3318, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.1285744927146218, "compression_ratio": 1.7627118644067796, "no_speech_prob": 6.375432803906733e-07}, {"id": 121, "seek": 55916, "start": 583.52, "end": 589.04, "text": " In this example, which looks artificial because it's a single feature input, we fit the", "tokens": [50364, 6456, 281, 502, 35, 11, 457, 281, 976, 24002, 466, 437, 309, 1542, 411, 11, 309, 576, 312, 382, 498, 337, 50706, 50706, 644, 295, 264, 4846, 291, 632, 257, 588, 6179, 2316, 300, 670, 6845, 11, 370, 309, 670, 13979, 281, 644, 51078, 51078, 295, 264, 4846, 13, 51134, 51134, 583, 550, 337, 512, 1778, 337, 661, 3166, 295, 264, 4846, 11, 309, 1177, 380, 754, 3318, 264, 3097, 51390, 51390, 1412, 731, 11, 293, 370, 309, 833, 13979, 337, 644, 295, 264, 4846, 13, 51582, 51582, 682, 341, 1365, 11, 597, 1542, 11677, 570, 309, 311, 257, 2167, 4111, 4846, 11, 321, 3318, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.1285744927146218, "compression_ratio": 1.7627118644067796, "no_speech_prob": 6.375432803906733e-07}, {"id": 122, "seek": 58904, "start": 589.04, "end": 594.1999999999999, "text": " training set really well and we overfit in part of the input, and we don't even fit the", "tokens": [50364, 3097, 992, 534, 731, 293, 321, 670, 6845, 294, 644, 295, 264, 4846, 11, 293, 321, 500, 380, 754, 3318, 264, 50622, 50622, 3097, 1412, 731, 293, 321, 833, 6845, 294, 644, 295, 264, 4846, 13, 50798, 50798, 400, 300, 311, 577, 294, 512, 5821, 291, 393, 7015, 917, 493, 365, 1293, 1090, 12577, 293, 51042, 51042, 1090, 21977, 13, 51176, 51176, 400, 264, 16961, 337, 300, 486, 312, 498, 264, 9284, 775, 22271, 322, 264, 3097, 992, 51364, 51364, 293, 309, 754, 775, 709, 5324, 813, 322, 264, 3097, 992, 13, 51526, 51526, 1171, 881, 2539, 5821, 11, 291, 1391, 362, 10029, 257, 1090, 12577, 420, 257, 1090, 21977, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.10992521152161715, "compression_ratio": 1.962962962962963, "no_speech_prob": 4.0524392375118623e-07}, {"id": 123, "seek": 58904, "start": 594.1999999999999, "end": 597.7199999999999, "text": " training data well and we underfit in part of the input.", "tokens": [50364, 3097, 992, 534, 731, 293, 321, 670, 6845, 294, 644, 295, 264, 4846, 11, 293, 321, 500, 380, 754, 3318, 264, 50622, 50622, 3097, 1412, 731, 293, 321, 833, 6845, 294, 644, 295, 264, 4846, 13, 50798, 50798, 400, 300, 311, 577, 294, 512, 5821, 291, 393, 7015, 917, 493, 365, 1293, 1090, 12577, 293, 51042, 51042, 1090, 21977, 13, 51176, 51176, 400, 264, 16961, 337, 300, 486, 312, 498, 264, 9284, 775, 22271, 322, 264, 3097, 992, 51364, 51364, 293, 309, 754, 775, 709, 5324, 813, 322, 264, 3097, 992, 13, 51526, 51526, 1171, 881, 2539, 5821, 11, 291, 1391, 362, 10029, 257, 1090, 12577, 420, 257, 1090, 21977, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.10992521152161715, "compression_ratio": 1.962962962962963, "no_speech_prob": 4.0524392375118623e-07}, {"id": 124, "seek": 58904, "start": 597.7199999999999, "end": 602.5999999999999, "text": " And that's how in some applications you can unfortunately end up with both high bias and", "tokens": [50364, 3097, 992, 534, 731, 293, 321, 670, 6845, 294, 644, 295, 264, 4846, 11, 293, 321, 500, 380, 754, 3318, 264, 50622, 50622, 3097, 1412, 731, 293, 321, 833, 6845, 294, 644, 295, 264, 4846, 13, 50798, 50798, 400, 300, 311, 577, 294, 512, 5821, 291, 393, 7015, 917, 493, 365, 1293, 1090, 12577, 293, 51042, 51042, 1090, 21977, 13, 51176, 51176, 400, 264, 16961, 337, 300, 486, 312, 498, 264, 9284, 775, 22271, 322, 264, 3097, 992, 51364, 51364, 293, 309, 754, 775, 709, 5324, 813, 322, 264, 3097, 992, 13, 51526, 51526, 1171, 881, 2539, 5821, 11, 291, 1391, 362, 10029, 257, 1090, 12577, 420, 257, 1090, 21977, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.10992521152161715, "compression_ratio": 1.962962962962963, "no_speech_prob": 4.0524392375118623e-07}, {"id": 125, "seek": 58904, "start": 602.5999999999999, "end": 605.28, "text": " high variance.", "tokens": [50364, 3097, 992, 534, 731, 293, 321, 670, 6845, 294, 644, 295, 264, 4846, 11, 293, 321, 500, 380, 754, 3318, 264, 50622, 50622, 3097, 1412, 731, 293, 321, 833, 6845, 294, 644, 295, 264, 4846, 13, 50798, 50798, 400, 300, 311, 577, 294, 512, 5821, 291, 393, 7015, 917, 493, 365, 1293, 1090, 12577, 293, 51042, 51042, 1090, 21977, 13, 51176, 51176, 400, 264, 16961, 337, 300, 486, 312, 498, 264, 9284, 775, 22271, 322, 264, 3097, 992, 51364, 51364, 293, 309, 754, 775, 709, 5324, 813, 322, 264, 3097, 992, 13, 51526, 51526, 1171, 881, 2539, 5821, 11, 291, 1391, 362, 10029, 257, 1090, 12577, 420, 257, 1090, 21977, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.10992521152161715, "compression_ratio": 1.962962962962963, "no_speech_prob": 4.0524392375118623e-07}, {"id": 126, "seek": 58904, "start": 605.28, "end": 609.04, "text": " And the indicator for that will be if the algorithm does poorly on the training set", "tokens": [50364, 3097, 992, 534, 731, 293, 321, 670, 6845, 294, 644, 295, 264, 4846, 11, 293, 321, 500, 380, 754, 3318, 264, 50622, 50622, 3097, 1412, 731, 293, 321, 833, 6845, 294, 644, 295, 264, 4846, 13, 50798, 50798, 400, 300, 311, 577, 294, 512, 5821, 291, 393, 7015, 917, 493, 365, 1293, 1090, 12577, 293, 51042, 51042, 1090, 21977, 13, 51176, 51176, 400, 264, 16961, 337, 300, 486, 312, 498, 264, 9284, 775, 22271, 322, 264, 3097, 992, 51364, 51364, 293, 309, 754, 775, 709, 5324, 813, 322, 264, 3097, 992, 13, 51526, 51526, 1171, 881, 2539, 5821, 11, 291, 1391, 362, 10029, 257, 1090, 12577, 420, 257, 1090, 21977, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.10992521152161715, "compression_ratio": 1.962962962962963, "no_speech_prob": 4.0524392375118623e-07}, {"id": 127, "seek": 58904, "start": 609.04, "end": 612.28, "text": " and it even does much worse than on the training set.", "tokens": [50364, 3097, 992, 534, 731, 293, 321, 670, 6845, 294, 644, 295, 264, 4846, 11, 293, 321, 500, 380, 754, 3318, 264, 50622, 50622, 3097, 1412, 731, 293, 321, 833, 6845, 294, 644, 295, 264, 4846, 13, 50798, 50798, 400, 300, 311, 577, 294, 512, 5821, 291, 393, 7015, 917, 493, 365, 1293, 1090, 12577, 293, 51042, 51042, 1090, 21977, 13, 51176, 51176, 400, 264, 16961, 337, 300, 486, 312, 498, 264, 9284, 775, 22271, 322, 264, 3097, 992, 51364, 51364, 293, 309, 754, 775, 709, 5324, 813, 322, 264, 3097, 992, 13, 51526, 51526, 1171, 881, 2539, 5821, 11, 291, 1391, 362, 10029, 257, 1090, 12577, 420, 257, 1090, 21977, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.10992521152161715, "compression_ratio": 1.962962962962963, "no_speech_prob": 4.0524392375118623e-07}, {"id": 128, "seek": 58904, "start": 612.28, "end": 617.7199999999999, "text": " For most learning applications, you probably have primarily a high bias or a high variance", "tokens": [50364, 3097, 992, 534, 731, 293, 321, 670, 6845, 294, 644, 295, 264, 4846, 11, 293, 321, 500, 380, 754, 3318, 264, 50622, 50622, 3097, 1412, 731, 293, 321, 833, 6845, 294, 644, 295, 264, 4846, 13, 50798, 50798, 400, 300, 311, 577, 294, 512, 5821, 291, 393, 7015, 917, 493, 365, 1293, 1090, 12577, 293, 51042, 51042, 1090, 21977, 13, 51176, 51176, 400, 264, 16961, 337, 300, 486, 312, 498, 264, 9284, 775, 22271, 322, 264, 3097, 992, 51364, 51364, 293, 309, 754, 775, 709, 5324, 813, 322, 264, 3097, 992, 13, 51526, 51526, 1171, 881, 2539, 5821, 11, 291, 1391, 362, 10029, 257, 1090, 12577, 420, 257, 1090, 21977, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.10992521152161715, "compression_ratio": 1.962962962962963, "no_speech_prob": 4.0524392375118623e-07}, {"id": 129, "seek": 61772, "start": 617.72, "end": 622.4, "text": " problem rather than both at the same time, but it is possible sometimes that both are", "tokens": [50364, 1154, 2831, 813, 1293, 412, 264, 912, 565, 11, 457, 309, 307, 1944, 2171, 300, 1293, 366, 50598, 50598, 264, 912, 565, 13, 50672, 50672, 407, 286, 458, 300, 456, 311, 257, 688, 281, 1399, 11, 456, 366, 257, 688, 295, 10392, 322, 264, 9788, 11, 457, 264, 50880, 50880, 2141, 45584, 366, 1090, 12577, 1355, 309, 311, 406, 754, 884, 731, 322, 264, 3097, 992, 11, 293, 1090, 51202, 51202, 21977, 1355, 309, 775, 709, 5324, 322, 264, 3278, 12, 3337, 327, 399, 992, 813, 264, 3097, 992, 13, 51524, 51524, 14159, 286, 478, 3097, 257, 3479, 2539, 9284, 11, 286, 486, 1920, 1009, 853, 281, 2573, 484, 281, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.10219379056964004, "compression_ratio": 1.776061776061776, "no_speech_prob": 2.332026497242623e-06}, {"id": 130, "seek": 61772, "start": 622.4, "end": 623.88, "text": " the same time.", "tokens": [50364, 1154, 2831, 813, 1293, 412, 264, 912, 565, 11, 457, 309, 307, 1944, 2171, 300, 1293, 366, 50598, 50598, 264, 912, 565, 13, 50672, 50672, 407, 286, 458, 300, 456, 311, 257, 688, 281, 1399, 11, 456, 366, 257, 688, 295, 10392, 322, 264, 9788, 11, 457, 264, 50880, 50880, 2141, 45584, 366, 1090, 12577, 1355, 309, 311, 406, 754, 884, 731, 322, 264, 3097, 992, 11, 293, 1090, 51202, 51202, 21977, 1355, 309, 775, 709, 5324, 322, 264, 3278, 12, 3337, 327, 399, 992, 813, 264, 3097, 992, 13, 51524, 51524, 14159, 286, 478, 3097, 257, 3479, 2539, 9284, 11, 286, 486, 1920, 1009, 853, 281, 2573, 484, 281, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.10219379056964004, "compression_ratio": 1.776061776061776, "no_speech_prob": 2.332026497242623e-06}, {"id": 131, "seek": 61772, "start": 623.88, "end": 628.0400000000001, "text": " So I know that there's a lot to process, there are a lot of concepts on the slides, but the", "tokens": [50364, 1154, 2831, 813, 1293, 412, 264, 912, 565, 11, 457, 309, 307, 1944, 2171, 300, 1293, 366, 50598, 50598, 264, 912, 565, 13, 50672, 50672, 407, 286, 458, 300, 456, 311, 257, 688, 281, 1399, 11, 456, 366, 257, 688, 295, 10392, 322, 264, 9788, 11, 457, 264, 50880, 50880, 2141, 45584, 366, 1090, 12577, 1355, 309, 311, 406, 754, 884, 731, 322, 264, 3097, 992, 11, 293, 1090, 51202, 51202, 21977, 1355, 309, 775, 709, 5324, 322, 264, 3278, 12, 3337, 327, 399, 992, 813, 264, 3097, 992, 13, 51524, 51524, 14159, 286, 478, 3097, 257, 3479, 2539, 9284, 11, 286, 486, 1920, 1009, 853, 281, 2573, 484, 281, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.10219379056964004, "compression_ratio": 1.776061776061776, "no_speech_prob": 2.332026497242623e-06}, {"id": 132, "seek": 61772, "start": 628.0400000000001, "end": 634.48, "text": " key takeaways are high bias means it's not even doing well on the training set, and high", "tokens": [50364, 1154, 2831, 813, 1293, 412, 264, 912, 565, 11, 457, 309, 307, 1944, 2171, 300, 1293, 366, 50598, 50598, 264, 912, 565, 13, 50672, 50672, 407, 286, 458, 300, 456, 311, 257, 688, 281, 1399, 11, 456, 366, 257, 688, 295, 10392, 322, 264, 9788, 11, 457, 264, 50880, 50880, 2141, 45584, 366, 1090, 12577, 1355, 309, 311, 406, 754, 884, 731, 322, 264, 3097, 992, 11, 293, 1090, 51202, 51202, 21977, 1355, 309, 775, 709, 5324, 322, 264, 3278, 12, 3337, 327, 399, 992, 813, 264, 3097, 992, 13, 51524, 51524, 14159, 286, 478, 3097, 257, 3479, 2539, 9284, 11, 286, 486, 1920, 1009, 853, 281, 2573, 484, 281, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.10219379056964004, "compression_ratio": 1.776061776061776, "no_speech_prob": 2.332026497242623e-06}, {"id": 133, "seek": 61772, "start": 634.48, "end": 640.9200000000001, "text": " variance means it does much worse on the cross-validation set than the training set.", "tokens": [50364, 1154, 2831, 813, 1293, 412, 264, 912, 565, 11, 457, 309, 307, 1944, 2171, 300, 1293, 366, 50598, 50598, 264, 912, 565, 13, 50672, 50672, 407, 286, 458, 300, 456, 311, 257, 688, 281, 1399, 11, 456, 366, 257, 688, 295, 10392, 322, 264, 9788, 11, 457, 264, 50880, 50880, 2141, 45584, 366, 1090, 12577, 1355, 309, 311, 406, 754, 884, 731, 322, 264, 3097, 992, 11, 293, 1090, 51202, 51202, 21977, 1355, 309, 775, 709, 5324, 322, 264, 3278, 12, 3337, 327, 399, 992, 813, 264, 3097, 992, 13, 51524, 51524, 14159, 286, 478, 3097, 257, 3479, 2539, 9284, 11, 286, 486, 1920, 1009, 853, 281, 2573, 484, 281, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.10219379056964004, "compression_ratio": 1.776061776061776, "no_speech_prob": 2.332026497242623e-06}, {"id": 134, "seek": 61772, "start": 640.9200000000001, "end": 645.44, "text": " Whenever I'm training a machine learning algorithm, I will almost always try to figure out to", "tokens": [50364, 1154, 2831, 813, 1293, 412, 264, 912, 565, 11, 457, 309, 307, 1944, 2171, 300, 1293, 366, 50598, 50598, 264, 912, 565, 13, 50672, 50672, 407, 286, 458, 300, 456, 311, 257, 688, 281, 1399, 11, 456, 366, 257, 688, 295, 10392, 322, 264, 9788, 11, 457, 264, 50880, 50880, 2141, 45584, 366, 1090, 12577, 1355, 309, 311, 406, 754, 884, 731, 322, 264, 3097, 992, 11, 293, 1090, 51202, 51202, 21977, 1355, 309, 775, 709, 5324, 322, 264, 3278, 12, 3337, 327, 399, 992, 813, 264, 3097, 992, 13, 51524, 51524, 14159, 286, 478, 3097, 257, 3479, 2539, 9284, 11, 286, 486, 1920, 1009, 853, 281, 2573, 484, 281, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.10219379056964004, "compression_ratio": 1.776061776061776, "no_speech_prob": 2.332026497242623e-06}, {"id": 135, "seek": 64544, "start": 645.44, "end": 651.4000000000001, "text": " what extent the algorithm has a high bias or underfitting versus a high variance or", "tokens": [50364, 437, 8396, 264, 9284, 575, 257, 1090, 12577, 420, 833, 69, 2414, 5717, 257, 1090, 21977, 420, 50662, 50662, 364, 670, 69, 2414, 1154, 13, 50736, 50736, 400, 341, 486, 976, 665, 10056, 11, 382, 321, 603, 536, 1780, 341, 1243, 11, 322, 577, 291, 393, 3470, 50952, 50952, 264, 3389, 295, 264, 9284, 13, 51066, 51066, 583, 700, 11, 718, 311, 747, 257, 574, 412, 577, 3890, 2144, 11807, 264, 12577, 293, 21977, 295, 257, 2539, 51316, 51316, 9284, 11, 570, 300, 486, 854, 291, 1101, 1223, 562, 291, 820, 764, 3890, 2144, 13, 51588, 51588, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51668], "temperature": 0.0, "avg_logprob": -0.13662918976375035, "compression_ratio": 1.796875, "no_speech_prob": 1.4499416465696413e-05}, {"id": 136, "seek": 64544, "start": 651.4000000000001, "end": 652.8800000000001, "text": " an overfitting problem.", "tokens": [50364, 437, 8396, 264, 9284, 575, 257, 1090, 12577, 420, 833, 69, 2414, 5717, 257, 1090, 21977, 420, 50662, 50662, 364, 670, 69, 2414, 1154, 13, 50736, 50736, 400, 341, 486, 976, 665, 10056, 11, 382, 321, 603, 536, 1780, 341, 1243, 11, 322, 577, 291, 393, 3470, 50952, 50952, 264, 3389, 295, 264, 9284, 13, 51066, 51066, 583, 700, 11, 718, 311, 747, 257, 574, 412, 577, 3890, 2144, 11807, 264, 12577, 293, 21977, 295, 257, 2539, 51316, 51316, 9284, 11, 570, 300, 486, 854, 291, 1101, 1223, 562, 291, 820, 764, 3890, 2144, 13, 51588, 51588, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51668], "temperature": 0.0, "avg_logprob": -0.13662918976375035, "compression_ratio": 1.796875, "no_speech_prob": 1.4499416465696413e-05}, {"id": 137, "seek": 64544, "start": 652.8800000000001, "end": 657.2, "text": " And this will give good guidance, as we'll see later this week, on how you can improve", "tokens": [50364, 437, 8396, 264, 9284, 575, 257, 1090, 12577, 420, 833, 69, 2414, 5717, 257, 1090, 21977, 420, 50662, 50662, 364, 670, 69, 2414, 1154, 13, 50736, 50736, 400, 341, 486, 976, 665, 10056, 11, 382, 321, 603, 536, 1780, 341, 1243, 11, 322, 577, 291, 393, 3470, 50952, 50952, 264, 3389, 295, 264, 9284, 13, 51066, 51066, 583, 700, 11, 718, 311, 747, 257, 574, 412, 577, 3890, 2144, 11807, 264, 12577, 293, 21977, 295, 257, 2539, 51316, 51316, 9284, 11, 570, 300, 486, 854, 291, 1101, 1223, 562, 291, 820, 764, 3890, 2144, 13, 51588, 51588, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51668], "temperature": 0.0, "avg_logprob": -0.13662918976375035, "compression_ratio": 1.796875, "no_speech_prob": 1.4499416465696413e-05}, {"id": 138, "seek": 64544, "start": 657.2, "end": 659.48, "text": " the performance of the algorithm.", "tokens": [50364, 437, 8396, 264, 9284, 575, 257, 1090, 12577, 420, 833, 69, 2414, 5717, 257, 1090, 21977, 420, 50662, 50662, 364, 670, 69, 2414, 1154, 13, 50736, 50736, 400, 341, 486, 976, 665, 10056, 11, 382, 321, 603, 536, 1780, 341, 1243, 11, 322, 577, 291, 393, 3470, 50952, 50952, 264, 3389, 295, 264, 9284, 13, 51066, 51066, 583, 700, 11, 718, 311, 747, 257, 574, 412, 577, 3890, 2144, 11807, 264, 12577, 293, 21977, 295, 257, 2539, 51316, 51316, 9284, 11, 570, 300, 486, 854, 291, 1101, 1223, 562, 291, 820, 764, 3890, 2144, 13, 51588, 51588, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51668], "temperature": 0.0, "avg_logprob": -0.13662918976375035, "compression_ratio": 1.796875, "no_speech_prob": 1.4499416465696413e-05}, {"id": 139, "seek": 64544, "start": 659.48, "end": 664.48, "text": " But first, let's take a look at how regularization affects the bias and variance of a learning", "tokens": [50364, 437, 8396, 264, 9284, 575, 257, 1090, 12577, 420, 833, 69, 2414, 5717, 257, 1090, 21977, 420, 50662, 50662, 364, 670, 69, 2414, 1154, 13, 50736, 50736, 400, 341, 486, 976, 665, 10056, 11, 382, 321, 603, 536, 1780, 341, 1243, 11, 322, 577, 291, 393, 3470, 50952, 50952, 264, 3389, 295, 264, 9284, 13, 51066, 51066, 583, 700, 11, 718, 311, 747, 257, 574, 412, 577, 3890, 2144, 11807, 264, 12577, 293, 21977, 295, 257, 2539, 51316, 51316, 9284, 11, 570, 300, 486, 854, 291, 1101, 1223, 562, 291, 820, 764, 3890, 2144, 13, 51588, 51588, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51668], "temperature": 0.0, "avg_logprob": -0.13662918976375035, "compression_ratio": 1.796875, "no_speech_prob": 1.4499416465696413e-05}, {"id": 140, "seek": 64544, "start": 664.48, "end": 669.9200000000001, "text": " algorithm, because that will help you better understand when you should use regularization.", "tokens": [50364, 437, 8396, 264, 9284, 575, 257, 1090, 12577, 420, 833, 69, 2414, 5717, 257, 1090, 21977, 420, 50662, 50662, 364, 670, 69, 2414, 1154, 13, 50736, 50736, 400, 341, 486, 976, 665, 10056, 11, 382, 321, 603, 536, 1780, 341, 1243, 11, 322, 577, 291, 393, 3470, 50952, 50952, 264, 3389, 295, 264, 9284, 13, 51066, 51066, 583, 700, 11, 718, 311, 747, 257, 574, 412, 577, 3890, 2144, 11807, 264, 12577, 293, 21977, 295, 257, 2539, 51316, 51316, 9284, 11, 570, 300, 486, 854, 291, 1101, 1223, 562, 291, 820, 764, 3890, 2144, 13, 51588, 51588, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51668], "temperature": 0.0, "avg_logprob": -0.13662918976375035, "compression_ratio": 1.796875, "no_speech_prob": 1.4499416465696413e-05}, {"id": 141, "seek": 66992, "start": 669.92, "end": 676.92, "text": " Let's take a look at that in the next video.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 50714], "temperature": 0.0, "avg_logprob": -0.33309335708618165, "compression_ratio": 0.8979591836734694, "no_speech_prob": 0.00046307785669341683}], "language": "en", "video_id": "YB61HDL7EzE", "entity": "ML Specialization, Andrew Ng (2022)"}}