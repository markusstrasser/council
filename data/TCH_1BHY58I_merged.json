{"video_id": "TCH_1BHY58I", "title": "Building makemore Part 2: MLP", "description": "We implement a multilayer perceptron (MLP) character-level language model. In this video we also introduce many basics of machine learning (e.g. model training, learning rate tuning, hyperparameters, evaluation, train/dev/test splits, under/overfitting, etc.).\n\nLinks:\n- makemore on github: https://github.com/karpathy/makemore\n- jupyter notebook I built in this video: https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part2_mlp.ipynb\n- collab notebook (new)!!!: https://colab.research.google.com/drive/1YIfmkftLrz6MPTOO9Vwqrop2Q5llHIGK?usp=sharing\n- Bengio et al. 2003 MLP language model paper (pdf): https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n- my website: https://karpathy.ai\n- my twitter: https://twitter.com/karpathy\n- (new) Neural Networks: Zero to Hero series Discord channel: https://discord.gg/Hp2m3kheJn , for people who'd like to chat more and go beyond youtube comments\n\nUseful links:\n- PyTorch internals ref http://blog.ezyang.com/2019/05/pytorch-internals/\n\nExercises:\n- E01: Tune the hyperparameters of the training to beat my best validation loss of 2.2\n- E02: I was not careful with the intialization of the network in this video. (1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? (2) Can you tune the initialization to get a starting loss that is much more similar to (1)?\n- E03: Read the Bengio et al 2003 paper (link above), implement and try any idea from the paper. Did it work?\n\nChapters:\n00:00:00 intro\n00:01:48 Bengio et al. 2003 (MLP language model) paper walkthrough\n00:09:03 (re-)building our training dataset\n00:12:19 implementing the embedding lookup table\n00:18:35 implementing the hidden layer + internals of torch.Tensor: storage, views\n00:29:15 implementing the output layer\n00:29:53 implementing the negative log likelihood loss\n00:32:17 summary of the full network\n00:32:49 introducing F.cross_entropy and why\n00:37:56 implementing the training loop, overfitting one batch\n00:41:25 training on the full dataset, minibatches\n00:45:40 finding a good initial learning rate\n00:53:20 splitting up the dataset into train/val/test splits and why\n01:00:49 experiment: larger hidden layer\n01:05:27 visualizing the character embeddings\n01:07:16 experiment: larger embedding size\n01:11:46 summary of our final code, conclusion\n01:13:24 sampling from the model\n01:14:55 google collab (new!!) notebook advertisement", "author": "Andrej Karpathy", "keywords": ["deep learning", "neural network", "multilayer perceptron", "nlp", "language model"], "channel_url": "https://www.youtube.com/channel/UCXUPKJO5MZQN11PqgIvyuvQ", "length": 4539, "views": 30237, "publish_date": "11/02/2022", "timestamp": 1662940800, "entity": "Andrew Kaparthy", "transcript": {"text": " Hi everyone. Today we are continuing our implementation of Make More. Now in the last lecture we implemented the bigram language model and we implemented it both using counts and also using a super simple neural network that has a single linear layer. Now this is the Jupyter Notebook that we built out last lecture and we saw that the way we approached this is that we looked at only the single previous character and we predicted the distribution for the character that would go next in the sequence and we did that by taking counts and normalizing them into probabilities so that each row here sums to one. Now this is all well and good if you only have one character of previous context and this works and it's approachable. The problem with this model of course is that the predictions from this model are not very good because you only take one character of context so the model didn't produce very name-like sounding things. Now the problem with this approach though is that if we are to take more context into account when predicting the next character in a sequence things quickly blow up and this table, the size of this table, grows and in fact it grows exponentially with the length of the context because if we only take a single character at a time that's 27 possibilities of context but if we take two characters in the past and try to predict the third one suddenly the number of rows in this matrix, you can look at it that way, is 27 times 27 so there's 729 possibilities for what could have come in the context. If we take three characters as the context suddenly we have 20,000 possibilities of context and so that's just way too many rows of this matrix it's way too few counts for each possibility and the whole thing just kind of explodes and doesn't work very well. So that's why today we're going to move on to this bullet point here and we're going to implement a multilayer perceptron model to predict the next character in a sequence and this modeling approach that we're going to adopt follows this paper Benjyotel 2003. So I have the paper pulled up here. Now this isn't the very first paper that proposed the use of multilayer perceptrons or neural networks to predict the next character or token in a sequence but it's definitely one that is was very influential around that time it is very often cited to stand in for this idea and I think it's a very nice write-up and so this is the paper that we're going to first look at and then implement. Now this paper has 19 pages so we don't have time to go into the full detail of this paper but I invite you to read it. It's very readable, interesting and has a lot of interesting ideas in it as well. In the introduction they describe the exact same problem I just described and then to address it they propose the following model. Now keep in mind that we are building a character level language model so we're working on the level of characters. In this paper they have a vocabulary of 17,000 possible words and they instead build a word level language model but we're going to still stick with the characters but we'll take the same modeling approach. Now what they do is basically they propose to take every one of these words 17,000 words and they're going to associate to each word a say 30 dimensional feature vector. So every word is now embedded into a 30 dimensional space you can think of it that way. So we have 17,000 points or vectors in a 30 dimensional space and that's you might imagine that's very crowded that's a lot of points for a very small space. Now in the beginning these words are initialized completely randomly so they're spread out at random but then we're going to tune these embeddings of these words using that propagation. So during the course of training of this neural network these points or vectors are going to basically move around in this space and you might imagine that for example words that have very similar meanings or there are indeed synonyms of each other might end up in a very similar part of the space and conversely words that mean very different things would go somewhere else in the space. Now their modeling approach otherwise is identical to ours. They are using a multilayer neural network to predict the next word given the previous words and to train the neural network they are maximizing the log likelihood of the training data just like we did. So the modeling approach itself is identical. Now here they have a concrete example of this intuition. Why does it work? Basically suppose that for example you are trying to predict a dog was running in a blank. Now suppose that the exact phrase a dog was running in a has never occurred in a training data and here you are at sort of test time later when the model is deployed somewhere and it's trying to make a sentence and it's saying a dog was running in a blank and because it's never encountered this exact phrase in the training set you're out of distribution as we say. Like you don't have fundamentally any reason to suspect what might come next but this approach actually allows you to get around that because maybe you didn't see the exact phrase a dog was running in a something but maybe you've seen similar phrases maybe you've seen the phrase the dog was running in a blank and maybe your network has learned that a and the are like frequently are interchangeable with each other and so maybe it took the embedding for a and the embedding for the and it actually put them like nearby each other in the space and so you can transfer knowledge through that embedding and you can generalize in that way. Similarly the network could know that cats and dogs are animals and they co-occur in lots of very similar contexts and so even though you haven't seen this exact phrase or if you haven't seen exactly walking or running you can through the embedding space transfer knowledge and you can generalize to novel scenarios. So let's now scroll down to the diagram of the neural network they have a nice diagram here and in this example we are taking three previous words and we are trying to predict the fourth word in a sequence. Now these three previous words as I mentioned we have a vocabulary of 17,000 possible words so every one of these basically are the index of the incoming word and because there are 17,000 words this is an integer between 0 and 16,999. Now there's also a lookup table that they call C. This lookup table is a matrix that is 17,000 by say 30 and basically what we're doing here is we're treating this as a lookup table and so every index is plucking out a row of this embedding matrix so that each index is converted to the 30 dimensional vector that corresponds to the embedding vector for that word. So here we have the input layer of 30 neurons for three words making up 90 neurons in total and here they're saying that this matrix C is shared across all the words so we're always indexing into the same matrix C over and over for each one of these words. Next up is the hidden layer of this neural network. The size of this hidden neural layer of this neural nut is a hyperparameter so we use the word hyperparameter when it's kind of like a design choice up to the designer of the neural nut and this can be as large as you'd like or as small as you'd like so for example the size could be a hundred and we are going to go over multiple choices of the size of this hidden layer and we're going to evaluate how well they work. So say there were a hundred neurons here all of them would be fully connected to the 90 words or 90 numbers that make up these three words. So this is a fully connected layer then there's a 10-inch long linearity and then there's this output layer and because there are 17,000 possible words that could come next this layer has 17,000 neurons and all of them are fully connected to all of these neurons in the hidden layer. So there's a lot of parameters here because there's a lot of words so most computation is here this is the expensive layer. Now there are 17,000 logits here so on top of there we have the softmax layer which we've seen in our previous video as well so every one of these logits is exponentiated and then everything is normalized to sum to one so that we have a nice probability distribution for the next word in the sequence. Now of course during training we actually have the label we have the identity of the next word in a sequence that word or its index is used to pluck out the probability of that word and then we are maximizing the probability of that word with respect to the parameters of this neural net. So the parameters are the weights and biases of this output layer, the weights and biases of the hidden layer and the embedding lookup table C and all of that is optimized using back propagation and these dashed arrows ignore those that represents a variation of a neural net that we are not going to explore in this video. So that's the setup and now let's implement it. Okay so I started a brand new notebook for this lecture we are importing pytorch and we are importing matplotlib so we can create figures then I am reading all the names into a list of words like I did before and I'm showing the first eight right here. Keep in mind that we have a 32,000 in total these are just the first eight and then here I'm building out the vocabulary of characters and all the mappings from the characters as strings to integers and vice versa. Now the first thing we want to do is we want to compile the data set for the neural network and I had to rewrite this code I'll show you in a second what it looks like. So this is the code that I created for the data set creation so let me first run it and then I'll briefly explain how this works. So first we're going to define something called block size and this is basically the context length of how many characters do we take to predict the next one. So here in this example we're taking three characters to predict the fourth one so we have a block size of three that's the size of the block that supports the prediction. Then here I'm building out the X and Y the X are the input to the neural net and the Y are the labels for each example inside X. Then I'm erasing over the first five words I'm doing first five just for efficiency while we are developing all the code but then later we're going to come here and erase this so that we use the entire training set. So here I'm printing the word Emma and here I'm basically showing the examples that we can generate the five examples that we can generate out of the single sort of word Emma. So when we are given the context of just dot dot dot the first character in a sequence is E in this context the label is M when the context is this the label is M and so forth. And so the way I build this out is first I start with a padded context of just zero tokens then I iterate over all the characters I get the character in the sequence and I basically build out the array Y of this current character and the array X which stores the current running context. And then here see I print everything and here I crop the context and enter the new character in a sequence. So this is kind of like a rolling window of context. Now we can change the block size here to for example 4 and in that case we would be predicting the fifth character given the previous form or it can be 5 and then it would look like this or it can be say 10 and then it would look something like this we're taking 10 characters to predict the 11th one and we're always padding with dots. So let me bring this back to 3 just so that we have what we have here in the paper. And finally the data set right now looks as follows. From these five words we have created a data set of 32 examples and each input of the neural net is three integers and we have a label that is also an integer Y so X looks like this these are the individual examples and then Y are the labels. So given this let's now write a neural network that takes these X's and predicts the Y's. First let's build the embedding lookup table C. So we have 27 possible characters and we're going to embed them in a lower dimensional space. In the paper they have 17,000 words and they embed them in spaces as small dimensional as 30. So they cram 17,000 words into 30 dimensional space. In our case we have only 27 possible characters so let's cram them in something as small as to start with for example a two dimensional space. So this lookup table will be random numbers and we'll have 27 rows and we'll have two columns right so each 20 each one of 27 characters will have a two dimensional embedding. So that's our matrix C of embeddings in the beginning initialized randomly. Now before we embed all of the integers inside the input X using this lookup table C let me actually just try to embed a single individual integer like say 5 so we get a sense of how this works. Now one way this works of course is we can just take the C and we can index into row 5 and that gives us a vector the fit row of C and this is one way to do it. The other way that I presented in the previous lecture is actually seemingly different but actually identical. So in the previous lecture what we did is we took these integers and we used the one hot encoding to first encode them. So F dot one hot we want to encode integer 5 and we want to tell it that their number of classes is 27 so that's the 26 dimensional vector of all zeros except the fifth bit is turned on. Now this actually doesn't work the reason is that this input actually must be a torch dot tensor and I'm making some of these errors intentionally just so you get to see some errors and how to fix them. So this must be a tensor not an int fairly straightforward to fix we get a one hot vector the fifth dimension is 1 and the shape of this is 27. And now notice that just as I briefly alluded to in a previous video if we take this one hot vector and we multiply it by C then what would you expect? Well number one first you'd expect an error because expected scalar type long but found float so a little bit confusing but the problem here is that one hot the data type of it is long it's a 64-bit integer but this is a float tensor and so PyTorch doesn't know how to multiply an int with a float and that's why we had to explicitly cast this to a float so that we can multiply. Now the output actually here is identical and that it's identical because of the way the matrix multiplication here works. We have the one hot vector multiplying columns of C and because of all the zeros they actually end up masking out everything in C except for the fifth row which is plucked out and so we actually arrive at the same result and that tells you that here we can interpret this first piece here this embedding of the integer we can either think of it as the integer indexing into a lookup table C but equivalently we can also think of this little piece here as a first layer of this bigger neural net this layer here has neurons that have no non-linearity there's no tanh they're just linear neurons and their weight matrix is C and then we are encoding integers into one hot and feeding those into a neural net and this first layer basically embeds them so those are two equivalent ways of doing the same thing we're just going to index because it's much much faster and we're going to discard this interpretation of one hot inputs into neural nets and we're just going to index integers and create and use embedding tables now embedding a single integer like five is easy enough we can simply ask by torch to retrieve the fifth row of C or the row index five of C but how do we simultaneously embed all of these 32 by three integers stored in array X luckily by torch indexing is fairly flexible and quite powerful so it doesn't just work to ask for a single element five like this you can actually index using lists so for example we can get the rows five six and seven and this will just work like this we can index with a list it doesn't just have to be a list it can also be a actually tensor of integers and we can index with that so this is a integer tensor five six seven and this will just work as well in fact we can also for example repeat row seven and retrieve it multiple times and that same index will just get embedded multiple times here so here we are indexing with a one-dimensional tensor of integers but it turns out that you can also index with multi-dimensional tensors of integers here we have a two-dimensional in tensor of integers so we can simply just do C at X and this just works and the shape of this is 32 by 3 which is the original shape and now for every one of those 32 by 3 integers we've retrieved the embedding vector here so basically we have that as an example the 13th or example index 13 the second dimension is the integer 1 as an example and so here if we do C of X which gives us that array and then we index into 13 by 2 of that array then we we get the embedding here and you can verify that C at 1 which is the integer at that location is indeed equal to this you see they're equal so basically a long story short pie torch indexing is awesome and to embed simultaneously all of the integers in X we can simply do C of X and that is our embedding and that just works now let's construct this layer here the hidden layer so we have that W1 as I'll call it are these weights which we will initialize randomly now the number of inputs to this layer is going to be three times two right because we have two-dimensional embeddings and we have three of them so the number of inputs is six and the number of neurons in this layer is a variable up to us let's use a hundred neurons as an example and then biases will be also initialized randomly as an example and let's and we just need 100 of them now the problem with this is we can't simply normally we would take the input in this case that's embedding and we'd like to multiply it with these weights and then we would like to add the bias this is roughly what we want to do but the problem here is that these embeddings are stacked up in the dimensions of this input tensor so this will not work this matrix multiplication because this is a shape 32 by 3 by 2 and I can't multiply that by 6 by 100 so somehow we need to concatenate these inputs here together so that we can do something along these lines which currently does not work so how do we transform this 32 by 3 by 2 into a 32 by 6 so that we can actually perform this multiplication over here I'd like to show you that there are usually many ways of implementing what you'd like to do in torch and some of them will be faster better shorter etc and that's because torch is a very large library and it's got lots and lots of functions so if we just go to the documentation and click on torch you'll see that my slider here is very tiny and that's because there are so many functions that you can call on these tensors to transform them create them multiply them add them perform all kinds of different operations on them and so this is kind of like the space of possibility if you will now one of the things that you can do is we can control here control F for concatenate and we see that there's a function torch.cat short for concatenate and this concatenate is a given sequence of tensors in a given dimension and these sensors must have the same shape etc so we can use the concatenate operation to in a naive way concatenate these three embeddings for each input so in this case we have M of M of the shape and really what we want to do is we want to retrieve these three parts and concatenate them so we want to grab all the examples we want to grab first the zero index and then all of this so this plucks out the 32 by 2 embeddings of just the first word here and so basically we want this guy we want the first dimension and we want the second dimension and these are the three pieces individually and then we want to treat this as a sequence and we want to torch.cat on that sequence so this is the list torch.cat takes a sequence of tensors and then we have to tell it along which dimension to concatenate so in this case all these are 32 by 2 and we want to concatenate not across dimension 0 but across dimension 1 so passing in 1 gives us a result the shape of this is 32 by 6 exactly as we'd like so that basically took 32 and squashed these by concatenating them into 32 by 6 now this is kind of ugly because this code would not generalize if we want to later change the block size right now we have three inputs three words but what if we had five then here we would have to change the code because I'm indexing directly well torch comes to rescue again because that turns out to be a function called unbind and it removes a tensor dimension so it removes a tensor dimension returns a tuple of all slices along a given dimension without it so this is exactly what we need and basically when we call torch.umbind torch.umbind of M and passing dimension 1 index 1 this gives us a list of tensors exactly equivalent to this so running this gives us a lang 3 and it's exactly this list and so we can call torch.cat on it and along the first dimension and this works and this shape is the same but now this is it doesn't matter if we have block size 3 or 5 or 10 this will just work so this is one way to do it but it turns out that in this case there's actually a significantly better and more efficient way and this gives me an opportunity to hint at some of the internals of torch.tensor so let's create an array here of elements from 0 to 17 and the shape of this is just 18 it's a single vector of 18 numbers it turns out that we can very quickly we represent this as different sized and dimensional tensors we do this by calling a view and we can say that actually this is not a single vector of 18 this is a 2 by 9 tensor or alternatively this is a 9 by 2 tensor or this is actually a 3 by 3 by 2 tensor as long as the total number of elements here multiply to be the same this will just work and in pytorch this operation calling that view is extremely efficient and the reason for that is that in each tensor there's something called the underlying storage and the storage is just the numbers always as a one-dimensional vector and this is how this tensor is represented in the computer memory it's always a one-dimensional vector but when we call that view we are manipulating some of attributes of that tensor that dictate how this one-dimensional sequence is interpreted to be an n-dimensional tensor and so what's happening here is that no memory is being changed copied moved or created when we call that view the storage is identical but when you call that view some of the internal attributes of the view of this tensor are being manipulated and changed in particular that something there's something called a storage offset strides and shapes and those are manipulated so that this one-dimensional sequence of bytes is seen as different and dimensional arrays there's a blog post here from Eric called pytorch internals where he goes into some of this with respect to tensor and how the view of a tensor is represented and this is really just like a logical construct of representing the physical memory and so this is a pretty good blog post that you can go into I might also create an entire video on the internals of torch tensor and how this works for here we just note that this is an extremely efficient operation and if I delete this and come back to our M we see that the shape of our M is 32 by 3 by 2 but we can simply ask for pytorch to view this instead as a 32 by 6 and the way this gets flattened into a 32 by 6 array just happens that these two get stacked up in a single row and so that's basically the concatenation operation that we're after and you can verify that this actually gives the exact same result as what we had before so this is an element y equals and you can see that all the elements of these two tensors are the same and so we get the exact same result so long story short we can actually just come here and if we just view this as a 32 by 6 instead then this multiplication will work and give us the hidden states that we're after so if this is H then H dash shaped is now the hundred dimensional activations for every one of our 32 examples and this gives the desired result let me do two things here number one let's not use 32 we can for example do something like m dot shape at 0 so that we don't hardcode these numbers and this would work for any size of this M or alternatively we can also do negative 1 when we do negative 1 pytorch will infer what this should be because the number of elements must be the same and we're saying that this is 6 pytorch will derive that this must be 32 or whatever else it is if M is of different size the other thing is here one more thing I'd like to point out is here when we do the concatenation this actually is much less efficient because this concatenation would create a whole new tensor with a whole new storage so new memory is being created because there's no way to concatenate tensors just by manipulating the view attributes so this is inefficient and creates all kinds of new memory so let me delete this now we don't need this and here to calculate H we want to also dot 10 H of this to get our oops to get our H so these are now numbers between negative 1 and 1 because the 10 H and we have that the shape is 32 by 100 and that is basically this hidden layer of activations here for every one of our 32 examples now there's one more thing I've lost over that we have to be very careful with and that this and that's this plus here in particular we want to make sure that the broadcasting will do what we like the shape of this is 32 by 100 and the one shape is 100 so we see that the addition here will broadcast these two and in particular we have 32 by 100 broadcasting to 100 so broadcasting will align on the right create a fake dimension here so this will become a 1 by 100 row vector and then it will copy vertically for every one of these rows of 32 and do an element-wise addition so in this case the correct thing will be happening because the same bias vector will be added to all the rows of this matrix so that is correct that's what we'd like and it's always good practice just make sure so that you don't shoot yourself in the foot and finally let's create the final layer here so let's create w2 and b2 the input now is 100 and the output number of neurons will be for us 27 because we have 27 possible characters that come next so the biases will be 27 as well so therefore the logits which are the outputs of this neural net are going to be H multiplied by w2 plus b2 logis that shape is 32 by 27 and the logits look good now exactly as we saw in the previous video we want to take these logits and we want to first exponentiate them to get our fake counts and then we want to normalize them into a probability so prob is counts divide and now counts dot sum along the first dimension and keep them as true exactly as in the previous video and so prob that shape now is 32 by 27 and you'll see that every row of prob sums to one so it's normalized so that gives us the probabilities now of course we have the actual letter that comes next and that comes from this array y which we created during the data set creation so y is this last piece here which is the identity of the next character in a sequence that we'd like to now predict so what we'd like to do now is just as in the previous video we'd like to index into the rows of prob and each row we'd like to pluck out the probability assigned to the correct character as given here so first we have torched out a range of 32 which is kind of like an iterator over numbers from 0 to 31 and then we can index into prob in the following way prob in torch dot a range of 32 which iterates the roads and then in each row we'd like to grab this column as given by y so this gives the current probabilities as assigned by this neural network with this setting of its weights to the correct character in the sequence and you can see here that this looks okay for some of these characters like this is basically 0.2 but it doesn't look very good at all for many other characters like this is 0.07 0s 1 probability and so the network thinks that some of these are extremely unlikely but of course we haven't trained a neural network yet so this will improve and ideally all of these numbers here of course are 1 because then we are correctly predicting the next character now just as in the previous video we want to take these probabilities we want to look at the lock probability and then we want to look at the average lock probability and the negative of it to create the negative log likelihood loss so the loss here is 17 and this is the loss that we'd like to minimize to get the network to predict the correct character in the sequence okay so I rewrote everything here and made it a bit more respectable so here's our data set here's all the parameters that we defined I'm now using a generator to make it reproducible I clustered all the parameters into a single list of parameters so that for example it's easy to count them and see that in total we currently have about 3,400 parameters and this is the forward pass as we developed it and we arrive at a single number here the loss that is currently expressing how well this neural network works with the current setting of parameters now I would like to make it even more respectable so in particular see these lines here where we take the logits and we calculate a loss we're not actually reinventing the wheel here this is just classification and many people use classification and that's why there is a functional dot cross entropy function in pytorch to calculate this much more efficiently so we could just simply call f dot cross entropy and we can pass in the logits and we can pass in the array of targets y and this calculates the exact same loss so in fact we can simply put this here and erase these three lines and we're going to get the exact same result now there are actually many good reasons to prefer f dot cross entropy over rolling your own implementation like this I did this for educational reasons but you'd never use this in practice why is that number one when you use f dot cross entropy pytorch will not actually create all these intermediate tensors because these are all new tensors in memory and all this is fairly inefficient to run like this instead pytorch will cluster up all these operations and very often create have fused kernels that very efficiently evaluate these expressions that are sort of like clustered mathematical operations number two the backward pass can be made much more efficient and not just because it's a fused kernel but also analytically and mathematically it's much it's often a very much simpler backward pass to implement we actually sell this with micro grad you see here when we implemented 10h the forward pass of this operation to calculate the 10h was actually a fairly complicated mathematical expression but because it's a clustered mathematical expression when we did the backward pass we didn't individually backward through the X and the 2 times and the minus 1 and division etc we just said it's 1 minus t squared and that's a much simpler mathematical expression and we were able to do this because we're able to reuse calculations and because we are able to mathematically and analytically derive the derivative and often that expression simplifies mathematically and so there's much less to implement so not only can it be made more efficient because it runs in a fused kernel but also because the expressions can take a much simpler form mathematically so that's number one number two under the hood f dot cross entropy can also be significantly more numerically well behaved let me show you an example of how this works suppose we have a logits of negative two three negative three zero and five and then we are taking the exponent of it and normalizing it to sum to one so when logits take on this values everything is well and good and we get a nice probability distribution now consider what happens when some of these logits take on more extreme values and that can happen during optimization of a neural network suppose that some of these numbers grow very negative like say negative 100 then actually everything will come out fine we still get a probabilities that you know are well-behaved and they sum to one and everything is great but because of the way the X works if you have very positive logits let's say positive 100 in here you actually start to run into trouble and we get not a number here and the reason for that is that these counts have an inf here so if you pass in a very negative number to X you just get a very negative sorry not negative but very small number very near very near zero and that's fine but you pass in a very positive number suddenly we run out of range in our floating point number that represents these counts so basically we're taking E and we're raising it to the power of 100 and that gives us inf because we run out of dynamic range on this floating point number that is count and so we cannot pass very large logits through this expression now let me reset these numbers do something reasonable the way pytorch solve this is that you see how we have a really well-behaved result here it turns out that because of the normalization here you can actually offset logits by any arbitrary constant value that you want so if I add one here you actually get the exact same result or if I add two or if I subtract three any offset will produce the exact same probabilities so because negative numbers are okay but positive numbers can actually overflow this X what petridge does is it internally calculates the maximum value that occurs in the logits and it subtracts it so in this case it would subtract five and so therefore the greatest number in logits will become zero and all the other numbers will become some negative numbers and then the result of this is always well behaved so even if we have a hundred here previously not good but because pytorch will subtract a hundred this will work and so there's many good reasons to call cross entropy number one the forward pass can be much more efficient the backward pass can be much more efficient and also things can be much more numerically well behaved okay so let's now set up the training of this neural net we have the forward pass we don't need these is that we have that loss is equal to the cross entropy that's the forward pass then we need the backward pass first we want to set the gradients to be zero so for p-parameters we want to make sure that p.grad is none which is the same as setting it to zero in pytorch and then loss.backward to populate those gradients once we have the gradients we can do the parameter update so for p-parameters we want to take all the data and we want to nudge it learning rate times p.grad and then we want to repeat this a few times and let's print the loss here as well now this won't suffice and it will create an error because we also have to go for p-parameters and we have to make sure that p.requiresgrad is set to true in pytorch and this should just work okay so we started off with loss of 17 and we're decreasing it let's run longer and you see how the loss decreases a lot here so if we just run for a thousand times we get a very very low loss and that means that we're making very good predictions now the reason that this is so straightforward right now is because we're only overfitting 32 examples so we only have 32 examples of the first five words and therefore it's very easy to make this neural net fit only these two 32 examples because we have 3,400 parameters and only 32 examples so we're doing what's called overfitting a single batch of the data and getting a very low loss and good predictions but that's just because we have so many parameters for so few examples so it's easy to make this be very low now we're not able to achieve exactly zero and the reason for that is we can for example look at logits which are being predicted and we can look at the max along the first dimension and in pytorch max reports both the actual values that take on the maximum number but also the indices of these and you'll see that the indices are very close to the labels but in some cases they differ for example in this very first example the predicted index is 19 but the label is 5 and we're not able to make loss be 0 and fundamentally that's because here the very first or the zeroth index is the example where dot dot dot is supposed to predict E but you see how dot dot dot is also supposed to predict an O and dot dot dot is also supposed to predict an I and then S as well and so basically E, O, A, or S are all possible outcomes in a training set for the exact same input so we're not able to completely overfit and and make the loss be exactly zero but we're getting very close in the cases where there's a unique input for a unique output in those cases we do what's called overfit and we basically get the exact same and the exact correct result so now all we have to do is we just need to make sure that we read in the full data set and optimize the neural net okay so let's swing back up where we created the data set and we see that here we only use the first five words so let me now erase this and let me erase the print statements otherwise we'd be printing way too much and so when we process the full data set of all the words we now had 228,000 examples instead of just 32 so let's now scroll back down the data set is much larger we initialize the weights the same number of parameters they all require gradients and then let's push this print I lost that item to be here and let's just see how the optimization goes if we run this okay so we started with a fairly high loss and then as we're optimizing the loss is coming down but you'll notice that it takes quite a bit of time for every single iteration so let's actually address that because we're doing way too much work forwarding and backwarding 228,000 examples in practice what people usually do is they perform forward and backward pass an update on many batches of the data so what we will want to do is we want to randomly select some portion of the data set and that's a mini batch and then only forward backward and update on that little mini batch and then we iterate on those many batches so in pytorch we can for example use torch.randind we can generate numbers between 0 and 5 and make 32 of them I believe the size has to be a tuple in pytorch so we can have a tuple 32 of numbers between 0 and 5 but actually we want x.shape of 0 here and so this creates integers that index into our data set and there's 32 of them so if our mini batch size is 32 then we can come here and we can first do mini batch construct so in the integers that we want to optimize in this single iteration are in the ix and then we want to index into x with ix to only grab those rows so we're only getting 32 rows of x and therefore embeddings will again be 32 by 3 by 2 not 200,000 by 3 by 2 and then this ix has to be used not just to index into x but also to index into y and now this should be mini batches and this should be much much faster so okay so it's instant almost so this way we can run many many examples nearly instantly and decrease the loss much much faster now because we're only dealing with mini batches the quality of our gradient is lower so the direction is not as reliable it's not the actual gradient direction but the gradient direction is good enough even when it's estimating on only 32 examples that it is useful and so it's much better to have an approximate gradient and just make more steps than it is to evaluate the exact gradient and take fewer steps so that's why in practice this works quite well so let's now continue the optimization let me take out this lost that item from here and place it over here at the end okay so we're hovering around 2.5 or so however this is only the loss for that mini batch so let's actually evaluate the loss here for all of X and for all of Y just so we have a full sense of exactly how well the model is doing right now so right now we're about 2.7 on the entire training set so let's run the optimization for a while okay we're at 2.6 2.57 2.53 okay so one issue of course is we don't know if we're stepping too slow or too fast so this point one I just guessed it so one question is how do you determine this learning rate and how do we gain confidence that we're stepping in the right sort of speed so I'll show you one way to determine a reasonable learning rate it works as follows let's reset our parameters to the initial settings and now let's print in every step but let's only do 10 steps or so or maybe maybe 100 steps we want to find like a very reasonable set the search range if you will so for example if this is like very low then we see that the loss is barely decreasing so that's not that's like too low basically so let's try this one okay so we're decreasing the loss but like not very quickly so that's a pretty good low range now let's reset it again and now let's try to find the place at which the loss kind of explodes so maybe at negative one okay we see that we're minimizing the loss but you see how it's kind of unstable it goes up and down quite a bit so negative one is probably like a fast learning rate let's try negative 10 okay so this isn't optimizing this is not working very well so negative 10 is way too big negative one was already kind of big so therefore native one was like somewhat reasonable if I reset so I'm thinking that the right learning rate is somewhere between negative 0.001 and negative 1 so the way we can do this here is we can use torque shuttling space and we want to basically do something like this between 0 and 1 but host number of steps is one more parameter that's required let's do a thousand steps this creates 1000 numbers between 0.001 and 1 but it doesn't really make sense to step between these linearly so instead let me create learning rate exponent and instead of 0.001 this will be a negative 3 and this will be a 0 and then the actual LRs that we want to search over are going to be 10 to the power of LRE so now what we're doing is we're stepping linearly between the exponents of these learning rates this is 0.001 and this is 1 because 10 to the power of 0 is 1 and therefore we are spaced exponentially in this interval so these are the candidate learning rates that we want to sort of like search over roughly so now what we're going to do is here we are going to run the optimization for 1000 steps and instead of using a fixed number we are going to use learning rate indexing into here LRs of i and make this i so basically let me reset this to be again starting from random creating these learning rates between negative 0.001 and 1 but exponentially stepped and here what we're doing is we're iterating a thousand times we're going to use the learning rate that's in the beginning very very low in the beginning is going to be 0.001 but by the end it's going to be 1 and then we're going to step with that learning rate and now what we want to do is we want to keep track of the learning rates that we used and we want to look at the losses that resulted and so here let me track stats so LRI.append LR and loss side.append loss.item okay so again reset everything and then run and so basically we started with a very low learning rate and we went all the way up to learning rate of negative 1 and now what we can do is we can p l t that plot and we can plot the two so we can plot the learning rates on the x-axis and the losses we saw on the y-axis and often you're going to find that your plot looks something like this where in the beginning you have very low learning rates we basically anything barely anything happened then we got to like a nice spot here and then as we increase the learning rate enough we basically started to be kind of unstable here so a good learning rate turns out to be somewhere around here and because we have LRI here we actually may want to do not LR not the learning rate but the exponent so that would be the LRE at I is maybe what we will log so let me reset this and redo that calculation but now on the x-axis we have the exponent of the learning rate and so we can see the exponent of the learning rate that is good to use it would be sort of like roughly in the valley here because here the learning rates are just way too low and then here we're we expect relatively good learning rate somewhere here and then here things are starting to explode so somewhere around negative 1 x the exponent of the learning rate is a pretty good setting and 10 to the negative 1 is 0.1 so 0.1 is actually quit one was actually a fairly good learning rate around here and that's what we had in the initial setting but that's roughly how you would determine it and so here now we can take out the tracking of these and we can just simply set a LR to be 10 to the negative 1 or basically otherwise 0.1 as it was before and now we have some confidence that this is actually a fairly good learning rate and so now what we can do is we can crank up the iterations we can reset our optimization and we can run for a pretty long time using this learning rate oops and we don't want to print it's way too much printing so let me again reset and run 10,000 steps okay so we're at point to 2.48 roughly let's run another 10,000 steps 2.46 and now let's do one learning rate decay what this means is we're going to take our learning rate and we're going to 10x lower it and sort of where the late stages of training potentially and we may want to go a bit slower let's do one more actually a quick one just to see if we're making a dent here okay we're still making dent and by the way the bigram loss that we achieved last video was 2.45 so we've already surpassed the bigram model and once I get a sense that this is actually kind of starting to plateau off people like to do as I mentioned this learning rate decay so let's try to decay the loss the learning rate I mean and we achieve it about 2.3 now obviously this is janky and not exactly how you would train it in production but this is roughly what you're going through you first find a decent learning rate using the approach that I showed you then you start with that learning rate and you train for a while and then at the end people like to do a learning rate decay where you decay the learning rate by say a factor of 10 and you do a few more steps and then you get a trained network roughly speaking so we've achieved 2.3 and dramatically improved on the bigram language model using this simple neural net as described here using these 3400 parameters now there's something we have to be careful with I said that we have a better model because we are achieving a lower loss 2.3 much lower than 2.45 with the bigram model previously now that's not exactly true and the reason that's not true is that this is actually fairly small model but these models can get larger and larger if you keep adding neurons and parameters so you can imagine that we don't potentially have a thousand parameters we could have ten thousand or hundred thousand or millions of parameters and as the capacity of the neural network grows it becomes more and more capable of overfitting your training set what that means is that the loss on the training set on the data that you're training on will become very very low as low as zero but all that the model is doing is memorizing your training set verbatim so if you take that model and it looks like it's working really well but you try to sample from it you will basically only get examples exactly as they are in the training set you won't get any new data in addition to that if you try to evaluate the loss on some withheld names or other words you will actually see that the loss on those it can be very high and so basically it's not a good model so the standard in the field it is to split up your data set into three splits as we call them we have the training split the dev split or the validation split and the test split so training split test or sorry dev or validation split and test split and typically this would be say 80% of your data set this could be 10% and this 10% roughly so you have these three splits of the data now these 80% of your trainings of the data set the training set is used to optimize the parameters of the model just like we're doing here using gradient descent these 10% of the examples the dev or validation split they're used for development over all the hyper parameters of your model so hyper parameters are for example the size of this hidden layer the size of the embedding so this is a hundred or a two for us but we could try different things the strength of the realization which we aren't using yet so far so there's lots of different hyper parameters and settings that go into defining a neural net and you can try many different variations of them and see whichever one works best on your validation split so this is used to train the parameters this is used to train the hyper parameters and test split is used to evaluate basically the performance of the model at the end so we're only evaluating the loss on the test split very very sparingly and very few times because every single time you evaluate your test loss and you learn something from it you are basically starting to also train on the test split so you are only allowed to test the loss on the test set very very few times otherwise you risk overfitting to it as well as you experiment on your model so let's also split up our training data into train dev and test and then we are going to train on train and only evaluate on test very very sparingly okay so here we go here is where we took all the words and put them into X and Y tensors so instead let me create a new cell here and let me just copy paste some code here because I don't think it's that complex but we're gonna try to save a little bit of time I'm converting this to be a function now and this function takes some list of words and builds the arrays X and Y for those words only and then here I am shuffling up all the words so these are the input words that we get we are randomly shuffling them all up and then we're going to set n1 to be the number of examples there's 80% of the words and n2 to be 90% of the way of the words so basically if length of words is 30,000 and one is well sorry I should probably run this and one is 25,000 and n2 is 28,000 and so here we see that I'm calling build data set to build a training set X and Y by indexing into up to n1 so we're going to have only 25,000 training words and then we're going to have roughly n2 minus n1 3,000 validation examples or dev examples and we're going to have length of words basically minus n2 or 3,204 examples here for the test set so now we have X's and Y's for all those three splits oh yeah I'm printing their size here inside the function as well but here we don't have words but these are already the individual examples made from those words so let's now scroll down here and the data set now for training is more like this and then when we reset the network when we're training we're only going to be training using X train X train and Y train so that's the only thing we're training on let's see where we are on a single batch let's now train maybe a few more steps training neural networks can take a while usually don't do it in line you launch a bunch of jobs and you wait for them to finish can take in multiple days and so on luckily this is a very small network okay so the loss is pretty good oh we accidentally used our learning rate that is way too low so let me actually come back we used the decay learning rate of 0.01 so this will train much faster and then here when we evaluate let's use the depth set here X dev and my dev to evaluate the loss okay and let's not decay the learning rate and only do say 10,000 examples and let's evaluate the dev loss once here okay so we're getting about 2.3 on dev and so the neural network when it was training did not see these dev examples it hasn't optimized on them and yet when we evaluate the loss on these dev we actually get a pretty decent loss and so we can also look at what the loss is on all of training set oops and so we see that the training and the dev loss are about equal so we're not overfitting this model is not powerful enough to just be purely memorizing the data and so far we are what's called underfitting because the training loss and the dev or test losses are roughly equal so what that typically means is that our network is very tiny very small and we expect to make performance improvements by scaling up the size of this neural net so let's do that now so let's come over here and let's increase the size with the neural net the easiest way to do this is we can come here to the hidden layer which currently is a hundred neurons and let's just bump this up so let's do 300 neurons and then this is also 300 biases and here we have 300 inputs into the final layer so let's initialize our neural net we now have 10,000 10,000 parameters instead of 3,000 parameters and then we're not using this and then here what I'd like to do is I'd like to actually keep track of that okay let's just do this let's keep stats again and here when we're keeping track of the loss let's just also keep track of the steps and let's just have a eye here and let's train on 30,000 or rather say okay let's try 30,000 and we are at point one and we should be able to run this and optimize the neural net and then here basically I want to plt.plot the steps against the loss so these are the X's and the Y's and this is the loss function and how it's being optimized now you see that there's quite a bit of thickness to this and that's because we are optimizing over these mini batches and the many batches create a little bit of noise in this where are we in the dev set we are at 2.5 so we still haven't optimized this neural net very well and that's probably because we make it bigger it might take longer for this neural net to converge and so let's continue training yeah let's just continue training one possibility is that the batch size is so low that we just have way too much noise in the training and we may want to increase the batch size so that we have a bit more correct gradient and we're not thrashing too much and we can actually like optimize more properly okay this will now become meaningless because we've reinitialized these so yeah this looks not pleasing right now but the probably is like a tiny improvement but it's so hard to tell let's go again 2.52 let's try to decrease the learning rate by factor 2 okay we're at 2.32 let's continue training we basically expect to see a lower loss than what we had before because now we have a much much bigger model and we were under fitting so we'd expect that increasing the size of the model should help the neural net 2.32 okay so that's not happening too well now one other concern is that even though we've made the 10-H layer here or the hidden layer much much bigger it could be that the bottleneck of the network right now are these embeddings that are two-dimensional it can be that we're just cramming way too many characters into just two dimensions and the neural net is not able to really use that space effectively and that that is sort of like the bottleneck to our networks performance okay 2.23 so just by decreasing the learning rate I was able to make quite a bit of progress let's run this one more time and then evaluate the training and the dev loss now one more thing after training that I'd like to do is I'd like to visualize the embedding vectors for these characters before we scale up the embedding size from 2 because we'd like to make this bottleneck potentially go away but once I make this greater than 2 we won't be able to visualize them so here look over at 2.23 and 2.24 so we're not improving much more and maybe the bottleneck now is the character embedding size which is 2 so here I have a bunch of code that will create a figure and then we're going to visualize the embeddings that were trained by the neural net on these characters because right now the embedding size is just 2 so we can visualize all the characters with the X and the Y coordinates as the two embedding locations for each of these characters and so here are the X coordinates and the Y coordinates which are the columns of C and then for each one I also include the text of the little character so here what we see is actually kind of interesting the network has basically learned to separate out the characters and cluster them a little bit so for example you see how the vowels A E I O U are clustered up here so what that's telling us that is that the neural net treats these as very similar right because when they feed into the neural net the embedding for all these characters is very similar and so the neural net thinks that they're very similar and kind of like interchangeable if that makes sense then the the points that are like really far away are for example Q, Q is kind of treated as an exception and Q has a very special embedding vector so to speak similarly dot which is a special character is all the way out here and a lot of the other letters are sort of like clustered up here and so it's kind of interesting that there's a little bit of structure here after the training and it's not definitely not random and these embeddings make sense so we're now going to scale up the embedding size and won't be able to visualize it directly but we expect that because we're under fitting and we made this layer much bigger and did not sufficiently improve the loss we're thinking that the constraint to better performance right now could be these embedding vectors so let's make them bigger okay so let's scroll up here and now we don't have two-dimensional embeddings we are going to have say 10 dimensional embeddings for each word then this layer will receive 3 times 10 so 30 inputs will go into the hidden layer let's also make the hidden layer a bit smaller so instead of 300 let's just do 200 neurons in that hidden layer so now the total number of elements will be slightly bigger at 11,000 and then we here we have to be a bit careful because okay the learning rate we set to 0.1 here we are hard coding 6 and obviously if you're working in production you don't want to be hard coding magic numbers but instead of 6 this should now be 30 and let's run for 50,000 iterations and let me split out the initialization here outside so that when we run this a multiple times it's not going to wipe out our loss in addition to that here let's instead of logging loss that item let's actually log them let's do log 10 I believe that's a function of the loss and I'll show you why in a second let's optimize this basically I'd like to plot the log loss instead of a loss because when you plot the loss many times it can have this hockey stick appearance and log squashes it in so it just kind of like looks nicer so the x-axis is step I and the y-axis will be the loss I and then here this is 30 ideally we wouldn't be hard coding these because let's look at the loss okay it's again very thick because the mini batch size is very small but the total loss over the training set is 2.3 and the tests or the def set is 2.38 as well so so far so good let's try to now decrease the learning rate by a factor of 10 and train for another 50,000 iterations we'd hope that we would be able to beat 2.3 too but again we're just kind of like doing this very haphazardly so I don't actually have confidence that our learning rate is set very well that our learning rate decay which we just do at random is set very well and so the optimization here is kind of suspect to be honest and this is not how you would do it typically in production in production you would create parameters or hyper parameters out of all these settings and then you would run lots of experiments and see whichever ones are working well for you okay so we have 2.17 now and 2.2 okay so you see how the training and the validation performance are starting to slightly slowly depart so maybe we're getting the sense that the neural net is getting good enough or that number of parameters is large enough that we are slowly starting to overfit let's maybe run one iteration of this and see where we get but yeah basically you would be running lots of experiments and then you are slowly scrutinizing whichever ones give you the best depth performance and then once you find all the hyper parameters that make your depth performance good you take that model and you evaluate the test set performance a single time and that's the number that you report in your paper or wherever else you want to talk about and brag about your model so let's then rerun the plot and rerun the train and dove and because we're getting lower loss now it is the case that the embedding size of these was holding us back very likely okay so 2.16 2.19 is what we're roughly getting so there's many ways to go from many ways to go from here we can continue tuning the optimization we can continue for example playing with the size of the neural net or we can increase the number of words or characters in our case that we are taking as an input so instead of just three characters we could be taking more characters that is an input and that could further improve the loss okay so I changed the code slightly so we have here 200,000 steps of the optimization and in the first 100,000 we're using a learning rate of 0.1 and then in the next 100,000 we're using a learning rate of 0.01 this is the loss that I achieve and these are the performance on the training and validation loss and in particular the best validation loss I've been able to obtain in the last 30 minutes or so is 2.17 so now I invite you to beat this number and you have quite a few knobs available to you to I think surpass this number so number one you can of course change the number of neurons in the hidden layer of this model you can change the dimensionality of the embedding lookup table you can change the number of characters that are feeding in as an input as the context into this model and then of course you can change the details of the optimization how long are we running where is the learning rate how does it change over time how does it decay you can change the batch size and you may be able to actually achieve a much better convergence speed in terms of how many seconds or minutes it takes to train the model and get your result in terms of really good loss and then of course I actually invite you to read this paper it is 19 pages but at this point you should actually be able to read a good chunk of this paper and understand pretty good chunks of it and this paper also has quite a few ideas for improvements that you can play with so all those are knobs available to you and you should be able to beat this number I'm leaving that as an exercise to the reader and that's it for now and I'll see you next time before we wrap up I also wanted to show how you would sample from the model so we're going to generate 20 samples at first we begin with all dots so that's the context and then until we generate the zero character again we're going to embed the current context using the embedding table C now usually here the first dimension was the size of the training set but here we're only working with a single example that we're generating so this is just the mesh in one just for simplicity and so this embedding then gets projected into the end state you get the logits now we calculate the probabilities for that you can use the F dot softmax of logits and that just basically exponentials logits and makes them sum to one and similar to cross entropy it is careful that there's no overflows once we have the probabilities we sample from them using torch of multinomial to get our next index and then we shift the context window to append the index and record it and then we can just decode all the integers to strings and print them out and so these are some example samples and you can see that the model now works much better so the words here are much more word like or name like so we have things like ham chose Lila you know it's starting to sound a little bit more name like so we're definitely making progress but we can still improve on this model quite a lot okay sorry there's some bonus content I wanted to mention that I want to make these notebooks more accessible and so that I don't want you to have to like install jibir notebooks and torch and everything else so I will be sharing a link to a Google CoLab and Google Colaba look like a notebook in your browser and you can just go to a URL and you'll be able to execute all of the code that you saw in the Google Colaba and so this is me executing the code in this lecture and I shortened it a little bit but basically you're able to train the exact same network and then plot and sample from the model and everything is ready for you to like tinker with the numbers right there in your browser no installation necessary so I just wanted to point that out and the link to this will be in the video description", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.76, "text": " Hi everyone. Today we are continuing our implementation of Make More. Now in the", "tokens": [50364, 2421, 1518, 13, 2692, 321, 366, 9289, 527, 11420, 295, 4387, 5048, 13, 823, 294, 264, 50652, 50652, 1036, 7991, 321, 12270, 264, 955, 2356, 2856, 2316, 293, 321, 12270, 309, 50821, 50821, 1293, 1228, 14893, 293, 611, 1228, 257, 1687, 2199, 18161, 3209, 300, 575, 257, 2167, 51052, 51052, 8213, 4583, 13, 823, 341, 307, 264, 22125, 88, 391, 11633, 2939, 300, 321, 3094, 484, 1036, 7991, 51349, 51349, 293, 321, 1866, 300, 264, 636, 321, 17247, 341, 307, 300, 321, 2956, 412, 787, 264, 51537, 51537, 2167, 3894, 2517, 293, 321, 19147, 264, 7316, 337, 264, 51695, 51695], "temperature": 0.0, "avg_logprob": -0.15949155770096124, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.01939000003039837}, {"id": 1, "seek": 0, "start": 5.76, "end": 9.14, "text": " last lecture we implemented the bigram language model and we implemented it", "tokens": [50364, 2421, 1518, 13, 2692, 321, 366, 9289, 527, 11420, 295, 4387, 5048, 13, 823, 294, 264, 50652, 50652, 1036, 7991, 321, 12270, 264, 955, 2356, 2856, 2316, 293, 321, 12270, 309, 50821, 50821, 1293, 1228, 14893, 293, 611, 1228, 257, 1687, 2199, 18161, 3209, 300, 575, 257, 2167, 51052, 51052, 8213, 4583, 13, 823, 341, 307, 264, 22125, 88, 391, 11633, 2939, 300, 321, 3094, 484, 1036, 7991, 51349, 51349, 293, 321, 1866, 300, 264, 636, 321, 17247, 341, 307, 300, 321, 2956, 412, 787, 264, 51537, 51537, 2167, 3894, 2517, 293, 321, 19147, 264, 7316, 337, 264, 51695, 51695], "temperature": 0.0, "avg_logprob": -0.15949155770096124, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.01939000003039837}, {"id": 2, "seek": 0, "start": 9.14, "end": 13.76, "text": " both using counts and also using a super simple neural network that has a single", "tokens": [50364, 2421, 1518, 13, 2692, 321, 366, 9289, 527, 11420, 295, 4387, 5048, 13, 823, 294, 264, 50652, 50652, 1036, 7991, 321, 12270, 264, 955, 2356, 2856, 2316, 293, 321, 12270, 309, 50821, 50821, 1293, 1228, 14893, 293, 611, 1228, 257, 1687, 2199, 18161, 3209, 300, 575, 257, 2167, 51052, 51052, 8213, 4583, 13, 823, 341, 307, 264, 22125, 88, 391, 11633, 2939, 300, 321, 3094, 484, 1036, 7991, 51349, 51349, 293, 321, 1866, 300, 264, 636, 321, 17247, 341, 307, 300, 321, 2956, 412, 787, 264, 51537, 51537, 2167, 3894, 2517, 293, 321, 19147, 264, 7316, 337, 264, 51695, 51695], "temperature": 0.0, "avg_logprob": -0.15949155770096124, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.01939000003039837}, {"id": 3, "seek": 0, "start": 13.76, "end": 19.7, "text": " linear layer. Now this is the Jupyter Notebook that we built out last lecture", "tokens": [50364, 2421, 1518, 13, 2692, 321, 366, 9289, 527, 11420, 295, 4387, 5048, 13, 823, 294, 264, 50652, 50652, 1036, 7991, 321, 12270, 264, 955, 2356, 2856, 2316, 293, 321, 12270, 309, 50821, 50821, 1293, 1228, 14893, 293, 611, 1228, 257, 1687, 2199, 18161, 3209, 300, 575, 257, 2167, 51052, 51052, 8213, 4583, 13, 823, 341, 307, 264, 22125, 88, 391, 11633, 2939, 300, 321, 3094, 484, 1036, 7991, 51349, 51349, 293, 321, 1866, 300, 264, 636, 321, 17247, 341, 307, 300, 321, 2956, 412, 787, 264, 51537, 51537, 2167, 3894, 2517, 293, 321, 19147, 264, 7316, 337, 264, 51695, 51695], "temperature": 0.0, "avg_logprob": -0.15949155770096124, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.01939000003039837}, {"id": 4, "seek": 0, "start": 19.7, "end": 23.46, "text": " and we saw that the way we approached this is that we looked at only the", "tokens": [50364, 2421, 1518, 13, 2692, 321, 366, 9289, 527, 11420, 295, 4387, 5048, 13, 823, 294, 264, 50652, 50652, 1036, 7991, 321, 12270, 264, 955, 2356, 2856, 2316, 293, 321, 12270, 309, 50821, 50821, 1293, 1228, 14893, 293, 611, 1228, 257, 1687, 2199, 18161, 3209, 300, 575, 257, 2167, 51052, 51052, 8213, 4583, 13, 823, 341, 307, 264, 22125, 88, 391, 11633, 2939, 300, 321, 3094, 484, 1036, 7991, 51349, 51349, 293, 321, 1866, 300, 264, 636, 321, 17247, 341, 307, 300, 321, 2956, 412, 787, 264, 51537, 51537, 2167, 3894, 2517, 293, 321, 19147, 264, 7316, 337, 264, 51695, 51695], "temperature": 0.0, "avg_logprob": -0.15949155770096124, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.01939000003039837}, {"id": 5, "seek": 0, "start": 23.46, "end": 26.62, "text": " single previous character and we predicted the distribution for the", "tokens": [50364, 2421, 1518, 13, 2692, 321, 366, 9289, 527, 11420, 295, 4387, 5048, 13, 823, 294, 264, 50652, 50652, 1036, 7991, 321, 12270, 264, 955, 2356, 2856, 2316, 293, 321, 12270, 309, 50821, 50821, 1293, 1228, 14893, 293, 611, 1228, 257, 1687, 2199, 18161, 3209, 300, 575, 257, 2167, 51052, 51052, 8213, 4583, 13, 823, 341, 307, 264, 22125, 88, 391, 11633, 2939, 300, 321, 3094, 484, 1036, 7991, 51349, 51349, 293, 321, 1866, 300, 264, 636, 321, 17247, 341, 307, 300, 321, 2956, 412, 787, 264, 51537, 51537, 2167, 3894, 2517, 293, 321, 19147, 264, 7316, 337, 264, 51695, 51695], "temperature": 0.0, "avg_logprob": -0.15949155770096124, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.01939000003039837}, {"id": 6, "seek": 2662, "start": 26.62, "end": 30.240000000000002, "text": " character that would go next in the sequence and we did that by taking", "tokens": [50364, 2517, 300, 576, 352, 958, 294, 264, 8310, 293, 321, 630, 300, 538, 1940, 50545, 50545, 14893, 293, 2710, 3319, 552, 666, 33783, 370, 300, 1184, 5386, 510, 50799, 50799, 34499, 281, 472, 13, 823, 341, 307, 439, 731, 293, 665, 498, 291, 787, 362, 472, 2517, 295, 51045, 51045, 3894, 4319, 293, 341, 1985, 293, 309, 311, 3109, 712, 13, 440, 1154, 365, 341, 2316, 51277, 51277, 295, 1164, 307, 300, 264, 21264, 490, 341, 2316, 366, 406, 588, 665, 570, 291, 51507, 51507, 787, 747, 472, 2517, 295, 4319, 370, 264, 2316, 994, 380, 5258, 588, 51717, 51717], "temperature": 0.0, "avg_logprob": -0.09210968017578125, "compression_ratio": 1.74609375, "no_speech_prob": 2.1779838789370842e-05}, {"id": 7, "seek": 2662, "start": 30.240000000000002, "end": 35.32, "text": " counts and normalizing them into probabilities so that each row here", "tokens": [50364, 2517, 300, 576, 352, 958, 294, 264, 8310, 293, 321, 630, 300, 538, 1940, 50545, 50545, 14893, 293, 2710, 3319, 552, 666, 33783, 370, 300, 1184, 5386, 510, 50799, 50799, 34499, 281, 472, 13, 823, 341, 307, 439, 731, 293, 665, 498, 291, 787, 362, 472, 2517, 295, 51045, 51045, 3894, 4319, 293, 341, 1985, 293, 309, 311, 3109, 712, 13, 440, 1154, 365, 341, 2316, 51277, 51277, 295, 1164, 307, 300, 264, 21264, 490, 341, 2316, 366, 406, 588, 665, 570, 291, 51507, 51507, 787, 747, 472, 2517, 295, 4319, 370, 264, 2316, 994, 380, 5258, 588, 51717, 51717], "temperature": 0.0, "avg_logprob": -0.09210968017578125, "compression_ratio": 1.74609375, "no_speech_prob": 2.1779838789370842e-05}, {"id": 8, "seek": 2662, "start": 35.32, "end": 40.24, "text": " sums to one. Now this is all well and good if you only have one character of", "tokens": [50364, 2517, 300, 576, 352, 958, 294, 264, 8310, 293, 321, 630, 300, 538, 1940, 50545, 50545, 14893, 293, 2710, 3319, 552, 666, 33783, 370, 300, 1184, 5386, 510, 50799, 50799, 34499, 281, 472, 13, 823, 341, 307, 439, 731, 293, 665, 498, 291, 787, 362, 472, 2517, 295, 51045, 51045, 3894, 4319, 293, 341, 1985, 293, 309, 311, 3109, 712, 13, 440, 1154, 365, 341, 2316, 51277, 51277, 295, 1164, 307, 300, 264, 21264, 490, 341, 2316, 366, 406, 588, 665, 570, 291, 51507, 51507, 787, 747, 472, 2517, 295, 4319, 370, 264, 2316, 994, 380, 5258, 588, 51717, 51717], "temperature": 0.0, "avg_logprob": -0.09210968017578125, "compression_ratio": 1.74609375, "no_speech_prob": 2.1779838789370842e-05}, {"id": 9, "seek": 2662, "start": 40.24, "end": 44.88, "text": " previous context and this works and it's approachable. The problem with this model", "tokens": [50364, 2517, 300, 576, 352, 958, 294, 264, 8310, 293, 321, 630, 300, 538, 1940, 50545, 50545, 14893, 293, 2710, 3319, 552, 666, 33783, 370, 300, 1184, 5386, 510, 50799, 50799, 34499, 281, 472, 13, 823, 341, 307, 439, 731, 293, 665, 498, 291, 787, 362, 472, 2517, 295, 51045, 51045, 3894, 4319, 293, 341, 1985, 293, 309, 311, 3109, 712, 13, 440, 1154, 365, 341, 2316, 51277, 51277, 295, 1164, 307, 300, 264, 21264, 490, 341, 2316, 366, 406, 588, 665, 570, 291, 51507, 51507, 787, 747, 472, 2517, 295, 4319, 370, 264, 2316, 994, 380, 5258, 588, 51717, 51717], "temperature": 0.0, "avg_logprob": -0.09210968017578125, "compression_ratio": 1.74609375, "no_speech_prob": 2.1779838789370842e-05}, {"id": 10, "seek": 2662, "start": 44.88, "end": 49.480000000000004, "text": " of course is that the predictions from this model are not very good because you", "tokens": [50364, 2517, 300, 576, 352, 958, 294, 264, 8310, 293, 321, 630, 300, 538, 1940, 50545, 50545, 14893, 293, 2710, 3319, 552, 666, 33783, 370, 300, 1184, 5386, 510, 50799, 50799, 34499, 281, 472, 13, 823, 341, 307, 439, 731, 293, 665, 498, 291, 787, 362, 472, 2517, 295, 51045, 51045, 3894, 4319, 293, 341, 1985, 293, 309, 311, 3109, 712, 13, 440, 1154, 365, 341, 2316, 51277, 51277, 295, 1164, 307, 300, 264, 21264, 490, 341, 2316, 366, 406, 588, 665, 570, 291, 51507, 51507, 787, 747, 472, 2517, 295, 4319, 370, 264, 2316, 994, 380, 5258, 588, 51717, 51717], "temperature": 0.0, "avg_logprob": -0.09210968017578125, "compression_ratio": 1.74609375, "no_speech_prob": 2.1779838789370842e-05}, {"id": 11, "seek": 2662, "start": 49.480000000000004, "end": 53.68000000000001, "text": " only take one character of context so the model didn't produce very", "tokens": [50364, 2517, 300, 576, 352, 958, 294, 264, 8310, 293, 321, 630, 300, 538, 1940, 50545, 50545, 14893, 293, 2710, 3319, 552, 666, 33783, 370, 300, 1184, 5386, 510, 50799, 50799, 34499, 281, 472, 13, 823, 341, 307, 439, 731, 293, 665, 498, 291, 787, 362, 472, 2517, 295, 51045, 51045, 3894, 4319, 293, 341, 1985, 293, 309, 311, 3109, 712, 13, 440, 1154, 365, 341, 2316, 51277, 51277, 295, 1164, 307, 300, 264, 21264, 490, 341, 2316, 366, 406, 588, 665, 570, 291, 51507, 51507, 787, 747, 472, 2517, 295, 4319, 370, 264, 2316, 994, 380, 5258, 588, 51717, 51717], "temperature": 0.0, "avg_logprob": -0.09210968017578125, "compression_ratio": 1.74609375, "no_speech_prob": 2.1779838789370842e-05}, {"id": 12, "seek": 5368, "start": 53.68, "end": 58.36, "text": " name-like sounding things. Now the problem with this approach though is", "tokens": [50364, 1315, 12, 4092, 24931, 721, 13, 823, 264, 1154, 365, 341, 3109, 1673, 307, 50598, 50598, 300, 498, 321, 366, 281, 747, 544, 4319, 666, 2696, 562, 32884, 264, 958, 50794, 50794, 2517, 294, 257, 8310, 721, 2661, 6327, 493, 293, 341, 3199, 11, 264, 2744, 295, 341, 51002, 51002, 3199, 11, 13156, 293, 294, 1186, 309, 13156, 37330, 365, 264, 4641, 295, 264, 51186, 51186, 4319, 570, 498, 321, 787, 747, 257, 2167, 2517, 412, 257, 565, 300, 311, 7634, 51382, 51382, 12178, 295, 4319, 457, 498, 321, 747, 732, 4342, 294, 264, 1791, 293, 853, 281, 51610, 51610, 6069, 264, 2636, 472, 5800, 264, 1230, 295, 13241, 294, 341, 8141, 11, 291, 393, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.09757393748820328, "compression_ratio": 1.7958477508650519, "no_speech_prob": 1.805678039090708e-05}, {"id": 13, "seek": 5368, "start": 58.36, "end": 62.28, "text": " that if we are to take more context into account when predicting the next", "tokens": [50364, 1315, 12, 4092, 24931, 721, 13, 823, 264, 1154, 365, 341, 3109, 1673, 307, 50598, 50598, 300, 498, 321, 366, 281, 747, 544, 4319, 666, 2696, 562, 32884, 264, 958, 50794, 50794, 2517, 294, 257, 8310, 721, 2661, 6327, 493, 293, 341, 3199, 11, 264, 2744, 295, 341, 51002, 51002, 3199, 11, 13156, 293, 294, 1186, 309, 13156, 37330, 365, 264, 4641, 295, 264, 51186, 51186, 4319, 570, 498, 321, 787, 747, 257, 2167, 2517, 412, 257, 565, 300, 311, 7634, 51382, 51382, 12178, 295, 4319, 457, 498, 321, 747, 732, 4342, 294, 264, 1791, 293, 853, 281, 51610, 51610, 6069, 264, 2636, 472, 5800, 264, 1230, 295, 13241, 294, 341, 8141, 11, 291, 393, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.09757393748820328, "compression_ratio": 1.7958477508650519, "no_speech_prob": 1.805678039090708e-05}, {"id": 14, "seek": 5368, "start": 62.28, "end": 66.44, "text": " character in a sequence things quickly blow up and this table, the size of this", "tokens": [50364, 1315, 12, 4092, 24931, 721, 13, 823, 264, 1154, 365, 341, 3109, 1673, 307, 50598, 50598, 300, 498, 321, 366, 281, 747, 544, 4319, 666, 2696, 562, 32884, 264, 958, 50794, 50794, 2517, 294, 257, 8310, 721, 2661, 6327, 493, 293, 341, 3199, 11, 264, 2744, 295, 341, 51002, 51002, 3199, 11, 13156, 293, 294, 1186, 309, 13156, 37330, 365, 264, 4641, 295, 264, 51186, 51186, 4319, 570, 498, 321, 787, 747, 257, 2167, 2517, 412, 257, 565, 300, 311, 7634, 51382, 51382, 12178, 295, 4319, 457, 498, 321, 747, 732, 4342, 294, 264, 1791, 293, 853, 281, 51610, 51610, 6069, 264, 2636, 472, 5800, 264, 1230, 295, 13241, 294, 341, 8141, 11, 291, 393, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.09757393748820328, "compression_ratio": 1.7958477508650519, "no_speech_prob": 1.805678039090708e-05}, {"id": 15, "seek": 5368, "start": 66.44, "end": 70.12, "text": " table, grows and in fact it grows exponentially with the length of the", "tokens": [50364, 1315, 12, 4092, 24931, 721, 13, 823, 264, 1154, 365, 341, 3109, 1673, 307, 50598, 50598, 300, 498, 321, 366, 281, 747, 544, 4319, 666, 2696, 562, 32884, 264, 958, 50794, 50794, 2517, 294, 257, 8310, 721, 2661, 6327, 493, 293, 341, 3199, 11, 264, 2744, 295, 341, 51002, 51002, 3199, 11, 13156, 293, 294, 1186, 309, 13156, 37330, 365, 264, 4641, 295, 264, 51186, 51186, 4319, 570, 498, 321, 787, 747, 257, 2167, 2517, 412, 257, 565, 300, 311, 7634, 51382, 51382, 12178, 295, 4319, 457, 498, 321, 747, 732, 4342, 294, 264, 1791, 293, 853, 281, 51610, 51610, 6069, 264, 2636, 472, 5800, 264, 1230, 295, 13241, 294, 341, 8141, 11, 291, 393, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.09757393748820328, "compression_ratio": 1.7958477508650519, "no_speech_prob": 1.805678039090708e-05}, {"id": 16, "seek": 5368, "start": 70.12, "end": 74.03999999999999, "text": " context because if we only take a single character at a time that's 27", "tokens": [50364, 1315, 12, 4092, 24931, 721, 13, 823, 264, 1154, 365, 341, 3109, 1673, 307, 50598, 50598, 300, 498, 321, 366, 281, 747, 544, 4319, 666, 2696, 562, 32884, 264, 958, 50794, 50794, 2517, 294, 257, 8310, 721, 2661, 6327, 493, 293, 341, 3199, 11, 264, 2744, 295, 341, 51002, 51002, 3199, 11, 13156, 293, 294, 1186, 309, 13156, 37330, 365, 264, 4641, 295, 264, 51186, 51186, 4319, 570, 498, 321, 787, 747, 257, 2167, 2517, 412, 257, 565, 300, 311, 7634, 51382, 51382, 12178, 295, 4319, 457, 498, 321, 747, 732, 4342, 294, 264, 1791, 293, 853, 281, 51610, 51610, 6069, 264, 2636, 472, 5800, 264, 1230, 295, 13241, 294, 341, 8141, 11, 291, 393, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.09757393748820328, "compression_ratio": 1.7958477508650519, "no_speech_prob": 1.805678039090708e-05}, {"id": 17, "seek": 5368, "start": 74.03999999999999, "end": 78.6, "text": " possibilities of context but if we take two characters in the past and try to", "tokens": [50364, 1315, 12, 4092, 24931, 721, 13, 823, 264, 1154, 365, 341, 3109, 1673, 307, 50598, 50598, 300, 498, 321, 366, 281, 747, 544, 4319, 666, 2696, 562, 32884, 264, 958, 50794, 50794, 2517, 294, 257, 8310, 721, 2661, 6327, 493, 293, 341, 3199, 11, 264, 2744, 295, 341, 51002, 51002, 3199, 11, 13156, 293, 294, 1186, 309, 13156, 37330, 365, 264, 4641, 295, 264, 51186, 51186, 4319, 570, 498, 321, 787, 747, 257, 2167, 2517, 412, 257, 565, 300, 311, 7634, 51382, 51382, 12178, 295, 4319, 457, 498, 321, 747, 732, 4342, 294, 264, 1791, 293, 853, 281, 51610, 51610, 6069, 264, 2636, 472, 5800, 264, 1230, 295, 13241, 294, 341, 8141, 11, 291, 393, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.09757393748820328, "compression_ratio": 1.7958477508650519, "no_speech_prob": 1.805678039090708e-05}, {"id": 18, "seek": 5368, "start": 78.6, "end": 82.76, "text": " predict the third one suddenly the number of rows in this matrix, you can", "tokens": [50364, 1315, 12, 4092, 24931, 721, 13, 823, 264, 1154, 365, 341, 3109, 1673, 307, 50598, 50598, 300, 498, 321, 366, 281, 747, 544, 4319, 666, 2696, 562, 32884, 264, 958, 50794, 50794, 2517, 294, 257, 8310, 721, 2661, 6327, 493, 293, 341, 3199, 11, 264, 2744, 295, 341, 51002, 51002, 3199, 11, 13156, 293, 294, 1186, 309, 13156, 37330, 365, 264, 4641, 295, 264, 51186, 51186, 4319, 570, 498, 321, 787, 747, 257, 2167, 2517, 412, 257, 565, 300, 311, 7634, 51382, 51382, 12178, 295, 4319, 457, 498, 321, 747, 732, 4342, 294, 264, 1791, 293, 853, 281, 51610, 51610, 6069, 264, 2636, 472, 5800, 264, 1230, 295, 13241, 294, 341, 8141, 11, 291, 393, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.09757393748820328, "compression_ratio": 1.7958477508650519, "no_speech_prob": 1.805678039090708e-05}, {"id": 19, "seek": 8276, "start": 82.76, "end": 87.92, "text": " look at it that way, is 27 times 27 so there's 729 possibilities for what could", "tokens": [50364, 574, 412, 309, 300, 636, 11, 307, 7634, 1413, 7634, 370, 456, 311, 1614, 11871, 12178, 337, 437, 727, 50622, 50622, 362, 808, 294, 264, 4319, 13, 759, 321, 747, 1045, 4342, 382, 264, 4319, 5800, 50884, 50884, 321, 362, 945, 11, 1360, 12178, 295, 4319, 293, 370, 300, 311, 445, 636, 886, 867, 13241, 295, 51222, 51222, 341, 8141, 309, 311, 636, 886, 1326, 14893, 337, 1184, 7959, 293, 264, 1379, 551, 51510, 51510, 445, 733, 295, 42610, 293, 1177, 380, 589, 588, 731, 13, 407, 300, 311, 983, 965, 321, 434, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.09409927092876631, "compression_ratio": 1.6883116883116882, "no_speech_prob": 6.96208144290722e-06}, {"id": 20, "seek": 8276, "start": 87.92, "end": 93.16000000000001, "text": " have come in the context. If we take three characters as the context suddenly", "tokens": [50364, 574, 412, 309, 300, 636, 11, 307, 7634, 1413, 7634, 370, 456, 311, 1614, 11871, 12178, 337, 437, 727, 50622, 50622, 362, 808, 294, 264, 4319, 13, 759, 321, 747, 1045, 4342, 382, 264, 4319, 5800, 50884, 50884, 321, 362, 945, 11, 1360, 12178, 295, 4319, 293, 370, 300, 311, 445, 636, 886, 867, 13241, 295, 51222, 51222, 341, 8141, 309, 311, 636, 886, 1326, 14893, 337, 1184, 7959, 293, 264, 1379, 551, 51510, 51510, 445, 733, 295, 42610, 293, 1177, 380, 589, 588, 731, 13, 407, 300, 311, 983, 965, 321, 434, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.09409927092876631, "compression_ratio": 1.6883116883116882, "no_speech_prob": 6.96208144290722e-06}, {"id": 21, "seek": 8276, "start": 93.16000000000001, "end": 99.92, "text": " we have 20,000 possibilities of context and so that's just way too many rows of", "tokens": [50364, 574, 412, 309, 300, 636, 11, 307, 7634, 1413, 7634, 370, 456, 311, 1614, 11871, 12178, 337, 437, 727, 50622, 50622, 362, 808, 294, 264, 4319, 13, 759, 321, 747, 1045, 4342, 382, 264, 4319, 5800, 50884, 50884, 321, 362, 945, 11, 1360, 12178, 295, 4319, 293, 370, 300, 311, 445, 636, 886, 867, 13241, 295, 51222, 51222, 341, 8141, 309, 311, 636, 886, 1326, 14893, 337, 1184, 7959, 293, 264, 1379, 551, 51510, 51510, 445, 733, 295, 42610, 293, 1177, 380, 589, 588, 731, 13, 407, 300, 311, 983, 965, 321, 434, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.09409927092876631, "compression_ratio": 1.6883116883116882, "no_speech_prob": 6.96208144290722e-06}, {"id": 22, "seek": 8276, "start": 99.92, "end": 105.68, "text": " this matrix it's way too few counts for each possibility and the whole thing", "tokens": [50364, 574, 412, 309, 300, 636, 11, 307, 7634, 1413, 7634, 370, 456, 311, 1614, 11871, 12178, 337, 437, 727, 50622, 50622, 362, 808, 294, 264, 4319, 13, 759, 321, 747, 1045, 4342, 382, 264, 4319, 5800, 50884, 50884, 321, 362, 945, 11, 1360, 12178, 295, 4319, 293, 370, 300, 311, 445, 636, 886, 867, 13241, 295, 51222, 51222, 341, 8141, 309, 311, 636, 886, 1326, 14893, 337, 1184, 7959, 293, 264, 1379, 551, 51510, 51510, 445, 733, 295, 42610, 293, 1177, 380, 589, 588, 731, 13, 407, 300, 311, 983, 965, 321, 434, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.09409927092876631, "compression_ratio": 1.6883116883116882, "no_speech_prob": 6.96208144290722e-06}, {"id": 23, "seek": 8276, "start": 105.68, "end": 110.08000000000001, "text": " just kind of explodes and doesn't work very well. So that's why today we're", "tokens": [50364, 574, 412, 309, 300, 636, 11, 307, 7634, 1413, 7634, 370, 456, 311, 1614, 11871, 12178, 337, 437, 727, 50622, 50622, 362, 808, 294, 264, 4319, 13, 759, 321, 747, 1045, 4342, 382, 264, 4319, 5800, 50884, 50884, 321, 362, 945, 11, 1360, 12178, 295, 4319, 293, 370, 300, 311, 445, 636, 886, 867, 13241, 295, 51222, 51222, 341, 8141, 309, 311, 636, 886, 1326, 14893, 337, 1184, 7959, 293, 264, 1379, 551, 51510, 51510, 445, 733, 295, 42610, 293, 1177, 380, 589, 588, 731, 13, 407, 300, 311, 983, 965, 321, 434, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.09409927092876631, "compression_ratio": 1.6883116883116882, "no_speech_prob": 6.96208144290722e-06}, {"id": 24, "seek": 11008, "start": 110.08, "end": 113.32, "text": " going to move on to this bullet point here and we're going to implement a", "tokens": [50364, 516, 281, 1286, 322, 281, 341, 11632, 935, 510, 293, 321, 434, 516, 281, 4445, 257, 50526, 50526, 2120, 388, 11167, 43276, 2044, 2316, 281, 6069, 264, 958, 2517, 294, 257, 8310, 293, 50758, 50758, 341, 15983, 3109, 300, 321, 434, 516, 281, 6878, 10002, 341, 3035, 3964, 73, 88, 310, 338, 50990, 50990, 16416, 13, 407, 286, 362, 264, 3035, 7373, 493, 510, 13, 823, 341, 1943, 380, 264, 588, 700, 3035, 300, 51272, 51272, 10348, 264, 764, 295, 2120, 388, 11167, 43276, 13270, 420, 18161, 9590, 281, 51450, 51450, 6069, 264, 958, 2517, 420, 14862, 294, 257, 8310, 457, 309, 311, 2138, 472, 300, 307, 51650, 51650, 390, 588, 22215, 926, 300, 565, 309, 307, 588, 2049, 30134, 281, 1463, 294, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13361044756070836, "compression_ratio": 1.8402777777777777, "no_speech_prob": 5.682079972757492e-06}, {"id": 25, "seek": 11008, "start": 113.32, "end": 117.96, "text": " multilayer perceptron model to predict the next character in a sequence and", "tokens": [50364, 516, 281, 1286, 322, 281, 341, 11632, 935, 510, 293, 321, 434, 516, 281, 4445, 257, 50526, 50526, 2120, 388, 11167, 43276, 2044, 2316, 281, 6069, 264, 958, 2517, 294, 257, 8310, 293, 50758, 50758, 341, 15983, 3109, 300, 321, 434, 516, 281, 6878, 10002, 341, 3035, 3964, 73, 88, 310, 338, 50990, 50990, 16416, 13, 407, 286, 362, 264, 3035, 7373, 493, 510, 13, 823, 341, 1943, 380, 264, 588, 700, 3035, 300, 51272, 51272, 10348, 264, 764, 295, 2120, 388, 11167, 43276, 13270, 420, 18161, 9590, 281, 51450, 51450, 6069, 264, 958, 2517, 420, 14862, 294, 257, 8310, 457, 309, 311, 2138, 472, 300, 307, 51650, 51650, 390, 588, 22215, 926, 300, 565, 309, 307, 588, 2049, 30134, 281, 1463, 294, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13361044756070836, "compression_ratio": 1.8402777777777777, "no_speech_prob": 5.682079972757492e-06}, {"id": 26, "seek": 11008, "start": 117.96, "end": 122.6, "text": " this modeling approach that we're going to adopt follows this paper Benjyotel", "tokens": [50364, 516, 281, 1286, 322, 281, 341, 11632, 935, 510, 293, 321, 434, 516, 281, 4445, 257, 50526, 50526, 2120, 388, 11167, 43276, 2044, 2316, 281, 6069, 264, 958, 2517, 294, 257, 8310, 293, 50758, 50758, 341, 15983, 3109, 300, 321, 434, 516, 281, 6878, 10002, 341, 3035, 3964, 73, 88, 310, 338, 50990, 50990, 16416, 13, 407, 286, 362, 264, 3035, 7373, 493, 510, 13, 823, 341, 1943, 380, 264, 588, 700, 3035, 300, 51272, 51272, 10348, 264, 764, 295, 2120, 388, 11167, 43276, 13270, 420, 18161, 9590, 281, 51450, 51450, 6069, 264, 958, 2517, 420, 14862, 294, 257, 8310, 457, 309, 311, 2138, 472, 300, 307, 51650, 51650, 390, 588, 22215, 926, 300, 565, 309, 307, 588, 2049, 30134, 281, 1463, 294, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13361044756070836, "compression_ratio": 1.8402777777777777, "no_speech_prob": 5.682079972757492e-06}, {"id": 27, "seek": 11008, "start": 122.6, "end": 128.24, "text": " 2003. So I have the paper pulled up here. Now this isn't the very first paper that", "tokens": [50364, 516, 281, 1286, 322, 281, 341, 11632, 935, 510, 293, 321, 434, 516, 281, 4445, 257, 50526, 50526, 2120, 388, 11167, 43276, 2044, 2316, 281, 6069, 264, 958, 2517, 294, 257, 8310, 293, 50758, 50758, 341, 15983, 3109, 300, 321, 434, 516, 281, 6878, 10002, 341, 3035, 3964, 73, 88, 310, 338, 50990, 50990, 16416, 13, 407, 286, 362, 264, 3035, 7373, 493, 510, 13, 823, 341, 1943, 380, 264, 588, 700, 3035, 300, 51272, 51272, 10348, 264, 764, 295, 2120, 388, 11167, 43276, 13270, 420, 18161, 9590, 281, 51450, 51450, 6069, 264, 958, 2517, 420, 14862, 294, 257, 8310, 457, 309, 311, 2138, 472, 300, 307, 51650, 51650, 390, 588, 22215, 926, 300, 565, 309, 307, 588, 2049, 30134, 281, 1463, 294, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13361044756070836, "compression_ratio": 1.8402777777777777, "no_speech_prob": 5.682079972757492e-06}, {"id": 28, "seek": 11008, "start": 128.24, "end": 131.8, "text": " proposed the use of multilayer perceptrons or neural networks to", "tokens": [50364, 516, 281, 1286, 322, 281, 341, 11632, 935, 510, 293, 321, 434, 516, 281, 4445, 257, 50526, 50526, 2120, 388, 11167, 43276, 2044, 2316, 281, 6069, 264, 958, 2517, 294, 257, 8310, 293, 50758, 50758, 341, 15983, 3109, 300, 321, 434, 516, 281, 6878, 10002, 341, 3035, 3964, 73, 88, 310, 338, 50990, 50990, 16416, 13, 407, 286, 362, 264, 3035, 7373, 493, 510, 13, 823, 341, 1943, 380, 264, 588, 700, 3035, 300, 51272, 51272, 10348, 264, 764, 295, 2120, 388, 11167, 43276, 13270, 420, 18161, 9590, 281, 51450, 51450, 6069, 264, 958, 2517, 420, 14862, 294, 257, 8310, 457, 309, 311, 2138, 472, 300, 307, 51650, 51650, 390, 588, 22215, 926, 300, 565, 309, 307, 588, 2049, 30134, 281, 1463, 294, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13361044756070836, "compression_ratio": 1.8402777777777777, "no_speech_prob": 5.682079972757492e-06}, {"id": 29, "seek": 11008, "start": 131.8, "end": 135.8, "text": " predict the next character or token in a sequence but it's definitely one that is", "tokens": [50364, 516, 281, 1286, 322, 281, 341, 11632, 935, 510, 293, 321, 434, 516, 281, 4445, 257, 50526, 50526, 2120, 388, 11167, 43276, 2044, 2316, 281, 6069, 264, 958, 2517, 294, 257, 8310, 293, 50758, 50758, 341, 15983, 3109, 300, 321, 434, 516, 281, 6878, 10002, 341, 3035, 3964, 73, 88, 310, 338, 50990, 50990, 16416, 13, 407, 286, 362, 264, 3035, 7373, 493, 510, 13, 823, 341, 1943, 380, 264, 588, 700, 3035, 300, 51272, 51272, 10348, 264, 764, 295, 2120, 388, 11167, 43276, 13270, 420, 18161, 9590, 281, 51450, 51450, 6069, 264, 958, 2517, 420, 14862, 294, 257, 8310, 457, 309, 311, 2138, 472, 300, 307, 51650, 51650, 390, 588, 22215, 926, 300, 565, 309, 307, 588, 2049, 30134, 281, 1463, 294, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13361044756070836, "compression_ratio": 1.8402777777777777, "no_speech_prob": 5.682079972757492e-06}, {"id": 30, "seek": 11008, "start": 135.8, "end": 140.04, "text": " was very influential around that time it is very often cited to stand in", "tokens": [50364, 516, 281, 1286, 322, 281, 341, 11632, 935, 510, 293, 321, 434, 516, 281, 4445, 257, 50526, 50526, 2120, 388, 11167, 43276, 2044, 2316, 281, 6069, 264, 958, 2517, 294, 257, 8310, 293, 50758, 50758, 341, 15983, 3109, 300, 321, 434, 516, 281, 6878, 10002, 341, 3035, 3964, 73, 88, 310, 338, 50990, 50990, 16416, 13, 407, 286, 362, 264, 3035, 7373, 493, 510, 13, 823, 341, 1943, 380, 264, 588, 700, 3035, 300, 51272, 51272, 10348, 264, 764, 295, 2120, 388, 11167, 43276, 13270, 420, 18161, 9590, 281, 51450, 51450, 6069, 264, 958, 2517, 420, 14862, 294, 257, 8310, 457, 309, 311, 2138, 472, 300, 307, 51650, 51650, 390, 588, 22215, 926, 300, 565, 309, 307, 588, 2049, 30134, 281, 1463, 294, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13361044756070836, "compression_ratio": 1.8402777777777777, "no_speech_prob": 5.682079972757492e-06}, {"id": 31, "seek": 14004, "start": 140.04, "end": 143.76, "text": " for this idea and I think it's a very nice write-up and so this is the paper", "tokens": [50364, 337, 341, 1558, 293, 286, 519, 309, 311, 257, 588, 1481, 2464, 12, 1010, 293, 370, 341, 307, 264, 3035, 50550, 50550, 300, 321, 434, 516, 281, 700, 574, 412, 293, 550, 4445, 13, 823, 341, 3035, 575, 1294, 50762, 50762, 7183, 370, 321, 500, 380, 362, 565, 281, 352, 666, 264, 1577, 2607, 295, 341, 3035, 457, 286, 50956, 50956, 7980, 291, 281, 1401, 309, 13, 467, 311, 588, 49857, 11, 1880, 293, 575, 257, 688, 295, 1880, 51136, 51136, 3487, 294, 309, 382, 731, 13, 682, 264, 9339, 436, 6786, 264, 1900, 912, 1154, 286, 51336, 51336, 445, 7619, 293, 550, 281, 2985, 309, 436, 17421, 264, 3480, 2316, 13, 823, 51574, 51574, 1066, 294, 1575, 300, 321, 366, 2390, 257, 2517, 1496, 2856, 2316, 370, 321, 434, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09292612502824014, "compression_ratio": 1.7516129032258065, "no_speech_prob": 8.012842044990975e-06}, {"id": 32, "seek": 14004, "start": 143.76, "end": 148.0, "text": " that we're going to first look at and then implement. Now this paper has 19", "tokens": [50364, 337, 341, 1558, 293, 286, 519, 309, 311, 257, 588, 1481, 2464, 12, 1010, 293, 370, 341, 307, 264, 3035, 50550, 50550, 300, 321, 434, 516, 281, 700, 574, 412, 293, 550, 4445, 13, 823, 341, 3035, 575, 1294, 50762, 50762, 7183, 370, 321, 500, 380, 362, 565, 281, 352, 666, 264, 1577, 2607, 295, 341, 3035, 457, 286, 50956, 50956, 7980, 291, 281, 1401, 309, 13, 467, 311, 588, 49857, 11, 1880, 293, 575, 257, 688, 295, 1880, 51136, 51136, 3487, 294, 309, 382, 731, 13, 682, 264, 9339, 436, 6786, 264, 1900, 912, 1154, 286, 51336, 51336, 445, 7619, 293, 550, 281, 2985, 309, 436, 17421, 264, 3480, 2316, 13, 823, 51574, 51574, 1066, 294, 1575, 300, 321, 366, 2390, 257, 2517, 1496, 2856, 2316, 370, 321, 434, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09292612502824014, "compression_ratio": 1.7516129032258065, "no_speech_prob": 8.012842044990975e-06}, {"id": 33, "seek": 14004, "start": 148.0, "end": 151.88, "text": " pages so we don't have time to go into the full detail of this paper but I", "tokens": [50364, 337, 341, 1558, 293, 286, 519, 309, 311, 257, 588, 1481, 2464, 12, 1010, 293, 370, 341, 307, 264, 3035, 50550, 50550, 300, 321, 434, 516, 281, 700, 574, 412, 293, 550, 4445, 13, 823, 341, 3035, 575, 1294, 50762, 50762, 7183, 370, 321, 500, 380, 362, 565, 281, 352, 666, 264, 1577, 2607, 295, 341, 3035, 457, 286, 50956, 50956, 7980, 291, 281, 1401, 309, 13, 467, 311, 588, 49857, 11, 1880, 293, 575, 257, 688, 295, 1880, 51136, 51136, 3487, 294, 309, 382, 731, 13, 682, 264, 9339, 436, 6786, 264, 1900, 912, 1154, 286, 51336, 51336, 445, 7619, 293, 550, 281, 2985, 309, 436, 17421, 264, 3480, 2316, 13, 823, 51574, 51574, 1066, 294, 1575, 300, 321, 366, 2390, 257, 2517, 1496, 2856, 2316, 370, 321, 434, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09292612502824014, "compression_ratio": 1.7516129032258065, "no_speech_prob": 8.012842044990975e-06}, {"id": 34, "seek": 14004, "start": 151.88, "end": 155.48, "text": " invite you to read it. It's very readable, interesting and has a lot of interesting", "tokens": [50364, 337, 341, 1558, 293, 286, 519, 309, 311, 257, 588, 1481, 2464, 12, 1010, 293, 370, 341, 307, 264, 3035, 50550, 50550, 300, 321, 434, 516, 281, 700, 574, 412, 293, 550, 4445, 13, 823, 341, 3035, 575, 1294, 50762, 50762, 7183, 370, 321, 500, 380, 362, 565, 281, 352, 666, 264, 1577, 2607, 295, 341, 3035, 457, 286, 50956, 50956, 7980, 291, 281, 1401, 309, 13, 467, 311, 588, 49857, 11, 1880, 293, 575, 257, 688, 295, 1880, 51136, 51136, 3487, 294, 309, 382, 731, 13, 682, 264, 9339, 436, 6786, 264, 1900, 912, 1154, 286, 51336, 51336, 445, 7619, 293, 550, 281, 2985, 309, 436, 17421, 264, 3480, 2316, 13, 823, 51574, 51574, 1066, 294, 1575, 300, 321, 366, 2390, 257, 2517, 1496, 2856, 2316, 370, 321, 434, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09292612502824014, "compression_ratio": 1.7516129032258065, "no_speech_prob": 8.012842044990975e-06}, {"id": 35, "seek": 14004, "start": 155.48, "end": 159.48, "text": " ideas in it as well. In the introduction they describe the exact same problem I", "tokens": [50364, 337, 341, 1558, 293, 286, 519, 309, 311, 257, 588, 1481, 2464, 12, 1010, 293, 370, 341, 307, 264, 3035, 50550, 50550, 300, 321, 434, 516, 281, 700, 574, 412, 293, 550, 4445, 13, 823, 341, 3035, 575, 1294, 50762, 50762, 7183, 370, 321, 500, 380, 362, 565, 281, 352, 666, 264, 1577, 2607, 295, 341, 3035, 457, 286, 50956, 50956, 7980, 291, 281, 1401, 309, 13, 467, 311, 588, 49857, 11, 1880, 293, 575, 257, 688, 295, 1880, 51136, 51136, 3487, 294, 309, 382, 731, 13, 682, 264, 9339, 436, 6786, 264, 1900, 912, 1154, 286, 51336, 51336, 445, 7619, 293, 550, 281, 2985, 309, 436, 17421, 264, 3480, 2316, 13, 823, 51574, 51574, 1066, 294, 1575, 300, 321, 366, 2390, 257, 2517, 1496, 2856, 2316, 370, 321, 434, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09292612502824014, "compression_ratio": 1.7516129032258065, "no_speech_prob": 8.012842044990975e-06}, {"id": 36, "seek": 14004, "start": 159.48, "end": 164.23999999999998, "text": " just described and then to address it they propose the following model. Now", "tokens": [50364, 337, 341, 1558, 293, 286, 519, 309, 311, 257, 588, 1481, 2464, 12, 1010, 293, 370, 341, 307, 264, 3035, 50550, 50550, 300, 321, 434, 516, 281, 700, 574, 412, 293, 550, 4445, 13, 823, 341, 3035, 575, 1294, 50762, 50762, 7183, 370, 321, 500, 380, 362, 565, 281, 352, 666, 264, 1577, 2607, 295, 341, 3035, 457, 286, 50956, 50956, 7980, 291, 281, 1401, 309, 13, 467, 311, 588, 49857, 11, 1880, 293, 575, 257, 688, 295, 1880, 51136, 51136, 3487, 294, 309, 382, 731, 13, 682, 264, 9339, 436, 6786, 264, 1900, 912, 1154, 286, 51336, 51336, 445, 7619, 293, 550, 281, 2985, 309, 436, 17421, 264, 3480, 2316, 13, 823, 51574, 51574, 1066, 294, 1575, 300, 321, 366, 2390, 257, 2517, 1496, 2856, 2316, 370, 321, 434, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09292612502824014, "compression_ratio": 1.7516129032258065, "no_speech_prob": 8.012842044990975e-06}, {"id": 37, "seek": 14004, "start": 164.23999999999998, "end": 168.35999999999999, "text": " keep in mind that we are building a character level language model so we're", "tokens": [50364, 337, 341, 1558, 293, 286, 519, 309, 311, 257, 588, 1481, 2464, 12, 1010, 293, 370, 341, 307, 264, 3035, 50550, 50550, 300, 321, 434, 516, 281, 700, 574, 412, 293, 550, 4445, 13, 823, 341, 3035, 575, 1294, 50762, 50762, 7183, 370, 321, 500, 380, 362, 565, 281, 352, 666, 264, 1577, 2607, 295, 341, 3035, 457, 286, 50956, 50956, 7980, 291, 281, 1401, 309, 13, 467, 311, 588, 49857, 11, 1880, 293, 575, 257, 688, 295, 1880, 51136, 51136, 3487, 294, 309, 382, 731, 13, 682, 264, 9339, 436, 6786, 264, 1900, 912, 1154, 286, 51336, 51336, 445, 7619, 293, 550, 281, 2985, 309, 436, 17421, 264, 3480, 2316, 13, 823, 51574, 51574, 1066, 294, 1575, 300, 321, 366, 2390, 257, 2517, 1496, 2856, 2316, 370, 321, 434, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09292612502824014, "compression_ratio": 1.7516129032258065, "no_speech_prob": 8.012842044990975e-06}, {"id": 38, "seek": 16836, "start": 168.36, "end": 172.20000000000002, "text": " working on the level of characters. In this paper they have a vocabulary of", "tokens": [50364, 1364, 322, 264, 1496, 295, 4342, 13, 682, 341, 3035, 436, 362, 257, 19864, 295, 50556, 50556, 3282, 11, 1360, 1944, 2283, 293, 436, 2602, 1322, 257, 1349, 1496, 2856, 2316, 457, 50806, 50806, 321, 434, 516, 281, 920, 2897, 365, 264, 4342, 457, 321, 603, 747, 264, 912, 50928, 50928, 15983, 3109, 13, 823, 437, 436, 360, 307, 1936, 436, 17421, 281, 747, 633, 472, 51180, 51180, 295, 613, 2283, 3282, 11, 1360, 2283, 293, 436, 434, 516, 281, 14644, 281, 1184, 1349, 257, 584, 51454, 51454, 2217, 18795, 4111, 8062, 13, 407, 633, 1349, 307, 586, 16741, 666, 257, 2217, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.10693655373915187, "compression_ratio": 1.7318007662835249, "no_speech_prob": 1.112452991947066e-05}, {"id": 39, "seek": 16836, "start": 172.20000000000002, "end": 177.20000000000002, "text": " 17,000 possible words and they instead build a word level language model but", "tokens": [50364, 1364, 322, 264, 1496, 295, 4342, 13, 682, 341, 3035, 436, 362, 257, 19864, 295, 50556, 50556, 3282, 11, 1360, 1944, 2283, 293, 436, 2602, 1322, 257, 1349, 1496, 2856, 2316, 457, 50806, 50806, 321, 434, 516, 281, 920, 2897, 365, 264, 4342, 457, 321, 603, 747, 264, 912, 50928, 50928, 15983, 3109, 13, 823, 437, 436, 360, 307, 1936, 436, 17421, 281, 747, 633, 472, 51180, 51180, 295, 613, 2283, 3282, 11, 1360, 2283, 293, 436, 434, 516, 281, 14644, 281, 1184, 1349, 257, 584, 51454, 51454, 2217, 18795, 4111, 8062, 13, 407, 633, 1349, 307, 586, 16741, 666, 257, 2217, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.10693655373915187, "compression_ratio": 1.7318007662835249, "no_speech_prob": 1.112452991947066e-05}, {"id": 40, "seek": 16836, "start": 177.20000000000002, "end": 179.64000000000001, "text": " we're going to still stick with the characters but we'll take the same", "tokens": [50364, 1364, 322, 264, 1496, 295, 4342, 13, 682, 341, 3035, 436, 362, 257, 19864, 295, 50556, 50556, 3282, 11, 1360, 1944, 2283, 293, 436, 2602, 1322, 257, 1349, 1496, 2856, 2316, 457, 50806, 50806, 321, 434, 516, 281, 920, 2897, 365, 264, 4342, 457, 321, 603, 747, 264, 912, 50928, 50928, 15983, 3109, 13, 823, 437, 436, 360, 307, 1936, 436, 17421, 281, 747, 633, 472, 51180, 51180, 295, 613, 2283, 3282, 11, 1360, 2283, 293, 436, 434, 516, 281, 14644, 281, 1184, 1349, 257, 584, 51454, 51454, 2217, 18795, 4111, 8062, 13, 407, 633, 1349, 307, 586, 16741, 666, 257, 2217, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.10693655373915187, "compression_ratio": 1.7318007662835249, "no_speech_prob": 1.112452991947066e-05}, {"id": 41, "seek": 16836, "start": 179.64000000000001, "end": 184.68, "text": " modeling approach. Now what they do is basically they propose to take every one", "tokens": [50364, 1364, 322, 264, 1496, 295, 4342, 13, 682, 341, 3035, 436, 362, 257, 19864, 295, 50556, 50556, 3282, 11, 1360, 1944, 2283, 293, 436, 2602, 1322, 257, 1349, 1496, 2856, 2316, 457, 50806, 50806, 321, 434, 516, 281, 920, 2897, 365, 264, 4342, 457, 321, 603, 747, 264, 912, 50928, 50928, 15983, 3109, 13, 823, 437, 436, 360, 307, 1936, 436, 17421, 281, 747, 633, 472, 51180, 51180, 295, 613, 2283, 3282, 11, 1360, 2283, 293, 436, 434, 516, 281, 14644, 281, 1184, 1349, 257, 584, 51454, 51454, 2217, 18795, 4111, 8062, 13, 407, 633, 1349, 307, 586, 16741, 666, 257, 2217, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.10693655373915187, "compression_ratio": 1.7318007662835249, "no_speech_prob": 1.112452991947066e-05}, {"id": 42, "seek": 16836, "start": 184.68, "end": 190.16000000000003, "text": " of these words 17,000 words and they're going to associate to each word a say", "tokens": [50364, 1364, 322, 264, 1496, 295, 4342, 13, 682, 341, 3035, 436, 362, 257, 19864, 295, 50556, 50556, 3282, 11, 1360, 1944, 2283, 293, 436, 2602, 1322, 257, 1349, 1496, 2856, 2316, 457, 50806, 50806, 321, 434, 516, 281, 920, 2897, 365, 264, 4342, 457, 321, 603, 747, 264, 912, 50928, 50928, 15983, 3109, 13, 823, 437, 436, 360, 307, 1936, 436, 17421, 281, 747, 633, 472, 51180, 51180, 295, 613, 2283, 3282, 11, 1360, 2283, 293, 436, 434, 516, 281, 14644, 281, 1184, 1349, 257, 584, 51454, 51454, 2217, 18795, 4111, 8062, 13, 407, 633, 1349, 307, 586, 16741, 666, 257, 2217, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.10693655373915187, "compression_ratio": 1.7318007662835249, "no_speech_prob": 1.112452991947066e-05}, {"id": 43, "seek": 16836, "start": 190.16000000000003, "end": 196.88000000000002, "text": " 30 dimensional feature vector. So every word is now embedded into a 30", "tokens": [50364, 1364, 322, 264, 1496, 295, 4342, 13, 682, 341, 3035, 436, 362, 257, 19864, 295, 50556, 50556, 3282, 11, 1360, 1944, 2283, 293, 436, 2602, 1322, 257, 1349, 1496, 2856, 2316, 457, 50806, 50806, 321, 434, 516, 281, 920, 2897, 365, 264, 4342, 457, 321, 603, 747, 264, 912, 50928, 50928, 15983, 3109, 13, 823, 437, 436, 360, 307, 1936, 436, 17421, 281, 747, 633, 472, 51180, 51180, 295, 613, 2283, 3282, 11, 1360, 2283, 293, 436, 434, 516, 281, 14644, 281, 1184, 1349, 257, 584, 51454, 51454, 2217, 18795, 4111, 8062, 13, 407, 633, 1349, 307, 586, 16741, 666, 257, 2217, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.10693655373915187, "compression_ratio": 1.7318007662835249, "no_speech_prob": 1.112452991947066e-05}, {"id": 44, "seek": 19688, "start": 196.88, "end": 201.56, "text": " dimensional space you can think of it that way. So we have 17,000 points or", "tokens": [50364, 18795, 1901, 291, 393, 519, 295, 309, 300, 636, 13, 407, 321, 362, 3282, 11, 1360, 2793, 420, 50598, 50598, 18875, 294, 257, 2217, 18795, 1901, 293, 300, 311, 291, 1062, 3811, 300, 311, 588, 50824, 50824, 21634, 300, 311, 257, 688, 295, 2793, 337, 257, 588, 1359, 1901, 13, 823, 294, 264, 2863, 51060, 51060, 613, 2283, 366, 5883, 1602, 2584, 16979, 370, 436, 434, 3974, 484, 412, 4974, 51222, 51222, 457, 550, 321, 434, 516, 281, 10864, 613, 12240, 29432, 295, 613, 2283, 1228, 300, 51456, 51456, 38377, 13, 407, 1830, 264, 1164, 295, 3097, 295, 341, 18161, 3209, 613, 51638, 51638, 2793, 420, 18875, 366, 516, 281, 1936, 1286, 926, 294, 341, 1901, 293, 291, 1062, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.08740836818043779, "compression_ratio": 1.8287671232876712, "no_speech_prob": 1.0782309800561052e-05}, {"id": 45, "seek": 19688, "start": 201.56, "end": 206.07999999999998, "text": " vectors in a 30 dimensional space and that's you might imagine that's very", "tokens": [50364, 18795, 1901, 291, 393, 519, 295, 309, 300, 636, 13, 407, 321, 362, 3282, 11, 1360, 2793, 420, 50598, 50598, 18875, 294, 257, 2217, 18795, 1901, 293, 300, 311, 291, 1062, 3811, 300, 311, 588, 50824, 50824, 21634, 300, 311, 257, 688, 295, 2793, 337, 257, 588, 1359, 1901, 13, 823, 294, 264, 2863, 51060, 51060, 613, 2283, 366, 5883, 1602, 2584, 16979, 370, 436, 434, 3974, 484, 412, 4974, 51222, 51222, 457, 550, 321, 434, 516, 281, 10864, 613, 12240, 29432, 295, 613, 2283, 1228, 300, 51456, 51456, 38377, 13, 407, 1830, 264, 1164, 295, 3097, 295, 341, 18161, 3209, 613, 51638, 51638, 2793, 420, 18875, 366, 516, 281, 1936, 1286, 926, 294, 341, 1901, 293, 291, 1062, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.08740836818043779, "compression_ratio": 1.8287671232876712, "no_speech_prob": 1.0782309800561052e-05}, {"id": 46, "seek": 19688, "start": 206.07999999999998, "end": 210.79999999999998, "text": " crowded that's a lot of points for a very small space. Now in the beginning", "tokens": [50364, 18795, 1901, 291, 393, 519, 295, 309, 300, 636, 13, 407, 321, 362, 3282, 11, 1360, 2793, 420, 50598, 50598, 18875, 294, 257, 2217, 18795, 1901, 293, 300, 311, 291, 1062, 3811, 300, 311, 588, 50824, 50824, 21634, 300, 311, 257, 688, 295, 2793, 337, 257, 588, 1359, 1901, 13, 823, 294, 264, 2863, 51060, 51060, 613, 2283, 366, 5883, 1602, 2584, 16979, 370, 436, 434, 3974, 484, 412, 4974, 51222, 51222, 457, 550, 321, 434, 516, 281, 10864, 613, 12240, 29432, 295, 613, 2283, 1228, 300, 51456, 51456, 38377, 13, 407, 1830, 264, 1164, 295, 3097, 295, 341, 18161, 3209, 613, 51638, 51638, 2793, 420, 18875, 366, 516, 281, 1936, 1286, 926, 294, 341, 1901, 293, 291, 1062, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.08740836818043779, "compression_ratio": 1.8287671232876712, "no_speech_prob": 1.0782309800561052e-05}, {"id": 47, "seek": 19688, "start": 210.79999999999998, "end": 214.04, "text": " these words are initialized completely randomly so they're spread out at random", "tokens": [50364, 18795, 1901, 291, 393, 519, 295, 309, 300, 636, 13, 407, 321, 362, 3282, 11, 1360, 2793, 420, 50598, 50598, 18875, 294, 257, 2217, 18795, 1901, 293, 300, 311, 291, 1062, 3811, 300, 311, 588, 50824, 50824, 21634, 300, 311, 257, 688, 295, 2793, 337, 257, 588, 1359, 1901, 13, 823, 294, 264, 2863, 51060, 51060, 613, 2283, 366, 5883, 1602, 2584, 16979, 370, 436, 434, 3974, 484, 412, 4974, 51222, 51222, 457, 550, 321, 434, 516, 281, 10864, 613, 12240, 29432, 295, 613, 2283, 1228, 300, 51456, 51456, 38377, 13, 407, 1830, 264, 1164, 295, 3097, 295, 341, 18161, 3209, 613, 51638, 51638, 2793, 420, 18875, 366, 516, 281, 1936, 1286, 926, 294, 341, 1901, 293, 291, 1062, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.08740836818043779, "compression_ratio": 1.8287671232876712, "no_speech_prob": 1.0782309800561052e-05}, {"id": 48, "seek": 19688, "start": 214.04, "end": 218.72, "text": " but then we're going to tune these embeddings of these words using that", "tokens": [50364, 18795, 1901, 291, 393, 519, 295, 309, 300, 636, 13, 407, 321, 362, 3282, 11, 1360, 2793, 420, 50598, 50598, 18875, 294, 257, 2217, 18795, 1901, 293, 300, 311, 291, 1062, 3811, 300, 311, 588, 50824, 50824, 21634, 300, 311, 257, 688, 295, 2793, 337, 257, 588, 1359, 1901, 13, 823, 294, 264, 2863, 51060, 51060, 613, 2283, 366, 5883, 1602, 2584, 16979, 370, 436, 434, 3974, 484, 412, 4974, 51222, 51222, 457, 550, 321, 434, 516, 281, 10864, 613, 12240, 29432, 295, 613, 2283, 1228, 300, 51456, 51456, 38377, 13, 407, 1830, 264, 1164, 295, 3097, 295, 341, 18161, 3209, 613, 51638, 51638, 2793, 420, 18875, 366, 516, 281, 1936, 1286, 926, 294, 341, 1901, 293, 291, 1062, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.08740836818043779, "compression_ratio": 1.8287671232876712, "no_speech_prob": 1.0782309800561052e-05}, {"id": 49, "seek": 19688, "start": 218.72, "end": 222.35999999999999, "text": " propagation. So during the course of training of this neural network these", "tokens": [50364, 18795, 1901, 291, 393, 519, 295, 309, 300, 636, 13, 407, 321, 362, 3282, 11, 1360, 2793, 420, 50598, 50598, 18875, 294, 257, 2217, 18795, 1901, 293, 300, 311, 291, 1062, 3811, 300, 311, 588, 50824, 50824, 21634, 300, 311, 257, 688, 295, 2793, 337, 257, 588, 1359, 1901, 13, 823, 294, 264, 2863, 51060, 51060, 613, 2283, 366, 5883, 1602, 2584, 16979, 370, 436, 434, 3974, 484, 412, 4974, 51222, 51222, 457, 550, 321, 434, 516, 281, 10864, 613, 12240, 29432, 295, 613, 2283, 1228, 300, 51456, 51456, 38377, 13, 407, 1830, 264, 1164, 295, 3097, 295, 341, 18161, 3209, 613, 51638, 51638, 2793, 420, 18875, 366, 516, 281, 1936, 1286, 926, 294, 341, 1901, 293, 291, 1062, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.08740836818043779, "compression_ratio": 1.8287671232876712, "no_speech_prob": 1.0782309800561052e-05}, {"id": 50, "seek": 19688, "start": 222.35999999999999, "end": 226.35999999999999, "text": " points or vectors are going to basically move around in this space and you might", "tokens": [50364, 18795, 1901, 291, 393, 519, 295, 309, 300, 636, 13, 407, 321, 362, 3282, 11, 1360, 2793, 420, 50598, 50598, 18875, 294, 257, 2217, 18795, 1901, 293, 300, 311, 291, 1062, 3811, 300, 311, 588, 50824, 50824, 21634, 300, 311, 257, 688, 295, 2793, 337, 257, 588, 1359, 1901, 13, 823, 294, 264, 2863, 51060, 51060, 613, 2283, 366, 5883, 1602, 2584, 16979, 370, 436, 434, 3974, 484, 412, 4974, 51222, 51222, 457, 550, 321, 434, 516, 281, 10864, 613, 12240, 29432, 295, 613, 2283, 1228, 300, 51456, 51456, 38377, 13, 407, 1830, 264, 1164, 295, 3097, 295, 341, 18161, 3209, 613, 51638, 51638, 2793, 420, 18875, 366, 516, 281, 1936, 1286, 926, 294, 341, 1901, 293, 291, 1062, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.08740836818043779, "compression_ratio": 1.8287671232876712, "no_speech_prob": 1.0782309800561052e-05}, {"id": 51, "seek": 22636, "start": 226.36, "end": 230.16000000000003, "text": " imagine that for example words that have very similar meanings or there are", "tokens": [50364, 3811, 300, 337, 1365, 2283, 300, 362, 588, 2531, 28138, 420, 456, 366, 50554, 50554, 6451, 5451, 2526, 2592, 295, 1184, 661, 1062, 917, 493, 294, 257, 588, 2531, 644, 295, 264, 1901, 50730, 50730, 293, 2615, 736, 2283, 300, 914, 588, 819, 721, 576, 352, 4079, 1646, 50920, 50920, 294, 264, 1901, 13, 823, 641, 15983, 3109, 5911, 307, 14800, 281, 11896, 13, 51160, 51160, 814, 366, 1228, 257, 2120, 388, 11167, 18161, 3209, 281, 6069, 264, 958, 1349, 2212, 51350, 51350, 264, 3894, 2283, 293, 281, 3847, 264, 18161, 3209, 436, 366, 5138, 3319, 264, 51538, 51538, 3565, 22119, 295, 264, 3097, 1412, 445, 411, 321, 630, 13, 407, 264, 15983, 3109, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.10399467662229377, "compression_ratio": 1.8767605633802817, "no_speech_prob": 2.0259763005014975e-06}, {"id": 52, "seek": 22636, "start": 230.16000000000003, "end": 233.68, "text": " indeed synonyms of each other might end up in a very similar part of the space", "tokens": [50364, 3811, 300, 337, 1365, 2283, 300, 362, 588, 2531, 28138, 420, 456, 366, 50554, 50554, 6451, 5451, 2526, 2592, 295, 1184, 661, 1062, 917, 493, 294, 257, 588, 2531, 644, 295, 264, 1901, 50730, 50730, 293, 2615, 736, 2283, 300, 914, 588, 819, 721, 576, 352, 4079, 1646, 50920, 50920, 294, 264, 1901, 13, 823, 641, 15983, 3109, 5911, 307, 14800, 281, 11896, 13, 51160, 51160, 814, 366, 1228, 257, 2120, 388, 11167, 18161, 3209, 281, 6069, 264, 958, 1349, 2212, 51350, 51350, 264, 3894, 2283, 293, 281, 3847, 264, 18161, 3209, 436, 366, 5138, 3319, 264, 51538, 51538, 3565, 22119, 295, 264, 3097, 1412, 445, 411, 321, 630, 13, 407, 264, 15983, 3109, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.10399467662229377, "compression_ratio": 1.8767605633802817, "no_speech_prob": 2.0259763005014975e-06}, {"id": 53, "seek": 22636, "start": 233.68, "end": 237.48000000000002, "text": " and conversely words that mean very different things would go somewhere else", "tokens": [50364, 3811, 300, 337, 1365, 2283, 300, 362, 588, 2531, 28138, 420, 456, 366, 50554, 50554, 6451, 5451, 2526, 2592, 295, 1184, 661, 1062, 917, 493, 294, 257, 588, 2531, 644, 295, 264, 1901, 50730, 50730, 293, 2615, 736, 2283, 300, 914, 588, 819, 721, 576, 352, 4079, 1646, 50920, 50920, 294, 264, 1901, 13, 823, 641, 15983, 3109, 5911, 307, 14800, 281, 11896, 13, 51160, 51160, 814, 366, 1228, 257, 2120, 388, 11167, 18161, 3209, 281, 6069, 264, 958, 1349, 2212, 51350, 51350, 264, 3894, 2283, 293, 281, 3847, 264, 18161, 3209, 436, 366, 5138, 3319, 264, 51538, 51538, 3565, 22119, 295, 264, 3097, 1412, 445, 411, 321, 630, 13, 407, 264, 15983, 3109, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.10399467662229377, "compression_ratio": 1.8767605633802817, "no_speech_prob": 2.0259763005014975e-06}, {"id": 54, "seek": 22636, "start": 237.48000000000002, "end": 242.28, "text": " in the space. Now their modeling approach otherwise is identical to ours.", "tokens": [50364, 3811, 300, 337, 1365, 2283, 300, 362, 588, 2531, 28138, 420, 456, 366, 50554, 50554, 6451, 5451, 2526, 2592, 295, 1184, 661, 1062, 917, 493, 294, 257, 588, 2531, 644, 295, 264, 1901, 50730, 50730, 293, 2615, 736, 2283, 300, 914, 588, 819, 721, 576, 352, 4079, 1646, 50920, 50920, 294, 264, 1901, 13, 823, 641, 15983, 3109, 5911, 307, 14800, 281, 11896, 13, 51160, 51160, 814, 366, 1228, 257, 2120, 388, 11167, 18161, 3209, 281, 6069, 264, 958, 1349, 2212, 51350, 51350, 264, 3894, 2283, 293, 281, 3847, 264, 18161, 3209, 436, 366, 5138, 3319, 264, 51538, 51538, 3565, 22119, 295, 264, 3097, 1412, 445, 411, 321, 630, 13, 407, 264, 15983, 3109, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.10399467662229377, "compression_ratio": 1.8767605633802817, "no_speech_prob": 2.0259763005014975e-06}, {"id": 55, "seek": 22636, "start": 242.28, "end": 246.08, "text": " They are using a multilayer neural network to predict the next word given", "tokens": [50364, 3811, 300, 337, 1365, 2283, 300, 362, 588, 2531, 28138, 420, 456, 366, 50554, 50554, 6451, 5451, 2526, 2592, 295, 1184, 661, 1062, 917, 493, 294, 257, 588, 2531, 644, 295, 264, 1901, 50730, 50730, 293, 2615, 736, 2283, 300, 914, 588, 819, 721, 576, 352, 4079, 1646, 50920, 50920, 294, 264, 1901, 13, 823, 641, 15983, 3109, 5911, 307, 14800, 281, 11896, 13, 51160, 51160, 814, 366, 1228, 257, 2120, 388, 11167, 18161, 3209, 281, 6069, 264, 958, 1349, 2212, 51350, 51350, 264, 3894, 2283, 293, 281, 3847, 264, 18161, 3209, 436, 366, 5138, 3319, 264, 51538, 51538, 3565, 22119, 295, 264, 3097, 1412, 445, 411, 321, 630, 13, 407, 264, 15983, 3109, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.10399467662229377, "compression_ratio": 1.8767605633802817, "no_speech_prob": 2.0259763005014975e-06}, {"id": 56, "seek": 22636, "start": 246.08, "end": 249.84, "text": " the previous words and to train the neural network they are maximizing the", "tokens": [50364, 3811, 300, 337, 1365, 2283, 300, 362, 588, 2531, 28138, 420, 456, 366, 50554, 50554, 6451, 5451, 2526, 2592, 295, 1184, 661, 1062, 917, 493, 294, 257, 588, 2531, 644, 295, 264, 1901, 50730, 50730, 293, 2615, 736, 2283, 300, 914, 588, 819, 721, 576, 352, 4079, 1646, 50920, 50920, 294, 264, 1901, 13, 823, 641, 15983, 3109, 5911, 307, 14800, 281, 11896, 13, 51160, 51160, 814, 366, 1228, 257, 2120, 388, 11167, 18161, 3209, 281, 6069, 264, 958, 1349, 2212, 51350, 51350, 264, 3894, 2283, 293, 281, 3847, 264, 18161, 3209, 436, 366, 5138, 3319, 264, 51538, 51538, 3565, 22119, 295, 264, 3097, 1412, 445, 411, 321, 630, 13, 407, 264, 15983, 3109, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.10399467662229377, "compression_ratio": 1.8767605633802817, "no_speech_prob": 2.0259763005014975e-06}, {"id": 57, "seek": 22636, "start": 249.84, "end": 253.88000000000002, "text": " log likelihood of the training data just like we did. So the modeling approach", "tokens": [50364, 3811, 300, 337, 1365, 2283, 300, 362, 588, 2531, 28138, 420, 456, 366, 50554, 50554, 6451, 5451, 2526, 2592, 295, 1184, 661, 1062, 917, 493, 294, 257, 588, 2531, 644, 295, 264, 1901, 50730, 50730, 293, 2615, 736, 2283, 300, 914, 588, 819, 721, 576, 352, 4079, 1646, 50920, 50920, 294, 264, 1901, 13, 823, 641, 15983, 3109, 5911, 307, 14800, 281, 11896, 13, 51160, 51160, 814, 366, 1228, 257, 2120, 388, 11167, 18161, 3209, 281, 6069, 264, 958, 1349, 2212, 51350, 51350, 264, 3894, 2283, 293, 281, 3847, 264, 18161, 3209, 436, 366, 5138, 3319, 264, 51538, 51538, 3565, 22119, 295, 264, 3097, 1412, 445, 411, 321, 630, 13, 407, 264, 15983, 3109, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.10399467662229377, "compression_ratio": 1.8767605633802817, "no_speech_prob": 2.0259763005014975e-06}, {"id": 58, "seek": 25388, "start": 253.88, "end": 259.15999999999997, "text": " itself is identical. Now here they have a concrete example of this intuition. Why", "tokens": [50364, 2564, 307, 14800, 13, 823, 510, 436, 362, 257, 9859, 1365, 295, 341, 24002, 13, 1545, 50628, 50628, 775, 309, 589, 30, 8537, 7297, 300, 337, 1365, 291, 366, 1382, 281, 6069, 257, 3000, 50834, 50834, 390, 2614, 294, 257, 8247, 13, 823, 7297, 300, 264, 1900, 9535, 257, 3000, 390, 2614, 294, 257, 51126, 51126, 575, 1128, 11068, 294, 257, 3097, 1412, 293, 510, 291, 366, 412, 1333, 295, 1500, 565, 51370, 51370, 1780, 562, 264, 2316, 307, 17826, 4079, 293, 309, 311, 1382, 281, 652, 257, 51554, 51554, 8174, 293, 309, 311, 1566, 257, 3000, 390, 2614, 294, 257, 8247, 293, 570, 309, 311, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.0787651235407049, "compression_ratio": 1.8658536585365855, "no_speech_prob": 1.922273077070713e-05}, {"id": 59, "seek": 25388, "start": 259.15999999999997, "end": 263.28, "text": " does it work? Basically suppose that for example you are trying to predict a dog", "tokens": [50364, 2564, 307, 14800, 13, 823, 510, 436, 362, 257, 9859, 1365, 295, 341, 24002, 13, 1545, 50628, 50628, 775, 309, 589, 30, 8537, 7297, 300, 337, 1365, 291, 366, 1382, 281, 6069, 257, 3000, 50834, 50834, 390, 2614, 294, 257, 8247, 13, 823, 7297, 300, 264, 1900, 9535, 257, 3000, 390, 2614, 294, 257, 51126, 51126, 575, 1128, 11068, 294, 257, 3097, 1412, 293, 510, 291, 366, 412, 1333, 295, 1500, 565, 51370, 51370, 1780, 562, 264, 2316, 307, 17826, 4079, 293, 309, 311, 1382, 281, 652, 257, 51554, 51554, 8174, 293, 309, 311, 1566, 257, 3000, 390, 2614, 294, 257, 8247, 293, 570, 309, 311, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.0787651235407049, "compression_ratio": 1.8658536585365855, "no_speech_prob": 1.922273077070713e-05}, {"id": 60, "seek": 25388, "start": 263.28, "end": 269.12, "text": " was running in a blank. Now suppose that the exact phrase a dog was running in a", "tokens": [50364, 2564, 307, 14800, 13, 823, 510, 436, 362, 257, 9859, 1365, 295, 341, 24002, 13, 1545, 50628, 50628, 775, 309, 589, 30, 8537, 7297, 300, 337, 1365, 291, 366, 1382, 281, 6069, 257, 3000, 50834, 50834, 390, 2614, 294, 257, 8247, 13, 823, 7297, 300, 264, 1900, 9535, 257, 3000, 390, 2614, 294, 257, 51126, 51126, 575, 1128, 11068, 294, 257, 3097, 1412, 293, 510, 291, 366, 412, 1333, 295, 1500, 565, 51370, 51370, 1780, 562, 264, 2316, 307, 17826, 4079, 293, 309, 311, 1382, 281, 652, 257, 51554, 51554, 8174, 293, 309, 311, 1566, 257, 3000, 390, 2614, 294, 257, 8247, 293, 570, 309, 311, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.0787651235407049, "compression_ratio": 1.8658536585365855, "no_speech_prob": 1.922273077070713e-05}, {"id": 61, "seek": 25388, "start": 269.12, "end": 274.0, "text": " has never occurred in a training data and here you are at sort of test time", "tokens": [50364, 2564, 307, 14800, 13, 823, 510, 436, 362, 257, 9859, 1365, 295, 341, 24002, 13, 1545, 50628, 50628, 775, 309, 589, 30, 8537, 7297, 300, 337, 1365, 291, 366, 1382, 281, 6069, 257, 3000, 50834, 50834, 390, 2614, 294, 257, 8247, 13, 823, 7297, 300, 264, 1900, 9535, 257, 3000, 390, 2614, 294, 257, 51126, 51126, 575, 1128, 11068, 294, 257, 3097, 1412, 293, 510, 291, 366, 412, 1333, 295, 1500, 565, 51370, 51370, 1780, 562, 264, 2316, 307, 17826, 4079, 293, 309, 311, 1382, 281, 652, 257, 51554, 51554, 8174, 293, 309, 311, 1566, 257, 3000, 390, 2614, 294, 257, 8247, 293, 570, 309, 311, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.0787651235407049, "compression_ratio": 1.8658536585365855, "no_speech_prob": 1.922273077070713e-05}, {"id": 62, "seek": 25388, "start": 274.0, "end": 277.68, "text": " later when the model is deployed somewhere and it's trying to make a", "tokens": [50364, 2564, 307, 14800, 13, 823, 510, 436, 362, 257, 9859, 1365, 295, 341, 24002, 13, 1545, 50628, 50628, 775, 309, 589, 30, 8537, 7297, 300, 337, 1365, 291, 366, 1382, 281, 6069, 257, 3000, 50834, 50834, 390, 2614, 294, 257, 8247, 13, 823, 7297, 300, 264, 1900, 9535, 257, 3000, 390, 2614, 294, 257, 51126, 51126, 575, 1128, 11068, 294, 257, 3097, 1412, 293, 510, 291, 366, 412, 1333, 295, 1500, 565, 51370, 51370, 1780, 562, 264, 2316, 307, 17826, 4079, 293, 309, 311, 1382, 281, 652, 257, 51554, 51554, 8174, 293, 309, 311, 1566, 257, 3000, 390, 2614, 294, 257, 8247, 293, 570, 309, 311, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.0787651235407049, "compression_ratio": 1.8658536585365855, "no_speech_prob": 1.922273077070713e-05}, {"id": 63, "seek": 25388, "start": 277.68, "end": 282.15999999999997, "text": " sentence and it's saying a dog was running in a blank and because it's", "tokens": [50364, 2564, 307, 14800, 13, 823, 510, 436, 362, 257, 9859, 1365, 295, 341, 24002, 13, 1545, 50628, 50628, 775, 309, 589, 30, 8537, 7297, 300, 337, 1365, 291, 366, 1382, 281, 6069, 257, 3000, 50834, 50834, 390, 2614, 294, 257, 8247, 13, 823, 7297, 300, 264, 1900, 9535, 257, 3000, 390, 2614, 294, 257, 51126, 51126, 575, 1128, 11068, 294, 257, 3097, 1412, 293, 510, 291, 366, 412, 1333, 295, 1500, 565, 51370, 51370, 1780, 562, 264, 2316, 307, 17826, 4079, 293, 309, 311, 1382, 281, 652, 257, 51554, 51554, 8174, 293, 309, 311, 1566, 257, 3000, 390, 2614, 294, 257, 8247, 293, 570, 309, 311, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.0787651235407049, "compression_ratio": 1.8658536585365855, "no_speech_prob": 1.922273077070713e-05}, {"id": 64, "seek": 28216, "start": 282.16, "end": 285.76000000000005, "text": " never encountered this exact phrase in the training set you're out of", "tokens": [50364, 1128, 20381, 341, 1900, 9535, 294, 264, 3097, 992, 291, 434, 484, 295, 50544, 50544, 7316, 382, 321, 584, 13, 1743, 291, 500, 380, 362, 17879, 604, 1778, 281, 9091, 50836, 50836, 437, 1062, 808, 958, 457, 341, 3109, 767, 4045, 291, 281, 483, 926, 300, 51080, 51080, 570, 1310, 291, 994, 380, 536, 264, 1900, 9535, 257, 3000, 390, 2614, 294, 257, 746, 51276, 51276, 457, 1310, 291, 600, 1612, 2531, 20312, 1310, 291, 600, 1612, 264, 9535, 264, 3000, 390, 51474, 51474, 2614, 294, 257, 8247, 293, 1310, 428, 3209, 575, 3264, 300, 257, 293, 264, 366, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10722964018293955, "compression_ratio": 1.832, "no_speech_prob": 5.862698344571982e-06}, {"id": 65, "seek": 28216, "start": 285.76000000000005, "end": 291.6, "text": " distribution as we say. Like you don't have fundamentally any reason to suspect", "tokens": [50364, 1128, 20381, 341, 1900, 9535, 294, 264, 3097, 992, 291, 434, 484, 295, 50544, 50544, 7316, 382, 321, 584, 13, 1743, 291, 500, 380, 362, 17879, 604, 1778, 281, 9091, 50836, 50836, 437, 1062, 808, 958, 457, 341, 3109, 767, 4045, 291, 281, 483, 926, 300, 51080, 51080, 570, 1310, 291, 994, 380, 536, 264, 1900, 9535, 257, 3000, 390, 2614, 294, 257, 746, 51276, 51276, 457, 1310, 291, 600, 1612, 2531, 20312, 1310, 291, 600, 1612, 264, 9535, 264, 3000, 390, 51474, 51474, 2614, 294, 257, 8247, 293, 1310, 428, 3209, 575, 3264, 300, 257, 293, 264, 366, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10722964018293955, "compression_ratio": 1.832, "no_speech_prob": 5.862698344571982e-06}, {"id": 66, "seek": 28216, "start": 291.6, "end": 296.48, "text": " what might come next but this approach actually allows you to get around that", "tokens": [50364, 1128, 20381, 341, 1900, 9535, 294, 264, 3097, 992, 291, 434, 484, 295, 50544, 50544, 7316, 382, 321, 584, 13, 1743, 291, 500, 380, 362, 17879, 604, 1778, 281, 9091, 50836, 50836, 437, 1062, 808, 958, 457, 341, 3109, 767, 4045, 291, 281, 483, 926, 300, 51080, 51080, 570, 1310, 291, 994, 380, 536, 264, 1900, 9535, 257, 3000, 390, 2614, 294, 257, 746, 51276, 51276, 457, 1310, 291, 600, 1612, 2531, 20312, 1310, 291, 600, 1612, 264, 9535, 264, 3000, 390, 51474, 51474, 2614, 294, 257, 8247, 293, 1310, 428, 3209, 575, 3264, 300, 257, 293, 264, 366, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10722964018293955, "compression_ratio": 1.832, "no_speech_prob": 5.862698344571982e-06}, {"id": 67, "seek": 28216, "start": 296.48, "end": 300.40000000000003, "text": " because maybe you didn't see the exact phrase a dog was running in a something", "tokens": [50364, 1128, 20381, 341, 1900, 9535, 294, 264, 3097, 992, 291, 434, 484, 295, 50544, 50544, 7316, 382, 321, 584, 13, 1743, 291, 500, 380, 362, 17879, 604, 1778, 281, 9091, 50836, 50836, 437, 1062, 808, 958, 457, 341, 3109, 767, 4045, 291, 281, 483, 926, 300, 51080, 51080, 570, 1310, 291, 994, 380, 536, 264, 1900, 9535, 257, 3000, 390, 2614, 294, 257, 746, 51276, 51276, 457, 1310, 291, 600, 1612, 2531, 20312, 1310, 291, 600, 1612, 264, 9535, 264, 3000, 390, 51474, 51474, 2614, 294, 257, 8247, 293, 1310, 428, 3209, 575, 3264, 300, 257, 293, 264, 366, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10722964018293955, "compression_ratio": 1.832, "no_speech_prob": 5.862698344571982e-06}, {"id": 68, "seek": 28216, "start": 300.40000000000003, "end": 304.36, "text": " but maybe you've seen similar phrases maybe you've seen the phrase the dog was", "tokens": [50364, 1128, 20381, 341, 1900, 9535, 294, 264, 3097, 992, 291, 434, 484, 295, 50544, 50544, 7316, 382, 321, 584, 13, 1743, 291, 500, 380, 362, 17879, 604, 1778, 281, 9091, 50836, 50836, 437, 1062, 808, 958, 457, 341, 3109, 767, 4045, 291, 281, 483, 926, 300, 51080, 51080, 570, 1310, 291, 994, 380, 536, 264, 1900, 9535, 257, 3000, 390, 2614, 294, 257, 746, 51276, 51276, 457, 1310, 291, 600, 1612, 2531, 20312, 1310, 291, 600, 1612, 264, 9535, 264, 3000, 390, 51474, 51474, 2614, 294, 257, 8247, 293, 1310, 428, 3209, 575, 3264, 300, 257, 293, 264, 366, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10722964018293955, "compression_ratio": 1.832, "no_speech_prob": 5.862698344571982e-06}, {"id": 69, "seek": 28216, "start": 304.36, "end": 309.16, "text": " running in a blank and maybe your network has learned that a and the are", "tokens": [50364, 1128, 20381, 341, 1900, 9535, 294, 264, 3097, 992, 291, 434, 484, 295, 50544, 50544, 7316, 382, 321, 584, 13, 1743, 291, 500, 380, 362, 17879, 604, 1778, 281, 9091, 50836, 50836, 437, 1062, 808, 958, 457, 341, 3109, 767, 4045, 291, 281, 483, 926, 300, 51080, 51080, 570, 1310, 291, 994, 380, 536, 264, 1900, 9535, 257, 3000, 390, 2614, 294, 257, 746, 51276, 51276, 457, 1310, 291, 600, 1612, 2531, 20312, 1310, 291, 600, 1612, 264, 9535, 264, 3000, 390, 51474, 51474, 2614, 294, 257, 8247, 293, 1310, 428, 3209, 575, 3264, 300, 257, 293, 264, 366, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10722964018293955, "compression_ratio": 1.832, "no_speech_prob": 5.862698344571982e-06}, {"id": 70, "seek": 30916, "start": 309.16, "end": 313.12, "text": " like frequently are interchangeable with each other and so maybe it took the", "tokens": [50364, 411, 10374, 366, 30358, 712, 365, 1184, 661, 293, 370, 1310, 309, 1890, 264, 50562, 50562, 12240, 3584, 337, 257, 293, 264, 12240, 3584, 337, 264, 293, 309, 767, 829, 552, 411, 11184, 50760, 50760, 1184, 661, 294, 264, 1901, 293, 370, 291, 393, 5003, 3601, 807, 300, 12240, 3584, 50964, 50964, 293, 291, 393, 2674, 1125, 294, 300, 636, 13, 13157, 264, 3209, 727, 458, 300, 51148, 51148, 11111, 293, 7197, 366, 4882, 293, 436, 598, 12, 905, 14112, 294, 3195, 295, 588, 2531, 30628, 51370, 51370, 293, 370, 754, 1673, 291, 2378, 380, 1612, 341, 1900, 9535, 420, 498, 291, 2378, 380, 1612, 51578, 51578, 2293, 4494, 420, 2614, 291, 393, 807, 264, 12240, 3584, 1901, 5003, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.08685799730502493, "compression_ratio": 1.9280575539568345, "no_speech_prob": 1.593594060977921e-05}, {"id": 71, "seek": 30916, "start": 313.12, "end": 317.08000000000004, "text": " embedding for a and the embedding for the and it actually put them like nearby", "tokens": [50364, 411, 10374, 366, 30358, 712, 365, 1184, 661, 293, 370, 1310, 309, 1890, 264, 50562, 50562, 12240, 3584, 337, 257, 293, 264, 12240, 3584, 337, 264, 293, 309, 767, 829, 552, 411, 11184, 50760, 50760, 1184, 661, 294, 264, 1901, 293, 370, 291, 393, 5003, 3601, 807, 300, 12240, 3584, 50964, 50964, 293, 291, 393, 2674, 1125, 294, 300, 636, 13, 13157, 264, 3209, 727, 458, 300, 51148, 51148, 11111, 293, 7197, 366, 4882, 293, 436, 598, 12, 905, 14112, 294, 3195, 295, 588, 2531, 30628, 51370, 51370, 293, 370, 754, 1673, 291, 2378, 380, 1612, 341, 1900, 9535, 420, 498, 291, 2378, 380, 1612, 51578, 51578, 2293, 4494, 420, 2614, 291, 393, 807, 264, 12240, 3584, 1901, 5003, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.08685799730502493, "compression_ratio": 1.9280575539568345, "no_speech_prob": 1.593594060977921e-05}, {"id": 72, "seek": 30916, "start": 317.08000000000004, "end": 321.16, "text": " each other in the space and so you can transfer knowledge through that embedding", "tokens": [50364, 411, 10374, 366, 30358, 712, 365, 1184, 661, 293, 370, 1310, 309, 1890, 264, 50562, 50562, 12240, 3584, 337, 257, 293, 264, 12240, 3584, 337, 264, 293, 309, 767, 829, 552, 411, 11184, 50760, 50760, 1184, 661, 294, 264, 1901, 293, 370, 291, 393, 5003, 3601, 807, 300, 12240, 3584, 50964, 50964, 293, 291, 393, 2674, 1125, 294, 300, 636, 13, 13157, 264, 3209, 727, 458, 300, 51148, 51148, 11111, 293, 7197, 366, 4882, 293, 436, 598, 12, 905, 14112, 294, 3195, 295, 588, 2531, 30628, 51370, 51370, 293, 370, 754, 1673, 291, 2378, 380, 1612, 341, 1900, 9535, 420, 498, 291, 2378, 380, 1612, 51578, 51578, 2293, 4494, 420, 2614, 291, 393, 807, 264, 12240, 3584, 1901, 5003, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.08685799730502493, "compression_ratio": 1.9280575539568345, "no_speech_prob": 1.593594060977921e-05}, {"id": 73, "seek": 30916, "start": 321.16, "end": 324.84000000000003, "text": " and you can generalize in that way. Similarly the network could know that", "tokens": [50364, 411, 10374, 366, 30358, 712, 365, 1184, 661, 293, 370, 1310, 309, 1890, 264, 50562, 50562, 12240, 3584, 337, 257, 293, 264, 12240, 3584, 337, 264, 293, 309, 767, 829, 552, 411, 11184, 50760, 50760, 1184, 661, 294, 264, 1901, 293, 370, 291, 393, 5003, 3601, 807, 300, 12240, 3584, 50964, 50964, 293, 291, 393, 2674, 1125, 294, 300, 636, 13, 13157, 264, 3209, 727, 458, 300, 51148, 51148, 11111, 293, 7197, 366, 4882, 293, 436, 598, 12, 905, 14112, 294, 3195, 295, 588, 2531, 30628, 51370, 51370, 293, 370, 754, 1673, 291, 2378, 380, 1612, 341, 1900, 9535, 420, 498, 291, 2378, 380, 1612, 51578, 51578, 2293, 4494, 420, 2614, 291, 393, 807, 264, 12240, 3584, 1901, 5003, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.08685799730502493, "compression_ratio": 1.9280575539568345, "no_speech_prob": 1.593594060977921e-05}, {"id": 74, "seek": 30916, "start": 324.84000000000003, "end": 329.28000000000003, "text": " cats and dogs are animals and they co-occur in lots of very similar contexts", "tokens": [50364, 411, 10374, 366, 30358, 712, 365, 1184, 661, 293, 370, 1310, 309, 1890, 264, 50562, 50562, 12240, 3584, 337, 257, 293, 264, 12240, 3584, 337, 264, 293, 309, 767, 829, 552, 411, 11184, 50760, 50760, 1184, 661, 294, 264, 1901, 293, 370, 291, 393, 5003, 3601, 807, 300, 12240, 3584, 50964, 50964, 293, 291, 393, 2674, 1125, 294, 300, 636, 13, 13157, 264, 3209, 727, 458, 300, 51148, 51148, 11111, 293, 7197, 366, 4882, 293, 436, 598, 12, 905, 14112, 294, 3195, 295, 588, 2531, 30628, 51370, 51370, 293, 370, 754, 1673, 291, 2378, 380, 1612, 341, 1900, 9535, 420, 498, 291, 2378, 380, 1612, 51578, 51578, 2293, 4494, 420, 2614, 291, 393, 807, 264, 12240, 3584, 1901, 5003, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.08685799730502493, "compression_ratio": 1.9280575539568345, "no_speech_prob": 1.593594060977921e-05}, {"id": 75, "seek": 30916, "start": 329.28000000000003, "end": 333.44000000000005, "text": " and so even though you haven't seen this exact phrase or if you haven't seen", "tokens": [50364, 411, 10374, 366, 30358, 712, 365, 1184, 661, 293, 370, 1310, 309, 1890, 264, 50562, 50562, 12240, 3584, 337, 257, 293, 264, 12240, 3584, 337, 264, 293, 309, 767, 829, 552, 411, 11184, 50760, 50760, 1184, 661, 294, 264, 1901, 293, 370, 291, 393, 5003, 3601, 807, 300, 12240, 3584, 50964, 50964, 293, 291, 393, 2674, 1125, 294, 300, 636, 13, 13157, 264, 3209, 727, 458, 300, 51148, 51148, 11111, 293, 7197, 366, 4882, 293, 436, 598, 12, 905, 14112, 294, 3195, 295, 588, 2531, 30628, 51370, 51370, 293, 370, 754, 1673, 291, 2378, 380, 1612, 341, 1900, 9535, 420, 498, 291, 2378, 380, 1612, 51578, 51578, 2293, 4494, 420, 2614, 291, 393, 807, 264, 12240, 3584, 1901, 5003, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.08685799730502493, "compression_ratio": 1.9280575539568345, "no_speech_prob": 1.593594060977921e-05}, {"id": 76, "seek": 30916, "start": 333.44000000000005, "end": 338.08000000000004, "text": " exactly walking or running you can through the embedding space transfer", "tokens": [50364, 411, 10374, 366, 30358, 712, 365, 1184, 661, 293, 370, 1310, 309, 1890, 264, 50562, 50562, 12240, 3584, 337, 257, 293, 264, 12240, 3584, 337, 264, 293, 309, 767, 829, 552, 411, 11184, 50760, 50760, 1184, 661, 294, 264, 1901, 293, 370, 291, 393, 5003, 3601, 807, 300, 12240, 3584, 50964, 50964, 293, 291, 393, 2674, 1125, 294, 300, 636, 13, 13157, 264, 3209, 727, 458, 300, 51148, 51148, 11111, 293, 7197, 366, 4882, 293, 436, 598, 12, 905, 14112, 294, 3195, 295, 588, 2531, 30628, 51370, 51370, 293, 370, 754, 1673, 291, 2378, 380, 1612, 341, 1900, 9535, 420, 498, 291, 2378, 380, 1612, 51578, 51578, 2293, 4494, 420, 2614, 291, 393, 807, 264, 12240, 3584, 1901, 5003, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.08685799730502493, "compression_ratio": 1.9280575539568345, "no_speech_prob": 1.593594060977921e-05}, {"id": 77, "seek": 33808, "start": 338.08, "end": 343.0, "text": " knowledge and you can generalize to novel scenarios. So let's now scroll down", "tokens": [50364, 3601, 293, 291, 393, 2674, 1125, 281, 7613, 15077, 13, 407, 718, 311, 586, 11369, 760, 50610, 50610, 281, 264, 10686, 295, 264, 18161, 3209, 436, 362, 257, 1481, 10686, 510, 293, 294, 50856, 50856, 341, 1365, 321, 366, 1940, 1045, 3894, 2283, 293, 321, 366, 1382, 281, 51086, 51086, 6069, 264, 6409, 1349, 294, 257, 8310, 13, 823, 613, 1045, 3894, 2283, 382, 286, 51352, 51352, 2835, 321, 362, 257, 19864, 295, 3282, 11, 1360, 1944, 2283, 370, 633, 472, 295, 613, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.09127366953882678, "compression_ratio": 1.6431718061674008, "no_speech_prob": 1.2410532690410037e-05}, {"id": 78, "seek": 33808, "start": 343.0, "end": 347.91999999999996, "text": " to the diagram of the neural network they have a nice diagram here and in", "tokens": [50364, 3601, 293, 291, 393, 2674, 1125, 281, 7613, 15077, 13, 407, 718, 311, 586, 11369, 760, 50610, 50610, 281, 264, 10686, 295, 264, 18161, 3209, 436, 362, 257, 1481, 10686, 510, 293, 294, 50856, 50856, 341, 1365, 321, 366, 1940, 1045, 3894, 2283, 293, 321, 366, 1382, 281, 51086, 51086, 6069, 264, 6409, 1349, 294, 257, 8310, 13, 823, 613, 1045, 3894, 2283, 382, 286, 51352, 51352, 2835, 321, 362, 257, 19864, 295, 3282, 11, 1360, 1944, 2283, 370, 633, 472, 295, 613, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.09127366953882678, "compression_ratio": 1.6431718061674008, "no_speech_prob": 1.2410532690410037e-05}, {"id": 79, "seek": 33808, "start": 347.91999999999996, "end": 352.52, "text": " this example we are taking three previous words and we are trying to", "tokens": [50364, 3601, 293, 291, 393, 2674, 1125, 281, 7613, 15077, 13, 407, 718, 311, 586, 11369, 760, 50610, 50610, 281, 264, 10686, 295, 264, 18161, 3209, 436, 362, 257, 1481, 10686, 510, 293, 294, 50856, 50856, 341, 1365, 321, 366, 1940, 1045, 3894, 2283, 293, 321, 366, 1382, 281, 51086, 51086, 6069, 264, 6409, 1349, 294, 257, 8310, 13, 823, 613, 1045, 3894, 2283, 382, 286, 51352, 51352, 2835, 321, 362, 257, 19864, 295, 3282, 11, 1360, 1944, 2283, 370, 633, 472, 295, 613, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.09127366953882678, "compression_ratio": 1.6431718061674008, "no_speech_prob": 1.2410532690410037e-05}, {"id": 80, "seek": 33808, "start": 352.52, "end": 357.84, "text": " predict the fourth word in a sequence. Now these three previous words as I", "tokens": [50364, 3601, 293, 291, 393, 2674, 1125, 281, 7613, 15077, 13, 407, 718, 311, 586, 11369, 760, 50610, 50610, 281, 264, 10686, 295, 264, 18161, 3209, 436, 362, 257, 1481, 10686, 510, 293, 294, 50856, 50856, 341, 1365, 321, 366, 1940, 1045, 3894, 2283, 293, 321, 366, 1382, 281, 51086, 51086, 6069, 264, 6409, 1349, 294, 257, 8310, 13, 823, 613, 1045, 3894, 2283, 382, 286, 51352, 51352, 2835, 321, 362, 257, 19864, 295, 3282, 11, 1360, 1944, 2283, 370, 633, 472, 295, 613, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.09127366953882678, "compression_ratio": 1.6431718061674008, "no_speech_prob": 1.2410532690410037e-05}, {"id": 81, "seek": 33808, "start": 357.84, "end": 364.36, "text": " mentioned we have a vocabulary of 17,000 possible words so every one of these", "tokens": [50364, 3601, 293, 291, 393, 2674, 1125, 281, 7613, 15077, 13, 407, 718, 311, 586, 11369, 760, 50610, 50610, 281, 264, 10686, 295, 264, 18161, 3209, 436, 362, 257, 1481, 10686, 510, 293, 294, 50856, 50856, 341, 1365, 321, 366, 1940, 1045, 3894, 2283, 293, 321, 366, 1382, 281, 51086, 51086, 6069, 264, 6409, 1349, 294, 257, 8310, 13, 823, 613, 1045, 3894, 2283, 382, 286, 51352, 51352, 2835, 321, 362, 257, 19864, 295, 3282, 11, 1360, 1944, 2283, 370, 633, 472, 295, 613, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.09127366953882678, "compression_ratio": 1.6431718061674008, "no_speech_prob": 1.2410532690410037e-05}, {"id": 82, "seek": 36436, "start": 364.36, "end": 371.16, "text": " basically are the index of the incoming word and because there are 17,000 words", "tokens": [50364, 1936, 366, 264, 8186, 295, 264, 22341, 1349, 293, 570, 456, 366, 3282, 11, 1360, 2283, 50704, 50704, 341, 307, 364, 24922, 1296, 1958, 293, 3165, 11, 49017, 13, 823, 456, 311, 611, 257, 574, 1010, 3199, 300, 51120, 51120, 436, 818, 383, 13, 639, 574, 1010, 3199, 307, 257, 8141, 300, 307, 3282, 11, 1360, 538, 584, 2217, 293, 51426, 51426, 1936, 437, 321, 434, 884, 510, 307, 321, 434, 15083, 341, 382, 257, 574, 1010, 3199, 293, 370, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.11203403699965704, "compression_ratio": 1.6612903225806452, "no_speech_prob": 8.800752766546793e-06}, {"id": 83, "seek": 36436, "start": 371.16, "end": 379.48, "text": " this is an integer between 0 and 16,999. Now there's also a lookup table that", "tokens": [50364, 1936, 366, 264, 8186, 295, 264, 22341, 1349, 293, 570, 456, 366, 3282, 11, 1360, 2283, 50704, 50704, 341, 307, 364, 24922, 1296, 1958, 293, 3165, 11, 49017, 13, 823, 456, 311, 611, 257, 574, 1010, 3199, 300, 51120, 51120, 436, 818, 383, 13, 639, 574, 1010, 3199, 307, 257, 8141, 300, 307, 3282, 11, 1360, 538, 584, 2217, 293, 51426, 51426, 1936, 437, 321, 434, 884, 510, 307, 321, 434, 15083, 341, 382, 257, 574, 1010, 3199, 293, 370, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.11203403699965704, "compression_ratio": 1.6612903225806452, "no_speech_prob": 8.800752766546793e-06}, {"id": 84, "seek": 36436, "start": 379.48, "end": 385.6, "text": " they call C. This lookup table is a matrix that is 17,000 by say 30 and", "tokens": [50364, 1936, 366, 264, 8186, 295, 264, 22341, 1349, 293, 570, 456, 366, 3282, 11, 1360, 2283, 50704, 50704, 341, 307, 364, 24922, 1296, 1958, 293, 3165, 11, 49017, 13, 823, 456, 311, 611, 257, 574, 1010, 3199, 300, 51120, 51120, 436, 818, 383, 13, 639, 574, 1010, 3199, 307, 257, 8141, 300, 307, 3282, 11, 1360, 538, 584, 2217, 293, 51426, 51426, 1936, 437, 321, 434, 884, 510, 307, 321, 434, 15083, 341, 382, 257, 574, 1010, 3199, 293, 370, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.11203403699965704, "compression_ratio": 1.6612903225806452, "no_speech_prob": 8.800752766546793e-06}, {"id": 85, "seek": 36436, "start": 385.6, "end": 390.04, "text": " basically what we're doing here is we're treating this as a lookup table and so", "tokens": [50364, 1936, 366, 264, 8186, 295, 264, 22341, 1349, 293, 570, 456, 366, 3282, 11, 1360, 2283, 50704, 50704, 341, 307, 364, 24922, 1296, 1958, 293, 3165, 11, 49017, 13, 823, 456, 311, 611, 257, 574, 1010, 3199, 300, 51120, 51120, 436, 818, 383, 13, 639, 574, 1010, 3199, 307, 257, 8141, 300, 307, 3282, 11, 1360, 538, 584, 2217, 293, 51426, 51426, 1936, 437, 321, 434, 884, 510, 307, 321, 434, 15083, 341, 382, 257, 574, 1010, 3199, 293, 370, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.11203403699965704, "compression_ratio": 1.6612903225806452, "no_speech_prob": 8.800752766546793e-06}, {"id": 86, "seek": 39004, "start": 390.04, "end": 396.56, "text": " every index is plucking out a row of this embedding matrix so that each index", "tokens": [50364, 633, 8186, 307, 499, 33260, 484, 257, 5386, 295, 341, 12240, 3584, 8141, 370, 300, 1184, 8186, 50690, 50690, 307, 16424, 281, 264, 2217, 18795, 8062, 300, 23249, 281, 264, 12240, 3584, 50888, 50888, 8062, 337, 300, 1349, 13, 407, 510, 321, 362, 264, 4846, 4583, 295, 2217, 22027, 337, 1045, 51216, 51216, 2283, 1455, 493, 4289, 22027, 294, 3217, 293, 510, 436, 434, 1566, 300, 341, 8141, 383, 51484, 51484, 307, 5507, 2108, 439, 264, 2283, 370, 321, 434, 1009, 8186, 278, 666, 264, 912, 8141, 383, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07381995346235193, "compression_ratio": 1.7844036697247707, "no_speech_prob": 1.5055974245115067e-06}, {"id": 87, "seek": 39004, "start": 396.56, "end": 400.52000000000004, "text": " is converted to the 30 dimensional vector that corresponds to the embedding", "tokens": [50364, 633, 8186, 307, 499, 33260, 484, 257, 5386, 295, 341, 12240, 3584, 8141, 370, 300, 1184, 8186, 50690, 50690, 307, 16424, 281, 264, 2217, 18795, 8062, 300, 23249, 281, 264, 12240, 3584, 50888, 50888, 8062, 337, 300, 1349, 13, 407, 510, 321, 362, 264, 4846, 4583, 295, 2217, 22027, 337, 1045, 51216, 51216, 2283, 1455, 493, 4289, 22027, 294, 3217, 293, 510, 436, 434, 1566, 300, 341, 8141, 383, 51484, 51484, 307, 5507, 2108, 439, 264, 2283, 370, 321, 434, 1009, 8186, 278, 666, 264, 912, 8141, 383, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07381995346235193, "compression_ratio": 1.7844036697247707, "no_speech_prob": 1.5055974245115067e-06}, {"id": 88, "seek": 39004, "start": 400.52000000000004, "end": 407.08000000000004, "text": " vector for that word. So here we have the input layer of 30 neurons for three", "tokens": [50364, 633, 8186, 307, 499, 33260, 484, 257, 5386, 295, 341, 12240, 3584, 8141, 370, 300, 1184, 8186, 50690, 50690, 307, 16424, 281, 264, 2217, 18795, 8062, 300, 23249, 281, 264, 12240, 3584, 50888, 50888, 8062, 337, 300, 1349, 13, 407, 510, 321, 362, 264, 4846, 4583, 295, 2217, 22027, 337, 1045, 51216, 51216, 2283, 1455, 493, 4289, 22027, 294, 3217, 293, 510, 436, 434, 1566, 300, 341, 8141, 383, 51484, 51484, 307, 5507, 2108, 439, 264, 2283, 370, 321, 434, 1009, 8186, 278, 666, 264, 912, 8141, 383, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07381995346235193, "compression_ratio": 1.7844036697247707, "no_speech_prob": 1.5055974245115067e-06}, {"id": 89, "seek": 39004, "start": 407.08000000000004, "end": 412.44, "text": " words making up 90 neurons in total and here they're saying that this matrix C", "tokens": [50364, 633, 8186, 307, 499, 33260, 484, 257, 5386, 295, 341, 12240, 3584, 8141, 370, 300, 1184, 8186, 50690, 50690, 307, 16424, 281, 264, 2217, 18795, 8062, 300, 23249, 281, 264, 12240, 3584, 50888, 50888, 8062, 337, 300, 1349, 13, 407, 510, 321, 362, 264, 4846, 4583, 295, 2217, 22027, 337, 1045, 51216, 51216, 2283, 1455, 493, 4289, 22027, 294, 3217, 293, 510, 436, 434, 1566, 300, 341, 8141, 383, 51484, 51484, 307, 5507, 2108, 439, 264, 2283, 370, 321, 434, 1009, 8186, 278, 666, 264, 912, 8141, 383, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07381995346235193, "compression_ratio": 1.7844036697247707, "no_speech_prob": 1.5055974245115067e-06}, {"id": 90, "seek": 39004, "start": 412.44, "end": 417.04, "text": " is shared across all the words so we're always indexing into the same matrix C", "tokens": [50364, 633, 8186, 307, 499, 33260, 484, 257, 5386, 295, 341, 12240, 3584, 8141, 370, 300, 1184, 8186, 50690, 50690, 307, 16424, 281, 264, 2217, 18795, 8062, 300, 23249, 281, 264, 12240, 3584, 50888, 50888, 8062, 337, 300, 1349, 13, 407, 510, 321, 362, 264, 4846, 4583, 295, 2217, 22027, 337, 1045, 51216, 51216, 2283, 1455, 493, 4289, 22027, 294, 3217, 293, 510, 436, 434, 1566, 300, 341, 8141, 383, 51484, 51484, 307, 5507, 2108, 439, 264, 2283, 370, 321, 434, 1009, 8186, 278, 666, 264, 912, 8141, 383, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07381995346235193, "compression_ratio": 1.7844036697247707, "no_speech_prob": 1.5055974245115067e-06}, {"id": 91, "seek": 41704, "start": 417.04, "end": 423.52000000000004, "text": " over and over for each one of these words. Next up is the hidden layer of", "tokens": [50364, 670, 293, 670, 337, 1184, 472, 295, 613, 2283, 13, 3087, 493, 307, 264, 7633, 4583, 295, 50688, 50688, 341, 18161, 3209, 13, 440, 2744, 295, 341, 7633, 18161, 4583, 295, 341, 18161, 5393, 50890, 50890, 307, 257, 9848, 2181, 335, 2398, 370, 321, 764, 264, 1349, 9848, 2181, 335, 2398, 562, 309, 311, 733, 295, 411, 257, 51080, 51080, 1715, 3922, 493, 281, 264, 11795, 295, 264, 18161, 5393, 293, 341, 393, 312, 382, 2416, 382, 51282, 51282, 291, 1116, 411, 420, 382, 1359, 382, 291, 1116, 411, 370, 337, 1365, 264, 2744, 727, 312, 257, 3262, 51444, 51444, 293, 321, 366, 516, 281, 352, 670, 3866, 7994, 295, 264, 2744, 295, 341, 7633, 4583, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.08664623101552328, "compression_ratio": 1.9579831932773109, "no_speech_prob": 3.1691728509031236e-05}, {"id": 92, "seek": 41704, "start": 423.52000000000004, "end": 427.56, "text": " this neural network. The size of this hidden neural layer of this neural nut", "tokens": [50364, 670, 293, 670, 337, 1184, 472, 295, 613, 2283, 13, 3087, 493, 307, 264, 7633, 4583, 295, 50688, 50688, 341, 18161, 3209, 13, 440, 2744, 295, 341, 7633, 18161, 4583, 295, 341, 18161, 5393, 50890, 50890, 307, 257, 9848, 2181, 335, 2398, 370, 321, 764, 264, 1349, 9848, 2181, 335, 2398, 562, 309, 311, 733, 295, 411, 257, 51080, 51080, 1715, 3922, 493, 281, 264, 11795, 295, 264, 18161, 5393, 293, 341, 393, 312, 382, 2416, 382, 51282, 51282, 291, 1116, 411, 420, 382, 1359, 382, 291, 1116, 411, 370, 337, 1365, 264, 2744, 727, 312, 257, 3262, 51444, 51444, 293, 321, 366, 516, 281, 352, 670, 3866, 7994, 295, 264, 2744, 295, 341, 7633, 4583, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.08664623101552328, "compression_ratio": 1.9579831932773109, "no_speech_prob": 3.1691728509031236e-05}, {"id": 93, "seek": 41704, "start": 427.56, "end": 431.36, "text": " is a hyperparameter so we use the word hyperparameter when it's kind of like a", "tokens": [50364, 670, 293, 670, 337, 1184, 472, 295, 613, 2283, 13, 3087, 493, 307, 264, 7633, 4583, 295, 50688, 50688, 341, 18161, 3209, 13, 440, 2744, 295, 341, 7633, 18161, 4583, 295, 341, 18161, 5393, 50890, 50890, 307, 257, 9848, 2181, 335, 2398, 370, 321, 764, 264, 1349, 9848, 2181, 335, 2398, 562, 309, 311, 733, 295, 411, 257, 51080, 51080, 1715, 3922, 493, 281, 264, 11795, 295, 264, 18161, 5393, 293, 341, 393, 312, 382, 2416, 382, 51282, 51282, 291, 1116, 411, 420, 382, 1359, 382, 291, 1116, 411, 370, 337, 1365, 264, 2744, 727, 312, 257, 3262, 51444, 51444, 293, 321, 366, 516, 281, 352, 670, 3866, 7994, 295, 264, 2744, 295, 341, 7633, 4583, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.08664623101552328, "compression_ratio": 1.9579831932773109, "no_speech_prob": 3.1691728509031236e-05}, {"id": 94, "seek": 41704, "start": 431.36, "end": 435.40000000000003, "text": " design choice up to the designer of the neural nut and this can be as large as", "tokens": [50364, 670, 293, 670, 337, 1184, 472, 295, 613, 2283, 13, 3087, 493, 307, 264, 7633, 4583, 295, 50688, 50688, 341, 18161, 3209, 13, 440, 2744, 295, 341, 7633, 18161, 4583, 295, 341, 18161, 5393, 50890, 50890, 307, 257, 9848, 2181, 335, 2398, 370, 321, 764, 264, 1349, 9848, 2181, 335, 2398, 562, 309, 311, 733, 295, 411, 257, 51080, 51080, 1715, 3922, 493, 281, 264, 11795, 295, 264, 18161, 5393, 293, 341, 393, 312, 382, 2416, 382, 51282, 51282, 291, 1116, 411, 420, 382, 1359, 382, 291, 1116, 411, 370, 337, 1365, 264, 2744, 727, 312, 257, 3262, 51444, 51444, 293, 321, 366, 516, 281, 352, 670, 3866, 7994, 295, 264, 2744, 295, 341, 7633, 4583, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.08664623101552328, "compression_ratio": 1.9579831932773109, "no_speech_prob": 3.1691728509031236e-05}, {"id": 95, "seek": 41704, "start": 435.40000000000003, "end": 438.64000000000004, "text": " you'd like or as small as you'd like so for example the size could be a hundred", "tokens": [50364, 670, 293, 670, 337, 1184, 472, 295, 613, 2283, 13, 3087, 493, 307, 264, 7633, 4583, 295, 50688, 50688, 341, 18161, 3209, 13, 440, 2744, 295, 341, 7633, 18161, 4583, 295, 341, 18161, 5393, 50890, 50890, 307, 257, 9848, 2181, 335, 2398, 370, 321, 764, 264, 1349, 9848, 2181, 335, 2398, 562, 309, 311, 733, 295, 411, 257, 51080, 51080, 1715, 3922, 493, 281, 264, 11795, 295, 264, 18161, 5393, 293, 341, 393, 312, 382, 2416, 382, 51282, 51282, 291, 1116, 411, 420, 382, 1359, 382, 291, 1116, 411, 370, 337, 1365, 264, 2744, 727, 312, 257, 3262, 51444, 51444, 293, 321, 366, 516, 281, 352, 670, 3866, 7994, 295, 264, 2744, 295, 341, 7633, 4583, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.08664623101552328, "compression_ratio": 1.9579831932773109, "no_speech_prob": 3.1691728509031236e-05}, {"id": 96, "seek": 41704, "start": 438.64000000000004, "end": 443.12, "text": " and we are going to go over multiple choices of the size of this hidden layer", "tokens": [50364, 670, 293, 670, 337, 1184, 472, 295, 613, 2283, 13, 3087, 493, 307, 264, 7633, 4583, 295, 50688, 50688, 341, 18161, 3209, 13, 440, 2744, 295, 341, 7633, 18161, 4583, 295, 341, 18161, 5393, 50890, 50890, 307, 257, 9848, 2181, 335, 2398, 370, 321, 764, 264, 1349, 9848, 2181, 335, 2398, 562, 309, 311, 733, 295, 411, 257, 51080, 51080, 1715, 3922, 493, 281, 264, 11795, 295, 264, 18161, 5393, 293, 341, 393, 312, 382, 2416, 382, 51282, 51282, 291, 1116, 411, 420, 382, 1359, 382, 291, 1116, 411, 370, 337, 1365, 264, 2744, 727, 312, 257, 3262, 51444, 51444, 293, 321, 366, 516, 281, 352, 670, 3866, 7994, 295, 264, 2744, 295, 341, 7633, 4583, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.08664623101552328, "compression_ratio": 1.9579831932773109, "no_speech_prob": 3.1691728509031236e-05}, {"id": 97, "seek": 44312, "start": 443.12, "end": 447.52, "text": " and we're going to evaluate how well they work. So say there were a hundred neurons", "tokens": [50364, 293, 321, 434, 516, 281, 13059, 577, 731, 436, 589, 13, 407, 584, 456, 645, 257, 3262, 22027, 50584, 50584, 510, 439, 295, 552, 576, 312, 4498, 4582, 281, 264, 4289, 2283, 420, 4289, 3547, 50882, 50882, 300, 652, 493, 613, 1045, 2283, 13, 407, 341, 307, 257, 4498, 4582, 4583, 550, 456, 311, 51130, 51130, 257, 1266, 12, 12415, 938, 8213, 507, 293, 550, 456, 311, 341, 5598, 4583, 293, 570, 51336, 51336, 456, 366, 3282, 11, 1360, 1944, 2283, 300, 727, 808, 958, 341, 4583, 575, 3282, 11, 1360, 51590, 51590, 22027, 293, 439, 295, 552, 366, 4498, 4582, 281, 439, 295, 613, 22027, 294, 264, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.10457563400268555, "compression_ratio": 1.8577235772357723, "no_speech_prob": 7.527752131863963e-06}, {"id": 98, "seek": 44312, "start": 447.52, "end": 453.48, "text": " here all of them would be fully connected to the 90 words or 90 numbers", "tokens": [50364, 293, 321, 434, 516, 281, 13059, 577, 731, 436, 589, 13, 407, 584, 456, 645, 257, 3262, 22027, 50584, 50584, 510, 439, 295, 552, 576, 312, 4498, 4582, 281, 264, 4289, 2283, 420, 4289, 3547, 50882, 50882, 300, 652, 493, 613, 1045, 2283, 13, 407, 341, 307, 257, 4498, 4582, 4583, 550, 456, 311, 51130, 51130, 257, 1266, 12, 12415, 938, 8213, 507, 293, 550, 456, 311, 341, 5598, 4583, 293, 570, 51336, 51336, 456, 366, 3282, 11, 1360, 1944, 2283, 300, 727, 808, 958, 341, 4583, 575, 3282, 11, 1360, 51590, 51590, 22027, 293, 439, 295, 552, 366, 4498, 4582, 281, 439, 295, 613, 22027, 294, 264, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.10457563400268555, "compression_ratio": 1.8577235772357723, "no_speech_prob": 7.527752131863963e-06}, {"id": 99, "seek": 44312, "start": 453.48, "end": 458.44, "text": " that make up these three words. So this is a fully connected layer then there's", "tokens": [50364, 293, 321, 434, 516, 281, 13059, 577, 731, 436, 589, 13, 407, 584, 456, 645, 257, 3262, 22027, 50584, 50584, 510, 439, 295, 552, 576, 312, 4498, 4582, 281, 264, 4289, 2283, 420, 4289, 3547, 50882, 50882, 300, 652, 493, 613, 1045, 2283, 13, 407, 341, 307, 257, 4498, 4582, 4583, 550, 456, 311, 51130, 51130, 257, 1266, 12, 12415, 938, 8213, 507, 293, 550, 456, 311, 341, 5598, 4583, 293, 570, 51336, 51336, 456, 366, 3282, 11, 1360, 1944, 2283, 300, 727, 808, 958, 341, 4583, 575, 3282, 11, 1360, 51590, 51590, 22027, 293, 439, 295, 552, 366, 4498, 4582, 281, 439, 295, 613, 22027, 294, 264, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.10457563400268555, "compression_ratio": 1.8577235772357723, "no_speech_prob": 7.527752131863963e-06}, {"id": 100, "seek": 44312, "start": 458.44, "end": 462.56, "text": " a 10-inch long linearity and then there's this output layer and because", "tokens": [50364, 293, 321, 434, 516, 281, 13059, 577, 731, 436, 589, 13, 407, 584, 456, 645, 257, 3262, 22027, 50584, 50584, 510, 439, 295, 552, 576, 312, 4498, 4582, 281, 264, 4289, 2283, 420, 4289, 3547, 50882, 50882, 300, 652, 493, 613, 1045, 2283, 13, 407, 341, 307, 257, 4498, 4582, 4583, 550, 456, 311, 51130, 51130, 257, 1266, 12, 12415, 938, 8213, 507, 293, 550, 456, 311, 341, 5598, 4583, 293, 570, 51336, 51336, 456, 366, 3282, 11, 1360, 1944, 2283, 300, 727, 808, 958, 341, 4583, 575, 3282, 11, 1360, 51590, 51590, 22027, 293, 439, 295, 552, 366, 4498, 4582, 281, 439, 295, 613, 22027, 294, 264, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.10457563400268555, "compression_ratio": 1.8577235772357723, "no_speech_prob": 7.527752131863963e-06}, {"id": 101, "seek": 44312, "start": 462.56, "end": 467.64, "text": " there are 17,000 possible words that could come next this layer has 17,000", "tokens": [50364, 293, 321, 434, 516, 281, 13059, 577, 731, 436, 589, 13, 407, 584, 456, 645, 257, 3262, 22027, 50584, 50584, 510, 439, 295, 552, 576, 312, 4498, 4582, 281, 264, 4289, 2283, 420, 4289, 3547, 50882, 50882, 300, 652, 493, 613, 1045, 2283, 13, 407, 341, 307, 257, 4498, 4582, 4583, 550, 456, 311, 51130, 51130, 257, 1266, 12, 12415, 938, 8213, 507, 293, 550, 456, 311, 341, 5598, 4583, 293, 570, 51336, 51336, 456, 366, 3282, 11, 1360, 1944, 2283, 300, 727, 808, 958, 341, 4583, 575, 3282, 11, 1360, 51590, 51590, 22027, 293, 439, 295, 552, 366, 4498, 4582, 281, 439, 295, 613, 22027, 294, 264, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.10457563400268555, "compression_ratio": 1.8577235772357723, "no_speech_prob": 7.527752131863963e-06}, {"id": 102, "seek": 44312, "start": 467.64, "end": 472.64, "text": " neurons and all of them are fully connected to all of these neurons in the", "tokens": [50364, 293, 321, 434, 516, 281, 13059, 577, 731, 436, 589, 13, 407, 584, 456, 645, 257, 3262, 22027, 50584, 50584, 510, 439, 295, 552, 576, 312, 4498, 4582, 281, 264, 4289, 2283, 420, 4289, 3547, 50882, 50882, 300, 652, 493, 613, 1045, 2283, 13, 407, 341, 307, 257, 4498, 4582, 4583, 550, 456, 311, 51130, 51130, 257, 1266, 12, 12415, 938, 8213, 507, 293, 550, 456, 311, 341, 5598, 4583, 293, 570, 51336, 51336, 456, 366, 3282, 11, 1360, 1944, 2283, 300, 727, 808, 958, 341, 4583, 575, 3282, 11, 1360, 51590, 51590, 22027, 293, 439, 295, 552, 366, 4498, 4582, 281, 439, 295, 613, 22027, 294, 264, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.10457563400268555, "compression_ratio": 1.8577235772357723, "no_speech_prob": 7.527752131863963e-06}, {"id": 103, "seek": 47264, "start": 472.64, "end": 477.08, "text": " hidden layer. So there's a lot of parameters here because there's a lot of", "tokens": [50364, 7633, 4583, 13, 407, 456, 311, 257, 688, 295, 9834, 510, 570, 456, 311, 257, 688, 295, 50586, 50586, 2283, 370, 881, 24903, 307, 510, 341, 307, 264, 5124, 4583, 13, 823, 456, 366, 50852, 50852, 3282, 11, 1360, 3565, 1208, 510, 370, 322, 1192, 295, 456, 321, 362, 264, 2787, 41167, 4583, 597, 321, 600, 1612, 51120, 51120, 294, 527, 3894, 960, 382, 731, 370, 633, 472, 295, 613, 3565, 1208, 307, 51266, 51266, 12680, 23012, 770, 293, 550, 1203, 307, 48704, 281, 2408, 281, 472, 370, 300, 321, 362, 51488, 51488, 257, 1481, 8482, 7316, 337, 264, 958, 1349, 294, 264, 8310, 13, 823, 295, 1164, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.10452878475189209, "compression_ratio": 1.7159090909090908, "no_speech_prob": 3.12008305627387e-05}, {"id": 104, "seek": 47264, "start": 477.08, "end": 482.4, "text": " words so most computation is here this is the expensive layer. Now there are", "tokens": [50364, 7633, 4583, 13, 407, 456, 311, 257, 688, 295, 9834, 510, 570, 456, 311, 257, 688, 295, 50586, 50586, 2283, 370, 881, 24903, 307, 510, 341, 307, 264, 5124, 4583, 13, 823, 456, 366, 50852, 50852, 3282, 11, 1360, 3565, 1208, 510, 370, 322, 1192, 295, 456, 321, 362, 264, 2787, 41167, 4583, 597, 321, 600, 1612, 51120, 51120, 294, 527, 3894, 960, 382, 731, 370, 633, 472, 295, 613, 3565, 1208, 307, 51266, 51266, 12680, 23012, 770, 293, 550, 1203, 307, 48704, 281, 2408, 281, 472, 370, 300, 321, 362, 51488, 51488, 257, 1481, 8482, 7316, 337, 264, 958, 1349, 294, 264, 8310, 13, 823, 295, 1164, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.10452878475189209, "compression_ratio": 1.7159090909090908, "no_speech_prob": 3.12008305627387e-05}, {"id": 105, "seek": 47264, "start": 482.4, "end": 487.76, "text": " 17,000 logits here so on top of there we have the softmax layer which we've seen", "tokens": [50364, 7633, 4583, 13, 407, 456, 311, 257, 688, 295, 9834, 510, 570, 456, 311, 257, 688, 295, 50586, 50586, 2283, 370, 881, 24903, 307, 510, 341, 307, 264, 5124, 4583, 13, 823, 456, 366, 50852, 50852, 3282, 11, 1360, 3565, 1208, 510, 370, 322, 1192, 295, 456, 321, 362, 264, 2787, 41167, 4583, 597, 321, 600, 1612, 51120, 51120, 294, 527, 3894, 960, 382, 731, 370, 633, 472, 295, 613, 3565, 1208, 307, 51266, 51266, 12680, 23012, 770, 293, 550, 1203, 307, 48704, 281, 2408, 281, 472, 370, 300, 321, 362, 51488, 51488, 257, 1481, 8482, 7316, 337, 264, 958, 1349, 294, 264, 8310, 13, 823, 295, 1164, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.10452878475189209, "compression_ratio": 1.7159090909090908, "no_speech_prob": 3.12008305627387e-05}, {"id": 106, "seek": 47264, "start": 487.76, "end": 490.68, "text": " in our previous video as well so every one of these logits is", "tokens": [50364, 7633, 4583, 13, 407, 456, 311, 257, 688, 295, 9834, 510, 570, 456, 311, 257, 688, 295, 50586, 50586, 2283, 370, 881, 24903, 307, 510, 341, 307, 264, 5124, 4583, 13, 823, 456, 366, 50852, 50852, 3282, 11, 1360, 3565, 1208, 510, 370, 322, 1192, 295, 456, 321, 362, 264, 2787, 41167, 4583, 597, 321, 600, 1612, 51120, 51120, 294, 527, 3894, 960, 382, 731, 370, 633, 472, 295, 613, 3565, 1208, 307, 51266, 51266, 12680, 23012, 770, 293, 550, 1203, 307, 48704, 281, 2408, 281, 472, 370, 300, 321, 362, 51488, 51488, 257, 1481, 8482, 7316, 337, 264, 958, 1349, 294, 264, 8310, 13, 823, 295, 1164, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.10452878475189209, "compression_ratio": 1.7159090909090908, "no_speech_prob": 3.12008305627387e-05}, {"id": 107, "seek": 47264, "start": 490.68, "end": 495.12, "text": " exponentiated and then everything is normalized to sum to one so that we have", "tokens": [50364, 7633, 4583, 13, 407, 456, 311, 257, 688, 295, 9834, 510, 570, 456, 311, 257, 688, 295, 50586, 50586, 2283, 370, 881, 24903, 307, 510, 341, 307, 264, 5124, 4583, 13, 823, 456, 366, 50852, 50852, 3282, 11, 1360, 3565, 1208, 510, 370, 322, 1192, 295, 456, 321, 362, 264, 2787, 41167, 4583, 597, 321, 600, 1612, 51120, 51120, 294, 527, 3894, 960, 382, 731, 370, 633, 472, 295, 613, 3565, 1208, 307, 51266, 51266, 12680, 23012, 770, 293, 550, 1203, 307, 48704, 281, 2408, 281, 472, 370, 300, 321, 362, 51488, 51488, 257, 1481, 8482, 7316, 337, 264, 958, 1349, 294, 264, 8310, 13, 823, 295, 1164, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.10452878475189209, "compression_ratio": 1.7159090909090908, "no_speech_prob": 3.12008305627387e-05}, {"id": 108, "seek": 47264, "start": 495.12, "end": 500.47999999999996, "text": " a nice probability distribution for the next word in the sequence. Now of course", "tokens": [50364, 7633, 4583, 13, 407, 456, 311, 257, 688, 295, 9834, 510, 570, 456, 311, 257, 688, 295, 50586, 50586, 2283, 370, 881, 24903, 307, 510, 341, 307, 264, 5124, 4583, 13, 823, 456, 366, 50852, 50852, 3282, 11, 1360, 3565, 1208, 510, 370, 322, 1192, 295, 456, 321, 362, 264, 2787, 41167, 4583, 597, 321, 600, 1612, 51120, 51120, 294, 527, 3894, 960, 382, 731, 370, 633, 472, 295, 613, 3565, 1208, 307, 51266, 51266, 12680, 23012, 770, 293, 550, 1203, 307, 48704, 281, 2408, 281, 472, 370, 300, 321, 362, 51488, 51488, 257, 1481, 8482, 7316, 337, 264, 958, 1349, 294, 264, 8310, 13, 823, 295, 1164, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.10452878475189209, "compression_ratio": 1.7159090909090908, "no_speech_prob": 3.12008305627387e-05}, {"id": 109, "seek": 50048, "start": 500.48, "end": 504.16, "text": " during training we actually have the label we have the identity of the next", "tokens": [50364, 1830, 3097, 321, 767, 362, 264, 7645, 321, 362, 264, 6575, 295, 264, 958, 50548, 50548, 1349, 294, 257, 8310, 300, 1349, 420, 1080, 8186, 307, 1143, 281, 41514, 484, 264, 50882, 50882, 8482, 295, 300, 1349, 293, 550, 321, 366, 5138, 3319, 264, 8482, 295, 300, 1349, 51160, 51160, 365, 3104, 281, 264, 9834, 295, 341, 18161, 2533, 13, 407, 264, 9834, 366, 264, 51394, 51394, 17443, 293, 32152, 295, 341, 5598, 4583, 11, 264, 17443, 293, 32152, 295, 264, 7633, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.10608785096989122, "compression_ratio": 1.9384615384615385, "no_speech_prob": 5.093534127809107e-06}, {"id": 110, "seek": 50048, "start": 504.16, "end": 510.84000000000003, "text": " word in a sequence that word or its index is used to pluck out the", "tokens": [50364, 1830, 3097, 321, 767, 362, 264, 7645, 321, 362, 264, 6575, 295, 264, 958, 50548, 50548, 1349, 294, 257, 8310, 300, 1349, 420, 1080, 8186, 307, 1143, 281, 41514, 484, 264, 50882, 50882, 8482, 295, 300, 1349, 293, 550, 321, 366, 5138, 3319, 264, 8482, 295, 300, 1349, 51160, 51160, 365, 3104, 281, 264, 9834, 295, 341, 18161, 2533, 13, 407, 264, 9834, 366, 264, 51394, 51394, 17443, 293, 32152, 295, 341, 5598, 4583, 11, 264, 17443, 293, 32152, 295, 264, 7633, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.10608785096989122, "compression_ratio": 1.9384615384615385, "no_speech_prob": 5.093534127809107e-06}, {"id": 111, "seek": 50048, "start": 510.84000000000003, "end": 516.4, "text": " probability of that word and then we are maximizing the probability of that word", "tokens": [50364, 1830, 3097, 321, 767, 362, 264, 7645, 321, 362, 264, 6575, 295, 264, 958, 50548, 50548, 1349, 294, 257, 8310, 300, 1349, 420, 1080, 8186, 307, 1143, 281, 41514, 484, 264, 50882, 50882, 8482, 295, 300, 1349, 293, 550, 321, 366, 5138, 3319, 264, 8482, 295, 300, 1349, 51160, 51160, 365, 3104, 281, 264, 9834, 295, 341, 18161, 2533, 13, 407, 264, 9834, 366, 264, 51394, 51394, 17443, 293, 32152, 295, 341, 5598, 4583, 11, 264, 17443, 293, 32152, 295, 264, 7633, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.10608785096989122, "compression_ratio": 1.9384615384615385, "no_speech_prob": 5.093534127809107e-06}, {"id": 112, "seek": 50048, "start": 516.4, "end": 521.08, "text": " with respect to the parameters of this neural net. So the parameters are the", "tokens": [50364, 1830, 3097, 321, 767, 362, 264, 7645, 321, 362, 264, 6575, 295, 264, 958, 50548, 50548, 1349, 294, 257, 8310, 300, 1349, 420, 1080, 8186, 307, 1143, 281, 41514, 484, 264, 50882, 50882, 8482, 295, 300, 1349, 293, 550, 321, 366, 5138, 3319, 264, 8482, 295, 300, 1349, 51160, 51160, 365, 3104, 281, 264, 9834, 295, 341, 18161, 2533, 13, 407, 264, 9834, 366, 264, 51394, 51394, 17443, 293, 32152, 295, 341, 5598, 4583, 11, 264, 17443, 293, 32152, 295, 264, 7633, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.10608785096989122, "compression_ratio": 1.9384615384615385, "no_speech_prob": 5.093534127809107e-06}, {"id": 113, "seek": 50048, "start": 521.08, "end": 525.96, "text": " weights and biases of this output layer, the weights and biases of the hidden", "tokens": [50364, 1830, 3097, 321, 767, 362, 264, 7645, 321, 362, 264, 6575, 295, 264, 958, 50548, 50548, 1349, 294, 257, 8310, 300, 1349, 420, 1080, 8186, 307, 1143, 281, 41514, 484, 264, 50882, 50882, 8482, 295, 300, 1349, 293, 550, 321, 366, 5138, 3319, 264, 8482, 295, 300, 1349, 51160, 51160, 365, 3104, 281, 264, 9834, 295, 341, 18161, 2533, 13, 407, 264, 9834, 366, 264, 51394, 51394, 17443, 293, 32152, 295, 341, 5598, 4583, 11, 264, 17443, 293, 32152, 295, 264, 7633, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.10608785096989122, "compression_ratio": 1.9384615384615385, "no_speech_prob": 5.093534127809107e-06}, {"id": 114, "seek": 52596, "start": 525.96, "end": 530.9200000000001, "text": " layer and the embedding lookup table C and all of that is optimized using back", "tokens": [50364, 4583, 293, 264, 12240, 3584, 574, 1010, 3199, 383, 293, 439, 295, 300, 307, 26941, 1228, 646, 50612, 50612, 38377, 293, 613, 8240, 292, 19669, 11200, 729, 300, 8855, 257, 12990, 50918, 50918, 295, 257, 18161, 2533, 300, 321, 366, 406, 516, 281, 6839, 294, 341, 960, 13, 407, 300, 311, 264, 51090, 51090, 8657, 293, 586, 718, 311, 4445, 309, 13, 1033, 370, 286, 1409, 257, 3360, 777, 21060, 337, 341, 51298, 51298, 7991, 321, 366, 43866, 25878, 284, 339, 293, 321, 366, 43866, 3803, 564, 310, 38270, 370, 321, 393, 51534, 51534, 1884, 9624, 550, 286, 669, 3760, 439, 264, 5288, 666, 257, 1329, 295, 2283, 411, 286, 630, 51797, 51797], "temperature": 0.0, "avg_logprob": -0.11681061620297639, "compression_ratio": 1.6931407942238268, "no_speech_prob": 1.593590059201233e-05}, {"id": 115, "seek": 52596, "start": 530.9200000000001, "end": 537.0400000000001, "text": " propagation and these dashed arrows ignore those that represents a variation", "tokens": [50364, 4583, 293, 264, 12240, 3584, 574, 1010, 3199, 383, 293, 439, 295, 300, 307, 26941, 1228, 646, 50612, 50612, 38377, 293, 613, 8240, 292, 19669, 11200, 729, 300, 8855, 257, 12990, 50918, 50918, 295, 257, 18161, 2533, 300, 321, 366, 406, 516, 281, 6839, 294, 341, 960, 13, 407, 300, 311, 264, 51090, 51090, 8657, 293, 586, 718, 311, 4445, 309, 13, 1033, 370, 286, 1409, 257, 3360, 777, 21060, 337, 341, 51298, 51298, 7991, 321, 366, 43866, 25878, 284, 339, 293, 321, 366, 43866, 3803, 564, 310, 38270, 370, 321, 393, 51534, 51534, 1884, 9624, 550, 286, 669, 3760, 439, 264, 5288, 666, 257, 1329, 295, 2283, 411, 286, 630, 51797, 51797], "temperature": 0.0, "avg_logprob": -0.11681061620297639, "compression_ratio": 1.6931407942238268, "no_speech_prob": 1.593590059201233e-05}, {"id": 116, "seek": 52596, "start": 537.0400000000001, "end": 540.48, "text": " of a neural net that we are not going to explore in this video. So that's the", "tokens": [50364, 4583, 293, 264, 12240, 3584, 574, 1010, 3199, 383, 293, 439, 295, 300, 307, 26941, 1228, 646, 50612, 50612, 38377, 293, 613, 8240, 292, 19669, 11200, 729, 300, 8855, 257, 12990, 50918, 50918, 295, 257, 18161, 2533, 300, 321, 366, 406, 516, 281, 6839, 294, 341, 960, 13, 407, 300, 311, 264, 51090, 51090, 8657, 293, 586, 718, 311, 4445, 309, 13, 1033, 370, 286, 1409, 257, 3360, 777, 21060, 337, 341, 51298, 51298, 7991, 321, 366, 43866, 25878, 284, 339, 293, 321, 366, 43866, 3803, 564, 310, 38270, 370, 321, 393, 51534, 51534, 1884, 9624, 550, 286, 669, 3760, 439, 264, 5288, 666, 257, 1329, 295, 2283, 411, 286, 630, 51797, 51797], "temperature": 0.0, "avg_logprob": -0.11681061620297639, "compression_ratio": 1.6931407942238268, "no_speech_prob": 1.593590059201233e-05}, {"id": 117, "seek": 52596, "start": 540.48, "end": 544.64, "text": " setup and now let's implement it. Okay so I started a brand new notebook for this", "tokens": [50364, 4583, 293, 264, 12240, 3584, 574, 1010, 3199, 383, 293, 439, 295, 300, 307, 26941, 1228, 646, 50612, 50612, 38377, 293, 613, 8240, 292, 19669, 11200, 729, 300, 8855, 257, 12990, 50918, 50918, 295, 257, 18161, 2533, 300, 321, 366, 406, 516, 281, 6839, 294, 341, 960, 13, 407, 300, 311, 264, 51090, 51090, 8657, 293, 586, 718, 311, 4445, 309, 13, 1033, 370, 286, 1409, 257, 3360, 777, 21060, 337, 341, 51298, 51298, 7991, 321, 366, 43866, 25878, 284, 339, 293, 321, 366, 43866, 3803, 564, 310, 38270, 370, 321, 393, 51534, 51534, 1884, 9624, 550, 286, 669, 3760, 439, 264, 5288, 666, 257, 1329, 295, 2283, 411, 286, 630, 51797, 51797], "temperature": 0.0, "avg_logprob": -0.11681061620297639, "compression_ratio": 1.6931407942238268, "no_speech_prob": 1.593590059201233e-05}, {"id": 118, "seek": 52596, "start": 544.64, "end": 549.36, "text": " lecture we are importing pytorch and we are importing matplotlib so we can", "tokens": [50364, 4583, 293, 264, 12240, 3584, 574, 1010, 3199, 383, 293, 439, 295, 300, 307, 26941, 1228, 646, 50612, 50612, 38377, 293, 613, 8240, 292, 19669, 11200, 729, 300, 8855, 257, 12990, 50918, 50918, 295, 257, 18161, 2533, 300, 321, 366, 406, 516, 281, 6839, 294, 341, 960, 13, 407, 300, 311, 264, 51090, 51090, 8657, 293, 586, 718, 311, 4445, 309, 13, 1033, 370, 286, 1409, 257, 3360, 777, 21060, 337, 341, 51298, 51298, 7991, 321, 366, 43866, 25878, 284, 339, 293, 321, 366, 43866, 3803, 564, 310, 38270, 370, 321, 393, 51534, 51534, 1884, 9624, 550, 286, 669, 3760, 439, 264, 5288, 666, 257, 1329, 295, 2283, 411, 286, 630, 51797, 51797], "temperature": 0.0, "avg_logprob": -0.11681061620297639, "compression_ratio": 1.6931407942238268, "no_speech_prob": 1.593590059201233e-05}, {"id": 119, "seek": 52596, "start": 549.36, "end": 554.62, "text": " create figures then I am reading all the names into a list of words like I did", "tokens": [50364, 4583, 293, 264, 12240, 3584, 574, 1010, 3199, 383, 293, 439, 295, 300, 307, 26941, 1228, 646, 50612, 50612, 38377, 293, 613, 8240, 292, 19669, 11200, 729, 300, 8855, 257, 12990, 50918, 50918, 295, 257, 18161, 2533, 300, 321, 366, 406, 516, 281, 6839, 294, 341, 960, 13, 407, 300, 311, 264, 51090, 51090, 8657, 293, 586, 718, 311, 4445, 309, 13, 1033, 370, 286, 1409, 257, 3360, 777, 21060, 337, 341, 51298, 51298, 7991, 321, 366, 43866, 25878, 284, 339, 293, 321, 366, 43866, 3803, 564, 310, 38270, 370, 321, 393, 51534, 51534, 1884, 9624, 550, 286, 669, 3760, 439, 264, 5288, 666, 257, 1329, 295, 2283, 411, 286, 630, 51797, 51797], "temperature": 0.0, "avg_logprob": -0.11681061620297639, "compression_ratio": 1.6931407942238268, "no_speech_prob": 1.593590059201233e-05}, {"id": 120, "seek": 55462, "start": 554.62, "end": 559.16, "text": " before and I'm showing the first eight right here. Keep in mind that we have a", "tokens": [50364, 949, 293, 286, 478, 4099, 264, 700, 3180, 558, 510, 13, 5527, 294, 1575, 300, 321, 362, 257, 50591, 50591, 8858, 11, 1360, 294, 3217, 613, 366, 445, 264, 700, 3180, 293, 550, 510, 286, 478, 2390, 484, 264, 50841, 50841, 19864, 295, 4342, 293, 439, 264, 463, 28968, 490, 264, 4342, 382, 13985, 51047, 51047, 281, 41674, 293, 11964, 25650, 13, 823, 264, 700, 551, 321, 528, 281, 360, 307, 321, 528, 281, 31413, 51309, 51309, 264, 1412, 992, 337, 264, 18161, 3209, 293, 286, 632, 281, 28132, 341, 3089, 286, 603, 855, 291, 51547, 51547, 294, 257, 1150, 437, 309, 1542, 411, 13, 407, 341, 307, 264, 3089, 300, 286, 2942, 337, 264, 1412, 992, 51819, 51819], "temperature": 0.0, "avg_logprob": -0.12650699457846398, "compression_ratio": 1.7737226277372262, "no_speech_prob": 1.1124743650725577e-05}, {"id": 121, "seek": 55462, "start": 559.16, "end": 564.16, "text": " 32,000 in total these are just the first eight and then here I'm building out the", "tokens": [50364, 949, 293, 286, 478, 4099, 264, 700, 3180, 558, 510, 13, 5527, 294, 1575, 300, 321, 362, 257, 50591, 50591, 8858, 11, 1360, 294, 3217, 613, 366, 445, 264, 700, 3180, 293, 550, 510, 286, 478, 2390, 484, 264, 50841, 50841, 19864, 295, 4342, 293, 439, 264, 463, 28968, 490, 264, 4342, 382, 13985, 51047, 51047, 281, 41674, 293, 11964, 25650, 13, 823, 264, 700, 551, 321, 528, 281, 360, 307, 321, 528, 281, 31413, 51309, 51309, 264, 1412, 992, 337, 264, 18161, 3209, 293, 286, 632, 281, 28132, 341, 3089, 286, 603, 855, 291, 51547, 51547, 294, 257, 1150, 437, 309, 1542, 411, 13, 407, 341, 307, 264, 3089, 300, 286, 2942, 337, 264, 1412, 992, 51819, 51819], "temperature": 0.0, "avg_logprob": -0.12650699457846398, "compression_ratio": 1.7737226277372262, "no_speech_prob": 1.1124743650725577e-05}, {"id": 122, "seek": 55462, "start": 564.16, "end": 568.28, "text": " vocabulary of characters and all the mappings from the characters as strings", "tokens": [50364, 949, 293, 286, 478, 4099, 264, 700, 3180, 558, 510, 13, 5527, 294, 1575, 300, 321, 362, 257, 50591, 50591, 8858, 11, 1360, 294, 3217, 613, 366, 445, 264, 700, 3180, 293, 550, 510, 286, 478, 2390, 484, 264, 50841, 50841, 19864, 295, 4342, 293, 439, 264, 463, 28968, 490, 264, 4342, 382, 13985, 51047, 51047, 281, 41674, 293, 11964, 25650, 13, 823, 264, 700, 551, 321, 528, 281, 360, 307, 321, 528, 281, 31413, 51309, 51309, 264, 1412, 992, 337, 264, 18161, 3209, 293, 286, 632, 281, 28132, 341, 3089, 286, 603, 855, 291, 51547, 51547, 294, 257, 1150, 437, 309, 1542, 411, 13, 407, 341, 307, 264, 3089, 300, 286, 2942, 337, 264, 1412, 992, 51819, 51819], "temperature": 0.0, "avg_logprob": -0.12650699457846398, "compression_ratio": 1.7737226277372262, "no_speech_prob": 1.1124743650725577e-05}, {"id": 123, "seek": 55462, "start": 568.28, "end": 573.52, "text": " to integers and vice versa. Now the first thing we want to do is we want to compile", "tokens": [50364, 949, 293, 286, 478, 4099, 264, 700, 3180, 558, 510, 13, 5527, 294, 1575, 300, 321, 362, 257, 50591, 50591, 8858, 11, 1360, 294, 3217, 613, 366, 445, 264, 700, 3180, 293, 550, 510, 286, 478, 2390, 484, 264, 50841, 50841, 19864, 295, 4342, 293, 439, 264, 463, 28968, 490, 264, 4342, 382, 13985, 51047, 51047, 281, 41674, 293, 11964, 25650, 13, 823, 264, 700, 551, 321, 528, 281, 360, 307, 321, 528, 281, 31413, 51309, 51309, 264, 1412, 992, 337, 264, 18161, 3209, 293, 286, 632, 281, 28132, 341, 3089, 286, 603, 855, 291, 51547, 51547, 294, 257, 1150, 437, 309, 1542, 411, 13, 407, 341, 307, 264, 3089, 300, 286, 2942, 337, 264, 1412, 992, 51819, 51819], "temperature": 0.0, "avg_logprob": -0.12650699457846398, "compression_ratio": 1.7737226277372262, "no_speech_prob": 1.1124743650725577e-05}, {"id": 124, "seek": 55462, "start": 573.52, "end": 578.28, "text": " the data set for the neural network and I had to rewrite this code I'll show you", "tokens": [50364, 949, 293, 286, 478, 4099, 264, 700, 3180, 558, 510, 13, 5527, 294, 1575, 300, 321, 362, 257, 50591, 50591, 8858, 11, 1360, 294, 3217, 613, 366, 445, 264, 700, 3180, 293, 550, 510, 286, 478, 2390, 484, 264, 50841, 50841, 19864, 295, 4342, 293, 439, 264, 463, 28968, 490, 264, 4342, 382, 13985, 51047, 51047, 281, 41674, 293, 11964, 25650, 13, 823, 264, 700, 551, 321, 528, 281, 360, 307, 321, 528, 281, 31413, 51309, 51309, 264, 1412, 992, 337, 264, 18161, 3209, 293, 286, 632, 281, 28132, 341, 3089, 286, 603, 855, 291, 51547, 51547, 294, 257, 1150, 437, 309, 1542, 411, 13, 407, 341, 307, 264, 3089, 300, 286, 2942, 337, 264, 1412, 992, 51819, 51819], "temperature": 0.0, "avg_logprob": -0.12650699457846398, "compression_ratio": 1.7737226277372262, "no_speech_prob": 1.1124743650725577e-05}, {"id": 125, "seek": 55462, "start": 578.28, "end": 583.72, "text": " in a second what it looks like. So this is the code that I created for the data set", "tokens": [50364, 949, 293, 286, 478, 4099, 264, 700, 3180, 558, 510, 13, 5527, 294, 1575, 300, 321, 362, 257, 50591, 50591, 8858, 11, 1360, 294, 3217, 613, 366, 445, 264, 700, 3180, 293, 550, 510, 286, 478, 2390, 484, 264, 50841, 50841, 19864, 295, 4342, 293, 439, 264, 463, 28968, 490, 264, 4342, 382, 13985, 51047, 51047, 281, 41674, 293, 11964, 25650, 13, 823, 264, 700, 551, 321, 528, 281, 360, 307, 321, 528, 281, 31413, 51309, 51309, 264, 1412, 992, 337, 264, 18161, 3209, 293, 286, 632, 281, 28132, 341, 3089, 286, 603, 855, 291, 51547, 51547, 294, 257, 1150, 437, 309, 1542, 411, 13, 407, 341, 307, 264, 3089, 300, 286, 2942, 337, 264, 1412, 992, 51819, 51819], "temperature": 0.0, "avg_logprob": -0.12650699457846398, "compression_ratio": 1.7737226277372262, "no_speech_prob": 1.1124743650725577e-05}, {"id": 126, "seek": 58372, "start": 583.72, "end": 588.8000000000001, "text": " creation so let me first run it and then I'll briefly explain how this works. So", "tokens": [50364, 8016, 370, 718, 385, 700, 1190, 309, 293, 550, 286, 603, 10515, 2903, 577, 341, 1985, 13, 407, 50618, 50618, 700, 321, 434, 516, 281, 6964, 746, 1219, 3461, 2744, 293, 341, 307, 1936, 50792, 50792, 264, 4319, 4641, 295, 577, 867, 4342, 360, 321, 747, 281, 6069, 264, 50986, 50986, 958, 472, 13, 407, 510, 294, 341, 1365, 321, 434, 1940, 1045, 4342, 281, 6069, 264, 51180, 51180, 6409, 472, 370, 321, 362, 257, 3461, 2744, 295, 1045, 300, 311, 264, 2744, 295, 264, 3461, 300, 51396, 51396, 9346, 264, 17630, 13, 1396, 510, 286, 478, 2390, 484, 264, 1783, 293, 398, 264, 1783, 366, 264, 51727, 51727], "temperature": 0.0, "avg_logprob": -0.08341087306941952, "compression_ratio": 1.7868217054263567, "no_speech_prob": 1.095265270123491e-05}, {"id": 127, "seek": 58372, "start": 588.8000000000001, "end": 592.28, "text": " first we're going to define something called block size and this is basically", "tokens": [50364, 8016, 370, 718, 385, 700, 1190, 309, 293, 550, 286, 603, 10515, 2903, 577, 341, 1985, 13, 407, 50618, 50618, 700, 321, 434, 516, 281, 6964, 746, 1219, 3461, 2744, 293, 341, 307, 1936, 50792, 50792, 264, 4319, 4641, 295, 577, 867, 4342, 360, 321, 747, 281, 6069, 264, 50986, 50986, 958, 472, 13, 407, 510, 294, 341, 1365, 321, 434, 1940, 1045, 4342, 281, 6069, 264, 51180, 51180, 6409, 472, 370, 321, 362, 257, 3461, 2744, 295, 1045, 300, 311, 264, 2744, 295, 264, 3461, 300, 51396, 51396, 9346, 264, 17630, 13, 1396, 510, 286, 478, 2390, 484, 264, 1783, 293, 398, 264, 1783, 366, 264, 51727, 51727], "temperature": 0.0, "avg_logprob": -0.08341087306941952, "compression_ratio": 1.7868217054263567, "no_speech_prob": 1.095265270123491e-05}, {"id": 128, "seek": 58372, "start": 592.28, "end": 596.1600000000001, "text": " the context length of how many characters do we take to predict the", "tokens": [50364, 8016, 370, 718, 385, 700, 1190, 309, 293, 550, 286, 603, 10515, 2903, 577, 341, 1985, 13, 407, 50618, 50618, 700, 321, 434, 516, 281, 6964, 746, 1219, 3461, 2744, 293, 341, 307, 1936, 50792, 50792, 264, 4319, 4641, 295, 577, 867, 4342, 360, 321, 747, 281, 6069, 264, 50986, 50986, 958, 472, 13, 407, 510, 294, 341, 1365, 321, 434, 1940, 1045, 4342, 281, 6069, 264, 51180, 51180, 6409, 472, 370, 321, 362, 257, 3461, 2744, 295, 1045, 300, 311, 264, 2744, 295, 264, 3461, 300, 51396, 51396, 9346, 264, 17630, 13, 1396, 510, 286, 478, 2390, 484, 264, 1783, 293, 398, 264, 1783, 366, 264, 51727, 51727], "temperature": 0.0, "avg_logprob": -0.08341087306941952, "compression_ratio": 1.7868217054263567, "no_speech_prob": 1.095265270123491e-05}, {"id": 129, "seek": 58372, "start": 596.1600000000001, "end": 600.0400000000001, "text": " next one. So here in this example we're taking three characters to predict the", "tokens": [50364, 8016, 370, 718, 385, 700, 1190, 309, 293, 550, 286, 603, 10515, 2903, 577, 341, 1985, 13, 407, 50618, 50618, 700, 321, 434, 516, 281, 6964, 746, 1219, 3461, 2744, 293, 341, 307, 1936, 50792, 50792, 264, 4319, 4641, 295, 577, 867, 4342, 360, 321, 747, 281, 6069, 264, 50986, 50986, 958, 472, 13, 407, 510, 294, 341, 1365, 321, 434, 1940, 1045, 4342, 281, 6069, 264, 51180, 51180, 6409, 472, 370, 321, 362, 257, 3461, 2744, 295, 1045, 300, 311, 264, 2744, 295, 264, 3461, 300, 51396, 51396, 9346, 264, 17630, 13, 1396, 510, 286, 478, 2390, 484, 264, 1783, 293, 398, 264, 1783, 366, 264, 51727, 51727], "temperature": 0.0, "avg_logprob": -0.08341087306941952, "compression_ratio": 1.7868217054263567, "no_speech_prob": 1.095265270123491e-05}, {"id": 130, "seek": 58372, "start": 600.0400000000001, "end": 604.36, "text": " fourth one so we have a block size of three that's the size of the block that", "tokens": [50364, 8016, 370, 718, 385, 700, 1190, 309, 293, 550, 286, 603, 10515, 2903, 577, 341, 1985, 13, 407, 50618, 50618, 700, 321, 434, 516, 281, 6964, 746, 1219, 3461, 2744, 293, 341, 307, 1936, 50792, 50792, 264, 4319, 4641, 295, 577, 867, 4342, 360, 321, 747, 281, 6069, 264, 50986, 50986, 958, 472, 13, 407, 510, 294, 341, 1365, 321, 434, 1940, 1045, 4342, 281, 6069, 264, 51180, 51180, 6409, 472, 370, 321, 362, 257, 3461, 2744, 295, 1045, 300, 311, 264, 2744, 295, 264, 3461, 300, 51396, 51396, 9346, 264, 17630, 13, 1396, 510, 286, 478, 2390, 484, 264, 1783, 293, 398, 264, 1783, 366, 264, 51727, 51727], "temperature": 0.0, "avg_logprob": -0.08341087306941952, "compression_ratio": 1.7868217054263567, "no_speech_prob": 1.095265270123491e-05}, {"id": 131, "seek": 58372, "start": 604.36, "end": 610.98, "text": " supports the prediction. Then here I'm building out the X and Y the X are the", "tokens": [50364, 8016, 370, 718, 385, 700, 1190, 309, 293, 550, 286, 603, 10515, 2903, 577, 341, 1985, 13, 407, 50618, 50618, 700, 321, 434, 516, 281, 6964, 746, 1219, 3461, 2744, 293, 341, 307, 1936, 50792, 50792, 264, 4319, 4641, 295, 577, 867, 4342, 360, 321, 747, 281, 6069, 264, 50986, 50986, 958, 472, 13, 407, 510, 294, 341, 1365, 321, 434, 1940, 1045, 4342, 281, 6069, 264, 51180, 51180, 6409, 472, 370, 321, 362, 257, 3461, 2744, 295, 1045, 300, 311, 264, 2744, 295, 264, 3461, 300, 51396, 51396, 9346, 264, 17630, 13, 1396, 510, 286, 478, 2390, 484, 264, 1783, 293, 398, 264, 1783, 366, 264, 51727, 51727], "temperature": 0.0, "avg_logprob": -0.08341087306941952, "compression_ratio": 1.7868217054263567, "no_speech_prob": 1.095265270123491e-05}, {"id": 132, "seek": 61098, "start": 610.98, "end": 617.88, "text": " input to the neural net and the Y are the labels for each example inside X. Then", "tokens": [50364, 4846, 281, 264, 18161, 2533, 293, 264, 398, 366, 264, 16949, 337, 1184, 1365, 1854, 1783, 13, 1396, 50709, 50709, 286, 478, 1189, 3349, 670, 264, 700, 1732, 2283, 286, 478, 884, 700, 1732, 445, 337, 10493, 50913, 50913, 1339, 321, 366, 6416, 439, 264, 3089, 457, 550, 1780, 321, 434, 516, 281, 808, 510, 293, 51095, 51095, 23525, 341, 370, 300, 321, 764, 264, 2302, 3097, 992, 13, 407, 510, 286, 478, 14699, 264, 51317, 51317, 1349, 17124, 293, 510, 286, 478, 1936, 4099, 264, 5110, 300, 321, 393, 8460, 264, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.11688622832298279, "compression_ratio": 1.7025862068965518, "no_speech_prob": 1.384543429594487e-05}, {"id": 133, "seek": 61098, "start": 617.88, "end": 621.96, "text": " I'm erasing over the first five words I'm doing first five just for efficiency", "tokens": [50364, 4846, 281, 264, 18161, 2533, 293, 264, 398, 366, 264, 16949, 337, 1184, 1365, 1854, 1783, 13, 1396, 50709, 50709, 286, 478, 1189, 3349, 670, 264, 700, 1732, 2283, 286, 478, 884, 700, 1732, 445, 337, 10493, 50913, 50913, 1339, 321, 366, 6416, 439, 264, 3089, 457, 550, 1780, 321, 434, 516, 281, 808, 510, 293, 51095, 51095, 23525, 341, 370, 300, 321, 764, 264, 2302, 3097, 992, 13, 407, 510, 286, 478, 14699, 264, 51317, 51317, 1349, 17124, 293, 510, 286, 478, 1936, 4099, 264, 5110, 300, 321, 393, 8460, 264, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.11688622832298279, "compression_ratio": 1.7025862068965518, "no_speech_prob": 1.384543429594487e-05}, {"id": 134, "seek": 61098, "start": 621.96, "end": 625.6, "text": " while we are developing all the code but then later we're going to come here and", "tokens": [50364, 4846, 281, 264, 18161, 2533, 293, 264, 398, 366, 264, 16949, 337, 1184, 1365, 1854, 1783, 13, 1396, 50709, 50709, 286, 478, 1189, 3349, 670, 264, 700, 1732, 2283, 286, 478, 884, 700, 1732, 445, 337, 10493, 50913, 50913, 1339, 321, 366, 6416, 439, 264, 3089, 457, 550, 1780, 321, 434, 516, 281, 808, 510, 293, 51095, 51095, 23525, 341, 370, 300, 321, 764, 264, 2302, 3097, 992, 13, 407, 510, 286, 478, 14699, 264, 51317, 51317, 1349, 17124, 293, 510, 286, 478, 1936, 4099, 264, 5110, 300, 321, 393, 8460, 264, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.11688622832298279, "compression_ratio": 1.7025862068965518, "no_speech_prob": 1.384543429594487e-05}, {"id": 135, "seek": 61098, "start": 625.6, "end": 630.04, "text": " erase this so that we use the entire training set. So here I'm printing the", "tokens": [50364, 4846, 281, 264, 18161, 2533, 293, 264, 398, 366, 264, 16949, 337, 1184, 1365, 1854, 1783, 13, 1396, 50709, 50709, 286, 478, 1189, 3349, 670, 264, 700, 1732, 2283, 286, 478, 884, 700, 1732, 445, 337, 10493, 50913, 50913, 1339, 321, 366, 6416, 439, 264, 3089, 457, 550, 1780, 321, 434, 516, 281, 808, 510, 293, 51095, 51095, 23525, 341, 370, 300, 321, 764, 264, 2302, 3097, 992, 13, 407, 510, 286, 478, 14699, 264, 51317, 51317, 1349, 17124, 293, 510, 286, 478, 1936, 4099, 264, 5110, 300, 321, 393, 8460, 264, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.11688622832298279, "compression_ratio": 1.7025862068965518, "no_speech_prob": 1.384543429594487e-05}, {"id": 136, "seek": 61098, "start": 630.04, "end": 635.94, "text": " word Emma and here I'm basically showing the examples that we can generate the", "tokens": [50364, 4846, 281, 264, 18161, 2533, 293, 264, 398, 366, 264, 16949, 337, 1184, 1365, 1854, 1783, 13, 1396, 50709, 50709, 286, 478, 1189, 3349, 670, 264, 700, 1732, 2283, 286, 478, 884, 700, 1732, 445, 337, 10493, 50913, 50913, 1339, 321, 366, 6416, 439, 264, 3089, 457, 550, 1780, 321, 434, 516, 281, 808, 510, 293, 51095, 51095, 23525, 341, 370, 300, 321, 764, 264, 2302, 3097, 992, 13, 407, 510, 286, 478, 14699, 264, 51317, 51317, 1349, 17124, 293, 510, 286, 478, 1936, 4099, 264, 5110, 300, 321, 393, 8460, 264, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.11688622832298279, "compression_ratio": 1.7025862068965518, "no_speech_prob": 1.384543429594487e-05}, {"id": 137, "seek": 63594, "start": 635.94, "end": 642.4200000000001, "text": " five examples that we can generate out of the single sort of word Emma. So when", "tokens": [50364, 1732, 5110, 300, 321, 393, 8460, 484, 295, 264, 2167, 1333, 295, 1349, 17124, 13, 407, 562, 50688, 50688, 321, 366, 2212, 264, 4319, 295, 445, 5893, 5893, 5893, 264, 700, 2517, 294, 257, 8310, 307, 50897, 50897, 462, 294, 341, 4319, 264, 7645, 307, 376, 562, 264, 4319, 307, 341, 264, 7645, 307, 376, 293, 370, 51256, 51256, 5220, 13, 400, 370, 264, 636, 286, 1322, 341, 484, 307, 700, 286, 722, 365, 257, 6887, 9207, 4319, 295, 51451, 51451, 445, 4018, 22667, 550, 286, 44497, 670, 439, 264, 4342, 286, 483, 264, 2517, 294, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.11505388259887696, "compression_ratio": 1.8227272727272728, "no_speech_prob": 6.2408084886556026e-06}, {"id": 138, "seek": 63594, "start": 642.4200000000001, "end": 646.6, "text": " we are given the context of just dot dot dot the first character in a sequence is", "tokens": [50364, 1732, 5110, 300, 321, 393, 8460, 484, 295, 264, 2167, 1333, 295, 1349, 17124, 13, 407, 562, 50688, 50688, 321, 366, 2212, 264, 4319, 295, 445, 5893, 5893, 5893, 264, 700, 2517, 294, 257, 8310, 307, 50897, 50897, 462, 294, 341, 4319, 264, 7645, 307, 376, 562, 264, 4319, 307, 341, 264, 7645, 307, 376, 293, 370, 51256, 51256, 5220, 13, 400, 370, 264, 636, 286, 1322, 341, 484, 307, 700, 286, 722, 365, 257, 6887, 9207, 4319, 295, 51451, 51451, 445, 4018, 22667, 550, 286, 44497, 670, 439, 264, 4342, 286, 483, 264, 2517, 294, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.11505388259887696, "compression_ratio": 1.8227272727272728, "no_speech_prob": 6.2408084886556026e-06}, {"id": 139, "seek": 63594, "start": 646.6, "end": 653.7800000000001, "text": " E in this context the label is M when the context is this the label is M and so", "tokens": [50364, 1732, 5110, 300, 321, 393, 8460, 484, 295, 264, 2167, 1333, 295, 1349, 17124, 13, 407, 562, 50688, 50688, 321, 366, 2212, 264, 4319, 295, 445, 5893, 5893, 5893, 264, 700, 2517, 294, 257, 8310, 307, 50897, 50897, 462, 294, 341, 4319, 264, 7645, 307, 376, 562, 264, 4319, 307, 341, 264, 7645, 307, 376, 293, 370, 51256, 51256, 5220, 13, 400, 370, 264, 636, 286, 1322, 341, 484, 307, 700, 286, 722, 365, 257, 6887, 9207, 4319, 295, 51451, 51451, 445, 4018, 22667, 550, 286, 44497, 670, 439, 264, 4342, 286, 483, 264, 2517, 294, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.11505388259887696, "compression_ratio": 1.8227272727272728, "no_speech_prob": 6.2408084886556026e-06}, {"id": 140, "seek": 63594, "start": 653.7800000000001, "end": 657.6800000000001, "text": " forth. And so the way I build this out is first I start with a padded context of", "tokens": [50364, 1732, 5110, 300, 321, 393, 8460, 484, 295, 264, 2167, 1333, 295, 1349, 17124, 13, 407, 562, 50688, 50688, 321, 366, 2212, 264, 4319, 295, 445, 5893, 5893, 5893, 264, 700, 2517, 294, 257, 8310, 307, 50897, 50897, 462, 294, 341, 4319, 264, 7645, 307, 376, 562, 264, 4319, 307, 341, 264, 7645, 307, 376, 293, 370, 51256, 51256, 5220, 13, 400, 370, 264, 636, 286, 1322, 341, 484, 307, 700, 286, 722, 365, 257, 6887, 9207, 4319, 295, 51451, 51451, 445, 4018, 22667, 550, 286, 44497, 670, 439, 264, 4342, 286, 483, 264, 2517, 294, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.11505388259887696, "compression_ratio": 1.8227272727272728, "no_speech_prob": 6.2408084886556026e-06}, {"id": 141, "seek": 63594, "start": 657.6800000000001, "end": 663.44, "text": " just zero tokens then I iterate over all the characters I get the character in", "tokens": [50364, 1732, 5110, 300, 321, 393, 8460, 484, 295, 264, 2167, 1333, 295, 1349, 17124, 13, 407, 562, 50688, 50688, 321, 366, 2212, 264, 4319, 295, 445, 5893, 5893, 5893, 264, 700, 2517, 294, 257, 8310, 307, 50897, 50897, 462, 294, 341, 4319, 264, 7645, 307, 376, 562, 264, 4319, 307, 341, 264, 7645, 307, 376, 293, 370, 51256, 51256, 5220, 13, 400, 370, 264, 636, 286, 1322, 341, 484, 307, 700, 286, 722, 365, 257, 6887, 9207, 4319, 295, 51451, 51451, 445, 4018, 22667, 550, 286, 44497, 670, 439, 264, 4342, 286, 483, 264, 2517, 294, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.11505388259887696, "compression_ratio": 1.8227272727272728, "no_speech_prob": 6.2408084886556026e-06}, {"id": 142, "seek": 66344, "start": 663.44, "end": 668.08, "text": " the sequence and I basically build out the array Y of this current character", "tokens": [50364, 264, 8310, 293, 286, 1936, 1322, 484, 264, 10225, 398, 295, 341, 2190, 2517, 50596, 50596, 293, 264, 10225, 1783, 597, 9512, 264, 2190, 2614, 4319, 13, 400, 550, 510, 536, 286, 50832, 50832, 4482, 1203, 293, 510, 286, 9086, 264, 4319, 293, 3242, 264, 777, 2517, 294, 51092, 51092, 257, 8310, 13, 407, 341, 307, 733, 295, 411, 257, 9439, 4910, 295, 4319, 13, 823, 321, 393, 51358, 51358, 1319, 264, 3461, 2744, 510, 281, 337, 1365, 1017, 293, 294, 300, 1389, 321, 576, 312, 51540, 51540, 32884, 264, 9266, 2517, 2212, 264, 3894, 1254, 420, 309, 393, 312, 1025, 293, 550, 309, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.10494213634067112, "compression_ratio": 1.739622641509434, "no_speech_prob": 4.936843652103562e-06}, {"id": 143, "seek": 66344, "start": 668.08, "end": 672.8000000000001, "text": " and the array X which stores the current running context. And then here see I", "tokens": [50364, 264, 8310, 293, 286, 1936, 1322, 484, 264, 10225, 398, 295, 341, 2190, 2517, 50596, 50596, 293, 264, 10225, 1783, 597, 9512, 264, 2190, 2614, 4319, 13, 400, 550, 510, 536, 286, 50832, 50832, 4482, 1203, 293, 510, 286, 9086, 264, 4319, 293, 3242, 264, 777, 2517, 294, 51092, 51092, 257, 8310, 13, 407, 341, 307, 733, 295, 411, 257, 9439, 4910, 295, 4319, 13, 823, 321, 393, 51358, 51358, 1319, 264, 3461, 2744, 510, 281, 337, 1365, 1017, 293, 294, 300, 1389, 321, 576, 312, 51540, 51540, 32884, 264, 9266, 2517, 2212, 264, 3894, 1254, 420, 309, 393, 312, 1025, 293, 550, 309, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.10494213634067112, "compression_ratio": 1.739622641509434, "no_speech_prob": 4.936843652103562e-06}, {"id": 144, "seek": 66344, "start": 672.8000000000001, "end": 678.0, "text": " print everything and here I crop the context and enter the new character in", "tokens": [50364, 264, 8310, 293, 286, 1936, 1322, 484, 264, 10225, 398, 295, 341, 2190, 2517, 50596, 50596, 293, 264, 10225, 1783, 597, 9512, 264, 2190, 2614, 4319, 13, 400, 550, 510, 536, 286, 50832, 50832, 4482, 1203, 293, 510, 286, 9086, 264, 4319, 293, 3242, 264, 777, 2517, 294, 51092, 51092, 257, 8310, 13, 407, 341, 307, 733, 295, 411, 257, 9439, 4910, 295, 4319, 13, 823, 321, 393, 51358, 51358, 1319, 264, 3461, 2744, 510, 281, 337, 1365, 1017, 293, 294, 300, 1389, 321, 576, 312, 51540, 51540, 32884, 264, 9266, 2517, 2212, 264, 3894, 1254, 420, 309, 393, 312, 1025, 293, 550, 309, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.10494213634067112, "compression_ratio": 1.739622641509434, "no_speech_prob": 4.936843652103562e-06}, {"id": 145, "seek": 66344, "start": 678.0, "end": 683.32, "text": " a sequence. So this is kind of like a rolling window of context. Now we can", "tokens": [50364, 264, 8310, 293, 286, 1936, 1322, 484, 264, 10225, 398, 295, 341, 2190, 2517, 50596, 50596, 293, 264, 10225, 1783, 597, 9512, 264, 2190, 2614, 4319, 13, 400, 550, 510, 536, 286, 50832, 50832, 4482, 1203, 293, 510, 286, 9086, 264, 4319, 293, 3242, 264, 777, 2517, 294, 51092, 51092, 257, 8310, 13, 407, 341, 307, 733, 295, 411, 257, 9439, 4910, 295, 4319, 13, 823, 321, 393, 51358, 51358, 1319, 264, 3461, 2744, 510, 281, 337, 1365, 1017, 293, 294, 300, 1389, 321, 576, 312, 51540, 51540, 32884, 264, 9266, 2517, 2212, 264, 3894, 1254, 420, 309, 393, 312, 1025, 293, 550, 309, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.10494213634067112, "compression_ratio": 1.739622641509434, "no_speech_prob": 4.936843652103562e-06}, {"id": 146, "seek": 66344, "start": 683.32, "end": 686.96, "text": " change the block size here to for example 4 and in that case we would be", "tokens": [50364, 264, 8310, 293, 286, 1936, 1322, 484, 264, 10225, 398, 295, 341, 2190, 2517, 50596, 50596, 293, 264, 10225, 1783, 597, 9512, 264, 2190, 2614, 4319, 13, 400, 550, 510, 536, 286, 50832, 50832, 4482, 1203, 293, 510, 286, 9086, 264, 4319, 293, 3242, 264, 777, 2517, 294, 51092, 51092, 257, 8310, 13, 407, 341, 307, 733, 295, 411, 257, 9439, 4910, 295, 4319, 13, 823, 321, 393, 51358, 51358, 1319, 264, 3461, 2744, 510, 281, 337, 1365, 1017, 293, 294, 300, 1389, 321, 576, 312, 51540, 51540, 32884, 264, 9266, 2517, 2212, 264, 3894, 1254, 420, 309, 393, 312, 1025, 293, 550, 309, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.10494213634067112, "compression_ratio": 1.739622641509434, "no_speech_prob": 4.936843652103562e-06}, {"id": 147, "seek": 66344, "start": 686.96, "end": 692.36, "text": " predicting the fifth character given the previous form or it can be 5 and then it", "tokens": [50364, 264, 8310, 293, 286, 1936, 1322, 484, 264, 10225, 398, 295, 341, 2190, 2517, 50596, 50596, 293, 264, 10225, 1783, 597, 9512, 264, 2190, 2614, 4319, 13, 400, 550, 510, 536, 286, 50832, 50832, 4482, 1203, 293, 510, 286, 9086, 264, 4319, 293, 3242, 264, 777, 2517, 294, 51092, 51092, 257, 8310, 13, 407, 341, 307, 733, 295, 411, 257, 9439, 4910, 295, 4319, 13, 823, 321, 393, 51358, 51358, 1319, 264, 3461, 2744, 510, 281, 337, 1365, 1017, 293, 294, 300, 1389, 321, 576, 312, 51540, 51540, 32884, 264, 9266, 2517, 2212, 264, 3894, 1254, 420, 309, 393, 312, 1025, 293, 550, 309, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.10494213634067112, "compression_ratio": 1.739622641509434, "no_speech_prob": 4.936843652103562e-06}, {"id": 148, "seek": 69236, "start": 692.36, "end": 697.8000000000001, "text": " would look like this or it can be say 10 and then it would look something like", "tokens": [50364, 576, 574, 411, 341, 420, 309, 393, 312, 584, 1266, 293, 550, 309, 576, 574, 746, 411, 50636, 50636, 341, 321, 434, 1940, 1266, 4342, 281, 6069, 264, 2975, 392, 472, 293, 321, 434, 1009, 50828, 50828, 39562, 365, 15026, 13, 407, 718, 385, 1565, 341, 646, 281, 805, 445, 370, 300, 321, 362, 437, 321, 51132, 51132, 362, 510, 294, 264, 3035, 13, 400, 2721, 264, 1412, 992, 558, 586, 1542, 382, 10002, 13, 3358, 51438, 51438, 613, 1732, 2283, 321, 362, 2942, 257, 1412, 992, 295, 8858, 5110, 293, 1184, 4846, 295, 264, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12728768165665444, "compression_ratio": 1.672340425531915, "no_speech_prob": 5.093544132250827e-06}, {"id": 149, "seek": 69236, "start": 697.8000000000001, "end": 701.64, "text": " this we're taking 10 characters to predict the 11th one and we're always", "tokens": [50364, 576, 574, 411, 341, 420, 309, 393, 312, 584, 1266, 293, 550, 309, 576, 574, 746, 411, 50636, 50636, 341, 321, 434, 1940, 1266, 4342, 281, 6069, 264, 2975, 392, 472, 293, 321, 434, 1009, 50828, 50828, 39562, 365, 15026, 13, 407, 718, 385, 1565, 341, 646, 281, 805, 445, 370, 300, 321, 362, 437, 321, 51132, 51132, 362, 510, 294, 264, 3035, 13, 400, 2721, 264, 1412, 992, 558, 586, 1542, 382, 10002, 13, 3358, 51438, 51438, 613, 1732, 2283, 321, 362, 2942, 257, 1412, 992, 295, 8858, 5110, 293, 1184, 4846, 295, 264, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12728768165665444, "compression_ratio": 1.672340425531915, "no_speech_prob": 5.093544132250827e-06}, {"id": 150, "seek": 69236, "start": 701.64, "end": 707.72, "text": " padding with dots. So let me bring this back to 3 just so that we have what we", "tokens": [50364, 576, 574, 411, 341, 420, 309, 393, 312, 584, 1266, 293, 550, 309, 576, 574, 746, 411, 50636, 50636, 341, 321, 434, 1940, 1266, 4342, 281, 6069, 264, 2975, 392, 472, 293, 321, 434, 1009, 50828, 50828, 39562, 365, 15026, 13, 407, 718, 385, 1565, 341, 646, 281, 805, 445, 370, 300, 321, 362, 437, 321, 51132, 51132, 362, 510, 294, 264, 3035, 13, 400, 2721, 264, 1412, 992, 558, 586, 1542, 382, 10002, 13, 3358, 51438, 51438, 613, 1732, 2283, 321, 362, 2942, 257, 1412, 992, 295, 8858, 5110, 293, 1184, 4846, 295, 264, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12728768165665444, "compression_ratio": 1.672340425531915, "no_speech_prob": 5.093544132250827e-06}, {"id": 151, "seek": 69236, "start": 707.72, "end": 713.84, "text": " have here in the paper. And finally the data set right now looks as follows. From", "tokens": [50364, 576, 574, 411, 341, 420, 309, 393, 312, 584, 1266, 293, 550, 309, 576, 574, 746, 411, 50636, 50636, 341, 321, 434, 1940, 1266, 4342, 281, 6069, 264, 2975, 392, 472, 293, 321, 434, 1009, 50828, 50828, 39562, 365, 15026, 13, 407, 718, 385, 1565, 341, 646, 281, 805, 445, 370, 300, 321, 362, 437, 321, 51132, 51132, 362, 510, 294, 264, 3035, 13, 400, 2721, 264, 1412, 992, 558, 586, 1542, 382, 10002, 13, 3358, 51438, 51438, 613, 1732, 2283, 321, 362, 2942, 257, 1412, 992, 295, 8858, 5110, 293, 1184, 4846, 295, 264, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12728768165665444, "compression_ratio": 1.672340425531915, "no_speech_prob": 5.093544132250827e-06}, {"id": 152, "seek": 69236, "start": 713.84, "end": 719.36, "text": " these five words we have created a data set of 32 examples and each input of the", "tokens": [50364, 576, 574, 411, 341, 420, 309, 393, 312, 584, 1266, 293, 550, 309, 576, 574, 746, 411, 50636, 50636, 341, 321, 434, 1940, 1266, 4342, 281, 6069, 264, 2975, 392, 472, 293, 321, 434, 1009, 50828, 50828, 39562, 365, 15026, 13, 407, 718, 385, 1565, 341, 646, 281, 805, 445, 370, 300, 321, 362, 437, 321, 51132, 51132, 362, 510, 294, 264, 3035, 13, 400, 2721, 264, 1412, 992, 558, 586, 1542, 382, 10002, 13, 3358, 51438, 51438, 613, 1732, 2283, 321, 362, 2942, 257, 1412, 992, 295, 8858, 5110, 293, 1184, 4846, 295, 264, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12728768165665444, "compression_ratio": 1.672340425531915, "no_speech_prob": 5.093544132250827e-06}, {"id": 153, "seek": 71936, "start": 719.36, "end": 725.16, "text": " neural net is three integers and we have a label that is also an integer Y so X", "tokens": [50364, 18161, 2533, 307, 1045, 41674, 293, 321, 362, 257, 7645, 300, 307, 611, 364, 24922, 398, 370, 1783, 50654, 50654, 1542, 411, 341, 613, 366, 264, 2609, 5110, 293, 550, 398, 366, 264, 16949, 13, 407, 51054, 51054, 2212, 341, 718, 311, 586, 2464, 257, 18161, 3209, 300, 2516, 613, 1783, 311, 293, 51314, 51314, 6069, 82, 264, 398, 311, 13, 2386, 718, 311, 1322, 264, 12240, 3584, 574, 1010, 3199, 383, 13, 407, 321, 362, 7634, 51618, 51618, 1944, 4342, 293, 321, 434, 516, 281, 12240, 552, 294, 257, 3126, 18795, 1901, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.128143631682104, "compression_ratio": 1.6943231441048034, "no_speech_prob": 1.0952807315334212e-05}, {"id": 154, "seek": 71936, "start": 725.16, "end": 733.16, "text": " looks like this these are the individual examples and then Y are the labels. So", "tokens": [50364, 18161, 2533, 307, 1045, 41674, 293, 321, 362, 257, 7645, 300, 307, 611, 364, 24922, 398, 370, 1783, 50654, 50654, 1542, 411, 341, 613, 366, 264, 2609, 5110, 293, 550, 398, 366, 264, 16949, 13, 407, 51054, 51054, 2212, 341, 718, 311, 586, 2464, 257, 18161, 3209, 300, 2516, 613, 1783, 311, 293, 51314, 51314, 6069, 82, 264, 398, 311, 13, 2386, 718, 311, 1322, 264, 12240, 3584, 574, 1010, 3199, 383, 13, 407, 321, 362, 7634, 51618, 51618, 1944, 4342, 293, 321, 434, 516, 281, 12240, 552, 294, 257, 3126, 18795, 1901, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.128143631682104, "compression_ratio": 1.6943231441048034, "no_speech_prob": 1.0952807315334212e-05}, {"id": 155, "seek": 71936, "start": 733.16, "end": 738.36, "text": " given this let's now write a neural network that takes these X's and", "tokens": [50364, 18161, 2533, 307, 1045, 41674, 293, 321, 362, 257, 7645, 300, 307, 611, 364, 24922, 398, 370, 1783, 50654, 50654, 1542, 411, 341, 613, 366, 264, 2609, 5110, 293, 550, 398, 366, 264, 16949, 13, 407, 51054, 51054, 2212, 341, 718, 311, 586, 2464, 257, 18161, 3209, 300, 2516, 613, 1783, 311, 293, 51314, 51314, 6069, 82, 264, 398, 311, 13, 2386, 718, 311, 1322, 264, 12240, 3584, 574, 1010, 3199, 383, 13, 407, 321, 362, 7634, 51618, 51618, 1944, 4342, 293, 321, 434, 516, 281, 12240, 552, 294, 257, 3126, 18795, 1901, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.128143631682104, "compression_ratio": 1.6943231441048034, "no_speech_prob": 1.0952807315334212e-05}, {"id": 156, "seek": 71936, "start": 738.36, "end": 744.44, "text": " predicts the Y's. First let's build the embedding lookup table C. So we have 27", "tokens": [50364, 18161, 2533, 307, 1045, 41674, 293, 321, 362, 257, 7645, 300, 307, 611, 364, 24922, 398, 370, 1783, 50654, 50654, 1542, 411, 341, 613, 366, 264, 2609, 5110, 293, 550, 398, 366, 264, 16949, 13, 407, 51054, 51054, 2212, 341, 718, 311, 586, 2464, 257, 18161, 3209, 300, 2516, 613, 1783, 311, 293, 51314, 51314, 6069, 82, 264, 398, 311, 13, 2386, 718, 311, 1322, 264, 12240, 3584, 574, 1010, 3199, 383, 13, 407, 321, 362, 7634, 51618, 51618, 1944, 4342, 293, 321, 434, 516, 281, 12240, 552, 294, 257, 3126, 18795, 1901, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.128143631682104, "compression_ratio": 1.6943231441048034, "no_speech_prob": 1.0952807315334212e-05}, {"id": 157, "seek": 71936, "start": 744.44, "end": 747.72, "text": " possible characters and we're going to embed them in a lower dimensional space.", "tokens": [50364, 18161, 2533, 307, 1045, 41674, 293, 321, 362, 257, 7645, 300, 307, 611, 364, 24922, 398, 370, 1783, 50654, 50654, 1542, 411, 341, 613, 366, 264, 2609, 5110, 293, 550, 398, 366, 264, 16949, 13, 407, 51054, 51054, 2212, 341, 718, 311, 586, 2464, 257, 18161, 3209, 300, 2516, 613, 1783, 311, 293, 51314, 51314, 6069, 82, 264, 398, 311, 13, 2386, 718, 311, 1322, 264, 12240, 3584, 574, 1010, 3199, 383, 13, 407, 321, 362, 7634, 51618, 51618, 1944, 4342, 293, 321, 434, 516, 281, 12240, 552, 294, 257, 3126, 18795, 1901, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.128143631682104, "compression_ratio": 1.6943231441048034, "no_speech_prob": 1.0952807315334212e-05}, {"id": 158, "seek": 74772, "start": 747.72, "end": 754.12, "text": " In the paper they have 17,000 words and they embed them in spaces as small", "tokens": [50364, 682, 264, 3035, 436, 362, 3282, 11, 1360, 2283, 293, 436, 12240, 552, 294, 7673, 382, 1359, 50684, 50684, 18795, 382, 2217, 13, 407, 436, 941, 335, 3282, 11, 1360, 2283, 666, 2217, 18795, 1901, 13, 682, 527, 51026, 51026, 1389, 321, 362, 787, 7634, 1944, 4342, 370, 718, 311, 941, 335, 552, 294, 746, 382, 1359, 51238, 51238, 382, 281, 722, 365, 337, 1365, 257, 732, 18795, 1901, 13, 407, 341, 574, 1010, 3199, 51450, 51450, 486, 312, 4974, 3547, 293, 321, 603, 362, 7634, 13241, 293, 321, 603, 362, 732, 13766, 558, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.1065517639627262, "compression_ratio": 1.7522522522522523, "no_speech_prob": 1.777792022039648e-05}, {"id": 159, "seek": 74772, "start": 754.12, "end": 760.96, "text": " dimensional as 30. So they cram 17,000 words into 30 dimensional space. In our", "tokens": [50364, 682, 264, 3035, 436, 362, 3282, 11, 1360, 2283, 293, 436, 12240, 552, 294, 7673, 382, 1359, 50684, 50684, 18795, 382, 2217, 13, 407, 436, 941, 335, 3282, 11, 1360, 2283, 666, 2217, 18795, 1901, 13, 682, 527, 51026, 51026, 1389, 321, 362, 787, 7634, 1944, 4342, 370, 718, 311, 941, 335, 552, 294, 746, 382, 1359, 51238, 51238, 382, 281, 722, 365, 337, 1365, 257, 732, 18795, 1901, 13, 407, 341, 574, 1010, 3199, 51450, 51450, 486, 312, 4974, 3547, 293, 321, 603, 362, 7634, 13241, 293, 321, 603, 362, 732, 13766, 558, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.1065517639627262, "compression_ratio": 1.7522522522522523, "no_speech_prob": 1.777792022039648e-05}, {"id": 160, "seek": 74772, "start": 760.96, "end": 765.2, "text": " case we have only 27 possible characters so let's cram them in something as small", "tokens": [50364, 682, 264, 3035, 436, 362, 3282, 11, 1360, 2283, 293, 436, 12240, 552, 294, 7673, 382, 1359, 50684, 50684, 18795, 382, 2217, 13, 407, 436, 941, 335, 3282, 11, 1360, 2283, 666, 2217, 18795, 1901, 13, 682, 527, 51026, 51026, 1389, 321, 362, 787, 7634, 1944, 4342, 370, 718, 311, 941, 335, 552, 294, 746, 382, 1359, 51238, 51238, 382, 281, 722, 365, 337, 1365, 257, 732, 18795, 1901, 13, 407, 341, 574, 1010, 3199, 51450, 51450, 486, 312, 4974, 3547, 293, 321, 603, 362, 7634, 13241, 293, 321, 603, 362, 732, 13766, 558, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.1065517639627262, "compression_ratio": 1.7522522522522523, "no_speech_prob": 1.777792022039648e-05}, {"id": 161, "seek": 74772, "start": 765.2, "end": 769.44, "text": " as to start with for example a two dimensional space. So this lookup table", "tokens": [50364, 682, 264, 3035, 436, 362, 3282, 11, 1360, 2283, 293, 436, 12240, 552, 294, 7673, 382, 1359, 50684, 50684, 18795, 382, 2217, 13, 407, 436, 941, 335, 3282, 11, 1360, 2283, 666, 2217, 18795, 1901, 13, 682, 527, 51026, 51026, 1389, 321, 362, 787, 7634, 1944, 4342, 370, 718, 311, 941, 335, 552, 294, 746, 382, 1359, 51238, 51238, 382, 281, 722, 365, 337, 1365, 257, 732, 18795, 1901, 13, 407, 341, 574, 1010, 3199, 51450, 51450, 486, 312, 4974, 3547, 293, 321, 603, 362, 7634, 13241, 293, 321, 603, 362, 732, 13766, 558, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.1065517639627262, "compression_ratio": 1.7522522522522523, "no_speech_prob": 1.777792022039648e-05}, {"id": 162, "seek": 74772, "start": 769.44, "end": 776.52, "text": " will be random numbers and we'll have 27 rows and we'll have two columns right", "tokens": [50364, 682, 264, 3035, 436, 362, 3282, 11, 1360, 2283, 293, 436, 12240, 552, 294, 7673, 382, 1359, 50684, 50684, 18795, 382, 2217, 13, 407, 436, 941, 335, 3282, 11, 1360, 2283, 666, 2217, 18795, 1901, 13, 682, 527, 51026, 51026, 1389, 321, 362, 787, 7634, 1944, 4342, 370, 718, 311, 941, 335, 552, 294, 746, 382, 1359, 51238, 51238, 382, 281, 722, 365, 337, 1365, 257, 732, 18795, 1901, 13, 407, 341, 574, 1010, 3199, 51450, 51450, 486, 312, 4974, 3547, 293, 321, 603, 362, 7634, 13241, 293, 321, 603, 362, 732, 13766, 558, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.1065517639627262, "compression_ratio": 1.7522522522522523, "no_speech_prob": 1.777792022039648e-05}, {"id": 163, "seek": 77652, "start": 776.52, "end": 781.4, "text": " so each 20 each one of 27 characters will have a two dimensional embedding.", "tokens": [50364, 370, 1184, 945, 1184, 472, 295, 7634, 4342, 486, 362, 257, 732, 18795, 12240, 3584, 13, 50608, 50608, 407, 300, 311, 527, 8141, 383, 295, 12240, 29432, 294, 264, 2863, 5883, 1602, 16979, 13, 823, 50948, 50948, 949, 321, 12240, 439, 295, 264, 41674, 1854, 264, 4846, 1783, 1228, 341, 574, 1010, 51171, 51171, 3199, 383, 718, 385, 767, 445, 853, 281, 12240, 257, 2167, 2609, 24922, 411, 51418, 51418, 584, 1025, 370, 321, 483, 257, 2020, 295, 577, 341, 1985, 13, 823, 472, 636, 341, 1985, 295, 1164, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.10067427030173681, "compression_ratio": 1.59915611814346, "no_speech_prob": 4.092849394510267e-06}, {"id": 164, "seek": 77652, "start": 781.4, "end": 788.1999999999999, "text": " So that's our matrix C of embeddings in the beginning initialized randomly. Now", "tokens": [50364, 370, 1184, 945, 1184, 472, 295, 7634, 4342, 486, 362, 257, 732, 18795, 12240, 3584, 13, 50608, 50608, 407, 300, 311, 527, 8141, 383, 295, 12240, 29432, 294, 264, 2863, 5883, 1602, 16979, 13, 823, 50948, 50948, 949, 321, 12240, 439, 295, 264, 41674, 1854, 264, 4846, 1783, 1228, 341, 574, 1010, 51171, 51171, 3199, 383, 718, 385, 767, 445, 853, 281, 12240, 257, 2167, 2609, 24922, 411, 51418, 51418, 584, 1025, 370, 321, 483, 257, 2020, 295, 577, 341, 1985, 13, 823, 472, 636, 341, 1985, 295, 1164, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.10067427030173681, "compression_ratio": 1.59915611814346, "no_speech_prob": 4.092849394510267e-06}, {"id": 165, "seek": 77652, "start": 788.1999999999999, "end": 792.66, "text": " before we embed all of the integers inside the input X using this lookup", "tokens": [50364, 370, 1184, 945, 1184, 472, 295, 7634, 4342, 486, 362, 257, 732, 18795, 12240, 3584, 13, 50608, 50608, 407, 300, 311, 527, 8141, 383, 295, 12240, 29432, 294, 264, 2863, 5883, 1602, 16979, 13, 823, 50948, 50948, 949, 321, 12240, 439, 295, 264, 41674, 1854, 264, 4846, 1783, 1228, 341, 574, 1010, 51171, 51171, 3199, 383, 718, 385, 767, 445, 853, 281, 12240, 257, 2167, 2609, 24922, 411, 51418, 51418, 584, 1025, 370, 321, 483, 257, 2020, 295, 577, 341, 1985, 13, 823, 472, 636, 341, 1985, 295, 1164, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.10067427030173681, "compression_ratio": 1.59915611814346, "no_speech_prob": 4.092849394510267e-06}, {"id": 166, "seek": 77652, "start": 792.66, "end": 797.6, "text": " table C let me actually just try to embed a single individual integer like", "tokens": [50364, 370, 1184, 945, 1184, 472, 295, 7634, 4342, 486, 362, 257, 732, 18795, 12240, 3584, 13, 50608, 50608, 407, 300, 311, 527, 8141, 383, 295, 12240, 29432, 294, 264, 2863, 5883, 1602, 16979, 13, 823, 50948, 50948, 949, 321, 12240, 439, 295, 264, 41674, 1854, 264, 4846, 1783, 1228, 341, 574, 1010, 51171, 51171, 3199, 383, 718, 385, 767, 445, 853, 281, 12240, 257, 2167, 2609, 24922, 411, 51418, 51418, 584, 1025, 370, 321, 483, 257, 2020, 295, 577, 341, 1985, 13, 823, 472, 636, 341, 1985, 295, 1164, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.10067427030173681, "compression_ratio": 1.59915611814346, "no_speech_prob": 4.092849394510267e-06}, {"id": 167, "seek": 77652, "start": 797.6, "end": 804.04, "text": " say 5 so we get a sense of how this works. Now one way this works of course", "tokens": [50364, 370, 1184, 945, 1184, 472, 295, 7634, 4342, 486, 362, 257, 732, 18795, 12240, 3584, 13, 50608, 50608, 407, 300, 311, 527, 8141, 383, 295, 12240, 29432, 294, 264, 2863, 5883, 1602, 16979, 13, 823, 50948, 50948, 949, 321, 12240, 439, 295, 264, 41674, 1854, 264, 4846, 1783, 1228, 341, 574, 1010, 51171, 51171, 3199, 383, 718, 385, 767, 445, 853, 281, 12240, 257, 2167, 2609, 24922, 411, 51418, 51418, 584, 1025, 370, 321, 483, 257, 2020, 295, 577, 341, 1985, 13, 823, 472, 636, 341, 1985, 295, 1164, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.10067427030173681, "compression_ratio": 1.59915611814346, "no_speech_prob": 4.092849394510267e-06}, {"id": 168, "seek": 80404, "start": 804.04, "end": 808.92, "text": " is we can just take the C and we can index into row 5 and that gives us a", "tokens": [50364, 307, 321, 393, 445, 747, 264, 383, 293, 321, 393, 8186, 666, 5386, 1025, 293, 300, 2709, 505, 257, 50608, 50608, 8062, 264, 3318, 5386, 295, 383, 293, 341, 307, 472, 636, 281, 360, 309, 13, 440, 661, 636, 300, 286, 50944, 50944, 8212, 294, 264, 3894, 7991, 307, 767, 18709, 819, 457, 51132, 51132, 767, 14800, 13, 407, 294, 264, 3894, 7991, 437, 321, 630, 307, 321, 1890, 613, 51302, 51302, 41674, 293, 321, 1143, 264, 472, 2368, 43430, 281, 700, 2058, 1429, 552, 13, 407, 479, 5893, 472, 2368, 51570, 51570, 321, 528, 281, 2058, 1429, 24922, 1025, 293, 321, 528, 281, 980, 309, 300, 641, 1230, 295, 5359, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.13737072322679603, "compression_ratio": 1.8273092369477912, "no_speech_prob": 5.771684755018214e-06}, {"id": 169, "seek": 80404, "start": 808.92, "end": 815.64, "text": " vector the fit row of C and this is one way to do it. The other way that I", "tokens": [50364, 307, 321, 393, 445, 747, 264, 383, 293, 321, 393, 8186, 666, 5386, 1025, 293, 300, 2709, 505, 257, 50608, 50608, 8062, 264, 3318, 5386, 295, 383, 293, 341, 307, 472, 636, 281, 360, 309, 13, 440, 661, 636, 300, 286, 50944, 50944, 8212, 294, 264, 3894, 7991, 307, 767, 18709, 819, 457, 51132, 51132, 767, 14800, 13, 407, 294, 264, 3894, 7991, 437, 321, 630, 307, 321, 1890, 613, 51302, 51302, 41674, 293, 321, 1143, 264, 472, 2368, 43430, 281, 700, 2058, 1429, 552, 13, 407, 479, 5893, 472, 2368, 51570, 51570, 321, 528, 281, 2058, 1429, 24922, 1025, 293, 321, 528, 281, 980, 309, 300, 641, 1230, 295, 5359, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.13737072322679603, "compression_ratio": 1.8273092369477912, "no_speech_prob": 5.771684755018214e-06}, {"id": 170, "seek": 80404, "start": 815.64, "end": 819.4, "text": " presented in the previous lecture is actually seemingly different but", "tokens": [50364, 307, 321, 393, 445, 747, 264, 383, 293, 321, 393, 8186, 666, 5386, 1025, 293, 300, 2709, 505, 257, 50608, 50608, 8062, 264, 3318, 5386, 295, 383, 293, 341, 307, 472, 636, 281, 360, 309, 13, 440, 661, 636, 300, 286, 50944, 50944, 8212, 294, 264, 3894, 7991, 307, 767, 18709, 819, 457, 51132, 51132, 767, 14800, 13, 407, 294, 264, 3894, 7991, 437, 321, 630, 307, 321, 1890, 613, 51302, 51302, 41674, 293, 321, 1143, 264, 472, 2368, 43430, 281, 700, 2058, 1429, 552, 13, 407, 479, 5893, 472, 2368, 51570, 51570, 321, 528, 281, 2058, 1429, 24922, 1025, 293, 321, 528, 281, 980, 309, 300, 641, 1230, 295, 5359, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.13737072322679603, "compression_ratio": 1.8273092369477912, "no_speech_prob": 5.771684755018214e-06}, {"id": 171, "seek": 80404, "start": 819.4, "end": 822.8, "text": " actually identical. So in the previous lecture what we did is we took these", "tokens": [50364, 307, 321, 393, 445, 747, 264, 383, 293, 321, 393, 8186, 666, 5386, 1025, 293, 300, 2709, 505, 257, 50608, 50608, 8062, 264, 3318, 5386, 295, 383, 293, 341, 307, 472, 636, 281, 360, 309, 13, 440, 661, 636, 300, 286, 50944, 50944, 8212, 294, 264, 3894, 7991, 307, 767, 18709, 819, 457, 51132, 51132, 767, 14800, 13, 407, 294, 264, 3894, 7991, 437, 321, 630, 307, 321, 1890, 613, 51302, 51302, 41674, 293, 321, 1143, 264, 472, 2368, 43430, 281, 700, 2058, 1429, 552, 13, 407, 479, 5893, 472, 2368, 51570, 51570, 321, 528, 281, 2058, 1429, 24922, 1025, 293, 321, 528, 281, 980, 309, 300, 641, 1230, 295, 5359, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.13737072322679603, "compression_ratio": 1.8273092369477912, "no_speech_prob": 5.771684755018214e-06}, {"id": 172, "seek": 80404, "start": 822.8, "end": 828.16, "text": " integers and we used the one hot encoding to first encode them. So F dot one hot", "tokens": [50364, 307, 321, 393, 445, 747, 264, 383, 293, 321, 393, 8186, 666, 5386, 1025, 293, 300, 2709, 505, 257, 50608, 50608, 8062, 264, 3318, 5386, 295, 383, 293, 341, 307, 472, 636, 281, 360, 309, 13, 440, 661, 636, 300, 286, 50944, 50944, 8212, 294, 264, 3894, 7991, 307, 767, 18709, 819, 457, 51132, 51132, 767, 14800, 13, 407, 294, 264, 3894, 7991, 437, 321, 630, 307, 321, 1890, 613, 51302, 51302, 41674, 293, 321, 1143, 264, 472, 2368, 43430, 281, 700, 2058, 1429, 552, 13, 407, 479, 5893, 472, 2368, 51570, 51570, 321, 528, 281, 2058, 1429, 24922, 1025, 293, 321, 528, 281, 980, 309, 300, 641, 1230, 295, 5359, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.13737072322679603, "compression_ratio": 1.8273092369477912, "no_speech_prob": 5.771684755018214e-06}, {"id": 173, "seek": 80404, "start": 828.16, "end": 832.7199999999999, "text": " we want to encode integer 5 and we want to tell it that their number of classes", "tokens": [50364, 307, 321, 393, 445, 747, 264, 383, 293, 321, 393, 8186, 666, 5386, 1025, 293, 300, 2709, 505, 257, 50608, 50608, 8062, 264, 3318, 5386, 295, 383, 293, 341, 307, 472, 636, 281, 360, 309, 13, 440, 661, 636, 300, 286, 50944, 50944, 8212, 294, 264, 3894, 7991, 307, 767, 18709, 819, 457, 51132, 51132, 767, 14800, 13, 407, 294, 264, 3894, 7991, 437, 321, 630, 307, 321, 1890, 613, 51302, 51302, 41674, 293, 321, 1143, 264, 472, 2368, 43430, 281, 700, 2058, 1429, 552, 13, 407, 479, 5893, 472, 2368, 51570, 51570, 321, 528, 281, 2058, 1429, 24922, 1025, 293, 321, 528, 281, 980, 309, 300, 641, 1230, 295, 5359, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.13737072322679603, "compression_ratio": 1.8273092369477912, "no_speech_prob": 5.771684755018214e-06}, {"id": 174, "seek": 83272, "start": 832.72, "end": 838.0, "text": " is 27 so that's the 26 dimensional vector of all zeros except the fifth bit is", "tokens": [50364, 307, 7634, 370, 300, 311, 264, 7551, 18795, 8062, 295, 439, 35193, 3993, 264, 9266, 857, 307, 50628, 50628, 3574, 322, 13, 823, 341, 767, 1177, 380, 589, 264, 1778, 307, 300, 341, 4846, 50994, 50994, 767, 1633, 312, 257, 27822, 5893, 40863, 293, 286, 478, 1455, 512, 295, 613, 13603, 51190, 51190, 22062, 445, 370, 291, 483, 281, 536, 512, 13603, 293, 577, 281, 3191, 552, 13, 407, 341, 51390, 51390, 1633, 312, 257, 40863, 406, 364, 560, 6457, 15325, 281, 3191, 321, 483, 257, 472, 2368, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.12745624003203018, "compression_ratio": 1.6233766233766234, "no_speech_prob": 9.97269216895802e-06}, {"id": 175, "seek": 83272, "start": 838.0, "end": 845.32, "text": " turned on. Now this actually doesn't work the reason is that this input", "tokens": [50364, 307, 7634, 370, 300, 311, 264, 7551, 18795, 8062, 295, 439, 35193, 3993, 264, 9266, 857, 307, 50628, 50628, 3574, 322, 13, 823, 341, 767, 1177, 380, 589, 264, 1778, 307, 300, 341, 4846, 50994, 50994, 767, 1633, 312, 257, 27822, 5893, 40863, 293, 286, 478, 1455, 512, 295, 613, 13603, 51190, 51190, 22062, 445, 370, 291, 483, 281, 536, 512, 13603, 293, 577, 281, 3191, 552, 13, 407, 341, 51390, 51390, 1633, 312, 257, 40863, 406, 364, 560, 6457, 15325, 281, 3191, 321, 483, 257, 472, 2368, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.12745624003203018, "compression_ratio": 1.6233766233766234, "no_speech_prob": 9.97269216895802e-06}, {"id": 176, "seek": 83272, "start": 845.32, "end": 849.24, "text": " actually must be a torch dot tensor and I'm making some of these errors", "tokens": [50364, 307, 7634, 370, 300, 311, 264, 7551, 18795, 8062, 295, 439, 35193, 3993, 264, 9266, 857, 307, 50628, 50628, 3574, 322, 13, 823, 341, 767, 1177, 380, 589, 264, 1778, 307, 300, 341, 4846, 50994, 50994, 767, 1633, 312, 257, 27822, 5893, 40863, 293, 286, 478, 1455, 512, 295, 613, 13603, 51190, 51190, 22062, 445, 370, 291, 483, 281, 536, 512, 13603, 293, 577, 281, 3191, 552, 13, 407, 341, 51390, 51390, 1633, 312, 257, 40863, 406, 364, 560, 6457, 15325, 281, 3191, 321, 483, 257, 472, 2368, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.12745624003203018, "compression_ratio": 1.6233766233766234, "no_speech_prob": 9.97269216895802e-06}, {"id": 177, "seek": 83272, "start": 849.24, "end": 853.24, "text": " intentionally just so you get to see some errors and how to fix them. So this", "tokens": [50364, 307, 7634, 370, 300, 311, 264, 7551, 18795, 8062, 295, 439, 35193, 3993, 264, 9266, 857, 307, 50628, 50628, 3574, 322, 13, 823, 341, 767, 1177, 380, 589, 264, 1778, 307, 300, 341, 4846, 50994, 50994, 767, 1633, 312, 257, 27822, 5893, 40863, 293, 286, 478, 1455, 512, 295, 613, 13603, 51190, 51190, 22062, 445, 370, 291, 483, 281, 536, 512, 13603, 293, 577, 281, 3191, 552, 13, 407, 341, 51390, 51390, 1633, 312, 257, 40863, 406, 364, 560, 6457, 15325, 281, 3191, 321, 483, 257, 472, 2368, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.12745624003203018, "compression_ratio": 1.6233766233766234, "no_speech_prob": 9.97269216895802e-06}, {"id": 178, "seek": 83272, "start": 853.24, "end": 857.0400000000001, "text": " must be a tensor not an int fairly straightforward to fix we get a one hot", "tokens": [50364, 307, 7634, 370, 300, 311, 264, 7551, 18795, 8062, 295, 439, 35193, 3993, 264, 9266, 857, 307, 50628, 50628, 3574, 322, 13, 823, 341, 767, 1177, 380, 589, 264, 1778, 307, 300, 341, 4846, 50994, 50994, 767, 1633, 312, 257, 27822, 5893, 40863, 293, 286, 478, 1455, 512, 295, 613, 13603, 51190, 51190, 22062, 445, 370, 291, 483, 281, 536, 512, 13603, 293, 577, 281, 3191, 552, 13, 407, 341, 51390, 51390, 1633, 312, 257, 40863, 406, 364, 560, 6457, 15325, 281, 3191, 321, 483, 257, 472, 2368, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.12745624003203018, "compression_ratio": 1.6233766233766234, "no_speech_prob": 9.97269216895802e-06}, {"id": 179, "seek": 85704, "start": 857.04, "end": 863.28, "text": " vector the fifth dimension is 1 and the shape of this is 27. And now notice that", "tokens": [50364, 8062, 264, 9266, 10139, 307, 502, 293, 264, 3909, 295, 341, 307, 7634, 13, 400, 586, 3449, 300, 50676, 50676, 445, 382, 286, 10515, 33919, 281, 294, 257, 3894, 960, 498, 321, 747, 341, 472, 2368, 50888, 50888, 8062, 293, 321, 12972, 309, 538, 383, 550, 437, 576, 291, 2066, 30, 1042, 1230, 472, 700, 51494, 51494, 291, 1116, 2066, 364, 6713, 570, 5176, 39684, 2010, 938, 457, 1352, 15706, 370, 257, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.12158209399173134, "compression_ratio": 1.4739336492890995, "no_speech_prob": 2.5465486032771878e-05}, {"id": 180, "seek": 85704, "start": 863.28, "end": 867.52, "text": " just as I briefly alluded to in a previous video if we take this one hot", "tokens": [50364, 8062, 264, 9266, 10139, 307, 502, 293, 264, 3909, 295, 341, 307, 7634, 13, 400, 586, 3449, 300, 50676, 50676, 445, 382, 286, 10515, 33919, 281, 294, 257, 3894, 960, 498, 321, 747, 341, 472, 2368, 50888, 50888, 8062, 293, 321, 12972, 309, 538, 383, 550, 437, 576, 291, 2066, 30, 1042, 1230, 472, 700, 51494, 51494, 291, 1116, 2066, 364, 6713, 570, 5176, 39684, 2010, 938, 457, 1352, 15706, 370, 257, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.12158209399173134, "compression_ratio": 1.4739336492890995, "no_speech_prob": 2.5465486032771878e-05}, {"id": 181, "seek": 85704, "start": 867.52, "end": 879.64, "text": " vector and we multiply it by C then what would you expect? Well number one first", "tokens": [50364, 8062, 264, 9266, 10139, 307, 502, 293, 264, 3909, 295, 341, 307, 7634, 13, 400, 586, 3449, 300, 50676, 50676, 445, 382, 286, 10515, 33919, 281, 294, 257, 3894, 960, 498, 321, 747, 341, 472, 2368, 50888, 50888, 8062, 293, 321, 12972, 309, 538, 383, 550, 437, 576, 291, 2066, 30, 1042, 1230, 472, 700, 51494, 51494, 291, 1116, 2066, 364, 6713, 570, 5176, 39684, 2010, 938, 457, 1352, 15706, 370, 257, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.12158209399173134, "compression_ratio": 1.4739336492890995, "no_speech_prob": 2.5465486032771878e-05}, {"id": 182, "seek": 85704, "start": 879.64, "end": 886.68, "text": " you'd expect an error because expected scalar type long but found float so a", "tokens": [50364, 8062, 264, 9266, 10139, 307, 502, 293, 264, 3909, 295, 341, 307, 7634, 13, 400, 586, 3449, 300, 50676, 50676, 445, 382, 286, 10515, 33919, 281, 294, 257, 3894, 960, 498, 321, 747, 341, 472, 2368, 50888, 50888, 8062, 293, 321, 12972, 309, 538, 383, 550, 437, 576, 291, 2066, 30, 1042, 1230, 472, 700, 51494, 51494, 291, 1116, 2066, 364, 6713, 570, 5176, 39684, 2010, 938, 457, 1352, 15706, 370, 257, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.12158209399173134, "compression_ratio": 1.4739336492890995, "no_speech_prob": 2.5465486032771878e-05}, {"id": 183, "seek": 88668, "start": 886.68, "end": 891.12, "text": " little bit confusing but the problem here is that one hot the data type of it", "tokens": [50364, 707, 857, 13181, 457, 264, 1154, 510, 307, 300, 472, 2368, 264, 1412, 2010, 295, 309, 50586, 50586, 307, 938, 309, 311, 257, 12145, 12, 5260, 24922, 457, 341, 307, 257, 15706, 40863, 293, 370, 9953, 51, 284, 339, 1177, 380, 50986, 50986, 458, 577, 281, 12972, 364, 560, 365, 257, 15706, 293, 300, 311, 983, 321, 632, 281, 20803, 51196, 51196, 4193, 341, 281, 257, 15706, 370, 300, 321, 393, 12972, 13, 823, 264, 5598, 767, 510, 307, 51498, 51498, 14800, 293, 300, 309, 311, 14800, 570, 295, 264, 636, 264, 8141, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.1262746201348059, "compression_ratio": 1.6711111111111112, "no_speech_prob": 4.222757979732705e-06}, {"id": 184, "seek": 88668, "start": 891.12, "end": 899.12, "text": " is long it's a 64-bit integer but this is a float tensor and so PyTorch doesn't", "tokens": [50364, 707, 857, 13181, 457, 264, 1154, 510, 307, 300, 472, 2368, 264, 1412, 2010, 295, 309, 50586, 50586, 307, 938, 309, 311, 257, 12145, 12, 5260, 24922, 457, 341, 307, 257, 15706, 40863, 293, 370, 9953, 51, 284, 339, 1177, 380, 50986, 50986, 458, 577, 281, 12972, 364, 560, 365, 257, 15706, 293, 300, 311, 983, 321, 632, 281, 20803, 51196, 51196, 4193, 341, 281, 257, 15706, 370, 300, 321, 393, 12972, 13, 823, 264, 5598, 767, 510, 307, 51498, 51498, 14800, 293, 300, 309, 311, 14800, 570, 295, 264, 636, 264, 8141, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.1262746201348059, "compression_ratio": 1.6711111111111112, "no_speech_prob": 4.222757979732705e-06}, {"id": 185, "seek": 88668, "start": 899.12, "end": 903.3199999999999, "text": " know how to multiply an int with a float and that's why we had to explicitly", "tokens": [50364, 707, 857, 13181, 457, 264, 1154, 510, 307, 300, 472, 2368, 264, 1412, 2010, 295, 309, 50586, 50586, 307, 938, 309, 311, 257, 12145, 12, 5260, 24922, 457, 341, 307, 257, 15706, 40863, 293, 370, 9953, 51, 284, 339, 1177, 380, 50986, 50986, 458, 577, 281, 12972, 364, 560, 365, 257, 15706, 293, 300, 311, 983, 321, 632, 281, 20803, 51196, 51196, 4193, 341, 281, 257, 15706, 370, 300, 321, 393, 12972, 13, 823, 264, 5598, 767, 510, 307, 51498, 51498, 14800, 293, 300, 309, 311, 14800, 570, 295, 264, 636, 264, 8141, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.1262746201348059, "compression_ratio": 1.6711111111111112, "no_speech_prob": 4.222757979732705e-06}, {"id": 186, "seek": 88668, "start": 903.3199999999999, "end": 909.3599999999999, "text": " cast this to a float so that we can multiply. Now the output actually here is", "tokens": [50364, 707, 857, 13181, 457, 264, 1154, 510, 307, 300, 472, 2368, 264, 1412, 2010, 295, 309, 50586, 50586, 307, 938, 309, 311, 257, 12145, 12, 5260, 24922, 457, 341, 307, 257, 15706, 40863, 293, 370, 9953, 51, 284, 339, 1177, 380, 50986, 50986, 458, 577, 281, 12972, 364, 560, 365, 257, 15706, 293, 300, 311, 983, 321, 632, 281, 20803, 51196, 51196, 4193, 341, 281, 257, 15706, 370, 300, 321, 393, 12972, 13, 823, 264, 5598, 767, 510, 307, 51498, 51498, 14800, 293, 300, 309, 311, 14800, 570, 295, 264, 636, 264, 8141, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.1262746201348059, "compression_ratio": 1.6711111111111112, "no_speech_prob": 4.222757979732705e-06}, {"id": 187, "seek": 88668, "start": 909.3599999999999, "end": 913.52, "text": " identical and that it's identical because of the way the matrix", "tokens": [50364, 707, 857, 13181, 457, 264, 1154, 510, 307, 300, 472, 2368, 264, 1412, 2010, 295, 309, 50586, 50586, 307, 938, 309, 311, 257, 12145, 12, 5260, 24922, 457, 341, 307, 257, 15706, 40863, 293, 370, 9953, 51, 284, 339, 1177, 380, 50986, 50986, 458, 577, 281, 12972, 364, 560, 365, 257, 15706, 293, 300, 311, 983, 321, 632, 281, 20803, 51196, 51196, 4193, 341, 281, 257, 15706, 370, 300, 321, 393, 12972, 13, 823, 264, 5598, 767, 510, 307, 51498, 51498, 14800, 293, 300, 309, 311, 14800, 570, 295, 264, 636, 264, 8141, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.1262746201348059, "compression_ratio": 1.6711111111111112, "no_speech_prob": 4.222757979732705e-06}, {"id": 188, "seek": 91352, "start": 913.52, "end": 919.48, "text": " multiplication here works. We have the one hot vector multiplying columns of C", "tokens": [50364, 27290, 510, 1985, 13, 492, 362, 264, 472, 2368, 8062, 30955, 13766, 295, 383, 50662, 50662, 293, 570, 295, 439, 264, 35193, 436, 767, 917, 493, 31226, 484, 1203, 50872, 50872, 294, 383, 3993, 337, 264, 9266, 5386, 597, 307, 41514, 292, 484, 293, 370, 321, 767, 8881, 412, 51120, 51120, 264, 912, 1874, 293, 300, 5112, 291, 300, 510, 321, 393, 7302, 341, 700, 2522, 51382, 51382, 510, 341, 12240, 3584, 295, 264, 24922, 321, 393, 2139, 519, 295, 309, 382, 264, 24922, 51572, 51572, 8186, 278, 666, 257, 574, 1010, 3199, 383, 457, 9052, 2276, 321, 393, 611, 519, 295, 341, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.09749960676531925, "compression_ratio": 1.7624521072796935, "no_speech_prob": 1.7502619812148623e-05}, {"id": 189, "seek": 91352, "start": 919.48, "end": 923.68, "text": " and because of all the zeros they actually end up masking out everything", "tokens": [50364, 27290, 510, 1985, 13, 492, 362, 264, 472, 2368, 8062, 30955, 13766, 295, 383, 50662, 50662, 293, 570, 295, 439, 264, 35193, 436, 767, 917, 493, 31226, 484, 1203, 50872, 50872, 294, 383, 3993, 337, 264, 9266, 5386, 597, 307, 41514, 292, 484, 293, 370, 321, 767, 8881, 412, 51120, 51120, 264, 912, 1874, 293, 300, 5112, 291, 300, 510, 321, 393, 7302, 341, 700, 2522, 51382, 51382, 510, 341, 12240, 3584, 295, 264, 24922, 321, 393, 2139, 519, 295, 309, 382, 264, 24922, 51572, 51572, 8186, 278, 666, 257, 574, 1010, 3199, 383, 457, 9052, 2276, 321, 393, 611, 519, 295, 341, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.09749960676531925, "compression_ratio": 1.7624521072796935, "no_speech_prob": 1.7502619812148623e-05}, {"id": 190, "seek": 91352, "start": 923.68, "end": 928.64, "text": " in C except for the fifth row which is plucked out and so we actually arrive at", "tokens": [50364, 27290, 510, 1985, 13, 492, 362, 264, 472, 2368, 8062, 30955, 13766, 295, 383, 50662, 50662, 293, 570, 295, 439, 264, 35193, 436, 767, 917, 493, 31226, 484, 1203, 50872, 50872, 294, 383, 3993, 337, 264, 9266, 5386, 597, 307, 41514, 292, 484, 293, 370, 321, 767, 8881, 412, 51120, 51120, 264, 912, 1874, 293, 300, 5112, 291, 300, 510, 321, 393, 7302, 341, 700, 2522, 51382, 51382, 510, 341, 12240, 3584, 295, 264, 24922, 321, 393, 2139, 519, 295, 309, 382, 264, 24922, 51572, 51572, 8186, 278, 666, 257, 574, 1010, 3199, 383, 457, 9052, 2276, 321, 393, 611, 519, 295, 341, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.09749960676531925, "compression_ratio": 1.7624521072796935, "no_speech_prob": 1.7502619812148623e-05}, {"id": 191, "seek": 91352, "start": 928.64, "end": 933.88, "text": " the same result and that tells you that here we can interpret this first piece", "tokens": [50364, 27290, 510, 1985, 13, 492, 362, 264, 472, 2368, 8062, 30955, 13766, 295, 383, 50662, 50662, 293, 570, 295, 439, 264, 35193, 436, 767, 917, 493, 31226, 484, 1203, 50872, 50872, 294, 383, 3993, 337, 264, 9266, 5386, 597, 307, 41514, 292, 484, 293, 370, 321, 767, 8881, 412, 51120, 51120, 264, 912, 1874, 293, 300, 5112, 291, 300, 510, 321, 393, 7302, 341, 700, 2522, 51382, 51382, 510, 341, 12240, 3584, 295, 264, 24922, 321, 393, 2139, 519, 295, 309, 382, 264, 24922, 51572, 51572, 8186, 278, 666, 257, 574, 1010, 3199, 383, 457, 9052, 2276, 321, 393, 611, 519, 295, 341, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.09749960676531925, "compression_ratio": 1.7624521072796935, "no_speech_prob": 1.7502619812148623e-05}, {"id": 192, "seek": 91352, "start": 933.88, "end": 937.68, "text": " here this embedding of the integer we can either think of it as the integer", "tokens": [50364, 27290, 510, 1985, 13, 492, 362, 264, 472, 2368, 8062, 30955, 13766, 295, 383, 50662, 50662, 293, 570, 295, 439, 264, 35193, 436, 767, 917, 493, 31226, 484, 1203, 50872, 50872, 294, 383, 3993, 337, 264, 9266, 5386, 597, 307, 41514, 292, 484, 293, 370, 321, 767, 8881, 412, 51120, 51120, 264, 912, 1874, 293, 300, 5112, 291, 300, 510, 321, 393, 7302, 341, 700, 2522, 51382, 51382, 510, 341, 12240, 3584, 295, 264, 24922, 321, 393, 2139, 519, 295, 309, 382, 264, 24922, 51572, 51572, 8186, 278, 666, 257, 574, 1010, 3199, 383, 457, 9052, 2276, 321, 393, 611, 519, 295, 341, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.09749960676531925, "compression_ratio": 1.7624521072796935, "no_speech_prob": 1.7502619812148623e-05}, {"id": 193, "seek": 91352, "start": 937.68, "end": 942.16, "text": " indexing into a lookup table C but equivalently we can also think of this", "tokens": [50364, 27290, 510, 1985, 13, 492, 362, 264, 472, 2368, 8062, 30955, 13766, 295, 383, 50662, 50662, 293, 570, 295, 439, 264, 35193, 436, 767, 917, 493, 31226, 484, 1203, 50872, 50872, 294, 383, 3993, 337, 264, 9266, 5386, 597, 307, 41514, 292, 484, 293, 370, 321, 767, 8881, 412, 51120, 51120, 264, 912, 1874, 293, 300, 5112, 291, 300, 510, 321, 393, 7302, 341, 700, 2522, 51382, 51382, 510, 341, 12240, 3584, 295, 264, 24922, 321, 393, 2139, 519, 295, 309, 382, 264, 24922, 51572, 51572, 8186, 278, 666, 257, 574, 1010, 3199, 383, 457, 9052, 2276, 321, 393, 611, 519, 295, 341, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.09749960676531925, "compression_ratio": 1.7624521072796935, "no_speech_prob": 1.7502619812148623e-05}, {"id": 194, "seek": 94216, "start": 942.16, "end": 947.48, "text": " little piece here as a first layer of this bigger neural net this layer here", "tokens": [50364, 707, 2522, 510, 382, 257, 700, 4583, 295, 341, 3801, 18161, 2533, 341, 4583, 510, 50630, 50630, 575, 22027, 300, 362, 572, 2107, 12, 1889, 17409, 456, 311, 572, 7603, 71, 436, 434, 445, 8213, 50828, 50828, 22027, 293, 641, 3364, 8141, 307, 383, 293, 550, 321, 366, 43430, 41674, 666, 472, 51164, 51164, 2368, 293, 12919, 729, 666, 257, 18161, 2533, 293, 341, 700, 4583, 1936, 12240, 82, 51354, 51354, 552, 370, 729, 366, 732, 10344, 2098, 295, 884, 264, 912, 551, 321, 434, 445, 516, 281, 51570, 51570, 8186, 570, 309, 311, 709, 709, 4663, 293, 321, 434, 516, 281, 31597, 341, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.094931337568495, "compression_ratio": 1.8582995951417005, "no_speech_prob": 1.5935907867969945e-05}, {"id": 195, "seek": 94216, "start": 947.48, "end": 951.4399999999999, "text": " has neurons that have no non-linearity there's no tanh they're just linear", "tokens": [50364, 707, 2522, 510, 382, 257, 700, 4583, 295, 341, 3801, 18161, 2533, 341, 4583, 510, 50630, 50630, 575, 22027, 300, 362, 572, 2107, 12, 1889, 17409, 456, 311, 572, 7603, 71, 436, 434, 445, 8213, 50828, 50828, 22027, 293, 641, 3364, 8141, 307, 383, 293, 550, 321, 366, 43430, 41674, 666, 472, 51164, 51164, 2368, 293, 12919, 729, 666, 257, 18161, 2533, 293, 341, 700, 4583, 1936, 12240, 82, 51354, 51354, 552, 370, 729, 366, 732, 10344, 2098, 295, 884, 264, 912, 551, 321, 434, 445, 516, 281, 51570, 51570, 8186, 570, 309, 311, 709, 709, 4663, 293, 321, 434, 516, 281, 31597, 341, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.094931337568495, "compression_ratio": 1.8582995951417005, "no_speech_prob": 1.5935907867969945e-05}, {"id": 196, "seek": 94216, "start": 951.4399999999999, "end": 958.16, "text": " neurons and their weight matrix is C and then we are encoding integers into one", "tokens": [50364, 707, 2522, 510, 382, 257, 700, 4583, 295, 341, 3801, 18161, 2533, 341, 4583, 510, 50630, 50630, 575, 22027, 300, 362, 572, 2107, 12, 1889, 17409, 456, 311, 572, 7603, 71, 436, 434, 445, 8213, 50828, 50828, 22027, 293, 641, 3364, 8141, 307, 383, 293, 550, 321, 366, 43430, 41674, 666, 472, 51164, 51164, 2368, 293, 12919, 729, 666, 257, 18161, 2533, 293, 341, 700, 4583, 1936, 12240, 82, 51354, 51354, 552, 370, 729, 366, 732, 10344, 2098, 295, 884, 264, 912, 551, 321, 434, 445, 516, 281, 51570, 51570, 8186, 570, 309, 311, 709, 709, 4663, 293, 321, 434, 516, 281, 31597, 341, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.094931337568495, "compression_ratio": 1.8582995951417005, "no_speech_prob": 1.5935907867969945e-05}, {"id": 197, "seek": 94216, "start": 958.16, "end": 961.9599999999999, "text": " hot and feeding those into a neural net and this first layer basically embeds", "tokens": [50364, 707, 2522, 510, 382, 257, 700, 4583, 295, 341, 3801, 18161, 2533, 341, 4583, 510, 50630, 50630, 575, 22027, 300, 362, 572, 2107, 12, 1889, 17409, 456, 311, 572, 7603, 71, 436, 434, 445, 8213, 50828, 50828, 22027, 293, 641, 3364, 8141, 307, 383, 293, 550, 321, 366, 43430, 41674, 666, 472, 51164, 51164, 2368, 293, 12919, 729, 666, 257, 18161, 2533, 293, 341, 700, 4583, 1936, 12240, 82, 51354, 51354, 552, 370, 729, 366, 732, 10344, 2098, 295, 884, 264, 912, 551, 321, 434, 445, 516, 281, 51570, 51570, 8186, 570, 309, 311, 709, 709, 4663, 293, 321, 434, 516, 281, 31597, 341, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.094931337568495, "compression_ratio": 1.8582995951417005, "no_speech_prob": 1.5935907867969945e-05}, {"id": 198, "seek": 94216, "start": 961.9599999999999, "end": 966.28, "text": " them so those are two equivalent ways of doing the same thing we're just going to", "tokens": [50364, 707, 2522, 510, 382, 257, 700, 4583, 295, 341, 3801, 18161, 2533, 341, 4583, 510, 50630, 50630, 575, 22027, 300, 362, 572, 2107, 12, 1889, 17409, 456, 311, 572, 7603, 71, 436, 434, 445, 8213, 50828, 50828, 22027, 293, 641, 3364, 8141, 307, 383, 293, 550, 321, 366, 43430, 41674, 666, 472, 51164, 51164, 2368, 293, 12919, 729, 666, 257, 18161, 2533, 293, 341, 700, 4583, 1936, 12240, 82, 51354, 51354, 552, 370, 729, 366, 732, 10344, 2098, 295, 884, 264, 912, 551, 321, 434, 445, 516, 281, 51570, 51570, 8186, 570, 309, 311, 709, 709, 4663, 293, 321, 434, 516, 281, 31597, 341, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.094931337568495, "compression_ratio": 1.8582995951417005, "no_speech_prob": 1.5935907867969945e-05}, {"id": 199, "seek": 94216, "start": 966.28, "end": 969.48, "text": " index because it's much much faster and we're going to discard this", "tokens": [50364, 707, 2522, 510, 382, 257, 700, 4583, 295, 341, 3801, 18161, 2533, 341, 4583, 510, 50630, 50630, 575, 22027, 300, 362, 572, 2107, 12, 1889, 17409, 456, 311, 572, 7603, 71, 436, 434, 445, 8213, 50828, 50828, 22027, 293, 641, 3364, 8141, 307, 383, 293, 550, 321, 366, 43430, 41674, 666, 472, 51164, 51164, 2368, 293, 12919, 729, 666, 257, 18161, 2533, 293, 341, 700, 4583, 1936, 12240, 82, 51354, 51354, 552, 370, 729, 366, 732, 10344, 2098, 295, 884, 264, 912, 551, 321, 434, 445, 516, 281, 51570, 51570, 8186, 570, 309, 311, 709, 709, 4663, 293, 321, 434, 516, 281, 31597, 341, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.094931337568495, "compression_ratio": 1.8582995951417005, "no_speech_prob": 1.5935907867969945e-05}, {"id": 200, "seek": 96948, "start": 969.48, "end": 974.04, "text": " interpretation of one hot inputs into neural nets and we're just going to", "tokens": [50364, 14174, 295, 472, 2368, 15743, 666, 18161, 36170, 293, 321, 434, 445, 516, 281, 50592, 50592, 8186, 41674, 293, 1884, 293, 764, 12240, 3584, 8020, 586, 12240, 3584, 257, 2167, 50816, 50816, 24922, 411, 1732, 307, 1858, 1547, 321, 393, 2935, 1029, 538, 27822, 281, 30254, 264, 51048, 51048, 9266, 5386, 295, 383, 420, 264, 5386, 8186, 1732, 295, 383, 457, 577, 360, 321, 16561, 12240, 439, 51398, 51398, 295, 613, 8858, 538, 1045, 41674, 12187, 294, 10225, 1783, 22880, 538, 27822, 8186, 278, 307, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.1270361268118526, "compression_ratio": 1.7008928571428572, "no_speech_prob": 4.936800905852579e-06}, {"id": 201, "seek": 96948, "start": 974.04, "end": 978.52, "text": " index integers and create and use embedding tables now embedding a single", "tokens": [50364, 14174, 295, 472, 2368, 15743, 666, 18161, 36170, 293, 321, 434, 445, 516, 281, 50592, 50592, 8186, 41674, 293, 1884, 293, 764, 12240, 3584, 8020, 586, 12240, 3584, 257, 2167, 50816, 50816, 24922, 411, 1732, 307, 1858, 1547, 321, 393, 2935, 1029, 538, 27822, 281, 30254, 264, 51048, 51048, 9266, 5386, 295, 383, 420, 264, 5386, 8186, 1732, 295, 383, 457, 577, 360, 321, 16561, 12240, 439, 51398, 51398, 295, 613, 8858, 538, 1045, 41674, 12187, 294, 10225, 1783, 22880, 538, 27822, 8186, 278, 307, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.1270361268118526, "compression_ratio": 1.7008928571428572, "no_speech_prob": 4.936800905852579e-06}, {"id": 202, "seek": 96948, "start": 978.52, "end": 983.16, "text": " integer like five is easy enough we can simply ask by torch to retrieve the", "tokens": [50364, 14174, 295, 472, 2368, 15743, 666, 18161, 36170, 293, 321, 434, 445, 516, 281, 50592, 50592, 8186, 41674, 293, 1884, 293, 764, 12240, 3584, 8020, 586, 12240, 3584, 257, 2167, 50816, 50816, 24922, 411, 1732, 307, 1858, 1547, 321, 393, 2935, 1029, 538, 27822, 281, 30254, 264, 51048, 51048, 9266, 5386, 295, 383, 420, 264, 5386, 8186, 1732, 295, 383, 457, 577, 360, 321, 16561, 12240, 439, 51398, 51398, 295, 613, 8858, 538, 1045, 41674, 12187, 294, 10225, 1783, 22880, 538, 27822, 8186, 278, 307, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.1270361268118526, "compression_ratio": 1.7008928571428572, "no_speech_prob": 4.936800905852579e-06}, {"id": 203, "seek": 96948, "start": 983.16, "end": 990.16, "text": " fifth row of C or the row index five of C but how do we simultaneously embed all", "tokens": [50364, 14174, 295, 472, 2368, 15743, 666, 18161, 36170, 293, 321, 434, 445, 516, 281, 50592, 50592, 8186, 41674, 293, 1884, 293, 764, 12240, 3584, 8020, 586, 12240, 3584, 257, 2167, 50816, 50816, 24922, 411, 1732, 307, 1858, 1547, 321, 393, 2935, 1029, 538, 27822, 281, 30254, 264, 51048, 51048, 9266, 5386, 295, 383, 420, 264, 5386, 8186, 1732, 295, 383, 457, 577, 360, 321, 16561, 12240, 439, 51398, 51398, 295, 613, 8858, 538, 1045, 41674, 12187, 294, 10225, 1783, 22880, 538, 27822, 8186, 278, 307, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.1270361268118526, "compression_ratio": 1.7008928571428572, "no_speech_prob": 4.936800905852579e-06}, {"id": 204, "seek": 96948, "start": 990.16, "end": 996.12, "text": " of these 32 by three integers stored in array X luckily by torch indexing is", "tokens": [50364, 14174, 295, 472, 2368, 15743, 666, 18161, 36170, 293, 321, 434, 445, 516, 281, 50592, 50592, 8186, 41674, 293, 1884, 293, 764, 12240, 3584, 8020, 586, 12240, 3584, 257, 2167, 50816, 50816, 24922, 411, 1732, 307, 1858, 1547, 321, 393, 2935, 1029, 538, 27822, 281, 30254, 264, 51048, 51048, 9266, 5386, 295, 383, 420, 264, 5386, 8186, 1732, 295, 383, 457, 577, 360, 321, 16561, 12240, 439, 51398, 51398, 295, 613, 8858, 538, 1045, 41674, 12187, 294, 10225, 1783, 22880, 538, 27822, 8186, 278, 307, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.1270361268118526, "compression_ratio": 1.7008928571428572, "no_speech_prob": 4.936800905852579e-06}, {"id": 205, "seek": 99612, "start": 996.12, "end": 1002.5600000000001, "text": " fairly flexible and quite powerful so it doesn't just work to ask for a single", "tokens": [50364, 6457, 11358, 293, 1596, 4005, 370, 309, 1177, 380, 445, 589, 281, 1029, 337, 257, 2167, 50686, 50686, 4478, 1732, 411, 341, 291, 393, 767, 8186, 1228, 14511, 370, 337, 1365, 321, 393, 50924, 50924, 483, 264, 13241, 1732, 2309, 293, 3407, 293, 341, 486, 445, 589, 411, 341, 321, 393, 8186, 51174, 51174, 365, 257, 1329, 309, 1177, 380, 445, 362, 281, 312, 257, 1329, 309, 393, 611, 312, 257, 767, 40863, 295, 51414, 51414, 41674, 293, 321, 393, 8186, 365, 300, 370, 341, 307, 257, 24922, 40863, 1732, 2309, 3407, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09418703118960063, "compression_ratio": 1.9509803921568627, "no_speech_prob": 4.565841663861647e-06}, {"id": 206, "seek": 99612, "start": 1002.5600000000001, "end": 1007.32, "text": " element five like this you can actually index using lists so for example we can", "tokens": [50364, 6457, 11358, 293, 1596, 4005, 370, 309, 1177, 380, 445, 589, 281, 1029, 337, 257, 2167, 50686, 50686, 4478, 1732, 411, 341, 291, 393, 767, 8186, 1228, 14511, 370, 337, 1365, 321, 393, 50924, 50924, 483, 264, 13241, 1732, 2309, 293, 3407, 293, 341, 486, 445, 589, 411, 341, 321, 393, 8186, 51174, 51174, 365, 257, 1329, 309, 1177, 380, 445, 362, 281, 312, 257, 1329, 309, 393, 611, 312, 257, 767, 40863, 295, 51414, 51414, 41674, 293, 321, 393, 8186, 365, 300, 370, 341, 307, 257, 24922, 40863, 1732, 2309, 3407, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09418703118960063, "compression_ratio": 1.9509803921568627, "no_speech_prob": 4.565841663861647e-06}, {"id": 207, "seek": 99612, "start": 1007.32, "end": 1012.32, "text": " get the rows five six and seven and this will just work like this we can index", "tokens": [50364, 6457, 11358, 293, 1596, 4005, 370, 309, 1177, 380, 445, 589, 281, 1029, 337, 257, 2167, 50686, 50686, 4478, 1732, 411, 341, 291, 393, 767, 8186, 1228, 14511, 370, 337, 1365, 321, 393, 50924, 50924, 483, 264, 13241, 1732, 2309, 293, 3407, 293, 341, 486, 445, 589, 411, 341, 321, 393, 8186, 51174, 51174, 365, 257, 1329, 309, 1177, 380, 445, 362, 281, 312, 257, 1329, 309, 393, 611, 312, 257, 767, 40863, 295, 51414, 51414, 41674, 293, 321, 393, 8186, 365, 300, 370, 341, 307, 257, 24922, 40863, 1732, 2309, 3407, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09418703118960063, "compression_ratio": 1.9509803921568627, "no_speech_prob": 4.565841663861647e-06}, {"id": 208, "seek": 99612, "start": 1012.32, "end": 1017.12, "text": " with a list it doesn't just have to be a list it can also be a actually tensor of", "tokens": [50364, 6457, 11358, 293, 1596, 4005, 370, 309, 1177, 380, 445, 589, 281, 1029, 337, 257, 2167, 50686, 50686, 4478, 1732, 411, 341, 291, 393, 767, 8186, 1228, 14511, 370, 337, 1365, 321, 393, 50924, 50924, 483, 264, 13241, 1732, 2309, 293, 3407, 293, 341, 486, 445, 589, 411, 341, 321, 393, 8186, 51174, 51174, 365, 257, 1329, 309, 1177, 380, 445, 362, 281, 312, 257, 1329, 309, 393, 611, 312, 257, 767, 40863, 295, 51414, 51414, 41674, 293, 321, 393, 8186, 365, 300, 370, 341, 307, 257, 24922, 40863, 1732, 2309, 3407, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09418703118960063, "compression_ratio": 1.9509803921568627, "no_speech_prob": 4.565841663861647e-06}, {"id": 209, "seek": 99612, "start": 1017.12, "end": 1023.12, "text": " integers and we can index with that so this is a integer tensor five six seven", "tokens": [50364, 6457, 11358, 293, 1596, 4005, 370, 309, 1177, 380, 445, 589, 281, 1029, 337, 257, 2167, 50686, 50686, 4478, 1732, 411, 341, 291, 393, 767, 8186, 1228, 14511, 370, 337, 1365, 321, 393, 50924, 50924, 483, 264, 13241, 1732, 2309, 293, 3407, 293, 341, 486, 445, 589, 411, 341, 321, 393, 8186, 51174, 51174, 365, 257, 1329, 309, 1177, 380, 445, 362, 281, 312, 257, 1329, 309, 393, 611, 312, 257, 767, 40863, 295, 51414, 51414, 41674, 293, 321, 393, 8186, 365, 300, 370, 341, 307, 257, 24922, 40863, 1732, 2309, 3407, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09418703118960063, "compression_ratio": 1.9509803921568627, "no_speech_prob": 4.565841663861647e-06}, {"id": 210, "seek": 102312, "start": 1023.12, "end": 1029.12, "text": " and this will just work as well in fact we can also for example repeat row seven", "tokens": [50364, 293, 341, 486, 445, 589, 382, 731, 294, 1186, 321, 393, 611, 337, 1365, 7149, 5386, 3407, 50664, 50664, 293, 30254, 309, 3866, 1413, 293, 300, 912, 8186, 486, 445, 483, 16741, 50920, 50920, 3866, 1413, 510, 370, 510, 321, 366, 8186, 278, 365, 257, 472, 12, 18759, 40863, 51180, 51180, 295, 41674, 457, 309, 4523, 484, 300, 291, 393, 611, 8186, 365, 4825, 12, 18759, 51410, 51410, 10688, 830, 295, 41674, 510, 321, 362, 257, 732, 12, 18759, 294, 40863, 295, 41674, 370, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.08785517649217085, "compression_ratio": 1.9487179487179487, "no_speech_prob": 3.041525815206114e-06}, {"id": 211, "seek": 102312, "start": 1029.12, "end": 1034.24, "text": " and retrieve it multiple times and that same index will just get embedded", "tokens": [50364, 293, 341, 486, 445, 589, 382, 731, 294, 1186, 321, 393, 611, 337, 1365, 7149, 5386, 3407, 50664, 50664, 293, 30254, 309, 3866, 1413, 293, 300, 912, 8186, 486, 445, 483, 16741, 50920, 50920, 3866, 1413, 510, 370, 510, 321, 366, 8186, 278, 365, 257, 472, 12, 18759, 40863, 51180, 51180, 295, 41674, 457, 309, 4523, 484, 300, 291, 393, 611, 8186, 365, 4825, 12, 18759, 51410, 51410, 10688, 830, 295, 41674, 510, 321, 362, 257, 732, 12, 18759, 294, 40863, 295, 41674, 370, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.08785517649217085, "compression_ratio": 1.9487179487179487, "no_speech_prob": 3.041525815206114e-06}, {"id": 212, "seek": 102312, "start": 1034.24, "end": 1039.44, "text": " multiple times here so here we are indexing with a one-dimensional tensor", "tokens": [50364, 293, 341, 486, 445, 589, 382, 731, 294, 1186, 321, 393, 611, 337, 1365, 7149, 5386, 3407, 50664, 50664, 293, 30254, 309, 3866, 1413, 293, 300, 912, 8186, 486, 445, 483, 16741, 50920, 50920, 3866, 1413, 510, 370, 510, 321, 366, 8186, 278, 365, 257, 472, 12, 18759, 40863, 51180, 51180, 295, 41674, 457, 309, 4523, 484, 300, 291, 393, 611, 8186, 365, 4825, 12, 18759, 51410, 51410, 10688, 830, 295, 41674, 510, 321, 362, 257, 732, 12, 18759, 294, 40863, 295, 41674, 370, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.08785517649217085, "compression_ratio": 1.9487179487179487, "no_speech_prob": 3.041525815206114e-06}, {"id": 213, "seek": 102312, "start": 1039.44, "end": 1044.04, "text": " of integers but it turns out that you can also index with multi-dimensional", "tokens": [50364, 293, 341, 486, 445, 589, 382, 731, 294, 1186, 321, 393, 611, 337, 1365, 7149, 5386, 3407, 50664, 50664, 293, 30254, 309, 3866, 1413, 293, 300, 912, 8186, 486, 445, 483, 16741, 50920, 50920, 3866, 1413, 510, 370, 510, 321, 366, 8186, 278, 365, 257, 472, 12, 18759, 40863, 51180, 51180, 295, 41674, 457, 309, 4523, 484, 300, 291, 393, 611, 8186, 365, 4825, 12, 18759, 51410, 51410, 10688, 830, 295, 41674, 510, 321, 362, 257, 732, 12, 18759, 294, 40863, 295, 41674, 370, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.08785517649217085, "compression_ratio": 1.9487179487179487, "no_speech_prob": 3.041525815206114e-06}, {"id": 214, "seek": 102312, "start": 1044.04, "end": 1048.92, "text": " tensors of integers here we have a two-dimensional in tensor of integers so", "tokens": [50364, 293, 341, 486, 445, 589, 382, 731, 294, 1186, 321, 393, 611, 337, 1365, 7149, 5386, 3407, 50664, 50664, 293, 30254, 309, 3866, 1413, 293, 300, 912, 8186, 486, 445, 483, 16741, 50920, 50920, 3866, 1413, 510, 370, 510, 321, 366, 8186, 278, 365, 257, 472, 12, 18759, 40863, 51180, 51180, 295, 41674, 457, 309, 4523, 484, 300, 291, 393, 611, 8186, 365, 4825, 12, 18759, 51410, 51410, 10688, 830, 295, 41674, 510, 321, 362, 257, 732, 12, 18759, 294, 40863, 295, 41674, 370, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.08785517649217085, "compression_ratio": 1.9487179487179487, "no_speech_prob": 3.041525815206114e-06}, {"id": 215, "seek": 104892, "start": 1048.92, "end": 1057.76, "text": " we can simply just do C at X and this just works and the shape of this is 32", "tokens": [50364, 321, 393, 2935, 445, 360, 383, 412, 1783, 293, 341, 445, 1985, 293, 264, 3909, 295, 341, 307, 8858, 50806, 50806, 538, 805, 597, 307, 264, 3380, 3909, 293, 586, 337, 633, 472, 295, 729, 8858, 538, 805, 41674, 51028, 51028, 321, 600, 19817, 937, 264, 12240, 3584, 8062, 510, 370, 1936, 321, 362, 300, 382, 364, 51350, 51350, 1365, 264, 3705, 392, 420, 1365, 8186, 3705, 264, 1150, 10139, 307, 264, 24922, 502, 382, 364, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.06402742266654968, "compression_ratio": 1.6507936507936507, "no_speech_prob": 5.507374225999229e-06}, {"id": 216, "seek": 104892, "start": 1057.76, "end": 1062.2, "text": " by 3 which is the original shape and now for every one of those 32 by 3 integers", "tokens": [50364, 321, 393, 2935, 445, 360, 383, 412, 1783, 293, 341, 445, 1985, 293, 264, 3909, 295, 341, 307, 8858, 50806, 50806, 538, 805, 597, 307, 264, 3380, 3909, 293, 586, 337, 633, 472, 295, 729, 8858, 538, 805, 41674, 51028, 51028, 321, 600, 19817, 937, 264, 12240, 3584, 8062, 510, 370, 1936, 321, 362, 300, 382, 364, 51350, 51350, 1365, 264, 3705, 392, 420, 1365, 8186, 3705, 264, 1150, 10139, 307, 264, 24922, 502, 382, 364, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.06402742266654968, "compression_ratio": 1.6507936507936507, "no_speech_prob": 5.507374225999229e-06}, {"id": 217, "seek": 104892, "start": 1062.2, "end": 1068.64, "text": " we've retrieved the embedding vector here so basically we have that as an", "tokens": [50364, 321, 393, 2935, 445, 360, 383, 412, 1783, 293, 341, 445, 1985, 293, 264, 3909, 295, 341, 307, 8858, 50806, 50806, 538, 805, 597, 307, 264, 3380, 3909, 293, 586, 337, 633, 472, 295, 729, 8858, 538, 805, 41674, 51028, 51028, 321, 600, 19817, 937, 264, 12240, 3584, 8062, 510, 370, 1936, 321, 362, 300, 382, 364, 51350, 51350, 1365, 264, 3705, 392, 420, 1365, 8186, 3705, 264, 1150, 10139, 307, 264, 24922, 502, 382, 364, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.06402742266654968, "compression_ratio": 1.6507936507936507, "no_speech_prob": 5.507374225999229e-06}, {"id": 218, "seek": 104892, "start": 1068.64, "end": 1077.44, "text": " example the 13th or example index 13 the second dimension is the integer 1 as an", "tokens": [50364, 321, 393, 2935, 445, 360, 383, 412, 1783, 293, 341, 445, 1985, 293, 264, 3909, 295, 341, 307, 8858, 50806, 50806, 538, 805, 597, 307, 264, 3380, 3909, 293, 586, 337, 633, 472, 295, 729, 8858, 538, 805, 41674, 51028, 51028, 321, 600, 19817, 937, 264, 12240, 3584, 8062, 510, 370, 1936, 321, 362, 300, 382, 364, 51350, 51350, 1365, 264, 3705, 392, 420, 1365, 8186, 3705, 264, 1150, 10139, 307, 264, 24922, 502, 382, 364, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.06402742266654968, "compression_ratio": 1.6507936507936507, "no_speech_prob": 5.507374225999229e-06}, {"id": 219, "seek": 107744, "start": 1077.44, "end": 1084.16, "text": " example and so here if we do C of X which gives us that array and then we", "tokens": [50364, 1365, 293, 370, 510, 498, 321, 360, 383, 295, 1783, 597, 2709, 505, 300, 10225, 293, 550, 321, 50700, 50700, 8186, 666, 3705, 538, 568, 295, 300, 10225, 550, 321, 321, 483, 264, 12240, 3584, 510, 293, 291, 393, 51042, 51042, 16888, 300, 383, 412, 502, 597, 307, 264, 24922, 412, 300, 4914, 307, 6451, 2681, 281, 51404, 51404, 341, 291, 536, 436, 434, 2681, 370, 1936, 257, 938, 1657, 2099, 1730, 27822, 8186, 278, 307, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.10826370716094971, "compression_ratio": 1.617801047120419, "no_speech_prob": 3.844793809548719e-06}, {"id": 220, "seek": 107744, "start": 1084.16, "end": 1091.0, "text": " index into 13 by 2 of that array then we we get the embedding here and you can", "tokens": [50364, 1365, 293, 370, 510, 498, 321, 360, 383, 295, 1783, 597, 2709, 505, 300, 10225, 293, 550, 321, 50700, 50700, 8186, 666, 3705, 538, 568, 295, 300, 10225, 550, 321, 321, 483, 264, 12240, 3584, 510, 293, 291, 393, 51042, 51042, 16888, 300, 383, 412, 502, 597, 307, 264, 24922, 412, 300, 4914, 307, 6451, 2681, 281, 51404, 51404, 341, 291, 536, 436, 434, 2681, 370, 1936, 257, 938, 1657, 2099, 1730, 27822, 8186, 278, 307, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.10826370716094971, "compression_ratio": 1.617801047120419, "no_speech_prob": 3.844793809548719e-06}, {"id": 221, "seek": 107744, "start": 1091.0, "end": 1098.24, "text": " verify that C at 1 which is the integer at that location is indeed equal to", "tokens": [50364, 1365, 293, 370, 510, 498, 321, 360, 383, 295, 1783, 597, 2709, 505, 300, 10225, 293, 550, 321, 50700, 50700, 8186, 666, 3705, 538, 568, 295, 300, 10225, 550, 321, 321, 483, 264, 12240, 3584, 510, 293, 291, 393, 51042, 51042, 16888, 300, 383, 412, 502, 597, 307, 264, 24922, 412, 300, 4914, 307, 6451, 2681, 281, 51404, 51404, 341, 291, 536, 436, 434, 2681, 370, 1936, 257, 938, 1657, 2099, 1730, 27822, 8186, 278, 307, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.10826370716094971, "compression_ratio": 1.617801047120419, "no_speech_prob": 3.844793809548719e-06}, {"id": 222, "seek": 107744, "start": 1098.24, "end": 1104.28, "text": " this you see they're equal so basically a long story short pie torch indexing is", "tokens": [50364, 1365, 293, 370, 510, 498, 321, 360, 383, 295, 1783, 597, 2709, 505, 300, 10225, 293, 550, 321, 50700, 50700, 8186, 666, 3705, 538, 568, 295, 300, 10225, 550, 321, 321, 483, 264, 12240, 3584, 510, 293, 291, 393, 51042, 51042, 16888, 300, 383, 412, 502, 597, 307, 264, 24922, 412, 300, 4914, 307, 6451, 2681, 281, 51404, 51404, 341, 291, 536, 436, 434, 2681, 370, 1936, 257, 938, 1657, 2099, 1730, 27822, 8186, 278, 307, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.10826370716094971, "compression_ratio": 1.617801047120419, "no_speech_prob": 3.844793809548719e-06}, {"id": 223, "seek": 110428, "start": 1104.28, "end": 1110.52, "text": " awesome and to embed simultaneously all of the integers in X we can simply do C", "tokens": [50364, 3476, 293, 281, 12240, 16561, 439, 295, 264, 41674, 294, 1783, 321, 393, 2935, 360, 383, 50676, 50676, 295, 1783, 293, 300, 307, 527, 12240, 3584, 293, 300, 445, 1985, 586, 718, 311, 7690, 341, 50972, 50972, 4583, 510, 264, 7633, 4583, 370, 321, 362, 300, 343, 16, 382, 286, 603, 818, 309, 366, 613, 51294, 51294, 17443, 597, 321, 486, 5883, 1125, 16979, 586, 264, 1230, 295, 15743, 281, 341, 4583, 307, 51562, 51562, 516, 281, 312, 1045, 1413, 732, 558, 570, 321, 362, 732, 12, 18759, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.09797643578570822, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.093641902931267e-06}, {"id": 224, "seek": 110428, "start": 1110.52, "end": 1116.44, "text": " of X and that is our embedding and that just works now let's construct this", "tokens": [50364, 3476, 293, 281, 12240, 16561, 439, 295, 264, 41674, 294, 1783, 321, 393, 2935, 360, 383, 50676, 50676, 295, 1783, 293, 300, 307, 527, 12240, 3584, 293, 300, 445, 1985, 586, 718, 311, 7690, 341, 50972, 50972, 4583, 510, 264, 7633, 4583, 370, 321, 362, 300, 343, 16, 382, 286, 603, 818, 309, 366, 613, 51294, 51294, 17443, 597, 321, 486, 5883, 1125, 16979, 586, 264, 1230, 295, 15743, 281, 341, 4583, 307, 51562, 51562, 516, 281, 312, 1045, 1413, 732, 558, 570, 321, 362, 732, 12, 18759, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.09797643578570822, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.093641902931267e-06}, {"id": 225, "seek": 110428, "start": 1116.44, "end": 1122.8799999999999, "text": " layer here the hidden layer so we have that W1 as I'll call it are these", "tokens": [50364, 3476, 293, 281, 12240, 16561, 439, 295, 264, 41674, 294, 1783, 321, 393, 2935, 360, 383, 50676, 50676, 295, 1783, 293, 300, 307, 527, 12240, 3584, 293, 300, 445, 1985, 586, 718, 311, 7690, 341, 50972, 50972, 4583, 510, 264, 7633, 4583, 370, 321, 362, 300, 343, 16, 382, 286, 603, 818, 309, 366, 613, 51294, 51294, 17443, 597, 321, 486, 5883, 1125, 16979, 586, 264, 1230, 295, 15743, 281, 341, 4583, 307, 51562, 51562, 516, 281, 312, 1045, 1413, 732, 558, 570, 321, 362, 732, 12, 18759, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.09797643578570822, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.093641902931267e-06}, {"id": 226, "seek": 110428, "start": 1122.8799999999999, "end": 1128.24, "text": " weights which we will initialize randomly now the number of inputs to this layer is", "tokens": [50364, 3476, 293, 281, 12240, 16561, 439, 295, 264, 41674, 294, 1783, 321, 393, 2935, 360, 383, 50676, 50676, 295, 1783, 293, 300, 307, 527, 12240, 3584, 293, 300, 445, 1985, 586, 718, 311, 7690, 341, 50972, 50972, 4583, 510, 264, 7633, 4583, 370, 321, 362, 300, 343, 16, 382, 286, 603, 818, 309, 366, 613, 51294, 51294, 17443, 597, 321, 486, 5883, 1125, 16979, 586, 264, 1230, 295, 15743, 281, 341, 4583, 307, 51562, 51562, 516, 281, 312, 1045, 1413, 732, 558, 570, 321, 362, 732, 12, 18759, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.09797643578570822, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.093641902931267e-06}, {"id": 227, "seek": 110428, "start": 1128.24, "end": 1132.04, "text": " going to be three times two right because we have two-dimensional", "tokens": [50364, 3476, 293, 281, 12240, 16561, 439, 295, 264, 41674, 294, 1783, 321, 393, 2935, 360, 383, 50676, 50676, 295, 1783, 293, 300, 307, 527, 12240, 3584, 293, 300, 445, 1985, 586, 718, 311, 7690, 341, 50972, 50972, 4583, 510, 264, 7633, 4583, 370, 321, 362, 300, 343, 16, 382, 286, 603, 818, 309, 366, 613, 51294, 51294, 17443, 597, 321, 486, 5883, 1125, 16979, 586, 264, 1230, 295, 15743, 281, 341, 4583, 307, 51562, 51562, 516, 281, 312, 1045, 1413, 732, 558, 570, 321, 362, 732, 12, 18759, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.09797643578570822, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.093641902931267e-06}, {"id": 228, "seek": 113204, "start": 1132.04, "end": 1136.56, "text": " embeddings and we have three of them so the number of inputs is six and the", "tokens": [50364, 12240, 29432, 293, 321, 362, 1045, 295, 552, 370, 264, 1230, 295, 15743, 307, 2309, 293, 264, 50590, 50590, 1230, 295, 22027, 294, 341, 4583, 307, 257, 7006, 493, 281, 505, 718, 311, 764, 257, 3262, 50826, 50826, 22027, 382, 364, 1365, 293, 550, 32152, 486, 312, 611, 5883, 1602, 16979, 382, 364, 51096, 51096, 1365, 293, 718, 311, 293, 321, 445, 643, 2319, 295, 552, 586, 264, 1154, 365, 341, 307, 321, 51428, 51428, 393, 380, 2935, 5646, 321, 576, 747, 264, 4846, 294, 341, 1389, 300, 311, 12240, 3584, 293, 51628, 51628, 321, 1116, 411, 281, 12972, 309, 365, 613, 17443, 293, 550, 321, 576, 411, 281, 909, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.06767216063382332, "compression_ratio": 1.9049586776859504, "no_speech_prob": 4.7107728278206196e-06}, {"id": 229, "seek": 113204, "start": 1136.56, "end": 1141.28, "text": " number of neurons in this layer is a variable up to us let's use a hundred", "tokens": [50364, 12240, 29432, 293, 321, 362, 1045, 295, 552, 370, 264, 1230, 295, 15743, 307, 2309, 293, 264, 50590, 50590, 1230, 295, 22027, 294, 341, 4583, 307, 257, 7006, 493, 281, 505, 718, 311, 764, 257, 3262, 50826, 50826, 22027, 382, 364, 1365, 293, 550, 32152, 486, 312, 611, 5883, 1602, 16979, 382, 364, 51096, 51096, 1365, 293, 718, 311, 293, 321, 445, 643, 2319, 295, 552, 586, 264, 1154, 365, 341, 307, 321, 51428, 51428, 393, 380, 2935, 5646, 321, 576, 747, 264, 4846, 294, 341, 1389, 300, 311, 12240, 3584, 293, 51628, 51628, 321, 1116, 411, 281, 12972, 309, 365, 613, 17443, 293, 550, 321, 576, 411, 281, 909, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.06767216063382332, "compression_ratio": 1.9049586776859504, "no_speech_prob": 4.7107728278206196e-06}, {"id": 230, "seek": 113204, "start": 1141.28, "end": 1146.68, "text": " neurons as an example and then biases will be also initialized randomly as an", "tokens": [50364, 12240, 29432, 293, 321, 362, 1045, 295, 552, 370, 264, 1230, 295, 15743, 307, 2309, 293, 264, 50590, 50590, 1230, 295, 22027, 294, 341, 4583, 307, 257, 7006, 493, 281, 505, 718, 311, 764, 257, 3262, 50826, 50826, 22027, 382, 364, 1365, 293, 550, 32152, 486, 312, 611, 5883, 1602, 16979, 382, 364, 51096, 51096, 1365, 293, 718, 311, 293, 321, 445, 643, 2319, 295, 552, 586, 264, 1154, 365, 341, 307, 321, 51428, 51428, 393, 380, 2935, 5646, 321, 576, 747, 264, 4846, 294, 341, 1389, 300, 311, 12240, 3584, 293, 51628, 51628, 321, 1116, 411, 281, 12972, 309, 365, 613, 17443, 293, 550, 321, 576, 411, 281, 909, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.06767216063382332, "compression_ratio": 1.9049586776859504, "no_speech_prob": 4.7107728278206196e-06}, {"id": 231, "seek": 113204, "start": 1146.68, "end": 1153.32, "text": " example and let's and we just need 100 of them now the problem with this is we", "tokens": [50364, 12240, 29432, 293, 321, 362, 1045, 295, 552, 370, 264, 1230, 295, 15743, 307, 2309, 293, 264, 50590, 50590, 1230, 295, 22027, 294, 341, 4583, 307, 257, 7006, 493, 281, 505, 718, 311, 764, 257, 3262, 50826, 50826, 22027, 382, 364, 1365, 293, 550, 32152, 486, 312, 611, 5883, 1602, 16979, 382, 364, 51096, 51096, 1365, 293, 718, 311, 293, 321, 445, 643, 2319, 295, 552, 586, 264, 1154, 365, 341, 307, 321, 51428, 51428, 393, 380, 2935, 5646, 321, 576, 747, 264, 4846, 294, 341, 1389, 300, 311, 12240, 3584, 293, 51628, 51628, 321, 1116, 411, 281, 12972, 309, 365, 613, 17443, 293, 550, 321, 576, 411, 281, 909, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.06767216063382332, "compression_ratio": 1.9049586776859504, "no_speech_prob": 4.7107728278206196e-06}, {"id": 232, "seek": 113204, "start": 1153.32, "end": 1157.32, "text": " can't simply normally we would take the input in this case that's embedding and", "tokens": [50364, 12240, 29432, 293, 321, 362, 1045, 295, 552, 370, 264, 1230, 295, 15743, 307, 2309, 293, 264, 50590, 50590, 1230, 295, 22027, 294, 341, 4583, 307, 257, 7006, 493, 281, 505, 718, 311, 764, 257, 3262, 50826, 50826, 22027, 382, 364, 1365, 293, 550, 32152, 486, 312, 611, 5883, 1602, 16979, 382, 364, 51096, 51096, 1365, 293, 718, 311, 293, 321, 445, 643, 2319, 295, 552, 586, 264, 1154, 365, 341, 307, 321, 51428, 51428, 393, 380, 2935, 5646, 321, 576, 747, 264, 4846, 294, 341, 1389, 300, 311, 12240, 3584, 293, 51628, 51628, 321, 1116, 411, 281, 12972, 309, 365, 613, 17443, 293, 550, 321, 576, 411, 281, 909, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.06767216063382332, "compression_ratio": 1.9049586776859504, "no_speech_prob": 4.7107728278206196e-06}, {"id": 233, "seek": 113204, "start": 1157.32, "end": 1161.6, "text": " we'd like to multiply it with these weights and then we would like to add", "tokens": [50364, 12240, 29432, 293, 321, 362, 1045, 295, 552, 370, 264, 1230, 295, 15743, 307, 2309, 293, 264, 50590, 50590, 1230, 295, 22027, 294, 341, 4583, 307, 257, 7006, 493, 281, 505, 718, 311, 764, 257, 3262, 50826, 50826, 22027, 382, 364, 1365, 293, 550, 32152, 486, 312, 611, 5883, 1602, 16979, 382, 364, 51096, 51096, 1365, 293, 718, 311, 293, 321, 445, 643, 2319, 295, 552, 586, 264, 1154, 365, 341, 307, 321, 51428, 51428, 393, 380, 2935, 5646, 321, 576, 747, 264, 4846, 294, 341, 1389, 300, 311, 12240, 3584, 293, 51628, 51628, 321, 1116, 411, 281, 12972, 309, 365, 613, 17443, 293, 550, 321, 576, 411, 281, 909, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.06767216063382332, "compression_ratio": 1.9049586776859504, "no_speech_prob": 4.7107728278206196e-06}, {"id": 234, "seek": 116160, "start": 1161.6, "end": 1166.0, "text": " the bias this is roughly what we want to do but the problem here is that these", "tokens": [50364, 264, 12577, 341, 307, 9810, 437, 321, 528, 281, 360, 457, 264, 1154, 510, 307, 300, 613, 50584, 50584, 12240, 29432, 366, 28867, 493, 294, 264, 12819, 295, 341, 4846, 40863, 370, 341, 50808, 50808, 486, 406, 589, 341, 8141, 27290, 570, 341, 307, 257, 3909, 8858, 538, 805, 538, 568, 293, 51006, 51006, 286, 393, 380, 12972, 300, 538, 1386, 538, 2319, 370, 6063, 321, 643, 281, 1588, 7186, 473, 613, 51262, 51262, 15743, 510, 1214, 370, 300, 321, 393, 360, 746, 2051, 613, 3876, 597, 51450, 51450, 4362, 775, 406, 589, 370, 577, 360, 321, 4088, 341, 8858, 538, 805, 538, 568, 666, 257, 8858, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.06104062484191344, "compression_ratio": 1.7366412213740459, "no_speech_prob": 2.5465518774581142e-05}, {"id": 235, "seek": 116160, "start": 1166.0, "end": 1170.48, "text": " embeddings are stacked up in the dimensions of this input tensor so this", "tokens": [50364, 264, 12577, 341, 307, 9810, 437, 321, 528, 281, 360, 457, 264, 1154, 510, 307, 300, 613, 50584, 50584, 12240, 29432, 366, 28867, 493, 294, 264, 12819, 295, 341, 4846, 40863, 370, 341, 50808, 50808, 486, 406, 589, 341, 8141, 27290, 570, 341, 307, 257, 3909, 8858, 538, 805, 538, 568, 293, 51006, 51006, 286, 393, 380, 12972, 300, 538, 1386, 538, 2319, 370, 6063, 321, 643, 281, 1588, 7186, 473, 613, 51262, 51262, 15743, 510, 1214, 370, 300, 321, 393, 360, 746, 2051, 613, 3876, 597, 51450, 51450, 4362, 775, 406, 589, 370, 577, 360, 321, 4088, 341, 8858, 538, 805, 538, 568, 666, 257, 8858, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.06104062484191344, "compression_ratio": 1.7366412213740459, "no_speech_prob": 2.5465518774581142e-05}, {"id": 236, "seek": 116160, "start": 1170.48, "end": 1174.4399999999998, "text": " will not work this matrix multiplication because this is a shape 32 by 3 by 2 and", "tokens": [50364, 264, 12577, 341, 307, 9810, 437, 321, 528, 281, 360, 457, 264, 1154, 510, 307, 300, 613, 50584, 50584, 12240, 29432, 366, 28867, 493, 294, 264, 12819, 295, 341, 4846, 40863, 370, 341, 50808, 50808, 486, 406, 589, 341, 8141, 27290, 570, 341, 307, 257, 3909, 8858, 538, 805, 538, 568, 293, 51006, 51006, 286, 393, 380, 12972, 300, 538, 1386, 538, 2319, 370, 6063, 321, 643, 281, 1588, 7186, 473, 613, 51262, 51262, 15743, 510, 1214, 370, 300, 321, 393, 360, 746, 2051, 613, 3876, 597, 51450, 51450, 4362, 775, 406, 589, 370, 577, 360, 321, 4088, 341, 8858, 538, 805, 538, 568, 666, 257, 8858, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.06104062484191344, "compression_ratio": 1.7366412213740459, "no_speech_prob": 2.5465518774581142e-05}, {"id": 237, "seek": 116160, "start": 1174.4399999999998, "end": 1179.56, "text": " I can't multiply that by 6 by 100 so somehow we need to concatenate these", "tokens": [50364, 264, 12577, 341, 307, 9810, 437, 321, 528, 281, 360, 457, 264, 1154, 510, 307, 300, 613, 50584, 50584, 12240, 29432, 366, 28867, 493, 294, 264, 12819, 295, 341, 4846, 40863, 370, 341, 50808, 50808, 486, 406, 589, 341, 8141, 27290, 570, 341, 307, 257, 3909, 8858, 538, 805, 538, 568, 293, 51006, 51006, 286, 393, 380, 12972, 300, 538, 1386, 538, 2319, 370, 6063, 321, 643, 281, 1588, 7186, 473, 613, 51262, 51262, 15743, 510, 1214, 370, 300, 321, 393, 360, 746, 2051, 613, 3876, 597, 51450, 51450, 4362, 775, 406, 589, 370, 577, 360, 321, 4088, 341, 8858, 538, 805, 538, 568, 666, 257, 8858, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.06104062484191344, "compression_ratio": 1.7366412213740459, "no_speech_prob": 2.5465518774581142e-05}, {"id": 238, "seek": 116160, "start": 1179.56, "end": 1183.32, "text": " inputs here together so that we can do something along these lines which", "tokens": [50364, 264, 12577, 341, 307, 9810, 437, 321, 528, 281, 360, 457, 264, 1154, 510, 307, 300, 613, 50584, 50584, 12240, 29432, 366, 28867, 493, 294, 264, 12819, 295, 341, 4846, 40863, 370, 341, 50808, 50808, 486, 406, 589, 341, 8141, 27290, 570, 341, 307, 257, 3909, 8858, 538, 805, 538, 568, 293, 51006, 51006, 286, 393, 380, 12972, 300, 538, 1386, 538, 2319, 370, 6063, 321, 643, 281, 1588, 7186, 473, 613, 51262, 51262, 15743, 510, 1214, 370, 300, 321, 393, 360, 746, 2051, 613, 3876, 597, 51450, 51450, 4362, 775, 406, 589, 370, 577, 360, 321, 4088, 341, 8858, 538, 805, 538, 568, 666, 257, 8858, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.06104062484191344, "compression_ratio": 1.7366412213740459, "no_speech_prob": 2.5465518774581142e-05}, {"id": 239, "seek": 116160, "start": 1183.32, "end": 1188.9199999999998, "text": " currently does not work so how do we transform this 32 by 3 by 2 into a 32", "tokens": [50364, 264, 12577, 341, 307, 9810, 437, 321, 528, 281, 360, 457, 264, 1154, 510, 307, 300, 613, 50584, 50584, 12240, 29432, 366, 28867, 493, 294, 264, 12819, 295, 341, 4846, 40863, 370, 341, 50808, 50808, 486, 406, 589, 341, 8141, 27290, 570, 341, 307, 257, 3909, 8858, 538, 805, 538, 568, 293, 51006, 51006, 286, 393, 380, 12972, 300, 538, 1386, 538, 2319, 370, 6063, 321, 643, 281, 1588, 7186, 473, 613, 51262, 51262, 15743, 510, 1214, 370, 300, 321, 393, 360, 746, 2051, 613, 3876, 597, 51450, 51450, 4362, 775, 406, 589, 370, 577, 360, 321, 4088, 341, 8858, 538, 805, 538, 568, 666, 257, 8858, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.06104062484191344, "compression_ratio": 1.7366412213740459, "no_speech_prob": 2.5465518774581142e-05}, {"id": 240, "seek": 118892, "start": 1188.92, "end": 1194.64, "text": " by 6 so that we can actually perform this multiplication over here I'd like", "tokens": [50364, 538, 1386, 370, 300, 321, 393, 767, 2042, 341, 27290, 670, 510, 286, 1116, 411, 50650, 50650, 281, 855, 291, 300, 456, 366, 2673, 867, 2098, 295, 18114, 437, 291, 1116, 411, 281, 50886, 50886, 360, 294, 27822, 293, 512, 295, 552, 486, 312, 4663, 1101, 11639, 5183, 293, 300, 311, 51132, 51132, 570, 27822, 307, 257, 588, 2416, 6405, 293, 309, 311, 658, 3195, 293, 3195, 295, 6828, 51340, 51340, 370, 498, 321, 445, 352, 281, 264, 14333, 293, 2052, 322, 27822, 291, 603, 536, 300, 452, 51540, 51540, 26046, 510, 307, 588, 5870, 293, 300, 311, 570, 456, 366, 370, 867, 6828, 300, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.07454710269193036, "compression_ratio": 1.811023622047244, "no_speech_prob": 5.422093181550736e-06}, {"id": 241, "seek": 118892, "start": 1194.64, "end": 1199.3600000000001, "text": " to show you that there are usually many ways of implementing what you'd like to", "tokens": [50364, 538, 1386, 370, 300, 321, 393, 767, 2042, 341, 27290, 670, 510, 286, 1116, 411, 50650, 50650, 281, 855, 291, 300, 456, 366, 2673, 867, 2098, 295, 18114, 437, 291, 1116, 411, 281, 50886, 50886, 360, 294, 27822, 293, 512, 295, 552, 486, 312, 4663, 1101, 11639, 5183, 293, 300, 311, 51132, 51132, 570, 27822, 307, 257, 588, 2416, 6405, 293, 309, 311, 658, 3195, 293, 3195, 295, 6828, 51340, 51340, 370, 498, 321, 445, 352, 281, 264, 14333, 293, 2052, 322, 27822, 291, 603, 536, 300, 452, 51540, 51540, 26046, 510, 307, 588, 5870, 293, 300, 311, 570, 456, 366, 370, 867, 6828, 300, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.07454710269193036, "compression_ratio": 1.811023622047244, "no_speech_prob": 5.422093181550736e-06}, {"id": 242, "seek": 118892, "start": 1199.3600000000001, "end": 1204.28, "text": " do in torch and some of them will be faster better shorter etc and that's", "tokens": [50364, 538, 1386, 370, 300, 321, 393, 767, 2042, 341, 27290, 670, 510, 286, 1116, 411, 50650, 50650, 281, 855, 291, 300, 456, 366, 2673, 867, 2098, 295, 18114, 437, 291, 1116, 411, 281, 50886, 50886, 360, 294, 27822, 293, 512, 295, 552, 486, 312, 4663, 1101, 11639, 5183, 293, 300, 311, 51132, 51132, 570, 27822, 307, 257, 588, 2416, 6405, 293, 309, 311, 658, 3195, 293, 3195, 295, 6828, 51340, 51340, 370, 498, 321, 445, 352, 281, 264, 14333, 293, 2052, 322, 27822, 291, 603, 536, 300, 452, 51540, 51540, 26046, 510, 307, 588, 5870, 293, 300, 311, 570, 456, 366, 370, 867, 6828, 300, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.07454710269193036, "compression_ratio": 1.811023622047244, "no_speech_prob": 5.422093181550736e-06}, {"id": 243, "seek": 118892, "start": 1204.28, "end": 1208.44, "text": " because torch is a very large library and it's got lots and lots of functions", "tokens": [50364, 538, 1386, 370, 300, 321, 393, 767, 2042, 341, 27290, 670, 510, 286, 1116, 411, 50650, 50650, 281, 855, 291, 300, 456, 366, 2673, 867, 2098, 295, 18114, 437, 291, 1116, 411, 281, 50886, 50886, 360, 294, 27822, 293, 512, 295, 552, 486, 312, 4663, 1101, 11639, 5183, 293, 300, 311, 51132, 51132, 570, 27822, 307, 257, 588, 2416, 6405, 293, 309, 311, 658, 3195, 293, 3195, 295, 6828, 51340, 51340, 370, 498, 321, 445, 352, 281, 264, 14333, 293, 2052, 322, 27822, 291, 603, 536, 300, 452, 51540, 51540, 26046, 510, 307, 588, 5870, 293, 300, 311, 570, 456, 366, 370, 867, 6828, 300, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.07454710269193036, "compression_ratio": 1.811023622047244, "no_speech_prob": 5.422093181550736e-06}, {"id": 244, "seek": 118892, "start": 1208.44, "end": 1212.44, "text": " so if we just go to the documentation and click on torch you'll see that my", "tokens": [50364, 538, 1386, 370, 300, 321, 393, 767, 2042, 341, 27290, 670, 510, 286, 1116, 411, 50650, 50650, 281, 855, 291, 300, 456, 366, 2673, 867, 2098, 295, 18114, 437, 291, 1116, 411, 281, 50886, 50886, 360, 294, 27822, 293, 512, 295, 552, 486, 312, 4663, 1101, 11639, 5183, 293, 300, 311, 51132, 51132, 570, 27822, 307, 257, 588, 2416, 6405, 293, 309, 311, 658, 3195, 293, 3195, 295, 6828, 51340, 51340, 370, 498, 321, 445, 352, 281, 264, 14333, 293, 2052, 322, 27822, 291, 603, 536, 300, 452, 51540, 51540, 26046, 510, 307, 588, 5870, 293, 300, 311, 570, 456, 366, 370, 867, 6828, 300, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.07454710269193036, "compression_ratio": 1.811023622047244, "no_speech_prob": 5.422093181550736e-06}, {"id": 245, "seek": 118892, "start": 1212.44, "end": 1216.04, "text": " slider here is very tiny and that's because there are so many functions that", "tokens": [50364, 538, 1386, 370, 300, 321, 393, 767, 2042, 341, 27290, 670, 510, 286, 1116, 411, 50650, 50650, 281, 855, 291, 300, 456, 366, 2673, 867, 2098, 295, 18114, 437, 291, 1116, 411, 281, 50886, 50886, 360, 294, 27822, 293, 512, 295, 552, 486, 312, 4663, 1101, 11639, 5183, 293, 300, 311, 51132, 51132, 570, 27822, 307, 257, 588, 2416, 6405, 293, 309, 311, 658, 3195, 293, 3195, 295, 6828, 51340, 51340, 370, 498, 321, 445, 352, 281, 264, 14333, 293, 2052, 322, 27822, 291, 603, 536, 300, 452, 51540, 51540, 26046, 510, 307, 588, 5870, 293, 300, 311, 570, 456, 366, 370, 867, 6828, 300, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.07454710269193036, "compression_ratio": 1.811023622047244, "no_speech_prob": 5.422093181550736e-06}, {"id": 246, "seek": 121604, "start": 1216.04, "end": 1220.2, "text": " you can call on these tensors to transform them create them multiply", "tokens": [50364, 291, 393, 818, 322, 613, 10688, 830, 281, 4088, 552, 1884, 552, 12972, 50572, 50572, 552, 909, 552, 2042, 439, 3685, 295, 819, 7705, 322, 552, 293, 370, 341, 50864, 50864, 307, 733, 295, 411, 264, 1901, 295, 7959, 498, 291, 486, 586, 472, 295, 264, 721, 300, 51168, 51168, 291, 393, 360, 307, 321, 393, 1969, 510, 1969, 479, 337, 1588, 7186, 473, 293, 321, 536, 51378, 51378, 300, 456, 311, 257, 2445, 27822, 13, 18035, 2099, 337, 1588, 7186, 473, 293, 341, 1588, 7186, 473, 307, 51642, 51642], "temperature": 0.0, "avg_logprob": -0.10621968559596849, "compression_ratio": 1.8028846153846154, "no_speech_prob": 2.123324748026789e-06}, {"id": 247, "seek": 121604, "start": 1220.2, "end": 1226.04, "text": " them add them perform all kinds of different operations on them and so this", "tokens": [50364, 291, 393, 818, 322, 613, 10688, 830, 281, 4088, 552, 1884, 552, 12972, 50572, 50572, 552, 909, 552, 2042, 439, 3685, 295, 819, 7705, 322, 552, 293, 370, 341, 50864, 50864, 307, 733, 295, 411, 264, 1901, 295, 7959, 498, 291, 486, 586, 472, 295, 264, 721, 300, 51168, 51168, 291, 393, 360, 307, 321, 393, 1969, 510, 1969, 479, 337, 1588, 7186, 473, 293, 321, 536, 51378, 51378, 300, 456, 311, 257, 2445, 27822, 13, 18035, 2099, 337, 1588, 7186, 473, 293, 341, 1588, 7186, 473, 307, 51642, 51642], "temperature": 0.0, "avg_logprob": -0.10621968559596849, "compression_ratio": 1.8028846153846154, "no_speech_prob": 2.123324748026789e-06}, {"id": 248, "seek": 121604, "start": 1226.04, "end": 1232.12, "text": " is kind of like the space of possibility if you will now one of the things that", "tokens": [50364, 291, 393, 818, 322, 613, 10688, 830, 281, 4088, 552, 1884, 552, 12972, 50572, 50572, 552, 909, 552, 2042, 439, 3685, 295, 819, 7705, 322, 552, 293, 370, 341, 50864, 50864, 307, 733, 295, 411, 264, 1901, 295, 7959, 498, 291, 486, 586, 472, 295, 264, 721, 300, 51168, 51168, 291, 393, 360, 307, 321, 393, 1969, 510, 1969, 479, 337, 1588, 7186, 473, 293, 321, 536, 51378, 51378, 300, 456, 311, 257, 2445, 27822, 13, 18035, 2099, 337, 1588, 7186, 473, 293, 341, 1588, 7186, 473, 307, 51642, 51642], "temperature": 0.0, "avg_logprob": -0.10621968559596849, "compression_ratio": 1.8028846153846154, "no_speech_prob": 2.123324748026789e-06}, {"id": 249, "seek": 121604, "start": 1232.12, "end": 1236.32, "text": " you can do is we can control here control F for concatenate and we see", "tokens": [50364, 291, 393, 818, 322, 613, 10688, 830, 281, 4088, 552, 1884, 552, 12972, 50572, 50572, 552, 909, 552, 2042, 439, 3685, 295, 819, 7705, 322, 552, 293, 370, 341, 50864, 50864, 307, 733, 295, 411, 264, 1901, 295, 7959, 498, 291, 486, 586, 472, 295, 264, 721, 300, 51168, 51168, 291, 393, 360, 307, 321, 393, 1969, 510, 1969, 479, 337, 1588, 7186, 473, 293, 321, 536, 51378, 51378, 300, 456, 311, 257, 2445, 27822, 13, 18035, 2099, 337, 1588, 7186, 473, 293, 341, 1588, 7186, 473, 307, 51642, 51642], "temperature": 0.0, "avg_logprob": -0.10621968559596849, "compression_ratio": 1.8028846153846154, "no_speech_prob": 2.123324748026789e-06}, {"id": 250, "seek": 121604, "start": 1236.32, "end": 1241.6, "text": " that there's a function torch.cat short for concatenate and this concatenate is", "tokens": [50364, 291, 393, 818, 322, 613, 10688, 830, 281, 4088, 552, 1884, 552, 12972, 50572, 50572, 552, 909, 552, 2042, 439, 3685, 295, 819, 7705, 322, 552, 293, 370, 341, 50864, 50864, 307, 733, 295, 411, 264, 1901, 295, 7959, 498, 291, 486, 586, 472, 295, 264, 721, 300, 51168, 51168, 291, 393, 360, 307, 321, 393, 1969, 510, 1969, 479, 337, 1588, 7186, 473, 293, 321, 536, 51378, 51378, 300, 456, 311, 257, 2445, 27822, 13, 18035, 2099, 337, 1588, 7186, 473, 293, 341, 1588, 7186, 473, 307, 51642, 51642], "temperature": 0.0, "avg_logprob": -0.10621968559596849, "compression_ratio": 1.8028846153846154, "no_speech_prob": 2.123324748026789e-06}, {"id": 251, "seek": 124160, "start": 1241.6, "end": 1246.6, "text": " a given sequence of tensors in a given dimension and these sensors must have", "tokens": [50364, 257, 2212, 8310, 295, 10688, 830, 294, 257, 2212, 10139, 293, 613, 14840, 1633, 362, 50614, 50614, 264, 912, 3909, 5183, 370, 321, 393, 764, 264, 1588, 7186, 473, 6916, 281, 294, 257, 29052, 636, 50848, 50848, 1588, 7186, 473, 613, 1045, 12240, 29432, 337, 1184, 4846, 370, 294, 341, 1389, 321, 362, 376, 295, 51188, 51188, 376, 295, 264, 3909, 293, 534, 437, 321, 528, 281, 360, 307, 321, 528, 281, 30254, 613, 1045, 51444, 51444, 3166, 293, 1588, 7186, 473, 552, 370, 321, 528, 281, 4444, 439, 264, 5110, 321, 528, 281, 4444, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11925639528216737, "compression_ratio": 1.9261083743842364, "no_speech_prob": 4.784734301210847e-06}, {"id": 252, "seek": 124160, "start": 1246.6, "end": 1251.28, "text": " the same shape etc so we can use the concatenate operation to in a naive way", "tokens": [50364, 257, 2212, 8310, 295, 10688, 830, 294, 257, 2212, 10139, 293, 613, 14840, 1633, 362, 50614, 50614, 264, 912, 3909, 5183, 370, 321, 393, 764, 264, 1588, 7186, 473, 6916, 281, 294, 257, 29052, 636, 50848, 50848, 1588, 7186, 473, 613, 1045, 12240, 29432, 337, 1184, 4846, 370, 294, 341, 1389, 321, 362, 376, 295, 51188, 51188, 376, 295, 264, 3909, 293, 534, 437, 321, 528, 281, 360, 307, 321, 528, 281, 30254, 613, 1045, 51444, 51444, 3166, 293, 1588, 7186, 473, 552, 370, 321, 528, 281, 4444, 439, 264, 5110, 321, 528, 281, 4444, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11925639528216737, "compression_ratio": 1.9261083743842364, "no_speech_prob": 4.784734301210847e-06}, {"id": 253, "seek": 124160, "start": 1251.28, "end": 1258.08, "text": " concatenate these three embeddings for each input so in this case we have M of", "tokens": [50364, 257, 2212, 8310, 295, 10688, 830, 294, 257, 2212, 10139, 293, 613, 14840, 1633, 362, 50614, 50614, 264, 912, 3909, 5183, 370, 321, 393, 764, 264, 1588, 7186, 473, 6916, 281, 294, 257, 29052, 636, 50848, 50848, 1588, 7186, 473, 613, 1045, 12240, 29432, 337, 1184, 4846, 370, 294, 341, 1389, 321, 362, 376, 295, 51188, 51188, 376, 295, 264, 3909, 293, 534, 437, 321, 528, 281, 360, 307, 321, 528, 281, 30254, 613, 1045, 51444, 51444, 3166, 293, 1588, 7186, 473, 552, 370, 321, 528, 281, 4444, 439, 264, 5110, 321, 528, 281, 4444, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11925639528216737, "compression_ratio": 1.9261083743842364, "no_speech_prob": 4.784734301210847e-06}, {"id": 254, "seek": 124160, "start": 1258.08, "end": 1263.1999999999998, "text": " M of the shape and really what we want to do is we want to retrieve these three", "tokens": [50364, 257, 2212, 8310, 295, 10688, 830, 294, 257, 2212, 10139, 293, 613, 14840, 1633, 362, 50614, 50614, 264, 912, 3909, 5183, 370, 321, 393, 764, 264, 1588, 7186, 473, 6916, 281, 294, 257, 29052, 636, 50848, 50848, 1588, 7186, 473, 613, 1045, 12240, 29432, 337, 1184, 4846, 370, 294, 341, 1389, 321, 362, 376, 295, 51188, 51188, 376, 295, 264, 3909, 293, 534, 437, 321, 528, 281, 360, 307, 321, 528, 281, 30254, 613, 1045, 51444, 51444, 3166, 293, 1588, 7186, 473, 552, 370, 321, 528, 281, 4444, 439, 264, 5110, 321, 528, 281, 4444, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11925639528216737, "compression_ratio": 1.9261083743842364, "no_speech_prob": 4.784734301210847e-06}, {"id": 255, "seek": 124160, "start": 1263.1999999999998, "end": 1269.6799999999998, "text": " parts and concatenate them so we want to grab all the examples we want to grab", "tokens": [50364, 257, 2212, 8310, 295, 10688, 830, 294, 257, 2212, 10139, 293, 613, 14840, 1633, 362, 50614, 50614, 264, 912, 3909, 5183, 370, 321, 393, 764, 264, 1588, 7186, 473, 6916, 281, 294, 257, 29052, 636, 50848, 50848, 1588, 7186, 473, 613, 1045, 12240, 29432, 337, 1184, 4846, 370, 294, 341, 1389, 321, 362, 376, 295, 51188, 51188, 376, 295, 264, 3909, 293, 534, 437, 321, 528, 281, 360, 307, 321, 528, 281, 30254, 613, 1045, 51444, 51444, 3166, 293, 1588, 7186, 473, 552, 370, 321, 528, 281, 4444, 439, 264, 5110, 321, 528, 281, 4444, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11925639528216737, "compression_ratio": 1.9261083743842364, "no_speech_prob": 4.784734301210847e-06}, {"id": 256, "seek": 126968, "start": 1269.68, "end": 1281.8, "text": " first the zero index and then all of this so this plucks out the 32 by 2", "tokens": [50364, 700, 264, 4018, 8186, 293, 550, 439, 295, 341, 370, 341, 499, 15493, 484, 264, 8858, 538, 568, 50970, 50970, 12240, 29432, 295, 445, 264, 700, 1349, 510, 293, 370, 1936, 321, 528, 341, 2146, 321, 51324, 51324, 528, 264, 700, 10139, 293, 321, 528, 264, 1150, 10139, 293, 613, 366, 264, 1045, 51574, 51574, 3755, 16652, 293, 550, 321, 528, 281, 2387, 341, 382, 257, 8310, 293, 321, 528, 281, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.08983234405517578, "compression_ratio": 1.8562874251497006, "no_speech_prob": 1.9524477465893142e-05}, {"id": 257, "seek": 126968, "start": 1281.8, "end": 1288.88, "text": " embeddings of just the first word here and so basically we want this guy we", "tokens": [50364, 700, 264, 4018, 8186, 293, 550, 439, 295, 341, 370, 341, 499, 15493, 484, 264, 8858, 538, 568, 50970, 50970, 12240, 29432, 295, 445, 264, 700, 1349, 510, 293, 370, 1936, 321, 528, 341, 2146, 321, 51324, 51324, 528, 264, 700, 10139, 293, 321, 528, 264, 1150, 10139, 293, 613, 366, 264, 1045, 51574, 51574, 3755, 16652, 293, 550, 321, 528, 281, 2387, 341, 382, 257, 8310, 293, 321, 528, 281, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.08983234405517578, "compression_ratio": 1.8562874251497006, "no_speech_prob": 1.9524477465893142e-05}, {"id": 258, "seek": 126968, "start": 1288.88, "end": 1293.88, "text": " want the first dimension and we want the second dimension and these are the three", "tokens": [50364, 700, 264, 4018, 8186, 293, 550, 439, 295, 341, 370, 341, 499, 15493, 484, 264, 8858, 538, 568, 50970, 50970, 12240, 29432, 295, 445, 264, 700, 1349, 510, 293, 370, 1936, 321, 528, 341, 2146, 321, 51324, 51324, 528, 264, 700, 10139, 293, 321, 528, 264, 1150, 10139, 293, 613, 366, 264, 1045, 51574, 51574, 3755, 16652, 293, 550, 321, 528, 281, 2387, 341, 382, 257, 8310, 293, 321, 528, 281, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.08983234405517578, "compression_ratio": 1.8562874251497006, "no_speech_prob": 1.9524477465893142e-05}, {"id": 259, "seek": 126968, "start": 1293.88, "end": 1299.2, "text": " pieces individually and then we want to treat this as a sequence and we want to", "tokens": [50364, 700, 264, 4018, 8186, 293, 550, 439, 295, 341, 370, 341, 499, 15493, 484, 264, 8858, 538, 568, 50970, 50970, 12240, 29432, 295, 445, 264, 700, 1349, 510, 293, 370, 1936, 321, 528, 341, 2146, 321, 51324, 51324, 528, 264, 700, 10139, 293, 321, 528, 264, 1150, 10139, 293, 613, 366, 264, 1045, 51574, 51574, 3755, 16652, 293, 550, 321, 528, 281, 2387, 341, 382, 257, 8310, 293, 321, 528, 281, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.08983234405517578, "compression_ratio": 1.8562874251497006, "no_speech_prob": 1.9524477465893142e-05}, {"id": 260, "seek": 129920, "start": 1299.2, "end": 1305.1200000000001, "text": " torch.cat on that sequence so this is the list torch.cat takes a", "tokens": [50364, 27822, 13, 18035, 322, 300, 8310, 370, 341, 307, 264, 1329, 27822, 13, 18035, 2516, 257, 50660, 50660, 8310, 295, 10688, 830, 293, 550, 321, 362, 281, 980, 309, 2051, 597, 10139, 281, 50892, 50892, 1588, 7186, 473, 370, 294, 341, 1389, 439, 613, 366, 8858, 538, 568, 293, 321, 528, 281, 1588, 7186, 473, 51136, 51136, 406, 2108, 10139, 1958, 457, 2108, 10139, 502, 370, 8437, 294, 502, 2709, 505, 257, 51436, 51436, 1874, 264, 3909, 295, 341, 307, 8858, 538, 1386, 2293, 382, 321, 1116, 411, 370, 300, 1936, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13367246326647306, "compression_ratio": 1.7403846153846154, "no_speech_prob": 1.028924361889949e-05}, {"id": 261, "seek": 129920, "start": 1305.1200000000001, "end": 1309.76, "text": " sequence of tensors and then we have to tell it along which dimension to", "tokens": [50364, 27822, 13, 18035, 322, 300, 8310, 370, 341, 307, 264, 1329, 27822, 13, 18035, 2516, 257, 50660, 50660, 8310, 295, 10688, 830, 293, 550, 321, 362, 281, 980, 309, 2051, 597, 10139, 281, 50892, 50892, 1588, 7186, 473, 370, 294, 341, 1389, 439, 613, 366, 8858, 538, 568, 293, 321, 528, 281, 1588, 7186, 473, 51136, 51136, 406, 2108, 10139, 1958, 457, 2108, 10139, 502, 370, 8437, 294, 502, 2709, 505, 257, 51436, 51436, 1874, 264, 3909, 295, 341, 307, 8858, 538, 1386, 2293, 382, 321, 1116, 411, 370, 300, 1936, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13367246326647306, "compression_ratio": 1.7403846153846154, "no_speech_prob": 1.028924361889949e-05}, {"id": 262, "seek": 129920, "start": 1309.76, "end": 1314.64, "text": " concatenate so in this case all these are 32 by 2 and we want to concatenate", "tokens": [50364, 27822, 13, 18035, 322, 300, 8310, 370, 341, 307, 264, 1329, 27822, 13, 18035, 2516, 257, 50660, 50660, 8310, 295, 10688, 830, 293, 550, 321, 362, 281, 980, 309, 2051, 597, 10139, 281, 50892, 50892, 1588, 7186, 473, 370, 294, 341, 1389, 439, 613, 366, 8858, 538, 568, 293, 321, 528, 281, 1588, 7186, 473, 51136, 51136, 406, 2108, 10139, 1958, 457, 2108, 10139, 502, 370, 8437, 294, 502, 2709, 505, 257, 51436, 51436, 1874, 264, 3909, 295, 341, 307, 8858, 538, 1386, 2293, 382, 321, 1116, 411, 370, 300, 1936, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13367246326647306, "compression_ratio": 1.7403846153846154, "no_speech_prob": 1.028924361889949e-05}, {"id": 263, "seek": 129920, "start": 1314.64, "end": 1320.64, "text": " not across dimension 0 but across dimension 1 so passing in 1 gives us a", "tokens": [50364, 27822, 13, 18035, 322, 300, 8310, 370, 341, 307, 264, 1329, 27822, 13, 18035, 2516, 257, 50660, 50660, 8310, 295, 10688, 830, 293, 550, 321, 362, 281, 980, 309, 2051, 597, 10139, 281, 50892, 50892, 1588, 7186, 473, 370, 294, 341, 1389, 439, 613, 366, 8858, 538, 568, 293, 321, 528, 281, 1588, 7186, 473, 51136, 51136, 406, 2108, 10139, 1958, 457, 2108, 10139, 502, 370, 8437, 294, 502, 2709, 505, 257, 51436, 51436, 1874, 264, 3909, 295, 341, 307, 8858, 538, 1386, 2293, 382, 321, 1116, 411, 370, 300, 1936, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13367246326647306, "compression_ratio": 1.7403846153846154, "no_speech_prob": 1.028924361889949e-05}, {"id": 264, "seek": 129920, "start": 1320.64, "end": 1326.1200000000001, "text": " result the shape of this is 32 by 6 exactly as we'd like so that basically", "tokens": [50364, 27822, 13, 18035, 322, 300, 8310, 370, 341, 307, 264, 1329, 27822, 13, 18035, 2516, 257, 50660, 50660, 8310, 295, 10688, 830, 293, 550, 321, 362, 281, 980, 309, 2051, 597, 10139, 281, 50892, 50892, 1588, 7186, 473, 370, 294, 341, 1389, 439, 613, 366, 8858, 538, 568, 293, 321, 528, 281, 1588, 7186, 473, 51136, 51136, 406, 2108, 10139, 1958, 457, 2108, 10139, 502, 370, 8437, 294, 502, 2709, 505, 257, 51436, 51436, 1874, 264, 3909, 295, 341, 307, 8858, 538, 1386, 2293, 382, 321, 1116, 411, 370, 300, 1936, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13367246326647306, "compression_ratio": 1.7403846153846154, "no_speech_prob": 1.028924361889949e-05}, {"id": 265, "seek": 132612, "start": 1326.12, "end": 1331.84, "text": " took 32 and squashed these by concatenating them into 32 by 6 now", "tokens": [50364, 1890, 8858, 293, 2339, 12219, 613, 538, 1588, 7186, 990, 552, 666, 8858, 538, 1386, 586, 50650, 50650, 341, 307, 733, 295, 12246, 570, 341, 3089, 576, 406, 2674, 1125, 498, 321, 528, 281, 50844, 50844, 1780, 1319, 264, 3461, 2744, 558, 586, 321, 362, 1045, 15743, 1045, 2283, 457, 437, 51110, 51110, 498, 321, 632, 1732, 550, 510, 321, 576, 362, 281, 1319, 264, 3089, 570, 286, 478, 8186, 278, 51303, 51303, 3838, 731, 27822, 1487, 281, 13283, 797, 570, 300, 4523, 484, 281, 312, 257, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.0909817140180986, "compression_ratio": 1.6940639269406392, "no_speech_prob": 2.769329512375407e-06}, {"id": 266, "seek": 132612, "start": 1331.84, "end": 1335.7199999999998, "text": " this is kind of ugly because this code would not generalize if we want to", "tokens": [50364, 1890, 8858, 293, 2339, 12219, 613, 538, 1588, 7186, 990, 552, 666, 8858, 538, 1386, 586, 50650, 50650, 341, 307, 733, 295, 12246, 570, 341, 3089, 576, 406, 2674, 1125, 498, 321, 528, 281, 50844, 50844, 1780, 1319, 264, 3461, 2744, 558, 586, 321, 362, 1045, 15743, 1045, 2283, 457, 437, 51110, 51110, 498, 321, 632, 1732, 550, 510, 321, 576, 362, 281, 1319, 264, 3089, 570, 286, 478, 8186, 278, 51303, 51303, 3838, 731, 27822, 1487, 281, 13283, 797, 570, 300, 4523, 484, 281, 312, 257, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.0909817140180986, "compression_ratio": 1.6940639269406392, "no_speech_prob": 2.769329512375407e-06}, {"id": 267, "seek": 132612, "start": 1335.7199999999998, "end": 1341.04, "text": " later change the block size right now we have three inputs three words but what", "tokens": [50364, 1890, 8858, 293, 2339, 12219, 613, 538, 1588, 7186, 990, 552, 666, 8858, 538, 1386, 586, 50650, 50650, 341, 307, 733, 295, 12246, 570, 341, 3089, 576, 406, 2674, 1125, 498, 321, 528, 281, 50844, 50844, 1780, 1319, 264, 3461, 2744, 558, 586, 321, 362, 1045, 15743, 1045, 2283, 457, 437, 51110, 51110, 498, 321, 632, 1732, 550, 510, 321, 576, 362, 281, 1319, 264, 3089, 570, 286, 478, 8186, 278, 51303, 51303, 3838, 731, 27822, 1487, 281, 13283, 797, 570, 300, 4523, 484, 281, 312, 257, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.0909817140180986, "compression_ratio": 1.6940639269406392, "no_speech_prob": 2.769329512375407e-06}, {"id": 268, "seek": 132612, "start": 1341.04, "end": 1344.8999999999999, "text": " if we had five then here we would have to change the code because I'm indexing", "tokens": [50364, 1890, 8858, 293, 2339, 12219, 613, 538, 1588, 7186, 990, 552, 666, 8858, 538, 1386, 586, 50650, 50650, 341, 307, 733, 295, 12246, 570, 341, 3089, 576, 406, 2674, 1125, 498, 321, 528, 281, 50844, 50844, 1780, 1319, 264, 3461, 2744, 558, 586, 321, 362, 1045, 15743, 1045, 2283, 457, 437, 51110, 51110, 498, 321, 632, 1732, 550, 510, 321, 576, 362, 281, 1319, 264, 3089, 570, 286, 478, 8186, 278, 51303, 51303, 3838, 731, 27822, 1487, 281, 13283, 797, 570, 300, 4523, 484, 281, 312, 257, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.0909817140180986, "compression_ratio": 1.6940639269406392, "no_speech_prob": 2.769329512375407e-06}, {"id": 269, "seek": 132612, "start": 1344.8999999999999, "end": 1349.32, "text": " directly well torch comes to rescue again because that turns out to be a", "tokens": [50364, 1890, 8858, 293, 2339, 12219, 613, 538, 1588, 7186, 990, 552, 666, 8858, 538, 1386, 586, 50650, 50650, 341, 307, 733, 295, 12246, 570, 341, 3089, 576, 406, 2674, 1125, 498, 321, 528, 281, 50844, 50844, 1780, 1319, 264, 3461, 2744, 558, 586, 321, 362, 1045, 15743, 1045, 2283, 457, 437, 51110, 51110, 498, 321, 632, 1732, 550, 510, 321, 576, 362, 281, 1319, 264, 3089, 570, 286, 478, 8186, 278, 51303, 51303, 3838, 731, 27822, 1487, 281, 13283, 797, 570, 300, 4523, 484, 281, 312, 257, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.0909817140180986, "compression_ratio": 1.6940639269406392, "no_speech_prob": 2.769329512375407e-06}, {"id": 270, "seek": 134932, "start": 1349.32, "end": 1356.3999999999999, "text": " function called unbind and it removes a tensor dimension so it removes a tensor", "tokens": [50364, 2445, 1219, 517, 65, 471, 293, 309, 30445, 257, 40863, 10139, 370, 309, 30445, 257, 40863, 50718, 50718, 10139, 11247, 257, 2604, 781, 295, 439, 19793, 2051, 257, 2212, 10139, 1553, 309, 370, 51000, 51000, 341, 307, 2293, 437, 321, 643, 293, 1936, 562, 321, 818, 27822, 13, 2860, 471, 51266, 51266, 27822, 13, 2860, 471, 295, 376, 293, 8437, 10139, 502, 8186, 502, 341, 2709, 505, 257, 1329, 295, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.19151085776251717, "compression_ratio": 1.7857142857142858, "no_speech_prob": 6.438937361963326e-06}, {"id": 271, "seek": 134932, "start": 1356.3999999999999, "end": 1362.04, "text": " dimension returns a tuple of all slices along a given dimension without it so", "tokens": [50364, 2445, 1219, 517, 65, 471, 293, 309, 30445, 257, 40863, 10139, 370, 309, 30445, 257, 40863, 50718, 50718, 10139, 11247, 257, 2604, 781, 295, 439, 19793, 2051, 257, 2212, 10139, 1553, 309, 370, 51000, 51000, 341, 307, 2293, 437, 321, 643, 293, 1936, 562, 321, 818, 27822, 13, 2860, 471, 51266, 51266, 27822, 13, 2860, 471, 295, 376, 293, 8437, 10139, 502, 8186, 502, 341, 2709, 505, 257, 1329, 295, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.19151085776251717, "compression_ratio": 1.7857142857142858, "no_speech_prob": 6.438937361963326e-06}, {"id": 272, "seek": 134932, "start": 1362.04, "end": 1367.36, "text": " this is exactly what we need and basically when we call torch.umbind", "tokens": [50364, 2445, 1219, 517, 65, 471, 293, 309, 30445, 257, 40863, 10139, 370, 309, 30445, 257, 40863, 50718, 50718, 10139, 11247, 257, 2604, 781, 295, 439, 19793, 2051, 257, 2212, 10139, 1553, 309, 370, 51000, 51000, 341, 307, 2293, 437, 321, 643, 293, 1936, 562, 321, 818, 27822, 13, 2860, 471, 51266, 51266, 27822, 13, 2860, 471, 295, 376, 293, 8437, 10139, 502, 8186, 502, 341, 2709, 505, 257, 1329, 295, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.19151085776251717, "compression_ratio": 1.7857142857142858, "no_speech_prob": 6.438937361963326e-06}, {"id": 273, "seek": 134932, "start": 1367.36, "end": 1379.04, "text": " torch.umbind of M and passing dimension 1 index 1 this gives us a list of", "tokens": [50364, 2445, 1219, 517, 65, 471, 293, 309, 30445, 257, 40863, 10139, 370, 309, 30445, 257, 40863, 50718, 50718, 10139, 11247, 257, 2604, 781, 295, 439, 19793, 2051, 257, 2212, 10139, 1553, 309, 370, 51000, 51000, 341, 307, 2293, 437, 321, 643, 293, 1936, 562, 321, 818, 27822, 13, 2860, 471, 51266, 51266, 27822, 13, 2860, 471, 295, 376, 293, 8437, 10139, 502, 8186, 502, 341, 2709, 505, 257, 1329, 295, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.19151085776251717, "compression_ratio": 1.7857142857142858, "no_speech_prob": 6.438937361963326e-06}, {"id": 274, "seek": 137904, "start": 1379.04, "end": 1387.08, "text": " tensors exactly equivalent to this so running this gives us a lang 3 and it's", "tokens": [50364, 10688, 830, 2293, 10344, 281, 341, 370, 2614, 341, 2709, 505, 257, 2265, 805, 293, 309, 311, 50766, 50766, 2293, 341, 1329, 293, 370, 321, 393, 818, 27822, 13, 18035, 322, 309, 293, 2051, 264, 700, 51056, 51056, 10139, 293, 341, 1985, 293, 341, 3909, 307, 264, 912, 457, 586, 341, 307, 309, 1177, 380, 51408, 51408, 1871, 498, 321, 362, 3461, 2744, 805, 420, 1025, 420, 1266, 341, 486, 445, 589, 370, 341, 307, 472, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.19000333547592163, "compression_ratio": 1.627659574468085, "no_speech_prob": 3.0415587843890535e-06}, {"id": 275, "seek": 137904, "start": 1387.08, "end": 1392.8799999999999, "text": " exactly this list and so we can call torch.cat on it and along the first", "tokens": [50364, 10688, 830, 2293, 10344, 281, 341, 370, 2614, 341, 2709, 505, 257, 2265, 805, 293, 309, 311, 50766, 50766, 2293, 341, 1329, 293, 370, 321, 393, 818, 27822, 13, 18035, 322, 309, 293, 2051, 264, 700, 51056, 51056, 10139, 293, 341, 1985, 293, 341, 3909, 307, 264, 912, 457, 586, 341, 307, 309, 1177, 380, 51408, 51408, 1871, 498, 321, 362, 3461, 2744, 805, 420, 1025, 420, 1266, 341, 486, 445, 589, 370, 341, 307, 472, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.19000333547592163, "compression_ratio": 1.627659574468085, "no_speech_prob": 3.0415587843890535e-06}, {"id": 276, "seek": 137904, "start": 1392.8799999999999, "end": 1399.92, "text": " dimension and this works and this shape is the same but now this is it doesn't", "tokens": [50364, 10688, 830, 2293, 10344, 281, 341, 370, 2614, 341, 2709, 505, 257, 2265, 805, 293, 309, 311, 50766, 50766, 2293, 341, 1329, 293, 370, 321, 393, 818, 27822, 13, 18035, 322, 309, 293, 2051, 264, 700, 51056, 51056, 10139, 293, 341, 1985, 293, 341, 3909, 307, 264, 912, 457, 586, 341, 307, 309, 1177, 380, 51408, 51408, 1871, 498, 321, 362, 3461, 2744, 805, 420, 1025, 420, 1266, 341, 486, 445, 589, 370, 341, 307, 472, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.19000333547592163, "compression_ratio": 1.627659574468085, "no_speech_prob": 3.0415587843890535e-06}, {"id": 277, "seek": 137904, "start": 1399.92, "end": 1404.6399999999999, "text": " matter if we have block size 3 or 5 or 10 this will just work so this is one", "tokens": [50364, 10688, 830, 2293, 10344, 281, 341, 370, 2614, 341, 2709, 505, 257, 2265, 805, 293, 309, 311, 50766, 50766, 2293, 341, 1329, 293, 370, 321, 393, 818, 27822, 13, 18035, 322, 309, 293, 2051, 264, 700, 51056, 51056, 10139, 293, 341, 1985, 293, 341, 3909, 307, 264, 912, 457, 586, 341, 307, 309, 1177, 380, 51408, 51408, 1871, 498, 321, 362, 3461, 2744, 805, 420, 1025, 420, 1266, 341, 486, 445, 589, 370, 341, 307, 472, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.19000333547592163, "compression_ratio": 1.627659574468085, "no_speech_prob": 3.0415587843890535e-06}, {"id": 278, "seek": 140464, "start": 1404.64, "end": 1409.2, "text": " way to do it but it turns out that in this case there's actually a significantly", "tokens": [50364, 636, 281, 360, 309, 457, 309, 4523, 484, 300, 294, 341, 1389, 456, 311, 767, 257, 10591, 50592, 50592, 1101, 293, 544, 7148, 636, 293, 341, 2709, 385, 364, 2650, 281, 12075, 412, 512, 50788, 50788, 295, 264, 2154, 1124, 295, 27822, 13, 83, 23153, 370, 718, 311, 1884, 364, 10225, 510, 295, 4959, 490, 51164, 51164, 1958, 281, 3282, 293, 264, 3909, 295, 341, 307, 445, 2443, 309, 311, 257, 2167, 8062, 295, 2443, 3547, 309, 51514, 51514, 4523, 484, 300, 321, 393, 588, 2661, 321, 2906, 341, 382, 819, 20004, 293, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.1724833852237033, "compression_ratio": 1.6554621848739495, "no_speech_prob": 1.0783000107039697e-05}, {"id": 279, "seek": 140464, "start": 1409.2, "end": 1413.1200000000001, "text": " better and more efficient way and this gives me an opportunity to hint at some", "tokens": [50364, 636, 281, 360, 309, 457, 309, 4523, 484, 300, 294, 341, 1389, 456, 311, 767, 257, 10591, 50592, 50592, 1101, 293, 544, 7148, 636, 293, 341, 2709, 385, 364, 2650, 281, 12075, 412, 512, 50788, 50788, 295, 264, 2154, 1124, 295, 27822, 13, 83, 23153, 370, 718, 311, 1884, 364, 10225, 510, 295, 4959, 490, 51164, 51164, 1958, 281, 3282, 293, 264, 3909, 295, 341, 307, 445, 2443, 309, 311, 257, 2167, 8062, 295, 2443, 3547, 309, 51514, 51514, 4523, 484, 300, 321, 393, 588, 2661, 321, 2906, 341, 382, 819, 20004, 293, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.1724833852237033, "compression_ratio": 1.6554621848739495, "no_speech_prob": 1.0783000107039697e-05}, {"id": 280, "seek": 140464, "start": 1413.1200000000001, "end": 1420.64, "text": " of the internals of torch.tensor so let's create an array here of elements from", "tokens": [50364, 636, 281, 360, 309, 457, 309, 4523, 484, 300, 294, 341, 1389, 456, 311, 767, 257, 10591, 50592, 50592, 1101, 293, 544, 7148, 636, 293, 341, 2709, 385, 364, 2650, 281, 12075, 412, 512, 50788, 50788, 295, 264, 2154, 1124, 295, 27822, 13, 83, 23153, 370, 718, 311, 1884, 364, 10225, 510, 295, 4959, 490, 51164, 51164, 1958, 281, 3282, 293, 264, 3909, 295, 341, 307, 445, 2443, 309, 311, 257, 2167, 8062, 295, 2443, 3547, 309, 51514, 51514, 4523, 484, 300, 321, 393, 588, 2661, 321, 2906, 341, 382, 819, 20004, 293, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.1724833852237033, "compression_ratio": 1.6554621848739495, "no_speech_prob": 1.0783000107039697e-05}, {"id": 281, "seek": 140464, "start": 1420.64, "end": 1427.64, "text": " 0 to 17 and the shape of this is just 18 it's a single vector of 18 numbers it", "tokens": [50364, 636, 281, 360, 309, 457, 309, 4523, 484, 300, 294, 341, 1389, 456, 311, 767, 257, 10591, 50592, 50592, 1101, 293, 544, 7148, 636, 293, 341, 2709, 385, 364, 2650, 281, 12075, 412, 512, 50788, 50788, 295, 264, 2154, 1124, 295, 27822, 13, 83, 23153, 370, 718, 311, 1884, 364, 10225, 510, 295, 4959, 490, 51164, 51164, 1958, 281, 3282, 293, 264, 3909, 295, 341, 307, 445, 2443, 309, 311, 257, 2167, 8062, 295, 2443, 3547, 309, 51514, 51514, 4523, 484, 300, 321, 393, 588, 2661, 321, 2906, 341, 382, 819, 20004, 293, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.1724833852237033, "compression_ratio": 1.6554621848739495, "no_speech_prob": 1.0783000107039697e-05}, {"id": 282, "seek": 140464, "start": 1427.64, "end": 1432.48, "text": " turns out that we can very quickly we represent this as different sized and", "tokens": [50364, 636, 281, 360, 309, 457, 309, 4523, 484, 300, 294, 341, 1389, 456, 311, 767, 257, 10591, 50592, 50592, 1101, 293, 544, 7148, 636, 293, 341, 2709, 385, 364, 2650, 281, 12075, 412, 512, 50788, 50788, 295, 264, 2154, 1124, 295, 27822, 13, 83, 23153, 370, 718, 311, 1884, 364, 10225, 510, 295, 4959, 490, 51164, 51164, 1958, 281, 3282, 293, 264, 3909, 295, 341, 307, 445, 2443, 309, 311, 257, 2167, 8062, 295, 2443, 3547, 309, 51514, 51514, 4523, 484, 300, 321, 393, 588, 2661, 321, 2906, 341, 382, 819, 20004, 293, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.1724833852237033, "compression_ratio": 1.6554621848739495, "no_speech_prob": 1.0783000107039697e-05}, {"id": 283, "seek": 143248, "start": 1432.48, "end": 1437.6, "text": " dimensional tensors we do this by calling a view and we can say that", "tokens": [50364, 18795, 10688, 830, 321, 360, 341, 538, 5141, 257, 1910, 293, 321, 393, 584, 300, 50620, 50620, 767, 341, 307, 406, 257, 2167, 8062, 295, 2443, 341, 307, 257, 568, 538, 1722, 40863, 420, 50928, 50928, 8535, 356, 341, 307, 257, 1722, 538, 568, 40863, 420, 341, 307, 767, 257, 805, 538, 805, 538, 568, 40863, 382, 51298, 51298, 938, 382, 264, 3217, 1230, 295, 4959, 510, 12972, 281, 312, 264, 912, 341, 486, 51550, 51550], "temperature": 0.0, "avg_logprob": -0.1502536626962515, "compression_ratio": 1.7514792899408285, "no_speech_prob": 5.0145522436650936e-06}, {"id": 284, "seek": 143248, "start": 1437.6, "end": 1443.76, "text": " actually this is not a single vector of 18 this is a 2 by 9 tensor or", "tokens": [50364, 18795, 10688, 830, 321, 360, 341, 538, 5141, 257, 1910, 293, 321, 393, 584, 300, 50620, 50620, 767, 341, 307, 406, 257, 2167, 8062, 295, 2443, 341, 307, 257, 568, 538, 1722, 40863, 420, 50928, 50928, 8535, 356, 341, 307, 257, 1722, 538, 568, 40863, 420, 341, 307, 767, 257, 805, 538, 805, 538, 568, 40863, 382, 51298, 51298, 938, 382, 264, 3217, 1230, 295, 4959, 510, 12972, 281, 312, 264, 912, 341, 486, 51550, 51550], "temperature": 0.0, "avg_logprob": -0.1502536626962515, "compression_ratio": 1.7514792899408285, "no_speech_prob": 5.0145522436650936e-06}, {"id": 285, "seek": 143248, "start": 1443.76, "end": 1451.16, "text": " alternatively this is a 9 by 2 tensor or this is actually a 3 by 3 by 2 tensor as", "tokens": [50364, 18795, 10688, 830, 321, 360, 341, 538, 5141, 257, 1910, 293, 321, 393, 584, 300, 50620, 50620, 767, 341, 307, 406, 257, 2167, 8062, 295, 2443, 341, 307, 257, 568, 538, 1722, 40863, 420, 50928, 50928, 8535, 356, 341, 307, 257, 1722, 538, 568, 40863, 420, 341, 307, 767, 257, 805, 538, 805, 538, 568, 40863, 382, 51298, 51298, 938, 382, 264, 3217, 1230, 295, 4959, 510, 12972, 281, 312, 264, 912, 341, 486, 51550, 51550], "temperature": 0.0, "avg_logprob": -0.1502536626962515, "compression_ratio": 1.7514792899408285, "no_speech_prob": 5.0145522436650936e-06}, {"id": 286, "seek": 143248, "start": 1451.16, "end": 1456.2, "text": " long as the total number of elements here multiply to be the same this will", "tokens": [50364, 18795, 10688, 830, 321, 360, 341, 538, 5141, 257, 1910, 293, 321, 393, 584, 300, 50620, 50620, 767, 341, 307, 406, 257, 2167, 8062, 295, 2443, 341, 307, 257, 568, 538, 1722, 40863, 420, 50928, 50928, 8535, 356, 341, 307, 257, 1722, 538, 568, 40863, 420, 341, 307, 767, 257, 805, 538, 805, 538, 568, 40863, 382, 51298, 51298, 938, 382, 264, 3217, 1230, 295, 4959, 510, 12972, 281, 312, 264, 912, 341, 486, 51550, 51550], "temperature": 0.0, "avg_logprob": -0.1502536626962515, "compression_ratio": 1.7514792899408285, "no_speech_prob": 5.0145522436650936e-06}, {"id": 287, "seek": 145620, "start": 1456.2, "end": 1462.68, "text": " just work and in pytorch this operation calling that view is extremely", "tokens": [50364, 445, 589, 293, 294, 25878, 284, 339, 341, 6916, 5141, 300, 1910, 307, 4664, 50688, 50688, 7148, 293, 264, 1778, 337, 300, 307, 300, 294, 1184, 40863, 456, 311, 746, 50916, 50916, 1219, 264, 14217, 6725, 293, 264, 6725, 307, 445, 264, 3547, 1009, 382, 257, 51206, 51206, 472, 12, 18759, 8062, 293, 341, 307, 577, 341, 40863, 307, 10379, 294, 264, 51396, 51396, 3820, 4675, 309, 311, 1009, 257, 472, 12, 18759, 8062, 457, 562, 321, 818, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.17649194670886528, "compression_ratio": 1.7853658536585366, "no_speech_prob": 1.8161915704695275e-06}, {"id": 288, "seek": 145620, "start": 1462.68, "end": 1467.24, "text": " efficient and the reason for that is that in each tensor there's something", "tokens": [50364, 445, 589, 293, 294, 25878, 284, 339, 341, 6916, 5141, 300, 1910, 307, 4664, 50688, 50688, 7148, 293, 264, 1778, 337, 300, 307, 300, 294, 1184, 40863, 456, 311, 746, 50916, 50916, 1219, 264, 14217, 6725, 293, 264, 6725, 307, 445, 264, 3547, 1009, 382, 257, 51206, 51206, 472, 12, 18759, 8062, 293, 341, 307, 577, 341, 40863, 307, 10379, 294, 264, 51396, 51396, 3820, 4675, 309, 311, 1009, 257, 472, 12, 18759, 8062, 457, 562, 321, 818, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.17649194670886528, "compression_ratio": 1.7853658536585366, "no_speech_prob": 1.8161915704695275e-06}, {"id": 289, "seek": 145620, "start": 1467.24, "end": 1473.04, "text": " called the underlying storage and the storage is just the numbers always as a", "tokens": [50364, 445, 589, 293, 294, 25878, 284, 339, 341, 6916, 5141, 300, 1910, 307, 4664, 50688, 50688, 7148, 293, 264, 1778, 337, 300, 307, 300, 294, 1184, 40863, 456, 311, 746, 50916, 50916, 1219, 264, 14217, 6725, 293, 264, 6725, 307, 445, 264, 3547, 1009, 382, 257, 51206, 51206, 472, 12, 18759, 8062, 293, 341, 307, 577, 341, 40863, 307, 10379, 294, 264, 51396, 51396, 3820, 4675, 309, 311, 1009, 257, 472, 12, 18759, 8062, 457, 562, 321, 818, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.17649194670886528, "compression_ratio": 1.7853658536585366, "no_speech_prob": 1.8161915704695275e-06}, {"id": 290, "seek": 145620, "start": 1473.04, "end": 1476.8400000000001, "text": " one-dimensional vector and this is how this tensor is represented in the", "tokens": [50364, 445, 589, 293, 294, 25878, 284, 339, 341, 6916, 5141, 300, 1910, 307, 4664, 50688, 50688, 7148, 293, 264, 1778, 337, 300, 307, 300, 294, 1184, 40863, 456, 311, 746, 50916, 50916, 1219, 264, 14217, 6725, 293, 264, 6725, 307, 445, 264, 3547, 1009, 382, 257, 51206, 51206, 472, 12, 18759, 8062, 293, 341, 307, 577, 341, 40863, 307, 10379, 294, 264, 51396, 51396, 3820, 4675, 309, 311, 1009, 257, 472, 12, 18759, 8062, 457, 562, 321, 818, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.17649194670886528, "compression_ratio": 1.7853658536585366, "no_speech_prob": 1.8161915704695275e-06}, {"id": 291, "seek": 145620, "start": 1476.8400000000001, "end": 1482.2, "text": " computer memory it's always a one-dimensional vector but when we call", "tokens": [50364, 445, 589, 293, 294, 25878, 284, 339, 341, 6916, 5141, 300, 1910, 307, 4664, 50688, 50688, 7148, 293, 264, 1778, 337, 300, 307, 300, 294, 1184, 40863, 456, 311, 746, 50916, 50916, 1219, 264, 14217, 6725, 293, 264, 6725, 307, 445, 264, 3547, 1009, 382, 257, 51206, 51206, 472, 12, 18759, 8062, 293, 341, 307, 577, 341, 40863, 307, 10379, 294, 264, 51396, 51396, 3820, 4675, 309, 311, 1009, 257, 472, 12, 18759, 8062, 457, 562, 321, 818, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.17649194670886528, "compression_ratio": 1.7853658536585366, "no_speech_prob": 1.8161915704695275e-06}, {"id": 292, "seek": 148220, "start": 1482.2, "end": 1487.92, "text": " that view we are manipulating some of attributes of that tensor that dictate", "tokens": [50364, 300, 1910, 321, 366, 40805, 512, 295, 17212, 295, 300, 40863, 300, 36071, 50650, 50650, 577, 341, 472, 12, 18759, 8310, 307, 26749, 281, 312, 364, 297, 12, 18759, 50850, 50850, 40863, 293, 370, 437, 311, 2737, 510, 307, 300, 572, 4675, 307, 885, 3105, 25365, 51074, 51074, 4259, 420, 2942, 562, 321, 818, 300, 1910, 264, 6725, 307, 14800, 457, 562, 291, 51334, 51334, 818, 300, 1910, 512, 295, 264, 6920, 17212, 295, 264, 1910, 295, 341, 40863, 51610, 51610, 366, 885, 37161, 293, 3105, 294, 1729, 300, 746, 456, 311, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1740403175354004, "compression_ratio": 1.9270386266094421, "no_speech_prob": 8.529900696885306e-06}, {"id": 293, "seek": 148220, "start": 1487.92, "end": 1491.92, "text": " how this one-dimensional sequence is interpreted to be an n-dimensional", "tokens": [50364, 300, 1910, 321, 366, 40805, 512, 295, 17212, 295, 300, 40863, 300, 36071, 50650, 50650, 577, 341, 472, 12, 18759, 8310, 307, 26749, 281, 312, 364, 297, 12, 18759, 50850, 50850, 40863, 293, 370, 437, 311, 2737, 510, 307, 300, 572, 4675, 307, 885, 3105, 25365, 51074, 51074, 4259, 420, 2942, 562, 321, 818, 300, 1910, 264, 6725, 307, 14800, 457, 562, 291, 51334, 51334, 818, 300, 1910, 512, 295, 264, 6920, 17212, 295, 264, 1910, 295, 341, 40863, 51610, 51610, 366, 885, 37161, 293, 3105, 294, 1729, 300, 746, 456, 311, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1740403175354004, "compression_ratio": 1.9270386266094421, "no_speech_prob": 8.529900696885306e-06}, {"id": 294, "seek": 148220, "start": 1491.92, "end": 1496.4, "text": " tensor and so what's happening here is that no memory is being changed copied", "tokens": [50364, 300, 1910, 321, 366, 40805, 512, 295, 17212, 295, 300, 40863, 300, 36071, 50650, 50650, 577, 341, 472, 12, 18759, 8310, 307, 26749, 281, 312, 364, 297, 12, 18759, 50850, 50850, 40863, 293, 370, 437, 311, 2737, 510, 307, 300, 572, 4675, 307, 885, 3105, 25365, 51074, 51074, 4259, 420, 2942, 562, 321, 818, 300, 1910, 264, 6725, 307, 14800, 457, 562, 291, 51334, 51334, 818, 300, 1910, 512, 295, 264, 6920, 17212, 295, 264, 1910, 295, 341, 40863, 51610, 51610, 366, 885, 37161, 293, 3105, 294, 1729, 300, 746, 456, 311, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1740403175354004, "compression_ratio": 1.9270386266094421, "no_speech_prob": 8.529900696885306e-06}, {"id": 295, "seek": 148220, "start": 1496.4, "end": 1501.6000000000001, "text": " moved or created when we call that view the storage is identical but when you", "tokens": [50364, 300, 1910, 321, 366, 40805, 512, 295, 17212, 295, 300, 40863, 300, 36071, 50650, 50650, 577, 341, 472, 12, 18759, 8310, 307, 26749, 281, 312, 364, 297, 12, 18759, 50850, 50850, 40863, 293, 370, 437, 311, 2737, 510, 307, 300, 572, 4675, 307, 885, 3105, 25365, 51074, 51074, 4259, 420, 2942, 562, 321, 818, 300, 1910, 264, 6725, 307, 14800, 457, 562, 291, 51334, 51334, 818, 300, 1910, 512, 295, 264, 6920, 17212, 295, 264, 1910, 295, 341, 40863, 51610, 51610, 366, 885, 37161, 293, 3105, 294, 1729, 300, 746, 456, 311, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1740403175354004, "compression_ratio": 1.9270386266094421, "no_speech_prob": 8.529900696885306e-06}, {"id": 296, "seek": 148220, "start": 1501.6000000000001, "end": 1507.1200000000001, "text": " call that view some of the internal attributes of the view of this tensor", "tokens": [50364, 300, 1910, 321, 366, 40805, 512, 295, 17212, 295, 300, 40863, 300, 36071, 50650, 50650, 577, 341, 472, 12, 18759, 8310, 307, 26749, 281, 312, 364, 297, 12, 18759, 50850, 50850, 40863, 293, 370, 437, 311, 2737, 510, 307, 300, 572, 4675, 307, 885, 3105, 25365, 51074, 51074, 4259, 420, 2942, 562, 321, 818, 300, 1910, 264, 6725, 307, 14800, 457, 562, 291, 51334, 51334, 818, 300, 1910, 512, 295, 264, 6920, 17212, 295, 264, 1910, 295, 341, 40863, 51610, 51610, 366, 885, 37161, 293, 3105, 294, 1729, 300, 746, 456, 311, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1740403175354004, "compression_ratio": 1.9270386266094421, "no_speech_prob": 8.529900696885306e-06}, {"id": 297, "seek": 148220, "start": 1507.1200000000001, "end": 1510.2, "text": " are being manipulated and changed in particular that something there's", "tokens": [50364, 300, 1910, 321, 366, 40805, 512, 295, 17212, 295, 300, 40863, 300, 36071, 50650, 50650, 577, 341, 472, 12, 18759, 8310, 307, 26749, 281, 312, 364, 297, 12, 18759, 50850, 50850, 40863, 293, 370, 437, 311, 2737, 510, 307, 300, 572, 4675, 307, 885, 3105, 25365, 51074, 51074, 4259, 420, 2942, 562, 321, 818, 300, 1910, 264, 6725, 307, 14800, 457, 562, 291, 51334, 51334, 818, 300, 1910, 512, 295, 264, 6920, 17212, 295, 264, 1910, 295, 341, 40863, 51610, 51610, 366, 885, 37161, 293, 3105, 294, 1729, 300, 746, 456, 311, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1740403175354004, "compression_ratio": 1.9270386266094421, "no_speech_prob": 8.529900696885306e-06}, {"id": 298, "seek": 151020, "start": 1510.2, "end": 1513.8, "text": " something called a storage offset strides and shapes and those are", "tokens": [50364, 746, 1219, 257, 6725, 18687, 1056, 1875, 293, 10854, 293, 729, 366, 50544, 50544, 37161, 370, 300, 341, 472, 12, 18759, 8310, 295, 36088, 307, 1612, 382, 819, 50764, 50764, 293, 18795, 41011, 456, 311, 257, 6968, 2183, 510, 490, 9336, 1219, 25878, 284, 339, 51006, 51006, 2154, 1124, 689, 415, 1709, 666, 512, 295, 341, 365, 3104, 281, 40863, 293, 577, 264, 51236, 51236, 1910, 295, 257, 40863, 307, 10379, 293, 341, 307, 534, 445, 411, 257, 14978, 7690, 51492, 51492, 295, 13460, 264, 4001, 4675, 293, 370, 341, 307, 257, 1238, 665, 6968, 2183, 300, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.15997323895444965, "compression_ratio": 1.7782101167315174, "no_speech_prob": 1.5445275494130328e-05}, {"id": 299, "seek": 151020, "start": 1513.8, "end": 1518.2, "text": " manipulated so that this one-dimensional sequence of bytes is seen as different", "tokens": [50364, 746, 1219, 257, 6725, 18687, 1056, 1875, 293, 10854, 293, 729, 366, 50544, 50544, 37161, 370, 300, 341, 472, 12, 18759, 8310, 295, 36088, 307, 1612, 382, 819, 50764, 50764, 293, 18795, 41011, 456, 311, 257, 6968, 2183, 510, 490, 9336, 1219, 25878, 284, 339, 51006, 51006, 2154, 1124, 689, 415, 1709, 666, 512, 295, 341, 365, 3104, 281, 40863, 293, 577, 264, 51236, 51236, 1910, 295, 257, 40863, 307, 10379, 293, 341, 307, 534, 445, 411, 257, 14978, 7690, 51492, 51492, 295, 13460, 264, 4001, 4675, 293, 370, 341, 307, 257, 1238, 665, 6968, 2183, 300, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.15997323895444965, "compression_ratio": 1.7782101167315174, "no_speech_prob": 1.5445275494130328e-05}, {"id": 300, "seek": 151020, "start": 1518.2, "end": 1523.04, "text": " and dimensional arrays there's a blog post here from Eric called pytorch", "tokens": [50364, 746, 1219, 257, 6725, 18687, 1056, 1875, 293, 10854, 293, 729, 366, 50544, 50544, 37161, 370, 300, 341, 472, 12, 18759, 8310, 295, 36088, 307, 1612, 382, 819, 50764, 50764, 293, 18795, 41011, 456, 311, 257, 6968, 2183, 510, 490, 9336, 1219, 25878, 284, 339, 51006, 51006, 2154, 1124, 689, 415, 1709, 666, 512, 295, 341, 365, 3104, 281, 40863, 293, 577, 264, 51236, 51236, 1910, 295, 257, 40863, 307, 10379, 293, 341, 307, 534, 445, 411, 257, 14978, 7690, 51492, 51492, 295, 13460, 264, 4001, 4675, 293, 370, 341, 307, 257, 1238, 665, 6968, 2183, 300, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.15997323895444965, "compression_ratio": 1.7782101167315174, "no_speech_prob": 1.5445275494130328e-05}, {"id": 301, "seek": 151020, "start": 1523.04, "end": 1527.64, "text": " internals where he goes into some of this with respect to tensor and how the", "tokens": [50364, 746, 1219, 257, 6725, 18687, 1056, 1875, 293, 10854, 293, 729, 366, 50544, 50544, 37161, 370, 300, 341, 472, 12, 18759, 8310, 295, 36088, 307, 1612, 382, 819, 50764, 50764, 293, 18795, 41011, 456, 311, 257, 6968, 2183, 510, 490, 9336, 1219, 25878, 284, 339, 51006, 51006, 2154, 1124, 689, 415, 1709, 666, 512, 295, 341, 365, 3104, 281, 40863, 293, 577, 264, 51236, 51236, 1910, 295, 257, 40863, 307, 10379, 293, 341, 307, 534, 445, 411, 257, 14978, 7690, 51492, 51492, 295, 13460, 264, 4001, 4675, 293, 370, 341, 307, 257, 1238, 665, 6968, 2183, 300, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.15997323895444965, "compression_ratio": 1.7782101167315174, "no_speech_prob": 1.5445275494130328e-05}, {"id": 302, "seek": 151020, "start": 1527.64, "end": 1532.76, "text": " view of a tensor is represented and this is really just like a logical construct", "tokens": [50364, 746, 1219, 257, 6725, 18687, 1056, 1875, 293, 10854, 293, 729, 366, 50544, 50544, 37161, 370, 300, 341, 472, 12, 18759, 8310, 295, 36088, 307, 1612, 382, 819, 50764, 50764, 293, 18795, 41011, 456, 311, 257, 6968, 2183, 510, 490, 9336, 1219, 25878, 284, 339, 51006, 51006, 2154, 1124, 689, 415, 1709, 666, 512, 295, 341, 365, 3104, 281, 40863, 293, 577, 264, 51236, 51236, 1910, 295, 257, 40863, 307, 10379, 293, 341, 307, 534, 445, 411, 257, 14978, 7690, 51492, 51492, 295, 13460, 264, 4001, 4675, 293, 370, 341, 307, 257, 1238, 665, 6968, 2183, 300, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.15997323895444965, "compression_ratio": 1.7782101167315174, "no_speech_prob": 1.5445275494130328e-05}, {"id": 303, "seek": 151020, "start": 1532.76, "end": 1538.32, "text": " of representing the physical memory and so this is a pretty good blog post that", "tokens": [50364, 746, 1219, 257, 6725, 18687, 1056, 1875, 293, 10854, 293, 729, 366, 50544, 50544, 37161, 370, 300, 341, 472, 12, 18759, 8310, 295, 36088, 307, 1612, 382, 819, 50764, 50764, 293, 18795, 41011, 456, 311, 257, 6968, 2183, 510, 490, 9336, 1219, 25878, 284, 339, 51006, 51006, 2154, 1124, 689, 415, 1709, 666, 512, 295, 341, 365, 3104, 281, 40863, 293, 577, 264, 51236, 51236, 1910, 295, 257, 40863, 307, 10379, 293, 341, 307, 534, 445, 411, 257, 14978, 7690, 51492, 51492, 295, 13460, 264, 4001, 4675, 293, 370, 341, 307, 257, 1238, 665, 6968, 2183, 300, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.15997323895444965, "compression_ratio": 1.7782101167315174, "no_speech_prob": 1.5445275494130328e-05}, {"id": 304, "seek": 153832, "start": 1538.32, "end": 1542.04, "text": " you can go into I might also create an entire video on the internals of torch", "tokens": [50364, 291, 393, 352, 666, 286, 1062, 611, 1884, 364, 2302, 960, 322, 264, 2154, 1124, 295, 27822, 50550, 50550, 40863, 293, 577, 341, 1985, 337, 510, 321, 445, 3637, 300, 341, 307, 364, 4664, 50766, 50766, 7148, 6916, 293, 498, 286, 12097, 341, 293, 808, 646, 281, 527, 376, 321, 536, 300, 264, 51126, 51126, 3909, 295, 527, 376, 307, 8858, 538, 805, 538, 568, 457, 321, 393, 2935, 1029, 337, 25878, 284, 339, 281, 1910, 341, 51428, 51428, 2602, 382, 257, 8858, 538, 1386, 293, 264, 636, 341, 2170, 24183, 292, 666, 257, 8858, 538, 1386, 10225, 445, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.15085358295625853, "compression_ratio": 1.6943231441048034, "no_speech_prob": 7.646036465303041e-06}, {"id": 305, "seek": 153832, "start": 1542.04, "end": 1546.36, "text": " tensor and how this works for here we just note that this is an extremely", "tokens": [50364, 291, 393, 352, 666, 286, 1062, 611, 1884, 364, 2302, 960, 322, 264, 2154, 1124, 295, 27822, 50550, 50550, 40863, 293, 577, 341, 1985, 337, 510, 321, 445, 3637, 300, 341, 307, 364, 4664, 50766, 50766, 7148, 6916, 293, 498, 286, 12097, 341, 293, 808, 646, 281, 527, 376, 321, 536, 300, 264, 51126, 51126, 3909, 295, 527, 376, 307, 8858, 538, 805, 538, 568, 457, 321, 393, 2935, 1029, 337, 25878, 284, 339, 281, 1910, 341, 51428, 51428, 2602, 382, 257, 8858, 538, 1386, 293, 264, 636, 341, 2170, 24183, 292, 666, 257, 8858, 538, 1386, 10225, 445, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.15085358295625853, "compression_ratio": 1.6943231441048034, "no_speech_prob": 7.646036465303041e-06}, {"id": 306, "seek": 153832, "start": 1546.36, "end": 1553.56, "text": " efficient operation and if I delete this and come back to our M we see that the", "tokens": [50364, 291, 393, 352, 666, 286, 1062, 611, 1884, 364, 2302, 960, 322, 264, 2154, 1124, 295, 27822, 50550, 50550, 40863, 293, 577, 341, 1985, 337, 510, 321, 445, 3637, 300, 341, 307, 364, 4664, 50766, 50766, 7148, 6916, 293, 498, 286, 12097, 341, 293, 808, 646, 281, 527, 376, 321, 536, 300, 264, 51126, 51126, 3909, 295, 527, 376, 307, 8858, 538, 805, 538, 568, 457, 321, 393, 2935, 1029, 337, 25878, 284, 339, 281, 1910, 341, 51428, 51428, 2602, 382, 257, 8858, 538, 1386, 293, 264, 636, 341, 2170, 24183, 292, 666, 257, 8858, 538, 1386, 10225, 445, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.15085358295625853, "compression_ratio": 1.6943231441048034, "no_speech_prob": 7.646036465303041e-06}, {"id": 307, "seek": 153832, "start": 1553.56, "end": 1559.6, "text": " shape of our M is 32 by 3 by 2 but we can simply ask for pytorch to view this", "tokens": [50364, 291, 393, 352, 666, 286, 1062, 611, 1884, 364, 2302, 960, 322, 264, 2154, 1124, 295, 27822, 50550, 50550, 40863, 293, 577, 341, 1985, 337, 510, 321, 445, 3637, 300, 341, 307, 364, 4664, 50766, 50766, 7148, 6916, 293, 498, 286, 12097, 341, 293, 808, 646, 281, 527, 376, 321, 536, 300, 264, 51126, 51126, 3909, 295, 527, 376, 307, 8858, 538, 805, 538, 568, 457, 321, 393, 2935, 1029, 337, 25878, 284, 339, 281, 1910, 341, 51428, 51428, 2602, 382, 257, 8858, 538, 1386, 293, 264, 636, 341, 2170, 24183, 292, 666, 257, 8858, 538, 1386, 10225, 445, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.15085358295625853, "compression_ratio": 1.6943231441048034, "no_speech_prob": 7.646036465303041e-06}, {"id": 308, "seek": 153832, "start": 1559.6, "end": 1567.6399999999999, "text": " instead as a 32 by 6 and the way this gets flattened into a 32 by 6 array just", "tokens": [50364, 291, 393, 352, 666, 286, 1062, 611, 1884, 364, 2302, 960, 322, 264, 2154, 1124, 295, 27822, 50550, 50550, 40863, 293, 577, 341, 1985, 337, 510, 321, 445, 3637, 300, 341, 307, 364, 4664, 50766, 50766, 7148, 6916, 293, 498, 286, 12097, 341, 293, 808, 646, 281, 527, 376, 321, 536, 300, 264, 51126, 51126, 3909, 295, 527, 376, 307, 8858, 538, 805, 538, 568, 457, 321, 393, 2935, 1029, 337, 25878, 284, 339, 281, 1910, 341, 51428, 51428, 2602, 382, 257, 8858, 538, 1386, 293, 264, 636, 341, 2170, 24183, 292, 666, 257, 8858, 538, 1386, 10225, 445, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.15085358295625853, "compression_ratio": 1.6943231441048034, "no_speech_prob": 7.646036465303041e-06}, {"id": 309, "seek": 156764, "start": 1567.64, "end": 1574.68, "text": " happens that these two get stacked up in a single row and so that's basically the", "tokens": [50364, 2314, 300, 613, 732, 483, 28867, 493, 294, 257, 2167, 5386, 293, 370, 300, 311, 1936, 264, 50716, 50716, 1588, 7186, 399, 6916, 300, 321, 434, 934, 293, 291, 393, 16888, 300, 341, 767, 50924, 50924, 2709, 264, 1900, 912, 1874, 382, 437, 321, 632, 949, 370, 341, 307, 364, 4478, 288, 6915, 51168, 51168, 293, 291, 393, 536, 300, 439, 264, 4959, 295, 613, 732, 10688, 830, 366, 264, 912, 293, 370, 321, 51406, 51406, 483, 264, 1900, 912, 1874, 370, 938, 1657, 2099, 321, 393, 767, 445, 808, 510, 293, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.0831455687681834, "compression_ratio": 1.8617511520737327, "no_speech_prob": 7.527784418925876e-06}, {"id": 310, "seek": 156764, "start": 1574.68, "end": 1578.8400000000001, "text": " concatenation operation that we're after and you can verify that this actually", "tokens": [50364, 2314, 300, 613, 732, 483, 28867, 493, 294, 257, 2167, 5386, 293, 370, 300, 311, 1936, 264, 50716, 50716, 1588, 7186, 399, 6916, 300, 321, 434, 934, 293, 291, 393, 16888, 300, 341, 767, 50924, 50924, 2709, 264, 1900, 912, 1874, 382, 437, 321, 632, 949, 370, 341, 307, 364, 4478, 288, 6915, 51168, 51168, 293, 291, 393, 536, 300, 439, 264, 4959, 295, 613, 732, 10688, 830, 366, 264, 912, 293, 370, 321, 51406, 51406, 483, 264, 1900, 912, 1874, 370, 938, 1657, 2099, 321, 393, 767, 445, 808, 510, 293, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.0831455687681834, "compression_ratio": 1.8617511520737327, "no_speech_prob": 7.527784418925876e-06}, {"id": 311, "seek": 156764, "start": 1578.8400000000001, "end": 1583.72, "text": " gives the exact same result as what we had before so this is an element y equals", "tokens": [50364, 2314, 300, 613, 732, 483, 28867, 493, 294, 257, 2167, 5386, 293, 370, 300, 311, 1936, 264, 50716, 50716, 1588, 7186, 399, 6916, 300, 321, 434, 934, 293, 291, 393, 16888, 300, 341, 767, 50924, 50924, 2709, 264, 1900, 912, 1874, 382, 437, 321, 632, 949, 370, 341, 307, 364, 4478, 288, 6915, 51168, 51168, 293, 291, 393, 536, 300, 439, 264, 4959, 295, 613, 732, 10688, 830, 366, 264, 912, 293, 370, 321, 51406, 51406, 483, 264, 1900, 912, 1874, 370, 938, 1657, 2099, 321, 393, 767, 445, 808, 510, 293, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.0831455687681834, "compression_ratio": 1.8617511520737327, "no_speech_prob": 7.527784418925876e-06}, {"id": 312, "seek": 156764, "start": 1583.72, "end": 1588.48, "text": " and you can see that all the elements of these two tensors are the same and so we", "tokens": [50364, 2314, 300, 613, 732, 483, 28867, 493, 294, 257, 2167, 5386, 293, 370, 300, 311, 1936, 264, 50716, 50716, 1588, 7186, 399, 6916, 300, 321, 434, 934, 293, 291, 393, 16888, 300, 341, 767, 50924, 50924, 2709, 264, 1900, 912, 1874, 382, 437, 321, 632, 949, 370, 341, 307, 364, 4478, 288, 6915, 51168, 51168, 293, 291, 393, 536, 300, 439, 264, 4959, 295, 613, 732, 10688, 830, 366, 264, 912, 293, 370, 321, 51406, 51406, 483, 264, 1900, 912, 1874, 370, 938, 1657, 2099, 321, 393, 767, 445, 808, 510, 293, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.0831455687681834, "compression_ratio": 1.8617511520737327, "no_speech_prob": 7.527784418925876e-06}, {"id": 313, "seek": 156764, "start": 1588.48, "end": 1593.8400000000001, "text": " get the exact same result so long story short we can actually just come here and", "tokens": [50364, 2314, 300, 613, 732, 483, 28867, 493, 294, 257, 2167, 5386, 293, 370, 300, 311, 1936, 264, 50716, 50716, 1588, 7186, 399, 6916, 300, 321, 434, 934, 293, 291, 393, 16888, 300, 341, 767, 50924, 50924, 2709, 264, 1900, 912, 1874, 382, 437, 321, 632, 949, 370, 341, 307, 364, 4478, 288, 6915, 51168, 51168, 293, 291, 393, 536, 300, 439, 264, 4959, 295, 613, 732, 10688, 830, 366, 264, 912, 293, 370, 321, 51406, 51406, 483, 264, 1900, 912, 1874, 370, 938, 1657, 2099, 321, 393, 767, 445, 808, 510, 293, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.0831455687681834, "compression_ratio": 1.8617511520737327, "no_speech_prob": 7.527784418925876e-06}, {"id": 314, "seek": 159384, "start": 1593.84, "end": 1600.9199999999998, "text": " if we just view this as a 32 by 6 instead then this multiplication will", "tokens": [50364, 498, 321, 445, 1910, 341, 382, 257, 8858, 538, 1386, 2602, 550, 341, 27290, 486, 50718, 50718, 589, 293, 976, 505, 264, 7633, 4368, 300, 321, 434, 934, 370, 498, 341, 307, 389, 550, 389, 8240, 51012, 51012, 13475, 307, 586, 264, 3262, 18795, 2430, 763, 337, 633, 472, 295, 527, 8858, 51282, 51282, 5110, 293, 341, 2709, 264, 14721, 1874, 718, 385, 360, 732, 721, 510, 1230, 51538, 51538], "temperature": 0.0, "avg_logprob": -0.12300542990366618, "compression_ratio": 1.5595854922279793, "no_speech_prob": 1.1124917364213616e-05}, {"id": 315, "seek": 159384, "start": 1600.9199999999998, "end": 1606.8, "text": " work and give us the hidden states that we're after so if this is H then H dash", "tokens": [50364, 498, 321, 445, 1910, 341, 382, 257, 8858, 538, 1386, 2602, 550, 341, 27290, 486, 50718, 50718, 589, 293, 976, 505, 264, 7633, 4368, 300, 321, 434, 934, 370, 498, 341, 307, 389, 550, 389, 8240, 51012, 51012, 13475, 307, 586, 264, 3262, 18795, 2430, 763, 337, 633, 472, 295, 527, 8858, 51282, 51282, 5110, 293, 341, 2709, 264, 14721, 1874, 718, 385, 360, 732, 721, 510, 1230, 51538, 51538], "temperature": 0.0, "avg_logprob": -0.12300542990366618, "compression_ratio": 1.5595854922279793, "no_speech_prob": 1.1124917364213616e-05}, {"id": 316, "seek": 159384, "start": 1606.8, "end": 1612.1999999999998, "text": " shaped is now the hundred dimensional activations for every one of our 32", "tokens": [50364, 498, 321, 445, 1910, 341, 382, 257, 8858, 538, 1386, 2602, 550, 341, 27290, 486, 50718, 50718, 589, 293, 976, 505, 264, 7633, 4368, 300, 321, 434, 934, 370, 498, 341, 307, 389, 550, 389, 8240, 51012, 51012, 13475, 307, 586, 264, 3262, 18795, 2430, 763, 337, 633, 472, 295, 527, 8858, 51282, 51282, 5110, 293, 341, 2709, 264, 14721, 1874, 718, 385, 360, 732, 721, 510, 1230, 51538, 51538], "temperature": 0.0, "avg_logprob": -0.12300542990366618, "compression_ratio": 1.5595854922279793, "no_speech_prob": 1.1124917364213616e-05}, {"id": 317, "seek": 159384, "start": 1612.1999999999998, "end": 1617.32, "text": " examples and this gives the desired result let me do two things here number", "tokens": [50364, 498, 321, 445, 1910, 341, 382, 257, 8858, 538, 1386, 2602, 550, 341, 27290, 486, 50718, 50718, 589, 293, 976, 505, 264, 7633, 4368, 300, 321, 434, 934, 370, 498, 341, 307, 389, 550, 389, 8240, 51012, 51012, 13475, 307, 586, 264, 3262, 18795, 2430, 763, 337, 633, 472, 295, 527, 8858, 51282, 51282, 5110, 293, 341, 2709, 264, 14721, 1874, 718, 385, 360, 732, 721, 510, 1230, 51538, 51538], "temperature": 0.0, "avg_logprob": -0.12300542990366618, "compression_ratio": 1.5595854922279793, "no_speech_prob": 1.1124917364213616e-05}, {"id": 318, "seek": 161732, "start": 1617.32, "end": 1625.76, "text": " one let's not use 32 we can for example do something like m dot shape at 0 so", "tokens": [50364, 472, 718, 311, 406, 764, 8858, 321, 393, 337, 1365, 360, 746, 411, 275, 5893, 3909, 412, 1958, 370, 50786, 50786, 300, 321, 500, 380, 1152, 22332, 613, 3547, 293, 341, 576, 589, 337, 604, 2744, 295, 341, 376, 50984, 50984, 420, 8535, 356, 321, 393, 611, 360, 3671, 502, 562, 321, 360, 3671, 502, 25878, 284, 339, 486, 51220, 51220, 13596, 437, 341, 820, 312, 570, 264, 1230, 295, 4959, 1633, 312, 264, 912, 293, 51436, 51436, 321, 434, 1566, 300, 341, 307, 1386, 25878, 284, 339, 486, 28446, 300, 341, 1633, 312, 8858, 420, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.09850951194763184, "compression_ratio": 1.7264573991031391, "no_speech_prob": 3.041470790776657e-06}, {"id": 319, "seek": 161732, "start": 1625.76, "end": 1629.72, "text": " that we don't hardcode these numbers and this would work for any size of this M", "tokens": [50364, 472, 718, 311, 406, 764, 8858, 321, 393, 337, 1365, 360, 746, 411, 275, 5893, 3909, 412, 1958, 370, 50786, 50786, 300, 321, 500, 380, 1152, 22332, 613, 3547, 293, 341, 576, 589, 337, 604, 2744, 295, 341, 376, 50984, 50984, 420, 8535, 356, 321, 393, 611, 360, 3671, 502, 562, 321, 360, 3671, 502, 25878, 284, 339, 486, 51220, 51220, 13596, 437, 341, 820, 312, 570, 264, 1230, 295, 4959, 1633, 312, 264, 912, 293, 51436, 51436, 321, 434, 1566, 300, 341, 307, 1386, 25878, 284, 339, 486, 28446, 300, 341, 1633, 312, 8858, 420, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.09850951194763184, "compression_ratio": 1.7264573991031391, "no_speech_prob": 3.041470790776657e-06}, {"id": 320, "seek": 161732, "start": 1629.72, "end": 1634.4399999999998, "text": " or alternatively we can also do negative 1 when we do negative 1 pytorch will", "tokens": [50364, 472, 718, 311, 406, 764, 8858, 321, 393, 337, 1365, 360, 746, 411, 275, 5893, 3909, 412, 1958, 370, 50786, 50786, 300, 321, 500, 380, 1152, 22332, 613, 3547, 293, 341, 576, 589, 337, 604, 2744, 295, 341, 376, 50984, 50984, 420, 8535, 356, 321, 393, 611, 360, 3671, 502, 562, 321, 360, 3671, 502, 25878, 284, 339, 486, 51220, 51220, 13596, 437, 341, 820, 312, 570, 264, 1230, 295, 4959, 1633, 312, 264, 912, 293, 51436, 51436, 321, 434, 1566, 300, 341, 307, 1386, 25878, 284, 339, 486, 28446, 300, 341, 1633, 312, 8858, 420, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.09850951194763184, "compression_ratio": 1.7264573991031391, "no_speech_prob": 3.041470790776657e-06}, {"id": 321, "seek": 161732, "start": 1634.4399999999998, "end": 1638.76, "text": " infer what this should be because the number of elements must be the same and", "tokens": [50364, 472, 718, 311, 406, 764, 8858, 321, 393, 337, 1365, 360, 746, 411, 275, 5893, 3909, 412, 1958, 370, 50786, 50786, 300, 321, 500, 380, 1152, 22332, 613, 3547, 293, 341, 576, 589, 337, 604, 2744, 295, 341, 376, 50984, 50984, 420, 8535, 356, 321, 393, 611, 360, 3671, 502, 562, 321, 360, 3671, 502, 25878, 284, 339, 486, 51220, 51220, 13596, 437, 341, 820, 312, 570, 264, 1230, 295, 4959, 1633, 312, 264, 912, 293, 51436, 51436, 321, 434, 1566, 300, 341, 307, 1386, 25878, 284, 339, 486, 28446, 300, 341, 1633, 312, 8858, 420, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.09850951194763184, "compression_ratio": 1.7264573991031391, "no_speech_prob": 3.041470790776657e-06}, {"id": 322, "seek": 161732, "start": 1638.76, "end": 1642.72, "text": " we're saying that this is 6 pytorch will derive that this must be 32 or", "tokens": [50364, 472, 718, 311, 406, 764, 8858, 321, 393, 337, 1365, 360, 746, 411, 275, 5893, 3909, 412, 1958, 370, 50786, 50786, 300, 321, 500, 380, 1152, 22332, 613, 3547, 293, 341, 576, 589, 337, 604, 2744, 295, 341, 376, 50984, 50984, 420, 8535, 356, 321, 393, 611, 360, 3671, 502, 562, 321, 360, 3671, 502, 25878, 284, 339, 486, 51220, 51220, 13596, 437, 341, 820, 312, 570, 264, 1230, 295, 4959, 1633, 312, 264, 912, 293, 51436, 51436, 321, 434, 1566, 300, 341, 307, 1386, 25878, 284, 339, 486, 28446, 300, 341, 1633, 312, 8858, 420, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.09850951194763184, "compression_ratio": 1.7264573991031391, "no_speech_prob": 3.041470790776657e-06}, {"id": 323, "seek": 164272, "start": 1642.72, "end": 1650.04, "text": " whatever else it is if M is of different size the other thing is here one more", "tokens": [50364, 2035, 1646, 309, 307, 498, 376, 307, 295, 819, 2744, 264, 661, 551, 307, 510, 472, 544, 50730, 50730, 551, 286, 1116, 411, 281, 935, 484, 307, 510, 562, 321, 360, 264, 1588, 7186, 399, 341, 767, 307, 51040, 51040, 709, 1570, 7148, 570, 341, 1588, 7186, 399, 576, 1884, 257, 1379, 777, 51262, 51262, 40863, 365, 257, 1379, 777, 6725, 370, 777, 4675, 307, 885, 2942, 570, 456, 311, 51456, 51456, 572, 636, 281, 1588, 7186, 473, 10688, 830, 445, 538, 40805, 264, 1910, 17212, 370, 341, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.07379060206205948, "compression_ratio": 1.7808219178082192, "no_speech_prob": 1.2482272495617508e-06}, {"id": 324, "seek": 164272, "start": 1650.04, "end": 1656.24, "text": " thing I'd like to point out is here when we do the concatenation this actually is", "tokens": [50364, 2035, 1646, 309, 307, 498, 376, 307, 295, 819, 2744, 264, 661, 551, 307, 510, 472, 544, 50730, 50730, 551, 286, 1116, 411, 281, 935, 484, 307, 510, 562, 321, 360, 264, 1588, 7186, 399, 341, 767, 307, 51040, 51040, 709, 1570, 7148, 570, 341, 1588, 7186, 399, 576, 1884, 257, 1379, 777, 51262, 51262, 40863, 365, 257, 1379, 777, 6725, 370, 777, 4675, 307, 885, 2942, 570, 456, 311, 51456, 51456, 572, 636, 281, 1588, 7186, 473, 10688, 830, 445, 538, 40805, 264, 1910, 17212, 370, 341, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.07379060206205948, "compression_ratio": 1.7808219178082192, "no_speech_prob": 1.2482272495617508e-06}, {"id": 325, "seek": 164272, "start": 1656.24, "end": 1660.68, "text": " much less efficient because this concatenation would create a whole new", "tokens": [50364, 2035, 1646, 309, 307, 498, 376, 307, 295, 819, 2744, 264, 661, 551, 307, 510, 472, 544, 50730, 50730, 551, 286, 1116, 411, 281, 935, 484, 307, 510, 562, 321, 360, 264, 1588, 7186, 399, 341, 767, 307, 51040, 51040, 709, 1570, 7148, 570, 341, 1588, 7186, 399, 576, 1884, 257, 1379, 777, 51262, 51262, 40863, 365, 257, 1379, 777, 6725, 370, 777, 4675, 307, 885, 2942, 570, 456, 311, 51456, 51456, 572, 636, 281, 1588, 7186, 473, 10688, 830, 445, 538, 40805, 264, 1910, 17212, 370, 341, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.07379060206205948, "compression_ratio": 1.7808219178082192, "no_speech_prob": 1.2482272495617508e-06}, {"id": 326, "seek": 164272, "start": 1660.68, "end": 1664.56, "text": " tensor with a whole new storage so new memory is being created because there's", "tokens": [50364, 2035, 1646, 309, 307, 498, 376, 307, 295, 819, 2744, 264, 661, 551, 307, 510, 472, 544, 50730, 50730, 551, 286, 1116, 411, 281, 935, 484, 307, 510, 562, 321, 360, 264, 1588, 7186, 399, 341, 767, 307, 51040, 51040, 709, 1570, 7148, 570, 341, 1588, 7186, 399, 576, 1884, 257, 1379, 777, 51262, 51262, 40863, 365, 257, 1379, 777, 6725, 370, 777, 4675, 307, 885, 2942, 570, 456, 311, 51456, 51456, 572, 636, 281, 1588, 7186, 473, 10688, 830, 445, 538, 40805, 264, 1910, 17212, 370, 341, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.07379060206205948, "compression_ratio": 1.7808219178082192, "no_speech_prob": 1.2482272495617508e-06}, {"id": 327, "seek": 164272, "start": 1664.56, "end": 1668.96, "text": " no way to concatenate tensors just by manipulating the view attributes so this", "tokens": [50364, 2035, 1646, 309, 307, 498, 376, 307, 295, 819, 2744, 264, 661, 551, 307, 510, 472, 544, 50730, 50730, 551, 286, 1116, 411, 281, 935, 484, 307, 510, 562, 321, 360, 264, 1588, 7186, 399, 341, 767, 307, 51040, 51040, 709, 1570, 7148, 570, 341, 1588, 7186, 399, 576, 1884, 257, 1379, 777, 51262, 51262, 40863, 365, 257, 1379, 777, 6725, 370, 777, 4675, 307, 885, 2942, 570, 456, 311, 51456, 51456, 572, 636, 281, 1588, 7186, 473, 10688, 830, 445, 538, 40805, 264, 1910, 17212, 370, 341, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.07379060206205948, "compression_ratio": 1.7808219178082192, "no_speech_prob": 1.2482272495617508e-06}, {"id": 328, "seek": 166896, "start": 1668.96, "end": 1675.8400000000001, "text": " is inefficient and creates all kinds of new memory so let me delete this now we", "tokens": [50364, 307, 43495, 293, 7829, 439, 3685, 295, 777, 4675, 370, 718, 385, 12097, 341, 586, 321, 50708, 50708, 500, 380, 643, 341, 293, 510, 281, 8873, 389, 321, 528, 281, 611, 5893, 1266, 389, 295, 341, 281, 483, 51064, 51064, 527, 34166, 281, 483, 527, 389, 370, 613, 366, 586, 3547, 1296, 3671, 502, 293, 502, 570, 51382, 51382, 264, 1266, 389, 293, 321, 362, 300, 264, 3909, 307, 8858, 538, 2319, 293, 300, 307, 1936, 341, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.1153203116522895, "compression_ratio": 1.6153846153846154, "no_speech_prob": 6.438943273678888e-06}, {"id": 329, "seek": 166896, "start": 1675.8400000000001, "end": 1682.96, "text": " don't need this and here to calculate H we want to also dot 10 H of this to get", "tokens": [50364, 307, 43495, 293, 7829, 439, 3685, 295, 777, 4675, 370, 718, 385, 12097, 341, 586, 321, 50708, 50708, 500, 380, 643, 341, 293, 510, 281, 8873, 389, 321, 528, 281, 611, 5893, 1266, 389, 295, 341, 281, 483, 51064, 51064, 527, 34166, 281, 483, 527, 389, 370, 613, 366, 586, 3547, 1296, 3671, 502, 293, 502, 570, 51382, 51382, 264, 1266, 389, 293, 321, 362, 300, 264, 3909, 307, 8858, 538, 2319, 293, 300, 307, 1936, 341, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.1153203116522895, "compression_ratio": 1.6153846153846154, "no_speech_prob": 6.438943273678888e-06}, {"id": 330, "seek": 166896, "start": 1682.96, "end": 1689.32, "text": " our oops to get our H so these are now numbers between negative 1 and 1 because", "tokens": [50364, 307, 43495, 293, 7829, 439, 3685, 295, 777, 4675, 370, 718, 385, 12097, 341, 586, 321, 50708, 50708, 500, 380, 643, 341, 293, 510, 281, 8873, 389, 321, 528, 281, 611, 5893, 1266, 389, 295, 341, 281, 483, 51064, 51064, 527, 34166, 281, 483, 527, 389, 370, 613, 366, 586, 3547, 1296, 3671, 502, 293, 502, 570, 51382, 51382, 264, 1266, 389, 293, 321, 362, 300, 264, 3909, 307, 8858, 538, 2319, 293, 300, 307, 1936, 341, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.1153203116522895, "compression_ratio": 1.6153846153846154, "no_speech_prob": 6.438943273678888e-06}, {"id": 331, "seek": 166896, "start": 1689.32, "end": 1695.28, "text": " the 10 H and we have that the shape is 32 by 100 and that is basically this", "tokens": [50364, 307, 43495, 293, 7829, 439, 3685, 295, 777, 4675, 370, 718, 385, 12097, 341, 586, 321, 50708, 50708, 500, 380, 643, 341, 293, 510, 281, 8873, 389, 321, 528, 281, 611, 5893, 1266, 389, 295, 341, 281, 483, 51064, 51064, 527, 34166, 281, 483, 527, 389, 370, 613, 366, 586, 3547, 1296, 3671, 502, 293, 502, 570, 51382, 51382, 264, 1266, 389, 293, 321, 362, 300, 264, 3909, 307, 8858, 538, 2319, 293, 300, 307, 1936, 341, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.1153203116522895, "compression_ratio": 1.6153846153846154, "no_speech_prob": 6.438943273678888e-06}, {"id": 332, "seek": 169528, "start": 1695.28, "end": 1700.8, "text": " hidden layer of activations here for every one of our 32 examples now there's", "tokens": [50364, 7633, 4583, 295, 2430, 763, 510, 337, 633, 472, 295, 527, 8858, 5110, 586, 456, 311, 50640, 50640, 472, 544, 551, 286, 600, 2731, 670, 300, 321, 362, 281, 312, 588, 5026, 365, 293, 300, 50786, 50786, 341, 293, 300, 311, 341, 1804, 510, 294, 1729, 321, 528, 281, 652, 988, 300, 264, 50994, 50994, 30024, 486, 360, 437, 321, 411, 264, 3909, 295, 341, 307, 8858, 538, 2319, 293, 264, 472, 51286, 51286, 3909, 307, 2319, 370, 321, 536, 300, 264, 4500, 510, 486, 9975, 613, 732, 293, 294, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.07597470790781874, "compression_ratio": 1.740909090909091, "no_speech_prob": 1.5688503481214866e-05}, {"id": 333, "seek": 169528, "start": 1700.8, "end": 1703.72, "text": " one more thing I've lost over that we have to be very careful with and that", "tokens": [50364, 7633, 4583, 295, 2430, 763, 510, 337, 633, 472, 295, 527, 8858, 5110, 586, 456, 311, 50640, 50640, 472, 544, 551, 286, 600, 2731, 670, 300, 321, 362, 281, 312, 588, 5026, 365, 293, 300, 50786, 50786, 341, 293, 300, 311, 341, 1804, 510, 294, 1729, 321, 528, 281, 652, 988, 300, 264, 50994, 50994, 30024, 486, 360, 437, 321, 411, 264, 3909, 295, 341, 307, 8858, 538, 2319, 293, 264, 472, 51286, 51286, 3909, 307, 2319, 370, 321, 536, 300, 264, 4500, 510, 486, 9975, 613, 732, 293, 294, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.07597470790781874, "compression_ratio": 1.740909090909091, "no_speech_prob": 1.5688503481214866e-05}, {"id": 334, "seek": 169528, "start": 1703.72, "end": 1707.8799999999999, "text": " this and that's this plus here in particular we want to make sure that the", "tokens": [50364, 7633, 4583, 295, 2430, 763, 510, 337, 633, 472, 295, 527, 8858, 5110, 586, 456, 311, 50640, 50640, 472, 544, 551, 286, 600, 2731, 670, 300, 321, 362, 281, 312, 588, 5026, 365, 293, 300, 50786, 50786, 341, 293, 300, 311, 341, 1804, 510, 294, 1729, 321, 528, 281, 652, 988, 300, 264, 50994, 50994, 30024, 486, 360, 437, 321, 411, 264, 3909, 295, 341, 307, 8858, 538, 2319, 293, 264, 472, 51286, 51286, 3909, 307, 2319, 370, 321, 536, 300, 264, 4500, 510, 486, 9975, 613, 732, 293, 294, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.07597470790781874, "compression_ratio": 1.740909090909091, "no_speech_prob": 1.5688503481214866e-05}, {"id": 335, "seek": 169528, "start": 1707.8799999999999, "end": 1713.72, "text": " broadcasting will do what we like the shape of this is 32 by 100 and the one", "tokens": [50364, 7633, 4583, 295, 2430, 763, 510, 337, 633, 472, 295, 527, 8858, 5110, 586, 456, 311, 50640, 50640, 472, 544, 551, 286, 600, 2731, 670, 300, 321, 362, 281, 312, 588, 5026, 365, 293, 300, 50786, 50786, 341, 293, 300, 311, 341, 1804, 510, 294, 1729, 321, 528, 281, 652, 988, 300, 264, 50994, 50994, 30024, 486, 360, 437, 321, 411, 264, 3909, 295, 341, 307, 8858, 538, 2319, 293, 264, 472, 51286, 51286, 3909, 307, 2319, 370, 321, 536, 300, 264, 4500, 510, 486, 9975, 613, 732, 293, 294, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.07597470790781874, "compression_ratio": 1.740909090909091, "no_speech_prob": 1.5688503481214866e-05}, {"id": 336, "seek": 169528, "start": 1713.72, "end": 1719.6, "text": " shape is 100 so we see that the addition here will broadcast these two and in", "tokens": [50364, 7633, 4583, 295, 2430, 763, 510, 337, 633, 472, 295, 527, 8858, 5110, 586, 456, 311, 50640, 50640, 472, 544, 551, 286, 600, 2731, 670, 300, 321, 362, 281, 312, 588, 5026, 365, 293, 300, 50786, 50786, 341, 293, 300, 311, 341, 1804, 510, 294, 1729, 321, 528, 281, 652, 988, 300, 264, 50994, 50994, 30024, 486, 360, 437, 321, 411, 264, 3909, 295, 341, 307, 8858, 538, 2319, 293, 264, 472, 51286, 51286, 3909, 307, 2319, 370, 321, 536, 300, 264, 4500, 510, 486, 9975, 613, 732, 293, 294, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.07597470790781874, "compression_ratio": 1.740909090909091, "no_speech_prob": 1.5688503481214866e-05}, {"id": 337, "seek": 171960, "start": 1719.6, "end": 1725.7199999999998, "text": " particular we have 32 by 100 broadcasting to 100 so broadcasting will", "tokens": [50364, 1729, 321, 362, 8858, 538, 2319, 30024, 281, 2319, 370, 30024, 486, 50670, 50670, 7975, 322, 264, 558, 1884, 257, 7592, 10139, 510, 370, 341, 486, 1813, 257, 502, 50890, 50890, 538, 2319, 5386, 8062, 293, 550, 309, 486, 5055, 28450, 337, 633, 472, 295, 613, 13241, 51182, 51182, 295, 8858, 293, 360, 364, 4478, 12, 3711, 4500, 370, 294, 341, 1389, 264, 3006, 551, 486, 312, 51422, 51422, 2737, 570, 264, 912, 12577, 8062, 486, 312, 3869, 281, 439, 264, 13241, 295, 341, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.0699571967124939, "compression_ratio": 1.7453703703703705, "no_speech_prob": 1.834154136304278e-05}, {"id": 338, "seek": 171960, "start": 1725.7199999999998, "end": 1730.12, "text": " align on the right create a fake dimension here so this will become a 1", "tokens": [50364, 1729, 321, 362, 8858, 538, 2319, 30024, 281, 2319, 370, 30024, 486, 50670, 50670, 7975, 322, 264, 558, 1884, 257, 7592, 10139, 510, 370, 341, 486, 1813, 257, 502, 50890, 50890, 538, 2319, 5386, 8062, 293, 550, 309, 486, 5055, 28450, 337, 633, 472, 295, 613, 13241, 51182, 51182, 295, 8858, 293, 360, 364, 4478, 12, 3711, 4500, 370, 294, 341, 1389, 264, 3006, 551, 486, 312, 51422, 51422, 2737, 570, 264, 912, 12577, 8062, 486, 312, 3869, 281, 439, 264, 13241, 295, 341, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.0699571967124939, "compression_ratio": 1.7453703703703705, "no_speech_prob": 1.834154136304278e-05}, {"id": 339, "seek": 171960, "start": 1730.12, "end": 1735.9599999999998, "text": " by 100 row vector and then it will copy vertically for every one of these rows", "tokens": [50364, 1729, 321, 362, 8858, 538, 2319, 30024, 281, 2319, 370, 30024, 486, 50670, 50670, 7975, 322, 264, 558, 1884, 257, 7592, 10139, 510, 370, 341, 486, 1813, 257, 502, 50890, 50890, 538, 2319, 5386, 8062, 293, 550, 309, 486, 5055, 28450, 337, 633, 472, 295, 613, 13241, 51182, 51182, 295, 8858, 293, 360, 364, 4478, 12, 3711, 4500, 370, 294, 341, 1389, 264, 3006, 551, 486, 312, 51422, 51422, 2737, 570, 264, 912, 12577, 8062, 486, 312, 3869, 281, 439, 264, 13241, 295, 341, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.0699571967124939, "compression_ratio": 1.7453703703703705, "no_speech_prob": 1.834154136304278e-05}, {"id": 340, "seek": 171960, "start": 1735.9599999999998, "end": 1740.76, "text": " of 32 and do an element-wise addition so in this case the correct thing will be", "tokens": [50364, 1729, 321, 362, 8858, 538, 2319, 30024, 281, 2319, 370, 30024, 486, 50670, 50670, 7975, 322, 264, 558, 1884, 257, 7592, 10139, 510, 370, 341, 486, 1813, 257, 502, 50890, 50890, 538, 2319, 5386, 8062, 293, 550, 309, 486, 5055, 28450, 337, 633, 472, 295, 613, 13241, 51182, 51182, 295, 8858, 293, 360, 364, 4478, 12, 3711, 4500, 370, 294, 341, 1389, 264, 3006, 551, 486, 312, 51422, 51422, 2737, 570, 264, 912, 12577, 8062, 486, 312, 3869, 281, 439, 264, 13241, 295, 341, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.0699571967124939, "compression_ratio": 1.7453703703703705, "no_speech_prob": 1.834154136304278e-05}, {"id": 341, "seek": 171960, "start": 1740.76, "end": 1747.04, "text": " happening because the same bias vector will be added to all the rows of this", "tokens": [50364, 1729, 321, 362, 8858, 538, 2319, 30024, 281, 2319, 370, 30024, 486, 50670, 50670, 7975, 322, 264, 558, 1884, 257, 7592, 10139, 510, 370, 341, 486, 1813, 257, 502, 50890, 50890, 538, 2319, 5386, 8062, 293, 550, 309, 486, 5055, 28450, 337, 633, 472, 295, 613, 13241, 51182, 51182, 295, 8858, 293, 360, 364, 4478, 12, 3711, 4500, 370, 294, 341, 1389, 264, 3006, 551, 486, 312, 51422, 51422, 2737, 570, 264, 912, 12577, 8062, 486, 312, 3869, 281, 439, 264, 13241, 295, 341, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.0699571967124939, "compression_ratio": 1.7453703703703705, "no_speech_prob": 1.834154136304278e-05}, {"id": 342, "seek": 174704, "start": 1747.04, "end": 1751.8, "text": " matrix so that is correct that's what we'd like and it's always good practice", "tokens": [50364, 8141, 370, 300, 307, 3006, 300, 311, 437, 321, 1116, 411, 293, 309, 311, 1009, 665, 3124, 50602, 50602, 445, 652, 988, 370, 300, 291, 500, 380, 3076, 1803, 294, 264, 2671, 293, 2721, 718, 311, 50794, 50794, 1884, 264, 2572, 4583, 510, 370, 718, 311, 1884, 261, 17, 293, 272, 17, 264, 4846, 586, 307, 2319, 51232, 51232, 293, 264, 5598, 1230, 295, 22027, 486, 312, 337, 505, 7634, 570, 321, 362, 7634, 1944, 51496, 51496, 4342, 300, 808, 958, 370, 264, 32152, 486, 312, 7634, 382, 731, 370, 4412, 264, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.10389559467633565, "compression_ratio": 1.7342342342342343, "no_speech_prob": 5.17378248332534e-06}, {"id": 343, "seek": 174704, "start": 1751.8, "end": 1755.6399999999999, "text": " just make sure so that you don't shoot yourself in the foot and finally let's", "tokens": [50364, 8141, 370, 300, 307, 3006, 300, 311, 437, 321, 1116, 411, 293, 309, 311, 1009, 665, 3124, 50602, 50602, 445, 652, 988, 370, 300, 291, 500, 380, 3076, 1803, 294, 264, 2671, 293, 2721, 718, 311, 50794, 50794, 1884, 264, 2572, 4583, 510, 370, 718, 311, 1884, 261, 17, 293, 272, 17, 264, 4846, 586, 307, 2319, 51232, 51232, 293, 264, 5598, 1230, 295, 22027, 486, 312, 337, 505, 7634, 570, 321, 362, 7634, 1944, 51496, 51496, 4342, 300, 808, 958, 370, 264, 32152, 486, 312, 7634, 382, 731, 370, 4412, 264, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.10389559467633565, "compression_ratio": 1.7342342342342343, "no_speech_prob": 5.17378248332534e-06}, {"id": 344, "seek": 174704, "start": 1755.6399999999999, "end": 1764.3999999999999, "text": " create the final layer here so let's create w2 and b2 the input now is 100", "tokens": [50364, 8141, 370, 300, 307, 3006, 300, 311, 437, 321, 1116, 411, 293, 309, 311, 1009, 665, 3124, 50602, 50602, 445, 652, 988, 370, 300, 291, 500, 380, 3076, 1803, 294, 264, 2671, 293, 2721, 718, 311, 50794, 50794, 1884, 264, 2572, 4583, 510, 370, 718, 311, 1884, 261, 17, 293, 272, 17, 264, 4846, 586, 307, 2319, 51232, 51232, 293, 264, 5598, 1230, 295, 22027, 486, 312, 337, 505, 7634, 570, 321, 362, 7634, 1944, 51496, 51496, 4342, 300, 808, 958, 370, 264, 32152, 486, 312, 7634, 382, 731, 370, 4412, 264, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.10389559467633565, "compression_ratio": 1.7342342342342343, "no_speech_prob": 5.17378248332534e-06}, {"id": 345, "seek": 174704, "start": 1764.3999999999999, "end": 1769.68, "text": " and the output number of neurons will be for us 27 because we have 27 possible", "tokens": [50364, 8141, 370, 300, 307, 3006, 300, 311, 437, 321, 1116, 411, 293, 309, 311, 1009, 665, 3124, 50602, 50602, 445, 652, 988, 370, 300, 291, 500, 380, 3076, 1803, 294, 264, 2671, 293, 2721, 718, 311, 50794, 50794, 1884, 264, 2572, 4583, 510, 370, 718, 311, 1884, 261, 17, 293, 272, 17, 264, 4846, 586, 307, 2319, 51232, 51232, 293, 264, 5598, 1230, 295, 22027, 486, 312, 337, 505, 7634, 570, 321, 362, 7634, 1944, 51496, 51496, 4342, 300, 808, 958, 370, 264, 32152, 486, 312, 7634, 382, 731, 370, 4412, 264, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.10389559467633565, "compression_ratio": 1.7342342342342343, "no_speech_prob": 5.17378248332534e-06}, {"id": 346, "seek": 174704, "start": 1769.68, "end": 1775.68, "text": " characters that come next so the biases will be 27 as well so therefore the", "tokens": [50364, 8141, 370, 300, 307, 3006, 300, 311, 437, 321, 1116, 411, 293, 309, 311, 1009, 665, 3124, 50602, 50602, 445, 652, 988, 370, 300, 291, 500, 380, 3076, 1803, 294, 264, 2671, 293, 2721, 718, 311, 50794, 50794, 1884, 264, 2572, 4583, 510, 370, 718, 311, 1884, 261, 17, 293, 272, 17, 264, 4846, 586, 307, 2319, 51232, 51232, 293, 264, 5598, 1230, 295, 22027, 486, 312, 337, 505, 7634, 570, 321, 362, 7634, 1944, 51496, 51496, 4342, 300, 808, 958, 370, 264, 32152, 486, 312, 7634, 382, 731, 370, 4412, 264, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.10389559467633565, "compression_ratio": 1.7342342342342343, "no_speech_prob": 5.17378248332534e-06}, {"id": 347, "seek": 177568, "start": 1775.68, "end": 1783.0800000000002, "text": " logits which are the outputs of this neural net are going to be H multiplied", "tokens": [50364, 3565, 1208, 597, 366, 264, 23930, 295, 341, 18161, 2533, 366, 516, 281, 312, 389, 17207, 50734, 50734, 538, 261, 17, 1804, 272, 17, 3565, 271, 300, 3909, 307, 8858, 538, 7634, 293, 264, 3565, 1208, 574, 665, 586, 2293, 51270, 51270, 382, 321, 1866, 294, 264, 3894, 960, 321, 528, 281, 747, 613, 3565, 1208, 293, 321, 528, 281, 51424, 51424, 700, 37871, 13024, 552, 281, 483, 527, 7592, 14893, 293, 550, 321, 528, 281, 2710, 1125, 51642, 51642], "temperature": 0.0, "avg_logprob": -0.1390342363497106, "compression_ratio": 1.617801047120419, "no_speech_prob": 1.6700641936040483e-05}, {"id": 348, "seek": 177568, "start": 1783.0800000000002, "end": 1793.8, "text": " by w2 plus b2 logis that shape is 32 by 27 and the logits look good now exactly", "tokens": [50364, 3565, 1208, 597, 366, 264, 23930, 295, 341, 18161, 2533, 366, 516, 281, 312, 389, 17207, 50734, 50734, 538, 261, 17, 1804, 272, 17, 3565, 271, 300, 3909, 307, 8858, 538, 7634, 293, 264, 3565, 1208, 574, 665, 586, 2293, 51270, 51270, 382, 321, 1866, 294, 264, 3894, 960, 321, 528, 281, 747, 613, 3565, 1208, 293, 321, 528, 281, 51424, 51424, 700, 37871, 13024, 552, 281, 483, 527, 7592, 14893, 293, 550, 321, 528, 281, 2710, 1125, 51642, 51642], "temperature": 0.0, "avg_logprob": -0.1390342363497106, "compression_ratio": 1.617801047120419, "no_speech_prob": 1.6700641936040483e-05}, {"id": 349, "seek": 177568, "start": 1793.8, "end": 1796.88, "text": " as we saw in the previous video we want to take these logits and we want to", "tokens": [50364, 3565, 1208, 597, 366, 264, 23930, 295, 341, 18161, 2533, 366, 516, 281, 312, 389, 17207, 50734, 50734, 538, 261, 17, 1804, 272, 17, 3565, 271, 300, 3909, 307, 8858, 538, 7634, 293, 264, 3565, 1208, 574, 665, 586, 2293, 51270, 51270, 382, 321, 1866, 294, 264, 3894, 960, 321, 528, 281, 747, 613, 3565, 1208, 293, 321, 528, 281, 51424, 51424, 700, 37871, 13024, 552, 281, 483, 527, 7592, 14893, 293, 550, 321, 528, 281, 2710, 1125, 51642, 51642], "temperature": 0.0, "avg_logprob": -0.1390342363497106, "compression_ratio": 1.617801047120419, "no_speech_prob": 1.6700641936040483e-05}, {"id": 350, "seek": 177568, "start": 1796.88, "end": 1801.24, "text": " first exponentiate them to get our fake counts and then we want to normalize", "tokens": [50364, 3565, 1208, 597, 366, 264, 23930, 295, 341, 18161, 2533, 366, 516, 281, 312, 389, 17207, 50734, 50734, 538, 261, 17, 1804, 272, 17, 3565, 271, 300, 3909, 307, 8858, 538, 7634, 293, 264, 3565, 1208, 574, 665, 586, 2293, 51270, 51270, 382, 321, 1866, 294, 264, 3894, 960, 321, 528, 281, 747, 613, 3565, 1208, 293, 321, 528, 281, 51424, 51424, 700, 37871, 13024, 552, 281, 483, 527, 7592, 14893, 293, 550, 321, 528, 281, 2710, 1125, 51642, 51642], "temperature": 0.0, "avg_logprob": -0.1390342363497106, "compression_ratio": 1.617801047120419, "no_speech_prob": 1.6700641936040483e-05}, {"id": 351, "seek": 180124, "start": 1801.24, "end": 1808.64, "text": " them into a probability so prob is counts divide and now counts dot sum", "tokens": [50364, 552, 666, 257, 8482, 370, 1239, 307, 14893, 9845, 293, 586, 14893, 5893, 2408, 50734, 50734, 2051, 264, 700, 10139, 293, 1066, 552, 382, 2074, 2293, 382, 294, 264, 3894, 960, 50964, 50964, 293, 370, 1239, 300, 3909, 586, 307, 8858, 538, 7634, 293, 291, 603, 536, 300, 633, 5386, 295, 1239, 51450, 51450, 34499, 281, 472, 370, 309, 311, 48704, 370, 300, 2709, 505, 264, 33783, 586, 295, 1164, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.11349357141030801, "compression_ratio": 1.6436170212765957, "no_speech_prob": 1.028931546898093e-05}, {"id": 352, "seek": 180124, "start": 1808.64, "end": 1813.24, "text": " along the first dimension and keep them as true exactly as in the previous video", "tokens": [50364, 552, 666, 257, 8482, 370, 1239, 307, 14893, 9845, 293, 586, 14893, 5893, 2408, 50734, 50734, 2051, 264, 700, 10139, 293, 1066, 552, 382, 2074, 2293, 382, 294, 264, 3894, 960, 50964, 50964, 293, 370, 1239, 300, 3909, 586, 307, 8858, 538, 7634, 293, 291, 603, 536, 300, 633, 5386, 295, 1239, 51450, 51450, 34499, 281, 472, 370, 309, 311, 48704, 370, 300, 2709, 505, 264, 33783, 586, 295, 1164, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.11349357141030801, "compression_ratio": 1.6436170212765957, "no_speech_prob": 1.028931546898093e-05}, {"id": 353, "seek": 180124, "start": 1813.24, "end": 1822.96, "text": " and so prob that shape now is 32 by 27 and you'll see that every row of prob", "tokens": [50364, 552, 666, 257, 8482, 370, 1239, 307, 14893, 9845, 293, 586, 14893, 5893, 2408, 50734, 50734, 2051, 264, 700, 10139, 293, 1066, 552, 382, 2074, 2293, 382, 294, 264, 3894, 960, 50964, 50964, 293, 370, 1239, 300, 3909, 586, 307, 8858, 538, 7634, 293, 291, 603, 536, 300, 633, 5386, 295, 1239, 51450, 51450, 34499, 281, 472, 370, 309, 311, 48704, 370, 300, 2709, 505, 264, 33783, 586, 295, 1164, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.11349357141030801, "compression_ratio": 1.6436170212765957, "no_speech_prob": 1.028931546898093e-05}, {"id": 354, "seek": 180124, "start": 1822.96, "end": 1829.08, "text": " sums to one so it's normalized so that gives us the probabilities now of course", "tokens": [50364, 552, 666, 257, 8482, 370, 1239, 307, 14893, 9845, 293, 586, 14893, 5893, 2408, 50734, 50734, 2051, 264, 700, 10139, 293, 1066, 552, 382, 2074, 2293, 382, 294, 264, 3894, 960, 50964, 50964, 293, 370, 1239, 300, 3909, 586, 307, 8858, 538, 7634, 293, 291, 603, 536, 300, 633, 5386, 295, 1239, 51450, 51450, 34499, 281, 472, 370, 309, 311, 48704, 370, 300, 2709, 505, 264, 33783, 586, 295, 1164, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.11349357141030801, "compression_ratio": 1.6436170212765957, "no_speech_prob": 1.028931546898093e-05}, {"id": 355, "seek": 182908, "start": 1829.08, "end": 1833.6, "text": " we have the actual letter that comes next and that comes from this array y", "tokens": [50364, 321, 362, 264, 3539, 5063, 300, 1487, 958, 293, 300, 1487, 490, 341, 10225, 288, 50590, 50590, 597, 321, 2942, 1830, 264, 1412, 992, 8016, 370, 288, 307, 341, 1036, 50856, 50856, 2522, 510, 597, 307, 264, 6575, 295, 264, 958, 2517, 294, 257, 8310, 300, 321, 1116, 51036, 51036, 411, 281, 586, 6069, 370, 437, 321, 1116, 411, 281, 360, 586, 307, 445, 382, 294, 264, 3894, 960, 51280, 51280, 321, 1116, 411, 281, 8186, 666, 264, 13241, 295, 1239, 293, 1184, 5386, 321, 1116, 411, 281, 41514, 484, 264, 51540, 51540, 8482, 13279, 281, 264, 3006, 2517, 382, 2212, 510, 370, 700, 321, 362, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.11261365196921609, "compression_ratio": 1.876543209876543, "no_speech_prob": 7.5277375799487345e-06}, {"id": 356, "seek": 182908, "start": 1833.6, "end": 1838.9199999999998, "text": " which we created during the data set creation so y is this last", "tokens": [50364, 321, 362, 264, 3539, 5063, 300, 1487, 958, 293, 300, 1487, 490, 341, 10225, 288, 50590, 50590, 597, 321, 2942, 1830, 264, 1412, 992, 8016, 370, 288, 307, 341, 1036, 50856, 50856, 2522, 510, 597, 307, 264, 6575, 295, 264, 958, 2517, 294, 257, 8310, 300, 321, 1116, 51036, 51036, 411, 281, 586, 6069, 370, 437, 321, 1116, 411, 281, 360, 586, 307, 445, 382, 294, 264, 3894, 960, 51280, 51280, 321, 1116, 411, 281, 8186, 666, 264, 13241, 295, 1239, 293, 1184, 5386, 321, 1116, 411, 281, 41514, 484, 264, 51540, 51540, 8482, 13279, 281, 264, 3006, 2517, 382, 2212, 510, 370, 700, 321, 362, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.11261365196921609, "compression_ratio": 1.876543209876543, "no_speech_prob": 7.5277375799487345e-06}, {"id": 357, "seek": 182908, "start": 1838.9199999999998, "end": 1842.52, "text": " piece here which is the identity of the next character in a sequence that we'd", "tokens": [50364, 321, 362, 264, 3539, 5063, 300, 1487, 958, 293, 300, 1487, 490, 341, 10225, 288, 50590, 50590, 597, 321, 2942, 1830, 264, 1412, 992, 8016, 370, 288, 307, 341, 1036, 50856, 50856, 2522, 510, 597, 307, 264, 6575, 295, 264, 958, 2517, 294, 257, 8310, 300, 321, 1116, 51036, 51036, 411, 281, 586, 6069, 370, 437, 321, 1116, 411, 281, 360, 586, 307, 445, 382, 294, 264, 3894, 960, 51280, 51280, 321, 1116, 411, 281, 8186, 666, 264, 13241, 295, 1239, 293, 1184, 5386, 321, 1116, 411, 281, 41514, 484, 264, 51540, 51540, 8482, 13279, 281, 264, 3006, 2517, 382, 2212, 510, 370, 700, 321, 362, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.11261365196921609, "compression_ratio": 1.876543209876543, "no_speech_prob": 7.5277375799487345e-06}, {"id": 358, "seek": 182908, "start": 1842.52, "end": 1847.3999999999999, "text": " like to now predict so what we'd like to do now is just as in the previous video", "tokens": [50364, 321, 362, 264, 3539, 5063, 300, 1487, 958, 293, 300, 1487, 490, 341, 10225, 288, 50590, 50590, 597, 321, 2942, 1830, 264, 1412, 992, 8016, 370, 288, 307, 341, 1036, 50856, 50856, 2522, 510, 597, 307, 264, 6575, 295, 264, 958, 2517, 294, 257, 8310, 300, 321, 1116, 51036, 51036, 411, 281, 586, 6069, 370, 437, 321, 1116, 411, 281, 360, 586, 307, 445, 382, 294, 264, 3894, 960, 51280, 51280, 321, 1116, 411, 281, 8186, 666, 264, 13241, 295, 1239, 293, 1184, 5386, 321, 1116, 411, 281, 41514, 484, 264, 51540, 51540, 8482, 13279, 281, 264, 3006, 2517, 382, 2212, 510, 370, 700, 321, 362, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.11261365196921609, "compression_ratio": 1.876543209876543, "no_speech_prob": 7.5277375799487345e-06}, {"id": 359, "seek": 182908, "start": 1847.3999999999999, "end": 1852.6, "text": " we'd like to index into the rows of prob and each row we'd like to pluck out the", "tokens": [50364, 321, 362, 264, 3539, 5063, 300, 1487, 958, 293, 300, 1487, 490, 341, 10225, 288, 50590, 50590, 597, 321, 2942, 1830, 264, 1412, 992, 8016, 370, 288, 307, 341, 1036, 50856, 50856, 2522, 510, 597, 307, 264, 6575, 295, 264, 958, 2517, 294, 257, 8310, 300, 321, 1116, 51036, 51036, 411, 281, 586, 6069, 370, 437, 321, 1116, 411, 281, 360, 586, 307, 445, 382, 294, 264, 3894, 960, 51280, 51280, 321, 1116, 411, 281, 8186, 666, 264, 13241, 295, 1239, 293, 1184, 5386, 321, 1116, 411, 281, 41514, 484, 264, 51540, 51540, 8482, 13279, 281, 264, 3006, 2517, 382, 2212, 510, 370, 700, 321, 362, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.11261365196921609, "compression_ratio": 1.876543209876543, "no_speech_prob": 7.5277375799487345e-06}, {"id": 360, "seek": 182908, "start": 1852.6, "end": 1858.24, "text": " probability assigned to the correct character as given here so first we have", "tokens": [50364, 321, 362, 264, 3539, 5063, 300, 1487, 958, 293, 300, 1487, 490, 341, 10225, 288, 50590, 50590, 597, 321, 2942, 1830, 264, 1412, 992, 8016, 370, 288, 307, 341, 1036, 50856, 50856, 2522, 510, 597, 307, 264, 6575, 295, 264, 958, 2517, 294, 257, 8310, 300, 321, 1116, 51036, 51036, 411, 281, 586, 6069, 370, 437, 321, 1116, 411, 281, 360, 586, 307, 445, 382, 294, 264, 3894, 960, 51280, 51280, 321, 1116, 411, 281, 8186, 666, 264, 13241, 295, 1239, 293, 1184, 5386, 321, 1116, 411, 281, 41514, 484, 264, 51540, 51540, 8482, 13279, 281, 264, 3006, 2517, 382, 2212, 510, 370, 700, 321, 362, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.11261365196921609, "compression_ratio": 1.876543209876543, "no_speech_prob": 7.5277375799487345e-06}, {"id": 361, "seek": 185824, "start": 1858.24, "end": 1864.48, "text": " torched out a range of 32 which is kind of like an iterator over numbers from 0", "tokens": [50364, 3930, 19318, 484, 257, 3613, 295, 8858, 597, 307, 733, 295, 411, 364, 17138, 1639, 670, 3547, 490, 1958, 50676, 50676, 281, 10353, 293, 550, 321, 393, 8186, 666, 1239, 294, 264, 3480, 636, 1239, 294, 27822, 5893, 257, 51000, 51000, 3613, 295, 8858, 597, 17138, 1024, 264, 11344, 293, 550, 294, 1184, 5386, 321, 1116, 411, 281, 4444, 51224, 51224, 341, 7738, 382, 2212, 538, 288, 370, 341, 2709, 264, 2190, 33783, 382, 13279, 538, 51554, 51554, 341, 18161, 3209, 365, 341, 3287, 295, 1080, 17443, 281, 264, 3006, 2517, 294, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.12075530489285786, "compression_ratio": 1.7444933920704846, "no_speech_prob": 1.4738387108081952e-05}, {"id": 362, "seek": 185824, "start": 1864.48, "end": 1870.96, "text": " to 31 and then we can index into prob in the following way prob in torch dot a", "tokens": [50364, 3930, 19318, 484, 257, 3613, 295, 8858, 597, 307, 733, 295, 411, 364, 17138, 1639, 670, 3547, 490, 1958, 50676, 50676, 281, 10353, 293, 550, 321, 393, 8186, 666, 1239, 294, 264, 3480, 636, 1239, 294, 27822, 5893, 257, 51000, 51000, 3613, 295, 8858, 597, 17138, 1024, 264, 11344, 293, 550, 294, 1184, 5386, 321, 1116, 411, 281, 4444, 51224, 51224, 341, 7738, 382, 2212, 538, 288, 370, 341, 2709, 264, 2190, 33783, 382, 13279, 538, 51554, 51554, 341, 18161, 3209, 365, 341, 3287, 295, 1080, 17443, 281, 264, 3006, 2517, 294, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.12075530489285786, "compression_ratio": 1.7444933920704846, "no_speech_prob": 1.4738387108081952e-05}, {"id": 363, "seek": 185824, "start": 1870.96, "end": 1875.44, "text": " range of 32 which iterates the roads and then in each row we'd like to grab", "tokens": [50364, 3930, 19318, 484, 257, 3613, 295, 8858, 597, 307, 733, 295, 411, 364, 17138, 1639, 670, 3547, 490, 1958, 50676, 50676, 281, 10353, 293, 550, 321, 393, 8186, 666, 1239, 294, 264, 3480, 636, 1239, 294, 27822, 5893, 257, 51000, 51000, 3613, 295, 8858, 597, 17138, 1024, 264, 11344, 293, 550, 294, 1184, 5386, 321, 1116, 411, 281, 4444, 51224, 51224, 341, 7738, 382, 2212, 538, 288, 370, 341, 2709, 264, 2190, 33783, 382, 13279, 538, 51554, 51554, 341, 18161, 3209, 365, 341, 3287, 295, 1080, 17443, 281, 264, 3006, 2517, 294, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.12075530489285786, "compression_ratio": 1.7444933920704846, "no_speech_prob": 1.4738387108081952e-05}, {"id": 364, "seek": 185824, "start": 1875.44, "end": 1882.04, "text": " this column as given by y so this gives the current probabilities as assigned by", "tokens": [50364, 3930, 19318, 484, 257, 3613, 295, 8858, 597, 307, 733, 295, 411, 364, 17138, 1639, 670, 3547, 490, 1958, 50676, 50676, 281, 10353, 293, 550, 321, 393, 8186, 666, 1239, 294, 264, 3480, 636, 1239, 294, 27822, 5893, 257, 51000, 51000, 3613, 295, 8858, 597, 17138, 1024, 264, 11344, 293, 550, 294, 1184, 5386, 321, 1116, 411, 281, 4444, 51224, 51224, 341, 7738, 382, 2212, 538, 288, 370, 341, 2709, 264, 2190, 33783, 382, 13279, 538, 51554, 51554, 341, 18161, 3209, 365, 341, 3287, 295, 1080, 17443, 281, 264, 3006, 2517, 294, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.12075530489285786, "compression_ratio": 1.7444933920704846, "no_speech_prob": 1.4738387108081952e-05}, {"id": 365, "seek": 185824, "start": 1882.04, "end": 1886.64, "text": " this neural network with this setting of its weights to the correct character in", "tokens": [50364, 3930, 19318, 484, 257, 3613, 295, 8858, 597, 307, 733, 295, 411, 364, 17138, 1639, 670, 3547, 490, 1958, 50676, 50676, 281, 10353, 293, 550, 321, 393, 8186, 666, 1239, 294, 264, 3480, 636, 1239, 294, 27822, 5893, 257, 51000, 51000, 3613, 295, 8858, 597, 17138, 1024, 264, 11344, 293, 550, 294, 1184, 5386, 321, 1116, 411, 281, 4444, 51224, 51224, 341, 7738, 382, 2212, 538, 288, 370, 341, 2709, 264, 2190, 33783, 382, 13279, 538, 51554, 51554, 341, 18161, 3209, 365, 341, 3287, 295, 1080, 17443, 281, 264, 3006, 2517, 294, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.12075530489285786, "compression_ratio": 1.7444933920704846, "no_speech_prob": 1.4738387108081952e-05}, {"id": 366, "seek": 188664, "start": 1886.64, "end": 1890.44, "text": " the sequence and you can see here that this looks okay for some of these", "tokens": [50364, 264, 8310, 293, 291, 393, 536, 510, 300, 341, 1542, 1392, 337, 512, 295, 613, 50554, 50554, 4342, 411, 341, 307, 1936, 1958, 13, 17, 457, 309, 1177, 380, 574, 588, 665, 412, 439, 337, 50764, 50764, 867, 661, 4342, 411, 341, 307, 1958, 13, 16231, 1958, 82, 502, 8482, 293, 370, 264, 3209, 51066, 51066, 7309, 300, 512, 295, 613, 366, 4664, 17518, 457, 295, 1164, 321, 2378, 380, 8895, 51236, 51236, 257, 18161, 3209, 1939, 370, 341, 486, 3470, 293, 22915, 439, 295, 613, 3547, 51512, 51512, 510, 295, 1164, 366, 502, 570, 550, 321, 366, 8944, 32884, 264, 958, 2517, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.12988159144036124, "compression_ratio": 1.7884615384615385, "no_speech_prob": 1.9222481569158845e-05}, {"id": 367, "seek": 188664, "start": 1890.44, "end": 1894.64, "text": " characters like this is basically 0.2 but it doesn't look very good at all for", "tokens": [50364, 264, 8310, 293, 291, 393, 536, 510, 300, 341, 1542, 1392, 337, 512, 295, 613, 50554, 50554, 4342, 411, 341, 307, 1936, 1958, 13, 17, 457, 309, 1177, 380, 574, 588, 665, 412, 439, 337, 50764, 50764, 867, 661, 4342, 411, 341, 307, 1958, 13, 16231, 1958, 82, 502, 8482, 293, 370, 264, 3209, 51066, 51066, 7309, 300, 512, 295, 613, 366, 4664, 17518, 457, 295, 1164, 321, 2378, 380, 8895, 51236, 51236, 257, 18161, 3209, 1939, 370, 341, 486, 3470, 293, 22915, 439, 295, 613, 3547, 51512, 51512, 510, 295, 1164, 366, 502, 570, 550, 321, 366, 8944, 32884, 264, 958, 2517, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.12988159144036124, "compression_ratio": 1.7884615384615385, "no_speech_prob": 1.9222481569158845e-05}, {"id": 368, "seek": 188664, "start": 1894.64, "end": 1900.68, "text": " many other characters like this is 0.07 0s 1 probability and so the network", "tokens": [50364, 264, 8310, 293, 291, 393, 536, 510, 300, 341, 1542, 1392, 337, 512, 295, 613, 50554, 50554, 4342, 411, 341, 307, 1936, 1958, 13, 17, 457, 309, 1177, 380, 574, 588, 665, 412, 439, 337, 50764, 50764, 867, 661, 4342, 411, 341, 307, 1958, 13, 16231, 1958, 82, 502, 8482, 293, 370, 264, 3209, 51066, 51066, 7309, 300, 512, 295, 613, 366, 4664, 17518, 457, 295, 1164, 321, 2378, 380, 8895, 51236, 51236, 257, 18161, 3209, 1939, 370, 341, 486, 3470, 293, 22915, 439, 295, 613, 3547, 51512, 51512, 510, 295, 1164, 366, 502, 570, 550, 321, 366, 8944, 32884, 264, 958, 2517, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.12988159144036124, "compression_ratio": 1.7884615384615385, "no_speech_prob": 1.9222481569158845e-05}, {"id": 369, "seek": 188664, "start": 1900.68, "end": 1904.0800000000002, "text": " thinks that some of these are extremely unlikely but of course we haven't trained", "tokens": [50364, 264, 8310, 293, 291, 393, 536, 510, 300, 341, 1542, 1392, 337, 512, 295, 613, 50554, 50554, 4342, 411, 341, 307, 1936, 1958, 13, 17, 457, 309, 1177, 380, 574, 588, 665, 412, 439, 337, 50764, 50764, 867, 661, 4342, 411, 341, 307, 1958, 13, 16231, 1958, 82, 502, 8482, 293, 370, 264, 3209, 51066, 51066, 7309, 300, 512, 295, 613, 366, 4664, 17518, 457, 295, 1164, 321, 2378, 380, 8895, 51236, 51236, 257, 18161, 3209, 1939, 370, 341, 486, 3470, 293, 22915, 439, 295, 613, 3547, 51512, 51512, 510, 295, 1164, 366, 502, 570, 550, 321, 366, 8944, 32884, 264, 958, 2517, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.12988159144036124, "compression_ratio": 1.7884615384615385, "no_speech_prob": 1.9222481569158845e-05}, {"id": 370, "seek": 188664, "start": 1904.0800000000002, "end": 1909.6000000000001, "text": " a neural network yet so this will improve and ideally all of these numbers", "tokens": [50364, 264, 8310, 293, 291, 393, 536, 510, 300, 341, 1542, 1392, 337, 512, 295, 613, 50554, 50554, 4342, 411, 341, 307, 1936, 1958, 13, 17, 457, 309, 1177, 380, 574, 588, 665, 412, 439, 337, 50764, 50764, 867, 661, 4342, 411, 341, 307, 1958, 13, 16231, 1958, 82, 502, 8482, 293, 370, 264, 3209, 51066, 51066, 7309, 300, 512, 295, 613, 366, 4664, 17518, 457, 295, 1164, 321, 2378, 380, 8895, 51236, 51236, 257, 18161, 3209, 1939, 370, 341, 486, 3470, 293, 22915, 439, 295, 613, 3547, 51512, 51512, 510, 295, 1164, 366, 502, 570, 550, 321, 366, 8944, 32884, 264, 958, 2517, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.12988159144036124, "compression_ratio": 1.7884615384615385, "no_speech_prob": 1.9222481569158845e-05}, {"id": 371, "seek": 188664, "start": 1909.6000000000001, "end": 1913.16, "text": " here of course are 1 because then we are correctly predicting the next character", "tokens": [50364, 264, 8310, 293, 291, 393, 536, 510, 300, 341, 1542, 1392, 337, 512, 295, 613, 50554, 50554, 4342, 411, 341, 307, 1936, 1958, 13, 17, 457, 309, 1177, 380, 574, 588, 665, 412, 439, 337, 50764, 50764, 867, 661, 4342, 411, 341, 307, 1958, 13, 16231, 1958, 82, 502, 8482, 293, 370, 264, 3209, 51066, 51066, 7309, 300, 512, 295, 613, 366, 4664, 17518, 457, 295, 1164, 321, 2378, 380, 8895, 51236, 51236, 257, 18161, 3209, 1939, 370, 341, 486, 3470, 293, 22915, 439, 295, 613, 3547, 51512, 51512, 510, 295, 1164, 366, 502, 570, 550, 321, 366, 8944, 32884, 264, 958, 2517, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.12988159144036124, "compression_ratio": 1.7884615384615385, "no_speech_prob": 1.9222481569158845e-05}, {"id": 372, "seek": 191316, "start": 1913.16, "end": 1917.64, "text": " now just as in the previous video we want to take these probabilities we want", "tokens": [50364, 586, 445, 382, 294, 264, 3894, 960, 321, 528, 281, 747, 613, 33783, 321, 528, 50588, 50588, 281, 574, 412, 264, 4017, 8482, 293, 550, 321, 528, 281, 574, 412, 264, 4274, 4017, 50762, 50762, 8482, 293, 264, 3671, 295, 309, 281, 1884, 264, 3671, 3565, 22119, 4470, 51046, 51046, 370, 264, 4470, 510, 307, 3282, 293, 341, 307, 264, 4470, 300, 321, 1116, 411, 281, 17522, 281, 483, 51358, 51358, 264, 3209, 281, 6069, 264, 3006, 2517, 294, 264, 8310, 1392, 370, 286, 51568, 51568, 319, 7449, 1370, 1203, 510, 293, 1027, 309, 257, 857, 544, 44279, 370, 510, 311, 527, 1412, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.08773509587082907, "compression_ratio": 1.9125, "no_speech_prob": 7.411108072119532e-06}, {"id": 373, "seek": 191316, "start": 1917.64, "end": 1921.1200000000001, "text": " to look at the lock probability and then we want to look at the average lock", "tokens": [50364, 586, 445, 382, 294, 264, 3894, 960, 321, 528, 281, 747, 613, 33783, 321, 528, 50588, 50588, 281, 574, 412, 264, 4017, 8482, 293, 550, 321, 528, 281, 574, 412, 264, 4274, 4017, 50762, 50762, 8482, 293, 264, 3671, 295, 309, 281, 1884, 264, 3671, 3565, 22119, 4470, 51046, 51046, 370, 264, 4470, 510, 307, 3282, 293, 341, 307, 264, 4470, 300, 321, 1116, 411, 281, 17522, 281, 483, 51358, 51358, 264, 3209, 281, 6069, 264, 3006, 2517, 294, 264, 8310, 1392, 370, 286, 51568, 51568, 319, 7449, 1370, 1203, 510, 293, 1027, 309, 257, 857, 544, 44279, 370, 510, 311, 527, 1412, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.08773509587082907, "compression_ratio": 1.9125, "no_speech_prob": 7.411108072119532e-06}, {"id": 374, "seek": 191316, "start": 1921.1200000000001, "end": 1926.8000000000002, "text": " probability and the negative of it to create the negative log likelihood loss", "tokens": [50364, 586, 445, 382, 294, 264, 3894, 960, 321, 528, 281, 747, 613, 33783, 321, 528, 50588, 50588, 281, 574, 412, 264, 4017, 8482, 293, 550, 321, 528, 281, 574, 412, 264, 4274, 4017, 50762, 50762, 8482, 293, 264, 3671, 295, 309, 281, 1884, 264, 3671, 3565, 22119, 4470, 51046, 51046, 370, 264, 4470, 510, 307, 3282, 293, 341, 307, 264, 4470, 300, 321, 1116, 411, 281, 17522, 281, 483, 51358, 51358, 264, 3209, 281, 6069, 264, 3006, 2517, 294, 264, 8310, 1392, 370, 286, 51568, 51568, 319, 7449, 1370, 1203, 510, 293, 1027, 309, 257, 857, 544, 44279, 370, 510, 311, 527, 1412, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.08773509587082907, "compression_ratio": 1.9125, "no_speech_prob": 7.411108072119532e-06}, {"id": 375, "seek": 191316, "start": 1926.8000000000002, "end": 1933.0400000000002, "text": " so the loss here is 17 and this is the loss that we'd like to minimize to get", "tokens": [50364, 586, 445, 382, 294, 264, 3894, 960, 321, 528, 281, 747, 613, 33783, 321, 528, 50588, 50588, 281, 574, 412, 264, 4017, 8482, 293, 550, 321, 528, 281, 574, 412, 264, 4274, 4017, 50762, 50762, 8482, 293, 264, 3671, 295, 309, 281, 1884, 264, 3671, 3565, 22119, 4470, 51046, 51046, 370, 264, 4470, 510, 307, 3282, 293, 341, 307, 264, 4470, 300, 321, 1116, 411, 281, 17522, 281, 483, 51358, 51358, 264, 3209, 281, 6069, 264, 3006, 2517, 294, 264, 8310, 1392, 370, 286, 51568, 51568, 319, 7449, 1370, 1203, 510, 293, 1027, 309, 257, 857, 544, 44279, 370, 510, 311, 527, 1412, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.08773509587082907, "compression_ratio": 1.9125, "no_speech_prob": 7.411108072119532e-06}, {"id": 376, "seek": 191316, "start": 1933.0400000000002, "end": 1937.24, "text": " the network to predict the correct character in the sequence okay so I", "tokens": [50364, 586, 445, 382, 294, 264, 3894, 960, 321, 528, 281, 747, 613, 33783, 321, 528, 50588, 50588, 281, 574, 412, 264, 4017, 8482, 293, 550, 321, 528, 281, 574, 412, 264, 4274, 4017, 50762, 50762, 8482, 293, 264, 3671, 295, 309, 281, 1884, 264, 3671, 3565, 22119, 4470, 51046, 51046, 370, 264, 4470, 510, 307, 3282, 293, 341, 307, 264, 4470, 300, 321, 1116, 411, 281, 17522, 281, 483, 51358, 51358, 264, 3209, 281, 6069, 264, 3006, 2517, 294, 264, 8310, 1392, 370, 286, 51568, 51568, 319, 7449, 1370, 1203, 510, 293, 1027, 309, 257, 857, 544, 44279, 370, 510, 311, 527, 1412, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.08773509587082907, "compression_ratio": 1.9125, "no_speech_prob": 7.411108072119532e-06}, {"id": 377, "seek": 191316, "start": 1937.24, "end": 1941.28, "text": " rewrote everything here and made it a bit more respectable so here's our data", "tokens": [50364, 586, 445, 382, 294, 264, 3894, 960, 321, 528, 281, 747, 613, 33783, 321, 528, 50588, 50588, 281, 574, 412, 264, 4017, 8482, 293, 550, 321, 528, 281, 574, 412, 264, 4274, 4017, 50762, 50762, 8482, 293, 264, 3671, 295, 309, 281, 1884, 264, 3671, 3565, 22119, 4470, 51046, 51046, 370, 264, 4470, 510, 307, 3282, 293, 341, 307, 264, 4470, 300, 321, 1116, 411, 281, 17522, 281, 483, 51358, 51358, 264, 3209, 281, 6069, 264, 3006, 2517, 294, 264, 8310, 1392, 370, 286, 51568, 51568, 319, 7449, 1370, 1203, 510, 293, 1027, 309, 257, 857, 544, 44279, 370, 510, 311, 527, 1412, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.08773509587082907, "compression_ratio": 1.9125, "no_speech_prob": 7.411108072119532e-06}, {"id": 378, "seek": 194128, "start": 1941.28, "end": 1946.44, "text": " set here's all the parameters that we defined I'm now using a generator to make", "tokens": [50364, 992, 510, 311, 439, 264, 9834, 300, 321, 7642, 286, 478, 586, 1228, 257, 19265, 281, 652, 50622, 50622, 309, 11408, 32128, 286, 596, 38624, 439, 264, 9834, 666, 257, 2167, 1329, 295, 50812, 50812, 9834, 370, 300, 337, 1365, 309, 311, 1858, 281, 1207, 552, 293, 536, 300, 294, 3217, 321, 51014, 51014, 4362, 362, 466, 805, 11, 13741, 9834, 293, 341, 307, 264, 2128, 1320, 382, 321, 51226, 51226, 4743, 309, 293, 321, 8881, 412, 257, 2167, 1230, 510, 264, 4470, 300, 307, 4362, 51474, 51474, 22171, 577, 731, 341, 18161, 3209, 1985, 365, 264, 2190, 3287, 295, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.08252111765054557, "compression_ratio": 1.790513833992095, "no_speech_prob": 2.5866174837574363e-05}, {"id": 379, "seek": 194128, "start": 1946.44, "end": 1950.24, "text": " it reproducible I clustered all the parameters into a single list of", "tokens": [50364, 992, 510, 311, 439, 264, 9834, 300, 321, 7642, 286, 478, 586, 1228, 257, 19265, 281, 652, 50622, 50622, 309, 11408, 32128, 286, 596, 38624, 439, 264, 9834, 666, 257, 2167, 1329, 295, 50812, 50812, 9834, 370, 300, 337, 1365, 309, 311, 1858, 281, 1207, 552, 293, 536, 300, 294, 3217, 321, 51014, 51014, 4362, 362, 466, 805, 11, 13741, 9834, 293, 341, 307, 264, 2128, 1320, 382, 321, 51226, 51226, 4743, 309, 293, 321, 8881, 412, 257, 2167, 1230, 510, 264, 4470, 300, 307, 4362, 51474, 51474, 22171, 577, 731, 341, 18161, 3209, 1985, 365, 264, 2190, 3287, 295, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.08252111765054557, "compression_ratio": 1.790513833992095, "no_speech_prob": 2.5866174837574363e-05}, {"id": 380, "seek": 194128, "start": 1950.24, "end": 1954.28, "text": " parameters so that for example it's easy to count them and see that in total we", "tokens": [50364, 992, 510, 311, 439, 264, 9834, 300, 321, 7642, 286, 478, 586, 1228, 257, 19265, 281, 652, 50622, 50622, 309, 11408, 32128, 286, 596, 38624, 439, 264, 9834, 666, 257, 2167, 1329, 295, 50812, 50812, 9834, 370, 300, 337, 1365, 309, 311, 1858, 281, 1207, 552, 293, 536, 300, 294, 3217, 321, 51014, 51014, 4362, 362, 466, 805, 11, 13741, 9834, 293, 341, 307, 264, 2128, 1320, 382, 321, 51226, 51226, 4743, 309, 293, 321, 8881, 412, 257, 2167, 1230, 510, 264, 4470, 300, 307, 4362, 51474, 51474, 22171, 577, 731, 341, 18161, 3209, 1985, 365, 264, 2190, 3287, 295, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.08252111765054557, "compression_ratio": 1.790513833992095, "no_speech_prob": 2.5866174837574363e-05}, {"id": 381, "seek": 194128, "start": 1954.28, "end": 1958.52, "text": " currently have about 3,400 parameters and this is the forward pass as we", "tokens": [50364, 992, 510, 311, 439, 264, 9834, 300, 321, 7642, 286, 478, 586, 1228, 257, 19265, 281, 652, 50622, 50622, 309, 11408, 32128, 286, 596, 38624, 439, 264, 9834, 666, 257, 2167, 1329, 295, 50812, 50812, 9834, 370, 300, 337, 1365, 309, 311, 1858, 281, 1207, 552, 293, 536, 300, 294, 3217, 321, 51014, 51014, 4362, 362, 466, 805, 11, 13741, 9834, 293, 341, 307, 264, 2128, 1320, 382, 321, 51226, 51226, 4743, 309, 293, 321, 8881, 412, 257, 2167, 1230, 510, 264, 4470, 300, 307, 4362, 51474, 51474, 22171, 577, 731, 341, 18161, 3209, 1985, 365, 264, 2190, 3287, 295, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.08252111765054557, "compression_ratio": 1.790513833992095, "no_speech_prob": 2.5866174837574363e-05}, {"id": 382, "seek": 194128, "start": 1958.52, "end": 1963.48, "text": " developed it and we arrive at a single number here the loss that is currently", "tokens": [50364, 992, 510, 311, 439, 264, 9834, 300, 321, 7642, 286, 478, 586, 1228, 257, 19265, 281, 652, 50622, 50622, 309, 11408, 32128, 286, 596, 38624, 439, 264, 9834, 666, 257, 2167, 1329, 295, 50812, 50812, 9834, 370, 300, 337, 1365, 309, 311, 1858, 281, 1207, 552, 293, 536, 300, 294, 3217, 321, 51014, 51014, 4362, 362, 466, 805, 11, 13741, 9834, 293, 341, 307, 264, 2128, 1320, 382, 321, 51226, 51226, 4743, 309, 293, 321, 8881, 412, 257, 2167, 1230, 510, 264, 4470, 300, 307, 4362, 51474, 51474, 22171, 577, 731, 341, 18161, 3209, 1985, 365, 264, 2190, 3287, 295, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.08252111765054557, "compression_ratio": 1.790513833992095, "no_speech_prob": 2.5866174837574363e-05}, {"id": 383, "seek": 194128, "start": 1963.48, "end": 1967.72, "text": " expressing how well this neural network works with the current setting of", "tokens": [50364, 992, 510, 311, 439, 264, 9834, 300, 321, 7642, 286, 478, 586, 1228, 257, 19265, 281, 652, 50622, 50622, 309, 11408, 32128, 286, 596, 38624, 439, 264, 9834, 666, 257, 2167, 1329, 295, 50812, 50812, 9834, 370, 300, 337, 1365, 309, 311, 1858, 281, 1207, 552, 293, 536, 300, 294, 3217, 321, 51014, 51014, 4362, 362, 466, 805, 11, 13741, 9834, 293, 341, 307, 264, 2128, 1320, 382, 321, 51226, 51226, 4743, 309, 293, 321, 8881, 412, 257, 2167, 1230, 510, 264, 4470, 300, 307, 4362, 51474, 51474, 22171, 577, 731, 341, 18161, 3209, 1985, 365, 264, 2190, 3287, 295, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.08252111765054557, "compression_ratio": 1.790513833992095, "no_speech_prob": 2.5866174837574363e-05}, {"id": 384, "seek": 196772, "start": 1967.72, "end": 1971.72, "text": " parameters now I would like to make it even more respectable so in particular", "tokens": [50364, 9834, 586, 286, 576, 411, 281, 652, 309, 754, 544, 44279, 370, 294, 1729, 50564, 50564, 536, 613, 3876, 510, 689, 321, 747, 264, 3565, 1208, 293, 321, 8873, 257, 4470, 321, 434, 406, 50872, 50872, 767, 33477, 278, 264, 5589, 510, 341, 307, 445, 21538, 293, 867, 561, 51146, 51146, 764, 21538, 293, 300, 311, 983, 456, 307, 257, 11745, 5893, 3278, 30867, 51336, 51336, 2445, 294, 25878, 284, 339, 281, 8873, 341, 709, 544, 19621, 370, 321, 727, 445, 51552, 51552, 2935, 818, 283, 5893, 3278, 30867, 293, 321, 393, 1320, 294, 264, 3565, 1208, 293, 321, 393, 1320, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.09994494120279948, "compression_ratio": 1.9020408163265305, "no_speech_prob": 5.5941823120519985e-06}, {"id": 385, "seek": 196772, "start": 1971.72, "end": 1977.88, "text": " see these lines here where we take the logits and we calculate a loss we're not", "tokens": [50364, 9834, 586, 286, 576, 411, 281, 652, 309, 754, 544, 44279, 370, 294, 1729, 50564, 50564, 536, 613, 3876, 510, 689, 321, 747, 264, 3565, 1208, 293, 321, 8873, 257, 4470, 321, 434, 406, 50872, 50872, 767, 33477, 278, 264, 5589, 510, 341, 307, 445, 21538, 293, 867, 561, 51146, 51146, 764, 21538, 293, 300, 311, 983, 456, 307, 257, 11745, 5893, 3278, 30867, 51336, 51336, 2445, 294, 25878, 284, 339, 281, 8873, 341, 709, 544, 19621, 370, 321, 727, 445, 51552, 51552, 2935, 818, 283, 5893, 3278, 30867, 293, 321, 393, 1320, 294, 264, 3565, 1208, 293, 321, 393, 1320, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.09994494120279948, "compression_ratio": 1.9020408163265305, "no_speech_prob": 5.5941823120519985e-06}, {"id": 386, "seek": 196772, "start": 1977.88, "end": 1983.3600000000001, "text": " actually reinventing the wheel here this is just classification and many people", "tokens": [50364, 9834, 586, 286, 576, 411, 281, 652, 309, 754, 544, 44279, 370, 294, 1729, 50564, 50564, 536, 613, 3876, 510, 689, 321, 747, 264, 3565, 1208, 293, 321, 8873, 257, 4470, 321, 434, 406, 50872, 50872, 767, 33477, 278, 264, 5589, 510, 341, 307, 445, 21538, 293, 867, 561, 51146, 51146, 764, 21538, 293, 300, 311, 983, 456, 307, 257, 11745, 5893, 3278, 30867, 51336, 51336, 2445, 294, 25878, 284, 339, 281, 8873, 341, 709, 544, 19621, 370, 321, 727, 445, 51552, 51552, 2935, 818, 283, 5893, 3278, 30867, 293, 321, 393, 1320, 294, 264, 3565, 1208, 293, 321, 393, 1320, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.09994494120279948, "compression_ratio": 1.9020408163265305, "no_speech_prob": 5.5941823120519985e-06}, {"id": 387, "seek": 196772, "start": 1983.3600000000001, "end": 1987.16, "text": " use classification and that's why there is a functional dot cross entropy", "tokens": [50364, 9834, 586, 286, 576, 411, 281, 652, 309, 754, 544, 44279, 370, 294, 1729, 50564, 50564, 536, 613, 3876, 510, 689, 321, 747, 264, 3565, 1208, 293, 321, 8873, 257, 4470, 321, 434, 406, 50872, 50872, 767, 33477, 278, 264, 5589, 510, 341, 307, 445, 21538, 293, 867, 561, 51146, 51146, 764, 21538, 293, 300, 311, 983, 456, 307, 257, 11745, 5893, 3278, 30867, 51336, 51336, 2445, 294, 25878, 284, 339, 281, 8873, 341, 709, 544, 19621, 370, 321, 727, 445, 51552, 51552, 2935, 818, 283, 5893, 3278, 30867, 293, 321, 393, 1320, 294, 264, 3565, 1208, 293, 321, 393, 1320, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.09994494120279948, "compression_ratio": 1.9020408163265305, "no_speech_prob": 5.5941823120519985e-06}, {"id": 388, "seek": 196772, "start": 1987.16, "end": 1991.48, "text": " function in pytorch to calculate this much more efficiently so we could just", "tokens": [50364, 9834, 586, 286, 576, 411, 281, 652, 309, 754, 544, 44279, 370, 294, 1729, 50564, 50564, 536, 613, 3876, 510, 689, 321, 747, 264, 3565, 1208, 293, 321, 8873, 257, 4470, 321, 434, 406, 50872, 50872, 767, 33477, 278, 264, 5589, 510, 341, 307, 445, 21538, 293, 867, 561, 51146, 51146, 764, 21538, 293, 300, 311, 983, 456, 307, 257, 11745, 5893, 3278, 30867, 51336, 51336, 2445, 294, 25878, 284, 339, 281, 8873, 341, 709, 544, 19621, 370, 321, 727, 445, 51552, 51552, 2935, 818, 283, 5893, 3278, 30867, 293, 321, 393, 1320, 294, 264, 3565, 1208, 293, 321, 393, 1320, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.09994494120279948, "compression_ratio": 1.9020408163265305, "no_speech_prob": 5.5941823120519985e-06}, {"id": 389, "seek": 196772, "start": 1991.48, "end": 1995.24, "text": " simply call f dot cross entropy and we can pass in the logits and we can pass", "tokens": [50364, 9834, 586, 286, 576, 411, 281, 652, 309, 754, 544, 44279, 370, 294, 1729, 50564, 50564, 536, 613, 3876, 510, 689, 321, 747, 264, 3565, 1208, 293, 321, 8873, 257, 4470, 321, 434, 406, 50872, 50872, 767, 33477, 278, 264, 5589, 510, 341, 307, 445, 21538, 293, 867, 561, 51146, 51146, 764, 21538, 293, 300, 311, 983, 456, 307, 257, 11745, 5893, 3278, 30867, 51336, 51336, 2445, 294, 25878, 284, 339, 281, 8873, 341, 709, 544, 19621, 370, 321, 727, 445, 51552, 51552, 2935, 818, 283, 5893, 3278, 30867, 293, 321, 393, 1320, 294, 264, 3565, 1208, 293, 321, 393, 1320, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.09994494120279948, "compression_ratio": 1.9020408163265305, "no_speech_prob": 5.5941823120519985e-06}, {"id": 390, "seek": 199524, "start": 1995.24, "end": 2002.72, "text": " in the array of targets y and this calculates the exact same loss so in", "tokens": [50364, 294, 264, 10225, 295, 12911, 288, 293, 341, 4322, 1024, 264, 1900, 912, 4470, 370, 294, 50738, 50738, 1186, 321, 393, 2935, 829, 341, 510, 293, 23525, 613, 1045, 3876, 293, 321, 434, 516, 50990, 50990, 281, 483, 264, 1900, 912, 1874, 586, 456, 366, 767, 867, 665, 4112, 281, 51178, 51178, 4382, 283, 5893, 3278, 30867, 670, 9439, 428, 1065, 11420, 411, 341, 286, 630, 51418, 51418, 341, 337, 10189, 4112, 457, 291, 1116, 1128, 764, 341, 294, 3124, 983, 307, 300, 51606, 51606, 1230, 472, 562, 291, 764, 283, 5893, 3278, 30867, 25878, 284, 339, 486, 406, 767, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11904635796180138, "compression_ratio": 1.8032128514056225, "no_speech_prob": 3.785183480431442e-06}, {"id": 391, "seek": 199524, "start": 2002.72, "end": 2007.76, "text": " fact we can simply put this here and erase these three lines and we're going", "tokens": [50364, 294, 264, 10225, 295, 12911, 288, 293, 341, 4322, 1024, 264, 1900, 912, 4470, 370, 294, 50738, 50738, 1186, 321, 393, 2935, 829, 341, 510, 293, 23525, 613, 1045, 3876, 293, 321, 434, 516, 50990, 50990, 281, 483, 264, 1900, 912, 1874, 586, 456, 366, 767, 867, 665, 4112, 281, 51178, 51178, 4382, 283, 5893, 3278, 30867, 670, 9439, 428, 1065, 11420, 411, 341, 286, 630, 51418, 51418, 341, 337, 10189, 4112, 457, 291, 1116, 1128, 764, 341, 294, 3124, 983, 307, 300, 51606, 51606, 1230, 472, 562, 291, 764, 283, 5893, 3278, 30867, 25878, 284, 339, 486, 406, 767, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11904635796180138, "compression_ratio": 1.8032128514056225, "no_speech_prob": 3.785183480431442e-06}, {"id": 392, "seek": 199524, "start": 2007.76, "end": 2011.52, "text": " to get the exact same result now there are actually many good reasons to", "tokens": [50364, 294, 264, 10225, 295, 12911, 288, 293, 341, 4322, 1024, 264, 1900, 912, 4470, 370, 294, 50738, 50738, 1186, 321, 393, 2935, 829, 341, 510, 293, 23525, 613, 1045, 3876, 293, 321, 434, 516, 50990, 50990, 281, 483, 264, 1900, 912, 1874, 586, 456, 366, 767, 867, 665, 4112, 281, 51178, 51178, 4382, 283, 5893, 3278, 30867, 670, 9439, 428, 1065, 11420, 411, 341, 286, 630, 51418, 51418, 341, 337, 10189, 4112, 457, 291, 1116, 1128, 764, 341, 294, 3124, 983, 307, 300, 51606, 51606, 1230, 472, 562, 291, 764, 283, 5893, 3278, 30867, 25878, 284, 339, 486, 406, 767, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11904635796180138, "compression_ratio": 1.8032128514056225, "no_speech_prob": 3.785183480431442e-06}, {"id": 393, "seek": 199524, "start": 2011.52, "end": 2016.32, "text": " prefer f dot cross entropy over rolling your own implementation like this I did", "tokens": [50364, 294, 264, 10225, 295, 12911, 288, 293, 341, 4322, 1024, 264, 1900, 912, 4470, 370, 294, 50738, 50738, 1186, 321, 393, 2935, 829, 341, 510, 293, 23525, 613, 1045, 3876, 293, 321, 434, 516, 50990, 50990, 281, 483, 264, 1900, 912, 1874, 586, 456, 366, 767, 867, 665, 4112, 281, 51178, 51178, 4382, 283, 5893, 3278, 30867, 670, 9439, 428, 1065, 11420, 411, 341, 286, 630, 51418, 51418, 341, 337, 10189, 4112, 457, 291, 1116, 1128, 764, 341, 294, 3124, 983, 307, 300, 51606, 51606, 1230, 472, 562, 291, 764, 283, 5893, 3278, 30867, 25878, 284, 339, 486, 406, 767, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11904635796180138, "compression_ratio": 1.8032128514056225, "no_speech_prob": 3.785183480431442e-06}, {"id": 394, "seek": 199524, "start": 2016.32, "end": 2020.08, "text": " this for educational reasons but you'd never use this in practice why is that", "tokens": [50364, 294, 264, 10225, 295, 12911, 288, 293, 341, 4322, 1024, 264, 1900, 912, 4470, 370, 294, 50738, 50738, 1186, 321, 393, 2935, 829, 341, 510, 293, 23525, 613, 1045, 3876, 293, 321, 434, 516, 50990, 50990, 281, 483, 264, 1900, 912, 1874, 586, 456, 366, 767, 867, 665, 4112, 281, 51178, 51178, 4382, 283, 5893, 3278, 30867, 670, 9439, 428, 1065, 11420, 411, 341, 286, 630, 51418, 51418, 341, 337, 10189, 4112, 457, 291, 1116, 1128, 764, 341, 294, 3124, 983, 307, 300, 51606, 51606, 1230, 472, 562, 291, 764, 283, 5893, 3278, 30867, 25878, 284, 339, 486, 406, 767, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11904635796180138, "compression_ratio": 1.8032128514056225, "no_speech_prob": 3.785183480431442e-06}, {"id": 395, "seek": 199524, "start": 2020.08, "end": 2024.0, "text": " number one when you use f dot cross entropy pytorch will not actually", "tokens": [50364, 294, 264, 10225, 295, 12911, 288, 293, 341, 4322, 1024, 264, 1900, 912, 4470, 370, 294, 50738, 50738, 1186, 321, 393, 2935, 829, 341, 510, 293, 23525, 613, 1045, 3876, 293, 321, 434, 516, 50990, 50990, 281, 483, 264, 1900, 912, 1874, 586, 456, 366, 767, 867, 665, 4112, 281, 51178, 51178, 4382, 283, 5893, 3278, 30867, 670, 9439, 428, 1065, 11420, 411, 341, 286, 630, 51418, 51418, 341, 337, 10189, 4112, 457, 291, 1116, 1128, 764, 341, 294, 3124, 983, 307, 300, 51606, 51606, 1230, 472, 562, 291, 764, 283, 5893, 3278, 30867, 25878, 284, 339, 486, 406, 767, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11904635796180138, "compression_ratio": 1.8032128514056225, "no_speech_prob": 3.785183480431442e-06}, {"id": 396, "seek": 202400, "start": 2024.0, "end": 2028.28, "text": " create all these intermediate tensors because these are all new tensors in", "tokens": [50364, 1884, 439, 613, 19376, 10688, 830, 570, 613, 366, 439, 777, 10688, 830, 294, 50578, 50578, 4675, 293, 439, 341, 307, 6457, 43495, 281, 1190, 411, 341, 2602, 50802, 50802, 25878, 284, 339, 486, 13630, 493, 439, 613, 7705, 293, 588, 2049, 1884, 362, 51030, 51030, 283, 4717, 23434, 1625, 300, 588, 19621, 13059, 613, 15277, 300, 366, 1333, 51242, 51242, 295, 411, 596, 38624, 18894, 7705, 1230, 732, 264, 23897, 1320, 51464, 51464, 393, 312, 1027, 709, 544, 7148, 293, 406, 445, 570, 309, 311, 257, 283, 4717, 28256, 457, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.07721165104916221, "compression_ratio": 1.8879310344827587, "no_speech_prob": 1.3211003533797339e-05}, {"id": 397, "seek": 202400, "start": 2028.28, "end": 2032.76, "text": " memory and all this is fairly inefficient to run like this instead", "tokens": [50364, 1884, 439, 613, 19376, 10688, 830, 570, 613, 366, 439, 777, 10688, 830, 294, 50578, 50578, 4675, 293, 439, 341, 307, 6457, 43495, 281, 1190, 411, 341, 2602, 50802, 50802, 25878, 284, 339, 486, 13630, 493, 439, 613, 7705, 293, 588, 2049, 1884, 362, 51030, 51030, 283, 4717, 23434, 1625, 300, 588, 19621, 13059, 613, 15277, 300, 366, 1333, 51242, 51242, 295, 411, 596, 38624, 18894, 7705, 1230, 732, 264, 23897, 1320, 51464, 51464, 393, 312, 1027, 709, 544, 7148, 293, 406, 445, 570, 309, 311, 257, 283, 4717, 28256, 457, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.07721165104916221, "compression_ratio": 1.8879310344827587, "no_speech_prob": 1.3211003533797339e-05}, {"id": 398, "seek": 202400, "start": 2032.76, "end": 2037.32, "text": " pytorch will cluster up all these operations and very often create have", "tokens": [50364, 1884, 439, 613, 19376, 10688, 830, 570, 613, 366, 439, 777, 10688, 830, 294, 50578, 50578, 4675, 293, 439, 341, 307, 6457, 43495, 281, 1190, 411, 341, 2602, 50802, 50802, 25878, 284, 339, 486, 13630, 493, 439, 613, 7705, 293, 588, 2049, 1884, 362, 51030, 51030, 283, 4717, 23434, 1625, 300, 588, 19621, 13059, 613, 15277, 300, 366, 1333, 51242, 51242, 295, 411, 596, 38624, 18894, 7705, 1230, 732, 264, 23897, 1320, 51464, 51464, 393, 312, 1027, 709, 544, 7148, 293, 406, 445, 570, 309, 311, 257, 283, 4717, 28256, 457, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.07721165104916221, "compression_ratio": 1.8879310344827587, "no_speech_prob": 1.3211003533797339e-05}, {"id": 399, "seek": 202400, "start": 2037.32, "end": 2041.56, "text": " fused kernels that very efficiently evaluate these expressions that are sort", "tokens": [50364, 1884, 439, 613, 19376, 10688, 830, 570, 613, 366, 439, 777, 10688, 830, 294, 50578, 50578, 4675, 293, 439, 341, 307, 6457, 43495, 281, 1190, 411, 341, 2602, 50802, 50802, 25878, 284, 339, 486, 13630, 493, 439, 613, 7705, 293, 588, 2049, 1884, 362, 51030, 51030, 283, 4717, 23434, 1625, 300, 588, 19621, 13059, 613, 15277, 300, 366, 1333, 51242, 51242, 295, 411, 596, 38624, 18894, 7705, 1230, 732, 264, 23897, 1320, 51464, 51464, 393, 312, 1027, 709, 544, 7148, 293, 406, 445, 570, 309, 311, 257, 283, 4717, 28256, 457, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.07721165104916221, "compression_ratio": 1.8879310344827587, "no_speech_prob": 1.3211003533797339e-05}, {"id": 400, "seek": 202400, "start": 2041.56, "end": 2046.0, "text": " of like clustered mathematical operations number two the backward pass", "tokens": [50364, 1884, 439, 613, 19376, 10688, 830, 570, 613, 366, 439, 777, 10688, 830, 294, 50578, 50578, 4675, 293, 439, 341, 307, 6457, 43495, 281, 1190, 411, 341, 2602, 50802, 50802, 25878, 284, 339, 486, 13630, 493, 439, 613, 7705, 293, 588, 2049, 1884, 362, 51030, 51030, 283, 4717, 23434, 1625, 300, 588, 19621, 13059, 613, 15277, 300, 366, 1333, 51242, 51242, 295, 411, 596, 38624, 18894, 7705, 1230, 732, 264, 23897, 1320, 51464, 51464, 393, 312, 1027, 709, 544, 7148, 293, 406, 445, 570, 309, 311, 257, 283, 4717, 28256, 457, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.07721165104916221, "compression_ratio": 1.8879310344827587, "no_speech_prob": 1.3211003533797339e-05}, {"id": 401, "seek": 202400, "start": 2046.0, "end": 2050.08, "text": " can be made much more efficient and not just because it's a fused kernel but", "tokens": [50364, 1884, 439, 613, 19376, 10688, 830, 570, 613, 366, 439, 777, 10688, 830, 294, 50578, 50578, 4675, 293, 439, 341, 307, 6457, 43495, 281, 1190, 411, 341, 2602, 50802, 50802, 25878, 284, 339, 486, 13630, 493, 439, 613, 7705, 293, 588, 2049, 1884, 362, 51030, 51030, 283, 4717, 23434, 1625, 300, 588, 19621, 13059, 613, 15277, 300, 366, 1333, 51242, 51242, 295, 411, 596, 38624, 18894, 7705, 1230, 732, 264, 23897, 1320, 51464, 51464, 393, 312, 1027, 709, 544, 7148, 293, 406, 445, 570, 309, 311, 257, 283, 4717, 28256, 457, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.07721165104916221, "compression_ratio": 1.8879310344827587, "no_speech_prob": 1.3211003533797339e-05}, {"id": 402, "seek": 205008, "start": 2050.08, "end": 2055.2, "text": " also analytically and mathematically it's much it's often a very much simpler", "tokens": [50364, 611, 10783, 984, 293, 44003, 309, 311, 709, 309, 311, 2049, 257, 588, 709, 18587, 50620, 50620, 23897, 1320, 281, 4445, 321, 767, 3607, 341, 365, 4532, 2771, 291, 536, 510, 50874, 50874, 562, 321, 12270, 1266, 71, 264, 2128, 1320, 295, 341, 6916, 281, 8873, 264, 1266, 71, 51106, 51106, 390, 767, 257, 6457, 6179, 18894, 6114, 457, 570, 309, 311, 51310, 51310, 257, 596, 38624, 18894, 6114, 562, 321, 630, 264, 23897, 1320, 321, 994, 380, 51508, 51508, 16652, 23897, 807, 264, 1783, 293, 264, 568, 1413, 293, 264, 3175, 502, 293, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.12973691006096041, "compression_ratio": 1.9243697478991597, "no_speech_prob": 9.665558536653407e-06}, {"id": 403, "seek": 205008, "start": 2055.2, "end": 2060.2799999999997, "text": " backward pass to implement we actually sell this with micro grad you see here", "tokens": [50364, 611, 10783, 984, 293, 44003, 309, 311, 709, 309, 311, 2049, 257, 588, 709, 18587, 50620, 50620, 23897, 1320, 281, 4445, 321, 767, 3607, 341, 365, 4532, 2771, 291, 536, 510, 50874, 50874, 562, 321, 12270, 1266, 71, 264, 2128, 1320, 295, 341, 6916, 281, 8873, 264, 1266, 71, 51106, 51106, 390, 767, 257, 6457, 6179, 18894, 6114, 457, 570, 309, 311, 51310, 51310, 257, 596, 38624, 18894, 6114, 562, 321, 630, 264, 23897, 1320, 321, 994, 380, 51508, 51508, 16652, 23897, 807, 264, 1783, 293, 264, 568, 1413, 293, 264, 3175, 502, 293, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.12973691006096041, "compression_ratio": 1.9243697478991597, "no_speech_prob": 9.665558536653407e-06}, {"id": 404, "seek": 205008, "start": 2060.2799999999997, "end": 2064.92, "text": " when we implemented 10h the forward pass of this operation to calculate the 10h", "tokens": [50364, 611, 10783, 984, 293, 44003, 309, 311, 709, 309, 311, 2049, 257, 588, 709, 18587, 50620, 50620, 23897, 1320, 281, 4445, 321, 767, 3607, 341, 365, 4532, 2771, 291, 536, 510, 50874, 50874, 562, 321, 12270, 1266, 71, 264, 2128, 1320, 295, 341, 6916, 281, 8873, 264, 1266, 71, 51106, 51106, 390, 767, 257, 6457, 6179, 18894, 6114, 457, 570, 309, 311, 51310, 51310, 257, 596, 38624, 18894, 6114, 562, 321, 630, 264, 23897, 1320, 321, 994, 380, 51508, 51508, 16652, 23897, 807, 264, 1783, 293, 264, 568, 1413, 293, 264, 3175, 502, 293, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.12973691006096041, "compression_ratio": 1.9243697478991597, "no_speech_prob": 9.665558536653407e-06}, {"id": 405, "seek": 205008, "start": 2064.92, "end": 2069.0, "text": " was actually a fairly complicated mathematical expression but because it's", "tokens": [50364, 611, 10783, 984, 293, 44003, 309, 311, 709, 309, 311, 2049, 257, 588, 709, 18587, 50620, 50620, 23897, 1320, 281, 4445, 321, 767, 3607, 341, 365, 4532, 2771, 291, 536, 510, 50874, 50874, 562, 321, 12270, 1266, 71, 264, 2128, 1320, 295, 341, 6916, 281, 8873, 264, 1266, 71, 51106, 51106, 390, 767, 257, 6457, 6179, 18894, 6114, 457, 570, 309, 311, 51310, 51310, 257, 596, 38624, 18894, 6114, 562, 321, 630, 264, 23897, 1320, 321, 994, 380, 51508, 51508, 16652, 23897, 807, 264, 1783, 293, 264, 568, 1413, 293, 264, 3175, 502, 293, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.12973691006096041, "compression_ratio": 1.9243697478991597, "no_speech_prob": 9.665558536653407e-06}, {"id": 406, "seek": 205008, "start": 2069.0, "end": 2072.96, "text": " a clustered mathematical expression when we did the backward pass we didn't", "tokens": [50364, 611, 10783, 984, 293, 44003, 309, 311, 709, 309, 311, 2049, 257, 588, 709, 18587, 50620, 50620, 23897, 1320, 281, 4445, 321, 767, 3607, 341, 365, 4532, 2771, 291, 536, 510, 50874, 50874, 562, 321, 12270, 1266, 71, 264, 2128, 1320, 295, 341, 6916, 281, 8873, 264, 1266, 71, 51106, 51106, 390, 767, 257, 6457, 6179, 18894, 6114, 457, 570, 309, 311, 51310, 51310, 257, 596, 38624, 18894, 6114, 562, 321, 630, 264, 23897, 1320, 321, 994, 380, 51508, 51508, 16652, 23897, 807, 264, 1783, 293, 264, 568, 1413, 293, 264, 3175, 502, 293, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.12973691006096041, "compression_ratio": 1.9243697478991597, "no_speech_prob": 9.665558536653407e-06}, {"id": 407, "seek": 205008, "start": 2072.96, "end": 2076.88, "text": " individually backward through the X and the 2 times and the minus 1 and", "tokens": [50364, 611, 10783, 984, 293, 44003, 309, 311, 709, 309, 311, 2049, 257, 588, 709, 18587, 50620, 50620, 23897, 1320, 281, 4445, 321, 767, 3607, 341, 365, 4532, 2771, 291, 536, 510, 50874, 50874, 562, 321, 12270, 1266, 71, 264, 2128, 1320, 295, 341, 6916, 281, 8873, 264, 1266, 71, 51106, 51106, 390, 767, 257, 6457, 6179, 18894, 6114, 457, 570, 309, 311, 51310, 51310, 257, 596, 38624, 18894, 6114, 562, 321, 630, 264, 23897, 1320, 321, 994, 380, 51508, 51508, 16652, 23897, 807, 264, 1783, 293, 264, 568, 1413, 293, 264, 3175, 502, 293, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.12973691006096041, "compression_ratio": 1.9243697478991597, "no_speech_prob": 9.665558536653407e-06}, {"id": 408, "seek": 207688, "start": 2076.88, "end": 2081.6800000000003, "text": " division etc we just said it's 1 minus t squared and that's a much simpler", "tokens": [50364, 10044, 5183, 321, 445, 848, 309, 311, 502, 3175, 256, 8889, 293, 300, 311, 257, 709, 18587, 50604, 50604, 18894, 6114, 293, 321, 645, 1075, 281, 360, 341, 570, 321, 434, 1075, 281, 26225, 50802, 50802, 20448, 293, 570, 321, 366, 1075, 281, 44003, 293, 10783, 984, 28446, 50992, 50992, 264, 13760, 293, 2049, 300, 6114, 6883, 11221, 44003, 293, 370, 456, 311, 51228, 51228, 709, 1570, 281, 4445, 370, 406, 787, 393, 309, 312, 1027, 544, 7148, 570, 309, 51460, 51460, 6676, 294, 257, 283, 4717, 28256, 457, 611, 570, 264, 15277, 393, 747, 257, 709, 18587, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.08438778858558804, "compression_ratio": 1.9707112970711298, "no_speech_prob": 1.1125242053822149e-05}, {"id": 409, "seek": 207688, "start": 2081.6800000000003, "end": 2085.6400000000003, "text": " mathematical expression and we were able to do this because we're able to reuse", "tokens": [50364, 10044, 5183, 321, 445, 848, 309, 311, 502, 3175, 256, 8889, 293, 300, 311, 257, 709, 18587, 50604, 50604, 18894, 6114, 293, 321, 645, 1075, 281, 360, 341, 570, 321, 434, 1075, 281, 26225, 50802, 50802, 20448, 293, 570, 321, 366, 1075, 281, 44003, 293, 10783, 984, 28446, 50992, 50992, 264, 13760, 293, 2049, 300, 6114, 6883, 11221, 44003, 293, 370, 456, 311, 51228, 51228, 709, 1570, 281, 4445, 370, 406, 787, 393, 309, 312, 1027, 544, 7148, 570, 309, 51460, 51460, 6676, 294, 257, 283, 4717, 28256, 457, 611, 570, 264, 15277, 393, 747, 257, 709, 18587, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.08438778858558804, "compression_ratio": 1.9707112970711298, "no_speech_prob": 1.1125242053822149e-05}, {"id": 410, "seek": 207688, "start": 2085.6400000000003, "end": 2089.44, "text": " calculations and because we are able to mathematically and analytically derive", "tokens": [50364, 10044, 5183, 321, 445, 848, 309, 311, 502, 3175, 256, 8889, 293, 300, 311, 257, 709, 18587, 50604, 50604, 18894, 6114, 293, 321, 645, 1075, 281, 360, 341, 570, 321, 434, 1075, 281, 26225, 50802, 50802, 20448, 293, 570, 321, 366, 1075, 281, 44003, 293, 10783, 984, 28446, 50992, 50992, 264, 13760, 293, 2049, 300, 6114, 6883, 11221, 44003, 293, 370, 456, 311, 51228, 51228, 709, 1570, 281, 4445, 370, 406, 787, 393, 309, 312, 1027, 544, 7148, 570, 309, 51460, 51460, 6676, 294, 257, 283, 4717, 28256, 457, 611, 570, 264, 15277, 393, 747, 257, 709, 18587, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.08438778858558804, "compression_ratio": 1.9707112970711298, "no_speech_prob": 1.1125242053822149e-05}, {"id": 411, "seek": 207688, "start": 2089.44, "end": 2094.1600000000003, "text": " the derivative and often that expression simplifies mathematically and so there's", "tokens": [50364, 10044, 5183, 321, 445, 848, 309, 311, 502, 3175, 256, 8889, 293, 300, 311, 257, 709, 18587, 50604, 50604, 18894, 6114, 293, 321, 645, 1075, 281, 360, 341, 570, 321, 434, 1075, 281, 26225, 50802, 50802, 20448, 293, 570, 321, 366, 1075, 281, 44003, 293, 10783, 984, 28446, 50992, 50992, 264, 13760, 293, 2049, 300, 6114, 6883, 11221, 44003, 293, 370, 456, 311, 51228, 51228, 709, 1570, 281, 4445, 370, 406, 787, 393, 309, 312, 1027, 544, 7148, 570, 309, 51460, 51460, 6676, 294, 257, 283, 4717, 28256, 457, 611, 570, 264, 15277, 393, 747, 257, 709, 18587, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.08438778858558804, "compression_ratio": 1.9707112970711298, "no_speech_prob": 1.1125242053822149e-05}, {"id": 412, "seek": 207688, "start": 2094.1600000000003, "end": 2098.8, "text": " much less to implement so not only can it be made more efficient because it", "tokens": [50364, 10044, 5183, 321, 445, 848, 309, 311, 502, 3175, 256, 8889, 293, 300, 311, 257, 709, 18587, 50604, 50604, 18894, 6114, 293, 321, 645, 1075, 281, 360, 341, 570, 321, 434, 1075, 281, 26225, 50802, 50802, 20448, 293, 570, 321, 366, 1075, 281, 44003, 293, 10783, 984, 28446, 50992, 50992, 264, 13760, 293, 2049, 300, 6114, 6883, 11221, 44003, 293, 370, 456, 311, 51228, 51228, 709, 1570, 281, 4445, 370, 406, 787, 393, 309, 312, 1027, 544, 7148, 570, 309, 51460, 51460, 6676, 294, 257, 283, 4717, 28256, 457, 611, 570, 264, 15277, 393, 747, 257, 709, 18587, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.08438778858558804, "compression_ratio": 1.9707112970711298, "no_speech_prob": 1.1125242053822149e-05}, {"id": 413, "seek": 207688, "start": 2098.8, "end": 2102.92, "text": " runs in a fused kernel but also because the expressions can take a much simpler", "tokens": [50364, 10044, 5183, 321, 445, 848, 309, 311, 502, 3175, 256, 8889, 293, 300, 311, 257, 709, 18587, 50604, 50604, 18894, 6114, 293, 321, 645, 1075, 281, 360, 341, 570, 321, 434, 1075, 281, 26225, 50802, 50802, 20448, 293, 570, 321, 366, 1075, 281, 44003, 293, 10783, 984, 28446, 50992, 50992, 264, 13760, 293, 2049, 300, 6114, 6883, 11221, 44003, 293, 370, 456, 311, 51228, 51228, 709, 1570, 281, 4445, 370, 406, 787, 393, 309, 312, 1027, 544, 7148, 570, 309, 51460, 51460, 6676, 294, 257, 283, 4717, 28256, 457, 611, 570, 264, 15277, 393, 747, 257, 709, 18587, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.08438778858558804, "compression_ratio": 1.9707112970711298, "no_speech_prob": 1.1125242053822149e-05}, {"id": 414, "seek": 210292, "start": 2102.92, "end": 2110.0, "text": " form mathematically so that's number one number two under the hood f dot cross", "tokens": [50364, 1254, 44003, 370, 300, 311, 1230, 472, 1230, 732, 833, 264, 13376, 283, 5893, 3278, 50718, 50718, 30867, 393, 611, 312, 10591, 544, 7866, 984, 731, 48249, 718, 385, 855, 291, 51000, 51000, 364, 1365, 295, 577, 341, 1985, 7297, 321, 362, 257, 3565, 1208, 295, 3671, 732, 1045, 51302, 51302, 3671, 1045, 4018, 293, 1732, 293, 550, 321, 366, 1940, 264, 37871, 295, 309, 293, 51510, 51510, 2710, 3319, 309, 281, 2408, 281, 472, 370, 562, 3565, 1208, 747, 322, 341, 4190, 1203, 307, 51754, 51754], "temperature": 0.0, "avg_logprob": -0.1389045501023196, "compression_ratio": 1.6973684210526316, "no_speech_prob": 5.862660600541858e-06}, {"id": 415, "seek": 210292, "start": 2110.0, "end": 2115.64, "text": " entropy can also be significantly more numerically well behaved let me show you", "tokens": [50364, 1254, 44003, 370, 300, 311, 1230, 472, 1230, 732, 833, 264, 13376, 283, 5893, 3278, 50718, 50718, 30867, 393, 611, 312, 10591, 544, 7866, 984, 731, 48249, 718, 385, 855, 291, 51000, 51000, 364, 1365, 295, 577, 341, 1985, 7297, 321, 362, 257, 3565, 1208, 295, 3671, 732, 1045, 51302, 51302, 3671, 1045, 4018, 293, 1732, 293, 550, 321, 366, 1940, 264, 37871, 295, 309, 293, 51510, 51510, 2710, 3319, 309, 281, 2408, 281, 472, 370, 562, 3565, 1208, 747, 322, 341, 4190, 1203, 307, 51754, 51754], "temperature": 0.0, "avg_logprob": -0.1389045501023196, "compression_ratio": 1.6973684210526316, "no_speech_prob": 5.862660600541858e-06}, {"id": 416, "seek": 210292, "start": 2115.64, "end": 2121.6800000000003, "text": " an example of how this works suppose we have a logits of negative two three", "tokens": [50364, 1254, 44003, 370, 300, 311, 1230, 472, 1230, 732, 833, 264, 13376, 283, 5893, 3278, 50718, 50718, 30867, 393, 611, 312, 10591, 544, 7866, 984, 731, 48249, 718, 385, 855, 291, 51000, 51000, 364, 1365, 295, 577, 341, 1985, 7297, 321, 362, 257, 3565, 1208, 295, 3671, 732, 1045, 51302, 51302, 3671, 1045, 4018, 293, 1732, 293, 550, 321, 366, 1940, 264, 37871, 295, 309, 293, 51510, 51510, 2710, 3319, 309, 281, 2408, 281, 472, 370, 562, 3565, 1208, 747, 322, 341, 4190, 1203, 307, 51754, 51754], "temperature": 0.0, "avg_logprob": -0.1389045501023196, "compression_ratio": 1.6973684210526316, "no_speech_prob": 5.862660600541858e-06}, {"id": 417, "seek": 210292, "start": 2121.6800000000003, "end": 2125.84, "text": " negative three zero and five and then we are taking the exponent of it and", "tokens": [50364, 1254, 44003, 370, 300, 311, 1230, 472, 1230, 732, 833, 264, 13376, 283, 5893, 3278, 50718, 50718, 30867, 393, 611, 312, 10591, 544, 7866, 984, 731, 48249, 718, 385, 855, 291, 51000, 51000, 364, 1365, 295, 577, 341, 1985, 7297, 321, 362, 257, 3565, 1208, 295, 3671, 732, 1045, 51302, 51302, 3671, 1045, 4018, 293, 1732, 293, 550, 321, 366, 1940, 264, 37871, 295, 309, 293, 51510, 51510, 2710, 3319, 309, 281, 2408, 281, 472, 370, 562, 3565, 1208, 747, 322, 341, 4190, 1203, 307, 51754, 51754], "temperature": 0.0, "avg_logprob": -0.1389045501023196, "compression_ratio": 1.6973684210526316, "no_speech_prob": 5.862660600541858e-06}, {"id": 418, "seek": 210292, "start": 2125.84, "end": 2130.7200000000003, "text": " normalizing it to sum to one so when logits take on this values everything is", "tokens": [50364, 1254, 44003, 370, 300, 311, 1230, 472, 1230, 732, 833, 264, 13376, 283, 5893, 3278, 50718, 50718, 30867, 393, 611, 312, 10591, 544, 7866, 984, 731, 48249, 718, 385, 855, 291, 51000, 51000, 364, 1365, 295, 577, 341, 1985, 7297, 321, 362, 257, 3565, 1208, 295, 3671, 732, 1045, 51302, 51302, 3671, 1045, 4018, 293, 1732, 293, 550, 321, 366, 1940, 264, 37871, 295, 309, 293, 51510, 51510, 2710, 3319, 309, 281, 2408, 281, 472, 370, 562, 3565, 1208, 747, 322, 341, 4190, 1203, 307, 51754, 51754], "temperature": 0.0, "avg_logprob": -0.1389045501023196, "compression_ratio": 1.6973684210526316, "no_speech_prob": 5.862660600541858e-06}, {"id": 419, "seek": 213072, "start": 2130.72, "end": 2134.48, "text": " well and good and we get a nice probability distribution now consider", "tokens": [50364, 731, 293, 665, 293, 321, 483, 257, 1481, 8482, 7316, 586, 1949, 50552, 50552, 437, 2314, 562, 512, 295, 613, 3565, 1208, 747, 322, 544, 8084, 4190, 293, 300, 50726, 50726, 393, 1051, 1830, 19618, 295, 257, 18161, 3209, 7297, 300, 512, 295, 50912, 50912, 613, 3547, 1852, 588, 3671, 411, 584, 3671, 2319, 550, 767, 51124, 51124, 1203, 486, 808, 484, 2489, 321, 920, 483, 257, 33783, 300, 291, 458, 366, 51378, 51378, 731, 12, 29437, 12865, 293, 436, 2408, 281, 472, 293, 1203, 307, 869, 457, 570, 295, 264, 51594, 51594, 636, 264, 1783, 1985, 498, 291, 362, 588, 3353, 3565, 1208, 718, 311, 584, 3353, 2319, 51817, 51817], "temperature": 0.0, "avg_logprob": -0.1226259586030403, "compression_ratio": 1.7992957746478873, "no_speech_prob": 2.812945922414656e-06}, {"id": 420, "seek": 213072, "start": 2134.48, "end": 2137.9599999999996, "text": " what happens when some of these logits take on more extreme values and that", "tokens": [50364, 731, 293, 665, 293, 321, 483, 257, 1481, 8482, 7316, 586, 1949, 50552, 50552, 437, 2314, 562, 512, 295, 613, 3565, 1208, 747, 322, 544, 8084, 4190, 293, 300, 50726, 50726, 393, 1051, 1830, 19618, 295, 257, 18161, 3209, 7297, 300, 512, 295, 50912, 50912, 613, 3547, 1852, 588, 3671, 411, 584, 3671, 2319, 550, 767, 51124, 51124, 1203, 486, 808, 484, 2489, 321, 920, 483, 257, 33783, 300, 291, 458, 366, 51378, 51378, 731, 12, 29437, 12865, 293, 436, 2408, 281, 472, 293, 1203, 307, 869, 457, 570, 295, 264, 51594, 51594, 636, 264, 1783, 1985, 498, 291, 362, 588, 3353, 3565, 1208, 718, 311, 584, 3353, 2319, 51817, 51817], "temperature": 0.0, "avg_logprob": -0.1226259586030403, "compression_ratio": 1.7992957746478873, "no_speech_prob": 2.812945922414656e-06}, {"id": 421, "seek": 213072, "start": 2137.9599999999996, "end": 2141.68, "text": " can happen during optimization of a neural network suppose that some of", "tokens": [50364, 731, 293, 665, 293, 321, 483, 257, 1481, 8482, 7316, 586, 1949, 50552, 50552, 437, 2314, 562, 512, 295, 613, 3565, 1208, 747, 322, 544, 8084, 4190, 293, 300, 50726, 50726, 393, 1051, 1830, 19618, 295, 257, 18161, 3209, 7297, 300, 512, 295, 50912, 50912, 613, 3547, 1852, 588, 3671, 411, 584, 3671, 2319, 550, 767, 51124, 51124, 1203, 486, 808, 484, 2489, 321, 920, 483, 257, 33783, 300, 291, 458, 366, 51378, 51378, 731, 12, 29437, 12865, 293, 436, 2408, 281, 472, 293, 1203, 307, 869, 457, 570, 295, 264, 51594, 51594, 636, 264, 1783, 1985, 498, 291, 362, 588, 3353, 3565, 1208, 718, 311, 584, 3353, 2319, 51817, 51817], "temperature": 0.0, "avg_logprob": -0.1226259586030403, "compression_ratio": 1.7992957746478873, "no_speech_prob": 2.812945922414656e-06}, {"id": 422, "seek": 213072, "start": 2141.68, "end": 2145.9199999999996, "text": " these numbers grow very negative like say negative 100 then actually", "tokens": [50364, 731, 293, 665, 293, 321, 483, 257, 1481, 8482, 7316, 586, 1949, 50552, 50552, 437, 2314, 562, 512, 295, 613, 3565, 1208, 747, 322, 544, 8084, 4190, 293, 300, 50726, 50726, 393, 1051, 1830, 19618, 295, 257, 18161, 3209, 7297, 300, 512, 295, 50912, 50912, 613, 3547, 1852, 588, 3671, 411, 584, 3671, 2319, 550, 767, 51124, 51124, 1203, 486, 808, 484, 2489, 321, 920, 483, 257, 33783, 300, 291, 458, 366, 51378, 51378, 731, 12, 29437, 12865, 293, 436, 2408, 281, 472, 293, 1203, 307, 869, 457, 570, 295, 264, 51594, 51594, 636, 264, 1783, 1985, 498, 291, 362, 588, 3353, 3565, 1208, 718, 311, 584, 3353, 2319, 51817, 51817], "temperature": 0.0, "avg_logprob": -0.1226259586030403, "compression_ratio": 1.7992957746478873, "no_speech_prob": 2.812945922414656e-06}, {"id": 423, "seek": 213072, "start": 2145.9199999999996, "end": 2151.0, "text": " everything will come out fine we still get a probabilities that you know are", "tokens": [50364, 731, 293, 665, 293, 321, 483, 257, 1481, 8482, 7316, 586, 1949, 50552, 50552, 437, 2314, 562, 512, 295, 613, 3565, 1208, 747, 322, 544, 8084, 4190, 293, 300, 50726, 50726, 393, 1051, 1830, 19618, 295, 257, 18161, 3209, 7297, 300, 512, 295, 50912, 50912, 613, 3547, 1852, 588, 3671, 411, 584, 3671, 2319, 550, 767, 51124, 51124, 1203, 486, 808, 484, 2489, 321, 920, 483, 257, 33783, 300, 291, 458, 366, 51378, 51378, 731, 12, 29437, 12865, 293, 436, 2408, 281, 472, 293, 1203, 307, 869, 457, 570, 295, 264, 51594, 51594, 636, 264, 1783, 1985, 498, 291, 362, 588, 3353, 3565, 1208, 718, 311, 584, 3353, 2319, 51817, 51817], "temperature": 0.0, "avg_logprob": -0.1226259586030403, "compression_ratio": 1.7992957746478873, "no_speech_prob": 2.812945922414656e-06}, {"id": 424, "seek": 213072, "start": 2151.0, "end": 2155.3199999999997, "text": " well-behaved and they sum to one and everything is great but because of the", "tokens": [50364, 731, 293, 665, 293, 321, 483, 257, 1481, 8482, 7316, 586, 1949, 50552, 50552, 437, 2314, 562, 512, 295, 613, 3565, 1208, 747, 322, 544, 8084, 4190, 293, 300, 50726, 50726, 393, 1051, 1830, 19618, 295, 257, 18161, 3209, 7297, 300, 512, 295, 50912, 50912, 613, 3547, 1852, 588, 3671, 411, 584, 3671, 2319, 550, 767, 51124, 51124, 1203, 486, 808, 484, 2489, 321, 920, 483, 257, 33783, 300, 291, 458, 366, 51378, 51378, 731, 12, 29437, 12865, 293, 436, 2408, 281, 472, 293, 1203, 307, 869, 457, 570, 295, 264, 51594, 51594, 636, 264, 1783, 1985, 498, 291, 362, 588, 3353, 3565, 1208, 718, 311, 584, 3353, 2319, 51817, 51817], "temperature": 0.0, "avg_logprob": -0.1226259586030403, "compression_ratio": 1.7992957746478873, "no_speech_prob": 2.812945922414656e-06}, {"id": 425, "seek": 213072, "start": 2155.3199999999997, "end": 2159.7799999999997, "text": " way the X works if you have very positive logits let's say positive 100", "tokens": [50364, 731, 293, 665, 293, 321, 483, 257, 1481, 8482, 7316, 586, 1949, 50552, 50552, 437, 2314, 562, 512, 295, 613, 3565, 1208, 747, 322, 544, 8084, 4190, 293, 300, 50726, 50726, 393, 1051, 1830, 19618, 295, 257, 18161, 3209, 7297, 300, 512, 295, 50912, 50912, 613, 3547, 1852, 588, 3671, 411, 584, 3671, 2319, 550, 767, 51124, 51124, 1203, 486, 808, 484, 2489, 321, 920, 483, 257, 33783, 300, 291, 458, 366, 51378, 51378, 731, 12, 29437, 12865, 293, 436, 2408, 281, 472, 293, 1203, 307, 869, 457, 570, 295, 264, 51594, 51594, 636, 264, 1783, 1985, 498, 291, 362, 588, 3353, 3565, 1208, 718, 311, 584, 3353, 2319, 51817, 51817], "temperature": 0.0, "avg_logprob": -0.1226259586030403, "compression_ratio": 1.7992957746478873, "no_speech_prob": 2.812945922414656e-06}, {"id": 426, "seek": 215978, "start": 2159.78, "end": 2163.92, "text": " in here you actually start to run into trouble and we get not a number here and", "tokens": [50364, 294, 510, 291, 767, 722, 281, 1190, 666, 5253, 293, 321, 483, 406, 257, 1230, 510, 293, 50571, 50571, 264, 1778, 337, 300, 307, 300, 613, 14893, 362, 364, 1536, 510, 370, 498, 291, 1320, 294, 257, 50955, 50955, 588, 3671, 1230, 281, 1783, 291, 445, 483, 257, 588, 3671, 2597, 406, 3671, 457, 51191, 51191, 588, 1359, 1230, 588, 2651, 588, 2651, 4018, 293, 300, 311, 2489, 457, 291, 1320, 294, 257, 51411, 51411, 588, 3353, 1230, 5800, 321, 1190, 484, 295, 3613, 294, 527, 12607, 935, 1230, 51635, 51635], "temperature": 0.0, "avg_logprob": -0.13576382462696363, "compression_ratio": 1.9261083743842364, "no_speech_prob": 1.0129598194907885e-05}, {"id": 427, "seek": 215978, "start": 2163.92, "end": 2171.6000000000004, "text": " the reason for that is that these counts have an inf here so if you pass in a", "tokens": [50364, 294, 510, 291, 767, 722, 281, 1190, 666, 5253, 293, 321, 483, 406, 257, 1230, 510, 293, 50571, 50571, 264, 1778, 337, 300, 307, 300, 613, 14893, 362, 364, 1536, 510, 370, 498, 291, 1320, 294, 257, 50955, 50955, 588, 3671, 1230, 281, 1783, 291, 445, 483, 257, 588, 3671, 2597, 406, 3671, 457, 51191, 51191, 588, 1359, 1230, 588, 2651, 588, 2651, 4018, 293, 300, 311, 2489, 457, 291, 1320, 294, 257, 51411, 51411, 588, 3353, 1230, 5800, 321, 1190, 484, 295, 3613, 294, 527, 12607, 935, 1230, 51635, 51635], "temperature": 0.0, "avg_logprob": -0.13576382462696363, "compression_ratio": 1.9261083743842364, "no_speech_prob": 1.0129598194907885e-05}, {"id": 428, "seek": 215978, "start": 2171.6000000000004, "end": 2176.32, "text": " very negative number to X you just get a very negative sorry not negative but", "tokens": [50364, 294, 510, 291, 767, 722, 281, 1190, 666, 5253, 293, 321, 483, 406, 257, 1230, 510, 293, 50571, 50571, 264, 1778, 337, 300, 307, 300, 613, 14893, 362, 364, 1536, 510, 370, 498, 291, 1320, 294, 257, 50955, 50955, 588, 3671, 1230, 281, 1783, 291, 445, 483, 257, 588, 3671, 2597, 406, 3671, 457, 51191, 51191, 588, 1359, 1230, 588, 2651, 588, 2651, 4018, 293, 300, 311, 2489, 457, 291, 1320, 294, 257, 51411, 51411, 588, 3353, 1230, 5800, 321, 1190, 484, 295, 3613, 294, 527, 12607, 935, 1230, 51635, 51635], "temperature": 0.0, "avg_logprob": -0.13576382462696363, "compression_ratio": 1.9261083743842364, "no_speech_prob": 1.0129598194907885e-05}, {"id": 429, "seek": 215978, "start": 2176.32, "end": 2180.7200000000003, "text": " very small number very near very near zero and that's fine but you pass in a", "tokens": [50364, 294, 510, 291, 767, 722, 281, 1190, 666, 5253, 293, 321, 483, 406, 257, 1230, 510, 293, 50571, 50571, 264, 1778, 337, 300, 307, 300, 613, 14893, 362, 364, 1536, 510, 370, 498, 291, 1320, 294, 257, 50955, 50955, 588, 3671, 1230, 281, 1783, 291, 445, 483, 257, 588, 3671, 2597, 406, 3671, 457, 51191, 51191, 588, 1359, 1230, 588, 2651, 588, 2651, 4018, 293, 300, 311, 2489, 457, 291, 1320, 294, 257, 51411, 51411, 588, 3353, 1230, 5800, 321, 1190, 484, 295, 3613, 294, 527, 12607, 935, 1230, 51635, 51635], "temperature": 0.0, "avg_logprob": -0.13576382462696363, "compression_ratio": 1.9261083743842364, "no_speech_prob": 1.0129598194907885e-05}, {"id": 430, "seek": 215978, "start": 2180.7200000000003, "end": 2185.2000000000003, "text": " very positive number suddenly we run out of range in our floating point number", "tokens": [50364, 294, 510, 291, 767, 722, 281, 1190, 666, 5253, 293, 321, 483, 406, 257, 1230, 510, 293, 50571, 50571, 264, 1778, 337, 300, 307, 300, 613, 14893, 362, 364, 1536, 510, 370, 498, 291, 1320, 294, 257, 50955, 50955, 588, 3671, 1230, 281, 1783, 291, 445, 483, 257, 588, 3671, 2597, 406, 3671, 457, 51191, 51191, 588, 1359, 1230, 588, 2651, 588, 2651, 4018, 293, 300, 311, 2489, 457, 291, 1320, 294, 257, 51411, 51411, 588, 3353, 1230, 5800, 321, 1190, 484, 295, 3613, 294, 527, 12607, 935, 1230, 51635, 51635], "temperature": 0.0, "avg_logprob": -0.13576382462696363, "compression_ratio": 1.9261083743842364, "no_speech_prob": 1.0129598194907885e-05}, {"id": 431, "seek": 218520, "start": 2185.2, "end": 2189.9199999999996, "text": " that represents these counts so basically we're taking E and we're", "tokens": [50364, 300, 8855, 613, 14893, 370, 1936, 321, 434, 1940, 462, 293, 321, 434, 50600, 50600, 11225, 309, 281, 264, 1347, 295, 2319, 293, 300, 2709, 505, 1536, 570, 321, 1190, 484, 295, 50812, 50812, 8546, 3613, 322, 341, 12607, 935, 1230, 300, 307, 1207, 293, 370, 321, 2644, 51056, 51056, 1320, 588, 2416, 3565, 1208, 807, 341, 6114, 586, 718, 385, 14322, 613, 51354, 51354, 3547, 360, 746, 10585, 264, 636, 25878, 284, 339, 5039, 341, 307, 300, 291, 536, 577, 51651, 51651, 321, 362, 257, 534, 731, 12, 29437, 12865, 1874, 510, 309, 4523, 484, 300, 570, 295, 264, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1189494591492873, "compression_ratio": 1.6988416988416988, "no_speech_prob": 3.7852466903132154e-06}, {"id": 432, "seek": 218520, "start": 2189.9199999999996, "end": 2194.16, "text": " raising it to the power of 100 and that gives us inf because we run out of", "tokens": [50364, 300, 8855, 613, 14893, 370, 1936, 321, 434, 1940, 462, 293, 321, 434, 50600, 50600, 11225, 309, 281, 264, 1347, 295, 2319, 293, 300, 2709, 505, 1536, 570, 321, 1190, 484, 295, 50812, 50812, 8546, 3613, 322, 341, 12607, 935, 1230, 300, 307, 1207, 293, 370, 321, 2644, 51056, 51056, 1320, 588, 2416, 3565, 1208, 807, 341, 6114, 586, 718, 385, 14322, 613, 51354, 51354, 3547, 360, 746, 10585, 264, 636, 25878, 284, 339, 5039, 341, 307, 300, 291, 536, 577, 51651, 51651, 321, 362, 257, 534, 731, 12, 29437, 12865, 1874, 510, 309, 4523, 484, 300, 570, 295, 264, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1189494591492873, "compression_ratio": 1.6988416988416988, "no_speech_prob": 3.7852466903132154e-06}, {"id": 433, "seek": 218520, "start": 2194.16, "end": 2199.04, "text": " dynamic range on this floating point number that is count and so we cannot", "tokens": [50364, 300, 8855, 613, 14893, 370, 1936, 321, 434, 1940, 462, 293, 321, 434, 50600, 50600, 11225, 309, 281, 264, 1347, 295, 2319, 293, 300, 2709, 505, 1536, 570, 321, 1190, 484, 295, 50812, 50812, 8546, 3613, 322, 341, 12607, 935, 1230, 300, 307, 1207, 293, 370, 321, 2644, 51056, 51056, 1320, 588, 2416, 3565, 1208, 807, 341, 6114, 586, 718, 385, 14322, 613, 51354, 51354, 3547, 360, 746, 10585, 264, 636, 25878, 284, 339, 5039, 341, 307, 300, 291, 536, 577, 51651, 51651, 321, 362, 257, 534, 731, 12, 29437, 12865, 1874, 510, 309, 4523, 484, 300, 570, 295, 264, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1189494591492873, "compression_ratio": 1.6988416988416988, "no_speech_prob": 3.7852466903132154e-06}, {"id": 434, "seek": 218520, "start": 2199.04, "end": 2205.0, "text": " pass very large logits through this expression now let me reset these", "tokens": [50364, 300, 8855, 613, 14893, 370, 1936, 321, 434, 1940, 462, 293, 321, 434, 50600, 50600, 11225, 309, 281, 264, 1347, 295, 2319, 293, 300, 2709, 505, 1536, 570, 321, 1190, 484, 295, 50812, 50812, 8546, 3613, 322, 341, 12607, 935, 1230, 300, 307, 1207, 293, 370, 321, 2644, 51056, 51056, 1320, 588, 2416, 3565, 1208, 807, 341, 6114, 586, 718, 385, 14322, 613, 51354, 51354, 3547, 360, 746, 10585, 264, 636, 25878, 284, 339, 5039, 341, 307, 300, 291, 536, 577, 51651, 51651, 321, 362, 257, 534, 731, 12, 29437, 12865, 1874, 510, 309, 4523, 484, 300, 570, 295, 264, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1189494591492873, "compression_ratio": 1.6988416988416988, "no_speech_prob": 3.7852466903132154e-06}, {"id": 435, "seek": 218520, "start": 2205.0, "end": 2210.9399999999996, "text": " numbers do something reasonable the way pytorch solve this is that you see how", "tokens": [50364, 300, 8855, 613, 14893, 370, 1936, 321, 434, 1940, 462, 293, 321, 434, 50600, 50600, 11225, 309, 281, 264, 1347, 295, 2319, 293, 300, 2709, 505, 1536, 570, 321, 1190, 484, 295, 50812, 50812, 8546, 3613, 322, 341, 12607, 935, 1230, 300, 307, 1207, 293, 370, 321, 2644, 51056, 51056, 1320, 588, 2416, 3565, 1208, 807, 341, 6114, 586, 718, 385, 14322, 613, 51354, 51354, 3547, 360, 746, 10585, 264, 636, 25878, 284, 339, 5039, 341, 307, 300, 291, 536, 577, 51651, 51651, 321, 362, 257, 534, 731, 12, 29437, 12865, 1874, 510, 309, 4523, 484, 300, 570, 295, 264, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1189494591492873, "compression_ratio": 1.6988416988416988, "no_speech_prob": 3.7852466903132154e-06}, {"id": 436, "seek": 218520, "start": 2210.9399999999996, "end": 2214.9199999999996, "text": " we have a really well-behaved result here it turns out that because of the", "tokens": [50364, 300, 8855, 613, 14893, 370, 1936, 321, 434, 1940, 462, 293, 321, 434, 50600, 50600, 11225, 309, 281, 264, 1347, 295, 2319, 293, 300, 2709, 505, 1536, 570, 321, 1190, 484, 295, 50812, 50812, 8546, 3613, 322, 341, 12607, 935, 1230, 300, 307, 1207, 293, 370, 321, 2644, 51056, 51056, 1320, 588, 2416, 3565, 1208, 807, 341, 6114, 586, 718, 385, 14322, 613, 51354, 51354, 3547, 360, 746, 10585, 264, 636, 25878, 284, 339, 5039, 341, 307, 300, 291, 536, 577, 51651, 51651, 321, 362, 257, 534, 731, 12, 29437, 12865, 1874, 510, 309, 4523, 484, 300, 570, 295, 264, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1189494591492873, "compression_ratio": 1.6988416988416988, "no_speech_prob": 3.7852466903132154e-06}, {"id": 437, "seek": 221492, "start": 2214.92, "end": 2219.8, "text": " normalization here you can actually offset logits by any arbitrary constant", "tokens": [50364, 2710, 2144, 510, 291, 393, 767, 18687, 3565, 1208, 538, 604, 23211, 5754, 50608, 50608, 2158, 300, 291, 528, 370, 498, 286, 909, 472, 510, 291, 767, 483, 264, 1900, 912, 1874, 50838, 50838, 420, 498, 286, 909, 732, 420, 498, 286, 16390, 1045, 604, 18687, 486, 5258, 264, 1900, 912, 51154, 51154, 33783, 370, 570, 3671, 3547, 366, 1392, 457, 3353, 3547, 51414, 51414, 393, 767, 37772, 341, 1783, 437, 3817, 15804, 775, 307, 309, 19501, 4322, 1024, 51658, 51658], "temperature": 0.0, "avg_logprob": -0.13473147656544146, "compression_ratio": 1.780373831775701, "no_speech_prob": 1.4063584785617422e-05}, {"id": 438, "seek": 221492, "start": 2219.8, "end": 2224.4, "text": " value that you want so if I add one here you actually get the exact same result", "tokens": [50364, 2710, 2144, 510, 291, 393, 767, 18687, 3565, 1208, 538, 604, 23211, 5754, 50608, 50608, 2158, 300, 291, 528, 370, 498, 286, 909, 472, 510, 291, 767, 483, 264, 1900, 912, 1874, 50838, 50838, 420, 498, 286, 909, 732, 420, 498, 286, 16390, 1045, 604, 18687, 486, 5258, 264, 1900, 912, 51154, 51154, 33783, 370, 570, 3671, 3547, 366, 1392, 457, 3353, 3547, 51414, 51414, 393, 767, 37772, 341, 1783, 437, 3817, 15804, 775, 307, 309, 19501, 4322, 1024, 51658, 51658], "temperature": 0.0, "avg_logprob": -0.13473147656544146, "compression_ratio": 1.780373831775701, "no_speech_prob": 1.4063584785617422e-05}, {"id": 439, "seek": 221492, "start": 2224.4, "end": 2230.7200000000003, "text": " or if I add two or if I subtract three any offset will produce the exact same", "tokens": [50364, 2710, 2144, 510, 291, 393, 767, 18687, 3565, 1208, 538, 604, 23211, 5754, 50608, 50608, 2158, 300, 291, 528, 370, 498, 286, 909, 472, 510, 291, 767, 483, 264, 1900, 912, 1874, 50838, 50838, 420, 498, 286, 909, 732, 420, 498, 286, 16390, 1045, 604, 18687, 486, 5258, 264, 1900, 912, 51154, 51154, 33783, 370, 570, 3671, 3547, 366, 1392, 457, 3353, 3547, 51414, 51414, 393, 767, 37772, 341, 1783, 437, 3817, 15804, 775, 307, 309, 19501, 4322, 1024, 51658, 51658], "temperature": 0.0, "avg_logprob": -0.13473147656544146, "compression_ratio": 1.780373831775701, "no_speech_prob": 1.4063584785617422e-05}, {"id": 440, "seek": 221492, "start": 2230.7200000000003, "end": 2235.92, "text": " probabilities so because negative numbers are okay but positive numbers", "tokens": [50364, 2710, 2144, 510, 291, 393, 767, 18687, 3565, 1208, 538, 604, 23211, 5754, 50608, 50608, 2158, 300, 291, 528, 370, 498, 286, 909, 472, 510, 291, 767, 483, 264, 1900, 912, 1874, 50838, 50838, 420, 498, 286, 909, 732, 420, 498, 286, 16390, 1045, 604, 18687, 486, 5258, 264, 1900, 912, 51154, 51154, 33783, 370, 570, 3671, 3547, 366, 1392, 457, 3353, 3547, 51414, 51414, 393, 767, 37772, 341, 1783, 437, 3817, 15804, 775, 307, 309, 19501, 4322, 1024, 51658, 51658], "temperature": 0.0, "avg_logprob": -0.13473147656544146, "compression_ratio": 1.780373831775701, "no_speech_prob": 1.4063584785617422e-05}, {"id": 441, "seek": 221492, "start": 2235.92, "end": 2240.8, "text": " can actually overflow this X what petridge does is it internally calculates", "tokens": [50364, 2710, 2144, 510, 291, 393, 767, 18687, 3565, 1208, 538, 604, 23211, 5754, 50608, 50608, 2158, 300, 291, 528, 370, 498, 286, 909, 472, 510, 291, 767, 483, 264, 1900, 912, 1874, 50838, 50838, 420, 498, 286, 909, 732, 420, 498, 286, 16390, 1045, 604, 18687, 486, 5258, 264, 1900, 912, 51154, 51154, 33783, 370, 570, 3671, 3547, 366, 1392, 457, 3353, 3547, 51414, 51414, 393, 767, 37772, 341, 1783, 437, 3817, 15804, 775, 307, 309, 19501, 4322, 1024, 51658, 51658], "temperature": 0.0, "avg_logprob": -0.13473147656544146, "compression_ratio": 1.780373831775701, "no_speech_prob": 1.4063584785617422e-05}, {"id": 442, "seek": 224080, "start": 2240.8, "end": 2245.04, "text": " the maximum value that occurs in the logits and it subtracts it so in this", "tokens": [50364, 264, 6674, 2158, 300, 11843, 294, 264, 3565, 1208, 293, 309, 16390, 82, 309, 370, 294, 341, 50576, 50576, 1389, 309, 576, 16390, 1732, 293, 370, 4412, 264, 6636, 1230, 294, 3565, 1208, 50784, 50784, 486, 1813, 4018, 293, 439, 264, 661, 3547, 486, 1813, 512, 3671, 50938, 50938, 3547, 293, 550, 264, 1874, 295, 341, 307, 1009, 731, 48249, 370, 754, 498, 321, 362, 257, 51180, 51180, 3262, 510, 8046, 406, 665, 457, 570, 25878, 284, 339, 486, 16390, 257, 3262, 51428, 51428, 341, 486, 589, 293, 370, 456, 311, 867, 665, 4112, 281, 818, 3278, 30867, 1230, 472, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.08780812300168551, "compression_ratio": 1.876543209876543, "no_speech_prob": 6.438880518544465e-06}, {"id": 443, "seek": 224080, "start": 2245.04, "end": 2249.2000000000003, "text": " case it would subtract five and so therefore the greatest number in logits", "tokens": [50364, 264, 6674, 2158, 300, 11843, 294, 264, 3565, 1208, 293, 309, 16390, 82, 309, 370, 294, 341, 50576, 50576, 1389, 309, 576, 16390, 1732, 293, 370, 4412, 264, 6636, 1230, 294, 3565, 1208, 50784, 50784, 486, 1813, 4018, 293, 439, 264, 661, 3547, 486, 1813, 512, 3671, 50938, 50938, 3547, 293, 550, 264, 1874, 295, 341, 307, 1009, 731, 48249, 370, 754, 498, 321, 362, 257, 51180, 51180, 3262, 510, 8046, 406, 665, 457, 570, 25878, 284, 339, 486, 16390, 257, 3262, 51428, 51428, 341, 486, 589, 293, 370, 456, 311, 867, 665, 4112, 281, 818, 3278, 30867, 1230, 472, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.08780812300168551, "compression_ratio": 1.876543209876543, "no_speech_prob": 6.438880518544465e-06}, {"id": 444, "seek": 224080, "start": 2249.2000000000003, "end": 2252.28, "text": " will become zero and all the other numbers will become some negative", "tokens": [50364, 264, 6674, 2158, 300, 11843, 294, 264, 3565, 1208, 293, 309, 16390, 82, 309, 370, 294, 341, 50576, 50576, 1389, 309, 576, 16390, 1732, 293, 370, 4412, 264, 6636, 1230, 294, 3565, 1208, 50784, 50784, 486, 1813, 4018, 293, 439, 264, 661, 3547, 486, 1813, 512, 3671, 50938, 50938, 3547, 293, 550, 264, 1874, 295, 341, 307, 1009, 731, 48249, 370, 754, 498, 321, 362, 257, 51180, 51180, 3262, 510, 8046, 406, 665, 457, 570, 25878, 284, 339, 486, 16390, 257, 3262, 51428, 51428, 341, 486, 589, 293, 370, 456, 311, 867, 665, 4112, 281, 818, 3278, 30867, 1230, 472, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.08780812300168551, "compression_ratio": 1.876543209876543, "no_speech_prob": 6.438880518544465e-06}, {"id": 445, "seek": 224080, "start": 2252.28, "end": 2257.1200000000003, "text": " numbers and then the result of this is always well behaved so even if we have a", "tokens": [50364, 264, 6674, 2158, 300, 11843, 294, 264, 3565, 1208, 293, 309, 16390, 82, 309, 370, 294, 341, 50576, 50576, 1389, 309, 576, 16390, 1732, 293, 370, 4412, 264, 6636, 1230, 294, 3565, 1208, 50784, 50784, 486, 1813, 4018, 293, 439, 264, 661, 3547, 486, 1813, 512, 3671, 50938, 50938, 3547, 293, 550, 264, 1874, 295, 341, 307, 1009, 731, 48249, 370, 754, 498, 321, 362, 257, 51180, 51180, 3262, 510, 8046, 406, 665, 457, 570, 25878, 284, 339, 486, 16390, 257, 3262, 51428, 51428, 341, 486, 589, 293, 370, 456, 311, 867, 665, 4112, 281, 818, 3278, 30867, 1230, 472, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.08780812300168551, "compression_ratio": 1.876543209876543, "no_speech_prob": 6.438880518544465e-06}, {"id": 446, "seek": 224080, "start": 2257.1200000000003, "end": 2262.0800000000004, "text": " hundred here previously not good but because pytorch will subtract a hundred", "tokens": [50364, 264, 6674, 2158, 300, 11843, 294, 264, 3565, 1208, 293, 309, 16390, 82, 309, 370, 294, 341, 50576, 50576, 1389, 309, 576, 16390, 1732, 293, 370, 4412, 264, 6636, 1230, 294, 3565, 1208, 50784, 50784, 486, 1813, 4018, 293, 439, 264, 661, 3547, 486, 1813, 512, 3671, 50938, 50938, 3547, 293, 550, 264, 1874, 295, 341, 307, 1009, 731, 48249, 370, 754, 498, 321, 362, 257, 51180, 51180, 3262, 510, 8046, 406, 665, 457, 570, 25878, 284, 339, 486, 16390, 257, 3262, 51428, 51428, 341, 486, 589, 293, 370, 456, 311, 867, 665, 4112, 281, 818, 3278, 30867, 1230, 472, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.08780812300168551, "compression_ratio": 1.876543209876543, "no_speech_prob": 6.438880518544465e-06}, {"id": 447, "seek": 224080, "start": 2262.0800000000004, "end": 2268.88, "text": " this will work and so there's many good reasons to call cross entropy number one", "tokens": [50364, 264, 6674, 2158, 300, 11843, 294, 264, 3565, 1208, 293, 309, 16390, 82, 309, 370, 294, 341, 50576, 50576, 1389, 309, 576, 16390, 1732, 293, 370, 4412, 264, 6636, 1230, 294, 3565, 1208, 50784, 50784, 486, 1813, 4018, 293, 439, 264, 661, 3547, 486, 1813, 512, 3671, 50938, 50938, 3547, 293, 550, 264, 1874, 295, 341, 307, 1009, 731, 48249, 370, 754, 498, 321, 362, 257, 51180, 51180, 3262, 510, 8046, 406, 665, 457, 570, 25878, 284, 339, 486, 16390, 257, 3262, 51428, 51428, 341, 486, 589, 293, 370, 456, 311, 867, 665, 4112, 281, 818, 3278, 30867, 1230, 472, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.08780812300168551, "compression_ratio": 1.876543209876543, "no_speech_prob": 6.438880518544465e-06}, {"id": 448, "seek": 226888, "start": 2268.88, "end": 2272.0, "text": " the forward pass can be much more efficient the backward pass can be much", "tokens": [50364, 264, 2128, 1320, 393, 312, 709, 544, 7148, 264, 23897, 1320, 393, 312, 709, 50520, 50520, 544, 7148, 293, 611, 721, 393, 312, 709, 544, 7866, 984, 731, 48249, 1392, 50766, 50766, 370, 718, 311, 586, 992, 493, 264, 3097, 295, 341, 18161, 2533, 321, 362, 264, 2128, 1320, 321, 51075, 51075, 500, 380, 643, 613, 307, 300, 321, 362, 300, 4470, 307, 2681, 281, 264, 3278, 30867, 51320, 51320, 300, 311, 264, 2128, 1320, 550, 321, 643, 264, 23897, 1320, 700, 321, 528, 281, 992, 264, 51570, 51570, 2771, 2448, 281, 312, 4018, 370, 337, 280, 12, 2181, 335, 6202, 321, 528, 281, 652, 988, 300, 280, 13, 7165, 307, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.12934243575386378, "compression_ratio": 2.0219298245614037, "no_speech_prob": 3.0241011700127274e-05}, {"id": 449, "seek": 226888, "start": 2272.0, "end": 2276.92, "text": " more efficient and also things can be much more numerically well behaved okay", "tokens": [50364, 264, 2128, 1320, 393, 312, 709, 544, 7148, 264, 23897, 1320, 393, 312, 709, 50520, 50520, 544, 7148, 293, 611, 721, 393, 312, 709, 544, 7866, 984, 731, 48249, 1392, 50766, 50766, 370, 718, 311, 586, 992, 493, 264, 3097, 295, 341, 18161, 2533, 321, 362, 264, 2128, 1320, 321, 51075, 51075, 500, 380, 643, 613, 307, 300, 321, 362, 300, 4470, 307, 2681, 281, 264, 3278, 30867, 51320, 51320, 300, 311, 264, 2128, 1320, 550, 321, 643, 264, 23897, 1320, 700, 321, 528, 281, 992, 264, 51570, 51570, 2771, 2448, 281, 312, 4018, 370, 337, 280, 12, 2181, 335, 6202, 321, 528, 281, 652, 988, 300, 280, 13, 7165, 307, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.12934243575386378, "compression_ratio": 2.0219298245614037, "no_speech_prob": 3.0241011700127274e-05}, {"id": 450, "seek": 226888, "start": 2276.92, "end": 2283.1, "text": " so let's now set up the training of this neural net we have the forward pass we", "tokens": [50364, 264, 2128, 1320, 393, 312, 709, 544, 7148, 264, 23897, 1320, 393, 312, 709, 50520, 50520, 544, 7148, 293, 611, 721, 393, 312, 709, 544, 7866, 984, 731, 48249, 1392, 50766, 50766, 370, 718, 311, 586, 992, 493, 264, 3097, 295, 341, 18161, 2533, 321, 362, 264, 2128, 1320, 321, 51075, 51075, 500, 380, 643, 613, 307, 300, 321, 362, 300, 4470, 307, 2681, 281, 264, 3278, 30867, 51320, 51320, 300, 311, 264, 2128, 1320, 550, 321, 643, 264, 23897, 1320, 700, 321, 528, 281, 992, 264, 51570, 51570, 2771, 2448, 281, 312, 4018, 370, 337, 280, 12, 2181, 335, 6202, 321, 528, 281, 652, 988, 300, 280, 13, 7165, 307, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.12934243575386378, "compression_ratio": 2.0219298245614037, "no_speech_prob": 3.0241011700127274e-05}, {"id": 451, "seek": 226888, "start": 2283.1, "end": 2288.0, "text": " don't need these is that we have that loss is equal to the cross entropy", "tokens": [50364, 264, 2128, 1320, 393, 312, 709, 544, 7148, 264, 23897, 1320, 393, 312, 709, 50520, 50520, 544, 7148, 293, 611, 721, 393, 312, 709, 544, 7866, 984, 731, 48249, 1392, 50766, 50766, 370, 718, 311, 586, 992, 493, 264, 3097, 295, 341, 18161, 2533, 321, 362, 264, 2128, 1320, 321, 51075, 51075, 500, 380, 643, 613, 307, 300, 321, 362, 300, 4470, 307, 2681, 281, 264, 3278, 30867, 51320, 51320, 300, 311, 264, 2128, 1320, 550, 321, 643, 264, 23897, 1320, 700, 321, 528, 281, 992, 264, 51570, 51570, 2771, 2448, 281, 312, 4018, 370, 337, 280, 12, 2181, 335, 6202, 321, 528, 281, 652, 988, 300, 280, 13, 7165, 307, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.12934243575386378, "compression_ratio": 2.0219298245614037, "no_speech_prob": 3.0241011700127274e-05}, {"id": 452, "seek": 226888, "start": 2288.0, "end": 2293.0, "text": " that's the forward pass then we need the backward pass first we want to set the", "tokens": [50364, 264, 2128, 1320, 393, 312, 709, 544, 7148, 264, 23897, 1320, 393, 312, 709, 50520, 50520, 544, 7148, 293, 611, 721, 393, 312, 709, 544, 7866, 984, 731, 48249, 1392, 50766, 50766, 370, 718, 311, 586, 992, 493, 264, 3097, 295, 341, 18161, 2533, 321, 362, 264, 2128, 1320, 321, 51075, 51075, 500, 380, 643, 613, 307, 300, 321, 362, 300, 4470, 307, 2681, 281, 264, 3278, 30867, 51320, 51320, 300, 311, 264, 2128, 1320, 550, 321, 643, 264, 23897, 1320, 700, 321, 528, 281, 992, 264, 51570, 51570, 2771, 2448, 281, 312, 4018, 370, 337, 280, 12, 2181, 335, 6202, 321, 528, 281, 652, 988, 300, 280, 13, 7165, 307, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.12934243575386378, "compression_ratio": 2.0219298245614037, "no_speech_prob": 3.0241011700127274e-05}, {"id": 453, "seek": 226888, "start": 2293.0, "end": 2298.1600000000003, "text": " gradients to be zero so for p-parameters we want to make sure that p.grad is", "tokens": [50364, 264, 2128, 1320, 393, 312, 709, 544, 7148, 264, 23897, 1320, 393, 312, 709, 50520, 50520, 544, 7148, 293, 611, 721, 393, 312, 709, 544, 7866, 984, 731, 48249, 1392, 50766, 50766, 370, 718, 311, 586, 992, 493, 264, 3097, 295, 341, 18161, 2533, 321, 362, 264, 2128, 1320, 321, 51075, 51075, 500, 380, 643, 613, 307, 300, 321, 362, 300, 4470, 307, 2681, 281, 264, 3278, 30867, 51320, 51320, 300, 311, 264, 2128, 1320, 550, 321, 643, 264, 23897, 1320, 700, 321, 528, 281, 992, 264, 51570, 51570, 2771, 2448, 281, 312, 4018, 370, 337, 280, 12, 2181, 335, 6202, 321, 528, 281, 652, 988, 300, 280, 13, 7165, 307, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.12934243575386378, "compression_ratio": 2.0219298245614037, "no_speech_prob": 3.0241011700127274e-05}, {"id": 454, "seek": 229816, "start": 2298.16, "end": 2302.2799999999997, "text": " none which is the same as setting it to zero in pytorch and then loss.backward", "tokens": [50364, 6022, 597, 307, 264, 912, 382, 3287, 309, 281, 4018, 294, 25878, 284, 339, 293, 550, 4470, 13, 3207, 1007, 50570, 50570, 281, 1665, 5256, 729, 2771, 2448, 1564, 321, 362, 264, 2771, 2448, 321, 393, 360, 264, 13075, 50776, 50776, 5623, 370, 337, 280, 12, 2181, 335, 6202, 321, 528, 281, 747, 439, 264, 1412, 293, 321, 528, 281, 297, 16032, 51036, 51036, 309, 2539, 3314, 1413, 280, 13, 7165, 293, 550, 321, 528, 281, 7149, 341, 257, 1326, 1413, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.17096348930807675, "compression_ratio": 1.7303370786516854, "no_speech_prob": 4.029407136840746e-06}, {"id": 455, "seek": 229816, "start": 2302.2799999999997, "end": 2306.3999999999996, "text": " to populate those gradients once we have the gradients we can do the parameter", "tokens": [50364, 6022, 597, 307, 264, 912, 382, 3287, 309, 281, 4018, 294, 25878, 284, 339, 293, 550, 4470, 13, 3207, 1007, 50570, 50570, 281, 1665, 5256, 729, 2771, 2448, 1564, 321, 362, 264, 2771, 2448, 321, 393, 360, 264, 13075, 50776, 50776, 5623, 370, 337, 280, 12, 2181, 335, 6202, 321, 528, 281, 747, 439, 264, 1412, 293, 321, 528, 281, 297, 16032, 51036, 51036, 309, 2539, 3314, 1413, 280, 13, 7165, 293, 550, 321, 528, 281, 7149, 341, 257, 1326, 1413, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.17096348930807675, "compression_ratio": 1.7303370786516854, "no_speech_prob": 4.029407136840746e-06}, {"id": 456, "seek": 229816, "start": 2306.3999999999996, "end": 2311.6, "text": " update so for p-parameters we want to take all the data and we want to nudge", "tokens": [50364, 6022, 597, 307, 264, 912, 382, 3287, 309, 281, 4018, 294, 25878, 284, 339, 293, 550, 4470, 13, 3207, 1007, 50570, 50570, 281, 1665, 5256, 729, 2771, 2448, 1564, 321, 362, 264, 2771, 2448, 321, 393, 360, 264, 13075, 50776, 50776, 5623, 370, 337, 280, 12, 2181, 335, 6202, 321, 528, 281, 747, 439, 264, 1412, 293, 321, 528, 281, 297, 16032, 51036, 51036, 309, 2539, 3314, 1413, 280, 13, 7165, 293, 550, 321, 528, 281, 7149, 341, 257, 1326, 1413, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.17096348930807675, "compression_ratio": 1.7303370786516854, "no_speech_prob": 4.029407136840746e-06}, {"id": 457, "seek": 229816, "start": 2311.6, "end": 2322.3999999999996, "text": " it learning rate times p.grad and then we want to repeat this a few times", "tokens": [50364, 6022, 597, 307, 264, 912, 382, 3287, 309, 281, 4018, 294, 25878, 284, 339, 293, 550, 4470, 13, 3207, 1007, 50570, 50570, 281, 1665, 5256, 729, 2771, 2448, 1564, 321, 362, 264, 2771, 2448, 321, 393, 360, 264, 13075, 50776, 50776, 5623, 370, 337, 280, 12, 2181, 335, 6202, 321, 528, 281, 747, 439, 264, 1412, 293, 321, 528, 281, 297, 16032, 51036, 51036, 309, 2539, 3314, 1413, 280, 13, 7165, 293, 550, 321, 528, 281, 7149, 341, 257, 1326, 1413, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.17096348930807675, "compression_ratio": 1.7303370786516854, "no_speech_prob": 4.029407136840746e-06}, {"id": 458, "seek": 232240, "start": 2322.4, "end": 2330.48, "text": " and let's print the loss here as well now this won't suffice and it will create", "tokens": [50364, 293, 718, 311, 4482, 264, 4470, 510, 382, 731, 586, 341, 1582, 380, 3889, 573, 293, 309, 486, 1884, 50768, 50768, 364, 6713, 570, 321, 611, 362, 281, 352, 337, 280, 12, 2181, 335, 6202, 293, 321, 362, 281, 652, 988, 50980, 50980, 300, 280, 13, 265, 358, 3145, 7165, 307, 992, 281, 2074, 294, 25878, 284, 339, 293, 341, 820, 445, 589, 1392, 51454, 51454, 370, 321, 1409, 766, 365, 4470, 295, 3282, 293, 321, 434, 23223, 309, 718, 311, 1190, 2854, 293, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.1254261298613115, "compression_ratio": 1.5572139303482586, "no_speech_prob": 1.095299649023218e-05}, {"id": 459, "seek": 232240, "start": 2330.48, "end": 2334.7200000000003, "text": " an error because we also have to go for p-parameters and we have to make sure", "tokens": [50364, 293, 718, 311, 4482, 264, 4470, 510, 382, 731, 586, 341, 1582, 380, 3889, 573, 293, 309, 486, 1884, 50768, 50768, 364, 6713, 570, 321, 611, 362, 281, 352, 337, 280, 12, 2181, 335, 6202, 293, 321, 362, 281, 652, 988, 50980, 50980, 300, 280, 13, 265, 358, 3145, 7165, 307, 992, 281, 2074, 294, 25878, 284, 339, 293, 341, 820, 445, 589, 1392, 51454, 51454, 370, 321, 1409, 766, 365, 4470, 295, 3282, 293, 321, 434, 23223, 309, 718, 311, 1190, 2854, 293, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.1254261298613115, "compression_ratio": 1.5572139303482586, "no_speech_prob": 1.095299649023218e-05}, {"id": 460, "seek": 232240, "start": 2334.7200000000003, "end": 2344.2000000000003, "text": " that p.requiresgrad is set to true in pytorch and this should just work okay", "tokens": [50364, 293, 718, 311, 4482, 264, 4470, 510, 382, 731, 586, 341, 1582, 380, 3889, 573, 293, 309, 486, 1884, 50768, 50768, 364, 6713, 570, 321, 611, 362, 281, 352, 337, 280, 12, 2181, 335, 6202, 293, 321, 362, 281, 652, 988, 50980, 50980, 300, 280, 13, 265, 358, 3145, 7165, 307, 992, 281, 2074, 294, 25878, 284, 339, 293, 341, 820, 445, 589, 1392, 51454, 51454, 370, 321, 1409, 766, 365, 4470, 295, 3282, 293, 321, 434, 23223, 309, 718, 311, 1190, 2854, 293, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.1254261298613115, "compression_ratio": 1.5572139303482586, "no_speech_prob": 1.095299649023218e-05}, {"id": 461, "seek": 232240, "start": 2344.2000000000003, "end": 2350.44, "text": " so we started off with loss of 17 and we're decreasing it let's run longer and", "tokens": [50364, 293, 718, 311, 4482, 264, 4470, 510, 382, 731, 586, 341, 1582, 380, 3889, 573, 293, 309, 486, 1884, 50768, 50768, 364, 6713, 570, 321, 611, 362, 281, 352, 337, 280, 12, 2181, 335, 6202, 293, 321, 362, 281, 652, 988, 50980, 50980, 300, 280, 13, 265, 358, 3145, 7165, 307, 992, 281, 2074, 294, 25878, 284, 339, 293, 341, 820, 445, 589, 1392, 51454, 51454, 370, 321, 1409, 766, 365, 4470, 295, 3282, 293, 321, 434, 23223, 309, 718, 311, 1190, 2854, 293, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.1254261298613115, "compression_ratio": 1.5572139303482586, "no_speech_prob": 1.095299649023218e-05}, {"id": 462, "seek": 235044, "start": 2350.44, "end": 2359.12, "text": " you see how the loss decreases a lot here so if we just run for a thousand times", "tokens": [50364, 291, 536, 577, 264, 4470, 24108, 257, 688, 510, 370, 498, 321, 445, 1190, 337, 257, 4714, 1413, 50798, 50798, 321, 483, 257, 588, 588, 2295, 4470, 293, 300, 1355, 300, 321, 434, 1455, 588, 665, 50974, 50974, 21264, 586, 264, 1778, 300, 341, 307, 370, 15325, 558, 586, 307, 570, 51224, 51224, 321, 434, 787, 670, 69, 2414, 8858, 5110, 370, 321, 787, 362, 8858, 5110, 295, 264, 700, 1732, 51606, 51606, 2283, 293, 4412, 309, 311, 588, 1858, 281, 652, 341, 18161, 2533, 3318, 787, 613, 732, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.08774369762789819, "compression_ratio": 1.7410714285714286, "no_speech_prob": 2.8572490009537432e-06}, {"id": 463, "seek": 235044, "start": 2359.12, "end": 2362.64, "text": " we get a very very low loss and that means that we're making very good", "tokens": [50364, 291, 536, 577, 264, 4470, 24108, 257, 688, 510, 370, 498, 321, 445, 1190, 337, 257, 4714, 1413, 50798, 50798, 321, 483, 257, 588, 588, 2295, 4470, 293, 300, 1355, 300, 321, 434, 1455, 588, 665, 50974, 50974, 21264, 586, 264, 1778, 300, 341, 307, 370, 15325, 558, 586, 307, 570, 51224, 51224, 321, 434, 787, 670, 69, 2414, 8858, 5110, 370, 321, 787, 362, 8858, 5110, 295, 264, 700, 1732, 51606, 51606, 2283, 293, 4412, 309, 311, 588, 1858, 281, 652, 341, 18161, 2533, 3318, 787, 613, 732, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.08774369762789819, "compression_ratio": 1.7410714285714286, "no_speech_prob": 2.8572490009537432e-06}, {"id": 464, "seek": 235044, "start": 2362.64, "end": 2367.64, "text": " predictions now the reason that this is so straightforward right now is because", "tokens": [50364, 291, 536, 577, 264, 4470, 24108, 257, 688, 510, 370, 498, 321, 445, 1190, 337, 257, 4714, 1413, 50798, 50798, 321, 483, 257, 588, 588, 2295, 4470, 293, 300, 1355, 300, 321, 434, 1455, 588, 665, 50974, 50974, 21264, 586, 264, 1778, 300, 341, 307, 370, 15325, 558, 586, 307, 570, 51224, 51224, 321, 434, 787, 670, 69, 2414, 8858, 5110, 370, 321, 787, 362, 8858, 5110, 295, 264, 700, 1732, 51606, 51606, 2283, 293, 4412, 309, 311, 588, 1858, 281, 652, 341, 18161, 2533, 3318, 787, 613, 732, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.08774369762789819, "compression_ratio": 1.7410714285714286, "no_speech_prob": 2.8572490009537432e-06}, {"id": 465, "seek": 235044, "start": 2367.64, "end": 2375.28, "text": " we're only overfitting 32 examples so we only have 32 examples of the first five", "tokens": [50364, 291, 536, 577, 264, 4470, 24108, 257, 688, 510, 370, 498, 321, 445, 1190, 337, 257, 4714, 1413, 50798, 50798, 321, 483, 257, 588, 588, 2295, 4470, 293, 300, 1355, 300, 321, 434, 1455, 588, 665, 50974, 50974, 21264, 586, 264, 1778, 300, 341, 307, 370, 15325, 558, 586, 307, 570, 51224, 51224, 321, 434, 787, 670, 69, 2414, 8858, 5110, 370, 321, 787, 362, 8858, 5110, 295, 264, 700, 1732, 51606, 51606, 2283, 293, 4412, 309, 311, 588, 1858, 281, 652, 341, 18161, 2533, 3318, 787, 613, 732, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.08774369762789819, "compression_ratio": 1.7410714285714286, "no_speech_prob": 2.8572490009537432e-06}, {"id": 466, "seek": 235044, "start": 2375.28, "end": 2380.28, "text": " words and therefore it's very easy to make this neural net fit only these two", "tokens": [50364, 291, 536, 577, 264, 4470, 24108, 257, 688, 510, 370, 498, 321, 445, 1190, 337, 257, 4714, 1413, 50798, 50798, 321, 483, 257, 588, 588, 2295, 4470, 293, 300, 1355, 300, 321, 434, 1455, 588, 665, 50974, 50974, 21264, 586, 264, 1778, 300, 341, 307, 370, 15325, 558, 586, 307, 570, 51224, 51224, 321, 434, 787, 670, 69, 2414, 8858, 5110, 370, 321, 787, 362, 8858, 5110, 295, 264, 700, 1732, 51606, 51606, 2283, 293, 4412, 309, 311, 588, 1858, 281, 652, 341, 18161, 2533, 3318, 787, 613, 732, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.08774369762789819, "compression_ratio": 1.7410714285714286, "no_speech_prob": 2.8572490009537432e-06}, {"id": 467, "seek": 238028, "start": 2380.28, "end": 2386.4, "text": " 32 examples because we have 3,400 parameters and only 32 examples so", "tokens": [50364, 8858, 5110, 570, 321, 362, 805, 11, 13741, 9834, 293, 787, 8858, 5110, 370, 50670, 50670, 321, 434, 884, 437, 311, 1219, 670, 69, 2414, 257, 2167, 15245, 295, 264, 1412, 293, 1242, 257, 50892, 50892, 588, 2295, 4470, 293, 665, 21264, 457, 300, 311, 445, 570, 321, 362, 370, 867, 51116, 51116, 9834, 337, 370, 1326, 5110, 370, 309, 311, 1858, 281, 652, 341, 312, 588, 2295, 586, 321, 434, 51382, 51382, 406, 1075, 281, 4584, 2293, 4018, 293, 264, 1778, 337, 300, 307, 321, 393, 337, 1365, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.0740316657609837, "compression_ratio": 1.7431192660550459, "no_speech_prob": 1.54457738972269e-05}, {"id": 468, "seek": 238028, "start": 2386.4, "end": 2390.84, "text": " we're doing what's called overfitting a single batch of the data and getting a", "tokens": [50364, 8858, 5110, 570, 321, 362, 805, 11, 13741, 9834, 293, 787, 8858, 5110, 370, 50670, 50670, 321, 434, 884, 437, 311, 1219, 670, 69, 2414, 257, 2167, 15245, 295, 264, 1412, 293, 1242, 257, 50892, 50892, 588, 2295, 4470, 293, 665, 21264, 457, 300, 311, 445, 570, 321, 362, 370, 867, 51116, 51116, 9834, 337, 370, 1326, 5110, 370, 309, 311, 1858, 281, 652, 341, 312, 588, 2295, 586, 321, 434, 51382, 51382, 406, 1075, 281, 4584, 2293, 4018, 293, 264, 1778, 337, 300, 307, 321, 393, 337, 1365, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.0740316657609837, "compression_ratio": 1.7431192660550459, "no_speech_prob": 1.54457738972269e-05}, {"id": 469, "seek": 238028, "start": 2390.84, "end": 2395.32, "text": " very low loss and good predictions but that's just because we have so many", "tokens": [50364, 8858, 5110, 570, 321, 362, 805, 11, 13741, 9834, 293, 787, 8858, 5110, 370, 50670, 50670, 321, 434, 884, 437, 311, 1219, 670, 69, 2414, 257, 2167, 15245, 295, 264, 1412, 293, 1242, 257, 50892, 50892, 588, 2295, 4470, 293, 665, 21264, 457, 300, 311, 445, 570, 321, 362, 370, 867, 51116, 51116, 9834, 337, 370, 1326, 5110, 370, 309, 311, 1858, 281, 652, 341, 312, 588, 2295, 586, 321, 434, 51382, 51382, 406, 1075, 281, 4584, 2293, 4018, 293, 264, 1778, 337, 300, 307, 321, 393, 337, 1365, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.0740316657609837, "compression_ratio": 1.7431192660550459, "no_speech_prob": 1.54457738972269e-05}, {"id": 470, "seek": 238028, "start": 2395.32, "end": 2400.6400000000003, "text": " parameters for so few examples so it's easy to make this be very low now we're", "tokens": [50364, 8858, 5110, 570, 321, 362, 805, 11, 13741, 9834, 293, 787, 8858, 5110, 370, 50670, 50670, 321, 434, 884, 437, 311, 1219, 670, 69, 2414, 257, 2167, 15245, 295, 264, 1412, 293, 1242, 257, 50892, 50892, 588, 2295, 4470, 293, 665, 21264, 457, 300, 311, 445, 570, 321, 362, 370, 867, 51116, 51116, 9834, 337, 370, 1326, 5110, 370, 309, 311, 1858, 281, 652, 341, 312, 588, 2295, 586, 321, 434, 51382, 51382, 406, 1075, 281, 4584, 2293, 4018, 293, 264, 1778, 337, 300, 307, 321, 393, 337, 1365, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.0740316657609837, "compression_ratio": 1.7431192660550459, "no_speech_prob": 1.54457738972269e-05}, {"id": 471, "seek": 238028, "start": 2400.6400000000003, "end": 2404.76, "text": " not able to achieve exactly zero and the reason for that is we can for example", "tokens": [50364, 8858, 5110, 570, 321, 362, 805, 11, 13741, 9834, 293, 787, 8858, 5110, 370, 50670, 50670, 321, 434, 884, 437, 311, 1219, 670, 69, 2414, 257, 2167, 15245, 295, 264, 1412, 293, 1242, 257, 50892, 50892, 588, 2295, 4470, 293, 665, 21264, 457, 300, 311, 445, 570, 321, 362, 370, 867, 51116, 51116, 9834, 337, 370, 1326, 5110, 370, 309, 311, 1858, 281, 652, 341, 312, 588, 2295, 586, 321, 434, 51382, 51382, 406, 1075, 281, 4584, 2293, 4018, 293, 264, 1778, 337, 300, 307, 321, 393, 337, 1365, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.0740316657609837, "compression_ratio": 1.7431192660550459, "no_speech_prob": 1.54457738972269e-05}, {"id": 472, "seek": 240476, "start": 2404.76, "end": 2411.6000000000004, "text": " look at logits which are being predicted and we can look at the max along the", "tokens": [50364, 574, 412, 3565, 1208, 597, 366, 885, 19147, 293, 321, 393, 574, 412, 264, 11469, 2051, 264, 50706, 50706, 700, 10139, 293, 294, 25878, 284, 339, 11469, 7122, 1293, 264, 3539, 4190, 300, 747, 51021, 51021, 322, 264, 6674, 1230, 457, 611, 264, 43840, 295, 613, 293, 291, 603, 536, 300, 264, 51272, 51272, 43840, 366, 588, 1998, 281, 264, 16949, 457, 294, 512, 3331, 436, 743, 337, 1365, 294, 51595, 51595], "temperature": 0.0, "avg_logprob": -0.0795867700834532, "compression_ratio": 1.6631016042780749, "no_speech_prob": 6.961751296330476e-06}, {"id": 473, "seek": 240476, "start": 2411.6000000000004, "end": 2417.9, "text": " first dimension and in pytorch max reports both the actual values that take", "tokens": [50364, 574, 412, 3565, 1208, 597, 366, 885, 19147, 293, 321, 393, 574, 412, 264, 11469, 2051, 264, 50706, 50706, 700, 10139, 293, 294, 25878, 284, 339, 11469, 7122, 1293, 264, 3539, 4190, 300, 747, 51021, 51021, 322, 264, 6674, 1230, 457, 611, 264, 43840, 295, 613, 293, 291, 603, 536, 300, 264, 51272, 51272, 43840, 366, 588, 1998, 281, 264, 16949, 457, 294, 512, 3331, 436, 743, 337, 1365, 294, 51595, 51595], "temperature": 0.0, "avg_logprob": -0.0795867700834532, "compression_ratio": 1.6631016042780749, "no_speech_prob": 6.961751296330476e-06}, {"id": 474, "seek": 240476, "start": 2417.9, "end": 2422.92, "text": " on the maximum number but also the indices of these and you'll see that the", "tokens": [50364, 574, 412, 3565, 1208, 597, 366, 885, 19147, 293, 321, 393, 574, 412, 264, 11469, 2051, 264, 50706, 50706, 700, 10139, 293, 294, 25878, 284, 339, 11469, 7122, 1293, 264, 3539, 4190, 300, 747, 51021, 51021, 322, 264, 6674, 1230, 457, 611, 264, 43840, 295, 613, 293, 291, 603, 536, 300, 264, 51272, 51272, 43840, 366, 588, 1998, 281, 264, 16949, 457, 294, 512, 3331, 436, 743, 337, 1365, 294, 51595, 51595], "temperature": 0.0, "avg_logprob": -0.0795867700834532, "compression_ratio": 1.6631016042780749, "no_speech_prob": 6.961751296330476e-06}, {"id": 475, "seek": 240476, "start": 2422.92, "end": 2429.38, "text": " indices are very close to the labels but in some cases they differ for example in", "tokens": [50364, 574, 412, 3565, 1208, 597, 366, 885, 19147, 293, 321, 393, 574, 412, 264, 11469, 2051, 264, 50706, 50706, 700, 10139, 293, 294, 25878, 284, 339, 11469, 7122, 1293, 264, 3539, 4190, 300, 747, 51021, 51021, 322, 264, 6674, 1230, 457, 611, 264, 43840, 295, 613, 293, 291, 603, 536, 300, 264, 51272, 51272, 43840, 366, 588, 1998, 281, 264, 16949, 457, 294, 512, 3331, 436, 743, 337, 1365, 294, 51595, 51595], "temperature": 0.0, "avg_logprob": -0.0795867700834532, "compression_ratio": 1.6631016042780749, "no_speech_prob": 6.961751296330476e-06}, {"id": 476, "seek": 242938, "start": 2429.38, "end": 2434.6, "text": " this very first example the predicted index is 19 but the label is 5 and", "tokens": [50364, 341, 588, 700, 1365, 264, 19147, 8186, 307, 1294, 457, 264, 7645, 307, 1025, 293, 50625, 50625, 321, 434, 406, 1075, 281, 652, 4470, 312, 1958, 293, 17879, 300, 311, 570, 510, 264, 50941, 50941, 588, 700, 420, 264, 44746, 900, 8186, 307, 264, 1365, 689, 5893, 5893, 5893, 307, 3442, 281, 51143, 51143, 6069, 462, 457, 291, 536, 577, 5893, 5893, 5893, 307, 611, 3442, 281, 6069, 364, 422, 293, 5893, 51370, 51370, 5893, 5893, 307, 611, 3442, 281, 6069, 364, 286, 293, 550, 318, 382, 731, 293, 370, 1936, 462, 11, 422, 11, 316, 11, 51661, 51661], "temperature": 0.0, "avg_logprob": -0.1424716345154413, "compression_ratio": 1.8708133971291867, "no_speech_prob": 5.954606876912294e-06}, {"id": 477, "seek": 242938, "start": 2434.6, "end": 2440.92, "text": " we're not able to make loss be 0 and fundamentally that's because here the", "tokens": [50364, 341, 588, 700, 1365, 264, 19147, 8186, 307, 1294, 457, 264, 7645, 307, 1025, 293, 50625, 50625, 321, 434, 406, 1075, 281, 652, 4470, 312, 1958, 293, 17879, 300, 311, 570, 510, 264, 50941, 50941, 588, 700, 420, 264, 44746, 900, 8186, 307, 264, 1365, 689, 5893, 5893, 5893, 307, 3442, 281, 51143, 51143, 6069, 462, 457, 291, 536, 577, 5893, 5893, 5893, 307, 611, 3442, 281, 6069, 364, 422, 293, 5893, 51370, 51370, 5893, 5893, 307, 611, 3442, 281, 6069, 364, 286, 293, 550, 318, 382, 731, 293, 370, 1936, 462, 11, 422, 11, 316, 11, 51661, 51661], "temperature": 0.0, "avg_logprob": -0.1424716345154413, "compression_ratio": 1.8708133971291867, "no_speech_prob": 5.954606876912294e-06}, {"id": 478, "seek": 242938, "start": 2440.92, "end": 2444.96, "text": " very first or the zeroth index is the example where dot dot dot is supposed to", "tokens": [50364, 341, 588, 700, 1365, 264, 19147, 8186, 307, 1294, 457, 264, 7645, 307, 1025, 293, 50625, 50625, 321, 434, 406, 1075, 281, 652, 4470, 312, 1958, 293, 17879, 300, 311, 570, 510, 264, 50941, 50941, 588, 700, 420, 264, 44746, 900, 8186, 307, 264, 1365, 689, 5893, 5893, 5893, 307, 3442, 281, 51143, 51143, 6069, 462, 457, 291, 536, 577, 5893, 5893, 5893, 307, 611, 3442, 281, 6069, 364, 422, 293, 5893, 51370, 51370, 5893, 5893, 307, 611, 3442, 281, 6069, 364, 286, 293, 550, 318, 382, 731, 293, 370, 1936, 462, 11, 422, 11, 316, 11, 51661, 51661], "temperature": 0.0, "avg_logprob": -0.1424716345154413, "compression_ratio": 1.8708133971291867, "no_speech_prob": 5.954606876912294e-06}, {"id": 479, "seek": 242938, "start": 2444.96, "end": 2449.5, "text": " predict E but you see how dot dot dot is also supposed to predict an O and dot", "tokens": [50364, 341, 588, 700, 1365, 264, 19147, 8186, 307, 1294, 457, 264, 7645, 307, 1025, 293, 50625, 50625, 321, 434, 406, 1075, 281, 652, 4470, 312, 1958, 293, 17879, 300, 311, 570, 510, 264, 50941, 50941, 588, 700, 420, 264, 44746, 900, 8186, 307, 264, 1365, 689, 5893, 5893, 5893, 307, 3442, 281, 51143, 51143, 6069, 462, 457, 291, 536, 577, 5893, 5893, 5893, 307, 611, 3442, 281, 6069, 364, 422, 293, 5893, 51370, 51370, 5893, 5893, 307, 611, 3442, 281, 6069, 364, 286, 293, 550, 318, 382, 731, 293, 370, 1936, 462, 11, 422, 11, 316, 11, 51661, 51661], "temperature": 0.0, "avg_logprob": -0.1424716345154413, "compression_ratio": 1.8708133971291867, "no_speech_prob": 5.954606876912294e-06}, {"id": 480, "seek": 242938, "start": 2449.5, "end": 2455.32, "text": " dot dot is also supposed to predict an I and then S as well and so basically E, O, A,", "tokens": [50364, 341, 588, 700, 1365, 264, 19147, 8186, 307, 1294, 457, 264, 7645, 307, 1025, 293, 50625, 50625, 321, 434, 406, 1075, 281, 652, 4470, 312, 1958, 293, 17879, 300, 311, 570, 510, 264, 50941, 50941, 588, 700, 420, 264, 44746, 900, 8186, 307, 264, 1365, 689, 5893, 5893, 5893, 307, 3442, 281, 51143, 51143, 6069, 462, 457, 291, 536, 577, 5893, 5893, 5893, 307, 611, 3442, 281, 6069, 364, 422, 293, 5893, 51370, 51370, 5893, 5893, 307, 611, 3442, 281, 6069, 364, 286, 293, 550, 318, 382, 731, 293, 370, 1936, 462, 11, 422, 11, 316, 11, 51661, 51661], "temperature": 0.0, "avg_logprob": -0.1424716345154413, "compression_ratio": 1.8708133971291867, "no_speech_prob": 5.954606876912294e-06}, {"id": 481, "seek": 245532, "start": 2455.32, "end": 2460.56, "text": " or S are all possible outcomes in a training set for the exact same input so", "tokens": [50364, 420, 318, 366, 439, 1944, 10070, 294, 257, 3097, 992, 337, 264, 1900, 912, 4846, 370, 50626, 50626, 321, 434, 406, 1075, 281, 2584, 670, 6845, 293, 293, 652, 264, 4470, 312, 2293, 4018, 457, 50932, 50932, 321, 434, 1242, 588, 1998, 294, 264, 3331, 689, 456, 311, 257, 3845, 4846, 337, 257, 51168, 51168, 3845, 5598, 294, 729, 3331, 321, 360, 437, 311, 1219, 670, 6845, 293, 321, 1936, 51376, 51376, 483, 264, 1900, 912, 293, 264, 1900, 3006, 1874, 370, 586, 439, 321, 362, 281, 360, 307, 321, 51680, 51680, 445, 643, 281, 652, 988, 300, 321, 1401, 294, 264, 1577, 1412, 992, 293, 19719, 264, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.08935714619500297, "compression_ratio": 1.8421052631578947, "no_speech_prob": 7.5276093411957845e-06}, {"id": 482, "seek": 245532, "start": 2460.56, "end": 2466.6800000000003, "text": " we're not able to completely overfit and and make the loss be exactly zero but", "tokens": [50364, 420, 318, 366, 439, 1944, 10070, 294, 257, 3097, 992, 337, 264, 1900, 912, 4846, 370, 50626, 50626, 321, 434, 406, 1075, 281, 2584, 670, 6845, 293, 293, 652, 264, 4470, 312, 2293, 4018, 457, 50932, 50932, 321, 434, 1242, 588, 1998, 294, 264, 3331, 689, 456, 311, 257, 3845, 4846, 337, 257, 51168, 51168, 3845, 5598, 294, 729, 3331, 321, 360, 437, 311, 1219, 670, 6845, 293, 321, 1936, 51376, 51376, 483, 264, 1900, 912, 293, 264, 1900, 3006, 1874, 370, 586, 439, 321, 362, 281, 360, 307, 321, 51680, 51680, 445, 643, 281, 652, 988, 300, 321, 1401, 294, 264, 1577, 1412, 992, 293, 19719, 264, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.08935714619500297, "compression_ratio": 1.8421052631578947, "no_speech_prob": 7.5276093411957845e-06}, {"id": 483, "seek": 245532, "start": 2466.6800000000003, "end": 2471.4, "text": " we're getting very close in the cases where there's a unique input for a", "tokens": [50364, 420, 318, 366, 439, 1944, 10070, 294, 257, 3097, 992, 337, 264, 1900, 912, 4846, 370, 50626, 50626, 321, 434, 406, 1075, 281, 2584, 670, 6845, 293, 293, 652, 264, 4470, 312, 2293, 4018, 457, 50932, 50932, 321, 434, 1242, 588, 1998, 294, 264, 3331, 689, 456, 311, 257, 3845, 4846, 337, 257, 51168, 51168, 3845, 5598, 294, 729, 3331, 321, 360, 437, 311, 1219, 670, 6845, 293, 321, 1936, 51376, 51376, 483, 264, 1900, 912, 293, 264, 1900, 3006, 1874, 370, 586, 439, 321, 362, 281, 360, 307, 321, 51680, 51680, 445, 643, 281, 652, 988, 300, 321, 1401, 294, 264, 1577, 1412, 992, 293, 19719, 264, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.08935714619500297, "compression_ratio": 1.8421052631578947, "no_speech_prob": 7.5276093411957845e-06}, {"id": 484, "seek": 245532, "start": 2471.4, "end": 2475.56, "text": " unique output in those cases we do what's called overfit and we basically", "tokens": [50364, 420, 318, 366, 439, 1944, 10070, 294, 257, 3097, 992, 337, 264, 1900, 912, 4846, 370, 50626, 50626, 321, 434, 406, 1075, 281, 2584, 670, 6845, 293, 293, 652, 264, 4470, 312, 2293, 4018, 457, 50932, 50932, 321, 434, 1242, 588, 1998, 294, 264, 3331, 689, 456, 311, 257, 3845, 4846, 337, 257, 51168, 51168, 3845, 5598, 294, 729, 3331, 321, 360, 437, 311, 1219, 670, 6845, 293, 321, 1936, 51376, 51376, 483, 264, 1900, 912, 293, 264, 1900, 3006, 1874, 370, 586, 439, 321, 362, 281, 360, 307, 321, 51680, 51680, 445, 643, 281, 652, 988, 300, 321, 1401, 294, 264, 1577, 1412, 992, 293, 19719, 264, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.08935714619500297, "compression_ratio": 1.8421052631578947, "no_speech_prob": 7.5276093411957845e-06}, {"id": 485, "seek": 245532, "start": 2475.56, "end": 2481.6400000000003, "text": " get the exact same and the exact correct result so now all we have to do is we", "tokens": [50364, 420, 318, 366, 439, 1944, 10070, 294, 257, 3097, 992, 337, 264, 1900, 912, 4846, 370, 50626, 50626, 321, 434, 406, 1075, 281, 2584, 670, 6845, 293, 293, 652, 264, 4470, 312, 2293, 4018, 457, 50932, 50932, 321, 434, 1242, 588, 1998, 294, 264, 3331, 689, 456, 311, 257, 3845, 4846, 337, 257, 51168, 51168, 3845, 5598, 294, 729, 3331, 321, 360, 437, 311, 1219, 670, 6845, 293, 321, 1936, 51376, 51376, 483, 264, 1900, 912, 293, 264, 1900, 3006, 1874, 370, 586, 439, 321, 362, 281, 360, 307, 321, 51680, 51680, 445, 643, 281, 652, 988, 300, 321, 1401, 294, 264, 1577, 1412, 992, 293, 19719, 264, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.08935714619500297, "compression_ratio": 1.8421052631578947, "no_speech_prob": 7.5276093411957845e-06}, {"id": 486, "seek": 245532, "start": 2481.6400000000003, "end": 2484.48, "text": " just need to make sure that we read in the full data set and optimize the", "tokens": [50364, 420, 318, 366, 439, 1944, 10070, 294, 257, 3097, 992, 337, 264, 1900, 912, 4846, 370, 50626, 50626, 321, 434, 406, 1075, 281, 2584, 670, 6845, 293, 293, 652, 264, 4470, 312, 2293, 4018, 457, 50932, 50932, 321, 434, 1242, 588, 1998, 294, 264, 3331, 689, 456, 311, 257, 3845, 4846, 337, 257, 51168, 51168, 3845, 5598, 294, 729, 3331, 321, 360, 437, 311, 1219, 670, 6845, 293, 321, 1936, 51376, 51376, 483, 264, 1900, 912, 293, 264, 1900, 3006, 1874, 370, 586, 439, 321, 362, 281, 360, 307, 321, 51680, 51680, 445, 643, 281, 652, 988, 300, 321, 1401, 294, 264, 1577, 1412, 992, 293, 19719, 264, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.08935714619500297, "compression_ratio": 1.8421052631578947, "no_speech_prob": 7.5276093411957845e-06}, {"id": 487, "seek": 248448, "start": 2484.48, "end": 2489.76, "text": " neural net okay so let's swing back up where we created the data set and we see", "tokens": [50364, 18161, 2533, 1392, 370, 718, 311, 11173, 646, 493, 689, 321, 2942, 264, 1412, 992, 293, 321, 536, 50628, 50628, 300, 510, 321, 787, 764, 264, 700, 1732, 2283, 370, 718, 385, 586, 23525, 341, 293, 718, 50846, 50846, 385, 23525, 264, 4482, 12363, 5911, 321, 1116, 312, 14699, 636, 886, 709, 293, 370, 51065, 51065, 562, 321, 1399, 264, 1577, 1412, 992, 295, 439, 264, 2283, 321, 586, 632, 5853, 23, 11, 1360, 5110, 51318, 51318, 2602, 295, 445, 8858, 370, 718, 311, 586, 11369, 646, 760, 264, 1412, 992, 307, 709, 4833, 51586, 51586, 321, 5883, 1125, 264, 17443, 264, 912, 1230, 295, 9834, 436, 439, 3651, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.13301957392059596, "compression_ratio": 1.773076923076923, "no_speech_prob": 1.8738415974439704e-06}, {"id": 488, "seek": 248448, "start": 2489.76, "end": 2494.12, "text": " that here we only use the first five words so let me now erase this and let", "tokens": [50364, 18161, 2533, 1392, 370, 718, 311, 11173, 646, 493, 689, 321, 2942, 264, 1412, 992, 293, 321, 536, 50628, 50628, 300, 510, 321, 787, 764, 264, 700, 1732, 2283, 370, 718, 385, 586, 23525, 341, 293, 718, 50846, 50846, 385, 23525, 264, 4482, 12363, 5911, 321, 1116, 312, 14699, 636, 886, 709, 293, 370, 51065, 51065, 562, 321, 1399, 264, 1577, 1412, 992, 295, 439, 264, 2283, 321, 586, 632, 5853, 23, 11, 1360, 5110, 51318, 51318, 2602, 295, 445, 8858, 370, 718, 311, 586, 11369, 646, 760, 264, 1412, 992, 307, 709, 4833, 51586, 51586, 321, 5883, 1125, 264, 17443, 264, 912, 1230, 295, 9834, 436, 439, 3651, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.13301957392059596, "compression_ratio": 1.773076923076923, "no_speech_prob": 1.8738415974439704e-06}, {"id": 489, "seek": 248448, "start": 2494.12, "end": 2498.5, "text": " me erase the print statements otherwise we'd be printing way too much and so", "tokens": [50364, 18161, 2533, 1392, 370, 718, 311, 11173, 646, 493, 689, 321, 2942, 264, 1412, 992, 293, 321, 536, 50628, 50628, 300, 510, 321, 787, 764, 264, 700, 1732, 2283, 370, 718, 385, 586, 23525, 341, 293, 718, 50846, 50846, 385, 23525, 264, 4482, 12363, 5911, 321, 1116, 312, 14699, 636, 886, 709, 293, 370, 51065, 51065, 562, 321, 1399, 264, 1577, 1412, 992, 295, 439, 264, 2283, 321, 586, 632, 5853, 23, 11, 1360, 5110, 51318, 51318, 2602, 295, 445, 8858, 370, 718, 311, 586, 11369, 646, 760, 264, 1412, 992, 307, 709, 4833, 51586, 51586, 321, 5883, 1125, 264, 17443, 264, 912, 1230, 295, 9834, 436, 439, 3651, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.13301957392059596, "compression_ratio": 1.773076923076923, "no_speech_prob": 1.8738415974439704e-06}, {"id": 490, "seek": 248448, "start": 2498.5, "end": 2503.56, "text": " when we process the full data set of all the words we now had 228,000 examples", "tokens": [50364, 18161, 2533, 1392, 370, 718, 311, 11173, 646, 493, 689, 321, 2942, 264, 1412, 992, 293, 321, 536, 50628, 50628, 300, 510, 321, 787, 764, 264, 700, 1732, 2283, 370, 718, 385, 586, 23525, 341, 293, 718, 50846, 50846, 385, 23525, 264, 4482, 12363, 5911, 321, 1116, 312, 14699, 636, 886, 709, 293, 370, 51065, 51065, 562, 321, 1399, 264, 1577, 1412, 992, 295, 439, 264, 2283, 321, 586, 632, 5853, 23, 11, 1360, 5110, 51318, 51318, 2602, 295, 445, 8858, 370, 718, 311, 586, 11369, 646, 760, 264, 1412, 992, 307, 709, 4833, 51586, 51586, 321, 5883, 1125, 264, 17443, 264, 912, 1230, 295, 9834, 436, 439, 3651, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.13301957392059596, "compression_ratio": 1.773076923076923, "no_speech_prob": 1.8738415974439704e-06}, {"id": 491, "seek": 248448, "start": 2503.56, "end": 2508.92, "text": " instead of just 32 so let's now scroll back down the data set is much larger", "tokens": [50364, 18161, 2533, 1392, 370, 718, 311, 11173, 646, 493, 689, 321, 2942, 264, 1412, 992, 293, 321, 536, 50628, 50628, 300, 510, 321, 787, 764, 264, 700, 1732, 2283, 370, 718, 385, 586, 23525, 341, 293, 718, 50846, 50846, 385, 23525, 264, 4482, 12363, 5911, 321, 1116, 312, 14699, 636, 886, 709, 293, 370, 51065, 51065, 562, 321, 1399, 264, 1577, 1412, 992, 295, 439, 264, 2283, 321, 586, 632, 5853, 23, 11, 1360, 5110, 51318, 51318, 2602, 295, 445, 8858, 370, 718, 311, 586, 11369, 646, 760, 264, 1412, 992, 307, 709, 4833, 51586, 51586, 321, 5883, 1125, 264, 17443, 264, 912, 1230, 295, 9834, 436, 439, 3651, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.13301957392059596, "compression_ratio": 1.773076923076923, "no_speech_prob": 1.8738415974439704e-06}, {"id": 492, "seek": 248448, "start": 2508.92, "end": 2513.12, "text": " we initialize the weights the same number of parameters they all require", "tokens": [50364, 18161, 2533, 1392, 370, 718, 311, 11173, 646, 493, 689, 321, 2942, 264, 1412, 992, 293, 321, 536, 50628, 50628, 300, 510, 321, 787, 764, 264, 700, 1732, 2283, 370, 718, 385, 586, 23525, 341, 293, 718, 50846, 50846, 385, 23525, 264, 4482, 12363, 5911, 321, 1116, 312, 14699, 636, 886, 709, 293, 370, 51065, 51065, 562, 321, 1399, 264, 1577, 1412, 992, 295, 439, 264, 2283, 321, 586, 632, 5853, 23, 11, 1360, 5110, 51318, 51318, 2602, 295, 445, 8858, 370, 718, 311, 586, 11369, 646, 760, 264, 1412, 992, 307, 709, 4833, 51586, 51586, 321, 5883, 1125, 264, 17443, 264, 912, 1230, 295, 9834, 436, 439, 3651, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.13301957392059596, "compression_ratio": 1.773076923076923, "no_speech_prob": 1.8738415974439704e-06}, {"id": 493, "seek": 251312, "start": 2513.12, "end": 2519.12, "text": " gradients and then let's push this print I lost that item to be here and let's", "tokens": [50364, 2771, 2448, 293, 550, 718, 311, 2944, 341, 4482, 286, 2731, 300, 3174, 281, 312, 510, 293, 718, 311, 50664, 50664, 445, 536, 577, 264, 19618, 1709, 498, 321, 1190, 341, 1392, 370, 321, 1409, 365, 257, 50982, 50982, 6457, 1090, 4470, 293, 550, 382, 321, 434, 40425, 264, 4470, 307, 1348, 760, 457, 51324, 51324, 291, 603, 3449, 300, 309, 2516, 1596, 257, 857, 295, 565, 337, 633, 2167, 24784, 370, 51516, 51516, 718, 311, 767, 2985, 300, 570, 321, 434, 884, 636, 886, 709, 589, 2128, 278, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.07156537681497553, "compression_ratio": 1.6872246696035242, "no_speech_prob": 2.190732175222365e-06}, {"id": 494, "seek": 251312, "start": 2519.12, "end": 2525.48, "text": " just see how the optimization goes if we run this okay so we started with a", "tokens": [50364, 2771, 2448, 293, 550, 718, 311, 2944, 341, 4482, 286, 2731, 300, 3174, 281, 312, 510, 293, 718, 311, 50664, 50664, 445, 536, 577, 264, 19618, 1709, 498, 321, 1190, 341, 1392, 370, 321, 1409, 365, 257, 50982, 50982, 6457, 1090, 4470, 293, 550, 382, 321, 434, 40425, 264, 4470, 307, 1348, 760, 457, 51324, 51324, 291, 603, 3449, 300, 309, 2516, 1596, 257, 857, 295, 565, 337, 633, 2167, 24784, 370, 51516, 51516, 718, 311, 767, 2985, 300, 570, 321, 434, 884, 636, 886, 709, 589, 2128, 278, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.07156537681497553, "compression_ratio": 1.6872246696035242, "no_speech_prob": 2.190732175222365e-06}, {"id": 495, "seek": 251312, "start": 2525.48, "end": 2532.3199999999997, "text": " fairly high loss and then as we're optimizing the loss is coming down but", "tokens": [50364, 2771, 2448, 293, 550, 718, 311, 2944, 341, 4482, 286, 2731, 300, 3174, 281, 312, 510, 293, 718, 311, 50664, 50664, 445, 536, 577, 264, 19618, 1709, 498, 321, 1190, 341, 1392, 370, 321, 1409, 365, 257, 50982, 50982, 6457, 1090, 4470, 293, 550, 382, 321, 434, 40425, 264, 4470, 307, 1348, 760, 457, 51324, 51324, 291, 603, 3449, 300, 309, 2516, 1596, 257, 857, 295, 565, 337, 633, 2167, 24784, 370, 51516, 51516, 718, 311, 767, 2985, 300, 570, 321, 434, 884, 636, 886, 709, 589, 2128, 278, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.07156537681497553, "compression_ratio": 1.6872246696035242, "no_speech_prob": 2.190732175222365e-06}, {"id": 496, "seek": 251312, "start": 2532.3199999999997, "end": 2536.16, "text": " you'll notice that it takes quite a bit of time for every single iteration so", "tokens": [50364, 2771, 2448, 293, 550, 718, 311, 2944, 341, 4482, 286, 2731, 300, 3174, 281, 312, 510, 293, 718, 311, 50664, 50664, 445, 536, 577, 264, 19618, 1709, 498, 321, 1190, 341, 1392, 370, 321, 1409, 365, 257, 50982, 50982, 6457, 1090, 4470, 293, 550, 382, 321, 434, 40425, 264, 4470, 307, 1348, 760, 457, 51324, 51324, 291, 603, 3449, 300, 309, 2516, 1596, 257, 857, 295, 565, 337, 633, 2167, 24784, 370, 51516, 51516, 718, 311, 767, 2985, 300, 570, 321, 434, 884, 636, 886, 709, 589, 2128, 278, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.07156537681497553, "compression_ratio": 1.6872246696035242, "no_speech_prob": 2.190732175222365e-06}, {"id": 497, "seek": 251312, "start": 2536.16, "end": 2539.68, "text": " let's actually address that because we're doing way too much work forwarding", "tokens": [50364, 2771, 2448, 293, 550, 718, 311, 2944, 341, 4482, 286, 2731, 300, 3174, 281, 312, 510, 293, 718, 311, 50664, 50664, 445, 536, 577, 264, 19618, 1709, 498, 321, 1190, 341, 1392, 370, 321, 1409, 365, 257, 50982, 50982, 6457, 1090, 4470, 293, 550, 382, 321, 434, 40425, 264, 4470, 307, 1348, 760, 457, 51324, 51324, 291, 603, 3449, 300, 309, 2516, 1596, 257, 857, 295, 565, 337, 633, 2167, 24784, 370, 51516, 51516, 718, 311, 767, 2985, 300, 570, 321, 434, 884, 636, 886, 709, 589, 2128, 278, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.07156537681497553, "compression_ratio": 1.6872246696035242, "no_speech_prob": 2.190732175222365e-06}, {"id": 498, "seek": 253968, "start": 2539.68, "end": 2544.68, "text": " and backwarding 228,000 examples in practice what people usually do is they", "tokens": [50364, 293, 23897, 278, 5853, 23, 11, 1360, 5110, 294, 3124, 437, 561, 2673, 360, 307, 436, 50614, 50614, 2042, 2128, 293, 23897, 1320, 364, 5623, 322, 867, 15245, 279, 295, 264, 1412, 370, 50878, 50878, 437, 321, 486, 528, 281, 360, 307, 321, 528, 281, 16979, 3048, 512, 8044, 295, 264, 51060, 51060, 1412, 992, 293, 300, 311, 257, 8382, 15245, 293, 550, 787, 2128, 23897, 293, 5623, 51248, 51248, 322, 300, 707, 8382, 15245, 293, 550, 321, 44497, 322, 729, 867, 15245, 279, 370, 294, 51512, 51512, 25878, 284, 339, 321, 393, 337, 1365, 764, 3930, 339, 13, 3699, 471, 321, 393, 8460, 3547, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.12900617581988694, "compression_ratio": 1.8969957081545064, "no_speech_prob": 1.4738468962605111e-05}, {"id": 499, "seek": 253968, "start": 2544.68, "end": 2549.96, "text": " perform forward and backward pass an update on many batches of the data so", "tokens": [50364, 293, 23897, 278, 5853, 23, 11, 1360, 5110, 294, 3124, 437, 561, 2673, 360, 307, 436, 50614, 50614, 2042, 2128, 293, 23897, 1320, 364, 5623, 322, 867, 15245, 279, 295, 264, 1412, 370, 50878, 50878, 437, 321, 486, 528, 281, 360, 307, 321, 528, 281, 16979, 3048, 512, 8044, 295, 264, 51060, 51060, 1412, 992, 293, 300, 311, 257, 8382, 15245, 293, 550, 787, 2128, 23897, 293, 5623, 51248, 51248, 322, 300, 707, 8382, 15245, 293, 550, 321, 44497, 322, 729, 867, 15245, 279, 370, 294, 51512, 51512, 25878, 284, 339, 321, 393, 337, 1365, 764, 3930, 339, 13, 3699, 471, 321, 393, 8460, 3547, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.12900617581988694, "compression_ratio": 1.8969957081545064, "no_speech_prob": 1.4738468962605111e-05}, {"id": 500, "seek": 253968, "start": 2549.96, "end": 2553.6, "text": " what we will want to do is we want to randomly select some portion of the", "tokens": [50364, 293, 23897, 278, 5853, 23, 11, 1360, 5110, 294, 3124, 437, 561, 2673, 360, 307, 436, 50614, 50614, 2042, 2128, 293, 23897, 1320, 364, 5623, 322, 867, 15245, 279, 295, 264, 1412, 370, 50878, 50878, 437, 321, 486, 528, 281, 360, 307, 321, 528, 281, 16979, 3048, 512, 8044, 295, 264, 51060, 51060, 1412, 992, 293, 300, 311, 257, 8382, 15245, 293, 550, 787, 2128, 23897, 293, 5623, 51248, 51248, 322, 300, 707, 8382, 15245, 293, 550, 321, 44497, 322, 729, 867, 15245, 279, 370, 294, 51512, 51512, 25878, 284, 339, 321, 393, 337, 1365, 764, 3930, 339, 13, 3699, 471, 321, 393, 8460, 3547, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.12900617581988694, "compression_ratio": 1.8969957081545064, "no_speech_prob": 1.4738468962605111e-05}, {"id": 501, "seek": 253968, "start": 2553.6, "end": 2557.3599999999997, "text": " data set and that's a mini batch and then only forward backward and update", "tokens": [50364, 293, 23897, 278, 5853, 23, 11, 1360, 5110, 294, 3124, 437, 561, 2673, 360, 307, 436, 50614, 50614, 2042, 2128, 293, 23897, 1320, 364, 5623, 322, 867, 15245, 279, 295, 264, 1412, 370, 50878, 50878, 437, 321, 486, 528, 281, 360, 307, 321, 528, 281, 16979, 3048, 512, 8044, 295, 264, 51060, 51060, 1412, 992, 293, 300, 311, 257, 8382, 15245, 293, 550, 787, 2128, 23897, 293, 5623, 51248, 51248, 322, 300, 707, 8382, 15245, 293, 550, 321, 44497, 322, 729, 867, 15245, 279, 370, 294, 51512, 51512, 25878, 284, 339, 321, 393, 337, 1365, 764, 3930, 339, 13, 3699, 471, 321, 393, 8460, 3547, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.12900617581988694, "compression_ratio": 1.8969957081545064, "no_speech_prob": 1.4738468962605111e-05}, {"id": 502, "seek": 253968, "start": 2557.3599999999997, "end": 2562.64, "text": " on that little mini batch and then we iterate on those many batches so in", "tokens": [50364, 293, 23897, 278, 5853, 23, 11, 1360, 5110, 294, 3124, 437, 561, 2673, 360, 307, 436, 50614, 50614, 2042, 2128, 293, 23897, 1320, 364, 5623, 322, 867, 15245, 279, 295, 264, 1412, 370, 50878, 50878, 437, 321, 486, 528, 281, 360, 307, 321, 528, 281, 16979, 3048, 512, 8044, 295, 264, 51060, 51060, 1412, 992, 293, 300, 311, 257, 8382, 15245, 293, 550, 787, 2128, 23897, 293, 5623, 51248, 51248, 322, 300, 707, 8382, 15245, 293, 550, 321, 44497, 322, 729, 867, 15245, 279, 370, 294, 51512, 51512, 25878, 284, 339, 321, 393, 337, 1365, 764, 3930, 339, 13, 3699, 471, 321, 393, 8460, 3547, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.12900617581988694, "compression_ratio": 1.8969957081545064, "no_speech_prob": 1.4738468962605111e-05}, {"id": 503, "seek": 253968, "start": 2562.64, "end": 2566.44, "text": " pytorch we can for example use torch.randind we can generate numbers", "tokens": [50364, 293, 23897, 278, 5853, 23, 11, 1360, 5110, 294, 3124, 437, 561, 2673, 360, 307, 436, 50614, 50614, 2042, 2128, 293, 23897, 1320, 364, 5623, 322, 867, 15245, 279, 295, 264, 1412, 370, 50878, 50878, 437, 321, 486, 528, 281, 360, 307, 321, 528, 281, 16979, 3048, 512, 8044, 295, 264, 51060, 51060, 1412, 992, 293, 300, 311, 257, 8382, 15245, 293, 550, 787, 2128, 23897, 293, 5623, 51248, 51248, 322, 300, 707, 8382, 15245, 293, 550, 321, 44497, 322, 729, 867, 15245, 279, 370, 294, 51512, 51512, 25878, 284, 339, 321, 393, 337, 1365, 764, 3930, 339, 13, 3699, 471, 321, 393, 8460, 3547, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.12900617581988694, "compression_ratio": 1.8969957081545064, "no_speech_prob": 1.4738468962605111e-05}, {"id": 504, "seek": 256644, "start": 2566.44, "end": 2576.7200000000003, "text": " between 0 and 5 and make 32 of them I believe the size has to be a tuple in", "tokens": [50364, 1296, 1958, 293, 1025, 293, 652, 8858, 295, 552, 286, 1697, 264, 2744, 575, 281, 312, 257, 2604, 781, 294, 50878, 50878, 25878, 284, 339, 370, 321, 393, 362, 257, 2604, 781, 8858, 295, 3547, 1296, 1958, 293, 1025, 457, 767, 321, 51178, 51178, 528, 2031, 13, 82, 42406, 295, 1958, 510, 293, 370, 341, 7829, 41674, 300, 8186, 666, 527, 51494, 51494, 1412, 992, 293, 456, 311, 8858, 295, 552, 370, 498, 527, 8382, 15245, 2744, 307, 8858, 550, 321, 393, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.10462155452994414, "compression_ratio": 1.6483516483516483, "no_speech_prob": 8.664416782266926e-06}, {"id": 505, "seek": 256644, "start": 2576.7200000000003, "end": 2582.7200000000003, "text": " pytorch so we can have a tuple 32 of numbers between 0 and 5 but actually we", "tokens": [50364, 1296, 1958, 293, 1025, 293, 652, 8858, 295, 552, 286, 1697, 264, 2744, 575, 281, 312, 257, 2604, 781, 294, 50878, 50878, 25878, 284, 339, 370, 321, 393, 362, 257, 2604, 781, 8858, 295, 3547, 1296, 1958, 293, 1025, 457, 767, 321, 51178, 51178, 528, 2031, 13, 82, 42406, 295, 1958, 510, 293, 370, 341, 7829, 41674, 300, 8186, 666, 527, 51494, 51494, 1412, 992, 293, 456, 311, 8858, 295, 552, 370, 498, 527, 8382, 15245, 2744, 307, 8858, 550, 321, 393, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.10462155452994414, "compression_ratio": 1.6483516483516483, "no_speech_prob": 8.664416782266926e-06}, {"id": 506, "seek": 256644, "start": 2582.7200000000003, "end": 2589.04, "text": " want x.shape of 0 here and so this creates integers that index into our", "tokens": [50364, 1296, 1958, 293, 1025, 293, 652, 8858, 295, 552, 286, 1697, 264, 2744, 575, 281, 312, 257, 2604, 781, 294, 50878, 50878, 25878, 284, 339, 370, 321, 393, 362, 257, 2604, 781, 8858, 295, 3547, 1296, 1958, 293, 1025, 457, 767, 321, 51178, 51178, 528, 2031, 13, 82, 42406, 295, 1958, 510, 293, 370, 341, 7829, 41674, 300, 8186, 666, 527, 51494, 51494, 1412, 992, 293, 456, 311, 8858, 295, 552, 370, 498, 527, 8382, 15245, 2744, 307, 8858, 550, 321, 393, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.10462155452994414, "compression_ratio": 1.6483516483516483, "no_speech_prob": 8.664416782266926e-06}, {"id": 507, "seek": 256644, "start": 2589.04, "end": 2593.8, "text": " data set and there's 32 of them so if our mini batch size is 32 then we can", "tokens": [50364, 1296, 1958, 293, 1025, 293, 652, 8858, 295, 552, 286, 1697, 264, 2744, 575, 281, 312, 257, 2604, 781, 294, 50878, 50878, 25878, 284, 339, 370, 321, 393, 362, 257, 2604, 781, 8858, 295, 3547, 1296, 1958, 293, 1025, 457, 767, 321, 51178, 51178, 528, 2031, 13, 82, 42406, 295, 1958, 510, 293, 370, 341, 7829, 41674, 300, 8186, 666, 527, 51494, 51494, 1412, 992, 293, 456, 311, 8858, 295, 552, 370, 498, 527, 8382, 15245, 2744, 307, 8858, 550, 321, 393, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.10462155452994414, "compression_ratio": 1.6483516483516483, "no_speech_prob": 8.664416782266926e-06}, {"id": 508, "seek": 259380, "start": 2593.8, "end": 2601.7200000000003, "text": " come here and we can first do mini batch construct so in the integers that we", "tokens": [50364, 808, 510, 293, 321, 393, 700, 360, 8382, 15245, 7690, 370, 294, 264, 41674, 300, 321, 50760, 50760, 528, 281, 19719, 294, 341, 2167, 24784, 366, 294, 264, 741, 87, 293, 550, 321, 528, 51096, 51096, 281, 8186, 666, 2031, 365, 741, 87, 281, 787, 4444, 729, 13241, 370, 321, 434, 787, 1242, 8858, 13241, 51464, 51464, 295, 2031, 293, 4412, 12240, 29432, 486, 797, 312, 8858, 538, 805, 538, 568, 406, 2331, 11, 1360, 538, 805, 538, 568, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.11708853618208184, "compression_ratio": 1.6329787234042554, "no_speech_prob": 6.240753464226145e-06}, {"id": 509, "seek": 259380, "start": 2601.7200000000003, "end": 2608.44, "text": " want to optimize in this single iteration are in the ix and then we want", "tokens": [50364, 808, 510, 293, 321, 393, 700, 360, 8382, 15245, 7690, 370, 294, 264, 41674, 300, 321, 50760, 50760, 528, 281, 19719, 294, 341, 2167, 24784, 366, 294, 264, 741, 87, 293, 550, 321, 528, 51096, 51096, 281, 8186, 666, 2031, 365, 741, 87, 281, 787, 4444, 729, 13241, 370, 321, 434, 787, 1242, 8858, 13241, 51464, 51464, 295, 2031, 293, 4412, 12240, 29432, 486, 797, 312, 8858, 538, 805, 538, 568, 406, 2331, 11, 1360, 538, 805, 538, 568, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.11708853618208184, "compression_ratio": 1.6329787234042554, "no_speech_prob": 6.240753464226145e-06}, {"id": 510, "seek": 259380, "start": 2608.44, "end": 2615.8, "text": " to index into x with ix to only grab those rows so we're only getting 32 rows", "tokens": [50364, 808, 510, 293, 321, 393, 700, 360, 8382, 15245, 7690, 370, 294, 264, 41674, 300, 321, 50760, 50760, 528, 281, 19719, 294, 341, 2167, 24784, 366, 294, 264, 741, 87, 293, 550, 321, 528, 51096, 51096, 281, 8186, 666, 2031, 365, 741, 87, 281, 787, 4444, 729, 13241, 370, 321, 434, 787, 1242, 8858, 13241, 51464, 51464, 295, 2031, 293, 4412, 12240, 29432, 486, 797, 312, 8858, 538, 805, 538, 568, 406, 2331, 11, 1360, 538, 805, 538, 568, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.11708853618208184, "compression_ratio": 1.6329787234042554, "no_speech_prob": 6.240753464226145e-06}, {"id": 511, "seek": 259380, "start": 2615.8, "end": 2622.44, "text": " of x and therefore embeddings will again be 32 by 3 by 2 not 200,000 by 3 by 2", "tokens": [50364, 808, 510, 293, 321, 393, 700, 360, 8382, 15245, 7690, 370, 294, 264, 41674, 300, 321, 50760, 50760, 528, 281, 19719, 294, 341, 2167, 24784, 366, 294, 264, 741, 87, 293, 550, 321, 528, 51096, 51096, 281, 8186, 666, 2031, 365, 741, 87, 281, 787, 4444, 729, 13241, 370, 321, 434, 787, 1242, 8858, 13241, 51464, 51464, 295, 2031, 293, 4412, 12240, 29432, 486, 797, 312, 8858, 538, 805, 538, 568, 406, 2331, 11, 1360, 538, 805, 538, 568, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.11708853618208184, "compression_ratio": 1.6329787234042554, "no_speech_prob": 6.240753464226145e-06}, {"id": 512, "seek": 262244, "start": 2622.44, "end": 2628.4, "text": " and then this ix has to be used not just to index into x but also to index into", "tokens": [50364, 293, 550, 341, 741, 87, 575, 281, 312, 1143, 406, 445, 281, 8186, 666, 2031, 457, 611, 281, 8186, 666, 50662, 50662, 288, 293, 586, 341, 820, 312, 8382, 15245, 279, 293, 341, 820, 312, 709, 709, 4663, 370, 50986, 50986, 1392, 370, 309, 311, 9836, 1920, 370, 341, 636, 321, 393, 1190, 867, 867, 5110, 6217, 51346, 51346, 13518, 293, 11514, 264, 4470, 709, 709, 4663, 586, 570, 321, 434, 787, 6260, 51586, 51586, 365, 8382, 15245, 279, 264, 3125, 295, 527, 16235, 307, 3126, 370, 264, 3513, 307, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.10275338558440512, "compression_ratio": 1.8564593301435406, "no_speech_prob": 8.530125342076644e-06}, {"id": 513, "seek": 262244, "start": 2628.4, "end": 2634.88, "text": " y and now this should be mini batches and this should be much much faster so", "tokens": [50364, 293, 550, 341, 741, 87, 575, 281, 312, 1143, 406, 445, 281, 8186, 666, 2031, 457, 611, 281, 8186, 666, 50662, 50662, 288, 293, 586, 341, 820, 312, 8382, 15245, 279, 293, 341, 820, 312, 709, 709, 4663, 370, 50986, 50986, 1392, 370, 309, 311, 9836, 1920, 370, 341, 636, 321, 393, 1190, 867, 867, 5110, 6217, 51346, 51346, 13518, 293, 11514, 264, 4470, 709, 709, 4663, 586, 570, 321, 434, 787, 6260, 51586, 51586, 365, 8382, 15245, 279, 264, 3125, 295, 527, 16235, 307, 3126, 370, 264, 3513, 307, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.10275338558440512, "compression_ratio": 1.8564593301435406, "no_speech_prob": 8.530125342076644e-06}, {"id": 514, "seek": 262244, "start": 2634.88, "end": 2642.08, "text": " okay so it's instant almost so this way we can run many many examples nearly", "tokens": [50364, 293, 550, 341, 741, 87, 575, 281, 312, 1143, 406, 445, 281, 8186, 666, 2031, 457, 611, 281, 8186, 666, 50662, 50662, 288, 293, 586, 341, 820, 312, 8382, 15245, 279, 293, 341, 820, 312, 709, 709, 4663, 370, 50986, 50986, 1392, 370, 309, 311, 9836, 1920, 370, 341, 636, 321, 393, 1190, 867, 867, 5110, 6217, 51346, 51346, 13518, 293, 11514, 264, 4470, 709, 709, 4663, 586, 570, 321, 434, 787, 6260, 51586, 51586, 365, 8382, 15245, 279, 264, 3125, 295, 527, 16235, 307, 3126, 370, 264, 3513, 307, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.10275338558440512, "compression_ratio": 1.8564593301435406, "no_speech_prob": 8.530125342076644e-06}, {"id": 515, "seek": 262244, "start": 2642.08, "end": 2646.88, "text": " instantly and decrease the loss much much faster now because we're only dealing", "tokens": [50364, 293, 550, 341, 741, 87, 575, 281, 312, 1143, 406, 445, 281, 8186, 666, 2031, 457, 611, 281, 8186, 666, 50662, 50662, 288, 293, 586, 341, 820, 312, 8382, 15245, 279, 293, 341, 820, 312, 709, 709, 4663, 370, 50986, 50986, 1392, 370, 309, 311, 9836, 1920, 370, 341, 636, 321, 393, 1190, 867, 867, 5110, 6217, 51346, 51346, 13518, 293, 11514, 264, 4470, 709, 709, 4663, 586, 570, 321, 434, 787, 6260, 51586, 51586, 365, 8382, 15245, 279, 264, 3125, 295, 527, 16235, 307, 3126, 370, 264, 3513, 307, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.10275338558440512, "compression_ratio": 1.8564593301435406, "no_speech_prob": 8.530125342076644e-06}, {"id": 516, "seek": 262244, "start": 2646.88, "end": 2651.2400000000002, "text": " with mini batches the quality of our gradient is lower so the direction is", "tokens": [50364, 293, 550, 341, 741, 87, 575, 281, 312, 1143, 406, 445, 281, 8186, 666, 2031, 457, 611, 281, 8186, 666, 50662, 50662, 288, 293, 586, 341, 820, 312, 8382, 15245, 279, 293, 341, 820, 312, 709, 709, 4663, 370, 50986, 50986, 1392, 370, 309, 311, 9836, 1920, 370, 341, 636, 321, 393, 1190, 867, 867, 5110, 6217, 51346, 51346, 13518, 293, 11514, 264, 4470, 709, 709, 4663, 586, 570, 321, 434, 787, 6260, 51586, 51586, 365, 8382, 15245, 279, 264, 3125, 295, 527, 16235, 307, 3126, 370, 264, 3513, 307, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.10275338558440512, "compression_ratio": 1.8564593301435406, "no_speech_prob": 8.530125342076644e-06}, {"id": 517, "seek": 265124, "start": 2651.24, "end": 2656.0, "text": " not as reliable it's not the actual gradient direction but the gradient", "tokens": [50364, 406, 382, 12924, 309, 311, 406, 264, 3539, 16235, 3513, 457, 264, 16235, 50602, 50602, 3513, 307, 665, 1547, 754, 562, 309, 311, 8017, 990, 322, 787, 8858, 5110, 300, 309, 50836, 50836, 307, 4420, 293, 370, 309, 311, 709, 1101, 281, 362, 364, 30874, 16235, 293, 445, 51082, 51082, 652, 544, 4439, 813, 309, 307, 281, 13059, 264, 1900, 16235, 293, 747, 13366, 4439, 51314, 51314, 370, 300, 311, 983, 294, 3124, 341, 1985, 1596, 731, 370, 718, 311, 586, 2354, 264, 51582, 51582], "temperature": 0.0, "avg_logprob": -0.07311314549939386, "compression_ratio": 1.726027397260274, "no_speech_prob": 3.237677219658508e-06}, {"id": 518, "seek": 265124, "start": 2656.0, "end": 2660.68, "text": " direction is good enough even when it's estimating on only 32 examples that it", "tokens": [50364, 406, 382, 12924, 309, 311, 406, 264, 3539, 16235, 3513, 457, 264, 16235, 50602, 50602, 3513, 307, 665, 1547, 754, 562, 309, 311, 8017, 990, 322, 787, 8858, 5110, 300, 309, 50836, 50836, 307, 4420, 293, 370, 309, 311, 709, 1101, 281, 362, 364, 30874, 16235, 293, 445, 51082, 51082, 652, 544, 4439, 813, 309, 307, 281, 13059, 264, 1900, 16235, 293, 747, 13366, 4439, 51314, 51314, 370, 300, 311, 983, 294, 3124, 341, 1985, 1596, 731, 370, 718, 311, 586, 2354, 264, 51582, 51582], "temperature": 0.0, "avg_logprob": -0.07311314549939386, "compression_ratio": 1.726027397260274, "no_speech_prob": 3.237677219658508e-06}, {"id": 519, "seek": 265124, "start": 2660.68, "end": 2665.6, "text": " is useful and so it's much better to have an approximate gradient and just", "tokens": [50364, 406, 382, 12924, 309, 311, 406, 264, 3539, 16235, 3513, 457, 264, 16235, 50602, 50602, 3513, 307, 665, 1547, 754, 562, 309, 311, 8017, 990, 322, 787, 8858, 5110, 300, 309, 50836, 50836, 307, 4420, 293, 370, 309, 311, 709, 1101, 281, 362, 364, 30874, 16235, 293, 445, 51082, 51082, 652, 544, 4439, 813, 309, 307, 281, 13059, 264, 1900, 16235, 293, 747, 13366, 4439, 51314, 51314, 370, 300, 311, 983, 294, 3124, 341, 1985, 1596, 731, 370, 718, 311, 586, 2354, 264, 51582, 51582], "temperature": 0.0, "avg_logprob": -0.07311314549939386, "compression_ratio": 1.726027397260274, "no_speech_prob": 3.237677219658508e-06}, {"id": 520, "seek": 265124, "start": 2665.6, "end": 2670.24, "text": " make more steps than it is to evaluate the exact gradient and take fewer steps", "tokens": [50364, 406, 382, 12924, 309, 311, 406, 264, 3539, 16235, 3513, 457, 264, 16235, 50602, 50602, 3513, 307, 665, 1547, 754, 562, 309, 311, 8017, 990, 322, 787, 8858, 5110, 300, 309, 50836, 50836, 307, 4420, 293, 370, 309, 311, 709, 1101, 281, 362, 364, 30874, 16235, 293, 445, 51082, 51082, 652, 544, 4439, 813, 309, 307, 281, 13059, 264, 1900, 16235, 293, 747, 13366, 4439, 51314, 51314, 370, 300, 311, 983, 294, 3124, 341, 1985, 1596, 731, 370, 718, 311, 586, 2354, 264, 51582, 51582], "temperature": 0.0, "avg_logprob": -0.07311314549939386, "compression_ratio": 1.726027397260274, "no_speech_prob": 3.237677219658508e-06}, {"id": 521, "seek": 265124, "start": 2670.24, "end": 2675.6, "text": " so that's why in practice this works quite well so let's now continue the", "tokens": [50364, 406, 382, 12924, 309, 311, 406, 264, 3539, 16235, 3513, 457, 264, 16235, 50602, 50602, 3513, 307, 665, 1547, 754, 562, 309, 311, 8017, 990, 322, 787, 8858, 5110, 300, 309, 50836, 50836, 307, 4420, 293, 370, 309, 311, 709, 1101, 281, 362, 364, 30874, 16235, 293, 445, 51082, 51082, 652, 544, 4439, 813, 309, 307, 281, 13059, 264, 1900, 16235, 293, 747, 13366, 4439, 51314, 51314, 370, 300, 311, 983, 294, 3124, 341, 1985, 1596, 731, 370, 718, 311, 586, 2354, 264, 51582, 51582], "temperature": 0.0, "avg_logprob": -0.07311314549939386, "compression_ratio": 1.726027397260274, "no_speech_prob": 3.237677219658508e-06}, {"id": 522, "seek": 267560, "start": 2675.6, "end": 2684.04, "text": " optimization let me take out this lost that item from here and place it over", "tokens": [50364, 19618, 718, 385, 747, 484, 341, 2731, 300, 3174, 490, 510, 293, 1081, 309, 670, 50786, 50786, 510, 412, 264, 917, 1392, 370, 321, 434, 44923, 926, 568, 13, 20, 420, 370, 4461, 341, 307, 787, 51138, 51138, 264, 4470, 337, 300, 8382, 15245, 370, 718, 311, 767, 13059, 264, 4470, 510, 337, 439, 51440, 51440, 295, 1783, 293, 337, 439, 295, 398, 445, 370, 321, 362, 257, 1577, 2020, 295, 2293, 577, 731, 264, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.10557762580581859, "compression_ratio": 1.6020942408376964, "no_speech_prob": 2.0144716472714208e-05}, {"id": 523, "seek": 267560, "start": 2684.04, "end": 2691.08, "text": " here at the end okay so we're hovering around 2.5 or so however this is only", "tokens": [50364, 19618, 718, 385, 747, 484, 341, 2731, 300, 3174, 490, 510, 293, 1081, 309, 670, 50786, 50786, 510, 412, 264, 917, 1392, 370, 321, 434, 44923, 926, 568, 13, 20, 420, 370, 4461, 341, 307, 787, 51138, 51138, 264, 4470, 337, 300, 8382, 15245, 370, 718, 311, 767, 13059, 264, 4470, 510, 337, 439, 51440, 51440, 295, 1783, 293, 337, 439, 295, 398, 445, 370, 321, 362, 257, 1577, 2020, 295, 2293, 577, 731, 264, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.10557762580581859, "compression_ratio": 1.6020942408376964, "no_speech_prob": 2.0144716472714208e-05}, {"id": 524, "seek": 267560, "start": 2691.08, "end": 2697.12, "text": " the loss for that mini batch so let's actually evaluate the loss here for all", "tokens": [50364, 19618, 718, 385, 747, 484, 341, 2731, 300, 3174, 490, 510, 293, 1081, 309, 670, 50786, 50786, 510, 412, 264, 917, 1392, 370, 321, 434, 44923, 926, 568, 13, 20, 420, 370, 4461, 341, 307, 787, 51138, 51138, 264, 4470, 337, 300, 8382, 15245, 370, 718, 311, 767, 13059, 264, 4470, 510, 337, 439, 51440, 51440, 295, 1783, 293, 337, 439, 295, 398, 445, 370, 321, 362, 257, 1577, 2020, 295, 2293, 577, 731, 264, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.10557762580581859, "compression_ratio": 1.6020942408376964, "no_speech_prob": 2.0144716472714208e-05}, {"id": 525, "seek": 267560, "start": 2697.12, "end": 2702.7999999999997, "text": " of X and for all of Y just so we have a full sense of exactly how well the", "tokens": [50364, 19618, 718, 385, 747, 484, 341, 2731, 300, 3174, 490, 510, 293, 1081, 309, 670, 50786, 50786, 510, 412, 264, 917, 1392, 370, 321, 434, 44923, 926, 568, 13, 20, 420, 370, 4461, 341, 307, 787, 51138, 51138, 264, 4470, 337, 300, 8382, 15245, 370, 718, 311, 767, 13059, 264, 4470, 510, 337, 439, 51440, 51440, 295, 1783, 293, 337, 439, 295, 398, 445, 370, 321, 362, 257, 1577, 2020, 295, 2293, 577, 731, 264, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.10557762580581859, "compression_ratio": 1.6020942408376964, "no_speech_prob": 2.0144716472714208e-05}, {"id": 526, "seek": 270280, "start": 2702.8, "end": 2708.32, "text": " model is doing right now so right now we're about 2.7 on the entire training", "tokens": [50364, 2316, 307, 884, 558, 586, 370, 558, 586, 321, 434, 466, 568, 13, 22, 322, 264, 2302, 3097, 50640, 50640, 992, 370, 718, 311, 1190, 264, 19618, 337, 257, 1339, 1392, 321, 434, 412, 568, 13, 21, 568, 13, 19004, 568, 13, 19584, 1392, 370, 51384, 51384, 472, 2734, 295, 1164, 307, 321, 500, 380, 458, 498, 321, 434, 16821, 886, 2964, 420, 886, 2370, 370, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.11144383294241769, "compression_ratio": 1.5192307692307692, "no_speech_prob": 1.9832350517390296e-05}, {"id": 527, "seek": 270280, "start": 2708.32, "end": 2723.2000000000003, "text": " set so let's run the optimization for a while okay we're at 2.6 2.57 2.53 okay so", "tokens": [50364, 2316, 307, 884, 558, 586, 370, 558, 586, 321, 434, 466, 568, 13, 22, 322, 264, 2302, 3097, 50640, 50640, 992, 370, 718, 311, 1190, 264, 19618, 337, 257, 1339, 1392, 321, 434, 412, 568, 13, 21, 568, 13, 19004, 568, 13, 19584, 1392, 370, 51384, 51384, 472, 2734, 295, 1164, 307, 321, 500, 380, 458, 498, 321, 434, 16821, 886, 2964, 420, 886, 2370, 370, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.11144383294241769, "compression_ratio": 1.5192307692307692, "no_speech_prob": 1.9832350517390296e-05}, {"id": 528, "seek": 270280, "start": 2723.2000000000003, "end": 2729.04, "text": " one issue of course is we don't know if we're stepping too slow or too fast so", "tokens": [50364, 2316, 307, 884, 558, 586, 370, 558, 586, 321, 434, 466, 568, 13, 22, 322, 264, 2302, 3097, 50640, 50640, 992, 370, 718, 311, 1190, 264, 19618, 337, 257, 1339, 1392, 321, 434, 412, 568, 13, 21, 568, 13, 19004, 568, 13, 19584, 1392, 370, 51384, 51384, 472, 2734, 295, 1164, 307, 321, 500, 380, 458, 498, 321, 434, 16821, 886, 2964, 420, 886, 2370, 370, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.11144383294241769, "compression_ratio": 1.5192307692307692, "no_speech_prob": 1.9832350517390296e-05}, {"id": 529, "seek": 272904, "start": 2729.04, "end": 2733.56, "text": " this point one I just guessed it so one question is how do you determine this", "tokens": [50364, 341, 935, 472, 286, 445, 21852, 309, 370, 472, 1168, 307, 577, 360, 291, 6997, 341, 50590, 50590, 2539, 3314, 293, 577, 360, 321, 6052, 6687, 300, 321, 434, 16821, 294, 264, 50814, 50814, 558, 1333, 295, 3073, 370, 286, 603, 855, 291, 472, 636, 281, 6997, 257, 10585, 2539, 51044, 51044, 3314, 309, 1985, 382, 10002, 718, 311, 14322, 527, 9834, 281, 264, 5883, 6257, 293, 51444, 51444], "temperature": 0.0, "avg_logprob": -0.09114478339611644, "compression_ratio": 1.6470588235294117, "no_speech_prob": 3.089446181547828e-06}, {"id": 530, "seek": 272904, "start": 2733.56, "end": 2738.04, "text": " learning rate and how do we gain confidence that we're stepping in the", "tokens": [50364, 341, 935, 472, 286, 445, 21852, 309, 370, 472, 1168, 307, 577, 360, 291, 6997, 341, 50590, 50590, 2539, 3314, 293, 577, 360, 321, 6052, 6687, 300, 321, 434, 16821, 294, 264, 50814, 50814, 558, 1333, 295, 3073, 370, 286, 603, 855, 291, 472, 636, 281, 6997, 257, 10585, 2539, 51044, 51044, 3314, 309, 1985, 382, 10002, 718, 311, 14322, 527, 9834, 281, 264, 5883, 6257, 293, 51444, 51444], "temperature": 0.0, "avg_logprob": -0.09114478339611644, "compression_ratio": 1.6470588235294117, "no_speech_prob": 3.089446181547828e-06}, {"id": 531, "seek": 272904, "start": 2738.04, "end": 2742.64, "text": " right sort of speed so I'll show you one way to determine a reasonable learning", "tokens": [50364, 341, 935, 472, 286, 445, 21852, 309, 370, 472, 1168, 307, 577, 360, 291, 6997, 341, 50590, 50590, 2539, 3314, 293, 577, 360, 321, 6052, 6687, 300, 321, 434, 16821, 294, 264, 50814, 50814, 558, 1333, 295, 3073, 370, 286, 603, 855, 291, 472, 636, 281, 6997, 257, 10585, 2539, 51044, 51044, 3314, 309, 1985, 382, 10002, 718, 311, 14322, 527, 9834, 281, 264, 5883, 6257, 293, 51444, 51444], "temperature": 0.0, "avg_logprob": -0.09114478339611644, "compression_ratio": 1.6470588235294117, "no_speech_prob": 3.089446181547828e-06}, {"id": 532, "seek": 272904, "start": 2742.64, "end": 2750.64, "text": " rate it works as follows let's reset our parameters to the initial settings and", "tokens": [50364, 341, 935, 472, 286, 445, 21852, 309, 370, 472, 1168, 307, 577, 360, 291, 6997, 341, 50590, 50590, 2539, 3314, 293, 577, 360, 321, 6052, 6687, 300, 321, 434, 16821, 294, 264, 50814, 50814, 558, 1333, 295, 3073, 370, 286, 603, 855, 291, 472, 636, 281, 6997, 257, 10585, 2539, 51044, 51044, 3314, 309, 1985, 382, 10002, 718, 311, 14322, 527, 9834, 281, 264, 5883, 6257, 293, 51444, 51444], "temperature": 0.0, "avg_logprob": -0.09114478339611644, "compression_ratio": 1.6470588235294117, "no_speech_prob": 3.089446181547828e-06}, {"id": 533, "seek": 275064, "start": 2750.64, "end": 2759.7599999999998, "text": " now let's print in every step but let's only do 10 steps or so or maybe maybe", "tokens": [50364, 586, 718, 311, 4482, 294, 633, 1823, 457, 718, 311, 787, 360, 1266, 4439, 420, 370, 420, 1310, 1310, 50820, 50820, 2319, 4439, 321, 528, 281, 915, 411, 257, 588, 10585, 992, 264, 3164, 3613, 498, 291, 51078, 51078, 486, 370, 337, 1365, 498, 341, 307, 411, 588, 2295, 550, 321, 536, 300, 264, 4470, 307, 10268, 51400, 51400, 23223, 370, 300, 311, 406, 300, 311, 411, 886, 2295, 1936, 370, 718, 311, 853, 341, 472, 1392, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.08485711062396015, "compression_ratio": 1.7135135135135136, "no_speech_prob": 1.8631189959705807e-05}, {"id": 534, "seek": 275064, "start": 2759.7599999999998, "end": 2764.92, "text": " 100 steps we want to find like a very reasonable set the search range if you", "tokens": [50364, 586, 718, 311, 4482, 294, 633, 1823, 457, 718, 311, 787, 360, 1266, 4439, 420, 370, 420, 1310, 1310, 50820, 50820, 2319, 4439, 321, 528, 281, 915, 411, 257, 588, 10585, 992, 264, 3164, 3613, 498, 291, 51078, 51078, 486, 370, 337, 1365, 498, 341, 307, 411, 588, 2295, 550, 321, 536, 300, 264, 4470, 307, 10268, 51400, 51400, 23223, 370, 300, 311, 406, 300, 311, 411, 886, 2295, 1936, 370, 718, 311, 853, 341, 472, 1392, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.08485711062396015, "compression_ratio": 1.7135135135135136, "no_speech_prob": 1.8631189959705807e-05}, {"id": 535, "seek": 275064, "start": 2764.92, "end": 2771.3599999999997, "text": " will so for example if this is like very low then we see that the loss is barely", "tokens": [50364, 586, 718, 311, 4482, 294, 633, 1823, 457, 718, 311, 787, 360, 1266, 4439, 420, 370, 420, 1310, 1310, 50820, 50820, 2319, 4439, 321, 528, 281, 915, 411, 257, 588, 10585, 992, 264, 3164, 3613, 498, 291, 51078, 51078, 486, 370, 337, 1365, 498, 341, 307, 411, 588, 2295, 550, 321, 536, 300, 264, 4470, 307, 10268, 51400, 51400, 23223, 370, 300, 311, 406, 300, 311, 411, 886, 2295, 1936, 370, 718, 311, 853, 341, 472, 1392, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.08485711062396015, "compression_ratio": 1.7135135135135136, "no_speech_prob": 1.8631189959705807e-05}, {"id": 536, "seek": 275064, "start": 2771.3599999999997, "end": 2779.16, "text": " decreasing so that's not that's like too low basically so let's try this one okay", "tokens": [50364, 586, 718, 311, 4482, 294, 633, 1823, 457, 718, 311, 787, 360, 1266, 4439, 420, 370, 420, 1310, 1310, 50820, 50820, 2319, 4439, 321, 528, 281, 915, 411, 257, 588, 10585, 992, 264, 3164, 3613, 498, 291, 51078, 51078, 486, 370, 337, 1365, 498, 341, 307, 411, 588, 2295, 550, 321, 536, 300, 264, 4470, 307, 10268, 51400, 51400, 23223, 370, 300, 311, 406, 300, 311, 411, 886, 2295, 1936, 370, 718, 311, 853, 341, 472, 1392, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.08485711062396015, "compression_ratio": 1.7135135135135136, "no_speech_prob": 1.8631189959705807e-05}, {"id": 537, "seek": 277916, "start": 2779.16, "end": 2782.04, "text": " so we're decreasing the loss but like not very quickly so that's a pretty good", "tokens": [50364, 370, 321, 434, 23223, 264, 4470, 457, 411, 406, 588, 2661, 370, 300, 311, 257, 1238, 665, 50508, 50508, 2295, 3613, 586, 718, 311, 14322, 309, 797, 293, 586, 718, 311, 853, 281, 915, 264, 1081, 412, 597, 50790, 50790, 264, 4470, 733, 295, 42610, 370, 1310, 412, 3671, 472, 1392, 321, 536, 300, 321, 434, 51114, 51114, 46608, 264, 4470, 457, 291, 536, 577, 309, 311, 733, 295, 23742, 309, 1709, 493, 293, 760, 51316, 51316, 1596, 257, 857, 370, 3671, 472, 307, 1391, 411, 257, 2370, 2539, 3314, 718, 311, 853, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.0803094008534225, "compression_ratio": 1.7788018433179724, "no_speech_prob": 1.2218857591506094e-05}, {"id": 538, "seek": 277916, "start": 2782.04, "end": 2787.68, "text": " low range now let's reset it again and now let's try to find the place at which", "tokens": [50364, 370, 321, 434, 23223, 264, 4470, 457, 411, 406, 588, 2661, 370, 300, 311, 257, 1238, 665, 50508, 50508, 2295, 3613, 586, 718, 311, 14322, 309, 797, 293, 586, 718, 311, 853, 281, 915, 264, 1081, 412, 597, 50790, 50790, 264, 4470, 733, 295, 42610, 370, 1310, 412, 3671, 472, 1392, 321, 536, 300, 321, 434, 51114, 51114, 46608, 264, 4470, 457, 291, 536, 577, 309, 311, 733, 295, 23742, 309, 1709, 493, 293, 760, 51316, 51316, 1596, 257, 857, 370, 3671, 472, 307, 1391, 411, 257, 2370, 2539, 3314, 718, 311, 853, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.0803094008534225, "compression_ratio": 1.7788018433179724, "no_speech_prob": 1.2218857591506094e-05}, {"id": 539, "seek": 277916, "start": 2787.68, "end": 2794.16, "text": " the loss kind of explodes so maybe at negative one okay we see that we're", "tokens": [50364, 370, 321, 434, 23223, 264, 4470, 457, 411, 406, 588, 2661, 370, 300, 311, 257, 1238, 665, 50508, 50508, 2295, 3613, 586, 718, 311, 14322, 309, 797, 293, 586, 718, 311, 853, 281, 915, 264, 1081, 412, 597, 50790, 50790, 264, 4470, 733, 295, 42610, 370, 1310, 412, 3671, 472, 1392, 321, 536, 300, 321, 434, 51114, 51114, 46608, 264, 4470, 457, 291, 536, 577, 309, 311, 733, 295, 23742, 309, 1709, 493, 293, 760, 51316, 51316, 1596, 257, 857, 370, 3671, 472, 307, 1391, 411, 257, 2370, 2539, 3314, 718, 311, 853, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.0803094008534225, "compression_ratio": 1.7788018433179724, "no_speech_prob": 1.2218857591506094e-05}, {"id": 540, "seek": 277916, "start": 2794.16, "end": 2798.2, "text": " minimizing the loss but you see how it's kind of unstable it goes up and down", "tokens": [50364, 370, 321, 434, 23223, 264, 4470, 457, 411, 406, 588, 2661, 370, 300, 311, 257, 1238, 665, 50508, 50508, 2295, 3613, 586, 718, 311, 14322, 309, 797, 293, 586, 718, 311, 853, 281, 915, 264, 1081, 412, 597, 50790, 50790, 264, 4470, 733, 295, 42610, 370, 1310, 412, 3671, 472, 1392, 321, 536, 300, 321, 434, 51114, 51114, 46608, 264, 4470, 457, 291, 536, 577, 309, 311, 733, 295, 23742, 309, 1709, 493, 293, 760, 51316, 51316, 1596, 257, 857, 370, 3671, 472, 307, 1391, 411, 257, 2370, 2539, 3314, 718, 311, 853, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.0803094008534225, "compression_ratio": 1.7788018433179724, "no_speech_prob": 1.2218857591506094e-05}, {"id": 541, "seek": 277916, "start": 2798.2, "end": 2803.8799999999997, "text": " quite a bit so negative one is probably like a fast learning rate let's try", "tokens": [50364, 370, 321, 434, 23223, 264, 4470, 457, 411, 406, 588, 2661, 370, 300, 311, 257, 1238, 665, 50508, 50508, 2295, 3613, 586, 718, 311, 14322, 309, 797, 293, 586, 718, 311, 853, 281, 915, 264, 1081, 412, 597, 50790, 50790, 264, 4470, 733, 295, 42610, 370, 1310, 412, 3671, 472, 1392, 321, 536, 300, 321, 434, 51114, 51114, 46608, 264, 4470, 457, 291, 536, 577, 309, 311, 733, 295, 23742, 309, 1709, 493, 293, 760, 51316, 51316, 1596, 257, 857, 370, 3671, 472, 307, 1391, 411, 257, 2370, 2539, 3314, 718, 311, 853, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.0803094008534225, "compression_ratio": 1.7788018433179724, "no_speech_prob": 1.2218857591506094e-05}, {"id": 542, "seek": 280388, "start": 2803.88, "end": 2809.36, "text": " negative 10 okay so this isn't optimizing this is not working very well", "tokens": [50364, 3671, 1266, 1392, 370, 341, 1943, 380, 40425, 341, 307, 406, 1364, 588, 731, 50638, 50638, 370, 3671, 1266, 307, 636, 886, 955, 3671, 472, 390, 1217, 733, 295, 955, 370, 4412, 50970, 50970, 8470, 472, 390, 411, 8344, 10585, 498, 286, 14322, 370, 286, 478, 1953, 300, 264, 51234, 51234, 558, 2539, 3314, 307, 4079, 1296, 3671, 1958, 13, 628, 16, 293, 3671, 502, 370, 264, 636, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.1485614776611328, "compression_ratio": 1.7166666666666666, "no_speech_prob": 1.8924643882201053e-05}, {"id": 543, "seek": 280388, "start": 2809.36, "end": 2816.0, "text": " so negative 10 is way too big negative one was already kind of big so therefore", "tokens": [50364, 3671, 1266, 1392, 370, 341, 1943, 380, 40425, 341, 307, 406, 1364, 588, 731, 50638, 50638, 370, 3671, 1266, 307, 636, 886, 955, 3671, 472, 390, 1217, 733, 295, 955, 370, 4412, 50970, 50970, 8470, 472, 390, 411, 8344, 10585, 498, 286, 14322, 370, 286, 478, 1953, 300, 264, 51234, 51234, 558, 2539, 3314, 307, 4079, 1296, 3671, 1958, 13, 628, 16, 293, 3671, 502, 370, 264, 636, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.1485614776611328, "compression_ratio": 1.7166666666666666, "no_speech_prob": 1.8924643882201053e-05}, {"id": 544, "seek": 280388, "start": 2816.0, "end": 2821.28, "text": " native one was like somewhat reasonable if I reset so I'm thinking that the", "tokens": [50364, 3671, 1266, 1392, 370, 341, 1943, 380, 40425, 341, 307, 406, 1364, 588, 731, 50638, 50638, 370, 3671, 1266, 307, 636, 886, 955, 3671, 472, 390, 1217, 733, 295, 955, 370, 4412, 50970, 50970, 8470, 472, 390, 411, 8344, 10585, 498, 286, 14322, 370, 286, 478, 1953, 300, 264, 51234, 51234, 558, 2539, 3314, 307, 4079, 1296, 3671, 1958, 13, 628, 16, 293, 3671, 502, 370, 264, 636, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.1485614776611328, "compression_ratio": 1.7166666666666666, "no_speech_prob": 1.8924643882201053e-05}, {"id": 545, "seek": 280388, "start": 2821.28, "end": 2828.84, "text": " right learning rate is somewhere between negative 0.001 and negative 1 so the way", "tokens": [50364, 3671, 1266, 1392, 370, 341, 1943, 380, 40425, 341, 307, 406, 1364, 588, 731, 50638, 50638, 370, 3671, 1266, 307, 636, 886, 955, 3671, 472, 390, 1217, 733, 295, 955, 370, 4412, 50970, 50970, 8470, 472, 390, 411, 8344, 10585, 498, 286, 14322, 370, 286, 478, 1953, 300, 264, 51234, 51234, 558, 2539, 3314, 307, 4079, 1296, 3671, 1958, 13, 628, 16, 293, 3671, 502, 370, 264, 636, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.1485614776611328, "compression_ratio": 1.7166666666666666, "no_speech_prob": 1.8924643882201053e-05}, {"id": 546, "seek": 282884, "start": 2828.84, "end": 2834.0, "text": " we can do this here is we can use torque shuttling space and we want to basically", "tokens": [50364, 321, 393, 360, 341, 510, 307, 321, 393, 764, 16437, 5309, 83, 1688, 1901, 293, 321, 528, 281, 1936, 50622, 50622, 360, 746, 411, 341, 1296, 1958, 293, 502, 457, 3975, 1230, 295, 4439, 307, 472, 544, 50978, 50978, 13075, 300, 311, 4739, 718, 311, 360, 257, 4714, 4439, 341, 7829, 9714, 51208, 51208, 3547, 1296, 1958, 13, 628, 16, 293, 502, 457, 309, 1177, 380, 534, 652, 2020, 281, 1823, 51498, 51498, 1296, 613, 43586, 370, 2602, 718, 385, 1884, 2539, 3314, 37871, 293, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.1742432519291224, "compression_ratio": 1.7175925925925926, "no_speech_prob": 1.9637793684523785e-06}, {"id": 547, "seek": 282884, "start": 2834.0, "end": 2841.1200000000003, "text": " do something like this between 0 and 1 but host number of steps is one more", "tokens": [50364, 321, 393, 360, 341, 510, 307, 321, 393, 764, 16437, 5309, 83, 1688, 1901, 293, 321, 528, 281, 1936, 50622, 50622, 360, 746, 411, 341, 1296, 1958, 293, 502, 457, 3975, 1230, 295, 4439, 307, 472, 544, 50978, 50978, 13075, 300, 311, 4739, 718, 311, 360, 257, 4714, 4439, 341, 7829, 9714, 51208, 51208, 3547, 1296, 1958, 13, 628, 16, 293, 502, 457, 309, 1177, 380, 534, 652, 2020, 281, 1823, 51498, 51498, 1296, 613, 43586, 370, 2602, 718, 385, 1884, 2539, 3314, 37871, 293, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.1742432519291224, "compression_ratio": 1.7175925925925926, "no_speech_prob": 1.9637793684523785e-06}, {"id": 548, "seek": 282884, "start": 2841.1200000000003, "end": 2845.7200000000003, "text": " parameter that's required let's do a thousand steps this creates 1000", "tokens": [50364, 321, 393, 360, 341, 510, 307, 321, 393, 764, 16437, 5309, 83, 1688, 1901, 293, 321, 528, 281, 1936, 50622, 50622, 360, 746, 411, 341, 1296, 1958, 293, 502, 457, 3975, 1230, 295, 4439, 307, 472, 544, 50978, 50978, 13075, 300, 311, 4739, 718, 311, 360, 257, 4714, 4439, 341, 7829, 9714, 51208, 51208, 3547, 1296, 1958, 13, 628, 16, 293, 502, 457, 309, 1177, 380, 534, 652, 2020, 281, 1823, 51498, 51498, 1296, 613, 43586, 370, 2602, 718, 385, 1884, 2539, 3314, 37871, 293, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.1742432519291224, "compression_ratio": 1.7175925925925926, "no_speech_prob": 1.9637793684523785e-06}, {"id": 549, "seek": 282884, "start": 2845.7200000000003, "end": 2851.52, "text": " numbers between 0.001 and 1 but it doesn't really make sense to step", "tokens": [50364, 321, 393, 360, 341, 510, 307, 321, 393, 764, 16437, 5309, 83, 1688, 1901, 293, 321, 528, 281, 1936, 50622, 50622, 360, 746, 411, 341, 1296, 1958, 293, 502, 457, 3975, 1230, 295, 4439, 307, 472, 544, 50978, 50978, 13075, 300, 311, 4739, 718, 311, 360, 257, 4714, 4439, 341, 7829, 9714, 51208, 51208, 3547, 1296, 1958, 13, 628, 16, 293, 502, 457, 309, 1177, 380, 534, 652, 2020, 281, 1823, 51498, 51498, 1296, 613, 43586, 370, 2602, 718, 385, 1884, 2539, 3314, 37871, 293, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.1742432519291224, "compression_ratio": 1.7175925925925926, "no_speech_prob": 1.9637793684523785e-06}, {"id": 550, "seek": 282884, "start": 2851.52, "end": 2856.1600000000003, "text": " between these linearly so instead let me create learning rate exponent and", "tokens": [50364, 321, 393, 360, 341, 510, 307, 321, 393, 764, 16437, 5309, 83, 1688, 1901, 293, 321, 528, 281, 1936, 50622, 50622, 360, 746, 411, 341, 1296, 1958, 293, 502, 457, 3975, 1230, 295, 4439, 307, 472, 544, 50978, 50978, 13075, 300, 311, 4739, 718, 311, 360, 257, 4714, 4439, 341, 7829, 9714, 51208, 51208, 3547, 1296, 1958, 13, 628, 16, 293, 502, 457, 309, 1177, 380, 534, 652, 2020, 281, 1823, 51498, 51498, 1296, 613, 43586, 370, 2602, 718, 385, 1884, 2539, 3314, 37871, 293, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.1742432519291224, "compression_ratio": 1.7175925925925926, "no_speech_prob": 1.9637793684523785e-06}, {"id": 551, "seek": 285616, "start": 2856.16, "end": 2862.24, "text": " instead of 0.001 this will be a negative 3 and this will be a 0 and then the", "tokens": [50364, 2602, 295, 1958, 13, 628, 16, 341, 486, 312, 257, 3671, 805, 293, 341, 486, 312, 257, 1958, 293, 550, 264, 50668, 50668, 3539, 441, 49, 82, 300, 321, 528, 281, 3164, 670, 366, 516, 281, 312, 1266, 281, 264, 1347, 295, 441, 3850, 50940, 50940, 370, 586, 437, 321, 434, 884, 307, 321, 434, 16821, 43586, 1296, 264, 12680, 791, 51126, 51126, 295, 613, 2539, 6846, 341, 307, 1958, 13, 628, 16, 293, 341, 307, 502, 570, 1266, 281, 264, 51406, 51406, 1347, 295, 1958, 307, 502, 293, 4412, 321, 366, 43766, 37330, 294, 341, 15035, 370, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.11179192860921223, "compression_ratio": 1.7605633802816902, "no_speech_prob": 6.048675459169317e-06}, {"id": 552, "seek": 285616, "start": 2862.24, "end": 2867.68, "text": " actual LRs that we want to search over are going to be 10 to the power of LRE", "tokens": [50364, 2602, 295, 1958, 13, 628, 16, 341, 486, 312, 257, 3671, 805, 293, 341, 486, 312, 257, 1958, 293, 550, 264, 50668, 50668, 3539, 441, 49, 82, 300, 321, 528, 281, 3164, 670, 366, 516, 281, 312, 1266, 281, 264, 1347, 295, 441, 3850, 50940, 50940, 370, 586, 437, 321, 434, 884, 307, 321, 434, 16821, 43586, 1296, 264, 12680, 791, 51126, 51126, 295, 613, 2539, 6846, 341, 307, 1958, 13, 628, 16, 293, 341, 307, 502, 570, 1266, 281, 264, 51406, 51406, 1347, 295, 1958, 307, 502, 293, 4412, 321, 366, 43766, 37330, 294, 341, 15035, 370, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.11179192860921223, "compression_ratio": 1.7605633802816902, "no_speech_prob": 6.048675459169317e-06}, {"id": 553, "seek": 285616, "start": 2867.68, "end": 2871.3999999999996, "text": " so now what we're doing is we're stepping linearly between the exponents", "tokens": [50364, 2602, 295, 1958, 13, 628, 16, 341, 486, 312, 257, 3671, 805, 293, 341, 486, 312, 257, 1958, 293, 550, 264, 50668, 50668, 3539, 441, 49, 82, 300, 321, 528, 281, 3164, 670, 366, 516, 281, 312, 1266, 281, 264, 1347, 295, 441, 3850, 50940, 50940, 370, 586, 437, 321, 434, 884, 307, 321, 434, 16821, 43586, 1296, 264, 12680, 791, 51126, 51126, 295, 613, 2539, 6846, 341, 307, 1958, 13, 628, 16, 293, 341, 307, 502, 570, 1266, 281, 264, 51406, 51406, 1347, 295, 1958, 307, 502, 293, 4412, 321, 366, 43766, 37330, 294, 341, 15035, 370, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.11179192860921223, "compression_ratio": 1.7605633802816902, "no_speech_prob": 6.048675459169317e-06}, {"id": 554, "seek": 285616, "start": 2871.3999999999996, "end": 2877.0, "text": " of these learning rates this is 0.001 and this is 1 because 10 to the", "tokens": [50364, 2602, 295, 1958, 13, 628, 16, 341, 486, 312, 257, 3671, 805, 293, 341, 486, 312, 257, 1958, 293, 550, 264, 50668, 50668, 3539, 441, 49, 82, 300, 321, 528, 281, 3164, 670, 366, 516, 281, 312, 1266, 281, 264, 1347, 295, 441, 3850, 50940, 50940, 370, 586, 437, 321, 434, 884, 307, 321, 434, 16821, 43586, 1296, 264, 12680, 791, 51126, 51126, 295, 613, 2539, 6846, 341, 307, 1958, 13, 628, 16, 293, 341, 307, 502, 570, 1266, 281, 264, 51406, 51406, 1347, 295, 1958, 307, 502, 293, 4412, 321, 366, 43766, 37330, 294, 341, 15035, 370, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.11179192860921223, "compression_ratio": 1.7605633802816902, "no_speech_prob": 6.048675459169317e-06}, {"id": 555, "seek": 285616, "start": 2877.0, "end": 2882.48, "text": " power of 0 is 1 and therefore we are spaced exponentially in this interval so", "tokens": [50364, 2602, 295, 1958, 13, 628, 16, 341, 486, 312, 257, 3671, 805, 293, 341, 486, 312, 257, 1958, 293, 550, 264, 50668, 50668, 3539, 441, 49, 82, 300, 321, 528, 281, 3164, 670, 366, 516, 281, 312, 1266, 281, 264, 1347, 295, 441, 3850, 50940, 50940, 370, 586, 437, 321, 434, 884, 307, 321, 434, 16821, 43586, 1296, 264, 12680, 791, 51126, 51126, 295, 613, 2539, 6846, 341, 307, 1958, 13, 628, 16, 293, 341, 307, 502, 570, 1266, 281, 264, 51406, 51406, 1347, 295, 1958, 307, 502, 293, 4412, 321, 366, 43766, 37330, 294, 341, 15035, 370, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.11179192860921223, "compression_ratio": 1.7605633802816902, "no_speech_prob": 6.048675459169317e-06}, {"id": 556, "seek": 288248, "start": 2882.48, "end": 2886.52, "text": " these are the candidate learning rates that we want to sort of like search over", "tokens": [50364, 613, 366, 264, 11532, 2539, 6846, 300, 321, 528, 281, 1333, 295, 411, 3164, 670, 50566, 50566, 9810, 370, 586, 437, 321, 434, 516, 281, 360, 307, 510, 321, 366, 516, 281, 1190, 264, 50874, 50874, 19618, 337, 9714, 4439, 293, 2602, 295, 1228, 257, 6806, 1230, 321, 366, 516, 281, 51116, 51116, 764, 2539, 3314, 8186, 278, 666, 510, 441, 49, 82, 295, 741, 293, 652, 341, 741, 370, 1936, 718, 385, 51578, 51578, 14322, 341, 281, 312, 797, 2891, 490, 4974, 4084, 613, 2539, 6846, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1107623026921199, "compression_ratio": 1.7342342342342343, "no_speech_prob": 9.132344871431997e-07}, {"id": 557, "seek": 288248, "start": 2886.52, "end": 2892.68, "text": " roughly so now what we're going to do is here we are going to run the", "tokens": [50364, 613, 366, 264, 11532, 2539, 6846, 300, 321, 528, 281, 1333, 295, 411, 3164, 670, 50566, 50566, 9810, 370, 586, 437, 321, 434, 516, 281, 360, 307, 510, 321, 366, 516, 281, 1190, 264, 50874, 50874, 19618, 337, 9714, 4439, 293, 2602, 295, 1228, 257, 6806, 1230, 321, 366, 516, 281, 51116, 51116, 764, 2539, 3314, 8186, 278, 666, 510, 441, 49, 82, 295, 741, 293, 652, 341, 741, 370, 1936, 718, 385, 51578, 51578, 14322, 341, 281, 312, 797, 2891, 490, 4974, 4084, 613, 2539, 6846, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1107623026921199, "compression_ratio": 1.7342342342342343, "no_speech_prob": 9.132344871431997e-07}, {"id": 558, "seek": 288248, "start": 2892.68, "end": 2897.52, "text": " optimization for 1000 steps and instead of using a fixed number we are going to", "tokens": [50364, 613, 366, 264, 11532, 2539, 6846, 300, 321, 528, 281, 1333, 295, 411, 3164, 670, 50566, 50566, 9810, 370, 586, 437, 321, 434, 516, 281, 360, 307, 510, 321, 366, 516, 281, 1190, 264, 50874, 50874, 19618, 337, 9714, 4439, 293, 2602, 295, 1228, 257, 6806, 1230, 321, 366, 516, 281, 51116, 51116, 764, 2539, 3314, 8186, 278, 666, 510, 441, 49, 82, 295, 741, 293, 652, 341, 741, 370, 1936, 718, 385, 51578, 51578, 14322, 341, 281, 312, 797, 2891, 490, 4974, 4084, 613, 2539, 6846, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1107623026921199, "compression_ratio": 1.7342342342342343, "no_speech_prob": 9.132344871431997e-07}, {"id": 559, "seek": 288248, "start": 2897.52, "end": 2906.76, "text": " use learning rate indexing into here LRs of i and make this i so basically let me", "tokens": [50364, 613, 366, 264, 11532, 2539, 6846, 300, 321, 528, 281, 1333, 295, 411, 3164, 670, 50566, 50566, 9810, 370, 586, 437, 321, 434, 516, 281, 360, 307, 510, 321, 366, 516, 281, 1190, 264, 50874, 50874, 19618, 337, 9714, 4439, 293, 2602, 295, 1228, 257, 6806, 1230, 321, 366, 516, 281, 51116, 51116, 764, 2539, 3314, 8186, 278, 666, 510, 441, 49, 82, 295, 741, 293, 652, 341, 741, 370, 1936, 718, 385, 51578, 51578, 14322, 341, 281, 312, 797, 2891, 490, 4974, 4084, 613, 2539, 6846, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1107623026921199, "compression_ratio": 1.7342342342342343, "no_speech_prob": 9.132344871431997e-07}, {"id": 560, "seek": 288248, "start": 2906.76, "end": 2911.64, "text": " reset this to be again starting from random creating these learning rates", "tokens": [50364, 613, 366, 264, 11532, 2539, 6846, 300, 321, 528, 281, 1333, 295, 411, 3164, 670, 50566, 50566, 9810, 370, 586, 437, 321, 434, 516, 281, 360, 307, 510, 321, 366, 516, 281, 1190, 264, 50874, 50874, 19618, 337, 9714, 4439, 293, 2602, 295, 1228, 257, 6806, 1230, 321, 366, 516, 281, 51116, 51116, 764, 2539, 3314, 8186, 278, 666, 510, 441, 49, 82, 295, 741, 293, 652, 341, 741, 370, 1936, 718, 385, 51578, 51578, 14322, 341, 281, 312, 797, 2891, 490, 4974, 4084, 613, 2539, 6846, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1107623026921199, "compression_ratio": 1.7342342342342343, "no_speech_prob": 9.132344871431997e-07}, {"id": 561, "seek": 291164, "start": 2911.64, "end": 2920.4, "text": " between negative 0.001 and 1 but exponentially stepped and here what", "tokens": [50364, 1296, 3671, 1958, 13, 628, 16, 293, 502, 457, 37330, 15251, 293, 510, 437, 50802, 50802, 321, 434, 884, 307, 321, 434, 17138, 990, 257, 4714, 1413, 321, 434, 516, 281, 764, 264, 50999, 50999, 2539, 3314, 300, 311, 294, 264, 2863, 588, 588, 2295, 294, 264, 2863, 307, 516, 51238, 51238, 281, 312, 1958, 13, 628, 16, 457, 538, 264, 917, 309, 311, 516, 281, 312, 502, 293, 550, 321, 434, 516, 281, 1823, 365, 51534, 51534, 300, 2539, 3314, 293, 586, 437, 321, 528, 281, 360, 307, 321, 528, 281, 1066, 2837, 295, 264, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.13193622589111328, "compression_ratio": 1.9329896907216495, "no_speech_prob": 8.267700650321785e-06}, {"id": 562, "seek": 291164, "start": 2920.4, "end": 2924.3399999999997, "text": " we're doing is we're iterating a thousand times we're going to use the", "tokens": [50364, 1296, 3671, 1958, 13, 628, 16, 293, 502, 457, 37330, 15251, 293, 510, 437, 50802, 50802, 321, 434, 884, 307, 321, 434, 17138, 990, 257, 4714, 1413, 321, 434, 516, 281, 764, 264, 50999, 50999, 2539, 3314, 300, 311, 294, 264, 2863, 588, 588, 2295, 294, 264, 2863, 307, 516, 51238, 51238, 281, 312, 1958, 13, 628, 16, 457, 538, 264, 917, 309, 311, 516, 281, 312, 502, 293, 550, 321, 434, 516, 281, 1823, 365, 51534, 51534, 300, 2539, 3314, 293, 586, 437, 321, 528, 281, 360, 307, 321, 528, 281, 1066, 2837, 295, 264, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.13193622589111328, "compression_ratio": 1.9329896907216495, "no_speech_prob": 8.267700650321785e-06}, {"id": 563, "seek": 291164, "start": 2924.3399999999997, "end": 2929.12, "text": " learning rate that's in the beginning very very low in the beginning is going", "tokens": [50364, 1296, 3671, 1958, 13, 628, 16, 293, 502, 457, 37330, 15251, 293, 510, 437, 50802, 50802, 321, 434, 884, 307, 321, 434, 17138, 990, 257, 4714, 1413, 321, 434, 516, 281, 764, 264, 50999, 50999, 2539, 3314, 300, 311, 294, 264, 2863, 588, 588, 2295, 294, 264, 2863, 307, 516, 51238, 51238, 281, 312, 1958, 13, 628, 16, 457, 538, 264, 917, 309, 311, 516, 281, 312, 502, 293, 550, 321, 434, 516, 281, 1823, 365, 51534, 51534, 300, 2539, 3314, 293, 586, 437, 321, 528, 281, 360, 307, 321, 528, 281, 1066, 2837, 295, 264, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.13193622589111328, "compression_ratio": 1.9329896907216495, "no_speech_prob": 8.267700650321785e-06}, {"id": 564, "seek": 291164, "start": 2929.12, "end": 2935.04, "text": " to be 0.001 but by the end it's going to be 1 and then we're going to step with", "tokens": [50364, 1296, 3671, 1958, 13, 628, 16, 293, 502, 457, 37330, 15251, 293, 510, 437, 50802, 50802, 321, 434, 884, 307, 321, 434, 17138, 990, 257, 4714, 1413, 321, 434, 516, 281, 764, 264, 50999, 50999, 2539, 3314, 300, 311, 294, 264, 2863, 588, 588, 2295, 294, 264, 2863, 307, 516, 51238, 51238, 281, 312, 1958, 13, 628, 16, 457, 538, 264, 917, 309, 311, 516, 281, 312, 502, 293, 550, 321, 434, 516, 281, 1823, 365, 51534, 51534, 300, 2539, 3314, 293, 586, 437, 321, 528, 281, 360, 307, 321, 528, 281, 1066, 2837, 295, 264, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.13193622589111328, "compression_ratio": 1.9329896907216495, "no_speech_prob": 8.267700650321785e-06}, {"id": 565, "seek": 291164, "start": 2935.04, "end": 2941.2, "text": " that learning rate and now what we want to do is we want to keep track of the", "tokens": [50364, 1296, 3671, 1958, 13, 628, 16, 293, 502, 457, 37330, 15251, 293, 510, 437, 50802, 50802, 321, 434, 884, 307, 321, 434, 17138, 990, 257, 4714, 1413, 321, 434, 516, 281, 764, 264, 50999, 50999, 2539, 3314, 300, 311, 294, 264, 2863, 588, 588, 2295, 294, 264, 2863, 307, 516, 51238, 51238, 281, 312, 1958, 13, 628, 16, 457, 538, 264, 917, 309, 311, 516, 281, 312, 502, 293, 550, 321, 434, 516, 281, 1823, 365, 51534, 51534, 300, 2539, 3314, 293, 586, 437, 321, 528, 281, 360, 307, 321, 528, 281, 1066, 2837, 295, 264, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.13193622589111328, "compression_ratio": 1.9329896907216495, "no_speech_prob": 8.267700650321785e-06}, {"id": 566, "seek": 294120, "start": 2941.2, "end": 2949.24, "text": " learning rates that we used and we want to look at the losses that resulted and", "tokens": [50364, 2539, 6846, 300, 321, 1143, 293, 321, 528, 281, 574, 412, 264, 15352, 300, 18753, 293, 50766, 50766, 370, 510, 718, 385, 2837, 18152, 370, 441, 5577, 13, 1746, 521, 441, 49, 293, 4470, 1252, 13, 1746, 521, 4470, 13, 270, 443, 51396, 51396], "temperature": 0.0, "avg_logprob": -0.2508753071660581, "compression_ratio": 1.4128440366972477, "no_speech_prob": 8.013178558030631e-06}, {"id": 567, "seek": 294120, "start": 2949.24, "end": 2961.8399999999997, "text": " so here let me track stats so LRI.append LR and loss side.append loss.item", "tokens": [50364, 2539, 6846, 300, 321, 1143, 293, 321, 528, 281, 574, 412, 264, 15352, 300, 18753, 293, 50766, 50766, 370, 510, 718, 385, 2837, 18152, 370, 441, 5577, 13, 1746, 521, 441, 49, 293, 4470, 1252, 13, 1746, 521, 4470, 13, 270, 443, 51396, 51396], "temperature": 0.0, "avg_logprob": -0.2508753071660581, "compression_ratio": 1.4128440366972477, "no_speech_prob": 8.013178558030631e-06}, {"id": 568, "seek": 296184, "start": 2961.84, "end": 2971.6800000000003, "text": " okay so again reset everything and then run and so basically we started with a", "tokens": [50364, 1392, 370, 797, 14322, 1203, 293, 550, 1190, 293, 370, 1936, 321, 1409, 365, 257, 50856, 50856, 588, 2295, 2539, 3314, 293, 321, 1437, 439, 264, 636, 493, 281, 2539, 3314, 295, 3671, 51036, 51036, 502, 293, 586, 437, 321, 393, 360, 307, 321, 393, 280, 287, 256, 300, 7542, 293, 321, 393, 7542, 264, 732, 370, 321, 51322, 51322, 393, 7542, 264, 2539, 6846, 322, 264, 2031, 12, 24633, 293, 264, 15352, 321, 1866, 322, 264, 288, 12, 24633, 293, 51566, 51566, 2049, 291, 434, 516, 281, 915, 300, 428, 7542, 1542, 746, 411, 341, 689, 294, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.14898858350865982, "compression_ratio": 1.8904761904761904, "no_speech_prob": 1.1125224773422815e-05}, {"id": 569, "seek": 296184, "start": 2971.6800000000003, "end": 2975.28, "text": " very low learning rate and we went all the way up to learning rate of negative", "tokens": [50364, 1392, 370, 797, 14322, 1203, 293, 550, 1190, 293, 370, 1936, 321, 1409, 365, 257, 50856, 50856, 588, 2295, 2539, 3314, 293, 321, 1437, 439, 264, 636, 493, 281, 2539, 3314, 295, 3671, 51036, 51036, 502, 293, 586, 437, 321, 393, 360, 307, 321, 393, 280, 287, 256, 300, 7542, 293, 321, 393, 7542, 264, 732, 370, 321, 51322, 51322, 393, 7542, 264, 2539, 6846, 322, 264, 2031, 12, 24633, 293, 264, 15352, 321, 1866, 322, 264, 288, 12, 24633, 293, 51566, 51566, 2049, 291, 434, 516, 281, 915, 300, 428, 7542, 1542, 746, 411, 341, 689, 294, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.14898858350865982, "compression_ratio": 1.8904761904761904, "no_speech_prob": 1.1125224773422815e-05}, {"id": 570, "seek": 296184, "start": 2975.28, "end": 2981.0, "text": " 1 and now what we can do is we can p l t that plot and we can plot the two so we", "tokens": [50364, 1392, 370, 797, 14322, 1203, 293, 550, 1190, 293, 370, 1936, 321, 1409, 365, 257, 50856, 50856, 588, 2295, 2539, 3314, 293, 321, 1437, 439, 264, 636, 493, 281, 2539, 3314, 295, 3671, 51036, 51036, 502, 293, 586, 437, 321, 393, 360, 307, 321, 393, 280, 287, 256, 300, 7542, 293, 321, 393, 7542, 264, 732, 370, 321, 51322, 51322, 393, 7542, 264, 2539, 6846, 322, 264, 2031, 12, 24633, 293, 264, 15352, 321, 1866, 322, 264, 288, 12, 24633, 293, 51566, 51566, 2049, 291, 434, 516, 281, 915, 300, 428, 7542, 1542, 746, 411, 341, 689, 294, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.14898858350865982, "compression_ratio": 1.8904761904761904, "no_speech_prob": 1.1125224773422815e-05}, {"id": 571, "seek": 296184, "start": 2981.0, "end": 2985.88, "text": " can plot the learning rates on the x-axis and the losses we saw on the y-axis and", "tokens": [50364, 1392, 370, 797, 14322, 1203, 293, 550, 1190, 293, 370, 1936, 321, 1409, 365, 257, 50856, 50856, 588, 2295, 2539, 3314, 293, 321, 1437, 439, 264, 636, 493, 281, 2539, 3314, 295, 3671, 51036, 51036, 502, 293, 586, 437, 321, 393, 360, 307, 321, 393, 280, 287, 256, 300, 7542, 293, 321, 393, 7542, 264, 732, 370, 321, 51322, 51322, 393, 7542, 264, 2539, 6846, 322, 264, 2031, 12, 24633, 293, 264, 15352, 321, 1866, 322, 264, 288, 12, 24633, 293, 51566, 51566, 2049, 291, 434, 516, 281, 915, 300, 428, 7542, 1542, 746, 411, 341, 689, 294, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.14898858350865982, "compression_ratio": 1.8904761904761904, "no_speech_prob": 1.1125224773422815e-05}, {"id": 572, "seek": 296184, "start": 2985.88, "end": 2990.6800000000003, "text": " often you're going to find that your plot looks something like this where in", "tokens": [50364, 1392, 370, 797, 14322, 1203, 293, 550, 1190, 293, 370, 1936, 321, 1409, 365, 257, 50856, 50856, 588, 2295, 2539, 3314, 293, 321, 1437, 439, 264, 636, 493, 281, 2539, 3314, 295, 3671, 51036, 51036, 502, 293, 586, 437, 321, 393, 360, 307, 321, 393, 280, 287, 256, 300, 7542, 293, 321, 393, 7542, 264, 732, 370, 321, 51322, 51322, 393, 7542, 264, 2539, 6846, 322, 264, 2031, 12, 24633, 293, 264, 15352, 321, 1866, 322, 264, 288, 12, 24633, 293, 51566, 51566, 2049, 291, 434, 516, 281, 915, 300, 428, 7542, 1542, 746, 411, 341, 689, 294, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.14898858350865982, "compression_ratio": 1.8904761904761904, "no_speech_prob": 1.1125224773422815e-05}, {"id": 573, "seek": 299068, "start": 2990.68, "end": 2995.44, "text": " the beginning you have very low learning rates we basically anything barely", "tokens": [50364, 264, 2863, 291, 362, 588, 2295, 2539, 6846, 321, 1936, 1340, 10268, 50602, 50602, 1340, 2011, 550, 321, 658, 281, 411, 257, 1481, 4008, 510, 293, 550, 382, 321, 3488, 50890, 50890, 264, 2539, 3314, 1547, 321, 1936, 1409, 281, 312, 733, 295, 23742, 510, 370, 257, 51142, 51142, 665, 2539, 3314, 4523, 484, 281, 312, 4079, 926, 510, 293, 570, 321, 51405, 51405], "temperature": 0.0, "avg_logprob": -0.10013183680447665, "compression_ratio": 1.8484848484848484, "no_speech_prob": 3.66873109669541e-06}, {"id": 574, "seek": 299068, "start": 2995.44, "end": 3001.2, "text": " anything happened then we got to like a nice spot here and then as we increase", "tokens": [50364, 264, 2863, 291, 362, 588, 2295, 2539, 6846, 321, 1936, 1340, 10268, 50602, 50602, 1340, 2011, 550, 321, 658, 281, 411, 257, 1481, 4008, 510, 293, 550, 382, 321, 3488, 50890, 50890, 264, 2539, 3314, 1547, 321, 1936, 1409, 281, 312, 733, 295, 23742, 510, 370, 257, 51142, 51142, 665, 2539, 3314, 4523, 484, 281, 312, 4079, 926, 510, 293, 570, 321, 51405, 51405], "temperature": 0.0, "avg_logprob": -0.10013183680447665, "compression_ratio": 1.8484848484848484, "no_speech_prob": 3.66873109669541e-06}, {"id": 575, "seek": 299068, "start": 3001.2, "end": 3006.24, "text": " the learning rate enough we basically started to be kind of unstable here so a", "tokens": [50364, 264, 2863, 291, 362, 588, 2295, 2539, 6846, 321, 1936, 1340, 10268, 50602, 50602, 1340, 2011, 550, 321, 658, 281, 411, 257, 1481, 4008, 510, 293, 550, 382, 321, 3488, 50890, 50890, 264, 2539, 3314, 1547, 321, 1936, 1409, 281, 312, 733, 295, 23742, 510, 370, 257, 51142, 51142, 665, 2539, 3314, 4523, 484, 281, 312, 4079, 926, 510, 293, 570, 321, 51405, 51405], "temperature": 0.0, "avg_logprob": -0.10013183680447665, "compression_ratio": 1.8484848484848484, "no_speech_prob": 3.66873109669541e-06}, {"id": 576, "seek": 299068, "start": 3006.24, "end": 3011.5, "text": " good learning rate turns out to be somewhere around here and because we", "tokens": [50364, 264, 2863, 291, 362, 588, 2295, 2539, 6846, 321, 1936, 1340, 10268, 50602, 50602, 1340, 2011, 550, 321, 658, 281, 411, 257, 1481, 4008, 510, 293, 550, 382, 321, 3488, 50890, 50890, 264, 2539, 3314, 1547, 321, 1936, 1409, 281, 312, 733, 295, 23742, 510, 370, 257, 51142, 51142, 665, 2539, 3314, 4523, 484, 281, 312, 4079, 926, 510, 293, 570, 321, 51405, 51405], "temperature": 0.0, "avg_logprob": -0.10013183680447665, "compression_ratio": 1.8484848484848484, "no_speech_prob": 3.66873109669541e-06}, {"id": 577, "seek": 301150, "start": 3011.5, "end": 3022.28, "text": " have LRI here we actually may want to do not LR not the learning rate but the", "tokens": [50364, 362, 441, 5577, 510, 321, 767, 815, 528, 281, 360, 406, 441, 49, 406, 264, 2539, 3314, 457, 264, 50903, 50903, 37871, 370, 300, 576, 312, 264, 441, 3850, 412, 286, 307, 1310, 437, 321, 486, 3565, 370, 718, 385, 51147, 51147, 14322, 341, 293, 29956, 300, 17108, 457, 586, 322, 264, 2031, 12, 24633, 321, 362, 264, 37871, 51525, 51525, 295, 264, 2539, 3314, 293, 370, 321, 393, 536, 264, 37871, 295, 264, 2539, 3314, 300, 51687, 51687], "temperature": 0.0, "avg_logprob": -0.08965816026852455, "compression_ratio": 1.8128654970760234, "no_speech_prob": 5.771778432972496e-06}, {"id": 578, "seek": 301150, "start": 3022.28, "end": 3027.16, "text": " exponent so that would be the LRE at I is maybe what we will log so let me", "tokens": [50364, 362, 441, 5577, 510, 321, 767, 815, 528, 281, 360, 406, 441, 49, 406, 264, 2539, 3314, 457, 264, 50903, 50903, 37871, 370, 300, 576, 312, 264, 441, 3850, 412, 286, 307, 1310, 437, 321, 486, 3565, 370, 718, 385, 51147, 51147, 14322, 341, 293, 29956, 300, 17108, 457, 586, 322, 264, 2031, 12, 24633, 321, 362, 264, 37871, 51525, 51525, 295, 264, 2539, 3314, 293, 370, 321, 393, 536, 264, 37871, 295, 264, 2539, 3314, 300, 51687, 51687], "temperature": 0.0, "avg_logprob": -0.08965816026852455, "compression_ratio": 1.8128654970760234, "no_speech_prob": 5.771778432972496e-06}, {"id": 579, "seek": 301150, "start": 3027.16, "end": 3034.72, "text": " reset this and redo that calculation but now on the x-axis we have the exponent", "tokens": [50364, 362, 441, 5577, 510, 321, 767, 815, 528, 281, 360, 406, 441, 49, 406, 264, 2539, 3314, 457, 264, 50903, 50903, 37871, 370, 300, 576, 312, 264, 441, 3850, 412, 286, 307, 1310, 437, 321, 486, 3565, 370, 718, 385, 51147, 51147, 14322, 341, 293, 29956, 300, 17108, 457, 586, 322, 264, 2031, 12, 24633, 321, 362, 264, 37871, 51525, 51525, 295, 264, 2539, 3314, 293, 370, 321, 393, 536, 264, 37871, 295, 264, 2539, 3314, 300, 51687, 51687], "temperature": 0.0, "avg_logprob": -0.08965816026852455, "compression_ratio": 1.8128654970760234, "no_speech_prob": 5.771778432972496e-06}, {"id": 580, "seek": 301150, "start": 3034.72, "end": 3037.96, "text": " of the learning rate and so we can see the exponent of the learning rate that", "tokens": [50364, 362, 441, 5577, 510, 321, 767, 815, 528, 281, 360, 406, 441, 49, 406, 264, 2539, 3314, 457, 264, 50903, 50903, 37871, 370, 300, 576, 312, 264, 441, 3850, 412, 286, 307, 1310, 437, 321, 486, 3565, 370, 718, 385, 51147, 51147, 14322, 341, 293, 29956, 300, 17108, 457, 586, 322, 264, 2031, 12, 24633, 321, 362, 264, 37871, 51525, 51525, 295, 264, 2539, 3314, 293, 370, 321, 393, 536, 264, 37871, 295, 264, 2539, 3314, 300, 51687, 51687], "temperature": 0.0, "avg_logprob": -0.08965816026852455, "compression_ratio": 1.8128654970760234, "no_speech_prob": 5.771778432972496e-06}, {"id": 581, "seek": 303796, "start": 3037.96, "end": 3042.04, "text": " is good to use it would be sort of like roughly in the valley here because here", "tokens": [50364, 307, 665, 281, 764, 309, 576, 312, 1333, 295, 411, 9810, 294, 264, 17636, 510, 570, 510, 50568, 50568, 264, 2539, 6846, 366, 445, 636, 886, 2295, 293, 550, 510, 321, 434, 321, 2066, 7226, 50756, 50756, 665, 2539, 3314, 4079, 510, 293, 550, 510, 721, 366, 2891, 281, 21411, 50902, 50902, 370, 4079, 926, 3671, 502, 2031, 264, 37871, 295, 264, 2539, 3314, 307, 257, 51102, 51102, 1238, 665, 3287, 293, 1266, 281, 264, 3671, 502, 307, 1958, 13, 16, 370, 1958, 13, 16, 307, 767, 51400, 51400, 10366, 472, 390, 767, 257, 6457, 665, 2539, 3314, 926, 510, 293, 300, 311, 51600, 51600, 437, 321, 632, 294, 264, 5883, 3287, 457, 300, 311, 9810, 577, 291, 576, 6997, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.15179169562555128, "compression_ratio": 2.05, "no_speech_prob": 1.9525125026120804e-05}, {"id": 582, "seek": 303796, "start": 3042.04, "end": 3045.8, "text": " the learning rates are just way too low and then here we're we expect relatively", "tokens": [50364, 307, 665, 281, 764, 309, 576, 312, 1333, 295, 411, 9810, 294, 264, 17636, 510, 570, 510, 50568, 50568, 264, 2539, 6846, 366, 445, 636, 886, 2295, 293, 550, 510, 321, 434, 321, 2066, 7226, 50756, 50756, 665, 2539, 3314, 4079, 510, 293, 550, 510, 721, 366, 2891, 281, 21411, 50902, 50902, 370, 4079, 926, 3671, 502, 2031, 264, 37871, 295, 264, 2539, 3314, 307, 257, 51102, 51102, 1238, 665, 3287, 293, 1266, 281, 264, 3671, 502, 307, 1958, 13, 16, 370, 1958, 13, 16, 307, 767, 51400, 51400, 10366, 472, 390, 767, 257, 6457, 665, 2539, 3314, 926, 510, 293, 300, 311, 51600, 51600, 437, 321, 632, 294, 264, 5883, 3287, 457, 300, 311, 9810, 577, 291, 576, 6997, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.15179169562555128, "compression_ratio": 2.05, "no_speech_prob": 1.9525125026120804e-05}, {"id": 583, "seek": 303796, "start": 3045.8, "end": 3048.7200000000003, "text": " good learning rate somewhere here and then here things are starting to explode", "tokens": [50364, 307, 665, 281, 764, 309, 576, 312, 1333, 295, 411, 9810, 294, 264, 17636, 510, 570, 510, 50568, 50568, 264, 2539, 6846, 366, 445, 636, 886, 2295, 293, 550, 510, 321, 434, 321, 2066, 7226, 50756, 50756, 665, 2539, 3314, 4079, 510, 293, 550, 510, 721, 366, 2891, 281, 21411, 50902, 50902, 370, 4079, 926, 3671, 502, 2031, 264, 37871, 295, 264, 2539, 3314, 307, 257, 51102, 51102, 1238, 665, 3287, 293, 1266, 281, 264, 3671, 502, 307, 1958, 13, 16, 370, 1958, 13, 16, 307, 767, 51400, 51400, 10366, 472, 390, 767, 257, 6457, 665, 2539, 3314, 926, 510, 293, 300, 311, 51600, 51600, 437, 321, 632, 294, 264, 5883, 3287, 457, 300, 311, 9810, 577, 291, 576, 6997, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.15179169562555128, "compression_ratio": 2.05, "no_speech_prob": 1.9525125026120804e-05}, {"id": 584, "seek": 303796, "start": 3048.7200000000003, "end": 3052.7200000000003, "text": " so somewhere around negative 1 x the exponent of the learning rate is a", "tokens": [50364, 307, 665, 281, 764, 309, 576, 312, 1333, 295, 411, 9810, 294, 264, 17636, 510, 570, 510, 50568, 50568, 264, 2539, 6846, 366, 445, 636, 886, 2295, 293, 550, 510, 321, 434, 321, 2066, 7226, 50756, 50756, 665, 2539, 3314, 4079, 510, 293, 550, 510, 721, 366, 2891, 281, 21411, 50902, 50902, 370, 4079, 926, 3671, 502, 2031, 264, 37871, 295, 264, 2539, 3314, 307, 257, 51102, 51102, 1238, 665, 3287, 293, 1266, 281, 264, 3671, 502, 307, 1958, 13, 16, 370, 1958, 13, 16, 307, 767, 51400, 51400, 10366, 472, 390, 767, 257, 6457, 665, 2539, 3314, 926, 510, 293, 300, 311, 51600, 51600, 437, 321, 632, 294, 264, 5883, 3287, 457, 300, 311, 9810, 577, 291, 576, 6997, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.15179169562555128, "compression_ratio": 2.05, "no_speech_prob": 1.9525125026120804e-05}, {"id": 585, "seek": 303796, "start": 3052.7200000000003, "end": 3058.68, "text": " pretty good setting and 10 to the negative 1 is 0.1 so 0.1 is actually", "tokens": [50364, 307, 665, 281, 764, 309, 576, 312, 1333, 295, 411, 9810, 294, 264, 17636, 510, 570, 510, 50568, 50568, 264, 2539, 6846, 366, 445, 636, 886, 2295, 293, 550, 510, 321, 434, 321, 2066, 7226, 50756, 50756, 665, 2539, 3314, 4079, 510, 293, 550, 510, 721, 366, 2891, 281, 21411, 50902, 50902, 370, 4079, 926, 3671, 502, 2031, 264, 37871, 295, 264, 2539, 3314, 307, 257, 51102, 51102, 1238, 665, 3287, 293, 1266, 281, 264, 3671, 502, 307, 1958, 13, 16, 370, 1958, 13, 16, 307, 767, 51400, 51400, 10366, 472, 390, 767, 257, 6457, 665, 2539, 3314, 926, 510, 293, 300, 311, 51600, 51600, 437, 321, 632, 294, 264, 5883, 3287, 457, 300, 311, 9810, 577, 291, 576, 6997, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.15179169562555128, "compression_ratio": 2.05, "no_speech_prob": 1.9525125026120804e-05}, {"id": 586, "seek": 303796, "start": 3058.68, "end": 3062.68, "text": " quit one was actually a fairly good learning rate around here and that's", "tokens": [50364, 307, 665, 281, 764, 309, 576, 312, 1333, 295, 411, 9810, 294, 264, 17636, 510, 570, 510, 50568, 50568, 264, 2539, 6846, 366, 445, 636, 886, 2295, 293, 550, 510, 321, 434, 321, 2066, 7226, 50756, 50756, 665, 2539, 3314, 4079, 510, 293, 550, 510, 721, 366, 2891, 281, 21411, 50902, 50902, 370, 4079, 926, 3671, 502, 2031, 264, 37871, 295, 264, 2539, 3314, 307, 257, 51102, 51102, 1238, 665, 3287, 293, 1266, 281, 264, 3671, 502, 307, 1958, 13, 16, 370, 1958, 13, 16, 307, 767, 51400, 51400, 10366, 472, 390, 767, 257, 6457, 665, 2539, 3314, 926, 510, 293, 300, 311, 51600, 51600, 437, 321, 632, 294, 264, 5883, 3287, 457, 300, 311, 9810, 577, 291, 576, 6997, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.15179169562555128, "compression_ratio": 2.05, "no_speech_prob": 1.9525125026120804e-05}, {"id": 587, "seek": 303796, "start": 3062.68, "end": 3067.12, "text": " what we had in the initial setting but that's roughly how you would determine", "tokens": [50364, 307, 665, 281, 764, 309, 576, 312, 1333, 295, 411, 9810, 294, 264, 17636, 510, 570, 510, 50568, 50568, 264, 2539, 6846, 366, 445, 636, 886, 2295, 293, 550, 510, 321, 434, 321, 2066, 7226, 50756, 50756, 665, 2539, 3314, 4079, 510, 293, 550, 510, 721, 366, 2891, 281, 21411, 50902, 50902, 370, 4079, 926, 3671, 502, 2031, 264, 37871, 295, 264, 2539, 3314, 307, 257, 51102, 51102, 1238, 665, 3287, 293, 1266, 281, 264, 3671, 502, 307, 1958, 13, 16, 370, 1958, 13, 16, 307, 767, 51400, 51400, 10366, 472, 390, 767, 257, 6457, 665, 2539, 3314, 926, 510, 293, 300, 311, 51600, 51600, 437, 321, 632, 294, 264, 5883, 3287, 457, 300, 311, 9810, 577, 291, 576, 6997, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.15179169562555128, "compression_ratio": 2.05, "no_speech_prob": 1.9525125026120804e-05}, {"id": 588, "seek": 306712, "start": 3067.12, "end": 3074.14, "text": " it and so here now we can take out the tracking of these and we can just", "tokens": [50364, 309, 293, 370, 510, 586, 321, 393, 747, 484, 264, 11603, 295, 613, 293, 321, 393, 445, 50715, 50715, 2935, 992, 257, 441, 49, 281, 312, 1266, 281, 264, 3671, 502, 420, 1936, 5911, 1958, 13, 16, 382, 309, 390, 51010, 51010, 949, 293, 586, 321, 362, 512, 6687, 300, 341, 307, 767, 257, 6457, 665, 51164, 51164, 2539, 3314, 293, 370, 586, 437, 321, 393, 360, 307, 321, 393, 21263, 493, 264, 36540, 321, 393, 51426, 51426, 14322, 527, 19618, 293, 321, 393, 1190, 337, 257, 1238, 938, 565, 1228, 341, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.1249282956123352, "compression_ratio": 1.704035874439462, "no_speech_prob": 1.670102938078344e-05}, {"id": 589, "seek": 306712, "start": 3074.14, "end": 3080.04, "text": " simply set a LR to be 10 to the negative 1 or basically otherwise 0.1 as it was", "tokens": [50364, 309, 293, 370, 510, 586, 321, 393, 747, 484, 264, 11603, 295, 613, 293, 321, 393, 445, 50715, 50715, 2935, 992, 257, 441, 49, 281, 312, 1266, 281, 264, 3671, 502, 420, 1936, 5911, 1958, 13, 16, 382, 309, 390, 51010, 51010, 949, 293, 586, 321, 362, 512, 6687, 300, 341, 307, 767, 257, 6457, 665, 51164, 51164, 2539, 3314, 293, 370, 586, 437, 321, 393, 360, 307, 321, 393, 21263, 493, 264, 36540, 321, 393, 51426, 51426, 14322, 527, 19618, 293, 321, 393, 1190, 337, 257, 1238, 938, 565, 1228, 341, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.1249282956123352, "compression_ratio": 1.704035874439462, "no_speech_prob": 1.670102938078344e-05}, {"id": 590, "seek": 306712, "start": 3080.04, "end": 3083.12, "text": " before and now we have some confidence that this is actually a fairly good", "tokens": [50364, 309, 293, 370, 510, 586, 321, 393, 747, 484, 264, 11603, 295, 613, 293, 321, 393, 445, 50715, 50715, 2935, 992, 257, 441, 49, 281, 312, 1266, 281, 264, 3671, 502, 420, 1936, 5911, 1958, 13, 16, 382, 309, 390, 51010, 51010, 949, 293, 586, 321, 362, 512, 6687, 300, 341, 307, 767, 257, 6457, 665, 51164, 51164, 2539, 3314, 293, 370, 586, 437, 321, 393, 360, 307, 321, 393, 21263, 493, 264, 36540, 321, 393, 51426, 51426, 14322, 527, 19618, 293, 321, 393, 1190, 337, 257, 1238, 938, 565, 1228, 341, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.1249282956123352, "compression_ratio": 1.704035874439462, "no_speech_prob": 1.670102938078344e-05}, {"id": 591, "seek": 306712, "start": 3083.12, "end": 3088.3599999999997, "text": " learning rate and so now what we can do is we can crank up the iterations we can", "tokens": [50364, 309, 293, 370, 510, 586, 321, 393, 747, 484, 264, 11603, 295, 613, 293, 321, 393, 445, 50715, 50715, 2935, 992, 257, 441, 49, 281, 312, 1266, 281, 264, 3671, 502, 420, 1936, 5911, 1958, 13, 16, 382, 309, 390, 51010, 51010, 949, 293, 586, 321, 362, 512, 6687, 300, 341, 307, 767, 257, 6457, 665, 51164, 51164, 2539, 3314, 293, 370, 586, 437, 321, 393, 360, 307, 321, 393, 21263, 493, 264, 36540, 321, 393, 51426, 51426, 14322, 527, 19618, 293, 321, 393, 1190, 337, 257, 1238, 938, 565, 1228, 341, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.1249282956123352, "compression_ratio": 1.704035874439462, "no_speech_prob": 1.670102938078344e-05}, {"id": 592, "seek": 306712, "start": 3088.3599999999997, "end": 3095.0, "text": " reset our optimization and we can run for a pretty long time using this", "tokens": [50364, 309, 293, 370, 510, 586, 321, 393, 747, 484, 264, 11603, 295, 613, 293, 321, 393, 445, 50715, 50715, 2935, 992, 257, 441, 49, 281, 312, 1266, 281, 264, 3671, 502, 420, 1936, 5911, 1958, 13, 16, 382, 309, 390, 51010, 51010, 949, 293, 586, 321, 362, 512, 6687, 300, 341, 307, 767, 257, 6457, 665, 51164, 51164, 2539, 3314, 293, 370, 586, 437, 321, 393, 360, 307, 321, 393, 21263, 493, 264, 36540, 321, 393, 51426, 51426, 14322, 527, 19618, 293, 321, 393, 1190, 337, 257, 1238, 938, 565, 1228, 341, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.1249282956123352, "compression_ratio": 1.704035874439462, "no_speech_prob": 1.670102938078344e-05}, {"id": 593, "seek": 309500, "start": 3095.0, "end": 3100.76, "text": " learning rate oops and we don't want to print it's way too much printing so let", "tokens": [50364, 2539, 3314, 34166, 293, 321, 500, 380, 528, 281, 4482, 309, 311, 636, 886, 709, 14699, 370, 718, 50652, 50652, 385, 797, 14322, 293, 1190, 1266, 11, 1360, 4439, 1392, 370, 321, 434, 412, 935, 281, 568, 13, 13318, 9810, 51192, 51192, 718, 311, 1190, 1071, 1266, 11, 1360, 4439, 568, 13, 16169, 293, 586, 718, 311, 360, 472, 2539, 3314, 21039, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.1734745574719978, "compression_ratio": 1.5298013245033113, "no_speech_prob": 2.392212627455592e-05}, {"id": 594, "seek": 309500, "start": 3100.76, "end": 3111.56, "text": " me again reset and run 10,000 steps okay so we're at point to 2.48 roughly", "tokens": [50364, 2539, 3314, 34166, 293, 321, 500, 380, 528, 281, 4482, 309, 311, 636, 886, 709, 14699, 370, 718, 50652, 50652, 385, 797, 14322, 293, 1190, 1266, 11, 1360, 4439, 1392, 370, 321, 434, 412, 935, 281, 568, 13, 13318, 9810, 51192, 51192, 718, 311, 1190, 1071, 1266, 11, 1360, 4439, 568, 13, 16169, 293, 586, 718, 311, 360, 472, 2539, 3314, 21039, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.1734745574719978, "compression_ratio": 1.5298013245033113, "no_speech_prob": 2.392212627455592e-05}, {"id": 595, "seek": 309500, "start": 3111.56, "end": 3122.36, "text": " let's run another 10,000 steps 2.46 and now let's do one learning rate decay", "tokens": [50364, 2539, 3314, 34166, 293, 321, 500, 380, 528, 281, 4482, 309, 311, 636, 886, 709, 14699, 370, 718, 50652, 50652, 385, 797, 14322, 293, 1190, 1266, 11, 1360, 4439, 1392, 370, 321, 434, 412, 935, 281, 568, 13, 13318, 9810, 51192, 51192, 718, 311, 1190, 1071, 1266, 11, 1360, 4439, 568, 13, 16169, 293, 586, 718, 311, 360, 472, 2539, 3314, 21039, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.1734745574719978, "compression_ratio": 1.5298013245033113, "no_speech_prob": 2.392212627455592e-05}, {"id": 596, "seek": 312236, "start": 3122.36, "end": 3125.84, "text": " what this means is we're going to take our learning rate and we're going to 10x", "tokens": [50364, 437, 341, 1355, 307, 321, 434, 516, 281, 747, 527, 2539, 3314, 293, 321, 434, 516, 281, 1266, 87, 50538, 50538, 3126, 309, 293, 1333, 295, 689, 264, 3469, 10232, 295, 3097, 7263, 293, 321, 50758, 50758, 815, 528, 281, 352, 257, 857, 14009, 718, 311, 360, 472, 544, 767, 257, 1702, 472, 445, 281, 536, 498, 51060, 51060, 321, 434, 1455, 257, 7059, 510, 1392, 321, 434, 920, 1455, 7059, 293, 538, 264, 636, 264, 51308, 51308, 955, 2356, 4470, 300, 321, 11042, 1036, 960, 390, 568, 13, 8465, 370, 321, 600, 1217, 27650, 292, 264, 51613, 51613, 955, 2356, 2316, 293, 1564, 286, 483, 257, 2020, 300, 341, 307, 767, 733, 295, 2891, 281, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.12218670845031739, "compression_ratio": 1.758490566037736, "no_speech_prob": 9.81806169875199e-06}, {"id": 597, "seek": 312236, "start": 3125.84, "end": 3130.2400000000002, "text": " lower it and sort of where the late stages of training potentially and we", "tokens": [50364, 437, 341, 1355, 307, 321, 434, 516, 281, 747, 527, 2539, 3314, 293, 321, 434, 516, 281, 1266, 87, 50538, 50538, 3126, 309, 293, 1333, 295, 689, 264, 3469, 10232, 295, 3097, 7263, 293, 321, 50758, 50758, 815, 528, 281, 352, 257, 857, 14009, 718, 311, 360, 472, 544, 767, 257, 1702, 472, 445, 281, 536, 498, 51060, 51060, 321, 434, 1455, 257, 7059, 510, 1392, 321, 434, 920, 1455, 7059, 293, 538, 264, 636, 264, 51308, 51308, 955, 2356, 4470, 300, 321, 11042, 1036, 960, 390, 568, 13, 8465, 370, 321, 600, 1217, 27650, 292, 264, 51613, 51613, 955, 2356, 2316, 293, 1564, 286, 483, 257, 2020, 300, 341, 307, 767, 733, 295, 2891, 281, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.12218670845031739, "compression_ratio": 1.758490566037736, "no_speech_prob": 9.81806169875199e-06}, {"id": 598, "seek": 312236, "start": 3130.2400000000002, "end": 3136.28, "text": " may want to go a bit slower let's do one more actually a quick one just to see if", "tokens": [50364, 437, 341, 1355, 307, 321, 434, 516, 281, 747, 527, 2539, 3314, 293, 321, 434, 516, 281, 1266, 87, 50538, 50538, 3126, 309, 293, 1333, 295, 689, 264, 3469, 10232, 295, 3097, 7263, 293, 321, 50758, 50758, 815, 528, 281, 352, 257, 857, 14009, 718, 311, 360, 472, 544, 767, 257, 1702, 472, 445, 281, 536, 498, 51060, 51060, 321, 434, 1455, 257, 7059, 510, 1392, 321, 434, 920, 1455, 7059, 293, 538, 264, 636, 264, 51308, 51308, 955, 2356, 4470, 300, 321, 11042, 1036, 960, 390, 568, 13, 8465, 370, 321, 600, 1217, 27650, 292, 264, 51613, 51613, 955, 2356, 2316, 293, 1564, 286, 483, 257, 2020, 300, 341, 307, 767, 733, 295, 2891, 281, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.12218670845031739, "compression_ratio": 1.758490566037736, "no_speech_prob": 9.81806169875199e-06}, {"id": 599, "seek": 312236, "start": 3136.28, "end": 3141.2400000000002, "text": " we're making a dent here okay we're still making dent and by the way the", "tokens": [50364, 437, 341, 1355, 307, 321, 434, 516, 281, 747, 527, 2539, 3314, 293, 321, 434, 516, 281, 1266, 87, 50538, 50538, 3126, 309, 293, 1333, 295, 689, 264, 3469, 10232, 295, 3097, 7263, 293, 321, 50758, 50758, 815, 528, 281, 352, 257, 857, 14009, 718, 311, 360, 472, 544, 767, 257, 1702, 472, 445, 281, 536, 498, 51060, 51060, 321, 434, 1455, 257, 7059, 510, 1392, 321, 434, 920, 1455, 7059, 293, 538, 264, 636, 264, 51308, 51308, 955, 2356, 4470, 300, 321, 11042, 1036, 960, 390, 568, 13, 8465, 370, 321, 600, 1217, 27650, 292, 264, 51613, 51613, 955, 2356, 2316, 293, 1564, 286, 483, 257, 2020, 300, 341, 307, 767, 733, 295, 2891, 281, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.12218670845031739, "compression_ratio": 1.758490566037736, "no_speech_prob": 9.81806169875199e-06}, {"id": 600, "seek": 312236, "start": 3141.2400000000002, "end": 3147.34, "text": " bigram loss that we achieved last video was 2.45 so we've already surpassed the", "tokens": [50364, 437, 341, 1355, 307, 321, 434, 516, 281, 747, 527, 2539, 3314, 293, 321, 434, 516, 281, 1266, 87, 50538, 50538, 3126, 309, 293, 1333, 295, 689, 264, 3469, 10232, 295, 3097, 7263, 293, 321, 50758, 50758, 815, 528, 281, 352, 257, 857, 14009, 718, 311, 360, 472, 544, 767, 257, 1702, 472, 445, 281, 536, 498, 51060, 51060, 321, 434, 1455, 257, 7059, 510, 1392, 321, 434, 920, 1455, 7059, 293, 538, 264, 636, 264, 51308, 51308, 955, 2356, 4470, 300, 321, 11042, 1036, 960, 390, 568, 13, 8465, 370, 321, 600, 1217, 27650, 292, 264, 51613, 51613, 955, 2356, 2316, 293, 1564, 286, 483, 257, 2020, 300, 341, 307, 767, 733, 295, 2891, 281, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.12218670845031739, "compression_ratio": 1.758490566037736, "no_speech_prob": 9.81806169875199e-06}, {"id": 601, "seek": 312236, "start": 3147.34, "end": 3151.44, "text": " bigram model and once I get a sense that this is actually kind of starting to", "tokens": [50364, 437, 341, 1355, 307, 321, 434, 516, 281, 747, 527, 2539, 3314, 293, 321, 434, 516, 281, 1266, 87, 50538, 50538, 3126, 309, 293, 1333, 295, 689, 264, 3469, 10232, 295, 3097, 7263, 293, 321, 50758, 50758, 815, 528, 281, 352, 257, 857, 14009, 718, 311, 360, 472, 544, 767, 257, 1702, 472, 445, 281, 536, 498, 51060, 51060, 321, 434, 1455, 257, 7059, 510, 1392, 321, 434, 920, 1455, 7059, 293, 538, 264, 636, 264, 51308, 51308, 955, 2356, 4470, 300, 321, 11042, 1036, 960, 390, 568, 13, 8465, 370, 321, 600, 1217, 27650, 292, 264, 51613, 51613, 955, 2356, 2316, 293, 1564, 286, 483, 257, 2020, 300, 341, 307, 767, 733, 295, 2891, 281, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.12218670845031739, "compression_ratio": 1.758490566037736, "no_speech_prob": 9.81806169875199e-06}, {"id": 602, "seek": 315144, "start": 3151.44, "end": 3155.84, "text": " plateau off people like to do as I mentioned this learning rate decay so", "tokens": [50364, 39885, 766, 561, 411, 281, 360, 382, 286, 2835, 341, 2539, 3314, 21039, 370, 50584, 50584, 718, 311, 853, 281, 21039, 264, 4470, 264, 2539, 3314, 286, 914, 293, 321, 4584, 309, 466, 568, 13, 18, 51028, 51028, 586, 2745, 341, 307, 361, 657, 88, 293, 406, 2293, 577, 291, 576, 3847, 309, 294, 51256, 51256, 4265, 457, 341, 307, 9810, 437, 291, 434, 516, 807, 291, 700, 915, 257, 51438, 51438, 8681, 2539, 3314, 1228, 264, 3109, 300, 286, 4712, 291, 550, 291, 722, 365, 51628, 51628, 300, 2539, 3314, 293, 291, 3847, 337, 257, 1339, 293, 550, 412, 264, 917, 561, 411, 281, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.08557729983548505, "compression_ratio": 1.82, "no_speech_prob": 6.048759587429231e-06}, {"id": 603, "seek": 315144, "start": 3155.84, "end": 3164.7200000000003, "text": " let's try to decay the loss the learning rate I mean and we achieve it about 2.3", "tokens": [50364, 39885, 766, 561, 411, 281, 360, 382, 286, 2835, 341, 2539, 3314, 21039, 370, 50584, 50584, 718, 311, 853, 281, 21039, 264, 4470, 264, 2539, 3314, 286, 914, 293, 321, 4584, 309, 466, 568, 13, 18, 51028, 51028, 586, 2745, 341, 307, 361, 657, 88, 293, 406, 2293, 577, 291, 576, 3847, 309, 294, 51256, 51256, 4265, 457, 341, 307, 9810, 437, 291, 434, 516, 807, 291, 700, 915, 257, 51438, 51438, 8681, 2539, 3314, 1228, 264, 3109, 300, 286, 4712, 291, 550, 291, 722, 365, 51628, 51628, 300, 2539, 3314, 293, 291, 3847, 337, 257, 1339, 293, 550, 412, 264, 917, 561, 411, 281, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.08557729983548505, "compression_ratio": 1.82, "no_speech_prob": 6.048759587429231e-06}, {"id": 604, "seek": 315144, "start": 3164.7200000000003, "end": 3169.28, "text": " now obviously this is janky and not exactly how you would train it in", "tokens": [50364, 39885, 766, 561, 411, 281, 360, 382, 286, 2835, 341, 2539, 3314, 21039, 370, 50584, 50584, 718, 311, 853, 281, 21039, 264, 4470, 264, 2539, 3314, 286, 914, 293, 321, 4584, 309, 466, 568, 13, 18, 51028, 51028, 586, 2745, 341, 307, 361, 657, 88, 293, 406, 2293, 577, 291, 576, 3847, 309, 294, 51256, 51256, 4265, 457, 341, 307, 9810, 437, 291, 434, 516, 807, 291, 700, 915, 257, 51438, 51438, 8681, 2539, 3314, 1228, 264, 3109, 300, 286, 4712, 291, 550, 291, 722, 365, 51628, 51628, 300, 2539, 3314, 293, 291, 3847, 337, 257, 1339, 293, 550, 412, 264, 917, 561, 411, 281, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.08557729983548505, "compression_ratio": 1.82, "no_speech_prob": 6.048759587429231e-06}, {"id": 605, "seek": 315144, "start": 3169.28, "end": 3172.92, "text": " production but this is roughly what you're going through you first find a", "tokens": [50364, 39885, 766, 561, 411, 281, 360, 382, 286, 2835, 341, 2539, 3314, 21039, 370, 50584, 50584, 718, 311, 853, 281, 21039, 264, 4470, 264, 2539, 3314, 286, 914, 293, 321, 4584, 309, 466, 568, 13, 18, 51028, 51028, 586, 2745, 341, 307, 361, 657, 88, 293, 406, 2293, 577, 291, 576, 3847, 309, 294, 51256, 51256, 4265, 457, 341, 307, 9810, 437, 291, 434, 516, 807, 291, 700, 915, 257, 51438, 51438, 8681, 2539, 3314, 1228, 264, 3109, 300, 286, 4712, 291, 550, 291, 722, 365, 51628, 51628, 300, 2539, 3314, 293, 291, 3847, 337, 257, 1339, 293, 550, 412, 264, 917, 561, 411, 281, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.08557729983548505, "compression_ratio": 1.82, "no_speech_prob": 6.048759587429231e-06}, {"id": 606, "seek": 315144, "start": 3172.92, "end": 3176.7200000000003, "text": " decent learning rate using the approach that I showed you then you start with", "tokens": [50364, 39885, 766, 561, 411, 281, 360, 382, 286, 2835, 341, 2539, 3314, 21039, 370, 50584, 50584, 718, 311, 853, 281, 21039, 264, 4470, 264, 2539, 3314, 286, 914, 293, 321, 4584, 309, 466, 568, 13, 18, 51028, 51028, 586, 2745, 341, 307, 361, 657, 88, 293, 406, 2293, 577, 291, 576, 3847, 309, 294, 51256, 51256, 4265, 457, 341, 307, 9810, 437, 291, 434, 516, 807, 291, 700, 915, 257, 51438, 51438, 8681, 2539, 3314, 1228, 264, 3109, 300, 286, 4712, 291, 550, 291, 722, 365, 51628, 51628, 300, 2539, 3314, 293, 291, 3847, 337, 257, 1339, 293, 550, 412, 264, 917, 561, 411, 281, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.08557729983548505, "compression_ratio": 1.82, "no_speech_prob": 6.048759587429231e-06}, {"id": 607, "seek": 315144, "start": 3176.7200000000003, "end": 3180.28, "text": " that learning rate and you train for a while and then at the end people like to", "tokens": [50364, 39885, 766, 561, 411, 281, 360, 382, 286, 2835, 341, 2539, 3314, 21039, 370, 50584, 50584, 718, 311, 853, 281, 21039, 264, 4470, 264, 2539, 3314, 286, 914, 293, 321, 4584, 309, 466, 568, 13, 18, 51028, 51028, 586, 2745, 341, 307, 361, 657, 88, 293, 406, 2293, 577, 291, 576, 3847, 309, 294, 51256, 51256, 4265, 457, 341, 307, 9810, 437, 291, 434, 516, 807, 291, 700, 915, 257, 51438, 51438, 8681, 2539, 3314, 1228, 264, 3109, 300, 286, 4712, 291, 550, 291, 722, 365, 51628, 51628, 300, 2539, 3314, 293, 291, 3847, 337, 257, 1339, 293, 550, 412, 264, 917, 561, 411, 281, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.08557729983548505, "compression_ratio": 1.82, "no_speech_prob": 6.048759587429231e-06}, {"id": 608, "seek": 318028, "start": 3180.28, "end": 3183.76, "text": " do a learning rate decay where you decay the learning rate by say a factor of 10", "tokens": [50364, 360, 257, 2539, 3314, 21039, 689, 291, 21039, 264, 2539, 3314, 538, 584, 257, 5952, 295, 1266, 50538, 50538, 293, 291, 360, 257, 1326, 544, 4439, 293, 550, 291, 483, 257, 8895, 3209, 9810, 4124, 50762, 50762, 370, 321, 600, 11042, 568, 13, 18, 293, 17548, 9689, 322, 264, 955, 2356, 2856, 2316, 51026, 51026, 1228, 341, 2199, 18161, 2533, 382, 7619, 510, 1228, 613, 12790, 628, 51312, 51312, 9834, 586, 456, 311, 746, 321, 362, 281, 312, 5026, 365, 286, 848, 300, 321, 362, 257, 51526, 51526, 1101, 2316, 570, 321, 366, 19626, 257, 3126, 4470, 568, 13, 18, 709, 3126, 813, 568, 13, 8465, 365, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.08739616634609464, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.5006069083465263e-06}, {"id": 609, "seek": 318028, "start": 3183.76, "end": 3188.2400000000002, "text": " and you do a few more steps and then you get a trained network roughly speaking", "tokens": [50364, 360, 257, 2539, 3314, 21039, 689, 291, 21039, 264, 2539, 3314, 538, 584, 257, 5952, 295, 1266, 50538, 50538, 293, 291, 360, 257, 1326, 544, 4439, 293, 550, 291, 483, 257, 8895, 3209, 9810, 4124, 50762, 50762, 370, 321, 600, 11042, 568, 13, 18, 293, 17548, 9689, 322, 264, 955, 2356, 2856, 2316, 51026, 51026, 1228, 341, 2199, 18161, 2533, 382, 7619, 510, 1228, 613, 12790, 628, 51312, 51312, 9834, 586, 456, 311, 746, 321, 362, 281, 312, 5026, 365, 286, 848, 300, 321, 362, 257, 51526, 51526, 1101, 2316, 570, 321, 366, 19626, 257, 3126, 4470, 568, 13, 18, 709, 3126, 813, 568, 13, 8465, 365, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.08739616634609464, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.5006069083465263e-06}, {"id": 610, "seek": 318028, "start": 3188.2400000000002, "end": 3193.52, "text": " so we've achieved 2.3 and dramatically improved on the bigram language model", "tokens": [50364, 360, 257, 2539, 3314, 21039, 689, 291, 21039, 264, 2539, 3314, 538, 584, 257, 5952, 295, 1266, 50538, 50538, 293, 291, 360, 257, 1326, 544, 4439, 293, 550, 291, 483, 257, 8895, 3209, 9810, 4124, 50762, 50762, 370, 321, 600, 11042, 568, 13, 18, 293, 17548, 9689, 322, 264, 955, 2356, 2856, 2316, 51026, 51026, 1228, 341, 2199, 18161, 2533, 382, 7619, 510, 1228, 613, 12790, 628, 51312, 51312, 9834, 586, 456, 311, 746, 321, 362, 281, 312, 5026, 365, 286, 848, 300, 321, 362, 257, 51526, 51526, 1101, 2316, 570, 321, 366, 19626, 257, 3126, 4470, 568, 13, 18, 709, 3126, 813, 568, 13, 8465, 365, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.08739616634609464, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.5006069083465263e-06}, {"id": 611, "seek": 318028, "start": 3193.52, "end": 3199.2400000000002, "text": " using this simple neural net as described here using these 3400", "tokens": [50364, 360, 257, 2539, 3314, 21039, 689, 291, 21039, 264, 2539, 3314, 538, 584, 257, 5952, 295, 1266, 50538, 50538, 293, 291, 360, 257, 1326, 544, 4439, 293, 550, 291, 483, 257, 8895, 3209, 9810, 4124, 50762, 50762, 370, 321, 600, 11042, 568, 13, 18, 293, 17548, 9689, 322, 264, 955, 2356, 2856, 2316, 51026, 51026, 1228, 341, 2199, 18161, 2533, 382, 7619, 510, 1228, 613, 12790, 628, 51312, 51312, 9834, 586, 456, 311, 746, 321, 362, 281, 312, 5026, 365, 286, 848, 300, 321, 362, 257, 51526, 51526, 1101, 2316, 570, 321, 366, 19626, 257, 3126, 4470, 568, 13, 18, 709, 3126, 813, 568, 13, 8465, 365, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.08739616634609464, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.5006069083465263e-06}, {"id": 612, "seek": 318028, "start": 3199.2400000000002, "end": 3203.52, "text": " parameters now there's something we have to be careful with I said that we have a", "tokens": [50364, 360, 257, 2539, 3314, 21039, 689, 291, 21039, 264, 2539, 3314, 538, 584, 257, 5952, 295, 1266, 50538, 50538, 293, 291, 360, 257, 1326, 544, 4439, 293, 550, 291, 483, 257, 8895, 3209, 9810, 4124, 50762, 50762, 370, 321, 600, 11042, 568, 13, 18, 293, 17548, 9689, 322, 264, 955, 2356, 2856, 2316, 51026, 51026, 1228, 341, 2199, 18161, 2533, 382, 7619, 510, 1228, 613, 12790, 628, 51312, 51312, 9834, 586, 456, 311, 746, 321, 362, 281, 312, 5026, 365, 286, 848, 300, 321, 362, 257, 51526, 51526, 1101, 2316, 570, 321, 366, 19626, 257, 3126, 4470, 568, 13, 18, 709, 3126, 813, 568, 13, 8465, 365, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.08739616634609464, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.5006069083465263e-06}, {"id": 613, "seek": 318028, "start": 3203.52, "end": 3209.0400000000004, "text": " better model because we are achieving a lower loss 2.3 much lower than 2.45 with", "tokens": [50364, 360, 257, 2539, 3314, 21039, 689, 291, 21039, 264, 2539, 3314, 538, 584, 257, 5952, 295, 1266, 50538, 50538, 293, 291, 360, 257, 1326, 544, 4439, 293, 550, 291, 483, 257, 8895, 3209, 9810, 4124, 50762, 50762, 370, 321, 600, 11042, 568, 13, 18, 293, 17548, 9689, 322, 264, 955, 2356, 2856, 2316, 51026, 51026, 1228, 341, 2199, 18161, 2533, 382, 7619, 510, 1228, 613, 12790, 628, 51312, 51312, 9834, 586, 456, 311, 746, 321, 362, 281, 312, 5026, 365, 286, 848, 300, 321, 362, 257, 51526, 51526, 1101, 2316, 570, 321, 366, 19626, 257, 3126, 4470, 568, 13, 18, 709, 3126, 813, 568, 13, 8465, 365, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.08739616634609464, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.5006069083465263e-06}, {"id": 614, "seek": 320904, "start": 3209.04, "end": 3213.2, "text": " the bigram model previously now that's not exactly true and the reason that's", "tokens": [50364, 264, 955, 2356, 2316, 8046, 586, 300, 311, 406, 2293, 2074, 293, 264, 1778, 300, 311, 50572, 50572, 406, 2074, 307, 300, 341, 307, 767, 6457, 1359, 2316, 457, 613, 5245, 393, 483, 50928, 50928, 4833, 293, 4833, 498, 291, 1066, 5127, 22027, 293, 9834, 370, 291, 393, 3811, 51122, 51122, 300, 321, 500, 380, 7263, 362, 257, 4714, 9834, 321, 727, 362, 2064, 51264, 51264, 4714, 420, 3262, 4714, 420, 6803, 295, 9834, 293, 382, 264, 6042, 295, 264, 51474, 51474, 18161, 3209, 13156, 309, 3643, 544, 293, 544, 8189, 295, 670, 69, 2414, 428, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.08876619338989258, "compression_ratio": 1.8816326530612244, "no_speech_prob": 9.223061169905122e-06}, {"id": 615, "seek": 320904, "start": 3213.2, "end": 3220.32, "text": " not true is that this is actually fairly small model but these models can get", "tokens": [50364, 264, 955, 2356, 2316, 8046, 586, 300, 311, 406, 2293, 2074, 293, 264, 1778, 300, 311, 50572, 50572, 406, 2074, 307, 300, 341, 307, 767, 6457, 1359, 2316, 457, 613, 5245, 393, 483, 50928, 50928, 4833, 293, 4833, 498, 291, 1066, 5127, 22027, 293, 9834, 370, 291, 393, 3811, 51122, 51122, 300, 321, 500, 380, 7263, 362, 257, 4714, 9834, 321, 727, 362, 2064, 51264, 51264, 4714, 420, 3262, 4714, 420, 6803, 295, 9834, 293, 382, 264, 6042, 295, 264, 51474, 51474, 18161, 3209, 13156, 309, 3643, 544, 293, 544, 8189, 295, 670, 69, 2414, 428, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.08876619338989258, "compression_ratio": 1.8816326530612244, "no_speech_prob": 9.223061169905122e-06}, {"id": 616, "seek": 320904, "start": 3220.32, "end": 3224.2, "text": " larger and larger if you keep adding neurons and parameters so you can imagine", "tokens": [50364, 264, 955, 2356, 2316, 8046, 586, 300, 311, 406, 2293, 2074, 293, 264, 1778, 300, 311, 50572, 50572, 406, 2074, 307, 300, 341, 307, 767, 6457, 1359, 2316, 457, 613, 5245, 393, 483, 50928, 50928, 4833, 293, 4833, 498, 291, 1066, 5127, 22027, 293, 9834, 370, 291, 393, 3811, 51122, 51122, 300, 321, 500, 380, 7263, 362, 257, 4714, 9834, 321, 727, 362, 2064, 51264, 51264, 4714, 420, 3262, 4714, 420, 6803, 295, 9834, 293, 382, 264, 6042, 295, 264, 51474, 51474, 18161, 3209, 13156, 309, 3643, 544, 293, 544, 8189, 295, 670, 69, 2414, 428, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.08876619338989258, "compression_ratio": 1.8816326530612244, "no_speech_prob": 9.223061169905122e-06}, {"id": 617, "seek": 320904, "start": 3224.2, "end": 3227.04, "text": " that we don't potentially have a thousand parameters we could have ten", "tokens": [50364, 264, 955, 2356, 2316, 8046, 586, 300, 311, 406, 2293, 2074, 293, 264, 1778, 300, 311, 50572, 50572, 406, 2074, 307, 300, 341, 307, 767, 6457, 1359, 2316, 457, 613, 5245, 393, 483, 50928, 50928, 4833, 293, 4833, 498, 291, 1066, 5127, 22027, 293, 9834, 370, 291, 393, 3811, 51122, 51122, 300, 321, 500, 380, 7263, 362, 257, 4714, 9834, 321, 727, 362, 2064, 51264, 51264, 4714, 420, 3262, 4714, 420, 6803, 295, 9834, 293, 382, 264, 6042, 295, 264, 51474, 51474, 18161, 3209, 13156, 309, 3643, 544, 293, 544, 8189, 295, 670, 69, 2414, 428, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.08876619338989258, "compression_ratio": 1.8816326530612244, "no_speech_prob": 9.223061169905122e-06}, {"id": 618, "seek": 320904, "start": 3227.04, "end": 3231.24, "text": " thousand or hundred thousand or millions of parameters and as the capacity of the", "tokens": [50364, 264, 955, 2356, 2316, 8046, 586, 300, 311, 406, 2293, 2074, 293, 264, 1778, 300, 311, 50572, 50572, 406, 2074, 307, 300, 341, 307, 767, 6457, 1359, 2316, 457, 613, 5245, 393, 483, 50928, 50928, 4833, 293, 4833, 498, 291, 1066, 5127, 22027, 293, 9834, 370, 291, 393, 3811, 51122, 51122, 300, 321, 500, 380, 7263, 362, 257, 4714, 9834, 321, 727, 362, 2064, 51264, 51264, 4714, 420, 3262, 4714, 420, 6803, 295, 9834, 293, 382, 264, 6042, 295, 264, 51474, 51474, 18161, 3209, 13156, 309, 3643, 544, 293, 544, 8189, 295, 670, 69, 2414, 428, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.08876619338989258, "compression_ratio": 1.8816326530612244, "no_speech_prob": 9.223061169905122e-06}, {"id": 619, "seek": 320904, "start": 3231.24, "end": 3235.84, "text": " neural network grows it becomes more and more capable of overfitting your", "tokens": [50364, 264, 955, 2356, 2316, 8046, 586, 300, 311, 406, 2293, 2074, 293, 264, 1778, 300, 311, 50572, 50572, 406, 2074, 307, 300, 341, 307, 767, 6457, 1359, 2316, 457, 613, 5245, 393, 483, 50928, 50928, 4833, 293, 4833, 498, 291, 1066, 5127, 22027, 293, 9834, 370, 291, 393, 3811, 51122, 51122, 300, 321, 500, 380, 7263, 362, 257, 4714, 9834, 321, 727, 362, 2064, 51264, 51264, 4714, 420, 3262, 4714, 420, 6803, 295, 9834, 293, 382, 264, 6042, 295, 264, 51474, 51474, 18161, 3209, 13156, 309, 3643, 544, 293, 544, 8189, 295, 670, 69, 2414, 428, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.08876619338989258, "compression_ratio": 1.8816326530612244, "no_speech_prob": 9.223061169905122e-06}, {"id": 620, "seek": 323584, "start": 3235.84, "end": 3240.6400000000003, "text": " training set what that means is that the loss on the training set on the data", "tokens": [50364, 3097, 992, 437, 300, 1355, 307, 300, 264, 4470, 322, 264, 3097, 992, 322, 264, 1412, 50604, 50604, 300, 291, 434, 3097, 322, 486, 1813, 588, 588, 2295, 382, 2295, 382, 4018, 457, 439, 300, 264, 50854, 50854, 2316, 307, 884, 307, 10560, 3319, 428, 3097, 992, 9595, 267, 332, 370, 498, 291, 747, 51052, 51052, 300, 2316, 293, 309, 1542, 411, 309, 311, 1364, 534, 731, 457, 291, 853, 281, 6889, 51216, 51216, 490, 309, 291, 486, 1936, 787, 483, 5110, 2293, 382, 436, 366, 294, 264, 51400, 51400, 3097, 992, 291, 1582, 380, 483, 604, 777, 1412, 294, 4500, 281, 300, 498, 291, 853, 281, 51598, 51598], "temperature": 0.0, "avg_logprob": -0.06491486661069028, "compression_ratio": 1.9737991266375545, "no_speech_prob": 1.8056358385365456e-05}, {"id": 621, "seek": 323584, "start": 3240.6400000000003, "end": 3245.6400000000003, "text": " that you're training on will become very very low as low as zero but all that the", "tokens": [50364, 3097, 992, 437, 300, 1355, 307, 300, 264, 4470, 322, 264, 3097, 992, 322, 264, 1412, 50604, 50604, 300, 291, 434, 3097, 322, 486, 1813, 588, 588, 2295, 382, 2295, 382, 4018, 457, 439, 300, 264, 50854, 50854, 2316, 307, 884, 307, 10560, 3319, 428, 3097, 992, 9595, 267, 332, 370, 498, 291, 747, 51052, 51052, 300, 2316, 293, 309, 1542, 411, 309, 311, 1364, 534, 731, 457, 291, 853, 281, 6889, 51216, 51216, 490, 309, 291, 486, 1936, 787, 483, 5110, 2293, 382, 436, 366, 294, 264, 51400, 51400, 3097, 992, 291, 1582, 380, 483, 604, 777, 1412, 294, 4500, 281, 300, 498, 291, 853, 281, 51598, 51598], "temperature": 0.0, "avg_logprob": -0.06491486661069028, "compression_ratio": 1.9737991266375545, "no_speech_prob": 1.8056358385365456e-05}, {"id": 622, "seek": 323584, "start": 3245.6400000000003, "end": 3249.6000000000004, "text": " model is doing is memorizing your training set verbatim so if you take", "tokens": [50364, 3097, 992, 437, 300, 1355, 307, 300, 264, 4470, 322, 264, 3097, 992, 322, 264, 1412, 50604, 50604, 300, 291, 434, 3097, 322, 486, 1813, 588, 588, 2295, 382, 2295, 382, 4018, 457, 439, 300, 264, 50854, 50854, 2316, 307, 884, 307, 10560, 3319, 428, 3097, 992, 9595, 267, 332, 370, 498, 291, 747, 51052, 51052, 300, 2316, 293, 309, 1542, 411, 309, 311, 1364, 534, 731, 457, 291, 853, 281, 6889, 51216, 51216, 490, 309, 291, 486, 1936, 787, 483, 5110, 2293, 382, 436, 366, 294, 264, 51400, 51400, 3097, 992, 291, 1582, 380, 483, 604, 777, 1412, 294, 4500, 281, 300, 498, 291, 853, 281, 51598, 51598], "temperature": 0.0, "avg_logprob": -0.06491486661069028, "compression_ratio": 1.9737991266375545, "no_speech_prob": 1.8056358385365456e-05}, {"id": 623, "seek": 323584, "start": 3249.6000000000004, "end": 3252.88, "text": " that model and it looks like it's working really well but you try to sample", "tokens": [50364, 3097, 992, 437, 300, 1355, 307, 300, 264, 4470, 322, 264, 3097, 992, 322, 264, 1412, 50604, 50604, 300, 291, 434, 3097, 322, 486, 1813, 588, 588, 2295, 382, 2295, 382, 4018, 457, 439, 300, 264, 50854, 50854, 2316, 307, 884, 307, 10560, 3319, 428, 3097, 992, 9595, 267, 332, 370, 498, 291, 747, 51052, 51052, 300, 2316, 293, 309, 1542, 411, 309, 311, 1364, 534, 731, 457, 291, 853, 281, 6889, 51216, 51216, 490, 309, 291, 486, 1936, 787, 483, 5110, 2293, 382, 436, 366, 294, 264, 51400, 51400, 3097, 992, 291, 1582, 380, 483, 604, 777, 1412, 294, 4500, 281, 300, 498, 291, 853, 281, 51598, 51598], "temperature": 0.0, "avg_logprob": -0.06491486661069028, "compression_ratio": 1.9737991266375545, "no_speech_prob": 1.8056358385365456e-05}, {"id": 624, "seek": 323584, "start": 3252.88, "end": 3256.56, "text": " from it you will basically only get examples exactly as they are in the", "tokens": [50364, 3097, 992, 437, 300, 1355, 307, 300, 264, 4470, 322, 264, 3097, 992, 322, 264, 1412, 50604, 50604, 300, 291, 434, 3097, 322, 486, 1813, 588, 588, 2295, 382, 2295, 382, 4018, 457, 439, 300, 264, 50854, 50854, 2316, 307, 884, 307, 10560, 3319, 428, 3097, 992, 9595, 267, 332, 370, 498, 291, 747, 51052, 51052, 300, 2316, 293, 309, 1542, 411, 309, 311, 1364, 534, 731, 457, 291, 853, 281, 6889, 51216, 51216, 490, 309, 291, 486, 1936, 787, 483, 5110, 2293, 382, 436, 366, 294, 264, 51400, 51400, 3097, 992, 291, 1582, 380, 483, 604, 777, 1412, 294, 4500, 281, 300, 498, 291, 853, 281, 51598, 51598], "temperature": 0.0, "avg_logprob": -0.06491486661069028, "compression_ratio": 1.9737991266375545, "no_speech_prob": 1.8056358385365456e-05}, {"id": 625, "seek": 323584, "start": 3256.56, "end": 3260.52, "text": " training set you won't get any new data in addition to that if you try to", "tokens": [50364, 3097, 992, 437, 300, 1355, 307, 300, 264, 4470, 322, 264, 3097, 992, 322, 264, 1412, 50604, 50604, 300, 291, 434, 3097, 322, 486, 1813, 588, 588, 2295, 382, 2295, 382, 4018, 457, 439, 300, 264, 50854, 50854, 2316, 307, 884, 307, 10560, 3319, 428, 3097, 992, 9595, 267, 332, 370, 498, 291, 747, 51052, 51052, 300, 2316, 293, 309, 1542, 411, 309, 311, 1364, 534, 731, 457, 291, 853, 281, 6889, 51216, 51216, 490, 309, 291, 486, 1936, 787, 483, 5110, 2293, 382, 436, 366, 294, 264, 51400, 51400, 3097, 992, 291, 1582, 380, 483, 604, 777, 1412, 294, 4500, 281, 300, 498, 291, 853, 281, 51598, 51598], "temperature": 0.0, "avg_logprob": -0.06491486661069028, "compression_ratio": 1.9737991266375545, "no_speech_prob": 1.8056358385365456e-05}, {"id": 626, "seek": 326052, "start": 3260.52, "end": 3265.8, "text": " evaluate the loss on some withheld names or other words you will actually see", "tokens": [50364, 13059, 264, 4470, 322, 512, 365, 23504, 5288, 420, 661, 2283, 291, 486, 767, 536, 50628, 50628, 300, 264, 4470, 322, 729, 309, 393, 312, 588, 1090, 293, 370, 1936, 309, 311, 406, 257, 665, 50838, 50838, 2316, 370, 264, 3832, 294, 264, 2519, 309, 307, 281, 7472, 493, 428, 1412, 992, 666, 1045, 51060, 51060, 37741, 382, 321, 818, 552, 321, 362, 264, 3097, 7472, 264, 1905, 7472, 420, 264, 51282, 51282, 24071, 7472, 293, 264, 1500, 7472, 370, 3097, 7472, 1500, 420, 2597, 1905, 420, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.1001898901803153, "compression_ratio": 1.821256038647343, "no_speech_prob": 1.1125286619062535e-05}, {"id": 627, "seek": 326052, "start": 3265.8, "end": 3270.0, "text": " that the loss on those it can be very high and so basically it's not a good", "tokens": [50364, 13059, 264, 4470, 322, 512, 365, 23504, 5288, 420, 661, 2283, 291, 486, 767, 536, 50628, 50628, 300, 264, 4470, 322, 729, 309, 393, 312, 588, 1090, 293, 370, 1936, 309, 311, 406, 257, 665, 50838, 50838, 2316, 370, 264, 3832, 294, 264, 2519, 309, 307, 281, 7472, 493, 428, 1412, 992, 666, 1045, 51060, 51060, 37741, 382, 321, 818, 552, 321, 362, 264, 3097, 7472, 264, 1905, 7472, 420, 264, 51282, 51282, 24071, 7472, 293, 264, 1500, 7472, 370, 3097, 7472, 1500, 420, 2597, 1905, 420, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.1001898901803153, "compression_ratio": 1.821256038647343, "no_speech_prob": 1.1125286619062535e-05}, {"id": 628, "seek": 326052, "start": 3270.0, "end": 3274.44, "text": " model so the standard in the field it is to split up your data set into three", "tokens": [50364, 13059, 264, 4470, 322, 512, 365, 23504, 5288, 420, 661, 2283, 291, 486, 767, 536, 50628, 50628, 300, 264, 4470, 322, 729, 309, 393, 312, 588, 1090, 293, 370, 1936, 309, 311, 406, 257, 665, 50838, 50838, 2316, 370, 264, 3832, 294, 264, 2519, 309, 307, 281, 7472, 493, 428, 1412, 992, 666, 1045, 51060, 51060, 37741, 382, 321, 818, 552, 321, 362, 264, 3097, 7472, 264, 1905, 7472, 420, 264, 51282, 51282, 24071, 7472, 293, 264, 1500, 7472, 370, 3097, 7472, 1500, 420, 2597, 1905, 420, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.1001898901803153, "compression_ratio": 1.821256038647343, "no_speech_prob": 1.1125286619062535e-05}, {"id": 629, "seek": 326052, "start": 3274.44, "end": 3278.88, "text": " splits as we call them we have the training split the dev split or the", "tokens": [50364, 13059, 264, 4470, 322, 512, 365, 23504, 5288, 420, 661, 2283, 291, 486, 767, 536, 50628, 50628, 300, 264, 4470, 322, 729, 309, 393, 312, 588, 1090, 293, 370, 1936, 309, 311, 406, 257, 665, 50838, 50838, 2316, 370, 264, 3832, 294, 264, 2519, 309, 307, 281, 7472, 493, 428, 1412, 992, 666, 1045, 51060, 51060, 37741, 382, 321, 818, 552, 321, 362, 264, 3097, 7472, 264, 1905, 7472, 420, 264, 51282, 51282, 24071, 7472, 293, 264, 1500, 7472, 370, 3097, 7472, 1500, 420, 2597, 1905, 420, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.1001898901803153, "compression_ratio": 1.821256038647343, "no_speech_prob": 1.1125286619062535e-05}, {"id": 630, "seek": 326052, "start": 3278.88, "end": 3288.16, "text": " validation split and the test split so training split test or sorry dev or", "tokens": [50364, 13059, 264, 4470, 322, 512, 365, 23504, 5288, 420, 661, 2283, 291, 486, 767, 536, 50628, 50628, 300, 264, 4470, 322, 729, 309, 393, 312, 588, 1090, 293, 370, 1936, 309, 311, 406, 257, 665, 50838, 50838, 2316, 370, 264, 3832, 294, 264, 2519, 309, 307, 281, 7472, 493, 428, 1412, 992, 666, 1045, 51060, 51060, 37741, 382, 321, 818, 552, 321, 362, 264, 3097, 7472, 264, 1905, 7472, 420, 264, 51282, 51282, 24071, 7472, 293, 264, 1500, 7472, 370, 3097, 7472, 1500, 420, 2597, 1905, 420, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.1001898901803153, "compression_ratio": 1.821256038647343, "no_speech_prob": 1.1125286619062535e-05}, {"id": 631, "seek": 328816, "start": 3288.16, "end": 3294.3999999999996, "text": " validation split and test split and typically this would be say 80% of your", "tokens": [50364, 24071, 7472, 293, 1500, 7472, 293, 5850, 341, 576, 312, 584, 4688, 4, 295, 428, 50676, 50676, 1412, 992, 341, 727, 312, 1266, 4, 293, 341, 1266, 4, 9810, 370, 291, 362, 613, 1045, 37741, 50934, 50934, 295, 264, 1412, 586, 613, 4688, 4, 295, 428, 33856, 295, 264, 1412, 992, 264, 3097, 51192, 51192, 992, 307, 1143, 281, 19719, 264, 9834, 295, 264, 2316, 445, 411, 321, 434, 884, 510, 51380, 51380, 1228, 16235, 23475, 613, 1266, 4, 295, 264, 5110, 264, 1905, 420, 24071, 7472, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.08041590386694604, "compression_ratio": 1.8502415458937198, "no_speech_prob": 5.682251412508776e-06}, {"id": 632, "seek": 328816, "start": 3294.3999999999996, "end": 3299.56, "text": " data set this could be 10% and this 10% roughly so you have these three splits", "tokens": [50364, 24071, 7472, 293, 1500, 7472, 293, 5850, 341, 576, 312, 584, 4688, 4, 295, 428, 50676, 50676, 1412, 992, 341, 727, 312, 1266, 4, 293, 341, 1266, 4, 9810, 370, 291, 362, 613, 1045, 37741, 50934, 50934, 295, 264, 1412, 586, 613, 4688, 4, 295, 428, 33856, 295, 264, 1412, 992, 264, 3097, 51192, 51192, 992, 307, 1143, 281, 19719, 264, 9834, 295, 264, 2316, 445, 411, 321, 434, 884, 510, 51380, 51380, 1228, 16235, 23475, 613, 1266, 4, 295, 264, 5110, 264, 1905, 420, 24071, 7472, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.08041590386694604, "compression_ratio": 1.8502415458937198, "no_speech_prob": 5.682251412508776e-06}, {"id": 633, "seek": 328816, "start": 3299.56, "end": 3304.72, "text": " of the data now these 80% of your trainings of the data set the training", "tokens": [50364, 24071, 7472, 293, 1500, 7472, 293, 5850, 341, 576, 312, 584, 4688, 4, 295, 428, 50676, 50676, 1412, 992, 341, 727, 312, 1266, 4, 293, 341, 1266, 4, 9810, 370, 291, 362, 613, 1045, 37741, 50934, 50934, 295, 264, 1412, 586, 613, 4688, 4, 295, 428, 33856, 295, 264, 1412, 992, 264, 3097, 51192, 51192, 992, 307, 1143, 281, 19719, 264, 9834, 295, 264, 2316, 445, 411, 321, 434, 884, 510, 51380, 51380, 1228, 16235, 23475, 613, 1266, 4, 295, 264, 5110, 264, 1905, 420, 24071, 7472, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.08041590386694604, "compression_ratio": 1.8502415458937198, "no_speech_prob": 5.682251412508776e-06}, {"id": 634, "seek": 328816, "start": 3304.72, "end": 3308.48, "text": " set is used to optimize the parameters of the model just like we're doing here", "tokens": [50364, 24071, 7472, 293, 1500, 7472, 293, 5850, 341, 576, 312, 584, 4688, 4, 295, 428, 50676, 50676, 1412, 992, 341, 727, 312, 1266, 4, 293, 341, 1266, 4, 9810, 370, 291, 362, 613, 1045, 37741, 50934, 50934, 295, 264, 1412, 586, 613, 4688, 4, 295, 428, 33856, 295, 264, 1412, 992, 264, 3097, 51192, 51192, 992, 307, 1143, 281, 19719, 264, 9834, 295, 264, 2316, 445, 411, 321, 434, 884, 510, 51380, 51380, 1228, 16235, 23475, 613, 1266, 4, 295, 264, 5110, 264, 1905, 420, 24071, 7472, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.08041590386694604, "compression_ratio": 1.8502415458937198, "no_speech_prob": 5.682251412508776e-06}, {"id": 635, "seek": 328816, "start": 3308.48, "end": 3314.6, "text": " using gradient descent these 10% of the examples the dev or validation split", "tokens": [50364, 24071, 7472, 293, 1500, 7472, 293, 5850, 341, 576, 312, 584, 4688, 4, 295, 428, 50676, 50676, 1412, 992, 341, 727, 312, 1266, 4, 293, 341, 1266, 4, 9810, 370, 291, 362, 613, 1045, 37741, 50934, 50934, 295, 264, 1412, 586, 613, 4688, 4, 295, 428, 33856, 295, 264, 1412, 992, 264, 3097, 51192, 51192, 992, 307, 1143, 281, 19719, 264, 9834, 295, 264, 2316, 445, 411, 321, 434, 884, 510, 51380, 51380, 1228, 16235, 23475, 613, 1266, 4, 295, 264, 5110, 264, 1905, 420, 24071, 7472, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.08041590386694604, "compression_ratio": 1.8502415458937198, "no_speech_prob": 5.682251412508776e-06}, {"id": 636, "seek": 331460, "start": 3314.6, "end": 3319.3199999999997, "text": " they're used for development over all the hyper parameters of your model so", "tokens": [50364, 436, 434, 1143, 337, 3250, 670, 439, 264, 9848, 9834, 295, 428, 2316, 370, 50600, 50600, 9848, 9834, 366, 337, 1365, 264, 2744, 295, 341, 7633, 4583, 264, 2744, 295, 50802, 50802, 264, 12240, 3584, 370, 341, 307, 257, 3262, 420, 257, 732, 337, 505, 457, 321, 727, 853, 819, 50980, 50980, 721, 264, 3800, 295, 264, 25138, 597, 321, 3212, 380, 1228, 1939, 370, 1400, 370, 51248, 51248, 456, 311, 3195, 295, 819, 9848, 9834, 293, 6257, 300, 352, 666, 51378, 51378, 17827, 257, 18161, 2533, 293, 291, 393, 853, 867, 819, 17840, 295, 552, 293, 51562, 51562], "temperature": 0.0, "avg_logprob": -0.10513746620404839, "compression_ratio": 1.8666666666666667, "no_speech_prob": 2.078285615425557e-05}, {"id": 637, "seek": 331460, "start": 3319.3199999999997, "end": 3323.36, "text": " hyper parameters are for example the size of this hidden layer the size of", "tokens": [50364, 436, 434, 1143, 337, 3250, 670, 439, 264, 9848, 9834, 295, 428, 2316, 370, 50600, 50600, 9848, 9834, 366, 337, 1365, 264, 2744, 295, 341, 7633, 4583, 264, 2744, 295, 50802, 50802, 264, 12240, 3584, 370, 341, 307, 257, 3262, 420, 257, 732, 337, 505, 457, 321, 727, 853, 819, 50980, 50980, 721, 264, 3800, 295, 264, 25138, 597, 321, 3212, 380, 1228, 1939, 370, 1400, 370, 51248, 51248, 456, 311, 3195, 295, 819, 9848, 9834, 293, 6257, 300, 352, 666, 51378, 51378, 17827, 257, 18161, 2533, 293, 291, 393, 853, 867, 819, 17840, 295, 552, 293, 51562, 51562], "temperature": 0.0, "avg_logprob": -0.10513746620404839, "compression_ratio": 1.8666666666666667, "no_speech_prob": 2.078285615425557e-05}, {"id": 638, "seek": 331460, "start": 3323.36, "end": 3326.92, "text": " the embedding so this is a hundred or a two for us but we could try different", "tokens": [50364, 436, 434, 1143, 337, 3250, 670, 439, 264, 9848, 9834, 295, 428, 2316, 370, 50600, 50600, 9848, 9834, 366, 337, 1365, 264, 2744, 295, 341, 7633, 4583, 264, 2744, 295, 50802, 50802, 264, 12240, 3584, 370, 341, 307, 257, 3262, 420, 257, 732, 337, 505, 457, 321, 727, 853, 819, 50980, 50980, 721, 264, 3800, 295, 264, 25138, 597, 321, 3212, 380, 1228, 1939, 370, 1400, 370, 51248, 51248, 456, 311, 3195, 295, 819, 9848, 9834, 293, 6257, 300, 352, 666, 51378, 51378, 17827, 257, 18161, 2533, 293, 291, 393, 853, 867, 819, 17840, 295, 552, 293, 51562, 51562], "temperature": 0.0, "avg_logprob": -0.10513746620404839, "compression_ratio": 1.8666666666666667, "no_speech_prob": 2.078285615425557e-05}, {"id": 639, "seek": 331460, "start": 3326.92, "end": 3332.2799999999997, "text": " things the strength of the realization which we aren't using yet so far so", "tokens": [50364, 436, 434, 1143, 337, 3250, 670, 439, 264, 9848, 9834, 295, 428, 2316, 370, 50600, 50600, 9848, 9834, 366, 337, 1365, 264, 2744, 295, 341, 7633, 4583, 264, 2744, 295, 50802, 50802, 264, 12240, 3584, 370, 341, 307, 257, 3262, 420, 257, 732, 337, 505, 457, 321, 727, 853, 819, 50980, 50980, 721, 264, 3800, 295, 264, 25138, 597, 321, 3212, 380, 1228, 1939, 370, 1400, 370, 51248, 51248, 456, 311, 3195, 295, 819, 9848, 9834, 293, 6257, 300, 352, 666, 51378, 51378, 17827, 257, 18161, 2533, 293, 291, 393, 853, 867, 819, 17840, 295, 552, 293, 51562, 51562], "temperature": 0.0, "avg_logprob": -0.10513746620404839, "compression_ratio": 1.8666666666666667, "no_speech_prob": 2.078285615425557e-05}, {"id": 640, "seek": 331460, "start": 3332.2799999999997, "end": 3334.88, "text": " there's lots of different hyper parameters and settings that go into", "tokens": [50364, 436, 434, 1143, 337, 3250, 670, 439, 264, 9848, 9834, 295, 428, 2316, 370, 50600, 50600, 9848, 9834, 366, 337, 1365, 264, 2744, 295, 341, 7633, 4583, 264, 2744, 295, 50802, 50802, 264, 12240, 3584, 370, 341, 307, 257, 3262, 420, 257, 732, 337, 505, 457, 321, 727, 853, 819, 50980, 50980, 721, 264, 3800, 295, 264, 25138, 597, 321, 3212, 380, 1228, 1939, 370, 1400, 370, 51248, 51248, 456, 311, 3195, 295, 819, 9848, 9834, 293, 6257, 300, 352, 666, 51378, 51378, 17827, 257, 18161, 2533, 293, 291, 393, 853, 867, 819, 17840, 295, 552, 293, 51562, 51562], "temperature": 0.0, "avg_logprob": -0.10513746620404839, "compression_ratio": 1.8666666666666667, "no_speech_prob": 2.078285615425557e-05}, {"id": 641, "seek": 331460, "start": 3334.88, "end": 3338.56, "text": " defining a neural net and you can try many different variations of them and", "tokens": [50364, 436, 434, 1143, 337, 3250, 670, 439, 264, 9848, 9834, 295, 428, 2316, 370, 50600, 50600, 9848, 9834, 366, 337, 1365, 264, 2744, 295, 341, 7633, 4583, 264, 2744, 295, 50802, 50802, 264, 12240, 3584, 370, 341, 307, 257, 3262, 420, 257, 732, 337, 505, 457, 321, 727, 853, 819, 50980, 50980, 721, 264, 3800, 295, 264, 25138, 597, 321, 3212, 380, 1228, 1939, 370, 1400, 370, 51248, 51248, 456, 311, 3195, 295, 819, 9848, 9834, 293, 6257, 300, 352, 666, 51378, 51378, 17827, 257, 18161, 2533, 293, 291, 393, 853, 867, 819, 17840, 295, 552, 293, 51562, 51562], "temperature": 0.0, "avg_logprob": -0.10513746620404839, "compression_ratio": 1.8666666666666667, "no_speech_prob": 2.078285615425557e-05}, {"id": 642, "seek": 333856, "start": 3338.56, "end": 3344.56, "text": " see whichever one works best on your validation split so this is used to", "tokens": [50364, 536, 24123, 472, 1985, 1151, 322, 428, 24071, 7472, 370, 341, 307, 1143, 281, 50664, 50664, 3847, 264, 9834, 341, 307, 1143, 281, 3847, 264, 9848, 9834, 293, 1500, 50900, 50900, 7472, 307, 1143, 281, 13059, 1936, 264, 3389, 295, 264, 2316, 412, 264, 917, 370, 51156, 51156, 321, 434, 787, 27479, 264, 4470, 322, 264, 1500, 7472, 588, 588, 637, 1921, 356, 293, 588, 51334, 51334, 1326, 1413, 570, 633, 2167, 565, 291, 13059, 428, 1500, 4470, 293, 291, 1466, 51566, 51566, 746, 490, 309, 291, 366, 1936, 2891, 281, 611, 3847, 322, 264, 1500, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.09233078956604004, "compression_ratio": 2.0407239819004523, "no_speech_prob": 9.66571951721562e-06}, {"id": 643, "seek": 333856, "start": 3344.56, "end": 3349.2799999999997, "text": " train the parameters this is used to train the hyper parameters and test", "tokens": [50364, 536, 24123, 472, 1985, 1151, 322, 428, 24071, 7472, 370, 341, 307, 1143, 281, 50664, 50664, 3847, 264, 9834, 341, 307, 1143, 281, 3847, 264, 9848, 9834, 293, 1500, 50900, 50900, 7472, 307, 1143, 281, 13059, 1936, 264, 3389, 295, 264, 2316, 412, 264, 917, 370, 51156, 51156, 321, 434, 787, 27479, 264, 4470, 322, 264, 1500, 7472, 588, 588, 637, 1921, 356, 293, 588, 51334, 51334, 1326, 1413, 570, 633, 2167, 565, 291, 13059, 428, 1500, 4470, 293, 291, 1466, 51566, 51566, 746, 490, 309, 291, 366, 1936, 2891, 281, 611, 3847, 322, 264, 1500, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.09233078956604004, "compression_ratio": 2.0407239819004523, "no_speech_prob": 9.66571951721562e-06}, {"id": 644, "seek": 333856, "start": 3349.2799999999997, "end": 3354.4, "text": " split is used to evaluate basically the performance of the model at the end so", "tokens": [50364, 536, 24123, 472, 1985, 1151, 322, 428, 24071, 7472, 370, 341, 307, 1143, 281, 50664, 50664, 3847, 264, 9834, 341, 307, 1143, 281, 3847, 264, 9848, 9834, 293, 1500, 50900, 50900, 7472, 307, 1143, 281, 13059, 1936, 264, 3389, 295, 264, 2316, 412, 264, 917, 370, 51156, 51156, 321, 434, 787, 27479, 264, 4470, 322, 264, 1500, 7472, 588, 588, 637, 1921, 356, 293, 588, 51334, 51334, 1326, 1413, 570, 633, 2167, 565, 291, 13059, 428, 1500, 4470, 293, 291, 1466, 51566, 51566, 746, 490, 309, 291, 366, 1936, 2891, 281, 611, 3847, 322, 264, 1500, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.09233078956604004, "compression_ratio": 2.0407239819004523, "no_speech_prob": 9.66571951721562e-06}, {"id": 645, "seek": 333856, "start": 3354.4, "end": 3357.96, "text": " we're only evaluating the loss on the test split very very sparingly and very", "tokens": [50364, 536, 24123, 472, 1985, 1151, 322, 428, 24071, 7472, 370, 341, 307, 1143, 281, 50664, 50664, 3847, 264, 9834, 341, 307, 1143, 281, 3847, 264, 9848, 9834, 293, 1500, 50900, 50900, 7472, 307, 1143, 281, 13059, 1936, 264, 3389, 295, 264, 2316, 412, 264, 917, 370, 51156, 51156, 321, 434, 787, 27479, 264, 4470, 322, 264, 1500, 7472, 588, 588, 637, 1921, 356, 293, 588, 51334, 51334, 1326, 1413, 570, 633, 2167, 565, 291, 13059, 428, 1500, 4470, 293, 291, 1466, 51566, 51566, 746, 490, 309, 291, 366, 1936, 2891, 281, 611, 3847, 322, 264, 1500, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.09233078956604004, "compression_ratio": 2.0407239819004523, "no_speech_prob": 9.66571951721562e-06}, {"id": 646, "seek": 333856, "start": 3357.96, "end": 3362.6, "text": " few times because every single time you evaluate your test loss and you learn", "tokens": [50364, 536, 24123, 472, 1985, 1151, 322, 428, 24071, 7472, 370, 341, 307, 1143, 281, 50664, 50664, 3847, 264, 9834, 341, 307, 1143, 281, 3847, 264, 9848, 9834, 293, 1500, 50900, 50900, 7472, 307, 1143, 281, 13059, 1936, 264, 3389, 295, 264, 2316, 412, 264, 917, 370, 51156, 51156, 321, 434, 787, 27479, 264, 4470, 322, 264, 1500, 7472, 588, 588, 637, 1921, 356, 293, 588, 51334, 51334, 1326, 1413, 570, 633, 2167, 565, 291, 13059, 428, 1500, 4470, 293, 291, 1466, 51566, 51566, 746, 490, 309, 291, 366, 1936, 2891, 281, 611, 3847, 322, 264, 1500, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.09233078956604004, "compression_ratio": 2.0407239819004523, "no_speech_prob": 9.66571951721562e-06}, {"id": 647, "seek": 333856, "start": 3362.6, "end": 3367.16, "text": " something from it you are basically starting to also train on the test", "tokens": [50364, 536, 24123, 472, 1985, 1151, 322, 428, 24071, 7472, 370, 341, 307, 1143, 281, 50664, 50664, 3847, 264, 9834, 341, 307, 1143, 281, 3847, 264, 9848, 9834, 293, 1500, 50900, 50900, 7472, 307, 1143, 281, 13059, 1936, 264, 3389, 295, 264, 2316, 412, 264, 917, 370, 51156, 51156, 321, 434, 787, 27479, 264, 4470, 322, 264, 1500, 7472, 588, 588, 637, 1921, 356, 293, 588, 51334, 51334, 1326, 1413, 570, 633, 2167, 565, 291, 13059, 428, 1500, 4470, 293, 291, 1466, 51566, 51566, 746, 490, 309, 291, 366, 1936, 2891, 281, 611, 3847, 322, 264, 1500, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.09233078956604004, "compression_ratio": 2.0407239819004523, "no_speech_prob": 9.66571951721562e-06}, {"id": 648, "seek": 336716, "start": 3367.16, "end": 3373.8799999999997, "text": " split so you are only allowed to test the loss on the test set very very few", "tokens": [50364, 7472, 370, 291, 366, 787, 4350, 281, 1500, 264, 4470, 322, 264, 1500, 992, 588, 588, 1326, 50700, 50700, 1413, 5911, 291, 3148, 670, 69, 2414, 281, 309, 382, 731, 382, 291, 5120, 322, 428, 50938, 50938, 2316, 370, 718, 311, 611, 7472, 493, 527, 3097, 1412, 666, 3847, 1905, 293, 1500, 51202, 51202, 293, 550, 321, 366, 516, 281, 3847, 322, 3847, 293, 787, 13059, 322, 1500, 588, 588, 51428, 51428, 637, 1921, 356, 1392, 370, 510, 321, 352, 510, 307, 689, 321, 1890, 439, 264, 2283, 293, 829, 552, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.08747902920371607, "compression_ratio": 1.8349514563106797, "no_speech_prob": 1.6700412743375637e-05}, {"id": 649, "seek": 336716, "start": 3373.8799999999997, "end": 3378.64, "text": " times otherwise you risk overfitting to it as well as you experiment on your", "tokens": [50364, 7472, 370, 291, 366, 787, 4350, 281, 1500, 264, 4470, 322, 264, 1500, 992, 588, 588, 1326, 50700, 50700, 1413, 5911, 291, 3148, 670, 69, 2414, 281, 309, 382, 731, 382, 291, 5120, 322, 428, 50938, 50938, 2316, 370, 718, 311, 611, 7472, 493, 527, 3097, 1412, 666, 3847, 1905, 293, 1500, 51202, 51202, 293, 550, 321, 366, 516, 281, 3847, 322, 3847, 293, 787, 13059, 322, 1500, 588, 588, 51428, 51428, 637, 1921, 356, 1392, 370, 510, 321, 352, 510, 307, 689, 321, 1890, 439, 264, 2283, 293, 829, 552, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.08747902920371607, "compression_ratio": 1.8349514563106797, "no_speech_prob": 1.6700412743375637e-05}, {"id": 650, "seek": 336716, "start": 3378.64, "end": 3383.92, "text": " model so let's also split up our training data into train dev and test", "tokens": [50364, 7472, 370, 291, 366, 787, 4350, 281, 1500, 264, 4470, 322, 264, 1500, 992, 588, 588, 1326, 50700, 50700, 1413, 5911, 291, 3148, 670, 69, 2414, 281, 309, 382, 731, 382, 291, 5120, 322, 428, 50938, 50938, 2316, 370, 718, 311, 611, 7472, 493, 527, 3097, 1412, 666, 3847, 1905, 293, 1500, 51202, 51202, 293, 550, 321, 366, 516, 281, 3847, 322, 3847, 293, 787, 13059, 322, 1500, 588, 588, 51428, 51428, 637, 1921, 356, 1392, 370, 510, 321, 352, 510, 307, 689, 321, 1890, 439, 264, 2283, 293, 829, 552, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.08747902920371607, "compression_ratio": 1.8349514563106797, "no_speech_prob": 1.6700412743375637e-05}, {"id": 651, "seek": 336716, "start": 3383.92, "end": 3388.44, "text": " and then we are going to train on train and only evaluate on test very very", "tokens": [50364, 7472, 370, 291, 366, 787, 4350, 281, 1500, 264, 4470, 322, 264, 1500, 992, 588, 588, 1326, 50700, 50700, 1413, 5911, 291, 3148, 670, 69, 2414, 281, 309, 382, 731, 382, 291, 5120, 322, 428, 50938, 50938, 2316, 370, 718, 311, 611, 7472, 493, 527, 3097, 1412, 666, 3847, 1905, 293, 1500, 51202, 51202, 293, 550, 321, 366, 516, 281, 3847, 322, 3847, 293, 787, 13059, 322, 1500, 588, 588, 51428, 51428, 637, 1921, 356, 1392, 370, 510, 321, 352, 510, 307, 689, 321, 1890, 439, 264, 2283, 293, 829, 552, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.08747902920371607, "compression_ratio": 1.8349514563106797, "no_speech_prob": 1.6700412743375637e-05}, {"id": 652, "seek": 336716, "start": 3388.44, "end": 3393.96, "text": " sparingly okay so here we go here is where we took all the words and put them", "tokens": [50364, 7472, 370, 291, 366, 787, 4350, 281, 1500, 264, 4470, 322, 264, 1500, 992, 588, 588, 1326, 50700, 50700, 1413, 5911, 291, 3148, 670, 69, 2414, 281, 309, 382, 731, 382, 291, 5120, 322, 428, 50938, 50938, 2316, 370, 718, 311, 611, 7472, 493, 527, 3097, 1412, 666, 3847, 1905, 293, 1500, 51202, 51202, 293, 550, 321, 366, 516, 281, 3847, 322, 3847, 293, 787, 13059, 322, 1500, 588, 588, 51428, 51428, 637, 1921, 356, 1392, 370, 510, 321, 352, 510, 307, 689, 321, 1890, 439, 264, 2283, 293, 829, 552, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.08747902920371607, "compression_ratio": 1.8349514563106797, "no_speech_prob": 1.6700412743375637e-05}, {"id": 653, "seek": 339396, "start": 3393.96, "end": 3398.88, "text": " into X and Y tensors so instead let me create a new cell here and let me just", "tokens": [50364, 666, 1783, 293, 398, 10688, 830, 370, 2602, 718, 385, 1884, 257, 777, 2815, 510, 293, 718, 385, 445, 50610, 50610, 5055, 9163, 512, 3089, 510, 570, 286, 500, 380, 519, 309, 311, 300, 3997, 457, 321, 434, 50954, 50954, 799, 853, 281, 3155, 257, 707, 857, 295, 565, 286, 478, 29942, 341, 281, 312, 257, 2445, 586, 51132, 51132, 293, 341, 2445, 2516, 512, 1329, 295, 2283, 293, 15182, 264, 41011, 1783, 293, 398, 337, 51390, 51390, 729, 2283, 787, 293, 550, 510, 286, 669, 402, 1245, 1688, 493, 439, 264, 2283, 370, 613, 366, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.07135527610778808, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.223066626873333e-06}, {"id": 654, "seek": 339396, "start": 3398.88, "end": 3405.76, "text": " copy paste some code here because I don't think it's that complex but we're", "tokens": [50364, 666, 1783, 293, 398, 10688, 830, 370, 2602, 718, 385, 1884, 257, 777, 2815, 510, 293, 718, 385, 445, 50610, 50610, 5055, 9163, 512, 3089, 510, 570, 286, 500, 380, 519, 309, 311, 300, 3997, 457, 321, 434, 50954, 50954, 799, 853, 281, 3155, 257, 707, 857, 295, 565, 286, 478, 29942, 341, 281, 312, 257, 2445, 586, 51132, 51132, 293, 341, 2445, 2516, 512, 1329, 295, 2283, 293, 15182, 264, 41011, 1783, 293, 398, 337, 51390, 51390, 729, 2283, 787, 293, 550, 510, 286, 669, 402, 1245, 1688, 493, 439, 264, 2283, 370, 613, 366, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.07135527610778808, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.223066626873333e-06}, {"id": 655, "seek": 339396, "start": 3405.76, "end": 3409.32, "text": " gonna try to save a little bit of time I'm converting this to be a function now", "tokens": [50364, 666, 1783, 293, 398, 10688, 830, 370, 2602, 718, 385, 1884, 257, 777, 2815, 510, 293, 718, 385, 445, 50610, 50610, 5055, 9163, 512, 3089, 510, 570, 286, 500, 380, 519, 309, 311, 300, 3997, 457, 321, 434, 50954, 50954, 799, 853, 281, 3155, 257, 707, 857, 295, 565, 286, 478, 29942, 341, 281, 312, 257, 2445, 586, 51132, 51132, 293, 341, 2445, 2516, 512, 1329, 295, 2283, 293, 15182, 264, 41011, 1783, 293, 398, 337, 51390, 51390, 729, 2283, 787, 293, 550, 510, 286, 669, 402, 1245, 1688, 493, 439, 264, 2283, 370, 613, 366, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.07135527610778808, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.223066626873333e-06}, {"id": 656, "seek": 339396, "start": 3409.32, "end": 3414.48, "text": " and this function takes some list of words and builds the arrays X and Y for", "tokens": [50364, 666, 1783, 293, 398, 10688, 830, 370, 2602, 718, 385, 1884, 257, 777, 2815, 510, 293, 718, 385, 445, 50610, 50610, 5055, 9163, 512, 3089, 510, 570, 286, 500, 380, 519, 309, 311, 300, 3997, 457, 321, 434, 50954, 50954, 799, 853, 281, 3155, 257, 707, 857, 295, 565, 286, 478, 29942, 341, 281, 312, 257, 2445, 586, 51132, 51132, 293, 341, 2445, 2516, 512, 1329, 295, 2283, 293, 15182, 264, 41011, 1783, 293, 398, 337, 51390, 51390, 729, 2283, 787, 293, 550, 510, 286, 669, 402, 1245, 1688, 493, 439, 264, 2283, 370, 613, 366, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.07135527610778808, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.223066626873333e-06}, {"id": 657, "seek": 339396, "start": 3414.48, "end": 3420.8, "text": " those words only and then here I am shuffling up all the words so these are", "tokens": [50364, 666, 1783, 293, 398, 10688, 830, 370, 2602, 718, 385, 1884, 257, 777, 2815, 510, 293, 718, 385, 445, 50610, 50610, 5055, 9163, 512, 3089, 510, 570, 286, 500, 380, 519, 309, 311, 300, 3997, 457, 321, 434, 50954, 50954, 799, 853, 281, 3155, 257, 707, 857, 295, 565, 286, 478, 29942, 341, 281, 312, 257, 2445, 586, 51132, 51132, 293, 341, 2445, 2516, 512, 1329, 295, 2283, 293, 15182, 264, 41011, 1783, 293, 398, 337, 51390, 51390, 729, 2283, 787, 293, 550, 510, 286, 669, 402, 1245, 1688, 493, 439, 264, 2283, 370, 613, 366, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.07135527610778808, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.223066626873333e-06}, {"id": 658, "seek": 342080, "start": 3420.8, "end": 3426.04, "text": " the input words that we get we are randomly shuffling them all up and then", "tokens": [50364, 264, 4846, 2283, 300, 321, 483, 321, 366, 16979, 402, 1245, 1688, 552, 439, 493, 293, 550, 50626, 50626, 321, 434, 516, 281, 992, 297, 16, 281, 312, 264, 1230, 295, 5110, 456, 311, 4688, 4, 295, 264, 2283, 293, 50954, 50954, 297, 17, 281, 312, 4289, 4, 295, 264, 636, 295, 264, 2283, 370, 1936, 498, 4641, 295, 2283, 307, 2217, 11, 1360, 51294, 51294, 293, 472, 307, 731, 2597, 286, 820, 1391, 1190, 341, 293, 472, 307, 3552, 11, 1360, 293, 297, 17, 307, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13469101587931315, "compression_ratio": 1.6145833333333333, "no_speech_prob": 3.7851866636628984e-06}, {"id": 659, "seek": 342080, "start": 3426.04, "end": 3432.6000000000004, "text": " we're going to set n1 to be the number of examples there's 80% of the words and", "tokens": [50364, 264, 4846, 2283, 300, 321, 483, 321, 366, 16979, 402, 1245, 1688, 552, 439, 493, 293, 550, 50626, 50626, 321, 434, 516, 281, 992, 297, 16, 281, 312, 264, 1230, 295, 5110, 456, 311, 4688, 4, 295, 264, 2283, 293, 50954, 50954, 297, 17, 281, 312, 4289, 4, 295, 264, 636, 295, 264, 2283, 370, 1936, 498, 4641, 295, 2283, 307, 2217, 11, 1360, 51294, 51294, 293, 472, 307, 731, 2597, 286, 820, 1391, 1190, 341, 293, 472, 307, 3552, 11, 1360, 293, 297, 17, 307, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13469101587931315, "compression_ratio": 1.6145833333333333, "no_speech_prob": 3.7851866636628984e-06}, {"id": 660, "seek": 342080, "start": 3432.6000000000004, "end": 3439.4, "text": " n2 to be 90% of the way of the words so basically if length of words is 30,000", "tokens": [50364, 264, 4846, 2283, 300, 321, 483, 321, 366, 16979, 402, 1245, 1688, 552, 439, 493, 293, 550, 50626, 50626, 321, 434, 516, 281, 992, 297, 16, 281, 312, 264, 1230, 295, 5110, 456, 311, 4688, 4, 295, 264, 2283, 293, 50954, 50954, 297, 17, 281, 312, 4289, 4, 295, 264, 636, 295, 264, 2283, 370, 1936, 498, 4641, 295, 2283, 307, 2217, 11, 1360, 51294, 51294, 293, 472, 307, 731, 2597, 286, 820, 1391, 1190, 341, 293, 472, 307, 3552, 11, 1360, 293, 297, 17, 307, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13469101587931315, "compression_ratio": 1.6145833333333333, "no_speech_prob": 3.7851866636628984e-06}, {"id": 661, "seek": 342080, "start": 3439.4, "end": 3446.8, "text": " and one is well sorry I should probably run this and one is 25,000 and n2 is", "tokens": [50364, 264, 4846, 2283, 300, 321, 483, 321, 366, 16979, 402, 1245, 1688, 552, 439, 493, 293, 550, 50626, 50626, 321, 434, 516, 281, 992, 297, 16, 281, 312, 264, 1230, 295, 5110, 456, 311, 4688, 4, 295, 264, 2283, 293, 50954, 50954, 297, 17, 281, 312, 4289, 4, 295, 264, 636, 295, 264, 2283, 370, 1936, 498, 4641, 295, 2283, 307, 2217, 11, 1360, 51294, 51294, 293, 472, 307, 731, 2597, 286, 820, 1391, 1190, 341, 293, 472, 307, 3552, 11, 1360, 293, 297, 17, 307, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13469101587931315, "compression_ratio": 1.6145833333333333, "no_speech_prob": 3.7851866636628984e-06}, {"id": 662, "seek": 344680, "start": 3446.8, "end": 3452.1200000000003, "text": " 28,000 and so here we see that I'm calling build data set to build a", "tokens": [50364, 7562, 11, 1360, 293, 370, 510, 321, 536, 300, 286, 478, 5141, 1322, 1412, 992, 281, 1322, 257, 50630, 50630, 3097, 992, 1783, 293, 398, 538, 8186, 278, 666, 493, 281, 297, 16, 370, 321, 434, 516, 281, 362, 787, 3552, 11, 1360, 50940, 50940, 3097, 2283, 293, 550, 321, 434, 516, 281, 362, 9810, 297, 17, 3175, 297, 16, 805, 11, 1360, 51386, 51386, 24071, 5110, 420, 1905, 5110, 293, 321, 434, 516, 281, 362, 4641, 295, 2283, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.11950745640030827, "compression_ratio": 1.6818181818181819, "no_speech_prob": 3.668816816571052e-06}, {"id": 663, "seek": 344680, "start": 3452.1200000000003, "end": 3458.32, "text": " training set X and Y by indexing into up to n1 so we're going to have only 25,000", "tokens": [50364, 7562, 11, 1360, 293, 370, 510, 321, 536, 300, 286, 478, 5141, 1322, 1412, 992, 281, 1322, 257, 50630, 50630, 3097, 992, 1783, 293, 398, 538, 8186, 278, 666, 493, 281, 297, 16, 370, 321, 434, 516, 281, 362, 787, 3552, 11, 1360, 50940, 50940, 3097, 2283, 293, 550, 321, 434, 516, 281, 362, 9810, 297, 17, 3175, 297, 16, 805, 11, 1360, 51386, 51386, 24071, 5110, 420, 1905, 5110, 293, 321, 434, 516, 281, 362, 4641, 295, 2283, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.11950745640030827, "compression_ratio": 1.6818181818181819, "no_speech_prob": 3.668816816571052e-06}, {"id": 664, "seek": 344680, "start": 3458.32, "end": 3467.2400000000002, "text": " training words and then we're going to have roughly n2 minus n1 3,000", "tokens": [50364, 7562, 11, 1360, 293, 370, 510, 321, 536, 300, 286, 478, 5141, 1322, 1412, 992, 281, 1322, 257, 50630, 50630, 3097, 992, 1783, 293, 398, 538, 8186, 278, 666, 493, 281, 297, 16, 370, 321, 434, 516, 281, 362, 787, 3552, 11, 1360, 50940, 50940, 3097, 2283, 293, 550, 321, 434, 516, 281, 362, 9810, 297, 17, 3175, 297, 16, 805, 11, 1360, 51386, 51386, 24071, 5110, 420, 1905, 5110, 293, 321, 434, 516, 281, 362, 4641, 295, 2283, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.11950745640030827, "compression_ratio": 1.6818181818181819, "no_speech_prob": 3.668816816571052e-06}, {"id": 665, "seek": 344680, "start": 3467.2400000000002, "end": 3474.36, "text": " validation examples or dev examples and we're going to have length of words", "tokens": [50364, 7562, 11, 1360, 293, 370, 510, 321, 536, 300, 286, 478, 5141, 1322, 1412, 992, 281, 1322, 257, 50630, 50630, 3097, 992, 1783, 293, 398, 538, 8186, 278, 666, 493, 281, 297, 16, 370, 321, 434, 516, 281, 362, 787, 3552, 11, 1360, 50940, 50940, 3097, 2283, 293, 550, 321, 434, 516, 281, 362, 9810, 297, 17, 3175, 297, 16, 805, 11, 1360, 51386, 51386, 24071, 5110, 420, 1905, 5110, 293, 321, 434, 516, 281, 362, 4641, 295, 2283, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.11950745640030827, "compression_ratio": 1.6818181818181819, "no_speech_prob": 3.668816816571052e-06}, {"id": 666, "seek": 347436, "start": 3474.36, "end": 3485.96, "text": " basically minus n2 or 3,204 examples here for the test set so now we have X's", "tokens": [50364, 1936, 3175, 297, 17, 420, 805, 11, 2009, 19, 5110, 510, 337, 264, 1500, 992, 370, 586, 321, 362, 1783, 311, 50944, 50944, 293, 398, 311, 337, 439, 729, 1045, 37741, 1954, 1338, 286, 478, 14699, 641, 2744, 510, 1854, 51396, 51396, 264, 2445, 382, 731, 457, 510, 321, 500, 380, 362, 2283, 457, 613, 366, 1217, 264, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11747554040724231, "compression_ratio": 1.4233128834355828, "no_speech_prob": 1.6963889720500447e-05}, {"id": 667, "seek": 347436, "start": 3485.96, "end": 3495.0, "text": " and Y's for all those three splits oh yeah I'm printing their size here inside", "tokens": [50364, 1936, 3175, 297, 17, 420, 805, 11, 2009, 19, 5110, 510, 337, 264, 1500, 992, 370, 586, 321, 362, 1783, 311, 50944, 50944, 293, 398, 311, 337, 439, 729, 1045, 37741, 1954, 1338, 286, 478, 14699, 641, 2744, 510, 1854, 51396, 51396, 264, 2445, 382, 731, 457, 510, 321, 500, 380, 362, 2283, 457, 613, 366, 1217, 264, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11747554040724231, "compression_ratio": 1.4233128834355828, "no_speech_prob": 1.6963889720500447e-05}, {"id": 668, "seek": 347436, "start": 3495.0, "end": 3501.36, "text": " the function as well but here we don't have words but these are already the", "tokens": [50364, 1936, 3175, 297, 17, 420, 805, 11, 2009, 19, 5110, 510, 337, 264, 1500, 992, 370, 586, 321, 362, 1783, 311, 50944, 50944, 293, 398, 311, 337, 439, 729, 1045, 37741, 1954, 1338, 286, 478, 14699, 641, 2744, 510, 1854, 51396, 51396, 264, 2445, 382, 731, 457, 510, 321, 500, 380, 362, 2283, 457, 613, 366, 1217, 264, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11747554040724231, "compression_ratio": 1.4233128834355828, "no_speech_prob": 1.6963889720500447e-05}, {"id": 669, "seek": 350136, "start": 3501.36, "end": 3507.1600000000003, "text": " individual examples made from those words so let's now scroll down here and", "tokens": [50364, 2609, 5110, 1027, 490, 729, 2283, 370, 718, 311, 586, 11369, 760, 510, 293, 50654, 50654, 264, 1412, 992, 586, 337, 3097, 307, 544, 411, 341, 293, 550, 562, 321, 14322, 264, 51050, 51050, 3209, 562, 321, 434, 3097, 321, 434, 787, 516, 281, 312, 3097, 1228, 1783, 3847, 1783, 51500, 51500], "temperature": 0.0, "avg_logprob": -0.09280528845610442, "compression_ratio": 1.6028368794326242, "no_speech_prob": 3.187498577972292e-06}, {"id": 670, "seek": 350136, "start": 3507.1600000000003, "end": 3515.08, "text": " the data set now for training is more like this and then when we reset the", "tokens": [50364, 2609, 5110, 1027, 490, 729, 2283, 370, 718, 311, 586, 11369, 760, 510, 293, 50654, 50654, 264, 1412, 992, 586, 337, 3097, 307, 544, 411, 341, 293, 550, 562, 321, 14322, 264, 51050, 51050, 3209, 562, 321, 434, 3097, 321, 434, 787, 516, 281, 312, 3097, 1228, 1783, 3847, 1783, 51500, 51500], "temperature": 0.0, "avg_logprob": -0.09280528845610442, "compression_ratio": 1.6028368794326242, "no_speech_prob": 3.187498577972292e-06}, {"id": 671, "seek": 350136, "start": 3515.08, "end": 3524.08, "text": " network when we're training we're only going to be training using X train X", "tokens": [50364, 2609, 5110, 1027, 490, 729, 2283, 370, 718, 311, 586, 11369, 760, 510, 293, 50654, 50654, 264, 1412, 992, 586, 337, 3097, 307, 544, 411, 341, 293, 550, 562, 321, 14322, 264, 51050, 51050, 3209, 562, 321, 434, 3097, 321, 434, 787, 516, 281, 312, 3097, 1228, 1783, 3847, 1783, 51500, 51500], "temperature": 0.0, "avg_logprob": -0.09280528845610442, "compression_ratio": 1.6028368794326242, "no_speech_prob": 3.187498577972292e-06}, {"id": 672, "seek": 352408, "start": 3524.08, "end": 3538.92, "text": " train and Y train so that's the only thing we're training on let's see where", "tokens": [50364, 3847, 293, 398, 3847, 370, 300, 311, 264, 787, 551, 321, 434, 3097, 322, 718, 311, 536, 689, 51106, 51106, 321, 366, 322, 257, 2167, 15245, 718, 311, 586, 3847, 1310, 257, 1326, 544, 4439, 3097, 18161, 51596, 51596, 9590, 393, 747, 257, 1339, 2673, 500, 380, 360, 309, 294, 1622, 291, 4025, 257, 3840, 295, 4782, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.14530938570616675, "compression_ratio": 1.5695364238410596, "no_speech_prob": 3.4465131193428533e-06}, {"id": 673, "seek": 352408, "start": 3538.92, "end": 3548.72, "text": " we are on a single batch let's now train maybe a few more steps training neural", "tokens": [50364, 3847, 293, 398, 3847, 370, 300, 311, 264, 787, 551, 321, 434, 3097, 322, 718, 311, 536, 689, 51106, 51106, 321, 366, 322, 257, 2167, 15245, 718, 311, 586, 3847, 1310, 257, 1326, 544, 4439, 3097, 18161, 51596, 51596, 9590, 393, 747, 257, 1339, 2673, 500, 380, 360, 309, 294, 1622, 291, 4025, 257, 3840, 295, 4782, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.14530938570616675, "compression_ratio": 1.5695364238410596, "no_speech_prob": 3.4465131193428533e-06}, {"id": 674, "seek": 352408, "start": 3548.72, "end": 3552.24, "text": " networks can take a while usually don't do it in line you launch a bunch of jobs", "tokens": [50364, 3847, 293, 398, 3847, 370, 300, 311, 264, 787, 551, 321, 434, 3097, 322, 718, 311, 536, 689, 51106, 51106, 321, 366, 322, 257, 2167, 15245, 718, 311, 586, 3847, 1310, 257, 1326, 544, 4439, 3097, 18161, 51596, 51596, 9590, 393, 747, 257, 1339, 2673, 500, 380, 360, 309, 294, 1622, 291, 4025, 257, 3840, 295, 4782, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.14530938570616675, "compression_ratio": 1.5695364238410596, "no_speech_prob": 3.4465131193428533e-06}, {"id": 675, "seek": 355224, "start": 3552.24, "end": 3556.3599999999997, "text": " and you wait for them to finish can take in multiple days and so on", "tokens": [50364, 293, 291, 1699, 337, 552, 281, 2413, 393, 747, 294, 3866, 1708, 293, 370, 322, 50570, 50570, 22880, 341, 307, 257, 588, 1359, 3209, 1392, 370, 264, 4470, 307, 1238, 665, 1954, 321, 50962, 50962, 15715, 1143, 527, 2539, 3314, 300, 307, 636, 886, 2295, 370, 718, 385, 767, 808, 51178, 51178, 646, 321, 1143, 264, 21039, 2539, 3314, 295, 1958, 13, 10607, 370, 341, 486, 3847, 709, 4663, 293, 51582, 51582], "temperature": 0.0, "avg_logprob": -0.12238417445002375, "compression_ratio": 1.6063829787234043, "no_speech_prob": 6.4388596001663245e-06}, {"id": 676, "seek": 355224, "start": 3556.3599999999997, "end": 3564.2, "text": " luckily this is a very small network okay so the loss is pretty good oh we", "tokens": [50364, 293, 291, 1699, 337, 552, 281, 2413, 393, 747, 294, 3866, 1708, 293, 370, 322, 50570, 50570, 22880, 341, 307, 257, 588, 1359, 3209, 1392, 370, 264, 4470, 307, 1238, 665, 1954, 321, 50962, 50962, 15715, 1143, 527, 2539, 3314, 300, 307, 636, 886, 2295, 370, 718, 385, 767, 808, 51178, 51178, 646, 321, 1143, 264, 21039, 2539, 3314, 295, 1958, 13, 10607, 370, 341, 486, 3847, 709, 4663, 293, 51582, 51582], "temperature": 0.0, "avg_logprob": -0.12238417445002375, "compression_ratio": 1.6063829787234043, "no_speech_prob": 6.4388596001663245e-06}, {"id": 677, "seek": 355224, "start": 3564.2, "end": 3568.52, "text": " accidentally used our learning rate that is way too low so let me actually come", "tokens": [50364, 293, 291, 1699, 337, 552, 281, 2413, 393, 747, 294, 3866, 1708, 293, 370, 322, 50570, 50570, 22880, 341, 307, 257, 588, 1359, 3209, 1392, 370, 264, 4470, 307, 1238, 665, 1954, 321, 50962, 50962, 15715, 1143, 527, 2539, 3314, 300, 307, 636, 886, 2295, 370, 718, 385, 767, 808, 51178, 51178, 646, 321, 1143, 264, 21039, 2539, 3314, 295, 1958, 13, 10607, 370, 341, 486, 3847, 709, 4663, 293, 51582, 51582], "temperature": 0.0, "avg_logprob": -0.12238417445002375, "compression_ratio": 1.6063829787234043, "no_speech_prob": 6.4388596001663245e-06}, {"id": 678, "seek": 355224, "start": 3568.52, "end": 3576.6, "text": " back we used the decay learning rate of 0.01 so this will train much faster and", "tokens": [50364, 293, 291, 1699, 337, 552, 281, 2413, 393, 747, 294, 3866, 1708, 293, 370, 322, 50570, 50570, 22880, 341, 307, 257, 588, 1359, 3209, 1392, 370, 264, 4470, 307, 1238, 665, 1954, 321, 50962, 50962, 15715, 1143, 527, 2539, 3314, 300, 307, 636, 886, 2295, 370, 718, 385, 767, 808, 51178, 51178, 646, 321, 1143, 264, 21039, 2539, 3314, 295, 1958, 13, 10607, 370, 341, 486, 3847, 709, 4663, 293, 51582, 51582], "temperature": 0.0, "avg_logprob": -0.12238417445002375, "compression_ratio": 1.6063829787234043, "no_speech_prob": 6.4388596001663245e-06}, {"id": 679, "seek": 357660, "start": 3576.6, "end": 3585.0, "text": " then here when we evaluate let's use the depth set here X dev and my dev to", "tokens": [50364, 550, 510, 562, 321, 13059, 718, 311, 764, 264, 7161, 992, 510, 1783, 1905, 293, 452, 1905, 281, 50784, 50784, 13059, 264, 4470, 1392, 293, 718, 311, 406, 21039, 264, 2539, 3314, 293, 787, 360, 584, 51118, 51118, 1266, 11, 1360, 5110, 293, 718, 311, 13059, 264, 1905, 4470, 1564, 510, 1392, 370, 321, 434, 1242, 51532, 51532, 466, 568, 13, 18, 322, 1905, 293, 370, 264, 18161, 3209, 562, 309, 390, 3097, 630, 406, 536, 51737, 51737], "temperature": 0.0, "avg_logprob": -0.18419655561447143, "compression_ratio": 1.701657458563536, "no_speech_prob": 2.6425504984217696e-06}, {"id": 680, "seek": 357660, "start": 3585.0, "end": 3591.68, "text": " evaluate the loss okay and let's not decay the learning rate and only do say", "tokens": [50364, 550, 510, 562, 321, 13059, 718, 311, 764, 264, 7161, 992, 510, 1783, 1905, 293, 452, 1905, 281, 50784, 50784, 13059, 264, 4470, 1392, 293, 718, 311, 406, 21039, 264, 2539, 3314, 293, 787, 360, 584, 51118, 51118, 1266, 11, 1360, 5110, 293, 718, 311, 13059, 264, 1905, 4470, 1564, 510, 1392, 370, 321, 434, 1242, 51532, 51532, 466, 568, 13, 18, 322, 1905, 293, 370, 264, 18161, 3209, 562, 309, 390, 3097, 630, 406, 536, 51737, 51737], "temperature": 0.0, "avg_logprob": -0.18419655561447143, "compression_ratio": 1.701657458563536, "no_speech_prob": 2.6425504984217696e-06}, {"id": 681, "seek": 357660, "start": 3591.68, "end": 3599.96, "text": " 10,000 examples and let's evaluate the dev loss once here okay so we're getting", "tokens": [50364, 550, 510, 562, 321, 13059, 718, 311, 764, 264, 7161, 992, 510, 1783, 1905, 293, 452, 1905, 281, 50784, 50784, 13059, 264, 4470, 1392, 293, 718, 311, 406, 21039, 264, 2539, 3314, 293, 787, 360, 584, 51118, 51118, 1266, 11, 1360, 5110, 293, 718, 311, 13059, 264, 1905, 4470, 1564, 510, 1392, 370, 321, 434, 1242, 51532, 51532, 466, 568, 13, 18, 322, 1905, 293, 370, 264, 18161, 3209, 562, 309, 390, 3097, 630, 406, 536, 51737, 51737], "temperature": 0.0, "avg_logprob": -0.18419655561447143, "compression_ratio": 1.701657458563536, "no_speech_prob": 2.6425504984217696e-06}, {"id": 682, "seek": 357660, "start": 3599.96, "end": 3604.06, "text": " about 2.3 on dev and so the neural network when it was training did not see", "tokens": [50364, 550, 510, 562, 321, 13059, 718, 311, 764, 264, 7161, 992, 510, 1783, 1905, 293, 452, 1905, 281, 50784, 50784, 13059, 264, 4470, 1392, 293, 718, 311, 406, 21039, 264, 2539, 3314, 293, 787, 360, 584, 51118, 51118, 1266, 11, 1360, 5110, 293, 718, 311, 13059, 264, 1905, 4470, 1564, 510, 1392, 370, 321, 434, 1242, 51532, 51532, 466, 568, 13, 18, 322, 1905, 293, 370, 264, 18161, 3209, 562, 309, 390, 3097, 630, 406, 536, 51737, 51737], "temperature": 0.0, "avg_logprob": -0.18419655561447143, "compression_ratio": 1.701657458563536, "no_speech_prob": 2.6425504984217696e-06}, {"id": 683, "seek": 360406, "start": 3604.06, "end": 3609.24, "text": " these dev examples it hasn't optimized on them and yet when we evaluate the loss", "tokens": [50364, 613, 1905, 5110, 309, 6132, 380, 26941, 322, 552, 293, 1939, 562, 321, 13059, 264, 4470, 50623, 50623, 322, 613, 1905, 321, 767, 483, 257, 1238, 8681, 4470, 293, 370, 321, 393, 611, 574, 412, 50891, 50891, 437, 264, 4470, 307, 322, 439, 295, 3097, 992, 34166, 293, 370, 321, 536, 300, 264, 3097, 293, 51277, 51277, 264, 1905, 4470, 366, 466, 2681, 370, 321, 434, 406, 670, 69, 2414, 341, 2316, 307, 406, 51525, 51525, 4005, 1547, 281, 445, 312, 17491, 10560, 3319, 264, 1412, 293, 370, 1400, 321, 366, 51781, 51781], "temperature": 0.0, "avg_logprob": -0.08865002080013877, "compression_ratio": 1.7685185185185186, "no_speech_prob": 9.223265806213021e-06}, {"id": 684, "seek": 360406, "start": 3609.24, "end": 3614.6, "text": " on these dev we actually get a pretty decent loss and so we can also look at", "tokens": [50364, 613, 1905, 5110, 309, 6132, 380, 26941, 322, 552, 293, 1939, 562, 321, 13059, 264, 4470, 50623, 50623, 322, 613, 1905, 321, 767, 483, 257, 1238, 8681, 4470, 293, 370, 321, 393, 611, 574, 412, 50891, 50891, 437, 264, 4470, 307, 322, 439, 295, 3097, 992, 34166, 293, 370, 321, 536, 300, 264, 3097, 293, 51277, 51277, 264, 1905, 4470, 366, 466, 2681, 370, 321, 434, 406, 670, 69, 2414, 341, 2316, 307, 406, 51525, 51525, 4005, 1547, 281, 445, 312, 17491, 10560, 3319, 264, 1412, 293, 370, 1400, 321, 366, 51781, 51781], "temperature": 0.0, "avg_logprob": -0.08865002080013877, "compression_ratio": 1.7685185185185186, "no_speech_prob": 9.223265806213021e-06}, {"id": 685, "seek": 360406, "start": 3614.6, "end": 3622.32, "text": " what the loss is on all of training set oops and so we see that the training and", "tokens": [50364, 613, 1905, 5110, 309, 6132, 380, 26941, 322, 552, 293, 1939, 562, 321, 13059, 264, 4470, 50623, 50623, 322, 613, 1905, 321, 767, 483, 257, 1238, 8681, 4470, 293, 370, 321, 393, 611, 574, 412, 50891, 50891, 437, 264, 4470, 307, 322, 439, 295, 3097, 992, 34166, 293, 370, 321, 536, 300, 264, 3097, 293, 51277, 51277, 264, 1905, 4470, 366, 466, 2681, 370, 321, 434, 406, 670, 69, 2414, 341, 2316, 307, 406, 51525, 51525, 4005, 1547, 281, 445, 312, 17491, 10560, 3319, 264, 1412, 293, 370, 1400, 321, 366, 51781, 51781], "temperature": 0.0, "avg_logprob": -0.08865002080013877, "compression_ratio": 1.7685185185185186, "no_speech_prob": 9.223265806213021e-06}, {"id": 686, "seek": 360406, "start": 3622.32, "end": 3627.2799999999997, "text": " the dev loss are about equal so we're not overfitting this model is not", "tokens": [50364, 613, 1905, 5110, 309, 6132, 380, 26941, 322, 552, 293, 1939, 562, 321, 13059, 264, 4470, 50623, 50623, 322, 613, 1905, 321, 767, 483, 257, 1238, 8681, 4470, 293, 370, 321, 393, 611, 574, 412, 50891, 50891, 437, 264, 4470, 307, 322, 439, 295, 3097, 992, 34166, 293, 370, 321, 536, 300, 264, 3097, 293, 51277, 51277, 264, 1905, 4470, 366, 466, 2681, 370, 321, 434, 406, 670, 69, 2414, 341, 2316, 307, 406, 51525, 51525, 4005, 1547, 281, 445, 312, 17491, 10560, 3319, 264, 1412, 293, 370, 1400, 321, 366, 51781, 51781], "temperature": 0.0, "avg_logprob": -0.08865002080013877, "compression_ratio": 1.7685185185185186, "no_speech_prob": 9.223265806213021e-06}, {"id": 687, "seek": 360406, "start": 3627.2799999999997, "end": 3632.4, "text": " powerful enough to just be purely memorizing the data and so far we are", "tokens": [50364, 613, 1905, 5110, 309, 6132, 380, 26941, 322, 552, 293, 1939, 562, 321, 13059, 264, 4470, 50623, 50623, 322, 613, 1905, 321, 767, 483, 257, 1238, 8681, 4470, 293, 370, 321, 393, 611, 574, 412, 50891, 50891, 437, 264, 4470, 307, 322, 439, 295, 3097, 992, 34166, 293, 370, 321, 536, 300, 264, 3097, 293, 51277, 51277, 264, 1905, 4470, 366, 466, 2681, 370, 321, 434, 406, 670, 69, 2414, 341, 2316, 307, 406, 51525, 51525, 4005, 1547, 281, 445, 312, 17491, 10560, 3319, 264, 1412, 293, 370, 1400, 321, 366, 51781, 51781], "temperature": 0.0, "avg_logprob": -0.08865002080013877, "compression_ratio": 1.7685185185185186, "no_speech_prob": 9.223265806213021e-06}, {"id": 688, "seek": 363240, "start": 3632.4, "end": 3636.64, "text": " what's called underfitting because the training loss and the dev or test losses", "tokens": [50364, 437, 311, 1219, 833, 69, 2414, 570, 264, 3097, 4470, 293, 264, 1905, 420, 1500, 15352, 50576, 50576, 366, 9810, 2681, 370, 437, 300, 5850, 1355, 307, 300, 527, 3209, 307, 588, 5870, 50808, 50808, 588, 1359, 293, 321, 2066, 281, 652, 3389, 13797, 538, 21589, 493, 51060, 51060, 264, 2744, 295, 341, 18161, 2533, 370, 718, 311, 360, 300, 586, 370, 718, 311, 808, 670, 510, 293, 51240, 51240, 718, 311, 3488, 264, 2744, 365, 264, 18161, 2533, 264, 12889, 636, 281, 360, 341, 307, 321, 393, 51458, 51458, 808, 510, 281, 264, 7633, 4583, 597, 4362, 307, 257, 3262, 22027, 293, 718, 311, 51610, 51610], "temperature": 0.0, "avg_logprob": -0.09567935313653508, "compression_ratio": 1.7923076923076924, "no_speech_prob": 7.071581876516575e-06}, {"id": 689, "seek": 363240, "start": 3636.64, "end": 3641.28, "text": " are roughly equal so what that typically means is that our network is very tiny", "tokens": [50364, 437, 311, 1219, 833, 69, 2414, 570, 264, 3097, 4470, 293, 264, 1905, 420, 1500, 15352, 50576, 50576, 366, 9810, 2681, 370, 437, 300, 5850, 1355, 307, 300, 527, 3209, 307, 588, 5870, 50808, 50808, 588, 1359, 293, 321, 2066, 281, 652, 3389, 13797, 538, 21589, 493, 51060, 51060, 264, 2744, 295, 341, 18161, 2533, 370, 718, 311, 360, 300, 586, 370, 718, 311, 808, 670, 510, 293, 51240, 51240, 718, 311, 3488, 264, 2744, 365, 264, 18161, 2533, 264, 12889, 636, 281, 360, 341, 307, 321, 393, 51458, 51458, 808, 510, 281, 264, 7633, 4583, 597, 4362, 307, 257, 3262, 22027, 293, 718, 311, 51610, 51610], "temperature": 0.0, "avg_logprob": -0.09567935313653508, "compression_ratio": 1.7923076923076924, "no_speech_prob": 7.071581876516575e-06}, {"id": 690, "seek": 363240, "start": 3641.28, "end": 3646.32, "text": " very small and we expect to make performance improvements by scaling up", "tokens": [50364, 437, 311, 1219, 833, 69, 2414, 570, 264, 3097, 4470, 293, 264, 1905, 420, 1500, 15352, 50576, 50576, 366, 9810, 2681, 370, 437, 300, 5850, 1355, 307, 300, 527, 3209, 307, 588, 5870, 50808, 50808, 588, 1359, 293, 321, 2066, 281, 652, 3389, 13797, 538, 21589, 493, 51060, 51060, 264, 2744, 295, 341, 18161, 2533, 370, 718, 311, 360, 300, 586, 370, 718, 311, 808, 670, 510, 293, 51240, 51240, 718, 311, 3488, 264, 2744, 365, 264, 18161, 2533, 264, 12889, 636, 281, 360, 341, 307, 321, 393, 51458, 51458, 808, 510, 281, 264, 7633, 4583, 597, 4362, 307, 257, 3262, 22027, 293, 718, 311, 51610, 51610], "temperature": 0.0, "avg_logprob": -0.09567935313653508, "compression_ratio": 1.7923076923076924, "no_speech_prob": 7.071581876516575e-06}, {"id": 691, "seek": 363240, "start": 3646.32, "end": 3649.92, "text": " the size of this neural net so let's do that now so let's come over here and", "tokens": [50364, 437, 311, 1219, 833, 69, 2414, 570, 264, 3097, 4470, 293, 264, 1905, 420, 1500, 15352, 50576, 50576, 366, 9810, 2681, 370, 437, 300, 5850, 1355, 307, 300, 527, 3209, 307, 588, 5870, 50808, 50808, 588, 1359, 293, 321, 2066, 281, 652, 3389, 13797, 538, 21589, 493, 51060, 51060, 264, 2744, 295, 341, 18161, 2533, 370, 718, 311, 360, 300, 586, 370, 718, 311, 808, 670, 510, 293, 51240, 51240, 718, 311, 3488, 264, 2744, 365, 264, 18161, 2533, 264, 12889, 636, 281, 360, 341, 307, 321, 393, 51458, 51458, 808, 510, 281, 264, 7633, 4583, 597, 4362, 307, 257, 3262, 22027, 293, 718, 311, 51610, 51610], "temperature": 0.0, "avg_logprob": -0.09567935313653508, "compression_ratio": 1.7923076923076924, "no_speech_prob": 7.071581876516575e-06}, {"id": 692, "seek": 363240, "start": 3649.92, "end": 3654.28, "text": " let's increase the size with the neural net the easiest way to do this is we can", "tokens": [50364, 437, 311, 1219, 833, 69, 2414, 570, 264, 3097, 4470, 293, 264, 1905, 420, 1500, 15352, 50576, 50576, 366, 9810, 2681, 370, 437, 300, 5850, 1355, 307, 300, 527, 3209, 307, 588, 5870, 50808, 50808, 588, 1359, 293, 321, 2066, 281, 652, 3389, 13797, 538, 21589, 493, 51060, 51060, 264, 2744, 295, 341, 18161, 2533, 370, 718, 311, 360, 300, 586, 370, 718, 311, 808, 670, 510, 293, 51240, 51240, 718, 311, 3488, 264, 2744, 365, 264, 18161, 2533, 264, 12889, 636, 281, 360, 341, 307, 321, 393, 51458, 51458, 808, 510, 281, 264, 7633, 4583, 597, 4362, 307, 257, 3262, 22027, 293, 718, 311, 51610, 51610], "temperature": 0.0, "avg_logprob": -0.09567935313653508, "compression_ratio": 1.7923076923076924, "no_speech_prob": 7.071581876516575e-06}, {"id": 693, "seek": 363240, "start": 3654.28, "end": 3657.32, "text": " come here to the hidden layer which currently is a hundred neurons and let's", "tokens": [50364, 437, 311, 1219, 833, 69, 2414, 570, 264, 3097, 4470, 293, 264, 1905, 420, 1500, 15352, 50576, 50576, 366, 9810, 2681, 370, 437, 300, 5850, 1355, 307, 300, 527, 3209, 307, 588, 5870, 50808, 50808, 588, 1359, 293, 321, 2066, 281, 652, 3389, 13797, 538, 21589, 493, 51060, 51060, 264, 2744, 295, 341, 18161, 2533, 370, 718, 311, 360, 300, 586, 370, 718, 311, 808, 670, 510, 293, 51240, 51240, 718, 311, 3488, 264, 2744, 365, 264, 18161, 2533, 264, 12889, 636, 281, 360, 341, 307, 321, 393, 51458, 51458, 808, 510, 281, 264, 7633, 4583, 597, 4362, 307, 257, 3262, 22027, 293, 718, 311, 51610, 51610], "temperature": 0.0, "avg_logprob": -0.09567935313653508, "compression_ratio": 1.7923076923076924, "no_speech_prob": 7.071581876516575e-06}, {"id": 694, "seek": 365732, "start": 3657.32, "end": 3662.92, "text": " just bump this up so let's do 300 neurons and then this is also 300 biases", "tokens": [50364, 445, 9961, 341, 493, 370, 718, 311, 360, 6641, 22027, 293, 550, 341, 307, 611, 6641, 32152, 50644, 50644, 293, 510, 321, 362, 6641, 15743, 666, 264, 2572, 4583, 370, 718, 311, 5883, 1125, 527, 51000, 51000, 18161, 2533, 321, 586, 362, 1266, 11, 1360, 1266, 11, 1360, 9834, 2602, 295, 805, 11, 1360, 9834, 51204, 51204, 293, 550, 321, 434, 406, 1228, 341, 293, 550, 510, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 51504, 51504], "temperature": 0.0, "avg_logprob": -0.10624302887335056, "compression_ratio": 1.7062146892655368, "no_speech_prob": 3.726493332578684e-06}, {"id": 695, "seek": 365732, "start": 3662.92, "end": 3670.04, "text": " and here we have 300 inputs into the final layer so let's initialize our", "tokens": [50364, 445, 9961, 341, 493, 370, 718, 311, 360, 6641, 22027, 293, 550, 341, 307, 611, 6641, 32152, 50644, 50644, 293, 510, 321, 362, 6641, 15743, 666, 264, 2572, 4583, 370, 718, 311, 5883, 1125, 527, 51000, 51000, 18161, 2533, 321, 586, 362, 1266, 11, 1360, 1266, 11, 1360, 9834, 2602, 295, 805, 11, 1360, 9834, 51204, 51204, 293, 550, 321, 434, 406, 1228, 341, 293, 550, 510, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 51504, 51504], "temperature": 0.0, "avg_logprob": -0.10624302887335056, "compression_ratio": 1.7062146892655368, "no_speech_prob": 3.726493332578684e-06}, {"id": 696, "seek": 365732, "start": 3670.04, "end": 3674.1200000000003, "text": " neural net we now have 10,000 10,000 parameters instead of 3,000 parameters", "tokens": [50364, 445, 9961, 341, 493, 370, 718, 311, 360, 6641, 22027, 293, 550, 341, 307, 611, 6641, 32152, 50644, 50644, 293, 510, 321, 362, 6641, 15743, 666, 264, 2572, 4583, 370, 718, 311, 5883, 1125, 527, 51000, 51000, 18161, 2533, 321, 586, 362, 1266, 11, 1360, 1266, 11, 1360, 9834, 2602, 295, 805, 11, 1360, 9834, 51204, 51204, 293, 550, 321, 434, 406, 1228, 341, 293, 550, 510, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 51504, 51504], "temperature": 0.0, "avg_logprob": -0.10624302887335056, "compression_ratio": 1.7062146892655368, "no_speech_prob": 3.726493332578684e-06}, {"id": 697, "seek": 365732, "start": 3674.1200000000003, "end": 3680.1200000000003, "text": " and then we're not using this and then here what I'd like to do is I'd like to", "tokens": [50364, 445, 9961, 341, 493, 370, 718, 311, 360, 6641, 22027, 293, 550, 341, 307, 611, 6641, 32152, 50644, 50644, 293, 510, 321, 362, 6641, 15743, 666, 264, 2572, 4583, 370, 718, 311, 5883, 1125, 527, 51000, 51000, 18161, 2533, 321, 586, 362, 1266, 11, 1360, 1266, 11, 1360, 9834, 2602, 295, 805, 11, 1360, 9834, 51204, 51204, 293, 550, 321, 434, 406, 1228, 341, 293, 550, 510, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 51504, 51504], "temperature": 0.0, "avg_logprob": -0.10624302887335056, "compression_ratio": 1.7062146892655368, "no_speech_prob": 3.726493332578684e-06}, {"id": 698, "seek": 368012, "start": 3680.12, "end": 3690.56, "text": " actually keep track of that okay let's just do this let's keep stats again and", "tokens": [50364, 767, 1066, 2837, 295, 300, 1392, 718, 311, 445, 360, 341, 718, 311, 1066, 18152, 797, 293, 50886, 50886, 510, 562, 321, 434, 5145, 2837, 295, 264, 4470, 718, 311, 445, 611, 1066, 2837, 295, 264, 51260, 51260, 4439, 293, 718, 311, 445, 362, 257, 3313, 510, 293, 718, 311, 3847, 322, 2217, 11, 1360, 420, 2831, 584, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.10978561832058814, "compression_ratio": 1.686131386861314, "no_speech_prob": 2.078390389215201e-05}, {"id": 699, "seek": 368012, "start": 3690.56, "end": 3698.04, "text": " here when we're keeping track of the loss let's just also keep track of the", "tokens": [50364, 767, 1066, 2837, 295, 300, 1392, 718, 311, 445, 360, 341, 718, 311, 1066, 18152, 797, 293, 50886, 50886, 510, 562, 321, 434, 5145, 2837, 295, 264, 4470, 718, 311, 445, 611, 1066, 2837, 295, 264, 51260, 51260, 4439, 293, 718, 311, 445, 362, 257, 3313, 510, 293, 718, 311, 3847, 322, 2217, 11, 1360, 420, 2831, 584, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.10978561832058814, "compression_ratio": 1.686131386861314, "no_speech_prob": 2.078390389215201e-05}, {"id": 700, "seek": 368012, "start": 3698.04, "end": 3705.64, "text": " steps and let's just have a eye here and let's train on 30,000 or rather say", "tokens": [50364, 767, 1066, 2837, 295, 300, 1392, 718, 311, 445, 360, 341, 718, 311, 1066, 18152, 797, 293, 50886, 50886, 510, 562, 321, 434, 5145, 2837, 295, 264, 4470, 718, 311, 445, 611, 1066, 2837, 295, 264, 51260, 51260, 4439, 293, 718, 311, 445, 362, 257, 3313, 510, 293, 718, 311, 3847, 322, 2217, 11, 1360, 420, 2831, 584, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.10978561832058814, "compression_ratio": 1.686131386861314, "no_speech_prob": 2.078390389215201e-05}, {"id": 701, "seek": 370564, "start": 3705.64, "end": 3714.12, "text": " okay let's try 30,000 and we are at point one and we should be able to run", "tokens": [50364, 1392, 718, 311, 853, 2217, 11, 1360, 293, 321, 366, 412, 935, 472, 293, 321, 820, 312, 1075, 281, 1190, 50788, 50788, 341, 293, 19719, 264, 18161, 2533, 293, 550, 510, 1936, 286, 528, 281, 499, 83, 13, 564, 310, 51100, 51100, 264, 4439, 1970, 264, 4470, 370, 613, 366, 264, 1783, 311, 293, 264, 398, 311, 293, 341, 307, 264, 4470, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.18123417189626984, "compression_ratio": 1.519736842105263, "no_speech_prob": 1.1842836102005094e-05}, {"id": 702, "seek": 370564, "start": 3714.12, "end": 3720.3599999999997, "text": " this and optimize the neural net and then here basically I want to plt.plot", "tokens": [50364, 1392, 718, 311, 853, 2217, 11, 1360, 293, 321, 366, 412, 935, 472, 293, 321, 820, 312, 1075, 281, 1190, 50788, 50788, 341, 293, 19719, 264, 18161, 2533, 293, 550, 510, 1936, 286, 528, 281, 499, 83, 13, 564, 310, 51100, 51100, 264, 4439, 1970, 264, 4470, 370, 613, 366, 264, 1783, 311, 293, 264, 398, 311, 293, 341, 307, 264, 4470, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.18123417189626984, "compression_ratio": 1.519736842105263, "no_speech_prob": 1.1842836102005094e-05}, {"id": 703, "seek": 370564, "start": 3720.3599999999997, "end": 3733.96, "text": " the steps against the loss so these are the X's and the Y's and this is the loss", "tokens": [50364, 1392, 718, 311, 853, 2217, 11, 1360, 293, 321, 366, 412, 935, 472, 293, 321, 820, 312, 1075, 281, 1190, 50788, 50788, 341, 293, 19719, 264, 18161, 2533, 293, 550, 510, 1936, 286, 528, 281, 499, 83, 13, 564, 310, 51100, 51100, 264, 4439, 1970, 264, 4470, 370, 613, 366, 264, 1783, 311, 293, 264, 398, 311, 293, 341, 307, 264, 4470, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.18123417189626984, "compression_ratio": 1.519736842105263, "no_speech_prob": 1.1842836102005094e-05}, {"id": 704, "seek": 373396, "start": 3733.96, "end": 3738.08, "text": " function and how it's being optimized now you see that there's quite a bit of", "tokens": [50364, 2445, 293, 577, 309, 311, 885, 26941, 586, 291, 536, 300, 456, 311, 1596, 257, 857, 295, 50570, 50570, 14855, 281, 341, 293, 300, 311, 570, 321, 366, 40425, 670, 613, 8382, 15245, 279, 50742, 50742, 293, 264, 867, 15245, 279, 1884, 257, 707, 857, 295, 5658, 294, 341, 689, 366, 321, 294, 264, 51016, 51016, 1905, 992, 321, 366, 412, 568, 13, 20, 370, 321, 920, 2378, 380, 26941, 341, 18161, 2533, 588, 731, 293, 51284, 51284, 300, 311, 1391, 570, 321, 652, 309, 3801, 309, 1062, 747, 2854, 337, 341, 51416, 51416, 18161, 2533, 281, 41881, 293, 370, 718, 311, 2354, 3097, 1338, 718, 311, 445, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.1375965816634042, "compression_ratio": 1.8393574297188755, "no_speech_prob": 9.223287634085864e-06}, {"id": 705, "seek": 373396, "start": 3738.08, "end": 3741.52, "text": " thickness to this and that's because we are optimizing over these mini batches", "tokens": [50364, 2445, 293, 577, 309, 311, 885, 26941, 586, 291, 536, 300, 456, 311, 1596, 257, 857, 295, 50570, 50570, 14855, 281, 341, 293, 300, 311, 570, 321, 366, 40425, 670, 613, 8382, 15245, 279, 50742, 50742, 293, 264, 867, 15245, 279, 1884, 257, 707, 857, 295, 5658, 294, 341, 689, 366, 321, 294, 264, 51016, 51016, 1905, 992, 321, 366, 412, 568, 13, 20, 370, 321, 920, 2378, 380, 26941, 341, 18161, 2533, 588, 731, 293, 51284, 51284, 300, 311, 1391, 570, 321, 652, 309, 3801, 309, 1062, 747, 2854, 337, 341, 51416, 51416, 18161, 2533, 281, 41881, 293, 370, 718, 311, 2354, 3097, 1338, 718, 311, 445, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.1375965816634042, "compression_ratio": 1.8393574297188755, "no_speech_prob": 9.223287634085864e-06}, {"id": 706, "seek": 373396, "start": 3741.52, "end": 3747.0, "text": " and the many batches create a little bit of noise in this where are we in the", "tokens": [50364, 2445, 293, 577, 309, 311, 885, 26941, 586, 291, 536, 300, 456, 311, 1596, 257, 857, 295, 50570, 50570, 14855, 281, 341, 293, 300, 311, 570, 321, 366, 40425, 670, 613, 8382, 15245, 279, 50742, 50742, 293, 264, 867, 15245, 279, 1884, 257, 707, 857, 295, 5658, 294, 341, 689, 366, 321, 294, 264, 51016, 51016, 1905, 992, 321, 366, 412, 568, 13, 20, 370, 321, 920, 2378, 380, 26941, 341, 18161, 2533, 588, 731, 293, 51284, 51284, 300, 311, 1391, 570, 321, 652, 309, 3801, 309, 1062, 747, 2854, 337, 341, 51416, 51416, 18161, 2533, 281, 41881, 293, 370, 718, 311, 2354, 3097, 1338, 718, 311, 445, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.1375965816634042, "compression_ratio": 1.8393574297188755, "no_speech_prob": 9.223287634085864e-06}, {"id": 707, "seek": 373396, "start": 3747.0, "end": 3752.36, "text": " dev set we are at 2.5 so we still haven't optimized this neural net very well and", "tokens": [50364, 2445, 293, 577, 309, 311, 885, 26941, 586, 291, 536, 300, 456, 311, 1596, 257, 857, 295, 50570, 50570, 14855, 281, 341, 293, 300, 311, 570, 321, 366, 40425, 670, 613, 8382, 15245, 279, 50742, 50742, 293, 264, 867, 15245, 279, 1884, 257, 707, 857, 295, 5658, 294, 341, 689, 366, 321, 294, 264, 51016, 51016, 1905, 992, 321, 366, 412, 568, 13, 20, 370, 321, 920, 2378, 380, 26941, 341, 18161, 2533, 588, 731, 293, 51284, 51284, 300, 311, 1391, 570, 321, 652, 309, 3801, 309, 1062, 747, 2854, 337, 341, 51416, 51416, 18161, 2533, 281, 41881, 293, 370, 718, 311, 2354, 3097, 1338, 718, 311, 445, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.1375965816634042, "compression_ratio": 1.8393574297188755, "no_speech_prob": 9.223287634085864e-06}, {"id": 708, "seek": 373396, "start": 3752.36, "end": 3755.0, "text": " that's probably because we make it bigger it might take longer for this", "tokens": [50364, 2445, 293, 577, 309, 311, 885, 26941, 586, 291, 536, 300, 456, 311, 1596, 257, 857, 295, 50570, 50570, 14855, 281, 341, 293, 300, 311, 570, 321, 366, 40425, 670, 613, 8382, 15245, 279, 50742, 50742, 293, 264, 867, 15245, 279, 1884, 257, 707, 857, 295, 5658, 294, 341, 689, 366, 321, 294, 264, 51016, 51016, 1905, 992, 321, 366, 412, 568, 13, 20, 370, 321, 920, 2378, 380, 26941, 341, 18161, 2533, 588, 731, 293, 51284, 51284, 300, 311, 1391, 570, 321, 652, 309, 3801, 309, 1062, 747, 2854, 337, 341, 51416, 51416, 18161, 2533, 281, 41881, 293, 370, 718, 311, 2354, 3097, 1338, 718, 311, 445, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.1375965816634042, "compression_ratio": 1.8393574297188755, "no_speech_prob": 9.223287634085864e-06}, {"id": 709, "seek": 373396, "start": 3755.0, "end": 3763.48, "text": " neural net to converge and so let's continue training yeah let's just", "tokens": [50364, 2445, 293, 577, 309, 311, 885, 26941, 586, 291, 536, 300, 456, 311, 1596, 257, 857, 295, 50570, 50570, 14855, 281, 341, 293, 300, 311, 570, 321, 366, 40425, 670, 613, 8382, 15245, 279, 50742, 50742, 293, 264, 867, 15245, 279, 1884, 257, 707, 857, 295, 5658, 294, 341, 689, 366, 321, 294, 264, 51016, 51016, 1905, 992, 321, 366, 412, 568, 13, 20, 370, 321, 920, 2378, 380, 26941, 341, 18161, 2533, 588, 731, 293, 51284, 51284, 300, 311, 1391, 570, 321, 652, 309, 3801, 309, 1062, 747, 2854, 337, 341, 51416, 51416, 18161, 2533, 281, 41881, 293, 370, 718, 311, 2354, 3097, 1338, 718, 311, 445, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.1375965816634042, "compression_ratio": 1.8393574297188755, "no_speech_prob": 9.223287634085864e-06}, {"id": 710, "seek": 376348, "start": 3763.48, "end": 3770.52, "text": " continue training one possibility is that the batch size is so low that we", "tokens": [50364, 2354, 3097, 472, 7959, 307, 300, 264, 15245, 2744, 307, 370, 2295, 300, 321, 50716, 50716, 445, 362, 636, 886, 709, 5658, 294, 264, 3097, 293, 321, 815, 528, 281, 3488, 264, 50890, 50890, 15245, 2744, 370, 300, 321, 362, 257, 857, 544, 3006, 16235, 293, 321, 434, 406, 739, 11077, 51108, 51108, 886, 709, 293, 321, 393, 767, 411, 19719, 544, 6108, 1392, 341, 486, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.0764829226902553, "compression_ratio": 1.696629213483146, "no_speech_prob": 3.321191616123542e-05}, {"id": 711, "seek": 376348, "start": 3770.52, "end": 3774.0, "text": " just have way too much noise in the training and we may want to increase the", "tokens": [50364, 2354, 3097, 472, 7959, 307, 300, 264, 15245, 2744, 307, 370, 2295, 300, 321, 50716, 50716, 445, 362, 636, 886, 709, 5658, 294, 264, 3097, 293, 321, 815, 528, 281, 3488, 264, 50890, 50890, 15245, 2744, 370, 300, 321, 362, 257, 857, 544, 3006, 16235, 293, 321, 434, 406, 739, 11077, 51108, 51108, 886, 709, 293, 321, 393, 767, 411, 19719, 544, 6108, 1392, 341, 486, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.0764829226902553, "compression_ratio": 1.696629213483146, "no_speech_prob": 3.321191616123542e-05}, {"id": 712, "seek": 376348, "start": 3774.0, "end": 3778.36, "text": " batch size so that we have a bit more correct gradient and we're not thrashing", "tokens": [50364, 2354, 3097, 472, 7959, 307, 300, 264, 15245, 2744, 307, 370, 2295, 300, 321, 50716, 50716, 445, 362, 636, 886, 709, 5658, 294, 264, 3097, 293, 321, 815, 528, 281, 3488, 264, 50890, 50890, 15245, 2744, 370, 300, 321, 362, 257, 857, 544, 3006, 16235, 293, 321, 434, 406, 739, 11077, 51108, 51108, 886, 709, 293, 321, 393, 767, 411, 19719, 544, 6108, 1392, 341, 486, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.0764829226902553, "compression_ratio": 1.696629213483146, "no_speech_prob": 3.321191616123542e-05}, {"id": 713, "seek": 376348, "start": 3778.36, "end": 3789.28, "text": " too much and we can actually like optimize more properly okay this will", "tokens": [50364, 2354, 3097, 472, 7959, 307, 300, 264, 15245, 2744, 307, 370, 2295, 300, 321, 50716, 50716, 445, 362, 636, 886, 709, 5658, 294, 264, 3097, 293, 321, 815, 528, 281, 3488, 264, 50890, 50890, 15245, 2744, 370, 300, 321, 362, 257, 857, 544, 3006, 16235, 293, 321, 434, 406, 739, 11077, 51108, 51108, 886, 709, 293, 321, 393, 767, 411, 19719, 544, 6108, 1392, 341, 486, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.0764829226902553, "compression_ratio": 1.696629213483146, "no_speech_prob": 3.321191616123542e-05}, {"id": 714, "seek": 378928, "start": 3789.28, "end": 3794.1600000000003, "text": " now become meaningless because we've reinitialized these so yeah this looks", "tokens": [50364, 586, 1813, 33232, 570, 321, 600, 6561, 270, 831, 1602, 613, 370, 1338, 341, 1542, 50608, 50608, 406, 32798, 558, 586, 457, 264, 1391, 307, 411, 257, 5870, 10444, 457, 309, 311, 370, 50824, 50824, 1152, 281, 980, 718, 311, 352, 797, 568, 13, 17602, 718, 311, 853, 281, 11514, 264, 2539, 3314, 538, 51268, 51268, 5952, 568, 51394], "temperature": 0.0, "avg_logprob": -0.14142969788097945, "compression_ratio": 1.5031446540880504, "no_speech_prob": 1.8630496924743056e-05}, {"id": 715, "seek": 378928, "start": 3794.1600000000003, "end": 3798.48, "text": " not pleasing right now but the probably is like a tiny improvement but it's so", "tokens": [50364, 586, 1813, 33232, 570, 321, 600, 6561, 270, 831, 1602, 613, 370, 1338, 341, 1542, 50608, 50608, 406, 32798, 558, 586, 457, 264, 1391, 307, 411, 257, 5870, 10444, 457, 309, 311, 370, 50824, 50824, 1152, 281, 980, 718, 311, 352, 797, 568, 13, 17602, 718, 311, 853, 281, 11514, 264, 2539, 3314, 538, 51268, 51268, 5952, 568, 51394], "temperature": 0.0, "avg_logprob": -0.14142969788097945, "compression_ratio": 1.5031446540880504, "no_speech_prob": 1.8630496924743056e-05}, {"id": 716, "seek": 378928, "start": 3798.48, "end": 3807.36, "text": " hard to tell let's go again 2.52 let's try to decrease the learning rate by", "tokens": [50364, 586, 1813, 33232, 570, 321, 600, 6561, 270, 831, 1602, 613, 370, 1338, 341, 1542, 50608, 50608, 406, 32798, 558, 586, 457, 264, 1391, 307, 411, 257, 5870, 10444, 457, 309, 311, 370, 50824, 50824, 1152, 281, 980, 718, 311, 352, 797, 568, 13, 17602, 718, 311, 853, 281, 11514, 264, 2539, 3314, 538, 51268, 51268, 5952, 568, 51394], "temperature": 0.0, "avg_logprob": -0.14142969788097945, "compression_ratio": 1.5031446540880504, "no_speech_prob": 1.8630496924743056e-05}, {"id": 717, "seek": 380736, "start": 3807.36, "end": 3835.2000000000003, "text": " factor 2 okay we're at 2.32 let's continue training", "tokens": [50364, 5952, 568, 1392, 321, 434, 412, 568, 13, 11440, 718, 311, 2354, 3097, 51756], "temperature": 0.0, "avg_logprob": -0.29029902815818787, "compression_ratio": 0.864406779661017, "no_speech_prob": 2.3185106329037808e-05}, {"id": 718, "seek": 383736, "start": 3837.36, "end": 3849.1200000000003, "text": " we basically expect to see a lower loss than what we had before because now we", "tokens": [50364, 321, 1936, 2066, 281, 536, 257, 3126, 4470, 813, 437, 321, 632, 949, 570, 586, 321, 50952, 50952, 362, 257, 709, 709, 3801, 2316, 293, 321, 645, 833, 15669, 370, 321, 1116, 2066, 300, 51142, 51142, 5662, 264, 2744, 295, 264, 2316, 820, 854, 264, 18161, 2533, 568, 13, 11440, 1392, 370, 300, 311, 51384, 51384, 406, 2737, 886, 731, 586, 472, 661, 3136, 307, 300, 754, 1673, 321, 600, 1027, 51590, 51590, 264, 1266, 12, 39, 4583, 510, 420, 264, 7633, 4583, 709, 709, 3801, 309, 727, 312, 300, 264, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.15651354538766962, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.002251481870189309}, {"id": 719, "seek": 383736, "start": 3849.1200000000003, "end": 3852.92, "text": " have a much much bigger model and we were under fitting so we'd expect that", "tokens": [50364, 321, 1936, 2066, 281, 536, 257, 3126, 4470, 813, 437, 321, 632, 949, 570, 586, 321, 50952, 50952, 362, 257, 709, 709, 3801, 2316, 293, 321, 645, 833, 15669, 370, 321, 1116, 2066, 300, 51142, 51142, 5662, 264, 2744, 295, 264, 2316, 820, 854, 264, 18161, 2533, 568, 13, 11440, 1392, 370, 300, 311, 51384, 51384, 406, 2737, 886, 731, 586, 472, 661, 3136, 307, 300, 754, 1673, 321, 600, 1027, 51590, 51590, 264, 1266, 12, 39, 4583, 510, 420, 264, 7633, 4583, 709, 709, 3801, 309, 727, 312, 300, 264, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.15651354538766962, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.002251481870189309}, {"id": 720, "seek": 383736, "start": 3852.92, "end": 3857.76, "text": " increasing the size of the model should help the neural net 2.32 okay so that's", "tokens": [50364, 321, 1936, 2066, 281, 536, 257, 3126, 4470, 813, 437, 321, 632, 949, 570, 586, 321, 50952, 50952, 362, 257, 709, 709, 3801, 2316, 293, 321, 645, 833, 15669, 370, 321, 1116, 2066, 300, 51142, 51142, 5662, 264, 2744, 295, 264, 2316, 820, 854, 264, 18161, 2533, 568, 13, 11440, 1392, 370, 300, 311, 51384, 51384, 406, 2737, 886, 731, 586, 472, 661, 3136, 307, 300, 754, 1673, 321, 600, 1027, 51590, 51590, 264, 1266, 12, 39, 4583, 510, 420, 264, 7633, 4583, 709, 709, 3801, 309, 727, 312, 300, 264, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.15651354538766962, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.002251481870189309}, {"id": 721, "seek": 383736, "start": 3857.76, "end": 3861.88, "text": " not happening too well now one other concern is that even though we've made", "tokens": [50364, 321, 1936, 2066, 281, 536, 257, 3126, 4470, 813, 437, 321, 632, 949, 570, 586, 321, 50952, 50952, 362, 257, 709, 709, 3801, 2316, 293, 321, 645, 833, 15669, 370, 321, 1116, 2066, 300, 51142, 51142, 5662, 264, 2744, 295, 264, 2316, 820, 854, 264, 18161, 2533, 568, 13, 11440, 1392, 370, 300, 311, 51384, 51384, 406, 2737, 886, 731, 586, 472, 661, 3136, 307, 300, 754, 1673, 321, 600, 1027, 51590, 51590, 264, 1266, 12, 39, 4583, 510, 420, 264, 7633, 4583, 709, 709, 3801, 309, 727, 312, 300, 264, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.15651354538766962, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.002251481870189309}, {"id": 722, "seek": 383736, "start": 3861.88, "end": 3866.2400000000002, "text": " the 10-H layer here or the hidden layer much much bigger it could be that the", "tokens": [50364, 321, 1936, 2066, 281, 536, 257, 3126, 4470, 813, 437, 321, 632, 949, 570, 586, 321, 50952, 50952, 362, 257, 709, 709, 3801, 2316, 293, 321, 645, 833, 15669, 370, 321, 1116, 2066, 300, 51142, 51142, 5662, 264, 2744, 295, 264, 2316, 820, 854, 264, 18161, 2533, 568, 13, 11440, 1392, 370, 300, 311, 51384, 51384, 406, 2737, 886, 731, 586, 472, 661, 3136, 307, 300, 754, 1673, 321, 600, 1027, 51590, 51590, 264, 1266, 12, 39, 4583, 510, 420, 264, 7633, 4583, 709, 709, 3801, 309, 727, 312, 300, 264, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.15651354538766962, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.002251481870189309}, {"id": 723, "seek": 386624, "start": 3866.24, "end": 3869.8799999999997, "text": " bottleneck of the network right now are these embeddings that are two-dimensional", "tokens": [50364, 44641, 547, 295, 264, 3209, 558, 586, 366, 613, 12240, 29432, 300, 366, 732, 12, 18759, 50546, 50546, 309, 393, 312, 300, 321, 434, 445, 941, 335, 2810, 636, 886, 867, 4342, 666, 445, 732, 50718, 50718, 12819, 293, 264, 18161, 2533, 307, 406, 1075, 281, 534, 764, 300, 1901, 50906, 50906, 8659, 293, 300, 300, 307, 1333, 295, 411, 264, 44641, 547, 281, 527, 9590, 51074, 51074, 3389, 1392, 568, 13, 9356, 370, 445, 538, 23223, 264, 2539, 3314, 286, 390, 1075, 51358, 51358, 281, 652, 1596, 257, 857, 295, 4205, 718, 311, 1190, 341, 472, 544, 565, 293, 550, 13059, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.10185430634696528, "compression_ratio": 1.702290076335878, "no_speech_prob": 1.4509600077872165e-05}, {"id": 724, "seek": 386624, "start": 3869.8799999999997, "end": 3873.3199999999997, "text": " it can be that we're just cramming way too many characters into just two", "tokens": [50364, 44641, 547, 295, 264, 3209, 558, 586, 366, 613, 12240, 29432, 300, 366, 732, 12, 18759, 50546, 50546, 309, 393, 312, 300, 321, 434, 445, 941, 335, 2810, 636, 886, 867, 4342, 666, 445, 732, 50718, 50718, 12819, 293, 264, 18161, 2533, 307, 406, 1075, 281, 534, 764, 300, 1901, 50906, 50906, 8659, 293, 300, 300, 307, 1333, 295, 411, 264, 44641, 547, 281, 527, 9590, 51074, 51074, 3389, 1392, 568, 13, 9356, 370, 445, 538, 23223, 264, 2539, 3314, 286, 390, 1075, 51358, 51358, 281, 652, 1596, 257, 857, 295, 4205, 718, 311, 1190, 341, 472, 544, 565, 293, 550, 13059, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.10185430634696528, "compression_ratio": 1.702290076335878, "no_speech_prob": 1.4509600077872165e-05}, {"id": 725, "seek": 386624, "start": 3873.3199999999997, "end": 3877.08, "text": " dimensions and the neural net is not able to really use that space", "tokens": [50364, 44641, 547, 295, 264, 3209, 558, 586, 366, 613, 12240, 29432, 300, 366, 732, 12, 18759, 50546, 50546, 309, 393, 312, 300, 321, 434, 445, 941, 335, 2810, 636, 886, 867, 4342, 666, 445, 732, 50718, 50718, 12819, 293, 264, 18161, 2533, 307, 406, 1075, 281, 534, 764, 300, 1901, 50906, 50906, 8659, 293, 300, 300, 307, 1333, 295, 411, 264, 44641, 547, 281, 527, 9590, 51074, 51074, 3389, 1392, 568, 13, 9356, 370, 445, 538, 23223, 264, 2539, 3314, 286, 390, 1075, 51358, 51358, 281, 652, 1596, 257, 857, 295, 4205, 718, 311, 1190, 341, 472, 544, 565, 293, 550, 13059, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.10185430634696528, "compression_ratio": 1.702290076335878, "no_speech_prob": 1.4509600077872165e-05}, {"id": 726, "seek": 386624, "start": 3877.08, "end": 3880.4399999999996, "text": " effectively and that that is sort of like the bottleneck to our networks", "tokens": [50364, 44641, 547, 295, 264, 3209, 558, 586, 366, 613, 12240, 29432, 300, 366, 732, 12, 18759, 50546, 50546, 309, 393, 312, 300, 321, 434, 445, 941, 335, 2810, 636, 886, 867, 4342, 666, 445, 732, 50718, 50718, 12819, 293, 264, 18161, 2533, 307, 406, 1075, 281, 534, 764, 300, 1901, 50906, 50906, 8659, 293, 300, 300, 307, 1333, 295, 411, 264, 44641, 547, 281, 527, 9590, 51074, 51074, 3389, 1392, 568, 13, 9356, 370, 445, 538, 23223, 264, 2539, 3314, 286, 390, 1075, 51358, 51358, 281, 652, 1596, 257, 857, 295, 4205, 718, 311, 1190, 341, 472, 544, 565, 293, 550, 13059, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.10185430634696528, "compression_ratio": 1.702290076335878, "no_speech_prob": 1.4509600077872165e-05}, {"id": 727, "seek": 386624, "start": 3880.4399999999996, "end": 3886.12, "text": " performance okay 2.23 so just by decreasing the learning rate I was able", "tokens": [50364, 44641, 547, 295, 264, 3209, 558, 586, 366, 613, 12240, 29432, 300, 366, 732, 12, 18759, 50546, 50546, 309, 393, 312, 300, 321, 434, 445, 941, 335, 2810, 636, 886, 867, 4342, 666, 445, 732, 50718, 50718, 12819, 293, 264, 18161, 2533, 307, 406, 1075, 281, 534, 764, 300, 1901, 50906, 50906, 8659, 293, 300, 300, 307, 1333, 295, 411, 264, 44641, 547, 281, 527, 9590, 51074, 51074, 3389, 1392, 568, 13, 9356, 370, 445, 538, 23223, 264, 2539, 3314, 286, 390, 1075, 51358, 51358, 281, 652, 1596, 257, 857, 295, 4205, 718, 311, 1190, 341, 472, 544, 565, 293, 550, 13059, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.10185430634696528, "compression_ratio": 1.702290076335878, "no_speech_prob": 1.4509600077872165e-05}, {"id": 728, "seek": 386624, "start": 3886.12, "end": 3892.4799999999996, "text": " to make quite a bit of progress let's run this one more time and then evaluate", "tokens": [50364, 44641, 547, 295, 264, 3209, 558, 586, 366, 613, 12240, 29432, 300, 366, 732, 12, 18759, 50546, 50546, 309, 393, 312, 300, 321, 434, 445, 941, 335, 2810, 636, 886, 867, 4342, 666, 445, 732, 50718, 50718, 12819, 293, 264, 18161, 2533, 307, 406, 1075, 281, 534, 764, 300, 1901, 50906, 50906, 8659, 293, 300, 300, 307, 1333, 295, 411, 264, 44641, 547, 281, 527, 9590, 51074, 51074, 3389, 1392, 568, 13, 9356, 370, 445, 538, 23223, 264, 2539, 3314, 286, 390, 1075, 51358, 51358, 281, 652, 1596, 257, 857, 295, 4205, 718, 311, 1190, 341, 472, 544, 565, 293, 550, 13059, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.10185430634696528, "compression_ratio": 1.702290076335878, "no_speech_prob": 1.4509600077872165e-05}, {"id": 729, "seek": 389248, "start": 3892.48, "end": 3898.56, "text": " the training and the dev loss now one more thing after training that I'd like", "tokens": [50364, 264, 3097, 293, 264, 1905, 4470, 586, 472, 544, 551, 934, 3097, 300, 286, 1116, 411, 50668, 50668, 281, 360, 307, 286, 1116, 411, 281, 23273, 264, 12240, 3584, 18875, 337, 613, 4342, 51052, 51052, 949, 321, 4373, 493, 264, 12240, 3584, 2744, 490, 568, 570, 321, 1116, 411, 281, 652, 341, 51312, 51312, 44641, 547, 7263, 352, 1314, 457, 1564, 286, 652, 341, 5044, 813, 568, 321, 1582, 380, 312, 51528, 51528], "temperature": 0.0, "avg_logprob": -0.07491641998291015, "compression_ratio": 1.6906077348066297, "no_speech_prob": 4.63775404568878e-06}, {"id": 730, "seek": 389248, "start": 3898.56, "end": 3906.2400000000002, "text": " to do is I'd like to visualize the embedding vectors for these characters", "tokens": [50364, 264, 3097, 293, 264, 1905, 4470, 586, 472, 544, 551, 934, 3097, 300, 286, 1116, 411, 50668, 50668, 281, 360, 307, 286, 1116, 411, 281, 23273, 264, 12240, 3584, 18875, 337, 613, 4342, 51052, 51052, 949, 321, 4373, 493, 264, 12240, 3584, 2744, 490, 568, 570, 321, 1116, 411, 281, 652, 341, 51312, 51312, 44641, 547, 7263, 352, 1314, 457, 1564, 286, 652, 341, 5044, 813, 568, 321, 1582, 380, 312, 51528, 51528], "temperature": 0.0, "avg_logprob": -0.07491641998291015, "compression_ratio": 1.6906077348066297, "no_speech_prob": 4.63775404568878e-06}, {"id": 731, "seek": 389248, "start": 3906.2400000000002, "end": 3911.44, "text": " before we scale up the embedding size from 2 because we'd like to make this", "tokens": [50364, 264, 3097, 293, 264, 1905, 4470, 586, 472, 544, 551, 934, 3097, 300, 286, 1116, 411, 50668, 50668, 281, 360, 307, 286, 1116, 411, 281, 23273, 264, 12240, 3584, 18875, 337, 613, 4342, 51052, 51052, 949, 321, 4373, 493, 264, 12240, 3584, 2744, 490, 568, 570, 321, 1116, 411, 281, 652, 341, 51312, 51312, 44641, 547, 7263, 352, 1314, 457, 1564, 286, 652, 341, 5044, 813, 568, 321, 1582, 380, 312, 51528, 51528], "temperature": 0.0, "avg_logprob": -0.07491641998291015, "compression_ratio": 1.6906077348066297, "no_speech_prob": 4.63775404568878e-06}, {"id": 732, "seek": 389248, "start": 3911.44, "end": 3915.76, "text": " bottleneck potentially go away but once I make this greater than 2 we won't be", "tokens": [50364, 264, 3097, 293, 264, 1905, 4470, 586, 472, 544, 551, 934, 3097, 300, 286, 1116, 411, 50668, 50668, 281, 360, 307, 286, 1116, 411, 281, 23273, 264, 12240, 3584, 18875, 337, 613, 4342, 51052, 51052, 949, 321, 4373, 493, 264, 12240, 3584, 2744, 490, 568, 570, 321, 1116, 411, 281, 652, 341, 51312, 51312, 44641, 547, 7263, 352, 1314, 457, 1564, 286, 652, 341, 5044, 813, 568, 321, 1582, 380, 312, 51528, 51528], "temperature": 0.0, "avg_logprob": -0.07491641998291015, "compression_ratio": 1.6906077348066297, "no_speech_prob": 4.63775404568878e-06}, {"id": 733, "seek": 391576, "start": 3915.76, "end": 3923.32, "text": " able to visualize them so here look over at 2.23 and 2.24 so we're not improving", "tokens": [50364, 1075, 281, 23273, 552, 370, 510, 574, 670, 412, 568, 13, 9356, 293, 568, 13, 7911, 370, 321, 434, 406, 11470, 50742, 50742, 709, 544, 293, 1310, 264, 44641, 547, 586, 307, 264, 2517, 12240, 3584, 2744, 597, 307, 50926, 50926, 568, 370, 510, 286, 362, 257, 3840, 295, 3089, 300, 486, 1884, 257, 2573, 293, 550, 321, 434, 51162, 51162, 516, 281, 23273, 264, 12240, 29432, 300, 645, 8895, 538, 264, 18161, 2533, 322, 613, 51442, 51442, 4342, 570, 558, 586, 264, 12240, 3584, 2744, 307, 445, 568, 370, 321, 393, 51610, 51610, 23273, 439, 264, 4342, 365, 264, 1783, 293, 264, 398, 21056, 382, 264, 732, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.08792781829833984, "compression_ratio": 1.8306451612903225, "no_speech_prob": 9.36844662646763e-06}, {"id": 734, "seek": 391576, "start": 3923.32, "end": 3927.0, "text": " much more and maybe the bottleneck now is the character embedding size which is", "tokens": [50364, 1075, 281, 23273, 552, 370, 510, 574, 670, 412, 568, 13, 9356, 293, 568, 13, 7911, 370, 321, 434, 406, 11470, 50742, 50742, 709, 544, 293, 1310, 264, 44641, 547, 586, 307, 264, 2517, 12240, 3584, 2744, 597, 307, 50926, 50926, 568, 370, 510, 286, 362, 257, 3840, 295, 3089, 300, 486, 1884, 257, 2573, 293, 550, 321, 434, 51162, 51162, 516, 281, 23273, 264, 12240, 29432, 300, 645, 8895, 538, 264, 18161, 2533, 322, 613, 51442, 51442, 4342, 570, 558, 586, 264, 12240, 3584, 2744, 307, 445, 568, 370, 321, 393, 51610, 51610, 23273, 439, 264, 4342, 365, 264, 1783, 293, 264, 398, 21056, 382, 264, 732, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.08792781829833984, "compression_ratio": 1.8306451612903225, "no_speech_prob": 9.36844662646763e-06}, {"id": 735, "seek": 391576, "start": 3927.0, "end": 3931.7200000000003, "text": " 2 so here I have a bunch of code that will create a figure and then we're", "tokens": [50364, 1075, 281, 23273, 552, 370, 510, 574, 670, 412, 568, 13, 9356, 293, 568, 13, 7911, 370, 321, 434, 406, 11470, 50742, 50742, 709, 544, 293, 1310, 264, 44641, 547, 586, 307, 264, 2517, 12240, 3584, 2744, 597, 307, 50926, 50926, 568, 370, 510, 286, 362, 257, 3840, 295, 3089, 300, 486, 1884, 257, 2573, 293, 550, 321, 434, 51162, 51162, 516, 281, 23273, 264, 12240, 29432, 300, 645, 8895, 538, 264, 18161, 2533, 322, 613, 51442, 51442, 4342, 570, 558, 586, 264, 12240, 3584, 2744, 307, 445, 568, 370, 321, 393, 51610, 51610, 23273, 439, 264, 4342, 365, 264, 1783, 293, 264, 398, 21056, 382, 264, 732, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.08792781829833984, "compression_ratio": 1.8306451612903225, "no_speech_prob": 9.36844662646763e-06}, {"id": 736, "seek": 391576, "start": 3931.7200000000003, "end": 3937.32, "text": " going to visualize the embeddings that were trained by the neural net on these", "tokens": [50364, 1075, 281, 23273, 552, 370, 510, 574, 670, 412, 568, 13, 9356, 293, 568, 13, 7911, 370, 321, 434, 406, 11470, 50742, 50742, 709, 544, 293, 1310, 264, 44641, 547, 586, 307, 264, 2517, 12240, 3584, 2744, 597, 307, 50926, 50926, 568, 370, 510, 286, 362, 257, 3840, 295, 3089, 300, 486, 1884, 257, 2573, 293, 550, 321, 434, 51162, 51162, 516, 281, 23273, 264, 12240, 29432, 300, 645, 8895, 538, 264, 18161, 2533, 322, 613, 51442, 51442, 4342, 570, 558, 586, 264, 12240, 3584, 2744, 307, 445, 568, 370, 321, 393, 51610, 51610, 23273, 439, 264, 4342, 365, 264, 1783, 293, 264, 398, 21056, 382, 264, 732, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.08792781829833984, "compression_ratio": 1.8306451612903225, "no_speech_prob": 9.36844662646763e-06}, {"id": 737, "seek": 391576, "start": 3937.32, "end": 3940.6800000000003, "text": " characters because right now the embedding size is just 2 so we can", "tokens": [50364, 1075, 281, 23273, 552, 370, 510, 574, 670, 412, 568, 13, 9356, 293, 568, 13, 7911, 370, 321, 434, 406, 11470, 50742, 50742, 709, 544, 293, 1310, 264, 44641, 547, 586, 307, 264, 2517, 12240, 3584, 2744, 597, 307, 50926, 50926, 568, 370, 510, 286, 362, 257, 3840, 295, 3089, 300, 486, 1884, 257, 2573, 293, 550, 321, 434, 51162, 51162, 516, 281, 23273, 264, 12240, 29432, 300, 645, 8895, 538, 264, 18161, 2533, 322, 613, 51442, 51442, 4342, 570, 558, 586, 264, 12240, 3584, 2744, 307, 445, 568, 370, 321, 393, 51610, 51610, 23273, 439, 264, 4342, 365, 264, 1783, 293, 264, 398, 21056, 382, 264, 732, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.08792781829833984, "compression_ratio": 1.8306451612903225, "no_speech_prob": 9.36844662646763e-06}, {"id": 738, "seek": 391576, "start": 3940.6800000000003, "end": 3943.96, "text": " visualize all the characters with the X and the Y coordinates as the two", "tokens": [50364, 1075, 281, 23273, 552, 370, 510, 574, 670, 412, 568, 13, 9356, 293, 568, 13, 7911, 370, 321, 434, 406, 11470, 50742, 50742, 709, 544, 293, 1310, 264, 44641, 547, 586, 307, 264, 2517, 12240, 3584, 2744, 597, 307, 50926, 50926, 568, 370, 510, 286, 362, 257, 3840, 295, 3089, 300, 486, 1884, 257, 2573, 293, 550, 321, 434, 51162, 51162, 516, 281, 23273, 264, 12240, 29432, 300, 645, 8895, 538, 264, 18161, 2533, 322, 613, 51442, 51442, 4342, 570, 558, 586, 264, 12240, 3584, 2744, 307, 445, 568, 370, 321, 393, 51610, 51610, 23273, 439, 264, 4342, 365, 264, 1783, 293, 264, 398, 21056, 382, 264, 732, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.08792781829833984, "compression_ratio": 1.8306451612903225, "no_speech_prob": 9.36844662646763e-06}, {"id": 739, "seek": 394396, "start": 3943.96, "end": 3949.36, "text": " embedding locations for each of these characters and so here are the X", "tokens": [50364, 12240, 3584, 9253, 337, 1184, 295, 613, 4342, 293, 370, 510, 366, 264, 1783, 50634, 50634, 21056, 293, 264, 398, 21056, 597, 366, 264, 13766, 295, 383, 293, 550, 337, 1184, 50850, 50850, 472, 286, 611, 4090, 264, 2487, 295, 264, 707, 2517, 370, 510, 437, 321, 536, 307, 51144, 51144, 767, 733, 295, 1880, 264, 3209, 575, 1936, 3264, 281, 4994, 484, 51426, 51426, 264, 4342, 293, 13630, 552, 257, 707, 857, 370, 337, 1365, 291, 536, 577, 264, 51610, 51610], "temperature": 0.0, "avg_logprob": -0.08938910847618467, "compression_ratio": 1.813397129186603, "no_speech_prob": 5.682245955540566e-06}, {"id": 740, "seek": 394396, "start": 3949.36, "end": 3953.68, "text": " coordinates and the Y coordinates which are the columns of C and then for each", "tokens": [50364, 12240, 3584, 9253, 337, 1184, 295, 613, 4342, 293, 370, 510, 366, 264, 1783, 50634, 50634, 21056, 293, 264, 398, 21056, 597, 366, 264, 13766, 295, 383, 293, 550, 337, 1184, 50850, 50850, 472, 286, 611, 4090, 264, 2487, 295, 264, 707, 2517, 370, 510, 437, 321, 536, 307, 51144, 51144, 767, 733, 295, 1880, 264, 3209, 575, 1936, 3264, 281, 4994, 484, 51426, 51426, 264, 4342, 293, 13630, 552, 257, 707, 857, 370, 337, 1365, 291, 536, 577, 264, 51610, 51610], "temperature": 0.0, "avg_logprob": -0.08938910847618467, "compression_ratio": 1.813397129186603, "no_speech_prob": 5.682245955540566e-06}, {"id": 741, "seek": 394396, "start": 3953.68, "end": 3959.56, "text": " one I also include the text of the little character so here what we see is", "tokens": [50364, 12240, 3584, 9253, 337, 1184, 295, 613, 4342, 293, 370, 510, 366, 264, 1783, 50634, 50634, 21056, 293, 264, 398, 21056, 597, 366, 264, 13766, 295, 383, 293, 550, 337, 1184, 50850, 50850, 472, 286, 611, 4090, 264, 2487, 295, 264, 707, 2517, 370, 510, 437, 321, 536, 307, 51144, 51144, 767, 733, 295, 1880, 264, 3209, 575, 1936, 3264, 281, 4994, 484, 51426, 51426, 264, 4342, 293, 13630, 552, 257, 707, 857, 370, 337, 1365, 291, 536, 577, 264, 51610, 51610], "temperature": 0.0, "avg_logprob": -0.08938910847618467, "compression_ratio": 1.813397129186603, "no_speech_prob": 5.682245955540566e-06}, {"id": 742, "seek": 394396, "start": 3959.56, "end": 3965.2, "text": " actually kind of interesting the network has basically learned to separate out", "tokens": [50364, 12240, 3584, 9253, 337, 1184, 295, 613, 4342, 293, 370, 510, 366, 264, 1783, 50634, 50634, 21056, 293, 264, 398, 21056, 597, 366, 264, 13766, 295, 383, 293, 550, 337, 1184, 50850, 50850, 472, 286, 611, 4090, 264, 2487, 295, 264, 707, 2517, 370, 510, 437, 321, 536, 307, 51144, 51144, 767, 733, 295, 1880, 264, 3209, 575, 1936, 3264, 281, 4994, 484, 51426, 51426, 264, 4342, 293, 13630, 552, 257, 707, 857, 370, 337, 1365, 291, 536, 577, 264, 51610, 51610], "temperature": 0.0, "avg_logprob": -0.08938910847618467, "compression_ratio": 1.813397129186603, "no_speech_prob": 5.682245955540566e-06}, {"id": 743, "seek": 394396, "start": 3965.2, "end": 3968.88, "text": " the characters and cluster them a little bit so for example you see how the", "tokens": [50364, 12240, 3584, 9253, 337, 1184, 295, 613, 4342, 293, 370, 510, 366, 264, 1783, 50634, 50634, 21056, 293, 264, 398, 21056, 597, 366, 264, 13766, 295, 383, 293, 550, 337, 1184, 50850, 50850, 472, 286, 611, 4090, 264, 2487, 295, 264, 707, 2517, 370, 510, 437, 321, 536, 307, 51144, 51144, 767, 733, 295, 1880, 264, 3209, 575, 1936, 3264, 281, 4994, 484, 51426, 51426, 264, 4342, 293, 13630, 552, 257, 707, 857, 370, 337, 1365, 291, 536, 577, 264, 51610, 51610], "temperature": 0.0, "avg_logprob": -0.08938910847618467, "compression_ratio": 1.813397129186603, "no_speech_prob": 5.682245955540566e-06}, {"id": 744, "seek": 396888, "start": 3968.88, "end": 3974.32, "text": " vowels A E I O U are clustered up here so what that's telling us that is that", "tokens": [50364, 44972, 316, 462, 286, 422, 624, 366, 596, 38624, 493, 510, 370, 437, 300, 311, 3585, 505, 300, 307, 300, 50636, 50636, 264, 18161, 2533, 19566, 613, 382, 588, 2531, 558, 570, 562, 436, 3154, 50786, 50786, 666, 264, 18161, 2533, 264, 12240, 3584, 337, 439, 613, 4342, 307, 588, 2531, 293, 51032, 51032, 370, 264, 18161, 2533, 7309, 300, 436, 434, 588, 2531, 293, 733, 295, 411, 51172, 51172, 30358, 712, 498, 300, 1669, 2020, 550, 264, 264, 2793, 300, 366, 411, 534, 1400, 51494, 51494, 1314, 366, 337, 1365, 1249, 11, 1249, 307, 733, 295, 8668, 382, 364, 11183, 293, 1249, 575, 257, 588, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.12194984609430487, "compression_ratio": 1.8461538461538463, "no_speech_prob": 4.356787940196227e-06}, {"id": 745, "seek": 396888, "start": 3974.32, "end": 3977.32, "text": " the neural net treats these as very similar right because when they feed", "tokens": [50364, 44972, 316, 462, 286, 422, 624, 366, 596, 38624, 493, 510, 370, 437, 300, 311, 3585, 505, 300, 307, 300, 50636, 50636, 264, 18161, 2533, 19566, 613, 382, 588, 2531, 558, 570, 562, 436, 3154, 50786, 50786, 666, 264, 18161, 2533, 264, 12240, 3584, 337, 439, 613, 4342, 307, 588, 2531, 293, 51032, 51032, 370, 264, 18161, 2533, 7309, 300, 436, 434, 588, 2531, 293, 733, 295, 411, 51172, 51172, 30358, 712, 498, 300, 1669, 2020, 550, 264, 264, 2793, 300, 366, 411, 534, 1400, 51494, 51494, 1314, 366, 337, 1365, 1249, 11, 1249, 307, 733, 295, 8668, 382, 364, 11183, 293, 1249, 575, 257, 588, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.12194984609430487, "compression_ratio": 1.8461538461538463, "no_speech_prob": 4.356787940196227e-06}, {"id": 746, "seek": 396888, "start": 3977.32, "end": 3982.2400000000002, "text": " into the neural net the embedding for all these characters is very similar and", "tokens": [50364, 44972, 316, 462, 286, 422, 624, 366, 596, 38624, 493, 510, 370, 437, 300, 311, 3585, 505, 300, 307, 300, 50636, 50636, 264, 18161, 2533, 19566, 613, 382, 588, 2531, 558, 570, 562, 436, 3154, 50786, 50786, 666, 264, 18161, 2533, 264, 12240, 3584, 337, 439, 613, 4342, 307, 588, 2531, 293, 51032, 51032, 370, 264, 18161, 2533, 7309, 300, 436, 434, 588, 2531, 293, 733, 295, 411, 51172, 51172, 30358, 712, 498, 300, 1669, 2020, 550, 264, 264, 2793, 300, 366, 411, 534, 1400, 51494, 51494, 1314, 366, 337, 1365, 1249, 11, 1249, 307, 733, 295, 8668, 382, 364, 11183, 293, 1249, 575, 257, 588, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.12194984609430487, "compression_ratio": 1.8461538461538463, "no_speech_prob": 4.356787940196227e-06}, {"id": 747, "seek": 396888, "start": 3982.2400000000002, "end": 3985.04, "text": " so the neural net thinks that they're very similar and kind of like", "tokens": [50364, 44972, 316, 462, 286, 422, 624, 366, 596, 38624, 493, 510, 370, 437, 300, 311, 3585, 505, 300, 307, 300, 50636, 50636, 264, 18161, 2533, 19566, 613, 382, 588, 2531, 558, 570, 562, 436, 3154, 50786, 50786, 666, 264, 18161, 2533, 264, 12240, 3584, 337, 439, 613, 4342, 307, 588, 2531, 293, 51032, 51032, 370, 264, 18161, 2533, 7309, 300, 436, 434, 588, 2531, 293, 733, 295, 411, 51172, 51172, 30358, 712, 498, 300, 1669, 2020, 550, 264, 264, 2793, 300, 366, 411, 534, 1400, 51494, 51494, 1314, 366, 337, 1365, 1249, 11, 1249, 307, 733, 295, 8668, 382, 364, 11183, 293, 1249, 575, 257, 588, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.12194984609430487, "compression_ratio": 1.8461538461538463, "no_speech_prob": 4.356787940196227e-06}, {"id": 748, "seek": 396888, "start": 3985.04, "end": 3991.48, "text": " interchangeable if that makes sense then the the points that are like really far", "tokens": [50364, 44972, 316, 462, 286, 422, 624, 366, 596, 38624, 493, 510, 370, 437, 300, 311, 3585, 505, 300, 307, 300, 50636, 50636, 264, 18161, 2533, 19566, 613, 382, 588, 2531, 558, 570, 562, 436, 3154, 50786, 50786, 666, 264, 18161, 2533, 264, 12240, 3584, 337, 439, 613, 4342, 307, 588, 2531, 293, 51032, 51032, 370, 264, 18161, 2533, 7309, 300, 436, 434, 588, 2531, 293, 733, 295, 411, 51172, 51172, 30358, 712, 498, 300, 1669, 2020, 550, 264, 264, 2793, 300, 366, 411, 534, 1400, 51494, 51494, 1314, 366, 337, 1365, 1249, 11, 1249, 307, 733, 295, 8668, 382, 364, 11183, 293, 1249, 575, 257, 588, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.12194984609430487, "compression_ratio": 1.8461538461538463, "no_speech_prob": 4.356787940196227e-06}, {"id": 749, "seek": 396888, "start": 3991.48, "end": 3995.7200000000003, "text": " away are for example Q, Q is kind of treated as an exception and Q has a very", "tokens": [50364, 44972, 316, 462, 286, 422, 624, 366, 596, 38624, 493, 510, 370, 437, 300, 311, 3585, 505, 300, 307, 300, 50636, 50636, 264, 18161, 2533, 19566, 613, 382, 588, 2531, 558, 570, 562, 436, 3154, 50786, 50786, 666, 264, 18161, 2533, 264, 12240, 3584, 337, 439, 613, 4342, 307, 588, 2531, 293, 51032, 51032, 370, 264, 18161, 2533, 7309, 300, 436, 434, 588, 2531, 293, 733, 295, 411, 51172, 51172, 30358, 712, 498, 300, 1669, 2020, 550, 264, 264, 2793, 300, 366, 411, 534, 1400, 51494, 51494, 1314, 366, 337, 1365, 1249, 11, 1249, 307, 733, 295, 8668, 382, 364, 11183, 293, 1249, 575, 257, 588, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.12194984609430487, "compression_ratio": 1.8461538461538463, "no_speech_prob": 4.356787940196227e-06}, {"id": 750, "seek": 399572, "start": 3995.72, "end": 4000.6, "text": " special embedding vector so to speak similarly dot which is a special", "tokens": [50364, 2121, 12240, 3584, 8062, 370, 281, 1710, 14138, 5893, 597, 307, 257, 2121, 50608, 50608, 2517, 307, 439, 264, 636, 484, 510, 293, 257, 688, 295, 264, 661, 7825, 366, 1333, 295, 50804, 50804, 411, 596, 38624, 493, 510, 293, 370, 309, 311, 733, 295, 1880, 300, 456, 311, 257, 707, 857, 51004, 51004, 295, 3877, 510, 934, 264, 3097, 293, 309, 311, 406, 2138, 406, 4974, 293, 613, 51278, 51278, 12240, 29432, 652, 2020, 370, 321, 434, 586, 516, 281, 4373, 493, 264, 12240, 3584, 2744, 293, 1582, 380, 51522, 51522, 312, 1075, 281, 23273, 309, 3838, 457, 321, 2066, 300, 570, 321, 434, 833, 15669, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.07883465397465336, "compression_ratio": 1.7840909090909092, "no_speech_prob": 7.29602288629394e-06}, {"id": 751, "seek": 399572, "start": 4000.6, "end": 4004.52, "text": " character is all the way out here and a lot of the other letters are sort of", "tokens": [50364, 2121, 12240, 3584, 8062, 370, 281, 1710, 14138, 5893, 597, 307, 257, 2121, 50608, 50608, 2517, 307, 439, 264, 636, 484, 510, 293, 257, 688, 295, 264, 661, 7825, 366, 1333, 295, 50804, 50804, 411, 596, 38624, 493, 510, 293, 370, 309, 311, 733, 295, 1880, 300, 456, 311, 257, 707, 857, 51004, 51004, 295, 3877, 510, 934, 264, 3097, 293, 309, 311, 406, 2138, 406, 4974, 293, 613, 51278, 51278, 12240, 29432, 652, 2020, 370, 321, 434, 586, 516, 281, 4373, 493, 264, 12240, 3584, 2744, 293, 1582, 380, 51522, 51522, 312, 1075, 281, 23273, 309, 3838, 457, 321, 2066, 300, 570, 321, 434, 833, 15669, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.07883465397465336, "compression_ratio": 1.7840909090909092, "no_speech_prob": 7.29602288629394e-06}, {"id": 752, "seek": 399572, "start": 4004.52, "end": 4008.52, "text": " like clustered up here and so it's kind of interesting that there's a little bit", "tokens": [50364, 2121, 12240, 3584, 8062, 370, 281, 1710, 14138, 5893, 597, 307, 257, 2121, 50608, 50608, 2517, 307, 439, 264, 636, 484, 510, 293, 257, 688, 295, 264, 661, 7825, 366, 1333, 295, 50804, 50804, 411, 596, 38624, 493, 510, 293, 370, 309, 311, 733, 295, 1880, 300, 456, 311, 257, 707, 857, 51004, 51004, 295, 3877, 510, 934, 264, 3097, 293, 309, 311, 406, 2138, 406, 4974, 293, 613, 51278, 51278, 12240, 29432, 652, 2020, 370, 321, 434, 586, 516, 281, 4373, 493, 264, 12240, 3584, 2744, 293, 1582, 380, 51522, 51522, 312, 1075, 281, 23273, 309, 3838, 457, 321, 2066, 300, 570, 321, 434, 833, 15669, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.07883465397465336, "compression_ratio": 1.7840909090909092, "no_speech_prob": 7.29602288629394e-06}, {"id": 753, "seek": 399572, "start": 4008.52, "end": 4014.0, "text": " of structure here after the training and it's not definitely not random and these", "tokens": [50364, 2121, 12240, 3584, 8062, 370, 281, 1710, 14138, 5893, 597, 307, 257, 2121, 50608, 50608, 2517, 307, 439, 264, 636, 484, 510, 293, 257, 688, 295, 264, 661, 7825, 366, 1333, 295, 50804, 50804, 411, 596, 38624, 493, 510, 293, 370, 309, 311, 733, 295, 1880, 300, 456, 311, 257, 707, 857, 51004, 51004, 295, 3877, 510, 934, 264, 3097, 293, 309, 311, 406, 2138, 406, 4974, 293, 613, 51278, 51278, 12240, 29432, 652, 2020, 370, 321, 434, 586, 516, 281, 4373, 493, 264, 12240, 3584, 2744, 293, 1582, 380, 51522, 51522, 312, 1075, 281, 23273, 309, 3838, 457, 321, 2066, 300, 570, 321, 434, 833, 15669, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.07883465397465336, "compression_ratio": 1.7840909090909092, "no_speech_prob": 7.29602288629394e-06}, {"id": 754, "seek": 399572, "start": 4014.0, "end": 4018.8799999999997, "text": " embeddings make sense so we're now going to scale up the embedding size and won't", "tokens": [50364, 2121, 12240, 3584, 8062, 370, 281, 1710, 14138, 5893, 597, 307, 257, 2121, 50608, 50608, 2517, 307, 439, 264, 636, 484, 510, 293, 257, 688, 295, 264, 661, 7825, 366, 1333, 295, 50804, 50804, 411, 596, 38624, 493, 510, 293, 370, 309, 311, 733, 295, 1880, 300, 456, 311, 257, 707, 857, 51004, 51004, 295, 3877, 510, 934, 264, 3097, 293, 309, 311, 406, 2138, 406, 4974, 293, 613, 51278, 51278, 12240, 29432, 652, 2020, 370, 321, 434, 586, 516, 281, 4373, 493, 264, 12240, 3584, 2744, 293, 1582, 380, 51522, 51522, 312, 1075, 281, 23273, 309, 3838, 457, 321, 2066, 300, 570, 321, 434, 833, 15669, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.07883465397465336, "compression_ratio": 1.7840909090909092, "no_speech_prob": 7.29602288629394e-06}, {"id": 755, "seek": 399572, "start": 4018.8799999999997, "end": 4023.08, "text": " be able to visualize it directly but we expect that because we're under fitting", "tokens": [50364, 2121, 12240, 3584, 8062, 370, 281, 1710, 14138, 5893, 597, 307, 257, 2121, 50608, 50608, 2517, 307, 439, 264, 636, 484, 510, 293, 257, 688, 295, 264, 661, 7825, 366, 1333, 295, 50804, 50804, 411, 596, 38624, 493, 510, 293, 370, 309, 311, 733, 295, 1880, 300, 456, 311, 257, 707, 857, 51004, 51004, 295, 3877, 510, 934, 264, 3097, 293, 309, 311, 406, 2138, 406, 4974, 293, 613, 51278, 51278, 12240, 29432, 652, 2020, 370, 321, 434, 586, 516, 281, 4373, 493, 264, 12240, 3584, 2744, 293, 1582, 380, 51522, 51522, 312, 1075, 281, 23273, 309, 3838, 457, 321, 2066, 300, 570, 321, 434, 833, 15669, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.07883465397465336, "compression_ratio": 1.7840909090909092, "no_speech_prob": 7.29602288629394e-06}, {"id": 756, "seek": 402308, "start": 4023.08, "end": 4027.7999999999997, "text": " and we made this layer much bigger and did not sufficiently improve the loss", "tokens": [50364, 293, 321, 1027, 341, 4583, 709, 3801, 293, 630, 406, 31868, 3470, 264, 4470, 50600, 50600, 321, 434, 1953, 300, 264, 25534, 281, 1101, 3389, 558, 586, 727, 312, 50888, 50888, 613, 12240, 3584, 18875, 370, 718, 311, 652, 552, 3801, 1392, 370, 718, 311, 11369, 493, 510, 51080, 51080, 293, 586, 321, 500, 380, 362, 732, 12, 18759, 12240, 29432, 321, 366, 516, 281, 362, 584, 1266, 51320, 51320, 18795, 12240, 29432, 337, 1184, 1349, 550, 341, 4583, 486, 4774, 805, 1413, 1266, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.09981573711742055, "compression_ratio": 1.71875, "no_speech_prob": 1.028908900480019e-05}, {"id": 757, "seek": 402308, "start": 4027.7999999999997, "end": 4033.56, "text": " we're thinking that the constraint to better performance right now could be", "tokens": [50364, 293, 321, 1027, 341, 4583, 709, 3801, 293, 630, 406, 31868, 3470, 264, 4470, 50600, 50600, 321, 434, 1953, 300, 264, 25534, 281, 1101, 3389, 558, 586, 727, 312, 50888, 50888, 613, 12240, 3584, 18875, 370, 718, 311, 652, 552, 3801, 1392, 370, 718, 311, 11369, 493, 510, 51080, 51080, 293, 586, 321, 500, 380, 362, 732, 12, 18759, 12240, 29432, 321, 366, 516, 281, 362, 584, 1266, 51320, 51320, 18795, 12240, 29432, 337, 1184, 1349, 550, 341, 4583, 486, 4774, 805, 1413, 1266, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.09981573711742055, "compression_ratio": 1.71875, "no_speech_prob": 1.028908900480019e-05}, {"id": 758, "seek": 402308, "start": 4033.56, "end": 4037.4, "text": " these embedding vectors so let's make them bigger okay so let's scroll up here", "tokens": [50364, 293, 321, 1027, 341, 4583, 709, 3801, 293, 630, 406, 31868, 3470, 264, 4470, 50600, 50600, 321, 434, 1953, 300, 264, 25534, 281, 1101, 3389, 558, 586, 727, 312, 50888, 50888, 613, 12240, 3584, 18875, 370, 718, 311, 652, 552, 3801, 1392, 370, 718, 311, 11369, 493, 510, 51080, 51080, 293, 586, 321, 500, 380, 362, 732, 12, 18759, 12240, 29432, 321, 366, 516, 281, 362, 584, 1266, 51320, 51320, 18795, 12240, 29432, 337, 1184, 1349, 550, 341, 4583, 486, 4774, 805, 1413, 1266, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.09981573711742055, "compression_ratio": 1.71875, "no_speech_prob": 1.028908900480019e-05}, {"id": 759, "seek": 402308, "start": 4037.4, "end": 4042.2, "text": " and now we don't have two-dimensional embeddings we are going to have say 10", "tokens": [50364, 293, 321, 1027, 341, 4583, 709, 3801, 293, 630, 406, 31868, 3470, 264, 4470, 50600, 50600, 321, 434, 1953, 300, 264, 25534, 281, 1101, 3389, 558, 586, 727, 312, 50888, 50888, 613, 12240, 3584, 18875, 370, 718, 311, 652, 552, 3801, 1392, 370, 718, 311, 11369, 493, 510, 51080, 51080, 293, 586, 321, 500, 380, 362, 732, 12, 18759, 12240, 29432, 321, 366, 516, 281, 362, 584, 1266, 51320, 51320, 18795, 12240, 29432, 337, 1184, 1349, 550, 341, 4583, 486, 4774, 805, 1413, 1266, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.09981573711742055, "compression_ratio": 1.71875, "no_speech_prob": 1.028908900480019e-05}, {"id": 760, "seek": 402308, "start": 4042.2, "end": 4049.48, "text": " dimensional embeddings for each word then this layer will receive 3 times 10", "tokens": [50364, 293, 321, 1027, 341, 4583, 709, 3801, 293, 630, 406, 31868, 3470, 264, 4470, 50600, 50600, 321, 434, 1953, 300, 264, 25534, 281, 1101, 3389, 558, 586, 727, 312, 50888, 50888, 613, 12240, 3584, 18875, 370, 718, 311, 652, 552, 3801, 1392, 370, 718, 311, 11369, 493, 510, 51080, 51080, 293, 586, 321, 500, 380, 362, 732, 12, 18759, 12240, 29432, 321, 366, 516, 281, 362, 584, 1266, 51320, 51320, 18795, 12240, 29432, 337, 1184, 1349, 550, 341, 4583, 486, 4774, 805, 1413, 1266, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.09981573711742055, "compression_ratio": 1.71875, "no_speech_prob": 1.028908900480019e-05}, {"id": 761, "seek": 404948, "start": 4049.48, "end": 4056.92, "text": " so 30 inputs will go into the hidden layer let's also make the hidden layer", "tokens": [50364, 370, 2217, 15743, 486, 352, 666, 264, 7633, 4583, 718, 311, 611, 652, 264, 7633, 4583, 50736, 50736, 257, 857, 4356, 370, 2602, 295, 6641, 718, 311, 445, 360, 2331, 22027, 294, 300, 7633, 50912, 50912, 4583, 370, 586, 264, 3217, 1230, 295, 4959, 486, 312, 4748, 3801, 412, 2975, 11, 1360, 51184, 51184, 293, 550, 321, 510, 321, 362, 281, 312, 257, 857, 5026, 570, 1392, 264, 2539, 3314, 51466, 51466, 321, 992, 281, 1958, 13, 16, 510, 321, 366, 1152, 17720, 1386, 293, 2745, 498, 291, 434, 1364, 294, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.13913006029630962, "compression_ratio": 1.6968325791855203, "no_speech_prob": 4.3999094486935064e-05}, {"id": 762, "seek": 404948, "start": 4056.92, "end": 4060.44, "text": " a bit smaller so instead of 300 let's just do 200 neurons in that hidden", "tokens": [50364, 370, 2217, 15743, 486, 352, 666, 264, 7633, 4583, 718, 311, 611, 652, 264, 7633, 4583, 50736, 50736, 257, 857, 4356, 370, 2602, 295, 6641, 718, 311, 445, 360, 2331, 22027, 294, 300, 7633, 50912, 50912, 4583, 370, 586, 264, 3217, 1230, 295, 4959, 486, 312, 4748, 3801, 412, 2975, 11, 1360, 51184, 51184, 293, 550, 321, 510, 321, 362, 281, 312, 257, 857, 5026, 570, 1392, 264, 2539, 3314, 51466, 51466, 321, 992, 281, 1958, 13, 16, 510, 321, 366, 1152, 17720, 1386, 293, 2745, 498, 291, 434, 1364, 294, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.13913006029630962, "compression_ratio": 1.6968325791855203, "no_speech_prob": 4.3999094486935064e-05}, {"id": 763, "seek": 404948, "start": 4060.44, "end": 4065.88, "text": " layer so now the total number of elements will be slightly bigger at 11,000", "tokens": [50364, 370, 2217, 15743, 486, 352, 666, 264, 7633, 4583, 718, 311, 611, 652, 264, 7633, 4583, 50736, 50736, 257, 857, 4356, 370, 2602, 295, 6641, 718, 311, 445, 360, 2331, 22027, 294, 300, 7633, 50912, 50912, 4583, 370, 586, 264, 3217, 1230, 295, 4959, 486, 312, 4748, 3801, 412, 2975, 11, 1360, 51184, 51184, 293, 550, 321, 510, 321, 362, 281, 312, 257, 857, 5026, 570, 1392, 264, 2539, 3314, 51466, 51466, 321, 992, 281, 1958, 13, 16, 510, 321, 366, 1152, 17720, 1386, 293, 2745, 498, 291, 434, 1364, 294, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.13913006029630962, "compression_ratio": 1.6968325791855203, "no_speech_prob": 4.3999094486935064e-05}, {"id": 764, "seek": 404948, "start": 4065.88, "end": 4071.52, "text": " and then we here we have to be a bit careful because okay the learning rate", "tokens": [50364, 370, 2217, 15743, 486, 352, 666, 264, 7633, 4583, 718, 311, 611, 652, 264, 7633, 4583, 50736, 50736, 257, 857, 4356, 370, 2602, 295, 6641, 718, 311, 445, 360, 2331, 22027, 294, 300, 7633, 50912, 50912, 4583, 370, 586, 264, 3217, 1230, 295, 4959, 486, 312, 4748, 3801, 412, 2975, 11, 1360, 51184, 51184, 293, 550, 321, 510, 321, 362, 281, 312, 257, 857, 5026, 570, 1392, 264, 2539, 3314, 51466, 51466, 321, 992, 281, 1958, 13, 16, 510, 321, 366, 1152, 17720, 1386, 293, 2745, 498, 291, 434, 1364, 294, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.13913006029630962, "compression_ratio": 1.6968325791855203, "no_speech_prob": 4.3999094486935064e-05}, {"id": 765, "seek": 404948, "start": 4071.52, "end": 4077.08, "text": " we set to 0.1 here we are hard coding 6 and obviously if you're working in", "tokens": [50364, 370, 2217, 15743, 486, 352, 666, 264, 7633, 4583, 718, 311, 611, 652, 264, 7633, 4583, 50736, 50736, 257, 857, 4356, 370, 2602, 295, 6641, 718, 311, 445, 360, 2331, 22027, 294, 300, 7633, 50912, 50912, 4583, 370, 586, 264, 3217, 1230, 295, 4959, 486, 312, 4748, 3801, 412, 2975, 11, 1360, 51184, 51184, 293, 550, 321, 510, 321, 362, 281, 312, 257, 857, 5026, 570, 1392, 264, 2539, 3314, 51466, 51466, 321, 992, 281, 1958, 13, 16, 510, 321, 366, 1152, 17720, 1386, 293, 2745, 498, 291, 434, 1364, 294, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.13913006029630962, "compression_ratio": 1.6968325791855203, "no_speech_prob": 4.3999094486935064e-05}, {"id": 766, "seek": 407708, "start": 4077.08, "end": 4080.44, "text": " production you don't want to be hard coding magic numbers but instead of 6", "tokens": [50364, 4265, 291, 500, 380, 528, 281, 312, 1152, 17720, 5585, 3547, 457, 2602, 295, 1386, 50532, 50532, 341, 820, 586, 312, 2217, 293, 718, 311, 1190, 337, 2625, 11, 1360, 36540, 293, 718, 385, 7472, 484, 50906, 50906, 264, 5883, 2144, 510, 2380, 370, 300, 562, 321, 1190, 341, 257, 3866, 1413, 309, 311, 51170, 51170, 406, 516, 281, 14082, 484, 527, 4470, 294, 4500, 281, 300, 510, 718, 311, 2602, 295, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.09737053042963932, "compression_ratio": 1.5803108808290156, "no_speech_prob": 2.5464900318183936e-05}, {"id": 767, "seek": 407708, "start": 4080.44, "end": 4087.92, "text": " this should now be 30 and let's run for 50,000 iterations and let me split out", "tokens": [50364, 4265, 291, 500, 380, 528, 281, 312, 1152, 17720, 5585, 3547, 457, 2602, 295, 1386, 50532, 50532, 341, 820, 586, 312, 2217, 293, 718, 311, 1190, 337, 2625, 11, 1360, 36540, 293, 718, 385, 7472, 484, 50906, 50906, 264, 5883, 2144, 510, 2380, 370, 300, 562, 321, 1190, 341, 257, 3866, 1413, 309, 311, 51170, 51170, 406, 516, 281, 14082, 484, 527, 4470, 294, 4500, 281, 300, 510, 718, 311, 2602, 295, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.09737053042963932, "compression_ratio": 1.5803108808290156, "no_speech_prob": 2.5464900318183936e-05}, {"id": 768, "seek": 407708, "start": 4087.92, "end": 4093.2, "text": " the initialization here outside so that when we run this a multiple times it's", "tokens": [50364, 4265, 291, 500, 380, 528, 281, 312, 1152, 17720, 5585, 3547, 457, 2602, 295, 1386, 50532, 50532, 341, 820, 586, 312, 2217, 293, 718, 311, 1190, 337, 2625, 11, 1360, 36540, 293, 718, 385, 7472, 484, 50906, 50906, 264, 5883, 2144, 510, 2380, 370, 300, 562, 321, 1190, 341, 257, 3866, 1413, 309, 311, 51170, 51170, 406, 516, 281, 14082, 484, 527, 4470, 294, 4500, 281, 300, 510, 718, 311, 2602, 295, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.09737053042963932, "compression_ratio": 1.5803108808290156, "no_speech_prob": 2.5464900318183936e-05}, {"id": 769, "seek": 407708, "start": 4093.2, "end": 4101.5599999999995, "text": " not going to wipe out our loss in addition to that here let's instead of", "tokens": [50364, 4265, 291, 500, 380, 528, 281, 312, 1152, 17720, 5585, 3547, 457, 2602, 295, 1386, 50532, 50532, 341, 820, 586, 312, 2217, 293, 718, 311, 1190, 337, 2625, 11, 1360, 36540, 293, 718, 385, 7472, 484, 50906, 50906, 264, 5883, 2144, 510, 2380, 370, 300, 562, 321, 1190, 341, 257, 3866, 1413, 309, 311, 51170, 51170, 406, 516, 281, 14082, 484, 527, 4470, 294, 4500, 281, 300, 510, 718, 311, 2602, 295, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.09737053042963932, "compression_ratio": 1.5803108808290156, "no_speech_prob": 2.5464900318183936e-05}, {"id": 770, "seek": 410156, "start": 4101.56, "end": 4109.120000000001, "text": " logging loss that item let's actually log them let's do log 10 I believe", "tokens": [50364, 27991, 4470, 300, 3174, 718, 311, 767, 3565, 552, 718, 311, 360, 3565, 1266, 286, 1697, 50742, 50742, 300, 311, 257, 2445, 295, 264, 4470, 293, 286, 603, 855, 291, 983, 294, 257, 1150, 718, 311, 19719, 51030, 51030, 341, 1936, 286, 1116, 411, 281, 7542, 264, 3565, 4470, 2602, 295, 257, 4470, 570, 562, 291, 51338, 51338, 7542, 264, 4470, 867, 1413, 309, 393, 362, 341, 22449, 2897, 8967, 293, 3565, 51526, 51526, 2339, 12808, 309, 294, 370, 309, 445, 733, 295, 411, 1542, 22842, 370, 264, 2031, 12, 24633, 307, 1823, 286, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.14909405610999282, "compression_ratio": 1.6740088105726871, "no_speech_prob": 1.280513151868945e-05}, {"id": 771, "seek": 410156, "start": 4109.120000000001, "end": 4114.88, "text": " that's a function of the loss and I'll show you why in a second let's optimize", "tokens": [50364, 27991, 4470, 300, 3174, 718, 311, 767, 3565, 552, 718, 311, 360, 3565, 1266, 286, 1697, 50742, 50742, 300, 311, 257, 2445, 295, 264, 4470, 293, 286, 603, 855, 291, 983, 294, 257, 1150, 718, 311, 19719, 51030, 51030, 341, 1936, 286, 1116, 411, 281, 7542, 264, 3565, 4470, 2602, 295, 257, 4470, 570, 562, 291, 51338, 51338, 7542, 264, 4470, 867, 1413, 309, 393, 362, 341, 22449, 2897, 8967, 293, 3565, 51526, 51526, 2339, 12808, 309, 294, 370, 309, 445, 733, 295, 411, 1542, 22842, 370, 264, 2031, 12, 24633, 307, 1823, 286, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.14909405610999282, "compression_ratio": 1.6740088105726871, "no_speech_prob": 1.280513151868945e-05}, {"id": 772, "seek": 410156, "start": 4114.88, "end": 4121.04, "text": " this basically I'd like to plot the log loss instead of a loss because when you", "tokens": [50364, 27991, 4470, 300, 3174, 718, 311, 767, 3565, 552, 718, 311, 360, 3565, 1266, 286, 1697, 50742, 50742, 300, 311, 257, 2445, 295, 264, 4470, 293, 286, 603, 855, 291, 983, 294, 257, 1150, 718, 311, 19719, 51030, 51030, 341, 1936, 286, 1116, 411, 281, 7542, 264, 3565, 4470, 2602, 295, 257, 4470, 570, 562, 291, 51338, 51338, 7542, 264, 4470, 867, 1413, 309, 393, 362, 341, 22449, 2897, 8967, 293, 3565, 51526, 51526, 2339, 12808, 309, 294, 370, 309, 445, 733, 295, 411, 1542, 22842, 370, 264, 2031, 12, 24633, 307, 1823, 286, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.14909405610999282, "compression_ratio": 1.6740088105726871, "no_speech_prob": 1.280513151868945e-05}, {"id": 773, "seek": 410156, "start": 4121.04, "end": 4124.8, "text": " plot the loss many times it can have this hockey stick appearance and log", "tokens": [50364, 27991, 4470, 300, 3174, 718, 311, 767, 3565, 552, 718, 311, 360, 3565, 1266, 286, 1697, 50742, 50742, 300, 311, 257, 2445, 295, 264, 4470, 293, 286, 603, 855, 291, 983, 294, 257, 1150, 718, 311, 19719, 51030, 51030, 341, 1936, 286, 1116, 411, 281, 7542, 264, 3565, 4470, 2602, 295, 257, 4470, 570, 562, 291, 51338, 51338, 7542, 264, 4470, 867, 1413, 309, 393, 362, 341, 22449, 2897, 8967, 293, 3565, 51526, 51526, 2339, 12808, 309, 294, 370, 309, 445, 733, 295, 411, 1542, 22842, 370, 264, 2031, 12, 24633, 307, 1823, 286, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.14909405610999282, "compression_ratio": 1.6740088105726871, "no_speech_prob": 1.280513151868945e-05}, {"id": 774, "seek": 410156, "start": 4124.8, "end": 4131.200000000001, "text": " squashes it in so it just kind of like looks nicer so the x-axis is step I", "tokens": [50364, 27991, 4470, 300, 3174, 718, 311, 767, 3565, 552, 718, 311, 360, 3565, 1266, 286, 1697, 50742, 50742, 300, 311, 257, 2445, 295, 264, 4470, 293, 286, 603, 855, 291, 983, 294, 257, 1150, 718, 311, 19719, 51030, 51030, 341, 1936, 286, 1116, 411, 281, 7542, 264, 3565, 4470, 2602, 295, 257, 4470, 570, 562, 291, 51338, 51338, 7542, 264, 4470, 867, 1413, 309, 393, 362, 341, 22449, 2897, 8967, 293, 3565, 51526, 51526, 2339, 12808, 309, 294, 370, 309, 445, 733, 295, 411, 1542, 22842, 370, 264, 2031, 12, 24633, 307, 1823, 286, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.14909405610999282, "compression_ratio": 1.6740088105726871, "no_speech_prob": 1.280513151868945e-05}, {"id": 775, "seek": 413120, "start": 4131.2, "end": 4144.2, "text": " and the y-axis will be the loss I and then here this is 30 ideally we wouldn't", "tokens": [50364, 293, 264, 288, 12, 24633, 486, 312, 264, 4470, 286, 293, 550, 510, 341, 307, 2217, 22915, 321, 2759, 380, 51014, 51014, 312, 1152, 17720, 613, 570, 718, 311, 574, 412, 264, 4470, 1392, 309, 311, 797, 588, 5060, 51464, 51464, 570, 264, 8382, 15245, 2744, 307, 588, 1359, 457, 264, 3217, 4470, 670, 264, 51628, 51628], "temperature": 0.0, "avg_logprob": -0.12302485967086534, "compression_ratio": 1.5547945205479452, "no_speech_prob": 1.805804822652135e-05}, {"id": 776, "seek": 413120, "start": 4144.2, "end": 4153.2, "text": " be hard coding these because let's look at the loss okay it's again very thick", "tokens": [50364, 293, 264, 288, 12, 24633, 486, 312, 264, 4470, 286, 293, 550, 510, 341, 307, 2217, 22915, 321, 2759, 380, 51014, 51014, 312, 1152, 17720, 613, 570, 718, 311, 574, 412, 264, 4470, 1392, 309, 311, 797, 588, 5060, 51464, 51464, 570, 264, 8382, 15245, 2744, 307, 588, 1359, 457, 264, 3217, 4470, 670, 264, 51628, 51628], "temperature": 0.0, "avg_logprob": -0.12302485967086534, "compression_ratio": 1.5547945205479452, "no_speech_prob": 1.805804822652135e-05}, {"id": 777, "seek": 413120, "start": 4153.2, "end": 4156.48, "text": " because the mini batch size is very small but the total loss over the", "tokens": [50364, 293, 264, 288, 12, 24633, 486, 312, 264, 4470, 286, 293, 550, 510, 341, 307, 2217, 22915, 321, 2759, 380, 51014, 51014, 312, 1152, 17720, 613, 570, 718, 311, 574, 412, 264, 4470, 1392, 309, 311, 797, 588, 5060, 51464, 51464, 570, 264, 8382, 15245, 2744, 307, 588, 1359, 457, 264, 3217, 4470, 670, 264, 51628, 51628], "temperature": 0.0, "avg_logprob": -0.12302485967086534, "compression_ratio": 1.5547945205479452, "no_speech_prob": 1.805804822652135e-05}, {"id": 778, "seek": 415648, "start": 4156.48, "end": 4162.759999999999, "text": " training set is 2.3 and the tests or the def set is 2.38 as well so so far so", "tokens": [50364, 3097, 992, 307, 568, 13, 18, 293, 264, 6921, 420, 264, 1060, 992, 307, 568, 13, 12625, 382, 731, 370, 370, 1400, 370, 50678, 50678, 665, 718, 311, 853, 281, 586, 11514, 264, 2539, 3314, 538, 257, 5952, 295, 1266, 293, 50976, 50976, 3847, 337, 1071, 2625, 11, 1360, 36540, 321, 1116, 1454, 300, 321, 576, 312, 1075, 281, 4224, 568, 13, 18, 51460, 51460], "temperature": 0.0, "avg_logprob": -0.17654147076962598, "compression_ratio": 1.4805194805194806, "no_speech_prob": 3.6117935451329686e-06}, {"id": 779, "seek": 415648, "start": 4162.759999999999, "end": 4168.719999999999, "text": " good let's try to now decrease the learning rate by a factor of 10 and", "tokens": [50364, 3097, 992, 307, 568, 13, 18, 293, 264, 6921, 420, 264, 1060, 992, 307, 568, 13, 12625, 382, 731, 370, 370, 1400, 370, 50678, 50678, 665, 718, 311, 853, 281, 586, 11514, 264, 2539, 3314, 538, 257, 5952, 295, 1266, 293, 50976, 50976, 3847, 337, 1071, 2625, 11, 1360, 36540, 321, 1116, 1454, 300, 321, 576, 312, 1075, 281, 4224, 568, 13, 18, 51460, 51460], "temperature": 0.0, "avg_logprob": -0.17654147076962598, "compression_ratio": 1.4805194805194806, "no_speech_prob": 3.6117935451329686e-06}, {"id": 780, "seek": 415648, "start": 4168.719999999999, "end": 4178.4, "text": " train for another 50,000 iterations we'd hope that we would be able to beat 2.3", "tokens": [50364, 3097, 992, 307, 568, 13, 18, 293, 264, 6921, 420, 264, 1060, 992, 307, 568, 13, 12625, 382, 731, 370, 370, 1400, 370, 50678, 50678, 665, 718, 311, 853, 281, 586, 11514, 264, 2539, 3314, 538, 257, 5952, 295, 1266, 293, 50976, 50976, 3847, 337, 1071, 2625, 11, 1360, 36540, 321, 1116, 1454, 300, 321, 576, 312, 1075, 281, 4224, 568, 13, 18, 51460, 51460], "temperature": 0.0, "avg_logprob": -0.17654147076962598, "compression_ratio": 1.4805194805194806, "no_speech_prob": 3.6117935451329686e-06}, {"id": 781, "seek": 417840, "start": 4178.4, "end": 4186.96, "text": " too but again we're just kind of like doing this very haphazardly so I don't", "tokens": [50364, 886, 457, 797, 321, 434, 445, 733, 295, 411, 884, 341, 588, 324, 950, 921, 515, 356, 370, 286, 500, 380, 50792, 50792, 767, 362, 6687, 300, 527, 2539, 3314, 307, 992, 588, 731, 300, 527, 50976, 50976, 2539, 3314, 21039, 597, 321, 445, 360, 412, 4974, 307, 992, 588, 731, 293, 370, 264, 51280, 51280, 19618, 510, 307, 733, 295, 9091, 281, 312, 3245, 293, 341, 307, 406, 577, 291, 576, 51436, 51436, 360, 309, 5850, 294, 4265, 294, 4265, 291, 576, 1884, 9834, 51596, 51596, 420, 9848, 9834, 484, 295, 439, 613, 6257, 293, 550, 291, 576, 1190, 3195, 295, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.07631550195082179, "compression_ratio": 1.883817427385892, "no_speech_prob": 1.0615390237944666e-05}, {"id": 782, "seek": 417840, "start": 4186.96, "end": 4190.639999999999, "text": " actually have confidence that our learning rate is set very well that our", "tokens": [50364, 886, 457, 797, 321, 434, 445, 733, 295, 411, 884, 341, 588, 324, 950, 921, 515, 356, 370, 286, 500, 380, 50792, 50792, 767, 362, 6687, 300, 527, 2539, 3314, 307, 992, 588, 731, 300, 527, 50976, 50976, 2539, 3314, 21039, 597, 321, 445, 360, 412, 4974, 307, 992, 588, 731, 293, 370, 264, 51280, 51280, 19618, 510, 307, 733, 295, 9091, 281, 312, 3245, 293, 341, 307, 406, 577, 291, 576, 51436, 51436, 360, 309, 5850, 294, 4265, 294, 4265, 291, 576, 1884, 9834, 51596, 51596, 420, 9848, 9834, 484, 295, 439, 613, 6257, 293, 550, 291, 576, 1190, 3195, 295, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.07631550195082179, "compression_ratio": 1.883817427385892, "no_speech_prob": 1.0615390237944666e-05}, {"id": 783, "seek": 417840, "start": 4190.639999999999, "end": 4196.719999999999, "text": " learning rate decay which we just do at random is set very well and so the", "tokens": [50364, 886, 457, 797, 321, 434, 445, 733, 295, 411, 884, 341, 588, 324, 950, 921, 515, 356, 370, 286, 500, 380, 50792, 50792, 767, 362, 6687, 300, 527, 2539, 3314, 307, 992, 588, 731, 300, 527, 50976, 50976, 2539, 3314, 21039, 597, 321, 445, 360, 412, 4974, 307, 992, 588, 731, 293, 370, 264, 51280, 51280, 19618, 510, 307, 733, 295, 9091, 281, 312, 3245, 293, 341, 307, 406, 577, 291, 576, 51436, 51436, 360, 309, 5850, 294, 4265, 294, 4265, 291, 576, 1884, 9834, 51596, 51596, 420, 9848, 9834, 484, 295, 439, 613, 6257, 293, 550, 291, 576, 1190, 3195, 295, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.07631550195082179, "compression_ratio": 1.883817427385892, "no_speech_prob": 1.0615390237944666e-05}, {"id": 784, "seek": 417840, "start": 4196.719999999999, "end": 4199.839999999999, "text": " optimization here is kind of suspect to be honest and this is not how you would", "tokens": [50364, 886, 457, 797, 321, 434, 445, 733, 295, 411, 884, 341, 588, 324, 950, 921, 515, 356, 370, 286, 500, 380, 50792, 50792, 767, 362, 6687, 300, 527, 2539, 3314, 307, 992, 588, 731, 300, 527, 50976, 50976, 2539, 3314, 21039, 597, 321, 445, 360, 412, 4974, 307, 992, 588, 731, 293, 370, 264, 51280, 51280, 19618, 510, 307, 733, 295, 9091, 281, 312, 3245, 293, 341, 307, 406, 577, 291, 576, 51436, 51436, 360, 309, 5850, 294, 4265, 294, 4265, 291, 576, 1884, 9834, 51596, 51596, 420, 9848, 9834, 484, 295, 439, 613, 6257, 293, 550, 291, 576, 1190, 3195, 295, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.07631550195082179, "compression_ratio": 1.883817427385892, "no_speech_prob": 1.0615390237944666e-05}, {"id": 785, "seek": 417840, "start": 4199.839999999999, "end": 4203.04, "text": " do it typically in production in production you would create parameters", "tokens": [50364, 886, 457, 797, 321, 434, 445, 733, 295, 411, 884, 341, 588, 324, 950, 921, 515, 356, 370, 286, 500, 380, 50792, 50792, 767, 362, 6687, 300, 527, 2539, 3314, 307, 992, 588, 731, 300, 527, 50976, 50976, 2539, 3314, 21039, 597, 321, 445, 360, 412, 4974, 307, 992, 588, 731, 293, 370, 264, 51280, 51280, 19618, 510, 307, 733, 295, 9091, 281, 312, 3245, 293, 341, 307, 406, 577, 291, 576, 51436, 51436, 360, 309, 5850, 294, 4265, 294, 4265, 291, 576, 1884, 9834, 51596, 51596, 420, 9848, 9834, 484, 295, 439, 613, 6257, 293, 550, 291, 576, 1190, 3195, 295, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.07631550195082179, "compression_ratio": 1.883817427385892, "no_speech_prob": 1.0615390237944666e-05}, {"id": 786, "seek": 417840, "start": 4203.04, "end": 4206.5599999999995, "text": " or hyper parameters out of all these settings and then you would run lots of", "tokens": [50364, 886, 457, 797, 321, 434, 445, 733, 295, 411, 884, 341, 588, 324, 950, 921, 515, 356, 370, 286, 500, 380, 50792, 50792, 767, 362, 6687, 300, 527, 2539, 3314, 307, 992, 588, 731, 300, 527, 50976, 50976, 2539, 3314, 21039, 597, 321, 445, 360, 412, 4974, 307, 992, 588, 731, 293, 370, 264, 51280, 51280, 19618, 510, 307, 733, 295, 9091, 281, 312, 3245, 293, 341, 307, 406, 577, 291, 576, 51436, 51436, 360, 309, 5850, 294, 4265, 294, 4265, 291, 576, 1884, 9834, 51596, 51596, 420, 9848, 9834, 484, 295, 439, 613, 6257, 293, 550, 291, 576, 1190, 3195, 295, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.07631550195082179, "compression_ratio": 1.883817427385892, "no_speech_prob": 1.0615390237944666e-05}, {"id": 787, "seek": 420656, "start": 4206.56, "end": 4214.400000000001, "text": " experiments and see whichever ones are working well for you okay so we have 2.17", "tokens": [50364, 12050, 293, 536, 24123, 2306, 366, 1364, 731, 337, 291, 1392, 370, 321, 362, 568, 13, 7773, 50756, 50756, 586, 293, 568, 13, 17, 1392, 370, 291, 536, 577, 264, 3097, 293, 264, 24071, 3389, 51040, 51040, 366, 2891, 281, 4748, 5692, 9110, 370, 1310, 321, 434, 1242, 264, 2020, 300, 51276, 51276, 264, 18161, 2533, 307, 1242, 665, 1547, 420, 300, 1230, 295, 9834, 307, 2416, 51556, 51556, 1547, 300, 321, 366, 5692, 2891, 281, 670, 6845, 718, 311, 1310, 1190, 472, 24784, 295, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13186576928985252, "compression_ratio": 1.7017543859649122, "no_speech_prob": 4.710754637926584e-06}, {"id": 788, "seek": 420656, "start": 4214.400000000001, "end": 4220.080000000001, "text": " now and 2.2 okay so you see how the training and the validation performance", "tokens": [50364, 12050, 293, 536, 24123, 2306, 366, 1364, 731, 337, 291, 1392, 370, 321, 362, 568, 13, 7773, 50756, 50756, 586, 293, 568, 13, 17, 1392, 370, 291, 536, 577, 264, 3097, 293, 264, 24071, 3389, 51040, 51040, 366, 2891, 281, 4748, 5692, 9110, 370, 1310, 321, 434, 1242, 264, 2020, 300, 51276, 51276, 264, 18161, 2533, 307, 1242, 665, 1547, 420, 300, 1230, 295, 9834, 307, 2416, 51556, 51556, 1547, 300, 321, 366, 5692, 2891, 281, 670, 6845, 718, 311, 1310, 1190, 472, 24784, 295, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13186576928985252, "compression_ratio": 1.7017543859649122, "no_speech_prob": 4.710754637926584e-06}, {"id": 789, "seek": 420656, "start": 4220.080000000001, "end": 4224.8, "text": " are starting to slightly slowly depart so maybe we're getting the sense that", "tokens": [50364, 12050, 293, 536, 24123, 2306, 366, 1364, 731, 337, 291, 1392, 370, 321, 362, 568, 13, 7773, 50756, 50756, 586, 293, 568, 13, 17, 1392, 370, 291, 536, 577, 264, 3097, 293, 264, 24071, 3389, 51040, 51040, 366, 2891, 281, 4748, 5692, 9110, 370, 1310, 321, 434, 1242, 264, 2020, 300, 51276, 51276, 264, 18161, 2533, 307, 1242, 665, 1547, 420, 300, 1230, 295, 9834, 307, 2416, 51556, 51556, 1547, 300, 321, 366, 5692, 2891, 281, 670, 6845, 718, 311, 1310, 1190, 472, 24784, 295, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13186576928985252, "compression_ratio": 1.7017543859649122, "no_speech_prob": 4.710754637926584e-06}, {"id": 790, "seek": 420656, "start": 4224.8, "end": 4230.400000000001, "text": " the neural net is getting good enough or that number of parameters is large", "tokens": [50364, 12050, 293, 536, 24123, 2306, 366, 1364, 731, 337, 291, 1392, 370, 321, 362, 568, 13, 7773, 50756, 50756, 586, 293, 568, 13, 17, 1392, 370, 291, 536, 577, 264, 3097, 293, 264, 24071, 3389, 51040, 51040, 366, 2891, 281, 4748, 5692, 9110, 370, 1310, 321, 434, 1242, 264, 2020, 300, 51276, 51276, 264, 18161, 2533, 307, 1242, 665, 1547, 420, 300, 1230, 295, 9834, 307, 2416, 51556, 51556, 1547, 300, 321, 366, 5692, 2891, 281, 670, 6845, 718, 311, 1310, 1190, 472, 24784, 295, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13186576928985252, "compression_ratio": 1.7017543859649122, "no_speech_prob": 4.710754637926584e-06}, {"id": 791, "seek": 420656, "start": 4230.400000000001, "end": 4236.52, "text": " enough that we are slowly starting to overfit let's maybe run one iteration of", "tokens": [50364, 12050, 293, 536, 24123, 2306, 366, 1364, 731, 337, 291, 1392, 370, 321, 362, 568, 13, 7773, 50756, 50756, 586, 293, 568, 13, 17, 1392, 370, 291, 536, 577, 264, 3097, 293, 264, 24071, 3389, 51040, 51040, 366, 2891, 281, 4748, 5692, 9110, 370, 1310, 321, 434, 1242, 264, 2020, 300, 51276, 51276, 264, 18161, 2533, 307, 1242, 665, 1547, 420, 300, 1230, 295, 9834, 307, 2416, 51556, 51556, 1547, 300, 321, 366, 5692, 2891, 281, 670, 6845, 718, 311, 1310, 1190, 472, 24784, 295, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13186576928985252, "compression_ratio": 1.7017543859649122, "no_speech_prob": 4.710754637926584e-06}, {"id": 792, "seek": 423652, "start": 4236.52, "end": 4243.72, "text": " this and see where we get but yeah basically you would be running lots of", "tokens": [50364, 341, 293, 536, 689, 321, 483, 457, 1338, 1936, 291, 576, 312, 2614, 3195, 295, 50724, 50724, 12050, 293, 550, 291, 366, 5692, 28949, 259, 3319, 24123, 2306, 976, 291, 264, 50874, 50874, 1151, 7161, 3389, 293, 550, 1564, 291, 915, 439, 264, 9848, 9834, 300, 652, 51108, 51108, 428, 7161, 3389, 665, 291, 747, 300, 2316, 293, 291, 13059, 264, 1500, 992, 51300, 51300, 3389, 257, 2167, 565, 293, 300, 311, 264, 1230, 300, 291, 2275, 294, 428, 3035, 420, 51498, 51498, 8660, 1646, 291, 528, 281, 751, 466, 293, 41995, 466, 428, 2316, 370, 718, 311, 550, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10063295456969622, "compression_ratio": 1.8531746031746033, "no_speech_prob": 1.0129294423677493e-05}, {"id": 793, "seek": 423652, "start": 4243.72, "end": 4246.72, "text": " experiments and then you are slowly scrutinizing whichever ones give you the", "tokens": [50364, 341, 293, 536, 689, 321, 483, 457, 1338, 1936, 291, 576, 312, 2614, 3195, 295, 50724, 50724, 12050, 293, 550, 291, 366, 5692, 28949, 259, 3319, 24123, 2306, 976, 291, 264, 50874, 50874, 1151, 7161, 3389, 293, 550, 1564, 291, 915, 439, 264, 9848, 9834, 300, 652, 51108, 51108, 428, 7161, 3389, 665, 291, 747, 300, 2316, 293, 291, 13059, 264, 1500, 992, 51300, 51300, 3389, 257, 2167, 565, 293, 300, 311, 264, 1230, 300, 291, 2275, 294, 428, 3035, 420, 51498, 51498, 8660, 1646, 291, 528, 281, 751, 466, 293, 41995, 466, 428, 2316, 370, 718, 311, 550, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10063295456969622, "compression_ratio": 1.8531746031746033, "no_speech_prob": 1.0129294423677493e-05}, {"id": 794, "seek": 423652, "start": 4246.72, "end": 4251.400000000001, "text": " best depth performance and then once you find all the hyper parameters that make", "tokens": [50364, 341, 293, 536, 689, 321, 483, 457, 1338, 1936, 291, 576, 312, 2614, 3195, 295, 50724, 50724, 12050, 293, 550, 291, 366, 5692, 28949, 259, 3319, 24123, 2306, 976, 291, 264, 50874, 50874, 1151, 7161, 3389, 293, 550, 1564, 291, 915, 439, 264, 9848, 9834, 300, 652, 51108, 51108, 428, 7161, 3389, 665, 291, 747, 300, 2316, 293, 291, 13059, 264, 1500, 992, 51300, 51300, 3389, 257, 2167, 565, 293, 300, 311, 264, 1230, 300, 291, 2275, 294, 428, 3035, 420, 51498, 51498, 8660, 1646, 291, 528, 281, 751, 466, 293, 41995, 466, 428, 2316, 370, 718, 311, 550, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10063295456969622, "compression_ratio": 1.8531746031746033, "no_speech_prob": 1.0129294423677493e-05}, {"id": 795, "seek": 423652, "start": 4251.400000000001, "end": 4255.240000000001, "text": " your depth performance good you take that model and you evaluate the test set", "tokens": [50364, 341, 293, 536, 689, 321, 483, 457, 1338, 1936, 291, 576, 312, 2614, 3195, 295, 50724, 50724, 12050, 293, 550, 291, 366, 5692, 28949, 259, 3319, 24123, 2306, 976, 291, 264, 50874, 50874, 1151, 7161, 3389, 293, 550, 1564, 291, 915, 439, 264, 9848, 9834, 300, 652, 51108, 51108, 428, 7161, 3389, 665, 291, 747, 300, 2316, 293, 291, 13059, 264, 1500, 992, 51300, 51300, 3389, 257, 2167, 565, 293, 300, 311, 264, 1230, 300, 291, 2275, 294, 428, 3035, 420, 51498, 51498, 8660, 1646, 291, 528, 281, 751, 466, 293, 41995, 466, 428, 2316, 370, 718, 311, 550, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10063295456969622, "compression_ratio": 1.8531746031746033, "no_speech_prob": 1.0129294423677493e-05}, {"id": 796, "seek": 423652, "start": 4255.240000000001, "end": 4259.200000000001, "text": " performance a single time and that's the number that you report in your paper or", "tokens": [50364, 341, 293, 536, 689, 321, 483, 457, 1338, 1936, 291, 576, 312, 2614, 3195, 295, 50724, 50724, 12050, 293, 550, 291, 366, 5692, 28949, 259, 3319, 24123, 2306, 976, 291, 264, 50874, 50874, 1151, 7161, 3389, 293, 550, 1564, 291, 915, 439, 264, 9848, 9834, 300, 652, 51108, 51108, 428, 7161, 3389, 665, 291, 747, 300, 2316, 293, 291, 13059, 264, 1500, 992, 51300, 51300, 3389, 257, 2167, 565, 293, 300, 311, 264, 1230, 300, 291, 2275, 294, 428, 3035, 420, 51498, 51498, 8660, 1646, 291, 528, 281, 751, 466, 293, 41995, 466, 428, 2316, 370, 718, 311, 550, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10063295456969622, "compression_ratio": 1.8531746031746033, "no_speech_prob": 1.0129294423677493e-05}, {"id": 797, "seek": 423652, "start": 4259.200000000001, "end": 4266.240000000001, "text": " wherever else you want to talk about and brag about your model so let's then", "tokens": [50364, 341, 293, 536, 689, 321, 483, 457, 1338, 1936, 291, 576, 312, 2614, 3195, 295, 50724, 50724, 12050, 293, 550, 291, 366, 5692, 28949, 259, 3319, 24123, 2306, 976, 291, 264, 50874, 50874, 1151, 7161, 3389, 293, 550, 1564, 291, 915, 439, 264, 9848, 9834, 300, 652, 51108, 51108, 428, 7161, 3389, 665, 291, 747, 300, 2316, 293, 291, 13059, 264, 1500, 992, 51300, 51300, 3389, 257, 2167, 565, 293, 300, 311, 264, 1230, 300, 291, 2275, 294, 428, 3035, 420, 51498, 51498, 8660, 1646, 291, 528, 281, 751, 466, 293, 41995, 466, 428, 2316, 370, 718, 311, 550, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10063295456969622, "compression_ratio": 1.8531746031746033, "no_speech_prob": 1.0129294423677493e-05}, {"id": 798, "seek": 426624, "start": 4266.24, "end": 4272.24, "text": " rerun the plot and rerun the train and dove and because we're getting lower", "tokens": [50364, 43819, 409, 264, 7542, 293, 43819, 409, 264, 3847, 293, 23287, 293, 570, 321, 434, 1242, 3126, 50664, 50664, 4470, 586, 309, 307, 264, 1389, 300, 264, 12240, 3584, 2744, 295, 613, 390, 5061, 505, 50864, 50864, 646, 588, 3700, 1392, 370, 568, 13, 6866, 568, 13, 3405, 307, 437, 321, 434, 9810, 1242, 370, 456, 311, 51308, 51308, 867, 2098, 281, 352, 490, 867, 2098, 281, 352, 490, 510, 321, 393, 2354, 15164, 264, 51492, 51492, 19618, 321, 393, 2354, 337, 1365, 2433, 365, 264, 2744, 295, 264, 18161, 2533, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.10150780928762336, "compression_ratio": 1.7419354838709677, "no_speech_prob": 8.267522389360238e-06}, {"id": 799, "seek": 426624, "start": 4272.24, "end": 4276.24, "text": " loss now it is the case that the embedding size of these was holding us", "tokens": [50364, 43819, 409, 264, 7542, 293, 43819, 409, 264, 3847, 293, 23287, 293, 570, 321, 434, 1242, 3126, 50664, 50664, 4470, 586, 309, 307, 264, 1389, 300, 264, 12240, 3584, 2744, 295, 613, 390, 5061, 505, 50864, 50864, 646, 588, 3700, 1392, 370, 568, 13, 6866, 568, 13, 3405, 307, 437, 321, 434, 9810, 1242, 370, 456, 311, 51308, 51308, 867, 2098, 281, 352, 490, 867, 2098, 281, 352, 490, 510, 321, 393, 2354, 15164, 264, 51492, 51492, 19618, 321, 393, 2354, 337, 1365, 2433, 365, 264, 2744, 295, 264, 18161, 2533, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.10150780928762336, "compression_ratio": 1.7419354838709677, "no_speech_prob": 8.267522389360238e-06}, {"id": 800, "seek": 426624, "start": 4276.24, "end": 4285.12, "text": " back very likely okay so 2.16 2.19 is what we're roughly getting so there's", "tokens": [50364, 43819, 409, 264, 7542, 293, 43819, 409, 264, 3847, 293, 23287, 293, 570, 321, 434, 1242, 3126, 50664, 50664, 4470, 586, 309, 307, 264, 1389, 300, 264, 12240, 3584, 2744, 295, 613, 390, 5061, 505, 50864, 50864, 646, 588, 3700, 1392, 370, 568, 13, 6866, 568, 13, 3405, 307, 437, 321, 434, 9810, 1242, 370, 456, 311, 51308, 51308, 867, 2098, 281, 352, 490, 867, 2098, 281, 352, 490, 510, 321, 393, 2354, 15164, 264, 51492, 51492, 19618, 321, 393, 2354, 337, 1365, 2433, 365, 264, 2744, 295, 264, 18161, 2533, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.10150780928762336, "compression_ratio": 1.7419354838709677, "no_speech_prob": 8.267522389360238e-06}, {"id": 801, "seek": 426624, "start": 4285.12, "end": 4288.8, "text": " many ways to go from many ways to go from here we can continue tuning the", "tokens": [50364, 43819, 409, 264, 7542, 293, 43819, 409, 264, 3847, 293, 23287, 293, 570, 321, 434, 1242, 3126, 50664, 50664, 4470, 586, 309, 307, 264, 1389, 300, 264, 12240, 3584, 2744, 295, 613, 390, 5061, 505, 50864, 50864, 646, 588, 3700, 1392, 370, 568, 13, 6866, 568, 13, 3405, 307, 437, 321, 434, 9810, 1242, 370, 456, 311, 51308, 51308, 867, 2098, 281, 352, 490, 867, 2098, 281, 352, 490, 510, 321, 393, 2354, 15164, 264, 51492, 51492, 19618, 321, 393, 2354, 337, 1365, 2433, 365, 264, 2744, 295, 264, 18161, 2533, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.10150780928762336, "compression_ratio": 1.7419354838709677, "no_speech_prob": 8.267522389360238e-06}, {"id": 802, "seek": 426624, "start": 4288.8, "end": 4293.2, "text": " optimization we can continue for example playing with the size of the neural net", "tokens": [50364, 43819, 409, 264, 7542, 293, 43819, 409, 264, 3847, 293, 23287, 293, 570, 321, 434, 1242, 3126, 50664, 50664, 4470, 586, 309, 307, 264, 1389, 300, 264, 12240, 3584, 2744, 295, 613, 390, 5061, 505, 50864, 50864, 646, 588, 3700, 1392, 370, 568, 13, 6866, 568, 13, 3405, 307, 437, 321, 434, 9810, 1242, 370, 456, 311, 51308, 51308, 867, 2098, 281, 352, 490, 867, 2098, 281, 352, 490, 510, 321, 393, 2354, 15164, 264, 51492, 51492, 19618, 321, 393, 2354, 337, 1365, 2433, 365, 264, 2744, 295, 264, 18161, 2533, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.10150780928762336, "compression_ratio": 1.7419354838709677, "no_speech_prob": 8.267522389360238e-06}, {"id": 803, "seek": 429320, "start": 4293.2, "end": 4298.599999999999, "text": " or we can increase the number of words or characters in our case that we are", "tokens": [50364, 420, 321, 393, 3488, 264, 1230, 295, 2283, 420, 4342, 294, 527, 1389, 300, 321, 366, 50634, 50634, 1940, 382, 364, 4846, 370, 2602, 295, 445, 1045, 4342, 321, 727, 312, 1940, 544, 50794, 50794, 4342, 300, 307, 364, 4846, 293, 300, 727, 3052, 3470, 264, 4470, 1392, 370, 51056, 51056, 286, 3105, 264, 3089, 4748, 370, 321, 362, 510, 2331, 11, 1360, 4439, 295, 264, 19618, 51266, 51266, 293, 294, 264, 700, 2319, 11, 1360, 321, 434, 1228, 257, 2539, 3314, 295, 1958, 13, 16, 293, 550, 294, 264, 958, 51468, 51468, 2319, 11, 1360, 321, 434, 1228, 257, 2539, 3314, 295, 1958, 13, 10607, 341, 307, 264, 4470, 300, 286, 4584, 293, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.11805303218001026, "compression_ratio": 1.9262295081967213, "no_speech_prob": 3.269431908847764e-05}, {"id": 804, "seek": 429320, "start": 4298.599999999999, "end": 4301.8, "text": " taking as an input so instead of just three characters we could be taking more", "tokens": [50364, 420, 321, 393, 3488, 264, 1230, 295, 2283, 420, 4342, 294, 527, 1389, 300, 321, 366, 50634, 50634, 1940, 382, 364, 4846, 370, 2602, 295, 445, 1045, 4342, 321, 727, 312, 1940, 544, 50794, 50794, 4342, 300, 307, 364, 4846, 293, 300, 727, 3052, 3470, 264, 4470, 1392, 370, 51056, 51056, 286, 3105, 264, 3089, 4748, 370, 321, 362, 510, 2331, 11, 1360, 4439, 295, 264, 19618, 51266, 51266, 293, 294, 264, 700, 2319, 11, 1360, 321, 434, 1228, 257, 2539, 3314, 295, 1958, 13, 16, 293, 550, 294, 264, 958, 51468, 51468, 2319, 11, 1360, 321, 434, 1228, 257, 2539, 3314, 295, 1958, 13, 10607, 341, 307, 264, 4470, 300, 286, 4584, 293, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.11805303218001026, "compression_ratio": 1.9262295081967213, "no_speech_prob": 3.269431908847764e-05}, {"id": 805, "seek": 429320, "start": 4301.8, "end": 4307.04, "text": " characters that is an input and that could further improve the loss okay so", "tokens": [50364, 420, 321, 393, 3488, 264, 1230, 295, 2283, 420, 4342, 294, 527, 1389, 300, 321, 366, 50634, 50634, 1940, 382, 364, 4846, 370, 2602, 295, 445, 1045, 4342, 321, 727, 312, 1940, 544, 50794, 50794, 4342, 300, 307, 364, 4846, 293, 300, 727, 3052, 3470, 264, 4470, 1392, 370, 51056, 51056, 286, 3105, 264, 3089, 4748, 370, 321, 362, 510, 2331, 11, 1360, 4439, 295, 264, 19618, 51266, 51266, 293, 294, 264, 700, 2319, 11, 1360, 321, 434, 1228, 257, 2539, 3314, 295, 1958, 13, 16, 293, 550, 294, 264, 958, 51468, 51468, 2319, 11, 1360, 321, 434, 1228, 257, 2539, 3314, 295, 1958, 13, 10607, 341, 307, 264, 4470, 300, 286, 4584, 293, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.11805303218001026, "compression_ratio": 1.9262295081967213, "no_speech_prob": 3.269431908847764e-05}, {"id": 806, "seek": 429320, "start": 4307.04, "end": 4311.24, "text": " I changed the code slightly so we have here 200,000 steps of the optimization", "tokens": [50364, 420, 321, 393, 3488, 264, 1230, 295, 2283, 420, 4342, 294, 527, 1389, 300, 321, 366, 50634, 50634, 1940, 382, 364, 4846, 370, 2602, 295, 445, 1045, 4342, 321, 727, 312, 1940, 544, 50794, 50794, 4342, 300, 307, 364, 4846, 293, 300, 727, 3052, 3470, 264, 4470, 1392, 370, 51056, 51056, 286, 3105, 264, 3089, 4748, 370, 321, 362, 510, 2331, 11, 1360, 4439, 295, 264, 19618, 51266, 51266, 293, 294, 264, 700, 2319, 11, 1360, 321, 434, 1228, 257, 2539, 3314, 295, 1958, 13, 16, 293, 550, 294, 264, 958, 51468, 51468, 2319, 11, 1360, 321, 434, 1228, 257, 2539, 3314, 295, 1958, 13, 10607, 341, 307, 264, 4470, 300, 286, 4584, 293, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.11805303218001026, "compression_ratio": 1.9262295081967213, "no_speech_prob": 3.269431908847764e-05}, {"id": 807, "seek": 429320, "start": 4311.24, "end": 4315.28, "text": " and in the first 100,000 we're using a learning rate of 0.1 and then in the next", "tokens": [50364, 420, 321, 393, 3488, 264, 1230, 295, 2283, 420, 4342, 294, 527, 1389, 300, 321, 366, 50634, 50634, 1940, 382, 364, 4846, 370, 2602, 295, 445, 1045, 4342, 321, 727, 312, 1940, 544, 50794, 50794, 4342, 300, 307, 364, 4846, 293, 300, 727, 3052, 3470, 264, 4470, 1392, 370, 51056, 51056, 286, 3105, 264, 3089, 4748, 370, 321, 362, 510, 2331, 11, 1360, 4439, 295, 264, 19618, 51266, 51266, 293, 294, 264, 700, 2319, 11, 1360, 321, 434, 1228, 257, 2539, 3314, 295, 1958, 13, 16, 293, 550, 294, 264, 958, 51468, 51468, 2319, 11, 1360, 321, 434, 1228, 257, 2539, 3314, 295, 1958, 13, 10607, 341, 307, 264, 4470, 300, 286, 4584, 293, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.11805303218001026, "compression_ratio": 1.9262295081967213, "no_speech_prob": 3.269431908847764e-05}, {"id": 808, "seek": 429320, "start": 4315.28, "end": 4320.16, "text": " 100,000 we're using a learning rate of 0.01 this is the loss that I achieve and", "tokens": [50364, 420, 321, 393, 3488, 264, 1230, 295, 2283, 420, 4342, 294, 527, 1389, 300, 321, 366, 50634, 50634, 1940, 382, 364, 4846, 370, 2602, 295, 445, 1045, 4342, 321, 727, 312, 1940, 544, 50794, 50794, 4342, 300, 307, 364, 4846, 293, 300, 727, 3052, 3470, 264, 4470, 1392, 370, 51056, 51056, 286, 3105, 264, 3089, 4748, 370, 321, 362, 510, 2331, 11, 1360, 4439, 295, 264, 19618, 51266, 51266, 293, 294, 264, 700, 2319, 11, 1360, 321, 434, 1228, 257, 2539, 3314, 295, 1958, 13, 16, 293, 550, 294, 264, 958, 51468, 51468, 2319, 11, 1360, 321, 434, 1228, 257, 2539, 3314, 295, 1958, 13, 10607, 341, 307, 264, 4470, 300, 286, 4584, 293, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.11805303218001026, "compression_ratio": 1.9262295081967213, "no_speech_prob": 3.269431908847764e-05}, {"id": 809, "seek": 432016, "start": 4320.16, "end": 4324.48, "text": " these are the performance on the training and validation loss and in", "tokens": [50364, 613, 366, 264, 3389, 322, 264, 3097, 293, 24071, 4470, 293, 294, 50580, 50580, 1729, 264, 1151, 24071, 4470, 286, 600, 668, 1075, 281, 12701, 294, 264, 1036, 2217, 50742, 50742, 2077, 420, 370, 307, 568, 13, 7773, 370, 586, 286, 7980, 291, 281, 4224, 341, 1230, 293, 291, 362, 1596, 257, 51034, 51034, 1326, 46999, 2435, 281, 291, 281, 286, 519, 27650, 341, 1230, 370, 1230, 472, 291, 393, 51272, 51272, 295, 1164, 1319, 264, 1230, 295, 22027, 294, 264, 7633, 4583, 295, 341, 2316, 291, 51454, 51454, 393, 1319, 264, 10139, 1860, 295, 264, 12240, 3584, 574, 1010, 3199, 291, 393, 1319, 264, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.09029168820162432, "compression_ratio": 1.8373015873015872, "no_speech_prob": 3.590935739339329e-05}, {"id": 810, "seek": 432016, "start": 4324.48, "end": 4327.72, "text": " particular the best validation loss I've been able to obtain in the last 30", "tokens": [50364, 613, 366, 264, 3389, 322, 264, 3097, 293, 24071, 4470, 293, 294, 50580, 50580, 1729, 264, 1151, 24071, 4470, 286, 600, 668, 1075, 281, 12701, 294, 264, 1036, 2217, 50742, 50742, 2077, 420, 370, 307, 568, 13, 7773, 370, 586, 286, 7980, 291, 281, 4224, 341, 1230, 293, 291, 362, 1596, 257, 51034, 51034, 1326, 46999, 2435, 281, 291, 281, 286, 519, 27650, 341, 1230, 370, 1230, 472, 291, 393, 51272, 51272, 295, 1164, 1319, 264, 1230, 295, 22027, 294, 264, 7633, 4583, 295, 341, 2316, 291, 51454, 51454, 393, 1319, 264, 10139, 1860, 295, 264, 12240, 3584, 574, 1010, 3199, 291, 393, 1319, 264, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.09029168820162432, "compression_ratio": 1.8373015873015872, "no_speech_prob": 3.590935739339329e-05}, {"id": 811, "seek": 432016, "start": 4327.72, "end": 4333.5599999999995, "text": " minutes or so is 2.17 so now I invite you to beat this number and you have quite a", "tokens": [50364, 613, 366, 264, 3389, 322, 264, 3097, 293, 24071, 4470, 293, 294, 50580, 50580, 1729, 264, 1151, 24071, 4470, 286, 600, 668, 1075, 281, 12701, 294, 264, 1036, 2217, 50742, 50742, 2077, 420, 370, 307, 568, 13, 7773, 370, 586, 286, 7980, 291, 281, 4224, 341, 1230, 293, 291, 362, 1596, 257, 51034, 51034, 1326, 46999, 2435, 281, 291, 281, 286, 519, 27650, 341, 1230, 370, 1230, 472, 291, 393, 51272, 51272, 295, 1164, 1319, 264, 1230, 295, 22027, 294, 264, 7633, 4583, 295, 341, 2316, 291, 51454, 51454, 393, 1319, 264, 10139, 1860, 295, 264, 12240, 3584, 574, 1010, 3199, 291, 393, 1319, 264, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.09029168820162432, "compression_ratio": 1.8373015873015872, "no_speech_prob": 3.590935739339329e-05}, {"id": 812, "seek": 432016, "start": 4333.5599999999995, "end": 4338.32, "text": " few knobs available to you to I think surpass this number so number one you can", "tokens": [50364, 613, 366, 264, 3389, 322, 264, 3097, 293, 24071, 4470, 293, 294, 50580, 50580, 1729, 264, 1151, 24071, 4470, 286, 600, 668, 1075, 281, 12701, 294, 264, 1036, 2217, 50742, 50742, 2077, 420, 370, 307, 568, 13, 7773, 370, 586, 286, 7980, 291, 281, 4224, 341, 1230, 293, 291, 362, 1596, 257, 51034, 51034, 1326, 46999, 2435, 281, 291, 281, 286, 519, 27650, 341, 1230, 370, 1230, 472, 291, 393, 51272, 51272, 295, 1164, 1319, 264, 1230, 295, 22027, 294, 264, 7633, 4583, 295, 341, 2316, 291, 51454, 51454, 393, 1319, 264, 10139, 1860, 295, 264, 12240, 3584, 574, 1010, 3199, 291, 393, 1319, 264, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.09029168820162432, "compression_ratio": 1.8373015873015872, "no_speech_prob": 3.590935739339329e-05}, {"id": 813, "seek": 432016, "start": 4338.32, "end": 4341.96, "text": " of course change the number of neurons in the hidden layer of this model you", "tokens": [50364, 613, 366, 264, 3389, 322, 264, 3097, 293, 24071, 4470, 293, 294, 50580, 50580, 1729, 264, 1151, 24071, 4470, 286, 600, 668, 1075, 281, 12701, 294, 264, 1036, 2217, 50742, 50742, 2077, 420, 370, 307, 568, 13, 7773, 370, 586, 286, 7980, 291, 281, 4224, 341, 1230, 293, 291, 362, 1596, 257, 51034, 51034, 1326, 46999, 2435, 281, 291, 281, 286, 519, 27650, 341, 1230, 370, 1230, 472, 291, 393, 51272, 51272, 295, 1164, 1319, 264, 1230, 295, 22027, 294, 264, 7633, 4583, 295, 341, 2316, 291, 51454, 51454, 393, 1319, 264, 10139, 1860, 295, 264, 12240, 3584, 574, 1010, 3199, 291, 393, 1319, 264, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.09029168820162432, "compression_ratio": 1.8373015873015872, "no_speech_prob": 3.590935739339329e-05}, {"id": 814, "seek": 432016, "start": 4341.96, "end": 4346.24, "text": " can change the dimensionality of the embedding lookup table you can change the", "tokens": [50364, 613, 366, 264, 3389, 322, 264, 3097, 293, 24071, 4470, 293, 294, 50580, 50580, 1729, 264, 1151, 24071, 4470, 286, 600, 668, 1075, 281, 12701, 294, 264, 1036, 2217, 50742, 50742, 2077, 420, 370, 307, 568, 13, 7773, 370, 586, 286, 7980, 291, 281, 4224, 341, 1230, 293, 291, 362, 1596, 257, 51034, 51034, 1326, 46999, 2435, 281, 291, 281, 286, 519, 27650, 341, 1230, 370, 1230, 472, 291, 393, 51272, 51272, 295, 1164, 1319, 264, 1230, 295, 22027, 294, 264, 7633, 4583, 295, 341, 2316, 291, 51454, 51454, 393, 1319, 264, 10139, 1860, 295, 264, 12240, 3584, 574, 1010, 3199, 291, 393, 1319, 264, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.09029168820162432, "compression_ratio": 1.8373015873015872, "no_speech_prob": 3.590935739339329e-05}, {"id": 815, "seek": 434624, "start": 4346.24, "end": 4351.0, "text": " number of characters that are feeding in as an input as the context into this", "tokens": [50364, 1230, 295, 4342, 300, 366, 12919, 294, 382, 364, 4846, 382, 264, 4319, 666, 341, 50602, 50602, 2316, 293, 550, 295, 1164, 291, 393, 1319, 264, 4365, 295, 264, 19618, 577, 938, 50830, 50830, 366, 321, 2614, 689, 307, 264, 2539, 3314, 577, 775, 309, 1319, 670, 565, 577, 51038, 51038, 775, 309, 21039, 291, 393, 1319, 264, 15245, 2744, 293, 291, 815, 312, 1075, 281, 767, 51228, 51228, 4584, 257, 709, 1101, 32181, 3073, 294, 2115, 295, 577, 867, 3949, 420, 2077, 51492, 51492, 309, 2516, 281, 3847, 264, 2316, 293, 483, 428, 1874, 294, 2115, 295, 534, 665, 4470, 293, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08290774867219745, "compression_ratio": 1.8503937007874016, "no_speech_prob": 1.8922783056041226e-05}, {"id": 816, "seek": 434624, "start": 4351.0, "end": 4355.5599999999995, "text": " model and then of course you can change the details of the optimization how long", "tokens": [50364, 1230, 295, 4342, 300, 366, 12919, 294, 382, 364, 4846, 382, 264, 4319, 666, 341, 50602, 50602, 2316, 293, 550, 295, 1164, 291, 393, 1319, 264, 4365, 295, 264, 19618, 577, 938, 50830, 50830, 366, 321, 2614, 689, 307, 264, 2539, 3314, 577, 775, 309, 1319, 670, 565, 577, 51038, 51038, 775, 309, 21039, 291, 393, 1319, 264, 15245, 2744, 293, 291, 815, 312, 1075, 281, 767, 51228, 51228, 4584, 257, 709, 1101, 32181, 3073, 294, 2115, 295, 577, 867, 3949, 420, 2077, 51492, 51492, 309, 2516, 281, 3847, 264, 2316, 293, 483, 428, 1874, 294, 2115, 295, 534, 665, 4470, 293, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08290774867219745, "compression_ratio": 1.8503937007874016, "no_speech_prob": 1.8922783056041226e-05}, {"id": 817, "seek": 434624, "start": 4355.5599999999995, "end": 4359.719999999999, "text": " are we running where is the learning rate how does it change over time how", "tokens": [50364, 1230, 295, 4342, 300, 366, 12919, 294, 382, 364, 4846, 382, 264, 4319, 666, 341, 50602, 50602, 2316, 293, 550, 295, 1164, 291, 393, 1319, 264, 4365, 295, 264, 19618, 577, 938, 50830, 50830, 366, 321, 2614, 689, 307, 264, 2539, 3314, 577, 775, 309, 1319, 670, 565, 577, 51038, 51038, 775, 309, 21039, 291, 393, 1319, 264, 15245, 2744, 293, 291, 815, 312, 1075, 281, 767, 51228, 51228, 4584, 257, 709, 1101, 32181, 3073, 294, 2115, 295, 577, 867, 3949, 420, 2077, 51492, 51492, 309, 2516, 281, 3847, 264, 2316, 293, 483, 428, 1874, 294, 2115, 295, 534, 665, 4470, 293, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08290774867219745, "compression_ratio": 1.8503937007874016, "no_speech_prob": 1.8922783056041226e-05}, {"id": 818, "seek": 434624, "start": 4359.719999999999, "end": 4363.5199999999995, "text": " does it decay you can change the batch size and you may be able to actually", "tokens": [50364, 1230, 295, 4342, 300, 366, 12919, 294, 382, 364, 4846, 382, 264, 4319, 666, 341, 50602, 50602, 2316, 293, 550, 295, 1164, 291, 393, 1319, 264, 4365, 295, 264, 19618, 577, 938, 50830, 50830, 366, 321, 2614, 689, 307, 264, 2539, 3314, 577, 775, 309, 1319, 670, 565, 577, 51038, 51038, 775, 309, 21039, 291, 393, 1319, 264, 15245, 2744, 293, 291, 815, 312, 1075, 281, 767, 51228, 51228, 4584, 257, 709, 1101, 32181, 3073, 294, 2115, 295, 577, 867, 3949, 420, 2077, 51492, 51492, 309, 2516, 281, 3847, 264, 2316, 293, 483, 428, 1874, 294, 2115, 295, 534, 665, 4470, 293, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08290774867219745, "compression_ratio": 1.8503937007874016, "no_speech_prob": 1.8922783056041226e-05}, {"id": 819, "seek": 434624, "start": 4363.5199999999995, "end": 4368.8, "text": " achieve a much better convergence speed in terms of how many seconds or minutes", "tokens": [50364, 1230, 295, 4342, 300, 366, 12919, 294, 382, 364, 4846, 382, 264, 4319, 666, 341, 50602, 50602, 2316, 293, 550, 295, 1164, 291, 393, 1319, 264, 4365, 295, 264, 19618, 577, 938, 50830, 50830, 366, 321, 2614, 689, 307, 264, 2539, 3314, 577, 775, 309, 1319, 670, 565, 577, 51038, 51038, 775, 309, 21039, 291, 393, 1319, 264, 15245, 2744, 293, 291, 815, 312, 1075, 281, 767, 51228, 51228, 4584, 257, 709, 1101, 32181, 3073, 294, 2115, 295, 577, 867, 3949, 420, 2077, 51492, 51492, 309, 2516, 281, 3847, 264, 2316, 293, 483, 428, 1874, 294, 2115, 295, 534, 665, 4470, 293, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08290774867219745, "compression_ratio": 1.8503937007874016, "no_speech_prob": 1.8922783056041226e-05}, {"id": 820, "seek": 434624, "start": 4368.8, "end": 4375.24, "text": " it takes to train the model and get your result in terms of really good loss and", "tokens": [50364, 1230, 295, 4342, 300, 366, 12919, 294, 382, 364, 4846, 382, 264, 4319, 666, 341, 50602, 50602, 2316, 293, 550, 295, 1164, 291, 393, 1319, 264, 4365, 295, 264, 19618, 577, 938, 50830, 50830, 366, 321, 2614, 689, 307, 264, 2539, 3314, 577, 775, 309, 1319, 670, 565, 577, 51038, 51038, 775, 309, 21039, 291, 393, 1319, 264, 15245, 2744, 293, 291, 815, 312, 1075, 281, 767, 51228, 51228, 4584, 257, 709, 1101, 32181, 3073, 294, 2115, 295, 577, 867, 3949, 420, 2077, 51492, 51492, 309, 2516, 281, 3847, 264, 2316, 293, 483, 428, 1874, 294, 2115, 295, 534, 665, 4470, 293, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08290774867219745, "compression_ratio": 1.8503937007874016, "no_speech_prob": 1.8922783056041226e-05}, {"id": 821, "seek": 437524, "start": 4375.24, "end": 4379.8, "text": " then of course I actually invite you to read this paper it is 19 pages but at", "tokens": [50364, 550, 295, 1164, 286, 767, 7980, 291, 281, 1401, 341, 3035, 309, 307, 1294, 7183, 457, 412, 50592, 50592, 341, 935, 291, 820, 767, 312, 1075, 281, 1401, 257, 665, 16635, 295, 341, 3035, 293, 50764, 50764, 1223, 1238, 665, 24004, 295, 309, 293, 341, 3035, 611, 575, 1596, 257, 1326, 3487, 51028, 51028, 337, 13797, 300, 291, 393, 862, 365, 370, 439, 729, 366, 46999, 2435, 281, 291, 51244, 51244, 293, 291, 820, 312, 1075, 281, 4224, 341, 1230, 286, 478, 5012, 300, 382, 364, 5380, 51420, 51420, 281, 264, 15149, 293, 300, 311, 309, 337, 586, 293, 286, 603, 536, 291, 958, 565, 51688, 51782], "temperature": 0.0, "avg_logprob": -0.09626786643211994, "compression_ratio": 1.825910931174089, "no_speech_prob": 3.6469722545007244e-05}, {"id": 822, "seek": 437524, "start": 4379.8, "end": 4383.24, "text": " this point you should actually be able to read a good chunk of this paper and", "tokens": [50364, 550, 295, 1164, 286, 767, 7980, 291, 281, 1401, 341, 3035, 309, 307, 1294, 7183, 457, 412, 50592, 50592, 341, 935, 291, 820, 767, 312, 1075, 281, 1401, 257, 665, 16635, 295, 341, 3035, 293, 50764, 50764, 1223, 1238, 665, 24004, 295, 309, 293, 341, 3035, 611, 575, 1596, 257, 1326, 3487, 51028, 51028, 337, 13797, 300, 291, 393, 862, 365, 370, 439, 729, 366, 46999, 2435, 281, 291, 51244, 51244, 293, 291, 820, 312, 1075, 281, 4224, 341, 1230, 286, 478, 5012, 300, 382, 364, 5380, 51420, 51420, 281, 264, 15149, 293, 300, 311, 309, 337, 586, 293, 286, 603, 536, 291, 958, 565, 51688, 51782], "temperature": 0.0, "avg_logprob": -0.09626786643211994, "compression_ratio": 1.825910931174089, "no_speech_prob": 3.6469722545007244e-05}, {"id": 823, "seek": 437524, "start": 4383.24, "end": 4388.5199999999995, "text": " understand pretty good chunks of it and this paper also has quite a few ideas", "tokens": [50364, 550, 295, 1164, 286, 767, 7980, 291, 281, 1401, 341, 3035, 309, 307, 1294, 7183, 457, 412, 50592, 50592, 341, 935, 291, 820, 767, 312, 1075, 281, 1401, 257, 665, 16635, 295, 341, 3035, 293, 50764, 50764, 1223, 1238, 665, 24004, 295, 309, 293, 341, 3035, 611, 575, 1596, 257, 1326, 3487, 51028, 51028, 337, 13797, 300, 291, 393, 862, 365, 370, 439, 729, 366, 46999, 2435, 281, 291, 51244, 51244, 293, 291, 820, 312, 1075, 281, 4224, 341, 1230, 286, 478, 5012, 300, 382, 364, 5380, 51420, 51420, 281, 264, 15149, 293, 300, 311, 309, 337, 586, 293, 286, 603, 536, 291, 958, 565, 51688, 51782], "temperature": 0.0, "avg_logprob": -0.09626786643211994, "compression_ratio": 1.825910931174089, "no_speech_prob": 3.6469722545007244e-05}, {"id": 824, "seek": 437524, "start": 4388.5199999999995, "end": 4392.84, "text": " for improvements that you can play with so all those are knobs available to you", "tokens": [50364, 550, 295, 1164, 286, 767, 7980, 291, 281, 1401, 341, 3035, 309, 307, 1294, 7183, 457, 412, 50592, 50592, 341, 935, 291, 820, 767, 312, 1075, 281, 1401, 257, 665, 16635, 295, 341, 3035, 293, 50764, 50764, 1223, 1238, 665, 24004, 295, 309, 293, 341, 3035, 611, 575, 1596, 257, 1326, 3487, 51028, 51028, 337, 13797, 300, 291, 393, 862, 365, 370, 439, 729, 366, 46999, 2435, 281, 291, 51244, 51244, 293, 291, 820, 312, 1075, 281, 4224, 341, 1230, 286, 478, 5012, 300, 382, 364, 5380, 51420, 51420, 281, 264, 15149, 293, 300, 311, 309, 337, 586, 293, 286, 603, 536, 291, 958, 565, 51688, 51782], "temperature": 0.0, "avg_logprob": -0.09626786643211994, "compression_ratio": 1.825910931174089, "no_speech_prob": 3.6469722545007244e-05}, {"id": 825, "seek": 437524, "start": 4392.84, "end": 4396.36, "text": " and you should be able to beat this number I'm leaving that as an exercise", "tokens": [50364, 550, 295, 1164, 286, 767, 7980, 291, 281, 1401, 341, 3035, 309, 307, 1294, 7183, 457, 412, 50592, 50592, 341, 935, 291, 820, 767, 312, 1075, 281, 1401, 257, 665, 16635, 295, 341, 3035, 293, 50764, 50764, 1223, 1238, 665, 24004, 295, 309, 293, 341, 3035, 611, 575, 1596, 257, 1326, 3487, 51028, 51028, 337, 13797, 300, 291, 393, 862, 365, 370, 439, 729, 366, 46999, 2435, 281, 291, 51244, 51244, 293, 291, 820, 312, 1075, 281, 4224, 341, 1230, 286, 478, 5012, 300, 382, 364, 5380, 51420, 51420, 281, 264, 15149, 293, 300, 311, 309, 337, 586, 293, 286, 603, 536, 291, 958, 565, 51688, 51782], "temperature": 0.0, "avg_logprob": -0.09626786643211994, "compression_ratio": 1.825910931174089, "no_speech_prob": 3.6469722545007244e-05}, {"id": 826, "seek": 437524, "start": 4396.36, "end": 4401.719999999999, "text": " to the reader and that's it for now and I'll see you next time", "tokens": [50364, 550, 295, 1164, 286, 767, 7980, 291, 281, 1401, 341, 3035, 309, 307, 1294, 7183, 457, 412, 50592, 50592, 341, 935, 291, 820, 767, 312, 1075, 281, 1401, 257, 665, 16635, 295, 341, 3035, 293, 50764, 50764, 1223, 1238, 665, 24004, 295, 309, 293, 341, 3035, 611, 575, 1596, 257, 1326, 3487, 51028, 51028, 337, 13797, 300, 291, 393, 862, 365, 370, 439, 729, 366, 46999, 2435, 281, 291, 51244, 51244, 293, 291, 820, 312, 1075, 281, 4224, 341, 1230, 286, 478, 5012, 300, 382, 364, 5380, 51420, 51420, 281, 264, 15149, 293, 300, 311, 309, 337, 586, 293, 286, 603, 536, 291, 958, 565, 51688, 51782], "temperature": 0.0, "avg_logprob": -0.09626786643211994, "compression_ratio": 1.825910931174089, "no_speech_prob": 3.6469722545007244e-05}, {"id": 827, "seek": 440172, "start": 4401.72, "end": 4408.4800000000005, "text": " before we wrap up I also wanted to show how you would sample from the model so", "tokens": [50364, 949, 321, 7019, 493, 286, 611, 1415, 281, 855, 577, 291, 576, 6889, 490, 264, 2316, 370, 50702, 50702, 321, 434, 516, 281, 8460, 945, 10938, 412, 700, 321, 1841, 365, 439, 15026, 370, 300, 311, 50986, 50986, 264, 4319, 293, 550, 1826, 321, 8460, 264, 4018, 2517, 797, 321, 434, 516, 281, 51316, 51316, 12240, 264, 2190, 4319, 1228, 264, 12240, 3584, 3199, 383, 586, 2673, 510, 264, 51692, 51692, 700, 10139, 390, 264, 2744, 295, 264, 3097, 992, 457, 510, 321, 434, 787, 1364, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.12661174138387044, "compression_ratio": 1.7079646017699115, "no_speech_prob": 7.84119765739888e-05}, {"id": 828, "seek": 440172, "start": 4408.4800000000005, "end": 4414.16, "text": " we're going to generate 20 samples at first we begin with all dots so that's", "tokens": [50364, 949, 321, 7019, 493, 286, 611, 1415, 281, 855, 577, 291, 576, 6889, 490, 264, 2316, 370, 50702, 50702, 321, 434, 516, 281, 8460, 945, 10938, 412, 700, 321, 1841, 365, 439, 15026, 370, 300, 311, 50986, 50986, 264, 4319, 293, 550, 1826, 321, 8460, 264, 4018, 2517, 797, 321, 434, 516, 281, 51316, 51316, 12240, 264, 2190, 4319, 1228, 264, 12240, 3584, 3199, 383, 586, 2673, 510, 264, 51692, 51692, 700, 10139, 390, 264, 2744, 295, 264, 3097, 992, 457, 510, 321, 434, 787, 1364, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.12661174138387044, "compression_ratio": 1.7079646017699115, "no_speech_prob": 7.84119765739888e-05}, {"id": 829, "seek": 440172, "start": 4414.16, "end": 4420.76, "text": " the context and then until we generate the zero character again we're going to", "tokens": [50364, 949, 321, 7019, 493, 286, 611, 1415, 281, 855, 577, 291, 576, 6889, 490, 264, 2316, 370, 50702, 50702, 321, 434, 516, 281, 8460, 945, 10938, 412, 700, 321, 1841, 365, 439, 15026, 370, 300, 311, 50986, 50986, 264, 4319, 293, 550, 1826, 321, 8460, 264, 4018, 2517, 797, 321, 434, 516, 281, 51316, 51316, 12240, 264, 2190, 4319, 1228, 264, 12240, 3584, 3199, 383, 586, 2673, 510, 264, 51692, 51692, 700, 10139, 390, 264, 2744, 295, 264, 3097, 992, 457, 510, 321, 434, 787, 1364, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.12661174138387044, "compression_ratio": 1.7079646017699115, "no_speech_prob": 7.84119765739888e-05}, {"id": 830, "seek": 440172, "start": 4420.76, "end": 4428.280000000001, "text": " embed the current context using the embedding table C now usually here the", "tokens": [50364, 949, 321, 7019, 493, 286, 611, 1415, 281, 855, 577, 291, 576, 6889, 490, 264, 2316, 370, 50702, 50702, 321, 434, 516, 281, 8460, 945, 10938, 412, 700, 321, 1841, 365, 439, 15026, 370, 300, 311, 50986, 50986, 264, 4319, 293, 550, 1826, 321, 8460, 264, 4018, 2517, 797, 321, 434, 516, 281, 51316, 51316, 12240, 264, 2190, 4319, 1228, 264, 12240, 3584, 3199, 383, 586, 2673, 510, 264, 51692, 51692, 700, 10139, 390, 264, 2744, 295, 264, 3097, 992, 457, 510, 321, 434, 787, 1364, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.12661174138387044, "compression_ratio": 1.7079646017699115, "no_speech_prob": 7.84119765739888e-05}, {"id": 831, "seek": 440172, "start": 4428.280000000001, "end": 4431.4800000000005, "text": " first dimension was the size of the training set but here we're only working", "tokens": [50364, 949, 321, 7019, 493, 286, 611, 1415, 281, 855, 577, 291, 576, 6889, 490, 264, 2316, 370, 50702, 50702, 321, 434, 516, 281, 8460, 945, 10938, 412, 700, 321, 1841, 365, 439, 15026, 370, 300, 311, 50986, 50986, 264, 4319, 293, 550, 1826, 321, 8460, 264, 4018, 2517, 797, 321, 434, 516, 281, 51316, 51316, 12240, 264, 2190, 4319, 1228, 264, 12240, 3584, 3199, 383, 586, 2673, 510, 264, 51692, 51692, 700, 10139, 390, 264, 2744, 295, 264, 3097, 992, 457, 510, 321, 434, 787, 1364, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.12661174138387044, "compression_ratio": 1.7079646017699115, "no_speech_prob": 7.84119765739888e-05}, {"id": 832, "seek": 443148, "start": 4431.48, "end": 4435.2, "text": " with a single example that we're generating so this is just the mesh in", "tokens": [50364, 365, 257, 2167, 1365, 300, 321, 434, 17746, 370, 341, 307, 445, 264, 17407, 294, 50550, 50550, 472, 445, 337, 25632, 293, 370, 341, 12240, 3584, 550, 2170, 26231, 666, 264, 50874, 50874, 917, 1785, 291, 483, 264, 3565, 1208, 586, 321, 8873, 264, 33783, 337, 300, 291, 51102, 51102, 393, 764, 264, 479, 5893, 2787, 41167, 295, 3565, 1208, 293, 300, 445, 1936, 21510, 82, 3565, 1208, 51394, 51394, 293, 1669, 552, 2408, 281, 472, 293, 2531, 281, 3278, 30867, 309, 307, 5026, 300, 51616, 51616, 456, 311, 572, 670, 33229, 1564, 321, 362, 264, 33783, 321, 6889, 490, 552, 1228, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.15185429914942328, "compression_ratio": 1.77734375, "no_speech_prob": 1.0288469638908282e-05}, {"id": 833, "seek": 443148, "start": 4435.2, "end": 4441.679999999999, "text": " one just for simplicity and so this embedding then gets projected into the", "tokens": [50364, 365, 257, 2167, 1365, 300, 321, 434, 17746, 370, 341, 307, 445, 264, 17407, 294, 50550, 50550, 472, 445, 337, 25632, 293, 370, 341, 12240, 3584, 550, 2170, 26231, 666, 264, 50874, 50874, 917, 1785, 291, 483, 264, 3565, 1208, 586, 321, 8873, 264, 33783, 337, 300, 291, 51102, 51102, 393, 764, 264, 479, 5893, 2787, 41167, 295, 3565, 1208, 293, 300, 445, 1936, 21510, 82, 3565, 1208, 51394, 51394, 293, 1669, 552, 2408, 281, 472, 293, 2531, 281, 3278, 30867, 309, 307, 5026, 300, 51616, 51616, 456, 311, 572, 670, 33229, 1564, 321, 362, 264, 33783, 321, 6889, 490, 552, 1228, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.15185429914942328, "compression_ratio": 1.77734375, "no_speech_prob": 1.0288469638908282e-05}, {"id": 834, "seek": 443148, "start": 4441.679999999999, "end": 4446.24, "text": " end state you get the logits now we calculate the probabilities for that you", "tokens": [50364, 365, 257, 2167, 1365, 300, 321, 434, 17746, 370, 341, 307, 445, 264, 17407, 294, 50550, 50550, 472, 445, 337, 25632, 293, 370, 341, 12240, 3584, 550, 2170, 26231, 666, 264, 50874, 50874, 917, 1785, 291, 483, 264, 3565, 1208, 586, 321, 8873, 264, 33783, 337, 300, 291, 51102, 51102, 393, 764, 264, 479, 5893, 2787, 41167, 295, 3565, 1208, 293, 300, 445, 1936, 21510, 82, 3565, 1208, 51394, 51394, 293, 1669, 552, 2408, 281, 472, 293, 2531, 281, 3278, 30867, 309, 307, 5026, 300, 51616, 51616, 456, 311, 572, 670, 33229, 1564, 321, 362, 264, 33783, 321, 6889, 490, 552, 1228, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.15185429914942328, "compression_ratio": 1.77734375, "no_speech_prob": 1.0288469638908282e-05}, {"id": 835, "seek": 443148, "start": 4446.24, "end": 4452.08, "text": " can use the F dot softmax of logits and that just basically exponentials logits", "tokens": [50364, 365, 257, 2167, 1365, 300, 321, 434, 17746, 370, 341, 307, 445, 264, 17407, 294, 50550, 50550, 472, 445, 337, 25632, 293, 370, 341, 12240, 3584, 550, 2170, 26231, 666, 264, 50874, 50874, 917, 1785, 291, 483, 264, 3565, 1208, 586, 321, 8873, 264, 33783, 337, 300, 291, 51102, 51102, 393, 764, 264, 479, 5893, 2787, 41167, 295, 3565, 1208, 293, 300, 445, 1936, 21510, 82, 3565, 1208, 51394, 51394, 293, 1669, 552, 2408, 281, 472, 293, 2531, 281, 3278, 30867, 309, 307, 5026, 300, 51616, 51616, 456, 311, 572, 670, 33229, 1564, 321, 362, 264, 33783, 321, 6889, 490, 552, 1228, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.15185429914942328, "compression_ratio": 1.77734375, "no_speech_prob": 1.0288469638908282e-05}, {"id": 836, "seek": 443148, "start": 4452.08, "end": 4456.5199999999995, "text": " and makes them sum to one and similar to cross entropy it is careful that", "tokens": [50364, 365, 257, 2167, 1365, 300, 321, 434, 17746, 370, 341, 307, 445, 264, 17407, 294, 50550, 50550, 472, 445, 337, 25632, 293, 370, 341, 12240, 3584, 550, 2170, 26231, 666, 264, 50874, 50874, 917, 1785, 291, 483, 264, 3565, 1208, 586, 321, 8873, 264, 33783, 337, 300, 291, 51102, 51102, 393, 764, 264, 479, 5893, 2787, 41167, 295, 3565, 1208, 293, 300, 445, 1936, 21510, 82, 3565, 1208, 51394, 51394, 293, 1669, 552, 2408, 281, 472, 293, 2531, 281, 3278, 30867, 309, 307, 5026, 300, 51616, 51616, 456, 311, 572, 670, 33229, 1564, 321, 362, 264, 33783, 321, 6889, 490, 552, 1228, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.15185429914942328, "compression_ratio": 1.77734375, "no_speech_prob": 1.0288469638908282e-05}, {"id": 837, "seek": 443148, "start": 4456.5199999999995, "end": 4461.0, "text": " there's no overflows once we have the probabilities we sample from them using", "tokens": [50364, 365, 257, 2167, 1365, 300, 321, 434, 17746, 370, 341, 307, 445, 264, 17407, 294, 50550, 50550, 472, 445, 337, 25632, 293, 370, 341, 12240, 3584, 550, 2170, 26231, 666, 264, 50874, 50874, 917, 1785, 291, 483, 264, 3565, 1208, 586, 321, 8873, 264, 33783, 337, 300, 291, 51102, 51102, 393, 764, 264, 479, 5893, 2787, 41167, 295, 3565, 1208, 293, 300, 445, 1936, 21510, 82, 3565, 1208, 51394, 51394, 293, 1669, 552, 2408, 281, 472, 293, 2531, 281, 3278, 30867, 309, 307, 5026, 300, 51616, 51616, 456, 311, 572, 670, 33229, 1564, 321, 362, 264, 33783, 321, 6889, 490, 552, 1228, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.15185429914942328, "compression_ratio": 1.77734375, "no_speech_prob": 1.0288469638908282e-05}, {"id": 838, "seek": 446100, "start": 4461.0, "end": 4465.12, "text": " torch of multinomial to get our next index and then we shift the context", "tokens": [50364, 27822, 295, 45872, 47429, 281, 483, 527, 958, 8186, 293, 550, 321, 5513, 264, 4319, 50570, 50570, 4910, 281, 34116, 264, 8186, 293, 2136, 309, 293, 550, 321, 393, 445, 979, 1429, 439, 264, 50842, 50842, 41674, 281, 13985, 293, 4482, 552, 484, 293, 370, 613, 366, 512, 1365, 10938, 51064, 51064, 293, 291, 393, 536, 300, 264, 2316, 586, 1985, 709, 1101, 370, 264, 2283, 510, 366, 709, 51268, 51268, 544, 1349, 411, 420, 1315, 411, 370, 321, 362, 721, 411, 7852, 5111, 441, 7371, 291, 458, 309, 311, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.14273431453299015, "compression_ratio": 1.8037383177570094, "no_speech_prob": 3.882732198690064e-05}, {"id": 839, "seek": 446100, "start": 4465.12, "end": 4470.56, "text": " window to append the index and record it and then we can just decode all the", "tokens": [50364, 27822, 295, 45872, 47429, 281, 483, 527, 958, 8186, 293, 550, 321, 5513, 264, 4319, 50570, 50570, 4910, 281, 34116, 264, 8186, 293, 2136, 309, 293, 550, 321, 393, 445, 979, 1429, 439, 264, 50842, 50842, 41674, 281, 13985, 293, 4482, 552, 484, 293, 370, 613, 366, 512, 1365, 10938, 51064, 51064, 293, 291, 393, 536, 300, 264, 2316, 586, 1985, 709, 1101, 370, 264, 2283, 510, 366, 709, 51268, 51268, 544, 1349, 411, 420, 1315, 411, 370, 321, 362, 721, 411, 7852, 5111, 441, 7371, 291, 458, 309, 311, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.14273431453299015, "compression_ratio": 1.8037383177570094, "no_speech_prob": 3.882732198690064e-05}, {"id": 840, "seek": 446100, "start": 4470.56, "end": 4475.0, "text": " integers to strings and print them out and so these are some example samples", "tokens": [50364, 27822, 295, 45872, 47429, 281, 483, 527, 958, 8186, 293, 550, 321, 5513, 264, 4319, 50570, 50570, 4910, 281, 34116, 264, 8186, 293, 2136, 309, 293, 550, 321, 393, 445, 979, 1429, 439, 264, 50842, 50842, 41674, 281, 13985, 293, 4482, 552, 484, 293, 370, 613, 366, 512, 1365, 10938, 51064, 51064, 293, 291, 393, 536, 300, 264, 2316, 586, 1985, 709, 1101, 370, 264, 2283, 510, 366, 709, 51268, 51268, 544, 1349, 411, 420, 1315, 411, 370, 321, 362, 721, 411, 7852, 5111, 441, 7371, 291, 458, 309, 311, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.14273431453299015, "compression_ratio": 1.8037383177570094, "no_speech_prob": 3.882732198690064e-05}, {"id": 841, "seek": 446100, "start": 4475.0, "end": 4479.08, "text": " and you can see that the model now works much better so the words here are much", "tokens": [50364, 27822, 295, 45872, 47429, 281, 483, 527, 958, 8186, 293, 550, 321, 5513, 264, 4319, 50570, 50570, 4910, 281, 34116, 264, 8186, 293, 2136, 309, 293, 550, 321, 393, 445, 979, 1429, 439, 264, 50842, 50842, 41674, 281, 13985, 293, 4482, 552, 484, 293, 370, 613, 366, 512, 1365, 10938, 51064, 51064, 293, 291, 393, 536, 300, 264, 2316, 586, 1985, 709, 1101, 370, 264, 2283, 510, 366, 709, 51268, 51268, 544, 1349, 411, 420, 1315, 411, 370, 321, 362, 721, 411, 7852, 5111, 441, 7371, 291, 458, 309, 311, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.14273431453299015, "compression_ratio": 1.8037383177570094, "no_speech_prob": 3.882732198690064e-05}, {"id": 842, "seek": 446100, "start": 4479.08, "end": 4488.72, "text": " more word like or name like so we have things like ham chose Lila you know it's", "tokens": [50364, 27822, 295, 45872, 47429, 281, 483, 527, 958, 8186, 293, 550, 321, 5513, 264, 4319, 50570, 50570, 4910, 281, 34116, 264, 8186, 293, 2136, 309, 293, 550, 321, 393, 445, 979, 1429, 439, 264, 50842, 50842, 41674, 281, 13985, 293, 4482, 552, 484, 293, 370, 613, 366, 512, 1365, 10938, 51064, 51064, 293, 291, 393, 536, 300, 264, 2316, 586, 1985, 709, 1101, 370, 264, 2283, 510, 366, 709, 51268, 51268, 544, 1349, 411, 420, 1315, 411, 370, 321, 362, 721, 411, 7852, 5111, 441, 7371, 291, 458, 309, 311, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.14273431453299015, "compression_ratio": 1.8037383177570094, "no_speech_prob": 3.882732198690064e-05}, {"id": 843, "seek": 448872, "start": 4488.72, "end": 4491.96, "text": " starting to sound a little bit more name like so we're definitely making progress", "tokens": [50364, 2891, 281, 1626, 257, 707, 857, 544, 1315, 411, 370, 321, 434, 2138, 1455, 4205, 50526, 50526, 457, 321, 393, 920, 3470, 322, 341, 2316, 1596, 257, 688, 1392, 2597, 456, 311, 512, 50752, 50752, 10882, 2701, 286, 1415, 281, 2152, 300, 286, 528, 281, 652, 613, 43782, 544, 50946, 50946, 9515, 293, 370, 300, 286, 500, 380, 528, 291, 281, 362, 281, 411, 3625, 361, 897, 347, 43782, 51142, 51142, 293, 27822, 293, 1203, 1646, 370, 286, 486, 312, 5414, 257, 2113, 281, 257, 3329, 3066, 37880, 293, 51356, 51356, 3329, 4004, 5509, 574, 411, 257, 21060, 294, 428, 11185, 293, 291, 393, 445, 352, 281, 257, 51616, 51616, 12905, 293, 291, 603, 312, 1075, 281, 14483, 439, 295, 264, 3089, 300, 291, 1866, 294, 264, 3329, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.16447442950624408, "compression_ratio": 1.7836065573770492, "no_speech_prob": 1.618616261112038e-05}, {"id": 844, "seek": 448872, "start": 4491.96, "end": 4496.4800000000005, "text": " but we can still improve on this model quite a lot okay sorry there's some", "tokens": [50364, 2891, 281, 1626, 257, 707, 857, 544, 1315, 411, 370, 321, 434, 2138, 1455, 4205, 50526, 50526, 457, 321, 393, 920, 3470, 322, 341, 2316, 1596, 257, 688, 1392, 2597, 456, 311, 512, 50752, 50752, 10882, 2701, 286, 1415, 281, 2152, 300, 286, 528, 281, 652, 613, 43782, 544, 50946, 50946, 9515, 293, 370, 300, 286, 500, 380, 528, 291, 281, 362, 281, 411, 3625, 361, 897, 347, 43782, 51142, 51142, 293, 27822, 293, 1203, 1646, 370, 286, 486, 312, 5414, 257, 2113, 281, 257, 3329, 3066, 37880, 293, 51356, 51356, 3329, 4004, 5509, 574, 411, 257, 21060, 294, 428, 11185, 293, 291, 393, 445, 352, 281, 257, 51616, 51616, 12905, 293, 291, 603, 312, 1075, 281, 14483, 439, 295, 264, 3089, 300, 291, 1866, 294, 264, 3329, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.16447442950624408, "compression_ratio": 1.7836065573770492, "no_speech_prob": 1.618616261112038e-05}, {"id": 845, "seek": 448872, "start": 4496.4800000000005, "end": 4500.360000000001, "text": " bonus content I wanted to mention that I want to make these notebooks more", "tokens": [50364, 2891, 281, 1626, 257, 707, 857, 544, 1315, 411, 370, 321, 434, 2138, 1455, 4205, 50526, 50526, 457, 321, 393, 920, 3470, 322, 341, 2316, 1596, 257, 688, 1392, 2597, 456, 311, 512, 50752, 50752, 10882, 2701, 286, 1415, 281, 2152, 300, 286, 528, 281, 652, 613, 43782, 544, 50946, 50946, 9515, 293, 370, 300, 286, 500, 380, 528, 291, 281, 362, 281, 411, 3625, 361, 897, 347, 43782, 51142, 51142, 293, 27822, 293, 1203, 1646, 370, 286, 486, 312, 5414, 257, 2113, 281, 257, 3329, 3066, 37880, 293, 51356, 51356, 3329, 4004, 5509, 574, 411, 257, 21060, 294, 428, 11185, 293, 291, 393, 445, 352, 281, 257, 51616, 51616, 12905, 293, 291, 603, 312, 1075, 281, 14483, 439, 295, 264, 3089, 300, 291, 1866, 294, 264, 3329, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.16447442950624408, "compression_ratio": 1.7836065573770492, "no_speech_prob": 1.618616261112038e-05}, {"id": 846, "seek": 448872, "start": 4500.360000000001, "end": 4504.280000000001, "text": " accessible and so that I don't want you to have to like install jibir notebooks", "tokens": [50364, 2891, 281, 1626, 257, 707, 857, 544, 1315, 411, 370, 321, 434, 2138, 1455, 4205, 50526, 50526, 457, 321, 393, 920, 3470, 322, 341, 2316, 1596, 257, 688, 1392, 2597, 456, 311, 512, 50752, 50752, 10882, 2701, 286, 1415, 281, 2152, 300, 286, 528, 281, 652, 613, 43782, 544, 50946, 50946, 9515, 293, 370, 300, 286, 500, 380, 528, 291, 281, 362, 281, 411, 3625, 361, 897, 347, 43782, 51142, 51142, 293, 27822, 293, 1203, 1646, 370, 286, 486, 312, 5414, 257, 2113, 281, 257, 3329, 3066, 37880, 293, 51356, 51356, 3329, 4004, 5509, 574, 411, 257, 21060, 294, 428, 11185, 293, 291, 393, 445, 352, 281, 257, 51616, 51616, 12905, 293, 291, 603, 312, 1075, 281, 14483, 439, 295, 264, 3089, 300, 291, 1866, 294, 264, 3329, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.16447442950624408, "compression_ratio": 1.7836065573770492, "no_speech_prob": 1.618616261112038e-05}, {"id": 847, "seek": 448872, "start": 4504.280000000001, "end": 4508.56, "text": " and torch and everything else so I will be sharing a link to a Google CoLab and", "tokens": [50364, 2891, 281, 1626, 257, 707, 857, 544, 1315, 411, 370, 321, 434, 2138, 1455, 4205, 50526, 50526, 457, 321, 393, 920, 3470, 322, 341, 2316, 1596, 257, 688, 1392, 2597, 456, 311, 512, 50752, 50752, 10882, 2701, 286, 1415, 281, 2152, 300, 286, 528, 281, 652, 613, 43782, 544, 50946, 50946, 9515, 293, 370, 300, 286, 500, 380, 528, 291, 281, 362, 281, 411, 3625, 361, 897, 347, 43782, 51142, 51142, 293, 27822, 293, 1203, 1646, 370, 286, 486, 312, 5414, 257, 2113, 281, 257, 3329, 3066, 37880, 293, 51356, 51356, 3329, 4004, 5509, 574, 411, 257, 21060, 294, 428, 11185, 293, 291, 393, 445, 352, 281, 257, 51616, 51616, 12905, 293, 291, 603, 312, 1075, 281, 14483, 439, 295, 264, 3089, 300, 291, 1866, 294, 264, 3329, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.16447442950624408, "compression_ratio": 1.7836065573770492, "no_speech_prob": 1.618616261112038e-05}, {"id": 848, "seek": 448872, "start": 4508.56, "end": 4513.76, "text": " Google Colaba look like a notebook in your browser and you can just go to a", "tokens": [50364, 2891, 281, 1626, 257, 707, 857, 544, 1315, 411, 370, 321, 434, 2138, 1455, 4205, 50526, 50526, 457, 321, 393, 920, 3470, 322, 341, 2316, 1596, 257, 688, 1392, 2597, 456, 311, 512, 50752, 50752, 10882, 2701, 286, 1415, 281, 2152, 300, 286, 528, 281, 652, 613, 43782, 544, 50946, 50946, 9515, 293, 370, 300, 286, 500, 380, 528, 291, 281, 362, 281, 411, 3625, 361, 897, 347, 43782, 51142, 51142, 293, 27822, 293, 1203, 1646, 370, 286, 486, 312, 5414, 257, 2113, 281, 257, 3329, 3066, 37880, 293, 51356, 51356, 3329, 4004, 5509, 574, 411, 257, 21060, 294, 428, 11185, 293, 291, 393, 445, 352, 281, 257, 51616, 51616, 12905, 293, 291, 603, 312, 1075, 281, 14483, 439, 295, 264, 3089, 300, 291, 1866, 294, 264, 3329, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.16447442950624408, "compression_ratio": 1.7836065573770492, "no_speech_prob": 1.618616261112038e-05}, {"id": 849, "seek": 448872, "start": 4513.76, "end": 4518.360000000001, "text": " URL and you'll be able to execute all of the code that you saw in the Google", "tokens": [50364, 2891, 281, 1626, 257, 707, 857, 544, 1315, 411, 370, 321, 434, 2138, 1455, 4205, 50526, 50526, 457, 321, 393, 920, 3470, 322, 341, 2316, 1596, 257, 688, 1392, 2597, 456, 311, 512, 50752, 50752, 10882, 2701, 286, 1415, 281, 2152, 300, 286, 528, 281, 652, 613, 43782, 544, 50946, 50946, 9515, 293, 370, 300, 286, 500, 380, 528, 291, 281, 362, 281, 411, 3625, 361, 897, 347, 43782, 51142, 51142, 293, 27822, 293, 1203, 1646, 370, 286, 486, 312, 5414, 257, 2113, 281, 257, 3329, 3066, 37880, 293, 51356, 51356, 3329, 4004, 5509, 574, 411, 257, 21060, 294, 428, 11185, 293, 291, 393, 445, 352, 281, 257, 51616, 51616, 12905, 293, 291, 603, 312, 1075, 281, 14483, 439, 295, 264, 3089, 300, 291, 1866, 294, 264, 3329, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.16447442950624408, "compression_ratio": 1.7836065573770492, "no_speech_prob": 1.618616261112038e-05}, {"id": 850, "seek": 451836, "start": 4518.36, "end": 4522.96, "text": " Colaba and so this is me executing the code in this lecture and I shortened it", "tokens": [50364, 4004, 5509, 293, 370, 341, 307, 385, 32368, 264, 3089, 294, 341, 7991, 293, 286, 45183, 309, 50594, 50594, 257, 707, 857, 457, 1936, 291, 434, 1075, 281, 3847, 264, 1900, 912, 3209, 293, 550, 50804, 50804, 7542, 293, 6889, 490, 264, 2316, 293, 1203, 307, 1919, 337, 291, 281, 411, 50978, 50978, 256, 40467, 365, 264, 3547, 558, 456, 294, 428, 11185, 572, 13260, 4818, 51190, 51190, 370, 286, 445, 1415, 281, 935, 300, 484, 293, 264, 2113, 281, 341, 486, 312, 294, 264, 960, 51362, 51362, 3855, 51476], "temperature": 0.0, "avg_logprob": -0.09659032199693762, "compression_ratio": 1.65, "no_speech_prob": 9.012443479150534e-05}, {"id": 851, "seek": 451836, "start": 4522.96, "end": 4527.16, "text": " a little bit but basically you're able to train the exact same network and then", "tokens": [50364, 4004, 5509, 293, 370, 341, 307, 385, 32368, 264, 3089, 294, 341, 7991, 293, 286, 45183, 309, 50594, 50594, 257, 707, 857, 457, 1936, 291, 434, 1075, 281, 3847, 264, 1900, 912, 3209, 293, 550, 50804, 50804, 7542, 293, 6889, 490, 264, 2316, 293, 1203, 307, 1919, 337, 291, 281, 411, 50978, 50978, 256, 40467, 365, 264, 3547, 558, 456, 294, 428, 11185, 572, 13260, 4818, 51190, 51190, 370, 286, 445, 1415, 281, 935, 300, 484, 293, 264, 2113, 281, 341, 486, 312, 294, 264, 960, 51362, 51362, 3855, 51476], "temperature": 0.0, "avg_logprob": -0.09659032199693762, "compression_ratio": 1.65, "no_speech_prob": 9.012443479150534e-05}, {"id": 852, "seek": 451836, "start": 4527.16, "end": 4530.639999999999, "text": " plot and sample from the model and everything is ready for you to like", "tokens": [50364, 4004, 5509, 293, 370, 341, 307, 385, 32368, 264, 3089, 294, 341, 7991, 293, 286, 45183, 309, 50594, 50594, 257, 707, 857, 457, 1936, 291, 434, 1075, 281, 3847, 264, 1900, 912, 3209, 293, 550, 50804, 50804, 7542, 293, 6889, 490, 264, 2316, 293, 1203, 307, 1919, 337, 291, 281, 411, 50978, 50978, 256, 40467, 365, 264, 3547, 558, 456, 294, 428, 11185, 572, 13260, 4818, 51190, 51190, 370, 286, 445, 1415, 281, 935, 300, 484, 293, 264, 2113, 281, 341, 486, 312, 294, 264, 960, 51362, 51362, 3855, 51476], "temperature": 0.0, "avg_logprob": -0.09659032199693762, "compression_ratio": 1.65, "no_speech_prob": 9.012443479150534e-05}, {"id": 853, "seek": 451836, "start": 4530.639999999999, "end": 4534.88, "text": " tinker with the numbers right there in your browser no installation necessary", "tokens": [50364, 4004, 5509, 293, 370, 341, 307, 385, 32368, 264, 3089, 294, 341, 7991, 293, 286, 45183, 309, 50594, 50594, 257, 707, 857, 457, 1936, 291, 434, 1075, 281, 3847, 264, 1900, 912, 3209, 293, 550, 50804, 50804, 7542, 293, 6889, 490, 264, 2316, 293, 1203, 307, 1919, 337, 291, 281, 411, 50978, 50978, 256, 40467, 365, 264, 3547, 558, 456, 294, 428, 11185, 572, 13260, 4818, 51190, 51190, 370, 286, 445, 1415, 281, 935, 300, 484, 293, 264, 2113, 281, 341, 486, 312, 294, 264, 960, 51362, 51362, 3855, 51476], "temperature": 0.0, "avg_logprob": -0.09659032199693762, "compression_ratio": 1.65, "no_speech_prob": 9.012443479150534e-05}, {"id": 854, "seek": 451836, "start": 4534.88, "end": 4538.32, "text": " so I just wanted to point that out and the link to this will be in the video", "tokens": [50364, 4004, 5509, 293, 370, 341, 307, 385, 32368, 264, 3089, 294, 341, 7991, 293, 286, 45183, 309, 50594, 50594, 257, 707, 857, 457, 1936, 291, 434, 1075, 281, 3847, 264, 1900, 912, 3209, 293, 550, 50804, 50804, 7542, 293, 6889, 490, 264, 2316, 293, 1203, 307, 1919, 337, 291, 281, 411, 50978, 50978, 256, 40467, 365, 264, 3547, 558, 456, 294, 428, 11185, 572, 13260, 4818, 51190, 51190, 370, 286, 445, 1415, 281, 935, 300, 484, 293, 264, 2113, 281, 341, 486, 312, 294, 264, 960, 51362, 51362, 3855, 51476], "temperature": 0.0, "avg_logprob": -0.09659032199693762, "compression_ratio": 1.65, "no_speech_prob": 9.012443479150534e-05}, {"id": 855, "seek": 453832, "start": 4538.32, "end": 4548.82, "text": " description", "tokens": [50364, 3855, 50889, 50989, 291, 51090], "temperature": 1.0, "avg_logprob": -2.680282320295061, "compression_ratio": 0.6521739130434783, "no_speech_prob": 0.000728199549484998}], "language": "en", "video_id": "TCH_1BHY58I", "entity": "Andrew Kaparthy"}}