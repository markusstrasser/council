{"video_id": "00gn1isOd70", "title": "6.8 Bias and variance | Deciding what to try next revisited --[Machine Learning | Andrew Ng]", "description": "Second Course:\nAdvanced Learning Algorithms.\n\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 527, "views": 68, "publish_date": "11/04/2022", "timestamp": 1661817600, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " You've seen how by looking at JTrain and JCV, that is the training error and the cross-validation error, or maybe even plotting a learning curve, you can try to get a sense of whether your learning algorithm has high bias or high variance. This is a procedure I routinely do when I'm training a learning algorithm. I'll often look at the training error and the cross-validation error to try to decide if my algorithm has high bias or high variance. It turns out this will help you make better decisions about what to try next in order to improve the performance of your learning algorithm. Let's look at an example. This is actually the example that you had seen earlier. If you've implemented regularized linear regression on predicting housing prices, but your algorithm makes unacceptably large errors in its predictions, what do you try next? And these were the six ideas that we had when we had looked over this slide earlier. Get more training examples, try a smaller set of features, additional features, and so on. It turns out that each of these six items either helps fix a high variance or a high bias problem. And in particular, if your learning algorithm has high bias, three of these techniques will be useful. If your learning algorithm has high variance, then a different three of these techniques will be useful. Let's see if we can figure out which is which. First one is get more training examples. We saw in the last video that if your algorithm has high bias, then if the only thing we do is get more training data, that by itself probably won't help that much. But in contrast, if your algorithm had high variance, say it was overfitting to a very small training set, then getting more training examples will help a lot. So this first option of getting more training examples helps to fix a high variance problem. How about the other five? Do you think you can figure out which of the remaining five fix high bias or high variance problems? I'm going to go through the rest of them in this video in a minute, but if you want, be free to pause the video and see if you can think through these five other things by yourself. So feel free to pause the video and just kidding, that was me pausing, not your video pausing. But seriously, if you want, go ahead and pause the video and think through it if you want or not. And we'll go over these with you in a minute. How about trying a smaller set of features? Sometimes if your learning algorithm has too many features, then it gives the algorithm too much flexibility to fit very complicated models. This is a little bit like if you had x squared, x cubed, x to the fourth, x to the fifth, and so on. And if only you were to eliminate a few of these, then your model won't be so complex and won't have such high variance. So if you suspect that your algorithm has a lot of features that are not actually relevant or helpful to predicting a housing price, or if you suspect that you had even somewhat redundant features, then eliminating or reducing the number of features will help reduce the flexibility of your algorithm to overfit the data. And so this is a tactic that will help you to fix high variance. Conversely, getting additional features that's just adding additional features is kind of the opposite of going to a smaller set of features. This will help you to fix a high bias problem. As a concrete example, if you were trying to predict the price of a house just based on the size, but it turns out that the price of a house also really depends on the number of bedrooms and on the number of floors and on the age of a house, then the algorithm will never do that well unless you add in those additional features. So that's a high bias problem because you just can't do that well on the training set when you know only the size. Because only when you tell the algorithm how many bedrooms are there, how many floors are there, what's the age of the house, that it finally has enough information to even do better on the training set. And so adding additional features is a way to fix a high bias problem. Adding polynomial features is a little bit like adding additional features. So if your linear function, straight line, can't fit the training set that well, then adding additional polynomial features could help you do better on the training set. And helping you do better on the training set is a way to fix a high bias problem. And then decreasing lambda means to use a lower value for the regularization parameter. And that means we're going to pay less attention to this term and pay more attention to this term to try to do better on the training set. And again, that helps you to fix a high bias problem. And finally, increasing lambda, well, that's the opposite of this. But that says you're overfitting the data. And so increasing lambda would make sense if it's overfitting the training set, just putting too much attention to fit in the training set, but at the expense of generalizing to new examples. And so increasing lambda would force the algorithm to fit a smoother function, maybe a less wiggly function and use this to fix a high variance problem. So I realized that this was a lot of stuff on the slide. But the takeaways I hope you have are, if you find that your algorithm has high variance, then the two main ways to fix that are either get more training data or simplify your model. And by simplify model, I mean, either get a smaller set of features or increase the regularization parameter lambda. So your algorithm has less flexibility to fit very complex, very wiggly curves. Conversely, if your algorithm has high bias, then that means it's not doing well even on the training set. So if that's the case, the main fixes are to make your model more powerful or to give it more flexibility to fit more complex or more wiggly functions. And so some ways to do that are to give it additional features or add these polynomial features or to decrease the regularization parameter lambda. By the way, in case you're wondering if you should fix high bias by reducing the training set size, that doesn't actually help. If you reduce the training set size, you will fit the training set better, but that tends to worsen your cross validation error and the performance of your learning algorithm. So don't randomly throw away training examples just to try to fix a high bias problem. One of my PhD students from Stanford, many years after he'd already graduated from Stanford, once said to me that while he was studying at Stanford, he learned about bias and variance and felt like he got it, he understood it. But that subsequently, after many years of work experience at a few different companies, he realized that bias and variance is one of those concepts that takes a short time to learn, but takes a lifetime to master. Those were his exact words. And I think bias and variance is one of those very powerful ideas. When I'm training learning algorithms, I almost always try to figure out if it is high bias or high variance. But the way you go about addressing it systematically is something that I think you will keep on getting better at through repeated practice. But you'll find, I think, that understanding these ideas will help you be much more effective at how you decide what to try next when developing a learning algorithm. Now I know that we did go through a lot in this video. And if you feel like, boy, there's just a lot of stuff here, it's okay, don't worry about it. Later this week in the practice labs and practice quizzes, we'll have also additional opportunities to go over these ideas so that you can get additional practice with thinking about bias and variance of different learning algorithms. So if it seems like a lot right now, it's okay. You get to practice these ideas later this week and hopefully deepen your understanding of them at that time. Before moving on, bias and variance also are very useful when thinking about how to train a neural network. So in the next video, let's take a look at these concepts applied to neural network training. Let's go on to the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.56, "text": " You've seen how by looking at JTrain and JCV, that is the training error and the cross-validation", "tokens": [50364, 509, 600, 1612, 577, 538, 1237, 412, 508, 51, 7146, 293, 49802, 53, 11, 300, 307, 264, 3097, 6713, 293, 264, 3278, 12, 3337, 327, 399, 50792, 50792, 6713, 11, 420, 1310, 754, 41178, 257, 2539, 7605, 11, 291, 393, 853, 281, 483, 257, 2020, 295, 1968, 428, 51064, 51064, 2539, 9284, 575, 1090, 12577, 420, 1090, 21977, 13, 51248, 51248, 639, 307, 257, 10747, 286, 40443, 360, 562, 286, 478, 3097, 257, 2539, 9284, 13, 51430, 51430, 286, 603, 2049, 574, 412, 264, 3097, 6713, 293, 264, 3278, 12, 3337, 327, 399, 6713, 281, 853, 281, 4536, 51638, 51638, 498, 452, 9284, 575, 1090, 12577, 420, 1090, 21977, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.15180365244547525, "compression_ratio": 1.9393939393939394, "no_speech_prob": 0.031120361760258675}, {"id": 1, "seek": 0, "start": 8.56, "end": 14.0, "text": " error, or maybe even plotting a learning curve, you can try to get a sense of whether your", "tokens": [50364, 509, 600, 1612, 577, 538, 1237, 412, 508, 51, 7146, 293, 49802, 53, 11, 300, 307, 264, 3097, 6713, 293, 264, 3278, 12, 3337, 327, 399, 50792, 50792, 6713, 11, 420, 1310, 754, 41178, 257, 2539, 7605, 11, 291, 393, 853, 281, 483, 257, 2020, 295, 1968, 428, 51064, 51064, 2539, 9284, 575, 1090, 12577, 420, 1090, 21977, 13, 51248, 51248, 639, 307, 257, 10747, 286, 40443, 360, 562, 286, 478, 3097, 257, 2539, 9284, 13, 51430, 51430, 286, 603, 2049, 574, 412, 264, 3097, 6713, 293, 264, 3278, 12, 3337, 327, 399, 6713, 281, 853, 281, 4536, 51638, 51638, 498, 452, 9284, 575, 1090, 12577, 420, 1090, 21977, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.15180365244547525, "compression_ratio": 1.9393939393939394, "no_speech_prob": 0.031120361760258675}, {"id": 2, "seek": 0, "start": 14.0, "end": 17.68, "text": " learning algorithm has high bias or high variance.", "tokens": [50364, 509, 600, 1612, 577, 538, 1237, 412, 508, 51, 7146, 293, 49802, 53, 11, 300, 307, 264, 3097, 6713, 293, 264, 3278, 12, 3337, 327, 399, 50792, 50792, 6713, 11, 420, 1310, 754, 41178, 257, 2539, 7605, 11, 291, 393, 853, 281, 483, 257, 2020, 295, 1968, 428, 51064, 51064, 2539, 9284, 575, 1090, 12577, 420, 1090, 21977, 13, 51248, 51248, 639, 307, 257, 10747, 286, 40443, 360, 562, 286, 478, 3097, 257, 2539, 9284, 13, 51430, 51430, 286, 603, 2049, 574, 412, 264, 3097, 6713, 293, 264, 3278, 12, 3337, 327, 399, 6713, 281, 853, 281, 4536, 51638, 51638, 498, 452, 9284, 575, 1090, 12577, 420, 1090, 21977, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.15180365244547525, "compression_ratio": 1.9393939393939394, "no_speech_prob": 0.031120361760258675}, {"id": 3, "seek": 0, "start": 17.68, "end": 21.32, "text": " This is a procedure I routinely do when I'm training a learning algorithm.", "tokens": [50364, 509, 600, 1612, 577, 538, 1237, 412, 508, 51, 7146, 293, 49802, 53, 11, 300, 307, 264, 3097, 6713, 293, 264, 3278, 12, 3337, 327, 399, 50792, 50792, 6713, 11, 420, 1310, 754, 41178, 257, 2539, 7605, 11, 291, 393, 853, 281, 483, 257, 2020, 295, 1968, 428, 51064, 51064, 2539, 9284, 575, 1090, 12577, 420, 1090, 21977, 13, 51248, 51248, 639, 307, 257, 10747, 286, 40443, 360, 562, 286, 478, 3097, 257, 2539, 9284, 13, 51430, 51430, 286, 603, 2049, 574, 412, 264, 3097, 6713, 293, 264, 3278, 12, 3337, 327, 399, 6713, 281, 853, 281, 4536, 51638, 51638, 498, 452, 9284, 575, 1090, 12577, 420, 1090, 21977, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.15180365244547525, "compression_ratio": 1.9393939393939394, "no_speech_prob": 0.031120361760258675}, {"id": 4, "seek": 0, "start": 21.32, "end": 25.48, "text": " I'll often look at the training error and the cross-validation error to try to decide", "tokens": [50364, 509, 600, 1612, 577, 538, 1237, 412, 508, 51, 7146, 293, 49802, 53, 11, 300, 307, 264, 3097, 6713, 293, 264, 3278, 12, 3337, 327, 399, 50792, 50792, 6713, 11, 420, 1310, 754, 41178, 257, 2539, 7605, 11, 291, 393, 853, 281, 483, 257, 2020, 295, 1968, 428, 51064, 51064, 2539, 9284, 575, 1090, 12577, 420, 1090, 21977, 13, 51248, 51248, 639, 307, 257, 10747, 286, 40443, 360, 562, 286, 478, 3097, 257, 2539, 9284, 13, 51430, 51430, 286, 603, 2049, 574, 412, 264, 3097, 6713, 293, 264, 3278, 12, 3337, 327, 399, 6713, 281, 853, 281, 4536, 51638, 51638, 498, 452, 9284, 575, 1090, 12577, 420, 1090, 21977, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.15180365244547525, "compression_ratio": 1.9393939393939394, "no_speech_prob": 0.031120361760258675}, {"id": 5, "seek": 0, "start": 25.48, "end": 28.240000000000002, "text": " if my algorithm has high bias or high variance.", "tokens": [50364, 509, 600, 1612, 577, 538, 1237, 412, 508, 51, 7146, 293, 49802, 53, 11, 300, 307, 264, 3097, 6713, 293, 264, 3278, 12, 3337, 327, 399, 50792, 50792, 6713, 11, 420, 1310, 754, 41178, 257, 2539, 7605, 11, 291, 393, 853, 281, 483, 257, 2020, 295, 1968, 428, 51064, 51064, 2539, 9284, 575, 1090, 12577, 420, 1090, 21977, 13, 51248, 51248, 639, 307, 257, 10747, 286, 40443, 360, 562, 286, 478, 3097, 257, 2539, 9284, 13, 51430, 51430, 286, 603, 2049, 574, 412, 264, 3097, 6713, 293, 264, 3278, 12, 3337, 327, 399, 6713, 281, 853, 281, 4536, 51638, 51638, 498, 452, 9284, 575, 1090, 12577, 420, 1090, 21977, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.15180365244547525, "compression_ratio": 1.9393939393939394, "no_speech_prob": 0.031120361760258675}, {"id": 6, "seek": 2824, "start": 28.24, "end": 32.8, "text": " It turns out this will help you make better decisions about what to try next in order", "tokens": [50364, 467, 4523, 484, 341, 486, 854, 291, 652, 1101, 5327, 466, 437, 281, 853, 958, 294, 1668, 50592, 50592, 281, 3470, 264, 3389, 295, 428, 2539, 9284, 13, 50750, 50750, 961, 311, 574, 412, 364, 1365, 13, 50850, 50850, 639, 307, 767, 264, 1365, 300, 291, 632, 1612, 3071, 13, 51038, 51038, 759, 291, 600, 12270, 3890, 1602, 8213, 24590, 322, 32884, 6849, 7901, 11, 457, 428, 9284, 51370, 51370, 1669, 517, 24870, 1188, 2416, 13603, 294, 1080, 21264, 11, 437, 360, 291, 853, 958, 30, 51611, 51611, 400, 613, 645, 264, 2309, 3487, 300, 321, 632, 562, 321, 632, 2956, 670, 341, 4137, 3071, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.15340295271439985, "compression_ratio": 1.6736111111111112, "no_speech_prob": 1.8161879324907204e-06}, {"id": 7, "seek": 2824, "start": 32.8, "end": 35.96, "text": " to improve the performance of your learning algorithm.", "tokens": [50364, 467, 4523, 484, 341, 486, 854, 291, 652, 1101, 5327, 466, 437, 281, 853, 958, 294, 1668, 50592, 50592, 281, 3470, 264, 3389, 295, 428, 2539, 9284, 13, 50750, 50750, 961, 311, 574, 412, 364, 1365, 13, 50850, 50850, 639, 307, 767, 264, 1365, 300, 291, 632, 1612, 3071, 13, 51038, 51038, 759, 291, 600, 12270, 3890, 1602, 8213, 24590, 322, 32884, 6849, 7901, 11, 457, 428, 9284, 51370, 51370, 1669, 517, 24870, 1188, 2416, 13603, 294, 1080, 21264, 11, 437, 360, 291, 853, 958, 30, 51611, 51611, 400, 613, 645, 264, 2309, 3487, 300, 321, 632, 562, 321, 632, 2956, 670, 341, 4137, 3071, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.15340295271439985, "compression_ratio": 1.6736111111111112, "no_speech_prob": 1.8161879324907204e-06}, {"id": 8, "seek": 2824, "start": 35.96, "end": 37.96, "text": " Let's look at an example.", "tokens": [50364, 467, 4523, 484, 341, 486, 854, 291, 652, 1101, 5327, 466, 437, 281, 853, 958, 294, 1668, 50592, 50592, 281, 3470, 264, 3389, 295, 428, 2539, 9284, 13, 50750, 50750, 961, 311, 574, 412, 364, 1365, 13, 50850, 50850, 639, 307, 767, 264, 1365, 300, 291, 632, 1612, 3071, 13, 51038, 51038, 759, 291, 600, 12270, 3890, 1602, 8213, 24590, 322, 32884, 6849, 7901, 11, 457, 428, 9284, 51370, 51370, 1669, 517, 24870, 1188, 2416, 13603, 294, 1080, 21264, 11, 437, 360, 291, 853, 958, 30, 51611, 51611, 400, 613, 645, 264, 2309, 3487, 300, 321, 632, 562, 321, 632, 2956, 670, 341, 4137, 3071, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.15340295271439985, "compression_ratio": 1.6736111111111112, "no_speech_prob": 1.8161879324907204e-06}, {"id": 9, "seek": 2824, "start": 37.96, "end": 41.72, "text": " This is actually the example that you had seen earlier.", "tokens": [50364, 467, 4523, 484, 341, 486, 854, 291, 652, 1101, 5327, 466, 437, 281, 853, 958, 294, 1668, 50592, 50592, 281, 3470, 264, 3389, 295, 428, 2539, 9284, 13, 50750, 50750, 961, 311, 574, 412, 364, 1365, 13, 50850, 50850, 639, 307, 767, 264, 1365, 300, 291, 632, 1612, 3071, 13, 51038, 51038, 759, 291, 600, 12270, 3890, 1602, 8213, 24590, 322, 32884, 6849, 7901, 11, 457, 428, 9284, 51370, 51370, 1669, 517, 24870, 1188, 2416, 13603, 294, 1080, 21264, 11, 437, 360, 291, 853, 958, 30, 51611, 51611, 400, 613, 645, 264, 2309, 3487, 300, 321, 632, 562, 321, 632, 2956, 670, 341, 4137, 3071, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.15340295271439985, "compression_ratio": 1.6736111111111112, "no_speech_prob": 1.8161879324907204e-06}, {"id": 10, "seek": 2824, "start": 41.72, "end": 48.36, "text": " If you've implemented regularized linear regression on predicting housing prices, but your algorithm", "tokens": [50364, 467, 4523, 484, 341, 486, 854, 291, 652, 1101, 5327, 466, 437, 281, 853, 958, 294, 1668, 50592, 50592, 281, 3470, 264, 3389, 295, 428, 2539, 9284, 13, 50750, 50750, 961, 311, 574, 412, 364, 1365, 13, 50850, 50850, 639, 307, 767, 264, 1365, 300, 291, 632, 1612, 3071, 13, 51038, 51038, 759, 291, 600, 12270, 3890, 1602, 8213, 24590, 322, 32884, 6849, 7901, 11, 457, 428, 9284, 51370, 51370, 1669, 517, 24870, 1188, 2416, 13603, 294, 1080, 21264, 11, 437, 360, 291, 853, 958, 30, 51611, 51611, 400, 613, 645, 264, 2309, 3487, 300, 321, 632, 562, 321, 632, 2956, 670, 341, 4137, 3071, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.15340295271439985, "compression_ratio": 1.6736111111111112, "no_speech_prob": 1.8161879324907204e-06}, {"id": 11, "seek": 2824, "start": 48.36, "end": 53.18, "text": " makes unacceptably large errors in its predictions, what do you try next?", "tokens": [50364, 467, 4523, 484, 341, 486, 854, 291, 652, 1101, 5327, 466, 437, 281, 853, 958, 294, 1668, 50592, 50592, 281, 3470, 264, 3389, 295, 428, 2539, 9284, 13, 50750, 50750, 961, 311, 574, 412, 364, 1365, 13, 50850, 50850, 639, 307, 767, 264, 1365, 300, 291, 632, 1612, 3071, 13, 51038, 51038, 759, 291, 600, 12270, 3890, 1602, 8213, 24590, 322, 32884, 6849, 7901, 11, 457, 428, 9284, 51370, 51370, 1669, 517, 24870, 1188, 2416, 13603, 294, 1080, 21264, 11, 437, 360, 291, 853, 958, 30, 51611, 51611, 400, 613, 645, 264, 2309, 3487, 300, 321, 632, 562, 321, 632, 2956, 670, 341, 4137, 3071, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.15340295271439985, "compression_ratio": 1.6736111111111112, "no_speech_prob": 1.8161879324907204e-06}, {"id": 12, "seek": 2824, "start": 53.18, "end": 57.28, "text": " And these were the six ideas that we had when we had looked over this slide earlier.", "tokens": [50364, 467, 4523, 484, 341, 486, 854, 291, 652, 1101, 5327, 466, 437, 281, 853, 958, 294, 1668, 50592, 50592, 281, 3470, 264, 3389, 295, 428, 2539, 9284, 13, 50750, 50750, 961, 311, 574, 412, 364, 1365, 13, 50850, 50850, 639, 307, 767, 264, 1365, 300, 291, 632, 1612, 3071, 13, 51038, 51038, 759, 291, 600, 12270, 3890, 1602, 8213, 24590, 322, 32884, 6849, 7901, 11, 457, 428, 9284, 51370, 51370, 1669, 517, 24870, 1188, 2416, 13603, 294, 1080, 21264, 11, 437, 360, 291, 853, 958, 30, 51611, 51611, 400, 613, 645, 264, 2309, 3487, 300, 321, 632, 562, 321, 632, 2956, 670, 341, 4137, 3071, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.15340295271439985, "compression_ratio": 1.6736111111111112, "no_speech_prob": 1.8161879324907204e-06}, {"id": 13, "seek": 5728, "start": 57.28, "end": 61.32, "text": " Get more training examples, try a smaller set of features, additional features, and", "tokens": [50364, 3240, 544, 3097, 5110, 11, 853, 257, 4356, 992, 295, 4122, 11, 4497, 4122, 11, 293, 50566, 50566, 370, 322, 13, 50616, 50616, 467, 4523, 484, 300, 1184, 295, 613, 2309, 4754, 2139, 3665, 3191, 257, 1090, 21977, 420, 257, 1090, 50974, 50974, 12577, 1154, 13, 51082, 51082, 400, 294, 1729, 11, 498, 428, 2539, 9284, 575, 1090, 12577, 11, 1045, 295, 613, 7512, 486, 51364, 51364, 312, 4420, 13, 51431, 51431, 759, 428, 2539, 9284, 575, 1090, 21977, 11, 550, 257, 819, 1045, 295, 613, 7512, 51640, 51640, 486, 312, 4420, 13, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.13531701589368053, "compression_ratio": 1.9271844660194175, "no_speech_prob": 2.090436282742303e-06}, {"id": 14, "seek": 5728, "start": 61.32, "end": 62.32, "text": " so on.", "tokens": [50364, 3240, 544, 3097, 5110, 11, 853, 257, 4356, 992, 295, 4122, 11, 4497, 4122, 11, 293, 50566, 50566, 370, 322, 13, 50616, 50616, 467, 4523, 484, 300, 1184, 295, 613, 2309, 4754, 2139, 3665, 3191, 257, 1090, 21977, 420, 257, 1090, 50974, 50974, 12577, 1154, 13, 51082, 51082, 400, 294, 1729, 11, 498, 428, 2539, 9284, 575, 1090, 12577, 11, 1045, 295, 613, 7512, 486, 51364, 51364, 312, 4420, 13, 51431, 51431, 759, 428, 2539, 9284, 575, 1090, 21977, 11, 550, 257, 819, 1045, 295, 613, 7512, 51640, 51640, 486, 312, 4420, 13, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.13531701589368053, "compression_ratio": 1.9271844660194175, "no_speech_prob": 2.090436282742303e-06}, {"id": 15, "seek": 5728, "start": 62.32, "end": 69.48, "text": " It turns out that each of these six items either helps fix a high variance or a high", "tokens": [50364, 3240, 544, 3097, 5110, 11, 853, 257, 4356, 992, 295, 4122, 11, 4497, 4122, 11, 293, 50566, 50566, 370, 322, 13, 50616, 50616, 467, 4523, 484, 300, 1184, 295, 613, 2309, 4754, 2139, 3665, 3191, 257, 1090, 21977, 420, 257, 1090, 50974, 50974, 12577, 1154, 13, 51082, 51082, 400, 294, 1729, 11, 498, 428, 2539, 9284, 575, 1090, 12577, 11, 1045, 295, 613, 7512, 486, 51364, 51364, 312, 4420, 13, 51431, 51431, 759, 428, 2539, 9284, 575, 1090, 21977, 11, 550, 257, 819, 1045, 295, 613, 7512, 51640, 51640, 486, 312, 4420, 13, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.13531701589368053, "compression_ratio": 1.9271844660194175, "no_speech_prob": 2.090436282742303e-06}, {"id": 16, "seek": 5728, "start": 69.48, "end": 71.64, "text": " bias problem.", "tokens": [50364, 3240, 544, 3097, 5110, 11, 853, 257, 4356, 992, 295, 4122, 11, 4497, 4122, 11, 293, 50566, 50566, 370, 322, 13, 50616, 50616, 467, 4523, 484, 300, 1184, 295, 613, 2309, 4754, 2139, 3665, 3191, 257, 1090, 21977, 420, 257, 1090, 50974, 50974, 12577, 1154, 13, 51082, 51082, 400, 294, 1729, 11, 498, 428, 2539, 9284, 575, 1090, 12577, 11, 1045, 295, 613, 7512, 486, 51364, 51364, 312, 4420, 13, 51431, 51431, 759, 428, 2539, 9284, 575, 1090, 21977, 11, 550, 257, 819, 1045, 295, 613, 7512, 51640, 51640, 486, 312, 4420, 13, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.13531701589368053, "compression_ratio": 1.9271844660194175, "no_speech_prob": 2.090436282742303e-06}, {"id": 17, "seek": 5728, "start": 71.64, "end": 77.28, "text": " And in particular, if your learning algorithm has high bias, three of these techniques will", "tokens": [50364, 3240, 544, 3097, 5110, 11, 853, 257, 4356, 992, 295, 4122, 11, 4497, 4122, 11, 293, 50566, 50566, 370, 322, 13, 50616, 50616, 467, 4523, 484, 300, 1184, 295, 613, 2309, 4754, 2139, 3665, 3191, 257, 1090, 21977, 420, 257, 1090, 50974, 50974, 12577, 1154, 13, 51082, 51082, 400, 294, 1729, 11, 498, 428, 2539, 9284, 575, 1090, 12577, 11, 1045, 295, 613, 7512, 486, 51364, 51364, 312, 4420, 13, 51431, 51431, 759, 428, 2539, 9284, 575, 1090, 21977, 11, 550, 257, 819, 1045, 295, 613, 7512, 51640, 51640, 486, 312, 4420, 13, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.13531701589368053, "compression_ratio": 1.9271844660194175, "no_speech_prob": 2.090436282742303e-06}, {"id": 18, "seek": 5728, "start": 77.28, "end": 78.62, "text": " be useful.", "tokens": [50364, 3240, 544, 3097, 5110, 11, 853, 257, 4356, 992, 295, 4122, 11, 4497, 4122, 11, 293, 50566, 50566, 370, 322, 13, 50616, 50616, 467, 4523, 484, 300, 1184, 295, 613, 2309, 4754, 2139, 3665, 3191, 257, 1090, 21977, 420, 257, 1090, 50974, 50974, 12577, 1154, 13, 51082, 51082, 400, 294, 1729, 11, 498, 428, 2539, 9284, 575, 1090, 12577, 11, 1045, 295, 613, 7512, 486, 51364, 51364, 312, 4420, 13, 51431, 51431, 759, 428, 2539, 9284, 575, 1090, 21977, 11, 550, 257, 819, 1045, 295, 613, 7512, 51640, 51640, 486, 312, 4420, 13, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.13531701589368053, "compression_ratio": 1.9271844660194175, "no_speech_prob": 2.090436282742303e-06}, {"id": 19, "seek": 5728, "start": 78.62, "end": 82.8, "text": " If your learning algorithm has high variance, then a different three of these techniques", "tokens": [50364, 3240, 544, 3097, 5110, 11, 853, 257, 4356, 992, 295, 4122, 11, 4497, 4122, 11, 293, 50566, 50566, 370, 322, 13, 50616, 50616, 467, 4523, 484, 300, 1184, 295, 613, 2309, 4754, 2139, 3665, 3191, 257, 1090, 21977, 420, 257, 1090, 50974, 50974, 12577, 1154, 13, 51082, 51082, 400, 294, 1729, 11, 498, 428, 2539, 9284, 575, 1090, 12577, 11, 1045, 295, 613, 7512, 486, 51364, 51364, 312, 4420, 13, 51431, 51431, 759, 428, 2539, 9284, 575, 1090, 21977, 11, 550, 257, 819, 1045, 295, 613, 7512, 51640, 51640, 486, 312, 4420, 13, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.13531701589368053, "compression_ratio": 1.9271844660194175, "no_speech_prob": 2.090436282742303e-06}, {"id": 20, "seek": 5728, "start": 82.8, "end": 83.8, "text": " will be useful.", "tokens": [50364, 3240, 544, 3097, 5110, 11, 853, 257, 4356, 992, 295, 4122, 11, 4497, 4122, 11, 293, 50566, 50566, 370, 322, 13, 50616, 50616, 467, 4523, 484, 300, 1184, 295, 613, 2309, 4754, 2139, 3665, 3191, 257, 1090, 21977, 420, 257, 1090, 50974, 50974, 12577, 1154, 13, 51082, 51082, 400, 294, 1729, 11, 498, 428, 2539, 9284, 575, 1090, 12577, 11, 1045, 295, 613, 7512, 486, 51364, 51364, 312, 4420, 13, 51431, 51431, 759, 428, 2539, 9284, 575, 1090, 21977, 11, 550, 257, 819, 1045, 295, 613, 7512, 51640, 51640, 486, 312, 4420, 13, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.13531701589368053, "compression_ratio": 1.9271844660194175, "no_speech_prob": 2.090436282742303e-06}, {"id": 21, "seek": 8380, "start": 83.8, "end": 87.6, "text": " Let's see if we can figure out which is which.", "tokens": [50364, 961, 311, 536, 498, 321, 393, 2573, 484, 597, 307, 597, 13, 50554, 50554, 2386, 472, 307, 483, 544, 3097, 5110, 13, 50770, 50770, 492, 1866, 294, 264, 1036, 960, 300, 498, 428, 9284, 575, 1090, 12577, 11, 550, 498, 264, 787, 551, 321, 360, 51072, 51072, 307, 483, 544, 3097, 1412, 11, 300, 538, 2564, 1391, 1582, 380, 854, 300, 709, 13, 51376, 51376, 583, 294, 8712, 11, 498, 428, 9284, 632, 1090, 21977, 11, 584, 309, 390, 670, 69, 2414, 281, 257, 588, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.09317893124698254, "compression_ratio": 1.6699507389162562, "no_speech_prob": 5.955028427706566e-06}, {"id": 22, "seek": 8380, "start": 87.6, "end": 91.92, "text": " First one is get more training examples.", "tokens": [50364, 961, 311, 536, 498, 321, 393, 2573, 484, 597, 307, 597, 13, 50554, 50554, 2386, 472, 307, 483, 544, 3097, 5110, 13, 50770, 50770, 492, 1866, 294, 264, 1036, 960, 300, 498, 428, 9284, 575, 1090, 12577, 11, 550, 498, 264, 787, 551, 321, 360, 51072, 51072, 307, 483, 544, 3097, 1412, 11, 300, 538, 2564, 1391, 1582, 380, 854, 300, 709, 13, 51376, 51376, 583, 294, 8712, 11, 498, 428, 9284, 632, 1090, 21977, 11, 584, 309, 390, 670, 69, 2414, 281, 257, 588, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.09317893124698254, "compression_ratio": 1.6699507389162562, "no_speech_prob": 5.955028427706566e-06}, {"id": 23, "seek": 8380, "start": 91.92, "end": 97.96, "text": " We saw in the last video that if your algorithm has high bias, then if the only thing we do", "tokens": [50364, 961, 311, 536, 498, 321, 393, 2573, 484, 597, 307, 597, 13, 50554, 50554, 2386, 472, 307, 483, 544, 3097, 5110, 13, 50770, 50770, 492, 1866, 294, 264, 1036, 960, 300, 498, 428, 9284, 575, 1090, 12577, 11, 550, 498, 264, 787, 551, 321, 360, 51072, 51072, 307, 483, 544, 3097, 1412, 11, 300, 538, 2564, 1391, 1582, 380, 854, 300, 709, 13, 51376, 51376, 583, 294, 8712, 11, 498, 428, 9284, 632, 1090, 21977, 11, 584, 309, 390, 670, 69, 2414, 281, 257, 588, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.09317893124698254, "compression_ratio": 1.6699507389162562, "no_speech_prob": 5.955028427706566e-06}, {"id": 24, "seek": 8380, "start": 97.96, "end": 104.03999999999999, "text": " is get more training data, that by itself probably won't help that much.", "tokens": [50364, 961, 311, 536, 498, 321, 393, 2573, 484, 597, 307, 597, 13, 50554, 50554, 2386, 472, 307, 483, 544, 3097, 5110, 13, 50770, 50770, 492, 1866, 294, 264, 1036, 960, 300, 498, 428, 9284, 575, 1090, 12577, 11, 550, 498, 264, 787, 551, 321, 360, 51072, 51072, 307, 483, 544, 3097, 1412, 11, 300, 538, 2564, 1391, 1582, 380, 854, 300, 709, 13, 51376, 51376, 583, 294, 8712, 11, 498, 428, 9284, 632, 1090, 21977, 11, 584, 309, 390, 670, 69, 2414, 281, 257, 588, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.09317893124698254, "compression_ratio": 1.6699507389162562, "no_speech_prob": 5.955028427706566e-06}, {"id": 25, "seek": 8380, "start": 104.03999999999999, "end": 109.24, "text": " But in contrast, if your algorithm had high variance, say it was overfitting to a very", "tokens": [50364, 961, 311, 536, 498, 321, 393, 2573, 484, 597, 307, 597, 13, 50554, 50554, 2386, 472, 307, 483, 544, 3097, 5110, 13, 50770, 50770, 492, 1866, 294, 264, 1036, 960, 300, 498, 428, 9284, 575, 1090, 12577, 11, 550, 498, 264, 787, 551, 321, 360, 51072, 51072, 307, 483, 544, 3097, 1412, 11, 300, 538, 2564, 1391, 1582, 380, 854, 300, 709, 13, 51376, 51376, 583, 294, 8712, 11, 498, 428, 9284, 632, 1090, 21977, 11, 584, 309, 390, 670, 69, 2414, 281, 257, 588, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.09317893124698254, "compression_ratio": 1.6699507389162562, "no_speech_prob": 5.955028427706566e-06}, {"id": 26, "seek": 10924, "start": 109.24, "end": 115.53999999999999, "text": " small training set, then getting more training examples will help a lot.", "tokens": [50364, 1359, 3097, 992, 11, 550, 1242, 544, 3097, 5110, 486, 854, 257, 688, 13, 50679, 50679, 407, 341, 700, 3614, 295, 1242, 544, 3097, 5110, 3665, 281, 3191, 257, 1090, 21977, 1154, 13, 51046, 51046, 1012, 466, 264, 661, 1732, 30, 51126, 51126, 1144, 291, 519, 291, 393, 2573, 484, 597, 295, 264, 8877, 1732, 3191, 1090, 12577, 420, 1090, 21977, 51392, 51392, 2740, 30, 51442, 51442, 286, 478, 516, 281, 352, 807, 264, 1472, 295, 552, 294, 341, 960, 294, 257, 3456, 11, 457, 498, 291, 528, 11, 312, 51602, 51602, 1737, 281, 10465, 264, 960, 293, 536, 498, 291, 393, 519, 807, 613, 1732, 661, 721, 538, 1803, 13, 51849, 51849], "temperature": 0.0, "avg_logprob": -0.10069188657014266, "compression_ratio": 1.8372093023255813, "no_speech_prob": 2.769379307210329e-06}, {"id": 27, "seek": 10924, "start": 115.53999999999999, "end": 122.88, "text": " So this first option of getting more training examples helps to fix a high variance problem.", "tokens": [50364, 1359, 3097, 992, 11, 550, 1242, 544, 3097, 5110, 486, 854, 257, 688, 13, 50679, 50679, 407, 341, 700, 3614, 295, 1242, 544, 3097, 5110, 3665, 281, 3191, 257, 1090, 21977, 1154, 13, 51046, 51046, 1012, 466, 264, 661, 1732, 30, 51126, 51126, 1144, 291, 519, 291, 393, 2573, 484, 597, 295, 264, 8877, 1732, 3191, 1090, 12577, 420, 1090, 21977, 51392, 51392, 2740, 30, 51442, 51442, 286, 478, 516, 281, 352, 807, 264, 1472, 295, 552, 294, 341, 960, 294, 257, 3456, 11, 457, 498, 291, 528, 11, 312, 51602, 51602, 1737, 281, 10465, 264, 960, 293, 536, 498, 291, 393, 519, 807, 613, 1732, 661, 721, 538, 1803, 13, 51849, 51849], "temperature": 0.0, "avg_logprob": -0.10069188657014266, "compression_ratio": 1.8372093023255813, "no_speech_prob": 2.769379307210329e-06}, {"id": 28, "seek": 10924, "start": 122.88, "end": 124.47999999999999, "text": " How about the other five?", "tokens": [50364, 1359, 3097, 992, 11, 550, 1242, 544, 3097, 5110, 486, 854, 257, 688, 13, 50679, 50679, 407, 341, 700, 3614, 295, 1242, 544, 3097, 5110, 3665, 281, 3191, 257, 1090, 21977, 1154, 13, 51046, 51046, 1012, 466, 264, 661, 1732, 30, 51126, 51126, 1144, 291, 519, 291, 393, 2573, 484, 597, 295, 264, 8877, 1732, 3191, 1090, 12577, 420, 1090, 21977, 51392, 51392, 2740, 30, 51442, 51442, 286, 478, 516, 281, 352, 807, 264, 1472, 295, 552, 294, 341, 960, 294, 257, 3456, 11, 457, 498, 291, 528, 11, 312, 51602, 51602, 1737, 281, 10465, 264, 960, 293, 536, 498, 291, 393, 519, 807, 613, 1732, 661, 721, 538, 1803, 13, 51849, 51849], "temperature": 0.0, "avg_logprob": -0.10069188657014266, "compression_ratio": 1.8372093023255813, "no_speech_prob": 2.769379307210329e-06}, {"id": 29, "seek": 10924, "start": 124.47999999999999, "end": 129.79999999999998, "text": " Do you think you can figure out which of the remaining five fix high bias or high variance", "tokens": [50364, 1359, 3097, 992, 11, 550, 1242, 544, 3097, 5110, 486, 854, 257, 688, 13, 50679, 50679, 407, 341, 700, 3614, 295, 1242, 544, 3097, 5110, 3665, 281, 3191, 257, 1090, 21977, 1154, 13, 51046, 51046, 1012, 466, 264, 661, 1732, 30, 51126, 51126, 1144, 291, 519, 291, 393, 2573, 484, 597, 295, 264, 8877, 1732, 3191, 1090, 12577, 420, 1090, 21977, 51392, 51392, 2740, 30, 51442, 51442, 286, 478, 516, 281, 352, 807, 264, 1472, 295, 552, 294, 341, 960, 294, 257, 3456, 11, 457, 498, 291, 528, 11, 312, 51602, 51602, 1737, 281, 10465, 264, 960, 293, 536, 498, 291, 393, 519, 807, 613, 1732, 661, 721, 538, 1803, 13, 51849, 51849], "temperature": 0.0, "avg_logprob": -0.10069188657014266, "compression_ratio": 1.8372093023255813, "no_speech_prob": 2.769379307210329e-06}, {"id": 30, "seek": 10924, "start": 129.79999999999998, "end": 130.79999999999998, "text": " problems?", "tokens": [50364, 1359, 3097, 992, 11, 550, 1242, 544, 3097, 5110, 486, 854, 257, 688, 13, 50679, 50679, 407, 341, 700, 3614, 295, 1242, 544, 3097, 5110, 3665, 281, 3191, 257, 1090, 21977, 1154, 13, 51046, 51046, 1012, 466, 264, 661, 1732, 30, 51126, 51126, 1144, 291, 519, 291, 393, 2573, 484, 597, 295, 264, 8877, 1732, 3191, 1090, 12577, 420, 1090, 21977, 51392, 51392, 2740, 30, 51442, 51442, 286, 478, 516, 281, 352, 807, 264, 1472, 295, 552, 294, 341, 960, 294, 257, 3456, 11, 457, 498, 291, 528, 11, 312, 51602, 51602, 1737, 281, 10465, 264, 960, 293, 536, 498, 291, 393, 519, 807, 613, 1732, 661, 721, 538, 1803, 13, 51849, 51849], "temperature": 0.0, "avg_logprob": -0.10069188657014266, "compression_ratio": 1.8372093023255813, "no_speech_prob": 2.769379307210329e-06}, {"id": 31, "seek": 10924, "start": 130.79999999999998, "end": 134.0, "text": " I'm going to go through the rest of them in this video in a minute, but if you want, be", "tokens": [50364, 1359, 3097, 992, 11, 550, 1242, 544, 3097, 5110, 486, 854, 257, 688, 13, 50679, 50679, 407, 341, 700, 3614, 295, 1242, 544, 3097, 5110, 3665, 281, 3191, 257, 1090, 21977, 1154, 13, 51046, 51046, 1012, 466, 264, 661, 1732, 30, 51126, 51126, 1144, 291, 519, 291, 393, 2573, 484, 597, 295, 264, 8877, 1732, 3191, 1090, 12577, 420, 1090, 21977, 51392, 51392, 2740, 30, 51442, 51442, 286, 478, 516, 281, 352, 807, 264, 1472, 295, 552, 294, 341, 960, 294, 257, 3456, 11, 457, 498, 291, 528, 11, 312, 51602, 51602, 1737, 281, 10465, 264, 960, 293, 536, 498, 291, 393, 519, 807, 613, 1732, 661, 721, 538, 1803, 13, 51849, 51849], "temperature": 0.0, "avg_logprob": -0.10069188657014266, "compression_ratio": 1.8372093023255813, "no_speech_prob": 2.769379307210329e-06}, {"id": 32, "seek": 10924, "start": 134.0, "end": 138.94, "text": " free to pause the video and see if you can think through these five other things by yourself.", "tokens": [50364, 1359, 3097, 992, 11, 550, 1242, 544, 3097, 5110, 486, 854, 257, 688, 13, 50679, 50679, 407, 341, 700, 3614, 295, 1242, 544, 3097, 5110, 3665, 281, 3191, 257, 1090, 21977, 1154, 13, 51046, 51046, 1012, 466, 264, 661, 1732, 30, 51126, 51126, 1144, 291, 519, 291, 393, 2573, 484, 597, 295, 264, 8877, 1732, 3191, 1090, 12577, 420, 1090, 21977, 51392, 51392, 2740, 30, 51442, 51442, 286, 478, 516, 281, 352, 807, 264, 1472, 295, 552, 294, 341, 960, 294, 257, 3456, 11, 457, 498, 291, 528, 11, 312, 51602, 51602, 1737, 281, 10465, 264, 960, 293, 536, 498, 291, 393, 519, 807, 613, 1732, 661, 721, 538, 1803, 13, 51849, 51849], "temperature": 0.0, "avg_logprob": -0.10069188657014266, "compression_ratio": 1.8372093023255813, "no_speech_prob": 2.769379307210329e-06}, {"id": 33, "seek": 13894, "start": 138.94, "end": 145.84, "text": " So feel free to pause the video and just kidding, that was me pausing, not your video pausing.", "tokens": [50364, 407, 841, 1737, 281, 10465, 264, 960, 293, 445, 9287, 11, 300, 390, 385, 2502, 7981, 11, 406, 428, 960, 2502, 7981, 13, 50709, 50709, 583, 6638, 11, 498, 291, 528, 11, 352, 2286, 293, 10465, 264, 960, 293, 519, 807, 309, 498, 291, 528, 50873, 50873, 420, 406, 13, 50923, 50923, 400, 321, 603, 352, 670, 613, 365, 291, 294, 257, 3456, 13, 51071, 51071, 1012, 466, 1382, 257, 4356, 992, 295, 4122, 30, 51291, 51291, 4803, 498, 428, 2539, 9284, 575, 886, 867, 4122, 11, 550, 309, 2709, 264, 9284, 51555, 51555, 886, 709, 12635, 281, 3318, 588, 6179, 5245, 13, 51775, 51775], "temperature": 0.0, "avg_logprob": -0.1444073525544639, "compression_ratio": 1.6852589641434264, "no_speech_prob": 1.8057533452520147e-05}, {"id": 34, "seek": 13894, "start": 145.84, "end": 149.12, "text": " But seriously, if you want, go ahead and pause the video and think through it if you want", "tokens": [50364, 407, 841, 1737, 281, 10465, 264, 960, 293, 445, 9287, 11, 300, 390, 385, 2502, 7981, 11, 406, 428, 960, 2502, 7981, 13, 50709, 50709, 583, 6638, 11, 498, 291, 528, 11, 352, 2286, 293, 10465, 264, 960, 293, 519, 807, 309, 498, 291, 528, 50873, 50873, 420, 406, 13, 50923, 50923, 400, 321, 603, 352, 670, 613, 365, 291, 294, 257, 3456, 13, 51071, 51071, 1012, 466, 1382, 257, 4356, 992, 295, 4122, 30, 51291, 51291, 4803, 498, 428, 2539, 9284, 575, 886, 867, 4122, 11, 550, 309, 2709, 264, 9284, 51555, 51555, 886, 709, 12635, 281, 3318, 588, 6179, 5245, 13, 51775, 51775], "temperature": 0.0, "avg_logprob": -0.1444073525544639, "compression_ratio": 1.6852589641434264, "no_speech_prob": 1.8057533452520147e-05}, {"id": 35, "seek": 13894, "start": 149.12, "end": 150.12, "text": " or not.", "tokens": [50364, 407, 841, 1737, 281, 10465, 264, 960, 293, 445, 9287, 11, 300, 390, 385, 2502, 7981, 11, 406, 428, 960, 2502, 7981, 13, 50709, 50709, 583, 6638, 11, 498, 291, 528, 11, 352, 2286, 293, 10465, 264, 960, 293, 519, 807, 309, 498, 291, 528, 50873, 50873, 420, 406, 13, 50923, 50923, 400, 321, 603, 352, 670, 613, 365, 291, 294, 257, 3456, 13, 51071, 51071, 1012, 466, 1382, 257, 4356, 992, 295, 4122, 30, 51291, 51291, 4803, 498, 428, 2539, 9284, 575, 886, 867, 4122, 11, 550, 309, 2709, 264, 9284, 51555, 51555, 886, 709, 12635, 281, 3318, 588, 6179, 5245, 13, 51775, 51775], "temperature": 0.0, "avg_logprob": -0.1444073525544639, "compression_ratio": 1.6852589641434264, "no_speech_prob": 1.8057533452520147e-05}, {"id": 36, "seek": 13894, "start": 150.12, "end": 153.07999999999998, "text": " And we'll go over these with you in a minute.", "tokens": [50364, 407, 841, 1737, 281, 10465, 264, 960, 293, 445, 9287, 11, 300, 390, 385, 2502, 7981, 11, 406, 428, 960, 2502, 7981, 13, 50709, 50709, 583, 6638, 11, 498, 291, 528, 11, 352, 2286, 293, 10465, 264, 960, 293, 519, 807, 309, 498, 291, 528, 50873, 50873, 420, 406, 13, 50923, 50923, 400, 321, 603, 352, 670, 613, 365, 291, 294, 257, 3456, 13, 51071, 51071, 1012, 466, 1382, 257, 4356, 992, 295, 4122, 30, 51291, 51291, 4803, 498, 428, 2539, 9284, 575, 886, 867, 4122, 11, 550, 309, 2709, 264, 9284, 51555, 51555, 886, 709, 12635, 281, 3318, 588, 6179, 5245, 13, 51775, 51775], "temperature": 0.0, "avg_logprob": -0.1444073525544639, "compression_ratio": 1.6852589641434264, "no_speech_prob": 1.8057533452520147e-05}, {"id": 37, "seek": 13894, "start": 153.07999999999998, "end": 157.48, "text": " How about trying a smaller set of features?", "tokens": [50364, 407, 841, 1737, 281, 10465, 264, 960, 293, 445, 9287, 11, 300, 390, 385, 2502, 7981, 11, 406, 428, 960, 2502, 7981, 13, 50709, 50709, 583, 6638, 11, 498, 291, 528, 11, 352, 2286, 293, 10465, 264, 960, 293, 519, 807, 309, 498, 291, 528, 50873, 50873, 420, 406, 13, 50923, 50923, 400, 321, 603, 352, 670, 613, 365, 291, 294, 257, 3456, 13, 51071, 51071, 1012, 466, 1382, 257, 4356, 992, 295, 4122, 30, 51291, 51291, 4803, 498, 428, 2539, 9284, 575, 886, 867, 4122, 11, 550, 309, 2709, 264, 9284, 51555, 51555, 886, 709, 12635, 281, 3318, 588, 6179, 5245, 13, 51775, 51775], "temperature": 0.0, "avg_logprob": -0.1444073525544639, "compression_ratio": 1.6852589641434264, "no_speech_prob": 1.8057533452520147e-05}, {"id": 38, "seek": 13894, "start": 157.48, "end": 162.76, "text": " Sometimes if your learning algorithm has too many features, then it gives the algorithm", "tokens": [50364, 407, 841, 1737, 281, 10465, 264, 960, 293, 445, 9287, 11, 300, 390, 385, 2502, 7981, 11, 406, 428, 960, 2502, 7981, 13, 50709, 50709, 583, 6638, 11, 498, 291, 528, 11, 352, 2286, 293, 10465, 264, 960, 293, 519, 807, 309, 498, 291, 528, 50873, 50873, 420, 406, 13, 50923, 50923, 400, 321, 603, 352, 670, 613, 365, 291, 294, 257, 3456, 13, 51071, 51071, 1012, 466, 1382, 257, 4356, 992, 295, 4122, 30, 51291, 51291, 4803, 498, 428, 2539, 9284, 575, 886, 867, 4122, 11, 550, 309, 2709, 264, 9284, 51555, 51555, 886, 709, 12635, 281, 3318, 588, 6179, 5245, 13, 51775, 51775], "temperature": 0.0, "avg_logprob": -0.1444073525544639, "compression_ratio": 1.6852589641434264, "no_speech_prob": 1.8057533452520147e-05}, {"id": 39, "seek": 13894, "start": 162.76, "end": 167.16, "text": " too much flexibility to fit very complicated models.", "tokens": [50364, 407, 841, 1737, 281, 10465, 264, 960, 293, 445, 9287, 11, 300, 390, 385, 2502, 7981, 11, 406, 428, 960, 2502, 7981, 13, 50709, 50709, 583, 6638, 11, 498, 291, 528, 11, 352, 2286, 293, 10465, 264, 960, 293, 519, 807, 309, 498, 291, 528, 50873, 50873, 420, 406, 13, 50923, 50923, 400, 321, 603, 352, 670, 613, 365, 291, 294, 257, 3456, 13, 51071, 51071, 1012, 466, 1382, 257, 4356, 992, 295, 4122, 30, 51291, 51291, 4803, 498, 428, 2539, 9284, 575, 886, 867, 4122, 11, 550, 309, 2709, 264, 9284, 51555, 51555, 886, 709, 12635, 281, 3318, 588, 6179, 5245, 13, 51775, 51775], "temperature": 0.0, "avg_logprob": -0.1444073525544639, "compression_ratio": 1.6852589641434264, "no_speech_prob": 1.8057533452520147e-05}, {"id": 40, "seek": 16716, "start": 167.16, "end": 173.28, "text": " This is a little bit like if you had x squared, x cubed, x to the fourth, x to the fifth,", "tokens": [50364, 639, 307, 257, 707, 857, 411, 498, 291, 632, 2031, 8889, 11, 2031, 36510, 11, 2031, 281, 264, 6409, 11, 2031, 281, 264, 9266, 11, 50670, 50670, 293, 370, 322, 13, 50750, 50750, 400, 498, 787, 291, 645, 281, 13819, 257, 1326, 295, 613, 11, 550, 428, 2316, 1582, 380, 312, 370, 3997, 51044, 51044, 293, 1582, 380, 362, 1270, 1090, 21977, 13, 51200, 51200, 407, 498, 291, 9091, 300, 428, 9284, 575, 257, 688, 295, 4122, 300, 366, 406, 767, 7340, 51506, 51506, 420, 4961, 281, 32884, 257, 6849, 3218, 11, 420, 498, 291, 9091, 300, 291, 632, 754, 8344, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.08870503561837333, "compression_ratio": 1.6764705882352942, "no_speech_prob": 9.080093150259927e-06}, {"id": 41, "seek": 16716, "start": 173.28, "end": 174.88, "text": " and so on.", "tokens": [50364, 639, 307, 257, 707, 857, 411, 498, 291, 632, 2031, 8889, 11, 2031, 36510, 11, 2031, 281, 264, 6409, 11, 2031, 281, 264, 9266, 11, 50670, 50670, 293, 370, 322, 13, 50750, 50750, 400, 498, 787, 291, 645, 281, 13819, 257, 1326, 295, 613, 11, 550, 428, 2316, 1582, 380, 312, 370, 3997, 51044, 51044, 293, 1582, 380, 362, 1270, 1090, 21977, 13, 51200, 51200, 407, 498, 291, 9091, 300, 428, 9284, 575, 257, 688, 295, 4122, 300, 366, 406, 767, 7340, 51506, 51506, 420, 4961, 281, 32884, 257, 6849, 3218, 11, 420, 498, 291, 9091, 300, 291, 632, 754, 8344, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.08870503561837333, "compression_ratio": 1.6764705882352942, "no_speech_prob": 9.080093150259927e-06}, {"id": 42, "seek": 16716, "start": 174.88, "end": 180.76, "text": " And if only you were to eliminate a few of these, then your model won't be so complex", "tokens": [50364, 639, 307, 257, 707, 857, 411, 498, 291, 632, 2031, 8889, 11, 2031, 36510, 11, 2031, 281, 264, 6409, 11, 2031, 281, 264, 9266, 11, 50670, 50670, 293, 370, 322, 13, 50750, 50750, 400, 498, 787, 291, 645, 281, 13819, 257, 1326, 295, 613, 11, 550, 428, 2316, 1582, 380, 312, 370, 3997, 51044, 51044, 293, 1582, 380, 362, 1270, 1090, 21977, 13, 51200, 51200, 407, 498, 291, 9091, 300, 428, 9284, 575, 257, 688, 295, 4122, 300, 366, 406, 767, 7340, 51506, 51506, 420, 4961, 281, 32884, 257, 6849, 3218, 11, 420, 498, 291, 9091, 300, 291, 632, 754, 8344, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.08870503561837333, "compression_ratio": 1.6764705882352942, "no_speech_prob": 9.080093150259927e-06}, {"id": 43, "seek": 16716, "start": 180.76, "end": 183.88, "text": " and won't have such high variance.", "tokens": [50364, 639, 307, 257, 707, 857, 411, 498, 291, 632, 2031, 8889, 11, 2031, 36510, 11, 2031, 281, 264, 6409, 11, 2031, 281, 264, 9266, 11, 50670, 50670, 293, 370, 322, 13, 50750, 50750, 400, 498, 787, 291, 645, 281, 13819, 257, 1326, 295, 613, 11, 550, 428, 2316, 1582, 380, 312, 370, 3997, 51044, 51044, 293, 1582, 380, 362, 1270, 1090, 21977, 13, 51200, 51200, 407, 498, 291, 9091, 300, 428, 9284, 575, 257, 688, 295, 4122, 300, 366, 406, 767, 7340, 51506, 51506, 420, 4961, 281, 32884, 257, 6849, 3218, 11, 420, 498, 291, 9091, 300, 291, 632, 754, 8344, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.08870503561837333, "compression_ratio": 1.6764705882352942, "no_speech_prob": 9.080093150259927e-06}, {"id": 44, "seek": 16716, "start": 183.88, "end": 190.0, "text": " So if you suspect that your algorithm has a lot of features that are not actually relevant", "tokens": [50364, 639, 307, 257, 707, 857, 411, 498, 291, 632, 2031, 8889, 11, 2031, 36510, 11, 2031, 281, 264, 6409, 11, 2031, 281, 264, 9266, 11, 50670, 50670, 293, 370, 322, 13, 50750, 50750, 400, 498, 787, 291, 645, 281, 13819, 257, 1326, 295, 613, 11, 550, 428, 2316, 1582, 380, 312, 370, 3997, 51044, 51044, 293, 1582, 380, 362, 1270, 1090, 21977, 13, 51200, 51200, 407, 498, 291, 9091, 300, 428, 9284, 575, 257, 688, 295, 4122, 300, 366, 406, 767, 7340, 51506, 51506, 420, 4961, 281, 32884, 257, 6849, 3218, 11, 420, 498, 291, 9091, 300, 291, 632, 754, 8344, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.08870503561837333, "compression_ratio": 1.6764705882352942, "no_speech_prob": 9.080093150259927e-06}, {"id": 45, "seek": 16716, "start": 190.0, "end": 195.51999999999998, "text": " or helpful to predicting a housing price, or if you suspect that you had even somewhat", "tokens": [50364, 639, 307, 257, 707, 857, 411, 498, 291, 632, 2031, 8889, 11, 2031, 36510, 11, 2031, 281, 264, 6409, 11, 2031, 281, 264, 9266, 11, 50670, 50670, 293, 370, 322, 13, 50750, 50750, 400, 498, 787, 291, 645, 281, 13819, 257, 1326, 295, 613, 11, 550, 428, 2316, 1582, 380, 312, 370, 3997, 51044, 51044, 293, 1582, 380, 362, 1270, 1090, 21977, 13, 51200, 51200, 407, 498, 291, 9091, 300, 428, 9284, 575, 257, 688, 295, 4122, 300, 366, 406, 767, 7340, 51506, 51506, 420, 4961, 281, 32884, 257, 6849, 3218, 11, 420, 498, 291, 9091, 300, 291, 632, 754, 8344, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.08870503561837333, "compression_ratio": 1.6764705882352942, "no_speech_prob": 9.080093150259927e-06}, {"id": 46, "seek": 19552, "start": 195.52, "end": 202.44, "text": " redundant features, then eliminating or reducing the number of features will help reduce the", "tokens": [50364, 40997, 4122, 11, 550, 31203, 420, 12245, 264, 1230, 295, 4122, 486, 854, 5407, 264, 50710, 50710, 12635, 295, 428, 9284, 281, 670, 6845, 264, 1412, 13, 50916, 50916, 400, 370, 341, 307, 257, 31012, 300, 486, 854, 291, 281, 3191, 1090, 21977, 13, 51112, 51112, 33247, 736, 11, 1242, 4497, 4122, 300, 311, 445, 5127, 4497, 4122, 307, 733, 295, 51334, 51334, 264, 6182, 295, 516, 281, 257, 4356, 992, 295, 4122, 13, 51548, 51548, 639, 486, 854, 291, 281, 3191, 257, 1090, 12577, 1154, 13, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.11610636868319668, "compression_ratio": 1.7644444444444445, "no_speech_prob": 2.295878630320658e-06}, {"id": 47, "seek": 19552, "start": 202.44, "end": 206.56, "text": " flexibility of your algorithm to overfit the data.", "tokens": [50364, 40997, 4122, 11, 550, 31203, 420, 12245, 264, 1230, 295, 4122, 486, 854, 5407, 264, 50710, 50710, 12635, 295, 428, 9284, 281, 670, 6845, 264, 1412, 13, 50916, 50916, 400, 370, 341, 307, 257, 31012, 300, 486, 854, 291, 281, 3191, 1090, 21977, 13, 51112, 51112, 33247, 736, 11, 1242, 4497, 4122, 300, 311, 445, 5127, 4497, 4122, 307, 733, 295, 51334, 51334, 264, 6182, 295, 516, 281, 257, 4356, 992, 295, 4122, 13, 51548, 51548, 639, 486, 854, 291, 281, 3191, 257, 1090, 12577, 1154, 13, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.11610636868319668, "compression_ratio": 1.7644444444444445, "no_speech_prob": 2.295878630320658e-06}, {"id": 48, "seek": 19552, "start": 206.56, "end": 210.48000000000002, "text": " And so this is a tactic that will help you to fix high variance.", "tokens": [50364, 40997, 4122, 11, 550, 31203, 420, 12245, 264, 1230, 295, 4122, 486, 854, 5407, 264, 50710, 50710, 12635, 295, 428, 9284, 281, 670, 6845, 264, 1412, 13, 50916, 50916, 400, 370, 341, 307, 257, 31012, 300, 486, 854, 291, 281, 3191, 1090, 21977, 13, 51112, 51112, 33247, 736, 11, 1242, 4497, 4122, 300, 311, 445, 5127, 4497, 4122, 307, 733, 295, 51334, 51334, 264, 6182, 295, 516, 281, 257, 4356, 992, 295, 4122, 13, 51548, 51548, 639, 486, 854, 291, 281, 3191, 257, 1090, 12577, 1154, 13, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.11610636868319668, "compression_ratio": 1.7644444444444445, "no_speech_prob": 2.295878630320658e-06}, {"id": 49, "seek": 19552, "start": 210.48000000000002, "end": 214.92000000000002, "text": " Conversely, getting additional features that's just adding additional features is kind of", "tokens": [50364, 40997, 4122, 11, 550, 31203, 420, 12245, 264, 1230, 295, 4122, 486, 854, 5407, 264, 50710, 50710, 12635, 295, 428, 9284, 281, 670, 6845, 264, 1412, 13, 50916, 50916, 400, 370, 341, 307, 257, 31012, 300, 486, 854, 291, 281, 3191, 1090, 21977, 13, 51112, 51112, 33247, 736, 11, 1242, 4497, 4122, 300, 311, 445, 5127, 4497, 4122, 307, 733, 295, 51334, 51334, 264, 6182, 295, 516, 281, 257, 4356, 992, 295, 4122, 13, 51548, 51548, 639, 486, 854, 291, 281, 3191, 257, 1090, 12577, 1154, 13, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.11610636868319668, "compression_ratio": 1.7644444444444445, "no_speech_prob": 2.295878630320658e-06}, {"id": 50, "seek": 19552, "start": 214.92000000000002, "end": 219.20000000000002, "text": " the opposite of going to a smaller set of features.", "tokens": [50364, 40997, 4122, 11, 550, 31203, 420, 12245, 264, 1230, 295, 4122, 486, 854, 5407, 264, 50710, 50710, 12635, 295, 428, 9284, 281, 670, 6845, 264, 1412, 13, 50916, 50916, 400, 370, 341, 307, 257, 31012, 300, 486, 854, 291, 281, 3191, 1090, 21977, 13, 51112, 51112, 33247, 736, 11, 1242, 4497, 4122, 300, 311, 445, 5127, 4497, 4122, 307, 733, 295, 51334, 51334, 264, 6182, 295, 516, 281, 257, 4356, 992, 295, 4122, 13, 51548, 51548, 639, 486, 854, 291, 281, 3191, 257, 1090, 12577, 1154, 13, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.11610636868319668, "compression_ratio": 1.7644444444444445, "no_speech_prob": 2.295878630320658e-06}, {"id": 51, "seek": 19552, "start": 219.20000000000002, "end": 223.04000000000002, "text": " This will help you to fix a high bias problem.", "tokens": [50364, 40997, 4122, 11, 550, 31203, 420, 12245, 264, 1230, 295, 4122, 486, 854, 5407, 264, 50710, 50710, 12635, 295, 428, 9284, 281, 670, 6845, 264, 1412, 13, 50916, 50916, 400, 370, 341, 307, 257, 31012, 300, 486, 854, 291, 281, 3191, 1090, 21977, 13, 51112, 51112, 33247, 736, 11, 1242, 4497, 4122, 300, 311, 445, 5127, 4497, 4122, 307, 733, 295, 51334, 51334, 264, 6182, 295, 516, 281, 257, 4356, 992, 295, 4122, 13, 51548, 51548, 639, 486, 854, 291, 281, 3191, 257, 1090, 12577, 1154, 13, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.11610636868319668, "compression_ratio": 1.7644444444444445, "no_speech_prob": 2.295878630320658e-06}, {"id": 52, "seek": 22304, "start": 223.04, "end": 226.5, "text": " As a concrete example, if you were trying to predict the price of a house just based", "tokens": [50364, 1018, 257, 9859, 1365, 11, 498, 291, 645, 1382, 281, 6069, 264, 3218, 295, 257, 1782, 445, 2361, 50537, 50537, 322, 264, 2744, 11, 457, 309, 4523, 484, 300, 264, 3218, 295, 257, 1782, 611, 534, 5946, 322, 264, 1230, 50834, 50834, 295, 39955, 293, 322, 264, 1230, 295, 21008, 293, 322, 264, 3205, 295, 257, 1782, 11, 550, 264, 9284, 51130, 51130, 486, 1128, 360, 300, 731, 5969, 291, 909, 294, 729, 4497, 4122, 13, 51300, 51300, 407, 300, 311, 257, 1090, 12577, 1154, 570, 291, 445, 393, 380, 360, 300, 731, 322, 264, 3097, 992, 51598, 51598, 562, 291, 458, 787, 264, 2744, 13, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.0992313038219105, "compression_ratio": 1.8271604938271604, "no_speech_prob": 3.340485591252218e-06}, {"id": 53, "seek": 22304, "start": 226.5, "end": 232.44, "text": " on the size, but it turns out that the price of a house also really depends on the number", "tokens": [50364, 1018, 257, 9859, 1365, 11, 498, 291, 645, 1382, 281, 6069, 264, 3218, 295, 257, 1782, 445, 2361, 50537, 50537, 322, 264, 2744, 11, 457, 309, 4523, 484, 300, 264, 3218, 295, 257, 1782, 611, 534, 5946, 322, 264, 1230, 50834, 50834, 295, 39955, 293, 322, 264, 1230, 295, 21008, 293, 322, 264, 3205, 295, 257, 1782, 11, 550, 264, 9284, 51130, 51130, 486, 1128, 360, 300, 731, 5969, 291, 909, 294, 729, 4497, 4122, 13, 51300, 51300, 407, 300, 311, 257, 1090, 12577, 1154, 570, 291, 445, 393, 380, 360, 300, 731, 322, 264, 3097, 992, 51598, 51598, 562, 291, 458, 787, 264, 2744, 13, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.0992313038219105, "compression_ratio": 1.8271604938271604, "no_speech_prob": 3.340485591252218e-06}, {"id": 54, "seek": 22304, "start": 232.44, "end": 238.35999999999999, "text": " of bedrooms and on the number of floors and on the age of a house, then the algorithm", "tokens": [50364, 1018, 257, 9859, 1365, 11, 498, 291, 645, 1382, 281, 6069, 264, 3218, 295, 257, 1782, 445, 2361, 50537, 50537, 322, 264, 2744, 11, 457, 309, 4523, 484, 300, 264, 3218, 295, 257, 1782, 611, 534, 5946, 322, 264, 1230, 50834, 50834, 295, 39955, 293, 322, 264, 1230, 295, 21008, 293, 322, 264, 3205, 295, 257, 1782, 11, 550, 264, 9284, 51130, 51130, 486, 1128, 360, 300, 731, 5969, 291, 909, 294, 729, 4497, 4122, 13, 51300, 51300, 407, 300, 311, 257, 1090, 12577, 1154, 570, 291, 445, 393, 380, 360, 300, 731, 322, 264, 3097, 992, 51598, 51598, 562, 291, 458, 787, 264, 2744, 13, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.0992313038219105, "compression_ratio": 1.8271604938271604, "no_speech_prob": 3.340485591252218e-06}, {"id": 55, "seek": 22304, "start": 238.35999999999999, "end": 241.76, "text": " will never do that well unless you add in those additional features.", "tokens": [50364, 1018, 257, 9859, 1365, 11, 498, 291, 645, 1382, 281, 6069, 264, 3218, 295, 257, 1782, 445, 2361, 50537, 50537, 322, 264, 2744, 11, 457, 309, 4523, 484, 300, 264, 3218, 295, 257, 1782, 611, 534, 5946, 322, 264, 1230, 50834, 50834, 295, 39955, 293, 322, 264, 1230, 295, 21008, 293, 322, 264, 3205, 295, 257, 1782, 11, 550, 264, 9284, 51130, 51130, 486, 1128, 360, 300, 731, 5969, 291, 909, 294, 729, 4497, 4122, 13, 51300, 51300, 407, 300, 311, 257, 1090, 12577, 1154, 570, 291, 445, 393, 380, 360, 300, 731, 322, 264, 3097, 992, 51598, 51598, 562, 291, 458, 787, 264, 2744, 13, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.0992313038219105, "compression_ratio": 1.8271604938271604, "no_speech_prob": 3.340485591252218e-06}, {"id": 56, "seek": 22304, "start": 241.76, "end": 247.72, "text": " So that's a high bias problem because you just can't do that well on the training set", "tokens": [50364, 1018, 257, 9859, 1365, 11, 498, 291, 645, 1382, 281, 6069, 264, 3218, 295, 257, 1782, 445, 2361, 50537, 50537, 322, 264, 2744, 11, 457, 309, 4523, 484, 300, 264, 3218, 295, 257, 1782, 611, 534, 5946, 322, 264, 1230, 50834, 50834, 295, 39955, 293, 322, 264, 1230, 295, 21008, 293, 322, 264, 3205, 295, 257, 1782, 11, 550, 264, 9284, 51130, 51130, 486, 1128, 360, 300, 731, 5969, 291, 909, 294, 729, 4497, 4122, 13, 51300, 51300, 407, 300, 311, 257, 1090, 12577, 1154, 570, 291, 445, 393, 380, 360, 300, 731, 322, 264, 3097, 992, 51598, 51598, 562, 291, 458, 787, 264, 2744, 13, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.0992313038219105, "compression_ratio": 1.8271604938271604, "no_speech_prob": 3.340485591252218e-06}, {"id": 57, "seek": 22304, "start": 247.72, "end": 250.16, "text": " when you know only the size.", "tokens": [50364, 1018, 257, 9859, 1365, 11, 498, 291, 645, 1382, 281, 6069, 264, 3218, 295, 257, 1782, 445, 2361, 50537, 50537, 322, 264, 2744, 11, 457, 309, 4523, 484, 300, 264, 3218, 295, 257, 1782, 611, 534, 5946, 322, 264, 1230, 50834, 50834, 295, 39955, 293, 322, 264, 1230, 295, 21008, 293, 322, 264, 3205, 295, 257, 1782, 11, 550, 264, 9284, 51130, 51130, 486, 1128, 360, 300, 731, 5969, 291, 909, 294, 729, 4497, 4122, 13, 51300, 51300, 407, 300, 311, 257, 1090, 12577, 1154, 570, 291, 445, 393, 380, 360, 300, 731, 322, 264, 3097, 992, 51598, 51598, 562, 291, 458, 787, 264, 2744, 13, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.0992313038219105, "compression_ratio": 1.8271604938271604, "no_speech_prob": 3.340485591252218e-06}, {"id": 58, "seek": 25016, "start": 250.16, "end": 254.2, "text": " Because only when you tell the algorithm how many bedrooms are there, how many floors are", "tokens": [50364, 1436, 787, 562, 291, 980, 264, 9284, 577, 867, 39955, 366, 456, 11, 577, 867, 21008, 366, 50566, 50566, 456, 11, 437, 311, 264, 3205, 295, 264, 1782, 11, 300, 309, 2721, 575, 1547, 1589, 281, 754, 360, 50746, 50746, 1101, 322, 264, 3097, 992, 13, 50880, 50880, 400, 370, 5127, 4497, 4122, 307, 257, 636, 281, 3191, 257, 1090, 12577, 1154, 13, 51188, 51188, 31204, 26110, 4122, 307, 257, 707, 857, 411, 5127, 4497, 4122, 13, 51419, 51419, 407, 498, 428, 8213, 2445, 11, 2997, 1622, 11, 393, 380, 3318, 264, 3097, 992, 300, 731, 11, 550, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.10930114633896772, "compression_ratio": 1.8016528925619835, "no_speech_prob": 1.6280338286378537e-06}, {"id": 59, "seek": 25016, "start": 254.2, "end": 257.8, "text": " there, what's the age of the house, that it finally has enough information to even do", "tokens": [50364, 1436, 787, 562, 291, 980, 264, 9284, 577, 867, 39955, 366, 456, 11, 577, 867, 21008, 366, 50566, 50566, 456, 11, 437, 311, 264, 3205, 295, 264, 1782, 11, 300, 309, 2721, 575, 1547, 1589, 281, 754, 360, 50746, 50746, 1101, 322, 264, 3097, 992, 13, 50880, 50880, 400, 370, 5127, 4497, 4122, 307, 257, 636, 281, 3191, 257, 1090, 12577, 1154, 13, 51188, 51188, 31204, 26110, 4122, 307, 257, 707, 857, 411, 5127, 4497, 4122, 13, 51419, 51419, 407, 498, 428, 8213, 2445, 11, 2997, 1622, 11, 393, 380, 3318, 264, 3097, 992, 300, 731, 11, 550, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.10930114633896772, "compression_ratio": 1.8016528925619835, "no_speech_prob": 1.6280338286378537e-06}, {"id": 60, "seek": 25016, "start": 257.8, "end": 260.48, "text": " better on the training set.", "tokens": [50364, 1436, 787, 562, 291, 980, 264, 9284, 577, 867, 39955, 366, 456, 11, 577, 867, 21008, 366, 50566, 50566, 456, 11, 437, 311, 264, 3205, 295, 264, 1782, 11, 300, 309, 2721, 575, 1547, 1589, 281, 754, 360, 50746, 50746, 1101, 322, 264, 3097, 992, 13, 50880, 50880, 400, 370, 5127, 4497, 4122, 307, 257, 636, 281, 3191, 257, 1090, 12577, 1154, 13, 51188, 51188, 31204, 26110, 4122, 307, 257, 707, 857, 411, 5127, 4497, 4122, 13, 51419, 51419, 407, 498, 428, 8213, 2445, 11, 2997, 1622, 11, 393, 380, 3318, 264, 3097, 992, 300, 731, 11, 550, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.10930114633896772, "compression_ratio": 1.8016528925619835, "no_speech_prob": 1.6280338286378537e-06}, {"id": 61, "seek": 25016, "start": 260.48, "end": 266.64, "text": " And so adding additional features is a way to fix a high bias problem.", "tokens": [50364, 1436, 787, 562, 291, 980, 264, 9284, 577, 867, 39955, 366, 456, 11, 577, 867, 21008, 366, 50566, 50566, 456, 11, 437, 311, 264, 3205, 295, 264, 1782, 11, 300, 309, 2721, 575, 1547, 1589, 281, 754, 360, 50746, 50746, 1101, 322, 264, 3097, 992, 13, 50880, 50880, 400, 370, 5127, 4497, 4122, 307, 257, 636, 281, 3191, 257, 1090, 12577, 1154, 13, 51188, 51188, 31204, 26110, 4122, 307, 257, 707, 857, 411, 5127, 4497, 4122, 13, 51419, 51419, 407, 498, 428, 8213, 2445, 11, 2997, 1622, 11, 393, 380, 3318, 264, 3097, 992, 300, 731, 11, 550, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.10930114633896772, "compression_ratio": 1.8016528925619835, "no_speech_prob": 1.6280338286378537e-06}, {"id": 62, "seek": 25016, "start": 266.64, "end": 271.26, "text": " Adding polynomial features is a little bit like adding additional features.", "tokens": [50364, 1436, 787, 562, 291, 980, 264, 9284, 577, 867, 39955, 366, 456, 11, 577, 867, 21008, 366, 50566, 50566, 456, 11, 437, 311, 264, 3205, 295, 264, 1782, 11, 300, 309, 2721, 575, 1547, 1589, 281, 754, 360, 50746, 50746, 1101, 322, 264, 3097, 992, 13, 50880, 50880, 400, 370, 5127, 4497, 4122, 307, 257, 636, 281, 3191, 257, 1090, 12577, 1154, 13, 51188, 51188, 31204, 26110, 4122, 307, 257, 707, 857, 411, 5127, 4497, 4122, 13, 51419, 51419, 407, 498, 428, 8213, 2445, 11, 2997, 1622, 11, 393, 380, 3318, 264, 3097, 992, 300, 731, 11, 550, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.10930114633896772, "compression_ratio": 1.8016528925619835, "no_speech_prob": 1.6280338286378537e-06}, {"id": 63, "seek": 25016, "start": 271.26, "end": 276.36, "text": " So if your linear function, straight line, can't fit the training set that well, then", "tokens": [50364, 1436, 787, 562, 291, 980, 264, 9284, 577, 867, 39955, 366, 456, 11, 577, 867, 21008, 366, 50566, 50566, 456, 11, 437, 311, 264, 3205, 295, 264, 1782, 11, 300, 309, 2721, 575, 1547, 1589, 281, 754, 360, 50746, 50746, 1101, 322, 264, 3097, 992, 13, 50880, 50880, 400, 370, 5127, 4497, 4122, 307, 257, 636, 281, 3191, 257, 1090, 12577, 1154, 13, 51188, 51188, 31204, 26110, 4122, 307, 257, 707, 857, 411, 5127, 4497, 4122, 13, 51419, 51419, 407, 498, 428, 8213, 2445, 11, 2997, 1622, 11, 393, 380, 3318, 264, 3097, 992, 300, 731, 11, 550, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.10930114633896772, "compression_ratio": 1.8016528925619835, "no_speech_prob": 1.6280338286378537e-06}, {"id": 64, "seek": 27636, "start": 276.36, "end": 281.16, "text": " adding additional polynomial features could help you do better on the training set.", "tokens": [50364, 5127, 4497, 26110, 4122, 727, 854, 291, 360, 1101, 322, 264, 3097, 992, 13, 50604, 50604, 400, 4315, 291, 360, 1101, 322, 264, 3097, 992, 307, 257, 636, 281, 3191, 257, 1090, 12577, 1154, 13, 50868, 50868, 400, 550, 23223, 13607, 1355, 281, 764, 257, 3126, 2158, 337, 264, 3890, 2144, 13075, 13, 51266, 51266, 400, 300, 1355, 321, 434, 516, 281, 1689, 1570, 3202, 281, 341, 1433, 293, 1689, 544, 3202, 281, 341, 51490, 51490, 1433, 281, 853, 281, 360, 1101, 322, 264, 3097, 992, 13, 51628, 51628, 400, 797, 11, 300, 3665, 291, 281, 3191, 257, 1090, 12577, 1154, 13, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.09644277140779316, "compression_ratio": 2.0599078341013826, "no_speech_prob": 5.626375809697493e-07}, {"id": 65, "seek": 27636, "start": 281.16, "end": 286.44, "text": " And helping you do better on the training set is a way to fix a high bias problem.", "tokens": [50364, 5127, 4497, 26110, 4122, 727, 854, 291, 360, 1101, 322, 264, 3097, 992, 13, 50604, 50604, 400, 4315, 291, 360, 1101, 322, 264, 3097, 992, 307, 257, 636, 281, 3191, 257, 1090, 12577, 1154, 13, 50868, 50868, 400, 550, 23223, 13607, 1355, 281, 764, 257, 3126, 2158, 337, 264, 3890, 2144, 13075, 13, 51266, 51266, 400, 300, 1355, 321, 434, 516, 281, 1689, 1570, 3202, 281, 341, 1433, 293, 1689, 544, 3202, 281, 341, 51490, 51490, 1433, 281, 853, 281, 360, 1101, 322, 264, 3097, 992, 13, 51628, 51628, 400, 797, 11, 300, 3665, 291, 281, 3191, 257, 1090, 12577, 1154, 13, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.09644277140779316, "compression_ratio": 2.0599078341013826, "no_speech_prob": 5.626375809697493e-07}, {"id": 66, "seek": 27636, "start": 286.44, "end": 294.40000000000003, "text": " And then decreasing lambda means to use a lower value for the regularization parameter.", "tokens": [50364, 5127, 4497, 26110, 4122, 727, 854, 291, 360, 1101, 322, 264, 3097, 992, 13, 50604, 50604, 400, 4315, 291, 360, 1101, 322, 264, 3097, 992, 307, 257, 636, 281, 3191, 257, 1090, 12577, 1154, 13, 50868, 50868, 400, 550, 23223, 13607, 1355, 281, 764, 257, 3126, 2158, 337, 264, 3890, 2144, 13075, 13, 51266, 51266, 400, 300, 1355, 321, 434, 516, 281, 1689, 1570, 3202, 281, 341, 1433, 293, 1689, 544, 3202, 281, 341, 51490, 51490, 1433, 281, 853, 281, 360, 1101, 322, 264, 3097, 992, 13, 51628, 51628, 400, 797, 11, 300, 3665, 291, 281, 3191, 257, 1090, 12577, 1154, 13, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.09644277140779316, "compression_ratio": 2.0599078341013826, "no_speech_prob": 5.626375809697493e-07}, {"id": 67, "seek": 27636, "start": 294.40000000000003, "end": 298.88, "text": " And that means we're going to pay less attention to this term and pay more attention to this", "tokens": [50364, 5127, 4497, 26110, 4122, 727, 854, 291, 360, 1101, 322, 264, 3097, 992, 13, 50604, 50604, 400, 4315, 291, 360, 1101, 322, 264, 3097, 992, 307, 257, 636, 281, 3191, 257, 1090, 12577, 1154, 13, 50868, 50868, 400, 550, 23223, 13607, 1355, 281, 764, 257, 3126, 2158, 337, 264, 3890, 2144, 13075, 13, 51266, 51266, 400, 300, 1355, 321, 434, 516, 281, 1689, 1570, 3202, 281, 341, 1433, 293, 1689, 544, 3202, 281, 341, 51490, 51490, 1433, 281, 853, 281, 360, 1101, 322, 264, 3097, 992, 13, 51628, 51628, 400, 797, 11, 300, 3665, 291, 281, 3191, 257, 1090, 12577, 1154, 13, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.09644277140779316, "compression_ratio": 2.0599078341013826, "no_speech_prob": 5.626375809697493e-07}, {"id": 68, "seek": 27636, "start": 298.88, "end": 301.64, "text": " term to try to do better on the training set.", "tokens": [50364, 5127, 4497, 26110, 4122, 727, 854, 291, 360, 1101, 322, 264, 3097, 992, 13, 50604, 50604, 400, 4315, 291, 360, 1101, 322, 264, 3097, 992, 307, 257, 636, 281, 3191, 257, 1090, 12577, 1154, 13, 50868, 50868, 400, 550, 23223, 13607, 1355, 281, 764, 257, 3126, 2158, 337, 264, 3890, 2144, 13075, 13, 51266, 51266, 400, 300, 1355, 321, 434, 516, 281, 1689, 1570, 3202, 281, 341, 1433, 293, 1689, 544, 3202, 281, 341, 51490, 51490, 1433, 281, 853, 281, 360, 1101, 322, 264, 3097, 992, 13, 51628, 51628, 400, 797, 11, 300, 3665, 291, 281, 3191, 257, 1090, 12577, 1154, 13, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.09644277140779316, "compression_ratio": 2.0599078341013826, "no_speech_prob": 5.626375809697493e-07}, {"id": 69, "seek": 27636, "start": 301.64, "end": 305.84000000000003, "text": " And again, that helps you to fix a high bias problem.", "tokens": [50364, 5127, 4497, 26110, 4122, 727, 854, 291, 360, 1101, 322, 264, 3097, 992, 13, 50604, 50604, 400, 4315, 291, 360, 1101, 322, 264, 3097, 992, 307, 257, 636, 281, 3191, 257, 1090, 12577, 1154, 13, 50868, 50868, 400, 550, 23223, 13607, 1355, 281, 764, 257, 3126, 2158, 337, 264, 3890, 2144, 13075, 13, 51266, 51266, 400, 300, 1355, 321, 434, 516, 281, 1689, 1570, 3202, 281, 341, 1433, 293, 1689, 544, 3202, 281, 341, 51490, 51490, 1433, 281, 853, 281, 360, 1101, 322, 264, 3097, 992, 13, 51628, 51628, 400, 797, 11, 300, 3665, 291, 281, 3191, 257, 1090, 12577, 1154, 13, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.09644277140779316, "compression_ratio": 2.0599078341013826, "no_speech_prob": 5.626375809697493e-07}, {"id": 70, "seek": 30584, "start": 305.84, "end": 309.76, "text": " And finally, increasing lambda, well, that's the opposite of this.", "tokens": [50364, 400, 2721, 11, 5662, 13607, 11, 731, 11, 300, 311, 264, 6182, 295, 341, 13, 50560, 50560, 583, 300, 1619, 291, 434, 670, 69, 2414, 264, 1412, 13, 50719, 50719, 400, 370, 5662, 13607, 576, 652, 2020, 498, 309, 311, 670, 69, 2414, 264, 3097, 992, 11, 445, 50940, 50940, 3372, 886, 709, 3202, 281, 3318, 294, 264, 3097, 992, 11, 457, 412, 264, 18406, 295, 2674, 3319, 281, 51224, 51224, 777, 5110, 13, 51315, 51315, 400, 370, 5662, 13607, 576, 3464, 264, 9284, 281, 3318, 257, 28640, 2445, 11, 1310, 257, 1570, 261, 46737, 51612, 51612, 2445, 293, 764, 341, 281, 3191, 257, 1090, 21977, 1154, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13417168174471175, "compression_ratio": 1.8455284552845528, "no_speech_prob": 1.529392875454505e-06}, {"id": 71, "seek": 30584, "start": 309.76, "end": 312.94, "text": " But that says you're overfitting the data.", "tokens": [50364, 400, 2721, 11, 5662, 13607, 11, 731, 11, 300, 311, 264, 6182, 295, 341, 13, 50560, 50560, 583, 300, 1619, 291, 434, 670, 69, 2414, 264, 1412, 13, 50719, 50719, 400, 370, 5662, 13607, 576, 652, 2020, 498, 309, 311, 670, 69, 2414, 264, 3097, 992, 11, 445, 50940, 50940, 3372, 886, 709, 3202, 281, 3318, 294, 264, 3097, 992, 11, 457, 412, 264, 18406, 295, 2674, 3319, 281, 51224, 51224, 777, 5110, 13, 51315, 51315, 400, 370, 5662, 13607, 576, 3464, 264, 9284, 281, 3318, 257, 28640, 2445, 11, 1310, 257, 1570, 261, 46737, 51612, 51612, 2445, 293, 764, 341, 281, 3191, 257, 1090, 21977, 1154, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13417168174471175, "compression_ratio": 1.8455284552845528, "no_speech_prob": 1.529392875454505e-06}, {"id": 72, "seek": 30584, "start": 312.94, "end": 317.35999999999996, "text": " And so increasing lambda would make sense if it's overfitting the training set, just", "tokens": [50364, 400, 2721, 11, 5662, 13607, 11, 731, 11, 300, 311, 264, 6182, 295, 341, 13, 50560, 50560, 583, 300, 1619, 291, 434, 670, 69, 2414, 264, 1412, 13, 50719, 50719, 400, 370, 5662, 13607, 576, 652, 2020, 498, 309, 311, 670, 69, 2414, 264, 3097, 992, 11, 445, 50940, 50940, 3372, 886, 709, 3202, 281, 3318, 294, 264, 3097, 992, 11, 457, 412, 264, 18406, 295, 2674, 3319, 281, 51224, 51224, 777, 5110, 13, 51315, 51315, 400, 370, 5662, 13607, 576, 3464, 264, 9284, 281, 3318, 257, 28640, 2445, 11, 1310, 257, 1570, 261, 46737, 51612, 51612, 2445, 293, 764, 341, 281, 3191, 257, 1090, 21977, 1154, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13417168174471175, "compression_ratio": 1.8455284552845528, "no_speech_prob": 1.529392875454505e-06}, {"id": 73, "seek": 30584, "start": 317.35999999999996, "end": 323.03999999999996, "text": " putting too much attention to fit in the training set, but at the expense of generalizing to", "tokens": [50364, 400, 2721, 11, 5662, 13607, 11, 731, 11, 300, 311, 264, 6182, 295, 341, 13, 50560, 50560, 583, 300, 1619, 291, 434, 670, 69, 2414, 264, 1412, 13, 50719, 50719, 400, 370, 5662, 13607, 576, 652, 2020, 498, 309, 311, 670, 69, 2414, 264, 3097, 992, 11, 445, 50940, 50940, 3372, 886, 709, 3202, 281, 3318, 294, 264, 3097, 992, 11, 457, 412, 264, 18406, 295, 2674, 3319, 281, 51224, 51224, 777, 5110, 13, 51315, 51315, 400, 370, 5662, 13607, 576, 3464, 264, 9284, 281, 3318, 257, 28640, 2445, 11, 1310, 257, 1570, 261, 46737, 51612, 51612, 2445, 293, 764, 341, 281, 3191, 257, 1090, 21977, 1154, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13417168174471175, "compression_ratio": 1.8455284552845528, "no_speech_prob": 1.529392875454505e-06}, {"id": 74, "seek": 30584, "start": 323.03999999999996, "end": 324.85999999999996, "text": " new examples.", "tokens": [50364, 400, 2721, 11, 5662, 13607, 11, 731, 11, 300, 311, 264, 6182, 295, 341, 13, 50560, 50560, 583, 300, 1619, 291, 434, 670, 69, 2414, 264, 1412, 13, 50719, 50719, 400, 370, 5662, 13607, 576, 652, 2020, 498, 309, 311, 670, 69, 2414, 264, 3097, 992, 11, 445, 50940, 50940, 3372, 886, 709, 3202, 281, 3318, 294, 264, 3097, 992, 11, 457, 412, 264, 18406, 295, 2674, 3319, 281, 51224, 51224, 777, 5110, 13, 51315, 51315, 400, 370, 5662, 13607, 576, 3464, 264, 9284, 281, 3318, 257, 28640, 2445, 11, 1310, 257, 1570, 261, 46737, 51612, 51612, 2445, 293, 764, 341, 281, 3191, 257, 1090, 21977, 1154, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13417168174471175, "compression_ratio": 1.8455284552845528, "no_speech_prob": 1.529392875454505e-06}, {"id": 75, "seek": 30584, "start": 324.85999999999996, "end": 330.79999999999995, "text": " And so increasing lambda would force the algorithm to fit a smoother function, maybe a less wiggly", "tokens": [50364, 400, 2721, 11, 5662, 13607, 11, 731, 11, 300, 311, 264, 6182, 295, 341, 13, 50560, 50560, 583, 300, 1619, 291, 434, 670, 69, 2414, 264, 1412, 13, 50719, 50719, 400, 370, 5662, 13607, 576, 652, 2020, 498, 309, 311, 670, 69, 2414, 264, 3097, 992, 11, 445, 50940, 50940, 3372, 886, 709, 3202, 281, 3318, 294, 264, 3097, 992, 11, 457, 412, 264, 18406, 295, 2674, 3319, 281, 51224, 51224, 777, 5110, 13, 51315, 51315, 400, 370, 5662, 13607, 576, 3464, 264, 9284, 281, 3318, 257, 28640, 2445, 11, 1310, 257, 1570, 261, 46737, 51612, 51612, 2445, 293, 764, 341, 281, 3191, 257, 1090, 21977, 1154, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13417168174471175, "compression_ratio": 1.8455284552845528, "no_speech_prob": 1.529392875454505e-06}, {"id": 76, "seek": 30584, "start": 330.79999999999995, "end": 335.79999999999995, "text": " function and use this to fix a high variance problem.", "tokens": [50364, 400, 2721, 11, 5662, 13607, 11, 731, 11, 300, 311, 264, 6182, 295, 341, 13, 50560, 50560, 583, 300, 1619, 291, 434, 670, 69, 2414, 264, 1412, 13, 50719, 50719, 400, 370, 5662, 13607, 576, 652, 2020, 498, 309, 311, 670, 69, 2414, 264, 3097, 992, 11, 445, 50940, 50940, 3372, 886, 709, 3202, 281, 3318, 294, 264, 3097, 992, 11, 457, 412, 264, 18406, 295, 2674, 3319, 281, 51224, 51224, 777, 5110, 13, 51315, 51315, 400, 370, 5662, 13607, 576, 3464, 264, 9284, 281, 3318, 257, 28640, 2445, 11, 1310, 257, 1570, 261, 46737, 51612, 51612, 2445, 293, 764, 341, 281, 3191, 257, 1090, 21977, 1154, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13417168174471175, "compression_ratio": 1.8455284552845528, "no_speech_prob": 1.529392875454505e-06}, {"id": 77, "seek": 33580, "start": 335.8, "end": 339.94, "text": " So I realized that this was a lot of stuff on the slide.", "tokens": [50364, 407, 286, 5334, 300, 341, 390, 257, 688, 295, 1507, 322, 264, 4137, 13, 50571, 50571, 583, 264, 45584, 286, 1454, 291, 362, 366, 11, 498, 291, 915, 300, 428, 9284, 575, 1090, 21977, 11, 50864, 50864, 550, 264, 732, 2135, 2098, 281, 3191, 300, 366, 2139, 483, 544, 3097, 1412, 420, 20460, 428, 2316, 13, 51267, 51267, 400, 538, 20460, 2316, 11, 286, 914, 11, 2139, 483, 257, 4356, 992, 295, 4122, 420, 3488, 264, 51576, 51576, 3890, 2144, 13075, 13607, 13, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.12188611085387482, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.7420739456829324e-07}, {"id": 78, "seek": 33580, "start": 339.94, "end": 345.8, "text": " But the takeaways I hope you have are, if you find that your algorithm has high variance,", "tokens": [50364, 407, 286, 5334, 300, 341, 390, 257, 688, 295, 1507, 322, 264, 4137, 13, 50571, 50571, 583, 264, 45584, 286, 1454, 291, 362, 366, 11, 498, 291, 915, 300, 428, 9284, 575, 1090, 21977, 11, 50864, 50864, 550, 264, 732, 2135, 2098, 281, 3191, 300, 366, 2139, 483, 544, 3097, 1412, 420, 20460, 428, 2316, 13, 51267, 51267, 400, 538, 20460, 2316, 11, 286, 914, 11, 2139, 483, 257, 4356, 992, 295, 4122, 420, 3488, 264, 51576, 51576, 3890, 2144, 13075, 13607, 13, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.12188611085387482, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.7420739456829324e-07}, {"id": 79, "seek": 33580, "start": 345.8, "end": 353.86, "text": " then the two main ways to fix that are either get more training data or simplify your model.", "tokens": [50364, 407, 286, 5334, 300, 341, 390, 257, 688, 295, 1507, 322, 264, 4137, 13, 50571, 50571, 583, 264, 45584, 286, 1454, 291, 362, 366, 11, 498, 291, 915, 300, 428, 9284, 575, 1090, 21977, 11, 50864, 50864, 550, 264, 732, 2135, 2098, 281, 3191, 300, 366, 2139, 483, 544, 3097, 1412, 420, 20460, 428, 2316, 13, 51267, 51267, 400, 538, 20460, 2316, 11, 286, 914, 11, 2139, 483, 257, 4356, 992, 295, 4122, 420, 3488, 264, 51576, 51576, 3890, 2144, 13075, 13607, 13, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.12188611085387482, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.7420739456829324e-07}, {"id": 80, "seek": 33580, "start": 353.86, "end": 360.04, "text": " And by simplify model, I mean, either get a smaller set of features or increase the", "tokens": [50364, 407, 286, 5334, 300, 341, 390, 257, 688, 295, 1507, 322, 264, 4137, 13, 50571, 50571, 583, 264, 45584, 286, 1454, 291, 362, 366, 11, 498, 291, 915, 300, 428, 9284, 575, 1090, 21977, 11, 50864, 50864, 550, 264, 732, 2135, 2098, 281, 3191, 300, 366, 2139, 483, 544, 3097, 1412, 420, 20460, 428, 2316, 13, 51267, 51267, 400, 538, 20460, 2316, 11, 286, 914, 11, 2139, 483, 257, 4356, 992, 295, 4122, 420, 3488, 264, 51576, 51576, 3890, 2144, 13075, 13607, 13, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.12188611085387482, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.7420739456829324e-07}, {"id": 81, "seek": 33580, "start": 360.04, "end": 362.44, "text": " regularization parameter lambda.", "tokens": [50364, 407, 286, 5334, 300, 341, 390, 257, 688, 295, 1507, 322, 264, 4137, 13, 50571, 50571, 583, 264, 45584, 286, 1454, 291, 362, 366, 11, 498, 291, 915, 300, 428, 9284, 575, 1090, 21977, 11, 50864, 50864, 550, 264, 732, 2135, 2098, 281, 3191, 300, 366, 2139, 483, 544, 3097, 1412, 420, 20460, 428, 2316, 13, 51267, 51267, 400, 538, 20460, 2316, 11, 286, 914, 11, 2139, 483, 257, 4356, 992, 295, 4122, 420, 3488, 264, 51576, 51576, 3890, 2144, 13075, 13607, 13, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.12188611085387482, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.7420739456829324e-07}, {"id": 82, "seek": 36244, "start": 362.44, "end": 368.16, "text": " So your algorithm has less flexibility to fit very complex, very wiggly curves.", "tokens": [50364, 407, 428, 9284, 575, 1570, 12635, 281, 3318, 588, 3997, 11, 588, 261, 46737, 19490, 13, 50650, 50650, 33247, 736, 11, 498, 428, 9284, 575, 1090, 12577, 11, 550, 300, 1355, 309, 311, 406, 884, 731, 754, 322, 50986, 50986, 264, 3097, 992, 13, 51052, 51052, 407, 498, 300, 311, 264, 1389, 11, 264, 2135, 32539, 366, 281, 652, 428, 2316, 544, 4005, 420, 281, 976, 51342, 51342, 309, 544, 12635, 281, 3318, 544, 3997, 420, 544, 261, 46737, 6828, 13, 51564, 51564, 400, 370, 512, 2098, 281, 360, 300, 366, 281, 976, 309, 4497, 4122, 420, 909, 613, 26110, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11047659470484807, "compression_ratio": 1.7634854771784232, "no_speech_prob": 7.571110813842097e-07}, {"id": 83, "seek": 36244, "start": 368.16, "end": 374.88, "text": " Conversely, if your algorithm has high bias, then that means it's not doing well even on", "tokens": [50364, 407, 428, 9284, 575, 1570, 12635, 281, 3318, 588, 3997, 11, 588, 261, 46737, 19490, 13, 50650, 50650, 33247, 736, 11, 498, 428, 9284, 575, 1090, 12577, 11, 550, 300, 1355, 309, 311, 406, 884, 731, 754, 322, 50986, 50986, 264, 3097, 992, 13, 51052, 51052, 407, 498, 300, 311, 264, 1389, 11, 264, 2135, 32539, 366, 281, 652, 428, 2316, 544, 4005, 420, 281, 976, 51342, 51342, 309, 544, 12635, 281, 3318, 544, 3997, 420, 544, 261, 46737, 6828, 13, 51564, 51564, 400, 370, 512, 2098, 281, 360, 300, 366, 281, 976, 309, 4497, 4122, 420, 909, 613, 26110, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11047659470484807, "compression_ratio": 1.7634854771784232, "no_speech_prob": 7.571110813842097e-07}, {"id": 84, "seek": 36244, "start": 374.88, "end": 376.2, "text": " the training set.", "tokens": [50364, 407, 428, 9284, 575, 1570, 12635, 281, 3318, 588, 3997, 11, 588, 261, 46737, 19490, 13, 50650, 50650, 33247, 736, 11, 498, 428, 9284, 575, 1090, 12577, 11, 550, 300, 1355, 309, 311, 406, 884, 731, 754, 322, 50986, 50986, 264, 3097, 992, 13, 51052, 51052, 407, 498, 300, 311, 264, 1389, 11, 264, 2135, 32539, 366, 281, 652, 428, 2316, 544, 4005, 420, 281, 976, 51342, 51342, 309, 544, 12635, 281, 3318, 544, 3997, 420, 544, 261, 46737, 6828, 13, 51564, 51564, 400, 370, 512, 2098, 281, 360, 300, 366, 281, 976, 309, 4497, 4122, 420, 909, 613, 26110, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11047659470484807, "compression_ratio": 1.7634854771784232, "no_speech_prob": 7.571110813842097e-07}, {"id": 85, "seek": 36244, "start": 376.2, "end": 382.0, "text": " So if that's the case, the main fixes are to make your model more powerful or to give", "tokens": [50364, 407, 428, 9284, 575, 1570, 12635, 281, 3318, 588, 3997, 11, 588, 261, 46737, 19490, 13, 50650, 50650, 33247, 736, 11, 498, 428, 9284, 575, 1090, 12577, 11, 550, 300, 1355, 309, 311, 406, 884, 731, 754, 322, 50986, 50986, 264, 3097, 992, 13, 51052, 51052, 407, 498, 300, 311, 264, 1389, 11, 264, 2135, 32539, 366, 281, 652, 428, 2316, 544, 4005, 420, 281, 976, 51342, 51342, 309, 544, 12635, 281, 3318, 544, 3997, 420, 544, 261, 46737, 6828, 13, 51564, 51564, 400, 370, 512, 2098, 281, 360, 300, 366, 281, 976, 309, 4497, 4122, 420, 909, 613, 26110, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11047659470484807, "compression_ratio": 1.7634854771784232, "no_speech_prob": 7.571110813842097e-07}, {"id": 86, "seek": 36244, "start": 382.0, "end": 386.44, "text": " it more flexibility to fit more complex or more wiggly functions.", "tokens": [50364, 407, 428, 9284, 575, 1570, 12635, 281, 3318, 588, 3997, 11, 588, 261, 46737, 19490, 13, 50650, 50650, 33247, 736, 11, 498, 428, 9284, 575, 1090, 12577, 11, 550, 300, 1355, 309, 311, 406, 884, 731, 754, 322, 50986, 50986, 264, 3097, 992, 13, 51052, 51052, 407, 498, 300, 311, 264, 1389, 11, 264, 2135, 32539, 366, 281, 652, 428, 2316, 544, 4005, 420, 281, 976, 51342, 51342, 309, 544, 12635, 281, 3318, 544, 3997, 420, 544, 261, 46737, 6828, 13, 51564, 51564, 400, 370, 512, 2098, 281, 360, 300, 366, 281, 976, 309, 4497, 4122, 420, 909, 613, 26110, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11047659470484807, "compression_ratio": 1.7634854771784232, "no_speech_prob": 7.571110813842097e-07}, {"id": 87, "seek": 36244, "start": 386.44, "end": 391.44, "text": " And so some ways to do that are to give it additional features or add these polynomial", "tokens": [50364, 407, 428, 9284, 575, 1570, 12635, 281, 3318, 588, 3997, 11, 588, 261, 46737, 19490, 13, 50650, 50650, 33247, 736, 11, 498, 428, 9284, 575, 1090, 12577, 11, 550, 300, 1355, 309, 311, 406, 884, 731, 754, 322, 50986, 50986, 264, 3097, 992, 13, 51052, 51052, 407, 498, 300, 311, 264, 1389, 11, 264, 2135, 32539, 366, 281, 652, 428, 2316, 544, 4005, 420, 281, 976, 51342, 51342, 309, 544, 12635, 281, 3318, 544, 3997, 420, 544, 261, 46737, 6828, 13, 51564, 51564, 400, 370, 512, 2098, 281, 360, 300, 366, 281, 976, 309, 4497, 4122, 420, 909, 613, 26110, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11047659470484807, "compression_ratio": 1.7634854771784232, "no_speech_prob": 7.571110813842097e-07}, {"id": 88, "seek": 39144, "start": 391.44, "end": 396.0, "text": " features or to decrease the regularization parameter lambda.", "tokens": [50364, 4122, 420, 281, 11514, 264, 3890, 2144, 13075, 13607, 13, 50592, 50592, 3146, 264, 636, 11, 294, 1389, 291, 434, 6359, 498, 291, 820, 3191, 1090, 12577, 538, 12245, 264, 3097, 50894, 50894, 992, 2744, 11, 300, 1177, 380, 767, 854, 13, 51032, 51032, 759, 291, 5407, 264, 3097, 992, 2744, 11, 291, 486, 3318, 264, 3097, 992, 1101, 11, 457, 300, 12258, 51244, 51244, 281, 469, 6748, 428, 3278, 24071, 6713, 293, 264, 3389, 295, 428, 2539, 9284, 13, 51444, 51444, 407, 500, 380, 16979, 3507, 1314, 3097, 5110, 445, 281, 853, 281, 3191, 257, 1090, 12577, 1154, 13, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.1135575404533973, "compression_ratio": 1.7279693486590038, "no_speech_prob": 2.332051053599571e-06}, {"id": 89, "seek": 39144, "start": 396.0, "end": 402.04, "text": " By the way, in case you're wondering if you should fix high bias by reducing the training", "tokens": [50364, 4122, 420, 281, 11514, 264, 3890, 2144, 13075, 13607, 13, 50592, 50592, 3146, 264, 636, 11, 294, 1389, 291, 434, 6359, 498, 291, 820, 3191, 1090, 12577, 538, 12245, 264, 3097, 50894, 50894, 992, 2744, 11, 300, 1177, 380, 767, 854, 13, 51032, 51032, 759, 291, 5407, 264, 3097, 992, 2744, 11, 291, 486, 3318, 264, 3097, 992, 1101, 11, 457, 300, 12258, 51244, 51244, 281, 469, 6748, 428, 3278, 24071, 6713, 293, 264, 3389, 295, 428, 2539, 9284, 13, 51444, 51444, 407, 500, 380, 16979, 3507, 1314, 3097, 5110, 445, 281, 853, 281, 3191, 257, 1090, 12577, 1154, 13, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.1135575404533973, "compression_ratio": 1.7279693486590038, "no_speech_prob": 2.332051053599571e-06}, {"id": 90, "seek": 39144, "start": 402.04, "end": 404.8, "text": " set size, that doesn't actually help.", "tokens": [50364, 4122, 420, 281, 11514, 264, 3890, 2144, 13075, 13607, 13, 50592, 50592, 3146, 264, 636, 11, 294, 1389, 291, 434, 6359, 498, 291, 820, 3191, 1090, 12577, 538, 12245, 264, 3097, 50894, 50894, 992, 2744, 11, 300, 1177, 380, 767, 854, 13, 51032, 51032, 759, 291, 5407, 264, 3097, 992, 2744, 11, 291, 486, 3318, 264, 3097, 992, 1101, 11, 457, 300, 12258, 51244, 51244, 281, 469, 6748, 428, 3278, 24071, 6713, 293, 264, 3389, 295, 428, 2539, 9284, 13, 51444, 51444, 407, 500, 380, 16979, 3507, 1314, 3097, 5110, 445, 281, 853, 281, 3191, 257, 1090, 12577, 1154, 13, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.1135575404533973, "compression_ratio": 1.7279693486590038, "no_speech_prob": 2.332051053599571e-06}, {"id": 91, "seek": 39144, "start": 404.8, "end": 409.04, "text": " If you reduce the training set size, you will fit the training set better, but that tends", "tokens": [50364, 4122, 420, 281, 11514, 264, 3890, 2144, 13075, 13607, 13, 50592, 50592, 3146, 264, 636, 11, 294, 1389, 291, 434, 6359, 498, 291, 820, 3191, 1090, 12577, 538, 12245, 264, 3097, 50894, 50894, 992, 2744, 11, 300, 1177, 380, 767, 854, 13, 51032, 51032, 759, 291, 5407, 264, 3097, 992, 2744, 11, 291, 486, 3318, 264, 3097, 992, 1101, 11, 457, 300, 12258, 51244, 51244, 281, 469, 6748, 428, 3278, 24071, 6713, 293, 264, 3389, 295, 428, 2539, 9284, 13, 51444, 51444, 407, 500, 380, 16979, 3507, 1314, 3097, 5110, 445, 281, 853, 281, 3191, 257, 1090, 12577, 1154, 13, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.1135575404533973, "compression_ratio": 1.7279693486590038, "no_speech_prob": 2.332051053599571e-06}, {"id": 92, "seek": 39144, "start": 409.04, "end": 413.04, "text": " to worsen your cross validation error and the performance of your learning algorithm.", "tokens": [50364, 4122, 420, 281, 11514, 264, 3890, 2144, 13075, 13607, 13, 50592, 50592, 3146, 264, 636, 11, 294, 1389, 291, 434, 6359, 498, 291, 820, 3191, 1090, 12577, 538, 12245, 264, 3097, 50894, 50894, 992, 2744, 11, 300, 1177, 380, 767, 854, 13, 51032, 51032, 759, 291, 5407, 264, 3097, 992, 2744, 11, 291, 486, 3318, 264, 3097, 992, 1101, 11, 457, 300, 12258, 51244, 51244, 281, 469, 6748, 428, 3278, 24071, 6713, 293, 264, 3389, 295, 428, 2539, 9284, 13, 51444, 51444, 407, 500, 380, 16979, 3507, 1314, 3097, 5110, 445, 281, 853, 281, 3191, 257, 1090, 12577, 1154, 13, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.1135575404533973, "compression_ratio": 1.7279693486590038, "no_speech_prob": 2.332051053599571e-06}, {"id": 93, "seek": 39144, "start": 413.04, "end": 418.15999999999997, "text": " So don't randomly throw away training examples just to try to fix a high bias problem.", "tokens": [50364, 4122, 420, 281, 11514, 264, 3890, 2144, 13075, 13607, 13, 50592, 50592, 3146, 264, 636, 11, 294, 1389, 291, 434, 6359, 498, 291, 820, 3191, 1090, 12577, 538, 12245, 264, 3097, 50894, 50894, 992, 2744, 11, 300, 1177, 380, 767, 854, 13, 51032, 51032, 759, 291, 5407, 264, 3097, 992, 2744, 11, 291, 486, 3318, 264, 3097, 992, 1101, 11, 457, 300, 12258, 51244, 51244, 281, 469, 6748, 428, 3278, 24071, 6713, 293, 264, 3389, 295, 428, 2539, 9284, 13, 51444, 51444, 407, 500, 380, 16979, 3507, 1314, 3097, 5110, 445, 281, 853, 281, 3191, 257, 1090, 12577, 1154, 13, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.1135575404533973, "compression_ratio": 1.7279693486590038, "no_speech_prob": 2.332051053599571e-06}, {"id": 94, "seek": 41816, "start": 418.16, "end": 423.20000000000005, "text": " One of my PhD students from Stanford, many years after he'd already graduated from Stanford,", "tokens": [50364, 1485, 295, 452, 14476, 1731, 490, 20374, 11, 867, 924, 934, 415, 1116, 1217, 13693, 490, 20374, 11, 50616, 50616, 1564, 848, 281, 385, 300, 1339, 415, 390, 7601, 412, 20374, 11, 415, 3264, 466, 12577, 293, 21977, 50905, 50905, 293, 2762, 411, 415, 658, 309, 11, 415, 7320, 309, 13, 51038, 51038, 583, 300, 26514, 11, 934, 867, 924, 295, 589, 1752, 412, 257, 1326, 819, 3431, 11, 51299, 51299, 415, 5334, 300, 12577, 293, 21977, 307, 472, 295, 729, 10392, 300, 2516, 257, 2099, 565, 51522, 51522, 281, 1466, 11, 457, 2516, 257, 11364, 281, 4505, 13, 51688, 51688, 3950, 645, 702, 1900, 2283, 13, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.10987424420880841, "compression_ratio": 1.763157894736842, "no_speech_prob": 2.753378612396773e-05}, {"id": 95, "seek": 41816, "start": 423.20000000000005, "end": 428.98, "text": " once said to me that while he was studying at Stanford, he learned about bias and variance", "tokens": [50364, 1485, 295, 452, 14476, 1731, 490, 20374, 11, 867, 924, 934, 415, 1116, 1217, 13693, 490, 20374, 11, 50616, 50616, 1564, 848, 281, 385, 300, 1339, 415, 390, 7601, 412, 20374, 11, 415, 3264, 466, 12577, 293, 21977, 50905, 50905, 293, 2762, 411, 415, 658, 309, 11, 415, 7320, 309, 13, 51038, 51038, 583, 300, 26514, 11, 934, 867, 924, 295, 589, 1752, 412, 257, 1326, 819, 3431, 11, 51299, 51299, 415, 5334, 300, 12577, 293, 21977, 307, 472, 295, 729, 10392, 300, 2516, 257, 2099, 565, 51522, 51522, 281, 1466, 11, 457, 2516, 257, 11364, 281, 4505, 13, 51688, 51688, 3950, 645, 702, 1900, 2283, 13, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.10987424420880841, "compression_ratio": 1.763157894736842, "no_speech_prob": 2.753378612396773e-05}, {"id": 96, "seek": 41816, "start": 428.98, "end": 431.64000000000004, "text": " and felt like he got it, he understood it.", "tokens": [50364, 1485, 295, 452, 14476, 1731, 490, 20374, 11, 867, 924, 934, 415, 1116, 1217, 13693, 490, 20374, 11, 50616, 50616, 1564, 848, 281, 385, 300, 1339, 415, 390, 7601, 412, 20374, 11, 415, 3264, 466, 12577, 293, 21977, 50905, 50905, 293, 2762, 411, 415, 658, 309, 11, 415, 7320, 309, 13, 51038, 51038, 583, 300, 26514, 11, 934, 867, 924, 295, 589, 1752, 412, 257, 1326, 819, 3431, 11, 51299, 51299, 415, 5334, 300, 12577, 293, 21977, 307, 472, 295, 729, 10392, 300, 2516, 257, 2099, 565, 51522, 51522, 281, 1466, 11, 457, 2516, 257, 11364, 281, 4505, 13, 51688, 51688, 3950, 645, 702, 1900, 2283, 13, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.10987424420880841, "compression_ratio": 1.763157894736842, "no_speech_prob": 2.753378612396773e-05}, {"id": 97, "seek": 41816, "start": 431.64000000000004, "end": 436.86, "text": " But that subsequently, after many years of work experience at a few different companies,", "tokens": [50364, 1485, 295, 452, 14476, 1731, 490, 20374, 11, 867, 924, 934, 415, 1116, 1217, 13693, 490, 20374, 11, 50616, 50616, 1564, 848, 281, 385, 300, 1339, 415, 390, 7601, 412, 20374, 11, 415, 3264, 466, 12577, 293, 21977, 50905, 50905, 293, 2762, 411, 415, 658, 309, 11, 415, 7320, 309, 13, 51038, 51038, 583, 300, 26514, 11, 934, 867, 924, 295, 589, 1752, 412, 257, 1326, 819, 3431, 11, 51299, 51299, 415, 5334, 300, 12577, 293, 21977, 307, 472, 295, 729, 10392, 300, 2516, 257, 2099, 565, 51522, 51522, 281, 1466, 11, 457, 2516, 257, 11364, 281, 4505, 13, 51688, 51688, 3950, 645, 702, 1900, 2283, 13, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.10987424420880841, "compression_ratio": 1.763157894736842, "no_speech_prob": 2.753378612396773e-05}, {"id": 98, "seek": 41816, "start": 436.86, "end": 441.32000000000005, "text": " he realized that bias and variance is one of those concepts that takes a short time", "tokens": [50364, 1485, 295, 452, 14476, 1731, 490, 20374, 11, 867, 924, 934, 415, 1116, 1217, 13693, 490, 20374, 11, 50616, 50616, 1564, 848, 281, 385, 300, 1339, 415, 390, 7601, 412, 20374, 11, 415, 3264, 466, 12577, 293, 21977, 50905, 50905, 293, 2762, 411, 415, 658, 309, 11, 415, 7320, 309, 13, 51038, 51038, 583, 300, 26514, 11, 934, 867, 924, 295, 589, 1752, 412, 257, 1326, 819, 3431, 11, 51299, 51299, 415, 5334, 300, 12577, 293, 21977, 307, 472, 295, 729, 10392, 300, 2516, 257, 2099, 565, 51522, 51522, 281, 1466, 11, 457, 2516, 257, 11364, 281, 4505, 13, 51688, 51688, 3950, 645, 702, 1900, 2283, 13, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.10987424420880841, "compression_ratio": 1.763157894736842, "no_speech_prob": 2.753378612396773e-05}, {"id": 99, "seek": 41816, "start": 441.32000000000005, "end": 444.64000000000004, "text": " to learn, but takes a lifetime to master.", "tokens": [50364, 1485, 295, 452, 14476, 1731, 490, 20374, 11, 867, 924, 934, 415, 1116, 1217, 13693, 490, 20374, 11, 50616, 50616, 1564, 848, 281, 385, 300, 1339, 415, 390, 7601, 412, 20374, 11, 415, 3264, 466, 12577, 293, 21977, 50905, 50905, 293, 2762, 411, 415, 658, 309, 11, 415, 7320, 309, 13, 51038, 51038, 583, 300, 26514, 11, 934, 867, 924, 295, 589, 1752, 412, 257, 1326, 819, 3431, 11, 51299, 51299, 415, 5334, 300, 12577, 293, 21977, 307, 472, 295, 729, 10392, 300, 2516, 257, 2099, 565, 51522, 51522, 281, 1466, 11, 457, 2516, 257, 11364, 281, 4505, 13, 51688, 51688, 3950, 645, 702, 1900, 2283, 13, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.10987424420880841, "compression_ratio": 1.763157894736842, "no_speech_prob": 2.753378612396773e-05}, {"id": 100, "seek": 41816, "start": 444.64000000000004, "end": 447.0, "text": " Those were his exact words.", "tokens": [50364, 1485, 295, 452, 14476, 1731, 490, 20374, 11, 867, 924, 934, 415, 1116, 1217, 13693, 490, 20374, 11, 50616, 50616, 1564, 848, 281, 385, 300, 1339, 415, 390, 7601, 412, 20374, 11, 415, 3264, 466, 12577, 293, 21977, 50905, 50905, 293, 2762, 411, 415, 658, 309, 11, 415, 7320, 309, 13, 51038, 51038, 583, 300, 26514, 11, 934, 867, 924, 295, 589, 1752, 412, 257, 1326, 819, 3431, 11, 51299, 51299, 415, 5334, 300, 12577, 293, 21977, 307, 472, 295, 729, 10392, 300, 2516, 257, 2099, 565, 51522, 51522, 281, 1466, 11, 457, 2516, 257, 11364, 281, 4505, 13, 51688, 51688, 3950, 645, 702, 1900, 2283, 13, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.10987424420880841, "compression_ratio": 1.763157894736842, "no_speech_prob": 2.753378612396773e-05}, {"id": 101, "seek": 44700, "start": 447.0, "end": 451.96, "text": " And I think bias and variance is one of those very powerful ideas.", "tokens": [50364, 400, 286, 519, 12577, 293, 21977, 307, 472, 295, 729, 588, 4005, 3487, 13, 50612, 50612, 1133, 286, 478, 3097, 2539, 14642, 11, 286, 1920, 1009, 853, 281, 2573, 484, 498, 309, 307, 1090, 12577, 50850, 50850, 420, 1090, 21977, 13, 50928, 50928, 583, 264, 636, 291, 352, 466, 14329, 309, 39531, 307, 746, 300, 286, 519, 291, 486, 1066, 322, 51242, 51242, 1242, 1101, 412, 807, 10477, 3124, 13, 51446, 51446, 583, 291, 603, 915, 11, 286, 519, 11, 300, 3701, 613, 3487, 486, 854, 291, 312, 709, 544, 4942, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.12096806576377467, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.4462839266780065e-06}, {"id": 102, "seek": 44700, "start": 451.96, "end": 456.72, "text": " When I'm training learning algorithms, I almost always try to figure out if it is high bias", "tokens": [50364, 400, 286, 519, 12577, 293, 21977, 307, 472, 295, 729, 588, 4005, 3487, 13, 50612, 50612, 1133, 286, 478, 3097, 2539, 14642, 11, 286, 1920, 1009, 853, 281, 2573, 484, 498, 309, 307, 1090, 12577, 50850, 50850, 420, 1090, 21977, 13, 50928, 50928, 583, 264, 636, 291, 352, 466, 14329, 309, 39531, 307, 746, 300, 286, 519, 291, 486, 1066, 322, 51242, 51242, 1242, 1101, 412, 807, 10477, 3124, 13, 51446, 51446, 583, 291, 603, 915, 11, 286, 519, 11, 300, 3701, 613, 3487, 486, 854, 291, 312, 709, 544, 4942, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.12096806576377467, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.4462839266780065e-06}, {"id": 103, "seek": 44700, "start": 456.72, "end": 458.28, "text": " or high variance.", "tokens": [50364, 400, 286, 519, 12577, 293, 21977, 307, 472, 295, 729, 588, 4005, 3487, 13, 50612, 50612, 1133, 286, 478, 3097, 2539, 14642, 11, 286, 1920, 1009, 853, 281, 2573, 484, 498, 309, 307, 1090, 12577, 50850, 50850, 420, 1090, 21977, 13, 50928, 50928, 583, 264, 636, 291, 352, 466, 14329, 309, 39531, 307, 746, 300, 286, 519, 291, 486, 1066, 322, 51242, 51242, 1242, 1101, 412, 807, 10477, 3124, 13, 51446, 51446, 583, 291, 603, 915, 11, 286, 519, 11, 300, 3701, 613, 3487, 486, 854, 291, 312, 709, 544, 4942, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.12096806576377467, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.4462839266780065e-06}, {"id": 104, "seek": 44700, "start": 458.28, "end": 464.56, "text": " But the way you go about addressing it systematically is something that I think you will keep on", "tokens": [50364, 400, 286, 519, 12577, 293, 21977, 307, 472, 295, 729, 588, 4005, 3487, 13, 50612, 50612, 1133, 286, 478, 3097, 2539, 14642, 11, 286, 1920, 1009, 853, 281, 2573, 484, 498, 309, 307, 1090, 12577, 50850, 50850, 420, 1090, 21977, 13, 50928, 50928, 583, 264, 636, 291, 352, 466, 14329, 309, 39531, 307, 746, 300, 286, 519, 291, 486, 1066, 322, 51242, 51242, 1242, 1101, 412, 807, 10477, 3124, 13, 51446, 51446, 583, 291, 603, 915, 11, 286, 519, 11, 300, 3701, 613, 3487, 486, 854, 291, 312, 709, 544, 4942, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.12096806576377467, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.4462839266780065e-06}, {"id": 105, "seek": 44700, "start": 464.56, "end": 468.64, "text": " getting better at through repeated practice.", "tokens": [50364, 400, 286, 519, 12577, 293, 21977, 307, 472, 295, 729, 588, 4005, 3487, 13, 50612, 50612, 1133, 286, 478, 3097, 2539, 14642, 11, 286, 1920, 1009, 853, 281, 2573, 484, 498, 309, 307, 1090, 12577, 50850, 50850, 420, 1090, 21977, 13, 50928, 50928, 583, 264, 636, 291, 352, 466, 14329, 309, 39531, 307, 746, 300, 286, 519, 291, 486, 1066, 322, 51242, 51242, 1242, 1101, 412, 807, 10477, 3124, 13, 51446, 51446, 583, 291, 603, 915, 11, 286, 519, 11, 300, 3701, 613, 3487, 486, 854, 291, 312, 709, 544, 4942, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.12096806576377467, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.4462839266780065e-06}, {"id": 106, "seek": 44700, "start": 468.64, "end": 473.96, "text": " But you'll find, I think, that understanding these ideas will help you be much more effective", "tokens": [50364, 400, 286, 519, 12577, 293, 21977, 307, 472, 295, 729, 588, 4005, 3487, 13, 50612, 50612, 1133, 286, 478, 3097, 2539, 14642, 11, 286, 1920, 1009, 853, 281, 2573, 484, 498, 309, 307, 1090, 12577, 50850, 50850, 420, 1090, 21977, 13, 50928, 50928, 583, 264, 636, 291, 352, 466, 14329, 309, 39531, 307, 746, 300, 286, 519, 291, 486, 1066, 322, 51242, 51242, 1242, 1101, 412, 807, 10477, 3124, 13, 51446, 51446, 583, 291, 603, 915, 11, 286, 519, 11, 300, 3701, 613, 3487, 486, 854, 291, 312, 709, 544, 4942, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.12096806576377467, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.4462839266780065e-06}, {"id": 107, "seek": 47396, "start": 473.96, "end": 479.64, "text": " at how you decide what to try next when developing a learning algorithm.", "tokens": [50364, 412, 577, 291, 4536, 437, 281, 853, 958, 562, 6416, 257, 2539, 9284, 13, 50648, 50648, 823, 286, 458, 300, 321, 630, 352, 807, 257, 688, 294, 341, 960, 13, 50844, 50844, 400, 498, 291, 841, 411, 11, 3237, 11, 456, 311, 445, 257, 688, 295, 1507, 510, 11, 309, 311, 1392, 11, 500, 380, 3292, 50994, 50994, 466, 309, 13, 51044, 51044, 11965, 341, 1243, 294, 264, 3124, 20339, 293, 3124, 48955, 11, 321, 603, 362, 611, 4497, 4786, 51362, 51362, 281, 352, 670, 613, 3487, 370, 300, 291, 393, 483, 4497, 3124, 365, 1953, 466, 12577, 51612, 51612, 293, 21977, 295, 819, 2539, 14642, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.1327372370539485, "compression_ratio": 1.6888888888888889, "no_speech_prob": 5.50745699001709e-06}, {"id": 108, "seek": 47396, "start": 479.64, "end": 483.56, "text": " Now I know that we did go through a lot in this video.", "tokens": [50364, 412, 577, 291, 4536, 437, 281, 853, 958, 562, 6416, 257, 2539, 9284, 13, 50648, 50648, 823, 286, 458, 300, 321, 630, 352, 807, 257, 688, 294, 341, 960, 13, 50844, 50844, 400, 498, 291, 841, 411, 11, 3237, 11, 456, 311, 445, 257, 688, 295, 1507, 510, 11, 309, 311, 1392, 11, 500, 380, 3292, 50994, 50994, 466, 309, 13, 51044, 51044, 11965, 341, 1243, 294, 264, 3124, 20339, 293, 3124, 48955, 11, 321, 603, 362, 611, 4497, 4786, 51362, 51362, 281, 352, 670, 613, 3487, 370, 300, 291, 393, 483, 4497, 3124, 365, 1953, 466, 12577, 51612, 51612, 293, 21977, 295, 819, 2539, 14642, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.1327372370539485, "compression_ratio": 1.6888888888888889, "no_speech_prob": 5.50745699001709e-06}, {"id": 109, "seek": 47396, "start": 483.56, "end": 486.56, "text": " And if you feel like, boy, there's just a lot of stuff here, it's okay, don't worry", "tokens": [50364, 412, 577, 291, 4536, 437, 281, 853, 958, 562, 6416, 257, 2539, 9284, 13, 50648, 50648, 823, 286, 458, 300, 321, 630, 352, 807, 257, 688, 294, 341, 960, 13, 50844, 50844, 400, 498, 291, 841, 411, 11, 3237, 11, 456, 311, 445, 257, 688, 295, 1507, 510, 11, 309, 311, 1392, 11, 500, 380, 3292, 50994, 50994, 466, 309, 13, 51044, 51044, 11965, 341, 1243, 294, 264, 3124, 20339, 293, 3124, 48955, 11, 321, 603, 362, 611, 4497, 4786, 51362, 51362, 281, 352, 670, 613, 3487, 370, 300, 291, 393, 483, 4497, 3124, 365, 1953, 466, 12577, 51612, 51612, 293, 21977, 295, 819, 2539, 14642, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.1327372370539485, "compression_ratio": 1.6888888888888889, "no_speech_prob": 5.50745699001709e-06}, {"id": 110, "seek": 47396, "start": 486.56, "end": 487.56, "text": " about it.", "tokens": [50364, 412, 577, 291, 4536, 437, 281, 853, 958, 562, 6416, 257, 2539, 9284, 13, 50648, 50648, 823, 286, 458, 300, 321, 630, 352, 807, 257, 688, 294, 341, 960, 13, 50844, 50844, 400, 498, 291, 841, 411, 11, 3237, 11, 456, 311, 445, 257, 688, 295, 1507, 510, 11, 309, 311, 1392, 11, 500, 380, 3292, 50994, 50994, 466, 309, 13, 51044, 51044, 11965, 341, 1243, 294, 264, 3124, 20339, 293, 3124, 48955, 11, 321, 603, 362, 611, 4497, 4786, 51362, 51362, 281, 352, 670, 613, 3487, 370, 300, 291, 393, 483, 4497, 3124, 365, 1953, 466, 12577, 51612, 51612, 293, 21977, 295, 819, 2539, 14642, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.1327372370539485, "compression_ratio": 1.6888888888888889, "no_speech_prob": 5.50745699001709e-06}, {"id": 111, "seek": 47396, "start": 487.56, "end": 493.91999999999996, "text": " Later this week in the practice labs and practice quizzes, we'll have also additional opportunities", "tokens": [50364, 412, 577, 291, 4536, 437, 281, 853, 958, 562, 6416, 257, 2539, 9284, 13, 50648, 50648, 823, 286, 458, 300, 321, 630, 352, 807, 257, 688, 294, 341, 960, 13, 50844, 50844, 400, 498, 291, 841, 411, 11, 3237, 11, 456, 311, 445, 257, 688, 295, 1507, 510, 11, 309, 311, 1392, 11, 500, 380, 3292, 50994, 50994, 466, 309, 13, 51044, 51044, 11965, 341, 1243, 294, 264, 3124, 20339, 293, 3124, 48955, 11, 321, 603, 362, 611, 4497, 4786, 51362, 51362, 281, 352, 670, 613, 3487, 370, 300, 291, 393, 483, 4497, 3124, 365, 1953, 466, 12577, 51612, 51612, 293, 21977, 295, 819, 2539, 14642, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.1327372370539485, "compression_ratio": 1.6888888888888889, "no_speech_prob": 5.50745699001709e-06}, {"id": 112, "seek": 47396, "start": 493.91999999999996, "end": 498.91999999999996, "text": " to go over these ideas so that you can get additional practice with thinking about bias", "tokens": [50364, 412, 577, 291, 4536, 437, 281, 853, 958, 562, 6416, 257, 2539, 9284, 13, 50648, 50648, 823, 286, 458, 300, 321, 630, 352, 807, 257, 688, 294, 341, 960, 13, 50844, 50844, 400, 498, 291, 841, 411, 11, 3237, 11, 456, 311, 445, 257, 688, 295, 1507, 510, 11, 309, 311, 1392, 11, 500, 380, 3292, 50994, 50994, 466, 309, 13, 51044, 51044, 11965, 341, 1243, 294, 264, 3124, 20339, 293, 3124, 48955, 11, 321, 603, 362, 611, 4497, 4786, 51362, 51362, 281, 352, 670, 613, 3487, 370, 300, 291, 393, 483, 4497, 3124, 365, 1953, 466, 12577, 51612, 51612, 293, 21977, 295, 819, 2539, 14642, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.1327372370539485, "compression_ratio": 1.6888888888888889, "no_speech_prob": 5.50745699001709e-06}, {"id": 113, "seek": 47396, "start": 498.91999999999996, "end": 501.4, "text": " and variance of different learning algorithms.", "tokens": [50364, 412, 577, 291, 4536, 437, 281, 853, 958, 562, 6416, 257, 2539, 9284, 13, 50648, 50648, 823, 286, 458, 300, 321, 630, 352, 807, 257, 688, 294, 341, 960, 13, 50844, 50844, 400, 498, 291, 841, 411, 11, 3237, 11, 456, 311, 445, 257, 688, 295, 1507, 510, 11, 309, 311, 1392, 11, 500, 380, 3292, 50994, 50994, 466, 309, 13, 51044, 51044, 11965, 341, 1243, 294, 264, 3124, 20339, 293, 3124, 48955, 11, 321, 603, 362, 611, 4497, 4786, 51362, 51362, 281, 352, 670, 613, 3487, 370, 300, 291, 393, 483, 4497, 3124, 365, 1953, 466, 12577, 51612, 51612, 293, 21977, 295, 819, 2539, 14642, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.1327372370539485, "compression_ratio": 1.6888888888888889, "no_speech_prob": 5.50745699001709e-06}, {"id": 114, "seek": 50140, "start": 501.4, "end": 504.44, "text": " So if it seems like a lot right now, it's okay.", "tokens": [50364, 407, 498, 309, 2544, 411, 257, 688, 558, 586, 11, 309, 311, 1392, 13, 50516, 50516, 509, 483, 281, 3124, 613, 3487, 1780, 341, 1243, 293, 4696, 45806, 428, 3701, 50722, 50722, 295, 552, 412, 300, 565, 13, 50882, 50882, 4546, 2684, 322, 11, 12577, 293, 21977, 611, 366, 588, 4420, 562, 1953, 466, 577, 281, 3847, 51170, 51170, 257, 18161, 3209, 13, 51262, 51262, 407, 294, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 613, 10392, 6456, 281, 18161, 3209, 3097, 13, 51526, 51526, 961, 311, 352, 322, 281, 264, 958, 960, 13, 51600], "temperature": 0.0, "avg_logprob": -0.12259501640242759, "compression_ratio": 1.6182572614107884, "no_speech_prob": 9.361299817101099e-06}, {"id": 115, "seek": 50140, "start": 504.44, "end": 508.56, "text": " You get to practice these ideas later this week and hopefully deepen your understanding", "tokens": [50364, 407, 498, 309, 2544, 411, 257, 688, 558, 586, 11, 309, 311, 1392, 13, 50516, 50516, 509, 483, 281, 3124, 613, 3487, 1780, 341, 1243, 293, 4696, 45806, 428, 3701, 50722, 50722, 295, 552, 412, 300, 565, 13, 50882, 50882, 4546, 2684, 322, 11, 12577, 293, 21977, 611, 366, 588, 4420, 562, 1953, 466, 577, 281, 3847, 51170, 51170, 257, 18161, 3209, 13, 51262, 51262, 407, 294, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 613, 10392, 6456, 281, 18161, 3209, 3097, 13, 51526, 51526, 961, 311, 352, 322, 281, 264, 958, 960, 13, 51600], "temperature": 0.0, "avg_logprob": -0.12259501640242759, "compression_ratio": 1.6182572614107884, "no_speech_prob": 9.361299817101099e-06}, {"id": 116, "seek": 50140, "start": 508.56, "end": 511.76, "text": " of them at that time.", "tokens": [50364, 407, 498, 309, 2544, 411, 257, 688, 558, 586, 11, 309, 311, 1392, 13, 50516, 50516, 509, 483, 281, 3124, 613, 3487, 1780, 341, 1243, 293, 4696, 45806, 428, 3701, 50722, 50722, 295, 552, 412, 300, 565, 13, 50882, 50882, 4546, 2684, 322, 11, 12577, 293, 21977, 611, 366, 588, 4420, 562, 1953, 466, 577, 281, 3847, 51170, 51170, 257, 18161, 3209, 13, 51262, 51262, 407, 294, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 613, 10392, 6456, 281, 18161, 3209, 3097, 13, 51526, 51526, 961, 311, 352, 322, 281, 264, 958, 960, 13, 51600], "temperature": 0.0, "avg_logprob": -0.12259501640242759, "compression_ratio": 1.6182572614107884, "no_speech_prob": 9.361299817101099e-06}, {"id": 117, "seek": 50140, "start": 511.76, "end": 517.52, "text": " Before moving on, bias and variance also are very useful when thinking about how to train", "tokens": [50364, 407, 498, 309, 2544, 411, 257, 688, 558, 586, 11, 309, 311, 1392, 13, 50516, 50516, 509, 483, 281, 3124, 613, 3487, 1780, 341, 1243, 293, 4696, 45806, 428, 3701, 50722, 50722, 295, 552, 412, 300, 565, 13, 50882, 50882, 4546, 2684, 322, 11, 12577, 293, 21977, 611, 366, 588, 4420, 562, 1953, 466, 577, 281, 3847, 51170, 51170, 257, 18161, 3209, 13, 51262, 51262, 407, 294, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 613, 10392, 6456, 281, 18161, 3209, 3097, 13, 51526, 51526, 961, 311, 352, 322, 281, 264, 958, 960, 13, 51600], "temperature": 0.0, "avg_logprob": -0.12259501640242759, "compression_ratio": 1.6182572614107884, "no_speech_prob": 9.361299817101099e-06}, {"id": 118, "seek": 50140, "start": 517.52, "end": 519.36, "text": " a neural network.", "tokens": [50364, 407, 498, 309, 2544, 411, 257, 688, 558, 586, 11, 309, 311, 1392, 13, 50516, 50516, 509, 483, 281, 3124, 613, 3487, 1780, 341, 1243, 293, 4696, 45806, 428, 3701, 50722, 50722, 295, 552, 412, 300, 565, 13, 50882, 50882, 4546, 2684, 322, 11, 12577, 293, 21977, 611, 366, 588, 4420, 562, 1953, 466, 577, 281, 3847, 51170, 51170, 257, 18161, 3209, 13, 51262, 51262, 407, 294, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 613, 10392, 6456, 281, 18161, 3209, 3097, 13, 51526, 51526, 961, 311, 352, 322, 281, 264, 958, 960, 13, 51600], "temperature": 0.0, "avg_logprob": -0.12259501640242759, "compression_ratio": 1.6182572614107884, "no_speech_prob": 9.361299817101099e-06}, {"id": 119, "seek": 50140, "start": 519.36, "end": 524.64, "text": " So in the next video, let's take a look at these concepts applied to neural network training.", "tokens": [50364, 407, 498, 309, 2544, 411, 257, 688, 558, 586, 11, 309, 311, 1392, 13, 50516, 50516, 509, 483, 281, 3124, 613, 3487, 1780, 341, 1243, 293, 4696, 45806, 428, 3701, 50722, 50722, 295, 552, 412, 300, 565, 13, 50882, 50882, 4546, 2684, 322, 11, 12577, 293, 21977, 611, 366, 588, 4420, 562, 1953, 466, 577, 281, 3847, 51170, 51170, 257, 18161, 3209, 13, 51262, 51262, 407, 294, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 613, 10392, 6456, 281, 18161, 3209, 3097, 13, 51526, 51526, 961, 311, 352, 322, 281, 264, 958, 960, 13, 51600], "temperature": 0.0, "avg_logprob": -0.12259501640242759, "compression_ratio": 1.6182572614107884, "no_speech_prob": 9.361299817101099e-06}, {"id": 120, "seek": 52464, "start": 524.64, "end": 531.64, "text": " Let's go on to the next video.", "tokens": [50364, 961, 311, 352, 322, 281, 264, 958, 960, 13, 50714], "temperature": 0.0, "avg_logprob": -0.4156843026479085, "compression_ratio": 0.7894736842105263, "no_speech_prob": 0.0002998473064508289}], "language": "en", "video_id": "00gn1isOd70", "entity": "ML Specialization, Andrew Ng (2022)"}}