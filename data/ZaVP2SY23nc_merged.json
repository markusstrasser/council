{"video_id": "ZaVP2SY23nc", "title": "Week 8 \u2013 Lecture: Contrastive methods and regularised latent variable models", "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Yann LeCun\nWeek 8: http://bit.ly/pDL-en-08\n\n0:00:00 \u2013 Week 8 \u2013 Lecture\n\nLECTURE Part A: http://bit.ly/pDL-en-08-1\nIn this section, we focused on the introduction of contrastive methods in Energy-Based Models in several aspects. First, we discuss the advantage brought by applying contrastive methods in self-supervised learning. Second, we discussed the architecture of denoising autoencoders and their weakness in image reconstruction tasks. We also talked about other contrastive methods, like contrastive divergence and persistent contrastive divergence.\n0:00:05 \u2013 Recap on EBM and Characteristics of Different Contrastive Methods\n0:10:13 \u2013 Contrastive Methods in Self-Supervised Learning\n0:23:04 \u2013 Denoising Autoencoder and other Contrastive methods\n\nLECTURE Part B: http://bit.ly/pDL-en-08-2\nIn this section, we discussed regularized latent variable EBMs in detail covering concepts of conditional and unconditional versions of these models. We then discussed the algorithms of ISTA, FISTA and LISTA and look at examples of sparse coding and filters learned from convolutional sparse encoders. Finally we talked about Variational Auto-Encoders and the underlying concepts involved.\n0:37:13 \u2013 Overview of Regularized Latent Variable Energy Based Models and Sparse Coding\n1:07:46 \u2013 Convolutional Sparse Auto-Encoders\n1:12:51 \u2013 Variational Auto-Encoders", "author": "Alfredo Canziani", "keywords": ["Yann LeCun", "Deep Learning", "PyTorch", "NYU", "EBM", "Energy Based Models", "SSL", "Semi Supervised Learning", "LV", "Latent Variable", "contrastive methods", "Regularised Latent Variables"], "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw", "length": 5966, "views": 11872, "publish_date": "11/02/2022", "timestamp": 1589760000, "entity": "Yann LeCun", "transcript": {"text": " All right, you guys see the slides, I assume? Alfredo, I can see you, I can see anyone else. Yes. All right. Yeah, it's all good. You can also make signs, I can see you. Okay, so we're going to still talk about energy-based models, and mostly in the context of self-supervised learning or unsupervised learning, continuing where we left off last time. So let me start with a little bit of a reminder of where we left last time. We talked about self-supervised learning as the idea of basically trying to predict everything from everything else, pretending that a part of the input is not visible to the system and another part is visible, and we train the system to predict the non-visible part from the visible part. And of course, it could be anything, it could be part of a video, or it could be something else. There is a special case where we don't assume that anything is visible at any time, and so we're just asking the system to just predict out of the blue without any input. So we talked about the approach of energy-based models, which consists in essentially basically having an implicit function that captures the dependency between X and Y, or in the case where you don't have an X, is the dependency between the various components of Y. And the reason why we need an implicit function is that for a particular value of X, there could be multiple values of Y that are possible. And so if we had a direct prediction from X to Y, we could only make one prediction, and using an implicit function, we can make multiple predictions implicitly by basically having a function that gives low energy for multiple values of Y for a given value of X. And that's a little bit what's represented on the left with essentially, you can think of this as some sort of landscape, mountainous landscape, where the data points are in the valleys, and everything else at size, the manifold of data is as higher energy. So inference in this context proceeds by basically finding a Y or a set of Ys that minimize f of X, Y for a given X. So this is not learning yet. Learning consists in shaping f, but we're just talking about inference, so it's very important to be able to make the difference between the inference process of minimizing the energy function to find Y, and then the learning process, which is minimizing a loss function, not the energy function, with respect to the parameters of the energy function. Okay, those are two different things. And in the case of the unconditional case, you don't have an X, and so you're only capturing the mutual dependencies between Ys. We talked about latent variable models, and the reason why we talked about latent variable models is that it's a particular way of representing, of building the architecture of the energy function in such a way that it can have multiple Y for a given X. So essentially, a latent variable is an extra variable Z that nobody gives you the value of, but the first thing you do when you see a Z is that you minimize your energy function with respect to that Z, and that gives you now an energy function that does not depend on Z anymore. Or if you want to do inference with a model with latent variable, I give you an X, and you find the combination of Y and Z that minimizes the energy, and then you give me Y. That's the inference process. There's two ways to do inference with respect to a variable that you don't observe. One is to just minimize over it, as I just indicated, and the other one is to marginalize over it if you're a probabilist, but even in other cases. And there's a simple formula to kind of go from one to the other, which is basically a log sum exponential over all possible values of Z. And this may be intractable, so we don't do this very often. Okay, so training an energy-based model consists in parameterizing the energy function and collecting, of course, a bunch of training samples, a bunch of X and Ys in the conditional case or just a bunch of Ys in the unconditional case. And then it consists in shaping the energy function so that you give low energy to good combinations of X and Ys and higher energy to bad combinations of X and Y. So for a given observed X, you try to make F of Y for the corresponding Y that corresponds to X as low as possible, but then you also need to make the energy F of X and Y larger for all other values of Y, all other possible values of Y. And it's probably a good idea to keep this energy function smooth if you are in a continuous space, if Y is a continuous variable, because that will make inference easier. Subsequently, you'll be able to use gradient descent-based methods to do inference on maybe other methods. So there's two classes of learning algorithms, as we talked about last time. The first class is contrastive methods, which consists in basically pushing down on the energy of training samples. So you get a training sample XI, YI, you plug it into the energy function, and then you tune the parameters of the energy function so that that energy goes down. And you can do this with, you know, with backfrop if your energy function is some sort of neural net with other things in it. As long as it's a differentiable function, you can do that. But then what you have to do as well is pick other points that are outside the manifold of data and then push their energy up so that the energy gets takes the right shape. Okay, so those are contrastive methods. And then there's architectural methods and the architectural methods basically consist in building F of X, Y in such a way that the volume of space that can take low energy is limited, perhaps minimized in some way. And so if you push down on the energy of certain points, automatically the rest will be up because we go up, because the volume of stuff that can take low energy is limited. And I've made a list here. So this is an important slide, which you saw last time. And there is a list of various methods that you may have heard of, some of which are contrastive, some of which are architectural. I must say that those two classes of methods are not necessarily compatible with each other. You can very well use a combination of the two. But most methods only use one. So things like maximum likelihood, if your probabilities consist in pushing down on the energy of data points and then pushing up everywhere else for every other value of Y in proportion to how low the energy is. And then you push up harder if the energy is lower so that in the end you get kind of the right shape. Maximum likelihood, incidentally, only cares about differences of energies, doesn't care about absolute values of energies, which is an important point. There are other methods like contrastive divergence, metric learning, ratio matching, noise contrast estimation, minimum probability flow, things like that, which, and generative adversarial networks that also are based on the idea of pushing up on the energy of data points outside the data manifold. And then there are similar methods, denosing autoencoders that we will talk about in just a minute. And as we saw last time, they've been extremely successful in the context of natural language processing. Systems like BERT, for example, basically are denosing autoencoders of a particular kind. And then there are architectural methods. And last time we talked a little bit about PCA and K-means. So we're going to talk about a few more today, particularly sparse coding and something called Lista. And I'm going to talk about the remaining ones. So this is a rehash again of something that we talked about last week, which is a very simple latent variable model for unsupervised learning, K-means, which I'm sure you've all heard about. The energy function is simply the square reconstruction error between the data vector and the product of a prototype matrix times latent variable vector. And that latent variable vector is constrained to be a one-hot vector. So in other words, it selects one of the columns of W when you multiply W by it. And so what you get in the end is the square distance between the data vector and the column of W that is closest to it. Once you do the minimization with respect to Z, which means looking for which column of W is closest to Y. So that's the energy function. That's the inference algorithm looking for the closest prototype. And the energy function, of course, is zero wherever there is a prototype and grows quadratically as you move away from the prototype until you get closer to another prototype, in which case the energy again goes down as you get closer to the second prototype. So if you train K-means on a data set where the training samples are generated by picking around this little spiral here shown at the bottom, with K equal 20 in that case, you get those little dark areas which indicate the minima of the energy function. So you get a ridge in the middle where the energy goes down on both sides. It's like a ridge. But here is a method that's become very popular over the last few months and is very recent. The first papers on this actually go back a long time. There are some of my papers from the early 90s and from the mid-2000s. And they were called Siamese networks or metric learning at the time. And the idea is to build a sort of energy-based model, if you want, by having two copies of the same network or two different networks. But very often it's two copies of the same network. And you feed X to the first network and Y to the second network. You have them compute some feature vector on the output, H and H prime. And then you compare those two feature vectors with some method, some way of computing a similarity or dissimilarity between vectors. It could be a dot product, it could be a cosine, a similarity, it could be something of that type. And what you do is to train the system is that you train it with a data point basically as a pair of X and Y. So you indicate the location of the data manifold to the system by basically telling it, here is a sample, tell me is a sample X. Give me another sample that basically has the same content as X but is different. And of course you're never going to ask the system to give you that sample. You're going to generate those samples and train the system with it. So there are two pairs, positive pairs. So pairs are compatible with each other, which is the whole idea of energy-based models. And a compatible pair is, or a positive pair if you want, consists of X being an image and Y being a transformation of this image that basically does not change its content. So it's still the same content of the image if you want. So you want basically representations extracted by those two networks to be very similar because those images are similar. And that's exactly what you're going to do. You're going to feed those two images to those two networks. And you're going to have a loss function that says minimize the energy, which means minimize the distance or similarity measure between H and H prime, between the outputs of the two networks. So that's the positive part. So that's a way to kind of lower the energy for training samples. And then you have to generate random negative samples. And the way you generate them is by basically picking, again, a sample for X and then picking another image that you know is different, that has nothing to do with X, is incompatible with it if you want. It's very different. And now what you do is you feed those two images to those two networks and you try to push H and H prime away from each other. So basically you're trying to make the similarity metric C of H and H prime large for those two samples. And the objective function here is going to take into account the energy function for similar pairs and the energy function for dissimilar pairs. It's going to push down on the energy function for similar pairs, push up on the energy function for dissimilar pairs. So there's been a number of \u2013 so people have used metric learning for various things for a long time, for image search, for example, for fit recognition, for things like that. But it's only in the last few months that there's been a couple of works that have shown you can use those methods to learn good features for object recognition. And those are really the first papers that produce features in an unsupervised or self-supervised way that produce features that can rival the features that are obtained through supervised learning. So the three papers in question are PEARL, which means Pre-Tecting Variant Representation Learning by Ishaan Mishra and Laurence van der Maten at Facebook in New York, another one called MOKO by Keiming He and his collaborators at Facebook in Manlo Park, and the third one, which appeared more recently, is called SIMCLEAR by a group from Google, Janet Al and the last author being Jeffrey Hinton. So there's been other work using those kind of methods. I think that it was a question, perhaps? No, it wasn't a question. It was actually my phone waking up because I said Google. Oh, I see. Okay, and slow features, something we'll talk about later, which is a little similar. Okay, so these are examples of results that are obtained with MOKO, and they essentially show that even with a very large model, which is basically a version of ResNet-50, that you train using this contrasting method, you get decent performance. This is, I believe, top five performance on ImageNet. Perl actually works quite a bit better than MOKO. This is top one accuracy this time with networks of various sizes. So here, there's several scenarios. The main scenario is you take all of ImageNet, you take a sample from ImageNet, distort it, and that gives you a positive pair. Run it through your two networks and train the network to produce similar outputs. Basically, the two networks are identical, actually, for both MOKO and Perl. It's the same as Net. And then take dissimilar pairs and push the outputs away from each other using a particular cost function that we'll see in a minute. And then you have to do this many, many times, and you have to be smart about how you cache the negative samples, because most samples are already very different by the time they get to the output of the network. So you basically have to be smart about how you kind of pick the good negatives. So the type of objective function that is used by Perl is called noise contrast estimation. And that goes back to kind of previous papers. It's not their invention. Where the similarity metric is the cosine similarity measure between the outputs of the convolutional nets. And then what you compute is this basically the softmax-like function, which computes the exponential of the similarity metric of two outputs for similar pairs and then divides by the sum of the similarity metric exponentiated for similar pairs and the sum of dissimilar pairs. So you have a batch where you have one similar pair and a bunch of dissimilar pairs, and you compute this kind of softmax thing. And if you minimize the softmax cost function, it's going to push the similarity metric of similar pair to be as large as possible. And the similarity metric, the cosine similarity of dissimilar pair to be basically as small as possible. I had this question that why are we separately using an LNC function, whereas we could have probably directly computed loss using the HVI, VI transformed probability that we have by taking the negative log of that probability. So what benefit would LNC provide using not directly taking the negative log of the probability that we have from H? Well, that's a good question. It's not entirely clear to me why. I think what happened there is that people tried lots and lots of different things, and this is what ended up working best. There is in the Hinton paper, there is kind of a similar thing where they tried different types of objective function and found that something like NC actually works quite well. So it's an empirical question, and I don't have a good intuition for why you need this term in addition to the denominator in H. I hope this answers your question, although sorry, I don't have any answer for it. Why do you use cosine similarity instead of L2 norm? Instead of L2 norm or instead of... Okay, it's because you want to normalize. It's very easy to make two vectors similar by making them very short or to make two vectors very dissimilar by making them very long. So by doing cosine similarity, you're basically normalizing, you're computing a dot product, but you're normalizing this dot product. And so you make the measure independent of the length of the vectors. And so it forces the system to kind of find a good solution to the problem without cheating by just making the vectors either short or large. It also removes any stability that could be in the system. The design of those contrastive functions is actually quite a bit of a black heart. Okay, so what they actually do in Perl is that they don't use directly the output of the conv net for the objective function. They have different heads. So basically, the conv net has a different set of heads, F and G, which are different for the two networks. And that's what they use in the context of this contrastive learning. And then there is another head that they use for the ultimate task of classification. So those F and G functions are, you can think of as sort of extra layers that are kind of on top of the network that are different for the two networks. All right, so these are the results that are produced by Perl. And you can get \u2013 so this particular experiment is one in which you pre-train the system using Perl on the ImageNet training set. And then what you do is you retrain, you fine-tune the system using either 1% of the labeled samples or 10% of the labeled samples. And you measure the performance, top five accuracy or top one accuracy. So this paper appeared in January on archive. And then just a few weeks ago, this paper appeared called SimClear by Chen and Al, which is a team from Google. And they have a very sophisticated corruption or data augmentation method to generate similar pairs. And they train for a very, very long time on a lot of TPUs. And they get really interestingly good results, so much better than either Perl or MoCo using very large models. And they can reach more than 75% correct top one on ImageNet by just pre-training in self-supervised fashion and then kind of fine-tuning with only 1% of the samples. Yeah, so this is, in fact, the previous slide is a different scenario where you only train a linear classifier on top of the network. This is the scenario where you train with either 1% or 10% of labeled samples. And you get 85% top five with 1% of the labels, which is pretty amazing results. To some extent, I think this shows the limits of contrastive methods because the amount of computation and training that is required for this is absolutely gigantic. It's really enormous. So here is a scenario where you just train a linear classifier on top. So you freeze the features produced by the system that has been pre-trained using self-supervised learning. And then you just train a linear classifier on top and you measure the performance, either top one or top five on the full ImageNet, having trained, supervised on the full ImageNet. And again, the numbers are really impressive. But again, I think it shows the limit of contrastive methods. Here is the main issue with contrastive methods is that there are many, many, many locations in a high dimensional space where you need to push up the energy to make sure that it's actually higher everywhere than on the data manifold. And so as you increase the dimension of the representation, you need more and more negative samples to make sure that the energy is higher where it needs to be higher. OK, so let's talk about another another crop of contrastive methods called denoising autoencoder. And that's become really kind of important over the last year and a half or so for natural language processing. So the idea of denoising autoencoder is that you take a Y and the way you generate X is by corrupting Y. So this sounds a little bit like the opposite of what we were just doing with contrastive methods. But basically you take a clean image, Y, you corrupt it in some way by removing a piece of it, for example, or you take a piece of text and you remove some of the words or you mask a piece of it. So a special case is the masked autoencoder where the corruption consists in masking a subset of the input. And then you run this through an autoencoder, which is essentially an encoder or called a predictor here, a decoder, and perhaps final layers that may have softmax in the context of text or not, nothing if it's images. And then you compare the predicted output Y bar with the observed data Y. And so what's the principle of this? The principle of this is that is the following. And you can thank Alfredo for those beautiful pictures. Yeah, we saw this in class last Tuesday. That's right. So this is basically just a reminder. You take a data point, which is one of those pink points, right, and you corrupt it. So you get one of those brown points. And then you train the autoencoder from the brown point to produce the pink points, the original pink point. What does that mean? That means that now the energy function, which is the reconstruction error, is going to be equal to the difference between the original point and the pink point, the distance, the square distance, if C is the Euclidean square Euclidean distance. So C of Y bar is going to be the, if you think it's properly trained, is going to be the distance between the corrupted point X, the brown point, and the pink point you started from Y. So basically it basically trains the system to produce an energy function that grows quadratically as you move away from the data manifold. And so it's an example of a contrastive method because you push up on the energy of points that are outside the data manifold. Essentially you tell them your energy should be the square distance to the data manifold or at least to the point that was used before, you know, through corruption. But the problem with it is that again, in a high dimensional continuous space, there is many, many, many ways you can corrupt a piece of data. And it's not entirely clear that you're going to be able to kind of shape the energy function the proper way by just pushing up on lots of different locations. It works in text because text is discrete. It doesn't work so well in images. People have used this in the context of image in painting, for example. So the corruption consists in masking a piece of the image and then training a system to reconstruct it. And the reason why it doesn't work is because people tend to train the system without latent variables. In my little diagram here, there is a latent variable. But in fact, in versions of this that are used in the context of images, there is no real latent variable. And it's very difficult for the system to just dream up a single solution to the in-painting problem here. It's a multimodal manifold. I mean, it's a manifold. It's probably not just a single point. There's many ways to complete the image here by filling in the masked part. And so without latent variable, the system produces blurry predictions and doesn't learn particularly good features. Here's the multimodal part, also the reason why we had that internal purple area in the spiral, because each of those points have two predictions, right, in between the two branches of the spiral. Right. So this is the additional problem that if you're not careful, the points that are in the middle that could be the result of a corruption of one pinpoint on one side of the manifold or a pinpoint on another side of the manifold, those points right in the middle don't know where to go because half the time they're trained to go to one part of the manifold, the other half of the time they're trained to go to the other part of the manifold. And so that might create kind of flat spots in the energy function that are not good. So there are ways to alleviate this, but they're not kind of completely worked out unless you use latent variable models. Okay, other contrasting methods. This is just in passing for your own interest. There are things like contrastive divergence and others which I'm not going to talk about. The contrastive divergence is a very simple idea. You pick a training sample. You lower the energy at that point, of course, and then from that sample, you using some sort of gradient based process you move down the energy surface with noise. So start from a sample and figure out how do I change my sample, how do I change my why in such a way that my current energy based model produces a lower energy than the one I just, I just measured for that sample. Okay, so basically you're trying to find another point in input space that has lower energy than the training point you just fed it. Okay, so you can think of this as kind of a smart way of corrupting a training sample smart because you, you don't randomly corrupt it, you corrupt it by basically modifying it to find a point in space that your model already gives low energy to. So it would be a point that you would want to push up because your model gives low energy to it. And you don't want it to have low energy so you push it up, and I'm going to professor have people tried contrastive methods with this image in painting method and how would one do that, does that really work if they if you do that together. So in painting is a contrastive method right you you take an image, you corrupt it by blocking some piece of it. And then you train a neural net is going to go to encoder to generate the full image, and then you compare this reconstruction of the full image with the original uncorrupted image, and that's your energy function. Okay. So it is, it is a contrast method. Right, so if we if we use like the NC loss with this in painting loss. Does that, like, is that useful. You can reuse NC loss because NC kind of relies on the fact that you have sort of a finite number of negative samples. Okay, here you sort of artificially generate negative samples. So, it's really completely different scenario I don't think you would. You could use something similar to NC. What is not in a meaningful way. Okay, so this is why space. Okay, why why why too. And let's say you're, you're data manifold is something like this. But let's say your energy function currently is something like this so here I'm drawing the region of low energy. And I'm drawing the lines of equal costs. Okay, so the energy looks nice at the bottom left right you have data points here that your model gives low energy to. But then your model is not good because at the bottom right it gives low energy to regions that have no data and then at the top, you have data points that your model gives high energy to. Okay, so here is our contrast divergence would work. You take a sample, a training sample, the test this guy. And by gradient descent, you go down the energy surface to a point that has low energy. Okay. Now, this was a training sample why the one you obtain now is a contrastive sample way bar. And what you do now is you change the parameters of your energy function so that you make the energy of why smaller and the energy of why bar larger. Okay, using some kind of loss function that you know pushes down on one pushes up on the other, which last function use is immaterial. You just need one that will do the right thing. Okay, so what I've described here is kind of a deterministic version of contrast divergence but in fact contrast divergence is kind of a probabilistic version of this, where what you do is you, you do this sort of gradient based descent. I mean this sort of search for a low energy point, but you do it with some level of randomness randomness some noise in it. And the way to do this in a continuous space like like this one is that you give it a random kick. You think of your data point here as sort of a marble that is going to go down the energy surface you give it a random kick in some random direction, say this. And then you let the system kind of follow the gradient. And you stop when you're tired, you don't wait for it to kind of go down all the way you just stop when you're tired, and then there is a rule to select whether you keep the point or not. And that's your that's your way bar. Why is the kick necessary. Okay, so the kick is necessary so that you can go over energy barriers that would be between you and the lowest energy areas. Okay, that's why you need the kick. Now, if you have a space, a white space that is not continuous but is discrete. You can kind of do this energy minimization by basically doing something called simulated annealing. So, essentially, if what is discrete variable you kind of perturb it randomly. If the energy you get by this perturbation is lower than you keep it if it's higher than you keep it with some probability, and then you keep doing this. And eventually the energy will go down. So this is a non gradient based optimization algorithm or gradient free optimization algorithm if you want, which you kind of have to resort to when the space is discrete and you can't use gradient information. This technique I just described of kicking a marble and sort of simulating it rolling down the energy is called Hamiltonian Monte Carlo HNC. And you might you might see this in other in other context. So that's another way of generating negative samples. Yes, Hamiltonian Monte Carlo. Some people call this hybrid Monte Carlo sometimes. So some of you may have heard of something called restricted boson machines and restricted boson machine is an energy based model in which the energy is very simple. It's written at the bottom here, the energy of Y and Z. So Y is basically an input data vector and Z is a latent variable. The energy function is minus Z transpose WY where W is a matrix, not necessarily square because Z and Y may have different dimensions. And generally Z and Y are both binary variables. And so I mean, binary vectors. So the components of binary variables. And they were kind of somewhat popular in the mid 2000s. But, you know, I'm not spending much time on it here because they've kind of falling out of favor a little bit. They're not they're not that popular. But just so that gives you some reference of what what this means. There's sort of refinements of contrastive divergence. One of them is called persistent contrastive divergence. And it consists in using a bunch of particles and you can remember the position. So they have sort of permanent persistent positions if you want. So you throw a bunch of marbles in your energy landscape and you keep making you keep making them roll down, maybe with a little bit of noise or kicks. And then you keep their position so you don't change the the position of the marble according to new samples, new training samples. You just keep the marbles where they are. And eventually they'll find low energy places in your in your energy surface and will cause them to be pushed up because because that's what happens during during training. But this doesn't scale very well. So things like RBMs become very, very expensive to to train in high dimension. OK, so now for regularized latent variable energy based model, which is my current favorite type of model. So we talked about the idea of building a predictive model by having a latent variable. Right. So you have the observed variable X, you run it to a predictor, it extracts some representation of the observed variables. And then that goes into a decoder that produces the prediction. But if you want your decoder to be able to make multiple predictions, then you feed it with a latent variable. And as you vary the value of this latent variable, the prediction will vary over a set over hopefully the manifold of data of in the space of Y that are compatible with X. So this architecture here, the formula for the energy can be written as on the left here. So you have C of Y and C is a cost function that compares these two arguments. So you compare Y, the data vector, with the result of applying the decoder to the output of the predictor that takes into account X and the decoder also takes into account Z. So here is the problem with this. If Z is too powerful, in other words, if Z has too much capacity, then there are always going to be a Z that is going to produce a Y bar that's going to be exactly equal to Y. So remember the inference algorithm here is that you give an X and a Y and then you find a Z that minimizes C of Y, Y bar. Right. That's how you do inference of the latent variable in an energy based model. Right. You get an X and a Y, find a Z that minimizes the energy. So if Z, for example, has the same dimension as Y and the decoder is powerful enough to represent the identity function, then for any Y, there's always going to be a Z that produces Y bar that's exactly equal to Y. Okay. And if the decoder is the identity function, which ignores H, the identity function from Z to Y, to Y bar, then you just set Z equal to Y and the energy is zero. And that would be a terrible energy based model because it would not give high energy to stuff outside the manifold of data. It gives low energy to everything. Okay. It gives zero energy to everything. So the way to prevent the system from giving low energy to points outside the manifold of data is to limit the information capacity of the latent variable Z. To be more precise, if Z can only take, let's say, 10 different values, what that means is, so you constrain Z to only take 10 possible different values. Let's say you make Z a one hot vector of dimension 10, like in K-means. Okay. Then there's only going to be 10 points in Y space that will have zero energy because either Y is equal to one of the Y bars that is produced from one of those 10 Zs or it's not. If it is, then the energy is zero. If it's not, the energy is going to have to be larger than zero. In fact, it's going to grow quadratically as you move away from that Z. And that's exactly the idea of K-means. Okay. But what if you find other ways to limit the information content of Z? So this seems like a kind of a small technical sub-problem, but in my opinion, the question of how you limit the information content of a latent variable in a model of this type is the most important question in AI today. Okay. And I'm not kidding. I think the main problem we're facing is how to do self-supervised learning properly. And contrastive methods have shown their limits. And so we have to find alternatives, and alternatives are regularized latent variable models. There might be other ideas that nobody has had so far, but these are the only two that I know of. And then the main technical issue that we need to solve is how do we limit the information content of the latent variable so that we limit the volume of white space that can take low energy, and therefore we automatically make the energy outside the manifold of data where we train the system to have low energy, we automatically make the energy outside higher. So I'm going to go through a few examples of systems that actually work, and things that people have done for 20 years in some cases. And so that's the idea here. So one of the ideas, you add a regularizer in the energy, and this regularizer takes low value on a kind of small part of the space of Z. And so the system will preferentially choose values of Z that are within this sort of restricted set, where R takes a small value. And if Z needs to go outside of that set to do a good reconstruction, you're paying a price for it in terms of energy. So, the volume of Z space that is determined by R basically limits the volume of space of Y that can take low energy. And the trade off is controlled by basically a coefficient lambda that you can adjust to make the volume of white space that take low energy as small as possible or not that small. So here are a few examples of R and Z. And some of them are kind of useful because they're differentiable with respect to Z and some of them are not so useful because they're not differentiable, so you have to look for kind of, you know, do discrete search. So one is the effective dimension of Z. So what you can do is you can decide that Z a priori has three dimension, four dimension, five dimension, six dimension. You train your model for various dimensions of Z, and there is one set of dimensions for which, you know, one dimension for which the prediction would be good, but at the same time, the dimension of Z would be minimized. And what you will have found is basically the lowest embedding dimension of your space. So imagine, for example, that your data set consists of lots and lots of pictures of someone making faces in front of a camera. We know that the effective dimension of the manifold of all the faces of a person is something like 60, at least less than 100, because it's bounded above by the number of muscles in your face. And so there has to be a Z of dimension 50 or 60 or something like that, such that when you run it through a convolutional net, you will generate all possible instances of the face of that person. Okay, that's the face manifold for that person if you want. So what you can do is this really super expensive method of kind of trying all different dimensions of Z. One way to formulate this mathematically is to minimize the L0 norm of Z. So it's actually a slightly different thing. So what you can do is you choose a Z that's relatively high dimension, but for any given sample, you minimize the number of components of Z that are non-zero. That's called the L0 norm. It's just the count of the number of components that are non-zero. And it's very difficult to minimize that norm because it's not differentiable. It's very discrete. So what people use is they use a convex relaxation of that norm called the L1 norm. So the L1 norm is the sum of the absolute values of the components of Z. And that's what you use for R and Z, the sum of the absolute values of the components of Z. When you add this to your energy function, what the system is trying to do is find a Z that reconstructs the Y because it needs to minimize C of Y and Y bar, but also tries to minimize the number of its components that are non-zero because that's the best way to minimize the L1 norm. And that's called sparse coding. And it works really well. And I'm going to show you some examples of this. Before I go there, I just want to mention that, and we'll talk about this a little more, and it's the idea that adding noise to Z will also limit the information content of Z. I'll come back to this in a minute. Okay. So here is the idea of sparse coding. So sparse coding is an unconditional version of energy-based models. So there's no X. There's only a Y and a Z. And the energy function is Y minus WZ, where W is a so-called dictionary matrix, very similar to the prototype matrix in K-means. Z is a vector. Generally, the dimension of Z is larger than Y. And so you measure the square distance, Euclidean distance, between Y and WZ. So basically, your decoder here is linear. It's just a matrix. And then you add a term, lambda times the L1 norm of Z, which is represented by those two bars. And that's the energy function for sparse coding. And you can think of it as a special case of the architecture I showed previously, except it's not conditional. There's no X. Now, what does this do? So I'll probably tell you that the picture I'm showing here on the left is inappropriate because it's actually generated with a slightly different model. But it's a good sort of pictorial representation of what sparse coding attempts to do, which is to approximate the manifold of data by a piecewise linear approximation, essentially. So imagine that you have this W matrix, OK, and someone has given it to you or you've learned it in some way. Now, if you decide a priori that a certain number of components of Z are non-zero, OK, most of the components of Z are zero, just a small number of components of Z are non-zero, and you vary the value of those components, you know, within some range, the set of vectors that you're going to generate, the set of Y bars that you're going to generate, are going to be the Y bars that are in the linear subspace spanned by the corresponding columns of the W matrix. For every value of the Z coefficients that are non-zero, you basically compute a linear combination of the corresponding columns of W. And so you're basically moving along a low dimensional linear subspace of Y space. So Y bar is going to be basically along a low dimensional space, a low dimensional linear subspace. The dimension of that space will be the number of non-zero components of Z. OK, so for one particular Y, when you find a Z that minimizes the energy, a number of components are going to be non-zero. And as you move Y slowly, those non-zero components are going to change value, but you're going to stay on the same linear subspace until Y changes too much. And then all of a sudden, you need a different set of non-zero Z to do the best reconstruction. And now you're switching to a different plane. OK, because a different set of Z components become non-zero. And so now you move Y again, and again the coefficients in Z keep changing values, except for the ones that are zero, that stay zero, and all of a sudden it switches again. It goes to another one. So it's kind of well symbolized by the picture on the left, where you see that the manifold of data is approximated by basically a bunch of linear subspace, in this case lines. The reason why it's difficult to represent the actual sparse coding in 2D is because it's going to degenerate in 2D. So one question is how do we train a system like this? So to train a system like this, our loss function is just going to be the average energy that our model gives to our training samples. So the loss function is just the average energy, basically the average F. And remember F of Y is equal to the minimum over Z of E of Y and Z. OK, so we're going to take the average of F over all our training samples and minimize that average with respect to the parameters of the model. And those parameters are the coefficients in the W matrix. Again, it's called the dictionary matrix. So how do we do this? We take a sample Y, we find the Z that minimizes the energy, the sum of the two terms that you see here. And then we take one step of gradient descent in W. So we compute the gradient of the energy with respect to W, which is very simple because it's a quadratic function of W. And we take one step of stochastic gradient, basically. Right? And now we take the next Y and do it again, minimize with respect to Z. And then for that value of Z, compute the gradient with respect to W and take one step in the negative gradient. And you keep doing this. Now, if you just do this, it doesn't work. It doesn't work because the result is that W will keep getting bigger and bigger and Z will keep getting smaller and smaller. But the problem will not actually, the system will not actually solve the problem. So what you need to do is normalize the W matrix so that it cannot grow indefinitely and allow Z to shrink correspondingly. And the way to do this is that you basically after every update of the W matrix, you normalize the sum of the squares of the terms in a column of each column of W. Right? So normalize the columns of W after each update. And that will prevent the terms in W from blowing up and the terms in Z from shrinking. And it will force the system to actually find a reasonable matrix W and not get away with just making Z shorter. OK. So that's sparse coding. This was the learning algorithm for this was invented by two computational neuroscientists, Bruno Alsassen and David Field in 1997. And so that goes back a long time. OK. So here is the problem with sparse coding. The inference algorithm is kind of expensive. What you have to do is, you know, for a given Y is to kind of minimize the sum of those two terms, one of which is L2, the other one is L1. There's a very large number of papers in applied mathematics that explain how to do this efficiently. In particular, one algorithm to do so is called ISTA. That means iterative shrinkage and thresholding algorithm. And I'm going to tell you what ISTA is in just a minute. But it basically consists in basically alternating a sort of minimization with respect to Z of the first term and then the second term alternately. So here is the kind of abstract form of the ISTA algorithm. There's a fast version of it called called FISTA. And here it is at the bottom. Actually, I'm realizing that I'm missing the reference for the ISTA algorithm. This is not any of the references I'm showing here. FISTA is DeBoule. Anyway, so here is the algorithm. You start with Z equals zero and then you apply this iteration here, which is the second last formula. So in the bracket, the thing that's in the bracket is basically a gradient step in the squared error, the square reconstruction error. So if you compute the gradient of the square reconstruction error and you do a gradient step, you basically get this formula where one over R is the gradient step size. So you basically update Z with the negative gradient of the square reconstruction error. And then the next operation you do is a shrinkage operation. So you take every component of the resulting Z vector and you shrink all of them towards zero. So you basically subtract if the component of Z is positive, you subtract a constant to it from it. And if it's negative, you add the same constant to it. But if you get too close to zero, you just clip at zero. So basically it's a function that is flat around zero and then grows like the identity function above a certain threshold and below a certain threshold. It shrinks towards zero. If you keep iterating this algorithm for proper values of L and lambda, the Z vector will converge to the solution of the energy minimization problem, which is the minimum of this energy here, E of YZ with respect to Z. OK, and that suggests. So keep this in mind. Now here is an issue. This algorithm is kind of expensive. If you want to run this over an image or over old patches of an image or something like this, you're not going to be able to do this in real time on large images. And so here is an idea. The idea is to basically train a neural net to predict what the solution of the energy minimization problem is. OK, so you see the diagram here on the right where we train an encoder that takes the Y value. For now, you can ignore the piece that depends on X, right? You have X going to a predictor predicting H and then H feeds into the encoder of the decoder. You can ignore this part for now. In the unconditional version, you just have Y that goes to an encoder. It produces a prediction for what the optimal value of the Z variable is, OK, called Z bar. And then the Z variable itself goes into the decoder. It goes, you know, it's being regularized as well and then produces a reconstruction Y bar. And what you do here is, again, you find the Z value that minimizes the energy. But what we're going to, but the energy now is still the sum of those two terms, C of Y, Y bar and R of Z. But then what we're going to do is we're going to train the encoder to predict this optimal value of Z obtained through minimization. And this encoder is going to be trained by minimizing this term D of Z and Z bar. So basically it views Z as a target value and you train it by back prop, by, you know, gradient descent, to basically make a prediction that's as close to Z as possible. OK, that's one form of this idea. Another form of this idea, slightly more sophisticated, is that when you're doing the minimization with respect to Z, of the energy with respect to Z, you take into account the fact that you don't want Z to get too far away from Z bar. So basically your energy function now has three terms. It has the reconstruction error. It has the regularization. But it also has the difference between the Z bar, the prediction from the encoder, and the current value of the Z variable. So the energy function now is written here, E of X, Y, Z is equal to the C function that compares Y and the output of the decoder applied to Z. This is the unconditional version here. And then you have a second term, which is the D function that sort of measures the distance between Z and the encoder applied to Y. There shouldn't be an X. And then you also regularize Z. OK. So basically you're telling the system, find a value for the latent variable that reconstructs, that is sparse, if R is an L1 norm or doesn't have too much information, but also is not too far away from whatever it is that the encoder predicted. And a specific idea there is called Lista, which means the learning Lista, and it's to shape the architecture of the autoencoder so that it looks very much like a Lista algorithm. So if we go back to the Lista algorithm, the formula, the second last formula here, you know, looks like some vector update with some matrix. So it's like a linear stage of a neural net, if you want, and then some non-linearity that happens to be a shrinkage, which is sort of a double ReLU, if you want. It's one ReLU going up married with another ReLU going down. And so if you look at the diagram of this whole Lista algorithm, it looks like this block diagram here that I've drawn on top. You start with Y, multiply it by some matrix, and then shrink the result. Then it gives you the next Z. Apply some other matrix to it, add it to the previous value of Z that you had, shrink again, and then multiply the matrix again, add to the previous value you had, shrink again, et cetera. And so you have two matrices here, W, E, and S. And at the bottom, if you define W, E as 1 over LWD, and you define S as the identity minus 1 over LWD transpose WD, where WD is the decoding matrix, then this diagram basically implements Lista. So the idea that one of my former postdocs, Carl Greger, had was to say, well, why don't we treat this as a recurrent neural net? And why don't we train those matrices W and S so as to give us a good approximation of the optimal sparse code as quickly as possible? So we're basically going to build our encoder network with this architecture that is copied on Lista. And we know for a fact that there is going to be a solution where the system basically learns the value of W, E, and S that correspond to the one they should be. But in fact, the system learns something else. So this is another representation of it here at the bottom left. We have those shrinkage function and then this S matrix. And then you add the Y multiplied by W, E to the S matrix, shrink again, et cetera, et cetera. So this is the recurrent net we're going to train with W, E, and S. The objective of this Lista is, can you repeat, what is the objective? I think I missed a point. So the objective of training this encoder, so the encoder in this diagram on the right here, the architecture of the encoder is the one you see at the bottom left. And the objective that you're training this with is the average of D of Z and Z bar. So the procedure is, in the case where there is no X, but if there is an X, it doesn't make much difference. So take a Y. For this particular Y, find a value of Z that minimizes the energy. And the energy is the sum of three terms, C of Y, Y bar, R of Z, and D of Z, Z bar. So find a Z that reconstructs, has minimal capacity, but also is not too far away from the output of the encoder. Once you have the Z, compute the gradient of the energy with respect to the weights of the decoder, of the encoder, and the predictor if you have one, by back prop. So the interesting thing is that the only gradient you're going to get for the encoder is the gradient of D of Z and Z bar. So the encoder is just going to train itself to minimize D of Z and Z bar. In other words, it's going to train itself to predict Z as well as possible, the optimal Z that you obtain through minimization. The decoder is going to train itself to, of course, reconstruct Y as well as it can with the Z that is being given. And then if you have a predictor, you're going to get gradient to the predictor and it's going to try to kind of produce an H that helps as well as possible. Is that clear? Yeah, thanks. Okay, so that's the architecture. It's basically just a pretty broadened variety recurrent net. And this works really well in the sense that as you go through the iterations of this ISTA algorithm or through this train neural net that is designed to basically approximate this solution, what you do is you can train the system, for example, to produce the best possible solution after three iterations only. So it knows the optimal value because it's been computed with ISTA. But then when you train it, it trains itself to produce the best approximation of that value with only three iterations. And what we see is that after three iterations, it produces a much, much better approximation than ISTA would produce in three iterations. And so what you see here is the number as a function of number of iterations of either ISTA or this Lista algorithm is the reconstruction error. So by training an encoder to kind of predict the result of the optimization, you actually get better results than if you actually run the optimization for the same number of iterations. So it accelerates inference a lot. Okay, so this is what sparse coding gives you with or without an encoder. Actually, you get pretty much the same results here when you train on MNIST. So these are basically a linear decoder. The code space here, the Z vector, has size 256. And so you take this 256 Z vector multiplied by matrix and you reconstruct a digit. And what you see here are the columns of this matrix represented as images. So each column has the same dimension as an MNIST digit, right? Each column of W. And so you can represent each of them as an image. And these are the 256 columns of W. And what you see is that they basically represent parts of characters like little pieces of strokes. And the reason for this is that you can basically reconstruct any character, any MNIST digit by a linear combination of a small number of those strokes. And so that's kind of beautiful because this system basically finds constitutive parts of objects in a completely unsupervised way. And that's kind of what you want out of unsupervised learning. You want sort of what are the components or the parts that can explain what my data looks like. So this works really beautifully for MNIST. It works quite nicely as well for natural image patches. There's supposed to be an animation here, but you're not seeing it obviously because it's PDF. But the result is this. So the animation shows the learning algorithm taking place. So here again, these are the columns of the decoding matrix of a sparse coding system with L1 regularization that has been trained on natural image patches. And I said that those natural image patches have been whitened, which means they've been sort of normalized in some way, you know, cancel the mean and kind of normalize the variance. And you get nice little what's called Gabor filters. So basically, you know, small edge detectors at various orientations, locations and sizes. So the reason why this was invented by neuroscientists is that this looks very much like what you observe in the primary area of the visual cortex. When you poke electrodes in the visual cortex of most animals and you figure out what patterns do they maximally respond to, they may actually respond to oriented edges. This is also what you observe when you train a convolutional net. On ImageNet, the first layer of features look very much like this as well. Except they are convolutional. These ones are not convolutional. It's trained on image patches, but there is no convolutions here. So this is nice because what it tells you is that with a very simple unsupervised learning algorithm, we get essentially qualitatively the same features that we would get by training a large convolutional net supervised. So that gives you a hint. So here is the convolutional version. So the convolutional version basically says you have an image. This is more responsive, by the way. What you're going to do is you're going to take feature maps. Okay. Let's say here four, but it could be more. And then you're going to convolve each of those feature maps with a kernel. Okay. So a feature map is, I don't know, let's call this ZK. And we have a kernel here. Well, let's call it Zi because I'm going to use K for the kernel. Ki. And this is going to be a reconstruction Y. And our reconstruction is simply going to be the sum over i of Zi convolved with Ki. Okay. So this is different from the original sparse coding where Y bar was equal to the sum over columns of a column of a W matrix. So and multiply by a coefficient Zi, which is now a scalar. So regular sparse coding, you have a weighted sum of columns where the weights are scalar coefficients of Zi's. In convolutional sparse coding, it's again a linear operation, but now the dictionary matrix is a bunch of convolution kernels and the latent variable is a bunch of feature maps. And you're doing a convolution of each feature map with the each kernel and sum up the results. This is what you get. So here there are, it's one of those systems that has a decoder and an encoder. The decoder is very simple here. It's basically just essentially a single layer network with nonlinearity. And then there is a simple layer after that, basically a diagonal layer after that to change the gains. But it's very, very simple. And the filters and the encoders and the decoder look very similar. So it's basically the encoder is just a convolution, then some nonlinearity. I think it was hyperbolic tangent in that case. And then basically what amounts to a diagonal layer that just changed the scale. And then the decoder, then there is a sparsity on the constraint on the code. And then the decoder is just a convolutional linear decoder. And the reconstruction is just a square distance. So if you impose that there is only one filter, the filter looks like the one at the top left. It's just a center surround type filter. If you allow two filters, you get kind of two weirdly shaped filters. If you let four filters, which is the third row, you get oriented edges, horizontal and vertical. We get two polarities for each of the filters. For eight filters, you get oriented edges at eight different orientations. 16 filters, you get more orientations and you also get center surround. And then as you increase the number of filters, you get sort of more diverse filters, not just edge detectors, but also grating detectors at various orientations, center surround, et cetera. And that's very interesting because this is the kind of stuff you see in the visual cortex. So again, this is an indication that you can learn really good features in completely uncircumcised way. Now, here is the sad news. If you take those features, you plug them into a convolutional net and you train that on some task, you don't necessarily get better results than you train on ImageNet from scratch. But there are a few instances where this has helped boost the performance, particularly in cases where the number of label samples is not that great or the number of categories is small. And so by training purely supervised, you get degenerate features, basically. Here's another example here. Same thing. Again, it's a convolutional sparse coding. Here the decoding kernel, this is on color images. The decoding kernels are nine by nine applied convolutionally over an image. And what you see on the left here are the sparse codes. Here you have 64 feature maps. And you can see that the Z vector is extremely sparse. There's only a few components here that are either white or black or non-gray, if you want. And this is because of sparse Z. In the last few minutes, we'll talk about variational autoencoder. And I guess you've heard a bit of this from... Tomorrow. We're going to be covering this tomorrow with the bubbles and the code and everything. So tomorrow is going to be one hour of just this. Right. So here is a preview for how variational encoders work. Variational autoencoders are basically the same architecture as the one I showed previously. So basically, an autoencoder ignores the conditional part, the part that's conditioned upon X for now. That could be a conditional variational autoencoder, but for now we're just going to have a regular variational autoencoder. So it's an autoencoder where you take a variable Y, you run it through an encoder. It could be a multilayer neural net, convolutional net, whatever you want. It produces a prediction for the sparse code. Z bar is a term in the energy function that measures the square Euclidean distance between Z, the latent variable, and Z bar. And there's also another cost function here, which is the L2 norm of Z bar. In fact, generally it's more the L2 norm of Z, actually. That would be more accurate, but it doesn't make much difference. And then Z goes through a decoder, which reconstructs Y, and that's your reconstruction error. Now, the difference with previous... so this looks very, very similar to the type of autoencoder we just talked about, except there is no sparsity here. And the reason there is no sparsity is because variational autoencoders use another way of limiting the information content of the code by basically making the code noisy. So here is the idea. The way you compute Z is not by minimizing the energy function with respect to Z, but by sampling Z randomly, according to a distribution whose logarithm is the cost that links it to Z bar. So basically, the encoder produces a Z bar, and then there is an energy function that measures the distance, if you want, between Z and Z bar. You think of this as the logarithm of a probability distribution. So if this distance is a square Euclidean distance, what that means is that the distribution of Z is going to be a conditional Gaussian, where the mean is Z bar. So what we're going to do is we're going to sample a random value of Z according to that distribution, basically a Gaussian whose mean is Z bar. And that just means adding a bit of Gaussian noise to Z bar. That's what our Z is going to be. And you run this through the decoder. So when you train a system like this, what the system wants to do is basically make the Z bar as large as possible, make the Z bar vectors as large as possible, so that the effect of the Gaussian noise on Z would be as small as possible, relatively speaking. If the variance of the noise on Z is one, and you make the Z bar vector very, very long, like a norm of a thousand, then the importance of the noise would be 0.1% with respect to Z. So if you train an autoencoder like this, ignoring the fact that you add noise by just backpropagation, what you'll get is Z bar vectors that get bigger and bigger. The weights of the encoder will get bigger and bigger, and the Z bar vector will get bigger and bigger. So what's the trick in variational autoencoder? Hold on, quick question. Where does this Z come from? Is Z a latent variable never observed? It's a latent variable that we are sampling and we're not minimizing with respect to it. So in previous cases, we were minimizing with respect to the Z variable, minimizing the energy with respect to the variable, finding the Z that minimizes the sum of C, D, and R. So here we're not minimizing, we're just sampling. We're viewing the energy as a distribution, as a log of a distribution, and we're sampling Z from that distribution. All right, so imagine our encoder produces the following points for training samples. So these are the Z bar vectors produced by the encoder at some point in training. So what the effect of this sampling of Z is going to do is basically turn every single one of those training samples into a fuzzy ball. Because we take a sample, we add noise to it, and so basically we've turned a single code vector into kind of a fuzzy ball. Now, the decoder needs to be able to reconstruct the input from whatever code is being fed. And so if two of those fuzzy ball intersect, then there is some probability for the decoder to basically make a mistake and confuse the two samples, confuse one sample for the other. So the effect of training the system, if you add fuzzy balls, if you make every one of your code a fuzzy ball, is that those fuzzy balls are going to fly away from each other. And as I said before, this is the same, I was saying this before in a different way, it's going to make the weights of the encoder very large so that the code vectors get very long and basically they get away from each other, and the noise of those fuzzy balls don't matter anymore. So here, if the fuzzy balls don't intersect, the system will be able to perfectly reconstruct every sample you throw at it. My question, it was a couple of slides ago, but again on the same topic, it was a couple of slides ago. So what exactly do you mean by degenerate features here when you said that when you were comparing self-supervision and normal completely supervision? I see. Okay, that's a good question. What I was saying is, it's something I said before in different terms, it's the fact that if you train a classifier, let's say a convolutional net, on a problem that has very few categories, let's say face detection, you only have two categories, the representation of faces you get out of the convolutional net are very degenerate in the sense that they don't represent every image properly. They're going to kind of collapse a lot of different images into sort of a common, into identical representations because the only thing the system needs to do is discriminate from faces with non-faces. And so it doesn't need to really kind of produce good representations of the entire space. It just needs to tell you if it's a face or not a face. So for example, the features you will get for two different faces will probably be fairly identical. So that's what I mean by degenerate features. What you want are features that basically, feature vectors that are different for different objects, regardless of whether you train them to be different or not. So if you train on ImageNet, for example, you have 1000 categories. And so because you have a lot of categories, you get features that are fairly diverse and they cover a lot of the space of possible images. I mean, they're still kind of fairly specialized, but they're not completely degenerate because you have many categories. And you have a lot of samples. The more samples and the more categories you have, the better your features are. In fact, if you think about it, an autoencoder is a neural net in which every training sample is its own category. Right? Because you're basically telling the system to produce a different output for every sample I show you. So you're basically training the system to represent every object in a different way. But it can be degenerated in another way because the system can learn the identity function and encode anything you want. If you think about the Siamese nets, the metric learning system, the contrastive methods, MoCo, Perl and SeamClear, I was telling you about. It's a little bit of the same thing. They try to learn non-degenerate features by telling the system, you know, here is two objects that I know are the same. Here are two objects that I know are different. And so make sure you produce different feature vectors for objects that I know are semantically different. That's kind of a way of making sure you get feature vector representations that are different for things that are actually different. But you don't get this by training a conv net on a two-class problem or a ten-class problem. You need as many classes as you can afford. So pre-training using self-supervised learning basically helps making the feature more generic and less degenerate for the problem. OK, so let's get back to variational autoencoders. So again, if you train your autoencoder with those fuzzy balls, they're going to fly away from each other. And what you want really is you want those fuzzy balls to basically kind of cluster around some sort of data manifold. So you want to actually keep them as close to each other as possible. And how can you do this? You can do this by essentially linking all of them to the origin with a spring. So basically, the spring wants to bring all those points towards the origin as close to each other to the origin as possible. And so in doing so, what the system is going to try to do is pack those fuzzy spheres as close to the origin as possible. And it's going to make them sort of overlap, interpenetrate. But of course, if they interpenetrate too much, if the two spheres for two very different samples interpenetrate too much, then those two samples are going to be confused by the decoder and the reconstruction energy is going to get large. And so what the system ends up doing is only letting two spheres overlap if the two samples are very similar. And so basically, by doing this, the system finds some sort of representation of the manifold. It puts those code vectors along a manifold if there is one. And that's the basic idea of Variational Autoencoder. Now, you can derive this with math, and it doesn't make anything much easier to understand. In fact, it's much more abstract. But that's basically what it does in the end. So there's a couple more tricks there in the Variational Autoencoder idea, and you'll get some details with Alfredo tomorrow. You can adapt the size of those fuzzy balls. So basically, you can have the encoder compute the optimal size of the balls in each direction. And what you have to do is make sure that the balls don't get too small. And so you put a penalty function that tries to make the variance of those balls, the size in each dimension, if you want, as close to one as possible. They can get a bit smaller. They can get larger if they want. But there's a cost for making them different from one. So now the trick, the problem you have with this is to adjust the relative importance of this spring strength. If the spring is too large, if the spring is too strong, then the fuzzy balls are all going to collapse in the center, and the system is not going to be able to reconstruct properly. If it's too weak, then the fuzzy balls are going to fly away from each other, and the system is going to be able to reconstruct everything and anything. And so you have to strike a balance between the two. And that's kind of the difficulty with Variational Autoencoder. If you increase the strength of the spring a little too much, it's a term called KL divergence in the system. So KL divergence between the Gaussian, basically, of a Gaussian, it collapses. All the fuzzy balls basically get to the center, and the system does not actually model it properly. I have a question about one of the previous lectures, actually. So is that all right? Sure. So when you were talking about linearizability, so generally when you were saying that stacking linear layers one after the other without having nonlinearities is basically redundant because we can have one linear layer to do it. But I remember that you also mentioned there is one particular reason why you might want to do this, where you just stack linear layers after this. And you said, well, there's one reason, but you didn't go into that reason. So I was wondering if there is anything significant behind that. So the situation I was describing is that imagine you have some big neural net, and it produces a feature vector of a certain size. And your output is extremely large because maybe you have many, many categories. Maybe you're doing phoneme classification for speech recognition system. So the number of categories here is 10,000 or something like that. OK, I have to draw slowly. So if your feature vector here is itself something like 10,000, the matrix to go from here to here will be 100 million, right? And that's probably a bit too much. So what people do is they say we're going to factorize that matrix into the product of two skinny matrices. So the middle dimension here is maybe a thousand. Well, you have 10k on the input, 10k on the output, and then the middle one is 1k, right? So if you don't have the middle one, then the number of parameters you have is 10 to the 8. If you do have the middle one, it's two times 10 to the 7. So you get a factor of 10. If you make it 100, then it's 10 to the 6. So it becomes more manageable. So basically you get a low rank factorization. So the overall matrices here that you can call W is now going to be the product of two smaller matrices U and V. And because U and V, the middle dimension, if you want, of U and V is smaller, say 100, then the rank of the corresponding matrix W will be smaller. There are people who do this without actually specifying the dimension of the middle layer by doing what's called a minimization of nuclear norm, which is equivalent. But I don't want to go into this, but that would be kind of a situation where you might want to actually decompose a matrix into a product of two matrices to kind of save parameters, essentially, or save computation. There's also another interesting phenomenon, which is that it turns out that both learning and generalization actually are better when you do this kind of factorization. So even though now the optimization with respect to this matrix becomes non-convex, it actually converges faster using stochastic gradient. There's a paper by, or series of paper by Nadav, if you're interested in this, Nadav Cohen. I think it's from 2018. His co-author with Sanjeev Arora from Princeton. Nadav was a postdoc with Sanjeev Arora, and they had a series of papers on this that explains why even linear networks actually converge faster, and they use it also to basically study the dynamics of learning in sort of non-convex optimization, as well as the generalization properties. How important is it to have a kind of matching in a variational autoencoder? How important is it to have a matching kind of architecture of an encoder and a decoder? There's no reason for the two architectures to match. It's very often the case that decoding is much easier than encoding. So if you take the example of sparse coding, which I talked about, in sparse coding, where the encoder is one of those sort of Lista type encoder, the encoder is actually quite complicated, whereas the decoder is linear. This is kind of a special case because the code is high dimensional and sparse. And so, high dimensional sparse codes, you know, any function of a high dimensional sparse code to anywhere basically can be, is quasi-linear. So one way to make a function linear is to basically represent this input variable in a high dimensional space using a nonlinear transformation. We've talked about this when we discussed what were good features. And good features generally consist in sort of expanding the dimension of the representation in a nonlinear way and making this representation sparse. And the reason for this is that now you make your function linear. So you could very well have a very complex encoder and a very simple decoder, possibly linear one, as long as your code is high dimensional. Now if you have a low dimensional code, so an autoencoder with a middle layer is very narrow, where the code layer is very narrow, then it could become very complicated to do the decoding. It may become a very highly nonlinear function now to do the decoding. And so now you may need multiple layers. There's no reason again to think that the architecture of the decoder should be similar to the architecture of the encoder. Now there is, you know, there might be, okay, that said, there might actually be good reason for it. Okay. And in fact there are models that I haven't talked about because they're not kind of proven, but which are called stacked autoencoders, where basically you have this idea. So, you essentially have an autoencoder where you have a reconstruction error. I'm actually going to erase this and make it look like the autoencoder that we talked about, where there is sort of a cost for making the latent variable here different from the output of the encoder. So this is a Z bar and this is a Z if you want. Okay. Now, that's an autoencoder drawn in a funny way. So this is Y and this is Y bar at the bottom. Now I can stack another one of those guys on top. Okay, now I'm going to have to call this Z1 and I'm going to call this Z2, etc. Okay, this I'm going to call Y bar and this I'm going to call Y. Now, what's interesting about, okay, I'm going to call the bottom one X now, change name. Now, if you look at the, you ignore the right part of the system, look at the left part, you go to Y, and that looks very much like a classical recognizer where X is the input, Y bar is a prediction for the output and Y is the desired output and there is a cost function that measures the difference between the two. Okay. The other branch that goes from Y to X, that's kind of like a decoder, where Y is the code. But then you have codes all the way in the middle because it's kind of a stacked autoencoder, right. So every pair of layer, every pair of encoder decoder is kind of a little autoencoder and you kind of stack them on top of each other. And what you'd like is find a way of training the system in such a way that if you don't have a label for a sample, so you don't know Y for a sample, you just train this as an autoencoder. But if you do have a Y, then you clamp Y to its desired value and then this system becomes now a combination of a predictor, a recognizer, and an autoencoder. Now there is a slight problem with this picture, a number of different problems. The first problem is that if, again, if Z1, for example, has enough capacity and you only train on unlabeled samples, the system is only going to carry the information through Z1 and is going to just completely ignore the top layers because it's not enough capacity in Z1 to do the perfect reconstruction. So it's going to just put all the information through Z1 and then all the other Z2 and Y will be constant because the system won't need them. So again, you will need to regularize Z to prevent it from capturing all the information. And same for the other layers, perhaps. Now the other thing is, would, you know, do this thing need to be linear or nonlinear? And that depends on the relative size of the various Zs. So if you go from low dimension to high dimension, you need something that's nonlinear. But if you go from high dimension to low dimension, you can probably do it with a linear, kind of like spot coding. And so you will see that the system may have, you know, an alternation of linear and nonlinear stages in sort of opposite phase if you want because you need linear to go from high dimension to low dimension and then nonlinear to go from low dimension to high dimension and then again linear to go back to low dimension and it's the opposite the other way around. This is, you know, people have been proposing things like this, but not really sort of trying them on a large scale. So there's a lot of sort of open questions around those things. If you're curious, one paper that I've worked on with one of my former students called Jake Jow, is a system called stacked what where autoencoder. And it's a system a bit of this type, but there is sort of extra variables kind of going this way, which are basically the position of switches in pooling. I don't want to go into details, but if you look for a paper about stacked what where autoencoders, you will find two paper, one by Jake and myself and a photo web paper by a group from Michigan, University of Michigan that basically enhanced it and sort of trained them on ImageNet and got some decent results. So those are kind of architectures you can use to do kind of self-supervised learning. Just to clarify the parameters for the spring, is for the KL divergence term in the loss? Right. Yeah, so the KL divergence term in the loss. We're going to see this tomorrow, guys. So we're going to be going through the equation and all the details. So I'll cover this tomorrow. So I'll see you tomorrow, hopefully with a video as well. If the bandwidth supports it, I will put the recording of this class online as soon as it's actually available to me. I will add it to the NYU streaming platform and then, you know, I will try to clean it up as I can and, you know, upload it as well on YouTube later on. All right. So thank you again. Stay home, stay warm and I see you tomorrow. Stay safe. Bye bye. Stay safe.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.0, "text": " All right, you guys see the slides, I assume?", "tokens": [50364, 1057, 558, 11, 291, 1074, 536, 264, 9788, 11, 286, 6552, 30, 50514, 50614, 28327, 78, 11, 286, 393, 536, 291, 11, 286, 393, 536, 2878, 1646, 13, 50714, 50714, 1079, 13, 50764, 50764, 1057, 558, 13, 50814, 50814, 865, 11, 309, 311, 439, 665, 13, 50864, 50864, 509, 393, 611, 652, 7880, 11, 286, 393, 536, 291, 13, 50964, 50964, 1033, 11, 370, 321, 434, 516, 281, 920, 751, 466, 2281, 12, 6032, 5245, 11, 293, 5240, 294, 264, 4319, 295, 2698, 12, 48172, 24420, 2539, 420, 2693, 12879, 24420, 2539, 11, 9289, 689, 321, 1411, 766, 1036, 565, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20281702677408855, "compression_ratio": 1.6175115207373272, "no_speech_prob": 0.10449784994125366}, {"id": 1, "seek": 0, "start": 5.0, "end": 7.0, "text": " Alfredo, I can see you, I can see anyone else.", "tokens": [50364, 1057, 558, 11, 291, 1074, 536, 264, 9788, 11, 286, 6552, 30, 50514, 50614, 28327, 78, 11, 286, 393, 536, 291, 11, 286, 393, 536, 2878, 1646, 13, 50714, 50714, 1079, 13, 50764, 50764, 1057, 558, 13, 50814, 50814, 865, 11, 309, 311, 439, 665, 13, 50864, 50864, 509, 393, 611, 652, 7880, 11, 286, 393, 536, 291, 13, 50964, 50964, 1033, 11, 370, 321, 434, 516, 281, 920, 751, 466, 2281, 12, 6032, 5245, 11, 293, 5240, 294, 264, 4319, 295, 2698, 12, 48172, 24420, 2539, 420, 2693, 12879, 24420, 2539, 11, 9289, 689, 321, 1411, 766, 1036, 565, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20281702677408855, "compression_ratio": 1.6175115207373272, "no_speech_prob": 0.10449784994125366}, {"id": 2, "seek": 0, "start": 7.0, "end": 8.0, "text": " Yes.", "tokens": [50364, 1057, 558, 11, 291, 1074, 536, 264, 9788, 11, 286, 6552, 30, 50514, 50614, 28327, 78, 11, 286, 393, 536, 291, 11, 286, 393, 536, 2878, 1646, 13, 50714, 50714, 1079, 13, 50764, 50764, 1057, 558, 13, 50814, 50814, 865, 11, 309, 311, 439, 665, 13, 50864, 50864, 509, 393, 611, 652, 7880, 11, 286, 393, 536, 291, 13, 50964, 50964, 1033, 11, 370, 321, 434, 516, 281, 920, 751, 466, 2281, 12, 6032, 5245, 11, 293, 5240, 294, 264, 4319, 295, 2698, 12, 48172, 24420, 2539, 420, 2693, 12879, 24420, 2539, 11, 9289, 689, 321, 1411, 766, 1036, 565, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20281702677408855, "compression_ratio": 1.6175115207373272, "no_speech_prob": 0.10449784994125366}, {"id": 3, "seek": 0, "start": 8.0, "end": 9.0, "text": " All right.", "tokens": [50364, 1057, 558, 11, 291, 1074, 536, 264, 9788, 11, 286, 6552, 30, 50514, 50614, 28327, 78, 11, 286, 393, 536, 291, 11, 286, 393, 536, 2878, 1646, 13, 50714, 50714, 1079, 13, 50764, 50764, 1057, 558, 13, 50814, 50814, 865, 11, 309, 311, 439, 665, 13, 50864, 50864, 509, 393, 611, 652, 7880, 11, 286, 393, 536, 291, 13, 50964, 50964, 1033, 11, 370, 321, 434, 516, 281, 920, 751, 466, 2281, 12, 6032, 5245, 11, 293, 5240, 294, 264, 4319, 295, 2698, 12, 48172, 24420, 2539, 420, 2693, 12879, 24420, 2539, 11, 9289, 689, 321, 1411, 766, 1036, 565, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20281702677408855, "compression_ratio": 1.6175115207373272, "no_speech_prob": 0.10449784994125366}, {"id": 4, "seek": 0, "start": 9.0, "end": 10.0, "text": " Yeah, it's all good.", "tokens": [50364, 1057, 558, 11, 291, 1074, 536, 264, 9788, 11, 286, 6552, 30, 50514, 50614, 28327, 78, 11, 286, 393, 536, 291, 11, 286, 393, 536, 2878, 1646, 13, 50714, 50714, 1079, 13, 50764, 50764, 1057, 558, 13, 50814, 50814, 865, 11, 309, 311, 439, 665, 13, 50864, 50864, 509, 393, 611, 652, 7880, 11, 286, 393, 536, 291, 13, 50964, 50964, 1033, 11, 370, 321, 434, 516, 281, 920, 751, 466, 2281, 12, 6032, 5245, 11, 293, 5240, 294, 264, 4319, 295, 2698, 12, 48172, 24420, 2539, 420, 2693, 12879, 24420, 2539, 11, 9289, 689, 321, 1411, 766, 1036, 565, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20281702677408855, "compression_ratio": 1.6175115207373272, "no_speech_prob": 0.10449784994125366}, {"id": 5, "seek": 0, "start": 10.0, "end": 12.0, "text": " You can also make signs, I can see you.", "tokens": [50364, 1057, 558, 11, 291, 1074, 536, 264, 9788, 11, 286, 6552, 30, 50514, 50614, 28327, 78, 11, 286, 393, 536, 291, 11, 286, 393, 536, 2878, 1646, 13, 50714, 50714, 1079, 13, 50764, 50764, 1057, 558, 13, 50814, 50814, 865, 11, 309, 311, 439, 665, 13, 50864, 50864, 509, 393, 611, 652, 7880, 11, 286, 393, 536, 291, 13, 50964, 50964, 1033, 11, 370, 321, 434, 516, 281, 920, 751, 466, 2281, 12, 6032, 5245, 11, 293, 5240, 294, 264, 4319, 295, 2698, 12, 48172, 24420, 2539, 420, 2693, 12879, 24420, 2539, 11, 9289, 689, 321, 1411, 766, 1036, 565, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20281702677408855, "compression_ratio": 1.6175115207373272, "no_speech_prob": 0.10449784994125366}, {"id": 6, "seek": 0, "start": 12.0, "end": 27.0, "text": " Okay, so we're going to still talk about energy-based models, and mostly in the context of self-supervised learning or unsupervised learning, continuing where we left off last time.", "tokens": [50364, 1057, 558, 11, 291, 1074, 536, 264, 9788, 11, 286, 6552, 30, 50514, 50614, 28327, 78, 11, 286, 393, 536, 291, 11, 286, 393, 536, 2878, 1646, 13, 50714, 50714, 1079, 13, 50764, 50764, 1057, 558, 13, 50814, 50814, 865, 11, 309, 311, 439, 665, 13, 50864, 50864, 509, 393, 611, 652, 7880, 11, 286, 393, 536, 291, 13, 50964, 50964, 1033, 11, 370, 321, 434, 516, 281, 920, 751, 466, 2281, 12, 6032, 5245, 11, 293, 5240, 294, 264, 4319, 295, 2698, 12, 48172, 24420, 2539, 420, 2693, 12879, 24420, 2539, 11, 9289, 689, 321, 1411, 766, 1036, 565, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20281702677408855, "compression_ratio": 1.6175115207373272, "no_speech_prob": 0.10449784994125366}, {"id": 7, "seek": 2700, "start": 27.0, "end": 35.0, "text": " So let me start with a little bit of a reminder of where we left last time.", "tokens": [50364, 407, 718, 385, 722, 365, 257, 707, 857, 295, 257, 13548, 295, 689, 321, 1411, 1036, 565, 13, 50764, 50764, 492, 2825, 466, 2698, 12, 48172, 24420, 2539, 382, 264, 1558, 295, 1936, 1382, 281, 6069, 1203, 490, 1203, 1646, 11, 22106, 300, 257, 644, 295, 264, 4846, 307, 406, 8974, 281, 264, 1185, 293, 1071, 644, 307, 8974, 11, 293, 321, 3847, 264, 1185, 281, 6069, 264, 2107, 12, 48990, 644, 490, 264, 8974, 644, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06751222014427186, "compression_ratio": 1.7718446601941749, "no_speech_prob": 0.0004736326227430254}, {"id": 8, "seek": 3500, "start": 35.0, "end": 57.0, "text": " We talked about self-supervised learning as the idea of basically trying to predict everything from everything else, pretending that a part of the input is not visible to the system and another part is visible, and we train the system to predict the non-visible part from the visible part.", "tokens": [50364, 492, 2825, 466, 2698, 12, 48172, 24420, 2539, 382, 264, 1558, 295, 1936, 1382, 281, 6069, 1203, 490, 1203, 1646, 11, 22106, 300, 257, 644, 295, 264, 4846, 307, 406, 8974, 281, 264, 1185, 293, 1071, 644, 307, 8974, 11, 293, 321, 3847, 264, 1185, 281, 6069, 264, 2107, 12, 48990, 644, 490, 264, 8974, 644, 13, 51464, 51464, 400, 295, 1164, 11, 309, 727, 312, 1340, 11, 309, 727, 312, 644, 295, 257, 960, 11, 420, 309, 727, 312, 746, 1646, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.059304177075967024, "compression_ratio": 1.892156862745098, "no_speech_prob": 0.0005426419666036963}, {"id": 9, "seek": 3500, "start": 57.0, "end": 61.0, "text": " And of course, it could be anything, it could be part of a video, or it could be something else.", "tokens": [50364, 492, 2825, 466, 2698, 12, 48172, 24420, 2539, 382, 264, 1558, 295, 1936, 1382, 281, 6069, 1203, 490, 1203, 1646, 11, 22106, 300, 257, 644, 295, 264, 4846, 307, 406, 8974, 281, 264, 1185, 293, 1071, 644, 307, 8974, 11, 293, 321, 3847, 264, 1185, 281, 6069, 264, 2107, 12, 48990, 644, 490, 264, 8974, 644, 13, 51464, 51464, 400, 295, 1164, 11, 309, 727, 312, 1340, 11, 309, 727, 312, 644, 295, 257, 960, 11, 420, 309, 727, 312, 746, 1646, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.059304177075967024, "compression_ratio": 1.892156862745098, "no_speech_prob": 0.0005426419666036963}, {"id": 10, "seek": 6100, "start": 61.0, "end": 73.0, "text": " There is a special case where we don't assume that anything is visible at any time, and so we're just asking the system to just predict out of the blue without any input.", "tokens": [50364, 821, 307, 257, 2121, 1389, 689, 321, 500, 380, 6552, 300, 1340, 307, 8974, 412, 604, 565, 11, 293, 370, 321, 434, 445, 3365, 264, 1185, 281, 445, 6069, 484, 295, 264, 3344, 1553, 604, 4846, 13, 50964, 50964], "temperature": 0.0, "avg_logprob": -0.03768504538187167, "compression_ratio": 1.3709677419354838, "no_speech_prob": 9.298178338212892e-05}, {"id": 11, "seek": 7300, "start": 73.0, "end": 92.0, "text": " So we talked about the approach of energy-based models, which consists in essentially basically having an implicit function that captures the dependency between X and Y, or in the case where you don't have an X, is the dependency between the various components of Y.", "tokens": [50364, 407, 321, 2825, 466, 264, 3109, 295, 2281, 12, 6032, 5245, 11, 597, 14689, 294, 4476, 1936, 1419, 364, 26947, 2445, 300, 27986, 264, 33621, 1296, 1783, 293, 398, 11, 420, 294, 264, 1389, 689, 291, 500, 380, 362, 364, 1783, 11, 307, 264, 33621, 1296, 264, 3683, 6677, 295, 398, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.08373924664088658, "compression_ratio": 1.528735632183908, "no_speech_prob": 5.646314821206033e-05}, {"id": 12, "seek": 9200, "start": 92.0, "end": 103.0, "text": " And the reason why we need an implicit function is that for a particular value of X, there could be multiple values of Y that are possible.", "tokens": [50364, 400, 264, 1778, 983, 321, 643, 364, 26947, 2445, 307, 300, 337, 257, 1729, 2158, 295, 1783, 11, 456, 727, 312, 3866, 4190, 295, 398, 300, 366, 1944, 13, 50914, 50914, 400, 370, 498, 321, 632, 257, 2047, 17630, 490, 1783, 281, 398, 11, 321, 727, 787, 652, 472, 17630, 11, 293, 1228, 364, 26947, 2445, 11, 321, 393, 652, 3866, 21264, 26947, 356, 538, 1936, 1419, 257, 2445, 300, 2709, 2295, 2281, 337, 3866, 4190, 295, 398, 337, 257, 2212, 2158, 295, 1783, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.05499517783690035, "compression_ratio": 1.869767441860465, "no_speech_prob": 0.00012279899965506047}, {"id": 13, "seek": 9200, "start": 103.0, "end": 118.0, "text": " And so if we had a direct prediction from X to Y, we could only make one prediction, and using an implicit function, we can make multiple predictions implicitly by basically having a function that gives low energy for multiple values of Y for a given value of X.", "tokens": [50364, 400, 264, 1778, 983, 321, 643, 364, 26947, 2445, 307, 300, 337, 257, 1729, 2158, 295, 1783, 11, 456, 727, 312, 3866, 4190, 295, 398, 300, 366, 1944, 13, 50914, 50914, 400, 370, 498, 321, 632, 257, 2047, 17630, 490, 1783, 281, 398, 11, 321, 727, 787, 652, 472, 17630, 11, 293, 1228, 364, 26947, 2445, 11, 321, 393, 652, 3866, 21264, 26947, 356, 538, 1936, 1419, 257, 2445, 300, 2709, 2295, 2281, 337, 3866, 4190, 295, 398, 337, 257, 2212, 2158, 295, 1783, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.05499517783690035, "compression_ratio": 1.869767441860465, "no_speech_prob": 0.00012279899965506047}, {"id": 14, "seek": 11800, "start": 118.0, "end": 138.0, "text": " And that's a little bit what's represented on the left with essentially, you can think of this as some sort of landscape, mountainous landscape, where the data points are in the valleys, and everything else at size, the manifold of data is as higher energy.", "tokens": [50364, 400, 300, 311, 257, 707, 857, 437, 311, 10379, 322, 264, 1411, 365, 4476, 11, 291, 393, 519, 295, 341, 382, 512, 1333, 295, 9661, 11, 6937, 563, 9661, 11, 689, 264, 1412, 2793, 366, 294, 264, 45614, 11, 293, 1203, 1646, 412, 2744, 11, 264, 47138, 295, 1412, 307, 382, 2946, 2281, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.10841527478448276, "compression_ratio": 1.5389221556886228, "no_speech_prob": 0.000131231703562662}, {"id": 15, "seek": 13800, "start": 138.0, "end": 148.0, "text": " So inference in this context proceeds by basically finding a Y or a set of Ys that minimize f of X, Y for a given X.", "tokens": [50364, 407, 38253, 294, 341, 4319, 32280, 538, 1936, 5006, 257, 398, 420, 257, 992, 295, 398, 82, 300, 17522, 283, 295, 1783, 11, 398, 337, 257, 2212, 1783, 13, 50864, 50864, 407, 341, 307, 406, 2539, 1939, 13, 15205, 14689, 294, 25945, 283, 11, 457, 321, 434, 445, 1417, 466, 38253, 11, 370, 309, 311, 588, 1021, 281, 312, 1075, 281, 652, 264, 2649, 1296, 264, 38253, 1399, 295, 46608, 264, 2281, 2445, 281, 915, 398, 11, 293, 550, 264, 2539, 1399, 11, 597, 307, 46608, 257, 4470, 2445, 11, 406, 264, 2281, 2445, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10002124670780066, "compression_ratio": 1.7609561752988048, "no_speech_prob": 3.2187512260861695e-05}, {"id": 16, "seek": 13800, "start": 148.0, "end": 166.0, "text": " So this is not learning yet. Learning consists in shaping f, but we're just talking about inference, so it's very important to be able to make the difference between the inference process of minimizing the energy function to find Y, and then the learning process, which is minimizing a loss function, not the energy function,", "tokens": [50364, 407, 38253, 294, 341, 4319, 32280, 538, 1936, 5006, 257, 398, 420, 257, 992, 295, 398, 82, 300, 17522, 283, 295, 1783, 11, 398, 337, 257, 2212, 1783, 13, 50864, 50864, 407, 341, 307, 406, 2539, 1939, 13, 15205, 14689, 294, 25945, 283, 11, 457, 321, 434, 445, 1417, 466, 38253, 11, 370, 309, 311, 588, 1021, 281, 312, 1075, 281, 652, 264, 2649, 1296, 264, 38253, 1399, 295, 46608, 264, 2281, 2445, 281, 915, 398, 11, 293, 550, 264, 2539, 1399, 11, 597, 307, 46608, 257, 4470, 2445, 11, 406, 264, 2281, 2445, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10002124670780066, "compression_ratio": 1.7609561752988048, "no_speech_prob": 3.2187512260861695e-05}, {"id": 17, "seek": 16600, "start": 166.0, "end": 172.0, "text": " with respect to the parameters of the energy function. Okay, those are two different things.", "tokens": [50364, 365, 3104, 281, 264, 9834, 295, 264, 2281, 2445, 13, 1033, 11, 729, 366, 732, 819, 721, 13, 50664, 50664, 400, 294, 264, 1389, 295, 264, 47916, 1389, 11, 291, 500, 380, 362, 364, 1783, 11, 293, 370, 291, 434, 787, 23384, 264, 16917, 36606, 1296, 398, 82, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.10695335999974664, "compression_ratio": 1.4076433121019107, "no_speech_prob": 0.00015341791731771082}, {"id": 18, "seek": 16600, "start": 172.0, "end": 185.0, "text": " And in the case of the unconditional case, you don't have an X, and so you're only capturing the mutual dependencies between Ys.", "tokens": [50364, 365, 3104, 281, 264, 9834, 295, 264, 2281, 2445, 13, 1033, 11, 729, 366, 732, 819, 721, 13, 50664, 50664, 400, 294, 264, 1389, 295, 264, 47916, 1389, 11, 291, 500, 380, 362, 364, 1783, 11, 293, 370, 291, 434, 787, 23384, 264, 16917, 36606, 1296, 398, 82, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.10695335999974664, "compression_ratio": 1.4076433121019107, "no_speech_prob": 0.00015341791731771082}, {"id": 19, "seek": 18500, "start": 185.0, "end": 200.0, "text": " We talked about latent variable models, and the reason why we talked about latent variable models is that it's a particular way of representing, of building the architecture of the energy function in such a way that it can have multiple Y for a given X.", "tokens": [50364, 492, 2825, 466, 48994, 7006, 5245, 11, 293, 264, 1778, 983, 321, 2825, 466, 48994, 7006, 5245, 307, 300, 309, 311, 257, 1729, 636, 295, 13460, 11, 295, 2390, 264, 9482, 295, 264, 2281, 2445, 294, 1270, 257, 636, 300, 309, 393, 362, 3866, 398, 337, 257, 2212, 1783, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.05699678703590676, "compression_ratio": 1.6012658227848102, "no_speech_prob": 7.246825407491997e-05}, {"id": 20, "seek": 20000, "start": 200.0, "end": 217.0, "text": " So essentially, a latent variable is an extra variable Z that nobody gives you the value of, but the first thing you do when you see a Z is that you minimize your energy function with respect to that Z, and that gives you now an energy function that does not depend on Z anymore.", "tokens": [50364, 407, 4476, 11, 257, 48994, 7006, 307, 364, 2857, 7006, 1176, 300, 5079, 2709, 291, 264, 2158, 295, 11, 457, 264, 700, 551, 291, 360, 562, 291, 536, 257, 1176, 307, 300, 291, 17522, 428, 2281, 2445, 365, 3104, 281, 300, 1176, 11, 293, 300, 2709, 291, 586, 364, 2281, 2445, 300, 775, 406, 5672, 322, 1176, 3602, 13, 51214, 51214, 1610, 498, 291, 528, 281, 360, 38253, 365, 257, 2316, 365, 48994, 7006, 11, 286, 976, 291, 364, 1783, 11, 293, 291, 915, 264, 6562, 295, 398, 293, 1176, 300, 4464, 5660, 264, 2281, 11, 293, 550, 291, 976, 385, 398, 13, 663, 311, 264, 38253, 1399, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06505048380488843, "compression_ratio": 1.872093023255814, "no_speech_prob": 7.24999190424569e-05}, {"id": 21, "seek": 20000, "start": 217.0, "end": 229.0, "text": " Or if you want to do inference with a model with latent variable, I give you an X, and you find the combination of Y and Z that minimizes the energy, and then you give me Y. That's the inference process.", "tokens": [50364, 407, 4476, 11, 257, 48994, 7006, 307, 364, 2857, 7006, 1176, 300, 5079, 2709, 291, 264, 2158, 295, 11, 457, 264, 700, 551, 291, 360, 562, 291, 536, 257, 1176, 307, 300, 291, 17522, 428, 2281, 2445, 365, 3104, 281, 300, 1176, 11, 293, 300, 2709, 291, 586, 364, 2281, 2445, 300, 775, 406, 5672, 322, 1176, 3602, 13, 51214, 51214, 1610, 498, 291, 528, 281, 360, 38253, 365, 257, 2316, 365, 48994, 7006, 11, 286, 976, 291, 364, 1783, 11, 293, 291, 915, 264, 6562, 295, 398, 293, 1176, 300, 4464, 5660, 264, 2281, 11, 293, 550, 291, 976, 385, 398, 13, 663, 311, 264, 38253, 1399, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06505048380488843, "compression_ratio": 1.872093023255814, "no_speech_prob": 7.24999190424569e-05}, {"id": 22, "seek": 22900, "start": 229.0, "end": 244.0, "text": " There's two ways to do inference with respect to a variable that you don't observe. One is to just minimize over it, as I just indicated, and the other one is to marginalize over it if you're a probabilist, but even in other cases.", "tokens": [50364, 821, 311, 732, 2098, 281, 360, 38253, 365, 3104, 281, 257, 7006, 300, 291, 500, 380, 11441, 13, 1485, 307, 281, 445, 17522, 670, 309, 11, 382, 286, 445, 16176, 11, 293, 264, 661, 472, 307, 281, 16885, 1125, 670, 309, 498, 291, 434, 257, 31959, 468, 11, 457, 754, 294, 661, 3331, 13, 51114, 51114, 400, 456, 311, 257, 2199, 8513, 281, 733, 295, 352, 490, 472, 281, 264, 661, 11, 597, 307, 1936, 257, 3565, 2408, 21510, 670, 439, 1944, 4190, 295, 1176, 13, 400, 341, 815, 312, 560, 1897, 712, 11, 370, 321, 500, 380, 360, 341, 588, 2049, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08469707275105413, "compression_ratio": 1.628787878787879, "no_speech_prob": 7.962850213516504e-05}, {"id": 23, "seek": 22900, "start": 244.0, "end": 258.0, "text": " And there's a simple formula to kind of go from one to the other, which is basically a log sum exponential over all possible values of Z. And this may be intractable, so we don't do this very often.", "tokens": [50364, 821, 311, 732, 2098, 281, 360, 38253, 365, 3104, 281, 257, 7006, 300, 291, 500, 380, 11441, 13, 1485, 307, 281, 445, 17522, 670, 309, 11, 382, 286, 445, 16176, 11, 293, 264, 661, 472, 307, 281, 16885, 1125, 670, 309, 498, 291, 434, 257, 31959, 468, 11, 457, 754, 294, 661, 3331, 13, 51114, 51114, 400, 456, 311, 257, 2199, 8513, 281, 733, 295, 352, 490, 472, 281, 264, 661, 11, 597, 307, 1936, 257, 3565, 2408, 21510, 670, 439, 1944, 4190, 295, 1176, 13, 400, 341, 815, 312, 560, 1897, 712, 11, 370, 321, 500, 380, 360, 341, 588, 2049, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08469707275105413, "compression_ratio": 1.628787878787879, "no_speech_prob": 7.962850213516504e-05}, {"id": 24, "seek": 25800, "start": 258.0, "end": 274.0, "text": " Okay, so training an energy-based model consists in parameterizing the energy function and collecting, of course, a bunch of training samples, a bunch of X and Ys in the conditional case or just a bunch of Ys in the unconditional case.", "tokens": [50364, 1033, 11, 370, 3097, 364, 2281, 12, 6032, 2316, 14689, 294, 13075, 3319, 264, 2281, 2445, 293, 12510, 11, 295, 1164, 11, 257, 3840, 295, 3097, 10938, 11, 257, 3840, 295, 1783, 293, 398, 82, 294, 264, 27708, 1389, 420, 445, 257, 3840, 295, 398, 82, 294, 264, 47916, 1389, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.11520451632412997, "compression_ratio": 1.6433566433566433, "no_speech_prob": 8.479849202558398e-05}, {"id": 25, "seek": 27400, "start": 274.0, "end": 301.0, "text": " And then it consists in shaping the energy function so that you give low energy to good combinations of X and Ys and higher energy to bad combinations of X and Y. So for a given observed X, you try to make F of Y for the corresponding Y that corresponds to X as low as possible, but then you also need to make the energy F of X and Y larger for all other values of Y, all other possible values of Y.", "tokens": [50364, 400, 550, 309, 14689, 294, 25945, 264, 2281, 2445, 370, 300, 291, 976, 2295, 2281, 281, 665, 21267, 295, 1783, 293, 398, 82, 293, 2946, 2281, 281, 1578, 21267, 295, 1783, 293, 398, 13, 407, 337, 257, 2212, 13095, 1783, 11, 291, 853, 281, 652, 479, 295, 398, 337, 264, 11760, 398, 300, 23249, 281, 1783, 382, 2295, 382, 1944, 11, 457, 550, 291, 611, 643, 281, 652, 264, 2281, 479, 295, 1783, 293, 398, 4833, 337, 439, 661, 4190, 295, 398, 11, 439, 661, 1944, 4190, 295, 398, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05672184964443775, "compression_ratio": 1.8732394366197183, "no_speech_prob": 5.475242141983472e-05}, {"id": 26, "seek": 30100, "start": 301.0, "end": 315.0, "text": " And it's probably a good idea to keep this energy function smooth if you are in a continuous space, if Y is a continuous variable, because that will make inference easier. Subsequently, you'll be able to use gradient descent-based methods to do inference on maybe other methods.", "tokens": [50364, 400, 309, 311, 1391, 257, 665, 1558, 281, 1066, 341, 2281, 2445, 5508, 498, 291, 366, 294, 257, 10957, 1901, 11, 498, 398, 307, 257, 10957, 7006, 11, 570, 300, 486, 652, 38253, 3571, 13, 8511, 46027, 11, 291, 603, 312, 1075, 281, 764, 16235, 23475, 12, 6032, 7150, 281, 360, 38253, 322, 1310, 661, 7150, 13, 51064, 51064, 407, 456, 311, 732, 5359, 295, 2539, 14642, 11, 382, 321, 2825, 466, 1036, 565, 13, 51364, 51364, 440, 700, 1508, 307, 8712, 488, 7150, 11, 597, 14689, 294, 1936, 7380, 760, 322, 264, 2281, 295, 3097, 10938, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09112781636855181, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.0001272909576073289}, {"id": 27, "seek": 30100, "start": 315.0, "end": 321.0, "text": " So there's two classes of learning algorithms, as we talked about last time.", "tokens": [50364, 400, 309, 311, 1391, 257, 665, 1558, 281, 1066, 341, 2281, 2445, 5508, 498, 291, 366, 294, 257, 10957, 1901, 11, 498, 398, 307, 257, 10957, 7006, 11, 570, 300, 486, 652, 38253, 3571, 13, 8511, 46027, 11, 291, 603, 312, 1075, 281, 764, 16235, 23475, 12, 6032, 7150, 281, 360, 38253, 322, 1310, 661, 7150, 13, 51064, 51064, 407, 456, 311, 732, 5359, 295, 2539, 14642, 11, 382, 321, 2825, 466, 1036, 565, 13, 51364, 51364, 440, 700, 1508, 307, 8712, 488, 7150, 11, 597, 14689, 294, 1936, 7380, 760, 322, 264, 2281, 295, 3097, 10938, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09112781636855181, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.0001272909576073289}, {"id": 28, "seek": 30100, "start": 321.0, "end": 329.0, "text": " The first class is contrastive methods, which consists in basically pushing down on the energy of training samples.", "tokens": [50364, 400, 309, 311, 1391, 257, 665, 1558, 281, 1066, 341, 2281, 2445, 5508, 498, 291, 366, 294, 257, 10957, 1901, 11, 498, 398, 307, 257, 10957, 7006, 11, 570, 300, 486, 652, 38253, 3571, 13, 8511, 46027, 11, 291, 603, 312, 1075, 281, 764, 16235, 23475, 12, 6032, 7150, 281, 360, 38253, 322, 1310, 661, 7150, 13, 51064, 51064, 407, 456, 311, 732, 5359, 295, 2539, 14642, 11, 382, 321, 2825, 466, 1036, 565, 13, 51364, 51364, 440, 700, 1508, 307, 8712, 488, 7150, 11, 597, 14689, 294, 1936, 7380, 760, 322, 264, 2281, 295, 3097, 10938, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09112781636855181, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.0001272909576073289}, {"id": 29, "seek": 32900, "start": 329.0, "end": 339.0, "text": " So you get a training sample XI, YI, you plug it into the energy function, and then you tune the parameters of the energy function so that that energy goes down.", "tokens": [50364, 407, 291, 483, 257, 3097, 6889, 1783, 40, 11, 398, 40, 11, 291, 5452, 309, 666, 264, 2281, 2445, 11, 293, 550, 291, 10864, 264, 9834, 295, 264, 2281, 2445, 370, 300, 300, 2281, 1709, 760, 13, 50864, 50864, 400, 291, 393, 360, 341, 365, 11, 291, 458, 11, 365, 646, 69, 1513, 498, 428, 2281, 2445, 307, 512, 1333, 295, 18161, 2533, 365, 661, 721, 294, 309, 13, 51364, 51364, 1018, 938, 382, 309, 311, 257, 819, 9364, 2445, 11, 291, 393, 360, 300, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1151535881890191, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.0001487111730966717}, {"id": 30, "seek": 32900, "start": 339.0, "end": 349.0, "text": " And you can do this with, you know, with backfrop if your energy function is some sort of neural net with other things in it.", "tokens": [50364, 407, 291, 483, 257, 3097, 6889, 1783, 40, 11, 398, 40, 11, 291, 5452, 309, 666, 264, 2281, 2445, 11, 293, 550, 291, 10864, 264, 9834, 295, 264, 2281, 2445, 370, 300, 300, 2281, 1709, 760, 13, 50864, 50864, 400, 291, 393, 360, 341, 365, 11, 291, 458, 11, 365, 646, 69, 1513, 498, 428, 2281, 2445, 307, 512, 1333, 295, 18161, 2533, 365, 661, 721, 294, 309, 13, 51364, 51364, 1018, 938, 382, 309, 311, 257, 819, 9364, 2445, 11, 291, 393, 360, 300, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1151535881890191, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.0001487111730966717}, {"id": 31, "seek": 32900, "start": 349.0, "end": 354.0, "text": " As long as it's a differentiable function, you can do that.", "tokens": [50364, 407, 291, 483, 257, 3097, 6889, 1783, 40, 11, 398, 40, 11, 291, 5452, 309, 666, 264, 2281, 2445, 11, 293, 550, 291, 10864, 264, 9834, 295, 264, 2281, 2445, 370, 300, 300, 2281, 1709, 760, 13, 50864, 50864, 400, 291, 393, 360, 341, 365, 11, 291, 458, 11, 365, 646, 69, 1513, 498, 428, 2281, 2445, 307, 512, 1333, 295, 18161, 2533, 365, 661, 721, 294, 309, 13, 51364, 51364, 1018, 938, 382, 309, 311, 257, 819, 9364, 2445, 11, 291, 393, 360, 300, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1151535881890191, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.0001487111730966717}, {"id": 32, "seek": 35400, "start": 354.0, "end": 365.0, "text": " But then what you have to do as well is pick other points that are outside the manifold of data and then push their energy up so that the energy gets takes the right shape.", "tokens": [50364, 583, 550, 437, 291, 362, 281, 360, 382, 731, 307, 1888, 661, 2793, 300, 366, 2380, 264, 47138, 295, 1412, 293, 550, 2944, 641, 2281, 493, 370, 300, 264, 2281, 2170, 2516, 264, 558, 3909, 13, 50914, 50914, 1033, 11, 370, 729, 366, 8712, 488, 7150, 13, 400, 550, 456, 311, 26621, 7150, 293, 264, 26621, 7150, 1936, 4603, 294, 2390, 479, 295, 1783, 11, 398, 294, 1270, 257, 636, 300, 264, 5523, 295, 1901, 300, 393, 747, 2295, 2281, 307, 5567, 11, 4317, 4464, 1602, 294, 512, 636, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11197329582052028, "compression_ratio": 1.7398373983739837, "no_speech_prob": 4.3988486140733585e-05}, {"id": 33, "seek": 35400, "start": 365.0, "end": 383.0, "text": " Okay, so those are contrastive methods. And then there's architectural methods and the architectural methods basically consist in building F of X, Y in such a way that the volume of space that can take low energy is limited, perhaps minimized in some way.", "tokens": [50364, 583, 550, 437, 291, 362, 281, 360, 382, 731, 307, 1888, 661, 2793, 300, 366, 2380, 264, 47138, 295, 1412, 293, 550, 2944, 641, 2281, 493, 370, 300, 264, 2281, 2170, 2516, 264, 558, 3909, 13, 50914, 50914, 1033, 11, 370, 729, 366, 8712, 488, 7150, 13, 400, 550, 456, 311, 26621, 7150, 293, 264, 26621, 7150, 1936, 4603, 294, 2390, 479, 295, 1783, 11, 398, 294, 1270, 257, 636, 300, 264, 5523, 295, 1901, 300, 393, 747, 2295, 2281, 307, 5567, 11, 4317, 4464, 1602, 294, 512, 636, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11197329582052028, "compression_ratio": 1.7398373983739837, "no_speech_prob": 4.3988486140733585e-05}, {"id": 34, "seek": 38300, "start": 383.0, "end": 396.0, "text": " And so if you push down on the energy of certain points, automatically the rest will be up because we go up, because the volume of stuff that can take low energy is limited.", "tokens": [50364, 400, 370, 498, 291, 2944, 760, 322, 264, 2281, 295, 1629, 2793, 11, 6772, 264, 1472, 486, 312, 493, 570, 321, 352, 493, 11, 570, 264, 5523, 295, 1507, 300, 393, 747, 2295, 2281, 307, 5567, 13, 51014, 51014, 400, 286, 600, 1027, 257, 1329, 510, 13, 407, 341, 307, 364, 1021, 4137, 11, 597, 291, 1866, 1036, 565, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09295027703046799, "compression_ratio": 1.4463276836158192, "no_speech_prob": 4.9806949391495436e-05}, {"id": 35, "seek": 38300, "start": 396.0, "end": 405.0, "text": " And I've made a list here. So this is an important slide, which you saw last time.", "tokens": [50364, 400, 370, 498, 291, 2944, 760, 322, 264, 2281, 295, 1629, 2793, 11, 6772, 264, 1472, 486, 312, 493, 570, 321, 352, 493, 11, 570, 264, 5523, 295, 1507, 300, 393, 747, 2295, 2281, 307, 5567, 13, 51014, 51014, 400, 286, 600, 1027, 257, 1329, 510, 13, 407, 341, 307, 364, 1021, 4137, 11, 597, 291, 1866, 1036, 565, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09295027703046799, "compression_ratio": 1.4463276836158192, "no_speech_prob": 4.9806949391495436e-05}, {"id": 36, "seek": 40500, "start": 405.0, "end": 416.0, "text": " And there is a list of various methods that you may have heard of, some of which are contrastive, some of which are architectural. I must say that those two classes of methods are not necessarily compatible with each other.", "tokens": [50364, 400, 456, 307, 257, 1329, 295, 3683, 7150, 300, 291, 815, 362, 2198, 295, 11, 512, 295, 597, 366, 8712, 488, 11, 512, 295, 597, 366, 26621, 13, 286, 1633, 584, 300, 729, 732, 5359, 295, 7150, 366, 406, 4725, 18218, 365, 1184, 661, 13, 50914, 50914, 509, 393, 588, 731, 764, 257, 6562, 295, 264, 732, 13, 583, 881, 7150, 787, 764, 472, 13, 407, 721, 411, 6674, 22119, 11, 498, 428, 33783, 4603, 294, 7380, 760, 322, 264, 2281, 295, 1412, 2793, 293, 550, 7380, 493, 5315, 1646, 337, 633, 661, 2158, 295, 398, 294, 16068, 281, 577, 2295, 264, 2281, 307, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07718677695738066, "compression_ratio": 1.7224080267558528, "no_speech_prob": 4.1974886698881164e-05}, {"id": 37, "seek": 40500, "start": 416.0, "end": 434.0, "text": " You can very well use a combination of the two. But most methods only use one. So things like maximum likelihood, if your probabilities consist in pushing down on the energy of data points and then pushing up everywhere else for every other value of Y in proportion to how low the energy is.", "tokens": [50364, 400, 456, 307, 257, 1329, 295, 3683, 7150, 300, 291, 815, 362, 2198, 295, 11, 512, 295, 597, 366, 8712, 488, 11, 512, 295, 597, 366, 26621, 13, 286, 1633, 584, 300, 729, 732, 5359, 295, 7150, 366, 406, 4725, 18218, 365, 1184, 661, 13, 50914, 50914, 509, 393, 588, 731, 764, 257, 6562, 295, 264, 732, 13, 583, 881, 7150, 787, 764, 472, 13, 407, 721, 411, 6674, 22119, 11, 498, 428, 33783, 4603, 294, 7380, 760, 322, 264, 2281, 295, 1412, 2793, 293, 550, 7380, 493, 5315, 1646, 337, 633, 661, 2158, 295, 398, 294, 16068, 281, 577, 2295, 264, 2281, 307, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07718677695738066, "compression_ratio": 1.7224080267558528, "no_speech_prob": 4.1974886698881164e-05}, {"id": 38, "seek": 43400, "start": 434.0, "end": 442.0, "text": " And then you push up harder if the energy is lower so that in the end you get kind of the right shape.", "tokens": [50364, 400, 550, 291, 2944, 493, 6081, 498, 264, 2281, 307, 3126, 370, 300, 294, 264, 917, 291, 483, 733, 295, 264, 558, 3909, 13, 50764, 50764, 29076, 449, 22119, 11, 9348, 379, 11, 787, 12310, 466, 7300, 295, 25737, 11, 1177, 380, 1127, 466, 8236, 4190, 295, 25737, 11, 597, 307, 364, 1021, 935, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.11780208652302371, "compression_ratio": 1.5178571428571428, "no_speech_prob": 5.8228240959579125e-05}, {"id": 39, "seek": 43400, "start": 442.0, "end": 452.0, "text": " Maximum likelihood, incidentally, only cares about differences of energies, doesn't care about absolute values of energies, which is an important point.", "tokens": [50364, 400, 550, 291, 2944, 493, 6081, 498, 264, 2281, 307, 3126, 370, 300, 294, 264, 917, 291, 483, 733, 295, 264, 558, 3909, 13, 50764, 50764, 29076, 449, 22119, 11, 9348, 379, 11, 787, 12310, 466, 7300, 295, 25737, 11, 1177, 380, 1127, 466, 8236, 4190, 295, 25737, 11, 597, 307, 364, 1021, 935, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.11780208652302371, "compression_ratio": 1.5178571428571428, "no_speech_prob": 5.8228240959579125e-05}, {"id": 40, "seek": 45200, "start": 452.0, "end": 475.0, "text": " There are other methods like contrastive divergence, metric learning, ratio matching, noise contrast estimation, minimum probability flow, things like that, which, and generative adversarial networks that also are based on the idea of pushing up on the energy of data points outside the data manifold.", "tokens": [50364, 821, 366, 661, 7150, 411, 8712, 488, 47387, 11, 20678, 2539, 11, 8509, 14324, 11, 5658, 8712, 35701, 11, 7285, 8482, 3095, 11, 721, 411, 300, 11, 597, 11, 293, 1337, 1166, 17641, 44745, 9590, 300, 611, 366, 2361, 322, 264, 1558, 295, 7380, 493, 322, 264, 2281, 295, 1412, 2793, 2380, 264, 1412, 47138, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11395309766133627, "compression_ratio": 1.5925925925925926, "no_speech_prob": 1.616533518244978e-05}, {"id": 41, "seek": 47500, "start": 475.0, "end": 488.0, "text": " And then there are similar methods, denosing autoencoders that we will talk about in just a minute. And as we saw last time, they've been extremely successful in the context of natural language processing.", "tokens": [50364, 400, 550, 456, 366, 2531, 7150, 11, 1441, 6110, 8399, 22660, 378, 433, 300, 321, 486, 751, 466, 294, 445, 257, 3456, 13, 400, 382, 321, 1866, 1036, 565, 11, 436, 600, 668, 4664, 4406, 294, 264, 4319, 295, 3303, 2856, 9007, 13, 51014, 51014, 27059, 411, 363, 31479, 11, 337, 1365, 11, 1936, 366, 1441, 6110, 8399, 22660, 378, 433, 295, 257, 1729, 733, 13, 51314, 51314, 400, 550, 456, 366, 26621, 7150, 13, 400, 1036, 565, 321, 2825, 257, 707, 857, 466, 6465, 32, 293, 591, 12, 1398, 599, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11687929431597392, "compression_ratio": 1.620408163265306, "no_speech_prob": 2.7103586035082117e-05}, {"id": 42, "seek": 47500, "start": 488.0, "end": 494.0, "text": " Systems like BERT, for example, basically are denosing autoencoders of a particular kind.", "tokens": [50364, 400, 550, 456, 366, 2531, 7150, 11, 1441, 6110, 8399, 22660, 378, 433, 300, 321, 486, 751, 466, 294, 445, 257, 3456, 13, 400, 382, 321, 1866, 1036, 565, 11, 436, 600, 668, 4664, 4406, 294, 264, 4319, 295, 3303, 2856, 9007, 13, 51014, 51014, 27059, 411, 363, 31479, 11, 337, 1365, 11, 1936, 366, 1441, 6110, 8399, 22660, 378, 433, 295, 257, 1729, 733, 13, 51314, 51314, 400, 550, 456, 366, 26621, 7150, 13, 400, 1036, 565, 321, 2825, 257, 707, 857, 466, 6465, 32, 293, 591, 12, 1398, 599, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11687929431597392, "compression_ratio": 1.620408163265306, "no_speech_prob": 2.7103586035082117e-05}, {"id": 43, "seek": 47500, "start": 494.0, "end": 500.0, "text": " And then there are architectural methods. And last time we talked a little bit about PCA and K-means.", "tokens": [50364, 400, 550, 456, 366, 2531, 7150, 11, 1441, 6110, 8399, 22660, 378, 433, 300, 321, 486, 751, 466, 294, 445, 257, 3456, 13, 400, 382, 321, 1866, 1036, 565, 11, 436, 600, 668, 4664, 4406, 294, 264, 4319, 295, 3303, 2856, 9007, 13, 51014, 51014, 27059, 411, 363, 31479, 11, 337, 1365, 11, 1936, 366, 1441, 6110, 8399, 22660, 378, 433, 295, 257, 1729, 733, 13, 51314, 51314, 400, 550, 456, 366, 26621, 7150, 13, 400, 1036, 565, 321, 2825, 257, 707, 857, 466, 6465, 32, 293, 591, 12, 1398, 599, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11687929431597392, "compression_ratio": 1.620408163265306, "no_speech_prob": 2.7103586035082117e-05}, {"id": 44, "seek": 50000, "start": 500.0, "end": 509.0, "text": " So we're going to talk about a few more today, particularly sparse coding and something called Lista.", "tokens": [50364, 407, 321, 434, 516, 281, 751, 466, 257, 1326, 544, 965, 11, 4098, 637, 11668, 17720, 293, 746, 1219, 441, 5236, 13, 50814, 50814, 400, 286, 478, 516, 281, 751, 466, 264, 8877, 2306, 13, 51014, 51014, 407, 341, 307, 257, 22355, 1299, 797, 295, 746, 300, 321, 2825, 466, 1036, 1243, 11, 597, 307, 257, 588, 2199, 48994, 7006, 2316, 337, 2693, 12879, 24420, 2539, 11, 591, 12, 1398, 599, 11, 597, 286, 478, 988, 291, 600, 439, 2198, 466, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09144332242566486, "compression_ratio": 1.588785046728972, "no_speech_prob": 5.6370867241639644e-05}, {"id": 45, "seek": 50000, "start": 509.0, "end": 513.0, "text": " And I'm going to talk about the remaining ones.", "tokens": [50364, 407, 321, 434, 516, 281, 751, 466, 257, 1326, 544, 965, 11, 4098, 637, 11668, 17720, 293, 746, 1219, 441, 5236, 13, 50814, 50814, 400, 286, 478, 516, 281, 751, 466, 264, 8877, 2306, 13, 51014, 51014, 407, 341, 307, 257, 22355, 1299, 797, 295, 746, 300, 321, 2825, 466, 1036, 1243, 11, 597, 307, 257, 588, 2199, 48994, 7006, 2316, 337, 2693, 12879, 24420, 2539, 11, 591, 12, 1398, 599, 11, 597, 286, 478, 988, 291, 600, 439, 2198, 466, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09144332242566486, "compression_ratio": 1.588785046728972, "no_speech_prob": 5.6370867241639644e-05}, {"id": 46, "seek": 50000, "start": 513.0, "end": 526.0, "text": " So this is a rehash again of something that we talked about last week, which is a very simple latent variable model for unsupervised learning, K-means, which I'm sure you've all heard about.", "tokens": [50364, 407, 321, 434, 516, 281, 751, 466, 257, 1326, 544, 965, 11, 4098, 637, 11668, 17720, 293, 746, 1219, 441, 5236, 13, 50814, 50814, 400, 286, 478, 516, 281, 751, 466, 264, 8877, 2306, 13, 51014, 51014, 407, 341, 307, 257, 22355, 1299, 797, 295, 746, 300, 321, 2825, 466, 1036, 1243, 11, 597, 307, 257, 588, 2199, 48994, 7006, 2316, 337, 2693, 12879, 24420, 2539, 11, 591, 12, 1398, 599, 11, 597, 286, 478, 988, 291, 600, 439, 2198, 466, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09144332242566486, "compression_ratio": 1.588785046728972, "no_speech_prob": 5.6370867241639644e-05}, {"id": 47, "seek": 52600, "start": 526.0, "end": 536.0, "text": " The energy function is simply the square reconstruction error between the data vector and the product of a prototype matrix times latent variable vector.", "tokens": [50364, 440, 2281, 2445, 307, 2935, 264, 3732, 31565, 6713, 1296, 264, 1412, 8062, 293, 264, 1674, 295, 257, 19475, 8141, 1413, 48994, 7006, 8062, 13, 50864, 50864, 400, 300, 48994, 7006, 8062, 307, 38901, 281, 312, 257, 472, 12, 12194, 8062, 13, 51064, 51064, 407, 294, 661, 2283, 11, 309, 3048, 82, 472, 295, 264, 13766, 295, 343, 562, 291, 12972, 343, 538, 309, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.05647421574247056, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0002605671470519155}, {"id": 48, "seek": 52600, "start": 536.0, "end": 540.0, "text": " And that latent variable vector is constrained to be a one-hot vector.", "tokens": [50364, 440, 2281, 2445, 307, 2935, 264, 3732, 31565, 6713, 1296, 264, 1412, 8062, 293, 264, 1674, 295, 257, 19475, 8141, 1413, 48994, 7006, 8062, 13, 50864, 50864, 400, 300, 48994, 7006, 8062, 307, 38901, 281, 312, 257, 472, 12, 12194, 8062, 13, 51064, 51064, 407, 294, 661, 2283, 11, 309, 3048, 82, 472, 295, 264, 13766, 295, 343, 562, 291, 12972, 343, 538, 309, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.05647421574247056, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0002605671470519155}, {"id": 49, "seek": 52600, "start": 540.0, "end": 547.0, "text": " So in other words, it selects one of the columns of W when you multiply W by it.", "tokens": [50364, 440, 2281, 2445, 307, 2935, 264, 3732, 31565, 6713, 1296, 264, 1412, 8062, 293, 264, 1674, 295, 257, 19475, 8141, 1413, 48994, 7006, 8062, 13, 50864, 50864, 400, 300, 48994, 7006, 8062, 307, 38901, 281, 312, 257, 472, 12, 12194, 8062, 13, 51064, 51064, 407, 294, 661, 2283, 11, 309, 3048, 82, 472, 295, 264, 13766, 295, 343, 562, 291, 12972, 343, 538, 309, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.05647421574247056, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0002605671470519155}, {"id": 50, "seek": 54700, "start": 547.0, "end": 558.0, "text": " And so what you get in the end is the square distance between the data vector and the column of W that is closest to it.", "tokens": [50364, 400, 370, 437, 291, 483, 294, 264, 917, 307, 264, 3732, 4560, 1296, 264, 1412, 8062, 293, 264, 7738, 295, 343, 300, 307, 13699, 281, 309, 13, 50914, 50914, 3443, 291, 360, 264, 4464, 2144, 365, 3104, 281, 1176, 11, 597, 1355, 1237, 337, 597, 7738, 295, 343, 307, 13699, 281, 398, 13, 51164, 51164, 407, 300, 311, 264, 2281, 2445, 13, 663, 311, 264, 38253, 9284, 1237, 337, 264, 13699, 19475, 13, 51414, 51414, 400, 264, 2281, 2445, 11, 295, 1164, 11, 307, 4018, 8660, 456, 307, 257, 19475, 293, 13156, 10787, 4481, 984, 382, 291, 1286, 1314, 490, 264, 19475, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05424599377614147, "compression_ratio": 1.7821011673151752, "no_speech_prob": 3.321033364045434e-05}, {"id": 51, "seek": 54700, "start": 558.0, "end": 563.0, "text": " Once you do the minimization with respect to Z, which means looking for which column of W is closest to Y.", "tokens": [50364, 400, 370, 437, 291, 483, 294, 264, 917, 307, 264, 3732, 4560, 1296, 264, 1412, 8062, 293, 264, 7738, 295, 343, 300, 307, 13699, 281, 309, 13, 50914, 50914, 3443, 291, 360, 264, 4464, 2144, 365, 3104, 281, 1176, 11, 597, 1355, 1237, 337, 597, 7738, 295, 343, 307, 13699, 281, 398, 13, 51164, 51164, 407, 300, 311, 264, 2281, 2445, 13, 663, 311, 264, 38253, 9284, 1237, 337, 264, 13699, 19475, 13, 51414, 51414, 400, 264, 2281, 2445, 11, 295, 1164, 11, 307, 4018, 8660, 456, 307, 257, 19475, 293, 13156, 10787, 4481, 984, 382, 291, 1286, 1314, 490, 264, 19475, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05424599377614147, "compression_ratio": 1.7821011673151752, "no_speech_prob": 3.321033364045434e-05}, {"id": 52, "seek": 54700, "start": 563.0, "end": 568.0, "text": " So that's the energy function. That's the inference algorithm looking for the closest prototype.", "tokens": [50364, 400, 370, 437, 291, 483, 294, 264, 917, 307, 264, 3732, 4560, 1296, 264, 1412, 8062, 293, 264, 7738, 295, 343, 300, 307, 13699, 281, 309, 13, 50914, 50914, 3443, 291, 360, 264, 4464, 2144, 365, 3104, 281, 1176, 11, 597, 1355, 1237, 337, 597, 7738, 295, 343, 307, 13699, 281, 398, 13, 51164, 51164, 407, 300, 311, 264, 2281, 2445, 13, 663, 311, 264, 38253, 9284, 1237, 337, 264, 13699, 19475, 13, 51414, 51414, 400, 264, 2281, 2445, 11, 295, 1164, 11, 307, 4018, 8660, 456, 307, 257, 19475, 293, 13156, 10787, 4481, 984, 382, 291, 1286, 1314, 490, 264, 19475, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05424599377614147, "compression_ratio": 1.7821011673151752, "no_speech_prob": 3.321033364045434e-05}, {"id": 53, "seek": 54700, "start": 568.0, "end": 576.0, "text": " And the energy function, of course, is zero wherever there is a prototype and grows quadratically as you move away from the prototype", "tokens": [50364, 400, 370, 437, 291, 483, 294, 264, 917, 307, 264, 3732, 4560, 1296, 264, 1412, 8062, 293, 264, 7738, 295, 343, 300, 307, 13699, 281, 309, 13, 50914, 50914, 3443, 291, 360, 264, 4464, 2144, 365, 3104, 281, 1176, 11, 597, 1355, 1237, 337, 597, 7738, 295, 343, 307, 13699, 281, 398, 13, 51164, 51164, 407, 300, 311, 264, 2281, 2445, 13, 663, 311, 264, 38253, 9284, 1237, 337, 264, 13699, 19475, 13, 51414, 51414, 400, 264, 2281, 2445, 11, 295, 1164, 11, 307, 4018, 8660, 456, 307, 257, 19475, 293, 13156, 10787, 4481, 984, 382, 291, 1286, 1314, 490, 264, 19475, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05424599377614147, "compression_ratio": 1.7821011673151752, "no_speech_prob": 3.321033364045434e-05}, {"id": 54, "seek": 57600, "start": 576.0, "end": 583.0, "text": " until you get closer to another prototype, in which case the energy again goes down as you get closer to the second prototype.", "tokens": [50364, 1826, 291, 483, 4966, 281, 1071, 19475, 11, 294, 597, 1389, 264, 2281, 797, 1709, 760, 382, 291, 483, 4966, 281, 264, 1150, 19475, 13, 50714, 50714, 407, 498, 291, 3847, 591, 12, 1398, 599, 322, 257, 1412, 992, 689, 264, 3097, 10938, 366, 10833, 538, 8867, 926, 341, 707, 25165, 510, 4898, 412, 264, 2767, 11, 51314, 51314, 365, 591, 2681, 945, 294, 300, 1389, 11, 291, 483, 729, 707, 2877, 3179, 597, 13330, 264, 4464, 64, 295, 264, 2281, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.091385660500362, "compression_ratio": 1.6888888888888889, "no_speech_prob": 2.796483204292599e-05}, {"id": 55, "seek": 57600, "start": 583.0, "end": 595.0, "text": " So if you train K-means on a data set where the training samples are generated by picking around this little spiral here shown at the bottom,", "tokens": [50364, 1826, 291, 483, 4966, 281, 1071, 19475, 11, 294, 597, 1389, 264, 2281, 797, 1709, 760, 382, 291, 483, 4966, 281, 264, 1150, 19475, 13, 50714, 50714, 407, 498, 291, 3847, 591, 12, 1398, 599, 322, 257, 1412, 992, 689, 264, 3097, 10938, 366, 10833, 538, 8867, 926, 341, 707, 25165, 510, 4898, 412, 264, 2767, 11, 51314, 51314, 365, 591, 2681, 945, 294, 300, 1389, 11, 291, 483, 729, 707, 2877, 3179, 597, 13330, 264, 4464, 64, 295, 264, 2281, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.091385660500362, "compression_ratio": 1.6888888888888889, "no_speech_prob": 2.796483204292599e-05}, {"id": 56, "seek": 57600, "start": 595.0, "end": 604.0, "text": " with K equal 20 in that case, you get those little dark areas which indicate the minima of the energy function.", "tokens": [50364, 1826, 291, 483, 4966, 281, 1071, 19475, 11, 294, 597, 1389, 264, 2281, 797, 1709, 760, 382, 291, 483, 4966, 281, 264, 1150, 19475, 13, 50714, 50714, 407, 498, 291, 3847, 591, 12, 1398, 599, 322, 257, 1412, 992, 689, 264, 3097, 10938, 366, 10833, 538, 8867, 926, 341, 707, 25165, 510, 4898, 412, 264, 2767, 11, 51314, 51314, 365, 591, 2681, 945, 294, 300, 1389, 11, 291, 483, 729, 707, 2877, 3179, 597, 13330, 264, 4464, 64, 295, 264, 2281, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.091385660500362, "compression_ratio": 1.6888888888888889, "no_speech_prob": 2.796483204292599e-05}, {"id": 57, "seek": 60400, "start": 604.0, "end": 614.0, "text": " So you get a ridge in the middle where the energy goes down on both sides. It's like a ridge.", "tokens": [50364, 407, 291, 483, 257, 34651, 294, 264, 2808, 689, 264, 2281, 1709, 760, 322, 1293, 4881, 13, 467, 311, 411, 257, 34651, 13, 50864, 50864, 583, 510, 307, 257, 3170, 300, 311, 1813, 588, 3743, 670, 264, 1036, 1326, 2493, 293, 307, 588, 5162, 13, 51314, 51314, 440, 700, 10577, 322, 341, 767, 352, 646, 257, 938, 565, 13, 51464, 51464, 821, 366, 512, 295, 452, 10577, 490, 264, 2440, 4289, 82, 293, 490, 264, 2062, 12, 25743, 82, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15337940624782018, "compression_ratio": 1.5219512195121951, "no_speech_prob": 0.00012720833183266222}, {"id": 58, "seek": 60400, "start": 614.0, "end": 623.0, "text": " But here is a method that's become very popular over the last few months and is very recent.", "tokens": [50364, 407, 291, 483, 257, 34651, 294, 264, 2808, 689, 264, 2281, 1709, 760, 322, 1293, 4881, 13, 467, 311, 411, 257, 34651, 13, 50864, 50864, 583, 510, 307, 257, 3170, 300, 311, 1813, 588, 3743, 670, 264, 1036, 1326, 2493, 293, 307, 588, 5162, 13, 51314, 51314, 440, 700, 10577, 322, 341, 767, 352, 646, 257, 938, 565, 13, 51464, 51464, 821, 366, 512, 295, 452, 10577, 490, 264, 2440, 4289, 82, 293, 490, 264, 2062, 12, 25743, 82, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15337940624782018, "compression_ratio": 1.5219512195121951, "no_speech_prob": 0.00012720833183266222}, {"id": 59, "seek": 60400, "start": 623.0, "end": 626.0, "text": " The first papers on this actually go back a long time.", "tokens": [50364, 407, 291, 483, 257, 34651, 294, 264, 2808, 689, 264, 2281, 1709, 760, 322, 1293, 4881, 13, 467, 311, 411, 257, 34651, 13, 50864, 50864, 583, 510, 307, 257, 3170, 300, 311, 1813, 588, 3743, 670, 264, 1036, 1326, 2493, 293, 307, 588, 5162, 13, 51314, 51314, 440, 700, 10577, 322, 341, 767, 352, 646, 257, 938, 565, 13, 51464, 51464, 821, 366, 512, 295, 452, 10577, 490, 264, 2440, 4289, 82, 293, 490, 264, 2062, 12, 25743, 82, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15337940624782018, "compression_ratio": 1.5219512195121951, "no_speech_prob": 0.00012720833183266222}, {"id": 60, "seek": 60400, "start": 626.0, "end": 631.0, "text": " There are some of my papers from the early 90s and from the mid-2000s.", "tokens": [50364, 407, 291, 483, 257, 34651, 294, 264, 2808, 689, 264, 2281, 1709, 760, 322, 1293, 4881, 13, 467, 311, 411, 257, 34651, 13, 50864, 50864, 583, 510, 307, 257, 3170, 300, 311, 1813, 588, 3743, 670, 264, 1036, 1326, 2493, 293, 307, 588, 5162, 13, 51314, 51314, 440, 700, 10577, 322, 341, 767, 352, 646, 257, 938, 565, 13, 51464, 51464, 821, 366, 512, 295, 452, 10577, 490, 264, 2440, 4289, 82, 293, 490, 264, 2062, 12, 25743, 82, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15337940624782018, "compression_ratio": 1.5219512195121951, "no_speech_prob": 0.00012720833183266222}, {"id": 61, "seek": 63100, "start": 631.0, "end": 634.0, "text": " And they were called Siamese networks or metric learning at the time.", "tokens": [50364, 400, 436, 645, 1219, 318, 2918, 1130, 9590, 420, 20678, 2539, 412, 264, 565, 13, 50514, 50514, 400, 264, 1558, 307, 281, 1322, 257, 1333, 295, 2281, 12, 6032, 2316, 11, 498, 291, 528, 11, 538, 1419, 732, 14341, 295, 264, 912, 3209, 420, 732, 819, 9590, 13, 51064, 51064, 583, 588, 2049, 309, 311, 732, 14341, 295, 264, 912, 3209, 13, 400, 291, 3154, 1783, 281, 264, 700, 3209, 293, 398, 281, 264, 1150, 3209, 13, 51464, 51464, 509, 362, 552, 14722, 512, 4111, 8062, 322, 264, 5598, 11, 389, 293, 389, 5835, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09888628757361209, "compression_ratio": 1.7356828193832599, "no_speech_prob": 5.141064684721641e-05}, {"id": 62, "seek": 63100, "start": 634.0, "end": 645.0, "text": " And the idea is to build a sort of energy-based model, if you want, by having two copies of the same network or two different networks.", "tokens": [50364, 400, 436, 645, 1219, 318, 2918, 1130, 9590, 420, 20678, 2539, 412, 264, 565, 13, 50514, 50514, 400, 264, 1558, 307, 281, 1322, 257, 1333, 295, 2281, 12, 6032, 2316, 11, 498, 291, 528, 11, 538, 1419, 732, 14341, 295, 264, 912, 3209, 420, 732, 819, 9590, 13, 51064, 51064, 583, 588, 2049, 309, 311, 732, 14341, 295, 264, 912, 3209, 13, 400, 291, 3154, 1783, 281, 264, 700, 3209, 293, 398, 281, 264, 1150, 3209, 13, 51464, 51464, 509, 362, 552, 14722, 512, 4111, 8062, 322, 264, 5598, 11, 389, 293, 389, 5835, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09888628757361209, "compression_ratio": 1.7356828193832599, "no_speech_prob": 5.141064684721641e-05}, {"id": 63, "seek": 63100, "start": 645.0, "end": 653.0, "text": " But very often it's two copies of the same network. And you feed X to the first network and Y to the second network.", "tokens": [50364, 400, 436, 645, 1219, 318, 2918, 1130, 9590, 420, 20678, 2539, 412, 264, 565, 13, 50514, 50514, 400, 264, 1558, 307, 281, 1322, 257, 1333, 295, 2281, 12, 6032, 2316, 11, 498, 291, 528, 11, 538, 1419, 732, 14341, 295, 264, 912, 3209, 420, 732, 819, 9590, 13, 51064, 51064, 583, 588, 2049, 309, 311, 732, 14341, 295, 264, 912, 3209, 13, 400, 291, 3154, 1783, 281, 264, 700, 3209, 293, 398, 281, 264, 1150, 3209, 13, 51464, 51464, 509, 362, 552, 14722, 512, 4111, 8062, 322, 264, 5598, 11, 389, 293, 389, 5835, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09888628757361209, "compression_ratio": 1.7356828193832599, "no_speech_prob": 5.141064684721641e-05}, {"id": 64, "seek": 63100, "start": 653.0, "end": 656.0, "text": " You have them compute some feature vector on the output, H and H prime.", "tokens": [50364, 400, 436, 645, 1219, 318, 2918, 1130, 9590, 420, 20678, 2539, 412, 264, 565, 13, 50514, 50514, 400, 264, 1558, 307, 281, 1322, 257, 1333, 295, 2281, 12, 6032, 2316, 11, 498, 291, 528, 11, 538, 1419, 732, 14341, 295, 264, 912, 3209, 420, 732, 819, 9590, 13, 51064, 51064, 583, 588, 2049, 309, 311, 732, 14341, 295, 264, 912, 3209, 13, 400, 291, 3154, 1783, 281, 264, 700, 3209, 293, 398, 281, 264, 1150, 3209, 13, 51464, 51464, 509, 362, 552, 14722, 512, 4111, 8062, 322, 264, 5598, 11, 389, 293, 389, 5835, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09888628757361209, "compression_ratio": 1.7356828193832599, "no_speech_prob": 5.141064684721641e-05}, {"id": 65, "seek": 65600, "start": 656.0, "end": 664.0, "text": " And then you compare those two feature vectors with some method, some way of computing a similarity or dissimilarity between vectors.", "tokens": [50364, 400, 550, 291, 6794, 729, 732, 4111, 18875, 365, 512, 3170, 11, 512, 636, 295, 15866, 257, 32194, 420, 7802, 332, 2202, 507, 1296, 18875, 13, 50764, 50764, 467, 727, 312, 257, 5893, 1674, 11, 309, 727, 312, 257, 23565, 11, 257, 32194, 11, 309, 727, 312, 746, 295, 300, 2010, 13, 51064, 51064, 400, 437, 291, 360, 307, 281, 3847, 264, 1185, 307, 300, 291, 3847, 309, 365, 257, 1412, 935, 1936, 382, 257, 6119, 295, 1783, 293, 398, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08577356338500977, "compression_ratio": 1.71, "no_speech_prob": 7.71691047702916e-05}, {"id": 66, "seek": 65600, "start": 664.0, "end": 670.0, "text": " It could be a dot product, it could be a cosine, a similarity, it could be something of that type.", "tokens": [50364, 400, 550, 291, 6794, 729, 732, 4111, 18875, 365, 512, 3170, 11, 512, 636, 295, 15866, 257, 32194, 420, 7802, 332, 2202, 507, 1296, 18875, 13, 50764, 50764, 467, 727, 312, 257, 5893, 1674, 11, 309, 727, 312, 257, 23565, 11, 257, 32194, 11, 309, 727, 312, 746, 295, 300, 2010, 13, 51064, 51064, 400, 437, 291, 360, 307, 281, 3847, 264, 1185, 307, 300, 291, 3847, 309, 365, 257, 1412, 935, 1936, 382, 257, 6119, 295, 1783, 293, 398, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08577356338500977, "compression_ratio": 1.71, "no_speech_prob": 7.71691047702916e-05}, {"id": 67, "seek": 65600, "start": 670.0, "end": 681.0, "text": " And what you do is to train the system is that you train it with a data point basically as a pair of X and Y.", "tokens": [50364, 400, 550, 291, 6794, 729, 732, 4111, 18875, 365, 512, 3170, 11, 512, 636, 295, 15866, 257, 32194, 420, 7802, 332, 2202, 507, 1296, 18875, 13, 50764, 50764, 467, 727, 312, 257, 5893, 1674, 11, 309, 727, 312, 257, 23565, 11, 257, 32194, 11, 309, 727, 312, 746, 295, 300, 2010, 13, 51064, 51064, 400, 437, 291, 360, 307, 281, 3847, 264, 1185, 307, 300, 291, 3847, 309, 365, 257, 1412, 935, 1936, 382, 257, 6119, 295, 1783, 293, 398, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08577356338500977, "compression_ratio": 1.71, "no_speech_prob": 7.71691047702916e-05}, {"id": 68, "seek": 68100, "start": 681.0, "end": 690.0, "text": " So you indicate the location of the data manifold to the system by basically telling it, here is a sample, tell me is a sample X.", "tokens": [50364, 407, 291, 13330, 264, 4914, 295, 264, 1412, 47138, 281, 264, 1185, 538, 1936, 3585, 309, 11, 510, 307, 257, 6889, 11, 980, 385, 307, 257, 6889, 1783, 13, 50814, 50814, 5303, 385, 1071, 6889, 300, 1936, 575, 264, 912, 2701, 382, 1783, 457, 307, 819, 13, 51164, 51164, 400, 295, 1164, 291, 434, 1128, 516, 281, 1029, 264, 1185, 281, 976, 291, 300, 6889, 13, 51314, 51314, 509, 434, 516, 281, 8460, 729, 10938, 293, 3847, 264, 1185, 365, 309, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0909468462300855, "compression_ratio": 1.7281553398058251, "no_speech_prob": 2.1780157112516463e-05}, {"id": 69, "seek": 68100, "start": 690.0, "end": 697.0, "text": " Give me another sample that basically has the same content as X but is different.", "tokens": [50364, 407, 291, 13330, 264, 4914, 295, 264, 1412, 47138, 281, 264, 1185, 538, 1936, 3585, 309, 11, 510, 307, 257, 6889, 11, 980, 385, 307, 257, 6889, 1783, 13, 50814, 50814, 5303, 385, 1071, 6889, 300, 1936, 575, 264, 912, 2701, 382, 1783, 457, 307, 819, 13, 51164, 51164, 400, 295, 1164, 291, 434, 1128, 516, 281, 1029, 264, 1185, 281, 976, 291, 300, 6889, 13, 51314, 51314, 509, 434, 516, 281, 8460, 729, 10938, 293, 3847, 264, 1185, 365, 309, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0909468462300855, "compression_ratio": 1.7281553398058251, "no_speech_prob": 2.1780157112516463e-05}, {"id": 70, "seek": 68100, "start": 697.0, "end": 700.0, "text": " And of course you're never going to ask the system to give you that sample.", "tokens": [50364, 407, 291, 13330, 264, 4914, 295, 264, 1412, 47138, 281, 264, 1185, 538, 1936, 3585, 309, 11, 510, 307, 257, 6889, 11, 980, 385, 307, 257, 6889, 1783, 13, 50814, 50814, 5303, 385, 1071, 6889, 300, 1936, 575, 264, 912, 2701, 382, 1783, 457, 307, 819, 13, 51164, 51164, 400, 295, 1164, 291, 434, 1128, 516, 281, 1029, 264, 1185, 281, 976, 291, 300, 6889, 13, 51314, 51314, 509, 434, 516, 281, 8460, 729, 10938, 293, 3847, 264, 1185, 365, 309, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0909468462300855, "compression_ratio": 1.7281553398058251, "no_speech_prob": 2.1780157112516463e-05}, {"id": 71, "seek": 68100, "start": 700.0, "end": 704.0, "text": " You're going to generate those samples and train the system with it.", "tokens": [50364, 407, 291, 13330, 264, 4914, 295, 264, 1412, 47138, 281, 264, 1185, 538, 1936, 3585, 309, 11, 510, 307, 257, 6889, 11, 980, 385, 307, 257, 6889, 1783, 13, 50814, 50814, 5303, 385, 1071, 6889, 300, 1936, 575, 264, 912, 2701, 382, 1783, 457, 307, 819, 13, 51164, 51164, 400, 295, 1164, 291, 434, 1128, 516, 281, 1029, 264, 1185, 281, 976, 291, 300, 6889, 13, 51314, 51314, 509, 434, 516, 281, 8460, 729, 10938, 293, 3847, 264, 1185, 365, 309, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0909468462300855, "compression_ratio": 1.7281553398058251, "no_speech_prob": 2.1780157112516463e-05}, {"id": 72, "seek": 70400, "start": 704.0, "end": 711.0, "text": " So there are two pairs, positive pairs. So pairs are compatible with each other, which is the whole idea of energy-based models.", "tokens": [50364, 407, 456, 366, 732, 15494, 11, 3353, 15494, 13, 407, 15494, 366, 18218, 365, 1184, 661, 11, 597, 307, 264, 1379, 1558, 295, 2281, 12, 6032, 5245, 13, 50714, 50714, 400, 257, 18218, 6119, 307, 11, 420, 257, 3353, 6119, 498, 291, 528, 11, 14689, 295, 1783, 885, 364, 3256, 293, 398, 885, 257, 9887, 295, 341, 3256, 300, 1936, 775, 406, 1319, 1080, 2701, 13, 51314, 51314, 407, 309, 311, 920, 264, 912, 2701, 295, 264, 3256, 498, 291, 528, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.0798501358475796, "compression_ratio": 1.6933962264150944, "no_speech_prob": 1.0782738172565587e-05}, {"id": 73, "seek": 70400, "start": 711.0, "end": 723.0, "text": " And a compatible pair is, or a positive pair if you want, consists of X being an image and Y being a transformation of this image that basically does not change its content.", "tokens": [50364, 407, 456, 366, 732, 15494, 11, 3353, 15494, 13, 407, 15494, 366, 18218, 365, 1184, 661, 11, 597, 307, 264, 1379, 1558, 295, 2281, 12, 6032, 5245, 13, 50714, 50714, 400, 257, 18218, 6119, 307, 11, 420, 257, 3353, 6119, 498, 291, 528, 11, 14689, 295, 1783, 885, 364, 3256, 293, 398, 885, 257, 9887, 295, 341, 3256, 300, 1936, 775, 406, 1319, 1080, 2701, 13, 51314, 51314, 407, 309, 311, 920, 264, 912, 2701, 295, 264, 3256, 498, 291, 528, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.0798501358475796, "compression_ratio": 1.6933962264150944, "no_speech_prob": 1.0782738172565587e-05}, {"id": 74, "seek": 70400, "start": 723.0, "end": 726.0, "text": " So it's still the same content of the image if you want.", "tokens": [50364, 407, 456, 366, 732, 15494, 11, 3353, 15494, 13, 407, 15494, 366, 18218, 365, 1184, 661, 11, 597, 307, 264, 1379, 1558, 295, 2281, 12, 6032, 5245, 13, 50714, 50714, 400, 257, 18218, 6119, 307, 11, 420, 257, 3353, 6119, 498, 291, 528, 11, 14689, 295, 1783, 885, 364, 3256, 293, 398, 885, 257, 9887, 295, 341, 3256, 300, 1936, 775, 406, 1319, 1080, 2701, 13, 51314, 51314, 407, 309, 311, 920, 264, 912, 2701, 295, 264, 3256, 498, 291, 528, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.0798501358475796, "compression_ratio": 1.6933962264150944, "no_speech_prob": 1.0782738172565587e-05}, {"id": 75, "seek": 72600, "start": 726.0, "end": 734.0, "text": " So you want basically representations extracted by those two networks to be very similar because those images are similar.", "tokens": [50364, 407, 291, 528, 1936, 33358, 34086, 538, 729, 732, 9590, 281, 312, 588, 2531, 570, 729, 5267, 366, 2531, 13, 50764, 50764, 400, 300, 311, 2293, 437, 291, 434, 516, 281, 360, 13, 509, 434, 516, 281, 3154, 729, 732, 5267, 281, 729, 732, 9590, 13, 51014, 51014, 400, 291, 434, 516, 281, 362, 257, 4470, 2445, 300, 1619, 17522, 264, 2281, 11, 597, 1355, 17522, 264, 4560, 420, 32194, 3481, 1296, 389, 293, 389, 5835, 11, 1296, 264, 23930, 295, 264, 732, 9590, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06377767712882396, "compression_ratio": 1.8458149779735682, "no_speech_prob": 1.892367799882777e-05}, {"id": 76, "seek": 72600, "start": 734.0, "end": 739.0, "text": " And that's exactly what you're going to do. You're going to feed those two images to those two networks.", "tokens": [50364, 407, 291, 528, 1936, 33358, 34086, 538, 729, 732, 9590, 281, 312, 588, 2531, 570, 729, 5267, 366, 2531, 13, 50764, 50764, 400, 300, 311, 2293, 437, 291, 434, 516, 281, 360, 13, 509, 434, 516, 281, 3154, 729, 732, 5267, 281, 729, 732, 9590, 13, 51014, 51014, 400, 291, 434, 516, 281, 362, 257, 4470, 2445, 300, 1619, 17522, 264, 2281, 11, 597, 1355, 17522, 264, 4560, 420, 32194, 3481, 1296, 389, 293, 389, 5835, 11, 1296, 264, 23930, 295, 264, 732, 9590, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06377767712882396, "compression_ratio": 1.8458149779735682, "no_speech_prob": 1.892367799882777e-05}, {"id": 77, "seek": 72600, "start": 739.0, "end": 750.0, "text": " And you're going to have a loss function that says minimize the energy, which means minimize the distance or similarity measure between H and H prime, between the outputs of the two networks.", "tokens": [50364, 407, 291, 528, 1936, 33358, 34086, 538, 729, 732, 9590, 281, 312, 588, 2531, 570, 729, 5267, 366, 2531, 13, 50764, 50764, 400, 300, 311, 2293, 437, 291, 434, 516, 281, 360, 13, 509, 434, 516, 281, 3154, 729, 732, 5267, 281, 729, 732, 9590, 13, 51014, 51014, 400, 291, 434, 516, 281, 362, 257, 4470, 2445, 300, 1619, 17522, 264, 2281, 11, 597, 1355, 17522, 264, 4560, 420, 32194, 3481, 1296, 389, 293, 389, 5835, 11, 1296, 264, 23930, 295, 264, 732, 9590, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06377767712882396, "compression_ratio": 1.8458149779735682, "no_speech_prob": 1.892367799882777e-05}, {"id": 78, "seek": 75000, "start": 750.0, "end": 758.0, "text": " So that's the positive part. So that's a way to kind of lower the energy for training samples.", "tokens": [50364, 407, 300, 311, 264, 3353, 644, 13, 407, 300, 311, 257, 636, 281, 733, 295, 3126, 264, 2281, 337, 3097, 10938, 13, 50764, 50764, 400, 550, 291, 362, 281, 8460, 4974, 3671, 10938, 13, 51014, 51014, 400, 264, 636, 291, 8460, 552, 307, 538, 1936, 8867, 11, 797, 11, 257, 6889, 337, 1783, 293, 550, 8867, 1071, 3256, 300, 291, 458, 307, 819, 11, 300, 575, 1825, 281, 360, 365, 1783, 11, 307, 40393, 267, 964, 365, 309, 498, 291, 528, 13, 51564, 51564, 467, 311, 588, 819, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06623675746302451, "compression_ratio": 1.7235023041474655, "no_speech_prob": 1.112503741751425e-05}, {"id": 79, "seek": 75000, "start": 758.0, "end": 763.0, "text": " And then you have to generate random negative samples.", "tokens": [50364, 407, 300, 311, 264, 3353, 644, 13, 407, 300, 311, 257, 636, 281, 733, 295, 3126, 264, 2281, 337, 3097, 10938, 13, 50764, 50764, 400, 550, 291, 362, 281, 8460, 4974, 3671, 10938, 13, 51014, 51014, 400, 264, 636, 291, 8460, 552, 307, 538, 1936, 8867, 11, 797, 11, 257, 6889, 337, 1783, 293, 550, 8867, 1071, 3256, 300, 291, 458, 307, 819, 11, 300, 575, 1825, 281, 360, 365, 1783, 11, 307, 40393, 267, 964, 365, 309, 498, 291, 528, 13, 51564, 51564, 467, 311, 588, 819, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06623675746302451, "compression_ratio": 1.7235023041474655, "no_speech_prob": 1.112503741751425e-05}, {"id": 80, "seek": 75000, "start": 763.0, "end": 774.0, "text": " And the way you generate them is by basically picking, again, a sample for X and then picking another image that you know is different, that has nothing to do with X, is incompatible with it if you want.", "tokens": [50364, 407, 300, 311, 264, 3353, 644, 13, 407, 300, 311, 257, 636, 281, 733, 295, 3126, 264, 2281, 337, 3097, 10938, 13, 50764, 50764, 400, 550, 291, 362, 281, 8460, 4974, 3671, 10938, 13, 51014, 51014, 400, 264, 636, 291, 8460, 552, 307, 538, 1936, 8867, 11, 797, 11, 257, 6889, 337, 1783, 293, 550, 8867, 1071, 3256, 300, 291, 458, 307, 819, 11, 300, 575, 1825, 281, 360, 365, 1783, 11, 307, 40393, 267, 964, 365, 309, 498, 291, 528, 13, 51564, 51564, 467, 311, 588, 819, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06623675746302451, "compression_ratio": 1.7235023041474655, "no_speech_prob": 1.112503741751425e-05}, {"id": 81, "seek": 75000, "start": 774.0, "end": 776.0, "text": " It's very different.", "tokens": [50364, 407, 300, 311, 264, 3353, 644, 13, 407, 300, 311, 257, 636, 281, 733, 295, 3126, 264, 2281, 337, 3097, 10938, 13, 50764, 50764, 400, 550, 291, 362, 281, 8460, 4974, 3671, 10938, 13, 51014, 51014, 400, 264, 636, 291, 8460, 552, 307, 538, 1936, 8867, 11, 797, 11, 257, 6889, 337, 1783, 293, 550, 8867, 1071, 3256, 300, 291, 458, 307, 819, 11, 300, 575, 1825, 281, 360, 365, 1783, 11, 307, 40393, 267, 964, 365, 309, 498, 291, 528, 13, 51564, 51564, 467, 311, 588, 819, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06623675746302451, "compression_ratio": 1.7235023041474655, "no_speech_prob": 1.112503741751425e-05}, {"id": 82, "seek": 77600, "start": 776.0, "end": 785.0, "text": " And now what you do is you feed those two images to those two networks and you try to push H and H prime away from each other.", "tokens": [50364, 400, 586, 437, 291, 360, 307, 291, 3154, 729, 732, 5267, 281, 729, 732, 9590, 293, 291, 853, 281, 2944, 389, 293, 389, 5835, 1314, 490, 1184, 661, 13, 50814, 50814, 407, 1936, 291, 434, 1382, 281, 652, 264, 32194, 20678, 383, 295, 389, 293, 389, 5835, 2416, 337, 729, 732, 10938, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.07915469219810084, "compression_ratio": 1.519736842105263, "no_speech_prob": 1.4738720892637502e-05}, {"id": 83, "seek": 77600, "start": 785.0, "end": 795.0, "text": " So basically you're trying to make the similarity metric C of H and H prime large for those two samples.", "tokens": [50364, 400, 586, 437, 291, 360, 307, 291, 3154, 729, 732, 5267, 281, 729, 732, 9590, 293, 291, 853, 281, 2944, 389, 293, 389, 5835, 1314, 490, 1184, 661, 13, 50814, 50814, 407, 1936, 291, 434, 1382, 281, 652, 264, 32194, 20678, 383, 295, 389, 293, 389, 5835, 2416, 337, 729, 732, 10938, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.07915469219810084, "compression_ratio": 1.519736842105263, "no_speech_prob": 1.4738720892637502e-05}, {"id": 84, "seek": 79500, "start": 795.0, "end": 807.0, "text": " And the objective function here is going to take into account the energy function for similar pairs and the energy function for dissimilar pairs.", "tokens": [50364, 400, 264, 10024, 2445, 510, 307, 516, 281, 747, 666, 2696, 264, 2281, 2445, 337, 2531, 15494, 293, 264, 2281, 2445, 337, 7802, 332, 2202, 15494, 13, 50964, 50964, 467, 311, 516, 281, 2944, 760, 322, 264, 2281, 2445, 337, 2531, 15494, 11, 2944, 493, 322, 264, 2281, 2445, 337, 7802, 332, 2202, 15494, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.05540458226608018, "compression_ratio": 2.129032258064516, "no_speech_prob": 8.397732926823664e-06}, {"id": 85, "seek": 79500, "start": 807.0, "end": 816.0, "text": " It's going to push down on the energy function for similar pairs, push up on the energy function for dissimilar pairs.", "tokens": [50364, 400, 264, 10024, 2445, 510, 307, 516, 281, 747, 666, 2696, 264, 2281, 2445, 337, 2531, 15494, 293, 264, 2281, 2445, 337, 7802, 332, 2202, 15494, 13, 50964, 50964, 467, 311, 516, 281, 2944, 760, 322, 264, 2281, 2445, 337, 2531, 15494, 11, 2944, 493, 322, 264, 2281, 2445, 337, 7802, 332, 2202, 15494, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.05540458226608018, "compression_ratio": 2.129032258064516, "no_speech_prob": 8.397732926823664e-06}, {"id": 86, "seek": 81600, "start": 816.0, "end": 830.0, "text": " So there's been a number of \u2013 so people have used metric learning for various things for a long time, for image search, for example, for fit recognition, for things like that.", "tokens": [50364, 407, 456, 311, 668, 257, 1230, 295, 1662, 370, 561, 362, 1143, 20678, 2539, 337, 3683, 721, 337, 257, 938, 565, 11, 337, 3256, 3164, 11, 337, 1365, 11, 337, 3318, 11150, 11, 337, 721, 411, 300, 13, 51064, 51064, 583, 309, 311, 787, 294, 264, 1036, 1326, 2493, 300, 456, 311, 668, 257, 1916, 295, 1985, 300, 362, 4898, 291, 393, 764, 729, 7150, 281, 1466, 665, 4122, 337, 2657, 11150, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09165548349355722, "compression_ratio": 1.6047619047619048, "no_speech_prob": 5.421738933364395e-06}, {"id": 87, "seek": 81600, "start": 830.0, "end": 840.0, "text": " But it's only in the last few months that there's been a couple of works that have shown you can use those methods to learn good features for object recognition.", "tokens": [50364, 407, 456, 311, 668, 257, 1230, 295, 1662, 370, 561, 362, 1143, 20678, 2539, 337, 3683, 721, 337, 257, 938, 565, 11, 337, 3256, 3164, 11, 337, 1365, 11, 337, 3318, 11150, 11, 337, 721, 411, 300, 13, 51064, 51064, 583, 309, 311, 787, 294, 264, 1036, 1326, 2493, 300, 456, 311, 668, 257, 1916, 295, 1985, 300, 362, 4898, 291, 393, 764, 729, 7150, 281, 1466, 665, 4122, 337, 2657, 11150, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09165548349355722, "compression_ratio": 1.6047619047619048, "no_speech_prob": 5.421738933364395e-06}, {"id": 88, "seek": 84000, "start": 840.0, "end": 851.0, "text": " And those are really the first papers that produce features in an unsupervised or self-supervised way that produce features that can rival the features that are obtained through supervised learning.", "tokens": [50364, 400, 729, 366, 534, 264, 700, 10577, 300, 5258, 4122, 294, 364, 2693, 12879, 24420, 420, 2698, 12, 48172, 24420, 636, 300, 5258, 4122, 300, 393, 16286, 264, 4122, 300, 366, 14879, 807, 46533, 2539, 13, 50914, 50914, 407, 264, 1045, 10577, 294, 1168, 366, 24346, 1899, 43, 11, 597, 1355, 6001, 12, 51, 557, 278, 32511, 394, 19945, 399, 15205, 538, 1119, 1641, 282, 376, 742, 424, 293, 18915, 384, 3161, 1163, 6789, 268, 412, 4384, 294, 1873, 3609, 11, 51464, 51464, 1071, 472, 1219, 376, 9443, 46, 538, 3189, 332, 278, 634, 293, 702, 39789, 412, 4384, 294, 2458, 752, 4964, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.212816768222385, "compression_ratio": 1.6777777777777778, "no_speech_prob": 4.5363176468526945e-05}, {"id": 89, "seek": 84000, "start": 851.0, "end": 862.0, "text": " So the three papers in question are PEARL, which means Pre-Tecting Variant Representation Learning by Ishaan Mishra and Laurence van der Maten at Facebook in New York,", "tokens": [50364, 400, 729, 366, 534, 264, 700, 10577, 300, 5258, 4122, 294, 364, 2693, 12879, 24420, 420, 2698, 12, 48172, 24420, 636, 300, 5258, 4122, 300, 393, 16286, 264, 4122, 300, 366, 14879, 807, 46533, 2539, 13, 50914, 50914, 407, 264, 1045, 10577, 294, 1168, 366, 24346, 1899, 43, 11, 597, 1355, 6001, 12, 51, 557, 278, 32511, 394, 19945, 399, 15205, 538, 1119, 1641, 282, 376, 742, 424, 293, 18915, 384, 3161, 1163, 6789, 268, 412, 4384, 294, 1873, 3609, 11, 51464, 51464, 1071, 472, 1219, 376, 9443, 46, 538, 3189, 332, 278, 634, 293, 702, 39789, 412, 4384, 294, 2458, 752, 4964, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.212816768222385, "compression_ratio": 1.6777777777777778, "no_speech_prob": 4.5363176468526945e-05}, {"id": 90, "seek": 84000, "start": 862.0, "end": 867.0, "text": " another one called MOKO by Keiming He and his collaborators at Facebook in Manlo Park,", "tokens": [50364, 400, 729, 366, 534, 264, 700, 10577, 300, 5258, 4122, 294, 364, 2693, 12879, 24420, 420, 2698, 12, 48172, 24420, 636, 300, 5258, 4122, 300, 393, 16286, 264, 4122, 300, 366, 14879, 807, 46533, 2539, 13, 50914, 50914, 407, 264, 1045, 10577, 294, 1168, 366, 24346, 1899, 43, 11, 597, 1355, 6001, 12, 51, 557, 278, 32511, 394, 19945, 399, 15205, 538, 1119, 1641, 282, 376, 742, 424, 293, 18915, 384, 3161, 1163, 6789, 268, 412, 4384, 294, 1873, 3609, 11, 51464, 51464, 1071, 472, 1219, 376, 9443, 46, 538, 3189, 332, 278, 634, 293, 702, 39789, 412, 4384, 294, 2458, 752, 4964, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.212816768222385, "compression_ratio": 1.6777777777777778, "no_speech_prob": 4.5363176468526945e-05}, {"id": 91, "seek": 86700, "start": 867.0, "end": 880.0, "text": " and the third one, which appeared more recently, is called SIMCLEAR by a group from Google, Janet Al and the last author being Jeffrey Hinton.", "tokens": [50364, 293, 264, 2636, 472, 11, 597, 8516, 544, 3938, 11, 307, 1219, 24738, 34, 2634, 1899, 538, 257, 1594, 490, 3329, 11, 26948, 967, 293, 264, 1036, 3793, 885, 28721, 389, 12442, 13, 51014, 51014, 407, 456, 311, 668, 661, 589, 1228, 729, 733, 295, 7150, 13, 51414, 51414, 286, 519, 300, 309, 390, 257, 1168, 11, 4317, 30, 51514, 51514, 883, 11, 309, 2067, 380, 257, 1168, 13, 467, 390, 767, 452, 2593, 20447, 493, 570, 286, 848, 3329, 13, 51714, 51714, 876, 11, 286, 536, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.173299851624862, "compression_ratio": 1.4649122807017543, "no_speech_prob": 0.0003882691671606153}, {"id": 92, "seek": 86700, "start": 880.0, "end": 888.0, "text": " So there's been other work using those kind of methods.", "tokens": [50364, 293, 264, 2636, 472, 11, 597, 8516, 544, 3938, 11, 307, 1219, 24738, 34, 2634, 1899, 538, 257, 1594, 490, 3329, 11, 26948, 967, 293, 264, 1036, 3793, 885, 28721, 389, 12442, 13, 51014, 51014, 407, 456, 311, 668, 661, 589, 1228, 729, 733, 295, 7150, 13, 51414, 51414, 286, 519, 300, 309, 390, 257, 1168, 11, 4317, 30, 51514, 51514, 883, 11, 309, 2067, 380, 257, 1168, 13, 467, 390, 767, 452, 2593, 20447, 493, 570, 286, 848, 3329, 13, 51714, 51714, 876, 11, 286, 536, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.173299851624862, "compression_ratio": 1.4649122807017543, "no_speech_prob": 0.0003882691671606153}, {"id": 93, "seek": 86700, "start": 888.0, "end": 890.0, "text": " I think that it was a question, perhaps?", "tokens": [50364, 293, 264, 2636, 472, 11, 597, 8516, 544, 3938, 11, 307, 1219, 24738, 34, 2634, 1899, 538, 257, 1594, 490, 3329, 11, 26948, 967, 293, 264, 1036, 3793, 885, 28721, 389, 12442, 13, 51014, 51014, 407, 456, 311, 668, 661, 589, 1228, 729, 733, 295, 7150, 13, 51414, 51414, 286, 519, 300, 309, 390, 257, 1168, 11, 4317, 30, 51514, 51514, 883, 11, 309, 2067, 380, 257, 1168, 13, 467, 390, 767, 452, 2593, 20447, 493, 570, 286, 848, 3329, 13, 51714, 51714, 876, 11, 286, 536, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.173299851624862, "compression_ratio": 1.4649122807017543, "no_speech_prob": 0.0003882691671606153}, {"id": 94, "seek": 86700, "start": 890.0, "end": 894.0, "text": " No, it wasn't a question. It was actually my phone waking up because I said Google.", "tokens": [50364, 293, 264, 2636, 472, 11, 597, 8516, 544, 3938, 11, 307, 1219, 24738, 34, 2634, 1899, 538, 257, 1594, 490, 3329, 11, 26948, 967, 293, 264, 1036, 3793, 885, 28721, 389, 12442, 13, 51014, 51014, 407, 456, 311, 668, 661, 589, 1228, 729, 733, 295, 7150, 13, 51414, 51414, 286, 519, 300, 309, 390, 257, 1168, 11, 4317, 30, 51514, 51514, 883, 11, 309, 2067, 380, 257, 1168, 13, 467, 390, 767, 452, 2593, 20447, 493, 570, 286, 848, 3329, 13, 51714, 51714, 876, 11, 286, 536, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.173299851624862, "compression_ratio": 1.4649122807017543, "no_speech_prob": 0.0003882691671606153}, {"id": 95, "seek": 86700, "start": 894.0, "end": 895.0, "text": " Oh, I see.", "tokens": [50364, 293, 264, 2636, 472, 11, 597, 8516, 544, 3938, 11, 307, 1219, 24738, 34, 2634, 1899, 538, 257, 1594, 490, 3329, 11, 26948, 967, 293, 264, 1036, 3793, 885, 28721, 389, 12442, 13, 51014, 51014, 407, 456, 311, 668, 661, 589, 1228, 729, 733, 295, 7150, 13, 51414, 51414, 286, 519, 300, 309, 390, 257, 1168, 11, 4317, 30, 51514, 51514, 883, 11, 309, 2067, 380, 257, 1168, 13, 467, 390, 767, 452, 2593, 20447, 493, 570, 286, 848, 3329, 13, 51714, 51714, 876, 11, 286, 536, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.173299851624862, "compression_ratio": 1.4649122807017543, "no_speech_prob": 0.0003882691671606153}, {"id": 96, "seek": 89500, "start": 895.0, "end": 903.0, "text": " Okay, and slow features, something we'll talk about later, which is a little similar.", "tokens": [50364, 1033, 11, 293, 2964, 4122, 11, 746, 321, 603, 751, 466, 1780, 11, 597, 307, 257, 707, 2531, 13, 50764, 50764, 1033, 11, 370, 613, 366, 5110, 295, 3542, 300, 366, 14879, 365, 376, 9443, 46, 11, 293, 436, 4476, 855, 300, 754, 365, 257, 588, 2416, 2316, 11, 51114, 51114, 597, 307, 1936, 257, 3037, 295, 5015, 31890, 12, 2803, 11, 300, 291, 3847, 1228, 341, 8712, 278, 3170, 11, 291, 483, 8681, 3389, 13, 51464, 51464, 639, 307, 11, 286, 1697, 11, 1192, 1732, 3389, 322, 29903, 31890, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.151872414036801, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.00010386779467808083}, {"id": 97, "seek": 89500, "start": 903.0, "end": 910.0, "text": " Okay, so these are examples of results that are obtained with MOKO, and they essentially show that even with a very large model,", "tokens": [50364, 1033, 11, 293, 2964, 4122, 11, 746, 321, 603, 751, 466, 1780, 11, 597, 307, 257, 707, 2531, 13, 50764, 50764, 1033, 11, 370, 613, 366, 5110, 295, 3542, 300, 366, 14879, 365, 376, 9443, 46, 11, 293, 436, 4476, 855, 300, 754, 365, 257, 588, 2416, 2316, 11, 51114, 51114, 597, 307, 1936, 257, 3037, 295, 5015, 31890, 12, 2803, 11, 300, 291, 3847, 1228, 341, 8712, 278, 3170, 11, 291, 483, 8681, 3389, 13, 51464, 51464, 639, 307, 11, 286, 1697, 11, 1192, 1732, 3389, 322, 29903, 31890, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.151872414036801, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.00010386779467808083}, {"id": 98, "seek": 89500, "start": 910.0, "end": 917.0, "text": " which is basically a version of ResNet-50, that you train using this contrasting method, you get decent performance.", "tokens": [50364, 1033, 11, 293, 2964, 4122, 11, 746, 321, 603, 751, 466, 1780, 11, 597, 307, 257, 707, 2531, 13, 50764, 50764, 1033, 11, 370, 613, 366, 5110, 295, 3542, 300, 366, 14879, 365, 376, 9443, 46, 11, 293, 436, 4476, 855, 300, 754, 365, 257, 588, 2416, 2316, 11, 51114, 51114, 597, 307, 1936, 257, 3037, 295, 5015, 31890, 12, 2803, 11, 300, 291, 3847, 1228, 341, 8712, 278, 3170, 11, 291, 483, 8681, 3389, 13, 51464, 51464, 639, 307, 11, 286, 1697, 11, 1192, 1732, 3389, 322, 29903, 31890, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.151872414036801, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.00010386779467808083}, {"id": 99, "seek": 89500, "start": 917.0, "end": 924.0, "text": " This is, I believe, top five performance on ImageNet.", "tokens": [50364, 1033, 11, 293, 2964, 4122, 11, 746, 321, 603, 751, 466, 1780, 11, 597, 307, 257, 707, 2531, 13, 50764, 50764, 1033, 11, 370, 613, 366, 5110, 295, 3542, 300, 366, 14879, 365, 376, 9443, 46, 11, 293, 436, 4476, 855, 300, 754, 365, 257, 588, 2416, 2316, 11, 51114, 51114, 597, 307, 1936, 257, 3037, 295, 5015, 31890, 12, 2803, 11, 300, 291, 3847, 1228, 341, 8712, 278, 3170, 11, 291, 483, 8681, 3389, 13, 51464, 51464, 639, 307, 11, 286, 1697, 11, 1192, 1732, 3389, 322, 29903, 31890, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.151872414036801, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.00010386779467808083}, {"id": 100, "seek": 92400, "start": 924.0, "end": 927.0, "text": " Perl actually works quite a bit better than MOKO.", "tokens": [50364, 3026, 75, 767, 1985, 1596, 257, 857, 1101, 813, 376, 9443, 46, 13, 50514, 50514, 639, 307, 1192, 472, 14170, 341, 565, 365, 9590, 295, 3683, 11602, 13, 50914, 50914, 407, 510, 11, 456, 311, 2940, 15077, 13, 51064, 51064, 440, 2135, 9005, 307, 291, 747, 439, 295, 29903, 31890, 11, 291, 747, 257, 6889, 490, 29903, 31890, 11, 37555, 309, 11, 293, 300, 2709, 291, 257, 3353, 6119, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10624939042168695, "compression_ratio": 1.4893617021276595, "no_speech_prob": 5.826396954944357e-05}, {"id": 101, "seek": 92400, "start": 927.0, "end": 935.0, "text": " This is top one accuracy this time with networks of various sizes.", "tokens": [50364, 3026, 75, 767, 1985, 1596, 257, 857, 1101, 813, 376, 9443, 46, 13, 50514, 50514, 639, 307, 1192, 472, 14170, 341, 565, 365, 9590, 295, 3683, 11602, 13, 50914, 50914, 407, 510, 11, 456, 311, 2940, 15077, 13, 51064, 51064, 440, 2135, 9005, 307, 291, 747, 439, 295, 29903, 31890, 11, 291, 747, 257, 6889, 490, 29903, 31890, 11, 37555, 309, 11, 293, 300, 2709, 291, 257, 3353, 6119, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10624939042168695, "compression_ratio": 1.4893617021276595, "no_speech_prob": 5.826396954944357e-05}, {"id": 102, "seek": 92400, "start": 935.0, "end": 938.0, "text": " So here, there's several scenarios.", "tokens": [50364, 3026, 75, 767, 1985, 1596, 257, 857, 1101, 813, 376, 9443, 46, 13, 50514, 50514, 639, 307, 1192, 472, 14170, 341, 565, 365, 9590, 295, 3683, 11602, 13, 50914, 50914, 407, 510, 11, 456, 311, 2940, 15077, 13, 51064, 51064, 440, 2135, 9005, 307, 291, 747, 439, 295, 29903, 31890, 11, 291, 747, 257, 6889, 490, 29903, 31890, 11, 37555, 309, 11, 293, 300, 2709, 291, 257, 3353, 6119, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10624939042168695, "compression_ratio": 1.4893617021276595, "no_speech_prob": 5.826396954944357e-05}, {"id": 103, "seek": 92400, "start": 938.0, "end": 948.0, "text": " The main scenario is you take all of ImageNet, you take a sample from ImageNet, distort it, and that gives you a positive pair.", "tokens": [50364, 3026, 75, 767, 1985, 1596, 257, 857, 1101, 813, 376, 9443, 46, 13, 50514, 50514, 639, 307, 1192, 472, 14170, 341, 565, 365, 9590, 295, 3683, 11602, 13, 50914, 50914, 407, 510, 11, 456, 311, 2940, 15077, 13, 51064, 51064, 440, 2135, 9005, 307, 291, 747, 439, 295, 29903, 31890, 11, 291, 747, 257, 6889, 490, 29903, 31890, 11, 37555, 309, 11, 293, 300, 2709, 291, 257, 3353, 6119, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10624939042168695, "compression_ratio": 1.4893617021276595, "no_speech_prob": 5.826396954944357e-05}, {"id": 104, "seek": 94800, "start": 948.0, "end": 955.0, "text": " Run it through your two networks and train the network to produce similar outputs.", "tokens": [50364, 8950, 309, 807, 428, 732, 9590, 293, 3847, 264, 3209, 281, 5258, 2531, 23930, 13, 50714, 50714, 8537, 11, 264, 732, 9590, 366, 14800, 11, 767, 11, 337, 1293, 376, 9443, 46, 293, 3026, 75, 13, 467, 311, 264, 912, 382, 6188, 13, 50964, 50964, 400, 550, 747, 7802, 332, 2202, 15494, 293, 2944, 264, 23930, 1314, 490, 1184, 661, 1228, 257, 1729, 2063, 2445, 300, 321, 603, 536, 294, 257, 3456, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09133162436547218, "compression_ratio": 1.5365853658536586, "no_speech_prob": 3.480015948298387e-05}, {"id": 105, "seek": 94800, "start": 955.0, "end": 960.0, "text": " Basically, the two networks are identical, actually, for both MOKO and Perl. It's the same as Net.", "tokens": [50364, 8950, 309, 807, 428, 732, 9590, 293, 3847, 264, 3209, 281, 5258, 2531, 23930, 13, 50714, 50714, 8537, 11, 264, 732, 9590, 366, 14800, 11, 767, 11, 337, 1293, 376, 9443, 46, 293, 3026, 75, 13, 467, 311, 264, 912, 382, 6188, 13, 50964, 50964, 400, 550, 747, 7802, 332, 2202, 15494, 293, 2944, 264, 23930, 1314, 490, 1184, 661, 1228, 257, 1729, 2063, 2445, 300, 321, 603, 536, 294, 257, 3456, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09133162436547218, "compression_ratio": 1.5365853658536586, "no_speech_prob": 3.480015948298387e-05}, {"id": 106, "seek": 94800, "start": 960.0, "end": 971.0, "text": " And then take dissimilar pairs and push the outputs away from each other using a particular cost function that we'll see in a minute.", "tokens": [50364, 8950, 309, 807, 428, 732, 9590, 293, 3847, 264, 3209, 281, 5258, 2531, 23930, 13, 50714, 50714, 8537, 11, 264, 732, 9590, 366, 14800, 11, 767, 11, 337, 1293, 376, 9443, 46, 293, 3026, 75, 13, 467, 311, 264, 912, 382, 6188, 13, 50964, 50964, 400, 550, 747, 7802, 332, 2202, 15494, 293, 2944, 264, 23930, 1314, 490, 1184, 661, 1228, 257, 1729, 2063, 2445, 300, 321, 603, 536, 294, 257, 3456, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09133162436547218, "compression_ratio": 1.5365853658536586, "no_speech_prob": 3.480015948298387e-05}, {"id": 107, "seek": 97100, "start": 971.0, "end": 979.0, "text": " And then you have to do this many, many times, and you have to be smart about how you cache the negative samples,", "tokens": [50364, 400, 550, 291, 362, 281, 360, 341, 867, 11, 867, 1413, 11, 293, 291, 362, 281, 312, 4069, 466, 577, 291, 19459, 264, 3671, 10938, 11, 50764, 50764, 570, 881, 10938, 366, 1217, 588, 819, 538, 264, 565, 436, 483, 281, 264, 5598, 295, 264, 3209, 13, 51064, 51064, 407, 291, 1936, 362, 281, 312, 4069, 466, 577, 291, 733, 295, 1888, 264, 665, 40019, 13, 51314, 51314, 407, 264, 2010, 295, 10024, 2445, 300, 307, 1143, 538, 3026, 75, 307, 1219, 5658, 8712, 35701, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0760436216990153, "compression_ratio": 1.7579908675799087, "no_speech_prob": 2.1431394998217e-05}, {"id": 108, "seek": 97100, "start": 979.0, "end": 985.0, "text": " because most samples are already very different by the time they get to the output of the network.", "tokens": [50364, 400, 550, 291, 362, 281, 360, 341, 867, 11, 867, 1413, 11, 293, 291, 362, 281, 312, 4069, 466, 577, 291, 19459, 264, 3671, 10938, 11, 50764, 50764, 570, 881, 10938, 366, 1217, 588, 819, 538, 264, 565, 436, 483, 281, 264, 5598, 295, 264, 3209, 13, 51064, 51064, 407, 291, 1936, 362, 281, 312, 4069, 466, 577, 291, 733, 295, 1888, 264, 665, 40019, 13, 51314, 51314, 407, 264, 2010, 295, 10024, 2445, 300, 307, 1143, 538, 3026, 75, 307, 1219, 5658, 8712, 35701, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0760436216990153, "compression_ratio": 1.7579908675799087, "no_speech_prob": 2.1431394998217e-05}, {"id": 109, "seek": 97100, "start": 985.0, "end": 990.0, "text": " So you basically have to be smart about how you kind of pick the good negatives.", "tokens": [50364, 400, 550, 291, 362, 281, 360, 341, 867, 11, 867, 1413, 11, 293, 291, 362, 281, 312, 4069, 466, 577, 291, 19459, 264, 3671, 10938, 11, 50764, 50764, 570, 881, 10938, 366, 1217, 588, 819, 538, 264, 565, 436, 483, 281, 264, 5598, 295, 264, 3209, 13, 51064, 51064, 407, 291, 1936, 362, 281, 312, 4069, 466, 577, 291, 733, 295, 1888, 264, 665, 40019, 13, 51314, 51314, 407, 264, 2010, 295, 10024, 2445, 300, 307, 1143, 538, 3026, 75, 307, 1219, 5658, 8712, 35701, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0760436216990153, "compression_ratio": 1.7579908675799087, "no_speech_prob": 2.1431394998217e-05}, {"id": 110, "seek": 97100, "start": 990.0, "end": 998.0, "text": " So the type of objective function that is used by Perl is called noise contrast estimation.", "tokens": [50364, 400, 550, 291, 362, 281, 360, 341, 867, 11, 867, 1413, 11, 293, 291, 362, 281, 312, 4069, 466, 577, 291, 19459, 264, 3671, 10938, 11, 50764, 50764, 570, 881, 10938, 366, 1217, 588, 819, 538, 264, 565, 436, 483, 281, 264, 5598, 295, 264, 3209, 13, 51064, 51064, 407, 291, 1936, 362, 281, 312, 4069, 466, 577, 291, 733, 295, 1888, 264, 665, 40019, 13, 51314, 51314, 407, 264, 2010, 295, 10024, 2445, 300, 307, 1143, 538, 3026, 75, 307, 1219, 5658, 8712, 35701, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0760436216990153, "compression_ratio": 1.7579908675799087, "no_speech_prob": 2.1431394998217e-05}, {"id": 111, "seek": 99800, "start": 998.0, "end": 1003.0, "text": " And that goes back to kind of previous papers. It's not their invention.", "tokens": [50364, 400, 300, 1709, 646, 281, 733, 295, 3894, 10577, 13, 467, 311, 406, 641, 22265, 13, 50614, 50614, 2305, 264, 32194, 20678, 307, 264, 23565, 32194, 3481, 1296, 264, 23930, 295, 264, 45216, 304, 36170, 13, 51014, 51014, 400, 550, 437, 291, 14722, 307, 341, 1936, 264, 2787, 41167, 12, 4092, 2445, 11, 51314, 51314, 597, 715, 1819, 264, 21510, 295, 264, 32194, 20678, 295, 732, 23930, 337, 2531, 15494, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10308250221046242, "compression_ratio": 1.7258883248730965, "no_speech_prob": 4.682247163145803e-05}, {"id": 112, "seek": 99800, "start": 1003.0, "end": 1011.0, "text": " Where the similarity metric is the cosine similarity measure between the outputs of the convolutional nets.", "tokens": [50364, 400, 300, 1709, 646, 281, 733, 295, 3894, 10577, 13, 467, 311, 406, 641, 22265, 13, 50614, 50614, 2305, 264, 32194, 20678, 307, 264, 23565, 32194, 3481, 1296, 264, 23930, 295, 264, 45216, 304, 36170, 13, 51014, 51014, 400, 550, 437, 291, 14722, 307, 341, 1936, 264, 2787, 41167, 12, 4092, 2445, 11, 51314, 51314, 597, 715, 1819, 264, 21510, 295, 264, 32194, 20678, 295, 732, 23930, 337, 2531, 15494, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10308250221046242, "compression_ratio": 1.7258883248730965, "no_speech_prob": 4.682247163145803e-05}, {"id": 113, "seek": 99800, "start": 1011.0, "end": 1017.0, "text": " And then what you compute is this basically the softmax-like function,", "tokens": [50364, 400, 300, 1709, 646, 281, 733, 295, 3894, 10577, 13, 467, 311, 406, 641, 22265, 13, 50614, 50614, 2305, 264, 32194, 20678, 307, 264, 23565, 32194, 3481, 1296, 264, 23930, 295, 264, 45216, 304, 36170, 13, 51014, 51014, 400, 550, 437, 291, 14722, 307, 341, 1936, 264, 2787, 41167, 12, 4092, 2445, 11, 51314, 51314, 597, 715, 1819, 264, 21510, 295, 264, 32194, 20678, 295, 732, 23930, 337, 2531, 15494, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10308250221046242, "compression_ratio": 1.7258883248730965, "no_speech_prob": 4.682247163145803e-05}, {"id": 114, "seek": 99800, "start": 1017.0, "end": 1023.0, "text": " which computes the exponential of the similarity metric of two outputs for similar pairs", "tokens": [50364, 400, 300, 1709, 646, 281, 733, 295, 3894, 10577, 13, 467, 311, 406, 641, 22265, 13, 50614, 50614, 2305, 264, 32194, 20678, 307, 264, 23565, 32194, 3481, 1296, 264, 23930, 295, 264, 45216, 304, 36170, 13, 51014, 51014, 400, 550, 437, 291, 14722, 307, 341, 1936, 264, 2787, 41167, 12, 4092, 2445, 11, 51314, 51314, 597, 715, 1819, 264, 21510, 295, 264, 32194, 20678, 295, 732, 23930, 337, 2531, 15494, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10308250221046242, "compression_ratio": 1.7258883248730965, "no_speech_prob": 4.682247163145803e-05}, {"id": 115, "seek": 102300, "start": 1023.0, "end": 1030.0, "text": " and then divides by the sum of the similarity metric exponentiated for similar pairs and the sum of dissimilar pairs.", "tokens": [50364, 293, 550, 41347, 538, 264, 2408, 295, 264, 32194, 20678, 12680, 23012, 770, 337, 2531, 15494, 293, 264, 2408, 295, 7802, 332, 2202, 15494, 13, 50714, 50714, 407, 291, 362, 257, 15245, 689, 291, 362, 472, 2531, 6119, 293, 257, 3840, 295, 7802, 332, 2202, 15494, 11, 293, 291, 14722, 341, 733, 295, 2787, 41167, 551, 13, 51064, 51064, 400, 498, 291, 17522, 264, 2787, 41167, 2063, 2445, 11, 309, 311, 516, 281, 2944, 264, 32194, 20678, 295, 2531, 6119, 281, 312, 382, 2416, 382, 1944, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06329109904530285, "compression_ratio": 1.8712871287128714, "no_speech_prob": 5.4740743507863954e-05}, {"id": 116, "seek": 102300, "start": 1030.0, "end": 1037.0, "text": " So you have a batch where you have one similar pair and a bunch of dissimilar pairs, and you compute this kind of softmax thing.", "tokens": [50364, 293, 550, 41347, 538, 264, 2408, 295, 264, 32194, 20678, 12680, 23012, 770, 337, 2531, 15494, 293, 264, 2408, 295, 7802, 332, 2202, 15494, 13, 50714, 50714, 407, 291, 362, 257, 15245, 689, 291, 362, 472, 2531, 6119, 293, 257, 3840, 295, 7802, 332, 2202, 15494, 11, 293, 291, 14722, 341, 733, 295, 2787, 41167, 551, 13, 51064, 51064, 400, 498, 291, 17522, 264, 2787, 41167, 2063, 2445, 11, 309, 311, 516, 281, 2944, 264, 32194, 20678, 295, 2531, 6119, 281, 312, 382, 2416, 382, 1944, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06329109904530285, "compression_ratio": 1.8712871287128714, "no_speech_prob": 5.4740743507863954e-05}, {"id": 117, "seek": 102300, "start": 1037.0, "end": 1046.0, "text": " And if you minimize the softmax cost function, it's going to push the similarity metric of similar pair to be as large as possible.", "tokens": [50364, 293, 550, 41347, 538, 264, 2408, 295, 264, 32194, 20678, 12680, 23012, 770, 337, 2531, 15494, 293, 264, 2408, 295, 7802, 332, 2202, 15494, 13, 50714, 50714, 407, 291, 362, 257, 15245, 689, 291, 362, 472, 2531, 6119, 293, 257, 3840, 295, 7802, 332, 2202, 15494, 11, 293, 291, 14722, 341, 733, 295, 2787, 41167, 551, 13, 51064, 51064, 400, 498, 291, 17522, 264, 2787, 41167, 2063, 2445, 11, 309, 311, 516, 281, 2944, 264, 32194, 20678, 295, 2531, 6119, 281, 312, 382, 2416, 382, 1944, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06329109904530285, "compression_ratio": 1.8712871287128714, "no_speech_prob": 5.4740743507863954e-05}, {"id": 118, "seek": 104600, "start": 1046.0, "end": 1053.0, "text": " And the similarity metric, the cosine similarity of dissimilar pair to be basically as small as possible.", "tokens": [50364, 400, 264, 32194, 20678, 11, 264, 23565, 32194, 295, 7802, 332, 2202, 6119, 281, 312, 1936, 382, 1359, 382, 1944, 13, 50714, 50714, 286, 632, 341, 1168, 300, 983, 366, 321, 14759, 1228, 364, 441, 45, 34, 2445, 11, 51064, 51064, 9735, 321, 727, 362, 1391, 3838, 40610, 4470, 1228, 264, 389, 25322, 11, 27619, 16894, 8482, 300, 321, 362, 538, 1940, 264, 3671, 3565, 295, 300, 8482, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13332783685971614, "compression_ratio": 1.603864734299517, "no_speech_prob": 7.88899342296645e-06}, {"id": 119, "seek": 104600, "start": 1053.0, "end": 1060.0, "text": " I had this question that why are we separately using an LNC function,", "tokens": [50364, 400, 264, 32194, 20678, 11, 264, 23565, 32194, 295, 7802, 332, 2202, 6119, 281, 312, 1936, 382, 1359, 382, 1944, 13, 50714, 50714, 286, 632, 341, 1168, 300, 983, 366, 321, 14759, 1228, 364, 441, 45, 34, 2445, 11, 51064, 51064, 9735, 321, 727, 362, 1391, 3838, 40610, 4470, 1228, 264, 389, 25322, 11, 27619, 16894, 8482, 300, 321, 362, 538, 1940, 264, 3671, 3565, 295, 300, 8482, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13332783685971614, "compression_ratio": 1.603864734299517, "no_speech_prob": 7.88899342296645e-06}, {"id": 120, "seek": 104600, "start": 1060.0, "end": 1071.0, "text": " whereas we could have probably directly computed loss using the HVI, VI transformed probability that we have by taking the negative log of that probability.", "tokens": [50364, 400, 264, 32194, 20678, 11, 264, 23565, 32194, 295, 7802, 332, 2202, 6119, 281, 312, 1936, 382, 1359, 382, 1944, 13, 50714, 50714, 286, 632, 341, 1168, 300, 983, 366, 321, 14759, 1228, 364, 441, 45, 34, 2445, 11, 51064, 51064, 9735, 321, 727, 362, 1391, 3838, 40610, 4470, 1228, 264, 389, 25322, 11, 27619, 16894, 8482, 300, 321, 362, 538, 1940, 264, 3671, 3565, 295, 300, 8482, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13332783685971614, "compression_ratio": 1.603864734299517, "no_speech_prob": 7.88899342296645e-06}, {"id": 121, "seek": 107100, "start": 1071.0, "end": 1082.0, "text": " So what benefit would LNC provide using not directly taking the negative log of the probability that we have from H?", "tokens": [50364, 407, 437, 5121, 576, 441, 45, 34, 2893, 1228, 406, 3838, 1940, 264, 3671, 3565, 295, 264, 8482, 300, 321, 362, 490, 389, 30, 50914, 50914, 1042, 11, 300, 311, 257, 665, 1168, 13, 467, 311, 406, 7696, 1850, 281, 385, 983, 13, 51164, 51164, 286, 519, 437, 2011, 456, 307, 300, 561, 3031, 3195, 293, 3195, 295, 819, 721, 11, 293, 341, 307, 437, 4590, 493, 1364, 1151, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08370888555372083, "compression_ratio": 1.5024630541871922, "no_speech_prob": 8.530057129974011e-06}, {"id": 122, "seek": 107100, "start": 1082.0, "end": 1087.0, "text": " Well, that's a good question. It's not entirely clear to me why.", "tokens": [50364, 407, 437, 5121, 576, 441, 45, 34, 2893, 1228, 406, 3838, 1940, 264, 3671, 3565, 295, 264, 8482, 300, 321, 362, 490, 389, 30, 50914, 50914, 1042, 11, 300, 311, 257, 665, 1168, 13, 467, 311, 406, 7696, 1850, 281, 385, 983, 13, 51164, 51164, 286, 519, 437, 2011, 456, 307, 300, 561, 3031, 3195, 293, 3195, 295, 819, 721, 11, 293, 341, 307, 437, 4590, 493, 1364, 1151, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08370888555372083, "compression_ratio": 1.5024630541871922, "no_speech_prob": 8.530057129974011e-06}, {"id": 123, "seek": 107100, "start": 1087.0, "end": 1094.0, "text": " I think what happened there is that people tried lots and lots of different things, and this is what ended up working best.", "tokens": [50364, 407, 437, 5121, 576, 441, 45, 34, 2893, 1228, 406, 3838, 1940, 264, 3671, 3565, 295, 264, 8482, 300, 321, 362, 490, 389, 30, 50914, 50914, 1042, 11, 300, 311, 257, 665, 1168, 13, 467, 311, 406, 7696, 1850, 281, 385, 983, 13, 51164, 51164, 286, 519, 437, 2011, 456, 307, 300, 561, 3031, 3195, 293, 3195, 295, 819, 721, 11, 293, 341, 307, 437, 4590, 493, 1364, 1151, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08370888555372083, "compression_ratio": 1.5024630541871922, "no_speech_prob": 8.530057129974011e-06}, {"id": 124, "seek": 109400, "start": 1094.0, "end": 1105.0, "text": " There is in the Hinton paper, there is kind of a similar thing where they tried different types of objective function and found that something like NC actually works quite well.", "tokens": [50364, 821, 307, 294, 264, 389, 12442, 3035, 11, 456, 307, 733, 295, 257, 2531, 551, 689, 436, 3031, 819, 3467, 295, 10024, 2445, 293, 1352, 300, 746, 411, 20786, 767, 1985, 1596, 731, 13, 50914, 50914, 407, 309, 311, 364, 31886, 1168, 11, 293, 286, 500, 380, 362, 257, 665, 24002, 337, 983, 291, 643, 341, 1433, 294, 4500, 281, 264, 20687, 294, 389, 13, 51464, 51464, 286, 1454, 341, 6338, 428, 1168, 11, 4878, 2597, 11, 286, 500, 380, 362, 604, 1867, 337, 309, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11931687461005316, "compression_ratio": 1.6008230452674896, "no_speech_prob": 7.25158752175048e-05}, {"id": 125, "seek": 109400, "start": 1105.0, "end": 1116.0, "text": " So it's an empirical question, and I don't have a good intuition for why you need this term in addition to the denominator in H.", "tokens": [50364, 821, 307, 294, 264, 389, 12442, 3035, 11, 456, 307, 733, 295, 257, 2531, 551, 689, 436, 3031, 819, 3467, 295, 10024, 2445, 293, 1352, 300, 746, 411, 20786, 767, 1985, 1596, 731, 13, 50914, 50914, 407, 309, 311, 364, 31886, 1168, 11, 293, 286, 500, 380, 362, 257, 665, 24002, 337, 983, 291, 643, 341, 1433, 294, 4500, 281, 264, 20687, 294, 389, 13, 51464, 51464, 286, 1454, 341, 6338, 428, 1168, 11, 4878, 2597, 11, 286, 500, 380, 362, 604, 1867, 337, 309, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11931687461005316, "compression_ratio": 1.6008230452674896, "no_speech_prob": 7.25158752175048e-05}, {"id": 126, "seek": 109400, "start": 1116.0, "end": 1123.0, "text": " I hope this answers your question, although sorry, I don't have any answer for it.", "tokens": [50364, 821, 307, 294, 264, 389, 12442, 3035, 11, 456, 307, 733, 295, 257, 2531, 551, 689, 436, 3031, 819, 3467, 295, 10024, 2445, 293, 1352, 300, 746, 411, 20786, 767, 1985, 1596, 731, 13, 50914, 50914, 407, 309, 311, 364, 31886, 1168, 11, 293, 286, 500, 380, 362, 257, 665, 24002, 337, 983, 291, 643, 341, 1433, 294, 4500, 281, 264, 20687, 294, 389, 13, 51464, 51464, 286, 1454, 341, 6338, 428, 1168, 11, 4878, 2597, 11, 286, 500, 380, 362, 604, 1867, 337, 309, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11931687461005316, "compression_ratio": 1.6008230452674896, "no_speech_prob": 7.25158752175048e-05}, {"id": 127, "seek": 112300, "start": 1123.0, "end": 1127.0, "text": " Why do you use cosine similarity instead of L2 norm?", "tokens": [50364, 1545, 360, 291, 764, 23565, 32194, 2602, 295, 441, 17, 2026, 30, 50564, 50564, 7156, 295, 441, 17, 2026, 420, 2602, 295, 485, 50664, 50664, 1033, 11, 309, 311, 570, 291, 528, 281, 2710, 1125, 13, 50814, 50814, 467, 311, 588, 1858, 281, 652, 732, 18875, 2531, 538, 1455, 552, 588, 2099, 420, 281, 652, 732, 18875, 588, 7802, 332, 2202, 538, 1455, 552, 588, 938, 13, 51214, 51214, 407, 538, 884, 23565, 32194, 11, 291, 434, 1936, 2710, 3319, 11, 291, 434, 15866, 257, 5893, 1674, 11, 457, 291, 434, 2710, 3319, 341, 5893, 1674, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11034252619979405, "compression_ratio": 1.922705314009662, "no_speech_prob": 0.00020011815649922937}, {"id": 128, "seek": 112300, "start": 1127.0, "end": 1129.0, "text": " Instead of L2 norm or instead of...", "tokens": [50364, 1545, 360, 291, 764, 23565, 32194, 2602, 295, 441, 17, 2026, 30, 50564, 50564, 7156, 295, 441, 17, 2026, 420, 2602, 295, 485, 50664, 50664, 1033, 11, 309, 311, 570, 291, 528, 281, 2710, 1125, 13, 50814, 50814, 467, 311, 588, 1858, 281, 652, 732, 18875, 2531, 538, 1455, 552, 588, 2099, 420, 281, 652, 732, 18875, 588, 7802, 332, 2202, 538, 1455, 552, 588, 938, 13, 51214, 51214, 407, 538, 884, 23565, 32194, 11, 291, 434, 1936, 2710, 3319, 11, 291, 434, 15866, 257, 5893, 1674, 11, 457, 291, 434, 2710, 3319, 341, 5893, 1674, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11034252619979405, "compression_ratio": 1.922705314009662, "no_speech_prob": 0.00020011815649922937}, {"id": 129, "seek": 112300, "start": 1129.0, "end": 1132.0, "text": " Okay, it's because you want to normalize.", "tokens": [50364, 1545, 360, 291, 764, 23565, 32194, 2602, 295, 441, 17, 2026, 30, 50564, 50564, 7156, 295, 441, 17, 2026, 420, 2602, 295, 485, 50664, 50664, 1033, 11, 309, 311, 570, 291, 528, 281, 2710, 1125, 13, 50814, 50814, 467, 311, 588, 1858, 281, 652, 732, 18875, 2531, 538, 1455, 552, 588, 2099, 420, 281, 652, 732, 18875, 588, 7802, 332, 2202, 538, 1455, 552, 588, 938, 13, 51214, 51214, 407, 538, 884, 23565, 32194, 11, 291, 434, 1936, 2710, 3319, 11, 291, 434, 15866, 257, 5893, 1674, 11, 457, 291, 434, 2710, 3319, 341, 5893, 1674, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11034252619979405, "compression_ratio": 1.922705314009662, "no_speech_prob": 0.00020011815649922937}, {"id": 130, "seek": 112300, "start": 1132.0, "end": 1140.0, "text": " It's very easy to make two vectors similar by making them very short or to make two vectors very dissimilar by making them very long.", "tokens": [50364, 1545, 360, 291, 764, 23565, 32194, 2602, 295, 441, 17, 2026, 30, 50564, 50564, 7156, 295, 441, 17, 2026, 420, 2602, 295, 485, 50664, 50664, 1033, 11, 309, 311, 570, 291, 528, 281, 2710, 1125, 13, 50814, 50814, 467, 311, 588, 1858, 281, 652, 732, 18875, 2531, 538, 1455, 552, 588, 2099, 420, 281, 652, 732, 18875, 588, 7802, 332, 2202, 538, 1455, 552, 588, 938, 13, 51214, 51214, 407, 538, 884, 23565, 32194, 11, 291, 434, 1936, 2710, 3319, 11, 291, 434, 15866, 257, 5893, 1674, 11, 457, 291, 434, 2710, 3319, 341, 5893, 1674, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11034252619979405, "compression_ratio": 1.922705314009662, "no_speech_prob": 0.00020011815649922937}, {"id": 131, "seek": 112300, "start": 1140.0, "end": 1148.0, "text": " So by doing cosine similarity, you're basically normalizing, you're computing a dot product, but you're normalizing this dot product.", "tokens": [50364, 1545, 360, 291, 764, 23565, 32194, 2602, 295, 441, 17, 2026, 30, 50564, 50564, 7156, 295, 441, 17, 2026, 420, 2602, 295, 485, 50664, 50664, 1033, 11, 309, 311, 570, 291, 528, 281, 2710, 1125, 13, 50814, 50814, 467, 311, 588, 1858, 281, 652, 732, 18875, 2531, 538, 1455, 552, 588, 2099, 420, 281, 652, 732, 18875, 588, 7802, 332, 2202, 538, 1455, 552, 588, 938, 13, 51214, 51214, 407, 538, 884, 23565, 32194, 11, 291, 434, 1936, 2710, 3319, 11, 291, 434, 15866, 257, 5893, 1674, 11, 457, 291, 434, 2710, 3319, 341, 5893, 1674, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11034252619979405, "compression_ratio": 1.922705314009662, "no_speech_prob": 0.00020011815649922937}, {"id": 132, "seek": 114800, "start": 1148.0, "end": 1154.0, "text": " And so you make the measure independent of the length of the vectors.", "tokens": [50364, 400, 370, 291, 652, 264, 3481, 6695, 295, 264, 4641, 295, 264, 18875, 13, 50664, 50664, 400, 370, 309, 5874, 264, 1185, 281, 733, 295, 915, 257, 665, 3827, 281, 264, 1154, 1553, 18309, 538, 445, 1455, 264, 18875, 2139, 2099, 420, 2416, 13, 51214, 51214, 467, 611, 30445, 604, 11826, 300, 727, 312, 294, 264, 1185, 13, 51414, 51414, 440, 1715, 295, 729, 8712, 488, 6828, 307, 767, 1596, 257, 857, 295, 257, 2211, 1917, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08000001789611062, "compression_ratio": 1.6090909090909091, "no_speech_prob": 6.706130807287991e-05}, {"id": 133, "seek": 114800, "start": 1154.0, "end": 1165.0, "text": " And so it forces the system to kind of find a good solution to the problem without cheating by just making the vectors either short or large.", "tokens": [50364, 400, 370, 291, 652, 264, 3481, 6695, 295, 264, 4641, 295, 264, 18875, 13, 50664, 50664, 400, 370, 309, 5874, 264, 1185, 281, 733, 295, 915, 257, 665, 3827, 281, 264, 1154, 1553, 18309, 538, 445, 1455, 264, 18875, 2139, 2099, 420, 2416, 13, 51214, 51214, 467, 611, 30445, 604, 11826, 300, 727, 312, 294, 264, 1185, 13, 51414, 51414, 440, 1715, 295, 729, 8712, 488, 6828, 307, 767, 1596, 257, 857, 295, 257, 2211, 1917, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08000001789611062, "compression_ratio": 1.6090909090909091, "no_speech_prob": 6.706130807287991e-05}, {"id": 134, "seek": 114800, "start": 1165.0, "end": 1169.0, "text": " It also removes any stability that could be in the system.", "tokens": [50364, 400, 370, 291, 652, 264, 3481, 6695, 295, 264, 4641, 295, 264, 18875, 13, 50664, 50664, 400, 370, 309, 5874, 264, 1185, 281, 733, 295, 915, 257, 665, 3827, 281, 264, 1154, 1553, 18309, 538, 445, 1455, 264, 18875, 2139, 2099, 420, 2416, 13, 51214, 51214, 467, 611, 30445, 604, 11826, 300, 727, 312, 294, 264, 1185, 13, 51414, 51414, 440, 1715, 295, 729, 8712, 488, 6828, 307, 767, 1596, 257, 857, 295, 257, 2211, 1917, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08000001789611062, "compression_ratio": 1.6090909090909091, "no_speech_prob": 6.706130807287991e-05}, {"id": 135, "seek": 114800, "start": 1169.0, "end": 1174.0, "text": " The design of those contrastive functions is actually quite a bit of a black heart.", "tokens": [50364, 400, 370, 291, 652, 264, 3481, 6695, 295, 264, 4641, 295, 264, 18875, 13, 50664, 50664, 400, 370, 309, 5874, 264, 1185, 281, 733, 295, 915, 257, 665, 3827, 281, 264, 1154, 1553, 18309, 538, 445, 1455, 264, 18875, 2139, 2099, 420, 2416, 13, 51214, 51214, 467, 611, 30445, 604, 11826, 300, 727, 312, 294, 264, 1185, 13, 51414, 51414, 440, 1715, 295, 729, 8712, 488, 6828, 307, 767, 1596, 257, 857, 295, 257, 2211, 1917, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08000001789611062, "compression_ratio": 1.6090909090909091, "no_speech_prob": 6.706130807287991e-05}, {"id": 136, "seek": 117400, "start": 1174.0, "end": 1186.0, "text": " Okay, so what they actually do in Perl is that they don't use directly the output of the conv net for the objective function.", "tokens": [50364, 1033, 11, 370, 437, 436, 767, 360, 294, 3026, 75, 307, 300, 436, 500, 380, 764, 3838, 264, 5598, 295, 264, 3754, 2533, 337, 264, 10024, 2445, 13, 50964, 50964, 814, 362, 819, 8050, 13, 51064, 51064, 407, 1936, 11, 264, 3754, 2533, 575, 257, 819, 992, 295, 8050, 11, 479, 293, 460, 11, 597, 366, 819, 337, 264, 732, 9590, 13, 51364, 51364, 400, 300, 311, 437, 436, 764, 294, 264, 4319, 295, 341, 8712, 488, 2539, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09269628180078714, "compression_ratio": 1.6582914572864322, "no_speech_prob": 5.472990596899763e-05}, {"id": 137, "seek": 117400, "start": 1186.0, "end": 1188.0, "text": " They have different heads.", "tokens": [50364, 1033, 11, 370, 437, 436, 767, 360, 294, 3026, 75, 307, 300, 436, 500, 380, 764, 3838, 264, 5598, 295, 264, 3754, 2533, 337, 264, 10024, 2445, 13, 50964, 50964, 814, 362, 819, 8050, 13, 51064, 51064, 407, 1936, 11, 264, 3754, 2533, 575, 257, 819, 992, 295, 8050, 11, 479, 293, 460, 11, 597, 366, 819, 337, 264, 732, 9590, 13, 51364, 51364, 400, 300, 311, 437, 436, 764, 294, 264, 4319, 295, 341, 8712, 488, 2539, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09269628180078714, "compression_ratio": 1.6582914572864322, "no_speech_prob": 5.472990596899763e-05}, {"id": 138, "seek": 117400, "start": 1188.0, "end": 1194.0, "text": " So basically, the conv net has a different set of heads, F and G, which are different for the two networks.", "tokens": [50364, 1033, 11, 370, 437, 436, 767, 360, 294, 3026, 75, 307, 300, 436, 500, 380, 764, 3838, 264, 5598, 295, 264, 3754, 2533, 337, 264, 10024, 2445, 13, 50964, 50964, 814, 362, 819, 8050, 13, 51064, 51064, 407, 1936, 11, 264, 3754, 2533, 575, 257, 819, 992, 295, 8050, 11, 479, 293, 460, 11, 597, 366, 819, 337, 264, 732, 9590, 13, 51364, 51364, 400, 300, 311, 437, 436, 764, 294, 264, 4319, 295, 341, 8712, 488, 2539, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09269628180078714, "compression_ratio": 1.6582914572864322, "no_speech_prob": 5.472990596899763e-05}, {"id": 139, "seek": 117400, "start": 1194.0, "end": 1199.0, "text": " And that's what they use in the context of this contrastive learning.", "tokens": [50364, 1033, 11, 370, 437, 436, 767, 360, 294, 3026, 75, 307, 300, 436, 500, 380, 764, 3838, 264, 5598, 295, 264, 3754, 2533, 337, 264, 10024, 2445, 13, 50964, 50964, 814, 362, 819, 8050, 13, 51064, 51064, 407, 1936, 11, 264, 3754, 2533, 575, 257, 819, 992, 295, 8050, 11, 479, 293, 460, 11, 597, 366, 819, 337, 264, 732, 9590, 13, 51364, 51364, 400, 300, 311, 437, 436, 764, 294, 264, 4319, 295, 341, 8712, 488, 2539, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09269628180078714, "compression_ratio": 1.6582914572864322, "no_speech_prob": 5.472990596899763e-05}, {"id": 140, "seek": 119900, "start": 1199.0, "end": 1205.0, "text": " And then there is another head that they use for the ultimate task of classification.", "tokens": [50364, 400, 550, 456, 307, 1071, 1378, 300, 436, 764, 337, 264, 9705, 5633, 295, 21538, 13, 50664, 50664, 407, 729, 479, 293, 460, 6828, 366, 11, 291, 393, 519, 295, 382, 1333, 295, 2857, 7914, 300, 366, 733, 295, 322, 1192, 295, 264, 3209, 300, 366, 819, 337, 264, 732, 9590, 13, 51164, 51164, 1057, 558, 11, 370, 613, 366, 264, 3542, 300, 366, 7126, 538, 3026, 75, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.09092608543291483, "compression_ratio": 1.618279569892473, "no_speech_prob": 2.7533244065125473e-05}, {"id": 141, "seek": 119900, "start": 1205.0, "end": 1215.0, "text": " So those F and G functions are, you can think of as sort of extra layers that are kind of on top of the network that are different for the two networks.", "tokens": [50364, 400, 550, 456, 307, 1071, 1378, 300, 436, 764, 337, 264, 9705, 5633, 295, 21538, 13, 50664, 50664, 407, 729, 479, 293, 460, 6828, 366, 11, 291, 393, 519, 295, 382, 1333, 295, 2857, 7914, 300, 366, 733, 295, 322, 1192, 295, 264, 3209, 300, 366, 819, 337, 264, 732, 9590, 13, 51164, 51164, 1057, 558, 11, 370, 613, 366, 264, 3542, 300, 366, 7126, 538, 3026, 75, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.09092608543291483, "compression_ratio": 1.618279569892473, "no_speech_prob": 2.7533244065125473e-05}, {"id": 142, "seek": 119900, "start": 1215.0, "end": 1219.0, "text": " All right, so these are the results that are produced by Perl.", "tokens": [50364, 400, 550, 456, 307, 1071, 1378, 300, 436, 764, 337, 264, 9705, 5633, 295, 21538, 13, 50664, 50664, 407, 729, 479, 293, 460, 6828, 366, 11, 291, 393, 519, 295, 382, 1333, 295, 2857, 7914, 300, 366, 733, 295, 322, 1192, 295, 264, 3209, 300, 366, 819, 337, 264, 732, 9590, 13, 51164, 51164, 1057, 558, 11, 370, 613, 366, 264, 3542, 300, 366, 7126, 538, 3026, 75, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.09092608543291483, "compression_ratio": 1.618279569892473, "no_speech_prob": 2.7533244065125473e-05}, {"id": 143, "seek": 121900, "start": 1219.0, "end": 1229.0, "text": " And you can get \u2013 so this particular experiment is one in which you pre-train the system using Perl on the ImageNet training set.", "tokens": [50364, 400, 291, 393, 483, 1662, 370, 341, 1729, 5120, 307, 472, 294, 597, 291, 659, 12, 83, 7146, 264, 1185, 1228, 3026, 75, 322, 264, 29903, 31890, 3097, 992, 13, 50864, 50864, 400, 550, 437, 291, 360, 307, 291, 1533, 7146, 11, 291, 2489, 12, 83, 2613, 264, 1185, 1228, 2139, 502, 4, 295, 264, 21335, 10938, 420, 1266, 4, 295, 264, 21335, 10938, 13, 51364, 51364, 400, 291, 3481, 264, 3389, 11, 1192, 1732, 14170, 420, 1192, 472, 14170, 13, 51564, 51564, 407, 341, 3035, 8516, 294, 7061, 322, 23507, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10683146119117737, "compression_ratio": 1.6478260869565218, "no_speech_prob": 1.3417054105957504e-05}, {"id": 144, "seek": 121900, "start": 1229.0, "end": 1239.0, "text": " And then what you do is you retrain, you fine-tune the system using either 1% of the labeled samples or 10% of the labeled samples.", "tokens": [50364, 400, 291, 393, 483, 1662, 370, 341, 1729, 5120, 307, 472, 294, 597, 291, 659, 12, 83, 7146, 264, 1185, 1228, 3026, 75, 322, 264, 29903, 31890, 3097, 992, 13, 50864, 50864, 400, 550, 437, 291, 360, 307, 291, 1533, 7146, 11, 291, 2489, 12, 83, 2613, 264, 1185, 1228, 2139, 502, 4, 295, 264, 21335, 10938, 420, 1266, 4, 295, 264, 21335, 10938, 13, 51364, 51364, 400, 291, 3481, 264, 3389, 11, 1192, 1732, 14170, 420, 1192, 472, 14170, 13, 51564, 51564, 407, 341, 3035, 8516, 294, 7061, 322, 23507, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10683146119117737, "compression_ratio": 1.6478260869565218, "no_speech_prob": 1.3417054105957504e-05}, {"id": 145, "seek": 121900, "start": 1239.0, "end": 1243.0, "text": " And you measure the performance, top five accuracy or top one accuracy.", "tokens": [50364, 400, 291, 393, 483, 1662, 370, 341, 1729, 5120, 307, 472, 294, 597, 291, 659, 12, 83, 7146, 264, 1185, 1228, 3026, 75, 322, 264, 29903, 31890, 3097, 992, 13, 50864, 50864, 400, 550, 437, 291, 360, 307, 291, 1533, 7146, 11, 291, 2489, 12, 83, 2613, 264, 1185, 1228, 2139, 502, 4, 295, 264, 21335, 10938, 420, 1266, 4, 295, 264, 21335, 10938, 13, 51364, 51364, 400, 291, 3481, 264, 3389, 11, 1192, 1732, 14170, 420, 1192, 472, 14170, 13, 51564, 51564, 407, 341, 3035, 8516, 294, 7061, 322, 23507, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10683146119117737, "compression_ratio": 1.6478260869565218, "no_speech_prob": 1.3417054105957504e-05}, {"id": 146, "seek": 121900, "start": 1243.0, "end": 1246.0, "text": " So this paper appeared in January on archive.", "tokens": [50364, 400, 291, 393, 483, 1662, 370, 341, 1729, 5120, 307, 472, 294, 597, 291, 659, 12, 83, 7146, 264, 1185, 1228, 3026, 75, 322, 264, 29903, 31890, 3097, 992, 13, 50864, 50864, 400, 550, 437, 291, 360, 307, 291, 1533, 7146, 11, 291, 2489, 12, 83, 2613, 264, 1185, 1228, 2139, 502, 4, 295, 264, 21335, 10938, 420, 1266, 4, 295, 264, 21335, 10938, 13, 51364, 51364, 400, 291, 3481, 264, 3389, 11, 1192, 1732, 14170, 420, 1192, 472, 14170, 13, 51564, 51564, 407, 341, 3035, 8516, 294, 7061, 322, 23507, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10683146119117737, "compression_ratio": 1.6478260869565218, "no_speech_prob": 1.3417054105957504e-05}, {"id": 147, "seek": 124600, "start": 1246.0, "end": 1254.0, "text": " And then just a few weeks ago, this paper appeared called SimClear by Chen and Al, which is a team from Google.", "tokens": [50364, 400, 550, 445, 257, 1326, 3259, 2057, 11, 341, 3035, 8516, 1219, 3998, 34, 5797, 538, 13682, 293, 967, 11, 597, 307, 257, 1469, 490, 3329, 13, 50764, 50764, 400, 436, 362, 257, 588, 16950, 17959, 420, 1412, 14501, 19631, 3170, 281, 8460, 2531, 15494, 13, 51214, 51214, 400, 436, 3847, 337, 257, 588, 11, 588, 938, 565, 322, 257, 688, 295, 314, 8115, 82, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09331511769975935, "compression_ratio": 1.4545454545454546, "no_speech_prob": 6.204074452398345e-05}, {"id": 148, "seek": 124600, "start": 1254.0, "end": 1263.0, "text": " And they have a very sophisticated corruption or data augmentation method to generate similar pairs.", "tokens": [50364, 400, 550, 445, 257, 1326, 3259, 2057, 11, 341, 3035, 8516, 1219, 3998, 34, 5797, 538, 13682, 293, 967, 11, 597, 307, 257, 1469, 490, 3329, 13, 50764, 50764, 400, 436, 362, 257, 588, 16950, 17959, 420, 1412, 14501, 19631, 3170, 281, 8460, 2531, 15494, 13, 51214, 51214, 400, 436, 3847, 337, 257, 588, 11, 588, 938, 565, 322, 257, 688, 295, 314, 8115, 82, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09331511769975935, "compression_ratio": 1.4545454545454546, "no_speech_prob": 6.204074452398345e-05}, {"id": 149, "seek": 124600, "start": 1263.0, "end": 1268.0, "text": " And they train for a very, very long time on a lot of TPUs.", "tokens": [50364, 400, 550, 445, 257, 1326, 3259, 2057, 11, 341, 3035, 8516, 1219, 3998, 34, 5797, 538, 13682, 293, 967, 11, 597, 307, 257, 1469, 490, 3329, 13, 50764, 50764, 400, 436, 362, 257, 588, 16950, 17959, 420, 1412, 14501, 19631, 3170, 281, 8460, 2531, 15494, 13, 51214, 51214, 400, 436, 3847, 337, 257, 588, 11, 588, 938, 565, 322, 257, 688, 295, 314, 8115, 82, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09331511769975935, "compression_ratio": 1.4545454545454546, "no_speech_prob": 6.204074452398345e-05}, {"id": 150, "seek": 126800, "start": 1268.0, "end": 1280.0, "text": " And they get really interestingly good results, so much better than either Perl or MoCo using very large models.", "tokens": [50364, 400, 436, 483, 534, 25873, 665, 3542, 11, 370, 709, 1101, 813, 2139, 3026, 75, 420, 3335, 21141, 1228, 588, 2416, 5245, 13, 50964, 50964, 400, 436, 393, 2524, 544, 813, 9562, 4, 3006, 1192, 472, 322, 29903, 31890, 538, 445, 659, 12, 17227, 1760, 294, 2698, 12, 48172, 24420, 6700, 293, 550, 733, 295, 2489, 12, 83, 37726, 365, 787, 502, 4, 295, 264, 10938, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0744548649855063, "compression_ratio": 1.4020100502512562, "no_speech_prob": 2.8847291105194017e-05}, {"id": 151, "seek": 126800, "start": 1280.0, "end": 1295.0, "text": " And they can reach more than 75% correct top one on ImageNet by just pre-training in self-supervised fashion and then kind of fine-tuning with only 1% of the samples.", "tokens": [50364, 400, 436, 483, 534, 25873, 665, 3542, 11, 370, 709, 1101, 813, 2139, 3026, 75, 420, 3335, 21141, 1228, 588, 2416, 5245, 13, 50964, 50964, 400, 436, 393, 2524, 544, 813, 9562, 4, 3006, 1192, 472, 322, 29903, 31890, 538, 445, 659, 12, 17227, 1760, 294, 2698, 12, 48172, 24420, 6700, 293, 550, 733, 295, 2489, 12, 83, 37726, 365, 787, 502, 4, 295, 264, 10938, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0744548649855063, "compression_ratio": 1.4020100502512562, "no_speech_prob": 2.8847291105194017e-05}, {"id": 152, "seek": 129500, "start": 1295.0, "end": 1306.0, "text": " Yeah, so this is, in fact, the previous slide is a different scenario where you only train a linear classifier on top of the network.", "tokens": [50364, 865, 11, 370, 341, 307, 11, 294, 1186, 11, 264, 3894, 4137, 307, 257, 819, 9005, 689, 291, 787, 3847, 257, 8213, 1508, 9902, 322, 1192, 295, 264, 3209, 13, 50914, 50914, 639, 307, 264, 9005, 689, 291, 3847, 365, 2139, 502, 4, 420, 1266, 4, 295, 21335, 10938, 13, 51164, 51164, 400, 291, 483, 14695, 4, 1192, 1732, 365, 502, 4, 295, 264, 16949, 11, 597, 307, 1238, 2243, 3542, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08056742266604774, "compression_ratio": 1.5260416666666667, "no_speech_prob": 0.00010221454431302845}, {"id": 153, "seek": 129500, "start": 1306.0, "end": 1311.0, "text": " This is the scenario where you train with either 1% or 10% of labeled samples.", "tokens": [50364, 865, 11, 370, 341, 307, 11, 294, 1186, 11, 264, 3894, 4137, 307, 257, 819, 9005, 689, 291, 787, 3847, 257, 8213, 1508, 9902, 322, 1192, 295, 264, 3209, 13, 50914, 50914, 639, 307, 264, 9005, 689, 291, 3847, 365, 2139, 502, 4, 420, 1266, 4, 295, 21335, 10938, 13, 51164, 51164, 400, 291, 483, 14695, 4, 1192, 1732, 365, 502, 4, 295, 264, 16949, 11, 597, 307, 1238, 2243, 3542, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08056742266604774, "compression_ratio": 1.5260416666666667, "no_speech_prob": 0.00010221454431302845}, {"id": 154, "seek": 129500, "start": 1311.0, "end": 1321.0, "text": " And you get 85% top five with 1% of the labels, which is pretty amazing results.", "tokens": [50364, 865, 11, 370, 341, 307, 11, 294, 1186, 11, 264, 3894, 4137, 307, 257, 819, 9005, 689, 291, 787, 3847, 257, 8213, 1508, 9902, 322, 1192, 295, 264, 3209, 13, 50914, 50914, 639, 307, 264, 9005, 689, 291, 3847, 365, 2139, 502, 4, 420, 1266, 4, 295, 21335, 10938, 13, 51164, 51164, 400, 291, 483, 14695, 4, 1192, 1732, 365, 502, 4, 295, 264, 16949, 11, 597, 307, 1238, 2243, 3542, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08056742266604774, "compression_ratio": 1.5260416666666667, "no_speech_prob": 0.00010221454431302845}, {"id": 155, "seek": 132100, "start": 1321.0, "end": 1330.0, "text": " To some extent, I think this shows the limits of contrastive methods because the amount of computation and training that is required for this is absolutely gigantic.", "tokens": [50364, 1407, 512, 8396, 11, 286, 519, 341, 3110, 264, 10406, 295, 8712, 488, 7150, 570, 264, 2372, 295, 24903, 293, 3097, 300, 307, 4739, 337, 341, 307, 3122, 26800, 13, 50814, 50814, 467, 311, 534, 11322, 13, 407, 510, 307, 257, 9005, 689, 291, 445, 3847, 257, 8213, 1508, 9902, 322, 1192, 13, 51064, 51064, 407, 291, 15959, 264, 4122, 7126, 538, 264, 1185, 300, 575, 668, 659, 12, 17227, 2001, 1228, 2698, 12, 48172, 24420, 2539, 13, 51414, 51414, 400, 550, 291, 445, 3847, 257, 8213, 1508, 9902, 322, 1192, 293, 291, 3481, 264, 3389, 11, 2139, 1192, 472, 420, 1192, 1732, 322, 264, 1577, 29903, 31890, 11, 1419, 8895, 11, 46533, 322, 264, 1577, 29903, 31890, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08805441662548034, "compression_ratio": 1.8205980066445182, "no_speech_prob": 0.00011934941721847281}, {"id": 156, "seek": 132100, "start": 1330.0, "end": 1335.0, "text": " It's really enormous. So here is a scenario where you just train a linear classifier on top.", "tokens": [50364, 1407, 512, 8396, 11, 286, 519, 341, 3110, 264, 10406, 295, 8712, 488, 7150, 570, 264, 2372, 295, 24903, 293, 3097, 300, 307, 4739, 337, 341, 307, 3122, 26800, 13, 50814, 50814, 467, 311, 534, 11322, 13, 407, 510, 307, 257, 9005, 689, 291, 445, 3847, 257, 8213, 1508, 9902, 322, 1192, 13, 51064, 51064, 407, 291, 15959, 264, 4122, 7126, 538, 264, 1185, 300, 575, 668, 659, 12, 17227, 2001, 1228, 2698, 12, 48172, 24420, 2539, 13, 51414, 51414, 400, 550, 291, 445, 3847, 257, 8213, 1508, 9902, 322, 1192, 293, 291, 3481, 264, 3389, 11, 2139, 1192, 472, 420, 1192, 1732, 322, 264, 1577, 29903, 31890, 11, 1419, 8895, 11, 46533, 322, 264, 1577, 29903, 31890, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08805441662548034, "compression_ratio": 1.8205980066445182, "no_speech_prob": 0.00011934941721847281}, {"id": 157, "seek": 132100, "start": 1335.0, "end": 1342.0, "text": " So you freeze the features produced by the system that has been pre-trained using self-supervised learning.", "tokens": [50364, 1407, 512, 8396, 11, 286, 519, 341, 3110, 264, 10406, 295, 8712, 488, 7150, 570, 264, 2372, 295, 24903, 293, 3097, 300, 307, 4739, 337, 341, 307, 3122, 26800, 13, 50814, 50814, 467, 311, 534, 11322, 13, 407, 510, 307, 257, 9005, 689, 291, 445, 3847, 257, 8213, 1508, 9902, 322, 1192, 13, 51064, 51064, 407, 291, 15959, 264, 4122, 7126, 538, 264, 1185, 300, 575, 668, 659, 12, 17227, 2001, 1228, 2698, 12, 48172, 24420, 2539, 13, 51414, 51414, 400, 550, 291, 445, 3847, 257, 8213, 1508, 9902, 322, 1192, 293, 291, 3481, 264, 3389, 11, 2139, 1192, 472, 420, 1192, 1732, 322, 264, 1577, 29903, 31890, 11, 1419, 8895, 11, 46533, 322, 264, 1577, 29903, 31890, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08805441662548034, "compression_ratio": 1.8205980066445182, "no_speech_prob": 0.00011934941721847281}, {"id": 158, "seek": 132100, "start": 1342.0, "end": 1350.0, "text": " And then you just train a linear classifier on top and you measure the performance, either top one or top five on the full ImageNet, having trained, supervised on the full ImageNet.", "tokens": [50364, 1407, 512, 8396, 11, 286, 519, 341, 3110, 264, 10406, 295, 8712, 488, 7150, 570, 264, 2372, 295, 24903, 293, 3097, 300, 307, 4739, 337, 341, 307, 3122, 26800, 13, 50814, 50814, 467, 311, 534, 11322, 13, 407, 510, 307, 257, 9005, 689, 291, 445, 3847, 257, 8213, 1508, 9902, 322, 1192, 13, 51064, 51064, 407, 291, 15959, 264, 4122, 7126, 538, 264, 1185, 300, 575, 668, 659, 12, 17227, 2001, 1228, 2698, 12, 48172, 24420, 2539, 13, 51414, 51414, 400, 550, 291, 445, 3847, 257, 8213, 1508, 9902, 322, 1192, 293, 291, 3481, 264, 3389, 11, 2139, 1192, 472, 420, 1192, 1732, 322, 264, 1577, 29903, 31890, 11, 1419, 8895, 11, 46533, 322, 264, 1577, 29903, 31890, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08805441662548034, "compression_ratio": 1.8205980066445182, "no_speech_prob": 0.00011934941721847281}, {"id": 159, "seek": 135000, "start": 1350.0, "end": 1357.0, "text": " And again, the numbers are really impressive. But again, I think it shows the limit of contrastive methods.", "tokens": [50364, 400, 797, 11, 264, 3547, 366, 534, 8992, 13, 583, 797, 11, 286, 519, 309, 3110, 264, 4948, 295, 8712, 488, 7150, 13, 50714, 50714, 1692, 307, 264, 2135, 2734, 365, 8712, 488, 7150, 307, 300, 456, 366, 867, 11, 867, 11, 867, 9253, 294, 257, 1090, 18795, 1901, 689, 291, 643, 281, 2944, 493, 264, 2281, 281, 652, 988, 300, 309, 311, 767, 2946, 5315, 813, 322, 264, 1412, 47138, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.060816946782563865, "compression_ratio": 1.6442307692307692, "no_speech_prob": 7.960254879435524e-05}, {"id": 160, "seek": 135000, "start": 1357.0, "end": 1374.0, "text": " Here is the main issue with contrastive methods is that there are many, many, many locations in a high dimensional space where you need to push up the energy to make sure that it's actually higher everywhere than on the data manifold.", "tokens": [50364, 400, 797, 11, 264, 3547, 366, 534, 8992, 13, 583, 797, 11, 286, 519, 309, 3110, 264, 4948, 295, 8712, 488, 7150, 13, 50714, 50714, 1692, 307, 264, 2135, 2734, 365, 8712, 488, 7150, 307, 300, 456, 366, 867, 11, 867, 11, 867, 9253, 294, 257, 1090, 18795, 1901, 689, 291, 643, 281, 2944, 493, 264, 2281, 281, 652, 988, 300, 309, 311, 767, 2946, 5315, 813, 322, 264, 1412, 47138, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.060816946782563865, "compression_ratio": 1.6442307692307692, "no_speech_prob": 7.960254879435524e-05}, {"id": 161, "seek": 137400, "start": 1374.0, "end": 1385.0, "text": " And so as you increase the dimension of the representation, you need more and more negative samples to make sure that the energy is higher where it needs to be higher.", "tokens": [50364, 400, 370, 382, 291, 3488, 264, 10139, 295, 264, 10290, 11, 291, 643, 544, 293, 544, 3671, 10938, 281, 652, 988, 300, 264, 2281, 307, 2946, 689, 309, 2203, 281, 312, 2946, 13, 50914, 50914, 2264, 11, 370, 718, 311, 751, 466, 1071, 1071, 9086, 295, 8712, 488, 7150, 1219, 1441, 78, 3436, 8399, 22660, 19866, 13, 51214, 51214, 400, 300, 311, 1813, 534, 733, 295, 1021, 670, 264, 1036, 1064, 293, 257, 1922, 420, 370, 337, 3303, 2856, 9007, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06071694317985984, "compression_ratio": 1.6111111111111112, "no_speech_prob": 2.8850419766968116e-05}, {"id": 162, "seek": 137400, "start": 1385.0, "end": 1391.0, "text": " OK, so let's talk about another another crop of contrastive methods called denoising autoencoder.", "tokens": [50364, 400, 370, 382, 291, 3488, 264, 10139, 295, 264, 10290, 11, 291, 643, 544, 293, 544, 3671, 10938, 281, 652, 988, 300, 264, 2281, 307, 2946, 689, 309, 2203, 281, 312, 2946, 13, 50914, 50914, 2264, 11, 370, 718, 311, 751, 466, 1071, 1071, 9086, 295, 8712, 488, 7150, 1219, 1441, 78, 3436, 8399, 22660, 19866, 13, 51214, 51214, 400, 300, 311, 1813, 534, 733, 295, 1021, 670, 264, 1036, 1064, 293, 257, 1922, 420, 370, 337, 3303, 2856, 9007, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06071694317985984, "compression_ratio": 1.6111111111111112, "no_speech_prob": 2.8850419766968116e-05}, {"id": 163, "seek": 137400, "start": 1391.0, "end": 1400.0, "text": " And that's become really kind of important over the last year and a half or so for natural language processing.", "tokens": [50364, 400, 370, 382, 291, 3488, 264, 10139, 295, 264, 10290, 11, 291, 643, 544, 293, 544, 3671, 10938, 281, 652, 988, 300, 264, 2281, 307, 2946, 689, 309, 2203, 281, 312, 2946, 13, 50914, 50914, 2264, 11, 370, 718, 311, 751, 466, 1071, 1071, 9086, 295, 8712, 488, 7150, 1219, 1441, 78, 3436, 8399, 22660, 19866, 13, 51214, 51214, 400, 300, 311, 1813, 534, 733, 295, 1021, 670, 264, 1036, 1064, 293, 257, 1922, 420, 370, 337, 3303, 2856, 9007, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06071694317985984, "compression_ratio": 1.6111111111111112, "no_speech_prob": 2.8850419766968116e-05}, {"id": 164, "seek": 140000, "start": 1400.0, "end": 1405.0, "text": " So the idea of denoising autoencoder is that you take a Y and the way you generate X is by corrupting Y.", "tokens": [50364, 407, 264, 1558, 295, 1441, 78, 3436, 8399, 22660, 19866, 307, 300, 291, 747, 257, 398, 293, 264, 636, 291, 8460, 1783, 307, 538, 17366, 278, 398, 13, 50614, 50614, 407, 341, 3263, 257, 707, 857, 411, 264, 6182, 295, 437, 321, 645, 445, 884, 365, 8712, 488, 7150, 13, 50864, 50864, 583, 1936, 291, 747, 257, 2541, 3256, 11, 398, 11, 291, 17366, 309, 294, 512, 636, 538, 12720, 257, 2522, 295, 309, 11, 337, 1365, 11, 420, 291, 747, 257, 2522, 295, 2487, 293, 291, 4159, 512, 295, 264, 2283, 420, 291, 6094, 257, 2522, 295, 309, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.06527898861811711, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.00013119516370352358}, {"id": 165, "seek": 140000, "start": 1405.0, "end": 1410.0, "text": " So this sounds a little bit like the opposite of what we were just doing with contrastive methods.", "tokens": [50364, 407, 264, 1558, 295, 1441, 78, 3436, 8399, 22660, 19866, 307, 300, 291, 747, 257, 398, 293, 264, 636, 291, 8460, 1783, 307, 538, 17366, 278, 398, 13, 50614, 50614, 407, 341, 3263, 257, 707, 857, 411, 264, 6182, 295, 437, 321, 645, 445, 884, 365, 8712, 488, 7150, 13, 50864, 50864, 583, 1936, 291, 747, 257, 2541, 3256, 11, 398, 11, 291, 17366, 309, 294, 512, 636, 538, 12720, 257, 2522, 295, 309, 11, 337, 1365, 11, 420, 291, 747, 257, 2522, 295, 2487, 293, 291, 4159, 512, 295, 264, 2283, 420, 291, 6094, 257, 2522, 295, 309, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.06527898861811711, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.00013119516370352358}, {"id": 166, "seek": 140000, "start": 1410.0, "end": 1422.0, "text": " But basically you take a clean image, Y, you corrupt it in some way by removing a piece of it, for example, or you take a piece of text and you remove some of the words or you mask a piece of it.", "tokens": [50364, 407, 264, 1558, 295, 1441, 78, 3436, 8399, 22660, 19866, 307, 300, 291, 747, 257, 398, 293, 264, 636, 291, 8460, 1783, 307, 538, 17366, 278, 398, 13, 50614, 50614, 407, 341, 3263, 257, 707, 857, 411, 264, 6182, 295, 437, 321, 645, 445, 884, 365, 8712, 488, 7150, 13, 50864, 50864, 583, 1936, 291, 747, 257, 2541, 3256, 11, 398, 11, 291, 17366, 309, 294, 512, 636, 538, 12720, 257, 2522, 295, 309, 11, 337, 1365, 11, 420, 291, 747, 257, 2522, 295, 2487, 293, 291, 4159, 512, 295, 264, 2283, 420, 291, 6094, 257, 2522, 295, 309, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.06527898861811711, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.00013119516370352358}, {"id": 167, "seek": 142200, "start": 1422.0, "end": 1430.0, "text": " So a special case is the masked autoencoder where the corruption consists in masking a subset of the input.", "tokens": [50364, 407, 257, 2121, 1389, 307, 264, 45249, 8399, 22660, 19866, 689, 264, 17959, 14689, 294, 31226, 257, 25993, 295, 264, 4846, 13, 50764, 50764, 400, 550, 291, 1190, 341, 807, 364, 8399, 22660, 19866, 11, 597, 307, 4476, 364, 2058, 19866, 420, 1219, 257, 6069, 284, 510, 11, 257, 979, 19866, 11, 293, 4317, 2572, 7914, 300, 815, 362, 2787, 41167, 294, 264, 4319, 295, 2487, 420, 406, 11, 1825, 498, 309, 311, 5267, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1339553639858584, "compression_ratio": 1.5980392156862746, "no_speech_prob": 5.306724415277131e-05}, {"id": 168, "seek": 142200, "start": 1430.0, "end": 1451.0, "text": " And then you run this through an autoencoder, which is essentially an encoder or called a predictor here, a decoder, and perhaps final layers that may have softmax in the context of text or not, nothing if it's images.", "tokens": [50364, 407, 257, 2121, 1389, 307, 264, 45249, 8399, 22660, 19866, 689, 264, 17959, 14689, 294, 31226, 257, 25993, 295, 264, 4846, 13, 50764, 50764, 400, 550, 291, 1190, 341, 807, 364, 8399, 22660, 19866, 11, 597, 307, 4476, 364, 2058, 19866, 420, 1219, 257, 6069, 284, 510, 11, 257, 979, 19866, 11, 293, 4317, 2572, 7914, 300, 815, 362, 2787, 41167, 294, 264, 4319, 295, 2487, 420, 406, 11, 1825, 498, 309, 311, 5267, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1339553639858584, "compression_ratio": 1.5980392156862746, "no_speech_prob": 5.306724415277131e-05}, {"id": 169, "seek": 145100, "start": 1451.0, "end": 1461.0, "text": " And then you compare the predicted output Y bar with the observed data Y.", "tokens": [50364, 400, 550, 291, 6794, 264, 19147, 5598, 398, 2159, 365, 264, 13095, 1412, 398, 13, 50864, 50864, 400, 370, 437, 311, 264, 8665, 295, 341, 30, 440, 8665, 295, 341, 307, 300, 307, 264, 3480, 13, 51214, 51214, 400, 291, 393, 1309, 28327, 78, 337, 729, 2238, 5242, 13, 51364, 51364, 865, 11, 321, 1866, 341, 294, 1508, 1036, 10017, 13, 51514, 51514, 663, 311, 558, 13, 407, 341, 307, 1936, 445, 257, 13548, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1364720863632009, "compression_ratio": 1.5583756345177664, "no_speech_prob": 9.309501183452085e-05}, {"id": 170, "seek": 145100, "start": 1461.0, "end": 1468.0, "text": " And so what's the principle of this? The principle of this is that is the following.", "tokens": [50364, 400, 550, 291, 6794, 264, 19147, 5598, 398, 2159, 365, 264, 13095, 1412, 398, 13, 50864, 50864, 400, 370, 437, 311, 264, 8665, 295, 341, 30, 440, 8665, 295, 341, 307, 300, 307, 264, 3480, 13, 51214, 51214, 400, 291, 393, 1309, 28327, 78, 337, 729, 2238, 5242, 13, 51364, 51364, 865, 11, 321, 1866, 341, 294, 1508, 1036, 10017, 13, 51514, 51514, 663, 311, 558, 13, 407, 341, 307, 1936, 445, 257, 13548, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1364720863632009, "compression_ratio": 1.5583756345177664, "no_speech_prob": 9.309501183452085e-05}, {"id": 171, "seek": 145100, "start": 1468.0, "end": 1471.0, "text": " And you can thank Alfredo for those beautiful pictures.", "tokens": [50364, 400, 550, 291, 6794, 264, 19147, 5598, 398, 2159, 365, 264, 13095, 1412, 398, 13, 50864, 50864, 400, 370, 437, 311, 264, 8665, 295, 341, 30, 440, 8665, 295, 341, 307, 300, 307, 264, 3480, 13, 51214, 51214, 400, 291, 393, 1309, 28327, 78, 337, 729, 2238, 5242, 13, 51364, 51364, 865, 11, 321, 1866, 341, 294, 1508, 1036, 10017, 13, 51514, 51514, 663, 311, 558, 13, 407, 341, 307, 1936, 445, 257, 13548, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1364720863632009, "compression_ratio": 1.5583756345177664, "no_speech_prob": 9.309501183452085e-05}, {"id": 172, "seek": 145100, "start": 1471.0, "end": 1474.0, "text": " Yeah, we saw this in class last Tuesday.", "tokens": [50364, 400, 550, 291, 6794, 264, 19147, 5598, 398, 2159, 365, 264, 13095, 1412, 398, 13, 50864, 50864, 400, 370, 437, 311, 264, 8665, 295, 341, 30, 440, 8665, 295, 341, 307, 300, 307, 264, 3480, 13, 51214, 51214, 400, 291, 393, 1309, 28327, 78, 337, 729, 2238, 5242, 13, 51364, 51364, 865, 11, 321, 1866, 341, 294, 1508, 1036, 10017, 13, 51514, 51514, 663, 311, 558, 13, 407, 341, 307, 1936, 445, 257, 13548, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1364720863632009, "compression_ratio": 1.5583756345177664, "no_speech_prob": 9.309501183452085e-05}, {"id": 173, "seek": 145100, "start": 1474.0, "end": 1479.0, "text": " That's right. So this is basically just a reminder.", "tokens": [50364, 400, 550, 291, 6794, 264, 19147, 5598, 398, 2159, 365, 264, 13095, 1412, 398, 13, 50864, 50864, 400, 370, 437, 311, 264, 8665, 295, 341, 30, 440, 8665, 295, 341, 307, 300, 307, 264, 3480, 13, 51214, 51214, 400, 291, 393, 1309, 28327, 78, 337, 729, 2238, 5242, 13, 51364, 51364, 865, 11, 321, 1866, 341, 294, 1508, 1036, 10017, 13, 51514, 51514, 663, 311, 558, 13, 407, 341, 307, 1936, 445, 257, 13548, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1364720863632009, "compression_ratio": 1.5583756345177664, "no_speech_prob": 9.309501183452085e-05}, {"id": 174, "seek": 147900, "start": 1479.0, "end": 1485.0, "text": " You take a data point, which is one of those pink points, right, and you corrupt it.", "tokens": [50364, 509, 747, 257, 1412, 935, 11, 597, 307, 472, 295, 729, 7022, 2793, 11, 558, 11, 293, 291, 17366, 309, 13, 50664, 50664, 407, 291, 483, 472, 295, 729, 6292, 2793, 13, 50814, 50814, 400, 550, 291, 3847, 264, 8399, 22660, 19866, 490, 264, 6292, 935, 281, 5258, 264, 7022, 2793, 11, 264, 3380, 7022, 935, 13, 51414, 51414, 708, 775, 300, 914, 30, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11214103418238022, "compression_ratio": 1.6363636363636365, "no_speech_prob": 4.832084596273489e-05}, {"id": 175, "seek": 147900, "start": 1485.0, "end": 1488.0, "text": " So you get one of those brown points.", "tokens": [50364, 509, 747, 257, 1412, 935, 11, 597, 307, 472, 295, 729, 7022, 2793, 11, 558, 11, 293, 291, 17366, 309, 13, 50664, 50664, 407, 291, 483, 472, 295, 729, 6292, 2793, 13, 50814, 50814, 400, 550, 291, 3847, 264, 8399, 22660, 19866, 490, 264, 6292, 935, 281, 5258, 264, 7022, 2793, 11, 264, 3380, 7022, 935, 13, 51414, 51414, 708, 775, 300, 914, 30, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11214103418238022, "compression_ratio": 1.6363636363636365, "no_speech_prob": 4.832084596273489e-05}, {"id": 176, "seek": 147900, "start": 1488.0, "end": 1500.0, "text": " And then you train the autoencoder from the brown point to produce the pink points, the original pink point.", "tokens": [50364, 509, 747, 257, 1412, 935, 11, 597, 307, 472, 295, 729, 7022, 2793, 11, 558, 11, 293, 291, 17366, 309, 13, 50664, 50664, 407, 291, 483, 472, 295, 729, 6292, 2793, 13, 50814, 50814, 400, 550, 291, 3847, 264, 8399, 22660, 19866, 490, 264, 6292, 935, 281, 5258, 264, 7022, 2793, 11, 264, 3380, 7022, 935, 13, 51414, 51414, 708, 775, 300, 914, 30, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11214103418238022, "compression_ratio": 1.6363636363636365, "no_speech_prob": 4.832084596273489e-05}, {"id": 177, "seek": 147900, "start": 1500.0, "end": 1501.0, "text": " What does that mean?", "tokens": [50364, 509, 747, 257, 1412, 935, 11, 597, 307, 472, 295, 729, 7022, 2793, 11, 558, 11, 293, 291, 17366, 309, 13, 50664, 50664, 407, 291, 483, 472, 295, 729, 6292, 2793, 13, 50814, 50814, 400, 550, 291, 3847, 264, 8399, 22660, 19866, 490, 264, 6292, 935, 281, 5258, 264, 7022, 2793, 11, 264, 3380, 7022, 935, 13, 51414, 51414, 708, 775, 300, 914, 30, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11214103418238022, "compression_ratio": 1.6363636363636365, "no_speech_prob": 4.832084596273489e-05}, {"id": 178, "seek": 150100, "start": 1501.0, "end": 1515.0, "text": " That means that now the energy function, which is the reconstruction error, is going to be equal to the difference between the original point and the pink point, the distance, the square distance, if C is the Euclidean square Euclidean distance.", "tokens": [50364, 663, 1355, 300, 586, 264, 2281, 2445, 11, 597, 307, 264, 31565, 6713, 11, 307, 516, 281, 312, 2681, 281, 264, 2649, 1296, 264, 3380, 935, 293, 264, 7022, 935, 11, 264, 4560, 11, 264, 3732, 4560, 11, 498, 383, 307, 264, 462, 1311, 31264, 282, 3732, 462, 1311, 31264, 282, 4560, 13, 51064, 51064, 407, 383, 295, 398, 2159, 307, 516, 281, 312, 264, 11, 498, 291, 519, 309, 311, 6108, 8895, 11, 307, 516, 281, 312, 264, 4560, 1296, 264, 39480, 935, 1783, 11, 264, 6292, 935, 11, 293, 264, 7022, 935, 291, 1409, 490, 398, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10458829564955628, "compression_ratio": 1.9634703196347032, "no_speech_prob": 5.507501100510126e-06}, {"id": 179, "seek": 150100, "start": 1515.0, "end": 1530.0, "text": " So C of Y bar is going to be the, if you think it's properly trained, is going to be the distance between the corrupted point X, the brown point, and the pink point you started from Y.", "tokens": [50364, 663, 1355, 300, 586, 264, 2281, 2445, 11, 597, 307, 264, 31565, 6713, 11, 307, 516, 281, 312, 2681, 281, 264, 2649, 1296, 264, 3380, 935, 293, 264, 7022, 935, 11, 264, 4560, 11, 264, 3732, 4560, 11, 498, 383, 307, 264, 462, 1311, 31264, 282, 3732, 462, 1311, 31264, 282, 4560, 13, 51064, 51064, 407, 383, 295, 398, 2159, 307, 516, 281, 312, 264, 11, 498, 291, 519, 309, 311, 6108, 8895, 11, 307, 516, 281, 312, 264, 4560, 1296, 264, 39480, 935, 1783, 11, 264, 6292, 935, 11, 293, 264, 7022, 935, 291, 1409, 490, 398, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10458829564955628, "compression_ratio": 1.9634703196347032, "no_speech_prob": 5.507501100510126e-06}, {"id": 180, "seek": 153000, "start": 1530.0, "end": 1541.0, "text": " So basically it basically trains the system to produce an energy function that grows quadratically as you move away from the data manifold.", "tokens": [50364, 407, 1936, 309, 1936, 16329, 264, 1185, 281, 5258, 364, 2281, 2445, 300, 13156, 10787, 4481, 984, 382, 291, 1286, 1314, 490, 264, 1412, 47138, 13, 50914, 50914], "temperature": 0.0, "avg_logprob": -0.11230400403340658, "compression_ratio": 1.3113207547169812, "no_speech_prob": 2.2123704184195958e-05}, {"id": 181, "seek": 154100, "start": 1541.0, "end": 1561.0, "text": " And so it's an example of a contrastive method because you push up on the energy of points that are outside the data manifold. Essentially you tell them your energy should be the square distance to the data manifold or at least to the point that was used before, you know, through corruption.", "tokens": [50364, 400, 370, 309, 311, 364, 1365, 295, 257, 8712, 488, 3170, 570, 291, 2944, 493, 322, 264, 2281, 295, 2793, 300, 366, 2380, 264, 1412, 47138, 13, 23596, 291, 980, 552, 428, 2281, 820, 312, 264, 3732, 4560, 281, 264, 1412, 47138, 420, 412, 1935, 281, 264, 935, 300, 390, 1143, 949, 11, 291, 458, 11, 807, 17959, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.09627084126548162, "compression_ratio": 1.5956284153005464, "no_speech_prob": 1.0450979971210472e-05}, {"id": 182, "seek": 156100, "start": 1561.0, "end": 1579.0, "text": " But the problem with it is that again, in a high dimensional continuous space, there is many, many, many ways you can corrupt a piece of data. And it's not entirely clear that you're going to be able to kind of shape the energy function the proper way by just pushing up on lots of different locations.", "tokens": [50364, 583, 264, 1154, 365, 309, 307, 300, 797, 11, 294, 257, 1090, 18795, 10957, 1901, 11, 456, 307, 867, 11, 867, 11, 867, 2098, 291, 393, 17366, 257, 2522, 295, 1412, 13, 400, 309, 311, 406, 7696, 1850, 300, 291, 434, 516, 281, 312, 1075, 281, 733, 295, 3909, 264, 2281, 2445, 264, 2296, 636, 538, 445, 7380, 493, 322, 3195, 295, 819, 9253, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.07630627397177876, "compression_ratio": 1.532994923857868, "no_speech_prob": 3.320976975373924e-05}, {"id": 183, "seek": 157900, "start": 1579.0, "end": 1593.0, "text": " It works in text because text is discrete. It doesn't work so well in images. People have used this in the context of image in painting, for example. So the corruption consists in masking a piece of the image and then training a system to reconstruct it.", "tokens": [50364, 467, 1985, 294, 2487, 570, 2487, 307, 27706, 13, 467, 1177, 380, 589, 370, 731, 294, 5267, 13, 3432, 362, 1143, 341, 294, 264, 4319, 295, 3256, 294, 5370, 11, 337, 1365, 13, 407, 264, 17959, 14689, 294, 31226, 257, 2522, 295, 264, 3256, 293, 550, 3097, 257, 1185, 281, 31499, 309, 13, 51064, 51064], "temperature": 0.0, "avg_logprob": -0.08327623835781164, "compression_ratio": 1.548780487804878, "no_speech_prob": 5.474986392073333e-05}, {"id": 184, "seek": 159300, "start": 1593.0, "end": 1611.0, "text": " And the reason why it doesn't work is because people tend to train the system without latent variables. In my little diagram here, there is a latent variable. But in fact, in versions of this that are used in the context of images, there is no real latent variable.", "tokens": [50364, 400, 264, 1778, 983, 309, 1177, 380, 589, 307, 570, 561, 3928, 281, 3847, 264, 1185, 1553, 48994, 9102, 13, 682, 452, 707, 10686, 510, 11, 456, 307, 257, 48994, 7006, 13, 583, 294, 1186, 11, 294, 9606, 295, 341, 300, 366, 1143, 294, 264, 4319, 295, 5267, 11, 456, 307, 572, 957, 48994, 7006, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.10251946449279785, "compression_ratio": 1.606060606060606, "no_speech_prob": 6.203584052855149e-05}, {"id": 185, "seek": 161100, "start": 1611.0, "end": 1626.0, "text": " And it's very difficult for the system to just dream up a single solution to the in-painting problem here. It's a multimodal manifold. I mean, it's a manifold. It's probably not just a single point.", "tokens": [50364, 400, 309, 311, 588, 2252, 337, 264, 1185, 281, 445, 3055, 493, 257, 2167, 3827, 281, 264, 294, 12, 79, 491, 783, 1154, 510, 13, 467, 311, 257, 32972, 378, 304, 47138, 13, 286, 914, 11, 309, 311, 257, 47138, 13, 467, 311, 1391, 406, 445, 257, 2167, 935, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.07012038319199174, "compression_ratio": 1.4558823529411764, "no_speech_prob": 6.9611810431524646e-06}, {"id": 186, "seek": 162600, "start": 1626.0, "end": 1641.0, "text": " There's many ways to complete the image here by filling in the masked part. And so without latent variable, the system produces blurry predictions and doesn't learn particularly good features.", "tokens": [50364, 821, 311, 867, 2098, 281, 3566, 264, 3256, 510, 538, 10623, 294, 264, 45249, 644, 13, 400, 370, 1553, 48994, 7006, 11, 264, 1185, 14725, 37644, 21264, 293, 1177, 380, 1466, 4098, 665, 4122, 13, 51114, 51114, 1692, 311, 264, 32972, 378, 304, 644, 11, 611, 264, 1778, 983, 321, 632, 300, 6920, 9656, 1859, 294, 264, 25165, 11, 570, 1184, 295, 729, 2793, 362, 732, 21264, 11, 558, 11, 294, 1296, 264, 732, 14770, 295, 264, 25165, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1106625982077725, "compression_ratio": 1.6008230452674896, "no_speech_prob": 2.318388214916922e-05}, {"id": 187, "seek": 162600, "start": 1641.0, "end": 1652.0, "text": " Here's the multimodal part, also the reason why we had that internal purple area in the spiral, because each of those points have two predictions, right, in between the two branches of the spiral.", "tokens": [50364, 821, 311, 867, 2098, 281, 3566, 264, 3256, 510, 538, 10623, 294, 264, 45249, 644, 13, 400, 370, 1553, 48994, 7006, 11, 264, 1185, 14725, 37644, 21264, 293, 1177, 380, 1466, 4098, 665, 4122, 13, 51114, 51114, 1692, 311, 264, 32972, 378, 304, 644, 11, 611, 264, 1778, 983, 321, 632, 300, 6920, 9656, 1859, 294, 264, 25165, 11, 570, 1184, 295, 729, 2793, 362, 732, 21264, 11, 558, 11, 294, 1296, 264, 732, 14770, 295, 264, 25165, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1106625982077725, "compression_ratio": 1.6008230452674896, "no_speech_prob": 2.318388214916922e-05}, {"id": 188, "seek": 165200, "start": 1652.0, "end": 1681.0, "text": " Right. So this is the additional problem that if you're not careful, the points that are in the middle that could be the result of a corruption of one pinpoint on one side of the manifold or a pinpoint on another side of the manifold, those points right in the middle don't know where to go because half the time they're trained to go to one part of the manifold, the other half of the time they're trained to go to the other part of the manifold.", "tokens": [50364, 1779, 13, 407, 341, 307, 264, 4497, 1154, 300, 498, 291, 434, 406, 5026, 11, 264, 2793, 300, 366, 294, 264, 2808, 300, 727, 312, 264, 1874, 295, 257, 17959, 295, 472, 40837, 322, 472, 1252, 295, 264, 47138, 420, 257, 40837, 322, 1071, 1252, 295, 264, 47138, 11, 729, 2793, 558, 294, 264, 2808, 500, 380, 458, 689, 281, 352, 570, 1922, 264, 565, 436, 434, 8895, 281, 352, 281, 472, 644, 295, 264, 47138, 11, 264, 661, 1922, 295, 264, 565, 436, 434, 8895, 281, 352, 281, 264, 661, 644, 295, 264, 47138, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09739871978759766, "compression_ratio": 2.118483412322275, "no_speech_prob": 6.603031215490773e-05}, {"id": 189, "seek": 168100, "start": 1681.0, "end": 1697.0, "text": " And so that might create kind of flat spots in the energy function that are not good. So there are ways to alleviate this, but they're not kind of completely worked out unless you use latent variable models.", "tokens": [50364, 400, 370, 300, 1062, 1884, 733, 295, 4962, 10681, 294, 264, 2281, 2445, 300, 366, 406, 665, 13, 407, 456, 366, 2098, 281, 42701, 341, 11, 457, 436, 434, 406, 733, 295, 2584, 2732, 484, 5969, 291, 764, 48994, 7006, 5245, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.0673024032426917, "compression_ratio": 1.417808219178082, "no_speech_prob": 4.005669688922353e-05}, {"id": 190, "seek": 169700, "start": 1697.0, "end": 1713.0, "text": " Okay, other contrasting methods. This is just in passing for your own interest. There are things like contrastive divergence and others which I'm not going to talk about. The contrastive divergence is a very simple idea.", "tokens": [50364, 1033, 11, 661, 8712, 278, 7150, 13, 639, 307, 445, 294, 8437, 337, 428, 1065, 1179, 13, 821, 366, 721, 411, 8712, 488, 47387, 293, 2357, 597, 286, 478, 406, 516, 281, 751, 466, 13, 440, 8712, 488, 47387, 307, 257, 588, 2199, 1558, 13, 51164, 51164, 509, 1888, 257, 3097, 6889, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.1877837766680801, "compression_ratio": 1.5403726708074534, "no_speech_prob": 7.965745317051187e-05}, {"id": 191, "seek": 169700, "start": 1713.0, "end": 1715.0, "text": " You pick a training sample.", "tokens": [50364, 1033, 11, 661, 8712, 278, 7150, 13, 639, 307, 445, 294, 8437, 337, 428, 1065, 1179, 13, 821, 366, 721, 411, 8712, 488, 47387, 293, 2357, 597, 286, 478, 406, 516, 281, 751, 466, 13, 440, 8712, 488, 47387, 307, 257, 588, 2199, 1558, 13, 51164, 51164, 509, 1888, 257, 3097, 6889, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.1877837766680801, "compression_ratio": 1.5403726708074534, "no_speech_prob": 7.965745317051187e-05}, {"id": 192, "seek": 171500, "start": 1715.0, "end": 1742.0, "text": " You lower the energy at that point, of course, and then from that sample, you using some sort of gradient based process you move down the energy surface with noise. So start from a sample and figure out how do I change my sample, how do I change my why in such a way that my current energy based model produces a lower energy than the one I just, I just measured for that sample.", "tokens": [50364, 509, 3126, 264, 2281, 412, 300, 935, 11, 295, 1164, 11, 293, 550, 490, 300, 6889, 11, 291, 1228, 512, 1333, 295, 16235, 2361, 1399, 291, 1286, 760, 264, 2281, 3753, 365, 5658, 13, 407, 722, 490, 257, 6889, 293, 2573, 484, 577, 360, 286, 1319, 452, 6889, 11, 577, 360, 286, 1319, 452, 983, 294, 1270, 257, 636, 300, 452, 2190, 2281, 2361, 2316, 14725, 257, 3126, 2281, 813, 264, 472, 286, 445, 11, 286, 445, 12690, 337, 300, 6889, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13708106861558073, "compression_ratio": 1.7465437788018434, "no_speech_prob": 4.637795427697711e-06}, {"id": 193, "seek": 174200, "start": 1742.0, "end": 1770.0, "text": " Okay, so basically you're trying to find another point in input space that has lower energy than the training point you just fed it. Okay, so you can think of this as kind of a smart way of corrupting a training sample smart because you, you don't randomly corrupt it, you corrupt it by basically modifying it to find a point in space that your model already gives low energy to.", "tokens": [50364, 1033, 11, 370, 1936, 291, 434, 1382, 281, 915, 1071, 935, 294, 4846, 1901, 300, 575, 3126, 2281, 813, 264, 3097, 935, 291, 445, 4636, 309, 13, 1033, 11, 370, 291, 393, 519, 295, 341, 382, 733, 295, 257, 4069, 636, 295, 17366, 278, 257, 3097, 6889, 4069, 570, 291, 11, 291, 500, 380, 16979, 17366, 309, 11, 291, 17366, 309, 538, 1936, 42626, 309, 281, 915, 257, 935, 294, 1901, 300, 428, 2316, 1217, 2709, 2295, 2281, 281, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12235139665149507, "compression_ratio": 1.7793427230046948, "no_speech_prob": 3.3212061680387706e-05}, {"id": 194, "seek": 177000, "start": 1770.0, "end": 1776.0, "text": " So it would be a point that you would want to push up because your model gives low energy to it.", "tokens": [50364, 407, 309, 576, 312, 257, 935, 300, 291, 576, 528, 281, 2944, 493, 570, 428, 2316, 2709, 2295, 2281, 281, 309, 13, 50664, 50664, 400, 291, 500, 380, 528, 309, 281, 362, 2295, 2281, 370, 291, 2944, 309, 493, 11, 293, 286, 478, 516, 281, 8304, 362, 561, 3031, 8712, 488, 7150, 365, 341, 3256, 294, 5370, 3170, 293, 577, 576, 472, 360, 300, 11, 775, 300, 534, 589, 498, 436, 498, 291, 360, 300, 1214, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1459744064896195, "compression_ratio": 1.648780487804878, "no_speech_prob": 3.590932465158403e-05}, {"id": 195, "seek": 177000, "start": 1776.0, "end": 1793.0, "text": " And you don't want it to have low energy so you push it up, and I'm going to professor have people tried contrastive methods with this image in painting method and how would one do that, does that really work if they if you do that together.", "tokens": [50364, 407, 309, 576, 312, 257, 935, 300, 291, 576, 528, 281, 2944, 493, 570, 428, 2316, 2709, 2295, 2281, 281, 309, 13, 50664, 50664, 400, 291, 500, 380, 528, 309, 281, 362, 2295, 2281, 370, 291, 2944, 309, 493, 11, 293, 286, 478, 516, 281, 8304, 362, 561, 3031, 8712, 488, 7150, 365, 341, 3256, 294, 5370, 3170, 293, 577, 576, 472, 360, 300, 11, 775, 300, 534, 589, 498, 436, 498, 291, 360, 300, 1214, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1459744064896195, "compression_ratio": 1.648780487804878, "no_speech_prob": 3.590932465158403e-05}, {"id": 196, "seek": 179300, "start": 1793.0, "end": 1803.0, "text": " So in painting is a contrastive method right you you take an image, you corrupt it by blocking some piece of it.", "tokens": [50364, 407, 294, 5370, 307, 257, 8712, 488, 3170, 558, 291, 291, 747, 364, 3256, 11, 291, 17366, 309, 538, 17776, 512, 2522, 295, 309, 13, 50864, 50864, 400, 550, 291, 3847, 257, 18161, 2533, 307, 516, 281, 352, 281, 2058, 19866, 281, 8460, 264, 1577, 3256, 11, 293, 550, 291, 6794, 341, 31565, 295, 264, 1577, 3256, 365, 264, 3380, 6219, 284, 5428, 292, 3256, 11, 293, 300, 311, 428, 2281, 2445, 13, 51614, 51614, 1033, 13, 51664, 51664, 407, 309, 307, 11, 309, 307, 257, 8712, 3170, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.20074032198998235, "compression_ratio": 1.727699530516432, "no_speech_prob": 2.7106745619676076e-05}, {"id": 197, "seek": 179300, "start": 1803.0, "end": 1818.0, "text": " And then you train a neural net is going to go to encoder to generate the full image, and then you compare this reconstruction of the full image with the original uncorrupted image, and that's your energy function.", "tokens": [50364, 407, 294, 5370, 307, 257, 8712, 488, 3170, 558, 291, 291, 747, 364, 3256, 11, 291, 17366, 309, 538, 17776, 512, 2522, 295, 309, 13, 50864, 50864, 400, 550, 291, 3847, 257, 18161, 2533, 307, 516, 281, 352, 281, 2058, 19866, 281, 8460, 264, 1577, 3256, 11, 293, 550, 291, 6794, 341, 31565, 295, 264, 1577, 3256, 365, 264, 3380, 6219, 284, 5428, 292, 3256, 11, 293, 300, 311, 428, 2281, 2445, 13, 51614, 51614, 1033, 13, 51664, 51664, 407, 309, 307, 11, 309, 307, 257, 8712, 3170, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.20074032198998235, "compression_ratio": 1.727699530516432, "no_speech_prob": 2.7106745619676076e-05}, {"id": 198, "seek": 179300, "start": 1818.0, "end": 1819.0, "text": " Okay.", "tokens": [50364, 407, 294, 5370, 307, 257, 8712, 488, 3170, 558, 291, 291, 747, 364, 3256, 11, 291, 17366, 309, 538, 17776, 512, 2522, 295, 309, 13, 50864, 50864, 400, 550, 291, 3847, 257, 18161, 2533, 307, 516, 281, 352, 281, 2058, 19866, 281, 8460, 264, 1577, 3256, 11, 293, 550, 291, 6794, 341, 31565, 295, 264, 1577, 3256, 365, 264, 3380, 6219, 284, 5428, 292, 3256, 11, 293, 300, 311, 428, 2281, 2445, 13, 51614, 51614, 1033, 13, 51664, 51664, 407, 309, 307, 11, 309, 307, 257, 8712, 3170, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.20074032198998235, "compression_ratio": 1.727699530516432, "no_speech_prob": 2.7106745619676076e-05}, {"id": 199, "seek": 179300, "start": 1819.0, "end": 1822.0, "text": " So it is, it is a contrast method.", "tokens": [50364, 407, 294, 5370, 307, 257, 8712, 488, 3170, 558, 291, 291, 747, 364, 3256, 11, 291, 17366, 309, 538, 17776, 512, 2522, 295, 309, 13, 50864, 50864, 400, 550, 291, 3847, 257, 18161, 2533, 307, 516, 281, 352, 281, 2058, 19866, 281, 8460, 264, 1577, 3256, 11, 293, 550, 291, 6794, 341, 31565, 295, 264, 1577, 3256, 365, 264, 3380, 6219, 284, 5428, 292, 3256, 11, 293, 300, 311, 428, 2281, 2445, 13, 51614, 51614, 1033, 13, 51664, 51664, 407, 309, 307, 11, 309, 307, 257, 8712, 3170, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.20074032198998235, "compression_ratio": 1.727699530516432, "no_speech_prob": 2.7106745619676076e-05}, {"id": 200, "seek": 182200, "start": 1822.0, "end": 1830.0, "text": " Right, so if we if we use like the NC loss with this in painting loss.", "tokens": [50364, 1779, 11, 370, 498, 321, 498, 321, 764, 411, 264, 20786, 4470, 365, 341, 294, 5370, 4470, 13, 50764, 50764, 4402, 300, 11, 411, 11, 307, 300, 4420, 13, 50914, 50914, 509, 393, 26225, 20786, 4470, 570, 20786, 733, 295, 30910, 322, 264, 1186, 300, 291, 362, 1333, 295, 257, 19362, 1230, 295, 3671, 10938, 13, 51464, 51464, 1033, 11, 510, 291, 1333, 295, 39905, 2270, 8460, 3671, 10938, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.139530323647164, "compression_ratio": 1.601123595505618, "no_speech_prob": 2.2825171981821768e-05}, {"id": 201, "seek": 182200, "start": 1830.0, "end": 1833.0, "text": " Does that, like, is that useful.", "tokens": [50364, 1779, 11, 370, 498, 321, 498, 321, 764, 411, 264, 20786, 4470, 365, 341, 294, 5370, 4470, 13, 50764, 50764, 4402, 300, 11, 411, 11, 307, 300, 4420, 13, 50914, 50914, 509, 393, 26225, 20786, 4470, 570, 20786, 733, 295, 30910, 322, 264, 1186, 300, 291, 362, 1333, 295, 257, 19362, 1230, 295, 3671, 10938, 13, 51464, 51464, 1033, 11, 510, 291, 1333, 295, 39905, 2270, 8460, 3671, 10938, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.139530323647164, "compression_ratio": 1.601123595505618, "no_speech_prob": 2.2825171981821768e-05}, {"id": 202, "seek": 182200, "start": 1833.0, "end": 1844.0, "text": " You can reuse NC loss because NC kind of relies on the fact that you have sort of a finite number of negative samples.", "tokens": [50364, 1779, 11, 370, 498, 321, 498, 321, 764, 411, 264, 20786, 4470, 365, 341, 294, 5370, 4470, 13, 50764, 50764, 4402, 300, 11, 411, 11, 307, 300, 4420, 13, 50914, 50914, 509, 393, 26225, 20786, 4470, 570, 20786, 733, 295, 30910, 322, 264, 1186, 300, 291, 362, 1333, 295, 257, 19362, 1230, 295, 3671, 10938, 13, 51464, 51464, 1033, 11, 510, 291, 1333, 295, 39905, 2270, 8460, 3671, 10938, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.139530323647164, "compression_ratio": 1.601123595505618, "no_speech_prob": 2.2825171981821768e-05}, {"id": 203, "seek": 182200, "start": 1844.0, "end": 1849.0, "text": " Okay, here you sort of artificially generate negative samples.", "tokens": [50364, 1779, 11, 370, 498, 321, 498, 321, 764, 411, 264, 20786, 4470, 365, 341, 294, 5370, 4470, 13, 50764, 50764, 4402, 300, 11, 411, 11, 307, 300, 4420, 13, 50914, 50914, 509, 393, 26225, 20786, 4470, 570, 20786, 733, 295, 30910, 322, 264, 1186, 300, 291, 362, 1333, 295, 257, 19362, 1230, 295, 3671, 10938, 13, 51464, 51464, 1033, 11, 510, 291, 1333, 295, 39905, 2270, 8460, 3671, 10938, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.139530323647164, "compression_ratio": 1.601123595505618, "no_speech_prob": 2.2825171981821768e-05}, {"id": 204, "seek": 184900, "start": 1849.0, "end": 1854.0, "text": " So, it's really completely different scenario I don't think you would.", "tokens": [50364, 407, 11, 309, 311, 534, 2584, 819, 9005, 286, 500, 380, 519, 291, 576, 13, 50614, 50614, 509, 727, 764, 746, 2531, 281, 20786, 13, 50814, 50814, 708, 307, 406, 294, 257, 10995, 636, 13, 51014, 51014, 1033, 11, 370, 341, 307, 983, 1901, 13, 51214, 51214, 1033, 11, 983, 983, 983, 886, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.19310550031990842, "compression_ratio": 1.3402777777777777, "no_speech_prob": 4.132568938075565e-05}, {"id": 205, "seek": 184900, "start": 1854.0, "end": 1858.0, "text": " You could use something similar to NC.", "tokens": [50364, 407, 11, 309, 311, 534, 2584, 819, 9005, 286, 500, 380, 519, 291, 576, 13, 50614, 50614, 509, 727, 764, 746, 2531, 281, 20786, 13, 50814, 50814, 708, 307, 406, 294, 257, 10995, 636, 13, 51014, 51014, 1033, 11, 370, 341, 307, 983, 1901, 13, 51214, 51214, 1033, 11, 983, 983, 983, 886, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.19310550031990842, "compression_ratio": 1.3402777777777777, "no_speech_prob": 4.132568938075565e-05}, {"id": 206, "seek": 184900, "start": 1858.0, "end": 1862.0, "text": " What is not in a meaningful way.", "tokens": [50364, 407, 11, 309, 311, 534, 2584, 819, 9005, 286, 500, 380, 519, 291, 576, 13, 50614, 50614, 509, 727, 764, 746, 2531, 281, 20786, 13, 50814, 50814, 708, 307, 406, 294, 257, 10995, 636, 13, 51014, 51014, 1033, 11, 370, 341, 307, 983, 1901, 13, 51214, 51214, 1033, 11, 983, 983, 983, 886, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.19310550031990842, "compression_ratio": 1.3402777777777777, "no_speech_prob": 4.132568938075565e-05}, {"id": 207, "seek": 184900, "start": 1862.0, "end": 1866.0, "text": " Okay, so this is why space.", "tokens": [50364, 407, 11, 309, 311, 534, 2584, 819, 9005, 286, 500, 380, 519, 291, 576, 13, 50614, 50614, 509, 727, 764, 746, 2531, 281, 20786, 13, 50814, 50814, 708, 307, 406, 294, 257, 10995, 636, 13, 51014, 51014, 1033, 11, 370, 341, 307, 983, 1901, 13, 51214, 51214, 1033, 11, 983, 983, 983, 886, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.19310550031990842, "compression_ratio": 1.3402777777777777, "no_speech_prob": 4.132568938075565e-05}, {"id": 208, "seek": 184900, "start": 1866.0, "end": 1869.0, "text": " Okay, why why why too.", "tokens": [50364, 407, 11, 309, 311, 534, 2584, 819, 9005, 286, 500, 380, 519, 291, 576, 13, 50614, 50614, 509, 727, 764, 746, 2531, 281, 20786, 13, 50814, 50814, 708, 307, 406, 294, 257, 10995, 636, 13, 51014, 51014, 1033, 11, 370, 341, 307, 983, 1901, 13, 51214, 51214, 1033, 11, 983, 983, 983, 886, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.19310550031990842, "compression_ratio": 1.3402777777777777, "no_speech_prob": 4.132568938075565e-05}, {"id": 209, "seek": 186900, "start": 1869.0, "end": 1879.0, "text": " And let's say you're, you're data manifold is something like this.", "tokens": [50364, 400, 718, 311, 584, 291, 434, 11, 291, 434, 1412, 47138, 307, 746, 411, 341, 13, 50864, 50864, 583, 718, 311, 584, 428, 2281, 2445, 4362, 307, 746, 411, 341, 370, 510, 286, 478, 6316, 264, 4458, 295, 2295, 2281, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.15010833740234375, "compression_ratio": 1.487603305785124, "no_speech_prob": 3.319184179417789e-05}, {"id": 210, "seek": 186900, "start": 1879.0, "end": 1894.0, "text": " But let's say your energy function currently is something like this so here I'm drawing the region of low energy.", "tokens": [50364, 400, 718, 311, 584, 291, 434, 11, 291, 434, 1412, 47138, 307, 746, 411, 341, 13, 50864, 50864, 583, 718, 311, 584, 428, 2281, 2445, 4362, 307, 746, 411, 341, 370, 510, 286, 478, 6316, 264, 4458, 295, 2295, 2281, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.15010833740234375, "compression_ratio": 1.487603305785124, "no_speech_prob": 3.319184179417789e-05}, {"id": 211, "seek": 189400, "start": 1894.0, "end": 1901.0, "text": " And I'm drawing the lines of equal costs.", "tokens": [50364, 400, 286, 478, 6316, 264, 3876, 295, 2681, 5497, 13, 50714, 50714, 1033, 11, 370, 264, 2281, 1542, 1481, 412, 264, 2767, 1411, 558, 291, 362, 1412, 2793, 510, 300, 428, 2316, 2709, 2295, 2281, 281, 13, 51114, 51114, 583, 550, 428, 2316, 307, 406, 665, 570, 412, 264, 2767, 558, 309, 2709, 2295, 2281, 281, 10682, 300, 362, 572, 1412, 293, 550, 412, 264, 1192, 11, 291, 362, 1412, 2793, 300, 428, 2316, 2709, 1090, 2281, 281, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12089347839355469, "compression_ratio": 1.9281767955801106, "no_speech_prob": 3.500763796182582e-06}, {"id": 212, "seek": 189400, "start": 1901.0, "end": 1909.0, "text": " Okay, so the energy looks nice at the bottom left right you have data points here that your model gives low energy to.", "tokens": [50364, 400, 286, 478, 6316, 264, 3876, 295, 2681, 5497, 13, 50714, 50714, 1033, 11, 370, 264, 2281, 1542, 1481, 412, 264, 2767, 1411, 558, 291, 362, 1412, 2793, 510, 300, 428, 2316, 2709, 2295, 2281, 281, 13, 51114, 51114, 583, 550, 428, 2316, 307, 406, 665, 570, 412, 264, 2767, 558, 309, 2709, 2295, 2281, 281, 10682, 300, 362, 572, 1412, 293, 550, 412, 264, 1192, 11, 291, 362, 1412, 2793, 300, 428, 2316, 2709, 1090, 2281, 281, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12089347839355469, "compression_ratio": 1.9281767955801106, "no_speech_prob": 3.500763796182582e-06}, {"id": 213, "seek": 189400, "start": 1909.0, "end": 1920.0, "text": " But then your model is not good because at the bottom right it gives low energy to regions that have no data and then at the top, you have data points that your model gives high energy to.", "tokens": [50364, 400, 286, 478, 6316, 264, 3876, 295, 2681, 5497, 13, 50714, 50714, 1033, 11, 370, 264, 2281, 1542, 1481, 412, 264, 2767, 1411, 558, 291, 362, 1412, 2793, 510, 300, 428, 2316, 2709, 2295, 2281, 281, 13, 51114, 51114, 583, 550, 428, 2316, 307, 406, 665, 570, 412, 264, 2767, 558, 309, 2709, 2295, 2281, 281, 10682, 300, 362, 572, 1412, 293, 550, 412, 264, 1192, 11, 291, 362, 1412, 2793, 300, 428, 2316, 2709, 1090, 2281, 281, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12089347839355469, "compression_ratio": 1.9281767955801106, "no_speech_prob": 3.500763796182582e-06}, {"id": 214, "seek": 192000, "start": 1920.0, "end": 1928.0, "text": " Okay, so here is our contrast divergence would work. You take a sample, a training sample, the test this guy.", "tokens": [50364, 1033, 11, 370, 510, 307, 527, 8712, 47387, 576, 589, 13, 509, 747, 257, 6889, 11, 257, 3097, 6889, 11, 264, 1500, 341, 2146, 13, 50764, 50764, 400, 538, 16235, 23475, 11, 291, 352, 760, 264, 2281, 3753, 281, 257, 935, 300, 575, 2295, 2281, 13, 51114, 51114, 1033, 13, 51214, 51214, 823, 11, 341, 390, 257, 3097, 6889, 983, 264, 472, 291, 12701, 586, 307, 257, 8712, 488, 6889, 636, 2159, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19525885891604733, "compression_ratio": 1.586021505376344, "no_speech_prob": 4.288803665986052e-06}, {"id": 215, "seek": 192000, "start": 1928.0, "end": 1935.0, "text": " And by gradient descent, you go down the energy surface to a point that has low energy.", "tokens": [50364, 1033, 11, 370, 510, 307, 527, 8712, 47387, 576, 589, 13, 509, 747, 257, 6889, 11, 257, 3097, 6889, 11, 264, 1500, 341, 2146, 13, 50764, 50764, 400, 538, 16235, 23475, 11, 291, 352, 760, 264, 2281, 3753, 281, 257, 935, 300, 575, 2295, 2281, 13, 51114, 51114, 1033, 13, 51214, 51214, 823, 11, 341, 390, 257, 3097, 6889, 983, 264, 472, 291, 12701, 586, 307, 257, 8712, 488, 6889, 636, 2159, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19525885891604733, "compression_ratio": 1.586021505376344, "no_speech_prob": 4.288803665986052e-06}, {"id": 216, "seek": 192000, "start": 1935.0, "end": 1937.0, "text": " Okay.", "tokens": [50364, 1033, 11, 370, 510, 307, 527, 8712, 47387, 576, 589, 13, 509, 747, 257, 6889, 11, 257, 3097, 6889, 11, 264, 1500, 341, 2146, 13, 50764, 50764, 400, 538, 16235, 23475, 11, 291, 352, 760, 264, 2281, 3753, 281, 257, 935, 300, 575, 2295, 2281, 13, 51114, 51114, 1033, 13, 51214, 51214, 823, 11, 341, 390, 257, 3097, 6889, 983, 264, 472, 291, 12701, 586, 307, 257, 8712, 488, 6889, 636, 2159, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19525885891604733, "compression_ratio": 1.586021505376344, "no_speech_prob": 4.288803665986052e-06}, {"id": 217, "seek": 192000, "start": 1937.0, "end": 1947.0, "text": " Now, this was a training sample why the one you obtain now is a contrastive sample way bar.", "tokens": [50364, 1033, 11, 370, 510, 307, 527, 8712, 47387, 576, 589, 13, 509, 747, 257, 6889, 11, 257, 3097, 6889, 11, 264, 1500, 341, 2146, 13, 50764, 50764, 400, 538, 16235, 23475, 11, 291, 352, 760, 264, 2281, 3753, 281, 257, 935, 300, 575, 2295, 2281, 13, 51114, 51114, 1033, 13, 51214, 51214, 823, 11, 341, 390, 257, 3097, 6889, 983, 264, 472, 291, 12701, 586, 307, 257, 8712, 488, 6889, 636, 2159, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19525885891604733, "compression_ratio": 1.586021505376344, "no_speech_prob": 4.288803665986052e-06}, {"id": 218, "seek": 194700, "start": 1947.0, "end": 1956.0, "text": " And what you do now is you change the parameters of your energy function so that you make the energy of why smaller and the energy of why bar larger.", "tokens": [50364, 400, 437, 291, 360, 586, 307, 291, 1319, 264, 9834, 295, 428, 2281, 2445, 370, 300, 291, 652, 264, 2281, 295, 983, 4356, 293, 264, 2281, 295, 983, 2159, 4833, 13, 50814, 50814, 1033, 11, 1228, 512, 733, 295, 4470, 2445, 300, 291, 458, 21020, 760, 322, 472, 21020, 493, 322, 264, 661, 11, 597, 1036, 2445, 764, 307, 3397, 40364, 13, 51214, 51214, 509, 445, 643, 472, 300, 486, 360, 264, 558, 551, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.12207518951802314, "compression_ratio": 1.6683417085427135, "no_speech_prob": 2.318612314411439e-05}, {"id": 219, "seek": 194700, "start": 1956.0, "end": 1964.0, "text": " Okay, using some kind of loss function that you know pushes down on one pushes up on the other, which last function use is immaterial.", "tokens": [50364, 400, 437, 291, 360, 586, 307, 291, 1319, 264, 9834, 295, 428, 2281, 2445, 370, 300, 291, 652, 264, 2281, 295, 983, 4356, 293, 264, 2281, 295, 983, 2159, 4833, 13, 50814, 50814, 1033, 11, 1228, 512, 733, 295, 4470, 2445, 300, 291, 458, 21020, 760, 322, 472, 21020, 493, 322, 264, 661, 11, 597, 1036, 2445, 764, 307, 3397, 40364, 13, 51214, 51214, 509, 445, 643, 472, 300, 486, 360, 264, 558, 551, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.12207518951802314, "compression_ratio": 1.6683417085427135, "no_speech_prob": 2.318612314411439e-05}, {"id": 220, "seek": 194700, "start": 1964.0, "end": 1968.0, "text": " You just need one that will do the right thing.", "tokens": [50364, 400, 437, 291, 360, 586, 307, 291, 1319, 264, 9834, 295, 428, 2281, 2445, 370, 300, 291, 652, 264, 2281, 295, 983, 4356, 293, 264, 2281, 295, 983, 2159, 4833, 13, 50814, 50814, 1033, 11, 1228, 512, 733, 295, 4470, 2445, 300, 291, 458, 21020, 760, 322, 472, 21020, 493, 322, 264, 661, 11, 597, 1036, 2445, 764, 307, 3397, 40364, 13, 51214, 51214, 509, 445, 643, 472, 300, 486, 360, 264, 558, 551, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.12207518951802314, "compression_ratio": 1.6683417085427135, "no_speech_prob": 2.318612314411439e-05}, {"id": 221, "seek": 196800, "start": 1968.0, "end": 1981.0, "text": " Okay, so what I've described here is kind of a deterministic version of contrast divergence but in fact contrast divergence is kind of a probabilistic version of this, where what you do is you, you do this sort of gradient based descent.", "tokens": [50364, 1033, 11, 370, 437, 286, 600, 7619, 510, 307, 733, 295, 257, 15957, 3142, 3037, 295, 8712, 47387, 457, 294, 1186, 8712, 47387, 307, 733, 295, 257, 31959, 3142, 3037, 295, 341, 11, 689, 437, 291, 360, 307, 291, 11, 291, 360, 341, 1333, 295, 16235, 2361, 23475, 13, 51014, 51014, 286, 914, 341, 1333, 295, 3164, 337, 257, 2295, 2281, 935, 11, 457, 291, 360, 309, 365, 512, 1496, 295, 4974, 1287, 4974, 1287, 512, 5658, 294, 309, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11582919529506139, "compression_ratio": 1.805, "no_speech_prob": 4.8317477194359526e-05}, {"id": 222, "seek": 196800, "start": 1981.0, "end": 1990.0, "text": " I mean this sort of search for a low energy point, but you do it with some level of randomness randomness some noise in it.", "tokens": [50364, 1033, 11, 370, 437, 286, 600, 7619, 510, 307, 733, 295, 257, 15957, 3142, 3037, 295, 8712, 47387, 457, 294, 1186, 8712, 47387, 307, 733, 295, 257, 31959, 3142, 3037, 295, 341, 11, 689, 437, 291, 360, 307, 291, 11, 291, 360, 341, 1333, 295, 16235, 2361, 23475, 13, 51014, 51014, 286, 914, 341, 1333, 295, 3164, 337, 257, 2295, 2281, 935, 11, 457, 291, 360, 309, 365, 512, 1496, 295, 4974, 1287, 4974, 1287, 512, 5658, 294, 309, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11582919529506139, "compression_ratio": 1.805, "no_speech_prob": 4.8317477194359526e-05}, {"id": 223, "seek": 199000, "start": 1990.0, "end": 2009.0, "text": " And the way to do this in a continuous space like like this one is that you give it a random kick. You think of your data point here as sort of a marble that is going to go down the energy surface you give it a random kick in some random direction, say this.", "tokens": [50364, 400, 264, 636, 281, 360, 341, 294, 257, 10957, 1901, 411, 411, 341, 472, 307, 300, 291, 976, 309, 257, 4974, 4437, 13, 509, 519, 295, 428, 1412, 935, 510, 382, 1333, 295, 257, 26844, 300, 307, 516, 281, 352, 760, 264, 2281, 3753, 291, 976, 309, 257, 4974, 4437, 294, 512, 4974, 3513, 11, 584, 341, 13, 51314, 51314, 400, 550, 291, 718, 264, 1185, 733, 295, 1524, 264, 16235, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09837089714251067, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.260619956156006e-05}, {"id": 224, "seek": 199000, "start": 2009.0, "end": 2015.0, "text": " And then you let the system kind of follow the gradient.", "tokens": [50364, 400, 264, 636, 281, 360, 341, 294, 257, 10957, 1901, 411, 411, 341, 472, 307, 300, 291, 976, 309, 257, 4974, 4437, 13, 509, 519, 295, 428, 1412, 935, 510, 382, 1333, 295, 257, 26844, 300, 307, 516, 281, 352, 760, 264, 2281, 3753, 291, 976, 309, 257, 4974, 4437, 294, 512, 4974, 3513, 11, 584, 341, 13, 51314, 51314, 400, 550, 291, 718, 264, 1185, 733, 295, 1524, 264, 16235, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09837089714251067, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.260619956156006e-05}, {"id": 225, "seek": 201500, "start": 2015.0, "end": 2025.0, "text": " And you stop when you're tired, you don't wait for it to kind of go down all the way you just stop when you're tired, and then there is a rule to select whether you keep the point or not.", "tokens": [50364, 400, 291, 1590, 562, 291, 434, 5868, 11, 291, 500, 380, 1699, 337, 309, 281, 733, 295, 352, 760, 439, 264, 636, 291, 445, 1590, 562, 291, 434, 5868, 11, 293, 550, 456, 307, 257, 4978, 281, 3048, 1968, 291, 1066, 264, 935, 420, 406, 13, 50864, 50864, 400, 300, 311, 428, 300, 311, 428, 636, 2159, 13, 51114, 51114, 1545, 307, 264, 4437, 4818, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.0733567305973598, "compression_ratio": 1.6513157894736843, "no_speech_prob": 6.500016752397642e-05}, {"id": 226, "seek": 201500, "start": 2025.0, "end": 2030.0, "text": " And that's your that's your way bar.", "tokens": [50364, 400, 291, 1590, 562, 291, 434, 5868, 11, 291, 500, 380, 1699, 337, 309, 281, 733, 295, 352, 760, 439, 264, 636, 291, 445, 1590, 562, 291, 434, 5868, 11, 293, 550, 456, 307, 257, 4978, 281, 3048, 1968, 291, 1066, 264, 935, 420, 406, 13, 50864, 50864, 400, 300, 311, 428, 300, 311, 428, 636, 2159, 13, 51114, 51114, 1545, 307, 264, 4437, 4818, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.0733567305973598, "compression_ratio": 1.6513157894736843, "no_speech_prob": 6.500016752397642e-05}, {"id": 227, "seek": 201500, "start": 2030.0, "end": 2033.0, "text": " Why is the kick necessary.", "tokens": [50364, 400, 291, 1590, 562, 291, 434, 5868, 11, 291, 500, 380, 1699, 337, 309, 281, 733, 295, 352, 760, 439, 264, 636, 291, 445, 1590, 562, 291, 434, 5868, 11, 293, 550, 456, 307, 257, 4978, 281, 3048, 1968, 291, 1066, 264, 935, 420, 406, 13, 50864, 50864, 400, 300, 311, 428, 300, 311, 428, 636, 2159, 13, 51114, 51114, 1545, 307, 264, 4437, 4818, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.0733567305973598, "compression_ratio": 1.6513157894736843, "no_speech_prob": 6.500016752397642e-05}, {"id": 228, "seek": 203300, "start": 2033.0, "end": 2045.0, "text": " Okay, so the kick is necessary so that you can go over energy barriers that would be between you and the lowest energy areas.", "tokens": [50364, 1033, 11, 370, 264, 4437, 307, 4818, 370, 300, 291, 393, 352, 670, 2281, 13565, 300, 576, 312, 1296, 291, 293, 264, 12437, 2281, 3179, 13, 50964, 50964, 1033, 11, 300, 311, 983, 291, 643, 264, 4437, 13, 51064, 51064, 823, 11, 498, 291, 362, 257, 1901, 11, 257, 2418, 1901, 300, 307, 406, 10957, 457, 307, 27706, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10564055518498497, "compression_ratio": 1.5548387096774194, "no_speech_prob": 1.7229796867468394e-05}, {"id": 229, "seek": 203300, "start": 2045.0, "end": 2047.0, "text": " Okay, that's why you need the kick.", "tokens": [50364, 1033, 11, 370, 264, 4437, 307, 4818, 370, 300, 291, 393, 352, 670, 2281, 13565, 300, 576, 312, 1296, 291, 293, 264, 12437, 2281, 3179, 13, 50964, 50964, 1033, 11, 300, 311, 983, 291, 643, 264, 4437, 13, 51064, 51064, 823, 11, 498, 291, 362, 257, 1901, 11, 257, 2418, 1901, 300, 307, 406, 10957, 457, 307, 27706, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10564055518498497, "compression_ratio": 1.5548387096774194, "no_speech_prob": 1.7229796867468394e-05}, {"id": 230, "seek": 203300, "start": 2047.0, "end": 2055.0, "text": " Now, if you have a space, a white space that is not continuous but is discrete.", "tokens": [50364, 1033, 11, 370, 264, 4437, 307, 4818, 370, 300, 291, 393, 352, 670, 2281, 13565, 300, 576, 312, 1296, 291, 293, 264, 12437, 2281, 3179, 13, 50964, 50964, 1033, 11, 300, 311, 983, 291, 643, 264, 4437, 13, 51064, 51064, 823, 11, 498, 291, 362, 257, 1901, 11, 257, 2418, 1901, 300, 307, 406, 10957, 457, 307, 27706, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10564055518498497, "compression_ratio": 1.5548387096774194, "no_speech_prob": 1.7229796867468394e-05}, {"id": 231, "seek": 205500, "start": 2055.0, "end": 2065.0, "text": " You can kind of do this energy minimization by basically doing something called simulated annealing. So, essentially, if what is discrete variable you kind of perturb it randomly.", "tokens": [50364, 509, 393, 733, 295, 360, 341, 2281, 4464, 2144, 538, 1936, 884, 746, 1219, 41713, 22256, 4270, 13, 407, 11, 4476, 11, 498, 437, 307, 27706, 7006, 291, 733, 295, 40468, 309, 16979, 13, 50864, 50864, 759, 264, 2281, 291, 483, 538, 341, 40468, 399, 307, 3126, 813, 291, 1066, 309, 498, 309, 311, 2946, 813, 291, 1066, 309, 365, 512, 8482, 11, 293, 550, 291, 1066, 884, 341, 13, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.15184170490986593, "compression_ratio": 1.6403940886699508, "no_speech_prob": 3.119999382761307e-05}, {"id": 232, "seek": 205500, "start": 2065.0, "end": 2072.0, "text": " If the energy you get by this perturbation is lower than you keep it if it's higher than you keep it with some probability, and then you keep doing this.", "tokens": [50364, 509, 393, 733, 295, 360, 341, 2281, 4464, 2144, 538, 1936, 884, 746, 1219, 41713, 22256, 4270, 13, 407, 11, 4476, 11, 498, 437, 307, 27706, 7006, 291, 733, 295, 40468, 309, 16979, 13, 50864, 50864, 759, 264, 2281, 291, 483, 538, 341, 40468, 399, 307, 3126, 813, 291, 1066, 309, 498, 309, 311, 2946, 813, 291, 1066, 309, 365, 512, 8482, 11, 293, 550, 291, 1066, 884, 341, 13, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.15184170490986593, "compression_ratio": 1.6403940886699508, "no_speech_prob": 3.119999382761307e-05}, {"id": 233, "seek": 207200, "start": 2072.0, "end": 2086.0, "text": " And eventually the energy will go down. So this is a non gradient based optimization algorithm or gradient free optimization algorithm if you want, which you kind of have to resort to when the space is discrete and you can't use gradient information.", "tokens": [50364, 400, 4728, 264, 2281, 486, 352, 760, 13, 407, 341, 307, 257, 2107, 16235, 2361, 19618, 9284, 420, 16235, 1737, 19618, 9284, 498, 291, 528, 11, 597, 291, 733, 295, 362, 281, 19606, 281, 562, 264, 1901, 307, 27706, 293, 291, 393, 380, 764, 16235, 1589, 13, 51064, 51064, 639, 6532, 286, 445, 7619, 295, 19137, 257, 26844, 293, 1333, 295, 1034, 12162, 309, 9439, 760, 264, 2281, 307, 1219, 18484, 952, 38105, 45112, 389, 45, 34, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09304835738205328, "compression_ratio": 1.615702479338843, "no_speech_prob": 4.0050901588983834e-05}, {"id": 234, "seek": 207200, "start": 2086.0, "end": 2098.0, "text": " This technique I just described of kicking a marble and sort of simulating it rolling down the energy is called Hamiltonian Monte Carlo HNC.", "tokens": [50364, 400, 4728, 264, 2281, 486, 352, 760, 13, 407, 341, 307, 257, 2107, 16235, 2361, 19618, 9284, 420, 16235, 1737, 19618, 9284, 498, 291, 528, 11, 597, 291, 733, 295, 362, 281, 19606, 281, 562, 264, 1901, 307, 27706, 293, 291, 393, 380, 764, 16235, 1589, 13, 51064, 51064, 639, 6532, 286, 445, 7619, 295, 19137, 257, 26844, 293, 1333, 295, 1034, 12162, 309, 9439, 760, 264, 2281, 307, 1219, 18484, 952, 38105, 45112, 389, 45, 34, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09304835738205328, "compression_ratio": 1.615702479338843, "no_speech_prob": 4.0050901588983834e-05}, {"id": 235, "seek": 209800, "start": 2098.0, "end": 2102.0, "text": " And you might you might see this in other in other context.", "tokens": [50364, 400, 291, 1062, 291, 1062, 536, 341, 294, 661, 294, 661, 4319, 13, 50564, 50564, 407, 300, 311, 1071, 636, 295, 17746, 3671, 10938, 13, 1079, 11, 18484, 952, 38105, 45112, 13, 2188, 561, 818, 341, 13051, 38105, 45112, 2171, 13, 50964, 50964, 407, 512, 295, 291, 815, 362, 2198, 295, 746, 1219, 20608, 30641, 266, 8379, 293, 20608, 30641, 266, 3479, 307, 364, 2281, 2361, 2316, 294, 597, 264, 2281, 307, 588, 2199, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11332813697525218, "compression_ratio": 1.7190476190476192, "no_speech_prob": 1.4737727724423166e-05}, {"id": 236, "seek": 209800, "start": 2102.0, "end": 2110.0, "text": " So that's another way of generating negative samples. Yes, Hamiltonian Monte Carlo. Some people call this hybrid Monte Carlo sometimes.", "tokens": [50364, 400, 291, 1062, 291, 1062, 536, 341, 294, 661, 294, 661, 4319, 13, 50564, 50564, 407, 300, 311, 1071, 636, 295, 17746, 3671, 10938, 13, 1079, 11, 18484, 952, 38105, 45112, 13, 2188, 561, 818, 341, 13051, 38105, 45112, 2171, 13, 50964, 50964, 407, 512, 295, 291, 815, 362, 2198, 295, 746, 1219, 20608, 30641, 266, 8379, 293, 20608, 30641, 266, 3479, 307, 364, 2281, 2361, 2316, 294, 597, 264, 2281, 307, 588, 2199, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11332813697525218, "compression_ratio": 1.7190476190476192, "no_speech_prob": 1.4737727724423166e-05}, {"id": 237, "seek": 209800, "start": 2110.0, "end": 2120.0, "text": " So some of you may have heard of something called restricted boson machines and restricted boson machine is an energy based model in which the energy is very simple.", "tokens": [50364, 400, 291, 1062, 291, 1062, 536, 341, 294, 661, 294, 661, 4319, 13, 50564, 50564, 407, 300, 311, 1071, 636, 295, 17746, 3671, 10938, 13, 1079, 11, 18484, 952, 38105, 45112, 13, 2188, 561, 818, 341, 13051, 38105, 45112, 2171, 13, 50964, 50964, 407, 512, 295, 291, 815, 362, 2198, 295, 746, 1219, 20608, 30641, 266, 8379, 293, 20608, 30641, 266, 3479, 307, 364, 2281, 2361, 2316, 294, 597, 264, 2281, 307, 588, 2199, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11332813697525218, "compression_ratio": 1.7190476190476192, "no_speech_prob": 1.4737727724423166e-05}, {"id": 238, "seek": 212000, "start": 2120.0, "end": 2128.0, "text": " It's written at the bottom here, the energy of Y and Z. So Y is basically an input data vector and Z is a latent variable.", "tokens": [50364, 467, 311, 3720, 412, 264, 2767, 510, 11, 264, 2281, 295, 398, 293, 1176, 13, 407, 398, 307, 1936, 364, 4846, 1412, 8062, 293, 1176, 307, 257, 48994, 7006, 13, 50764, 50764, 440, 2281, 2445, 307, 3175, 1176, 25167, 343, 56, 689, 343, 307, 257, 8141, 11, 406, 4725, 3732, 570, 1176, 293, 398, 815, 362, 819, 12819, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.16075240241156685, "compression_ratio": 1.4519774011299436, "no_speech_prob": 2.1109890440129675e-05}, {"id": 239, "seek": 212000, "start": 2128.0, "end": 2140.0, "text": " The energy function is minus Z transpose WY where W is a matrix, not necessarily square because Z and Y may have different dimensions.", "tokens": [50364, 467, 311, 3720, 412, 264, 2767, 510, 11, 264, 2281, 295, 398, 293, 1176, 13, 407, 398, 307, 1936, 364, 4846, 1412, 8062, 293, 1176, 307, 257, 48994, 7006, 13, 50764, 50764, 440, 2281, 2445, 307, 3175, 1176, 25167, 343, 56, 689, 343, 307, 257, 8141, 11, 406, 4725, 3732, 570, 1176, 293, 398, 815, 362, 819, 12819, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.16075240241156685, "compression_ratio": 1.4519774011299436, "no_speech_prob": 2.1109890440129675e-05}, {"id": 240, "seek": 214000, "start": 2140.0, "end": 2150.0, "text": " And generally Z and Y are both binary variables. And so I mean, binary vectors. So the components of binary variables.", "tokens": [50364, 400, 5101, 1176, 293, 398, 366, 1293, 17434, 9102, 13, 400, 370, 286, 914, 11, 17434, 18875, 13, 407, 264, 6677, 295, 17434, 9102, 13, 50864, 50864, 400, 436, 645, 733, 295, 8344, 3743, 294, 264, 2062, 8132, 82, 13, 51114, 51114, 583, 11, 291, 458, 11, 286, 478, 406, 6434, 709, 565, 322, 309, 510, 570, 436, 600, 733, 295, 7440, 484, 295, 2294, 257, 707, 857, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13665938703981165, "compression_ratio": 1.5185185185185186, "no_speech_prob": 5.173728823137935e-06}, {"id": 241, "seek": 214000, "start": 2150.0, "end": 2155.0, "text": " And they were kind of somewhat popular in the mid 2000s.", "tokens": [50364, 400, 5101, 1176, 293, 398, 366, 1293, 17434, 9102, 13, 400, 370, 286, 914, 11, 17434, 18875, 13, 407, 264, 6677, 295, 17434, 9102, 13, 50864, 50864, 400, 436, 645, 733, 295, 8344, 3743, 294, 264, 2062, 8132, 82, 13, 51114, 51114, 583, 11, 291, 458, 11, 286, 478, 406, 6434, 709, 565, 322, 309, 510, 570, 436, 600, 733, 295, 7440, 484, 295, 2294, 257, 707, 857, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13665938703981165, "compression_ratio": 1.5185185185185186, "no_speech_prob": 5.173728823137935e-06}, {"id": 242, "seek": 214000, "start": 2155.0, "end": 2163.0, "text": " But, you know, I'm not spending much time on it here because they've kind of falling out of favor a little bit.", "tokens": [50364, 400, 5101, 1176, 293, 398, 366, 1293, 17434, 9102, 13, 400, 370, 286, 914, 11, 17434, 18875, 13, 407, 264, 6677, 295, 17434, 9102, 13, 50864, 50864, 400, 436, 645, 733, 295, 8344, 3743, 294, 264, 2062, 8132, 82, 13, 51114, 51114, 583, 11, 291, 458, 11, 286, 478, 406, 6434, 709, 565, 322, 309, 510, 570, 436, 600, 733, 295, 7440, 484, 295, 2294, 257, 707, 857, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13665938703981165, "compression_ratio": 1.5185185185185186, "no_speech_prob": 5.173728823137935e-06}, {"id": 243, "seek": 216300, "start": 2163.0, "end": 2170.0, "text": " They're not they're not that popular. But just so that gives you some reference of what what this means.", "tokens": [50364, 814, 434, 406, 436, 434, 406, 300, 3743, 13, 583, 445, 370, 300, 2709, 291, 512, 6408, 295, 437, 437, 341, 1355, 13, 50714, 50714, 821, 311, 1333, 295, 44395, 6400, 295, 8712, 488, 47387, 13, 1485, 295, 552, 307, 1219, 24315, 8712, 488, 47387, 13, 50964, 50964, 400, 309, 14689, 294, 1228, 257, 3840, 295, 10007, 293, 291, 393, 1604, 264, 2535, 13, 51264, 51264, 407, 436, 362, 1333, 295, 10996, 24315, 8432, 498, 291, 528, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13748751617059474, "compression_ratio": 1.7156398104265402, "no_speech_prob": 2.6685960619943216e-05}, {"id": 244, "seek": 216300, "start": 2170.0, "end": 2175.0, "text": " There's sort of refinements of contrastive divergence. One of them is called persistent contrastive divergence.", "tokens": [50364, 814, 434, 406, 436, 434, 406, 300, 3743, 13, 583, 445, 370, 300, 2709, 291, 512, 6408, 295, 437, 437, 341, 1355, 13, 50714, 50714, 821, 311, 1333, 295, 44395, 6400, 295, 8712, 488, 47387, 13, 1485, 295, 552, 307, 1219, 24315, 8712, 488, 47387, 13, 50964, 50964, 400, 309, 14689, 294, 1228, 257, 3840, 295, 10007, 293, 291, 393, 1604, 264, 2535, 13, 51264, 51264, 407, 436, 362, 1333, 295, 10996, 24315, 8432, 498, 291, 528, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13748751617059474, "compression_ratio": 1.7156398104265402, "no_speech_prob": 2.6685960619943216e-05}, {"id": 245, "seek": 216300, "start": 2175.0, "end": 2181.0, "text": " And it consists in using a bunch of particles and you can remember the position.", "tokens": [50364, 814, 434, 406, 436, 434, 406, 300, 3743, 13, 583, 445, 370, 300, 2709, 291, 512, 6408, 295, 437, 437, 341, 1355, 13, 50714, 50714, 821, 311, 1333, 295, 44395, 6400, 295, 8712, 488, 47387, 13, 1485, 295, 552, 307, 1219, 24315, 8712, 488, 47387, 13, 50964, 50964, 400, 309, 14689, 294, 1228, 257, 3840, 295, 10007, 293, 291, 393, 1604, 264, 2535, 13, 51264, 51264, 407, 436, 362, 1333, 295, 10996, 24315, 8432, 498, 291, 528, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13748751617059474, "compression_ratio": 1.7156398104265402, "no_speech_prob": 2.6685960619943216e-05}, {"id": 246, "seek": 216300, "start": 2181.0, "end": 2187.0, "text": " So they have sort of permanent persistent positions if you want.", "tokens": [50364, 814, 434, 406, 436, 434, 406, 300, 3743, 13, 583, 445, 370, 300, 2709, 291, 512, 6408, 295, 437, 437, 341, 1355, 13, 50714, 50714, 821, 311, 1333, 295, 44395, 6400, 295, 8712, 488, 47387, 13, 1485, 295, 552, 307, 1219, 24315, 8712, 488, 47387, 13, 50964, 50964, 400, 309, 14689, 294, 1228, 257, 3840, 295, 10007, 293, 291, 393, 1604, 264, 2535, 13, 51264, 51264, 407, 436, 362, 1333, 295, 10996, 24315, 8432, 498, 291, 528, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13748751617059474, "compression_ratio": 1.7156398104265402, "no_speech_prob": 2.6685960619943216e-05}, {"id": 247, "seek": 218700, "start": 2187.0, "end": 2199.0, "text": " So you throw a bunch of marbles in your energy landscape and you keep making you keep making them roll down, maybe with a little bit of noise or kicks.", "tokens": [50364, 407, 291, 3507, 257, 3840, 295, 1849, 8806, 294, 428, 2281, 9661, 293, 291, 1066, 1455, 291, 1066, 1455, 552, 3373, 760, 11, 1310, 365, 257, 707, 857, 295, 5658, 420, 21293, 13, 50964, 50964, 400, 550, 291, 1066, 641, 2535, 370, 291, 500, 380, 1319, 264, 264, 2535, 295, 264, 26844, 4650, 281, 777, 10938, 11, 777, 3097, 10938, 13, 51314, 51314, 509, 445, 1066, 264, 1849, 8806, 689, 436, 366, 13, 400, 4728, 436, 603, 915, 2295, 2281, 3190, 294, 428, 294, 428, 2281, 3753, 293, 486, 3082, 552, 281, 312, 9152, 493, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08305705677379262, "compression_ratio": 1.7877551020408162, "no_speech_prob": 2.3551179765490815e-05}, {"id": 248, "seek": 218700, "start": 2199.0, "end": 2206.0, "text": " And then you keep their position so you don't change the the position of the marble according to new samples, new training samples.", "tokens": [50364, 407, 291, 3507, 257, 3840, 295, 1849, 8806, 294, 428, 2281, 9661, 293, 291, 1066, 1455, 291, 1066, 1455, 552, 3373, 760, 11, 1310, 365, 257, 707, 857, 295, 5658, 420, 21293, 13, 50964, 50964, 400, 550, 291, 1066, 641, 2535, 370, 291, 500, 380, 1319, 264, 264, 2535, 295, 264, 26844, 4650, 281, 777, 10938, 11, 777, 3097, 10938, 13, 51314, 51314, 509, 445, 1066, 264, 1849, 8806, 689, 436, 366, 13, 400, 4728, 436, 603, 915, 2295, 2281, 3190, 294, 428, 294, 428, 2281, 3753, 293, 486, 3082, 552, 281, 312, 9152, 493, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08305705677379262, "compression_ratio": 1.7877551020408162, "no_speech_prob": 2.3551179765490815e-05}, {"id": 249, "seek": 218700, "start": 2206.0, "end": 2215.0, "text": " You just keep the marbles where they are. And eventually they'll find low energy places in your in your energy surface and will cause them to be pushed up", "tokens": [50364, 407, 291, 3507, 257, 3840, 295, 1849, 8806, 294, 428, 2281, 9661, 293, 291, 1066, 1455, 291, 1066, 1455, 552, 3373, 760, 11, 1310, 365, 257, 707, 857, 295, 5658, 420, 21293, 13, 50964, 50964, 400, 550, 291, 1066, 641, 2535, 370, 291, 500, 380, 1319, 264, 264, 2535, 295, 264, 26844, 4650, 281, 777, 10938, 11, 777, 3097, 10938, 13, 51314, 51314, 509, 445, 1066, 264, 1849, 8806, 689, 436, 366, 13, 400, 4728, 436, 603, 915, 2295, 2281, 3190, 294, 428, 294, 428, 2281, 3753, 293, 486, 3082, 552, 281, 312, 9152, 493, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08305705677379262, "compression_ratio": 1.7877551020408162, "no_speech_prob": 2.3551179765490815e-05}, {"id": 250, "seek": 221500, "start": 2215.0, "end": 2222.0, "text": " because because that's what happens during during training.", "tokens": [50364, 570, 570, 300, 311, 437, 2314, 1830, 1830, 3097, 13, 50714, 50714, 583, 341, 1177, 380, 4373, 588, 731, 13, 407, 721, 411, 497, 18345, 82, 1813, 588, 11, 588, 5124, 281, 281, 3847, 294, 1090, 10139, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.1572896753038679, "compression_ratio": 1.3650793650793651, "no_speech_prob": 2.6272478862665594e-05}, {"id": 251, "seek": 221500, "start": 2222.0, "end": 2236.0, "text": " But this doesn't scale very well. So things like RBMs become very, very expensive to to train in high dimension.", "tokens": [50364, 570, 570, 300, 311, 437, 2314, 1830, 1830, 3097, 13, 50714, 50714, 583, 341, 1177, 380, 4373, 588, 731, 13, 407, 721, 411, 497, 18345, 82, 1813, 588, 11, 588, 5124, 281, 281, 3847, 294, 1090, 10139, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.1572896753038679, "compression_ratio": 1.3650793650793651, "no_speech_prob": 2.6272478862665594e-05}, {"id": 252, "seek": 223600, "start": 2236.0, "end": 2250.0, "text": " OK, so now for regularized latent variable energy based model, which is my current favorite type of model.", "tokens": [50364, 2264, 11, 370, 586, 337, 3890, 1602, 48994, 7006, 2281, 2361, 2316, 11, 597, 307, 452, 2190, 2954, 2010, 295, 2316, 13, 51064, 51064, 407, 321, 2825, 466, 264, 1558, 295, 2390, 257, 35521, 2316, 538, 1419, 257, 48994, 7006, 13, 51364, 51364, 1779, 13, 407, 291, 362, 264, 13095, 7006, 1783, 11, 291, 1190, 309, 281, 257, 6069, 284, 11, 309, 8947, 82, 512, 10290, 295, 264, 13095, 9102, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1706292724609375, "compression_ratio": 1.653061224489796, "no_speech_prob": 1.6962774679996073e-05}, {"id": 253, "seek": 223600, "start": 2250.0, "end": 2256.0, "text": " So we talked about the idea of building a predictive model by having a latent variable.", "tokens": [50364, 2264, 11, 370, 586, 337, 3890, 1602, 48994, 7006, 2281, 2361, 2316, 11, 597, 307, 452, 2190, 2954, 2010, 295, 2316, 13, 51064, 51064, 407, 321, 2825, 466, 264, 1558, 295, 2390, 257, 35521, 2316, 538, 1419, 257, 48994, 7006, 13, 51364, 51364, 1779, 13, 407, 291, 362, 264, 13095, 7006, 1783, 11, 291, 1190, 309, 281, 257, 6069, 284, 11, 309, 8947, 82, 512, 10290, 295, 264, 13095, 9102, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1706292724609375, "compression_ratio": 1.653061224489796, "no_speech_prob": 1.6962774679996073e-05}, {"id": 254, "seek": 223600, "start": 2256.0, "end": 2264.0, "text": " Right. So you have the observed variable X, you run it to a predictor, it extracts some representation of the observed variables.", "tokens": [50364, 2264, 11, 370, 586, 337, 3890, 1602, 48994, 7006, 2281, 2361, 2316, 11, 597, 307, 452, 2190, 2954, 2010, 295, 2316, 13, 51064, 51064, 407, 321, 2825, 466, 264, 1558, 295, 2390, 257, 35521, 2316, 538, 1419, 257, 48994, 7006, 13, 51364, 51364, 1779, 13, 407, 291, 362, 264, 13095, 7006, 1783, 11, 291, 1190, 309, 281, 257, 6069, 284, 11, 309, 8947, 82, 512, 10290, 295, 264, 13095, 9102, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1706292724609375, "compression_ratio": 1.653061224489796, "no_speech_prob": 1.6962774679996073e-05}, {"id": 255, "seek": 226400, "start": 2264.0, "end": 2267.0, "text": " And then that goes into a decoder that produces the prediction.", "tokens": [50364, 400, 550, 300, 1709, 666, 257, 979, 19866, 300, 14725, 264, 17630, 13, 50514, 50514, 583, 498, 291, 528, 428, 979, 19866, 281, 312, 1075, 281, 652, 3866, 21264, 11, 550, 291, 3154, 309, 365, 257, 48994, 7006, 13, 50764, 50764, 400, 382, 291, 10559, 264, 2158, 295, 341, 48994, 7006, 11, 264, 17630, 486, 10559, 670, 257, 992, 670, 4696, 264, 47138, 295, 1412, 295, 294, 264, 1901, 295, 398, 300, 366, 18218, 365, 1783, 13, 51464, 51464, 407, 341, 9482, 510, 11, 264, 8513, 337, 264, 2281, 393, 312, 3720, 382, 322, 264, 1411, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08158282672657687, "compression_ratio": 1.8016528925619835, "no_speech_prob": 2.5058565370272845e-05}, {"id": 256, "seek": 226400, "start": 2267.0, "end": 2272.0, "text": " But if you want your decoder to be able to make multiple predictions, then you feed it with a latent variable.", "tokens": [50364, 400, 550, 300, 1709, 666, 257, 979, 19866, 300, 14725, 264, 17630, 13, 50514, 50514, 583, 498, 291, 528, 428, 979, 19866, 281, 312, 1075, 281, 652, 3866, 21264, 11, 550, 291, 3154, 309, 365, 257, 48994, 7006, 13, 50764, 50764, 400, 382, 291, 10559, 264, 2158, 295, 341, 48994, 7006, 11, 264, 17630, 486, 10559, 670, 257, 992, 670, 4696, 264, 47138, 295, 1412, 295, 294, 264, 1901, 295, 398, 300, 366, 18218, 365, 1783, 13, 51464, 51464, 407, 341, 9482, 510, 11, 264, 8513, 337, 264, 2281, 393, 312, 3720, 382, 322, 264, 1411, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08158282672657687, "compression_ratio": 1.8016528925619835, "no_speech_prob": 2.5058565370272845e-05}, {"id": 257, "seek": 226400, "start": 2272.0, "end": 2286.0, "text": " And as you vary the value of this latent variable, the prediction will vary over a set over hopefully the manifold of data of in the space of Y that are compatible with X.", "tokens": [50364, 400, 550, 300, 1709, 666, 257, 979, 19866, 300, 14725, 264, 17630, 13, 50514, 50514, 583, 498, 291, 528, 428, 979, 19866, 281, 312, 1075, 281, 652, 3866, 21264, 11, 550, 291, 3154, 309, 365, 257, 48994, 7006, 13, 50764, 50764, 400, 382, 291, 10559, 264, 2158, 295, 341, 48994, 7006, 11, 264, 17630, 486, 10559, 670, 257, 992, 670, 4696, 264, 47138, 295, 1412, 295, 294, 264, 1901, 295, 398, 300, 366, 18218, 365, 1783, 13, 51464, 51464, 407, 341, 9482, 510, 11, 264, 8513, 337, 264, 2281, 393, 312, 3720, 382, 322, 264, 1411, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08158282672657687, "compression_ratio": 1.8016528925619835, "no_speech_prob": 2.5058565370272845e-05}, {"id": 258, "seek": 226400, "start": 2286.0, "end": 2292.0, "text": " So this architecture here, the formula for the energy can be written as on the left here.", "tokens": [50364, 400, 550, 300, 1709, 666, 257, 979, 19866, 300, 14725, 264, 17630, 13, 50514, 50514, 583, 498, 291, 528, 428, 979, 19866, 281, 312, 1075, 281, 652, 3866, 21264, 11, 550, 291, 3154, 309, 365, 257, 48994, 7006, 13, 50764, 50764, 400, 382, 291, 10559, 264, 2158, 295, 341, 48994, 7006, 11, 264, 17630, 486, 10559, 670, 257, 992, 670, 4696, 264, 47138, 295, 1412, 295, 294, 264, 1901, 295, 398, 300, 366, 18218, 365, 1783, 13, 51464, 51464, 407, 341, 9482, 510, 11, 264, 8513, 337, 264, 2281, 393, 312, 3720, 382, 322, 264, 1411, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08158282672657687, "compression_ratio": 1.8016528925619835, "no_speech_prob": 2.5058565370272845e-05}, {"id": 259, "seek": 229200, "start": 2292.0, "end": 2299.0, "text": " So you have C of Y and C is a cost function that compares these two arguments.", "tokens": [50364, 407, 291, 362, 383, 295, 398, 293, 383, 307, 257, 2063, 2445, 300, 38334, 613, 732, 12869, 13, 50714, 50714, 407, 291, 6794, 398, 11, 264, 1412, 8062, 11, 365, 264, 1874, 295, 9275, 264, 979, 19866, 281, 264, 5598, 295, 264, 6069, 284, 300, 2516, 666, 2696, 1783, 293, 264, 979, 19866, 611, 2516, 666, 2696, 1176, 13, 51364, 51364, 407, 510, 307, 264, 1154, 365, 341, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.17623969300152503, "compression_ratio": 1.6686046511627908, "no_speech_prob": 5.306736784405075e-05}, {"id": 260, "seek": 229200, "start": 2299.0, "end": 2312.0, "text": " So you compare Y, the data vector, with the result of applying the decoder to the output of the predictor that takes into account X and the decoder also takes into account Z.", "tokens": [50364, 407, 291, 362, 383, 295, 398, 293, 383, 307, 257, 2063, 2445, 300, 38334, 613, 732, 12869, 13, 50714, 50714, 407, 291, 6794, 398, 11, 264, 1412, 8062, 11, 365, 264, 1874, 295, 9275, 264, 979, 19866, 281, 264, 5598, 295, 264, 6069, 284, 300, 2516, 666, 2696, 1783, 293, 264, 979, 19866, 611, 2516, 666, 2696, 1176, 13, 51364, 51364, 407, 510, 307, 264, 1154, 365, 341, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.17623969300152503, "compression_ratio": 1.6686046511627908, "no_speech_prob": 5.306736784405075e-05}, {"id": 261, "seek": 229200, "start": 2312.0, "end": 2314.0, "text": " So here is the problem with this.", "tokens": [50364, 407, 291, 362, 383, 295, 398, 293, 383, 307, 257, 2063, 2445, 300, 38334, 613, 732, 12869, 13, 50714, 50714, 407, 291, 6794, 398, 11, 264, 1412, 8062, 11, 365, 264, 1874, 295, 9275, 264, 979, 19866, 281, 264, 5598, 295, 264, 6069, 284, 300, 2516, 666, 2696, 1783, 293, 264, 979, 19866, 611, 2516, 666, 2696, 1176, 13, 51364, 51364, 407, 510, 307, 264, 1154, 365, 341, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.17623969300152503, "compression_ratio": 1.6686046511627908, "no_speech_prob": 5.306736784405075e-05}, {"id": 262, "seek": 231400, "start": 2314.0, "end": 2326.0, "text": " If Z is too powerful, in other words, if Z has too much capacity, then there are always going to be a Z that is going to produce a Y bar that's going to be exactly equal to Y.", "tokens": [50364, 759, 1176, 307, 886, 4005, 11, 294, 661, 2283, 11, 498, 1176, 575, 886, 709, 6042, 11, 550, 456, 366, 1009, 516, 281, 312, 257, 1176, 300, 307, 516, 281, 5258, 257, 398, 2159, 300, 311, 516, 281, 312, 2293, 2681, 281, 398, 13, 50964, 50964, 407, 1604, 264, 38253, 9284, 510, 307, 300, 291, 976, 364, 1783, 293, 257, 398, 293, 550, 291, 915, 257, 1176, 300, 4464, 5660, 383, 295, 398, 11, 398, 2159, 13, 51414, 51414, 1779, 13, 663, 311, 577, 291, 360, 38253, 295, 264, 48994, 7006, 294, 364, 2281, 2361, 2316, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09242174885060528, "compression_ratio": 1.646551724137931, "no_speech_prob": 8.139522833516821e-06}, {"id": 263, "seek": 231400, "start": 2326.0, "end": 2335.0, "text": " So remember the inference algorithm here is that you give an X and a Y and then you find a Z that minimizes C of Y, Y bar.", "tokens": [50364, 759, 1176, 307, 886, 4005, 11, 294, 661, 2283, 11, 498, 1176, 575, 886, 709, 6042, 11, 550, 456, 366, 1009, 516, 281, 312, 257, 1176, 300, 307, 516, 281, 5258, 257, 398, 2159, 300, 311, 516, 281, 312, 2293, 2681, 281, 398, 13, 50964, 50964, 407, 1604, 264, 38253, 9284, 510, 307, 300, 291, 976, 364, 1783, 293, 257, 398, 293, 550, 291, 915, 257, 1176, 300, 4464, 5660, 383, 295, 398, 11, 398, 2159, 13, 51414, 51414, 1779, 13, 663, 311, 577, 291, 360, 38253, 295, 264, 48994, 7006, 294, 364, 2281, 2361, 2316, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09242174885060528, "compression_ratio": 1.646551724137931, "no_speech_prob": 8.139522833516821e-06}, {"id": 264, "seek": 231400, "start": 2335.0, "end": 2341.0, "text": " Right. That's how you do inference of the latent variable in an energy based model.", "tokens": [50364, 759, 1176, 307, 886, 4005, 11, 294, 661, 2283, 11, 498, 1176, 575, 886, 709, 6042, 11, 550, 456, 366, 1009, 516, 281, 312, 257, 1176, 300, 307, 516, 281, 5258, 257, 398, 2159, 300, 311, 516, 281, 312, 2293, 2681, 281, 398, 13, 50964, 50964, 407, 1604, 264, 38253, 9284, 510, 307, 300, 291, 976, 364, 1783, 293, 257, 398, 293, 550, 291, 915, 257, 1176, 300, 4464, 5660, 383, 295, 398, 11, 398, 2159, 13, 51414, 51414, 1779, 13, 663, 311, 577, 291, 360, 38253, 295, 264, 48994, 7006, 294, 364, 2281, 2361, 2316, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09242174885060528, "compression_ratio": 1.646551724137931, "no_speech_prob": 8.139522833516821e-06}, {"id": 265, "seek": 234100, "start": 2341.0, "end": 2346.0, "text": " Right. You get an X and a Y, find a Z that minimizes the energy.", "tokens": [50364, 1779, 13, 509, 483, 364, 1783, 293, 257, 398, 11, 915, 257, 1176, 300, 4464, 5660, 264, 2281, 13, 50614, 50614, 407, 498, 1176, 11, 337, 1365, 11, 575, 264, 912, 10139, 382, 398, 293, 264, 979, 19866, 307, 4005, 1547, 281, 2906, 264, 6575, 2445, 11, 550, 337, 604, 398, 11, 456, 311, 1009, 516, 281, 312, 257, 1176, 300, 14725, 398, 2159, 300, 311, 2293, 2681, 281, 398, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.12430672963460286, "compression_ratio": 1.4840425531914894, "no_speech_prob": 4.399811223265715e-05}, {"id": 266, "seek": 234100, "start": 2346.0, "end": 2362.0, "text": " So if Z, for example, has the same dimension as Y and the decoder is powerful enough to represent the identity function, then for any Y, there's always going to be a Z that produces Y bar that's exactly equal to Y.", "tokens": [50364, 1779, 13, 509, 483, 364, 1783, 293, 257, 398, 11, 915, 257, 1176, 300, 4464, 5660, 264, 2281, 13, 50614, 50614, 407, 498, 1176, 11, 337, 1365, 11, 575, 264, 912, 10139, 382, 398, 293, 264, 979, 19866, 307, 4005, 1547, 281, 2906, 264, 6575, 2445, 11, 550, 337, 604, 398, 11, 456, 311, 1009, 516, 281, 312, 257, 1176, 300, 14725, 398, 2159, 300, 311, 2293, 2681, 281, 398, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.12430672963460286, "compression_ratio": 1.4840425531914894, "no_speech_prob": 4.399811223265715e-05}, {"id": 267, "seek": 236200, "start": 2362.0, "end": 2377.0, "text": " Okay. And if the decoder is the identity function, which ignores H, the identity function from Z to Y, to Y bar, then you just set Z equal to Y and the energy is zero.", "tokens": [50364, 1033, 13, 400, 498, 264, 979, 19866, 307, 264, 6575, 2445, 11, 597, 5335, 2706, 389, 11, 264, 6575, 2445, 490, 1176, 281, 398, 11, 281, 398, 2159, 11, 550, 291, 445, 992, 1176, 2681, 281, 398, 293, 264, 2281, 307, 4018, 13, 51114, 51114, 400, 300, 576, 312, 257, 6237, 2281, 2361, 2316, 570, 309, 576, 406, 976, 1090, 2281, 281, 1507, 2380, 264, 47138, 295, 1412, 13, 51464, 51464, 467, 2709, 2295, 2281, 281, 1203, 13, 1033, 13, 467, 2709, 4018, 2281, 281, 1203, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08752494329934592, "compression_ratio": 1.7571428571428571, "no_speech_prob": 1.221818911290029e-05}, {"id": 268, "seek": 236200, "start": 2377.0, "end": 2384.0, "text": " And that would be a terrible energy based model because it would not give high energy to stuff outside the manifold of data.", "tokens": [50364, 1033, 13, 400, 498, 264, 979, 19866, 307, 264, 6575, 2445, 11, 597, 5335, 2706, 389, 11, 264, 6575, 2445, 490, 1176, 281, 398, 11, 281, 398, 2159, 11, 550, 291, 445, 992, 1176, 2681, 281, 398, 293, 264, 2281, 307, 4018, 13, 51114, 51114, 400, 300, 576, 312, 257, 6237, 2281, 2361, 2316, 570, 309, 576, 406, 976, 1090, 2281, 281, 1507, 2380, 264, 47138, 295, 1412, 13, 51464, 51464, 467, 2709, 2295, 2281, 281, 1203, 13, 1033, 13, 467, 2709, 4018, 2281, 281, 1203, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08752494329934592, "compression_ratio": 1.7571428571428571, "no_speech_prob": 1.221818911290029e-05}, {"id": 269, "seek": 236200, "start": 2384.0, "end": 2390.0, "text": " It gives low energy to everything. Okay. It gives zero energy to everything.", "tokens": [50364, 1033, 13, 400, 498, 264, 979, 19866, 307, 264, 6575, 2445, 11, 597, 5335, 2706, 389, 11, 264, 6575, 2445, 490, 1176, 281, 398, 11, 281, 398, 2159, 11, 550, 291, 445, 992, 1176, 2681, 281, 398, 293, 264, 2281, 307, 4018, 13, 51114, 51114, 400, 300, 576, 312, 257, 6237, 2281, 2361, 2316, 570, 309, 576, 406, 976, 1090, 2281, 281, 1507, 2380, 264, 47138, 295, 1412, 13, 51464, 51464, 467, 2709, 2295, 2281, 281, 1203, 13, 1033, 13, 467, 2709, 4018, 2281, 281, 1203, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08752494329934592, "compression_ratio": 1.7571428571428571, "no_speech_prob": 1.221818911290029e-05}, {"id": 270, "seek": 239000, "start": 2390.0, "end": 2403.0, "text": " So the way to prevent the system from giving low energy to points outside the manifold of data is to limit the information capacity of the latent variable Z.", "tokens": [50364, 407, 264, 636, 281, 4871, 264, 1185, 490, 2902, 2295, 2281, 281, 2793, 2380, 264, 47138, 295, 1412, 307, 281, 4948, 264, 1589, 6042, 295, 264, 48994, 7006, 1176, 13, 51014, 51014, 1407, 312, 544, 13600, 11, 498, 1176, 393, 787, 747, 11, 718, 311, 584, 11, 1266, 819, 4190, 11, 437, 300, 1355, 307, 11, 370, 291, 1817, 7146, 1176, 281, 787, 747, 1266, 1944, 819, 4190, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06291933582253652, "compression_ratio": 1.5707070707070707, "no_speech_prob": 1.1476800864329562e-05}, {"id": 271, "seek": 239000, "start": 2403.0, "end": 2415.0, "text": " To be more precise, if Z can only take, let's say, 10 different values, what that means is, so you constrain Z to only take 10 possible different values.", "tokens": [50364, 407, 264, 636, 281, 4871, 264, 1185, 490, 2902, 2295, 2281, 281, 2793, 2380, 264, 47138, 295, 1412, 307, 281, 4948, 264, 1589, 6042, 295, 264, 48994, 7006, 1176, 13, 51014, 51014, 1407, 312, 544, 13600, 11, 498, 1176, 393, 787, 747, 11, 718, 311, 584, 11, 1266, 819, 4190, 11, 437, 300, 1355, 307, 11, 370, 291, 1817, 7146, 1176, 281, 787, 747, 1266, 1944, 819, 4190, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06291933582253652, "compression_ratio": 1.5707070707070707, "no_speech_prob": 1.1476800864329562e-05}, {"id": 272, "seek": 241500, "start": 2415.0, "end": 2420.0, "text": " Let's say you make Z a one hot vector of dimension 10, like in K-means.", "tokens": [50364, 961, 311, 584, 291, 652, 1176, 257, 472, 2368, 8062, 295, 10139, 1266, 11, 411, 294, 591, 12, 1398, 599, 13, 50614, 50614, 1033, 13, 1396, 456, 311, 787, 516, 281, 312, 1266, 2793, 294, 398, 1901, 300, 486, 362, 4018, 2281, 570, 2139, 398, 307, 2681, 281, 472, 295, 264, 398, 10228, 300, 307, 7126, 490, 472, 295, 729, 1266, 1176, 82, 420, 309, 311, 406, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.10677256186803182, "compression_ratio": 1.4065934065934067, "no_speech_prob": 3.5008183658646885e-06}, {"id": 273, "seek": 241500, "start": 2420.0, "end": 2435.0, "text": " Okay. Then there's only going to be 10 points in Y space that will have zero energy because either Y is equal to one of the Y bars that is produced from one of those 10 Zs or it's not.", "tokens": [50364, 961, 311, 584, 291, 652, 1176, 257, 472, 2368, 8062, 295, 10139, 1266, 11, 411, 294, 591, 12, 1398, 599, 13, 50614, 50614, 1033, 13, 1396, 456, 311, 787, 516, 281, 312, 1266, 2793, 294, 398, 1901, 300, 486, 362, 4018, 2281, 570, 2139, 398, 307, 2681, 281, 472, 295, 264, 398, 10228, 300, 307, 7126, 490, 472, 295, 729, 1266, 1176, 82, 420, 309, 311, 406, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.10677256186803182, "compression_ratio": 1.4065934065934067, "no_speech_prob": 3.5008183658646885e-06}, {"id": 274, "seek": 243500, "start": 2435.0, "end": 2445.0, "text": " If it is, then the energy is zero. If it's not, the energy is going to have to be larger than zero. In fact, it's going to grow quadratically as you move away from that Z.", "tokens": [50364, 759, 309, 307, 11, 550, 264, 2281, 307, 4018, 13, 759, 309, 311, 406, 11, 264, 2281, 307, 516, 281, 362, 281, 312, 4833, 813, 4018, 13, 682, 1186, 11, 309, 311, 516, 281, 1852, 10787, 4481, 984, 382, 291, 1286, 1314, 490, 300, 1176, 13, 50864, 50864, 400, 300, 311, 2293, 264, 1558, 295, 591, 12, 1398, 599, 13, 1033, 13, 51064, 51064, 583, 437, 498, 291, 915, 661, 2098, 281, 4948, 264, 1589, 2701, 295, 1176, 30, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.057988999837852384, "compression_ratio": 1.5157894736842106, "no_speech_prob": 8.397755664191209e-06}, {"id": 275, "seek": 243500, "start": 2445.0, "end": 2449.0, "text": " And that's exactly the idea of K-means. Okay.", "tokens": [50364, 759, 309, 307, 11, 550, 264, 2281, 307, 4018, 13, 759, 309, 311, 406, 11, 264, 2281, 307, 516, 281, 362, 281, 312, 4833, 813, 4018, 13, 682, 1186, 11, 309, 311, 516, 281, 1852, 10787, 4481, 984, 382, 291, 1286, 1314, 490, 300, 1176, 13, 50864, 50864, 400, 300, 311, 2293, 264, 1558, 295, 591, 12, 1398, 599, 13, 1033, 13, 51064, 51064, 583, 437, 498, 291, 915, 661, 2098, 281, 4948, 264, 1589, 2701, 295, 1176, 30, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.057988999837852384, "compression_ratio": 1.5157894736842106, "no_speech_prob": 8.397755664191209e-06}, {"id": 276, "seek": 243500, "start": 2449.0, "end": 2453.0, "text": " But what if you find other ways to limit the information content of Z?", "tokens": [50364, 759, 309, 307, 11, 550, 264, 2281, 307, 4018, 13, 759, 309, 311, 406, 11, 264, 2281, 307, 516, 281, 362, 281, 312, 4833, 813, 4018, 13, 682, 1186, 11, 309, 311, 516, 281, 1852, 10787, 4481, 984, 382, 291, 1286, 1314, 490, 300, 1176, 13, 50864, 50864, 400, 300, 311, 2293, 264, 1558, 295, 591, 12, 1398, 599, 13, 1033, 13, 51064, 51064, 583, 437, 498, 291, 915, 661, 2098, 281, 4948, 264, 1589, 2701, 295, 1176, 30, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.057988999837852384, "compression_ratio": 1.5157894736842106, "no_speech_prob": 8.397755664191209e-06}, {"id": 277, "seek": 245300, "start": 2453.0, "end": 2471.0, "text": " So this seems like a kind of a small technical sub-problem, but in my opinion, the question of how you limit the information content of a latent variable in a model of this type is the most important question in AI today.", "tokens": [50364, 407, 341, 2544, 411, 257, 733, 295, 257, 1359, 6191, 1422, 12, 47419, 11, 457, 294, 452, 4800, 11, 264, 1168, 295, 577, 291, 4948, 264, 1589, 2701, 295, 257, 48994, 7006, 294, 257, 2316, 295, 341, 2010, 307, 264, 881, 1021, 1168, 294, 7318, 965, 13, 51264, 51264, 1033, 13, 400, 286, 478, 406, 9287, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.09897443896434346, "compression_ratio": 1.4335260115606936, "no_speech_prob": 6.239666163310176e-06}, {"id": 278, "seek": 245300, "start": 2471.0, "end": 2474.0, "text": " Okay. And I'm not kidding.", "tokens": [50364, 407, 341, 2544, 411, 257, 733, 295, 257, 1359, 6191, 1422, 12, 47419, 11, 457, 294, 452, 4800, 11, 264, 1168, 295, 577, 291, 4948, 264, 1589, 2701, 295, 257, 48994, 7006, 294, 257, 2316, 295, 341, 2010, 307, 264, 881, 1021, 1168, 294, 7318, 965, 13, 51264, 51264, 1033, 13, 400, 286, 478, 406, 9287, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.09897443896434346, "compression_ratio": 1.4335260115606936, "no_speech_prob": 6.239666163310176e-06}, {"id": 279, "seek": 247400, "start": 2474.0, "end": 2484.0, "text": " I think the main problem we're facing is how to do self-supervised learning properly. And contrastive methods have shown their limits.", "tokens": [50364, 286, 519, 264, 2135, 1154, 321, 434, 7170, 307, 577, 281, 360, 2698, 12, 48172, 24420, 2539, 6108, 13, 400, 8712, 488, 7150, 362, 4898, 641, 10406, 13, 50864, 50864, 400, 370, 321, 362, 281, 915, 20478, 11, 293, 20478, 366, 3890, 1602, 48994, 7006, 5245, 13, 51114, 51114, 821, 1062, 312, 661, 3487, 300, 5079, 575, 632, 370, 1400, 11, 457, 613, 366, 264, 787, 732, 300, 286, 458, 295, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07028663158416748, "compression_ratio": 1.5450236966824644, "no_speech_prob": 9.078324183064979e-06}, {"id": 280, "seek": 247400, "start": 2484.0, "end": 2489.0, "text": " And so we have to find alternatives, and alternatives are regularized latent variable models.", "tokens": [50364, 286, 519, 264, 2135, 1154, 321, 434, 7170, 307, 577, 281, 360, 2698, 12, 48172, 24420, 2539, 6108, 13, 400, 8712, 488, 7150, 362, 4898, 641, 10406, 13, 50864, 50864, 400, 370, 321, 362, 281, 915, 20478, 11, 293, 20478, 366, 3890, 1602, 48994, 7006, 5245, 13, 51114, 51114, 821, 1062, 312, 661, 3487, 300, 5079, 575, 632, 370, 1400, 11, 457, 613, 366, 264, 787, 732, 300, 286, 458, 295, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07028663158416748, "compression_ratio": 1.5450236966824644, "no_speech_prob": 9.078324183064979e-06}, {"id": 281, "seek": 247400, "start": 2489.0, "end": 2495.0, "text": " There might be other ideas that nobody has had so far, but these are the only two that I know of.", "tokens": [50364, 286, 519, 264, 2135, 1154, 321, 434, 7170, 307, 577, 281, 360, 2698, 12, 48172, 24420, 2539, 6108, 13, 400, 8712, 488, 7150, 362, 4898, 641, 10406, 13, 50864, 50864, 400, 370, 321, 362, 281, 915, 20478, 11, 293, 20478, 366, 3890, 1602, 48994, 7006, 5245, 13, 51114, 51114, 821, 1062, 312, 661, 3487, 300, 5079, 575, 632, 370, 1400, 11, 457, 613, 366, 264, 787, 732, 300, 286, 458, 295, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07028663158416748, "compression_ratio": 1.5450236966824644, "no_speech_prob": 9.078324183064979e-06}, {"id": 282, "seek": 249500, "start": 2495.0, "end": 2516.0, "text": " And then the main technical issue that we need to solve is how do we limit the information content of the latent variable so that we limit the volume of white space that can take low energy, and therefore we automatically make the energy outside the manifold of data where we train the system to have low energy, we automatically make the energy outside higher.", "tokens": [50364, 400, 550, 264, 2135, 6191, 2734, 300, 321, 643, 281, 5039, 307, 577, 360, 321, 4948, 264, 1589, 2701, 295, 264, 48994, 7006, 370, 300, 321, 4948, 264, 5523, 295, 2418, 1901, 300, 393, 747, 2295, 2281, 11, 293, 4412, 321, 6772, 652, 264, 2281, 2380, 264, 47138, 295, 1412, 689, 321, 3847, 264, 1185, 281, 362, 2295, 2281, 11, 321, 6772, 652, 264, 2281, 2380, 2946, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08250170283847386, "compression_ratio": 1.8802083333333333, "no_speech_prob": 1.1842067578982096e-05}, {"id": 283, "seek": 251600, "start": 2516.0, "end": 2529.0, "text": " So I'm going to go through a few examples of systems that actually work, and things that people have done for 20 years in some cases.", "tokens": [50364, 407, 286, 478, 516, 281, 352, 807, 257, 1326, 5110, 295, 3652, 300, 767, 589, 11, 293, 721, 300, 561, 362, 1096, 337, 945, 924, 294, 512, 3331, 13, 51014, 51014, 400, 370, 300, 311, 264, 1558, 510, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.12109479238820631, "compression_ratio": 1.265625, "no_speech_prob": 8.397584679187275e-06}, {"id": 284, "seek": 251600, "start": 2529.0, "end": 2532.0, "text": " And so that's the idea here.", "tokens": [50364, 407, 286, 478, 516, 281, 352, 807, 257, 1326, 5110, 295, 3652, 300, 767, 589, 11, 293, 721, 300, 561, 362, 1096, 337, 945, 924, 294, 512, 3331, 13, 51014, 51014, 400, 370, 300, 311, 264, 1558, 510, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.12109479238820631, "compression_ratio": 1.265625, "no_speech_prob": 8.397584679187275e-06}, {"id": 285, "seek": 253200, "start": 2532.0, "end": 2552.0, "text": " So one of the ideas, you add a regularizer in the energy, and this regularizer takes low value on a kind of small part of the space of Z. And so the system will preferentially choose values of Z that are within this sort of restricted set, where R takes a small value.", "tokens": [50364, 407, 472, 295, 264, 3487, 11, 291, 909, 257, 3890, 6545, 294, 264, 2281, 11, 293, 341, 3890, 6545, 2516, 2295, 2158, 322, 257, 733, 295, 1359, 644, 295, 264, 1901, 295, 1176, 13, 400, 370, 264, 1185, 486, 4382, 3137, 2826, 4190, 295, 1176, 300, 366, 1951, 341, 1333, 295, 20608, 992, 11, 689, 497, 2516, 257, 1359, 2158, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.12316282712496245, "compression_ratio": 1.6047904191616766, "no_speech_prob": 2.7534199034562334e-05}, {"id": 286, "seek": 255200, "start": 2552.0, "end": 2564.0, "text": " And if Z needs to go outside of that set to do a good reconstruction, you're paying a price for it in terms of energy.", "tokens": [50364, 400, 498, 1176, 2203, 281, 352, 2380, 295, 300, 992, 281, 360, 257, 665, 31565, 11, 291, 434, 6229, 257, 3218, 337, 309, 294, 2115, 295, 2281, 13, 50964, 50964], "temperature": 0.0, "avg_logprob": -0.06779581308364868, "compression_ratio": 1.2040816326530612, "no_speech_prob": 7.646211997780483e-06}, {"id": 287, "seek": 256400, "start": 2564.0, "end": 2592.0, "text": " So, the volume of Z space that is determined by R basically limits the volume of space of Y that can take low energy. And the trade off is controlled by basically a coefficient lambda that you can adjust to make the volume of white space that take low energy as small as possible or not that small.", "tokens": [50364, 407, 11, 264, 5523, 295, 1176, 1901, 300, 307, 9540, 538, 497, 1936, 10406, 264, 5523, 295, 1901, 295, 398, 300, 393, 747, 2295, 2281, 13, 400, 264, 4923, 766, 307, 10164, 538, 1936, 257, 17619, 13607, 300, 291, 393, 4369, 281, 652, 264, 5523, 295, 2418, 1901, 300, 747, 2295, 2281, 382, 1359, 382, 1944, 420, 406, 300, 1359, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09405360588660606, "compression_ratio": 1.7325581395348837, "no_speech_prob": 8.939205144997686e-06}, {"id": 288, "seek": 259200, "start": 2592.0, "end": 2598.0, "text": " So here are a few examples of R and Z.", "tokens": [50364, 407, 510, 366, 257, 1326, 5110, 295, 497, 293, 1176, 13, 50664, 50664, 400, 512, 295, 552, 366, 733, 295, 4420, 570, 436, 434, 819, 9364, 365, 3104, 281, 1176, 293, 512, 295, 552, 366, 406, 370, 4420, 570, 436, 434, 406, 819, 9364, 11, 370, 291, 362, 281, 574, 337, 733, 295, 11, 291, 458, 11, 360, 27706, 3164, 13, 51214, 51214, 407, 472, 307, 264, 4942, 10139, 295, 1176, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.11876506554452997, "compression_ratio": 1.6875, "no_speech_prob": 3.218697747797705e-05}, {"id": 289, "seek": 259200, "start": 2598.0, "end": 2609.0, "text": " And some of them are kind of useful because they're differentiable with respect to Z and some of them are not so useful because they're not differentiable, so you have to look for kind of, you know, do discrete search.", "tokens": [50364, 407, 510, 366, 257, 1326, 5110, 295, 497, 293, 1176, 13, 50664, 50664, 400, 512, 295, 552, 366, 733, 295, 4420, 570, 436, 434, 819, 9364, 365, 3104, 281, 1176, 293, 512, 295, 552, 366, 406, 370, 4420, 570, 436, 434, 406, 819, 9364, 11, 370, 291, 362, 281, 574, 337, 733, 295, 11, 291, 458, 11, 360, 27706, 3164, 13, 51214, 51214, 407, 472, 307, 264, 4942, 10139, 295, 1176, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.11876506554452997, "compression_ratio": 1.6875, "no_speech_prob": 3.218697747797705e-05}, {"id": 290, "seek": 259200, "start": 2609.0, "end": 2612.0, "text": " So one is the effective dimension of Z.", "tokens": [50364, 407, 510, 366, 257, 1326, 5110, 295, 497, 293, 1176, 13, 50664, 50664, 400, 512, 295, 552, 366, 733, 295, 4420, 570, 436, 434, 819, 9364, 365, 3104, 281, 1176, 293, 512, 295, 552, 366, 406, 370, 4420, 570, 436, 434, 406, 819, 9364, 11, 370, 291, 362, 281, 574, 337, 733, 295, 11, 291, 458, 11, 360, 27706, 3164, 13, 51214, 51214, 407, 472, 307, 264, 4942, 10139, 295, 1176, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.11876506554452997, "compression_ratio": 1.6875, "no_speech_prob": 3.218697747797705e-05}, {"id": 291, "seek": 261200, "start": 2612.0, "end": 2628.0, "text": " So what you can do is you can decide that Z a priori has three dimension, four dimension, five dimension, six dimension. You train your model for various dimensions of Z, and there is one set of dimensions for which, you know, one dimension for which the prediction would be good, but at the same time, the dimension of Z would be minimized.", "tokens": [50364, 407, 437, 291, 393, 360, 307, 291, 393, 4536, 300, 1176, 257, 4059, 72, 575, 1045, 10139, 11, 1451, 10139, 11, 1732, 10139, 11, 2309, 10139, 13, 509, 3847, 428, 2316, 337, 3683, 12819, 295, 1176, 11, 293, 456, 307, 472, 992, 295, 12819, 337, 597, 11, 291, 458, 11, 472, 10139, 337, 597, 264, 17630, 576, 312, 665, 11, 457, 412, 264, 912, 565, 11, 264, 10139, 295, 1176, 576, 312, 4464, 1602, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.10075734536859053, "compression_ratio": 1.8736263736263736, "no_speech_prob": 1.7777110770111904e-05}, {"id": 292, "seek": 262800, "start": 2628.0, "end": 2644.0, "text": " And what you will have found is basically the lowest embedding dimension of your space. So imagine, for example, that your data set consists of lots and lots of pictures of someone making faces in front of a camera.", "tokens": [50364, 400, 437, 291, 486, 362, 1352, 307, 1936, 264, 12437, 12240, 3584, 10139, 295, 428, 1901, 13, 407, 3811, 11, 337, 1365, 11, 300, 428, 1412, 992, 14689, 295, 3195, 293, 3195, 295, 5242, 295, 1580, 1455, 8475, 294, 1868, 295, 257, 2799, 13, 51164, 51164, 492, 458, 300, 264, 4942, 10139, 295, 264, 47138, 295, 439, 264, 8475, 295, 257, 954, 307, 746, 411, 4060, 11, 412, 1935, 1570, 813, 2319, 11, 570, 309, 311, 37498, 3673, 538, 264, 1230, 295, 9530, 294, 428, 1851, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.053541786068088404, "compression_ratio": 1.623015873015873, "no_speech_prob": 7.030116830719635e-05}, {"id": 293, "seek": 262800, "start": 2644.0, "end": 2656.0, "text": " We know that the effective dimension of the manifold of all the faces of a person is something like 60, at least less than 100, because it's bounded above by the number of muscles in your face.", "tokens": [50364, 400, 437, 291, 486, 362, 1352, 307, 1936, 264, 12437, 12240, 3584, 10139, 295, 428, 1901, 13, 407, 3811, 11, 337, 1365, 11, 300, 428, 1412, 992, 14689, 295, 3195, 293, 3195, 295, 5242, 295, 1580, 1455, 8475, 294, 1868, 295, 257, 2799, 13, 51164, 51164, 492, 458, 300, 264, 4942, 10139, 295, 264, 47138, 295, 439, 264, 8475, 295, 257, 954, 307, 746, 411, 4060, 11, 412, 1935, 1570, 813, 2319, 11, 570, 309, 311, 37498, 3673, 538, 264, 1230, 295, 9530, 294, 428, 1851, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.053541786068088404, "compression_ratio": 1.623015873015873, "no_speech_prob": 7.030116830719635e-05}, {"id": 294, "seek": 265600, "start": 2656.0, "end": 2674.0, "text": " And so there has to be a Z of dimension 50 or 60 or something like that, such that when you run it through a convolutional net, you will generate all possible instances of the face of that person.", "tokens": [50364, 400, 370, 456, 575, 281, 312, 257, 1176, 295, 10139, 2625, 420, 4060, 420, 746, 411, 300, 11, 1270, 300, 562, 291, 1190, 309, 807, 257, 45216, 304, 2533, 11, 291, 486, 8460, 439, 1944, 14519, 295, 264, 1851, 295, 300, 954, 13, 51264, 51264, 1033, 11, 300, 311, 264, 1851, 47138, 337, 300, 954, 498, 291, 528, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.062018689655122305, "compression_ratio": 1.4883720930232558, "no_speech_prob": 1.1124367119919043e-05}, {"id": 295, "seek": 265600, "start": 2674.0, "end": 2681.0, "text": " Okay, that's the face manifold for that person if you want.", "tokens": [50364, 400, 370, 456, 575, 281, 312, 257, 1176, 295, 10139, 2625, 420, 4060, 420, 746, 411, 300, 11, 1270, 300, 562, 291, 1190, 309, 807, 257, 45216, 304, 2533, 11, 291, 486, 8460, 439, 1944, 14519, 295, 264, 1851, 295, 300, 954, 13, 51264, 51264, 1033, 11, 300, 311, 264, 1851, 47138, 337, 300, 954, 498, 291, 528, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.062018689655122305, "compression_ratio": 1.4883720930232558, "no_speech_prob": 1.1124367119919043e-05}, {"id": 296, "seek": 268100, "start": 2681.0, "end": 2690.0, "text": " So what you can do is this really super expensive method of kind of trying all different dimensions of Z.", "tokens": [50364, 407, 437, 291, 393, 360, 307, 341, 534, 1687, 5124, 3170, 295, 733, 295, 1382, 439, 819, 12819, 295, 1176, 13, 50814, 50814, 1485, 636, 281, 47881, 341, 44003, 307, 281, 17522, 264, 441, 15, 2026, 295, 1176, 13, 407, 309, 311, 767, 257, 4748, 819, 551, 13, 51264, 51264, 407, 437, 291, 393, 360, 307, 291, 2826, 257, 1176, 300, 311, 7226, 1090, 10139, 11, 457, 337, 604, 2212, 6889, 11, 291, 17522, 264, 1230, 295, 6677, 295, 1176, 300, 366, 2107, 12, 32226, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09527802997165256, "compression_ratio": 1.6946902654867257, "no_speech_prob": 4.831463593291119e-05}, {"id": 297, "seek": 268100, "start": 2690.0, "end": 2699.0, "text": " One way to formulate this mathematically is to minimize the L0 norm of Z. So it's actually a slightly different thing.", "tokens": [50364, 407, 437, 291, 393, 360, 307, 341, 534, 1687, 5124, 3170, 295, 733, 295, 1382, 439, 819, 12819, 295, 1176, 13, 50814, 50814, 1485, 636, 281, 47881, 341, 44003, 307, 281, 17522, 264, 441, 15, 2026, 295, 1176, 13, 407, 309, 311, 767, 257, 4748, 819, 551, 13, 51264, 51264, 407, 437, 291, 393, 360, 307, 291, 2826, 257, 1176, 300, 311, 7226, 1090, 10139, 11, 457, 337, 604, 2212, 6889, 11, 291, 17522, 264, 1230, 295, 6677, 295, 1176, 300, 366, 2107, 12, 32226, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09527802997165256, "compression_ratio": 1.6946902654867257, "no_speech_prob": 4.831463593291119e-05}, {"id": 298, "seek": 268100, "start": 2699.0, "end": 2710.0, "text": " So what you can do is you choose a Z that's relatively high dimension, but for any given sample, you minimize the number of components of Z that are non-zero.", "tokens": [50364, 407, 437, 291, 393, 360, 307, 341, 534, 1687, 5124, 3170, 295, 733, 295, 1382, 439, 819, 12819, 295, 1176, 13, 50814, 50814, 1485, 636, 281, 47881, 341, 44003, 307, 281, 17522, 264, 441, 15, 2026, 295, 1176, 13, 407, 309, 311, 767, 257, 4748, 819, 551, 13, 51264, 51264, 407, 437, 291, 393, 360, 307, 291, 2826, 257, 1176, 300, 311, 7226, 1090, 10139, 11, 457, 337, 604, 2212, 6889, 11, 291, 17522, 264, 1230, 295, 6677, 295, 1176, 300, 366, 2107, 12, 32226, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09527802997165256, "compression_ratio": 1.6946902654867257, "no_speech_prob": 4.831463593291119e-05}, {"id": 299, "seek": 271000, "start": 2710.0, "end": 2717.0, "text": " That's called the L0 norm. It's just the count of the number of components that are non-zero.", "tokens": [50364, 663, 311, 1219, 264, 441, 15, 2026, 13, 467, 311, 445, 264, 1207, 295, 264, 1230, 295, 6677, 300, 366, 2107, 12, 32226, 13, 50714, 50714, 400, 309, 311, 588, 2252, 281, 17522, 300, 2026, 570, 309, 311, 406, 819, 9364, 13, 467, 311, 588, 27706, 13, 51114, 51114, 407, 437, 561, 764, 307, 436, 764, 257, 42432, 30315, 295, 300, 2026, 1219, 264, 441, 16, 2026, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.10338387224409315, "compression_ratio": 1.5593220338983051, "no_speech_prob": 6.811677303630859e-05}, {"id": 300, "seek": 271000, "start": 2717.0, "end": 2725.0, "text": " And it's very difficult to minimize that norm because it's not differentiable. It's very discrete.", "tokens": [50364, 663, 311, 1219, 264, 441, 15, 2026, 13, 467, 311, 445, 264, 1207, 295, 264, 1230, 295, 6677, 300, 366, 2107, 12, 32226, 13, 50714, 50714, 400, 309, 311, 588, 2252, 281, 17522, 300, 2026, 570, 309, 311, 406, 819, 9364, 13, 467, 311, 588, 27706, 13, 51114, 51114, 407, 437, 561, 764, 307, 436, 764, 257, 42432, 30315, 295, 300, 2026, 1219, 264, 441, 16, 2026, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.10338387224409315, "compression_ratio": 1.5593220338983051, "no_speech_prob": 6.811677303630859e-05}, {"id": 301, "seek": 271000, "start": 2725.0, "end": 2730.0, "text": " So what people use is they use a convex relaxation of that norm called the L1 norm.", "tokens": [50364, 663, 311, 1219, 264, 441, 15, 2026, 13, 467, 311, 445, 264, 1207, 295, 264, 1230, 295, 6677, 300, 366, 2107, 12, 32226, 13, 50714, 50714, 400, 309, 311, 588, 2252, 281, 17522, 300, 2026, 570, 309, 311, 406, 819, 9364, 13, 467, 311, 588, 27706, 13, 51114, 51114, 407, 437, 561, 764, 307, 436, 764, 257, 42432, 30315, 295, 300, 2026, 1219, 264, 441, 16, 2026, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.10338387224409315, "compression_ratio": 1.5593220338983051, "no_speech_prob": 6.811677303630859e-05}, {"id": 302, "seek": 273000, "start": 2730.0, "end": 2743.0, "text": " So the L1 norm is the sum of the absolute values of the components of Z. And that's what you use for R and Z, the sum of the absolute values of the components of Z.", "tokens": [50364, 407, 264, 441, 16, 2026, 307, 264, 2408, 295, 264, 8236, 4190, 295, 264, 6677, 295, 1176, 13, 400, 300, 311, 437, 291, 764, 337, 497, 293, 1176, 11, 264, 2408, 295, 264, 8236, 4190, 295, 264, 6677, 295, 1176, 13, 51014, 51014, 1133, 291, 909, 341, 281, 428, 2281, 2445, 11, 437, 264, 1185, 307, 1382, 281, 360, 307, 915, 257, 1176, 300, 31499, 82, 264, 398, 570, 309, 2203, 281, 17522, 383, 295, 398, 293, 398, 2159, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05726595719655355, "compression_ratio": 1.7595628415300546, "no_speech_prob": 9.817847967497073e-06}, {"id": 303, "seek": 273000, "start": 2743.0, "end": 2757.0, "text": " When you add this to your energy function, what the system is trying to do is find a Z that reconstructs the Y because it needs to minimize C of Y and Y bar,", "tokens": [50364, 407, 264, 441, 16, 2026, 307, 264, 2408, 295, 264, 8236, 4190, 295, 264, 6677, 295, 1176, 13, 400, 300, 311, 437, 291, 764, 337, 497, 293, 1176, 11, 264, 2408, 295, 264, 8236, 4190, 295, 264, 6677, 295, 1176, 13, 51014, 51014, 1133, 291, 909, 341, 281, 428, 2281, 2445, 11, 437, 264, 1185, 307, 1382, 281, 360, 307, 915, 257, 1176, 300, 31499, 82, 264, 398, 570, 309, 2203, 281, 17522, 383, 295, 398, 293, 398, 2159, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05726595719655355, "compression_ratio": 1.7595628415300546, "no_speech_prob": 9.817847967497073e-06}, {"id": 304, "seek": 275700, "start": 2757.0, "end": 2766.0, "text": " but also tries to minimize the number of its components that are non-zero because that's the best way to minimize the L1 norm.", "tokens": [50364, 457, 611, 9898, 281, 17522, 264, 1230, 295, 1080, 6677, 300, 366, 2107, 12, 32226, 570, 300, 311, 264, 1151, 636, 281, 17522, 264, 441, 16, 2026, 13, 50814, 50814, 400, 300, 311, 1219, 637, 11668, 17720, 13, 400, 309, 1985, 534, 731, 13, 400, 286, 478, 516, 281, 855, 291, 512, 5110, 295, 341, 13, 51214, 51214, 4546, 286, 352, 456, 11, 286, 445, 528, 281, 2152, 300, 11, 293, 321, 603, 751, 466, 341, 257, 707, 544, 11, 293, 309, 311, 264, 1558, 300, 5127, 5658, 281, 1176, 486, 611, 4948, 264, 1589, 2701, 295, 1176, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09673451914370639, "compression_ratio": 1.626984126984127, "no_speech_prob": 3.426706462050788e-05}, {"id": 305, "seek": 275700, "start": 2766.0, "end": 2774.0, "text": " And that's called sparse coding. And it works really well. And I'm going to show you some examples of this.", "tokens": [50364, 457, 611, 9898, 281, 17522, 264, 1230, 295, 1080, 6677, 300, 366, 2107, 12, 32226, 570, 300, 311, 264, 1151, 636, 281, 17522, 264, 441, 16, 2026, 13, 50814, 50814, 400, 300, 311, 1219, 637, 11668, 17720, 13, 400, 309, 1985, 534, 731, 13, 400, 286, 478, 516, 281, 855, 291, 512, 5110, 295, 341, 13, 51214, 51214, 4546, 286, 352, 456, 11, 286, 445, 528, 281, 2152, 300, 11, 293, 321, 603, 751, 466, 341, 257, 707, 544, 11, 293, 309, 311, 264, 1558, 300, 5127, 5658, 281, 1176, 486, 611, 4948, 264, 1589, 2701, 295, 1176, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09673451914370639, "compression_ratio": 1.626984126984127, "no_speech_prob": 3.426706462050788e-05}, {"id": 306, "seek": 275700, "start": 2774.0, "end": 2786.0, "text": " Before I go there, I just want to mention that, and we'll talk about this a little more, and it's the idea that adding noise to Z will also limit the information content of Z.", "tokens": [50364, 457, 611, 9898, 281, 17522, 264, 1230, 295, 1080, 6677, 300, 366, 2107, 12, 32226, 570, 300, 311, 264, 1151, 636, 281, 17522, 264, 441, 16, 2026, 13, 50814, 50814, 400, 300, 311, 1219, 637, 11668, 17720, 13, 400, 309, 1985, 534, 731, 13, 400, 286, 478, 516, 281, 855, 291, 512, 5110, 295, 341, 13, 51214, 51214, 4546, 286, 352, 456, 11, 286, 445, 528, 281, 2152, 300, 11, 293, 321, 603, 751, 466, 341, 257, 707, 544, 11, 293, 309, 311, 264, 1558, 300, 5127, 5658, 281, 1176, 486, 611, 4948, 264, 1589, 2701, 295, 1176, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09673451914370639, "compression_ratio": 1.626984126984127, "no_speech_prob": 3.426706462050788e-05}, {"id": 307, "seek": 278600, "start": 2786.0, "end": 2793.0, "text": " I'll come back to this in a minute. Okay. So here is the idea of sparse coding.", "tokens": [50364, 286, 603, 808, 646, 281, 341, 294, 257, 3456, 13, 1033, 13, 407, 510, 307, 264, 1558, 295, 637, 11668, 17720, 13, 50714, 50714, 407, 637, 11668, 17720, 307, 364, 47916, 3037, 295, 2281, 12, 6032, 5245, 13, 407, 456, 311, 572, 1783, 13, 821, 311, 787, 257, 398, 293, 257, 1176, 13, 51064, 51064, 400, 264, 2281, 2445, 307, 398, 3175, 343, 57, 11, 689, 343, 307, 257, 370, 12, 11880, 25890, 8141, 11, 588, 2531, 281, 264, 19475, 8141, 294, 591, 12, 1398, 599, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10526880327161851, "compression_ratio": 1.5141509433962264, "no_speech_prob": 4.2644718632800505e-05}, {"id": 308, "seek": 278600, "start": 2793.0, "end": 2800.0, "text": " So sparse coding is an unconditional version of energy-based models. So there's no X. There's only a Y and a Z.", "tokens": [50364, 286, 603, 808, 646, 281, 341, 294, 257, 3456, 13, 1033, 13, 407, 510, 307, 264, 1558, 295, 637, 11668, 17720, 13, 50714, 50714, 407, 637, 11668, 17720, 307, 364, 47916, 3037, 295, 2281, 12, 6032, 5245, 13, 407, 456, 311, 572, 1783, 13, 821, 311, 787, 257, 398, 293, 257, 1176, 13, 51064, 51064, 400, 264, 2281, 2445, 307, 398, 3175, 343, 57, 11, 689, 343, 307, 257, 370, 12, 11880, 25890, 8141, 11, 588, 2531, 281, 264, 19475, 8141, 294, 591, 12, 1398, 599, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10526880327161851, "compression_ratio": 1.5141509433962264, "no_speech_prob": 4.2644718632800505e-05}, {"id": 309, "seek": 278600, "start": 2800.0, "end": 2810.0, "text": " And the energy function is Y minus WZ, where W is a so-called dictionary matrix, very similar to the prototype matrix in K-means.", "tokens": [50364, 286, 603, 808, 646, 281, 341, 294, 257, 3456, 13, 1033, 13, 407, 510, 307, 264, 1558, 295, 637, 11668, 17720, 13, 50714, 50714, 407, 637, 11668, 17720, 307, 364, 47916, 3037, 295, 2281, 12, 6032, 5245, 13, 407, 456, 311, 572, 1783, 13, 821, 311, 787, 257, 398, 293, 257, 1176, 13, 51064, 51064, 400, 264, 2281, 2445, 307, 398, 3175, 343, 57, 11, 689, 343, 307, 257, 370, 12, 11880, 25890, 8141, 11, 588, 2531, 281, 264, 19475, 8141, 294, 591, 12, 1398, 599, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10526880327161851, "compression_ratio": 1.5141509433962264, "no_speech_prob": 4.2644718632800505e-05}, {"id": 310, "seek": 281000, "start": 2810.0, "end": 2819.0, "text": " Z is a vector. Generally, the dimension of Z is larger than Y. And so you measure the square distance, Euclidean distance, between Y and WZ.", "tokens": [50364, 1176, 307, 257, 8062, 13, 21082, 11, 264, 10139, 295, 1176, 307, 4833, 813, 398, 13, 400, 370, 291, 3481, 264, 3732, 4560, 11, 462, 1311, 31264, 282, 4560, 11, 1296, 398, 293, 343, 57, 13, 50814, 50814, 407, 1936, 11, 428, 979, 19866, 510, 307, 8213, 13, 467, 311, 445, 257, 8141, 13, 400, 550, 291, 909, 257, 1433, 11, 13607, 1413, 264, 441, 16, 2026, 295, 1176, 11, 597, 307, 10379, 538, 729, 732, 10228, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.08809181539023794, "compression_ratio": 1.4444444444444444, "no_speech_prob": 3.5348311939742416e-05}, {"id": 311, "seek": 281000, "start": 2819.0, "end": 2830.0, "text": " So basically, your decoder here is linear. It's just a matrix. And then you add a term, lambda times the L1 norm of Z, which is represented by those two bars.", "tokens": [50364, 1176, 307, 257, 8062, 13, 21082, 11, 264, 10139, 295, 1176, 307, 4833, 813, 398, 13, 400, 370, 291, 3481, 264, 3732, 4560, 11, 462, 1311, 31264, 282, 4560, 11, 1296, 398, 293, 343, 57, 13, 50814, 50814, 407, 1936, 11, 428, 979, 19866, 510, 307, 8213, 13, 467, 311, 445, 257, 8141, 13, 400, 550, 291, 909, 257, 1433, 11, 13607, 1413, 264, 441, 16, 2026, 295, 1176, 11, 597, 307, 10379, 538, 729, 732, 10228, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.08809181539023794, "compression_ratio": 1.4444444444444444, "no_speech_prob": 3.5348311939742416e-05}, {"id": 312, "seek": 283000, "start": 2830.0, "end": 2845.0, "text": " And that's the energy function for sparse coding. And you can think of it as a special case of the architecture I showed previously, except it's not conditional. There's no X.", "tokens": [50364, 400, 300, 311, 264, 2281, 2445, 337, 637, 11668, 17720, 13, 400, 291, 393, 519, 295, 309, 382, 257, 2121, 1389, 295, 264, 9482, 286, 4712, 8046, 11, 3993, 309, 311, 406, 27708, 13, 821, 311, 572, 1783, 13, 51114, 51114, 823, 11, 437, 775, 341, 360, 30, 407, 286, 603, 1391, 980, 291, 300, 264, 3036, 286, 478, 4099, 510, 322, 264, 1411, 307, 26723, 570, 309, 311, 767, 10833, 365, 257, 4748, 819, 2316, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10554567972819011, "compression_ratio": 1.5085470085470085, "no_speech_prob": 4.682262078858912e-05}, {"id": 313, "seek": 283000, "start": 2845.0, "end": 2859.0, "text": " Now, what does this do? So I'll probably tell you that the picture I'm showing here on the left is inappropriate because it's actually generated with a slightly different model.", "tokens": [50364, 400, 300, 311, 264, 2281, 2445, 337, 637, 11668, 17720, 13, 400, 291, 393, 519, 295, 309, 382, 257, 2121, 1389, 295, 264, 9482, 286, 4712, 8046, 11, 3993, 309, 311, 406, 27708, 13, 821, 311, 572, 1783, 13, 51114, 51114, 823, 11, 437, 775, 341, 360, 30, 407, 286, 603, 1391, 980, 291, 300, 264, 3036, 286, 478, 4099, 510, 322, 264, 1411, 307, 26723, 570, 309, 311, 767, 10833, 365, 257, 4748, 819, 2316, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10554567972819011, "compression_ratio": 1.5085470085470085, "no_speech_prob": 4.682262078858912e-05}, {"id": 314, "seek": 285900, "start": 2859.0, "end": 2872.0, "text": " But it's a good sort of pictorial representation of what sparse coding attempts to do, which is to approximate the manifold of data by a piecewise linear approximation, essentially.", "tokens": [50364, 583, 309, 311, 257, 665, 1333, 295, 2317, 5181, 10290, 295, 437, 637, 11668, 17720, 15257, 281, 360, 11, 597, 307, 281, 30874, 264, 47138, 295, 1412, 538, 257, 2522, 3711, 8213, 28023, 11, 4476, 13, 51014, 51014, 407, 3811, 300, 291, 362, 341, 343, 8141, 11, 2264, 11, 293, 1580, 575, 2212, 309, 281, 291, 420, 291, 600, 3264, 309, 294, 512, 636, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07052643402763036, "compression_ratio": 1.5, "no_speech_prob": 3.4265860449522734e-05}, {"id": 315, "seek": 285900, "start": 2872.0, "end": 2882.0, "text": " So imagine that you have this W matrix, OK, and someone has given it to you or you've learned it in some way.", "tokens": [50364, 583, 309, 311, 257, 665, 1333, 295, 2317, 5181, 10290, 295, 437, 637, 11668, 17720, 15257, 281, 360, 11, 597, 307, 281, 30874, 264, 47138, 295, 1412, 538, 257, 2522, 3711, 8213, 28023, 11, 4476, 13, 51014, 51014, 407, 3811, 300, 291, 362, 341, 343, 8141, 11, 2264, 11, 293, 1580, 575, 2212, 309, 281, 291, 420, 291, 600, 3264, 309, 294, 512, 636, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07052643402763036, "compression_ratio": 1.5, "no_speech_prob": 3.4265860449522734e-05}, {"id": 316, "seek": 288200, "start": 2882.0, "end": 2900.0, "text": " Now, if you decide a priori that a certain number of components of Z are non-zero, OK, most of the components of Z are zero, just a small number of components of Z are non-zero, and you vary the value of those components, you know, within some range,", "tokens": [50364, 823, 11, 498, 291, 4536, 257, 4059, 72, 300, 257, 1629, 1230, 295, 6677, 295, 1176, 366, 2107, 12, 32226, 11, 2264, 11, 881, 295, 264, 6677, 295, 1176, 366, 4018, 11, 445, 257, 1359, 1230, 295, 6677, 295, 1176, 366, 2107, 12, 32226, 11, 293, 291, 10559, 264, 2158, 295, 729, 6677, 11, 291, 458, 11, 1951, 512, 3613, 11, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.09945115309495192, "compression_ratio": 1.7605633802816902, "no_speech_prob": 3.426665352890268e-05}, {"id": 317, "seek": 290000, "start": 2900.0, "end": 2914.0, "text": " the set of vectors that you're going to generate, the set of Y bars that you're going to generate, are going to be the Y bars that are in the linear subspace spanned by the corresponding columns of the W matrix.", "tokens": [50364, 264, 992, 295, 18875, 300, 291, 434, 516, 281, 8460, 11, 264, 992, 295, 398, 10228, 300, 291, 434, 516, 281, 8460, 11, 366, 516, 281, 312, 264, 398, 10228, 300, 366, 294, 264, 8213, 2090, 17940, 637, 5943, 538, 264, 11760, 13766, 295, 264, 343, 8141, 13, 51064, 51064], "temperature": 0.0, "avg_logprob": -0.07222965130439171, "compression_ratio": 1.7154471544715446, "no_speech_prob": 8.139322744682431e-06}, {"id": 318, "seek": 291400, "start": 2914.0, "end": 2930.0, "text": " For every value of the Z coefficients that are non-zero, you basically compute a linear combination of the corresponding columns of W. And so you're basically moving along a low dimensional linear subspace of Y space.", "tokens": [50364, 1171, 633, 2158, 295, 264, 1176, 31994, 300, 366, 2107, 12, 32226, 11, 291, 1936, 14722, 257, 8213, 6562, 295, 264, 11760, 13766, 295, 343, 13, 400, 370, 291, 434, 1936, 2684, 2051, 257, 2295, 18795, 8213, 2090, 17940, 295, 398, 1901, 13, 51164, 51164, 407, 398, 2159, 307, 516, 281, 312, 1936, 2051, 257, 2295, 18795, 1901, 11, 257, 2295, 18795, 8213, 2090, 17940, 13, 440, 10139, 295, 300, 1901, 486, 312, 264, 1230, 295, 2107, 12, 32226, 6677, 295, 1176, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07582755472468233, "compression_ratio": 1.827906976744186, "no_speech_prob": 1.1123983313154895e-05}, {"id": 319, "seek": 291400, "start": 2930.0, "end": 2942.0, "text": " So Y bar is going to be basically along a low dimensional space, a low dimensional linear subspace. The dimension of that space will be the number of non-zero components of Z.", "tokens": [50364, 1171, 633, 2158, 295, 264, 1176, 31994, 300, 366, 2107, 12, 32226, 11, 291, 1936, 14722, 257, 8213, 6562, 295, 264, 11760, 13766, 295, 343, 13, 400, 370, 291, 434, 1936, 2684, 2051, 257, 2295, 18795, 8213, 2090, 17940, 295, 398, 1901, 13, 51164, 51164, 407, 398, 2159, 307, 516, 281, 312, 1936, 2051, 257, 2295, 18795, 1901, 11, 257, 2295, 18795, 8213, 2090, 17940, 13, 440, 10139, 295, 300, 1901, 486, 312, 264, 1230, 295, 2107, 12, 32226, 6677, 295, 1176, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07582755472468233, "compression_ratio": 1.827906976744186, "no_speech_prob": 1.1123983313154895e-05}, {"id": 320, "seek": 294200, "start": 2942.0, "end": 2952.0, "text": " OK, so for one particular Y, when you find a Z that minimizes the energy, a number of components are going to be non-zero.", "tokens": [50364, 2264, 11, 370, 337, 472, 1729, 398, 11, 562, 291, 915, 257, 1176, 300, 4464, 5660, 264, 2281, 11, 257, 1230, 295, 6677, 366, 516, 281, 312, 2107, 12, 32226, 13, 50864, 50864, 400, 382, 291, 1286, 398, 5692, 11, 729, 2107, 12, 32226, 6677, 366, 516, 281, 1319, 2158, 11, 457, 291, 434, 516, 281, 1754, 322, 264, 912, 8213, 2090, 17940, 1826, 398, 2962, 886, 709, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.08987101463422384, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.3006694643991068e-05}, {"id": 321, "seek": 294200, "start": 2952.0, "end": 2964.0, "text": " And as you move Y slowly, those non-zero components are going to change value, but you're going to stay on the same linear subspace until Y changes too much.", "tokens": [50364, 2264, 11, 370, 337, 472, 1729, 398, 11, 562, 291, 915, 257, 1176, 300, 4464, 5660, 264, 2281, 11, 257, 1230, 295, 6677, 366, 516, 281, 312, 2107, 12, 32226, 13, 50864, 50864, 400, 382, 291, 1286, 398, 5692, 11, 729, 2107, 12, 32226, 6677, 366, 516, 281, 1319, 2158, 11, 457, 291, 434, 516, 281, 1754, 322, 264, 912, 8213, 2090, 17940, 1826, 398, 2962, 886, 709, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.08987101463422384, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.3006694643991068e-05}, {"id": 322, "seek": 296400, "start": 2964.0, "end": 2973.0, "text": " And then all of a sudden, you need a different set of non-zero Z to do the best reconstruction. And now you're switching to a different plane.", "tokens": [50364, 400, 550, 439, 295, 257, 3990, 11, 291, 643, 257, 819, 992, 295, 2107, 12, 32226, 1176, 281, 360, 264, 1151, 31565, 13, 400, 586, 291, 434, 16493, 281, 257, 819, 5720, 13, 50814, 50814, 2264, 11, 570, 257, 819, 992, 295, 1176, 6677, 1813, 2107, 12, 32226, 13, 51164, 51164, 400, 370, 586, 291, 1286, 398, 797, 11, 293, 797, 264, 31994, 294, 1176, 1066, 4473, 4190, 11, 3993, 337, 264, 2306, 300, 366, 4018, 11, 300, 1754, 4018, 11, 293, 439, 295, 257, 3990, 309, 19458, 797, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09277230120719747, "compression_ratio": 1.7327188940092166, "no_speech_prob": 7.183174147940008e-06}, {"id": 323, "seek": 296400, "start": 2973.0, "end": 2980.0, "text": " OK, because a different set of Z components become non-zero.", "tokens": [50364, 400, 550, 439, 295, 257, 3990, 11, 291, 643, 257, 819, 992, 295, 2107, 12, 32226, 1176, 281, 360, 264, 1151, 31565, 13, 400, 586, 291, 434, 16493, 281, 257, 819, 5720, 13, 50814, 50814, 2264, 11, 570, 257, 819, 992, 295, 1176, 6677, 1813, 2107, 12, 32226, 13, 51164, 51164, 400, 370, 586, 291, 1286, 398, 797, 11, 293, 797, 264, 31994, 294, 1176, 1066, 4473, 4190, 11, 3993, 337, 264, 2306, 300, 366, 4018, 11, 300, 1754, 4018, 11, 293, 439, 295, 257, 3990, 309, 19458, 797, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09277230120719747, "compression_ratio": 1.7327188940092166, "no_speech_prob": 7.183174147940008e-06}, {"id": 324, "seek": 296400, "start": 2980.0, "end": 2990.0, "text": " And so now you move Y again, and again the coefficients in Z keep changing values, except for the ones that are zero, that stay zero, and all of a sudden it switches again.", "tokens": [50364, 400, 550, 439, 295, 257, 3990, 11, 291, 643, 257, 819, 992, 295, 2107, 12, 32226, 1176, 281, 360, 264, 1151, 31565, 13, 400, 586, 291, 434, 16493, 281, 257, 819, 5720, 13, 50814, 50814, 2264, 11, 570, 257, 819, 992, 295, 1176, 6677, 1813, 2107, 12, 32226, 13, 51164, 51164, 400, 370, 586, 291, 1286, 398, 797, 11, 293, 797, 264, 31994, 294, 1176, 1066, 4473, 4190, 11, 3993, 337, 264, 2306, 300, 366, 4018, 11, 300, 1754, 4018, 11, 293, 439, 295, 257, 3990, 309, 19458, 797, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09277230120719747, "compression_ratio": 1.7327188940092166, "no_speech_prob": 7.183174147940008e-06}, {"id": 325, "seek": 299000, "start": 2990.0, "end": 3004.0, "text": " It goes to another one. So it's kind of well symbolized by the picture on the left, where you see that the manifold of data is approximated by basically a bunch of linear subspace, in this case lines.", "tokens": [50364, 467, 1709, 281, 1071, 472, 13, 407, 309, 311, 733, 295, 731, 5986, 1602, 538, 264, 3036, 322, 264, 1411, 11, 689, 291, 536, 300, 264, 47138, 295, 1412, 307, 8542, 770, 538, 1936, 257, 3840, 295, 8213, 2090, 17940, 11, 294, 341, 1389, 3876, 13, 51064, 51064, 440, 1778, 983, 309, 311, 2252, 281, 2906, 264, 3539, 637, 11668, 17720, 294, 568, 35, 307, 570, 309, 311, 516, 281, 40520, 473, 294, 568, 35, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08474111557006836, "compression_ratio": 1.5240384615384615, "no_speech_prob": 3.426571129239164e-05}, {"id": 326, "seek": 299000, "start": 3004.0, "end": 3011.0, "text": " The reason why it's difficult to represent the actual sparse coding in 2D is because it's going to degenerate in 2D.", "tokens": [50364, 467, 1709, 281, 1071, 472, 13, 407, 309, 311, 733, 295, 731, 5986, 1602, 538, 264, 3036, 322, 264, 1411, 11, 689, 291, 536, 300, 264, 47138, 295, 1412, 307, 8542, 770, 538, 1936, 257, 3840, 295, 8213, 2090, 17940, 11, 294, 341, 1389, 3876, 13, 51064, 51064, 440, 1778, 983, 309, 311, 2252, 281, 2906, 264, 3539, 637, 11668, 17720, 294, 568, 35, 307, 570, 309, 311, 516, 281, 40520, 473, 294, 568, 35, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08474111557006836, "compression_ratio": 1.5240384615384615, "no_speech_prob": 3.426571129239164e-05}, {"id": 327, "seek": 301100, "start": 3011.0, "end": 3024.0, "text": " So one question is how do we train a system like this? So to train a system like this, our loss function is just going to be the average energy that our model gives to our training samples.", "tokens": [50364, 407, 472, 1168, 307, 577, 360, 321, 3847, 257, 1185, 411, 341, 30, 407, 281, 3847, 257, 1185, 411, 341, 11, 527, 4470, 2445, 307, 445, 516, 281, 312, 264, 4274, 2281, 300, 527, 2316, 2709, 281, 527, 3097, 10938, 13, 51014, 51014, 407, 264, 4470, 2445, 307, 445, 264, 4274, 2281, 11, 1936, 264, 4274, 479, 13, 400, 1604, 479, 295, 398, 307, 2681, 281, 264, 7285, 670, 1176, 295, 462, 295, 398, 293, 1176, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07035685174259139, "compression_ratio": 1.7891891891891891, "no_speech_prob": 1.2218408301123418e-05}, {"id": 328, "seek": 301100, "start": 3024.0, "end": 3034.0, "text": " So the loss function is just the average energy, basically the average F. And remember F of Y is equal to the minimum over Z of E of Y and Z.", "tokens": [50364, 407, 472, 1168, 307, 577, 360, 321, 3847, 257, 1185, 411, 341, 30, 407, 281, 3847, 257, 1185, 411, 341, 11, 527, 4470, 2445, 307, 445, 516, 281, 312, 264, 4274, 2281, 300, 527, 2316, 2709, 281, 527, 3097, 10938, 13, 51014, 51014, 407, 264, 4470, 2445, 307, 445, 264, 4274, 2281, 11, 1936, 264, 4274, 479, 13, 400, 1604, 479, 295, 398, 307, 2681, 281, 264, 7285, 670, 1176, 295, 462, 295, 398, 293, 1176, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07035685174259139, "compression_ratio": 1.7891891891891891, "no_speech_prob": 1.2218408301123418e-05}, {"id": 329, "seek": 303400, "start": 3034.0, "end": 3044.0, "text": " OK, so we're going to take the average of F over all our training samples and minimize that average with respect to the parameters of the model.", "tokens": [50364, 2264, 11, 370, 321, 434, 516, 281, 747, 264, 4274, 295, 479, 670, 439, 527, 3097, 10938, 293, 17522, 300, 4274, 365, 3104, 281, 264, 9834, 295, 264, 2316, 13, 50864, 50864, 400, 729, 9834, 366, 264, 31994, 294, 264, 343, 8141, 13, 3764, 11, 309, 311, 1219, 264, 25890, 8141, 13, 51114, 51114, 407, 577, 360, 321, 360, 341, 30, 492, 747, 257, 6889, 398, 11, 321, 915, 264, 1176, 300, 4464, 5660, 264, 2281, 11, 264, 2408, 295, 264, 732, 2115, 300, 291, 536, 510, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08313069136246391, "compression_ratio": 1.6387665198237886, "no_speech_prob": 6.339000719890464e-06}, {"id": 330, "seek": 303400, "start": 3044.0, "end": 3049.0, "text": " And those parameters are the coefficients in the W matrix. Again, it's called the dictionary matrix.", "tokens": [50364, 2264, 11, 370, 321, 434, 516, 281, 747, 264, 4274, 295, 479, 670, 439, 527, 3097, 10938, 293, 17522, 300, 4274, 365, 3104, 281, 264, 9834, 295, 264, 2316, 13, 50864, 50864, 400, 729, 9834, 366, 264, 31994, 294, 264, 343, 8141, 13, 3764, 11, 309, 311, 1219, 264, 25890, 8141, 13, 51114, 51114, 407, 577, 360, 321, 360, 341, 30, 492, 747, 257, 6889, 398, 11, 321, 915, 264, 1176, 300, 4464, 5660, 264, 2281, 11, 264, 2408, 295, 264, 732, 2115, 300, 291, 536, 510, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08313069136246391, "compression_ratio": 1.6387665198237886, "no_speech_prob": 6.339000719890464e-06}, {"id": 331, "seek": 303400, "start": 3049.0, "end": 3057.0, "text": " So how do we do this? We take a sample Y, we find the Z that minimizes the energy, the sum of the two terms that you see here.", "tokens": [50364, 2264, 11, 370, 321, 434, 516, 281, 747, 264, 4274, 295, 479, 670, 439, 527, 3097, 10938, 293, 17522, 300, 4274, 365, 3104, 281, 264, 9834, 295, 264, 2316, 13, 50864, 50864, 400, 729, 9834, 366, 264, 31994, 294, 264, 343, 8141, 13, 3764, 11, 309, 311, 1219, 264, 25890, 8141, 13, 51114, 51114, 407, 577, 360, 321, 360, 341, 30, 492, 747, 257, 6889, 398, 11, 321, 915, 264, 1176, 300, 4464, 5660, 264, 2281, 11, 264, 2408, 295, 264, 732, 2115, 300, 291, 536, 510, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08313069136246391, "compression_ratio": 1.6387665198237886, "no_speech_prob": 6.339000719890464e-06}, {"id": 332, "seek": 305700, "start": 3057.0, "end": 3067.0, "text": " And then we take one step of gradient descent in W. So we compute the gradient of the energy with respect to W, which is very simple because it's a quadratic function of W.", "tokens": [50364, 400, 550, 321, 747, 472, 1823, 295, 16235, 23475, 294, 343, 13, 407, 321, 14722, 264, 16235, 295, 264, 2281, 365, 3104, 281, 343, 11, 597, 307, 588, 2199, 570, 309, 311, 257, 37262, 2445, 295, 343, 13, 50864, 50864, 400, 321, 747, 472, 1823, 295, 342, 8997, 2750, 16235, 11, 1936, 13, 1779, 30, 400, 586, 321, 747, 264, 958, 398, 293, 360, 309, 797, 11, 17522, 365, 3104, 281, 1176, 13, 51364, 51364, 400, 550, 337, 300, 2158, 295, 1176, 11, 14722, 264, 16235, 365, 3104, 281, 343, 293, 747, 472, 1823, 294, 264, 3671, 16235, 13, 400, 291, 1066, 884, 341, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06559196524663803, "compression_ratio": 1.8388429752066116, "no_speech_prob": 2.0143575966358185e-05}, {"id": 333, "seek": 305700, "start": 3067.0, "end": 3077.0, "text": " And we take one step of stochastic gradient, basically. Right? And now we take the next Y and do it again, minimize with respect to Z.", "tokens": [50364, 400, 550, 321, 747, 472, 1823, 295, 16235, 23475, 294, 343, 13, 407, 321, 14722, 264, 16235, 295, 264, 2281, 365, 3104, 281, 343, 11, 597, 307, 588, 2199, 570, 309, 311, 257, 37262, 2445, 295, 343, 13, 50864, 50864, 400, 321, 747, 472, 1823, 295, 342, 8997, 2750, 16235, 11, 1936, 13, 1779, 30, 400, 586, 321, 747, 264, 958, 398, 293, 360, 309, 797, 11, 17522, 365, 3104, 281, 1176, 13, 51364, 51364, 400, 550, 337, 300, 2158, 295, 1176, 11, 14722, 264, 16235, 365, 3104, 281, 343, 293, 747, 472, 1823, 294, 264, 3671, 16235, 13, 400, 291, 1066, 884, 341, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06559196524663803, "compression_ratio": 1.8388429752066116, "no_speech_prob": 2.0143575966358185e-05}, {"id": 334, "seek": 305700, "start": 3077.0, "end": 3084.0, "text": " And then for that value of Z, compute the gradient with respect to W and take one step in the negative gradient. And you keep doing this.", "tokens": [50364, 400, 550, 321, 747, 472, 1823, 295, 16235, 23475, 294, 343, 13, 407, 321, 14722, 264, 16235, 295, 264, 2281, 365, 3104, 281, 343, 11, 597, 307, 588, 2199, 570, 309, 311, 257, 37262, 2445, 295, 343, 13, 50864, 50864, 400, 321, 747, 472, 1823, 295, 342, 8997, 2750, 16235, 11, 1936, 13, 1779, 30, 400, 586, 321, 747, 264, 958, 398, 293, 360, 309, 797, 11, 17522, 365, 3104, 281, 1176, 13, 51364, 51364, 400, 550, 337, 300, 2158, 295, 1176, 11, 14722, 264, 16235, 365, 3104, 281, 343, 293, 747, 472, 1823, 294, 264, 3671, 16235, 13, 400, 291, 1066, 884, 341, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06559196524663803, "compression_ratio": 1.8388429752066116, "no_speech_prob": 2.0143575966358185e-05}, {"id": 335, "seek": 308400, "start": 3084.0, "end": 3095.0, "text": " Now, if you just do this, it doesn't work. It doesn't work because the result is that W will keep getting bigger and bigger and Z will keep getting smaller and smaller.", "tokens": [50364, 823, 11, 498, 291, 445, 360, 341, 11, 309, 1177, 380, 589, 13, 467, 1177, 380, 589, 570, 264, 1874, 307, 300, 343, 486, 1066, 1242, 3801, 293, 3801, 293, 1176, 486, 1066, 1242, 4356, 293, 4356, 13, 50914, 50914, 583, 264, 1154, 486, 406, 767, 11, 264, 1185, 486, 406, 767, 5039, 264, 1154, 13, 51114, 51114, 407, 437, 291, 643, 281, 360, 307, 2710, 1125, 264, 343, 8141, 370, 300, 309, 2644, 1852, 24162, 10925, 293, 2089, 1176, 281, 23060, 11760, 356, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07131370801604196, "compression_ratio": 1.7361111111111112, "no_speech_prob": 1.6186739230761304e-05}, {"id": 336, "seek": 308400, "start": 3095.0, "end": 3099.0, "text": " But the problem will not actually, the system will not actually solve the problem.", "tokens": [50364, 823, 11, 498, 291, 445, 360, 341, 11, 309, 1177, 380, 589, 13, 467, 1177, 380, 589, 570, 264, 1874, 307, 300, 343, 486, 1066, 1242, 3801, 293, 3801, 293, 1176, 486, 1066, 1242, 4356, 293, 4356, 13, 50914, 50914, 583, 264, 1154, 486, 406, 767, 11, 264, 1185, 486, 406, 767, 5039, 264, 1154, 13, 51114, 51114, 407, 437, 291, 643, 281, 360, 307, 2710, 1125, 264, 343, 8141, 370, 300, 309, 2644, 1852, 24162, 10925, 293, 2089, 1176, 281, 23060, 11760, 356, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07131370801604196, "compression_ratio": 1.7361111111111112, "no_speech_prob": 1.6186739230761304e-05}, {"id": 337, "seek": 308400, "start": 3099.0, "end": 3108.0, "text": " So what you need to do is normalize the W matrix so that it cannot grow indefinitely and allow Z to shrink correspondingly.", "tokens": [50364, 823, 11, 498, 291, 445, 360, 341, 11, 309, 1177, 380, 589, 13, 467, 1177, 380, 589, 570, 264, 1874, 307, 300, 343, 486, 1066, 1242, 3801, 293, 3801, 293, 1176, 486, 1066, 1242, 4356, 293, 4356, 13, 50914, 50914, 583, 264, 1154, 486, 406, 767, 11, 264, 1185, 486, 406, 767, 5039, 264, 1154, 13, 51114, 51114, 407, 437, 291, 643, 281, 360, 307, 2710, 1125, 264, 343, 8141, 370, 300, 309, 2644, 1852, 24162, 10925, 293, 2089, 1176, 281, 23060, 11760, 356, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07131370801604196, "compression_ratio": 1.7361111111111112, "no_speech_prob": 1.6186739230761304e-05}, {"id": 338, "seek": 310800, "start": 3108.0, "end": 3121.0, "text": " And the way to do this is that you basically after every update of the W matrix, you normalize the sum of the squares of the terms in a column of each column of W.", "tokens": [50364, 400, 264, 636, 281, 360, 341, 307, 300, 291, 1936, 934, 633, 5623, 295, 264, 343, 8141, 11, 291, 2710, 1125, 264, 2408, 295, 264, 19368, 295, 264, 2115, 294, 257, 7738, 295, 1184, 7738, 295, 343, 13, 51014, 51014, 1779, 30, 407, 2710, 1125, 264, 13766, 295, 343, 934, 1184, 5623, 13, 51264, 51264, 400, 300, 486, 4871, 264, 2115, 294, 343, 490, 15068, 493, 293, 264, 2115, 294, 1176, 490, 41684, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09834830577556904, "compression_ratio": 1.6775956284153006, "no_speech_prob": 5.093448635307141e-06}, {"id": 339, "seek": 310800, "start": 3121.0, "end": 3126.0, "text": " Right? So normalize the columns of W after each update.", "tokens": [50364, 400, 264, 636, 281, 360, 341, 307, 300, 291, 1936, 934, 633, 5623, 295, 264, 343, 8141, 11, 291, 2710, 1125, 264, 2408, 295, 264, 19368, 295, 264, 2115, 294, 257, 7738, 295, 1184, 7738, 295, 343, 13, 51014, 51014, 1779, 30, 407, 2710, 1125, 264, 13766, 295, 343, 934, 1184, 5623, 13, 51264, 51264, 400, 300, 486, 4871, 264, 2115, 294, 343, 490, 15068, 493, 293, 264, 2115, 294, 1176, 490, 41684, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09834830577556904, "compression_ratio": 1.6775956284153006, "no_speech_prob": 5.093448635307141e-06}, {"id": 340, "seek": 310800, "start": 3126.0, "end": 3133.0, "text": " And that will prevent the terms in W from blowing up and the terms in Z from shrinking.", "tokens": [50364, 400, 264, 636, 281, 360, 341, 307, 300, 291, 1936, 934, 633, 5623, 295, 264, 343, 8141, 11, 291, 2710, 1125, 264, 2408, 295, 264, 19368, 295, 264, 2115, 294, 257, 7738, 295, 1184, 7738, 295, 343, 13, 51014, 51014, 1779, 30, 407, 2710, 1125, 264, 13766, 295, 343, 934, 1184, 5623, 13, 51264, 51264, 400, 300, 486, 4871, 264, 2115, 294, 343, 490, 15068, 493, 293, 264, 2115, 294, 1176, 490, 41684, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09834830577556904, "compression_ratio": 1.6775956284153006, "no_speech_prob": 5.093448635307141e-06}, {"id": 341, "seek": 313300, "start": 3133.0, "end": 3141.0, "text": " And it will force the system to actually find a reasonable matrix W and not get away with just making Z shorter.", "tokens": [50364, 400, 309, 486, 3464, 264, 1185, 281, 767, 915, 257, 10585, 8141, 343, 293, 406, 483, 1314, 365, 445, 1455, 1176, 11639, 13, 50764, 50764, 2264, 13, 407, 300, 311, 637, 11668, 17720, 13, 639, 390, 264, 2539, 9284, 337, 341, 390, 14479, 538, 732, 28270, 28813, 5412, 1751, 11, 23046, 12948, 8356, 293, 4389, 17952, 294, 22383, 13, 51414, 51414, 400, 370, 300, 1709, 646, 257, 938, 565, 13, 2264, 13, 407, 510, 307, 264, 1154, 365, 637, 11668, 17720, 13, 440, 38253, 9284, 307, 733, 295, 5124, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13572030371807992, "compression_ratio": 1.554263565891473, "no_speech_prob": 7.5277976065990515e-06}, {"id": 342, "seek": 313300, "start": 3141.0, "end": 3154.0, "text": " OK. So that's sparse coding. This was the learning algorithm for this was invented by two computational neuroscientists, Bruno Alsassen and David Field in 1997.", "tokens": [50364, 400, 309, 486, 3464, 264, 1185, 281, 767, 915, 257, 10585, 8141, 343, 293, 406, 483, 1314, 365, 445, 1455, 1176, 11639, 13, 50764, 50764, 2264, 13, 407, 300, 311, 637, 11668, 17720, 13, 639, 390, 264, 2539, 9284, 337, 341, 390, 14479, 538, 732, 28270, 28813, 5412, 1751, 11, 23046, 12948, 8356, 293, 4389, 17952, 294, 22383, 13, 51414, 51414, 400, 370, 300, 1709, 646, 257, 938, 565, 13, 2264, 13, 407, 510, 307, 264, 1154, 365, 637, 11668, 17720, 13, 440, 38253, 9284, 307, 733, 295, 5124, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13572030371807992, "compression_ratio": 1.554263565891473, "no_speech_prob": 7.5277976065990515e-06}, {"id": 343, "seek": 313300, "start": 3154.0, "end": 3160.0, "text": " And so that goes back a long time. OK. So here is the problem with sparse coding. The inference algorithm is kind of expensive.", "tokens": [50364, 400, 309, 486, 3464, 264, 1185, 281, 767, 915, 257, 10585, 8141, 343, 293, 406, 483, 1314, 365, 445, 1455, 1176, 11639, 13, 50764, 50764, 2264, 13, 407, 300, 311, 637, 11668, 17720, 13, 639, 390, 264, 2539, 9284, 337, 341, 390, 14479, 538, 732, 28270, 28813, 5412, 1751, 11, 23046, 12948, 8356, 293, 4389, 17952, 294, 22383, 13, 51414, 51414, 400, 370, 300, 1709, 646, 257, 938, 565, 13, 2264, 13, 407, 510, 307, 264, 1154, 365, 637, 11668, 17720, 13, 440, 38253, 9284, 307, 733, 295, 5124, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13572030371807992, "compression_ratio": 1.554263565891473, "no_speech_prob": 7.5277976065990515e-06}, {"id": 344, "seek": 316000, "start": 3160.0, "end": 3172.0, "text": " What you have to do is, you know, for a given Y is to kind of minimize the sum of those two terms, one of which is L2, the other one is L1.", "tokens": [50364, 708, 291, 362, 281, 360, 307, 11, 291, 458, 11, 337, 257, 2212, 398, 307, 281, 733, 295, 17522, 264, 2408, 295, 729, 732, 2115, 11, 472, 295, 597, 307, 441, 17, 11, 264, 661, 472, 307, 441, 16, 13, 50964, 50964, 821, 311, 257, 588, 2416, 1230, 295, 10577, 294, 6456, 18666, 300, 2903, 577, 281, 360, 341, 19621, 13, 51314, 51314, 682, 1729, 11, 472, 9284, 281, 360, 370, 307, 1219, 6205, 8241, 13, 663, 1355, 17138, 1166, 23060, 609, 293, 14678, 278, 9284, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0865982915018941, "compression_ratio": 1.5128205128205128, "no_speech_prob": 2.1440928321680985e-05}, {"id": 345, "seek": 316000, "start": 3172.0, "end": 3179.0, "text": " There's a very large number of papers in applied mathematics that explain how to do this efficiently.", "tokens": [50364, 708, 291, 362, 281, 360, 307, 11, 291, 458, 11, 337, 257, 2212, 398, 307, 281, 733, 295, 17522, 264, 2408, 295, 729, 732, 2115, 11, 472, 295, 597, 307, 441, 17, 11, 264, 661, 472, 307, 441, 16, 13, 50964, 50964, 821, 311, 257, 588, 2416, 1230, 295, 10577, 294, 6456, 18666, 300, 2903, 577, 281, 360, 341, 19621, 13, 51314, 51314, 682, 1729, 11, 472, 9284, 281, 360, 370, 307, 1219, 6205, 8241, 13, 663, 1355, 17138, 1166, 23060, 609, 293, 14678, 278, 9284, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0865982915018941, "compression_ratio": 1.5128205128205128, "no_speech_prob": 2.1440928321680985e-05}, {"id": 346, "seek": 316000, "start": 3179.0, "end": 3187.0, "text": " In particular, one algorithm to do so is called ISTA. That means iterative shrinkage and thresholding algorithm.", "tokens": [50364, 708, 291, 362, 281, 360, 307, 11, 291, 458, 11, 337, 257, 2212, 398, 307, 281, 733, 295, 17522, 264, 2408, 295, 729, 732, 2115, 11, 472, 295, 597, 307, 441, 17, 11, 264, 661, 472, 307, 441, 16, 13, 50964, 50964, 821, 311, 257, 588, 2416, 1230, 295, 10577, 294, 6456, 18666, 300, 2903, 577, 281, 360, 341, 19621, 13, 51314, 51314, 682, 1729, 11, 472, 9284, 281, 360, 370, 307, 1219, 6205, 8241, 13, 663, 1355, 17138, 1166, 23060, 609, 293, 14678, 278, 9284, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0865982915018941, "compression_ratio": 1.5128205128205128, "no_speech_prob": 2.1440928321680985e-05}, {"id": 347, "seek": 318700, "start": 3187.0, "end": 3194.0, "text": " And I'm going to tell you what ISTA is in just a minute.", "tokens": [50364, 400, 286, 478, 516, 281, 980, 291, 437, 6205, 8241, 307, 294, 445, 257, 3456, 13, 50714, 50714, 583, 309, 1936, 14689, 294, 1936, 40062, 257, 1333, 295, 4464, 2144, 365, 3104, 281, 1176, 295, 264, 700, 1433, 293, 550, 264, 1150, 1433, 5400, 1592, 13, 51414, 51414, 407, 510, 307, 264, 733, 295, 12649, 1254, 295, 264, 6205, 8241, 9284, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11518429264877782, "compression_ratio": 1.5172413793103448, "no_speech_prob": 2.0783592844964005e-05}, {"id": 348, "seek": 318700, "start": 3194.0, "end": 3208.0, "text": " But it basically consists in basically alternating a sort of minimization with respect to Z of the first term and then the second term alternately.", "tokens": [50364, 400, 286, 478, 516, 281, 980, 291, 437, 6205, 8241, 307, 294, 445, 257, 3456, 13, 50714, 50714, 583, 309, 1936, 14689, 294, 1936, 40062, 257, 1333, 295, 4464, 2144, 365, 3104, 281, 1176, 295, 264, 700, 1433, 293, 550, 264, 1150, 1433, 5400, 1592, 13, 51414, 51414, 407, 510, 307, 264, 733, 295, 12649, 1254, 295, 264, 6205, 8241, 9284, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11518429264877782, "compression_ratio": 1.5172413793103448, "no_speech_prob": 2.0783592844964005e-05}, {"id": 349, "seek": 318700, "start": 3208.0, "end": 3215.0, "text": " So here is the kind of abstract form of the ISTA algorithm.", "tokens": [50364, 400, 286, 478, 516, 281, 980, 291, 437, 6205, 8241, 307, 294, 445, 257, 3456, 13, 50714, 50714, 583, 309, 1936, 14689, 294, 1936, 40062, 257, 1333, 295, 4464, 2144, 365, 3104, 281, 1176, 295, 264, 700, 1433, 293, 550, 264, 1150, 1433, 5400, 1592, 13, 51414, 51414, 407, 510, 307, 264, 733, 295, 12649, 1254, 295, 264, 6205, 8241, 9284, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11518429264877782, "compression_ratio": 1.5172413793103448, "no_speech_prob": 2.0783592844964005e-05}, {"id": 350, "seek": 321500, "start": 3215.0, "end": 3219.0, "text": " There's a fast version of it called called FISTA.", "tokens": [50364, 821, 311, 257, 2370, 3037, 295, 309, 1219, 1219, 479, 2343, 8241, 13, 50564, 50564, 400, 510, 309, 307, 412, 264, 2767, 13, 5135, 11, 286, 478, 16734, 300, 286, 478, 5361, 264, 6408, 337, 264, 6205, 8241, 9284, 13, 51014, 51014, 639, 307, 406, 604, 295, 264, 15400, 286, 478, 4099, 510, 13, 479, 2343, 8241, 307, 1346, 33, 26757, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.214268193100438, "compression_ratio": 1.4805194805194806, "no_speech_prob": 6.0127633332740515e-05}, {"id": 351, "seek": 321500, "start": 3219.0, "end": 3228.0, "text": " And here it is at the bottom. Actually, I'm realizing that I'm missing the reference for the ISTA algorithm.", "tokens": [50364, 821, 311, 257, 2370, 3037, 295, 309, 1219, 1219, 479, 2343, 8241, 13, 50564, 50564, 400, 510, 309, 307, 412, 264, 2767, 13, 5135, 11, 286, 478, 16734, 300, 286, 478, 5361, 264, 6408, 337, 264, 6205, 8241, 9284, 13, 51014, 51014, 639, 307, 406, 604, 295, 264, 15400, 286, 478, 4099, 510, 13, 479, 2343, 8241, 307, 1346, 33, 26757, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.214268193100438, "compression_ratio": 1.4805194805194806, "no_speech_prob": 6.0127633332740515e-05}, {"id": 352, "seek": 321500, "start": 3228.0, "end": 3235.0, "text": " This is not any of the references I'm showing here. FISTA is DeBoule.", "tokens": [50364, 821, 311, 257, 2370, 3037, 295, 309, 1219, 1219, 479, 2343, 8241, 13, 50564, 50564, 400, 510, 309, 307, 412, 264, 2767, 13, 5135, 11, 286, 478, 16734, 300, 286, 478, 5361, 264, 6408, 337, 264, 6205, 8241, 9284, 13, 51014, 51014, 639, 307, 406, 604, 295, 264, 15400, 286, 478, 4099, 510, 13, 479, 2343, 8241, 307, 1346, 33, 26757, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.214268193100438, "compression_ratio": 1.4805194805194806, "no_speech_prob": 6.0127633332740515e-05}, {"id": 353, "seek": 323500, "start": 3235.0, "end": 3247.0, "text": " Anyway, so here is the algorithm. You start with Z equals zero and then you apply this iteration here, which is the second last formula.", "tokens": [50364, 5684, 11, 370, 510, 307, 264, 9284, 13, 509, 722, 365, 1176, 6915, 4018, 293, 550, 291, 3079, 341, 24784, 510, 11, 597, 307, 264, 1150, 1036, 8513, 13, 50964, 50964, 407, 294, 264, 16904, 11, 264, 551, 300, 311, 294, 264, 16904, 307, 1936, 257, 16235, 1823, 294, 264, 8889, 6713, 11, 264, 3732, 31565, 6713, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07827490375887963, "compression_ratio": 1.5574712643678161, "no_speech_prob": 4.26399965363089e-05}, {"id": 354, "seek": 323500, "start": 3247.0, "end": 3256.0, "text": " So in the bracket, the thing that's in the bracket is basically a gradient step in the squared error, the square reconstruction error.", "tokens": [50364, 5684, 11, 370, 510, 307, 264, 9284, 13, 509, 722, 365, 1176, 6915, 4018, 293, 550, 291, 3079, 341, 24784, 510, 11, 597, 307, 264, 1150, 1036, 8513, 13, 50964, 50964, 407, 294, 264, 16904, 11, 264, 551, 300, 311, 294, 264, 16904, 307, 1936, 257, 16235, 1823, 294, 264, 8889, 6713, 11, 264, 3732, 31565, 6713, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07827490375887963, "compression_ratio": 1.5574712643678161, "no_speech_prob": 4.26399965363089e-05}, {"id": 355, "seek": 325600, "start": 3256.0, "end": 3272.0, "text": " So if you compute the gradient of the square reconstruction error and you do a gradient step, you basically get this formula where one over R is the gradient step size.", "tokens": [50364, 407, 498, 291, 14722, 264, 16235, 295, 264, 3732, 31565, 6713, 293, 291, 360, 257, 16235, 1823, 11, 291, 1936, 483, 341, 8513, 689, 472, 670, 497, 307, 264, 16235, 1823, 2744, 13, 51164, 51164, 407, 291, 1936, 5623, 1176, 365, 264, 3671, 16235, 295, 264, 3732, 31565, 6713, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.10192116101582845, "compression_ratio": 1.7847222222222223, "no_speech_prob": 3.6470544728217646e-05}, {"id": 356, "seek": 325600, "start": 3272.0, "end": 3279.0, "text": " So you basically update Z with the negative gradient of the square reconstruction error.", "tokens": [50364, 407, 498, 291, 14722, 264, 16235, 295, 264, 3732, 31565, 6713, 293, 291, 360, 257, 16235, 1823, 11, 291, 1936, 483, 341, 8513, 689, 472, 670, 497, 307, 264, 16235, 1823, 2744, 13, 51164, 51164, 407, 291, 1936, 5623, 1176, 365, 264, 3671, 16235, 295, 264, 3732, 31565, 6713, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.10192116101582845, "compression_ratio": 1.7847222222222223, "no_speech_prob": 3.6470544728217646e-05}, {"id": 357, "seek": 327900, "start": 3279.0, "end": 3289.0, "text": " And then the next operation you do is a shrinkage operation. So you take every component of the resulting Z vector and you shrink all of them towards zero.", "tokens": [50364, 400, 550, 264, 958, 6916, 291, 360, 307, 257, 23060, 609, 6916, 13, 407, 291, 747, 633, 6542, 295, 264, 16505, 1176, 8062, 293, 291, 23060, 439, 295, 552, 3030, 4018, 13, 50864, 50864, 407, 291, 1936, 16390, 498, 264, 6542, 295, 1176, 307, 3353, 11, 291, 16390, 257, 5754, 281, 309, 490, 309, 13, 51164, 51164, 400, 498, 309, 311, 3671, 11, 291, 909, 264, 912, 5754, 281, 309, 13, 583, 498, 291, 483, 886, 1998, 281, 4018, 11, 291, 445, 7353, 412, 4018, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08324002689785427, "compression_ratio": 1.7311320754716981, "no_speech_prob": 3.1195035262499005e-05}, {"id": 358, "seek": 327900, "start": 3289.0, "end": 3295.0, "text": " So you basically subtract if the component of Z is positive, you subtract a constant to it from it.", "tokens": [50364, 400, 550, 264, 958, 6916, 291, 360, 307, 257, 23060, 609, 6916, 13, 407, 291, 747, 633, 6542, 295, 264, 16505, 1176, 8062, 293, 291, 23060, 439, 295, 552, 3030, 4018, 13, 50864, 50864, 407, 291, 1936, 16390, 498, 264, 6542, 295, 1176, 307, 3353, 11, 291, 16390, 257, 5754, 281, 309, 490, 309, 13, 51164, 51164, 400, 498, 309, 311, 3671, 11, 291, 909, 264, 912, 5754, 281, 309, 13, 583, 498, 291, 483, 886, 1998, 281, 4018, 11, 291, 445, 7353, 412, 4018, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08324002689785427, "compression_ratio": 1.7311320754716981, "no_speech_prob": 3.1195035262499005e-05}, {"id": 359, "seek": 327900, "start": 3295.0, "end": 3304.0, "text": " And if it's negative, you add the same constant to it. But if you get too close to zero, you just clip at zero.", "tokens": [50364, 400, 550, 264, 958, 6916, 291, 360, 307, 257, 23060, 609, 6916, 13, 407, 291, 747, 633, 6542, 295, 264, 16505, 1176, 8062, 293, 291, 23060, 439, 295, 552, 3030, 4018, 13, 50864, 50864, 407, 291, 1936, 16390, 498, 264, 6542, 295, 1176, 307, 3353, 11, 291, 16390, 257, 5754, 281, 309, 490, 309, 13, 51164, 51164, 400, 498, 309, 311, 3671, 11, 291, 909, 264, 912, 5754, 281, 309, 13, 583, 498, 291, 483, 886, 1998, 281, 4018, 11, 291, 445, 7353, 412, 4018, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08324002689785427, "compression_ratio": 1.7311320754716981, "no_speech_prob": 3.1195035262499005e-05}, {"id": 360, "seek": 330400, "start": 3304.0, "end": 3320.0, "text": " So basically it's a function that is flat around zero and then grows like the identity function above a certain threshold and below a certain threshold.", "tokens": [50364, 407, 1936, 309, 311, 257, 2445, 300, 307, 4962, 926, 4018, 293, 550, 13156, 411, 264, 6575, 2445, 3673, 257, 1629, 14678, 293, 2507, 257, 1629, 14678, 13, 51164, 51164, 467, 9884, 16431, 3030, 4018, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.09720563888549805, "compression_ratio": 1.4274193548387097, "no_speech_prob": 1.7231062884093262e-05}, {"id": 361, "seek": 330400, "start": 3320.0, "end": 3324.0, "text": " It shrinks towards zero.", "tokens": [50364, 407, 1936, 309, 311, 257, 2445, 300, 307, 4962, 926, 4018, 293, 550, 13156, 411, 264, 6575, 2445, 3673, 257, 1629, 14678, 293, 2507, 257, 1629, 14678, 13, 51164, 51164, 467, 9884, 16431, 3030, 4018, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.09720563888549805, "compression_ratio": 1.4274193548387097, "no_speech_prob": 1.7231062884093262e-05}, {"id": 362, "seek": 332400, "start": 3324.0, "end": 3338.0, "text": " If you keep iterating this algorithm for proper values of L and lambda, the Z vector will converge to the solution of the energy minimization problem,", "tokens": [50364, 759, 291, 1066, 17138, 990, 341, 9284, 337, 2296, 4190, 295, 441, 293, 13607, 11, 264, 1176, 8062, 486, 41881, 281, 264, 3827, 295, 264, 2281, 4464, 2144, 1154, 11, 51064, 51064, 597, 307, 264, 7285, 295, 341, 2281, 510, 11, 462, 295, 398, 57, 365, 3104, 281, 1176, 13, 51314, 51314, 2264, 11, 293, 300, 13409, 13, 407, 1066, 341, 294, 1575, 13, 823, 510, 307, 364, 2734, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1466921342385782, "compression_ratio": 1.4742268041237114, "no_speech_prob": 1.7230950106750242e-05}, {"id": 363, "seek": 332400, "start": 3338.0, "end": 3343.0, "text": " which is the minimum of this energy here, E of YZ with respect to Z.", "tokens": [50364, 759, 291, 1066, 17138, 990, 341, 9284, 337, 2296, 4190, 295, 441, 293, 13607, 11, 264, 1176, 8062, 486, 41881, 281, 264, 3827, 295, 264, 2281, 4464, 2144, 1154, 11, 51064, 51064, 597, 307, 264, 7285, 295, 341, 2281, 510, 11, 462, 295, 398, 57, 365, 3104, 281, 1176, 13, 51314, 51314, 2264, 11, 293, 300, 13409, 13, 407, 1066, 341, 294, 1575, 13, 823, 510, 307, 364, 2734, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1466921342385782, "compression_ratio": 1.4742268041237114, "no_speech_prob": 1.7230950106750242e-05}, {"id": 364, "seek": 332400, "start": 3343.0, "end": 3349.0, "text": " OK, and that suggests. So keep this in mind. Now here is an issue.", "tokens": [50364, 759, 291, 1066, 17138, 990, 341, 9284, 337, 2296, 4190, 295, 441, 293, 13607, 11, 264, 1176, 8062, 486, 41881, 281, 264, 3827, 295, 264, 2281, 4464, 2144, 1154, 11, 51064, 51064, 597, 307, 264, 7285, 295, 341, 2281, 510, 11, 462, 295, 398, 57, 365, 3104, 281, 1176, 13, 51314, 51314, 2264, 11, 293, 300, 13409, 13, 407, 1066, 341, 294, 1575, 13, 823, 510, 307, 364, 2734, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1466921342385782, "compression_ratio": 1.4742268041237114, "no_speech_prob": 1.7230950106750242e-05}, {"id": 365, "seek": 334900, "start": 3349.0, "end": 3358.0, "text": " This algorithm is kind of expensive. If you want to run this over an image or over old patches of an image or something like this,", "tokens": [50364, 639, 9284, 307, 733, 295, 5124, 13, 759, 291, 528, 281, 1190, 341, 670, 364, 3256, 420, 670, 1331, 26531, 295, 364, 3256, 420, 746, 411, 341, 11, 50814, 50814, 291, 434, 406, 516, 281, 312, 1075, 281, 360, 341, 294, 957, 565, 322, 2416, 5267, 13, 51064, 51064, 400, 370, 510, 307, 364, 1558, 13, 440, 1558, 307, 281, 1936, 3847, 257, 18161, 2533, 281, 6069, 437, 264, 3827, 295, 264, 2281, 4464, 2144, 1154, 307, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.05114954273875167, "compression_ratio": 1.580188679245283, "no_speech_prob": 3.647356788860634e-05}, {"id": 366, "seek": 334900, "start": 3358.0, "end": 3363.0, "text": " you're not going to be able to do this in real time on large images.", "tokens": [50364, 639, 9284, 307, 733, 295, 5124, 13, 759, 291, 528, 281, 1190, 341, 670, 364, 3256, 420, 670, 1331, 26531, 295, 364, 3256, 420, 746, 411, 341, 11, 50814, 50814, 291, 434, 406, 516, 281, 312, 1075, 281, 360, 341, 294, 957, 565, 322, 2416, 5267, 13, 51064, 51064, 400, 370, 510, 307, 364, 1558, 13, 440, 1558, 307, 281, 1936, 3847, 257, 18161, 2533, 281, 6069, 437, 264, 3827, 295, 264, 2281, 4464, 2144, 1154, 307, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.05114954273875167, "compression_ratio": 1.580188679245283, "no_speech_prob": 3.647356788860634e-05}, {"id": 367, "seek": 334900, "start": 3363.0, "end": 3371.0, "text": " And so here is an idea. The idea is to basically train a neural net to predict what the solution of the energy minimization problem is.", "tokens": [50364, 639, 9284, 307, 733, 295, 5124, 13, 759, 291, 528, 281, 1190, 341, 670, 364, 3256, 420, 670, 1331, 26531, 295, 364, 3256, 420, 746, 411, 341, 11, 50814, 50814, 291, 434, 406, 516, 281, 312, 1075, 281, 360, 341, 294, 957, 565, 322, 2416, 5267, 13, 51064, 51064, 400, 370, 510, 307, 364, 1558, 13, 440, 1558, 307, 281, 1936, 3847, 257, 18161, 2533, 281, 6069, 437, 264, 3827, 295, 264, 2281, 4464, 2144, 1154, 307, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.05114954273875167, "compression_ratio": 1.580188679245283, "no_speech_prob": 3.647356788860634e-05}, {"id": 368, "seek": 337100, "start": 3371.0, "end": 3380.0, "text": " OK, so you see the diagram here on the right where we train an encoder that takes the Y value.", "tokens": [50364, 2264, 11, 370, 291, 536, 264, 10686, 510, 322, 264, 558, 689, 321, 3847, 364, 2058, 19866, 300, 2516, 264, 398, 2158, 13, 50814, 50814, 1171, 586, 11, 291, 393, 11200, 264, 2522, 300, 5946, 322, 1783, 11, 558, 30, 509, 362, 1783, 516, 281, 257, 6069, 284, 32884, 389, 293, 550, 389, 23712, 666, 264, 2058, 19866, 295, 264, 979, 19866, 13, 51214, 51214, 509, 393, 11200, 341, 644, 337, 586, 13, 682, 264, 47916, 3037, 11, 291, 445, 362, 398, 300, 1709, 281, 364, 2058, 19866, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09986441622498216, "compression_ratio": 1.69377990430622, "no_speech_prob": 8.470002649119124e-05}, {"id": 369, "seek": 337100, "start": 3380.0, "end": 3388.0, "text": " For now, you can ignore the piece that depends on X, right? You have X going to a predictor predicting H and then H feeds into the encoder of the decoder.", "tokens": [50364, 2264, 11, 370, 291, 536, 264, 10686, 510, 322, 264, 558, 689, 321, 3847, 364, 2058, 19866, 300, 2516, 264, 398, 2158, 13, 50814, 50814, 1171, 586, 11, 291, 393, 11200, 264, 2522, 300, 5946, 322, 1783, 11, 558, 30, 509, 362, 1783, 516, 281, 257, 6069, 284, 32884, 389, 293, 550, 389, 23712, 666, 264, 2058, 19866, 295, 264, 979, 19866, 13, 51214, 51214, 509, 393, 11200, 341, 644, 337, 586, 13, 682, 264, 47916, 3037, 11, 291, 445, 362, 398, 300, 1709, 281, 364, 2058, 19866, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09986441622498216, "compression_ratio": 1.69377990430622, "no_speech_prob": 8.470002649119124e-05}, {"id": 370, "seek": 337100, "start": 3388.0, "end": 3394.0, "text": " You can ignore this part for now. In the unconditional version, you just have Y that goes to an encoder.", "tokens": [50364, 2264, 11, 370, 291, 536, 264, 10686, 510, 322, 264, 558, 689, 321, 3847, 364, 2058, 19866, 300, 2516, 264, 398, 2158, 13, 50814, 50814, 1171, 586, 11, 291, 393, 11200, 264, 2522, 300, 5946, 322, 1783, 11, 558, 30, 509, 362, 1783, 516, 281, 257, 6069, 284, 32884, 389, 293, 550, 389, 23712, 666, 264, 2058, 19866, 295, 264, 979, 19866, 13, 51214, 51214, 509, 393, 11200, 341, 644, 337, 586, 13, 682, 264, 47916, 3037, 11, 291, 445, 362, 398, 300, 1709, 281, 364, 2058, 19866, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09986441622498216, "compression_ratio": 1.69377990430622, "no_speech_prob": 8.470002649119124e-05}, {"id": 371, "seek": 339400, "start": 3394.0, "end": 3401.0, "text": " It produces a prediction for what the optimal value of the Z variable is, OK, called Z bar.", "tokens": [50364, 467, 14725, 257, 17630, 337, 437, 264, 16252, 2158, 295, 264, 1176, 7006, 307, 11, 2264, 11, 1219, 1176, 2159, 13, 50714, 50714, 400, 550, 264, 1176, 7006, 2564, 1709, 666, 264, 979, 19866, 13, 467, 1709, 11, 291, 458, 11, 309, 311, 885, 3890, 1602, 382, 731, 293, 550, 14725, 257, 31565, 398, 2159, 13, 51064, 51064, 400, 437, 291, 360, 510, 307, 11, 797, 11, 291, 915, 264, 1176, 2158, 300, 4464, 5660, 264, 2281, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11618482775804473, "compression_ratio": 1.6256410256410256, "no_speech_prob": 3.3209322282345966e-05}, {"id": 372, "seek": 339400, "start": 3401.0, "end": 3408.0, "text": " And then the Z variable itself goes into the decoder. It goes, you know, it's being regularized as well and then produces a reconstruction Y bar.", "tokens": [50364, 467, 14725, 257, 17630, 337, 437, 264, 16252, 2158, 295, 264, 1176, 7006, 307, 11, 2264, 11, 1219, 1176, 2159, 13, 50714, 50714, 400, 550, 264, 1176, 7006, 2564, 1709, 666, 264, 979, 19866, 13, 467, 1709, 11, 291, 458, 11, 309, 311, 885, 3890, 1602, 382, 731, 293, 550, 14725, 257, 31565, 398, 2159, 13, 51064, 51064, 400, 437, 291, 360, 510, 307, 11, 797, 11, 291, 915, 264, 1176, 2158, 300, 4464, 5660, 264, 2281, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11618482775804473, "compression_ratio": 1.6256410256410256, "no_speech_prob": 3.3209322282345966e-05}, {"id": 373, "seek": 339400, "start": 3408.0, "end": 3417.0, "text": " And what you do here is, again, you find the Z value that minimizes the energy.", "tokens": [50364, 467, 14725, 257, 17630, 337, 437, 264, 16252, 2158, 295, 264, 1176, 7006, 307, 11, 2264, 11, 1219, 1176, 2159, 13, 50714, 50714, 400, 550, 264, 1176, 7006, 2564, 1709, 666, 264, 979, 19866, 13, 467, 1709, 11, 291, 458, 11, 309, 311, 885, 3890, 1602, 382, 731, 293, 550, 14725, 257, 31565, 398, 2159, 13, 51064, 51064, 400, 437, 291, 360, 510, 307, 11, 797, 11, 291, 915, 264, 1176, 2158, 300, 4464, 5660, 264, 2281, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11618482775804473, "compression_ratio": 1.6256410256410256, "no_speech_prob": 3.3209322282345966e-05}, {"id": 374, "seek": 341700, "start": 3417.0, "end": 3424.0, "text": " But what we're going to, but the energy now is still the sum of those two terms, C of Y, Y bar and R of Z.", "tokens": [50364, 583, 437, 321, 434, 516, 281, 11, 457, 264, 2281, 586, 307, 920, 264, 2408, 295, 729, 732, 2115, 11, 383, 295, 398, 11, 398, 2159, 293, 497, 295, 1176, 13, 50714, 50714, 583, 550, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 3847, 264, 2058, 19866, 281, 6069, 341, 16252, 2158, 295, 1176, 14879, 807, 4464, 2144, 13, 51264, 51264, 400, 341, 2058, 19866, 307, 516, 281, 312, 8895, 538, 46608, 341, 1433, 413, 295, 1176, 293, 1176, 2159, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09834940866990523, "compression_ratio": 1.6858638743455496, "no_speech_prob": 1.7230135199497454e-05}, {"id": 375, "seek": 341700, "start": 3424.0, "end": 3435.0, "text": " But then what we're going to do is we're going to train the encoder to predict this optimal value of Z obtained through minimization.", "tokens": [50364, 583, 437, 321, 434, 516, 281, 11, 457, 264, 2281, 586, 307, 920, 264, 2408, 295, 729, 732, 2115, 11, 383, 295, 398, 11, 398, 2159, 293, 497, 295, 1176, 13, 50714, 50714, 583, 550, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 3847, 264, 2058, 19866, 281, 6069, 341, 16252, 2158, 295, 1176, 14879, 807, 4464, 2144, 13, 51264, 51264, 400, 341, 2058, 19866, 307, 516, 281, 312, 8895, 538, 46608, 341, 1433, 413, 295, 1176, 293, 1176, 2159, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09834940866990523, "compression_ratio": 1.6858638743455496, "no_speech_prob": 1.7230135199497454e-05}, {"id": 376, "seek": 341700, "start": 3435.0, "end": 3439.0, "text": " And this encoder is going to be trained by minimizing this term D of Z and Z bar.", "tokens": [50364, 583, 437, 321, 434, 516, 281, 11, 457, 264, 2281, 586, 307, 920, 264, 2408, 295, 729, 732, 2115, 11, 383, 295, 398, 11, 398, 2159, 293, 497, 295, 1176, 13, 50714, 50714, 583, 550, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 3847, 264, 2058, 19866, 281, 6069, 341, 16252, 2158, 295, 1176, 14879, 807, 4464, 2144, 13, 51264, 51264, 400, 341, 2058, 19866, 307, 516, 281, 312, 8895, 538, 46608, 341, 1433, 413, 295, 1176, 293, 1176, 2159, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09834940866990523, "compression_ratio": 1.6858638743455496, "no_speech_prob": 1.7230135199497454e-05}, {"id": 377, "seek": 343900, "start": 3439.0, "end": 3452.0, "text": " So basically it views Z as a target value and you train it by back prop, by, you know, gradient descent, to basically make a prediction that's as close to Z as possible.", "tokens": [50364, 407, 1936, 309, 6809, 1176, 382, 257, 3779, 2158, 293, 291, 3847, 309, 538, 646, 2365, 11, 538, 11, 291, 458, 11, 16235, 23475, 11, 281, 1936, 652, 257, 17630, 300, 311, 382, 1998, 281, 1176, 382, 1944, 13, 51014, 51014, 2264, 11, 300, 311, 472, 1254, 295, 341, 1558, 13, 3996, 1254, 295, 341, 1558, 11, 4748, 544, 16950, 11, 307, 300, 562, 291, 434, 884, 264, 4464, 2144, 365, 3104, 281, 1176, 11, 295, 264, 2281, 365, 3104, 281, 1176, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11450164619533496, "compression_ratio": 1.6388888888888888, "no_speech_prob": 1.4284895769378636e-05}, {"id": 378, "seek": 343900, "start": 3452.0, "end": 3466.0, "text": " OK, that's one form of this idea. Another form of this idea, slightly more sophisticated, is that when you're doing the minimization with respect to Z, of the energy with respect to Z,", "tokens": [50364, 407, 1936, 309, 6809, 1176, 382, 257, 3779, 2158, 293, 291, 3847, 309, 538, 646, 2365, 11, 538, 11, 291, 458, 11, 16235, 23475, 11, 281, 1936, 652, 257, 17630, 300, 311, 382, 1998, 281, 1176, 382, 1944, 13, 51014, 51014, 2264, 11, 300, 311, 472, 1254, 295, 341, 1558, 13, 3996, 1254, 295, 341, 1558, 11, 4748, 544, 16950, 11, 307, 300, 562, 291, 434, 884, 264, 4464, 2144, 365, 3104, 281, 1176, 11, 295, 264, 2281, 365, 3104, 281, 1176, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11450164619533496, "compression_ratio": 1.6388888888888888, "no_speech_prob": 1.4284895769378636e-05}, {"id": 379, "seek": 346600, "start": 3466.0, "end": 3471.0, "text": " you take into account the fact that you don't want Z to get too far away from Z bar.", "tokens": [50364, 291, 747, 666, 2696, 264, 1186, 300, 291, 500, 380, 528, 1176, 281, 483, 886, 1400, 1314, 490, 1176, 2159, 13, 50614, 50614, 407, 1936, 428, 2281, 2445, 586, 575, 1045, 2115, 13, 467, 575, 264, 31565, 6713, 13, 467, 575, 264, 3890, 2144, 13, 50964, 50964, 583, 309, 611, 575, 264, 2649, 1296, 264, 1176, 2159, 11, 264, 17630, 490, 264, 2058, 19866, 11, 293, 264, 2190, 2158, 295, 264, 1176, 7006, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.05682245278969789, "compression_ratio": 1.6231155778894473, "no_speech_prob": 1.52061047629104e-05}, {"id": 380, "seek": 346600, "start": 3471.0, "end": 3478.0, "text": " So basically your energy function now has three terms. It has the reconstruction error. It has the regularization.", "tokens": [50364, 291, 747, 666, 2696, 264, 1186, 300, 291, 500, 380, 528, 1176, 281, 483, 886, 1400, 1314, 490, 1176, 2159, 13, 50614, 50614, 407, 1936, 428, 2281, 2445, 586, 575, 1045, 2115, 13, 467, 575, 264, 31565, 6713, 13, 467, 575, 264, 3890, 2144, 13, 50964, 50964, 583, 309, 611, 575, 264, 2649, 1296, 264, 1176, 2159, 11, 264, 17630, 490, 264, 2058, 19866, 11, 293, 264, 2190, 2158, 295, 264, 1176, 7006, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.05682245278969789, "compression_ratio": 1.6231155778894473, "no_speech_prob": 1.52061047629104e-05}, {"id": 381, "seek": 346600, "start": 3478.0, "end": 3487.0, "text": " But it also has the difference between the Z bar, the prediction from the encoder, and the current value of the Z variable.", "tokens": [50364, 291, 747, 666, 2696, 264, 1186, 300, 291, 500, 380, 528, 1176, 281, 483, 886, 1400, 1314, 490, 1176, 2159, 13, 50614, 50614, 407, 1936, 428, 2281, 2445, 586, 575, 1045, 2115, 13, 467, 575, 264, 31565, 6713, 13, 467, 575, 264, 3890, 2144, 13, 50964, 50964, 583, 309, 611, 575, 264, 2649, 1296, 264, 1176, 2159, 11, 264, 17630, 490, 264, 2058, 19866, 11, 293, 264, 2190, 2158, 295, 264, 1176, 7006, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.05682245278969789, "compression_ratio": 1.6231155778894473, "no_speech_prob": 1.52061047629104e-05}, {"id": 382, "seek": 348700, "start": 3487.0, "end": 3500.0, "text": " So the energy function now is written here, E of X, Y, Z is equal to the C function that compares Y and the output of the decoder applied to Z.", "tokens": [50364, 407, 264, 2281, 2445, 586, 307, 3720, 510, 11, 462, 295, 1783, 11, 398, 11, 1176, 307, 2681, 281, 264, 383, 2445, 300, 38334, 398, 293, 264, 5598, 295, 264, 979, 19866, 6456, 281, 1176, 13, 51014, 51014, 639, 307, 264, 47916, 3037, 510, 13, 400, 550, 291, 362, 257, 1150, 1433, 11, 597, 307, 264, 413, 2445, 300, 1333, 295, 8000, 264, 4560, 1296, 1176, 293, 264, 2058, 19866, 6456, 281, 398, 13, 51464, 51464, 821, 4659, 380, 312, 364, 1783, 13, 400, 550, 291, 611, 3890, 1125, 1176, 13, 2264, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10824223154598904, "compression_ratio": 1.6785714285714286, "no_speech_prob": 2.507076715119183e-05}, {"id": 383, "seek": 348700, "start": 3500.0, "end": 3509.0, "text": " This is the unconditional version here. And then you have a second term, which is the D function that sort of measures the distance between Z and the encoder applied to Y.", "tokens": [50364, 407, 264, 2281, 2445, 586, 307, 3720, 510, 11, 462, 295, 1783, 11, 398, 11, 1176, 307, 2681, 281, 264, 383, 2445, 300, 38334, 398, 293, 264, 5598, 295, 264, 979, 19866, 6456, 281, 1176, 13, 51014, 51014, 639, 307, 264, 47916, 3037, 510, 13, 400, 550, 291, 362, 257, 1150, 1433, 11, 597, 307, 264, 413, 2445, 300, 1333, 295, 8000, 264, 4560, 1296, 1176, 293, 264, 2058, 19866, 6456, 281, 398, 13, 51464, 51464, 821, 4659, 380, 312, 364, 1783, 13, 400, 550, 291, 611, 3890, 1125, 1176, 13, 2264, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10824223154598904, "compression_ratio": 1.6785714285714286, "no_speech_prob": 2.507076715119183e-05}, {"id": 384, "seek": 348700, "start": 3509.0, "end": 3516.0, "text": " There shouldn't be an X. And then you also regularize Z. OK.", "tokens": [50364, 407, 264, 2281, 2445, 586, 307, 3720, 510, 11, 462, 295, 1783, 11, 398, 11, 1176, 307, 2681, 281, 264, 383, 2445, 300, 38334, 398, 293, 264, 5598, 295, 264, 979, 19866, 6456, 281, 1176, 13, 51014, 51014, 639, 307, 264, 47916, 3037, 510, 13, 400, 550, 291, 362, 257, 1150, 1433, 11, 597, 307, 264, 413, 2445, 300, 1333, 295, 8000, 264, 4560, 1296, 1176, 293, 264, 2058, 19866, 6456, 281, 398, 13, 51464, 51464, 821, 4659, 380, 312, 364, 1783, 13, 400, 550, 291, 611, 3890, 1125, 1176, 13, 2264, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10824223154598904, "compression_ratio": 1.6785714285714286, "no_speech_prob": 2.507076715119183e-05}, {"id": 385, "seek": 351600, "start": 3516.0, "end": 3530.0, "text": " So basically you're telling the system, find a value for the latent variable that reconstructs, that is sparse, if R is an L1 norm or doesn't have too much information,", "tokens": [50364, 407, 1936, 291, 434, 3585, 264, 1185, 11, 915, 257, 2158, 337, 264, 48994, 7006, 300, 31499, 82, 11, 300, 307, 637, 11668, 11, 498, 497, 307, 364, 441, 16, 2026, 420, 1177, 380, 362, 886, 709, 1589, 11, 51064, 51064, 457, 611, 307, 406, 886, 1400, 1314, 490, 2035, 309, 307, 300, 264, 2058, 19866, 19147, 13, 51264, 51264, 400, 257, 2685, 1558, 456, 307, 1219, 441, 5236, 11, 597, 1355, 264, 2539, 441, 5236, 11, 293, 309, 311, 281, 3909, 264, 9482, 295, 264, 8399, 22660, 19866, 370, 300, 309, 1542, 588, 709, 411, 257, 441, 5236, 9284, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1041671389625186, "compression_ratio": 1.628352490421456, "no_speech_prob": 4.3992626160616055e-05}, {"id": 386, "seek": 351600, "start": 3530.0, "end": 3534.0, "text": " but also is not too far away from whatever it is that the encoder predicted.", "tokens": [50364, 407, 1936, 291, 434, 3585, 264, 1185, 11, 915, 257, 2158, 337, 264, 48994, 7006, 300, 31499, 82, 11, 300, 307, 637, 11668, 11, 498, 497, 307, 364, 441, 16, 2026, 420, 1177, 380, 362, 886, 709, 1589, 11, 51064, 51064, 457, 611, 307, 406, 886, 1400, 1314, 490, 2035, 309, 307, 300, 264, 2058, 19866, 19147, 13, 51264, 51264, 400, 257, 2685, 1558, 456, 307, 1219, 441, 5236, 11, 597, 1355, 264, 2539, 441, 5236, 11, 293, 309, 311, 281, 3909, 264, 9482, 295, 264, 8399, 22660, 19866, 370, 300, 309, 1542, 588, 709, 411, 257, 441, 5236, 9284, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1041671389625186, "compression_ratio": 1.628352490421456, "no_speech_prob": 4.3992626160616055e-05}, {"id": 387, "seek": 351600, "start": 3534.0, "end": 3545.0, "text": " And a specific idea there is called Lista, which means the learning Lista, and it's to shape the architecture of the autoencoder so that it looks very much like a Lista algorithm.", "tokens": [50364, 407, 1936, 291, 434, 3585, 264, 1185, 11, 915, 257, 2158, 337, 264, 48994, 7006, 300, 31499, 82, 11, 300, 307, 637, 11668, 11, 498, 497, 307, 364, 441, 16, 2026, 420, 1177, 380, 362, 886, 709, 1589, 11, 51064, 51064, 457, 611, 307, 406, 886, 1400, 1314, 490, 2035, 309, 307, 300, 264, 2058, 19866, 19147, 13, 51264, 51264, 400, 257, 2685, 1558, 456, 307, 1219, 441, 5236, 11, 597, 1355, 264, 2539, 441, 5236, 11, 293, 309, 311, 281, 3909, 264, 9482, 295, 264, 8399, 22660, 19866, 370, 300, 309, 1542, 588, 709, 411, 257, 441, 5236, 9284, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1041671389625186, "compression_ratio": 1.628352490421456, "no_speech_prob": 4.3992626160616055e-05}, {"id": 388, "seek": 354500, "start": 3545.0, "end": 3561.0, "text": " So if we go back to the Lista algorithm, the formula, the second last formula here, you know, looks like some vector update with some matrix.", "tokens": [50364, 407, 498, 321, 352, 646, 281, 264, 441, 5236, 9284, 11, 264, 8513, 11, 264, 1150, 1036, 8513, 510, 11, 291, 458, 11, 1542, 411, 512, 8062, 5623, 365, 512, 8141, 13, 51164, 51164, 407, 309, 311, 411, 257, 8213, 3233, 295, 257, 18161, 2533, 11, 498, 291, 528, 11, 293, 550, 512, 2107, 12, 1889, 17409, 300, 2314, 281, 312, 257, 23060, 609, 11, 597, 307, 1333, 295, 257, 3834, 1300, 43, 52, 11, 498, 291, 528, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13661447777805558, "compression_ratio": 1.5510204081632653, "no_speech_prob": 3.8217065593926236e-05}, {"id": 389, "seek": 354500, "start": 3561.0, "end": 3569.0, "text": " So it's like a linear stage of a neural net, if you want, and then some non-linearity that happens to be a shrinkage, which is sort of a double ReLU, if you want.", "tokens": [50364, 407, 498, 321, 352, 646, 281, 264, 441, 5236, 9284, 11, 264, 8513, 11, 264, 1150, 1036, 8513, 510, 11, 291, 458, 11, 1542, 411, 512, 8062, 5623, 365, 512, 8141, 13, 51164, 51164, 407, 309, 311, 411, 257, 8213, 3233, 295, 257, 18161, 2533, 11, 498, 291, 528, 11, 293, 550, 512, 2107, 12, 1889, 17409, 300, 2314, 281, 312, 257, 23060, 609, 11, 597, 307, 1333, 295, 257, 3834, 1300, 43, 52, 11, 498, 291, 528, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13661447777805558, "compression_ratio": 1.5510204081632653, "no_speech_prob": 3.8217065593926236e-05}, {"id": 390, "seek": 356900, "start": 3569.0, "end": 3583.0, "text": " It's one ReLU going up married with another ReLU going down. And so if you look at the diagram of this whole Lista algorithm, it looks like this block diagram here that I've drawn on top.", "tokens": [50364, 467, 311, 472, 1300, 43, 52, 516, 493, 5259, 365, 1071, 1300, 43, 52, 516, 760, 13, 400, 370, 498, 291, 574, 412, 264, 10686, 295, 341, 1379, 441, 5236, 9284, 11, 309, 1542, 411, 341, 3461, 10686, 510, 300, 286, 600, 10117, 322, 1192, 13, 51064, 51064, 509, 722, 365, 398, 11, 12972, 309, 538, 512, 8141, 11, 293, 550, 23060, 264, 1874, 13, 1396, 309, 2709, 291, 264, 958, 1176, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09287960498363941, "compression_ratio": 1.4623115577889447, "no_speech_prob": 4.830004399991594e-05}, {"id": 391, "seek": 356900, "start": 3583.0, "end": 3591.0, "text": " You start with Y, multiply it by some matrix, and then shrink the result. Then it gives you the next Z.", "tokens": [50364, 467, 311, 472, 1300, 43, 52, 516, 493, 5259, 365, 1071, 1300, 43, 52, 516, 760, 13, 400, 370, 498, 291, 574, 412, 264, 10686, 295, 341, 1379, 441, 5236, 9284, 11, 309, 1542, 411, 341, 3461, 10686, 510, 300, 286, 600, 10117, 322, 1192, 13, 51064, 51064, 509, 722, 365, 398, 11, 12972, 309, 538, 512, 8141, 11, 293, 550, 23060, 264, 1874, 13, 1396, 309, 2709, 291, 264, 958, 1176, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09287960498363941, "compression_ratio": 1.4623115577889447, "no_speech_prob": 4.830004399991594e-05}, {"id": 392, "seek": 359100, "start": 3591.0, "end": 3601.0, "text": " Apply some other matrix to it, add it to the previous value of Z that you had, shrink again, and then multiply the matrix again, add to the previous value you had, shrink again, et cetera.", "tokens": [50364, 25264, 512, 661, 8141, 281, 309, 11, 909, 309, 281, 264, 3894, 2158, 295, 1176, 300, 291, 632, 11, 23060, 797, 11, 293, 550, 12972, 264, 8141, 797, 11, 909, 281, 264, 3894, 2158, 291, 632, 11, 23060, 797, 11, 1030, 11458, 13, 50864, 50864, 400, 370, 291, 362, 732, 32284, 510, 11, 343, 11, 462, 11, 293, 318, 13, 400, 412, 264, 2767, 11, 498, 291, 6964, 343, 11, 462, 382, 502, 670, 441, 54, 35, 11, 293, 291, 6964, 318, 382, 264, 6575, 3175, 502, 670, 441, 54, 35, 25167, 343, 35, 11, 689, 343, 35, 307, 264, 979, 8616, 8141, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09160711147167065, "compression_ratio": 1.787037037037037, "no_speech_prob": 2.9307566364877857e-05}, {"id": 393, "seek": 359100, "start": 3601.0, "end": 3617.0, "text": " And so you have two matrices here, W, E, and S. And at the bottom, if you define W, E as 1 over LWD, and you define S as the identity minus 1 over LWD transpose WD, where WD is the decoding matrix,", "tokens": [50364, 25264, 512, 661, 8141, 281, 309, 11, 909, 309, 281, 264, 3894, 2158, 295, 1176, 300, 291, 632, 11, 23060, 797, 11, 293, 550, 12972, 264, 8141, 797, 11, 909, 281, 264, 3894, 2158, 291, 632, 11, 23060, 797, 11, 1030, 11458, 13, 50864, 50864, 400, 370, 291, 362, 732, 32284, 510, 11, 343, 11, 462, 11, 293, 318, 13, 400, 412, 264, 2767, 11, 498, 291, 6964, 343, 11, 462, 382, 502, 670, 441, 54, 35, 11, 293, 291, 6964, 318, 382, 264, 6575, 3175, 502, 670, 441, 54, 35, 25167, 343, 35, 11, 689, 343, 35, 307, 264, 979, 8616, 8141, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09160711147167065, "compression_ratio": 1.787037037037037, "no_speech_prob": 2.9307566364877857e-05}, {"id": 394, "seek": 361700, "start": 3617.0, "end": 3623.0, "text": " then this diagram basically implements Lista.", "tokens": [50364, 550, 341, 10686, 1936, 704, 17988, 441, 5236, 13, 50664, 50664, 407, 264, 1558, 300, 472, 295, 452, 5819, 2183, 39966, 82, 11, 14256, 14986, 1321, 11, 632, 390, 281, 584, 11, 731, 11, 983, 500, 380, 321, 2387, 341, 382, 257, 18680, 1753, 18161, 2533, 30, 51064, 51064, 400, 983, 500, 380, 321, 3847, 729, 32284, 343, 293, 318, 370, 382, 281, 976, 505, 257, 665, 28023, 295, 264, 16252, 637, 11668, 3089, 382, 2661, 382, 1944, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10195966513760119, "compression_ratio": 1.4903846153846154, "no_speech_prob": 1.0129709153261501e-05}, {"id": 395, "seek": 361700, "start": 3623.0, "end": 3631.0, "text": " So the idea that one of my former postdocs, Carl Greger, had was to say, well, why don't we treat this as a recurrent neural net?", "tokens": [50364, 550, 341, 10686, 1936, 704, 17988, 441, 5236, 13, 50664, 50664, 407, 264, 1558, 300, 472, 295, 452, 5819, 2183, 39966, 82, 11, 14256, 14986, 1321, 11, 632, 390, 281, 584, 11, 731, 11, 983, 500, 380, 321, 2387, 341, 382, 257, 18680, 1753, 18161, 2533, 30, 51064, 51064, 400, 983, 500, 380, 321, 3847, 729, 32284, 343, 293, 318, 370, 382, 281, 976, 505, 257, 665, 28023, 295, 264, 16252, 637, 11668, 3089, 382, 2661, 382, 1944, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10195966513760119, "compression_ratio": 1.4903846153846154, "no_speech_prob": 1.0129709153261501e-05}, {"id": 396, "seek": 361700, "start": 3631.0, "end": 3641.0, "text": " And why don't we train those matrices W and S so as to give us a good approximation of the optimal sparse code as quickly as possible?", "tokens": [50364, 550, 341, 10686, 1936, 704, 17988, 441, 5236, 13, 50664, 50664, 407, 264, 1558, 300, 472, 295, 452, 5819, 2183, 39966, 82, 11, 14256, 14986, 1321, 11, 632, 390, 281, 584, 11, 731, 11, 983, 500, 380, 321, 2387, 341, 382, 257, 18680, 1753, 18161, 2533, 30, 51064, 51064, 400, 983, 500, 380, 321, 3847, 729, 32284, 343, 293, 318, 370, 382, 281, 976, 505, 257, 665, 28023, 295, 264, 16252, 637, 11668, 3089, 382, 2661, 382, 1944, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10195966513760119, "compression_ratio": 1.4903846153846154, "no_speech_prob": 1.0129709153261501e-05}, {"id": 397, "seek": 364100, "start": 3641.0, "end": 3648.0, "text": " So we're basically going to build our encoder network with this architecture that is copied on Lista.", "tokens": [50364, 407, 321, 434, 1936, 516, 281, 1322, 527, 2058, 19866, 3209, 365, 341, 9482, 300, 307, 25365, 322, 441, 5236, 13, 50714, 50714, 400, 321, 458, 337, 257, 1186, 300, 456, 307, 516, 281, 312, 257, 3827, 689, 264, 1185, 1936, 27152, 264, 2158, 295, 343, 11, 462, 11, 293, 318, 300, 6805, 281, 264, 472, 436, 820, 312, 13, 51414, 51414, 583, 294, 1186, 11, 264, 1185, 27152, 746, 1646, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08378342578285619, "compression_ratio": 1.5656565656565657, "no_speech_prob": 4.832168633583933e-05}, {"id": 398, "seek": 364100, "start": 3648.0, "end": 3662.0, "text": " And we know for a fact that there is going to be a solution where the system basically learns the value of W, E, and S that correspond to the one they should be.", "tokens": [50364, 407, 321, 434, 1936, 516, 281, 1322, 527, 2058, 19866, 3209, 365, 341, 9482, 300, 307, 25365, 322, 441, 5236, 13, 50714, 50714, 400, 321, 458, 337, 257, 1186, 300, 456, 307, 516, 281, 312, 257, 3827, 689, 264, 1185, 1936, 27152, 264, 2158, 295, 343, 11, 462, 11, 293, 318, 300, 6805, 281, 264, 472, 436, 820, 312, 13, 51414, 51414, 583, 294, 1186, 11, 264, 1185, 27152, 746, 1646, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08378342578285619, "compression_ratio": 1.5656565656565657, "no_speech_prob": 4.832168633583933e-05}, {"id": 399, "seek": 364100, "start": 3662.0, "end": 3665.0, "text": " But in fact, the system learns something else.", "tokens": [50364, 407, 321, 434, 1936, 516, 281, 1322, 527, 2058, 19866, 3209, 365, 341, 9482, 300, 307, 25365, 322, 441, 5236, 13, 50714, 50714, 400, 321, 458, 337, 257, 1186, 300, 456, 307, 516, 281, 312, 257, 3827, 689, 264, 1185, 1936, 27152, 264, 2158, 295, 343, 11, 462, 11, 293, 318, 300, 6805, 281, 264, 472, 436, 820, 312, 13, 51414, 51414, 583, 294, 1186, 11, 264, 1185, 27152, 746, 1646, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08378342578285619, "compression_ratio": 1.5656565656565657, "no_speech_prob": 4.832168633583933e-05}, {"id": 400, "seek": 366500, "start": 3665.0, "end": 3673.0, "text": " So this is another representation of it here at the bottom left. We have those shrinkage function and then this S matrix.", "tokens": [50364, 407, 341, 307, 1071, 10290, 295, 309, 510, 412, 264, 2767, 1411, 13, 492, 362, 729, 23060, 609, 2445, 293, 550, 341, 318, 8141, 13, 50764, 50764, 400, 550, 291, 909, 264, 398, 17207, 538, 343, 11, 462, 281, 264, 318, 8141, 11, 23060, 797, 11, 1030, 11458, 11, 1030, 11458, 13, 51064, 51064, 407, 341, 307, 264, 18680, 1753, 2533, 321, 434, 516, 281, 3847, 365, 343, 11, 462, 11, 293, 318, 13, 51314, 51314, 440, 10024, 295, 341, 441, 5236, 307, 11, 393, 291, 7149, 11, 437, 307, 264, 10024, 30, 286, 519, 286, 6721, 257, 935, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11942637883699857, "compression_ratio": 1.6144067796610169, "no_speech_prob": 3.8223945011850446e-05}, {"id": 401, "seek": 366500, "start": 3673.0, "end": 3679.0, "text": " And then you add the Y multiplied by W, E to the S matrix, shrink again, et cetera, et cetera.", "tokens": [50364, 407, 341, 307, 1071, 10290, 295, 309, 510, 412, 264, 2767, 1411, 13, 492, 362, 729, 23060, 609, 2445, 293, 550, 341, 318, 8141, 13, 50764, 50764, 400, 550, 291, 909, 264, 398, 17207, 538, 343, 11, 462, 281, 264, 318, 8141, 11, 23060, 797, 11, 1030, 11458, 11, 1030, 11458, 13, 51064, 51064, 407, 341, 307, 264, 18680, 1753, 2533, 321, 434, 516, 281, 3847, 365, 343, 11, 462, 11, 293, 318, 13, 51314, 51314, 440, 10024, 295, 341, 441, 5236, 307, 11, 393, 291, 7149, 11, 437, 307, 264, 10024, 30, 286, 519, 286, 6721, 257, 935, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11942637883699857, "compression_ratio": 1.6144067796610169, "no_speech_prob": 3.8223945011850446e-05}, {"id": 402, "seek": 366500, "start": 3679.0, "end": 3684.0, "text": " So this is the recurrent net we're going to train with W, E, and S.", "tokens": [50364, 407, 341, 307, 1071, 10290, 295, 309, 510, 412, 264, 2767, 1411, 13, 492, 362, 729, 23060, 609, 2445, 293, 550, 341, 318, 8141, 13, 50764, 50764, 400, 550, 291, 909, 264, 398, 17207, 538, 343, 11, 462, 281, 264, 318, 8141, 11, 23060, 797, 11, 1030, 11458, 11, 1030, 11458, 13, 51064, 51064, 407, 341, 307, 264, 18680, 1753, 2533, 321, 434, 516, 281, 3847, 365, 343, 11, 462, 11, 293, 318, 13, 51314, 51314, 440, 10024, 295, 341, 441, 5236, 307, 11, 393, 291, 7149, 11, 437, 307, 264, 10024, 30, 286, 519, 286, 6721, 257, 935, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11942637883699857, "compression_ratio": 1.6144067796610169, "no_speech_prob": 3.8223945011850446e-05}, {"id": 403, "seek": 366500, "start": 3684.0, "end": 3689.0, "text": " The objective of this Lista is, can you repeat, what is the objective? I think I missed a point.", "tokens": [50364, 407, 341, 307, 1071, 10290, 295, 309, 510, 412, 264, 2767, 1411, 13, 492, 362, 729, 23060, 609, 2445, 293, 550, 341, 318, 8141, 13, 50764, 50764, 400, 550, 291, 909, 264, 398, 17207, 538, 343, 11, 462, 281, 264, 318, 8141, 11, 23060, 797, 11, 1030, 11458, 11, 1030, 11458, 13, 51064, 51064, 407, 341, 307, 264, 18680, 1753, 2533, 321, 434, 516, 281, 3847, 365, 343, 11, 462, 11, 293, 318, 13, 51314, 51314, 440, 10024, 295, 341, 441, 5236, 307, 11, 393, 291, 7149, 11, 437, 307, 264, 10024, 30, 286, 519, 286, 6721, 257, 935, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11942637883699857, "compression_ratio": 1.6144067796610169, "no_speech_prob": 3.8223945011850446e-05}, {"id": 404, "seek": 368900, "start": 3689.0, "end": 3700.0, "text": " So the objective of training this encoder, so the encoder in this diagram on the right here, the architecture of the encoder is the one you see at the bottom left.", "tokens": [50364, 407, 264, 10024, 295, 3097, 341, 2058, 19866, 11, 370, 264, 2058, 19866, 294, 341, 10686, 322, 264, 558, 510, 11, 264, 9482, 295, 264, 2058, 19866, 307, 264, 472, 291, 536, 412, 264, 2767, 1411, 13, 50914, 50914, 400, 264, 10024, 300, 291, 434, 3097, 341, 365, 307, 264, 4274, 295, 413, 295, 1176, 293, 1176, 2159, 13, 51414, 51414, 407, 264, 10747, 307, 11, 294, 264, 1389, 689, 456, 307, 572, 1783, 11, 457, 498, 456, 307, 364, 1783, 11, 309, 1177, 380, 652, 709, 2649, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12636442081902616, "compression_ratio": 1.7246376811594204, "no_speech_prob": 1.834205431805458e-05}, {"id": 405, "seek": 368900, "start": 3700.0, "end": 3710.0, "text": " And the objective that you're training this with is the average of D of Z and Z bar.", "tokens": [50364, 407, 264, 10024, 295, 3097, 341, 2058, 19866, 11, 370, 264, 2058, 19866, 294, 341, 10686, 322, 264, 558, 510, 11, 264, 9482, 295, 264, 2058, 19866, 307, 264, 472, 291, 536, 412, 264, 2767, 1411, 13, 50914, 50914, 400, 264, 10024, 300, 291, 434, 3097, 341, 365, 307, 264, 4274, 295, 413, 295, 1176, 293, 1176, 2159, 13, 51414, 51414, 407, 264, 10747, 307, 11, 294, 264, 1389, 689, 456, 307, 572, 1783, 11, 457, 498, 456, 307, 364, 1783, 11, 309, 1177, 380, 652, 709, 2649, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12636442081902616, "compression_ratio": 1.7246376811594204, "no_speech_prob": 1.834205431805458e-05}, {"id": 406, "seek": 368900, "start": 3710.0, "end": 3717.0, "text": " So the procedure is, in the case where there is no X, but if there is an X, it doesn't make much difference.", "tokens": [50364, 407, 264, 10024, 295, 3097, 341, 2058, 19866, 11, 370, 264, 2058, 19866, 294, 341, 10686, 322, 264, 558, 510, 11, 264, 9482, 295, 264, 2058, 19866, 307, 264, 472, 291, 536, 412, 264, 2767, 1411, 13, 50914, 50914, 400, 264, 10024, 300, 291, 434, 3097, 341, 365, 307, 264, 4274, 295, 413, 295, 1176, 293, 1176, 2159, 13, 51414, 51414, 407, 264, 10747, 307, 11, 294, 264, 1389, 689, 456, 307, 572, 1783, 11, 457, 498, 456, 307, 364, 1783, 11, 309, 1177, 380, 652, 709, 2649, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12636442081902616, "compression_ratio": 1.7246376811594204, "no_speech_prob": 1.834205431805458e-05}, {"id": 407, "seek": 371700, "start": 3717.0, "end": 3724.0, "text": " So take a Y. For this particular Y, find a value of Z that minimizes the energy.", "tokens": [50364, 407, 747, 257, 398, 13, 1171, 341, 1729, 398, 11, 915, 257, 2158, 295, 1176, 300, 4464, 5660, 264, 2281, 13, 50714, 50714, 400, 264, 2281, 307, 264, 2408, 295, 1045, 2115, 11, 383, 295, 398, 11, 398, 2159, 11, 497, 295, 1176, 11, 293, 413, 295, 1176, 11, 1176, 2159, 13, 51014, 51014, 407, 915, 257, 1176, 300, 31499, 82, 11, 575, 13206, 6042, 11, 457, 611, 307, 406, 886, 1400, 1314, 490, 264, 5598, 295, 264, 2058, 19866, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.054653414557961855, "compression_ratio": 1.5027027027027027, "no_speech_prob": 4.6833458327455446e-05}, {"id": 408, "seek": 371700, "start": 3724.0, "end": 3730.0, "text": " And the energy is the sum of three terms, C of Y, Y bar, R of Z, and D of Z, Z bar.", "tokens": [50364, 407, 747, 257, 398, 13, 1171, 341, 1729, 398, 11, 915, 257, 2158, 295, 1176, 300, 4464, 5660, 264, 2281, 13, 50714, 50714, 400, 264, 2281, 307, 264, 2408, 295, 1045, 2115, 11, 383, 295, 398, 11, 398, 2159, 11, 497, 295, 1176, 11, 293, 413, 295, 1176, 11, 1176, 2159, 13, 51014, 51014, 407, 915, 257, 1176, 300, 31499, 82, 11, 575, 13206, 6042, 11, 457, 611, 307, 406, 886, 1400, 1314, 490, 264, 5598, 295, 264, 2058, 19866, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.054653414557961855, "compression_ratio": 1.5027027027027027, "no_speech_prob": 4.6833458327455446e-05}, {"id": 409, "seek": 371700, "start": 3730.0, "end": 3739.0, "text": " So find a Z that reconstructs, has minimal capacity, but also is not too far away from the output of the encoder.", "tokens": [50364, 407, 747, 257, 398, 13, 1171, 341, 1729, 398, 11, 915, 257, 2158, 295, 1176, 300, 4464, 5660, 264, 2281, 13, 50714, 50714, 400, 264, 2281, 307, 264, 2408, 295, 1045, 2115, 11, 383, 295, 398, 11, 398, 2159, 11, 497, 295, 1176, 11, 293, 413, 295, 1176, 11, 1176, 2159, 13, 51014, 51014, 407, 915, 257, 1176, 300, 31499, 82, 11, 575, 13206, 6042, 11, 457, 611, 307, 406, 886, 1400, 1314, 490, 264, 5598, 295, 264, 2058, 19866, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.054653414557961855, "compression_ratio": 1.5027027027027027, "no_speech_prob": 4.6833458327455446e-05}, {"id": 410, "seek": 373900, "start": 3739.0, "end": 3751.0, "text": " Once you have the Z, compute the gradient of the energy with respect to the weights of the decoder, of the encoder, and the predictor if you have one, by back prop.", "tokens": [50364, 3443, 291, 362, 264, 1176, 11, 14722, 264, 16235, 295, 264, 2281, 365, 3104, 281, 264, 17443, 295, 264, 979, 19866, 11, 295, 264, 2058, 19866, 11, 293, 264, 6069, 284, 498, 291, 362, 472, 11, 538, 646, 2365, 13, 50964, 50964, 407, 264, 1880, 551, 307, 300, 264, 787, 16235, 291, 434, 516, 281, 483, 337, 264, 2058, 19866, 307, 264, 16235, 295, 413, 295, 1176, 293, 1176, 2159, 13, 51364, 51364, 407, 264, 2058, 19866, 307, 445, 516, 281, 3847, 2564, 281, 17522, 413, 295, 1176, 293, 1176, 2159, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07279156148433685, "compression_ratio": 1.8808290155440415, "no_speech_prob": 9.309564484283328e-05}, {"id": 411, "seek": 373900, "start": 3751.0, "end": 3759.0, "text": " So the interesting thing is that the only gradient you're going to get for the encoder is the gradient of D of Z and Z bar.", "tokens": [50364, 3443, 291, 362, 264, 1176, 11, 14722, 264, 16235, 295, 264, 2281, 365, 3104, 281, 264, 17443, 295, 264, 979, 19866, 11, 295, 264, 2058, 19866, 11, 293, 264, 6069, 284, 498, 291, 362, 472, 11, 538, 646, 2365, 13, 50964, 50964, 407, 264, 1880, 551, 307, 300, 264, 787, 16235, 291, 434, 516, 281, 483, 337, 264, 2058, 19866, 307, 264, 16235, 295, 413, 295, 1176, 293, 1176, 2159, 13, 51364, 51364, 407, 264, 2058, 19866, 307, 445, 516, 281, 3847, 2564, 281, 17522, 413, 295, 1176, 293, 1176, 2159, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07279156148433685, "compression_ratio": 1.8808290155440415, "no_speech_prob": 9.309564484283328e-05}, {"id": 412, "seek": 373900, "start": 3759.0, "end": 3763.0, "text": " So the encoder is just going to train itself to minimize D of Z and Z bar.", "tokens": [50364, 3443, 291, 362, 264, 1176, 11, 14722, 264, 16235, 295, 264, 2281, 365, 3104, 281, 264, 17443, 295, 264, 979, 19866, 11, 295, 264, 2058, 19866, 11, 293, 264, 6069, 284, 498, 291, 362, 472, 11, 538, 646, 2365, 13, 50964, 50964, 407, 264, 1880, 551, 307, 300, 264, 787, 16235, 291, 434, 516, 281, 483, 337, 264, 2058, 19866, 307, 264, 16235, 295, 413, 295, 1176, 293, 1176, 2159, 13, 51364, 51364, 407, 264, 2058, 19866, 307, 445, 516, 281, 3847, 2564, 281, 17522, 413, 295, 1176, 293, 1176, 2159, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07279156148433685, "compression_ratio": 1.8808290155440415, "no_speech_prob": 9.309564484283328e-05}, {"id": 413, "seek": 376300, "start": 3763.0, "end": 3771.0, "text": " In other words, it's going to train itself to predict Z as well as possible, the optimal Z that you obtain through minimization.", "tokens": [50364, 682, 661, 2283, 11, 309, 311, 516, 281, 3847, 2564, 281, 6069, 1176, 382, 731, 382, 1944, 11, 264, 16252, 1176, 300, 291, 12701, 807, 4464, 2144, 13, 50764, 50764, 440, 979, 19866, 307, 516, 281, 3847, 2564, 281, 11, 295, 1164, 11, 31499, 398, 382, 731, 382, 309, 393, 365, 264, 1176, 300, 307, 885, 2212, 13, 51164, 51164, 400, 550, 498, 291, 362, 257, 6069, 284, 11, 291, 434, 516, 281, 483, 16235, 281, 264, 6069, 284, 293, 309, 311, 516, 281, 853, 281, 733, 295, 5258, 364, 389, 300, 3665, 382, 731, 382, 1944, 13, 51514, 51514, 1119, 300, 1850, 30, 51614, 51614, 865, 11, 3231, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08384108961674205, "compression_ratio": 1.7736625514403292, "no_speech_prob": 0.00012336124200373888}, {"id": 414, "seek": 376300, "start": 3771.0, "end": 3779.0, "text": " The decoder is going to train itself to, of course, reconstruct Y as well as it can with the Z that is being given.", "tokens": [50364, 682, 661, 2283, 11, 309, 311, 516, 281, 3847, 2564, 281, 6069, 1176, 382, 731, 382, 1944, 11, 264, 16252, 1176, 300, 291, 12701, 807, 4464, 2144, 13, 50764, 50764, 440, 979, 19866, 307, 516, 281, 3847, 2564, 281, 11, 295, 1164, 11, 31499, 398, 382, 731, 382, 309, 393, 365, 264, 1176, 300, 307, 885, 2212, 13, 51164, 51164, 400, 550, 498, 291, 362, 257, 6069, 284, 11, 291, 434, 516, 281, 483, 16235, 281, 264, 6069, 284, 293, 309, 311, 516, 281, 853, 281, 733, 295, 5258, 364, 389, 300, 3665, 382, 731, 382, 1944, 13, 51514, 51514, 1119, 300, 1850, 30, 51614, 51614, 865, 11, 3231, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08384108961674205, "compression_ratio": 1.7736625514403292, "no_speech_prob": 0.00012336124200373888}, {"id": 415, "seek": 376300, "start": 3779.0, "end": 3786.0, "text": " And then if you have a predictor, you're going to get gradient to the predictor and it's going to try to kind of produce an H that helps as well as possible.", "tokens": [50364, 682, 661, 2283, 11, 309, 311, 516, 281, 3847, 2564, 281, 6069, 1176, 382, 731, 382, 1944, 11, 264, 16252, 1176, 300, 291, 12701, 807, 4464, 2144, 13, 50764, 50764, 440, 979, 19866, 307, 516, 281, 3847, 2564, 281, 11, 295, 1164, 11, 31499, 398, 382, 731, 382, 309, 393, 365, 264, 1176, 300, 307, 885, 2212, 13, 51164, 51164, 400, 550, 498, 291, 362, 257, 6069, 284, 11, 291, 434, 516, 281, 483, 16235, 281, 264, 6069, 284, 293, 309, 311, 516, 281, 853, 281, 733, 295, 5258, 364, 389, 300, 3665, 382, 731, 382, 1944, 13, 51514, 51514, 1119, 300, 1850, 30, 51614, 51614, 865, 11, 3231, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08384108961674205, "compression_ratio": 1.7736625514403292, "no_speech_prob": 0.00012336124200373888}, {"id": 416, "seek": 376300, "start": 3786.0, "end": 3788.0, "text": " Is that clear?", "tokens": [50364, 682, 661, 2283, 11, 309, 311, 516, 281, 3847, 2564, 281, 6069, 1176, 382, 731, 382, 1944, 11, 264, 16252, 1176, 300, 291, 12701, 807, 4464, 2144, 13, 50764, 50764, 440, 979, 19866, 307, 516, 281, 3847, 2564, 281, 11, 295, 1164, 11, 31499, 398, 382, 731, 382, 309, 393, 365, 264, 1176, 300, 307, 885, 2212, 13, 51164, 51164, 400, 550, 498, 291, 362, 257, 6069, 284, 11, 291, 434, 516, 281, 483, 16235, 281, 264, 6069, 284, 293, 309, 311, 516, 281, 853, 281, 733, 295, 5258, 364, 389, 300, 3665, 382, 731, 382, 1944, 13, 51514, 51514, 1119, 300, 1850, 30, 51614, 51614, 865, 11, 3231, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08384108961674205, "compression_ratio": 1.7736625514403292, "no_speech_prob": 0.00012336124200373888}, {"id": 417, "seek": 376300, "start": 3788.0, "end": 3792.0, "text": " Yeah, thanks.", "tokens": [50364, 682, 661, 2283, 11, 309, 311, 516, 281, 3847, 2564, 281, 6069, 1176, 382, 731, 382, 1944, 11, 264, 16252, 1176, 300, 291, 12701, 807, 4464, 2144, 13, 50764, 50764, 440, 979, 19866, 307, 516, 281, 3847, 2564, 281, 11, 295, 1164, 11, 31499, 398, 382, 731, 382, 309, 393, 365, 264, 1176, 300, 307, 885, 2212, 13, 51164, 51164, 400, 550, 498, 291, 362, 257, 6069, 284, 11, 291, 434, 516, 281, 483, 16235, 281, 264, 6069, 284, 293, 309, 311, 516, 281, 853, 281, 733, 295, 5258, 364, 389, 300, 3665, 382, 731, 382, 1944, 13, 51514, 51514, 1119, 300, 1850, 30, 51614, 51614, 865, 11, 3231, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08384108961674205, "compression_ratio": 1.7736625514403292, "no_speech_prob": 0.00012336124200373888}, {"id": 418, "seek": 379200, "start": 3792.0, "end": 3801.0, "text": " Okay, so that's the architecture. It's basically just a pretty broadened variety recurrent net.", "tokens": [50364, 1033, 11, 370, 300, 311, 264, 9482, 13, 467, 311, 1936, 445, 257, 1238, 4152, 5320, 5673, 18680, 1753, 2533, 13, 50814, 50814, 400, 341, 1985, 534, 731, 294, 264, 2020, 300, 382, 291, 352, 807, 264, 36540, 295, 341, 6205, 8241, 9284, 420, 807, 341, 3847, 18161, 2533, 300, 307, 4761, 281, 1936, 30874, 341, 3827, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.18300139519476122, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.00010228456812910736}, {"id": 419, "seek": 379200, "start": 3801.0, "end": 3821.0, "text": " And this works really well in the sense that as you go through the iterations of this ISTA algorithm or through this train neural net that is designed to basically approximate this solution,", "tokens": [50364, 1033, 11, 370, 300, 311, 264, 9482, 13, 467, 311, 1936, 445, 257, 1238, 4152, 5320, 5673, 18680, 1753, 2533, 13, 50814, 50814, 400, 341, 1985, 534, 731, 294, 264, 2020, 300, 382, 291, 352, 807, 264, 36540, 295, 341, 6205, 8241, 9284, 420, 807, 341, 3847, 18161, 2533, 300, 307, 4761, 281, 1936, 30874, 341, 3827, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.18300139519476122, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.00010228456812910736}, {"id": 420, "seek": 382100, "start": 3821.0, "end": 3829.0, "text": " what you do is you can train the system, for example, to produce the best possible solution after three iterations only.", "tokens": [50364, 437, 291, 360, 307, 291, 393, 3847, 264, 1185, 11, 337, 1365, 11, 281, 5258, 264, 1151, 1944, 3827, 934, 1045, 36540, 787, 13, 50764, 50764, 407, 309, 3255, 264, 16252, 2158, 570, 309, 311, 668, 40610, 365, 6205, 8241, 13, 51014, 51014, 583, 550, 562, 291, 3847, 309, 11, 309, 16329, 2564, 281, 5258, 264, 1151, 28023, 295, 300, 2158, 365, 787, 1045, 36540, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.0562999997820173, "compression_ratio": 1.6524064171122994, "no_speech_prob": 5.143095040693879e-05}, {"id": 421, "seek": 382100, "start": 3829.0, "end": 3834.0, "text": " So it knows the optimal value because it's been computed with ISTA.", "tokens": [50364, 437, 291, 360, 307, 291, 393, 3847, 264, 1185, 11, 337, 1365, 11, 281, 5258, 264, 1151, 1944, 3827, 934, 1045, 36540, 787, 13, 50764, 50764, 407, 309, 3255, 264, 16252, 2158, 570, 309, 311, 668, 40610, 365, 6205, 8241, 13, 51014, 51014, 583, 550, 562, 291, 3847, 309, 11, 309, 16329, 2564, 281, 5258, 264, 1151, 28023, 295, 300, 2158, 365, 787, 1045, 36540, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.0562999997820173, "compression_ratio": 1.6524064171122994, "no_speech_prob": 5.143095040693879e-05}, {"id": 422, "seek": 382100, "start": 3834.0, "end": 3840.0, "text": " But then when you train it, it trains itself to produce the best approximation of that value with only three iterations.", "tokens": [50364, 437, 291, 360, 307, 291, 393, 3847, 264, 1185, 11, 337, 1365, 11, 281, 5258, 264, 1151, 1944, 3827, 934, 1045, 36540, 787, 13, 50764, 50764, 407, 309, 3255, 264, 16252, 2158, 570, 309, 311, 668, 40610, 365, 6205, 8241, 13, 51014, 51014, 583, 550, 562, 291, 3847, 309, 11, 309, 16329, 2564, 281, 5258, 264, 1151, 28023, 295, 300, 2158, 365, 787, 1045, 36540, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.0562999997820173, "compression_ratio": 1.6524064171122994, "no_speech_prob": 5.143095040693879e-05}, {"id": 423, "seek": 384000, "start": 3840.0, "end": 3855.0, "text": " And what we see is that after three iterations, it produces a much, much better approximation than ISTA would produce in three iterations.", "tokens": [50364, 400, 437, 321, 536, 307, 300, 934, 1045, 36540, 11, 309, 14725, 257, 709, 11, 709, 1101, 28023, 813, 6205, 8241, 576, 5258, 294, 1045, 36540, 13, 51114, 51114, 400, 370, 437, 291, 536, 510, 307, 264, 1230, 382, 257, 2445, 295, 1230, 295, 36540, 295, 2139, 6205, 8241, 420, 341, 441, 5236, 9284, 307, 264, 31565, 6713, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08730248042515346, "compression_ratio": 1.6745562130177514, "no_speech_prob": 1.0289019883202855e-05}, {"id": 424, "seek": 384000, "start": 3855.0, "end": 3863.0, "text": " And so what you see here is the number as a function of number of iterations of either ISTA or this Lista algorithm is the reconstruction error.", "tokens": [50364, 400, 437, 321, 536, 307, 300, 934, 1045, 36540, 11, 309, 14725, 257, 709, 11, 709, 1101, 28023, 813, 6205, 8241, 576, 5258, 294, 1045, 36540, 13, 51114, 51114, 400, 370, 437, 291, 536, 510, 307, 264, 1230, 382, 257, 2445, 295, 1230, 295, 36540, 295, 2139, 6205, 8241, 420, 341, 441, 5236, 9284, 307, 264, 31565, 6713, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08730248042515346, "compression_ratio": 1.6745562130177514, "no_speech_prob": 1.0289019883202855e-05}, {"id": 425, "seek": 386300, "start": 3863.0, "end": 3873.0, "text": " So by training an encoder to kind of predict the result of the optimization, you actually get better results than if you actually run the optimization for the same number of iterations.", "tokens": [50364, 407, 538, 3097, 364, 2058, 19866, 281, 733, 295, 6069, 264, 1874, 295, 264, 19618, 11, 291, 767, 483, 1101, 3542, 813, 498, 291, 767, 1190, 264, 19618, 337, 264, 912, 1230, 295, 36540, 13, 50864, 50864, 407, 309, 10172, 1024, 38253, 257, 688, 13, 50964, 50964, 1033, 11, 370, 341, 307, 437, 637, 11668, 17720, 2709, 291, 365, 420, 1553, 364, 2058, 19866, 13, 51264, 51264, 5135, 11, 291, 483, 1238, 709, 264, 912, 3542, 510, 562, 291, 3847, 322, 376, 45, 19756, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.09528664792521616, "compression_ratio": 1.663677130044843, "no_speech_prob": 2.4680903152329847e-05}, {"id": 426, "seek": 386300, "start": 3873.0, "end": 3875.0, "text": " So it accelerates inference a lot.", "tokens": [50364, 407, 538, 3097, 364, 2058, 19866, 281, 733, 295, 6069, 264, 1874, 295, 264, 19618, 11, 291, 767, 483, 1101, 3542, 813, 498, 291, 767, 1190, 264, 19618, 337, 264, 912, 1230, 295, 36540, 13, 50864, 50864, 407, 309, 10172, 1024, 38253, 257, 688, 13, 50964, 50964, 1033, 11, 370, 341, 307, 437, 637, 11668, 17720, 2709, 291, 365, 420, 1553, 364, 2058, 19866, 13, 51264, 51264, 5135, 11, 291, 483, 1238, 709, 264, 912, 3542, 510, 562, 291, 3847, 322, 376, 45, 19756, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.09528664792521616, "compression_ratio": 1.663677130044843, "no_speech_prob": 2.4680903152329847e-05}, {"id": 427, "seek": 386300, "start": 3875.0, "end": 3881.0, "text": " Okay, so this is what sparse coding gives you with or without an encoder.", "tokens": [50364, 407, 538, 3097, 364, 2058, 19866, 281, 733, 295, 6069, 264, 1874, 295, 264, 19618, 11, 291, 767, 483, 1101, 3542, 813, 498, 291, 767, 1190, 264, 19618, 337, 264, 912, 1230, 295, 36540, 13, 50864, 50864, 407, 309, 10172, 1024, 38253, 257, 688, 13, 50964, 50964, 1033, 11, 370, 341, 307, 437, 637, 11668, 17720, 2709, 291, 365, 420, 1553, 364, 2058, 19866, 13, 51264, 51264, 5135, 11, 291, 483, 1238, 709, 264, 912, 3542, 510, 562, 291, 3847, 322, 376, 45, 19756, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.09528664792521616, "compression_ratio": 1.663677130044843, "no_speech_prob": 2.4680903152329847e-05}, {"id": 428, "seek": 386300, "start": 3881.0, "end": 3884.0, "text": " Actually, you get pretty much the same results here when you train on MNIST.", "tokens": [50364, 407, 538, 3097, 364, 2058, 19866, 281, 733, 295, 6069, 264, 1874, 295, 264, 19618, 11, 291, 767, 483, 1101, 3542, 813, 498, 291, 767, 1190, 264, 19618, 337, 264, 912, 1230, 295, 36540, 13, 50864, 50864, 407, 309, 10172, 1024, 38253, 257, 688, 13, 50964, 50964, 1033, 11, 370, 341, 307, 437, 637, 11668, 17720, 2709, 291, 365, 420, 1553, 364, 2058, 19866, 13, 51264, 51264, 5135, 11, 291, 483, 1238, 709, 264, 912, 3542, 510, 562, 291, 3847, 322, 376, 45, 19756, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.09528664792521616, "compression_ratio": 1.663677130044843, "no_speech_prob": 2.4680903152329847e-05}, {"id": 429, "seek": 388400, "start": 3884.0, "end": 3896.0, "text": " So these are basically a linear decoder. The code space here, the Z vector, has size 256.", "tokens": [50364, 407, 613, 366, 1936, 257, 8213, 979, 19866, 13, 440, 3089, 1901, 510, 11, 264, 1176, 8062, 11, 575, 2744, 38882, 13, 50964, 50964, 400, 370, 291, 747, 341, 38882, 1176, 8062, 17207, 538, 8141, 293, 291, 31499, 257, 14293, 13, 51364, 51364, 400, 437, 291, 536, 510, 366, 264, 13766, 295, 341, 8141, 10379, 382, 5267, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12339767332999937, "compression_ratio": 1.447674418604651, "no_speech_prob": 1.0448407920193858e-05}, {"id": 430, "seek": 388400, "start": 3896.0, "end": 3904.0, "text": " And so you take this 256 Z vector multiplied by matrix and you reconstruct a digit.", "tokens": [50364, 407, 613, 366, 1936, 257, 8213, 979, 19866, 13, 440, 3089, 1901, 510, 11, 264, 1176, 8062, 11, 575, 2744, 38882, 13, 50964, 50964, 400, 370, 291, 747, 341, 38882, 1176, 8062, 17207, 538, 8141, 293, 291, 31499, 257, 14293, 13, 51364, 51364, 400, 437, 291, 536, 510, 366, 264, 13766, 295, 341, 8141, 10379, 382, 5267, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12339767332999937, "compression_ratio": 1.447674418604651, "no_speech_prob": 1.0448407920193858e-05}, {"id": 431, "seek": 388400, "start": 3904.0, "end": 3911.0, "text": " And what you see here are the columns of this matrix represented as images.", "tokens": [50364, 407, 613, 366, 1936, 257, 8213, 979, 19866, 13, 440, 3089, 1901, 510, 11, 264, 1176, 8062, 11, 575, 2744, 38882, 13, 50964, 50964, 400, 370, 291, 747, 341, 38882, 1176, 8062, 17207, 538, 8141, 293, 291, 31499, 257, 14293, 13, 51364, 51364, 400, 437, 291, 536, 510, 366, 264, 13766, 295, 341, 8141, 10379, 382, 5267, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12339767332999937, "compression_ratio": 1.447674418604651, "no_speech_prob": 1.0448407920193858e-05}, {"id": 432, "seek": 391100, "start": 3911.0, "end": 3915.0, "text": " So each column has the same dimension as an MNIST digit, right?", "tokens": [50364, 407, 1184, 7738, 575, 264, 912, 10139, 382, 364, 376, 45, 19756, 14293, 11, 558, 30, 50564, 50564, 6947, 7738, 295, 343, 13, 400, 370, 291, 393, 2906, 1184, 295, 552, 382, 364, 3256, 13, 50814, 50814, 400, 613, 366, 264, 38882, 13766, 295, 343, 13, 51064, 51064, 400, 437, 291, 536, 307, 300, 436, 1936, 2906, 3166, 295, 4342, 411, 707, 3755, 295, 24493, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07952774592808315, "compression_ratio": 1.5112359550561798, "no_speech_prob": 6.013759411871433e-05}, {"id": 433, "seek": 391100, "start": 3915.0, "end": 3920.0, "text": " Each column of W. And so you can represent each of them as an image.", "tokens": [50364, 407, 1184, 7738, 575, 264, 912, 10139, 382, 364, 376, 45, 19756, 14293, 11, 558, 30, 50564, 50564, 6947, 7738, 295, 343, 13, 400, 370, 291, 393, 2906, 1184, 295, 552, 382, 364, 3256, 13, 50814, 50814, 400, 613, 366, 264, 38882, 13766, 295, 343, 13, 51064, 51064, 400, 437, 291, 536, 307, 300, 436, 1936, 2906, 3166, 295, 4342, 411, 707, 3755, 295, 24493, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07952774592808315, "compression_ratio": 1.5112359550561798, "no_speech_prob": 6.013759411871433e-05}, {"id": 434, "seek": 391100, "start": 3920.0, "end": 3925.0, "text": " And these are the 256 columns of W.", "tokens": [50364, 407, 1184, 7738, 575, 264, 912, 10139, 382, 364, 376, 45, 19756, 14293, 11, 558, 30, 50564, 50564, 6947, 7738, 295, 343, 13, 400, 370, 291, 393, 2906, 1184, 295, 552, 382, 364, 3256, 13, 50814, 50814, 400, 613, 366, 264, 38882, 13766, 295, 343, 13, 51064, 51064, 400, 437, 291, 536, 307, 300, 436, 1936, 2906, 3166, 295, 4342, 411, 707, 3755, 295, 24493, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07952774592808315, "compression_ratio": 1.5112359550561798, "no_speech_prob": 6.013759411871433e-05}, {"id": 435, "seek": 391100, "start": 3925.0, "end": 3932.0, "text": " And what you see is that they basically represent parts of characters like little pieces of strokes.", "tokens": [50364, 407, 1184, 7738, 575, 264, 912, 10139, 382, 364, 376, 45, 19756, 14293, 11, 558, 30, 50564, 50564, 6947, 7738, 295, 343, 13, 400, 370, 291, 393, 2906, 1184, 295, 552, 382, 364, 3256, 13, 50814, 50814, 400, 613, 366, 264, 38882, 13766, 295, 343, 13, 51064, 51064, 400, 437, 291, 536, 307, 300, 436, 1936, 2906, 3166, 295, 4342, 411, 707, 3755, 295, 24493, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07952774592808315, "compression_ratio": 1.5112359550561798, "no_speech_prob": 6.013759411871433e-05}, {"id": 436, "seek": 393200, "start": 3932.0, "end": 3943.0, "text": " And the reason for this is that you can basically reconstruct any character, any MNIST digit by a linear combination of a small number of those strokes.", "tokens": [50364, 400, 264, 1778, 337, 341, 307, 300, 291, 393, 1936, 31499, 604, 2517, 11, 604, 376, 45, 19756, 14293, 538, 257, 8213, 6562, 295, 257, 1359, 1230, 295, 729, 24493, 13, 50914, 50914, 400, 370, 300, 311, 733, 295, 2238, 570, 341, 1185, 1936, 10704, 23079, 17254, 3166, 295, 6565, 294, 257, 2584, 2693, 12879, 24420, 636, 13, 51414, 51414, 400, 300, 311, 733, 295, 437, 291, 528, 484, 295, 2693, 12879, 24420, 2539, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06727944144719764, "compression_ratio": 1.5844748858447488, "no_speech_prob": 2.3551639969809912e-05}, {"id": 437, "seek": 393200, "start": 3943.0, "end": 3953.0, "text": " And so that's kind of beautiful because this system basically finds constitutive parts of objects in a completely unsupervised way.", "tokens": [50364, 400, 264, 1778, 337, 341, 307, 300, 291, 393, 1936, 31499, 604, 2517, 11, 604, 376, 45, 19756, 14293, 538, 257, 8213, 6562, 295, 257, 1359, 1230, 295, 729, 24493, 13, 50914, 50914, 400, 370, 300, 311, 733, 295, 2238, 570, 341, 1185, 1936, 10704, 23079, 17254, 3166, 295, 6565, 294, 257, 2584, 2693, 12879, 24420, 636, 13, 51414, 51414, 400, 300, 311, 733, 295, 437, 291, 528, 484, 295, 2693, 12879, 24420, 2539, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06727944144719764, "compression_ratio": 1.5844748858447488, "no_speech_prob": 2.3551639969809912e-05}, {"id": 438, "seek": 393200, "start": 3953.0, "end": 3956.0, "text": " And that's kind of what you want out of unsupervised learning.", "tokens": [50364, 400, 264, 1778, 337, 341, 307, 300, 291, 393, 1936, 31499, 604, 2517, 11, 604, 376, 45, 19756, 14293, 538, 257, 8213, 6562, 295, 257, 1359, 1230, 295, 729, 24493, 13, 50914, 50914, 400, 370, 300, 311, 733, 295, 2238, 570, 341, 1185, 1936, 10704, 23079, 17254, 3166, 295, 6565, 294, 257, 2584, 2693, 12879, 24420, 636, 13, 51414, 51414, 400, 300, 311, 733, 295, 437, 291, 528, 484, 295, 2693, 12879, 24420, 2539, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06727944144719764, "compression_ratio": 1.5844748858447488, "no_speech_prob": 2.3551639969809912e-05}, {"id": 439, "seek": 395600, "start": 3956.0, "end": 3963.0, "text": " You want sort of what are the components or the parts that can explain what my data looks like.", "tokens": [50364, 509, 528, 1333, 295, 437, 366, 264, 6677, 420, 264, 3166, 300, 393, 2903, 437, 452, 1412, 1542, 411, 13, 50714, 50714, 407, 341, 1985, 534, 16525, 337, 376, 45, 19756, 13, 467, 1985, 1596, 9594, 382, 731, 337, 3303, 3256, 26531, 13, 51164, 51164, 821, 311, 3442, 281, 312, 364, 9603, 510, 11, 457, 291, 434, 406, 2577, 309, 2745, 570, 309, 311, 17752, 13, 51364, 51364, 583, 264, 1874, 307, 341, 13, 407, 264, 9603, 3110, 264, 2539, 9284, 1940, 1081, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07217500426552513, "compression_ratio": 1.530612244897959, "no_speech_prob": 2.586613300081808e-05}, {"id": 440, "seek": 395600, "start": 3963.0, "end": 3972.0, "text": " So this works really beautifully for MNIST. It works quite nicely as well for natural image patches.", "tokens": [50364, 509, 528, 1333, 295, 437, 366, 264, 6677, 420, 264, 3166, 300, 393, 2903, 437, 452, 1412, 1542, 411, 13, 50714, 50714, 407, 341, 1985, 534, 16525, 337, 376, 45, 19756, 13, 467, 1985, 1596, 9594, 382, 731, 337, 3303, 3256, 26531, 13, 51164, 51164, 821, 311, 3442, 281, 312, 364, 9603, 510, 11, 457, 291, 434, 406, 2577, 309, 2745, 570, 309, 311, 17752, 13, 51364, 51364, 583, 264, 1874, 307, 341, 13, 407, 264, 9603, 3110, 264, 2539, 9284, 1940, 1081, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07217500426552513, "compression_ratio": 1.530612244897959, "no_speech_prob": 2.586613300081808e-05}, {"id": 441, "seek": 395600, "start": 3972.0, "end": 3976.0, "text": " There's supposed to be an animation here, but you're not seeing it obviously because it's PDF.", "tokens": [50364, 509, 528, 1333, 295, 437, 366, 264, 6677, 420, 264, 3166, 300, 393, 2903, 437, 452, 1412, 1542, 411, 13, 50714, 50714, 407, 341, 1985, 534, 16525, 337, 376, 45, 19756, 13, 467, 1985, 1596, 9594, 382, 731, 337, 3303, 3256, 26531, 13, 51164, 51164, 821, 311, 3442, 281, 312, 364, 9603, 510, 11, 457, 291, 434, 406, 2577, 309, 2745, 570, 309, 311, 17752, 13, 51364, 51364, 583, 264, 1874, 307, 341, 13, 407, 264, 9603, 3110, 264, 2539, 9284, 1940, 1081, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07217500426552513, "compression_ratio": 1.530612244897959, "no_speech_prob": 2.586613300081808e-05}, {"id": 442, "seek": 395600, "start": 3976.0, "end": 3981.0, "text": " But the result is this. So the animation shows the learning algorithm taking place.", "tokens": [50364, 509, 528, 1333, 295, 437, 366, 264, 6677, 420, 264, 3166, 300, 393, 2903, 437, 452, 1412, 1542, 411, 13, 50714, 50714, 407, 341, 1985, 534, 16525, 337, 376, 45, 19756, 13, 467, 1985, 1596, 9594, 382, 731, 337, 3303, 3256, 26531, 13, 51164, 51164, 821, 311, 3442, 281, 312, 364, 9603, 510, 11, 457, 291, 434, 406, 2577, 309, 2745, 570, 309, 311, 17752, 13, 51364, 51364, 583, 264, 1874, 307, 341, 13, 407, 264, 9603, 3110, 264, 2539, 9284, 1940, 1081, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07217500426552513, "compression_ratio": 1.530612244897959, "no_speech_prob": 2.586613300081808e-05}, {"id": 443, "seek": 398100, "start": 3981.0, "end": 3992.0, "text": " So here again, these are the columns of the decoding matrix of a sparse coding system with L1 regularization that has been trained on natural image patches.", "tokens": [50364, 407, 510, 797, 11, 613, 366, 264, 13766, 295, 264, 979, 8616, 8141, 295, 257, 637, 11668, 17720, 1185, 365, 441, 16, 3890, 2144, 300, 575, 668, 8895, 322, 3303, 3256, 26531, 13, 50914, 50914, 400, 286, 848, 300, 729, 3303, 3256, 26531, 362, 668, 47548, 5320, 11, 597, 1355, 436, 600, 668, 1333, 295, 48704, 294, 512, 636, 11, 291, 458, 11, 10373, 264, 914, 293, 733, 295, 2710, 1125, 264, 21977, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.09830745061238606, "compression_ratio": 1.6066350710900474, "no_speech_prob": 4.539199289865792e-05}, {"id": 444, "seek": 398100, "start": 3992.0, "end": 4001.0, "text": " And I said that those natural image patches have been whitened, which means they've been sort of normalized in some way, you know, cancel the mean and kind of normalize the variance.", "tokens": [50364, 407, 510, 797, 11, 613, 366, 264, 13766, 295, 264, 979, 8616, 8141, 295, 257, 637, 11668, 17720, 1185, 365, 441, 16, 3890, 2144, 300, 575, 668, 8895, 322, 3303, 3256, 26531, 13, 50914, 50914, 400, 286, 848, 300, 729, 3303, 3256, 26531, 362, 668, 47548, 5320, 11, 597, 1355, 436, 600, 668, 1333, 295, 48704, 294, 512, 636, 11, 291, 458, 11, 10373, 264, 914, 293, 733, 295, 2710, 1125, 264, 21977, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.09830745061238606, "compression_ratio": 1.6066350710900474, "no_speech_prob": 4.539199289865792e-05}, {"id": 445, "seek": 400100, "start": 4001.0, "end": 4014.0, "text": " And you get nice little what's called Gabor filters. So basically, you know, small edge detectors at various orientations, locations and sizes.", "tokens": [50364, 400, 291, 483, 1481, 707, 437, 311, 1219, 460, 3816, 15995, 13, 407, 1936, 11, 291, 458, 11, 1359, 4691, 46866, 412, 3683, 8579, 763, 11, 9253, 293, 11602, 13, 51014, 51014, 407, 264, 1778, 983, 341, 390, 14479, 538, 28813, 5412, 1751, 307, 300, 341, 1542, 588, 709, 411, 437, 291, 11441, 294, 264, 6194, 1859, 295, 264, 5056, 33312, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10678893869573419, "compression_ratio": 1.5077720207253886, "no_speech_prob": 5.475039142766036e-05}, {"id": 446, "seek": 400100, "start": 4014.0, "end": 4023.0, "text": " So the reason why this was invented by neuroscientists is that this looks very much like what you observe in the primary area of the visual cortex.", "tokens": [50364, 400, 291, 483, 1481, 707, 437, 311, 1219, 460, 3816, 15995, 13, 407, 1936, 11, 291, 458, 11, 1359, 4691, 46866, 412, 3683, 8579, 763, 11, 9253, 293, 11602, 13, 51014, 51014, 407, 264, 1778, 983, 341, 390, 14479, 538, 28813, 5412, 1751, 307, 300, 341, 1542, 588, 709, 411, 437, 291, 11441, 294, 264, 6194, 1859, 295, 264, 5056, 33312, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10678893869573419, "compression_ratio": 1.5077720207253886, "no_speech_prob": 5.475039142766036e-05}, {"id": 447, "seek": 402300, "start": 4023.0, "end": 4039.0, "text": " When you poke electrodes in the visual cortex of most animals and you figure out what patterns do they maximally respond to, they may actually respond to oriented edges.", "tokens": [50364, 1133, 291, 19712, 47824, 294, 264, 5056, 33312, 295, 881, 4882, 293, 291, 2573, 484, 437, 8294, 360, 436, 5138, 379, 4196, 281, 11, 436, 815, 767, 4196, 281, 21841, 8819, 13, 51164, 51164, 639, 307, 611, 437, 291, 11441, 562, 291, 3847, 257, 45216, 304, 2533, 13, 1282, 29903, 31890, 11, 264, 700, 4583, 295, 4122, 574, 588, 709, 411, 341, 382, 731, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11340854478918988, "compression_ratio": 1.5270935960591132, "no_speech_prob": 4.399367026053369e-05}, {"id": 448, "seek": 402300, "start": 4039.0, "end": 4047.0, "text": " This is also what you observe when you train a convolutional net. On ImageNet, the first layer of features look very much like this as well.", "tokens": [50364, 1133, 291, 19712, 47824, 294, 264, 5056, 33312, 295, 881, 4882, 293, 291, 2573, 484, 437, 8294, 360, 436, 5138, 379, 4196, 281, 11, 436, 815, 767, 4196, 281, 21841, 8819, 13, 51164, 51164, 639, 307, 611, 437, 291, 11441, 562, 291, 3847, 257, 45216, 304, 2533, 13, 1282, 29903, 31890, 11, 264, 700, 4583, 295, 4122, 574, 588, 709, 411, 341, 382, 731, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11340854478918988, "compression_ratio": 1.5270935960591132, "no_speech_prob": 4.399367026053369e-05}, {"id": 449, "seek": 404700, "start": 4047.0, "end": 4055.0, "text": " Except they are convolutional. These ones are not convolutional. It's trained on image patches, but there is no convolutions here.", "tokens": [50364, 16192, 436, 366, 45216, 304, 13, 1981, 2306, 366, 406, 45216, 304, 13, 467, 311, 8895, 322, 3256, 26531, 11, 457, 456, 307, 572, 3754, 15892, 510, 13, 50764, 50764, 407, 341, 307, 1481, 570, 437, 309, 5112, 291, 307, 300, 365, 257, 588, 2199, 2693, 12879, 24420, 2539, 9284, 11, 321, 483, 4476, 31312, 356, 264, 912, 4122, 300, 321, 576, 483, 538, 3097, 257, 2416, 45216, 304, 2533, 46533, 13, 51364, 51364, 407, 300, 2709, 291, 257, 12075, 13, 51464, 51464, 407, 510, 307, 264, 45216, 304, 3037, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08996009826660156, "compression_ratio": 1.7965367965367964, "no_speech_prob": 4.683325096266344e-05}, {"id": 450, "seek": 404700, "start": 4055.0, "end": 4067.0, "text": " So this is nice because what it tells you is that with a very simple unsupervised learning algorithm, we get essentially qualitatively the same features that we would get by training a large convolutional net supervised.", "tokens": [50364, 16192, 436, 366, 45216, 304, 13, 1981, 2306, 366, 406, 45216, 304, 13, 467, 311, 8895, 322, 3256, 26531, 11, 457, 456, 307, 572, 3754, 15892, 510, 13, 50764, 50764, 407, 341, 307, 1481, 570, 437, 309, 5112, 291, 307, 300, 365, 257, 588, 2199, 2693, 12879, 24420, 2539, 9284, 11, 321, 483, 4476, 31312, 356, 264, 912, 4122, 300, 321, 576, 483, 538, 3097, 257, 2416, 45216, 304, 2533, 46533, 13, 51364, 51364, 407, 300, 2709, 291, 257, 12075, 13, 51464, 51464, 407, 510, 307, 264, 45216, 304, 3037, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08996009826660156, "compression_ratio": 1.7965367965367964, "no_speech_prob": 4.683325096266344e-05}, {"id": 451, "seek": 404700, "start": 4067.0, "end": 4069.0, "text": " So that gives you a hint.", "tokens": [50364, 16192, 436, 366, 45216, 304, 13, 1981, 2306, 366, 406, 45216, 304, 13, 467, 311, 8895, 322, 3256, 26531, 11, 457, 456, 307, 572, 3754, 15892, 510, 13, 50764, 50764, 407, 341, 307, 1481, 570, 437, 309, 5112, 291, 307, 300, 365, 257, 588, 2199, 2693, 12879, 24420, 2539, 9284, 11, 321, 483, 4476, 31312, 356, 264, 912, 4122, 300, 321, 576, 483, 538, 3097, 257, 2416, 45216, 304, 2533, 46533, 13, 51364, 51364, 407, 300, 2709, 291, 257, 12075, 13, 51464, 51464, 407, 510, 307, 264, 45216, 304, 3037, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08996009826660156, "compression_ratio": 1.7965367965367964, "no_speech_prob": 4.683325096266344e-05}, {"id": 452, "seek": 404700, "start": 4069.0, "end": 4073.0, "text": " So here is the convolutional version.", "tokens": [50364, 16192, 436, 366, 45216, 304, 13, 1981, 2306, 366, 406, 45216, 304, 13, 467, 311, 8895, 322, 3256, 26531, 11, 457, 456, 307, 572, 3754, 15892, 510, 13, 50764, 50764, 407, 341, 307, 1481, 570, 437, 309, 5112, 291, 307, 300, 365, 257, 588, 2199, 2693, 12879, 24420, 2539, 9284, 11, 321, 483, 4476, 31312, 356, 264, 912, 4122, 300, 321, 576, 483, 538, 3097, 257, 2416, 45216, 304, 2533, 46533, 13, 51364, 51364, 407, 300, 2709, 291, 257, 12075, 13, 51464, 51464, 407, 510, 307, 264, 45216, 304, 3037, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08996009826660156, "compression_ratio": 1.7965367965367964, "no_speech_prob": 4.683325096266344e-05}, {"id": 453, "seek": 407300, "start": 4073.0, "end": 4079.0, "text": " So the convolutional version basically says you have an image.", "tokens": [50364, 407, 264, 45216, 304, 3037, 1936, 1619, 291, 362, 364, 3256, 13, 50664, 50664, 639, 307, 544, 21826, 11, 538, 264, 636, 13, 50864, 50864, 708, 291, 434, 516, 281, 360, 307, 291, 434, 516, 281, 747, 4111, 11317, 13, 51364, 51364, 1033, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12437655528386433, "compression_ratio": 1.3253968253968254, "no_speech_prob": 9.459553984925151e-05}, {"id": 454, "seek": 407300, "start": 4079.0, "end": 4083.0, "text": " This is more responsive, by the way.", "tokens": [50364, 407, 264, 45216, 304, 3037, 1936, 1619, 291, 362, 364, 3256, 13, 50664, 50664, 639, 307, 544, 21826, 11, 538, 264, 636, 13, 50864, 50864, 708, 291, 434, 516, 281, 360, 307, 291, 434, 516, 281, 747, 4111, 11317, 13, 51364, 51364, 1033, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12437655528386433, "compression_ratio": 1.3253968253968254, "no_speech_prob": 9.459553984925151e-05}, {"id": 455, "seek": 407300, "start": 4083.0, "end": 4093.0, "text": " What you're going to do is you're going to take feature maps.", "tokens": [50364, 407, 264, 45216, 304, 3037, 1936, 1619, 291, 362, 364, 3256, 13, 50664, 50664, 639, 307, 544, 21826, 11, 538, 264, 636, 13, 50864, 50864, 708, 291, 434, 516, 281, 360, 307, 291, 434, 516, 281, 747, 4111, 11317, 13, 51364, 51364, 1033, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12437655528386433, "compression_ratio": 1.3253968253968254, "no_speech_prob": 9.459553984925151e-05}, {"id": 456, "seek": 407300, "start": 4093.0, "end": 4096.0, "text": " Okay.", "tokens": [50364, 407, 264, 45216, 304, 3037, 1936, 1619, 291, 362, 364, 3256, 13, 50664, 50664, 639, 307, 544, 21826, 11, 538, 264, 636, 13, 50864, 50864, 708, 291, 434, 516, 281, 360, 307, 291, 434, 516, 281, 747, 4111, 11317, 13, 51364, 51364, 1033, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12437655528386433, "compression_ratio": 1.3253968253968254, "no_speech_prob": 9.459553984925151e-05}, {"id": 457, "seek": 409600, "start": 4096.0, "end": 4105.0, "text": " Let's say here four, but it could be more. And then you're going to convolve each of those feature maps with a kernel.", "tokens": [50364, 961, 311, 584, 510, 1451, 11, 457, 309, 727, 312, 544, 13, 400, 550, 291, 434, 516, 281, 3754, 37361, 1184, 295, 729, 4111, 11317, 365, 257, 28256, 13, 50814, 50814, 1033, 13, 407, 257, 4111, 4471, 307, 11, 286, 500, 380, 458, 11, 718, 311, 818, 341, 1176, 42, 13, 51164, 51164, 400, 321, 362, 257, 28256, 510, 13, 51414, 51414, 1042, 11, 718, 311, 818, 309, 26190, 570, 286, 478, 516, 281, 764, 591, 337, 264, 28256, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.16158625057765416, "compression_ratio": 1.511111111111111, "no_speech_prob": 0.00011408910359023139}, {"id": 458, "seek": 409600, "start": 4105.0, "end": 4112.0, "text": " Okay. So a feature map is, I don't know, let's call this ZK.", "tokens": [50364, 961, 311, 584, 510, 1451, 11, 457, 309, 727, 312, 544, 13, 400, 550, 291, 434, 516, 281, 3754, 37361, 1184, 295, 729, 4111, 11317, 365, 257, 28256, 13, 50814, 50814, 1033, 13, 407, 257, 4111, 4471, 307, 11, 286, 500, 380, 458, 11, 718, 311, 818, 341, 1176, 42, 13, 51164, 51164, 400, 321, 362, 257, 28256, 510, 13, 51414, 51414, 1042, 11, 718, 311, 818, 309, 26190, 570, 286, 478, 516, 281, 764, 591, 337, 264, 28256, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.16158625057765416, "compression_ratio": 1.511111111111111, "no_speech_prob": 0.00011408910359023139}, {"id": 459, "seek": 409600, "start": 4112.0, "end": 4117.0, "text": " And we have a kernel here.", "tokens": [50364, 961, 311, 584, 510, 1451, 11, 457, 309, 727, 312, 544, 13, 400, 550, 291, 434, 516, 281, 3754, 37361, 1184, 295, 729, 4111, 11317, 365, 257, 28256, 13, 50814, 50814, 1033, 13, 407, 257, 4111, 4471, 307, 11, 286, 500, 380, 458, 11, 718, 311, 818, 341, 1176, 42, 13, 51164, 51164, 400, 321, 362, 257, 28256, 510, 13, 51414, 51414, 1042, 11, 718, 311, 818, 309, 26190, 570, 286, 478, 516, 281, 764, 591, 337, 264, 28256, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.16158625057765416, "compression_ratio": 1.511111111111111, "no_speech_prob": 0.00011408910359023139}, {"id": 460, "seek": 409600, "start": 4117.0, "end": 4120.0, "text": " Well, let's call it Zi because I'm going to use K for the kernel.", "tokens": [50364, 961, 311, 584, 510, 1451, 11, 457, 309, 727, 312, 544, 13, 400, 550, 291, 434, 516, 281, 3754, 37361, 1184, 295, 729, 4111, 11317, 365, 257, 28256, 13, 50814, 50814, 1033, 13, 407, 257, 4111, 4471, 307, 11, 286, 500, 380, 458, 11, 718, 311, 818, 341, 1176, 42, 13, 51164, 51164, 400, 321, 362, 257, 28256, 510, 13, 51414, 51414, 1042, 11, 718, 311, 818, 309, 26190, 570, 286, 478, 516, 281, 764, 591, 337, 264, 28256, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.16158625057765416, "compression_ratio": 1.511111111111111, "no_speech_prob": 0.00011408910359023139}, {"id": 461, "seek": 412000, "start": 4120.0, "end": 4127.0, "text": " Ki. And this is going to be a reconstruction Y.", "tokens": [50364, 17459, 13, 400, 341, 307, 516, 281, 312, 257, 31565, 398, 13, 50714, 50714, 400, 527, 31565, 307, 2935, 516, 281, 312, 264, 2408, 670, 741, 295, 26190, 3754, 29110, 365, 17459, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.1974556252763078, "compression_ratio": 1.346938775510204, "no_speech_prob": 2.9308956072782166e-05}, {"id": 462, "seek": 412000, "start": 4127.0, "end": 4141.0, "text": " And our reconstruction is simply going to be the sum over i of Zi convolved with Ki.", "tokens": [50364, 17459, 13, 400, 341, 307, 516, 281, 312, 257, 31565, 398, 13, 50714, 50714, 400, 527, 31565, 307, 2935, 516, 281, 312, 264, 2408, 670, 741, 295, 26190, 3754, 29110, 365, 17459, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.1974556252763078, "compression_ratio": 1.346938775510204, "no_speech_prob": 2.9308956072782166e-05}, {"id": 463, "seek": 414100, "start": 4141.0, "end": 4158.0, "text": " Okay. So this is different from the original sparse coding where Y bar was equal to the sum over columns of a column of a W matrix.", "tokens": [50364, 1033, 13, 407, 341, 307, 819, 490, 264, 3380, 637, 11668, 17720, 689, 398, 2159, 390, 2681, 281, 264, 2408, 670, 13766, 295, 257, 7738, 295, 257, 343, 8141, 13, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.10117094657000374, "compression_ratio": 1.212962962962963, "no_speech_prob": 1.5204918781819288e-05}, {"id": 464, "seek": 415800, "start": 4158.0, "end": 4172.0, "text": " So and multiply by a coefficient Zi, which is now a scalar.", "tokens": [50364, 407, 293, 12972, 538, 257, 17619, 26190, 11, 597, 307, 586, 257, 39684, 13, 51064, 51064, 407, 3890, 637, 11668, 17720, 11, 291, 362, 257, 32807, 2408, 295, 13766, 689, 264, 17443, 366, 39684, 31994, 295, 26190, 311, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1941893821538881, "compression_ratio": 1.3464566929133859, "no_speech_prob": 4.399269164423458e-05}, {"id": 465, "seek": 415800, "start": 4172.0, "end": 4181.0, "text": " So regular sparse coding, you have a weighted sum of columns where the weights are scalar coefficients of Zi's.", "tokens": [50364, 407, 293, 12972, 538, 257, 17619, 26190, 11, 597, 307, 586, 257, 39684, 13, 51064, 51064, 407, 3890, 637, 11668, 17720, 11, 291, 362, 257, 32807, 2408, 295, 13766, 689, 264, 17443, 366, 39684, 31994, 295, 26190, 311, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1941893821538881, "compression_ratio": 1.3464566929133859, "no_speech_prob": 4.399269164423458e-05}, {"id": 466, "seek": 418100, "start": 4181.0, "end": 4192.0, "text": " In convolutional sparse coding, it's again a linear operation, but now the dictionary matrix is a bunch of convolution kernels and the latent variable is a bunch of feature maps.", "tokens": [50364, 682, 45216, 304, 637, 11668, 17720, 11, 309, 311, 797, 257, 8213, 6916, 11, 457, 586, 264, 25890, 8141, 307, 257, 3840, 295, 45216, 23434, 1625, 293, 264, 48994, 7006, 307, 257, 3840, 295, 4111, 11317, 13, 50914, 50914, 400, 291, 434, 884, 257, 45216, 295, 1184, 4111, 4471, 365, 264, 1184, 28256, 293, 2408, 493, 264, 3542, 13, 51314, 51314, 639, 307, 437, 291, 483, 13, 407, 510, 456, 366, 11, 309, 311, 472, 295, 729, 3652, 300, 575, 257, 979, 19866, 293, 364, 2058, 19866, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10787027815113896, "compression_ratio": 1.7013574660633484, "no_speech_prob": 3.5353215935174376e-05}, {"id": 467, "seek": 418100, "start": 4192.0, "end": 4200.0, "text": " And you're doing a convolution of each feature map with the each kernel and sum up the results.", "tokens": [50364, 682, 45216, 304, 637, 11668, 17720, 11, 309, 311, 797, 257, 8213, 6916, 11, 457, 586, 264, 25890, 8141, 307, 257, 3840, 295, 45216, 23434, 1625, 293, 264, 48994, 7006, 307, 257, 3840, 295, 4111, 11317, 13, 50914, 50914, 400, 291, 434, 884, 257, 45216, 295, 1184, 4111, 4471, 365, 264, 1184, 28256, 293, 2408, 493, 264, 3542, 13, 51314, 51314, 639, 307, 437, 291, 483, 13, 407, 510, 456, 366, 11, 309, 311, 472, 295, 729, 3652, 300, 575, 257, 979, 19866, 293, 364, 2058, 19866, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10787027815113896, "compression_ratio": 1.7013574660633484, "no_speech_prob": 3.5353215935174376e-05}, {"id": 468, "seek": 418100, "start": 4200.0, "end": 4207.0, "text": " This is what you get. So here there are, it's one of those systems that has a decoder and an encoder.", "tokens": [50364, 682, 45216, 304, 637, 11668, 17720, 11, 309, 311, 797, 257, 8213, 6916, 11, 457, 586, 264, 25890, 8141, 307, 257, 3840, 295, 45216, 23434, 1625, 293, 264, 48994, 7006, 307, 257, 3840, 295, 4111, 11317, 13, 50914, 50914, 400, 291, 434, 884, 257, 45216, 295, 1184, 4111, 4471, 365, 264, 1184, 28256, 293, 2408, 493, 264, 3542, 13, 51314, 51314, 639, 307, 437, 291, 483, 13, 407, 510, 456, 366, 11, 309, 311, 472, 295, 729, 3652, 300, 575, 257, 979, 19866, 293, 364, 2058, 19866, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10787027815113896, "compression_ratio": 1.7013574660633484, "no_speech_prob": 3.5353215935174376e-05}, {"id": 469, "seek": 420700, "start": 4207.0, "end": 4212.0, "text": " The decoder is very simple here. It's basically just essentially a single layer network with nonlinearity.", "tokens": [50364, 440, 979, 19866, 307, 588, 2199, 510, 13, 467, 311, 1936, 445, 4476, 257, 2167, 4583, 3209, 365, 2107, 1889, 17409, 13, 50614, 50614, 400, 550, 456, 307, 257, 2199, 4583, 934, 300, 11, 1936, 257, 21539, 4583, 934, 300, 281, 1319, 264, 16823, 13, 50864, 50864, 583, 309, 311, 588, 11, 588, 2199, 13, 400, 264, 15995, 293, 264, 2058, 378, 433, 293, 264, 979, 19866, 574, 588, 2531, 13, 51164, 51164, 407, 309, 311, 1936, 264, 2058, 19866, 307, 445, 257, 45216, 11, 550, 512, 2107, 1889, 17409, 13, 286, 519, 309, 390, 9848, 65, 7940, 27747, 294, 300, 1389, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11452435110216942, "compression_ratio": 1.849785407725322, "no_speech_prob": 0.00013979413779452443}, {"id": 470, "seek": 420700, "start": 4212.0, "end": 4217.0, "text": " And then there is a simple layer after that, basically a diagonal layer after that to change the gains.", "tokens": [50364, 440, 979, 19866, 307, 588, 2199, 510, 13, 467, 311, 1936, 445, 4476, 257, 2167, 4583, 3209, 365, 2107, 1889, 17409, 13, 50614, 50614, 400, 550, 456, 307, 257, 2199, 4583, 934, 300, 11, 1936, 257, 21539, 4583, 934, 300, 281, 1319, 264, 16823, 13, 50864, 50864, 583, 309, 311, 588, 11, 588, 2199, 13, 400, 264, 15995, 293, 264, 2058, 378, 433, 293, 264, 979, 19866, 574, 588, 2531, 13, 51164, 51164, 407, 309, 311, 1936, 264, 2058, 19866, 307, 445, 257, 45216, 11, 550, 512, 2107, 1889, 17409, 13, 286, 519, 309, 390, 9848, 65, 7940, 27747, 294, 300, 1389, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11452435110216942, "compression_ratio": 1.849785407725322, "no_speech_prob": 0.00013979413779452443}, {"id": 471, "seek": 420700, "start": 4217.0, "end": 4223.0, "text": " But it's very, very simple. And the filters and the encoders and the decoder look very similar.", "tokens": [50364, 440, 979, 19866, 307, 588, 2199, 510, 13, 467, 311, 1936, 445, 4476, 257, 2167, 4583, 3209, 365, 2107, 1889, 17409, 13, 50614, 50614, 400, 550, 456, 307, 257, 2199, 4583, 934, 300, 11, 1936, 257, 21539, 4583, 934, 300, 281, 1319, 264, 16823, 13, 50864, 50864, 583, 309, 311, 588, 11, 588, 2199, 13, 400, 264, 15995, 293, 264, 2058, 378, 433, 293, 264, 979, 19866, 574, 588, 2531, 13, 51164, 51164, 407, 309, 311, 1936, 264, 2058, 19866, 307, 445, 257, 45216, 11, 550, 512, 2107, 1889, 17409, 13, 286, 519, 309, 390, 9848, 65, 7940, 27747, 294, 300, 1389, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11452435110216942, "compression_ratio": 1.849785407725322, "no_speech_prob": 0.00013979413779452443}, {"id": 472, "seek": 420700, "start": 4223.0, "end": 4230.0, "text": " So it's basically the encoder is just a convolution, then some nonlinearity. I think it was hyperbolic tangent in that case.", "tokens": [50364, 440, 979, 19866, 307, 588, 2199, 510, 13, 467, 311, 1936, 445, 4476, 257, 2167, 4583, 3209, 365, 2107, 1889, 17409, 13, 50614, 50614, 400, 550, 456, 307, 257, 2199, 4583, 934, 300, 11, 1936, 257, 21539, 4583, 934, 300, 281, 1319, 264, 16823, 13, 50864, 50864, 583, 309, 311, 588, 11, 588, 2199, 13, 400, 264, 15995, 293, 264, 2058, 378, 433, 293, 264, 979, 19866, 574, 588, 2531, 13, 51164, 51164, 407, 309, 311, 1936, 264, 2058, 19866, 307, 445, 257, 45216, 11, 550, 512, 2107, 1889, 17409, 13, 286, 519, 309, 390, 9848, 65, 7940, 27747, 294, 300, 1389, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11452435110216942, "compression_ratio": 1.849785407725322, "no_speech_prob": 0.00013979413779452443}, {"id": 473, "seek": 423000, "start": 4230.0, "end": 4237.0, "text": " And then basically what amounts to a diagonal layer that just changed the scale.", "tokens": [50364, 400, 550, 1936, 437, 11663, 281, 257, 21539, 4583, 300, 445, 3105, 264, 4373, 13, 50714, 50714, 400, 550, 264, 979, 19866, 11, 550, 456, 307, 257, 637, 685, 507, 322, 264, 25534, 322, 264, 3089, 13, 50964, 50964, 400, 550, 264, 979, 19866, 307, 445, 257, 45216, 304, 8213, 979, 19866, 13, 400, 264, 31565, 307, 445, 257, 3732, 4560, 13, 51264, 51264, 407, 498, 291, 26952, 300, 456, 307, 787, 472, 6608, 11, 264, 6608, 1542, 411, 264, 472, 412, 264, 1192, 1411, 13, 467, 311, 445, 257, 3056, 6262, 2010, 6608, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08551383740974194, "compression_ratio": 1.875, "no_speech_prob": 6.643065717071295e-06}, {"id": 474, "seek": 423000, "start": 4237.0, "end": 4242.0, "text": " And then the decoder, then there is a sparsity on the constraint on the code.", "tokens": [50364, 400, 550, 1936, 437, 11663, 281, 257, 21539, 4583, 300, 445, 3105, 264, 4373, 13, 50714, 50714, 400, 550, 264, 979, 19866, 11, 550, 456, 307, 257, 637, 685, 507, 322, 264, 25534, 322, 264, 3089, 13, 50964, 50964, 400, 550, 264, 979, 19866, 307, 445, 257, 45216, 304, 8213, 979, 19866, 13, 400, 264, 31565, 307, 445, 257, 3732, 4560, 13, 51264, 51264, 407, 498, 291, 26952, 300, 456, 307, 787, 472, 6608, 11, 264, 6608, 1542, 411, 264, 472, 412, 264, 1192, 1411, 13, 467, 311, 445, 257, 3056, 6262, 2010, 6608, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08551383740974194, "compression_ratio": 1.875, "no_speech_prob": 6.643065717071295e-06}, {"id": 475, "seek": 423000, "start": 4242.0, "end": 4248.0, "text": " And then the decoder is just a convolutional linear decoder. And the reconstruction is just a square distance.", "tokens": [50364, 400, 550, 1936, 437, 11663, 281, 257, 21539, 4583, 300, 445, 3105, 264, 4373, 13, 50714, 50714, 400, 550, 264, 979, 19866, 11, 550, 456, 307, 257, 637, 685, 507, 322, 264, 25534, 322, 264, 3089, 13, 50964, 50964, 400, 550, 264, 979, 19866, 307, 445, 257, 45216, 304, 8213, 979, 19866, 13, 400, 264, 31565, 307, 445, 257, 3732, 4560, 13, 51264, 51264, 407, 498, 291, 26952, 300, 456, 307, 787, 472, 6608, 11, 264, 6608, 1542, 411, 264, 472, 412, 264, 1192, 1411, 13, 467, 311, 445, 257, 3056, 6262, 2010, 6608, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08551383740974194, "compression_ratio": 1.875, "no_speech_prob": 6.643065717071295e-06}, {"id": 476, "seek": 423000, "start": 4248.0, "end": 4256.0, "text": " So if you impose that there is only one filter, the filter looks like the one at the top left. It's just a center surround type filter.", "tokens": [50364, 400, 550, 1936, 437, 11663, 281, 257, 21539, 4583, 300, 445, 3105, 264, 4373, 13, 50714, 50714, 400, 550, 264, 979, 19866, 11, 550, 456, 307, 257, 637, 685, 507, 322, 264, 25534, 322, 264, 3089, 13, 50964, 50964, 400, 550, 264, 979, 19866, 307, 445, 257, 45216, 304, 8213, 979, 19866, 13, 400, 264, 31565, 307, 445, 257, 3732, 4560, 13, 51264, 51264, 407, 498, 291, 26952, 300, 456, 307, 787, 472, 6608, 11, 264, 6608, 1542, 411, 264, 472, 412, 264, 1192, 1411, 13, 467, 311, 445, 257, 3056, 6262, 2010, 6608, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08551383740974194, "compression_ratio": 1.875, "no_speech_prob": 6.643065717071295e-06}, {"id": 477, "seek": 425600, "start": 4256.0, "end": 4265.0, "text": " If you allow two filters, you get kind of two weirdly shaped filters. If you let four filters, which is the third row, you get oriented edges, horizontal and vertical.", "tokens": [50364, 759, 291, 2089, 732, 15995, 11, 291, 483, 733, 295, 732, 48931, 13475, 15995, 13, 759, 291, 718, 1451, 15995, 11, 597, 307, 264, 2636, 5386, 11, 291, 483, 21841, 8819, 11, 12750, 293, 9429, 13, 50814, 50814, 492, 483, 732, 12367, 1088, 337, 1184, 295, 264, 15995, 13, 1171, 3180, 15995, 11, 291, 483, 21841, 8819, 412, 3180, 819, 8579, 763, 13, 51264, 51264, 3165, 15995, 11, 291, 483, 544, 8579, 763, 293, 291, 611, 483, 3056, 6262, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11451086543855213, "compression_ratio": 1.8608247422680413, "no_speech_prob": 0.00011582761362660676}, {"id": 478, "seek": 425600, "start": 4265.0, "end": 4274.0, "text": " We get two polarities for each of the filters. For eight filters, you get oriented edges at eight different orientations.", "tokens": [50364, 759, 291, 2089, 732, 15995, 11, 291, 483, 733, 295, 732, 48931, 13475, 15995, 13, 759, 291, 718, 1451, 15995, 11, 597, 307, 264, 2636, 5386, 11, 291, 483, 21841, 8819, 11, 12750, 293, 9429, 13, 50814, 50814, 492, 483, 732, 12367, 1088, 337, 1184, 295, 264, 15995, 13, 1171, 3180, 15995, 11, 291, 483, 21841, 8819, 412, 3180, 819, 8579, 763, 13, 51264, 51264, 3165, 15995, 11, 291, 483, 544, 8579, 763, 293, 291, 611, 483, 3056, 6262, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11451086543855213, "compression_ratio": 1.8608247422680413, "no_speech_prob": 0.00011582761362660676}, {"id": 479, "seek": 425600, "start": 4274.0, "end": 4278.0, "text": " 16 filters, you get more orientations and you also get center surround.", "tokens": [50364, 759, 291, 2089, 732, 15995, 11, 291, 483, 733, 295, 732, 48931, 13475, 15995, 13, 759, 291, 718, 1451, 15995, 11, 597, 307, 264, 2636, 5386, 11, 291, 483, 21841, 8819, 11, 12750, 293, 9429, 13, 50814, 50814, 492, 483, 732, 12367, 1088, 337, 1184, 295, 264, 15995, 13, 1171, 3180, 15995, 11, 291, 483, 21841, 8819, 412, 3180, 819, 8579, 763, 13, 51264, 51264, 3165, 15995, 11, 291, 483, 544, 8579, 763, 293, 291, 611, 483, 3056, 6262, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11451086543855213, "compression_ratio": 1.8608247422680413, "no_speech_prob": 0.00011582761362660676}, {"id": 480, "seek": 427800, "start": 4278.0, "end": 4291.0, "text": " And then as you increase the number of filters, you get sort of more diverse filters, not just edge detectors, but also grating detectors at various orientations, center surround, et cetera.", "tokens": [50364, 400, 550, 382, 291, 3488, 264, 1230, 295, 15995, 11, 291, 483, 1333, 295, 544, 9521, 15995, 11, 406, 445, 4691, 46866, 11, 457, 611, 677, 990, 46866, 412, 3683, 8579, 763, 11, 3056, 6262, 11, 1030, 11458, 13, 51014, 51014, 400, 300, 311, 588, 1880, 570, 341, 307, 264, 733, 295, 1507, 291, 536, 294, 264, 5056, 33312, 13, 51264, 51264, 407, 797, 11, 341, 307, 364, 18877, 300, 291, 393, 1466, 534, 665, 4122, 294, 2584, 6219, 347, 66, 449, 66, 2640, 636, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08764592276679145, "compression_ratio": 1.6398305084745763, "no_speech_prob": 2.2124004317447543e-05}, {"id": 481, "seek": 427800, "start": 4291.0, "end": 4296.0, "text": " And that's very interesting because this is the kind of stuff you see in the visual cortex.", "tokens": [50364, 400, 550, 382, 291, 3488, 264, 1230, 295, 15995, 11, 291, 483, 1333, 295, 544, 9521, 15995, 11, 406, 445, 4691, 46866, 11, 457, 611, 677, 990, 46866, 412, 3683, 8579, 763, 11, 3056, 6262, 11, 1030, 11458, 13, 51014, 51014, 400, 300, 311, 588, 1880, 570, 341, 307, 264, 733, 295, 1507, 291, 536, 294, 264, 5056, 33312, 13, 51264, 51264, 407, 797, 11, 341, 307, 364, 18877, 300, 291, 393, 1466, 534, 665, 4122, 294, 2584, 6219, 347, 66, 449, 66, 2640, 636, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08764592276679145, "compression_ratio": 1.6398305084745763, "no_speech_prob": 2.2124004317447543e-05}, {"id": 482, "seek": 427800, "start": 4296.0, "end": 4302.0, "text": " So again, this is an indication that you can learn really good features in completely uncircumcised way.", "tokens": [50364, 400, 550, 382, 291, 3488, 264, 1230, 295, 15995, 11, 291, 483, 1333, 295, 544, 9521, 15995, 11, 406, 445, 4691, 46866, 11, 457, 611, 677, 990, 46866, 412, 3683, 8579, 763, 11, 3056, 6262, 11, 1030, 11458, 13, 51014, 51014, 400, 300, 311, 588, 1880, 570, 341, 307, 264, 733, 295, 1507, 291, 536, 294, 264, 5056, 33312, 13, 51264, 51264, 407, 797, 11, 341, 307, 364, 18877, 300, 291, 393, 1466, 534, 665, 4122, 294, 2584, 6219, 347, 66, 449, 66, 2640, 636, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08764592276679145, "compression_ratio": 1.6398305084745763, "no_speech_prob": 2.2124004317447543e-05}, {"id": 483, "seek": 430200, "start": 4302.0, "end": 4316.0, "text": " Now, here is the sad news. If you take those features, you plug them into a convolutional net and you train that on some task, you don't necessarily get better results than you train on ImageNet from scratch.", "tokens": [50364, 823, 11, 510, 307, 264, 4227, 2583, 13, 759, 291, 747, 729, 4122, 11, 291, 5452, 552, 666, 257, 45216, 304, 2533, 293, 291, 3847, 300, 322, 512, 5633, 11, 291, 500, 380, 4725, 483, 1101, 3542, 813, 291, 3847, 322, 29903, 31890, 490, 8459, 13, 51064, 51064, 583, 456, 366, 257, 1326, 14519, 689, 341, 575, 4254, 9194, 264, 3389, 11, 4098, 294, 3331, 689, 264, 1230, 295, 7645, 10938, 307, 406, 300, 869, 420, 264, 1230, 295, 10479, 307, 1359, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07114062912162693, "compression_ratio": 1.6596638655462186, "no_speech_prob": 3.373293293407187e-05}, {"id": 484, "seek": 430200, "start": 4316.0, "end": 4326.0, "text": " But there are a few instances where this has helped boost the performance, particularly in cases where the number of label samples is not that great or the number of categories is small.", "tokens": [50364, 823, 11, 510, 307, 264, 4227, 2583, 13, 759, 291, 747, 729, 4122, 11, 291, 5452, 552, 666, 257, 45216, 304, 2533, 293, 291, 3847, 300, 322, 512, 5633, 11, 291, 500, 380, 4725, 483, 1101, 3542, 813, 291, 3847, 322, 29903, 31890, 490, 8459, 13, 51064, 51064, 583, 456, 366, 257, 1326, 14519, 689, 341, 575, 4254, 9194, 264, 3389, 11, 4098, 294, 3331, 689, 264, 1230, 295, 7645, 10938, 307, 406, 300, 869, 420, 264, 1230, 295, 10479, 307, 1359, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07114062912162693, "compression_ratio": 1.6596638655462186, "no_speech_prob": 3.373293293407187e-05}, {"id": 485, "seek": 432600, "start": 4326.0, "end": 4333.0, "text": " And so by training purely supervised, you get degenerate features, basically.", "tokens": [50364, 400, 370, 538, 3097, 17491, 46533, 11, 291, 483, 40520, 473, 4122, 11, 1936, 13, 50714, 50714, 1692, 311, 1071, 1365, 510, 13, 10635, 551, 13, 50864, 50864, 3764, 11, 309, 311, 257, 45216, 304, 637, 11668, 17720, 13, 1692, 264, 979, 8616, 28256, 11, 341, 307, 322, 2017, 5267, 13, 51164, 51164, 440, 979, 8616, 23434, 1625, 366, 4949, 538, 4949, 6456, 45216, 379, 670, 364, 3256, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.16607703901317022, "compression_ratio": 1.553763440860215, "no_speech_prob": 5.648453952744603e-05}, {"id": 486, "seek": 432600, "start": 4333.0, "end": 4336.0, "text": " Here's another example here. Same thing.", "tokens": [50364, 400, 370, 538, 3097, 17491, 46533, 11, 291, 483, 40520, 473, 4122, 11, 1936, 13, 50714, 50714, 1692, 311, 1071, 1365, 510, 13, 10635, 551, 13, 50864, 50864, 3764, 11, 309, 311, 257, 45216, 304, 637, 11668, 17720, 13, 1692, 264, 979, 8616, 28256, 11, 341, 307, 322, 2017, 5267, 13, 51164, 51164, 440, 979, 8616, 23434, 1625, 366, 4949, 538, 4949, 6456, 45216, 379, 670, 364, 3256, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.16607703901317022, "compression_ratio": 1.553763440860215, "no_speech_prob": 5.648453952744603e-05}, {"id": 487, "seek": 432600, "start": 4336.0, "end": 4342.0, "text": " Again, it's a convolutional sparse coding. Here the decoding kernel, this is on color images.", "tokens": [50364, 400, 370, 538, 3097, 17491, 46533, 11, 291, 483, 40520, 473, 4122, 11, 1936, 13, 50714, 50714, 1692, 311, 1071, 1365, 510, 13, 10635, 551, 13, 50864, 50864, 3764, 11, 309, 311, 257, 45216, 304, 637, 11668, 17720, 13, 1692, 264, 979, 8616, 28256, 11, 341, 307, 322, 2017, 5267, 13, 51164, 51164, 440, 979, 8616, 23434, 1625, 366, 4949, 538, 4949, 6456, 45216, 379, 670, 364, 3256, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.16607703901317022, "compression_ratio": 1.553763440860215, "no_speech_prob": 5.648453952744603e-05}, {"id": 488, "seek": 432600, "start": 4342.0, "end": 4349.0, "text": " The decoding kernels are nine by nine applied convolutionally over an image.", "tokens": [50364, 400, 370, 538, 3097, 17491, 46533, 11, 291, 483, 40520, 473, 4122, 11, 1936, 13, 50714, 50714, 1692, 311, 1071, 1365, 510, 13, 10635, 551, 13, 50864, 50864, 3764, 11, 309, 311, 257, 45216, 304, 637, 11668, 17720, 13, 1692, 264, 979, 8616, 28256, 11, 341, 307, 322, 2017, 5267, 13, 51164, 51164, 440, 979, 8616, 23434, 1625, 366, 4949, 538, 4949, 6456, 45216, 379, 670, 364, 3256, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.16607703901317022, "compression_ratio": 1.553763440860215, "no_speech_prob": 5.648453952744603e-05}, {"id": 489, "seek": 434900, "start": 4349.0, "end": 4360.0, "text": " And what you see on the left here are the sparse codes. Here you have 64 feature maps.", "tokens": [50364, 400, 437, 291, 536, 322, 264, 1411, 510, 366, 264, 637, 11668, 14211, 13, 1692, 291, 362, 12145, 4111, 11317, 13, 50914, 50914, 400, 291, 393, 536, 300, 264, 1176, 8062, 307, 4664, 637, 11668, 13, 821, 311, 787, 257, 1326, 6677, 510, 300, 366, 2139, 2418, 420, 2211, 420, 2107, 12, 861, 320, 11, 498, 291, 528, 13, 51364, 51364, 400, 341, 307, 570, 295, 637, 11668, 1176, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1212006646233636, "compression_ratio": 1.4860335195530727, "no_speech_prob": 4.3972875573672354e-05}, {"id": 490, "seek": 434900, "start": 4360.0, "end": 4369.0, "text": " And you can see that the Z vector is extremely sparse. There's only a few components here that are either white or black or non-gray, if you want.", "tokens": [50364, 400, 437, 291, 536, 322, 264, 1411, 510, 366, 264, 637, 11668, 14211, 13, 1692, 291, 362, 12145, 4111, 11317, 13, 50914, 50914, 400, 291, 393, 536, 300, 264, 1176, 8062, 307, 4664, 637, 11668, 13, 821, 311, 787, 257, 1326, 6677, 510, 300, 366, 2139, 2418, 420, 2211, 420, 2107, 12, 861, 320, 11, 498, 291, 528, 13, 51364, 51364, 400, 341, 307, 570, 295, 637, 11668, 1176, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1212006646233636, "compression_ratio": 1.4860335195530727, "no_speech_prob": 4.3972875573672354e-05}, {"id": 491, "seek": 434900, "start": 4369.0, "end": 4374.0, "text": " And this is because of sparse Z.", "tokens": [50364, 400, 437, 291, 536, 322, 264, 1411, 510, 366, 264, 637, 11668, 14211, 13, 1692, 291, 362, 12145, 4111, 11317, 13, 50914, 50914, 400, 291, 393, 536, 300, 264, 1176, 8062, 307, 4664, 637, 11668, 13, 821, 311, 787, 257, 1326, 6677, 510, 300, 366, 2139, 2418, 420, 2211, 420, 2107, 12, 861, 320, 11, 498, 291, 528, 13, 51364, 51364, 400, 341, 307, 570, 295, 637, 11668, 1176, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1212006646233636, "compression_ratio": 1.4860335195530727, "no_speech_prob": 4.3972875573672354e-05}, {"id": 492, "seek": 437400, "start": 4374.0, "end": 4381.0, "text": " In the last few minutes, we'll talk about variational autoencoder. And I guess you've heard a bit of this from...", "tokens": [50364, 682, 264, 1036, 1326, 2077, 11, 321, 603, 751, 466, 3034, 1478, 8399, 22660, 19866, 13, 400, 286, 2041, 291, 600, 2198, 257, 857, 295, 341, 490, 485, 50714, 50714, 17499, 13, 492, 434, 516, 281, 312, 10322, 341, 4153, 365, 264, 16295, 293, 264, 3089, 293, 1203, 13, 50964, 50964, 407, 4153, 307, 516, 281, 312, 472, 1773, 295, 445, 341, 13, 51164, 51164, 1779, 13, 407, 510, 307, 257, 14281, 337, 577, 3034, 1478, 2058, 378, 433, 589, 13, 51514, 51514, 32511, 1478, 8399, 22660, 378, 433, 366, 1936, 264, 912, 9482, 382, 264, 472, 286, 4712, 8046, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12552599225725447, "compression_ratio": 1.6910569105691058, "no_speech_prob": 6.501766620203853e-05}, {"id": 493, "seek": 437400, "start": 4381.0, "end": 4386.0, "text": " Tomorrow. We're going to be covering this tomorrow with the bubbles and the code and everything.", "tokens": [50364, 682, 264, 1036, 1326, 2077, 11, 321, 603, 751, 466, 3034, 1478, 8399, 22660, 19866, 13, 400, 286, 2041, 291, 600, 2198, 257, 857, 295, 341, 490, 485, 50714, 50714, 17499, 13, 492, 434, 516, 281, 312, 10322, 341, 4153, 365, 264, 16295, 293, 264, 3089, 293, 1203, 13, 50964, 50964, 407, 4153, 307, 516, 281, 312, 472, 1773, 295, 445, 341, 13, 51164, 51164, 1779, 13, 407, 510, 307, 257, 14281, 337, 577, 3034, 1478, 2058, 378, 433, 589, 13, 51514, 51514, 32511, 1478, 8399, 22660, 378, 433, 366, 1936, 264, 912, 9482, 382, 264, 472, 286, 4712, 8046, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12552599225725447, "compression_ratio": 1.6910569105691058, "no_speech_prob": 6.501766620203853e-05}, {"id": 494, "seek": 437400, "start": 4386.0, "end": 4390.0, "text": " So tomorrow is going to be one hour of just this.", "tokens": [50364, 682, 264, 1036, 1326, 2077, 11, 321, 603, 751, 466, 3034, 1478, 8399, 22660, 19866, 13, 400, 286, 2041, 291, 600, 2198, 257, 857, 295, 341, 490, 485, 50714, 50714, 17499, 13, 492, 434, 516, 281, 312, 10322, 341, 4153, 365, 264, 16295, 293, 264, 3089, 293, 1203, 13, 50964, 50964, 407, 4153, 307, 516, 281, 312, 472, 1773, 295, 445, 341, 13, 51164, 51164, 1779, 13, 407, 510, 307, 257, 14281, 337, 577, 3034, 1478, 2058, 378, 433, 589, 13, 51514, 51514, 32511, 1478, 8399, 22660, 378, 433, 366, 1936, 264, 912, 9482, 382, 264, 472, 286, 4712, 8046, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12552599225725447, "compression_ratio": 1.6910569105691058, "no_speech_prob": 6.501766620203853e-05}, {"id": 495, "seek": 437400, "start": 4390.0, "end": 4397.0, "text": " Right. So here is a preview for how variational encoders work.", "tokens": [50364, 682, 264, 1036, 1326, 2077, 11, 321, 603, 751, 466, 3034, 1478, 8399, 22660, 19866, 13, 400, 286, 2041, 291, 600, 2198, 257, 857, 295, 341, 490, 485, 50714, 50714, 17499, 13, 492, 434, 516, 281, 312, 10322, 341, 4153, 365, 264, 16295, 293, 264, 3089, 293, 1203, 13, 50964, 50964, 407, 4153, 307, 516, 281, 312, 472, 1773, 295, 445, 341, 13, 51164, 51164, 1779, 13, 407, 510, 307, 257, 14281, 337, 577, 3034, 1478, 2058, 378, 433, 589, 13, 51514, 51514, 32511, 1478, 8399, 22660, 378, 433, 366, 1936, 264, 912, 9482, 382, 264, 472, 286, 4712, 8046, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12552599225725447, "compression_ratio": 1.6910569105691058, "no_speech_prob": 6.501766620203853e-05}, {"id": 496, "seek": 437400, "start": 4397.0, "end": 4402.0, "text": " Variational autoencoders are basically the same architecture as the one I showed previously.", "tokens": [50364, 682, 264, 1036, 1326, 2077, 11, 321, 603, 751, 466, 3034, 1478, 8399, 22660, 19866, 13, 400, 286, 2041, 291, 600, 2198, 257, 857, 295, 341, 490, 485, 50714, 50714, 17499, 13, 492, 434, 516, 281, 312, 10322, 341, 4153, 365, 264, 16295, 293, 264, 3089, 293, 1203, 13, 50964, 50964, 407, 4153, 307, 516, 281, 312, 472, 1773, 295, 445, 341, 13, 51164, 51164, 1779, 13, 407, 510, 307, 257, 14281, 337, 577, 3034, 1478, 2058, 378, 433, 589, 13, 51514, 51514, 32511, 1478, 8399, 22660, 378, 433, 366, 1936, 264, 912, 9482, 382, 264, 472, 286, 4712, 8046, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12552599225725447, "compression_ratio": 1.6910569105691058, "no_speech_prob": 6.501766620203853e-05}, {"id": 497, "seek": 440200, "start": 4402.0, "end": 4409.0, "text": " So basically, an autoencoder ignores the conditional part, the part that's conditioned upon X for now.", "tokens": [50364, 407, 1936, 11, 364, 8399, 22660, 19866, 5335, 2706, 264, 27708, 644, 11, 264, 644, 300, 311, 35833, 3564, 1783, 337, 586, 13, 50714, 50714, 663, 727, 312, 257, 27708, 3034, 1478, 8399, 22660, 19866, 11, 457, 337, 586, 321, 434, 445, 516, 281, 362, 257, 3890, 3034, 1478, 8399, 22660, 19866, 13, 50964, 50964, 407, 309, 311, 364, 8399, 22660, 19866, 689, 291, 747, 257, 7006, 398, 11, 291, 1190, 309, 807, 364, 2058, 19866, 13, 51314, 51314, 467, 727, 312, 257, 2120, 388, 11167, 18161, 2533, 11, 45216, 304, 2533, 11, 2035, 291, 528, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10310895372145247, "compression_ratio": 1.8160377358490567, "no_speech_prob": 5.648272417602129e-05}, {"id": 498, "seek": 440200, "start": 4409.0, "end": 4414.0, "text": " That could be a conditional variational autoencoder, but for now we're just going to have a regular variational autoencoder.", "tokens": [50364, 407, 1936, 11, 364, 8399, 22660, 19866, 5335, 2706, 264, 27708, 644, 11, 264, 644, 300, 311, 35833, 3564, 1783, 337, 586, 13, 50714, 50714, 663, 727, 312, 257, 27708, 3034, 1478, 8399, 22660, 19866, 11, 457, 337, 586, 321, 434, 445, 516, 281, 362, 257, 3890, 3034, 1478, 8399, 22660, 19866, 13, 50964, 50964, 407, 309, 311, 364, 8399, 22660, 19866, 689, 291, 747, 257, 7006, 398, 11, 291, 1190, 309, 807, 364, 2058, 19866, 13, 51314, 51314, 467, 727, 312, 257, 2120, 388, 11167, 18161, 2533, 11, 45216, 304, 2533, 11, 2035, 291, 528, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10310895372145247, "compression_ratio": 1.8160377358490567, "no_speech_prob": 5.648272417602129e-05}, {"id": 499, "seek": 440200, "start": 4414.0, "end": 4421.0, "text": " So it's an autoencoder where you take a variable Y, you run it through an encoder.", "tokens": [50364, 407, 1936, 11, 364, 8399, 22660, 19866, 5335, 2706, 264, 27708, 644, 11, 264, 644, 300, 311, 35833, 3564, 1783, 337, 586, 13, 50714, 50714, 663, 727, 312, 257, 27708, 3034, 1478, 8399, 22660, 19866, 11, 457, 337, 586, 321, 434, 445, 516, 281, 362, 257, 3890, 3034, 1478, 8399, 22660, 19866, 13, 50964, 50964, 407, 309, 311, 364, 8399, 22660, 19866, 689, 291, 747, 257, 7006, 398, 11, 291, 1190, 309, 807, 364, 2058, 19866, 13, 51314, 51314, 467, 727, 312, 257, 2120, 388, 11167, 18161, 2533, 11, 45216, 304, 2533, 11, 2035, 291, 528, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10310895372145247, "compression_ratio": 1.8160377358490567, "no_speech_prob": 5.648272417602129e-05}, {"id": 500, "seek": 440200, "start": 4421.0, "end": 4424.0, "text": " It could be a multilayer neural net, convolutional net, whatever you want.", "tokens": [50364, 407, 1936, 11, 364, 8399, 22660, 19866, 5335, 2706, 264, 27708, 644, 11, 264, 644, 300, 311, 35833, 3564, 1783, 337, 586, 13, 50714, 50714, 663, 727, 312, 257, 27708, 3034, 1478, 8399, 22660, 19866, 11, 457, 337, 586, 321, 434, 445, 516, 281, 362, 257, 3890, 3034, 1478, 8399, 22660, 19866, 13, 50964, 50964, 407, 309, 311, 364, 8399, 22660, 19866, 689, 291, 747, 257, 7006, 398, 11, 291, 1190, 309, 807, 364, 2058, 19866, 13, 51314, 51314, 467, 727, 312, 257, 2120, 388, 11167, 18161, 2533, 11, 45216, 304, 2533, 11, 2035, 291, 528, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10310895372145247, "compression_ratio": 1.8160377358490567, "no_speech_prob": 5.648272417602129e-05}, {"id": 501, "seek": 442400, "start": 4424.0, "end": 4437.0, "text": " It produces a prediction for the sparse code. Z bar is a term in the energy function that measures the square Euclidean distance between Z, the latent variable, and Z bar.", "tokens": [50364, 467, 14725, 257, 17630, 337, 264, 637, 11668, 3089, 13, 1176, 2159, 307, 257, 1433, 294, 264, 2281, 2445, 300, 8000, 264, 3732, 462, 1311, 31264, 282, 4560, 1296, 1176, 11, 264, 48994, 7006, 11, 293, 1176, 2159, 13, 51014, 51014, 400, 456, 311, 611, 1071, 2063, 2445, 510, 11, 597, 307, 264, 441, 17, 2026, 295, 1176, 2159, 13, 51464, 51464, 682, 1186, 11, 5101, 309, 311, 544, 264, 441, 17, 2026, 295, 1176, 11, 767, 13, 663, 576, 312, 544, 8559, 11, 457, 309, 1177, 380, 652, 709, 2649, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10155102610588074, "compression_ratio": 1.5811965811965811, "no_speech_prob": 3.3209285902557895e-05}, {"id": 502, "seek": 442400, "start": 4437.0, "end": 4446.0, "text": " And there's also another cost function here, which is the L2 norm of Z bar.", "tokens": [50364, 467, 14725, 257, 17630, 337, 264, 637, 11668, 3089, 13, 1176, 2159, 307, 257, 1433, 294, 264, 2281, 2445, 300, 8000, 264, 3732, 462, 1311, 31264, 282, 4560, 1296, 1176, 11, 264, 48994, 7006, 11, 293, 1176, 2159, 13, 51014, 51014, 400, 456, 311, 611, 1071, 2063, 2445, 510, 11, 597, 307, 264, 441, 17, 2026, 295, 1176, 2159, 13, 51464, 51464, 682, 1186, 11, 5101, 309, 311, 544, 264, 441, 17, 2026, 295, 1176, 11, 767, 13, 663, 576, 312, 544, 8559, 11, 457, 309, 1177, 380, 652, 709, 2649, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10155102610588074, "compression_ratio": 1.5811965811965811, "no_speech_prob": 3.3209285902557895e-05}, {"id": 503, "seek": 442400, "start": 4446.0, "end": 4452.0, "text": " In fact, generally it's more the L2 norm of Z, actually. That would be more accurate, but it doesn't make much difference.", "tokens": [50364, 467, 14725, 257, 17630, 337, 264, 637, 11668, 3089, 13, 1176, 2159, 307, 257, 1433, 294, 264, 2281, 2445, 300, 8000, 264, 3732, 462, 1311, 31264, 282, 4560, 1296, 1176, 11, 264, 48994, 7006, 11, 293, 1176, 2159, 13, 51014, 51014, 400, 456, 311, 611, 1071, 2063, 2445, 510, 11, 597, 307, 264, 441, 17, 2026, 295, 1176, 2159, 13, 51464, 51464, 682, 1186, 11, 5101, 309, 311, 544, 264, 441, 17, 2026, 295, 1176, 11, 767, 13, 663, 576, 312, 544, 8559, 11, 457, 309, 1177, 380, 652, 709, 2649, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10155102610588074, "compression_ratio": 1.5811965811965811, "no_speech_prob": 3.3209285902557895e-05}, {"id": 504, "seek": 445200, "start": 4452.0, "end": 4459.0, "text": " And then Z goes through a decoder, which reconstructs Y, and that's your reconstruction error.", "tokens": [50364, 400, 550, 1176, 1709, 807, 257, 979, 19866, 11, 597, 31499, 82, 398, 11, 293, 300, 311, 428, 31565, 6713, 13, 50714, 50714, 823, 11, 264, 2649, 365, 3894, 485, 370, 341, 1542, 588, 11, 588, 2531, 281, 264, 2010, 295, 8399, 22660, 19866, 321, 445, 2825, 466, 11, 3993, 456, 307, 572, 637, 685, 507, 510, 13, 51214, 51214, 400, 264, 1778, 456, 307, 572, 637, 685, 507, 307, 570, 3034, 1478, 8399, 22660, 378, 433, 764, 1071, 636, 295, 22083, 264, 1589, 2701, 295, 264, 3089, 538, 1936, 1455, 264, 3089, 24518, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08629149619979087, "compression_ratio": 1.6827309236947792, "no_speech_prob": 3.426556577323936e-05}, {"id": 505, "seek": 445200, "start": 4459.0, "end": 4469.0, "text": " Now, the difference with previous... so this looks very, very similar to the type of autoencoder we just talked about, except there is no sparsity here.", "tokens": [50364, 400, 550, 1176, 1709, 807, 257, 979, 19866, 11, 597, 31499, 82, 398, 11, 293, 300, 311, 428, 31565, 6713, 13, 50714, 50714, 823, 11, 264, 2649, 365, 3894, 485, 370, 341, 1542, 588, 11, 588, 2531, 281, 264, 2010, 295, 8399, 22660, 19866, 321, 445, 2825, 466, 11, 3993, 456, 307, 572, 637, 685, 507, 510, 13, 51214, 51214, 400, 264, 1778, 456, 307, 572, 637, 685, 507, 307, 570, 3034, 1478, 8399, 22660, 378, 433, 764, 1071, 636, 295, 22083, 264, 1589, 2701, 295, 264, 3089, 538, 1936, 1455, 264, 3089, 24518, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08629149619979087, "compression_ratio": 1.6827309236947792, "no_speech_prob": 3.426556577323936e-05}, {"id": 506, "seek": 445200, "start": 4469.0, "end": 4479.0, "text": " And the reason there is no sparsity is because variational autoencoders use another way of limiting the information content of the code by basically making the code noisy.", "tokens": [50364, 400, 550, 1176, 1709, 807, 257, 979, 19866, 11, 597, 31499, 82, 398, 11, 293, 300, 311, 428, 31565, 6713, 13, 50714, 50714, 823, 11, 264, 2649, 365, 3894, 485, 370, 341, 1542, 588, 11, 588, 2531, 281, 264, 2010, 295, 8399, 22660, 19866, 321, 445, 2825, 466, 11, 3993, 456, 307, 572, 637, 685, 507, 510, 13, 51214, 51214, 400, 264, 1778, 456, 307, 572, 637, 685, 507, 307, 570, 3034, 1478, 8399, 22660, 378, 433, 764, 1071, 636, 295, 22083, 264, 1589, 2701, 295, 264, 3089, 538, 1936, 1455, 264, 3089, 24518, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08629149619979087, "compression_ratio": 1.6827309236947792, "no_speech_prob": 3.426556577323936e-05}, {"id": 507, "seek": 447900, "start": 4479.0, "end": 4484.0, "text": " So here is the idea.", "tokens": [50364, 407, 510, 307, 264, 1558, 13, 50614, 50614, 440, 636, 291, 14722, 1176, 307, 406, 538, 46608, 264, 2281, 2445, 365, 3104, 281, 1176, 11, 457, 538, 21179, 1176, 16979, 11, 4650, 281, 257, 7316, 6104, 41473, 32674, 307, 264, 2063, 300, 6123, 309, 281, 1176, 2159, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.07640328774085411, "compression_ratio": 1.4189189189189189, "no_speech_prob": 2.0144269001320936e-05}, {"id": 508, "seek": 447900, "start": 4484.0, "end": 4501.0, "text": " The way you compute Z is not by minimizing the energy function with respect to Z, but by sampling Z randomly, according to a distribution whose logarithm is the cost that links it to Z bar.", "tokens": [50364, 407, 510, 307, 264, 1558, 13, 50614, 50614, 440, 636, 291, 14722, 1176, 307, 406, 538, 46608, 264, 2281, 2445, 365, 3104, 281, 1176, 11, 457, 538, 21179, 1176, 16979, 11, 4650, 281, 257, 7316, 6104, 41473, 32674, 307, 264, 2063, 300, 6123, 309, 281, 1176, 2159, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.07640328774085411, "compression_ratio": 1.4189189189189189, "no_speech_prob": 2.0144269001320936e-05}, {"id": 509, "seek": 450100, "start": 4501.0, "end": 4511.0, "text": " So basically, the encoder produces a Z bar, and then there is an energy function that measures the distance, if you want, between Z and Z bar.", "tokens": [50364, 407, 1936, 11, 264, 2058, 19866, 14725, 257, 1176, 2159, 11, 293, 550, 456, 307, 364, 2281, 2445, 300, 8000, 264, 4560, 11, 498, 291, 528, 11, 1296, 1176, 293, 1176, 2159, 13, 50864, 50864, 509, 519, 295, 341, 382, 264, 41473, 32674, 295, 257, 8482, 7316, 13, 51114, 51114, 407, 498, 341, 4560, 307, 257, 3732, 462, 1311, 31264, 282, 4560, 11, 437, 300, 1355, 307, 300, 264, 7316, 295, 1176, 307, 516, 281, 312, 257, 27708, 39148, 11, 689, 264, 914, 307, 1176, 2159, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05712053801987197, "compression_ratio": 1.6894977168949772, "no_speech_prob": 1.3419043170870282e-05}, {"id": 510, "seek": 450100, "start": 4511.0, "end": 4516.0, "text": " You think of this as the logarithm of a probability distribution.", "tokens": [50364, 407, 1936, 11, 264, 2058, 19866, 14725, 257, 1176, 2159, 11, 293, 550, 456, 307, 364, 2281, 2445, 300, 8000, 264, 4560, 11, 498, 291, 528, 11, 1296, 1176, 293, 1176, 2159, 13, 50864, 50864, 509, 519, 295, 341, 382, 264, 41473, 32674, 295, 257, 8482, 7316, 13, 51114, 51114, 407, 498, 341, 4560, 307, 257, 3732, 462, 1311, 31264, 282, 4560, 11, 437, 300, 1355, 307, 300, 264, 7316, 295, 1176, 307, 516, 281, 312, 257, 27708, 39148, 11, 689, 264, 914, 307, 1176, 2159, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05712053801987197, "compression_ratio": 1.6894977168949772, "no_speech_prob": 1.3419043170870282e-05}, {"id": 511, "seek": 450100, "start": 4516.0, "end": 4528.0, "text": " So if this distance is a square Euclidean distance, what that means is that the distribution of Z is going to be a conditional Gaussian, where the mean is Z bar.", "tokens": [50364, 407, 1936, 11, 264, 2058, 19866, 14725, 257, 1176, 2159, 11, 293, 550, 456, 307, 364, 2281, 2445, 300, 8000, 264, 4560, 11, 498, 291, 528, 11, 1296, 1176, 293, 1176, 2159, 13, 50864, 50864, 509, 519, 295, 341, 382, 264, 41473, 32674, 295, 257, 8482, 7316, 13, 51114, 51114, 407, 498, 341, 4560, 307, 257, 3732, 462, 1311, 31264, 282, 4560, 11, 437, 300, 1355, 307, 300, 264, 7316, 295, 1176, 307, 516, 281, 312, 257, 27708, 39148, 11, 689, 264, 914, 307, 1176, 2159, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05712053801987197, "compression_ratio": 1.6894977168949772, "no_speech_prob": 1.3419043170870282e-05}, {"id": 512, "seek": 452800, "start": 4528.0, "end": 4536.0, "text": " So what we're going to do is we're going to sample a random value of Z according to that distribution, basically a Gaussian whose mean is Z bar.", "tokens": [50364, 407, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 6889, 257, 4974, 2158, 295, 1176, 4650, 281, 300, 7316, 11, 1936, 257, 39148, 6104, 914, 307, 1176, 2159, 13, 50764, 50764, 400, 300, 445, 1355, 5127, 257, 857, 295, 39148, 5658, 281, 1176, 2159, 13, 663, 311, 437, 527, 1176, 307, 516, 281, 312, 13, 51114, 51114, 400, 291, 1190, 341, 807, 264, 979, 19866, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.07314995543597495, "compression_ratio": 1.582857142857143, "no_speech_prob": 3.763171116588637e-05}, {"id": 513, "seek": 452800, "start": 4536.0, "end": 4543.0, "text": " And that just means adding a bit of Gaussian noise to Z bar. That's what our Z is going to be.", "tokens": [50364, 407, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 6889, 257, 4974, 2158, 295, 1176, 4650, 281, 300, 7316, 11, 1936, 257, 39148, 6104, 914, 307, 1176, 2159, 13, 50764, 50764, 400, 300, 445, 1355, 5127, 257, 857, 295, 39148, 5658, 281, 1176, 2159, 13, 663, 311, 437, 527, 1176, 307, 516, 281, 312, 13, 51114, 51114, 400, 291, 1190, 341, 807, 264, 979, 19866, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.07314995543597495, "compression_ratio": 1.582857142857143, "no_speech_prob": 3.763171116588637e-05}, {"id": 514, "seek": 452800, "start": 4543.0, "end": 4546.0, "text": " And you run this through the decoder.", "tokens": [50364, 407, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 6889, 257, 4974, 2158, 295, 1176, 4650, 281, 300, 7316, 11, 1936, 257, 39148, 6104, 914, 307, 1176, 2159, 13, 50764, 50764, 400, 300, 445, 1355, 5127, 257, 857, 295, 39148, 5658, 281, 1176, 2159, 13, 663, 311, 437, 527, 1176, 307, 516, 281, 312, 13, 51114, 51114, 400, 291, 1190, 341, 807, 264, 979, 19866, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.07314995543597495, "compression_ratio": 1.582857142857143, "no_speech_prob": 3.763171116588637e-05}, {"id": 515, "seek": 454600, "start": 4546.0, "end": 4566.0, "text": " So when you train a system like this, what the system wants to do is basically make the Z bar as large as possible, make the Z bar vectors as large as possible, so that the effect of the Gaussian noise on Z would be as small as possible, relatively speaking.", "tokens": [50364, 407, 562, 291, 3847, 257, 1185, 411, 341, 11, 437, 264, 1185, 2738, 281, 360, 307, 1936, 652, 264, 1176, 2159, 382, 2416, 382, 1944, 11, 652, 264, 1176, 2159, 18875, 382, 2416, 382, 1944, 11, 370, 300, 264, 1802, 295, 264, 39148, 5658, 322, 1176, 576, 312, 382, 1359, 382, 1944, 11, 7226, 4124, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.05925416946411133, "compression_ratio": 1.6862745098039216, "no_speech_prob": 3.219029895262793e-05}, {"id": 516, "seek": 456600, "start": 4566.0, "end": 4580.0, "text": " If the variance of the noise on Z is one, and you make the Z bar vector very, very long, like a norm of a thousand, then the importance of the noise would be 0.1% with respect to Z.", "tokens": [50364, 759, 264, 21977, 295, 264, 5658, 322, 1176, 307, 472, 11, 293, 291, 652, 264, 1176, 2159, 8062, 588, 11, 588, 938, 11, 411, 257, 2026, 295, 257, 4714, 11, 550, 264, 7379, 295, 264, 5658, 576, 312, 1958, 13, 16, 4, 365, 3104, 281, 1176, 13, 51064, 51064, 407, 498, 291, 3847, 364, 8399, 22660, 19866, 411, 341, 11, 26258, 264, 1186, 300, 291, 909, 5658, 538, 445, 646, 79, 1513, 559, 399, 11, 437, 291, 603, 483, 307, 1176, 2159, 18875, 300, 483, 3801, 293, 3801, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1191038111204742, "compression_ratio": 1.624413145539906, "no_speech_prob": 1.7777654647943564e-05}, {"id": 517, "seek": 456600, "start": 4580.0, "end": 4592.0, "text": " So if you train an autoencoder like this, ignoring the fact that you add noise by just backpropagation, what you'll get is Z bar vectors that get bigger and bigger.", "tokens": [50364, 759, 264, 21977, 295, 264, 5658, 322, 1176, 307, 472, 11, 293, 291, 652, 264, 1176, 2159, 8062, 588, 11, 588, 938, 11, 411, 257, 2026, 295, 257, 4714, 11, 550, 264, 7379, 295, 264, 5658, 576, 312, 1958, 13, 16, 4, 365, 3104, 281, 1176, 13, 51064, 51064, 407, 498, 291, 3847, 364, 8399, 22660, 19866, 411, 341, 11, 26258, 264, 1186, 300, 291, 909, 5658, 538, 445, 646, 79, 1513, 559, 399, 11, 437, 291, 603, 483, 307, 1176, 2159, 18875, 300, 483, 3801, 293, 3801, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1191038111204742, "compression_ratio": 1.624413145539906, "no_speech_prob": 1.7777654647943564e-05}, {"id": 518, "seek": 459200, "start": 4592.0, "end": 4599.0, "text": " The weights of the encoder will get bigger and bigger, and the Z bar vector will get bigger and bigger.", "tokens": [50364, 440, 17443, 295, 264, 2058, 19866, 486, 483, 3801, 293, 3801, 11, 293, 264, 1176, 2159, 8062, 486, 483, 3801, 293, 3801, 13, 50714, 50714, 407, 437, 311, 264, 4282, 294, 3034, 1478, 8399, 22660, 19866, 30, 50864, 50864, 6962, 322, 11, 1702, 1168, 13, 2305, 775, 341, 1176, 808, 490, 30, 1119, 1176, 257, 48994, 7006, 1128, 13095, 30, 51164, 51164, 467, 311, 257, 48994, 7006, 300, 321, 366, 21179, 293, 321, 434, 406, 46608, 365, 3104, 281, 309, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.12075653076171874, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.00010541924712015316}, {"id": 519, "seek": 459200, "start": 4599.0, "end": 4602.0, "text": " So what's the trick in variational autoencoder?", "tokens": [50364, 440, 17443, 295, 264, 2058, 19866, 486, 483, 3801, 293, 3801, 11, 293, 264, 1176, 2159, 8062, 486, 483, 3801, 293, 3801, 13, 50714, 50714, 407, 437, 311, 264, 4282, 294, 3034, 1478, 8399, 22660, 19866, 30, 50864, 50864, 6962, 322, 11, 1702, 1168, 13, 2305, 775, 341, 1176, 808, 490, 30, 1119, 1176, 257, 48994, 7006, 1128, 13095, 30, 51164, 51164, 467, 311, 257, 48994, 7006, 300, 321, 366, 21179, 293, 321, 434, 406, 46608, 365, 3104, 281, 309, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.12075653076171874, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.00010541924712015316}, {"id": 520, "seek": 459200, "start": 4602.0, "end": 4608.0, "text": " Hold on, quick question. Where does this Z come from? Is Z a latent variable never observed?", "tokens": [50364, 440, 17443, 295, 264, 2058, 19866, 486, 483, 3801, 293, 3801, 11, 293, 264, 1176, 2159, 8062, 486, 483, 3801, 293, 3801, 13, 50714, 50714, 407, 437, 311, 264, 4282, 294, 3034, 1478, 8399, 22660, 19866, 30, 50864, 50864, 6962, 322, 11, 1702, 1168, 13, 2305, 775, 341, 1176, 808, 490, 30, 1119, 1176, 257, 48994, 7006, 1128, 13095, 30, 51164, 51164, 467, 311, 257, 48994, 7006, 300, 321, 366, 21179, 293, 321, 434, 406, 46608, 365, 3104, 281, 309, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.12075653076171874, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.00010541924712015316}, {"id": 521, "seek": 459200, "start": 4608.0, "end": 4612.0, "text": " It's a latent variable that we are sampling and we're not minimizing with respect to it.", "tokens": [50364, 440, 17443, 295, 264, 2058, 19866, 486, 483, 3801, 293, 3801, 11, 293, 264, 1176, 2159, 8062, 486, 483, 3801, 293, 3801, 13, 50714, 50714, 407, 437, 311, 264, 4282, 294, 3034, 1478, 8399, 22660, 19866, 30, 50864, 50864, 6962, 322, 11, 1702, 1168, 13, 2305, 775, 341, 1176, 808, 490, 30, 1119, 1176, 257, 48994, 7006, 1128, 13095, 30, 51164, 51164, 467, 311, 257, 48994, 7006, 300, 321, 366, 21179, 293, 321, 434, 406, 46608, 365, 3104, 281, 309, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.12075653076171874, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.00010541924712015316}, {"id": 522, "seek": 461200, "start": 4612.0, "end": 4626.0, "text": " So in previous cases, we were minimizing with respect to the Z variable, minimizing the energy with respect to the variable, finding the Z that minimizes the sum of C, D, and R.", "tokens": [50364, 407, 294, 3894, 3331, 11, 321, 645, 46608, 365, 3104, 281, 264, 1176, 7006, 11, 46608, 264, 2281, 365, 3104, 281, 264, 7006, 11, 5006, 264, 1176, 300, 4464, 5660, 264, 2408, 295, 383, 11, 413, 11, 293, 497, 13, 51064, 51064, 407, 510, 321, 434, 406, 46608, 11, 321, 434, 445, 21179, 13, 492, 434, 17480, 264, 2281, 382, 257, 7316, 11, 382, 257, 3565, 295, 257, 7316, 11, 293, 321, 434, 21179, 1176, 490, 300, 7316, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08122029936457255, "compression_ratio": 1.875, "no_speech_prob": 9.368162864120677e-06}, {"id": 523, "seek": 461200, "start": 4626.0, "end": 4636.0, "text": " So here we're not minimizing, we're just sampling. We're viewing the energy as a distribution, as a log of a distribution, and we're sampling Z from that distribution.", "tokens": [50364, 407, 294, 3894, 3331, 11, 321, 645, 46608, 365, 3104, 281, 264, 1176, 7006, 11, 46608, 264, 2281, 365, 3104, 281, 264, 7006, 11, 5006, 264, 1176, 300, 4464, 5660, 264, 2408, 295, 383, 11, 413, 11, 293, 497, 13, 51064, 51064, 407, 510, 321, 434, 406, 46608, 11, 321, 434, 445, 21179, 13, 492, 434, 17480, 264, 2281, 382, 257, 7316, 11, 382, 257, 3565, 295, 257, 7316, 11, 293, 321, 434, 21179, 1176, 490, 300, 7316, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08122029936457255, "compression_ratio": 1.875, "no_speech_prob": 9.368162864120677e-06}, {"id": 524, "seek": 463600, "start": 4636.0, "end": 4642.0, "text": " All right, so imagine our encoder produces the following points for training samples.", "tokens": [50364, 1057, 558, 11, 370, 3811, 527, 2058, 19866, 14725, 264, 3480, 2793, 337, 3097, 10938, 13, 50664, 50664, 407, 613, 366, 264, 1176, 2159, 18875, 7126, 538, 264, 2058, 19866, 412, 512, 935, 294, 3097, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.07579997777938843, "compression_ratio": 1.3916666666666666, "no_speech_prob": 5.421929017757066e-06}, {"id": 525, "seek": 463600, "start": 4642.0, "end": 4654.0, "text": " So these are the Z bar vectors produced by the encoder at some point in training.", "tokens": [50364, 1057, 558, 11, 370, 3811, 527, 2058, 19866, 14725, 264, 3480, 2793, 337, 3097, 10938, 13, 50664, 50664, 407, 613, 366, 264, 1176, 2159, 18875, 7126, 538, 264, 2058, 19866, 412, 512, 935, 294, 3097, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.07579997777938843, "compression_ratio": 1.3916666666666666, "no_speech_prob": 5.421929017757066e-06}, {"id": 526, "seek": 465400, "start": 4654.0, "end": 4667.0, "text": " So what the effect of this sampling of Z is going to do is basically turn every single one of those training samples into a fuzzy ball.", "tokens": [50364, 407, 437, 264, 1802, 295, 341, 21179, 295, 1176, 307, 516, 281, 360, 307, 1936, 1261, 633, 2167, 472, 295, 729, 3097, 10938, 666, 257, 34710, 2594, 13, 51014, 51014, 1436, 321, 747, 257, 6889, 11, 321, 909, 5658, 281, 309, 11, 293, 370, 1936, 321, 600, 3574, 257, 2167, 3089, 8062, 666, 733, 295, 257, 34710, 2594, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05365566223386734, "compression_ratio": 1.579268292682927, "no_speech_prob": 5.507471996679669e-06}, {"id": 527, "seek": 465400, "start": 4667.0, "end": 4677.0, "text": " Because we take a sample, we add noise to it, and so basically we've turned a single code vector into kind of a fuzzy ball.", "tokens": [50364, 407, 437, 264, 1802, 295, 341, 21179, 295, 1176, 307, 516, 281, 360, 307, 1936, 1261, 633, 2167, 472, 295, 729, 3097, 10938, 666, 257, 34710, 2594, 13, 51014, 51014, 1436, 321, 747, 257, 6889, 11, 321, 909, 5658, 281, 309, 11, 293, 370, 1936, 321, 600, 3574, 257, 2167, 3089, 8062, 666, 733, 295, 257, 34710, 2594, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05365566223386734, "compression_ratio": 1.579268292682927, "no_speech_prob": 5.507471996679669e-06}, {"id": 528, "seek": 467700, "start": 4677.0, "end": 4686.0, "text": " Now, the decoder needs to be able to reconstruct the input from whatever code is being fed.", "tokens": [50364, 823, 11, 264, 979, 19866, 2203, 281, 312, 1075, 281, 31499, 264, 4846, 490, 2035, 3089, 307, 885, 4636, 13, 50814, 50814, 400, 370, 498, 732, 295, 729, 34710, 2594, 27815, 11, 550, 456, 307, 512, 8482, 337, 264, 979, 19866, 281, 1936, 652, 257, 6146, 293, 28584, 264, 732, 10938, 11, 28584, 472, 6889, 337, 264, 661, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06341765040443056, "compression_ratio": 1.583815028901734, "no_speech_prob": 5.954910648142686e-06}, {"id": 529, "seek": 467700, "start": 4686.0, "end": 4702.0, "text": " And so if two of those fuzzy ball intersect, then there is some probability for the decoder to basically make a mistake and confuse the two samples, confuse one sample for the other.", "tokens": [50364, 823, 11, 264, 979, 19866, 2203, 281, 312, 1075, 281, 31499, 264, 4846, 490, 2035, 3089, 307, 885, 4636, 13, 50814, 50814, 400, 370, 498, 732, 295, 729, 34710, 2594, 27815, 11, 550, 456, 307, 512, 8482, 337, 264, 979, 19866, 281, 1936, 652, 257, 6146, 293, 28584, 264, 732, 10938, 11, 28584, 472, 6889, 337, 264, 661, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06341765040443056, "compression_ratio": 1.583815028901734, "no_speech_prob": 5.954910648142686e-06}, {"id": 530, "seek": 470200, "start": 4702.0, "end": 4715.0, "text": " So the effect of training the system, if you add fuzzy balls, if you make every one of your code a fuzzy ball, is that those fuzzy balls are going to fly away from each other.", "tokens": [50364, 407, 264, 1802, 295, 3097, 264, 1185, 11, 498, 291, 909, 34710, 9803, 11, 498, 291, 652, 633, 472, 295, 428, 3089, 257, 34710, 2594, 11, 307, 300, 729, 34710, 9803, 366, 516, 281, 3603, 1314, 490, 1184, 661, 13, 51014, 51014, 400, 382, 286, 848, 949, 11, 341, 307, 264, 912, 11, 286, 390, 1566, 341, 949, 294, 257, 819, 636, 11, 309, 311, 516, 281, 652, 264, 17443, 295, 264, 2058, 19866, 588, 2416, 370, 300, 264, 3089, 18875, 483, 588, 938, 293, 1936, 436, 483, 1314, 490, 1184, 661, 11, 293, 264, 5658, 295, 729, 34710, 9803, 500, 380, 1871, 3602, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07071493743756495, "compression_ratio": 1.832, "no_speech_prob": 2.2826649001217447e-05}, {"id": 531, "seek": 470200, "start": 4715.0, "end": 4730.0, "text": " And as I said before, this is the same, I was saying this before in a different way, it's going to make the weights of the encoder very large so that the code vectors get very long and basically they get away from each other, and the noise of those fuzzy balls don't matter anymore.", "tokens": [50364, 407, 264, 1802, 295, 3097, 264, 1185, 11, 498, 291, 909, 34710, 9803, 11, 498, 291, 652, 633, 472, 295, 428, 3089, 257, 34710, 2594, 11, 307, 300, 729, 34710, 9803, 366, 516, 281, 3603, 1314, 490, 1184, 661, 13, 51014, 51014, 400, 382, 286, 848, 949, 11, 341, 307, 264, 912, 11, 286, 390, 1566, 341, 949, 294, 257, 819, 636, 11, 309, 311, 516, 281, 652, 264, 17443, 295, 264, 2058, 19866, 588, 2416, 370, 300, 264, 3089, 18875, 483, 588, 938, 293, 1936, 436, 483, 1314, 490, 1184, 661, 11, 293, 264, 5658, 295, 729, 34710, 9803, 500, 380, 1871, 3602, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07071493743756495, "compression_ratio": 1.832, "no_speech_prob": 2.2826649001217447e-05}, {"id": 532, "seek": 473000, "start": 4730.0, "end": 4736.0, "text": " So here, if the fuzzy balls don't intersect, the system will be able to perfectly reconstruct every sample you throw at it.", "tokens": [50364, 407, 510, 11, 498, 264, 34710, 9803, 500, 380, 27815, 11, 264, 1185, 486, 312, 1075, 281, 6239, 31499, 633, 6889, 291, 3507, 412, 309, 13, 50664, 50664, 1222, 1168, 11, 309, 390, 257, 1916, 295, 9788, 2057, 11, 457, 797, 322, 264, 912, 4829, 11, 309, 390, 257, 1916, 295, 9788, 2057, 13, 50914, 50914, 407, 437, 2293, 360, 291, 914, 538, 40520, 473, 4122, 510, 562, 291, 848, 300, 562, 291, 645, 15763, 2698, 12, 48172, 6763, 293, 2710, 2584, 32675, 30, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.12470065463672984, "compression_ratio": 1.6506550218340612, "no_speech_prob": 4.7565172280883417e-05}, {"id": 533, "seek": 473000, "start": 4736.0, "end": 4741.0, "text": " My question, it was a couple of slides ago, but again on the same topic, it was a couple of slides ago.", "tokens": [50364, 407, 510, 11, 498, 264, 34710, 9803, 500, 380, 27815, 11, 264, 1185, 486, 312, 1075, 281, 6239, 31499, 633, 6889, 291, 3507, 412, 309, 13, 50664, 50664, 1222, 1168, 11, 309, 390, 257, 1916, 295, 9788, 2057, 11, 457, 797, 322, 264, 912, 4829, 11, 309, 390, 257, 1916, 295, 9788, 2057, 13, 50914, 50914, 407, 437, 2293, 360, 291, 914, 538, 40520, 473, 4122, 510, 562, 291, 848, 300, 562, 291, 645, 15763, 2698, 12, 48172, 6763, 293, 2710, 2584, 32675, 30, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.12470065463672984, "compression_ratio": 1.6506550218340612, "no_speech_prob": 4.7565172280883417e-05}, {"id": 534, "seek": 473000, "start": 4741.0, "end": 4751.0, "text": " So what exactly do you mean by degenerate features here when you said that when you were comparing self-supervision and normal completely supervision?", "tokens": [50364, 407, 510, 11, 498, 264, 34710, 9803, 500, 380, 27815, 11, 264, 1185, 486, 312, 1075, 281, 6239, 31499, 633, 6889, 291, 3507, 412, 309, 13, 50664, 50664, 1222, 1168, 11, 309, 390, 257, 1916, 295, 9788, 2057, 11, 457, 797, 322, 264, 912, 4829, 11, 309, 390, 257, 1916, 295, 9788, 2057, 13, 50914, 50914, 407, 437, 2293, 360, 291, 914, 538, 40520, 473, 4122, 510, 562, 291, 848, 300, 562, 291, 645, 15763, 2698, 12, 48172, 6763, 293, 2710, 2584, 32675, 30, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.12470065463672984, "compression_ratio": 1.6506550218340612, "no_speech_prob": 4.7565172280883417e-05}, {"id": 535, "seek": 475100, "start": 4751.0, "end": 4769.0, "text": " I see. Okay, that's a good question. What I was saying is, it's something I said before in different terms, it's the fact that if you train a classifier, let's say a convolutional net, on a problem that has very few categories, let's say face detection, you only have two categories,", "tokens": [50364, 286, 536, 13, 1033, 11, 300, 311, 257, 665, 1168, 13, 708, 286, 390, 1566, 307, 11, 309, 311, 746, 286, 848, 949, 294, 819, 2115, 11, 309, 311, 264, 1186, 300, 498, 291, 3847, 257, 1508, 9902, 11, 718, 311, 584, 257, 45216, 304, 2533, 11, 322, 257, 1154, 300, 575, 588, 1326, 10479, 11, 718, 311, 584, 1851, 17784, 11, 291, 787, 362, 732, 10479, 11, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.08981585502624512, "compression_ratio": 1.5635359116022098, "no_speech_prob": 3.315881622256711e-05}, {"id": 536, "seek": 476900, "start": 4769.0, "end": 4784.0, "text": " the representation of faces you get out of the convolutional net are very degenerate in the sense that they don't represent every image properly. They're going to kind of collapse a lot of different images into sort of a common,", "tokens": [50364, 264, 10290, 295, 8475, 291, 483, 484, 295, 264, 45216, 304, 2533, 366, 588, 40520, 473, 294, 264, 2020, 300, 436, 500, 380, 2906, 633, 3256, 6108, 13, 814, 434, 516, 281, 733, 295, 15584, 257, 688, 295, 819, 5267, 666, 1333, 295, 257, 2689, 11, 51114, 51114, 666, 14800, 33358, 570, 264, 787, 551, 264, 1185, 2203, 281, 360, 307, 47833, 490, 8475, 365, 2107, 12, 69, 2116, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07638445416012325, "compression_ratio": 1.6730769230769231, "no_speech_prob": 1.4966080925660208e-05}, {"id": 537, "seek": 476900, "start": 4784.0, "end": 4793.0, "text": " into identical representations because the only thing the system needs to do is discriminate from faces with non-faces.", "tokens": [50364, 264, 10290, 295, 8475, 291, 483, 484, 295, 264, 45216, 304, 2533, 366, 588, 40520, 473, 294, 264, 2020, 300, 436, 500, 380, 2906, 633, 3256, 6108, 13, 814, 434, 516, 281, 733, 295, 15584, 257, 688, 295, 819, 5267, 666, 1333, 295, 257, 2689, 11, 51114, 51114, 666, 14800, 33358, 570, 264, 787, 551, 264, 1185, 2203, 281, 360, 307, 47833, 490, 8475, 365, 2107, 12, 69, 2116, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07638445416012325, "compression_ratio": 1.6730769230769231, "no_speech_prob": 1.4966080925660208e-05}, {"id": 538, "seek": 479300, "start": 4793.0, "end": 4800.0, "text": " And so it doesn't need to really kind of produce good representations of the entire space.", "tokens": [50364, 400, 370, 309, 1177, 380, 643, 281, 534, 733, 295, 5258, 665, 33358, 295, 264, 2302, 1901, 13, 50714, 50714, 467, 445, 2203, 281, 980, 291, 498, 309, 311, 257, 1851, 420, 406, 257, 1851, 13, 407, 337, 1365, 11, 264, 4122, 291, 486, 483, 337, 732, 819, 8475, 486, 1391, 312, 6457, 14800, 13, 51164, 51164, 407, 300, 311, 437, 286, 914, 538, 40520, 473, 4122, 13, 708, 291, 528, 366, 4122, 300, 1936, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07416561245918274, "compression_ratio": 1.6, "no_speech_prob": 1.3006072549615055e-05}, {"id": 539, "seek": 479300, "start": 4800.0, "end": 4809.0, "text": " It just needs to tell you if it's a face or not a face. So for example, the features you will get for two different faces will probably be fairly identical.", "tokens": [50364, 400, 370, 309, 1177, 380, 643, 281, 534, 733, 295, 5258, 665, 33358, 295, 264, 2302, 1901, 13, 50714, 50714, 467, 445, 2203, 281, 980, 291, 498, 309, 311, 257, 1851, 420, 406, 257, 1851, 13, 407, 337, 1365, 11, 264, 4122, 291, 486, 483, 337, 732, 819, 8475, 486, 1391, 312, 6457, 14800, 13, 51164, 51164, 407, 300, 311, 437, 286, 914, 538, 40520, 473, 4122, 13, 708, 291, 528, 366, 4122, 300, 1936, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07416561245918274, "compression_ratio": 1.6, "no_speech_prob": 1.3006072549615055e-05}, {"id": 540, "seek": 479300, "start": 4809.0, "end": 4818.0, "text": " So that's what I mean by degenerate features. What you want are features that basically,", "tokens": [50364, 400, 370, 309, 1177, 380, 643, 281, 534, 733, 295, 5258, 665, 33358, 295, 264, 2302, 1901, 13, 50714, 50714, 467, 445, 2203, 281, 980, 291, 498, 309, 311, 257, 1851, 420, 406, 257, 1851, 13, 407, 337, 1365, 11, 264, 4122, 291, 486, 483, 337, 732, 819, 8475, 486, 1391, 312, 6457, 14800, 13, 51164, 51164, 407, 300, 311, 437, 286, 914, 538, 40520, 473, 4122, 13, 708, 291, 528, 366, 4122, 300, 1936, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07416561245918274, "compression_ratio": 1.6, "no_speech_prob": 1.3006072549615055e-05}, {"id": 541, "seek": 481800, "start": 4818.0, "end": 4828.0, "text": " feature vectors that are different for different objects, regardless of whether you train them to be different or not. So if you train on ImageNet, for example, you have 1000 categories.", "tokens": [50364, 4111, 18875, 300, 366, 819, 337, 819, 6565, 11, 10060, 295, 1968, 291, 3847, 552, 281, 312, 819, 420, 406, 13, 407, 498, 291, 3847, 322, 29903, 31890, 11, 337, 1365, 11, 291, 362, 9714, 10479, 13, 50864, 50864, 400, 370, 570, 291, 362, 257, 688, 295, 10479, 11, 291, 483, 4122, 300, 366, 6457, 9521, 293, 436, 2060, 257, 688, 295, 264, 1901, 295, 1944, 5267, 13, 51264, 51264, 286, 914, 11, 436, 434, 920, 733, 295, 6457, 19813, 11, 457, 436, 434, 406, 2584, 40520, 473, 570, 291, 362, 867, 10479, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08945477738672374, "compression_ratio": 1.8137651821862348, "no_speech_prob": 1.3006155313632917e-05}, {"id": 542, "seek": 481800, "start": 4828.0, "end": 4836.0, "text": " And so because you have a lot of categories, you get features that are fairly diverse and they cover a lot of the space of possible images.", "tokens": [50364, 4111, 18875, 300, 366, 819, 337, 819, 6565, 11, 10060, 295, 1968, 291, 3847, 552, 281, 312, 819, 420, 406, 13, 407, 498, 291, 3847, 322, 29903, 31890, 11, 337, 1365, 11, 291, 362, 9714, 10479, 13, 50864, 50864, 400, 370, 570, 291, 362, 257, 688, 295, 10479, 11, 291, 483, 4122, 300, 366, 6457, 9521, 293, 436, 2060, 257, 688, 295, 264, 1901, 295, 1944, 5267, 13, 51264, 51264, 286, 914, 11, 436, 434, 920, 733, 295, 6457, 19813, 11, 457, 436, 434, 406, 2584, 40520, 473, 570, 291, 362, 867, 10479, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08945477738672374, "compression_ratio": 1.8137651821862348, "no_speech_prob": 1.3006155313632917e-05}, {"id": 543, "seek": 481800, "start": 4836.0, "end": 4842.0, "text": " I mean, they're still kind of fairly specialized, but they're not completely degenerate because you have many categories.", "tokens": [50364, 4111, 18875, 300, 366, 819, 337, 819, 6565, 11, 10060, 295, 1968, 291, 3847, 552, 281, 312, 819, 420, 406, 13, 407, 498, 291, 3847, 322, 29903, 31890, 11, 337, 1365, 11, 291, 362, 9714, 10479, 13, 50864, 50864, 400, 370, 570, 291, 362, 257, 688, 295, 10479, 11, 291, 483, 4122, 300, 366, 6457, 9521, 293, 436, 2060, 257, 688, 295, 264, 1901, 295, 1944, 5267, 13, 51264, 51264, 286, 914, 11, 436, 434, 920, 733, 295, 6457, 19813, 11, 457, 436, 434, 406, 2584, 40520, 473, 570, 291, 362, 867, 10479, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08945477738672374, "compression_ratio": 1.8137651821862348, "no_speech_prob": 1.3006155313632917e-05}, {"id": 544, "seek": 484200, "start": 4842.0, "end": 4848.0, "text": " And you have a lot of samples. The more samples and the more categories you have, the better your features are.", "tokens": [50364, 400, 291, 362, 257, 688, 295, 10938, 13, 440, 544, 10938, 293, 264, 544, 10479, 291, 362, 11, 264, 1101, 428, 4122, 366, 13, 50664, 50664, 682, 1186, 11, 498, 291, 519, 466, 309, 11, 364, 8399, 22660, 19866, 307, 257, 18161, 2533, 294, 597, 633, 3097, 6889, 307, 1080, 1065, 7719, 13, 51214, 51214, 1779, 30, 1436, 291, 434, 1936, 3585, 264, 1185, 281, 5258, 257, 819, 5598, 337, 633, 6889, 286, 855, 291, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08586872816085815, "compression_ratio": 1.5849056603773586, "no_speech_prob": 1.384532151860185e-05}, {"id": 545, "seek": 484200, "start": 4848.0, "end": 4859.0, "text": " In fact, if you think about it, an autoencoder is a neural net in which every training sample is its own category.", "tokens": [50364, 400, 291, 362, 257, 688, 295, 10938, 13, 440, 544, 10938, 293, 264, 544, 10479, 291, 362, 11, 264, 1101, 428, 4122, 366, 13, 50664, 50664, 682, 1186, 11, 498, 291, 519, 466, 309, 11, 364, 8399, 22660, 19866, 307, 257, 18161, 2533, 294, 597, 633, 3097, 6889, 307, 1080, 1065, 7719, 13, 51214, 51214, 1779, 30, 1436, 291, 434, 1936, 3585, 264, 1185, 281, 5258, 257, 819, 5598, 337, 633, 6889, 286, 855, 291, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08586872816085815, "compression_ratio": 1.5849056603773586, "no_speech_prob": 1.384532151860185e-05}, {"id": 546, "seek": 484200, "start": 4859.0, "end": 4866.0, "text": " Right? Because you're basically telling the system to produce a different output for every sample I show you.", "tokens": [50364, 400, 291, 362, 257, 688, 295, 10938, 13, 440, 544, 10938, 293, 264, 544, 10479, 291, 362, 11, 264, 1101, 428, 4122, 366, 13, 50664, 50664, 682, 1186, 11, 498, 291, 519, 466, 309, 11, 364, 8399, 22660, 19866, 307, 257, 18161, 2533, 294, 597, 633, 3097, 6889, 307, 1080, 1065, 7719, 13, 51214, 51214, 1779, 30, 1436, 291, 434, 1936, 3585, 264, 1185, 281, 5258, 257, 819, 5598, 337, 633, 6889, 286, 855, 291, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08586872816085815, "compression_ratio": 1.5849056603773586, "no_speech_prob": 1.384532151860185e-05}, {"id": 547, "seek": 486600, "start": 4866.0, "end": 4874.0, "text": " So you're basically training the system to represent every object in a different way.", "tokens": [50364, 407, 291, 434, 1936, 3097, 264, 1185, 281, 2906, 633, 2657, 294, 257, 819, 636, 13, 50764, 50764, 583, 309, 393, 312, 40520, 770, 294, 1071, 636, 570, 264, 1185, 393, 1466, 264, 6575, 2445, 293, 2058, 1429, 1340, 291, 528, 13, 51164, 51164, 759, 291, 519, 466, 264, 318, 2918, 1130, 36170, 11, 264, 20678, 2539, 1185, 11, 264, 8712, 488, 7150, 11, 3335, 21141, 11, 3026, 75, 293, 1100, 335, 34, 5797, 11, 286, 390, 3585, 291, 466, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14800174937528723, "compression_ratio": 1.6036866359447004, "no_speech_prob": 1.0770144399430137e-05}, {"id": 548, "seek": 486600, "start": 4874.0, "end": 4882.0, "text": " But it can be degenerated in another way because the system can learn the identity function and encode anything you want.", "tokens": [50364, 407, 291, 434, 1936, 3097, 264, 1185, 281, 2906, 633, 2657, 294, 257, 819, 636, 13, 50764, 50764, 583, 309, 393, 312, 40520, 770, 294, 1071, 636, 570, 264, 1185, 393, 1466, 264, 6575, 2445, 293, 2058, 1429, 1340, 291, 528, 13, 51164, 51164, 759, 291, 519, 466, 264, 318, 2918, 1130, 36170, 11, 264, 20678, 2539, 1185, 11, 264, 8712, 488, 7150, 11, 3335, 21141, 11, 3026, 75, 293, 1100, 335, 34, 5797, 11, 286, 390, 3585, 291, 466, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14800174937528723, "compression_ratio": 1.6036866359447004, "no_speech_prob": 1.0770144399430137e-05}, {"id": 549, "seek": 486600, "start": 4882.0, "end": 4895.0, "text": " If you think about the Siamese nets, the metric learning system, the contrastive methods, MoCo, Perl and SeamClear, I was telling you about.", "tokens": [50364, 407, 291, 434, 1936, 3097, 264, 1185, 281, 2906, 633, 2657, 294, 257, 819, 636, 13, 50764, 50764, 583, 309, 393, 312, 40520, 770, 294, 1071, 636, 570, 264, 1185, 393, 1466, 264, 6575, 2445, 293, 2058, 1429, 1340, 291, 528, 13, 51164, 51164, 759, 291, 519, 466, 264, 318, 2918, 1130, 36170, 11, 264, 20678, 2539, 1185, 11, 264, 8712, 488, 7150, 11, 3335, 21141, 11, 3026, 75, 293, 1100, 335, 34, 5797, 11, 286, 390, 3585, 291, 466, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14800174937528723, "compression_ratio": 1.6036866359447004, "no_speech_prob": 1.0770144399430137e-05}, {"id": 550, "seek": 489500, "start": 4895.0, "end": 4898.0, "text": " It's a little bit of the same thing.", "tokens": [50364, 467, 311, 257, 707, 857, 295, 264, 912, 551, 13, 50514, 50514, 814, 853, 281, 1466, 2107, 12, 67, 1146, 7971, 473, 4122, 538, 3585, 264, 1185, 11, 291, 458, 11, 510, 307, 732, 6565, 300, 286, 458, 366, 264, 912, 13, 50914, 50914, 1692, 366, 732, 6565, 300, 286, 458, 366, 819, 13, 51014, 51014, 400, 370, 652, 988, 291, 5258, 819, 4111, 18875, 337, 6565, 300, 286, 458, 366, 4361, 49505, 819, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.06436432464213311, "compression_ratio": 1.752808988764045, "no_speech_prob": 2.6680916562327184e-05}, {"id": 551, "seek": 489500, "start": 4898.0, "end": 4906.0, "text": " They try to learn non-degenerate features by telling the system, you know, here is two objects that I know are the same.", "tokens": [50364, 467, 311, 257, 707, 857, 295, 264, 912, 551, 13, 50514, 50514, 814, 853, 281, 1466, 2107, 12, 67, 1146, 7971, 473, 4122, 538, 3585, 264, 1185, 11, 291, 458, 11, 510, 307, 732, 6565, 300, 286, 458, 366, 264, 912, 13, 50914, 50914, 1692, 366, 732, 6565, 300, 286, 458, 366, 819, 13, 51014, 51014, 400, 370, 652, 988, 291, 5258, 819, 4111, 18875, 337, 6565, 300, 286, 458, 366, 4361, 49505, 819, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.06436432464213311, "compression_ratio": 1.752808988764045, "no_speech_prob": 2.6680916562327184e-05}, {"id": 552, "seek": 489500, "start": 4906.0, "end": 4908.0, "text": " Here are two objects that I know are different.", "tokens": [50364, 467, 311, 257, 707, 857, 295, 264, 912, 551, 13, 50514, 50514, 814, 853, 281, 1466, 2107, 12, 67, 1146, 7971, 473, 4122, 538, 3585, 264, 1185, 11, 291, 458, 11, 510, 307, 732, 6565, 300, 286, 458, 366, 264, 912, 13, 50914, 50914, 1692, 366, 732, 6565, 300, 286, 458, 366, 819, 13, 51014, 51014, 400, 370, 652, 988, 291, 5258, 819, 4111, 18875, 337, 6565, 300, 286, 458, 366, 4361, 49505, 819, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.06436432464213311, "compression_ratio": 1.752808988764045, "no_speech_prob": 2.6680916562327184e-05}, {"id": 553, "seek": 489500, "start": 4908.0, "end": 4915.0, "text": " And so make sure you produce different feature vectors for objects that I know are semantically different.", "tokens": [50364, 467, 311, 257, 707, 857, 295, 264, 912, 551, 13, 50514, 50514, 814, 853, 281, 1466, 2107, 12, 67, 1146, 7971, 473, 4122, 538, 3585, 264, 1185, 11, 291, 458, 11, 510, 307, 732, 6565, 300, 286, 458, 366, 264, 912, 13, 50914, 50914, 1692, 366, 732, 6565, 300, 286, 458, 366, 819, 13, 51014, 51014, 400, 370, 652, 988, 291, 5258, 819, 4111, 18875, 337, 6565, 300, 286, 458, 366, 4361, 49505, 819, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.06436432464213311, "compression_ratio": 1.752808988764045, "no_speech_prob": 2.6680916562327184e-05}, {"id": 554, "seek": 491500, "start": 4915.0, "end": 4926.0, "text": " That's kind of a way of making sure you get feature vector representations that are different for things that are actually different.", "tokens": [50364, 663, 311, 733, 295, 257, 636, 295, 1455, 988, 291, 483, 4111, 8062, 33358, 300, 366, 819, 337, 721, 300, 366, 767, 819, 13, 50914, 50914, 583, 291, 500, 380, 483, 341, 538, 3097, 257, 3754, 2533, 322, 257, 732, 12, 11665, 1154, 420, 257, 2064, 12, 11665, 1154, 13, 51164, 51164, 509, 643, 382, 867, 5359, 382, 291, 393, 6157, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09353683934067235, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.355707460490521e-06}, {"id": 555, "seek": 491500, "start": 4926.0, "end": 4931.0, "text": " But you don't get this by training a conv net on a two-class problem or a ten-class problem.", "tokens": [50364, 663, 311, 733, 295, 257, 636, 295, 1455, 988, 291, 483, 4111, 8062, 33358, 300, 366, 819, 337, 721, 300, 366, 767, 819, 13, 50914, 50914, 583, 291, 500, 380, 483, 341, 538, 3097, 257, 3754, 2533, 322, 257, 732, 12, 11665, 1154, 420, 257, 2064, 12, 11665, 1154, 13, 51164, 51164, 509, 643, 382, 867, 5359, 382, 291, 393, 6157, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09353683934067235, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.355707460490521e-06}, {"id": 556, "seek": 491500, "start": 4931.0, "end": 4937.0, "text": " You need as many classes as you can afford.", "tokens": [50364, 663, 311, 733, 295, 257, 636, 295, 1455, 988, 291, 483, 4111, 8062, 33358, 300, 366, 819, 337, 721, 300, 366, 767, 819, 13, 50914, 50914, 583, 291, 500, 380, 483, 341, 538, 3097, 257, 3754, 2533, 322, 257, 732, 12, 11665, 1154, 420, 257, 2064, 12, 11665, 1154, 13, 51164, 51164, 509, 643, 382, 867, 5359, 382, 291, 393, 6157, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09353683934067235, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.355707460490521e-06}, {"id": 557, "seek": 493700, "start": 4937.0, "end": 4949.0, "text": " So pre-training using self-supervised learning basically helps making the feature more generic and less degenerate for the problem.", "tokens": [50364, 407, 659, 12, 17227, 1760, 1228, 2698, 12, 48172, 24420, 2539, 1936, 3665, 1455, 264, 4111, 544, 19577, 293, 1570, 40520, 473, 337, 264, 1154, 13, 50964, 50964, 2264, 11, 370, 718, 311, 483, 646, 281, 3034, 1478, 8399, 22660, 378, 433, 13, 51114, 51114, 407, 797, 11, 498, 291, 3847, 428, 8399, 22660, 19866, 365, 729, 34710, 9803, 11, 436, 434, 516, 281, 3603, 1314, 490, 1184, 661, 13, 51364, 51364, 400, 437, 291, 528, 534, 307, 291, 528, 729, 34710, 9803, 281, 1936, 733, 295, 13630, 926, 512, 1333, 295, 1412, 47138, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06617343305337309, "compression_ratio": 1.6518218623481782, "no_speech_prob": 5.093514118925668e-06}, {"id": 558, "seek": 493700, "start": 4949.0, "end": 4952.0, "text": " OK, so let's get back to variational autoencoders.", "tokens": [50364, 407, 659, 12, 17227, 1760, 1228, 2698, 12, 48172, 24420, 2539, 1936, 3665, 1455, 264, 4111, 544, 19577, 293, 1570, 40520, 473, 337, 264, 1154, 13, 50964, 50964, 2264, 11, 370, 718, 311, 483, 646, 281, 3034, 1478, 8399, 22660, 378, 433, 13, 51114, 51114, 407, 797, 11, 498, 291, 3847, 428, 8399, 22660, 19866, 365, 729, 34710, 9803, 11, 436, 434, 516, 281, 3603, 1314, 490, 1184, 661, 13, 51364, 51364, 400, 437, 291, 528, 534, 307, 291, 528, 729, 34710, 9803, 281, 1936, 733, 295, 13630, 926, 512, 1333, 295, 1412, 47138, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06617343305337309, "compression_ratio": 1.6518218623481782, "no_speech_prob": 5.093514118925668e-06}, {"id": 559, "seek": 493700, "start": 4952.0, "end": 4957.0, "text": " So again, if you train your autoencoder with those fuzzy balls, they're going to fly away from each other.", "tokens": [50364, 407, 659, 12, 17227, 1760, 1228, 2698, 12, 48172, 24420, 2539, 1936, 3665, 1455, 264, 4111, 544, 19577, 293, 1570, 40520, 473, 337, 264, 1154, 13, 50964, 50964, 2264, 11, 370, 718, 311, 483, 646, 281, 3034, 1478, 8399, 22660, 378, 433, 13, 51114, 51114, 407, 797, 11, 498, 291, 3847, 428, 8399, 22660, 19866, 365, 729, 34710, 9803, 11, 436, 434, 516, 281, 3603, 1314, 490, 1184, 661, 13, 51364, 51364, 400, 437, 291, 528, 534, 307, 291, 528, 729, 34710, 9803, 281, 1936, 733, 295, 13630, 926, 512, 1333, 295, 1412, 47138, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06617343305337309, "compression_ratio": 1.6518218623481782, "no_speech_prob": 5.093514118925668e-06}, {"id": 560, "seek": 493700, "start": 4957.0, "end": 4965.0, "text": " And what you want really is you want those fuzzy balls to basically kind of cluster around some sort of data manifold.", "tokens": [50364, 407, 659, 12, 17227, 1760, 1228, 2698, 12, 48172, 24420, 2539, 1936, 3665, 1455, 264, 4111, 544, 19577, 293, 1570, 40520, 473, 337, 264, 1154, 13, 50964, 50964, 2264, 11, 370, 718, 311, 483, 646, 281, 3034, 1478, 8399, 22660, 378, 433, 13, 51114, 51114, 407, 797, 11, 498, 291, 3847, 428, 8399, 22660, 19866, 365, 729, 34710, 9803, 11, 436, 434, 516, 281, 3603, 1314, 490, 1184, 661, 13, 51364, 51364, 400, 437, 291, 528, 534, 307, 291, 528, 729, 34710, 9803, 281, 1936, 733, 295, 13630, 926, 512, 1333, 295, 1412, 47138, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06617343305337309, "compression_ratio": 1.6518218623481782, "no_speech_prob": 5.093514118925668e-06}, {"id": 561, "seek": 496500, "start": 4965.0, "end": 4969.0, "text": " So you want to actually keep them as close to each other as possible.", "tokens": [50364, 407, 291, 528, 281, 767, 1066, 552, 382, 1998, 281, 1184, 661, 382, 1944, 13, 50564, 50564, 400, 577, 393, 291, 360, 341, 30, 50614, 50614, 509, 393, 360, 341, 538, 4476, 25775, 439, 295, 552, 281, 264, 4957, 365, 257, 5587, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.06304434005250321, "compression_ratio": 1.380952380952381, "no_speech_prob": 2.50700340984622e-05}, {"id": 562, "seek": 496500, "start": 4969.0, "end": 4970.0, "text": " And how can you do this?", "tokens": [50364, 407, 291, 528, 281, 767, 1066, 552, 382, 1998, 281, 1184, 661, 382, 1944, 13, 50564, 50564, 400, 577, 393, 291, 360, 341, 30, 50614, 50614, 509, 393, 360, 341, 538, 4476, 25775, 439, 295, 552, 281, 264, 4957, 365, 257, 5587, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.06304434005250321, "compression_ratio": 1.380952380952381, "no_speech_prob": 2.50700340984622e-05}, {"id": 563, "seek": 496500, "start": 4970.0, "end": 4985.0, "text": " You can do this by essentially linking all of them to the origin with a spring.", "tokens": [50364, 407, 291, 528, 281, 767, 1066, 552, 382, 1998, 281, 1184, 661, 382, 1944, 13, 50564, 50564, 400, 577, 393, 291, 360, 341, 30, 50614, 50614, 509, 393, 360, 341, 538, 4476, 25775, 439, 295, 552, 281, 264, 4957, 365, 257, 5587, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.06304434005250321, "compression_ratio": 1.380952380952381, "no_speech_prob": 2.50700340984622e-05}, {"id": 564, "seek": 498500, "start": 4985.0, "end": 4995.0, "text": " So basically, the spring wants to bring all those points towards the origin as close to each other to the origin as possible.", "tokens": [50364, 407, 1936, 11, 264, 5587, 2738, 281, 1565, 439, 729, 2793, 3030, 264, 4957, 382, 1998, 281, 1184, 661, 281, 264, 4957, 382, 1944, 13, 50864, 50864, 400, 370, 294, 884, 370, 11, 437, 264, 1185, 307, 516, 281, 853, 281, 360, 307, 2844, 729, 34710, 41225, 382, 1998, 281, 264, 4957, 382, 1944, 13, 51214, 51214, 400, 309, 311, 516, 281, 652, 552, 1333, 295, 19959, 11, 728, 5200, 302, 4404, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05611787523542132, "compression_ratio": 1.7701149425287357, "no_speech_prob": 2.7693640731740743e-06}, {"id": 565, "seek": 498500, "start": 4995.0, "end": 5002.0, "text": " And so in doing so, what the system is going to try to do is pack those fuzzy spheres as close to the origin as possible.", "tokens": [50364, 407, 1936, 11, 264, 5587, 2738, 281, 1565, 439, 729, 2793, 3030, 264, 4957, 382, 1998, 281, 1184, 661, 281, 264, 4957, 382, 1944, 13, 50864, 50864, 400, 370, 294, 884, 370, 11, 437, 264, 1185, 307, 516, 281, 853, 281, 360, 307, 2844, 729, 34710, 41225, 382, 1998, 281, 264, 4957, 382, 1944, 13, 51214, 51214, 400, 309, 311, 516, 281, 652, 552, 1333, 295, 19959, 11, 728, 5200, 302, 4404, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05611787523542132, "compression_ratio": 1.7701149425287357, "no_speech_prob": 2.7693640731740743e-06}, {"id": 566, "seek": 498500, "start": 5002.0, "end": 5008.0, "text": " And it's going to make them sort of overlap, interpenetrate.", "tokens": [50364, 407, 1936, 11, 264, 5587, 2738, 281, 1565, 439, 729, 2793, 3030, 264, 4957, 382, 1998, 281, 1184, 661, 281, 264, 4957, 382, 1944, 13, 50864, 50864, 400, 370, 294, 884, 370, 11, 437, 264, 1185, 307, 516, 281, 853, 281, 360, 307, 2844, 729, 34710, 41225, 382, 1998, 281, 264, 4957, 382, 1944, 13, 51214, 51214, 400, 309, 311, 516, 281, 652, 552, 1333, 295, 19959, 11, 728, 5200, 302, 4404, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05611787523542132, "compression_ratio": 1.7701149425287357, "no_speech_prob": 2.7693640731740743e-06}, {"id": 567, "seek": 500800, "start": 5008.0, "end": 5018.0, "text": " But of course, if they interpenetrate too much, if the two spheres for two very different samples interpenetrate too much,", "tokens": [50364, 583, 295, 1164, 11, 498, 436, 728, 5200, 302, 4404, 886, 709, 11, 498, 264, 732, 41225, 337, 732, 588, 819, 10938, 728, 5200, 302, 4404, 886, 709, 11, 50864, 50864, 550, 729, 732, 10938, 366, 516, 281, 312, 9019, 538, 264, 979, 19866, 293, 264, 31565, 2281, 307, 516, 281, 483, 2416, 13, 51214, 51214, 400, 370, 437, 264, 1185, 5314, 493, 884, 307, 787, 8295, 732, 41225, 19959, 498, 264, 732, 10938, 366, 588, 2531, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.030623244076240355, "compression_ratio": 1.8125, "no_speech_prob": 2.090382395181223e-06}, {"id": 568, "seek": 500800, "start": 5018.0, "end": 5025.0, "text": " then those two samples are going to be confused by the decoder and the reconstruction energy is going to get large.", "tokens": [50364, 583, 295, 1164, 11, 498, 436, 728, 5200, 302, 4404, 886, 709, 11, 498, 264, 732, 41225, 337, 732, 588, 819, 10938, 728, 5200, 302, 4404, 886, 709, 11, 50864, 50864, 550, 729, 732, 10938, 366, 516, 281, 312, 9019, 538, 264, 979, 19866, 293, 264, 31565, 2281, 307, 516, 281, 483, 2416, 13, 51214, 51214, 400, 370, 437, 264, 1185, 5314, 493, 884, 307, 787, 8295, 732, 41225, 19959, 498, 264, 732, 10938, 366, 588, 2531, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.030623244076240355, "compression_ratio": 1.8125, "no_speech_prob": 2.090382395181223e-06}, {"id": 569, "seek": 500800, "start": 5025.0, "end": 5034.0, "text": " And so what the system ends up doing is only letting two spheres overlap if the two samples are very similar.", "tokens": [50364, 583, 295, 1164, 11, 498, 436, 728, 5200, 302, 4404, 886, 709, 11, 498, 264, 732, 41225, 337, 732, 588, 819, 10938, 728, 5200, 302, 4404, 886, 709, 11, 50864, 50864, 550, 729, 732, 10938, 366, 516, 281, 312, 9019, 538, 264, 979, 19866, 293, 264, 31565, 2281, 307, 516, 281, 483, 2416, 13, 51214, 51214, 400, 370, 437, 264, 1185, 5314, 493, 884, 307, 787, 8295, 732, 41225, 19959, 498, 264, 732, 10938, 366, 588, 2531, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.030623244076240355, "compression_ratio": 1.8125, "no_speech_prob": 2.090382395181223e-06}, {"id": 570, "seek": 503400, "start": 5034.0, "end": 5041.0, "text": " And so basically, by doing this, the system finds some sort of representation of the manifold.", "tokens": [50364, 400, 370, 1936, 11, 538, 884, 341, 11, 264, 1185, 10704, 512, 1333, 295, 10290, 295, 264, 47138, 13, 50714, 50714, 467, 8137, 729, 3089, 18875, 2051, 257, 47138, 498, 456, 307, 472, 13, 51114, 51114, 400, 300, 311, 264, 3875, 1558, 295, 32511, 1478, 13738, 22660, 19866, 13, 51264, 51264, 823, 11, 291, 393, 28446, 341, 365, 5221, 11, 293, 309, 1177, 380, 652, 1340, 709, 3571, 281, 1223, 13, 51664, 51664, 682, 1186, 11, 309, 311, 709, 544, 12649, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07507619746895723, "compression_ratio": 1.543778801843318, "no_speech_prob": 4.495059329201467e-06}, {"id": 571, "seek": 503400, "start": 5041.0, "end": 5049.0, "text": " It puts those code vectors along a manifold if there is one.", "tokens": [50364, 400, 370, 1936, 11, 538, 884, 341, 11, 264, 1185, 10704, 512, 1333, 295, 10290, 295, 264, 47138, 13, 50714, 50714, 467, 8137, 729, 3089, 18875, 2051, 257, 47138, 498, 456, 307, 472, 13, 51114, 51114, 400, 300, 311, 264, 3875, 1558, 295, 32511, 1478, 13738, 22660, 19866, 13, 51264, 51264, 823, 11, 291, 393, 28446, 341, 365, 5221, 11, 293, 309, 1177, 380, 652, 1340, 709, 3571, 281, 1223, 13, 51664, 51664, 682, 1186, 11, 309, 311, 709, 544, 12649, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07507619746895723, "compression_ratio": 1.543778801843318, "no_speech_prob": 4.495059329201467e-06}, {"id": 572, "seek": 503400, "start": 5049.0, "end": 5052.0, "text": " And that's the basic idea of Variational Autoencoder.", "tokens": [50364, 400, 370, 1936, 11, 538, 884, 341, 11, 264, 1185, 10704, 512, 1333, 295, 10290, 295, 264, 47138, 13, 50714, 50714, 467, 8137, 729, 3089, 18875, 2051, 257, 47138, 498, 456, 307, 472, 13, 51114, 51114, 400, 300, 311, 264, 3875, 1558, 295, 32511, 1478, 13738, 22660, 19866, 13, 51264, 51264, 823, 11, 291, 393, 28446, 341, 365, 5221, 11, 293, 309, 1177, 380, 652, 1340, 709, 3571, 281, 1223, 13, 51664, 51664, 682, 1186, 11, 309, 311, 709, 544, 12649, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07507619746895723, "compression_ratio": 1.543778801843318, "no_speech_prob": 4.495059329201467e-06}, {"id": 573, "seek": 503400, "start": 5052.0, "end": 5060.0, "text": " Now, you can derive this with math, and it doesn't make anything much easier to understand.", "tokens": [50364, 400, 370, 1936, 11, 538, 884, 341, 11, 264, 1185, 10704, 512, 1333, 295, 10290, 295, 264, 47138, 13, 50714, 50714, 467, 8137, 729, 3089, 18875, 2051, 257, 47138, 498, 456, 307, 472, 13, 51114, 51114, 400, 300, 311, 264, 3875, 1558, 295, 32511, 1478, 13738, 22660, 19866, 13, 51264, 51264, 823, 11, 291, 393, 28446, 341, 365, 5221, 11, 293, 309, 1177, 380, 652, 1340, 709, 3571, 281, 1223, 13, 51664, 51664, 682, 1186, 11, 309, 311, 709, 544, 12649, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07507619746895723, "compression_ratio": 1.543778801843318, "no_speech_prob": 4.495059329201467e-06}, {"id": 574, "seek": 503400, "start": 5060.0, "end": 5062.0, "text": " In fact, it's much more abstract.", "tokens": [50364, 400, 370, 1936, 11, 538, 884, 341, 11, 264, 1185, 10704, 512, 1333, 295, 10290, 295, 264, 47138, 13, 50714, 50714, 467, 8137, 729, 3089, 18875, 2051, 257, 47138, 498, 456, 307, 472, 13, 51114, 51114, 400, 300, 311, 264, 3875, 1558, 295, 32511, 1478, 13738, 22660, 19866, 13, 51264, 51264, 823, 11, 291, 393, 28446, 341, 365, 5221, 11, 293, 309, 1177, 380, 652, 1340, 709, 3571, 281, 1223, 13, 51664, 51664, 682, 1186, 11, 309, 311, 709, 544, 12649, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07507619746895723, "compression_ratio": 1.543778801843318, "no_speech_prob": 4.495059329201467e-06}, {"id": 575, "seek": 506200, "start": 5062.0, "end": 5064.0, "text": " But that's basically what it does in the end.", "tokens": [50364, 583, 300, 311, 1936, 437, 309, 775, 294, 264, 917, 13, 50464, 50464, 407, 456, 311, 257, 1916, 544, 11733, 456, 294, 264, 32511, 1478, 13738, 22660, 19866, 1558, 11, 293, 291, 603, 483, 512, 4365, 365, 28327, 78, 4153, 13, 51164, 51164, 509, 393, 6231, 264, 2744, 295, 729, 34710, 9803, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.08381621879443787, "compression_ratio": 1.3670886075949367, "no_speech_prob": 1.8339516827836633e-05}, {"id": 576, "seek": 506200, "start": 5064.0, "end": 5078.0, "text": " So there's a couple more tricks there in the Variational Autoencoder idea, and you'll get some details with Alfredo tomorrow.", "tokens": [50364, 583, 300, 311, 1936, 437, 309, 775, 294, 264, 917, 13, 50464, 50464, 407, 456, 311, 257, 1916, 544, 11733, 456, 294, 264, 32511, 1478, 13738, 22660, 19866, 1558, 11, 293, 291, 603, 483, 512, 4365, 365, 28327, 78, 4153, 13, 51164, 51164, 509, 393, 6231, 264, 2744, 295, 729, 34710, 9803, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.08381621879443787, "compression_ratio": 1.3670886075949367, "no_speech_prob": 1.8339516827836633e-05}, {"id": 577, "seek": 506200, "start": 5078.0, "end": 5084.0, "text": " You can adapt the size of those fuzzy balls.", "tokens": [50364, 583, 300, 311, 1936, 437, 309, 775, 294, 264, 917, 13, 50464, 50464, 407, 456, 311, 257, 1916, 544, 11733, 456, 294, 264, 32511, 1478, 13738, 22660, 19866, 1558, 11, 293, 291, 603, 483, 512, 4365, 365, 28327, 78, 4153, 13, 51164, 51164, 509, 393, 6231, 264, 2744, 295, 729, 34710, 9803, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.08381621879443787, "compression_ratio": 1.3670886075949367, "no_speech_prob": 1.8339516827836633e-05}, {"id": 578, "seek": 508400, "start": 5084.0, "end": 5092.0, "text": " So basically, you can have the encoder compute the optimal size of the balls in each direction.", "tokens": [50364, 407, 1936, 11, 291, 393, 362, 264, 2058, 19866, 14722, 264, 16252, 2744, 295, 264, 9803, 294, 1184, 3513, 13, 50764, 50764, 400, 437, 291, 362, 281, 360, 307, 652, 988, 300, 264, 9803, 500, 380, 483, 886, 1359, 13, 50914, 50914, 400, 370, 291, 829, 257, 16263, 2445, 300, 9898, 281, 652, 264, 21977, 295, 729, 9803, 11, 264, 2744, 294, 1184, 10139, 11, 498, 291, 528, 11, 382, 1998, 281, 472, 382, 1944, 13, 51514, 51514, 814, 393, 483, 257, 857, 4356, 13, 51614, 51614, 814, 393, 483, 4833, 498, 436, 528, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0753883689340919, "compression_ratio": 1.7522935779816513, "no_speech_prob": 9.30203459574841e-05}, {"id": 579, "seek": 508400, "start": 5092.0, "end": 5095.0, "text": " And what you have to do is make sure that the balls don't get too small.", "tokens": [50364, 407, 1936, 11, 291, 393, 362, 264, 2058, 19866, 14722, 264, 16252, 2744, 295, 264, 9803, 294, 1184, 3513, 13, 50764, 50764, 400, 437, 291, 362, 281, 360, 307, 652, 988, 300, 264, 9803, 500, 380, 483, 886, 1359, 13, 50914, 50914, 400, 370, 291, 829, 257, 16263, 2445, 300, 9898, 281, 652, 264, 21977, 295, 729, 9803, 11, 264, 2744, 294, 1184, 10139, 11, 498, 291, 528, 11, 382, 1998, 281, 472, 382, 1944, 13, 51514, 51514, 814, 393, 483, 257, 857, 4356, 13, 51614, 51614, 814, 393, 483, 4833, 498, 436, 528, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0753883689340919, "compression_ratio": 1.7522935779816513, "no_speech_prob": 9.30203459574841e-05}, {"id": 580, "seek": 508400, "start": 5095.0, "end": 5107.0, "text": " And so you put a penalty function that tries to make the variance of those balls, the size in each dimension, if you want, as close to one as possible.", "tokens": [50364, 407, 1936, 11, 291, 393, 362, 264, 2058, 19866, 14722, 264, 16252, 2744, 295, 264, 9803, 294, 1184, 3513, 13, 50764, 50764, 400, 437, 291, 362, 281, 360, 307, 652, 988, 300, 264, 9803, 500, 380, 483, 886, 1359, 13, 50914, 50914, 400, 370, 291, 829, 257, 16263, 2445, 300, 9898, 281, 652, 264, 21977, 295, 729, 9803, 11, 264, 2744, 294, 1184, 10139, 11, 498, 291, 528, 11, 382, 1998, 281, 472, 382, 1944, 13, 51514, 51514, 814, 393, 483, 257, 857, 4356, 13, 51614, 51614, 814, 393, 483, 4833, 498, 436, 528, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0753883689340919, "compression_ratio": 1.7522935779816513, "no_speech_prob": 9.30203459574841e-05}, {"id": 581, "seek": 508400, "start": 5107.0, "end": 5109.0, "text": " They can get a bit smaller.", "tokens": [50364, 407, 1936, 11, 291, 393, 362, 264, 2058, 19866, 14722, 264, 16252, 2744, 295, 264, 9803, 294, 1184, 3513, 13, 50764, 50764, 400, 437, 291, 362, 281, 360, 307, 652, 988, 300, 264, 9803, 500, 380, 483, 886, 1359, 13, 50914, 50914, 400, 370, 291, 829, 257, 16263, 2445, 300, 9898, 281, 652, 264, 21977, 295, 729, 9803, 11, 264, 2744, 294, 1184, 10139, 11, 498, 291, 528, 11, 382, 1998, 281, 472, 382, 1944, 13, 51514, 51514, 814, 393, 483, 257, 857, 4356, 13, 51614, 51614, 814, 393, 483, 4833, 498, 436, 528, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0753883689340919, "compression_ratio": 1.7522935779816513, "no_speech_prob": 9.30203459574841e-05}, {"id": 582, "seek": 508400, "start": 5109.0, "end": 5110.0, "text": " They can get larger if they want.", "tokens": [50364, 407, 1936, 11, 291, 393, 362, 264, 2058, 19866, 14722, 264, 16252, 2744, 295, 264, 9803, 294, 1184, 3513, 13, 50764, 50764, 400, 437, 291, 362, 281, 360, 307, 652, 988, 300, 264, 9803, 500, 380, 483, 886, 1359, 13, 50914, 50914, 400, 370, 291, 829, 257, 16263, 2445, 300, 9898, 281, 652, 264, 21977, 295, 729, 9803, 11, 264, 2744, 294, 1184, 10139, 11, 498, 291, 528, 11, 382, 1998, 281, 472, 382, 1944, 13, 51514, 51514, 814, 393, 483, 257, 857, 4356, 13, 51614, 51614, 814, 393, 483, 4833, 498, 436, 528, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0753883689340919, "compression_ratio": 1.7522935779816513, "no_speech_prob": 9.30203459574841e-05}, {"id": 583, "seek": 511000, "start": 5110.0, "end": 5116.0, "text": " But there's a cost for making them different from one.", "tokens": [50364, 583, 456, 311, 257, 2063, 337, 1455, 552, 819, 490, 472, 13, 50664, 50664, 407, 586, 264, 4282, 11, 264, 1154, 291, 362, 365, 341, 307, 281, 4369, 264, 4972, 7379, 295, 341, 5587, 3800, 13, 51264, 51264, 759, 264, 5587, 307, 886, 2416, 11, 498, 264, 5587, 307, 886, 2068, 11, 550, 264, 34710, 9803, 366, 439, 516, 281, 15584, 294, 264, 3056, 11, 293, 264, 1185, 307, 406, 516, 281, 312, 1075, 281, 31499, 6108, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05032580654795577, "compression_ratio": 1.6492890995260663, "no_speech_prob": 8.216083369916305e-05}, {"id": 584, "seek": 511000, "start": 5116.0, "end": 5128.0, "text": " So now the trick, the problem you have with this is to adjust the relative importance of this spring strength.", "tokens": [50364, 583, 456, 311, 257, 2063, 337, 1455, 552, 819, 490, 472, 13, 50664, 50664, 407, 586, 264, 4282, 11, 264, 1154, 291, 362, 365, 341, 307, 281, 4369, 264, 4972, 7379, 295, 341, 5587, 3800, 13, 51264, 51264, 759, 264, 5587, 307, 886, 2416, 11, 498, 264, 5587, 307, 886, 2068, 11, 550, 264, 34710, 9803, 366, 439, 516, 281, 15584, 294, 264, 3056, 11, 293, 264, 1185, 307, 406, 516, 281, 312, 1075, 281, 31499, 6108, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05032580654795577, "compression_ratio": 1.6492890995260663, "no_speech_prob": 8.216083369916305e-05}, {"id": 585, "seek": 511000, "start": 5128.0, "end": 5139.0, "text": " If the spring is too large, if the spring is too strong, then the fuzzy balls are all going to collapse in the center, and the system is not going to be able to reconstruct properly.", "tokens": [50364, 583, 456, 311, 257, 2063, 337, 1455, 552, 819, 490, 472, 13, 50664, 50664, 407, 586, 264, 4282, 11, 264, 1154, 291, 362, 365, 341, 307, 281, 4369, 264, 4972, 7379, 295, 341, 5587, 3800, 13, 51264, 51264, 759, 264, 5587, 307, 886, 2416, 11, 498, 264, 5587, 307, 886, 2068, 11, 550, 264, 34710, 9803, 366, 439, 516, 281, 15584, 294, 264, 3056, 11, 293, 264, 1185, 307, 406, 516, 281, 312, 1075, 281, 31499, 6108, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05032580654795577, "compression_ratio": 1.6492890995260663, "no_speech_prob": 8.216083369916305e-05}, {"id": 586, "seek": 513900, "start": 5139.0, "end": 5148.0, "text": " If it's too weak, then the fuzzy balls are going to fly away from each other, and the system is going to be able to reconstruct everything and anything.", "tokens": [50364, 759, 309, 311, 886, 5336, 11, 550, 264, 34710, 9803, 366, 516, 281, 3603, 1314, 490, 1184, 661, 11, 293, 264, 1185, 307, 516, 281, 312, 1075, 281, 31499, 1203, 293, 1340, 13, 50814, 50814, 400, 370, 291, 362, 281, 9302, 257, 4772, 1296, 264, 732, 13, 50964, 50964, 400, 300, 311, 733, 295, 264, 10360, 365, 32511, 1478, 13738, 22660, 19866, 13, 51114, 51114, 759, 291, 3488, 264, 3800, 295, 264, 5587, 257, 707, 886, 709, 11, 309, 311, 257, 1433, 1219, 47991, 47387, 294, 264, 1185, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.05886223495647471, "compression_ratio": 1.6127659574468085, "no_speech_prob": 3.586825550883077e-05}, {"id": 587, "seek": 513900, "start": 5148.0, "end": 5151.0, "text": " And so you have to strike a balance between the two.", "tokens": [50364, 759, 309, 311, 886, 5336, 11, 550, 264, 34710, 9803, 366, 516, 281, 3603, 1314, 490, 1184, 661, 11, 293, 264, 1185, 307, 516, 281, 312, 1075, 281, 31499, 1203, 293, 1340, 13, 50814, 50814, 400, 370, 291, 362, 281, 9302, 257, 4772, 1296, 264, 732, 13, 50964, 50964, 400, 300, 311, 733, 295, 264, 10360, 365, 32511, 1478, 13738, 22660, 19866, 13, 51114, 51114, 759, 291, 3488, 264, 3800, 295, 264, 5587, 257, 707, 886, 709, 11, 309, 311, 257, 1433, 1219, 47991, 47387, 294, 264, 1185, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.05886223495647471, "compression_ratio": 1.6127659574468085, "no_speech_prob": 3.586825550883077e-05}, {"id": 588, "seek": 513900, "start": 5151.0, "end": 5154.0, "text": " And that's kind of the difficulty with Variational Autoencoder.", "tokens": [50364, 759, 309, 311, 886, 5336, 11, 550, 264, 34710, 9803, 366, 516, 281, 3603, 1314, 490, 1184, 661, 11, 293, 264, 1185, 307, 516, 281, 312, 1075, 281, 31499, 1203, 293, 1340, 13, 50814, 50814, 400, 370, 291, 362, 281, 9302, 257, 4772, 1296, 264, 732, 13, 50964, 50964, 400, 300, 311, 733, 295, 264, 10360, 365, 32511, 1478, 13738, 22660, 19866, 13, 51114, 51114, 759, 291, 3488, 264, 3800, 295, 264, 5587, 257, 707, 886, 709, 11, 309, 311, 257, 1433, 1219, 47991, 47387, 294, 264, 1185, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.05886223495647471, "compression_ratio": 1.6127659574468085, "no_speech_prob": 3.586825550883077e-05}, {"id": 589, "seek": 513900, "start": 5154.0, "end": 5163.0, "text": " If you increase the strength of the spring a little too much, it's a term called KL divergence in the system.", "tokens": [50364, 759, 309, 311, 886, 5336, 11, 550, 264, 34710, 9803, 366, 516, 281, 3603, 1314, 490, 1184, 661, 11, 293, 264, 1185, 307, 516, 281, 312, 1075, 281, 31499, 1203, 293, 1340, 13, 50814, 50814, 400, 370, 291, 362, 281, 9302, 257, 4772, 1296, 264, 732, 13, 50964, 50964, 400, 300, 311, 733, 295, 264, 10360, 365, 32511, 1478, 13738, 22660, 19866, 13, 51114, 51114, 759, 291, 3488, 264, 3800, 295, 264, 5587, 257, 707, 886, 709, 11, 309, 311, 257, 1433, 1219, 47991, 47387, 294, 264, 1185, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.05886223495647471, "compression_ratio": 1.6127659574468085, "no_speech_prob": 3.586825550883077e-05}, {"id": 590, "seek": 516300, "start": 5163.0, "end": 5173.0, "text": " So KL divergence between the Gaussian, basically, of a Gaussian, it collapses.", "tokens": [50364, 407, 47991, 47387, 1296, 264, 39148, 11, 1936, 11, 295, 257, 39148, 11, 309, 48765, 13, 50864, 50864, 1057, 264, 34710, 9803, 1936, 483, 281, 264, 3056, 11, 293, 264, 1185, 775, 406, 767, 2316, 309, 6108, 13, 51164, 51164, 286, 362, 257, 1168, 466, 472, 295, 264, 3894, 16564, 11, 767, 13, 51414, 51414, 407, 307, 300, 439, 558, 30, 51464, 51464, 4894, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1932083074597345, "compression_ratio": 1.4808743169398908, "no_speech_prob": 4.495069788390538e-06}, {"id": 591, "seek": 516300, "start": 5173.0, "end": 5179.0, "text": " All the fuzzy balls basically get to the center, and the system does not actually model it properly.", "tokens": [50364, 407, 47991, 47387, 1296, 264, 39148, 11, 1936, 11, 295, 257, 39148, 11, 309, 48765, 13, 50864, 50864, 1057, 264, 34710, 9803, 1936, 483, 281, 264, 3056, 11, 293, 264, 1185, 775, 406, 767, 2316, 309, 6108, 13, 51164, 51164, 286, 362, 257, 1168, 466, 472, 295, 264, 3894, 16564, 11, 767, 13, 51414, 51414, 407, 307, 300, 439, 558, 30, 51464, 51464, 4894, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1932083074597345, "compression_ratio": 1.4808743169398908, "no_speech_prob": 4.495069788390538e-06}, {"id": 592, "seek": 516300, "start": 5179.0, "end": 5184.0, "text": " I have a question about one of the previous lectures, actually.", "tokens": [50364, 407, 47991, 47387, 1296, 264, 39148, 11, 1936, 11, 295, 257, 39148, 11, 309, 48765, 13, 50864, 50864, 1057, 264, 34710, 9803, 1936, 483, 281, 264, 3056, 11, 293, 264, 1185, 775, 406, 767, 2316, 309, 6108, 13, 51164, 51164, 286, 362, 257, 1168, 466, 472, 295, 264, 3894, 16564, 11, 767, 13, 51414, 51414, 407, 307, 300, 439, 558, 30, 51464, 51464, 4894, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1932083074597345, "compression_ratio": 1.4808743169398908, "no_speech_prob": 4.495069788390538e-06}, {"id": 593, "seek": 516300, "start": 5184.0, "end": 5185.0, "text": " So is that all right?", "tokens": [50364, 407, 47991, 47387, 1296, 264, 39148, 11, 1936, 11, 295, 257, 39148, 11, 309, 48765, 13, 50864, 50864, 1057, 264, 34710, 9803, 1936, 483, 281, 264, 3056, 11, 293, 264, 1185, 775, 406, 767, 2316, 309, 6108, 13, 51164, 51164, 286, 362, 257, 1168, 466, 472, 295, 264, 3894, 16564, 11, 767, 13, 51414, 51414, 407, 307, 300, 439, 558, 30, 51464, 51464, 4894, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1932083074597345, "compression_ratio": 1.4808743169398908, "no_speech_prob": 4.495069788390538e-06}, {"id": 594, "seek": 516300, "start": 5185.0, "end": 5187.0, "text": " Sure.", "tokens": [50364, 407, 47991, 47387, 1296, 264, 39148, 11, 1936, 11, 295, 257, 39148, 11, 309, 48765, 13, 50864, 50864, 1057, 264, 34710, 9803, 1936, 483, 281, 264, 3056, 11, 293, 264, 1185, 775, 406, 767, 2316, 309, 6108, 13, 51164, 51164, 286, 362, 257, 1168, 466, 472, 295, 264, 3894, 16564, 11, 767, 13, 51414, 51414, 407, 307, 300, 439, 558, 30, 51464, 51464, 4894, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1932083074597345, "compression_ratio": 1.4808743169398908, "no_speech_prob": 4.495069788390538e-06}, {"id": 595, "seek": 518700, "start": 5187.0, "end": 5200.0, "text": " So when you were talking about linearizability, so generally when you were saying that stacking linear layers one after the other without having nonlinearities is basically redundant because we can have one linear layer to do it.", "tokens": [50364, 407, 562, 291, 645, 1417, 466, 8213, 590, 2310, 11, 370, 5101, 562, 291, 645, 1566, 300, 41376, 8213, 7914, 472, 934, 264, 661, 1553, 1419, 2107, 28263, 1088, 307, 1936, 40997, 570, 321, 393, 362, 472, 8213, 4583, 281, 360, 309, 13, 51014, 51014, 583, 286, 1604, 300, 291, 611, 2835, 456, 307, 472, 1729, 1778, 983, 291, 1062, 528, 281, 360, 341, 11, 689, 291, 445, 8630, 8213, 7914, 934, 341, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08576198724599984, "compression_ratio": 1.737327188940092, "no_speech_prob": 3.320766700198874e-05}, {"id": 596, "seek": 518700, "start": 5200.0, "end": 5210.0, "text": " But I remember that you also mentioned there is one particular reason why you might want to do this, where you just stack linear layers after this.", "tokens": [50364, 407, 562, 291, 645, 1417, 466, 8213, 590, 2310, 11, 370, 5101, 562, 291, 645, 1566, 300, 41376, 8213, 7914, 472, 934, 264, 661, 1553, 1419, 2107, 28263, 1088, 307, 1936, 40997, 570, 321, 393, 362, 472, 8213, 4583, 281, 360, 309, 13, 51014, 51014, 583, 286, 1604, 300, 291, 611, 2835, 456, 307, 472, 1729, 1778, 983, 291, 1062, 528, 281, 360, 341, 11, 689, 291, 445, 8630, 8213, 7914, 934, 341, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08576198724599984, "compression_ratio": 1.737327188940092, "no_speech_prob": 3.320766700198874e-05}, {"id": 597, "seek": 521000, "start": 5210.0, "end": 5218.0, "text": " And you said, well, there's one reason, but you didn't go into that reason. So I was wondering if there is anything significant behind that.", "tokens": [50364, 400, 291, 848, 11, 731, 11, 456, 311, 472, 1778, 11, 457, 291, 994, 380, 352, 666, 300, 1778, 13, 407, 286, 390, 6359, 498, 456, 307, 1340, 4776, 2261, 300, 13, 50764, 50764, 407, 264, 2590, 286, 390, 16141, 307, 300, 3811, 291, 362, 512, 955, 18161, 2533, 11, 293, 309, 14725, 257, 4111, 8062, 295, 257, 1629, 2744, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.09305351697481595, "compression_ratio": 1.511111111111111, "no_speech_prob": 1.3842350199411158e-05}, {"id": 598, "seek": 521000, "start": 5218.0, "end": 5230.0, "text": " So the situation I was describing is that imagine you have some big neural net, and it produces a feature vector of a certain size.", "tokens": [50364, 400, 291, 848, 11, 731, 11, 456, 311, 472, 1778, 11, 457, 291, 994, 380, 352, 666, 300, 1778, 13, 407, 286, 390, 6359, 498, 456, 307, 1340, 4776, 2261, 300, 13, 50764, 50764, 407, 264, 2590, 286, 390, 16141, 307, 300, 3811, 291, 362, 512, 955, 18161, 2533, 11, 293, 309, 14725, 257, 4111, 8062, 295, 257, 1629, 2744, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.09305351697481595, "compression_ratio": 1.511111111111111, "no_speech_prob": 1.3842350199411158e-05}, {"id": 599, "seek": 523000, "start": 5230.0, "end": 5240.0, "text": " And your output is extremely large because maybe you have many, many categories. Maybe you're doing phoneme classification for speech recognition system.", "tokens": [50364, 400, 428, 5598, 307, 4664, 2416, 570, 1310, 291, 362, 867, 11, 867, 10479, 13, 2704, 291, 434, 884, 30754, 5729, 21538, 337, 6218, 11150, 1185, 13, 50864, 50864, 407, 264, 1230, 295, 10479, 510, 307, 1266, 11, 1360, 420, 746, 411, 300, 13, 51264, 51264, 2264, 11, 286, 362, 281, 2642, 5692, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.12518823557886585, "compression_ratio": 1.3798882681564246, "no_speech_prob": 9.664157005317975e-06}, {"id": 600, "seek": 523000, "start": 5240.0, "end": 5248.0, "text": " So the number of categories here is 10,000 or something like that.", "tokens": [50364, 400, 428, 5598, 307, 4664, 2416, 570, 1310, 291, 362, 867, 11, 867, 10479, 13, 2704, 291, 434, 884, 30754, 5729, 21538, 337, 6218, 11150, 1185, 13, 50864, 50864, 407, 264, 1230, 295, 10479, 510, 307, 1266, 11, 1360, 420, 746, 411, 300, 13, 51264, 51264, 2264, 11, 286, 362, 281, 2642, 5692, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.12518823557886585, "compression_ratio": 1.3798882681564246, "no_speech_prob": 9.664157005317975e-06}, {"id": 601, "seek": 523000, "start": 5248.0, "end": 5251.0, "text": " OK, I have to draw slowly.", "tokens": [50364, 400, 428, 5598, 307, 4664, 2416, 570, 1310, 291, 362, 867, 11, 867, 10479, 13, 2704, 291, 434, 884, 30754, 5729, 21538, 337, 6218, 11150, 1185, 13, 50864, 50864, 407, 264, 1230, 295, 10479, 510, 307, 1266, 11, 1360, 420, 746, 411, 300, 13, 51264, 51264, 2264, 11, 286, 362, 281, 2642, 5692, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.12518823557886585, "compression_ratio": 1.3798882681564246, "no_speech_prob": 9.664157005317975e-06}, {"id": 602, "seek": 525100, "start": 5251.0, "end": 5266.0, "text": " So if your feature vector here is itself something like 10,000, the matrix to go from here to here will be 100 million, right?", "tokens": [50364, 407, 498, 428, 4111, 8062, 510, 307, 2564, 746, 411, 1266, 11, 1360, 11, 264, 8141, 281, 352, 490, 510, 281, 510, 486, 312, 2319, 2459, 11, 558, 30, 51114, 51114, 400, 300, 311, 1391, 257, 857, 886, 709, 13, 51214, 51214, 407, 437, 561, 360, 307, 436, 584, 321, 434, 516, 281, 5952, 1125, 300, 8141, 666, 264, 1674, 295, 732, 25193, 32284, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08657892199530118, "compression_ratio": 1.4361702127659575, "no_speech_prob": 3.089278834522702e-06}, {"id": 603, "seek": 525100, "start": 5266.0, "end": 5268.0, "text": " And that's probably a bit too much.", "tokens": [50364, 407, 498, 428, 4111, 8062, 510, 307, 2564, 746, 411, 1266, 11, 1360, 11, 264, 8141, 281, 352, 490, 510, 281, 510, 486, 312, 2319, 2459, 11, 558, 30, 51114, 51114, 400, 300, 311, 1391, 257, 857, 886, 709, 13, 51214, 51214, 407, 437, 561, 360, 307, 436, 584, 321, 434, 516, 281, 5952, 1125, 300, 8141, 666, 264, 1674, 295, 732, 25193, 32284, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08657892199530118, "compression_ratio": 1.4361702127659575, "no_speech_prob": 3.089278834522702e-06}, {"id": 604, "seek": 525100, "start": 5268.0, "end": 5280.0, "text": " So what people do is they say we're going to factorize that matrix into the product of two skinny matrices.", "tokens": [50364, 407, 498, 428, 4111, 8062, 510, 307, 2564, 746, 411, 1266, 11, 1360, 11, 264, 8141, 281, 352, 490, 510, 281, 510, 486, 312, 2319, 2459, 11, 558, 30, 51114, 51114, 400, 300, 311, 1391, 257, 857, 886, 709, 13, 51214, 51214, 407, 437, 561, 360, 307, 436, 584, 321, 434, 516, 281, 5952, 1125, 300, 8141, 666, 264, 1674, 295, 732, 25193, 32284, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08657892199530118, "compression_ratio": 1.4361702127659575, "no_speech_prob": 3.089278834522702e-06}, {"id": 605, "seek": 528000, "start": 5280.0, "end": 5285.0, "text": " So the middle dimension here is maybe a thousand.", "tokens": [50364, 407, 264, 2808, 10139, 510, 307, 1310, 257, 4714, 13, 50614, 50614, 1042, 11, 291, 362, 1266, 74, 322, 264, 4846, 11, 1266, 74, 322, 264, 5598, 11, 293, 550, 264, 2808, 472, 307, 502, 74, 11, 558, 30, 50864, 50864, 407, 498, 291, 500, 380, 362, 264, 2808, 472, 11, 550, 264, 1230, 295, 9834, 291, 362, 307, 1266, 281, 264, 1649, 13, 51114, 51114, 759, 291, 360, 362, 264, 2808, 472, 11, 309, 311, 732, 1413, 1266, 281, 264, 1614, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13704028074768768, "compression_ratio": 1.7575757575757576, "no_speech_prob": 2.2472719138022512e-05}, {"id": 606, "seek": 528000, "start": 5285.0, "end": 5290.0, "text": " Well, you have 10k on the input, 10k on the output, and then the middle one is 1k, right?", "tokens": [50364, 407, 264, 2808, 10139, 510, 307, 1310, 257, 4714, 13, 50614, 50614, 1042, 11, 291, 362, 1266, 74, 322, 264, 4846, 11, 1266, 74, 322, 264, 5598, 11, 293, 550, 264, 2808, 472, 307, 502, 74, 11, 558, 30, 50864, 50864, 407, 498, 291, 500, 380, 362, 264, 2808, 472, 11, 550, 264, 1230, 295, 9834, 291, 362, 307, 1266, 281, 264, 1649, 13, 51114, 51114, 759, 291, 360, 362, 264, 2808, 472, 11, 309, 311, 732, 1413, 1266, 281, 264, 1614, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13704028074768768, "compression_ratio": 1.7575757575757576, "no_speech_prob": 2.2472719138022512e-05}, {"id": 607, "seek": 528000, "start": 5290.0, "end": 5295.0, "text": " So if you don't have the middle one, then the number of parameters you have is 10 to the 8.", "tokens": [50364, 407, 264, 2808, 10139, 510, 307, 1310, 257, 4714, 13, 50614, 50614, 1042, 11, 291, 362, 1266, 74, 322, 264, 4846, 11, 1266, 74, 322, 264, 5598, 11, 293, 550, 264, 2808, 472, 307, 502, 74, 11, 558, 30, 50864, 50864, 407, 498, 291, 500, 380, 362, 264, 2808, 472, 11, 550, 264, 1230, 295, 9834, 291, 362, 307, 1266, 281, 264, 1649, 13, 51114, 51114, 759, 291, 360, 362, 264, 2808, 472, 11, 309, 311, 732, 1413, 1266, 281, 264, 1614, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13704028074768768, "compression_ratio": 1.7575757575757576, "no_speech_prob": 2.2472719138022512e-05}, {"id": 608, "seek": 528000, "start": 5295.0, "end": 5303.0, "text": " If you do have the middle one, it's two times 10 to the 7.", "tokens": [50364, 407, 264, 2808, 10139, 510, 307, 1310, 257, 4714, 13, 50614, 50614, 1042, 11, 291, 362, 1266, 74, 322, 264, 4846, 11, 1266, 74, 322, 264, 5598, 11, 293, 550, 264, 2808, 472, 307, 502, 74, 11, 558, 30, 50864, 50864, 407, 498, 291, 500, 380, 362, 264, 2808, 472, 11, 550, 264, 1230, 295, 9834, 291, 362, 307, 1266, 281, 264, 1649, 13, 51114, 51114, 759, 291, 360, 362, 264, 2808, 472, 11, 309, 311, 732, 1413, 1266, 281, 264, 1614, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13704028074768768, "compression_ratio": 1.7575757575757576, "no_speech_prob": 2.2472719138022512e-05}, {"id": 609, "seek": 530300, "start": 5303.0, "end": 5310.0, "text": " So you get a factor of 10. If you make it 100, then it's 10 to the 6. So it becomes more manageable.", "tokens": [50364, 407, 291, 483, 257, 5952, 295, 1266, 13, 759, 291, 652, 309, 2319, 11, 550, 309, 311, 1266, 281, 264, 1386, 13, 407, 309, 3643, 544, 38798, 13, 50714, 50714, 407, 1936, 291, 483, 257, 2295, 6181, 5952, 2144, 13, 50914, 50914, 407, 264, 4787, 32284, 510, 300, 291, 393, 818, 343, 307, 586, 516, 281, 312, 264, 1674, 295, 732, 4356, 32284, 624, 293, 691, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08916823293121767, "compression_ratio": 1.5028901734104045, "no_speech_prob": 1.2602214155776892e-05}, {"id": 610, "seek": 530300, "start": 5310.0, "end": 5314.0, "text": " So basically you get a low rank factorization.", "tokens": [50364, 407, 291, 483, 257, 5952, 295, 1266, 13, 759, 291, 652, 309, 2319, 11, 550, 309, 311, 1266, 281, 264, 1386, 13, 407, 309, 3643, 544, 38798, 13, 50714, 50714, 407, 1936, 291, 483, 257, 2295, 6181, 5952, 2144, 13, 50914, 50914, 407, 264, 4787, 32284, 510, 300, 291, 393, 818, 343, 307, 586, 516, 281, 312, 264, 1674, 295, 732, 4356, 32284, 624, 293, 691, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08916823293121767, "compression_ratio": 1.5028901734104045, "no_speech_prob": 1.2602214155776892e-05}, {"id": 611, "seek": 530300, "start": 5314.0, "end": 5324.0, "text": " So the overall matrices here that you can call W is now going to be the product of two smaller matrices U and V.", "tokens": [50364, 407, 291, 483, 257, 5952, 295, 1266, 13, 759, 291, 652, 309, 2319, 11, 550, 309, 311, 1266, 281, 264, 1386, 13, 407, 309, 3643, 544, 38798, 13, 50714, 50714, 407, 1936, 291, 483, 257, 2295, 6181, 5952, 2144, 13, 50914, 50914, 407, 264, 4787, 32284, 510, 300, 291, 393, 818, 343, 307, 586, 516, 281, 312, 264, 1674, 295, 732, 4356, 32284, 624, 293, 691, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08916823293121767, "compression_ratio": 1.5028901734104045, "no_speech_prob": 1.2602214155776892e-05}, {"id": 612, "seek": 532400, "start": 5324.0, "end": 5336.0, "text": " And because U and V, the middle dimension, if you want, of U and V is smaller, say 100, then the rank of the corresponding matrix W will be smaller.", "tokens": [50364, 400, 570, 624, 293, 691, 11, 264, 2808, 10139, 11, 498, 291, 528, 11, 295, 624, 293, 691, 307, 4356, 11, 584, 2319, 11, 550, 264, 6181, 295, 264, 11760, 8141, 343, 486, 312, 4356, 13, 50964, 50964, 821, 366, 561, 567, 360, 341, 1553, 767, 1608, 5489, 264, 10139, 295, 264, 2808, 4583, 538, 884, 437, 311, 1219, 257, 4464, 2144, 295, 8179, 2026, 11, 597, 307, 10344, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.07333331494717985, "compression_ratio": 1.529126213592233, "no_speech_prob": 1.4282685697253328e-05}, {"id": 613, "seek": 532400, "start": 5336.0, "end": 5346.0, "text": " There are people who do this without actually specifying the dimension of the middle layer by doing what's called a minimization of nuclear norm, which is equivalent.", "tokens": [50364, 400, 570, 624, 293, 691, 11, 264, 2808, 10139, 11, 498, 291, 528, 11, 295, 624, 293, 691, 307, 4356, 11, 584, 2319, 11, 550, 264, 6181, 295, 264, 11760, 8141, 343, 486, 312, 4356, 13, 50964, 50964, 821, 366, 561, 567, 360, 341, 1553, 767, 1608, 5489, 264, 10139, 295, 264, 2808, 4583, 538, 884, 437, 311, 1219, 257, 4464, 2144, 295, 8179, 2026, 11, 597, 307, 10344, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.07333331494717985, "compression_ratio": 1.529126213592233, "no_speech_prob": 1.4282685697253328e-05}, {"id": 614, "seek": 534600, "start": 5346.0, "end": 5361.0, "text": " But I don't want to go into this, but that would be kind of a situation where you might want to actually decompose a matrix into a product of two matrices to kind of save parameters, essentially, or save computation.", "tokens": [50364, 583, 286, 500, 380, 528, 281, 352, 666, 341, 11, 457, 300, 576, 312, 733, 295, 257, 2590, 689, 291, 1062, 528, 281, 767, 22867, 541, 257, 8141, 666, 257, 1674, 295, 732, 32284, 281, 733, 295, 3155, 9834, 11, 4476, 11, 420, 3155, 24903, 13, 51114, 51114, 821, 311, 611, 1071, 1880, 14029, 11, 597, 307, 300, 309, 4523, 484, 300, 1293, 2539, 293, 2674, 2144, 767, 366, 1101, 562, 291, 360, 341, 733, 295, 5952, 2144, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07567944584122624, "compression_ratio": 1.6782608695652175, "no_speech_prob": 1.4736083358002361e-05}, {"id": 615, "seek": 534600, "start": 5361.0, "end": 5372.0, "text": " There's also another interesting phenomenon, which is that it turns out that both learning and generalization actually are better when you do this kind of factorization.", "tokens": [50364, 583, 286, 500, 380, 528, 281, 352, 666, 341, 11, 457, 300, 576, 312, 733, 295, 257, 2590, 689, 291, 1062, 528, 281, 767, 22867, 541, 257, 8141, 666, 257, 1674, 295, 732, 32284, 281, 733, 295, 3155, 9834, 11, 4476, 11, 420, 3155, 24903, 13, 51114, 51114, 821, 311, 611, 1071, 1880, 14029, 11, 597, 307, 300, 309, 4523, 484, 300, 1293, 2539, 293, 2674, 2144, 767, 366, 1101, 562, 291, 360, 341, 733, 295, 5952, 2144, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07567944584122624, "compression_ratio": 1.6782608695652175, "no_speech_prob": 1.4736083358002361e-05}, {"id": 616, "seek": 537200, "start": 5372.0, "end": 5382.0, "text": " So even though now the optimization with respect to this matrix becomes non-convex, it actually converges faster using stochastic gradient.", "tokens": [50364, 407, 754, 1673, 586, 264, 19618, 365, 3104, 281, 341, 8141, 3643, 2107, 12, 1671, 303, 87, 11, 309, 767, 9652, 2880, 4663, 1228, 342, 8997, 2750, 16235, 13, 50864, 50864, 821, 311, 257, 3035, 538, 11, 420, 2638, 295, 3035, 538, 23269, 706, 11, 498, 291, 434, 3102, 294, 341, 11, 23269, 706, 32968, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.15351629257202148, "compression_ratio": 1.4085365853658536, "no_speech_prob": 7.02631296007894e-05}, {"id": 617, "seek": 537200, "start": 5382.0, "end": 5395.0, "text": " There's a paper by, or series of paper by Nadav, if you're interested in this, Nadav Cohen.", "tokens": [50364, 407, 754, 1673, 586, 264, 19618, 365, 3104, 281, 341, 8141, 3643, 2107, 12, 1671, 303, 87, 11, 309, 767, 9652, 2880, 4663, 1228, 342, 8997, 2750, 16235, 13, 50864, 50864, 821, 311, 257, 3035, 538, 11, 420, 2638, 295, 3035, 538, 23269, 706, 11, 498, 291, 434, 3102, 294, 341, 11, 23269, 706, 32968, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.15351629257202148, "compression_ratio": 1.4085365853658536, "no_speech_prob": 7.02631296007894e-05}, {"id": 618, "seek": 539500, "start": 5395.0, "end": 5412.0, "text": " I think it's from 2018. His co-author with Sanjeev Arora from Princeton.", "tokens": [50364, 286, 519, 309, 311, 490, 6096, 13, 2812, 598, 12, 34224, 365, 5271, 73, 1653, 85, 1587, 3252, 490, 36592, 13, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.1733342742919922, "compression_ratio": 0.9473684210526315, "no_speech_prob": 0.00014876830391585827}, {"id": 619, "seek": 541200, "start": 5412.0, "end": 5438.0, "text": " Nadav was a postdoc with Sanjeev Arora, and they had a series of papers on this that explains why even linear networks actually converge faster, and they use it also to basically study the dynamics of learning in sort of non-convex optimization, as well as the generalization properties.", "tokens": [50364, 23269, 706, 390, 257, 2183, 39966, 365, 5271, 73, 1653, 85, 1587, 3252, 11, 293, 436, 632, 257, 2638, 295, 10577, 322, 341, 300, 13948, 983, 754, 8213, 9590, 767, 41881, 4663, 11, 293, 436, 764, 309, 611, 281, 1936, 2979, 264, 15679, 295, 2539, 294, 1333, 295, 2107, 12, 1671, 303, 87, 19618, 11, 382, 731, 382, 264, 2674, 2144, 7221, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08963198448295023, "compression_ratio": 1.4947916666666667, "no_speech_prob": 3.7043508200440556e-05}, {"id": 620, "seek": 543800, "start": 5438.0, "end": 5448.0, "text": " How important is it to have a kind of matching in a variational autoencoder? How important is it to have a matching kind of architecture of an encoder and a decoder?", "tokens": [50364, 1012, 1021, 307, 309, 281, 362, 257, 733, 295, 14324, 294, 257, 3034, 1478, 8399, 22660, 19866, 30, 1012, 1021, 307, 309, 281, 362, 257, 14324, 733, 295, 9482, 295, 364, 2058, 19866, 293, 257, 979, 19866, 30, 50864, 50864, 821, 311, 572, 1778, 337, 264, 732, 6331, 1303, 281, 2995, 13, 51164, 51164, 467, 311, 588, 2049, 264, 1389, 300, 979, 8616, 307, 709, 3571, 813, 43430, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07131300887016401, "compression_ratio": 1.7668711656441718, "no_speech_prob": 8.479539974359795e-05}, {"id": 621, "seek": 543800, "start": 5448.0, "end": 5454.0, "text": " There's no reason for the two architectures to match.", "tokens": [50364, 1012, 1021, 307, 309, 281, 362, 257, 733, 295, 14324, 294, 257, 3034, 1478, 8399, 22660, 19866, 30, 1012, 1021, 307, 309, 281, 362, 257, 14324, 733, 295, 9482, 295, 364, 2058, 19866, 293, 257, 979, 19866, 30, 50864, 50864, 821, 311, 572, 1778, 337, 264, 732, 6331, 1303, 281, 2995, 13, 51164, 51164, 467, 311, 588, 2049, 264, 1389, 300, 979, 8616, 307, 709, 3571, 813, 43430, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07131300887016401, "compression_ratio": 1.7668711656441718, "no_speech_prob": 8.479539974359795e-05}, {"id": 622, "seek": 543800, "start": 5454.0, "end": 5459.0, "text": " It's very often the case that decoding is much easier than encoding.", "tokens": [50364, 1012, 1021, 307, 309, 281, 362, 257, 733, 295, 14324, 294, 257, 3034, 1478, 8399, 22660, 19866, 30, 1012, 1021, 307, 309, 281, 362, 257, 14324, 733, 295, 9482, 295, 364, 2058, 19866, 293, 257, 979, 19866, 30, 50864, 50864, 821, 311, 572, 1778, 337, 264, 732, 6331, 1303, 281, 2995, 13, 51164, 51164, 467, 311, 588, 2049, 264, 1389, 300, 979, 8616, 307, 709, 3571, 813, 43430, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07131300887016401, "compression_ratio": 1.7668711656441718, "no_speech_prob": 8.479539974359795e-05}, {"id": 623, "seek": 545900, "start": 5459.0, "end": 5476.0, "text": " So if you take the example of sparse coding, which I talked about, in sparse coding, where the encoder is one of those sort of Lista type encoder, the encoder is actually quite complicated, whereas the decoder is linear.", "tokens": [50364, 407, 498, 291, 747, 264, 1365, 295, 637, 11668, 17720, 11, 597, 286, 2825, 466, 11, 294, 637, 11668, 17720, 11, 689, 264, 2058, 19866, 307, 472, 295, 729, 1333, 295, 441, 5236, 2010, 2058, 19866, 11, 264, 2058, 19866, 307, 767, 1596, 6179, 11, 9735, 264, 979, 19866, 307, 8213, 13, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.1316731572151184, "compression_ratio": 1.6058394160583942, "no_speech_prob": 0.00023037154460325837}, {"id": 624, "seek": 547600, "start": 5476.0, "end": 5492.0, "text": " This is kind of a special case because the code is high dimensional and sparse. And so, high dimensional sparse codes, you know, any function of a high dimensional sparse code to anywhere basically can be, is quasi-linear.", "tokens": [50364, 639, 307, 733, 295, 257, 2121, 1389, 570, 264, 3089, 307, 1090, 18795, 293, 637, 11668, 13, 400, 370, 11, 1090, 18795, 637, 11668, 14211, 11, 291, 458, 11, 604, 2445, 295, 257, 1090, 18795, 637, 11668, 3089, 281, 4992, 1936, 393, 312, 11, 307, 20954, 12, 28263, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.14918410103276092, "compression_ratio": 1.6204379562043796, "no_speech_prob": 7.138725050026551e-05}, {"id": 625, "seek": 549200, "start": 5492.0, "end": 5508.0, "text": " So one way to make a function linear is to basically represent this input variable in a high dimensional space using a nonlinear transformation. We've talked about this when we discussed what were good features.", "tokens": [50364, 407, 472, 636, 281, 652, 257, 2445, 8213, 307, 281, 1936, 2906, 341, 4846, 7006, 294, 257, 1090, 18795, 1901, 1228, 257, 2107, 28263, 9887, 13, 492, 600, 2825, 466, 341, 562, 321, 7152, 437, 645, 665, 4122, 13, 51164, 51164, 400, 665, 4122, 5101, 4603, 294, 1333, 295, 14702, 264, 10139, 295, 264, 10290, 294, 257, 2107, 28263, 636, 293, 1455, 341, 10290, 637, 11668, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09573688641400405, "compression_ratio": 1.7075471698113207, "no_speech_prob": 2.177798523916863e-05}, {"id": 626, "seek": 549200, "start": 5508.0, "end": 5517.0, "text": " And good features generally consist in sort of expanding the dimension of the representation in a nonlinear way and making this representation sparse.", "tokens": [50364, 407, 472, 636, 281, 652, 257, 2445, 8213, 307, 281, 1936, 2906, 341, 4846, 7006, 294, 257, 1090, 18795, 1901, 1228, 257, 2107, 28263, 9887, 13, 492, 600, 2825, 466, 341, 562, 321, 7152, 437, 645, 665, 4122, 13, 51164, 51164, 400, 665, 4122, 5101, 4603, 294, 1333, 295, 14702, 264, 10139, 295, 264, 10290, 294, 257, 2107, 28263, 636, 293, 1455, 341, 10290, 637, 11668, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09573688641400405, "compression_ratio": 1.7075471698113207, "no_speech_prob": 2.177798523916863e-05}, {"id": 627, "seek": 551700, "start": 5517.0, "end": 5530.0, "text": " And the reason for this is that now you make your function linear. So you could very well have a very complex encoder and a very simple decoder, possibly linear one, as long as your code is high dimensional.", "tokens": [50364, 400, 264, 1778, 337, 341, 307, 300, 586, 291, 652, 428, 2445, 8213, 13, 407, 291, 727, 588, 731, 362, 257, 588, 3997, 2058, 19866, 293, 257, 588, 2199, 979, 19866, 11, 6264, 8213, 472, 11, 382, 938, 382, 428, 3089, 307, 1090, 18795, 13, 51014, 51014, 823, 498, 291, 362, 257, 2295, 18795, 3089, 11, 370, 364, 8399, 22660, 19866, 365, 257, 2808, 4583, 307, 588, 9432, 11, 689, 264, 3089, 4583, 307, 588, 9432, 11, 550, 309, 727, 1813, 588, 6179, 281, 360, 264, 979, 8616, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06293308606711767, "compression_ratio": 1.76, "no_speech_prob": 3.071606988669373e-05}, {"id": 628, "seek": 551700, "start": 5530.0, "end": 5542.0, "text": " Now if you have a low dimensional code, so an autoencoder with a middle layer is very narrow, where the code layer is very narrow, then it could become very complicated to do the decoding.", "tokens": [50364, 400, 264, 1778, 337, 341, 307, 300, 586, 291, 652, 428, 2445, 8213, 13, 407, 291, 727, 588, 731, 362, 257, 588, 3997, 2058, 19866, 293, 257, 588, 2199, 979, 19866, 11, 6264, 8213, 472, 11, 382, 938, 382, 428, 3089, 307, 1090, 18795, 13, 51014, 51014, 823, 498, 291, 362, 257, 2295, 18795, 3089, 11, 370, 364, 8399, 22660, 19866, 365, 257, 2808, 4583, 307, 588, 9432, 11, 689, 264, 3089, 4583, 307, 588, 9432, 11, 550, 309, 727, 1813, 588, 6179, 281, 360, 264, 979, 8616, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06293308606711767, "compression_ratio": 1.76, "no_speech_prob": 3.071606988669373e-05}, {"id": 629, "seek": 554200, "start": 5542.0, "end": 5556.0, "text": " It may become a very highly nonlinear function now to do the decoding. And so now you may need multiple layers. There's no reason again to think that the architecture of the decoder should be similar to the architecture of the encoder.", "tokens": [50364, 467, 815, 1813, 257, 588, 5405, 2107, 28263, 2445, 586, 281, 360, 264, 979, 8616, 13, 400, 370, 586, 291, 815, 643, 3866, 7914, 13, 821, 311, 572, 1778, 797, 281, 519, 300, 264, 9482, 295, 264, 979, 19866, 820, 312, 2531, 281, 264, 9482, 295, 264, 2058, 19866, 13, 51064, 51064], "temperature": 0.0, "avg_logprob": -0.0871607639171459, "compression_ratio": 1.5562913907284768, "no_speech_prob": 2.3550595869892277e-05}, {"id": 630, "seek": 555600, "start": 5556.0, "end": 5576.0, "text": " Now there is, you know, there might be, okay, that said, there might actually be good reason for it. Okay. And in fact there are models that I haven't talked about because they're not kind of proven, but which are called stacked autoencoders, where basically you have this idea.", "tokens": [50364, 823, 456, 307, 11, 291, 458, 11, 456, 1062, 312, 11, 1392, 11, 300, 848, 11, 456, 1062, 767, 312, 665, 1778, 337, 309, 13, 1033, 13, 400, 294, 1186, 456, 366, 5245, 300, 286, 2378, 380, 2825, 466, 570, 436, 434, 406, 733, 295, 12785, 11, 457, 597, 366, 1219, 28867, 8399, 22660, 378, 433, 11, 689, 1936, 291, 362, 341, 1558, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.17719407642588897, "compression_ratio": 1.5444444444444445, "no_speech_prob": 2.282585955981631e-05}, {"id": 631, "seek": 557600, "start": 5576.0, "end": 5598.0, "text": " So, you essentially have an autoencoder where you have a reconstruction error. I'm actually going to erase this and make it look like the autoencoder that we talked about, where there is sort of a cost for making the latent variable here different from the output of the encoder.", "tokens": [50364, 407, 11, 291, 4476, 362, 364, 8399, 22660, 19866, 689, 291, 362, 257, 31565, 6713, 13, 286, 478, 767, 516, 281, 23525, 341, 293, 652, 309, 574, 411, 264, 8399, 22660, 19866, 300, 321, 2825, 466, 11, 689, 456, 307, 1333, 295, 257, 2063, 337, 1455, 264, 48994, 7006, 510, 819, 490, 264, 5598, 295, 264, 2058, 19866, 13, 51464, 51464, 407, 341, 307, 257, 1176, 2159, 293, 341, 307, 257, 1176, 498, 291, 528, 13, 1033, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11483579728661514, "compression_ratio": 1.6403940886699508, "no_speech_prob": 3.3209784305654466e-05}, {"id": 632, "seek": 557600, "start": 5598.0, "end": 5603.0, "text": " So this is a Z bar and this is a Z if you want. Okay.", "tokens": [50364, 407, 11, 291, 4476, 362, 364, 8399, 22660, 19866, 689, 291, 362, 257, 31565, 6713, 13, 286, 478, 767, 516, 281, 23525, 341, 293, 652, 309, 574, 411, 264, 8399, 22660, 19866, 300, 321, 2825, 466, 11, 689, 456, 307, 1333, 295, 257, 2063, 337, 1455, 264, 48994, 7006, 510, 819, 490, 264, 5598, 295, 264, 2058, 19866, 13, 51464, 51464, 407, 341, 307, 257, 1176, 2159, 293, 341, 307, 257, 1176, 498, 291, 528, 13, 1033, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11483579728661514, "compression_ratio": 1.6403940886699508, "no_speech_prob": 3.3209784305654466e-05}, {"id": 633, "seek": 560300, "start": 5603.0, "end": 5613.0, "text": " Now, that's an autoencoder drawn in a funny way. So this is Y and this is Y bar at the bottom.", "tokens": [50364, 823, 11, 300, 311, 364, 8399, 22660, 19866, 10117, 294, 257, 4074, 636, 13, 407, 341, 307, 398, 293, 341, 307, 398, 2159, 412, 264, 2767, 13, 50864, 50864, 823, 286, 393, 8630, 1071, 472, 295, 729, 1074, 322, 1192, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10284672843085395, "compression_ratio": 1.2743362831858407, "no_speech_prob": 6.813562504248694e-05}, {"id": 634, "seek": 560300, "start": 5613.0, "end": 5625.0, "text": " Now I can stack another one of those guys on top.", "tokens": [50364, 823, 11, 300, 311, 364, 8399, 22660, 19866, 10117, 294, 257, 4074, 636, 13, 407, 341, 307, 398, 293, 341, 307, 398, 2159, 412, 264, 2767, 13, 50864, 50864, 823, 286, 393, 8630, 1071, 472, 295, 729, 1074, 322, 1192, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10284672843085395, "compression_ratio": 1.2743362831858407, "no_speech_prob": 6.813562504248694e-05}, {"id": 635, "seek": 562500, "start": 5625.0, "end": 5649.0, "text": " Okay, now I'm going to have to call this Z1 and I'm going to call this Z2, etc.", "tokens": [50364, 1033, 11, 586, 286, 478, 516, 281, 362, 281, 818, 341, 1176, 16, 293, 286, 478, 516, 281, 818, 341, 1176, 17, 11, 5183, 13, 51564, 51564, 1033, 11, 341, 286, 478, 516, 281, 818, 398, 2159, 293, 341, 286, 478, 516, 281, 818, 398, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13526544570922852, "compression_ratio": 1.7560975609756098, "no_speech_prob": 4.46785161329899e-05}, {"id": 636, "seek": 562500, "start": 5649.0, "end": 5653.0, "text": " Okay, this I'm going to call Y bar and this I'm going to call Y.", "tokens": [50364, 1033, 11, 586, 286, 478, 516, 281, 362, 281, 818, 341, 1176, 16, 293, 286, 478, 516, 281, 818, 341, 1176, 17, 11, 5183, 13, 51564, 51564, 1033, 11, 341, 286, 478, 516, 281, 818, 398, 2159, 293, 341, 286, 478, 516, 281, 818, 398, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13526544570922852, "compression_ratio": 1.7560975609756098, "no_speech_prob": 4.46785161329899e-05}, {"id": 637, "seek": 565300, "start": 5653.0, "end": 5664.0, "text": " Now, what's interesting about, okay, I'm going to call the bottom one X now, change name.", "tokens": [50364, 823, 11, 437, 311, 1880, 466, 11, 1392, 11, 286, 478, 516, 281, 818, 264, 2767, 472, 1783, 586, 11, 1319, 1315, 13, 50914, 50914, 823, 11, 498, 291, 574, 412, 264, 11, 291, 11200, 264, 558, 644, 295, 264, 1185, 11, 574, 412, 264, 1411, 644, 11, 291, 352, 281, 398, 11, 293, 300, 1542, 588, 709, 411, 257, 13735, 3068, 6545, 689, 1783, 307, 264, 4846, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13917973923356566, "compression_ratio": 1.502824858757062, "no_speech_prob": 7.721339352428913e-05}, {"id": 638, "seek": 565300, "start": 5664.0, "end": 5678.0, "text": " Now, if you look at the, you ignore the right part of the system, look at the left part, you go to Y, and that looks very much like a classical recognizer where X is the input,", "tokens": [50364, 823, 11, 437, 311, 1880, 466, 11, 1392, 11, 286, 478, 516, 281, 818, 264, 2767, 472, 1783, 586, 11, 1319, 1315, 13, 50914, 50914, 823, 11, 498, 291, 574, 412, 264, 11, 291, 11200, 264, 558, 644, 295, 264, 1185, 11, 574, 412, 264, 1411, 644, 11, 291, 352, 281, 398, 11, 293, 300, 1542, 588, 709, 411, 257, 13735, 3068, 6545, 689, 1783, 307, 264, 4846, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13917973923356566, "compression_ratio": 1.502824858757062, "no_speech_prob": 7.721339352428913e-05}, {"id": 639, "seek": 567800, "start": 5678.0, "end": 5686.0, "text": " Y bar is a prediction for the output and Y is the desired output and there is a cost function that measures the difference between the two.", "tokens": [50364, 398, 2159, 307, 257, 17630, 337, 264, 5598, 293, 398, 307, 264, 14721, 5598, 293, 456, 307, 257, 2063, 2445, 300, 8000, 264, 2649, 1296, 264, 732, 13, 50764, 50764, 1033, 13, 50864, 50864, 440, 661, 9819, 300, 1709, 490, 398, 281, 1783, 11, 300, 311, 733, 295, 411, 257, 979, 19866, 11, 689, 398, 307, 264, 3089, 13, 51164, 51164, 583, 550, 291, 362, 14211, 439, 264, 636, 294, 264, 2808, 570, 309, 311, 733, 295, 257, 28867, 8399, 22660, 19866, 11, 558, 13, 51364, 51364, 407, 633, 6119, 295, 4583, 11, 633, 6119, 295, 2058, 19866, 979, 19866, 307, 733, 295, 257, 707, 8399, 22660, 19866, 293, 291, 733, 295, 8630, 552, 322, 1192, 295, 1184, 661, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0966980149669032, "compression_ratio": 1.8549019607843138, "no_speech_prob": 3.5353881685296074e-05}, {"id": 640, "seek": 567800, "start": 5686.0, "end": 5688.0, "text": " Okay.", "tokens": [50364, 398, 2159, 307, 257, 17630, 337, 264, 5598, 293, 398, 307, 264, 14721, 5598, 293, 456, 307, 257, 2063, 2445, 300, 8000, 264, 2649, 1296, 264, 732, 13, 50764, 50764, 1033, 13, 50864, 50864, 440, 661, 9819, 300, 1709, 490, 398, 281, 1783, 11, 300, 311, 733, 295, 411, 257, 979, 19866, 11, 689, 398, 307, 264, 3089, 13, 51164, 51164, 583, 550, 291, 362, 14211, 439, 264, 636, 294, 264, 2808, 570, 309, 311, 733, 295, 257, 28867, 8399, 22660, 19866, 11, 558, 13, 51364, 51364, 407, 633, 6119, 295, 4583, 11, 633, 6119, 295, 2058, 19866, 979, 19866, 307, 733, 295, 257, 707, 8399, 22660, 19866, 293, 291, 733, 295, 8630, 552, 322, 1192, 295, 1184, 661, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0966980149669032, "compression_ratio": 1.8549019607843138, "no_speech_prob": 3.5353881685296074e-05}, {"id": 641, "seek": 567800, "start": 5688.0, "end": 5694.0, "text": " The other branch that goes from Y to X, that's kind of like a decoder, where Y is the code.", "tokens": [50364, 398, 2159, 307, 257, 17630, 337, 264, 5598, 293, 398, 307, 264, 14721, 5598, 293, 456, 307, 257, 2063, 2445, 300, 8000, 264, 2649, 1296, 264, 732, 13, 50764, 50764, 1033, 13, 50864, 50864, 440, 661, 9819, 300, 1709, 490, 398, 281, 1783, 11, 300, 311, 733, 295, 411, 257, 979, 19866, 11, 689, 398, 307, 264, 3089, 13, 51164, 51164, 583, 550, 291, 362, 14211, 439, 264, 636, 294, 264, 2808, 570, 309, 311, 733, 295, 257, 28867, 8399, 22660, 19866, 11, 558, 13, 51364, 51364, 407, 633, 6119, 295, 4583, 11, 633, 6119, 295, 2058, 19866, 979, 19866, 307, 733, 295, 257, 707, 8399, 22660, 19866, 293, 291, 733, 295, 8630, 552, 322, 1192, 295, 1184, 661, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0966980149669032, "compression_ratio": 1.8549019607843138, "no_speech_prob": 3.5353881685296074e-05}, {"id": 642, "seek": 567800, "start": 5694.0, "end": 5698.0, "text": " But then you have codes all the way in the middle because it's kind of a stacked autoencoder, right.", "tokens": [50364, 398, 2159, 307, 257, 17630, 337, 264, 5598, 293, 398, 307, 264, 14721, 5598, 293, 456, 307, 257, 2063, 2445, 300, 8000, 264, 2649, 1296, 264, 732, 13, 50764, 50764, 1033, 13, 50864, 50864, 440, 661, 9819, 300, 1709, 490, 398, 281, 1783, 11, 300, 311, 733, 295, 411, 257, 979, 19866, 11, 689, 398, 307, 264, 3089, 13, 51164, 51164, 583, 550, 291, 362, 14211, 439, 264, 636, 294, 264, 2808, 570, 309, 311, 733, 295, 257, 28867, 8399, 22660, 19866, 11, 558, 13, 51364, 51364, 407, 633, 6119, 295, 4583, 11, 633, 6119, 295, 2058, 19866, 979, 19866, 307, 733, 295, 257, 707, 8399, 22660, 19866, 293, 291, 733, 295, 8630, 552, 322, 1192, 295, 1184, 661, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0966980149669032, "compression_ratio": 1.8549019607843138, "no_speech_prob": 3.5353881685296074e-05}, {"id": 643, "seek": 567800, "start": 5698.0, "end": 5705.0, "text": " So every pair of layer, every pair of encoder decoder is kind of a little autoencoder and you kind of stack them on top of each other.", "tokens": [50364, 398, 2159, 307, 257, 17630, 337, 264, 5598, 293, 398, 307, 264, 14721, 5598, 293, 456, 307, 257, 2063, 2445, 300, 8000, 264, 2649, 1296, 264, 732, 13, 50764, 50764, 1033, 13, 50864, 50864, 440, 661, 9819, 300, 1709, 490, 398, 281, 1783, 11, 300, 311, 733, 295, 411, 257, 979, 19866, 11, 689, 398, 307, 264, 3089, 13, 51164, 51164, 583, 550, 291, 362, 14211, 439, 264, 636, 294, 264, 2808, 570, 309, 311, 733, 295, 257, 28867, 8399, 22660, 19866, 11, 558, 13, 51364, 51364, 407, 633, 6119, 295, 4583, 11, 633, 6119, 295, 2058, 19866, 979, 19866, 307, 733, 295, 257, 707, 8399, 22660, 19866, 293, 291, 733, 295, 8630, 552, 322, 1192, 295, 1184, 661, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0966980149669032, "compression_ratio": 1.8549019607843138, "no_speech_prob": 3.5353881685296074e-05}, {"id": 644, "seek": 570500, "start": 5705.0, "end": 5718.0, "text": " And what you'd like is find a way of training the system in such a way that if you don't have a label for a sample, so you don't know Y for a sample, you just train this as an autoencoder.", "tokens": [50364, 400, 437, 291, 1116, 411, 307, 915, 257, 636, 295, 3097, 264, 1185, 294, 1270, 257, 636, 300, 498, 291, 500, 380, 362, 257, 7645, 337, 257, 6889, 11, 370, 291, 500, 380, 458, 398, 337, 257, 6889, 11, 291, 445, 3847, 341, 382, 364, 8399, 22660, 19866, 13, 51014, 51014, 583, 498, 291, 360, 362, 257, 398, 11, 550, 291, 17690, 398, 281, 1080, 14721, 2158, 293, 550, 341, 1185, 3643, 586, 257, 6562, 295, 257, 6069, 284, 11, 257, 3068, 6545, 11, 293, 364, 8399, 22660, 19866, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07887468946740982, "compression_ratio": 1.6844660194174756, "no_speech_prob": 3.3202930353581905e-05}, {"id": 645, "seek": 570500, "start": 5718.0, "end": 5732.0, "text": " But if you do have a Y, then you clamp Y to its desired value and then this system becomes now a combination of a predictor, a recognizer, and an autoencoder.", "tokens": [50364, 400, 437, 291, 1116, 411, 307, 915, 257, 636, 295, 3097, 264, 1185, 294, 1270, 257, 636, 300, 498, 291, 500, 380, 362, 257, 7645, 337, 257, 6889, 11, 370, 291, 500, 380, 458, 398, 337, 257, 6889, 11, 291, 445, 3847, 341, 382, 364, 8399, 22660, 19866, 13, 51014, 51014, 583, 498, 291, 360, 362, 257, 398, 11, 550, 291, 17690, 398, 281, 1080, 14721, 2158, 293, 550, 341, 1185, 3643, 586, 257, 6562, 295, 257, 6069, 284, 11, 257, 3068, 6545, 11, 293, 364, 8399, 22660, 19866, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07887468946740982, "compression_ratio": 1.6844660194174756, "no_speech_prob": 3.3202930353581905e-05}, {"id": 646, "seek": 573200, "start": 5732.0, "end": 5736.0, "text": " Now there is a slight problem with this picture, a number of different problems.", "tokens": [50364, 823, 456, 307, 257, 4036, 1154, 365, 341, 3036, 11, 257, 1230, 295, 819, 2740, 13, 50564, 50564, 440, 700, 1154, 307, 300, 498, 11, 797, 11, 498, 1176, 16, 11, 337, 1365, 11, 575, 1547, 6042, 293, 291, 787, 3847, 322, 32118, 18657, 292, 10938, 11, 51114, 51114, 264, 1185, 307, 787, 516, 281, 3985, 264, 1589, 807, 1176, 16, 293, 307, 516, 281, 445, 2584, 11200, 264, 1192, 7914, 570, 309, 311, 406, 1547, 6042, 294, 1176, 16, 281, 360, 264, 2176, 31565, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08486412896050347, "compression_ratio": 1.6425531914893616, "no_speech_prob": 8.218709990615025e-05}, {"id": 647, "seek": 573200, "start": 5736.0, "end": 5747.0, "text": " The first problem is that if, again, if Z1, for example, has enough capacity and you only train on unlabeled samples,", "tokens": [50364, 823, 456, 307, 257, 4036, 1154, 365, 341, 3036, 11, 257, 1230, 295, 819, 2740, 13, 50564, 50564, 440, 700, 1154, 307, 300, 498, 11, 797, 11, 498, 1176, 16, 11, 337, 1365, 11, 575, 1547, 6042, 293, 291, 787, 3847, 322, 32118, 18657, 292, 10938, 11, 51114, 51114, 264, 1185, 307, 787, 516, 281, 3985, 264, 1589, 807, 1176, 16, 293, 307, 516, 281, 445, 2584, 11200, 264, 1192, 7914, 570, 309, 311, 406, 1547, 6042, 294, 1176, 16, 281, 360, 264, 2176, 31565, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08486412896050347, "compression_ratio": 1.6425531914893616, "no_speech_prob": 8.218709990615025e-05}, {"id": 648, "seek": 573200, "start": 5747.0, "end": 5757.0, "text": " the system is only going to carry the information through Z1 and is going to just completely ignore the top layers because it's not enough capacity in Z1 to do the perfect reconstruction.", "tokens": [50364, 823, 456, 307, 257, 4036, 1154, 365, 341, 3036, 11, 257, 1230, 295, 819, 2740, 13, 50564, 50564, 440, 700, 1154, 307, 300, 498, 11, 797, 11, 498, 1176, 16, 11, 337, 1365, 11, 575, 1547, 6042, 293, 291, 787, 3847, 322, 32118, 18657, 292, 10938, 11, 51114, 51114, 264, 1185, 307, 787, 516, 281, 3985, 264, 1589, 807, 1176, 16, 293, 307, 516, 281, 445, 2584, 11200, 264, 1192, 7914, 570, 309, 311, 406, 1547, 6042, 294, 1176, 16, 281, 360, 264, 2176, 31565, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08486412896050347, "compression_ratio": 1.6425531914893616, "no_speech_prob": 8.218709990615025e-05}, {"id": 649, "seek": 575700, "start": 5757.0, "end": 5768.0, "text": " So it's going to just put all the information through Z1 and then all the other Z2 and Y will be constant because the system won't need them.", "tokens": [50364, 407, 309, 311, 516, 281, 445, 829, 439, 264, 1589, 807, 1176, 16, 293, 550, 439, 264, 661, 1176, 17, 293, 398, 486, 312, 5754, 570, 264, 1185, 1582, 380, 643, 552, 13, 50914, 50914, 407, 797, 11, 291, 486, 643, 281, 3890, 1125, 1176, 281, 4871, 309, 490, 23384, 439, 264, 1589, 13, 51414, 51414, 400, 912, 337, 264, 661, 7914, 11, 4317, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07624116151229195, "compression_ratio": 1.5310734463276836, "no_speech_prob": 2.2120546418591402e-05}, {"id": 650, "seek": 575700, "start": 5768.0, "end": 5778.0, "text": " So again, you will need to regularize Z to prevent it from capturing all the information.", "tokens": [50364, 407, 309, 311, 516, 281, 445, 829, 439, 264, 1589, 807, 1176, 16, 293, 550, 439, 264, 661, 1176, 17, 293, 398, 486, 312, 5754, 570, 264, 1185, 1582, 380, 643, 552, 13, 50914, 50914, 407, 797, 11, 291, 486, 643, 281, 3890, 1125, 1176, 281, 4871, 309, 490, 23384, 439, 264, 1589, 13, 51414, 51414, 400, 912, 337, 264, 661, 7914, 11, 4317, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07624116151229195, "compression_ratio": 1.5310734463276836, "no_speech_prob": 2.2120546418591402e-05}, {"id": 651, "seek": 575700, "start": 5778.0, "end": 5783.0, "text": " And same for the other layers, perhaps.", "tokens": [50364, 407, 309, 311, 516, 281, 445, 829, 439, 264, 1589, 807, 1176, 16, 293, 550, 439, 264, 661, 1176, 17, 293, 398, 486, 312, 5754, 570, 264, 1185, 1582, 380, 643, 552, 13, 50914, 50914, 407, 797, 11, 291, 486, 643, 281, 3890, 1125, 1176, 281, 4871, 309, 490, 23384, 439, 264, 1589, 13, 51414, 51414, 400, 912, 337, 264, 661, 7914, 11, 4317, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07624116151229195, "compression_ratio": 1.5310734463276836, "no_speech_prob": 2.2120546418591402e-05}, {"id": 652, "seek": 578300, "start": 5783.0, "end": 5791.0, "text": " Now the other thing is, would, you know, do this thing need to be linear or nonlinear?", "tokens": [50364, 823, 264, 661, 551, 307, 11, 576, 11, 291, 458, 11, 360, 341, 551, 643, 281, 312, 8213, 420, 2107, 28263, 30, 50764, 50764, 400, 300, 5946, 322, 264, 4972, 2744, 295, 264, 3683, 1176, 82, 13, 50914, 50914, 407, 498, 291, 352, 490, 2295, 10139, 281, 1090, 10139, 11, 291, 643, 746, 300, 311, 2107, 28263, 13, 51264, 51264, 583, 498, 291, 352, 490, 1090, 10139, 281, 2295, 10139, 11, 291, 393, 1391, 360, 309, 365, 257, 8213, 11, 733, 295, 411, 4008, 17720, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11595902972751193, "compression_ratio": 1.6926829268292682, "no_speech_prob": 3.533957351464778e-05}, {"id": 653, "seek": 578300, "start": 5791.0, "end": 5794.0, "text": " And that depends on the relative size of the various Zs.", "tokens": [50364, 823, 264, 661, 551, 307, 11, 576, 11, 291, 458, 11, 360, 341, 551, 643, 281, 312, 8213, 420, 2107, 28263, 30, 50764, 50764, 400, 300, 5946, 322, 264, 4972, 2744, 295, 264, 3683, 1176, 82, 13, 50914, 50914, 407, 498, 291, 352, 490, 2295, 10139, 281, 1090, 10139, 11, 291, 643, 746, 300, 311, 2107, 28263, 13, 51264, 51264, 583, 498, 291, 352, 490, 1090, 10139, 281, 2295, 10139, 11, 291, 393, 1391, 360, 309, 365, 257, 8213, 11, 733, 295, 411, 4008, 17720, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11595902972751193, "compression_ratio": 1.6926829268292682, "no_speech_prob": 3.533957351464778e-05}, {"id": 654, "seek": 578300, "start": 5794.0, "end": 5801.0, "text": " So if you go from low dimension to high dimension, you need something that's nonlinear.", "tokens": [50364, 823, 264, 661, 551, 307, 11, 576, 11, 291, 458, 11, 360, 341, 551, 643, 281, 312, 8213, 420, 2107, 28263, 30, 50764, 50764, 400, 300, 5946, 322, 264, 4972, 2744, 295, 264, 3683, 1176, 82, 13, 50914, 50914, 407, 498, 291, 352, 490, 2295, 10139, 281, 1090, 10139, 11, 291, 643, 746, 300, 311, 2107, 28263, 13, 51264, 51264, 583, 498, 291, 352, 490, 1090, 10139, 281, 2295, 10139, 11, 291, 393, 1391, 360, 309, 365, 257, 8213, 11, 733, 295, 411, 4008, 17720, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11595902972751193, "compression_ratio": 1.6926829268292682, "no_speech_prob": 3.533957351464778e-05}, {"id": 655, "seek": 578300, "start": 5801.0, "end": 5811.0, "text": " But if you go from high dimension to low dimension, you can probably do it with a linear, kind of like spot coding.", "tokens": [50364, 823, 264, 661, 551, 307, 11, 576, 11, 291, 458, 11, 360, 341, 551, 643, 281, 312, 8213, 420, 2107, 28263, 30, 50764, 50764, 400, 300, 5946, 322, 264, 4972, 2744, 295, 264, 3683, 1176, 82, 13, 50914, 50914, 407, 498, 291, 352, 490, 2295, 10139, 281, 1090, 10139, 11, 291, 643, 746, 300, 311, 2107, 28263, 13, 51264, 51264, 583, 498, 291, 352, 490, 1090, 10139, 281, 2295, 10139, 11, 291, 393, 1391, 360, 309, 365, 257, 8213, 11, 733, 295, 411, 4008, 17720, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11595902972751193, "compression_ratio": 1.6926829268292682, "no_speech_prob": 3.533957351464778e-05}, {"id": 656, "seek": 581100, "start": 5811.0, "end": 5826.0, "text": " And so you will see that the system may have, you know, an alternation of linear and nonlinear stages in sort of opposite phase if you want", "tokens": [50364, 400, 370, 291, 486, 536, 300, 264, 1185, 815, 362, 11, 291, 458, 11, 364, 5400, 399, 295, 8213, 293, 2107, 28263, 10232, 294, 1333, 295, 6182, 5574, 498, 291, 528, 51114, 51114, 570, 291, 643, 8213, 281, 352, 490, 1090, 10139, 281, 2295, 10139, 293, 550, 2107, 28263, 281, 352, 490, 2295, 10139, 281, 1090, 10139, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04865221899063861, "compression_ratio": 1.763157894736842, "no_speech_prob": 1.0129111615242437e-05}, {"id": 657, "seek": 581100, "start": 5826.0, "end": 5835.0, "text": " because you need linear to go from high dimension to low dimension and then nonlinear to go from low dimension to high dimension", "tokens": [50364, 400, 370, 291, 486, 536, 300, 264, 1185, 815, 362, 11, 291, 458, 11, 364, 5400, 399, 295, 8213, 293, 2107, 28263, 10232, 294, 1333, 295, 6182, 5574, 498, 291, 528, 51114, 51114, 570, 291, 643, 8213, 281, 352, 490, 1090, 10139, 281, 2295, 10139, 293, 550, 2107, 28263, 281, 352, 490, 2295, 10139, 281, 1090, 10139, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04865221899063861, "compression_ratio": 1.763157894736842, "no_speech_prob": 1.0129111615242437e-05}, {"id": 658, "seek": 583500, "start": 5835.0, "end": 5843.0, "text": " and then again linear to go back to low dimension and it's the opposite the other way around.", "tokens": [50364, 293, 550, 797, 8213, 281, 352, 646, 281, 2295, 10139, 293, 309, 311, 264, 6182, 264, 661, 636, 926, 13, 50764, 50764, 639, 307, 11, 291, 458, 11, 561, 362, 668, 29939, 721, 411, 341, 11, 457, 406, 534, 1333, 295, 1382, 552, 322, 257, 2416, 4373, 13, 51064, 51064, 407, 456, 311, 257, 688, 295, 1333, 295, 1269, 1651, 926, 729, 721, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.08169390874750473, "compression_ratio": 1.5307262569832403, "no_speech_prob": 5.303853686200455e-05}, {"id": 659, "seek": 583500, "start": 5843.0, "end": 5849.0, "text": " This is, you know, people have been proposing things like this, but not really sort of trying them on a large scale.", "tokens": [50364, 293, 550, 797, 8213, 281, 352, 646, 281, 2295, 10139, 293, 309, 311, 264, 6182, 264, 661, 636, 926, 13, 50764, 50764, 639, 307, 11, 291, 458, 11, 561, 362, 668, 29939, 721, 411, 341, 11, 457, 406, 534, 1333, 295, 1382, 552, 322, 257, 2416, 4373, 13, 51064, 51064, 407, 456, 311, 257, 688, 295, 1333, 295, 1269, 1651, 926, 729, 721, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.08169390874750473, "compression_ratio": 1.5307262569832403, "no_speech_prob": 5.303853686200455e-05}, {"id": 660, "seek": 583500, "start": 5849.0, "end": 5853.0, "text": " So there's a lot of sort of open questions around those things.", "tokens": [50364, 293, 550, 797, 8213, 281, 352, 646, 281, 2295, 10139, 293, 309, 311, 264, 6182, 264, 661, 636, 926, 13, 50764, 50764, 639, 307, 11, 291, 458, 11, 561, 362, 668, 29939, 721, 411, 341, 11, 457, 406, 534, 1333, 295, 1382, 552, 322, 257, 2416, 4373, 13, 51064, 51064, 407, 456, 311, 257, 688, 295, 1333, 295, 1269, 1651, 926, 729, 721, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.08169390874750473, "compression_ratio": 1.5307262569832403, "no_speech_prob": 5.303853686200455e-05}, {"id": 661, "seek": 585300, "start": 5853.0, "end": 5875.0, "text": " If you're curious, one paper that I've worked on with one of my former students called Jake Jow, is a system called stacked what where autoencoder.", "tokens": [50364, 759, 291, 434, 6369, 11, 472, 3035, 300, 286, 600, 2732, 322, 365, 472, 295, 452, 5819, 1731, 1219, 15822, 508, 305, 11, 307, 257, 1185, 1219, 28867, 437, 689, 8399, 22660, 19866, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.2536269740054482, "compression_ratio": 1.2782608695652173, "no_speech_prob": 6.807713361922652e-05}, {"id": 662, "seek": 587500, "start": 5875.0, "end": 5885.0, "text": " And it's a system a bit of this type, but there is sort of extra variables kind of going this way, which are basically the position of switches in pooling.", "tokens": [50364, 400, 309, 311, 257, 1185, 257, 857, 295, 341, 2010, 11, 457, 456, 307, 1333, 295, 2857, 9102, 733, 295, 516, 341, 636, 11, 597, 366, 1936, 264, 2535, 295, 19458, 294, 7005, 278, 13, 50864, 50864, 286, 500, 380, 528, 281, 352, 666, 4365, 11, 457, 498, 291, 574, 337, 257, 3035, 466, 28867, 437, 689, 8399, 22660, 378, 433, 11, 291, 486, 915, 732, 3035, 11, 472, 538, 15822, 293, 2059, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.09128919205108246, "compression_ratio": 1.5226130653266332, "no_speech_prob": 0.00011229536175960675}, {"id": 663, "seek": 587500, "start": 5885.0, "end": 5893.0, "text": " I don't want to go into details, but if you look for a paper about stacked what where autoencoders, you will find two paper, one by Jake and myself", "tokens": [50364, 400, 309, 311, 257, 1185, 257, 857, 295, 341, 2010, 11, 457, 456, 307, 1333, 295, 2857, 9102, 733, 295, 516, 341, 636, 11, 597, 366, 1936, 264, 2535, 295, 19458, 294, 7005, 278, 13, 50864, 50864, 286, 500, 380, 528, 281, 352, 666, 4365, 11, 457, 498, 291, 574, 337, 257, 3035, 466, 28867, 437, 689, 8399, 22660, 378, 433, 11, 291, 486, 915, 732, 3035, 11, 472, 538, 15822, 293, 2059, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.09128919205108246, "compression_ratio": 1.5226130653266332, "no_speech_prob": 0.00011229536175960675}, {"id": 664, "seek": 589300, "start": 5893.0, "end": 5906.0, "text": " and a photo web paper by a group from Michigan, University of Michigan that basically enhanced it and sort of trained them on ImageNet and got some decent results.", "tokens": [50364, 293, 257, 5052, 3670, 3035, 538, 257, 1594, 490, 11925, 11, 3535, 295, 11925, 300, 1936, 21191, 309, 293, 1333, 295, 8895, 552, 322, 29903, 31890, 293, 658, 512, 8681, 3542, 13, 51014, 51014, 407, 729, 366, 733, 295, 6331, 1303, 291, 393, 764, 281, 360, 733, 295, 2698, 12, 48172, 24420, 2539, 13, 51264, 51264, 1449, 281, 17594, 264, 9834, 337, 264, 5587, 11, 307, 337, 264, 47991, 47387, 1433, 294, 264, 4470, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.15785478036614914, "compression_ratio": 1.5454545454545454, "no_speech_prob": 2.586181835795287e-05}, {"id": 665, "seek": 589300, "start": 5906.0, "end": 5911.0, "text": " So those are kind of architectures you can use to do kind of self-supervised learning.", "tokens": [50364, 293, 257, 5052, 3670, 3035, 538, 257, 1594, 490, 11925, 11, 3535, 295, 11925, 300, 1936, 21191, 309, 293, 1333, 295, 8895, 552, 322, 29903, 31890, 293, 658, 512, 8681, 3542, 13, 51014, 51014, 407, 729, 366, 733, 295, 6331, 1303, 291, 393, 764, 281, 360, 733, 295, 2698, 12, 48172, 24420, 2539, 13, 51264, 51264, 1449, 281, 17594, 264, 9834, 337, 264, 5587, 11, 307, 337, 264, 47991, 47387, 1433, 294, 264, 4470, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.15785478036614914, "compression_ratio": 1.5454545454545454, "no_speech_prob": 2.586181835795287e-05}, {"id": 666, "seek": 589300, "start": 5911.0, "end": 5917.0, "text": " Just to clarify the parameters for the spring, is for the KL divergence term in the loss?", "tokens": [50364, 293, 257, 5052, 3670, 3035, 538, 257, 1594, 490, 11925, 11, 3535, 295, 11925, 300, 1936, 21191, 309, 293, 1333, 295, 8895, 552, 322, 29903, 31890, 293, 658, 512, 8681, 3542, 13, 51014, 51014, 407, 729, 366, 733, 295, 6331, 1303, 291, 393, 764, 281, 360, 733, 295, 2698, 12, 48172, 24420, 2539, 13, 51264, 51264, 1449, 281, 17594, 264, 9834, 337, 264, 5587, 11, 307, 337, 264, 47991, 47387, 1433, 294, 264, 4470, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.15785478036614914, "compression_ratio": 1.5454545454545454, "no_speech_prob": 2.586181835795287e-05}, {"id": 667, "seek": 591700, "start": 5917.0, "end": 5923.0, "text": " Right. Yeah, so the KL divergence term in the loss.", "tokens": [50364, 1779, 13, 865, 11, 370, 264, 47991, 47387, 1433, 294, 264, 4470, 13, 50664, 50664, 492, 434, 516, 281, 536, 341, 4153, 11, 1074, 13, 407, 321, 434, 516, 281, 312, 516, 807, 264, 5367, 293, 439, 264, 4365, 13, 50914, 50914, 407, 286, 603, 2060, 341, 4153, 13, 407, 286, 603, 536, 291, 4153, 11, 4696, 365, 257, 960, 382, 731, 13, 51214, 51214, 759, 264, 23647, 9346, 309, 11, 286, 486, 829, 264, 6613, 295, 341, 1508, 2950, 382, 2321, 382, 309, 311, 767, 2435, 281, 385, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1492619311555903, "compression_ratio": 1.6441441441441442, "no_speech_prob": 7.58378955652006e-05}, {"id": 668, "seek": 591700, "start": 5923.0, "end": 5928.0, "text": " We're going to see this tomorrow, guys. So we're going to be going through the equation and all the details.", "tokens": [50364, 1779, 13, 865, 11, 370, 264, 47991, 47387, 1433, 294, 264, 4470, 13, 50664, 50664, 492, 434, 516, 281, 536, 341, 4153, 11, 1074, 13, 407, 321, 434, 516, 281, 312, 516, 807, 264, 5367, 293, 439, 264, 4365, 13, 50914, 50914, 407, 286, 603, 2060, 341, 4153, 13, 407, 286, 603, 536, 291, 4153, 11, 4696, 365, 257, 960, 382, 731, 13, 51214, 51214, 759, 264, 23647, 9346, 309, 11, 286, 486, 829, 264, 6613, 295, 341, 1508, 2950, 382, 2321, 382, 309, 311, 767, 2435, 281, 385, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1492619311555903, "compression_ratio": 1.6441441441441442, "no_speech_prob": 7.58378955652006e-05}, {"id": 669, "seek": 591700, "start": 5928.0, "end": 5934.0, "text": " So I'll cover this tomorrow. So I'll see you tomorrow, hopefully with a video as well.", "tokens": [50364, 1779, 13, 865, 11, 370, 264, 47991, 47387, 1433, 294, 264, 4470, 13, 50664, 50664, 492, 434, 516, 281, 536, 341, 4153, 11, 1074, 13, 407, 321, 434, 516, 281, 312, 516, 807, 264, 5367, 293, 439, 264, 4365, 13, 50914, 50914, 407, 286, 603, 2060, 341, 4153, 13, 407, 286, 603, 536, 291, 4153, 11, 4696, 365, 257, 960, 382, 731, 13, 51214, 51214, 759, 264, 23647, 9346, 309, 11, 286, 486, 829, 264, 6613, 295, 341, 1508, 2950, 382, 2321, 382, 309, 311, 767, 2435, 281, 385, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1492619311555903, "compression_ratio": 1.6441441441441442, "no_speech_prob": 7.58378955652006e-05}, {"id": 670, "seek": 591700, "start": 5934.0, "end": 5944.0, "text": " If the bandwidth supports it, I will put the recording of this class online as soon as it's actually available to me.", "tokens": [50364, 1779, 13, 865, 11, 370, 264, 47991, 47387, 1433, 294, 264, 4470, 13, 50664, 50664, 492, 434, 516, 281, 536, 341, 4153, 11, 1074, 13, 407, 321, 434, 516, 281, 312, 516, 807, 264, 5367, 293, 439, 264, 4365, 13, 50914, 50914, 407, 286, 603, 2060, 341, 4153, 13, 407, 286, 603, 536, 291, 4153, 11, 4696, 365, 257, 960, 382, 731, 13, 51214, 51214, 759, 264, 23647, 9346, 309, 11, 286, 486, 829, 264, 6613, 295, 341, 1508, 2950, 382, 2321, 382, 309, 311, 767, 2435, 281, 385, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1492619311555903, "compression_ratio": 1.6441441441441442, "no_speech_prob": 7.58378955652006e-05}, {"id": 671, "seek": 594400, "start": 5944.0, "end": 5955.0, "text": " I will add it to the NYU streaming platform and then, you know, I will try to clean it up as I can and, you know, upload it as well on YouTube later on.", "tokens": [50364, 286, 486, 909, 309, 281, 264, 42682, 11791, 3663, 293, 550, 11, 291, 458, 11, 286, 486, 853, 281, 2541, 309, 493, 382, 286, 393, 293, 11, 291, 458, 11, 6580, 309, 382, 731, 322, 3088, 1780, 322, 13, 50914, 50914, 1057, 558, 13, 407, 1309, 291, 797, 13, 8691, 1280, 13, 8691, 4561, 13, 400, 286, 536, 291, 4153, 13, 8691, 3273, 13, 4621, 6543, 13, 8691, 3273, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14961464764320687, "compression_ratio": 1.5204678362573099, "no_speech_prob": 6.80600933264941e-05}, {"id": 672, "seek": 595500, "start": 5955.0, "end": 5978.0, "text": " All right. So thank you again. Stay home, stay warm and I see you tomorrow. Stay safe. Bye bye. Stay safe.", "tokens": [50364, 1057, 558, 13, 407, 1309, 291, 797, 13, 8691, 1280, 11, 1754, 4561, 293, 286, 536, 291, 4153, 13, 8691, 3273, 13, 4621, 6543, 13, 8691, 3273, 13, 51514], "temperature": 0.0, "avg_logprob": -0.18760581170358964, "compression_ratio": 1.1777777777777778, "no_speech_prob": 0.0002643383922986686}], "language": "en", "video_id": "ZaVP2SY23nc", "entity": "Yann LeCun"}}