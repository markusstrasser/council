{"video_id": "Eu8lt4j9xiU", "title": "1.18 Machine Learning Overview | Learning rate  --[Machine Learning | Andrew Ng]", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 543, "views": 351, "publish_date": "11/04/2022", "timestamp": 1660953600, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " The choice of the learning rate alpha will have a huge impact on the efficiency of your implementation of gradient descent, and if alpha, the learning rate, is chosen poorly, gradient descent may not even work at all. In this video, let's take a deeper look at the learning rate. This will also help you choose better learning rates for your implementations of gradient descent. So, here again is the gradient descent rule. W is updated to be W minus the learning rate alpha times the derivative term. To learn more about what the learning rate alpha is doing, let's see what could happen if the learning rate alpha is either too small, or if it is too large. For the case where the learning rate is too small, here's a graph where the horizontal axis is W and the vertical axis is the cos j, and here's a graph of the function j of W. Let's start gradient descent at this point here. If the learning rate is too small, then what happens is that you multiply your derivative term by some really, really, really small number. So, you're going to be multiplying by a number alpha that's really small, like 0.0000001, and so you end up taking a very small baby step like that. Then from this point, you're going to take another, you know, tiny, tiny little baby step, but because the learning rate is so small, the second step is also just minus Q. The outcome of this process is that you do end up decreasing the cos j, but incredibly slowly. So, here's another step, and another step, another tiny step, until you finally approach the minimum. But, as you may notice, you're going to need a lot of steps to get to the minimum. So, to summarize, if the learning rate is too small, then gradient descent will work, but it will be slow. It will take a very long time, because it's going to take these tiny, tiny baby steps, and it's going to need a lot of steps before it gets anywhere close to the minimum. Now, let's look at a different case. What happens if the learning rate is too large? Here's another graph of the cos function, and let's say we start gradient descent with W at this value here. So, it's actually already pretty close to the minimum. So, the derivative points to the right, but if the learning rate is too large, then you update W via a giant step to be all the way over here, and that's this point here on the function j. So, you move from this point on the left all the way to this point on the right, and now the cost has actually gotten worse. It has increased because it has started out at this value here, and after one step, it actually increased to this value here. Now, the derivative at this new point says to decrease W, but when the learning rate is too big, then you may take a huge step going from here all the way out here. So, now you've gotten to this point here, and again, if the learning rate is too big, then you take another huge step at the next iteration and way overshoot the minimum again. So, now you're at this point on the right, and one more time, you do another update and end up all the way here, and so you're now at this point here. So, as you may notice, you're actually getting further and further away from the minimum. So, if the learning rate is too large, then gradient descent may overshoot and may never reach the minimum. And another way to say that is that gradient descent may fail to converge and may even diverge. So, here's another question you may be wondering. What if your parameter W is already at this point here, so that your cost J is already at a local minimum? What do you think one step of gradient descent will do if you've already reached a minimum? So, this is a tricky one. When I was first learning this stuff, it actually took me a long time to figure it out, but let's work through this together. Let's suppose you have some cost function J, and the one you see here isn't a square error cost function, and this cost function has two local minima corresponding to the two values that you see here. Now, let's suppose that after some number of steps of gradient descent, your parameter W is over here, say equal to 5, and so this is the current value of W. This means that you're at this point on the cost function J, and that happens to be a local minimum. Turns out, if you draw a tangent to the function at this point, the slope of this line is zero, and thus, the derivative term here is equal to zero for the current value of W, and so your gradient descent update becomes W is updated to W minus the learning rate times zero, where here, that's because the derivative term is equal to zero. And this is the same as saying, let's set W to be equal to W. So, this means that if you're already at a local minimum, gradient descent leaves W unchanged because it just updates the new value of W to be the exact same old value of W. So concretely, let's say if the current value of W is 5, and alpha is 0.1, after one iteration, you update W as W minus alpha times zero, and it is still equal to 5. So, if your parameters have already brought you to a local minimum, then further gradient descent steps do absolutely nothing. It doesn't change the parameters, which is what you want, because it keeps the solution at that local minimum. This also explains why gradient descent can reach a local minimum, even with a fixed learning rate alpha. Here's what I mean. To illustrate this, let's look at another example. Here's the cost function J of W that we want to minimize. Let's initialize gradient descent up here at this point. If we take one update step, maybe it'll take us to that point. And because this derivative is pretty large, gradient descent takes a relatively big step, right? Now, we're at this second point, where we take another step. And you may notice that the slope is not as steep as it was at the first point, so the derivative isn't as large. And so the next update step will not be as large as that first step. Now, we're at this third point here, and the derivative is smaller than it was at the previous step, and we'll take an even smaller step. As we approach the minimum, the derivative gets closer and closer to zero. So as we run gradient descent, eventually we're taking very small steps until you finally reach a local minimum. So just to recap, as we get near a local minimum, gradient descent will automatically take smaller steps. And that's because as we approach the local minimum, the derivative automatically gets smaller, and that means the update steps also automatically get smaller, even if the learning rate alpha is kept at some fixed value. So that's the gradient descent algorithm. You can use it to try to minimize any cost function j, not just the mean squared error cost function that we're using for linear regression. In the next video, we're going to take the function j and set that back to be exactly the linear regression model's cost function, the mean squared error cost function that we had come up with earlier. And putting together gradient descent with this cost function that will give you your first learning algorithm, the linear regression algorithm.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.12, "text": " The choice of the learning rate alpha will have a huge impact on the efficiency of your", "tokens": [50364, 440, 3922, 295, 264, 2539, 3314, 8961, 486, 362, 257, 2603, 2712, 322, 264, 10493, 295, 428, 50720, 50720, 11420, 295, 16235, 23475, 11, 293, 498, 8961, 11, 264, 2539, 3314, 11, 307, 8614, 22271, 11, 51028, 51028, 16235, 23475, 815, 406, 754, 589, 412, 439, 13, 51161, 51161, 682, 341, 960, 11, 718, 311, 747, 257, 7731, 574, 412, 264, 2539, 3314, 13, 51354, 51354, 639, 486, 611, 854, 291, 2826, 1101, 2539, 6846, 337, 428, 4445, 763, 295, 16235, 51616, 51616, 23475, 13, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.1414222288667486, "compression_ratio": 1.8805970149253732, "no_speech_prob": 0.012618676759302616}, {"id": 1, "seek": 0, "start": 7.12, "end": 13.280000000000001, "text": " implementation of gradient descent, and if alpha, the learning rate, is chosen poorly,", "tokens": [50364, 440, 3922, 295, 264, 2539, 3314, 8961, 486, 362, 257, 2603, 2712, 322, 264, 10493, 295, 428, 50720, 50720, 11420, 295, 16235, 23475, 11, 293, 498, 8961, 11, 264, 2539, 3314, 11, 307, 8614, 22271, 11, 51028, 51028, 16235, 23475, 815, 406, 754, 589, 412, 439, 13, 51161, 51161, 682, 341, 960, 11, 718, 311, 747, 257, 7731, 574, 412, 264, 2539, 3314, 13, 51354, 51354, 639, 486, 611, 854, 291, 2826, 1101, 2539, 6846, 337, 428, 4445, 763, 295, 16235, 51616, 51616, 23475, 13, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.1414222288667486, "compression_ratio": 1.8805970149253732, "no_speech_prob": 0.012618676759302616}, {"id": 2, "seek": 0, "start": 13.280000000000001, "end": 15.94, "text": " gradient descent may not even work at all.", "tokens": [50364, 440, 3922, 295, 264, 2539, 3314, 8961, 486, 362, 257, 2603, 2712, 322, 264, 10493, 295, 428, 50720, 50720, 11420, 295, 16235, 23475, 11, 293, 498, 8961, 11, 264, 2539, 3314, 11, 307, 8614, 22271, 11, 51028, 51028, 16235, 23475, 815, 406, 754, 589, 412, 439, 13, 51161, 51161, 682, 341, 960, 11, 718, 311, 747, 257, 7731, 574, 412, 264, 2539, 3314, 13, 51354, 51354, 639, 486, 611, 854, 291, 2826, 1101, 2539, 6846, 337, 428, 4445, 763, 295, 16235, 51616, 51616, 23475, 13, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.1414222288667486, "compression_ratio": 1.8805970149253732, "no_speech_prob": 0.012618676759302616}, {"id": 3, "seek": 0, "start": 15.94, "end": 19.8, "text": " In this video, let's take a deeper look at the learning rate.", "tokens": [50364, 440, 3922, 295, 264, 2539, 3314, 8961, 486, 362, 257, 2603, 2712, 322, 264, 10493, 295, 428, 50720, 50720, 11420, 295, 16235, 23475, 11, 293, 498, 8961, 11, 264, 2539, 3314, 11, 307, 8614, 22271, 11, 51028, 51028, 16235, 23475, 815, 406, 754, 589, 412, 439, 13, 51161, 51161, 682, 341, 960, 11, 718, 311, 747, 257, 7731, 574, 412, 264, 2539, 3314, 13, 51354, 51354, 639, 486, 611, 854, 291, 2826, 1101, 2539, 6846, 337, 428, 4445, 763, 295, 16235, 51616, 51616, 23475, 13, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.1414222288667486, "compression_ratio": 1.8805970149253732, "no_speech_prob": 0.012618676759302616}, {"id": 4, "seek": 0, "start": 19.8, "end": 25.04, "text": " This will also help you choose better learning rates for your implementations of gradient", "tokens": [50364, 440, 3922, 295, 264, 2539, 3314, 8961, 486, 362, 257, 2603, 2712, 322, 264, 10493, 295, 428, 50720, 50720, 11420, 295, 16235, 23475, 11, 293, 498, 8961, 11, 264, 2539, 3314, 11, 307, 8614, 22271, 11, 51028, 51028, 16235, 23475, 815, 406, 754, 589, 412, 439, 13, 51161, 51161, 682, 341, 960, 11, 718, 311, 747, 257, 7731, 574, 412, 264, 2539, 3314, 13, 51354, 51354, 639, 486, 611, 854, 291, 2826, 1101, 2539, 6846, 337, 428, 4445, 763, 295, 16235, 51616, 51616, 23475, 13, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.1414222288667486, "compression_ratio": 1.8805970149253732, "no_speech_prob": 0.012618676759302616}, {"id": 5, "seek": 0, "start": 25.04, "end": 26.04, "text": " descent.", "tokens": [50364, 440, 3922, 295, 264, 2539, 3314, 8961, 486, 362, 257, 2603, 2712, 322, 264, 10493, 295, 428, 50720, 50720, 11420, 295, 16235, 23475, 11, 293, 498, 8961, 11, 264, 2539, 3314, 11, 307, 8614, 22271, 11, 51028, 51028, 16235, 23475, 815, 406, 754, 589, 412, 439, 13, 51161, 51161, 682, 341, 960, 11, 718, 311, 747, 257, 7731, 574, 412, 264, 2539, 3314, 13, 51354, 51354, 639, 486, 611, 854, 291, 2826, 1101, 2539, 6846, 337, 428, 4445, 763, 295, 16235, 51616, 51616, 23475, 13, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.1414222288667486, "compression_ratio": 1.8805970149253732, "no_speech_prob": 0.012618676759302616}, {"id": 6, "seek": 2604, "start": 26.04, "end": 30.04, "text": " So, here again is the gradient descent rule.", "tokens": [50364, 407, 11, 510, 797, 307, 264, 16235, 23475, 4978, 13, 50564, 50564, 343, 307, 10588, 281, 312, 343, 3175, 264, 2539, 3314, 8961, 1413, 264, 13760, 1433, 13, 50858, 50858, 1407, 1466, 544, 466, 437, 264, 2539, 3314, 8961, 307, 884, 11, 718, 311, 536, 437, 727, 1051, 51060, 51060, 498, 264, 2539, 3314, 8961, 307, 2139, 886, 1359, 11, 420, 498, 309, 307, 886, 2416, 13, 51380, 51380, 1171, 264, 1389, 689, 264, 2539, 3314, 307, 886, 1359, 11, 510, 311, 257, 4295, 689, 264, 12750, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.1299639167366447, "compression_ratio": 1.8629441624365481, "no_speech_prob": 3.321039548609406e-05}, {"id": 7, "seek": 2604, "start": 30.04, "end": 35.92, "text": " W is updated to be W minus the learning rate alpha times the derivative term.", "tokens": [50364, 407, 11, 510, 797, 307, 264, 16235, 23475, 4978, 13, 50564, 50564, 343, 307, 10588, 281, 312, 343, 3175, 264, 2539, 3314, 8961, 1413, 264, 13760, 1433, 13, 50858, 50858, 1407, 1466, 544, 466, 437, 264, 2539, 3314, 8961, 307, 884, 11, 718, 311, 536, 437, 727, 1051, 51060, 51060, 498, 264, 2539, 3314, 8961, 307, 2139, 886, 1359, 11, 420, 498, 309, 307, 886, 2416, 13, 51380, 51380, 1171, 264, 1389, 689, 264, 2539, 3314, 307, 886, 1359, 11, 510, 311, 257, 4295, 689, 264, 12750, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.1299639167366447, "compression_ratio": 1.8629441624365481, "no_speech_prob": 3.321039548609406e-05}, {"id": 8, "seek": 2604, "start": 35.92, "end": 39.96, "text": " To learn more about what the learning rate alpha is doing, let's see what could happen", "tokens": [50364, 407, 11, 510, 797, 307, 264, 16235, 23475, 4978, 13, 50564, 50564, 343, 307, 10588, 281, 312, 343, 3175, 264, 2539, 3314, 8961, 1413, 264, 13760, 1433, 13, 50858, 50858, 1407, 1466, 544, 466, 437, 264, 2539, 3314, 8961, 307, 884, 11, 718, 311, 536, 437, 727, 1051, 51060, 51060, 498, 264, 2539, 3314, 8961, 307, 2139, 886, 1359, 11, 420, 498, 309, 307, 886, 2416, 13, 51380, 51380, 1171, 264, 1389, 689, 264, 2539, 3314, 307, 886, 1359, 11, 510, 311, 257, 4295, 689, 264, 12750, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.1299639167366447, "compression_ratio": 1.8629441624365481, "no_speech_prob": 3.321039548609406e-05}, {"id": 9, "seek": 2604, "start": 39.96, "end": 46.36, "text": " if the learning rate alpha is either too small, or if it is too large.", "tokens": [50364, 407, 11, 510, 797, 307, 264, 16235, 23475, 4978, 13, 50564, 50564, 343, 307, 10588, 281, 312, 343, 3175, 264, 2539, 3314, 8961, 1413, 264, 13760, 1433, 13, 50858, 50858, 1407, 1466, 544, 466, 437, 264, 2539, 3314, 8961, 307, 884, 11, 718, 311, 536, 437, 727, 1051, 51060, 51060, 498, 264, 2539, 3314, 8961, 307, 2139, 886, 1359, 11, 420, 498, 309, 307, 886, 2416, 13, 51380, 51380, 1171, 264, 1389, 689, 264, 2539, 3314, 307, 886, 1359, 11, 510, 311, 257, 4295, 689, 264, 12750, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.1299639167366447, "compression_ratio": 1.8629441624365481, "no_speech_prob": 3.321039548609406e-05}, {"id": 10, "seek": 2604, "start": 46.36, "end": 51.2, "text": " For the case where the learning rate is too small, here's a graph where the horizontal", "tokens": [50364, 407, 11, 510, 797, 307, 264, 16235, 23475, 4978, 13, 50564, 50564, 343, 307, 10588, 281, 312, 343, 3175, 264, 2539, 3314, 8961, 1413, 264, 13760, 1433, 13, 50858, 50858, 1407, 1466, 544, 466, 437, 264, 2539, 3314, 8961, 307, 884, 11, 718, 311, 536, 437, 727, 1051, 51060, 51060, 498, 264, 2539, 3314, 8961, 307, 2139, 886, 1359, 11, 420, 498, 309, 307, 886, 2416, 13, 51380, 51380, 1171, 264, 1389, 689, 264, 2539, 3314, 307, 886, 1359, 11, 510, 311, 257, 4295, 689, 264, 12750, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.1299639167366447, "compression_ratio": 1.8629441624365481, "no_speech_prob": 3.321039548609406e-05}, {"id": 11, "seek": 5120, "start": 51.2, "end": 59.800000000000004, "text": " axis is W and the vertical axis is the cos j, and here's a graph of the function j of", "tokens": [50364, 10298, 307, 343, 293, 264, 9429, 10298, 307, 264, 3792, 361, 11, 293, 510, 311, 257, 4295, 295, 264, 2445, 361, 295, 50794, 50794, 343, 13, 961, 311, 722, 16235, 23475, 412, 341, 935, 510, 13, 51068, 51068, 759, 264, 2539, 3314, 307, 886, 1359, 11, 550, 437, 2314, 307, 300, 291, 12972, 428, 13760, 51370, 51370, 1433, 538, 512, 534, 11, 534, 11, 534, 1359, 1230, 13, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.1546376413769192, "compression_ratio": 1.5649717514124293, "no_speech_prob": 5.173841145733604e-06}, {"id": 12, "seek": 5120, "start": 59.800000000000004, "end": 65.28, "text": " W. Let's start gradient descent at this point here.", "tokens": [50364, 10298, 307, 343, 293, 264, 9429, 10298, 307, 264, 3792, 361, 11, 293, 510, 311, 257, 4295, 295, 264, 2445, 361, 295, 50794, 50794, 343, 13, 961, 311, 722, 16235, 23475, 412, 341, 935, 510, 13, 51068, 51068, 759, 264, 2539, 3314, 307, 886, 1359, 11, 550, 437, 2314, 307, 300, 291, 12972, 428, 13760, 51370, 51370, 1433, 538, 512, 534, 11, 534, 11, 534, 1359, 1230, 13, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.1546376413769192, "compression_ratio": 1.5649717514124293, "no_speech_prob": 5.173841145733604e-06}, {"id": 13, "seek": 5120, "start": 65.28, "end": 71.32000000000001, "text": " If the learning rate is too small, then what happens is that you multiply your derivative", "tokens": [50364, 10298, 307, 343, 293, 264, 9429, 10298, 307, 264, 3792, 361, 11, 293, 510, 311, 257, 4295, 295, 264, 2445, 361, 295, 50794, 50794, 343, 13, 961, 311, 722, 16235, 23475, 412, 341, 935, 510, 13, 51068, 51068, 759, 264, 2539, 3314, 307, 886, 1359, 11, 550, 437, 2314, 307, 300, 291, 12972, 428, 13760, 51370, 51370, 1433, 538, 512, 534, 11, 534, 11, 534, 1359, 1230, 13, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.1546376413769192, "compression_ratio": 1.5649717514124293, "no_speech_prob": 5.173841145733604e-06}, {"id": 14, "seek": 5120, "start": 71.32000000000001, "end": 74.64, "text": " term by some really, really, really small number.", "tokens": [50364, 10298, 307, 343, 293, 264, 9429, 10298, 307, 264, 3792, 361, 11, 293, 510, 311, 257, 4295, 295, 264, 2445, 361, 295, 50794, 50794, 343, 13, 961, 311, 722, 16235, 23475, 412, 341, 935, 510, 13, 51068, 51068, 759, 264, 2539, 3314, 307, 886, 1359, 11, 550, 437, 2314, 307, 300, 291, 12972, 428, 13760, 51370, 51370, 1433, 538, 512, 534, 11, 534, 11, 534, 1359, 1230, 13, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.1546376413769192, "compression_ratio": 1.5649717514124293, "no_speech_prob": 5.173841145733604e-06}, {"id": 15, "seek": 7464, "start": 74.64, "end": 83.52, "text": " So, you're going to be multiplying by a number alpha that's really small, like 0.0000001,", "tokens": [50364, 407, 11, 291, 434, 516, 281, 312, 30955, 538, 257, 1230, 8961, 300, 311, 534, 1359, 11, 411, 1958, 13, 33202, 628, 16, 11, 50808, 50808, 293, 370, 291, 917, 493, 1940, 257, 588, 1359, 3186, 1823, 411, 300, 13, 51060, 51060, 1396, 490, 341, 935, 11, 291, 434, 516, 281, 747, 1071, 11, 291, 458, 11, 5870, 11, 5870, 707, 3186, 51270, 51270, 1823, 11, 457, 570, 264, 2539, 3314, 307, 370, 1359, 11, 264, 1150, 1823, 307, 611, 445, 3175, 51608, 51608, 1249, 13, 51658, 51658], "temperature": 0.0, "avg_logprob": -0.1489020135667589, "compression_ratio": 1.6080402010050252, "no_speech_prob": 2.4060743726295186e-06}, {"id": 16, "seek": 7464, "start": 83.52, "end": 88.56, "text": " and so you end up taking a very small baby step like that.", "tokens": [50364, 407, 11, 291, 434, 516, 281, 312, 30955, 538, 257, 1230, 8961, 300, 311, 534, 1359, 11, 411, 1958, 13, 33202, 628, 16, 11, 50808, 50808, 293, 370, 291, 917, 493, 1940, 257, 588, 1359, 3186, 1823, 411, 300, 13, 51060, 51060, 1396, 490, 341, 935, 11, 291, 434, 516, 281, 747, 1071, 11, 291, 458, 11, 5870, 11, 5870, 707, 3186, 51270, 51270, 1823, 11, 457, 570, 264, 2539, 3314, 307, 370, 1359, 11, 264, 1150, 1823, 307, 611, 445, 3175, 51608, 51608, 1249, 13, 51658, 51658], "temperature": 0.0, "avg_logprob": -0.1489020135667589, "compression_ratio": 1.6080402010050252, "no_speech_prob": 2.4060743726295186e-06}, {"id": 17, "seek": 7464, "start": 88.56, "end": 92.76, "text": " Then from this point, you're going to take another, you know, tiny, tiny little baby", "tokens": [50364, 407, 11, 291, 434, 516, 281, 312, 30955, 538, 257, 1230, 8961, 300, 311, 534, 1359, 11, 411, 1958, 13, 33202, 628, 16, 11, 50808, 50808, 293, 370, 291, 917, 493, 1940, 257, 588, 1359, 3186, 1823, 411, 300, 13, 51060, 51060, 1396, 490, 341, 935, 11, 291, 434, 516, 281, 747, 1071, 11, 291, 458, 11, 5870, 11, 5870, 707, 3186, 51270, 51270, 1823, 11, 457, 570, 264, 2539, 3314, 307, 370, 1359, 11, 264, 1150, 1823, 307, 611, 445, 3175, 51608, 51608, 1249, 13, 51658, 51658], "temperature": 0.0, "avg_logprob": -0.1489020135667589, "compression_ratio": 1.6080402010050252, "no_speech_prob": 2.4060743726295186e-06}, {"id": 18, "seek": 7464, "start": 92.76, "end": 99.52, "text": " step, but because the learning rate is so small, the second step is also just minus", "tokens": [50364, 407, 11, 291, 434, 516, 281, 312, 30955, 538, 257, 1230, 8961, 300, 311, 534, 1359, 11, 411, 1958, 13, 33202, 628, 16, 11, 50808, 50808, 293, 370, 291, 917, 493, 1940, 257, 588, 1359, 3186, 1823, 411, 300, 13, 51060, 51060, 1396, 490, 341, 935, 11, 291, 434, 516, 281, 747, 1071, 11, 291, 458, 11, 5870, 11, 5870, 707, 3186, 51270, 51270, 1823, 11, 457, 570, 264, 2539, 3314, 307, 370, 1359, 11, 264, 1150, 1823, 307, 611, 445, 3175, 51608, 51608, 1249, 13, 51658, 51658], "temperature": 0.0, "avg_logprob": -0.1489020135667589, "compression_ratio": 1.6080402010050252, "no_speech_prob": 2.4060743726295186e-06}, {"id": 19, "seek": 7464, "start": 99.52, "end": 100.52, "text": " Q.", "tokens": [50364, 407, 11, 291, 434, 516, 281, 312, 30955, 538, 257, 1230, 8961, 300, 311, 534, 1359, 11, 411, 1958, 13, 33202, 628, 16, 11, 50808, 50808, 293, 370, 291, 917, 493, 1940, 257, 588, 1359, 3186, 1823, 411, 300, 13, 51060, 51060, 1396, 490, 341, 935, 11, 291, 434, 516, 281, 747, 1071, 11, 291, 458, 11, 5870, 11, 5870, 707, 3186, 51270, 51270, 1823, 11, 457, 570, 264, 2539, 3314, 307, 370, 1359, 11, 264, 1150, 1823, 307, 611, 445, 3175, 51608, 51608, 1249, 13, 51658, 51658], "temperature": 0.0, "avg_logprob": -0.1489020135667589, "compression_ratio": 1.6080402010050252, "no_speech_prob": 2.4060743726295186e-06}, {"id": 20, "seek": 10052, "start": 100.52, "end": 106.24, "text": " The outcome of this process is that you do end up decreasing the cos j, but incredibly", "tokens": [50364, 440, 9700, 295, 341, 1399, 307, 300, 291, 360, 917, 493, 23223, 264, 3792, 361, 11, 457, 6252, 50650, 50650, 5692, 13, 50700, 50700, 407, 11, 510, 311, 1071, 1823, 11, 293, 1071, 1823, 11, 1071, 5870, 1823, 11, 1826, 291, 2721, 3109, 51002, 51002, 264, 7285, 13, 51052, 51052, 583, 11, 382, 291, 815, 3449, 11, 291, 434, 516, 281, 643, 257, 688, 295, 4439, 281, 483, 281, 264, 7285, 13, 51292, 51292, 407, 11, 281, 20858, 11, 498, 264, 2539, 3314, 307, 886, 1359, 11, 550, 16235, 23475, 486, 589, 11, 51656, 51656, 457, 309, 486, 312, 2964, 13, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.14614760080973307, "compression_ratio": 1.685589519650655, "no_speech_prob": 5.7717898016562685e-06}, {"id": 21, "seek": 10052, "start": 106.24, "end": 107.24, "text": " slowly.", "tokens": [50364, 440, 9700, 295, 341, 1399, 307, 300, 291, 360, 917, 493, 23223, 264, 3792, 361, 11, 457, 6252, 50650, 50650, 5692, 13, 50700, 50700, 407, 11, 510, 311, 1071, 1823, 11, 293, 1071, 1823, 11, 1071, 5870, 1823, 11, 1826, 291, 2721, 3109, 51002, 51002, 264, 7285, 13, 51052, 51052, 583, 11, 382, 291, 815, 3449, 11, 291, 434, 516, 281, 643, 257, 688, 295, 4439, 281, 483, 281, 264, 7285, 13, 51292, 51292, 407, 11, 281, 20858, 11, 498, 264, 2539, 3314, 307, 886, 1359, 11, 550, 16235, 23475, 486, 589, 11, 51656, 51656, 457, 309, 486, 312, 2964, 13, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.14614760080973307, "compression_ratio": 1.685589519650655, "no_speech_prob": 5.7717898016562685e-06}, {"id": 22, "seek": 10052, "start": 107.24, "end": 113.28, "text": " So, here's another step, and another step, another tiny step, until you finally approach", "tokens": [50364, 440, 9700, 295, 341, 1399, 307, 300, 291, 360, 917, 493, 23223, 264, 3792, 361, 11, 457, 6252, 50650, 50650, 5692, 13, 50700, 50700, 407, 11, 510, 311, 1071, 1823, 11, 293, 1071, 1823, 11, 1071, 5870, 1823, 11, 1826, 291, 2721, 3109, 51002, 51002, 264, 7285, 13, 51052, 51052, 583, 11, 382, 291, 815, 3449, 11, 291, 434, 516, 281, 643, 257, 688, 295, 4439, 281, 483, 281, 264, 7285, 13, 51292, 51292, 407, 11, 281, 20858, 11, 498, 264, 2539, 3314, 307, 886, 1359, 11, 550, 16235, 23475, 486, 589, 11, 51656, 51656, 457, 309, 486, 312, 2964, 13, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.14614760080973307, "compression_ratio": 1.685589519650655, "no_speech_prob": 5.7717898016562685e-06}, {"id": 23, "seek": 10052, "start": 113.28, "end": 114.28, "text": " the minimum.", "tokens": [50364, 440, 9700, 295, 341, 1399, 307, 300, 291, 360, 917, 493, 23223, 264, 3792, 361, 11, 457, 6252, 50650, 50650, 5692, 13, 50700, 50700, 407, 11, 510, 311, 1071, 1823, 11, 293, 1071, 1823, 11, 1071, 5870, 1823, 11, 1826, 291, 2721, 3109, 51002, 51002, 264, 7285, 13, 51052, 51052, 583, 11, 382, 291, 815, 3449, 11, 291, 434, 516, 281, 643, 257, 688, 295, 4439, 281, 483, 281, 264, 7285, 13, 51292, 51292, 407, 11, 281, 20858, 11, 498, 264, 2539, 3314, 307, 886, 1359, 11, 550, 16235, 23475, 486, 589, 11, 51656, 51656, 457, 309, 486, 312, 2964, 13, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.14614760080973307, "compression_ratio": 1.685589519650655, "no_speech_prob": 5.7717898016562685e-06}, {"id": 24, "seek": 10052, "start": 114.28, "end": 119.08, "text": " But, as you may notice, you're going to need a lot of steps to get to the minimum.", "tokens": [50364, 440, 9700, 295, 341, 1399, 307, 300, 291, 360, 917, 493, 23223, 264, 3792, 361, 11, 457, 6252, 50650, 50650, 5692, 13, 50700, 50700, 407, 11, 510, 311, 1071, 1823, 11, 293, 1071, 1823, 11, 1071, 5870, 1823, 11, 1826, 291, 2721, 3109, 51002, 51002, 264, 7285, 13, 51052, 51052, 583, 11, 382, 291, 815, 3449, 11, 291, 434, 516, 281, 643, 257, 688, 295, 4439, 281, 483, 281, 264, 7285, 13, 51292, 51292, 407, 11, 281, 20858, 11, 498, 264, 2539, 3314, 307, 886, 1359, 11, 550, 16235, 23475, 486, 589, 11, 51656, 51656, 457, 309, 486, 312, 2964, 13, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.14614760080973307, "compression_ratio": 1.685589519650655, "no_speech_prob": 5.7717898016562685e-06}, {"id": 25, "seek": 10052, "start": 119.08, "end": 126.36, "text": " So, to summarize, if the learning rate is too small, then gradient descent will work,", "tokens": [50364, 440, 9700, 295, 341, 1399, 307, 300, 291, 360, 917, 493, 23223, 264, 3792, 361, 11, 457, 6252, 50650, 50650, 5692, 13, 50700, 50700, 407, 11, 510, 311, 1071, 1823, 11, 293, 1071, 1823, 11, 1071, 5870, 1823, 11, 1826, 291, 2721, 3109, 51002, 51002, 264, 7285, 13, 51052, 51052, 583, 11, 382, 291, 815, 3449, 11, 291, 434, 516, 281, 643, 257, 688, 295, 4439, 281, 483, 281, 264, 7285, 13, 51292, 51292, 407, 11, 281, 20858, 11, 498, 264, 2539, 3314, 307, 886, 1359, 11, 550, 16235, 23475, 486, 589, 11, 51656, 51656, 457, 309, 486, 312, 2964, 13, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.14614760080973307, "compression_ratio": 1.685589519650655, "no_speech_prob": 5.7717898016562685e-06}, {"id": 26, "seek": 10052, "start": 126.36, "end": 127.92, "text": " but it will be slow.", "tokens": [50364, 440, 9700, 295, 341, 1399, 307, 300, 291, 360, 917, 493, 23223, 264, 3792, 361, 11, 457, 6252, 50650, 50650, 5692, 13, 50700, 50700, 407, 11, 510, 311, 1071, 1823, 11, 293, 1071, 1823, 11, 1071, 5870, 1823, 11, 1826, 291, 2721, 3109, 51002, 51002, 264, 7285, 13, 51052, 51052, 583, 11, 382, 291, 815, 3449, 11, 291, 434, 516, 281, 643, 257, 688, 295, 4439, 281, 483, 281, 264, 7285, 13, 51292, 51292, 407, 11, 281, 20858, 11, 498, 264, 2539, 3314, 307, 886, 1359, 11, 550, 16235, 23475, 486, 589, 11, 51656, 51656, 457, 309, 486, 312, 2964, 13, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.14614760080973307, "compression_ratio": 1.685589519650655, "no_speech_prob": 5.7717898016562685e-06}, {"id": 27, "seek": 12792, "start": 127.92, "end": 132.92000000000002, "text": " It will take a very long time, because it's going to take these tiny, tiny baby steps,", "tokens": [50364, 467, 486, 747, 257, 588, 938, 565, 11, 570, 309, 311, 516, 281, 747, 613, 5870, 11, 5870, 3186, 4439, 11, 50614, 50614, 293, 309, 311, 516, 281, 643, 257, 688, 295, 4439, 949, 309, 2170, 4992, 1998, 281, 264, 7285, 13, 50814, 50814, 823, 11, 718, 311, 574, 412, 257, 819, 1389, 13, 51018, 51018, 708, 2314, 498, 264, 2539, 3314, 307, 886, 2416, 30, 51190, 51190, 1692, 311, 1071, 4295, 295, 264, 3792, 2445, 11, 293, 718, 311, 584, 321, 722, 16235, 23475, 365, 51412, 51412, 343, 412, 341, 2158, 510, 13, 51570, 51570, 407, 11, 309, 311, 767, 1217, 1238, 1998, 281, 264, 7285, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1805858782359532, "compression_ratio": 1.6626984126984128, "no_speech_prob": 4.936925051879371e-06}, {"id": 28, "seek": 12792, "start": 132.92000000000002, "end": 136.92000000000002, "text": " and it's going to need a lot of steps before it gets anywhere close to the minimum.", "tokens": [50364, 467, 486, 747, 257, 588, 938, 565, 11, 570, 309, 311, 516, 281, 747, 613, 5870, 11, 5870, 3186, 4439, 11, 50614, 50614, 293, 309, 311, 516, 281, 643, 257, 688, 295, 4439, 949, 309, 2170, 4992, 1998, 281, 264, 7285, 13, 50814, 50814, 823, 11, 718, 311, 574, 412, 257, 819, 1389, 13, 51018, 51018, 708, 2314, 498, 264, 2539, 3314, 307, 886, 2416, 30, 51190, 51190, 1692, 311, 1071, 4295, 295, 264, 3792, 2445, 11, 293, 718, 311, 584, 321, 722, 16235, 23475, 365, 51412, 51412, 343, 412, 341, 2158, 510, 13, 51570, 51570, 407, 11, 309, 311, 767, 1217, 1238, 1998, 281, 264, 7285, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1805858782359532, "compression_ratio": 1.6626984126984128, "no_speech_prob": 4.936925051879371e-06}, {"id": 29, "seek": 12792, "start": 136.92000000000002, "end": 141.0, "text": " Now, let's look at a different case.", "tokens": [50364, 467, 486, 747, 257, 588, 938, 565, 11, 570, 309, 311, 516, 281, 747, 613, 5870, 11, 5870, 3186, 4439, 11, 50614, 50614, 293, 309, 311, 516, 281, 643, 257, 688, 295, 4439, 949, 309, 2170, 4992, 1998, 281, 264, 7285, 13, 50814, 50814, 823, 11, 718, 311, 574, 412, 257, 819, 1389, 13, 51018, 51018, 708, 2314, 498, 264, 2539, 3314, 307, 886, 2416, 30, 51190, 51190, 1692, 311, 1071, 4295, 295, 264, 3792, 2445, 11, 293, 718, 311, 584, 321, 722, 16235, 23475, 365, 51412, 51412, 343, 412, 341, 2158, 510, 13, 51570, 51570, 407, 11, 309, 311, 767, 1217, 1238, 1998, 281, 264, 7285, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1805858782359532, "compression_ratio": 1.6626984126984128, "no_speech_prob": 4.936925051879371e-06}, {"id": 30, "seek": 12792, "start": 141.0, "end": 144.44, "text": " What happens if the learning rate is too large?", "tokens": [50364, 467, 486, 747, 257, 588, 938, 565, 11, 570, 309, 311, 516, 281, 747, 613, 5870, 11, 5870, 3186, 4439, 11, 50614, 50614, 293, 309, 311, 516, 281, 643, 257, 688, 295, 4439, 949, 309, 2170, 4992, 1998, 281, 264, 7285, 13, 50814, 50814, 823, 11, 718, 311, 574, 412, 257, 819, 1389, 13, 51018, 51018, 708, 2314, 498, 264, 2539, 3314, 307, 886, 2416, 30, 51190, 51190, 1692, 311, 1071, 4295, 295, 264, 3792, 2445, 11, 293, 718, 311, 584, 321, 722, 16235, 23475, 365, 51412, 51412, 343, 412, 341, 2158, 510, 13, 51570, 51570, 407, 11, 309, 311, 767, 1217, 1238, 1998, 281, 264, 7285, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1805858782359532, "compression_ratio": 1.6626984126984128, "no_speech_prob": 4.936925051879371e-06}, {"id": 31, "seek": 12792, "start": 144.44, "end": 148.88, "text": " Here's another graph of the cos function, and let's say we start gradient descent with", "tokens": [50364, 467, 486, 747, 257, 588, 938, 565, 11, 570, 309, 311, 516, 281, 747, 613, 5870, 11, 5870, 3186, 4439, 11, 50614, 50614, 293, 309, 311, 516, 281, 643, 257, 688, 295, 4439, 949, 309, 2170, 4992, 1998, 281, 264, 7285, 13, 50814, 50814, 823, 11, 718, 311, 574, 412, 257, 819, 1389, 13, 51018, 51018, 708, 2314, 498, 264, 2539, 3314, 307, 886, 2416, 30, 51190, 51190, 1692, 311, 1071, 4295, 295, 264, 3792, 2445, 11, 293, 718, 311, 584, 321, 722, 16235, 23475, 365, 51412, 51412, 343, 412, 341, 2158, 510, 13, 51570, 51570, 407, 11, 309, 311, 767, 1217, 1238, 1998, 281, 264, 7285, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1805858782359532, "compression_ratio": 1.6626984126984128, "no_speech_prob": 4.936925051879371e-06}, {"id": 32, "seek": 12792, "start": 148.88, "end": 152.04, "text": " W at this value here.", "tokens": [50364, 467, 486, 747, 257, 588, 938, 565, 11, 570, 309, 311, 516, 281, 747, 613, 5870, 11, 5870, 3186, 4439, 11, 50614, 50614, 293, 309, 311, 516, 281, 643, 257, 688, 295, 4439, 949, 309, 2170, 4992, 1998, 281, 264, 7285, 13, 50814, 50814, 823, 11, 718, 311, 574, 412, 257, 819, 1389, 13, 51018, 51018, 708, 2314, 498, 264, 2539, 3314, 307, 886, 2416, 30, 51190, 51190, 1692, 311, 1071, 4295, 295, 264, 3792, 2445, 11, 293, 718, 311, 584, 321, 722, 16235, 23475, 365, 51412, 51412, 343, 412, 341, 2158, 510, 13, 51570, 51570, 407, 11, 309, 311, 767, 1217, 1238, 1998, 281, 264, 7285, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1805858782359532, "compression_ratio": 1.6626984126984128, "no_speech_prob": 4.936925051879371e-06}, {"id": 33, "seek": 12792, "start": 152.04, "end": 156.68, "text": " So, it's actually already pretty close to the minimum.", "tokens": [50364, 467, 486, 747, 257, 588, 938, 565, 11, 570, 309, 311, 516, 281, 747, 613, 5870, 11, 5870, 3186, 4439, 11, 50614, 50614, 293, 309, 311, 516, 281, 643, 257, 688, 295, 4439, 949, 309, 2170, 4992, 1998, 281, 264, 7285, 13, 50814, 50814, 823, 11, 718, 311, 574, 412, 257, 819, 1389, 13, 51018, 51018, 708, 2314, 498, 264, 2539, 3314, 307, 886, 2416, 30, 51190, 51190, 1692, 311, 1071, 4295, 295, 264, 3792, 2445, 11, 293, 718, 311, 584, 321, 722, 16235, 23475, 365, 51412, 51412, 343, 412, 341, 2158, 510, 13, 51570, 51570, 407, 11, 309, 311, 767, 1217, 1238, 1998, 281, 264, 7285, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1805858782359532, "compression_ratio": 1.6626984126984128, "no_speech_prob": 4.936925051879371e-06}, {"id": 34, "seek": 15668, "start": 156.68, "end": 165.20000000000002, "text": " So, the derivative points to the right, but if the learning rate is too large, then you", "tokens": [50364, 407, 11, 264, 13760, 2793, 281, 264, 558, 11, 457, 498, 264, 2539, 3314, 307, 886, 2416, 11, 550, 291, 50790, 50790, 5623, 343, 5766, 257, 7410, 1823, 281, 312, 439, 264, 636, 670, 510, 11, 293, 300, 311, 341, 935, 510, 322, 264, 51258, 51258, 2445, 361, 13, 51308, 51308, 407, 11, 291, 1286, 490, 341, 935, 322, 264, 1411, 439, 264, 636, 281, 341, 935, 322, 264, 558, 11, 293, 586, 51644, 51644, 264, 2063, 575, 767, 5768, 5324, 13, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.1162518678709518, "compression_ratio": 1.701086956521739, "no_speech_prob": 4.356807949079666e-06}, {"id": 35, "seek": 15668, "start": 165.20000000000002, "end": 174.56, "text": " update W via a giant step to be all the way over here, and that's this point here on the", "tokens": [50364, 407, 11, 264, 13760, 2793, 281, 264, 558, 11, 457, 498, 264, 2539, 3314, 307, 886, 2416, 11, 550, 291, 50790, 50790, 5623, 343, 5766, 257, 7410, 1823, 281, 312, 439, 264, 636, 670, 510, 11, 293, 300, 311, 341, 935, 510, 322, 264, 51258, 51258, 2445, 361, 13, 51308, 51308, 407, 11, 291, 1286, 490, 341, 935, 322, 264, 1411, 439, 264, 636, 281, 341, 935, 322, 264, 558, 11, 293, 586, 51644, 51644, 264, 2063, 575, 767, 5768, 5324, 13, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.1162518678709518, "compression_ratio": 1.701086956521739, "no_speech_prob": 4.356807949079666e-06}, {"id": 36, "seek": 15668, "start": 174.56, "end": 175.56, "text": " function j.", "tokens": [50364, 407, 11, 264, 13760, 2793, 281, 264, 558, 11, 457, 498, 264, 2539, 3314, 307, 886, 2416, 11, 550, 291, 50790, 50790, 5623, 343, 5766, 257, 7410, 1823, 281, 312, 439, 264, 636, 670, 510, 11, 293, 300, 311, 341, 935, 510, 322, 264, 51258, 51258, 2445, 361, 13, 51308, 51308, 407, 11, 291, 1286, 490, 341, 935, 322, 264, 1411, 439, 264, 636, 281, 341, 935, 322, 264, 558, 11, 293, 586, 51644, 51644, 264, 2063, 575, 767, 5768, 5324, 13, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.1162518678709518, "compression_ratio": 1.701086956521739, "no_speech_prob": 4.356807949079666e-06}, {"id": 37, "seek": 15668, "start": 175.56, "end": 182.28, "text": " So, you move from this point on the left all the way to this point on the right, and now", "tokens": [50364, 407, 11, 264, 13760, 2793, 281, 264, 558, 11, 457, 498, 264, 2539, 3314, 307, 886, 2416, 11, 550, 291, 50790, 50790, 5623, 343, 5766, 257, 7410, 1823, 281, 312, 439, 264, 636, 670, 510, 11, 293, 300, 311, 341, 935, 510, 322, 264, 51258, 51258, 2445, 361, 13, 51308, 51308, 407, 11, 291, 1286, 490, 341, 935, 322, 264, 1411, 439, 264, 636, 281, 341, 935, 322, 264, 558, 11, 293, 586, 51644, 51644, 264, 2063, 575, 767, 5768, 5324, 13, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.1162518678709518, "compression_ratio": 1.701086956521739, "no_speech_prob": 4.356807949079666e-06}, {"id": 38, "seek": 15668, "start": 182.28, "end": 184.4, "text": " the cost has actually gotten worse.", "tokens": [50364, 407, 11, 264, 13760, 2793, 281, 264, 558, 11, 457, 498, 264, 2539, 3314, 307, 886, 2416, 11, 550, 291, 50790, 50790, 5623, 343, 5766, 257, 7410, 1823, 281, 312, 439, 264, 636, 670, 510, 11, 293, 300, 311, 341, 935, 510, 322, 264, 51258, 51258, 2445, 361, 13, 51308, 51308, 407, 11, 291, 1286, 490, 341, 935, 322, 264, 1411, 439, 264, 636, 281, 341, 935, 322, 264, 558, 11, 293, 586, 51644, 51644, 264, 2063, 575, 767, 5768, 5324, 13, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.1162518678709518, "compression_ratio": 1.701086956521739, "no_speech_prob": 4.356807949079666e-06}, {"id": 39, "seek": 18440, "start": 184.4, "end": 190.24, "text": " It has increased because it has started out at this value here, and after one step, it", "tokens": [50364, 467, 575, 6505, 570, 309, 575, 1409, 484, 412, 341, 2158, 510, 11, 293, 934, 472, 1823, 11, 309, 50656, 50656, 767, 6505, 281, 341, 2158, 510, 13, 50808, 50808, 823, 11, 264, 13760, 412, 341, 777, 935, 1619, 281, 11514, 343, 11, 457, 562, 264, 2539, 3314, 51188, 51188, 307, 886, 955, 11, 550, 291, 815, 747, 257, 2603, 1823, 516, 490, 510, 439, 264, 636, 484, 510, 13, 51506, 51506, 407, 11, 586, 291, 600, 5768, 281, 341, 935, 510, 11, 293, 797, 11, 498, 264, 2539, 3314, 307, 886, 955, 11, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.13604208887839803, "compression_ratio": 1.8076923076923077, "no_speech_prob": 2.6425464056956116e-06}, {"id": 40, "seek": 18440, "start": 190.24, "end": 193.28, "text": " actually increased to this value here.", "tokens": [50364, 467, 575, 6505, 570, 309, 575, 1409, 484, 412, 341, 2158, 510, 11, 293, 934, 472, 1823, 11, 309, 50656, 50656, 767, 6505, 281, 341, 2158, 510, 13, 50808, 50808, 823, 11, 264, 13760, 412, 341, 777, 935, 1619, 281, 11514, 343, 11, 457, 562, 264, 2539, 3314, 51188, 51188, 307, 886, 955, 11, 550, 291, 815, 747, 257, 2603, 1823, 516, 490, 510, 439, 264, 636, 484, 510, 13, 51506, 51506, 407, 11, 586, 291, 600, 5768, 281, 341, 935, 510, 11, 293, 797, 11, 498, 264, 2539, 3314, 307, 886, 955, 11, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.13604208887839803, "compression_ratio": 1.8076923076923077, "no_speech_prob": 2.6425464056956116e-06}, {"id": 41, "seek": 18440, "start": 193.28, "end": 200.88, "text": " Now, the derivative at this new point says to decrease W, but when the learning rate", "tokens": [50364, 467, 575, 6505, 570, 309, 575, 1409, 484, 412, 341, 2158, 510, 11, 293, 934, 472, 1823, 11, 309, 50656, 50656, 767, 6505, 281, 341, 2158, 510, 13, 50808, 50808, 823, 11, 264, 13760, 412, 341, 777, 935, 1619, 281, 11514, 343, 11, 457, 562, 264, 2539, 3314, 51188, 51188, 307, 886, 955, 11, 550, 291, 815, 747, 257, 2603, 1823, 516, 490, 510, 439, 264, 636, 484, 510, 13, 51506, 51506, 407, 11, 586, 291, 600, 5768, 281, 341, 935, 510, 11, 293, 797, 11, 498, 264, 2539, 3314, 307, 886, 955, 11, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.13604208887839803, "compression_ratio": 1.8076923076923077, "no_speech_prob": 2.6425464056956116e-06}, {"id": 42, "seek": 18440, "start": 200.88, "end": 207.24, "text": " is too big, then you may take a huge step going from here all the way out here.", "tokens": [50364, 467, 575, 6505, 570, 309, 575, 1409, 484, 412, 341, 2158, 510, 11, 293, 934, 472, 1823, 11, 309, 50656, 50656, 767, 6505, 281, 341, 2158, 510, 13, 50808, 50808, 823, 11, 264, 13760, 412, 341, 777, 935, 1619, 281, 11514, 343, 11, 457, 562, 264, 2539, 3314, 51188, 51188, 307, 886, 955, 11, 550, 291, 815, 747, 257, 2603, 1823, 516, 490, 510, 439, 264, 636, 484, 510, 13, 51506, 51506, 407, 11, 586, 291, 600, 5768, 281, 341, 935, 510, 11, 293, 797, 11, 498, 264, 2539, 3314, 307, 886, 955, 11, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.13604208887839803, "compression_ratio": 1.8076923076923077, "no_speech_prob": 2.6425464056956116e-06}, {"id": 43, "seek": 18440, "start": 207.24, "end": 213.64000000000001, "text": " So, now you've gotten to this point here, and again, if the learning rate is too big,", "tokens": [50364, 467, 575, 6505, 570, 309, 575, 1409, 484, 412, 341, 2158, 510, 11, 293, 934, 472, 1823, 11, 309, 50656, 50656, 767, 6505, 281, 341, 2158, 510, 13, 50808, 50808, 823, 11, 264, 13760, 412, 341, 777, 935, 1619, 281, 11514, 343, 11, 457, 562, 264, 2539, 3314, 51188, 51188, 307, 886, 955, 11, 550, 291, 815, 747, 257, 2603, 1823, 516, 490, 510, 439, 264, 636, 484, 510, 13, 51506, 51506, 407, 11, 586, 291, 600, 5768, 281, 341, 935, 510, 11, 293, 797, 11, 498, 264, 2539, 3314, 307, 886, 955, 11, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.13604208887839803, "compression_ratio": 1.8076923076923077, "no_speech_prob": 2.6425464056956116e-06}, {"id": 44, "seek": 21364, "start": 213.64, "end": 218.6, "text": " then you take another huge step at the next iteration and way overshoot the minimum again.", "tokens": [50364, 550, 291, 747, 1071, 2603, 1823, 412, 264, 958, 24784, 293, 636, 15488, 24467, 264, 7285, 797, 13, 50612, 50612, 407, 11, 586, 291, 434, 412, 341, 935, 322, 264, 558, 11, 293, 472, 544, 565, 11, 291, 360, 1071, 5623, 293, 50898, 50898, 917, 493, 439, 264, 636, 510, 11, 293, 370, 291, 434, 586, 412, 341, 935, 510, 13, 51202, 51202, 407, 11, 382, 291, 815, 3449, 11, 291, 434, 767, 1242, 3052, 293, 3052, 1314, 490, 264, 7285, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13803517541220023, "compression_ratio": 1.7700534759358288, "no_speech_prob": 7.76671186031308e-06}, {"id": 45, "seek": 21364, "start": 218.6, "end": 224.32, "text": " So, now you're at this point on the right, and one more time, you do another update and", "tokens": [50364, 550, 291, 747, 1071, 2603, 1823, 412, 264, 958, 24784, 293, 636, 15488, 24467, 264, 7285, 797, 13, 50612, 50612, 407, 11, 586, 291, 434, 412, 341, 935, 322, 264, 558, 11, 293, 472, 544, 565, 11, 291, 360, 1071, 5623, 293, 50898, 50898, 917, 493, 439, 264, 636, 510, 11, 293, 370, 291, 434, 586, 412, 341, 935, 510, 13, 51202, 51202, 407, 11, 382, 291, 815, 3449, 11, 291, 434, 767, 1242, 3052, 293, 3052, 1314, 490, 264, 7285, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13803517541220023, "compression_ratio": 1.7700534759358288, "no_speech_prob": 7.76671186031308e-06}, {"id": 46, "seek": 21364, "start": 224.32, "end": 230.39999999999998, "text": " end up all the way here, and so you're now at this point here.", "tokens": [50364, 550, 291, 747, 1071, 2603, 1823, 412, 264, 958, 24784, 293, 636, 15488, 24467, 264, 7285, 797, 13, 50612, 50612, 407, 11, 586, 291, 434, 412, 341, 935, 322, 264, 558, 11, 293, 472, 544, 565, 11, 291, 360, 1071, 5623, 293, 50898, 50898, 917, 493, 439, 264, 636, 510, 11, 293, 370, 291, 434, 586, 412, 341, 935, 510, 13, 51202, 51202, 407, 11, 382, 291, 815, 3449, 11, 291, 434, 767, 1242, 3052, 293, 3052, 1314, 490, 264, 7285, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13803517541220023, "compression_ratio": 1.7700534759358288, "no_speech_prob": 7.76671186031308e-06}, {"id": 47, "seek": 21364, "start": 230.39999999999998, "end": 236.64, "text": " So, as you may notice, you're actually getting further and further away from the minimum.", "tokens": [50364, 550, 291, 747, 1071, 2603, 1823, 412, 264, 958, 24784, 293, 636, 15488, 24467, 264, 7285, 797, 13, 50612, 50612, 407, 11, 586, 291, 434, 412, 341, 935, 322, 264, 558, 11, 293, 472, 544, 565, 11, 291, 360, 1071, 5623, 293, 50898, 50898, 917, 493, 439, 264, 636, 510, 11, 293, 370, 291, 434, 586, 412, 341, 935, 510, 13, 51202, 51202, 407, 11, 382, 291, 815, 3449, 11, 291, 434, 767, 1242, 3052, 293, 3052, 1314, 490, 264, 7285, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13803517541220023, "compression_ratio": 1.7700534759358288, "no_speech_prob": 7.76671186031308e-06}, {"id": 48, "seek": 23664, "start": 236.64, "end": 243.83999999999997, "text": " So, if the learning rate is too large, then gradient descent may overshoot and may never", "tokens": [50364, 407, 11, 498, 264, 2539, 3314, 307, 886, 2416, 11, 550, 16235, 23475, 815, 15488, 24467, 293, 815, 1128, 50724, 50724, 2524, 264, 7285, 13, 50836, 50836, 400, 1071, 636, 281, 584, 300, 307, 300, 16235, 23475, 815, 3061, 281, 41881, 293, 815, 754, 51174, 51174, 18558, 432, 13, 51224, 51224, 407, 11, 510, 311, 1071, 1168, 291, 815, 312, 6359, 13, 51498, 51498], "temperature": 0.0, "avg_logprob": -0.125890775160356, "compression_ratio": 1.6114649681528663, "no_speech_prob": 3.5559289699449437e-06}, {"id": 49, "seek": 23664, "start": 243.83999999999997, "end": 246.07999999999998, "text": " reach the minimum.", "tokens": [50364, 407, 11, 498, 264, 2539, 3314, 307, 886, 2416, 11, 550, 16235, 23475, 815, 15488, 24467, 293, 815, 1128, 50724, 50724, 2524, 264, 7285, 13, 50836, 50836, 400, 1071, 636, 281, 584, 300, 307, 300, 16235, 23475, 815, 3061, 281, 41881, 293, 815, 754, 51174, 51174, 18558, 432, 13, 51224, 51224, 407, 11, 510, 311, 1071, 1168, 291, 815, 312, 6359, 13, 51498, 51498], "temperature": 0.0, "avg_logprob": -0.125890775160356, "compression_ratio": 1.6114649681528663, "no_speech_prob": 3.5559289699449437e-06}, {"id": 50, "seek": 23664, "start": 246.07999999999998, "end": 252.83999999999997, "text": " And another way to say that is that gradient descent may fail to converge and may even", "tokens": [50364, 407, 11, 498, 264, 2539, 3314, 307, 886, 2416, 11, 550, 16235, 23475, 815, 15488, 24467, 293, 815, 1128, 50724, 50724, 2524, 264, 7285, 13, 50836, 50836, 400, 1071, 636, 281, 584, 300, 307, 300, 16235, 23475, 815, 3061, 281, 41881, 293, 815, 754, 51174, 51174, 18558, 432, 13, 51224, 51224, 407, 11, 510, 311, 1071, 1168, 291, 815, 312, 6359, 13, 51498, 51498], "temperature": 0.0, "avg_logprob": -0.125890775160356, "compression_ratio": 1.6114649681528663, "no_speech_prob": 3.5559289699449437e-06}, {"id": 51, "seek": 23664, "start": 252.83999999999997, "end": 253.83999999999997, "text": " diverge.", "tokens": [50364, 407, 11, 498, 264, 2539, 3314, 307, 886, 2416, 11, 550, 16235, 23475, 815, 15488, 24467, 293, 815, 1128, 50724, 50724, 2524, 264, 7285, 13, 50836, 50836, 400, 1071, 636, 281, 584, 300, 307, 300, 16235, 23475, 815, 3061, 281, 41881, 293, 815, 754, 51174, 51174, 18558, 432, 13, 51224, 51224, 407, 11, 510, 311, 1071, 1168, 291, 815, 312, 6359, 13, 51498, 51498], "temperature": 0.0, "avg_logprob": -0.125890775160356, "compression_ratio": 1.6114649681528663, "no_speech_prob": 3.5559289699449437e-06}, {"id": 52, "seek": 23664, "start": 253.83999999999997, "end": 259.32, "text": " So, here's another question you may be wondering.", "tokens": [50364, 407, 11, 498, 264, 2539, 3314, 307, 886, 2416, 11, 550, 16235, 23475, 815, 15488, 24467, 293, 815, 1128, 50724, 50724, 2524, 264, 7285, 13, 50836, 50836, 400, 1071, 636, 281, 584, 300, 307, 300, 16235, 23475, 815, 3061, 281, 41881, 293, 815, 754, 51174, 51174, 18558, 432, 13, 51224, 51224, 407, 11, 510, 311, 1071, 1168, 291, 815, 312, 6359, 13, 51498, 51498], "temperature": 0.0, "avg_logprob": -0.125890775160356, "compression_ratio": 1.6114649681528663, "no_speech_prob": 3.5559289699449437e-06}, {"id": 53, "seek": 25932, "start": 259.32, "end": 266.64, "text": " What if your parameter W is already at this point here, so that your cost J is already", "tokens": [50364, 708, 498, 428, 13075, 343, 307, 1217, 412, 341, 935, 510, 11, 370, 300, 428, 2063, 508, 307, 1217, 50730, 50730, 412, 257, 2654, 7285, 30, 50878, 50878, 708, 360, 291, 519, 472, 1823, 295, 16235, 23475, 486, 360, 498, 291, 600, 1217, 6488, 257, 7285, 30, 51184, 51184, 407, 11, 341, 307, 257, 12414, 472, 13, 51354, 51354, 1133, 286, 390, 700, 2539, 341, 1507, 11, 309, 767, 1890, 385, 257, 938, 565, 281, 2573, 309, 484, 11, 457, 51594, 51594, 718, 311, 589, 807, 341, 1214, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.13557156183386362, "compression_ratio": 1.6055045871559632, "no_speech_prob": 1.3496961628334248e-06}, {"id": 54, "seek": 25932, "start": 266.64, "end": 269.59999999999997, "text": " at a local minimum?", "tokens": [50364, 708, 498, 428, 13075, 343, 307, 1217, 412, 341, 935, 510, 11, 370, 300, 428, 2063, 508, 307, 1217, 50730, 50730, 412, 257, 2654, 7285, 30, 50878, 50878, 708, 360, 291, 519, 472, 1823, 295, 16235, 23475, 486, 360, 498, 291, 600, 1217, 6488, 257, 7285, 30, 51184, 51184, 407, 11, 341, 307, 257, 12414, 472, 13, 51354, 51354, 1133, 286, 390, 700, 2539, 341, 1507, 11, 309, 767, 1890, 385, 257, 938, 565, 281, 2573, 309, 484, 11, 457, 51594, 51594, 718, 311, 589, 807, 341, 1214, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.13557156183386362, "compression_ratio": 1.6055045871559632, "no_speech_prob": 1.3496961628334248e-06}, {"id": 55, "seek": 25932, "start": 269.59999999999997, "end": 275.71999999999997, "text": " What do you think one step of gradient descent will do if you've already reached a minimum?", "tokens": [50364, 708, 498, 428, 13075, 343, 307, 1217, 412, 341, 935, 510, 11, 370, 300, 428, 2063, 508, 307, 1217, 50730, 50730, 412, 257, 2654, 7285, 30, 50878, 50878, 708, 360, 291, 519, 472, 1823, 295, 16235, 23475, 486, 360, 498, 291, 600, 1217, 6488, 257, 7285, 30, 51184, 51184, 407, 11, 341, 307, 257, 12414, 472, 13, 51354, 51354, 1133, 286, 390, 700, 2539, 341, 1507, 11, 309, 767, 1890, 385, 257, 938, 565, 281, 2573, 309, 484, 11, 457, 51594, 51594, 718, 311, 589, 807, 341, 1214, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.13557156183386362, "compression_ratio": 1.6055045871559632, "no_speech_prob": 1.3496961628334248e-06}, {"id": 56, "seek": 25932, "start": 275.71999999999997, "end": 279.12, "text": " So, this is a tricky one.", "tokens": [50364, 708, 498, 428, 13075, 343, 307, 1217, 412, 341, 935, 510, 11, 370, 300, 428, 2063, 508, 307, 1217, 50730, 50730, 412, 257, 2654, 7285, 30, 50878, 50878, 708, 360, 291, 519, 472, 1823, 295, 16235, 23475, 486, 360, 498, 291, 600, 1217, 6488, 257, 7285, 30, 51184, 51184, 407, 11, 341, 307, 257, 12414, 472, 13, 51354, 51354, 1133, 286, 390, 700, 2539, 341, 1507, 11, 309, 767, 1890, 385, 257, 938, 565, 281, 2573, 309, 484, 11, 457, 51594, 51594, 718, 311, 589, 807, 341, 1214, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.13557156183386362, "compression_ratio": 1.6055045871559632, "no_speech_prob": 1.3496961628334248e-06}, {"id": 57, "seek": 25932, "start": 279.12, "end": 283.92, "text": " When I was first learning this stuff, it actually took me a long time to figure it out, but", "tokens": [50364, 708, 498, 428, 13075, 343, 307, 1217, 412, 341, 935, 510, 11, 370, 300, 428, 2063, 508, 307, 1217, 50730, 50730, 412, 257, 2654, 7285, 30, 50878, 50878, 708, 360, 291, 519, 472, 1823, 295, 16235, 23475, 486, 360, 498, 291, 600, 1217, 6488, 257, 7285, 30, 51184, 51184, 407, 11, 341, 307, 257, 12414, 472, 13, 51354, 51354, 1133, 286, 390, 700, 2539, 341, 1507, 11, 309, 767, 1890, 385, 257, 938, 565, 281, 2573, 309, 484, 11, 457, 51594, 51594, 718, 311, 589, 807, 341, 1214, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.13557156183386362, "compression_ratio": 1.6055045871559632, "no_speech_prob": 1.3496961628334248e-06}, {"id": 58, "seek": 25932, "start": 283.92, "end": 286.12, "text": " let's work through this together.", "tokens": [50364, 708, 498, 428, 13075, 343, 307, 1217, 412, 341, 935, 510, 11, 370, 300, 428, 2063, 508, 307, 1217, 50730, 50730, 412, 257, 2654, 7285, 30, 50878, 50878, 708, 360, 291, 519, 472, 1823, 295, 16235, 23475, 486, 360, 498, 291, 600, 1217, 6488, 257, 7285, 30, 51184, 51184, 407, 11, 341, 307, 257, 12414, 472, 13, 51354, 51354, 1133, 286, 390, 700, 2539, 341, 1507, 11, 309, 767, 1890, 385, 257, 938, 565, 281, 2573, 309, 484, 11, 457, 51594, 51594, 718, 311, 589, 807, 341, 1214, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.13557156183386362, "compression_ratio": 1.6055045871559632, "no_speech_prob": 1.3496961628334248e-06}, {"id": 59, "seek": 28612, "start": 286.12, "end": 293.08, "text": " Let's suppose you have some cost function J, and the one you see here isn't a square", "tokens": [50364, 961, 311, 7297, 291, 362, 512, 2063, 2445, 508, 11, 293, 264, 472, 291, 536, 510, 1943, 380, 257, 3732, 50712, 50712, 6713, 2063, 2445, 11, 293, 341, 2063, 2445, 575, 732, 2654, 4464, 64, 11760, 281, 264, 51014, 51014, 732, 4190, 300, 291, 536, 510, 13, 51184, 51184, 823, 11, 718, 311, 7297, 300, 934, 512, 1230, 295, 4439, 295, 16235, 23475, 11, 428, 13075, 51488, 51488], "temperature": 0.0, "avg_logprob": -0.15149414879935127, "compression_ratio": 1.688235294117647, "no_speech_prob": 2.5612728222768055e-06}, {"id": 60, "seek": 28612, "start": 293.08, "end": 299.12, "text": " error cost function, and this cost function has two local minima corresponding to the", "tokens": [50364, 961, 311, 7297, 291, 362, 512, 2063, 2445, 508, 11, 293, 264, 472, 291, 536, 510, 1943, 380, 257, 3732, 50712, 50712, 6713, 2063, 2445, 11, 293, 341, 2063, 2445, 575, 732, 2654, 4464, 64, 11760, 281, 264, 51014, 51014, 732, 4190, 300, 291, 536, 510, 13, 51184, 51184, 823, 11, 718, 311, 7297, 300, 934, 512, 1230, 295, 4439, 295, 16235, 23475, 11, 428, 13075, 51488, 51488], "temperature": 0.0, "avg_logprob": -0.15149414879935127, "compression_ratio": 1.688235294117647, "no_speech_prob": 2.5612728222768055e-06}, {"id": 61, "seek": 28612, "start": 299.12, "end": 302.52, "text": " two values that you see here.", "tokens": [50364, 961, 311, 7297, 291, 362, 512, 2063, 2445, 508, 11, 293, 264, 472, 291, 536, 510, 1943, 380, 257, 3732, 50712, 50712, 6713, 2063, 2445, 11, 293, 341, 2063, 2445, 575, 732, 2654, 4464, 64, 11760, 281, 264, 51014, 51014, 732, 4190, 300, 291, 536, 510, 13, 51184, 51184, 823, 11, 718, 311, 7297, 300, 934, 512, 1230, 295, 4439, 295, 16235, 23475, 11, 428, 13075, 51488, 51488], "temperature": 0.0, "avg_logprob": -0.15149414879935127, "compression_ratio": 1.688235294117647, "no_speech_prob": 2.5612728222768055e-06}, {"id": 62, "seek": 28612, "start": 302.52, "end": 308.6, "text": " Now, let's suppose that after some number of steps of gradient descent, your parameter", "tokens": [50364, 961, 311, 7297, 291, 362, 512, 2063, 2445, 508, 11, 293, 264, 472, 291, 536, 510, 1943, 380, 257, 3732, 50712, 50712, 6713, 2063, 2445, 11, 293, 341, 2063, 2445, 575, 732, 2654, 4464, 64, 11760, 281, 264, 51014, 51014, 732, 4190, 300, 291, 536, 510, 13, 51184, 51184, 823, 11, 718, 311, 7297, 300, 934, 512, 1230, 295, 4439, 295, 16235, 23475, 11, 428, 13075, 51488, 51488], "temperature": 0.0, "avg_logprob": -0.15149414879935127, "compression_ratio": 1.688235294117647, "no_speech_prob": 2.5612728222768055e-06}, {"id": 63, "seek": 30860, "start": 308.6, "end": 317.36, "text": " W is over here, say equal to 5, and so this is the current value of W. This means that", "tokens": [50364, 343, 307, 670, 510, 11, 584, 2681, 281, 1025, 11, 293, 370, 341, 307, 264, 2190, 2158, 295, 343, 13, 639, 1355, 300, 50802, 50802, 291, 434, 412, 341, 935, 322, 264, 2063, 2445, 508, 11, 293, 300, 2314, 281, 312, 257, 2654, 7285, 13, 51130, 51130, 29524, 484, 11, 498, 291, 2642, 257, 27747, 281, 264, 2445, 412, 341, 935, 11, 264, 13525, 295, 341, 1622, 307, 4018, 11, 51476, 51476, 293, 8807, 11, 264, 13760, 1433, 510, 307, 2681, 281, 4018, 337, 264, 2190, 2158, 295, 343, 11, 293, 370, 51823, 51823], "temperature": 0.0, "avg_logprob": -0.11020657420158386, "compression_ratio": 1.7524752475247525, "no_speech_prob": 1.6797251873867936e-06}, {"id": 64, "seek": 30860, "start": 317.36, "end": 323.92, "text": " you're at this point on the cost function J, and that happens to be a local minimum.", "tokens": [50364, 343, 307, 670, 510, 11, 584, 2681, 281, 1025, 11, 293, 370, 341, 307, 264, 2190, 2158, 295, 343, 13, 639, 1355, 300, 50802, 50802, 291, 434, 412, 341, 935, 322, 264, 2063, 2445, 508, 11, 293, 300, 2314, 281, 312, 257, 2654, 7285, 13, 51130, 51130, 29524, 484, 11, 498, 291, 2642, 257, 27747, 281, 264, 2445, 412, 341, 935, 11, 264, 13525, 295, 341, 1622, 307, 4018, 11, 51476, 51476, 293, 8807, 11, 264, 13760, 1433, 510, 307, 2681, 281, 4018, 337, 264, 2190, 2158, 295, 343, 11, 293, 370, 51823, 51823], "temperature": 0.0, "avg_logprob": -0.11020657420158386, "compression_ratio": 1.7524752475247525, "no_speech_prob": 1.6797251873867936e-06}, {"id": 65, "seek": 30860, "start": 323.92, "end": 330.84000000000003, "text": " Turns out, if you draw a tangent to the function at this point, the slope of this line is zero,", "tokens": [50364, 343, 307, 670, 510, 11, 584, 2681, 281, 1025, 11, 293, 370, 341, 307, 264, 2190, 2158, 295, 343, 13, 639, 1355, 300, 50802, 50802, 291, 434, 412, 341, 935, 322, 264, 2063, 2445, 508, 11, 293, 300, 2314, 281, 312, 257, 2654, 7285, 13, 51130, 51130, 29524, 484, 11, 498, 291, 2642, 257, 27747, 281, 264, 2445, 412, 341, 935, 11, 264, 13525, 295, 341, 1622, 307, 4018, 11, 51476, 51476, 293, 8807, 11, 264, 13760, 1433, 510, 307, 2681, 281, 4018, 337, 264, 2190, 2158, 295, 343, 11, 293, 370, 51823, 51823], "temperature": 0.0, "avg_logprob": -0.11020657420158386, "compression_ratio": 1.7524752475247525, "no_speech_prob": 1.6797251873867936e-06}, {"id": 66, "seek": 30860, "start": 330.84000000000003, "end": 337.78000000000003, "text": " and thus, the derivative term here is equal to zero for the current value of W, and so", "tokens": [50364, 343, 307, 670, 510, 11, 584, 2681, 281, 1025, 11, 293, 370, 341, 307, 264, 2190, 2158, 295, 343, 13, 639, 1355, 300, 50802, 50802, 291, 434, 412, 341, 935, 322, 264, 2063, 2445, 508, 11, 293, 300, 2314, 281, 312, 257, 2654, 7285, 13, 51130, 51130, 29524, 484, 11, 498, 291, 2642, 257, 27747, 281, 264, 2445, 412, 341, 935, 11, 264, 13525, 295, 341, 1622, 307, 4018, 11, 51476, 51476, 293, 8807, 11, 264, 13760, 1433, 510, 307, 2681, 281, 4018, 337, 264, 2190, 2158, 295, 343, 11, 293, 370, 51823, 51823], "temperature": 0.0, "avg_logprob": -0.11020657420158386, "compression_ratio": 1.7524752475247525, "no_speech_prob": 1.6797251873867936e-06}, {"id": 67, "seek": 33778, "start": 337.78, "end": 344.67999999999995, "text": " your gradient descent update becomes W is updated to W minus the learning rate times", "tokens": [50364, 428, 16235, 23475, 5623, 3643, 343, 307, 10588, 281, 343, 3175, 264, 2539, 3314, 1413, 50709, 50709, 4018, 11, 689, 510, 11, 300, 311, 570, 264, 13760, 1433, 307, 2681, 281, 4018, 13, 51025, 51025, 400, 341, 307, 264, 912, 382, 1566, 11, 718, 311, 992, 343, 281, 312, 2681, 281, 343, 13, 407, 11, 341, 1355, 300, 498, 51417, 51417, 291, 434, 1217, 412, 257, 2654, 7285, 11, 16235, 23475, 5510, 343, 44553, 570, 309, 445, 51715, 51715], "temperature": 0.0, "avg_logprob": -0.11840759088963639, "compression_ratio": 1.6108374384236452, "no_speech_prob": 1.39253734232625e-06}, {"id": 68, "seek": 33778, "start": 344.67999999999995, "end": 351.0, "text": " zero, where here, that's because the derivative term is equal to zero.", "tokens": [50364, 428, 16235, 23475, 5623, 3643, 343, 307, 10588, 281, 343, 3175, 264, 2539, 3314, 1413, 50709, 50709, 4018, 11, 689, 510, 11, 300, 311, 570, 264, 13760, 1433, 307, 2681, 281, 4018, 13, 51025, 51025, 400, 341, 307, 264, 912, 382, 1566, 11, 718, 311, 992, 343, 281, 312, 2681, 281, 343, 13, 407, 11, 341, 1355, 300, 498, 51417, 51417, 291, 434, 1217, 412, 257, 2654, 7285, 11, 16235, 23475, 5510, 343, 44553, 570, 309, 445, 51715, 51715], "temperature": 0.0, "avg_logprob": -0.11840759088963639, "compression_ratio": 1.6108374384236452, "no_speech_prob": 1.39253734232625e-06}, {"id": 69, "seek": 33778, "start": 351.0, "end": 358.84, "text": " And this is the same as saying, let's set W to be equal to W. So, this means that if", "tokens": [50364, 428, 16235, 23475, 5623, 3643, 343, 307, 10588, 281, 343, 3175, 264, 2539, 3314, 1413, 50709, 50709, 4018, 11, 689, 510, 11, 300, 311, 570, 264, 13760, 1433, 307, 2681, 281, 4018, 13, 51025, 51025, 400, 341, 307, 264, 912, 382, 1566, 11, 718, 311, 992, 343, 281, 312, 2681, 281, 343, 13, 407, 11, 341, 1355, 300, 498, 51417, 51417, 291, 434, 1217, 412, 257, 2654, 7285, 11, 16235, 23475, 5510, 343, 44553, 570, 309, 445, 51715, 51715], "temperature": 0.0, "avg_logprob": -0.11840759088963639, "compression_ratio": 1.6108374384236452, "no_speech_prob": 1.39253734232625e-06}, {"id": 70, "seek": 33778, "start": 358.84, "end": 364.79999999999995, "text": " you're already at a local minimum, gradient descent leaves W unchanged because it just", "tokens": [50364, 428, 16235, 23475, 5623, 3643, 343, 307, 10588, 281, 343, 3175, 264, 2539, 3314, 1413, 50709, 50709, 4018, 11, 689, 510, 11, 300, 311, 570, 264, 13760, 1433, 307, 2681, 281, 4018, 13, 51025, 51025, 400, 341, 307, 264, 912, 382, 1566, 11, 718, 311, 992, 343, 281, 312, 2681, 281, 343, 13, 407, 11, 341, 1355, 300, 498, 51417, 51417, 291, 434, 1217, 412, 257, 2654, 7285, 11, 16235, 23475, 5510, 343, 44553, 570, 309, 445, 51715, 51715], "temperature": 0.0, "avg_logprob": -0.11840759088963639, "compression_ratio": 1.6108374384236452, "no_speech_prob": 1.39253734232625e-06}, {"id": 71, "seek": 36480, "start": 364.8, "end": 372.32, "text": " updates the new value of W to be the exact same old value of W. So concretely, let's", "tokens": [50364, 9205, 264, 777, 2158, 295, 343, 281, 312, 264, 1900, 912, 1331, 2158, 295, 343, 13, 407, 39481, 736, 11, 718, 311, 50740, 50740, 584, 498, 264, 2190, 2158, 295, 343, 307, 1025, 11, 293, 8961, 307, 1958, 13, 16, 11, 934, 472, 24784, 11, 291, 5623, 343, 51200, 51200, 382, 343, 3175, 8961, 1413, 4018, 11, 293, 309, 307, 920, 2681, 281, 1025, 13, 51550, 51550], "temperature": 0.0, "avg_logprob": -0.1553167813066123, "compression_ratio": 1.4774193548387098, "no_speech_prob": 5.453240419228678e-07}, {"id": 72, "seek": 36480, "start": 372.32, "end": 381.52, "text": " say if the current value of W is 5, and alpha is 0.1, after one iteration, you update W", "tokens": [50364, 9205, 264, 777, 2158, 295, 343, 281, 312, 264, 1900, 912, 1331, 2158, 295, 343, 13, 407, 39481, 736, 11, 718, 311, 50740, 50740, 584, 498, 264, 2190, 2158, 295, 343, 307, 1025, 11, 293, 8961, 307, 1958, 13, 16, 11, 934, 472, 24784, 11, 291, 5623, 343, 51200, 51200, 382, 343, 3175, 8961, 1413, 4018, 11, 293, 309, 307, 920, 2681, 281, 1025, 13, 51550, 51550], "temperature": 0.0, "avg_logprob": -0.1553167813066123, "compression_ratio": 1.4774193548387098, "no_speech_prob": 5.453240419228678e-07}, {"id": 73, "seek": 36480, "start": 381.52, "end": 388.52, "text": " as W minus alpha times zero, and it is still equal to 5.", "tokens": [50364, 9205, 264, 777, 2158, 295, 343, 281, 312, 264, 1900, 912, 1331, 2158, 295, 343, 13, 407, 39481, 736, 11, 718, 311, 50740, 50740, 584, 498, 264, 2190, 2158, 295, 343, 307, 1025, 11, 293, 8961, 307, 1958, 13, 16, 11, 934, 472, 24784, 11, 291, 5623, 343, 51200, 51200, 382, 343, 3175, 8961, 1413, 4018, 11, 293, 309, 307, 920, 2681, 281, 1025, 13, 51550, 51550], "temperature": 0.0, "avg_logprob": -0.1553167813066123, "compression_ratio": 1.4774193548387098, "no_speech_prob": 5.453240419228678e-07}, {"id": 74, "seek": 38852, "start": 388.52, "end": 395.12, "text": " So, if your parameters have already brought you to a local minimum, then further gradient", "tokens": [50364, 407, 11, 498, 428, 9834, 362, 1217, 3038, 291, 281, 257, 2654, 7285, 11, 550, 3052, 16235, 50694, 50694, 23475, 4439, 360, 3122, 1825, 13, 467, 1177, 380, 1319, 264, 9834, 11, 597, 307, 437, 291, 528, 11, 50966, 50966, 570, 309, 5965, 264, 3827, 412, 300, 2654, 7285, 13, 639, 611, 13948, 983, 16235, 23475, 51258, 51258, 393, 2524, 257, 2654, 7285, 11, 754, 365, 257, 6806, 2539, 3314, 8961, 13, 1692, 311, 437, 286, 914, 13, 51606, 51606], "temperature": 0.0, "avg_logprob": -0.1108985993920303, "compression_ratio": 1.665137614678899, "no_speech_prob": 2.026132506216527e-06}, {"id": 75, "seek": 38852, "start": 395.12, "end": 400.56, "text": " descent steps do absolutely nothing. It doesn't change the parameters, which is what you want,", "tokens": [50364, 407, 11, 498, 428, 9834, 362, 1217, 3038, 291, 281, 257, 2654, 7285, 11, 550, 3052, 16235, 50694, 50694, 23475, 4439, 360, 3122, 1825, 13, 467, 1177, 380, 1319, 264, 9834, 11, 597, 307, 437, 291, 528, 11, 50966, 50966, 570, 309, 5965, 264, 3827, 412, 300, 2654, 7285, 13, 639, 611, 13948, 983, 16235, 23475, 51258, 51258, 393, 2524, 257, 2654, 7285, 11, 754, 365, 257, 6806, 2539, 3314, 8961, 13, 1692, 311, 437, 286, 914, 13, 51606, 51606], "temperature": 0.0, "avg_logprob": -0.1108985993920303, "compression_ratio": 1.665137614678899, "no_speech_prob": 2.026132506216527e-06}, {"id": 76, "seek": 38852, "start": 400.56, "end": 406.4, "text": " because it keeps the solution at that local minimum. This also explains why gradient descent", "tokens": [50364, 407, 11, 498, 428, 9834, 362, 1217, 3038, 291, 281, 257, 2654, 7285, 11, 550, 3052, 16235, 50694, 50694, 23475, 4439, 360, 3122, 1825, 13, 467, 1177, 380, 1319, 264, 9834, 11, 597, 307, 437, 291, 528, 11, 50966, 50966, 570, 309, 5965, 264, 3827, 412, 300, 2654, 7285, 13, 639, 611, 13948, 983, 16235, 23475, 51258, 51258, 393, 2524, 257, 2654, 7285, 11, 754, 365, 257, 6806, 2539, 3314, 8961, 13, 1692, 311, 437, 286, 914, 13, 51606, 51606], "temperature": 0.0, "avg_logprob": -0.1108985993920303, "compression_ratio": 1.665137614678899, "no_speech_prob": 2.026132506216527e-06}, {"id": 77, "seek": 38852, "start": 406.4, "end": 413.35999999999996, "text": " can reach a local minimum, even with a fixed learning rate alpha. Here's what I mean.", "tokens": [50364, 407, 11, 498, 428, 9834, 362, 1217, 3038, 291, 281, 257, 2654, 7285, 11, 550, 3052, 16235, 50694, 50694, 23475, 4439, 360, 3122, 1825, 13, 467, 1177, 380, 1319, 264, 9834, 11, 597, 307, 437, 291, 528, 11, 50966, 50966, 570, 309, 5965, 264, 3827, 412, 300, 2654, 7285, 13, 639, 611, 13948, 983, 16235, 23475, 51258, 51258, 393, 2524, 257, 2654, 7285, 11, 754, 365, 257, 6806, 2539, 3314, 8961, 13, 1692, 311, 437, 286, 914, 13, 51606, 51606], "temperature": 0.0, "avg_logprob": -0.1108985993920303, "compression_ratio": 1.665137614678899, "no_speech_prob": 2.026132506216527e-06}, {"id": 78, "seek": 41336, "start": 413.36, "end": 420.52000000000004, "text": " To illustrate this, let's look at another example. Here's the cost function J of W that", "tokens": [50364, 1407, 23221, 341, 11, 718, 311, 574, 412, 1071, 1365, 13, 1692, 311, 264, 2063, 2445, 508, 295, 343, 300, 50722, 50722, 321, 528, 281, 17522, 13, 961, 311, 5883, 1125, 16235, 23475, 493, 510, 412, 341, 935, 13, 759, 321, 747, 51108, 51108, 472, 5623, 1823, 11, 1310, 309, 603, 747, 505, 281, 300, 935, 13, 400, 570, 341, 13760, 307, 1238, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.13474801761000904, "compression_ratio": 1.4301075268817205, "no_speech_prob": 1.37094946239813e-06}, {"id": 79, "seek": 41336, "start": 420.52000000000004, "end": 428.24, "text": " we want to minimize. Let's initialize gradient descent up here at this point. If we take", "tokens": [50364, 1407, 23221, 341, 11, 718, 311, 574, 412, 1071, 1365, 13, 1692, 311, 264, 2063, 2445, 508, 295, 343, 300, 50722, 50722, 321, 528, 281, 17522, 13, 961, 311, 5883, 1125, 16235, 23475, 493, 510, 412, 341, 935, 13, 759, 321, 747, 51108, 51108, 472, 5623, 1823, 11, 1310, 309, 603, 747, 505, 281, 300, 935, 13, 400, 570, 341, 13760, 307, 1238, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.13474801761000904, "compression_ratio": 1.4301075268817205, "no_speech_prob": 1.37094946239813e-06}, {"id": 80, "seek": 41336, "start": 428.24, "end": 436.56, "text": " one update step, maybe it'll take us to that point. And because this derivative is pretty", "tokens": [50364, 1407, 23221, 341, 11, 718, 311, 574, 412, 1071, 1365, 13, 1692, 311, 264, 2063, 2445, 508, 295, 343, 300, 50722, 50722, 321, 528, 281, 17522, 13, 961, 311, 5883, 1125, 16235, 23475, 493, 510, 412, 341, 935, 13, 759, 321, 747, 51108, 51108, 472, 5623, 1823, 11, 1310, 309, 603, 747, 505, 281, 300, 935, 13, 400, 570, 341, 13760, 307, 1238, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.13474801761000904, "compression_ratio": 1.4301075268817205, "no_speech_prob": 1.37094946239813e-06}, {"id": 81, "seek": 43656, "start": 436.56, "end": 443.36, "text": " large, gradient descent takes a relatively big step, right? Now, we're at this second", "tokens": [50364, 2416, 11, 16235, 23475, 2516, 257, 7226, 955, 1823, 11, 558, 30, 823, 11, 321, 434, 412, 341, 1150, 50704, 50704, 935, 11, 689, 321, 747, 1071, 1823, 13, 400, 291, 815, 3449, 300, 264, 13525, 307, 406, 382, 16841, 51024, 51024, 382, 309, 390, 412, 264, 700, 935, 11, 370, 264, 13760, 1943, 380, 382, 2416, 13, 400, 370, 264, 958, 5623, 1823, 51344, 51344, 486, 406, 312, 382, 2416, 382, 300, 700, 1823, 13, 823, 11, 321, 434, 412, 341, 2636, 935, 510, 11, 293, 264, 13760, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.09313834610805717, "compression_ratio": 1.7860696517412935, "no_speech_prob": 1.4970824850024655e-05}, {"id": 82, "seek": 43656, "start": 443.36, "end": 449.76, "text": " point, where we take another step. And you may notice that the slope is not as steep", "tokens": [50364, 2416, 11, 16235, 23475, 2516, 257, 7226, 955, 1823, 11, 558, 30, 823, 11, 321, 434, 412, 341, 1150, 50704, 50704, 935, 11, 689, 321, 747, 1071, 1823, 13, 400, 291, 815, 3449, 300, 264, 13525, 307, 406, 382, 16841, 51024, 51024, 382, 309, 390, 412, 264, 700, 935, 11, 370, 264, 13760, 1943, 380, 382, 2416, 13, 400, 370, 264, 958, 5623, 1823, 51344, 51344, 486, 406, 312, 382, 2416, 382, 300, 700, 1823, 13, 823, 11, 321, 434, 412, 341, 2636, 935, 510, 11, 293, 264, 13760, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.09313834610805717, "compression_ratio": 1.7860696517412935, "no_speech_prob": 1.4970824850024655e-05}, {"id": 83, "seek": 43656, "start": 449.76, "end": 456.16, "text": " as it was at the first point, so the derivative isn't as large. And so the next update step", "tokens": [50364, 2416, 11, 16235, 23475, 2516, 257, 7226, 955, 1823, 11, 558, 30, 823, 11, 321, 434, 412, 341, 1150, 50704, 50704, 935, 11, 689, 321, 747, 1071, 1823, 13, 400, 291, 815, 3449, 300, 264, 13525, 307, 406, 382, 16841, 51024, 51024, 382, 309, 390, 412, 264, 700, 935, 11, 370, 264, 13760, 1943, 380, 382, 2416, 13, 400, 370, 264, 958, 5623, 1823, 51344, 51344, 486, 406, 312, 382, 2416, 382, 300, 700, 1823, 13, 823, 11, 321, 434, 412, 341, 2636, 935, 510, 11, 293, 264, 13760, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.09313834610805717, "compression_ratio": 1.7860696517412935, "no_speech_prob": 1.4970824850024655e-05}, {"id": 84, "seek": 43656, "start": 456.16, "end": 464.6, "text": " will not be as large as that first step. Now, we're at this third point here, and the derivative", "tokens": [50364, 2416, 11, 16235, 23475, 2516, 257, 7226, 955, 1823, 11, 558, 30, 823, 11, 321, 434, 412, 341, 1150, 50704, 50704, 935, 11, 689, 321, 747, 1071, 1823, 13, 400, 291, 815, 3449, 300, 264, 13525, 307, 406, 382, 16841, 51024, 51024, 382, 309, 390, 412, 264, 700, 935, 11, 370, 264, 13760, 1943, 380, 382, 2416, 13, 400, 370, 264, 958, 5623, 1823, 51344, 51344, 486, 406, 312, 382, 2416, 382, 300, 700, 1823, 13, 823, 11, 321, 434, 412, 341, 2636, 935, 510, 11, 293, 264, 13760, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.09313834610805717, "compression_ratio": 1.7860696517412935, "no_speech_prob": 1.4970824850024655e-05}, {"id": 85, "seek": 46460, "start": 464.6, "end": 471.28000000000003, "text": " is smaller than it was at the previous step, and we'll take an even smaller step. As we", "tokens": [50364, 307, 4356, 813, 309, 390, 412, 264, 3894, 1823, 11, 293, 321, 603, 747, 364, 754, 4356, 1823, 13, 1018, 321, 50698, 50698, 3109, 264, 7285, 11, 264, 13760, 2170, 4966, 293, 4966, 281, 4018, 13, 407, 382, 321, 1190, 16235, 51036, 51036, 23475, 11, 4728, 321, 434, 1940, 588, 1359, 4439, 1826, 291, 2721, 2524, 257, 2654, 7285, 13, 51398, 51398, 407, 445, 281, 20928, 11, 382, 321, 483, 2651, 257, 2654, 7285, 11, 16235, 23475, 486, 6772, 747, 4356, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.10473743887508617, "compression_ratio": 1.7440758293838863, "no_speech_prob": 5.68234054298955e-06}, {"id": 86, "seek": 46460, "start": 471.28000000000003, "end": 478.04, "text": " approach the minimum, the derivative gets closer and closer to zero. So as we run gradient", "tokens": [50364, 307, 4356, 813, 309, 390, 412, 264, 3894, 1823, 11, 293, 321, 603, 747, 364, 754, 4356, 1823, 13, 1018, 321, 50698, 50698, 3109, 264, 7285, 11, 264, 13760, 2170, 4966, 293, 4966, 281, 4018, 13, 407, 382, 321, 1190, 16235, 51036, 51036, 23475, 11, 4728, 321, 434, 1940, 588, 1359, 4439, 1826, 291, 2721, 2524, 257, 2654, 7285, 13, 51398, 51398, 407, 445, 281, 20928, 11, 382, 321, 483, 2651, 257, 2654, 7285, 11, 16235, 23475, 486, 6772, 747, 4356, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.10473743887508617, "compression_ratio": 1.7440758293838863, "no_speech_prob": 5.68234054298955e-06}, {"id": 87, "seek": 46460, "start": 478.04, "end": 485.28000000000003, "text": " descent, eventually we're taking very small steps until you finally reach a local minimum.", "tokens": [50364, 307, 4356, 813, 309, 390, 412, 264, 3894, 1823, 11, 293, 321, 603, 747, 364, 754, 4356, 1823, 13, 1018, 321, 50698, 50698, 3109, 264, 7285, 11, 264, 13760, 2170, 4966, 293, 4966, 281, 4018, 13, 407, 382, 321, 1190, 16235, 51036, 51036, 23475, 11, 4728, 321, 434, 1940, 588, 1359, 4439, 1826, 291, 2721, 2524, 257, 2654, 7285, 13, 51398, 51398, 407, 445, 281, 20928, 11, 382, 321, 483, 2651, 257, 2654, 7285, 11, 16235, 23475, 486, 6772, 747, 4356, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.10473743887508617, "compression_ratio": 1.7440758293838863, "no_speech_prob": 5.68234054298955e-06}, {"id": 88, "seek": 46460, "start": 485.28000000000003, "end": 492.0, "text": " So just to recap, as we get near a local minimum, gradient descent will automatically take smaller", "tokens": [50364, 307, 4356, 813, 309, 390, 412, 264, 3894, 1823, 11, 293, 321, 603, 747, 364, 754, 4356, 1823, 13, 1018, 321, 50698, 50698, 3109, 264, 7285, 11, 264, 13760, 2170, 4966, 293, 4966, 281, 4018, 13, 407, 382, 321, 1190, 16235, 51036, 51036, 23475, 11, 4728, 321, 434, 1940, 588, 1359, 4439, 1826, 291, 2721, 2524, 257, 2654, 7285, 13, 51398, 51398, 407, 445, 281, 20928, 11, 382, 321, 483, 2651, 257, 2654, 7285, 11, 16235, 23475, 486, 6772, 747, 4356, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.10473743887508617, "compression_ratio": 1.7440758293838863, "no_speech_prob": 5.68234054298955e-06}, {"id": 89, "seek": 49200, "start": 492.0, "end": 498.28, "text": " steps. And that's because as we approach the local minimum, the derivative automatically", "tokens": [50364, 4439, 13, 400, 300, 311, 570, 382, 321, 3109, 264, 2654, 7285, 11, 264, 13760, 6772, 50678, 50678, 2170, 4356, 11, 293, 300, 1355, 264, 5623, 4439, 611, 6772, 483, 4356, 11, 754, 498, 264, 51018, 51018, 2539, 3314, 8961, 307, 4305, 412, 512, 6806, 2158, 13, 407, 300, 311, 264, 16235, 23475, 9284, 13, 51358, 51358, 509, 393, 764, 309, 281, 853, 281, 17522, 604, 2063, 2445, 361, 11, 406, 445, 264, 914, 8889, 6713, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.11959325075149536, "compression_ratio": 1.6108597285067874, "no_speech_prob": 4.356736553745577e-06}, {"id": 90, "seek": 49200, "start": 498.28, "end": 505.08, "text": " gets smaller, and that means the update steps also automatically get smaller, even if the", "tokens": [50364, 4439, 13, 400, 300, 311, 570, 382, 321, 3109, 264, 2654, 7285, 11, 264, 13760, 6772, 50678, 50678, 2170, 4356, 11, 293, 300, 1355, 264, 5623, 4439, 611, 6772, 483, 4356, 11, 754, 498, 264, 51018, 51018, 2539, 3314, 8961, 307, 4305, 412, 512, 6806, 2158, 13, 407, 300, 311, 264, 16235, 23475, 9284, 13, 51358, 51358, 509, 393, 764, 309, 281, 853, 281, 17522, 604, 2063, 2445, 361, 11, 406, 445, 264, 914, 8889, 6713, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.11959325075149536, "compression_ratio": 1.6108597285067874, "no_speech_prob": 4.356736553745577e-06}, {"id": 91, "seek": 49200, "start": 505.08, "end": 511.88, "text": " learning rate alpha is kept at some fixed value. So that's the gradient descent algorithm.", "tokens": [50364, 4439, 13, 400, 300, 311, 570, 382, 321, 3109, 264, 2654, 7285, 11, 264, 13760, 6772, 50678, 50678, 2170, 4356, 11, 293, 300, 1355, 264, 5623, 4439, 611, 6772, 483, 4356, 11, 754, 498, 264, 51018, 51018, 2539, 3314, 8961, 307, 4305, 412, 512, 6806, 2158, 13, 407, 300, 311, 264, 16235, 23475, 9284, 13, 51358, 51358, 509, 393, 764, 309, 281, 853, 281, 17522, 604, 2063, 2445, 361, 11, 406, 445, 264, 914, 8889, 6713, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.11959325075149536, "compression_ratio": 1.6108597285067874, "no_speech_prob": 4.356736553745577e-06}, {"id": 92, "seek": 49200, "start": 511.88, "end": 517.2, "text": " You can use it to try to minimize any cost function j, not just the mean squared error", "tokens": [50364, 4439, 13, 400, 300, 311, 570, 382, 321, 3109, 264, 2654, 7285, 11, 264, 13760, 6772, 50678, 50678, 2170, 4356, 11, 293, 300, 1355, 264, 5623, 4439, 611, 6772, 483, 4356, 11, 754, 498, 264, 51018, 51018, 2539, 3314, 8961, 307, 4305, 412, 512, 6806, 2158, 13, 407, 300, 311, 264, 16235, 23475, 9284, 13, 51358, 51358, 509, 393, 764, 309, 281, 853, 281, 17522, 604, 2063, 2445, 361, 11, 406, 445, 264, 914, 8889, 6713, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.11959325075149536, "compression_ratio": 1.6108597285067874, "no_speech_prob": 4.356736553745577e-06}, {"id": 93, "seek": 51720, "start": 517.2, "end": 523.2, "text": " cost function that we're using for linear regression. In the next video, we're going", "tokens": [50364, 2063, 2445, 300, 321, 434, 1228, 337, 8213, 24590, 13, 682, 264, 958, 960, 11, 321, 434, 516, 50664, 50664, 281, 747, 264, 2445, 361, 293, 992, 300, 646, 281, 312, 2293, 264, 8213, 24590, 2316, 311, 2063, 50972, 50972, 2445, 11, 264, 914, 8889, 6713, 2063, 2445, 300, 321, 632, 808, 493, 365, 3071, 13, 400, 3372, 51262, 51262, 1214, 16235, 23475, 365, 341, 2063, 2445, 300, 486, 976, 291, 428, 700, 2539, 9284, 11, 51566, 51566], "temperature": 0.0, "avg_logprob": -0.08645695447921753, "compression_ratio": 1.7815533980582525, "no_speech_prob": 1.8341655959375203e-05}, {"id": 94, "seek": 51720, "start": 523.2, "end": 529.36, "text": " to take the function j and set that back to be exactly the linear regression model's cost", "tokens": [50364, 2063, 2445, 300, 321, 434, 1228, 337, 8213, 24590, 13, 682, 264, 958, 960, 11, 321, 434, 516, 50664, 50664, 281, 747, 264, 2445, 361, 293, 992, 300, 646, 281, 312, 2293, 264, 8213, 24590, 2316, 311, 2063, 50972, 50972, 2445, 11, 264, 914, 8889, 6713, 2063, 2445, 300, 321, 632, 808, 493, 365, 3071, 13, 400, 3372, 51262, 51262, 1214, 16235, 23475, 365, 341, 2063, 2445, 300, 486, 976, 291, 428, 700, 2539, 9284, 11, 51566, 51566], "temperature": 0.0, "avg_logprob": -0.08645695447921753, "compression_ratio": 1.7815533980582525, "no_speech_prob": 1.8341655959375203e-05}, {"id": 95, "seek": 51720, "start": 529.36, "end": 535.1600000000001, "text": " function, the mean squared error cost function that we had come up with earlier. And putting", "tokens": [50364, 2063, 2445, 300, 321, 434, 1228, 337, 8213, 24590, 13, 682, 264, 958, 960, 11, 321, 434, 516, 50664, 50664, 281, 747, 264, 2445, 361, 293, 992, 300, 646, 281, 312, 2293, 264, 8213, 24590, 2316, 311, 2063, 50972, 50972, 2445, 11, 264, 914, 8889, 6713, 2063, 2445, 300, 321, 632, 808, 493, 365, 3071, 13, 400, 3372, 51262, 51262, 1214, 16235, 23475, 365, 341, 2063, 2445, 300, 486, 976, 291, 428, 700, 2539, 9284, 11, 51566, 51566], "temperature": 0.0, "avg_logprob": -0.08645695447921753, "compression_ratio": 1.7815533980582525, "no_speech_prob": 1.8341655959375203e-05}, {"id": 96, "seek": 51720, "start": 535.1600000000001, "end": 541.24, "text": " together gradient descent with this cost function that will give you your first learning algorithm,", "tokens": [50364, 2063, 2445, 300, 321, 434, 1228, 337, 8213, 24590, 13, 682, 264, 958, 960, 11, 321, 434, 516, 50664, 50664, 281, 747, 264, 2445, 361, 293, 992, 300, 646, 281, 312, 2293, 264, 8213, 24590, 2316, 311, 2063, 50972, 50972, 2445, 11, 264, 914, 8889, 6713, 2063, 2445, 300, 321, 632, 808, 493, 365, 3071, 13, 400, 3372, 51262, 51262, 1214, 16235, 23475, 365, 341, 2063, 2445, 300, 486, 976, 291, 428, 700, 2539, 9284, 11, 51566, 51566], "temperature": 0.0, "avg_logprob": -0.08645695447921753, "compression_ratio": 1.7815533980582525, "no_speech_prob": 1.8341655959375203e-05}, {"id": 97, "seek": 54124, "start": 541.24, "end": 548.24, "text": " the linear regression algorithm.", "tokens": [50364, 264, 8213, 24590, 9284, 13, 50714], "temperature": 0.0, "avg_logprob": -0.4347541630268097, "compression_ratio": 0.8, "no_speech_prob": 5.055079236626625e-05}], "language": "en", "video_id": "Eu8lt4j9xiU", "entity": "ML Specialization, Andrew Ng (2022)"}}