{"video_id": "6D4EWKJgNn0", "title": "Week 12 \u2013 Lecture: Deep Learning for Natural Language Processing (NLP)", "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Mike Lewis\nWeek 12: http://bit.ly/pDL-en-12\n\n0:00:00 \u2013 Week 12 \u2013 Lecture\n\nLECTURE Part A: http://bit.ly/pDL-en-12-1\nIn this section we discuss the various architectures used in NLP applications, beginning with CNNs, RNNs, and eventually covering the state of-the art architecture, transformers. We then discuss the various modules that comprise transformers and how they make transformers advantageous for NLP tasks. Finally, we discuss tricks that allow transformers to be trained effectively.\n0:00:44 \u2013 Introduction to deep learning in NLP and language models\n0:13:48 \u2013 Transformer language model structure and intuition\n0:32:55 \u2013 Some tricks and facts of Transformer Language Models and decoding Language Models\n\nLECTURE Part B: http://bit.ly/pDL-en-12-2\nIn this section we introduce beam search as a middle ground between greedy decoding and exhaustive search. We consider the case of wanting to sample from the generative distribution (i.e. when generating text) and introduce \u201ctop-k\u201d sampling. Subsequently, we introduce sequence to sequence models (with a transformer variant) and back-translation. We then introduce unsupervised learning approaches for learning embeddings and discuss word2vec, GPT, and BERT.\n0:45:32 \u2013 Beam Search, Sampling and Text Generation\n1:03:31 \u2013 Back-translation, word2vec and BERT's\n1:22:43 \u2013 Pre-training for NLP and Next Steps", "author": "Alfredo Canziani", "keywords": ["Yann LeCun", "Deep Learning", "PyTorch", "NYU", "NLP", "Natural Language Processing", "BERT", "GPT", "GPT-2", "GPT-3", "transformer", "attention"], "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw", "length": 6056, "views": 7814, "publish_date": "11/02/2022", "timestamp": 1594684800, "entity": "Yann LeCun", "transcript": {"text": " We have today Mike Lewis. He's a research scientist at Facebook AI Research working on natural language processing. Previously he was a postdoc at the University of Washington and working with Luke Zettelmoyer on the search-based structure prediction. He completed his PhD at the University of Edinburgh and based on combining distributional and logical approaches to semantics. He has a Master's degree from the University of Oxford and won the best paper award on EEMNLP in 2016. So without further ado, let's get started with today's presentation. Okay, well thank you very much for the introduction, Alfredo, and for inviting me to talk. So yeah, this lecture just wants to try and give a high level of view of how deep learning is used for natural language processing these days. So I think this per se has been really dramatic progress in NLP in the last few years with applying deep learning. So these days, I mean, you can get machine translation systems which will produce translations which blind human erasers prefer to ones produced by professional translators. You can make some question answering systems that you ask them a question, give them Wikipedia, and they'll give you answers that are more accurate than the ones that people will give you. I could have language models that can generate several paragraphs of fluent text. And for all of these things, if you'd asked me maybe five years ago, I'd have told you there's absolutely no way any of this is possible in 2020. But there seems to be a few techniques that have been introduced that have really made dramatic differences. One really nice property is actually you can achieve all of these things with fairly generic models. So all the models for all these tasks are actually look very similar. There's just a few high level principles which are really useful. So I'm going to cover quite a lot of ground on this quite quickly, so please interrupt me often with questions. Right. So, yeah, also one thing I should say is that with all the progress we've been seeing in NLP, I probably lost the stuff I'm going to tell you right now is going to be out of date in a year or two. I mean, I hope we continue to make progress and stuff continues to be out of date. So as well as explaining some of the models we're using, I want to try and give you some intuitions about what kinds of principles are working out well. So hopefully you have a bit more of a sharp life. Okay. So the first topic I want to cover in this lecture is language modeling. Language modeling isn't necessarily a useful task in itself, but it seems to be a really good building block for introducing all the techniques that we'll need later on. So I don't know if any of you have seen this before. This is an example from a language model called GPT-2, which came out in 2019. So what's going on here is that we've got some humans applied some introduction here about scientists finding a herd of unicorns in the Andes that apparently speak English. And then given that text, they've asked this language model to write some more text and stuff like that. And the text we get here is actually quite impressive. Like everyone was really shocked that language models could work this well last year. So you can see the continuing text seems actually quite plausible for a news article about this. It's talking about unicorns. The text is very fluent grammatical. It's not really any flaws there. And it seems to like invent quite a lot of details like the name of the scientist who discovered them. Obviously all this is complete nonsense. Nothing here is true, but also none of this will look like anything the model was ever trained on. I'm pretty sure this paragraph that was unified has no neighbors anywhere on the internet. This is all completely new language, but it's all actually quite high quality text. I'm not going to read all this out, but if you read the rest of the article I wrote, then there are some flaws, but they're quite hard to spot. And generally this seems to be quite a good language model. So I'm going to try to show you the kind of techniques you need to actually build a language model that works as well. So very briefly, what is a language model? So a language model is just basically density estimation for text. So we're going to assign a probability to every possible string. And hopefully we can put some more probability on strings which are fluent English than other strings. So how do we model this density? Well, obviously there are quite a lot of possible sentences, exponentially many. So we can't just predict classifiers directly. There are different techniques you can do for this, but the one I'm going to talk about is the one that's most popularly used, which is basically to factorize this distribution using the chain rule. So here all we're going to do is just factorize it to say we're going to predict the first word, then predict the second word, give them the first, then the third, give them the previous two. This is an exact characterization, it doesn't cost us anything to do it like this. So really what we've turned is the density estimation problem into a series of classification problems. These classification problems are performed given a bunch of text, predict the next word. And that's going to be a theme through a lot of techniques we have in this talk. So more concretely, we have this, from this example I showed you before, we've got this string and model output, like the sciences, name of population after the distinctive form of it, and it's going to predict the next word. And the correct word in this case is Unicum. Okay, so at high level, all these language models look something like this. Basically, we input this text into a neural network somehow. The neural network will map all this context onto a vector, this vector represents the next word. And then we're also going to have some big word-abiding matrix. So word-abiding matrix will basically contain a vector for every possible word model knows how to output. And then all we need to do is compute the similarity by just doing a dot product between the context vector and each of these word vectors. And we'll get a likelihood of predicting the next word. Then we'll just trade this model by maximum likelihood in the obvious way. Okay, so I mean, the detail here is often we don't deal with words directly, we deal with things called subwords or even characters, but all the modeling techniques remain the same. All right, so how all the skill here is in this context encoder, how do we build this? So kind of the first approach people took for this was basically convolutional models. So these convolutional models kind of encode this inductive bias that language kind of has this translation of variance property in it, which is interpreted phrase the same way no matter what position it occurs in. So a typical model might look like this, where basically first of all, for every word we'll just map it to some vector, which is just a lookup table into an abiding matrix. So the word will get the same vector no matter what context it appears in. And then we'll apply a bunch of layers of 1D convolutions followed by nonlinearities until eventually we end up with some kind of vector representing that context, which really here the vector means what should the next word be. And these models were the first, I think this is actually maybe the first language model from Benjio in 2003, the first neural language model. And these kind of convolutional approaches were actually showed to work quite well by Jan de Vond in 2016, and then we applied them on deep learning techniques. These models are very fast, which is great. I'd say speed is very important for language modeling because typically we use huge amounts of training data. This one came down side, which is that they're only able to really condition on a certain perceptive field. So in this toy example, this word unicorn can only condition the previous five words because of the kernel width and number of layers we're using here. Obviously, realistic convolutional models have a much bigger perceptive field like this, but natural language tends to have extremely long range dependencies. For an extreme example, you can imagine if you're trying to build a line 12 of a complete book, it might actually help you to be able to condition on the title of the book at all time steps. And obviously, the title will be hundreds of thousands of words previously. And it's quite hard to kind of build this into, build a convolutional perceptive field that's big enough to do this. OK, so how do we condition on our context? I guess the most popular approach until a couple of years ago was what's called recurrent neural networks. This is kind of a conceptually quite straightforward idea that basically every time step we're going to maintain some states or have some state coming in from the previous time step, which represents what we've read so far. We'll combine the state with the current word we've read, and we'll use that to update our states. And then we'll just iterate this for as many time steps as we need. So I think this seems like quite a natural model of reading. I mean, I think for the most part, people kind of read left to right and maintain some kind of states as they go. At least in principle as well, you can model unbounded context like this. At least in principle, like the title of the book would affect the hidden states of the last word of the book. In practice, there are some fairly significant issues with this model. Firstly, there's kind of no free lunch here. So by the time the effect of maintaining the state is we're going to compress the whole history of the document reading into a single vector at each time step. And then once you've read a word, you can never look at it again. You have to memorize that. And that means you have to actually have to cram a huge amount of information into a single vector. And this is basically kind of a bottleneck in the model. Certainly, there's a question about how much information you can really store in one vector. But also, it's kind of a practical learning problem to you. That's the issue called the profanity gradient problem, where it means that every time you go through one of these steps, then you'll have some kind of non-linearity, which will mean that the effect of words in the past will kind of get exponentially smaller each time step. That means that once you have no gradient to a particular word in the past, it's very hard to actually suddenly learn later on that word was imposed. One final issue I want to mention with RNNs is that they're actually quite slow. The reason for this is particularly for training. So the reason is that in order to build your state for a particular word, you actually have to build your state for every previous word first. That means essentially you have a big for loop that's going over your entire document. And the longer your document is, the bigger this for loop is. And most of these operations you can't actually confuse in parallel. You actually have to do it sequentially. And one GPU hardware is really based around being able to do operations in parallel. So the convolutional network didn't have this problem. Everything's in parallel. But on the other hand, you get this bound to the receptor field. The recurrent models you have in principle an infinite amount of receptor field, but it's quite slow to train. So the solution to this is now what's called the transformer, which is the model that's used in all the state of the art NLP systems these days. So I want to go through the transformer in quite a lot more detail than I did the RNNs or CNNs. The transformers were introduced in 2017 by Ashish Vaswani and a famous paper called Attention is All You Need. And they really revolutionized lots of NLP. So I included a figure here from the original transformer paper. I don't know how confusing this is to you, but certainly when I first saw this figure in 2017, it took me quite a while to get my head around it. There's quite a lot of details going on in these boxes. So I'm going to just try to slowly drill into them. All right. So what's going on? Basically, you see we have this input state, this N times this transformer block, and then an output of phase. So this N times block thing just means we're going to unroll the same block with different parameters a certain number of times. So this example has six layers, which I think had the original transformer paper, which seems quite cute these days. These days, people are training models with billions of parameters and many, many dozens of layers. So I'm just going to drill into this box in more detail. So this is kind of the core of the transformer, the transformer block. You can see it's actually incorporated into two different sub layers, which are both very important. Sub layer two is maybe the more obvious one. This is just a big feed forward network. It could be any MLP, but it's important it's actually quite expressive. And beneath we have this multi-headed tension module. Multi-headed tension is kind of the key building block behind transformers and why they work. So these sub layers are also connected by these boxes labeled adding norm. So the add part means this is a residual connection, which helps stop the gradient transition in large models. The norm here means layer normalization. I'm not going to go into layer norm in detail here, but it's actually very important to make these models work. And there's actually some subtleties about how exactly you do the layer normalization that makes a big difference in practice. Hey, excuse me. I have a question. Sure. So this isn't immediately clear, but could you talk a little bit more about just the intuition behind using multi-headed tension as opposed to a single head? I mean, presumably each head learns something different and attends over its input differently. But what's the intuition behind that? I'll answer that question a bit. I'm going to go through exactly what multi-headed tension is a bit first, and then I'll try to give some intuitions as to why this is a good thing to do. If I don't answer your question, then please follow me up a few slides time. Thank you. Any other questions at this stage, by the way? I have a question. You said that the transformer module uses layer normalization. Can you provide some intuition into why that works better than group normalization or batch normalization? I do think I can actually give a very satisfying technical answer to this. I think a lot of this is quite empirical as to why in NLP, layer normalization works great, and in computer version, batch normalization works great. A nice proffs about layer normalization is that it doesn't depend on the batch dimension, which batch normalization does. In practice, that's quite a big advantage because it's quite hard to train with large batches with them for large models. You can see people wrote a lot of papers on why things like batch normalization even work for computer version. At least last time I read, there are still some debates as to what it's doing. Maybe the intuitions in the original paper for why even batch normalization works are not great. Personally, I would say this is one of the slightly unsatisfying things in deep learning, where it works, but it's a little bit unclear why. Thank you. I might be able to give a more satisfying answer than that. Another question is coming here. Do transformers share weights across time steps like RNN LSTMs? Great question. I should have made that clear. All these weights can be shared across time steps. It's kind of convolutional in that sense. You have one block and you apply it every time step. You can actually also apply them using weights in every layer. That works quite well too, but it's not what people normally do. Thank you. Any other questions so far? I think those were the questions I read so far on the chat. What is this mysterious multi-head attention thing? Here's another figure. I don't know if this helps. Basically, compute these three quantities called B, K, and Q here. They stamp the query key and value respectively. Do the scale.product detection operation and then concatenate the outputs. Drilling into this scale.product detection. Eventually we will run out of boxes to expand. It looks something like this. We're going to do, compute this query key, do a.product for self-packs, and use this as a way to sum up the values. Don't worry if that doesn't make sense. I will do more detail. Let's take this example where the context here is, let's say, these horns silver-white. We're trying to predict the next word, which in the example before was unicorn. For the word we're trying to predict, we're going to compute this value called the query. For all the previous words, we're going to compute the quantity called the key. These are linear layers based on the current state of this layer. Tomorrow we're going to be coding this in practice. We're going to be seeing this in all the small details in the code as well. We're going to think of this query as the model asking a question of its context so far. It's going to help us predict what the next word should be. The query could be something like, tell me what the previous adjective is or tell me what the previous determiner is. The key is going to be things that label the current word. This word is an adjective, this word is a determiner, this word is a verb, something like that. Or it could be something more complex like any arbitrary relation like a coreference or something. The model is going to compute this question as a query. Then it's going to compute, just do a dot product with all of the keys. Then you do a softmax as well. This is going to induce a distribution over all the previous words. Here you can imagine a query as something like, tell me what the previous adjective is. The attention will produce this distribution over these three previous words. It's going to put most probability maths on either horn or silver white. We're also going to compute this other quantity called the value. We'll do that for all the previous words as well. Maybe the value will tell us something slightly more about what the content of the word is. Then I'm going to compute this hidden state by basically marginalizing out the attention distribution. Here this hidden state is going to be a weighted sum of the values of all the previous words. It's going to be weighted by the probability of that word. That's basically what's going on the left side of this figure here. I left out this detail about the scaling. Let's just try to make the gradients more stable. There's another detail here, which is that that's kind of single-headed attention I've described so far. We're actually going to do this thing called multi-headed attention. That basically just means we're going to compute the same thing with different queries, keys, and values multiple times in parallel. There's a question for what the intuition behind that is. Really, you actually want to know lots of different things. Just an example. Let's say the next word here should be unicorns, plural. To know it should be unicorns, you probably want to know both that it's horned and silver white because the conjunction of those makes it more likely to be a unicorn. You also want to know that the term here was these, not a. If it was a horned silver white, it would be unicorn singular. The fact it's these means it should be unicorn plural, so you get a plural of the three words. You actually need to look at all these three words at once to have a good idea what the next word should be. Multi-headed attention is a way of letting each word look at multiple previous words or techniques. A question here is why are we actually in need of using the softmax? Why do we use the softmax? It's a good question. Firstly, having some kind of normalization effect is probably good. Otherwise, when you go to longer sequences, this summation would get bigger and bigger the further you went through. So having normalization is probably good. Normalization also kind of lets the model discard information too, so it can say this word just isn't relevant, which is good. I think I have seen people experiment with using things like Rayleigh use instead, which gives a different way of discarding information. But I think the evidence of the softmax is the best. Another question, so you may have missed this. The mask there in pink, what that is, briefly? Right, sure. The mask is actually important. I was going to say, make a point for this. One of the big wins about this whole set of multi-headed attention is that it's extremely parallelizable. So the computation of queries, keys, and values at every time step depends on what you're doing at any of the other time steps. So unlike a recurrent network, you can actually compute all of these simultaneously. And that works very well with the kind of hardware we have these days. So not only are we going to compute all the different heads at once, we're going to compute all the time steps at once in a single forward pass. So that's great, except that if you're computing all the time steps at once, there's nothing to actually stop you from computing all the time steps at once, you're going to be able to compute all the time steps at once. So that's a great point. So let's look at some of the other things that we can do to help us understand how to do this. So let's look at some of the other things that we can do to help us understand how to do this. So the solutions right here is what we call self-potential masking. So a mask is just an upper triangular matrix that sort of has zeros in the lower triangle and negative infinity in the upper triangle. And we're going to just add this to the attention scores. And the effect of that is going to be that every word to the left has a much higher attention score than every word to the right, so the model will only end up practicing using words to the left. This is deterministic mask without trainable weights. Just values either zero or negative infinity. So you'd only mask in case of an application specific training task, correct? If you had to just build representations, for example, you wouldn't need to mask because it wouldn't matter? Yeah, that's a great question. So we'll go on to more general representation learning later. Most of what you just want to text encoder, you don't need to mask them. Bidirectional context is absolutely helpful. In this case of language modeling, which we're working through so far, then the mask is sort of crucial to make the model mathematically correct and compute the correct factorization. We get a great question. Okay. So one other detail we need to do to make all this work is add something to the input code for positional embedding. So as I've described the model so far, it's actually it's doing a little bias model. It knows very little about language. The inputs could be anything and it would work. In particular, you can model a set or a graph or anything like this. It should be fine. You know, I mean, in language, there are some properties which are useful, like, for example, there's an ordering to words, which is really important to how you interpret them. And this model doesn't actually know anything about that. And that's in contrast to the convolutional models and the current models I showed you earlier, which both have different ways of encoding the order of text. So one of the techniques that was introduced in this paper was called positional embedding. There are different ways you can do this. They describe something in the paper, which is slightly weird. Actually, I'm not going to describe, but it works just as well to essentially learn a separate embedding for every time step. So for every position in the document, 0, 1, 2, 3, 4, 5, you just learn a separate embedding and then you add this to your inputs. So your input now is a summation of the word vector and some kind of positional vector. And it's very simple, but it gives the model the kind of the order of information it needs and it works great. Okay, so why are these models so good? Why does everyone use them? I think the really powerful thing about the model is that it kind of gives you direct connections between every pair of words. So each word can directly access the hidden state of every previous word. That's a contrast. The convolutional model could maybe get to the state of all the words in this receptor field, but nothing further back in time than that. And the recurrent model, the state has to go through this bottleneck at each time step. You can actually directly access the previous words beyond the literally one previous word. Anything further in the past than that had to get somehow compressed and you can lost information on this. For self-attention, you can in principle put 100% attention on any word in the distant past and see exactly what was there. And this just makes it a really powerful model. It avoids things like issues like vanishing gradients quite effectively. And means it can just learn very expressive functions very easily. The other great thing about this is how parallelizable it is. So on the one hand, this model is doing quite a lot of computation in that it's doing this. So the self-attention operation is quadratic, basically, because every word can look at every other word. That sounds quite slow, but the really nice thing is you can do it in parallel. Because all these operations are independent to each other, you just do it as one big matrix multiplication. Even though sometimes you probably do kind of more add-multiply operations than you would do with the equivalent RNN, you can do all these operations much faster because you do them all at once rather sequentially. So this is a really good trade-off. I also want to quickly talk about some other things. So I've described stuff like multi-headed attention and additional bad instances and stuff. So I've got all the attention when Transformers was first released. But Transformers also came along with a whole bag of other tricks as well. And these tricks were all actually really important to make this stuff work. And in some sense, this paper really kind of modernized NLP, I think. So for example, I mentioned a bit about this useful learning normalization before. The learning normalization is really helpful. They also started doing these things like these learning rate schedules. For whatever reason, to make Transformers work well, you have to sort of linearly warm up your learning rate from zero to your goal learning rate over several thousand steps. And people do warm up learning rates in other settings, but Transformers really, really need this to work. Also, things like the initialization actually really do matter with these. And some initializations don't work. And they throw in these other tricks like a label smoothing of the output, which, again, wasn't invented in this paper, but turns out to be quite helpful for the task-like machine translation. So to give you some idea of how well these things are working, these models I've described so far, here are some results on a language modeling benchmark. So the number on the right is what's called complexity, which is a measure of the likelihood of held out data. And here, lower is better. So you see that NLSTM for 2016 gets complexity of 48. You see that Yandera finds convolutional models for 2016 doing quite a bit better at about 37. Also, people played around with a whole bunch of RNN variants. There used to be dozens of papers on how you make variations in NLSTMs, and some of these get to below 30s as well. But then when you introduce transformers, you get a really big jump down to go to 18 and 20. And in terms of language modeling, that's a really enormous jump in performance. And I should say these gains we saw were particularly large on kind of long context language modeling. So language modeling where... So there's some benchmarks where you just get single sentence predicts. On this task, you actually get a whole Wikipedia article, so potentially thousands of words. And the transformers really shine on this, where you've got thousands of words context model, and you need to retain information across all of them. Okay. What else can I say? Yeah, so this quick comparison just to visualize how transformers and NLSTMs look. Which is kind of integrated into the points I was showing before. So in the NLSTM, you have a lot fewer connections between the words, like everything's kind of very sequential left to right. In transformer, you don't have any of this. So every word is directly connected to every other word. I guess I should say as well, in some sense, this is maybe slightly unnatural model for reading. So it kind of suggests that however small is reading text, every time it reads a word, it goes and re-reads every other word very quickly. But it's very effective. One other good thing about transformers is that they do scale up extremely well. So with tasks like language modeling, you get essentially infinite amounts of data, because there's just hundreds of billions of words out there. That's far more than you'd ever need. That means to actually fit this kind of distribution, you need very big models. If you just keep on adding parameters to transformers, they keep on just working better and better. The examples I showed you before were from this GPT-2 model with 2 billion parameters, which was quite big for 2019, but in 2020 we're up to 17 billion and there are rumors that 100 billion parameter models will be coming along soon. Excuse me, I have a question. You said transformers are really good for scaling up. I was just wondering in the language modeling task, if we have like, say a 10,000, 20,000 word document, in an RNN, we can just insert a word step by step and we wouldn't need a lot of memory, per se. For a transformer, we'd need to have a batch size of 10,000, wouldn't we? Like the length of the sequence. But if you have a really long sequence, can we model these long-term dependencies? Yeah, that's a really great question. I actually meant to mention this point. Two things I want to say. So firstly, you're absolutely right. The self-tension is, because it's quadratic, the expense obviously grows super linearly, and that is a problem in practice. It means, so mostly transformers do 512-token context windows, which is fairly affordable for GPUs. I love these language models to do more than that. They may model a few thousand tokens, which kind of limits what we can do. But it's definitely the case that a vanilla transformer can't model, say, a 50,000 word book at all. There is like, this whole kind of technology industry recently of building various transformers, which can do long sequences. It's a very hot topic right now. There's a bunch of things you can do, but one kind of thing you can do is, if you replace the self-tension with something like nearest neighbor search, you can do the self-tension subquadratic time, and that makes it faster. There are also versions that try to do sparse attention, where you can't tend to every previous word directly, but you kind of have some dilated set of previous words you can look at, and you don't quite get direct connections to every word, but you can still guarantee that you have short paths across every word. There are also things like compressed transformers, which try to bundle the night and total into compressed, distant paths into shorter representations. Okay, so you brought up the question of RNN. So at impressed time, absolutely, and RNN can model infinite context with absolutely no additional cost, which is great. It can output like a million words, and output can go in this just fine. Well, the question is actually, is it used in this context? The answer is probably not. So at training time, you can't do this. At training time, you actually have to back propagate through what's called back propagation through time, where the LSTM would have to, if model is in context, like grading will have to propagate all the way back through the sort of all the recurrent steps to make a difference in the distant paths. And in practice, firstly, the grading will vanish like well before you hit a thousand steps. And also, this is very expensive. So this, at training time, this isn't for free. And this back propagation operation will get more and more expensive as you go through the longer sequences, you know, modeling. So that means you can't actually really learn to follow distant paths, even if the time is not expensive, you just won't know what to do with this. And in practice, you probably just forget it anyway, because you can't remember that much data at once. All right, one more quick point on that, because it's interesting. I guess one case where the RNNs do have an advantage is on certain algorithmic tasks. So if you aren't modeling language, let's say you're doing something like addition or trying to like model parity of a string. So if you use a string of zeros and ones and ask you, are there an even number of ones or not? In those cases, you basically do actually want to apply literally the same operation to every time step. And you don't actually have to have much memory because your state really just needs to be a zero or one. In this case, the RNNs actually do work very well because you can train them on short sequences and then they'll create generalizations along sequences on these kinds of toy problems. And I haven't seen anyone try, but I imagine the transformer would actually find it much harder to get that kind of generalization. That only really applies to these kind of algorithmic problems in terms of modeling natural language then. Yeah, it seems like various transformers are going to be much more effective than recurrent Nets. Thank you. That was really helpful. Any other questions on transformers? I've been addressing the questions I could via text, so I think we are all good right now. Okay. All right. So next thing we want to cover is what's called decoding or inference language models. So we're training the language model. The language model puts the, hopefully, probability maths on things that's good English and the probability on grammatical and nonsensical things. But if you want to create these samples like I showed you before, how do we actually generate the text? So often when we think about the inference of the graphical model, what we care, what we'd like to do is find the max. So like finding the sensors which maximizes language models probability. Unfortunately, there's, as I mentioned before, there's quite a lot of English sentences that are possible and we can't just like score the models to find the max. Also, these models don't really, there's not really a lot to do with dynamic programming here. So sometimes you can't find the max of over exponential structures when you have a model that factorizes some way that lets you build a dynamic program, which lets you kind of share stage across different hypotheses. But these models don't decompose in a friendly way. So you kind of, whatever choice you make, the first word could affect all the other decisions. So given that, one thing to do is to do greedy decoding. This is where we're just going to take the most likely first word and then given that word, put it in the most likely second word and then guess the most likely third word. So that's okay, but there's no guarantee that it's actually going to be the most likely sequence you want to output, because if you happen to make a bad step at some point, then you've got no way of backtracking your search to undo any previous decisions. The slide says that exhaustive search is impossible. And then we're going to hit some middle ground with what's called beam search. So beam search is a way of trying to keep track of an M-best list of hypotheses. And then we're going to just every time step try and keep track, update this list with new words we've added. This is easy to show you an example. This slide is from Abigail C. at Stanford. So we start, we output, let's say we have been size of two, we'll output these two possible words. These are two most likely words. So we're going to generate, work out what the two most likely next words are. So obviously with IOH, that does affect what the next word should be, and we'll come up with different hypotheses for each of these. And then at the same time, we're going to kind of compress these down to a list of two that we're going to continue processing. So we're looking for the lowest total sum, is it correct? Yeah, so these are log likelihoods. We actually want the highest sum, if that makes sense, which should be where zero would be a probability one. So every score is going to be less than zero, but we're going to find the sequence where some of the scores is the highest. Did that make sense? It is, sorry, yeah, I got the sign flipped. I was meaning like in magnitude, we tried to get the smallest number in magnitude. As well as the magnitude, sure. Yeah, sure, sorry. Sorry, maybe this is not the best way to show this. How deep does this tree go into the Bing search? When do you stop looking for candidate sequences, I guess? So we basically, more detail I haven't told you, we have this like end of sentence token. And the end of sentence token means once you output that, this hypothesis is finished. And the aim is to find complete hypotheses, so from start to the end token, that have the highest possible score. We'll just keep on generating new hypotheses until there's no possible new words for that that would be the K complete hypothesis we have still. There is another question here. Why do you think in NMT very, very large beam size will most often result in empty translations? Great question. I have opinions on this. So the research is good in the sense that it's guaranteed to give you a higher scoring hypothesis than the greedy search I mentioned on the previous slide. But it's kind of a catch here, which is that we're not actually, at training time, we're typically not using a beam. So at training time we normally just use kind of the autoregressive factorization I showed you before, where like given the N previous correct outputs, predict the N plus first word. What we're not doing is exposing the model to its own mistakes. So when you do beam search, you can get all kinds of nonsense showing up in your beam for whatever reason, because if you have a very big beam then probably some of it will be garbage. And in these garbage states, the model has no idea what to do because it was never trained in the situation. So it's kind of reasonable to expect your model to generalize well for like making correct predictions for some completely nonsensical series of words, which is like very powerful training distribution. And in these cases the model can do all kinds of weird and insoluble things, like maybe they put a very high probability on something else. For a kind of a classic example of this, I don't know if you, I don't have one here, but you may have seen these Limes models get stuck in these feedback loops where they end up just like repeating the same word or phrase infinitely many times. I think this is kind of example of this, where once the model starts going into this kind of loop, then it doesn't really know what to do and the easiest thing for it to carry on doing is just keep on looping. So, yeah, I think the issue with beam search is one of not exposing the model to its own mistakes at training time. So it actually puts much probability on all kinds of things it shouldn't do. Kind of the obvious solution to that is like, why don't you have a beam at training time? There's a, and the short answer to that is because it's expensive. It gets to go around this whole inference procedure at training time, but first it gets rid of all the nice parallelism, but we have the transformer and it uses all the search and also gives you many more things to score for every training example. So, in practice people tend to just ignore this problem and train a big model for as long as they can, like exploring some nice fast parallelism you get from this autoregressive version, and then a test time people will often tune the size of their beam to get the best performance. I think that translation like increase the beam normally helps with your plays, but basically worse game we started covering these weird degenerate outputs. But yes, just the unsatisfying thing people have to do. Sorry, that's quite a long answer. Does that answer your question? I think the student, I will see now what the student says. Like, that was a question from a student. And there is another question, a small one on this current slide, why is the A and 1 in green? On the right hand side. I don't know. I saw this slide from Abby. I'm not quite sure what points you're trying to make there. Okay, no, it's okay. Oh, okay, the point actually is some answer, because they are interchangeable. Regardless of the one you pick, you get both outputs, pi and tart. Like either or you go for either you go for A or 1, both of them will tell you pi tart or pi tart. I'm not sure. Even if you were to do that, then you can't compress these. It's not that you can get a dynamic program where you can collapse these hypotheses. Because the hidden states for these two hypotheses will still be different depending on which path you took together. So I don't know. Hopefully this illustrates my theme search. Okay, any more questions on this? Okay, this is a bit more description of how the algorithm looks. Basically, all you do is every time step you generate a distribution of next words for each hypothesis you have. And then you take the top k hypotheses, next words across all your hypotheses and pull these before you go into the next word. Alright, so beam search is sometimes the right thing to do, but often it actually isn't. So this is a result of applying beam search to the example I showed you before. So this is an example of GPT-2 about scientists finding unicorns in the Andes. You can see here that the model is actually, it starts out putting some good stuff and then gets stuck in this weird feedback loop. So I think we're just going to repeat the same phrase over and over and over again. I'll probably just keep on repeating this phrase forever. I guess kind of what's going on here is once you said this phrase twice, maybe just saying it the third time is actually the most likely thing to do. And then all these other hypotheses get very high probability, even if they're not good. But there's also kind of a slightly different problem, which is like maybe in some cases we don't actually want the most likely sequence after all. Maybe what we want is something interesting. So you see this problem also a lot in things like dialogue response generation. So you're trying to build a model of conversation with someone. And if you do this kind of beam search, often what you'll get is it will just give you the most generic response to anything you say. So whatever you say, it'll be like, oh, that's interesting. Thanks. And maybe that actually genuinely is the most likely thing to do because these responses are good in most situations. However, there's not actually a very good experience or very good system. But if instead of taking the max, we're going to sample from the model distribution instead. So this is conceptually kind of appealing, but it doesn't actually give you very good output. So this is again the sort of sampling on that same input. So this is kind of good, but then it gets more weird and generic as it goes on. And again, you get to that problem where once you sample the bad choice, then the model's in state it was never in training. And then once the model's in state it was in during training, it's more likely to give you some more bad output, and then you'll get stuck in these horrible feedback loops. Okay. So here's something that actually does work, the technique that was used to get you those beautiful outputs we had before. Unfortunately, it's not a very satisfying technique, but just to give you a disclosure. So this is called top case sampling. It's introduced by Angela Fan a couple of years ago. Basically top case sampling, what we're going to do is truncate a distribution to just taking the k best and then sample from that. So this is an advantage of giving you this kind of diversity, like choosing randomly amongst good options, but tries to stop us kind of falling off this kind of manifold of actually good language by when we sample something bad. So the idea is basically just chop off a long tail and just sample from the head of the distribution. And this is the sampling for the beam search, is it? Sorry, this isn't a beam search, this is just generation. So we're going to... There's not going to be a beam here, this is going to be one hypothesis. I guess you could integrate this beam search too, but this is actually pure sampling. So I can generate a word in this method and use that to generate the next word. So when you do all that, then this is finally the technique that was used to generate this nice sample. Obviously, this top case sampling is a bit of a hack, it's not very satisfying. I was an author on that paper, so I can be mean about the method, but it does seem to work quite well. I guess one thing to be aware of is when you see these great samples and things like this, which OpenAI are very happy to put in their publicity, it's kind of useful to know how it was actually made. This is not like a real sample for model distribution, the model is not putting most of its probability matter on this. This is something that's generated by doing a slightly weird inference procedure with the model. I just want to quickly cover... So give us some text like this, how do we know if it's any good or not? How do you evaluate this? So like evaluating a language model is quite easy. I mean, language modeling is task density estimation, so you just look at the log likelihood of how that data... If you want to do instead, take some text to model and say it's any good or not, then this is not true at all. People tend to use these automating metrics like blue and rouge, which measure N-gram overlapped with a reference, but we're not very satisfied. This is recent research in trying to do automating metrics. I should probably speed up a bit. So that's unconditional language models, so they're generating samples of text. This actually isn't a very useful thing to do. What's much more useful is conditional language models, so models which will give us some input, generate use of output. So for example, you can think about things like given a French sentence translated into English, or given document, generate summary, or given a dialogue, generate the next response. Or you can give them a question and output the answer. So these things are called sequence models, because you get given some input sequence, and then you have to generate some output sequence. The first models introduced by Ilya Setskever look kind of like this with recurrent neural networks, where basically you'd have some encoded network, which would read your inputs, produce some vectors, which you'd modestly call the block vector. And then you'd use this to initialize your decoder, which would generate tokens word by word. Again, hopefully you get your theme here, that carrying these kind of bottlenecks and recurrents is not a good thing to do. You also have express models, which you might see everything. So there's a variation of transformer for sequence models. Here we're going to have two stacks, an encoder stack and decoder stack. Basically, the encoder stack is the same as what I showed you before, apart from self-attention isn't masked. So every token, the input can look at every other token in the inputs. The decoder stack will be similar, except that as well as doing self-attention over itself, it's also going to do attention over the complete inputs. So this means that every token in the output has direct connection to every previous token in the output and also to every word in the inputs, which makes these models very expressive and powerful. When transformers were introduced, they got quite a nice improvement on translation scores over the previous recurrent convolutional models. So what we train these models, typically what we do is we rely on label text. So just try a translation system, for example, you try and get lots of manually translated text. It turns out one of the best ways to get this is things like parliament proceedings, because they always translate the European Parliament or the rest of the proceedings in lots of different languages. And then you just use those as inputs and outputs. However, not all languages are represented in the European Parliament. These transformers are very data hungry and more text we can throw at them, the best they will do. So this question of how we use model label text to improve these. So this is actually saying, can we learn without just having input-output pairs? The way we can do this is a technique called back translation, which is quite simple conceptually. That's how I got trained translation system that inputs German and will output English. First of all, we're going to actually do the opposite. We're going to train a reverse translation model, which will give an English output German. Then we can run this model over all the English text we can find. And we can find lots of English text on the Internet and we'll translate it all as German. And that gives us lots more pairs of English and German text. And then we're going to train a forward model that will try and translate this German into English. The nice thing to see here is that it doesn't actually matter how good the initial model is. It doesn't matter if your reverse model is making mistakes. So if your reverse model makes mistakes, then your final train data will contain kind of noisy German translated to clean English, which might even help regularize your model, but probably shouldn't pass its performance when you show it clean German data. What's a bytext? Oh, I'm sorry. A bytext just means a parallel text. So the same sentences in two different languages. OK, thanks. And the nice thing about translation is that the outputs you get are actually always, you know, high quality, because these are the outputs of the system, I think, to you. So you can use the same sentences you found in the wild on the Internet. You're just going to create some noisy inputs that you can use for these pair with these outputs. Could you go back a slide? The third point translate billions of words of English to German. Is that through the reverse translation model? Exactly. Yeah. So you're saying that the reverse translation is because of regularization. It's not just regularization. The really useful thing is it gives you lots of clean output data. So let's say you want to be a good German to English translation model. You kind of need both to understand German, but also be able to write lots of fluent English as well and understand English grammar too. So the reverse translation gives you a way of incorporating tons of additional English data beyond what you have translations for. OK. So it kind of combines translation model with the language model. You can also iterate this procedure too. So you can use that whole set of words I described before to train an atom model and then do this to help you generate better back translations, which you can use to train again. And this can be really helpful. I mean, it helps even in English German, but it's particularly helpful in cases where you don't have much data. So this is on the East to English translation where there isn't a lot of parallel data, but you can get really large improvements by just iterating back translation. And there's a recent work from Fair, which I forgot to add a reference for. And again, here are some results on the English German, showing some good improvements. One of the directions in machine translation people are exploring now is a massively multilingual MTE. So people are trying to not just translate between two languages, but trying to take dozens of hundred languages and try and train a single neural network that can translate between all of these. And if you do this, you start seeing a big improvement, particularly in languages where you don't have much text. Assuming the model is learning some kind of more general language and dependent information. Okay, so the last topic I want to cover in this is really important, which is self supervised learning. So, yeah, I'm just sick of seeing this cake now, but I think it's actually a good image for this. So the idea is really that to learn stuff like most of the information we need is going to be, most of the learning we do has to be unsupervised. So we have huge amounts of text and lots of new labels on it. So we just have a little bit of sort of supervised training data. And that's represented by the cake here being unsupervised learning and the supervised learning just being a little bit of icing on top of the cake. I think actually the recent progress in self supervised learning for NLP has really proved this metaphor to work. Okay, so I'm going to describe a few methods for how you can do self supervised learning for NLP. Just so you can try and get some ideas as to what's actually working. So the first one I'm going to describe is Word2Vec. So the idea of Word2Vec was trying to, I think it's really the first paper that showed got people excited about self supervised learning in NLP. They're having some previous work from a call by Aaron Weston, which also showed some good gains. So the goal here is to try and learn what's called word embedding. So practice space representations for words. And the hope is that by just by looking at unlabeled English text, we can learn something about what these words mean. And so the intuition behind all this is that if two words are close together in the text, then they're likely to have some kind of relationship between each other. So the pre-training task we're going to do is going to be a filling in the blanks task. So in this sentence, I'm going to mask out this word in the middle, which is unicorns, and try and predict what this word should be based on the context. And hope would be that words like unknown, silverhead, or horns will somehow are more likely to occur in the context of a unicorn than they are a word like some other animal. So basically, this is going to be a very simple model where basically we're going to take all these context words, we're just going to apply some linear projection to these and map them all out in a fixed size context. And then just do a softmax over a vocabulary. So it looks a little bit like a convolutional language model. The only difference is we're predicting the word in the middle, not the word at the end. And in practice, this model was just a shallow linear projection and was not a very deep model. Okay. So one of the things that were quite interesting about this was these word embeddings, which show some kind of surprising stretches to them. I'm sure you'll hear this fun, people debate about how meaningful this is. But basically, the claim was that if you took your embedding to the word king, which you train like this, and you subtract your embedding to the word man, then you add the embedding to the word woman, you'll get something that's fairly close to the embedding to the word queen. So somehow, just this kind of unsupervised fill in the blanks learning task isn't using this kind of linear structure with kind of meaningful differences between vectors. Okay, so I mean, this was great. And the really good thing about this was there's a really, really fast thing to do. So you could train this on billions of words of text back in 2013. But there's a big limitation, which is these word embeddings are independent of the context. You get one vector per word in either the capillary. But it doesn't know anything about how that word is placed on other words. And we know that to in language like a sentence is more than just a bag of words, it depends, each word interacts with other words somehow. And these interactions are in some ways a really powerful thing. So in more examples, an obvious example is like ambiguous words. So lots of words can have many different meanings. And these word vectors won't capture that or at best, will just be a supposition of all the meanings. So how do we add context to these? Well, the most obvious way is to do a language model. I think I'm missing a slide here. Basically, what we do is train a conditional language model. Sorry, an unconditional language model that's going to be exactly the kind of model I described earlier in this talk. And then given this language model, the language model will be outputting in the states every time step, predicting the next word. And instead, when we want to sell supervised learning, what we're going to do is replace these outputs with some other output that depends on our task. So the pre-training phase is just going to be predict the next word. But then kind of the icing on the cake of supervised learning will be the predict some other property. So I'll show you an example here for a task called path speech tagging, which is trying to say put some labels in every word here, so turn a light label. Scientist and noun and distinctive as an adjective. But you can actually fit all kinds of tasks into this kind of framework. So, for example, maybe you can fit some language like this is a sentiment analysis task where you're giving some text to predict from an Amazon review and predict the rating. This is a review that says what can I say about this banana slicer that hasn't already been said about the wheel penicillin or the iPhone. And this review got five stars. So here we're going to predict one output from this language model, which is going to be the a token at the end, which is some kind of task specific label. So one of the really nice things about this approach called GPT was that it kind of eliminated test specific modeling. So now suddenly we have one model which we can pre train and we can now fine tune this model to do basically any task we want to do. So it involves classification. So before this, there was actually a few years when people were building all these crazy architectures, so you build a different architecture to do. Build a question answering model or to do a sentiment analysis model or something. Now you train pre train one big model and then it's really easy to fine tune it to do whatever you like. So that was a really big step forward. Unfortunately, that model has kind of the obvious limitation. I said the important thing to do was kind of contextualize words. So like let words know. Build word representations depending on the context. But if you pre train just language model, you can only really condition on your left with context. So your representation for each word necessarily can't depend on the representation for any future words. And that kind of limits what the model can do quite a lot. There's one kind of obvious fix to that, which is the approach taken by Elmo. So Elmo runs training one left right language model, also training the second language model, which operates in the reverse direction. So this is like the last word in the document and it keeps training the previous ones. And then you get word representation by concatenating the output layers of your left right model and your right left model. So this model is in some ways better in that now your word representations can't condition on the left with context and on the right with context. And that's really helpful for lots of tasks. But it's still kind of limited in that you don't really model the interactions in these contexts. So these you just get this shallow concatenation of left representation and right representation. And what you realize to do is have kind of like rich interactions between left context and the right context. And all this brings me to BERT, which is maybe you've heard of, which has made a very big difference in LP. BERT actually looks quite a lot like Word2Vec. It's basically a fillable blanks task. So you take some text, you hide some tokens by masking them out, and then you just try and fill it in the mask. So you get this text like something is a golden something map. It's you fill in the URL. So. The thing I want you to notice is firstly, that actually looks quite a lot like Word2Vec. Word2Vec was also given some text fill in the blanks. The reason it works much better is that in Word2Vec you would just have this linear projection, like encoding the context words, whereas BERT you have them a very large transformer, which will have much more context and much more interactions in that context. So there is a question here. How are context representations maintained when fine tuning for a specific task? How are they maintained? I guess it's not clear they are maintained. So when you fine tune for a particular task, you kind of hope the models learn enough general stuff about language for a pre-training task, and then during the fine tuning probably I guess it's forgetting a lot of this stuff that it doesn't need to solve this particular task. So if you fine tune your sentiment analysis or something, you probably lose a lot of this information during fine tuning. That seems fine. Thanks. Okay. So BERT worked very well. It gave quite large improvements on a bunch of tasks. It was actually achieving the performance of humans, or at least humans approximated by Amazon Mechanical Turk, on a bunch of very important question and answer benchmarks. But BERT was definitely not the end of the story here. BERT got lots of people very excited about self-supervised training. Just to quickly summarize details there. It's a very simple model. It's just going to mask out 15% of the tokens and try and fill in the masks. To build on that, there's some work at Facebook, led by Yihan Lu, which looked at scaling out this. So BERT actually had a second pre-training objective, which we showed didn't actually help. Can I ask a question for the previous slide? So there were three bars. I think I missed one point. The dark blue, what is the dark blue? So we have Amazon Turk. Thank you. Sorry I shouldn't have said that. Dark blue here is previous state of the art. Okay, which was? These models were probably Elmo. So the previous idea was definitely using self-supervised training. But BERT improved Elmo by having this kind of like... I see. And blue actually is a benchmark that we've been creating here at NYU. Exactly, yeah. Students were involved. So yeah, blue is a big benchmark. It's very important. So to beat BERT, it turned out all you had to do was firstly simplify training objective, then just scale it up. So scaling up here means bigger batch sizes, huge numbers of GPUs, more free training text. And then you get very large gains on top of BERT. In fact, much larger gains on top of BERT than BERT had over the previous Elmo work. So yellow here is then this new Roberta model. And actually Roberta, the question answering is superhuman by quite a few pints. And also on this blue benchmark from NYU is also out-performed people. And this isn't really about doing anything smart. It's just taking self-supervised training and doing it well. Right. Why do you say, like if you go on this slide here, so there is a very large improvement between BERT and Roberta in the gloom, but not such a huge change maybe in the squad, which is just a zooming factor. Oh, right, yeah. Maybe it's just a zooming factor, right? Those bars on the left are taller maybe. Yeah, I think maybe the scale is just dosing this way. I think the point is if you compare to human performance, I know BERT was what, 0.6 points better than people, whereas Roberta is 3.5 points better. So by that metric is actually quite a big jump. Yeah. Okay. So I'll just quickly discuss a few of the other things people have been doing self-supervised training. So there's one called ExcelNet. Basically, so in BERT, when you predict all your mass tokens, you predict all the masks conditionally independently. ExcelNet has a trick that lets you predict these mass tokens autoregressively, but in a random order. And they claim some improvements from doing that. It's also SpamBERT, which rather than masking out words, you're going to mask out a sequence of consecutive words. There is Electra, which rather than masking words, we're going to substitute words with sort of similar ones and then have a binary classification problem to tell you which words change or not. There's ALBURTS, which is BERT, but you title weights across layers. Also, XLM and XLMR, which look into doing this multilingualing. It turns out basically you run a BERT pre-training objective, but rather than feeding in English text, feeding in text in every language you can find, it does a great job of learning cross-lingual structure. The key point I want you to take from this is really like, these are all kind of variations of the theme. But in the end, lots of different things work. The important thing is you have big models, you have bidirectional interactions between the words. And if anything, the scale you do this at is the most important thing. So one limitation these models have described is they're only doing kind of text classification problems. But often we'll want to do problems where the output isn't a classification problem, it's actually outputs from more text. So pre-training for sequence-to-sequence modeling. Two papers came out about the same time for this, one of which I was involved in, called BERT and T5. So these models basically are going to pre-train sequence-to-sequence models by denoising text. The pre-training objective looks kind of like BERT. Basically all you're going to do is take some text, corrupt it somehow by applying some kind of masking scheme, and then rather than predict to fill in the blanks, you're going to feed the corrupted text into a sequence-to-sequence model and try and predict the complete output. And this is kind of nice because then you can actually go beyond just masking text and you can apply any random corruption to the input you want. So for example, you can shuffle the sentences, or delete whole phrases, or insert new phrases, and the sequence-to-sequence framework is very flexible. But it turns out just doing simple masking actually works about as well as anything else. And then if you do this as well as doing well on things like Squab and Blue, which are classification benchmarks, you can also get state-of-the-art results on tasks like summarization, so where the output is attached. On the left here we've got some length of document we've fed in. We asked them all to produce some kind of summary of this, and it is a great job, like actually using this context from across the whole document and solve things like coreference. Generally, this project tells us some of the surrounding of the inputs. Okay, we're running out of time. So briefly, I don't think this is the end story, I don't think NLP is now solved. A few open questions, which I think are interesting, including how we integrate background knowledge into this. Do we just want these models to try and memorize the whole Internet, or should we build memory mechanisms somehow? As someone brought up earlier, how do we model long documents? We're typically doing 512 tokens here, how can we do a whole book at once? One unsatisfying thing about this is we have the same model architecture that can solve all kinds of problems, but it tends to not be able to solve all these problems at once, and typically you find a separate model for each task. It would be great if you actually have one model that solves everything. Just kind of related to that, we basically have human-level performance in any task where you have, say, 100,000 labeled examples to be learning from, but can we build models that do well with 1,000 or 10 or 1 labeled examples? And finally, but people bring these questions out to see whether these models are just actually understanding language, or they're just really good at breaking benchmarks. Okay, so to wrap up this lecture, I think the main sort of takeaways from this are these kind of low-biased models like Transformers work great. We shouldn't try to explicitly model linguistic structure. We should have very expressive models, and should have lots of text to let them learn whatever structure they need. So, I think that incorporating words and text is a great unsupervised learning objective. It's crucial to incorporate words in context, in particular bidirectional context. Okay, so that's all I have, so thank you very much for listening. Let's see if we have some questions now. I think there will be several. Thank you, Mike. Yeah, there's a whole bunch of discussions while you were talking. Okay. Links to various papers and explanations of various concepts in the background. Okay. Any more questions? Yeah, I had one. On one of the open questions, it's like understanding whether or not these models are actually understanding the language. What are some ways that exist right now to quantify and measure that, and how would we do that? Yeah, okay. What typically happens is someone says, I know, these models aren't understanding the language. If they could, they'd be able to solve this new task that they're introducing. Then they introduce a new task, which say Bert can't do, and then say Bert isn't doing it. Then next week someone trains a bigger model, and then actually gets human performance on this task. And I think really what's happening is like some people just have intuitions that these neural networks can't be understanding language, and they must just be gaming data sets somehow. And to the extent that these models do well, there must be some kind of weird biases in our data sets that the models can exploit without really understanding anything. And it's definitely true that a lot of our data sets do have biases in them, and it's kind of hard to make ones that don't do it. I think it's a lot of skill, but on the other hand, it's like people are failing to use good counter examples, so what these models can't do. Basically, that's it. Sorry. A good example in recent times was the Winograd schema results, where Winograd schemas are those sentences that are ambiguous, and there is a pronoun that refers to one of the words, and you can't tell which word the pronoun refers to unless you know something about how the word works. So the standard example is the trophy doesn't fit in the suitcase because it's too large, or the trophy doesn't fit in the suitcase because it's too small. And in one case, the pronoun refers to the trophy, and in the other case it refers to the suitcase. And there was a list of those, and people created a data set of those things, and until about two years ago, the best results were around 60% for computers. Humans do 95% or something, but the best artificial systems were getting about 60%, and now I think it's about 90%. Yeah, something like that. Something like that, right. You don't even get any training dates for these? This is just a purely unsupervised problem? Right. And so the question is, you know, it's clear that those systems have learned something about the role of objects and, you know, a little bit about how the word works by just observing statistics about text, but it's relatively superficial. It's not, I mean, you know, as it's pretty obvious when you look at text as generated, you know, we're talking about a unicorn, and then the first sentence is, unicorn has four horns, right, which of course doesn't make sense because unicorns have only one. That's kind of the point of being a unicorn. So, the whole idea of, I mean, the whole problem of learning common sense has not been solved very far from it, but those systems, but they work surprisingly well. It's surprising how far you can go with just, you know, looking at text. Yeah, learning common sense is super hard because in some sense, the things you want to learn are things that aren't written down, like no one ever writes down common sense knowledge. Probably no one ever writes down like a unicorn has exactly one horn and not four. Because everyone knows that. So, probably there are limitations to what you can learn from just looking at text. Can you tell us something about the word, I mean, about some of the work you're doing on grounded language? Sure, yeah. So, I mean, I put nothing about grounding in this, but it's a whole interesting topic. So, I think it's really interesting trying, no one produces text to say people don't use text because it's like, it's my representation of the world. So, one topic that's really interesting is trying to ground dialogue usage and goals. So, rather than trying to build chit chat systems that talk to each other or talk to people to have a conversation, like can you build systems where you try to achieve some goal? We had some work a couple years ago on doing this in the context of negotiations, so trying to, there's two of you which are trying to have a conversation, trying to come to some agreement with each other. And the only way you can come to agreement is by having a dialogue in natural language and there's like some outcomes which are good for you, some which are good for them, you have to find some compromise. And yeah, that's kind of interesting to say, I think, because yeah, it seems like you actually using language for a purpose is going to be really crucial to really understanding things like, I think there's limits to what we learn from just like purely observation use of language, to just purely seeing text out there in the wild that other people wrote, like really to understand things you want to be agents which are using language to try and achieve some goal and interacting with people and seeing what works and learning from that kind of signal as well. Maybe that's it's just the cherry on the icing on the cake or whatever, but I think that's, it is like a part of it is there. You need a cherry. More questions from the audience. Come on guys, don't be too shy. I have a question. The first point on the open questions how should we integrate world knowledge so the way I was thinking about is that these billion parameter Transformers have so much information about the world in them, and then if we try to find you or train this model on some new data, could we like forget some of the earlier concepts that this model had learned, and how would we like quantify what concepts, like, has the model forgotten, apart from the validation set. Does that make sense. Yeah, so probably you will forget it, I mean, probably if you find trainers model to do something that doesn't need world knowledge, you'll happily forget all the knowledge you told it. There's some evidence these models are like memorizing quite a lot of facts so there's a remarkable result recently from this paper. This Google system called T5 which I think has 12 billion parameters. And it's just training themselves the first way and then people. Just fine tune to answer questions, which could questions about anything but you don't show it any. You don't show Wikipedia or something to know you like see what's memorized and you can test how much knowledge is in the model from that. And it's not safe to be out of that but it like, it's kind of scary really good. It's like, it turns out we have 12 billion parameters you train for a long time you can just fit huge numbers of facts into these models. So it's not the most desirable way necessarily to memorize knowledge but seems somewhat effective. Okay. That was it. Thank you everyone for attending. Thank you so much Mike for for giving a guest lecture. It's good to hear that stuff from the hostess mouth. And we'll see see everyone again next next week. Well actually tomorrow right tomorrow we're going to implement in everything from scratch. Don't forget. So tomorrow you get the details on the code side of all of this. Then Monday you'll hear for you'll hear about graph neural nets from the Vibra song. Yeah about graph neural nets and graph knowledge is that is graph knowledge used for language as well somehow because right. I don't know I have not really knowledgeable about this part of the field. You can view to some extent you can view all the those self supervised training in text like word to back Burt etc. They use a graph and the graph is how you know how often towards appear next to each other, or some distance away from each other in the text that's, you know, the graph of similarity between words basically is defined by how far you know how often they appear nearby. Yeah, that's that's when you decide to put them in the input of a neural net, because they appear, you know, within a sequence. So you can think of those, those, those self supervised learning systems as basically a very simple form of graph neural net. But the graph always has the same structure it's always linear, but it always indicates your, your neighbors in the text. All right. All right. Thank you so much, everyone. Have a good evening. Bye bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.0, "text": " We have today Mike Lewis. He's a research scientist at Facebook AI Research working on natural language processing.", "tokens": [50364, 492, 362, 965, 6602, 17412, 13, 634, 311, 257, 2132, 12662, 412, 4384, 7318, 10303, 1364, 322, 3303, 2856, 9007, 13, 50714, 50714, 33606, 415, 390, 257, 2183, 39966, 412, 264, 3535, 295, 6149, 293, 1364, 365, 13044, 1176, 3093, 338, 76, 939, 260, 322, 264, 3164, 12, 6032, 3877, 17630, 13, 51164, 51164, 634, 7365, 702, 14476, 412, 264, 3535, 295, 41215, 293, 2361, 322, 21928, 7316, 304, 293, 14978, 11587, 281, 4361, 45298, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1959996223449707, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.018224911764264107}, {"id": 1, "seek": 0, "start": 7.0, "end": 16.0, "text": " Previously he was a postdoc at the University of Washington and working with Luke Zettelmoyer on the search-based structure prediction.", "tokens": [50364, 492, 362, 965, 6602, 17412, 13, 634, 311, 257, 2132, 12662, 412, 4384, 7318, 10303, 1364, 322, 3303, 2856, 9007, 13, 50714, 50714, 33606, 415, 390, 257, 2183, 39966, 412, 264, 3535, 295, 6149, 293, 1364, 365, 13044, 1176, 3093, 338, 76, 939, 260, 322, 264, 3164, 12, 6032, 3877, 17630, 13, 51164, 51164, 634, 7365, 702, 14476, 412, 264, 3535, 295, 41215, 293, 2361, 322, 21928, 7316, 304, 293, 14978, 11587, 281, 4361, 45298, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1959996223449707, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.018224911764264107}, {"id": 2, "seek": 0, "start": 16.0, "end": 25.0, "text": " He completed his PhD at the University of Edinburgh and based on combining distributional and logical approaches to semantics.", "tokens": [50364, 492, 362, 965, 6602, 17412, 13, 634, 311, 257, 2132, 12662, 412, 4384, 7318, 10303, 1364, 322, 3303, 2856, 9007, 13, 50714, 50714, 33606, 415, 390, 257, 2183, 39966, 412, 264, 3535, 295, 6149, 293, 1364, 365, 13044, 1176, 3093, 338, 76, 939, 260, 322, 264, 3164, 12, 6032, 3877, 17630, 13, 51164, 51164, 634, 7365, 702, 14476, 412, 264, 3535, 295, 41215, 293, 2361, 322, 21928, 7316, 304, 293, 14978, 11587, 281, 4361, 45298, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1959996223449707, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.018224911764264107}, {"id": 3, "seek": 2500, "start": 25.0, "end": 32.0, "text": " He has a Master's degree from the University of Oxford and won the best paper award on EEMNLP in 2016.", "tokens": [50364, 634, 575, 257, 6140, 311, 4314, 490, 264, 3535, 295, 24786, 293, 1582, 264, 1151, 3035, 7130, 322, 462, 6683, 45, 45196, 294, 6549, 13, 50714, 50714, 407, 1553, 3052, 22450, 11, 718, 311, 483, 1409, 365, 965, 311, 5860, 13, 50964, 50964, 1033, 11, 731, 1309, 291, 588, 709, 337, 264, 9339, 11, 28327, 78, 11, 293, 337, 18202, 385, 281, 751, 13, 51214, 51214, 407, 1338, 11, 341, 7991, 445, 2738, 281, 853, 293, 976, 257, 1090, 1496, 295, 1910, 295, 577, 2452, 2539, 307, 1143, 337, 3303, 2856, 9007, 613, 1708, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1731735576282848, "compression_ratio": 1.4835164835164836, "no_speech_prob": 0.0002027242589974776}, {"id": 4, "seek": 2500, "start": 32.0, "end": 37.0, "text": " So without further ado, let's get started with today's presentation.", "tokens": [50364, 634, 575, 257, 6140, 311, 4314, 490, 264, 3535, 295, 24786, 293, 1582, 264, 1151, 3035, 7130, 322, 462, 6683, 45, 45196, 294, 6549, 13, 50714, 50714, 407, 1553, 3052, 22450, 11, 718, 311, 483, 1409, 365, 965, 311, 5860, 13, 50964, 50964, 1033, 11, 731, 1309, 291, 588, 709, 337, 264, 9339, 11, 28327, 78, 11, 293, 337, 18202, 385, 281, 751, 13, 51214, 51214, 407, 1338, 11, 341, 7991, 445, 2738, 281, 853, 293, 976, 257, 1090, 1496, 295, 1910, 295, 577, 2452, 2539, 307, 1143, 337, 3303, 2856, 9007, 613, 1708, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1731735576282848, "compression_ratio": 1.4835164835164836, "no_speech_prob": 0.0002027242589974776}, {"id": 5, "seek": 2500, "start": 37.0, "end": 42.0, "text": " Okay, well thank you very much for the introduction, Alfredo, and for inviting me to talk.", "tokens": [50364, 634, 575, 257, 6140, 311, 4314, 490, 264, 3535, 295, 24786, 293, 1582, 264, 1151, 3035, 7130, 322, 462, 6683, 45, 45196, 294, 6549, 13, 50714, 50714, 407, 1553, 3052, 22450, 11, 718, 311, 483, 1409, 365, 965, 311, 5860, 13, 50964, 50964, 1033, 11, 731, 1309, 291, 588, 709, 337, 264, 9339, 11, 28327, 78, 11, 293, 337, 18202, 385, 281, 751, 13, 51214, 51214, 407, 1338, 11, 341, 7991, 445, 2738, 281, 853, 293, 976, 257, 1090, 1496, 295, 1910, 295, 577, 2452, 2539, 307, 1143, 337, 3303, 2856, 9007, 613, 1708, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1731735576282848, "compression_ratio": 1.4835164835164836, "no_speech_prob": 0.0002027242589974776}, {"id": 6, "seek": 2500, "start": 42.0, "end": 51.0, "text": " So yeah, this lecture just wants to try and give a high level of view of how deep learning is used for natural language processing these days.", "tokens": [50364, 634, 575, 257, 6140, 311, 4314, 490, 264, 3535, 295, 24786, 293, 1582, 264, 1151, 3035, 7130, 322, 462, 6683, 45, 45196, 294, 6549, 13, 50714, 50714, 407, 1553, 3052, 22450, 11, 718, 311, 483, 1409, 365, 965, 311, 5860, 13, 50964, 50964, 1033, 11, 731, 1309, 291, 588, 709, 337, 264, 9339, 11, 28327, 78, 11, 293, 337, 18202, 385, 281, 751, 13, 51214, 51214, 407, 1338, 11, 341, 7991, 445, 2738, 281, 853, 293, 976, 257, 1090, 1496, 295, 1910, 295, 577, 2452, 2539, 307, 1143, 337, 3303, 2856, 9007, 613, 1708, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1731735576282848, "compression_ratio": 1.4835164835164836, "no_speech_prob": 0.0002027242589974776}, {"id": 7, "seek": 5100, "start": 51.0, "end": 58.0, "text": " So I think this per se has been really dramatic progress in NLP in the last few years with applying deep learning.", "tokens": [50364, 407, 286, 519, 341, 680, 369, 575, 668, 534, 12023, 4205, 294, 426, 45196, 294, 264, 1036, 1326, 924, 365, 9275, 2452, 2539, 13, 50714, 50714, 407, 613, 1708, 11, 286, 914, 11, 291, 393, 483, 3479, 12853, 3652, 597, 486, 5258, 37578, 597, 6865, 1952, 1189, 296, 433, 4382, 281, 2306, 7126, 538, 4843, 5105, 3391, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.183345579331921, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.0003841138386633247}, {"id": 8, "seek": 5100, "start": 58.0, "end": 71.0, "text": " So these days, I mean, you can get machine translation systems which will produce translations which blind human erasers prefer to ones produced by professional translators.", "tokens": [50364, 407, 286, 519, 341, 680, 369, 575, 668, 534, 12023, 4205, 294, 426, 45196, 294, 264, 1036, 1326, 924, 365, 9275, 2452, 2539, 13, 50714, 50714, 407, 613, 1708, 11, 286, 914, 11, 291, 393, 483, 3479, 12853, 3652, 597, 486, 5258, 37578, 597, 6865, 1952, 1189, 296, 433, 4382, 281, 2306, 7126, 538, 4843, 5105, 3391, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.183345579331921, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.0003841138386633247}, {"id": 9, "seek": 7100, "start": 71.0, "end": 81.0, "text": " You can make some question answering systems that you ask them a question, give them Wikipedia, and they'll give you answers that are more accurate than the ones that people will give you.", "tokens": [50364, 509, 393, 652, 512, 1168, 13430, 3652, 300, 291, 1029, 552, 257, 1168, 11, 976, 552, 28999, 11, 293, 436, 603, 976, 291, 6338, 300, 366, 544, 8559, 813, 264, 2306, 300, 561, 486, 976, 291, 13, 50864, 50864, 286, 727, 362, 2856, 5245, 300, 393, 8460, 2940, 48910, 295, 40799, 2487, 13, 51264, 51264, 400, 337, 439, 295, 613, 721, 11, 498, 291, 1116, 2351, 385, 1310, 1732, 924, 2057, 11, 286, 1116, 362, 1907, 291, 456, 311, 3122, 572, 636, 604, 295, 341, 307, 1944, 294, 4808, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13845431551020196, "compression_ratio": 1.606177606177606, "no_speech_prob": 0.00011207351053599268}, {"id": 10, "seek": 7100, "start": 81.0, "end": 89.0, "text": " I could have language models that can generate several paragraphs of fluent text.", "tokens": [50364, 509, 393, 652, 512, 1168, 13430, 3652, 300, 291, 1029, 552, 257, 1168, 11, 976, 552, 28999, 11, 293, 436, 603, 976, 291, 6338, 300, 366, 544, 8559, 813, 264, 2306, 300, 561, 486, 976, 291, 13, 50864, 50864, 286, 727, 362, 2856, 5245, 300, 393, 8460, 2940, 48910, 295, 40799, 2487, 13, 51264, 51264, 400, 337, 439, 295, 613, 721, 11, 498, 291, 1116, 2351, 385, 1310, 1732, 924, 2057, 11, 286, 1116, 362, 1907, 291, 456, 311, 3122, 572, 636, 604, 295, 341, 307, 1944, 294, 4808, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13845431551020196, "compression_ratio": 1.606177606177606, "no_speech_prob": 0.00011207351053599268}, {"id": 11, "seek": 7100, "start": 89.0, "end": 96.0, "text": " And for all of these things, if you'd asked me maybe five years ago, I'd have told you there's absolutely no way any of this is possible in 2020.", "tokens": [50364, 509, 393, 652, 512, 1168, 13430, 3652, 300, 291, 1029, 552, 257, 1168, 11, 976, 552, 28999, 11, 293, 436, 603, 976, 291, 6338, 300, 366, 544, 8559, 813, 264, 2306, 300, 561, 486, 976, 291, 13, 50864, 50864, 286, 727, 362, 2856, 5245, 300, 393, 8460, 2940, 48910, 295, 40799, 2487, 13, 51264, 51264, 400, 337, 439, 295, 613, 721, 11, 498, 291, 1116, 2351, 385, 1310, 1732, 924, 2057, 11, 286, 1116, 362, 1907, 291, 456, 311, 3122, 572, 636, 604, 295, 341, 307, 1944, 294, 4808, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13845431551020196, "compression_ratio": 1.606177606177606, "no_speech_prob": 0.00011207351053599268}, {"id": 12, "seek": 9600, "start": 96.0, "end": 103.0, "text": " But there seems to be a few techniques that have been introduced that have really made dramatic differences.", "tokens": [50364, 583, 456, 2544, 281, 312, 257, 1326, 7512, 300, 362, 668, 7268, 300, 362, 534, 1027, 12023, 7300, 13, 50714, 50714, 1485, 534, 1481, 4707, 307, 767, 291, 393, 4584, 439, 295, 613, 721, 365, 6457, 19577, 5245, 13, 51014, 51014, 407, 439, 264, 5245, 337, 439, 613, 9608, 366, 767, 574, 588, 2531, 13, 51214, 51214, 821, 311, 445, 257, 1326, 1090, 1496, 9156, 597, 366, 534, 4420, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10978734815442884, "compression_ratio": 1.6046511627906976, "no_speech_prob": 3.2177154935197905e-05}, {"id": 13, "seek": 9600, "start": 103.0, "end": 109.0, "text": " One really nice property is actually you can achieve all of these things with fairly generic models.", "tokens": [50364, 583, 456, 2544, 281, 312, 257, 1326, 7512, 300, 362, 668, 7268, 300, 362, 534, 1027, 12023, 7300, 13, 50714, 50714, 1485, 534, 1481, 4707, 307, 767, 291, 393, 4584, 439, 295, 613, 721, 365, 6457, 19577, 5245, 13, 51014, 51014, 407, 439, 264, 5245, 337, 439, 613, 9608, 366, 767, 574, 588, 2531, 13, 51214, 51214, 821, 311, 445, 257, 1326, 1090, 1496, 9156, 597, 366, 534, 4420, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10978734815442884, "compression_ratio": 1.6046511627906976, "no_speech_prob": 3.2177154935197905e-05}, {"id": 14, "seek": 9600, "start": 109.0, "end": 113.0, "text": " So all the models for all these tasks are actually look very similar.", "tokens": [50364, 583, 456, 2544, 281, 312, 257, 1326, 7512, 300, 362, 668, 7268, 300, 362, 534, 1027, 12023, 7300, 13, 50714, 50714, 1485, 534, 1481, 4707, 307, 767, 291, 393, 4584, 439, 295, 613, 721, 365, 6457, 19577, 5245, 13, 51014, 51014, 407, 439, 264, 5245, 337, 439, 613, 9608, 366, 767, 574, 588, 2531, 13, 51214, 51214, 821, 311, 445, 257, 1326, 1090, 1496, 9156, 597, 366, 534, 4420, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10978734815442884, "compression_ratio": 1.6046511627906976, "no_speech_prob": 3.2177154935197905e-05}, {"id": 15, "seek": 9600, "start": 113.0, "end": 118.0, "text": " There's just a few high level principles which are really useful.", "tokens": [50364, 583, 456, 2544, 281, 312, 257, 1326, 7512, 300, 362, 668, 7268, 300, 362, 534, 1027, 12023, 7300, 13, 50714, 50714, 1485, 534, 1481, 4707, 307, 767, 291, 393, 4584, 439, 295, 613, 721, 365, 6457, 19577, 5245, 13, 51014, 51014, 407, 439, 264, 5245, 337, 439, 613, 9608, 366, 767, 574, 588, 2531, 13, 51214, 51214, 821, 311, 445, 257, 1326, 1090, 1496, 9156, 597, 366, 534, 4420, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10978734815442884, "compression_ratio": 1.6046511627906976, "no_speech_prob": 3.2177154935197905e-05}, {"id": 16, "seek": 11800, "start": 118.0, "end": 126.0, "text": " So I'm going to cover quite a lot of ground on this quite quickly, so please interrupt me often with questions.", "tokens": [50364, 407, 286, 478, 516, 281, 2060, 1596, 257, 688, 295, 2727, 322, 341, 1596, 2661, 11, 370, 1767, 12729, 385, 2049, 365, 1651, 13, 50764, 50764, 1779, 13, 50864, 50864, 407, 11, 1338, 11, 611, 472, 551, 286, 820, 584, 307, 300, 365, 439, 264, 4205, 321, 600, 668, 2577, 294, 426, 45196, 11, 51314, 51314, 286, 1391, 2731, 264, 1507, 286, 478, 516, 281, 980, 291, 558, 586, 307, 516, 281, 312, 484, 295, 4002, 294, 257, 1064, 420, 732, 13, 51514, 51514, 286, 914, 11, 286, 1454, 321, 2354, 281, 652, 4205, 293, 1507, 6515, 281, 312, 484, 295, 4002, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1716292372373777, "compression_ratio": 1.6722689075630253, "no_speech_prob": 2.795752152451314e-05}, {"id": 17, "seek": 11800, "start": 126.0, "end": 128.0, "text": " Right.", "tokens": [50364, 407, 286, 478, 516, 281, 2060, 1596, 257, 688, 295, 2727, 322, 341, 1596, 2661, 11, 370, 1767, 12729, 385, 2049, 365, 1651, 13, 50764, 50764, 1779, 13, 50864, 50864, 407, 11, 1338, 11, 611, 472, 551, 286, 820, 584, 307, 300, 365, 439, 264, 4205, 321, 600, 668, 2577, 294, 426, 45196, 11, 51314, 51314, 286, 1391, 2731, 264, 1507, 286, 478, 516, 281, 980, 291, 558, 586, 307, 516, 281, 312, 484, 295, 4002, 294, 257, 1064, 420, 732, 13, 51514, 51514, 286, 914, 11, 286, 1454, 321, 2354, 281, 652, 4205, 293, 1507, 6515, 281, 312, 484, 295, 4002, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1716292372373777, "compression_ratio": 1.6722689075630253, "no_speech_prob": 2.795752152451314e-05}, {"id": 18, "seek": 11800, "start": 128.0, "end": 137.0, "text": " So, yeah, also one thing I should say is that with all the progress we've been seeing in NLP,", "tokens": [50364, 407, 286, 478, 516, 281, 2060, 1596, 257, 688, 295, 2727, 322, 341, 1596, 2661, 11, 370, 1767, 12729, 385, 2049, 365, 1651, 13, 50764, 50764, 1779, 13, 50864, 50864, 407, 11, 1338, 11, 611, 472, 551, 286, 820, 584, 307, 300, 365, 439, 264, 4205, 321, 600, 668, 2577, 294, 426, 45196, 11, 51314, 51314, 286, 1391, 2731, 264, 1507, 286, 478, 516, 281, 980, 291, 558, 586, 307, 516, 281, 312, 484, 295, 4002, 294, 257, 1064, 420, 732, 13, 51514, 51514, 286, 914, 11, 286, 1454, 321, 2354, 281, 652, 4205, 293, 1507, 6515, 281, 312, 484, 295, 4002, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1716292372373777, "compression_ratio": 1.6722689075630253, "no_speech_prob": 2.795752152451314e-05}, {"id": 19, "seek": 11800, "start": 137.0, "end": 141.0, "text": " I probably lost the stuff I'm going to tell you right now is going to be out of date in a year or two.", "tokens": [50364, 407, 286, 478, 516, 281, 2060, 1596, 257, 688, 295, 2727, 322, 341, 1596, 2661, 11, 370, 1767, 12729, 385, 2049, 365, 1651, 13, 50764, 50764, 1779, 13, 50864, 50864, 407, 11, 1338, 11, 611, 472, 551, 286, 820, 584, 307, 300, 365, 439, 264, 4205, 321, 600, 668, 2577, 294, 426, 45196, 11, 51314, 51314, 286, 1391, 2731, 264, 1507, 286, 478, 516, 281, 980, 291, 558, 586, 307, 516, 281, 312, 484, 295, 4002, 294, 257, 1064, 420, 732, 13, 51514, 51514, 286, 914, 11, 286, 1454, 321, 2354, 281, 652, 4205, 293, 1507, 6515, 281, 312, 484, 295, 4002, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1716292372373777, "compression_ratio": 1.6722689075630253, "no_speech_prob": 2.795752152451314e-05}, {"id": 20, "seek": 11800, "start": 141.0, "end": 146.0, "text": " I mean, I hope we continue to make progress and stuff continues to be out of date.", "tokens": [50364, 407, 286, 478, 516, 281, 2060, 1596, 257, 688, 295, 2727, 322, 341, 1596, 2661, 11, 370, 1767, 12729, 385, 2049, 365, 1651, 13, 50764, 50764, 1779, 13, 50864, 50864, 407, 11, 1338, 11, 611, 472, 551, 286, 820, 584, 307, 300, 365, 439, 264, 4205, 321, 600, 668, 2577, 294, 426, 45196, 11, 51314, 51314, 286, 1391, 2731, 264, 1507, 286, 478, 516, 281, 980, 291, 558, 586, 307, 516, 281, 312, 484, 295, 4002, 294, 257, 1064, 420, 732, 13, 51514, 51514, 286, 914, 11, 286, 1454, 321, 2354, 281, 652, 4205, 293, 1507, 6515, 281, 312, 484, 295, 4002, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1716292372373777, "compression_ratio": 1.6722689075630253, "no_speech_prob": 2.795752152451314e-05}, {"id": 21, "seek": 14600, "start": 146.0, "end": 155.0, "text": " So as well as explaining some of the models we're using, I want to try and give you some intuitions about what kinds of principles are working out well.", "tokens": [50364, 407, 382, 731, 382, 13468, 512, 295, 264, 5245, 321, 434, 1228, 11, 286, 528, 281, 853, 293, 976, 291, 512, 16224, 626, 466, 437, 3685, 295, 9156, 366, 1364, 484, 731, 13, 50814, 50814, 407, 4696, 291, 362, 257, 857, 544, 295, 257, 8199, 993, 13, 51014, 51014, 1033, 13, 51064, 51064, 407, 264, 700, 4829, 286, 528, 281, 2060, 294, 341, 7991, 307, 2856, 15983, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.1555735402637058, "compression_ratio": 1.4946808510638299, "no_speech_prob": 3.6463679862208664e-05}, {"id": 22, "seek": 14600, "start": 155.0, "end": 159.0, "text": " So hopefully you have a bit more of a sharp life.", "tokens": [50364, 407, 382, 731, 382, 13468, 512, 295, 264, 5245, 321, 434, 1228, 11, 286, 528, 281, 853, 293, 976, 291, 512, 16224, 626, 466, 437, 3685, 295, 9156, 366, 1364, 484, 731, 13, 50814, 50814, 407, 4696, 291, 362, 257, 857, 544, 295, 257, 8199, 993, 13, 51014, 51014, 1033, 13, 51064, 51064, 407, 264, 700, 4829, 286, 528, 281, 2060, 294, 341, 7991, 307, 2856, 15983, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.1555735402637058, "compression_ratio": 1.4946808510638299, "no_speech_prob": 3.6463679862208664e-05}, {"id": 23, "seek": 14600, "start": 159.0, "end": 160.0, "text": " Okay.", "tokens": [50364, 407, 382, 731, 382, 13468, 512, 295, 264, 5245, 321, 434, 1228, 11, 286, 528, 281, 853, 293, 976, 291, 512, 16224, 626, 466, 437, 3685, 295, 9156, 366, 1364, 484, 731, 13, 50814, 50814, 407, 4696, 291, 362, 257, 857, 544, 295, 257, 8199, 993, 13, 51014, 51014, 1033, 13, 51064, 51064, 407, 264, 700, 4829, 286, 528, 281, 2060, 294, 341, 7991, 307, 2856, 15983, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.1555735402637058, "compression_ratio": 1.4946808510638299, "no_speech_prob": 3.6463679862208664e-05}, {"id": 24, "seek": 14600, "start": 160.0, "end": 166.0, "text": " So the first topic I want to cover in this lecture is language modeling.", "tokens": [50364, 407, 382, 731, 382, 13468, 512, 295, 264, 5245, 321, 434, 1228, 11, 286, 528, 281, 853, 293, 976, 291, 512, 16224, 626, 466, 437, 3685, 295, 9156, 366, 1364, 484, 731, 13, 50814, 50814, 407, 4696, 291, 362, 257, 857, 544, 295, 257, 8199, 993, 13, 51014, 51014, 1033, 13, 51064, 51064, 407, 264, 700, 4829, 286, 528, 281, 2060, 294, 341, 7991, 307, 2856, 15983, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.1555735402637058, "compression_ratio": 1.4946808510638299, "no_speech_prob": 3.6463679862208664e-05}, {"id": 25, "seek": 16600, "start": 166.0, "end": 178.0, "text": " Language modeling isn't necessarily a useful task in itself, but it seems to be a really good building block for introducing all the techniques that we'll need later on.", "tokens": [50364, 24445, 15983, 1943, 380, 4725, 257, 4420, 5633, 294, 2564, 11, 457, 309, 2544, 281, 312, 257, 534, 665, 2390, 3461, 337, 15424, 439, 264, 7512, 300, 321, 603, 643, 1780, 322, 13, 50964, 50964, 407, 286, 500, 380, 458, 498, 604, 295, 291, 362, 1612, 341, 949, 13, 51114, 51114, 639, 307, 364, 1365, 490, 257, 2856, 2316, 1219, 26039, 51, 12, 17, 11, 597, 1361, 484, 294, 6071, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07799072901407877, "compression_ratio": 1.4131455399061033, "no_speech_prob": 2.391278394497931e-05}, {"id": 26, "seek": 16600, "start": 178.0, "end": 181.0, "text": " So I don't know if any of you have seen this before.", "tokens": [50364, 24445, 15983, 1943, 380, 4725, 257, 4420, 5633, 294, 2564, 11, 457, 309, 2544, 281, 312, 257, 534, 665, 2390, 3461, 337, 15424, 439, 264, 7512, 300, 321, 603, 643, 1780, 322, 13, 50964, 50964, 407, 286, 500, 380, 458, 498, 604, 295, 291, 362, 1612, 341, 949, 13, 51114, 51114, 639, 307, 364, 1365, 490, 257, 2856, 2316, 1219, 26039, 51, 12, 17, 11, 597, 1361, 484, 294, 6071, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07799072901407877, "compression_ratio": 1.4131455399061033, "no_speech_prob": 2.391278394497931e-05}, {"id": 27, "seek": 16600, "start": 181.0, "end": 189.0, "text": " This is an example from a language model called GPT-2, which came out in 2019.", "tokens": [50364, 24445, 15983, 1943, 380, 4725, 257, 4420, 5633, 294, 2564, 11, 457, 309, 2544, 281, 312, 257, 534, 665, 2390, 3461, 337, 15424, 439, 264, 7512, 300, 321, 603, 643, 1780, 322, 13, 50964, 50964, 407, 286, 500, 380, 458, 498, 604, 295, 291, 362, 1612, 341, 949, 13, 51114, 51114, 639, 307, 364, 1365, 490, 257, 2856, 2316, 1219, 26039, 51, 12, 17, 11, 597, 1361, 484, 294, 6071, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07799072901407877, "compression_ratio": 1.4131455399061033, "no_speech_prob": 2.391278394497931e-05}, {"id": 28, "seek": 18900, "start": 189.0, "end": 201.0, "text": " So what's going on here is that we've got some humans applied some introduction here about scientists finding a herd of unicorns in the Andes that apparently speak English.", "tokens": [50364, 407, 437, 311, 516, 322, 510, 307, 300, 321, 600, 658, 512, 6255, 6456, 512, 9339, 510, 466, 7708, 5006, 257, 29484, 295, 28122, 82, 294, 264, 400, 279, 300, 7970, 1710, 3669, 13, 50964, 50964, 400, 550, 2212, 300, 2487, 11, 436, 600, 2351, 341, 2856, 2316, 281, 2464, 512, 544, 2487, 293, 1507, 411, 300, 13, 51264, 51264, 400, 264, 2487, 321, 483, 510, 307, 767, 1596, 8992, 13, 51414, 51414, 1743, 1518, 390, 534, 12763, 300, 2856, 5245, 727, 589, 341, 731, 1036, 1064, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1203572646431301, "compression_ratio": 1.678714859437751, "no_speech_prob": 5.472288466989994e-05}, {"id": 29, "seek": 18900, "start": 201.0, "end": 207.0, "text": " And then given that text, they've asked this language model to write some more text and stuff like that.", "tokens": [50364, 407, 437, 311, 516, 322, 510, 307, 300, 321, 600, 658, 512, 6255, 6456, 512, 9339, 510, 466, 7708, 5006, 257, 29484, 295, 28122, 82, 294, 264, 400, 279, 300, 7970, 1710, 3669, 13, 50964, 50964, 400, 550, 2212, 300, 2487, 11, 436, 600, 2351, 341, 2856, 2316, 281, 2464, 512, 544, 2487, 293, 1507, 411, 300, 13, 51264, 51264, 400, 264, 2487, 321, 483, 510, 307, 767, 1596, 8992, 13, 51414, 51414, 1743, 1518, 390, 534, 12763, 300, 2856, 5245, 727, 589, 341, 731, 1036, 1064, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1203572646431301, "compression_ratio": 1.678714859437751, "no_speech_prob": 5.472288466989994e-05}, {"id": 30, "seek": 18900, "start": 207.0, "end": 210.0, "text": " And the text we get here is actually quite impressive.", "tokens": [50364, 407, 437, 311, 516, 322, 510, 307, 300, 321, 600, 658, 512, 6255, 6456, 512, 9339, 510, 466, 7708, 5006, 257, 29484, 295, 28122, 82, 294, 264, 400, 279, 300, 7970, 1710, 3669, 13, 50964, 50964, 400, 550, 2212, 300, 2487, 11, 436, 600, 2351, 341, 2856, 2316, 281, 2464, 512, 544, 2487, 293, 1507, 411, 300, 13, 51264, 51264, 400, 264, 2487, 321, 483, 510, 307, 767, 1596, 8992, 13, 51414, 51414, 1743, 1518, 390, 534, 12763, 300, 2856, 5245, 727, 589, 341, 731, 1036, 1064, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1203572646431301, "compression_ratio": 1.678714859437751, "no_speech_prob": 5.472288466989994e-05}, {"id": 31, "seek": 18900, "start": 210.0, "end": 217.0, "text": " Like everyone was really shocked that language models could work this well last year.", "tokens": [50364, 407, 437, 311, 516, 322, 510, 307, 300, 321, 600, 658, 512, 6255, 6456, 512, 9339, 510, 466, 7708, 5006, 257, 29484, 295, 28122, 82, 294, 264, 400, 279, 300, 7970, 1710, 3669, 13, 50964, 50964, 400, 550, 2212, 300, 2487, 11, 436, 600, 2351, 341, 2856, 2316, 281, 2464, 512, 544, 2487, 293, 1507, 411, 300, 13, 51264, 51264, 400, 264, 2487, 321, 483, 510, 307, 767, 1596, 8992, 13, 51414, 51414, 1743, 1518, 390, 534, 12763, 300, 2856, 5245, 727, 589, 341, 731, 1036, 1064, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1203572646431301, "compression_ratio": 1.678714859437751, "no_speech_prob": 5.472288466989994e-05}, {"id": 32, "seek": 21700, "start": 217.0, "end": 224.0, "text": " So you can see the continuing text seems actually quite plausible for a news article about this.", "tokens": [50364, 407, 291, 393, 536, 264, 9289, 2487, 2544, 767, 1596, 39925, 337, 257, 2583, 7222, 466, 341, 13, 50714, 50714, 467, 311, 1417, 466, 28122, 82, 13, 50914, 50914, 440, 2487, 307, 588, 40799, 17570, 267, 804, 13, 51014, 51014, 467, 311, 406, 534, 604, 27108, 456, 13, 51114, 51114, 400, 309, 2544, 281, 411, 7962, 1596, 257, 688, 295, 4365, 411, 264, 1315, 295, 264, 12662, 567, 6941, 552, 13, 51514, 51514, 7580, 439, 341, 307, 3566, 14925, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1746308690025693, "compression_ratio": 1.5622119815668203, "no_speech_prob": 2.7957426937064156e-05}, {"id": 33, "seek": 21700, "start": 224.0, "end": 228.0, "text": " It's talking about unicorns.", "tokens": [50364, 407, 291, 393, 536, 264, 9289, 2487, 2544, 767, 1596, 39925, 337, 257, 2583, 7222, 466, 341, 13, 50714, 50714, 467, 311, 1417, 466, 28122, 82, 13, 50914, 50914, 440, 2487, 307, 588, 40799, 17570, 267, 804, 13, 51014, 51014, 467, 311, 406, 534, 604, 27108, 456, 13, 51114, 51114, 400, 309, 2544, 281, 411, 7962, 1596, 257, 688, 295, 4365, 411, 264, 1315, 295, 264, 12662, 567, 6941, 552, 13, 51514, 51514, 7580, 439, 341, 307, 3566, 14925, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1746308690025693, "compression_ratio": 1.5622119815668203, "no_speech_prob": 2.7957426937064156e-05}, {"id": 34, "seek": 21700, "start": 228.0, "end": 230.0, "text": " The text is very fluent grammatical.", "tokens": [50364, 407, 291, 393, 536, 264, 9289, 2487, 2544, 767, 1596, 39925, 337, 257, 2583, 7222, 466, 341, 13, 50714, 50714, 467, 311, 1417, 466, 28122, 82, 13, 50914, 50914, 440, 2487, 307, 588, 40799, 17570, 267, 804, 13, 51014, 51014, 467, 311, 406, 534, 604, 27108, 456, 13, 51114, 51114, 400, 309, 2544, 281, 411, 7962, 1596, 257, 688, 295, 4365, 411, 264, 1315, 295, 264, 12662, 567, 6941, 552, 13, 51514, 51514, 7580, 439, 341, 307, 3566, 14925, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1746308690025693, "compression_ratio": 1.5622119815668203, "no_speech_prob": 2.7957426937064156e-05}, {"id": 35, "seek": 21700, "start": 230.0, "end": 232.0, "text": " It's not really any flaws there.", "tokens": [50364, 407, 291, 393, 536, 264, 9289, 2487, 2544, 767, 1596, 39925, 337, 257, 2583, 7222, 466, 341, 13, 50714, 50714, 467, 311, 1417, 466, 28122, 82, 13, 50914, 50914, 440, 2487, 307, 588, 40799, 17570, 267, 804, 13, 51014, 51014, 467, 311, 406, 534, 604, 27108, 456, 13, 51114, 51114, 400, 309, 2544, 281, 411, 7962, 1596, 257, 688, 295, 4365, 411, 264, 1315, 295, 264, 12662, 567, 6941, 552, 13, 51514, 51514, 7580, 439, 341, 307, 3566, 14925, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1746308690025693, "compression_ratio": 1.5622119815668203, "no_speech_prob": 2.7957426937064156e-05}, {"id": 36, "seek": 21700, "start": 232.0, "end": 240.0, "text": " And it seems to like invent quite a lot of details like the name of the scientist who discovered them.", "tokens": [50364, 407, 291, 393, 536, 264, 9289, 2487, 2544, 767, 1596, 39925, 337, 257, 2583, 7222, 466, 341, 13, 50714, 50714, 467, 311, 1417, 466, 28122, 82, 13, 50914, 50914, 440, 2487, 307, 588, 40799, 17570, 267, 804, 13, 51014, 51014, 467, 311, 406, 534, 604, 27108, 456, 13, 51114, 51114, 400, 309, 2544, 281, 411, 7962, 1596, 257, 688, 295, 4365, 411, 264, 1315, 295, 264, 12662, 567, 6941, 552, 13, 51514, 51514, 7580, 439, 341, 307, 3566, 14925, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1746308690025693, "compression_ratio": 1.5622119815668203, "no_speech_prob": 2.7957426937064156e-05}, {"id": 37, "seek": 21700, "start": 240.0, "end": 242.0, "text": " Obviously all this is complete nonsense.", "tokens": [50364, 407, 291, 393, 536, 264, 9289, 2487, 2544, 767, 1596, 39925, 337, 257, 2583, 7222, 466, 341, 13, 50714, 50714, 467, 311, 1417, 466, 28122, 82, 13, 50914, 50914, 440, 2487, 307, 588, 40799, 17570, 267, 804, 13, 51014, 51014, 467, 311, 406, 534, 604, 27108, 456, 13, 51114, 51114, 400, 309, 2544, 281, 411, 7962, 1596, 257, 688, 295, 4365, 411, 264, 1315, 295, 264, 12662, 567, 6941, 552, 13, 51514, 51514, 7580, 439, 341, 307, 3566, 14925, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1746308690025693, "compression_ratio": 1.5622119815668203, "no_speech_prob": 2.7957426937064156e-05}, {"id": 38, "seek": 24200, "start": 242.0, "end": 248.0, "text": " Nothing here is true, but also none of this will look like anything the model was ever trained on.", "tokens": [50364, 6693, 510, 307, 2074, 11, 457, 611, 6022, 295, 341, 486, 574, 411, 1340, 264, 2316, 390, 1562, 8895, 322, 13, 50664, 50664, 286, 478, 1238, 988, 341, 18865, 300, 390, 26787, 575, 572, 12512, 4992, 322, 264, 4705, 13, 51014, 51014, 639, 307, 439, 2584, 777, 2856, 11, 457, 309, 311, 439, 767, 1596, 1090, 3125, 2487, 13, 51314, 51314, 286, 478, 406, 516, 281, 1401, 439, 341, 484, 11, 457, 498, 291, 1401, 264, 1472, 295, 264, 7222, 286, 4114, 11, 550, 456, 366, 512, 27108, 11, 457, 436, 434, 1596, 1152, 281, 4008, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14467239379882812, "compression_ratio": 1.6547619047619047, "no_speech_prob": 1.428394898539409e-05}, {"id": 39, "seek": 24200, "start": 248.0, "end": 255.0, "text": " I'm pretty sure this paragraph that was unified has no neighbors anywhere on the internet.", "tokens": [50364, 6693, 510, 307, 2074, 11, 457, 611, 6022, 295, 341, 486, 574, 411, 1340, 264, 2316, 390, 1562, 8895, 322, 13, 50664, 50664, 286, 478, 1238, 988, 341, 18865, 300, 390, 26787, 575, 572, 12512, 4992, 322, 264, 4705, 13, 51014, 51014, 639, 307, 439, 2584, 777, 2856, 11, 457, 309, 311, 439, 767, 1596, 1090, 3125, 2487, 13, 51314, 51314, 286, 478, 406, 516, 281, 1401, 439, 341, 484, 11, 457, 498, 291, 1401, 264, 1472, 295, 264, 7222, 286, 4114, 11, 550, 456, 366, 512, 27108, 11, 457, 436, 434, 1596, 1152, 281, 4008, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14467239379882812, "compression_ratio": 1.6547619047619047, "no_speech_prob": 1.428394898539409e-05}, {"id": 40, "seek": 24200, "start": 255.0, "end": 261.0, "text": " This is all completely new language, but it's all actually quite high quality text.", "tokens": [50364, 6693, 510, 307, 2074, 11, 457, 611, 6022, 295, 341, 486, 574, 411, 1340, 264, 2316, 390, 1562, 8895, 322, 13, 50664, 50664, 286, 478, 1238, 988, 341, 18865, 300, 390, 26787, 575, 572, 12512, 4992, 322, 264, 4705, 13, 51014, 51014, 639, 307, 439, 2584, 777, 2856, 11, 457, 309, 311, 439, 767, 1596, 1090, 3125, 2487, 13, 51314, 51314, 286, 478, 406, 516, 281, 1401, 439, 341, 484, 11, 457, 498, 291, 1401, 264, 1472, 295, 264, 7222, 286, 4114, 11, 550, 456, 366, 512, 27108, 11, 457, 436, 434, 1596, 1152, 281, 4008, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14467239379882812, "compression_ratio": 1.6547619047619047, "no_speech_prob": 1.428394898539409e-05}, {"id": 41, "seek": 24200, "start": 261.0, "end": 268.0, "text": " I'm not going to read all this out, but if you read the rest of the article I wrote, then there are some flaws, but they're quite hard to spot.", "tokens": [50364, 6693, 510, 307, 2074, 11, 457, 611, 6022, 295, 341, 486, 574, 411, 1340, 264, 2316, 390, 1562, 8895, 322, 13, 50664, 50664, 286, 478, 1238, 988, 341, 18865, 300, 390, 26787, 575, 572, 12512, 4992, 322, 264, 4705, 13, 51014, 51014, 639, 307, 439, 2584, 777, 2856, 11, 457, 309, 311, 439, 767, 1596, 1090, 3125, 2487, 13, 51314, 51314, 286, 478, 406, 516, 281, 1401, 439, 341, 484, 11, 457, 498, 291, 1401, 264, 1472, 295, 264, 7222, 286, 4114, 11, 550, 456, 366, 512, 27108, 11, 457, 436, 434, 1596, 1152, 281, 4008, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14467239379882812, "compression_ratio": 1.6547619047619047, "no_speech_prob": 1.428394898539409e-05}, {"id": 42, "seek": 26800, "start": 268.0, "end": 273.0, "text": " And generally this seems to be quite a good language model.", "tokens": [50364, 400, 5101, 341, 2544, 281, 312, 1596, 257, 665, 2856, 2316, 13, 50614, 50614, 407, 286, 478, 516, 281, 853, 281, 855, 291, 264, 733, 295, 7512, 291, 643, 281, 767, 1322, 257, 2856, 2316, 300, 1985, 382, 731, 13, 50964, 50964, 407, 588, 10515, 11, 437, 307, 257, 2856, 2316, 30, 51214, 51214, 407, 257, 2856, 2316, 307, 445, 1936, 10305, 35701, 337, 2487, 13, 51414, 51414, 407, 321, 434, 516, 281, 6269, 257, 8482, 281, 633, 1944, 6798, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11190994487089269, "compression_ratio": 1.6650943396226414, "no_speech_prob": 1.321079889748944e-05}, {"id": 43, "seek": 26800, "start": 273.0, "end": 280.0, "text": " So I'm going to try to show you the kind of techniques you need to actually build a language model that works as well.", "tokens": [50364, 400, 5101, 341, 2544, 281, 312, 1596, 257, 665, 2856, 2316, 13, 50614, 50614, 407, 286, 478, 516, 281, 853, 281, 855, 291, 264, 733, 295, 7512, 291, 643, 281, 767, 1322, 257, 2856, 2316, 300, 1985, 382, 731, 13, 50964, 50964, 407, 588, 10515, 11, 437, 307, 257, 2856, 2316, 30, 51214, 51214, 407, 257, 2856, 2316, 307, 445, 1936, 10305, 35701, 337, 2487, 13, 51414, 51414, 407, 321, 434, 516, 281, 6269, 257, 8482, 281, 633, 1944, 6798, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11190994487089269, "compression_ratio": 1.6650943396226414, "no_speech_prob": 1.321079889748944e-05}, {"id": 44, "seek": 26800, "start": 280.0, "end": 285.0, "text": " So very briefly, what is a language model?", "tokens": [50364, 400, 5101, 341, 2544, 281, 312, 1596, 257, 665, 2856, 2316, 13, 50614, 50614, 407, 286, 478, 516, 281, 853, 281, 855, 291, 264, 733, 295, 7512, 291, 643, 281, 767, 1322, 257, 2856, 2316, 300, 1985, 382, 731, 13, 50964, 50964, 407, 588, 10515, 11, 437, 307, 257, 2856, 2316, 30, 51214, 51214, 407, 257, 2856, 2316, 307, 445, 1936, 10305, 35701, 337, 2487, 13, 51414, 51414, 407, 321, 434, 516, 281, 6269, 257, 8482, 281, 633, 1944, 6798, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11190994487089269, "compression_ratio": 1.6650943396226414, "no_speech_prob": 1.321079889748944e-05}, {"id": 45, "seek": 26800, "start": 285.0, "end": 289.0, "text": " So a language model is just basically density estimation for text.", "tokens": [50364, 400, 5101, 341, 2544, 281, 312, 1596, 257, 665, 2856, 2316, 13, 50614, 50614, 407, 286, 478, 516, 281, 853, 281, 855, 291, 264, 733, 295, 7512, 291, 643, 281, 767, 1322, 257, 2856, 2316, 300, 1985, 382, 731, 13, 50964, 50964, 407, 588, 10515, 11, 437, 307, 257, 2856, 2316, 30, 51214, 51214, 407, 257, 2856, 2316, 307, 445, 1936, 10305, 35701, 337, 2487, 13, 51414, 51414, 407, 321, 434, 516, 281, 6269, 257, 8482, 281, 633, 1944, 6798, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11190994487089269, "compression_ratio": 1.6650943396226414, "no_speech_prob": 1.321079889748944e-05}, {"id": 46, "seek": 26800, "start": 289.0, "end": 295.0, "text": " So we're going to assign a probability to every possible string.", "tokens": [50364, 400, 5101, 341, 2544, 281, 312, 1596, 257, 665, 2856, 2316, 13, 50614, 50614, 407, 286, 478, 516, 281, 853, 281, 855, 291, 264, 733, 295, 7512, 291, 643, 281, 767, 1322, 257, 2856, 2316, 300, 1985, 382, 731, 13, 50964, 50964, 407, 588, 10515, 11, 437, 307, 257, 2856, 2316, 30, 51214, 51214, 407, 257, 2856, 2316, 307, 445, 1936, 10305, 35701, 337, 2487, 13, 51414, 51414, 407, 321, 434, 516, 281, 6269, 257, 8482, 281, 633, 1944, 6798, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11190994487089269, "compression_ratio": 1.6650943396226414, "no_speech_prob": 1.321079889748944e-05}, {"id": 47, "seek": 29500, "start": 295.0, "end": 302.0, "text": " And hopefully we can put some more probability on strings which are fluent English than other strings.", "tokens": [50364, 400, 4696, 321, 393, 829, 512, 544, 8482, 322, 13985, 597, 366, 40799, 3669, 813, 661, 13985, 13, 50714, 50714, 407, 577, 360, 321, 2316, 341, 10305, 30, 50814, 50814, 1042, 11, 2745, 456, 366, 1596, 257, 688, 295, 1944, 16579, 11, 37330, 867, 13, 51164, 51164, 407, 321, 393, 380, 445, 6069, 1508, 23463, 3838, 13, 51364, 51364, 821, 366, 819, 7512, 291, 393, 360, 337, 341, 11, 457, 264, 472, 286, 478, 516, 281, 751, 466, 307, 264, 472, 300, 311, 881, 3743, 356, 1143, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.15463992823725162, "compression_ratio": 1.541501976284585, "no_speech_prob": 7.36706133466214e-05}, {"id": 48, "seek": 29500, "start": 302.0, "end": 304.0, "text": " So how do we model this density?", "tokens": [50364, 400, 4696, 321, 393, 829, 512, 544, 8482, 322, 13985, 597, 366, 40799, 3669, 813, 661, 13985, 13, 50714, 50714, 407, 577, 360, 321, 2316, 341, 10305, 30, 50814, 50814, 1042, 11, 2745, 456, 366, 1596, 257, 688, 295, 1944, 16579, 11, 37330, 867, 13, 51164, 51164, 407, 321, 393, 380, 445, 6069, 1508, 23463, 3838, 13, 51364, 51364, 821, 366, 819, 7512, 291, 393, 360, 337, 341, 11, 457, 264, 472, 286, 478, 516, 281, 751, 466, 307, 264, 472, 300, 311, 881, 3743, 356, 1143, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.15463992823725162, "compression_ratio": 1.541501976284585, "no_speech_prob": 7.36706133466214e-05}, {"id": 49, "seek": 29500, "start": 304.0, "end": 311.0, "text": " Well, obviously there are quite a lot of possible sentences, exponentially many.", "tokens": [50364, 400, 4696, 321, 393, 829, 512, 544, 8482, 322, 13985, 597, 366, 40799, 3669, 813, 661, 13985, 13, 50714, 50714, 407, 577, 360, 321, 2316, 341, 10305, 30, 50814, 50814, 1042, 11, 2745, 456, 366, 1596, 257, 688, 295, 1944, 16579, 11, 37330, 867, 13, 51164, 51164, 407, 321, 393, 380, 445, 6069, 1508, 23463, 3838, 13, 51364, 51364, 821, 366, 819, 7512, 291, 393, 360, 337, 341, 11, 457, 264, 472, 286, 478, 516, 281, 751, 466, 307, 264, 472, 300, 311, 881, 3743, 356, 1143, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.15463992823725162, "compression_ratio": 1.541501976284585, "no_speech_prob": 7.36706133466214e-05}, {"id": 50, "seek": 29500, "start": 311.0, "end": 315.0, "text": " So we can't just predict classifiers directly.", "tokens": [50364, 400, 4696, 321, 393, 829, 512, 544, 8482, 322, 13985, 597, 366, 40799, 3669, 813, 661, 13985, 13, 50714, 50714, 407, 577, 360, 321, 2316, 341, 10305, 30, 50814, 50814, 1042, 11, 2745, 456, 366, 1596, 257, 688, 295, 1944, 16579, 11, 37330, 867, 13, 51164, 51164, 407, 321, 393, 380, 445, 6069, 1508, 23463, 3838, 13, 51364, 51364, 821, 366, 819, 7512, 291, 393, 360, 337, 341, 11, 457, 264, 472, 286, 478, 516, 281, 751, 466, 307, 264, 472, 300, 311, 881, 3743, 356, 1143, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.15463992823725162, "compression_ratio": 1.541501976284585, "no_speech_prob": 7.36706133466214e-05}, {"id": 51, "seek": 29500, "start": 315.0, "end": 320.0, "text": " There are different techniques you can do for this, but the one I'm going to talk about is the one that's most popularly used,", "tokens": [50364, 400, 4696, 321, 393, 829, 512, 544, 8482, 322, 13985, 597, 366, 40799, 3669, 813, 661, 13985, 13, 50714, 50714, 407, 577, 360, 321, 2316, 341, 10305, 30, 50814, 50814, 1042, 11, 2745, 456, 366, 1596, 257, 688, 295, 1944, 16579, 11, 37330, 867, 13, 51164, 51164, 407, 321, 393, 380, 445, 6069, 1508, 23463, 3838, 13, 51364, 51364, 821, 366, 819, 7512, 291, 393, 360, 337, 341, 11, 457, 264, 472, 286, 478, 516, 281, 751, 466, 307, 264, 472, 300, 311, 881, 3743, 356, 1143, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.15463992823725162, "compression_ratio": 1.541501976284585, "no_speech_prob": 7.36706133466214e-05}, {"id": 52, "seek": 32000, "start": 320.0, "end": 326.0, "text": " which is basically to factorize this distribution using the chain rule.", "tokens": [50364, 597, 307, 1936, 281, 5952, 1125, 341, 7316, 1228, 264, 5021, 4978, 13, 50664, 50664, 407, 510, 439, 321, 434, 516, 281, 360, 307, 445, 5952, 1125, 309, 281, 584, 321, 434, 516, 281, 6069, 264, 700, 1349, 11, 550, 6069, 264, 1150, 1349, 11, 976, 552, 264, 700, 11, 550, 264, 2636, 11, 976, 552, 264, 3894, 732, 13, 51514, 51514, 639, 307, 364, 1900, 49246, 11, 309, 1177, 380, 2063, 505, 1340, 281, 360, 309, 411, 341, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14323888506208146, "compression_ratio": 1.7835051546391754, "no_speech_prob": 6.39789504930377e-05}, {"id": 53, "seek": 32000, "start": 326.0, "end": 343.0, "text": " So here all we're going to do is just factorize it to say we're going to predict the first word, then predict the second word, give them the first, then the third, give them the previous two.", "tokens": [50364, 597, 307, 1936, 281, 5952, 1125, 341, 7316, 1228, 264, 5021, 4978, 13, 50664, 50664, 407, 510, 439, 321, 434, 516, 281, 360, 307, 445, 5952, 1125, 309, 281, 584, 321, 434, 516, 281, 6069, 264, 700, 1349, 11, 550, 6069, 264, 1150, 1349, 11, 976, 552, 264, 700, 11, 550, 264, 2636, 11, 976, 552, 264, 3894, 732, 13, 51514, 51514, 639, 307, 364, 1900, 49246, 11, 309, 1177, 380, 2063, 505, 1340, 281, 360, 309, 411, 341, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14323888506208146, "compression_ratio": 1.7835051546391754, "no_speech_prob": 6.39789504930377e-05}, {"id": 54, "seek": 32000, "start": 343.0, "end": 349.0, "text": " This is an exact characterization, it doesn't cost us anything to do it like this.", "tokens": [50364, 597, 307, 1936, 281, 5952, 1125, 341, 7316, 1228, 264, 5021, 4978, 13, 50664, 50664, 407, 510, 439, 321, 434, 516, 281, 360, 307, 445, 5952, 1125, 309, 281, 584, 321, 434, 516, 281, 6069, 264, 700, 1349, 11, 550, 6069, 264, 1150, 1349, 11, 976, 552, 264, 700, 11, 550, 264, 2636, 11, 976, 552, 264, 3894, 732, 13, 51514, 51514, 639, 307, 364, 1900, 49246, 11, 309, 1177, 380, 2063, 505, 1340, 281, 360, 309, 411, 341, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14323888506208146, "compression_ratio": 1.7835051546391754, "no_speech_prob": 6.39789504930377e-05}, {"id": 55, "seek": 34900, "start": 349.0, "end": 354.0, "text": " So really what we've turned is the density estimation problem into a series of classification problems.", "tokens": [50364, 407, 534, 437, 321, 600, 3574, 307, 264, 10305, 35701, 1154, 666, 257, 2638, 295, 21538, 2740, 13, 50614, 50614, 1981, 21538, 2740, 366, 10332, 2212, 257, 3840, 295, 2487, 11, 6069, 264, 958, 1349, 13, 50864, 50864, 400, 300, 311, 516, 281, 312, 257, 6314, 807, 257, 688, 295, 7512, 321, 362, 294, 341, 751, 13, 51114, 51114, 407, 544, 39481, 736, 11, 321, 362, 341, 11, 490, 341, 1365, 286, 4712, 291, 949, 11, 321, 600, 658, 341, 6798, 293, 2316, 5598, 11, 411, 264, 17677, 11, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.18115470229938466, "compression_ratio": 1.6370967741935485, "no_speech_prob": 4.39774630649481e-05}, {"id": 56, "seek": 34900, "start": 354.0, "end": 359.0, "text": " These classification problems are performed given a bunch of text, predict the next word.", "tokens": [50364, 407, 534, 437, 321, 600, 3574, 307, 264, 10305, 35701, 1154, 666, 257, 2638, 295, 21538, 2740, 13, 50614, 50614, 1981, 21538, 2740, 366, 10332, 2212, 257, 3840, 295, 2487, 11, 6069, 264, 958, 1349, 13, 50864, 50864, 400, 300, 311, 516, 281, 312, 257, 6314, 807, 257, 688, 295, 7512, 321, 362, 294, 341, 751, 13, 51114, 51114, 407, 544, 39481, 736, 11, 321, 362, 341, 11, 490, 341, 1365, 286, 4712, 291, 949, 11, 321, 600, 658, 341, 6798, 293, 2316, 5598, 11, 411, 264, 17677, 11, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.18115470229938466, "compression_ratio": 1.6370967741935485, "no_speech_prob": 4.39774630649481e-05}, {"id": 57, "seek": 34900, "start": 359.0, "end": 364.0, "text": " And that's going to be a theme through a lot of techniques we have in this talk.", "tokens": [50364, 407, 534, 437, 321, 600, 3574, 307, 264, 10305, 35701, 1154, 666, 257, 2638, 295, 21538, 2740, 13, 50614, 50614, 1981, 21538, 2740, 366, 10332, 2212, 257, 3840, 295, 2487, 11, 6069, 264, 958, 1349, 13, 50864, 50864, 400, 300, 311, 516, 281, 312, 257, 6314, 807, 257, 688, 295, 7512, 321, 362, 294, 341, 751, 13, 51114, 51114, 407, 544, 39481, 736, 11, 321, 362, 341, 11, 490, 341, 1365, 286, 4712, 291, 949, 11, 321, 600, 658, 341, 6798, 293, 2316, 5598, 11, 411, 264, 17677, 11, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.18115470229938466, "compression_ratio": 1.6370967741935485, "no_speech_prob": 4.39774630649481e-05}, {"id": 58, "seek": 34900, "start": 364.0, "end": 373.0, "text": " So more concretely, we have this, from this example I showed you before, we've got this string and model output, like the sciences,", "tokens": [50364, 407, 534, 437, 321, 600, 3574, 307, 264, 10305, 35701, 1154, 666, 257, 2638, 295, 21538, 2740, 13, 50614, 50614, 1981, 21538, 2740, 366, 10332, 2212, 257, 3840, 295, 2487, 11, 6069, 264, 958, 1349, 13, 50864, 50864, 400, 300, 311, 516, 281, 312, 257, 6314, 807, 257, 688, 295, 7512, 321, 362, 294, 341, 751, 13, 51114, 51114, 407, 544, 39481, 736, 11, 321, 362, 341, 11, 490, 341, 1365, 286, 4712, 291, 949, 11, 321, 600, 658, 341, 6798, 293, 2316, 5598, 11, 411, 264, 17677, 11, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.18115470229938466, "compression_ratio": 1.6370967741935485, "no_speech_prob": 4.39774630649481e-05}, {"id": 59, "seek": 37300, "start": 373.0, "end": 379.0, "text": " name of population after the distinctive form of it, and it's going to predict the next word.", "tokens": [50364, 1315, 295, 4415, 934, 264, 27766, 1254, 295, 309, 11, 293, 309, 311, 516, 281, 6069, 264, 958, 1349, 13, 50664, 50664, 400, 264, 3006, 1349, 294, 341, 1389, 307, 1156, 299, 449, 13, 50914, 50914, 1033, 11, 370, 412, 1090, 1496, 11, 439, 613, 2856, 5245, 574, 746, 411, 341, 13, 51214, 51214, 8537, 11, 321, 4846, 341, 2487, 666, 257, 18161, 3209, 6063, 13, 51414, 51414, 440, 18161, 3209, 486, 4471, 439, 341, 4319, 3911, 257, 8062, 11, 341, 8062, 8855, 264, 958, 1349, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.22444333086956988, "compression_ratio": 1.6577777777777778, "no_speech_prob": 2.2806665583630092e-05}, {"id": 60, "seek": 37300, "start": 379.0, "end": 384.0, "text": " And the correct word in this case is Unicum.", "tokens": [50364, 1315, 295, 4415, 934, 264, 27766, 1254, 295, 309, 11, 293, 309, 311, 516, 281, 6069, 264, 958, 1349, 13, 50664, 50664, 400, 264, 3006, 1349, 294, 341, 1389, 307, 1156, 299, 449, 13, 50914, 50914, 1033, 11, 370, 412, 1090, 1496, 11, 439, 613, 2856, 5245, 574, 746, 411, 341, 13, 51214, 51214, 8537, 11, 321, 4846, 341, 2487, 666, 257, 18161, 3209, 6063, 13, 51414, 51414, 440, 18161, 3209, 486, 4471, 439, 341, 4319, 3911, 257, 8062, 11, 341, 8062, 8855, 264, 958, 1349, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.22444333086956988, "compression_ratio": 1.6577777777777778, "no_speech_prob": 2.2806665583630092e-05}, {"id": 61, "seek": 37300, "start": 384.0, "end": 390.0, "text": " Okay, so at high level, all these language models look something like this.", "tokens": [50364, 1315, 295, 4415, 934, 264, 27766, 1254, 295, 309, 11, 293, 309, 311, 516, 281, 6069, 264, 958, 1349, 13, 50664, 50664, 400, 264, 3006, 1349, 294, 341, 1389, 307, 1156, 299, 449, 13, 50914, 50914, 1033, 11, 370, 412, 1090, 1496, 11, 439, 613, 2856, 5245, 574, 746, 411, 341, 13, 51214, 51214, 8537, 11, 321, 4846, 341, 2487, 666, 257, 18161, 3209, 6063, 13, 51414, 51414, 440, 18161, 3209, 486, 4471, 439, 341, 4319, 3911, 257, 8062, 11, 341, 8062, 8855, 264, 958, 1349, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.22444333086956988, "compression_ratio": 1.6577777777777778, "no_speech_prob": 2.2806665583630092e-05}, {"id": 62, "seek": 37300, "start": 390.0, "end": 394.0, "text": " Basically, we input this text into a neural network somehow.", "tokens": [50364, 1315, 295, 4415, 934, 264, 27766, 1254, 295, 309, 11, 293, 309, 311, 516, 281, 6069, 264, 958, 1349, 13, 50664, 50664, 400, 264, 3006, 1349, 294, 341, 1389, 307, 1156, 299, 449, 13, 50914, 50914, 1033, 11, 370, 412, 1090, 1496, 11, 439, 613, 2856, 5245, 574, 746, 411, 341, 13, 51214, 51214, 8537, 11, 321, 4846, 341, 2487, 666, 257, 18161, 3209, 6063, 13, 51414, 51414, 440, 18161, 3209, 486, 4471, 439, 341, 4319, 3911, 257, 8062, 11, 341, 8062, 8855, 264, 958, 1349, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.22444333086956988, "compression_ratio": 1.6577777777777778, "no_speech_prob": 2.2806665583630092e-05}, {"id": 63, "seek": 37300, "start": 394.0, "end": 401.0, "text": " The neural network will map all this context onto a vector, this vector represents the next word.", "tokens": [50364, 1315, 295, 4415, 934, 264, 27766, 1254, 295, 309, 11, 293, 309, 311, 516, 281, 6069, 264, 958, 1349, 13, 50664, 50664, 400, 264, 3006, 1349, 294, 341, 1389, 307, 1156, 299, 449, 13, 50914, 50914, 1033, 11, 370, 412, 1090, 1496, 11, 439, 613, 2856, 5245, 574, 746, 411, 341, 13, 51214, 51214, 8537, 11, 321, 4846, 341, 2487, 666, 257, 18161, 3209, 6063, 13, 51414, 51414, 440, 18161, 3209, 486, 4471, 439, 341, 4319, 3911, 257, 8062, 11, 341, 8062, 8855, 264, 958, 1349, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.22444333086956988, "compression_ratio": 1.6577777777777778, "no_speech_prob": 2.2806665583630092e-05}, {"id": 64, "seek": 40100, "start": 401.0, "end": 405.0, "text": " And then we're also going to have some big word-abiding matrix.", "tokens": [50364, 400, 550, 321, 434, 611, 516, 281, 362, 512, 955, 1349, 12, 455, 2819, 8141, 13, 50564, 50564, 407, 1349, 12, 455, 2819, 8141, 486, 1936, 5304, 257, 8062, 337, 633, 1944, 1349, 2316, 3255, 577, 281, 5598, 13, 51064, 51064, 400, 550, 439, 321, 643, 281, 360, 307, 14722, 264, 32194, 538, 445, 884, 257, 5893, 1674, 1296, 264, 4319, 8062, 293, 1184, 295, 613, 1349, 18875, 13, 51464, 51464, 400, 321, 603, 483, 257, 22119, 295, 32884, 264, 958, 1349, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1638387811594996, "compression_ratio": 1.6367713004484306, "no_speech_prob": 2.144012978533283e-05}, {"id": 65, "seek": 40100, "start": 405.0, "end": 415.0, "text": " So word-abiding matrix will basically contain a vector for every possible word model knows how to output.", "tokens": [50364, 400, 550, 321, 434, 611, 516, 281, 362, 512, 955, 1349, 12, 455, 2819, 8141, 13, 50564, 50564, 407, 1349, 12, 455, 2819, 8141, 486, 1936, 5304, 257, 8062, 337, 633, 1944, 1349, 2316, 3255, 577, 281, 5598, 13, 51064, 51064, 400, 550, 439, 321, 643, 281, 360, 307, 14722, 264, 32194, 538, 445, 884, 257, 5893, 1674, 1296, 264, 4319, 8062, 293, 1184, 295, 613, 1349, 18875, 13, 51464, 51464, 400, 321, 603, 483, 257, 22119, 295, 32884, 264, 958, 1349, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1638387811594996, "compression_ratio": 1.6367713004484306, "no_speech_prob": 2.144012978533283e-05}, {"id": 66, "seek": 40100, "start": 415.0, "end": 423.0, "text": " And then all we need to do is compute the similarity by just doing a dot product between the context vector and each of these word vectors.", "tokens": [50364, 400, 550, 321, 434, 611, 516, 281, 362, 512, 955, 1349, 12, 455, 2819, 8141, 13, 50564, 50564, 407, 1349, 12, 455, 2819, 8141, 486, 1936, 5304, 257, 8062, 337, 633, 1944, 1349, 2316, 3255, 577, 281, 5598, 13, 51064, 51064, 400, 550, 439, 321, 643, 281, 360, 307, 14722, 264, 32194, 538, 445, 884, 257, 5893, 1674, 1296, 264, 4319, 8062, 293, 1184, 295, 613, 1349, 18875, 13, 51464, 51464, 400, 321, 603, 483, 257, 22119, 295, 32884, 264, 958, 1349, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1638387811594996, "compression_ratio": 1.6367713004484306, "no_speech_prob": 2.144012978533283e-05}, {"id": 67, "seek": 40100, "start": 423.0, "end": 427.0, "text": " And we'll get a likelihood of predicting the next word.", "tokens": [50364, 400, 550, 321, 434, 611, 516, 281, 362, 512, 955, 1349, 12, 455, 2819, 8141, 13, 50564, 50564, 407, 1349, 12, 455, 2819, 8141, 486, 1936, 5304, 257, 8062, 337, 633, 1944, 1349, 2316, 3255, 577, 281, 5598, 13, 51064, 51064, 400, 550, 439, 321, 643, 281, 360, 307, 14722, 264, 32194, 538, 445, 884, 257, 5893, 1674, 1296, 264, 4319, 8062, 293, 1184, 295, 613, 1349, 18875, 13, 51464, 51464, 400, 321, 603, 483, 257, 22119, 295, 32884, 264, 958, 1349, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1638387811594996, "compression_ratio": 1.6367713004484306, "no_speech_prob": 2.144012978533283e-05}, {"id": 68, "seek": 42700, "start": 427.0, "end": 434.0, "text": " Then we'll just trade this model by maximum likelihood in the obvious way.", "tokens": [50364, 1396, 321, 603, 445, 4923, 341, 2316, 538, 6674, 22119, 294, 264, 6322, 636, 13, 50714, 50714, 1033, 11, 370, 286, 914, 11, 264, 2607, 510, 307, 2049, 321, 500, 380, 2028, 365, 2283, 3838, 11, 321, 2028, 365, 721, 1219, 1422, 13832, 420, 754, 4342, 11, 51214, 51214, 457, 439, 264, 15983, 7512, 6222, 264, 912, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.16169624943887034, "compression_ratio": 1.4628571428571429, "no_speech_prob": 2.5462541088927537e-05}, {"id": 69, "seek": 42700, "start": 434.0, "end": 444.0, "text": " Okay, so I mean, the detail here is often we don't deal with words directly, we deal with things called subwords or even characters,", "tokens": [50364, 1396, 321, 603, 445, 4923, 341, 2316, 538, 6674, 22119, 294, 264, 6322, 636, 13, 50714, 50714, 1033, 11, 370, 286, 914, 11, 264, 2607, 510, 307, 2049, 321, 500, 380, 2028, 365, 2283, 3838, 11, 321, 2028, 365, 721, 1219, 1422, 13832, 420, 754, 4342, 11, 51214, 51214, 457, 439, 264, 15983, 7512, 6222, 264, 912, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.16169624943887034, "compression_ratio": 1.4628571428571429, "no_speech_prob": 2.5462541088927537e-05}, {"id": 70, "seek": 42700, "start": 444.0, "end": 449.0, "text": " but all the modeling techniques remain the same.", "tokens": [50364, 1396, 321, 603, 445, 4923, 341, 2316, 538, 6674, 22119, 294, 264, 6322, 636, 13, 50714, 50714, 1033, 11, 370, 286, 914, 11, 264, 2607, 510, 307, 2049, 321, 500, 380, 2028, 365, 2283, 3838, 11, 321, 2028, 365, 721, 1219, 1422, 13832, 420, 754, 4342, 11, 51214, 51214, 457, 439, 264, 15983, 7512, 6222, 264, 912, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.16169624943887034, "compression_ratio": 1.4628571428571429, "no_speech_prob": 2.5462541088927537e-05}, {"id": 71, "seek": 44900, "start": 449.0, "end": 459.0, "text": " All right, so how all the skill here is in this context encoder, how do we build this?", "tokens": [50364, 1057, 558, 11, 370, 577, 439, 264, 5389, 510, 307, 294, 341, 4319, 2058, 19866, 11, 577, 360, 321, 1322, 341, 30, 50864, 50864, 407, 733, 295, 264, 700, 3109, 561, 1890, 337, 341, 390, 1936, 45216, 304, 5245, 13, 51114, 51114, 407, 613, 45216, 304, 5245, 733, 295, 2058, 1429, 341, 31612, 488, 12577, 300, 2856, 733, 295, 575, 341, 12853, 295, 21977, 4707, 294, 309, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.16748723718855116, "compression_ratio": 1.6263157894736842, "no_speech_prob": 9.222091648553032e-06}, {"id": 72, "seek": 44900, "start": 459.0, "end": 464.0, "text": " So kind of the first approach people took for this was basically convolutional models.", "tokens": [50364, 1057, 558, 11, 370, 577, 439, 264, 5389, 510, 307, 294, 341, 4319, 2058, 19866, 11, 577, 360, 321, 1322, 341, 30, 50864, 50864, 407, 733, 295, 264, 700, 3109, 561, 1890, 337, 341, 390, 1936, 45216, 304, 5245, 13, 51114, 51114, 407, 613, 45216, 304, 5245, 733, 295, 2058, 1429, 341, 31612, 488, 12577, 300, 2856, 733, 295, 575, 341, 12853, 295, 21977, 4707, 294, 309, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.16748723718855116, "compression_ratio": 1.6263157894736842, "no_speech_prob": 9.222091648553032e-06}, {"id": 73, "seek": 44900, "start": 464.0, "end": 474.0, "text": " So these convolutional models kind of encode this inductive bias that language kind of has this translation of variance property in it,", "tokens": [50364, 1057, 558, 11, 370, 577, 439, 264, 5389, 510, 307, 294, 341, 4319, 2058, 19866, 11, 577, 360, 321, 1322, 341, 30, 50864, 50864, 407, 733, 295, 264, 700, 3109, 561, 1890, 337, 341, 390, 1936, 45216, 304, 5245, 13, 51114, 51114, 407, 613, 45216, 304, 5245, 733, 295, 2058, 1429, 341, 31612, 488, 12577, 300, 2856, 733, 295, 575, 341, 12853, 295, 21977, 4707, 294, 309, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.16748723718855116, "compression_ratio": 1.6263157894736842, "no_speech_prob": 9.222091648553032e-06}, {"id": 74, "seek": 47400, "start": 474.0, "end": 482.0, "text": " which is interpreted phrase the same way no matter what position it occurs in.", "tokens": [50364, 597, 307, 26749, 9535, 264, 912, 636, 572, 1871, 437, 2535, 309, 11843, 294, 13, 50764, 50764, 407, 257, 7476, 2316, 1062, 574, 411, 341, 11, 689, 1936, 700, 295, 439, 11, 337, 633, 1349, 321, 603, 445, 4471, 309, 281, 512, 8062, 11, 51064, 51064, 597, 307, 445, 257, 574, 1010, 3199, 666, 364, 410, 2819, 8141, 13, 51264, 51264, 407, 264, 1349, 486, 483, 264, 912, 8062, 572, 1871, 437, 4319, 309, 7038, 294, 13, 51514, 51514, 400, 550, 321, 603, 3079, 257, 3840, 295, 7914, 295, 502, 35, 3754, 15892, 6263, 538, 2107, 28263, 1088, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1389582858366125, "compression_ratio": 1.6374501992031874, "no_speech_prob": 1.834123395383358e-05}, {"id": 75, "seek": 47400, "start": 482.0, "end": 488.0, "text": " So a typical model might look like this, where basically first of all, for every word we'll just map it to some vector,", "tokens": [50364, 597, 307, 26749, 9535, 264, 912, 636, 572, 1871, 437, 2535, 309, 11843, 294, 13, 50764, 50764, 407, 257, 7476, 2316, 1062, 574, 411, 341, 11, 689, 1936, 700, 295, 439, 11, 337, 633, 1349, 321, 603, 445, 4471, 309, 281, 512, 8062, 11, 51064, 51064, 597, 307, 445, 257, 574, 1010, 3199, 666, 364, 410, 2819, 8141, 13, 51264, 51264, 407, 264, 1349, 486, 483, 264, 912, 8062, 572, 1871, 437, 4319, 309, 7038, 294, 13, 51514, 51514, 400, 550, 321, 603, 3079, 257, 3840, 295, 7914, 295, 502, 35, 3754, 15892, 6263, 538, 2107, 28263, 1088, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1389582858366125, "compression_ratio": 1.6374501992031874, "no_speech_prob": 1.834123395383358e-05}, {"id": 76, "seek": 47400, "start": 488.0, "end": 492.0, "text": " which is just a lookup table into an abiding matrix.", "tokens": [50364, 597, 307, 26749, 9535, 264, 912, 636, 572, 1871, 437, 2535, 309, 11843, 294, 13, 50764, 50764, 407, 257, 7476, 2316, 1062, 574, 411, 341, 11, 689, 1936, 700, 295, 439, 11, 337, 633, 1349, 321, 603, 445, 4471, 309, 281, 512, 8062, 11, 51064, 51064, 597, 307, 445, 257, 574, 1010, 3199, 666, 364, 410, 2819, 8141, 13, 51264, 51264, 407, 264, 1349, 486, 483, 264, 912, 8062, 572, 1871, 437, 4319, 309, 7038, 294, 13, 51514, 51514, 400, 550, 321, 603, 3079, 257, 3840, 295, 7914, 295, 502, 35, 3754, 15892, 6263, 538, 2107, 28263, 1088, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1389582858366125, "compression_ratio": 1.6374501992031874, "no_speech_prob": 1.834123395383358e-05}, {"id": 77, "seek": 47400, "start": 492.0, "end": 497.0, "text": " So the word will get the same vector no matter what context it appears in.", "tokens": [50364, 597, 307, 26749, 9535, 264, 912, 636, 572, 1871, 437, 2535, 309, 11843, 294, 13, 50764, 50764, 407, 257, 7476, 2316, 1062, 574, 411, 341, 11, 689, 1936, 700, 295, 439, 11, 337, 633, 1349, 321, 603, 445, 4471, 309, 281, 512, 8062, 11, 51064, 51064, 597, 307, 445, 257, 574, 1010, 3199, 666, 364, 410, 2819, 8141, 13, 51264, 51264, 407, 264, 1349, 486, 483, 264, 912, 8062, 572, 1871, 437, 4319, 309, 7038, 294, 13, 51514, 51514, 400, 550, 321, 603, 3079, 257, 3840, 295, 7914, 295, 502, 35, 3754, 15892, 6263, 538, 2107, 28263, 1088, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1389582858366125, "compression_ratio": 1.6374501992031874, "no_speech_prob": 1.834123395383358e-05}, {"id": 78, "seek": 47400, "start": 497.0, "end": 503.0, "text": " And then we'll apply a bunch of layers of 1D convolutions followed by nonlinearities", "tokens": [50364, 597, 307, 26749, 9535, 264, 912, 636, 572, 1871, 437, 2535, 309, 11843, 294, 13, 50764, 50764, 407, 257, 7476, 2316, 1062, 574, 411, 341, 11, 689, 1936, 700, 295, 439, 11, 337, 633, 1349, 321, 603, 445, 4471, 309, 281, 512, 8062, 11, 51064, 51064, 597, 307, 445, 257, 574, 1010, 3199, 666, 364, 410, 2819, 8141, 13, 51264, 51264, 407, 264, 1349, 486, 483, 264, 912, 8062, 572, 1871, 437, 4319, 309, 7038, 294, 13, 51514, 51514, 400, 550, 321, 603, 3079, 257, 3840, 295, 7914, 295, 502, 35, 3754, 15892, 6263, 538, 2107, 28263, 1088, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1389582858366125, "compression_ratio": 1.6374501992031874, "no_speech_prob": 1.834123395383358e-05}, {"id": 79, "seek": 50300, "start": 503.0, "end": 508.0, "text": " until eventually we end up with some kind of vector representing that context,", "tokens": [50364, 1826, 4728, 321, 917, 493, 365, 512, 733, 295, 8062, 13460, 300, 4319, 11, 50614, 50614, 597, 534, 510, 264, 8062, 1355, 437, 820, 264, 958, 1349, 312, 13, 51064, 51064, 400, 613, 5245, 645, 264, 700, 11, 286, 519, 341, 307, 767, 1310, 264, 700, 2856, 2316, 490, 3964, 73, 1004, 294, 16416, 11, 51314, 51314, 264, 700, 18161, 2856, 2316, 13, 51464, 51464, 400, 613, 733, 295, 45216, 304, 11587, 645, 767, 4712, 281, 589, 1596, 731, 538, 4956, 368, 691, 684, 294, 6549, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.22158626933674236, "compression_ratio": 1.6866952789699572, "no_speech_prob": 2.6272520699421875e-05}, {"id": 80, "seek": 50300, "start": 508.0, "end": 517.0, "text": " which really here the vector means what should the next word be.", "tokens": [50364, 1826, 4728, 321, 917, 493, 365, 512, 733, 295, 8062, 13460, 300, 4319, 11, 50614, 50614, 597, 534, 510, 264, 8062, 1355, 437, 820, 264, 958, 1349, 312, 13, 51064, 51064, 400, 613, 5245, 645, 264, 700, 11, 286, 519, 341, 307, 767, 1310, 264, 700, 2856, 2316, 490, 3964, 73, 1004, 294, 16416, 11, 51314, 51314, 264, 700, 18161, 2856, 2316, 13, 51464, 51464, 400, 613, 733, 295, 45216, 304, 11587, 645, 767, 4712, 281, 589, 1596, 731, 538, 4956, 368, 691, 684, 294, 6549, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.22158626933674236, "compression_ratio": 1.6866952789699572, "no_speech_prob": 2.6272520699421875e-05}, {"id": 81, "seek": 50300, "start": 517.0, "end": 522.0, "text": " And these models were the first, I think this is actually maybe the first language model from Benjio in 2003,", "tokens": [50364, 1826, 4728, 321, 917, 493, 365, 512, 733, 295, 8062, 13460, 300, 4319, 11, 50614, 50614, 597, 534, 510, 264, 8062, 1355, 437, 820, 264, 958, 1349, 312, 13, 51064, 51064, 400, 613, 5245, 645, 264, 700, 11, 286, 519, 341, 307, 767, 1310, 264, 700, 2856, 2316, 490, 3964, 73, 1004, 294, 16416, 11, 51314, 51314, 264, 700, 18161, 2856, 2316, 13, 51464, 51464, 400, 613, 733, 295, 45216, 304, 11587, 645, 767, 4712, 281, 589, 1596, 731, 538, 4956, 368, 691, 684, 294, 6549, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.22158626933674236, "compression_ratio": 1.6866952789699572, "no_speech_prob": 2.6272520699421875e-05}, {"id": 82, "seek": 50300, "start": 522.0, "end": 525.0, "text": " the first neural language model.", "tokens": [50364, 1826, 4728, 321, 917, 493, 365, 512, 733, 295, 8062, 13460, 300, 4319, 11, 50614, 50614, 597, 534, 510, 264, 8062, 1355, 437, 820, 264, 958, 1349, 312, 13, 51064, 51064, 400, 613, 5245, 645, 264, 700, 11, 286, 519, 341, 307, 767, 1310, 264, 700, 2856, 2316, 490, 3964, 73, 1004, 294, 16416, 11, 51314, 51314, 264, 700, 18161, 2856, 2316, 13, 51464, 51464, 400, 613, 733, 295, 45216, 304, 11587, 645, 767, 4712, 281, 589, 1596, 731, 538, 4956, 368, 691, 684, 294, 6549, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.22158626933674236, "compression_ratio": 1.6866952789699572, "no_speech_prob": 2.6272520699421875e-05}, {"id": 83, "seek": 50300, "start": 525.0, "end": 531.0, "text": " And these kind of convolutional approaches were actually showed to work quite well by Jan de Vond in 2016,", "tokens": [50364, 1826, 4728, 321, 917, 493, 365, 512, 733, 295, 8062, 13460, 300, 4319, 11, 50614, 50614, 597, 534, 510, 264, 8062, 1355, 437, 820, 264, 958, 1349, 312, 13, 51064, 51064, 400, 613, 5245, 645, 264, 700, 11, 286, 519, 341, 307, 767, 1310, 264, 700, 2856, 2316, 490, 3964, 73, 1004, 294, 16416, 11, 51314, 51314, 264, 700, 18161, 2856, 2316, 13, 51464, 51464, 400, 613, 733, 295, 45216, 304, 11587, 645, 767, 4712, 281, 589, 1596, 731, 538, 4956, 368, 691, 684, 294, 6549, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.22158626933674236, "compression_ratio": 1.6866952789699572, "no_speech_prob": 2.6272520699421875e-05}, {"id": 84, "seek": 53100, "start": 531.0, "end": 537.0, "text": " and then we applied them on deep learning techniques.", "tokens": [50364, 293, 550, 321, 6456, 552, 322, 2452, 2539, 7512, 13, 50664, 50664, 1981, 5245, 366, 588, 2370, 11, 597, 307, 869, 13, 50914, 50914, 286, 1116, 584, 3073, 307, 588, 1021, 337, 2856, 15983, 570, 5850, 321, 764, 2603, 11663, 295, 3097, 1412, 13, 51314, 51314, 639, 472, 1361, 760, 1252, 11, 597, 307, 300, 436, 434, 787, 1075, 281, 534, 4188, 322, 257, 1629, 43276, 488, 2519, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.27880200947800726, "compression_ratio": 1.5217391304347827, "no_speech_prob": 2.1109644876560196e-05}, {"id": 85, "seek": 53100, "start": 537.0, "end": 542.0, "text": " These models are very fast, which is great.", "tokens": [50364, 293, 550, 321, 6456, 552, 322, 2452, 2539, 7512, 13, 50664, 50664, 1981, 5245, 366, 588, 2370, 11, 597, 307, 869, 13, 50914, 50914, 286, 1116, 584, 3073, 307, 588, 1021, 337, 2856, 15983, 570, 5850, 321, 764, 2603, 11663, 295, 3097, 1412, 13, 51314, 51314, 639, 472, 1361, 760, 1252, 11, 597, 307, 300, 436, 434, 787, 1075, 281, 534, 4188, 322, 257, 1629, 43276, 488, 2519, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.27880200947800726, "compression_ratio": 1.5217391304347827, "no_speech_prob": 2.1109644876560196e-05}, {"id": 86, "seek": 53100, "start": 542.0, "end": 550.0, "text": " I'd say speed is very important for language modeling because typically we use huge amounts of training data.", "tokens": [50364, 293, 550, 321, 6456, 552, 322, 2452, 2539, 7512, 13, 50664, 50664, 1981, 5245, 366, 588, 2370, 11, 597, 307, 869, 13, 50914, 50914, 286, 1116, 584, 3073, 307, 588, 1021, 337, 2856, 15983, 570, 5850, 321, 764, 2603, 11663, 295, 3097, 1412, 13, 51314, 51314, 639, 472, 1361, 760, 1252, 11, 597, 307, 300, 436, 434, 787, 1075, 281, 534, 4188, 322, 257, 1629, 43276, 488, 2519, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.27880200947800726, "compression_ratio": 1.5217391304347827, "no_speech_prob": 2.1109644876560196e-05}, {"id": 87, "seek": 53100, "start": 550.0, "end": 556.0, "text": " This one came down side, which is that they're only able to really condition on a certain perceptive field.", "tokens": [50364, 293, 550, 321, 6456, 552, 322, 2452, 2539, 7512, 13, 50664, 50664, 1981, 5245, 366, 588, 2370, 11, 597, 307, 869, 13, 50914, 50914, 286, 1116, 584, 3073, 307, 588, 1021, 337, 2856, 15983, 570, 5850, 321, 764, 2603, 11663, 295, 3097, 1412, 13, 51314, 51314, 639, 472, 1361, 760, 1252, 11, 597, 307, 300, 436, 434, 787, 1075, 281, 534, 4188, 322, 257, 1629, 43276, 488, 2519, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.27880200947800726, "compression_ratio": 1.5217391304347827, "no_speech_prob": 2.1109644876560196e-05}, {"id": 88, "seek": 55600, "start": 556.0, "end": 564.0, "text": " So in this toy example, this word unicorn can only condition the previous five words", "tokens": [50364, 407, 294, 341, 12058, 1365, 11, 341, 1349, 28122, 393, 787, 4188, 264, 3894, 1732, 2283, 50764, 50764, 570, 295, 264, 28256, 11402, 293, 1230, 295, 7914, 321, 434, 1228, 510, 13, 51014, 51014, 7580, 11, 12465, 45216, 304, 5245, 362, 257, 709, 3801, 43276, 488, 2519, 411, 341, 11, 51264, 51264, 457, 3303, 2856, 12258, 281, 362, 4664, 938, 3613, 36606, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.15385479713553812, "compression_ratio": 1.5270935960591132, "no_speech_prob": 1.1476387953734957e-05}, {"id": 89, "seek": 55600, "start": 564.0, "end": 569.0, "text": " because of the kernel width and number of layers we're using here.", "tokens": [50364, 407, 294, 341, 12058, 1365, 11, 341, 1349, 28122, 393, 787, 4188, 264, 3894, 1732, 2283, 50764, 50764, 570, 295, 264, 28256, 11402, 293, 1230, 295, 7914, 321, 434, 1228, 510, 13, 51014, 51014, 7580, 11, 12465, 45216, 304, 5245, 362, 257, 709, 3801, 43276, 488, 2519, 411, 341, 11, 51264, 51264, 457, 3303, 2856, 12258, 281, 362, 4664, 938, 3613, 36606, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.15385479713553812, "compression_ratio": 1.5270935960591132, "no_speech_prob": 1.1476387953734957e-05}, {"id": 90, "seek": 55600, "start": 569.0, "end": 574.0, "text": " Obviously, realistic convolutional models have a much bigger perceptive field like this,", "tokens": [50364, 407, 294, 341, 12058, 1365, 11, 341, 1349, 28122, 393, 787, 4188, 264, 3894, 1732, 2283, 50764, 50764, 570, 295, 264, 28256, 11402, 293, 1230, 295, 7914, 321, 434, 1228, 510, 13, 51014, 51014, 7580, 11, 12465, 45216, 304, 5245, 362, 257, 709, 3801, 43276, 488, 2519, 411, 341, 11, 51264, 51264, 457, 3303, 2856, 12258, 281, 362, 4664, 938, 3613, 36606, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.15385479713553812, "compression_ratio": 1.5270935960591132, "no_speech_prob": 1.1476387953734957e-05}, {"id": 91, "seek": 55600, "start": 574.0, "end": 581.0, "text": " but natural language tends to have extremely long range dependencies.", "tokens": [50364, 407, 294, 341, 12058, 1365, 11, 341, 1349, 28122, 393, 787, 4188, 264, 3894, 1732, 2283, 50764, 50764, 570, 295, 264, 28256, 11402, 293, 1230, 295, 7914, 321, 434, 1228, 510, 13, 51014, 51014, 7580, 11, 12465, 45216, 304, 5245, 362, 257, 709, 3801, 43276, 488, 2519, 411, 341, 11, 51264, 51264, 457, 3303, 2856, 12258, 281, 362, 4664, 938, 3613, 36606, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.15385479713553812, "compression_ratio": 1.5270935960591132, "no_speech_prob": 1.1476387953734957e-05}, {"id": 92, "seek": 58100, "start": 581.0, "end": 586.0, "text": " For an extreme example, you can imagine if you're trying to build a line 12 of a complete book,", "tokens": [50364, 1171, 364, 8084, 1365, 11, 291, 393, 3811, 498, 291, 434, 1382, 281, 1322, 257, 1622, 2272, 295, 257, 3566, 1446, 11, 50614, 50614, 309, 1062, 767, 854, 291, 281, 312, 1075, 281, 4188, 322, 264, 4876, 295, 264, 1446, 412, 439, 565, 4439, 13, 50814, 50814, 400, 2745, 11, 264, 4876, 486, 312, 6779, 295, 5383, 295, 2283, 8046, 13, 51064, 51064, 400, 309, 311, 1596, 1152, 281, 733, 295, 1322, 341, 666, 11, 1322, 257, 45216, 304, 43276, 488, 2519, 300, 311, 955, 1547, 281, 360, 341, 13, 51464, 51464, 2264, 11, 370, 577, 360, 321, 4188, 322, 527, 4319, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14287288166652215, "compression_ratio": 1.6384615384615384, "no_speech_prob": 3.217330959159881e-05}, {"id": 93, "seek": 58100, "start": 586.0, "end": 590.0, "text": " it might actually help you to be able to condition on the title of the book at all time steps.", "tokens": [50364, 1171, 364, 8084, 1365, 11, 291, 393, 3811, 498, 291, 434, 1382, 281, 1322, 257, 1622, 2272, 295, 257, 3566, 1446, 11, 50614, 50614, 309, 1062, 767, 854, 291, 281, 312, 1075, 281, 4188, 322, 264, 4876, 295, 264, 1446, 412, 439, 565, 4439, 13, 50814, 50814, 400, 2745, 11, 264, 4876, 486, 312, 6779, 295, 5383, 295, 2283, 8046, 13, 51064, 51064, 400, 309, 311, 1596, 1152, 281, 733, 295, 1322, 341, 666, 11, 1322, 257, 45216, 304, 43276, 488, 2519, 300, 311, 955, 1547, 281, 360, 341, 13, 51464, 51464, 2264, 11, 370, 577, 360, 321, 4188, 322, 527, 4319, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14287288166652215, "compression_ratio": 1.6384615384615384, "no_speech_prob": 3.217330959159881e-05}, {"id": 94, "seek": 58100, "start": 590.0, "end": 595.0, "text": " And obviously, the title will be hundreds of thousands of words previously.", "tokens": [50364, 1171, 364, 8084, 1365, 11, 291, 393, 3811, 498, 291, 434, 1382, 281, 1322, 257, 1622, 2272, 295, 257, 3566, 1446, 11, 50614, 50614, 309, 1062, 767, 854, 291, 281, 312, 1075, 281, 4188, 322, 264, 4876, 295, 264, 1446, 412, 439, 565, 4439, 13, 50814, 50814, 400, 2745, 11, 264, 4876, 486, 312, 6779, 295, 5383, 295, 2283, 8046, 13, 51064, 51064, 400, 309, 311, 1596, 1152, 281, 733, 295, 1322, 341, 666, 11, 1322, 257, 45216, 304, 43276, 488, 2519, 300, 311, 955, 1547, 281, 360, 341, 13, 51464, 51464, 2264, 11, 370, 577, 360, 321, 4188, 322, 527, 4319, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14287288166652215, "compression_ratio": 1.6384615384615384, "no_speech_prob": 3.217330959159881e-05}, {"id": 95, "seek": 58100, "start": 595.0, "end": 603.0, "text": " And it's quite hard to kind of build this into, build a convolutional perceptive field that's big enough to do this.", "tokens": [50364, 1171, 364, 8084, 1365, 11, 291, 393, 3811, 498, 291, 434, 1382, 281, 1322, 257, 1622, 2272, 295, 257, 3566, 1446, 11, 50614, 50614, 309, 1062, 767, 854, 291, 281, 312, 1075, 281, 4188, 322, 264, 4876, 295, 264, 1446, 412, 439, 565, 4439, 13, 50814, 50814, 400, 2745, 11, 264, 4876, 486, 312, 6779, 295, 5383, 295, 2283, 8046, 13, 51064, 51064, 400, 309, 311, 1596, 1152, 281, 733, 295, 1322, 341, 666, 11, 1322, 257, 45216, 304, 43276, 488, 2519, 300, 311, 955, 1547, 281, 360, 341, 13, 51464, 51464, 2264, 11, 370, 577, 360, 321, 4188, 322, 527, 4319, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14287288166652215, "compression_ratio": 1.6384615384615384, "no_speech_prob": 3.217330959159881e-05}, {"id": 96, "seek": 58100, "start": 603.0, "end": 608.0, "text": " OK, so how do we condition on our context?", "tokens": [50364, 1171, 364, 8084, 1365, 11, 291, 393, 3811, 498, 291, 434, 1382, 281, 1322, 257, 1622, 2272, 295, 257, 3566, 1446, 11, 50614, 50614, 309, 1062, 767, 854, 291, 281, 312, 1075, 281, 4188, 322, 264, 4876, 295, 264, 1446, 412, 439, 565, 4439, 13, 50814, 50814, 400, 2745, 11, 264, 4876, 486, 312, 6779, 295, 5383, 295, 2283, 8046, 13, 51064, 51064, 400, 309, 311, 1596, 1152, 281, 733, 295, 1322, 341, 666, 11, 1322, 257, 45216, 304, 43276, 488, 2519, 300, 311, 955, 1547, 281, 360, 341, 13, 51464, 51464, 2264, 11, 370, 577, 360, 321, 4188, 322, 527, 4319, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14287288166652215, "compression_ratio": 1.6384615384615384, "no_speech_prob": 3.217330959159881e-05}, {"id": 97, "seek": 60800, "start": 608.0, "end": 619.0, "text": " I guess the most popular approach until a couple of years ago was what's called recurrent neural networks.", "tokens": [50364, 286, 2041, 264, 881, 3743, 3109, 1826, 257, 1916, 295, 924, 2057, 390, 437, 311, 1219, 18680, 1753, 18161, 9590, 13, 50914, 50914, 639, 307, 733, 295, 257, 3410, 671, 1596, 15325, 1558, 300, 1936, 633, 565, 1823, 321, 434, 516, 281, 6909, 512, 4368, 51264, 51264, 420, 362, 512, 1785, 1348, 294, 490, 264, 3894, 565, 1823, 11, 597, 8855, 437, 321, 600, 1401, 370, 1400, 13, 51514, 51514, 492, 603, 10432, 264, 1785, 365, 264, 2190, 1349, 321, 600, 1401, 11, 293, 321, 603, 764, 300, 281, 5623, 527, 4368, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12170465213736308, "compression_ratio": 1.6692607003891051, "no_speech_prob": 1.6698973922757432e-05}, {"id": 98, "seek": 60800, "start": 619.0, "end": 626.0, "text": " This is kind of a conceptually quite straightforward idea that basically every time step we're going to maintain some states", "tokens": [50364, 286, 2041, 264, 881, 3743, 3109, 1826, 257, 1916, 295, 924, 2057, 390, 437, 311, 1219, 18680, 1753, 18161, 9590, 13, 50914, 50914, 639, 307, 733, 295, 257, 3410, 671, 1596, 15325, 1558, 300, 1936, 633, 565, 1823, 321, 434, 516, 281, 6909, 512, 4368, 51264, 51264, 420, 362, 512, 1785, 1348, 294, 490, 264, 3894, 565, 1823, 11, 597, 8855, 437, 321, 600, 1401, 370, 1400, 13, 51514, 51514, 492, 603, 10432, 264, 1785, 365, 264, 2190, 1349, 321, 600, 1401, 11, 293, 321, 603, 764, 300, 281, 5623, 527, 4368, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12170465213736308, "compression_ratio": 1.6692607003891051, "no_speech_prob": 1.6698973922757432e-05}, {"id": 99, "seek": 60800, "start": 626.0, "end": 631.0, "text": " or have some state coming in from the previous time step, which represents what we've read so far.", "tokens": [50364, 286, 2041, 264, 881, 3743, 3109, 1826, 257, 1916, 295, 924, 2057, 390, 437, 311, 1219, 18680, 1753, 18161, 9590, 13, 50914, 50914, 639, 307, 733, 295, 257, 3410, 671, 1596, 15325, 1558, 300, 1936, 633, 565, 1823, 321, 434, 516, 281, 6909, 512, 4368, 51264, 51264, 420, 362, 512, 1785, 1348, 294, 490, 264, 3894, 565, 1823, 11, 597, 8855, 437, 321, 600, 1401, 370, 1400, 13, 51514, 51514, 492, 603, 10432, 264, 1785, 365, 264, 2190, 1349, 321, 600, 1401, 11, 293, 321, 603, 764, 300, 281, 5623, 527, 4368, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12170465213736308, "compression_ratio": 1.6692607003891051, "no_speech_prob": 1.6698973922757432e-05}, {"id": 100, "seek": 60800, "start": 631.0, "end": 637.0, "text": " We'll combine the state with the current word we've read, and we'll use that to update our states.", "tokens": [50364, 286, 2041, 264, 881, 3743, 3109, 1826, 257, 1916, 295, 924, 2057, 390, 437, 311, 1219, 18680, 1753, 18161, 9590, 13, 50914, 50914, 639, 307, 733, 295, 257, 3410, 671, 1596, 15325, 1558, 300, 1936, 633, 565, 1823, 321, 434, 516, 281, 6909, 512, 4368, 51264, 51264, 420, 362, 512, 1785, 1348, 294, 490, 264, 3894, 565, 1823, 11, 597, 8855, 437, 321, 600, 1401, 370, 1400, 13, 51514, 51514, 492, 603, 10432, 264, 1785, 365, 264, 2190, 1349, 321, 600, 1401, 11, 293, 321, 603, 764, 300, 281, 5623, 527, 4368, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12170465213736308, "compression_ratio": 1.6692607003891051, "no_speech_prob": 1.6698973922757432e-05}, {"id": 101, "seek": 63700, "start": 637.0, "end": 645.0, "text": " And then we'll just iterate this for as many time steps as we need.", "tokens": [50364, 400, 550, 321, 603, 445, 44497, 341, 337, 382, 867, 565, 4439, 382, 321, 643, 13, 50764, 50764, 407, 286, 519, 341, 2544, 411, 1596, 257, 3303, 2316, 295, 3760, 13, 50914, 50914, 286, 914, 11, 286, 519, 337, 264, 881, 644, 11, 561, 733, 295, 1401, 1411, 281, 558, 293, 6909, 512, 733, 295, 4368, 382, 436, 352, 13, 51264, 51264, 1711, 1935, 294, 8665, 382, 731, 11, 291, 393, 2316, 517, 18767, 292, 4319, 411, 341, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09745288756956537, "compression_ratio": 1.5643564356435644, "no_speech_prob": 9.079738447326235e-06}, {"id": 102, "seek": 63700, "start": 645.0, "end": 648.0, "text": " So I think this seems like quite a natural model of reading.", "tokens": [50364, 400, 550, 321, 603, 445, 44497, 341, 337, 382, 867, 565, 4439, 382, 321, 643, 13, 50764, 50764, 407, 286, 519, 341, 2544, 411, 1596, 257, 3303, 2316, 295, 3760, 13, 50914, 50914, 286, 914, 11, 286, 519, 337, 264, 881, 644, 11, 561, 733, 295, 1401, 1411, 281, 558, 293, 6909, 512, 733, 295, 4368, 382, 436, 352, 13, 51264, 51264, 1711, 1935, 294, 8665, 382, 731, 11, 291, 393, 2316, 517, 18767, 292, 4319, 411, 341, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09745288756956537, "compression_ratio": 1.5643564356435644, "no_speech_prob": 9.079738447326235e-06}, {"id": 103, "seek": 63700, "start": 648.0, "end": 655.0, "text": " I mean, I think for the most part, people kind of read left to right and maintain some kind of states as they go.", "tokens": [50364, 400, 550, 321, 603, 445, 44497, 341, 337, 382, 867, 565, 4439, 382, 321, 643, 13, 50764, 50764, 407, 286, 519, 341, 2544, 411, 1596, 257, 3303, 2316, 295, 3760, 13, 50914, 50914, 286, 914, 11, 286, 519, 337, 264, 881, 644, 11, 561, 733, 295, 1401, 1411, 281, 558, 293, 6909, 512, 733, 295, 4368, 382, 436, 352, 13, 51264, 51264, 1711, 1935, 294, 8665, 382, 731, 11, 291, 393, 2316, 517, 18767, 292, 4319, 411, 341, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09745288756956537, "compression_ratio": 1.5643564356435644, "no_speech_prob": 9.079738447326235e-06}, {"id": 104, "seek": 63700, "start": 655.0, "end": 663.0, "text": " At least in principle as well, you can model unbounded context like this.", "tokens": [50364, 400, 550, 321, 603, 445, 44497, 341, 337, 382, 867, 565, 4439, 382, 321, 643, 13, 50764, 50764, 407, 286, 519, 341, 2544, 411, 1596, 257, 3303, 2316, 295, 3760, 13, 50914, 50914, 286, 914, 11, 286, 519, 337, 264, 881, 644, 11, 561, 733, 295, 1401, 1411, 281, 558, 293, 6909, 512, 733, 295, 4368, 382, 436, 352, 13, 51264, 51264, 1711, 1935, 294, 8665, 382, 731, 11, 291, 393, 2316, 517, 18767, 292, 4319, 411, 341, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09745288756956537, "compression_ratio": 1.5643564356435644, "no_speech_prob": 9.079738447326235e-06}, {"id": 105, "seek": 66300, "start": 663.0, "end": 672.0, "text": " At least in principle, like the title of the book would affect the hidden states of the last word of the book.", "tokens": [50364, 1711, 1935, 294, 8665, 11, 411, 264, 4876, 295, 264, 1446, 576, 3345, 264, 7633, 4368, 295, 264, 1036, 1349, 295, 264, 1446, 13, 50814, 50814, 682, 3124, 11, 456, 366, 512, 6457, 4776, 2663, 365, 341, 2316, 13, 51114, 51114, 20042, 11, 456, 311, 733, 295, 572, 1737, 6349, 510, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.1317545005253383, "compression_ratio": 1.4580645161290322, "no_speech_prob": 1.2803493518731557e-05}, {"id": 106, "seek": 66300, "start": 672.0, "end": 678.0, "text": " In practice, there are some fairly significant issues with this model.", "tokens": [50364, 1711, 1935, 294, 8665, 11, 411, 264, 4876, 295, 264, 1446, 576, 3345, 264, 7633, 4368, 295, 264, 1036, 1349, 295, 264, 1446, 13, 50814, 50814, 682, 3124, 11, 456, 366, 512, 6457, 4776, 2663, 365, 341, 2316, 13, 51114, 51114, 20042, 11, 456, 311, 733, 295, 572, 1737, 6349, 510, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.1317545005253383, "compression_ratio": 1.4580645161290322, "no_speech_prob": 1.2803493518731557e-05}, {"id": 107, "seek": 66300, "start": 678.0, "end": 683.0, "text": " Firstly, there's kind of no free lunch here.", "tokens": [50364, 1711, 1935, 294, 8665, 11, 411, 264, 4876, 295, 264, 1446, 576, 3345, 264, 7633, 4368, 295, 264, 1036, 1349, 295, 264, 1446, 13, 50814, 50814, 682, 3124, 11, 456, 366, 512, 6457, 4776, 2663, 365, 341, 2316, 13, 51114, 51114, 20042, 11, 456, 311, 733, 295, 572, 1737, 6349, 510, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.1317545005253383, "compression_ratio": 1.4580645161290322, "no_speech_prob": 1.2803493518731557e-05}, {"id": 108, "seek": 68300, "start": 683.0, "end": 695.0, "text": " So by the time the effect of maintaining the state is we're going to compress the whole history of the document reading into a single vector at each time step.", "tokens": [50364, 407, 538, 264, 565, 264, 1802, 295, 14916, 264, 1785, 307, 321, 434, 516, 281, 14778, 264, 1379, 2503, 295, 264, 4166, 3760, 666, 257, 2167, 8062, 412, 1184, 565, 1823, 13, 50964, 50964, 400, 550, 1564, 291, 600, 1401, 257, 1349, 11, 291, 393, 1128, 574, 412, 309, 797, 13, 51114, 51114, 509, 362, 281, 27478, 300, 13, 51214, 51214, 400, 300, 1355, 291, 362, 281, 767, 362, 281, 941, 335, 257, 2603, 2372, 295, 1589, 666, 257, 2167, 8062, 13, 51564, 51564, 400, 341, 307, 1936, 733, 295, 257, 44641, 547, 294, 264, 2316, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13546285534849262, "compression_ratio": 1.7041666666666666, "no_speech_prob": 5.59365980734583e-06}, {"id": 109, "seek": 68300, "start": 695.0, "end": 698.0, "text": " And then once you've read a word, you can never look at it again.", "tokens": [50364, 407, 538, 264, 565, 264, 1802, 295, 14916, 264, 1785, 307, 321, 434, 516, 281, 14778, 264, 1379, 2503, 295, 264, 4166, 3760, 666, 257, 2167, 8062, 412, 1184, 565, 1823, 13, 50964, 50964, 400, 550, 1564, 291, 600, 1401, 257, 1349, 11, 291, 393, 1128, 574, 412, 309, 797, 13, 51114, 51114, 509, 362, 281, 27478, 300, 13, 51214, 51214, 400, 300, 1355, 291, 362, 281, 767, 362, 281, 941, 335, 257, 2603, 2372, 295, 1589, 666, 257, 2167, 8062, 13, 51564, 51564, 400, 341, 307, 1936, 733, 295, 257, 44641, 547, 294, 264, 2316, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13546285534849262, "compression_ratio": 1.7041666666666666, "no_speech_prob": 5.59365980734583e-06}, {"id": 110, "seek": 68300, "start": 698.0, "end": 700.0, "text": " You have to memorize that.", "tokens": [50364, 407, 538, 264, 565, 264, 1802, 295, 14916, 264, 1785, 307, 321, 434, 516, 281, 14778, 264, 1379, 2503, 295, 264, 4166, 3760, 666, 257, 2167, 8062, 412, 1184, 565, 1823, 13, 50964, 50964, 400, 550, 1564, 291, 600, 1401, 257, 1349, 11, 291, 393, 1128, 574, 412, 309, 797, 13, 51114, 51114, 509, 362, 281, 27478, 300, 13, 51214, 51214, 400, 300, 1355, 291, 362, 281, 767, 362, 281, 941, 335, 257, 2603, 2372, 295, 1589, 666, 257, 2167, 8062, 13, 51564, 51564, 400, 341, 307, 1936, 733, 295, 257, 44641, 547, 294, 264, 2316, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13546285534849262, "compression_ratio": 1.7041666666666666, "no_speech_prob": 5.59365980734583e-06}, {"id": 111, "seek": 68300, "start": 700.0, "end": 707.0, "text": " And that means you have to actually have to cram a huge amount of information into a single vector.", "tokens": [50364, 407, 538, 264, 565, 264, 1802, 295, 14916, 264, 1785, 307, 321, 434, 516, 281, 14778, 264, 1379, 2503, 295, 264, 4166, 3760, 666, 257, 2167, 8062, 412, 1184, 565, 1823, 13, 50964, 50964, 400, 550, 1564, 291, 600, 1401, 257, 1349, 11, 291, 393, 1128, 574, 412, 309, 797, 13, 51114, 51114, 509, 362, 281, 27478, 300, 13, 51214, 51214, 400, 300, 1355, 291, 362, 281, 767, 362, 281, 941, 335, 257, 2603, 2372, 295, 1589, 666, 257, 2167, 8062, 13, 51564, 51564, 400, 341, 307, 1936, 733, 295, 257, 44641, 547, 294, 264, 2316, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13546285534849262, "compression_ratio": 1.7041666666666666, "no_speech_prob": 5.59365980734583e-06}, {"id": 112, "seek": 68300, "start": 707.0, "end": 710.0, "text": " And this is basically kind of a bottleneck in the model.", "tokens": [50364, 407, 538, 264, 565, 264, 1802, 295, 14916, 264, 1785, 307, 321, 434, 516, 281, 14778, 264, 1379, 2503, 295, 264, 4166, 3760, 666, 257, 2167, 8062, 412, 1184, 565, 1823, 13, 50964, 50964, 400, 550, 1564, 291, 600, 1401, 257, 1349, 11, 291, 393, 1128, 574, 412, 309, 797, 13, 51114, 51114, 509, 362, 281, 27478, 300, 13, 51214, 51214, 400, 300, 1355, 291, 362, 281, 767, 362, 281, 941, 335, 257, 2603, 2372, 295, 1589, 666, 257, 2167, 8062, 13, 51564, 51564, 400, 341, 307, 1936, 733, 295, 257, 44641, 547, 294, 264, 2316, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13546285534849262, "compression_ratio": 1.7041666666666666, "no_speech_prob": 5.59365980734583e-06}, {"id": 113, "seek": 71000, "start": 710.0, "end": 715.0, "text": " Certainly, there's a question about how much information you can really store in one vector.", "tokens": [50364, 16628, 11, 456, 311, 257, 1168, 466, 577, 709, 1589, 291, 393, 534, 3531, 294, 472, 8062, 13, 50614, 50614, 583, 611, 11, 309, 311, 733, 295, 257, 8496, 2539, 1154, 281, 291, 13, 50964, 50964, 663, 311, 264, 2734, 1219, 264, 1740, 282, 507, 16235, 1154, 11, 689, 309, 1355, 300, 633, 565, 291, 352, 807, 472, 295, 613, 4439, 11, 550, 291, 603, 362, 512, 733, 295, 2107, 12, 1889, 17409, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.22170558342566857, "compression_ratio": 1.5336538461538463, "no_speech_prob": 2.1778680093120784e-05}, {"id": 114, "seek": 71000, "start": 715.0, "end": 722.0, "text": " But also, it's kind of a practical learning problem to you.", "tokens": [50364, 16628, 11, 456, 311, 257, 1168, 466, 577, 709, 1589, 291, 393, 534, 3531, 294, 472, 8062, 13, 50614, 50614, 583, 611, 11, 309, 311, 733, 295, 257, 8496, 2539, 1154, 281, 291, 13, 50964, 50964, 663, 311, 264, 2734, 1219, 264, 1740, 282, 507, 16235, 1154, 11, 689, 309, 1355, 300, 633, 565, 291, 352, 807, 472, 295, 613, 4439, 11, 550, 291, 603, 362, 512, 733, 295, 2107, 12, 1889, 17409, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.22170558342566857, "compression_ratio": 1.5336538461538463, "no_speech_prob": 2.1778680093120784e-05}, {"id": 115, "seek": 71000, "start": 722.0, "end": 736.0, "text": " That's the issue called the profanity gradient problem, where it means that every time you go through one of these steps, then you'll have some kind of non-linearity,", "tokens": [50364, 16628, 11, 456, 311, 257, 1168, 466, 577, 709, 1589, 291, 393, 534, 3531, 294, 472, 8062, 13, 50614, 50614, 583, 611, 11, 309, 311, 733, 295, 257, 8496, 2539, 1154, 281, 291, 13, 50964, 50964, 663, 311, 264, 2734, 1219, 264, 1740, 282, 507, 16235, 1154, 11, 689, 309, 1355, 300, 633, 565, 291, 352, 807, 472, 295, 613, 4439, 11, 550, 291, 603, 362, 512, 733, 295, 2107, 12, 1889, 17409, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.22170558342566857, "compression_ratio": 1.5336538461538463, "no_speech_prob": 2.1778680093120784e-05}, {"id": 116, "seek": 73600, "start": 736.0, "end": 742.0, "text": " which will mean that the effect of words in the past will kind of get exponentially smaller each time step.", "tokens": [50364, 597, 486, 914, 300, 264, 1802, 295, 2283, 294, 264, 1791, 486, 733, 295, 483, 37330, 4356, 1184, 565, 1823, 13, 50664, 50664, 663, 1355, 300, 1564, 291, 362, 572, 16235, 281, 257, 1729, 1349, 294, 264, 1791, 11, 309, 311, 588, 1152, 281, 767, 5800, 1466, 1780, 322, 300, 1349, 390, 26491, 13, 51214, 51214, 1485, 2572, 2734, 286, 528, 281, 2152, 365, 45702, 45, 82, 307, 300, 436, 434, 767, 1596, 2964, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1168102312691604, "compression_ratio": 1.5943396226415094, "no_speech_prob": 2.6684103431762196e-05}, {"id": 117, "seek": 73600, "start": 742.0, "end": 753.0, "text": " That means that once you have no gradient to a particular word in the past, it's very hard to actually suddenly learn later on that word was imposed.", "tokens": [50364, 597, 486, 914, 300, 264, 1802, 295, 2283, 294, 264, 1791, 486, 733, 295, 483, 37330, 4356, 1184, 565, 1823, 13, 50664, 50664, 663, 1355, 300, 1564, 291, 362, 572, 16235, 281, 257, 1729, 1349, 294, 264, 1791, 11, 309, 311, 588, 1152, 281, 767, 5800, 1466, 1780, 322, 300, 1349, 390, 26491, 13, 51214, 51214, 1485, 2572, 2734, 286, 528, 281, 2152, 365, 45702, 45, 82, 307, 300, 436, 434, 767, 1596, 2964, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1168102312691604, "compression_ratio": 1.5943396226415094, "no_speech_prob": 2.6684103431762196e-05}, {"id": 118, "seek": 73600, "start": 753.0, "end": 762.0, "text": " One final issue I want to mention with RNNs is that they're actually quite slow.", "tokens": [50364, 597, 486, 914, 300, 264, 1802, 295, 2283, 294, 264, 1791, 486, 733, 295, 483, 37330, 4356, 1184, 565, 1823, 13, 50664, 50664, 663, 1355, 300, 1564, 291, 362, 572, 16235, 281, 257, 1729, 1349, 294, 264, 1791, 11, 309, 311, 588, 1152, 281, 767, 5800, 1466, 1780, 322, 300, 1349, 390, 26491, 13, 51214, 51214, 1485, 2572, 2734, 286, 528, 281, 2152, 365, 45702, 45, 82, 307, 300, 436, 434, 767, 1596, 2964, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1168102312691604, "compression_ratio": 1.5943396226415094, "no_speech_prob": 2.6684103431762196e-05}, {"id": 119, "seek": 76200, "start": 762.0, "end": 768.0, "text": " The reason for this is particularly for training.", "tokens": [50364, 440, 1778, 337, 341, 307, 4098, 337, 3097, 13, 50664, 50664, 407, 264, 1778, 307, 300, 294, 1668, 281, 1322, 428, 1785, 337, 257, 1729, 1349, 11, 291, 767, 362, 281, 1322, 428, 1785, 337, 633, 3894, 1349, 700, 13, 51314, 51314, 663, 1355, 4476, 291, 362, 257, 955, 337, 6367, 300, 311, 516, 670, 428, 2302, 4166, 13, 51514, 51514, 400, 264, 2854, 428, 4166, 307, 11, 264, 3801, 341, 337, 6367, 307, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08299673659892022, "compression_ratio": 1.7577319587628866, "no_speech_prob": 3.94346789107658e-05}, {"id": 120, "seek": 76200, "start": 768.0, "end": 781.0, "text": " So the reason is that in order to build your state for a particular word, you actually have to build your state for every previous word first.", "tokens": [50364, 440, 1778, 337, 341, 307, 4098, 337, 3097, 13, 50664, 50664, 407, 264, 1778, 307, 300, 294, 1668, 281, 1322, 428, 1785, 337, 257, 1729, 1349, 11, 291, 767, 362, 281, 1322, 428, 1785, 337, 633, 3894, 1349, 700, 13, 51314, 51314, 663, 1355, 4476, 291, 362, 257, 955, 337, 6367, 300, 311, 516, 670, 428, 2302, 4166, 13, 51514, 51514, 400, 264, 2854, 428, 4166, 307, 11, 264, 3801, 341, 337, 6367, 307, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08299673659892022, "compression_ratio": 1.7577319587628866, "no_speech_prob": 3.94346789107658e-05}, {"id": 121, "seek": 76200, "start": 781.0, "end": 785.0, "text": " That means essentially you have a big for loop that's going over your entire document.", "tokens": [50364, 440, 1778, 337, 341, 307, 4098, 337, 3097, 13, 50664, 50664, 407, 264, 1778, 307, 300, 294, 1668, 281, 1322, 428, 1785, 337, 257, 1729, 1349, 11, 291, 767, 362, 281, 1322, 428, 1785, 337, 633, 3894, 1349, 700, 13, 51314, 51314, 663, 1355, 4476, 291, 362, 257, 955, 337, 6367, 300, 311, 516, 670, 428, 2302, 4166, 13, 51514, 51514, 400, 264, 2854, 428, 4166, 307, 11, 264, 3801, 341, 337, 6367, 307, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08299673659892022, "compression_ratio": 1.7577319587628866, "no_speech_prob": 3.94346789107658e-05}, {"id": 122, "seek": 76200, "start": 785.0, "end": 789.0, "text": " And the longer your document is, the bigger this for loop is.", "tokens": [50364, 440, 1778, 337, 341, 307, 4098, 337, 3097, 13, 50664, 50664, 407, 264, 1778, 307, 300, 294, 1668, 281, 1322, 428, 1785, 337, 257, 1729, 1349, 11, 291, 767, 362, 281, 1322, 428, 1785, 337, 633, 3894, 1349, 700, 13, 51314, 51314, 663, 1355, 4476, 291, 362, 257, 955, 337, 6367, 300, 311, 516, 670, 428, 2302, 4166, 13, 51514, 51514, 400, 264, 2854, 428, 4166, 307, 11, 264, 3801, 341, 337, 6367, 307, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08299673659892022, "compression_ratio": 1.7577319587628866, "no_speech_prob": 3.94346789107658e-05}, {"id": 123, "seek": 78900, "start": 789.0, "end": 795.0, "text": " And most of these operations you can't actually confuse in parallel. You actually have to do it sequentially.", "tokens": [50364, 400, 881, 295, 613, 7705, 291, 393, 380, 767, 28584, 294, 8952, 13, 509, 767, 362, 281, 360, 309, 5123, 3137, 13, 50664, 50664, 400, 472, 18407, 8837, 307, 534, 2361, 926, 885, 1075, 281, 360, 7705, 294, 8952, 13, 51214, 51214, 407, 264, 45216, 304, 3209, 994, 380, 362, 341, 1154, 13, 5471, 311, 294, 8952, 13, 51414, 51414, 583, 322, 264, 661, 1011, 11, 291, 483, 341, 5472, 281, 264, 32264, 2519, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1762153528913667, "compression_ratio": 1.6113744075829384, "no_speech_prob": 3.269159060437232e-05}, {"id": 124, "seek": 78900, "start": 795.0, "end": 806.0, "text": " And one GPU hardware is really based around being able to do operations in parallel.", "tokens": [50364, 400, 881, 295, 613, 7705, 291, 393, 380, 767, 28584, 294, 8952, 13, 509, 767, 362, 281, 360, 309, 5123, 3137, 13, 50664, 50664, 400, 472, 18407, 8837, 307, 534, 2361, 926, 885, 1075, 281, 360, 7705, 294, 8952, 13, 51214, 51214, 407, 264, 45216, 304, 3209, 994, 380, 362, 341, 1154, 13, 5471, 311, 294, 8952, 13, 51414, 51414, 583, 322, 264, 661, 1011, 11, 291, 483, 341, 5472, 281, 264, 32264, 2519, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1762153528913667, "compression_ratio": 1.6113744075829384, "no_speech_prob": 3.269159060437232e-05}, {"id": 125, "seek": 78900, "start": 806.0, "end": 810.0, "text": " So the convolutional network didn't have this problem. Everything's in parallel.", "tokens": [50364, 400, 881, 295, 613, 7705, 291, 393, 380, 767, 28584, 294, 8952, 13, 509, 767, 362, 281, 360, 309, 5123, 3137, 13, 50664, 50664, 400, 472, 18407, 8837, 307, 534, 2361, 926, 885, 1075, 281, 360, 7705, 294, 8952, 13, 51214, 51214, 407, 264, 45216, 304, 3209, 994, 380, 362, 341, 1154, 13, 5471, 311, 294, 8952, 13, 51414, 51414, 583, 322, 264, 661, 1011, 11, 291, 483, 341, 5472, 281, 264, 32264, 2519, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1762153528913667, "compression_ratio": 1.6113744075829384, "no_speech_prob": 3.269159060437232e-05}, {"id": 126, "seek": 78900, "start": 810.0, "end": 815.0, "text": " But on the other hand, you get this bound to the receptor field.", "tokens": [50364, 400, 881, 295, 613, 7705, 291, 393, 380, 767, 28584, 294, 8952, 13, 509, 767, 362, 281, 360, 309, 5123, 3137, 13, 50664, 50664, 400, 472, 18407, 8837, 307, 534, 2361, 926, 885, 1075, 281, 360, 7705, 294, 8952, 13, 51214, 51214, 407, 264, 45216, 304, 3209, 994, 380, 362, 341, 1154, 13, 5471, 311, 294, 8952, 13, 51414, 51414, 583, 322, 264, 661, 1011, 11, 291, 483, 341, 5472, 281, 264, 32264, 2519, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1762153528913667, "compression_ratio": 1.6113744075829384, "no_speech_prob": 3.269159060437232e-05}, {"id": 127, "seek": 81500, "start": 815.0, "end": 826.0, "text": " The recurrent models you have in principle an infinite amount of receptor field, but it's quite slow to train.", "tokens": [50364, 440, 18680, 1753, 5245, 291, 362, 294, 8665, 364, 13785, 2372, 295, 32264, 2519, 11, 457, 309, 311, 1596, 2964, 281, 3847, 13, 50914, 50914, 407, 264, 3827, 281, 341, 307, 586, 437, 311, 1219, 264, 31782, 11, 597, 307, 264, 2316, 300, 311, 1143, 294, 439, 264, 1785, 295, 264, 1523, 426, 45196, 3652, 613, 1708, 13, 51364, 51364, 407, 286, 528, 281, 352, 807, 264, 31782, 294, 1596, 257, 688, 544, 2607, 813, 286, 630, 264, 45702, 45, 82, 420, 24859, 82, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16224740060527673, "compression_ratio": 1.588235294117647, "no_speech_prob": 7.410856142087141e-06}, {"id": 128, "seek": 81500, "start": 826.0, "end": 835.0, "text": " So the solution to this is now what's called the transformer, which is the model that's used in all the state of the art NLP systems these days.", "tokens": [50364, 440, 18680, 1753, 5245, 291, 362, 294, 8665, 364, 13785, 2372, 295, 32264, 2519, 11, 457, 309, 311, 1596, 2964, 281, 3847, 13, 50914, 50914, 407, 264, 3827, 281, 341, 307, 586, 437, 311, 1219, 264, 31782, 11, 597, 307, 264, 2316, 300, 311, 1143, 294, 439, 264, 1785, 295, 264, 1523, 426, 45196, 3652, 613, 1708, 13, 51364, 51364, 407, 286, 528, 281, 352, 807, 264, 31782, 294, 1596, 257, 688, 544, 2607, 813, 286, 630, 264, 45702, 45, 82, 420, 24859, 82, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16224740060527673, "compression_ratio": 1.588235294117647, "no_speech_prob": 7.410856142087141e-06}, {"id": 129, "seek": 81500, "start": 835.0, "end": 843.0, "text": " So I want to go through the transformer in quite a lot more detail than I did the RNNs or CNNs.", "tokens": [50364, 440, 18680, 1753, 5245, 291, 362, 294, 8665, 364, 13785, 2372, 295, 32264, 2519, 11, 457, 309, 311, 1596, 2964, 281, 3847, 13, 50914, 50914, 407, 264, 3827, 281, 341, 307, 586, 437, 311, 1219, 264, 31782, 11, 597, 307, 264, 2316, 300, 311, 1143, 294, 439, 264, 1785, 295, 264, 1523, 426, 45196, 3652, 613, 1708, 13, 51364, 51364, 407, 286, 528, 281, 352, 807, 264, 31782, 294, 1596, 257, 688, 544, 2607, 813, 286, 630, 264, 45702, 45, 82, 420, 24859, 82, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16224740060527673, "compression_ratio": 1.588235294117647, "no_speech_prob": 7.410856142087141e-06}, {"id": 130, "seek": 84300, "start": 843.0, "end": 852.0, "text": " The transformers were introduced in 2017 by Ashish Vaswani and a famous paper called Attention is All You Need.", "tokens": [50364, 440, 4088, 433, 645, 7268, 294, 6591, 538, 10279, 742, 23299, 86, 3782, 293, 257, 4618, 3035, 1219, 31858, 307, 1057, 509, 16984, 13, 50814, 50814, 400, 436, 534, 8894, 1602, 3195, 295, 426, 45196, 13, 51014, 51014, 407, 286, 5556, 257, 2573, 510, 490, 264, 3380, 31782, 3035, 13, 51364, 51364, 286, 500, 380, 458, 577, 13181, 341, 307, 281, 291, 11, 457, 3297, 562, 286, 700, 1866, 341, 2573, 294, 6591, 11, 309, 1890, 385, 1596, 257, 1339, 281, 483, 452, 1378, 926, 309, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1727509236597753, "compression_ratio": 1.4817813765182186, "no_speech_prob": 3.426417970331386e-05}, {"id": 131, "seek": 84300, "start": 852.0, "end": 856.0, "text": " And they really revolutionized lots of NLP.", "tokens": [50364, 440, 4088, 433, 645, 7268, 294, 6591, 538, 10279, 742, 23299, 86, 3782, 293, 257, 4618, 3035, 1219, 31858, 307, 1057, 509, 16984, 13, 50814, 50814, 400, 436, 534, 8894, 1602, 3195, 295, 426, 45196, 13, 51014, 51014, 407, 286, 5556, 257, 2573, 510, 490, 264, 3380, 31782, 3035, 13, 51364, 51364, 286, 500, 380, 458, 577, 13181, 341, 307, 281, 291, 11, 457, 3297, 562, 286, 700, 1866, 341, 2573, 294, 6591, 11, 309, 1890, 385, 1596, 257, 1339, 281, 483, 452, 1378, 926, 309, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1727509236597753, "compression_ratio": 1.4817813765182186, "no_speech_prob": 3.426417970331386e-05}, {"id": 132, "seek": 84300, "start": 856.0, "end": 863.0, "text": " So I included a figure here from the original transformer paper.", "tokens": [50364, 440, 4088, 433, 645, 7268, 294, 6591, 538, 10279, 742, 23299, 86, 3782, 293, 257, 4618, 3035, 1219, 31858, 307, 1057, 509, 16984, 13, 50814, 50814, 400, 436, 534, 8894, 1602, 3195, 295, 426, 45196, 13, 51014, 51014, 407, 286, 5556, 257, 2573, 510, 490, 264, 3380, 31782, 3035, 13, 51364, 51364, 286, 500, 380, 458, 577, 13181, 341, 307, 281, 291, 11, 457, 3297, 562, 286, 700, 1866, 341, 2573, 294, 6591, 11, 309, 1890, 385, 1596, 257, 1339, 281, 483, 452, 1378, 926, 309, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1727509236597753, "compression_ratio": 1.4817813765182186, "no_speech_prob": 3.426417970331386e-05}, {"id": 133, "seek": 84300, "start": 863.0, "end": 870.0, "text": " I don't know how confusing this is to you, but certainly when I first saw this figure in 2017, it took me quite a while to get my head around it.", "tokens": [50364, 440, 4088, 433, 645, 7268, 294, 6591, 538, 10279, 742, 23299, 86, 3782, 293, 257, 4618, 3035, 1219, 31858, 307, 1057, 509, 16984, 13, 50814, 50814, 400, 436, 534, 8894, 1602, 3195, 295, 426, 45196, 13, 51014, 51014, 407, 286, 5556, 257, 2573, 510, 490, 264, 3380, 31782, 3035, 13, 51364, 51364, 286, 500, 380, 458, 577, 13181, 341, 307, 281, 291, 11, 457, 3297, 562, 286, 700, 1866, 341, 2573, 294, 6591, 11, 309, 1890, 385, 1596, 257, 1339, 281, 483, 452, 1378, 926, 309, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1727509236597753, "compression_ratio": 1.4817813765182186, "no_speech_prob": 3.426417970331386e-05}, {"id": 134, "seek": 87000, "start": 870.0, "end": 878.0, "text": " There's quite a lot of details going on in these boxes. So I'm going to just try to slowly drill into them.", "tokens": [50364, 821, 311, 1596, 257, 688, 295, 4365, 516, 322, 294, 613, 9002, 13, 407, 286, 478, 516, 281, 445, 853, 281, 5692, 11392, 666, 552, 13, 50764, 50764, 1057, 558, 13, 407, 437, 311, 516, 322, 30, 50964, 50964, 8537, 11, 291, 536, 321, 362, 341, 4846, 1785, 11, 341, 426, 1413, 341, 31782, 3461, 11, 293, 550, 364, 5598, 295, 5574, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.2415447235107422, "compression_ratio": 1.4067796610169492, "no_speech_prob": 1.5688436178606935e-05}, {"id": 135, "seek": 87000, "start": 878.0, "end": 882.0, "text": " All right. So what's going on?", "tokens": [50364, 821, 311, 1596, 257, 688, 295, 4365, 516, 322, 294, 613, 9002, 13, 407, 286, 478, 516, 281, 445, 853, 281, 5692, 11392, 666, 552, 13, 50764, 50764, 1057, 558, 13, 407, 437, 311, 516, 322, 30, 50964, 50964, 8537, 11, 291, 536, 321, 362, 341, 4846, 1785, 11, 341, 426, 1413, 341, 31782, 3461, 11, 293, 550, 364, 5598, 295, 5574, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.2415447235107422, "compression_ratio": 1.4067796610169492, "no_speech_prob": 1.5688436178606935e-05}, {"id": 136, "seek": 87000, "start": 882.0, "end": 891.0, "text": " Basically, you see we have this input state, this N times this transformer block, and then an output of phase.", "tokens": [50364, 821, 311, 1596, 257, 688, 295, 4365, 516, 322, 294, 613, 9002, 13, 407, 286, 478, 516, 281, 445, 853, 281, 5692, 11392, 666, 552, 13, 50764, 50764, 1057, 558, 13, 407, 437, 311, 516, 322, 30, 50964, 50964, 8537, 11, 291, 536, 321, 362, 341, 4846, 1785, 11, 341, 426, 1413, 341, 31782, 3461, 11, 293, 550, 364, 5598, 295, 5574, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.2415447235107422, "compression_ratio": 1.4067796610169492, "no_speech_prob": 1.5688436178606935e-05}, {"id": 137, "seek": 89100, "start": 891.0, "end": 902.0, "text": " So this N times block thing just means we're going to unroll the same block with different parameters a certain number of times.", "tokens": [50364, 407, 341, 426, 1413, 3461, 551, 445, 1355, 321, 434, 516, 281, 517, 3970, 264, 912, 3461, 365, 819, 9834, 257, 1629, 1230, 295, 1413, 13, 50914, 50914, 407, 341, 1365, 575, 2309, 7914, 11, 597, 286, 519, 632, 264, 3380, 31782, 3035, 11, 597, 2544, 1596, 4052, 613, 1708, 13, 51314, 51314, 1981, 1708, 11, 561, 366, 3097, 5245, 365, 17375, 295, 9834, 293, 867, 11, 867, 18431, 295, 7914, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08358932168860185, "compression_ratio": 1.5825688073394495, "no_speech_prob": 7.0712153501517605e-06}, {"id": 138, "seek": 89100, "start": 902.0, "end": 910.0, "text": " So this example has six layers, which I think had the original transformer paper, which seems quite cute these days.", "tokens": [50364, 407, 341, 426, 1413, 3461, 551, 445, 1355, 321, 434, 516, 281, 517, 3970, 264, 912, 3461, 365, 819, 9834, 257, 1629, 1230, 295, 1413, 13, 50914, 50914, 407, 341, 1365, 575, 2309, 7914, 11, 597, 286, 519, 632, 264, 3380, 31782, 3035, 11, 597, 2544, 1596, 4052, 613, 1708, 13, 51314, 51314, 1981, 1708, 11, 561, 366, 3097, 5245, 365, 17375, 295, 9834, 293, 867, 11, 867, 18431, 295, 7914, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08358932168860185, "compression_ratio": 1.5825688073394495, "no_speech_prob": 7.0712153501517605e-06}, {"id": 139, "seek": 89100, "start": 910.0, "end": 918.0, "text": " These days, people are training models with billions of parameters and many, many dozens of layers.", "tokens": [50364, 407, 341, 426, 1413, 3461, 551, 445, 1355, 321, 434, 516, 281, 517, 3970, 264, 912, 3461, 365, 819, 9834, 257, 1629, 1230, 295, 1413, 13, 50914, 50914, 407, 341, 1365, 575, 2309, 7914, 11, 597, 286, 519, 632, 264, 3380, 31782, 3035, 11, 597, 2544, 1596, 4052, 613, 1708, 13, 51314, 51314, 1981, 1708, 11, 561, 366, 3097, 5245, 365, 17375, 295, 9834, 293, 867, 11, 867, 18431, 295, 7914, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08358932168860185, "compression_ratio": 1.5825688073394495, "no_speech_prob": 7.0712153501517605e-06}, {"id": 140, "seek": 91800, "start": 918.0, "end": 921.0, "text": " So I'm just going to drill into this box in more detail.", "tokens": [50364, 407, 286, 478, 445, 516, 281, 11392, 666, 341, 2424, 294, 544, 2607, 13, 50514, 50514, 407, 341, 307, 733, 295, 264, 4965, 295, 264, 31782, 11, 264, 31782, 3461, 13, 50764, 50764, 509, 393, 536, 309, 311, 767, 21654, 666, 732, 819, 1422, 7914, 11, 597, 366, 1293, 588, 1021, 13, 51214, 51214, 8511, 4583, 732, 307, 1310, 264, 544, 6322, 472, 13, 639, 307, 445, 257, 955, 3154, 2128, 3209, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13236334416773413, "compression_ratio": 1.57, "no_speech_prob": 7.888612344686408e-06}, {"id": 141, "seek": 91800, "start": 921.0, "end": 926.0, "text": " So this is kind of the core of the transformer, the transformer block.", "tokens": [50364, 407, 286, 478, 445, 516, 281, 11392, 666, 341, 2424, 294, 544, 2607, 13, 50514, 50514, 407, 341, 307, 733, 295, 264, 4965, 295, 264, 31782, 11, 264, 31782, 3461, 13, 50764, 50764, 509, 393, 536, 309, 311, 767, 21654, 666, 732, 819, 1422, 7914, 11, 597, 366, 1293, 588, 1021, 13, 51214, 51214, 8511, 4583, 732, 307, 1310, 264, 544, 6322, 472, 13, 639, 307, 445, 257, 955, 3154, 2128, 3209, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13236334416773413, "compression_ratio": 1.57, "no_speech_prob": 7.888612344686408e-06}, {"id": 142, "seek": 91800, "start": 926.0, "end": 935.0, "text": " You can see it's actually incorporated into two different sub layers, which are both very important.", "tokens": [50364, 407, 286, 478, 445, 516, 281, 11392, 666, 341, 2424, 294, 544, 2607, 13, 50514, 50514, 407, 341, 307, 733, 295, 264, 4965, 295, 264, 31782, 11, 264, 31782, 3461, 13, 50764, 50764, 509, 393, 536, 309, 311, 767, 21654, 666, 732, 819, 1422, 7914, 11, 597, 366, 1293, 588, 1021, 13, 51214, 51214, 8511, 4583, 732, 307, 1310, 264, 544, 6322, 472, 13, 639, 307, 445, 257, 955, 3154, 2128, 3209, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13236334416773413, "compression_ratio": 1.57, "no_speech_prob": 7.888612344686408e-06}, {"id": 143, "seek": 91800, "start": 935.0, "end": 944.0, "text": " Sub layer two is maybe the more obvious one. This is just a big feed forward network.", "tokens": [50364, 407, 286, 478, 445, 516, 281, 11392, 666, 341, 2424, 294, 544, 2607, 13, 50514, 50514, 407, 341, 307, 733, 295, 264, 4965, 295, 264, 31782, 11, 264, 31782, 3461, 13, 50764, 50764, 509, 393, 536, 309, 311, 767, 21654, 666, 732, 819, 1422, 7914, 11, 597, 366, 1293, 588, 1021, 13, 51214, 51214, 8511, 4583, 732, 307, 1310, 264, 544, 6322, 472, 13, 639, 307, 445, 257, 955, 3154, 2128, 3209, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13236334416773413, "compression_ratio": 1.57, "no_speech_prob": 7.888612344686408e-06}, {"id": 144, "seek": 94400, "start": 944.0, "end": 950.0, "text": " It could be any MLP, but it's important it's actually quite expressive.", "tokens": [50364, 467, 727, 312, 604, 21601, 47, 11, 457, 309, 311, 1021, 309, 311, 767, 1596, 40189, 13, 50664, 50664, 400, 17149, 321, 362, 341, 4825, 12, 28409, 8980, 10088, 13, 29238, 12, 28409, 8980, 307, 733, 295, 264, 2141, 2390, 3461, 2261, 4088, 433, 293, 983, 436, 589, 13, 51364, 51364, 407, 613, 1422, 7914, 366, 611, 4582, 538, 613, 9002, 21335, 5127, 2026, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1460835830025051, "compression_ratio": 1.5, "no_speech_prob": 7.1826912062533665e-06}, {"id": 145, "seek": 94400, "start": 950.0, "end": 964.0, "text": " And beneath we have this multi-headed tension module. Multi-headed tension is kind of the key building block behind transformers and why they work.", "tokens": [50364, 467, 727, 312, 604, 21601, 47, 11, 457, 309, 311, 1021, 309, 311, 767, 1596, 40189, 13, 50664, 50664, 400, 17149, 321, 362, 341, 4825, 12, 28409, 8980, 10088, 13, 29238, 12, 28409, 8980, 307, 733, 295, 264, 2141, 2390, 3461, 2261, 4088, 433, 293, 983, 436, 589, 13, 51364, 51364, 407, 613, 1422, 7914, 366, 611, 4582, 538, 613, 9002, 21335, 5127, 2026, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1460835830025051, "compression_ratio": 1.5, "no_speech_prob": 7.1826912062533665e-06}, {"id": 146, "seek": 94400, "start": 964.0, "end": 970.0, "text": " So these sub layers are also connected by these boxes labeled adding norm.", "tokens": [50364, 467, 727, 312, 604, 21601, 47, 11, 457, 309, 311, 1021, 309, 311, 767, 1596, 40189, 13, 50664, 50664, 400, 17149, 321, 362, 341, 4825, 12, 28409, 8980, 10088, 13, 29238, 12, 28409, 8980, 307, 733, 295, 264, 2141, 2390, 3461, 2261, 4088, 433, 293, 983, 436, 589, 13, 51364, 51364, 407, 613, 1422, 7914, 366, 611, 4582, 538, 613, 9002, 21335, 5127, 2026, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1460835830025051, "compression_ratio": 1.5, "no_speech_prob": 7.1826912062533665e-06}, {"id": 147, "seek": 97000, "start": 970.0, "end": 979.0, "text": " So the add part means this is a residual connection, which helps stop the gradient transition in large models.", "tokens": [50364, 407, 264, 909, 644, 1355, 341, 307, 257, 27980, 4984, 11, 597, 3665, 1590, 264, 16235, 6034, 294, 2416, 5245, 13, 50814, 50814, 440, 2026, 510, 1355, 4583, 2710, 2144, 13, 51014, 51014, 286, 478, 406, 516, 281, 352, 666, 4583, 2026, 294, 2607, 510, 11, 457, 309, 311, 767, 588, 1021, 281, 652, 613, 5245, 589, 13, 51264, 51264, 400, 456, 311, 767, 512, 7257, 2631, 530, 466, 577, 2293, 291, 360, 264, 4583, 2710, 2144, 300, 1669, 257, 955, 2649, 294, 3124, 13, 51614, 51614, 1911, 11, 8960, 385, 13, 286, 362, 257, 1168, 13, 51714, 51714, 4894, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14896156674339658, "compression_ratio": 1.6513409961685823, "no_speech_prob": 3.0713224987266585e-05}, {"id": 148, "seek": 97000, "start": 979.0, "end": 983.0, "text": " The norm here means layer normalization.", "tokens": [50364, 407, 264, 909, 644, 1355, 341, 307, 257, 27980, 4984, 11, 597, 3665, 1590, 264, 16235, 6034, 294, 2416, 5245, 13, 50814, 50814, 440, 2026, 510, 1355, 4583, 2710, 2144, 13, 51014, 51014, 286, 478, 406, 516, 281, 352, 666, 4583, 2026, 294, 2607, 510, 11, 457, 309, 311, 767, 588, 1021, 281, 652, 613, 5245, 589, 13, 51264, 51264, 400, 456, 311, 767, 512, 7257, 2631, 530, 466, 577, 2293, 291, 360, 264, 4583, 2710, 2144, 300, 1669, 257, 955, 2649, 294, 3124, 13, 51614, 51614, 1911, 11, 8960, 385, 13, 286, 362, 257, 1168, 13, 51714, 51714, 4894, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14896156674339658, "compression_ratio": 1.6513409961685823, "no_speech_prob": 3.0713224987266585e-05}, {"id": 149, "seek": 97000, "start": 983.0, "end": 988.0, "text": " I'm not going to go into layer norm in detail here, but it's actually very important to make these models work.", "tokens": [50364, 407, 264, 909, 644, 1355, 341, 307, 257, 27980, 4984, 11, 597, 3665, 1590, 264, 16235, 6034, 294, 2416, 5245, 13, 50814, 50814, 440, 2026, 510, 1355, 4583, 2710, 2144, 13, 51014, 51014, 286, 478, 406, 516, 281, 352, 666, 4583, 2026, 294, 2607, 510, 11, 457, 309, 311, 767, 588, 1021, 281, 652, 613, 5245, 589, 13, 51264, 51264, 400, 456, 311, 767, 512, 7257, 2631, 530, 466, 577, 2293, 291, 360, 264, 4583, 2710, 2144, 300, 1669, 257, 955, 2649, 294, 3124, 13, 51614, 51614, 1911, 11, 8960, 385, 13, 286, 362, 257, 1168, 13, 51714, 51714, 4894, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14896156674339658, "compression_ratio": 1.6513409961685823, "no_speech_prob": 3.0713224987266585e-05}, {"id": 150, "seek": 97000, "start": 988.0, "end": 995.0, "text": " And there's actually some subtleties about how exactly you do the layer normalization that makes a big difference in practice.", "tokens": [50364, 407, 264, 909, 644, 1355, 341, 307, 257, 27980, 4984, 11, 597, 3665, 1590, 264, 16235, 6034, 294, 2416, 5245, 13, 50814, 50814, 440, 2026, 510, 1355, 4583, 2710, 2144, 13, 51014, 51014, 286, 478, 406, 516, 281, 352, 666, 4583, 2026, 294, 2607, 510, 11, 457, 309, 311, 767, 588, 1021, 281, 652, 613, 5245, 589, 13, 51264, 51264, 400, 456, 311, 767, 512, 7257, 2631, 530, 466, 577, 2293, 291, 360, 264, 4583, 2710, 2144, 300, 1669, 257, 955, 2649, 294, 3124, 13, 51614, 51614, 1911, 11, 8960, 385, 13, 286, 362, 257, 1168, 13, 51714, 51714, 4894, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14896156674339658, "compression_ratio": 1.6513409961685823, "no_speech_prob": 3.0713224987266585e-05}, {"id": 151, "seek": 97000, "start": 995.0, "end": 997.0, "text": " Hey, excuse me. I have a question.", "tokens": [50364, 407, 264, 909, 644, 1355, 341, 307, 257, 27980, 4984, 11, 597, 3665, 1590, 264, 16235, 6034, 294, 2416, 5245, 13, 50814, 50814, 440, 2026, 510, 1355, 4583, 2710, 2144, 13, 51014, 51014, 286, 478, 406, 516, 281, 352, 666, 4583, 2026, 294, 2607, 510, 11, 457, 309, 311, 767, 588, 1021, 281, 652, 613, 5245, 589, 13, 51264, 51264, 400, 456, 311, 767, 512, 7257, 2631, 530, 466, 577, 2293, 291, 360, 264, 4583, 2710, 2144, 300, 1669, 257, 955, 2649, 294, 3124, 13, 51614, 51614, 1911, 11, 8960, 385, 13, 286, 362, 257, 1168, 13, 51714, 51714, 4894, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14896156674339658, "compression_ratio": 1.6513409961685823, "no_speech_prob": 3.0713224987266585e-05}, {"id": 152, "seek": 97000, "start": 997.0, "end": 998.0, "text": " Sure.", "tokens": [50364, 407, 264, 909, 644, 1355, 341, 307, 257, 27980, 4984, 11, 597, 3665, 1590, 264, 16235, 6034, 294, 2416, 5245, 13, 50814, 50814, 440, 2026, 510, 1355, 4583, 2710, 2144, 13, 51014, 51014, 286, 478, 406, 516, 281, 352, 666, 4583, 2026, 294, 2607, 510, 11, 457, 309, 311, 767, 588, 1021, 281, 652, 613, 5245, 589, 13, 51264, 51264, 400, 456, 311, 767, 512, 7257, 2631, 530, 466, 577, 2293, 291, 360, 264, 4583, 2710, 2144, 300, 1669, 257, 955, 2649, 294, 3124, 13, 51614, 51614, 1911, 11, 8960, 385, 13, 286, 362, 257, 1168, 13, 51714, 51714, 4894, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14896156674339658, "compression_ratio": 1.6513409961685823, "no_speech_prob": 3.0713224987266585e-05}, {"id": 153, "seek": 99800, "start": 998.0, "end": 1008.0, "text": " So this isn't immediately clear, but could you talk a little bit more about just the intuition behind using multi-headed tension as opposed to a single head?", "tokens": [50364, 407, 341, 1943, 380, 4258, 1850, 11, 457, 727, 291, 751, 257, 707, 857, 544, 466, 445, 264, 24002, 2261, 1228, 4825, 12, 28409, 8980, 382, 8851, 281, 257, 2167, 1378, 30, 50864, 50864, 286, 914, 11, 26742, 1184, 1378, 27152, 746, 819, 293, 49837, 670, 1080, 4846, 7614, 13, 51164, 51164, 583, 437, 311, 264, 24002, 2261, 300, 30, 51414, 51414, 286, 603, 1867, 300, 1168, 257, 857, 13, 286, 478, 516, 281, 352, 807, 2293, 437, 4825, 12, 28409, 8980, 307, 257, 857, 700, 11, 293, 550, 286, 603, 853, 281, 976, 512, 16224, 626, 382, 281, 983, 341, 307, 257, 665, 551, 281, 360, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1307028021131243, "compression_ratio": 1.7638376383763839, "no_speech_prob": 3.426542753004469e-05}, {"id": 154, "seek": 99800, "start": 1008.0, "end": 1014.0, "text": " I mean, presumably each head learns something different and attends over its input differently.", "tokens": [50364, 407, 341, 1943, 380, 4258, 1850, 11, 457, 727, 291, 751, 257, 707, 857, 544, 466, 445, 264, 24002, 2261, 1228, 4825, 12, 28409, 8980, 382, 8851, 281, 257, 2167, 1378, 30, 50864, 50864, 286, 914, 11, 26742, 1184, 1378, 27152, 746, 819, 293, 49837, 670, 1080, 4846, 7614, 13, 51164, 51164, 583, 437, 311, 264, 24002, 2261, 300, 30, 51414, 51414, 286, 603, 1867, 300, 1168, 257, 857, 13, 286, 478, 516, 281, 352, 807, 2293, 437, 4825, 12, 28409, 8980, 307, 257, 857, 700, 11, 293, 550, 286, 603, 853, 281, 976, 512, 16224, 626, 382, 281, 983, 341, 307, 257, 665, 551, 281, 360, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1307028021131243, "compression_ratio": 1.7638376383763839, "no_speech_prob": 3.426542753004469e-05}, {"id": 155, "seek": 99800, "start": 1014.0, "end": 1019.0, "text": " But what's the intuition behind that?", "tokens": [50364, 407, 341, 1943, 380, 4258, 1850, 11, 457, 727, 291, 751, 257, 707, 857, 544, 466, 445, 264, 24002, 2261, 1228, 4825, 12, 28409, 8980, 382, 8851, 281, 257, 2167, 1378, 30, 50864, 50864, 286, 914, 11, 26742, 1184, 1378, 27152, 746, 819, 293, 49837, 670, 1080, 4846, 7614, 13, 51164, 51164, 583, 437, 311, 264, 24002, 2261, 300, 30, 51414, 51414, 286, 603, 1867, 300, 1168, 257, 857, 13, 286, 478, 516, 281, 352, 807, 2293, 437, 4825, 12, 28409, 8980, 307, 257, 857, 700, 11, 293, 550, 286, 603, 853, 281, 976, 512, 16224, 626, 382, 281, 983, 341, 307, 257, 665, 551, 281, 360, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1307028021131243, "compression_ratio": 1.7638376383763839, "no_speech_prob": 3.426542753004469e-05}, {"id": 156, "seek": 99800, "start": 1019.0, "end": 1027.0, "text": " I'll answer that question a bit. I'm going to go through exactly what multi-headed tension is a bit first, and then I'll try to give some intuitions as to why this is a good thing to do.", "tokens": [50364, 407, 341, 1943, 380, 4258, 1850, 11, 457, 727, 291, 751, 257, 707, 857, 544, 466, 445, 264, 24002, 2261, 1228, 4825, 12, 28409, 8980, 382, 8851, 281, 257, 2167, 1378, 30, 50864, 50864, 286, 914, 11, 26742, 1184, 1378, 27152, 746, 819, 293, 49837, 670, 1080, 4846, 7614, 13, 51164, 51164, 583, 437, 311, 264, 24002, 2261, 300, 30, 51414, 51414, 286, 603, 1867, 300, 1168, 257, 857, 13, 286, 478, 516, 281, 352, 807, 2293, 437, 4825, 12, 28409, 8980, 307, 257, 857, 700, 11, 293, 550, 286, 603, 853, 281, 976, 512, 16224, 626, 382, 281, 983, 341, 307, 257, 665, 551, 281, 360, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1307028021131243, "compression_ratio": 1.7638376383763839, "no_speech_prob": 3.426542753004469e-05}, {"id": 157, "seek": 102700, "start": 1027.0, "end": 1033.0, "text": " If I don't answer your question, then please follow me up a few slides time.", "tokens": [50364, 759, 286, 500, 380, 1867, 428, 1168, 11, 550, 1767, 1524, 385, 493, 257, 1326, 9788, 565, 13, 50664, 50664, 1044, 291, 13, 50864, 50864, 2639, 661, 1651, 412, 341, 3233, 11, 538, 264, 636, 30, 51114, 51114, 286, 362, 257, 1168, 13, 509, 848, 300, 264, 31782, 10088, 4960, 4583, 2710, 2144, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.14540328650638976, "compression_ratio": 1.3935483870967742, "no_speech_prob": 4.2627907532732934e-05}, {"id": 158, "seek": 102700, "start": 1033.0, "end": 1037.0, "text": " Thank you.", "tokens": [50364, 759, 286, 500, 380, 1867, 428, 1168, 11, 550, 1767, 1524, 385, 493, 257, 1326, 9788, 565, 13, 50664, 50664, 1044, 291, 13, 50864, 50864, 2639, 661, 1651, 412, 341, 3233, 11, 538, 264, 636, 30, 51114, 51114, 286, 362, 257, 1168, 13, 509, 848, 300, 264, 31782, 10088, 4960, 4583, 2710, 2144, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.14540328650638976, "compression_ratio": 1.3935483870967742, "no_speech_prob": 4.2627907532732934e-05}, {"id": 159, "seek": 102700, "start": 1037.0, "end": 1042.0, "text": " Any other questions at this stage, by the way?", "tokens": [50364, 759, 286, 500, 380, 1867, 428, 1168, 11, 550, 1767, 1524, 385, 493, 257, 1326, 9788, 565, 13, 50664, 50664, 1044, 291, 13, 50864, 50864, 2639, 661, 1651, 412, 341, 3233, 11, 538, 264, 636, 30, 51114, 51114, 286, 362, 257, 1168, 13, 509, 848, 300, 264, 31782, 10088, 4960, 4583, 2710, 2144, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.14540328650638976, "compression_ratio": 1.3935483870967742, "no_speech_prob": 4.2627907532732934e-05}, {"id": 160, "seek": 102700, "start": 1042.0, "end": 1050.0, "text": " I have a question. You said that the transformer module uses layer normalization.", "tokens": [50364, 759, 286, 500, 380, 1867, 428, 1168, 11, 550, 1767, 1524, 385, 493, 257, 1326, 9788, 565, 13, 50664, 50664, 1044, 291, 13, 50864, 50864, 2639, 661, 1651, 412, 341, 3233, 11, 538, 264, 636, 30, 51114, 51114, 286, 362, 257, 1168, 13, 509, 848, 300, 264, 31782, 10088, 4960, 4583, 2710, 2144, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.14540328650638976, "compression_ratio": 1.3935483870967742, "no_speech_prob": 4.2627907532732934e-05}, {"id": 161, "seek": 105000, "start": 1050.0, "end": 1063.0, "text": " Can you provide some intuition into why that works better than group normalization or batch normalization?", "tokens": [50364, 1664, 291, 2893, 512, 24002, 666, 983, 300, 1985, 1101, 813, 1594, 2710, 2144, 420, 15245, 2710, 2144, 30, 51014, 51014, 286, 360, 519, 286, 393, 767, 976, 257, 588, 18348, 6191, 1867, 281, 341, 13, 286, 519, 257, 688, 295, 341, 307, 1596, 31886, 382, 281, 983, 294, 426, 45196, 11, 4583, 2710, 2144, 1985, 869, 11, 293, 294, 3820, 3037, 11, 15245, 2710, 2144, 1985, 869, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1788222169222897, "compression_ratio": 1.6633165829145728, "no_speech_prob": 3.426304829190485e-05}, {"id": 162, "seek": 105000, "start": 1063.0, "end": 1077.0, "text": " I do think I can actually give a very satisfying technical answer to this. I think a lot of this is quite empirical as to why in NLP, layer normalization works great, and in computer version, batch normalization works great.", "tokens": [50364, 1664, 291, 2893, 512, 24002, 666, 983, 300, 1985, 1101, 813, 1594, 2710, 2144, 420, 15245, 2710, 2144, 30, 51014, 51014, 286, 360, 519, 286, 393, 767, 976, 257, 588, 18348, 6191, 1867, 281, 341, 13, 286, 519, 257, 688, 295, 341, 307, 1596, 31886, 382, 281, 983, 294, 426, 45196, 11, 4583, 2710, 2144, 1985, 869, 11, 293, 294, 3820, 3037, 11, 15245, 2710, 2144, 1985, 869, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1788222169222897, "compression_ratio": 1.6633165829145728, "no_speech_prob": 3.426304829190485e-05}, {"id": 163, "seek": 107700, "start": 1077.0, "end": 1086.0, "text": " A nice proffs about layer normalization is that it doesn't depend on the batch dimension, which batch normalization does.", "tokens": [50364, 316, 1481, 447, 602, 82, 466, 4583, 2710, 2144, 307, 300, 309, 1177, 380, 5672, 322, 264, 15245, 10139, 11, 597, 15245, 2710, 2144, 775, 13, 50814, 50814, 682, 3124, 11, 300, 311, 1596, 257, 955, 5002, 570, 309, 311, 1596, 1152, 281, 3847, 365, 2416, 15245, 279, 365, 552, 337, 2416, 5245, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.19914225874275998, "compression_ratio": 1.51875, "no_speech_prob": 2.506088094378356e-05}, {"id": 164, "seek": 107700, "start": 1086.0, "end": 1099.0, "text": " In practice, that's quite a big advantage because it's quite hard to train with large batches with them for large models.", "tokens": [50364, 316, 1481, 447, 602, 82, 466, 4583, 2710, 2144, 307, 300, 309, 1177, 380, 5672, 322, 264, 15245, 10139, 11, 597, 15245, 2710, 2144, 775, 13, 50814, 50814, 682, 3124, 11, 300, 311, 1596, 257, 955, 5002, 570, 309, 311, 1596, 1152, 281, 3847, 365, 2416, 15245, 279, 365, 552, 337, 2416, 5245, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.19914225874275998, "compression_ratio": 1.51875, "no_speech_prob": 2.506088094378356e-05}, {"id": 165, "seek": 109900, "start": 1099.0, "end": 1107.0, "text": " You can see people wrote a lot of papers on why things like batch normalization even work for computer version.", "tokens": [50364, 509, 393, 536, 561, 4114, 257, 688, 295, 10577, 322, 983, 721, 411, 15245, 2710, 2144, 754, 589, 337, 3820, 3037, 13, 50764, 50764, 1711, 1935, 1036, 565, 286, 1401, 11, 456, 366, 920, 512, 24203, 382, 281, 437, 309, 311, 884, 13, 2704, 264, 16224, 626, 294, 264, 3380, 3035, 337, 983, 754, 15245, 2710, 2144, 1985, 366, 406, 869, 13, 51264, 51264, 21079, 11, 286, 576, 584, 341, 307, 472, 295, 264, 4748, 2693, 25239, 1840, 721, 294, 2452, 2539, 11, 689, 309, 1985, 11, 457, 309, 311, 257, 707, 857, 25636, 983, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.22595914840698242, "compression_ratio": 1.6398467432950192, "no_speech_prob": 7.138149521779269e-05}, {"id": 166, "seek": 109900, "start": 1107.0, "end": 1117.0, "text": " At least last time I read, there are still some debates as to what it's doing. Maybe the intuitions in the original paper for why even batch normalization works are not great.", "tokens": [50364, 509, 393, 536, 561, 4114, 257, 688, 295, 10577, 322, 983, 721, 411, 15245, 2710, 2144, 754, 589, 337, 3820, 3037, 13, 50764, 50764, 1711, 1935, 1036, 565, 286, 1401, 11, 456, 366, 920, 512, 24203, 382, 281, 437, 309, 311, 884, 13, 2704, 264, 16224, 626, 294, 264, 3380, 3035, 337, 983, 754, 15245, 2710, 2144, 1985, 366, 406, 869, 13, 51264, 51264, 21079, 11, 286, 576, 584, 341, 307, 472, 295, 264, 4748, 2693, 25239, 1840, 721, 294, 2452, 2539, 11, 689, 309, 1985, 11, 457, 309, 311, 257, 707, 857, 25636, 983, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.22595914840698242, "compression_ratio": 1.6398467432950192, "no_speech_prob": 7.138149521779269e-05}, {"id": 167, "seek": 109900, "start": 1117.0, "end": 1127.0, "text": " Personally, I would say this is one of the slightly unsatisfying things in deep learning, where it works, but it's a little bit unclear why.", "tokens": [50364, 509, 393, 536, 561, 4114, 257, 688, 295, 10577, 322, 983, 721, 411, 15245, 2710, 2144, 754, 589, 337, 3820, 3037, 13, 50764, 50764, 1711, 1935, 1036, 565, 286, 1401, 11, 456, 366, 920, 512, 24203, 382, 281, 437, 309, 311, 884, 13, 2704, 264, 16224, 626, 294, 264, 3380, 3035, 337, 983, 754, 15245, 2710, 2144, 1985, 366, 406, 869, 13, 51264, 51264, 21079, 11, 286, 576, 584, 341, 307, 472, 295, 264, 4748, 2693, 25239, 1840, 721, 294, 2452, 2539, 11, 689, 309, 1985, 11, 457, 309, 311, 257, 707, 857, 25636, 983, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.22595914840698242, "compression_ratio": 1.6398467432950192, "no_speech_prob": 7.138149521779269e-05}, {"id": 168, "seek": 112700, "start": 1127.0, "end": 1129.0, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50464, 50464, 286, 1062, 312, 1075, 281, 976, 257, 544, 18348, 1867, 813, 300, 13, 50964, 50964, 3996, 1168, 307, 1348, 510, 13, 1144, 4088, 433, 2073, 17443, 2108, 565, 4439, 411, 45702, 45, 441, 6840, 26386, 30, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.2320413801405165, "compression_ratio": 1.237037037037037, "no_speech_prob": 0.0002974829403683543}, {"id": 169, "seek": 112700, "start": 1129.0, "end": 1139.0, "text": " I might be able to give a more satisfying answer than that.", "tokens": [50364, 1044, 291, 13, 50464, 50464, 286, 1062, 312, 1075, 281, 976, 257, 544, 18348, 1867, 813, 300, 13, 50964, 50964, 3996, 1168, 307, 1348, 510, 13, 1144, 4088, 433, 2073, 17443, 2108, 565, 4439, 411, 45702, 45, 441, 6840, 26386, 30, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.2320413801405165, "compression_ratio": 1.237037037037037, "no_speech_prob": 0.0002974829403683543}, {"id": 170, "seek": 112700, "start": 1139.0, "end": 1146.0, "text": " Another question is coming here. Do transformers share weights across time steps like RNN LSTMs?", "tokens": [50364, 1044, 291, 13, 50464, 50464, 286, 1062, 312, 1075, 281, 976, 257, 544, 18348, 1867, 813, 300, 13, 50964, 50964, 3996, 1168, 307, 1348, 510, 13, 1144, 4088, 433, 2073, 17443, 2108, 565, 4439, 411, 45702, 45, 441, 6840, 26386, 30, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.2320413801405165, "compression_ratio": 1.237037037037037, "no_speech_prob": 0.0002974829403683543}, {"id": 171, "seek": 114600, "start": 1146.0, "end": 1157.0, "text": " Great question. I should have made that clear. All these weights can be shared across time steps. It's kind of convolutional in that sense.", "tokens": [50364, 3769, 1168, 13, 286, 820, 362, 1027, 300, 1850, 13, 1057, 613, 17443, 393, 312, 5507, 2108, 565, 4439, 13, 467, 311, 733, 295, 45216, 304, 294, 300, 2020, 13, 50914, 50914, 509, 362, 472, 3461, 293, 291, 3079, 309, 633, 565, 1823, 13, 509, 393, 767, 611, 3079, 552, 1228, 17443, 294, 633, 4583, 13, 663, 1985, 1596, 731, 886, 11, 457, 309, 311, 406, 437, 561, 5646, 360, 13, 51564, 51564, 1044, 291, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19039325714111327, "compression_ratio": 1.5613207547169812, "no_speech_prob": 6.087897054385394e-05}, {"id": 172, "seek": 114600, "start": 1157.0, "end": 1170.0, "text": " You have one block and you apply it every time step. You can actually also apply them using weights in every layer. That works quite well too, but it's not what people normally do.", "tokens": [50364, 3769, 1168, 13, 286, 820, 362, 1027, 300, 1850, 13, 1057, 613, 17443, 393, 312, 5507, 2108, 565, 4439, 13, 467, 311, 733, 295, 45216, 304, 294, 300, 2020, 13, 50914, 50914, 509, 362, 472, 3461, 293, 291, 3079, 309, 633, 565, 1823, 13, 509, 393, 767, 611, 3079, 552, 1228, 17443, 294, 633, 4583, 13, 663, 1985, 1596, 731, 886, 11, 457, 309, 311, 406, 437, 561, 5646, 360, 13, 51564, 51564, 1044, 291, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19039325714111327, "compression_ratio": 1.5613207547169812, "no_speech_prob": 6.087897054385394e-05}, {"id": 173, "seek": 114600, "start": 1170.0, "end": 1173.0, "text": " Thank you.", "tokens": [50364, 3769, 1168, 13, 286, 820, 362, 1027, 300, 1850, 13, 1057, 613, 17443, 393, 312, 5507, 2108, 565, 4439, 13, 467, 311, 733, 295, 45216, 304, 294, 300, 2020, 13, 50914, 50914, 509, 362, 472, 3461, 293, 291, 3079, 309, 633, 565, 1823, 13, 509, 393, 767, 611, 3079, 552, 1228, 17443, 294, 633, 4583, 13, 663, 1985, 1596, 731, 886, 11, 457, 309, 311, 406, 437, 561, 5646, 360, 13, 51564, 51564, 1044, 291, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19039325714111327, "compression_ratio": 1.5613207547169812, "no_speech_prob": 6.087897054385394e-05}, {"id": 174, "seek": 117300, "start": 1173.0, "end": 1177.0, "text": " Any other questions so far?", "tokens": [50364, 2639, 661, 1651, 370, 1400, 30, 50564, 50564, 286, 519, 729, 645, 264, 1651, 286, 1401, 370, 1400, 322, 264, 5081, 13, 50914, 50914, 708, 307, 341, 13831, 4825, 12, 1934, 3202, 551, 30, 51164, 51164, 1692, 311, 1071, 2573, 13, 286, 500, 380, 458, 498, 341, 3665, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.14796337991390587, "compression_ratio": 1.3868613138686132, "no_speech_prob": 7.243957952596247e-05}, {"id": 175, "seek": 117300, "start": 1177.0, "end": 1184.0, "text": " I think those were the questions I read so far on the chat.", "tokens": [50364, 2639, 661, 1651, 370, 1400, 30, 50564, 50564, 286, 519, 729, 645, 264, 1651, 286, 1401, 370, 1400, 322, 264, 5081, 13, 50914, 50914, 708, 307, 341, 13831, 4825, 12, 1934, 3202, 551, 30, 51164, 51164, 1692, 311, 1071, 2573, 13, 286, 500, 380, 458, 498, 341, 3665, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.14796337991390587, "compression_ratio": 1.3868613138686132, "no_speech_prob": 7.243957952596247e-05}, {"id": 176, "seek": 117300, "start": 1184.0, "end": 1189.0, "text": " What is this mysterious multi-head attention thing?", "tokens": [50364, 2639, 661, 1651, 370, 1400, 30, 50564, 50564, 286, 519, 729, 645, 264, 1651, 286, 1401, 370, 1400, 322, 264, 5081, 13, 50914, 50914, 708, 307, 341, 13831, 4825, 12, 1934, 3202, 551, 30, 51164, 51164, 1692, 311, 1071, 2573, 13, 286, 500, 380, 458, 498, 341, 3665, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.14796337991390587, "compression_ratio": 1.3868613138686132, "no_speech_prob": 7.243957952596247e-05}, {"id": 177, "seek": 117300, "start": 1189.0, "end": 1194.0, "text": " Here's another figure. I don't know if this helps.", "tokens": [50364, 2639, 661, 1651, 370, 1400, 30, 50564, 50564, 286, 519, 729, 645, 264, 1651, 286, 1401, 370, 1400, 322, 264, 5081, 13, 50914, 50914, 708, 307, 341, 13831, 4825, 12, 1934, 3202, 551, 30, 51164, 51164, 1692, 311, 1071, 2573, 13, 286, 500, 380, 458, 498, 341, 3665, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.14796337991390587, "compression_ratio": 1.3868613138686132, "no_speech_prob": 7.243957952596247e-05}, {"id": 178, "seek": 119400, "start": 1194.0, "end": 1208.0, "text": " Basically, compute these three quantities called B, K, and Q here. They stamp the query key and value respectively.", "tokens": [50364, 8537, 11, 14722, 613, 1045, 22927, 1219, 363, 11, 591, 11, 293, 1249, 510, 13, 814, 9921, 264, 14581, 2141, 293, 2158, 25009, 13, 51064, 51064, 1144, 264, 4373, 13, 33244, 17784, 6916, 293, 550, 1588, 7186, 473, 264, 23930, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.2702440473768446, "compression_ratio": 1.3571428571428572, "no_speech_prob": 1.0780920092656743e-05}, {"id": 179, "seek": 119400, "start": 1208.0, "end": 1214.0, "text": " Do the scale.product detection operation and then concatenate the outputs.", "tokens": [50364, 8537, 11, 14722, 613, 1045, 22927, 1219, 363, 11, 591, 11, 293, 1249, 510, 13, 814, 9921, 264, 14581, 2141, 293, 2158, 25009, 13, 51064, 51064, 1144, 264, 4373, 13, 33244, 17784, 6916, 293, 550, 1588, 7186, 473, 264, 23930, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.2702440473768446, "compression_ratio": 1.3571428571428572, "no_speech_prob": 1.0780920092656743e-05}, {"id": 180, "seek": 121400, "start": 1214.0, "end": 1228.0, "text": " Drilling into this scale.product detection. Eventually we will run out of boxes to expand. It looks something like this.", "tokens": [50364, 2491, 7345, 666, 341, 4373, 13, 33244, 17784, 13, 17586, 321, 486, 1190, 484, 295, 9002, 281, 5268, 13, 467, 1542, 746, 411, 341, 13, 51064, 51064, 492, 434, 516, 281, 360, 11, 14722, 341, 14581, 2141, 11, 360, 257, 2411, 33244, 337, 2698, 12, 79, 7424, 11, 293, 764, 341, 382, 257, 636, 281, 2408, 493, 264, 4190, 13, 51564, 51564, 1468, 380, 3292, 498, 300, 1177, 380, 652, 2020, 13, 286, 486, 360, 544, 2607, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.23467747758074506, "compression_ratio": 1.4656862745098038, "no_speech_prob": 2.5214426386810374e-06}, {"id": 181, "seek": 121400, "start": 1228.0, "end": 1238.0, "text": " We're going to do, compute this query key, do a.product for self-packs, and use this as a way to sum up the values.", "tokens": [50364, 2491, 7345, 666, 341, 4373, 13, 33244, 17784, 13, 17586, 321, 486, 1190, 484, 295, 9002, 281, 5268, 13, 467, 1542, 746, 411, 341, 13, 51064, 51064, 492, 434, 516, 281, 360, 11, 14722, 341, 14581, 2141, 11, 360, 257, 2411, 33244, 337, 2698, 12, 79, 7424, 11, 293, 764, 341, 382, 257, 636, 281, 2408, 493, 264, 4190, 13, 51564, 51564, 1468, 380, 3292, 498, 300, 1177, 380, 652, 2020, 13, 286, 486, 360, 544, 2607, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.23467747758074506, "compression_ratio": 1.4656862745098038, "no_speech_prob": 2.5214426386810374e-06}, {"id": 182, "seek": 121400, "start": 1238.0, "end": 1243.0, "text": " Don't worry if that doesn't make sense. I will do more detail.", "tokens": [50364, 2491, 7345, 666, 341, 4373, 13, 33244, 17784, 13, 17586, 321, 486, 1190, 484, 295, 9002, 281, 5268, 13, 467, 1542, 746, 411, 341, 13, 51064, 51064, 492, 434, 516, 281, 360, 11, 14722, 341, 14581, 2141, 11, 360, 257, 2411, 33244, 337, 2698, 12, 79, 7424, 11, 293, 764, 341, 382, 257, 636, 281, 2408, 493, 264, 4190, 13, 51564, 51564, 1468, 380, 3292, 498, 300, 1177, 380, 652, 2020, 13, 286, 486, 360, 544, 2607, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.23467747758074506, "compression_ratio": 1.4656862745098038, "no_speech_prob": 2.5214426386810374e-06}, {"id": 183, "seek": 124300, "start": 1243.0, "end": 1249.0, "text": " Let's take this example where the context here is, let's say, these horns silver-white.", "tokens": [50364, 961, 311, 747, 341, 1365, 689, 264, 4319, 510, 307, 11, 718, 311, 584, 11, 613, 28818, 8753, 12, 28865, 13, 50664, 50664, 492, 434, 1382, 281, 6069, 264, 958, 1349, 11, 597, 294, 264, 1365, 949, 390, 28122, 13, 51164, 51164, 1171, 264, 1349, 321, 434, 1382, 281, 6069, 11, 321, 434, 516, 281, 14722, 341, 2158, 1219, 264, 14581, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.21849688616665927, "compression_ratio": 1.60625, "no_speech_prob": 4.5374545152299106e-05}, {"id": 184, "seek": 124300, "start": 1249.0, "end": 1259.0, "text": " We're trying to predict the next word, which in the example before was unicorn.", "tokens": [50364, 961, 311, 747, 341, 1365, 689, 264, 4319, 510, 307, 11, 718, 311, 584, 11, 613, 28818, 8753, 12, 28865, 13, 50664, 50664, 492, 434, 1382, 281, 6069, 264, 958, 1349, 11, 597, 294, 264, 1365, 949, 390, 28122, 13, 51164, 51164, 1171, 264, 1349, 321, 434, 1382, 281, 6069, 11, 321, 434, 516, 281, 14722, 341, 2158, 1219, 264, 14581, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.21849688616665927, "compression_ratio": 1.60625, "no_speech_prob": 4.5374545152299106e-05}, {"id": 185, "seek": 124300, "start": 1259.0, "end": 1264.0, "text": " For the word we're trying to predict, we're going to compute this value called the query.", "tokens": [50364, 961, 311, 747, 341, 1365, 689, 264, 4319, 510, 307, 11, 718, 311, 584, 11, 613, 28818, 8753, 12, 28865, 13, 50664, 50664, 492, 434, 1382, 281, 6069, 264, 958, 1349, 11, 597, 294, 264, 1365, 949, 390, 28122, 13, 51164, 51164, 1171, 264, 1349, 321, 434, 1382, 281, 6069, 11, 321, 434, 516, 281, 14722, 341, 2158, 1219, 264, 14581, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.21849688616665927, "compression_ratio": 1.60625, "no_speech_prob": 4.5374545152299106e-05}, {"id": 186, "seek": 126400, "start": 1264.0, "end": 1275.0, "text": " For all the previous words, we're going to compute the quantity called the key. These are linear layers based on the current state of this layer.", "tokens": [50364, 1171, 439, 264, 3894, 2283, 11, 321, 434, 516, 281, 14722, 264, 11275, 1219, 264, 2141, 13, 1981, 366, 8213, 7914, 2361, 322, 264, 2190, 1785, 295, 341, 4583, 13, 50914, 50914, 17499, 321, 434, 516, 281, 312, 17720, 341, 294, 3124, 13, 492, 434, 516, 281, 312, 2577, 341, 294, 439, 264, 1359, 4365, 294, 264, 3089, 382, 731, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16429236485407903, "compression_ratio": 1.6545454545454545, "no_speech_prob": 8.910567703424022e-05}, {"id": 187, "seek": 126400, "start": 1275.0, "end": 1290.0, "text": " Tomorrow we're going to be coding this in practice. We're going to be seeing this in all the small details in the code as well.", "tokens": [50364, 1171, 439, 264, 3894, 2283, 11, 321, 434, 516, 281, 14722, 264, 11275, 1219, 264, 2141, 13, 1981, 366, 8213, 7914, 2361, 322, 264, 2190, 1785, 295, 341, 4583, 13, 50914, 50914, 17499, 321, 434, 516, 281, 312, 17720, 341, 294, 3124, 13, 492, 434, 516, 281, 312, 2577, 341, 294, 439, 264, 1359, 4365, 294, 264, 3089, 382, 731, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16429236485407903, "compression_ratio": 1.6545454545454545, "no_speech_prob": 8.910567703424022e-05}, {"id": 188, "seek": 129000, "start": 1290.0, "end": 1300.0, "text": " We're going to think of this query as the model asking a question of its context so far. It's going to help us predict what the next word should be.", "tokens": [50364, 492, 434, 516, 281, 519, 295, 341, 14581, 382, 264, 2316, 3365, 257, 1168, 295, 1080, 4319, 370, 1400, 13, 467, 311, 516, 281, 854, 505, 6069, 437, 264, 958, 1349, 820, 312, 13, 50864, 50864, 440, 14581, 727, 312, 746, 411, 11, 980, 385, 437, 264, 3894, 44129, 307, 420, 980, 385, 437, 264, 3894, 3618, 4564, 307, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.21674111485481262, "compression_ratio": 1.6219512195121952, "no_speech_prob": 6.853407285234425e-06}, {"id": 189, "seek": 129000, "start": 1300.0, "end": 1312.0, "text": " The query could be something like, tell me what the previous adjective is or tell me what the previous determiner is.", "tokens": [50364, 492, 434, 516, 281, 519, 295, 341, 14581, 382, 264, 2316, 3365, 257, 1168, 295, 1080, 4319, 370, 1400, 13, 467, 311, 516, 281, 854, 505, 6069, 437, 264, 958, 1349, 820, 312, 13, 50864, 50864, 440, 14581, 727, 312, 746, 411, 11, 980, 385, 437, 264, 3894, 44129, 307, 420, 980, 385, 437, 264, 3894, 3618, 4564, 307, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.21674111485481262, "compression_ratio": 1.6219512195121952, "no_speech_prob": 6.853407285234425e-06}, {"id": 190, "seek": 131200, "start": 1312.0, "end": 1322.0, "text": " The key is going to be things that label the current word.", "tokens": [50364, 440, 2141, 307, 516, 281, 312, 721, 300, 7645, 264, 2190, 1349, 13, 50864, 50864, 639, 1349, 307, 364, 44129, 11, 341, 1349, 307, 257, 3618, 4564, 11, 341, 1349, 307, 257, 9595, 11, 746, 411, 300, 13, 51214, 51214, 1610, 309, 727, 312, 746, 544, 3997, 411, 604, 23211, 9721, 411, 257, 4965, 5158, 420, 746, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.3052094982516381, "compression_ratio": 1.632258064516129, "no_speech_prob": 1.0951161129924003e-05}, {"id": 191, "seek": 131200, "start": 1322.0, "end": 1329.0, "text": " This word is an adjective, this word is a determiner, this word is a verb, something like that.", "tokens": [50364, 440, 2141, 307, 516, 281, 312, 721, 300, 7645, 264, 2190, 1349, 13, 50864, 50864, 639, 1349, 307, 364, 44129, 11, 341, 1349, 307, 257, 3618, 4564, 11, 341, 1349, 307, 257, 9595, 11, 746, 411, 300, 13, 51214, 51214, 1610, 309, 727, 312, 746, 544, 3997, 411, 604, 23211, 9721, 411, 257, 4965, 5158, 420, 746, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.3052094982516381, "compression_ratio": 1.632258064516129, "no_speech_prob": 1.0951161129924003e-05}, {"id": 192, "seek": 131200, "start": 1329.0, "end": 1337.0, "text": " Or it could be something more complex like any arbitrary relation like a coreference or something.", "tokens": [50364, 440, 2141, 307, 516, 281, 312, 721, 300, 7645, 264, 2190, 1349, 13, 50864, 50864, 639, 1349, 307, 364, 44129, 11, 341, 1349, 307, 257, 3618, 4564, 11, 341, 1349, 307, 257, 9595, 11, 746, 411, 300, 13, 51214, 51214, 1610, 309, 727, 312, 746, 544, 3997, 411, 604, 23211, 9721, 411, 257, 4965, 5158, 420, 746, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.3052094982516381, "compression_ratio": 1.632258064516129, "no_speech_prob": 1.0951161129924003e-05}, {"id": 193, "seek": 133700, "start": 1337.0, "end": 1347.0, "text": " The model is going to compute this question as a query. Then it's going to compute, just do a dot product with all of the keys.", "tokens": [50364, 440, 2316, 307, 516, 281, 14722, 341, 1168, 382, 257, 14581, 13, 1396, 309, 311, 516, 281, 14722, 11, 445, 360, 257, 5893, 1674, 365, 439, 295, 264, 9317, 13, 50864, 50864, 1396, 291, 360, 257, 2787, 41167, 382, 731, 13, 639, 307, 516, 281, 41263, 257, 7316, 670, 439, 264, 3894, 2283, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.23920904356857825, "compression_ratio": 1.5374149659863945, "no_speech_prob": 1.1841631931019947e-05}, {"id": 194, "seek": 133700, "start": 1347.0, "end": 1357.0, "text": " Then you do a softmax as well. This is going to induce a distribution over all the previous words.", "tokens": [50364, 440, 2316, 307, 516, 281, 14722, 341, 1168, 382, 257, 14581, 13, 1396, 309, 311, 516, 281, 14722, 11, 445, 360, 257, 5893, 1674, 365, 439, 295, 264, 9317, 13, 50864, 50864, 1396, 291, 360, 257, 2787, 41167, 382, 731, 13, 639, 307, 516, 281, 41263, 257, 7316, 670, 439, 264, 3894, 2283, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.23920904356857825, "compression_ratio": 1.5374149659863945, "no_speech_prob": 1.1841631931019947e-05}, {"id": 195, "seek": 135700, "start": 1357.0, "end": 1367.0, "text": " Here you can imagine a query as something like, tell me what the previous adjective is. The attention will produce this distribution over these three previous words.", "tokens": [50364, 1692, 291, 393, 3811, 257, 14581, 382, 746, 411, 11, 980, 385, 437, 264, 3894, 44129, 307, 13, 440, 3202, 486, 5258, 341, 7316, 670, 613, 1045, 3894, 2283, 13, 50864, 50864, 467, 311, 516, 281, 829, 881, 8482, 36287, 322, 2139, 13482, 420, 8753, 2418, 13, 51364, 51364, 492, 434, 611, 516, 281, 14722, 341, 661, 11275, 1219, 264, 2158, 13, 492, 603, 360, 300, 337, 439, 264, 3894, 2283, 382, 731, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1932386985191932, "compression_ratio": 1.6164383561643836, "no_speech_prob": 1.963685235750745e-06}, {"id": 196, "seek": 135700, "start": 1367.0, "end": 1377.0, "text": " It's going to put most probability maths on either horn or silver white.", "tokens": [50364, 1692, 291, 393, 3811, 257, 14581, 382, 746, 411, 11, 980, 385, 437, 264, 3894, 44129, 307, 13, 440, 3202, 486, 5258, 341, 7316, 670, 613, 1045, 3894, 2283, 13, 50864, 50864, 467, 311, 516, 281, 829, 881, 8482, 36287, 322, 2139, 13482, 420, 8753, 2418, 13, 51364, 51364, 492, 434, 611, 516, 281, 14722, 341, 661, 11275, 1219, 264, 2158, 13, 492, 603, 360, 300, 337, 439, 264, 3894, 2283, 382, 731, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1932386985191932, "compression_ratio": 1.6164383561643836, "no_speech_prob": 1.963685235750745e-06}, {"id": 197, "seek": 135700, "start": 1377.0, "end": 1385.0, "text": " We're also going to compute this other quantity called the value. We'll do that for all the previous words as well.", "tokens": [50364, 1692, 291, 393, 3811, 257, 14581, 382, 746, 411, 11, 980, 385, 437, 264, 3894, 44129, 307, 13, 440, 3202, 486, 5258, 341, 7316, 670, 613, 1045, 3894, 2283, 13, 50864, 50864, 467, 311, 516, 281, 829, 881, 8482, 36287, 322, 2139, 13482, 420, 8753, 2418, 13, 51364, 51364, 492, 434, 611, 516, 281, 14722, 341, 661, 11275, 1219, 264, 2158, 13, 492, 603, 360, 300, 337, 439, 264, 3894, 2283, 382, 731, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1932386985191932, "compression_ratio": 1.6164383561643836, "no_speech_prob": 1.963685235750745e-06}, {"id": 198, "seek": 138500, "start": 1385.0, "end": 1391.0, "text": " Maybe the value will tell us something slightly more about what the content of the word is.", "tokens": [50364, 2704, 264, 2158, 486, 980, 505, 746, 4748, 544, 466, 437, 264, 2701, 295, 264, 1349, 307, 13, 50664, 50664, 1396, 286, 478, 516, 281, 14722, 341, 7633, 1785, 538, 1936, 16885, 3319, 484, 264, 3202, 7316, 13, 51064, 51064], "temperature": 0.0, "avg_logprob": -0.12276224862961542, "compression_ratio": 1.375886524822695, "no_speech_prob": 3.119858956779353e-05}, {"id": 199, "seek": 138500, "start": 1391.0, "end": 1399.0, "text": " Then I'm going to compute this hidden state by basically marginalizing out the attention distribution.", "tokens": [50364, 2704, 264, 2158, 486, 980, 505, 746, 4748, 544, 466, 437, 264, 2701, 295, 264, 1349, 307, 13, 50664, 50664, 1396, 286, 478, 516, 281, 14722, 341, 7633, 1785, 538, 1936, 16885, 3319, 484, 264, 3202, 7316, 13, 51064, 51064], "temperature": 0.0, "avg_logprob": -0.12276224862961542, "compression_ratio": 1.375886524822695, "no_speech_prob": 3.119858956779353e-05}, {"id": 200, "seek": 139900, "start": 1399.0, "end": 1418.0, "text": " Here this hidden state is going to be a weighted sum of the values of all the previous words. It's going to be weighted by the probability of that word.", "tokens": [50364, 1692, 341, 7633, 1785, 307, 516, 281, 312, 257, 32807, 2408, 295, 264, 4190, 295, 439, 264, 3894, 2283, 13, 467, 311, 516, 281, 312, 32807, 538, 264, 8482, 295, 300, 1349, 13, 51314, 51314, 663, 311, 1936, 437, 311, 516, 322, 264, 1411, 1252, 295, 341, 2573, 510, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08197237827159741, "compression_ratio": 1.5492957746478873, "no_speech_prob": 6.852173100924119e-06}, {"id": 201, "seek": 139900, "start": 1418.0, "end": 1422.0, "text": " That's basically what's going on the left side of this figure here.", "tokens": [50364, 1692, 341, 7633, 1785, 307, 516, 281, 312, 257, 32807, 2408, 295, 264, 4190, 295, 439, 264, 3894, 2283, 13, 467, 311, 516, 281, 312, 32807, 538, 264, 8482, 295, 300, 1349, 13, 51314, 51314, 663, 311, 1936, 437, 311, 516, 322, 264, 1411, 1252, 295, 341, 2573, 510, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08197237827159741, "compression_ratio": 1.5492957746478873, "no_speech_prob": 6.852173100924119e-06}, {"id": 202, "seek": 142200, "start": 1422.0, "end": 1430.0, "text": " I left out this detail about the scaling. Let's just try to make the gradients more stable.", "tokens": [50364, 286, 1411, 484, 341, 2607, 466, 264, 21589, 13, 961, 311, 445, 853, 281, 652, 264, 2771, 2448, 544, 8351, 13, 50764, 50764, 821, 311, 1071, 2607, 510, 11, 597, 307, 300, 300, 311, 733, 295, 2167, 12, 28409, 3202, 286, 600, 7619, 370, 1400, 13, 51164, 51164, 492, 434, 767, 516, 281, 360, 341, 551, 1219, 4825, 12, 28409, 3202, 13, 663, 1936, 445, 1355, 321, 434, 516, 281, 14722, 264, 912, 551, 365, 819, 24109, 11, 9317, 11, 293, 4190, 3866, 1413, 294, 8952, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1465558376941052, "compression_ratio": 1.6378600823045268, "no_speech_prob": 3.392545295355376e-06}, {"id": 203, "seek": 142200, "start": 1430.0, "end": 1438.0, "text": " There's another detail here, which is that that's kind of single-headed attention I've described so far.", "tokens": [50364, 286, 1411, 484, 341, 2607, 466, 264, 21589, 13, 961, 311, 445, 853, 281, 652, 264, 2771, 2448, 544, 8351, 13, 50764, 50764, 821, 311, 1071, 2607, 510, 11, 597, 307, 300, 300, 311, 733, 295, 2167, 12, 28409, 3202, 286, 600, 7619, 370, 1400, 13, 51164, 51164, 492, 434, 767, 516, 281, 360, 341, 551, 1219, 4825, 12, 28409, 3202, 13, 663, 1936, 445, 1355, 321, 434, 516, 281, 14722, 264, 912, 551, 365, 819, 24109, 11, 9317, 11, 293, 4190, 3866, 1413, 294, 8952, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1465558376941052, "compression_ratio": 1.6378600823045268, "no_speech_prob": 3.392545295355376e-06}, {"id": 204, "seek": 142200, "start": 1438.0, "end": 1451.0, "text": " We're actually going to do this thing called multi-headed attention. That basically just means we're going to compute the same thing with different queries, keys, and values multiple times in parallel.", "tokens": [50364, 286, 1411, 484, 341, 2607, 466, 264, 21589, 13, 961, 311, 445, 853, 281, 652, 264, 2771, 2448, 544, 8351, 13, 50764, 50764, 821, 311, 1071, 2607, 510, 11, 597, 307, 300, 300, 311, 733, 295, 2167, 12, 28409, 3202, 286, 600, 7619, 370, 1400, 13, 51164, 51164, 492, 434, 767, 516, 281, 360, 341, 551, 1219, 4825, 12, 28409, 3202, 13, 663, 1936, 445, 1355, 321, 434, 516, 281, 14722, 264, 912, 551, 365, 819, 24109, 11, 9317, 11, 293, 4190, 3866, 1413, 294, 8952, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1465558376941052, "compression_ratio": 1.6378600823045268, "no_speech_prob": 3.392545295355376e-06}, {"id": 205, "seek": 145100, "start": 1451.0, "end": 1458.0, "text": " There's a question for what the intuition behind that is.", "tokens": [50364, 821, 311, 257, 1168, 337, 437, 264, 24002, 2261, 300, 307, 13, 50714, 50714, 4083, 11, 291, 767, 528, 281, 458, 3195, 295, 819, 721, 13, 1449, 364, 1365, 13, 51264, 51264, 961, 311, 584, 264, 958, 1349, 510, 820, 312, 28122, 82, 11, 25377, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.30142147064208985, "compression_ratio": 1.3263888888888888, "no_speech_prob": 4.6814471716061234e-05}, {"id": 206, "seek": 145100, "start": 1458.0, "end": 1469.0, "text": " Really, you actually want to know lots of different things. Just an example.", "tokens": [50364, 821, 311, 257, 1168, 337, 437, 264, 24002, 2261, 300, 307, 13, 50714, 50714, 4083, 11, 291, 767, 528, 281, 458, 3195, 295, 819, 721, 13, 1449, 364, 1365, 13, 51264, 51264, 961, 311, 584, 264, 958, 1349, 510, 820, 312, 28122, 82, 11, 25377, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.30142147064208985, "compression_ratio": 1.3263888888888888, "no_speech_prob": 4.6814471716061234e-05}, {"id": 207, "seek": 145100, "start": 1469.0, "end": 1473.0, "text": " Let's say the next word here should be unicorns, plural.", "tokens": [50364, 821, 311, 257, 1168, 337, 437, 264, 24002, 2261, 300, 307, 13, 50714, 50714, 4083, 11, 291, 767, 528, 281, 458, 3195, 295, 819, 721, 13, 1449, 364, 1365, 13, 51264, 51264, 961, 311, 584, 264, 958, 1349, 510, 820, 312, 28122, 82, 11, 25377, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.30142147064208985, "compression_ratio": 1.3263888888888888, "no_speech_prob": 4.6814471716061234e-05}, {"id": 208, "seek": 147300, "start": 1473.0, "end": 1483.0, "text": " To know it should be unicorns, you probably want to know both that it's horned and silver white because the conjunction of those makes it more likely to be a unicorn.", "tokens": [50364, 1407, 458, 309, 820, 312, 28122, 82, 11, 291, 1391, 528, 281, 458, 1293, 300, 309, 311, 13482, 292, 293, 8753, 2418, 570, 264, 27482, 295, 729, 1669, 309, 544, 3700, 281, 312, 257, 28122, 13, 50864, 50864, 509, 611, 528, 281, 458, 300, 264, 1433, 510, 390, 613, 11, 406, 257, 13, 759, 309, 390, 257, 13482, 292, 8753, 2418, 11, 309, 576, 312, 28122, 20010, 13, 51314, 51314, 440, 1186, 309, 311, 613, 1355, 309, 820, 312, 28122, 25377, 11, 370, 291, 483, 257, 25377, 295, 264, 1045, 2283, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.2317912975947062, "compression_ratio": 1.7731481481481481, "no_speech_prob": 2.4679649868630804e-05}, {"id": 209, "seek": 147300, "start": 1483.0, "end": 1492.0, "text": " You also want to know that the term here was these, not a. If it was a horned silver white, it would be unicorn singular.", "tokens": [50364, 1407, 458, 309, 820, 312, 28122, 82, 11, 291, 1391, 528, 281, 458, 1293, 300, 309, 311, 13482, 292, 293, 8753, 2418, 570, 264, 27482, 295, 729, 1669, 309, 544, 3700, 281, 312, 257, 28122, 13, 50864, 50864, 509, 611, 528, 281, 458, 300, 264, 1433, 510, 390, 613, 11, 406, 257, 13, 759, 309, 390, 257, 13482, 292, 8753, 2418, 11, 309, 576, 312, 28122, 20010, 13, 51314, 51314, 440, 1186, 309, 311, 613, 1355, 309, 820, 312, 28122, 25377, 11, 370, 291, 483, 257, 25377, 295, 264, 1045, 2283, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.2317912975947062, "compression_ratio": 1.7731481481481481, "no_speech_prob": 2.4679649868630804e-05}, {"id": 210, "seek": 147300, "start": 1492.0, "end": 1497.0, "text": " The fact it's these means it should be unicorn plural, so you get a plural of the three words.", "tokens": [50364, 1407, 458, 309, 820, 312, 28122, 82, 11, 291, 1391, 528, 281, 458, 1293, 300, 309, 311, 13482, 292, 293, 8753, 2418, 570, 264, 27482, 295, 729, 1669, 309, 544, 3700, 281, 312, 257, 28122, 13, 50864, 50864, 509, 611, 528, 281, 458, 300, 264, 1433, 510, 390, 613, 11, 406, 257, 13, 759, 309, 390, 257, 13482, 292, 8753, 2418, 11, 309, 576, 312, 28122, 20010, 13, 51314, 51314, 440, 1186, 309, 311, 613, 1355, 309, 820, 312, 28122, 25377, 11, 370, 291, 483, 257, 25377, 295, 264, 1045, 2283, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.2317912975947062, "compression_ratio": 1.7731481481481481, "no_speech_prob": 2.4679649868630804e-05}, {"id": 211, "seek": 149700, "start": 1497.0, "end": 1503.0, "text": " You actually need to look at all these three words at once to have a good idea what the next word should be.", "tokens": [50364, 509, 767, 643, 281, 574, 412, 439, 613, 1045, 2283, 412, 1564, 281, 362, 257, 665, 1558, 437, 264, 958, 1349, 820, 312, 13, 50664, 50664, 29238, 12, 28409, 3202, 307, 257, 636, 295, 8295, 1184, 1349, 574, 412, 3866, 3894, 2283, 420, 7512, 13, 51014, 51014, 316, 1168, 510, 307, 983, 366, 321, 767, 294, 643, 295, 1228, 264, 2787, 41167, 30, 51314, 51314, 1545, 360, 321, 764, 264, 2787, 41167, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13069119391503273, "compression_ratio": 1.6, "no_speech_prob": 1.3005475921090692e-05}, {"id": 212, "seek": 149700, "start": 1503.0, "end": 1510.0, "text": " Multi-headed attention is a way of letting each word look at multiple previous words or techniques.", "tokens": [50364, 509, 767, 643, 281, 574, 412, 439, 613, 1045, 2283, 412, 1564, 281, 362, 257, 665, 1558, 437, 264, 958, 1349, 820, 312, 13, 50664, 50664, 29238, 12, 28409, 3202, 307, 257, 636, 295, 8295, 1184, 1349, 574, 412, 3866, 3894, 2283, 420, 7512, 13, 51014, 51014, 316, 1168, 510, 307, 983, 366, 321, 767, 294, 643, 295, 1228, 264, 2787, 41167, 30, 51314, 51314, 1545, 360, 321, 764, 264, 2787, 41167, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13069119391503273, "compression_ratio": 1.6, "no_speech_prob": 1.3005475921090692e-05}, {"id": 213, "seek": 149700, "start": 1510.0, "end": 1516.0, "text": " A question here is why are we actually in need of using the softmax?", "tokens": [50364, 509, 767, 643, 281, 574, 412, 439, 613, 1045, 2283, 412, 1564, 281, 362, 257, 665, 1558, 437, 264, 958, 1349, 820, 312, 13, 50664, 50664, 29238, 12, 28409, 3202, 307, 257, 636, 295, 8295, 1184, 1349, 574, 412, 3866, 3894, 2283, 420, 7512, 13, 51014, 51014, 316, 1168, 510, 307, 983, 366, 321, 767, 294, 643, 295, 1228, 264, 2787, 41167, 30, 51314, 51314, 1545, 360, 321, 764, 264, 2787, 41167, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13069119391503273, "compression_ratio": 1.6, "no_speech_prob": 1.3005475921090692e-05}, {"id": 214, "seek": 149700, "start": 1516.0, "end": 1521.0, "text": " Why do we use the softmax?", "tokens": [50364, 509, 767, 643, 281, 574, 412, 439, 613, 1045, 2283, 412, 1564, 281, 362, 257, 665, 1558, 437, 264, 958, 1349, 820, 312, 13, 50664, 50664, 29238, 12, 28409, 3202, 307, 257, 636, 295, 8295, 1184, 1349, 574, 412, 3866, 3894, 2283, 420, 7512, 13, 51014, 51014, 316, 1168, 510, 307, 983, 366, 321, 767, 294, 643, 295, 1228, 264, 2787, 41167, 30, 51314, 51314, 1545, 360, 321, 764, 264, 2787, 41167, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13069119391503273, "compression_ratio": 1.6, "no_speech_prob": 1.3005475921090692e-05}, {"id": 215, "seek": 152100, "start": 1521.0, "end": 1529.0, "text": " It's a good question.", "tokens": [50364, 467, 311, 257, 665, 1168, 13, 50764, 50764, 20042, 11, 1419, 512, 733, 295, 2710, 2144, 1802, 307, 1391, 665, 13, 51014, 51014, 10328, 11, 562, 291, 352, 281, 2854, 22978, 11, 341, 28811, 576, 483, 3801, 293, 3801, 264, 3052, 291, 1437, 807, 13, 51414, 51414, 407, 1419, 2710, 2144, 307, 1391, 665, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.17990315970727952, "compression_ratio": 1.5121951219512195, "no_speech_prob": 3.480030864011496e-05}, {"id": 216, "seek": 152100, "start": 1529.0, "end": 1534.0, "text": " Firstly, having some kind of normalization effect is probably good.", "tokens": [50364, 467, 311, 257, 665, 1168, 13, 50764, 50764, 20042, 11, 1419, 512, 733, 295, 2710, 2144, 1802, 307, 1391, 665, 13, 51014, 51014, 10328, 11, 562, 291, 352, 281, 2854, 22978, 11, 341, 28811, 576, 483, 3801, 293, 3801, 264, 3052, 291, 1437, 807, 13, 51414, 51414, 407, 1419, 2710, 2144, 307, 1391, 665, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.17990315970727952, "compression_ratio": 1.5121951219512195, "no_speech_prob": 3.480030864011496e-05}, {"id": 217, "seek": 152100, "start": 1534.0, "end": 1542.0, "text": " Otherwise, when you go to longer sequences, this summation would get bigger and bigger the further you went through.", "tokens": [50364, 467, 311, 257, 665, 1168, 13, 50764, 50764, 20042, 11, 1419, 512, 733, 295, 2710, 2144, 1802, 307, 1391, 665, 13, 51014, 51014, 10328, 11, 562, 291, 352, 281, 2854, 22978, 11, 341, 28811, 576, 483, 3801, 293, 3801, 264, 3052, 291, 1437, 807, 13, 51414, 51414, 407, 1419, 2710, 2144, 307, 1391, 665, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.17990315970727952, "compression_ratio": 1.5121951219512195, "no_speech_prob": 3.480030864011496e-05}, {"id": 218, "seek": 152100, "start": 1542.0, "end": 1545.0, "text": " So having normalization is probably good.", "tokens": [50364, 467, 311, 257, 665, 1168, 13, 50764, 50764, 20042, 11, 1419, 512, 733, 295, 2710, 2144, 1802, 307, 1391, 665, 13, 51014, 51014, 10328, 11, 562, 291, 352, 281, 2854, 22978, 11, 341, 28811, 576, 483, 3801, 293, 3801, 264, 3052, 291, 1437, 807, 13, 51414, 51414, 407, 1419, 2710, 2144, 307, 1391, 665, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.17990315970727952, "compression_ratio": 1.5121951219512195, "no_speech_prob": 3.480030864011496e-05}, {"id": 219, "seek": 154500, "start": 1545.0, "end": 1557.0, "text": " Normalization also kind of lets the model discard information too, so it can say this word just isn't relevant, which is good.", "tokens": [50364, 21277, 2144, 611, 733, 295, 6653, 264, 2316, 31597, 1589, 886, 11, 370, 309, 393, 584, 341, 1349, 445, 1943, 380, 7340, 11, 597, 307, 665, 13, 50964, 50964, 286, 519, 286, 362, 1612, 561, 5120, 365, 1228, 721, 411, 10883, 45533, 764, 2602, 11, 597, 2709, 257, 819, 636, 295, 31597, 278, 1589, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.1829859846729343, "compression_ratio": 1.4666666666666666, "no_speech_prob": 2.2826314307167195e-05}, {"id": 220, "seek": 154500, "start": 1557.0, "end": 1564.0, "text": " I think I have seen people experiment with using things like Rayleigh use instead, which gives a different way of discarding information.", "tokens": [50364, 21277, 2144, 611, 733, 295, 6653, 264, 2316, 31597, 1589, 886, 11, 370, 309, 393, 584, 341, 1349, 445, 1943, 380, 7340, 11, 597, 307, 665, 13, 50964, 50964, 286, 519, 286, 362, 1612, 561, 5120, 365, 1228, 721, 411, 10883, 45533, 764, 2602, 11, 597, 2709, 257, 819, 636, 295, 31597, 278, 1589, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.1829859846729343, "compression_ratio": 1.4666666666666666, "no_speech_prob": 2.2826314307167195e-05}, {"id": 221, "seek": 156400, "start": 1564.0, "end": 1575.0, "text": " But I think the evidence of the softmax is the best.", "tokens": [50364, 583, 286, 519, 264, 4467, 295, 264, 2787, 41167, 307, 264, 1151, 13, 50914, 50914, 3996, 1168, 11, 370, 291, 815, 362, 6721, 341, 13, 51064, 51064, 440, 6094, 456, 294, 7022, 11, 437, 300, 307, 11, 10515, 30, 51364, 51364, 1779, 11, 988, 13, 440, 6094, 307, 767, 1021, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.30420507951216263, "compression_ratio": 1.3450704225352113, "no_speech_prob": 8.083547436399385e-05}, {"id": 222, "seek": 156400, "start": 1575.0, "end": 1578.0, "text": " Another question, so you may have missed this.", "tokens": [50364, 583, 286, 519, 264, 4467, 295, 264, 2787, 41167, 307, 264, 1151, 13, 50914, 50914, 3996, 1168, 11, 370, 291, 815, 362, 6721, 341, 13, 51064, 51064, 440, 6094, 456, 294, 7022, 11, 437, 300, 307, 11, 10515, 30, 51364, 51364, 1779, 11, 988, 13, 440, 6094, 307, 767, 1021, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.30420507951216263, "compression_ratio": 1.3450704225352113, "no_speech_prob": 8.083547436399385e-05}, {"id": 223, "seek": 156400, "start": 1578.0, "end": 1584.0, "text": " The mask there in pink, what that is, briefly?", "tokens": [50364, 583, 286, 519, 264, 4467, 295, 264, 2787, 41167, 307, 264, 1151, 13, 50914, 50914, 3996, 1168, 11, 370, 291, 815, 362, 6721, 341, 13, 51064, 51064, 440, 6094, 456, 294, 7022, 11, 437, 300, 307, 11, 10515, 30, 51364, 51364, 1779, 11, 988, 13, 440, 6094, 307, 767, 1021, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.30420507951216263, "compression_ratio": 1.3450704225352113, "no_speech_prob": 8.083547436399385e-05}, {"id": 224, "seek": 156400, "start": 1584.0, "end": 1590.0, "text": " Right, sure. The mask is actually important.", "tokens": [50364, 583, 286, 519, 264, 4467, 295, 264, 2787, 41167, 307, 264, 1151, 13, 50914, 50914, 3996, 1168, 11, 370, 291, 815, 362, 6721, 341, 13, 51064, 51064, 440, 6094, 456, 294, 7022, 11, 437, 300, 307, 11, 10515, 30, 51364, 51364, 1779, 11, 988, 13, 440, 6094, 307, 767, 1021, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.30420507951216263, "compression_ratio": 1.3450704225352113, "no_speech_prob": 8.083547436399385e-05}, {"id": 225, "seek": 159000, "start": 1590.0, "end": 1595.0, "text": " I was going to say, make a point for this.", "tokens": [50364, 286, 390, 516, 281, 584, 11, 652, 257, 935, 337, 341, 13, 50614, 50614, 1485, 295, 264, 955, 10641, 466, 341, 1379, 992, 295, 4825, 12, 28409, 3202, 307, 300, 309, 311, 4664, 8952, 22395, 13, 50964, 50964, 407, 264, 24903, 295, 24109, 11, 9317, 11, 293, 4190, 412, 633, 565, 1823, 5946, 322, 437, 291, 434, 884, 412, 604, 295, 264, 661, 565, 4439, 13, 51364, 51364, 407, 8343, 257, 18680, 1753, 3209, 11, 291, 393, 767, 14722, 439, 295, 613, 16561, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.307077862999656, "compression_ratio": 1.5695652173913044, "no_speech_prob": 8.138771590893157e-06}, {"id": 226, "seek": 159000, "start": 1595.0, "end": 1602.0, "text": " One of the big wins about this whole set of multi-headed attention is that it's extremely parallelizable.", "tokens": [50364, 286, 390, 516, 281, 584, 11, 652, 257, 935, 337, 341, 13, 50614, 50614, 1485, 295, 264, 955, 10641, 466, 341, 1379, 992, 295, 4825, 12, 28409, 3202, 307, 300, 309, 311, 4664, 8952, 22395, 13, 50964, 50964, 407, 264, 24903, 295, 24109, 11, 9317, 11, 293, 4190, 412, 633, 565, 1823, 5946, 322, 437, 291, 434, 884, 412, 604, 295, 264, 661, 565, 4439, 13, 51364, 51364, 407, 8343, 257, 18680, 1753, 3209, 11, 291, 393, 767, 14722, 439, 295, 613, 16561, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.307077862999656, "compression_ratio": 1.5695652173913044, "no_speech_prob": 8.138771590893157e-06}, {"id": 227, "seek": 159000, "start": 1602.0, "end": 1610.0, "text": " So the computation of queries, keys, and values at every time step depends on what you're doing at any of the other time steps.", "tokens": [50364, 286, 390, 516, 281, 584, 11, 652, 257, 935, 337, 341, 13, 50614, 50614, 1485, 295, 264, 955, 10641, 466, 341, 1379, 992, 295, 4825, 12, 28409, 3202, 307, 300, 309, 311, 4664, 8952, 22395, 13, 50964, 50964, 407, 264, 24903, 295, 24109, 11, 9317, 11, 293, 4190, 412, 633, 565, 1823, 5946, 322, 437, 291, 434, 884, 412, 604, 295, 264, 661, 565, 4439, 13, 51364, 51364, 407, 8343, 257, 18680, 1753, 3209, 11, 291, 393, 767, 14722, 439, 295, 613, 16561, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.307077862999656, "compression_ratio": 1.5695652173913044, "no_speech_prob": 8.138771590893157e-06}, {"id": 228, "seek": 159000, "start": 1610.0, "end": 1616.0, "text": " So unlike a recurrent network, you can actually compute all of these simultaneously.", "tokens": [50364, 286, 390, 516, 281, 584, 11, 652, 257, 935, 337, 341, 13, 50614, 50614, 1485, 295, 264, 955, 10641, 466, 341, 1379, 992, 295, 4825, 12, 28409, 3202, 307, 300, 309, 311, 4664, 8952, 22395, 13, 50964, 50964, 407, 264, 24903, 295, 24109, 11, 9317, 11, 293, 4190, 412, 633, 565, 1823, 5946, 322, 437, 291, 434, 884, 412, 604, 295, 264, 661, 565, 4439, 13, 51364, 51364, 407, 8343, 257, 18680, 1753, 3209, 11, 291, 393, 767, 14722, 439, 295, 613, 16561, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.307077862999656, "compression_ratio": 1.5695652173913044, "no_speech_prob": 8.138771590893157e-06}, {"id": 229, "seek": 161600, "start": 1616.0, "end": 1620.0, "text": " And that works very well with the kind of hardware we have these days.", "tokens": [50364, 400, 300, 1985, 588, 731, 365, 264, 733, 295, 8837, 321, 362, 613, 1708, 13, 50564, 50564, 407, 406, 787, 366, 321, 516, 281, 14722, 439, 264, 819, 8050, 412, 1564, 11, 321, 434, 516, 281, 14722, 439, 264, 565, 4439, 412, 1564, 294, 257, 2167, 2128, 1320, 13, 51014, 51014, 407, 300, 311, 869, 11, 3993, 300, 498, 291, 434, 15866, 439, 264, 565, 4439, 412, 1564, 11, 456, 311, 1825, 281, 767, 1590, 291, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13601107597351075, "compression_ratio": 1.7978142076502732, "no_speech_prob": 1.3006426343054045e-05}, {"id": 230, "seek": 161600, "start": 1620.0, "end": 1629.0, "text": " So not only are we going to compute all the different heads at once, we're going to compute all the time steps at once in a single forward pass.", "tokens": [50364, 400, 300, 1985, 588, 731, 365, 264, 733, 295, 8837, 321, 362, 613, 1708, 13, 50564, 50564, 407, 406, 787, 366, 321, 516, 281, 14722, 439, 264, 819, 8050, 412, 1564, 11, 321, 434, 516, 281, 14722, 439, 264, 565, 4439, 412, 1564, 294, 257, 2167, 2128, 1320, 13, 51014, 51014, 407, 300, 311, 869, 11, 3993, 300, 498, 291, 434, 15866, 439, 264, 565, 4439, 412, 1564, 11, 456, 311, 1825, 281, 767, 1590, 291, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13601107597351075, "compression_ratio": 1.7978142076502732, "no_speech_prob": 1.3006426343054045e-05}, {"id": 231, "seek": 161600, "start": 1629.0, "end": 1639.0, "text": " So that's great, except that if you're computing all the time steps at once, there's nothing to actually stop you", "tokens": [50364, 400, 300, 1985, 588, 731, 365, 264, 733, 295, 8837, 321, 362, 613, 1708, 13, 50564, 50564, 407, 406, 787, 366, 321, 516, 281, 14722, 439, 264, 819, 8050, 412, 1564, 11, 321, 434, 516, 281, 14722, 439, 264, 565, 4439, 412, 1564, 294, 257, 2167, 2128, 1320, 13, 51014, 51014, 407, 300, 311, 869, 11, 3993, 300, 498, 291, 434, 15866, 439, 264, 565, 4439, 412, 1564, 11, 456, 311, 1825, 281, 767, 1590, 291, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13601107597351075, "compression_ratio": 1.7978142076502732, "no_speech_prob": 1.3006426343054045e-05}, {"id": 232, "seek": 163900, "start": 1639.0, "end": 1647.0, "text": " from computing all the time steps at once, you're going to be able to compute all the time steps at once.", "tokens": [50364, 490, 15866, 439, 264, 565, 4439, 412, 1564, 11, 291, 434, 516, 281, 312, 1075, 281, 14722, 439, 264, 565, 4439, 412, 1564, 13, 50764, 50764, 407, 300, 311, 257, 869, 935, 13, 50914, 50914, 407, 718, 311, 574, 412, 512, 295, 264, 661, 721, 300, 321, 393, 360, 281, 854, 505, 1223, 577, 281, 360, 341, 13, 51214, 51214, 407, 718, 311, 574, 412, 512, 295, 264, 661, 721, 300, 321, 393, 360, 281, 854, 505, 1223, 577, 281, 360, 341, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.8001925589024336, "compression_ratio": 2.237762237762238, "no_speech_prob": 2.0143785150139593e-05}, {"id": 233, "seek": 163900, "start": 1647.0, "end": 1650.0, "text": " So that's a great point.", "tokens": [50364, 490, 15866, 439, 264, 565, 4439, 412, 1564, 11, 291, 434, 516, 281, 312, 1075, 281, 14722, 439, 264, 565, 4439, 412, 1564, 13, 50764, 50764, 407, 300, 311, 257, 869, 935, 13, 50914, 50914, 407, 718, 311, 574, 412, 512, 295, 264, 661, 721, 300, 321, 393, 360, 281, 854, 505, 1223, 577, 281, 360, 341, 13, 51214, 51214, 407, 718, 311, 574, 412, 512, 295, 264, 661, 721, 300, 321, 393, 360, 281, 854, 505, 1223, 577, 281, 360, 341, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.8001925589024336, "compression_ratio": 2.237762237762238, "no_speech_prob": 2.0143785150139593e-05}, {"id": 234, "seek": 163900, "start": 1650.0, "end": 1656.0, "text": " So let's look at some of the other things that we can do to help us understand how to do this.", "tokens": [50364, 490, 15866, 439, 264, 565, 4439, 412, 1564, 11, 291, 434, 516, 281, 312, 1075, 281, 14722, 439, 264, 565, 4439, 412, 1564, 13, 50764, 50764, 407, 300, 311, 257, 869, 935, 13, 50914, 50914, 407, 718, 311, 574, 412, 512, 295, 264, 661, 721, 300, 321, 393, 360, 281, 854, 505, 1223, 577, 281, 360, 341, 13, 51214, 51214, 407, 718, 311, 574, 412, 512, 295, 264, 661, 721, 300, 321, 393, 360, 281, 854, 505, 1223, 577, 281, 360, 341, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.8001925589024336, "compression_ratio": 2.237762237762238, "no_speech_prob": 2.0143785150139593e-05}, {"id": 235, "seek": 163900, "start": 1656.0, "end": 1661.0, "text": " So let's look at some of the other things that we can do to help us understand how to do this.", "tokens": [50364, 490, 15866, 439, 264, 565, 4439, 412, 1564, 11, 291, 434, 516, 281, 312, 1075, 281, 14722, 439, 264, 565, 4439, 412, 1564, 13, 50764, 50764, 407, 300, 311, 257, 869, 935, 13, 50914, 50914, 407, 718, 311, 574, 412, 512, 295, 264, 661, 721, 300, 321, 393, 360, 281, 854, 505, 1223, 577, 281, 360, 341, 13, 51214, 51214, 407, 718, 311, 574, 412, 512, 295, 264, 661, 721, 300, 321, 393, 360, 281, 854, 505, 1223, 577, 281, 360, 341, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.8001925589024336, "compression_ratio": 2.237762237762238, "no_speech_prob": 2.0143785150139593e-05}, {"id": 236, "seek": 166100, "start": 1661.0, "end": 1669.0, "text": " So the solutions right here is what we call self-potential masking.", "tokens": [50364, 407, 264, 6547, 558, 510, 307, 437, 321, 818, 2698, 12, 17698, 2549, 31226, 13, 50764, 50764, 407, 257, 6094, 307, 445, 364, 6597, 38190, 8141, 300, 1333, 295, 575, 35193, 294, 264, 3126, 13369, 293, 3671, 13202, 294, 264, 6597, 13369, 13, 51264, 51264, 400, 321, 434, 516, 281, 445, 909, 341, 281, 264, 3202, 13444, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.2075290064657888, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00012915211846120656}, {"id": 237, "seek": 166100, "start": 1669.0, "end": 1679.0, "text": " So a mask is just an upper triangular matrix that sort of has zeros in the lower triangle and negative infinity in the upper triangle.", "tokens": [50364, 407, 264, 6547, 558, 510, 307, 437, 321, 818, 2698, 12, 17698, 2549, 31226, 13, 50764, 50764, 407, 257, 6094, 307, 445, 364, 6597, 38190, 8141, 300, 1333, 295, 575, 35193, 294, 264, 3126, 13369, 293, 3671, 13202, 294, 264, 6597, 13369, 13, 51264, 51264, 400, 321, 434, 516, 281, 445, 909, 341, 281, 264, 3202, 13444, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.2075290064657888, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00012915211846120656}, {"id": 238, "seek": 166100, "start": 1679.0, "end": 1683.0, "text": " And we're going to just add this to the attention scores.", "tokens": [50364, 407, 264, 6547, 558, 510, 307, 437, 321, 818, 2698, 12, 17698, 2549, 31226, 13, 50764, 50764, 407, 257, 6094, 307, 445, 364, 6597, 38190, 8141, 300, 1333, 295, 575, 35193, 294, 264, 3126, 13369, 293, 3671, 13202, 294, 264, 6597, 13369, 13, 51264, 51264, 400, 321, 434, 516, 281, 445, 909, 341, 281, 264, 3202, 13444, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.2075290064657888, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00012915211846120656}, {"id": 239, "seek": 168300, "start": 1683.0, "end": 1694.0, "text": " And the effect of that is going to be that every word to the left has a much higher attention score than every word to the right, so the model will only end up practicing using words to the left.", "tokens": [50364, 400, 264, 1802, 295, 300, 307, 516, 281, 312, 300, 633, 1349, 281, 264, 1411, 575, 257, 709, 2946, 3202, 6175, 813, 633, 1349, 281, 264, 558, 11, 370, 264, 2316, 486, 787, 917, 493, 11350, 1228, 2283, 281, 264, 1411, 13, 50914, 50914, 639, 307, 15957, 3142, 6094, 1553, 3847, 712, 17443, 13, 51114, 51114, 1449, 4190, 2139, 4018, 420, 3671, 13202, 13, 51314, 51314, 407, 291, 1116, 787, 6094, 294, 1389, 295, 364, 3861, 2685, 3097, 5633, 11, 3006, 30, 51564, 51564, 759, 291, 632, 281, 445, 1322, 33358, 11, 337, 1365, 11, 291, 2759, 380, 643, 281, 6094, 570, 309, 2759, 380, 1871, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1349127228195603, "compression_ratio": 1.6736111111111112, "no_speech_prob": 2.4292430680361576e-05}, {"id": 240, "seek": 168300, "start": 1694.0, "end": 1698.0, "text": " This is deterministic mask without trainable weights.", "tokens": [50364, 400, 264, 1802, 295, 300, 307, 516, 281, 312, 300, 633, 1349, 281, 264, 1411, 575, 257, 709, 2946, 3202, 6175, 813, 633, 1349, 281, 264, 558, 11, 370, 264, 2316, 486, 787, 917, 493, 11350, 1228, 2283, 281, 264, 1411, 13, 50914, 50914, 639, 307, 15957, 3142, 6094, 1553, 3847, 712, 17443, 13, 51114, 51114, 1449, 4190, 2139, 4018, 420, 3671, 13202, 13, 51314, 51314, 407, 291, 1116, 787, 6094, 294, 1389, 295, 364, 3861, 2685, 3097, 5633, 11, 3006, 30, 51564, 51564, 759, 291, 632, 281, 445, 1322, 33358, 11, 337, 1365, 11, 291, 2759, 380, 643, 281, 6094, 570, 309, 2759, 380, 1871, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1349127228195603, "compression_ratio": 1.6736111111111112, "no_speech_prob": 2.4292430680361576e-05}, {"id": 241, "seek": 168300, "start": 1698.0, "end": 1702.0, "text": " Just values either zero or negative infinity.", "tokens": [50364, 400, 264, 1802, 295, 300, 307, 516, 281, 312, 300, 633, 1349, 281, 264, 1411, 575, 257, 709, 2946, 3202, 6175, 813, 633, 1349, 281, 264, 558, 11, 370, 264, 2316, 486, 787, 917, 493, 11350, 1228, 2283, 281, 264, 1411, 13, 50914, 50914, 639, 307, 15957, 3142, 6094, 1553, 3847, 712, 17443, 13, 51114, 51114, 1449, 4190, 2139, 4018, 420, 3671, 13202, 13, 51314, 51314, 407, 291, 1116, 787, 6094, 294, 1389, 295, 364, 3861, 2685, 3097, 5633, 11, 3006, 30, 51564, 51564, 759, 291, 632, 281, 445, 1322, 33358, 11, 337, 1365, 11, 291, 2759, 380, 643, 281, 6094, 570, 309, 2759, 380, 1871, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1349127228195603, "compression_ratio": 1.6736111111111112, "no_speech_prob": 2.4292430680361576e-05}, {"id": 242, "seek": 168300, "start": 1702.0, "end": 1707.0, "text": " So you'd only mask in case of an application specific training task, correct?", "tokens": [50364, 400, 264, 1802, 295, 300, 307, 516, 281, 312, 300, 633, 1349, 281, 264, 1411, 575, 257, 709, 2946, 3202, 6175, 813, 633, 1349, 281, 264, 558, 11, 370, 264, 2316, 486, 787, 917, 493, 11350, 1228, 2283, 281, 264, 1411, 13, 50914, 50914, 639, 307, 15957, 3142, 6094, 1553, 3847, 712, 17443, 13, 51114, 51114, 1449, 4190, 2139, 4018, 420, 3671, 13202, 13, 51314, 51314, 407, 291, 1116, 787, 6094, 294, 1389, 295, 364, 3861, 2685, 3097, 5633, 11, 3006, 30, 51564, 51564, 759, 291, 632, 281, 445, 1322, 33358, 11, 337, 1365, 11, 291, 2759, 380, 643, 281, 6094, 570, 309, 2759, 380, 1871, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1349127228195603, "compression_ratio": 1.6736111111111112, "no_speech_prob": 2.4292430680361576e-05}, {"id": 243, "seek": 168300, "start": 1707.0, "end": 1712.0, "text": " If you had to just build representations, for example, you wouldn't need to mask because it wouldn't matter?", "tokens": [50364, 400, 264, 1802, 295, 300, 307, 516, 281, 312, 300, 633, 1349, 281, 264, 1411, 575, 257, 709, 2946, 3202, 6175, 813, 633, 1349, 281, 264, 558, 11, 370, 264, 2316, 486, 787, 917, 493, 11350, 1228, 2283, 281, 264, 1411, 13, 50914, 50914, 639, 307, 15957, 3142, 6094, 1553, 3847, 712, 17443, 13, 51114, 51114, 1449, 4190, 2139, 4018, 420, 3671, 13202, 13, 51314, 51314, 407, 291, 1116, 787, 6094, 294, 1389, 295, 364, 3861, 2685, 3097, 5633, 11, 3006, 30, 51564, 51564, 759, 291, 632, 281, 445, 1322, 33358, 11, 337, 1365, 11, 291, 2759, 380, 643, 281, 6094, 570, 309, 2759, 380, 1871, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1349127228195603, "compression_ratio": 1.6736111111111112, "no_speech_prob": 2.4292430680361576e-05}, {"id": 244, "seek": 171200, "start": 1712.0, "end": 1719.0, "text": " Yeah, that's a great question. So we'll go on to more general representation learning later.", "tokens": [50364, 865, 11, 300, 311, 257, 869, 1168, 13, 407, 321, 603, 352, 322, 281, 544, 2674, 10290, 2539, 1780, 13, 50714, 50714, 4534, 295, 437, 291, 445, 528, 281, 2487, 2058, 19866, 11, 291, 500, 380, 643, 281, 6094, 552, 13, 363, 327, 621, 41048, 4319, 307, 3122, 4961, 13, 51014, 51014, 682, 341, 1389, 295, 2856, 15983, 11, 597, 321, 434, 1364, 807, 370, 1400, 11, 550, 264, 6094, 307, 1333, 295, 11462, 281, 652, 264, 2316, 44003, 3006, 293, 14722, 264, 3006, 5952, 2144, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.21297662336747725, "compression_ratio": 1.5697211155378485, "no_speech_prob": 4.6827281039441004e-05}, {"id": 245, "seek": 171200, "start": 1719.0, "end": 1725.0, "text": " Most of what you just want to text encoder, you don't need to mask them. Bidirectional context is absolutely helpful.", "tokens": [50364, 865, 11, 300, 311, 257, 869, 1168, 13, 407, 321, 603, 352, 322, 281, 544, 2674, 10290, 2539, 1780, 13, 50714, 50714, 4534, 295, 437, 291, 445, 528, 281, 2487, 2058, 19866, 11, 291, 500, 380, 643, 281, 6094, 552, 13, 363, 327, 621, 41048, 4319, 307, 3122, 4961, 13, 51014, 51014, 682, 341, 1389, 295, 2856, 15983, 11, 597, 321, 434, 1364, 807, 370, 1400, 11, 550, 264, 6094, 307, 1333, 295, 11462, 281, 652, 264, 2316, 44003, 3006, 293, 14722, 264, 3006, 5952, 2144, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.21297662336747725, "compression_ratio": 1.5697211155378485, "no_speech_prob": 4.6827281039441004e-05}, {"id": 246, "seek": 171200, "start": 1725.0, "end": 1734.0, "text": " In this case of language modeling, which we're working through so far, then the mask is sort of crucial to make the model mathematically correct and compute the correct factorization.", "tokens": [50364, 865, 11, 300, 311, 257, 869, 1168, 13, 407, 321, 603, 352, 322, 281, 544, 2674, 10290, 2539, 1780, 13, 50714, 50714, 4534, 295, 437, 291, 445, 528, 281, 2487, 2058, 19866, 11, 291, 500, 380, 643, 281, 6094, 552, 13, 363, 327, 621, 41048, 4319, 307, 3122, 4961, 13, 51014, 51014, 682, 341, 1389, 295, 2856, 15983, 11, 597, 321, 434, 1364, 807, 370, 1400, 11, 550, 264, 6094, 307, 1333, 295, 11462, 281, 652, 264, 2316, 44003, 3006, 293, 14722, 264, 3006, 5952, 2144, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.21297662336747725, "compression_ratio": 1.5697211155378485, "no_speech_prob": 4.6827281039441004e-05}, {"id": 247, "seek": 173400, "start": 1734.0, "end": 1743.0, "text": " We get a great question.", "tokens": [50364, 492, 483, 257, 869, 1168, 13, 50814, 50814, 1033, 13, 51114, 51114, 407, 472, 661, 2607, 321, 643, 281, 360, 281, 652, 439, 341, 589, 307, 909, 746, 281, 264, 4846, 3089, 337, 2535, 304, 12240, 3584, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.287783940633138, "compression_ratio": 1.2782608695652173, "no_speech_prob": 2.5394590920768678e-05}, {"id": 248, "seek": 173400, "start": 1743.0, "end": 1749.0, "text": " Okay.", "tokens": [50364, 492, 483, 257, 869, 1168, 13, 50814, 50814, 1033, 13, 51114, 51114, 407, 472, 661, 2607, 321, 643, 281, 360, 281, 652, 439, 341, 589, 307, 909, 746, 281, 264, 4846, 3089, 337, 2535, 304, 12240, 3584, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.287783940633138, "compression_ratio": 1.2782608695652173, "no_speech_prob": 2.5394590920768678e-05}, {"id": 249, "seek": 173400, "start": 1749.0, "end": 1758.0, "text": " So one other detail we need to do to make all this work is add something to the input code for positional embedding.", "tokens": [50364, 492, 483, 257, 869, 1168, 13, 50814, 50814, 1033, 13, 51114, 51114, 407, 472, 661, 2607, 321, 643, 281, 360, 281, 652, 439, 341, 589, 307, 909, 746, 281, 264, 4846, 3089, 337, 2535, 304, 12240, 3584, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.287783940633138, "compression_ratio": 1.2782608695652173, "no_speech_prob": 2.5394590920768678e-05}, {"id": 250, "seek": 175800, "start": 1758.0, "end": 1767.0, "text": " So as I've described the model so far, it's actually", "tokens": [50364, 407, 382, 286, 600, 7619, 264, 2316, 370, 1400, 11, 309, 311, 767, 50814, 50814, 309, 311, 884, 257, 707, 12577, 2316, 13, 467, 3255, 588, 707, 466, 2856, 13, 440, 15743, 727, 312, 1340, 293, 309, 576, 589, 13, 51364, 51364, 682, 1729, 11, 291, 393, 2316, 257, 992, 420, 257, 4295, 420, 1340, 411, 341, 13, 467, 820, 312, 2489, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.24726369487705516, "compression_ratio": 1.5029239766081872, "no_speech_prob": 7.766026101307943e-06}, {"id": 251, "seek": 175800, "start": 1767.0, "end": 1778.0, "text": " it's doing a little bias model. It knows very little about language. The inputs could be anything and it would work.", "tokens": [50364, 407, 382, 286, 600, 7619, 264, 2316, 370, 1400, 11, 309, 311, 767, 50814, 50814, 309, 311, 884, 257, 707, 12577, 2316, 13, 467, 3255, 588, 707, 466, 2856, 13, 440, 15743, 727, 312, 1340, 293, 309, 576, 589, 13, 51364, 51364, 682, 1729, 11, 291, 393, 2316, 257, 992, 420, 257, 4295, 420, 1340, 411, 341, 13, 467, 820, 312, 2489, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.24726369487705516, "compression_ratio": 1.5029239766081872, "no_speech_prob": 7.766026101307943e-06}, {"id": 252, "seek": 175800, "start": 1778.0, "end": 1783.0, "text": " In particular, you can model a set or a graph or anything like this. It should be fine.", "tokens": [50364, 407, 382, 286, 600, 7619, 264, 2316, 370, 1400, 11, 309, 311, 767, 50814, 50814, 309, 311, 884, 257, 707, 12577, 2316, 13, 467, 3255, 588, 707, 466, 2856, 13, 440, 15743, 727, 312, 1340, 293, 309, 576, 589, 13, 51364, 51364, 682, 1729, 11, 291, 393, 2316, 257, 992, 420, 257, 4295, 420, 1340, 411, 341, 13, 467, 820, 312, 2489, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.24726369487705516, "compression_ratio": 1.5029239766081872, "no_speech_prob": 7.766026101307943e-06}, {"id": 253, "seek": 178300, "start": 1783.0, "end": 1790.0, "text": " You know, I mean, in language, there are some properties which are useful, like, for example,", "tokens": [50364, 509, 458, 11, 286, 914, 11, 294, 2856, 11, 456, 366, 512, 7221, 597, 366, 4420, 11, 411, 11, 337, 1365, 11, 50714, 50714, 456, 311, 364, 21739, 281, 2283, 11, 597, 307, 534, 1021, 281, 577, 291, 7302, 552, 13, 51014, 51014, 400, 341, 2316, 1177, 380, 767, 458, 1340, 466, 300, 13, 400, 300, 311, 294, 8712, 281, 264, 45216, 304, 5245, 293, 264, 2190, 5245, 286, 4712, 291, 3071, 11, 597, 1293, 362, 819, 2098, 295, 43430, 264, 1668, 295, 2487, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1508644725499528, "compression_ratio": 1.6401673640167365, "no_speech_prob": 9.222932931152172e-06}, {"id": 254, "seek": 178300, "start": 1790.0, "end": 1796.0, "text": " there's an ordering to words, which is really important to how you interpret them.", "tokens": [50364, 509, 458, 11, 286, 914, 11, 294, 2856, 11, 456, 366, 512, 7221, 597, 366, 4420, 11, 411, 11, 337, 1365, 11, 50714, 50714, 456, 311, 364, 21739, 281, 2283, 11, 597, 307, 534, 1021, 281, 577, 291, 7302, 552, 13, 51014, 51014, 400, 341, 2316, 1177, 380, 767, 458, 1340, 466, 300, 13, 400, 300, 311, 294, 8712, 281, 264, 45216, 304, 5245, 293, 264, 2190, 5245, 286, 4712, 291, 3071, 11, 597, 1293, 362, 819, 2098, 295, 43430, 264, 1668, 295, 2487, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1508644725499528, "compression_ratio": 1.6401673640167365, "no_speech_prob": 9.222932931152172e-06}, {"id": 255, "seek": 178300, "start": 1796.0, "end": 1809.0, "text": " And this model doesn't actually know anything about that. And that's in contrast to the convolutional models and the current models I showed you earlier, which both have different ways of encoding the order of text.", "tokens": [50364, 509, 458, 11, 286, 914, 11, 294, 2856, 11, 456, 366, 512, 7221, 597, 366, 4420, 11, 411, 11, 337, 1365, 11, 50714, 50714, 456, 311, 364, 21739, 281, 2283, 11, 597, 307, 534, 1021, 281, 577, 291, 7302, 552, 13, 51014, 51014, 400, 341, 2316, 1177, 380, 767, 458, 1340, 466, 300, 13, 400, 300, 311, 294, 8712, 281, 264, 45216, 304, 5245, 293, 264, 2190, 5245, 286, 4712, 291, 3071, 11, 597, 1293, 362, 819, 2098, 295, 43430, 264, 1668, 295, 2487, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1508644725499528, "compression_ratio": 1.6401673640167365, "no_speech_prob": 9.222932931152172e-06}, {"id": 256, "seek": 180900, "start": 1809.0, "end": 1816.0, "text": " So one of the techniques that was introduced in this paper was called positional embedding.", "tokens": [50364, 407, 472, 295, 264, 7512, 300, 390, 7268, 294, 341, 3035, 390, 1219, 2535, 304, 12240, 3584, 13, 50714, 50714, 821, 366, 819, 2098, 291, 393, 360, 341, 13, 814, 6786, 746, 294, 264, 3035, 11, 597, 307, 4748, 3657, 13, 5135, 11, 286, 478, 406, 516, 281, 6786, 11, 457, 309, 1985, 445, 382, 731, 281, 4476, 51214, 51214, 1466, 257, 4994, 12240, 3584, 337, 633, 565, 1823, 13, 407, 337, 633, 2535, 294, 264, 4166, 11, 1958, 11, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 291, 445, 1466, 257, 4994, 12240, 3584, 293, 550, 291, 909, 341, 281, 428, 15743, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1229903828014027, "compression_ratio": 1.6902985074626866, "no_speech_prob": 1.9522836737451144e-05}, {"id": 257, "seek": 180900, "start": 1816.0, "end": 1826.0, "text": " There are different ways you can do this. They describe something in the paper, which is slightly weird. Actually, I'm not going to describe, but it works just as well to essentially", "tokens": [50364, 407, 472, 295, 264, 7512, 300, 390, 7268, 294, 341, 3035, 390, 1219, 2535, 304, 12240, 3584, 13, 50714, 50714, 821, 366, 819, 2098, 291, 393, 360, 341, 13, 814, 6786, 746, 294, 264, 3035, 11, 597, 307, 4748, 3657, 13, 5135, 11, 286, 478, 406, 516, 281, 6786, 11, 457, 309, 1985, 445, 382, 731, 281, 4476, 51214, 51214, 1466, 257, 4994, 12240, 3584, 337, 633, 565, 1823, 13, 407, 337, 633, 2535, 294, 264, 4166, 11, 1958, 11, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 291, 445, 1466, 257, 4994, 12240, 3584, 293, 550, 291, 909, 341, 281, 428, 15743, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1229903828014027, "compression_ratio": 1.6902985074626866, "no_speech_prob": 1.9522836737451144e-05}, {"id": 258, "seek": 180900, "start": 1826.0, "end": 1836.0, "text": " learn a separate embedding for every time step. So for every position in the document, 0, 1, 2, 3, 4, 5, you just learn a separate embedding and then you add this to your inputs.", "tokens": [50364, 407, 472, 295, 264, 7512, 300, 390, 7268, 294, 341, 3035, 390, 1219, 2535, 304, 12240, 3584, 13, 50714, 50714, 821, 366, 819, 2098, 291, 393, 360, 341, 13, 814, 6786, 746, 294, 264, 3035, 11, 597, 307, 4748, 3657, 13, 5135, 11, 286, 478, 406, 516, 281, 6786, 11, 457, 309, 1985, 445, 382, 731, 281, 4476, 51214, 51214, 1466, 257, 4994, 12240, 3584, 337, 633, 565, 1823, 13, 407, 337, 633, 2535, 294, 264, 4166, 11, 1958, 11, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 291, 445, 1466, 257, 4994, 12240, 3584, 293, 550, 291, 909, 341, 281, 428, 15743, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1229903828014027, "compression_ratio": 1.6902985074626866, "no_speech_prob": 1.9522836737451144e-05}, {"id": 259, "seek": 183600, "start": 1836.0, "end": 1843.0, "text": " So your input now is a summation of the word vector and some kind of positional vector.", "tokens": [50364, 407, 428, 4846, 586, 307, 257, 28811, 295, 264, 1349, 8062, 293, 512, 733, 295, 2535, 304, 8062, 13, 50714, 50714, 400, 309, 311, 588, 2199, 11, 457, 309, 2709, 264, 2316, 264, 733, 295, 264, 1668, 295, 1589, 309, 2203, 293, 309, 1985, 869, 13, 51114, 51114, 1033, 11, 370, 983, 366, 613, 5245, 370, 665, 30, 1545, 775, 1518, 764, 552, 30, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.13545330832986271, "compression_ratio": 1.5317919075144508, "no_speech_prob": 2.769108959910227e-06}, {"id": 260, "seek": 183600, "start": 1843.0, "end": 1851.0, "text": " And it's very simple, but it gives the model the kind of the order of information it needs and it works great.", "tokens": [50364, 407, 428, 4846, 586, 307, 257, 28811, 295, 264, 1349, 8062, 293, 512, 733, 295, 2535, 304, 8062, 13, 50714, 50714, 400, 309, 311, 588, 2199, 11, 457, 309, 2709, 264, 2316, 264, 733, 295, 264, 1668, 295, 1589, 309, 2203, 293, 309, 1985, 869, 13, 51114, 51114, 1033, 11, 370, 983, 366, 613, 5245, 370, 665, 30, 1545, 775, 1518, 764, 552, 30, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.13545330832986271, "compression_ratio": 1.5317919075144508, "no_speech_prob": 2.769108959910227e-06}, {"id": 261, "seek": 183600, "start": 1851.0, "end": 1858.0, "text": " Okay, so why are these models so good? Why does everyone use them?", "tokens": [50364, 407, 428, 4846, 586, 307, 257, 28811, 295, 264, 1349, 8062, 293, 512, 733, 295, 2535, 304, 8062, 13, 50714, 50714, 400, 309, 311, 588, 2199, 11, 457, 309, 2709, 264, 2316, 264, 733, 295, 264, 1668, 295, 1589, 309, 2203, 293, 309, 1985, 869, 13, 51114, 51114, 1033, 11, 370, 983, 366, 613, 5245, 370, 665, 30, 1545, 775, 1518, 764, 552, 30, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.13545330832986271, "compression_ratio": 1.5317919075144508, "no_speech_prob": 2.769108959910227e-06}, {"id": 262, "seek": 185800, "start": 1858.0, "end": 1866.0, "text": " I think the really powerful thing about the model is that it kind of gives you direct connections between every pair of words.", "tokens": [50364, 286, 519, 264, 534, 4005, 551, 466, 264, 2316, 307, 300, 309, 733, 295, 2709, 291, 2047, 9271, 1296, 633, 6119, 295, 2283, 13, 50764, 50764, 407, 1184, 1349, 393, 3838, 2105, 264, 7633, 1785, 295, 633, 3894, 1349, 13, 51264, 51264, 663, 311, 257, 8712, 13, 440, 45216, 304, 2316, 727, 1310, 483, 281, 264, 1785, 295, 439, 264, 2283, 294, 341, 32264, 2519, 11, 457, 1825, 3052, 646, 294, 565, 813, 300, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10186236417746242, "compression_ratio": 1.63013698630137, "no_speech_prob": 1.2803725439880509e-05}, {"id": 263, "seek": 185800, "start": 1866.0, "end": 1876.0, "text": " So each word can directly access the hidden state of every previous word.", "tokens": [50364, 286, 519, 264, 534, 4005, 551, 466, 264, 2316, 307, 300, 309, 733, 295, 2709, 291, 2047, 9271, 1296, 633, 6119, 295, 2283, 13, 50764, 50764, 407, 1184, 1349, 393, 3838, 2105, 264, 7633, 1785, 295, 633, 3894, 1349, 13, 51264, 51264, 663, 311, 257, 8712, 13, 440, 45216, 304, 2316, 727, 1310, 483, 281, 264, 1785, 295, 439, 264, 2283, 294, 341, 32264, 2519, 11, 457, 1825, 3052, 646, 294, 565, 813, 300, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10186236417746242, "compression_ratio": 1.63013698630137, "no_speech_prob": 1.2803725439880509e-05}, {"id": 264, "seek": 185800, "start": 1876.0, "end": 1884.0, "text": " That's a contrast. The convolutional model could maybe get to the state of all the words in this receptor field, but nothing further back in time than that.", "tokens": [50364, 286, 519, 264, 534, 4005, 551, 466, 264, 2316, 307, 300, 309, 733, 295, 2709, 291, 2047, 9271, 1296, 633, 6119, 295, 2283, 13, 50764, 50764, 407, 1184, 1349, 393, 3838, 2105, 264, 7633, 1785, 295, 633, 3894, 1349, 13, 51264, 51264, 663, 311, 257, 8712, 13, 440, 45216, 304, 2316, 727, 1310, 483, 281, 264, 1785, 295, 439, 264, 2283, 294, 341, 32264, 2519, 11, 457, 1825, 3052, 646, 294, 565, 813, 300, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10186236417746242, "compression_ratio": 1.63013698630137, "no_speech_prob": 1.2803725439880509e-05}, {"id": 265, "seek": 188400, "start": 1884.0, "end": 1893.0, "text": " And the recurrent model, the state has to go through this bottleneck at each time step.", "tokens": [50364, 400, 264, 18680, 1753, 2316, 11, 264, 1785, 575, 281, 352, 807, 341, 44641, 547, 412, 1184, 565, 1823, 13, 50814, 50814, 509, 393, 767, 3838, 2105, 264, 3894, 2283, 4399, 264, 3736, 472, 3894, 1349, 13, 51114, 51114, 11998, 3052, 294, 264, 1791, 813, 300, 632, 281, 483, 6063, 30353, 293, 291, 393, 2731, 1589, 322, 341, 13, 51364, 51364, 1171, 2698, 12, 1591, 1251, 11, 291, 393, 294, 8665, 829, 2319, 4, 3202, 322, 604, 1349, 294, 264, 17275, 1791, 293, 536, 2293, 437, 390, 456, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1943735102171539, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.397174497076776e-06}, {"id": 266, "seek": 188400, "start": 1893.0, "end": 1899.0, "text": " You can actually directly access the previous words beyond the literally one previous word.", "tokens": [50364, 400, 264, 18680, 1753, 2316, 11, 264, 1785, 575, 281, 352, 807, 341, 44641, 547, 412, 1184, 565, 1823, 13, 50814, 50814, 509, 393, 767, 3838, 2105, 264, 3894, 2283, 4399, 264, 3736, 472, 3894, 1349, 13, 51114, 51114, 11998, 3052, 294, 264, 1791, 813, 300, 632, 281, 483, 6063, 30353, 293, 291, 393, 2731, 1589, 322, 341, 13, 51364, 51364, 1171, 2698, 12, 1591, 1251, 11, 291, 393, 294, 8665, 829, 2319, 4, 3202, 322, 604, 1349, 294, 264, 17275, 1791, 293, 536, 2293, 437, 390, 456, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1943735102171539, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.397174497076776e-06}, {"id": 267, "seek": 188400, "start": 1899.0, "end": 1904.0, "text": " Anything further in the past than that had to get somehow compressed and you can lost information on this.", "tokens": [50364, 400, 264, 18680, 1753, 2316, 11, 264, 1785, 575, 281, 352, 807, 341, 44641, 547, 412, 1184, 565, 1823, 13, 50814, 50814, 509, 393, 767, 3838, 2105, 264, 3894, 2283, 4399, 264, 3736, 472, 3894, 1349, 13, 51114, 51114, 11998, 3052, 294, 264, 1791, 813, 300, 632, 281, 483, 6063, 30353, 293, 291, 393, 2731, 1589, 322, 341, 13, 51364, 51364, 1171, 2698, 12, 1591, 1251, 11, 291, 393, 294, 8665, 829, 2319, 4, 3202, 322, 604, 1349, 294, 264, 17275, 1791, 293, 536, 2293, 437, 390, 456, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1943735102171539, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.397174497076776e-06}, {"id": 268, "seek": 188400, "start": 1904.0, "end": 1912.0, "text": " For self-attention, you can in principle put 100% attention on any word in the distant past and see exactly what was there.", "tokens": [50364, 400, 264, 18680, 1753, 2316, 11, 264, 1785, 575, 281, 352, 807, 341, 44641, 547, 412, 1184, 565, 1823, 13, 50814, 50814, 509, 393, 767, 3838, 2105, 264, 3894, 2283, 4399, 264, 3736, 472, 3894, 1349, 13, 51114, 51114, 11998, 3052, 294, 264, 1791, 813, 300, 632, 281, 483, 6063, 30353, 293, 291, 393, 2731, 1589, 322, 341, 13, 51364, 51364, 1171, 2698, 12, 1591, 1251, 11, 291, 393, 294, 8665, 829, 2319, 4, 3202, 322, 604, 1349, 294, 264, 17275, 1791, 293, 536, 2293, 437, 390, 456, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1943735102171539, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.397174497076776e-06}, {"id": 269, "seek": 191200, "start": 1912.0, "end": 1924.0, "text": " And this just makes it a really powerful model. It avoids things like issues like vanishing gradients quite effectively.", "tokens": [50364, 400, 341, 445, 1669, 309, 257, 534, 4005, 2316, 13, 467, 3641, 3742, 721, 411, 2663, 411, 3161, 3807, 2771, 2448, 1596, 8659, 13, 50964, 50964, 400, 1355, 309, 393, 445, 1466, 588, 40189, 6828, 588, 3612, 13, 51214, 51214, 440, 661, 869, 551, 466, 341, 307, 577, 8952, 22395, 309, 307, 13, 51414, 51414, 407, 322, 264, 472, 1011, 11, 341, 2316, 307, 884, 1596, 257, 688, 295, 24903, 294, 300, 309, 311, 884, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12667456968331042, "compression_ratio": 1.5934579439252337, "no_speech_prob": 1.7230411685886793e-05}, {"id": 270, "seek": 191200, "start": 1924.0, "end": 1929.0, "text": " And means it can just learn very expressive functions very easily.", "tokens": [50364, 400, 341, 445, 1669, 309, 257, 534, 4005, 2316, 13, 467, 3641, 3742, 721, 411, 2663, 411, 3161, 3807, 2771, 2448, 1596, 8659, 13, 50964, 50964, 400, 1355, 309, 393, 445, 1466, 588, 40189, 6828, 588, 3612, 13, 51214, 51214, 440, 661, 869, 551, 466, 341, 307, 577, 8952, 22395, 309, 307, 13, 51414, 51414, 407, 322, 264, 472, 1011, 11, 341, 2316, 307, 884, 1596, 257, 688, 295, 24903, 294, 300, 309, 311, 884, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12667456968331042, "compression_ratio": 1.5934579439252337, "no_speech_prob": 1.7230411685886793e-05}, {"id": 271, "seek": 191200, "start": 1929.0, "end": 1933.0, "text": " The other great thing about this is how parallelizable it is.", "tokens": [50364, 400, 341, 445, 1669, 309, 257, 534, 4005, 2316, 13, 467, 3641, 3742, 721, 411, 2663, 411, 3161, 3807, 2771, 2448, 1596, 8659, 13, 50964, 50964, 400, 1355, 309, 393, 445, 1466, 588, 40189, 6828, 588, 3612, 13, 51214, 51214, 440, 661, 869, 551, 466, 341, 307, 577, 8952, 22395, 309, 307, 13, 51414, 51414, 407, 322, 264, 472, 1011, 11, 341, 2316, 307, 884, 1596, 257, 688, 295, 24903, 294, 300, 309, 311, 884, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12667456968331042, "compression_ratio": 1.5934579439252337, "no_speech_prob": 1.7230411685886793e-05}, {"id": 272, "seek": 191200, "start": 1933.0, "end": 1940.0, "text": " So on the one hand, this model is doing quite a lot of computation in that it's doing this.", "tokens": [50364, 400, 341, 445, 1669, 309, 257, 534, 4005, 2316, 13, 467, 3641, 3742, 721, 411, 2663, 411, 3161, 3807, 2771, 2448, 1596, 8659, 13, 50964, 50964, 400, 1355, 309, 393, 445, 1466, 588, 40189, 6828, 588, 3612, 13, 51214, 51214, 440, 661, 869, 551, 466, 341, 307, 577, 8952, 22395, 309, 307, 13, 51414, 51414, 407, 322, 264, 472, 1011, 11, 341, 2316, 307, 884, 1596, 257, 688, 295, 24903, 294, 300, 309, 311, 884, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12667456968331042, "compression_ratio": 1.5934579439252337, "no_speech_prob": 1.7230411685886793e-05}, {"id": 273, "seek": 194000, "start": 1940.0, "end": 1946.0, "text": " So the self-attention operation is quadratic, basically, because every word can look at every other word.", "tokens": [50364, 407, 264, 2698, 12, 1591, 1251, 6916, 307, 37262, 11, 1936, 11, 570, 633, 1349, 393, 574, 412, 633, 661, 1349, 13, 50664, 50664, 663, 3263, 1596, 2964, 11, 457, 264, 534, 1481, 551, 307, 291, 393, 360, 309, 294, 8952, 13, 50864, 50864, 1436, 439, 613, 7705, 366, 6695, 281, 1184, 661, 11, 291, 445, 360, 309, 382, 472, 955, 8141, 27290, 13, 51164, 51164, 2754, 1673, 2171, 291, 1391, 360, 733, 295, 544, 909, 12, 76, 723, 647, 356, 7705, 813, 291, 576, 360, 365, 264, 10344, 45702, 45, 11, 51564, 51564, 291, 393, 360, 439, 613, 7705, 709, 4663, 570, 291, 360, 552, 439, 412, 1564, 2831, 5123, 3137, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10629330333481488, "compression_ratio": 1.7673611111111112, "no_speech_prob": 4.3305895815137774e-05}, {"id": 274, "seek": 194000, "start": 1946.0, "end": 1950.0, "text": " That sounds quite slow, but the really nice thing is you can do it in parallel.", "tokens": [50364, 407, 264, 2698, 12, 1591, 1251, 6916, 307, 37262, 11, 1936, 11, 570, 633, 1349, 393, 574, 412, 633, 661, 1349, 13, 50664, 50664, 663, 3263, 1596, 2964, 11, 457, 264, 534, 1481, 551, 307, 291, 393, 360, 309, 294, 8952, 13, 50864, 50864, 1436, 439, 613, 7705, 366, 6695, 281, 1184, 661, 11, 291, 445, 360, 309, 382, 472, 955, 8141, 27290, 13, 51164, 51164, 2754, 1673, 2171, 291, 1391, 360, 733, 295, 544, 909, 12, 76, 723, 647, 356, 7705, 813, 291, 576, 360, 365, 264, 10344, 45702, 45, 11, 51564, 51564, 291, 393, 360, 439, 613, 7705, 709, 4663, 570, 291, 360, 552, 439, 412, 1564, 2831, 5123, 3137, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10629330333481488, "compression_ratio": 1.7673611111111112, "no_speech_prob": 4.3305895815137774e-05}, {"id": 275, "seek": 194000, "start": 1950.0, "end": 1956.0, "text": " Because all these operations are independent to each other, you just do it as one big matrix multiplication.", "tokens": [50364, 407, 264, 2698, 12, 1591, 1251, 6916, 307, 37262, 11, 1936, 11, 570, 633, 1349, 393, 574, 412, 633, 661, 1349, 13, 50664, 50664, 663, 3263, 1596, 2964, 11, 457, 264, 534, 1481, 551, 307, 291, 393, 360, 309, 294, 8952, 13, 50864, 50864, 1436, 439, 613, 7705, 366, 6695, 281, 1184, 661, 11, 291, 445, 360, 309, 382, 472, 955, 8141, 27290, 13, 51164, 51164, 2754, 1673, 2171, 291, 1391, 360, 733, 295, 544, 909, 12, 76, 723, 647, 356, 7705, 813, 291, 576, 360, 365, 264, 10344, 45702, 45, 11, 51564, 51564, 291, 393, 360, 439, 613, 7705, 709, 4663, 570, 291, 360, 552, 439, 412, 1564, 2831, 5123, 3137, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10629330333481488, "compression_ratio": 1.7673611111111112, "no_speech_prob": 4.3305895815137774e-05}, {"id": 276, "seek": 194000, "start": 1956.0, "end": 1964.0, "text": " Even though sometimes you probably do kind of more add-multiply operations than you would do with the equivalent RNN,", "tokens": [50364, 407, 264, 2698, 12, 1591, 1251, 6916, 307, 37262, 11, 1936, 11, 570, 633, 1349, 393, 574, 412, 633, 661, 1349, 13, 50664, 50664, 663, 3263, 1596, 2964, 11, 457, 264, 534, 1481, 551, 307, 291, 393, 360, 309, 294, 8952, 13, 50864, 50864, 1436, 439, 613, 7705, 366, 6695, 281, 1184, 661, 11, 291, 445, 360, 309, 382, 472, 955, 8141, 27290, 13, 51164, 51164, 2754, 1673, 2171, 291, 1391, 360, 733, 295, 544, 909, 12, 76, 723, 647, 356, 7705, 813, 291, 576, 360, 365, 264, 10344, 45702, 45, 11, 51564, 51564, 291, 393, 360, 439, 613, 7705, 709, 4663, 570, 291, 360, 552, 439, 412, 1564, 2831, 5123, 3137, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10629330333481488, "compression_ratio": 1.7673611111111112, "no_speech_prob": 4.3305895815137774e-05}, {"id": 277, "seek": 194000, "start": 1964.0, "end": 1969.0, "text": " you can do all these operations much faster because you do them all at once rather sequentially.", "tokens": [50364, 407, 264, 2698, 12, 1591, 1251, 6916, 307, 37262, 11, 1936, 11, 570, 633, 1349, 393, 574, 412, 633, 661, 1349, 13, 50664, 50664, 663, 3263, 1596, 2964, 11, 457, 264, 534, 1481, 551, 307, 291, 393, 360, 309, 294, 8952, 13, 50864, 50864, 1436, 439, 613, 7705, 366, 6695, 281, 1184, 661, 11, 291, 445, 360, 309, 382, 472, 955, 8141, 27290, 13, 51164, 51164, 2754, 1673, 2171, 291, 1391, 360, 733, 295, 544, 909, 12, 76, 723, 647, 356, 7705, 813, 291, 576, 360, 365, 264, 10344, 45702, 45, 11, 51564, 51564, 291, 393, 360, 439, 613, 7705, 709, 4663, 570, 291, 360, 552, 439, 412, 1564, 2831, 5123, 3137, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10629330333481488, "compression_ratio": 1.7673611111111112, "no_speech_prob": 4.3305895815137774e-05}, {"id": 278, "seek": 196900, "start": 1969.0, "end": 1975.0, "text": " So this is a really good trade-off.", "tokens": [50364, 407, 341, 307, 257, 534, 665, 4923, 12, 4506, 13, 50664, 50664, 286, 611, 528, 281, 2661, 751, 466, 512, 661, 721, 13, 50764, 50764, 407, 286, 600, 7619, 1507, 411, 4825, 12, 28409, 3202, 293, 4497, 1578, 14519, 293, 1507, 13, 51114, 51114, 407, 286, 600, 658, 439, 264, 3202, 562, 27938, 433, 390, 700, 4736, 13, 51264, 51264, 583, 27938, 433, 611, 1361, 2051, 365, 257, 1379, 3411, 295, 661, 11733, 382, 731, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.18776782751083373, "compression_ratio": 1.5603864734299517, "no_speech_prob": 4.3313535570632666e-05}, {"id": 279, "seek": 196900, "start": 1975.0, "end": 1977.0, "text": " I also want to quickly talk about some other things.", "tokens": [50364, 407, 341, 307, 257, 534, 665, 4923, 12, 4506, 13, 50664, 50664, 286, 611, 528, 281, 2661, 751, 466, 512, 661, 721, 13, 50764, 50764, 407, 286, 600, 7619, 1507, 411, 4825, 12, 28409, 3202, 293, 4497, 1578, 14519, 293, 1507, 13, 51114, 51114, 407, 286, 600, 658, 439, 264, 3202, 562, 27938, 433, 390, 700, 4736, 13, 51264, 51264, 583, 27938, 433, 611, 1361, 2051, 365, 257, 1379, 3411, 295, 661, 11733, 382, 731, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.18776782751083373, "compression_ratio": 1.5603864734299517, "no_speech_prob": 4.3313535570632666e-05}, {"id": 280, "seek": 196900, "start": 1977.0, "end": 1984.0, "text": " So I've described stuff like multi-headed attention and additional bad instances and stuff.", "tokens": [50364, 407, 341, 307, 257, 534, 665, 4923, 12, 4506, 13, 50664, 50664, 286, 611, 528, 281, 2661, 751, 466, 512, 661, 721, 13, 50764, 50764, 407, 286, 600, 7619, 1507, 411, 4825, 12, 28409, 3202, 293, 4497, 1578, 14519, 293, 1507, 13, 51114, 51114, 407, 286, 600, 658, 439, 264, 3202, 562, 27938, 433, 390, 700, 4736, 13, 51264, 51264, 583, 27938, 433, 611, 1361, 2051, 365, 257, 1379, 3411, 295, 661, 11733, 382, 731, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.18776782751083373, "compression_ratio": 1.5603864734299517, "no_speech_prob": 4.3313535570632666e-05}, {"id": 281, "seek": 196900, "start": 1984.0, "end": 1987.0, "text": " So I've got all the attention when Transformers was first released.", "tokens": [50364, 407, 341, 307, 257, 534, 665, 4923, 12, 4506, 13, 50664, 50664, 286, 611, 528, 281, 2661, 751, 466, 512, 661, 721, 13, 50764, 50764, 407, 286, 600, 7619, 1507, 411, 4825, 12, 28409, 3202, 293, 4497, 1578, 14519, 293, 1507, 13, 51114, 51114, 407, 286, 600, 658, 439, 264, 3202, 562, 27938, 433, 390, 700, 4736, 13, 51264, 51264, 583, 27938, 433, 611, 1361, 2051, 365, 257, 1379, 3411, 295, 661, 11733, 382, 731, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.18776782751083373, "compression_ratio": 1.5603864734299517, "no_speech_prob": 4.3313535570632666e-05}, {"id": 282, "seek": 196900, "start": 1987.0, "end": 1991.0, "text": " But Transformers also came along with a whole bag of other tricks as well.", "tokens": [50364, 407, 341, 307, 257, 534, 665, 4923, 12, 4506, 13, 50664, 50664, 286, 611, 528, 281, 2661, 751, 466, 512, 661, 721, 13, 50764, 50764, 407, 286, 600, 7619, 1507, 411, 4825, 12, 28409, 3202, 293, 4497, 1578, 14519, 293, 1507, 13, 51114, 51114, 407, 286, 600, 658, 439, 264, 3202, 562, 27938, 433, 390, 700, 4736, 13, 51264, 51264, 583, 27938, 433, 611, 1361, 2051, 365, 257, 1379, 3411, 295, 661, 11733, 382, 731, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.18776782751083373, "compression_ratio": 1.5603864734299517, "no_speech_prob": 4.3313535570632666e-05}, {"id": 283, "seek": 199100, "start": 1991.0, "end": 1999.0, "text": " And these tricks were all actually really important to make this stuff work.", "tokens": [50364, 400, 613, 11733, 645, 439, 767, 534, 1021, 281, 652, 341, 1507, 589, 13, 50764, 50764, 400, 294, 512, 2020, 11, 341, 3035, 534, 733, 295, 4363, 1602, 426, 45196, 11, 286, 519, 13, 50964, 50964, 407, 337, 1365, 11, 286, 2835, 257, 857, 466, 341, 4420, 2539, 2710, 2144, 949, 13, 51114, 51114, 440, 2539, 2710, 2144, 307, 534, 4961, 13, 51414, 51414, 814, 611, 1409, 884, 613, 721, 411, 613, 2539, 3314, 28078, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.18853214979171753, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.0288356861565262e-05}, {"id": 284, "seek": 199100, "start": 1999.0, "end": 2003.0, "text": " And in some sense, this paper really kind of modernized NLP, I think.", "tokens": [50364, 400, 613, 11733, 645, 439, 767, 534, 1021, 281, 652, 341, 1507, 589, 13, 50764, 50764, 400, 294, 512, 2020, 11, 341, 3035, 534, 733, 295, 4363, 1602, 426, 45196, 11, 286, 519, 13, 50964, 50964, 407, 337, 1365, 11, 286, 2835, 257, 857, 466, 341, 4420, 2539, 2710, 2144, 949, 13, 51114, 51114, 440, 2539, 2710, 2144, 307, 534, 4961, 13, 51414, 51414, 814, 611, 1409, 884, 613, 721, 411, 613, 2539, 3314, 28078, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.18853214979171753, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.0288356861565262e-05}, {"id": 285, "seek": 199100, "start": 2003.0, "end": 2006.0, "text": " So for example, I mentioned a bit about this useful learning normalization before.", "tokens": [50364, 400, 613, 11733, 645, 439, 767, 534, 1021, 281, 652, 341, 1507, 589, 13, 50764, 50764, 400, 294, 512, 2020, 11, 341, 3035, 534, 733, 295, 4363, 1602, 426, 45196, 11, 286, 519, 13, 50964, 50964, 407, 337, 1365, 11, 286, 2835, 257, 857, 466, 341, 4420, 2539, 2710, 2144, 949, 13, 51114, 51114, 440, 2539, 2710, 2144, 307, 534, 4961, 13, 51414, 51414, 814, 611, 1409, 884, 613, 721, 411, 613, 2539, 3314, 28078, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.18853214979171753, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.0288356861565262e-05}, {"id": 286, "seek": 199100, "start": 2006.0, "end": 2012.0, "text": " The learning normalization is really helpful.", "tokens": [50364, 400, 613, 11733, 645, 439, 767, 534, 1021, 281, 652, 341, 1507, 589, 13, 50764, 50764, 400, 294, 512, 2020, 11, 341, 3035, 534, 733, 295, 4363, 1602, 426, 45196, 11, 286, 519, 13, 50964, 50964, 407, 337, 1365, 11, 286, 2835, 257, 857, 466, 341, 4420, 2539, 2710, 2144, 949, 13, 51114, 51114, 440, 2539, 2710, 2144, 307, 534, 4961, 13, 51414, 51414, 814, 611, 1409, 884, 613, 721, 411, 613, 2539, 3314, 28078, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.18853214979171753, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.0288356861565262e-05}, {"id": 287, "seek": 199100, "start": 2012.0, "end": 2017.0, "text": " They also started doing these things like these learning rate schedules.", "tokens": [50364, 400, 613, 11733, 645, 439, 767, 534, 1021, 281, 652, 341, 1507, 589, 13, 50764, 50764, 400, 294, 512, 2020, 11, 341, 3035, 534, 733, 295, 4363, 1602, 426, 45196, 11, 286, 519, 13, 50964, 50964, 407, 337, 1365, 11, 286, 2835, 257, 857, 466, 341, 4420, 2539, 2710, 2144, 949, 13, 51114, 51114, 440, 2539, 2710, 2144, 307, 534, 4961, 13, 51414, 51414, 814, 611, 1409, 884, 613, 721, 411, 613, 2539, 3314, 28078, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.18853214979171753, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.0288356861565262e-05}, {"id": 288, "seek": 201700, "start": 2017.0, "end": 2024.0, "text": " For whatever reason, to make Transformers work well, you have to sort of linearly warm up your learning rate", "tokens": [50364, 1171, 2035, 1778, 11, 281, 652, 27938, 433, 589, 731, 11, 291, 362, 281, 1333, 295, 43586, 4561, 493, 428, 2539, 3314, 50714, 50714, 490, 4018, 281, 428, 3387, 2539, 3314, 670, 2940, 4714, 4439, 13, 51114, 51114, 400, 561, 360, 4561, 493, 2539, 6846, 294, 661, 6257, 11, 457, 27938, 433, 534, 11, 534, 643, 341, 281, 589, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12488070875406265, "compression_ratio": 1.6432748538011697, "no_speech_prob": 1.0952064258162864e-05}, {"id": 289, "seek": 201700, "start": 2024.0, "end": 2032.0, "text": " from zero to your goal learning rate over several thousand steps.", "tokens": [50364, 1171, 2035, 1778, 11, 281, 652, 27938, 433, 589, 731, 11, 291, 362, 281, 1333, 295, 43586, 4561, 493, 428, 2539, 3314, 50714, 50714, 490, 4018, 281, 428, 3387, 2539, 3314, 670, 2940, 4714, 4439, 13, 51114, 51114, 400, 561, 360, 4561, 493, 2539, 6846, 294, 661, 6257, 11, 457, 27938, 433, 534, 11, 534, 643, 341, 281, 589, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12488070875406265, "compression_ratio": 1.6432748538011697, "no_speech_prob": 1.0952064258162864e-05}, {"id": 290, "seek": 201700, "start": 2032.0, "end": 2043.0, "text": " And people do warm up learning rates in other settings, but Transformers really, really need this to work.", "tokens": [50364, 1171, 2035, 1778, 11, 281, 652, 27938, 433, 589, 731, 11, 291, 362, 281, 1333, 295, 43586, 4561, 493, 428, 2539, 3314, 50714, 50714, 490, 4018, 281, 428, 3387, 2539, 3314, 670, 2940, 4714, 4439, 13, 51114, 51114, 400, 561, 360, 4561, 493, 2539, 6846, 294, 661, 6257, 11, 457, 27938, 433, 534, 11, 534, 643, 341, 281, 589, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12488070875406265, "compression_ratio": 1.6432748538011697, "no_speech_prob": 1.0952064258162864e-05}, {"id": 291, "seek": 204300, "start": 2043.0, "end": 2047.0, "text": " Also, things like the initialization actually really do matter with these.", "tokens": [50364, 2743, 11, 721, 411, 264, 5883, 2144, 767, 534, 360, 1871, 365, 613, 13, 50564, 50564, 400, 512, 5883, 14455, 500, 380, 589, 13, 50714, 50714, 400, 436, 3507, 294, 613, 661, 11733, 411, 257, 7645, 899, 6259, 571, 295, 264, 5598, 11, 597, 11, 797, 11, 2067, 380, 14479, 294, 341, 3035, 11, 51114, 51114, 457, 4523, 484, 281, 312, 1596, 4961, 337, 264, 5633, 12, 4092, 3479, 12853, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.19763811747233073, "compression_ratio": 1.5916230366492146, "no_speech_prob": 1.7495283827884123e-05}, {"id": 292, "seek": 204300, "start": 2047.0, "end": 2050.0, "text": " And some initializations don't work.", "tokens": [50364, 2743, 11, 721, 411, 264, 5883, 2144, 767, 534, 360, 1871, 365, 613, 13, 50564, 50564, 400, 512, 5883, 14455, 500, 380, 589, 13, 50714, 50714, 400, 436, 3507, 294, 613, 661, 11733, 411, 257, 7645, 899, 6259, 571, 295, 264, 5598, 11, 597, 11, 797, 11, 2067, 380, 14479, 294, 341, 3035, 11, 51114, 51114, 457, 4523, 484, 281, 312, 1596, 4961, 337, 264, 5633, 12, 4092, 3479, 12853, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.19763811747233073, "compression_ratio": 1.5916230366492146, "no_speech_prob": 1.7495283827884123e-05}, {"id": 293, "seek": 204300, "start": 2050.0, "end": 2058.0, "text": " And they throw in these other tricks like a label smoothing of the output, which, again, wasn't invented in this paper,", "tokens": [50364, 2743, 11, 721, 411, 264, 5883, 2144, 767, 534, 360, 1871, 365, 613, 13, 50564, 50564, 400, 512, 5883, 14455, 500, 380, 589, 13, 50714, 50714, 400, 436, 3507, 294, 613, 661, 11733, 411, 257, 7645, 899, 6259, 571, 295, 264, 5598, 11, 597, 11, 797, 11, 2067, 380, 14479, 294, 341, 3035, 11, 51114, 51114, 457, 4523, 484, 281, 312, 1596, 4961, 337, 264, 5633, 12, 4092, 3479, 12853, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.19763811747233073, "compression_ratio": 1.5916230366492146, "no_speech_prob": 1.7495283827884123e-05}, {"id": 294, "seek": 204300, "start": 2058.0, "end": 2064.0, "text": " but turns out to be quite helpful for the task-like machine translation.", "tokens": [50364, 2743, 11, 721, 411, 264, 5883, 2144, 767, 534, 360, 1871, 365, 613, 13, 50564, 50564, 400, 512, 5883, 14455, 500, 380, 589, 13, 50714, 50714, 400, 436, 3507, 294, 613, 661, 11733, 411, 257, 7645, 899, 6259, 571, 295, 264, 5598, 11, 597, 11, 797, 11, 2067, 380, 14479, 294, 341, 3035, 11, 51114, 51114, 457, 4523, 484, 281, 312, 1596, 4961, 337, 264, 5633, 12, 4092, 3479, 12853, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.19763811747233073, "compression_ratio": 1.5916230366492146, "no_speech_prob": 1.7495283827884123e-05}, {"id": 295, "seek": 206400, "start": 2064.0, "end": 2074.0, "text": " So to give you some idea of how well these things are working, these models I've described so far,", "tokens": [50364, 407, 281, 976, 291, 512, 1558, 295, 577, 731, 613, 721, 366, 1364, 11, 613, 5245, 286, 600, 7619, 370, 1400, 11, 50864, 50864, 510, 366, 512, 3542, 322, 257, 2856, 15983, 18927, 13, 51114, 51114, 407, 264, 1230, 322, 264, 558, 307, 437, 311, 1219, 14024, 11, 597, 307, 257, 3481, 295, 264, 22119, 295, 5167, 484, 1412, 13, 51514, 51514, 400, 510, 11, 3126, 307, 1101, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12263027609211125, "compression_ratio": 1.482233502538071, "no_speech_prob": 3.533198469085619e-05}, {"id": 296, "seek": 206400, "start": 2074.0, "end": 2079.0, "text": " here are some results on a language modeling benchmark.", "tokens": [50364, 407, 281, 976, 291, 512, 1558, 295, 577, 731, 613, 721, 366, 1364, 11, 613, 5245, 286, 600, 7619, 370, 1400, 11, 50864, 50864, 510, 366, 512, 3542, 322, 257, 2856, 15983, 18927, 13, 51114, 51114, 407, 264, 1230, 322, 264, 558, 307, 437, 311, 1219, 14024, 11, 597, 307, 257, 3481, 295, 264, 22119, 295, 5167, 484, 1412, 13, 51514, 51514, 400, 510, 11, 3126, 307, 1101, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12263027609211125, "compression_ratio": 1.482233502538071, "no_speech_prob": 3.533198469085619e-05}, {"id": 297, "seek": 206400, "start": 2079.0, "end": 2087.0, "text": " So the number on the right is what's called complexity, which is a measure of the likelihood of held out data.", "tokens": [50364, 407, 281, 976, 291, 512, 1558, 295, 577, 731, 613, 721, 366, 1364, 11, 613, 5245, 286, 600, 7619, 370, 1400, 11, 50864, 50864, 510, 366, 512, 3542, 322, 257, 2856, 15983, 18927, 13, 51114, 51114, 407, 264, 1230, 322, 264, 558, 307, 437, 311, 1219, 14024, 11, 597, 307, 257, 3481, 295, 264, 22119, 295, 5167, 484, 1412, 13, 51514, 51514, 400, 510, 11, 3126, 307, 1101, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12263027609211125, "compression_ratio": 1.482233502538071, "no_speech_prob": 3.533198469085619e-05}, {"id": 298, "seek": 206400, "start": 2087.0, "end": 2090.0, "text": " And here, lower is better.", "tokens": [50364, 407, 281, 976, 291, 512, 1558, 295, 577, 731, 613, 721, 366, 1364, 11, 613, 5245, 286, 600, 7619, 370, 1400, 11, 50864, 50864, 510, 366, 512, 3542, 322, 257, 2856, 15983, 18927, 13, 51114, 51114, 407, 264, 1230, 322, 264, 558, 307, 437, 311, 1219, 14024, 11, 597, 307, 257, 3481, 295, 264, 22119, 295, 5167, 484, 1412, 13, 51514, 51514, 400, 510, 11, 3126, 307, 1101, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12263027609211125, "compression_ratio": 1.482233502538071, "no_speech_prob": 3.533198469085619e-05}, {"id": 299, "seek": 209000, "start": 2090.0, "end": 2099.0, "text": " So you see that NLSTM for 2016 gets complexity of 48.", "tokens": [50364, 407, 291, 536, 300, 426, 43, 6840, 44, 337, 6549, 2170, 14024, 295, 11174, 13, 50814, 50814, 509, 536, 300, 398, 474, 1663, 10704, 45216, 304, 5245, 337, 6549, 884, 1596, 257, 857, 1101, 412, 466, 13435, 13, 51264, 51264, 2743, 11, 561, 3737, 926, 365, 257, 1379, 3840, 295, 45702, 45, 21669, 13, 51464, 51464, 821, 1143, 281, 312, 18431, 295, 10577, 322, 577, 291, 652, 17840, 294, 426, 43, 6840, 26386, 11, 293, 512, 295, 613, 483, 281, 2507, 2217, 82, 382, 731, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.2634202321370443, "compression_ratio": 1.4533333333333334, "no_speech_prob": 2.5068024115171283e-05}, {"id": 300, "seek": 209000, "start": 2099.0, "end": 2108.0, "text": " You see that Yandera finds convolutional models for 2016 doing quite a bit better at about 37.", "tokens": [50364, 407, 291, 536, 300, 426, 43, 6840, 44, 337, 6549, 2170, 14024, 295, 11174, 13, 50814, 50814, 509, 536, 300, 398, 474, 1663, 10704, 45216, 304, 5245, 337, 6549, 884, 1596, 257, 857, 1101, 412, 466, 13435, 13, 51264, 51264, 2743, 11, 561, 3737, 926, 365, 257, 1379, 3840, 295, 45702, 45, 21669, 13, 51464, 51464, 821, 1143, 281, 312, 18431, 295, 10577, 322, 577, 291, 652, 17840, 294, 426, 43, 6840, 26386, 11, 293, 512, 295, 613, 483, 281, 2507, 2217, 82, 382, 731, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.2634202321370443, "compression_ratio": 1.4533333333333334, "no_speech_prob": 2.5068024115171283e-05}, {"id": 301, "seek": 209000, "start": 2108.0, "end": 2112.0, "text": " Also, people played around with a whole bunch of RNN variants.", "tokens": [50364, 407, 291, 536, 300, 426, 43, 6840, 44, 337, 6549, 2170, 14024, 295, 11174, 13, 50814, 50814, 509, 536, 300, 398, 474, 1663, 10704, 45216, 304, 5245, 337, 6549, 884, 1596, 257, 857, 1101, 412, 466, 13435, 13, 51264, 51264, 2743, 11, 561, 3737, 926, 365, 257, 1379, 3840, 295, 45702, 45, 21669, 13, 51464, 51464, 821, 1143, 281, 312, 18431, 295, 10577, 322, 577, 291, 652, 17840, 294, 426, 43, 6840, 26386, 11, 293, 512, 295, 613, 483, 281, 2507, 2217, 82, 382, 731, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.2634202321370443, "compression_ratio": 1.4533333333333334, "no_speech_prob": 2.5068024115171283e-05}, {"id": 302, "seek": 209000, "start": 2112.0, "end": 2119.0, "text": " There used to be dozens of papers on how you make variations in NLSTMs, and some of these get to below 30s as well.", "tokens": [50364, 407, 291, 536, 300, 426, 43, 6840, 44, 337, 6549, 2170, 14024, 295, 11174, 13, 50814, 50814, 509, 536, 300, 398, 474, 1663, 10704, 45216, 304, 5245, 337, 6549, 884, 1596, 257, 857, 1101, 412, 466, 13435, 13, 51264, 51264, 2743, 11, 561, 3737, 926, 365, 257, 1379, 3840, 295, 45702, 45, 21669, 13, 51464, 51464, 821, 1143, 281, 312, 18431, 295, 10577, 322, 577, 291, 652, 17840, 294, 426, 43, 6840, 26386, 11, 293, 512, 295, 613, 483, 281, 2507, 2217, 82, 382, 731, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.2634202321370443, "compression_ratio": 1.4533333333333334, "no_speech_prob": 2.5068024115171283e-05}, {"id": 303, "seek": 211900, "start": 2119.0, "end": 2126.0, "text": " But then when you introduce transformers, you get a really big jump down to go to 18 and 20.", "tokens": [50364, 583, 550, 562, 291, 5366, 4088, 433, 11, 291, 483, 257, 534, 955, 3012, 760, 281, 352, 281, 2443, 293, 945, 13, 50714, 50714, 400, 294, 2115, 295, 2856, 15983, 11, 300, 311, 257, 534, 11322, 3012, 294, 3389, 13, 51164, 51164, 400, 286, 820, 584, 613, 16823, 321, 1866, 645, 4098, 2416, 322, 733, 295, 938, 4319, 2856, 15983, 13, 51514, 51514, 407, 2856, 15983, 689, 485, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1759736802842882, "compression_ratio": 1.5854922279792747, "no_speech_prob": 8.012725629669148e-06}, {"id": 304, "seek": 211900, "start": 2126.0, "end": 2135.0, "text": " And in terms of language modeling, that's a really enormous jump in performance.", "tokens": [50364, 583, 550, 562, 291, 5366, 4088, 433, 11, 291, 483, 257, 534, 955, 3012, 760, 281, 352, 281, 2443, 293, 945, 13, 50714, 50714, 400, 294, 2115, 295, 2856, 15983, 11, 300, 311, 257, 534, 11322, 3012, 294, 3389, 13, 51164, 51164, 400, 286, 820, 584, 613, 16823, 321, 1866, 645, 4098, 2416, 322, 733, 295, 938, 4319, 2856, 15983, 13, 51514, 51514, 407, 2856, 15983, 689, 485, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1759736802842882, "compression_ratio": 1.5854922279792747, "no_speech_prob": 8.012725629669148e-06}, {"id": 305, "seek": 211900, "start": 2135.0, "end": 2142.0, "text": " And I should say these gains we saw were particularly large on kind of long context language modeling.", "tokens": [50364, 583, 550, 562, 291, 5366, 4088, 433, 11, 291, 483, 257, 534, 955, 3012, 760, 281, 352, 281, 2443, 293, 945, 13, 50714, 50714, 400, 294, 2115, 295, 2856, 15983, 11, 300, 311, 257, 534, 11322, 3012, 294, 3389, 13, 51164, 51164, 400, 286, 820, 584, 613, 16823, 321, 1866, 645, 4098, 2416, 322, 733, 295, 938, 4319, 2856, 15983, 13, 51514, 51514, 407, 2856, 15983, 689, 485, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1759736802842882, "compression_ratio": 1.5854922279792747, "no_speech_prob": 8.012725629669148e-06}, {"id": 306, "seek": 211900, "start": 2142.0, "end": 2145.0, "text": " So language modeling where...", "tokens": [50364, 583, 550, 562, 291, 5366, 4088, 433, 11, 291, 483, 257, 534, 955, 3012, 760, 281, 352, 281, 2443, 293, 945, 13, 50714, 50714, 400, 294, 2115, 295, 2856, 15983, 11, 300, 311, 257, 534, 11322, 3012, 294, 3389, 13, 51164, 51164, 400, 286, 820, 584, 613, 16823, 321, 1866, 645, 4098, 2416, 322, 733, 295, 938, 4319, 2856, 15983, 13, 51514, 51514, 407, 2856, 15983, 689, 485, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1759736802842882, "compression_ratio": 1.5854922279792747, "no_speech_prob": 8.012725629669148e-06}, {"id": 307, "seek": 214500, "start": 2145.0, "end": 2153.0, "text": " So there's some benchmarks where you just get single sentence predicts. On this task, you actually get a whole Wikipedia article, so", "tokens": [50364, 407, 456, 311, 512, 43751, 689, 291, 445, 483, 2167, 8174, 6069, 82, 13, 1282, 341, 5633, 11, 291, 767, 483, 257, 1379, 28999, 7222, 11, 370, 50764, 50764, 7263, 5383, 295, 2283, 13, 400, 264, 4088, 433, 534, 12207, 322, 341, 11, 689, 291, 600, 658, 5383, 295, 2283, 4319, 2316, 11, 293, 291, 643, 281, 18340, 1589, 2108, 439, 295, 552, 13, 51464, 51464, 1033, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.22404032283359104, "compression_ratio": 1.5792079207920793, "no_speech_prob": 2.3538344976259395e-05}, {"id": 308, "seek": 214500, "start": 2153.0, "end": 2167.0, "text": " potentially thousands of words. And the transformers really shine on this, where you've got thousands of words context model, and you need to retain information across all of them.", "tokens": [50364, 407, 456, 311, 512, 43751, 689, 291, 445, 483, 2167, 8174, 6069, 82, 13, 1282, 341, 5633, 11, 291, 767, 483, 257, 1379, 28999, 7222, 11, 370, 50764, 50764, 7263, 5383, 295, 2283, 13, 400, 264, 4088, 433, 534, 12207, 322, 341, 11, 689, 291, 600, 658, 5383, 295, 2283, 4319, 2316, 11, 293, 291, 643, 281, 18340, 1589, 2108, 439, 295, 552, 13, 51464, 51464, 1033, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.22404032283359104, "compression_ratio": 1.5792079207920793, "no_speech_prob": 2.3538344976259395e-05}, {"id": 309, "seek": 214500, "start": 2167.0, "end": 2174.0, "text": " Okay.", "tokens": [50364, 407, 456, 311, 512, 43751, 689, 291, 445, 483, 2167, 8174, 6069, 82, 13, 1282, 341, 5633, 11, 291, 767, 483, 257, 1379, 28999, 7222, 11, 370, 50764, 50764, 7263, 5383, 295, 2283, 13, 400, 264, 4088, 433, 534, 12207, 322, 341, 11, 689, 291, 600, 658, 5383, 295, 2283, 4319, 2316, 11, 293, 291, 643, 281, 18340, 1589, 2108, 439, 295, 552, 13, 51464, 51464, 1033, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.22404032283359104, "compression_ratio": 1.5792079207920793, "no_speech_prob": 2.3538344976259395e-05}, {"id": 310, "seek": 217400, "start": 2174.0, "end": 2181.0, "text": " What else can I say? Yeah, so this quick comparison just to visualize how transformers and NLSTMs look.", "tokens": [50364, 708, 1646, 393, 286, 584, 30, 865, 11, 370, 341, 1702, 9660, 445, 281, 23273, 577, 4088, 433, 293, 426, 43, 6840, 26386, 574, 13, 50714, 50714, 3013, 307, 733, 295, 10919, 666, 264, 2793, 286, 390, 4099, 949, 13, 407, 294, 264, 426, 43, 6840, 44, 11, 291, 362, 257, 688, 13366, 9271, 1296, 264, 2283, 11, 411, 1203, 311, 733, 295, 588, 42881, 1411, 281, 558, 13, 51064, 51064, 682, 31782, 11, 291, 500, 380, 362, 604, 295, 341, 13, 51264, 51264, 407, 633, 1349, 307, 3838, 4582, 281, 633, 661, 1349, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.23514932574647845, "compression_ratio": 1.5903614457831325, "no_speech_prob": 2.8851623937953264e-05}, {"id": 311, "seek": 217400, "start": 2181.0, "end": 2188.0, "text": " Which is kind of integrated into the points I was showing before. So in the NLSTM, you have a lot fewer connections between the words, like everything's kind of very sequential left to right.", "tokens": [50364, 708, 1646, 393, 286, 584, 30, 865, 11, 370, 341, 1702, 9660, 445, 281, 23273, 577, 4088, 433, 293, 426, 43, 6840, 26386, 574, 13, 50714, 50714, 3013, 307, 733, 295, 10919, 666, 264, 2793, 286, 390, 4099, 949, 13, 407, 294, 264, 426, 43, 6840, 44, 11, 291, 362, 257, 688, 13366, 9271, 1296, 264, 2283, 11, 411, 1203, 311, 733, 295, 588, 42881, 1411, 281, 558, 13, 51064, 51064, 682, 31782, 11, 291, 500, 380, 362, 604, 295, 341, 13, 51264, 51264, 407, 633, 1349, 307, 3838, 4582, 281, 633, 661, 1349, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.23514932574647845, "compression_ratio": 1.5903614457831325, "no_speech_prob": 2.8851623937953264e-05}, {"id": 312, "seek": 217400, "start": 2188.0, "end": 2192.0, "text": " In transformer, you don't have any of this.", "tokens": [50364, 708, 1646, 393, 286, 584, 30, 865, 11, 370, 341, 1702, 9660, 445, 281, 23273, 577, 4088, 433, 293, 426, 43, 6840, 26386, 574, 13, 50714, 50714, 3013, 307, 733, 295, 10919, 666, 264, 2793, 286, 390, 4099, 949, 13, 407, 294, 264, 426, 43, 6840, 44, 11, 291, 362, 257, 688, 13366, 9271, 1296, 264, 2283, 11, 411, 1203, 311, 733, 295, 588, 42881, 1411, 281, 558, 13, 51064, 51064, 682, 31782, 11, 291, 500, 380, 362, 604, 295, 341, 13, 51264, 51264, 407, 633, 1349, 307, 3838, 4582, 281, 633, 661, 1349, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.23514932574647845, "compression_ratio": 1.5903614457831325, "no_speech_prob": 2.8851623937953264e-05}, {"id": 313, "seek": 217400, "start": 2192.0, "end": 2197.0, "text": " So every word is directly connected to every other word.", "tokens": [50364, 708, 1646, 393, 286, 584, 30, 865, 11, 370, 341, 1702, 9660, 445, 281, 23273, 577, 4088, 433, 293, 426, 43, 6840, 26386, 574, 13, 50714, 50714, 3013, 307, 733, 295, 10919, 666, 264, 2793, 286, 390, 4099, 949, 13, 407, 294, 264, 426, 43, 6840, 44, 11, 291, 362, 257, 688, 13366, 9271, 1296, 264, 2283, 11, 411, 1203, 311, 733, 295, 588, 42881, 1411, 281, 558, 13, 51064, 51064, 682, 31782, 11, 291, 500, 380, 362, 604, 295, 341, 13, 51264, 51264, 407, 633, 1349, 307, 3838, 4582, 281, 633, 661, 1349, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.23514932574647845, "compression_ratio": 1.5903614457831325, "no_speech_prob": 2.8851623937953264e-05}, {"id": 314, "seek": 219700, "start": 2197.0, "end": 2205.0, "text": " I guess I should say as well, in some sense, this is maybe slightly unnatural model for reading. So it kind of suggests that", "tokens": [50364, 286, 2041, 286, 820, 584, 382, 731, 11, 294, 512, 2020, 11, 341, 307, 1310, 4748, 43470, 2316, 337, 3760, 13, 407, 309, 733, 295, 13409, 300, 50764, 50764, 4461, 1359, 307, 3760, 2487, 11, 633, 565, 309, 15700, 257, 1349, 11, 309, 1709, 293, 319, 12, 2538, 82, 633, 661, 1349, 588, 2661, 13, 51164, 51164, 583, 309, 311, 588, 4942, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.25908299346468344, "compression_ratio": 1.4689265536723164, "no_speech_prob": 6.438770469685551e-06}, {"id": 315, "seek": 219700, "start": 2205.0, "end": 2213.0, "text": " however small is reading text, every time it reads a word, it goes and re-reads every other word very quickly.", "tokens": [50364, 286, 2041, 286, 820, 584, 382, 731, 11, 294, 512, 2020, 11, 341, 307, 1310, 4748, 43470, 2316, 337, 3760, 13, 407, 309, 733, 295, 13409, 300, 50764, 50764, 4461, 1359, 307, 3760, 2487, 11, 633, 565, 309, 15700, 257, 1349, 11, 309, 1709, 293, 319, 12, 2538, 82, 633, 661, 1349, 588, 2661, 13, 51164, 51164, 583, 309, 311, 588, 4942, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.25908299346468344, "compression_ratio": 1.4689265536723164, "no_speech_prob": 6.438770469685551e-06}, {"id": 316, "seek": 219700, "start": 2213.0, "end": 2218.0, "text": " But it's very effective.", "tokens": [50364, 286, 2041, 286, 820, 584, 382, 731, 11, 294, 512, 2020, 11, 341, 307, 1310, 4748, 43470, 2316, 337, 3760, 13, 407, 309, 733, 295, 13409, 300, 50764, 50764, 4461, 1359, 307, 3760, 2487, 11, 633, 565, 309, 15700, 257, 1349, 11, 309, 1709, 293, 319, 12, 2538, 82, 633, 661, 1349, 588, 2661, 13, 51164, 51164, 583, 309, 311, 588, 4942, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.25908299346468344, "compression_ratio": 1.4689265536723164, "no_speech_prob": 6.438770469685551e-06}, {"id": 317, "seek": 221800, "start": 2218.0, "end": 2230.0, "text": " One other good thing about transformers is that they do scale up extremely well. So with tasks like language modeling, you get essentially infinite amounts of data, because there's just hundreds of billions of words out there.", "tokens": [50364, 1485, 661, 665, 551, 466, 4088, 433, 307, 300, 436, 360, 4373, 493, 4664, 731, 13, 407, 365, 9608, 411, 2856, 15983, 11, 291, 483, 4476, 13785, 11663, 295, 1412, 11, 570, 456, 311, 445, 6779, 295, 17375, 295, 2283, 484, 456, 13, 50964, 50964, 663, 311, 1400, 544, 813, 291, 1116, 1562, 643, 13, 51114, 51114, 663, 1355, 281, 767, 3318, 341, 733, 295, 7316, 11, 291, 643, 588, 955, 5245, 13, 51464, 51464, 759, 291, 445, 1066, 322, 5127, 9834, 281, 4088, 433, 11, 436, 1066, 322, 445, 1364, 1101, 293, 1101, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14036894326258187, "compression_ratio": 1.6629213483146068, "no_speech_prob": 6.961254257475957e-06}, {"id": 318, "seek": 221800, "start": 2230.0, "end": 2233.0, "text": " That's far more than you'd ever need.", "tokens": [50364, 1485, 661, 665, 551, 466, 4088, 433, 307, 300, 436, 360, 4373, 493, 4664, 731, 13, 407, 365, 9608, 411, 2856, 15983, 11, 291, 483, 4476, 13785, 11663, 295, 1412, 11, 570, 456, 311, 445, 6779, 295, 17375, 295, 2283, 484, 456, 13, 50964, 50964, 663, 311, 1400, 544, 813, 291, 1116, 1562, 643, 13, 51114, 51114, 663, 1355, 281, 767, 3318, 341, 733, 295, 7316, 11, 291, 643, 588, 955, 5245, 13, 51464, 51464, 759, 291, 445, 1066, 322, 5127, 9834, 281, 4088, 433, 11, 436, 1066, 322, 445, 1364, 1101, 293, 1101, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14036894326258187, "compression_ratio": 1.6629213483146068, "no_speech_prob": 6.961254257475957e-06}, {"id": 319, "seek": 221800, "start": 2233.0, "end": 2240.0, "text": " That means to actually fit this kind of distribution, you need very big models.", "tokens": [50364, 1485, 661, 665, 551, 466, 4088, 433, 307, 300, 436, 360, 4373, 493, 4664, 731, 13, 407, 365, 9608, 411, 2856, 15983, 11, 291, 483, 4476, 13785, 11663, 295, 1412, 11, 570, 456, 311, 445, 6779, 295, 17375, 295, 2283, 484, 456, 13, 50964, 50964, 663, 311, 1400, 544, 813, 291, 1116, 1562, 643, 13, 51114, 51114, 663, 1355, 281, 767, 3318, 341, 733, 295, 7316, 11, 291, 643, 588, 955, 5245, 13, 51464, 51464, 759, 291, 445, 1066, 322, 5127, 9834, 281, 4088, 433, 11, 436, 1066, 322, 445, 1364, 1101, 293, 1101, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14036894326258187, "compression_ratio": 1.6629213483146068, "no_speech_prob": 6.961254257475957e-06}, {"id": 320, "seek": 221800, "start": 2240.0, "end": 2245.0, "text": " If you just keep on adding parameters to transformers, they keep on just working better and better.", "tokens": [50364, 1485, 661, 665, 551, 466, 4088, 433, 307, 300, 436, 360, 4373, 493, 4664, 731, 13, 407, 365, 9608, 411, 2856, 15983, 11, 291, 483, 4476, 13785, 11663, 295, 1412, 11, 570, 456, 311, 445, 6779, 295, 17375, 295, 2283, 484, 456, 13, 50964, 50964, 663, 311, 1400, 544, 813, 291, 1116, 1562, 643, 13, 51114, 51114, 663, 1355, 281, 767, 3318, 341, 733, 295, 7316, 11, 291, 643, 588, 955, 5245, 13, 51464, 51464, 759, 291, 445, 1066, 322, 5127, 9834, 281, 4088, 433, 11, 436, 1066, 322, 445, 1364, 1101, 293, 1101, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14036894326258187, "compression_ratio": 1.6629213483146068, "no_speech_prob": 6.961254257475957e-06}, {"id": 321, "seek": 224500, "start": 2245.0, "end": 2261.0, "text": " The examples I showed you before were from this GPT-2 model with 2 billion parameters, which was quite big for 2019, but in 2020 we're up to 17 billion and there are rumors that 100 billion parameter models will be coming along soon.", "tokens": [50364, 440, 5110, 286, 4712, 291, 949, 645, 490, 341, 26039, 51, 12, 17, 2316, 365, 568, 5218, 9834, 11, 597, 390, 1596, 955, 337, 6071, 11, 457, 294, 4808, 321, 434, 493, 281, 3282, 5218, 293, 456, 366, 21201, 300, 2319, 5218, 13075, 5245, 486, 312, 1348, 2051, 2321, 13, 51164, 51164, 11359, 385, 11, 286, 362, 257, 1168, 13, 51314, 51314, 509, 848, 4088, 433, 366, 534, 665, 337, 21589, 493, 13, 286, 390, 445, 6359, 294, 264, 2856, 15983, 5633, 11, 498, 321, 362, 411, 11, 584, 257, 1266, 11, 1360, 11, 945, 11, 1360, 1349, 4166, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19825207270108736, "compression_ratio": 1.5289855072463767, "no_speech_prob": 2.111129288095981e-05}, {"id": 322, "seek": 224500, "start": 2261.0, "end": 2264.0, "text": " Excuse me, I have a question.", "tokens": [50364, 440, 5110, 286, 4712, 291, 949, 645, 490, 341, 26039, 51, 12, 17, 2316, 365, 568, 5218, 9834, 11, 597, 390, 1596, 955, 337, 6071, 11, 457, 294, 4808, 321, 434, 493, 281, 3282, 5218, 293, 456, 366, 21201, 300, 2319, 5218, 13075, 5245, 486, 312, 1348, 2051, 2321, 13, 51164, 51164, 11359, 385, 11, 286, 362, 257, 1168, 13, 51314, 51314, 509, 848, 4088, 433, 366, 534, 665, 337, 21589, 493, 13, 286, 390, 445, 6359, 294, 264, 2856, 15983, 5633, 11, 498, 321, 362, 411, 11, 584, 257, 1266, 11, 1360, 11, 945, 11, 1360, 1349, 4166, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19825207270108736, "compression_ratio": 1.5289855072463767, "no_speech_prob": 2.111129288095981e-05}, {"id": 323, "seek": 224500, "start": 2264.0, "end": 2274.0, "text": " You said transformers are really good for scaling up. I was just wondering in the language modeling task, if we have like, say a 10,000, 20,000 word document,", "tokens": [50364, 440, 5110, 286, 4712, 291, 949, 645, 490, 341, 26039, 51, 12, 17, 2316, 365, 568, 5218, 9834, 11, 597, 390, 1596, 955, 337, 6071, 11, 457, 294, 4808, 321, 434, 493, 281, 3282, 5218, 293, 456, 366, 21201, 300, 2319, 5218, 13075, 5245, 486, 312, 1348, 2051, 2321, 13, 51164, 51164, 11359, 385, 11, 286, 362, 257, 1168, 13, 51314, 51314, 509, 848, 4088, 433, 366, 534, 665, 337, 21589, 493, 13, 286, 390, 445, 6359, 294, 264, 2856, 15983, 5633, 11, 498, 321, 362, 411, 11, 584, 257, 1266, 11, 1360, 11, 945, 11, 1360, 1349, 4166, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19825207270108736, "compression_ratio": 1.5289855072463767, "no_speech_prob": 2.111129288095981e-05}, {"id": 324, "seek": 227400, "start": 2274.0, "end": 2284.0, "text": " in an RNN, we can just insert a word step by step and we wouldn't need a lot of memory, per se.", "tokens": [50364, 294, 364, 45702, 45, 11, 321, 393, 445, 8969, 257, 1349, 1823, 538, 1823, 293, 321, 2759, 380, 643, 257, 688, 295, 4675, 11, 680, 369, 13, 50864, 50864, 1171, 257, 31782, 11, 321, 1116, 643, 281, 362, 257, 15245, 2744, 295, 1266, 11, 1360, 11, 2759, 380, 321, 30, 51114, 51114, 1743, 264, 4641, 295, 264, 8310, 13, 583, 498, 291, 362, 257, 534, 938, 8310, 11, 393, 321, 2316, 613, 938, 12, 7039, 36606, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13692614472942588, "compression_ratio": 1.4467005076142132, "no_speech_prob": 1.805694046197459e-05}, {"id": 325, "seek": 227400, "start": 2284.0, "end": 2289.0, "text": " For a transformer, we'd need to have a batch size of 10,000, wouldn't we?", "tokens": [50364, 294, 364, 45702, 45, 11, 321, 393, 445, 8969, 257, 1349, 1823, 538, 1823, 293, 321, 2759, 380, 643, 257, 688, 295, 4675, 11, 680, 369, 13, 50864, 50864, 1171, 257, 31782, 11, 321, 1116, 643, 281, 362, 257, 15245, 2744, 295, 1266, 11, 1360, 11, 2759, 380, 321, 30, 51114, 51114, 1743, 264, 4641, 295, 264, 8310, 13, 583, 498, 291, 362, 257, 534, 938, 8310, 11, 393, 321, 2316, 613, 938, 12, 7039, 36606, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13692614472942588, "compression_ratio": 1.4467005076142132, "no_speech_prob": 1.805694046197459e-05}, {"id": 326, "seek": 227400, "start": 2289.0, "end": 2302.0, "text": " Like the length of the sequence. But if you have a really long sequence, can we model these long-term dependencies?", "tokens": [50364, 294, 364, 45702, 45, 11, 321, 393, 445, 8969, 257, 1349, 1823, 538, 1823, 293, 321, 2759, 380, 643, 257, 688, 295, 4675, 11, 680, 369, 13, 50864, 50864, 1171, 257, 31782, 11, 321, 1116, 643, 281, 362, 257, 15245, 2744, 295, 1266, 11, 1360, 11, 2759, 380, 321, 30, 51114, 51114, 1743, 264, 4641, 295, 264, 8310, 13, 583, 498, 291, 362, 257, 534, 938, 8310, 11, 393, 321, 2316, 613, 938, 12, 7039, 36606, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13692614472942588, "compression_ratio": 1.4467005076142132, "no_speech_prob": 1.805694046197459e-05}, {"id": 327, "seek": 230200, "start": 2302.0, "end": 2313.0, "text": " Yeah, that's a really great question. I actually meant to mention this point.", "tokens": [50364, 865, 11, 300, 311, 257, 534, 869, 1168, 13, 286, 767, 4140, 281, 2152, 341, 935, 13, 50914, 50914, 4453, 721, 286, 528, 281, 584, 13, 407, 27376, 11, 291, 434, 3122, 558, 13, 440, 2698, 12, 83, 3378, 307, 11, 570, 309, 311, 37262, 11, 264, 18406, 2745, 13156, 1687, 43586, 11, 293, 300, 307, 257, 1154, 294, 3124, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.15966714712289665, "compression_ratio": 1.4505494505494505, "no_speech_prob": 2.0140887500019744e-05}, {"id": 328, "seek": 230200, "start": 2313.0, "end": 2324.0, "text": " Two things I want to say. So firstly, you're absolutely right. The self-tension is, because it's quadratic, the expense obviously grows super linearly, and that is a problem in practice.", "tokens": [50364, 865, 11, 300, 311, 257, 534, 869, 1168, 13, 286, 767, 4140, 281, 2152, 341, 935, 13, 50914, 50914, 4453, 721, 286, 528, 281, 584, 13, 407, 27376, 11, 291, 434, 3122, 558, 13, 440, 2698, 12, 83, 3378, 307, 11, 570, 309, 311, 37262, 11, 264, 18406, 2745, 13156, 1687, 43586, 11, 293, 300, 307, 257, 1154, 294, 3124, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.15966714712289665, "compression_ratio": 1.4505494505494505, "no_speech_prob": 2.0140887500019744e-05}, {"id": 329, "seek": 232400, "start": 2324.0, "end": 2338.0, "text": " It means, so mostly transformers do 512-token context windows, which is fairly affordable for GPUs.", "tokens": [50364, 467, 1355, 11, 370, 5240, 4088, 433, 360, 1025, 4762, 12, 83, 8406, 4319, 9309, 11, 597, 307, 6457, 12028, 337, 18407, 82, 13, 51064, 51064, 286, 959, 613, 2856, 5245, 281, 360, 544, 813, 300, 13, 814, 815, 2316, 257, 1326, 4714, 22667, 11, 597, 733, 295, 10406, 437, 321, 393, 360, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.19374674764172783, "compression_ratio": 1.3636363636363635, "no_speech_prob": 2.1774443666799925e-05}, {"id": 330, "seek": 232400, "start": 2338.0, "end": 2346.0, "text": " I love these language models to do more than that. They may model a few thousand tokens, which kind of limits what we can do.", "tokens": [50364, 467, 1355, 11, 370, 5240, 4088, 433, 360, 1025, 4762, 12, 83, 8406, 4319, 9309, 11, 597, 307, 6457, 12028, 337, 18407, 82, 13, 51064, 51064, 286, 959, 613, 2856, 5245, 281, 360, 544, 813, 300, 13, 814, 815, 2316, 257, 1326, 4714, 22667, 11, 597, 733, 295, 10406, 437, 321, 393, 360, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.19374674764172783, "compression_ratio": 1.3636363636363635, "no_speech_prob": 2.1774443666799925e-05}, {"id": 331, "seek": 234600, "start": 2346.0, "end": 2357.0, "text": " But it's definitely the case that a vanilla transformer can't model, say, a 50,000 word book at all.", "tokens": [50364, 583, 309, 311, 2138, 264, 1389, 300, 257, 17528, 31782, 393, 380, 2316, 11, 584, 11, 257, 2625, 11, 1360, 1349, 1446, 412, 439, 13, 50914, 50914, 821, 307, 411, 11, 341, 1379, 733, 295, 2899, 3518, 3938, 295, 2390, 3683, 4088, 433, 11, 597, 393, 360, 938, 22978, 13, 467, 311, 257, 588, 2368, 4829, 558, 586, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.19412021031455387, "compression_ratio": 1.4, "no_speech_prob": 4.710615030489862e-06}, {"id": 332, "seek": 234600, "start": 2357.0, "end": 2368.0, "text": " There is like, this whole kind of technology industry recently of building various transformers, which can do long sequences. It's a very hot topic right now.", "tokens": [50364, 583, 309, 311, 2138, 264, 1389, 300, 257, 17528, 31782, 393, 380, 2316, 11, 584, 11, 257, 2625, 11, 1360, 1349, 1446, 412, 439, 13, 50914, 50914, 821, 307, 411, 11, 341, 1379, 733, 295, 2899, 3518, 3938, 295, 2390, 3683, 4088, 433, 11, 597, 393, 360, 938, 22978, 13, 467, 311, 257, 588, 2368, 4829, 558, 586, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.19412021031455387, "compression_ratio": 1.4, "no_speech_prob": 4.710615030489862e-06}, {"id": 333, "seek": 236800, "start": 2368.0, "end": 2387.0, "text": " There's a bunch of things you can do, but one kind of thing you can do is, if you replace the self-tension with something like nearest neighbor search, you can do the self-tension subquadratic time, and that makes it faster.", "tokens": [50364, 821, 311, 257, 3840, 295, 721, 291, 393, 360, 11, 457, 472, 733, 295, 551, 291, 393, 360, 307, 11, 498, 291, 7406, 264, 2698, 12, 83, 3378, 365, 746, 411, 23831, 5987, 3164, 11, 291, 393, 360, 264, 2698, 12, 83, 3378, 1422, 358, 345, 25198, 565, 11, 293, 300, 1669, 309, 4663, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.1438207303063344, "compression_ratio": 1.5664335664335665, "no_speech_prob": 6.74748253004509e-06}, {"id": 334, "seek": 238700, "start": 2387.0, "end": 2407.0, "text": " There are also versions that try to do sparse attention, where you can't tend to every previous word directly, but you kind of have some dilated set of previous words you can look at, and you don't quite get direct connections to every word, but you can still guarantee that you have short paths across every word.", "tokens": [50364, 821, 366, 611, 9606, 300, 853, 281, 360, 637, 11668, 3202, 11, 689, 291, 393, 380, 3928, 281, 633, 3894, 1349, 3838, 11, 457, 291, 733, 295, 362, 512, 11504, 770, 992, 295, 3894, 2283, 291, 393, 574, 412, 11, 293, 291, 500, 380, 1596, 483, 2047, 9271, 281, 633, 1349, 11, 457, 291, 393, 920, 10815, 300, 291, 362, 2099, 14518, 2108, 633, 1349, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.1416924476623535, "compression_ratio": 1.715846994535519, "no_speech_prob": 8.937378879636526e-06}, {"id": 335, "seek": 240700, "start": 2407.0, "end": 2422.0, "text": " There are also things like compressed transformers, which try to bundle the night and total into compressed, distant paths into shorter representations.", "tokens": [50364, 821, 366, 611, 721, 411, 30353, 4088, 433, 11, 597, 853, 281, 24438, 264, 1818, 293, 3217, 666, 30353, 11, 17275, 14518, 666, 11639, 33358, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.5572031656901042, "compression_ratio": 1.345132743362832, "no_speech_prob": 2.7956717531196773e-05}, {"id": 336, "seek": 242200, "start": 2422.0, "end": 2439.0, "text": " Okay, so you brought up the question of RNN. So at impressed time, absolutely, and RNN can model infinite context with absolutely no additional cost, which is great. It can output like a million words, and output can go in this just fine.", "tokens": [50364, 1033, 11, 370, 291, 3038, 493, 264, 1168, 295, 45702, 45, 13, 407, 412, 11679, 565, 11, 3122, 11, 293, 45702, 45, 393, 2316, 13785, 4319, 365, 3122, 572, 4497, 2063, 11, 597, 307, 869, 13, 467, 393, 5598, 411, 257, 2459, 2283, 11, 293, 5598, 393, 352, 294, 341, 445, 2489, 13, 51214, 51214, 1042, 11, 264, 1168, 307, 767, 11, 307, 309, 1143, 294, 341, 4319, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.259209201760488, "compression_ratio": 1.5360824742268042, "no_speech_prob": 1.862972385424655e-05}, {"id": 337, "seek": 242200, "start": 2439.0, "end": 2446.0, "text": " Well, the question is actually, is it used in this context?", "tokens": [50364, 1033, 11, 370, 291, 3038, 493, 264, 1168, 295, 45702, 45, 13, 407, 412, 11679, 565, 11, 3122, 11, 293, 45702, 45, 393, 2316, 13785, 4319, 365, 3122, 572, 4497, 2063, 11, 597, 307, 869, 13, 467, 393, 5598, 411, 257, 2459, 2283, 11, 293, 5598, 393, 352, 294, 341, 445, 2489, 13, 51214, 51214, 1042, 11, 264, 1168, 307, 767, 11, 307, 309, 1143, 294, 341, 4319, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.259209201760488, "compression_ratio": 1.5360824742268042, "no_speech_prob": 1.862972385424655e-05}, {"id": 338, "seek": 244600, "start": 2446.0, "end": 2460.0, "text": " The answer is probably not. So at training time, you can't do this. At training time, you actually have to back propagate through what's called back propagation through time, where the LSTM would", "tokens": [50364, 440, 1867, 307, 1391, 406, 13, 407, 412, 3097, 565, 11, 291, 393, 380, 360, 341, 13, 1711, 3097, 565, 11, 291, 767, 362, 281, 646, 48256, 807, 437, 311, 1219, 646, 38377, 807, 565, 11, 689, 264, 441, 6840, 44, 576, 51064, 51064, 362, 281, 11, 498, 2316, 307, 294, 4319, 11, 411, 35540, 486, 362, 281, 48256, 439, 264, 636, 646, 807, 264, 1333, 295, 439, 264, 18680, 1753, 4439, 281, 652, 257, 2649, 294, 264, 17275, 14518, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.22729624579934513, "compression_ratio": 1.719626168224299, "no_speech_prob": 1.8924332835013047e-05}, {"id": 339, "seek": 244600, "start": 2460.0, "end": 2472.0, "text": " have to, if model is in context, like grading will have to propagate all the way back through the sort of all the recurrent steps to make a difference in the distant paths.", "tokens": [50364, 440, 1867, 307, 1391, 406, 13, 407, 412, 3097, 565, 11, 291, 393, 380, 360, 341, 13, 1711, 3097, 565, 11, 291, 767, 362, 281, 646, 48256, 807, 437, 311, 1219, 646, 38377, 807, 565, 11, 689, 264, 441, 6840, 44, 576, 51064, 51064, 362, 281, 11, 498, 2316, 307, 294, 4319, 11, 411, 35540, 486, 362, 281, 48256, 439, 264, 636, 646, 807, 264, 1333, 295, 439, 264, 18680, 1753, 4439, 281, 652, 257, 2649, 294, 264, 17275, 14518, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.22729624579934513, "compression_ratio": 1.719626168224299, "no_speech_prob": 1.8924332835013047e-05}, {"id": 340, "seek": 247200, "start": 2472.0, "end": 2488.0, "text": " And in practice, firstly, the grading will vanish like well before you hit a thousand steps. And also, this is very expensive. So this, at training time, this isn't for free. And this back propagation operation will get more and more expensive", "tokens": [50364, 400, 294, 3124, 11, 27376, 11, 264, 35540, 486, 43584, 411, 731, 949, 291, 2045, 257, 4714, 4439, 13, 400, 611, 11, 341, 307, 588, 5124, 13, 407, 341, 11, 412, 3097, 565, 11, 341, 1943, 380, 337, 1737, 13, 400, 341, 646, 38377, 6916, 486, 483, 544, 293, 544, 5124, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.22736329165371982, "compression_ratio": 1.5093167701863355, "no_speech_prob": 5.6822027545422316e-06}, {"id": 341, "seek": 248800, "start": 2488.0, "end": 2504.0, "text": " as you go through the longer sequences, you know, modeling. So that means you can't actually really learn to follow distant paths, even if the time is not expensive, you just won't know what to do with this. And in practice, you probably just forget it anyway, because", "tokens": [50364, 382, 291, 352, 807, 264, 2854, 22978, 11, 291, 458, 11, 15983, 13, 407, 300, 1355, 291, 393, 380, 767, 534, 1466, 281, 1524, 17275, 14518, 11, 754, 498, 264, 565, 307, 406, 5124, 11, 291, 445, 1582, 380, 458, 437, 281, 360, 365, 341, 13, 400, 294, 3124, 11, 291, 1391, 445, 2870, 309, 4033, 11, 570, 51164, 51164, 291, 393, 380, 1604, 300, 709, 1412, 412, 1564, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.3451567727166253, "compression_ratio": 1.5320197044334976, "no_speech_prob": 8.800097930361517e-06}, {"id": 342, "seek": 248800, "start": 2504.0, "end": 2511.0, "text": " you can't remember that much data at once.", "tokens": [50364, 382, 291, 352, 807, 264, 2854, 22978, 11, 291, 458, 11, 15983, 13, 407, 300, 1355, 291, 393, 380, 767, 534, 1466, 281, 1524, 17275, 14518, 11, 754, 498, 264, 565, 307, 406, 5124, 11, 291, 445, 1582, 380, 458, 437, 281, 360, 365, 341, 13, 400, 294, 3124, 11, 291, 1391, 445, 2870, 309, 4033, 11, 570, 51164, 51164, 291, 393, 380, 1604, 300, 709, 1412, 412, 1564, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.3451567727166253, "compression_ratio": 1.5320197044334976, "no_speech_prob": 8.800097930361517e-06}, {"id": 343, "seek": 251100, "start": 2511.0, "end": 2521.0, "text": " All right, one more quick point on that, because it's interesting. I guess one case where the RNNs do have an advantage is on certain algorithmic tasks.", "tokens": [50364, 1057, 558, 11, 472, 544, 1702, 935, 322, 300, 11, 570, 309, 311, 1880, 13, 286, 2041, 472, 1389, 689, 264, 45702, 45, 82, 360, 362, 364, 5002, 307, 322, 1629, 9284, 299, 9608, 13, 50864, 50864, 407, 498, 291, 3212, 380, 15983, 2856, 11, 718, 311, 584, 291, 434, 884, 746, 411, 4500, 420, 1382, 281, 411, 2316, 44747, 295, 257, 6798, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.13974529154160442, "compression_ratio": 1.4684210526315788, "no_speech_prob": 2.212220715591684e-05}, {"id": 344, "seek": 251100, "start": 2521.0, "end": 2532.0, "text": " So if you aren't modeling language, let's say you're doing something like addition or trying to like model parity of a string.", "tokens": [50364, 1057, 558, 11, 472, 544, 1702, 935, 322, 300, 11, 570, 309, 311, 1880, 13, 286, 2041, 472, 1389, 689, 264, 45702, 45, 82, 360, 362, 364, 5002, 307, 322, 1629, 9284, 299, 9608, 13, 50864, 50864, 407, 498, 291, 3212, 380, 15983, 2856, 11, 718, 311, 584, 291, 434, 884, 746, 411, 4500, 420, 1382, 281, 411, 2316, 44747, 295, 257, 6798, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.13974529154160442, "compression_ratio": 1.4684210526315788, "no_speech_prob": 2.212220715591684e-05}, {"id": 345, "seek": 253200, "start": 2532.0, "end": 2547.0, "text": " So if you use a string of zeros and ones and ask you, are there an even number of ones or not? In those cases, you basically do actually want to apply literally the same operation to every time step.", "tokens": [50364, 407, 498, 291, 764, 257, 6798, 295, 35193, 293, 2306, 293, 1029, 291, 11, 366, 456, 364, 754, 1230, 295, 2306, 420, 406, 30, 682, 729, 3331, 11, 291, 1936, 360, 767, 528, 281, 3079, 3736, 264, 912, 6916, 281, 633, 565, 1823, 13, 51114, 51114, 400, 291, 500, 380, 767, 362, 281, 362, 709, 4675, 570, 428, 1785, 534, 445, 2203, 281, 312, 257, 4018, 420, 472, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.19230641404243365, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.7061183825717308e-06}, {"id": 346, "seek": 253200, "start": 2547.0, "end": 2553.0, "text": " And you don't actually have to have much memory because your state really just needs to be a zero or one.", "tokens": [50364, 407, 498, 291, 764, 257, 6798, 295, 35193, 293, 2306, 293, 1029, 291, 11, 366, 456, 364, 754, 1230, 295, 2306, 420, 406, 30, 682, 729, 3331, 11, 291, 1936, 360, 767, 528, 281, 3079, 3736, 264, 912, 6916, 281, 633, 565, 1823, 13, 51114, 51114, 400, 291, 500, 380, 767, 362, 281, 362, 709, 4675, 570, 428, 1785, 534, 445, 2203, 281, 312, 257, 4018, 420, 472, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.19230641404243365, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.7061183825717308e-06}, {"id": 347, "seek": 255300, "start": 2553.0, "end": 2563.0, "text": " In this case, the RNNs actually do work very well because you can train them on short sequences and then they'll create generalizations along sequences on these kinds of toy problems.", "tokens": [50364, 682, 341, 1389, 11, 264, 45702, 45, 82, 767, 360, 589, 588, 731, 570, 291, 393, 3847, 552, 322, 2099, 22978, 293, 550, 436, 603, 1884, 2674, 14455, 2051, 22978, 322, 613, 3685, 295, 12058, 2740, 13, 50864, 50864, 400, 286, 2378, 380, 1612, 2878, 853, 11, 457, 286, 3811, 264, 31782, 576, 767, 915, 309, 709, 6081, 281, 483, 300, 733, 295, 2674, 2144, 13, 51214, 51214, 663, 787, 534, 13165, 281, 613, 733, 295, 9284, 299, 2740, 294, 2115, 295, 15983, 3303, 2856, 550, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13050671462174301, "compression_ratio": 1.6484375, "no_speech_prob": 2.4290815417771228e-05}, {"id": 348, "seek": 255300, "start": 2563.0, "end": 2570.0, "text": " And I haven't seen anyone try, but I imagine the transformer would actually find it much harder to get that kind of generalization.", "tokens": [50364, 682, 341, 1389, 11, 264, 45702, 45, 82, 767, 360, 589, 588, 731, 570, 291, 393, 3847, 552, 322, 2099, 22978, 293, 550, 436, 603, 1884, 2674, 14455, 2051, 22978, 322, 613, 3685, 295, 12058, 2740, 13, 50864, 50864, 400, 286, 2378, 380, 1612, 2878, 853, 11, 457, 286, 3811, 264, 31782, 576, 767, 915, 309, 709, 6081, 281, 483, 300, 733, 295, 2674, 2144, 13, 51214, 51214, 663, 787, 534, 13165, 281, 613, 733, 295, 9284, 299, 2740, 294, 2115, 295, 15983, 3303, 2856, 550, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13050671462174301, "compression_ratio": 1.6484375, "no_speech_prob": 2.4290815417771228e-05}, {"id": 349, "seek": 255300, "start": 2570.0, "end": 2578.0, "text": " That only really applies to these kind of algorithmic problems in terms of modeling natural language then.", "tokens": [50364, 682, 341, 1389, 11, 264, 45702, 45, 82, 767, 360, 589, 588, 731, 570, 291, 393, 3847, 552, 322, 2099, 22978, 293, 550, 436, 603, 1884, 2674, 14455, 2051, 22978, 322, 613, 3685, 295, 12058, 2740, 13, 50864, 50864, 400, 286, 2378, 380, 1612, 2878, 853, 11, 457, 286, 3811, 264, 31782, 576, 767, 915, 309, 709, 6081, 281, 483, 300, 733, 295, 2674, 2144, 13, 51214, 51214, 663, 787, 534, 13165, 281, 613, 733, 295, 9284, 299, 2740, 294, 2115, 295, 15983, 3303, 2856, 550, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13050671462174301, "compression_ratio": 1.6484375, "no_speech_prob": 2.4290815417771228e-05}, {"id": 350, "seek": 257800, "start": 2578.0, "end": 2584.0, "text": " Yeah, it seems like various transformers are going to be much more effective than recurrent Nets.", "tokens": [50364, 865, 11, 309, 2544, 411, 3683, 4088, 433, 366, 516, 281, 312, 709, 544, 4942, 813, 18680, 1753, 426, 1385, 13, 50664, 50664, 1044, 291, 13, 663, 390, 534, 4961, 13, 50914, 50914, 2639, 661, 1651, 322, 4088, 433, 30, 51064, 51064, 286, 600, 668, 14329, 264, 1651, 286, 727, 5766, 2487, 11, 370, 286, 519, 321, 366, 439, 665, 558, 586, 13, 51414, 51414, 1033, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13582637948049625, "compression_ratio": 1.435483870967742, "no_speech_prob": 7.95688247308135e-05}, {"id": 351, "seek": 257800, "start": 2584.0, "end": 2589.0, "text": " Thank you. That was really helpful.", "tokens": [50364, 865, 11, 309, 2544, 411, 3683, 4088, 433, 366, 516, 281, 312, 709, 544, 4942, 813, 18680, 1753, 426, 1385, 13, 50664, 50664, 1044, 291, 13, 663, 390, 534, 4961, 13, 50914, 50914, 2639, 661, 1651, 322, 4088, 433, 30, 51064, 51064, 286, 600, 668, 14329, 264, 1651, 286, 727, 5766, 2487, 11, 370, 286, 519, 321, 366, 439, 665, 558, 586, 13, 51414, 51414, 1033, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13582637948049625, "compression_ratio": 1.435483870967742, "no_speech_prob": 7.95688247308135e-05}, {"id": 352, "seek": 257800, "start": 2589.0, "end": 2592.0, "text": " Any other questions on transformers?", "tokens": [50364, 865, 11, 309, 2544, 411, 3683, 4088, 433, 366, 516, 281, 312, 709, 544, 4942, 813, 18680, 1753, 426, 1385, 13, 50664, 50664, 1044, 291, 13, 663, 390, 534, 4961, 13, 50914, 50914, 2639, 661, 1651, 322, 4088, 433, 30, 51064, 51064, 286, 600, 668, 14329, 264, 1651, 286, 727, 5766, 2487, 11, 370, 286, 519, 321, 366, 439, 665, 558, 586, 13, 51414, 51414, 1033, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13582637948049625, "compression_ratio": 1.435483870967742, "no_speech_prob": 7.95688247308135e-05}, {"id": 353, "seek": 257800, "start": 2592.0, "end": 2599.0, "text": " I've been addressing the questions I could via text, so I think we are all good right now.", "tokens": [50364, 865, 11, 309, 2544, 411, 3683, 4088, 433, 366, 516, 281, 312, 709, 544, 4942, 813, 18680, 1753, 426, 1385, 13, 50664, 50664, 1044, 291, 13, 663, 390, 534, 4961, 13, 50914, 50914, 2639, 661, 1651, 322, 4088, 433, 30, 51064, 51064, 286, 600, 668, 14329, 264, 1651, 286, 727, 5766, 2487, 11, 370, 286, 519, 321, 366, 439, 665, 558, 586, 13, 51414, 51414, 1033, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13582637948049625, "compression_ratio": 1.435483870967742, "no_speech_prob": 7.95688247308135e-05}, {"id": 354, "seek": 257800, "start": 2599.0, "end": 2602.0, "text": " Okay.", "tokens": [50364, 865, 11, 309, 2544, 411, 3683, 4088, 433, 366, 516, 281, 312, 709, 544, 4942, 813, 18680, 1753, 426, 1385, 13, 50664, 50664, 1044, 291, 13, 663, 390, 534, 4961, 13, 50914, 50914, 2639, 661, 1651, 322, 4088, 433, 30, 51064, 51064, 286, 600, 668, 14329, 264, 1651, 286, 727, 5766, 2487, 11, 370, 286, 519, 321, 366, 439, 665, 558, 586, 13, 51414, 51414, 1033, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13582637948049625, "compression_ratio": 1.435483870967742, "no_speech_prob": 7.95688247308135e-05}, {"id": 355, "seek": 260200, "start": 2602.0, "end": 2609.0, "text": " All right. So next thing we want to cover is what's called decoding or inference language models.", "tokens": [50364, 1057, 558, 13, 407, 958, 551, 321, 528, 281, 2060, 307, 437, 311, 1219, 979, 8616, 420, 38253, 2856, 5245, 13, 50714, 50714, 407, 321, 434, 3097, 264, 2856, 2316, 13, 440, 2856, 2316, 8137, 264, 11, 4696, 11, 8482, 36287, 322, 721, 300, 311, 665, 3669, 293, 264, 8482, 322, 17570, 267, 804, 293, 297, 892, 694, 804, 721, 13, 51414, 51414, 583, 498, 291, 528, 281, 1884, 613, 10938, 411, 286, 4712, 291, 949, 11, 577, 360, 321, 767, 8460, 264, 2487, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.3332475597938795, "compression_ratio": 1.670995670995671, "no_speech_prob": 2.1438439944176935e-05}, {"id": 356, "seek": 260200, "start": 2609.0, "end": 2623.0, "text": " So we're training the language model. The language model puts the, hopefully, probability maths on things that's good English and the probability on grammatical and nonsensical things.", "tokens": [50364, 1057, 558, 13, 407, 958, 551, 321, 528, 281, 2060, 307, 437, 311, 1219, 979, 8616, 420, 38253, 2856, 5245, 13, 50714, 50714, 407, 321, 434, 3097, 264, 2856, 2316, 13, 440, 2856, 2316, 8137, 264, 11, 4696, 11, 8482, 36287, 322, 721, 300, 311, 665, 3669, 293, 264, 8482, 322, 17570, 267, 804, 293, 297, 892, 694, 804, 721, 13, 51414, 51414, 583, 498, 291, 528, 281, 1884, 613, 10938, 411, 286, 4712, 291, 949, 11, 577, 360, 321, 767, 8460, 264, 2487, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.3332475597938795, "compression_ratio": 1.670995670995671, "no_speech_prob": 2.1438439944176935e-05}, {"id": 357, "seek": 260200, "start": 2623.0, "end": 2631.0, "text": " But if you want to create these samples like I showed you before, how do we actually generate the text?", "tokens": [50364, 1057, 558, 13, 407, 958, 551, 321, 528, 281, 2060, 307, 437, 311, 1219, 979, 8616, 420, 38253, 2856, 5245, 13, 50714, 50714, 407, 321, 434, 3097, 264, 2856, 2316, 13, 440, 2856, 2316, 8137, 264, 11, 4696, 11, 8482, 36287, 322, 721, 300, 311, 665, 3669, 293, 264, 8482, 322, 17570, 267, 804, 293, 297, 892, 694, 804, 721, 13, 51414, 51414, 583, 498, 291, 528, 281, 1884, 613, 10938, 411, 286, 4712, 291, 949, 11, 577, 360, 321, 767, 8460, 264, 2487, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.3332475597938795, "compression_ratio": 1.670995670995671, "no_speech_prob": 2.1438439944176935e-05}, {"id": 358, "seek": 263100, "start": 2631.0, "end": 2643.0, "text": " So often when we think about the inference of the graphical model, what we care, what we'd like to do is find the max. So like finding the sensors which maximizes language models probability.", "tokens": [50364, 407, 2049, 562, 321, 519, 466, 264, 38253, 295, 264, 35942, 2316, 11, 437, 321, 1127, 11, 437, 321, 1116, 411, 281, 360, 307, 915, 264, 11469, 13, 407, 411, 5006, 264, 14840, 597, 5138, 5660, 2856, 5245, 8482, 13, 50964, 50964, 8590, 11, 456, 311, 11, 382, 286, 2835, 949, 11, 456, 311, 1596, 257, 688, 295, 3669, 16579, 300, 366, 1944, 293, 321, 393, 380, 445, 411, 6175, 264, 5245, 281, 915, 264, 11469, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.2919932942331573, "compression_ratio": 1.6164383561643836, "no_speech_prob": 2.8397786081768572e-05}, {"id": 359, "seek": 263100, "start": 2643.0, "end": 2653.0, "text": " Unfortunately, there's, as I mentioned before, there's quite a lot of English sentences that are possible and we can't just like score the models to find the max.", "tokens": [50364, 407, 2049, 562, 321, 519, 466, 264, 38253, 295, 264, 35942, 2316, 11, 437, 321, 1127, 11, 437, 321, 1116, 411, 281, 360, 307, 915, 264, 11469, 13, 407, 411, 5006, 264, 14840, 597, 5138, 5660, 2856, 5245, 8482, 13, 50964, 50964, 8590, 11, 456, 311, 11, 382, 286, 2835, 949, 11, 456, 311, 1596, 257, 688, 295, 3669, 16579, 300, 366, 1944, 293, 321, 393, 380, 445, 411, 6175, 264, 5245, 281, 915, 264, 11469, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.2919932942331573, "compression_ratio": 1.6164383561643836, "no_speech_prob": 2.8397786081768572e-05}, {"id": 360, "seek": 265300, "start": 2653.0, "end": 2671.0, "text": " Also, these models don't really, there's not really a lot to do with dynamic programming here. So sometimes you can't find the max of over exponential structures when you have a model that factorizes some way that lets you build a dynamic program,", "tokens": [50364, 2743, 11, 613, 5245, 500, 380, 534, 11, 456, 311, 406, 534, 257, 688, 281, 360, 365, 8546, 9410, 510, 13, 407, 2171, 291, 393, 380, 915, 264, 11469, 295, 670, 21510, 9227, 562, 291, 362, 257, 2316, 300, 5952, 5660, 512, 636, 300, 6653, 291, 1322, 257, 8546, 1461, 11, 51264, 51264, 597, 6653, 291, 733, 295, 2073, 3233, 2108, 819, 49969, 13, 51464, 51464, 583, 613, 5245, 500, 380, 22867, 541, 294, 257, 9208, 636, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.262866694752763, "compression_ratio": 1.635135135135135, "no_speech_prob": 4.5362001401372254e-05}, {"id": 361, "seek": 265300, "start": 2671.0, "end": 2675.0, "text": " which lets you kind of share stage across different hypotheses.", "tokens": [50364, 2743, 11, 613, 5245, 500, 380, 534, 11, 456, 311, 406, 534, 257, 688, 281, 360, 365, 8546, 9410, 510, 13, 407, 2171, 291, 393, 380, 915, 264, 11469, 295, 670, 21510, 9227, 562, 291, 362, 257, 2316, 300, 5952, 5660, 512, 636, 300, 6653, 291, 1322, 257, 8546, 1461, 11, 51264, 51264, 597, 6653, 291, 733, 295, 2073, 3233, 2108, 819, 49969, 13, 51464, 51464, 583, 613, 5245, 500, 380, 22867, 541, 294, 257, 9208, 636, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.262866694752763, "compression_ratio": 1.635135135135135, "no_speech_prob": 4.5362001401372254e-05}, {"id": 362, "seek": 265300, "start": 2675.0, "end": 2678.0, "text": " But these models don't decompose in a friendly way.", "tokens": [50364, 2743, 11, 613, 5245, 500, 380, 534, 11, 456, 311, 406, 534, 257, 688, 281, 360, 365, 8546, 9410, 510, 13, 407, 2171, 291, 393, 380, 915, 264, 11469, 295, 670, 21510, 9227, 562, 291, 362, 257, 2316, 300, 5952, 5660, 512, 636, 300, 6653, 291, 1322, 257, 8546, 1461, 11, 51264, 51264, 597, 6653, 291, 733, 295, 2073, 3233, 2108, 819, 49969, 13, 51464, 51464, 583, 613, 5245, 500, 380, 22867, 541, 294, 257, 9208, 636, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.262866694752763, "compression_ratio": 1.635135135135135, "no_speech_prob": 4.5362001401372254e-05}, {"id": 363, "seek": 267800, "start": 2678.0, "end": 2687.0, "text": " So you kind of, whatever choice you make, the first word could affect all the other decisions.", "tokens": [50364, 407, 291, 733, 295, 11, 2035, 3922, 291, 652, 11, 264, 700, 1349, 727, 3345, 439, 264, 661, 5327, 13, 50814, 50814], "temperature": 0.0, "avg_logprob": -0.21495819091796875, "compression_ratio": 1.1325301204819278, "no_speech_prob": 2.5036368242581375e-05}, {"id": 364, "seek": 268700, "start": 2687.0, "end": 2708.0, "text": " So given that, one thing to do is to do greedy decoding. This is where we're just going to take the most likely first word and then given that word, put it in the most likely second word and then guess the most likely third word.", "tokens": [50364, 407, 2212, 300, 11, 472, 551, 281, 360, 307, 281, 360, 28228, 979, 8616, 13, 639, 307, 689, 321, 434, 445, 516, 281, 747, 264, 881, 3700, 700, 1349, 293, 550, 2212, 300, 1349, 11, 829, 309, 294, 264, 881, 3700, 1150, 1349, 293, 550, 2041, 264, 881, 3700, 2636, 1349, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.23885725225721086, "compression_ratio": 1.7218045112781954, "no_speech_prob": 1.4962907698645722e-05}, {"id": 365, "seek": 270800, "start": 2708.0, "end": 2728.0, "text": " So that's okay, but there's no guarantee that it's actually going to be the most likely sequence you want to output, because if you happen to make a bad step at some point, then you've got no way of backtracking your search to undo any previous decisions.", "tokens": [50364, 407, 300, 311, 1392, 11, 457, 456, 311, 572, 10815, 300, 309, 311, 767, 516, 281, 312, 264, 881, 3700, 8310, 291, 528, 281, 5598, 11, 570, 498, 291, 1051, 281, 652, 257, 1578, 1823, 412, 512, 935, 11, 550, 291, 600, 658, 572, 636, 295, 646, 6903, 14134, 428, 3164, 281, 23779, 604, 3894, 5327, 13, 51364, 51364, 440, 4137, 1619, 300, 14687, 488, 3164, 307, 6243, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.19014298425961848, "compression_ratio": 1.5247524752475248, "no_speech_prob": 1.3709205859413487e-06}, {"id": 366, "seek": 270800, "start": 2728.0, "end": 2733.0, "text": " The slide says that exhaustive search is impossible.", "tokens": [50364, 407, 300, 311, 1392, 11, 457, 456, 311, 572, 10815, 300, 309, 311, 767, 516, 281, 312, 264, 881, 3700, 8310, 291, 528, 281, 5598, 11, 570, 498, 291, 1051, 281, 652, 257, 1578, 1823, 412, 512, 935, 11, 550, 291, 600, 658, 572, 636, 295, 646, 6903, 14134, 428, 3164, 281, 23779, 604, 3894, 5327, 13, 51364, 51364, 440, 4137, 1619, 300, 14687, 488, 3164, 307, 6243, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.19014298425961848, "compression_ratio": 1.5247524752475248, "no_speech_prob": 1.3709205859413487e-06}, {"id": 367, "seek": 273300, "start": 2733.0, "end": 2738.0, "text": " And then we're going to hit some middle ground with what's called beam search.", "tokens": [50364, 400, 550, 321, 434, 516, 281, 2045, 512, 2808, 2727, 365, 437, 311, 1219, 14269, 3164, 13, 50614, 50614, 407, 14269, 3164, 307, 257, 636, 295, 1382, 281, 1066, 2837, 295, 364, 376, 12, 25331, 1329, 295, 49969, 13, 50964, 50964, 400, 550, 321, 434, 516, 281, 445, 633, 565, 1823, 853, 293, 1066, 2837, 11, 5623, 341, 1329, 365, 777, 2283, 321, 600, 3869, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.23577071598597935, "compression_ratio": 1.5549132947976878, "no_speech_prob": 4.565136350720422e-06}, {"id": 368, "seek": 273300, "start": 2738.0, "end": 2745.0, "text": " So beam search is a way of trying to keep track of an M-best list of hypotheses.", "tokens": [50364, 400, 550, 321, 434, 516, 281, 2045, 512, 2808, 2727, 365, 437, 311, 1219, 14269, 3164, 13, 50614, 50614, 407, 14269, 3164, 307, 257, 636, 295, 1382, 281, 1066, 2837, 295, 364, 376, 12, 25331, 1329, 295, 49969, 13, 50964, 50964, 400, 550, 321, 434, 516, 281, 445, 633, 565, 1823, 853, 293, 1066, 2837, 11, 5623, 341, 1329, 365, 777, 2283, 321, 600, 3869, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.23577071598597935, "compression_ratio": 1.5549132947976878, "no_speech_prob": 4.565136350720422e-06}, {"id": 369, "seek": 273300, "start": 2745.0, "end": 2758.0, "text": " And then we're going to just every time step try and keep track, update this list with new words we've added.", "tokens": [50364, 400, 550, 321, 434, 516, 281, 2045, 512, 2808, 2727, 365, 437, 311, 1219, 14269, 3164, 13, 50614, 50614, 407, 14269, 3164, 307, 257, 636, 295, 1382, 281, 1066, 2837, 295, 364, 376, 12, 25331, 1329, 295, 49969, 13, 50964, 50964, 400, 550, 321, 434, 516, 281, 445, 633, 565, 1823, 853, 293, 1066, 2837, 11, 5623, 341, 1329, 365, 777, 2283, 321, 600, 3869, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.23577071598597935, "compression_ratio": 1.5549132947976878, "no_speech_prob": 4.565136350720422e-06}, {"id": 370, "seek": 275800, "start": 2758.0, "end": 2765.0, "text": " This is easy to show you an example. This slide is from Abigail C. at Stanford.", "tokens": [50364, 639, 307, 1858, 281, 855, 291, 364, 1365, 13, 639, 4137, 307, 490, 47174, 383, 13, 412, 20374, 13, 50714, 50714, 407, 321, 722, 11, 321, 5598, 11, 718, 311, 584, 321, 362, 668, 2744, 295, 732, 11, 321, 603, 5598, 613, 732, 1944, 2283, 13, 1981, 366, 732, 881, 3700, 2283, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.34535220631381924, "compression_ratio": 1.4161073825503356, "no_speech_prob": 7.25310601410456e-05}, {"id": 371, "seek": 275800, "start": 2765.0, "end": 2779.0, "text": " So we start, we output, let's say we have been size of two, we'll output these two possible words. These are two most likely words.", "tokens": [50364, 639, 307, 1858, 281, 855, 291, 364, 1365, 13, 639, 4137, 307, 490, 47174, 383, 13, 412, 20374, 13, 50714, 50714, 407, 321, 722, 11, 321, 5598, 11, 718, 311, 584, 321, 362, 668, 2744, 295, 732, 11, 321, 603, 5598, 613, 732, 1944, 2283, 13, 1981, 366, 732, 881, 3700, 2283, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.34535220631381924, "compression_ratio": 1.4161073825503356, "no_speech_prob": 7.25310601410456e-05}, {"id": 372, "seek": 277900, "start": 2779.0, "end": 2788.0, "text": " So we're going to generate, work out what the two most likely next words are.", "tokens": [50364, 407, 321, 434, 516, 281, 8460, 11, 589, 484, 437, 264, 732, 881, 3700, 958, 2283, 366, 13, 50814, 50814, 407, 2745, 365, 286, 31187, 11, 300, 775, 3345, 437, 264, 958, 1349, 820, 312, 11, 293, 321, 603, 808, 493, 365, 819, 49969, 337, 1184, 295, 613, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.28148726697237986, "compression_ratio": 1.4093959731543624, "no_speech_prob": 4.3900083255721256e-05}, {"id": 373, "seek": 277900, "start": 2788.0, "end": 2800.0, "text": " So obviously with IOH, that does affect what the next word should be, and we'll come up with different hypotheses for each of these.", "tokens": [50364, 407, 321, 434, 516, 281, 8460, 11, 589, 484, 437, 264, 732, 881, 3700, 958, 2283, 366, 13, 50814, 50814, 407, 2745, 365, 286, 31187, 11, 300, 775, 3345, 437, 264, 958, 1349, 820, 312, 11, 293, 321, 603, 808, 493, 365, 819, 49969, 337, 1184, 295, 613, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.28148726697237986, "compression_ratio": 1.4093959731543624, "no_speech_prob": 4.3900083255721256e-05}, {"id": 374, "seek": 280000, "start": 2800.0, "end": 2809.0, "text": " And then at the same time, we're going to kind of compress these down to a list of two that we're going to continue processing.", "tokens": [50364, 400, 550, 412, 264, 912, 565, 11, 321, 434, 516, 281, 733, 295, 14778, 613, 760, 281, 257, 1329, 295, 732, 300, 321, 434, 516, 281, 2354, 9007, 13, 50814, 50814, 407, 321, 434, 1237, 337, 264, 12437, 3217, 2408, 11, 307, 309, 3006, 30, 50964, 50964, 865, 11, 370, 613, 366, 3565, 22119, 82, 13, 51314, 51314, 492, 767, 528, 264, 6343, 2408, 11, 498, 300, 1669, 2020, 11, 597, 820, 312, 689, 4018, 576, 312, 257, 8482, 472, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20522276934455422, "compression_ratio": 1.5837320574162679, "no_speech_prob": 1.4498830751108471e-05}, {"id": 375, "seek": 280000, "start": 2809.0, "end": 2812.0, "text": " So we're looking for the lowest total sum, is it correct?", "tokens": [50364, 400, 550, 412, 264, 912, 565, 11, 321, 434, 516, 281, 733, 295, 14778, 613, 760, 281, 257, 1329, 295, 732, 300, 321, 434, 516, 281, 2354, 9007, 13, 50814, 50814, 407, 321, 434, 1237, 337, 264, 12437, 3217, 2408, 11, 307, 309, 3006, 30, 50964, 50964, 865, 11, 370, 613, 366, 3565, 22119, 82, 13, 51314, 51314, 492, 767, 528, 264, 6343, 2408, 11, 498, 300, 1669, 2020, 11, 597, 820, 312, 689, 4018, 576, 312, 257, 8482, 472, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20522276934455422, "compression_ratio": 1.5837320574162679, "no_speech_prob": 1.4498830751108471e-05}, {"id": 376, "seek": 280000, "start": 2812.0, "end": 2819.0, "text": " Yeah, so these are log likelihoods.", "tokens": [50364, 400, 550, 412, 264, 912, 565, 11, 321, 434, 516, 281, 733, 295, 14778, 613, 760, 281, 257, 1329, 295, 732, 300, 321, 434, 516, 281, 2354, 9007, 13, 50814, 50814, 407, 321, 434, 1237, 337, 264, 12437, 3217, 2408, 11, 307, 309, 3006, 30, 50964, 50964, 865, 11, 370, 613, 366, 3565, 22119, 82, 13, 51314, 51314, 492, 767, 528, 264, 6343, 2408, 11, 498, 300, 1669, 2020, 11, 597, 820, 312, 689, 4018, 576, 312, 257, 8482, 472, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20522276934455422, "compression_ratio": 1.5837320574162679, "no_speech_prob": 1.4498830751108471e-05}, {"id": 377, "seek": 280000, "start": 2819.0, "end": 2827.0, "text": " We actually want the highest sum, if that makes sense, which should be where zero would be a probability one.", "tokens": [50364, 400, 550, 412, 264, 912, 565, 11, 321, 434, 516, 281, 733, 295, 14778, 613, 760, 281, 257, 1329, 295, 732, 300, 321, 434, 516, 281, 2354, 9007, 13, 50814, 50814, 407, 321, 434, 1237, 337, 264, 12437, 3217, 2408, 11, 307, 309, 3006, 30, 50964, 50964, 865, 11, 370, 613, 366, 3565, 22119, 82, 13, 51314, 51314, 492, 767, 528, 264, 6343, 2408, 11, 498, 300, 1669, 2020, 11, 597, 820, 312, 689, 4018, 576, 312, 257, 8482, 472, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20522276934455422, "compression_ratio": 1.5837320574162679, "no_speech_prob": 1.4498830751108471e-05}, {"id": 378, "seek": 282700, "start": 2827.0, "end": 2849.0, "text": " So every score is going to be less than zero, but we're going to find the sequence where some of the scores is the highest.", "tokens": [50364, 407, 633, 6175, 307, 516, 281, 312, 1570, 813, 4018, 11, 457, 321, 434, 516, 281, 915, 264, 8310, 689, 512, 295, 264, 13444, 307, 264, 6343, 13, 51464, 51464, 2589, 300, 652, 2020, 30, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.18478721227401343, "compression_ratio": 1.2857142857142858, "no_speech_prob": 4.6692672185599804e-05}, {"id": 379, "seek": 282700, "start": 2849.0, "end": 2850.0, "text": " Did that make sense?", "tokens": [50364, 407, 633, 6175, 307, 516, 281, 312, 1570, 813, 4018, 11, 457, 321, 434, 516, 281, 915, 264, 8310, 689, 512, 295, 264, 13444, 307, 264, 6343, 13, 51464, 51464, 2589, 300, 652, 2020, 30, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.18478721227401343, "compression_ratio": 1.2857142857142858, "no_speech_prob": 4.6692672185599804e-05}, {"id": 380, "seek": 285000, "start": 2850.0, "end": 2858.0, "text": " It is, sorry, yeah, I got the sign flipped. I was meaning like in magnitude, we tried to get the smallest number in magnitude.", "tokens": [50364, 467, 307, 11, 2597, 11, 1338, 11, 286, 658, 264, 1465, 26273, 13, 286, 390, 3620, 411, 294, 15668, 11, 321, 3031, 281, 483, 264, 16998, 1230, 294, 15668, 13, 50764, 50764, 1018, 731, 382, 264, 15668, 11, 988, 13, 50814, 50814, 865, 11, 988, 11, 2597, 13, 50914, 50914, 4919, 11, 1310, 341, 307, 406, 264, 1151, 636, 281, 855, 341, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.24000438291635087, "compression_ratio": 1.5266666666666666, "no_speech_prob": 8.012065336515661e-06}, {"id": 381, "seek": 285000, "start": 2858.0, "end": 2859.0, "text": " As well as the magnitude, sure.", "tokens": [50364, 467, 307, 11, 2597, 11, 1338, 11, 286, 658, 264, 1465, 26273, 13, 286, 390, 3620, 411, 294, 15668, 11, 321, 3031, 281, 483, 264, 16998, 1230, 294, 15668, 13, 50764, 50764, 1018, 731, 382, 264, 15668, 11, 988, 13, 50814, 50814, 865, 11, 988, 11, 2597, 13, 50914, 50914, 4919, 11, 1310, 341, 307, 406, 264, 1151, 636, 281, 855, 341, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.24000438291635087, "compression_ratio": 1.5266666666666666, "no_speech_prob": 8.012065336515661e-06}, {"id": 382, "seek": 285000, "start": 2859.0, "end": 2861.0, "text": " Yeah, sure, sorry.", "tokens": [50364, 467, 307, 11, 2597, 11, 1338, 11, 286, 658, 264, 1465, 26273, 13, 286, 390, 3620, 411, 294, 15668, 11, 321, 3031, 281, 483, 264, 16998, 1230, 294, 15668, 13, 50764, 50764, 1018, 731, 382, 264, 15668, 11, 988, 13, 50814, 50814, 865, 11, 988, 11, 2597, 13, 50914, 50914, 4919, 11, 1310, 341, 307, 406, 264, 1151, 636, 281, 855, 341, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.24000438291635087, "compression_ratio": 1.5266666666666666, "no_speech_prob": 8.012065336515661e-06}, {"id": 383, "seek": 285000, "start": 2861.0, "end": 2865.0, "text": " Sorry, maybe this is not the best way to show this.", "tokens": [50364, 467, 307, 11, 2597, 11, 1338, 11, 286, 658, 264, 1465, 26273, 13, 286, 390, 3620, 411, 294, 15668, 11, 321, 3031, 281, 483, 264, 16998, 1230, 294, 15668, 13, 50764, 50764, 1018, 731, 382, 264, 15668, 11, 988, 13, 50814, 50814, 865, 11, 988, 11, 2597, 13, 50914, 50914, 4919, 11, 1310, 341, 307, 406, 264, 1151, 636, 281, 855, 341, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.24000438291635087, "compression_ratio": 1.5266666666666666, "no_speech_prob": 8.012065336515661e-06}, {"id": 384, "seek": 286500, "start": 2865.0, "end": 2882.0, "text": " How deep does this tree go into the Bing search? When do you stop looking for candidate sequences, I guess?", "tokens": [50364, 1012, 2452, 775, 341, 4230, 352, 666, 264, 30755, 3164, 30, 1133, 360, 291, 1590, 1237, 337, 11532, 22978, 11, 286, 2041, 30, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.18148920271131727, "compression_ratio": 1.1030927835051547, "no_speech_prob": 2.0783558284165338e-05}, {"id": 385, "seek": 288200, "start": 2882.0, "end": 2895.0, "text": " So we basically, more detail I haven't told you, we have this like end of sentence token. And the end of sentence token means once you output that, this hypothesis is finished.", "tokens": [50364, 407, 321, 1936, 11, 544, 2607, 286, 2378, 380, 1907, 291, 11, 321, 362, 341, 411, 917, 295, 8174, 14862, 13, 400, 264, 917, 295, 8174, 14862, 1355, 1564, 291, 5598, 300, 11, 341, 17291, 307, 4335, 13, 51014, 51014, 400, 264, 5939, 307, 281, 915, 3566, 49969, 11, 370, 490, 722, 281, 264, 917, 14862, 11, 300, 362, 264, 6343, 1944, 6175, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1989905553705552, "compression_ratio": 1.6477272727272727, "no_speech_prob": 6.538753950735554e-06}, {"id": 386, "seek": 288200, "start": 2895.0, "end": 2910.0, "text": " And the aim is to find complete hypotheses, so from start to the end token, that have the highest possible score.", "tokens": [50364, 407, 321, 1936, 11, 544, 2607, 286, 2378, 380, 1907, 291, 11, 321, 362, 341, 411, 917, 295, 8174, 14862, 13, 400, 264, 917, 295, 8174, 14862, 1355, 1564, 291, 5598, 300, 11, 341, 17291, 307, 4335, 13, 51014, 51014, 400, 264, 5939, 307, 281, 915, 3566, 49969, 11, 370, 490, 722, 281, 264, 917, 14862, 11, 300, 362, 264, 6343, 1944, 6175, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1989905553705552, "compression_ratio": 1.6477272727272727, "no_speech_prob": 6.538753950735554e-06}, {"id": 387, "seek": 291000, "start": 2910.0, "end": 2929.0, "text": " We'll just keep on generating new hypotheses until there's no possible new words for that that would be the K complete hypothesis we have still.", "tokens": [50364, 492, 603, 445, 1066, 322, 17746, 777, 49969, 1826, 456, 311, 572, 1944, 777, 2283, 337, 300, 300, 576, 312, 264, 591, 3566, 17291, 321, 362, 920, 13, 51314, 51314, 821, 307, 1071, 1168, 510, 13, 1545, 360, 291, 519, 294, 426, 44, 51, 588, 11, 588, 2416, 14269, 2744, 486, 881, 2049, 1874, 294, 6707, 37578, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19534209466749622, "compression_ratio": 1.4293193717277486, "no_speech_prob": 8.336441533174366e-05}, {"id": 388, "seek": 291000, "start": 2929.0, "end": 2939.0, "text": " There is another question here. Why do you think in NMT very, very large beam size will most often result in empty translations?", "tokens": [50364, 492, 603, 445, 1066, 322, 17746, 777, 49969, 1826, 456, 311, 572, 1944, 777, 2283, 337, 300, 300, 576, 312, 264, 591, 3566, 17291, 321, 362, 920, 13, 51314, 51314, 821, 307, 1071, 1168, 510, 13, 1545, 360, 291, 519, 294, 426, 44, 51, 588, 11, 588, 2416, 14269, 2744, 486, 881, 2049, 1874, 294, 6707, 37578, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19534209466749622, "compression_ratio": 1.4293193717277486, "no_speech_prob": 8.336441533174366e-05}, {"id": 389, "seek": 293900, "start": 2939.0, "end": 2951.0, "text": " Great question. I have opinions on this.", "tokens": [50364, 3769, 1168, 13, 286, 362, 11819, 322, 341, 13, 50964, 50964, 407, 264, 2132, 307, 665, 294, 264, 2020, 300, 309, 311, 18031, 281, 976, 291, 257, 2946, 22358, 17291, 813, 264, 28228, 3164, 286, 2835, 322, 264, 3894, 4137, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.20319330427381727, "compression_ratio": 1.4202898550724639, "no_speech_prob": 5.6428609241265804e-05}, {"id": 390, "seek": 293900, "start": 2951.0, "end": 2965.0, "text": " So the research is good in the sense that it's guaranteed to give you a higher scoring hypothesis than the greedy search I mentioned on the previous slide.", "tokens": [50364, 3769, 1168, 13, 286, 362, 11819, 322, 341, 13, 50964, 50964, 407, 264, 2132, 307, 665, 294, 264, 2020, 300, 309, 311, 18031, 281, 976, 291, 257, 2946, 22358, 17291, 813, 264, 28228, 3164, 286, 2835, 322, 264, 3894, 4137, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.20319330427381727, "compression_ratio": 1.4202898550724639, "no_speech_prob": 5.6428609241265804e-05}, {"id": 391, "seek": 296500, "start": 2965.0, "end": 2975.0, "text": " But it's kind of a catch here, which is that we're not actually, at training time, we're typically not using a beam.", "tokens": [50364, 583, 309, 311, 733, 295, 257, 3745, 510, 11, 597, 307, 300, 321, 434, 406, 767, 11, 412, 3097, 565, 11, 321, 434, 5850, 406, 1228, 257, 14269, 13, 50864, 50864, 407, 412, 3097, 565, 321, 5646, 445, 764, 733, 295, 264, 1476, 418, 3091, 488, 5952, 2144, 286, 4712, 291, 949, 11, 689, 411, 2212, 264, 426, 3894, 3006, 23930, 11, 6069, 264, 426, 1804, 700, 1349, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1617519300277919, "compression_ratio": 1.549222797927461, "no_speech_prob": 8.938392966229003e-06}, {"id": 392, "seek": 296500, "start": 2975.0, "end": 2994.0, "text": " So at training time we normally just use kind of the autoregressive factorization I showed you before, where like given the N previous correct outputs, predict the N plus first word.", "tokens": [50364, 583, 309, 311, 733, 295, 257, 3745, 510, 11, 597, 307, 300, 321, 434, 406, 767, 11, 412, 3097, 565, 11, 321, 434, 5850, 406, 1228, 257, 14269, 13, 50864, 50864, 407, 412, 3097, 565, 321, 5646, 445, 764, 733, 295, 264, 1476, 418, 3091, 488, 5952, 2144, 286, 4712, 291, 949, 11, 689, 411, 2212, 264, 426, 3894, 3006, 23930, 11, 6069, 264, 426, 1804, 700, 1349, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1617519300277919, "compression_ratio": 1.549222797927461, "no_speech_prob": 8.938392966229003e-06}, {"id": 393, "seek": 299400, "start": 2994.0, "end": 3000.0, "text": " What we're not doing is exposing the model to its own mistakes.", "tokens": [50364, 708, 321, 434, 406, 884, 307, 33178, 264, 2316, 281, 1080, 1065, 8038, 13, 50664, 50664, 407, 562, 291, 360, 14269, 3164, 11, 291, 393, 483, 439, 3685, 295, 14925, 4099, 493, 294, 428, 14269, 337, 2035, 1778, 11, 570, 498, 291, 362, 257, 588, 955, 14269, 550, 1391, 512, 295, 309, 486, 312, 14150, 13, 51314, 51314, 400, 294, 613, 14150, 4368, 11, 264, 2316, 575, 572, 1558, 437, 281, 360, 570, 309, 390, 1128, 8895, 294, 264, 2590, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1126441057990579, "compression_ratio": 1.6153846153846154, "no_speech_prob": 6.961807685001986e-06}, {"id": 394, "seek": 299400, "start": 3000.0, "end": 3013.0, "text": " So when you do beam search, you can get all kinds of nonsense showing up in your beam for whatever reason, because if you have a very big beam then probably some of it will be garbage.", "tokens": [50364, 708, 321, 434, 406, 884, 307, 33178, 264, 2316, 281, 1080, 1065, 8038, 13, 50664, 50664, 407, 562, 291, 360, 14269, 3164, 11, 291, 393, 483, 439, 3685, 295, 14925, 4099, 493, 294, 428, 14269, 337, 2035, 1778, 11, 570, 498, 291, 362, 257, 588, 955, 14269, 550, 1391, 512, 295, 309, 486, 312, 14150, 13, 51314, 51314, 400, 294, 613, 14150, 4368, 11, 264, 2316, 575, 572, 1558, 437, 281, 360, 570, 309, 390, 1128, 8895, 294, 264, 2590, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1126441057990579, "compression_ratio": 1.6153846153846154, "no_speech_prob": 6.961807685001986e-06}, {"id": 395, "seek": 299400, "start": 3013.0, "end": 3021.0, "text": " And in these garbage states, the model has no idea what to do because it was never trained in the situation.", "tokens": [50364, 708, 321, 434, 406, 884, 307, 33178, 264, 2316, 281, 1080, 1065, 8038, 13, 50664, 50664, 407, 562, 291, 360, 14269, 3164, 11, 291, 393, 483, 439, 3685, 295, 14925, 4099, 493, 294, 428, 14269, 337, 2035, 1778, 11, 570, 498, 291, 362, 257, 588, 955, 14269, 550, 1391, 512, 295, 309, 486, 312, 14150, 13, 51314, 51314, 400, 294, 613, 14150, 4368, 11, 264, 2316, 575, 572, 1558, 437, 281, 360, 570, 309, 390, 1128, 8895, 294, 264, 2590, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1126441057990579, "compression_ratio": 1.6153846153846154, "no_speech_prob": 6.961807685001986e-06}, {"id": 396, "seek": 302100, "start": 3021.0, "end": 3032.0, "text": " So it's kind of reasonable to expect your model to generalize well for like making correct predictions for some completely nonsensical series of words, which is like very powerful training distribution.", "tokens": [50364, 407, 309, 311, 733, 295, 10585, 281, 2066, 428, 2316, 281, 2674, 1125, 731, 337, 411, 1455, 3006, 21264, 337, 512, 2584, 297, 892, 694, 804, 2638, 295, 2283, 11, 597, 307, 411, 588, 4005, 3097, 7316, 13, 50914, 50914, 400, 294, 613, 3331, 264, 2316, 393, 360, 439, 3685, 295, 3657, 293, 1028, 39754, 638, 721, 11, 411, 1310, 436, 829, 257, 588, 1090, 8482, 322, 746, 1646, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.17037770554826065, "compression_ratio": 1.576036866359447, "no_speech_prob": 2.5463179554208182e-05}, {"id": 397, "seek": 302100, "start": 3032.0, "end": 3046.0, "text": " And in these cases the model can do all kinds of weird and insoluble things, like maybe they put a very high probability on something else.", "tokens": [50364, 407, 309, 311, 733, 295, 10585, 281, 2066, 428, 2316, 281, 2674, 1125, 731, 337, 411, 1455, 3006, 21264, 337, 512, 2584, 297, 892, 694, 804, 2638, 295, 2283, 11, 597, 307, 411, 588, 4005, 3097, 7316, 13, 50914, 50914, 400, 294, 613, 3331, 264, 2316, 393, 360, 439, 3685, 295, 3657, 293, 1028, 39754, 638, 721, 11, 411, 1310, 436, 829, 257, 588, 1090, 8482, 322, 746, 1646, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.17037770554826065, "compression_ratio": 1.576036866359447, "no_speech_prob": 2.5463179554208182e-05}, {"id": 398, "seek": 304600, "start": 3046.0, "end": 3056.0, "text": " For a kind of a classic example of this, I don't know if you, I don't have one here, but you may have seen these Limes models get stuck in these feedback loops where they end up just like repeating the same word or phrase infinitely many times.", "tokens": [50364, 1171, 257, 733, 295, 257, 7230, 1365, 295, 341, 11, 286, 500, 380, 458, 498, 291, 11, 286, 500, 380, 362, 472, 510, 11, 457, 291, 815, 362, 1612, 613, 441, 1532, 5245, 483, 5541, 294, 613, 5824, 16121, 689, 436, 917, 493, 445, 411, 18617, 264, 912, 1349, 420, 9535, 36227, 867, 1413, 13, 50864, 50864, 286, 519, 341, 307, 733, 295, 1365, 295, 341, 11, 689, 1564, 264, 2316, 3719, 516, 666, 341, 733, 295, 6367, 11, 550, 309, 1177, 380, 534, 458, 437, 281, 360, 293, 264, 12889, 551, 337, 309, 281, 3985, 322, 884, 307, 445, 1066, 322, 6367, 278, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1726943374773778, "compression_ratio": 1.739463601532567, "no_speech_prob": 1.4736563571204897e-05}, {"id": 399, "seek": 304600, "start": 3056.0, "end": 3069.0, "text": " I think this is kind of example of this, where once the model starts going into this kind of loop, then it doesn't really know what to do and the easiest thing for it to carry on doing is just keep on looping.", "tokens": [50364, 1171, 257, 733, 295, 257, 7230, 1365, 295, 341, 11, 286, 500, 380, 458, 498, 291, 11, 286, 500, 380, 362, 472, 510, 11, 457, 291, 815, 362, 1612, 613, 441, 1532, 5245, 483, 5541, 294, 613, 5824, 16121, 689, 436, 917, 493, 445, 411, 18617, 264, 912, 1349, 420, 9535, 36227, 867, 1413, 13, 50864, 50864, 286, 519, 341, 307, 733, 295, 1365, 295, 341, 11, 689, 1564, 264, 2316, 3719, 516, 666, 341, 733, 295, 6367, 11, 550, 309, 1177, 380, 534, 458, 437, 281, 360, 293, 264, 12889, 551, 337, 309, 281, 3985, 322, 884, 307, 445, 1066, 322, 6367, 278, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1726943374773778, "compression_ratio": 1.739463601532567, "no_speech_prob": 1.4736563571204897e-05}, {"id": 400, "seek": 306900, "start": 3069.0, "end": 3082.0, "text": " So, yeah, I think the issue with beam search is one of not exposing the model to its own mistakes at training time.", "tokens": [50364, 407, 11, 1338, 11, 286, 519, 264, 2734, 365, 14269, 3164, 307, 472, 295, 406, 33178, 264, 2316, 281, 1080, 1065, 8038, 412, 3097, 565, 13, 51014, 51014, 407, 309, 767, 8137, 709, 8482, 322, 439, 3685, 295, 721, 309, 4659, 380, 360, 13, 51314, 51314, 9242, 295, 264, 6322, 3827, 281, 300, 307, 411, 11, 983, 500, 380, 291, 362, 257, 14269, 412, 3097, 565, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.20721011094644037, "compression_ratio": 1.5080213903743316, "no_speech_prob": 6.336935257422738e-06}, {"id": 401, "seek": 306900, "start": 3082.0, "end": 3088.0, "text": " So it actually puts much probability on all kinds of things it shouldn't do.", "tokens": [50364, 407, 11, 1338, 11, 286, 519, 264, 2734, 365, 14269, 3164, 307, 472, 295, 406, 33178, 264, 2316, 281, 1080, 1065, 8038, 412, 3097, 565, 13, 51014, 51014, 407, 309, 767, 8137, 709, 8482, 322, 439, 3685, 295, 721, 309, 4659, 380, 360, 13, 51314, 51314, 9242, 295, 264, 6322, 3827, 281, 300, 307, 411, 11, 983, 500, 380, 291, 362, 257, 14269, 412, 3097, 565, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.20721011094644037, "compression_ratio": 1.5080213903743316, "no_speech_prob": 6.336935257422738e-06}, {"id": 402, "seek": 306900, "start": 3088.0, "end": 3095.0, "text": " Kind of the obvious solution to that is like, why don't you have a beam at training time?", "tokens": [50364, 407, 11, 1338, 11, 286, 519, 264, 2734, 365, 14269, 3164, 307, 472, 295, 406, 33178, 264, 2316, 281, 1080, 1065, 8038, 412, 3097, 565, 13, 51014, 51014, 407, 309, 767, 8137, 709, 8482, 322, 439, 3685, 295, 721, 309, 4659, 380, 360, 13, 51314, 51314, 9242, 295, 264, 6322, 3827, 281, 300, 307, 411, 11, 983, 500, 380, 291, 362, 257, 14269, 412, 3097, 565, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.20721011094644037, "compression_ratio": 1.5080213903743316, "no_speech_prob": 6.336935257422738e-06}, {"id": 403, "seek": 309500, "start": 3095.0, "end": 3102.0, "text": " There's a, and the short answer to that is because it's expensive.", "tokens": [50364, 821, 311, 257, 11, 293, 264, 2099, 1867, 281, 300, 307, 570, 309, 311, 5124, 13, 50714, 50714, 467, 2170, 281, 352, 926, 341, 1379, 38253, 10747, 412, 3097, 565, 11, 457, 700, 309, 2170, 3973, 295, 439, 264, 1481, 8952, 1434, 11, 457, 321, 362, 264, 31782, 293, 309, 4960, 439, 264, 3164, 293, 611, 2709, 291, 867, 544, 721, 281, 6175, 337, 633, 3097, 1365, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.21217241552140978, "compression_ratio": 1.5829145728643217, "no_speech_prob": 8.938535756897181e-06}, {"id": 404, "seek": 309500, "start": 3102.0, "end": 3120.0, "text": " It gets to go around this whole inference procedure at training time, but first it gets rid of all the nice parallelism, but we have the transformer and it uses all the search and also gives you many more things to score for every training example.", "tokens": [50364, 821, 311, 257, 11, 293, 264, 2099, 1867, 281, 300, 307, 570, 309, 311, 5124, 13, 50714, 50714, 467, 2170, 281, 352, 926, 341, 1379, 38253, 10747, 412, 3097, 565, 11, 457, 700, 309, 2170, 3973, 295, 439, 264, 1481, 8952, 1434, 11, 457, 321, 362, 264, 31782, 293, 309, 4960, 439, 264, 3164, 293, 611, 2709, 291, 867, 544, 721, 281, 6175, 337, 633, 3097, 1365, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.21217241552140978, "compression_ratio": 1.5829145728643217, "no_speech_prob": 8.938535756897181e-06}, {"id": 405, "seek": 312000, "start": 3120.0, "end": 3146.0, "text": " So, in practice people tend to just ignore this problem and train a big model for as long as they can, like exploring some nice fast parallelism you get from this autoregressive version, and then a test time people will often tune the size of their beam to get the best performance.", "tokens": [50364, 407, 11, 294, 3124, 561, 3928, 281, 445, 11200, 341, 1154, 293, 3847, 257, 955, 2316, 337, 382, 938, 382, 436, 393, 11, 411, 12736, 512, 1481, 2370, 8952, 1434, 291, 483, 490, 341, 1476, 418, 3091, 488, 3037, 11, 293, 550, 257, 1500, 565, 561, 486, 2049, 10864, 264, 2744, 295, 641, 14269, 281, 483, 264, 1151, 3389, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.2119220793247223, "compression_ratio": 1.5161290322580645, "no_speech_prob": 6.338064395094989e-06}, {"id": 406, "seek": 314600, "start": 3146.0, "end": 3158.0, "text": " I think that translation like increase the beam normally helps with your plays, but basically worse game we started covering these weird degenerate outputs.", "tokens": [50364, 286, 519, 300, 12853, 411, 3488, 264, 14269, 5646, 3665, 365, 428, 5749, 11, 457, 1936, 5324, 1216, 321, 1409, 10322, 613, 3657, 40520, 473, 23930, 13, 50964, 50964, 583, 2086, 11, 445, 264, 2693, 25239, 1840, 551, 561, 362, 281, 360, 13, 51214, 51214, 4919, 11, 300, 311, 1596, 257, 938, 1867, 13, 4402, 300, 1867, 428, 1168, 30, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.3160565197467804, "compression_ratio": 1.453125, "no_speech_prob": 4.468043698580004e-05}, {"id": 407, "seek": 314600, "start": 3158.0, "end": 3163.0, "text": " But yes, just the unsatisfying thing people have to do.", "tokens": [50364, 286, 519, 300, 12853, 411, 3488, 264, 14269, 5646, 3665, 365, 428, 5749, 11, 457, 1936, 5324, 1216, 321, 1409, 10322, 613, 3657, 40520, 473, 23930, 13, 50964, 50964, 583, 2086, 11, 445, 264, 2693, 25239, 1840, 551, 561, 362, 281, 360, 13, 51214, 51214, 4919, 11, 300, 311, 1596, 257, 938, 1867, 13, 4402, 300, 1867, 428, 1168, 30, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.3160565197467804, "compression_ratio": 1.453125, "no_speech_prob": 4.468043698580004e-05}, {"id": 408, "seek": 314600, "start": 3163.0, "end": 3166.0, "text": " Sorry, that's quite a long answer. Does that answer your question?", "tokens": [50364, 286, 519, 300, 12853, 411, 3488, 264, 14269, 5646, 3665, 365, 428, 5749, 11, 457, 1936, 5324, 1216, 321, 1409, 10322, 613, 3657, 40520, 473, 23930, 13, 50964, 50964, 583, 2086, 11, 445, 264, 2693, 25239, 1840, 551, 561, 362, 281, 360, 13, 51214, 51214, 4919, 11, 300, 311, 1596, 257, 938, 1867, 13, 4402, 300, 1867, 428, 1168, 30, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.3160565197467804, "compression_ratio": 1.453125, "no_speech_prob": 4.468043698580004e-05}, {"id": 409, "seek": 316600, "start": 3166.0, "end": 3176.0, "text": " I think the student, I will see now what the student says. Like, that was a question from a student.", "tokens": [50364, 286, 519, 264, 3107, 11, 286, 486, 536, 586, 437, 264, 3107, 1619, 13, 1743, 11, 300, 390, 257, 1168, 490, 257, 3107, 13, 50864, 50864, 400, 456, 307, 1071, 1168, 11, 257, 1359, 472, 322, 341, 2190, 4137, 11, 983, 307, 264, 316, 293, 502, 294, 3092, 30, 51264, 51264, 1282, 264, 558, 1011, 1252, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.20647008301781827, "compression_ratio": 1.46, "no_speech_prob": 0.00013943552039563656}, {"id": 410, "seek": 316600, "start": 3176.0, "end": 3184.0, "text": " And there is another question, a small one on this current slide, why is the A and 1 in green?", "tokens": [50364, 286, 519, 264, 3107, 11, 286, 486, 536, 586, 437, 264, 3107, 1619, 13, 1743, 11, 300, 390, 257, 1168, 490, 257, 3107, 13, 50864, 50864, 400, 456, 307, 1071, 1168, 11, 257, 1359, 472, 322, 341, 2190, 4137, 11, 983, 307, 264, 316, 293, 502, 294, 3092, 30, 51264, 51264, 1282, 264, 558, 1011, 1252, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.20647008301781827, "compression_ratio": 1.46, "no_speech_prob": 0.00013943552039563656}, {"id": 411, "seek": 316600, "start": 3184.0, "end": 3190.0, "text": " On the right hand side.", "tokens": [50364, 286, 519, 264, 3107, 11, 286, 486, 536, 586, 437, 264, 3107, 1619, 13, 1743, 11, 300, 390, 257, 1168, 490, 257, 3107, 13, 50864, 50864, 400, 456, 307, 1071, 1168, 11, 257, 1359, 472, 322, 341, 2190, 4137, 11, 983, 307, 264, 316, 293, 502, 294, 3092, 30, 51264, 51264, 1282, 264, 558, 1011, 1252, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.20647008301781827, "compression_ratio": 1.46, "no_speech_prob": 0.00013943552039563656}, {"id": 412, "seek": 319000, "start": 3190.0, "end": 3198.0, "text": " I don't know. I saw this slide from Abby. I'm not quite sure what points you're trying to make there.", "tokens": [50364, 286, 500, 380, 458, 13, 286, 1866, 341, 4137, 490, 27726, 13, 286, 478, 406, 1596, 988, 437, 2793, 291, 434, 1382, 281, 652, 456, 13, 50764, 50764, 1033, 11, 572, 11, 309, 311, 1392, 13, 51014, 51014, 876, 11, 1392, 11, 264, 935, 767, 307, 512, 1867, 11, 570, 436, 366, 30358, 712, 13, 25148, 295, 264, 472, 291, 1888, 11, 291, 483, 1293, 23930, 11, 3895, 293, 22491, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.3089326477050781, "compression_ratio": 1.4486486486486487, "no_speech_prob": 9.020607831189409e-05}, {"id": 413, "seek": 319000, "start": 3198.0, "end": 3203.0, "text": " Okay, no, it's okay.", "tokens": [50364, 286, 500, 380, 458, 13, 286, 1866, 341, 4137, 490, 27726, 13, 286, 478, 406, 1596, 988, 437, 2793, 291, 434, 1382, 281, 652, 456, 13, 50764, 50764, 1033, 11, 572, 11, 309, 311, 1392, 13, 51014, 51014, 876, 11, 1392, 11, 264, 935, 767, 307, 512, 1867, 11, 570, 436, 366, 30358, 712, 13, 25148, 295, 264, 472, 291, 1888, 11, 291, 483, 1293, 23930, 11, 3895, 293, 22491, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.3089326477050781, "compression_ratio": 1.4486486486486487, "no_speech_prob": 9.020607831189409e-05}, {"id": 414, "seek": 319000, "start": 3203.0, "end": 3218.0, "text": " Oh, okay, the point actually is some answer, because they are interchangeable. Regardless of the one you pick, you get both outputs, pi and tart.", "tokens": [50364, 286, 500, 380, 458, 13, 286, 1866, 341, 4137, 490, 27726, 13, 286, 478, 406, 1596, 988, 437, 2793, 291, 434, 1382, 281, 652, 456, 13, 50764, 50764, 1033, 11, 572, 11, 309, 311, 1392, 13, 51014, 51014, 876, 11, 1392, 11, 264, 935, 767, 307, 512, 1867, 11, 570, 436, 366, 30358, 712, 13, 25148, 295, 264, 472, 291, 1888, 11, 291, 483, 1293, 23930, 11, 3895, 293, 22491, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.3089326477050781, "compression_ratio": 1.4486486486486487, "no_speech_prob": 9.020607831189409e-05}, {"id": 415, "seek": 321800, "start": 3218.0, "end": 3227.0, "text": " Like either or you go for either you go for A or 1, both of them will tell you pi tart or pi tart.", "tokens": [50364, 1743, 2139, 420, 291, 352, 337, 2139, 291, 352, 337, 316, 420, 502, 11, 1293, 295, 552, 486, 980, 291, 3895, 22491, 420, 3895, 22491, 13, 50814, 50814, 286, 478, 406, 988, 13, 50964, 50964, 2754, 498, 291, 645, 281, 360, 300, 11, 550, 291, 393, 380, 14778, 613, 13, 467, 311, 406, 300, 291, 393, 483, 257, 8546, 1461, 689, 291, 393, 15584, 613, 49969, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.24219940078090613, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.00010225241567241028}, {"id": 416, "seek": 321800, "start": 3227.0, "end": 3230.0, "text": " I'm not sure.", "tokens": [50364, 1743, 2139, 420, 291, 352, 337, 2139, 291, 352, 337, 316, 420, 502, 11, 1293, 295, 552, 486, 980, 291, 3895, 22491, 420, 3895, 22491, 13, 50814, 50814, 286, 478, 406, 988, 13, 50964, 50964, 2754, 498, 291, 645, 281, 360, 300, 11, 550, 291, 393, 380, 14778, 613, 13, 467, 311, 406, 300, 291, 393, 483, 257, 8546, 1461, 689, 291, 393, 15584, 613, 49969, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.24219940078090613, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.00010225241567241028}, {"id": 417, "seek": 321800, "start": 3230.0, "end": 3238.0, "text": " Even if you were to do that, then you can't compress these. It's not that you can get a dynamic program where you can collapse these hypotheses.", "tokens": [50364, 1743, 2139, 420, 291, 352, 337, 2139, 291, 352, 337, 316, 420, 502, 11, 1293, 295, 552, 486, 980, 291, 3895, 22491, 420, 3895, 22491, 13, 50814, 50814, 286, 478, 406, 988, 13, 50964, 50964, 2754, 498, 291, 645, 281, 360, 300, 11, 550, 291, 393, 380, 14778, 613, 13, 467, 311, 406, 300, 291, 393, 483, 257, 8546, 1461, 689, 291, 393, 15584, 613, 49969, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.24219940078090613, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.00010225241567241028}, {"id": 418, "seek": 323800, "start": 3238.0, "end": 3248.0, "text": " Because the hidden states for these two hypotheses will still be different depending on which path you took together.", "tokens": [50364, 1436, 264, 7633, 4368, 337, 613, 732, 49969, 486, 920, 312, 819, 5413, 322, 597, 3100, 291, 1890, 1214, 13, 50864, 50864, 407, 286, 500, 380, 458, 13, 10429, 341, 41718, 452, 6314, 3164, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.2536377295469626, "compression_ratio": 1.328358208955224, "no_speech_prob": 2.012807635765057e-05}, {"id": 419, "seek": 323800, "start": 3248.0, "end": 3257.0, "text": " So I don't know. Hopefully this illustrates my theme search.", "tokens": [50364, 1436, 264, 7633, 4368, 337, 613, 732, 49969, 486, 920, 312, 819, 5413, 322, 597, 3100, 291, 1890, 1214, 13, 50864, 50864, 407, 286, 500, 380, 458, 13, 10429, 341, 41718, 452, 6314, 3164, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.2536377295469626, "compression_ratio": 1.328358208955224, "no_speech_prob": 2.012807635765057e-05}, {"id": 420, "seek": 325700, "start": 3257.0, "end": 3268.0, "text": " Okay, any more questions on this?", "tokens": [50364, 1033, 11, 604, 544, 1651, 322, 341, 30, 50914, 50914, 1033, 11, 341, 307, 257, 857, 544, 3855, 295, 577, 264, 9284, 1542, 13, 51164, 51164, 8537, 11, 439, 291, 360, 307, 633, 565, 1823, 291, 8460, 257, 7316, 295, 958, 2283, 337, 1184, 17291, 291, 362, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12418646078843337, "compression_ratio": 1.4066666666666667, "no_speech_prob": 6.848581506346818e-06}, {"id": 421, "seek": 325700, "start": 3268.0, "end": 3273.0, "text": " Okay, this is a bit more description of how the algorithm looks.", "tokens": [50364, 1033, 11, 604, 544, 1651, 322, 341, 30, 50914, 50914, 1033, 11, 341, 307, 257, 857, 544, 3855, 295, 577, 264, 9284, 1542, 13, 51164, 51164, 8537, 11, 439, 291, 360, 307, 633, 565, 1823, 291, 8460, 257, 7316, 295, 958, 2283, 337, 1184, 17291, 291, 362, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12418646078843337, "compression_ratio": 1.4066666666666667, "no_speech_prob": 6.848581506346818e-06}, {"id": 422, "seek": 325700, "start": 3273.0, "end": 3280.0, "text": " Basically, all you do is every time step you generate a distribution of next words for each hypothesis you have.", "tokens": [50364, 1033, 11, 604, 544, 1651, 322, 341, 30, 50914, 50914, 1033, 11, 341, 307, 257, 857, 544, 3855, 295, 577, 264, 9284, 1542, 13, 51164, 51164, 8537, 11, 439, 291, 360, 307, 633, 565, 1823, 291, 8460, 257, 7316, 295, 958, 2283, 337, 1184, 17291, 291, 362, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12418646078843337, "compression_ratio": 1.4066666666666667, "no_speech_prob": 6.848581506346818e-06}, {"id": 423, "seek": 328000, "start": 3280.0, "end": 3297.0, "text": " And then you take the top k hypotheses, next words across all your hypotheses and pull these before you go into the next word.", "tokens": [50364, 400, 550, 291, 747, 264, 1192, 350, 49969, 11, 958, 2283, 2108, 439, 428, 49969, 293, 2235, 613, 949, 291, 352, 666, 264, 958, 1349, 13, 51214, 51214, 2798, 11, 370, 14269, 3164, 307, 2171, 264, 558, 551, 281, 360, 11, 457, 2049, 309, 767, 1943, 380, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.2274908469273494, "compression_ratio": 1.4827586206896552, "no_speech_prob": 6.237999969016528e-06}, {"id": 424, "seek": 328000, "start": 3297.0, "end": 3303.0, "text": " Alright, so beam search is sometimes the right thing to do, but often it actually isn't.", "tokens": [50364, 400, 550, 291, 747, 264, 1192, 350, 49969, 11, 958, 2283, 2108, 439, 428, 49969, 293, 2235, 613, 949, 291, 352, 666, 264, 958, 1349, 13, 51214, 51214, 2798, 11, 370, 14269, 3164, 307, 2171, 264, 558, 551, 281, 360, 11, 457, 2049, 309, 767, 1943, 380, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.2274908469273494, "compression_ratio": 1.4827586206896552, "no_speech_prob": 6.237999969016528e-06}, {"id": 425, "seek": 330300, "start": 3303.0, "end": 3315.0, "text": " So this is a result of applying beam search to the example I showed you before. So this is an example of GPT-2 about scientists finding unicorns in the Andes.", "tokens": [50364, 407, 341, 307, 257, 1874, 295, 9275, 14269, 3164, 281, 264, 1365, 286, 4712, 291, 949, 13, 407, 341, 307, 364, 1365, 295, 26039, 51, 12, 17, 466, 7708, 5006, 28122, 82, 294, 264, 400, 279, 13, 50964, 50964, 509, 393, 536, 510, 300, 264, 2316, 307, 767, 11, 309, 3719, 484, 3372, 512, 665, 1507, 293, 550, 2170, 5541, 294, 341, 3657, 5824, 6367, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.18527795246669224, "compression_ratio": 1.4795918367346939, "no_speech_prob": 9.816983947530389e-06}, {"id": 426, "seek": 330300, "start": 3315.0, "end": 3327.0, "text": " You can see here that the model is actually, it starts out putting some good stuff and then gets stuck in this weird feedback loop.", "tokens": [50364, 407, 341, 307, 257, 1874, 295, 9275, 14269, 3164, 281, 264, 1365, 286, 4712, 291, 949, 13, 407, 341, 307, 364, 1365, 295, 26039, 51, 12, 17, 466, 7708, 5006, 28122, 82, 294, 264, 400, 279, 13, 50964, 50964, 509, 393, 536, 510, 300, 264, 2316, 307, 767, 11, 309, 3719, 484, 3372, 512, 665, 1507, 293, 550, 2170, 5541, 294, 341, 3657, 5824, 6367, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.18527795246669224, "compression_ratio": 1.4795918367346939, "no_speech_prob": 9.816983947530389e-06}, {"id": 427, "seek": 332700, "start": 3327.0, "end": 3337.0, "text": " So I think we're just going to repeat the same phrase over and over and over again. I'll probably just keep on repeating this phrase forever.", "tokens": [50364, 407, 286, 519, 321, 434, 445, 516, 281, 7149, 264, 912, 9535, 670, 293, 670, 293, 670, 797, 13, 286, 603, 1391, 445, 1066, 322, 18617, 341, 9535, 5680, 13, 50864, 50864, 286, 2041, 733, 295, 437, 311, 516, 322, 510, 307, 1564, 291, 848, 341, 9535, 6091, 11, 1310, 445, 1566, 309, 264, 2636, 565, 307, 767, 264, 881, 3700, 551, 281, 360, 13, 51264, 51264, 400, 550, 439, 613, 661, 49969, 483, 588, 1090, 8482, 11, 754, 498, 436, 434, 406, 665, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1338198586796107, "compression_ratio": 1.7117117117117118, "no_speech_prob": 1.36296348500764e-05}, {"id": 428, "seek": 332700, "start": 3337.0, "end": 3345.0, "text": " I guess kind of what's going on here is once you said this phrase twice, maybe just saying it the third time is actually the most likely thing to do.", "tokens": [50364, 407, 286, 519, 321, 434, 445, 516, 281, 7149, 264, 912, 9535, 670, 293, 670, 293, 670, 797, 13, 286, 603, 1391, 445, 1066, 322, 18617, 341, 9535, 5680, 13, 50864, 50864, 286, 2041, 733, 295, 437, 311, 516, 322, 510, 307, 1564, 291, 848, 341, 9535, 6091, 11, 1310, 445, 1566, 309, 264, 2636, 565, 307, 767, 264, 881, 3700, 551, 281, 360, 13, 51264, 51264, 400, 550, 439, 613, 661, 49969, 483, 588, 1090, 8482, 11, 754, 498, 436, 434, 406, 665, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1338198586796107, "compression_ratio": 1.7117117117117118, "no_speech_prob": 1.36296348500764e-05}, {"id": 429, "seek": 332700, "start": 3345.0, "end": 3353.0, "text": " And then all these other hypotheses get very high probability, even if they're not good.", "tokens": [50364, 407, 286, 519, 321, 434, 445, 516, 281, 7149, 264, 912, 9535, 670, 293, 670, 293, 670, 797, 13, 286, 603, 1391, 445, 1066, 322, 18617, 341, 9535, 5680, 13, 50864, 50864, 286, 2041, 733, 295, 437, 311, 516, 322, 510, 307, 1564, 291, 848, 341, 9535, 6091, 11, 1310, 445, 1566, 309, 264, 2636, 565, 307, 767, 264, 881, 3700, 551, 281, 360, 13, 51264, 51264, 400, 550, 439, 613, 661, 49969, 483, 588, 1090, 8482, 11, 754, 498, 436, 434, 406, 665, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1338198586796107, "compression_ratio": 1.7117117117117118, "no_speech_prob": 1.36296348500764e-05}, {"id": 430, "seek": 335300, "start": 3353.0, "end": 3362.0, "text": " But there's also kind of a slightly different problem, which is like maybe in some cases we don't actually want the most likely sequence after all.", "tokens": [50364, 583, 456, 311, 611, 733, 295, 257, 4748, 819, 1154, 11, 597, 307, 411, 1310, 294, 512, 3331, 321, 500, 380, 767, 528, 264, 881, 3700, 8310, 934, 439, 13, 50814, 50814, 2704, 437, 321, 528, 307, 746, 1880, 13, 407, 291, 536, 341, 1154, 611, 257, 688, 294, 721, 411, 10221, 4134, 5125, 13, 51264, 51264, 407, 291, 434, 1382, 281, 1322, 257, 2316, 295, 3761, 365, 1580, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1645768268688305, "compression_ratio": 1.5700934579439252, "no_speech_prob": 1.6185591448447667e-05}, {"id": 431, "seek": 335300, "start": 3362.0, "end": 3371.0, "text": " Maybe what we want is something interesting. So you see this problem also a lot in things like dialogue response generation.", "tokens": [50364, 583, 456, 311, 611, 733, 295, 257, 4748, 819, 1154, 11, 597, 307, 411, 1310, 294, 512, 3331, 321, 500, 380, 767, 528, 264, 881, 3700, 8310, 934, 439, 13, 50814, 50814, 2704, 437, 321, 528, 307, 746, 1880, 13, 407, 291, 536, 341, 1154, 611, 257, 688, 294, 721, 411, 10221, 4134, 5125, 13, 51264, 51264, 407, 291, 434, 1382, 281, 1322, 257, 2316, 295, 3761, 365, 1580, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1645768268688305, "compression_ratio": 1.5700934579439252, "no_speech_prob": 1.6185591448447667e-05}, {"id": 432, "seek": 335300, "start": 3371.0, "end": 3375.0, "text": " So you're trying to build a model of conversation with someone.", "tokens": [50364, 583, 456, 311, 611, 733, 295, 257, 4748, 819, 1154, 11, 597, 307, 411, 1310, 294, 512, 3331, 321, 500, 380, 767, 528, 264, 881, 3700, 8310, 934, 439, 13, 50814, 50814, 2704, 437, 321, 528, 307, 746, 1880, 13, 407, 291, 536, 341, 1154, 611, 257, 688, 294, 721, 411, 10221, 4134, 5125, 13, 51264, 51264, 407, 291, 434, 1382, 281, 1322, 257, 2316, 295, 3761, 365, 1580, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1645768268688305, "compression_ratio": 1.5700934579439252, "no_speech_prob": 1.6185591448447667e-05}, {"id": 433, "seek": 337500, "start": 3375.0, "end": 3383.0, "text": " And if you do this kind of beam search, often what you'll get is it will just give you the most generic response to anything you say.", "tokens": [50364, 400, 498, 291, 360, 341, 733, 295, 14269, 3164, 11, 2049, 437, 291, 603, 483, 307, 309, 486, 445, 976, 291, 264, 881, 19577, 4134, 281, 1340, 291, 584, 13, 50764, 50764, 407, 2035, 291, 584, 11, 309, 603, 312, 411, 11, 1954, 11, 300, 311, 1880, 13, 2561, 13, 51014, 51014, 400, 1310, 300, 767, 17839, 307, 264, 881, 3700, 551, 281, 360, 570, 613, 13019, 366, 665, 294, 881, 6851, 13, 51514, 51514, 2908, 11, 456, 311, 406, 767, 257, 588, 665, 1752, 420, 588, 665, 1185, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12014244972391332, "compression_ratio": 1.6652542372881356, "no_speech_prob": 4.784280918102013e-06}, {"id": 434, "seek": 337500, "start": 3383.0, "end": 3388.0, "text": " So whatever you say, it'll be like, oh, that's interesting. Thanks.", "tokens": [50364, 400, 498, 291, 360, 341, 733, 295, 14269, 3164, 11, 2049, 437, 291, 603, 483, 307, 309, 486, 445, 976, 291, 264, 881, 19577, 4134, 281, 1340, 291, 584, 13, 50764, 50764, 407, 2035, 291, 584, 11, 309, 603, 312, 411, 11, 1954, 11, 300, 311, 1880, 13, 2561, 13, 51014, 51014, 400, 1310, 300, 767, 17839, 307, 264, 881, 3700, 551, 281, 360, 570, 613, 13019, 366, 665, 294, 881, 6851, 13, 51514, 51514, 2908, 11, 456, 311, 406, 767, 257, 588, 665, 1752, 420, 588, 665, 1185, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12014244972391332, "compression_ratio": 1.6652542372881356, "no_speech_prob": 4.784280918102013e-06}, {"id": 435, "seek": 337500, "start": 3388.0, "end": 3398.0, "text": " And maybe that actually genuinely is the most likely thing to do because these responses are good in most situations.", "tokens": [50364, 400, 498, 291, 360, 341, 733, 295, 14269, 3164, 11, 2049, 437, 291, 603, 483, 307, 309, 486, 445, 976, 291, 264, 881, 19577, 4134, 281, 1340, 291, 584, 13, 50764, 50764, 407, 2035, 291, 584, 11, 309, 603, 312, 411, 11, 1954, 11, 300, 311, 1880, 13, 2561, 13, 51014, 51014, 400, 1310, 300, 767, 17839, 307, 264, 881, 3700, 551, 281, 360, 570, 613, 13019, 366, 665, 294, 881, 6851, 13, 51514, 51514, 2908, 11, 456, 311, 406, 767, 257, 588, 665, 1752, 420, 588, 665, 1185, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12014244972391332, "compression_ratio": 1.6652542372881356, "no_speech_prob": 4.784280918102013e-06}, {"id": 436, "seek": 337500, "start": 3398.0, "end": 3404.0, "text": " However, there's not actually a very good experience or very good system.", "tokens": [50364, 400, 498, 291, 360, 341, 733, 295, 14269, 3164, 11, 2049, 437, 291, 603, 483, 307, 309, 486, 445, 976, 291, 264, 881, 19577, 4134, 281, 1340, 291, 584, 13, 50764, 50764, 407, 2035, 291, 584, 11, 309, 603, 312, 411, 11, 1954, 11, 300, 311, 1880, 13, 2561, 13, 51014, 51014, 400, 1310, 300, 767, 17839, 307, 264, 881, 3700, 551, 281, 360, 570, 613, 13019, 366, 665, 294, 881, 6851, 13, 51514, 51514, 2908, 11, 456, 311, 406, 767, 257, 588, 665, 1752, 420, 588, 665, 1185, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12014244972391332, "compression_ratio": 1.6652542372881356, "no_speech_prob": 4.784280918102013e-06}, {"id": 437, "seek": 340400, "start": 3404.0, "end": 3414.0, "text": " But if instead of taking the max, we're going to sample from the model distribution instead.", "tokens": [50364, 583, 498, 2602, 295, 1940, 264, 11469, 11, 321, 434, 516, 281, 6889, 490, 264, 2316, 7316, 2602, 13, 50864, 50864, 407, 341, 307, 3410, 671, 733, 295, 23842, 11, 457, 309, 1177, 380, 767, 976, 291, 588, 665, 5598, 13, 51214, 51214, 407, 341, 307, 797, 264, 1333, 295, 21179, 322, 300, 912, 4846, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20116124153137208, "compression_ratio": 1.525, "no_speech_prob": 3.844764250970911e-06}, {"id": 438, "seek": 340400, "start": 3414.0, "end": 3421.0, "text": " So this is conceptually kind of appealing, but it doesn't actually give you very good output.", "tokens": [50364, 583, 498, 2602, 295, 1940, 264, 11469, 11, 321, 434, 516, 281, 6889, 490, 264, 2316, 7316, 2602, 13, 50864, 50864, 407, 341, 307, 3410, 671, 733, 295, 23842, 11, 457, 309, 1177, 380, 767, 976, 291, 588, 665, 5598, 13, 51214, 51214, 407, 341, 307, 797, 264, 1333, 295, 21179, 322, 300, 912, 4846, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20116124153137208, "compression_ratio": 1.525, "no_speech_prob": 3.844764250970911e-06}, {"id": 439, "seek": 340400, "start": 3421.0, "end": 3431.0, "text": " So this is again the sort of sampling on that same input.", "tokens": [50364, 583, 498, 2602, 295, 1940, 264, 11469, 11, 321, 434, 516, 281, 6889, 490, 264, 2316, 7316, 2602, 13, 50864, 50864, 407, 341, 307, 3410, 671, 733, 295, 23842, 11, 457, 309, 1177, 380, 767, 976, 291, 588, 665, 5598, 13, 51214, 51214, 407, 341, 307, 797, 264, 1333, 295, 21179, 322, 300, 912, 4846, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20116124153137208, "compression_ratio": 1.525, "no_speech_prob": 3.844764250970911e-06}, {"id": 440, "seek": 343100, "start": 3431.0, "end": 3441.0, "text": " So this is kind of good, but then it gets more weird and generic as it goes on.", "tokens": [50364, 407, 341, 307, 733, 295, 665, 11, 457, 550, 309, 2170, 544, 3657, 293, 19577, 382, 309, 1709, 322, 13, 50864, 50864, 400, 797, 11, 291, 483, 281, 300, 1154, 689, 1564, 291, 6889, 264, 1578, 3922, 11, 550, 264, 2316, 311, 294, 1785, 309, 390, 1128, 294, 3097, 13, 51314, 51314, 400, 550, 1564, 264, 2316, 311, 294, 1785, 309, 390, 294, 1830, 3097, 11, 309, 311, 544, 3700, 281, 976, 291, 512, 544, 1578, 5598, 11, 293, 550, 291, 603, 483, 5541, 294, 613, 9263, 5824, 16121, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.20271956666986993, "compression_ratio": 1.7293577981651376, "no_speech_prob": 1.0289072633895557e-05}, {"id": 441, "seek": 343100, "start": 3441.0, "end": 3450.0, "text": " And again, you get to that problem where once you sample the bad choice, then the model's in state it was never in training.", "tokens": [50364, 407, 341, 307, 733, 295, 665, 11, 457, 550, 309, 2170, 544, 3657, 293, 19577, 382, 309, 1709, 322, 13, 50864, 50864, 400, 797, 11, 291, 483, 281, 300, 1154, 689, 1564, 291, 6889, 264, 1578, 3922, 11, 550, 264, 2316, 311, 294, 1785, 309, 390, 1128, 294, 3097, 13, 51314, 51314, 400, 550, 1564, 264, 2316, 311, 294, 1785, 309, 390, 294, 1830, 3097, 11, 309, 311, 544, 3700, 281, 976, 291, 512, 544, 1578, 5598, 11, 293, 550, 291, 603, 483, 5541, 294, 613, 9263, 5824, 16121, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.20271956666986993, "compression_ratio": 1.7293577981651376, "no_speech_prob": 1.0289072633895557e-05}, {"id": 442, "seek": 343100, "start": 3450.0, "end": 3459.0, "text": " And then once the model's in state it was in during training, it's more likely to give you some more bad output, and then you'll get stuck in these horrible feedback loops.", "tokens": [50364, 407, 341, 307, 733, 295, 665, 11, 457, 550, 309, 2170, 544, 3657, 293, 19577, 382, 309, 1709, 322, 13, 50864, 50864, 400, 797, 11, 291, 483, 281, 300, 1154, 689, 1564, 291, 6889, 264, 1578, 3922, 11, 550, 264, 2316, 311, 294, 1785, 309, 390, 1128, 294, 3097, 13, 51314, 51314, 400, 550, 1564, 264, 2316, 311, 294, 1785, 309, 390, 294, 1830, 3097, 11, 309, 311, 544, 3700, 281, 976, 291, 512, 544, 1578, 5598, 11, 293, 550, 291, 603, 483, 5541, 294, 613, 9263, 5824, 16121, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.20271956666986993, "compression_ratio": 1.7293577981651376, "no_speech_prob": 1.0289072633895557e-05}, {"id": 443, "seek": 345900, "start": 3459.0, "end": 3464.0, "text": " Okay.", "tokens": [50364, 1033, 13, 50614, 50614, 407, 510, 311, 746, 300, 767, 775, 589, 11, 264, 6532, 300, 390, 1143, 281, 483, 291, 729, 2238, 23930, 321, 632, 949, 13, 50964, 50964, 8590, 11, 309, 311, 406, 257, 588, 18348, 6532, 11, 457, 445, 281, 976, 291, 257, 30392, 13, 51214, 51214, 407, 341, 307, 1219, 1192, 1389, 21179, 13, 467, 311, 7268, 538, 20848, 18564, 257, 1916, 295, 924, 2057, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.2424733187701251, "compression_ratio": 1.4502369668246446, "no_speech_prob": 1.66995214385679e-05}, {"id": 444, "seek": 345900, "start": 3464.0, "end": 3471.0, "text": " So here's something that actually does work, the technique that was used to get you those beautiful outputs we had before.", "tokens": [50364, 1033, 13, 50614, 50614, 407, 510, 311, 746, 300, 767, 775, 589, 11, 264, 6532, 300, 390, 1143, 281, 483, 291, 729, 2238, 23930, 321, 632, 949, 13, 50964, 50964, 8590, 11, 309, 311, 406, 257, 588, 18348, 6532, 11, 457, 445, 281, 976, 291, 257, 30392, 13, 51214, 51214, 407, 341, 307, 1219, 1192, 1389, 21179, 13, 467, 311, 7268, 538, 20848, 18564, 257, 1916, 295, 924, 2057, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.2424733187701251, "compression_ratio": 1.4502369668246446, "no_speech_prob": 1.66995214385679e-05}, {"id": 445, "seek": 345900, "start": 3471.0, "end": 3476.0, "text": " Unfortunately, it's not a very satisfying technique, but just to give you a disclosure.", "tokens": [50364, 1033, 13, 50614, 50614, 407, 510, 311, 746, 300, 767, 775, 589, 11, 264, 6532, 300, 390, 1143, 281, 483, 291, 729, 2238, 23930, 321, 632, 949, 13, 50964, 50964, 8590, 11, 309, 311, 406, 257, 588, 18348, 6532, 11, 457, 445, 281, 976, 291, 257, 30392, 13, 51214, 51214, 407, 341, 307, 1219, 1192, 1389, 21179, 13, 467, 311, 7268, 538, 20848, 18564, 257, 1916, 295, 924, 2057, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.2424733187701251, "compression_ratio": 1.4502369668246446, "no_speech_prob": 1.66995214385679e-05}, {"id": 446, "seek": 345900, "start": 3476.0, "end": 3484.0, "text": " So this is called top case sampling. It's introduced by Angela Fan a couple of years ago.", "tokens": [50364, 1033, 13, 50614, 50614, 407, 510, 311, 746, 300, 767, 775, 589, 11, 264, 6532, 300, 390, 1143, 281, 483, 291, 729, 2238, 23930, 321, 632, 949, 13, 50964, 50964, 8590, 11, 309, 311, 406, 257, 588, 18348, 6532, 11, 457, 445, 281, 976, 291, 257, 30392, 13, 51214, 51214, 407, 341, 307, 1219, 1192, 1389, 21179, 13, 467, 311, 7268, 538, 20848, 18564, 257, 1916, 295, 924, 2057, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.2424733187701251, "compression_ratio": 1.4502369668246446, "no_speech_prob": 1.66995214385679e-05}, {"id": 447, "seek": 348400, "start": 3484.0, "end": 3494.0, "text": " Basically top case sampling, what we're going to do is truncate a distribution to just taking the k best and then sample from that.", "tokens": [50364, 8537, 1192, 1389, 21179, 11, 437, 321, 434, 516, 281, 360, 307, 504, 409, 66, 473, 257, 7316, 281, 445, 1940, 264, 350, 1151, 293, 550, 6889, 490, 300, 13, 50864, 50864, 407, 341, 307, 364, 5002, 295, 2902, 291, 341, 733, 295, 8811, 11, 411, 10875, 16979, 12918, 665, 3956, 11, 457, 9898, 281, 1590, 505, 733, 295, 51464, 51464, 7440, 766, 341, 733, 295, 47138, 295, 767, 665, 2856, 538, 562, 321, 6889, 746, 1578, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.20379219985589747, "compression_ratio": 1.6143497757847534, "no_speech_prob": 5.421863079391187e-06}, {"id": 448, "seek": 348400, "start": 3494.0, "end": 3506.0, "text": " So this is an advantage of giving you this kind of diversity, like choosing randomly amongst good options, but tries to stop us kind of", "tokens": [50364, 8537, 1192, 1389, 21179, 11, 437, 321, 434, 516, 281, 360, 307, 504, 409, 66, 473, 257, 7316, 281, 445, 1940, 264, 350, 1151, 293, 550, 6889, 490, 300, 13, 50864, 50864, 407, 341, 307, 364, 5002, 295, 2902, 291, 341, 733, 295, 8811, 11, 411, 10875, 16979, 12918, 665, 3956, 11, 457, 9898, 281, 1590, 505, 733, 295, 51464, 51464, 7440, 766, 341, 733, 295, 47138, 295, 767, 665, 2856, 538, 562, 321, 6889, 746, 1578, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.20379219985589747, "compression_ratio": 1.6143497757847534, "no_speech_prob": 5.421863079391187e-06}, {"id": 449, "seek": 348400, "start": 3506.0, "end": 3512.0, "text": " falling off this kind of manifold of actually good language by when we sample something bad.", "tokens": [50364, 8537, 1192, 1389, 21179, 11, 437, 321, 434, 516, 281, 360, 307, 504, 409, 66, 473, 257, 7316, 281, 445, 1940, 264, 350, 1151, 293, 550, 6889, 490, 300, 13, 50864, 50864, 407, 341, 307, 364, 5002, 295, 2902, 291, 341, 733, 295, 8811, 11, 411, 10875, 16979, 12918, 665, 3956, 11, 457, 9898, 281, 1590, 505, 733, 295, 51464, 51464, 7440, 766, 341, 733, 295, 47138, 295, 767, 665, 2856, 538, 562, 321, 6889, 746, 1578, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.20379219985589747, "compression_ratio": 1.6143497757847534, "no_speech_prob": 5.421863079391187e-06}, {"id": 450, "seek": 351200, "start": 3512.0, "end": 3517.0, "text": " So the idea is basically just chop off a long tail and just sample from the head of the distribution.", "tokens": [50364, 407, 264, 1558, 307, 1936, 445, 7931, 766, 257, 938, 6838, 293, 445, 6889, 490, 264, 1378, 295, 264, 7316, 13, 50614, 50614, 400, 341, 307, 264, 21179, 337, 264, 14269, 3164, 11, 307, 309, 30, 50714, 50714, 4919, 11, 341, 1943, 380, 257, 14269, 3164, 11, 341, 307, 445, 5125, 13, 50864, 50864, 407, 321, 434, 516, 281, 485, 51164, 51164, 821, 311, 406, 516, 281, 312, 257, 14269, 510, 11, 341, 307, 516, 281, 312, 472, 17291, 13, 51314, 51314, 286, 2041, 291, 727, 13365, 341, 14269, 3164, 886, 11, 457, 341, 307, 767, 6075, 21179, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.18290456753332637, "compression_ratio": 1.7737556561085972, "no_speech_prob": 1.6696571037755348e-05}, {"id": 451, "seek": 351200, "start": 3517.0, "end": 3519.0, "text": " And this is the sampling for the beam search, is it?", "tokens": [50364, 407, 264, 1558, 307, 1936, 445, 7931, 766, 257, 938, 6838, 293, 445, 6889, 490, 264, 1378, 295, 264, 7316, 13, 50614, 50614, 400, 341, 307, 264, 21179, 337, 264, 14269, 3164, 11, 307, 309, 30, 50714, 50714, 4919, 11, 341, 1943, 380, 257, 14269, 3164, 11, 341, 307, 445, 5125, 13, 50864, 50864, 407, 321, 434, 516, 281, 485, 51164, 51164, 821, 311, 406, 516, 281, 312, 257, 14269, 510, 11, 341, 307, 516, 281, 312, 472, 17291, 13, 51314, 51314, 286, 2041, 291, 727, 13365, 341, 14269, 3164, 886, 11, 457, 341, 307, 767, 6075, 21179, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.18290456753332637, "compression_ratio": 1.7737556561085972, "no_speech_prob": 1.6696571037755348e-05}, {"id": 452, "seek": 351200, "start": 3519.0, "end": 3522.0, "text": " Sorry, this isn't a beam search, this is just generation.", "tokens": [50364, 407, 264, 1558, 307, 1936, 445, 7931, 766, 257, 938, 6838, 293, 445, 6889, 490, 264, 1378, 295, 264, 7316, 13, 50614, 50614, 400, 341, 307, 264, 21179, 337, 264, 14269, 3164, 11, 307, 309, 30, 50714, 50714, 4919, 11, 341, 1943, 380, 257, 14269, 3164, 11, 341, 307, 445, 5125, 13, 50864, 50864, 407, 321, 434, 516, 281, 485, 51164, 51164, 821, 311, 406, 516, 281, 312, 257, 14269, 510, 11, 341, 307, 516, 281, 312, 472, 17291, 13, 51314, 51314, 286, 2041, 291, 727, 13365, 341, 14269, 3164, 886, 11, 457, 341, 307, 767, 6075, 21179, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.18290456753332637, "compression_ratio": 1.7737556561085972, "no_speech_prob": 1.6696571037755348e-05}, {"id": 453, "seek": 351200, "start": 3522.0, "end": 3528.0, "text": " So we're going to...", "tokens": [50364, 407, 264, 1558, 307, 1936, 445, 7931, 766, 257, 938, 6838, 293, 445, 6889, 490, 264, 1378, 295, 264, 7316, 13, 50614, 50614, 400, 341, 307, 264, 21179, 337, 264, 14269, 3164, 11, 307, 309, 30, 50714, 50714, 4919, 11, 341, 1943, 380, 257, 14269, 3164, 11, 341, 307, 445, 5125, 13, 50864, 50864, 407, 321, 434, 516, 281, 485, 51164, 51164, 821, 311, 406, 516, 281, 312, 257, 14269, 510, 11, 341, 307, 516, 281, 312, 472, 17291, 13, 51314, 51314, 286, 2041, 291, 727, 13365, 341, 14269, 3164, 886, 11, 457, 341, 307, 767, 6075, 21179, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.18290456753332637, "compression_ratio": 1.7737556561085972, "no_speech_prob": 1.6696571037755348e-05}, {"id": 454, "seek": 351200, "start": 3528.0, "end": 3531.0, "text": " There's not going to be a beam here, this is going to be one hypothesis.", "tokens": [50364, 407, 264, 1558, 307, 1936, 445, 7931, 766, 257, 938, 6838, 293, 445, 6889, 490, 264, 1378, 295, 264, 7316, 13, 50614, 50614, 400, 341, 307, 264, 21179, 337, 264, 14269, 3164, 11, 307, 309, 30, 50714, 50714, 4919, 11, 341, 1943, 380, 257, 14269, 3164, 11, 341, 307, 445, 5125, 13, 50864, 50864, 407, 321, 434, 516, 281, 485, 51164, 51164, 821, 311, 406, 516, 281, 312, 257, 14269, 510, 11, 341, 307, 516, 281, 312, 472, 17291, 13, 51314, 51314, 286, 2041, 291, 727, 13365, 341, 14269, 3164, 886, 11, 457, 341, 307, 767, 6075, 21179, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.18290456753332637, "compression_ratio": 1.7737556561085972, "no_speech_prob": 1.6696571037755348e-05}, {"id": 455, "seek": 351200, "start": 3531.0, "end": 3537.0, "text": " I guess you could integrate this beam search too, but this is actually pure sampling.", "tokens": [50364, 407, 264, 1558, 307, 1936, 445, 7931, 766, 257, 938, 6838, 293, 445, 6889, 490, 264, 1378, 295, 264, 7316, 13, 50614, 50614, 400, 341, 307, 264, 21179, 337, 264, 14269, 3164, 11, 307, 309, 30, 50714, 50714, 4919, 11, 341, 1943, 380, 257, 14269, 3164, 11, 341, 307, 445, 5125, 13, 50864, 50864, 407, 321, 434, 516, 281, 485, 51164, 51164, 821, 311, 406, 516, 281, 312, 257, 14269, 510, 11, 341, 307, 516, 281, 312, 472, 17291, 13, 51314, 51314, 286, 2041, 291, 727, 13365, 341, 14269, 3164, 886, 11, 457, 341, 307, 767, 6075, 21179, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.18290456753332637, "compression_ratio": 1.7737556561085972, "no_speech_prob": 1.6696571037755348e-05}, {"id": 456, "seek": 353700, "start": 3537.0, "end": 3548.0, "text": " So I can generate a word in this method and use that to generate the next word.", "tokens": [50364, 407, 286, 393, 8460, 257, 1349, 294, 341, 3170, 293, 764, 300, 281, 8460, 264, 958, 1349, 13, 50914, 50914, 407, 562, 291, 360, 439, 300, 11, 550, 341, 307, 2721, 264, 6532, 300, 390, 1143, 281, 8460, 341, 1481, 6889, 13, 51464, 51464, 7580, 11, 341, 1192, 1389, 21179, 307, 257, 857, 295, 257, 10339, 11, 309, 311, 406, 588, 18348, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15803321439828447, "compression_ratio": 1.5562130177514792, "no_speech_prob": 3.446451501076808e-06}, {"id": 457, "seek": 353700, "start": 3548.0, "end": 3559.0, "text": " So when you do all that, then this is finally the technique that was used to generate this nice sample.", "tokens": [50364, 407, 286, 393, 8460, 257, 1349, 294, 341, 3170, 293, 764, 300, 281, 8460, 264, 958, 1349, 13, 50914, 50914, 407, 562, 291, 360, 439, 300, 11, 550, 341, 307, 2721, 264, 6532, 300, 390, 1143, 281, 8460, 341, 1481, 6889, 13, 51464, 51464, 7580, 11, 341, 1192, 1389, 21179, 307, 257, 857, 295, 257, 10339, 11, 309, 311, 406, 588, 18348, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15803321439828447, "compression_ratio": 1.5562130177514792, "no_speech_prob": 3.446451501076808e-06}, {"id": 458, "seek": 353700, "start": 3559.0, "end": 3564.0, "text": " Obviously, this top case sampling is a bit of a hack, it's not very satisfying.", "tokens": [50364, 407, 286, 393, 8460, 257, 1349, 294, 341, 3170, 293, 764, 300, 281, 8460, 264, 958, 1349, 13, 50914, 50914, 407, 562, 291, 360, 439, 300, 11, 550, 341, 307, 2721, 264, 6532, 300, 390, 1143, 281, 8460, 341, 1481, 6889, 13, 51464, 51464, 7580, 11, 341, 1192, 1389, 21179, 307, 257, 857, 295, 257, 10339, 11, 309, 311, 406, 588, 18348, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15803321439828447, "compression_ratio": 1.5562130177514792, "no_speech_prob": 3.446451501076808e-06}, {"id": 459, "seek": 356400, "start": 3564.0, "end": 3574.0, "text": " I was an author on that paper, so I can be mean about the method, but it does seem to work quite well.", "tokens": [50364, 286, 390, 364, 3793, 322, 300, 3035, 11, 370, 286, 393, 312, 914, 466, 264, 3170, 11, 457, 309, 775, 1643, 281, 589, 1596, 731, 13, 50864, 50864, 286, 2041, 472, 551, 281, 312, 3650, 295, 307, 562, 291, 536, 613, 869, 10938, 293, 721, 411, 341, 11, 597, 7238, 48698, 366, 588, 2055, 281, 829, 294, 641, 37264, 11, 51314, 51314, 309, 311, 733, 295, 4420, 281, 458, 577, 309, 390, 767, 1027, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.11504656151880192, "compression_ratio": 1.4876847290640394, "no_speech_prob": 1.7216170817846432e-05}, {"id": 460, "seek": 356400, "start": 3574.0, "end": 3583.0, "text": " I guess one thing to be aware of is when you see these great samples and things like this, which OpenAI are very happy to put in their publicity,", "tokens": [50364, 286, 390, 364, 3793, 322, 300, 3035, 11, 370, 286, 393, 312, 914, 466, 264, 3170, 11, 457, 309, 775, 1643, 281, 589, 1596, 731, 13, 50864, 50864, 286, 2041, 472, 551, 281, 312, 3650, 295, 307, 562, 291, 536, 613, 869, 10938, 293, 721, 411, 341, 11, 597, 7238, 48698, 366, 588, 2055, 281, 829, 294, 641, 37264, 11, 51314, 51314, 309, 311, 733, 295, 4420, 281, 458, 577, 309, 390, 767, 1027, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.11504656151880192, "compression_ratio": 1.4876847290640394, "no_speech_prob": 1.7216170817846432e-05}, {"id": 461, "seek": 356400, "start": 3583.0, "end": 3585.0, "text": " it's kind of useful to know how it was actually made.", "tokens": [50364, 286, 390, 364, 3793, 322, 300, 3035, 11, 370, 286, 393, 312, 914, 466, 264, 3170, 11, 457, 309, 775, 1643, 281, 589, 1596, 731, 13, 50864, 50864, 286, 2041, 472, 551, 281, 312, 3650, 295, 307, 562, 291, 536, 613, 869, 10938, 293, 721, 411, 341, 11, 597, 7238, 48698, 366, 588, 2055, 281, 829, 294, 641, 37264, 11, 51314, 51314, 309, 311, 733, 295, 4420, 281, 458, 577, 309, 390, 767, 1027, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.11504656151880192, "compression_ratio": 1.4876847290640394, "no_speech_prob": 1.7216170817846432e-05}, {"id": 462, "seek": 358500, "start": 3585.0, "end": 3594.0, "text": " This is not like a real sample for model distribution, the model is not putting most of its probability matter on this.", "tokens": [50364, 639, 307, 406, 411, 257, 957, 6889, 337, 2316, 7316, 11, 264, 2316, 307, 406, 3372, 881, 295, 1080, 8482, 1871, 322, 341, 13, 50814, 50814, 639, 307, 746, 300, 311, 10833, 538, 884, 257, 4748, 3657, 38253, 10747, 365, 264, 2316, 13, 51414, 51414, 286, 445, 528, 281, 2661, 2060, 485, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.2099903311048235, "compression_ratio": 1.4588235294117646, "no_speech_prob": 6.045484497008147e-06}, {"id": 463, "seek": 358500, "start": 3594.0, "end": 3606.0, "text": " This is something that's generated by doing a slightly weird inference procedure with the model.", "tokens": [50364, 639, 307, 406, 411, 257, 957, 6889, 337, 2316, 7316, 11, 264, 2316, 307, 406, 3372, 881, 295, 1080, 8482, 1871, 322, 341, 13, 50814, 50814, 639, 307, 746, 300, 311, 10833, 538, 884, 257, 4748, 3657, 38253, 10747, 365, 264, 2316, 13, 51414, 51414, 286, 445, 528, 281, 2661, 2060, 485, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.2099903311048235, "compression_ratio": 1.4588235294117646, "no_speech_prob": 6.045484497008147e-06}, {"id": 464, "seek": 358500, "start": 3606.0, "end": 3608.0, "text": " I just want to quickly cover...", "tokens": [50364, 639, 307, 406, 411, 257, 957, 6889, 337, 2316, 7316, 11, 264, 2316, 307, 406, 3372, 881, 295, 1080, 8482, 1871, 322, 341, 13, 50814, 50814, 639, 307, 746, 300, 311, 10833, 538, 884, 257, 4748, 3657, 38253, 10747, 365, 264, 2316, 13, 51414, 51414, 286, 445, 528, 281, 2661, 2060, 485, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.2099903311048235, "compression_ratio": 1.4588235294117646, "no_speech_prob": 6.045484497008147e-06}, {"id": 465, "seek": 360800, "start": 3608.0, "end": 3616.0, "text": " So give us some text like this, how do we know if it's any good or not? How do you evaluate this?", "tokens": [50364, 407, 976, 505, 512, 2487, 411, 341, 11, 577, 360, 321, 458, 498, 309, 311, 604, 665, 420, 406, 30, 1012, 360, 291, 13059, 341, 30, 50764, 50764, 407, 411, 27479, 257, 2856, 2316, 307, 1596, 1858, 13, 50914, 50914, 286, 914, 11, 2856, 15983, 307, 5633, 10305, 35701, 11, 370, 291, 445, 574, 412, 264, 3565, 22119, 295, 577, 300, 1412, 485, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.2340746352921671, "compression_ratio": 1.535294117647059, "no_speech_prob": 1.012885149975773e-05}, {"id": 466, "seek": 360800, "start": 3616.0, "end": 3619.0, "text": " So like evaluating a language model is quite easy.", "tokens": [50364, 407, 976, 505, 512, 2487, 411, 341, 11, 577, 360, 321, 458, 498, 309, 311, 604, 665, 420, 406, 30, 1012, 360, 291, 13059, 341, 30, 50764, 50764, 407, 411, 27479, 257, 2856, 2316, 307, 1596, 1858, 13, 50914, 50914, 286, 914, 11, 2856, 15983, 307, 5633, 10305, 35701, 11, 370, 291, 445, 574, 412, 264, 3565, 22119, 295, 577, 300, 1412, 485, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.2340746352921671, "compression_ratio": 1.535294117647059, "no_speech_prob": 1.012885149975773e-05}, {"id": 467, "seek": 360800, "start": 3619.0, "end": 3628.0, "text": " I mean, language modeling is task density estimation, so you just look at the log likelihood of how that data...", "tokens": [50364, 407, 976, 505, 512, 2487, 411, 341, 11, 577, 360, 321, 458, 498, 309, 311, 604, 665, 420, 406, 30, 1012, 360, 291, 13059, 341, 30, 50764, 50764, 407, 411, 27479, 257, 2856, 2316, 307, 1596, 1858, 13, 50914, 50914, 286, 914, 11, 2856, 15983, 307, 5633, 10305, 35701, 11, 370, 291, 445, 574, 412, 264, 3565, 22119, 295, 577, 300, 1412, 485, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.2340746352921671, "compression_ratio": 1.535294117647059, "no_speech_prob": 1.012885149975773e-05}, {"id": 468, "seek": 362800, "start": 3628.0, "end": 3640.0, "text": " If you want to do instead, take some text to model and say it's any good or not, then this is not true at all.", "tokens": [50364, 759, 291, 528, 281, 360, 2602, 11, 747, 512, 2487, 281, 2316, 293, 584, 309, 311, 604, 665, 420, 406, 11, 550, 341, 307, 406, 2074, 412, 439, 13, 50964, 50964, 3432, 3928, 281, 764, 613, 3553, 990, 16367, 411, 3344, 293, 40605, 11, 597, 3481, 426, 12, 1342, 670, 875, 3320, 365, 257, 6408, 11, 457, 321, 434, 406, 588, 11239, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.33125125828073987, "compression_ratio": 1.4245810055865922, "no_speech_prob": 4.28873090640991e-06}, {"id": 469, "seek": 362800, "start": 3640.0, "end": 3651.0, "text": " People tend to use these automating metrics like blue and rouge, which measure N-gram overlapped with a reference, but we're not very satisfied.", "tokens": [50364, 759, 291, 528, 281, 360, 2602, 11, 747, 512, 2487, 281, 2316, 293, 584, 309, 311, 604, 665, 420, 406, 11, 550, 341, 307, 406, 2074, 412, 439, 13, 50964, 50964, 3432, 3928, 281, 764, 613, 3553, 990, 16367, 411, 3344, 293, 40605, 11, 597, 3481, 426, 12, 1342, 670, 875, 3320, 365, 257, 6408, 11, 457, 321, 434, 406, 588, 11239, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.33125125828073987, "compression_ratio": 1.4245810055865922, "no_speech_prob": 4.28873090640991e-06}, {"id": 470, "seek": 365100, "start": 3651.0, "end": 3658.0, "text": " This is recent research in trying to do automating metrics.", "tokens": [50364, 639, 307, 5162, 2132, 294, 1382, 281, 360, 3553, 990, 16367, 13, 50714, 50714, 286, 820, 1391, 3073, 493, 257, 857, 13, 50814, 50814, 407, 300, 311, 47916, 2856, 5245, 11, 370, 436, 434, 17746, 10938, 295, 2487, 13, 51214, 51214, 639, 767, 1943, 380, 257, 588, 4420, 551, 281, 360, 13, 708, 311, 709, 544, 4420, 307, 27708, 2856, 5245, 11, 51514, 51514, 370, 5245, 597, 486, 976, 505, 512, 4846, 11, 8460, 764, 295, 5598, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.19308030896070527, "compression_ratio": 1.6394230769230769, "no_speech_prob": 9.513059922028333e-06}, {"id": 471, "seek": 365100, "start": 3658.0, "end": 3660.0, "text": " I should probably speed up a bit.", "tokens": [50364, 639, 307, 5162, 2132, 294, 1382, 281, 360, 3553, 990, 16367, 13, 50714, 50714, 286, 820, 1391, 3073, 493, 257, 857, 13, 50814, 50814, 407, 300, 311, 47916, 2856, 5245, 11, 370, 436, 434, 17746, 10938, 295, 2487, 13, 51214, 51214, 639, 767, 1943, 380, 257, 588, 4420, 551, 281, 360, 13, 708, 311, 709, 544, 4420, 307, 27708, 2856, 5245, 11, 51514, 51514, 370, 5245, 597, 486, 976, 505, 512, 4846, 11, 8460, 764, 295, 5598, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.19308030896070527, "compression_ratio": 1.6394230769230769, "no_speech_prob": 9.513059922028333e-06}, {"id": 472, "seek": 365100, "start": 3660.0, "end": 3668.0, "text": " So that's unconditional language models, so they're generating samples of text.", "tokens": [50364, 639, 307, 5162, 2132, 294, 1382, 281, 360, 3553, 990, 16367, 13, 50714, 50714, 286, 820, 1391, 3073, 493, 257, 857, 13, 50814, 50814, 407, 300, 311, 47916, 2856, 5245, 11, 370, 436, 434, 17746, 10938, 295, 2487, 13, 51214, 51214, 639, 767, 1943, 380, 257, 588, 4420, 551, 281, 360, 13, 708, 311, 709, 544, 4420, 307, 27708, 2856, 5245, 11, 51514, 51514, 370, 5245, 597, 486, 976, 505, 512, 4846, 11, 8460, 764, 295, 5598, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.19308030896070527, "compression_ratio": 1.6394230769230769, "no_speech_prob": 9.513059922028333e-06}, {"id": 473, "seek": 365100, "start": 3668.0, "end": 3674.0, "text": " This actually isn't a very useful thing to do. What's much more useful is conditional language models,", "tokens": [50364, 639, 307, 5162, 2132, 294, 1382, 281, 360, 3553, 990, 16367, 13, 50714, 50714, 286, 820, 1391, 3073, 493, 257, 857, 13, 50814, 50814, 407, 300, 311, 47916, 2856, 5245, 11, 370, 436, 434, 17746, 10938, 295, 2487, 13, 51214, 51214, 639, 767, 1943, 380, 257, 588, 4420, 551, 281, 360, 13, 708, 311, 709, 544, 4420, 307, 27708, 2856, 5245, 11, 51514, 51514, 370, 5245, 597, 486, 976, 505, 512, 4846, 11, 8460, 764, 295, 5598, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.19308030896070527, "compression_ratio": 1.6394230769230769, "no_speech_prob": 9.513059922028333e-06}, {"id": 474, "seek": 365100, "start": 3674.0, "end": 3679.0, "text": " so models which will give us some input, generate use of output.", "tokens": [50364, 639, 307, 5162, 2132, 294, 1382, 281, 360, 3553, 990, 16367, 13, 50714, 50714, 286, 820, 1391, 3073, 493, 257, 857, 13, 50814, 50814, 407, 300, 311, 47916, 2856, 5245, 11, 370, 436, 434, 17746, 10938, 295, 2487, 13, 51214, 51214, 639, 767, 1943, 380, 257, 588, 4420, 551, 281, 360, 13, 708, 311, 709, 544, 4420, 307, 27708, 2856, 5245, 11, 51514, 51514, 370, 5245, 597, 486, 976, 505, 512, 4846, 11, 8460, 764, 295, 5598, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.19308030896070527, "compression_ratio": 1.6394230769230769, "no_speech_prob": 9.513059922028333e-06}, {"id": 475, "seek": 367900, "start": 3679.0, "end": 3691.0, "text": " So for example, you can think about things like given a French sentence translated into English, or given document, generate summary,", "tokens": [50364, 407, 337, 1365, 11, 291, 393, 519, 466, 721, 411, 2212, 257, 5522, 8174, 16805, 666, 3669, 11, 420, 2212, 4166, 11, 8460, 12691, 11, 50964, 50964, 420, 2212, 257, 10221, 11, 8460, 264, 958, 4134, 13, 51214, 51214, 1610, 291, 393, 976, 552, 257, 1168, 293, 5598, 264, 1867, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.2421826275912198, "compression_ratio": 1.48125, "no_speech_prob": 1.7500537069281563e-05}, {"id": 476, "seek": 367900, "start": 3691.0, "end": 3696.0, "text": " or given a dialogue, generate the next response.", "tokens": [50364, 407, 337, 1365, 11, 291, 393, 519, 466, 721, 411, 2212, 257, 5522, 8174, 16805, 666, 3669, 11, 420, 2212, 4166, 11, 8460, 12691, 11, 50964, 50964, 420, 2212, 257, 10221, 11, 8460, 264, 958, 4134, 13, 51214, 51214, 1610, 291, 393, 976, 552, 257, 1168, 293, 5598, 264, 1867, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.2421826275912198, "compression_ratio": 1.48125, "no_speech_prob": 1.7500537069281563e-05}, {"id": 477, "seek": 367900, "start": 3696.0, "end": 3699.0, "text": " Or you can give them a question and output the answer.", "tokens": [50364, 407, 337, 1365, 11, 291, 393, 519, 466, 721, 411, 2212, 257, 5522, 8174, 16805, 666, 3669, 11, 420, 2212, 4166, 11, 8460, 12691, 11, 50964, 50964, 420, 2212, 257, 10221, 11, 8460, 264, 958, 4134, 13, 51214, 51214, 1610, 291, 393, 976, 552, 257, 1168, 293, 5598, 264, 1867, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.2421826275912198, "compression_ratio": 1.48125, "no_speech_prob": 1.7500537069281563e-05}, {"id": 478, "seek": 369900, "start": 3699.0, "end": 3709.0, "text": " So these things are called sequence models, because you get given some input sequence, and then you have to generate some output sequence.", "tokens": [50364, 407, 613, 721, 366, 1219, 8310, 5245, 11, 570, 291, 483, 2212, 512, 4846, 8310, 11, 293, 550, 291, 362, 281, 8460, 512, 5598, 8310, 13, 50864, 50864, 440, 700, 5245, 7268, 538, 286, 45106, 318, 1385, 330, 331, 574, 733, 295, 411, 341, 365, 18680, 1753, 18161, 9590, 11, 51214, 51214, 689, 1936, 291, 1116, 362, 512, 2058, 12340, 3209, 11, 597, 576, 1401, 428, 15743, 11, 5258, 512, 18875, 11, 597, 291, 1116, 1072, 11154, 818, 264, 3461, 8062, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.24562432045160337, "compression_ratio": 1.6916299559471366, "no_speech_prob": 1.300581880059326e-05}, {"id": 479, "seek": 369900, "start": 3709.0, "end": 3716.0, "text": " The first models introduced by Ilya Setskever look kind of like this with recurrent neural networks,", "tokens": [50364, 407, 613, 721, 366, 1219, 8310, 5245, 11, 570, 291, 483, 2212, 512, 4846, 8310, 11, 293, 550, 291, 362, 281, 8460, 512, 5598, 8310, 13, 50864, 50864, 440, 700, 5245, 7268, 538, 286, 45106, 318, 1385, 330, 331, 574, 733, 295, 411, 341, 365, 18680, 1753, 18161, 9590, 11, 51214, 51214, 689, 1936, 291, 1116, 362, 512, 2058, 12340, 3209, 11, 597, 576, 1401, 428, 15743, 11, 5258, 512, 18875, 11, 597, 291, 1116, 1072, 11154, 818, 264, 3461, 8062, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.24562432045160337, "compression_ratio": 1.6916299559471366, "no_speech_prob": 1.300581880059326e-05}, {"id": 480, "seek": 369900, "start": 3716.0, "end": 3725.0, "text": " where basically you'd have some encoded network, which would read your inputs, produce some vectors, which you'd modestly call the block vector.", "tokens": [50364, 407, 613, 721, 366, 1219, 8310, 5245, 11, 570, 291, 483, 2212, 512, 4846, 8310, 11, 293, 550, 291, 362, 281, 8460, 512, 5598, 8310, 13, 50864, 50864, 440, 700, 5245, 7268, 538, 286, 45106, 318, 1385, 330, 331, 574, 733, 295, 411, 341, 365, 18680, 1753, 18161, 9590, 11, 51214, 51214, 689, 1936, 291, 1116, 362, 512, 2058, 12340, 3209, 11, 597, 576, 1401, 428, 15743, 11, 5258, 512, 18875, 11, 597, 291, 1116, 1072, 11154, 818, 264, 3461, 8062, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.24562432045160337, "compression_ratio": 1.6916299559471366, "no_speech_prob": 1.300581880059326e-05}, {"id": 481, "seek": 372500, "start": 3725.0, "end": 3731.0, "text": " And then you'd use this to initialize your decoder, which would generate tokens word by word.", "tokens": [50364, 400, 550, 291, 1116, 764, 341, 281, 5883, 1125, 428, 979, 19866, 11, 597, 576, 8460, 22667, 1349, 538, 1349, 13, 50664, 50664, 3764, 11, 4696, 291, 483, 428, 6314, 510, 11, 300, 9792, 613, 733, 295, 44641, 2761, 293, 18680, 1753, 82, 307, 406, 257, 665, 551, 281, 360, 13, 50914, 50914, 509, 611, 362, 5109, 5245, 11, 597, 291, 1062, 536, 1203, 13, 51164, 51164, 407, 456, 311, 257, 12990, 295, 31782, 337, 8310, 5245, 13, 51414, 51414, 1692, 321, 434, 516, 281, 362, 732, 30792, 11, 364, 2058, 19866, 8630, 293, 979, 19866, 8630, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.15319777470009, "compression_ratio": 1.6245059288537549, "no_speech_prob": 2.561197106842883e-06}, {"id": 482, "seek": 372500, "start": 3731.0, "end": 3736.0, "text": " Again, hopefully you get your theme here, that carrying these kind of bottlenecks and recurrents is not a good thing to do.", "tokens": [50364, 400, 550, 291, 1116, 764, 341, 281, 5883, 1125, 428, 979, 19866, 11, 597, 576, 8460, 22667, 1349, 538, 1349, 13, 50664, 50664, 3764, 11, 4696, 291, 483, 428, 6314, 510, 11, 300, 9792, 613, 733, 295, 44641, 2761, 293, 18680, 1753, 82, 307, 406, 257, 665, 551, 281, 360, 13, 50914, 50914, 509, 611, 362, 5109, 5245, 11, 597, 291, 1062, 536, 1203, 13, 51164, 51164, 407, 456, 311, 257, 12990, 295, 31782, 337, 8310, 5245, 13, 51414, 51414, 1692, 321, 434, 516, 281, 362, 732, 30792, 11, 364, 2058, 19866, 8630, 293, 979, 19866, 8630, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.15319777470009, "compression_ratio": 1.6245059288537549, "no_speech_prob": 2.561197106842883e-06}, {"id": 483, "seek": 372500, "start": 3736.0, "end": 3741.0, "text": " You also have express models, which you might see everything.", "tokens": [50364, 400, 550, 291, 1116, 764, 341, 281, 5883, 1125, 428, 979, 19866, 11, 597, 576, 8460, 22667, 1349, 538, 1349, 13, 50664, 50664, 3764, 11, 4696, 291, 483, 428, 6314, 510, 11, 300, 9792, 613, 733, 295, 44641, 2761, 293, 18680, 1753, 82, 307, 406, 257, 665, 551, 281, 360, 13, 50914, 50914, 509, 611, 362, 5109, 5245, 11, 597, 291, 1062, 536, 1203, 13, 51164, 51164, 407, 456, 311, 257, 12990, 295, 31782, 337, 8310, 5245, 13, 51414, 51414, 1692, 321, 434, 516, 281, 362, 732, 30792, 11, 364, 2058, 19866, 8630, 293, 979, 19866, 8630, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.15319777470009, "compression_ratio": 1.6245059288537549, "no_speech_prob": 2.561197106842883e-06}, {"id": 484, "seek": 372500, "start": 3741.0, "end": 3746.0, "text": " So there's a variation of transformer for sequence models.", "tokens": [50364, 400, 550, 291, 1116, 764, 341, 281, 5883, 1125, 428, 979, 19866, 11, 597, 576, 8460, 22667, 1349, 538, 1349, 13, 50664, 50664, 3764, 11, 4696, 291, 483, 428, 6314, 510, 11, 300, 9792, 613, 733, 295, 44641, 2761, 293, 18680, 1753, 82, 307, 406, 257, 665, 551, 281, 360, 13, 50914, 50914, 509, 611, 362, 5109, 5245, 11, 597, 291, 1062, 536, 1203, 13, 51164, 51164, 407, 456, 311, 257, 12990, 295, 31782, 337, 8310, 5245, 13, 51414, 51414, 1692, 321, 434, 516, 281, 362, 732, 30792, 11, 364, 2058, 19866, 8630, 293, 979, 19866, 8630, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.15319777470009, "compression_ratio": 1.6245059288537549, "no_speech_prob": 2.561197106842883e-06}, {"id": 485, "seek": 372500, "start": 3746.0, "end": 3751.0, "text": " Here we're going to have two stacks, an encoder stack and decoder stack.", "tokens": [50364, 400, 550, 291, 1116, 764, 341, 281, 5883, 1125, 428, 979, 19866, 11, 597, 576, 8460, 22667, 1349, 538, 1349, 13, 50664, 50664, 3764, 11, 4696, 291, 483, 428, 6314, 510, 11, 300, 9792, 613, 733, 295, 44641, 2761, 293, 18680, 1753, 82, 307, 406, 257, 665, 551, 281, 360, 13, 50914, 50914, 509, 611, 362, 5109, 5245, 11, 597, 291, 1062, 536, 1203, 13, 51164, 51164, 407, 456, 311, 257, 12990, 295, 31782, 337, 8310, 5245, 13, 51414, 51414, 1692, 321, 434, 516, 281, 362, 732, 30792, 11, 364, 2058, 19866, 8630, 293, 979, 19866, 8630, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.15319777470009, "compression_ratio": 1.6245059288537549, "no_speech_prob": 2.561197106842883e-06}, {"id": 486, "seek": 375100, "start": 3751.0, "end": 3757.0, "text": " Basically, the encoder stack is the same as what I showed you before, apart from self-attention isn't masked.", "tokens": [50364, 8537, 11, 264, 2058, 19866, 8630, 307, 264, 912, 382, 437, 286, 4712, 291, 949, 11, 4936, 490, 2698, 12, 1591, 1251, 1943, 380, 45249, 13, 50664, 50664, 407, 633, 14862, 11, 264, 4846, 393, 574, 412, 633, 661, 14862, 294, 264, 15743, 13, 50964, 50964, 440, 979, 19866, 8630, 486, 312, 2531, 11, 3993, 300, 382, 731, 382, 884, 2698, 12, 1591, 1251, 670, 2564, 11, 51264, 51264, 309, 311, 611, 516, 281, 360, 3202, 670, 264, 3566, 15743, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11676191442153033, "compression_ratio": 1.683673469387755, "no_speech_prob": 6.438611762860091e-06}, {"id": 487, "seek": 375100, "start": 3757.0, "end": 3763.0, "text": " So every token, the input can look at every other token in the inputs.", "tokens": [50364, 8537, 11, 264, 2058, 19866, 8630, 307, 264, 912, 382, 437, 286, 4712, 291, 949, 11, 4936, 490, 2698, 12, 1591, 1251, 1943, 380, 45249, 13, 50664, 50664, 407, 633, 14862, 11, 264, 4846, 393, 574, 412, 633, 661, 14862, 294, 264, 15743, 13, 50964, 50964, 440, 979, 19866, 8630, 486, 312, 2531, 11, 3993, 300, 382, 731, 382, 884, 2698, 12, 1591, 1251, 670, 2564, 11, 51264, 51264, 309, 311, 611, 516, 281, 360, 3202, 670, 264, 3566, 15743, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11676191442153033, "compression_ratio": 1.683673469387755, "no_speech_prob": 6.438611762860091e-06}, {"id": 488, "seek": 375100, "start": 3763.0, "end": 3769.0, "text": " The decoder stack will be similar, except that as well as doing self-attention over itself,", "tokens": [50364, 8537, 11, 264, 2058, 19866, 8630, 307, 264, 912, 382, 437, 286, 4712, 291, 949, 11, 4936, 490, 2698, 12, 1591, 1251, 1943, 380, 45249, 13, 50664, 50664, 407, 633, 14862, 11, 264, 4846, 393, 574, 412, 633, 661, 14862, 294, 264, 15743, 13, 50964, 50964, 440, 979, 19866, 8630, 486, 312, 2531, 11, 3993, 300, 382, 731, 382, 884, 2698, 12, 1591, 1251, 670, 2564, 11, 51264, 51264, 309, 311, 611, 516, 281, 360, 3202, 670, 264, 3566, 15743, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11676191442153033, "compression_ratio": 1.683673469387755, "no_speech_prob": 6.438611762860091e-06}, {"id": 489, "seek": 375100, "start": 3769.0, "end": 3779.0, "text": " it's also going to do attention over the complete inputs.", "tokens": [50364, 8537, 11, 264, 2058, 19866, 8630, 307, 264, 912, 382, 437, 286, 4712, 291, 949, 11, 4936, 490, 2698, 12, 1591, 1251, 1943, 380, 45249, 13, 50664, 50664, 407, 633, 14862, 11, 264, 4846, 393, 574, 412, 633, 661, 14862, 294, 264, 15743, 13, 50964, 50964, 440, 979, 19866, 8630, 486, 312, 2531, 11, 3993, 300, 382, 731, 382, 884, 2698, 12, 1591, 1251, 670, 2564, 11, 51264, 51264, 309, 311, 611, 516, 281, 360, 3202, 670, 264, 3566, 15743, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11676191442153033, "compression_ratio": 1.683673469387755, "no_speech_prob": 6.438611762860091e-06}, {"id": 490, "seek": 377900, "start": 3779.0, "end": 3791.0, "text": " So this means that every token in the output has direct connection to every previous token in the output and also to every word in the inputs,", "tokens": [50364, 407, 341, 1355, 300, 633, 14862, 294, 264, 5598, 575, 2047, 4984, 281, 633, 3894, 14862, 294, 264, 5598, 293, 611, 281, 633, 1349, 294, 264, 15743, 11, 50964, 50964, 597, 1669, 613, 5245, 588, 40189, 293, 4005, 13, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.12172700083532999, "compression_ratio": 1.5390625, "no_speech_prob": 8.939073268265929e-06}, {"id": 491, "seek": 377900, "start": 3791.0, "end": 3796.0, "text": " which makes these models very expressive and powerful.", "tokens": [50364, 407, 341, 1355, 300, 633, 14862, 294, 264, 5598, 575, 2047, 4984, 281, 633, 3894, 14862, 294, 264, 5598, 293, 611, 281, 633, 1349, 294, 264, 15743, 11, 50964, 50964, 597, 1669, 613, 5245, 588, 40189, 293, 4005, 13, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.12172700083532999, "compression_ratio": 1.5390625, "no_speech_prob": 8.939073268265929e-06}, {"id": 492, "seek": 379600, "start": 3796.0, "end": 3810.0, "text": " When transformers were introduced, they got quite a nice improvement on translation scores over the previous recurrent convolutional models.", "tokens": [50364, 1133, 4088, 433, 645, 7268, 11, 436, 658, 1596, 257, 1481, 10444, 322, 12853, 13444, 670, 264, 3894, 18680, 1753, 45216, 304, 5245, 13, 51064, 51064, 407, 437, 321, 3847, 613, 5245, 11, 5850, 437, 321, 360, 307, 321, 10687, 322, 7645, 2487, 13, 51364, 51364, 407, 445, 853, 257, 12853, 1185, 11, 337, 1365, 11, 291, 853, 293, 483, 3195, 295, 16945, 16805, 2487, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.21135356085641044, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.4283909877121914e-05}, {"id": 493, "seek": 379600, "start": 3810.0, "end": 3816.0, "text": " So what we train these models, typically what we do is we rely on label text.", "tokens": [50364, 1133, 4088, 433, 645, 7268, 11, 436, 658, 1596, 257, 1481, 10444, 322, 12853, 13444, 670, 264, 3894, 18680, 1753, 45216, 304, 5245, 13, 51064, 51064, 407, 437, 321, 3847, 613, 5245, 11, 5850, 437, 321, 360, 307, 321, 10687, 322, 7645, 2487, 13, 51364, 51364, 407, 445, 853, 257, 12853, 1185, 11, 337, 1365, 11, 291, 853, 293, 483, 3195, 295, 16945, 16805, 2487, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.21135356085641044, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.4283909877121914e-05}, {"id": 494, "seek": 379600, "start": 3816.0, "end": 3822.0, "text": " So just try a translation system, for example, you try and get lots of manually translated text.", "tokens": [50364, 1133, 4088, 433, 645, 7268, 11, 436, 658, 1596, 257, 1481, 10444, 322, 12853, 13444, 670, 264, 3894, 18680, 1753, 45216, 304, 5245, 13, 51064, 51064, 407, 437, 321, 3847, 613, 5245, 11, 5850, 437, 321, 360, 307, 321, 10687, 322, 7645, 2487, 13, 51364, 51364, 407, 445, 853, 257, 12853, 1185, 11, 337, 1365, 11, 291, 853, 293, 483, 3195, 295, 16945, 16805, 2487, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.21135356085641044, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.4283909877121914e-05}, {"id": 495, "seek": 382200, "start": 3822.0, "end": 3826.0, "text": " It turns out one of the best ways to get this is things like parliament proceedings,", "tokens": [50364, 467, 4523, 484, 472, 295, 264, 1151, 2098, 281, 483, 341, 307, 721, 411, 19520, 37254, 11, 50564, 50564, 570, 436, 1009, 13799, 264, 6473, 15538, 420, 264, 1472, 295, 264, 37254, 294, 3195, 295, 819, 8650, 13, 50964, 50964, 400, 550, 291, 445, 764, 729, 382, 15743, 293, 23930, 13, 51164, 51164, 2908, 11, 406, 439, 8650, 366, 10379, 294, 264, 6473, 15538, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.16122483516084976, "compression_ratio": 1.6822916666666667, "no_speech_prob": 0.00012140852777520195}, {"id": 496, "seek": 382200, "start": 3826.0, "end": 3834.0, "text": " because they always translate the European Parliament or the rest of the proceedings in lots of different languages.", "tokens": [50364, 467, 4523, 484, 472, 295, 264, 1151, 2098, 281, 483, 341, 307, 721, 411, 19520, 37254, 11, 50564, 50564, 570, 436, 1009, 13799, 264, 6473, 15538, 420, 264, 1472, 295, 264, 37254, 294, 3195, 295, 819, 8650, 13, 50964, 50964, 400, 550, 291, 445, 764, 729, 382, 15743, 293, 23930, 13, 51164, 51164, 2908, 11, 406, 439, 8650, 366, 10379, 294, 264, 6473, 15538, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.16122483516084976, "compression_ratio": 1.6822916666666667, "no_speech_prob": 0.00012140852777520195}, {"id": 497, "seek": 382200, "start": 3834.0, "end": 3838.0, "text": " And then you just use those as inputs and outputs.", "tokens": [50364, 467, 4523, 484, 472, 295, 264, 1151, 2098, 281, 483, 341, 307, 721, 411, 19520, 37254, 11, 50564, 50564, 570, 436, 1009, 13799, 264, 6473, 15538, 420, 264, 1472, 295, 264, 37254, 294, 3195, 295, 819, 8650, 13, 50964, 50964, 400, 550, 291, 445, 764, 729, 382, 15743, 293, 23930, 13, 51164, 51164, 2908, 11, 406, 439, 8650, 366, 10379, 294, 264, 6473, 15538, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.16122483516084976, "compression_ratio": 1.6822916666666667, "no_speech_prob": 0.00012140852777520195}, {"id": 498, "seek": 382200, "start": 3838.0, "end": 3846.0, "text": " However, not all languages are represented in the European Parliament.", "tokens": [50364, 467, 4523, 484, 472, 295, 264, 1151, 2098, 281, 483, 341, 307, 721, 411, 19520, 37254, 11, 50564, 50564, 570, 436, 1009, 13799, 264, 6473, 15538, 420, 264, 1472, 295, 264, 37254, 294, 3195, 295, 819, 8650, 13, 50964, 50964, 400, 550, 291, 445, 764, 729, 382, 15743, 293, 23930, 13, 51164, 51164, 2908, 11, 406, 439, 8650, 366, 10379, 294, 264, 6473, 15538, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.16122483516084976, "compression_ratio": 1.6822916666666667, "no_speech_prob": 0.00012140852777520195}, {"id": 499, "seek": 384600, "start": 3846.0, "end": 3852.0, "text": " These transformers are very data hungry and more text we can throw at them, the best they will do.", "tokens": [50364, 1981, 4088, 433, 366, 588, 1412, 8067, 293, 544, 2487, 321, 393, 3507, 412, 552, 11, 264, 1151, 436, 486, 360, 13, 50664, 50664, 407, 341, 1168, 295, 577, 321, 764, 2316, 7645, 2487, 281, 3470, 613, 13, 50964, 50964, 407, 341, 307, 767, 1566, 11, 393, 321, 1466, 1553, 445, 1419, 4846, 12, 346, 2582, 15494, 30, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.24059801716958323, "compression_ratio": 1.467065868263473, "no_speech_prob": 2.428875450277701e-05}, {"id": 500, "seek": 384600, "start": 3852.0, "end": 3858.0, "text": " So this question of how we use model label text to improve these.", "tokens": [50364, 1981, 4088, 433, 366, 588, 1412, 8067, 293, 544, 2487, 321, 393, 3507, 412, 552, 11, 264, 1151, 436, 486, 360, 13, 50664, 50664, 407, 341, 1168, 295, 577, 321, 764, 2316, 7645, 2487, 281, 3470, 613, 13, 50964, 50964, 407, 341, 307, 767, 1566, 11, 393, 321, 1466, 1553, 445, 1419, 4846, 12, 346, 2582, 15494, 30, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.24059801716958323, "compression_ratio": 1.467065868263473, "no_speech_prob": 2.428875450277701e-05}, {"id": 501, "seek": 384600, "start": 3858.0, "end": 3868.0, "text": " So this is actually saying, can we learn without just having input-output pairs?", "tokens": [50364, 1981, 4088, 433, 366, 588, 1412, 8067, 293, 544, 2487, 321, 393, 3507, 412, 552, 11, 264, 1151, 436, 486, 360, 13, 50664, 50664, 407, 341, 1168, 295, 577, 321, 764, 2316, 7645, 2487, 281, 3470, 613, 13, 50964, 50964, 407, 341, 307, 767, 1566, 11, 393, 321, 1466, 1553, 445, 1419, 4846, 12, 346, 2582, 15494, 30, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.24059801716958323, "compression_ratio": 1.467065868263473, "no_speech_prob": 2.428875450277701e-05}, {"id": 502, "seek": 386800, "start": 3868.0, "end": 3876.0, "text": " The way we can do this is a technique called back translation, which is quite simple conceptually.", "tokens": [50364, 440, 636, 321, 393, 360, 341, 307, 257, 6532, 1219, 646, 12853, 11, 597, 307, 1596, 2199, 3410, 671, 13, 50764, 50764, 663, 311, 577, 286, 658, 8895, 12853, 1185, 300, 15743, 6521, 293, 486, 5598, 3669, 13, 51114, 51114, 2386, 295, 439, 11, 321, 434, 516, 281, 767, 360, 264, 6182, 13, 51214, 51214, 492, 434, 516, 281, 3847, 257, 9943, 12853, 2316, 11, 597, 486, 976, 364, 3669, 5598, 6521, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.17869847780698306, "compression_ratio": 1.6323529411764706, "no_speech_prob": 6.70580193400383e-05}, {"id": 503, "seek": 386800, "start": 3876.0, "end": 3883.0, "text": " That's how I got trained translation system that inputs German and will output English.", "tokens": [50364, 440, 636, 321, 393, 360, 341, 307, 257, 6532, 1219, 646, 12853, 11, 597, 307, 1596, 2199, 3410, 671, 13, 50764, 50764, 663, 311, 577, 286, 658, 8895, 12853, 1185, 300, 15743, 6521, 293, 486, 5598, 3669, 13, 51114, 51114, 2386, 295, 439, 11, 321, 434, 516, 281, 767, 360, 264, 6182, 13, 51214, 51214, 492, 434, 516, 281, 3847, 257, 9943, 12853, 2316, 11, 597, 486, 976, 364, 3669, 5598, 6521, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.17869847780698306, "compression_ratio": 1.6323529411764706, "no_speech_prob": 6.70580193400383e-05}, {"id": 504, "seek": 386800, "start": 3883.0, "end": 3885.0, "text": " First of all, we're going to actually do the opposite.", "tokens": [50364, 440, 636, 321, 393, 360, 341, 307, 257, 6532, 1219, 646, 12853, 11, 597, 307, 1596, 2199, 3410, 671, 13, 50764, 50764, 663, 311, 577, 286, 658, 8895, 12853, 1185, 300, 15743, 6521, 293, 486, 5598, 3669, 13, 51114, 51114, 2386, 295, 439, 11, 321, 434, 516, 281, 767, 360, 264, 6182, 13, 51214, 51214, 492, 434, 516, 281, 3847, 257, 9943, 12853, 2316, 11, 597, 486, 976, 364, 3669, 5598, 6521, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.17869847780698306, "compression_ratio": 1.6323529411764706, "no_speech_prob": 6.70580193400383e-05}, {"id": 505, "seek": 386800, "start": 3885.0, "end": 3894.0, "text": " We're going to train a reverse translation model, which will give an English output German.", "tokens": [50364, 440, 636, 321, 393, 360, 341, 307, 257, 6532, 1219, 646, 12853, 11, 597, 307, 1596, 2199, 3410, 671, 13, 50764, 50764, 663, 311, 577, 286, 658, 8895, 12853, 1185, 300, 15743, 6521, 293, 486, 5598, 3669, 13, 51114, 51114, 2386, 295, 439, 11, 321, 434, 516, 281, 767, 360, 264, 6182, 13, 51214, 51214, 492, 434, 516, 281, 3847, 257, 9943, 12853, 2316, 11, 597, 486, 976, 364, 3669, 5598, 6521, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.17869847780698306, "compression_ratio": 1.6323529411764706, "no_speech_prob": 6.70580193400383e-05}, {"id": 506, "seek": 389400, "start": 3894.0, "end": 3899.0, "text": " Then we can run this model over all the English text we can find.", "tokens": [50364, 1396, 321, 393, 1190, 341, 2316, 670, 439, 264, 3669, 2487, 321, 393, 915, 13, 50614, 50614, 400, 321, 393, 915, 3195, 295, 3669, 2487, 322, 264, 7703, 293, 321, 603, 13799, 309, 439, 382, 6521, 13, 51064, 51064, 400, 300, 2709, 505, 3195, 544, 15494, 295, 3669, 293, 6521, 2487, 13, 51314, 51314, 400, 550, 321, 434, 516, 281, 3847, 257, 2128, 2316, 300, 486, 853, 293, 13799, 341, 6521, 666, 3669, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11605898539225261, "compression_ratio": 1.8381502890173411, "no_speech_prob": 3.424087481107563e-05}, {"id": 507, "seek": 389400, "start": 3899.0, "end": 3908.0, "text": " And we can find lots of English text on the Internet and we'll translate it all as German.", "tokens": [50364, 1396, 321, 393, 1190, 341, 2316, 670, 439, 264, 3669, 2487, 321, 393, 915, 13, 50614, 50614, 400, 321, 393, 915, 3195, 295, 3669, 2487, 322, 264, 7703, 293, 321, 603, 13799, 309, 439, 382, 6521, 13, 51064, 51064, 400, 300, 2709, 505, 3195, 544, 15494, 295, 3669, 293, 6521, 2487, 13, 51314, 51314, 400, 550, 321, 434, 516, 281, 3847, 257, 2128, 2316, 300, 486, 853, 293, 13799, 341, 6521, 666, 3669, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11605898539225261, "compression_ratio": 1.8381502890173411, "no_speech_prob": 3.424087481107563e-05}, {"id": 508, "seek": 389400, "start": 3908.0, "end": 3913.0, "text": " And that gives us lots more pairs of English and German text.", "tokens": [50364, 1396, 321, 393, 1190, 341, 2316, 670, 439, 264, 3669, 2487, 321, 393, 915, 13, 50614, 50614, 400, 321, 393, 915, 3195, 295, 3669, 2487, 322, 264, 7703, 293, 321, 603, 13799, 309, 439, 382, 6521, 13, 51064, 51064, 400, 300, 2709, 505, 3195, 544, 15494, 295, 3669, 293, 6521, 2487, 13, 51314, 51314, 400, 550, 321, 434, 516, 281, 3847, 257, 2128, 2316, 300, 486, 853, 293, 13799, 341, 6521, 666, 3669, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11605898539225261, "compression_ratio": 1.8381502890173411, "no_speech_prob": 3.424087481107563e-05}, {"id": 509, "seek": 389400, "start": 3913.0, "end": 3920.0, "text": " And then we're going to train a forward model that will try and translate this German into English.", "tokens": [50364, 1396, 321, 393, 1190, 341, 2316, 670, 439, 264, 3669, 2487, 321, 393, 915, 13, 50614, 50614, 400, 321, 393, 915, 3195, 295, 3669, 2487, 322, 264, 7703, 293, 321, 603, 13799, 309, 439, 382, 6521, 13, 51064, 51064, 400, 300, 2709, 505, 3195, 544, 15494, 295, 3669, 293, 6521, 2487, 13, 51314, 51314, 400, 550, 321, 434, 516, 281, 3847, 257, 2128, 2316, 300, 486, 853, 293, 13799, 341, 6521, 666, 3669, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11605898539225261, "compression_ratio": 1.8381502890173411, "no_speech_prob": 3.424087481107563e-05}, {"id": 510, "seek": 392000, "start": 3920.0, "end": 3929.0, "text": " The nice thing to see here is that it doesn't actually matter how good the initial model is.", "tokens": [50364, 440, 1481, 551, 281, 536, 510, 307, 300, 309, 1177, 380, 767, 1871, 577, 665, 264, 5883, 2316, 307, 13, 50814, 50814, 467, 1177, 380, 1871, 498, 428, 9943, 2316, 307, 1455, 8038, 13, 51064, 51064, 407, 498, 428, 9943, 2316, 1669, 8038, 11, 550, 428, 2572, 3847, 1412, 486, 5304, 733, 295, 24518, 6521, 16805, 281, 2541, 3669, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1147991344332695, "compression_ratio": 1.6358381502890174, "no_speech_prob": 4.68118378194049e-05}, {"id": 511, "seek": 392000, "start": 3929.0, "end": 3934.0, "text": " It doesn't matter if your reverse model is making mistakes.", "tokens": [50364, 440, 1481, 551, 281, 536, 510, 307, 300, 309, 1177, 380, 767, 1871, 577, 665, 264, 5883, 2316, 307, 13, 50814, 50814, 467, 1177, 380, 1871, 498, 428, 9943, 2316, 307, 1455, 8038, 13, 51064, 51064, 407, 498, 428, 9943, 2316, 1669, 8038, 11, 550, 428, 2572, 3847, 1412, 486, 5304, 733, 295, 24518, 6521, 16805, 281, 2541, 3669, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1147991344332695, "compression_ratio": 1.6358381502890174, "no_speech_prob": 4.68118378194049e-05}, {"id": 512, "seek": 392000, "start": 3934.0, "end": 3942.0, "text": " So if your reverse model makes mistakes, then your final train data will contain kind of noisy German translated to clean English,", "tokens": [50364, 440, 1481, 551, 281, 536, 510, 307, 300, 309, 1177, 380, 767, 1871, 577, 665, 264, 5883, 2316, 307, 13, 50814, 50814, 467, 1177, 380, 1871, 498, 428, 9943, 2316, 307, 1455, 8038, 13, 51064, 51064, 407, 498, 428, 9943, 2316, 1669, 8038, 11, 550, 428, 2572, 3847, 1412, 486, 5304, 733, 295, 24518, 6521, 16805, 281, 2541, 3669, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1147991344332695, "compression_ratio": 1.6358381502890174, "no_speech_prob": 4.68118378194049e-05}, {"id": 513, "seek": 394200, "start": 3942.0, "end": 3951.0, "text": " which might even help regularize your model, but probably shouldn't pass its performance when you show it clean German data.", "tokens": [50364, 597, 1062, 754, 854, 3890, 1125, 428, 2316, 11, 457, 1391, 4659, 380, 1320, 1080, 3389, 562, 291, 855, 309, 2541, 6521, 1412, 13, 50814, 50814, 708, 311, 257, 538, 25111, 30, 50914, 50914, 876, 11, 286, 478, 2597, 13, 316, 538, 25111, 445, 1355, 257, 8952, 2487, 13, 51164, 51164, 407, 264, 912, 16579, 294, 732, 819, 8650, 13, 51414, 51414, 2264, 11, 3231, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.20263182776314873, "compression_ratio": 1.3563829787234043, "no_speech_prob": 5.4698175517842174e-05}, {"id": 514, "seek": 394200, "start": 3951.0, "end": 3953.0, "text": " What's a bytext?", "tokens": [50364, 597, 1062, 754, 854, 3890, 1125, 428, 2316, 11, 457, 1391, 4659, 380, 1320, 1080, 3389, 562, 291, 855, 309, 2541, 6521, 1412, 13, 50814, 50814, 708, 311, 257, 538, 25111, 30, 50914, 50914, 876, 11, 286, 478, 2597, 13, 316, 538, 25111, 445, 1355, 257, 8952, 2487, 13, 51164, 51164, 407, 264, 912, 16579, 294, 732, 819, 8650, 13, 51414, 51414, 2264, 11, 3231, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.20263182776314873, "compression_ratio": 1.3563829787234043, "no_speech_prob": 5.4698175517842174e-05}, {"id": 515, "seek": 394200, "start": 3953.0, "end": 3958.0, "text": " Oh, I'm sorry. A bytext just means a parallel text.", "tokens": [50364, 597, 1062, 754, 854, 3890, 1125, 428, 2316, 11, 457, 1391, 4659, 380, 1320, 1080, 3389, 562, 291, 855, 309, 2541, 6521, 1412, 13, 50814, 50814, 708, 311, 257, 538, 25111, 30, 50914, 50914, 876, 11, 286, 478, 2597, 13, 316, 538, 25111, 445, 1355, 257, 8952, 2487, 13, 51164, 51164, 407, 264, 912, 16579, 294, 732, 819, 8650, 13, 51414, 51414, 2264, 11, 3231, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.20263182776314873, "compression_ratio": 1.3563829787234043, "no_speech_prob": 5.4698175517842174e-05}, {"id": 516, "seek": 394200, "start": 3958.0, "end": 3963.0, "text": " So the same sentences in two different languages.", "tokens": [50364, 597, 1062, 754, 854, 3890, 1125, 428, 2316, 11, 457, 1391, 4659, 380, 1320, 1080, 3389, 562, 291, 855, 309, 2541, 6521, 1412, 13, 50814, 50814, 708, 311, 257, 538, 25111, 30, 50914, 50914, 876, 11, 286, 478, 2597, 13, 316, 538, 25111, 445, 1355, 257, 8952, 2487, 13, 51164, 51164, 407, 264, 912, 16579, 294, 732, 819, 8650, 13, 51414, 51414, 2264, 11, 3231, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.20263182776314873, "compression_ratio": 1.3563829787234043, "no_speech_prob": 5.4698175517842174e-05}, {"id": 517, "seek": 394200, "start": 3963.0, "end": 3965.0, "text": " OK, thanks.", "tokens": [50364, 597, 1062, 754, 854, 3890, 1125, 428, 2316, 11, 457, 1391, 4659, 380, 1320, 1080, 3389, 562, 291, 855, 309, 2541, 6521, 1412, 13, 50814, 50814, 708, 311, 257, 538, 25111, 30, 50914, 50914, 876, 11, 286, 478, 2597, 13, 316, 538, 25111, 445, 1355, 257, 8952, 2487, 13, 51164, 51164, 407, 264, 912, 16579, 294, 732, 819, 8650, 13, 51414, 51414, 2264, 11, 3231, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.20263182776314873, "compression_ratio": 1.3563829787234043, "no_speech_prob": 5.4698175517842174e-05}, {"id": 518, "seek": 396500, "start": 3965.0, "end": 3977.0, "text": " And the nice thing about translation is that the outputs you get are actually always, you know, high quality,", "tokens": [50364, 400, 264, 1481, 551, 466, 12853, 307, 300, 264, 23930, 291, 483, 366, 767, 1009, 11, 291, 458, 11, 1090, 3125, 11, 50964, 50964, 570, 613, 366, 264, 23930, 295, 264, 1185, 11, 286, 519, 11, 281, 291, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.2620408479557481, "compression_ratio": 1.3790322580645162, "no_speech_prob": 2.0453728211577982e-05}, {"id": 519, "seek": 396500, "start": 3977.0, "end": 3985.0, "text": " because these are the outputs of the system, I think, to you.", "tokens": [50364, 400, 264, 1481, 551, 466, 12853, 307, 300, 264, 23930, 291, 483, 366, 767, 1009, 11, 291, 458, 11, 1090, 3125, 11, 50964, 50964, 570, 613, 366, 264, 23930, 295, 264, 1185, 11, 286, 519, 11, 281, 291, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.2620408479557481, "compression_ratio": 1.3790322580645162, "no_speech_prob": 2.0453728211577982e-05}, {"id": 520, "seek": 398500, "start": 3985.0, "end": 3995.0, "text": " So you can use the same sentences you found in the wild on the Internet.", "tokens": [50364, 407, 291, 393, 764, 264, 912, 16579, 291, 1352, 294, 264, 4868, 322, 264, 7703, 13, 50864, 50864, 509, 434, 445, 516, 281, 1884, 512, 24518, 15743, 300, 291, 393, 764, 337, 613, 6119, 365, 613, 23930, 13, 51264, 51264, 7497, 291, 352, 646, 257, 4137, 30, 51414, 51414, 440, 2636, 935, 13799, 17375, 295, 2283, 295, 3669, 281, 6521, 13, 51614, 51614, 1119, 300, 807, 264, 9943, 12853, 2316, 30, 51764, 51764, 7587, 13, 865, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.3538540498709973, "compression_ratio": 1.5330188679245282, "no_speech_prob": 4.264038216206245e-05}, {"id": 521, "seek": 398500, "start": 3995.0, "end": 4003.0, "text": " You're just going to create some noisy inputs that you can use for these pair with these outputs.", "tokens": [50364, 407, 291, 393, 764, 264, 912, 16579, 291, 1352, 294, 264, 4868, 322, 264, 7703, 13, 50864, 50864, 509, 434, 445, 516, 281, 1884, 512, 24518, 15743, 300, 291, 393, 764, 337, 613, 6119, 365, 613, 23930, 13, 51264, 51264, 7497, 291, 352, 646, 257, 4137, 30, 51414, 51414, 440, 2636, 935, 13799, 17375, 295, 2283, 295, 3669, 281, 6521, 13, 51614, 51614, 1119, 300, 807, 264, 9943, 12853, 2316, 30, 51764, 51764, 7587, 13, 865, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.3538540498709973, "compression_ratio": 1.5330188679245282, "no_speech_prob": 4.264038216206245e-05}, {"id": 522, "seek": 398500, "start": 4003.0, "end": 4006.0, "text": " Could you go back a slide?", "tokens": [50364, 407, 291, 393, 764, 264, 912, 16579, 291, 1352, 294, 264, 4868, 322, 264, 7703, 13, 50864, 50864, 509, 434, 445, 516, 281, 1884, 512, 24518, 15743, 300, 291, 393, 764, 337, 613, 6119, 365, 613, 23930, 13, 51264, 51264, 7497, 291, 352, 646, 257, 4137, 30, 51414, 51414, 440, 2636, 935, 13799, 17375, 295, 2283, 295, 3669, 281, 6521, 13, 51614, 51614, 1119, 300, 807, 264, 9943, 12853, 2316, 30, 51764, 51764, 7587, 13, 865, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.3538540498709973, "compression_ratio": 1.5330188679245282, "no_speech_prob": 4.264038216206245e-05}, {"id": 523, "seek": 398500, "start": 4006.0, "end": 4010.0, "text": " The third point translate billions of words of English to German.", "tokens": [50364, 407, 291, 393, 764, 264, 912, 16579, 291, 1352, 294, 264, 4868, 322, 264, 7703, 13, 50864, 50864, 509, 434, 445, 516, 281, 1884, 512, 24518, 15743, 300, 291, 393, 764, 337, 613, 6119, 365, 613, 23930, 13, 51264, 51264, 7497, 291, 352, 646, 257, 4137, 30, 51414, 51414, 440, 2636, 935, 13799, 17375, 295, 2283, 295, 3669, 281, 6521, 13, 51614, 51614, 1119, 300, 807, 264, 9943, 12853, 2316, 30, 51764, 51764, 7587, 13, 865, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.3538540498709973, "compression_ratio": 1.5330188679245282, "no_speech_prob": 4.264038216206245e-05}, {"id": 524, "seek": 398500, "start": 4010.0, "end": 4013.0, "text": " Is that through the reverse translation model?", "tokens": [50364, 407, 291, 393, 764, 264, 912, 16579, 291, 1352, 294, 264, 4868, 322, 264, 7703, 13, 50864, 50864, 509, 434, 445, 516, 281, 1884, 512, 24518, 15743, 300, 291, 393, 764, 337, 613, 6119, 365, 613, 23930, 13, 51264, 51264, 7497, 291, 352, 646, 257, 4137, 30, 51414, 51414, 440, 2636, 935, 13799, 17375, 295, 2283, 295, 3669, 281, 6521, 13, 51614, 51614, 1119, 300, 807, 264, 9943, 12853, 2316, 30, 51764, 51764, 7587, 13, 865, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.3538540498709973, "compression_ratio": 1.5330188679245282, "no_speech_prob": 4.264038216206245e-05}, {"id": 525, "seek": 398500, "start": 4013.0, "end": 4014.0, "text": " Exactly. Yeah.", "tokens": [50364, 407, 291, 393, 764, 264, 912, 16579, 291, 1352, 294, 264, 4868, 322, 264, 7703, 13, 50864, 50864, 509, 434, 445, 516, 281, 1884, 512, 24518, 15743, 300, 291, 393, 764, 337, 613, 6119, 365, 613, 23930, 13, 51264, 51264, 7497, 291, 352, 646, 257, 4137, 30, 51414, 51414, 440, 2636, 935, 13799, 17375, 295, 2283, 295, 3669, 281, 6521, 13, 51614, 51614, 1119, 300, 807, 264, 9943, 12853, 2316, 30, 51764, 51764, 7587, 13, 865, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.3538540498709973, "compression_ratio": 1.5330188679245282, "no_speech_prob": 4.264038216206245e-05}, {"id": 526, "seek": 401400, "start": 4014.0, "end": 4018.0, "text": " So you're saying that the reverse translation is because of regularization.", "tokens": [50364, 407, 291, 434, 1566, 300, 264, 9943, 12853, 307, 570, 295, 3890, 2144, 13, 50564, 50564, 467, 311, 406, 445, 3890, 2144, 13, 50664, 50664, 440, 534, 4420, 551, 307, 309, 2709, 291, 3195, 295, 2541, 5598, 1412, 13, 50964, 50964, 407, 718, 311, 584, 291, 528, 281, 312, 257, 665, 6521, 281, 3669, 12853, 2316, 13, 51164, 51164, 509, 733, 295, 643, 1293, 281, 1223, 6521, 11, 457, 611, 312, 1075, 281, 2464, 3195, 295, 40799, 3669, 382, 731, 293, 1223, 3669, 22317, 886, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.3025169160630968, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.0001333697437075898}, {"id": 527, "seek": 401400, "start": 4018.0, "end": 4020.0, "text": " It's not just regularization.", "tokens": [50364, 407, 291, 434, 1566, 300, 264, 9943, 12853, 307, 570, 295, 3890, 2144, 13, 50564, 50564, 467, 311, 406, 445, 3890, 2144, 13, 50664, 50664, 440, 534, 4420, 551, 307, 309, 2709, 291, 3195, 295, 2541, 5598, 1412, 13, 50964, 50964, 407, 718, 311, 584, 291, 528, 281, 312, 257, 665, 6521, 281, 3669, 12853, 2316, 13, 51164, 51164, 509, 733, 295, 643, 1293, 281, 1223, 6521, 11, 457, 611, 312, 1075, 281, 2464, 3195, 295, 40799, 3669, 382, 731, 293, 1223, 3669, 22317, 886, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.3025169160630968, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.0001333697437075898}, {"id": 528, "seek": 401400, "start": 4020.0, "end": 4026.0, "text": " The really useful thing is it gives you lots of clean output data.", "tokens": [50364, 407, 291, 434, 1566, 300, 264, 9943, 12853, 307, 570, 295, 3890, 2144, 13, 50564, 50564, 467, 311, 406, 445, 3890, 2144, 13, 50664, 50664, 440, 534, 4420, 551, 307, 309, 2709, 291, 3195, 295, 2541, 5598, 1412, 13, 50964, 50964, 407, 718, 311, 584, 291, 528, 281, 312, 257, 665, 6521, 281, 3669, 12853, 2316, 13, 51164, 51164, 509, 733, 295, 643, 1293, 281, 1223, 6521, 11, 457, 611, 312, 1075, 281, 2464, 3195, 295, 40799, 3669, 382, 731, 293, 1223, 3669, 22317, 886, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.3025169160630968, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.0001333697437075898}, {"id": 529, "seek": 401400, "start": 4026.0, "end": 4030.0, "text": " So let's say you want to be a good German to English translation model.", "tokens": [50364, 407, 291, 434, 1566, 300, 264, 9943, 12853, 307, 570, 295, 3890, 2144, 13, 50564, 50564, 467, 311, 406, 445, 3890, 2144, 13, 50664, 50664, 440, 534, 4420, 551, 307, 309, 2709, 291, 3195, 295, 2541, 5598, 1412, 13, 50964, 50964, 407, 718, 311, 584, 291, 528, 281, 312, 257, 665, 6521, 281, 3669, 12853, 2316, 13, 51164, 51164, 509, 733, 295, 643, 1293, 281, 1223, 6521, 11, 457, 611, 312, 1075, 281, 2464, 3195, 295, 40799, 3669, 382, 731, 293, 1223, 3669, 22317, 886, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.3025169160630968, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.0001333697437075898}, {"id": 530, "seek": 401400, "start": 4030.0, "end": 4039.0, "text": " You kind of need both to understand German, but also be able to write lots of fluent English as well and understand English grammar too.", "tokens": [50364, 407, 291, 434, 1566, 300, 264, 9943, 12853, 307, 570, 295, 3890, 2144, 13, 50564, 50564, 467, 311, 406, 445, 3890, 2144, 13, 50664, 50664, 440, 534, 4420, 551, 307, 309, 2709, 291, 3195, 295, 2541, 5598, 1412, 13, 50964, 50964, 407, 718, 311, 584, 291, 528, 281, 312, 257, 665, 6521, 281, 3669, 12853, 2316, 13, 51164, 51164, 509, 733, 295, 643, 1293, 281, 1223, 6521, 11, 457, 611, 312, 1075, 281, 2464, 3195, 295, 40799, 3669, 382, 731, 293, 1223, 3669, 22317, 886, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.3025169160630968, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.0001333697437075898}, {"id": 531, "seek": 403900, "start": 4039.0, "end": 4052.0, "text": " So the reverse translation gives you a way of incorporating tons of additional English data beyond what you have translations for.", "tokens": [50364, 407, 264, 9943, 12853, 2709, 291, 257, 636, 295, 33613, 9131, 295, 4497, 3669, 1412, 4399, 437, 291, 362, 37578, 337, 13, 51014, 51014, 2264, 13, 51064, 51064, 407, 309, 733, 295, 29520, 12853, 2316, 365, 264, 2856, 2316, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.2650148435072465, "compression_ratio": 1.4388489208633093, "no_speech_prob": 2.7953050448559225e-05}, {"id": 532, "seek": 403900, "start": 4052.0, "end": 4053.0, "text": " OK.", "tokens": [50364, 407, 264, 9943, 12853, 2709, 291, 257, 636, 295, 33613, 9131, 295, 4497, 3669, 1412, 4399, 437, 291, 362, 37578, 337, 13, 51014, 51014, 2264, 13, 51064, 51064, 407, 309, 733, 295, 29520, 12853, 2316, 365, 264, 2856, 2316, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.2650148435072465, "compression_ratio": 1.4388489208633093, "no_speech_prob": 2.7953050448559225e-05}, {"id": 533, "seek": 403900, "start": 4053.0, "end": 4057.0, "text": " So it kind of combines translation model with the language model.", "tokens": [50364, 407, 264, 9943, 12853, 2709, 291, 257, 636, 295, 33613, 9131, 295, 4497, 3669, 1412, 4399, 437, 291, 362, 37578, 337, 13, 51014, 51014, 2264, 13, 51064, 51064, 407, 309, 733, 295, 29520, 12853, 2316, 365, 264, 2856, 2316, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.2650148435072465, "compression_ratio": 1.4388489208633093, "no_speech_prob": 2.7953050448559225e-05}, {"id": 534, "seek": 405700, "start": 4057.0, "end": 4080.0, "text": " You can also iterate this procedure too. So you can use that whole set of words I described before to train an atom model and then do this to help you generate better back translations, which you can use to train again.", "tokens": [50364, 509, 393, 611, 44497, 341, 10747, 886, 13, 407, 291, 393, 764, 300, 1379, 992, 295, 2283, 286, 7619, 949, 281, 3847, 364, 12018, 2316, 293, 550, 360, 341, 281, 854, 291, 8460, 1101, 646, 37578, 11, 597, 291, 393, 764, 281, 3847, 797, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.37064692438865193, "compression_ratio": 1.5, "no_speech_prob": 1.3843278793501668e-05}, {"id": 535, "seek": 408000, "start": 4080.0, "end": 4088.0, "text": " And this can be really helpful. I mean, it helps even in English German, but it's particularly helpful in cases where you don't have much data.", "tokens": [50364, 400, 341, 393, 312, 534, 4961, 13, 286, 914, 11, 309, 3665, 754, 294, 3669, 6521, 11, 457, 309, 311, 4098, 4961, 294, 3331, 689, 291, 500, 380, 362, 709, 1412, 13, 50764, 50764, 407, 341, 307, 322, 264, 6747, 281, 3669, 12853, 689, 456, 1943, 380, 257, 688, 295, 8952, 1412, 11, 457, 291, 393, 483, 534, 2416, 13797, 538, 445, 17138, 990, 646, 12853, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.19942126475589375, "compression_ratio": 1.5606060606060606, "no_speech_prob": 1.202557632495882e-05}, {"id": 536, "seek": 408000, "start": 4088.0, "end": 4103.0, "text": " So this is on the East to English translation where there isn't a lot of parallel data, but you can get really large improvements by just iterating back translation.", "tokens": [50364, 400, 341, 393, 312, 534, 4961, 13, 286, 914, 11, 309, 3665, 754, 294, 3669, 6521, 11, 457, 309, 311, 4098, 4961, 294, 3331, 689, 291, 500, 380, 362, 709, 1412, 13, 50764, 50764, 407, 341, 307, 322, 264, 6747, 281, 3669, 12853, 689, 456, 1943, 380, 257, 688, 295, 8952, 1412, 11, 457, 291, 393, 483, 534, 2416, 13797, 538, 445, 17138, 990, 646, 12853, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.19942126475589375, "compression_ratio": 1.5606060606060606, "no_speech_prob": 1.202557632495882e-05}, {"id": 537, "seek": 410300, "start": 4103.0, "end": 4110.0, "text": " And there's a recent work from Fair, which I forgot to add a reference for.", "tokens": [50364, 400, 456, 311, 257, 5162, 589, 490, 12157, 11, 597, 286, 5298, 281, 909, 257, 6408, 337, 13, 50714, 50714, 400, 797, 11, 510, 366, 512, 3542, 322, 264, 3669, 6521, 11, 4099, 512, 665, 13797, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.374181817217571, "compression_ratio": 1.2834645669291338, "no_speech_prob": 2.6261875973432325e-05}, {"id": 538, "seek": 410300, "start": 4110.0, "end": 4119.0, "text": " And again, here are some results on the English German, showing some good improvements.", "tokens": [50364, 400, 456, 311, 257, 5162, 589, 490, 12157, 11, 597, 286, 5298, 281, 909, 257, 6408, 337, 13, 50714, 50714, 400, 797, 11, 510, 366, 512, 3542, 322, 264, 3669, 6521, 11, 4099, 512, 665, 13797, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.374181817217571, "compression_ratio": 1.2834645669291338, "no_speech_prob": 2.6261875973432325e-05}, {"id": 539, "seek": 411900, "start": 4119.0, "end": 4139.0, "text": " One of the directions in machine translation people are exploring now is a massively multilingual MTE. So people are trying to not just translate between two languages, but trying to take dozens of hundred languages and try and train a single neural network that can translate between all of these.", "tokens": [50364, 1485, 295, 264, 11095, 294, 3479, 12853, 561, 366, 12736, 586, 307, 257, 29379, 2120, 38219, 376, 13639, 13, 407, 561, 366, 1382, 281, 406, 445, 13799, 1296, 732, 8650, 11, 457, 1382, 281, 747, 18431, 295, 3262, 8650, 293, 853, 293, 3847, 257, 2167, 18161, 3209, 300, 393, 13799, 1296, 439, 295, 613, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.14621592376191736, "compression_ratio": 1.6284153005464481, "no_speech_prob": 6.008399577694945e-05}, {"id": 540, "seek": 413900, "start": 4139.0, "end": 4154.0, "text": " And if you do this, you start seeing a big improvement, particularly in languages where you don't have much text. Assuming the model is learning some kind of more general language and dependent information.", "tokens": [50364, 400, 498, 291, 360, 341, 11, 291, 722, 2577, 257, 955, 10444, 11, 4098, 294, 8650, 689, 291, 500, 380, 362, 709, 2487, 13, 6281, 24919, 264, 2316, 307, 2539, 512, 733, 295, 544, 2674, 2856, 293, 12334, 1589, 13, 51114, 51114, 1033, 11, 370, 264, 1036, 4829, 286, 528, 281, 2060, 294, 341, 307, 534, 1021, 11, 597, 307, 2698, 46533, 2539, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.16877947134130142, "compression_ratio": 1.5270935960591132, "no_speech_prob": 2.0138908439548686e-05}, {"id": 541, "seek": 413900, "start": 4154.0, "end": 4163.0, "text": " Okay, so the last topic I want to cover in this is really important, which is self supervised learning.", "tokens": [50364, 400, 498, 291, 360, 341, 11, 291, 722, 2577, 257, 955, 10444, 11, 4098, 294, 8650, 689, 291, 500, 380, 362, 709, 2487, 13, 6281, 24919, 264, 2316, 307, 2539, 512, 733, 295, 544, 2674, 2856, 293, 12334, 1589, 13, 51114, 51114, 1033, 11, 370, 264, 1036, 4829, 286, 528, 281, 2060, 294, 341, 307, 534, 1021, 11, 597, 307, 2698, 46533, 2539, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.16877947134130142, "compression_ratio": 1.5270935960591132, "no_speech_prob": 2.0138908439548686e-05}, {"id": 542, "seek": 416300, "start": 4163.0, "end": 4173.0, "text": " So, yeah, I'm just sick of seeing this cake now, but I think it's actually a good image for this. So the idea is", "tokens": [50364, 407, 11, 1338, 11, 286, 478, 445, 4998, 295, 2577, 341, 5908, 586, 11, 457, 286, 519, 309, 311, 767, 257, 665, 3256, 337, 341, 13, 407, 264, 1558, 307, 50864, 50864, 534, 300, 281, 1466, 1507, 411, 881, 295, 264, 1589, 321, 643, 307, 516, 281, 312, 11, 881, 295, 264, 2539, 321, 360, 575, 281, 312, 2693, 12879, 24420, 13, 51214, 51214, 407, 321, 362, 2603, 11663, 295, 2487, 293, 3195, 295, 777, 16949, 322, 309, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.24332437170557228, "compression_ratio": 1.5561224489795917, "no_speech_prob": 7.36098299967125e-05}, {"id": 543, "seek": 416300, "start": 4173.0, "end": 4180.0, "text": " really that to learn stuff like most of the information we need is going to be, most of the learning we do has to be unsupervised.", "tokens": [50364, 407, 11, 1338, 11, 286, 478, 445, 4998, 295, 2577, 341, 5908, 586, 11, 457, 286, 519, 309, 311, 767, 257, 665, 3256, 337, 341, 13, 407, 264, 1558, 307, 50864, 50864, 534, 300, 281, 1466, 1507, 411, 881, 295, 264, 1589, 321, 643, 307, 516, 281, 312, 11, 881, 295, 264, 2539, 321, 360, 575, 281, 312, 2693, 12879, 24420, 13, 51214, 51214, 407, 321, 362, 2603, 11663, 295, 2487, 293, 3195, 295, 777, 16949, 322, 309, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.24332437170557228, "compression_ratio": 1.5561224489795917, "no_speech_prob": 7.36098299967125e-05}, {"id": 544, "seek": 416300, "start": 4180.0, "end": 4184.0, "text": " So we have huge amounts of text and lots of new labels on it.", "tokens": [50364, 407, 11, 1338, 11, 286, 478, 445, 4998, 295, 2577, 341, 5908, 586, 11, 457, 286, 519, 309, 311, 767, 257, 665, 3256, 337, 341, 13, 407, 264, 1558, 307, 50864, 50864, 534, 300, 281, 1466, 1507, 411, 881, 295, 264, 1589, 321, 643, 307, 516, 281, 312, 11, 881, 295, 264, 2539, 321, 360, 575, 281, 312, 2693, 12879, 24420, 13, 51214, 51214, 407, 321, 362, 2603, 11663, 295, 2487, 293, 3195, 295, 777, 16949, 322, 309, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.24332437170557228, "compression_ratio": 1.5561224489795917, "no_speech_prob": 7.36098299967125e-05}, {"id": 545, "seek": 418400, "start": 4184.0, "end": 4197.0, "text": " So we just have a little bit of sort of supervised training data. And that's represented by the cake here being unsupervised learning and the supervised learning just being a little bit of icing on top of the cake.", "tokens": [50364, 407, 321, 445, 362, 257, 707, 857, 295, 1333, 295, 46533, 3097, 1412, 13, 400, 300, 311, 10379, 538, 264, 5908, 510, 885, 2693, 12879, 24420, 2539, 293, 264, 46533, 2539, 445, 885, 257, 707, 857, 295, 30086, 322, 1192, 295, 264, 5908, 13, 51014, 51014, 286, 519, 767, 264, 5162, 4205, 294, 2698, 46533, 2539, 337, 426, 45196, 575, 534, 14617, 341, 19157, 281, 589, 13, 51414, 51414, 1033, 11, 370, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1870497653358861, "compression_ratio": 1.719387755102041, "no_speech_prob": 2.9300988899194635e-05}, {"id": 546, "seek": 418400, "start": 4197.0, "end": 4205.0, "text": " I think actually the recent progress in self supervised learning for NLP has really proved this metaphor to work.", "tokens": [50364, 407, 321, 445, 362, 257, 707, 857, 295, 1333, 295, 46533, 3097, 1412, 13, 400, 300, 311, 10379, 538, 264, 5908, 510, 885, 2693, 12879, 24420, 2539, 293, 264, 46533, 2539, 445, 885, 257, 707, 857, 295, 30086, 322, 1192, 295, 264, 5908, 13, 51014, 51014, 286, 519, 767, 264, 5162, 4205, 294, 2698, 46533, 2539, 337, 426, 45196, 575, 534, 14617, 341, 19157, 281, 589, 13, 51414, 51414, 1033, 11, 370, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1870497653358861, "compression_ratio": 1.719387755102041, "no_speech_prob": 2.9300988899194635e-05}, {"id": 547, "seek": 418400, "start": 4205.0, "end": 4210.0, "text": " Okay, so", "tokens": [50364, 407, 321, 445, 362, 257, 707, 857, 295, 1333, 295, 46533, 3097, 1412, 13, 400, 300, 311, 10379, 538, 264, 5908, 510, 885, 2693, 12879, 24420, 2539, 293, 264, 46533, 2539, 445, 885, 257, 707, 857, 295, 30086, 322, 1192, 295, 264, 5908, 13, 51014, 51014, 286, 519, 767, 264, 5162, 4205, 294, 2698, 46533, 2539, 337, 426, 45196, 575, 534, 14617, 341, 19157, 281, 589, 13, 51414, 51414, 1033, 11, 370, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1870497653358861, "compression_ratio": 1.719387755102041, "no_speech_prob": 2.9300988899194635e-05}, {"id": 548, "seek": 421000, "start": 4210.0, "end": 4214.0, "text": " I'm going to describe a few methods for how you can do self supervised learning for NLP.", "tokens": [50364, 286, 478, 516, 281, 6786, 257, 1326, 7150, 337, 577, 291, 393, 360, 2698, 46533, 2539, 337, 426, 45196, 13, 50564, 50564, 1449, 370, 291, 393, 853, 293, 483, 512, 3487, 382, 281, 437, 311, 767, 1364, 13, 50864, 50864, 407, 264, 700, 472, 286, 478, 516, 281, 6786, 307, 8725, 17, 53, 3045, 13, 51264, 51264, 407, 264, 1558, 295, 8725, 17, 53, 3045, 390, 1382, 281, 11, 286, 519, 309, 311, 534, 264, 700, 3035, 300, 4712, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.17549499833440207, "compression_ratio": 1.563157894736842, "no_speech_prob": 5.465690992423333e-05}, {"id": 549, "seek": 421000, "start": 4214.0, "end": 4220.0, "text": " Just so you can try and get some ideas as to what's actually working.", "tokens": [50364, 286, 478, 516, 281, 6786, 257, 1326, 7150, 337, 577, 291, 393, 360, 2698, 46533, 2539, 337, 426, 45196, 13, 50564, 50564, 1449, 370, 291, 393, 853, 293, 483, 512, 3487, 382, 281, 437, 311, 767, 1364, 13, 50864, 50864, 407, 264, 700, 472, 286, 478, 516, 281, 6786, 307, 8725, 17, 53, 3045, 13, 51264, 51264, 407, 264, 1558, 295, 8725, 17, 53, 3045, 390, 1382, 281, 11, 286, 519, 309, 311, 534, 264, 700, 3035, 300, 4712, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.17549499833440207, "compression_ratio": 1.563157894736842, "no_speech_prob": 5.465690992423333e-05}, {"id": 550, "seek": 421000, "start": 4220.0, "end": 4228.0, "text": " So the first one I'm going to describe is Word2Vec.", "tokens": [50364, 286, 478, 516, 281, 6786, 257, 1326, 7150, 337, 577, 291, 393, 360, 2698, 46533, 2539, 337, 426, 45196, 13, 50564, 50564, 1449, 370, 291, 393, 853, 293, 483, 512, 3487, 382, 281, 437, 311, 767, 1364, 13, 50864, 50864, 407, 264, 700, 472, 286, 478, 516, 281, 6786, 307, 8725, 17, 53, 3045, 13, 51264, 51264, 407, 264, 1558, 295, 8725, 17, 53, 3045, 390, 1382, 281, 11, 286, 519, 309, 311, 534, 264, 700, 3035, 300, 4712, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.17549499833440207, "compression_ratio": 1.563157894736842, "no_speech_prob": 5.465690992423333e-05}, {"id": 551, "seek": 421000, "start": 4228.0, "end": 4235.0, "text": " So the idea of Word2Vec was trying to, I think it's really the first paper that showed", "tokens": [50364, 286, 478, 516, 281, 6786, 257, 1326, 7150, 337, 577, 291, 393, 360, 2698, 46533, 2539, 337, 426, 45196, 13, 50564, 50564, 1449, 370, 291, 393, 853, 293, 483, 512, 3487, 382, 281, 437, 311, 767, 1364, 13, 50864, 50864, 407, 264, 700, 472, 286, 478, 516, 281, 6786, 307, 8725, 17, 53, 3045, 13, 51264, 51264, 407, 264, 1558, 295, 8725, 17, 53, 3045, 390, 1382, 281, 11, 286, 519, 309, 311, 534, 264, 700, 3035, 300, 4712, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.17549499833440207, "compression_ratio": 1.563157894736842, "no_speech_prob": 5.465690992423333e-05}, {"id": 552, "seek": 423500, "start": 4235.0, "end": 4246.0, "text": " got people excited about self supervised learning in NLP. They're having some previous work from a call by Aaron Weston, which also showed some good gains.", "tokens": [50364, 658, 561, 2919, 466, 2698, 46533, 2539, 294, 426, 45196, 13, 814, 434, 1419, 512, 3894, 589, 490, 257, 818, 538, 14018, 4055, 266, 11, 597, 611, 4712, 512, 665, 16823, 13, 50914, 50914, 407, 264, 3387, 510, 307, 281, 853, 293, 1466, 437, 311, 1219, 1349, 12240, 3584, 13, 407, 3124, 1901, 33358, 337, 2283, 13, 51214, 51214, 400, 264, 1454, 307, 300, 51464, 51464, 538, 445, 538, 1237, 412, 32118, 18657, 292, 3669, 2487, 11, 321, 393, 1466, 746, 466, 437, 613, 2283, 914, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.2403343536041595, "compression_ratio": 1.5564516129032258, "no_speech_prob": 3.2124091376317665e-05}, {"id": 553, "seek": 423500, "start": 4246.0, "end": 4252.0, "text": " So the goal here is to try and learn what's called word embedding. So practice space representations for words.", "tokens": [50364, 658, 561, 2919, 466, 2698, 46533, 2539, 294, 426, 45196, 13, 814, 434, 1419, 512, 3894, 589, 490, 257, 818, 538, 14018, 4055, 266, 11, 597, 611, 4712, 512, 665, 16823, 13, 50914, 50914, 407, 264, 3387, 510, 307, 281, 853, 293, 1466, 437, 311, 1219, 1349, 12240, 3584, 13, 407, 3124, 1901, 33358, 337, 2283, 13, 51214, 51214, 400, 264, 1454, 307, 300, 51464, 51464, 538, 445, 538, 1237, 412, 32118, 18657, 292, 3669, 2487, 11, 321, 393, 1466, 746, 466, 437, 613, 2283, 914, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.2403343536041595, "compression_ratio": 1.5564516129032258, "no_speech_prob": 3.2124091376317665e-05}, {"id": 554, "seek": 423500, "start": 4252.0, "end": 4257.0, "text": " And the hope is that", "tokens": [50364, 658, 561, 2919, 466, 2698, 46533, 2539, 294, 426, 45196, 13, 814, 434, 1419, 512, 3894, 589, 490, 257, 818, 538, 14018, 4055, 266, 11, 597, 611, 4712, 512, 665, 16823, 13, 50914, 50914, 407, 264, 3387, 510, 307, 281, 853, 293, 1466, 437, 311, 1219, 1349, 12240, 3584, 13, 407, 3124, 1901, 33358, 337, 2283, 13, 51214, 51214, 400, 264, 1454, 307, 300, 51464, 51464, 538, 445, 538, 1237, 412, 32118, 18657, 292, 3669, 2487, 11, 321, 393, 1466, 746, 466, 437, 613, 2283, 914, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.2403343536041595, "compression_ratio": 1.5564516129032258, "no_speech_prob": 3.2124091376317665e-05}, {"id": 555, "seek": 423500, "start": 4257.0, "end": 4262.0, "text": " by just by looking at unlabeled English text, we can learn something about what these words mean.", "tokens": [50364, 658, 561, 2919, 466, 2698, 46533, 2539, 294, 426, 45196, 13, 814, 434, 1419, 512, 3894, 589, 490, 257, 818, 538, 14018, 4055, 266, 11, 597, 611, 4712, 512, 665, 16823, 13, 50914, 50914, 407, 264, 3387, 510, 307, 281, 853, 293, 1466, 437, 311, 1219, 1349, 12240, 3584, 13, 407, 3124, 1901, 33358, 337, 2283, 13, 51214, 51214, 400, 264, 1454, 307, 300, 51464, 51464, 538, 445, 538, 1237, 412, 32118, 18657, 292, 3669, 2487, 11, 321, 393, 1466, 746, 466, 437, 613, 2283, 914, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.2403343536041595, "compression_ratio": 1.5564516129032258, "no_speech_prob": 3.2124091376317665e-05}, {"id": 556, "seek": 426200, "start": 4262.0, "end": 4267.0, "text": " And so the intuition behind all this is that if two words are", "tokens": [50364, 400, 370, 264, 24002, 2261, 439, 341, 307, 300, 498, 732, 2283, 366, 50614, 50614, 1998, 1214, 294, 264, 2487, 11, 550, 436, 434, 3700, 281, 362, 512, 733, 295, 2480, 1296, 1184, 661, 13, 50914, 50914, 407, 264, 659, 12, 17227, 1760, 5633, 321, 434, 516, 281, 360, 307, 516, 281, 312, 257, 10623, 294, 264, 8247, 82, 5633, 13, 51114, 51114, 407, 294, 341, 8174, 11, 286, 478, 516, 281, 6094, 484, 341, 1349, 294, 264, 2808, 11, 597, 307, 28122, 82, 11, 293, 853, 293, 6069, 437, 341, 1349, 820, 312, 2361, 322, 264, 4319, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14541425982725273, "compression_ratio": 1.6875, "no_speech_prob": 3.480211307760328e-05}, {"id": 557, "seek": 426200, "start": 4267.0, "end": 4273.0, "text": " close together in the text, then they're likely to have some kind of relationship between each other.", "tokens": [50364, 400, 370, 264, 24002, 2261, 439, 341, 307, 300, 498, 732, 2283, 366, 50614, 50614, 1998, 1214, 294, 264, 2487, 11, 550, 436, 434, 3700, 281, 362, 512, 733, 295, 2480, 1296, 1184, 661, 13, 50914, 50914, 407, 264, 659, 12, 17227, 1760, 5633, 321, 434, 516, 281, 360, 307, 516, 281, 312, 257, 10623, 294, 264, 8247, 82, 5633, 13, 51114, 51114, 407, 294, 341, 8174, 11, 286, 478, 516, 281, 6094, 484, 341, 1349, 294, 264, 2808, 11, 597, 307, 28122, 82, 11, 293, 853, 293, 6069, 437, 341, 1349, 820, 312, 2361, 322, 264, 4319, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14541425982725273, "compression_ratio": 1.6875, "no_speech_prob": 3.480211307760328e-05}, {"id": 558, "seek": 426200, "start": 4273.0, "end": 4277.0, "text": " So the pre-training task we're going to do is going to be a filling in the blanks task.", "tokens": [50364, 400, 370, 264, 24002, 2261, 439, 341, 307, 300, 498, 732, 2283, 366, 50614, 50614, 1998, 1214, 294, 264, 2487, 11, 550, 436, 434, 3700, 281, 362, 512, 733, 295, 2480, 1296, 1184, 661, 13, 50914, 50914, 407, 264, 659, 12, 17227, 1760, 5633, 321, 434, 516, 281, 360, 307, 516, 281, 312, 257, 10623, 294, 264, 8247, 82, 5633, 13, 51114, 51114, 407, 294, 341, 8174, 11, 286, 478, 516, 281, 6094, 484, 341, 1349, 294, 264, 2808, 11, 597, 307, 28122, 82, 11, 293, 853, 293, 6069, 437, 341, 1349, 820, 312, 2361, 322, 264, 4319, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14541425982725273, "compression_ratio": 1.6875, "no_speech_prob": 3.480211307760328e-05}, {"id": 559, "seek": 426200, "start": 4277.0, "end": 4287.0, "text": " So in this sentence, I'm going to mask out this word in the middle, which is unicorns, and try and predict what this word should be based on the context.", "tokens": [50364, 400, 370, 264, 24002, 2261, 439, 341, 307, 300, 498, 732, 2283, 366, 50614, 50614, 1998, 1214, 294, 264, 2487, 11, 550, 436, 434, 3700, 281, 362, 512, 733, 295, 2480, 1296, 1184, 661, 13, 50914, 50914, 407, 264, 659, 12, 17227, 1760, 5633, 321, 434, 516, 281, 360, 307, 516, 281, 312, 257, 10623, 294, 264, 8247, 82, 5633, 13, 51114, 51114, 407, 294, 341, 8174, 11, 286, 478, 516, 281, 6094, 484, 341, 1349, 294, 264, 2808, 11, 597, 307, 28122, 82, 11, 293, 853, 293, 6069, 437, 341, 1349, 820, 312, 2361, 322, 264, 4319, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14541425982725273, "compression_ratio": 1.6875, "no_speech_prob": 3.480211307760328e-05}, {"id": 560, "seek": 428700, "start": 4287.0, "end": 4299.0, "text": " And hope would be that words like unknown, silverhead, or horns will somehow are more likely to occur in the context of a unicorn than they are", "tokens": [50364, 400, 1454, 576, 312, 300, 2283, 411, 9841, 11, 8753, 1934, 11, 420, 28818, 486, 6063, 366, 544, 3700, 281, 5160, 294, 264, 4319, 295, 257, 28122, 813, 436, 366, 50964, 50964, 257, 1349, 411, 51164, 51164, 512, 661, 5496, 13, 51364, 51364, 407, 1936, 11, 341, 307, 516, 281, 312, 257, 588, 2199, 2316, 689, 1936, 321, 434, 516, 281, 747, 439, 613, 4319, 2283, 11, 321, 434, 445, 516, 281, 3079, 512, 8213, 22743, 281, 613, 293, 4471, 552, 439, 484, 294, 257, 6806, 2744, 4319, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.21407482188235047, "compression_ratio": 1.7012987012987013, "no_speech_prob": 8.52915400173515e-06}, {"id": 561, "seek": 428700, "start": 4299.0, "end": 4303.0, "text": " a word like", "tokens": [50364, 400, 1454, 576, 312, 300, 2283, 411, 9841, 11, 8753, 1934, 11, 420, 28818, 486, 6063, 366, 544, 3700, 281, 5160, 294, 264, 4319, 295, 257, 28122, 813, 436, 366, 50964, 50964, 257, 1349, 411, 51164, 51164, 512, 661, 5496, 13, 51364, 51364, 407, 1936, 11, 341, 307, 516, 281, 312, 257, 588, 2199, 2316, 689, 1936, 321, 434, 516, 281, 747, 439, 613, 4319, 2283, 11, 321, 434, 445, 516, 281, 3079, 512, 8213, 22743, 281, 613, 293, 4471, 552, 439, 484, 294, 257, 6806, 2744, 4319, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.21407482188235047, "compression_ratio": 1.7012987012987013, "no_speech_prob": 8.52915400173515e-06}, {"id": 562, "seek": 428700, "start": 4303.0, "end": 4307.0, "text": " some other animal.", "tokens": [50364, 400, 1454, 576, 312, 300, 2283, 411, 9841, 11, 8753, 1934, 11, 420, 28818, 486, 6063, 366, 544, 3700, 281, 5160, 294, 264, 4319, 295, 257, 28122, 813, 436, 366, 50964, 50964, 257, 1349, 411, 51164, 51164, 512, 661, 5496, 13, 51364, 51364, 407, 1936, 11, 341, 307, 516, 281, 312, 257, 588, 2199, 2316, 689, 1936, 321, 434, 516, 281, 747, 439, 613, 4319, 2283, 11, 321, 434, 445, 516, 281, 3079, 512, 8213, 22743, 281, 613, 293, 4471, 552, 439, 484, 294, 257, 6806, 2744, 4319, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.21407482188235047, "compression_ratio": 1.7012987012987013, "no_speech_prob": 8.52915400173515e-06}, {"id": 563, "seek": 428700, "start": 4307.0, "end": 4316.0, "text": " So basically, this is going to be a very simple model where basically we're going to take all these context words, we're just going to apply some linear projection to these and map them all out in a fixed size context.", "tokens": [50364, 400, 1454, 576, 312, 300, 2283, 411, 9841, 11, 8753, 1934, 11, 420, 28818, 486, 6063, 366, 544, 3700, 281, 5160, 294, 264, 4319, 295, 257, 28122, 813, 436, 366, 50964, 50964, 257, 1349, 411, 51164, 51164, 512, 661, 5496, 13, 51364, 51364, 407, 1936, 11, 341, 307, 516, 281, 312, 257, 588, 2199, 2316, 689, 1936, 321, 434, 516, 281, 747, 439, 613, 4319, 2283, 11, 321, 434, 445, 516, 281, 3079, 512, 8213, 22743, 281, 613, 293, 4471, 552, 439, 484, 294, 257, 6806, 2744, 4319, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.21407482188235047, "compression_ratio": 1.7012987012987013, "no_speech_prob": 8.52915400173515e-06}, {"id": 564, "seek": 431600, "start": 4316.0, "end": 4322.0, "text": " And then just do a softmax over a vocabulary.", "tokens": [50364, 400, 550, 445, 360, 257, 2787, 41167, 670, 257, 19864, 13, 50664, 50664, 407, 309, 1542, 257, 707, 857, 411, 257, 45216, 304, 2856, 2316, 13, 440, 787, 2649, 307, 321, 434, 32884, 264, 1349, 294, 264, 2808, 11, 406, 264, 1349, 412, 264, 917, 13, 51014, 51014, 400, 294, 3124, 11, 341, 2316, 390, 445, 257, 20488, 8213, 22743, 293, 390, 406, 257, 588, 2452, 2316, 13, 51464, 51464, 1033, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1896841149581106, "compression_ratio": 1.4974874371859297, "no_speech_prob": 1.2605682968569454e-05}, {"id": 565, "seek": 431600, "start": 4322.0, "end": 4329.0, "text": " So it looks a little bit like a convolutional language model. The only difference is we're predicting the word in the middle, not the word at the end.", "tokens": [50364, 400, 550, 445, 360, 257, 2787, 41167, 670, 257, 19864, 13, 50664, 50664, 407, 309, 1542, 257, 707, 857, 411, 257, 45216, 304, 2856, 2316, 13, 440, 787, 2649, 307, 321, 434, 32884, 264, 1349, 294, 264, 2808, 11, 406, 264, 1349, 412, 264, 917, 13, 51014, 51014, 400, 294, 3124, 11, 341, 2316, 390, 445, 257, 20488, 8213, 22743, 293, 390, 406, 257, 588, 2452, 2316, 13, 51464, 51464, 1033, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1896841149581106, "compression_ratio": 1.4974874371859297, "no_speech_prob": 1.2605682968569454e-05}, {"id": 566, "seek": 431600, "start": 4329.0, "end": 4338.0, "text": " And in practice, this model was just a shallow linear projection and was not a very deep model.", "tokens": [50364, 400, 550, 445, 360, 257, 2787, 41167, 670, 257, 19864, 13, 50664, 50664, 407, 309, 1542, 257, 707, 857, 411, 257, 45216, 304, 2856, 2316, 13, 440, 787, 2649, 307, 321, 434, 32884, 264, 1349, 294, 264, 2808, 11, 406, 264, 1349, 412, 264, 917, 13, 51014, 51014, 400, 294, 3124, 11, 341, 2316, 390, 445, 257, 20488, 8213, 22743, 293, 390, 406, 257, 588, 2452, 2316, 13, 51464, 51464, 1033, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1896841149581106, "compression_ratio": 1.4974874371859297, "no_speech_prob": 1.2605682968569454e-05}, {"id": 567, "seek": 431600, "start": 4338.0, "end": 4340.0, "text": " Okay.", "tokens": [50364, 400, 550, 445, 360, 257, 2787, 41167, 670, 257, 19864, 13, 50664, 50664, 407, 309, 1542, 257, 707, 857, 411, 257, 45216, 304, 2856, 2316, 13, 440, 787, 2649, 307, 321, 434, 32884, 264, 1349, 294, 264, 2808, 11, 406, 264, 1349, 412, 264, 917, 13, 51014, 51014, 400, 294, 3124, 11, 341, 2316, 390, 445, 257, 20488, 8213, 22743, 293, 390, 406, 257, 588, 2452, 2316, 13, 51464, 51464, 1033, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1896841149581106, "compression_ratio": 1.4974874371859297, "no_speech_prob": 1.2605682968569454e-05}, {"id": 568, "seek": 434000, "start": 4340.0, "end": 4350.0, "text": " So one of the things that were quite interesting about this was these word embeddings, which show some kind of surprising stretches to them.", "tokens": [50364, 407, 472, 295, 264, 721, 300, 645, 1596, 1880, 466, 341, 390, 613, 1349, 12240, 29432, 11, 597, 855, 512, 733, 295, 8830, 29058, 281, 552, 13, 50864, 50864, 286, 478, 988, 291, 603, 1568, 341, 1019, 11, 561, 7958, 466, 577, 10995, 341, 307, 13, 51064, 51064, 583, 1936, 11, 264, 3932, 390, 300, 498, 291, 1890, 428, 12240, 3584, 281, 264, 1349, 4867, 11, 597, 291, 3847, 411, 341, 11, 293, 291, 16390, 428, 12240, 3584, 281, 264, 1349, 587, 11, 550, 291, 909, 264, 12240, 3584, 281, 264, 1349, 3059, 11, 291, 603, 483, 746, 300, 311, 6457, 1998, 281, 264, 12240, 3584, 281, 264, 1349, 12206, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.30339461616847824, "compression_ratio": 1.92578125, "no_speech_prob": 8.347390394192189e-05}, {"id": 569, "seek": 434000, "start": 4350.0, "end": 4354.0, "text": " I'm sure you'll hear this fun, people debate about how meaningful this is.", "tokens": [50364, 407, 472, 295, 264, 721, 300, 645, 1596, 1880, 466, 341, 390, 613, 1349, 12240, 29432, 11, 597, 855, 512, 733, 295, 8830, 29058, 281, 552, 13, 50864, 50864, 286, 478, 988, 291, 603, 1568, 341, 1019, 11, 561, 7958, 466, 577, 10995, 341, 307, 13, 51064, 51064, 583, 1936, 11, 264, 3932, 390, 300, 498, 291, 1890, 428, 12240, 3584, 281, 264, 1349, 4867, 11, 597, 291, 3847, 411, 341, 11, 293, 291, 16390, 428, 12240, 3584, 281, 264, 1349, 587, 11, 550, 291, 909, 264, 12240, 3584, 281, 264, 1349, 3059, 11, 291, 603, 483, 746, 300, 311, 6457, 1998, 281, 264, 12240, 3584, 281, 264, 1349, 12206, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.30339461616847824, "compression_ratio": 1.92578125, "no_speech_prob": 8.347390394192189e-05}, {"id": 570, "seek": 434000, "start": 4354.0, "end": 4368.0, "text": " But basically, the claim was that if you took your embedding to the word king, which you train like this, and you subtract your embedding to the word man, then you add the embedding to the word woman, you'll get something that's fairly close to the embedding to the word queen.", "tokens": [50364, 407, 472, 295, 264, 721, 300, 645, 1596, 1880, 466, 341, 390, 613, 1349, 12240, 29432, 11, 597, 855, 512, 733, 295, 8830, 29058, 281, 552, 13, 50864, 50864, 286, 478, 988, 291, 603, 1568, 341, 1019, 11, 561, 7958, 466, 577, 10995, 341, 307, 13, 51064, 51064, 583, 1936, 11, 264, 3932, 390, 300, 498, 291, 1890, 428, 12240, 3584, 281, 264, 1349, 4867, 11, 597, 291, 3847, 411, 341, 11, 293, 291, 16390, 428, 12240, 3584, 281, 264, 1349, 587, 11, 550, 291, 909, 264, 12240, 3584, 281, 264, 1349, 3059, 11, 291, 603, 483, 746, 300, 311, 6457, 1998, 281, 264, 12240, 3584, 281, 264, 1349, 12206, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.30339461616847824, "compression_ratio": 1.92578125, "no_speech_prob": 8.347390394192189e-05}, {"id": 571, "seek": 436800, "start": 4368.0, "end": 4373.0, "text": " So somehow,", "tokens": [50364, 407, 6063, 11, 50614, 50614, 445, 341, 733, 295, 2693, 12879, 24420, 2836, 294, 264, 8247, 82, 2539, 5633, 50864, 50864, 1943, 380, 1228, 341, 733, 295, 8213, 3877, 365, 733, 295, 10995, 7300, 1296, 18875, 13, 51264, 51264, 1033, 11, 370, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.18620047361954398, "compression_ratio": 1.366412213740458, "no_speech_prob": 2.8386475605657324e-05}, {"id": 572, "seek": 436800, "start": 4373.0, "end": 4378.0, "text": " just this kind of unsupervised fill in the blanks learning task", "tokens": [50364, 407, 6063, 11, 50614, 50614, 445, 341, 733, 295, 2693, 12879, 24420, 2836, 294, 264, 8247, 82, 2539, 5633, 50864, 50864, 1943, 380, 1228, 341, 733, 295, 8213, 3877, 365, 733, 295, 10995, 7300, 1296, 18875, 13, 51264, 51264, 1033, 11, 370, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.18620047361954398, "compression_ratio": 1.366412213740458, "no_speech_prob": 2.8386475605657324e-05}, {"id": 573, "seek": 436800, "start": 4378.0, "end": 4386.0, "text": " isn't using this kind of linear structure with kind of meaningful differences between vectors.", "tokens": [50364, 407, 6063, 11, 50614, 50614, 445, 341, 733, 295, 2693, 12879, 24420, 2836, 294, 264, 8247, 82, 2539, 5633, 50864, 50864, 1943, 380, 1228, 341, 733, 295, 8213, 3877, 365, 733, 295, 10995, 7300, 1296, 18875, 13, 51264, 51264, 1033, 11, 370, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.18620047361954398, "compression_ratio": 1.366412213740458, "no_speech_prob": 2.8386475605657324e-05}, {"id": 574, "seek": 436800, "start": 4386.0, "end": 4389.0, "text": " Okay, so", "tokens": [50364, 407, 6063, 11, 50614, 50614, 445, 341, 733, 295, 2693, 12879, 24420, 2836, 294, 264, 8247, 82, 2539, 5633, 50864, 50864, 1943, 380, 1228, 341, 733, 295, 8213, 3877, 365, 733, 295, 10995, 7300, 1296, 18875, 13, 51264, 51264, 1033, 11, 370, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.18620047361954398, "compression_ratio": 1.366412213740458, "no_speech_prob": 2.8386475605657324e-05}, {"id": 575, "seek": 438900, "start": 4389.0, "end": 4400.0, "text": " I mean, this was great. And the really good thing about this was there's a really, really fast thing to do. So you could train this on billions of words of text back in 2013.", "tokens": [50364, 286, 914, 11, 341, 390, 869, 13, 400, 264, 534, 665, 551, 466, 341, 390, 456, 311, 257, 534, 11, 534, 2370, 551, 281, 360, 13, 407, 291, 727, 3847, 341, 322, 17375, 295, 2283, 295, 2487, 646, 294, 9012, 13, 50914, 50914, 583, 456, 311, 257, 955, 27432, 11, 597, 307, 613, 1349, 12240, 29432, 366, 6695, 295, 264, 4319, 13, 51264, 51264, 509, 483, 472, 8062, 680, 1349, 294, 2139, 264, 1410, 46367, 13, 51514, 51514, 583, 309, 1177, 380, 458, 1340, 466, 577, 300, 1349, 307, 7074, 322, 661, 2283, 13, 400, 321, 458, 300, 281, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19682647186575583, "compression_ratio": 1.7078189300411524, "no_speech_prob": 1.9830651581287384e-05}, {"id": 576, "seek": 438900, "start": 4400.0, "end": 4407.0, "text": " But there's a big limitation, which is these word embeddings are independent of the context.", "tokens": [50364, 286, 914, 11, 341, 390, 869, 13, 400, 264, 534, 665, 551, 466, 341, 390, 456, 311, 257, 534, 11, 534, 2370, 551, 281, 360, 13, 407, 291, 727, 3847, 341, 322, 17375, 295, 2283, 295, 2487, 646, 294, 9012, 13, 50914, 50914, 583, 456, 311, 257, 955, 27432, 11, 597, 307, 613, 1349, 12240, 29432, 366, 6695, 295, 264, 4319, 13, 51264, 51264, 509, 483, 472, 8062, 680, 1349, 294, 2139, 264, 1410, 46367, 13, 51514, 51514, 583, 309, 1177, 380, 458, 1340, 466, 577, 300, 1349, 307, 7074, 322, 661, 2283, 13, 400, 321, 458, 300, 281, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19682647186575583, "compression_ratio": 1.7078189300411524, "no_speech_prob": 1.9830651581287384e-05}, {"id": 577, "seek": 438900, "start": 4407.0, "end": 4412.0, "text": " You get one vector per word in either the capillary.", "tokens": [50364, 286, 914, 11, 341, 390, 869, 13, 400, 264, 534, 665, 551, 466, 341, 390, 456, 311, 257, 534, 11, 534, 2370, 551, 281, 360, 13, 407, 291, 727, 3847, 341, 322, 17375, 295, 2283, 295, 2487, 646, 294, 9012, 13, 50914, 50914, 583, 456, 311, 257, 955, 27432, 11, 597, 307, 613, 1349, 12240, 29432, 366, 6695, 295, 264, 4319, 13, 51264, 51264, 509, 483, 472, 8062, 680, 1349, 294, 2139, 264, 1410, 46367, 13, 51514, 51514, 583, 309, 1177, 380, 458, 1340, 466, 577, 300, 1349, 307, 7074, 322, 661, 2283, 13, 400, 321, 458, 300, 281, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19682647186575583, "compression_ratio": 1.7078189300411524, "no_speech_prob": 1.9830651581287384e-05}, {"id": 578, "seek": 438900, "start": 4412.0, "end": 4418.0, "text": " But it doesn't know anything about how that word is placed on other words. And we know that to", "tokens": [50364, 286, 914, 11, 341, 390, 869, 13, 400, 264, 534, 665, 551, 466, 341, 390, 456, 311, 257, 534, 11, 534, 2370, 551, 281, 360, 13, 407, 291, 727, 3847, 341, 322, 17375, 295, 2283, 295, 2487, 646, 294, 9012, 13, 50914, 50914, 583, 456, 311, 257, 955, 27432, 11, 597, 307, 613, 1349, 12240, 29432, 366, 6695, 295, 264, 4319, 13, 51264, 51264, 509, 483, 472, 8062, 680, 1349, 294, 2139, 264, 1410, 46367, 13, 51514, 51514, 583, 309, 1177, 380, 458, 1340, 466, 577, 300, 1349, 307, 7074, 322, 661, 2283, 13, 400, 321, 458, 300, 281, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19682647186575583, "compression_ratio": 1.7078189300411524, "no_speech_prob": 1.9830651581287384e-05}, {"id": 579, "seek": 441800, "start": 4418.0, "end": 4421.0, "text": " in language like", "tokens": [50364, 294, 2856, 411, 50514, 50514, 257, 8174, 307, 544, 813, 445, 257, 3411, 295, 2283, 11, 309, 5946, 11, 1184, 1349, 43582, 365, 661, 2283, 6063, 13, 50814, 50814, 400, 613, 13280, 366, 294, 512, 2098, 257, 534, 4005, 551, 13, 51014, 51014, 407, 294, 544, 5110, 11, 51164, 51164, 364, 6322, 1365, 307, 411, 39465, 2283, 13, 407, 3195, 295, 2283, 393, 362, 867, 819, 28138, 13, 400, 613, 1349, 18875, 1582, 380, 7983, 300, 51564, 51564, 420, 412, 1151, 11, 486, 445, 312, 257, 1003, 5830, 295, 439, 264, 28138, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.22214700757842704, "compression_ratio": 1.6962025316455696, "no_speech_prob": 2.8400976589182392e-05}, {"id": 580, "seek": 441800, "start": 4421.0, "end": 4427.0, "text": " a sentence is more than just a bag of words, it depends, each word interacts with other words somehow.", "tokens": [50364, 294, 2856, 411, 50514, 50514, 257, 8174, 307, 544, 813, 445, 257, 3411, 295, 2283, 11, 309, 5946, 11, 1184, 1349, 43582, 365, 661, 2283, 6063, 13, 50814, 50814, 400, 613, 13280, 366, 294, 512, 2098, 257, 534, 4005, 551, 13, 51014, 51014, 407, 294, 544, 5110, 11, 51164, 51164, 364, 6322, 1365, 307, 411, 39465, 2283, 13, 407, 3195, 295, 2283, 393, 362, 867, 819, 28138, 13, 400, 613, 1349, 18875, 1582, 380, 7983, 300, 51564, 51564, 420, 412, 1151, 11, 486, 445, 312, 257, 1003, 5830, 295, 439, 264, 28138, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.22214700757842704, "compression_ratio": 1.6962025316455696, "no_speech_prob": 2.8400976589182392e-05}, {"id": 581, "seek": 441800, "start": 4427.0, "end": 4431.0, "text": " And these interactions are in some ways a really powerful thing.", "tokens": [50364, 294, 2856, 411, 50514, 50514, 257, 8174, 307, 544, 813, 445, 257, 3411, 295, 2283, 11, 309, 5946, 11, 1184, 1349, 43582, 365, 661, 2283, 6063, 13, 50814, 50814, 400, 613, 13280, 366, 294, 512, 2098, 257, 534, 4005, 551, 13, 51014, 51014, 407, 294, 544, 5110, 11, 51164, 51164, 364, 6322, 1365, 307, 411, 39465, 2283, 13, 407, 3195, 295, 2283, 393, 362, 867, 819, 28138, 13, 400, 613, 1349, 18875, 1582, 380, 7983, 300, 51564, 51564, 420, 412, 1151, 11, 486, 445, 312, 257, 1003, 5830, 295, 439, 264, 28138, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.22214700757842704, "compression_ratio": 1.6962025316455696, "no_speech_prob": 2.8400976589182392e-05}, {"id": 582, "seek": 441800, "start": 4431.0, "end": 4434.0, "text": " So in more examples,", "tokens": [50364, 294, 2856, 411, 50514, 50514, 257, 8174, 307, 544, 813, 445, 257, 3411, 295, 2283, 11, 309, 5946, 11, 1184, 1349, 43582, 365, 661, 2283, 6063, 13, 50814, 50814, 400, 613, 13280, 366, 294, 512, 2098, 257, 534, 4005, 551, 13, 51014, 51014, 407, 294, 544, 5110, 11, 51164, 51164, 364, 6322, 1365, 307, 411, 39465, 2283, 13, 407, 3195, 295, 2283, 393, 362, 867, 819, 28138, 13, 400, 613, 1349, 18875, 1582, 380, 7983, 300, 51564, 51564, 420, 412, 1151, 11, 486, 445, 312, 257, 1003, 5830, 295, 439, 264, 28138, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.22214700757842704, "compression_ratio": 1.6962025316455696, "no_speech_prob": 2.8400976589182392e-05}, {"id": 583, "seek": 441800, "start": 4434.0, "end": 4442.0, "text": " an obvious example is like ambiguous words. So lots of words can have many different meanings. And these word vectors won't capture that", "tokens": [50364, 294, 2856, 411, 50514, 50514, 257, 8174, 307, 544, 813, 445, 257, 3411, 295, 2283, 11, 309, 5946, 11, 1184, 1349, 43582, 365, 661, 2283, 6063, 13, 50814, 50814, 400, 613, 13280, 366, 294, 512, 2098, 257, 534, 4005, 551, 13, 51014, 51014, 407, 294, 544, 5110, 11, 51164, 51164, 364, 6322, 1365, 307, 411, 39465, 2283, 13, 407, 3195, 295, 2283, 393, 362, 867, 819, 28138, 13, 400, 613, 1349, 18875, 1582, 380, 7983, 300, 51564, 51564, 420, 412, 1151, 11, 486, 445, 312, 257, 1003, 5830, 295, 439, 264, 28138, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.22214700757842704, "compression_ratio": 1.6962025316455696, "no_speech_prob": 2.8400976589182392e-05}, {"id": 584, "seek": 441800, "start": 4442.0, "end": 4447.0, "text": " or at best, will just be a supposition of all the meanings.", "tokens": [50364, 294, 2856, 411, 50514, 50514, 257, 8174, 307, 544, 813, 445, 257, 3411, 295, 2283, 11, 309, 5946, 11, 1184, 1349, 43582, 365, 661, 2283, 6063, 13, 50814, 50814, 400, 613, 13280, 366, 294, 512, 2098, 257, 534, 4005, 551, 13, 51014, 51014, 407, 294, 544, 5110, 11, 51164, 51164, 364, 6322, 1365, 307, 411, 39465, 2283, 13, 407, 3195, 295, 2283, 393, 362, 867, 819, 28138, 13, 400, 613, 1349, 18875, 1582, 380, 7983, 300, 51564, 51564, 420, 412, 1151, 11, 486, 445, 312, 257, 1003, 5830, 295, 439, 264, 28138, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.22214700757842704, "compression_ratio": 1.6962025316455696, "no_speech_prob": 2.8400976589182392e-05}, {"id": 585, "seek": 444700, "start": 4447.0, "end": 4453.0, "text": " So how do we add context to these? Well,", "tokens": [50364, 407, 577, 360, 321, 909, 4319, 281, 613, 30, 1042, 11, 50664, 50664, 264, 881, 6322, 636, 307, 281, 360, 257, 2856, 2316, 13, 286, 519, 286, 478, 5361, 257, 4137, 510, 13, 8537, 11, 51064, 51064, 437, 321, 360, 307, 3847, 257, 27708, 2856, 2316, 13, 4919, 11, 364, 47916, 2856, 2316, 300, 311, 516, 281, 312, 2293, 264, 733, 295, 2316, 286, 7619, 3071, 294, 341, 751, 13, 51514, 51514, 400, 550, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16545980404584837, "compression_ratio": 1.597938144329897, "no_speech_prob": 5.093241270515136e-06}, {"id": 586, "seek": 444700, "start": 4453.0, "end": 4461.0, "text": " the most obvious way is to do a language model. I think I'm missing a slide here. Basically,", "tokens": [50364, 407, 577, 360, 321, 909, 4319, 281, 613, 30, 1042, 11, 50664, 50664, 264, 881, 6322, 636, 307, 281, 360, 257, 2856, 2316, 13, 286, 519, 286, 478, 5361, 257, 4137, 510, 13, 8537, 11, 51064, 51064, 437, 321, 360, 307, 3847, 257, 27708, 2856, 2316, 13, 4919, 11, 364, 47916, 2856, 2316, 300, 311, 516, 281, 312, 2293, 264, 733, 295, 2316, 286, 7619, 3071, 294, 341, 751, 13, 51514, 51514, 400, 550, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16545980404584837, "compression_ratio": 1.597938144329897, "no_speech_prob": 5.093241270515136e-06}, {"id": 587, "seek": 444700, "start": 4461.0, "end": 4470.0, "text": " what we do is train a conditional language model. Sorry, an unconditional language model that's going to be exactly the kind of model I described earlier in this talk.", "tokens": [50364, 407, 577, 360, 321, 909, 4319, 281, 613, 30, 1042, 11, 50664, 50664, 264, 881, 6322, 636, 307, 281, 360, 257, 2856, 2316, 13, 286, 519, 286, 478, 5361, 257, 4137, 510, 13, 8537, 11, 51064, 51064, 437, 321, 360, 307, 3847, 257, 27708, 2856, 2316, 13, 4919, 11, 364, 47916, 2856, 2316, 300, 311, 516, 281, 312, 2293, 264, 733, 295, 2316, 286, 7619, 3071, 294, 341, 751, 13, 51514, 51514, 400, 550, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16545980404584837, "compression_ratio": 1.597938144329897, "no_speech_prob": 5.093241270515136e-06}, {"id": 588, "seek": 444700, "start": 4470.0, "end": 4473.0, "text": " And then", "tokens": [50364, 407, 577, 360, 321, 909, 4319, 281, 613, 30, 1042, 11, 50664, 50664, 264, 881, 6322, 636, 307, 281, 360, 257, 2856, 2316, 13, 286, 519, 286, 478, 5361, 257, 4137, 510, 13, 8537, 11, 51064, 51064, 437, 321, 360, 307, 3847, 257, 27708, 2856, 2316, 13, 4919, 11, 364, 47916, 2856, 2316, 300, 311, 516, 281, 312, 2293, 264, 733, 295, 2316, 286, 7619, 3071, 294, 341, 751, 13, 51514, 51514, 400, 550, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16545980404584837, "compression_ratio": 1.597938144329897, "no_speech_prob": 5.093241270515136e-06}, {"id": 589, "seek": 447300, "start": 4473.0, "end": 4481.0, "text": " given this language model, the language model will be outputting in the states every time step, predicting the next word.", "tokens": [50364, 2212, 341, 2856, 2316, 11, 264, 2856, 2316, 486, 312, 5598, 783, 294, 264, 4368, 633, 565, 1823, 11, 32884, 264, 958, 1349, 13, 50764, 50764, 400, 2602, 11, 562, 321, 528, 281, 3607, 46533, 2539, 11, 437, 321, 434, 516, 281, 360, 307, 7406, 613, 23930, 365, 51164, 51164, 512, 661, 5598, 300, 5946, 322, 527, 5633, 13, 407, 264, 659, 12, 17227, 1760, 5574, 307, 445, 516, 281, 312, 6069, 264, 958, 1349, 13, 51514, 51514, 583, 550, 733, 295, 264, 30086, 322, 264, 5908, 295, 46533, 2539, 486, 312, 264, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16687772691864328, "compression_ratio": 1.808695652173913, "no_speech_prob": 6.853432751086075e-06}, {"id": 590, "seek": 447300, "start": 4481.0, "end": 4489.0, "text": " And instead, when we want to sell supervised learning, what we're going to do is replace these outputs with", "tokens": [50364, 2212, 341, 2856, 2316, 11, 264, 2856, 2316, 486, 312, 5598, 783, 294, 264, 4368, 633, 565, 1823, 11, 32884, 264, 958, 1349, 13, 50764, 50764, 400, 2602, 11, 562, 321, 528, 281, 3607, 46533, 2539, 11, 437, 321, 434, 516, 281, 360, 307, 7406, 613, 23930, 365, 51164, 51164, 512, 661, 5598, 300, 5946, 322, 527, 5633, 13, 407, 264, 659, 12, 17227, 1760, 5574, 307, 445, 516, 281, 312, 6069, 264, 958, 1349, 13, 51514, 51514, 583, 550, 733, 295, 264, 30086, 322, 264, 5908, 295, 46533, 2539, 486, 312, 264, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16687772691864328, "compression_ratio": 1.808695652173913, "no_speech_prob": 6.853432751086075e-06}, {"id": 591, "seek": 447300, "start": 4489.0, "end": 4496.0, "text": " some other output that depends on our task. So the pre-training phase is just going to be predict the next word.", "tokens": [50364, 2212, 341, 2856, 2316, 11, 264, 2856, 2316, 486, 312, 5598, 783, 294, 264, 4368, 633, 565, 1823, 11, 32884, 264, 958, 1349, 13, 50764, 50764, 400, 2602, 11, 562, 321, 528, 281, 3607, 46533, 2539, 11, 437, 321, 434, 516, 281, 360, 307, 7406, 613, 23930, 365, 51164, 51164, 512, 661, 5598, 300, 5946, 322, 527, 5633, 13, 407, 264, 659, 12, 17227, 1760, 5574, 307, 445, 516, 281, 312, 6069, 264, 958, 1349, 13, 51514, 51514, 583, 550, 733, 295, 264, 30086, 322, 264, 5908, 295, 46533, 2539, 486, 312, 264, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16687772691864328, "compression_ratio": 1.808695652173913, "no_speech_prob": 6.853432751086075e-06}, {"id": 592, "seek": 447300, "start": 4496.0, "end": 4502.0, "text": " But then kind of the icing on the cake of supervised learning will be the", "tokens": [50364, 2212, 341, 2856, 2316, 11, 264, 2856, 2316, 486, 312, 5598, 783, 294, 264, 4368, 633, 565, 1823, 11, 32884, 264, 958, 1349, 13, 50764, 50764, 400, 2602, 11, 562, 321, 528, 281, 3607, 46533, 2539, 11, 437, 321, 434, 516, 281, 360, 307, 7406, 613, 23930, 365, 51164, 51164, 512, 661, 5598, 300, 5946, 322, 527, 5633, 13, 407, 264, 659, 12, 17227, 1760, 5574, 307, 445, 516, 281, 312, 6069, 264, 958, 1349, 13, 51514, 51514, 583, 550, 733, 295, 264, 30086, 322, 264, 5908, 295, 46533, 2539, 486, 312, 264, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16687772691864328, "compression_ratio": 1.808695652173913, "no_speech_prob": 6.853432751086075e-06}, {"id": 593, "seek": 450200, "start": 4502.0, "end": 4513.0, "text": " predict some other property. So I'll show you an example here for a task called path speech tagging, which is trying to say put some labels in every word here, so turn a light label.", "tokens": [50364, 6069, 512, 661, 4707, 13, 407, 286, 603, 855, 291, 364, 1365, 510, 337, 257, 5633, 1219, 3100, 6218, 6162, 3249, 11, 597, 307, 1382, 281, 584, 829, 512, 16949, 294, 633, 1349, 510, 11, 370, 1261, 257, 1442, 7645, 13, 50914, 50914, 18944, 468, 293, 23307, 293, 27766, 382, 364, 44129, 13, 51114, 51114, 583, 291, 393, 767, 3318, 439, 3685, 295, 9608, 666, 341, 733, 295, 8388, 13, 51414, 51414, 407, 11, 337, 1365, 11, 1310, 291, 393, 3318, 512, 2856, 411, 341, 307, 257, 16149, 5215, 5633, 689, 291, 434, 2902, 512, 2487, 281, 6069, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.25839749504538145, "compression_ratio": 1.6390977443609023, "no_speech_prob": 1.9829180018859915e-05}, {"id": 594, "seek": 450200, "start": 4513.0, "end": 4517.0, "text": " Scientist and noun and distinctive as an adjective.", "tokens": [50364, 6069, 512, 661, 4707, 13, 407, 286, 603, 855, 291, 364, 1365, 510, 337, 257, 5633, 1219, 3100, 6218, 6162, 3249, 11, 597, 307, 1382, 281, 584, 829, 512, 16949, 294, 633, 1349, 510, 11, 370, 1261, 257, 1442, 7645, 13, 50914, 50914, 18944, 468, 293, 23307, 293, 27766, 382, 364, 44129, 13, 51114, 51114, 583, 291, 393, 767, 3318, 439, 3685, 295, 9608, 666, 341, 733, 295, 8388, 13, 51414, 51414, 407, 11, 337, 1365, 11, 1310, 291, 393, 3318, 512, 2856, 411, 341, 307, 257, 16149, 5215, 5633, 689, 291, 434, 2902, 512, 2487, 281, 6069, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.25839749504538145, "compression_ratio": 1.6390977443609023, "no_speech_prob": 1.9829180018859915e-05}, {"id": 595, "seek": 450200, "start": 4517.0, "end": 4523.0, "text": " But you can actually fit all kinds of tasks into this kind of framework.", "tokens": [50364, 6069, 512, 661, 4707, 13, 407, 286, 603, 855, 291, 364, 1365, 510, 337, 257, 5633, 1219, 3100, 6218, 6162, 3249, 11, 597, 307, 1382, 281, 584, 829, 512, 16949, 294, 633, 1349, 510, 11, 370, 1261, 257, 1442, 7645, 13, 50914, 50914, 18944, 468, 293, 23307, 293, 27766, 382, 364, 44129, 13, 51114, 51114, 583, 291, 393, 767, 3318, 439, 3685, 295, 9608, 666, 341, 733, 295, 8388, 13, 51414, 51414, 407, 11, 337, 1365, 11, 1310, 291, 393, 3318, 512, 2856, 411, 341, 307, 257, 16149, 5215, 5633, 689, 291, 434, 2902, 512, 2487, 281, 6069, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.25839749504538145, "compression_ratio": 1.6390977443609023, "no_speech_prob": 1.9829180018859915e-05}, {"id": 596, "seek": 450200, "start": 4523.0, "end": 4530.0, "text": " So, for example, maybe you can fit some language like this is a sentiment analysis task where you're giving some text to predict", "tokens": [50364, 6069, 512, 661, 4707, 13, 407, 286, 603, 855, 291, 364, 1365, 510, 337, 257, 5633, 1219, 3100, 6218, 6162, 3249, 11, 597, 307, 1382, 281, 584, 829, 512, 16949, 294, 633, 1349, 510, 11, 370, 1261, 257, 1442, 7645, 13, 50914, 50914, 18944, 468, 293, 23307, 293, 27766, 382, 364, 44129, 13, 51114, 51114, 583, 291, 393, 767, 3318, 439, 3685, 295, 9608, 666, 341, 733, 295, 8388, 13, 51414, 51414, 407, 11, 337, 1365, 11, 1310, 291, 393, 3318, 512, 2856, 411, 341, 307, 257, 16149, 5215, 5633, 689, 291, 434, 2902, 512, 2487, 281, 6069, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.25839749504538145, "compression_ratio": 1.6390977443609023, "no_speech_prob": 1.9829180018859915e-05}, {"id": 597, "seek": 453000, "start": 4530.0, "end": 4537.0, "text": " from an Amazon review and predict the rating.", "tokens": [50364, 490, 364, 6795, 3131, 293, 6069, 264, 10990, 13, 50714, 50714, 639, 307, 257, 3131, 300, 1619, 437, 393, 286, 584, 466, 341, 14194, 12377, 260, 300, 6132, 380, 1217, 668, 848, 466, 264, 5589, 3435, 299, 373, 259, 420, 264, 7252, 13, 51064, 51064, 400, 341, 3131, 658, 1732, 6105, 13, 51314, 51314, 407, 510, 321, 434, 516, 281, 6069, 472, 5598, 490, 341, 2856, 2316, 11, 597, 307, 516, 281, 312, 264, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.17609778428689027, "compression_ratio": 1.5527638190954773, "no_speech_prob": 7.4100771598750725e-06}, {"id": 598, "seek": 453000, "start": 4537.0, "end": 4544.0, "text": " This is a review that says what can I say about this banana slicer that hasn't already been said about the wheel penicillin or the iPhone.", "tokens": [50364, 490, 364, 6795, 3131, 293, 6069, 264, 10990, 13, 50714, 50714, 639, 307, 257, 3131, 300, 1619, 437, 393, 286, 584, 466, 341, 14194, 12377, 260, 300, 6132, 380, 1217, 668, 848, 466, 264, 5589, 3435, 299, 373, 259, 420, 264, 7252, 13, 51064, 51064, 400, 341, 3131, 658, 1732, 6105, 13, 51314, 51314, 407, 510, 321, 434, 516, 281, 6069, 472, 5598, 490, 341, 2856, 2316, 11, 597, 307, 516, 281, 312, 264, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.17609778428689027, "compression_ratio": 1.5527638190954773, "no_speech_prob": 7.4100771598750725e-06}, {"id": 599, "seek": 453000, "start": 4544.0, "end": 4549.0, "text": " And this review got five stars.", "tokens": [50364, 490, 364, 6795, 3131, 293, 6069, 264, 10990, 13, 50714, 50714, 639, 307, 257, 3131, 300, 1619, 437, 393, 286, 584, 466, 341, 14194, 12377, 260, 300, 6132, 380, 1217, 668, 848, 466, 264, 5589, 3435, 299, 373, 259, 420, 264, 7252, 13, 51064, 51064, 400, 341, 3131, 658, 1732, 6105, 13, 51314, 51314, 407, 510, 321, 434, 516, 281, 6069, 472, 5598, 490, 341, 2856, 2316, 11, 597, 307, 516, 281, 312, 264, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.17609778428689027, "compression_ratio": 1.5527638190954773, "no_speech_prob": 7.4100771598750725e-06}, {"id": 600, "seek": 453000, "start": 4549.0, "end": 4554.0, "text": " So here we're going to predict one output from this language model, which is going to be the", "tokens": [50364, 490, 364, 6795, 3131, 293, 6069, 264, 10990, 13, 50714, 50714, 639, 307, 257, 3131, 300, 1619, 437, 393, 286, 584, 466, 341, 14194, 12377, 260, 300, 6132, 380, 1217, 668, 848, 466, 264, 5589, 3435, 299, 373, 259, 420, 264, 7252, 13, 51064, 51064, 400, 341, 3131, 658, 1732, 6105, 13, 51314, 51314, 407, 510, 321, 434, 516, 281, 6069, 472, 5598, 490, 341, 2856, 2316, 11, 597, 307, 516, 281, 312, 264, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.17609778428689027, "compression_ratio": 1.5527638190954773, "no_speech_prob": 7.4100771598750725e-06}, {"id": 601, "seek": 455400, "start": 4554.0, "end": 4562.0, "text": " a token at the end, which is some kind of task specific label.", "tokens": [50364, 257, 14862, 412, 264, 917, 11, 597, 307, 512, 733, 295, 5633, 2685, 7645, 13, 50764, 50764, 407, 472, 295, 264, 534, 1481, 721, 466, 341, 3109, 1219, 26039, 51, 390, 300, 309, 51164, 51164, 733, 295, 20308, 1500, 2685, 15983, 13, 407, 586, 5800, 321, 362, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.21099559933531517, "compression_ratio": 1.4335664335664335, "no_speech_prob": 1.8447231013851706e-06}, {"id": 602, "seek": 455400, "start": 4562.0, "end": 4570.0, "text": " So one of the really nice things about this approach called GPT was that it", "tokens": [50364, 257, 14862, 412, 264, 917, 11, 597, 307, 512, 733, 295, 5633, 2685, 7645, 13, 50764, 50764, 407, 472, 295, 264, 534, 1481, 721, 466, 341, 3109, 1219, 26039, 51, 390, 300, 309, 51164, 51164, 733, 295, 20308, 1500, 2685, 15983, 13, 407, 586, 5800, 321, 362, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.21099559933531517, "compression_ratio": 1.4335664335664335, "no_speech_prob": 1.8447231013851706e-06}, {"id": 603, "seek": 455400, "start": 4570.0, "end": 4576.0, "text": " kind of eliminated test specific modeling. So now suddenly we have", "tokens": [50364, 257, 14862, 412, 264, 917, 11, 597, 307, 512, 733, 295, 5633, 2685, 7645, 13, 50764, 50764, 407, 472, 295, 264, 534, 1481, 721, 466, 341, 3109, 1219, 26039, 51, 390, 300, 309, 51164, 51164, 733, 295, 20308, 1500, 2685, 15983, 13, 407, 586, 5800, 321, 362, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.21099559933531517, "compression_ratio": 1.4335664335664335, "no_speech_prob": 1.8447231013851706e-06}, {"id": 604, "seek": 457600, "start": 4576.0, "end": 4585.0, "text": " one model which we can pre train and we can now fine tune this model to do basically any task we want to do.", "tokens": [50364, 472, 2316, 597, 321, 393, 659, 3847, 293, 321, 393, 586, 2489, 10864, 341, 2316, 281, 360, 1936, 604, 5633, 321, 528, 281, 360, 13, 50814, 50814, 407, 309, 11626, 21538, 13, 51064, 51064, 407, 949, 341, 11, 456, 390, 767, 257, 1326, 924, 562, 561, 645, 2390, 439, 613, 3219, 6331, 1303, 11, 370, 291, 1322, 257, 819, 9482, 281, 360, 13, 51414, 51414, 11875, 257, 1168, 13430, 2316, 420, 281, 360, 257, 16149, 5215, 2316, 420, 746, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.2523915654137021, "compression_ratio": 1.7096774193548387, "no_speech_prob": 2.1763979020761326e-05}, {"id": 605, "seek": 457600, "start": 4585.0, "end": 4590.0, "text": " So it involves classification.", "tokens": [50364, 472, 2316, 597, 321, 393, 659, 3847, 293, 321, 393, 586, 2489, 10864, 341, 2316, 281, 360, 1936, 604, 5633, 321, 528, 281, 360, 13, 50814, 50814, 407, 309, 11626, 21538, 13, 51064, 51064, 407, 949, 341, 11, 456, 390, 767, 257, 1326, 924, 562, 561, 645, 2390, 439, 613, 3219, 6331, 1303, 11, 370, 291, 1322, 257, 819, 9482, 281, 360, 13, 51414, 51414, 11875, 257, 1168, 13430, 2316, 420, 281, 360, 257, 16149, 5215, 2316, 420, 746, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.2523915654137021, "compression_ratio": 1.7096774193548387, "no_speech_prob": 2.1763979020761326e-05}, {"id": 606, "seek": 457600, "start": 4590.0, "end": 4597.0, "text": " So before this, there was actually a few years when people were building all these crazy architectures, so you build a different architecture to do.", "tokens": [50364, 472, 2316, 597, 321, 393, 659, 3847, 293, 321, 393, 586, 2489, 10864, 341, 2316, 281, 360, 1936, 604, 5633, 321, 528, 281, 360, 13, 50814, 50814, 407, 309, 11626, 21538, 13, 51064, 51064, 407, 949, 341, 11, 456, 390, 767, 257, 1326, 924, 562, 561, 645, 2390, 439, 613, 3219, 6331, 1303, 11, 370, 291, 1322, 257, 819, 9482, 281, 360, 13, 51414, 51414, 11875, 257, 1168, 13430, 2316, 420, 281, 360, 257, 16149, 5215, 2316, 420, 746, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.2523915654137021, "compression_ratio": 1.7096774193548387, "no_speech_prob": 2.1763979020761326e-05}, {"id": 607, "seek": 457600, "start": 4597.0, "end": 4603.0, "text": " Build a question answering model or to do a sentiment analysis model or something.", "tokens": [50364, 472, 2316, 597, 321, 393, 659, 3847, 293, 321, 393, 586, 2489, 10864, 341, 2316, 281, 360, 1936, 604, 5633, 321, 528, 281, 360, 13, 50814, 50814, 407, 309, 11626, 21538, 13, 51064, 51064, 407, 949, 341, 11, 456, 390, 767, 257, 1326, 924, 562, 561, 645, 2390, 439, 613, 3219, 6331, 1303, 11, 370, 291, 1322, 257, 819, 9482, 281, 360, 13, 51414, 51414, 11875, 257, 1168, 13430, 2316, 420, 281, 360, 257, 16149, 5215, 2316, 420, 746, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.2523915654137021, "compression_ratio": 1.7096774193548387, "no_speech_prob": 2.1763979020761326e-05}, {"id": 608, "seek": 460300, "start": 4603.0, "end": 4609.0, "text": " Now you train pre train one big model and then it's really easy to fine tune it to do whatever you like.", "tokens": [50364, 823, 291, 3847, 659, 3847, 472, 955, 2316, 293, 550, 309, 311, 534, 1858, 281, 2489, 10864, 309, 281, 360, 2035, 291, 411, 13, 50664, 50664, 407, 300, 390, 257, 534, 955, 1823, 2128, 13, 50914, 50914, 8590, 11, 300, 2316, 575, 733, 295, 264, 6322, 27432, 13, 286, 848, 264, 1021, 551, 281, 360, 390, 733, 295, 35526, 1125, 2283, 13, 51214, 51214, 407, 411, 718, 2283, 458, 13, 51364, 51364, 11875, 1349, 33358, 5413, 322, 264, 4319, 13, 51514, 51514, 583, 498, 291, 659, 3847, 445, 2856, 2316, 11, 291, 393, 787, 534, 4188, 322, 428, 1411, 365, 4319, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16042191127561173, "compression_ratio": 1.7192307692307693, "no_speech_prob": 8.663809239806142e-06}, {"id": 609, "seek": 460300, "start": 4609.0, "end": 4614.0, "text": " So that was a really big step forward.", "tokens": [50364, 823, 291, 3847, 659, 3847, 472, 955, 2316, 293, 550, 309, 311, 534, 1858, 281, 2489, 10864, 309, 281, 360, 2035, 291, 411, 13, 50664, 50664, 407, 300, 390, 257, 534, 955, 1823, 2128, 13, 50914, 50914, 8590, 11, 300, 2316, 575, 733, 295, 264, 6322, 27432, 13, 286, 848, 264, 1021, 551, 281, 360, 390, 733, 295, 35526, 1125, 2283, 13, 51214, 51214, 407, 411, 718, 2283, 458, 13, 51364, 51364, 11875, 1349, 33358, 5413, 322, 264, 4319, 13, 51514, 51514, 583, 498, 291, 659, 3847, 445, 2856, 2316, 11, 291, 393, 787, 534, 4188, 322, 428, 1411, 365, 4319, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16042191127561173, "compression_ratio": 1.7192307692307693, "no_speech_prob": 8.663809239806142e-06}, {"id": 610, "seek": 460300, "start": 4614.0, "end": 4620.0, "text": " Unfortunately, that model has kind of the obvious limitation. I said the important thing to do was kind of contextualize words.", "tokens": [50364, 823, 291, 3847, 659, 3847, 472, 955, 2316, 293, 550, 309, 311, 534, 1858, 281, 2489, 10864, 309, 281, 360, 2035, 291, 411, 13, 50664, 50664, 407, 300, 390, 257, 534, 955, 1823, 2128, 13, 50914, 50914, 8590, 11, 300, 2316, 575, 733, 295, 264, 6322, 27432, 13, 286, 848, 264, 1021, 551, 281, 360, 390, 733, 295, 35526, 1125, 2283, 13, 51214, 51214, 407, 411, 718, 2283, 458, 13, 51364, 51364, 11875, 1349, 33358, 5413, 322, 264, 4319, 13, 51514, 51514, 583, 498, 291, 659, 3847, 445, 2856, 2316, 11, 291, 393, 787, 534, 4188, 322, 428, 1411, 365, 4319, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16042191127561173, "compression_ratio": 1.7192307692307693, "no_speech_prob": 8.663809239806142e-06}, {"id": 611, "seek": 460300, "start": 4620.0, "end": 4623.0, "text": " So like let words know.", "tokens": [50364, 823, 291, 3847, 659, 3847, 472, 955, 2316, 293, 550, 309, 311, 534, 1858, 281, 2489, 10864, 309, 281, 360, 2035, 291, 411, 13, 50664, 50664, 407, 300, 390, 257, 534, 955, 1823, 2128, 13, 50914, 50914, 8590, 11, 300, 2316, 575, 733, 295, 264, 6322, 27432, 13, 286, 848, 264, 1021, 551, 281, 360, 390, 733, 295, 35526, 1125, 2283, 13, 51214, 51214, 407, 411, 718, 2283, 458, 13, 51364, 51364, 11875, 1349, 33358, 5413, 322, 264, 4319, 13, 51514, 51514, 583, 498, 291, 659, 3847, 445, 2856, 2316, 11, 291, 393, 787, 534, 4188, 322, 428, 1411, 365, 4319, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16042191127561173, "compression_ratio": 1.7192307692307693, "no_speech_prob": 8.663809239806142e-06}, {"id": 612, "seek": 460300, "start": 4623.0, "end": 4626.0, "text": " Build word representations depending on the context.", "tokens": [50364, 823, 291, 3847, 659, 3847, 472, 955, 2316, 293, 550, 309, 311, 534, 1858, 281, 2489, 10864, 309, 281, 360, 2035, 291, 411, 13, 50664, 50664, 407, 300, 390, 257, 534, 955, 1823, 2128, 13, 50914, 50914, 8590, 11, 300, 2316, 575, 733, 295, 264, 6322, 27432, 13, 286, 848, 264, 1021, 551, 281, 360, 390, 733, 295, 35526, 1125, 2283, 13, 51214, 51214, 407, 411, 718, 2283, 458, 13, 51364, 51364, 11875, 1349, 33358, 5413, 322, 264, 4319, 13, 51514, 51514, 583, 498, 291, 659, 3847, 445, 2856, 2316, 11, 291, 393, 787, 534, 4188, 322, 428, 1411, 365, 4319, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16042191127561173, "compression_ratio": 1.7192307692307693, "no_speech_prob": 8.663809239806142e-06}, {"id": 613, "seek": 460300, "start": 4626.0, "end": 4632.0, "text": " But if you pre train just language model, you can only really condition on your left with context.", "tokens": [50364, 823, 291, 3847, 659, 3847, 472, 955, 2316, 293, 550, 309, 311, 534, 1858, 281, 2489, 10864, 309, 281, 360, 2035, 291, 411, 13, 50664, 50664, 407, 300, 390, 257, 534, 955, 1823, 2128, 13, 50914, 50914, 8590, 11, 300, 2316, 575, 733, 295, 264, 6322, 27432, 13, 286, 848, 264, 1021, 551, 281, 360, 390, 733, 295, 35526, 1125, 2283, 13, 51214, 51214, 407, 411, 718, 2283, 458, 13, 51364, 51364, 11875, 1349, 33358, 5413, 322, 264, 4319, 13, 51514, 51514, 583, 498, 291, 659, 3847, 445, 2856, 2316, 11, 291, 393, 787, 534, 4188, 322, 428, 1411, 365, 4319, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16042191127561173, "compression_ratio": 1.7192307692307693, "no_speech_prob": 8.663809239806142e-06}, {"id": 614, "seek": 463200, "start": 4632.0, "end": 4645.0, "text": " So your representation for each word necessarily can't depend on the representation for any future words.", "tokens": [50364, 407, 428, 10290, 337, 1184, 1349, 4725, 393, 380, 5672, 322, 264, 10290, 337, 604, 2027, 2283, 13, 51014, 51014, 400, 300, 733, 295, 10406, 437, 264, 2316, 393, 360, 1596, 257, 688, 13, 51164, 51164, 821, 311, 472, 733, 295, 6322, 3191, 281, 300, 11, 597, 307, 264, 3109, 2726, 538, 38722, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11186234704379378, "compression_ratio": 1.4938271604938271, "no_speech_prob": 1.2995955330552533e-05}, {"id": 615, "seek": 463200, "start": 4645.0, "end": 4648.0, "text": " And that kind of limits what the model can do quite a lot.", "tokens": [50364, 407, 428, 10290, 337, 1184, 1349, 4725, 393, 380, 5672, 322, 264, 10290, 337, 604, 2027, 2283, 13, 51014, 51014, 400, 300, 733, 295, 10406, 437, 264, 2316, 393, 360, 1596, 257, 688, 13, 51164, 51164, 821, 311, 472, 733, 295, 6322, 3191, 281, 300, 11, 597, 307, 264, 3109, 2726, 538, 38722, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11186234704379378, "compression_ratio": 1.4938271604938271, "no_speech_prob": 1.2995955330552533e-05}, {"id": 616, "seek": 463200, "start": 4648.0, "end": 4655.0, "text": " There's one kind of obvious fix to that, which is the approach taken by Elmo.", "tokens": [50364, 407, 428, 10290, 337, 1184, 1349, 4725, 393, 380, 5672, 322, 264, 10290, 337, 604, 2027, 2283, 13, 51014, 51014, 400, 300, 733, 295, 10406, 437, 264, 2316, 393, 360, 1596, 257, 688, 13, 51164, 51164, 821, 311, 472, 733, 295, 6322, 3191, 281, 300, 11, 597, 307, 264, 3109, 2726, 538, 38722, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11186234704379378, "compression_ratio": 1.4938271604938271, "no_speech_prob": 1.2995955330552533e-05}, {"id": 617, "seek": 465500, "start": 4655.0, "end": 4665.0, "text": " So Elmo runs training one left right language model, also training the second language model, which operates in the reverse direction.", "tokens": [50364, 407, 38722, 6676, 3097, 472, 1411, 558, 2856, 2316, 11, 611, 3097, 264, 1150, 2856, 2316, 11, 597, 22577, 294, 264, 9943, 3513, 13, 50864, 50864, 407, 341, 307, 411, 264, 1036, 1349, 294, 264, 4166, 293, 309, 5965, 3097, 264, 3894, 2306, 13, 51164, 51164, 400, 550, 291, 483, 1349, 10290, 538, 1588, 7186, 990, 264, 5598, 7914, 295, 428, 1411, 558, 2316, 293, 428, 558, 1411, 2316, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.19753708710541595, "compression_ratio": 1.7336683417085428, "no_speech_prob": 5.68210452911444e-06}, {"id": 618, "seek": 465500, "start": 4665.0, "end": 4671.0, "text": " So this is like the last word in the document and it keeps training the previous ones.", "tokens": [50364, 407, 38722, 6676, 3097, 472, 1411, 558, 2856, 2316, 11, 611, 3097, 264, 1150, 2856, 2316, 11, 597, 22577, 294, 264, 9943, 3513, 13, 50864, 50864, 407, 341, 307, 411, 264, 1036, 1349, 294, 264, 4166, 293, 309, 5965, 3097, 264, 3894, 2306, 13, 51164, 51164, 400, 550, 291, 483, 1349, 10290, 538, 1588, 7186, 990, 264, 5598, 7914, 295, 428, 1411, 558, 2316, 293, 428, 558, 1411, 2316, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.19753708710541595, "compression_ratio": 1.7336683417085428, "no_speech_prob": 5.68210452911444e-06}, {"id": 619, "seek": 465500, "start": 4671.0, "end": 4681.0, "text": " And then you get word representation by concatenating the output layers of your left right model and your right left model.", "tokens": [50364, 407, 38722, 6676, 3097, 472, 1411, 558, 2856, 2316, 11, 611, 3097, 264, 1150, 2856, 2316, 11, 597, 22577, 294, 264, 9943, 3513, 13, 50864, 50864, 407, 341, 307, 411, 264, 1036, 1349, 294, 264, 4166, 293, 309, 5965, 3097, 264, 3894, 2306, 13, 51164, 51164, 400, 550, 291, 483, 1349, 10290, 538, 1588, 7186, 990, 264, 5598, 7914, 295, 428, 1411, 558, 2316, 293, 428, 558, 1411, 2316, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.19753708710541595, "compression_ratio": 1.7336683417085428, "no_speech_prob": 5.68210452911444e-06}, {"id": 620, "seek": 468100, "start": 4681.0, "end": 4692.0, "text": " So this model is in some ways better in that now your word representations can't condition on the left with context and on the right with context.", "tokens": [50364, 407, 341, 2316, 307, 294, 512, 2098, 1101, 294, 300, 586, 428, 1349, 33358, 393, 380, 4188, 322, 264, 1411, 365, 4319, 293, 322, 264, 558, 365, 4319, 13, 50914, 50914, 400, 300, 311, 534, 4961, 337, 3195, 295, 9608, 13, 51114, 51114, 583, 309, 311, 920, 733, 295, 5567, 294, 300, 291, 500, 380, 534, 2316, 264, 13280, 294, 613, 30628, 13, 51364, 51364, 407, 613, 291, 445, 483, 341, 20488, 1588, 7186, 399, 295, 1411, 10290, 293, 558, 10290, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.2062524529390557, "compression_ratio": 1.808411214953271, "no_speech_prob": 1.0129520887858234e-05}, {"id": 621, "seek": 468100, "start": 4692.0, "end": 4696.0, "text": " And that's really helpful for lots of tasks.", "tokens": [50364, 407, 341, 2316, 307, 294, 512, 2098, 1101, 294, 300, 586, 428, 1349, 33358, 393, 380, 4188, 322, 264, 1411, 365, 4319, 293, 322, 264, 558, 365, 4319, 13, 50914, 50914, 400, 300, 311, 534, 4961, 337, 3195, 295, 9608, 13, 51114, 51114, 583, 309, 311, 920, 733, 295, 5567, 294, 300, 291, 500, 380, 534, 2316, 264, 13280, 294, 613, 30628, 13, 51364, 51364, 407, 613, 291, 445, 483, 341, 20488, 1588, 7186, 399, 295, 1411, 10290, 293, 558, 10290, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.2062524529390557, "compression_ratio": 1.808411214953271, "no_speech_prob": 1.0129520887858234e-05}, {"id": 622, "seek": 468100, "start": 4696.0, "end": 4701.0, "text": " But it's still kind of limited in that you don't really model the interactions in these contexts.", "tokens": [50364, 407, 341, 2316, 307, 294, 512, 2098, 1101, 294, 300, 586, 428, 1349, 33358, 393, 380, 4188, 322, 264, 1411, 365, 4319, 293, 322, 264, 558, 365, 4319, 13, 50914, 50914, 400, 300, 311, 534, 4961, 337, 3195, 295, 9608, 13, 51114, 51114, 583, 309, 311, 920, 733, 295, 5567, 294, 300, 291, 500, 380, 534, 2316, 264, 13280, 294, 613, 30628, 13, 51364, 51364, 407, 613, 291, 445, 483, 341, 20488, 1588, 7186, 399, 295, 1411, 10290, 293, 558, 10290, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.2062524529390557, "compression_ratio": 1.808411214953271, "no_speech_prob": 1.0129520887858234e-05}, {"id": 623, "seek": 468100, "start": 4701.0, "end": 4708.0, "text": " So these you just get this shallow concatenation of left representation and right representation.", "tokens": [50364, 407, 341, 2316, 307, 294, 512, 2098, 1101, 294, 300, 586, 428, 1349, 33358, 393, 380, 4188, 322, 264, 1411, 365, 4319, 293, 322, 264, 558, 365, 4319, 13, 50914, 50914, 400, 300, 311, 534, 4961, 337, 3195, 295, 9608, 13, 51114, 51114, 583, 309, 311, 920, 733, 295, 5567, 294, 300, 291, 500, 380, 534, 2316, 264, 13280, 294, 613, 30628, 13, 51364, 51364, 407, 613, 291, 445, 483, 341, 20488, 1588, 7186, 399, 295, 1411, 10290, 293, 558, 10290, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.2062524529390557, "compression_ratio": 1.808411214953271, "no_speech_prob": 1.0129520887858234e-05}, {"id": 624, "seek": 470800, "start": 4708.0, "end": 4715.0, "text": " And what you realize to do is have kind of like rich interactions between left context and the right context.", "tokens": [50364, 400, 437, 291, 4325, 281, 360, 307, 362, 733, 295, 411, 4593, 13280, 1296, 1411, 4319, 293, 264, 558, 4319, 13, 50714, 50714, 400, 439, 341, 5607, 385, 281, 363, 31479, 11, 597, 307, 1310, 291, 600, 2198, 295, 11, 597, 575, 1027, 257, 588, 955, 2649, 294, 38095, 13, 51514, 51514, 363, 31479, 767, 1542, 1596, 257, 688, 411, 8725, 17, 53, 3045, 13, 51614, 51614, 467, 311, 1936, 257, 2836, 712, 8247, 82, 5633, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.22842524964132427, "compression_ratio": 1.4567307692307692, "no_speech_prob": 1.5441697541973554e-05}, {"id": 625, "seek": 470800, "start": 4715.0, "end": 4731.0, "text": " And all this brings me to BERT, which is maybe you've heard of, which has made a very big difference in LP.", "tokens": [50364, 400, 437, 291, 4325, 281, 360, 307, 362, 733, 295, 411, 4593, 13280, 1296, 1411, 4319, 293, 264, 558, 4319, 13, 50714, 50714, 400, 439, 341, 5607, 385, 281, 363, 31479, 11, 597, 307, 1310, 291, 600, 2198, 295, 11, 597, 575, 1027, 257, 588, 955, 2649, 294, 38095, 13, 51514, 51514, 363, 31479, 767, 1542, 1596, 257, 688, 411, 8725, 17, 53, 3045, 13, 51614, 51614, 467, 311, 1936, 257, 2836, 712, 8247, 82, 5633, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.22842524964132427, "compression_ratio": 1.4567307692307692, "no_speech_prob": 1.5441697541973554e-05}, {"id": 626, "seek": 470800, "start": 4731.0, "end": 4733.0, "text": " BERT actually looks quite a lot like Word2Vec.", "tokens": [50364, 400, 437, 291, 4325, 281, 360, 307, 362, 733, 295, 411, 4593, 13280, 1296, 1411, 4319, 293, 264, 558, 4319, 13, 50714, 50714, 400, 439, 341, 5607, 385, 281, 363, 31479, 11, 597, 307, 1310, 291, 600, 2198, 295, 11, 597, 575, 1027, 257, 588, 955, 2649, 294, 38095, 13, 51514, 51514, 363, 31479, 767, 1542, 1596, 257, 688, 411, 8725, 17, 53, 3045, 13, 51614, 51614, 467, 311, 1936, 257, 2836, 712, 8247, 82, 5633, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.22842524964132427, "compression_ratio": 1.4567307692307692, "no_speech_prob": 1.5441697541973554e-05}, {"id": 627, "seek": 470800, "start": 4733.0, "end": 4735.0, "text": " It's basically a fillable blanks task.", "tokens": [50364, 400, 437, 291, 4325, 281, 360, 307, 362, 733, 295, 411, 4593, 13280, 1296, 1411, 4319, 293, 264, 558, 4319, 13, 50714, 50714, 400, 439, 341, 5607, 385, 281, 363, 31479, 11, 597, 307, 1310, 291, 600, 2198, 295, 11, 597, 575, 1027, 257, 588, 955, 2649, 294, 38095, 13, 51514, 51514, 363, 31479, 767, 1542, 1596, 257, 688, 411, 8725, 17, 53, 3045, 13, 51614, 51614, 467, 311, 1936, 257, 2836, 712, 8247, 82, 5633, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.22842524964132427, "compression_ratio": 1.4567307692307692, "no_speech_prob": 1.5441697541973554e-05}, {"id": 628, "seek": 473500, "start": 4735.0, "end": 4741.0, "text": " So you take some text, you hide some tokens by masking them out, and then you just try and fill it in the mask.", "tokens": [50364, 407, 291, 747, 512, 2487, 11, 291, 6479, 512, 22667, 538, 31226, 552, 484, 11, 293, 550, 291, 445, 853, 293, 2836, 309, 294, 264, 6094, 13, 50664, 50664, 407, 291, 483, 341, 2487, 411, 746, 307, 257, 9729, 746, 4471, 13, 50864, 50864, 467, 311, 291, 2836, 294, 264, 12905, 13, 51214, 51214, 407, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.303324826558431, "compression_ratio": 1.5338345864661653, "no_speech_prob": 1.86289798875805e-05}, {"id": 629, "seek": 473500, "start": 4741.0, "end": 4745.0, "text": " So you get this text like something is a golden something map.", "tokens": [50364, 407, 291, 747, 512, 2487, 11, 291, 6479, 512, 22667, 538, 31226, 552, 484, 11, 293, 550, 291, 445, 853, 293, 2836, 309, 294, 264, 6094, 13, 50664, 50664, 407, 291, 483, 341, 2487, 411, 746, 307, 257, 9729, 746, 4471, 13, 50864, 50864, 467, 311, 291, 2836, 294, 264, 12905, 13, 51214, 51214, 407, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.303324826558431, "compression_ratio": 1.5338345864661653, "no_speech_prob": 1.86289798875805e-05}, {"id": 630, "seek": 473500, "start": 4745.0, "end": 4752.0, "text": " It's you fill in the URL.", "tokens": [50364, 407, 291, 747, 512, 2487, 11, 291, 6479, 512, 22667, 538, 31226, 552, 484, 11, 293, 550, 291, 445, 853, 293, 2836, 309, 294, 264, 6094, 13, 50664, 50664, 407, 291, 483, 341, 2487, 411, 746, 307, 257, 9729, 746, 4471, 13, 50864, 50864, 467, 311, 291, 2836, 294, 264, 12905, 13, 51214, 51214, 407, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.303324826558431, "compression_ratio": 1.5338345864661653, "no_speech_prob": 1.86289798875805e-05}, {"id": 631, "seek": 473500, "start": 4752.0, "end": 4760.0, "text": " So.", "tokens": [50364, 407, 291, 747, 512, 2487, 11, 291, 6479, 512, 22667, 538, 31226, 552, 484, 11, 293, 550, 291, 445, 853, 293, 2836, 309, 294, 264, 6094, 13, 50664, 50664, 407, 291, 483, 341, 2487, 411, 746, 307, 257, 9729, 746, 4471, 13, 50864, 50864, 467, 311, 291, 2836, 294, 264, 12905, 13, 51214, 51214, 407, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.303324826558431, "compression_ratio": 1.5338345864661653, "no_speech_prob": 1.86289798875805e-05}, {"id": 632, "seek": 476000, "start": 4760.0, "end": 4765.0, "text": " The thing I want you to notice is firstly, that actually looks quite a lot like Word2Vec.", "tokens": [50364, 440, 551, 286, 528, 291, 281, 3449, 307, 27376, 11, 300, 767, 1542, 1596, 257, 688, 411, 8725, 17, 53, 3045, 13, 50614, 50614, 8725, 17, 53, 3045, 390, 611, 2212, 512, 2487, 2836, 294, 264, 8247, 82, 13, 50914, 50914, 440, 1778, 309, 1985, 709, 1101, 307, 300, 294, 8725, 17, 53, 3045, 291, 576, 445, 362, 341, 8213, 22743, 11, 51214, 51214, 411, 43430, 264, 4319, 2283, 11, 9735, 363, 31479, 291, 362, 552, 257, 588, 2416, 31782, 11, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.22491069120519303, "compression_ratio": 1.4976958525345623, "no_speech_prob": 3.268253567512147e-05}, {"id": 633, "seek": 476000, "start": 4765.0, "end": 4771.0, "text": " Word2Vec was also given some text fill in the blanks.", "tokens": [50364, 440, 551, 286, 528, 291, 281, 3449, 307, 27376, 11, 300, 767, 1542, 1596, 257, 688, 411, 8725, 17, 53, 3045, 13, 50614, 50614, 8725, 17, 53, 3045, 390, 611, 2212, 512, 2487, 2836, 294, 264, 8247, 82, 13, 50914, 50914, 440, 1778, 309, 1985, 709, 1101, 307, 300, 294, 8725, 17, 53, 3045, 291, 576, 445, 362, 341, 8213, 22743, 11, 51214, 51214, 411, 43430, 264, 4319, 2283, 11, 9735, 363, 31479, 291, 362, 552, 257, 588, 2416, 31782, 11, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.22491069120519303, "compression_ratio": 1.4976958525345623, "no_speech_prob": 3.268253567512147e-05}, {"id": 634, "seek": 476000, "start": 4771.0, "end": 4777.0, "text": " The reason it works much better is that in Word2Vec you would just have this linear projection,", "tokens": [50364, 440, 551, 286, 528, 291, 281, 3449, 307, 27376, 11, 300, 767, 1542, 1596, 257, 688, 411, 8725, 17, 53, 3045, 13, 50614, 50614, 8725, 17, 53, 3045, 390, 611, 2212, 512, 2487, 2836, 294, 264, 8247, 82, 13, 50914, 50914, 440, 1778, 309, 1985, 709, 1101, 307, 300, 294, 8725, 17, 53, 3045, 291, 576, 445, 362, 341, 8213, 22743, 11, 51214, 51214, 411, 43430, 264, 4319, 2283, 11, 9735, 363, 31479, 291, 362, 552, 257, 588, 2416, 31782, 11, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.22491069120519303, "compression_ratio": 1.4976958525345623, "no_speech_prob": 3.268253567512147e-05}, {"id": 635, "seek": 476000, "start": 4777.0, "end": 4783.0, "text": " like encoding the context words, whereas BERT you have them a very large transformer,", "tokens": [50364, 440, 551, 286, 528, 291, 281, 3449, 307, 27376, 11, 300, 767, 1542, 1596, 257, 688, 411, 8725, 17, 53, 3045, 13, 50614, 50614, 8725, 17, 53, 3045, 390, 611, 2212, 512, 2487, 2836, 294, 264, 8247, 82, 13, 50914, 50914, 440, 1778, 309, 1985, 709, 1101, 307, 300, 294, 8725, 17, 53, 3045, 291, 576, 445, 362, 341, 8213, 22743, 11, 51214, 51214, 411, 43430, 264, 4319, 2283, 11, 9735, 363, 31479, 291, 362, 552, 257, 588, 2416, 31782, 11, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.22491069120519303, "compression_ratio": 1.4976958525345623, "no_speech_prob": 3.268253567512147e-05}, {"id": 636, "seek": 478300, "start": 4783.0, "end": 4790.0, "text": " which will have much more context and much more interactions in that context.", "tokens": [50364, 597, 486, 362, 709, 544, 4319, 293, 709, 544, 13280, 294, 300, 4319, 13, 50714, 50714, 407, 456, 307, 257, 1168, 510, 13, 50814, 50814, 1012, 366, 4319, 33358, 17578, 562, 2489, 15164, 337, 257, 2685, 5633, 30, 51164, 51164, 1012, 366, 436, 17578, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1617550071404905, "compression_ratio": 1.5588235294117647, "no_speech_prob": 4.384811836644076e-05}, {"id": 637, "seek": 478300, "start": 4790.0, "end": 4792.0, "text": " So there is a question here.", "tokens": [50364, 597, 486, 362, 709, 544, 4319, 293, 709, 544, 13280, 294, 300, 4319, 13, 50714, 50714, 407, 456, 307, 257, 1168, 510, 13, 50814, 50814, 1012, 366, 4319, 33358, 17578, 562, 2489, 15164, 337, 257, 2685, 5633, 30, 51164, 51164, 1012, 366, 436, 17578, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1617550071404905, "compression_ratio": 1.5588235294117647, "no_speech_prob": 4.384811836644076e-05}, {"id": 638, "seek": 478300, "start": 4792.0, "end": 4799.0, "text": " How are context representations maintained when fine tuning for a specific task?", "tokens": [50364, 597, 486, 362, 709, 544, 4319, 293, 709, 544, 13280, 294, 300, 4319, 13, 50714, 50714, 407, 456, 307, 257, 1168, 510, 13, 50814, 50814, 1012, 366, 4319, 33358, 17578, 562, 2489, 15164, 337, 257, 2685, 5633, 30, 51164, 51164, 1012, 366, 436, 17578, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1617550071404905, "compression_ratio": 1.5588235294117647, "no_speech_prob": 4.384811836644076e-05}, {"id": 639, "seek": 478300, "start": 4799.0, "end": 4807.0, "text": " How are they maintained?", "tokens": [50364, 597, 486, 362, 709, 544, 4319, 293, 709, 544, 13280, 294, 300, 4319, 13, 50714, 50714, 407, 456, 307, 257, 1168, 510, 13, 50814, 50814, 1012, 366, 4319, 33358, 17578, 562, 2489, 15164, 337, 257, 2685, 5633, 30, 51164, 51164, 1012, 366, 436, 17578, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1617550071404905, "compression_ratio": 1.5588235294117647, "no_speech_prob": 4.384811836644076e-05}, {"id": 640, "seek": 480700, "start": 4807.0, "end": 4813.0, "text": " I guess it's not clear they are maintained.", "tokens": [50364, 286, 2041, 309, 311, 406, 1850, 436, 366, 17578, 13, 50664, 50664, 407, 562, 291, 2489, 10864, 337, 257, 1729, 5633, 11, 50864, 50864, 291, 733, 295, 1454, 264, 5245, 1466, 1547, 2674, 1507, 466, 2856, 337, 257, 659, 12, 17227, 1760, 5633, 11, 51164, 51164, 293, 550, 1830, 264, 2489, 15164, 1391, 286, 2041, 309, 311, 25428, 257, 688, 295, 341, 1507, 300, 309, 1177, 380, 643, 281, 5039, 341, 1729, 5633, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.21875442602695563, "compression_ratio": 1.6326530612244898, "no_speech_prob": 6.3380539359059185e-06}, {"id": 641, "seek": 480700, "start": 4813.0, "end": 4817.0, "text": " So when you fine tune for a particular task,", "tokens": [50364, 286, 2041, 309, 311, 406, 1850, 436, 366, 17578, 13, 50664, 50664, 407, 562, 291, 2489, 10864, 337, 257, 1729, 5633, 11, 50864, 50864, 291, 733, 295, 1454, 264, 5245, 1466, 1547, 2674, 1507, 466, 2856, 337, 257, 659, 12, 17227, 1760, 5633, 11, 51164, 51164, 293, 550, 1830, 264, 2489, 15164, 1391, 286, 2041, 309, 311, 25428, 257, 688, 295, 341, 1507, 300, 309, 1177, 380, 643, 281, 5039, 341, 1729, 5633, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.21875442602695563, "compression_ratio": 1.6326530612244898, "no_speech_prob": 6.3380539359059185e-06}, {"id": 642, "seek": 480700, "start": 4817.0, "end": 4823.0, "text": " you kind of hope the models learn enough general stuff about language for a pre-training task,", "tokens": [50364, 286, 2041, 309, 311, 406, 1850, 436, 366, 17578, 13, 50664, 50664, 407, 562, 291, 2489, 10864, 337, 257, 1729, 5633, 11, 50864, 50864, 291, 733, 295, 1454, 264, 5245, 1466, 1547, 2674, 1507, 466, 2856, 337, 257, 659, 12, 17227, 1760, 5633, 11, 51164, 51164, 293, 550, 1830, 264, 2489, 15164, 1391, 286, 2041, 309, 311, 25428, 257, 688, 295, 341, 1507, 300, 309, 1177, 380, 643, 281, 5039, 341, 1729, 5633, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.21875442602695563, "compression_ratio": 1.6326530612244898, "no_speech_prob": 6.3380539359059185e-06}, {"id": 643, "seek": 480700, "start": 4823.0, "end": 4832.0, "text": " and then during the fine tuning probably I guess it's forgetting a lot of this stuff that it doesn't need to solve this particular task.", "tokens": [50364, 286, 2041, 309, 311, 406, 1850, 436, 366, 17578, 13, 50664, 50664, 407, 562, 291, 2489, 10864, 337, 257, 1729, 5633, 11, 50864, 50864, 291, 733, 295, 1454, 264, 5245, 1466, 1547, 2674, 1507, 466, 2856, 337, 257, 659, 12, 17227, 1760, 5633, 11, 51164, 51164, 293, 550, 1830, 264, 2489, 15164, 1391, 286, 2041, 309, 311, 25428, 257, 688, 295, 341, 1507, 300, 309, 1177, 380, 643, 281, 5039, 341, 1729, 5633, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.21875442602695563, "compression_ratio": 1.6326530612244898, "no_speech_prob": 6.3380539359059185e-06}, {"id": 644, "seek": 483200, "start": 4832.0, "end": 4841.0, "text": " So if you fine tune your sentiment analysis or something, you probably lose a lot of this information during fine tuning.", "tokens": [50364, 407, 498, 291, 2489, 10864, 428, 16149, 5215, 420, 746, 11, 291, 1391, 3624, 257, 688, 295, 341, 1589, 1830, 2489, 15164, 13, 50814, 50814, 663, 2544, 2489, 13, 51064, 51064, 2561, 13, 51164, 51164, 1033, 13, 51414, 51414, 407, 363, 31479, 2732, 588, 731, 13, 51514, 51514, 467, 2729, 1596, 2416, 13797, 322, 257, 3840, 295, 9608, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16045246427021329, "compression_ratio": 1.3892215568862276, "no_speech_prob": 1.4506501429423224e-05}, {"id": 645, "seek": 483200, "start": 4841.0, "end": 4846.0, "text": " That seems fine.", "tokens": [50364, 407, 498, 291, 2489, 10864, 428, 16149, 5215, 420, 746, 11, 291, 1391, 3624, 257, 688, 295, 341, 1589, 1830, 2489, 15164, 13, 50814, 50814, 663, 2544, 2489, 13, 51064, 51064, 2561, 13, 51164, 51164, 1033, 13, 51414, 51414, 407, 363, 31479, 2732, 588, 731, 13, 51514, 51514, 467, 2729, 1596, 2416, 13797, 322, 257, 3840, 295, 9608, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16045246427021329, "compression_ratio": 1.3892215568862276, "no_speech_prob": 1.4506501429423224e-05}, {"id": 646, "seek": 483200, "start": 4846.0, "end": 4848.0, "text": " Thanks.", "tokens": [50364, 407, 498, 291, 2489, 10864, 428, 16149, 5215, 420, 746, 11, 291, 1391, 3624, 257, 688, 295, 341, 1589, 1830, 2489, 15164, 13, 50814, 50814, 663, 2544, 2489, 13, 51064, 51064, 2561, 13, 51164, 51164, 1033, 13, 51414, 51414, 407, 363, 31479, 2732, 588, 731, 13, 51514, 51514, 467, 2729, 1596, 2416, 13797, 322, 257, 3840, 295, 9608, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16045246427021329, "compression_ratio": 1.3892215568862276, "no_speech_prob": 1.4506501429423224e-05}, {"id": 647, "seek": 483200, "start": 4848.0, "end": 4853.0, "text": " Okay.", "tokens": [50364, 407, 498, 291, 2489, 10864, 428, 16149, 5215, 420, 746, 11, 291, 1391, 3624, 257, 688, 295, 341, 1589, 1830, 2489, 15164, 13, 50814, 50814, 663, 2544, 2489, 13, 51064, 51064, 2561, 13, 51164, 51164, 1033, 13, 51414, 51414, 407, 363, 31479, 2732, 588, 731, 13, 51514, 51514, 467, 2729, 1596, 2416, 13797, 322, 257, 3840, 295, 9608, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16045246427021329, "compression_ratio": 1.3892215568862276, "no_speech_prob": 1.4506501429423224e-05}, {"id": 648, "seek": 483200, "start": 4853.0, "end": 4855.0, "text": " So BERT worked very well.", "tokens": [50364, 407, 498, 291, 2489, 10864, 428, 16149, 5215, 420, 746, 11, 291, 1391, 3624, 257, 688, 295, 341, 1589, 1830, 2489, 15164, 13, 50814, 50814, 663, 2544, 2489, 13, 51064, 51064, 2561, 13, 51164, 51164, 1033, 13, 51414, 51414, 407, 363, 31479, 2732, 588, 731, 13, 51514, 51514, 467, 2729, 1596, 2416, 13797, 322, 257, 3840, 295, 9608, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16045246427021329, "compression_ratio": 1.3892215568862276, "no_speech_prob": 1.4506501429423224e-05}, {"id": 649, "seek": 483200, "start": 4855.0, "end": 4858.0, "text": " It gave quite large improvements on a bunch of tasks.", "tokens": [50364, 407, 498, 291, 2489, 10864, 428, 16149, 5215, 420, 746, 11, 291, 1391, 3624, 257, 688, 295, 341, 1589, 1830, 2489, 15164, 13, 50814, 50814, 663, 2544, 2489, 13, 51064, 51064, 2561, 13, 51164, 51164, 1033, 13, 51414, 51414, 407, 363, 31479, 2732, 588, 731, 13, 51514, 51514, 467, 2729, 1596, 2416, 13797, 322, 257, 3840, 295, 9608, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16045246427021329, "compression_ratio": 1.3892215568862276, "no_speech_prob": 1.4506501429423224e-05}, {"id": 650, "seek": 485800, "start": 4858.0, "end": 4865.0, "text": " It was actually achieving the performance of humans, or at least humans approximated by Amazon Mechanical Turk,", "tokens": [50364, 467, 390, 767, 19626, 264, 3389, 295, 6255, 11, 420, 412, 1935, 6255, 8542, 770, 538, 6795, 30175, 804, 15714, 11, 50714, 50714, 322, 257, 3840, 295, 588, 1021, 1168, 293, 1867, 43751, 13, 51014, 51014, 583, 363, 31479, 390, 2138, 406, 264, 917, 295, 264, 1657, 510, 13, 51214, 51214, 363, 31479, 658, 3195, 295, 561, 588, 2919, 466, 2698, 12, 48172, 24420, 3097, 13, 51514, 51514, 1449, 281, 2661, 20858, 4365, 456, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13577862027325208, "compression_ratio": 1.4588744588744589, "no_speech_prob": 5.472880366141908e-05}, {"id": 651, "seek": 485800, "start": 4865.0, "end": 4871.0, "text": " on a bunch of very important question and answer benchmarks.", "tokens": [50364, 467, 390, 767, 19626, 264, 3389, 295, 6255, 11, 420, 412, 1935, 6255, 8542, 770, 538, 6795, 30175, 804, 15714, 11, 50714, 50714, 322, 257, 3840, 295, 588, 1021, 1168, 293, 1867, 43751, 13, 51014, 51014, 583, 363, 31479, 390, 2138, 406, 264, 917, 295, 264, 1657, 510, 13, 51214, 51214, 363, 31479, 658, 3195, 295, 561, 588, 2919, 466, 2698, 12, 48172, 24420, 3097, 13, 51514, 51514, 1449, 281, 2661, 20858, 4365, 456, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13577862027325208, "compression_ratio": 1.4588744588744589, "no_speech_prob": 5.472880366141908e-05}, {"id": 652, "seek": 485800, "start": 4871.0, "end": 4875.0, "text": " But BERT was definitely not the end of the story here.", "tokens": [50364, 467, 390, 767, 19626, 264, 3389, 295, 6255, 11, 420, 412, 1935, 6255, 8542, 770, 538, 6795, 30175, 804, 15714, 11, 50714, 50714, 322, 257, 3840, 295, 588, 1021, 1168, 293, 1867, 43751, 13, 51014, 51014, 583, 363, 31479, 390, 2138, 406, 264, 917, 295, 264, 1657, 510, 13, 51214, 51214, 363, 31479, 658, 3195, 295, 561, 588, 2919, 466, 2698, 12, 48172, 24420, 3097, 13, 51514, 51514, 1449, 281, 2661, 20858, 4365, 456, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13577862027325208, "compression_ratio": 1.4588744588744589, "no_speech_prob": 5.472880366141908e-05}, {"id": 653, "seek": 485800, "start": 4875.0, "end": 4881.0, "text": " BERT got lots of people very excited about self-supervised training.", "tokens": [50364, 467, 390, 767, 19626, 264, 3389, 295, 6255, 11, 420, 412, 1935, 6255, 8542, 770, 538, 6795, 30175, 804, 15714, 11, 50714, 50714, 322, 257, 3840, 295, 588, 1021, 1168, 293, 1867, 43751, 13, 51014, 51014, 583, 363, 31479, 390, 2138, 406, 264, 917, 295, 264, 1657, 510, 13, 51214, 51214, 363, 31479, 658, 3195, 295, 561, 588, 2919, 466, 2698, 12, 48172, 24420, 3097, 13, 51514, 51514, 1449, 281, 2661, 20858, 4365, 456, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13577862027325208, "compression_ratio": 1.4588744588744589, "no_speech_prob": 5.472880366141908e-05}, {"id": 654, "seek": 485800, "start": 4881.0, "end": 4886.0, "text": " Just to quickly summarize details there.", "tokens": [50364, 467, 390, 767, 19626, 264, 3389, 295, 6255, 11, 420, 412, 1935, 6255, 8542, 770, 538, 6795, 30175, 804, 15714, 11, 50714, 50714, 322, 257, 3840, 295, 588, 1021, 1168, 293, 1867, 43751, 13, 51014, 51014, 583, 363, 31479, 390, 2138, 406, 264, 917, 295, 264, 1657, 510, 13, 51214, 51214, 363, 31479, 658, 3195, 295, 561, 588, 2919, 466, 2698, 12, 48172, 24420, 3097, 13, 51514, 51514, 1449, 281, 2661, 20858, 4365, 456, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13577862027325208, "compression_ratio": 1.4588744588744589, "no_speech_prob": 5.472880366141908e-05}, {"id": 655, "seek": 488600, "start": 4886.0, "end": 4897.0, "text": " It's a very simple model. It's just going to mask out 15% of the tokens and try and fill in the masks.", "tokens": [50364, 467, 311, 257, 588, 2199, 2316, 13, 467, 311, 445, 516, 281, 6094, 484, 2119, 4, 295, 264, 22667, 293, 853, 293, 2836, 294, 264, 11830, 13, 50914, 50914, 1407, 1322, 322, 300, 11, 456, 311, 512, 589, 412, 4384, 11, 4684, 538, 16747, 3451, 5047, 11, 597, 2956, 412, 21589, 484, 341, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.264334251140726, "compression_ratio": 1.303225806451613, "no_speech_prob": 3.478581857052632e-05}, {"id": 656, "seek": 488600, "start": 4897.0, "end": 4909.0, "text": " To build on that, there's some work at Facebook, led by Yihan Lu, which looked at scaling out this.", "tokens": [50364, 467, 311, 257, 588, 2199, 2316, 13, 467, 311, 445, 516, 281, 6094, 484, 2119, 4, 295, 264, 22667, 293, 853, 293, 2836, 294, 264, 11830, 13, 50914, 50914, 1407, 1322, 322, 300, 11, 456, 311, 512, 589, 412, 4384, 11, 4684, 538, 16747, 3451, 5047, 11, 597, 2956, 412, 21589, 484, 341, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.264334251140726, "compression_ratio": 1.303225806451613, "no_speech_prob": 3.478581857052632e-05}, {"id": 657, "seek": 490900, "start": 4909.0, "end": 4917.0, "text": " So BERT actually had a second pre-training objective, which we showed didn't actually help.", "tokens": [50364, 407, 363, 31479, 767, 632, 257, 1150, 659, 12, 17227, 1760, 10024, 11, 597, 321, 4712, 994, 380, 767, 854, 13, 50764, 50764, 1664, 286, 1029, 257, 1168, 337, 264, 3894, 4137, 30, 50914, 50914, 407, 456, 645, 1045, 10228, 13, 286, 519, 286, 6721, 472, 935, 13, 51064, 51064, 440, 2877, 3344, 11, 437, 307, 264, 2877, 3344, 30, 51214, 51214, 407, 321, 362, 6795, 15714, 13, 51314, 51314, 1044, 291, 13, 4919, 286, 4659, 380, 362, 848, 300, 13, 51414, 51414, 9563, 3344, 510, 307, 3894, 1785, 295, 264, 1523, 13, 51564, 51564, 1033, 11, 597, 390, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19726777076721191, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.00020314016728661954}, {"id": 658, "seek": 490900, "start": 4917.0, "end": 4920.0, "text": " Can I ask a question for the previous slide?", "tokens": [50364, 407, 363, 31479, 767, 632, 257, 1150, 659, 12, 17227, 1760, 10024, 11, 597, 321, 4712, 994, 380, 767, 854, 13, 50764, 50764, 1664, 286, 1029, 257, 1168, 337, 264, 3894, 4137, 30, 50914, 50914, 407, 456, 645, 1045, 10228, 13, 286, 519, 286, 6721, 472, 935, 13, 51064, 51064, 440, 2877, 3344, 11, 437, 307, 264, 2877, 3344, 30, 51214, 51214, 407, 321, 362, 6795, 15714, 13, 51314, 51314, 1044, 291, 13, 4919, 286, 4659, 380, 362, 848, 300, 13, 51414, 51414, 9563, 3344, 510, 307, 3894, 1785, 295, 264, 1523, 13, 51564, 51564, 1033, 11, 597, 390, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19726777076721191, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.00020314016728661954}, {"id": 659, "seek": 490900, "start": 4920.0, "end": 4923.0, "text": " So there were three bars. I think I missed one point.", "tokens": [50364, 407, 363, 31479, 767, 632, 257, 1150, 659, 12, 17227, 1760, 10024, 11, 597, 321, 4712, 994, 380, 767, 854, 13, 50764, 50764, 1664, 286, 1029, 257, 1168, 337, 264, 3894, 4137, 30, 50914, 50914, 407, 456, 645, 1045, 10228, 13, 286, 519, 286, 6721, 472, 935, 13, 51064, 51064, 440, 2877, 3344, 11, 437, 307, 264, 2877, 3344, 30, 51214, 51214, 407, 321, 362, 6795, 15714, 13, 51314, 51314, 1044, 291, 13, 4919, 286, 4659, 380, 362, 848, 300, 13, 51414, 51414, 9563, 3344, 510, 307, 3894, 1785, 295, 264, 1523, 13, 51564, 51564, 1033, 11, 597, 390, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19726777076721191, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.00020314016728661954}, {"id": 660, "seek": 490900, "start": 4923.0, "end": 4926.0, "text": " The dark blue, what is the dark blue?", "tokens": [50364, 407, 363, 31479, 767, 632, 257, 1150, 659, 12, 17227, 1760, 10024, 11, 597, 321, 4712, 994, 380, 767, 854, 13, 50764, 50764, 1664, 286, 1029, 257, 1168, 337, 264, 3894, 4137, 30, 50914, 50914, 407, 456, 645, 1045, 10228, 13, 286, 519, 286, 6721, 472, 935, 13, 51064, 51064, 440, 2877, 3344, 11, 437, 307, 264, 2877, 3344, 30, 51214, 51214, 407, 321, 362, 6795, 15714, 13, 51314, 51314, 1044, 291, 13, 4919, 286, 4659, 380, 362, 848, 300, 13, 51414, 51414, 9563, 3344, 510, 307, 3894, 1785, 295, 264, 1523, 13, 51564, 51564, 1033, 11, 597, 390, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19726777076721191, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.00020314016728661954}, {"id": 661, "seek": 490900, "start": 4926.0, "end": 4928.0, "text": " So we have Amazon Turk.", "tokens": [50364, 407, 363, 31479, 767, 632, 257, 1150, 659, 12, 17227, 1760, 10024, 11, 597, 321, 4712, 994, 380, 767, 854, 13, 50764, 50764, 1664, 286, 1029, 257, 1168, 337, 264, 3894, 4137, 30, 50914, 50914, 407, 456, 645, 1045, 10228, 13, 286, 519, 286, 6721, 472, 935, 13, 51064, 51064, 440, 2877, 3344, 11, 437, 307, 264, 2877, 3344, 30, 51214, 51214, 407, 321, 362, 6795, 15714, 13, 51314, 51314, 1044, 291, 13, 4919, 286, 4659, 380, 362, 848, 300, 13, 51414, 51414, 9563, 3344, 510, 307, 3894, 1785, 295, 264, 1523, 13, 51564, 51564, 1033, 11, 597, 390, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19726777076721191, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.00020314016728661954}, {"id": 662, "seek": 490900, "start": 4928.0, "end": 4930.0, "text": " Thank you. Sorry I shouldn't have said that.", "tokens": [50364, 407, 363, 31479, 767, 632, 257, 1150, 659, 12, 17227, 1760, 10024, 11, 597, 321, 4712, 994, 380, 767, 854, 13, 50764, 50764, 1664, 286, 1029, 257, 1168, 337, 264, 3894, 4137, 30, 50914, 50914, 407, 456, 645, 1045, 10228, 13, 286, 519, 286, 6721, 472, 935, 13, 51064, 51064, 440, 2877, 3344, 11, 437, 307, 264, 2877, 3344, 30, 51214, 51214, 407, 321, 362, 6795, 15714, 13, 51314, 51314, 1044, 291, 13, 4919, 286, 4659, 380, 362, 848, 300, 13, 51414, 51414, 9563, 3344, 510, 307, 3894, 1785, 295, 264, 1523, 13, 51564, 51564, 1033, 11, 597, 390, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19726777076721191, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.00020314016728661954}, {"id": 663, "seek": 490900, "start": 4930.0, "end": 4933.0, "text": " Dark blue here is previous state of the art.", "tokens": [50364, 407, 363, 31479, 767, 632, 257, 1150, 659, 12, 17227, 1760, 10024, 11, 597, 321, 4712, 994, 380, 767, 854, 13, 50764, 50764, 1664, 286, 1029, 257, 1168, 337, 264, 3894, 4137, 30, 50914, 50914, 407, 456, 645, 1045, 10228, 13, 286, 519, 286, 6721, 472, 935, 13, 51064, 51064, 440, 2877, 3344, 11, 437, 307, 264, 2877, 3344, 30, 51214, 51214, 407, 321, 362, 6795, 15714, 13, 51314, 51314, 1044, 291, 13, 4919, 286, 4659, 380, 362, 848, 300, 13, 51414, 51414, 9563, 3344, 510, 307, 3894, 1785, 295, 264, 1523, 13, 51564, 51564, 1033, 11, 597, 390, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19726777076721191, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.00020314016728661954}, {"id": 664, "seek": 490900, "start": 4933.0, "end": 4936.0, "text": " Okay, which was?", "tokens": [50364, 407, 363, 31479, 767, 632, 257, 1150, 659, 12, 17227, 1760, 10024, 11, 597, 321, 4712, 994, 380, 767, 854, 13, 50764, 50764, 1664, 286, 1029, 257, 1168, 337, 264, 3894, 4137, 30, 50914, 50914, 407, 456, 645, 1045, 10228, 13, 286, 519, 286, 6721, 472, 935, 13, 51064, 51064, 440, 2877, 3344, 11, 437, 307, 264, 2877, 3344, 30, 51214, 51214, 407, 321, 362, 6795, 15714, 13, 51314, 51314, 1044, 291, 13, 4919, 286, 4659, 380, 362, 848, 300, 13, 51414, 51414, 9563, 3344, 510, 307, 3894, 1785, 295, 264, 1523, 13, 51564, 51564, 1033, 11, 597, 390, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19726777076721191, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.00020314016728661954}, {"id": 665, "seek": 493600, "start": 4936.0, "end": 4939.0, "text": " These models were probably Elmo.", "tokens": [50364, 1981, 5245, 645, 1391, 38722, 13, 50514, 50514, 407, 264, 3894, 1558, 390, 2138, 1228, 2698, 12, 48172, 24420, 3097, 13, 50864, 50864, 583, 363, 31479, 9689, 38722, 538, 1419, 341, 733, 295, 411, 485, 51014, 51014, 286, 536, 13, 400, 3344, 767, 307, 257, 18927, 300, 321, 600, 668, 4084, 510, 412, 42682, 13, 51314, 51314, 7587, 11, 1338, 13, 51364, 51364, 17244, 645, 3288, 13, 51464, 51464, 407, 1338, 11, 3344, 307, 257, 955, 18927, 13, 467, 311, 588, 1021, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.27310621327367324, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.00047166264266707003}, {"id": 666, "seek": 493600, "start": 4939.0, "end": 4946.0, "text": " So the previous idea was definitely using self-supervised training.", "tokens": [50364, 1981, 5245, 645, 1391, 38722, 13, 50514, 50514, 407, 264, 3894, 1558, 390, 2138, 1228, 2698, 12, 48172, 24420, 3097, 13, 50864, 50864, 583, 363, 31479, 9689, 38722, 538, 1419, 341, 733, 295, 411, 485, 51014, 51014, 286, 536, 13, 400, 3344, 767, 307, 257, 18927, 300, 321, 600, 668, 4084, 510, 412, 42682, 13, 51314, 51314, 7587, 11, 1338, 13, 51364, 51364, 17244, 645, 3288, 13, 51464, 51464, 407, 1338, 11, 3344, 307, 257, 955, 18927, 13, 467, 311, 588, 1021, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.27310621327367324, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.00047166264266707003}, {"id": 667, "seek": 493600, "start": 4946.0, "end": 4949.0, "text": " But BERT improved Elmo by having this kind of like...", "tokens": [50364, 1981, 5245, 645, 1391, 38722, 13, 50514, 50514, 407, 264, 3894, 1558, 390, 2138, 1228, 2698, 12, 48172, 24420, 3097, 13, 50864, 50864, 583, 363, 31479, 9689, 38722, 538, 1419, 341, 733, 295, 411, 485, 51014, 51014, 286, 536, 13, 400, 3344, 767, 307, 257, 18927, 300, 321, 600, 668, 4084, 510, 412, 42682, 13, 51314, 51314, 7587, 11, 1338, 13, 51364, 51364, 17244, 645, 3288, 13, 51464, 51464, 407, 1338, 11, 3344, 307, 257, 955, 18927, 13, 467, 311, 588, 1021, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.27310621327367324, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.00047166264266707003}, {"id": 668, "seek": 493600, "start": 4949.0, "end": 4955.0, "text": " I see. And blue actually is a benchmark that we've been creating here at NYU.", "tokens": [50364, 1981, 5245, 645, 1391, 38722, 13, 50514, 50514, 407, 264, 3894, 1558, 390, 2138, 1228, 2698, 12, 48172, 24420, 3097, 13, 50864, 50864, 583, 363, 31479, 9689, 38722, 538, 1419, 341, 733, 295, 411, 485, 51014, 51014, 286, 536, 13, 400, 3344, 767, 307, 257, 18927, 300, 321, 600, 668, 4084, 510, 412, 42682, 13, 51314, 51314, 7587, 11, 1338, 13, 51364, 51364, 17244, 645, 3288, 13, 51464, 51464, 407, 1338, 11, 3344, 307, 257, 955, 18927, 13, 467, 311, 588, 1021, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.27310621327367324, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.00047166264266707003}, {"id": 669, "seek": 493600, "start": 4955.0, "end": 4956.0, "text": " Exactly, yeah.", "tokens": [50364, 1981, 5245, 645, 1391, 38722, 13, 50514, 50514, 407, 264, 3894, 1558, 390, 2138, 1228, 2698, 12, 48172, 24420, 3097, 13, 50864, 50864, 583, 363, 31479, 9689, 38722, 538, 1419, 341, 733, 295, 411, 485, 51014, 51014, 286, 536, 13, 400, 3344, 767, 307, 257, 18927, 300, 321, 600, 668, 4084, 510, 412, 42682, 13, 51314, 51314, 7587, 11, 1338, 13, 51364, 51364, 17244, 645, 3288, 13, 51464, 51464, 407, 1338, 11, 3344, 307, 257, 955, 18927, 13, 467, 311, 588, 1021, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.27310621327367324, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.00047166264266707003}, {"id": 670, "seek": 493600, "start": 4956.0, "end": 4958.0, "text": " Students were involved.", "tokens": [50364, 1981, 5245, 645, 1391, 38722, 13, 50514, 50514, 407, 264, 3894, 1558, 390, 2138, 1228, 2698, 12, 48172, 24420, 3097, 13, 50864, 50864, 583, 363, 31479, 9689, 38722, 538, 1419, 341, 733, 295, 411, 485, 51014, 51014, 286, 536, 13, 400, 3344, 767, 307, 257, 18927, 300, 321, 600, 668, 4084, 510, 412, 42682, 13, 51314, 51314, 7587, 11, 1338, 13, 51364, 51364, 17244, 645, 3288, 13, 51464, 51464, 407, 1338, 11, 3344, 307, 257, 955, 18927, 13, 467, 311, 588, 1021, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.27310621327367324, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.00047166264266707003}, {"id": 671, "seek": 493600, "start": 4958.0, "end": 4962.0, "text": " So yeah, blue is a big benchmark. It's very important.", "tokens": [50364, 1981, 5245, 645, 1391, 38722, 13, 50514, 50514, 407, 264, 3894, 1558, 390, 2138, 1228, 2698, 12, 48172, 24420, 3097, 13, 50864, 50864, 583, 363, 31479, 9689, 38722, 538, 1419, 341, 733, 295, 411, 485, 51014, 51014, 286, 536, 13, 400, 3344, 767, 307, 257, 18927, 300, 321, 600, 668, 4084, 510, 412, 42682, 13, 51314, 51314, 7587, 11, 1338, 13, 51364, 51364, 17244, 645, 3288, 13, 51464, 51464, 407, 1338, 11, 3344, 307, 257, 955, 18927, 13, 467, 311, 588, 1021, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.27310621327367324, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.00047166264266707003}, {"id": 672, "seek": 496200, "start": 4962.0, "end": 4970.0, "text": " So to beat BERT, it turned out all you had to do was firstly simplify training objective, then just scale it up.", "tokens": [50364, 407, 281, 4224, 363, 31479, 11, 309, 3574, 484, 439, 291, 632, 281, 360, 390, 27376, 20460, 3097, 10024, 11, 550, 445, 4373, 309, 493, 13, 50764, 50764, 407, 21589, 493, 510, 1355, 3801, 15245, 11602, 11, 2603, 3547, 295, 18407, 82, 11, 544, 1737, 3097, 2487, 13, 51314, 51314, 400, 550, 291, 483, 588, 2416, 16823, 322, 1192, 295, 363, 31479, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1456315766519575, "compression_ratio": 1.3804347826086956, "no_speech_prob": 1.4506787920254283e-05}, {"id": 673, "seek": 496200, "start": 4970.0, "end": 4981.0, "text": " So scaling up here means bigger batch sizes, huge numbers of GPUs, more free training text.", "tokens": [50364, 407, 281, 4224, 363, 31479, 11, 309, 3574, 484, 439, 291, 632, 281, 360, 390, 27376, 20460, 3097, 10024, 11, 550, 445, 4373, 309, 493, 13, 50764, 50764, 407, 21589, 493, 510, 1355, 3801, 15245, 11602, 11, 2603, 3547, 295, 18407, 82, 11, 544, 1737, 3097, 2487, 13, 51314, 51314, 400, 550, 291, 483, 588, 2416, 16823, 322, 1192, 295, 363, 31479, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1456315766519575, "compression_ratio": 1.3804347826086956, "no_speech_prob": 1.4506787920254283e-05}, {"id": 674, "seek": 496200, "start": 4981.0, "end": 4985.0, "text": " And then you get very large gains on top of BERT.", "tokens": [50364, 407, 281, 4224, 363, 31479, 11, 309, 3574, 484, 439, 291, 632, 281, 360, 390, 27376, 20460, 3097, 10024, 11, 550, 445, 4373, 309, 493, 13, 50764, 50764, 407, 21589, 493, 510, 1355, 3801, 15245, 11602, 11, 2603, 3547, 295, 18407, 82, 11, 544, 1737, 3097, 2487, 13, 51314, 51314, 400, 550, 291, 483, 588, 2416, 16823, 322, 1192, 295, 363, 31479, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1456315766519575, "compression_ratio": 1.3804347826086956, "no_speech_prob": 1.4506787920254283e-05}, {"id": 675, "seek": 498500, "start": 4985.0, "end": 4992.0, "text": " In fact, much larger gains on top of BERT than BERT had over the previous Elmo work.", "tokens": [50364, 682, 1186, 11, 709, 4833, 16823, 322, 1192, 295, 363, 31479, 813, 363, 31479, 632, 670, 264, 3894, 38722, 589, 13, 50714, 50714, 407, 5566, 510, 307, 550, 341, 777, 15800, 1328, 2316, 13, 50964, 50964, 400, 767, 15800, 1328, 11, 264, 1168, 13430, 307, 1687, 18796, 538, 1596, 257, 1326, 280, 8654, 13, 51314, 51314, 400, 611, 322, 341, 3344, 18927, 490, 42682, 307, 611, 484, 12, 610, 22892, 561, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.18689491874293276, "compression_ratio": 1.415, "no_speech_prob": 1.3416022738965694e-05}, {"id": 676, "seek": 498500, "start": 4992.0, "end": 4997.0, "text": " So yellow here is then this new Roberta model.", "tokens": [50364, 682, 1186, 11, 709, 4833, 16823, 322, 1192, 295, 363, 31479, 813, 363, 31479, 632, 670, 264, 3894, 38722, 589, 13, 50714, 50714, 407, 5566, 510, 307, 550, 341, 777, 15800, 1328, 2316, 13, 50964, 50964, 400, 767, 15800, 1328, 11, 264, 1168, 13430, 307, 1687, 18796, 538, 1596, 257, 1326, 280, 8654, 13, 51314, 51314, 400, 611, 322, 341, 3344, 18927, 490, 42682, 307, 611, 484, 12, 610, 22892, 561, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.18689491874293276, "compression_ratio": 1.415, "no_speech_prob": 1.3416022738965694e-05}, {"id": 677, "seek": 498500, "start": 4997.0, "end": 5004.0, "text": " And actually Roberta, the question answering is superhuman by quite a few pints.", "tokens": [50364, 682, 1186, 11, 709, 4833, 16823, 322, 1192, 295, 363, 31479, 813, 363, 31479, 632, 670, 264, 3894, 38722, 589, 13, 50714, 50714, 407, 5566, 510, 307, 550, 341, 777, 15800, 1328, 2316, 13, 50964, 50964, 400, 767, 15800, 1328, 11, 264, 1168, 13430, 307, 1687, 18796, 538, 1596, 257, 1326, 280, 8654, 13, 51314, 51314, 400, 611, 322, 341, 3344, 18927, 490, 42682, 307, 611, 484, 12, 610, 22892, 561, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.18689491874293276, "compression_ratio": 1.415, "no_speech_prob": 1.3416022738965694e-05}, {"id": 678, "seek": 498500, "start": 5004.0, "end": 5011.0, "text": " And also on this blue benchmark from NYU is also out-performed people.", "tokens": [50364, 682, 1186, 11, 709, 4833, 16823, 322, 1192, 295, 363, 31479, 813, 363, 31479, 632, 670, 264, 3894, 38722, 589, 13, 50714, 50714, 407, 5566, 510, 307, 550, 341, 777, 15800, 1328, 2316, 13, 50964, 50964, 400, 767, 15800, 1328, 11, 264, 1168, 13430, 307, 1687, 18796, 538, 1596, 257, 1326, 280, 8654, 13, 51314, 51314, 400, 611, 322, 341, 3344, 18927, 490, 42682, 307, 611, 484, 12, 610, 22892, 561, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.18689491874293276, "compression_ratio": 1.415, "no_speech_prob": 1.3416022738965694e-05}, {"id": 679, "seek": 501100, "start": 5011.0, "end": 5019.0, "text": " And this isn't really about doing anything smart. It's just taking self-supervised training and doing it well.", "tokens": [50364, 400, 341, 1943, 380, 534, 466, 884, 1340, 4069, 13, 467, 311, 445, 1940, 2698, 12, 48172, 24420, 3097, 293, 884, 309, 731, 13, 50764, 50764, 1779, 13, 50814, 50814, 1545, 360, 291, 584, 11, 411, 498, 291, 352, 322, 341, 4137, 510, 11, 370, 456, 307, 257, 588, 2416, 10444, 1296, 363, 31479, 293, 15800, 1328, 294, 264, 3114, 298, 11, 51214, 51214, 457, 406, 1270, 257, 2603, 1319, 1310, 294, 264, 15310, 11, 597, 307, 445, 257, 48226, 5952, 13, 51564, 51564, 876, 11, 558, 11, 1338, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20738920252373877, "compression_ratio": 1.4718614718614718, "no_speech_prob": 0.00017664766346570104}, {"id": 680, "seek": 501100, "start": 5019.0, "end": 5020.0, "text": " Right.", "tokens": [50364, 400, 341, 1943, 380, 534, 466, 884, 1340, 4069, 13, 467, 311, 445, 1940, 2698, 12, 48172, 24420, 3097, 293, 884, 309, 731, 13, 50764, 50764, 1779, 13, 50814, 50814, 1545, 360, 291, 584, 11, 411, 498, 291, 352, 322, 341, 4137, 510, 11, 370, 456, 307, 257, 588, 2416, 10444, 1296, 363, 31479, 293, 15800, 1328, 294, 264, 3114, 298, 11, 51214, 51214, 457, 406, 1270, 257, 2603, 1319, 1310, 294, 264, 15310, 11, 597, 307, 445, 257, 48226, 5952, 13, 51564, 51564, 876, 11, 558, 11, 1338, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20738920252373877, "compression_ratio": 1.4718614718614718, "no_speech_prob": 0.00017664766346570104}, {"id": 681, "seek": 501100, "start": 5020.0, "end": 5028.0, "text": " Why do you say, like if you go on this slide here, so there is a very large improvement between BERT and Roberta in the gloom,", "tokens": [50364, 400, 341, 1943, 380, 534, 466, 884, 1340, 4069, 13, 467, 311, 445, 1940, 2698, 12, 48172, 24420, 3097, 293, 884, 309, 731, 13, 50764, 50764, 1779, 13, 50814, 50814, 1545, 360, 291, 584, 11, 411, 498, 291, 352, 322, 341, 4137, 510, 11, 370, 456, 307, 257, 588, 2416, 10444, 1296, 363, 31479, 293, 15800, 1328, 294, 264, 3114, 298, 11, 51214, 51214, 457, 406, 1270, 257, 2603, 1319, 1310, 294, 264, 15310, 11, 597, 307, 445, 257, 48226, 5952, 13, 51564, 51564, 876, 11, 558, 11, 1338, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20738920252373877, "compression_ratio": 1.4718614718614718, "no_speech_prob": 0.00017664766346570104}, {"id": 682, "seek": 501100, "start": 5028.0, "end": 5035.0, "text": " but not such a huge change maybe in the squad, which is just a zooming factor.", "tokens": [50364, 400, 341, 1943, 380, 534, 466, 884, 1340, 4069, 13, 467, 311, 445, 1940, 2698, 12, 48172, 24420, 3097, 293, 884, 309, 731, 13, 50764, 50764, 1779, 13, 50814, 50814, 1545, 360, 291, 584, 11, 411, 498, 291, 352, 322, 341, 4137, 510, 11, 370, 456, 307, 257, 588, 2416, 10444, 1296, 363, 31479, 293, 15800, 1328, 294, 264, 3114, 298, 11, 51214, 51214, 457, 406, 1270, 257, 2603, 1319, 1310, 294, 264, 15310, 11, 597, 307, 445, 257, 48226, 5952, 13, 51564, 51564, 876, 11, 558, 11, 1338, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20738920252373877, "compression_ratio": 1.4718614718614718, "no_speech_prob": 0.00017664766346570104}, {"id": 683, "seek": 501100, "start": 5035.0, "end": 5038.0, "text": " Oh, right, yeah.", "tokens": [50364, 400, 341, 1943, 380, 534, 466, 884, 1340, 4069, 13, 467, 311, 445, 1940, 2698, 12, 48172, 24420, 3097, 293, 884, 309, 731, 13, 50764, 50764, 1779, 13, 50814, 50814, 1545, 360, 291, 584, 11, 411, 498, 291, 352, 322, 341, 4137, 510, 11, 370, 456, 307, 257, 588, 2416, 10444, 1296, 363, 31479, 293, 15800, 1328, 294, 264, 3114, 298, 11, 51214, 51214, 457, 406, 1270, 257, 2603, 1319, 1310, 294, 264, 15310, 11, 597, 307, 445, 257, 48226, 5952, 13, 51564, 51564, 876, 11, 558, 11, 1338, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20738920252373877, "compression_ratio": 1.4718614718614718, "no_speech_prob": 0.00017664766346570104}, {"id": 684, "seek": 503800, "start": 5038.0, "end": 5043.0, "text": " Maybe it's just a zooming factor, right? Those bars on the left are taller maybe.", "tokens": [50364, 2704, 309, 311, 445, 257, 48226, 5952, 11, 558, 30, 3950, 10228, 322, 264, 1411, 366, 22406, 1310, 13, 50614, 50614, 865, 11, 286, 519, 1310, 264, 4373, 307, 445, 4491, 278, 341, 636, 13, 50914, 50914, 286, 519, 264, 935, 307, 498, 291, 6794, 281, 1952, 3389, 11, 286, 458, 363, 31479, 390, 437, 11, 1958, 13, 21, 2793, 1101, 813, 561, 11, 51414, 51414, 9735, 15800, 1328, 307, 805, 13, 20, 2793, 1101, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.21660192012786866, "compression_ratio": 1.4540816326530612, "no_speech_prob": 4.004941729363054e-05}, {"id": 685, "seek": 503800, "start": 5043.0, "end": 5049.0, "text": " Yeah, I think maybe the scale is just dosing this way.", "tokens": [50364, 2704, 309, 311, 445, 257, 48226, 5952, 11, 558, 30, 3950, 10228, 322, 264, 1411, 366, 22406, 1310, 13, 50614, 50614, 865, 11, 286, 519, 1310, 264, 4373, 307, 445, 4491, 278, 341, 636, 13, 50914, 50914, 286, 519, 264, 935, 307, 498, 291, 6794, 281, 1952, 3389, 11, 286, 458, 363, 31479, 390, 437, 11, 1958, 13, 21, 2793, 1101, 813, 561, 11, 51414, 51414, 9735, 15800, 1328, 307, 805, 13, 20, 2793, 1101, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.21660192012786866, "compression_ratio": 1.4540816326530612, "no_speech_prob": 4.004941729363054e-05}, {"id": 686, "seek": 503800, "start": 5049.0, "end": 5059.0, "text": " I think the point is if you compare to human performance, I know BERT was what, 0.6 points better than people,", "tokens": [50364, 2704, 309, 311, 445, 257, 48226, 5952, 11, 558, 30, 3950, 10228, 322, 264, 1411, 366, 22406, 1310, 13, 50614, 50614, 865, 11, 286, 519, 1310, 264, 4373, 307, 445, 4491, 278, 341, 636, 13, 50914, 50914, 286, 519, 264, 935, 307, 498, 291, 6794, 281, 1952, 3389, 11, 286, 458, 363, 31479, 390, 437, 11, 1958, 13, 21, 2793, 1101, 813, 561, 11, 51414, 51414, 9735, 15800, 1328, 307, 805, 13, 20, 2793, 1101, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.21660192012786866, "compression_ratio": 1.4540816326530612, "no_speech_prob": 4.004941729363054e-05}, {"id": 687, "seek": 503800, "start": 5059.0, "end": 5063.0, "text": " whereas Roberta is 3.5 points better.", "tokens": [50364, 2704, 309, 311, 445, 257, 48226, 5952, 11, 558, 30, 3950, 10228, 322, 264, 1411, 366, 22406, 1310, 13, 50614, 50614, 865, 11, 286, 519, 1310, 264, 4373, 307, 445, 4491, 278, 341, 636, 13, 50914, 50914, 286, 519, 264, 935, 307, 498, 291, 6794, 281, 1952, 3389, 11, 286, 458, 363, 31479, 390, 437, 11, 1958, 13, 21, 2793, 1101, 813, 561, 11, 51414, 51414, 9735, 15800, 1328, 307, 805, 13, 20, 2793, 1101, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.21660192012786866, "compression_ratio": 1.4540816326530612, "no_speech_prob": 4.004941729363054e-05}, {"id": 688, "seek": 506300, "start": 5063.0, "end": 5068.0, "text": " So by that metric is actually quite a big jump.", "tokens": [50364, 407, 538, 300, 20678, 307, 767, 1596, 257, 955, 3012, 13, 50614, 50614, 865, 13, 50764, 50764, 1033, 13, 50864, 50864, 407, 286, 603, 445, 2661, 2248, 257, 1326, 295, 264, 661, 721, 561, 362, 668, 884, 2698, 12, 48172, 24420, 3097, 13, 51264, 51264, 407, 456, 311, 472, 1219, 19060, 31890, 13, 51464, 51464, 8537, 11, 370, 294, 363, 31479, 11, 562, 291, 6069, 439, 428, 2758, 22667, 11, 291, 6069, 439, 264, 11830, 4188, 379, 21761, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16482765703316193, "compression_ratio": 1.4511627906976745, "no_speech_prob": 1.4969090443628374e-05}, {"id": 689, "seek": 506300, "start": 5068.0, "end": 5071.0, "text": " Yeah.", "tokens": [50364, 407, 538, 300, 20678, 307, 767, 1596, 257, 955, 3012, 13, 50614, 50614, 865, 13, 50764, 50764, 1033, 13, 50864, 50864, 407, 286, 603, 445, 2661, 2248, 257, 1326, 295, 264, 661, 721, 561, 362, 668, 884, 2698, 12, 48172, 24420, 3097, 13, 51264, 51264, 407, 456, 311, 472, 1219, 19060, 31890, 13, 51464, 51464, 8537, 11, 370, 294, 363, 31479, 11, 562, 291, 6069, 439, 428, 2758, 22667, 11, 291, 6069, 439, 264, 11830, 4188, 379, 21761, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16482765703316193, "compression_ratio": 1.4511627906976745, "no_speech_prob": 1.4969090443628374e-05}, {"id": 690, "seek": 506300, "start": 5071.0, "end": 5073.0, "text": " Okay.", "tokens": [50364, 407, 538, 300, 20678, 307, 767, 1596, 257, 955, 3012, 13, 50614, 50614, 865, 13, 50764, 50764, 1033, 13, 50864, 50864, 407, 286, 603, 445, 2661, 2248, 257, 1326, 295, 264, 661, 721, 561, 362, 668, 884, 2698, 12, 48172, 24420, 3097, 13, 51264, 51264, 407, 456, 311, 472, 1219, 19060, 31890, 13, 51464, 51464, 8537, 11, 370, 294, 363, 31479, 11, 562, 291, 6069, 439, 428, 2758, 22667, 11, 291, 6069, 439, 264, 11830, 4188, 379, 21761, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16482765703316193, "compression_ratio": 1.4511627906976745, "no_speech_prob": 1.4969090443628374e-05}, {"id": 691, "seek": 506300, "start": 5073.0, "end": 5081.0, "text": " So I'll just quickly discuss a few of the other things people have been doing self-supervised training.", "tokens": [50364, 407, 538, 300, 20678, 307, 767, 1596, 257, 955, 3012, 13, 50614, 50614, 865, 13, 50764, 50764, 1033, 13, 50864, 50864, 407, 286, 603, 445, 2661, 2248, 257, 1326, 295, 264, 661, 721, 561, 362, 668, 884, 2698, 12, 48172, 24420, 3097, 13, 51264, 51264, 407, 456, 311, 472, 1219, 19060, 31890, 13, 51464, 51464, 8537, 11, 370, 294, 363, 31479, 11, 562, 291, 6069, 439, 428, 2758, 22667, 11, 291, 6069, 439, 264, 11830, 4188, 379, 21761, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16482765703316193, "compression_ratio": 1.4511627906976745, "no_speech_prob": 1.4969090443628374e-05}, {"id": 692, "seek": 506300, "start": 5081.0, "end": 5085.0, "text": " So there's one called ExcelNet.", "tokens": [50364, 407, 538, 300, 20678, 307, 767, 1596, 257, 955, 3012, 13, 50614, 50614, 865, 13, 50764, 50764, 1033, 13, 50864, 50864, 407, 286, 603, 445, 2661, 2248, 257, 1326, 295, 264, 661, 721, 561, 362, 668, 884, 2698, 12, 48172, 24420, 3097, 13, 51264, 51264, 407, 456, 311, 472, 1219, 19060, 31890, 13, 51464, 51464, 8537, 11, 370, 294, 363, 31479, 11, 562, 291, 6069, 439, 428, 2758, 22667, 11, 291, 6069, 439, 264, 11830, 4188, 379, 21761, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16482765703316193, "compression_ratio": 1.4511627906976745, "no_speech_prob": 1.4969090443628374e-05}, {"id": 693, "seek": 506300, "start": 5085.0, "end": 5092.0, "text": " Basically, so in BERT, when you predict all your mass tokens, you predict all the masks conditionally independently.", "tokens": [50364, 407, 538, 300, 20678, 307, 767, 1596, 257, 955, 3012, 13, 50614, 50614, 865, 13, 50764, 50764, 1033, 13, 50864, 50864, 407, 286, 603, 445, 2661, 2248, 257, 1326, 295, 264, 661, 721, 561, 362, 668, 884, 2698, 12, 48172, 24420, 3097, 13, 51264, 51264, 407, 456, 311, 472, 1219, 19060, 31890, 13, 51464, 51464, 8537, 11, 370, 294, 363, 31479, 11, 562, 291, 6069, 439, 428, 2758, 22667, 11, 291, 6069, 439, 264, 11830, 4188, 379, 21761, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16482765703316193, "compression_ratio": 1.4511627906976745, "no_speech_prob": 1.4969090443628374e-05}, {"id": 694, "seek": 509200, "start": 5092.0, "end": 5099.0, "text": " ExcelNet has a trick that lets you predict these mass tokens autoregressively, but in a random order.", "tokens": [50364, 19060, 31890, 575, 257, 4282, 300, 6653, 291, 6069, 613, 2758, 22667, 1476, 418, 3091, 3413, 11, 457, 294, 257, 4974, 1668, 13, 50714, 50714, 400, 436, 3932, 512, 13797, 490, 884, 300, 13, 50864, 50864, 467, 311, 611, 1738, 335, 33, 31479, 11, 597, 2831, 813, 31226, 484, 2283, 11, 291, 434, 516, 281, 6094, 484, 257, 8310, 295, 30497, 2283, 13, 51314, 51314, 821, 307, 12575, 424, 11, 597, 2831, 813, 31226, 2283, 11, 321, 434, 516, 281, 15802, 2283, 365, 1333, 295, 2531, 2306, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.126758051442576, "compression_ratio": 1.6233766233766234, "no_speech_prob": 3.3205531508428976e-05}, {"id": 695, "seek": 509200, "start": 5099.0, "end": 5102.0, "text": " And they claim some improvements from doing that.", "tokens": [50364, 19060, 31890, 575, 257, 4282, 300, 6653, 291, 6069, 613, 2758, 22667, 1476, 418, 3091, 3413, 11, 457, 294, 257, 4974, 1668, 13, 50714, 50714, 400, 436, 3932, 512, 13797, 490, 884, 300, 13, 50864, 50864, 467, 311, 611, 1738, 335, 33, 31479, 11, 597, 2831, 813, 31226, 484, 2283, 11, 291, 434, 516, 281, 6094, 484, 257, 8310, 295, 30497, 2283, 13, 51314, 51314, 821, 307, 12575, 424, 11, 597, 2831, 813, 31226, 2283, 11, 321, 434, 516, 281, 15802, 2283, 365, 1333, 295, 2531, 2306, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.126758051442576, "compression_ratio": 1.6233766233766234, "no_speech_prob": 3.3205531508428976e-05}, {"id": 696, "seek": 509200, "start": 5102.0, "end": 5111.0, "text": " It's also SpamBERT, which rather than masking out words, you're going to mask out a sequence of consecutive words.", "tokens": [50364, 19060, 31890, 575, 257, 4282, 300, 6653, 291, 6069, 613, 2758, 22667, 1476, 418, 3091, 3413, 11, 457, 294, 257, 4974, 1668, 13, 50714, 50714, 400, 436, 3932, 512, 13797, 490, 884, 300, 13, 50864, 50864, 467, 311, 611, 1738, 335, 33, 31479, 11, 597, 2831, 813, 31226, 484, 2283, 11, 291, 434, 516, 281, 6094, 484, 257, 8310, 295, 30497, 2283, 13, 51314, 51314, 821, 307, 12575, 424, 11, 597, 2831, 813, 31226, 2283, 11, 321, 434, 516, 281, 15802, 2283, 365, 1333, 295, 2531, 2306, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.126758051442576, "compression_ratio": 1.6233766233766234, "no_speech_prob": 3.3205531508428976e-05}, {"id": 697, "seek": 509200, "start": 5111.0, "end": 5121.0, "text": " There is Electra, which rather than masking words, we're going to substitute words with sort of similar ones", "tokens": [50364, 19060, 31890, 575, 257, 4282, 300, 6653, 291, 6069, 613, 2758, 22667, 1476, 418, 3091, 3413, 11, 457, 294, 257, 4974, 1668, 13, 50714, 50714, 400, 436, 3932, 512, 13797, 490, 884, 300, 13, 50864, 50864, 467, 311, 611, 1738, 335, 33, 31479, 11, 597, 2831, 813, 31226, 484, 2283, 11, 291, 434, 516, 281, 6094, 484, 257, 8310, 295, 30497, 2283, 13, 51314, 51314, 821, 307, 12575, 424, 11, 597, 2831, 813, 31226, 2283, 11, 321, 434, 516, 281, 15802, 2283, 365, 1333, 295, 2531, 2306, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.126758051442576, "compression_ratio": 1.6233766233766234, "no_speech_prob": 3.3205531508428976e-05}, {"id": 698, "seek": 512100, "start": 5121.0, "end": 5127.0, "text": " and then have a binary classification problem to tell you which words change or not.", "tokens": [50364, 293, 550, 362, 257, 17434, 21538, 1154, 281, 980, 291, 597, 2283, 1319, 420, 406, 13, 50664, 50664, 821, 311, 7056, 33, 7932, 7327, 11, 597, 307, 363, 31479, 11, 457, 291, 4876, 17443, 2108, 7914, 13, 51014, 51014, 2743, 11, 37210, 44, 293, 37210, 21173, 11, 597, 574, 666, 884, 341, 2120, 38219, 278, 13, 51264, 51264, 467, 4523, 484, 1936, 291, 1190, 257, 363, 31479, 659, 12, 17227, 1760, 10024, 11, 457, 2831, 813, 12919, 294, 3669, 2487, 11, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.19883202945484835, "compression_ratio": 1.415929203539823, "no_speech_prob": 5.5610806157346815e-05}, {"id": 699, "seek": 512100, "start": 5127.0, "end": 5134.0, "text": " There's ALBURTS, which is BERT, but you title weights across layers.", "tokens": [50364, 293, 550, 362, 257, 17434, 21538, 1154, 281, 980, 291, 597, 2283, 1319, 420, 406, 13, 50664, 50664, 821, 311, 7056, 33, 7932, 7327, 11, 597, 307, 363, 31479, 11, 457, 291, 4876, 17443, 2108, 7914, 13, 51014, 51014, 2743, 11, 37210, 44, 293, 37210, 21173, 11, 597, 574, 666, 884, 341, 2120, 38219, 278, 13, 51264, 51264, 467, 4523, 484, 1936, 291, 1190, 257, 363, 31479, 659, 12, 17227, 1760, 10024, 11, 457, 2831, 813, 12919, 294, 3669, 2487, 11, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.19883202945484835, "compression_ratio": 1.415929203539823, "no_speech_prob": 5.5610806157346815e-05}, {"id": 700, "seek": 512100, "start": 5134.0, "end": 5139.0, "text": " Also, XLM and XLMR, which look into doing this multilingualing.", "tokens": [50364, 293, 550, 362, 257, 17434, 21538, 1154, 281, 980, 291, 597, 2283, 1319, 420, 406, 13, 50664, 50664, 821, 311, 7056, 33, 7932, 7327, 11, 597, 307, 363, 31479, 11, 457, 291, 4876, 17443, 2108, 7914, 13, 51014, 51014, 2743, 11, 37210, 44, 293, 37210, 21173, 11, 597, 574, 666, 884, 341, 2120, 38219, 278, 13, 51264, 51264, 467, 4523, 484, 1936, 291, 1190, 257, 363, 31479, 659, 12, 17227, 1760, 10024, 11, 457, 2831, 813, 12919, 294, 3669, 2487, 11, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.19883202945484835, "compression_ratio": 1.415929203539823, "no_speech_prob": 5.5610806157346815e-05}, {"id": 701, "seek": 512100, "start": 5139.0, "end": 5144.0, "text": " It turns out basically you run a BERT pre-training objective, but rather than feeding in English text,", "tokens": [50364, 293, 550, 362, 257, 17434, 21538, 1154, 281, 980, 291, 597, 2283, 1319, 420, 406, 13, 50664, 50664, 821, 311, 7056, 33, 7932, 7327, 11, 597, 307, 363, 31479, 11, 457, 291, 4876, 17443, 2108, 7914, 13, 51014, 51014, 2743, 11, 37210, 44, 293, 37210, 21173, 11, 597, 574, 666, 884, 341, 2120, 38219, 278, 13, 51264, 51264, 467, 4523, 484, 1936, 291, 1190, 257, 363, 31479, 659, 12, 17227, 1760, 10024, 11, 457, 2831, 813, 12919, 294, 3669, 2487, 11, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.19883202945484835, "compression_ratio": 1.415929203539823, "no_speech_prob": 5.5610806157346815e-05}, {"id": 702, "seek": 514400, "start": 5144.0, "end": 5155.0, "text": " feeding in text in every language you can find, it does a great job of learning cross-lingual structure.", "tokens": [50364, 12919, 294, 2487, 294, 633, 2856, 291, 393, 915, 11, 309, 775, 257, 869, 1691, 295, 2539, 3278, 12, 1688, 901, 3877, 13, 50914, 50914, 440, 2141, 935, 286, 528, 291, 281, 747, 490, 341, 307, 534, 411, 11, 613, 366, 439, 733, 295, 17840, 295, 264, 6314, 13, 51564, 51564, 583, 294, 264, 917, 11, 3195, 295, 819, 721, 589, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1837307612101237, "compression_ratio": 1.4602272727272727, "no_speech_prob": 5.337860784493387e-06}, {"id": 703, "seek": 514400, "start": 5155.0, "end": 5168.0, "text": " The key point I want you to take from this is really like, these are all kind of variations of the theme.", "tokens": [50364, 12919, 294, 2487, 294, 633, 2856, 291, 393, 915, 11, 309, 775, 257, 869, 1691, 295, 2539, 3278, 12, 1688, 901, 3877, 13, 50914, 50914, 440, 2141, 935, 286, 528, 291, 281, 747, 490, 341, 307, 534, 411, 11, 613, 366, 439, 733, 295, 17840, 295, 264, 6314, 13, 51564, 51564, 583, 294, 264, 917, 11, 3195, 295, 819, 721, 589, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1837307612101237, "compression_ratio": 1.4602272727272727, "no_speech_prob": 5.337860784493387e-06}, {"id": 704, "seek": 514400, "start": 5168.0, "end": 5171.0, "text": " But in the end, lots of different things work.", "tokens": [50364, 12919, 294, 2487, 294, 633, 2856, 291, 393, 915, 11, 309, 775, 257, 869, 1691, 295, 2539, 3278, 12, 1688, 901, 3877, 13, 50914, 50914, 440, 2141, 935, 286, 528, 291, 281, 747, 490, 341, 307, 534, 411, 11, 613, 366, 439, 733, 295, 17840, 295, 264, 6314, 13, 51564, 51564, 583, 294, 264, 917, 11, 3195, 295, 819, 721, 589, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1837307612101237, "compression_ratio": 1.4602272727272727, "no_speech_prob": 5.337860784493387e-06}, {"id": 705, "seek": 517100, "start": 5171.0, "end": 5176.0, "text": " The important thing is you have big models, you have bidirectional interactions between the words.", "tokens": [50364, 440, 1021, 551, 307, 291, 362, 955, 5245, 11, 291, 362, 12957, 621, 41048, 13280, 1296, 264, 2283, 13, 50614, 50614, 400, 498, 1340, 11, 264, 4373, 291, 360, 341, 412, 307, 264, 881, 1021, 551, 13, 51114, 51114, 407, 472, 27432, 613, 5245, 362, 7619, 307, 436, 434, 787, 884, 733, 295, 2487, 21538, 2740, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.13168497554591443, "compression_ratio": 1.5895953757225434, "no_speech_prob": 1.592816443007905e-05}, {"id": 706, "seek": 517100, "start": 5176.0, "end": 5186.0, "text": " And if anything, the scale you do this at is the most important thing.", "tokens": [50364, 440, 1021, 551, 307, 291, 362, 955, 5245, 11, 291, 362, 12957, 621, 41048, 13280, 1296, 264, 2283, 13, 50614, 50614, 400, 498, 1340, 11, 264, 4373, 291, 360, 341, 412, 307, 264, 881, 1021, 551, 13, 51114, 51114, 407, 472, 27432, 613, 5245, 362, 7619, 307, 436, 434, 787, 884, 733, 295, 2487, 21538, 2740, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.13168497554591443, "compression_ratio": 1.5895953757225434, "no_speech_prob": 1.592816443007905e-05}, {"id": 707, "seek": 517100, "start": 5186.0, "end": 5193.0, "text": " So one limitation these models have described is they're only doing kind of text classification problems.", "tokens": [50364, 440, 1021, 551, 307, 291, 362, 955, 5245, 11, 291, 362, 12957, 621, 41048, 13280, 1296, 264, 2283, 13, 50614, 50614, 400, 498, 1340, 11, 264, 4373, 291, 360, 341, 412, 307, 264, 881, 1021, 551, 13, 51114, 51114, 407, 472, 27432, 613, 5245, 362, 7619, 307, 436, 434, 787, 884, 733, 295, 2487, 21538, 2740, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.13168497554591443, "compression_ratio": 1.5895953757225434, "no_speech_prob": 1.592816443007905e-05}, {"id": 708, "seek": 519300, "start": 5193.0, "end": 5203.0, "text": " But often we'll want to do problems where the output isn't a classification problem, it's actually outputs from more text.", "tokens": [50364, 583, 2049, 321, 603, 528, 281, 360, 2740, 689, 264, 5598, 1943, 380, 257, 21538, 1154, 11, 309, 311, 767, 23930, 490, 544, 2487, 13, 50864, 50864, 407, 659, 12, 17227, 1760, 337, 8310, 12, 1353, 12, 11834, 655, 15983, 13, 51064, 51064, 4453, 10577, 1361, 484, 466, 264, 912, 565, 337, 341, 11, 472, 295, 597, 286, 390, 3288, 294, 11, 1219, 363, 31479, 293, 314, 20, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14914741254832647, "compression_ratio": 1.4322916666666667, "no_speech_prob": 2.025889443757478e-06}, {"id": 709, "seek": 519300, "start": 5203.0, "end": 5207.0, "text": " So pre-training for sequence-to-sequence modeling.", "tokens": [50364, 583, 2049, 321, 603, 528, 281, 360, 2740, 689, 264, 5598, 1943, 380, 257, 21538, 1154, 11, 309, 311, 767, 23930, 490, 544, 2487, 13, 50864, 50864, 407, 659, 12, 17227, 1760, 337, 8310, 12, 1353, 12, 11834, 655, 15983, 13, 51064, 51064, 4453, 10577, 1361, 484, 466, 264, 912, 565, 337, 341, 11, 472, 295, 597, 286, 390, 3288, 294, 11, 1219, 363, 31479, 293, 314, 20, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14914741254832647, "compression_ratio": 1.4322916666666667, "no_speech_prob": 2.025889443757478e-06}, {"id": 710, "seek": 519300, "start": 5207.0, "end": 5218.0, "text": " Two papers came out about the same time for this, one of which I was involved in, called BERT and T5.", "tokens": [50364, 583, 2049, 321, 603, 528, 281, 360, 2740, 689, 264, 5598, 1943, 380, 257, 21538, 1154, 11, 309, 311, 767, 23930, 490, 544, 2487, 13, 50864, 50864, 407, 659, 12, 17227, 1760, 337, 8310, 12, 1353, 12, 11834, 655, 15983, 13, 51064, 51064, 4453, 10577, 1361, 484, 466, 264, 912, 565, 337, 341, 11, 472, 295, 597, 286, 390, 3288, 294, 11, 1219, 363, 31479, 293, 314, 20, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14914741254832647, "compression_ratio": 1.4322916666666667, "no_speech_prob": 2.025889443757478e-06}, {"id": 711, "seek": 521800, "start": 5218.0, "end": 5226.0, "text": " So these models basically are going to pre-train sequence-to-sequence models by denoising text.", "tokens": [50364, 407, 613, 5245, 1936, 366, 516, 281, 659, 12, 83, 7146, 8310, 12, 1353, 12, 11834, 655, 5245, 538, 1441, 78, 3436, 2487, 13, 50764, 50764, 440, 659, 12, 17227, 1760, 10024, 1542, 733, 295, 411, 363, 31479, 13, 50864, 50864, 8537, 439, 291, 434, 516, 281, 360, 307, 747, 512, 2487, 11, 17366, 309, 6063, 538, 9275, 512, 733, 295, 31226, 12232, 11, 51314, 51314, 293, 550, 2831, 813, 6069, 281, 2836, 294, 264, 8247, 82, 11, 291, 434, 516, 281, 3154, 264, 39480, 2487, 666, 257, 8310, 12, 1353, 12, 11834, 655, 2316, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09192460955995502, "compression_ratio": 1.7824074074074074, "no_speech_prob": 8.01252099336125e-06}, {"id": 712, "seek": 521800, "start": 5226.0, "end": 5228.0, "text": " The pre-training objective looks kind of like BERT.", "tokens": [50364, 407, 613, 5245, 1936, 366, 516, 281, 659, 12, 83, 7146, 8310, 12, 1353, 12, 11834, 655, 5245, 538, 1441, 78, 3436, 2487, 13, 50764, 50764, 440, 659, 12, 17227, 1760, 10024, 1542, 733, 295, 411, 363, 31479, 13, 50864, 50864, 8537, 439, 291, 434, 516, 281, 360, 307, 747, 512, 2487, 11, 17366, 309, 6063, 538, 9275, 512, 733, 295, 31226, 12232, 11, 51314, 51314, 293, 550, 2831, 813, 6069, 281, 2836, 294, 264, 8247, 82, 11, 291, 434, 516, 281, 3154, 264, 39480, 2487, 666, 257, 8310, 12, 1353, 12, 11834, 655, 2316, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09192460955995502, "compression_ratio": 1.7824074074074074, "no_speech_prob": 8.01252099336125e-06}, {"id": 713, "seek": 521800, "start": 5228.0, "end": 5237.0, "text": " Basically all you're going to do is take some text, corrupt it somehow by applying some kind of masking scheme,", "tokens": [50364, 407, 613, 5245, 1936, 366, 516, 281, 659, 12, 83, 7146, 8310, 12, 1353, 12, 11834, 655, 5245, 538, 1441, 78, 3436, 2487, 13, 50764, 50764, 440, 659, 12, 17227, 1760, 10024, 1542, 733, 295, 411, 363, 31479, 13, 50864, 50864, 8537, 439, 291, 434, 516, 281, 360, 307, 747, 512, 2487, 11, 17366, 309, 6063, 538, 9275, 512, 733, 295, 31226, 12232, 11, 51314, 51314, 293, 550, 2831, 813, 6069, 281, 2836, 294, 264, 8247, 82, 11, 291, 434, 516, 281, 3154, 264, 39480, 2487, 666, 257, 8310, 12, 1353, 12, 11834, 655, 2316, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09192460955995502, "compression_ratio": 1.7824074074074074, "no_speech_prob": 8.01252099336125e-06}, {"id": 714, "seek": 521800, "start": 5237.0, "end": 5244.0, "text": " and then rather than predict to fill in the blanks, you're going to feed the corrupted text into a sequence-to-sequence model", "tokens": [50364, 407, 613, 5245, 1936, 366, 516, 281, 659, 12, 83, 7146, 8310, 12, 1353, 12, 11834, 655, 5245, 538, 1441, 78, 3436, 2487, 13, 50764, 50764, 440, 659, 12, 17227, 1760, 10024, 1542, 733, 295, 411, 363, 31479, 13, 50864, 50864, 8537, 439, 291, 434, 516, 281, 360, 307, 747, 512, 2487, 11, 17366, 309, 6063, 538, 9275, 512, 733, 295, 31226, 12232, 11, 51314, 51314, 293, 550, 2831, 813, 6069, 281, 2836, 294, 264, 8247, 82, 11, 291, 434, 516, 281, 3154, 264, 39480, 2487, 666, 257, 8310, 12, 1353, 12, 11834, 655, 2316, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09192460955995502, "compression_ratio": 1.7824074074074074, "no_speech_prob": 8.01252099336125e-06}, {"id": 715, "seek": 524400, "start": 5244.0, "end": 5248.0, "text": " and try and predict the complete output.", "tokens": [50364, 293, 853, 293, 6069, 264, 3566, 5598, 13, 50564, 50564, 400, 341, 307, 733, 295, 1481, 570, 550, 291, 393, 767, 352, 4399, 445, 31226, 2487, 50964, 50964, 293, 291, 393, 3079, 604, 4974, 17959, 281, 264, 4846, 291, 528, 13, 51214, 51214, 407, 337, 1365, 11, 291, 393, 39426, 264, 16579, 11, 420, 12097, 1379, 20312, 11, 420, 8969, 777, 20312, 11, 293, 264, 8310, 12, 1353, 12, 11834, 655, 8388, 307, 588, 11358, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19047529697418214, "compression_ratio": 1.6018957345971565, "no_speech_prob": 4.784722023032373e-06}, {"id": 716, "seek": 524400, "start": 5248.0, "end": 5256.0, "text": " And this is kind of nice because then you can actually go beyond just masking text", "tokens": [50364, 293, 853, 293, 6069, 264, 3566, 5598, 13, 50564, 50564, 400, 341, 307, 733, 295, 1481, 570, 550, 291, 393, 767, 352, 4399, 445, 31226, 2487, 50964, 50964, 293, 291, 393, 3079, 604, 4974, 17959, 281, 264, 4846, 291, 528, 13, 51214, 51214, 407, 337, 1365, 11, 291, 393, 39426, 264, 16579, 11, 420, 12097, 1379, 20312, 11, 420, 8969, 777, 20312, 11, 293, 264, 8310, 12, 1353, 12, 11834, 655, 8388, 307, 588, 11358, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19047529697418214, "compression_ratio": 1.6018957345971565, "no_speech_prob": 4.784722023032373e-06}, {"id": 717, "seek": 524400, "start": 5256.0, "end": 5261.0, "text": " and you can apply any random corruption to the input you want.", "tokens": [50364, 293, 853, 293, 6069, 264, 3566, 5598, 13, 50564, 50564, 400, 341, 307, 733, 295, 1481, 570, 550, 291, 393, 767, 352, 4399, 445, 31226, 2487, 50964, 50964, 293, 291, 393, 3079, 604, 4974, 17959, 281, 264, 4846, 291, 528, 13, 51214, 51214, 407, 337, 1365, 11, 291, 393, 39426, 264, 16579, 11, 420, 12097, 1379, 20312, 11, 420, 8969, 777, 20312, 11, 293, 264, 8310, 12, 1353, 12, 11834, 655, 8388, 307, 588, 11358, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19047529697418214, "compression_ratio": 1.6018957345971565, "no_speech_prob": 4.784722023032373e-06}, {"id": 718, "seek": 524400, "start": 5261.0, "end": 5271.0, "text": " So for example, you can shuffle the sentences, or delete whole phrases, or insert new phrases, and the sequence-to-sequence framework is very flexible.", "tokens": [50364, 293, 853, 293, 6069, 264, 3566, 5598, 13, 50564, 50564, 400, 341, 307, 733, 295, 1481, 570, 550, 291, 393, 767, 352, 4399, 445, 31226, 2487, 50964, 50964, 293, 291, 393, 3079, 604, 4974, 17959, 281, 264, 4846, 291, 528, 13, 51214, 51214, 407, 337, 1365, 11, 291, 393, 39426, 264, 16579, 11, 420, 12097, 1379, 20312, 11, 420, 8969, 777, 20312, 11, 293, 264, 8310, 12, 1353, 12, 11834, 655, 8388, 307, 588, 11358, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19047529697418214, "compression_ratio": 1.6018957345971565, "no_speech_prob": 4.784722023032373e-06}, {"id": 719, "seek": 527100, "start": 5271.0, "end": 5279.0, "text": " But it turns out just doing simple masking actually works about as well as anything else.", "tokens": [50364, 583, 309, 4523, 484, 445, 884, 2199, 31226, 767, 1985, 466, 382, 731, 382, 1340, 1646, 13, 50764, 50764, 400, 550, 498, 291, 360, 341, 382, 731, 382, 884, 731, 322, 721, 411, 8683, 455, 293, 8510, 11, 597, 366, 21538, 43751, 11, 51114, 51114, 291, 393, 611, 483, 1785, 12, 2670, 12, 3322, 12, 446, 3542, 322, 9608, 411, 14611, 2144, 11, 370, 689, 264, 5598, 307, 8570, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.245465510600322, "compression_ratio": 1.535, "no_speech_prob": 2.684078708625748e-06}, {"id": 720, "seek": 527100, "start": 5279.0, "end": 5286.0, "text": " And then if you do this as well as doing well on things like Squab and Blue, which are classification benchmarks,", "tokens": [50364, 583, 309, 4523, 484, 445, 884, 2199, 31226, 767, 1985, 466, 382, 731, 382, 1340, 1646, 13, 50764, 50764, 400, 550, 498, 291, 360, 341, 382, 731, 382, 884, 731, 322, 721, 411, 8683, 455, 293, 8510, 11, 597, 366, 21538, 43751, 11, 51114, 51114, 291, 393, 611, 483, 1785, 12, 2670, 12, 3322, 12, 446, 3542, 322, 9608, 411, 14611, 2144, 11, 370, 689, 264, 5598, 307, 8570, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.245465510600322, "compression_ratio": 1.535, "no_speech_prob": 2.684078708625748e-06}, {"id": 721, "seek": 527100, "start": 5286.0, "end": 5300.0, "text": " you can also get state-of-the-art results on tasks like summarization, so where the output is attached.", "tokens": [50364, 583, 309, 4523, 484, 445, 884, 2199, 31226, 767, 1985, 466, 382, 731, 382, 1340, 1646, 13, 50764, 50764, 400, 550, 498, 291, 360, 341, 382, 731, 382, 884, 731, 322, 721, 411, 8683, 455, 293, 8510, 11, 597, 366, 21538, 43751, 11, 51114, 51114, 291, 393, 611, 483, 1785, 12, 2670, 12, 3322, 12, 446, 3542, 322, 9608, 411, 14611, 2144, 11, 370, 689, 264, 5598, 307, 8570, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.245465510600322, "compression_ratio": 1.535, "no_speech_prob": 2.684078708625748e-06}, {"id": 722, "seek": 530000, "start": 5300.0, "end": 5305.0, "text": " On the left here we've got some length of document we've fed in. We asked them all to produce some kind of summary of this,", "tokens": [50364, 1282, 264, 1411, 510, 321, 600, 658, 512, 4641, 295, 4166, 321, 600, 4636, 294, 13, 492, 2351, 552, 439, 281, 5258, 512, 733, 295, 12691, 295, 341, 11, 50614, 50614, 293, 309, 307, 257, 869, 1691, 11, 411, 767, 1228, 341, 4319, 490, 2108, 264, 1379, 4166, 293, 5039, 721, 411, 4965, 5158, 13, 51014, 51014, 21082, 11, 341, 1716, 5112, 505, 512, 295, 264, 11498, 295, 264, 15743, 13, 51314, 51314, 1033, 11, 321, 434, 2614, 484, 295, 565, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.3619179836539335, "compression_ratio": 1.5981735159817352, "no_speech_prob": 6.0484294408524875e-06}, {"id": 723, "seek": 530000, "start": 5305.0, "end": 5313.0, "text": " and it is a great job, like actually using this context from across the whole document and solve things like coreference.", "tokens": [50364, 1282, 264, 1411, 510, 321, 600, 658, 512, 4641, 295, 4166, 321, 600, 4636, 294, 13, 492, 2351, 552, 439, 281, 5258, 512, 733, 295, 12691, 295, 341, 11, 50614, 50614, 293, 309, 307, 257, 869, 1691, 11, 411, 767, 1228, 341, 4319, 490, 2108, 264, 1379, 4166, 293, 5039, 721, 411, 4965, 5158, 13, 51014, 51014, 21082, 11, 341, 1716, 5112, 505, 512, 295, 264, 11498, 295, 264, 15743, 13, 51314, 51314, 1033, 11, 321, 434, 2614, 484, 295, 565, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.3619179836539335, "compression_ratio": 1.5981735159817352, "no_speech_prob": 6.0484294408524875e-06}, {"id": 724, "seek": 530000, "start": 5313.0, "end": 5319.0, "text": " Generally, this project tells us some of the surrounding of the inputs.", "tokens": [50364, 1282, 264, 1411, 510, 321, 600, 658, 512, 4641, 295, 4166, 321, 600, 4636, 294, 13, 492, 2351, 552, 439, 281, 5258, 512, 733, 295, 12691, 295, 341, 11, 50614, 50614, 293, 309, 307, 257, 869, 1691, 11, 411, 767, 1228, 341, 4319, 490, 2108, 264, 1379, 4166, 293, 5039, 721, 411, 4965, 5158, 13, 51014, 51014, 21082, 11, 341, 1716, 5112, 505, 512, 295, 264, 11498, 295, 264, 15743, 13, 51314, 51314, 1033, 11, 321, 434, 2614, 484, 295, 565, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.3619179836539335, "compression_ratio": 1.5981735159817352, "no_speech_prob": 6.0484294408524875e-06}, {"id": 725, "seek": 530000, "start": 5319.0, "end": 5323.0, "text": " Okay, we're running out of time.", "tokens": [50364, 1282, 264, 1411, 510, 321, 600, 658, 512, 4641, 295, 4166, 321, 600, 4636, 294, 13, 492, 2351, 552, 439, 281, 5258, 512, 733, 295, 12691, 295, 341, 11, 50614, 50614, 293, 309, 307, 257, 869, 1691, 11, 411, 767, 1228, 341, 4319, 490, 2108, 264, 1379, 4166, 293, 5039, 721, 411, 4965, 5158, 13, 51014, 51014, 21082, 11, 341, 1716, 5112, 505, 512, 295, 264, 11498, 295, 264, 15743, 13, 51314, 51314, 1033, 11, 321, 434, 2614, 484, 295, 565, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.3619179836539335, "compression_ratio": 1.5981735159817352, "no_speech_prob": 6.0484294408524875e-06}, {"id": 726, "seek": 532300, "start": 5323.0, "end": 5331.0, "text": " So briefly, I don't think this is the end story, I don't think NLP is now solved.", "tokens": [50364, 407, 10515, 11, 286, 500, 380, 519, 341, 307, 264, 917, 1657, 11, 286, 500, 380, 519, 426, 45196, 307, 586, 13041, 13, 50764, 50764, 316, 1326, 1269, 1651, 11, 597, 286, 519, 366, 1880, 11, 3009, 577, 321, 13365, 3678, 3601, 666, 341, 13, 51114, 51114, 1144, 321, 445, 528, 613, 5245, 281, 853, 293, 27478, 264, 1379, 7703, 11, 420, 820, 321, 1322, 4675, 15902, 6063, 30, 51564, 51564, 1018, 1580, 3038, 493, 3071, 11, 577, 360, 321, 2316, 938, 8512, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12610979513688522, "compression_ratio": 1.5523012552301256, "no_speech_prob": 1.8050248399958946e-05}, {"id": 727, "seek": 532300, "start": 5331.0, "end": 5338.0, "text": " A few open questions, which I think are interesting, including how we integrate background knowledge into this.", "tokens": [50364, 407, 10515, 11, 286, 500, 380, 519, 341, 307, 264, 917, 1657, 11, 286, 500, 380, 519, 426, 45196, 307, 586, 13041, 13, 50764, 50764, 316, 1326, 1269, 1651, 11, 597, 286, 519, 366, 1880, 11, 3009, 577, 321, 13365, 3678, 3601, 666, 341, 13, 51114, 51114, 1144, 321, 445, 528, 613, 5245, 281, 853, 293, 27478, 264, 1379, 7703, 11, 420, 820, 321, 1322, 4675, 15902, 6063, 30, 51564, 51564, 1018, 1580, 3038, 493, 3071, 11, 577, 360, 321, 2316, 938, 8512, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12610979513688522, "compression_ratio": 1.5523012552301256, "no_speech_prob": 1.8050248399958946e-05}, {"id": 728, "seek": 532300, "start": 5338.0, "end": 5347.0, "text": " Do we just want these models to try and memorize the whole Internet, or should we build memory mechanisms somehow?", "tokens": [50364, 407, 10515, 11, 286, 500, 380, 519, 341, 307, 264, 917, 1657, 11, 286, 500, 380, 519, 426, 45196, 307, 586, 13041, 13, 50764, 50764, 316, 1326, 1269, 1651, 11, 597, 286, 519, 366, 1880, 11, 3009, 577, 321, 13365, 3678, 3601, 666, 341, 13, 51114, 51114, 1144, 321, 445, 528, 613, 5245, 281, 853, 293, 27478, 264, 1379, 7703, 11, 420, 820, 321, 1322, 4675, 15902, 6063, 30, 51564, 51564, 1018, 1580, 3038, 493, 3071, 11, 577, 360, 321, 2316, 938, 8512, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12610979513688522, "compression_ratio": 1.5523012552301256, "no_speech_prob": 1.8050248399958946e-05}, {"id": 729, "seek": 532300, "start": 5347.0, "end": 5350.0, "text": " As someone brought up earlier, how do we model long documents?", "tokens": [50364, 407, 10515, 11, 286, 500, 380, 519, 341, 307, 264, 917, 1657, 11, 286, 500, 380, 519, 426, 45196, 307, 586, 13041, 13, 50764, 50764, 316, 1326, 1269, 1651, 11, 597, 286, 519, 366, 1880, 11, 3009, 577, 321, 13365, 3678, 3601, 666, 341, 13, 51114, 51114, 1144, 321, 445, 528, 613, 5245, 281, 853, 293, 27478, 264, 1379, 7703, 11, 420, 820, 321, 1322, 4675, 15902, 6063, 30, 51564, 51564, 1018, 1580, 3038, 493, 3071, 11, 577, 360, 321, 2316, 938, 8512, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12610979513688522, "compression_ratio": 1.5523012552301256, "no_speech_prob": 1.8050248399958946e-05}, {"id": 730, "seek": 535000, "start": 5350.0, "end": 5358.0, "text": " We're typically doing 512 tokens here, how can we do a whole book at once?", "tokens": [50364, 492, 434, 5850, 884, 1025, 4762, 22667, 510, 11, 577, 393, 321, 360, 257, 1379, 1446, 412, 1564, 30, 50764, 50764, 1485, 2693, 25239, 1840, 551, 466, 341, 307, 321, 362, 264, 912, 2316, 9482, 300, 393, 5039, 439, 3685, 295, 2740, 11, 51064, 51064, 457, 309, 12258, 281, 406, 312, 1075, 281, 5039, 439, 613, 2740, 412, 1564, 11, 293, 5850, 291, 915, 257, 4994, 2316, 337, 1184, 5633, 13, 51364, 51364, 467, 576, 312, 869, 498, 291, 767, 362, 472, 2316, 300, 39890, 1203, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1649225151145851, "compression_ratio": 1.6223175965665235, "no_speech_prob": 3.2692387321731076e-05}, {"id": 731, "seek": 535000, "start": 5358.0, "end": 5364.0, "text": " One unsatisfying thing about this is we have the same model architecture that can solve all kinds of problems,", "tokens": [50364, 492, 434, 5850, 884, 1025, 4762, 22667, 510, 11, 577, 393, 321, 360, 257, 1379, 1446, 412, 1564, 30, 50764, 50764, 1485, 2693, 25239, 1840, 551, 466, 341, 307, 321, 362, 264, 912, 2316, 9482, 300, 393, 5039, 439, 3685, 295, 2740, 11, 51064, 51064, 457, 309, 12258, 281, 406, 312, 1075, 281, 5039, 439, 613, 2740, 412, 1564, 11, 293, 5850, 291, 915, 257, 4994, 2316, 337, 1184, 5633, 13, 51364, 51364, 467, 576, 312, 869, 498, 291, 767, 362, 472, 2316, 300, 39890, 1203, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1649225151145851, "compression_ratio": 1.6223175965665235, "no_speech_prob": 3.2692387321731076e-05}, {"id": 732, "seek": 535000, "start": 5364.0, "end": 5370.0, "text": " but it tends to not be able to solve all these problems at once, and typically you find a separate model for each task.", "tokens": [50364, 492, 434, 5850, 884, 1025, 4762, 22667, 510, 11, 577, 393, 321, 360, 257, 1379, 1446, 412, 1564, 30, 50764, 50764, 1485, 2693, 25239, 1840, 551, 466, 341, 307, 321, 362, 264, 912, 2316, 9482, 300, 393, 5039, 439, 3685, 295, 2740, 11, 51064, 51064, 457, 309, 12258, 281, 406, 312, 1075, 281, 5039, 439, 613, 2740, 412, 1564, 11, 293, 5850, 291, 915, 257, 4994, 2316, 337, 1184, 5633, 13, 51364, 51364, 467, 576, 312, 869, 498, 291, 767, 362, 472, 2316, 300, 39890, 1203, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1649225151145851, "compression_ratio": 1.6223175965665235, "no_speech_prob": 3.2692387321731076e-05}, {"id": 733, "seek": 535000, "start": 5370.0, "end": 5379.0, "text": " It would be great if you actually have one model that solves everything.", "tokens": [50364, 492, 434, 5850, 884, 1025, 4762, 22667, 510, 11, 577, 393, 321, 360, 257, 1379, 1446, 412, 1564, 30, 50764, 50764, 1485, 2693, 25239, 1840, 551, 466, 341, 307, 321, 362, 264, 912, 2316, 9482, 300, 393, 5039, 439, 3685, 295, 2740, 11, 51064, 51064, 457, 309, 12258, 281, 406, 312, 1075, 281, 5039, 439, 613, 2740, 412, 1564, 11, 293, 5850, 291, 915, 257, 4994, 2316, 337, 1184, 5633, 13, 51364, 51364, 467, 576, 312, 869, 498, 291, 767, 362, 472, 2316, 300, 39890, 1203, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1649225151145851, "compression_ratio": 1.6223175965665235, "no_speech_prob": 3.2692387321731076e-05}, {"id": 734, "seek": 537900, "start": 5379.0, "end": 5385.0, "text": " Just kind of related to that, we basically have human-level performance in any task where you have, say,", "tokens": [50364, 1449, 733, 295, 4077, 281, 300, 11, 321, 1936, 362, 1952, 12, 12418, 3389, 294, 604, 5633, 689, 291, 362, 11, 584, 11, 50664, 50664, 2319, 11, 1360, 21335, 5110, 281, 312, 2539, 490, 11, 457, 393, 321, 1322, 5245, 300, 360, 731, 365, 502, 11, 1360, 420, 1266, 420, 502, 21335, 5110, 30, 51264, 51264, 400, 2721, 11, 457, 561, 1565, 613, 1651, 484, 281, 536, 1968, 613, 5245, 366, 445, 767, 3701, 2856, 11, 51514, 51514, 420, 436, 434, 445, 534, 665, 412, 7697, 43751, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.31782032095867657, "compression_ratio": 1.6153846153846154, "no_speech_prob": 3.7618672649841756e-05}, {"id": 735, "seek": 537900, "start": 5385.0, "end": 5397.0, "text": " 100,000 labeled examples to be learning from, but can we build models that do well with 1,000 or 10 or 1 labeled examples?", "tokens": [50364, 1449, 733, 295, 4077, 281, 300, 11, 321, 1936, 362, 1952, 12, 12418, 3389, 294, 604, 5633, 689, 291, 362, 11, 584, 11, 50664, 50664, 2319, 11, 1360, 21335, 5110, 281, 312, 2539, 490, 11, 457, 393, 321, 1322, 5245, 300, 360, 731, 365, 502, 11, 1360, 420, 1266, 420, 502, 21335, 5110, 30, 51264, 51264, 400, 2721, 11, 457, 561, 1565, 613, 1651, 484, 281, 536, 1968, 613, 5245, 366, 445, 767, 3701, 2856, 11, 51514, 51514, 420, 436, 434, 445, 534, 665, 412, 7697, 43751, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.31782032095867657, "compression_ratio": 1.6153846153846154, "no_speech_prob": 3.7618672649841756e-05}, {"id": 736, "seek": 537900, "start": 5397.0, "end": 5402.0, "text": " And finally, but people bring these questions out to see whether these models are just actually understanding language,", "tokens": [50364, 1449, 733, 295, 4077, 281, 300, 11, 321, 1936, 362, 1952, 12, 12418, 3389, 294, 604, 5633, 689, 291, 362, 11, 584, 11, 50664, 50664, 2319, 11, 1360, 21335, 5110, 281, 312, 2539, 490, 11, 457, 393, 321, 1322, 5245, 300, 360, 731, 365, 502, 11, 1360, 420, 1266, 420, 502, 21335, 5110, 30, 51264, 51264, 400, 2721, 11, 457, 561, 1565, 613, 1651, 484, 281, 536, 1968, 613, 5245, 366, 445, 767, 3701, 2856, 11, 51514, 51514, 420, 436, 434, 445, 534, 665, 412, 7697, 43751, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.31782032095867657, "compression_ratio": 1.6153846153846154, "no_speech_prob": 3.7618672649841756e-05}, {"id": 737, "seek": 537900, "start": 5402.0, "end": 5407.0, "text": " or they're just really good at breaking benchmarks.", "tokens": [50364, 1449, 733, 295, 4077, 281, 300, 11, 321, 1936, 362, 1952, 12, 12418, 3389, 294, 604, 5633, 689, 291, 362, 11, 584, 11, 50664, 50664, 2319, 11, 1360, 21335, 5110, 281, 312, 2539, 490, 11, 457, 393, 321, 1322, 5245, 300, 360, 731, 365, 502, 11, 1360, 420, 1266, 420, 502, 21335, 5110, 30, 51264, 51264, 400, 2721, 11, 457, 561, 1565, 613, 1651, 484, 281, 536, 1968, 613, 5245, 366, 445, 767, 3701, 2856, 11, 51514, 51514, 420, 436, 434, 445, 534, 665, 412, 7697, 43751, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.31782032095867657, "compression_ratio": 1.6153846153846154, "no_speech_prob": 3.7618672649841756e-05}, {"id": 738, "seek": 540700, "start": 5407.0, "end": 5419.0, "text": " Okay, so to wrap up this lecture, I think the main sort of takeaways from this are", "tokens": [50364, 1033, 11, 370, 281, 7019, 493, 341, 7991, 11, 286, 519, 264, 2135, 1333, 295, 45584, 490, 341, 366, 50964, 50964, 613, 733, 295, 2295, 12, 5614, 1937, 5245, 411, 27938, 433, 589, 869, 13, 51114, 51114, 492, 4659, 380, 853, 281, 20803, 2316, 43002, 3877, 13, 51214, 51214, 492, 820, 362, 588, 40189, 5245, 11, 293, 820, 362, 3195, 295, 2487, 281, 718, 552, 1466, 2035, 3877, 436, 643, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.17510852813720704, "compression_ratio": 1.595, "no_speech_prob": 3.5340057365829125e-05}, {"id": 739, "seek": 540700, "start": 5419.0, "end": 5422.0, "text": " these kind of low-biased models like Transformers work great.", "tokens": [50364, 1033, 11, 370, 281, 7019, 493, 341, 7991, 11, 286, 519, 264, 2135, 1333, 295, 45584, 490, 341, 366, 50964, 50964, 613, 733, 295, 2295, 12, 5614, 1937, 5245, 411, 27938, 433, 589, 869, 13, 51114, 51114, 492, 4659, 380, 853, 281, 20803, 2316, 43002, 3877, 13, 51214, 51214, 492, 820, 362, 588, 40189, 5245, 11, 293, 820, 362, 3195, 295, 2487, 281, 718, 552, 1466, 2035, 3877, 436, 643, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.17510852813720704, "compression_ratio": 1.595, "no_speech_prob": 3.5340057365829125e-05}, {"id": 740, "seek": 540700, "start": 5422.0, "end": 5424.0, "text": " We shouldn't try to explicitly model linguistic structure.", "tokens": [50364, 1033, 11, 370, 281, 7019, 493, 341, 7991, 11, 286, 519, 264, 2135, 1333, 295, 45584, 490, 341, 366, 50964, 50964, 613, 733, 295, 2295, 12, 5614, 1937, 5245, 411, 27938, 433, 589, 869, 13, 51114, 51114, 492, 4659, 380, 853, 281, 20803, 2316, 43002, 3877, 13, 51214, 51214, 492, 820, 362, 588, 40189, 5245, 11, 293, 820, 362, 3195, 295, 2487, 281, 718, 552, 1466, 2035, 3877, 436, 643, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.17510852813720704, "compression_ratio": 1.595, "no_speech_prob": 3.5340057365829125e-05}, {"id": 741, "seek": 540700, "start": 5424.0, "end": 5433.0, "text": " We should have very expressive models, and should have lots of text to let them learn whatever structure they need.", "tokens": [50364, 1033, 11, 370, 281, 7019, 493, 341, 7991, 11, 286, 519, 264, 2135, 1333, 295, 45584, 490, 341, 366, 50964, 50964, 613, 733, 295, 2295, 12, 5614, 1937, 5245, 411, 27938, 433, 589, 869, 13, 51114, 51114, 492, 4659, 380, 853, 281, 20803, 2316, 43002, 3877, 13, 51214, 51214, 492, 820, 362, 588, 40189, 5245, 11, 293, 820, 362, 3195, 295, 2487, 281, 718, 552, 1466, 2035, 3877, 436, 643, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.17510852813720704, "compression_ratio": 1.595, "no_speech_prob": 3.5340057365829125e-05}, {"id": 742, "seek": 543300, "start": 5433.0, "end": 5440.0, "text": " So, I think that incorporating words and text is a great unsupervised learning objective.", "tokens": [50364, 407, 11, 286, 519, 300, 33613, 2283, 293, 2487, 307, 257, 869, 2693, 12879, 24420, 2539, 10024, 13, 50714, 50714, 467, 311, 11462, 281, 16091, 2283, 294, 4319, 11, 294, 1729, 12957, 621, 41048, 4319, 13, 51064, 51064, 1033, 11, 370, 300, 311, 439, 286, 362, 11, 370, 1309, 291, 588, 709, 337, 4764, 13, 51264, 51264, 961, 311, 536, 498, 321, 362, 512, 1651, 586, 13, 286, 519, 456, 486, 312, 2940, 13, 51564, 51564, 1044, 291, 11, 6602, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.29779023563160617, "compression_ratio": 1.5497630331753554, "no_speech_prob": 7.252291834447533e-05}, {"id": 743, "seek": 543300, "start": 5440.0, "end": 5447.0, "text": " It's crucial to incorporate words in context, in particular bidirectional context.", "tokens": [50364, 407, 11, 286, 519, 300, 33613, 2283, 293, 2487, 307, 257, 869, 2693, 12879, 24420, 2539, 10024, 13, 50714, 50714, 467, 311, 11462, 281, 16091, 2283, 294, 4319, 11, 294, 1729, 12957, 621, 41048, 4319, 13, 51064, 51064, 1033, 11, 370, 300, 311, 439, 286, 362, 11, 370, 1309, 291, 588, 709, 337, 4764, 13, 51264, 51264, 961, 311, 536, 498, 321, 362, 512, 1651, 586, 13, 286, 519, 456, 486, 312, 2940, 13, 51564, 51564, 1044, 291, 11, 6602, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.29779023563160617, "compression_ratio": 1.5497630331753554, "no_speech_prob": 7.252291834447533e-05}, {"id": 744, "seek": 543300, "start": 5447.0, "end": 5451.0, "text": " Okay, so that's all I have, so thank you very much for listening.", "tokens": [50364, 407, 11, 286, 519, 300, 33613, 2283, 293, 2487, 307, 257, 869, 2693, 12879, 24420, 2539, 10024, 13, 50714, 50714, 467, 311, 11462, 281, 16091, 2283, 294, 4319, 11, 294, 1729, 12957, 621, 41048, 4319, 13, 51064, 51064, 1033, 11, 370, 300, 311, 439, 286, 362, 11, 370, 1309, 291, 588, 709, 337, 4764, 13, 51264, 51264, 961, 311, 536, 498, 321, 362, 512, 1651, 586, 13, 286, 519, 456, 486, 312, 2940, 13, 51564, 51564, 1044, 291, 11, 6602, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.29779023563160617, "compression_ratio": 1.5497630331753554, "no_speech_prob": 7.252291834447533e-05}, {"id": 745, "seek": 543300, "start": 5451.0, "end": 5457.0, "text": " Let's see if we have some questions now. I think there will be several.", "tokens": [50364, 407, 11, 286, 519, 300, 33613, 2283, 293, 2487, 307, 257, 869, 2693, 12879, 24420, 2539, 10024, 13, 50714, 50714, 467, 311, 11462, 281, 16091, 2283, 294, 4319, 11, 294, 1729, 12957, 621, 41048, 4319, 13, 51064, 51064, 1033, 11, 370, 300, 311, 439, 286, 362, 11, 370, 1309, 291, 588, 709, 337, 4764, 13, 51264, 51264, 961, 311, 536, 498, 321, 362, 512, 1651, 586, 13, 286, 519, 456, 486, 312, 2940, 13, 51564, 51564, 1044, 291, 11, 6602, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.29779023563160617, "compression_ratio": 1.5497630331753554, "no_speech_prob": 7.252291834447533e-05}, {"id": 746, "seek": 543300, "start": 5457.0, "end": 5460.0, "text": " Thank you, Mike.", "tokens": [50364, 407, 11, 286, 519, 300, 33613, 2283, 293, 2487, 307, 257, 869, 2693, 12879, 24420, 2539, 10024, 13, 50714, 50714, 467, 311, 11462, 281, 16091, 2283, 294, 4319, 11, 294, 1729, 12957, 621, 41048, 4319, 13, 51064, 51064, 1033, 11, 370, 300, 311, 439, 286, 362, 11, 370, 1309, 291, 588, 709, 337, 4764, 13, 51264, 51264, 961, 311, 536, 498, 321, 362, 512, 1651, 586, 13, 286, 519, 456, 486, 312, 2940, 13, 51564, 51564, 1044, 291, 11, 6602, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.29779023563160617, "compression_ratio": 1.5497630331753554, "no_speech_prob": 7.252291834447533e-05}, {"id": 747, "seek": 546000, "start": 5460.0, "end": 5465.0, "text": " Yeah, there's a whole bunch of discussions while you were talking.", "tokens": [50364, 865, 11, 456, 311, 257, 1379, 3840, 295, 11088, 1339, 291, 645, 1417, 13, 50614, 50614, 1033, 13, 50664, 50664, 37156, 281, 3683, 10577, 293, 28708, 295, 3683, 10392, 294, 264, 3678, 13, 50864, 50864, 1033, 13, 50914, 50914, 2639, 544, 1651, 30, 51014, 51014, 865, 11, 286, 632, 472, 13, 1282, 472, 295, 264, 1269, 1651, 11, 309, 311, 411, 3701, 1968, 420, 406, 613, 5245, 366, 767, 3701, 264, 2856, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11084125568340351, "compression_ratio": 1.551219512195122, "no_speech_prob": 6.400566780939698e-05}, {"id": 748, "seek": 546000, "start": 5465.0, "end": 5466.0, "text": " Okay.", "tokens": [50364, 865, 11, 456, 311, 257, 1379, 3840, 295, 11088, 1339, 291, 645, 1417, 13, 50614, 50614, 1033, 13, 50664, 50664, 37156, 281, 3683, 10577, 293, 28708, 295, 3683, 10392, 294, 264, 3678, 13, 50864, 50864, 1033, 13, 50914, 50914, 2639, 544, 1651, 30, 51014, 51014, 865, 11, 286, 632, 472, 13, 1282, 472, 295, 264, 1269, 1651, 11, 309, 311, 411, 3701, 1968, 420, 406, 613, 5245, 366, 767, 3701, 264, 2856, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11084125568340351, "compression_ratio": 1.551219512195122, "no_speech_prob": 6.400566780939698e-05}, {"id": 749, "seek": 546000, "start": 5466.0, "end": 5470.0, "text": " Links to various papers and explanations of various concepts in the background.", "tokens": [50364, 865, 11, 456, 311, 257, 1379, 3840, 295, 11088, 1339, 291, 645, 1417, 13, 50614, 50614, 1033, 13, 50664, 50664, 37156, 281, 3683, 10577, 293, 28708, 295, 3683, 10392, 294, 264, 3678, 13, 50864, 50864, 1033, 13, 50914, 50914, 2639, 544, 1651, 30, 51014, 51014, 865, 11, 286, 632, 472, 13, 1282, 472, 295, 264, 1269, 1651, 11, 309, 311, 411, 3701, 1968, 420, 406, 613, 5245, 366, 767, 3701, 264, 2856, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11084125568340351, "compression_ratio": 1.551219512195122, "no_speech_prob": 6.400566780939698e-05}, {"id": 750, "seek": 546000, "start": 5470.0, "end": 5471.0, "text": " Okay.", "tokens": [50364, 865, 11, 456, 311, 257, 1379, 3840, 295, 11088, 1339, 291, 645, 1417, 13, 50614, 50614, 1033, 13, 50664, 50664, 37156, 281, 3683, 10577, 293, 28708, 295, 3683, 10392, 294, 264, 3678, 13, 50864, 50864, 1033, 13, 50914, 50914, 2639, 544, 1651, 30, 51014, 51014, 865, 11, 286, 632, 472, 13, 1282, 472, 295, 264, 1269, 1651, 11, 309, 311, 411, 3701, 1968, 420, 406, 613, 5245, 366, 767, 3701, 264, 2856, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11084125568340351, "compression_ratio": 1.551219512195122, "no_speech_prob": 6.400566780939698e-05}, {"id": 751, "seek": 546000, "start": 5471.0, "end": 5473.0, "text": " Any more questions?", "tokens": [50364, 865, 11, 456, 311, 257, 1379, 3840, 295, 11088, 1339, 291, 645, 1417, 13, 50614, 50614, 1033, 13, 50664, 50664, 37156, 281, 3683, 10577, 293, 28708, 295, 3683, 10392, 294, 264, 3678, 13, 50864, 50864, 1033, 13, 50914, 50914, 2639, 544, 1651, 30, 51014, 51014, 865, 11, 286, 632, 472, 13, 1282, 472, 295, 264, 1269, 1651, 11, 309, 311, 411, 3701, 1968, 420, 406, 613, 5245, 366, 767, 3701, 264, 2856, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11084125568340351, "compression_ratio": 1.551219512195122, "no_speech_prob": 6.400566780939698e-05}, {"id": 752, "seek": 546000, "start": 5473.0, "end": 5484.0, "text": " Yeah, I had one. On one of the open questions, it's like understanding whether or not these models are actually understanding the language.", "tokens": [50364, 865, 11, 456, 311, 257, 1379, 3840, 295, 11088, 1339, 291, 645, 1417, 13, 50614, 50614, 1033, 13, 50664, 50664, 37156, 281, 3683, 10577, 293, 28708, 295, 3683, 10392, 294, 264, 3678, 13, 50864, 50864, 1033, 13, 50914, 50914, 2639, 544, 1651, 30, 51014, 51014, 865, 11, 286, 632, 472, 13, 1282, 472, 295, 264, 1269, 1651, 11, 309, 311, 411, 3701, 1968, 420, 406, 613, 5245, 366, 767, 3701, 264, 2856, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11084125568340351, "compression_ratio": 1.551219512195122, "no_speech_prob": 6.400566780939698e-05}, {"id": 753, "seek": 548400, "start": 5484.0, "end": 5494.0, "text": " What are some ways that exist right now to quantify and measure that, and how would we do that?", "tokens": [50364, 708, 366, 512, 2098, 300, 2514, 558, 586, 281, 40421, 293, 3481, 300, 11, 293, 577, 576, 321, 360, 300, 30, 50864, 50864, 865, 11, 1392, 13, 51114, 51114, 708, 5850, 2314, 307, 1580, 1619, 11, 286, 458, 11, 613, 5245, 3212, 380, 3701, 264, 2856, 13, 51414, 51414, 759, 436, 727, 11, 436, 1116, 312, 1075, 281, 5039, 341, 777, 5633, 300, 436, 434, 15424, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11535232167848398, "compression_ratio": 1.46875, "no_speech_prob": 2.5861960239126347e-05}, {"id": 754, "seek": 548400, "start": 5494.0, "end": 5499.0, "text": " Yeah, okay.", "tokens": [50364, 708, 366, 512, 2098, 300, 2514, 558, 586, 281, 40421, 293, 3481, 300, 11, 293, 577, 576, 321, 360, 300, 30, 50864, 50864, 865, 11, 1392, 13, 51114, 51114, 708, 5850, 2314, 307, 1580, 1619, 11, 286, 458, 11, 613, 5245, 3212, 380, 3701, 264, 2856, 13, 51414, 51414, 759, 436, 727, 11, 436, 1116, 312, 1075, 281, 5039, 341, 777, 5633, 300, 436, 434, 15424, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11535232167848398, "compression_ratio": 1.46875, "no_speech_prob": 2.5861960239126347e-05}, {"id": 755, "seek": 548400, "start": 5499.0, "end": 5505.0, "text": " What typically happens is someone says, I know, these models aren't understanding the language.", "tokens": [50364, 708, 366, 512, 2098, 300, 2514, 558, 586, 281, 40421, 293, 3481, 300, 11, 293, 577, 576, 321, 360, 300, 30, 50864, 50864, 865, 11, 1392, 13, 51114, 51114, 708, 5850, 2314, 307, 1580, 1619, 11, 286, 458, 11, 613, 5245, 3212, 380, 3701, 264, 2856, 13, 51414, 51414, 759, 436, 727, 11, 436, 1116, 312, 1075, 281, 5039, 341, 777, 5633, 300, 436, 434, 15424, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11535232167848398, "compression_ratio": 1.46875, "no_speech_prob": 2.5861960239126347e-05}, {"id": 756, "seek": 548400, "start": 5505.0, "end": 5509.0, "text": " If they could, they'd be able to solve this new task that they're introducing.", "tokens": [50364, 708, 366, 512, 2098, 300, 2514, 558, 586, 281, 40421, 293, 3481, 300, 11, 293, 577, 576, 321, 360, 300, 30, 50864, 50864, 865, 11, 1392, 13, 51114, 51114, 708, 5850, 2314, 307, 1580, 1619, 11, 286, 458, 11, 613, 5245, 3212, 380, 3701, 264, 2856, 13, 51414, 51414, 759, 436, 727, 11, 436, 1116, 312, 1075, 281, 5039, 341, 777, 5633, 300, 436, 434, 15424, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11535232167848398, "compression_ratio": 1.46875, "no_speech_prob": 2.5861960239126347e-05}, {"id": 757, "seek": 550900, "start": 5509.0, "end": 5516.0, "text": " Then they introduce a new task, which say Bert can't do, and then say Bert isn't doing it.", "tokens": [50364, 1396, 436, 5366, 257, 777, 5633, 11, 597, 584, 29594, 393, 380, 360, 11, 293, 550, 584, 29594, 1943, 380, 884, 309, 13, 50714, 50714, 1396, 958, 1243, 1580, 16329, 257, 3801, 2316, 11, 293, 550, 767, 2170, 1952, 3389, 322, 341, 5633, 13, 51014, 51014, 400, 286, 519, 534, 437, 311, 2737, 307, 411, 512, 561, 445, 362, 16224, 626, 300, 613, 18161, 9590, 393, 380, 312, 3701, 2856, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19997862497965493, "compression_ratio": 1.5545023696682465, "no_speech_prob": 2.282293644384481e-05}, {"id": 758, "seek": 550900, "start": 5516.0, "end": 5522.0, "text": " Then next week someone trains a bigger model, and then actually gets human performance on this task.", "tokens": [50364, 1396, 436, 5366, 257, 777, 5633, 11, 597, 584, 29594, 393, 380, 360, 11, 293, 550, 584, 29594, 1943, 380, 884, 309, 13, 50714, 50714, 1396, 958, 1243, 1580, 16329, 257, 3801, 2316, 11, 293, 550, 767, 2170, 1952, 3389, 322, 341, 5633, 13, 51014, 51014, 400, 286, 519, 534, 437, 311, 2737, 307, 411, 512, 561, 445, 362, 16224, 626, 300, 613, 18161, 9590, 393, 380, 312, 3701, 2856, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19997862497965493, "compression_ratio": 1.5545023696682465, "no_speech_prob": 2.282293644384481e-05}, {"id": 759, "seek": 550900, "start": 5522.0, "end": 5536.0, "text": " And I think really what's happening is like some people just have intuitions that these neural networks can't be understanding language,", "tokens": [50364, 1396, 436, 5366, 257, 777, 5633, 11, 597, 584, 29594, 393, 380, 360, 11, 293, 550, 584, 29594, 1943, 380, 884, 309, 13, 50714, 50714, 1396, 958, 1243, 1580, 16329, 257, 3801, 2316, 11, 293, 550, 767, 2170, 1952, 3389, 322, 341, 5633, 13, 51014, 51014, 400, 286, 519, 534, 437, 311, 2737, 307, 411, 512, 561, 445, 362, 16224, 626, 300, 613, 18161, 9590, 393, 380, 312, 3701, 2856, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19997862497965493, "compression_ratio": 1.5545023696682465, "no_speech_prob": 2.282293644384481e-05}, {"id": 760, "seek": 553600, "start": 5536.0, "end": 5539.0, "text": " and they must just be gaming data sets somehow.", "tokens": [50364, 293, 436, 1633, 445, 312, 9703, 1412, 6352, 6063, 13, 50514, 50514, 400, 281, 264, 8396, 300, 613, 5245, 360, 731, 11, 456, 1633, 312, 512, 733, 295, 3657, 32152, 294, 527, 1412, 6352, 300, 264, 5245, 393, 25924, 1553, 534, 3701, 1340, 13, 51014, 51014, 400, 309, 311, 2138, 2074, 300, 257, 688, 295, 527, 1412, 6352, 360, 362, 32152, 294, 552, 11, 293, 309, 311, 733, 295, 1152, 281, 652, 2306, 300, 500, 380, 360, 309, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1441732544496835, "compression_ratio": 1.7474747474747474, "no_speech_prob": 1.3418593880487606e-05}, {"id": 761, "seek": 553600, "start": 5539.0, "end": 5549.0, "text": " And to the extent that these models do well, there must be some kind of weird biases in our data sets that the models can exploit without really understanding anything.", "tokens": [50364, 293, 436, 1633, 445, 312, 9703, 1412, 6352, 6063, 13, 50514, 50514, 400, 281, 264, 8396, 300, 613, 5245, 360, 731, 11, 456, 1633, 312, 512, 733, 295, 3657, 32152, 294, 527, 1412, 6352, 300, 264, 5245, 393, 25924, 1553, 534, 3701, 1340, 13, 51014, 51014, 400, 309, 311, 2138, 2074, 300, 257, 688, 295, 527, 1412, 6352, 360, 362, 32152, 294, 552, 11, 293, 309, 311, 733, 295, 1152, 281, 652, 2306, 300, 500, 380, 360, 309, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1441732544496835, "compression_ratio": 1.7474747474747474, "no_speech_prob": 1.3418593880487606e-05}, {"id": 762, "seek": 553600, "start": 5549.0, "end": 5559.0, "text": " And it's definitely true that a lot of our data sets do have biases in them, and it's kind of hard to make ones that don't do it.", "tokens": [50364, 293, 436, 1633, 445, 312, 9703, 1412, 6352, 6063, 13, 50514, 50514, 400, 281, 264, 8396, 300, 613, 5245, 360, 731, 11, 456, 1633, 312, 512, 733, 295, 3657, 32152, 294, 527, 1412, 6352, 300, 264, 5245, 393, 25924, 1553, 534, 3701, 1340, 13, 51014, 51014, 400, 309, 311, 2138, 2074, 300, 257, 688, 295, 527, 1412, 6352, 360, 362, 32152, 294, 552, 11, 293, 309, 311, 733, 295, 1152, 281, 652, 2306, 300, 500, 380, 360, 309, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1441732544496835, "compression_ratio": 1.7474747474747474, "no_speech_prob": 1.3418593880487606e-05}, {"id": 763, "seek": 555900, "start": 5559.0, "end": 5570.0, "text": " I think it's a lot of skill, but on the other hand, it's like people are failing to use good counter examples, so what these models can't do.", "tokens": [50364, 286, 519, 309, 311, 257, 688, 295, 5389, 11, 457, 322, 264, 661, 1011, 11, 309, 311, 411, 561, 366, 18223, 281, 764, 665, 5682, 5110, 11, 370, 437, 613, 5245, 393, 380, 360, 13, 50914, 50914, 8537, 11, 300, 311, 309, 13, 51014, 51014, 4919, 13, 51064, 51064, 316, 665, 1365, 294, 5162, 1413, 390, 264, 10427, 664, 6206, 34078, 3542, 11, 689, 10427, 664, 6206, 22627, 296, 366, 729, 16579, 300, 366, 39465, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.29465999603271487, "compression_ratio": 1.523076923076923, "no_speech_prob": 6.92156536388211e-05}, {"id": 764, "seek": 555900, "start": 5570.0, "end": 5572.0, "text": " Basically, that's it.", "tokens": [50364, 286, 519, 309, 311, 257, 688, 295, 5389, 11, 457, 322, 264, 661, 1011, 11, 309, 311, 411, 561, 366, 18223, 281, 764, 665, 5682, 5110, 11, 370, 437, 613, 5245, 393, 380, 360, 13, 50914, 50914, 8537, 11, 300, 311, 309, 13, 51014, 51014, 4919, 13, 51064, 51064, 316, 665, 1365, 294, 5162, 1413, 390, 264, 10427, 664, 6206, 34078, 3542, 11, 689, 10427, 664, 6206, 22627, 296, 366, 729, 16579, 300, 366, 39465, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.29465999603271487, "compression_ratio": 1.523076923076923, "no_speech_prob": 6.92156536388211e-05}, {"id": 765, "seek": 555900, "start": 5572.0, "end": 5573.0, "text": " Sorry.", "tokens": [50364, 286, 519, 309, 311, 257, 688, 295, 5389, 11, 457, 322, 264, 661, 1011, 11, 309, 311, 411, 561, 366, 18223, 281, 764, 665, 5682, 5110, 11, 370, 437, 613, 5245, 393, 380, 360, 13, 50914, 50914, 8537, 11, 300, 311, 309, 13, 51014, 51014, 4919, 13, 51064, 51064, 316, 665, 1365, 294, 5162, 1413, 390, 264, 10427, 664, 6206, 34078, 3542, 11, 689, 10427, 664, 6206, 22627, 296, 366, 729, 16579, 300, 366, 39465, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.29465999603271487, "compression_ratio": 1.523076923076923, "no_speech_prob": 6.92156536388211e-05}, {"id": 766, "seek": 555900, "start": 5573.0, "end": 5581.0, "text": " A good example in recent times was the Winograd schema results, where Winograd schemas are those sentences that are ambiguous,", "tokens": [50364, 286, 519, 309, 311, 257, 688, 295, 5389, 11, 457, 322, 264, 661, 1011, 11, 309, 311, 411, 561, 366, 18223, 281, 764, 665, 5682, 5110, 11, 370, 437, 613, 5245, 393, 380, 360, 13, 50914, 50914, 8537, 11, 300, 311, 309, 13, 51014, 51014, 4919, 13, 51064, 51064, 316, 665, 1365, 294, 5162, 1413, 390, 264, 10427, 664, 6206, 34078, 3542, 11, 689, 10427, 664, 6206, 22627, 296, 366, 729, 16579, 300, 366, 39465, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.29465999603271487, "compression_ratio": 1.523076923076923, "no_speech_prob": 6.92156536388211e-05}, {"id": 767, "seek": 558100, "start": 5581.0, "end": 5591.0, "text": " and there is a pronoun that refers to one of the words, and you can't tell which word the pronoun refers to unless you know something about how the word works.", "tokens": [50364, 293, 456, 307, 257, 14144, 300, 14942, 281, 472, 295, 264, 2283, 11, 293, 291, 393, 380, 980, 597, 1349, 264, 14144, 14942, 281, 5969, 291, 458, 746, 466, 577, 264, 1349, 1985, 13, 50864, 50864, 407, 264, 3832, 1365, 307, 264, 28639, 1177, 380, 3318, 294, 264, 34545, 570, 309, 311, 886, 2416, 11, 420, 264, 28639, 1177, 380, 3318, 294, 264, 34545, 570, 309, 311, 886, 1359, 13, 51364, 51364, 400, 294, 472, 1389, 11, 264, 14144, 14942, 281, 264, 28639, 11, 293, 294, 264, 661, 1389, 309, 14942, 281, 264, 34545, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10122831903322778, "compression_ratio": 2.0646766169154227, "no_speech_prob": 6.708523142151535e-05}, {"id": 768, "seek": 558100, "start": 5591.0, "end": 5601.0, "text": " So the standard example is the trophy doesn't fit in the suitcase because it's too large, or the trophy doesn't fit in the suitcase because it's too small.", "tokens": [50364, 293, 456, 307, 257, 14144, 300, 14942, 281, 472, 295, 264, 2283, 11, 293, 291, 393, 380, 980, 597, 1349, 264, 14144, 14942, 281, 5969, 291, 458, 746, 466, 577, 264, 1349, 1985, 13, 50864, 50864, 407, 264, 3832, 1365, 307, 264, 28639, 1177, 380, 3318, 294, 264, 34545, 570, 309, 311, 886, 2416, 11, 420, 264, 28639, 1177, 380, 3318, 294, 264, 34545, 570, 309, 311, 886, 1359, 13, 51364, 51364, 400, 294, 472, 1389, 11, 264, 14144, 14942, 281, 264, 28639, 11, 293, 294, 264, 661, 1389, 309, 14942, 281, 264, 34545, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10122831903322778, "compression_ratio": 2.0646766169154227, "no_speech_prob": 6.708523142151535e-05}, {"id": 769, "seek": 558100, "start": 5601.0, "end": 5609.0, "text": " And in one case, the pronoun refers to the trophy, and in the other case it refers to the suitcase.", "tokens": [50364, 293, 456, 307, 257, 14144, 300, 14942, 281, 472, 295, 264, 2283, 11, 293, 291, 393, 380, 980, 597, 1349, 264, 14144, 14942, 281, 5969, 291, 458, 746, 466, 577, 264, 1349, 1985, 13, 50864, 50864, 407, 264, 3832, 1365, 307, 264, 28639, 1177, 380, 3318, 294, 264, 34545, 570, 309, 311, 886, 2416, 11, 420, 264, 28639, 1177, 380, 3318, 294, 264, 34545, 570, 309, 311, 886, 1359, 13, 51364, 51364, 400, 294, 472, 1389, 11, 264, 14144, 14942, 281, 264, 28639, 11, 293, 294, 264, 661, 1389, 309, 14942, 281, 264, 34545, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10122831903322778, "compression_ratio": 2.0646766169154227, "no_speech_prob": 6.708523142151535e-05}, {"id": 770, "seek": 560900, "start": 5609.0, "end": 5620.0, "text": " And there was a list of those, and people created a data set of those things, and until about two years ago, the best results were around 60% for computers.", "tokens": [50364, 400, 456, 390, 257, 1329, 295, 729, 11, 293, 561, 2942, 257, 1412, 992, 295, 729, 721, 11, 293, 1826, 466, 732, 924, 2057, 11, 264, 1151, 3542, 645, 926, 4060, 4, 337, 10807, 13, 50914, 50914, 35809, 360, 13420, 4, 420, 746, 11, 457, 264, 1151, 11677, 3652, 645, 1242, 466, 4060, 8923, 293, 586, 286, 519, 309, 311, 466, 4289, 6856, 51464, 51464, 865, 11, 746, 411, 300, 13, 51564, 51564, 6595, 411, 300, 11, 558, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12835144134889165, "compression_ratio": 1.5797101449275361, "no_speech_prob": 4.005269147455692e-05}, {"id": 771, "seek": 560900, "start": 5620.0, "end": 5631.0, "text": " Humans do 95% or something, but the best artificial systems were getting about 60%, and now I think it's about 90%.", "tokens": [50364, 400, 456, 390, 257, 1329, 295, 729, 11, 293, 561, 2942, 257, 1412, 992, 295, 729, 721, 11, 293, 1826, 466, 732, 924, 2057, 11, 264, 1151, 3542, 645, 926, 4060, 4, 337, 10807, 13, 50914, 50914, 35809, 360, 13420, 4, 420, 746, 11, 457, 264, 1151, 11677, 3652, 645, 1242, 466, 4060, 8923, 293, 586, 286, 519, 309, 311, 466, 4289, 6856, 51464, 51464, 865, 11, 746, 411, 300, 13, 51564, 51564, 6595, 411, 300, 11, 558, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12835144134889165, "compression_ratio": 1.5797101449275361, "no_speech_prob": 4.005269147455692e-05}, {"id": 772, "seek": 560900, "start": 5631.0, "end": 5633.0, "text": " Yeah, something like that.", "tokens": [50364, 400, 456, 390, 257, 1329, 295, 729, 11, 293, 561, 2942, 257, 1412, 992, 295, 729, 721, 11, 293, 1826, 466, 732, 924, 2057, 11, 264, 1151, 3542, 645, 926, 4060, 4, 337, 10807, 13, 50914, 50914, 35809, 360, 13420, 4, 420, 746, 11, 457, 264, 1151, 11677, 3652, 645, 1242, 466, 4060, 8923, 293, 586, 286, 519, 309, 311, 466, 4289, 6856, 51464, 51464, 865, 11, 746, 411, 300, 13, 51564, 51564, 6595, 411, 300, 11, 558, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12835144134889165, "compression_ratio": 1.5797101449275361, "no_speech_prob": 4.005269147455692e-05}, {"id": 773, "seek": 560900, "start": 5633.0, "end": 5634.0, "text": " Something like that, right.", "tokens": [50364, 400, 456, 390, 257, 1329, 295, 729, 11, 293, 561, 2942, 257, 1412, 992, 295, 729, 721, 11, 293, 1826, 466, 732, 924, 2057, 11, 264, 1151, 3542, 645, 926, 4060, 4, 337, 10807, 13, 50914, 50914, 35809, 360, 13420, 4, 420, 746, 11, 457, 264, 1151, 11677, 3652, 645, 1242, 466, 4060, 8923, 293, 586, 286, 519, 309, 311, 466, 4289, 6856, 51464, 51464, 865, 11, 746, 411, 300, 13, 51564, 51564, 6595, 411, 300, 11, 558, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12835144134889165, "compression_ratio": 1.5797101449275361, "no_speech_prob": 4.005269147455692e-05}, {"id": 774, "seek": 563400, "start": 5634.0, "end": 5641.0, "text": " You don't even get any training dates for these? This is just a purely unsupervised problem?", "tokens": [50364, 509, 500, 380, 754, 483, 604, 3097, 11691, 337, 613, 30, 639, 307, 445, 257, 17491, 2693, 12879, 24420, 1154, 30, 50714, 50714, 1779, 13, 50764, 50764, 400, 370, 264, 1168, 307, 11, 291, 458, 11, 309, 311, 1850, 300, 729, 3652, 362, 3264, 746, 466, 264, 3090, 295, 6565, 293, 11, 291, 458, 11, 257, 707, 857, 466, 577, 264, 1349, 1985, 538, 445, 22107, 12523, 466, 2487, 11, 51414, 51414, 457, 309, 311, 7226, 34622, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11621972991199028, "compression_ratio": 1.527027027027027, "no_speech_prob": 2.0144123482168652e-05}, {"id": 775, "seek": 563400, "start": 5641.0, "end": 5642.0, "text": " Right.", "tokens": [50364, 509, 500, 380, 754, 483, 604, 3097, 11691, 337, 613, 30, 639, 307, 445, 257, 17491, 2693, 12879, 24420, 1154, 30, 50714, 50714, 1779, 13, 50764, 50764, 400, 370, 264, 1168, 307, 11, 291, 458, 11, 309, 311, 1850, 300, 729, 3652, 362, 3264, 746, 466, 264, 3090, 295, 6565, 293, 11, 291, 458, 11, 257, 707, 857, 466, 577, 264, 1349, 1985, 538, 445, 22107, 12523, 466, 2487, 11, 51414, 51414, 457, 309, 311, 7226, 34622, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11621972991199028, "compression_ratio": 1.527027027027027, "no_speech_prob": 2.0144123482168652e-05}, {"id": 776, "seek": 563400, "start": 5642.0, "end": 5655.0, "text": " And so the question is, you know, it's clear that those systems have learned something about the role of objects and, you know, a little bit about how the word works by just observing statistics about text,", "tokens": [50364, 509, 500, 380, 754, 483, 604, 3097, 11691, 337, 613, 30, 639, 307, 445, 257, 17491, 2693, 12879, 24420, 1154, 30, 50714, 50714, 1779, 13, 50764, 50764, 400, 370, 264, 1168, 307, 11, 291, 458, 11, 309, 311, 1850, 300, 729, 3652, 362, 3264, 746, 466, 264, 3090, 295, 6565, 293, 11, 291, 458, 11, 257, 707, 857, 466, 577, 264, 1349, 1985, 538, 445, 22107, 12523, 466, 2487, 11, 51414, 51414, 457, 309, 311, 7226, 34622, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11621972991199028, "compression_ratio": 1.527027027027027, "no_speech_prob": 2.0144123482168652e-05}, {"id": 777, "seek": 563400, "start": 5655.0, "end": 5657.0, "text": " but it's relatively superficial.", "tokens": [50364, 509, 500, 380, 754, 483, 604, 3097, 11691, 337, 613, 30, 639, 307, 445, 257, 17491, 2693, 12879, 24420, 1154, 30, 50714, 50714, 1779, 13, 50764, 50764, 400, 370, 264, 1168, 307, 11, 291, 458, 11, 309, 311, 1850, 300, 729, 3652, 362, 3264, 746, 466, 264, 3090, 295, 6565, 293, 11, 291, 458, 11, 257, 707, 857, 466, 577, 264, 1349, 1985, 538, 445, 22107, 12523, 466, 2487, 11, 51414, 51414, 457, 309, 311, 7226, 34622, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11621972991199028, "compression_ratio": 1.527027027027027, "no_speech_prob": 2.0144123482168652e-05}, {"id": 778, "seek": 565700, "start": 5657.0, "end": 5672.0, "text": " It's not, I mean, you know, as it's pretty obvious when you look at text as generated, you know, we're talking about a unicorn, and then the first sentence is, unicorn has four horns, right, which of course doesn't make sense because unicorns have only one.", "tokens": [50364, 467, 311, 406, 11, 286, 914, 11, 291, 458, 11, 382, 309, 311, 1238, 6322, 562, 291, 574, 412, 2487, 382, 10833, 11, 291, 458, 11, 321, 434, 1417, 466, 257, 28122, 11, 293, 550, 264, 700, 8174, 307, 11, 28122, 575, 1451, 28818, 11, 558, 11, 597, 295, 1164, 1177, 380, 652, 2020, 570, 28122, 82, 362, 787, 472, 13, 51114, 51114, 663, 311, 733, 295, 264, 935, 295, 885, 257, 28122, 13, 51214, 51214, 407, 11, 264, 1379, 1558, 295, 11, 286, 914, 11, 264, 1379, 1154, 295, 2539, 2689, 2020, 575, 406, 668, 13041, 588, 1400, 490, 309, 11, 457, 729, 3652, 11, 457, 436, 589, 17600, 731, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1523808610850367, "compression_ratio": 1.6884057971014492, "no_speech_prob": 1.1478170563350432e-05}, {"id": 779, "seek": 565700, "start": 5672.0, "end": 5674.0, "text": " That's kind of the point of being a unicorn.", "tokens": [50364, 467, 311, 406, 11, 286, 914, 11, 291, 458, 11, 382, 309, 311, 1238, 6322, 562, 291, 574, 412, 2487, 382, 10833, 11, 291, 458, 11, 321, 434, 1417, 466, 257, 28122, 11, 293, 550, 264, 700, 8174, 307, 11, 28122, 575, 1451, 28818, 11, 558, 11, 597, 295, 1164, 1177, 380, 652, 2020, 570, 28122, 82, 362, 787, 472, 13, 51114, 51114, 663, 311, 733, 295, 264, 935, 295, 885, 257, 28122, 13, 51214, 51214, 407, 11, 264, 1379, 1558, 295, 11, 286, 914, 11, 264, 1379, 1154, 295, 2539, 2689, 2020, 575, 406, 668, 13041, 588, 1400, 490, 309, 11, 457, 729, 3652, 11, 457, 436, 589, 17600, 731, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1523808610850367, "compression_ratio": 1.6884057971014492, "no_speech_prob": 1.1478170563350432e-05}, {"id": 780, "seek": 565700, "start": 5674.0, "end": 5684.0, "text": " So, the whole idea of, I mean, the whole problem of learning common sense has not been solved very far from it, but those systems, but they work surprisingly well.", "tokens": [50364, 467, 311, 406, 11, 286, 914, 11, 291, 458, 11, 382, 309, 311, 1238, 6322, 562, 291, 574, 412, 2487, 382, 10833, 11, 291, 458, 11, 321, 434, 1417, 466, 257, 28122, 11, 293, 550, 264, 700, 8174, 307, 11, 28122, 575, 1451, 28818, 11, 558, 11, 597, 295, 1164, 1177, 380, 652, 2020, 570, 28122, 82, 362, 787, 472, 13, 51114, 51114, 663, 311, 733, 295, 264, 935, 295, 885, 257, 28122, 13, 51214, 51214, 407, 11, 264, 1379, 1558, 295, 11, 286, 914, 11, 264, 1379, 1154, 295, 2539, 2689, 2020, 575, 406, 668, 13041, 588, 1400, 490, 309, 11, 457, 729, 3652, 11, 457, 436, 589, 17600, 731, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1523808610850367, "compression_ratio": 1.6884057971014492, "no_speech_prob": 1.1478170563350432e-05}, {"id": 781, "seek": 568400, "start": 5684.0, "end": 5691.0, "text": " It's surprising how far you can go with just, you know, looking at text.", "tokens": [50364, 467, 311, 8830, 577, 1400, 291, 393, 352, 365, 445, 11, 291, 458, 11, 1237, 412, 2487, 13, 50714, 50714, 865, 11, 2539, 2689, 2020, 307, 1687, 1152, 570, 294, 512, 2020, 11, 264, 721, 291, 528, 281, 1466, 366, 721, 300, 3212, 380, 3720, 760, 11, 411, 572, 472, 1562, 13657, 760, 2689, 2020, 3601, 13, 51164, 51164, 9210, 572, 472, 1562, 13657, 760, 411, 257, 28122, 575, 2293, 472, 13482, 293, 406, 1451, 13, 51414, 51414, 1436, 1518, 3255, 300, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1702062891817641, "compression_ratio": 1.727699530516432, "no_speech_prob": 3.822602957370691e-05}, {"id": 782, "seek": 568400, "start": 5691.0, "end": 5700.0, "text": " Yeah, learning common sense is super hard because in some sense, the things you want to learn are things that aren't written down, like no one ever writes down common sense knowledge.", "tokens": [50364, 467, 311, 8830, 577, 1400, 291, 393, 352, 365, 445, 11, 291, 458, 11, 1237, 412, 2487, 13, 50714, 50714, 865, 11, 2539, 2689, 2020, 307, 1687, 1152, 570, 294, 512, 2020, 11, 264, 721, 291, 528, 281, 1466, 366, 721, 300, 3212, 380, 3720, 760, 11, 411, 572, 472, 1562, 13657, 760, 2689, 2020, 3601, 13, 51164, 51164, 9210, 572, 472, 1562, 13657, 760, 411, 257, 28122, 575, 2293, 472, 13482, 293, 406, 1451, 13, 51414, 51414, 1436, 1518, 3255, 300, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1702062891817641, "compression_ratio": 1.727699530516432, "no_speech_prob": 3.822602957370691e-05}, {"id": 783, "seek": 568400, "start": 5700.0, "end": 5705.0, "text": " Probably no one ever writes down like a unicorn has exactly one horn and not four.", "tokens": [50364, 467, 311, 8830, 577, 1400, 291, 393, 352, 365, 445, 11, 291, 458, 11, 1237, 412, 2487, 13, 50714, 50714, 865, 11, 2539, 2689, 2020, 307, 1687, 1152, 570, 294, 512, 2020, 11, 264, 721, 291, 528, 281, 1466, 366, 721, 300, 3212, 380, 3720, 760, 11, 411, 572, 472, 1562, 13657, 760, 2689, 2020, 3601, 13, 51164, 51164, 9210, 572, 472, 1562, 13657, 760, 411, 257, 28122, 575, 2293, 472, 13482, 293, 406, 1451, 13, 51414, 51414, 1436, 1518, 3255, 300, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1702062891817641, "compression_ratio": 1.727699530516432, "no_speech_prob": 3.822602957370691e-05}, {"id": 784, "seek": 568400, "start": 5705.0, "end": 5709.0, "text": " Because everyone knows that.", "tokens": [50364, 467, 311, 8830, 577, 1400, 291, 393, 352, 365, 445, 11, 291, 458, 11, 1237, 412, 2487, 13, 50714, 50714, 865, 11, 2539, 2689, 2020, 307, 1687, 1152, 570, 294, 512, 2020, 11, 264, 721, 291, 528, 281, 1466, 366, 721, 300, 3212, 380, 3720, 760, 11, 411, 572, 472, 1562, 13657, 760, 2689, 2020, 3601, 13, 51164, 51164, 9210, 572, 472, 1562, 13657, 760, 411, 257, 28122, 575, 2293, 472, 13482, 293, 406, 1451, 13, 51414, 51414, 1436, 1518, 3255, 300, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1702062891817641, "compression_ratio": 1.727699530516432, "no_speech_prob": 3.822602957370691e-05}, {"id": 785, "seek": 570900, "start": 5709.0, "end": 5714.0, "text": " So, probably there are limitations to what you can learn from just looking at text.", "tokens": [50364, 407, 11, 1391, 456, 366, 15705, 281, 437, 291, 393, 1466, 490, 445, 1237, 412, 2487, 13, 50614, 50614, 1664, 291, 980, 505, 746, 466, 264, 1349, 11, 286, 914, 11, 466, 512, 295, 264, 589, 291, 434, 884, 322, 23535, 2856, 30, 51014, 51014, 4894, 11, 1338, 13, 51064, 51064, 407, 11, 286, 914, 11, 286, 829, 1825, 466, 46727, 294, 341, 11, 457, 309, 311, 257, 1379, 1880, 4829, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.14099560285869397, "compression_ratio": 1.5183246073298429, "no_speech_prob": 4.6832308726152405e-05}, {"id": 786, "seek": 570900, "start": 5714.0, "end": 5722.0, "text": " Can you tell us something about the word, I mean, about some of the work you're doing on grounded language?", "tokens": [50364, 407, 11, 1391, 456, 366, 15705, 281, 437, 291, 393, 1466, 490, 445, 1237, 412, 2487, 13, 50614, 50614, 1664, 291, 980, 505, 746, 466, 264, 1349, 11, 286, 914, 11, 466, 512, 295, 264, 589, 291, 434, 884, 322, 23535, 2856, 30, 51014, 51014, 4894, 11, 1338, 13, 51064, 51064, 407, 11, 286, 914, 11, 286, 829, 1825, 466, 46727, 294, 341, 11, 457, 309, 311, 257, 1379, 1880, 4829, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.14099560285869397, "compression_ratio": 1.5183246073298429, "no_speech_prob": 4.6832308726152405e-05}, {"id": 787, "seek": 570900, "start": 5722.0, "end": 5723.0, "text": " Sure, yeah.", "tokens": [50364, 407, 11, 1391, 456, 366, 15705, 281, 437, 291, 393, 1466, 490, 445, 1237, 412, 2487, 13, 50614, 50614, 1664, 291, 980, 505, 746, 466, 264, 1349, 11, 286, 914, 11, 466, 512, 295, 264, 589, 291, 434, 884, 322, 23535, 2856, 30, 51014, 51014, 4894, 11, 1338, 13, 51064, 51064, 407, 11, 286, 914, 11, 286, 829, 1825, 466, 46727, 294, 341, 11, 457, 309, 311, 257, 1379, 1880, 4829, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.14099560285869397, "compression_ratio": 1.5183246073298429, "no_speech_prob": 4.6832308726152405e-05}, {"id": 788, "seek": 570900, "start": 5723.0, "end": 5730.0, "text": " So, I mean, I put nothing about grounding in this, but it's a whole interesting topic.", "tokens": [50364, 407, 11, 1391, 456, 366, 15705, 281, 437, 291, 393, 1466, 490, 445, 1237, 412, 2487, 13, 50614, 50614, 1664, 291, 980, 505, 746, 466, 264, 1349, 11, 286, 914, 11, 466, 512, 295, 264, 589, 291, 434, 884, 322, 23535, 2856, 30, 51014, 51014, 4894, 11, 1338, 13, 51064, 51064, 407, 11, 286, 914, 11, 286, 829, 1825, 466, 46727, 294, 341, 11, 457, 309, 311, 257, 1379, 1880, 4829, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.14099560285869397, "compression_ratio": 1.5183246073298429, "no_speech_prob": 4.6832308726152405e-05}, {"id": 789, "seek": 573000, "start": 5730.0, "end": 5741.0, "text": " So, I think it's really interesting trying, no one produces text to say people don't use text because it's like, it's my representation of the world.", "tokens": [50364, 407, 11, 286, 519, 309, 311, 534, 1880, 1382, 11, 572, 472, 14725, 2487, 281, 584, 561, 500, 380, 764, 2487, 570, 309, 311, 411, 11, 309, 311, 452, 10290, 295, 264, 1002, 13, 50914, 50914, 407, 11, 472, 4829, 300, 311, 534, 1880, 307, 1382, 281, 2727, 10221, 14924, 293, 5493, 13, 51264, 51264, 407, 11, 2831, 813, 1382, 281, 1322, 417, 270, 5081, 3652, 300, 751, 281, 1184, 661, 420, 751, 281, 561, 281, 362, 257, 3761, 11, 411, 393, 291, 1322, 3652, 689, 291, 853, 281, 4584, 512, 3387, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.2410710324946138, "compression_ratio": 1.7542372881355932, "no_speech_prob": 9.664808203524444e-06}, {"id": 790, "seek": 573000, "start": 5741.0, "end": 5748.0, "text": " So, one topic that's really interesting is trying to ground dialogue usage and goals.", "tokens": [50364, 407, 11, 286, 519, 309, 311, 534, 1880, 1382, 11, 572, 472, 14725, 2487, 281, 584, 561, 500, 380, 764, 2487, 570, 309, 311, 411, 11, 309, 311, 452, 10290, 295, 264, 1002, 13, 50914, 50914, 407, 11, 472, 4829, 300, 311, 534, 1880, 307, 1382, 281, 2727, 10221, 14924, 293, 5493, 13, 51264, 51264, 407, 11, 2831, 813, 1382, 281, 1322, 417, 270, 5081, 3652, 300, 751, 281, 1184, 661, 420, 751, 281, 561, 281, 362, 257, 3761, 11, 411, 393, 291, 1322, 3652, 689, 291, 853, 281, 4584, 512, 3387, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.2410710324946138, "compression_ratio": 1.7542372881355932, "no_speech_prob": 9.664808203524444e-06}, {"id": 791, "seek": 573000, "start": 5748.0, "end": 5759.0, "text": " So, rather than trying to build chit chat systems that talk to each other or talk to people to have a conversation, like can you build systems where you try to achieve some goal?", "tokens": [50364, 407, 11, 286, 519, 309, 311, 534, 1880, 1382, 11, 572, 472, 14725, 2487, 281, 584, 561, 500, 380, 764, 2487, 570, 309, 311, 411, 11, 309, 311, 452, 10290, 295, 264, 1002, 13, 50914, 50914, 407, 11, 472, 4829, 300, 311, 534, 1880, 307, 1382, 281, 2727, 10221, 14924, 293, 5493, 13, 51264, 51264, 407, 11, 2831, 813, 1382, 281, 1322, 417, 270, 5081, 3652, 300, 751, 281, 1184, 661, 420, 751, 281, 561, 281, 362, 257, 3761, 11, 411, 393, 291, 1322, 3652, 689, 291, 853, 281, 4584, 512, 3387, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.2410710324946138, "compression_ratio": 1.7542372881355932, "no_speech_prob": 9.664808203524444e-06}, {"id": 792, "seek": 575900, "start": 5759.0, "end": 5773.0, "text": " We had some work a couple years ago on doing this in the context of negotiations, so trying to, there's two of you which are trying to have a conversation, trying to come to some agreement with each other.", "tokens": [50364, 492, 632, 512, 589, 257, 1916, 924, 2057, 322, 884, 341, 294, 264, 4319, 295, 20476, 11, 370, 1382, 281, 11, 456, 311, 732, 295, 291, 597, 366, 1382, 281, 362, 257, 3761, 11, 1382, 281, 808, 281, 512, 8106, 365, 1184, 661, 13, 51064, 51064, 400, 264, 787, 636, 291, 393, 808, 281, 8106, 307, 538, 1419, 257, 10221, 294, 3303, 2856, 293, 456, 311, 411, 512, 10070, 597, 366, 665, 337, 291, 11, 512, 597, 366, 665, 337, 552, 11, 291, 362, 281, 915, 512, 18577, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.19351122456212197, "compression_ratio": 1.794759825327511, "no_speech_prob": 8.213733963202685e-05}, {"id": 793, "seek": 575900, "start": 5773.0, "end": 5785.0, "text": " And the only way you can come to agreement is by having a dialogue in natural language and there's like some outcomes which are good for you, some which are good for them, you have to find some compromise.", "tokens": [50364, 492, 632, 512, 589, 257, 1916, 924, 2057, 322, 884, 341, 294, 264, 4319, 295, 20476, 11, 370, 1382, 281, 11, 456, 311, 732, 295, 291, 597, 366, 1382, 281, 362, 257, 3761, 11, 1382, 281, 808, 281, 512, 8106, 365, 1184, 661, 13, 51064, 51064, 400, 264, 787, 636, 291, 393, 808, 281, 8106, 307, 538, 1419, 257, 10221, 294, 3303, 2856, 293, 456, 311, 411, 512, 10070, 597, 366, 665, 337, 291, 11, 512, 597, 366, 665, 337, 552, 11, 291, 362, 281, 915, 512, 18577, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.19351122456212197, "compression_ratio": 1.794759825327511, "no_speech_prob": 8.213733963202685e-05}, {"id": 794, "seek": 578500, "start": 5785.0, "end": 5792.0, "text": " And yeah, that's kind of interesting to say, I think, because", "tokens": [50364, 400, 1338, 11, 300, 311, 733, 295, 1880, 281, 584, 11, 286, 519, 11, 570, 50714, 50714, 1338, 11, 309, 2544, 411, 291, 767, 1228, 2856, 337, 257, 4334, 307, 516, 281, 312, 534, 11462, 281, 534, 3701, 721, 411, 11, 286, 519, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.28751255603546794, "compression_ratio": 1.4776119402985075, "no_speech_prob": 7.028435356914997e-05}, {"id": 795, "seek": 578500, "start": 5792.0, "end": 5801.0, "text": " yeah, it seems like you actually using language for a purpose is going to be really crucial to really understanding things like, I think", "tokens": [50364, 400, 1338, 11, 300, 311, 733, 295, 1880, 281, 584, 11, 286, 519, 11, 570, 50714, 50714, 1338, 11, 309, 2544, 411, 291, 767, 1228, 2856, 337, 257, 4334, 307, 516, 281, 312, 534, 11462, 281, 534, 3701, 721, 411, 11, 286, 519, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.28751255603546794, "compression_ratio": 1.4776119402985075, "no_speech_prob": 7.028435356914997e-05}, {"id": 796, "seek": 580100, "start": 5801.0, "end": 5823.0, "text": " there's limits to what we learn from just like purely observation use of language, to just purely seeing text out there in the wild that other people wrote, like really to understand things you want to be agents which are using language to try and achieve some goal and interacting with people and seeing what works and learning from that kind of signal as well.", "tokens": [50364, 456, 311, 10406, 281, 437, 321, 1466, 490, 445, 411, 17491, 14816, 764, 295, 2856, 11, 281, 445, 17491, 2577, 2487, 484, 456, 294, 264, 4868, 300, 661, 561, 4114, 11, 411, 534, 281, 1223, 721, 291, 528, 281, 312, 12554, 597, 366, 1228, 2856, 281, 853, 293, 4584, 512, 3387, 293, 18017, 365, 561, 293, 2577, 437, 1985, 293, 2539, 490, 300, 733, 295, 6358, 382, 731, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.2624850338452483, "compression_ratio": 1.691588785046729, "no_speech_prob": 9.907391358865425e-05}, {"id": 797, "seek": 582300, "start": 5823.0, "end": 5831.0, "text": " Maybe that's it's just the cherry on the icing on the cake or whatever, but I think that's, it is like a part of it is there.", "tokens": [50364, 2704, 300, 311, 309, 311, 445, 264, 20164, 322, 264, 30086, 322, 264, 5908, 420, 2035, 11, 457, 286, 519, 300, 311, 11, 309, 307, 411, 257, 644, 295, 309, 307, 456, 13, 50764, 50764, 509, 643, 257, 20164, 13, 51064, 51064, 5048, 1651, 490, 264, 4034, 13, 2492, 322, 1074, 11, 500, 380, 312, 886, 12685, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.22957957175470167, "compression_ratio": 1.4093959731543624, "no_speech_prob": 0.00021956114505883306}, {"id": 798, "seek": 582300, "start": 5831.0, "end": 5837.0, "text": " You need a cherry.", "tokens": [50364, 2704, 300, 311, 309, 311, 445, 264, 20164, 322, 264, 30086, 322, 264, 5908, 420, 2035, 11, 457, 286, 519, 300, 311, 11, 309, 307, 411, 257, 644, 295, 309, 307, 456, 13, 50764, 50764, 509, 643, 257, 20164, 13, 51064, 51064, 5048, 1651, 490, 264, 4034, 13, 2492, 322, 1074, 11, 500, 380, 312, 886, 12685, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.22957957175470167, "compression_ratio": 1.4093959731543624, "no_speech_prob": 0.00021956114505883306}, {"id": 799, "seek": 582300, "start": 5837.0, "end": 5843.0, "text": " More questions from the audience. Come on guys, don't be too shy.", "tokens": [50364, 2704, 300, 311, 309, 311, 445, 264, 20164, 322, 264, 30086, 322, 264, 5908, 420, 2035, 11, 457, 286, 519, 300, 311, 11, 309, 307, 411, 257, 644, 295, 309, 307, 456, 13, 50764, 50764, 509, 643, 257, 20164, 13, 51064, 51064, 5048, 1651, 490, 264, 4034, 13, 2492, 322, 1074, 11, 500, 380, 312, 886, 12685, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.22957957175470167, "compression_ratio": 1.4093959731543624, "no_speech_prob": 0.00021956114505883306}, {"id": 800, "seek": 584300, "start": 5843.0, "end": 5846.0, "text": " I have a question.", "tokens": [50364, 286, 362, 257, 1168, 13, 50514, 50514, 440, 700, 935, 322, 264, 1269, 1651, 577, 820, 321, 13365, 1002, 3601, 370, 264, 636, 286, 390, 1953, 466, 307, 300, 613, 5218, 13075, 27938, 433, 362, 370, 709, 1589, 466, 264, 1002, 294, 552, 11, 293, 550, 498, 321, 853, 281, 915, 291, 420, 3847, 341, 2316, 322, 512, 777, 1412, 11, 727, 321, 411, 2870, 512, 295, 264, 3071, 10392, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.15800537934174408, "compression_ratio": 1.6056338028169015, "no_speech_prob": 2.7105543267680332e-05}, {"id": 801, "seek": 584300, "start": 5846.0, "end": 5869.0, "text": " The first point on the open questions how should we integrate world knowledge so the way I was thinking about is that these billion parameter Transformers have so much information about the world in them, and then if we try to find you or train this model on some new data, could we like forget some of the earlier concepts", "tokens": [50364, 286, 362, 257, 1168, 13, 50514, 50514, 440, 700, 935, 322, 264, 1269, 1651, 577, 820, 321, 13365, 1002, 3601, 370, 264, 636, 286, 390, 1953, 466, 307, 300, 613, 5218, 13075, 27938, 433, 362, 370, 709, 1589, 466, 264, 1002, 294, 552, 11, 293, 550, 498, 321, 853, 281, 915, 291, 420, 3847, 341, 2316, 322, 512, 777, 1412, 11, 727, 321, 411, 2870, 512, 295, 264, 3071, 10392, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.15800537934174408, "compression_ratio": 1.6056338028169015, "no_speech_prob": 2.7105543267680332e-05}, {"id": 802, "seek": 586900, "start": 5869.0, "end": 5881.0, "text": " that this model had learned, and how would we like quantify what concepts, like, has the model forgotten, apart from the validation set.", "tokens": [50364, 300, 341, 2316, 632, 3264, 11, 293, 577, 576, 321, 411, 40421, 437, 10392, 11, 411, 11, 575, 264, 2316, 11832, 11, 4936, 490, 264, 24071, 992, 13, 50964, 50964, 4402, 300, 652, 2020, 13, 51114, 51114, 865, 11, 370, 1391, 291, 486, 2870, 309, 11, 286, 914, 11, 1391, 498, 291, 915, 3847, 433, 2316, 281, 360, 746, 300, 1177, 380, 643, 1002, 3601, 11, 291, 603, 19909, 2870, 439, 264, 3601, 291, 1907, 309, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.31054840558840907, "compression_ratio": 1.6074766355140186, "no_speech_prob": 7.842691411497071e-05}, {"id": 803, "seek": 586900, "start": 5881.0, "end": 5884.0, "text": " Does that make sense.", "tokens": [50364, 300, 341, 2316, 632, 3264, 11, 293, 577, 576, 321, 411, 40421, 437, 10392, 11, 411, 11, 575, 264, 2316, 11832, 11, 4936, 490, 264, 24071, 992, 13, 50964, 50964, 4402, 300, 652, 2020, 13, 51114, 51114, 865, 11, 370, 1391, 291, 486, 2870, 309, 11, 286, 914, 11, 1391, 498, 291, 915, 3847, 433, 2316, 281, 360, 746, 300, 1177, 380, 643, 1002, 3601, 11, 291, 603, 19909, 2870, 439, 264, 3601, 291, 1907, 309, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.31054840558840907, "compression_ratio": 1.6074766355140186, "no_speech_prob": 7.842691411497071e-05}, {"id": 804, "seek": 586900, "start": 5884.0, "end": 5895.0, "text": " Yeah, so probably you will forget it, I mean, probably if you find trainers model to do something that doesn't need world knowledge, you'll happily forget all the knowledge you told it.", "tokens": [50364, 300, 341, 2316, 632, 3264, 11, 293, 577, 576, 321, 411, 40421, 437, 10392, 11, 411, 11, 575, 264, 2316, 11832, 11, 4936, 490, 264, 24071, 992, 13, 50964, 50964, 4402, 300, 652, 2020, 13, 51114, 51114, 865, 11, 370, 1391, 291, 486, 2870, 309, 11, 286, 914, 11, 1391, 498, 291, 915, 3847, 433, 2316, 281, 360, 746, 300, 1177, 380, 643, 1002, 3601, 11, 291, 603, 19909, 2870, 439, 264, 3601, 291, 1907, 309, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.31054840558840907, "compression_ratio": 1.6074766355140186, "no_speech_prob": 7.842691411497071e-05}, {"id": 805, "seek": 589500, "start": 5895.0, "end": 5903.0, "text": " There's some evidence these models are like memorizing quite a lot of facts so there's a remarkable result recently from this paper.", "tokens": [50364, 821, 311, 512, 4467, 613, 5245, 366, 411, 10560, 3319, 1596, 257, 688, 295, 9130, 370, 456, 311, 257, 12802, 1874, 3938, 490, 341, 3035, 13, 50764, 50764, 639, 3329, 1185, 1219, 314, 20, 597, 286, 519, 575, 2272, 5218, 9834, 13, 50964, 50964, 400, 309, 311, 445, 3097, 2969, 264, 700, 636, 293, 550, 561, 13, 51264, 51264, 1449, 2489, 10864, 281, 1867, 1651, 11, 597, 727, 1651, 466, 1340, 457, 291, 500, 380, 855, 309, 604, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.25109477215502635, "compression_ratio": 1.5040983606557377, "no_speech_prob": 1.8055239706882276e-05}, {"id": 806, "seek": 589500, "start": 5903.0, "end": 5907.0, "text": " This Google system called T5 which I think has 12 billion parameters.", "tokens": [50364, 821, 311, 512, 4467, 613, 5245, 366, 411, 10560, 3319, 1596, 257, 688, 295, 9130, 370, 456, 311, 257, 12802, 1874, 3938, 490, 341, 3035, 13, 50764, 50764, 639, 3329, 1185, 1219, 314, 20, 597, 286, 519, 575, 2272, 5218, 9834, 13, 50964, 50964, 400, 309, 311, 445, 3097, 2969, 264, 700, 636, 293, 550, 561, 13, 51264, 51264, 1449, 2489, 10864, 281, 1867, 1651, 11, 597, 727, 1651, 466, 1340, 457, 291, 500, 380, 855, 309, 604, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.25109477215502635, "compression_ratio": 1.5040983606557377, "no_speech_prob": 1.8055239706882276e-05}, {"id": 807, "seek": 589500, "start": 5907.0, "end": 5913.0, "text": " And it's just training themselves the first way and then people.", "tokens": [50364, 821, 311, 512, 4467, 613, 5245, 366, 411, 10560, 3319, 1596, 257, 688, 295, 9130, 370, 456, 311, 257, 12802, 1874, 3938, 490, 341, 3035, 13, 50764, 50764, 639, 3329, 1185, 1219, 314, 20, 597, 286, 519, 575, 2272, 5218, 9834, 13, 50964, 50964, 400, 309, 311, 445, 3097, 2969, 264, 700, 636, 293, 550, 561, 13, 51264, 51264, 1449, 2489, 10864, 281, 1867, 1651, 11, 597, 727, 1651, 466, 1340, 457, 291, 500, 380, 855, 309, 604, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.25109477215502635, "compression_ratio": 1.5040983606557377, "no_speech_prob": 1.8055239706882276e-05}, {"id": 808, "seek": 589500, "start": 5913.0, "end": 5918.0, "text": " Just fine tune to answer questions, which could questions about anything but you don't show it any.", "tokens": [50364, 821, 311, 512, 4467, 613, 5245, 366, 411, 10560, 3319, 1596, 257, 688, 295, 9130, 370, 456, 311, 257, 12802, 1874, 3938, 490, 341, 3035, 13, 50764, 50764, 639, 3329, 1185, 1219, 314, 20, 597, 286, 519, 575, 2272, 5218, 9834, 13, 50964, 50964, 400, 309, 311, 445, 3097, 2969, 264, 700, 636, 293, 550, 561, 13, 51264, 51264, 1449, 2489, 10864, 281, 1867, 1651, 11, 597, 727, 1651, 466, 1340, 457, 291, 500, 380, 855, 309, 604, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.25109477215502635, "compression_ratio": 1.5040983606557377, "no_speech_prob": 1.8055239706882276e-05}, {"id": 809, "seek": 591800, "start": 5918.0, "end": 5925.0, "text": " You don't show Wikipedia or something to know you like see what's memorized and you can test how much knowledge is in the model from that.", "tokens": [50364, 509, 500, 380, 855, 28999, 420, 746, 281, 458, 291, 411, 536, 437, 311, 46677, 293, 291, 393, 1500, 577, 709, 3601, 307, 294, 264, 2316, 490, 300, 13, 50714, 50714, 400, 309, 311, 406, 3273, 281, 312, 484, 295, 300, 457, 309, 411, 11, 309, 311, 733, 295, 6958, 534, 665, 13, 51064, 51064, 467, 311, 411, 11, 309, 4523, 484, 321, 362, 2272, 5218, 9834, 291, 3847, 337, 257, 938, 565, 291, 393, 445, 3318, 2603, 3547, 295, 9130, 666, 613, 5245, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.23491752281617584, "compression_ratio": 1.5589519650655022, "no_speech_prob": 5.5939058256626595e-06}, {"id": 810, "seek": 591800, "start": 5925.0, "end": 5932.0, "text": " And it's not safe to be out of that but it like, it's kind of scary really good.", "tokens": [50364, 509, 500, 380, 855, 28999, 420, 746, 281, 458, 291, 411, 536, 437, 311, 46677, 293, 291, 393, 1500, 577, 709, 3601, 307, 294, 264, 2316, 490, 300, 13, 50714, 50714, 400, 309, 311, 406, 3273, 281, 312, 484, 295, 300, 457, 309, 411, 11, 309, 311, 733, 295, 6958, 534, 665, 13, 51064, 51064, 467, 311, 411, 11, 309, 4523, 484, 321, 362, 2272, 5218, 9834, 291, 3847, 337, 257, 938, 565, 291, 393, 445, 3318, 2603, 3547, 295, 9130, 666, 613, 5245, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.23491752281617584, "compression_ratio": 1.5589519650655022, "no_speech_prob": 5.5939058256626595e-06}, {"id": 811, "seek": 591800, "start": 5932.0, "end": 5941.0, "text": " It's like, it turns out we have 12 billion parameters you train for a long time you can just fit huge numbers of facts into these models.", "tokens": [50364, 509, 500, 380, 855, 28999, 420, 746, 281, 458, 291, 411, 536, 437, 311, 46677, 293, 291, 393, 1500, 577, 709, 3601, 307, 294, 264, 2316, 490, 300, 13, 50714, 50714, 400, 309, 311, 406, 3273, 281, 312, 484, 295, 300, 457, 309, 411, 11, 309, 311, 733, 295, 6958, 534, 665, 13, 51064, 51064, 467, 311, 411, 11, 309, 4523, 484, 321, 362, 2272, 5218, 9834, 291, 3847, 337, 257, 938, 565, 291, 393, 445, 3318, 2603, 3547, 295, 9130, 666, 613, 5245, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.23491752281617584, "compression_ratio": 1.5589519650655022, "no_speech_prob": 5.5939058256626595e-06}, {"id": 812, "seek": 594100, "start": 5941.0, "end": 5955.0, "text": " So it's not the most desirable way necessarily to memorize knowledge but seems somewhat effective.", "tokens": [50364, 407, 309, 311, 406, 264, 881, 30533, 636, 4725, 281, 27478, 3601, 457, 2544, 8344, 4942, 13, 51064, 51064, 1033, 13, 51114, 51114, 663, 390, 309, 13, 51164, 51164, 1044, 291, 1518, 337, 15862, 13, 1044, 291, 370, 709, 6602, 337, 337, 2902, 257, 8341, 7991, 13, 51514, 51514, 467, 311, 665, 281, 1568, 300, 1507, 490, 264, 3975, 442, 4525, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.2808083909930605, "compression_ratio": 1.4388888888888889, "no_speech_prob": 1.8056574845104478e-05}, {"id": 813, "seek": 594100, "start": 5955.0, "end": 5956.0, "text": " Okay.", "tokens": [50364, 407, 309, 311, 406, 264, 881, 30533, 636, 4725, 281, 27478, 3601, 457, 2544, 8344, 4942, 13, 51064, 51064, 1033, 13, 51114, 51114, 663, 390, 309, 13, 51164, 51164, 1044, 291, 1518, 337, 15862, 13, 1044, 291, 370, 709, 6602, 337, 337, 2902, 257, 8341, 7991, 13, 51514, 51514, 467, 311, 665, 281, 1568, 300, 1507, 490, 264, 3975, 442, 4525, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.2808083909930605, "compression_ratio": 1.4388888888888889, "no_speech_prob": 1.8056574845104478e-05}, {"id": 814, "seek": 594100, "start": 5956.0, "end": 5957.0, "text": " That was it.", "tokens": [50364, 407, 309, 311, 406, 264, 881, 30533, 636, 4725, 281, 27478, 3601, 457, 2544, 8344, 4942, 13, 51064, 51064, 1033, 13, 51114, 51114, 663, 390, 309, 13, 51164, 51164, 1044, 291, 1518, 337, 15862, 13, 1044, 291, 370, 709, 6602, 337, 337, 2902, 257, 8341, 7991, 13, 51514, 51514, 467, 311, 665, 281, 1568, 300, 1507, 490, 264, 3975, 442, 4525, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.2808083909930605, "compression_ratio": 1.4388888888888889, "no_speech_prob": 1.8056574845104478e-05}, {"id": 815, "seek": 594100, "start": 5957.0, "end": 5964.0, "text": " Thank you everyone for attending. Thank you so much Mike for for giving a guest lecture.", "tokens": [50364, 407, 309, 311, 406, 264, 881, 30533, 636, 4725, 281, 27478, 3601, 457, 2544, 8344, 4942, 13, 51064, 51064, 1033, 13, 51114, 51114, 663, 390, 309, 13, 51164, 51164, 1044, 291, 1518, 337, 15862, 13, 1044, 291, 370, 709, 6602, 337, 337, 2902, 257, 8341, 7991, 13, 51514, 51514, 467, 311, 665, 281, 1568, 300, 1507, 490, 264, 3975, 442, 4525, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.2808083909930605, "compression_ratio": 1.4388888888888889, "no_speech_prob": 1.8056574845104478e-05}, {"id": 816, "seek": 594100, "start": 5964.0, "end": 5967.0, "text": " It's good to hear that stuff from the hostess mouth.", "tokens": [50364, 407, 309, 311, 406, 264, 881, 30533, 636, 4725, 281, 27478, 3601, 457, 2544, 8344, 4942, 13, 51064, 51064, 1033, 13, 51114, 51114, 663, 390, 309, 13, 51164, 51164, 1044, 291, 1518, 337, 15862, 13, 1044, 291, 370, 709, 6602, 337, 337, 2902, 257, 8341, 7991, 13, 51514, 51514, 467, 311, 665, 281, 1568, 300, 1507, 490, 264, 3975, 442, 4525, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.2808083909930605, "compression_ratio": 1.4388888888888889, "no_speech_prob": 1.8056574845104478e-05}, {"id": 817, "seek": 596700, "start": 5967.0, "end": 5974.0, "text": " And we'll see see everyone again next next week.", "tokens": [50364, 400, 321, 603, 536, 536, 1518, 797, 958, 958, 1243, 13, 50714, 50714, 1042, 767, 4153, 558, 4153, 321, 434, 516, 281, 4445, 294, 1203, 490, 8459, 13, 50864, 50864, 1468, 380, 2870, 13, 50914, 50914, 407, 4153, 291, 483, 264, 4365, 322, 264, 3089, 1252, 295, 439, 295, 341, 13, 51114, 51114, 1396, 8138, 291, 603, 1568, 337, 291, 603, 1568, 466, 4295, 18161, 36170, 490, 264, 691, 897, 424, 2153, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.2946575709751674, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.00020652385137509555}, {"id": 818, "seek": 596700, "start": 5974.0, "end": 5977.0, "text": " Well actually tomorrow right tomorrow we're going to implement in everything from scratch.", "tokens": [50364, 400, 321, 603, 536, 536, 1518, 797, 958, 958, 1243, 13, 50714, 50714, 1042, 767, 4153, 558, 4153, 321, 434, 516, 281, 4445, 294, 1203, 490, 8459, 13, 50864, 50864, 1468, 380, 2870, 13, 50914, 50914, 407, 4153, 291, 483, 264, 4365, 322, 264, 3089, 1252, 295, 439, 295, 341, 13, 51114, 51114, 1396, 8138, 291, 603, 1568, 337, 291, 603, 1568, 466, 4295, 18161, 36170, 490, 264, 691, 897, 424, 2153, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.2946575709751674, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.00020652385137509555}, {"id": 819, "seek": 596700, "start": 5977.0, "end": 5978.0, "text": " Don't forget.", "tokens": [50364, 400, 321, 603, 536, 536, 1518, 797, 958, 958, 1243, 13, 50714, 50714, 1042, 767, 4153, 558, 4153, 321, 434, 516, 281, 4445, 294, 1203, 490, 8459, 13, 50864, 50864, 1468, 380, 2870, 13, 50914, 50914, 407, 4153, 291, 483, 264, 4365, 322, 264, 3089, 1252, 295, 439, 295, 341, 13, 51114, 51114, 1396, 8138, 291, 603, 1568, 337, 291, 603, 1568, 466, 4295, 18161, 36170, 490, 264, 691, 897, 424, 2153, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.2946575709751674, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.00020652385137509555}, {"id": 820, "seek": 596700, "start": 5978.0, "end": 5982.0, "text": " So tomorrow you get the details on the code side of all of this.", "tokens": [50364, 400, 321, 603, 536, 536, 1518, 797, 958, 958, 1243, 13, 50714, 50714, 1042, 767, 4153, 558, 4153, 321, 434, 516, 281, 4445, 294, 1203, 490, 8459, 13, 50864, 50864, 1468, 380, 2870, 13, 50914, 50914, 407, 4153, 291, 483, 264, 4365, 322, 264, 3089, 1252, 295, 439, 295, 341, 13, 51114, 51114, 1396, 8138, 291, 603, 1568, 337, 291, 603, 1568, 466, 4295, 18161, 36170, 490, 264, 691, 897, 424, 2153, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.2946575709751674, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.00020652385137509555}, {"id": 821, "seek": 596700, "start": 5982.0, "end": 5989.0, "text": " Then Monday you'll hear for you'll hear about graph neural nets from the Vibra song.", "tokens": [50364, 400, 321, 603, 536, 536, 1518, 797, 958, 958, 1243, 13, 50714, 50714, 1042, 767, 4153, 558, 4153, 321, 434, 516, 281, 4445, 294, 1203, 490, 8459, 13, 50864, 50864, 1468, 380, 2870, 13, 50914, 50914, 407, 4153, 291, 483, 264, 4365, 322, 264, 3089, 1252, 295, 439, 295, 341, 13, 51114, 51114, 1396, 8138, 291, 603, 1568, 337, 291, 603, 1568, 466, 4295, 18161, 36170, 490, 264, 691, 897, 424, 2153, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.2946575709751674, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.00020652385137509555}, {"id": 822, "seek": 598900, "start": 5989.0, "end": 6002.0, "text": " Yeah about graph neural nets and graph knowledge is that is graph knowledge used for language as well somehow because right. I don't know I have not really knowledgeable about this part of the field.", "tokens": [50364, 865, 466, 4295, 18161, 36170, 293, 4295, 3601, 307, 300, 307, 4295, 3601, 1143, 337, 2856, 382, 731, 6063, 570, 558, 13, 286, 500, 380, 458, 286, 362, 406, 534, 33800, 466, 341, 644, 295, 264, 2519, 13, 51014, 51014], "temperature": 0.0, "avg_logprob": -0.23313256672450475, "compression_ratio": 1.4962406015037595, "no_speech_prob": 9.025937470141798e-05}, {"id": 823, "seek": 600200, "start": 6002.0, "end": 6022.0, "text": " You can view to some extent you can view all the those self supervised training in text like word to back Burt etc. They use a graph and the graph is how you know how often towards appear next to each other, or some distance away from each other in the text that's, you know, the graph of similarity between words basically", "tokens": [50364, 509, 393, 1910, 281, 512, 8396, 291, 393, 1910, 439, 264, 729, 2698, 46533, 3097, 294, 2487, 411, 1349, 281, 646, 363, 6224, 5183, 13, 814, 764, 257, 4295, 293, 264, 4295, 307, 577, 291, 458, 577, 2049, 3030, 4204, 958, 281, 1184, 661, 11, 420, 512, 4560, 1314, 490, 1184, 661, 294, 264, 2487, 300, 311, 11, 291, 458, 11, 264, 4295, 295, 32194, 1296, 2283, 1936, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.32254396544562447, "compression_ratio": 1.599009900990099, "no_speech_prob": 2.710269473027438e-05}, {"id": 824, "seek": 602200, "start": 6022.0, "end": 6033.0, "text": " is defined by how far you know how often they appear nearby. Yeah, that's that's when you decide to put them in the input of a neural net, because they appear, you know, within a sequence.", "tokens": [50364, 307, 7642, 538, 577, 1400, 291, 458, 577, 2049, 436, 4204, 11184, 13, 865, 11, 300, 311, 300, 311, 562, 291, 4536, 281, 829, 552, 294, 264, 4846, 295, 257, 18161, 2533, 11, 570, 436, 4204, 11, 291, 458, 11, 1951, 257, 8310, 13, 50914, 50914, 407, 291, 393, 519, 295, 729, 11, 729, 11, 729, 2698, 46533, 2539, 3652, 382, 1936, 257, 588, 2199, 1254, 295, 4295, 18161, 2533, 13, 51314, 51314, 583, 264, 4295, 1009, 575, 264, 912, 3877, 309, 311, 1009, 8213, 11, 457, 309, 1009, 16203, 428, 11, 428, 12512, 294, 264, 2487, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2226893107096354, "compression_ratio": 1.7370517928286853, "no_speech_prob": 2.0779720216523856e-05}, {"id": 825, "seek": 602200, "start": 6033.0, "end": 6041.0, "text": " So you can think of those, those, those self supervised learning systems as basically a very simple form of graph neural net.", "tokens": [50364, 307, 7642, 538, 577, 1400, 291, 458, 577, 2049, 436, 4204, 11184, 13, 865, 11, 300, 311, 300, 311, 562, 291, 4536, 281, 829, 552, 294, 264, 4846, 295, 257, 18161, 2533, 11, 570, 436, 4204, 11, 291, 458, 11, 1951, 257, 8310, 13, 50914, 50914, 407, 291, 393, 519, 295, 729, 11, 729, 11, 729, 2698, 46533, 2539, 3652, 382, 1936, 257, 588, 2199, 1254, 295, 4295, 18161, 2533, 13, 51314, 51314, 583, 264, 4295, 1009, 575, 264, 912, 3877, 309, 311, 1009, 8213, 11, 457, 309, 1009, 16203, 428, 11, 428, 12512, 294, 264, 2487, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2226893107096354, "compression_ratio": 1.7370517928286853, "no_speech_prob": 2.0779720216523856e-05}, {"id": 826, "seek": 602200, "start": 6041.0, "end": 6050.0, "text": " But the graph always has the same structure it's always linear, but it always indicates your, your neighbors in the text.", "tokens": [50364, 307, 7642, 538, 577, 1400, 291, 458, 577, 2049, 436, 4204, 11184, 13, 865, 11, 300, 311, 300, 311, 562, 291, 4536, 281, 829, 552, 294, 264, 4846, 295, 257, 18161, 2533, 11, 570, 436, 4204, 11, 291, 458, 11, 1951, 257, 8310, 13, 50914, 50914, 407, 291, 393, 519, 295, 729, 11, 729, 11, 729, 2698, 46533, 2539, 3652, 382, 1936, 257, 588, 2199, 1254, 295, 4295, 18161, 2533, 13, 51314, 51314, 583, 264, 4295, 1009, 575, 264, 912, 3877, 309, 311, 1009, 8213, 11, 457, 309, 1009, 16203, 428, 11, 428, 12512, 294, 264, 2487, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2226893107096354, "compression_ratio": 1.7370517928286853, "no_speech_prob": 2.0779720216523856e-05}, {"id": 827, "seek": 605000, "start": 6050.0, "end": 6053.0, "text": " All right. All right. Thank you so much, everyone.", "tokens": [50364, 1057, 558, 13, 1057, 558, 13, 1044, 291, 370, 709, 11, 1518, 13, 50514, 50514, 2205, 5634, 13, 4621, 6543, 13, 50614], "temperature": 0.0, "avg_logprob": -0.25000107288360596, "compression_ratio": 1.042857142857143, "no_speech_prob": 0.00044948566937819123}, {"id": 828, "seek": 605300, "start": 6053.0, "end": 6081.0, "text": " Have a good evening. Bye bye.", "tokens": [50364, 3560, 257, 665, 5634, 13, 4621, 6543, 13, 51764], "temperature": 0.0, "avg_logprob": -0.26437204534357245, "compression_ratio": 0.7837837837837838, "no_speech_prob": 0.00044710407382808626}], "language": "en", "video_id": "6D4EWKJgNn0", "entity": "Yann LeCun"}}