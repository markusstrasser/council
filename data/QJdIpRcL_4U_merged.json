{"video_id": "QJdIpRcL_4U", "title": "3.3 Classification | Decision boundary  --[Machine Learning | Andrew Ng]", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 643, "views": 331, "publish_date": "11/04/2022", "timestamp": 1661126400, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " In the last video, you learned about the logistic regression model. Now, let's take a look at the decision boundary to get a better sense of how logistic regression is computing its predictions. To recap, here's how the logistic regression model's outputs are computed in two steps. In the first step, you compute z as w dot x plus b. Then you apply the sigmoid function g to this value z, and here again is the formula for the sigmoid function. Another way to write this is we can say f of x is equal to g, the sigmoid function, also called the logistic function, applied to w dot x plus b, where this is of course the value of z. And if you take the definition of the sigmoid function and plug in the definition of z, then you find that f of x is equal to this formula over here, 1 over 1 plus e to the negative z, where z is w x plus b. And you may remember we said in the previous video that we interpret this as the probability that y is equal to 1, given x, and with parameters w and b. And so this is going to be a number, like maybe 0.7 or 0.3. Now what if you want the learning algorithm to predict, is the value of y going to be 0 or 1? Well one thing you might do is set a threshold above which you predict y is 1, or you set y hat, the prediction, to be equal to 1, and below which you might say y hat, my prediction, is going to be equal to 0. So a common choice would be to pick a threshold of 0.5, so that if f of x is greater than or equal to 0.5, then predict y is 1, and we write that prediction as y hat equals 1, or if f of x is less than 0.5, then predict y is 0, or in other words, the prediction y hat is equal to 0. So now let's dive deeper into when the model would predict 1. In other words, when is f of x greater than or equal to 0.5? Well recall that f of x is just equal to g of z, and so f is greater than or equal to 0.5 whenever g of z is greater than or equal to 0.5. But when is g of z greater than or equal to 0.5? Well here's the sigmoid function over here, and so g of z is greater than or equal to 0.5 whenever z is greater than or equal to 0, right? That is, whenever z is on the right half of this axis. And finally, when is z greater than or equal to 0? Well z is equal to w dot x plus b, and so z is greater than or equal to 0 whenever w dot x plus b is greater than or equal to 0. So to recap, what you've seen here is that the model predicts 1 whenever w dot x plus b is greater than or equal to 0. And conversely, when w dot x plus b is less than 0, the algorithm predicts y is 0. So given this, let's now visualize how the model makes predictions. I'm going to take an example of a classification problem where you have two features, x1 and x2 instead of just one feature. Here's a training set where the little red crosses denote the positive examples, and the little blue circles denote negative examples. So the red crosses corresponds to y equals 1, and the blue circles correspond to y equals 0. So the logistic regression model will make predictions using this function, f of x equals g of z, where z is now this expression over here, w1 x1 plus w2 x2 plus b, because we have two features, x1 and x2. And let's just say for this example that the value of the parameters are w1 equals 1, w2 equals 1, and b equals negative 3. And let's now take a look at how logistic regression makes predictions. In particular, let's figure out when wx plus b is greater than or equal to 0, and when wx plus b is less than 0. To figure that out, there's a very interesting line to look at, which is when wx plus b is exactly equal to 0. It turns out that this line is also called the decision boundary, because that's the line where you're just almost neutral about whether y is 0 or y is 1. Now for the values of the parameters w1, w2, and b that we had written down above, this decision boundary is just x1 plus x2 minus 3. And so when is x1 plus x2 minus 3 equal to 0? Well that will correspond to the line x1 plus x2 equals 3. And that is this line shown over here. And so this line turns out to be the decision boundary, where if the features x are to the right of this line, logistic regression would predict 1, and to the left of this line, logistic regression would predict 0. In other words, what we have just visualized is the decision boundary for logistic regression when the parameters w1, w2, and b are 1, 1, and negative 3. Of course, if you had a different choice of the parameters, the decision boundary would be a different line. Now let's look at a more complex example where the decision boundary is no longer a straight line. As before, crosses denote the class y equals 1, and the little circles denote the class y equals 0. Earlier last week, you saw how to use polynomials in linear regression, and you can do the same in logistic regression. So let's set z to be w1 x1 squared plus w2 x2 squared plus b. With this choice of features, polynomial features into logistic regression, so f of x, which equals g of z, is now g of this expression over here. And let's say that we end up choosing w1 and w2 to be 1, and b to be negative 1. So z is equal to 1 times x1 squared plus 1 times x2 squared minus 1. And the decision boundary as before will correspond to when z is equal to 0, and so this expression will be equal to 0 when x1 squared plus x2 squared is equal to 1. And if you plot on the diagram on the left, the curve corresponding to x1 squared plus x2 squared equals 1, this turns out to be this circle. When x1 squared plus x2 squared is greater than or equal to 1, that's this area outside the circle, and that's when you predict y to be 1. Conversely, when x1 squared plus x2 squared is less than 1, that's this area inside the circle, and that's when you would predict y to be 0. So can we come up with even more complex decision boundaries than these? Yes you can. You can do so by having even higher order polynomial terms. Say z is w1 x1 plus w2 x2 plus w3 x1 squared plus w4 x1 x2 plus w5 x2 squared. Then it's possible you can get even more complex decision boundaries. The model can define decision boundaries such as this example, an ellipse that's like this, or with a different choice of the parameters. You can even get more complex decision boundaries which can look like functions that maybe look like that. So this is an example of an even more complex decision boundary than the ones we've seen previously, and this implementation of logistic regression will predict y equals 1 inside this shape, and outside the shape it will predict y equals 0. So with these polynomial features you can get very complex decision boundaries. In other words logistic regression can learn to fit pretty complex data. Although if you were to not include any of these higher order polynomials, so if the only features you use are x1 x2 x3 and so on, then the decision boundary for logistic regression will always be linear, will always be a straight line. In the upcoming optional lab you also get to see the code implementation of the decision boundary. In the example in the lab there will be two features so you can see the decision boundary as a line. So with these visualizations I hope that you now have a sense of the range of possible models you can get with logistic regression. Now that you've seen what f of x can potentially compute, let's take a look at how you can actually train a logistic regression model. We'll start by looking at the cost function for logistic regression and after that figure out how to apply gradient descent to it. Let's go on to the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.2, "text": " In the last video, you learned about the logistic regression model.", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 3264, 466, 264, 3565, 3142, 24590, 2316, 13, 50624, 50624, 823, 11, 718, 311, 747, 257, 574, 412, 264, 3537, 12866, 281, 483, 257, 1101, 2020, 295, 577, 3565, 3142, 24590, 50950, 50950, 307, 15866, 1080, 21264, 13, 51060, 51060, 1407, 20928, 11, 510, 311, 577, 264, 3565, 3142, 24590, 2316, 311, 23930, 366, 40610, 294, 732, 4439, 13, 51416, 51416, 682, 264, 700, 1823, 11, 291, 14722, 710, 382, 261, 5893, 2031, 1804, 272, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13807982067729152, "compression_ratio": 1.7128205128205127, "no_speech_prob": 0.00831218808889389}, {"id": 1, "seek": 0, "start": 5.2, "end": 11.72, "text": " Now, let's take a look at the decision boundary to get a better sense of how logistic regression", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 3264, 466, 264, 3565, 3142, 24590, 2316, 13, 50624, 50624, 823, 11, 718, 311, 747, 257, 574, 412, 264, 3537, 12866, 281, 483, 257, 1101, 2020, 295, 577, 3565, 3142, 24590, 50950, 50950, 307, 15866, 1080, 21264, 13, 51060, 51060, 1407, 20928, 11, 510, 311, 577, 264, 3565, 3142, 24590, 2316, 311, 23930, 366, 40610, 294, 732, 4439, 13, 51416, 51416, 682, 264, 700, 1823, 11, 291, 14722, 710, 382, 261, 5893, 2031, 1804, 272, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13807982067729152, "compression_ratio": 1.7128205128205127, "no_speech_prob": 0.00831218808889389}, {"id": 2, "seek": 0, "start": 11.72, "end": 13.92, "text": " is computing its predictions.", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 3264, 466, 264, 3565, 3142, 24590, 2316, 13, 50624, 50624, 823, 11, 718, 311, 747, 257, 574, 412, 264, 3537, 12866, 281, 483, 257, 1101, 2020, 295, 577, 3565, 3142, 24590, 50950, 50950, 307, 15866, 1080, 21264, 13, 51060, 51060, 1407, 20928, 11, 510, 311, 577, 264, 3565, 3142, 24590, 2316, 311, 23930, 366, 40610, 294, 732, 4439, 13, 51416, 51416, 682, 264, 700, 1823, 11, 291, 14722, 710, 382, 261, 5893, 2031, 1804, 272, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13807982067729152, "compression_ratio": 1.7128205128205127, "no_speech_prob": 0.00831218808889389}, {"id": 3, "seek": 0, "start": 13.92, "end": 21.04, "text": " To recap, here's how the logistic regression model's outputs are computed in two steps.", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 3264, 466, 264, 3565, 3142, 24590, 2316, 13, 50624, 50624, 823, 11, 718, 311, 747, 257, 574, 412, 264, 3537, 12866, 281, 483, 257, 1101, 2020, 295, 577, 3565, 3142, 24590, 50950, 50950, 307, 15866, 1080, 21264, 13, 51060, 51060, 1407, 20928, 11, 510, 311, 577, 264, 3565, 3142, 24590, 2316, 311, 23930, 366, 40610, 294, 732, 4439, 13, 51416, 51416, 682, 264, 700, 1823, 11, 291, 14722, 710, 382, 261, 5893, 2031, 1804, 272, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13807982067729152, "compression_ratio": 1.7128205128205127, "no_speech_prob": 0.00831218808889389}, {"id": 4, "seek": 0, "start": 21.04, "end": 26.92, "text": " In the first step, you compute z as w dot x plus b.", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 3264, 466, 264, 3565, 3142, 24590, 2316, 13, 50624, 50624, 823, 11, 718, 311, 747, 257, 574, 412, 264, 3537, 12866, 281, 483, 257, 1101, 2020, 295, 577, 3565, 3142, 24590, 50950, 50950, 307, 15866, 1080, 21264, 13, 51060, 51060, 1407, 20928, 11, 510, 311, 577, 264, 3565, 3142, 24590, 2316, 311, 23930, 366, 40610, 294, 732, 4439, 13, 51416, 51416, 682, 264, 700, 1823, 11, 291, 14722, 710, 382, 261, 5893, 2031, 1804, 272, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13807982067729152, "compression_ratio": 1.7128205128205127, "no_speech_prob": 0.00831218808889389}, {"id": 5, "seek": 2692, "start": 26.92, "end": 33.120000000000005, "text": " Then you apply the sigmoid function g to this value z, and here again is the formula for", "tokens": [50364, 1396, 291, 3079, 264, 4556, 3280, 327, 2445, 290, 281, 341, 2158, 710, 11, 293, 510, 797, 307, 264, 8513, 337, 50674, 50674, 264, 4556, 3280, 327, 2445, 13, 50817, 50817, 3996, 636, 281, 2464, 341, 307, 321, 393, 584, 283, 295, 2031, 307, 2681, 281, 290, 11, 264, 4556, 3280, 327, 2445, 11, 51178, 51178, 611, 1219, 264, 3565, 3142, 2445, 11, 6456, 281, 261, 5893, 2031, 1804, 272, 11, 689, 341, 307, 295, 1164, 51526, 51526, 264, 2158, 295, 710, 13, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.1351171252371251, "compression_ratio": 1.7109826589595376, "no_speech_prob": 4.9857724661706015e-05}, {"id": 6, "seek": 2692, "start": 33.120000000000005, "end": 35.980000000000004, "text": " the sigmoid function.", "tokens": [50364, 1396, 291, 3079, 264, 4556, 3280, 327, 2445, 290, 281, 341, 2158, 710, 11, 293, 510, 797, 307, 264, 8513, 337, 50674, 50674, 264, 4556, 3280, 327, 2445, 13, 50817, 50817, 3996, 636, 281, 2464, 341, 307, 321, 393, 584, 283, 295, 2031, 307, 2681, 281, 290, 11, 264, 4556, 3280, 327, 2445, 11, 51178, 51178, 611, 1219, 264, 3565, 3142, 2445, 11, 6456, 281, 261, 5893, 2031, 1804, 272, 11, 689, 341, 307, 295, 1164, 51526, 51526, 264, 2158, 295, 710, 13, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.1351171252371251, "compression_ratio": 1.7109826589595376, "no_speech_prob": 4.9857724661706015e-05}, {"id": 7, "seek": 2692, "start": 35.980000000000004, "end": 43.2, "text": " Another way to write this is we can say f of x is equal to g, the sigmoid function,", "tokens": [50364, 1396, 291, 3079, 264, 4556, 3280, 327, 2445, 290, 281, 341, 2158, 710, 11, 293, 510, 797, 307, 264, 8513, 337, 50674, 50674, 264, 4556, 3280, 327, 2445, 13, 50817, 50817, 3996, 636, 281, 2464, 341, 307, 321, 393, 584, 283, 295, 2031, 307, 2681, 281, 290, 11, 264, 4556, 3280, 327, 2445, 11, 51178, 51178, 611, 1219, 264, 3565, 3142, 2445, 11, 6456, 281, 261, 5893, 2031, 1804, 272, 11, 689, 341, 307, 295, 1164, 51526, 51526, 264, 2158, 295, 710, 13, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.1351171252371251, "compression_ratio": 1.7109826589595376, "no_speech_prob": 4.9857724661706015e-05}, {"id": 8, "seek": 2692, "start": 43.2, "end": 50.160000000000004, "text": " also called the logistic function, applied to w dot x plus b, where this is of course", "tokens": [50364, 1396, 291, 3079, 264, 4556, 3280, 327, 2445, 290, 281, 341, 2158, 710, 11, 293, 510, 797, 307, 264, 8513, 337, 50674, 50674, 264, 4556, 3280, 327, 2445, 13, 50817, 50817, 3996, 636, 281, 2464, 341, 307, 321, 393, 584, 283, 295, 2031, 307, 2681, 281, 290, 11, 264, 4556, 3280, 327, 2445, 11, 51178, 51178, 611, 1219, 264, 3565, 3142, 2445, 11, 6456, 281, 261, 5893, 2031, 1804, 272, 11, 689, 341, 307, 295, 1164, 51526, 51526, 264, 2158, 295, 710, 13, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.1351171252371251, "compression_ratio": 1.7109826589595376, "no_speech_prob": 4.9857724661706015e-05}, {"id": 9, "seek": 2692, "start": 50.160000000000004, "end": 53.120000000000005, "text": " the value of z.", "tokens": [50364, 1396, 291, 3079, 264, 4556, 3280, 327, 2445, 290, 281, 341, 2158, 710, 11, 293, 510, 797, 307, 264, 8513, 337, 50674, 50674, 264, 4556, 3280, 327, 2445, 13, 50817, 50817, 3996, 636, 281, 2464, 341, 307, 321, 393, 584, 283, 295, 2031, 307, 2681, 281, 290, 11, 264, 4556, 3280, 327, 2445, 11, 51178, 51178, 611, 1219, 264, 3565, 3142, 2445, 11, 6456, 281, 261, 5893, 2031, 1804, 272, 11, 689, 341, 307, 295, 1164, 51526, 51526, 264, 2158, 295, 710, 13, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.1351171252371251, "compression_ratio": 1.7109826589595376, "no_speech_prob": 4.9857724661706015e-05}, {"id": 10, "seek": 5312, "start": 53.12, "end": 59.879999999999995, "text": " And if you take the definition of the sigmoid function and plug in the definition of z,", "tokens": [50364, 400, 498, 291, 747, 264, 7123, 295, 264, 4556, 3280, 327, 2445, 293, 5452, 294, 264, 7123, 295, 710, 11, 50702, 50702, 550, 291, 915, 300, 283, 295, 2031, 307, 2681, 281, 341, 8513, 670, 510, 11, 502, 670, 502, 1804, 308, 281, 264, 51076, 51076, 3671, 710, 11, 689, 710, 307, 261, 2031, 1804, 272, 13, 51312, 51312, 400, 291, 815, 1604, 321, 848, 294, 264, 3894, 960, 300, 321, 7302, 341, 382, 264, 8482, 51582, 51582, 300, 288, 307, 2681, 281, 502, 11, 2212, 2031, 11, 293, 365, 9834, 261, 293, 272, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.08688897797555635, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.222803454467794e-06}, {"id": 11, "seek": 5312, "start": 59.879999999999995, "end": 67.36, "text": " then you find that f of x is equal to this formula over here, 1 over 1 plus e to the", "tokens": [50364, 400, 498, 291, 747, 264, 7123, 295, 264, 4556, 3280, 327, 2445, 293, 5452, 294, 264, 7123, 295, 710, 11, 50702, 50702, 550, 291, 915, 300, 283, 295, 2031, 307, 2681, 281, 341, 8513, 670, 510, 11, 502, 670, 502, 1804, 308, 281, 264, 51076, 51076, 3671, 710, 11, 689, 710, 307, 261, 2031, 1804, 272, 13, 51312, 51312, 400, 291, 815, 1604, 321, 848, 294, 264, 3894, 960, 300, 321, 7302, 341, 382, 264, 8482, 51582, 51582, 300, 288, 307, 2681, 281, 502, 11, 2212, 2031, 11, 293, 365, 9834, 261, 293, 272, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.08688897797555635, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.222803454467794e-06}, {"id": 12, "seek": 5312, "start": 67.36, "end": 72.08, "text": " negative z, where z is w x plus b.", "tokens": [50364, 400, 498, 291, 747, 264, 7123, 295, 264, 4556, 3280, 327, 2445, 293, 5452, 294, 264, 7123, 295, 710, 11, 50702, 50702, 550, 291, 915, 300, 283, 295, 2031, 307, 2681, 281, 341, 8513, 670, 510, 11, 502, 670, 502, 1804, 308, 281, 264, 51076, 51076, 3671, 710, 11, 689, 710, 307, 261, 2031, 1804, 272, 13, 51312, 51312, 400, 291, 815, 1604, 321, 848, 294, 264, 3894, 960, 300, 321, 7302, 341, 382, 264, 8482, 51582, 51582, 300, 288, 307, 2681, 281, 502, 11, 2212, 2031, 11, 293, 365, 9834, 261, 293, 272, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.08688897797555635, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.222803454467794e-06}, {"id": 13, "seek": 5312, "start": 72.08, "end": 77.47999999999999, "text": " And you may remember we said in the previous video that we interpret this as the probability", "tokens": [50364, 400, 498, 291, 747, 264, 7123, 295, 264, 4556, 3280, 327, 2445, 293, 5452, 294, 264, 7123, 295, 710, 11, 50702, 50702, 550, 291, 915, 300, 283, 295, 2031, 307, 2681, 281, 341, 8513, 670, 510, 11, 502, 670, 502, 1804, 308, 281, 264, 51076, 51076, 3671, 710, 11, 689, 710, 307, 261, 2031, 1804, 272, 13, 51312, 51312, 400, 291, 815, 1604, 321, 848, 294, 264, 3894, 960, 300, 321, 7302, 341, 382, 264, 8482, 51582, 51582, 300, 288, 307, 2681, 281, 502, 11, 2212, 2031, 11, 293, 365, 9834, 261, 293, 272, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.08688897797555635, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.222803454467794e-06}, {"id": 14, "seek": 5312, "start": 77.47999999999999, "end": 82.36, "text": " that y is equal to 1, given x, and with parameters w and b.", "tokens": [50364, 400, 498, 291, 747, 264, 7123, 295, 264, 4556, 3280, 327, 2445, 293, 5452, 294, 264, 7123, 295, 710, 11, 50702, 50702, 550, 291, 915, 300, 283, 295, 2031, 307, 2681, 281, 341, 8513, 670, 510, 11, 502, 670, 502, 1804, 308, 281, 264, 51076, 51076, 3671, 710, 11, 689, 710, 307, 261, 2031, 1804, 272, 13, 51312, 51312, 400, 291, 815, 1604, 321, 848, 294, 264, 3894, 960, 300, 321, 7302, 341, 382, 264, 8482, 51582, 51582, 300, 288, 307, 2681, 281, 502, 11, 2212, 2031, 11, 293, 365, 9834, 261, 293, 272, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.08688897797555635, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.222803454467794e-06}, {"id": 15, "seek": 8236, "start": 82.36, "end": 87.84, "text": " And so this is going to be a number, like maybe 0.7 or 0.3.", "tokens": [50364, 400, 370, 341, 307, 516, 281, 312, 257, 1230, 11, 411, 1310, 1958, 13, 22, 420, 1958, 13, 18, 13, 50638, 50638, 823, 437, 498, 291, 528, 264, 2539, 9284, 281, 6069, 11, 307, 264, 2158, 295, 288, 516, 281, 312, 50908, 50908, 1958, 420, 502, 30, 51032, 51032, 1042, 472, 551, 291, 1062, 360, 307, 992, 257, 14678, 3673, 597, 291, 6069, 288, 307, 502, 11, 420, 291, 992, 51382, 51382, 288, 2385, 11, 264, 17630, 11, 281, 312, 2681, 281, 502, 11, 293, 2507, 597, 291, 1062, 584, 288, 2385, 11, 452, 17630, 11, 51696, 51696, 307, 516, 281, 312, 2681, 281, 1958, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.13686329234730113, "compression_ratio": 1.6962616822429906, "no_speech_prob": 4.936948243994266e-06}, {"id": 16, "seek": 8236, "start": 87.84, "end": 93.24, "text": " Now what if you want the learning algorithm to predict, is the value of y going to be", "tokens": [50364, 400, 370, 341, 307, 516, 281, 312, 257, 1230, 11, 411, 1310, 1958, 13, 22, 420, 1958, 13, 18, 13, 50638, 50638, 823, 437, 498, 291, 528, 264, 2539, 9284, 281, 6069, 11, 307, 264, 2158, 295, 288, 516, 281, 312, 50908, 50908, 1958, 420, 502, 30, 51032, 51032, 1042, 472, 551, 291, 1062, 360, 307, 992, 257, 14678, 3673, 597, 291, 6069, 288, 307, 502, 11, 420, 291, 992, 51382, 51382, 288, 2385, 11, 264, 17630, 11, 281, 312, 2681, 281, 502, 11, 293, 2507, 597, 291, 1062, 584, 288, 2385, 11, 452, 17630, 11, 51696, 51696, 307, 516, 281, 312, 2681, 281, 1958, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.13686329234730113, "compression_ratio": 1.6962616822429906, "no_speech_prob": 4.936948243994266e-06}, {"id": 17, "seek": 8236, "start": 93.24, "end": 95.72, "text": " 0 or 1?", "tokens": [50364, 400, 370, 341, 307, 516, 281, 312, 257, 1230, 11, 411, 1310, 1958, 13, 22, 420, 1958, 13, 18, 13, 50638, 50638, 823, 437, 498, 291, 528, 264, 2539, 9284, 281, 6069, 11, 307, 264, 2158, 295, 288, 516, 281, 312, 50908, 50908, 1958, 420, 502, 30, 51032, 51032, 1042, 472, 551, 291, 1062, 360, 307, 992, 257, 14678, 3673, 597, 291, 6069, 288, 307, 502, 11, 420, 291, 992, 51382, 51382, 288, 2385, 11, 264, 17630, 11, 281, 312, 2681, 281, 502, 11, 293, 2507, 597, 291, 1062, 584, 288, 2385, 11, 452, 17630, 11, 51696, 51696, 307, 516, 281, 312, 2681, 281, 1958, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.13686329234730113, "compression_ratio": 1.6962616822429906, "no_speech_prob": 4.936948243994266e-06}, {"id": 18, "seek": 8236, "start": 95.72, "end": 102.72, "text": " Well one thing you might do is set a threshold above which you predict y is 1, or you set", "tokens": [50364, 400, 370, 341, 307, 516, 281, 312, 257, 1230, 11, 411, 1310, 1958, 13, 22, 420, 1958, 13, 18, 13, 50638, 50638, 823, 437, 498, 291, 528, 264, 2539, 9284, 281, 6069, 11, 307, 264, 2158, 295, 288, 516, 281, 312, 50908, 50908, 1958, 420, 502, 30, 51032, 51032, 1042, 472, 551, 291, 1062, 360, 307, 992, 257, 14678, 3673, 597, 291, 6069, 288, 307, 502, 11, 420, 291, 992, 51382, 51382, 288, 2385, 11, 264, 17630, 11, 281, 312, 2681, 281, 502, 11, 293, 2507, 597, 291, 1062, 584, 288, 2385, 11, 452, 17630, 11, 51696, 51696, 307, 516, 281, 312, 2681, 281, 1958, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.13686329234730113, "compression_ratio": 1.6962616822429906, "no_speech_prob": 4.936948243994266e-06}, {"id": 19, "seek": 8236, "start": 102.72, "end": 109.0, "text": " y hat, the prediction, to be equal to 1, and below which you might say y hat, my prediction,", "tokens": [50364, 400, 370, 341, 307, 516, 281, 312, 257, 1230, 11, 411, 1310, 1958, 13, 22, 420, 1958, 13, 18, 13, 50638, 50638, 823, 437, 498, 291, 528, 264, 2539, 9284, 281, 6069, 11, 307, 264, 2158, 295, 288, 516, 281, 312, 50908, 50908, 1958, 420, 502, 30, 51032, 51032, 1042, 472, 551, 291, 1062, 360, 307, 992, 257, 14678, 3673, 597, 291, 6069, 288, 307, 502, 11, 420, 291, 992, 51382, 51382, 288, 2385, 11, 264, 17630, 11, 281, 312, 2681, 281, 502, 11, 293, 2507, 597, 291, 1062, 584, 288, 2385, 11, 452, 17630, 11, 51696, 51696, 307, 516, 281, 312, 2681, 281, 1958, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.13686329234730113, "compression_ratio": 1.6962616822429906, "no_speech_prob": 4.936948243994266e-06}, {"id": 20, "seek": 8236, "start": 109.0, "end": 112.16, "text": " is going to be equal to 0.", "tokens": [50364, 400, 370, 341, 307, 516, 281, 312, 257, 1230, 11, 411, 1310, 1958, 13, 22, 420, 1958, 13, 18, 13, 50638, 50638, 823, 437, 498, 291, 528, 264, 2539, 9284, 281, 6069, 11, 307, 264, 2158, 295, 288, 516, 281, 312, 50908, 50908, 1958, 420, 502, 30, 51032, 51032, 1042, 472, 551, 291, 1062, 360, 307, 992, 257, 14678, 3673, 597, 291, 6069, 288, 307, 502, 11, 420, 291, 992, 51382, 51382, 288, 2385, 11, 264, 17630, 11, 281, 312, 2681, 281, 502, 11, 293, 2507, 597, 291, 1062, 584, 288, 2385, 11, 452, 17630, 11, 51696, 51696, 307, 516, 281, 312, 2681, 281, 1958, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.13686329234730113, "compression_ratio": 1.6962616822429906, "no_speech_prob": 4.936948243994266e-06}, {"id": 21, "seek": 11216, "start": 112.16, "end": 119.08, "text": " So a common choice would be to pick a threshold of 0.5, so that if f of x is greater than", "tokens": [50364, 407, 257, 2689, 3922, 576, 312, 281, 1888, 257, 14678, 295, 1958, 13, 20, 11, 370, 300, 498, 283, 295, 2031, 307, 5044, 813, 50710, 50710, 420, 2681, 281, 1958, 13, 20, 11, 550, 6069, 288, 307, 502, 11, 293, 321, 2464, 300, 17630, 382, 288, 2385, 6915, 502, 11, 51104, 51104, 420, 498, 283, 295, 2031, 307, 1570, 813, 1958, 13, 20, 11, 550, 6069, 288, 307, 1958, 11, 420, 294, 661, 2283, 11, 264, 17630, 51416, 51416, 288, 2385, 307, 2681, 281, 1958, 13, 51566, 51566, 407, 586, 718, 311, 9192, 7731, 666, 562, 264, 2316, 576, 6069, 502, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.08565057898467442, "compression_ratio": 1.82010582010582, "no_speech_prob": 7.071826985338703e-06}, {"id": 22, "seek": 11216, "start": 119.08, "end": 126.96, "text": " or equal to 0.5, then predict y is 1, and we write that prediction as y hat equals 1,", "tokens": [50364, 407, 257, 2689, 3922, 576, 312, 281, 1888, 257, 14678, 295, 1958, 13, 20, 11, 370, 300, 498, 283, 295, 2031, 307, 5044, 813, 50710, 50710, 420, 2681, 281, 1958, 13, 20, 11, 550, 6069, 288, 307, 502, 11, 293, 321, 2464, 300, 17630, 382, 288, 2385, 6915, 502, 11, 51104, 51104, 420, 498, 283, 295, 2031, 307, 1570, 813, 1958, 13, 20, 11, 550, 6069, 288, 307, 1958, 11, 420, 294, 661, 2283, 11, 264, 17630, 51416, 51416, 288, 2385, 307, 2681, 281, 1958, 13, 51566, 51566, 407, 586, 718, 311, 9192, 7731, 666, 562, 264, 2316, 576, 6069, 502, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.08565057898467442, "compression_ratio": 1.82010582010582, "no_speech_prob": 7.071826985338703e-06}, {"id": 23, "seek": 11216, "start": 126.96, "end": 133.2, "text": " or if f of x is less than 0.5, then predict y is 0, or in other words, the prediction", "tokens": [50364, 407, 257, 2689, 3922, 576, 312, 281, 1888, 257, 14678, 295, 1958, 13, 20, 11, 370, 300, 498, 283, 295, 2031, 307, 5044, 813, 50710, 50710, 420, 2681, 281, 1958, 13, 20, 11, 550, 6069, 288, 307, 502, 11, 293, 321, 2464, 300, 17630, 382, 288, 2385, 6915, 502, 11, 51104, 51104, 420, 498, 283, 295, 2031, 307, 1570, 813, 1958, 13, 20, 11, 550, 6069, 288, 307, 1958, 11, 420, 294, 661, 2283, 11, 264, 17630, 51416, 51416, 288, 2385, 307, 2681, 281, 1958, 13, 51566, 51566, 407, 586, 718, 311, 9192, 7731, 666, 562, 264, 2316, 576, 6069, 502, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.08565057898467442, "compression_ratio": 1.82010582010582, "no_speech_prob": 7.071826985338703e-06}, {"id": 24, "seek": 11216, "start": 133.2, "end": 136.2, "text": " y hat is equal to 0.", "tokens": [50364, 407, 257, 2689, 3922, 576, 312, 281, 1888, 257, 14678, 295, 1958, 13, 20, 11, 370, 300, 498, 283, 295, 2031, 307, 5044, 813, 50710, 50710, 420, 2681, 281, 1958, 13, 20, 11, 550, 6069, 288, 307, 502, 11, 293, 321, 2464, 300, 17630, 382, 288, 2385, 6915, 502, 11, 51104, 51104, 420, 498, 283, 295, 2031, 307, 1570, 813, 1958, 13, 20, 11, 550, 6069, 288, 307, 1958, 11, 420, 294, 661, 2283, 11, 264, 17630, 51416, 51416, 288, 2385, 307, 2681, 281, 1958, 13, 51566, 51566, 407, 586, 718, 311, 9192, 7731, 666, 562, 264, 2316, 576, 6069, 502, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.08565057898467442, "compression_ratio": 1.82010582010582, "no_speech_prob": 7.071826985338703e-06}, {"id": 25, "seek": 11216, "start": 136.2, "end": 140.12, "text": " So now let's dive deeper into when the model would predict 1.", "tokens": [50364, 407, 257, 2689, 3922, 576, 312, 281, 1888, 257, 14678, 295, 1958, 13, 20, 11, 370, 300, 498, 283, 295, 2031, 307, 5044, 813, 50710, 50710, 420, 2681, 281, 1958, 13, 20, 11, 550, 6069, 288, 307, 502, 11, 293, 321, 2464, 300, 17630, 382, 288, 2385, 6915, 502, 11, 51104, 51104, 420, 498, 283, 295, 2031, 307, 1570, 813, 1958, 13, 20, 11, 550, 6069, 288, 307, 1958, 11, 420, 294, 661, 2283, 11, 264, 17630, 51416, 51416, 288, 2385, 307, 2681, 281, 1958, 13, 51566, 51566, 407, 586, 718, 311, 9192, 7731, 666, 562, 264, 2316, 576, 6069, 502, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.08565057898467442, "compression_ratio": 1.82010582010582, "no_speech_prob": 7.071826985338703e-06}, {"id": 26, "seek": 14012, "start": 140.12, "end": 145.68, "text": " In other words, when is f of x greater than or equal to 0.5?", "tokens": [50364, 682, 661, 2283, 11, 562, 307, 283, 295, 2031, 5044, 813, 420, 2681, 281, 1958, 13, 20, 30, 50642, 50642, 1042, 9901, 300, 283, 295, 2031, 307, 445, 2681, 281, 290, 295, 710, 11, 293, 370, 283, 307, 5044, 813, 420, 2681, 281, 51030, 51030, 1958, 13, 20, 5699, 290, 295, 710, 307, 5044, 813, 420, 2681, 281, 1958, 13, 20, 13, 51318, 51318, 583, 562, 307, 290, 295, 710, 5044, 813, 420, 2681, 281, 1958, 13, 20, 30, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.11393347131200583, "compression_ratio": 1.984, "no_speech_prob": 9.97286133497255e-06}, {"id": 27, "seek": 14012, "start": 145.68, "end": 153.44, "text": " Well recall that f of x is just equal to g of z, and so f is greater than or equal to", "tokens": [50364, 682, 661, 2283, 11, 562, 307, 283, 295, 2031, 5044, 813, 420, 2681, 281, 1958, 13, 20, 30, 50642, 50642, 1042, 9901, 300, 283, 295, 2031, 307, 445, 2681, 281, 290, 295, 710, 11, 293, 370, 283, 307, 5044, 813, 420, 2681, 281, 51030, 51030, 1958, 13, 20, 5699, 290, 295, 710, 307, 5044, 813, 420, 2681, 281, 1958, 13, 20, 13, 51318, 51318, 583, 562, 307, 290, 295, 710, 5044, 813, 420, 2681, 281, 1958, 13, 20, 30, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.11393347131200583, "compression_ratio": 1.984, "no_speech_prob": 9.97286133497255e-06}, {"id": 28, "seek": 14012, "start": 153.44, "end": 159.20000000000002, "text": " 0.5 whenever g of z is greater than or equal to 0.5.", "tokens": [50364, 682, 661, 2283, 11, 562, 307, 283, 295, 2031, 5044, 813, 420, 2681, 281, 1958, 13, 20, 30, 50642, 50642, 1042, 9901, 300, 283, 295, 2031, 307, 445, 2681, 281, 290, 295, 710, 11, 293, 370, 283, 307, 5044, 813, 420, 2681, 281, 51030, 51030, 1958, 13, 20, 5699, 290, 295, 710, 307, 5044, 813, 420, 2681, 281, 1958, 13, 20, 13, 51318, 51318, 583, 562, 307, 290, 295, 710, 5044, 813, 420, 2681, 281, 1958, 13, 20, 30, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.11393347131200583, "compression_ratio": 1.984, "no_speech_prob": 9.97286133497255e-06}, {"id": 29, "seek": 14012, "start": 159.20000000000002, "end": 163.8, "text": " But when is g of z greater than or equal to 0.5?", "tokens": [50364, 682, 661, 2283, 11, 562, 307, 283, 295, 2031, 5044, 813, 420, 2681, 281, 1958, 13, 20, 30, 50642, 50642, 1042, 9901, 300, 283, 295, 2031, 307, 445, 2681, 281, 290, 295, 710, 11, 293, 370, 283, 307, 5044, 813, 420, 2681, 281, 51030, 51030, 1958, 13, 20, 5699, 290, 295, 710, 307, 5044, 813, 420, 2681, 281, 1958, 13, 20, 13, 51318, 51318, 583, 562, 307, 290, 295, 710, 5044, 813, 420, 2681, 281, 1958, 13, 20, 30, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.11393347131200583, "compression_ratio": 1.984, "no_speech_prob": 9.97286133497255e-06}, {"id": 30, "seek": 16380, "start": 163.8, "end": 171.08, "text": " Well here's the sigmoid function over here, and so g of z is greater than or equal to", "tokens": [50364, 1042, 510, 311, 264, 4556, 3280, 327, 2445, 670, 510, 11, 293, 370, 290, 295, 710, 307, 5044, 813, 420, 2681, 281, 50728, 50728, 1958, 13, 20, 5699, 710, 307, 5044, 813, 420, 2681, 281, 1958, 11, 558, 30, 51048, 51048, 663, 307, 11, 5699, 710, 307, 322, 264, 558, 1922, 295, 341, 10298, 13, 51344, 51344, 400, 2721, 11, 562, 307, 710, 5044, 813, 420, 2681, 281, 1958, 30, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.11220299231039511, "compression_ratio": 1.7304964539007093, "no_speech_prob": 4.0294326026923954e-06}, {"id": 31, "seek": 16380, "start": 171.08, "end": 177.48000000000002, "text": " 0.5 whenever z is greater than or equal to 0, right?", "tokens": [50364, 1042, 510, 311, 264, 4556, 3280, 327, 2445, 670, 510, 11, 293, 370, 290, 295, 710, 307, 5044, 813, 420, 2681, 281, 50728, 50728, 1958, 13, 20, 5699, 710, 307, 5044, 813, 420, 2681, 281, 1958, 11, 558, 30, 51048, 51048, 663, 307, 11, 5699, 710, 307, 322, 264, 558, 1922, 295, 341, 10298, 13, 51344, 51344, 400, 2721, 11, 562, 307, 710, 5044, 813, 420, 2681, 281, 1958, 30, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.11220299231039511, "compression_ratio": 1.7304964539007093, "no_speech_prob": 4.0294326026923954e-06}, {"id": 32, "seek": 16380, "start": 177.48000000000002, "end": 183.4, "text": " That is, whenever z is on the right half of this axis.", "tokens": [50364, 1042, 510, 311, 264, 4556, 3280, 327, 2445, 670, 510, 11, 293, 370, 290, 295, 710, 307, 5044, 813, 420, 2681, 281, 50728, 50728, 1958, 13, 20, 5699, 710, 307, 5044, 813, 420, 2681, 281, 1958, 11, 558, 30, 51048, 51048, 663, 307, 11, 5699, 710, 307, 322, 264, 558, 1922, 295, 341, 10298, 13, 51344, 51344, 400, 2721, 11, 562, 307, 710, 5044, 813, 420, 2681, 281, 1958, 30, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.11220299231039511, "compression_ratio": 1.7304964539007093, "no_speech_prob": 4.0294326026923954e-06}, {"id": 33, "seek": 16380, "start": 183.4, "end": 187.12, "text": " And finally, when is z greater than or equal to 0?", "tokens": [50364, 1042, 510, 311, 264, 4556, 3280, 327, 2445, 670, 510, 11, 293, 370, 290, 295, 710, 307, 5044, 813, 420, 2681, 281, 50728, 50728, 1958, 13, 20, 5699, 710, 307, 5044, 813, 420, 2681, 281, 1958, 11, 558, 30, 51048, 51048, 663, 307, 11, 5699, 710, 307, 322, 264, 558, 1922, 295, 341, 10298, 13, 51344, 51344, 400, 2721, 11, 562, 307, 710, 5044, 813, 420, 2681, 281, 1958, 30, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.11220299231039511, "compression_ratio": 1.7304964539007093, "no_speech_prob": 4.0294326026923954e-06}, {"id": 34, "seek": 18712, "start": 187.12, "end": 196.24, "text": " Well z is equal to w dot x plus b, and so z is greater than or equal to 0 whenever w", "tokens": [50364, 1042, 710, 307, 2681, 281, 261, 5893, 2031, 1804, 272, 11, 293, 370, 710, 307, 5044, 813, 420, 2681, 281, 1958, 5699, 261, 50820, 50820, 5893, 2031, 1804, 272, 307, 5044, 813, 420, 2681, 281, 1958, 13, 51028, 51028, 407, 281, 20928, 11, 437, 291, 600, 1612, 510, 307, 300, 264, 2316, 6069, 82, 502, 5699, 261, 5893, 2031, 1804, 51484, 51484, 272, 307, 5044, 813, 420, 2681, 281, 1958, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.07328849792480469, "compression_ratio": 1.976, "no_speech_prob": 9.223380402545445e-06}, {"id": 35, "seek": 18712, "start": 196.24, "end": 200.4, "text": " dot x plus b is greater than or equal to 0.", "tokens": [50364, 1042, 710, 307, 2681, 281, 261, 5893, 2031, 1804, 272, 11, 293, 370, 710, 307, 5044, 813, 420, 2681, 281, 1958, 5699, 261, 50820, 50820, 5893, 2031, 1804, 272, 307, 5044, 813, 420, 2681, 281, 1958, 13, 51028, 51028, 407, 281, 20928, 11, 437, 291, 600, 1612, 510, 307, 300, 264, 2316, 6069, 82, 502, 5699, 261, 5893, 2031, 1804, 51484, 51484, 272, 307, 5044, 813, 420, 2681, 281, 1958, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.07328849792480469, "compression_ratio": 1.976, "no_speech_prob": 9.223380402545445e-06}, {"id": 36, "seek": 18712, "start": 200.4, "end": 209.52, "text": " So to recap, what you've seen here is that the model predicts 1 whenever w dot x plus", "tokens": [50364, 1042, 710, 307, 2681, 281, 261, 5893, 2031, 1804, 272, 11, 293, 370, 710, 307, 5044, 813, 420, 2681, 281, 1958, 5699, 261, 50820, 50820, 5893, 2031, 1804, 272, 307, 5044, 813, 420, 2681, 281, 1958, 13, 51028, 51028, 407, 281, 20928, 11, 437, 291, 600, 1612, 510, 307, 300, 264, 2316, 6069, 82, 502, 5699, 261, 5893, 2031, 1804, 51484, 51484, 272, 307, 5044, 813, 420, 2681, 281, 1958, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.07328849792480469, "compression_ratio": 1.976, "no_speech_prob": 9.223380402545445e-06}, {"id": 37, "seek": 18712, "start": 209.52, "end": 213.36, "text": " b is greater than or equal to 0.", "tokens": [50364, 1042, 710, 307, 2681, 281, 261, 5893, 2031, 1804, 272, 11, 293, 370, 710, 307, 5044, 813, 420, 2681, 281, 1958, 5699, 261, 50820, 50820, 5893, 2031, 1804, 272, 307, 5044, 813, 420, 2681, 281, 1958, 13, 51028, 51028, 407, 281, 20928, 11, 437, 291, 600, 1612, 510, 307, 300, 264, 2316, 6069, 82, 502, 5699, 261, 5893, 2031, 1804, 51484, 51484, 272, 307, 5044, 813, 420, 2681, 281, 1958, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.07328849792480469, "compression_ratio": 1.976, "no_speech_prob": 9.223380402545445e-06}, {"id": 38, "seek": 21336, "start": 213.36, "end": 223.36, "text": " And conversely, when w dot x plus b is less than 0, the algorithm predicts y is 0.", "tokens": [50364, 400, 2615, 736, 11, 562, 261, 5893, 2031, 1804, 272, 307, 1570, 813, 1958, 11, 264, 9284, 6069, 82, 288, 307, 1958, 13, 50864, 50864, 407, 2212, 341, 11, 718, 311, 586, 23273, 577, 264, 2316, 1669, 21264, 13, 51174, 51174, 286, 478, 516, 281, 747, 364, 1365, 295, 257, 21538, 1154, 689, 291, 362, 732, 4122, 11, 2031, 16, 293, 51496, 51496, 2031, 17, 2602, 295, 445, 472, 4111, 13, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.10737258911132813, "compression_ratio": 1.4248704663212435, "no_speech_prob": 3.0415715173148783e-06}, {"id": 39, "seek": 21336, "start": 223.36, "end": 229.56, "text": " So given this, let's now visualize how the model makes predictions.", "tokens": [50364, 400, 2615, 736, 11, 562, 261, 5893, 2031, 1804, 272, 307, 1570, 813, 1958, 11, 264, 9284, 6069, 82, 288, 307, 1958, 13, 50864, 50864, 407, 2212, 341, 11, 718, 311, 586, 23273, 577, 264, 2316, 1669, 21264, 13, 51174, 51174, 286, 478, 516, 281, 747, 364, 1365, 295, 257, 21538, 1154, 689, 291, 362, 732, 4122, 11, 2031, 16, 293, 51496, 51496, 2031, 17, 2602, 295, 445, 472, 4111, 13, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.10737258911132813, "compression_ratio": 1.4248704663212435, "no_speech_prob": 3.0415715173148783e-06}, {"id": 40, "seek": 21336, "start": 229.56, "end": 236.0, "text": " I'm going to take an example of a classification problem where you have two features, x1 and", "tokens": [50364, 400, 2615, 736, 11, 562, 261, 5893, 2031, 1804, 272, 307, 1570, 813, 1958, 11, 264, 9284, 6069, 82, 288, 307, 1958, 13, 50864, 50864, 407, 2212, 341, 11, 718, 311, 586, 23273, 577, 264, 2316, 1669, 21264, 13, 51174, 51174, 286, 478, 516, 281, 747, 364, 1365, 295, 257, 21538, 1154, 689, 291, 362, 732, 4122, 11, 2031, 16, 293, 51496, 51496, 2031, 17, 2602, 295, 445, 472, 4111, 13, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.10737258911132813, "compression_ratio": 1.4248704663212435, "no_speech_prob": 3.0415715173148783e-06}, {"id": 41, "seek": 21336, "start": 236.0, "end": 239.28000000000003, "text": " x2 instead of just one feature.", "tokens": [50364, 400, 2615, 736, 11, 562, 261, 5893, 2031, 1804, 272, 307, 1570, 813, 1958, 11, 264, 9284, 6069, 82, 288, 307, 1958, 13, 50864, 50864, 407, 2212, 341, 11, 718, 311, 586, 23273, 577, 264, 2316, 1669, 21264, 13, 51174, 51174, 286, 478, 516, 281, 747, 364, 1365, 295, 257, 21538, 1154, 689, 291, 362, 732, 4122, 11, 2031, 16, 293, 51496, 51496, 2031, 17, 2602, 295, 445, 472, 4111, 13, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.10737258911132813, "compression_ratio": 1.4248704663212435, "no_speech_prob": 3.0415715173148783e-06}, {"id": 42, "seek": 23928, "start": 239.28, "end": 245.08, "text": " Here's a training set where the little red crosses denote the positive examples, and", "tokens": [50364, 1692, 311, 257, 3097, 992, 689, 264, 707, 2182, 28467, 45708, 264, 3353, 5110, 11, 293, 50654, 50654, 264, 707, 3344, 13040, 45708, 3671, 5110, 13, 50830, 50830, 407, 264, 2182, 28467, 23249, 281, 288, 6915, 502, 11, 293, 264, 3344, 13040, 6805, 281, 288, 6915, 51258, 51258, 1958, 13, 51362, 51362, 407, 264, 3565, 3142, 24590, 2316, 486, 652, 21264, 1228, 341, 2445, 11, 283, 295, 2031, 6915, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.145222494046982, "compression_ratio": 1.8114285714285714, "no_speech_prob": 6.4389719227619935e-06}, {"id": 43, "seek": 23928, "start": 245.08, "end": 248.6, "text": " the little blue circles denote negative examples.", "tokens": [50364, 1692, 311, 257, 3097, 992, 689, 264, 707, 2182, 28467, 45708, 264, 3353, 5110, 11, 293, 50654, 50654, 264, 707, 3344, 13040, 45708, 3671, 5110, 13, 50830, 50830, 407, 264, 2182, 28467, 23249, 281, 288, 6915, 502, 11, 293, 264, 3344, 13040, 6805, 281, 288, 6915, 51258, 51258, 1958, 13, 51362, 51362, 407, 264, 3565, 3142, 24590, 2316, 486, 652, 21264, 1228, 341, 2445, 11, 283, 295, 2031, 6915, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.145222494046982, "compression_ratio": 1.8114285714285714, "no_speech_prob": 6.4389719227619935e-06}, {"id": 44, "seek": 23928, "start": 248.6, "end": 257.16, "text": " So the red crosses corresponds to y equals 1, and the blue circles correspond to y equals", "tokens": [50364, 1692, 311, 257, 3097, 992, 689, 264, 707, 2182, 28467, 45708, 264, 3353, 5110, 11, 293, 50654, 50654, 264, 707, 3344, 13040, 45708, 3671, 5110, 13, 50830, 50830, 407, 264, 2182, 28467, 23249, 281, 288, 6915, 502, 11, 293, 264, 3344, 13040, 6805, 281, 288, 6915, 51258, 51258, 1958, 13, 51362, 51362, 407, 264, 3565, 3142, 24590, 2316, 486, 652, 21264, 1228, 341, 2445, 11, 283, 295, 2031, 6915, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.145222494046982, "compression_ratio": 1.8114285714285714, "no_speech_prob": 6.4389719227619935e-06}, {"id": 45, "seek": 23928, "start": 257.16, "end": 259.24, "text": " 0.", "tokens": [50364, 1692, 311, 257, 3097, 992, 689, 264, 707, 2182, 28467, 45708, 264, 3353, 5110, 11, 293, 50654, 50654, 264, 707, 3344, 13040, 45708, 3671, 5110, 13, 50830, 50830, 407, 264, 2182, 28467, 23249, 281, 288, 6915, 502, 11, 293, 264, 3344, 13040, 6805, 281, 288, 6915, 51258, 51258, 1958, 13, 51362, 51362, 407, 264, 3565, 3142, 24590, 2316, 486, 652, 21264, 1228, 341, 2445, 11, 283, 295, 2031, 6915, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.145222494046982, "compression_ratio": 1.8114285714285714, "no_speech_prob": 6.4389719227619935e-06}, {"id": 46, "seek": 23928, "start": 259.24, "end": 265.32, "text": " So the logistic regression model will make predictions using this function, f of x equals", "tokens": [50364, 1692, 311, 257, 3097, 992, 689, 264, 707, 2182, 28467, 45708, 264, 3353, 5110, 11, 293, 50654, 50654, 264, 707, 3344, 13040, 45708, 3671, 5110, 13, 50830, 50830, 407, 264, 2182, 28467, 23249, 281, 288, 6915, 502, 11, 293, 264, 3344, 13040, 6805, 281, 288, 6915, 51258, 51258, 1958, 13, 51362, 51362, 407, 264, 3565, 3142, 24590, 2316, 486, 652, 21264, 1228, 341, 2445, 11, 283, 295, 2031, 6915, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.145222494046982, "compression_ratio": 1.8114285714285714, "no_speech_prob": 6.4389719227619935e-06}, {"id": 47, "seek": 26532, "start": 265.32, "end": 275.2, "text": " g of z, where z is now this expression over here, w1 x1 plus w2 x2 plus b, because we", "tokens": [50364, 290, 295, 710, 11, 689, 710, 307, 586, 341, 6114, 670, 510, 11, 261, 16, 2031, 16, 1804, 261, 17, 2031, 17, 1804, 272, 11, 570, 321, 50858, 50858, 362, 732, 4122, 11, 2031, 16, 293, 2031, 17, 13, 51022, 51022, 400, 718, 311, 445, 584, 337, 341, 1365, 300, 264, 2158, 295, 264, 9834, 366, 261, 16, 6915, 502, 11, 261, 17, 51424, 51424, 6915, 502, 11, 293, 272, 6915, 3671, 805, 13, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.11636207042596279, "compression_ratio": 1.4753086419753085, "no_speech_prob": 2.813002993207192e-06}, {"id": 48, "seek": 26532, "start": 275.2, "end": 278.48, "text": " have two features, x1 and x2.", "tokens": [50364, 290, 295, 710, 11, 689, 710, 307, 586, 341, 6114, 670, 510, 11, 261, 16, 2031, 16, 1804, 261, 17, 2031, 17, 1804, 272, 11, 570, 321, 50858, 50858, 362, 732, 4122, 11, 2031, 16, 293, 2031, 17, 13, 51022, 51022, 400, 718, 311, 445, 584, 337, 341, 1365, 300, 264, 2158, 295, 264, 9834, 366, 261, 16, 6915, 502, 11, 261, 17, 51424, 51424, 6915, 502, 11, 293, 272, 6915, 3671, 805, 13, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.11636207042596279, "compression_ratio": 1.4753086419753085, "no_speech_prob": 2.813002993207192e-06}, {"id": 49, "seek": 26532, "start": 278.48, "end": 286.52, "text": " And let's just say for this example that the value of the parameters are w1 equals 1, w2", "tokens": [50364, 290, 295, 710, 11, 689, 710, 307, 586, 341, 6114, 670, 510, 11, 261, 16, 2031, 16, 1804, 261, 17, 2031, 17, 1804, 272, 11, 570, 321, 50858, 50858, 362, 732, 4122, 11, 2031, 16, 293, 2031, 17, 13, 51022, 51022, 400, 718, 311, 445, 584, 337, 341, 1365, 300, 264, 2158, 295, 264, 9834, 366, 261, 16, 6915, 502, 11, 261, 17, 51424, 51424, 6915, 502, 11, 293, 272, 6915, 3671, 805, 13, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.11636207042596279, "compression_ratio": 1.4753086419753085, "no_speech_prob": 2.813002993207192e-06}, {"id": 50, "seek": 26532, "start": 286.52, "end": 291.0, "text": " equals 1, and b equals negative 3.", "tokens": [50364, 290, 295, 710, 11, 689, 710, 307, 586, 341, 6114, 670, 510, 11, 261, 16, 2031, 16, 1804, 261, 17, 2031, 17, 1804, 272, 11, 570, 321, 50858, 50858, 362, 732, 4122, 11, 2031, 16, 293, 2031, 17, 13, 51022, 51022, 400, 718, 311, 445, 584, 337, 341, 1365, 300, 264, 2158, 295, 264, 9834, 366, 261, 16, 6915, 502, 11, 261, 17, 51424, 51424, 6915, 502, 11, 293, 272, 6915, 3671, 805, 13, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.11636207042596279, "compression_ratio": 1.4753086419753085, "no_speech_prob": 2.813002993207192e-06}, {"id": 51, "seek": 29100, "start": 291.0, "end": 295.88, "text": " And let's now take a look at how logistic regression makes predictions.", "tokens": [50364, 400, 718, 311, 586, 747, 257, 574, 412, 577, 3565, 3142, 24590, 1669, 21264, 13, 50608, 50608, 682, 1729, 11, 718, 311, 2573, 484, 562, 261, 87, 1804, 272, 307, 5044, 813, 420, 2681, 281, 1958, 11, 293, 562, 50878, 50878, 261, 87, 1804, 272, 307, 1570, 813, 1958, 13, 51054, 51054, 1407, 2573, 300, 484, 11, 456, 311, 257, 588, 1880, 1622, 281, 574, 412, 11, 597, 307, 562, 261, 87, 1804, 272, 307, 51368, 51368, 2293, 2681, 281, 1958, 13, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.06950532558352449, "compression_ratio": 1.6298342541436464, "no_speech_prob": 3.72661179426359e-06}, {"id": 52, "seek": 29100, "start": 295.88, "end": 301.28, "text": " In particular, let's figure out when wx plus b is greater than or equal to 0, and when", "tokens": [50364, 400, 718, 311, 586, 747, 257, 574, 412, 577, 3565, 3142, 24590, 1669, 21264, 13, 50608, 50608, 682, 1729, 11, 718, 311, 2573, 484, 562, 261, 87, 1804, 272, 307, 5044, 813, 420, 2681, 281, 1958, 11, 293, 562, 50878, 50878, 261, 87, 1804, 272, 307, 1570, 813, 1958, 13, 51054, 51054, 1407, 2573, 300, 484, 11, 456, 311, 257, 588, 1880, 1622, 281, 574, 412, 11, 597, 307, 562, 261, 87, 1804, 272, 307, 51368, 51368, 2293, 2681, 281, 1958, 13, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.06950532558352449, "compression_ratio": 1.6298342541436464, "no_speech_prob": 3.72661179426359e-06}, {"id": 53, "seek": 29100, "start": 301.28, "end": 304.8, "text": " wx plus b is less than 0.", "tokens": [50364, 400, 718, 311, 586, 747, 257, 574, 412, 577, 3565, 3142, 24590, 1669, 21264, 13, 50608, 50608, 682, 1729, 11, 718, 311, 2573, 484, 562, 261, 87, 1804, 272, 307, 5044, 813, 420, 2681, 281, 1958, 11, 293, 562, 50878, 50878, 261, 87, 1804, 272, 307, 1570, 813, 1958, 13, 51054, 51054, 1407, 2573, 300, 484, 11, 456, 311, 257, 588, 1880, 1622, 281, 574, 412, 11, 597, 307, 562, 261, 87, 1804, 272, 307, 51368, 51368, 2293, 2681, 281, 1958, 13, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.06950532558352449, "compression_ratio": 1.6298342541436464, "no_speech_prob": 3.72661179426359e-06}, {"id": 54, "seek": 29100, "start": 304.8, "end": 311.08, "text": " To figure that out, there's a very interesting line to look at, which is when wx plus b is", "tokens": [50364, 400, 718, 311, 586, 747, 257, 574, 412, 577, 3565, 3142, 24590, 1669, 21264, 13, 50608, 50608, 682, 1729, 11, 718, 311, 2573, 484, 562, 261, 87, 1804, 272, 307, 5044, 813, 420, 2681, 281, 1958, 11, 293, 562, 50878, 50878, 261, 87, 1804, 272, 307, 1570, 813, 1958, 13, 51054, 51054, 1407, 2573, 300, 484, 11, 456, 311, 257, 588, 1880, 1622, 281, 574, 412, 11, 597, 307, 562, 261, 87, 1804, 272, 307, 51368, 51368, 2293, 2681, 281, 1958, 13, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.06950532558352449, "compression_ratio": 1.6298342541436464, "no_speech_prob": 3.72661179426359e-06}, {"id": 55, "seek": 29100, "start": 311.08, "end": 314.2, "text": " exactly equal to 0.", "tokens": [50364, 400, 718, 311, 586, 747, 257, 574, 412, 577, 3565, 3142, 24590, 1669, 21264, 13, 50608, 50608, 682, 1729, 11, 718, 311, 2573, 484, 562, 261, 87, 1804, 272, 307, 5044, 813, 420, 2681, 281, 1958, 11, 293, 562, 50878, 50878, 261, 87, 1804, 272, 307, 1570, 813, 1958, 13, 51054, 51054, 1407, 2573, 300, 484, 11, 456, 311, 257, 588, 1880, 1622, 281, 574, 412, 11, 597, 307, 562, 261, 87, 1804, 272, 307, 51368, 51368, 2293, 2681, 281, 1958, 13, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.06950532558352449, "compression_ratio": 1.6298342541436464, "no_speech_prob": 3.72661179426359e-06}, {"id": 56, "seek": 31420, "start": 314.2, "end": 321.08, "text": " It turns out that this line is also called the decision boundary, because that's the", "tokens": [50364, 467, 4523, 484, 300, 341, 1622, 307, 611, 1219, 264, 3537, 12866, 11, 570, 300, 311, 264, 50708, 50708, 1622, 689, 291, 434, 445, 1920, 10598, 466, 1968, 288, 307, 1958, 420, 288, 307, 502, 13, 51034, 51034, 823, 337, 264, 4190, 295, 264, 9834, 261, 16, 11, 261, 17, 11, 293, 272, 300, 321, 632, 3720, 760, 3673, 11, 341, 51476, 51476, 3537, 12866, 307, 445, 2031, 16, 1804, 2031, 17, 3175, 805, 13, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.08304454103300843, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.3320626496570185e-06}, {"id": 57, "seek": 31420, "start": 321.08, "end": 327.59999999999997, "text": " line where you're just almost neutral about whether y is 0 or y is 1.", "tokens": [50364, 467, 4523, 484, 300, 341, 1622, 307, 611, 1219, 264, 3537, 12866, 11, 570, 300, 311, 264, 50708, 50708, 1622, 689, 291, 434, 445, 1920, 10598, 466, 1968, 288, 307, 1958, 420, 288, 307, 502, 13, 51034, 51034, 823, 337, 264, 4190, 295, 264, 9834, 261, 16, 11, 261, 17, 11, 293, 272, 300, 321, 632, 3720, 760, 3673, 11, 341, 51476, 51476, 3537, 12866, 307, 445, 2031, 16, 1804, 2031, 17, 3175, 805, 13, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.08304454103300843, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.3320626496570185e-06}, {"id": 58, "seek": 31420, "start": 327.59999999999997, "end": 336.44, "text": " Now for the values of the parameters w1, w2, and b that we had written down above, this", "tokens": [50364, 467, 4523, 484, 300, 341, 1622, 307, 611, 1219, 264, 3537, 12866, 11, 570, 300, 311, 264, 50708, 50708, 1622, 689, 291, 434, 445, 1920, 10598, 466, 1968, 288, 307, 1958, 420, 288, 307, 502, 13, 51034, 51034, 823, 337, 264, 4190, 295, 264, 9834, 261, 16, 11, 261, 17, 11, 293, 272, 300, 321, 632, 3720, 760, 3673, 11, 341, 51476, 51476, 3537, 12866, 307, 445, 2031, 16, 1804, 2031, 17, 3175, 805, 13, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.08304454103300843, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.3320626496570185e-06}, {"id": 59, "seek": 31420, "start": 336.44, "end": 343.59999999999997, "text": " decision boundary is just x1 plus x2 minus 3.", "tokens": [50364, 467, 4523, 484, 300, 341, 1622, 307, 611, 1219, 264, 3537, 12866, 11, 570, 300, 311, 264, 50708, 50708, 1622, 689, 291, 434, 445, 1920, 10598, 466, 1968, 288, 307, 1958, 420, 288, 307, 502, 13, 51034, 51034, 823, 337, 264, 4190, 295, 264, 9834, 261, 16, 11, 261, 17, 11, 293, 272, 300, 321, 632, 3720, 760, 3673, 11, 341, 51476, 51476, 3537, 12866, 307, 445, 2031, 16, 1804, 2031, 17, 3175, 805, 13, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.08304454103300843, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.3320626496570185e-06}, {"id": 60, "seek": 34360, "start": 343.6, "end": 350.12, "text": " And so when is x1 plus x2 minus 3 equal to 0?", "tokens": [50364, 400, 370, 562, 307, 2031, 16, 1804, 2031, 17, 3175, 805, 2681, 281, 1958, 30, 50690, 50690, 1042, 300, 486, 6805, 281, 264, 1622, 2031, 16, 1804, 2031, 17, 6915, 805, 13, 51022, 51022, 400, 300, 307, 341, 1622, 4898, 670, 510, 13, 51272, 51272, 400, 370, 341, 1622, 4523, 484, 281, 312, 264, 3537, 12866, 11, 689, 498, 264, 4122, 2031, 366, 281, 264, 51647, 51647], "temperature": 0.0, "avg_logprob": -0.11087622849837593, "compression_ratio": 1.5294117647058822, "no_speech_prob": 4.565927611110965e-06}, {"id": 61, "seek": 34360, "start": 350.12, "end": 356.76000000000005, "text": " Well that will correspond to the line x1 plus x2 equals 3.", "tokens": [50364, 400, 370, 562, 307, 2031, 16, 1804, 2031, 17, 3175, 805, 2681, 281, 1958, 30, 50690, 50690, 1042, 300, 486, 6805, 281, 264, 1622, 2031, 16, 1804, 2031, 17, 6915, 805, 13, 51022, 51022, 400, 300, 307, 341, 1622, 4898, 670, 510, 13, 51272, 51272, 400, 370, 341, 1622, 4523, 484, 281, 312, 264, 3537, 12866, 11, 689, 498, 264, 4122, 2031, 366, 281, 264, 51647, 51647], "temperature": 0.0, "avg_logprob": -0.11087622849837593, "compression_ratio": 1.5294117647058822, "no_speech_prob": 4.565927611110965e-06}, {"id": 62, "seek": 34360, "start": 356.76000000000005, "end": 361.76000000000005, "text": " And that is this line shown over here.", "tokens": [50364, 400, 370, 562, 307, 2031, 16, 1804, 2031, 17, 3175, 805, 2681, 281, 1958, 30, 50690, 50690, 1042, 300, 486, 6805, 281, 264, 1622, 2031, 16, 1804, 2031, 17, 6915, 805, 13, 51022, 51022, 400, 300, 307, 341, 1622, 4898, 670, 510, 13, 51272, 51272, 400, 370, 341, 1622, 4523, 484, 281, 312, 264, 3537, 12866, 11, 689, 498, 264, 4122, 2031, 366, 281, 264, 51647, 51647], "temperature": 0.0, "avg_logprob": -0.11087622849837593, "compression_ratio": 1.5294117647058822, "no_speech_prob": 4.565927611110965e-06}, {"id": 63, "seek": 34360, "start": 361.76000000000005, "end": 369.26000000000005, "text": " And so this line turns out to be the decision boundary, where if the features x are to the", "tokens": [50364, 400, 370, 562, 307, 2031, 16, 1804, 2031, 17, 3175, 805, 2681, 281, 1958, 30, 50690, 50690, 1042, 300, 486, 6805, 281, 264, 1622, 2031, 16, 1804, 2031, 17, 6915, 805, 13, 51022, 51022, 400, 300, 307, 341, 1622, 4898, 670, 510, 13, 51272, 51272, 400, 370, 341, 1622, 4523, 484, 281, 312, 264, 3537, 12866, 11, 689, 498, 264, 4122, 2031, 366, 281, 264, 51647, 51647], "temperature": 0.0, "avg_logprob": -0.11087622849837593, "compression_ratio": 1.5294117647058822, "no_speech_prob": 4.565927611110965e-06}, {"id": 64, "seek": 36926, "start": 369.26, "end": 375.96, "text": " right of this line, logistic regression would predict 1, and to the left of this line, logistic", "tokens": [50364, 558, 295, 341, 1622, 11, 3565, 3142, 24590, 576, 6069, 502, 11, 293, 281, 264, 1411, 295, 341, 1622, 11, 3565, 3142, 50699, 50699, 24590, 576, 6069, 1958, 13, 50897, 50897, 682, 661, 2283, 11, 437, 321, 362, 445, 5056, 1602, 307, 264, 3537, 12866, 337, 3565, 3142, 24590, 51245, 51245, 562, 264, 9834, 261, 16, 11, 261, 17, 11, 293, 272, 366, 502, 11, 502, 11, 293, 3671, 805, 13, 51550, 51550, 2720, 1164, 11, 498, 291, 632, 257, 819, 3922, 295, 264, 9834, 11, 264, 3537, 12866, 576, 51771, 51771], "temperature": 0.0, "avg_logprob": -0.08985643183931391, "compression_ratio": 1.8911917098445596, "no_speech_prob": 3.611948613979621e-06}, {"id": 65, "seek": 36926, "start": 375.96, "end": 379.92, "text": " regression would predict 0.", "tokens": [50364, 558, 295, 341, 1622, 11, 3565, 3142, 24590, 576, 6069, 502, 11, 293, 281, 264, 1411, 295, 341, 1622, 11, 3565, 3142, 50699, 50699, 24590, 576, 6069, 1958, 13, 50897, 50897, 682, 661, 2283, 11, 437, 321, 362, 445, 5056, 1602, 307, 264, 3537, 12866, 337, 3565, 3142, 24590, 51245, 51245, 562, 264, 9834, 261, 16, 11, 261, 17, 11, 293, 272, 366, 502, 11, 502, 11, 293, 3671, 805, 13, 51550, 51550, 2720, 1164, 11, 498, 291, 632, 257, 819, 3922, 295, 264, 9834, 11, 264, 3537, 12866, 576, 51771, 51771], "temperature": 0.0, "avg_logprob": -0.08985643183931391, "compression_ratio": 1.8911917098445596, "no_speech_prob": 3.611948613979621e-06}, {"id": 66, "seek": 36926, "start": 379.92, "end": 386.88, "text": " In other words, what we have just visualized is the decision boundary for logistic regression", "tokens": [50364, 558, 295, 341, 1622, 11, 3565, 3142, 24590, 576, 6069, 502, 11, 293, 281, 264, 1411, 295, 341, 1622, 11, 3565, 3142, 50699, 50699, 24590, 576, 6069, 1958, 13, 50897, 50897, 682, 661, 2283, 11, 437, 321, 362, 445, 5056, 1602, 307, 264, 3537, 12866, 337, 3565, 3142, 24590, 51245, 51245, 562, 264, 9834, 261, 16, 11, 261, 17, 11, 293, 272, 366, 502, 11, 502, 11, 293, 3671, 805, 13, 51550, 51550, 2720, 1164, 11, 498, 291, 632, 257, 819, 3922, 295, 264, 9834, 11, 264, 3537, 12866, 576, 51771, 51771], "temperature": 0.0, "avg_logprob": -0.08985643183931391, "compression_ratio": 1.8911917098445596, "no_speech_prob": 3.611948613979621e-06}, {"id": 67, "seek": 36926, "start": 386.88, "end": 392.98, "text": " when the parameters w1, w2, and b are 1, 1, and negative 3.", "tokens": [50364, 558, 295, 341, 1622, 11, 3565, 3142, 24590, 576, 6069, 502, 11, 293, 281, 264, 1411, 295, 341, 1622, 11, 3565, 3142, 50699, 50699, 24590, 576, 6069, 1958, 13, 50897, 50897, 682, 661, 2283, 11, 437, 321, 362, 445, 5056, 1602, 307, 264, 3537, 12866, 337, 3565, 3142, 24590, 51245, 51245, 562, 264, 9834, 261, 16, 11, 261, 17, 11, 293, 272, 366, 502, 11, 502, 11, 293, 3671, 805, 13, 51550, 51550, 2720, 1164, 11, 498, 291, 632, 257, 819, 3922, 295, 264, 9834, 11, 264, 3537, 12866, 576, 51771, 51771], "temperature": 0.0, "avg_logprob": -0.08985643183931391, "compression_ratio": 1.8911917098445596, "no_speech_prob": 3.611948613979621e-06}, {"id": 68, "seek": 36926, "start": 392.98, "end": 397.4, "text": " Of course, if you had a different choice of the parameters, the decision boundary would", "tokens": [50364, 558, 295, 341, 1622, 11, 3565, 3142, 24590, 576, 6069, 502, 11, 293, 281, 264, 1411, 295, 341, 1622, 11, 3565, 3142, 50699, 50699, 24590, 576, 6069, 1958, 13, 50897, 50897, 682, 661, 2283, 11, 437, 321, 362, 445, 5056, 1602, 307, 264, 3537, 12866, 337, 3565, 3142, 24590, 51245, 51245, 562, 264, 9834, 261, 16, 11, 261, 17, 11, 293, 272, 366, 502, 11, 502, 11, 293, 3671, 805, 13, 51550, 51550, 2720, 1164, 11, 498, 291, 632, 257, 819, 3922, 295, 264, 9834, 11, 264, 3537, 12866, 576, 51771, 51771], "temperature": 0.0, "avg_logprob": -0.08985643183931391, "compression_ratio": 1.8911917098445596, "no_speech_prob": 3.611948613979621e-06}, {"id": 69, "seek": 39740, "start": 397.4, "end": 399.76, "text": " be a different line.", "tokens": [50364, 312, 257, 819, 1622, 13, 50482, 50482, 823, 718, 311, 574, 412, 257, 544, 3997, 1365, 689, 264, 3537, 12866, 307, 572, 2854, 257, 2997, 50738, 50738, 1622, 13, 50810, 50810, 1018, 949, 11, 28467, 45708, 264, 1508, 288, 6915, 502, 11, 293, 264, 707, 13040, 45708, 264, 1508, 51196, 51196, 288, 6915, 1958, 13, 51386, 51386, 24552, 1036, 1243, 11, 291, 1866, 577, 281, 764, 22560, 12356, 294, 8213, 24590, 11, 293, 291, 393, 360, 264, 912, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.12750560481373857, "compression_ratio": 1.598984771573604, "no_speech_prob": 1.191102342090744e-06}, {"id": 70, "seek": 39740, "start": 399.76, "end": 404.88, "text": " Now let's look at a more complex example where the decision boundary is no longer a straight", "tokens": [50364, 312, 257, 819, 1622, 13, 50482, 50482, 823, 718, 311, 574, 412, 257, 544, 3997, 1365, 689, 264, 3537, 12866, 307, 572, 2854, 257, 2997, 50738, 50738, 1622, 13, 50810, 50810, 1018, 949, 11, 28467, 45708, 264, 1508, 288, 6915, 502, 11, 293, 264, 707, 13040, 45708, 264, 1508, 51196, 51196, 288, 6915, 1958, 13, 51386, 51386, 24552, 1036, 1243, 11, 291, 1866, 577, 281, 764, 22560, 12356, 294, 8213, 24590, 11, 293, 291, 393, 360, 264, 912, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.12750560481373857, "compression_ratio": 1.598984771573604, "no_speech_prob": 1.191102342090744e-06}, {"id": 71, "seek": 39740, "start": 404.88, "end": 406.32, "text": " line.", "tokens": [50364, 312, 257, 819, 1622, 13, 50482, 50482, 823, 718, 311, 574, 412, 257, 544, 3997, 1365, 689, 264, 3537, 12866, 307, 572, 2854, 257, 2997, 50738, 50738, 1622, 13, 50810, 50810, 1018, 949, 11, 28467, 45708, 264, 1508, 288, 6915, 502, 11, 293, 264, 707, 13040, 45708, 264, 1508, 51196, 51196, 288, 6915, 1958, 13, 51386, 51386, 24552, 1036, 1243, 11, 291, 1866, 577, 281, 764, 22560, 12356, 294, 8213, 24590, 11, 293, 291, 393, 360, 264, 912, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.12750560481373857, "compression_ratio": 1.598984771573604, "no_speech_prob": 1.191102342090744e-06}, {"id": 72, "seek": 39740, "start": 406.32, "end": 414.03999999999996, "text": " As before, crosses denote the class y equals 1, and the little circles denote the class", "tokens": [50364, 312, 257, 819, 1622, 13, 50482, 50482, 823, 718, 311, 574, 412, 257, 544, 3997, 1365, 689, 264, 3537, 12866, 307, 572, 2854, 257, 2997, 50738, 50738, 1622, 13, 50810, 50810, 1018, 949, 11, 28467, 45708, 264, 1508, 288, 6915, 502, 11, 293, 264, 707, 13040, 45708, 264, 1508, 51196, 51196, 288, 6915, 1958, 13, 51386, 51386, 24552, 1036, 1243, 11, 291, 1866, 577, 281, 764, 22560, 12356, 294, 8213, 24590, 11, 293, 291, 393, 360, 264, 912, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.12750560481373857, "compression_ratio": 1.598984771573604, "no_speech_prob": 1.191102342090744e-06}, {"id": 73, "seek": 39740, "start": 414.03999999999996, "end": 417.84, "text": " y equals 0.", "tokens": [50364, 312, 257, 819, 1622, 13, 50482, 50482, 823, 718, 311, 574, 412, 257, 544, 3997, 1365, 689, 264, 3537, 12866, 307, 572, 2854, 257, 2997, 50738, 50738, 1622, 13, 50810, 50810, 1018, 949, 11, 28467, 45708, 264, 1508, 288, 6915, 502, 11, 293, 264, 707, 13040, 45708, 264, 1508, 51196, 51196, 288, 6915, 1958, 13, 51386, 51386, 24552, 1036, 1243, 11, 291, 1866, 577, 281, 764, 22560, 12356, 294, 8213, 24590, 11, 293, 291, 393, 360, 264, 912, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.12750560481373857, "compression_ratio": 1.598984771573604, "no_speech_prob": 1.191102342090744e-06}, {"id": 74, "seek": 39740, "start": 417.84, "end": 424.71999999999997, "text": " Earlier last week, you saw how to use polynomials in linear regression, and you can do the same", "tokens": [50364, 312, 257, 819, 1622, 13, 50482, 50482, 823, 718, 311, 574, 412, 257, 544, 3997, 1365, 689, 264, 3537, 12866, 307, 572, 2854, 257, 2997, 50738, 50738, 1622, 13, 50810, 50810, 1018, 949, 11, 28467, 45708, 264, 1508, 288, 6915, 502, 11, 293, 264, 707, 13040, 45708, 264, 1508, 51196, 51196, 288, 6915, 1958, 13, 51386, 51386, 24552, 1036, 1243, 11, 291, 1866, 577, 281, 764, 22560, 12356, 294, 8213, 24590, 11, 293, 291, 393, 360, 264, 912, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.12750560481373857, "compression_ratio": 1.598984771573604, "no_speech_prob": 1.191102342090744e-06}, {"id": 75, "seek": 42472, "start": 424.72, "end": 427.58000000000004, "text": " in logistic regression.", "tokens": [50364, 294, 3565, 3142, 24590, 13, 50507, 50507, 407, 718, 311, 992, 710, 281, 312, 261, 16, 2031, 16, 8889, 1804, 261, 17, 2031, 17, 8889, 1804, 272, 13, 50987, 50987, 2022, 341, 3922, 295, 4122, 11, 26110, 4122, 666, 3565, 3142, 24590, 11, 370, 283, 295, 2031, 11, 597, 51249, 51249, 6915, 290, 295, 710, 11, 307, 586, 290, 295, 341, 6114, 670, 510, 13, 51502, 51502], "temperature": 0.0, "avg_logprob": -0.13722248353819916, "compression_ratio": 1.5364238410596027, "no_speech_prob": 2.3687896373303374e-06}, {"id": 76, "seek": 42472, "start": 427.58000000000004, "end": 437.18, "text": " So let's set z to be w1 x1 squared plus w2 x2 squared plus b.", "tokens": [50364, 294, 3565, 3142, 24590, 13, 50507, 50507, 407, 718, 311, 992, 710, 281, 312, 261, 16, 2031, 16, 8889, 1804, 261, 17, 2031, 17, 8889, 1804, 272, 13, 50987, 50987, 2022, 341, 3922, 295, 4122, 11, 26110, 4122, 666, 3565, 3142, 24590, 11, 370, 283, 295, 2031, 11, 597, 51249, 51249, 6915, 290, 295, 710, 11, 307, 586, 290, 295, 341, 6114, 670, 510, 13, 51502, 51502], "temperature": 0.0, "avg_logprob": -0.13722248353819916, "compression_ratio": 1.5364238410596027, "no_speech_prob": 2.3687896373303374e-06}, {"id": 77, "seek": 42472, "start": 437.18, "end": 442.42, "text": " With this choice of features, polynomial features into logistic regression, so f of x, which", "tokens": [50364, 294, 3565, 3142, 24590, 13, 50507, 50507, 407, 718, 311, 992, 710, 281, 312, 261, 16, 2031, 16, 8889, 1804, 261, 17, 2031, 17, 8889, 1804, 272, 13, 50987, 50987, 2022, 341, 3922, 295, 4122, 11, 26110, 4122, 666, 3565, 3142, 24590, 11, 370, 283, 295, 2031, 11, 597, 51249, 51249, 6915, 290, 295, 710, 11, 307, 586, 290, 295, 341, 6114, 670, 510, 13, 51502, 51502], "temperature": 0.0, "avg_logprob": -0.13722248353819916, "compression_ratio": 1.5364238410596027, "no_speech_prob": 2.3687896373303374e-06}, {"id": 78, "seek": 42472, "start": 442.42, "end": 447.48, "text": " equals g of z, is now g of this expression over here.", "tokens": [50364, 294, 3565, 3142, 24590, 13, 50507, 50507, 407, 718, 311, 992, 710, 281, 312, 261, 16, 2031, 16, 8889, 1804, 261, 17, 2031, 17, 8889, 1804, 272, 13, 50987, 50987, 2022, 341, 3922, 295, 4122, 11, 26110, 4122, 666, 3565, 3142, 24590, 11, 370, 283, 295, 2031, 11, 597, 51249, 51249, 6915, 290, 295, 710, 11, 307, 586, 290, 295, 341, 6114, 670, 510, 13, 51502, 51502], "temperature": 0.0, "avg_logprob": -0.13722248353819916, "compression_ratio": 1.5364238410596027, "no_speech_prob": 2.3687896373303374e-06}, {"id": 79, "seek": 44748, "start": 447.48, "end": 456.54, "text": " And let's say that we end up choosing w1 and w2 to be 1, and b to be negative 1.", "tokens": [50364, 400, 718, 311, 584, 300, 321, 917, 493, 10875, 261, 16, 293, 261, 17, 281, 312, 502, 11, 293, 272, 281, 312, 3671, 502, 13, 50817, 50817, 407, 710, 307, 2681, 281, 502, 1413, 2031, 16, 8889, 1804, 502, 1413, 2031, 17, 8889, 3175, 502, 13, 51192, 51192, 400, 264, 3537, 12866, 382, 949, 486, 6805, 281, 562, 710, 307, 2681, 281, 1958, 11, 293, 370, 341, 6114, 51587, 51587], "temperature": 0.0, "avg_logprob": -0.10907524161868626, "compression_ratio": 1.537037037037037, "no_speech_prob": 1.0676998272174387e-06}, {"id": 80, "seek": 44748, "start": 456.54, "end": 464.04, "text": " So z is equal to 1 times x1 squared plus 1 times x2 squared minus 1.", "tokens": [50364, 400, 718, 311, 584, 300, 321, 917, 493, 10875, 261, 16, 293, 261, 17, 281, 312, 502, 11, 293, 272, 281, 312, 3671, 502, 13, 50817, 50817, 407, 710, 307, 2681, 281, 502, 1413, 2031, 16, 8889, 1804, 502, 1413, 2031, 17, 8889, 3175, 502, 13, 51192, 51192, 400, 264, 3537, 12866, 382, 949, 486, 6805, 281, 562, 710, 307, 2681, 281, 1958, 11, 293, 370, 341, 6114, 51587, 51587], "temperature": 0.0, "avg_logprob": -0.10907524161868626, "compression_ratio": 1.537037037037037, "no_speech_prob": 1.0676998272174387e-06}, {"id": 81, "seek": 44748, "start": 464.04, "end": 471.94, "text": " And the decision boundary as before will correspond to when z is equal to 0, and so this expression", "tokens": [50364, 400, 718, 311, 584, 300, 321, 917, 493, 10875, 261, 16, 293, 261, 17, 281, 312, 502, 11, 293, 272, 281, 312, 3671, 502, 13, 50817, 50817, 407, 710, 307, 2681, 281, 502, 1413, 2031, 16, 8889, 1804, 502, 1413, 2031, 17, 8889, 3175, 502, 13, 51192, 51192, 400, 264, 3537, 12866, 382, 949, 486, 6805, 281, 562, 710, 307, 2681, 281, 1958, 11, 293, 370, 341, 6114, 51587, 51587], "temperature": 0.0, "avg_logprob": -0.10907524161868626, "compression_ratio": 1.537037037037037, "no_speech_prob": 1.0676998272174387e-06}, {"id": 82, "seek": 47194, "start": 471.94, "end": 477.8, "text": " will be equal to 0 when x1 squared plus x2 squared is equal to 1.", "tokens": [50364, 486, 312, 2681, 281, 1958, 562, 2031, 16, 8889, 1804, 2031, 17, 8889, 307, 2681, 281, 502, 13, 50657, 50657, 400, 498, 291, 7542, 322, 264, 10686, 322, 264, 1411, 11, 264, 7605, 11760, 281, 2031, 16, 8889, 1804, 50955, 50955, 2031, 17, 8889, 6915, 502, 11, 341, 4523, 484, 281, 312, 341, 6329, 13, 51239, 51239, 1133, 2031, 16, 8889, 1804, 2031, 17, 8889, 307, 5044, 813, 420, 2681, 281, 502, 11, 300, 311, 341, 1859, 2380, 51504, 51504, 264, 6329, 11, 293, 300, 311, 562, 291, 6069, 288, 281, 312, 502, 13, 51745, 51745], "temperature": 0.0, "avg_logprob": -0.07732768934600208, "compression_ratio": 1.901098901098901, "no_speech_prob": 3.5559667139750673e-06}, {"id": 83, "seek": 47194, "start": 477.8, "end": 483.76, "text": " And if you plot on the diagram on the left, the curve corresponding to x1 squared plus", "tokens": [50364, 486, 312, 2681, 281, 1958, 562, 2031, 16, 8889, 1804, 2031, 17, 8889, 307, 2681, 281, 502, 13, 50657, 50657, 400, 498, 291, 7542, 322, 264, 10686, 322, 264, 1411, 11, 264, 7605, 11760, 281, 2031, 16, 8889, 1804, 50955, 50955, 2031, 17, 8889, 6915, 502, 11, 341, 4523, 484, 281, 312, 341, 6329, 13, 51239, 51239, 1133, 2031, 16, 8889, 1804, 2031, 17, 8889, 307, 5044, 813, 420, 2681, 281, 502, 11, 300, 311, 341, 1859, 2380, 51504, 51504, 264, 6329, 11, 293, 300, 311, 562, 291, 6069, 288, 281, 312, 502, 13, 51745, 51745], "temperature": 0.0, "avg_logprob": -0.07732768934600208, "compression_ratio": 1.901098901098901, "no_speech_prob": 3.5559667139750673e-06}, {"id": 84, "seek": 47194, "start": 483.76, "end": 489.44, "text": " x2 squared equals 1, this turns out to be this circle.", "tokens": [50364, 486, 312, 2681, 281, 1958, 562, 2031, 16, 8889, 1804, 2031, 17, 8889, 307, 2681, 281, 502, 13, 50657, 50657, 400, 498, 291, 7542, 322, 264, 10686, 322, 264, 1411, 11, 264, 7605, 11760, 281, 2031, 16, 8889, 1804, 50955, 50955, 2031, 17, 8889, 6915, 502, 11, 341, 4523, 484, 281, 312, 341, 6329, 13, 51239, 51239, 1133, 2031, 16, 8889, 1804, 2031, 17, 8889, 307, 5044, 813, 420, 2681, 281, 502, 11, 300, 311, 341, 1859, 2380, 51504, 51504, 264, 6329, 11, 293, 300, 311, 562, 291, 6069, 288, 281, 312, 502, 13, 51745, 51745], "temperature": 0.0, "avg_logprob": -0.07732768934600208, "compression_ratio": 1.901098901098901, "no_speech_prob": 3.5559667139750673e-06}, {"id": 85, "seek": 47194, "start": 489.44, "end": 494.74, "text": " When x1 squared plus x2 squared is greater than or equal to 1, that's this area outside", "tokens": [50364, 486, 312, 2681, 281, 1958, 562, 2031, 16, 8889, 1804, 2031, 17, 8889, 307, 2681, 281, 502, 13, 50657, 50657, 400, 498, 291, 7542, 322, 264, 10686, 322, 264, 1411, 11, 264, 7605, 11760, 281, 2031, 16, 8889, 1804, 50955, 50955, 2031, 17, 8889, 6915, 502, 11, 341, 4523, 484, 281, 312, 341, 6329, 13, 51239, 51239, 1133, 2031, 16, 8889, 1804, 2031, 17, 8889, 307, 5044, 813, 420, 2681, 281, 502, 11, 300, 311, 341, 1859, 2380, 51504, 51504, 264, 6329, 11, 293, 300, 311, 562, 291, 6069, 288, 281, 312, 502, 13, 51745, 51745], "temperature": 0.0, "avg_logprob": -0.07732768934600208, "compression_ratio": 1.901098901098901, "no_speech_prob": 3.5559667139750673e-06}, {"id": 86, "seek": 47194, "start": 494.74, "end": 499.56, "text": " the circle, and that's when you predict y to be 1.", "tokens": [50364, 486, 312, 2681, 281, 1958, 562, 2031, 16, 8889, 1804, 2031, 17, 8889, 307, 2681, 281, 502, 13, 50657, 50657, 400, 498, 291, 7542, 322, 264, 10686, 322, 264, 1411, 11, 264, 7605, 11760, 281, 2031, 16, 8889, 1804, 50955, 50955, 2031, 17, 8889, 6915, 502, 11, 341, 4523, 484, 281, 312, 341, 6329, 13, 51239, 51239, 1133, 2031, 16, 8889, 1804, 2031, 17, 8889, 307, 5044, 813, 420, 2681, 281, 502, 11, 300, 311, 341, 1859, 2380, 51504, 51504, 264, 6329, 11, 293, 300, 311, 562, 291, 6069, 288, 281, 312, 502, 13, 51745, 51745], "temperature": 0.0, "avg_logprob": -0.07732768934600208, "compression_ratio": 1.901098901098901, "no_speech_prob": 3.5559667139750673e-06}, {"id": 87, "seek": 49956, "start": 499.56, "end": 506.96, "text": " Conversely, when x1 squared plus x2 squared is less than 1, that's this area inside the", "tokens": [50364, 33247, 736, 11, 562, 2031, 16, 8889, 1804, 2031, 17, 8889, 307, 1570, 813, 502, 11, 300, 311, 341, 1859, 1854, 264, 50734, 50734, 6329, 11, 293, 300, 311, 562, 291, 576, 6069, 288, 281, 312, 1958, 13, 50983, 50983, 407, 393, 321, 808, 493, 365, 754, 544, 3997, 3537, 13180, 813, 613, 30, 51232, 51232, 1079, 291, 393, 13, 51300, 51300, 509, 393, 360, 370, 538, 1419, 754, 2946, 1668, 26110, 2115, 13, 51517, 51517], "temperature": 0.0, "avg_logprob": -0.13232755661010742, "compression_ratio": 1.4742268041237114, "no_speech_prob": 2.482468516973313e-06}, {"id": 88, "seek": 49956, "start": 506.96, "end": 511.94, "text": " circle, and that's when you would predict y to be 0.", "tokens": [50364, 33247, 736, 11, 562, 2031, 16, 8889, 1804, 2031, 17, 8889, 307, 1570, 813, 502, 11, 300, 311, 341, 1859, 1854, 264, 50734, 50734, 6329, 11, 293, 300, 311, 562, 291, 576, 6069, 288, 281, 312, 1958, 13, 50983, 50983, 407, 393, 321, 808, 493, 365, 754, 544, 3997, 3537, 13180, 813, 613, 30, 51232, 51232, 1079, 291, 393, 13, 51300, 51300, 509, 393, 360, 370, 538, 1419, 754, 2946, 1668, 26110, 2115, 13, 51517, 51517], "temperature": 0.0, "avg_logprob": -0.13232755661010742, "compression_ratio": 1.4742268041237114, "no_speech_prob": 2.482468516973313e-06}, {"id": 89, "seek": 49956, "start": 511.94, "end": 516.92, "text": " So can we come up with even more complex decision boundaries than these?", "tokens": [50364, 33247, 736, 11, 562, 2031, 16, 8889, 1804, 2031, 17, 8889, 307, 1570, 813, 502, 11, 300, 311, 341, 1859, 1854, 264, 50734, 50734, 6329, 11, 293, 300, 311, 562, 291, 576, 6069, 288, 281, 312, 1958, 13, 50983, 50983, 407, 393, 321, 808, 493, 365, 754, 544, 3997, 3537, 13180, 813, 613, 30, 51232, 51232, 1079, 291, 393, 13, 51300, 51300, 509, 393, 360, 370, 538, 1419, 754, 2946, 1668, 26110, 2115, 13, 51517, 51517], "temperature": 0.0, "avg_logprob": -0.13232755661010742, "compression_ratio": 1.4742268041237114, "no_speech_prob": 2.482468516973313e-06}, {"id": 90, "seek": 49956, "start": 516.92, "end": 518.28, "text": " Yes you can.", "tokens": [50364, 33247, 736, 11, 562, 2031, 16, 8889, 1804, 2031, 17, 8889, 307, 1570, 813, 502, 11, 300, 311, 341, 1859, 1854, 264, 50734, 50734, 6329, 11, 293, 300, 311, 562, 291, 576, 6069, 288, 281, 312, 1958, 13, 50983, 50983, 407, 393, 321, 808, 493, 365, 754, 544, 3997, 3537, 13180, 813, 613, 30, 51232, 51232, 1079, 291, 393, 13, 51300, 51300, 509, 393, 360, 370, 538, 1419, 754, 2946, 1668, 26110, 2115, 13, 51517, 51517], "temperature": 0.0, "avg_logprob": -0.13232755661010742, "compression_ratio": 1.4742268041237114, "no_speech_prob": 2.482468516973313e-06}, {"id": 91, "seek": 49956, "start": 518.28, "end": 522.62, "text": " You can do so by having even higher order polynomial terms.", "tokens": [50364, 33247, 736, 11, 562, 2031, 16, 8889, 1804, 2031, 17, 8889, 307, 1570, 813, 502, 11, 300, 311, 341, 1859, 1854, 264, 50734, 50734, 6329, 11, 293, 300, 311, 562, 291, 576, 6069, 288, 281, 312, 1958, 13, 50983, 50983, 407, 393, 321, 808, 493, 365, 754, 544, 3997, 3537, 13180, 813, 613, 30, 51232, 51232, 1079, 291, 393, 13, 51300, 51300, 509, 393, 360, 370, 538, 1419, 754, 2946, 1668, 26110, 2115, 13, 51517, 51517], "temperature": 0.0, "avg_logprob": -0.13232755661010742, "compression_ratio": 1.4742268041237114, "no_speech_prob": 2.482468516973313e-06}, {"id": 92, "seek": 52262, "start": 522.62, "end": 533.8, "text": " Say z is w1 x1 plus w2 x2 plus w3 x1 squared plus w4 x1 x2 plus w5 x2 squared.", "tokens": [50364, 6463, 710, 307, 261, 16, 2031, 16, 1804, 261, 17, 2031, 17, 1804, 261, 18, 2031, 16, 8889, 1804, 261, 19, 2031, 16, 2031, 17, 1804, 261, 20, 2031, 17, 8889, 13, 50923, 50923, 1396, 309, 311, 1944, 291, 393, 483, 754, 544, 3997, 3537, 13180, 13, 51145, 51145, 440, 2316, 393, 6964, 3537, 13180, 1270, 382, 341, 1365, 11, 364, 8284, 48041, 300, 311, 411, 341, 11, 51507, 51507, 420, 365, 257, 819, 3922, 295, 264, 9834, 13, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.09339029243193477, "compression_ratio": 1.5628415300546448, "no_speech_prob": 1.994720605580369e-06}, {"id": 93, "seek": 52262, "start": 533.8, "end": 538.24, "text": " Then it's possible you can get even more complex decision boundaries.", "tokens": [50364, 6463, 710, 307, 261, 16, 2031, 16, 1804, 261, 17, 2031, 17, 1804, 261, 18, 2031, 16, 8889, 1804, 261, 19, 2031, 16, 2031, 17, 1804, 261, 20, 2031, 17, 8889, 13, 50923, 50923, 1396, 309, 311, 1944, 291, 393, 483, 754, 544, 3997, 3537, 13180, 13, 51145, 51145, 440, 2316, 393, 6964, 3537, 13180, 1270, 382, 341, 1365, 11, 364, 8284, 48041, 300, 311, 411, 341, 11, 51507, 51507, 420, 365, 257, 819, 3922, 295, 264, 9834, 13, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.09339029243193477, "compression_ratio": 1.5628415300546448, "no_speech_prob": 1.994720605580369e-06}, {"id": 94, "seek": 52262, "start": 538.24, "end": 545.48, "text": " The model can define decision boundaries such as this example, an ellipse that's like this,", "tokens": [50364, 6463, 710, 307, 261, 16, 2031, 16, 1804, 261, 17, 2031, 17, 1804, 261, 18, 2031, 16, 8889, 1804, 261, 19, 2031, 16, 2031, 17, 1804, 261, 20, 2031, 17, 8889, 13, 50923, 50923, 1396, 309, 311, 1944, 291, 393, 483, 754, 544, 3997, 3537, 13180, 13, 51145, 51145, 440, 2316, 393, 6964, 3537, 13180, 1270, 382, 341, 1365, 11, 364, 8284, 48041, 300, 311, 411, 341, 11, 51507, 51507, 420, 365, 257, 819, 3922, 295, 264, 9834, 13, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.09339029243193477, "compression_ratio": 1.5628415300546448, "no_speech_prob": 1.994720605580369e-06}, {"id": 95, "seek": 52262, "start": 545.48, "end": 549.48, "text": " or with a different choice of the parameters.", "tokens": [50364, 6463, 710, 307, 261, 16, 2031, 16, 1804, 261, 17, 2031, 17, 1804, 261, 18, 2031, 16, 8889, 1804, 261, 19, 2031, 16, 2031, 17, 1804, 261, 20, 2031, 17, 8889, 13, 50923, 50923, 1396, 309, 311, 1944, 291, 393, 483, 754, 544, 3997, 3537, 13180, 13, 51145, 51145, 440, 2316, 393, 6964, 3537, 13180, 1270, 382, 341, 1365, 11, 364, 8284, 48041, 300, 311, 411, 341, 11, 51507, 51507, 420, 365, 257, 819, 3922, 295, 264, 9834, 13, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.09339029243193477, "compression_ratio": 1.5628415300546448, "no_speech_prob": 1.994720605580369e-06}, {"id": 96, "seek": 54948, "start": 549.48, "end": 554.24, "text": " You can even get more complex decision boundaries which can look like functions that maybe look", "tokens": [50364, 509, 393, 754, 483, 544, 3997, 3537, 13180, 597, 393, 574, 411, 6828, 300, 1310, 574, 50602, 50602, 411, 300, 13, 50723, 50723, 407, 341, 307, 364, 1365, 295, 364, 754, 544, 3997, 3537, 12866, 813, 264, 2306, 321, 600, 1612, 50982, 50982, 8046, 11, 293, 341, 11420, 295, 3565, 3142, 24590, 486, 6069, 288, 6915, 502, 1854, 51368, 51368, 341, 3909, 11, 293, 2380, 264, 3909, 309, 486, 6069, 288, 6915, 1958, 13, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.13242704440385866, "compression_ratio": 1.788659793814433, "no_speech_prob": 8.9909087819251e-07}, {"id": 97, "seek": 54948, "start": 554.24, "end": 556.66, "text": " like that.", "tokens": [50364, 509, 393, 754, 483, 544, 3997, 3537, 13180, 597, 393, 574, 411, 6828, 300, 1310, 574, 50602, 50602, 411, 300, 13, 50723, 50723, 407, 341, 307, 364, 1365, 295, 364, 754, 544, 3997, 3537, 12866, 813, 264, 2306, 321, 600, 1612, 50982, 50982, 8046, 11, 293, 341, 11420, 295, 3565, 3142, 24590, 486, 6069, 288, 6915, 502, 1854, 51368, 51368, 341, 3909, 11, 293, 2380, 264, 3909, 309, 486, 6069, 288, 6915, 1958, 13, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.13242704440385866, "compression_ratio": 1.788659793814433, "no_speech_prob": 8.9909087819251e-07}, {"id": 98, "seek": 54948, "start": 556.66, "end": 561.84, "text": " So this is an example of an even more complex decision boundary than the ones we've seen", "tokens": [50364, 509, 393, 754, 483, 544, 3997, 3537, 13180, 597, 393, 574, 411, 6828, 300, 1310, 574, 50602, 50602, 411, 300, 13, 50723, 50723, 407, 341, 307, 364, 1365, 295, 364, 754, 544, 3997, 3537, 12866, 813, 264, 2306, 321, 600, 1612, 50982, 50982, 8046, 11, 293, 341, 11420, 295, 3565, 3142, 24590, 486, 6069, 288, 6915, 502, 1854, 51368, 51368, 341, 3909, 11, 293, 2380, 264, 3909, 309, 486, 6069, 288, 6915, 1958, 13, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.13242704440385866, "compression_ratio": 1.788659793814433, "no_speech_prob": 8.9909087819251e-07}, {"id": 99, "seek": 54948, "start": 561.84, "end": 569.5600000000001, "text": " previously, and this implementation of logistic regression will predict y equals 1 inside", "tokens": [50364, 509, 393, 754, 483, 544, 3997, 3537, 13180, 597, 393, 574, 411, 6828, 300, 1310, 574, 50602, 50602, 411, 300, 13, 50723, 50723, 407, 341, 307, 364, 1365, 295, 364, 754, 544, 3997, 3537, 12866, 813, 264, 2306, 321, 600, 1612, 50982, 50982, 8046, 11, 293, 341, 11420, 295, 3565, 3142, 24590, 486, 6069, 288, 6915, 502, 1854, 51368, 51368, 341, 3909, 11, 293, 2380, 264, 3909, 309, 486, 6069, 288, 6915, 1958, 13, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.13242704440385866, "compression_ratio": 1.788659793814433, "no_speech_prob": 8.9909087819251e-07}, {"id": 100, "seek": 54948, "start": 569.5600000000001, "end": 574.84, "text": " this shape, and outside the shape it will predict y equals 0.", "tokens": [50364, 509, 393, 754, 483, 544, 3997, 3537, 13180, 597, 393, 574, 411, 6828, 300, 1310, 574, 50602, 50602, 411, 300, 13, 50723, 50723, 407, 341, 307, 364, 1365, 295, 364, 754, 544, 3997, 3537, 12866, 813, 264, 2306, 321, 600, 1612, 50982, 50982, 8046, 11, 293, 341, 11420, 295, 3565, 3142, 24590, 486, 6069, 288, 6915, 502, 1854, 51368, 51368, 341, 3909, 11, 293, 2380, 264, 3909, 309, 486, 6069, 288, 6915, 1958, 13, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.13242704440385866, "compression_ratio": 1.788659793814433, "no_speech_prob": 8.9909087819251e-07}, {"id": 101, "seek": 57484, "start": 574.84, "end": 580.48, "text": " So with these polynomial features you can get very complex decision boundaries.", "tokens": [50364, 407, 365, 613, 26110, 4122, 291, 393, 483, 588, 3997, 3537, 13180, 13, 50646, 50646, 682, 661, 2283, 3565, 3142, 24590, 393, 1466, 281, 3318, 1238, 3997, 1412, 13, 50922, 50922, 5780, 498, 291, 645, 281, 406, 4090, 604, 295, 613, 2946, 1668, 22560, 12356, 11, 370, 498, 264, 51164, 51164, 787, 4122, 291, 764, 366, 2031, 16, 2031, 17, 2031, 18, 293, 370, 322, 11, 550, 264, 3537, 12866, 337, 3565, 3142, 51462, 51462, 24590, 486, 1009, 312, 8213, 11, 486, 1009, 312, 257, 2997, 1622, 13, 51711, 51711], "temperature": 0.0, "avg_logprob": -0.11319559553395146, "compression_ratio": 1.768181818181818, "no_speech_prob": 1.7603123296794365e-06}, {"id": 102, "seek": 57484, "start": 580.48, "end": 586.0, "text": " In other words logistic regression can learn to fit pretty complex data.", "tokens": [50364, 407, 365, 613, 26110, 4122, 291, 393, 483, 588, 3997, 3537, 13180, 13, 50646, 50646, 682, 661, 2283, 3565, 3142, 24590, 393, 1466, 281, 3318, 1238, 3997, 1412, 13, 50922, 50922, 5780, 498, 291, 645, 281, 406, 4090, 604, 295, 613, 2946, 1668, 22560, 12356, 11, 370, 498, 264, 51164, 51164, 787, 4122, 291, 764, 366, 2031, 16, 2031, 17, 2031, 18, 293, 370, 322, 11, 550, 264, 3537, 12866, 337, 3565, 3142, 51462, 51462, 24590, 486, 1009, 312, 8213, 11, 486, 1009, 312, 257, 2997, 1622, 13, 51711, 51711], "temperature": 0.0, "avg_logprob": -0.11319559553395146, "compression_ratio": 1.768181818181818, "no_speech_prob": 1.7603123296794365e-06}, {"id": 103, "seek": 57484, "start": 586.0, "end": 590.84, "text": " Although if you were to not include any of these higher order polynomials, so if the", "tokens": [50364, 407, 365, 613, 26110, 4122, 291, 393, 483, 588, 3997, 3537, 13180, 13, 50646, 50646, 682, 661, 2283, 3565, 3142, 24590, 393, 1466, 281, 3318, 1238, 3997, 1412, 13, 50922, 50922, 5780, 498, 291, 645, 281, 406, 4090, 604, 295, 613, 2946, 1668, 22560, 12356, 11, 370, 498, 264, 51164, 51164, 787, 4122, 291, 764, 366, 2031, 16, 2031, 17, 2031, 18, 293, 370, 322, 11, 550, 264, 3537, 12866, 337, 3565, 3142, 51462, 51462, 24590, 486, 1009, 312, 8213, 11, 486, 1009, 312, 257, 2997, 1622, 13, 51711, 51711], "temperature": 0.0, "avg_logprob": -0.11319559553395146, "compression_ratio": 1.768181818181818, "no_speech_prob": 1.7603123296794365e-06}, {"id": 104, "seek": 57484, "start": 590.84, "end": 596.8000000000001, "text": " only features you use are x1 x2 x3 and so on, then the decision boundary for logistic", "tokens": [50364, 407, 365, 613, 26110, 4122, 291, 393, 483, 588, 3997, 3537, 13180, 13, 50646, 50646, 682, 661, 2283, 3565, 3142, 24590, 393, 1466, 281, 3318, 1238, 3997, 1412, 13, 50922, 50922, 5780, 498, 291, 645, 281, 406, 4090, 604, 295, 613, 2946, 1668, 22560, 12356, 11, 370, 498, 264, 51164, 51164, 787, 4122, 291, 764, 366, 2031, 16, 2031, 17, 2031, 18, 293, 370, 322, 11, 550, 264, 3537, 12866, 337, 3565, 3142, 51462, 51462, 24590, 486, 1009, 312, 8213, 11, 486, 1009, 312, 257, 2997, 1622, 13, 51711, 51711], "temperature": 0.0, "avg_logprob": -0.11319559553395146, "compression_ratio": 1.768181818181818, "no_speech_prob": 1.7603123296794365e-06}, {"id": 105, "seek": 57484, "start": 596.8000000000001, "end": 601.7800000000001, "text": " regression will always be linear, will always be a straight line.", "tokens": [50364, 407, 365, 613, 26110, 4122, 291, 393, 483, 588, 3997, 3537, 13180, 13, 50646, 50646, 682, 661, 2283, 3565, 3142, 24590, 393, 1466, 281, 3318, 1238, 3997, 1412, 13, 50922, 50922, 5780, 498, 291, 645, 281, 406, 4090, 604, 295, 613, 2946, 1668, 22560, 12356, 11, 370, 498, 264, 51164, 51164, 787, 4122, 291, 764, 366, 2031, 16, 2031, 17, 2031, 18, 293, 370, 322, 11, 550, 264, 3537, 12866, 337, 3565, 3142, 51462, 51462, 24590, 486, 1009, 312, 8213, 11, 486, 1009, 312, 257, 2997, 1622, 13, 51711, 51711], "temperature": 0.0, "avg_logprob": -0.11319559553395146, "compression_ratio": 1.768181818181818, "no_speech_prob": 1.7603123296794365e-06}, {"id": 106, "seek": 60178, "start": 601.78, "end": 607.6, "text": " In the upcoming optional lab you also get to see the code implementation of the decision", "tokens": [50364, 682, 264, 11500, 17312, 2715, 291, 611, 483, 281, 536, 264, 3089, 11420, 295, 264, 3537, 50655, 50655, 12866, 13, 50726, 50726, 682, 264, 1365, 294, 264, 2715, 456, 486, 312, 732, 4122, 370, 291, 393, 536, 264, 3537, 12866, 50977, 50977, 382, 257, 1622, 13, 51062, 51062, 407, 365, 613, 5056, 14455, 286, 1454, 300, 291, 586, 362, 257, 2020, 295, 264, 3613, 295, 1944, 51275, 51275, 5245, 291, 393, 483, 365, 3565, 3142, 24590, 13, 51472, 51472, 823, 300, 291, 600, 1612, 437, 283, 295, 2031, 393, 7263, 14722, 11, 718, 311, 747, 257, 574, 412, 577, 291, 393, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.10158285413469587, "compression_ratio": 1.7154471544715446, "no_speech_prob": 1.2482552165238303e-06}, {"id": 107, "seek": 60178, "start": 607.6, "end": 609.02, "text": " boundary.", "tokens": [50364, 682, 264, 11500, 17312, 2715, 291, 611, 483, 281, 536, 264, 3089, 11420, 295, 264, 3537, 50655, 50655, 12866, 13, 50726, 50726, 682, 264, 1365, 294, 264, 2715, 456, 486, 312, 732, 4122, 370, 291, 393, 536, 264, 3537, 12866, 50977, 50977, 382, 257, 1622, 13, 51062, 51062, 407, 365, 613, 5056, 14455, 286, 1454, 300, 291, 586, 362, 257, 2020, 295, 264, 3613, 295, 1944, 51275, 51275, 5245, 291, 393, 483, 365, 3565, 3142, 24590, 13, 51472, 51472, 823, 300, 291, 600, 1612, 437, 283, 295, 2031, 393, 7263, 14722, 11, 718, 311, 747, 257, 574, 412, 577, 291, 393, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.10158285413469587, "compression_ratio": 1.7154471544715446, "no_speech_prob": 1.2482552165238303e-06}, {"id": 108, "seek": 60178, "start": 609.02, "end": 614.04, "text": " In the example in the lab there will be two features so you can see the decision boundary", "tokens": [50364, 682, 264, 11500, 17312, 2715, 291, 611, 483, 281, 536, 264, 3089, 11420, 295, 264, 3537, 50655, 50655, 12866, 13, 50726, 50726, 682, 264, 1365, 294, 264, 2715, 456, 486, 312, 732, 4122, 370, 291, 393, 536, 264, 3537, 12866, 50977, 50977, 382, 257, 1622, 13, 51062, 51062, 407, 365, 613, 5056, 14455, 286, 1454, 300, 291, 586, 362, 257, 2020, 295, 264, 3613, 295, 1944, 51275, 51275, 5245, 291, 393, 483, 365, 3565, 3142, 24590, 13, 51472, 51472, 823, 300, 291, 600, 1612, 437, 283, 295, 2031, 393, 7263, 14722, 11, 718, 311, 747, 257, 574, 412, 577, 291, 393, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.10158285413469587, "compression_ratio": 1.7154471544715446, "no_speech_prob": 1.2482552165238303e-06}, {"id": 109, "seek": 60178, "start": 614.04, "end": 615.74, "text": " as a line.", "tokens": [50364, 682, 264, 11500, 17312, 2715, 291, 611, 483, 281, 536, 264, 3089, 11420, 295, 264, 3537, 50655, 50655, 12866, 13, 50726, 50726, 682, 264, 1365, 294, 264, 2715, 456, 486, 312, 732, 4122, 370, 291, 393, 536, 264, 3537, 12866, 50977, 50977, 382, 257, 1622, 13, 51062, 51062, 407, 365, 613, 5056, 14455, 286, 1454, 300, 291, 586, 362, 257, 2020, 295, 264, 3613, 295, 1944, 51275, 51275, 5245, 291, 393, 483, 365, 3565, 3142, 24590, 13, 51472, 51472, 823, 300, 291, 600, 1612, 437, 283, 295, 2031, 393, 7263, 14722, 11, 718, 311, 747, 257, 574, 412, 577, 291, 393, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.10158285413469587, "compression_ratio": 1.7154471544715446, "no_speech_prob": 1.2482552165238303e-06}, {"id": 110, "seek": 60178, "start": 615.74, "end": 620.0, "text": " So with these visualizations I hope that you now have a sense of the range of possible", "tokens": [50364, 682, 264, 11500, 17312, 2715, 291, 611, 483, 281, 536, 264, 3089, 11420, 295, 264, 3537, 50655, 50655, 12866, 13, 50726, 50726, 682, 264, 1365, 294, 264, 2715, 456, 486, 312, 732, 4122, 370, 291, 393, 536, 264, 3537, 12866, 50977, 50977, 382, 257, 1622, 13, 51062, 51062, 407, 365, 613, 5056, 14455, 286, 1454, 300, 291, 586, 362, 257, 2020, 295, 264, 3613, 295, 1944, 51275, 51275, 5245, 291, 393, 483, 365, 3565, 3142, 24590, 13, 51472, 51472, 823, 300, 291, 600, 1612, 437, 283, 295, 2031, 393, 7263, 14722, 11, 718, 311, 747, 257, 574, 412, 577, 291, 393, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.10158285413469587, "compression_ratio": 1.7154471544715446, "no_speech_prob": 1.2482552165238303e-06}, {"id": 111, "seek": 60178, "start": 620.0, "end": 623.9399999999999, "text": " models you can get with logistic regression.", "tokens": [50364, 682, 264, 11500, 17312, 2715, 291, 611, 483, 281, 536, 264, 3089, 11420, 295, 264, 3537, 50655, 50655, 12866, 13, 50726, 50726, 682, 264, 1365, 294, 264, 2715, 456, 486, 312, 732, 4122, 370, 291, 393, 536, 264, 3537, 12866, 50977, 50977, 382, 257, 1622, 13, 51062, 51062, 407, 365, 613, 5056, 14455, 286, 1454, 300, 291, 586, 362, 257, 2020, 295, 264, 3613, 295, 1944, 51275, 51275, 5245, 291, 393, 483, 365, 3565, 3142, 24590, 13, 51472, 51472, 823, 300, 291, 600, 1612, 437, 283, 295, 2031, 393, 7263, 14722, 11, 718, 311, 747, 257, 574, 412, 577, 291, 393, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.10158285413469587, "compression_ratio": 1.7154471544715446, "no_speech_prob": 1.2482552165238303e-06}, {"id": 112, "seek": 60178, "start": 623.9399999999999, "end": 629.28, "text": " Now that you've seen what f of x can potentially compute, let's take a look at how you can", "tokens": [50364, 682, 264, 11500, 17312, 2715, 291, 611, 483, 281, 536, 264, 3089, 11420, 295, 264, 3537, 50655, 50655, 12866, 13, 50726, 50726, 682, 264, 1365, 294, 264, 2715, 456, 486, 312, 732, 4122, 370, 291, 393, 536, 264, 3537, 12866, 50977, 50977, 382, 257, 1622, 13, 51062, 51062, 407, 365, 613, 5056, 14455, 286, 1454, 300, 291, 586, 362, 257, 2020, 295, 264, 3613, 295, 1944, 51275, 51275, 5245, 291, 393, 483, 365, 3565, 3142, 24590, 13, 51472, 51472, 823, 300, 291, 600, 1612, 437, 283, 295, 2031, 393, 7263, 14722, 11, 718, 311, 747, 257, 574, 412, 577, 291, 393, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.10158285413469587, "compression_ratio": 1.7154471544715446, "no_speech_prob": 1.2482552165238303e-06}, {"id": 113, "seek": 62928, "start": 629.28, "end": 632.72, "text": " actually train a logistic regression model.", "tokens": [50364, 767, 3847, 257, 3565, 3142, 24590, 2316, 13, 50536, 50536, 492, 603, 722, 538, 1237, 412, 264, 2063, 2445, 337, 3565, 3142, 24590, 293, 934, 300, 2573, 50776, 50776, 484, 577, 281, 3079, 16235, 23475, 281, 309, 13, 50912, 50912, 961, 311, 352, 322, 281, 264, 958, 960, 13, 50978], "temperature": 0.0, "avg_logprob": -0.16620447085453913, "compression_ratio": 1.4642857142857142, "no_speech_prob": 1.2600639820448123e-05}, {"id": 114, "seek": 62928, "start": 632.72, "end": 637.52, "text": " We'll start by looking at the cost function for logistic regression and after that figure", "tokens": [50364, 767, 3847, 257, 3565, 3142, 24590, 2316, 13, 50536, 50536, 492, 603, 722, 538, 1237, 412, 264, 2063, 2445, 337, 3565, 3142, 24590, 293, 934, 300, 2573, 50776, 50776, 484, 577, 281, 3079, 16235, 23475, 281, 309, 13, 50912, 50912, 961, 311, 352, 322, 281, 264, 958, 960, 13, 50978], "temperature": 0.0, "avg_logprob": -0.16620447085453913, "compression_ratio": 1.4642857142857142, "no_speech_prob": 1.2600639820448123e-05}, {"id": 115, "seek": 62928, "start": 637.52, "end": 640.24, "text": " out how to apply gradient descent to it.", "tokens": [50364, 767, 3847, 257, 3565, 3142, 24590, 2316, 13, 50536, 50536, 492, 603, 722, 538, 1237, 412, 264, 2063, 2445, 337, 3565, 3142, 24590, 293, 934, 300, 2573, 50776, 50776, 484, 577, 281, 3079, 16235, 23475, 281, 309, 13, 50912, 50912, 961, 311, 352, 322, 281, 264, 958, 960, 13, 50978], "temperature": 0.0, "avg_logprob": -0.16620447085453913, "compression_ratio": 1.4642857142857142, "no_speech_prob": 1.2600639820448123e-05}, {"id": 116, "seek": 64024, "start": 640.24, "end": 660.28, "text": " Let's go on to the next video.", "tokens": [50364, 961, 311, 352, 322, 281, 264, 958, 960, 13, 51366], "temperature": 0.0, "avg_logprob": -0.4419074058532715, "compression_ratio": 0.7894736842105263, "no_speech_prob": 0.00039714016020298004}], "language": "en", "video_id": "QJdIpRcL_4U", "entity": "ML Specialization, Andrew Ng (2022)"}}