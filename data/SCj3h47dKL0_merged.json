{"video_id": "SCj3h47dKL0", "title": "3.9 Regularization to Reduce Overfitting | Cost function with regularization- [ML | Andrew Ng]", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 543, "views": 185, "publish_date": "11/04/2022", "timestamp": 1661299200, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " In the last video, we saw that regularization tries to make the parameter values w1 through wn small to reduce overfitting. In this video, we'll build on that intuition and develop a modified cost function for your learning algorithm they can use to actually apply regularization. Let's jump in. Recall this example from the previous video in which we saw that if you fit a quadratic function to this data, it gives a pretty good fit. But if you fit a very high order polynomial, you end up with a curve that overfits the data. But now consider the following. Suppose that you had a way to make the parameters w3 and w4 really, really small, say close to zero. Here's what I mean. Let's say instead of minimizing this objective function, this is a cost function for linear regression. Let's say you were to modify the cost function and add to it 1000 times w3 squared plus 1000 times w4 squared. And here I'm just choosing 1000 because it's a big number, but any other really large number would be okay. So with this modified cost function, you'd in fact be penalizing the model if w3 and w4 are large. Because if you want to minimize this function, the only way to make this new cost function small is if w3 and w4 are both small, right? Because otherwise this 1000 times w3 squared and 1000 times w4 squared terms are going to be really, really big. So when you minimize this function, you're going to end up with w3 close to zero and w4 close to zero. So we're effectively nearly canceling out the effect of the features x cubed and x to the power of four and getting rid of these two terms over here. And if we do that, then we end up with a fit to the data that's much closer to the quadratic function, including maybe just tiny contributions from the features x cubed and x to the fourth. And this is good because it's a much better fit to the data compared to if all the parameters could be large and you end up with this wiggly quadratic function. More generally, here's the idea behind regularization. The idea is that if there are smaller values for the parameters, then that's a bit like having a simpler model, maybe one with fewer features, which is therefore less prone to overfitting. On the last slide, we penalize or we say we regularize only w3 and w4. But more generally, the way that regularization tends to be implemented is if you have a lot of features, say 100 features, you may not know which are the most important features and which ones to penalize. So the way regularization is typically implemented is to penalize all of the features or more precisely, you penalize all the wj parameters. And it's possible to show that this will usually result in fitting a smoother, simpler, less wiggly function that's less prone to overfitting. So for this example, if you have data with 100 features for each house, it may be hard to pick in advance which features to include and which ones to exclude. So let's build a model that uses all 100 features. So you have these 100 parameters, w1 through w100, as well as the 101st parameter b. Because we don't know which of these parameters are going to be the important ones, let's penalize all of them a bit and shrink all of them by adding this new term lambda times the sum from j equals 1 through n, where n is 100, the number of features of wj squared. This value lambda here is the Greek alphabet lambda, and it's also called a regularization parameter. So similar to picking a learning rate alpha, you now also have to choose a number for lambda. A couple things I would like to point out, by convention, instead of using lambda times the sum of wj squared, we also divide lambda by 2m, so that both the first and second terms here are scaled by 1 over 2m. It turns out that by scaling both terms the same way, it becomes a little bit easier to choose a good value for lambda, and in particular, you find that even if your training set size grows, say you find more training examples, so m, the training set size, is now bigger, the same value of lambda that you picked previously is now also more likely to continue to work if you have this extra scaling by 2m. Also by the way, by convention, we're not going to penalize the parameter b for being large. In practice, it makes very little difference whether you do or not, and some machine learning engineers, and actually some learning algorithm implementations, will also include lambda over 2m times the b squared term. But this makes very little difference in practice, and the more common convention which we'll use in this course is to regularize only the parameters w rather than the parameter b. So to summarize, in this modified cost function, we want to minimize the original cost, which is the mean squared error cost, plus additionally the second term, which is called the regularization term. And so this new cost function trades off two goals that you might have. Trying to minimize this first term encourages the algorithm to fit the training data well by minimizing the squared differences of the predictions and the actual values, and trying to minimize the second term, the algorithm also tries to keep the parameters wj small, which will tend to reduce overfitting. The value of lambda that you choose specifies the relative importance or the relative tradeoff or how you balance between these two goals. Let's take a look at what different values of lambda will cause your learning algorithm to do. Let's use the housing price prediction example using linear regression. So f of x is the linear regression model. If lambda was set to be zero, then you're not using the regularization term at all, because the regularization term is multiplied by zero. And so if lambda was zero, you end up fitting this overly wiggly, overly complex curve, and it overfits. So that was one extreme of if lambda was zero. Let's now look at the other extreme. If you set lambda to be a really, really, really large number, say lambda equals 10 to the power of 10, then you're placing a very heavy weight on this regularization term on the right. And the only way to minimize this is to be sure that all the values of w are pretty much very close to zero. So if lambda is very, very large, the learning algorithm will choose w1, w2, w3, and w4 to be extremely close to zero. And thus, f of x is basically equal to b, and so the learning algorithm fits a horizontal straight line and underfits. To recap, if lambda is zero, this model will overfit. If lambda is enormous, like 10 to the power of 10, this model will underfit. And so what you want is some value of lambda that is in between that more appropriately balances these first and second terms of trading off, minimizing the mean squared error and keeping the parameters small. And when the value of lambda is not too small and not too large, but just right, then hopefully you end up able to fit a fourth order polynomial keeping all of these features, but with a function that looks like this. So that's how regularization works. When we talk about model selection later in the specialization, we'll also see a variety of ways to choose good values for lambda. In the next two videos, we'll flesh out how to apply regularization to linear regression and logistic regression, and how to train these models with gradient descent. With that, you'll be able to avoid overfitting with both of these algorithms.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.28, "text": " In the last video, we saw that regularization tries to make the parameter values w1 through", "tokens": [50364, 682, 264, 1036, 960, 11, 321, 1866, 300, 3890, 2144, 9898, 281, 652, 264, 13075, 4190, 261, 16, 807, 50728, 50728, 261, 77, 1359, 281, 5407, 670, 69, 2414, 13, 50904, 50904, 682, 341, 960, 11, 321, 603, 1322, 322, 300, 24002, 293, 1499, 257, 15873, 2063, 2445, 337, 428, 51156, 51156, 2539, 9284, 436, 393, 764, 281, 767, 3079, 3890, 2144, 13, 51396, 51396, 961, 311, 3012, 294, 13, 51470, 51470, 9647, 336, 341, 1365, 490, 264, 3894, 960, 294, 597, 321, 1866, 300, 498, 291, 3318, 257, 37262, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.16718232378046563, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.0053835115395486355}, {"id": 1, "seek": 0, "start": 7.28, "end": 10.8, "text": " wn small to reduce overfitting.", "tokens": [50364, 682, 264, 1036, 960, 11, 321, 1866, 300, 3890, 2144, 9898, 281, 652, 264, 13075, 4190, 261, 16, 807, 50728, 50728, 261, 77, 1359, 281, 5407, 670, 69, 2414, 13, 50904, 50904, 682, 341, 960, 11, 321, 603, 1322, 322, 300, 24002, 293, 1499, 257, 15873, 2063, 2445, 337, 428, 51156, 51156, 2539, 9284, 436, 393, 764, 281, 767, 3079, 3890, 2144, 13, 51396, 51396, 961, 311, 3012, 294, 13, 51470, 51470, 9647, 336, 341, 1365, 490, 264, 3894, 960, 294, 597, 321, 1866, 300, 498, 291, 3318, 257, 37262, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.16718232378046563, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.0053835115395486355}, {"id": 2, "seek": 0, "start": 10.8, "end": 15.84, "text": " In this video, we'll build on that intuition and develop a modified cost function for your", "tokens": [50364, 682, 264, 1036, 960, 11, 321, 1866, 300, 3890, 2144, 9898, 281, 652, 264, 13075, 4190, 261, 16, 807, 50728, 50728, 261, 77, 1359, 281, 5407, 670, 69, 2414, 13, 50904, 50904, 682, 341, 960, 11, 321, 603, 1322, 322, 300, 24002, 293, 1499, 257, 15873, 2063, 2445, 337, 428, 51156, 51156, 2539, 9284, 436, 393, 764, 281, 767, 3079, 3890, 2144, 13, 51396, 51396, 961, 311, 3012, 294, 13, 51470, 51470, 9647, 336, 341, 1365, 490, 264, 3894, 960, 294, 597, 321, 1866, 300, 498, 291, 3318, 257, 37262, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.16718232378046563, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.0053835115395486355}, {"id": 3, "seek": 0, "start": 15.84, "end": 20.64, "text": " learning algorithm they can use to actually apply regularization.", "tokens": [50364, 682, 264, 1036, 960, 11, 321, 1866, 300, 3890, 2144, 9898, 281, 652, 264, 13075, 4190, 261, 16, 807, 50728, 50728, 261, 77, 1359, 281, 5407, 670, 69, 2414, 13, 50904, 50904, 682, 341, 960, 11, 321, 603, 1322, 322, 300, 24002, 293, 1499, 257, 15873, 2063, 2445, 337, 428, 51156, 51156, 2539, 9284, 436, 393, 764, 281, 767, 3079, 3890, 2144, 13, 51396, 51396, 961, 311, 3012, 294, 13, 51470, 51470, 9647, 336, 341, 1365, 490, 264, 3894, 960, 294, 597, 321, 1866, 300, 498, 291, 3318, 257, 37262, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.16718232378046563, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.0053835115395486355}, {"id": 4, "seek": 0, "start": 20.64, "end": 22.12, "text": " Let's jump in.", "tokens": [50364, 682, 264, 1036, 960, 11, 321, 1866, 300, 3890, 2144, 9898, 281, 652, 264, 13075, 4190, 261, 16, 807, 50728, 50728, 261, 77, 1359, 281, 5407, 670, 69, 2414, 13, 50904, 50904, 682, 341, 960, 11, 321, 603, 1322, 322, 300, 24002, 293, 1499, 257, 15873, 2063, 2445, 337, 428, 51156, 51156, 2539, 9284, 436, 393, 764, 281, 767, 3079, 3890, 2144, 13, 51396, 51396, 961, 311, 3012, 294, 13, 51470, 51470, 9647, 336, 341, 1365, 490, 264, 3894, 960, 294, 597, 321, 1866, 300, 498, 291, 3318, 257, 37262, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.16718232378046563, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.0053835115395486355}, {"id": 5, "seek": 0, "start": 22.12, "end": 28.16, "text": " Recall this example from the previous video in which we saw that if you fit a quadratic", "tokens": [50364, 682, 264, 1036, 960, 11, 321, 1866, 300, 3890, 2144, 9898, 281, 652, 264, 13075, 4190, 261, 16, 807, 50728, 50728, 261, 77, 1359, 281, 5407, 670, 69, 2414, 13, 50904, 50904, 682, 341, 960, 11, 321, 603, 1322, 322, 300, 24002, 293, 1499, 257, 15873, 2063, 2445, 337, 428, 51156, 51156, 2539, 9284, 436, 393, 764, 281, 767, 3079, 3890, 2144, 13, 51396, 51396, 961, 311, 3012, 294, 13, 51470, 51470, 9647, 336, 341, 1365, 490, 264, 3894, 960, 294, 597, 321, 1866, 300, 498, 291, 3318, 257, 37262, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.16718232378046563, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.0053835115395486355}, {"id": 6, "seek": 2816, "start": 28.16, "end": 32.0, "text": " function to this data, it gives a pretty good fit.", "tokens": [50364, 2445, 281, 341, 1412, 11, 309, 2709, 257, 1238, 665, 3318, 13, 50556, 50556, 583, 498, 291, 3318, 257, 588, 1090, 1668, 26110, 11, 291, 917, 493, 365, 257, 7605, 300, 670, 13979, 264, 50800, 50800, 1412, 13, 50850, 50850, 583, 586, 1949, 264, 3480, 13, 51036, 51036, 21360, 300, 291, 632, 257, 636, 281, 652, 264, 9834, 261, 18, 293, 261, 19, 534, 11, 534, 1359, 11, 584, 1998, 51400, 51400, 281, 4018, 13, 51450, 51450, 1692, 311, 437, 286, 914, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.1608912369300579, "compression_ratio": 1.48, "no_speech_prob": 2.84086763713276e-05}, {"id": 7, "seek": 2816, "start": 32.0, "end": 36.88, "text": " But if you fit a very high order polynomial, you end up with a curve that overfits the", "tokens": [50364, 2445, 281, 341, 1412, 11, 309, 2709, 257, 1238, 665, 3318, 13, 50556, 50556, 583, 498, 291, 3318, 257, 588, 1090, 1668, 26110, 11, 291, 917, 493, 365, 257, 7605, 300, 670, 13979, 264, 50800, 50800, 1412, 13, 50850, 50850, 583, 586, 1949, 264, 3480, 13, 51036, 51036, 21360, 300, 291, 632, 257, 636, 281, 652, 264, 9834, 261, 18, 293, 261, 19, 534, 11, 534, 1359, 11, 584, 1998, 51400, 51400, 281, 4018, 13, 51450, 51450, 1692, 311, 437, 286, 914, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.1608912369300579, "compression_ratio": 1.48, "no_speech_prob": 2.84086763713276e-05}, {"id": 8, "seek": 2816, "start": 36.88, "end": 37.88, "text": " data.", "tokens": [50364, 2445, 281, 341, 1412, 11, 309, 2709, 257, 1238, 665, 3318, 13, 50556, 50556, 583, 498, 291, 3318, 257, 588, 1090, 1668, 26110, 11, 291, 917, 493, 365, 257, 7605, 300, 670, 13979, 264, 50800, 50800, 1412, 13, 50850, 50850, 583, 586, 1949, 264, 3480, 13, 51036, 51036, 21360, 300, 291, 632, 257, 636, 281, 652, 264, 9834, 261, 18, 293, 261, 19, 534, 11, 534, 1359, 11, 584, 1998, 51400, 51400, 281, 4018, 13, 51450, 51450, 1692, 311, 437, 286, 914, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.1608912369300579, "compression_ratio": 1.48, "no_speech_prob": 2.84086763713276e-05}, {"id": 9, "seek": 2816, "start": 37.88, "end": 41.6, "text": " But now consider the following.", "tokens": [50364, 2445, 281, 341, 1412, 11, 309, 2709, 257, 1238, 665, 3318, 13, 50556, 50556, 583, 498, 291, 3318, 257, 588, 1090, 1668, 26110, 11, 291, 917, 493, 365, 257, 7605, 300, 670, 13979, 264, 50800, 50800, 1412, 13, 50850, 50850, 583, 586, 1949, 264, 3480, 13, 51036, 51036, 21360, 300, 291, 632, 257, 636, 281, 652, 264, 9834, 261, 18, 293, 261, 19, 534, 11, 534, 1359, 11, 584, 1998, 51400, 51400, 281, 4018, 13, 51450, 51450, 1692, 311, 437, 286, 914, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.1608912369300579, "compression_ratio": 1.48, "no_speech_prob": 2.84086763713276e-05}, {"id": 10, "seek": 2816, "start": 41.6, "end": 48.879999999999995, "text": " Suppose that you had a way to make the parameters w3 and w4 really, really small, say close", "tokens": [50364, 2445, 281, 341, 1412, 11, 309, 2709, 257, 1238, 665, 3318, 13, 50556, 50556, 583, 498, 291, 3318, 257, 588, 1090, 1668, 26110, 11, 291, 917, 493, 365, 257, 7605, 300, 670, 13979, 264, 50800, 50800, 1412, 13, 50850, 50850, 583, 586, 1949, 264, 3480, 13, 51036, 51036, 21360, 300, 291, 632, 257, 636, 281, 652, 264, 9834, 261, 18, 293, 261, 19, 534, 11, 534, 1359, 11, 584, 1998, 51400, 51400, 281, 4018, 13, 51450, 51450, 1692, 311, 437, 286, 914, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.1608912369300579, "compression_ratio": 1.48, "no_speech_prob": 2.84086763713276e-05}, {"id": 11, "seek": 2816, "start": 48.879999999999995, "end": 49.879999999999995, "text": " to zero.", "tokens": [50364, 2445, 281, 341, 1412, 11, 309, 2709, 257, 1238, 665, 3318, 13, 50556, 50556, 583, 498, 291, 3318, 257, 588, 1090, 1668, 26110, 11, 291, 917, 493, 365, 257, 7605, 300, 670, 13979, 264, 50800, 50800, 1412, 13, 50850, 50850, 583, 586, 1949, 264, 3480, 13, 51036, 51036, 21360, 300, 291, 632, 257, 636, 281, 652, 264, 9834, 261, 18, 293, 261, 19, 534, 11, 534, 1359, 11, 584, 1998, 51400, 51400, 281, 4018, 13, 51450, 51450, 1692, 311, 437, 286, 914, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.1608912369300579, "compression_ratio": 1.48, "no_speech_prob": 2.84086763713276e-05}, {"id": 12, "seek": 2816, "start": 49.879999999999995, "end": 52.400000000000006, "text": " Here's what I mean.", "tokens": [50364, 2445, 281, 341, 1412, 11, 309, 2709, 257, 1238, 665, 3318, 13, 50556, 50556, 583, 498, 291, 3318, 257, 588, 1090, 1668, 26110, 11, 291, 917, 493, 365, 257, 7605, 300, 670, 13979, 264, 50800, 50800, 1412, 13, 50850, 50850, 583, 586, 1949, 264, 3480, 13, 51036, 51036, 21360, 300, 291, 632, 257, 636, 281, 652, 264, 9834, 261, 18, 293, 261, 19, 534, 11, 534, 1359, 11, 584, 1998, 51400, 51400, 281, 4018, 13, 51450, 51450, 1692, 311, 437, 286, 914, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.1608912369300579, "compression_ratio": 1.48, "no_speech_prob": 2.84086763713276e-05}, {"id": 13, "seek": 5240, "start": 52.4, "end": 58.56, "text": " Let's say instead of minimizing this objective function, this is a cost function for linear", "tokens": [50364, 961, 311, 584, 2602, 295, 46608, 341, 10024, 2445, 11, 341, 307, 257, 2063, 2445, 337, 8213, 50672, 50672, 24590, 13, 50760, 50760, 961, 311, 584, 291, 645, 281, 16927, 264, 2063, 2445, 293, 909, 281, 309, 9714, 1413, 261, 18, 8889, 1804, 9714, 51154, 51154, 1413, 261, 19, 8889, 13, 51282, 51282, 400, 510, 286, 478, 445, 10875, 9714, 570, 309, 311, 257, 955, 1230, 11, 457, 604, 661, 534, 2416, 1230, 51536, 51536, 576, 312, 1392, 13, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.14978093986051627, "compression_ratio": 1.5555555555555556, "no_speech_prob": 8.939585313783027e-06}, {"id": 14, "seek": 5240, "start": 58.56, "end": 60.32, "text": " regression.", "tokens": [50364, 961, 311, 584, 2602, 295, 46608, 341, 10024, 2445, 11, 341, 307, 257, 2063, 2445, 337, 8213, 50672, 50672, 24590, 13, 50760, 50760, 961, 311, 584, 291, 645, 281, 16927, 264, 2063, 2445, 293, 909, 281, 309, 9714, 1413, 261, 18, 8889, 1804, 9714, 51154, 51154, 1413, 261, 19, 8889, 13, 51282, 51282, 400, 510, 286, 478, 445, 10875, 9714, 570, 309, 311, 257, 955, 1230, 11, 457, 604, 661, 534, 2416, 1230, 51536, 51536, 576, 312, 1392, 13, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.14978093986051627, "compression_ratio": 1.5555555555555556, "no_speech_prob": 8.939585313783027e-06}, {"id": 15, "seek": 5240, "start": 60.32, "end": 68.2, "text": " Let's say you were to modify the cost function and add to it 1000 times w3 squared plus 1000", "tokens": [50364, 961, 311, 584, 2602, 295, 46608, 341, 10024, 2445, 11, 341, 307, 257, 2063, 2445, 337, 8213, 50672, 50672, 24590, 13, 50760, 50760, 961, 311, 584, 291, 645, 281, 16927, 264, 2063, 2445, 293, 909, 281, 309, 9714, 1413, 261, 18, 8889, 1804, 9714, 51154, 51154, 1413, 261, 19, 8889, 13, 51282, 51282, 400, 510, 286, 478, 445, 10875, 9714, 570, 309, 311, 257, 955, 1230, 11, 457, 604, 661, 534, 2416, 1230, 51536, 51536, 576, 312, 1392, 13, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.14978093986051627, "compression_ratio": 1.5555555555555556, "no_speech_prob": 8.939585313783027e-06}, {"id": 16, "seek": 5240, "start": 68.2, "end": 70.75999999999999, "text": " times w4 squared.", "tokens": [50364, 961, 311, 584, 2602, 295, 46608, 341, 10024, 2445, 11, 341, 307, 257, 2063, 2445, 337, 8213, 50672, 50672, 24590, 13, 50760, 50760, 961, 311, 584, 291, 645, 281, 16927, 264, 2063, 2445, 293, 909, 281, 309, 9714, 1413, 261, 18, 8889, 1804, 9714, 51154, 51154, 1413, 261, 19, 8889, 13, 51282, 51282, 400, 510, 286, 478, 445, 10875, 9714, 570, 309, 311, 257, 955, 1230, 11, 457, 604, 661, 534, 2416, 1230, 51536, 51536, 576, 312, 1392, 13, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.14978093986051627, "compression_ratio": 1.5555555555555556, "no_speech_prob": 8.939585313783027e-06}, {"id": 17, "seek": 5240, "start": 70.75999999999999, "end": 75.84, "text": " And here I'm just choosing 1000 because it's a big number, but any other really large number", "tokens": [50364, 961, 311, 584, 2602, 295, 46608, 341, 10024, 2445, 11, 341, 307, 257, 2063, 2445, 337, 8213, 50672, 50672, 24590, 13, 50760, 50760, 961, 311, 584, 291, 645, 281, 16927, 264, 2063, 2445, 293, 909, 281, 309, 9714, 1413, 261, 18, 8889, 1804, 9714, 51154, 51154, 1413, 261, 19, 8889, 13, 51282, 51282, 400, 510, 286, 478, 445, 10875, 9714, 570, 309, 311, 257, 955, 1230, 11, 457, 604, 661, 534, 2416, 1230, 51536, 51536, 576, 312, 1392, 13, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.14978093986051627, "compression_ratio": 1.5555555555555556, "no_speech_prob": 8.939585313783027e-06}, {"id": 18, "seek": 5240, "start": 75.84, "end": 77.88, "text": " would be okay.", "tokens": [50364, 961, 311, 584, 2602, 295, 46608, 341, 10024, 2445, 11, 341, 307, 257, 2063, 2445, 337, 8213, 50672, 50672, 24590, 13, 50760, 50760, 961, 311, 584, 291, 645, 281, 16927, 264, 2063, 2445, 293, 909, 281, 309, 9714, 1413, 261, 18, 8889, 1804, 9714, 51154, 51154, 1413, 261, 19, 8889, 13, 51282, 51282, 400, 510, 286, 478, 445, 10875, 9714, 570, 309, 311, 257, 955, 1230, 11, 457, 604, 661, 534, 2416, 1230, 51536, 51536, 576, 312, 1392, 13, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.14978093986051627, "compression_ratio": 1.5555555555555556, "no_speech_prob": 8.939585313783027e-06}, {"id": 19, "seek": 7788, "start": 77.88, "end": 84.24, "text": " So with this modified cost function, you'd in fact be penalizing the model if w3 and", "tokens": [50364, 407, 365, 341, 15873, 2063, 2445, 11, 291, 1116, 294, 1186, 312, 13661, 3319, 264, 2316, 498, 261, 18, 293, 50682, 50682, 261, 19, 366, 2416, 13, 50798, 50798, 1436, 498, 291, 528, 281, 17522, 341, 2445, 11, 264, 787, 636, 281, 652, 341, 777, 2063, 2445, 51038, 51038, 1359, 307, 498, 261, 18, 293, 261, 19, 366, 1293, 1359, 11, 558, 30, 51256, 51256, 1436, 5911, 341, 9714, 1413, 261, 18, 8889, 293, 9714, 1413, 261, 19, 8889, 2115, 366, 516, 51588, 51588, 281, 312, 534, 11, 534, 955, 13, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.08978153027986226, "compression_ratio": 1.70935960591133, "no_speech_prob": 7.224423370644217e-07}, {"id": 20, "seek": 7788, "start": 84.24, "end": 86.56, "text": " w4 are large.", "tokens": [50364, 407, 365, 341, 15873, 2063, 2445, 11, 291, 1116, 294, 1186, 312, 13661, 3319, 264, 2316, 498, 261, 18, 293, 50682, 50682, 261, 19, 366, 2416, 13, 50798, 50798, 1436, 498, 291, 528, 281, 17522, 341, 2445, 11, 264, 787, 636, 281, 652, 341, 777, 2063, 2445, 51038, 51038, 1359, 307, 498, 261, 18, 293, 261, 19, 366, 1293, 1359, 11, 558, 30, 51256, 51256, 1436, 5911, 341, 9714, 1413, 261, 18, 8889, 293, 9714, 1413, 261, 19, 8889, 2115, 366, 516, 51588, 51588, 281, 312, 534, 11, 534, 955, 13, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.08978153027986226, "compression_ratio": 1.70935960591133, "no_speech_prob": 7.224423370644217e-07}, {"id": 21, "seek": 7788, "start": 86.56, "end": 91.36, "text": " Because if you want to minimize this function, the only way to make this new cost function", "tokens": [50364, 407, 365, 341, 15873, 2063, 2445, 11, 291, 1116, 294, 1186, 312, 13661, 3319, 264, 2316, 498, 261, 18, 293, 50682, 50682, 261, 19, 366, 2416, 13, 50798, 50798, 1436, 498, 291, 528, 281, 17522, 341, 2445, 11, 264, 787, 636, 281, 652, 341, 777, 2063, 2445, 51038, 51038, 1359, 307, 498, 261, 18, 293, 261, 19, 366, 1293, 1359, 11, 558, 30, 51256, 51256, 1436, 5911, 341, 9714, 1413, 261, 18, 8889, 293, 9714, 1413, 261, 19, 8889, 2115, 366, 516, 51588, 51588, 281, 312, 534, 11, 534, 955, 13, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.08978153027986226, "compression_ratio": 1.70935960591133, "no_speech_prob": 7.224423370644217e-07}, {"id": 22, "seek": 7788, "start": 91.36, "end": 95.72, "text": " small is if w3 and w4 are both small, right?", "tokens": [50364, 407, 365, 341, 15873, 2063, 2445, 11, 291, 1116, 294, 1186, 312, 13661, 3319, 264, 2316, 498, 261, 18, 293, 50682, 50682, 261, 19, 366, 2416, 13, 50798, 50798, 1436, 498, 291, 528, 281, 17522, 341, 2445, 11, 264, 787, 636, 281, 652, 341, 777, 2063, 2445, 51038, 51038, 1359, 307, 498, 261, 18, 293, 261, 19, 366, 1293, 1359, 11, 558, 30, 51256, 51256, 1436, 5911, 341, 9714, 1413, 261, 18, 8889, 293, 9714, 1413, 261, 19, 8889, 2115, 366, 516, 51588, 51588, 281, 312, 534, 11, 534, 955, 13, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.08978153027986226, "compression_ratio": 1.70935960591133, "no_speech_prob": 7.224423370644217e-07}, {"id": 23, "seek": 7788, "start": 95.72, "end": 102.36, "text": " Because otherwise this 1000 times w3 squared and 1000 times w4 squared terms are going", "tokens": [50364, 407, 365, 341, 15873, 2063, 2445, 11, 291, 1116, 294, 1186, 312, 13661, 3319, 264, 2316, 498, 261, 18, 293, 50682, 50682, 261, 19, 366, 2416, 13, 50798, 50798, 1436, 498, 291, 528, 281, 17522, 341, 2445, 11, 264, 787, 636, 281, 652, 341, 777, 2063, 2445, 51038, 51038, 1359, 307, 498, 261, 18, 293, 261, 19, 366, 1293, 1359, 11, 558, 30, 51256, 51256, 1436, 5911, 341, 9714, 1413, 261, 18, 8889, 293, 9714, 1413, 261, 19, 8889, 2115, 366, 516, 51588, 51588, 281, 312, 534, 11, 534, 955, 13, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.08978153027986226, "compression_ratio": 1.70935960591133, "no_speech_prob": 7.224423370644217e-07}, {"id": 24, "seek": 7788, "start": 102.36, "end": 105.25999999999999, "text": " to be really, really big.", "tokens": [50364, 407, 365, 341, 15873, 2063, 2445, 11, 291, 1116, 294, 1186, 312, 13661, 3319, 264, 2316, 498, 261, 18, 293, 50682, 50682, 261, 19, 366, 2416, 13, 50798, 50798, 1436, 498, 291, 528, 281, 17522, 341, 2445, 11, 264, 787, 636, 281, 652, 341, 777, 2063, 2445, 51038, 51038, 1359, 307, 498, 261, 18, 293, 261, 19, 366, 1293, 1359, 11, 558, 30, 51256, 51256, 1436, 5911, 341, 9714, 1413, 261, 18, 8889, 293, 9714, 1413, 261, 19, 8889, 2115, 366, 516, 51588, 51588, 281, 312, 534, 11, 534, 955, 13, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.08978153027986226, "compression_ratio": 1.70935960591133, "no_speech_prob": 7.224423370644217e-07}, {"id": 25, "seek": 10526, "start": 105.26, "end": 111.04, "text": " So when you minimize this function, you're going to end up with w3 close to zero and", "tokens": [50364, 407, 562, 291, 17522, 341, 2445, 11, 291, 434, 516, 281, 917, 493, 365, 261, 18, 1998, 281, 4018, 293, 50653, 50653, 261, 19, 1998, 281, 4018, 13, 50775, 50775, 407, 321, 434, 8659, 6217, 10373, 278, 484, 264, 1802, 295, 264, 4122, 2031, 36510, 293, 2031, 281, 51127, 51127, 264, 1347, 295, 1451, 293, 1242, 3973, 295, 613, 732, 2115, 670, 510, 13, 51396, 51396, 400, 498, 321, 360, 300, 11, 550, 321, 917, 493, 365, 257, 3318, 281, 264, 1412, 300, 311, 709, 4966, 281, 264, 37262, 51633, 51633], "temperature": 0.0, "avg_logprob": -0.08536556202878234, "compression_ratio": 1.6586538461538463, "no_speech_prob": 1.1911024557775818e-06}, {"id": 26, "seek": 10526, "start": 111.04, "end": 113.48, "text": " w4 close to zero.", "tokens": [50364, 407, 562, 291, 17522, 341, 2445, 11, 291, 434, 516, 281, 917, 493, 365, 261, 18, 1998, 281, 4018, 293, 50653, 50653, 261, 19, 1998, 281, 4018, 13, 50775, 50775, 407, 321, 434, 8659, 6217, 10373, 278, 484, 264, 1802, 295, 264, 4122, 2031, 36510, 293, 2031, 281, 51127, 51127, 264, 1347, 295, 1451, 293, 1242, 3973, 295, 613, 732, 2115, 670, 510, 13, 51396, 51396, 400, 498, 321, 360, 300, 11, 550, 321, 917, 493, 365, 257, 3318, 281, 264, 1412, 300, 311, 709, 4966, 281, 264, 37262, 51633, 51633], "temperature": 0.0, "avg_logprob": -0.08536556202878234, "compression_ratio": 1.6586538461538463, "no_speech_prob": 1.1911024557775818e-06}, {"id": 27, "seek": 10526, "start": 113.48, "end": 120.52000000000001, "text": " So we're effectively nearly canceling out the effect of the features x cubed and x to", "tokens": [50364, 407, 562, 291, 17522, 341, 2445, 11, 291, 434, 516, 281, 917, 493, 365, 261, 18, 1998, 281, 4018, 293, 50653, 50653, 261, 19, 1998, 281, 4018, 13, 50775, 50775, 407, 321, 434, 8659, 6217, 10373, 278, 484, 264, 1802, 295, 264, 4122, 2031, 36510, 293, 2031, 281, 51127, 51127, 264, 1347, 295, 1451, 293, 1242, 3973, 295, 613, 732, 2115, 670, 510, 13, 51396, 51396, 400, 498, 321, 360, 300, 11, 550, 321, 917, 493, 365, 257, 3318, 281, 264, 1412, 300, 311, 709, 4966, 281, 264, 37262, 51633, 51633], "temperature": 0.0, "avg_logprob": -0.08536556202878234, "compression_ratio": 1.6586538461538463, "no_speech_prob": 1.1911024557775818e-06}, {"id": 28, "seek": 10526, "start": 120.52000000000001, "end": 125.9, "text": " the power of four and getting rid of these two terms over here.", "tokens": [50364, 407, 562, 291, 17522, 341, 2445, 11, 291, 434, 516, 281, 917, 493, 365, 261, 18, 1998, 281, 4018, 293, 50653, 50653, 261, 19, 1998, 281, 4018, 13, 50775, 50775, 407, 321, 434, 8659, 6217, 10373, 278, 484, 264, 1802, 295, 264, 4122, 2031, 36510, 293, 2031, 281, 51127, 51127, 264, 1347, 295, 1451, 293, 1242, 3973, 295, 613, 732, 2115, 670, 510, 13, 51396, 51396, 400, 498, 321, 360, 300, 11, 550, 321, 917, 493, 365, 257, 3318, 281, 264, 1412, 300, 311, 709, 4966, 281, 264, 37262, 51633, 51633], "temperature": 0.0, "avg_logprob": -0.08536556202878234, "compression_ratio": 1.6586538461538463, "no_speech_prob": 1.1911024557775818e-06}, {"id": 29, "seek": 10526, "start": 125.9, "end": 130.64000000000001, "text": " And if we do that, then we end up with a fit to the data that's much closer to the quadratic", "tokens": [50364, 407, 562, 291, 17522, 341, 2445, 11, 291, 434, 516, 281, 917, 493, 365, 261, 18, 1998, 281, 4018, 293, 50653, 50653, 261, 19, 1998, 281, 4018, 13, 50775, 50775, 407, 321, 434, 8659, 6217, 10373, 278, 484, 264, 1802, 295, 264, 4122, 2031, 36510, 293, 2031, 281, 51127, 51127, 264, 1347, 295, 1451, 293, 1242, 3973, 295, 613, 732, 2115, 670, 510, 13, 51396, 51396, 400, 498, 321, 360, 300, 11, 550, 321, 917, 493, 365, 257, 3318, 281, 264, 1412, 300, 311, 709, 4966, 281, 264, 37262, 51633, 51633], "temperature": 0.0, "avg_logprob": -0.08536556202878234, "compression_ratio": 1.6586538461538463, "no_speech_prob": 1.1911024557775818e-06}, {"id": 30, "seek": 13064, "start": 130.64, "end": 138.0, "text": " function, including maybe just tiny contributions from the features x cubed and x to the fourth.", "tokens": [50364, 2445, 11, 3009, 1310, 445, 5870, 15725, 490, 264, 4122, 2031, 36510, 293, 2031, 281, 264, 6409, 13, 50732, 50732, 400, 341, 307, 665, 570, 309, 311, 257, 709, 1101, 3318, 281, 264, 1412, 5347, 281, 498, 439, 264, 9834, 51008, 51008, 727, 312, 2416, 293, 291, 917, 493, 365, 341, 261, 46737, 37262, 2445, 13, 51224, 51224, 5048, 5101, 11, 510, 311, 264, 1558, 2261, 3890, 2144, 13, 51392, 51392, 440, 1558, 307, 300, 498, 456, 366, 4356, 4190, 337, 264, 9834, 11, 550, 300, 311, 257, 857, 411, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07644082130269801, "compression_ratio": 1.639344262295082, "no_speech_prob": 7.571100013592513e-07}, {"id": 31, "seek": 13064, "start": 138.0, "end": 143.51999999999998, "text": " And this is good because it's a much better fit to the data compared to if all the parameters", "tokens": [50364, 2445, 11, 3009, 1310, 445, 5870, 15725, 490, 264, 4122, 2031, 36510, 293, 2031, 281, 264, 6409, 13, 50732, 50732, 400, 341, 307, 665, 570, 309, 311, 257, 709, 1101, 3318, 281, 264, 1412, 5347, 281, 498, 439, 264, 9834, 51008, 51008, 727, 312, 2416, 293, 291, 917, 493, 365, 341, 261, 46737, 37262, 2445, 13, 51224, 51224, 5048, 5101, 11, 510, 311, 264, 1558, 2261, 3890, 2144, 13, 51392, 51392, 440, 1558, 307, 300, 498, 456, 366, 4356, 4190, 337, 264, 9834, 11, 550, 300, 311, 257, 857, 411, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07644082130269801, "compression_ratio": 1.639344262295082, "no_speech_prob": 7.571100013592513e-07}, {"id": 32, "seek": 13064, "start": 143.51999999999998, "end": 147.83999999999997, "text": " could be large and you end up with this wiggly quadratic function.", "tokens": [50364, 2445, 11, 3009, 1310, 445, 5870, 15725, 490, 264, 4122, 2031, 36510, 293, 2031, 281, 264, 6409, 13, 50732, 50732, 400, 341, 307, 665, 570, 309, 311, 257, 709, 1101, 3318, 281, 264, 1412, 5347, 281, 498, 439, 264, 9834, 51008, 51008, 727, 312, 2416, 293, 291, 917, 493, 365, 341, 261, 46737, 37262, 2445, 13, 51224, 51224, 5048, 5101, 11, 510, 311, 264, 1558, 2261, 3890, 2144, 13, 51392, 51392, 440, 1558, 307, 300, 498, 456, 366, 4356, 4190, 337, 264, 9834, 11, 550, 300, 311, 257, 857, 411, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07644082130269801, "compression_ratio": 1.639344262295082, "no_speech_prob": 7.571100013592513e-07}, {"id": 33, "seek": 13064, "start": 147.83999999999997, "end": 151.2, "text": " More generally, here's the idea behind regularization.", "tokens": [50364, 2445, 11, 3009, 1310, 445, 5870, 15725, 490, 264, 4122, 2031, 36510, 293, 2031, 281, 264, 6409, 13, 50732, 50732, 400, 341, 307, 665, 570, 309, 311, 257, 709, 1101, 3318, 281, 264, 1412, 5347, 281, 498, 439, 264, 9834, 51008, 51008, 727, 312, 2416, 293, 291, 917, 493, 365, 341, 261, 46737, 37262, 2445, 13, 51224, 51224, 5048, 5101, 11, 510, 311, 264, 1558, 2261, 3890, 2144, 13, 51392, 51392, 440, 1558, 307, 300, 498, 456, 366, 4356, 4190, 337, 264, 9834, 11, 550, 300, 311, 257, 857, 411, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07644082130269801, "compression_ratio": 1.639344262295082, "no_speech_prob": 7.571100013592513e-07}, {"id": 34, "seek": 13064, "start": 151.2, "end": 156.64, "text": " The idea is that if there are smaller values for the parameters, then that's a bit like", "tokens": [50364, 2445, 11, 3009, 1310, 445, 5870, 15725, 490, 264, 4122, 2031, 36510, 293, 2031, 281, 264, 6409, 13, 50732, 50732, 400, 341, 307, 665, 570, 309, 311, 257, 709, 1101, 3318, 281, 264, 1412, 5347, 281, 498, 439, 264, 9834, 51008, 51008, 727, 312, 2416, 293, 291, 917, 493, 365, 341, 261, 46737, 37262, 2445, 13, 51224, 51224, 5048, 5101, 11, 510, 311, 264, 1558, 2261, 3890, 2144, 13, 51392, 51392, 440, 1558, 307, 300, 498, 456, 366, 4356, 4190, 337, 264, 9834, 11, 550, 300, 311, 257, 857, 411, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07644082130269801, "compression_ratio": 1.639344262295082, "no_speech_prob": 7.571100013592513e-07}, {"id": 35, "seek": 15664, "start": 156.64, "end": 161.83999999999997, "text": " having a simpler model, maybe one with fewer features, which is therefore less prone to", "tokens": [50364, 1419, 257, 18587, 2316, 11, 1310, 472, 365, 13366, 4122, 11, 597, 307, 4412, 1570, 25806, 281, 50624, 50624, 670, 69, 2414, 13, 50750, 50750, 1282, 264, 1036, 4137, 11, 321, 13661, 1125, 420, 321, 584, 321, 3890, 1125, 787, 261, 18, 293, 261, 19, 13, 51156, 51156, 583, 544, 5101, 11, 264, 636, 300, 3890, 2144, 12258, 281, 312, 12270, 307, 498, 291, 362, 257, 688, 51442, 51442, 295, 4122, 11, 584, 2319, 4122, 11, 291, 815, 406, 458, 597, 366, 264, 881, 1021, 4122, 51696, 51696, 293, 597, 2306, 281, 13661, 1125, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11890506744384766, "compression_ratio": 1.6434782608695653, "no_speech_prob": 1.602802285560756e-06}, {"id": 36, "seek": 15664, "start": 161.83999999999997, "end": 164.35999999999999, "text": " overfitting.", "tokens": [50364, 1419, 257, 18587, 2316, 11, 1310, 472, 365, 13366, 4122, 11, 597, 307, 4412, 1570, 25806, 281, 50624, 50624, 670, 69, 2414, 13, 50750, 50750, 1282, 264, 1036, 4137, 11, 321, 13661, 1125, 420, 321, 584, 321, 3890, 1125, 787, 261, 18, 293, 261, 19, 13, 51156, 51156, 583, 544, 5101, 11, 264, 636, 300, 3890, 2144, 12258, 281, 312, 12270, 307, 498, 291, 362, 257, 688, 51442, 51442, 295, 4122, 11, 584, 2319, 4122, 11, 291, 815, 406, 458, 597, 366, 264, 881, 1021, 4122, 51696, 51696, 293, 597, 2306, 281, 13661, 1125, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11890506744384766, "compression_ratio": 1.6434782608695653, "no_speech_prob": 1.602802285560756e-06}, {"id": 37, "seek": 15664, "start": 164.35999999999999, "end": 172.48, "text": " On the last slide, we penalize or we say we regularize only w3 and w4.", "tokens": [50364, 1419, 257, 18587, 2316, 11, 1310, 472, 365, 13366, 4122, 11, 597, 307, 4412, 1570, 25806, 281, 50624, 50624, 670, 69, 2414, 13, 50750, 50750, 1282, 264, 1036, 4137, 11, 321, 13661, 1125, 420, 321, 584, 321, 3890, 1125, 787, 261, 18, 293, 261, 19, 13, 51156, 51156, 583, 544, 5101, 11, 264, 636, 300, 3890, 2144, 12258, 281, 312, 12270, 307, 498, 291, 362, 257, 688, 51442, 51442, 295, 4122, 11, 584, 2319, 4122, 11, 291, 815, 406, 458, 597, 366, 264, 881, 1021, 4122, 51696, 51696, 293, 597, 2306, 281, 13661, 1125, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11890506744384766, "compression_ratio": 1.6434782608695653, "no_speech_prob": 1.602802285560756e-06}, {"id": 38, "seek": 15664, "start": 172.48, "end": 178.2, "text": " But more generally, the way that regularization tends to be implemented is if you have a lot", "tokens": [50364, 1419, 257, 18587, 2316, 11, 1310, 472, 365, 13366, 4122, 11, 597, 307, 4412, 1570, 25806, 281, 50624, 50624, 670, 69, 2414, 13, 50750, 50750, 1282, 264, 1036, 4137, 11, 321, 13661, 1125, 420, 321, 584, 321, 3890, 1125, 787, 261, 18, 293, 261, 19, 13, 51156, 51156, 583, 544, 5101, 11, 264, 636, 300, 3890, 2144, 12258, 281, 312, 12270, 307, 498, 291, 362, 257, 688, 51442, 51442, 295, 4122, 11, 584, 2319, 4122, 11, 291, 815, 406, 458, 597, 366, 264, 881, 1021, 4122, 51696, 51696, 293, 597, 2306, 281, 13661, 1125, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11890506744384766, "compression_ratio": 1.6434782608695653, "no_speech_prob": 1.602802285560756e-06}, {"id": 39, "seek": 15664, "start": 178.2, "end": 183.27999999999997, "text": " of features, say 100 features, you may not know which are the most important features", "tokens": [50364, 1419, 257, 18587, 2316, 11, 1310, 472, 365, 13366, 4122, 11, 597, 307, 4412, 1570, 25806, 281, 50624, 50624, 670, 69, 2414, 13, 50750, 50750, 1282, 264, 1036, 4137, 11, 321, 13661, 1125, 420, 321, 584, 321, 3890, 1125, 787, 261, 18, 293, 261, 19, 13, 51156, 51156, 583, 544, 5101, 11, 264, 636, 300, 3890, 2144, 12258, 281, 312, 12270, 307, 498, 291, 362, 257, 688, 51442, 51442, 295, 4122, 11, 584, 2319, 4122, 11, 291, 815, 406, 458, 597, 366, 264, 881, 1021, 4122, 51696, 51696, 293, 597, 2306, 281, 13661, 1125, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11890506744384766, "compression_ratio": 1.6434782608695653, "no_speech_prob": 1.602802285560756e-06}, {"id": 40, "seek": 15664, "start": 183.27999999999997, "end": 185.39999999999998, "text": " and which ones to penalize.", "tokens": [50364, 1419, 257, 18587, 2316, 11, 1310, 472, 365, 13366, 4122, 11, 597, 307, 4412, 1570, 25806, 281, 50624, 50624, 670, 69, 2414, 13, 50750, 50750, 1282, 264, 1036, 4137, 11, 321, 13661, 1125, 420, 321, 584, 321, 3890, 1125, 787, 261, 18, 293, 261, 19, 13, 51156, 51156, 583, 544, 5101, 11, 264, 636, 300, 3890, 2144, 12258, 281, 312, 12270, 307, 498, 291, 362, 257, 688, 51442, 51442, 295, 4122, 11, 584, 2319, 4122, 11, 291, 815, 406, 458, 597, 366, 264, 881, 1021, 4122, 51696, 51696, 293, 597, 2306, 281, 13661, 1125, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11890506744384766, "compression_ratio": 1.6434782608695653, "no_speech_prob": 1.602802285560756e-06}, {"id": 41, "seek": 18540, "start": 185.4, "end": 190.20000000000002, "text": " So the way regularization is typically implemented is to penalize all of the features or more", "tokens": [50364, 407, 264, 636, 3890, 2144, 307, 5850, 12270, 307, 281, 13661, 1125, 439, 295, 264, 4122, 420, 544, 50604, 50604, 13402, 11, 291, 13661, 1125, 439, 264, 261, 73, 9834, 13, 50852, 50852, 400, 309, 311, 1944, 281, 855, 300, 341, 486, 2673, 1874, 294, 15669, 257, 28640, 11, 18587, 11, 1570, 51119, 51119, 261, 46737, 2445, 300, 311, 1570, 25806, 281, 670, 69, 2414, 13, 51318, 51318, 407, 337, 341, 1365, 11, 498, 291, 362, 1412, 365, 2319, 4122, 337, 1184, 1782, 11, 309, 815, 312, 1152, 51530, 51530, 281, 1888, 294, 7295, 597, 4122, 281, 4090, 293, 597, 2306, 281, 33536, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.09616604557743778, "compression_ratio": 1.6806083650190113, "no_speech_prob": 9.422397511116287e-07}, {"id": 42, "seek": 18540, "start": 190.20000000000002, "end": 195.16, "text": " precisely, you penalize all the wj parameters.", "tokens": [50364, 407, 264, 636, 3890, 2144, 307, 5850, 12270, 307, 281, 13661, 1125, 439, 295, 264, 4122, 420, 544, 50604, 50604, 13402, 11, 291, 13661, 1125, 439, 264, 261, 73, 9834, 13, 50852, 50852, 400, 309, 311, 1944, 281, 855, 300, 341, 486, 2673, 1874, 294, 15669, 257, 28640, 11, 18587, 11, 1570, 51119, 51119, 261, 46737, 2445, 300, 311, 1570, 25806, 281, 670, 69, 2414, 13, 51318, 51318, 407, 337, 341, 1365, 11, 498, 291, 362, 1412, 365, 2319, 4122, 337, 1184, 1782, 11, 309, 815, 312, 1152, 51530, 51530, 281, 1888, 294, 7295, 597, 4122, 281, 4090, 293, 597, 2306, 281, 33536, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.09616604557743778, "compression_ratio": 1.6806083650190113, "no_speech_prob": 9.422397511116287e-07}, {"id": 43, "seek": 18540, "start": 195.16, "end": 200.5, "text": " And it's possible to show that this will usually result in fitting a smoother, simpler, less", "tokens": [50364, 407, 264, 636, 3890, 2144, 307, 5850, 12270, 307, 281, 13661, 1125, 439, 295, 264, 4122, 420, 544, 50604, 50604, 13402, 11, 291, 13661, 1125, 439, 264, 261, 73, 9834, 13, 50852, 50852, 400, 309, 311, 1944, 281, 855, 300, 341, 486, 2673, 1874, 294, 15669, 257, 28640, 11, 18587, 11, 1570, 51119, 51119, 261, 46737, 2445, 300, 311, 1570, 25806, 281, 670, 69, 2414, 13, 51318, 51318, 407, 337, 341, 1365, 11, 498, 291, 362, 1412, 365, 2319, 4122, 337, 1184, 1782, 11, 309, 815, 312, 1152, 51530, 51530, 281, 1888, 294, 7295, 597, 4122, 281, 4090, 293, 597, 2306, 281, 33536, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.09616604557743778, "compression_ratio": 1.6806083650190113, "no_speech_prob": 9.422397511116287e-07}, {"id": 44, "seek": 18540, "start": 200.5, "end": 204.48000000000002, "text": " wiggly function that's less prone to overfitting.", "tokens": [50364, 407, 264, 636, 3890, 2144, 307, 5850, 12270, 307, 281, 13661, 1125, 439, 295, 264, 4122, 420, 544, 50604, 50604, 13402, 11, 291, 13661, 1125, 439, 264, 261, 73, 9834, 13, 50852, 50852, 400, 309, 311, 1944, 281, 855, 300, 341, 486, 2673, 1874, 294, 15669, 257, 28640, 11, 18587, 11, 1570, 51119, 51119, 261, 46737, 2445, 300, 311, 1570, 25806, 281, 670, 69, 2414, 13, 51318, 51318, 407, 337, 341, 1365, 11, 498, 291, 362, 1412, 365, 2319, 4122, 337, 1184, 1782, 11, 309, 815, 312, 1152, 51530, 51530, 281, 1888, 294, 7295, 597, 4122, 281, 4090, 293, 597, 2306, 281, 33536, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.09616604557743778, "compression_ratio": 1.6806083650190113, "no_speech_prob": 9.422397511116287e-07}, {"id": 45, "seek": 18540, "start": 204.48000000000002, "end": 208.72, "text": " So for this example, if you have data with 100 features for each house, it may be hard", "tokens": [50364, 407, 264, 636, 3890, 2144, 307, 5850, 12270, 307, 281, 13661, 1125, 439, 295, 264, 4122, 420, 544, 50604, 50604, 13402, 11, 291, 13661, 1125, 439, 264, 261, 73, 9834, 13, 50852, 50852, 400, 309, 311, 1944, 281, 855, 300, 341, 486, 2673, 1874, 294, 15669, 257, 28640, 11, 18587, 11, 1570, 51119, 51119, 261, 46737, 2445, 300, 311, 1570, 25806, 281, 670, 69, 2414, 13, 51318, 51318, 407, 337, 341, 1365, 11, 498, 291, 362, 1412, 365, 2319, 4122, 337, 1184, 1782, 11, 309, 815, 312, 1152, 51530, 51530, 281, 1888, 294, 7295, 597, 4122, 281, 4090, 293, 597, 2306, 281, 33536, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.09616604557743778, "compression_ratio": 1.6806083650190113, "no_speech_prob": 9.422397511116287e-07}, {"id": 46, "seek": 18540, "start": 208.72, "end": 212.76, "text": " to pick in advance which features to include and which ones to exclude.", "tokens": [50364, 407, 264, 636, 3890, 2144, 307, 5850, 12270, 307, 281, 13661, 1125, 439, 295, 264, 4122, 420, 544, 50604, 50604, 13402, 11, 291, 13661, 1125, 439, 264, 261, 73, 9834, 13, 50852, 50852, 400, 309, 311, 1944, 281, 855, 300, 341, 486, 2673, 1874, 294, 15669, 257, 28640, 11, 18587, 11, 1570, 51119, 51119, 261, 46737, 2445, 300, 311, 1570, 25806, 281, 670, 69, 2414, 13, 51318, 51318, 407, 337, 341, 1365, 11, 498, 291, 362, 1412, 365, 2319, 4122, 337, 1184, 1782, 11, 309, 815, 312, 1152, 51530, 51530, 281, 1888, 294, 7295, 597, 4122, 281, 4090, 293, 597, 2306, 281, 33536, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.09616604557743778, "compression_ratio": 1.6806083650190113, "no_speech_prob": 9.422397511116287e-07}, {"id": 47, "seek": 21276, "start": 212.76, "end": 217.64, "text": " So let's build a model that uses all 100 features.", "tokens": [50364, 407, 718, 311, 1322, 257, 2316, 300, 4960, 439, 2319, 4122, 13, 50608, 50608, 407, 291, 362, 613, 2319, 9834, 11, 261, 16, 807, 261, 6879, 11, 382, 731, 382, 264, 21055, 372, 13075, 272, 13, 51108, 51108, 1436, 321, 500, 380, 458, 597, 295, 613, 9834, 366, 516, 281, 312, 264, 1021, 2306, 11, 718, 311, 51324, 51324, 13661, 1125, 439, 295, 552, 257, 857, 293, 23060, 439, 295, 552, 538, 5127, 341, 777, 1433, 13607, 1413, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.1024458233903094, "compression_ratio": 1.5918367346938775, "no_speech_prob": 2.8572994779096916e-06}, {"id": 48, "seek": 21276, "start": 217.64, "end": 227.64, "text": " So you have these 100 parameters, w1 through w100, as well as the 101st parameter b.", "tokens": [50364, 407, 718, 311, 1322, 257, 2316, 300, 4960, 439, 2319, 4122, 13, 50608, 50608, 407, 291, 362, 613, 2319, 9834, 11, 261, 16, 807, 261, 6879, 11, 382, 731, 382, 264, 21055, 372, 13075, 272, 13, 51108, 51108, 1436, 321, 500, 380, 458, 597, 295, 613, 9834, 366, 516, 281, 312, 264, 1021, 2306, 11, 718, 311, 51324, 51324, 13661, 1125, 439, 295, 552, 257, 857, 293, 23060, 439, 295, 552, 538, 5127, 341, 777, 1433, 13607, 1413, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.1024458233903094, "compression_ratio": 1.5918367346938775, "no_speech_prob": 2.8572994779096916e-06}, {"id": 49, "seek": 21276, "start": 227.64, "end": 231.95999999999998, "text": " Because we don't know which of these parameters are going to be the important ones, let's", "tokens": [50364, 407, 718, 311, 1322, 257, 2316, 300, 4960, 439, 2319, 4122, 13, 50608, 50608, 407, 291, 362, 613, 2319, 9834, 11, 261, 16, 807, 261, 6879, 11, 382, 731, 382, 264, 21055, 372, 13075, 272, 13, 51108, 51108, 1436, 321, 500, 380, 458, 597, 295, 613, 9834, 366, 516, 281, 312, 264, 1021, 2306, 11, 718, 311, 51324, 51324, 13661, 1125, 439, 295, 552, 257, 857, 293, 23060, 439, 295, 552, 538, 5127, 341, 777, 1433, 13607, 1413, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.1024458233903094, "compression_ratio": 1.5918367346938775, "no_speech_prob": 2.8572994779096916e-06}, {"id": 50, "seek": 21276, "start": 231.95999999999998, "end": 239.35999999999999, "text": " penalize all of them a bit and shrink all of them by adding this new term lambda times", "tokens": [50364, 407, 718, 311, 1322, 257, 2316, 300, 4960, 439, 2319, 4122, 13, 50608, 50608, 407, 291, 362, 613, 2319, 9834, 11, 261, 16, 807, 261, 6879, 11, 382, 731, 382, 264, 21055, 372, 13075, 272, 13, 51108, 51108, 1436, 321, 500, 380, 458, 597, 295, 613, 9834, 366, 516, 281, 312, 264, 1021, 2306, 11, 718, 311, 51324, 51324, 13661, 1125, 439, 295, 552, 257, 857, 293, 23060, 439, 295, 552, 538, 5127, 341, 777, 1433, 13607, 1413, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.1024458233903094, "compression_ratio": 1.5918367346938775, "no_speech_prob": 2.8572994779096916e-06}, {"id": 51, "seek": 23936, "start": 239.36, "end": 248.88000000000002, "text": " the sum from j equals 1 through n, where n is 100, the number of features of wj squared.", "tokens": [50364, 264, 2408, 490, 361, 6915, 502, 807, 297, 11, 689, 297, 307, 2319, 11, 264, 1230, 295, 4122, 295, 261, 73, 8889, 13, 50840, 50840, 639, 2158, 13607, 510, 307, 264, 10281, 23339, 13607, 11, 293, 309, 311, 611, 1219, 257, 3890, 2144, 51208, 51208, 13075, 13, 51296, 51296, 407, 2531, 281, 8867, 257, 2539, 3314, 8961, 11, 291, 586, 611, 362, 281, 2826, 257, 1230, 337, 13607, 13, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.14297988316784166, "compression_ratio": 1.486910994764398, "no_speech_prob": 8.851538950693794e-07}, {"id": 52, "seek": 23936, "start": 248.88000000000002, "end": 256.24, "text": " This value lambda here is the Greek alphabet lambda, and it's also called a regularization", "tokens": [50364, 264, 2408, 490, 361, 6915, 502, 807, 297, 11, 689, 297, 307, 2319, 11, 264, 1230, 295, 4122, 295, 261, 73, 8889, 13, 50840, 50840, 639, 2158, 13607, 510, 307, 264, 10281, 23339, 13607, 11, 293, 309, 311, 611, 1219, 257, 3890, 2144, 51208, 51208, 13075, 13, 51296, 51296, 407, 2531, 281, 8867, 257, 2539, 3314, 8961, 11, 291, 586, 611, 362, 281, 2826, 257, 1230, 337, 13607, 13, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.14297988316784166, "compression_ratio": 1.486910994764398, "no_speech_prob": 8.851538950693794e-07}, {"id": 53, "seek": 23936, "start": 256.24, "end": 258.0, "text": " parameter.", "tokens": [50364, 264, 2408, 490, 361, 6915, 502, 807, 297, 11, 689, 297, 307, 2319, 11, 264, 1230, 295, 4122, 295, 261, 73, 8889, 13, 50840, 50840, 639, 2158, 13607, 510, 307, 264, 10281, 23339, 13607, 11, 293, 309, 311, 611, 1219, 257, 3890, 2144, 51208, 51208, 13075, 13, 51296, 51296, 407, 2531, 281, 8867, 257, 2539, 3314, 8961, 11, 291, 586, 611, 362, 281, 2826, 257, 1230, 337, 13607, 13, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.14297988316784166, "compression_ratio": 1.486910994764398, "no_speech_prob": 8.851538950693794e-07}, {"id": 54, "seek": 23936, "start": 258.0, "end": 265.92, "text": " So similar to picking a learning rate alpha, you now also have to choose a number for lambda.", "tokens": [50364, 264, 2408, 490, 361, 6915, 502, 807, 297, 11, 689, 297, 307, 2319, 11, 264, 1230, 295, 4122, 295, 261, 73, 8889, 13, 50840, 50840, 639, 2158, 13607, 510, 307, 264, 10281, 23339, 13607, 11, 293, 309, 311, 611, 1219, 257, 3890, 2144, 51208, 51208, 13075, 13, 51296, 51296, 407, 2531, 281, 8867, 257, 2539, 3314, 8961, 11, 291, 586, 611, 362, 281, 2826, 257, 1230, 337, 13607, 13, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.14297988316784166, "compression_ratio": 1.486910994764398, "no_speech_prob": 8.851538950693794e-07}, {"id": 55, "seek": 26592, "start": 265.92, "end": 272.40000000000003, "text": " A couple things I would like to point out, by convention, instead of using lambda times", "tokens": [50364, 316, 1916, 721, 286, 576, 411, 281, 935, 484, 11, 538, 10286, 11, 2602, 295, 1228, 13607, 1413, 50688, 50688, 264, 2408, 295, 261, 73, 8889, 11, 321, 611, 9845, 13607, 538, 568, 76, 11, 370, 300, 1293, 264, 700, 293, 1150, 2115, 51088, 51088, 510, 366, 36039, 538, 502, 670, 568, 76, 13, 51286, 51286, 467, 4523, 484, 300, 538, 21589, 1293, 2115, 264, 912, 636, 11, 309, 3643, 257, 707, 857, 3571, 281, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.08071269264704065, "compression_ratio": 1.5153061224489797, "no_speech_prob": 3.39311691277544e-06}, {"id": 56, "seek": 26592, "start": 272.40000000000003, "end": 280.40000000000003, "text": " the sum of wj squared, we also divide lambda by 2m, so that both the first and second terms", "tokens": [50364, 316, 1916, 721, 286, 576, 411, 281, 935, 484, 11, 538, 10286, 11, 2602, 295, 1228, 13607, 1413, 50688, 50688, 264, 2408, 295, 261, 73, 8889, 11, 321, 611, 9845, 13607, 538, 568, 76, 11, 370, 300, 1293, 264, 700, 293, 1150, 2115, 51088, 51088, 510, 366, 36039, 538, 502, 670, 568, 76, 13, 51286, 51286, 467, 4523, 484, 300, 538, 21589, 1293, 2115, 264, 912, 636, 11, 309, 3643, 257, 707, 857, 3571, 281, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.08071269264704065, "compression_ratio": 1.5153061224489797, "no_speech_prob": 3.39311691277544e-06}, {"id": 57, "seek": 26592, "start": 280.40000000000003, "end": 284.36, "text": " here are scaled by 1 over 2m.", "tokens": [50364, 316, 1916, 721, 286, 576, 411, 281, 935, 484, 11, 538, 10286, 11, 2602, 295, 1228, 13607, 1413, 50688, 50688, 264, 2408, 295, 261, 73, 8889, 11, 321, 611, 9845, 13607, 538, 568, 76, 11, 370, 300, 1293, 264, 700, 293, 1150, 2115, 51088, 51088, 510, 366, 36039, 538, 502, 670, 568, 76, 13, 51286, 51286, 467, 4523, 484, 300, 538, 21589, 1293, 2115, 264, 912, 636, 11, 309, 3643, 257, 707, 857, 3571, 281, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.08071269264704065, "compression_ratio": 1.5153061224489797, "no_speech_prob": 3.39311691277544e-06}, {"id": 58, "seek": 26592, "start": 284.36, "end": 290.08000000000004, "text": " It turns out that by scaling both terms the same way, it becomes a little bit easier to", "tokens": [50364, 316, 1916, 721, 286, 576, 411, 281, 935, 484, 11, 538, 10286, 11, 2602, 295, 1228, 13607, 1413, 50688, 50688, 264, 2408, 295, 261, 73, 8889, 11, 321, 611, 9845, 13607, 538, 568, 76, 11, 370, 300, 1293, 264, 700, 293, 1150, 2115, 51088, 51088, 510, 366, 36039, 538, 502, 670, 568, 76, 13, 51286, 51286, 467, 4523, 484, 300, 538, 21589, 1293, 2115, 264, 912, 636, 11, 309, 3643, 257, 707, 857, 3571, 281, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.08071269264704065, "compression_ratio": 1.5153061224489797, "no_speech_prob": 3.39311691277544e-06}, {"id": 59, "seek": 29008, "start": 290.08, "end": 297.03999999999996, "text": " choose a good value for lambda, and in particular, you find that even if your training set size", "tokens": [50364, 2826, 257, 665, 2158, 337, 13607, 11, 293, 294, 1729, 11, 291, 915, 300, 754, 498, 428, 3097, 992, 2744, 50712, 50712, 13156, 11, 584, 291, 915, 544, 3097, 5110, 11, 370, 275, 11, 264, 3097, 992, 2744, 11, 307, 586, 3801, 11, 50998, 50998, 264, 912, 2158, 295, 13607, 300, 291, 6183, 8046, 307, 586, 611, 544, 3700, 281, 2354, 281, 589, 51332, 51332, 498, 291, 362, 341, 2857, 21589, 538, 568, 76, 13, 51532, 51532, 2743, 538, 264, 636, 11, 538, 10286, 11, 321, 434, 406, 516, 281, 13661, 1125, 264, 13075, 272, 337, 885, 51770, 51770, 2416, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.12820468175978889, "compression_ratio": 1.6872427983539096, "no_speech_prob": 1.9222517948946916e-05}, {"id": 60, "seek": 29008, "start": 297.03999999999996, "end": 302.76, "text": " grows, say you find more training examples, so m, the training set size, is now bigger,", "tokens": [50364, 2826, 257, 665, 2158, 337, 13607, 11, 293, 294, 1729, 11, 291, 915, 300, 754, 498, 428, 3097, 992, 2744, 50712, 50712, 13156, 11, 584, 291, 915, 544, 3097, 5110, 11, 370, 275, 11, 264, 3097, 992, 2744, 11, 307, 586, 3801, 11, 50998, 50998, 264, 912, 2158, 295, 13607, 300, 291, 6183, 8046, 307, 586, 611, 544, 3700, 281, 2354, 281, 589, 51332, 51332, 498, 291, 362, 341, 2857, 21589, 538, 568, 76, 13, 51532, 51532, 2743, 538, 264, 636, 11, 538, 10286, 11, 321, 434, 406, 516, 281, 13661, 1125, 264, 13075, 272, 337, 885, 51770, 51770, 2416, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.12820468175978889, "compression_ratio": 1.6872427983539096, "no_speech_prob": 1.9222517948946916e-05}, {"id": 61, "seek": 29008, "start": 302.76, "end": 309.44, "text": " the same value of lambda that you picked previously is now also more likely to continue to work", "tokens": [50364, 2826, 257, 665, 2158, 337, 13607, 11, 293, 294, 1729, 11, 291, 915, 300, 754, 498, 428, 3097, 992, 2744, 50712, 50712, 13156, 11, 584, 291, 915, 544, 3097, 5110, 11, 370, 275, 11, 264, 3097, 992, 2744, 11, 307, 586, 3801, 11, 50998, 50998, 264, 912, 2158, 295, 13607, 300, 291, 6183, 8046, 307, 586, 611, 544, 3700, 281, 2354, 281, 589, 51332, 51332, 498, 291, 362, 341, 2857, 21589, 538, 568, 76, 13, 51532, 51532, 2743, 538, 264, 636, 11, 538, 10286, 11, 321, 434, 406, 516, 281, 13661, 1125, 264, 13075, 272, 337, 885, 51770, 51770, 2416, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.12820468175978889, "compression_ratio": 1.6872427983539096, "no_speech_prob": 1.9222517948946916e-05}, {"id": 62, "seek": 29008, "start": 309.44, "end": 313.44, "text": " if you have this extra scaling by 2m.", "tokens": [50364, 2826, 257, 665, 2158, 337, 13607, 11, 293, 294, 1729, 11, 291, 915, 300, 754, 498, 428, 3097, 992, 2744, 50712, 50712, 13156, 11, 584, 291, 915, 544, 3097, 5110, 11, 370, 275, 11, 264, 3097, 992, 2744, 11, 307, 586, 3801, 11, 50998, 50998, 264, 912, 2158, 295, 13607, 300, 291, 6183, 8046, 307, 586, 611, 544, 3700, 281, 2354, 281, 589, 51332, 51332, 498, 291, 362, 341, 2857, 21589, 538, 568, 76, 13, 51532, 51532, 2743, 538, 264, 636, 11, 538, 10286, 11, 321, 434, 406, 516, 281, 13661, 1125, 264, 13075, 272, 337, 885, 51770, 51770, 2416, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.12820468175978889, "compression_ratio": 1.6872427983539096, "no_speech_prob": 1.9222517948946916e-05}, {"id": 63, "seek": 29008, "start": 313.44, "end": 318.2, "text": " Also by the way, by convention, we're not going to penalize the parameter b for being", "tokens": [50364, 2826, 257, 665, 2158, 337, 13607, 11, 293, 294, 1729, 11, 291, 915, 300, 754, 498, 428, 3097, 992, 2744, 50712, 50712, 13156, 11, 584, 291, 915, 544, 3097, 5110, 11, 370, 275, 11, 264, 3097, 992, 2744, 11, 307, 586, 3801, 11, 50998, 50998, 264, 912, 2158, 295, 13607, 300, 291, 6183, 8046, 307, 586, 611, 544, 3700, 281, 2354, 281, 589, 51332, 51332, 498, 291, 362, 341, 2857, 21589, 538, 568, 76, 13, 51532, 51532, 2743, 538, 264, 636, 11, 538, 10286, 11, 321, 434, 406, 516, 281, 13661, 1125, 264, 13075, 272, 337, 885, 51770, 51770, 2416, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.12820468175978889, "compression_ratio": 1.6872427983539096, "no_speech_prob": 1.9222517948946916e-05}, {"id": 64, "seek": 29008, "start": 318.2, "end": 319.2, "text": " large.", "tokens": [50364, 2826, 257, 665, 2158, 337, 13607, 11, 293, 294, 1729, 11, 291, 915, 300, 754, 498, 428, 3097, 992, 2744, 50712, 50712, 13156, 11, 584, 291, 915, 544, 3097, 5110, 11, 370, 275, 11, 264, 3097, 992, 2744, 11, 307, 586, 3801, 11, 50998, 50998, 264, 912, 2158, 295, 13607, 300, 291, 6183, 8046, 307, 586, 611, 544, 3700, 281, 2354, 281, 589, 51332, 51332, 498, 291, 362, 341, 2857, 21589, 538, 568, 76, 13, 51532, 51532, 2743, 538, 264, 636, 11, 538, 10286, 11, 321, 434, 406, 516, 281, 13661, 1125, 264, 13075, 272, 337, 885, 51770, 51770, 2416, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.12820468175978889, "compression_ratio": 1.6872427983539096, "no_speech_prob": 1.9222517948946916e-05}, {"id": 65, "seek": 31920, "start": 319.2, "end": 323.68, "text": " In practice, it makes very little difference whether you do or not, and some machine learning", "tokens": [50364, 682, 3124, 11, 309, 1669, 588, 707, 2649, 1968, 291, 360, 420, 406, 11, 293, 512, 3479, 2539, 50588, 50588, 11955, 11, 293, 767, 512, 2539, 9284, 4445, 763, 11, 486, 611, 4090, 13607, 50928, 50928, 670, 568, 76, 1413, 264, 272, 8889, 1433, 13, 51106, 51106, 583, 341, 1669, 588, 707, 2649, 294, 3124, 11, 293, 264, 544, 2689, 10286, 597, 321, 603, 51347, 51347, 764, 294, 341, 1164, 307, 281, 3890, 1125, 787, 264, 9834, 261, 2831, 813, 264, 13075, 272, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.13047051429748535, "compression_ratio": 1.7008547008547008, "no_speech_prob": 3.446522214289871e-06}, {"id": 66, "seek": 31920, "start": 323.68, "end": 330.47999999999996, "text": " engineers, and actually some learning algorithm implementations, will also include lambda", "tokens": [50364, 682, 3124, 11, 309, 1669, 588, 707, 2649, 1968, 291, 360, 420, 406, 11, 293, 512, 3479, 2539, 50588, 50588, 11955, 11, 293, 767, 512, 2539, 9284, 4445, 763, 11, 486, 611, 4090, 13607, 50928, 50928, 670, 568, 76, 1413, 264, 272, 8889, 1433, 13, 51106, 51106, 583, 341, 1669, 588, 707, 2649, 294, 3124, 11, 293, 264, 544, 2689, 10286, 597, 321, 603, 51347, 51347, 764, 294, 341, 1164, 307, 281, 3890, 1125, 787, 264, 9834, 261, 2831, 813, 264, 13075, 272, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.13047051429748535, "compression_ratio": 1.7008547008547008, "no_speech_prob": 3.446522214289871e-06}, {"id": 67, "seek": 31920, "start": 330.47999999999996, "end": 334.03999999999996, "text": " over 2m times the b squared term.", "tokens": [50364, 682, 3124, 11, 309, 1669, 588, 707, 2649, 1968, 291, 360, 420, 406, 11, 293, 512, 3479, 2539, 50588, 50588, 11955, 11, 293, 767, 512, 2539, 9284, 4445, 763, 11, 486, 611, 4090, 13607, 50928, 50928, 670, 568, 76, 1413, 264, 272, 8889, 1433, 13, 51106, 51106, 583, 341, 1669, 588, 707, 2649, 294, 3124, 11, 293, 264, 544, 2689, 10286, 597, 321, 603, 51347, 51347, 764, 294, 341, 1164, 307, 281, 3890, 1125, 787, 264, 9834, 261, 2831, 813, 264, 13075, 272, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.13047051429748535, "compression_ratio": 1.7008547008547008, "no_speech_prob": 3.446522214289871e-06}, {"id": 68, "seek": 31920, "start": 334.03999999999996, "end": 338.86, "text": " But this makes very little difference in practice, and the more common convention which we'll", "tokens": [50364, 682, 3124, 11, 309, 1669, 588, 707, 2649, 1968, 291, 360, 420, 406, 11, 293, 512, 3479, 2539, 50588, 50588, 11955, 11, 293, 767, 512, 2539, 9284, 4445, 763, 11, 486, 611, 4090, 13607, 50928, 50928, 670, 568, 76, 1413, 264, 272, 8889, 1433, 13, 51106, 51106, 583, 341, 1669, 588, 707, 2649, 294, 3124, 11, 293, 264, 544, 2689, 10286, 597, 321, 603, 51347, 51347, 764, 294, 341, 1164, 307, 281, 3890, 1125, 787, 264, 9834, 261, 2831, 813, 264, 13075, 272, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.13047051429748535, "compression_ratio": 1.7008547008547008, "no_speech_prob": 3.446522214289871e-06}, {"id": 69, "seek": 31920, "start": 338.86, "end": 346.0, "text": " use in this course is to regularize only the parameters w rather than the parameter b.", "tokens": [50364, 682, 3124, 11, 309, 1669, 588, 707, 2649, 1968, 291, 360, 420, 406, 11, 293, 512, 3479, 2539, 50588, 50588, 11955, 11, 293, 767, 512, 2539, 9284, 4445, 763, 11, 486, 611, 4090, 13607, 50928, 50928, 670, 568, 76, 1413, 264, 272, 8889, 1433, 13, 51106, 51106, 583, 341, 1669, 588, 707, 2649, 294, 3124, 11, 293, 264, 544, 2689, 10286, 597, 321, 603, 51347, 51347, 764, 294, 341, 1164, 307, 281, 3890, 1125, 787, 264, 9834, 261, 2831, 813, 264, 13075, 272, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.13047051429748535, "compression_ratio": 1.7008547008547008, "no_speech_prob": 3.446522214289871e-06}, {"id": 70, "seek": 34600, "start": 346.0, "end": 353.52, "text": " So to summarize, in this modified cost function, we want to minimize the original cost, which", "tokens": [50364, 407, 281, 20858, 11, 294, 341, 15873, 2063, 2445, 11, 321, 528, 281, 17522, 264, 3380, 2063, 11, 597, 50740, 50740, 307, 264, 914, 8889, 6713, 2063, 11, 1804, 43181, 264, 1150, 1433, 11, 597, 307, 1219, 264, 3890, 2144, 51022, 51022, 1433, 13, 51125, 51125, 400, 370, 341, 777, 2063, 2445, 21287, 766, 732, 5493, 300, 291, 1062, 362, 13, 51368, 51368, 20180, 281, 17522, 341, 700, 1433, 28071, 264, 9284, 281, 3318, 264, 3097, 1412, 731, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10623795811722918, "compression_ratio": 1.6883720930232557, "no_speech_prob": 2.4060850591922645e-06}, {"id": 71, "seek": 34600, "start": 353.52, "end": 359.16, "text": " is the mean squared error cost, plus additionally the second term, which is called the regularization", "tokens": [50364, 407, 281, 20858, 11, 294, 341, 15873, 2063, 2445, 11, 321, 528, 281, 17522, 264, 3380, 2063, 11, 597, 50740, 50740, 307, 264, 914, 8889, 6713, 2063, 11, 1804, 43181, 264, 1150, 1433, 11, 597, 307, 1219, 264, 3890, 2144, 51022, 51022, 1433, 13, 51125, 51125, 400, 370, 341, 777, 2063, 2445, 21287, 766, 732, 5493, 300, 291, 1062, 362, 13, 51368, 51368, 20180, 281, 17522, 341, 700, 1433, 28071, 264, 9284, 281, 3318, 264, 3097, 1412, 731, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10623795811722918, "compression_ratio": 1.6883720930232557, "no_speech_prob": 2.4060850591922645e-06}, {"id": 72, "seek": 34600, "start": 359.16, "end": 361.22, "text": " term.", "tokens": [50364, 407, 281, 20858, 11, 294, 341, 15873, 2063, 2445, 11, 321, 528, 281, 17522, 264, 3380, 2063, 11, 597, 50740, 50740, 307, 264, 914, 8889, 6713, 2063, 11, 1804, 43181, 264, 1150, 1433, 11, 597, 307, 1219, 264, 3890, 2144, 51022, 51022, 1433, 13, 51125, 51125, 400, 370, 341, 777, 2063, 2445, 21287, 766, 732, 5493, 300, 291, 1062, 362, 13, 51368, 51368, 20180, 281, 17522, 341, 700, 1433, 28071, 264, 9284, 281, 3318, 264, 3097, 1412, 731, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10623795811722918, "compression_ratio": 1.6883720930232557, "no_speech_prob": 2.4060850591922645e-06}, {"id": 73, "seek": 34600, "start": 361.22, "end": 366.08, "text": " And so this new cost function trades off two goals that you might have.", "tokens": [50364, 407, 281, 20858, 11, 294, 341, 15873, 2063, 2445, 11, 321, 528, 281, 17522, 264, 3380, 2063, 11, 597, 50740, 50740, 307, 264, 914, 8889, 6713, 2063, 11, 1804, 43181, 264, 1150, 1433, 11, 597, 307, 1219, 264, 3890, 2144, 51022, 51022, 1433, 13, 51125, 51125, 400, 370, 341, 777, 2063, 2445, 21287, 766, 732, 5493, 300, 291, 1062, 362, 13, 51368, 51368, 20180, 281, 17522, 341, 700, 1433, 28071, 264, 9284, 281, 3318, 264, 3097, 1412, 731, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10623795811722918, "compression_ratio": 1.6883720930232557, "no_speech_prob": 2.4060850591922645e-06}, {"id": 74, "seek": 34600, "start": 366.08, "end": 371.0, "text": " Trying to minimize this first term encourages the algorithm to fit the training data well", "tokens": [50364, 407, 281, 20858, 11, 294, 341, 15873, 2063, 2445, 11, 321, 528, 281, 17522, 264, 3380, 2063, 11, 597, 50740, 50740, 307, 264, 914, 8889, 6713, 2063, 11, 1804, 43181, 264, 1150, 1433, 11, 597, 307, 1219, 264, 3890, 2144, 51022, 51022, 1433, 13, 51125, 51125, 400, 370, 341, 777, 2063, 2445, 21287, 766, 732, 5493, 300, 291, 1062, 362, 13, 51368, 51368, 20180, 281, 17522, 341, 700, 1433, 28071, 264, 9284, 281, 3318, 264, 3097, 1412, 731, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10623795811722918, "compression_ratio": 1.6883720930232557, "no_speech_prob": 2.4060850591922645e-06}, {"id": 75, "seek": 37100, "start": 371.0, "end": 376.4, "text": " by minimizing the squared differences of the predictions and the actual values, and trying", "tokens": [50364, 538, 46608, 264, 8889, 7300, 295, 264, 21264, 293, 264, 3539, 4190, 11, 293, 1382, 50634, 50634, 281, 17522, 264, 1150, 1433, 11, 264, 9284, 611, 9898, 281, 1066, 264, 9834, 261, 73, 1359, 11, 50968, 50968, 597, 486, 3928, 281, 5407, 670, 69, 2414, 13, 51128, 51128, 440, 2158, 295, 13607, 300, 291, 2826, 1608, 11221, 264, 4972, 7379, 420, 264, 4972, 4923, 4506, 51446, 51446, 420, 577, 291, 4772, 1296, 613, 732, 5493, 13, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.11420717239379882, "compression_ratio": 1.6511627906976745, "no_speech_prob": 2.2602823719353182e-06}, {"id": 76, "seek": 37100, "start": 376.4, "end": 383.08, "text": " to minimize the second term, the algorithm also tries to keep the parameters wj small,", "tokens": [50364, 538, 46608, 264, 8889, 7300, 295, 264, 21264, 293, 264, 3539, 4190, 11, 293, 1382, 50634, 50634, 281, 17522, 264, 1150, 1433, 11, 264, 9284, 611, 9898, 281, 1066, 264, 9834, 261, 73, 1359, 11, 50968, 50968, 597, 486, 3928, 281, 5407, 670, 69, 2414, 13, 51128, 51128, 440, 2158, 295, 13607, 300, 291, 2826, 1608, 11221, 264, 4972, 7379, 420, 264, 4972, 4923, 4506, 51446, 51446, 420, 577, 291, 4772, 1296, 613, 732, 5493, 13, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.11420717239379882, "compression_ratio": 1.6511627906976745, "no_speech_prob": 2.2602823719353182e-06}, {"id": 77, "seek": 37100, "start": 383.08, "end": 386.28, "text": " which will tend to reduce overfitting.", "tokens": [50364, 538, 46608, 264, 8889, 7300, 295, 264, 21264, 293, 264, 3539, 4190, 11, 293, 1382, 50634, 50634, 281, 17522, 264, 1150, 1433, 11, 264, 9284, 611, 9898, 281, 1066, 264, 9834, 261, 73, 1359, 11, 50968, 50968, 597, 486, 3928, 281, 5407, 670, 69, 2414, 13, 51128, 51128, 440, 2158, 295, 13607, 300, 291, 2826, 1608, 11221, 264, 4972, 7379, 420, 264, 4972, 4923, 4506, 51446, 51446, 420, 577, 291, 4772, 1296, 613, 732, 5493, 13, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.11420717239379882, "compression_ratio": 1.6511627906976745, "no_speech_prob": 2.2602823719353182e-06}, {"id": 78, "seek": 37100, "start": 386.28, "end": 392.64, "text": " The value of lambda that you choose specifies the relative importance or the relative tradeoff", "tokens": [50364, 538, 46608, 264, 8889, 7300, 295, 264, 21264, 293, 264, 3539, 4190, 11, 293, 1382, 50634, 50634, 281, 17522, 264, 1150, 1433, 11, 264, 9284, 611, 9898, 281, 1066, 264, 9834, 261, 73, 1359, 11, 50968, 50968, 597, 486, 3928, 281, 5407, 670, 69, 2414, 13, 51128, 51128, 440, 2158, 295, 13607, 300, 291, 2826, 1608, 11221, 264, 4972, 7379, 420, 264, 4972, 4923, 4506, 51446, 51446, 420, 577, 291, 4772, 1296, 613, 732, 5493, 13, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.11420717239379882, "compression_ratio": 1.6511627906976745, "no_speech_prob": 2.2602823719353182e-06}, {"id": 79, "seek": 37100, "start": 392.64, "end": 396.64, "text": " or how you balance between these two goals.", "tokens": [50364, 538, 46608, 264, 8889, 7300, 295, 264, 21264, 293, 264, 3539, 4190, 11, 293, 1382, 50634, 50634, 281, 17522, 264, 1150, 1433, 11, 264, 9284, 611, 9898, 281, 1066, 264, 9834, 261, 73, 1359, 11, 50968, 50968, 597, 486, 3928, 281, 5407, 670, 69, 2414, 13, 51128, 51128, 440, 2158, 295, 13607, 300, 291, 2826, 1608, 11221, 264, 4972, 7379, 420, 264, 4972, 4923, 4506, 51446, 51446, 420, 577, 291, 4772, 1296, 613, 732, 5493, 13, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.11420717239379882, "compression_ratio": 1.6511627906976745, "no_speech_prob": 2.2602823719353182e-06}, {"id": 80, "seek": 39664, "start": 396.64, "end": 401.4, "text": " Let's take a look at what different values of lambda will cause your learning algorithm", "tokens": [50364, 961, 311, 747, 257, 574, 412, 437, 819, 4190, 295, 13607, 486, 3082, 428, 2539, 9284, 50602, 50602, 281, 360, 13, 50678, 50678, 961, 311, 764, 264, 6849, 3218, 17630, 1365, 1228, 8213, 24590, 13, 50885, 50885, 407, 283, 295, 2031, 307, 264, 8213, 24590, 2316, 13, 51052, 51052, 759, 13607, 390, 992, 281, 312, 4018, 11, 550, 291, 434, 406, 1228, 264, 3890, 2144, 1433, 412, 439, 11, 51372, 51372, 570, 264, 3890, 2144, 1433, 307, 17207, 538, 4018, 13, 51560, 51560, 400, 370, 498, 13607, 390, 4018, 11, 291, 917, 493, 15669, 341, 24324, 261, 46737, 11, 24324, 3997, 7605, 11, 51857, 51857], "temperature": 0.0, "avg_logprob": -0.10273460138623959, "compression_ratio": 1.74, "no_speech_prob": 4.2893011595879216e-06}, {"id": 81, "seek": 39664, "start": 401.4, "end": 402.91999999999996, "text": " to do.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 437, 819, 4190, 295, 13607, 486, 3082, 428, 2539, 9284, 50602, 50602, 281, 360, 13, 50678, 50678, 961, 311, 764, 264, 6849, 3218, 17630, 1365, 1228, 8213, 24590, 13, 50885, 50885, 407, 283, 295, 2031, 307, 264, 8213, 24590, 2316, 13, 51052, 51052, 759, 13607, 390, 992, 281, 312, 4018, 11, 550, 291, 434, 406, 1228, 264, 3890, 2144, 1433, 412, 439, 11, 51372, 51372, 570, 264, 3890, 2144, 1433, 307, 17207, 538, 4018, 13, 51560, 51560, 400, 370, 498, 13607, 390, 4018, 11, 291, 917, 493, 15669, 341, 24324, 261, 46737, 11, 24324, 3997, 7605, 11, 51857, 51857], "temperature": 0.0, "avg_logprob": -0.10273460138623959, "compression_ratio": 1.74, "no_speech_prob": 4.2893011595879216e-06}, {"id": 82, "seek": 39664, "start": 402.91999999999996, "end": 407.06, "text": " Let's use the housing price prediction example using linear regression.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 437, 819, 4190, 295, 13607, 486, 3082, 428, 2539, 9284, 50602, 50602, 281, 360, 13, 50678, 50678, 961, 311, 764, 264, 6849, 3218, 17630, 1365, 1228, 8213, 24590, 13, 50885, 50885, 407, 283, 295, 2031, 307, 264, 8213, 24590, 2316, 13, 51052, 51052, 759, 13607, 390, 992, 281, 312, 4018, 11, 550, 291, 434, 406, 1228, 264, 3890, 2144, 1433, 412, 439, 11, 51372, 51372, 570, 264, 3890, 2144, 1433, 307, 17207, 538, 4018, 13, 51560, 51560, 400, 370, 498, 13607, 390, 4018, 11, 291, 917, 493, 15669, 341, 24324, 261, 46737, 11, 24324, 3997, 7605, 11, 51857, 51857], "temperature": 0.0, "avg_logprob": -0.10273460138623959, "compression_ratio": 1.74, "no_speech_prob": 4.2893011595879216e-06}, {"id": 83, "seek": 39664, "start": 407.06, "end": 410.4, "text": " So f of x is the linear regression model.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 437, 819, 4190, 295, 13607, 486, 3082, 428, 2539, 9284, 50602, 50602, 281, 360, 13, 50678, 50678, 961, 311, 764, 264, 6849, 3218, 17630, 1365, 1228, 8213, 24590, 13, 50885, 50885, 407, 283, 295, 2031, 307, 264, 8213, 24590, 2316, 13, 51052, 51052, 759, 13607, 390, 992, 281, 312, 4018, 11, 550, 291, 434, 406, 1228, 264, 3890, 2144, 1433, 412, 439, 11, 51372, 51372, 570, 264, 3890, 2144, 1433, 307, 17207, 538, 4018, 13, 51560, 51560, 400, 370, 498, 13607, 390, 4018, 11, 291, 917, 493, 15669, 341, 24324, 261, 46737, 11, 24324, 3997, 7605, 11, 51857, 51857], "temperature": 0.0, "avg_logprob": -0.10273460138623959, "compression_ratio": 1.74, "no_speech_prob": 4.2893011595879216e-06}, {"id": 84, "seek": 39664, "start": 410.4, "end": 416.8, "text": " If lambda was set to be zero, then you're not using the regularization term at all,", "tokens": [50364, 961, 311, 747, 257, 574, 412, 437, 819, 4190, 295, 13607, 486, 3082, 428, 2539, 9284, 50602, 50602, 281, 360, 13, 50678, 50678, 961, 311, 764, 264, 6849, 3218, 17630, 1365, 1228, 8213, 24590, 13, 50885, 50885, 407, 283, 295, 2031, 307, 264, 8213, 24590, 2316, 13, 51052, 51052, 759, 13607, 390, 992, 281, 312, 4018, 11, 550, 291, 434, 406, 1228, 264, 3890, 2144, 1433, 412, 439, 11, 51372, 51372, 570, 264, 3890, 2144, 1433, 307, 17207, 538, 4018, 13, 51560, 51560, 400, 370, 498, 13607, 390, 4018, 11, 291, 917, 493, 15669, 341, 24324, 261, 46737, 11, 24324, 3997, 7605, 11, 51857, 51857], "temperature": 0.0, "avg_logprob": -0.10273460138623959, "compression_ratio": 1.74, "no_speech_prob": 4.2893011595879216e-06}, {"id": 85, "seek": 39664, "start": 416.8, "end": 420.56, "text": " because the regularization term is multiplied by zero.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 437, 819, 4190, 295, 13607, 486, 3082, 428, 2539, 9284, 50602, 50602, 281, 360, 13, 50678, 50678, 961, 311, 764, 264, 6849, 3218, 17630, 1365, 1228, 8213, 24590, 13, 50885, 50885, 407, 283, 295, 2031, 307, 264, 8213, 24590, 2316, 13, 51052, 51052, 759, 13607, 390, 992, 281, 312, 4018, 11, 550, 291, 434, 406, 1228, 264, 3890, 2144, 1433, 412, 439, 11, 51372, 51372, 570, 264, 3890, 2144, 1433, 307, 17207, 538, 4018, 13, 51560, 51560, 400, 370, 498, 13607, 390, 4018, 11, 291, 917, 493, 15669, 341, 24324, 261, 46737, 11, 24324, 3997, 7605, 11, 51857, 51857], "temperature": 0.0, "avg_logprob": -0.10273460138623959, "compression_ratio": 1.74, "no_speech_prob": 4.2893011595879216e-06}, {"id": 86, "seek": 39664, "start": 420.56, "end": 426.5, "text": " And so if lambda was zero, you end up fitting this overly wiggly, overly complex curve,", "tokens": [50364, 961, 311, 747, 257, 574, 412, 437, 819, 4190, 295, 13607, 486, 3082, 428, 2539, 9284, 50602, 50602, 281, 360, 13, 50678, 50678, 961, 311, 764, 264, 6849, 3218, 17630, 1365, 1228, 8213, 24590, 13, 50885, 50885, 407, 283, 295, 2031, 307, 264, 8213, 24590, 2316, 13, 51052, 51052, 759, 13607, 390, 992, 281, 312, 4018, 11, 550, 291, 434, 406, 1228, 264, 3890, 2144, 1433, 412, 439, 11, 51372, 51372, 570, 264, 3890, 2144, 1433, 307, 17207, 538, 4018, 13, 51560, 51560, 400, 370, 498, 13607, 390, 4018, 11, 291, 917, 493, 15669, 341, 24324, 261, 46737, 11, 24324, 3997, 7605, 11, 51857, 51857], "temperature": 0.0, "avg_logprob": -0.10273460138623959, "compression_ratio": 1.74, "no_speech_prob": 4.2893011595879216e-06}, {"id": 87, "seek": 42650, "start": 426.5, "end": 428.36, "text": " and it overfits.", "tokens": [50364, 293, 309, 670, 13979, 13, 50457, 50457, 407, 300, 390, 472, 8084, 295, 498, 13607, 390, 4018, 13, 50649, 50649, 961, 311, 586, 574, 412, 264, 661, 8084, 13, 50758, 50758, 759, 291, 992, 13607, 281, 312, 257, 534, 11, 534, 11, 534, 2416, 1230, 11, 584, 13607, 6915, 1266, 51023, 51023, 281, 264, 1347, 295, 1266, 11, 550, 291, 434, 17221, 257, 588, 4676, 3364, 322, 341, 3890, 2144, 1433, 51280, 51280, 322, 264, 558, 13, 51341, 51341, 400, 264, 787, 636, 281, 17522, 341, 307, 281, 312, 988, 300, 439, 264, 4190, 295, 261, 366, 1238, 709, 51601, 51601, 588, 1998, 281, 4018, 13, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.10309388420798561, "compression_ratio": 1.625514403292181, "no_speech_prob": 6.540351932926569e-06}, {"id": 88, "seek": 42650, "start": 428.36, "end": 432.2, "text": " So that was one extreme of if lambda was zero.", "tokens": [50364, 293, 309, 670, 13979, 13, 50457, 50457, 407, 300, 390, 472, 8084, 295, 498, 13607, 390, 4018, 13, 50649, 50649, 961, 311, 586, 574, 412, 264, 661, 8084, 13, 50758, 50758, 759, 291, 992, 13607, 281, 312, 257, 534, 11, 534, 11, 534, 2416, 1230, 11, 584, 13607, 6915, 1266, 51023, 51023, 281, 264, 1347, 295, 1266, 11, 550, 291, 434, 17221, 257, 588, 4676, 3364, 322, 341, 3890, 2144, 1433, 51280, 51280, 322, 264, 558, 13, 51341, 51341, 400, 264, 787, 636, 281, 17522, 341, 307, 281, 312, 988, 300, 439, 264, 4190, 295, 261, 366, 1238, 709, 51601, 51601, 588, 1998, 281, 4018, 13, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.10309388420798561, "compression_ratio": 1.625514403292181, "no_speech_prob": 6.540351932926569e-06}, {"id": 89, "seek": 42650, "start": 432.2, "end": 434.38, "text": " Let's now look at the other extreme.", "tokens": [50364, 293, 309, 670, 13979, 13, 50457, 50457, 407, 300, 390, 472, 8084, 295, 498, 13607, 390, 4018, 13, 50649, 50649, 961, 311, 586, 574, 412, 264, 661, 8084, 13, 50758, 50758, 759, 291, 992, 13607, 281, 312, 257, 534, 11, 534, 11, 534, 2416, 1230, 11, 584, 13607, 6915, 1266, 51023, 51023, 281, 264, 1347, 295, 1266, 11, 550, 291, 434, 17221, 257, 588, 4676, 3364, 322, 341, 3890, 2144, 1433, 51280, 51280, 322, 264, 558, 13, 51341, 51341, 400, 264, 787, 636, 281, 17522, 341, 307, 281, 312, 988, 300, 439, 264, 4190, 295, 261, 366, 1238, 709, 51601, 51601, 588, 1998, 281, 4018, 13, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.10309388420798561, "compression_ratio": 1.625514403292181, "no_speech_prob": 6.540351932926569e-06}, {"id": 90, "seek": 42650, "start": 434.38, "end": 439.68, "text": " If you set lambda to be a really, really, really large number, say lambda equals 10", "tokens": [50364, 293, 309, 670, 13979, 13, 50457, 50457, 407, 300, 390, 472, 8084, 295, 498, 13607, 390, 4018, 13, 50649, 50649, 961, 311, 586, 574, 412, 264, 661, 8084, 13, 50758, 50758, 759, 291, 992, 13607, 281, 312, 257, 534, 11, 534, 11, 534, 2416, 1230, 11, 584, 13607, 6915, 1266, 51023, 51023, 281, 264, 1347, 295, 1266, 11, 550, 291, 434, 17221, 257, 588, 4676, 3364, 322, 341, 3890, 2144, 1433, 51280, 51280, 322, 264, 558, 13, 51341, 51341, 400, 264, 787, 636, 281, 17522, 341, 307, 281, 312, 988, 300, 439, 264, 4190, 295, 261, 366, 1238, 709, 51601, 51601, 588, 1998, 281, 4018, 13, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.10309388420798561, "compression_ratio": 1.625514403292181, "no_speech_prob": 6.540351932926569e-06}, {"id": 91, "seek": 42650, "start": 439.68, "end": 444.82, "text": " to the power of 10, then you're placing a very heavy weight on this regularization term", "tokens": [50364, 293, 309, 670, 13979, 13, 50457, 50457, 407, 300, 390, 472, 8084, 295, 498, 13607, 390, 4018, 13, 50649, 50649, 961, 311, 586, 574, 412, 264, 661, 8084, 13, 50758, 50758, 759, 291, 992, 13607, 281, 312, 257, 534, 11, 534, 11, 534, 2416, 1230, 11, 584, 13607, 6915, 1266, 51023, 51023, 281, 264, 1347, 295, 1266, 11, 550, 291, 434, 17221, 257, 588, 4676, 3364, 322, 341, 3890, 2144, 1433, 51280, 51280, 322, 264, 558, 13, 51341, 51341, 400, 264, 787, 636, 281, 17522, 341, 307, 281, 312, 988, 300, 439, 264, 4190, 295, 261, 366, 1238, 709, 51601, 51601, 588, 1998, 281, 4018, 13, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.10309388420798561, "compression_ratio": 1.625514403292181, "no_speech_prob": 6.540351932926569e-06}, {"id": 92, "seek": 42650, "start": 444.82, "end": 446.04, "text": " on the right.", "tokens": [50364, 293, 309, 670, 13979, 13, 50457, 50457, 407, 300, 390, 472, 8084, 295, 498, 13607, 390, 4018, 13, 50649, 50649, 961, 311, 586, 574, 412, 264, 661, 8084, 13, 50758, 50758, 759, 291, 992, 13607, 281, 312, 257, 534, 11, 534, 11, 534, 2416, 1230, 11, 584, 13607, 6915, 1266, 51023, 51023, 281, 264, 1347, 295, 1266, 11, 550, 291, 434, 17221, 257, 588, 4676, 3364, 322, 341, 3890, 2144, 1433, 51280, 51280, 322, 264, 558, 13, 51341, 51341, 400, 264, 787, 636, 281, 17522, 341, 307, 281, 312, 988, 300, 439, 264, 4190, 295, 261, 366, 1238, 709, 51601, 51601, 588, 1998, 281, 4018, 13, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.10309388420798561, "compression_ratio": 1.625514403292181, "no_speech_prob": 6.540351932926569e-06}, {"id": 93, "seek": 42650, "start": 446.04, "end": 451.24, "text": " And the only way to minimize this is to be sure that all the values of w are pretty much", "tokens": [50364, 293, 309, 670, 13979, 13, 50457, 50457, 407, 300, 390, 472, 8084, 295, 498, 13607, 390, 4018, 13, 50649, 50649, 961, 311, 586, 574, 412, 264, 661, 8084, 13, 50758, 50758, 759, 291, 992, 13607, 281, 312, 257, 534, 11, 534, 11, 534, 2416, 1230, 11, 584, 13607, 6915, 1266, 51023, 51023, 281, 264, 1347, 295, 1266, 11, 550, 291, 434, 17221, 257, 588, 4676, 3364, 322, 341, 3890, 2144, 1433, 51280, 51280, 322, 264, 558, 13, 51341, 51341, 400, 264, 787, 636, 281, 17522, 341, 307, 281, 312, 988, 300, 439, 264, 4190, 295, 261, 366, 1238, 709, 51601, 51601, 588, 1998, 281, 4018, 13, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.10309388420798561, "compression_ratio": 1.625514403292181, "no_speech_prob": 6.540351932926569e-06}, {"id": 94, "seek": 42650, "start": 451.24, "end": 454.62, "text": " very close to zero.", "tokens": [50364, 293, 309, 670, 13979, 13, 50457, 50457, 407, 300, 390, 472, 8084, 295, 498, 13607, 390, 4018, 13, 50649, 50649, 961, 311, 586, 574, 412, 264, 661, 8084, 13, 50758, 50758, 759, 291, 992, 13607, 281, 312, 257, 534, 11, 534, 11, 534, 2416, 1230, 11, 584, 13607, 6915, 1266, 51023, 51023, 281, 264, 1347, 295, 1266, 11, 550, 291, 434, 17221, 257, 588, 4676, 3364, 322, 341, 3890, 2144, 1433, 51280, 51280, 322, 264, 558, 13, 51341, 51341, 400, 264, 787, 636, 281, 17522, 341, 307, 281, 312, 988, 300, 439, 264, 4190, 295, 261, 366, 1238, 709, 51601, 51601, 588, 1998, 281, 4018, 13, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.10309388420798561, "compression_ratio": 1.625514403292181, "no_speech_prob": 6.540351932926569e-06}, {"id": 95, "seek": 45462, "start": 454.62, "end": 462.12, "text": " So if lambda is very, very large, the learning algorithm will choose w1, w2, w3, and w4 to", "tokens": [50364, 407, 498, 13607, 307, 588, 11, 588, 2416, 11, 264, 2539, 9284, 486, 2826, 261, 16, 11, 261, 17, 11, 261, 18, 11, 293, 261, 19, 281, 50739, 50739, 312, 4664, 1998, 281, 4018, 13, 50853, 50853, 400, 8807, 11, 283, 295, 2031, 307, 1936, 2681, 281, 272, 11, 293, 370, 264, 2539, 9284, 9001, 257, 12750, 51245, 51245, 2997, 1622, 293, 833, 13979, 13, 51402, 51402, 1407, 20928, 11, 498, 13607, 307, 4018, 11, 341, 2316, 486, 670, 6845, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.08869825250962202, "compression_ratio": 1.564516129032258, "no_speech_prob": 3.5559560274123214e-06}, {"id": 96, "seek": 45462, "start": 462.12, "end": 464.4, "text": " be extremely close to zero.", "tokens": [50364, 407, 498, 13607, 307, 588, 11, 588, 2416, 11, 264, 2539, 9284, 486, 2826, 261, 16, 11, 261, 17, 11, 261, 18, 11, 293, 261, 19, 281, 50739, 50739, 312, 4664, 1998, 281, 4018, 13, 50853, 50853, 400, 8807, 11, 283, 295, 2031, 307, 1936, 2681, 281, 272, 11, 293, 370, 264, 2539, 9284, 9001, 257, 12750, 51245, 51245, 2997, 1622, 293, 833, 13979, 13, 51402, 51402, 1407, 20928, 11, 498, 13607, 307, 4018, 11, 341, 2316, 486, 670, 6845, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.08869825250962202, "compression_ratio": 1.564516129032258, "no_speech_prob": 3.5559560274123214e-06}, {"id": 97, "seek": 45462, "start": 464.4, "end": 472.24, "text": " And thus, f of x is basically equal to b, and so the learning algorithm fits a horizontal", "tokens": [50364, 407, 498, 13607, 307, 588, 11, 588, 2416, 11, 264, 2539, 9284, 486, 2826, 261, 16, 11, 261, 17, 11, 261, 18, 11, 293, 261, 19, 281, 50739, 50739, 312, 4664, 1998, 281, 4018, 13, 50853, 50853, 400, 8807, 11, 283, 295, 2031, 307, 1936, 2681, 281, 272, 11, 293, 370, 264, 2539, 9284, 9001, 257, 12750, 51245, 51245, 2997, 1622, 293, 833, 13979, 13, 51402, 51402, 1407, 20928, 11, 498, 13607, 307, 4018, 11, 341, 2316, 486, 670, 6845, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.08869825250962202, "compression_ratio": 1.564516129032258, "no_speech_prob": 3.5559560274123214e-06}, {"id": 98, "seek": 45462, "start": 472.24, "end": 475.38, "text": " straight line and underfits.", "tokens": [50364, 407, 498, 13607, 307, 588, 11, 588, 2416, 11, 264, 2539, 9284, 486, 2826, 261, 16, 11, 261, 17, 11, 261, 18, 11, 293, 261, 19, 281, 50739, 50739, 312, 4664, 1998, 281, 4018, 13, 50853, 50853, 400, 8807, 11, 283, 295, 2031, 307, 1936, 2681, 281, 272, 11, 293, 370, 264, 2539, 9284, 9001, 257, 12750, 51245, 51245, 2997, 1622, 293, 833, 13979, 13, 51402, 51402, 1407, 20928, 11, 498, 13607, 307, 4018, 11, 341, 2316, 486, 670, 6845, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.08869825250962202, "compression_ratio": 1.564516129032258, "no_speech_prob": 3.5559560274123214e-06}, {"id": 99, "seek": 45462, "start": 475.38, "end": 480.38, "text": " To recap, if lambda is zero, this model will overfit.", "tokens": [50364, 407, 498, 13607, 307, 588, 11, 588, 2416, 11, 264, 2539, 9284, 486, 2826, 261, 16, 11, 261, 17, 11, 261, 18, 11, 293, 261, 19, 281, 50739, 50739, 312, 4664, 1998, 281, 4018, 13, 50853, 50853, 400, 8807, 11, 283, 295, 2031, 307, 1936, 2681, 281, 272, 11, 293, 370, 264, 2539, 9284, 9001, 257, 12750, 51245, 51245, 2997, 1622, 293, 833, 13979, 13, 51402, 51402, 1407, 20928, 11, 498, 13607, 307, 4018, 11, 341, 2316, 486, 670, 6845, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.08869825250962202, "compression_ratio": 1.564516129032258, "no_speech_prob": 3.5559560274123214e-06}, {"id": 100, "seek": 48038, "start": 480.38, "end": 485.94, "text": " If lambda is enormous, like 10 to the power of 10, this model will underfit.", "tokens": [50364, 759, 13607, 307, 11322, 11, 411, 1266, 281, 264, 1347, 295, 1266, 11, 341, 2316, 486, 833, 6845, 13, 50642, 50642, 400, 370, 437, 291, 528, 307, 512, 2158, 295, 13607, 300, 307, 294, 1296, 300, 544, 23505, 50908, 50908, 33993, 613, 700, 293, 1150, 2115, 295, 9529, 766, 11, 46608, 264, 914, 8889, 6713, 293, 51291, 51291, 5145, 264, 9834, 1359, 13, 51477, 51477, 400, 562, 264, 2158, 295, 13607, 307, 406, 886, 1359, 293, 406, 886, 2416, 11, 457, 445, 558, 11, 550, 4696, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.07491314676072862, "compression_ratio": 1.6394849785407726, "no_speech_prob": 1.6797171156213153e-06}, {"id": 101, "seek": 48038, "start": 485.94, "end": 491.26, "text": " And so what you want is some value of lambda that is in between that more appropriately", "tokens": [50364, 759, 13607, 307, 11322, 11, 411, 1266, 281, 264, 1347, 295, 1266, 11, 341, 2316, 486, 833, 6845, 13, 50642, 50642, 400, 370, 437, 291, 528, 307, 512, 2158, 295, 13607, 300, 307, 294, 1296, 300, 544, 23505, 50908, 50908, 33993, 613, 700, 293, 1150, 2115, 295, 9529, 766, 11, 46608, 264, 914, 8889, 6713, 293, 51291, 51291, 5145, 264, 9834, 1359, 13, 51477, 51477, 400, 562, 264, 2158, 295, 13607, 307, 406, 886, 1359, 293, 406, 886, 2416, 11, 457, 445, 558, 11, 550, 4696, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.07491314676072862, "compression_ratio": 1.6394849785407726, "no_speech_prob": 1.6797171156213153e-06}, {"id": 102, "seek": 48038, "start": 491.26, "end": 498.92, "text": " balances these first and second terms of trading off, minimizing the mean squared error and", "tokens": [50364, 759, 13607, 307, 11322, 11, 411, 1266, 281, 264, 1347, 295, 1266, 11, 341, 2316, 486, 833, 6845, 13, 50642, 50642, 400, 370, 437, 291, 528, 307, 512, 2158, 295, 13607, 300, 307, 294, 1296, 300, 544, 23505, 50908, 50908, 33993, 613, 700, 293, 1150, 2115, 295, 9529, 766, 11, 46608, 264, 914, 8889, 6713, 293, 51291, 51291, 5145, 264, 9834, 1359, 13, 51477, 51477, 400, 562, 264, 2158, 295, 13607, 307, 406, 886, 1359, 293, 406, 886, 2416, 11, 457, 445, 558, 11, 550, 4696, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.07491314676072862, "compression_ratio": 1.6394849785407726, "no_speech_prob": 1.6797171156213153e-06}, {"id": 103, "seek": 48038, "start": 498.92, "end": 502.64, "text": " keeping the parameters small.", "tokens": [50364, 759, 13607, 307, 11322, 11, 411, 1266, 281, 264, 1347, 295, 1266, 11, 341, 2316, 486, 833, 6845, 13, 50642, 50642, 400, 370, 437, 291, 528, 307, 512, 2158, 295, 13607, 300, 307, 294, 1296, 300, 544, 23505, 50908, 50908, 33993, 613, 700, 293, 1150, 2115, 295, 9529, 766, 11, 46608, 264, 914, 8889, 6713, 293, 51291, 51291, 5145, 264, 9834, 1359, 13, 51477, 51477, 400, 562, 264, 2158, 295, 13607, 307, 406, 886, 1359, 293, 406, 886, 2416, 11, 457, 445, 558, 11, 550, 4696, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.07491314676072862, "compression_ratio": 1.6394849785407726, "no_speech_prob": 1.6797171156213153e-06}, {"id": 104, "seek": 48038, "start": 502.64, "end": 508.15999999999997, "text": " And when the value of lambda is not too small and not too large, but just right, then hopefully", "tokens": [50364, 759, 13607, 307, 11322, 11, 411, 1266, 281, 264, 1347, 295, 1266, 11, 341, 2316, 486, 833, 6845, 13, 50642, 50642, 400, 370, 437, 291, 528, 307, 512, 2158, 295, 13607, 300, 307, 294, 1296, 300, 544, 23505, 50908, 50908, 33993, 613, 700, 293, 1150, 2115, 295, 9529, 766, 11, 46608, 264, 914, 8889, 6713, 293, 51291, 51291, 5145, 264, 9834, 1359, 13, 51477, 51477, 400, 562, 264, 2158, 295, 13607, 307, 406, 886, 1359, 293, 406, 886, 2416, 11, 457, 445, 558, 11, 550, 4696, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.07491314676072862, "compression_ratio": 1.6394849785407726, "no_speech_prob": 1.6797171156213153e-06}, {"id": 105, "seek": 50816, "start": 508.16, "end": 513.4200000000001, "text": " you end up able to fit a fourth order polynomial keeping all of these features, but with a", "tokens": [50364, 291, 917, 493, 1075, 281, 3318, 257, 6409, 1668, 26110, 5145, 439, 295, 613, 4122, 11, 457, 365, 257, 50627, 50627, 2445, 300, 1542, 411, 341, 13, 50791, 50791, 407, 300, 311, 577, 3890, 2144, 1985, 13, 50930, 50930, 1133, 321, 751, 466, 2316, 9450, 1780, 294, 264, 2121, 2144, 11, 321, 603, 611, 536, 257, 5673, 51186, 51186, 295, 2098, 281, 2826, 665, 4190, 337, 13607, 13, 51378, 51378, 682, 264, 958, 732, 2145, 11, 321, 603, 12497, 484, 577, 281, 3079, 3890, 2144, 281, 8213, 24590, 51625, 51625, 293, 3565, 3142, 24590, 11, 293, 577, 281, 3847, 613, 5245, 365, 16235, 23475, 13, 51853, 51853], "temperature": 0.0, "avg_logprob": -0.09853370911484464, "compression_ratio": 1.6727941176470589, "no_speech_prob": 1.0030076964540058e-06}, {"id": 106, "seek": 50816, "start": 513.4200000000001, "end": 516.7, "text": " function that looks like this.", "tokens": [50364, 291, 917, 493, 1075, 281, 3318, 257, 6409, 1668, 26110, 5145, 439, 295, 613, 4122, 11, 457, 365, 257, 50627, 50627, 2445, 300, 1542, 411, 341, 13, 50791, 50791, 407, 300, 311, 577, 3890, 2144, 1985, 13, 50930, 50930, 1133, 321, 751, 466, 2316, 9450, 1780, 294, 264, 2121, 2144, 11, 321, 603, 611, 536, 257, 5673, 51186, 51186, 295, 2098, 281, 2826, 665, 4190, 337, 13607, 13, 51378, 51378, 682, 264, 958, 732, 2145, 11, 321, 603, 12497, 484, 577, 281, 3079, 3890, 2144, 281, 8213, 24590, 51625, 51625, 293, 3565, 3142, 24590, 11, 293, 577, 281, 3847, 613, 5245, 365, 16235, 23475, 13, 51853, 51853], "temperature": 0.0, "avg_logprob": -0.09853370911484464, "compression_ratio": 1.6727941176470589, "no_speech_prob": 1.0030076964540058e-06}, {"id": 107, "seek": 50816, "start": 516.7, "end": 519.48, "text": " So that's how regularization works.", "tokens": [50364, 291, 917, 493, 1075, 281, 3318, 257, 6409, 1668, 26110, 5145, 439, 295, 613, 4122, 11, 457, 365, 257, 50627, 50627, 2445, 300, 1542, 411, 341, 13, 50791, 50791, 407, 300, 311, 577, 3890, 2144, 1985, 13, 50930, 50930, 1133, 321, 751, 466, 2316, 9450, 1780, 294, 264, 2121, 2144, 11, 321, 603, 611, 536, 257, 5673, 51186, 51186, 295, 2098, 281, 2826, 665, 4190, 337, 13607, 13, 51378, 51378, 682, 264, 958, 732, 2145, 11, 321, 603, 12497, 484, 577, 281, 3079, 3890, 2144, 281, 8213, 24590, 51625, 51625, 293, 3565, 3142, 24590, 11, 293, 577, 281, 3847, 613, 5245, 365, 16235, 23475, 13, 51853, 51853], "temperature": 0.0, "avg_logprob": -0.09853370911484464, "compression_ratio": 1.6727941176470589, "no_speech_prob": 1.0030076964540058e-06}, {"id": 108, "seek": 50816, "start": 519.48, "end": 524.6, "text": " When we talk about model selection later in the specialization, we'll also see a variety", "tokens": [50364, 291, 917, 493, 1075, 281, 3318, 257, 6409, 1668, 26110, 5145, 439, 295, 613, 4122, 11, 457, 365, 257, 50627, 50627, 2445, 300, 1542, 411, 341, 13, 50791, 50791, 407, 300, 311, 577, 3890, 2144, 1985, 13, 50930, 50930, 1133, 321, 751, 466, 2316, 9450, 1780, 294, 264, 2121, 2144, 11, 321, 603, 611, 536, 257, 5673, 51186, 51186, 295, 2098, 281, 2826, 665, 4190, 337, 13607, 13, 51378, 51378, 682, 264, 958, 732, 2145, 11, 321, 603, 12497, 484, 577, 281, 3079, 3890, 2144, 281, 8213, 24590, 51625, 51625, 293, 3565, 3142, 24590, 11, 293, 577, 281, 3847, 613, 5245, 365, 16235, 23475, 13, 51853, 51853], "temperature": 0.0, "avg_logprob": -0.09853370911484464, "compression_ratio": 1.6727941176470589, "no_speech_prob": 1.0030076964540058e-06}, {"id": 109, "seek": 50816, "start": 524.6, "end": 528.44, "text": " of ways to choose good values for lambda.", "tokens": [50364, 291, 917, 493, 1075, 281, 3318, 257, 6409, 1668, 26110, 5145, 439, 295, 613, 4122, 11, 457, 365, 257, 50627, 50627, 2445, 300, 1542, 411, 341, 13, 50791, 50791, 407, 300, 311, 577, 3890, 2144, 1985, 13, 50930, 50930, 1133, 321, 751, 466, 2316, 9450, 1780, 294, 264, 2121, 2144, 11, 321, 603, 611, 536, 257, 5673, 51186, 51186, 295, 2098, 281, 2826, 665, 4190, 337, 13607, 13, 51378, 51378, 682, 264, 958, 732, 2145, 11, 321, 603, 12497, 484, 577, 281, 3079, 3890, 2144, 281, 8213, 24590, 51625, 51625, 293, 3565, 3142, 24590, 11, 293, 577, 281, 3847, 613, 5245, 365, 16235, 23475, 13, 51853, 51853], "temperature": 0.0, "avg_logprob": -0.09853370911484464, "compression_ratio": 1.6727941176470589, "no_speech_prob": 1.0030076964540058e-06}, {"id": 110, "seek": 50816, "start": 528.44, "end": 533.38, "text": " In the next two videos, we'll flesh out how to apply regularization to linear regression", "tokens": [50364, 291, 917, 493, 1075, 281, 3318, 257, 6409, 1668, 26110, 5145, 439, 295, 613, 4122, 11, 457, 365, 257, 50627, 50627, 2445, 300, 1542, 411, 341, 13, 50791, 50791, 407, 300, 311, 577, 3890, 2144, 1985, 13, 50930, 50930, 1133, 321, 751, 466, 2316, 9450, 1780, 294, 264, 2121, 2144, 11, 321, 603, 611, 536, 257, 5673, 51186, 51186, 295, 2098, 281, 2826, 665, 4190, 337, 13607, 13, 51378, 51378, 682, 264, 958, 732, 2145, 11, 321, 603, 12497, 484, 577, 281, 3079, 3890, 2144, 281, 8213, 24590, 51625, 51625, 293, 3565, 3142, 24590, 11, 293, 577, 281, 3847, 613, 5245, 365, 16235, 23475, 13, 51853, 51853], "temperature": 0.0, "avg_logprob": -0.09853370911484464, "compression_ratio": 1.6727941176470589, "no_speech_prob": 1.0030076964540058e-06}, {"id": 111, "seek": 50816, "start": 533.38, "end": 537.94, "text": " and logistic regression, and how to train these models with gradient descent.", "tokens": [50364, 291, 917, 493, 1075, 281, 3318, 257, 6409, 1668, 26110, 5145, 439, 295, 613, 4122, 11, 457, 365, 257, 50627, 50627, 2445, 300, 1542, 411, 341, 13, 50791, 50791, 407, 300, 311, 577, 3890, 2144, 1985, 13, 50930, 50930, 1133, 321, 751, 466, 2316, 9450, 1780, 294, 264, 2121, 2144, 11, 321, 603, 611, 536, 257, 5673, 51186, 51186, 295, 2098, 281, 2826, 665, 4190, 337, 13607, 13, 51378, 51378, 682, 264, 958, 732, 2145, 11, 321, 603, 12497, 484, 577, 281, 3079, 3890, 2144, 281, 8213, 24590, 51625, 51625, 293, 3565, 3142, 24590, 11, 293, 577, 281, 3847, 613, 5245, 365, 16235, 23475, 13, 51853, 51853], "temperature": 0.0, "avg_logprob": -0.09853370911484464, "compression_ratio": 1.6727941176470589, "no_speech_prob": 1.0030076964540058e-06}, {"id": 112, "seek": 53794, "start": 537.94, "end": 541.62, "text": " With that, you'll be able to avoid overfitting with both of these algorithms.", "tokens": [50364, 2022, 300, 11, 291, 603, 312, 1075, 281, 5042, 670, 69, 2414, 365, 1293, 295, 613, 14642, 13, 50548], "temperature": 0.0, "avg_logprob": -0.09547971543811616, "compression_ratio": 1.013157894736842, "no_speech_prob": 1.299624091188889e-05}], "language": "en", "video_id": "SCj3h47dKL0", "entity": "ML Specialization, Andrew Ng (2022)"}}