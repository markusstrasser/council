{"video_id": "AbtWSXHPfS0", "title": "2.5 Practical Tips for Linear Regression | Feature scaling part 1-- [Machine Learning | Andrew Ng]", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart! First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 395, "views": 335, "publish_date": "11/04/2022", "timestamp": 1661040000, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " So, welcome back. Let's take a look at some techniques that will make gradient descent work much better. In this video, you see a technique called feature scaling that will enable gradient descent to run much faster. Let's start by taking a look at the relationship between the size of a feature, that is, how big are the numbers for that feature, and the size of its associated parameter. As a concrete example, let's predict the price of a house using two features, x1, the size of the house, and x2, the number of bedrooms. Let's say that x1 typically ranges from 300 to 2000 square feet, and x2, in the dataset, ranges from 0 to 5 bedrooms. So for this example, x1 takes on a relatively large range of values, and x2 takes on a relatively small range of values. Now let's take an example of a house that has a size of 2000 square feet, has 5 bedrooms, and a price of 500k or $500,000. For this one trading example, what do you think are reasonable values for the size of parameters w1 and w2? Well, let's look at one possible set of parameters. Say w1 is 50, and w2 is 0.1, and b is 50 for the purposes of discussion. So in this case, the estimated price in thousands of dollars is 100,000k here, plus 0.5k, plus 50k, which is slightly over $100 million. So that's clearly very far from the actual price of $500,000. And so this is not a very good set of parameter choices for w1 and w2. Now let's take a look at another possibility. Say w1 and w2 were the other way around. w1 is 0.1, and w2 is 50, and b is still also 50. In this choice of w1 and w2, w1 is relatively small and w2 is relatively large. 50 is much bigger than 0.1. So here the predicted price is 0.1 times 2000 plus 50 times 5 plus 50. The first term becomes 200k, the second term becomes 250k, and then plus 50. So this version of the model predicts a price of $500,000, which is a much more reasonable estimate and happens to be the same price as the true price of the house. So hopefully you might notice that when a possible range of values of a feature is large, like the size in square feet, which goes all the way up to 2000, it's more likely that a good model will learn to choose a relatively small parameter value like 0.1. Likewise, when the possible values of a feature are small, like the number of bedrooms, then a reasonable value for its parameters will be relatively large, like 50. So how does this relate to gradient descent? Well, let's take a look at a scatter plot of the features, where the size in square feet is the horizontal axis, x1, and the number of bedrooms, x2, is on the vertical axis. If you plot the training data, you notice that the horizontal axis is on a much larger scale or much larger range of values compared to the vertical axis. Next, let's look at how the cost function might look in a contour plot. You might see a contour plot where the horizontal axis has a much narrower range, say between 0 and 1, whereas the vertical axis takes on much larger values, say between 10 and 100. So the contours form ovals or ellipses, and they're shorter on one side and longer on the other. And this is because a very small change to w1 can have a very large impact on the estimated price and thus a very large impact on the cost j, because w1 tends to be multiplied by a very large number, the size in square feet. In contrast, it takes a much larger change in w2 in order to change the predictions much, and thus small changes to w2 don't change the cost function nearly as much. So where does this leave us? This is what might end up happening if you were to run gradient descent, if you were to use your training data as is. Because the contours are so tall and skinny, gradient descent may end up bouncing back and forth for a long time before it can finally find its way to the global minimum. In situations like this, a useful thing to do is to scale the features. This means performing some transformation of your training data so that x1, say, might now range from 0 to 1, and x2 might also range from 0 to 1. So the data points now look more like this, and you might notice that the scale of the plot on the bottom is now quite different than the one on top. The key point is that the rescaled x1 and x2 are both now taking comparable ranges of values to each other. And if you run gradient descent on a cost function defined on this rescaled x1 and x2 using this transformed data, then the contours will look more like this, more like circles and less tall and skinny, and gradient descent can find a much more direct path to the global minimum. So to recap, when you have different features that take on very different ranges of values, you can cause gradient descent to run slowly, but rescaling the different features so they all take on a comparable range of values can speed up gradient descent significantly. How do you actually do this? Let's take a look at that in the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.28, "text": " So, welcome back.", "tokens": [50364, 407, 11, 2928, 646, 13, 50578, 50578, 961, 311, 747, 257, 574, 412, 512, 7512, 300, 486, 652, 16235, 23475, 589, 709, 1101, 13, 50804, 50804, 682, 341, 960, 11, 291, 536, 257, 6532, 1219, 4111, 21589, 300, 486, 9528, 16235, 51032, 51032, 23475, 281, 1190, 709, 4663, 13, 51176, 51176, 961, 311, 722, 538, 1940, 257, 574, 412, 264, 2480, 1296, 264, 2744, 295, 257, 4111, 11, 300, 307, 11, 577, 51412, 51412, 955, 366, 264, 3547, 337, 300, 4111, 11, 293, 264, 2744, 295, 1080, 6615, 13075, 13, 51719, 51719], "temperature": 0.0, "avg_logprob": -0.13833655702306868, "compression_ratio": 1.7366071428571428, "no_speech_prob": 0.01132478378713131}, {"id": 1, "seek": 0, "start": 4.28, "end": 8.8, "text": " Let's take a look at some techniques that will make gradient descent work much better.", "tokens": [50364, 407, 11, 2928, 646, 13, 50578, 50578, 961, 311, 747, 257, 574, 412, 512, 7512, 300, 486, 652, 16235, 23475, 589, 709, 1101, 13, 50804, 50804, 682, 341, 960, 11, 291, 536, 257, 6532, 1219, 4111, 21589, 300, 486, 9528, 16235, 51032, 51032, 23475, 281, 1190, 709, 4663, 13, 51176, 51176, 961, 311, 722, 538, 1940, 257, 574, 412, 264, 2480, 1296, 264, 2744, 295, 257, 4111, 11, 300, 307, 11, 577, 51412, 51412, 955, 366, 264, 3547, 337, 300, 4111, 11, 293, 264, 2744, 295, 1080, 6615, 13075, 13, 51719, 51719], "temperature": 0.0, "avg_logprob": -0.13833655702306868, "compression_ratio": 1.7366071428571428, "no_speech_prob": 0.01132478378713131}, {"id": 2, "seek": 0, "start": 8.8, "end": 13.36, "text": " In this video, you see a technique called feature scaling that will enable gradient", "tokens": [50364, 407, 11, 2928, 646, 13, 50578, 50578, 961, 311, 747, 257, 574, 412, 512, 7512, 300, 486, 652, 16235, 23475, 589, 709, 1101, 13, 50804, 50804, 682, 341, 960, 11, 291, 536, 257, 6532, 1219, 4111, 21589, 300, 486, 9528, 16235, 51032, 51032, 23475, 281, 1190, 709, 4663, 13, 51176, 51176, 961, 311, 722, 538, 1940, 257, 574, 412, 264, 2480, 1296, 264, 2744, 295, 257, 4111, 11, 300, 307, 11, 577, 51412, 51412, 955, 366, 264, 3547, 337, 300, 4111, 11, 293, 264, 2744, 295, 1080, 6615, 13075, 13, 51719, 51719], "temperature": 0.0, "avg_logprob": -0.13833655702306868, "compression_ratio": 1.7366071428571428, "no_speech_prob": 0.01132478378713131}, {"id": 3, "seek": 0, "start": 13.36, "end": 16.240000000000002, "text": " descent to run much faster.", "tokens": [50364, 407, 11, 2928, 646, 13, 50578, 50578, 961, 311, 747, 257, 574, 412, 512, 7512, 300, 486, 652, 16235, 23475, 589, 709, 1101, 13, 50804, 50804, 682, 341, 960, 11, 291, 536, 257, 6532, 1219, 4111, 21589, 300, 486, 9528, 16235, 51032, 51032, 23475, 281, 1190, 709, 4663, 13, 51176, 51176, 961, 311, 722, 538, 1940, 257, 574, 412, 264, 2480, 1296, 264, 2744, 295, 257, 4111, 11, 300, 307, 11, 577, 51412, 51412, 955, 366, 264, 3547, 337, 300, 4111, 11, 293, 264, 2744, 295, 1080, 6615, 13075, 13, 51719, 51719], "temperature": 0.0, "avg_logprob": -0.13833655702306868, "compression_ratio": 1.7366071428571428, "no_speech_prob": 0.01132478378713131}, {"id": 4, "seek": 0, "start": 16.240000000000002, "end": 20.96, "text": " Let's start by taking a look at the relationship between the size of a feature, that is, how", "tokens": [50364, 407, 11, 2928, 646, 13, 50578, 50578, 961, 311, 747, 257, 574, 412, 512, 7512, 300, 486, 652, 16235, 23475, 589, 709, 1101, 13, 50804, 50804, 682, 341, 960, 11, 291, 536, 257, 6532, 1219, 4111, 21589, 300, 486, 9528, 16235, 51032, 51032, 23475, 281, 1190, 709, 4663, 13, 51176, 51176, 961, 311, 722, 538, 1940, 257, 574, 412, 264, 2480, 1296, 264, 2744, 295, 257, 4111, 11, 300, 307, 11, 577, 51412, 51412, 955, 366, 264, 3547, 337, 300, 4111, 11, 293, 264, 2744, 295, 1080, 6615, 13075, 13, 51719, 51719], "temperature": 0.0, "avg_logprob": -0.13833655702306868, "compression_ratio": 1.7366071428571428, "no_speech_prob": 0.01132478378713131}, {"id": 5, "seek": 0, "start": 20.96, "end": 27.1, "text": " big are the numbers for that feature, and the size of its associated parameter.", "tokens": [50364, 407, 11, 2928, 646, 13, 50578, 50578, 961, 311, 747, 257, 574, 412, 512, 7512, 300, 486, 652, 16235, 23475, 589, 709, 1101, 13, 50804, 50804, 682, 341, 960, 11, 291, 536, 257, 6532, 1219, 4111, 21589, 300, 486, 9528, 16235, 51032, 51032, 23475, 281, 1190, 709, 4663, 13, 51176, 51176, 961, 311, 722, 538, 1940, 257, 574, 412, 264, 2480, 1296, 264, 2744, 295, 257, 4111, 11, 300, 307, 11, 577, 51412, 51412, 955, 366, 264, 3547, 337, 300, 4111, 11, 293, 264, 2744, 295, 1080, 6615, 13075, 13, 51719, 51719], "temperature": 0.0, "avg_logprob": -0.13833655702306868, "compression_ratio": 1.7366071428571428, "no_speech_prob": 0.01132478378713131}, {"id": 6, "seek": 2710, "start": 27.1, "end": 33.32, "text": " As a concrete example, let's predict the price of a house using two features, x1, the size", "tokens": [50364, 1018, 257, 9859, 1365, 11, 718, 311, 6069, 264, 3218, 295, 257, 1782, 1228, 732, 4122, 11, 2031, 16, 11, 264, 2744, 50675, 50675, 295, 264, 1782, 11, 293, 2031, 17, 11, 264, 1230, 295, 39955, 13, 50885, 50885, 961, 311, 584, 300, 2031, 16, 5850, 22526, 490, 6641, 281, 8132, 3732, 3521, 11, 293, 2031, 17, 11, 294, 264, 28872, 11, 51273, 51273, 22526, 490, 1958, 281, 1025, 39955, 13, 51431, 51431, 407, 337, 341, 1365, 11, 2031, 16, 2516, 322, 257, 7226, 2416, 3613, 295, 4190, 11, 293, 2031, 17, 2516, 322, 257, 7226, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.12186003684997558, "compression_ratio": 1.6761904761904762, "no_speech_prob": 2.468133061483968e-05}, {"id": 7, "seek": 2710, "start": 33.32, "end": 37.52, "text": " of the house, and x2, the number of bedrooms.", "tokens": [50364, 1018, 257, 9859, 1365, 11, 718, 311, 6069, 264, 3218, 295, 257, 1782, 1228, 732, 4122, 11, 2031, 16, 11, 264, 2744, 50675, 50675, 295, 264, 1782, 11, 293, 2031, 17, 11, 264, 1230, 295, 39955, 13, 50885, 50885, 961, 311, 584, 300, 2031, 16, 5850, 22526, 490, 6641, 281, 8132, 3732, 3521, 11, 293, 2031, 17, 11, 294, 264, 28872, 11, 51273, 51273, 22526, 490, 1958, 281, 1025, 39955, 13, 51431, 51431, 407, 337, 341, 1365, 11, 2031, 16, 2516, 322, 257, 7226, 2416, 3613, 295, 4190, 11, 293, 2031, 17, 2516, 322, 257, 7226, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.12186003684997558, "compression_ratio": 1.6761904761904762, "no_speech_prob": 2.468133061483968e-05}, {"id": 8, "seek": 2710, "start": 37.52, "end": 45.28, "text": " Let's say that x1 typically ranges from 300 to 2000 square feet, and x2, in the dataset,", "tokens": [50364, 1018, 257, 9859, 1365, 11, 718, 311, 6069, 264, 3218, 295, 257, 1782, 1228, 732, 4122, 11, 2031, 16, 11, 264, 2744, 50675, 50675, 295, 264, 1782, 11, 293, 2031, 17, 11, 264, 1230, 295, 39955, 13, 50885, 50885, 961, 311, 584, 300, 2031, 16, 5850, 22526, 490, 6641, 281, 8132, 3732, 3521, 11, 293, 2031, 17, 11, 294, 264, 28872, 11, 51273, 51273, 22526, 490, 1958, 281, 1025, 39955, 13, 51431, 51431, 407, 337, 341, 1365, 11, 2031, 16, 2516, 322, 257, 7226, 2416, 3613, 295, 4190, 11, 293, 2031, 17, 2516, 322, 257, 7226, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.12186003684997558, "compression_ratio": 1.6761904761904762, "no_speech_prob": 2.468133061483968e-05}, {"id": 9, "seek": 2710, "start": 45.28, "end": 48.44, "text": " ranges from 0 to 5 bedrooms.", "tokens": [50364, 1018, 257, 9859, 1365, 11, 718, 311, 6069, 264, 3218, 295, 257, 1782, 1228, 732, 4122, 11, 2031, 16, 11, 264, 2744, 50675, 50675, 295, 264, 1782, 11, 293, 2031, 17, 11, 264, 1230, 295, 39955, 13, 50885, 50885, 961, 311, 584, 300, 2031, 16, 5850, 22526, 490, 6641, 281, 8132, 3732, 3521, 11, 293, 2031, 17, 11, 294, 264, 28872, 11, 51273, 51273, 22526, 490, 1958, 281, 1025, 39955, 13, 51431, 51431, 407, 337, 341, 1365, 11, 2031, 16, 2516, 322, 257, 7226, 2416, 3613, 295, 4190, 11, 293, 2031, 17, 2516, 322, 257, 7226, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.12186003684997558, "compression_ratio": 1.6761904761904762, "no_speech_prob": 2.468133061483968e-05}, {"id": 10, "seek": 2710, "start": 48.44, "end": 55.72, "text": " So for this example, x1 takes on a relatively large range of values, and x2 takes on a relatively", "tokens": [50364, 1018, 257, 9859, 1365, 11, 718, 311, 6069, 264, 3218, 295, 257, 1782, 1228, 732, 4122, 11, 2031, 16, 11, 264, 2744, 50675, 50675, 295, 264, 1782, 11, 293, 2031, 17, 11, 264, 1230, 295, 39955, 13, 50885, 50885, 961, 311, 584, 300, 2031, 16, 5850, 22526, 490, 6641, 281, 8132, 3732, 3521, 11, 293, 2031, 17, 11, 294, 264, 28872, 11, 51273, 51273, 22526, 490, 1958, 281, 1025, 39955, 13, 51431, 51431, 407, 337, 341, 1365, 11, 2031, 16, 2516, 322, 257, 7226, 2416, 3613, 295, 4190, 11, 293, 2031, 17, 2516, 322, 257, 7226, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.12186003684997558, "compression_ratio": 1.6761904761904762, "no_speech_prob": 2.468133061483968e-05}, {"id": 11, "seek": 5572, "start": 55.72, "end": 58.4, "text": " small range of values.", "tokens": [50364, 1359, 3613, 295, 4190, 13, 50498, 50498, 823, 718, 311, 747, 364, 1365, 295, 257, 1782, 300, 575, 257, 2744, 295, 8132, 3732, 3521, 11, 575, 1025, 39955, 11, 50922, 50922, 293, 257, 3218, 295, 5923, 74, 420, 1848, 7526, 11, 1360, 13, 51180, 51180, 1171, 341, 472, 9529, 1365, 11, 437, 360, 291, 519, 366, 10585, 4190, 337, 264, 2744, 295, 51398, 51398, 9834, 261, 16, 293, 261, 17, 30, 51572, 51572, 1042, 11, 718, 311, 574, 412, 472, 1944, 992, 295, 9834, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.13864632938685043, "compression_ratio": 1.5174129353233832, "no_speech_prob": 3.1381086955661885e-06}, {"id": 12, "seek": 5572, "start": 58.4, "end": 66.88, "text": " Now let's take an example of a house that has a size of 2000 square feet, has 5 bedrooms,", "tokens": [50364, 1359, 3613, 295, 4190, 13, 50498, 50498, 823, 718, 311, 747, 364, 1365, 295, 257, 1782, 300, 575, 257, 2744, 295, 8132, 3732, 3521, 11, 575, 1025, 39955, 11, 50922, 50922, 293, 257, 3218, 295, 5923, 74, 420, 1848, 7526, 11, 1360, 13, 51180, 51180, 1171, 341, 472, 9529, 1365, 11, 437, 360, 291, 519, 366, 10585, 4190, 337, 264, 2744, 295, 51398, 51398, 9834, 261, 16, 293, 261, 17, 30, 51572, 51572, 1042, 11, 718, 311, 574, 412, 472, 1944, 992, 295, 9834, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.13864632938685043, "compression_ratio": 1.5174129353233832, "no_speech_prob": 3.1381086955661885e-06}, {"id": 13, "seek": 5572, "start": 66.88, "end": 72.03999999999999, "text": " and a price of 500k or $500,000.", "tokens": [50364, 1359, 3613, 295, 4190, 13, 50498, 50498, 823, 718, 311, 747, 364, 1365, 295, 257, 1782, 300, 575, 257, 2744, 295, 8132, 3732, 3521, 11, 575, 1025, 39955, 11, 50922, 50922, 293, 257, 3218, 295, 5923, 74, 420, 1848, 7526, 11, 1360, 13, 51180, 51180, 1171, 341, 472, 9529, 1365, 11, 437, 360, 291, 519, 366, 10585, 4190, 337, 264, 2744, 295, 51398, 51398, 9834, 261, 16, 293, 261, 17, 30, 51572, 51572, 1042, 11, 718, 311, 574, 412, 472, 1944, 992, 295, 9834, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.13864632938685043, "compression_ratio": 1.5174129353233832, "no_speech_prob": 3.1381086955661885e-06}, {"id": 14, "seek": 5572, "start": 72.03999999999999, "end": 76.4, "text": " For this one trading example, what do you think are reasonable values for the size of", "tokens": [50364, 1359, 3613, 295, 4190, 13, 50498, 50498, 823, 718, 311, 747, 364, 1365, 295, 257, 1782, 300, 575, 257, 2744, 295, 8132, 3732, 3521, 11, 575, 1025, 39955, 11, 50922, 50922, 293, 257, 3218, 295, 5923, 74, 420, 1848, 7526, 11, 1360, 13, 51180, 51180, 1171, 341, 472, 9529, 1365, 11, 437, 360, 291, 519, 366, 10585, 4190, 337, 264, 2744, 295, 51398, 51398, 9834, 261, 16, 293, 261, 17, 30, 51572, 51572, 1042, 11, 718, 311, 574, 412, 472, 1944, 992, 295, 9834, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.13864632938685043, "compression_ratio": 1.5174129353233832, "no_speech_prob": 3.1381086955661885e-06}, {"id": 15, "seek": 5572, "start": 76.4, "end": 79.88, "text": " parameters w1 and w2?", "tokens": [50364, 1359, 3613, 295, 4190, 13, 50498, 50498, 823, 718, 311, 747, 364, 1365, 295, 257, 1782, 300, 575, 257, 2744, 295, 8132, 3732, 3521, 11, 575, 1025, 39955, 11, 50922, 50922, 293, 257, 3218, 295, 5923, 74, 420, 1848, 7526, 11, 1360, 13, 51180, 51180, 1171, 341, 472, 9529, 1365, 11, 437, 360, 291, 519, 366, 10585, 4190, 337, 264, 2744, 295, 51398, 51398, 9834, 261, 16, 293, 261, 17, 30, 51572, 51572, 1042, 11, 718, 311, 574, 412, 472, 1944, 992, 295, 9834, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.13864632938685043, "compression_ratio": 1.5174129353233832, "no_speech_prob": 3.1381086955661885e-06}, {"id": 16, "seek": 5572, "start": 79.88, "end": 84.0, "text": " Well, let's look at one possible set of parameters.", "tokens": [50364, 1359, 3613, 295, 4190, 13, 50498, 50498, 823, 718, 311, 747, 364, 1365, 295, 257, 1782, 300, 575, 257, 2744, 295, 8132, 3732, 3521, 11, 575, 1025, 39955, 11, 50922, 50922, 293, 257, 3218, 295, 5923, 74, 420, 1848, 7526, 11, 1360, 13, 51180, 51180, 1171, 341, 472, 9529, 1365, 11, 437, 360, 291, 519, 366, 10585, 4190, 337, 264, 2744, 295, 51398, 51398, 9834, 261, 16, 293, 261, 17, 30, 51572, 51572, 1042, 11, 718, 311, 574, 412, 472, 1944, 992, 295, 9834, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.13864632938685043, "compression_ratio": 1.5174129353233832, "no_speech_prob": 3.1381086955661885e-06}, {"id": 17, "seek": 8400, "start": 84.0, "end": 94.32, "text": " Say w1 is 50, and w2 is 0.1, and b is 50 for the purposes of discussion.", "tokens": [50364, 6463, 261, 16, 307, 2625, 11, 293, 261, 17, 307, 1958, 13, 16, 11, 293, 272, 307, 2625, 337, 264, 9932, 295, 5017, 13, 50880, 50880, 407, 294, 341, 1389, 11, 264, 14109, 3218, 294, 5383, 295, 3808, 307, 2319, 11, 1360, 74, 510, 11, 1804, 1958, 13, 20, 74, 11, 1804, 51394, 51394, 2625, 74, 11, 597, 307, 4748, 670, 1848, 6879, 2459, 13, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.08248454245968141, "compression_ratio": 1.4026845637583893, "no_speech_prob": 4.425398856255924e-06}, {"id": 18, "seek": 8400, "start": 94.32, "end": 104.6, "text": " So in this case, the estimated price in thousands of dollars is 100,000k here, plus 0.5k, plus", "tokens": [50364, 6463, 261, 16, 307, 2625, 11, 293, 261, 17, 307, 1958, 13, 16, 11, 293, 272, 307, 2625, 337, 264, 9932, 295, 5017, 13, 50880, 50880, 407, 294, 341, 1389, 11, 264, 14109, 3218, 294, 5383, 295, 3808, 307, 2319, 11, 1360, 74, 510, 11, 1804, 1958, 13, 20, 74, 11, 1804, 51394, 51394, 2625, 74, 11, 597, 307, 4748, 670, 1848, 6879, 2459, 13, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.08248454245968141, "compression_ratio": 1.4026845637583893, "no_speech_prob": 4.425398856255924e-06}, {"id": 19, "seek": 8400, "start": 104.6, "end": 110.52, "text": " 50k, which is slightly over $100 million.", "tokens": [50364, 6463, 261, 16, 307, 2625, 11, 293, 261, 17, 307, 1958, 13, 16, 11, 293, 272, 307, 2625, 337, 264, 9932, 295, 5017, 13, 50880, 50880, 407, 294, 341, 1389, 11, 264, 14109, 3218, 294, 5383, 295, 3808, 307, 2319, 11, 1360, 74, 510, 11, 1804, 1958, 13, 20, 74, 11, 1804, 51394, 51394, 2625, 74, 11, 597, 307, 4748, 670, 1848, 6879, 2459, 13, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.08248454245968141, "compression_ratio": 1.4026845637583893, "no_speech_prob": 4.425398856255924e-06}, {"id": 20, "seek": 11052, "start": 110.52, "end": 117.96, "text": " So that's clearly very far from the actual price of $500,000.", "tokens": [50364, 407, 300, 311, 4448, 588, 1400, 490, 264, 3539, 3218, 295, 1848, 7526, 11, 1360, 13, 50736, 50736, 400, 370, 341, 307, 406, 257, 588, 665, 992, 295, 13075, 7994, 337, 261, 16, 293, 261, 17, 13, 51026, 51026, 823, 718, 311, 747, 257, 574, 412, 1071, 7959, 13, 51192, 51192, 6463, 261, 16, 293, 261, 17, 645, 264, 661, 636, 926, 13, 51402, 51402, 261, 16, 307, 1958, 13, 16, 11, 293, 261, 17, 307, 2625, 11, 293, 272, 307, 920, 611, 2625, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.1074311760034454, "compression_ratio": 1.4408602150537635, "no_speech_prob": 1.7880466884889756e-06}, {"id": 21, "seek": 11052, "start": 117.96, "end": 123.75999999999999, "text": " And so this is not a very good set of parameter choices for w1 and w2.", "tokens": [50364, 407, 300, 311, 4448, 588, 1400, 490, 264, 3539, 3218, 295, 1848, 7526, 11, 1360, 13, 50736, 50736, 400, 370, 341, 307, 406, 257, 588, 665, 992, 295, 13075, 7994, 337, 261, 16, 293, 261, 17, 13, 51026, 51026, 823, 718, 311, 747, 257, 574, 412, 1071, 7959, 13, 51192, 51192, 6463, 261, 16, 293, 261, 17, 645, 264, 661, 636, 926, 13, 51402, 51402, 261, 16, 307, 1958, 13, 16, 11, 293, 261, 17, 307, 2625, 11, 293, 272, 307, 920, 611, 2625, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.1074311760034454, "compression_ratio": 1.4408602150537635, "no_speech_prob": 1.7880466884889756e-06}, {"id": 22, "seek": 11052, "start": 123.75999999999999, "end": 127.08, "text": " Now let's take a look at another possibility.", "tokens": [50364, 407, 300, 311, 4448, 588, 1400, 490, 264, 3539, 3218, 295, 1848, 7526, 11, 1360, 13, 50736, 50736, 400, 370, 341, 307, 406, 257, 588, 665, 992, 295, 13075, 7994, 337, 261, 16, 293, 261, 17, 13, 51026, 51026, 823, 718, 311, 747, 257, 574, 412, 1071, 7959, 13, 51192, 51192, 6463, 261, 16, 293, 261, 17, 645, 264, 661, 636, 926, 13, 51402, 51402, 261, 16, 307, 1958, 13, 16, 11, 293, 261, 17, 307, 2625, 11, 293, 272, 307, 920, 611, 2625, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.1074311760034454, "compression_ratio": 1.4408602150537635, "no_speech_prob": 1.7880466884889756e-06}, {"id": 23, "seek": 11052, "start": 127.08, "end": 131.28, "text": " Say w1 and w2 were the other way around.", "tokens": [50364, 407, 300, 311, 4448, 588, 1400, 490, 264, 3539, 3218, 295, 1848, 7526, 11, 1360, 13, 50736, 50736, 400, 370, 341, 307, 406, 257, 588, 665, 992, 295, 13075, 7994, 337, 261, 16, 293, 261, 17, 13, 51026, 51026, 823, 718, 311, 747, 257, 574, 412, 1071, 7959, 13, 51192, 51192, 6463, 261, 16, 293, 261, 17, 645, 264, 661, 636, 926, 13, 51402, 51402, 261, 16, 307, 1958, 13, 16, 11, 293, 261, 17, 307, 2625, 11, 293, 272, 307, 920, 611, 2625, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.1074311760034454, "compression_ratio": 1.4408602150537635, "no_speech_prob": 1.7880466884889756e-06}, {"id": 24, "seek": 11052, "start": 131.28, "end": 137.48, "text": " w1 is 0.1, and w2 is 50, and b is still also 50.", "tokens": [50364, 407, 300, 311, 4448, 588, 1400, 490, 264, 3539, 3218, 295, 1848, 7526, 11, 1360, 13, 50736, 50736, 400, 370, 341, 307, 406, 257, 588, 665, 992, 295, 13075, 7994, 337, 261, 16, 293, 261, 17, 13, 51026, 51026, 823, 718, 311, 747, 257, 574, 412, 1071, 7959, 13, 51192, 51192, 6463, 261, 16, 293, 261, 17, 645, 264, 661, 636, 926, 13, 51402, 51402, 261, 16, 307, 1958, 13, 16, 11, 293, 261, 17, 307, 2625, 11, 293, 272, 307, 920, 611, 2625, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.1074311760034454, "compression_ratio": 1.4408602150537635, "no_speech_prob": 1.7880466884889756e-06}, {"id": 25, "seek": 13748, "start": 137.48, "end": 145.48, "text": " In this choice of w1 and w2, w1 is relatively small and w2 is relatively large.", "tokens": [50364, 682, 341, 3922, 295, 261, 16, 293, 261, 17, 11, 261, 16, 307, 7226, 1359, 293, 261, 17, 307, 7226, 2416, 13, 50764, 50764, 2625, 307, 709, 3801, 813, 1958, 13, 16, 13, 50916, 50916, 407, 510, 264, 19147, 3218, 307, 1958, 13, 16, 1413, 8132, 1804, 2625, 1413, 1025, 1804, 2625, 13, 51410, 51410, 440, 700, 1433, 3643, 2331, 74, 11, 264, 1150, 1433, 3643, 11650, 74, 11, 293, 550, 1804, 2625, 13, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.09274891706613395, "compression_ratio": 1.59375, "no_speech_prob": 4.356855697551509e-06}, {"id": 26, "seek": 13748, "start": 145.48, "end": 148.51999999999998, "text": " 50 is much bigger than 0.1.", "tokens": [50364, 682, 341, 3922, 295, 261, 16, 293, 261, 17, 11, 261, 16, 307, 7226, 1359, 293, 261, 17, 307, 7226, 2416, 13, 50764, 50764, 2625, 307, 709, 3801, 813, 1958, 13, 16, 13, 50916, 50916, 407, 510, 264, 19147, 3218, 307, 1958, 13, 16, 1413, 8132, 1804, 2625, 1413, 1025, 1804, 2625, 13, 51410, 51410, 440, 700, 1433, 3643, 2331, 74, 11, 264, 1150, 1433, 3643, 11650, 74, 11, 293, 550, 1804, 2625, 13, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.09274891706613395, "compression_ratio": 1.59375, "no_speech_prob": 4.356855697551509e-06}, {"id": 27, "seek": 13748, "start": 148.51999999999998, "end": 158.39999999999998, "text": " So here the predicted price is 0.1 times 2000 plus 50 times 5 plus 50.", "tokens": [50364, 682, 341, 3922, 295, 261, 16, 293, 261, 17, 11, 261, 16, 307, 7226, 1359, 293, 261, 17, 307, 7226, 2416, 13, 50764, 50764, 2625, 307, 709, 3801, 813, 1958, 13, 16, 13, 50916, 50916, 407, 510, 264, 19147, 3218, 307, 1958, 13, 16, 1413, 8132, 1804, 2625, 1413, 1025, 1804, 2625, 13, 51410, 51410, 440, 700, 1433, 3643, 2331, 74, 11, 264, 1150, 1433, 3643, 11650, 74, 11, 293, 550, 1804, 2625, 13, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.09274891706613395, "compression_ratio": 1.59375, "no_speech_prob": 4.356855697551509e-06}, {"id": 28, "seek": 13748, "start": 158.39999999999998, "end": 165.6, "text": " The first term becomes 200k, the second term becomes 250k, and then plus 50.", "tokens": [50364, 682, 341, 3922, 295, 261, 16, 293, 261, 17, 11, 261, 16, 307, 7226, 1359, 293, 261, 17, 307, 7226, 2416, 13, 50764, 50764, 2625, 307, 709, 3801, 813, 1958, 13, 16, 13, 50916, 50916, 407, 510, 264, 19147, 3218, 307, 1958, 13, 16, 1413, 8132, 1804, 2625, 1413, 1025, 1804, 2625, 13, 51410, 51410, 440, 700, 1433, 3643, 2331, 74, 11, 264, 1150, 1433, 3643, 11650, 74, 11, 293, 550, 1804, 2625, 13, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.09274891706613395, "compression_ratio": 1.59375, "no_speech_prob": 4.356855697551509e-06}, {"id": 29, "seek": 16560, "start": 165.6, "end": 171.6, "text": " So this version of the model predicts a price of $500,000, which is a much more reasonable", "tokens": [50364, 407, 341, 3037, 295, 264, 2316, 6069, 82, 257, 3218, 295, 1848, 7526, 11, 1360, 11, 597, 307, 257, 709, 544, 10585, 50664, 50664, 12539, 293, 2314, 281, 312, 264, 912, 3218, 382, 264, 2074, 3218, 295, 264, 1782, 13, 50923, 50923, 407, 4696, 291, 1062, 3449, 300, 562, 257, 1944, 3613, 295, 4190, 295, 257, 4111, 307, 2416, 11, 51224, 51224, 411, 264, 2744, 294, 3732, 3521, 11, 597, 1709, 439, 264, 636, 493, 281, 8132, 11, 309, 311, 544, 3700, 300, 51466, 51466, 257, 665, 2316, 486, 1466, 281, 2826, 257, 7226, 1359, 13075, 2158, 411, 1958, 13, 16, 13, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.11346909684954949, "compression_ratio": 1.6342412451361867, "no_speech_prob": 2.6425202577229356e-06}, {"id": 30, "seek": 16560, "start": 171.6, "end": 176.78, "text": " estimate and happens to be the same price as the true price of the house.", "tokens": [50364, 407, 341, 3037, 295, 264, 2316, 6069, 82, 257, 3218, 295, 1848, 7526, 11, 1360, 11, 597, 307, 257, 709, 544, 10585, 50664, 50664, 12539, 293, 2314, 281, 312, 264, 912, 3218, 382, 264, 2074, 3218, 295, 264, 1782, 13, 50923, 50923, 407, 4696, 291, 1062, 3449, 300, 562, 257, 1944, 3613, 295, 4190, 295, 257, 4111, 307, 2416, 11, 51224, 51224, 411, 264, 2744, 294, 3732, 3521, 11, 597, 1709, 439, 264, 636, 493, 281, 8132, 11, 309, 311, 544, 3700, 300, 51466, 51466, 257, 665, 2316, 486, 1466, 281, 2826, 257, 7226, 1359, 13075, 2158, 411, 1958, 13, 16, 13, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.11346909684954949, "compression_ratio": 1.6342412451361867, "no_speech_prob": 2.6425202577229356e-06}, {"id": 31, "seek": 16560, "start": 176.78, "end": 182.79999999999998, "text": " So hopefully you might notice that when a possible range of values of a feature is large,", "tokens": [50364, 407, 341, 3037, 295, 264, 2316, 6069, 82, 257, 3218, 295, 1848, 7526, 11, 1360, 11, 597, 307, 257, 709, 544, 10585, 50664, 50664, 12539, 293, 2314, 281, 312, 264, 912, 3218, 382, 264, 2074, 3218, 295, 264, 1782, 13, 50923, 50923, 407, 4696, 291, 1062, 3449, 300, 562, 257, 1944, 3613, 295, 4190, 295, 257, 4111, 307, 2416, 11, 51224, 51224, 411, 264, 2744, 294, 3732, 3521, 11, 597, 1709, 439, 264, 636, 493, 281, 8132, 11, 309, 311, 544, 3700, 300, 51466, 51466, 257, 665, 2316, 486, 1466, 281, 2826, 257, 7226, 1359, 13075, 2158, 411, 1958, 13, 16, 13, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.11346909684954949, "compression_ratio": 1.6342412451361867, "no_speech_prob": 2.6425202577229356e-06}, {"id": 32, "seek": 16560, "start": 182.79999999999998, "end": 187.64, "text": " like the size in square feet, which goes all the way up to 2000, it's more likely that", "tokens": [50364, 407, 341, 3037, 295, 264, 2316, 6069, 82, 257, 3218, 295, 1848, 7526, 11, 1360, 11, 597, 307, 257, 709, 544, 10585, 50664, 50664, 12539, 293, 2314, 281, 312, 264, 912, 3218, 382, 264, 2074, 3218, 295, 264, 1782, 13, 50923, 50923, 407, 4696, 291, 1062, 3449, 300, 562, 257, 1944, 3613, 295, 4190, 295, 257, 4111, 307, 2416, 11, 51224, 51224, 411, 264, 2744, 294, 3732, 3521, 11, 597, 1709, 439, 264, 636, 493, 281, 8132, 11, 309, 311, 544, 3700, 300, 51466, 51466, 257, 665, 2316, 486, 1466, 281, 2826, 257, 7226, 1359, 13075, 2158, 411, 1958, 13, 16, 13, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.11346909684954949, "compression_ratio": 1.6342412451361867, "no_speech_prob": 2.6425202577229356e-06}, {"id": 33, "seek": 16560, "start": 187.64, "end": 194.0, "text": " a good model will learn to choose a relatively small parameter value like 0.1.", "tokens": [50364, 407, 341, 3037, 295, 264, 2316, 6069, 82, 257, 3218, 295, 1848, 7526, 11, 1360, 11, 597, 307, 257, 709, 544, 10585, 50664, 50664, 12539, 293, 2314, 281, 312, 264, 912, 3218, 382, 264, 2074, 3218, 295, 264, 1782, 13, 50923, 50923, 407, 4696, 291, 1062, 3449, 300, 562, 257, 1944, 3613, 295, 4190, 295, 257, 4111, 307, 2416, 11, 51224, 51224, 411, 264, 2744, 294, 3732, 3521, 11, 597, 1709, 439, 264, 636, 493, 281, 8132, 11, 309, 311, 544, 3700, 300, 51466, 51466, 257, 665, 2316, 486, 1466, 281, 2826, 257, 7226, 1359, 13075, 2158, 411, 1958, 13, 16, 13, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.11346909684954949, "compression_ratio": 1.6342412451361867, "no_speech_prob": 2.6425202577229356e-06}, {"id": 34, "seek": 19400, "start": 194.0, "end": 200.92, "text": " Likewise, when the possible values of a feature are small, like the number of bedrooms, then", "tokens": [50364, 30269, 11, 562, 264, 1944, 4190, 295, 257, 4111, 366, 1359, 11, 411, 264, 1230, 295, 39955, 11, 550, 50710, 50710, 257, 10585, 2158, 337, 1080, 9834, 486, 312, 7226, 2416, 11, 411, 2625, 13, 50998, 50998, 407, 577, 775, 341, 10961, 281, 16235, 23475, 30, 51164, 51164, 1042, 11, 718, 311, 747, 257, 574, 412, 257, 34951, 7542, 295, 264, 4122, 11, 689, 264, 2744, 294, 3732, 51508, 51508, 3521, 307, 264, 12750, 10298, 11, 2031, 16, 11, 293, 264, 1230, 295, 39955, 11, 2031, 17, 11, 307, 322, 264, 9429, 10298, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13551387981492646, "compression_ratio": 1.6271186440677967, "no_speech_prob": 4.5658889575861394e-06}, {"id": 35, "seek": 19400, "start": 200.92, "end": 206.68, "text": " a reasonable value for its parameters will be relatively large, like 50.", "tokens": [50364, 30269, 11, 562, 264, 1944, 4190, 295, 257, 4111, 366, 1359, 11, 411, 264, 1230, 295, 39955, 11, 550, 50710, 50710, 257, 10585, 2158, 337, 1080, 9834, 486, 312, 7226, 2416, 11, 411, 2625, 13, 50998, 50998, 407, 577, 775, 341, 10961, 281, 16235, 23475, 30, 51164, 51164, 1042, 11, 718, 311, 747, 257, 574, 412, 257, 34951, 7542, 295, 264, 4122, 11, 689, 264, 2744, 294, 3732, 51508, 51508, 3521, 307, 264, 12750, 10298, 11, 2031, 16, 11, 293, 264, 1230, 295, 39955, 11, 2031, 17, 11, 307, 322, 264, 9429, 10298, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13551387981492646, "compression_ratio": 1.6271186440677967, "no_speech_prob": 4.5658889575861394e-06}, {"id": 36, "seek": 19400, "start": 206.68, "end": 210.0, "text": " So how does this relate to gradient descent?", "tokens": [50364, 30269, 11, 562, 264, 1944, 4190, 295, 257, 4111, 366, 1359, 11, 411, 264, 1230, 295, 39955, 11, 550, 50710, 50710, 257, 10585, 2158, 337, 1080, 9834, 486, 312, 7226, 2416, 11, 411, 2625, 13, 50998, 50998, 407, 577, 775, 341, 10961, 281, 16235, 23475, 30, 51164, 51164, 1042, 11, 718, 311, 747, 257, 574, 412, 257, 34951, 7542, 295, 264, 4122, 11, 689, 264, 2744, 294, 3732, 51508, 51508, 3521, 307, 264, 12750, 10298, 11, 2031, 16, 11, 293, 264, 1230, 295, 39955, 11, 2031, 17, 11, 307, 322, 264, 9429, 10298, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13551387981492646, "compression_ratio": 1.6271186440677967, "no_speech_prob": 4.5658889575861394e-06}, {"id": 37, "seek": 19400, "start": 210.0, "end": 216.88, "text": " Well, let's take a look at a scatter plot of the features, where the size in square", "tokens": [50364, 30269, 11, 562, 264, 1944, 4190, 295, 257, 4111, 366, 1359, 11, 411, 264, 1230, 295, 39955, 11, 550, 50710, 50710, 257, 10585, 2158, 337, 1080, 9834, 486, 312, 7226, 2416, 11, 411, 2625, 13, 50998, 50998, 407, 577, 775, 341, 10961, 281, 16235, 23475, 30, 51164, 51164, 1042, 11, 718, 311, 747, 257, 574, 412, 257, 34951, 7542, 295, 264, 4122, 11, 689, 264, 2744, 294, 3732, 51508, 51508, 3521, 307, 264, 12750, 10298, 11, 2031, 16, 11, 293, 264, 1230, 295, 39955, 11, 2031, 17, 11, 307, 322, 264, 9429, 10298, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13551387981492646, "compression_ratio": 1.6271186440677967, "no_speech_prob": 4.5658889575861394e-06}, {"id": 38, "seek": 19400, "start": 216.88, "end": 223.96, "text": " feet is the horizontal axis, x1, and the number of bedrooms, x2, is on the vertical axis.", "tokens": [50364, 30269, 11, 562, 264, 1944, 4190, 295, 257, 4111, 366, 1359, 11, 411, 264, 1230, 295, 39955, 11, 550, 50710, 50710, 257, 10585, 2158, 337, 1080, 9834, 486, 312, 7226, 2416, 11, 411, 2625, 13, 50998, 50998, 407, 577, 775, 341, 10961, 281, 16235, 23475, 30, 51164, 51164, 1042, 11, 718, 311, 747, 257, 574, 412, 257, 34951, 7542, 295, 264, 4122, 11, 689, 264, 2744, 294, 3732, 51508, 51508, 3521, 307, 264, 12750, 10298, 11, 2031, 16, 11, 293, 264, 1230, 295, 39955, 11, 2031, 17, 11, 307, 322, 264, 9429, 10298, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13551387981492646, "compression_ratio": 1.6271186440677967, "no_speech_prob": 4.5658889575861394e-06}, {"id": 39, "seek": 22396, "start": 223.96, "end": 229.72, "text": " If you plot the training data, you notice that the horizontal axis is on a much larger", "tokens": [50364, 759, 291, 7542, 264, 3097, 1412, 11, 291, 3449, 300, 264, 12750, 10298, 307, 322, 257, 709, 4833, 50652, 50652, 4373, 420, 709, 4833, 3613, 295, 4190, 5347, 281, 264, 9429, 10298, 13, 50902, 50902, 3087, 11, 718, 311, 574, 412, 577, 264, 2063, 2445, 1062, 574, 294, 257, 21234, 7542, 13, 51228, 51228, 509, 1062, 536, 257, 21234, 7542, 689, 264, 12750, 10298, 575, 257, 709, 46751, 3613, 11, 584, 1296, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.10622512666802657, "compression_ratio": 1.7204301075268817, "no_speech_prob": 1.4738659046997782e-05}, {"id": 40, "seek": 22396, "start": 229.72, "end": 234.72, "text": " scale or much larger range of values compared to the vertical axis.", "tokens": [50364, 759, 291, 7542, 264, 3097, 1412, 11, 291, 3449, 300, 264, 12750, 10298, 307, 322, 257, 709, 4833, 50652, 50652, 4373, 420, 709, 4833, 3613, 295, 4190, 5347, 281, 264, 9429, 10298, 13, 50902, 50902, 3087, 11, 718, 311, 574, 412, 577, 264, 2063, 2445, 1062, 574, 294, 257, 21234, 7542, 13, 51228, 51228, 509, 1062, 536, 257, 21234, 7542, 689, 264, 12750, 10298, 575, 257, 709, 46751, 3613, 11, 584, 1296, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.10622512666802657, "compression_ratio": 1.7204301075268817, "no_speech_prob": 1.4738659046997782e-05}, {"id": 41, "seek": 22396, "start": 234.72, "end": 241.24, "text": " Next, let's look at how the cost function might look in a contour plot.", "tokens": [50364, 759, 291, 7542, 264, 3097, 1412, 11, 291, 3449, 300, 264, 12750, 10298, 307, 322, 257, 709, 4833, 50652, 50652, 4373, 420, 709, 4833, 3613, 295, 4190, 5347, 281, 264, 9429, 10298, 13, 50902, 50902, 3087, 11, 718, 311, 574, 412, 577, 264, 2063, 2445, 1062, 574, 294, 257, 21234, 7542, 13, 51228, 51228, 509, 1062, 536, 257, 21234, 7542, 689, 264, 12750, 10298, 575, 257, 709, 46751, 3613, 11, 584, 1296, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.10622512666802657, "compression_ratio": 1.7204301075268817, "no_speech_prob": 1.4738659046997782e-05}, {"id": 42, "seek": 22396, "start": 241.24, "end": 248.16, "text": " You might see a contour plot where the horizontal axis has a much narrower range, say between", "tokens": [50364, 759, 291, 7542, 264, 3097, 1412, 11, 291, 3449, 300, 264, 12750, 10298, 307, 322, 257, 709, 4833, 50652, 50652, 4373, 420, 709, 4833, 3613, 295, 4190, 5347, 281, 264, 9429, 10298, 13, 50902, 50902, 3087, 11, 718, 311, 574, 412, 577, 264, 2063, 2445, 1062, 574, 294, 257, 21234, 7542, 13, 51228, 51228, 509, 1062, 536, 257, 21234, 7542, 689, 264, 12750, 10298, 575, 257, 709, 46751, 3613, 11, 584, 1296, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.10622512666802657, "compression_ratio": 1.7204301075268817, "no_speech_prob": 1.4738659046997782e-05}, {"id": 43, "seek": 24816, "start": 248.16, "end": 256.2, "text": " 0 and 1, whereas the vertical axis takes on much larger values, say between 10 and 100.", "tokens": [50364, 1958, 293, 502, 11, 9735, 264, 9429, 10298, 2516, 322, 709, 4833, 4190, 11, 584, 1296, 1266, 293, 2319, 13, 50766, 50766, 407, 264, 660, 5067, 1254, 14187, 1124, 420, 8284, 2600, 279, 11, 293, 436, 434, 11639, 322, 472, 1252, 293, 2854, 322, 51076, 51076, 264, 661, 13, 51156, 51156, 400, 341, 307, 570, 257, 588, 1359, 1319, 281, 261, 16, 393, 362, 257, 588, 2416, 2712, 322, 264, 14109, 51470, 51470, 3218, 293, 8807, 257, 588, 2416, 2712, 322, 264, 2063, 361, 11, 570, 261, 16, 12258, 281, 312, 17207, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.13007151087125143, "compression_ratio": 1.6589861751152073, "no_speech_prob": 5.42218276677886e-06}, {"id": 44, "seek": 24816, "start": 256.2, "end": 262.4, "text": " So the contours form ovals or ellipses, and they're shorter on one side and longer on", "tokens": [50364, 1958, 293, 502, 11, 9735, 264, 9429, 10298, 2516, 322, 709, 4833, 4190, 11, 584, 1296, 1266, 293, 2319, 13, 50766, 50766, 407, 264, 660, 5067, 1254, 14187, 1124, 420, 8284, 2600, 279, 11, 293, 436, 434, 11639, 322, 472, 1252, 293, 2854, 322, 51076, 51076, 264, 661, 13, 51156, 51156, 400, 341, 307, 570, 257, 588, 1359, 1319, 281, 261, 16, 393, 362, 257, 588, 2416, 2712, 322, 264, 14109, 51470, 51470, 3218, 293, 8807, 257, 588, 2416, 2712, 322, 264, 2063, 361, 11, 570, 261, 16, 12258, 281, 312, 17207, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.13007151087125143, "compression_ratio": 1.6589861751152073, "no_speech_prob": 5.42218276677886e-06}, {"id": 45, "seek": 24816, "start": 262.4, "end": 264.0, "text": " the other.", "tokens": [50364, 1958, 293, 502, 11, 9735, 264, 9429, 10298, 2516, 322, 709, 4833, 4190, 11, 584, 1296, 1266, 293, 2319, 13, 50766, 50766, 407, 264, 660, 5067, 1254, 14187, 1124, 420, 8284, 2600, 279, 11, 293, 436, 434, 11639, 322, 472, 1252, 293, 2854, 322, 51076, 51076, 264, 661, 13, 51156, 51156, 400, 341, 307, 570, 257, 588, 1359, 1319, 281, 261, 16, 393, 362, 257, 588, 2416, 2712, 322, 264, 14109, 51470, 51470, 3218, 293, 8807, 257, 588, 2416, 2712, 322, 264, 2063, 361, 11, 570, 261, 16, 12258, 281, 312, 17207, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.13007151087125143, "compression_ratio": 1.6589861751152073, "no_speech_prob": 5.42218276677886e-06}, {"id": 46, "seek": 24816, "start": 264.0, "end": 270.28, "text": " And this is because a very small change to w1 can have a very large impact on the estimated", "tokens": [50364, 1958, 293, 502, 11, 9735, 264, 9429, 10298, 2516, 322, 709, 4833, 4190, 11, 584, 1296, 1266, 293, 2319, 13, 50766, 50766, 407, 264, 660, 5067, 1254, 14187, 1124, 420, 8284, 2600, 279, 11, 293, 436, 434, 11639, 322, 472, 1252, 293, 2854, 322, 51076, 51076, 264, 661, 13, 51156, 51156, 400, 341, 307, 570, 257, 588, 1359, 1319, 281, 261, 16, 393, 362, 257, 588, 2416, 2712, 322, 264, 14109, 51470, 51470, 3218, 293, 8807, 257, 588, 2416, 2712, 322, 264, 2063, 361, 11, 570, 261, 16, 12258, 281, 312, 17207, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.13007151087125143, "compression_ratio": 1.6589861751152073, "no_speech_prob": 5.42218276677886e-06}, {"id": 47, "seek": 24816, "start": 270.28, "end": 277.36, "text": " price and thus a very large impact on the cost j, because w1 tends to be multiplied", "tokens": [50364, 1958, 293, 502, 11, 9735, 264, 9429, 10298, 2516, 322, 709, 4833, 4190, 11, 584, 1296, 1266, 293, 2319, 13, 50766, 50766, 407, 264, 660, 5067, 1254, 14187, 1124, 420, 8284, 2600, 279, 11, 293, 436, 434, 11639, 322, 472, 1252, 293, 2854, 322, 51076, 51076, 264, 661, 13, 51156, 51156, 400, 341, 307, 570, 257, 588, 1359, 1319, 281, 261, 16, 393, 362, 257, 588, 2416, 2712, 322, 264, 14109, 51470, 51470, 3218, 293, 8807, 257, 588, 2416, 2712, 322, 264, 2063, 361, 11, 570, 261, 16, 12258, 281, 312, 17207, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.13007151087125143, "compression_ratio": 1.6589861751152073, "no_speech_prob": 5.42218276677886e-06}, {"id": 48, "seek": 27736, "start": 277.36, "end": 281.76, "text": " by a very large number, the size in square feet.", "tokens": [50364, 538, 257, 588, 2416, 1230, 11, 264, 2744, 294, 3732, 3521, 13, 50584, 50584, 682, 8712, 11, 309, 2516, 257, 709, 4833, 1319, 294, 261, 17, 294, 1668, 281, 1319, 264, 21264, 709, 11, 50916, 50916, 293, 8807, 1359, 2962, 281, 261, 17, 500, 380, 1319, 264, 2063, 2445, 6217, 382, 709, 13, 51239, 51239, 407, 689, 775, 341, 1856, 505, 30, 51340, 51340, 639, 307, 437, 1062, 917, 493, 2737, 498, 291, 645, 281, 1190, 16235, 23475, 11, 498, 291, 645, 51578, 51578, 281, 764, 428, 3097, 1412, 382, 307, 13, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.07625896732012431, "compression_ratio": 1.6188340807174888, "no_speech_prob": 3.96683981307433e-06}, {"id": 49, "seek": 27736, "start": 281.76, "end": 288.40000000000003, "text": " In contrast, it takes a much larger change in w2 in order to change the predictions much,", "tokens": [50364, 538, 257, 588, 2416, 1230, 11, 264, 2744, 294, 3732, 3521, 13, 50584, 50584, 682, 8712, 11, 309, 2516, 257, 709, 4833, 1319, 294, 261, 17, 294, 1668, 281, 1319, 264, 21264, 709, 11, 50916, 50916, 293, 8807, 1359, 2962, 281, 261, 17, 500, 380, 1319, 264, 2063, 2445, 6217, 382, 709, 13, 51239, 51239, 407, 689, 775, 341, 1856, 505, 30, 51340, 51340, 639, 307, 437, 1062, 917, 493, 2737, 498, 291, 645, 281, 1190, 16235, 23475, 11, 498, 291, 645, 51578, 51578, 281, 764, 428, 3097, 1412, 382, 307, 13, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.07625896732012431, "compression_ratio": 1.6188340807174888, "no_speech_prob": 3.96683981307433e-06}, {"id": 50, "seek": 27736, "start": 288.40000000000003, "end": 294.86, "text": " and thus small changes to w2 don't change the cost function nearly as much.", "tokens": [50364, 538, 257, 588, 2416, 1230, 11, 264, 2744, 294, 3732, 3521, 13, 50584, 50584, 682, 8712, 11, 309, 2516, 257, 709, 4833, 1319, 294, 261, 17, 294, 1668, 281, 1319, 264, 21264, 709, 11, 50916, 50916, 293, 8807, 1359, 2962, 281, 261, 17, 500, 380, 1319, 264, 2063, 2445, 6217, 382, 709, 13, 51239, 51239, 407, 689, 775, 341, 1856, 505, 30, 51340, 51340, 639, 307, 437, 1062, 917, 493, 2737, 498, 291, 645, 281, 1190, 16235, 23475, 11, 498, 291, 645, 51578, 51578, 281, 764, 428, 3097, 1412, 382, 307, 13, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.07625896732012431, "compression_ratio": 1.6188340807174888, "no_speech_prob": 3.96683981307433e-06}, {"id": 51, "seek": 27736, "start": 294.86, "end": 296.88, "text": " So where does this leave us?", "tokens": [50364, 538, 257, 588, 2416, 1230, 11, 264, 2744, 294, 3732, 3521, 13, 50584, 50584, 682, 8712, 11, 309, 2516, 257, 709, 4833, 1319, 294, 261, 17, 294, 1668, 281, 1319, 264, 21264, 709, 11, 50916, 50916, 293, 8807, 1359, 2962, 281, 261, 17, 500, 380, 1319, 264, 2063, 2445, 6217, 382, 709, 13, 51239, 51239, 407, 689, 775, 341, 1856, 505, 30, 51340, 51340, 639, 307, 437, 1062, 917, 493, 2737, 498, 291, 645, 281, 1190, 16235, 23475, 11, 498, 291, 645, 51578, 51578, 281, 764, 428, 3097, 1412, 382, 307, 13, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.07625896732012431, "compression_ratio": 1.6188340807174888, "no_speech_prob": 3.96683981307433e-06}, {"id": 52, "seek": 27736, "start": 296.88, "end": 301.64, "text": " This is what might end up happening if you were to run gradient descent, if you were", "tokens": [50364, 538, 257, 588, 2416, 1230, 11, 264, 2744, 294, 3732, 3521, 13, 50584, 50584, 682, 8712, 11, 309, 2516, 257, 709, 4833, 1319, 294, 261, 17, 294, 1668, 281, 1319, 264, 21264, 709, 11, 50916, 50916, 293, 8807, 1359, 2962, 281, 261, 17, 500, 380, 1319, 264, 2063, 2445, 6217, 382, 709, 13, 51239, 51239, 407, 689, 775, 341, 1856, 505, 30, 51340, 51340, 639, 307, 437, 1062, 917, 493, 2737, 498, 291, 645, 281, 1190, 16235, 23475, 11, 498, 291, 645, 51578, 51578, 281, 764, 428, 3097, 1412, 382, 307, 13, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.07625896732012431, "compression_ratio": 1.6188340807174888, "no_speech_prob": 3.96683981307433e-06}, {"id": 53, "seek": 27736, "start": 301.64, "end": 305.04, "text": " to use your training data as is.", "tokens": [50364, 538, 257, 588, 2416, 1230, 11, 264, 2744, 294, 3732, 3521, 13, 50584, 50584, 682, 8712, 11, 309, 2516, 257, 709, 4833, 1319, 294, 261, 17, 294, 1668, 281, 1319, 264, 21264, 709, 11, 50916, 50916, 293, 8807, 1359, 2962, 281, 261, 17, 500, 380, 1319, 264, 2063, 2445, 6217, 382, 709, 13, 51239, 51239, 407, 689, 775, 341, 1856, 505, 30, 51340, 51340, 639, 307, 437, 1062, 917, 493, 2737, 498, 291, 645, 281, 1190, 16235, 23475, 11, 498, 291, 645, 51578, 51578, 281, 764, 428, 3097, 1412, 382, 307, 13, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.07625896732012431, "compression_ratio": 1.6188340807174888, "no_speech_prob": 3.96683981307433e-06}, {"id": 54, "seek": 30504, "start": 305.04, "end": 311.16, "text": " Because the contours are so tall and skinny, gradient descent may end up bouncing back", "tokens": [50364, 1436, 264, 660, 5067, 366, 370, 6764, 293, 25193, 11, 16235, 23475, 815, 917, 493, 27380, 646, 50670, 50670, 293, 5220, 337, 257, 938, 565, 949, 309, 393, 2721, 915, 1080, 636, 281, 264, 4338, 7285, 13, 50990, 50990, 682, 6851, 411, 341, 11, 257, 4420, 551, 281, 360, 307, 281, 4373, 264, 4122, 13, 51249, 51249, 639, 1355, 10205, 512, 9887, 295, 428, 3097, 1412, 370, 300, 2031, 16, 11, 584, 11, 1062, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.06860184669494629, "compression_ratio": 1.5446009389671362, "no_speech_prob": 9.080268682737369e-06}, {"id": 55, "seek": 30504, "start": 311.16, "end": 317.56, "text": " and forth for a long time before it can finally find its way to the global minimum.", "tokens": [50364, 1436, 264, 660, 5067, 366, 370, 6764, 293, 25193, 11, 16235, 23475, 815, 917, 493, 27380, 646, 50670, 50670, 293, 5220, 337, 257, 938, 565, 949, 309, 393, 2721, 915, 1080, 636, 281, 264, 4338, 7285, 13, 50990, 50990, 682, 6851, 411, 341, 11, 257, 4420, 551, 281, 360, 307, 281, 4373, 264, 4122, 13, 51249, 51249, 639, 1355, 10205, 512, 9887, 295, 428, 3097, 1412, 370, 300, 2031, 16, 11, 584, 11, 1062, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.06860184669494629, "compression_ratio": 1.5446009389671362, "no_speech_prob": 9.080268682737369e-06}, {"id": 56, "seek": 30504, "start": 317.56, "end": 322.74, "text": " In situations like this, a useful thing to do is to scale the features.", "tokens": [50364, 1436, 264, 660, 5067, 366, 370, 6764, 293, 25193, 11, 16235, 23475, 815, 917, 493, 27380, 646, 50670, 50670, 293, 5220, 337, 257, 938, 565, 949, 309, 393, 2721, 915, 1080, 636, 281, 264, 4338, 7285, 13, 50990, 50990, 682, 6851, 411, 341, 11, 257, 4420, 551, 281, 360, 307, 281, 4373, 264, 4122, 13, 51249, 51249, 639, 1355, 10205, 512, 9887, 295, 428, 3097, 1412, 370, 300, 2031, 16, 11, 584, 11, 1062, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.06860184669494629, "compression_ratio": 1.5446009389671362, "no_speech_prob": 9.080268682737369e-06}, {"id": 57, "seek": 30504, "start": 322.74, "end": 328.8, "text": " This means performing some transformation of your training data so that x1, say, might", "tokens": [50364, 1436, 264, 660, 5067, 366, 370, 6764, 293, 25193, 11, 16235, 23475, 815, 917, 493, 27380, 646, 50670, 50670, 293, 5220, 337, 257, 938, 565, 949, 309, 393, 2721, 915, 1080, 636, 281, 264, 4338, 7285, 13, 50990, 50990, 682, 6851, 411, 341, 11, 257, 4420, 551, 281, 360, 307, 281, 4373, 264, 4122, 13, 51249, 51249, 639, 1355, 10205, 512, 9887, 295, 428, 3097, 1412, 370, 300, 2031, 16, 11, 584, 11, 1062, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.06860184669494629, "compression_ratio": 1.5446009389671362, "no_speech_prob": 9.080268682737369e-06}, {"id": 58, "seek": 32880, "start": 328.8, "end": 335.32, "text": " now range from 0 to 1, and x2 might also range from 0 to 1.", "tokens": [50364, 586, 3613, 490, 1958, 281, 502, 11, 293, 2031, 17, 1062, 611, 3613, 490, 1958, 281, 502, 13, 50690, 50690, 407, 264, 1412, 2793, 586, 574, 544, 411, 341, 11, 293, 291, 1062, 3449, 300, 264, 4373, 295, 264, 50924, 50924, 7542, 322, 264, 2767, 307, 586, 1596, 819, 813, 264, 472, 322, 1192, 13, 51186, 51186, 440, 2141, 935, 307, 300, 264, 9610, 5573, 2031, 16, 293, 2031, 17, 366, 1293, 586, 1940, 25323, 22526, 295, 51506, 51506, 4190, 281, 1184, 661, 13, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.0856027223847129, "compression_ratio": 1.6772486772486772, "no_speech_prob": 2.7264052278042072e-06}, {"id": 59, "seek": 32880, "start": 335.32, "end": 340.0, "text": " So the data points now look more like this, and you might notice that the scale of the", "tokens": [50364, 586, 3613, 490, 1958, 281, 502, 11, 293, 2031, 17, 1062, 611, 3613, 490, 1958, 281, 502, 13, 50690, 50690, 407, 264, 1412, 2793, 586, 574, 544, 411, 341, 11, 293, 291, 1062, 3449, 300, 264, 4373, 295, 264, 50924, 50924, 7542, 322, 264, 2767, 307, 586, 1596, 819, 813, 264, 472, 322, 1192, 13, 51186, 51186, 440, 2141, 935, 307, 300, 264, 9610, 5573, 2031, 16, 293, 2031, 17, 366, 1293, 586, 1940, 25323, 22526, 295, 51506, 51506, 4190, 281, 1184, 661, 13, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.0856027223847129, "compression_ratio": 1.6772486772486772, "no_speech_prob": 2.7264052278042072e-06}, {"id": 60, "seek": 32880, "start": 340.0, "end": 345.24, "text": " plot on the bottom is now quite different than the one on top.", "tokens": [50364, 586, 3613, 490, 1958, 281, 502, 11, 293, 2031, 17, 1062, 611, 3613, 490, 1958, 281, 502, 13, 50690, 50690, 407, 264, 1412, 2793, 586, 574, 544, 411, 341, 11, 293, 291, 1062, 3449, 300, 264, 4373, 295, 264, 50924, 50924, 7542, 322, 264, 2767, 307, 586, 1596, 819, 813, 264, 472, 322, 1192, 13, 51186, 51186, 440, 2141, 935, 307, 300, 264, 9610, 5573, 2031, 16, 293, 2031, 17, 366, 1293, 586, 1940, 25323, 22526, 295, 51506, 51506, 4190, 281, 1184, 661, 13, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.0856027223847129, "compression_ratio": 1.6772486772486772, "no_speech_prob": 2.7264052278042072e-06}, {"id": 61, "seek": 32880, "start": 345.24, "end": 351.64, "text": " The key point is that the rescaled x1 and x2 are both now taking comparable ranges of", "tokens": [50364, 586, 3613, 490, 1958, 281, 502, 11, 293, 2031, 17, 1062, 611, 3613, 490, 1958, 281, 502, 13, 50690, 50690, 407, 264, 1412, 2793, 586, 574, 544, 411, 341, 11, 293, 291, 1062, 3449, 300, 264, 4373, 295, 264, 50924, 50924, 7542, 322, 264, 2767, 307, 586, 1596, 819, 813, 264, 472, 322, 1192, 13, 51186, 51186, 440, 2141, 935, 307, 300, 264, 9610, 5573, 2031, 16, 293, 2031, 17, 366, 1293, 586, 1940, 25323, 22526, 295, 51506, 51506, 4190, 281, 1184, 661, 13, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.0856027223847129, "compression_ratio": 1.6772486772486772, "no_speech_prob": 2.7264052278042072e-06}, {"id": 62, "seek": 32880, "start": 351.64, "end": 354.44, "text": " values to each other.", "tokens": [50364, 586, 3613, 490, 1958, 281, 502, 11, 293, 2031, 17, 1062, 611, 3613, 490, 1958, 281, 502, 13, 50690, 50690, 407, 264, 1412, 2793, 586, 574, 544, 411, 341, 11, 293, 291, 1062, 3449, 300, 264, 4373, 295, 264, 50924, 50924, 7542, 322, 264, 2767, 307, 586, 1596, 819, 813, 264, 472, 322, 1192, 13, 51186, 51186, 440, 2141, 935, 307, 300, 264, 9610, 5573, 2031, 16, 293, 2031, 17, 366, 1293, 586, 1940, 25323, 22526, 295, 51506, 51506, 4190, 281, 1184, 661, 13, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.0856027223847129, "compression_ratio": 1.6772486772486772, "no_speech_prob": 2.7264052278042072e-06}, {"id": 63, "seek": 35444, "start": 354.44, "end": 361.24, "text": " And if you run gradient descent on a cost function defined on this rescaled x1 and x2", "tokens": [50364, 400, 498, 291, 1190, 16235, 23475, 322, 257, 2063, 2445, 7642, 322, 341, 9610, 5573, 2031, 16, 293, 2031, 17, 50704, 50704, 1228, 341, 16894, 1412, 11, 550, 264, 660, 5067, 486, 574, 544, 411, 341, 11, 544, 411, 13040, 51003, 51003, 293, 1570, 6764, 293, 25193, 11, 293, 16235, 23475, 393, 915, 257, 709, 544, 2047, 3100, 281, 264, 4338, 51292, 51292, 7285, 13, 51392, 51392, 407, 281, 20928, 11, 562, 291, 362, 819, 4122, 300, 747, 322, 588, 819, 22526, 295, 4190, 11, 51650, 51650], "temperature": 0.0, "avg_logprob": -0.09701473793286956, "compression_ratio": 1.6488888888888888, "no_speech_prob": 2.05801120500837e-06}, {"id": 64, "seek": 35444, "start": 361.24, "end": 367.22, "text": " using this transformed data, then the contours will look more like this, more like circles", "tokens": [50364, 400, 498, 291, 1190, 16235, 23475, 322, 257, 2063, 2445, 7642, 322, 341, 9610, 5573, 2031, 16, 293, 2031, 17, 50704, 50704, 1228, 341, 16894, 1412, 11, 550, 264, 660, 5067, 486, 574, 544, 411, 341, 11, 544, 411, 13040, 51003, 51003, 293, 1570, 6764, 293, 25193, 11, 293, 16235, 23475, 393, 915, 257, 709, 544, 2047, 3100, 281, 264, 4338, 51292, 51292, 7285, 13, 51392, 51392, 407, 281, 20928, 11, 562, 291, 362, 819, 4122, 300, 747, 322, 588, 819, 22526, 295, 4190, 11, 51650, 51650], "temperature": 0.0, "avg_logprob": -0.09701473793286956, "compression_ratio": 1.6488888888888888, "no_speech_prob": 2.05801120500837e-06}, {"id": 65, "seek": 35444, "start": 367.22, "end": 373.0, "text": " and less tall and skinny, and gradient descent can find a much more direct path to the global", "tokens": [50364, 400, 498, 291, 1190, 16235, 23475, 322, 257, 2063, 2445, 7642, 322, 341, 9610, 5573, 2031, 16, 293, 2031, 17, 50704, 50704, 1228, 341, 16894, 1412, 11, 550, 264, 660, 5067, 486, 574, 544, 411, 341, 11, 544, 411, 13040, 51003, 51003, 293, 1570, 6764, 293, 25193, 11, 293, 16235, 23475, 393, 915, 257, 709, 544, 2047, 3100, 281, 264, 4338, 51292, 51292, 7285, 13, 51392, 51392, 407, 281, 20928, 11, 562, 291, 362, 819, 4122, 300, 747, 322, 588, 819, 22526, 295, 4190, 11, 51650, 51650], "temperature": 0.0, "avg_logprob": -0.09701473793286956, "compression_ratio": 1.6488888888888888, "no_speech_prob": 2.05801120500837e-06}, {"id": 66, "seek": 35444, "start": 373.0, "end": 375.0, "text": " minimum.", "tokens": [50364, 400, 498, 291, 1190, 16235, 23475, 322, 257, 2063, 2445, 7642, 322, 341, 9610, 5573, 2031, 16, 293, 2031, 17, 50704, 50704, 1228, 341, 16894, 1412, 11, 550, 264, 660, 5067, 486, 574, 544, 411, 341, 11, 544, 411, 13040, 51003, 51003, 293, 1570, 6764, 293, 25193, 11, 293, 16235, 23475, 393, 915, 257, 709, 544, 2047, 3100, 281, 264, 4338, 51292, 51292, 7285, 13, 51392, 51392, 407, 281, 20928, 11, 562, 291, 362, 819, 4122, 300, 747, 322, 588, 819, 22526, 295, 4190, 11, 51650, 51650], "temperature": 0.0, "avg_logprob": -0.09701473793286956, "compression_ratio": 1.6488888888888888, "no_speech_prob": 2.05801120500837e-06}, {"id": 67, "seek": 35444, "start": 375.0, "end": 380.15999999999997, "text": " So to recap, when you have different features that take on very different ranges of values,", "tokens": [50364, 400, 498, 291, 1190, 16235, 23475, 322, 257, 2063, 2445, 7642, 322, 341, 9610, 5573, 2031, 16, 293, 2031, 17, 50704, 50704, 1228, 341, 16894, 1412, 11, 550, 264, 660, 5067, 486, 574, 544, 411, 341, 11, 544, 411, 13040, 51003, 51003, 293, 1570, 6764, 293, 25193, 11, 293, 16235, 23475, 393, 915, 257, 709, 544, 2047, 3100, 281, 264, 4338, 51292, 51292, 7285, 13, 51392, 51392, 407, 281, 20928, 11, 562, 291, 362, 819, 4122, 300, 747, 322, 588, 819, 22526, 295, 4190, 11, 51650, 51650], "temperature": 0.0, "avg_logprob": -0.09701473793286956, "compression_ratio": 1.6488888888888888, "no_speech_prob": 2.05801120500837e-06}, {"id": 68, "seek": 38016, "start": 380.16, "end": 385.08000000000004, "text": " you can cause gradient descent to run slowly, but rescaling the different features so they", "tokens": [50364, 291, 393, 3082, 16235, 23475, 281, 1190, 5692, 11, 457, 9610, 4270, 264, 819, 4122, 370, 436, 50610, 50610, 439, 747, 322, 257, 25323, 3613, 295, 4190, 393, 3073, 493, 16235, 23475, 10591, 13, 50898, 50898, 1012, 360, 291, 767, 360, 341, 30, 50970, 50970, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1321264108022054, "compression_ratio": 1.4792899408284024, "no_speech_prob": 1.2376975064398721e-05}, {"id": 69, "seek": 38016, "start": 385.08000000000004, "end": 390.84000000000003, "text": " all take on a comparable range of values can speed up gradient descent significantly.", "tokens": [50364, 291, 393, 3082, 16235, 23475, 281, 1190, 5692, 11, 457, 9610, 4270, 264, 819, 4122, 370, 436, 50610, 50610, 439, 747, 322, 257, 25323, 3613, 295, 4190, 393, 3073, 493, 16235, 23475, 10591, 13, 50898, 50898, 1012, 360, 291, 767, 360, 341, 30, 50970, 50970, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1321264108022054, "compression_ratio": 1.4792899408284024, "no_speech_prob": 1.2376975064398721e-05}, {"id": 70, "seek": 38016, "start": 390.84000000000003, "end": 392.28000000000003, "text": " How do you actually do this?", "tokens": [50364, 291, 393, 3082, 16235, 23475, 281, 1190, 5692, 11, 457, 9610, 4270, 264, 819, 4122, 370, 436, 50610, 50610, 439, 747, 322, 257, 25323, 3613, 295, 4190, 393, 3073, 493, 16235, 23475, 10591, 13, 50898, 50898, 1012, 360, 291, 767, 360, 341, 30, 50970, 50970, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1321264108022054, "compression_ratio": 1.4792899408284024, "no_speech_prob": 1.2376975064398721e-05}, {"id": 71, "seek": 39228, "start": 392.28, "end": 410.67999999999995, "text": " Let's take a look at that in the next video.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51284], "temperature": 0.0, "avg_logprob": -0.36827659606933594, "compression_ratio": 0.8979591836734694, "no_speech_prob": 0.00033309988793917}], "language": "en", "video_id": "AbtWSXHPfS0", "entity": "ML Specialization, Andrew Ng (2022)"}}