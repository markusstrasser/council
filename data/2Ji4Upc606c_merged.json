{"video_id": "2Ji4Upc606c", "title": "6.5 Bias and variance | Regularization and bias/variance -[Machine Learning | Andrew Ng]", "description": "Second Course:\nAdvanced Learning Algorithms.\n\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 636, "views": 91, "publish_date": "11/04/2022", "timestamp": 1661817600, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " You saw in the last video how different choices of the degree of polynomial d affects the bias and variance of your learning algorithm and therefore its overall performance. In this video, let's take a look at how regularization, specifically the choice of the regularization parameter lambda, affects the bias and variance and therefore the overall performance of the algorithm. This, it turns out, will be helpful for when you want to choose a good value of lambda of the regularization parameter for your algorithm. Let's take a look. In this example, I'm going to use a fourth order polynomial, but we're going to fit this model using regularization, where here the value of lambda is the regularization parameter that controls how much you trade off keeping the parameters w small versus fitting the training data well. Let's start with the example of setting lambda to be a very large value. Say lambda is equal to 10,000. If you were to do so, you would end up fitting a model that looks roughly like this. Because if lambda were very, very large, then the algorithm is highly motivated to keep these parameters w very small, and so you end up with w1, w2, really all of these parameters would be very close to zero. The model ends up being f of x is just approximately b, a constant value, which is why you end up with a model like this. This model clearly has high bias and it underfits the training data because it doesn't even do well on the training set, and J train is large. Let's take a look at the other extreme. Let's say you set lambda to be a very small value. So with a small value of lambda, in fact, let's go to extreme of setting lambda equals zero. With that choice of lambda, there is no regularization, and so we're just fitting a four-folder polynomial with no regularization, and you end up with that curve that you saw previously that overfits the data. What we saw previously was when you have a model like this, J train is small, but JCV is much larger than J train. JCV is large, and so this indicates we have high variance and it overfits this data. It would be if you have some intermediate value of lambda, not really large, 10,000, but not so small as zero, that hopefully you get a model that looks like this, that is just right and fits the data well with small J train and small JCV. If you are trying to decide what is a good value of lambda to use for the regularization parameter, cross-validation gives you a way to do so as well. Let's take a look at how we could do so, and just as a reminder, the problem we're addressing is if you're fitting a four-folder polynomial, so that's the model, and you're using regularization, how can you choose a good value of lambda? This would be a procedure similar to what you had seen for choosing the degree of polynomial D using cross-validation. Specifically, let's say we try to fit a model using lambda equals zero, and so we would minimize the cost function using lambda equals zero and end up with some parameters w1, b1, and you can then compute the cross-validation error, JCV of w1, b1, and now let's try a different value of lambda. Let's say you try lambda equals 0.01, then again, minimizing the cost function gives you a second set of parameters, w2, b2, and you can also see how well that does on the cross-validation set and so on. Let's keep trying other values of lambda, and in this example, I'm going to try doubling it to lambda equals 0.02, and so that would give you JCV of w3, b3, and so on. Let's double the gain and double the gain. After doubling this number of times, you end up with lambda approximately equal to 10, and that would give you parameters w12, b12, and JCV w12, b12, and by trying out the large range of possible values for lambda, fitting parameters using those different regularization parameters and then evaluating the performance on the cross-validation set, you can then try to pick what is the best value for the regularization parameter. Concretely, if in this example, you find that JCV of w5, b5, has the lowest value of all of these different cross-validation errors, you might then decide to pick this value for lambda and so use w5, b5 as the chosen parameters. And finally, if you want to report out an estimate of the generalization error, you would then report out the test set error J test of w5, b5. To further hone intuition about what this algorithm is doing, let's take a look at how training error and cross-validation error vary as a function of the parameter lambda. So in this figure, I've changed the x-axis again. Notice that the x-axis here is annotated with the value of the regularization parameter lambda. And if we look at the extreme of lambda equals zero, here on the left, that corresponds to not using any regularization and so that's where we want to end up with this very wiggly curve if lambda was small or was even zero. And in that case, we have a high variance model and so J train is going to be small and JCV is going to be large because it does great on the training data but does much worse on the cross-validation data. This extreme on the right with very large values of lambda, say lambda equals 10,000, ends up with fitting a model that looks like that. So this has high bias, it underfits the data and it turns out J train will be high and JCV will be high as well. And in fact, if you were to look at how J train varies as a function of lambda, you find that J train will go up like this because in the optimization cost function, the larger lambda is, the more the algorithm is trying to keep w squared small, that is the more weight is given to this regularization term and thus the less attention is paid to actually doing well on the training set. Right, this term on the left is J train, so the more it's trying to keep the parameters small, the less good a job it does on minimizing the training error. So that's why as lambda increases, the training error J train will tend to increase like so. Now how about the cross-validation error? Turns out the cross-validation error will look like this because we've seen that if lambda is too small or too large, then it doesn't do well on the cross-validation set. It either overfits here on the left or underfits here on the right and there'll be some intermediate value of lambda that causes the algorithm to perform best. And what cross-validation is doing is trying out a lot of different values of lambda. This is what we saw on the last slide, try out lambda equals 0, lambda equals 0.01, lambda equals 0.02, try out a lot of different values of lambda and evaluate the cross-validation error at a lot of these different points and then hopefully pick a value that has low cross-validation error and this will hopefully correspond to a good model for your application. If you compare this diagram to the one that we had in the previous video where the horizontal axis was the degree of polynomial, these two diagrams look a little bit, not mathematically and not in any formal way, but they look a little bit like mirror images of each other. And that's because when you're fitting a degree of polynomial, the left part of this curve corresponded to overfitting and high bias, the right part corresponded to underfitting and high variance, whereas in this one, high variance was on the left and high bias was on the right. But that's why these two images are a little bit like mirror images of each other. But in both cases, cross-validation, evaluating different values can help you choose a good value of t or a good value of lambda. So that's how the choice of regularization parameter lambda affects the bias and variance and overall performance of your algorithm. And you've also seen how you can use cross-validation to make a good choice for the regularization parameter lambda. Now so far, we've talked about how having a high training set error, high J train is indicative of high bias and how having a high cross-validation error, JCV, specifically if it's much higher than J train, how that's indicative of a variance problem. But what do these words high or much higher actually mean? Let's take a look at that in the next video where we'll look at how you can look at the numbers J train and JCV and judge if it's high or low. And it turns out that one further refinement of these ideas, that is establishing a baseline level of performance for your learning algorithm, will make it much easier for you to look at these numbers J train, JCV and judge if they're high or low. Let's take a look at what all this means in the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.44, "text": " You saw in the last video how different choices of the degree of polynomial d affects the", "tokens": [50364, 509, 1866, 294, 264, 1036, 960, 577, 819, 7994, 295, 264, 4314, 295, 26110, 274, 11807, 264, 50786, 50786, 12577, 293, 21977, 295, 428, 2539, 9284, 293, 4412, 1080, 4787, 3389, 13, 51032, 51032, 682, 341, 960, 11, 718, 311, 747, 257, 574, 412, 577, 3890, 2144, 11, 4682, 264, 3922, 295, 264, 3890, 2144, 51306, 51306, 13075, 13607, 11, 11807, 264, 12577, 293, 21977, 293, 4412, 264, 4787, 3389, 295, 264, 51562, 51562, 9284, 13, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.1358945608139038, "compression_ratio": 1.8487804878048781, "no_speech_prob": 0.025162959471344948}, {"id": 1, "seek": 0, "start": 8.44, "end": 13.36, "text": " bias and variance of your learning algorithm and therefore its overall performance.", "tokens": [50364, 509, 1866, 294, 264, 1036, 960, 577, 819, 7994, 295, 264, 4314, 295, 26110, 274, 11807, 264, 50786, 50786, 12577, 293, 21977, 295, 428, 2539, 9284, 293, 4412, 1080, 4787, 3389, 13, 51032, 51032, 682, 341, 960, 11, 718, 311, 747, 257, 574, 412, 577, 3890, 2144, 11, 4682, 264, 3922, 295, 264, 3890, 2144, 51306, 51306, 13075, 13607, 11, 11807, 264, 12577, 293, 21977, 293, 4412, 264, 4787, 3389, 295, 264, 51562, 51562, 9284, 13, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.1358945608139038, "compression_ratio": 1.8487804878048781, "no_speech_prob": 0.025162959471344948}, {"id": 2, "seek": 0, "start": 13.36, "end": 18.84, "text": " In this video, let's take a look at how regularization, specifically the choice of the regularization", "tokens": [50364, 509, 1866, 294, 264, 1036, 960, 577, 819, 7994, 295, 264, 4314, 295, 26110, 274, 11807, 264, 50786, 50786, 12577, 293, 21977, 295, 428, 2539, 9284, 293, 4412, 1080, 4787, 3389, 13, 51032, 51032, 682, 341, 960, 11, 718, 311, 747, 257, 574, 412, 577, 3890, 2144, 11, 4682, 264, 3922, 295, 264, 3890, 2144, 51306, 51306, 13075, 13607, 11, 11807, 264, 12577, 293, 21977, 293, 4412, 264, 4787, 3389, 295, 264, 51562, 51562, 9284, 13, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.1358945608139038, "compression_ratio": 1.8487804878048781, "no_speech_prob": 0.025162959471344948}, {"id": 3, "seek": 0, "start": 18.84, "end": 23.96, "text": " parameter lambda, affects the bias and variance and therefore the overall performance of the", "tokens": [50364, 509, 1866, 294, 264, 1036, 960, 577, 819, 7994, 295, 264, 4314, 295, 26110, 274, 11807, 264, 50786, 50786, 12577, 293, 21977, 295, 428, 2539, 9284, 293, 4412, 1080, 4787, 3389, 13, 51032, 51032, 682, 341, 960, 11, 718, 311, 747, 257, 574, 412, 577, 3890, 2144, 11, 4682, 264, 3922, 295, 264, 3890, 2144, 51306, 51306, 13075, 13607, 11, 11807, 264, 12577, 293, 21977, 293, 4412, 264, 4787, 3389, 295, 264, 51562, 51562, 9284, 13, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.1358945608139038, "compression_ratio": 1.8487804878048781, "no_speech_prob": 0.025162959471344948}, {"id": 4, "seek": 0, "start": 23.96, "end": 24.96, "text": " algorithm.", "tokens": [50364, 509, 1866, 294, 264, 1036, 960, 577, 819, 7994, 295, 264, 4314, 295, 26110, 274, 11807, 264, 50786, 50786, 12577, 293, 21977, 295, 428, 2539, 9284, 293, 4412, 1080, 4787, 3389, 13, 51032, 51032, 682, 341, 960, 11, 718, 311, 747, 257, 574, 412, 577, 3890, 2144, 11, 4682, 264, 3922, 295, 264, 3890, 2144, 51306, 51306, 13075, 13607, 11, 11807, 264, 12577, 293, 21977, 293, 4412, 264, 4787, 3389, 295, 264, 51562, 51562, 9284, 13, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.1358945608139038, "compression_ratio": 1.8487804878048781, "no_speech_prob": 0.025162959471344948}, {"id": 5, "seek": 2496, "start": 24.96, "end": 29.96, "text": " This, it turns out, will be helpful for when you want to choose a good value of lambda", "tokens": [50364, 639, 11, 309, 4523, 484, 11, 486, 312, 4961, 337, 562, 291, 528, 281, 2826, 257, 665, 2158, 295, 13607, 50614, 50614, 295, 264, 3890, 2144, 13075, 337, 428, 9284, 13, 50770, 50770, 961, 311, 747, 257, 574, 13, 50838, 50838, 682, 341, 1365, 11, 286, 478, 516, 281, 764, 257, 6409, 1668, 26110, 11, 457, 321, 434, 516, 281, 3318, 341, 51104, 51104, 2316, 1228, 3890, 2144, 11, 689, 510, 264, 2158, 295, 13607, 307, 264, 3890, 2144, 13075, 51520, 51520, 300, 9003, 577, 709, 291, 4923, 766, 5145, 264, 9834, 261, 1359, 5717, 15669, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11488310181268371, "compression_ratio": 1.7448559670781894, "no_speech_prob": 5.064197102910839e-05}, {"id": 6, "seek": 2496, "start": 29.96, "end": 33.08, "text": " of the regularization parameter for your algorithm.", "tokens": [50364, 639, 11, 309, 4523, 484, 11, 486, 312, 4961, 337, 562, 291, 528, 281, 2826, 257, 665, 2158, 295, 13607, 50614, 50614, 295, 264, 3890, 2144, 13075, 337, 428, 9284, 13, 50770, 50770, 961, 311, 747, 257, 574, 13, 50838, 50838, 682, 341, 1365, 11, 286, 478, 516, 281, 764, 257, 6409, 1668, 26110, 11, 457, 321, 434, 516, 281, 3318, 341, 51104, 51104, 2316, 1228, 3890, 2144, 11, 689, 510, 264, 2158, 295, 13607, 307, 264, 3890, 2144, 13075, 51520, 51520, 300, 9003, 577, 709, 291, 4923, 766, 5145, 264, 9834, 261, 1359, 5717, 15669, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11488310181268371, "compression_ratio": 1.7448559670781894, "no_speech_prob": 5.064197102910839e-05}, {"id": 7, "seek": 2496, "start": 33.08, "end": 34.44, "text": " Let's take a look.", "tokens": [50364, 639, 11, 309, 4523, 484, 11, 486, 312, 4961, 337, 562, 291, 528, 281, 2826, 257, 665, 2158, 295, 13607, 50614, 50614, 295, 264, 3890, 2144, 13075, 337, 428, 9284, 13, 50770, 50770, 961, 311, 747, 257, 574, 13, 50838, 50838, 682, 341, 1365, 11, 286, 478, 516, 281, 764, 257, 6409, 1668, 26110, 11, 457, 321, 434, 516, 281, 3318, 341, 51104, 51104, 2316, 1228, 3890, 2144, 11, 689, 510, 264, 2158, 295, 13607, 307, 264, 3890, 2144, 13075, 51520, 51520, 300, 9003, 577, 709, 291, 4923, 766, 5145, 264, 9834, 261, 1359, 5717, 15669, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11488310181268371, "compression_ratio": 1.7448559670781894, "no_speech_prob": 5.064197102910839e-05}, {"id": 8, "seek": 2496, "start": 34.44, "end": 39.760000000000005, "text": " In this example, I'm going to use a fourth order polynomial, but we're going to fit this", "tokens": [50364, 639, 11, 309, 4523, 484, 11, 486, 312, 4961, 337, 562, 291, 528, 281, 2826, 257, 665, 2158, 295, 13607, 50614, 50614, 295, 264, 3890, 2144, 13075, 337, 428, 9284, 13, 50770, 50770, 961, 311, 747, 257, 574, 13, 50838, 50838, 682, 341, 1365, 11, 286, 478, 516, 281, 764, 257, 6409, 1668, 26110, 11, 457, 321, 434, 516, 281, 3318, 341, 51104, 51104, 2316, 1228, 3890, 2144, 11, 689, 510, 264, 2158, 295, 13607, 307, 264, 3890, 2144, 13075, 51520, 51520, 300, 9003, 577, 709, 291, 4923, 766, 5145, 264, 9834, 261, 1359, 5717, 15669, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11488310181268371, "compression_ratio": 1.7448559670781894, "no_speech_prob": 5.064197102910839e-05}, {"id": 9, "seek": 2496, "start": 39.760000000000005, "end": 48.08, "text": " model using regularization, where here the value of lambda is the regularization parameter", "tokens": [50364, 639, 11, 309, 4523, 484, 11, 486, 312, 4961, 337, 562, 291, 528, 281, 2826, 257, 665, 2158, 295, 13607, 50614, 50614, 295, 264, 3890, 2144, 13075, 337, 428, 9284, 13, 50770, 50770, 961, 311, 747, 257, 574, 13, 50838, 50838, 682, 341, 1365, 11, 286, 478, 516, 281, 764, 257, 6409, 1668, 26110, 11, 457, 321, 434, 516, 281, 3318, 341, 51104, 51104, 2316, 1228, 3890, 2144, 11, 689, 510, 264, 2158, 295, 13607, 307, 264, 3890, 2144, 13075, 51520, 51520, 300, 9003, 577, 709, 291, 4923, 766, 5145, 264, 9834, 261, 1359, 5717, 15669, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11488310181268371, "compression_ratio": 1.7448559670781894, "no_speech_prob": 5.064197102910839e-05}, {"id": 10, "seek": 2496, "start": 48.08, "end": 54.84, "text": " that controls how much you trade off keeping the parameters w small versus fitting the", "tokens": [50364, 639, 11, 309, 4523, 484, 11, 486, 312, 4961, 337, 562, 291, 528, 281, 2826, 257, 665, 2158, 295, 13607, 50614, 50614, 295, 264, 3890, 2144, 13075, 337, 428, 9284, 13, 50770, 50770, 961, 311, 747, 257, 574, 13, 50838, 50838, 682, 341, 1365, 11, 286, 478, 516, 281, 764, 257, 6409, 1668, 26110, 11, 457, 321, 434, 516, 281, 3318, 341, 51104, 51104, 2316, 1228, 3890, 2144, 11, 689, 510, 264, 2158, 295, 13607, 307, 264, 3890, 2144, 13075, 51520, 51520, 300, 9003, 577, 709, 291, 4923, 766, 5145, 264, 9834, 261, 1359, 5717, 15669, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11488310181268371, "compression_ratio": 1.7448559670781894, "no_speech_prob": 5.064197102910839e-05}, {"id": 11, "seek": 5484, "start": 54.84, "end": 57.28, "text": " training data well.", "tokens": [50364, 3097, 1412, 731, 13, 50486, 50486, 961, 311, 722, 365, 264, 1365, 295, 3287, 13607, 281, 312, 257, 588, 2416, 2158, 13, 50794, 50794, 6463, 13607, 307, 2681, 281, 1266, 11, 1360, 13, 50994, 50994, 759, 291, 645, 281, 360, 370, 11, 291, 576, 917, 493, 15669, 257, 2316, 300, 1542, 9810, 411, 341, 13, 51414, 51414, 1436, 498, 13607, 645, 588, 11, 588, 2416, 11, 550, 264, 9284, 307, 5405, 14515, 281, 1066, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.11413328464214618, "compression_ratio": 1.50253807106599, "no_speech_prob": 3.219078280380927e-05}, {"id": 12, "seek": 5484, "start": 57.28, "end": 63.440000000000005, "text": " Let's start with the example of setting lambda to be a very large value.", "tokens": [50364, 3097, 1412, 731, 13, 50486, 50486, 961, 311, 722, 365, 264, 1365, 295, 3287, 13607, 281, 312, 257, 588, 2416, 2158, 13, 50794, 50794, 6463, 13607, 307, 2681, 281, 1266, 11, 1360, 13, 50994, 50994, 759, 291, 645, 281, 360, 370, 11, 291, 576, 917, 493, 15669, 257, 2316, 300, 1542, 9810, 411, 341, 13, 51414, 51414, 1436, 498, 13607, 645, 588, 11, 588, 2416, 11, 550, 264, 9284, 307, 5405, 14515, 281, 1066, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.11413328464214618, "compression_ratio": 1.50253807106599, "no_speech_prob": 3.219078280380927e-05}, {"id": 13, "seek": 5484, "start": 63.440000000000005, "end": 67.44, "text": " Say lambda is equal to 10,000.", "tokens": [50364, 3097, 1412, 731, 13, 50486, 50486, 961, 311, 722, 365, 264, 1365, 295, 3287, 13607, 281, 312, 257, 588, 2416, 2158, 13, 50794, 50794, 6463, 13607, 307, 2681, 281, 1266, 11, 1360, 13, 50994, 50994, 759, 291, 645, 281, 360, 370, 11, 291, 576, 917, 493, 15669, 257, 2316, 300, 1542, 9810, 411, 341, 13, 51414, 51414, 1436, 498, 13607, 645, 588, 11, 588, 2416, 11, 550, 264, 9284, 307, 5405, 14515, 281, 1066, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.11413328464214618, "compression_ratio": 1.50253807106599, "no_speech_prob": 3.219078280380927e-05}, {"id": 14, "seek": 5484, "start": 67.44, "end": 75.84, "text": " If you were to do so, you would end up fitting a model that looks roughly like this.", "tokens": [50364, 3097, 1412, 731, 13, 50486, 50486, 961, 311, 722, 365, 264, 1365, 295, 3287, 13607, 281, 312, 257, 588, 2416, 2158, 13, 50794, 50794, 6463, 13607, 307, 2681, 281, 1266, 11, 1360, 13, 50994, 50994, 759, 291, 645, 281, 360, 370, 11, 291, 576, 917, 493, 15669, 257, 2316, 300, 1542, 9810, 411, 341, 13, 51414, 51414, 1436, 498, 13607, 645, 588, 11, 588, 2416, 11, 550, 264, 9284, 307, 5405, 14515, 281, 1066, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.11413328464214618, "compression_ratio": 1.50253807106599, "no_speech_prob": 3.219078280380927e-05}, {"id": 15, "seek": 5484, "start": 75.84, "end": 81.4, "text": " Because if lambda were very, very large, then the algorithm is highly motivated to keep", "tokens": [50364, 3097, 1412, 731, 13, 50486, 50486, 961, 311, 722, 365, 264, 1365, 295, 3287, 13607, 281, 312, 257, 588, 2416, 2158, 13, 50794, 50794, 6463, 13607, 307, 2681, 281, 1266, 11, 1360, 13, 50994, 50994, 759, 291, 645, 281, 360, 370, 11, 291, 576, 917, 493, 15669, 257, 2316, 300, 1542, 9810, 411, 341, 13, 51414, 51414, 1436, 498, 13607, 645, 588, 11, 588, 2416, 11, 550, 264, 9284, 307, 5405, 14515, 281, 1066, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.11413328464214618, "compression_ratio": 1.50253807106599, "no_speech_prob": 3.219078280380927e-05}, {"id": 16, "seek": 8140, "start": 81.4, "end": 88.88000000000001, "text": " these parameters w very small, and so you end up with w1, w2, really all of these parameters", "tokens": [50364, 613, 9834, 261, 588, 1359, 11, 293, 370, 291, 917, 493, 365, 261, 16, 11, 261, 17, 11, 534, 439, 295, 613, 9834, 50738, 50738, 576, 312, 588, 1998, 281, 4018, 13, 50942, 50942, 440, 2316, 5314, 493, 885, 283, 295, 2031, 307, 445, 10447, 272, 11, 257, 5754, 2158, 11, 597, 307, 983, 291, 917, 51248, 51248, 493, 365, 257, 2316, 411, 341, 13, 51416, 51416, 639, 2316, 4448, 575, 1090, 12577, 293, 309, 833, 13979, 264, 3097, 1412, 570, 309, 1177, 380, 754, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.13293902793627108, "compression_ratio": 1.6028708133971292, "no_speech_prob": 2.9023019578744425e-06}, {"id": 17, "seek": 8140, "start": 88.88000000000001, "end": 92.96000000000001, "text": " would be very close to zero.", "tokens": [50364, 613, 9834, 261, 588, 1359, 11, 293, 370, 291, 917, 493, 365, 261, 16, 11, 261, 17, 11, 534, 439, 295, 613, 9834, 50738, 50738, 576, 312, 588, 1998, 281, 4018, 13, 50942, 50942, 440, 2316, 5314, 493, 885, 283, 295, 2031, 307, 445, 10447, 272, 11, 257, 5754, 2158, 11, 597, 307, 983, 291, 917, 51248, 51248, 493, 365, 257, 2316, 411, 341, 13, 51416, 51416, 639, 2316, 4448, 575, 1090, 12577, 293, 309, 833, 13979, 264, 3097, 1412, 570, 309, 1177, 380, 754, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.13293902793627108, "compression_ratio": 1.6028708133971292, "no_speech_prob": 2.9023019578744425e-06}, {"id": 18, "seek": 8140, "start": 92.96000000000001, "end": 99.08000000000001, "text": " The model ends up being f of x is just approximately b, a constant value, which is why you end", "tokens": [50364, 613, 9834, 261, 588, 1359, 11, 293, 370, 291, 917, 493, 365, 261, 16, 11, 261, 17, 11, 534, 439, 295, 613, 9834, 50738, 50738, 576, 312, 588, 1998, 281, 4018, 13, 50942, 50942, 440, 2316, 5314, 493, 885, 283, 295, 2031, 307, 445, 10447, 272, 11, 257, 5754, 2158, 11, 597, 307, 983, 291, 917, 51248, 51248, 493, 365, 257, 2316, 411, 341, 13, 51416, 51416, 639, 2316, 4448, 575, 1090, 12577, 293, 309, 833, 13979, 264, 3097, 1412, 570, 309, 1177, 380, 754, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.13293902793627108, "compression_ratio": 1.6028708133971292, "no_speech_prob": 2.9023019578744425e-06}, {"id": 19, "seek": 8140, "start": 99.08000000000001, "end": 102.44, "text": " up with a model like this.", "tokens": [50364, 613, 9834, 261, 588, 1359, 11, 293, 370, 291, 917, 493, 365, 261, 16, 11, 261, 17, 11, 534, 439, 295, 613, 9834, 50738, 50738, 576, 312, 588, 1998, 281, 4018, 13, 50942, 50942, 440, 2316, 5314, 493, 885, 283, 295, 2031, 307, 445, 10447, 272, 11, 257, 5754, 2158, 11, 597, 307, 983, 291, 917, 51248, 51248, 493, 365, 257, 2316, 411, 341, 13, 51416, 51416, 639, 2316, 4448, 575, 1090, 12577, 293, 309, 833, 13979, 264, 3097, 1412, 570, 309, 1177, 380, 754, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.13293902793627108, "compression_ratio": 1.6028708133971292, "no_speech_prob": 2.9023019578744425e-06}, {"id": 20, "seek": 8140, "start": 102.44, "end": 109.96000000000001, "text": " This model clearly has high bias and it underfits the training data because it doesn't even", "tokens": [50364, 613, 9834, 261, 588, 1359, 11, 293, 370, 291, 917, 493, 365, 261, 16, 11, 261, 17, 11, 534, 439, 295, 613, 9834, 50738, 50738, 576, 312, 588, 1998, 281, 4018, 13, 50942, 50942, 440, 2316, 5314, 493, 885, 283, 295, 2031, 307, 445, 10447, 272, 11, 257, 5754, 2158, 11, 597, 307, 983, 291, 917, 51248, 51248, 493, 365, 257, 2316, 411, 341, 13, 51416, 51416, 639, 2316, 4448, 575, 1090, 12577, 293, 309, 833, 13979, 264, 3097, 1412, 570, 309, 1177, 380, 754, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.13293902793627108, "compression_ratio": 1.6028708133971292, "no_speech_prob": 2.9023019578744425e-06}, {"id": 21, "seek": 10996, "start": 109.96, "end": 117.11999999999999, "text": " do well on the training set, and J train is large.", "tokens": [50364, 360, 731, 322, 264, 3097, 992, 11, 293, 508, 3847, 307, 2416, 13, 50722, 50722, 961, 311, 747, 257, 574, 412, 264, 661, 8084, 13, 50944, 50944, 961, 311, 584, 291, 992, 13607, 281, 312, 257, 588, 1359, 2158, 13, 51266, 51266, 407, 365, 257, 1359, 2158, 295, 13607, 11, 294, 1186, 11, 718, 311, 352, 281, 8084, 295, 3287, 13607, 6915, 51544, 51544, 4018, 13, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.13384577887398855, "compression_ratio": 1.5294117647058822, "no_speech_prob": 6.962195129744941e-06}, {"id": 22, "seek": 10996, "start": 117.11999999999999, "end": 121.55999999999999, "text": " Let's take a look at the other extreme.", "tokens": [50364, 360, 731, 322, 264, 3097, 992, 11, 293, 508, 3847, 307, 2416, 13, 50722, 50722, 961, 311, 747, 257, 574, 412, 264, 661, 8084, 13, 50944, 50944, 961, 311, 584, 291, 992, 13607, 281, 312, 257, 588, 1359, 2158, 13, 51266, 51266, 407, 365, 257, 1359, 2158, 295, 13607, 11, 294, 1186, 11, 718, 311, 352, 281, 8084, 295, 3287, 13607, 6915, 51544, 51544, 4018, 13, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.13384577887398855, "compression_ratio": 1.5294117647058822, "no_speech_prob": 6.962195129744941e-06}, {"id": 23, "seek": 10996, "start": 121.55999999999999, "end": 128.0, "text": " Let's say you set lambda to be a very small value.", "tokens": [50364, 360, 731, 322, 264, 3097, 992, 11, 293, 508, 3847, 307, 2416, 13, 50722, 50722, 961, 311, 747, 257, 574, 412, 264, 661, 8084, 13, 50944, 50944, 961, 311, 584, 291, 992, 13607, 281, 312, 257, 588, 1359, 2158, 13, 51266, 51266, 407, 365, 257, 1359, 2158, 295, 13607, 11, 294, 1186, 11, 718, 311, 352, 281, 8084, 295, 3287, 13607, 6915, 51544, 51544, 4018, 13, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.13384577887398855, "compression_ratio": 1.5294117647058822, "no_speech_prob": 6.962195129744941e-06}, {"id": 24, "seek": 10996, "start": 128.0, "end": 133.56, "text": " So with a small value of lambda, in fact, let's go to extreme of setting lambda equals", "tokens": [50364, 360, 731, 322, 264, 3097, 992, 11, 293, 508, 3847, 307, 2416, 13, 50722, 50722, 961, 311, 747, 257, 574, 412, 264, 661, 8084, 13, 50944, 50944, 961, 311, 584, 291, 992, 13607, 281, 312, 257, 588, 1359, 2158, 13, 51266, 51266, 407, 365, 257, 1359, 2158, 295, 13607, 11, 294, 1186, 11, 718, 311, 352, 281, 8084, 295, 3287, 13607, 6915, 51544, 51544, 4018, 13, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.13384577887398855, "compression_ratio": 1.5294117647058822, "no_speech_prob": 6.962195129744941e-06}, {"id": 25, "seek": 10996, "start": 133.56, "end": 135.28, "text": " zero.", "tokens": [50364, 360, 731, 322, 264, 3097, 992, 11, 293, 508, 3847, 307, 2416, 13, 50722, 50722, 961, 311, 747, 257, 574, 412, 264, 661, 8084, 13, 50944, 50944, 961, 311, 584, 291, 992, 13607, 281, 312, 257, 588, 1359, 2158, 13, 51266, 51266, 407, 365, 257, 1359, 2158, 295, 13607, 11, 294, 1186, 11, 718, 311, 352, 281, 8084, 295, 3287, 13607, 6915, 51544, 51544, 4018, 13, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.13384577887398855, "compression_ratio": 1.5294117647058822, "no_speech_prob": 6.962195129744941e-06}, {"id": 26, "seek": 13528, "start": 135.28, "end": 141.12, "text": " With that choice of lambda, there is no regularization, and so we're just fitting a four-folder polynomial", "tokens": [50364, 2022, 300, 3922, 295, 13607, 11, 456, 307, 572, 3890, 2144, 11, 293, 370, 321, 434, 445, 15669, 257, 1451, 12, 18353, 260, 26110, 50656, 50656, 365, 572, 3890, 2144, 11, 293, 291, 917, 493, 365, 300, 7605, 300, 291, 1866, 8046, 300, 670, 13979, 51032, 51032, 264, 1412, 13, 51156, 51156, 708, 321, 1866, 8046, 390, 562, 291, 362, 257, 2316, 411, 341, 11, 508, 3847, 307, 1359, 11, 457, 49802, 53, 51454, 51454, 307, 709, 4833, 813, 508, 3847, 13, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.16526081395703693, "compression_ratio": 1.62, "no_speech_prob": 9.276268997382431e-07}, {"id": 27, "seek": 13528, "start": 141.12, "end": 148.64, "text": " with no regularization, and you end up with that curve that you saw previously that overfits", "tokens": [50364, 2022, 300, 3922, 295, 13607, 11, 456, 307, 572, 3890, 2144, 11, 293, 370, 321, 434, 445, 15669, 257, 1451, 12, 18353, 260, 26110, 50656, 50656, 365, 572, 3890, 2144, 11, 293, 291, 917, 493, 365, 300, 7605, 300, 291, 1866, 8046, 300, 670, 13979, 51032, 51032, 264, 1412, 13, 51156, 51156, 708, 321, 1866, 8046, 390, 562, 291, 362, 257, 2316, 411, 341, 11, 508, 3847, 307, 1359, 11, 457, 49802, 53, 51454, 51454, 307, 709, 4833, 813, 508, 3847, 13, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.16526081395703693, "compression_ratio": 1.62, "no_speech_prob": 9.276268997382431e-07}, {"id": 28, "seek": 13528, "start": 148.64, "end": 151.12, "text": " the data.", "tokens": [50364, 2022, 300, 3922, 295, 13607, 11, 456, 307, 572, 3890, 2144, 11, 293, 370, 321, 434, 445, 15669, 257, 1451, 12, 18353, 260, 26110, 50656, 50656, 365, 572, 3890, 2144, 11, 293, 291, 917, 493, 365, 300, 7605, 300, 291, 1866, 8046, 300, 670, 13979, 51032, 51032, 264, 1412, 13, 51156, 51156, 708, 321, 1866, 8046, 390, 562, 291, 362, 257, 2316, 411, 341, 11, 508, 3847, 307, 1359, 11, 457, 49802, 53, 51454, 51454, 307, 709, 4833, 813, 508, 3847, 13, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.16526081395703693, "compression_ratio": 1.62, "no_speech_prob": 9.276268997382431e-07}, {"id": 29, "seek": 13528, "start": 151.12, "end": 157.08, "text": " What we saw previously was when you have a model like this, J train is small, but JCV", "tokens": [50364, 2022, 300, 3922, 295, 13607, 11, 456, 307, 572, 3890, 2144, 11, 293, 370, 321, 434, 445, 15669, 257, 1451, 12, 18353, 260, 26110, 50656, 50656, 365, 572, 3890, 2144, 11, 293, 291, 917, 493, 365, 300, 7605, 300, 291, 1866, 8046, 300, 670, 13979, 51032, 51032, 264, 1412, 13, 51156, 51156, 708, 321, 1866, 8046, 390, 562, 291, 362, 257, 2316, 411, 341, 11, 508, 3847, 307, 1359, 11, 457, 49802, 53, 51454, 51454, 307, 709, 4833, 813, 508, 3847, 13, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.16526081395703693, "compression_ratio": 1.62, "no_speech_prob": 9.276268997382431e-07}, {"id": 30, "seek": 13528, "start": 157.08, "end": 159.48, "text": " is much larger than J train.", "tokens": [50364, 2022, 300, 3922, 295, 13607, 11, 456, 307, 572, 3890, 2144, 11, 293, 370, 321, 434, 445, 15669, 257, 1451, 12, 18353, 260, 26110, 50656, 50656, 365, 572, 3890, 2144, 11, 293, 291, 917, 493, 365, 300, 7605, 300, 291, 1866, 8046, 300, 670, 13979, 51032, 51032, 264, 1412, 13, 51156, 51156, 708, 321, 1866, 8046, 390, 562, 291, 362, 257, 2316, 411, 341, 11, 508, 3847, 307, 1359, 11, 457, 49802, 53, 51454, 51454, 307, 709, 4833, 813, 508, 3847, 13, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.16526081395703693, "compression_ratio": 1.62, "no_speech_prob": 9.276268997382431e-07}, {"id": 31, "seek": 15948, "start": 159.48, "end": 167.56, "text": " JCV is large, and so this indicates we have high variance and it overfits this data.", "tokens": [50364, 49802, 53, 307, 2416, 11, 293, 370, 341, 16203, 321, 362, 1090, 21977, 293, 309, 670, 13979, 341, 1412, 13, 50768, 50768, 467, 576, 312, 498, 291, 362, 512, 19376, 2158, 295, 13607, 11, 406, 534, 2416, 11, 1266, 11, 1360, 11, 51064, 51064, 457, 406, 370, 1359, 382, 4018, 11, 300, 4696, 291, 483, 257, 2316, 300, 1542, 411, 341, 11, 300, 307, 51342, 51342, 445, 558, 293, 9001, 264, 1412, 731, 365, 1359, 508, 3847, 293, 1359, 49802, 53, 13, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.1578466171442076, "compression_ratio": 1.5576923076923077, "no_speech_prob": 4.289226581022376e-06}, {"id": 32, "seek": 15948, "start": 167.56, "end": 173.48, "text": " It would be if you have some intermediate value of lambda, not really large, 10,000,", "tokens": [50364, 49802, 53, 307, 2416, 11, 293, 370, 341, 16203, 321, 362, 1090, 21977, 293, 309, 670, 13979, 341, 1412, 13, 50768, 50768, 467, 576, 312, 498, 291, 362, 512, 19376, 2158, 295, 13607, 11, 406, 534, 2416, 11, 1266, 11, 1360, 11, 51064, 51064, 457, 406, 370, 1359, 382, 4018, 11, 300, 4696, 291, 483, 257, 2316, 300, 1542, 411, 341, 11, 300, 307, 51342, 51342, 445, 558, 293, 9001, 264, 1412, 731, 365, 1359, 508, 3847, 293, 1359, 49802, 53, 13, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.1578466171442076, "compression_ratio": 1.5576923076923077, "no_speech_prob": 4.289226581022376e-06}, {"id": 33, "seek": 15948, "start": 173.48, "end": 179.04, "text": " but not so small as zero, that hopefully you get a model that looks like this, that is", "tokens": [50364, 49802, 53, 307, 2416, 11, 293, 370, 341, 16203, 321, 362, 1090, 21977, 293, 309, 670, 13979, 341, 1412, 13, 50768, 50768, 467, 576, 312, 498, 291, 362, 512, 19376, 2158, 295, 13607, 11, 406, 534, 2416, 11, 1266, 11, 1360, 11, 51064, 51064, 457, 406, 370, 1359, 382, 4018, 11, 300, 4696, 291, 483, 257, 2316, 300, 1542, 411, 341, 11, 300, 307, 51342, 51342, 445, 558, 293, 9001, 264, 1412, 731, 365, 1359, 508, 3847, 293, 1359, 49802, 53, 13, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.1578466171442076, "compression_ratio": 1.5576923076923077, "no_speech_prob": 4.289226581022376e-06}, {"id": 34, "seek": 15948, "start": 179.04, "end": 186.51999999999998, "text": " just right and fits the data well with small J train and small JCV.", "tokens": [50364, 49802, 53, 307, 2416, 11, 293, 370, 341, 16203, 321, 362, 1090, 21977, 293, 309, 670, 13979, 341, 1412, 13, 50768, 50768, 467, 576, 312, 498, 291, 362, 512, 19376, 2158, 295, 13607, 11, 406, 534, 2416, 11, 1266, 11, 1360, 11, 51064, 51064, 457, 406, 370, 1359, 382, 4018, 11, 300, 4696, 291, 483, 257, 2316, 300, 1542, 411, 341, 11, 300, 307, 51342, 51342, 445, 558, 293, 9001, 264, 1412, 731, 365, 1359, 508, 3847, 293, 1359, 49802, 53, 13, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.1578466171442076, "compression_ratio": 1.5576923076923077, "no_speech_prob": 4.289226581022376e-06}, {"id": 35, "seek": 18652, "start": 186.52, "end": 192.44, "text": " If you are trying to decide what is a good value of lambda to use for the regularization", "tokens": [50364, 759, 291, 366, 1382, 281, 4536, 437, 307, 257, 665, 2158, 295, 13607, 281, 764, 337, 264, 3890, 2144, 50660, 50660, 13075, 11, 3278, 12, 3337, 327, 399, 2709, 291, 257, 636, 281, 360, 370, 382, 731, 13, 50928, 50928, 961, 311, 747, 257, 574, 412, 577, 321, 727, 360, 370, 11, 293, 445, 382, 257, 13548, 11, 264, 1154, 321, 434, 14329, 51172, 51172, 307, 498, 291, 434, 15669, 257, 1451, 12, 18353, 260, 26110, 11, 370, 300, 311, 264, 2316, 11, 293, 291, 434, 1228, 3890, 2144, 11, 51494, 51494, 577, 393, 291, 2826, 257, 665, 2158, 295, 13607, 30, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.06767403404667692, "compression_ratio": 1.6724137931034482, "no_speech_prob": 2.3320590116782114e-06}, {"id": 36, "seek": 18652, "start": 192.44, "end": 197.8, "text": " parameter, cross-validation gives you a way to do so as well.", "tokens": [50364, 759, 291, 366, 1382, 281, 4536, 437, 307, 257, 665, 2158, 295, 13607, 281, 764, 337, 264, 3890, 2144, 50660, 50660, 13075, 11, 3278, 12, 3337, 327, 399, 2709, 291, 257, 636, 281, 360, 370, 382, 731, 13, 50928, 50928, 961, 311, 747, 257, 574, 412, 577, 321, 727, 360, 370, 11, 293, 445, 382, 257, 13548, 11, 264, 1154, 321, 434, 14329, 51172, 51172, 307, 498, 291, 434, 15669, 257, 1451, 12, 18353, 260, 26110, 11, 370, 300, 311, 264, 2316, 11, 293, 291, 434, 1228, 3890, 2144, 11, 51494, 51494, 577, 393, 291, 2826, 257, 665, 2158, 295, 13607, 30, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.06767403404667692, "compression_ratio": 1.6724137931034482, "no_speech_prob": 2.3320590116782114e-06}, {"id": 37, "seek": 18652, "start": 197.8, "end": 202.68, "text": " Let's take a look at how we could do so, and just as a reminder, the problem we're addressing", "tokens": [50364, 759, 291, 366, 1382, 281, 4536, 437, 307, 257, 665, 2158, 295, 13607, 281, 764, 337, 264, 3890, 2144, 50660, 50660, 13075, 11, 3278, 12, 3337, 327, 399, 2709, 291, 257, 636, 281, 360, 370, 382, 731, 13, 50928, 50928, 961, 311, 747, 257, 574, 412, 577, 321, 727, 360, 370, 11, 293, 445, 382, 257, 13548, 11, 264, 1154, 321, 434, 14329, 51172, 51172, 307, 498, 291, 434, 15669, 257, 1451, 12, 18353, 260, 26110, 11, 370, 300, 311, 264, 2316, 11, 293, 291, 434, 1228, 3890, 2144, 11, 51494, 51494, 577, 393, 291, 2826, 257, 665, 2158, 295, 13607, 30, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.06767403404667692, "compression_ratio": 1.6724137931034482, "no_speech_prob": 2.3320590116782114e-06}, {"id": 38, "seek": 18652, "start": 202.68, "end": 209.12, "text": " is if you're fitting a four-folder polynomial, so that's the model, and you're using regularization,", "tokens": [50364, 759, 291, 366, 1382, 281, 4536, 437, 307, 257, 665, 2158, 295, 13607, 281, 764, 337, 264, 3890, 2144, 50660, 50660, 13075, 11, 3278, 12, 3337, 327, 399, 2709, 291, 257, 636, 281, 360, 370, 382, 731, 13, 50928, 50928, 961, 311, 747, 257, 574, 412, 577, 321, 727, 360, 370, 11, 293, 445, 382, 257, 13548, 11, 264, 1154, 321, 434, 14329, 51172, 51172, 307, 498, 291, 434, 15669, 257, 1451, 12, 18353, 260, 26110, 11, 370, 300, 311, 264, 2316, 11, 293, 291, 434, 1228, 3890, 2144, 11, 51494, 51494, 577, 393, 291, 2826, 257, 665, 2158, 295, 13607, 30, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.06767403404667692, "compression_ratio": 1.6724137931034482, "no_speech_prob": 2.3320590116782114e-06}, {"id": 39, "seek": 18652, "start": 209.12, "end": 212.36, "text": " how can you choose a good value of lambda?", "tokens": [50364, 759, 291, 366, 1382, 281, 4536, 437, 307, 257, 665, 2158, 295, 13607, 281, 764, 337, 264, 3890, 2144, 50660, 50660, 13075, 11, 3278, 12, 3337, 327, 399, 2709, 291, 257, 636, 281, 360, 370, 382, 731, 13, 50928, 50928, 961, 311, 747, 257, 574, 412, 577, 321, 727, 360, 370, 11, 293, 445, 382, 257, 13548, 11, 264, 1154, 321, 434, 14329, 51172, 51172, 307, 498, 291, 434, 15669, 257, 1451, 12, 18353, 260, 26110, 11, 370, 300, 311, 264, 2316, 11, 293, 291, 434, 1228, 3890, 2144, 11, 51494, 51494, 577, 393, 291, 2826, 257, 665, 2158, 295, 13607, 30, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.06767403404667692, "compression_ratio": 1.6724137931034482, "no_speech_prob": 2.3320590116782114e-06}, {"id": 40, "seek": 21236, "start": 212.36, "end": 217.12, "text": " This would be a procedure similar to what you had seen for choosing the degree of polynomial", "tokens": [50364, 639, 576, 312, 257, 10747, 2531, 281, 437, 291, 632, 1612, 337, 10875, 264, 4314, 295, 26110, 50602, 50602, 413, 1228, 3278, 12, 3337, 327, 399, 13, 50716, 50716, 26058, 11, 718, 311, 584, 321, 853, 281, 3318, 257, 2316, 1228, 13607, 6915, 4018, 11, 293, 370, 321, 576, 51098, 51098, 17522, 264, 2063, 2445, 1228, 13607, 6915, 4018, 293, 917, 493, 365, 512, 9834, 261, 16, 11, 272, 16, 11, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.17077608744303385, "compression_ratio": 1.5204081632653061, "no_speech_prob": 5.507507921720389e-06}, {"id": 41, "seek": 21236, "start": 217.12, "end": 219.4, "text": " D using cross-validation.", "tokens": [50364, 639, 576, 312, 257, 10747, 2531, 281, 437, 291, 632, 1612, 337, 10875, 264, 4314, 295, 26110, 50602, 50602, 413, 1228, 3278, 12, 3337, 327, 399, 13, 50716, 50716, 26058, 11, 718, 311, 584, 321, 853, 281, 3318, 257, 2316, 1228, 13607, 6915, 4018, 11, 293, 370, 321, 576, 51098, 51098, 17522, 264, 2063, 2445, 1228, 13607, 6915, 4018, 293, 917, 493, 365, 512, 9834, 261, 16, 11, 272, 16, 11, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.17077608744303385, "compression_ratio": 1.5204081632653061, "no_speech_prob": 5.507507921720389e-06}, {"id": 42, "seek": 21236, "start": 219.4, "end": 227.04000000000002, "text": " Specifically, let's say we try to fit a model using lambda equals zero, and so we would", "tokens": [50364, 639, 576, 312, 257, 10747, 2531, 281, 437, 291, 632, 1612, 337, 10875, 264, 4314, 295, 26110, 50602, 50602, 413, 1228, 3278, 12, 3337, 327, 399, 13, 50716, 50716, 26058, 11, 718, 311, 584, 321, 853, 281, 3318, 257, 2316, 1228, 13607, 6915, 4018, 11, 293, 370, 321, 576, 51098, 51098, 17522, 264, 2063, 2445, 1228, 13607, 6915, 4018, 293, 917, 493, 365, 512, 9834, 261, 16, 11, 272, 16, 11, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.17077608744303385, "compression_ratio": 1.5204081632653061, "no_speech_prob": 5.507507921720389e-06}, {"id": 43, "seek": 21236, "start": 227.04000000000002, "end": 238.52, "text": " minimize the cost function using lambda equals zero and end up with some parameters w1, b1,", "tokens": [50364, 639, 576, 312, 257, 10747, 2531, 281, 437, 291, 632, 1612, 337, 10875, 264, 4314, 295, 26110, 50602, 50602, 413, 1228, 3278, 12, 3337, 327, 399, 13, 50716, 50716, 26058, 11, 718, 311, 584, 321, 853, 281, 3318, 257, 2316, 1228, 13607, 6915, 4018, 11, 293, 370, 321, 576, 51098, 51098, 17522, 264, 2063, 2445, 1228, 13607, 6915, 4018, 293, 917, 493, 365, 512, 9834, 261, 16, 11, 272, 16, 11, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.17077608744303385, "compression_ratio": 1.5204081632653061, "no_speech_prob": 5.507507921720389e-06}, {"id": 44, "seek": 23852, "start": 238.52, "end": 245.8, "text": " and you can then compute the cross-validation error, JCV of w1, b1, and now let's try a", "tokens": [50364, 293, 291, 393, 550, 14722, 264, 3278, 12, 3337, 327, 399, 6713, 11, 49802, 53, 295, 261, 16, 11, 272, 16, 11, 293, 586, 718, 311, 853, 257, 50728, 50728, 819, 2158, 295, 13607, 13, 50788, 50788, 961, 311, 584, 291, 853, 13607, 6915, 1958, 13, 10607, 11, 550, 797, 11, 46608, 264, 2063, 2445, 2709, 51075, 51075, 291, 257, 1150, 992, 295, 9834, 11, 261, 17, 11, 272, 17, 11, 293, 291, 393, 611, 536, 577, 731, 300, 775, 322, 264, 51410, 51410, 3278, 12, 3337, 327, 399, 992, 293, 370, 322, 13, 51572, 51572, 961, 311, 1066, 1382, 661, 4190, 295, 13607, 11, 293, 294, 341, 1365, 11, 286, 478, 516, 281, 853, 33651, 51821, 51821], "temperature": 0.0, "avg_logprob": -0.1334397315979004, "compression_ratio": 1.6612244897959183, "no_speech_prob": 3.6688204545498593e-06}, {"id": 45, "seek": 23852, "start": 245.8, "end": 247.0, "text": " different value of lambda.", "tokens": [50364, 293, 291, 393, 550, 14722, 264, 3278, 12, 3337, 327, 399, 6713, 11, 49802, 53, 295, 261, 16, 11, 272, 16, 11, 293, 586, 718, 311, 853, 257, 50728, 50728, 819, 2158, 295, 13607, 13, 50788, 50788, 961, 311, 584, 291, 853, 13607, 6915, 1958, 13, 10607, 11, 550, 797, 11, 46608, 264, 2063, 2445, 2709, 51075, 51075, 291, 257, 1150, 992, 295, 9834, 11, 261, 17, 11, 272, 17, 11, 293, 291, 393, 611, 536, 577, 731, 300, 775, 322, 264, 51410, 51410, 3278, 12, 3337, 327, 399, 992, 293, 370, 322, 13, 51572, 51572, 961, 311, 1066, 1382, 661, 4190, 295, 13607, 11, 293, 294, 341, 1365, 11, 286, 478, 516, 281, 853, 33651, 51821, 51821], "temperature": 0.0, "avg_logprob": -0.1334397315979004, "compression_ratio": 1.6612244897959183, "no_speech_prob": 3.6688204545498593e-06}, {"id": 46, "seek": 23852, "start": 247.0, "end": 252.74, "text": " Let's say you try lambda equals 0.01, then again, minimizing the cost function gives", "tokens": [50364, 293, 291, 393, 550, 14722, 264, 3278, 12, 3337, 327, 399, 6713, 11, 49802, 53, 295, 261, 16, 11, 272, 16, 11, 293, 586, 718, 311, 853, 257, 50728, 50728, 819, 2158, 295, 13607, 13, 50788, 50788, 961, 311, 584, 291, 853, 13607, 6915, 1958, 13, 10607, 11, 550, 797, 11, 46608, 264, 2063, 2445, 2709, 51075, 51075, 291, 257, 1150, 992, 295, 9834, 11, 261, 17, 11, 272, 17, 11, 293, 291, 393, 611, 536, 577, 731, 300, 775, 322, 264, 51410, 51410, 3278, 12, 3337, 327, 399, 992, 293, 370, 322, 13, 51572, 51572, 961, 311, 1066, 1382, 661, 4190, 295, 13607, 11, 293, 294, 341, 1365, 11, 286, 478, 516, 281, 853, 33651, 51821, 51821], "temperature": 0.0, "avg_logprob": -0.1334397315979004, "compression_ratio": 1.6612244897959183, "no_speech_prob": 3.6688204545498593e-06}, {"id": 47, "seek": 23852, "start": 252.74, "end": 259.44, "text": " you a second set of parameters, w2, b2, and you can also see how well that does on the", "tokens": [50364, 293, 291, 393, 550, 14722, 264, 3278, 12, 3337, 327, 399, 6713, 11, 49802, 53, 295, 261, 16, 11, 272, 16, 11, 293, 586, 718, 311, 853, 257, 50728, 50728, 819, 2158, 295, 13607, 13, 50788, 50788, 961, 311, 584, 291, 853, 13607, 6915, 1958, 13, 10607, 11, 550, 797, 11, 46608, 264, 2063, 2445, 2709, 51075, 51075, 291, 257, 1150, 992, 295, 9834, 11, 261, 17, 11, 272, 17, 11, 293, 291, 393, 611, 536, 577, 731, 300, 775, 322, 264, 51410, 51410, 3278, 12, 3337, 327, 399, 992, 293, 370, 322, 13, 51572, 51572, 961, 311, 1066, 1382, 661, 4190, 295, 13607, 11, 293, 294, 341, 1365, 11, 286, 478, 516, 281, 853, 33651, 51821, 51821], "temperature": 0.0, "avg_logprob": -0.1334397315979004, "compression_ratio": 1.6612244897959183, "no_speech_prob": 3.6688204545498593e-06}, {"id": 48, "seek": 23852, "start": 259.44, "end": 262.68, "text": " cross-validation set and so on.", "tokens": [50364, 293, 291, 393, 550, 14722, 264, 3278, 12, 3337, 327, 399, 6713, 11, 49802, 53, 295, 261, 16, 11, 272, 16, 11, 293, 586, 718, 311, 853, 257, 50728, 50728, 819, 2158, 295, 13607, 13, 50788, 50788, 961, 311, 584, 291, 853, 13607, 6915, 1958, 13, 10607, 11, 550, 797, 11, 46608, 264, 2063, 2445, 2709, 51075, 51075, 291, 257, 1150, 992, 295, 9834, 11, 261, 17, 11, 272, 17, 11, 293, 291, 393, 611, 536, 577, 731, 300, 775, 322, 264, 51410, 51410, 3278, 12, 3337, 327, 399, 992, 293, 370, 322, 13, 51572, 51572, 961, 311, 1066, 1382, 661, 4190, 295, 13607, 11, 293, 294, 341, 1365, 11, 286, 478, 516, 281, 853, 33651, 51821, 51821], "temperature": 0.0, "avg_logprob": -0.1334397315979004, "compression_ratio": 1.6612244897959183, "no_speech_prob": 3.6688204545498593e-06}, {"id": 49, "seek": 23852, "start": 262.68, "end": 267.66, "text": " Let's keep trying other values of lambda, and in this example, I'm going to try doubling", "tokens": [50364, 293, 291, 393, 550, 14722, 264, 3278, 12, 3337, 327, 399, 6713, 11, 49802, 53, 295, 261, 16, 11, 272, 16, 11, 293, 586, 718, 311, 853, 257, 50728, 50728, 819, 2158, 295, 13607, 13, 50788, 50788, 961, 311, 584, 291, 853, 13607, 6915, 1958, 13, 10607, 11, 550, 797, 11, 46608, 264, 2063, 2445, 2709, 51075, 51075, 291, 257, 1150, 992, 295, 9834, 11, 261, 17, 11, 272, 17, 11, 293, 291, 393, 611, 536, 577, 731, 300, 775, 322, 264, 51410, 51410, 3278, 12, 3337, 327, 399, 992, 293, 370, 322, 13, 51572, 51572, 961, 311, 1066, 1382, 661, 4190, 295, 13607, 11, 293, 294, 341, 1365, 11, 286, 478, 516, 281, 853, 33651, 51821, 51821], "temperature": 0.0, "avg_logprob": -0.1334397315979004, "compression_ratio": 1.6612244897959183, "no_speech_prob": 3.6688204545498593e-06}, {"id": 50, "seek": 26766, "start": 267.66, "end": 276.16, "text": " it to lambda equals 0.02, and so that would give you JCV of w3, b3, and so on.", "tokens": [50364, 309, 281, 13607, 6915, 1958, 13, 12756, 11, 293, 370, 300, 576, 976, 291, 49802, 53, 295, 261, 18, 11, 272, 18, 11, 293, 370, 322, 13, 50789, 50789, 961, 311, 3834, 264, 6052, 293, 3834, 264, 6052, 13, 50917, 50917, 2381, 33651, 341, 1230, 295, 1413, 11, 291, 917, 493, 365, 13607, 10447, 2681, 281, 1266, 11, 51161, 51161, 293, 300, 576, 976, 291, 9834, 261, 4762, 11, 272, 4762, 11, 293, 49802, 53, 261, 4762, 11, 272, 4762, 11, 293, 538, 1382, 484, 264, 2416, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.13902384370237916, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.994422857227619e-06}, {"id": 51, "seek": 26766, "start": 276.16, "end": 278.72, "text": " Let's double the gain and double the gain.", "tokens": [50364, 309, 281, 13607, 6915, 1958, 13, 12756, 11, 293, 370, 300, 576, 976, 291, 49802, 53, 295, 261, 18, 11, 272, 18, 11, 293, 370, 322, 13, 50789, 50789, 961, 311, 3834, 264, 6052, 293, 3834, 264, 6052, 13, 50917, 50917, 2381, 33651, 341, 1230, 295, 1413, 11, 291, 917, 493, 365, 13607, 10447, 2681, 281, 1266, 11, 51161, 51161, 293, 300, 576, 976, 291, 9834, 261, 4762, 11, 272, 4762, 11, 293, 49802, 53, 261, 4762, 11, 272, 4762, 11, 293, 538, 1382, 484, 264, 2416, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.13902384370237916, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.994422857227619e-06}, {"id": 52, "seek": 26766, "start": 278.72, "end": 283.6, "text": " After doubling this number of times, you end up with lambda approximately equal to 10,", "tokens": [50364, 309, 281, 13607, 6915, 1958, 13, 12756, 11, 293, 370, 300, 576, 976, 291, 49802, 53, 295, 261, 18, 11, 272, 18, 11, 293, 370, 322, 13, 50789, 50789, 961, 311, 3834, 264, 6052, 293, 3834, 264, 6052, 13, 50917, 50917, 2381, 33651, 341, 1230, 295, 1413, 11, 291, 917, 493, 365, 13607, 10447, 2681, 281, 1266, 11, 51161, 51161, 293, 300, 576, 976, 291, 9834, 261, 4762, 11, 272, 4762, 11, 293, 49802, 53, 261, 4762, 11, 272, 4762, 11, 293, 538, 1382, 484, 264, 2416, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.13902384370237916, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.994422857227619e-06}, {"id": 53, "seek": 26766, "start": 283.6, "end": 295.04, "text": " and that would give you parameters w12, b12, and JCV w12, b12, and by trying out the large", "tokens": [50364, 309, 281, 13607, 6915, 1958, 13, 12756, 11, 293, 370, 300, 576, 976, 291, 49802, 53, 295, 261, 18, 11, 272, 18, 11, 293, 370, 322, 13, 50789, 50789, 961, 311, 3834, 264, 6052, 293, 3834, 264, 6052, 13, 50917, 50917, 2381, 33651, 341, 1230, 295, 1413, 11, 291, 917, 493, 365, 13607, 10447, 2681, 281, 1266, 11, 51161, 51161, 293, 300, 576, 976, 291, 9834, 261, 4762, 11, 272, 4762, 11, 293, 49802, 53, 261, 4762, 11, 272, 4762, 11, 293, 538, 1382, 484, 264, 2416, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.13902384370237916, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.994422857227619e-06}, {"id": 54, "seek": 29504, "start": 295.04, "end": 300.24, "text": " range of possible values for lambda, fitting parameters using those different regularization", "tokens": [50364, 3613, 295, 1944, 4190, 337, 13607, 11, 15669, 9834, 1228, 729, 819, 3890, 2144, 50624, 50624, 9834, 293, 550, 27479, 264, 3389, 322, 264, 3278, 12, 3337, 327, 399, 992, 11, 291, 393, 550, 50880, 50880, 853, 281, 1888, 437, 307, 264, 1151, 2158, 337, 264, 3890, 2144, 13075, 13, 51094, 51094, 18200, 1505, 736, 11, 498, 294, 341, 1365, 11, 291, 915, 300, 49802, 53, 295, 261, 20, 11, 272, 20, 11, 575, 264, 12437, 2158, 295, 439, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.15046161054128623, "compression_ratio": 1.661764705882353, "no_speech_prob": 3.7266042909323005e-06}, {"id": 55, "seek": 29504, "start": 300.24, "end": 305.36, "text": " parameters and then evaluating the performance on the cross-validation set, you can then", "tokens": [50364, 3613, 295, 1944, 4190, 337, 13607, 11, 15669, 9834, 1228, 729, 819, 3890, 2144, 50624, 50624, 9834, 293, 550, 27479, 264, 3389, 322, 264, 3278, 12, 3337, 327, 399, 992, 11, 291, 393, 550, 50880, 50880, 853, 281, 1888, 437, 307, 264, 1151, 2158, 337, 264, 3890, 2144, 13075, 13, 51094, 51094, 18200, 1505, 736, 11, 498, 294, 341, 1365, 11, 291, 915, 300, 49802, 53, 295, 261, 20, 11, 272, 20, 11, 575, 264, 12437, 2158, 295, 439, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.15046161054128623, "compression_ratio": 1.661764705882353, "no_speech_prob": 3.7266042909323005e-06}, {"id": 56, "seek": 29504, "start": 305.36, "end": 309.64000000000004, "text": " try to pick what is the best value for the regularization parameter.", "tokens": [50364, 3613, 295, 1944, 4190, 337, 13607, 11, 15669, 9834, 1228, 729, 819, 3890, 2144, 50624, 50624, 9834, 293, 550, 27479, 264, 3389, 322, 264, 3278, 12, 3337, 327, 399, 992, 11, 291, 393, 550, 50880, 50880, 853, 281, 1888, 437, 307, 264, 1151, 2158, 337, 264, 3890, 2144, 13075, 13, 51094, 51094, 18200, 1505, 736, 11, 498, 294, 341, 1365, 11, 291, 915, 300, 49802, 53, 295, 261, 20, 11, 272, 20, 11, 575, 264, 12437, 2158, 295, 439, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.15046161054128623, "compression_ratio": 1.661764705882353, "no_speech_prob": 3.7266042909323005e-06}, {"id": 57, "seek": 29504, "start": 309.64000000000004, "end": 319.36, "text": " Concretely, if in this example, you find that JCV of w5, b5, has the lowest value of all", "tokens": [50364, 3613, 295, 1944, 4190, 337, 13607, 11, 15669, 9834, 1228, 729, 819, 3890, 2144, 50624, 50624, 9834, 293, 550, 27479, 264, 3389, 322, 264, 3278, 12, 3337, 327, 399, 992, 11, 291, 393, 550, 50880, 50880, 853, 281, 1888, 437, 307, 264, 1151, 2158, 337, 264, 3890, 2144, 13075, 13, 51094, 51094, 18200, 1505, 736, 11, 498, 294, 341, 1365, 11, 291, 915, 300, 49802, 53, 295, 261, 20, 11, 272, 20, 11, 575, 264, 12437, 2158, 295, 439, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.15046161054128623, "compression_ratio": 1.661764705882353, "no_speech_prob": 3.7266042909323005e-06}, {"id": 58, "seek": 31936, "start": 319.36, "end": 325.12, "text": " of these different cross-validation errors, you might then decide to pick this value for", "tokens": [50364, 295, 613, 819, 3278, 12, 3337, 327, 399, 13603, 11, 291, 1062, 550, 4536, 281, 1888, 341, 2158, 337, 50652, 50652, 13607, 293, 370, 764, 261, 20, 11, 272, 20, 382, 264, 8614, 9834, 13, 50988, 50988, 400, 2721, 11, 498, 291, 528, 281, 2275, 484, 364, 12539, 295, 264, 2674, 2144, 6713, 11, 291, 51244, 51244, 576, 550, 2275, 484, 264, 1500, 992, 6713, 508, 1500, 295, 261, 20, 11, 272, 20, 13, 51556, 51556, 1407, 3052, 43212, 24002, 466, 437, 341, 9284, 307, 884, 11, 718, 311, 747, 257, 574, 412, 577, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.0991147586277553, "compression_ratio": 1.618421052631579, "no_speech_prob": 2.6015800358436536e-06}, {"id": 59, "seek": 31936, "start": 325.12, "end": 331.84000000000003, "text": " lambda and so use w5, b5 as the chosen parameters.", "tokens": [50364, 295, 613, 819, 3278, 12, 3337, 327, 399, 13603, 11, 291, 1062, 550, 4536, 281, 1888, 341, 2158, 337, 50652, 50652, 13607, 293, 370, 764, 261, 20, 11, 272, 20, 382, 264, 8614, 9834, 13, 50988, 50988, 400, 2721, 11, 498, 291, 528, 281, 2275, 484, 364, 12539, 295, 264, 2674, 2144, 6713, 11, 291, 51244, 51244, 576, 550, 2275, 484, 264, 1500, 992, 6713, 508, 1500, 295, 261, 20, 11, 272, 20, 13, 51556, 51556, 1407, 3052, 43212, 24002, 466, 437, 341, 9284, 307, 884, 11, 718, 311, 747, 257, 574, 412, 577, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.0991147586277553, "compression_ratio": 1.618421052631579, "no_speech_prob": 2.6015800358436536e-06}, {"id": 60, "seek": 31936, "start": 331.84000000000003, "end": 336.96000000000004, "text": " And finally, if you want to report out an estimate of the generalization error, you", "tokens": [50364, 295, 613, 819, 3278, 12, 3337, 327, 399, 13603, 11, 291, 1062, 550, 4536, 281, 1888, 341, 2158, 337, 50652, 50652, 13607, 293, 370, 764, 261, 20, 11, 272, 20, 382, 264, 8614, 9834, 13, 50988, 50988, 400, 2721, 11, 498, 291, 528, 281, 2275, 484, 364, 12539, 295, 264, 2674, 2144, 6713, 11, 291, 51244, 51244, 576, 550, 2275, 484, 264, 1500, 992, 6713, 508, 1500, 295, 261, 20, 11, 272, 20, 13, 51556, 51556, 1407, 3052, 43212, 24002, 466, 437, 341, 9284, 307, 884, 11, 718, 311, 747, 257, 574, 412, 577, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.0991147586277553, "compression_ratio": 1.618421052631579, "no_speech_prob": 2.6015800358436536e-06}, {"id": 61, "seek": 31936, "start": 336.96000000000004, "end": 343.2, "text": " would then report out the test set error J test of w5, b5.", "tokens": [50364, 295, 613, 819, 3278, 12, 3337, 327, 399, 13603, 11, 291, 1062, 550, 4536, 281, 1888, 341, 2158, 337, 50652, 50652, 13607, 293, 370, 764, 261, 20, 11, 272, 20, 382, 264, 8614, 9834, 13, 50988, 50988, 400, 2721, 11, 498, 291, 528, 281, 2275, 484, 364, 12539, 295, 264, 2674, 2144, 6713, 11, 291, 51244, 51244, 576, 550, 2275, 484, 264, 1500, 992, 6713, 508, 1500, 295, 261, 20, 11, 272, 20, 13, 51556, 51556, 1407, 3052, 43212, 24002, 466, 437, 341, 9284, 307, 884, 11, 718, 311, 747, 257, 574, 412, 577, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.0991147586277553, "compression_ratio": 1.618421052631579, "no_speech_prob": 2.6015800358436536e-06}, {"id": 62, "seek": 31936, "start": 343.2, "end": 347.76, "text": " To further hone intuition about what this algorithm is doing, let's take a look at how", "tokens": [50364, 295, 613, 819, 3278, 12, 3337, 327, 399, 13603, 11, 291, 1062, 550, 4536, 281, 1888, 341, 2158, 337, 50652, 50652, 13607, 293, 370, 764, 261, 20, 11, 272, 20, 382, 264, 8614, 9834, 13, 50988, 50988, 400, 2721, 11, 498, 291, 528, 281, 2275, 484, 364, 12539, 295, 264, 2674, 2144, 6713, 11, 291, 51244, 51244, 576, 550, 2275, 484, 264, 1500, 992, 6713, 508, 1500, 295, 261, 20, 11, 272, 20, 13, 51556, 51556, 1407, 3052, 43212, 24002, 466, 437, 341, 9284, 307, 884, 11, 718, 311, 747, 257, 574, 412, 577, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.0991147586277553, "compression_ratio": 1.618421052631579, "no_speech_prob": 2.6015800358436536e-06}, {"id": 63, "seek": 34776, "start": 347.76, "end": 354.28, "text": " training error and cross-validation error vary as a function of the parameter lambda.", "tokens": [50364, 3097, 6713, 293, 3278, 12, 3337, 327, 399, 6713, 10559, 382, 257, 2445, 295, 264, 13075, 13607, 13, 50690, 50690, 407, 294, 341, 2573, 11, 286, 600, 3105, 264, 2031, 12, 24633, 797, 13, 50864, 50864, 13428, 300, 264, 2031, 12, 24633, 510, 307, 25339, 770, 365, 264, 2158, 295, 264, 3890, 2144, 13075, 51176, 51176, 13607, 13, 51306, 51306, 400, 498, 321, 574, 412, 264, 8084, 295, 13607, 6915, 4018, 11, 510, 322, 264, 1411, 11, 300, 23249, 281, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.11440140860421318, "compression_ratio": 1.634517766497462, "no_speech_prob": 2.561269411671674e-06}, {"id": 64, "seek": 34776, "start": 354.28, "end": 357.76, "text": " So in this figure, I've changed the x-axis again.", "tokens": [50364, 3097, 6713, 293, 3278, 12, 3337, 327, 399, 6713, 10559, 382, 257, 2445, 295, 264, 13075, 13607, 13, 50690, 50690, 407, 294, 341, 2573, 11, 286, 600, 3105, 264, 2031, 12, 24633, 797, 13, 50864, 50864, 13428, 300, 264, 2031, 12, 24633, 510, 307, 25339, 770, 365, 264, 2158, 295, 264, 3890, 2144, 13075, 51176, 51176, 13607, 13, 51306, 51306, 400, 498, 321, 574, 412, 264, 8084, 295, 13607, 6915, 4018, 11, 510, 322, 264, 1411, 11, 300, 23249, 281, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.11440140860421318, "compression_ratio": 1.634517766497462, "no_speech_prob": 2.561269411671674e-06}, {"id": 65, "seek": 34776, "start": 357.76, "end": 364.0, "text": " Notice that the x-axis here is annotated with the value of the regularization parameter", "tokens": [50364, 3097, 6713, 293, 3278, 12, 3337, 327, 399, 6713, 10559, 382, 257, 2445, 295, 264, 13075, 13607, 13, 50690, 50690, 407, 294, 341, 2573, 11, 286, 600, 3105, 264, 2031, 12, 24633, 797, 13, 50864, 50864, 13428, 300, 264, 2031, 12, 24633, 510, 307, 25339, 770, 365, 264, 2158, 295, 264, 3890, 2144, 13075, 51176, 51176, 13607, 13, 51306, 51306, 400, 498, 321, 574, 412, 264, 8084, 295, 13607, 6915, 4018, 11, 510, 322, 264, 1411, 11, 300, 23249, 281, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.11440140860421318, "compression_ratio": 1.634517766497462, "no_speech_prob": 2.561269411671674e-06}, {"id": 66, "seek": 34776, "start": 364.0, "end": 366.59999999999997, "text": " lambda.", "tokens": [50364, 3097, 6713, 293, 3278, 12, 3337, 327, 399, 6713, 10559, 382, 257, 2445, 295, 264, 13075, 13607, 13, 50690, 50690, 407, 294, 341, 2573, 11, 286, 600, 3105, 264, 2031, 12, 24633, 797, 13, 50864, 50864, 13428, 300, 264, 2031, 12, 24633, 510, 307, 25339, 770, 365, 264, 2158, 295, 264, 3890, 2144, 13075, 51176, 51176, 13607, 13, 51306, 51306, 400, 498, 321, 574, 412, 264, 8084, 295, 13607, 6915, 4018, 11, 510, 322, 264, 1411, 11, 300, 23249, 281, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.11440140860421318, "compression_ratio": 1.634517766497462, "no_speech_prob": 2.561269411671674e-06}, {"id": 67, "seek": 34776, "start": 366.59999999999997, "end": 374.44, "text": " And if we look at the extreme of lambda equals zero, here on the left, that corresponds to", "tokens": [50364, 3097, 6713, 293, 3278, 12, 3337, 327, 399, 6713, 10559, 382, 257, 2445, 295, 264, 13075, 13607, 13, 50690, 50690, 407, 294, 341, 2573, 11, 286, 600, 3105, 264, 2031, 12, 24633, 797, 13, 50864, 50864, 13428, 300, 264, 2031, 12, 24633, 510, 307, 25339, 770, 365, 264, 2158, 295, 264, 3890, 2144, 13075, 51176, 51176, 13607, 13, 51306, 51306, 400, 498, 321, 574, 412, 264, 8084, 295, 13607, 6915, 4018, 11, 510, 322, 264, 1411, 11, 300, 23249, 281, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.11440140860421318, "compression_ratio": 1.634517766497462, "no_speech_prob": 2.561269411671674e-06}, {"id": 68, "seek": 37444, "start": 374.44, "end": 379.16, "text": " not using any regularization and so that's where we want to end up with this very wiggly", "tokens": [50364, 406, 1228, 604, 3890, 2144, 293, 370, 300, 311, 689, 321, 528, 281, 917, 493, 365, 341, 588, 261, 46737, 50600, 50600, 7605, 498, 13607, 390, 1359, 420, 390, 754, 4018, 13, 50848, 50848, 400, 294, 300, 1389, 11, 321, 362, 257, 1090, 21977, 2316, 293, 370, 508, 3847, 307, 516, 281, 312, 1359, 51234, 51234, 293, 49802, 53, 307, 516, 281, 312, 2416, 570, 309, 775, 869, 322, 264, 3097, 1412, 457, 775, 709, 5324, 51528, 51528, 322, 264, 3278, 12, 3337, 327, 399, 1412, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.1501565136752286, "compression_ratio": 1.5868544600938967, "no_speech_prob": 7.889142580097541e-06}, {"id": 69, "seek": 37444, "start": 379.16, "end": 384.12, "text": " curve if lambda was small or was even zero.", "tokens": [50364, 406, 1228, 604, 3890, 2144, 293, 370, 300, 311, 689, 321, 528, 281, 917, 493, 365, 341, 588, 261, 46737, 50600, 50600, 7605, 498, 13607, 390, 1359, 420, 390, 754, 4018, 13, 50848, 50848, 400, 294, 300, 1389, 11, 321, 362, 257, 1090, 21977, 2316, 293, 370, 508, 3847, 307, 516, 281, 312, 1359, 51234, 51234, 293, 49802, 53, 307, 516, 281, 312, 2416, 570, 309, 775, 869, 322, 264, 3097, 1412, 457, 775, 709, 5324, 51528, 51528, 322, 264, 3278, 12, 3337, 327, 399, 1412, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.1501565136752286, "compression_ratio": 1.5868544600938967, "no_speech_prob": 7.889142580097541e-06}, {"id": 70, "seek": 37444, "start": 384.12, "end": 391.84, "text": " And in that case, we have a high variance model and so J train is going to be small", "tokens": [50364, 406, 1228, 604, 3890, 2144, 293, 370, 300, 311, 689, 321, 528, 281, 917, 493, 365, 341, 588, 261, 46737, 50600, 50600, 7605, 498, 13607, 390, 1359, 420, 390, 754, 4018, 13, 50848, 50848, 400, 294, 300, 1389, 11, 321, 362, 257, 1090, 21977, 2316, 293, 370, 508, 3847, 307, 516, 281, 312, 1359, 51234, 51234, 293, 49802, 53, 307, 516, 281, 312, 2416, 570, 309, 775, 869, 322, 264, 3097, 1412, 457, 775, 709, 5324, 51528, 51528, 322, 264, 3278, 12, 3337, 327, 399, 1412, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.1501565136752286, "compression_ratio": 1.5868544600938967, "no_speech_prob": 7.889142580097541e-06}, {"id": 71, "seek": 37444, "start": 391.84, "end": 397.72, "text": " and JCV is going to be large because it does great on the training data but does much worse", "tokens": [50364, 406, 1228, 604, 3890, 2144, 293, 370, 300, 311, 689, 321, 528, 281, 917, 493, 365, 341, 588, 261, 46737, 50600, 50600, 7605, 498, 13607, 390, 1359, 420, 390, 754, 4018, 13, 50848, 50848, 400, 294, 300, 1389, 11, 321, 362, 257, 1090, 21977, 2316, 293, 370, 508, 3847, 307, 516, 281, 312, 1359, 51234, 51234, 293, 49802, 53, 307, 516, 281, 312, 2416, 570, 309, 775, 869, 322, 264, 3097, 1412, 457, 775, 709, 5324, 51528, 51528, 322, 264, 3278, 12, 3337, 327, 399, 1412, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.1501565136752286, "compression_ratio": 1.5868544600938967, "no_speech_prob": 7.889142580097541e-06}, {"id": 72, "seek": 37444, "start": 397.72, "end": 400.56, "text": " on the cross-validation data.", "tokens": [50364, 406, 1228, 604, 3890, 2144, 293, 370, 300, 311, 689, 321, 528, 281, 917, 493, 365, 341, 588, 261, 46737, 50600, 50600, 7605, 498, 13607, 390, 1359, 420, 390, 754, 4018, 13, 50848, 50848, 400, 294, 300, 1389, 11, 321, 362, 257, 1090, 21977, 2316, 293, 370, 508, 3847, 307, 516, 281, 312, 1359, 51234, 51234, 293, 49802, 53, 307, 516, 281, 312, 2416, 570, 309, 775, 869, 322, 264, 3097, 1412, 457, 775, 709, 5324, 51528, 51528, 322, 264, 3278, 12, 3337, 327, 399, 1412, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.1501565136752286, "compression_ratio": 1.5868544600938967, "no_speech_prob": 7.889142580097541e-06}, {"id": 73, "seek": 40056, "start": 400.56, "end": 406.28000000000003, "text": " This extreme on the right with very large values of lambda, say lambda equals 10,000,", "tokens": [50364, 639, 8084, 322, 264, 558, 365, 588, 2416, 4190, 295, 13607, 11, 584, 13607, 6915, 1266, 11, 1360, 11, 50650, 50650, 5314, 493, 365, 15669, 257, 2316, 300, 1542, 411, 300, 13, 50810, 50810, 407, 341, 575, 1090, 12577, 11, 309, 833, 13979, 264, 1412, 293, 309, 4523, 484, 508, 3847, 486, 312, 1090, 293, 51138, 51138, 49802, 53, 486, 312, 1090, 382, 731, 13, 51346, 51346, 400, 294, 1186, 11, 498, 291, 645, 281, 574, 412, 577, 508, 3847, 21716, 382, 257, 2445, 295, 13607, 11, 291, 51662, 51662], "temperature": 0.0, "avg_logprob": -0.10477754344110904, "compression_ratio": 1.5488372093023255, "no_speech_prob": 1.81621339834237e-06}, {"id": 74, "seek": 40056, "start": 406.28000000000003, "end": 409.48, "text": " ends up with fitting a model that looks like that.", "tokens": [50364, 639, 8084, 322, 264, 558, 365, 588, 2416, 4190, 295, 13607, 11, 584, 13607, 6915, 1266, 11, 1360, 11, 50650, 50650, 5314, 493, 365, 15669, 257, 2316, 300, 1542, 411, 300, 13, 50810, 50810, 407, 341, 575, 1090, 12577, 11, 309, 833, 13979, 264, 1412, 293, 309, 4523, 484, 508, 3847, 486, 312, 1090, 293, 51138, 51138, 49802, 53, 486, 312, 1090, 382, 731, 13, 51346, 51346, 400, 294, 1186, 11, 498, 291, 645, 281, 574, 412, 577, 508, 3847, 21716, 382, 257, 2445, 295, 13607, 11, 291, 51662, 51662], "temperature": 0.0, "avg_logprob": -0.10477754344110904, "compression_ratio": 1.5488372093023255, "no_speech_prob": 1.81621339834237e-06}, {"id": 75, "seek": 40056, "start": 409.48, "end": 416.04, "text": " So this has high bias, it underfits the data and it turns out J train will be high and", "tokens": [50364, 639, 8084, 322, 264, 558, 365, 588, 2416, 4190, 295, 13607, 11, 584, 13607, 6915, 1266, 11, 1360, 11, 50650, 50650, 5314, 493, 365, 15669, 257, 2316, 300, 1542, 411, 300, 13, 50810, 50810, 407, 341, 575, 1090, 12577, 11, 309, 833, 13979, 264, 1412, 293, 309, 4523, 484, 508, 3847, 486, 312, 1090, 293, 51138, 51138, 49802, 53, 486, 312, 1090, 382, 731, 13, 51346, 51346, 400, 294, 1186, 11, 498, 291, 645, 281, 574, 412, 577, 508, 3847, 21716, 382, 257, 2445, 295, 13607, 11, 291, 51662, 51662], "temperature": 0.0, "avg_logprob": -0.10477754344110904, "compression_ratio": 1.5488372093023255, "no_speech_prob": 1.81621339834237e-06}, {"id": 76, "seek": 40056, "start": 416.04, "end": 420.2, "text": " JCV will be high as well.", "tokens": [50364, 639, 8084, 322, 264, 558, 365, 588, 2416, 4190, 295, 13607, 11, 584, 13607, 6915, 1266, 11, 1360, 11, 50650, 50650, 5314, 493, 365, 15669, 257, 2316, 300, 1542, 411, 300, 13, 50810, 50810, 407, 341, 575, 1090, 12577, 11, 309, 833, 13979, 264, 1412, 293, 309, 4523, 484, 508, 3847, 486, 312, 1090, 293, 51138, 51138, 49802, 53, 486, 312, 1090, 382, 731, 13, 51346, 51346, 400, 294, 1186, 11, 498, 291, 645, 281, 574, 412, 577, 508, 3847, 21716, 382, 257, 2445, 295, 13607, 11, 291, 51662, 51662], "temperature": 0.0, "avg_logprob": -0.10477754344110904, "compression_ratio": 1.5488372093023255, "no_speech_prob": 1.81621339834237e-06}, {"id": 77, "seek": 40056, "start": 420.2, "end": 426.52, "text": " And in fact, if you were to look at how J train varies as a function of lambda, you", "tokens": [50364, 639, 8084, 322, 264, 558, 365, 588, 2416, 4190, 295, 13607, 11, 584, 13607, 6915, 1266, 11, 1360, 11, 50650, 50650, 5314, 493, 365, 15669, 257, 2316, 300, 1542, 411, 300, 13, 50810, 50810, 407, 341, 575, 1090, 12577, 11, 309, 833, 13979, 264, 1412, 293, 309, 4523, 484, 508, 3847, 486, 312, 1090, 293, 51138, 51138, 49802, 53, 486, 312, 1090, 382, 731, 13, 51346, 51346, 400, 294, 1186, 11, 498, 291, 645, 281, 574, 412, 577, 508, 3847, 21716, 382, 257, 2445, 295, 13607, 11, 291, 51662, 51662], "temperature": 0.0, "avg_logprob": -0.10477754344110904, "compression_ratio": 1.5488372093023255, "no_speech_prob": 1.81621339834237e-06}, {"id": 78, "seek": 42652, "start": 426.52, "end": 433.52, "text": " find that J train will go up like this because in the optimization cost function, the larger", "tokens": [50364, 915, 300, 508, 3847, 486, 352, 493, 411, 341, 570, 294, 264, 19618, 2063, 2445, 11, 264, 4833, 50714, 50714, 13607, 307, 11, 264, 544, 264, 9284, 307, 1382, 281, 1066, 261, 8889, 1359, 11, 300, 307, 264, 544, 50992, 50992, 3364, 307, 2212, 281, 341, 3890, 2144, 1433, 293, 8807, 264, 1570, 3202, 307, 4835, 281, 767, 51262, 51262, 884, 731, 322, 264, 3097, 992, 13, 51356, 51356, 1779, 11, 341, 1433, 322, 264, 1411, 307, 508, 3847, 11, 370, 264, 544, 309, 311, 1382, 281, 1066, 264, 9834, 51588, 51588, 1359, 11, 264, 1570, 665, 257, 1691, 309, 775, 322, 46608, 264, 3097, 6713, 13, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.16991036217491906, "compression_ratio": 1.8214285714285714, "no_speech_prob": 2.521546093703364e-06}, {"id": 79, "seek": 42652, "start": 433.52, "end": 439.08, "text": " lambda is, the more the algorithm is trying to keep w squared small, that is the more", "tokens": [50364, 915, 300, 508, 3847, 486, 352, 493, 411, 341, 570, 294, 264, 19618, 2063, 2445, 11, 264, 4833, 50714, 50714, 13607, 307, 11, 264, 544, 264, 9284, 307, 1382, 281, 1066, 261, 8889, 1359, 11, 300, 307, 264, 544, 50992, 50992, 3364, 307, 2212, 281, 341, 3890, 2144, 1433, 293, 8807, 264, 1570, 3202, 307, 4835, 281, 767, 51262, 51262, 884, 731, 322, 264, 3097, 992, 13, 51356, 51356, 1779, 11, 341, 1433, 322, 264, 1411, 307, 508, 3847, 11, 370, 264, 544, 309, 311, 1382, 281, 1066, 264, 9834, 51588, 51588, 1359, 11, 264, 1570, 665, 257, 1691, 309, 775, 322, 46608, 264, 3097, 6713, 13, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.16991036217491906, "compression_ratio": 1.8214285714285714, "no_speech_prob": 2.521546093703364e-06}, {"id": 80, "seek": 42652, "start": 439.08, "end": 444.47999999999996, "text": " weight is given to this regularization term and thus the less attention is paid to actually", "tokens": [50364, 915, 300, 508, 3847, 486, 352, 493, 411, 341, 570, 294, 264, 19618, 2063, 2445, 11, 264, 4833, 50714, 50714, 13607, 307, 11, 264, 544, 264, 9284, 307, 1382, 281, 1066, 261, 8889, 1359, 11, 300, 307, 264, 544, 50992, 50992, 3364, 307, 2212, 281, 341, 3890, 2144, 1433, 293, 8807, 264, 1570, 3202, 307, 4835, 281, 767, 51262, 51262, 884, 731, 322, 264, 3097, 992, 13, 51356, 51356, 1779, 11, 341, 1433, 322, 264, 1411, 307, 508, 3847, 11, 370, 264, 544, 309, 311, 1382, 281, 1066, 264, 9834, 51588, 51588, 1359, 11, 264, 1570, 665, 257, 1691, 309, 775, 322, 46608, 264, 3097, 6713, 13, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.16991036217491906, "compression_ratio": 1.8214285714285714, "no_speech_prob": 2.521546093703364e-06}, {"id": 81, "seek": 42652, "start": 444.47999999999996, "end": 446.35999999999996, "text": " doing well on the training set.", "tokens": [50364, 915, 300, 508, 3847, 486, 352, 493, 411, 341, 570, 294, 264, 19618, 2063, 2445, 11, 264, 4833, 50714, 50714, 13607, 307, 11, 264, 544, 264, 9284, 307, 1382, 281, 1066, 261, 8889, 1359, 11, 300, 307, 264, 544, 50992, 50992, 3364, 307, 2212, 281, 341, 3890, 2144, 1433, 293, 8807, 264, 1570, 3202, 307, 4835, 281, 767, 51262, 51262, 884, 731, 322, 264, 3097, 992, 13, 51356, 51356, 1779, 11, 341, 1433, 322, 264, 1411, 307, 508, 3847, 11, 370, 264, 544, 309, 311, 1382, 281, 1066, 264, 9834, 51588, 51588, 1359, 11, 264, 1570, 665, 257, 1691, 309, 775, 322, 46608, 264, 3097, 6713, 13, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.16991036217491906, "compression_ratio": 1.8214285714285714, "no_speech_prob": 2.521546093703364e-06}, {"id": 82, "seek": 42652, "start": 446.35999999999996, "end": 451.0, "text": " Right, this term on the left is J train, so the more it's trying to keep the parameters", "tokens": [50364, 915, 300, 508, 3847, 486, 352, 493, 411, 341, 570, 294, 264, 19618, 2063, 2445, 11, 264, 4833, 50714, 50714, 13607, 307, 11, 264, 544, 264, 9284, 307, 1382, 281, 1066, 261, 8889, 1359, 11, 300, 307, 264, 544, 50992, 50992, 3364, 307, 2212, 281, 341, 3890, 2144, 1433, 293, 8807, 264, 1570, 3202, 307, 4835, 281, 767, 51262, 51262, 884, 731, 322, 264, 3097, 992, 13, 51356, 51356, 1779, 11, 341, 1433, 322, 264, 1411, 307, 508, 3847, 11, 370, 264, 544, 309, 311, 1382, 281, 1066, 264, 9834, 51588, 51588, 1359, 11, 264, 1570, 665, 257, 1691, 309, 775, 322, 46608, 264, 3097, 6713, 13, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.16991036217491906, "compression_ratio": 1.8214285714285714, "no_speech_prob": 2.521546093703364e-06}, {"id": 83, "seek": 42652, "start": 451.0, "end": 455.96, "text": " small, the less good a job it does on minimizing the training error.", "tokens": [50364, 915, 300, 508, 3847, 486, 352, 493, 411, 341, 570, 294, 264, 19618, 2063, 2445, 11, 264, 4833, 50714, 50714, 13607, 307, 11, 264, 544, 264, 9284, 307, 1382, 281, 1066, 261, 8889, 1359, 11, 300, 307, 264, 544, 50992, 50992, 3364, 307, 2212, 281, 341, 3890, 2144, 1433, 293, 8807, 264, 1570, 3202, 307, 4835, 281, 767, 51262, 51262, 884, 731, 322, 264, 3097, 992, 13, 51356, 51356, 1779, 11, 341, 1433, 322, 264, 1411, 307, 508, 3847, 11, 370, 264, 544, 309, 311, 1382, 281, 1066, 264, 9834, 51588, 51588, 1359, 11, 264, 1570, 665, 257, 1691, 309, 775, 322, 46608, 264, 3097, 6713, 13, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.16991036217491906, "compression_ratio": 1.8214285714285714, "no_speech_prob": 2.521546093703364e-06}, {"id": 84, "seek": 45596, "start": 455.96, "end": 463.12, "text": " So that's why as lambda increases, the training error J train will tend to increase like so.", "tokens": [50364, 407, 300, 311, 983, 382, 13607, 8637, 11, 264, 3097, 6713, 508, 3847, 486, 3928, 281, 3488, 411, 370, 13, 50722, 50722, 823, 577, 466, 264, 3278, 12, 3337, 327, 399, 6713, 30, 50852, 50852, 29524, 484, 264, 3278, 12, 3337, 327, 399, 6713, 486, 574, 411, 341, 570, 321, 600, 1612, 300, 498, 51152, 51152, 13607, 307, 886, 1359, 420, 886, 2416, 11, 550, 309, 1177, 380, 360, 731, 322, 264, 3278, 12, 3337, 327, 399, 992, 13, 51434, 51434, 467, 2139, 670, 13979, 510, 322, 264, 1411, 420, 833, 13979, 510, 322, 264, 558, 293, 456, 603, 312, 512, 19376, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.1039795605641491, "compression_ratio": 1.83710407239819, "no_speech_prob": 4.157279363425914e-06}, {"id": 85, "seek": 45596, "start": 463.12, "end": 465.71999999999997, "text": " Now how about the cross-validation error?", "tokens": [50364, 407, 300, 311, 983, 382, 13607, 8637, 11, 264, 3097, 6713, 508, 3847, 486, 3928, 281, 3488, 411, 370, 13, 50722, 50722, 823, 577, 466, 264, 3278, 12, 3337, 327, 399, 6713, 30, 50852, 50852, 29524, 484, 264, 3278, 12, 3337, 327, 399, 6713, 486, 574, 411, 341, 570, 321, 600, 1612, 300, 498, 51152, 51152, 13607, 307, 886, 1359, 420, 886, 2416, 11, 550, 309, 1177, 380, 360, 731, 322, 264, 3278, 12, 3337, 327, 399, 992, 13, 51434, 51434, 467, 2139, 670, 13979, 510, 322, 264, 1411, 420, 833, 13979, 510, 322, 264, 558, 293, 456, 603, 312, 512, 19376, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.1039795605641491, "compression_ratio": 1.83710407239819, "no_speech_prob": 4.157279363425914e-06}, {"id": 86, "seek": 45596, "start": 465.71999999999997, "end": 471.71999999999997, "text": " Turns out the cross-validation error will look like this because we've seen that if", "tokens": [50364, 407, 300, 311, 983, 382, 13607, 8637, 11, 264, 3097, 6713, 508, 3847, 486, 3928, 281, 3488, 411, 370, 13, 50722, 50722, 823, 577, 466, 264, 3278, 12, 3337, 327, 399, 6713, 30, 50852, 50852, 29524, 484, 264, 3278, 12, 3337, 327, 399, 6713, 486, 574, 411, 341, 570, 321, 600, 1612, 300, 498, 51152, 51152, 13607, 307, 886, 1359, 420, 886, 2416, 11, 550, 309, 1177, 380, 360, 731, 322, 264, 3278, 12, 3337, 327, 399, 992, 13, 51434, 51434, 467, 2139, 670, 13979, 510, 322, 264, 1411, 420, 833, 13979, 510, 322, 264, 558, 293, 456, 603, 312, 512, 19376, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.1039795605641491, "compression_ratio": 1.83710407239819, "no_speech_prob": 4.157279363425914e-06}, {"id": 87, "seek": 45596, "start": 471.71999999999997, "end": 477.35999999999996, "text": " lambda is too small or too large, then it doesn't do well on the cross-validation set.", "tokens": [50364, 407, 300, 311, 983, 382, 13607, 8637, 11, 264, 3097, 6713, 508, 3847, 486, 3928, 281, 3488, 411, 370, 13, 50722, 50722, 823, 577, 466, 264, 3278, 12, 3337, 327, 399, 6713, 30, 50852, 50852, 29524, 484, 264, 3278, 12, 3337, 327, 399, 6713, 486, 574, 411, 341, 570, 321, 600, 1612, 300, 498, 51152, 51152, 13607, 307, 886, 1359, 420, 886, 2416, 11, 550, 309, 1177, 380, 360, 731, 322, 264, 3278, 12, 3337, 327, 399, 992, 13, 51434, 51434, 467, 2139, 670, 13979, 510, 322, 264, 1411, 420, 833, 13979, 510, 322, 264, 558, 293, 456, 603, 312, 512, 19376, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.1039795605641491, "compression_ratio": 1.83710407239819, "no_speech_prob": 4.157279363425914e-06}, {"id": 88, "seek": 45596, "start": 477.35999999999996, "end": 485.15999999999997, "text": " It either overfits here on the left or underfits here on the right and there'll be some intermediate", "tokens": [50364, 407, 300, 311, 983, 382, 13607, 8637, 11, 264, 3097, 6713, 508, 3847, 486, 3928, 281, 3488, 411, 370, 13, 50722, 50722, 823, 577, 466, 264, 3278, 12, 3337, 327, 399, 6713, 30, 50852, 50852, 29524, 484, 264, 3278, 12, 3337, 327, 399, 6713, 486, 574, 411, 341, 570, 321, 600, 1612, 300, 498, 51152, 51152, 13607, 307, 886, 1359, 420, 886, 2416, 11, 550, 309, 1177, 380, 360, 731, 322, 264, 3278, 12, 3337, 327, 399, 992, 13, 51434, 51434, 467, 2139, 670, 13979, 510, 322, 264, 1411, 420, 833, 13979, 510, 322, 264, 558, 293, 456, 603, 312, 512, 19376, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.1039795605641491, "compression_ratio": 1.83710407239819, "no_speech_prob": 4.157279363425914e-06}, {"id": 89, "seek": 48516, "start": 485.16, "end": 490.68, "text": " value of lambda that causes the algorithm to perform best.", "tokens": [50364, 2158, 295, 13607, 300, 7700, 264, 9284, 281, 2042, 1151, 13, 50640, 50640, 400, 437, 3278, 12, 3337, 327, 399, 307, 884, 307, 1382, 484, 257, 688, 295, 819, 4190, 295, 13607, 13, 50916, 50916, 639, 307, 437, 321, 1866, 322, 264, 1036, 4137, 11, 853, 484, 13607, 6915, 1958, 11, 13607, 6915, 1958, 13, 10607, 11, 13607, 51146, 51146, 6915, 1958, 13, 12756, 11, 853, 484, 257, 688, 295, 819, 4190, 295, 13607, 293, 13059, 264, 3278, 12, 3337, 327, 399, 51438, 51438, 6713, 412, 257, 688, 295, 613, 819, 2793, 293, 550, 4696, 1888, 257, 2158, 300, 575, 2295, 3278, 12, 3337, 327, 399, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.12776697332208806, "compression_ratio": 2.0046728971962615, "no_speech_prob": 8.139611963997595e-06}, {"id": 90, "seek": 48516, "start": 490.68, "end": 496.20000000000005, "text": " And what cross-validation is doing is trying out a lot of different values of lambda.", "tokens": [50364, 2158, 295, 13607, 300, 7700, 264, 9284, 281, 2042, 1151, 13, 50640, 50640, 400, 437, 3278, 12, 3337, 327, 399, 307, 884, 307, 1382, 484, 257, 688, 295, 819, 4190, 295, 13607, 13, 50916, 50916, 639, 307, 437, 321, 1866, 322, 264, 1036, 4137, 11, 853, 484, 13607, 6915, 1958, 11, 13607, 6915, 1958, 13, 10607, 11, 13607, 51146, 51146, 6915, 1958, 13, 12756, 11, 853, 484, 257, 688, 295, 819, 4190, 295, 13607, 293, 13059, 264, 3278, 12, 3337, 327, 399, 51438, 51438, 6713, 412, 257, 688, 295, 613, 819, 2793, 293, 550, 4696, 1888, 257, 2158, 300, 575, 2295, 3278, 12, 3337, 327, 399, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.12776697332208806, "compression_ratio": 2.0046728971962615, "no_speech_prob": 8.139611963997595e-06}, {"id": 91, "seek": 48516, "start": 496.20000000000005, "end": 500.8, "text": " This is what we saw on the last slide, try out lambda equals 0, lambda equals 0.01, lambda", "tokens": [50364, 2158, 295, 13607, 300, 7700, 264, 9284, 281, 2042, 1151, 13, 50640, 50640, 400, 437, 3278, 12, 3337, 327, 399, 307, 884, 307, 1382, 484, 257, 688, 295, 819, 4190, 295, 13607, 13, 50916, 50916, 639, 307, 437, 321, 1866, 322, 264, 1036, 4137, 11, 853, 484, 13607, 6915, 1958, 11, 13607, 6915, 1958, 13, 10607, 11, 13607, 51146, 51146, 6915, 1958, 13, 12756, 11, 853, 484, 257, 688, 295, 819, 4190, 295, 13607, 293, 13059, 264, 3278, 12, 3337, 327, 399, 51438, 51438, 6713, 412, 257, 688, 295, 613, 819, 2793, 293, 550, 4696, 1888, 257, 2158, 300, 575, 2295, 3278, 12, 3337, 327, 399, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.12776697332208806, "compression_ratio": 2.0046728971962615, "no_speech_prob": 8.139611963997595e-06}, {"id": 92, "seek": 48516, "start": 500.8, "end": 506.64000000000004, "text": " equals 0.02, try out a lot of different values of lambda and evaluate the cross-validation", "tokens": [50364, 2158, 295, 13607, 300, 7700, 264, 9284, 281, 2042, 1151, 13, 50640, 50640, 400, 437, 3278, 12, 3337, 327, 399, 307, 884, 307, 1382, 484, 257, 688, 295, 819, 4190, 295, 13607, 13, 50916, 50916, 639, 307, 437, 321, 1866, 322, 264, 1036, 4137, 11, 853, 484, 13607, 6915, 1958, 11, 13607, 6915, 1958, 13, 10607, 11, 13607, 51146, 51146, 6915, 1958, 13, 12756, 11, 853, 484, 257, 688, 295, 819, 4190, 295, 13607, 293, 13059, 264, 3278, 12, 3337, 327, 399, 51438, 51438, 6713, 412, 257, 688, 295, 613, 819, 2793, 293, 550, 4696, 1888, 257, 2158, 300, 575, 2295, 3278, 12, 3337, 327, 399, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.12776697332208806, "compression_ratio": 2.0046728971962615, "no_speech_prob": 8.139611963997595e-06}, {"id": 93, "seek": 48516, "start": 506.64000000000004, "end": 515.0400000000001, "text": " error at a lot of these different points and then hopefully pick a value that has low cross-validation", "tokens": [50364, 2158, 295, 13607, 300, 7700, 264, 9284, 281, 2042, 1151, 13, 50640, 50640, 400, 437, 3278, 12, 3337, 327, 399, 307, 884, 307, 1382, 484, 257, 688, 295, 819, 4190, 295, 13607, 13, 50916, 50916, 639, 307, 437, 321, 1866, 322, 264, 1036, 4137, 11, 853, 484, 13607, 6915, 1958, 11, 13607, 6915, 1958, 13, 10607, 11, 13607, 51146, 51146, 6915, 1958, 13, 12756, 11, 853, 484, 257, 688, 295, 819, 4190, 295, 13607, 293, 13059, 264, 3278, 12, 3337, 327, 399, 51438, 51438, 6713, 412, 257, 688, 295, 613, 819, 2793, 293, 550, 4696, 1888, 257, 2158, 300, 575, 2295, 3278, 12, 3337, 327, 399, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.12776697332208806, "compression_ratio": 2.0046728971962615, "no_speech_prob": 8.139611963997595e-06}, {"id": 94, "seek": 51504, "start": 515.04, "end": 521.52, "text": " error and this will hopefully correspond to a good model for your application.", "tokens": [50364, 6713, 293, 341, 486, 4696, 6805, 281, 257, 665, 2316, 337, 428, 3861, 13, 50688, 50688, 759, 291, 6794, 341, 10686, 281, 264, 472, 300, 321, 632, 294, 264, 3894, 960, 689, 264, 12750, 50984, 50984, 10298, 390, 264, 4314, 295, 26110, 11, 613, 732, 36709, 574, 257, 707, 857, 11, 406, 44003, 51298, 51298, 293, 406, 294, 604, 9860, 636, 11, 457, 436, 574, 257, 707, 857, 411, 8013, 5267, 295, 1184, 661, 13, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.07501835762699947, "compression_ratio": 1.6296296296296295, "no_speech_prob": 3.0241344575188123e-05}, {"id": 95, "seek": 51504, "start": 521.52, "end": 527.4399999999999, "text": " If you compare this diagram to the one that we had in the previous video where the horizontal", "tokens": [50364, 6713, 293, 341, 486, 4696, 6805, 281, 257, 665, 2316, 337, 428, 3861, 13, 50688, 50688, 759, 291, 6794, 341, 10686, 281, 264, 472, 300, 321, 632, 294, 264, 3894, 960, 689, 264, 12750, 50984, 50984, 10298, 390, 264, 4314, 295, 26110, 11, 613, 732, 36709, 574, 257, 707, 857, 11, 406, 44003, 51298, 51298, 293, 406, 294, 604, 9860, 636, 11, 457, 436, 574, 257, 707, 857, 411, 8013, 5267, 295, 1184, 661, 13, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.07501835762699947, "compression_ratio": 1.6296296296296295, "no_speech_prob": 3.0241344575188123e-05}, {"id": 96, "seek": 51504, "start": 527.4399999999999, "end": 533.7199999999999, "text": " axis was the degree of polynomial, these two diagrams look a little bit, not mathematically", "tokens": [50364, 6713, 293, 341, 486, 4696, 6805, 281, 257, 665, 2316, 337, 428, 3861, 13, 50688, 50688, 759, 291, 6794, 341, 10686, 281, 264, 472, 300, 321, 632, 294, 264, 3894, 960, 689, 264, 12750, 50984, 50984, 10298, 390, 264, 4314, 295, 26110, 11, 613, 732, 36709, 574, 257, 707, 857, 11, 406, 44003, 51298, 51298, 293, 406, 294, 604, 9860, 636, 11, 457, 436, 574, 257, 707, 857, 411, 8013, 5267, 295, 1184, 661, 13, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.07501835762699947, "compression_ratio": 1.6296296296296295, "no_speech_prob": 3.0241344575188123e-05}, {"id": 97, "seek": 51504, "start": 533.7199999999999, "end": 539.76, "text": " and not in any formal way, but they look a little bit like mirror images of each other.", "tokens": [50364, 6713, 293, 341, 486, 4696, 6805, 281, 257, 665, 2316, 337, 428, 3861, 13, 50688, 50688, 759, 291, 6794, 341, 10686, 281, 264, 472, 300, 321, 632, 294, 264, 3894, 960, 689, 264, 12750, 50984, 50984, 10298, 390, 264, 4314, 295, 26110, 11, 613, 732, 36709, 574, 257, 707, 857, 11, 406, 44003, 51298, 51298, 293, 406, 294, 604, 9860, 636, 11, 457, 436, 574, 257, 707, 857, 411, 8013, 5267, 295, 1184, 661, 13, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.07501835762699947, "compression_ratio": 1.6296296296296295, "no_speech_prob": 3.0241344575188123e-05}, {"id": 98, "seek": 53976, "start": 539.76, "end": 545.56, "text": " And that's because when you're fitting a degree of polynomial, the left part of this curve", "tokens": [50364, 400, 300, 311, 570, 562, 291, 434, 15669, 257, 4314, 295, 26110, 11, 264, 1411, 644, 295, 341, 7605, 50654, 50654, 6805, 292, 281, 670, 69, 2414, 293, 1090, 12577, 11, 264, 558, 644, 6805, 292, 281, 833, 69, 2414, 50954, 50954, 293, 1090, 21977, 11, 9735, 294, 341, 472, 11, 1090, 21977, 390, 322, 264, 1411, 293, 1090, 12577, 390, 51305, 51305, 322, 264, 558, 13, 51355, 51355, 583, 300, 311, 983, 613, 732, 5267, 366, 257, 707, 857, 411, 8013, 5267, 295, 1184, 661, 13, 51548, 51548, 583, 294, 1293, 3331, 11, 3278, 12, 3337, 327, 399, 11, 27479, 819, 4190, 393, 854, 291, 2826, 257, 665, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.08433399369231367, "compression_ratio": 1.853061224489796, "no_speech_prob": 6.240691163839074e-06}, {"id": 99, "seek": 53976, "start": 545.56, "end": 551.56, "text": " corresponded to overfitting and high bias, the right part corresponded to underfitting", "tokens": [50364, 400, 300, 311, 570, 562, 291, 434, 15669, 257, 4314, 295, 26110, 11, 264, 1411, 644, 295, 341, 7605, 50654, 50654, 6805, 292, 281, 670, 69, 2414, 293, 1090, 12577, 11, 264, 558, 644, 6805, 292, 281, 833, 69, 2414, 50954, 50954, 293, 1090, 21977, 11, 9735, 294, 341, 472, 11, 1090, 21977, 390, 322, 264, 1411, 293, 1090, 12577, 390, 51305, 51305, 322, 264, 558, 13, 51355, 51355, 583, 300, 311, 983, 613, 732, 5267, 366, 257, 707, 857, 411, 8013, 5267, 295, 1184, 661, 13, 51548, 51548, 583, 294, 1293, 3331, 11, 3278, 12, 3337, 327, 399, 11, 27479, 819, 4190, 393, 854, 291, 2826, 257, 665, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.08433399369231367, "compression_ratio": 1.853061224489796, "no_speech_prob": 6.240691163839074e-06}, {"id": 100, "seek": 53976, "start": 551.56, "end": 558.58, "text": " and high variance, whereas in this one, high variance was on the left and high bias was", "tokens": [50364, 400, 300, 311, 570, 562, 291, 434, 15669, 257, 4314, 295, 26110, 11, 264, 1411, 644, 295, 341, 7605, 50654, 50654, 6805, 292, 281, 670, 69, 2414, 293, 1090, 12577, 11, 264, 558, 644, 6805, 292, 281, 833, 69, 2414, 50954, 50954, 293, 1090, 21977, 11, 9735, 294, 341, 472, 11, 1090, 21977, 390, 322, 264, 1411, 293, 1090, 12577, 390, 51305, 51305, 322, 264, 558, 13, 51355, 51355, 583, 300, 311, 983, 613, 732, 5267, 366, 257, 707, 857, 411, 8013, 5267, 295, 1184, 661, 13, 51548, 51548, 583, 294, 1293, 3331, 11, 3278, 12, 3337, 327, 399, 11, 27479, 819, 4190, 393, 854, 291, 2826, 257, 665, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.08433399369231367, "compression_ratio": 1.853061224489796, "no_speech_prob": 6.240691163839074e-06}, {"id": 101, "seek": 53976, "start": 558.58, "end": 559.58, "text": " on the right.", "tokens": [50364, 400, 300, 311, 570, 562, 291, 434, 15669, 257, 4314, 295, 26110, 11, 264, 1411, 644, 295, 341, 7605, 50654, 50654, 6805, 292, 281, 670, 69, 2414, 293, 1090, 12577, 11, 264, 558, 644, 6805, 292, 281, 833, 69, 2414, 50954, 50954, 293, 1090, 21977, 11, 9735, 294, 341, 472, 11, 1090, 21977, 390, 322, 264, 1411, 293, 1090, 12577, 390, 51305, 51305, 322, 264, 558, 13, 51355, 51355, 583, 300, 311, 983, 613, 732, 5267, 366, 257, 707, 857, 411, 8013, 5267, 295, 1184, 661, 13, 51548, 51548, 583, 294, 1293, 3331, 11, 3278, 12, 3337, 327, 399, 11, 27479, 819, 4190, 393, 854, 291, 2826, 257, 665, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.08433399369231367, "compression_ratio": 1.853061224489796, "no_speech_prob": 6.240691163839074e-06}, {"id": 102, "seek": 53976, "start": 559.58, "end": 563.4399999999999, "text": " But that's why these two images are a little bit like mirror images of each other.", "tokens": [50364, 400, 300, 311, 570, 562, 291, 434, 15669, 257, 4314, 295, 26110, 11, 264, 1411, 644, 295, 341, 7605, 50654, 50654, 6805, 292, 281, 670, 69, 2414, 293, 1090, 12577, 11, 264, 558, 644, 6805, 292, 281, 833, 69, 2414, 50954, 50954, 293, 1090, 21977, 11, 9735, 294, 341, 472, 11, 1090, 21977, 390, 322, 264, 1411, 293, 1090, 12577, 390, 51305, 51305, 322, 264, 558, 13, 51355, 51355, 583, 300, 311, 983, 613, 732, 5267, 366, 257, 707, 857, 411, 8013, 5267, 295, 1184, 661, 13, 51548, 51548, 583, 294, 1293, 3331, 11, 3278, 12, 3337, 327, 399, 11, 27479, 819, 4190, 393, 854, 291, 2826, 257, 665, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.08433399369231367, "compression_ratio": 1.853061224489796, "no_speech_prob": 6.240691163839074e-06}, {"id": 103, "seek": 53976, "start": 563.4399999999999, "end": 569.12, "text": " But in both cases, cross-validation, evaluating different values can help you choose a good", "tokens": [50364, 400, 300, 311, 570, 562, 291, 434, 15669, 257, 4314, 295, 26110, 11, 264, 1411, 644, 295, 341, 7605, 50654, 50654, 6805, 292, 281, 670, 69, 2414, 293, 1090, 12577, 11, 264, 558, 644, 6805, 292, 281, 833, 69, 2414, 50954, 50954, 293, 1090, 21977, 11, 9735, 294, 341, 472, 11, 1090, 21977, 390, 322, 264, 1411, 293, 1090, 12577, 390, 51305, 51305, 322, 264, 558, 13, 51355, 51355, 583, 300, 311, 983, 613, 732, 5267, 366, 257, 707, 857, 411, 8013, 5267, 295, 1184, 661, 13, 51548, 51548, 583, 294, 1293, 3331, 11, 3278, 12, 3337, 327, 399, 11, 27479, 819, 4190, 393, 854, 291, 2826, 257, 665, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.08433399369231367, "compression_ratio": 1.853061224489796, "no_speech_prob": 6.240691163839074e-06}, {"id": 104, "seek": 56912, "start": 569.12, "end": 572.28, "text": " value of t or a good value of lambda.", "tokens": [50364, 2158, 295, 256, 420, 257, 665, 2158, 295, 13607, 13, 50522, 50522, 407, 300, 311, 577, 264, 3922, 295, 3890, 2144, 13075, 13607, 11807, 264, 12577, 293, 21977, 50771, 50771, 293, 4787, 3389, 295, 428, 9284, 13, 50922, 50922, 400, 291, 600, 611, 1612, 577, 291, 393, 764, 3278, 12, 3337, 327, 399, 281, 652, 257, 665, 3922, 337, 264, 3890, 2144, 51179, 51179, 13075, 13607, 13, 51308, 51308, 823, 370, 1400, 11, 321, 600, 2825, 466, 577, 1419, 257, 1090, 3097, 992, 6713, 11, 1090, 508, 3847, 307, 51615, 51615], "temperature": 0.0, "avg_logprob": -0.12852389838105888, "compression_ratio": 1.7064220183486238, "no_speech_prob": 1.933317207658547e-06}, {"id": 105, "seek": 56912, "start": 572.28, "end": 577.26, "text": " So that's how the choice of regularization parameter lambda affects the bias and variance", "tokens": [50364, 2158, 295, 256, 420, 257, 665, 2158, 295, 13607, 13, 50522, 50522, 407, 300, 311, 577, 264, 3922, 295, 3890, 2144, 13075, 13607, 11807, 264, 12577, 293, 21977, 50771, 50771, 293, 4787, 3389, 295, 428, 9284, 13, 50922, 50922, 400, 291, 600, 611, 1612, 577, 291, 393, 764, 3278, 12, 3337, 327, 399, 281, 652, 257, 665, 3922, 337, 264, 3890, 2144, 51179, 51179, 13075, 13607, 13, 51308, 51308, 823, 370, 1400, 11, 321, 600, 2825, 466, 577, 1419, 257, 1090, 3097, 992, 6713, 11, 1090, 508, 3847, 307, 51615, 51615], "temperature": 0.0, "avg_logprob": -0.12852389838105888, "compression_ratio": 1.7064220183486238, "no_speech_prob": 1.933317207658547e-06}, {"id": 106, "seek": 56912, "start": 577.26, "end": 580.28, "text": " and overall performance of your algorithm.", "tokens": [50364, 2158, 295, 256, 420, 257, 665, 2158, 295, 13607, 13, 50522, 50522, 407, 300, 311, 577, 264, 3922, 295, 3890, 2144, 13075, 13607, 11807, 264, 12577, 293, 21977, 50771, 50771, 293, 4787, 3389, 295, 428, 9284, 13, 50922, 50922, 400, 291, 600, 611, 1612, 577, 291, 393, 764, 3278, 12, 3337, 327, 399, 281, 652, 257, 665, 3922, 337, 264, 3890, 2144, 51179, 51179, 13075, 13607, 13, 51308, 51308, 823, 370, 1400, 11, 321, 600, 2825, 466, 577, 1419, 257, 1090, 3097, 992, 6713, 11, 1090, 508, 3847, 307, 51615, 51615], "temperature": 0.0, "avg_logprob": -0.12852389838105888, "compression_ratio": 1.7064220183486238, "no_speech_prob": 1.933317207658547e-06}, {"id": 107, "seek": 56912, "start": 580.28, "end": 585.42, "text": " And you've also seen how you can use cross-validation to make a good choice for the regularization", "tokens": [50364, 2158, 295, 256, 420, 257, 665, 2158, 295, 13607, 13, 50522, 50522, 407, 300, 311, 577, 264, 3922, 295, 3890, 2144, 13075, 13607, 11807, 264, 12577, 293, 21977, 50771, 50771, 293, 4787, 3389, 295, 428, 9284, 13, 50922, 50922, 400, 291, 600, 611, 1612, 577, 291, 393, 764, 3278, 12, 3337, 327, 399, 281, 652, 257, 665, 3922, 337, 264, 3890, 2144, 51179, 51179, 13075, 13607, 13, 51308, 51308, 823, 370, 1400, 11, 321, 600, 2825, 466, 577, 1419, 257, 1090, 3097, 992, 6713, 11, 1090, 508, 3847, 307, 51615, 51615], "temperature": 0.0, "avg_logprob": -0.12852389838105888, "compression_ratio": 1.7064220183486238, "no_speech_prob": 1.933317207658547e-06}, {"id": 108, "seek": 56912, "start": 585.42, "end": 588.0, "text": " parameter lambda.", "tokens": [50364, 2158, 295, 256, 420, 257, 665, 2158, 295, 13607, 13, 50522, 50522, 407, 300, 311, 577, 264, 3922, 295, 3890, 2144, 13075, 13607, 11807, 264, 12577, 293, 21977, 50771, 50771, 293, 4787, 3389, 295, 428, 9284, 13, 50922, 50922, 400, 291, 600, 611, 1612, 577, 291, 393, 764, 3278, 12, 3337, 327, 399, 281, 652, 257, 665, 3922, 337, 264, 3890, 2144, 51179, 51179, 13075, 13607, 13, 51308, 51308, 823, 370, 1400, 11, 321, 600, 2825, 466, 577, 1419, 257, 1090, 3097, 992, 6713, 11, 1090, 508, 3847, 307, 51615, 51615], "temperature": 0.0, "avg_logprob": -0.12852389838105888, "compression_ratio": 1.7064220183486238, "no_speech_prob": 1.933317207658547e-06}, {"id": 109, "seek": 56912, "start": 588.0, "end": 594.14, "text": " Now so far, we've talked about how having a high training set error, high J train is", "tokens": [50364, 2158, 295, 256, 420, 257, 665, 2158, 295, 13607, 13, 50522, 50522, 407, 300, 311, 577, 264, 3922, 295, 3890, 2144, 13075, 13607, 11807, 264, 12577, 293, 21977, 50771, 50771, 293, 4787, 3389, 295, 428, 9284, 13, 50922, 50922, 400, 291, 600, 611, 1612, 577, 291, 393, 764, 3278, 12, 3337, 327, 399, 281, 652, 257, 665, 3922, 337, 264, 3890, 2144, 51179, 51179, 13075, 13607, 13, 51308, 51308, 823, 370, 1400, 11, 321, 600, 2825, 466, 577, 1419, 257, 1090, 3097, 992, 6713, 11, 1090, 508, 3847, 307, 51615, 51615], "temperature": 0.0, "avg_logprob": -0.12852389838105888, "compression_ratio": 1.7064220183486238, "no_speech_prob": 1.933317207658547e-06}, {"id": 110, "seek": 59414, "start": 594.14, "end": 600.3199999999999, "text": " indicative of high bias and how having a high cross-validation error, JCV, specifically", "tokens": [50364, 47513, 295, 1090, 12577, 293, 577, 1419, 257, 1090, 3278, 12, 3337, 327, 399, 6713, 11, 49802, 53, 11, 4682, 50673, 50673, 498, 309, 311, 709, 2946, 813, 508, 3847, 11, 577, 300, 311, 47513, 295, 257, 21977, 1154, 13, 51004, 51004, 583, 437, 360, 613, 2283, 1090, 420, 709, 2946, 767, 914, 30, 51209, 51209, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 689, 321, 603, 574, 412, 577, 291, 393, 574, 412, 264, 51389, 51389, 3547, 508, 3847, 293, 49802, 53, 293, 6995, 498, 309, 311, 1090, 420, 2295, 13, 51631, 51631], "temperature": 0.0, "avg_logprob": -0.126412555424854, "compression_ratio": 1.6502242152466369, "no_speech_prob": 4.0293680285685696e-06}, {"id": 111, "seek": 59414, "start": 600.3199999999999, "end": 606.9399999999999, "text": " if it's much higher than J train, how that's indicative of a variance problem.", "tokens": [50364, 47513, 295, 1090, 12577, 293, 577, 1419, 257, 1090, 3278, 12, 3337, 327, 399, 6713, 11, 49802, 53, 11, 4682, 50673, 50673, 498, 309, 311, 709, 2946, 813, 508, 3847, 11, 577, 300, 311, 47513, 295, 257, 21977, 1154, 13, 51004, 51004, 583, 437, 360, 613, 2283, 1090, 420, 709, 2946, 767, 914, 30, 51209, 51209, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 689, 321, 603, 574, 412, 577, 291, 393, 574, 412, 264, 51389, 51389, 3547, 508, 3847, 293, 49802, 53, 293, 6995, 498, 309, 311, 1090, 420, 2295, 13, 51631, 51631], "temperature": 0.0, "avg_logprob": -0.126412555424854, "compression_ratio": 1.6502242152466369, "no_speech_prob": 4.0293680285685696e-06}, {"id": 112, "seek": 59414, "start": 606.9399999999999, "end": 611.04, "text": " But what do these words high or much higher actually mean?", "tokens": [50364, 47513, 295, 1090, 12577, 293, 577, 1419, 257, 1090, 3278, 12, 3337, 327, 399, 6713, 11, 49802, 53, 11, 4682, 50673, 50673, 498, 309, 311, 709, 2946, 813, 508, 3847, 11, 577, 300, 311, 47513, 295, 257, 21977, 1154, 13, 51004, 51004, 583, 437, 360, 613, 2283, 1090, 420, 709, 2946, 767, 914, 30, 51209, 51209, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 689, 321, 603, 574, 412, 577, 291, 393, 574, 412, 264, 51389, 51389, 3547, 508, 3847, 293, 49802, 53, 293, 6995, 498, 309, 311, 1090, 420, 2295, 13, 51631, 51631], "temperature": 0.0, "avg_logprob": -0.126412555424854, "compression_ratio": 1.6502242152466369, "no_speech_prob": 4.0293680285685696e-06}, {"id": 113, "seek": 59414, "start": 611.04, "end": 614.64, "text": " Let's take a look at that in the next video where we'll look at how you can look at the", "tokens": [50364, 47513, 295, 1090, 12577, 293, 577, 1419, 257, 1090, 3278, 12, 3337, 327, 399, 6713, 11, 49802, 53, 11, 4682, 50673, 50673, 498, 309, 311, 709, 2946, 813, 508, 3847, 11, 577, 300, 311, 47513, 295, 257, 21977, 1154, 13, 51004, 51004, 583, 437, 360, 613, 2283, 1090, 420, 709, 2946, 767, 914, 30, 51209, 51209, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 689, 321, 603, 574, 412, 577, 291, 393, 574, 412, 264, 51389, 51389, 3547, 508, 3847, 293, 49802, 53, 293, 6995, 498, 309, 311, 1090, 420, 2295, 13, 51631, 51631], "temperature": 0.0, "avg_logprob": -0.126412555424854, "compression_ratio": 1.6502242152466369, "no_speech_prob": 4.0293680285685696e-06}, {"id": 114, "seek": 59414, "start": 614.64, "end": 619.48, "text": " numbers J train and JCV and judge if it's high or low.", "tokens": [50364, 47513, 295, 1090, 12577, 293, 577, 1419, 257, 1090, 3278, 12, 3337, 327, 399, 6713, 11, 49802, 53, 11, 4682, 50673, 50673, 498, 309, 311, 709, 2946, 813, 508, 3847, 11, 577, 300, 311, 47513, 295, 257, 21977, 1154, 13, 51004, 51004, 583, 437, 360, 613, 2283, 1090, 420, 709, 2946, 767, 914, 30, 51209, 51209, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 689, 321, 603, 574, 412, 577, 291, 393, 574, 412, 264, 51389, 51389, 3547, 508, 3847, 293, 49802, 53, 293, 6995, 498, 309, 311, 1090, 420, 2295, 13, 51631, 51631], "temperature": 0.0, "avg_logprob": -0.126412555424854, "compression_ratio": 1.6502242152466369, "no_speech_prob": 4.0293680285685696e-06}, {"id": 115, "seek": 61948, "start": 619.48, "end": 624.24, "text": " And it turns out that one further refinement of these ideas, that is establishing a baseline", "tokens": [50364, 400, 309, 4523, 484, 300, 472, 3052, 1895, 30229, 295, 613, 3487, 11, 300, 307, 22494, 257, 20518, 50602, 50602, 1496, 295, 3389, 337, 428, 2539, 9284, 11, 486, 652, 309, 709, 3571, 337, 291, 281, 574, 412, 50784, 50784, 613, 3547, 508, 3847, 11, 49802, 53, 293, 6995, 498, 436, 434, 1090, 420, 2295, 13, 51044, 51044, 961, 311, 747, 257, 574, 412, 437, 439, 341, 1355, 294, 264, 958, 960, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1428464964816445, "compression_ratio": 1.4902912621359223, "no_speech_prob": 2.821697671606671e-05}, {"id": 116, "seek": 61948, "start": 624.24, "end": 627.88, "text": " level of performance for your learning algorithm, will make it much easier for you to look at", "tokens": [50364, 400, 309, 4523, 484, 300, 472, 3052, 1895, 30229, 295, 613, 3487, 11, 300, 307, 22494, 257, 20518, 50602, 50602, 1496, 295, 3389, 337, 428, 2539, 9284, 11, 486, 652, 309, 709, 3571, 337, 291, 281, 574, 412, 50784, 50784, 613, 3547, 508, 3847, 11, 49802, 53, 293, 6995, 498, 436, 434, 1090, 420, 2295, 13, 51044, 51044, 961, 311, 747, 257, 574, 412, 437, 439, 341, 1355, 294, 264, 958, 960, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1428464964816445, "compression_ratio": 1.4902912621359223, "no_speech_prob": 2.821697671606671e-05}, {"id": 117, "seek": 61948, "start": 627.88, "end": 633.08, "text": " these numbers J train, JCV and judge if they're high or low.", "tokens": [50364, 400, 309, 4523, 484, 300, 472, 3052, 1895, 30229, 295, 613, 3487, 11, 300, 307, 22494, 257, 20518, 50602, 50602, 1496, 295, 3389, 337, 428, 2539, 9284, 11, 486, 652, 309, 709, 3571, 337, 291, 281, 574, 412, 50784, 50784, 613, 3547, 508, 3847, 11, 49802, 53, 293, 6995, 498, 436, 434, 1090, 420, 2295, 13, 51044, 51044, 961, 311, 747, 257, 574, 412, 437, 439, 341, 1355, 294, 264, 958, 960, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1428464964816445, "compression_ratio": 1.4902912621359223, "no_speech_prob": 2.821697671606671e-05}, {"id": 118, "seek": 63308, "start": 633.08, "end": 650.0400000000001, "text": " Let's take a look at what all this means in the next video.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 437, 439, 341, 1355, 294, 264, 958, 960, 13, 51212], "temperature": 0.0, "avg_logprob": -0.28724561797247994, "compression_ratio": 0.9365079365079365, "no_speech_prob": 5.457442603074014e-05}], "language": "en", "video_id": "2Ji4Upc606c", "entity": "ML Specialization, Andrew Ng (2022)"}}