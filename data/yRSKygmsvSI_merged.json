{"video_id": "yRSKygmsvSI", "title": "3.10 Regularization to Reduce Overfitting | Regularized linear regression-- [ML | Andrew Ng]", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 532, "views": 144, "publish_date": "11/04/2022", "timestamp": 1661299200, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " In this video, we'll figure out how to get gradient descent to work with regularized linear regression. Let's jump in. Here's the cost function we've come up with in the last video for regularized linear regression. The first part is the usual squared error cost function. And now you have this additional regularization term, where lambda is the regularization parameter. And you'd like to find parameters w and b that minimize the regularized cost function. Previously, we were using gradient descent for the original cost function, just the first term, before we added that second regularization term. And previously, we had the following gradient descent algorithm, which is that we repeatedly update the parameters wj and b for j equals 1 through n according to this formula, and b is also updated similarly. Again, alpha is a very small positive number called the learning rate. In fact, the updates for regularized linear regression look exactly the same, except that now the cost j is defined a bit differently. Previously, the derivative of j with respect to wj was given by this expression over here, and the derivative with respect to b was given by this expression over here. Now that we've added this additional regularization term, the only thing that changes is that the expression for the derivative with respect to wj ends up with one additional term. This plus lambda over m times wj. And in particular, for the new definition of the cost function j, these two expressions over here, these are the new derivatives of j with respect to wj, and the derivative of j with respect to b. Recall that we don't regularize b, so we're not trying to shrink b. That's why the update to b remains the same as before, whereas the update to w changes because the regularization term causes us to try to shrink wj. So let's take these definitions for the derivatives and put them back into the expression on the left to write out the gradient descent algorithm for regularized linear regression. So to implement gradient descent for regularized linear regression, this is what you would have your code do. Here's the update for wj for j equals 1 through n, and here's the update for b, and as usual, please remember to carry out simultaneous updates for all of these parameters. Now in order for you to get this algorithm to work, this is all you need to know. But what I'd like to do in the remainder of this video is to go over some optional material to convey a slightly deeper intuition about what this formula is actually doing, as well as chat briefly about how these derivatives are derived. The rest of this video is completely optional. It's completely okay if you skip the rest of this video, but if you have a strong interest in math, then stick with me. It's always nice to hang out with you here, and through these equations, perhaps you can build a deeper intuition about what the math and what the derivatives are doing as well. So let's take a look. Let's look at the update rule for wj and rewrite it in another way. We're updating wj as 1 times wj minus alpha times lambda over m times wj. So I've moved the term from the end to the front here, and then minus alpha times 1 over m, and then the rest of that term over there. So we just rearrange the terms a little bit. And if we simplify, then we're seeing that wj is updated as wj times 1 minus alpha times lambda over m minus alpha times this other term over here. And you might recognize this second term as the usual gradient descent update for unregularized linear regression. This is the update for linear regression before we had regularization, and this is the term we saw in week two of this course. And so the only change when you add regularization is that instead of wj being set to be equal to wj minus alpha times this term is now w times this number minus the usual update. So this is what we had in week one of this course. So what is this first term over here? Well, alpha is a very small positive number, say 0.01. Lambda is usually a small number, say 1 or maybe 10. Let's say lambda is 1 for this example, and m is the training set size, say 50. And so when you multiply alpha lambda over m, say 0.01 times 1 divided by 50, this term ends up being a small positive number, say 0.0002. And thus, 1 minus alpha lambda over m is going to be a number just slightly less than 1. In this case, 0.9998. And so the effect of this term is that on every single iteration of gradient descent, you're taking wj and multiplying it by 0.9998. That is by some numbers slightly less than 1 before carrying out the usual update. So what regularization is doing on every single iteration is you're multiplying w by a number slightly less than 1, and that has the effect of shrinking the value of wj just a little bit. So this gives us another view on why regularization has the effect of shrinking the parameters wj a little bit on every iteration. And so that's how regularization works. If you're curious about how these derivative terms were computed, I have just one last optional slide that goes through just a little bit of the calculation of the derivative term. Again, this slide and the rest of this video are completely optional, meaning you won't need any of this to do the practice labs and the quizzes. So let's step through quickly the derivative calculation. The derivative of j with respect to wj looks like this. Recall that f of x for linear regression is defined as w dot x plus b or w dot product x plus b. And it turns out that by the rules of calculus, the derivatives look like this is 1 over 2m times the sum i equals 1 through m of w dot x plus b minus y times 2xj plus the derivative of the regularization term, which is lambda over 2m times 2wj. Notice that the second term does not have the summation term from j equals 1 through n anymore. The twos cancel out here and here and also here and here. And so it simplifies to this expression over here. And finally, remember that wx plus b is f of x. And so you can rewrite it as this expression down here. So this is why this expression is used to compute the gradient in regularized linear regression. So you now know how to implement regularized linear regression. Using this, you will reduce overfitting when you have a lot of features and relatively small training set. And this should let you get linear regression to work much better on many problems. In the next video, we'll take this regularization idea and apply it to logistic regression to avoid overfitting for logistic regression as well. Let's take a look at that in the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.140000000000001, "text": " In this video, we'll figure out how to get gradient descent to work with regularized", "tokens": [50364, 682, 341, 960, 11, 321, 603, 2573, 484, 577, 281, 483, 16235, 23475, 281, 589, 365, 3890, 1602, 50671, 50671, 8213, 24590, 13, 50721, 50721, 961, 311, 3012, 294, 13, 50771, 50771, 1692, 311, 264, 2063, 2445, 321, 600, 808, 493, 365, 294, 264, 1036, 960, 337, 3890, 1602, 8213, 24590, 13, 51097, 51097, 440, 700, 644, 307, 264, 7713, 8889, 6713, 2063, 2445, 13, 51306, 51306, 400, 586, 291, 362, 341, 4497, 3890, 2144, 1433, 11, 689, 13607, 307, 264, 3890, 2144, 13075, 13, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.1379989559730787, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.00297628459520638}, {"id": 1, "seek": 0, "start": 6.140000000000001, "end": 7.140000000000001, "text": " linear regression.", "tokens": [50364, 682, 341, 960, 11, 321, 603, 2573, 484, 577, 281, 483, 16235, 23475, 281, 589, 365, 3890, 1602, 50671, 50671, 8213, 24590, 13, 50721, 50721, 961, 311, 3012, 294, 13, 50771, 50771, 1692, 311, 264, 2063, 2445, 321, 600, 808, 493, 365, 294, 264, 1036, 960, 337, 3890, 1602, 8213, 24590, 13, 51097, 51097, 440, 700, 644, 307, 264, 7713, 8889, 6713, 2063, 2445, 13, 51306, 51306, 400, 586, 291, 362, 341, 4497, 3890, 2144, 1433, 11, 689, 13607, 307, 264, 3890, 2144, 13075, 13, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.1379989559730787, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.00297628459520638}, {"id": 2, "seek": 0, "start": 7.140000000000001, "end": 8.14, "text": " Let's jump in.", "tokens": [50364, 682, 341, 960, 11, 321, 603, 2573, 484, 577, 281, 483, 16235, 23475, 281, 589, 365, 3890, 1602, 50671, 50671, 8213, 24590, 13, 50721, 50721, 961, 311, 3012, 294, 13, 50771, 50771, 1692, 311, 264, 2063, 2445, 321, 600, 808, 493, 365, 294, 264, 1036, 960, 337, 3890, 1602, 8213, 24590, 13, 51097, 51097, 440, 700, 644, 307, 264, 7713, 8889, 6713, 2063, 2445, 13, 51306, 51306, 400, 586, 291, 362, 341, 4497, 3890, 2144, 1433, 11, 689, 13607, 307, 264, 3890, 2144, 13075, 13, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.1379989559730787, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.00297628459520638}, {"id": 3, "seek": 0, "start": 8.14, "end": 14.66, "text": " Here's the cost function we've come up with in the last video for regularized linear regression.", "tokens": [50364, 682, 341, 960, 11, 321, 603, 2573, 484, 577, 281, 483, 16235, 23475, 281, 589, 365, 3890, 1602, 50671, 50671, 8213, 24590, 13, 50721, 50721, 961, 311, 3012, 294, 13, 50771, 50771, 1692, 311, 264, 2063, 2445, 321, 600, 808, 493, 365, 294, 264, 1036, 960, 337, 3890, 1602, 8213, 24590, 13, 51097, 51097, 440, 700, 644, 307, 264, 7713, 8889, 6713, 2063, 2445, 13, 51306, 51306, 400, 586, 291, 362, 341, 4497, 3890, 2144, 1433, 11, 689, 13607, 307, 264, 3890, 2144, 13075, 13, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.1379989559730787, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.00297628459520638}, {"id": 4, "seek": 0, "start": 14.66, "end": 18.84, "text": " The first part is the usual squared error cost function.", "tokens": [50364, 682, 341, 960, 11, 321, 603, 2573, 484, 577, 281, 483, 16235, 23475, 281, 589, 365, 3890, 1602, 50671, 50671, 8213, 24590, 13, 50721, 50721, 961, 311, 3012, 294, 13, 50771, 50771, 1692, 311, 264, 2063, 2445, 321, 600, 808, 493, 365, 294, 264, 1036, 960, 337, 3890, 1602, 8213, 24590, 13, 51097, 51097, 440, 700, 644, 307, 264, 7713, 8889, 6713, 2063, 2445, 13, 51306, 51306, 400, 586, 291, 362, 341, 4497, 3890, 2144, 1433, 11, 689, 13607, 307, 264, 3890, 2144, 13075, 13, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.1379989559730787, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.00297628459520638}, {"id": 5, "seek": 0, "start": 18.84, "end": 25.68, "text": " And now you have this additional regularization term, where lambda is the regularization parameter.", "tokens": [50364, 682, 341, 960, 11, 321, 603, 2573, 484, 577, 281, 483, 16235, 23475, 281, 589, 365, 3890, 1602, 50671, 50671, 8213, 24590, 13, 50721, 50721, 961, 311, 3012, 294, 13, 50771, 50771, 1692, 311, 264, 2063, 2445, 321, 600, 808, 493, 365, 294, 264, 1036, 960, 337, 3890, 1602, 8213, 24590, 13, 51097, 51097, 440, 700, 644, 307, 264, 7713, 8889, 6713, 2063, 2445, 13, 51306, 51306, 400, 586, 291, 362, 341, 4497, 3890, 2144, 1433, 11, 689, 13607, 307, 264, 3890, 2144, 13075, 13, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.1379989559730787, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.00297628459520638}, {"id": 6, "seek": 2568, "start": 25.68, "end": 32.48, "text": " And you'd like to find parameters w and b that minimize the regularized cost function.", "tokens": [50364, 400, 291, 1116, 411, 281, 915, 9834, 261, 293, 272, 300, 17522, 264, 3890, 1602, 2063, 2445, 13, 50704, 50704, 33606, 11, 321, 645, 1228, 16235, 23475, 337, 264, 3380, 2063, 2445, 11, 445, 264, 700, 51016, 51016, 1433, 11, 949, 321, 3869, 300, 1150, 3890, 2144, 1433, 13, 51252, 51252, 400, 8046, 11, 321, 632, 264, 3480, 16235, 23475, 9284, 11, 597, 307, 300, 321, 18227, 51544, 51544], "temperature": 0.0, "avg_logprob": -0.12246557692406883, "compression_ratio": 1.653061224489796, "no_speech_prob": 9.2233058239799e-06}, {"id": 7, "seek": 2568, "start": 32.48, "end": 38.72, "text": " Previously, we were using gradient descent for the original cost function, just the first", "tokens": [50364, 400, 291, 1116, 411, 281, 915, 9834, 261, 293, 272, 300, 17522, 264, 3890, 1602, 2063, 2445, 13, 50704, 50704, 33606, 11, 321, 645, 1228, 16235, 23475, 337, 264, 3380, 2063, 2445, 11, 445, 264, 700, 51016, 51016, 1433, 11, 949, 321, 3869, 300, 1150, 3890, 2144, 1433, 13, 51252, 51252, 400, 8046, 11, 321, 632, 264, 3480, 16235, 23475, 9284, 11, 597, 307, 300, 321, 18227, 51544, 51544], "temperature": 0.0, "avg_logprob": -0.12246557692406883, "compression_ratio": 1.653061224489796, "no_speech_prob": 9.2233058239799e-06}, {"id": 8, "seek": 2568, "start": 38.72, "end": 43.44, "text": " term, before we added that second regularization term.", "tokens": [50364, 400, 291, 1116, 411, 281, 915, 9834, 261, 293, 272, 300, 17522, 264, 3890, 1602, 2063, 2445, 13, 50704, 50704, 33606, 11, 321, 645, 1228, 16235, 23475, 337, 264, 3380, 2063, 2445, 11, 445, 264, 700, 51016, 51016, 1433, 11, 949, 321, 3869, 300, 1150, 3890, 2144, 1433, 13, 51252, 51252, 400, 8046, 11, 321, 632, 264, 3480, 16235, 23475, 9284, 11, 597, 307, 300, 321, 18227, 51544, 51544], "temperature": 0.0, "avg_logprob": -0.12246557692406883, "compression_ratio": 1.653061224489796, "no_speech_prob": 9.2233058239799e-06}, {"id": 9, "seek": 2568, "start": 43.44, "end": 49.28, "text": " And previously, we had the following gradient descent algorithm, which is that we repeatedly", "tokens": [50364, 400, 291, 1116, 411, 281, 915, 9834, 261, 293, 272, 300, 17522, 264, 3890, 1602, 2063, 2445, 13, 50704, 50704, 33606, 11, 321, 645, 1228, 16235, 23475, 337, 264, 3380, 2063, 2445, 11, 445, 264, 700, 51016, 51016, 1433, 11, 949, 321, 3869, 300, 1150, 3890, 2144, 1433, 13, 51252, 51252, 400, 8046, 11, 321, 632, 264, 3480, 16235, 23475, 9284, 11, 597, 307, 300, 321, 18227, 51544, 51544], "temperature": 0.0, "avg_logprob": -0.12246557692406883, "compression_ratio": 1.653061224489796, "no_speech_prob": 9.2233058239799e-06}, {"id": 10, "seek": 4928, "start": 49.28, "end": 56.4, "text": " update the parameters wj and b for j equals 1 through n according to this formula, and", "tokens": [50364, 5623, 264, 9834, 261, 73, 293, 272, 337, 361, 6915, 502, 807, 297, 4650, 281, 341, 8513, 11, 293, 50720, 50720, 272, 307, 611, 10588, 14138, 13, 50842, 50842, 3764, 11, 8961, 307, 257, 588, 1359, 3353, 1230, 1219, 264, 2539, 3314, 13, 51114, 51114, 682, 1186, 11, 264, 9205, 337, 3890, 1602, 8213, 24590, 574, 2293, 264, 912, 11, 3993, 300, 51374, 51374, 586, 264, 2063, 361, 307, 7642, 257, 857, 7614, 13, 51590, 51590], "temperature": 0.0, "avg_logprob": -0.1851185162862142, "compression_ratio": 1.5358851674641147, "no_speech_prob": 4.2227929952787235e-06}, {"id": 11, "seek": 4928, "start": 56.4, "end": 58.84, "text": " b is also updated similarly.", "tokens": [50364, 5623, 264, 9834, 261, 73, 293, 272, 337, 361, 6915, 502, 807, 297, 4650, 281, 341, 8513, 11, 293, 50720, 50720, 272, 307, 611, 10588, 14138, 13, 50842, 50842, 3764, 11, 8961, 307, 257, 588, 1359, 3353, 1230, 1219, 264, 2539, 3314, 13, 51114, 51114, 682, 1186, 11, 264, 9205, 337, 3890, 1602, 8213, 24590, 574, 2293, 264, 912, 11, 3993, 300, 51374, 51374, 586, 264, 2063, 361, 307, 7642, 257, 857, 7614, 13, 51590, 51590], "temperature": 0.0, "avg_logprob": -0.1851185162862142, "compression_ratio": 1.5358851674641147, "no_speech_prob": 4.2227929952787235e-06}, {"id": 12, "seek": 4928, "start": 58.84, "end": 64.28, "text": " Again, alpha is a very small positive number called the learning rate.", "tokens": [50364, 5623, 264, 9834, 261, 73, 293, 272, 337, 361, 6915, 502, 807, 297, 4650, 281, 341, 8513, 11, 293, 50720, 50720, 272, 307, 611, 10588, 14138, 13, 50842, 50842, 3764, 11, 8961, 307, 257, 588, 1359, 3353, 1230, 1219, 264, 2539, 3314, 13, 51114, 51114, 682, 1186, 11, 264, 9205, 337, 3890, 1602, 8213, 24590, 574, 2293, 264, 912, 11, 3993, 300, 51374, 51374, 586, 264, 2063, 361, 307, 7642, 257, 857, 7614, 13, 51590, 51590], "temperature": 0.0, "avg_logprob": -0.1851185162862142, "compression_ratio": 1.5358851674641147, "no_speech_prob": 4.2227929952787235e-06}, {"id": 13, "seek": 4928, "start": 64.28, "end": 69.48, "text": " In fact, the updates for regularized linear regression look exactly the same, except that", "tokens": [50364, 5623, 264, 9834, 261, 73, 293, 272, 337, 361, 6915, 502, 807, 297, 4650, 281, 341, 8513, 11, 293, 50720, 50720, 272, 307, 611, 10588, 14138, 13, 50842, 50842, 3764, 11, 8961, 307, 257, 588, 1359, 3353, 1230, 1219, 264, 2539, 3314, 13, 51114, 51114, 682, 1186, 11, 264, 9205, 337, 3890, 1602, 8213, 24590, 574, 2293, 264, 912, 11, 3993, 300, 51374, 51374, 586, 264, 2063, 361, 307, 7642, 257, 857, 7614, 13, 51590, 51590], "temperature": 0.0, "avg_logprob": -0.1851185162862142, "compression_ratio": 1.5358851674641147, "no_speech_prob": 4.2227929952787235e-06}, {"id": 14, "seek": 4928, "start": 69.48, "end": 73.8, "text": " now the cost j is defined a bit differently.", "tokens": [50364, 5623, 264, 9834, 261, 73, 293, 272, 337, 361, 6915, 502, 807, 297, 4650, 281, 341, 8513, 11, 293, 50720, 50720, 272, 307, 611, 10588, 14138, 13, 50842, 50842, 3764, 11, 8961, 307, 257, 588, 1359, 3353, 1230, 1219, 264, 2539, 3314, 13, 51114, 51114, 682, 1186, 11, 264, 9205, 337, 3890, 1602, 8213, 24590, 574, 2293, 264, 912, 11, 3993, 300, 51374, 51374, 586, 264, 2063, 361, 307, 7642, 257, 857, 7614, 13, 51590, 51590], "temperature": 0.0, "avg_logprob": -0.1851185162862142, "compression_ratio": 1.5358851674641147, "no_speech_prob": 4.2227929952787235e-06}, {"id": 15, "seek": 7380, "start": 73.8, "end": 81.39999999999999, "text": " Previously, the derivative of j with respect to wj was given by this expression over here,", "tokens": [50364, 33606, 11, 264, 13760, 295, 361, 365, 3104, 281, 261, 73, 390, 2212, 538, 341, 6114, 670, 510, 11, 50744, 50744, 293, 264, 13760, 365, 3104, 281, 272, 390, 2212, 538, 341, 6114, 670, 510, 13, 51042, 51042, 823, 300, 321, 600, 3869, 341, 4497, 3890, 2144, 1433, 11, 264, 787, 551, 300, 2962, 307, 300, 51322, 51322, 264, 6114, 337, 264, 13760, 365, 3104, 281, 261, 73, 5314, 493, 365, 472, 4497, 1433, 13, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.09302602840375297, "compression_ratio": 2.0350877192982457, "no_speech_prob": 2.7264397886028746e-06}, {"id": 16, "seek": 7380, "start": 81.39999999999999, "end": 87.36, "text": " and the derivative with respect to b was given by this expression over here.", "tokens": [50364, 33606, 11, 264, 13760, 295, 361, 365, 3104, 281, 261, 73, 390, 2212, 538, 341, 6114, 670, 510, 11, 50744, 50744, 293, 264, 13760, 365, 3104, 281, 272, 390, 2212, 538, 341, 6114, 670, 510, 13, 51042, 51042, 823, 300, 321, 600, 3869, 341, 4497, 3890, 2144, 1433, 11, 264, 787, 551, 300, 2962, 307, 300, 51322, 51322, 264, 6114, 337, 264, 13760, 365, 3104, 281, 261, 73, 5314, 493, 365, 472, 4497, 1433, 13, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.09302602840375297, "compression_ratio": 2.0350877192982457, "no_speech_prob": 2.7264397886028746e-06}, {"id": 17, "seek": 7380, "start": 87.36, "end": 92.96, "text": " Now that we've added this additional regularization term, the only thing that changes is that", "tokens": [50364, 33606, 11, 264, 13760, 295, 361, 365, 3104, 281, 261, 73, 390, 2212, 538, 341, 6114, 670, 510, 11, 50744, 50744, 293, 264, 13760, 365, 3104, 281, 272, 390, 2212, 538, 341, 6114, 670, 510, 13, 51042, 51042, 823, 300, 321, 600, 3869, 341, 4497, 3890, 2144, 1433, 11, 264, 787, 551, 300, 2962, 307, 300, 51322, 51322, 264, 6114, 337, 264, 13760, 365, 3104, 281, 261, 73, 5314, 493, 365, 472, 4497, 1433, 13, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.09302602840375297, "compression_ratio": 2.0350877192982457, "no_speech_prob": 2.7264397886028746e-06}, {"id": 18, "seek": 7380, "start": 92.96, "end": 99.24, "text": " the expression for the derivative with respect to wj ends up with one additional term.", "tokens": [50364, 33606, 11, 264, 13760, 295, 361, 365, 3104, 281, 261, 73, 390, 2212, 538, 341, 6114, 670, 510, 11, 50744, 50744, 293, 264, 13760, 365, 3104, 281, 272, 390, 2212, 538, 341, 6114, 670, 510, 13, 51042, 51042, 823, 300, 321, 600, 3869, 341, 4497, 3890, 2144, 1433, 11, 264, 787, 551, 300, 2962, 307, 300, 51322, 51322, 264, 6114, 337, 264, 13760, 365, 3104, 281, 261, 73, 5314, 493, 365, 472, 4497, 1433, 13, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.09302602840375297, "compression_ratio": 2.0350877192982457, "no_speech_prob": 2.7264397886028746e-06}, {"id": 19, "seek": 9924, "start": 99.24, "end": 104.36, "text": " This plus lambda over m times wj.", "tokens": [50364, 639, 1804, 13607, 670, 275, 1413, 261, 73, 13, 50620, 50620, 400, 294, 1729, 11, 337, 264, 777, 7123, 295, 264, 2063, 2445, 361, 11, 613, 732, 15277, 50872, 50872, 670, 510, 11, 613, 366, 264, 777, 33733, 295, 361, 365, 3104, 281, 261, 73, 11, 293, 264, 13760, 295, 51240, 51240, 361, 365, 3104, 281, 272, 13, 51360, 51360, 9647, 336, 300, 321, 500, 380, 3890, 1125, 272, 11, 370, 321, 434, 406, 1382, 281, 23060, 272, 13, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.09385855801134224, "compression_ratio": 1.5935828877005347, "no_speech_prob": 5.955055712547619e-06}, {"id": 20, "seek": 9924, "start": 104.36, "end": 109.39999999999999, "text": " And in particular, for the new definition of the cost function j, these two expressions", "tokens": [50364, 639, 1804, 13607, 670, 275, 1413, 261, 73, 13, 50620, 50620, 400, 294, 1729, 11, 337, 264, 777, 7123, 295, 264, 2063, 2445, 361, 11, 613, 732, 15277, 50872, 50872, 670, 510, 11, 613, 366, 264, 777, 33733, 295, 361, 365, 3104, 281, 261, 73, 11, 293, 264, 13760, 295, 51240, 51240, 361, 365, 3104, 281, 272, 13, 51360, 51360, 9647, 336, 300, 321, 500, 380, 3890, 1125, 272, 11, 370, 321, 434, 406, 1382, 281, 23060, 272, 13, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.09385855801134224, "compression_ratio": 1.5935828877005347, "no_speech_prob": 5.955055712547619e-06}, {"id": 21, "seek": 9924, "start": 109.39999999999999, "end": 116.75999999999999, "text": " over here, these are the new derivatives of j with respect to wj, and the derivative of", "tokens": [50364, 639, 1804, 13607, 670, 275, 1413, 261, 73, 13, 50620, 50620, 400, 294, 1729, 11, 337, 264, 777, 7123, 295, 264, 2063, 2445, 361, 11, 613, 732, 15277, 50872, 50872, 670, 510, 11, 613, 366, 264, 777, 33733, 295, 361, 365, 3104, 281, 261, 73, 11, 293, 264, 13760, 295, 51240, 51240, 361, 365, 3104, 281, 272, 13, 51360, 51360, 9647, 336, 300, 321, 500, 380, 3890, 1125, 272, 11, 370, 321, 434, 406, 1382, 281, 23060, 272, 13, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.09385855801134224, "compression_ratio": 1.5935828877005347, "no_speech_prob": 5.955055712547619e-06}, {"id": 22, "seek": 9924, "start": 116.75999999999999, "end": 119.16, "text": " j with respect to b.", "tokens": [50364, 639, 1804, 13607, 670, 275, 1413, 261, 73, 13, 50620, 50620, 400, 294, 1729, 11, 337, 264, 777, 7123, 295, 264, 2063, 2445, 361, 11, 613, 732, 15277, 50872, 50872, 670, 510, 11, 613, 366, 264, 777, 33733, 295, 361, 365, 3104, 281, 261, 73, 11, 293, 264, 13760, 295, 51240, 51240, 361, 365, 3104, 281, 272, 13, 51360, 51360, 9647, 336, 300, 321, 500, 380, 3890, 1125, 272, 11, 370, 321, 434, 406, 1382, 281, 23060, 272, 13, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.09385855801134224, "compression_ratio": 1.5935828877005347, "no_speech_prob": 5.955055712547619e-06}, {"id": 23, "seek": 9924, "start": 119.16, "end": 124.56, "text": " Recall that we don't regularize b, so we're not trying to shrink b.", "tokens": [50364, 639, 1804, 13607, 670, 275, 1413, 261, 73, 13, 50620, 50620, 400, 294, 1729, 11, 337, 264, 777, 7123, 295, 264, 2063, 2445, 361, 11, 613, 732, 15277, 50872, 50872, 670, 510, 11, 613, 366, 264, 777, 33733, 295, 361, 365, 3104, 281, 261, 73, 11, 293, 264, 13760, 295, 51240, 51240, 361, 365, 3104, 281, 272, 13, 51360, 51360, 9647, 336, 300, 321, 500, 380, 3890, 1125, 272, 11, 370, 321, 434, 406, 1382, 281, 23060, 272, 13, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.09385855801134224, "compression_ratio": 1.5935828877005347, "no_speech_prob": 5.955055712547619e-06}, {"id": 24, "seek": 12456, "start": 124.56, "end": 130.22, "text": " That's why the update to b remains the same as before, whereas the update to w changes", "tokens": [50364, 663, 311, 983, 264, 5623, 281, 272, 7023, 264, 912, 382, 949, 11, 9735, 264, 5623, 281, 261, 2962, 50647, 50647, 570, 264, 3890, 2144, 1433, 7700, 505, 281, 853, 281, 23060, 261, 73, 13, 50976, 50976, 407, 718, 311, 747, 613, 21988, 337, 264, 33733, 293, 829, 552, 646, 666, 264, 6114, 322, 264, 51240, 51240, 1411, 281, 2464, 484, 264, 16235, 23475, 9284, 337, 3890, 1602, 8213, 24590, 13, 51593, 51593], "temperature": 0.0, "avg_logprob": -0.08773020426432292, "compression_ratio": 1.6417910447761195, "no_speech_prob": 5.453267704069731e-07}, {"id": 25, "seek": 12456, "start": 130.22, "end": 136.8, "text": " because the regularization term causes us to try to shrink wj.", "tokens": [50364, 663, 311, 983, 264, 5623, 281, 272, 7023, 264, 912, 382, 949, 11, 9735, 264, 5623, 281, 261, 2962, 50647, 50647, 570, 264, 3890, 2144, 1433, 7700, 505, 281, 853, 281, 23060, 261, 73, 13, 50976, 50976, 407, 718, 311, 747, 613, 21988, 337, 264, 33733, 293, 829, 552, 646, 666, 264, 6114, 322, 264, 51240, 51240, 1411, 281, 2464, 484, 264, 16235, 23475, 9284, 337, 3890, 1602, 8213, 24590, 13, 51593, 51593], "temperature": 0.0, "avg_logprob": -0.08773020426432292, "compression_ratio": 1.6417910447761195, "no_speech_prob": 5.453267704069731e-07}, {"id": 26, "seek": 12456, "start": 136.8, "end": 142.08, "text": " So let's take these definitions for the derivatives and put them back into the expression on the", "tokens": [50364, 663, 311, 983, 264, 5623, 281, 272, 7023, 264, 912, 382, 949, 11, 9735, 264, 5623, 281, 261, 2962, 50647, 50647, 570, 264, 3890, 2144, 1433, 7700, 505, 281, 853, 281, 23060, 261, 73, 13, 50976, 50976, 407, 718, 311, 747, 613, 21988, 337, 264, 33733, 293, 829, 552, 646, 666, 264, 6114, 322, 264, 51240, 51240, 1411, 281, 2464, 484, 264, 16235, 23475, 9284, 337, 3890, 1602, 8213, 24590, 13, 51593, 51593], "temperature": 0.0, "avg_logprob": -0.08773020426432292, "compression_ratio": 1.6417910447761195, "no_speech_prob": 5.453267704069731e-07}, {"id": 27, "seek": 12456, "start": 142.08, "end": 149.14000000000001, "text": " left to write out the gradient descent algorithm for regularized linear regression.", "tokens": [50364, 663, 311, 983, 264, 5623, 281, 272, 7023, 264, 912, 382, 949, 11, 9735, 264, 5623, 281, 261, 2962, 50647, 50647, 570, 264, 3890, 2144, 1433, 7700, 505, 281, 853, 281, 23060, 261, 73, 13, 50976, 50976, 407, 718, 311, 747, 613, 21988, 337, 264, 33733, 293, 829, 552, 646, 666, 264, 6114, 322, 264, 51240, 51240, 1411, 281, 2464, 484, 264, 16235, 23475, 9284, 337, 3890, 1602, 8213, 24590, 13, 51593, 51593], "temperature": 0.0, "avg_logprob": -0.08773020426432292, "compression_ratio": 1.6417910447761195, "no_speech_prob": 5.453267704069731e-07}, {"id": 28, "seek": 14914, "start": 149.14, "end": 155.16, "text": " So to implement gradient descent for regularized linear regression, this is what you would", "tokens": [50364, 407, 281, 4445, 16235, 23475, 337, 3890, 1602, 8213, 24590, 11, 341, 307, 437, 291, 576, 50665, 50665, 362, 428, 3089, 360, 13, 50789, 50789, 1692, 311, 264, 5623, 337, 261, 73, 337, 361, 6915, 502, 807, 297, 11, 293, 510, 311, 264, 5623, 337, 272, 11, 293, 382, 7713, 11, 51129, 51129, 1767, 1604, 281, 3985, 484, 46218, 9205, 337, 439, 295, 613, 9834, 13, 51435, 51435, 823, 294, 1668, 337, 291, 281, 483, 341, 9284, 281, 589, 11, 341, 307, 439, 291, 643, 281, 458, 13, 51701, 51701], "temperature": 0.0, "avg_logprob": -0.12600917401521103, "compression_ratio": 1.6177777777777778, "no_speech_prob": 1.384546521876473e-05}, {"id": 29, "seek": 14914, "start": 155.16, "end": 157.64, "text": " have your code do.", "tokens": [50364, 407, 281, 4445, 16235, 23475, 337, 3890, 1602, 8213, 24590, 11, 341, 307, 437, 291, 576, 50665, 50665, 362, 428, 3089, 360, 13, 50789, 50789, 1692, 311, 264, 5623, 337, 261, 73, 337, 361, 6915, 502, 807, 297, 11, 293, 510, 311, 264, 5623, 337, 272, 11, 293, 382, 7713, 11, 51129, 51129, 1767, 1604, 281, 3985, 484, 46218, 9205, 337, 439, 295, 613, 9834, 13, 51435, 51435, 823, 294, 1668, 337, 291, 281, 483, 341, 9284, 281, 589, 11, 341, 307, 439, 291, 643, 281, 458, 13, 51701, 51701], "temperature": 0.0, "avg_logprob": -0.12600917401521103, "compression_ratio": 1.6177777777777778, "no_speech_prob": 1.384546521876473e-05}, {"id": 30, "seek": 14914, "start": 157.64, "end": 164.44, "text": " Here's the update for wj for j equals 1 through n, and here's the update for b, and as usual,", "tokens": [50364, 407, 281, 4445, 16235, 23475, 337, 3890, 1602, 8213, 24590, 11, 341, 307, 437, 291, 576, 50665, 50665, 362, 428, 3089, 360, 13, 50789, 50789, 1692, 311, 264, 5623, 337, 261, 73, 337, 361, 6915, 502, 807, 297, 11, 293, 510, 311, 264, 5623, 337, 272, 11, 293, 382, 7713, 11, 51129, 51129, 1767, 1604, 281, 3985, 484, 46218, 9205, 337, 439, 295, 613, 9834, 13, 51435, 51435, 823, 294, 1668, 337, 291, 281, 483, 341, 9284, 281, 589, 11, 341, 307, 439, 291, 643, 281, 458, 13, 51701, 51701], "temperature": 0.0, "avg_logprob": -0.12600917401521103, "compression_ratio": 1.6177777777777778, "no_speech_prob": 1.384546521876473e-05}, {"id": 31, "seek": 14914, "start": 164.44, "end": 170.56, "text": " please remember to carry out simultaneous updates for all of these parameters.", "tokens": [50364, 407, 281, 4445, 16235, 23475, 337, 3890, 1602, 8213, 24590, 11, 341, 307, 437, 291, 576, 50665, 50665, 362, 428, 3089, 360, 13, 50789, 50789, 1692, 311, 264, 5623, 337, 261, 73, 337, 361, 6915, 502, 807, 297, 11, 293, 510, 311, 264, 5623, 337, 272, 11, 293, 382, 7713, 11, 51129, 51129, 1767, 1604, 281, 3985, 484, 46218, 9205, 337, 439, 295, 613, 9834, 13, 51435, 51435, 823, 294, 1668, 337, 291, 281, 483, 341, 9284, 281, 589, 11, 341, 307, 439, 291, 643, 281, 458, 13, 51701, 51701], "temperature": 0.0, "avg_logprob": -0.12600917401521103, "compression_ratio": 1.6177777777777778, "no_speech_prob": 1.384546521876473e-05}, {"id": 32, "seek": 14914, "start": 170.56, "end": 175.88, "text": " Now in order for you to get this algorithm to work, this is all you need to know.", "tokens": [50364, 407, 281, 4445, 16235, 23475, 337, 3890, 1602, 8213, 24590, 11, 341, 307, 437, 291, 576, 50665, 50665, 362, 428, 3089, 360, 13, 50789, 50789, 1692, 311, 264, 5623, 337, 261, 73, 337, 361, 6915, 502, 807, 297, 11, 293, 510, 311, 264, 5623, 337, 272, 11, 293, 382, 7713, 11, 51129, 51129, 1767, 1604, 281, 3985, 484, 46218, 9205, 337, 439, 295, 613, 9834, 13, 51435, 51435, 823, 294, 1668, 337, 291, 281, 483, 341, 9284, 281, 589, 11, 341, 307, 439, 291, 643, 281, 458, 13, 51701, 51701], "temperature": 0.0, "avg_logprob": -0.12600917401521103, "compression_ratio": 1.6177777777777778, "no_speech_prob": 1.384546521876473e-05}, {"id": 33, "seek": 17588, "start": 175.88, "end": 180.64, "text": " But what I'd like to do in the remainder of this video is to go over some optional material", "tokens": [50364, 583, 437, 286, 1116, 411, 281, 360, 294, 264, 29837, 295, 341, 960, 307, 281, 352, 670, 512, 17312, 2527, 50602, 50602, 281, 16965, 257, 4748, 7731, 24002, 466, 437, 341, 8513, 307, 767, 884, 11, 382, 731, 50872, 50872, 382, 5081, 10515, 466, 577, 613, 33733, 366, 18949, 13, 51082, 51082, 440, 1472, 295, 341, 960, 307, 2584, 17312, 13, 51200, 51200, 467, 311, 2584, 1392, 498, 291, 10023, 264, 1472, 295, 341, 960, 11, 457, 498, 291, 362, 257, 2068, 1179, 51494, 51494, 294, 5221, 11, 550, 2897, 365, 385, 13, 51564, 51564, 467, 311, 1009, 1481, 281, 3967, 484, 365, 291, 510, 11, 293, 807, 613, 11787, 11, 4317, 291, 393, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10991992788799738, "compression_ratio": 1.7992700729927007, "no_speech_prob": 1.7231037418241613e-05}, {"id": 34, "seek": 17588, "start": 180.64, "end": 186.04, "text": " to convey a slightly deeper intuition about what this formula is actually doing, as well", "tokens": [50364, 583, 437, 286, 1116, 411, 281, 360, 294, 264, 29837, 295, 341, 960, 307, 281, 352, 670, 512, 17312, 2527, 50602, 50602, 281, 16965, 257, 4748, 7731, 24002, 466, 437, 341, 8513, 307, 767, 884, 11, 382, 731, 50872, 50872, 382, 5081, 10515, 466, 577, 613, 33733, 366, 18949, 13, 51082, 51082, 440, 1472, 295, 341, 960, 307, 2584, 17312, 13, 51200, 51200, 467, 311, 2584, 1392, 498, 291, 10023, 264, 1472, 295, 341, 960, 11, 457, 498, 291, 362, 257, 2068, 1179, 51494, 51494, 294, 5221, 11, 550, 2897, 365, 385, 13, 51564, 51564, 467, 311, 1009, 1481, 281, 3967, 484, 365, 291, 510, 11, 293, 807, 613, 11787, 11, 4317, 291, 393, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10991992788799738, "compression_ratio": 1.7992700729927007, "no_speech_prob": 1.7231037418241613e-05}, {"id": 35, "seek": 17588, "start": 186.04, "end": 190.24, "text": " as chat briefly about how these derivatives are derived.", "tokens": [50364, 583, 437, 286, 1116, 411, 281, 360, 294, 264, 29837, 295, 341, 960, 307, 281, 352, 670, 512, 17312, 2527, 50602, 50602, 281, 16965, 257, 4748, 7731, 24002, 466, 437, 341, 8513, 307, 767, 884, 11, 382, 731, 50872, 50872, 382, 5081, 10515, 466, 577, 613, 33733, 366, 18949, 13, 51082, 51082, 440, 1472, 295, 341, 960, 307, 2584, 17312, 13, 51200, 51200, 467, 311, 2584, 1392, 498, 291, 10023, 264, 1472, 295, 341, 960, 11, 457, 498, 291, 362, 257, 2068, 1179, 51494, 51494, 294, 5221, 11, 550, 2897, 365, 385, 13, 51564, 51564, 467, 311, 1009, 1481, 281, 3967, 484, 365, 291, 510, 11, 293, 807, 613, 11787, 11, 4317, 291, 393, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10991992788799738, "compression_ratio": 1.7992700729927007, "no_speech_prob": 1.7231037418241613e-05}, {"id": 36, "seek": 17588, "start": 190.24, "end": 192.6, "text": " The rest of this video is completely optional.", "tokens": [50364, 583, 437, 286, 1116, 411, 281, 360, 294, 264, 29837, 295, 341, 960, 307, 281, 352, 670, 512, 17312, 2527, 50602, 50602, 281, 16965, 257, 4748, 7731, 24002, 466, 437, 341, 8513, 307, 767, 884, 11, 382, 731, 50872, 50872, 382, 5081, 10515, 466, 577, 613, 33733, 366, 18949, 13, 51082, 51082, 440, 1472, 295, 341, 960, 307, 2584, 17312, 13, 51200, 51200, 467, 311, 2584, 1392, 498, 291, 10023, 264, 1472, 295, 341, 960, 11, 457, 498, 291, 362, 257, 2068, 1179, 51494, 51494, 294, 5221, 11, 550, 2897, 365, 385, 13, 51564, 51564, 467, 311, 1009, 1481, 281, 3967, 484, 365, 291, 510, 11, 293, 807, 613, 11787, 11, 4317, 291, 393, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10991992788799738, "compression_ratio": 1.7992700729927007, "no_speech_prob": 1.7231037418241613e-05}, {"id": 37, "seek": 17588, "start": 192.6, "end": 198.48, "text": " It's completely okay if you skip the rest of this video, but if you have a strong interest", "tokens": [50364, 583, 437, 286, 1116, 411, 281, 360, 294, 264, 29837, 295, 341, 960, 307, 281, 352, 670, 512, 17312, 2527, 50602, 50602, 281, 16965, 257, 4748, 7731, 24002, 466, 437, 341, 8513, 307, 767, 884, 11, 382, 731, 50872, 50872, 382, 5081, 10515, 466, 577, 613, 33733, 366, 18949, 13, 51082, 51082, 440, 1472, 295, 341, 960, 307, 2584, 17312, 13, 51200, 51200, 467, 311, 2584, 1392, 498, 291, 10023, 264, 1472, 295, 341, 960, 11, 457, 498, 291, 362, 257, 2068, 1179, 51494, 51494, 294, 5221, 11, 550, 2897, 365, 385, 13, 51564, 51564, 467, 311, 1009, 1481, 281, 3967, 484, 365, 291, 510, 11, 293, 807, 613, 11787, 11, 4317, 291, 393, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10991992788799738, "compression_ratio": 1.7992700729927007, "no_speech_prob": 1.7231037418241613e-05}, {"id": 38, "seek": 17588, "start": 198.48, "end": 199.88, "text": " in math, then stick with me.", "tokens": [50364, 583, 437, 286, 1116, 411, 281, 360, 294, 264, 29837, 295, 341, 960, 307, 281, 352, 670, 512, 17312, 2527, 50602, 50602, 281, 16965, 257, 4748, 7731, 24002, 466, 437, 341, 8513, 307, 767, 884, 11, 382, 731, 50872, 50872, 382, 5081, 10515, 466, 577, 613, 33733, 366, 18949, 13, 51082, 51082, 440, 1472, 295, 341, 960, 307, 2584, 17312, 13, 51200, 51200, 467, 311, 2584, 1392, 498, 291, 10023, 264, 1472, 295, 341, 960, 11, 457, 498, 291, 362, 257, 2068, 1179, 51494, 51494, 294, 5221, 11, 550, 2897, 365, 385, 13, 51564, 51564, 467, 311, 1009, 1481, 281, 3967, 484, 365, 291, 510, 11, 293, 807, 613, 11787, 11, 4317, 291, 393, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10991992788799738, "compression_ratio": 1.7992700729927007, "no_speech_prob": 1.7231037418241613e-05}, {"id": 39, "seek": 17588, "start": 199.88, "end": 204.84, "text": " It's always nice to hang out with you here, and through these equations, perhaps you can", "tokens": [50364, 583, 437, 286, 1116, 411, 281, 360, 294, 264, 29837, 295, 341, 960, 307, 281, 352, 670, 512, 17312, 2527, 50602, 50602, 281, 16965, 257, 4748, 7731, 24002, 466, 437, 341, 8513, 307, 767, 884, 11, 382, 731, 50872, 50872, 382, 5081, 10515, 466, 577, 613, 33733, 366, 18949, 13, 51082, 51082, 440, 1472, 295, 341, 960, 307, 2584, 17312, 13, 51200, 51200, 467, 311, 2584, 1392, 498, 291, 10023, 264, 1472, 295, 341, 960, 11, 457, 498, 291, 362, 257, 2068, 1179, 51494, 51494, 294, 5221, 11, 550, 2897, 365, 385, 13, 51564, 51564, 467, 311, 1009, 1481, 281, 3967, 484, 365, 291, 510, 11, 293, 807, 613, 11787, 11, 4317, 291, 393, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10991992788799738, "compression_ratio": 1.7992700729927007, "no_speech_prob": 1.7231037418241613e-05}, {"id": 40, "seek": 20484, "start": 204.84, "end": 209.8, "text": " build a deeper intuition about what the math and what the derivatives are doing as well.", "tokens": [50364, 1322, 257, 7731, 24002, 466, 437, 264, 5221, 293, 437, 264, 33733, 366, 884, 382, 731, 13, 50612, 50612, 407, 718, 311, 747, 257, 574, 13, 50746, 50746, 961, 311, 574, 412, 264, 5623, 4978, 337, 261, 73, 293, 28132, 309, 294, 1071, 636, 13, 51008, 51008, 492, 434, 25113, 261, 73, 382, 502, 1413, 261, 73, 3175, 8961, 1413, 13607, 670, 275, 1413, 261, 73, 13, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.08994898325960402, "compression_ratio": 1.5365853658536586, "no_speech_prob": 1.1478541637188755e-05}, {"id": 41, "seek": 20484, "start": 209.8, "end": 212.48000000000002, "text": " So let's take a look.", "tokens": [50364, 1322, 257, 7731, 24002, 466, 437, 264, 5221, 293, 437, 264, 33733, 366, 884, 382, 731, 13, 50612, 50612, 407, 718, 311, 747, 257, 574, 13, 50746, 50746, 961, 311, 574, 412, 264, 5623, 4978, 337, 261, 73, 293, 28132, 309, 294, 1071, 636, 13, 51008, 51008, 492, 434, 25113, 261, 73, 382, 502, 1413, 261, 73, 3175, 8961, 1413, 13607, 670, 275, 1413, 261, 73, 13, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.08994898325960402, "compression_ratio": 1.5365853658536586, "no_speech_prob": 1.1478541637188755e-05}, {"id": 42, "seek": 20484, "start": 212.48000000000002, "end": 217.72, "text": " Let's look at the update rule for wj and rewrite it in another way.", "tokens": [50364, 1322, 257, 7731, 24002, 466, 437, 264, 5221, 293, 437, 264, 33733, 366, 884, 382, 731, 13, 50612, 50612, 407, 718, 311, 747, 257, 574, 13, 50746, 50746, 961, 311, 574, 412, 264, 5623, 4978, 337, 261, 73, 293, 28132, 309, 294, 1071, 636, 13, 51008, 51008, 492, 434, 25113, 261, 73, 382, 502, 1413, 261, 73, 3175, 8961, 1413, 13607, 670, 275, 1413, 261, 73, 13, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.08994898325960402, "compression_ratio": 1.5365853658536586, "no_speech_prob": 1.1478541637188755e-05}, {"id": 43, "seek": 20484, "start": 217.72, "end": 229.48000000000002, "text": " We're updating wj as 1 times wj minus alpha times lambda over m times wj.", "tokens": [50364, 1322, 257, 7731, 24002, 466, 437, 264, 5221, 293, 437, 264, 33733, 366, 884, 382, 731, 13, 50612, 50612, 407, 718, 311, 747, 257, 574, 13, 50746, 50746, 961, 311, 574, 412, 264, 5623, 4978, 337, 261, 73, 293, 28132, 309, 294, 1071, 636, 13, 51008, 51008, 492, 434, 25113, 261, 73, 382, 502, 1413, 261, 73, 3175, 8961, 1413, 13607, 670, 275, 1413, 261, 73, 13, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.08994898325960402, "compression_ratio": 1.5365853658536586, "no_speech_prob": 1.1478541637188755e-05}, {"id": 44, "seek": 22948, "start": 229.48, "end": 236.76, "text": " So I've moved the term from the end to the front here, and then minus alpha times 1 over", "tokens": [50364, 407, 286, 600, 4259, 264, 1433, 490, 264, 917, 281, 264, 1868, 510, 11, 293, 550, 3175, 8961, 1413, 502, 670, 50728, 50728, 275, 11, 293, 550, 264, 1472, 295, 300, 1433, 670, 456, 13, 50966, 50966, 407, 321, 445, 39568, 264, 2115, 257, 707, 857, 13, 51128, 51128, 400, 498, 321, 20460, 11, 550, 321, 434, 2577, 300, 261, 73, 307, 10588, 382, 261, 73, 1413, 502, 3175, 8961, 1413, 51571, 51571], "temperature": 0.0, "avg_logprob": -0.1211020278930664, "compression_ratio": 1.6341463414634145, "no_speech_prob": 3.9669016587140504e-06}, {"id": 45, "seek": 22948, "start": 236.76, "end": 241.51999999999998, "text": " m, and then the rest of that term over there.", "tokens": [50364, 407, 286, 600, 4259, 264, 1433, 490, 264, 917, 281, 264, 1868, 510, 11, 293, 550, 3175, 8961, 1413, 502, 670, 50728, 50728, 275, 11, 293, 550, 264, 1472, 295, 300, 1433, 670, 456, 13, 50966, 50966, 407, 321, 445, 39568, 264, 2115, 257, 707, 857, 13, 51128, 51128, 400, 498, 321, 20460, 11, 550, 321, 434, 2577, 300, 261, 73, 307, 10588, 382, 261, 73, 1413, 502, 3175, 8961, 1413, 51571, 51571], "temperature": 0.0, "avg_logprob": -0.1211020278930664, "compression_ratio": 1.6341463414634145, "no_speech_prob": 3.9669016587140504e-06}, {"id": 46, "seek": 22948, "start": 241.51999999999998, "end": 244.76, "text": " So we just rearrange the terms a little bit.", "tokens": [50364, 407, 286, 600, 4259, 264, 1433, 490, 264, 917, 281, 264, 1868, 510, 11, 293, 550, 3175, 8961, 1413, 502, 670, 50728, 50728, 275, 11, 293, 550, 264, 1472, 295, 300, 1433, 670, 456, 13, 50966, 50966, 407, 321, 445, 39568, 264, 2115, 257, 707, 857, 13, 51128, 51128, 400, 498, 321, 20460, 11, 550, 321, 434, 2577, 300, 261, 73, 307, 10588, 382, 261, 73, 1413, 502, 3175, 8961, 1413, 51571, 51571], "temperature": 0.0, "avg_logprob": -0.1211020278930664, "compression_ratio": 1.6341463414634145, "no_speech_prob": 3.9669016587140504e-06}, {"id": 47, "seek": 22948, "start": 244.76, "end": 253.62, "text": " And if we simplify, then we're seeing that wj is updated as wj times 1 minus alpha times", "tokens": [50364, 407, 286, 600, 4259, 264, 1433, 490, 264, 917, 281, 264, 1868, 510, 11, 293, 550, 3175, 8961, 1413, 502, 670, 50728, 50728, 275, 11, 293, 550, 264, 1472, 295, 300, 1433, 670, 456, 13, 50966, 50966, 407, 321, 445, 39568, 264, 2115, 257, 707, 857, 13, 51128, 51128, 400, 498, 321, 20460, 11, 550, 321, 434, 2577, 300, 261, 73, 307, 10588, 382, 261, 73, 1413, 502, 3175, 8961, 1413, 51571, 51571], "temperature": 0.0, "avg_logprob": -0.1211020278930664, "compression_ratio": 1.6341463414634145, "no_speech_prob": 3.9669016587140504e-06}, {"id": 48, "seek": 25362, "start": 253.62, "end": 260.12, "text": " lambda over m minus alpha times this other term over here.", "tokens": [50364, 13607, 670, 275, 3175, 8961, 1413, 341, 661, 1433, 670, 510, 13, 50689, 50689, 400, 291, 1062, 5521, 341, 1150, 1433, 382, 264, 7713, 16235, 23475, 5623, 337, 517, 26713, 1602, 51009, 51009, 8213, 24590, 13, 51096, 51096, 639, 307, 264, 5623, 337, 8213, 24590, 949, 321, 632, 3890, 2144, 11, 293, 341, 307, 264, 1433, 51377, 51377, 321, 1866, 294, 1243, 732, 295, 341, 1164, 13, 51560, 51560, 400, 370, 264, 787, 1319, 562, 291, 909, 3890, 2144, 307, 300, 2602, 295, 261, 73, 885, 992, 281, 312, 2681, 51851, 51851], "temperature": 0.0, "avg_logprob": -0.1121004693051602, "compression_ratio": 1.7792792792792793, "no_speech_prob": 3.7852380501135485e-06}, {"id": 49, "seek": 25362, "start": 260.12, "end": 266.52, "text": " And you might recognize this second term as the usual gradient descent update for unregularized", "tokens": [50364, 13607, 670, 275, 3175, 8961, 1413, 341, 661, 1433, 670, 510, 13, 50689, 50689, 400, 291, 1062, 5521, 341, 1150, 1433, 382, 264, 7713, 16235, 23475, 5623, 337, 517, 26713, 1602, 51009, 51009, 8213, 24590, 13, 51096, 51096, 639, 307, 264, 5623, 337, 8213, 24590, 949, 321, 632, 3890, 2144, 11, 293, 341, 307, 264, 1433, 51377, 51377, 321, 1866, 294, 1243, 732, 295, 341, 1164, 13, 51560, 51560, 400, 370, 264, 787, 1319, 562, 291, 909, 3890, 2144, 307, 300, 2602, 295, 261, 73, 885, 992, 281, 312, 2681, 51851, 51851], "temperature": 0.0, "avg_logprob": -0.1121004693051602, "compression_ratio": 1.7792792792792793, "no_speech_prob": 3.7852380501135485e-06}, {"id": 50, "seek": 25362, "start": 266.52, "end": 268.26, "text": " linear regression.", "tokens": [50364, 13607, 670, 275, 3175, 8961, 1413, 341, 661, 1433, 670, 510, 13, 50689, 50689, 400, 291, 1062, 5521, 341, 1150, 1433, 382, 264, 7713, 16235, 23475, 5623, 337, 517, 26713, 1602, 51009, 51009, 8213, 24590, 13, 51096, 51096, 639, 307, 264, 5623, 337, 8213, 24590, 949, 321, 632, 3890, 2144, 11, 293, 341, 307, 264, 1433, 51377, 51377, 321, 1866, 294, 1243, 732, 295, 341, 1164, 13, 51560, 51560, 400, 370, 264, 787, 1319, 562, 291, 909, 3890, 2144, 307, 300, 2602, 295, 261, 73, 885, 992, 281, 312, 2681, 51851, 51851], "temperature": 0.0, "avg_logprob": -0.1121004693051602, "compression_ratio": 1.7792792792792793, "no_speech_prob": 3.7852380501135485e-06}, {"id": 51, "seek": 25362, "start": 268.26, "end": 273.88, "text": " This is the update for linear regression before we had regularization, and this is the term", "tokens": [50364, 13607, 670, 275, 3175, 8961, 1413, 341, 661, 1433, 670, 510, 13, 50689, 50689, 400, 291, 1062, 5521, 341, 1150, 1433, 382, 264, 7713, 16235, 23475, 5623, 337, 517, 26713, 1602, 51009, 51009, 8213, 24590, 13, 51096, 51096, 639, 307, 264, 5623, 337, 8213, 24590, 949, 321, 632, 3890, 2144, 11, 293, 341, 307, 264, 1433, 51377, 51377, 321, 1866, 294, 1243, 732, 295, 341, 1164, 13, 51560, 51560, 400, 370, 264, 787, 1319, 562, 291, 909, 3890, 2144, 307, 300, 2602, 295, 261, 73, 885, 992, 281, 312, 2681, 51851, 51851], "temperature": 0.0, "avg_logprob": -0.1121004693051602, "compression_ratio": 1.7792792792792793, "no_speech_prob": 3.7852380501135485e-06}, {"id": 52, "seek": 25362, "start": 273.88, "end": 277.54, "text": " we saw in week two of this course.", "tokens": [50364, 13607, 670, 275, 3175, 8961, 1413, 341, 661, 1433, 670, 510, 13, 50689, 50689, 400, 291, 1062, 5521, 341, 1150, 1433, 382, 264, 7713, 16235, 23475, 5623, 337, 517, 26713, 1602, 51009, 51009, 8213, 24590, 13, 51096, 51096, 639, 307, 264, 5623, 337, 8213, 24590, 949, 321, 632, 3890, 2144, 11, 293, 341, 307, 264, 1433, 51377, 51377, 321, 1866, 294, 1243, 732, 295, 341, 1164, 13, 51560, 51560, 400, 370, 264, 787, 1319, 562, 291, 909, 3890, 2144, 307, 300, 2602, 295, 261, 73, 885, 992, 281, 312, 2681, 51851, 51851], "temperature": 0.0, "avg_logprob": -0.1121004693051602, "compression_ratio": 1.7792792792792793, "no_speech_prob": 3.7852380501135485e-06}, {"id": 53, "seek": 25362, "start": 277.54, "end": 283.36, "text": " And so the only change when you add regularization is that instead of wj being set to be equal", "tokens": [50364, 13607, 670, 275, 3175, 8961, 1413, 341, 661, 1433, 670, 510, 13, 50689, 50689, 400, 291, 1062, 5521, 341, 1150, 1433, 382, 264, 7713, 16235, 23475, 5623, 337, 517, 26713, 1602, 51009, 51009, 8213, 24590, 13, 51096, 51096, 639, 307, 264, 5623, 337, 8213, 24590, 949, 321, 632, 3890, 2144, 11, 293, 341, 307, 264, 1433, 51377, 51377, 321, 1866, 294, 1243, 732, 295, 341, 1164, 13, 51560, 51560, 400, 370, 264, 787, 1319, 562, 291, 909, 3890, 2144, 307, 300, 2602, 295, 261, 73, 885, 992, 281, 312, 2681, 51851, 51851], "temperature": 0.0, "avg_logprob": -0.1121004693051602, "compression_ratio": 1.7792792792792793, "no_speech_prob": 3.7852380501135485e-06}, {"id": 54, "seek": 28336, "start": 283.36, "end": 293.52000000000004, "text": " to wj minus alpha times this term is now w times this number minus the usual update.", "tokens": [50364, 281, 261, 73, 3175, 8961, 1413, 341, 1433, 307, 586, 261, 1413, 341, 1230, 3175, 264, 7713, 5623, 13, 50872, 50872, 407, 341, 307, 437, 321, 632, 294, 1243, 472, 295, 341, 1164, 13, 51027, 51027, 407, 437, 307, 341, 700, 1433, 670, 510, 30, 51174, 51174, 1042, 11, 8961, 307, 257, 588, 1359, 3353, 1230, 11, 584, 1958, 13, 10607, 13, 51484, 51484, 45691, 307, 2673, 257, 1359, 1230, 11, 584, 502, 420, 1310, 1266, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11980279286702473, "compression_ratio": 1.6149425287356323, "no_speech_prob": 3.0415758374147117e-06}, {"id": 55, "seek": 28336, "start": 293.52000000000004, "end": 296.62, "text": " So this is what we had in week one of this course.", "tokens": [50364, 281, 261, 73, 3175, 8961, 1413, 341, 1433, 307, 586, 261, 1413, 341, 1230, 3175, 264, 7713, 5623, 13, 50872, 50872, 407, 341, 307, 437, 321, 632, 294, 1243, 472, 295, 341, 1164, 13, 51027, 51027, 407, 437, 307, 341, 700, 1433, 670, 510, 30, 51174, 51174, 1042, 11, 8961, 307, 257, 588, 1359, 3353, 1230, 11, 584, 1958, 13, 10607, 13, 51484, 51484, 45691, 307, 2673, 257, 1359, 1230, 11, 584, 502, 420, 1310, 1266, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11980279286702473, "compression_ratio": 1.6149425287356323, "no_speech_prob": 3.0415758374147117e-06}, {"id": 56, "seek": 28336, "start": 296.62, "end": 299.56, "text": " So what is this first term over here?", "tokens": [50364, 281, 261, 73, 3175, 8961, 1413, 341, 1433, 307, 586, 261, 1413, 341, 1230, 3175, 264, 7713, 5623, 13, 50872, 50872, 407, 341, 307, 437, 321, 632, 294, 1243, 472, 295, 341, 1164, 13, 51027, 51027, 407, 437, 307, 341, 700, 1433, 670, 510, 30, 51174, 51174, 1042, 11, 8961, 307, 257, 588, 1359, 3353, 1230, 11, 584, 1958, 13, 10607, 13, 51484, 51484, 45691, 307, 2673, 257, 1359, 1230, 11, 584, 502, 420, 1310, 1266, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11980279286702473, "compression_ratio": 1.6149425287356323, "no_speech_prob": 3.0415758374147117e-06}, {"id": 57, "seek": 28336, "start": 299.56, "end": 305.76, "text": " Well, alpha is a very small positive number, say 0.01.", "tokens": [50364, 281, 261, 73, 3175, 8961, 1413, 341, 1433, 307, 586, 261, 1413, 341, 1230, 3175, 264, 7713, 5623, 13, 50872, 50872, 407, 341, 307, 437, 321, 632, 294, 1243, 472, 295, 341, 1164, 13, 51027, 51027, 407, 437, 307, 341, 700, 1433, 670, 510, 30, 51174, 51174, 1042, 11, 8961, 307, 257, 588, 1359, 3353, 1230, 11, 584, 1958, 13, 10607, 13, 51484, 51484, 45691, 307, 2673, 257, 1359, 1230, 11, 584, 502, 420, 1310, 1266, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11980279286702473, "compression_ratio": 1.6149425287356323, "no_speech_prob": 3.0415758374147117e-06}, {"id": 58, "seek": 28336, "start": 305.76, "end": 311.44, "text": " Lambda is usually a small number, say 1 or maybe 10.", "tokens": [50364, 281, 261, 73, 3175, 8961, 1413, 341, 1433, 307, 586, 261, 1413, 341, 1230, 3175, 264, 7713, 5623, 13, 50872, 50872, 407, 341, 307, 437, 321, 632, 294, 1243, 472, 295, 341, 1164, 13, 51027, 51027, 407, 437, 307, 341, 700, 1433, 670, 510, 30, 51174, 51174, 1042, 11, 8961, 307, 257, 588, 1359, 3353, 1230, 11, 584, 1958, 13, 10607, 13, 51484, 51484, 45691, 307, 2673, 257, 1359, 1230, 11, 584, 502, 420, 1310, 1266, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11980279286702473, "compression_ratio": 1.6149425287356323, "no_speech_prob": 3.0415758374147117e-06}, {"id": 59, "seek": 31144, "start": 311.44, "end": 318.32, "text": " Let's say lambda is 1 for this example, and m is the training set size, say 50.", "tokens": [50364, 961, 311, 584, 13607, 307, 502, 337, 341, 1365, 11, 293, 275, 307, 264, 3097, 992, 2744, 11, 584, 2625, 13, 50708, 50708, 400, 370, 562, 291, 12972, 8961, 13607, 670, 275, 11, 584, 1958, 13, 10607, 1413, 502, 6666, 538, 2625, 11, 341, 1433, 51232, 51232, 5314, 493, 885, 257, 1359, 3353, 1230, 11, 584, 1958, 13, 1360, 17, 13, 51544, 51544, 400, 8807, 11, 502, 3175, 8961, 13607, 670, 275, 307, 516, 281, 312, 257, 1230, 445, 4748, 1570, 813, 502, 13, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.09304430268027565, "compression_ratio": 1.566326530612245, "no_speech_prob": 1.9637932382465806e-06}, {"id": 60, "seek": 31144, "start": 318.32, "end": 328.8, "text": " And so when you multiply alpha lambda over m, say 0.01 times 1 divided by 50, this term", "tokens": [50364, 961, 311, 584, 13607, 307, 502, 337, 341, 1365, 11, 293, 275, 307, 264, 3097, 992, 2744, 11, 584, 2625, 13, 50708, 50708, 400, 370, 562, 291, 12972, 8961, 13607, 670, 275, 11, 584, 1958, 13, 10607, 1413, 502, 6666, 538, 2625, 11, 341, 1433, 51232, 51232, 5314, 493, 885, 257, 1359, 3353, 1230, 11, 584, 1958, 13, 1360, 17, 13, 51544, 51544, 400, 8807, 11, 502, 3175, 8961, 13607, 670, 275, 307, 516, 281, 312, 257, 1230, 445, 4748, 1570, 813, 502, 13, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.09304430268027565, "compression_ratio": 1.566326530612245, "no_speech_prob": 1.9637932382465806e-06}, {"id": 61, "seek": 31144, "start": 328.8, "end": 335.04, "text": " ends up being a small positive number, say 0.0002.", "tokens": [50364, 961, 311, 584, 13607, 307, 502, 337, 341, 1365, 11, 293, 275, 307, 264, 3097, 992, 2744, 11, 584, 2625, 13, 50708, 50708, 400, 370, 562, 291, 12972, 8961, 13607, 670, 275, 11, 584, 1958, 13, 10607, 1413, 502, 6666, 538, 2625, 11, 341, 1433, 51232, 51232, 5314, 493, 885, 257, 1359, 3353, 1230, 11, 584, 1958, 13, 1360, 17, 13, 51544, 51544, 400, 8807, 11, 502, 3175, 8961, 13607, 670, 275, 307, 516, 281, 312, 257, 1230, 445, 4748, 1570, 813, 502, 13, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.09304430268027565, "compression_ratio": 1.566326530612245, "no_speech_prob": 1.9637932382465806e-06}, {"id": 62, "seek": 31144, "start": 335.04, "end": 340.92, "text": " And thus, 1 minus alpha lambda over m is going to be a number just slightly less than 1.", "tokens": [50364, 961, 311, 584, 13607, 307, 502, 337, 341, 1365, 11, 293, 275, 307, 264, 3097, 992, 2744, 11, 584, 2625, 13, 50708, 50708, 400, 370, 562, 291, 12972, 8961, 13607, 670, 275, 11, 584, 1958, 13, 10607, 1413, 502, 6666, 538, 2625, 11, 341, 1433, 51232, 51232, 5314, 493, 885, 257, 1359, 3353, 1230, 11, 584, 1958, 13, 1360, 17, 13, 51544, 51544, 400, 8807, 11, 502, 3175, 8961, 13607, 670, 275, 307, 516, 281, 312, 257, 1230, 445, 4748, 1570, 813, 502, 13, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.09304430268027565, "compression_ratio": 1.566326530612245, "no_speech_prob": 1.9637932382465806e-06}, {"id": 63, "seek": 34092, "start": 340.92, "end": 344.12, "text": " In this case, 0.9998.", "tokens": [50364, 682, 341, 1389, 11, 1958, 13, 8494, 22516, 13, 50524, 50524, 400, 370, 264, 1802, 295, 341, 1433, 307, 300, 322, 633, 2167, 24784, 295, 16235, 23475, 11, 50798, 50798, 291, 434, 1940, 261, 73, 293, 30955, 309, 538, 1958, 13, 8494, 22516, 13, 51062, 51062, 663, 307, 538, 512, 3547, 4748, 1570, 813, 502, 949, 9792, 484, 264, 7713, 5623, 13, 51333, 51333, 407, 437, 3890, 2144, 307, 884, 322, 633, 2167, 24784, 307, 291, 434, 30955, 261, 538, 257, 1230, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.098869756210682, "compression_ratio": 1.6225490196078431, "no_speech_prob": 3.2376949548051925e-06}, {"id": 64, "seek": 34092, "start": 344.12, "end": 349.6, "text": " And so the effect of this term is that on every single iteration of gradient descent,", "tokens": [50364, 682, 341, 1389, 11, 1958, 13, 8494, 22516, 13, 50524, 50524, 400, 370, 264, 1802, 295, 341, 1433, 307, 300, 322, 633, 2167, 24784, 295, 16235, 23475, 11, 50798, 50798, 291, 434, 1940, 261, 73, 293, 30955, 309, 538, 1958, 13, 8494, 22516, 13, 51062, 51062, 663, 307, 538, 512, 3547, 4748, 1570, 813, 502, 949, 9792, 484, 264, 7713, 5623, 13, 51333, 51333, 407, 437, 3890, 2144, 307, 884, 322, 633, 2167, 24784, 307, 291, 434, 30955, 261, 538, 257, 1230, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.098869756210682, "compression_ratio": 1.6225490196078431, "no_speech_prob": 3.2376949548051925e-06}, {"id": 65, "seek": 34092, "start": 349.6, "end": 354.88, "text": " you're taking wj and multiplying it by 0.9998.", "tokens": [50364, 682, 341, 1389, 11, 1958, 13, 8494, 22516, 13, 50524, 50524, 400, 370, 264, 1802, 295, 341, 1433, 307, 300, 322, 633, 2167, 24784, 295, 16235, 23475, 11, 50798, 50798, 291, 434, 1940, 261, 73, 293, 30955, 309, 538, 1958, 13, 8494, 22516, 13, 51062, 51062, 663, 307, 538, 512, 3547, 4748, 1570, 813, 502, 949, 9792, 484, 264, 7713, 5623, 13, 51333, 51333, 407, 437, 3890, 2144, 307, 884, 322, 633, 2167, 24784, 307, 291, 434, 30955, 261, 538, 257, 1230, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.098869756210682, "compression_ratio": 1.6225490196078431, "no_speech_prob": 3.2376949548051925e-06}, {"id": 66, "seek": 34092, "start": 354.88, "end": 360.3, "text": " That is by some numbers slightly less than 1 before carrying out the usual update.", "tokens": [50364, 682, 341, 1389, 11, 1958, 13, 8494, 22516, 13, 50524, 50524, 400, 370, 264, 1802, 295, 341, 1433, 307, 300, 322, 633, 2167, 24784, 295, 16235, 23475, 11, 50798, 50798, 291, 434, 1940, 261, 73, 293, 30955, 309, 538, 1958, 13, 8494, 22516, 13, 51062, 51062, 663, 307, 538, 512, 3547, 4748, 1570, 813, 502, 949, 9792, 484, 264, 7713, 5623, 13, 51333, 51333, 407, 437, 3890, 2144, 307, 884, 322, 633, 2167, 24784, 307, 291, 434, 30955, 261, 538, 257, 1230, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.098869756210682, "compression_ratio": 1.6225490196078431, "no_speech_prob": 3.2376949548051925e-06}, {"id": 67, "seek": 34092, "start": 360.3, "end": 366.24, "text": " So what regularization is doing on every single iteration is you're multiplying w by a number", "tokens": [50364, 682, 341, 1389, 11, 1958, 13, 8494, 22516, 13, 50524, 50524, 400, 370, 264, 1802, 295, 341, 1433, 307, 300, 322, 633, 2167, 24784, 295, 16235, 23475, 11, 50798, 50798, 291, 434, 1940, 261, 73, 293, 30955, 309, 538, 1958, 13, 8494, 22516, 13, 51062, 51062, 663, 307, 538, 512, 3547, 4748, 1570, 813, 502, 949, 9792, 484, 264, 7713, 5623, 13, 51333, 51333, 407, 437, 3890, 2144, 307, 884, 322, 633, 2167, 24784, 307, 291, 434, 30955, 261, 538, 257, 1230, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.098869756210682, "compression_ratio": 1.6225490196078431, "no_speech_prob": 3.2376949548051925e-06}, {"id": 68, "seek": 36624, "start": 366.24, "end": 372.24, "text": " slightly less than 1, and that has the effect of shrinking the value of wj just a little", "tokens": [50364, 4748, 1570, 813, 502, 11, 293, 300, 575, 264, 1802, 295, 41684, 264, 2158, 295, 261, 73, 445, 257, 707, 50664, 50664, 857, 13, 50726, 50726, 407, 341, 2709, 505, 1071, 1910, 322, 983, 3890, 2144, 575, 264, 1802, 295, 41684, 264, 9834, 51006, 51006, 261, 73, 257, 707, 857, 322, 633, 24784, 13, 51168, 51168, 400, 370, 300, 311, 577, 3890, 2144, 1985, 13, 51344, 51344, 759, 291, 434, 6369, 466, 577, 613, 13760, 2115, 645, 40610, 11, 286, 362, 445, 472, 1036, 51572, 51572, 17312, 4137, 300, 1709, 807, 445, 257, 707, 857, 295, 264, 17108, 295, 264, 13760, 1433, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.10734224765100212, "compression_ratio": 1.831275720164609, "no_speech_prob": 1.4823516494288924e-06}, {"id": 69, "seek": 36624, "start": 372.24, "end": 373.48, "text": " bit.", "tokens": [50364, 4748, 1570, 813, 502, 11, 293, 300, 575, 264, 1802, 295, 41684, 264, 2158, 295, 261, 73, 445, 257, 707, 50664, 50664, 857, 13, 50726, 50726, 407, 341, 2709, 505, 1071, 1910, 322, 983, 3890, 2144, 575, 264, 1802, 295, 41684, 264, 9834, 51006, 51006, 261, 73, 257, 707, 857, 322, 633, 24784, 13, 51168, 51168, 400, 370, 300, 311, 577, 3890, 2144, 1985, 13, 51344, 51344, 759, 291, 434, 6369, 466, 577, 613, 13760, 2115, 645, 40610, 11, 286, 362, 445, 472, 1036, 51572, 51572, 17312, 4137, 300, 1709, 807, 445, 257, 707, 857, 295, 264, 17108, 295, 264, 13760, 1433, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.10734224765100212, "compression_ratio": 1.831275720164609, "no_speech_prob": 1.4823516494288924e-06}, {"id": 70, "seek": 36624, "start": 373.48, "end": 379.08, "text": " So this gives us another view on why regularization has the effect of shrinking the parameters", "tokens": [50364, 4748, 1570, 813, 502, 11, 293, 300, 575, 264, 1802, 295, 41684, 264, 2158, 295, 261, 73, 445, 257, 707, 50664, 50664, 857, 13, 50726, 50726, 407, 341, 2709, 505, 1071, 1910, 322, 983, 3890, 2144, 575, 264, 1802, 295, 41684, 264, 9834, 51006, 51006, 261, 73, 257, 707, 857, 322, 633, 24784, 13, 51168, 51168, 400, 370, 300, 311, 577, 3890, 2144, 1985, 13, 51344, 51344, 759, 291, 434, 6369, 466, 577, 613, 13760, 2115, 645, 40610, 11, 286, 362, 445, 472, 1036, 51572, 51572, 17312, 4137, 300, 1709, 807, 445, 257, 707, 857, 295, 264, 17108, 295, 264, 13760, 1433, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.10734224765100212, "compression_ratio": 1.831275720164609, "no_speech_prob": 1.4823516494288924e-06}, {"id": 71, "seek": 36624, "start": 379.08, "end": 382.32, "text": " wj a little bit on every iteration.", "tokens": [50364, 4748, 1570, 813, 502, 11, 293, 300, 575, 264, 1802, 295, 41684, 264, 2158, 295, 261, 73, 445, 257, 707, 50664, 50664, 857, 13, 50726, 50726, 407, 341, 2709, 505, 1071, 1910, 322, 983, 3890, 2144, 575, 264, 1802, 295, 41684, 264, 9834, 51006, 51006, 261, 73, 257, 707, 857, 322, 633, 24784, 13, 51168, 51168, 400, 370, 300, 311, 577, 3890, 2144, 1985, 13, 51344, 51344, 759, 291, 434, 6369, 466, 577, 613, 13760, 2115, 645, 40610, 11, 286, 362, 445, 472, 1036, 51572, 51572, 17312, 4137, 300, 1709, 807, 445, 257, 707, 857, 295, 264, 17108, 295, 264, 13760, 1433, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.10734224765100212, "compression_ratio": 1.831275720164609, "no_speech_prob": 1.4823516494288924e-06}, {"id": 72, "seek": 36624, "start": 382.32, "end": 385.84000000000003, "text": " And so that's how regularization works.", "tokens": [50364, 4748, 1570, 813, 502, 11, 293, 300, 575, 264, 1802, 295, 41684, 264, 2158, 295, 261, 73, 445, 257, 707, 50664, 50664, 857, 13, 50726, 50726, 407, 341, 2709, 505, 1071, 1910, 322, 983, 3890, 2144, 575, 264, 1802, 295, 41684, 264, 9834, 51006, 51006, 261, 73, 257, 707, 857, 322, 633, 24784, 13, 51168, 51168, 400, 370, 300, 311, 577, 3890, 2144, 1985, 13, 51344, 51344, 759, 291, 434, 6369, 466, 577, 613, 13760, 2115, 645, 40610, 11, 286, 362, 445, 472, 1036, 51572, 51572, 17312, 4137, 300, 1709, 807, 445, 257, 707, 857, 295, 264, 17108, 295, 264, 13760, 1433, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.10734224765100212, "compression_ratio": 1.831275720164609, "no_speech_prob": 1.4823516494288924e-06}, {"id": 73, "seek": 36624, "start": 385.84000000000003, "end": 390.40000000000003, "text": " If you're curious about how these derivative terms were computed, I have just one last", "tokens": [50364, 4748, 1570, 813, 502, 11, 293, 300, 575, 264, 1802, 295, 41684, 264, 2158, 295, 261, 73, 445, 257, 707, 50664, 50664, 857, 13, 50726, 50726, 407, 341, 2709, 505, 1071, 1910, 322, 983, 3890, 2144, 575, 264, 1802, 295, 41684, 264, 9834, 51006, 51006, 261, 73, 257, 707, 857, 322, 633, 24784, 13, 51168, 51168, 400, 370, 300, 311, 577, 3890, 2144, 1985, 13, 51344, 51344, 759, 291, 434, 6369, 466, 577, 613, 13760, 2115, 645, 40610, 11, 286, 362, 445, 472, 1036, 51572, 51572, 17312, 4137, 300, 1709, 807, 445, 257, 707, 857, 295, 264, 17108, 295, 264, 13760, 1433, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.10734224765100212, "compression_ratio": 1.831275720164609, "no_speech_prob": 1.4823516494288924e-06}, {"id": 74, "seek": 36624, "start": 390.40000000000003, "end": 396.12, "text": " optional slide that goes through just a little bit of the calculation of the derivative term.", "tokens": [50364, 4748, 1570, 813, 502, 11, 293, 300, 575, 264, 1802, 295, 41684, 264, 2158, 295, 261, 73, 445, 257, 707, 50664, 50664, 857, 13, 50726, 50726, 407, 341, 2709, 505, 1071, 1910, 322, 983, 3890, 2144, 575, 264, 1802, 295, 41684, 264, 9834, 51006, 51006, 261, 73, 257, 707, 857, 322, 633, 24784, 13, 51168, 51168, 400, 370, 300, 311, 577, 3890, 2144, 1985, 13, 51344, 51344, 759, 291, 434, 6369, 466, 577, 613, 13760, 2115, 645, 40610, 11, 286, 362, 445, 472, 1036, 51572, 51572, 17312, 4137, 300, 1709, 807, 445, 257, 707, 857, 295, 264, 17108, 295, 264, 13760, 1433, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.10734224765100212, "compression_ratio": 1.831275720164609, "no_speech_prob": 1.4823516494288924e-06}, {"id": 75, "seek": 39612, "start": 396.12, "end": 401.0, "text": " Again, this slide and the rest of this video are completely optional, meaning you won't", "tokens": [50364, 3764, 11, 341, 4137, 293, 264, 1472, 295, 341, 960, 366, 2584, 17312, 11, 3620, 291, 1582, 380, 50608, 50608, 643, 604, 295, 341, 281, 360, 264, 3124, 20339, 293, 264, 48955, 13, 50842, 50842, 407, 718, 311, 1823, 807, 2661, 264, 13760, 17108, 13, 51024, 51024, 440, 13760, 295, 361, 365, 3104, 281, 261, 73, 1542, 411, 341, 13, 51304], "temperature": 0.0, "avg_logprob": -0.13969727168007504, "compression_ratio": 1.4885057471264367, "no_speech_prob": 4.860297394770896e-06}, {"id": 76, "seek": 39612, "start": 401.0, "end": 405.68, "text": " need any of this to do the practice labs and the quizzes.", "tokens": [50364, 3764, 11, 341, 4137, 293, 264, 1472, 295, 341, 960, 366, 2584, 17312, 11, 3620, 291, 1582, 380, 50608, 50608, 643, 604, 295, 341, 281, 360, 264, 3124, 20339, 293, 264, 48955, 13, 50842, 50842, 407, 718, 311, 1823, 807, 2661, 264, 13760, 17108, 13, 51024, 51024, 440, 13760, 295, 361, 365, 3104, 281, 261, 73, 1542, 411, 341, 13, 51304], "temperature": 0.0, "avg_logprob": -0.13969727168007504, "compression_ratio": 1.4885057471264367, "no_speech_prob": 4.860297394770896e-06}, {"id": 77, "seek": 39612, "start": 405.68, "end": 409.32, "text": " So let's step through quickly the derivative calculation.", "tokens": [50364, 3764, 11, 341, 4137, 293, 264, 1472, 295, 341, 960, 366, 2584, 17312, 11, 3620, 291, 1582, 380, 50608, 50608, 643, 604, 295, 341, 281, 360, 264, 3124, 20339, 293, 264, 48955, 13, 50842, 50842, 407, 718, 311, 1823, 807, 2661, 264, 13760, 17108, 13, 51024, 51024, 440, 13760, 295, 361, 365, 3104, 281, 261, 73, 1542, 411, 341, 13, 51304], "temperature": 0.0, "avg_logprob": -0.13969727168007504, "compression_ratio": 1.4885057471264367, "no_speech_prob": 4.860297394770896e-06}, {"id": 78, "seek": 40932, "start": 409.32, "end": 426.84, "text": " The derivative of j with respect to wj looks like this.", "tokens": [50364, 440, 13760, 295, 361, 365, 3104, 281, 261, 73, 1542, 411, 341, 13, 51240, 51240, 9647, 336, 300, 283, 295, 2031, 337, 8213, 24590, 307, 7642, 382, 261, 5893, 2031, 1804, 272, 420, 261, 5893, 1674, 51636, 51636, 2031, 1804, 272, 13, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.1361971730771272, "compression_ratio": 1.3333333333333333, "no_speech_prob": 3.785214630624978e-06}, {"id": 79, "seek": 40932, "start": 426.84, "end": 434.76, "text": " Recall that f of x for linear regression is defined as w dot x plus b or w dot product", "tokens": [50364, 440, 13760, 295, 361, 365, 3104, 281, 261, 73, 1542, 411, 341, 13, 51240, 51240, 9647, 336, 300, 283, 295, 2031, 337, 8213, 24590, 307, 7642, 382, 261, 5893, 2031, 1804, 272, 420, 261, 5893, 1674, 51636, 51636, 2031, 1804, 272, 13, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.1361971730771272, "compression_ratio": 1.3333333333333333, "no_speech_prob": 3.785214630624978e-06}, {"id": 80, "seek": 40932, "start": 434.76, "end": 436.64, "text": " x plus b.", "tokens": [50364, 440, 13760, 295, 361, 365, 3104, 281, 261, 73, 1542, 411, 341, 13, 51240, 51240, 9647, 336, 300, 283, 295, 2031, 337, 8213, 24590, 307, 7642, 382, 261, 5893, 2031, 1804, 272, 420, 261, 5893, 1674, 51636, 51636, 2031, 1804, 272, 13, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.1361971730771272, "compression_ratio": 1.3333333333333333, "no_speech_prob": 3.785214630624978e-06}, {"id": 81, "seek": 43664, "start": 436.64, "end": 444.2, "text": " And it turns out that by the rules of calculus, the derivatives look like this is 1 over 2m", "tokens": [50364, 400, 309, 4523, 484, 300, 538, 264, 4474, 295, 33400, 11, 264, 33733, 574, 411, 341, 307, 502, 670, 568, 76, 50742, 50742, 1413, 264, 2408, 741, 6915, 502, 807, 275, 295, 261, 5893, 2031, 1804, 272, 3175, 288, 1413, 568, 87, 73, 1804, 264, 13760, 51416, 51416], "temperature": 0.0, "avg_logprob": -0.13102651596069337, "compression_ratio": 1.4330708661417322, "no_speech_prob": 3.2887112411117414e-06}, {"id": 82, "seek": 43664, "start": 444.2, "end": 457.68, "text": " times the sum i equals 1 through m of w dot x plus b minus y times 2xj plus the derivative", "tokens": [50364, 400, 309, 4523, 484, 300, 538, 264, 4474, 295, 33400, 11, 264, 33733, 574, 411, 341, 307, 502, 670, 568, 76, 50742, 50742, 1413, 264, 2408, 741, 6915, 502, 807, 275, 295, 261, 5893, 2031, 1804, 272, 3175, 288, 1413, 568, 87, 73, 1804, 264, 13760, 51416, 51416], "temperature": 0.0, "avg_logprob": -0.13102651596069337, "compression_ratio": 1.4330708661417322, "no_speech_prob": 3.2887112411117414e-06}, {"id": 83, "seek": 45768, "start": 457.68, "end": 467.2, "text": " of the regularization term, which is lambda over 2m times 2wj.", "tokens": [50364, 295, 264, 3890, 2144, 1433, 11, 597, 307, 13607, 670, 568, 76, 1413, 568, 86, 73, 13, 50840, 50840, 13428, 300, 264, 1150, 1433, 775, 406, 362, 264, 28811, 1433, 490, 361, 6915, 502, 807, 51100, 51100, 297, 3602, 13, 51179, 51179, 440, 683, 329, 10373, 484, 510, 293, 510, 293, 611, 510, 293, 510, 13, 51496, 51496], "temperature": 0.0, "avg_logprob": -0.15700257619222005, "compression_ratio": 1.4896551724137932, "no_speech_prob": 2.1691634799481108e-07}, {"id": 84, "seek": 45768, "start": 467.2, "end": 472.40000000000003, "text": " Notice that the second term does not have the summation term from j equals 1 through", "tokens": [50364, 295, 264, 3890, 2144, 1433, 11, 597, 307, 13607, 670, 568, 76, 1413, 568, 86, 73, 13, 50840, 50840, 13428, 300, 264, 1150, 1433, 775, 406, 362, 264, 28811, 1433, 490, 361, 6915, 502, 807, 51100, 51100, 297, 3602, 13, 51179, 51179, 440, 683, 329, 10373, 484, 510, 293, 510, 293, 611, 510, 293, 510, 13, 51496, 51496], "temperature": 0.0, "avg_logprob": -0.15700257619222005, "compression_ratio": 1.4896551724137932, "no_speech_prob": 2.1691634799481108e-07}, {"id": 85, "seek": 45768, "start": 472.40000000000003, "end": 473.98, "text": " n anymore.", "tokens": [50364, 295, 264, 3890, 2144, 1433, 11, 597, 307, 13607, 670, 568, 76, 1413, 568, 86, 73, 13, 50840, 50840, 13428, 300, 264, 1150, 1433, 775, 406, 362, 264, 28811, 1433, 490, 361, 6915, 502, 807, 51100, 51100, 297, 3602, 13, 51179, 51179, 440, 683, 329, 10373, 484, 510, 293, 510, 293, 611, 510, 293, 510, 13, 51496, 51496], "temperature": 0.0, "avg_logprob": -0.15700257619222005, "compression_ratio": 1.4896551724137932, "no_speech_prob": 2.1691634799481108e-07}, {"id": 86, "seek": 45768, "start": 473.98, "end": 480.32, "text": " The twos cancel out here and here and also here and here.", "tokens": [50364, 295, 264, 3890, 2144, 1433, 11, 597, 307, 13607, 670, 568, 76, 1413, 568, 86, 73, 13, 50840, 50840, 13428, 300, 264, 1150, 1433, 775, 406, 362, 264, 28811, 1433, 490, 361, 6915, 502, 807, 51100, 51100, 297, 3602, 13, 51179, 51179, 440, 683, 329, 10373, 484, 510, 293, 510, 293, 611, 510, 293, 510, 13, 51496, 51496], "temperature": 0.0, "avg_logprob": -0.15700257619222005, "compression_ratio": 1.4896551724137932, "no_speech_prob": 2.1691634799481108e-07}, {"id": 87, "seek": 48032, "start": 480.32, "end": 488.04, "text": " And so it simplifies to this expression over here.", "tokens": [50364, 400, 370, 309, 6883, 11221, 281, 341, 6114, 670, 510, 13, 50750, 50750, 400, 2721, 11, 1604, 300, 261, 87, 1804, 272, 307, 283, 295, 2031, 13, 50998, 50998, 400, 370, 291, 393, 28132, 309, 382, 341, 6114, 760, 510, 13, 51249, 51249, 407, 341, 307, 983, 341, 6114, 307, 1143, 281, 14722, 264, 16235, 294, 3890, 1602, 8213, 51517, 51517, 24590, 13, 51608, 51608, 407, 291, 586, 458, 577, 281, 4445, 3890, 1602, 8213, 24590, 13, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.09793425195011092, "compression_ratio": 1.7696629213483146, "no_speech_prob": 4.181164001693105e-07}, {"id": 88, "seek": 48032, "start": 488.04, "end": 493.0, "text": " And finally, remember that wx plus b is f of x.", "tokens": [50364, 400, 370, 309, 6883, 11221, 281, 341, 6114, 670, 510, 13, 50750, 50750, 400, 2721, 11, 1604, 300, 261, 87, 1804, 272, 307, 283, 295, 2031, 13, 50998, 50998, 400, 370, 291, 393, 28132, 309, 382, 341, 6114, 760, 510, 13, 51249, 51249, 407, 341, 307, 983, 341, 6114, 307, 1143, 281, 14722, 264, 16235, 294, 3890, 1602, 8213, 51517, 51517, 24590, 13, 51608, 51608, 407, 291, 586, 458, 577, 281, 4445, 3890, 1602, 8213, 24590, 13, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.09793425195011092, "compression_ratio": 1.7696629213483146, "no_speech_prob": 4.181164001693105e-07}, {"id": 89, "seek": 48032, "start": 493.0, "end": 498.02, "text": " And so you can rewrite it as this expression down here.", "tokens": [50364, 400, 370, 309, 6883, 11221, 281, 341, 6114, 670, 510, 13, 50750, 50750, 400, 2721, 11, 1604, 300, 261, 87, 1804, 272, 307, 283, 295, 2031, 13, 50998, 50998, 400, 370, 291, 393, 28132, 309, 382, 341, 6114, 760, 510, 13, 51249, 51249, 407, 341, 307, 983, 341, 6114, 307, 1143, 281, 14722, 264, 16235, 294, 3890, 1602, 8213, 51517, 51517, 24590, 13, 51608, 51608, 407, 291, 586, 458, 577, 281, 4445, 3890, 1602, 8213, 24590, 13, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.09793425195011092, "compression_ratio": 1.7696629213483146, "no_speech_prob": 4.181164001693105e-07}, {"id": 90, "seek": 48032, "start": 498.02, "end": 503.38, "text": " So this is why this expression is used to compute the gradient in regularized linear", "tokens": [50364, 400, 370, 309, 6883, 11221, 281, 341, 6114, 670, 510, 13, 50750, 50750, 400, 2721, 11, 1604, 300, 261, 87, 1804, 272, 307, 283, 295, 2031, 13, 50998, 50998, 400, 370, 291, 393, 28132, 309, 382, 341, 6114, 760, 510, 13, 51249, 51249, 407, 341, 307, 983, 341, 6114, 307, 1143, 281, 14722, 264, 16235, 294, 3890, 1602, 8213, 51517, 51517, 24590, 13, 51608, 51608, 407, 291, 586, 458, 577, 281, 4445, 3890, 1602, 8213, 24590, 13, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.09793425195011092, "compression_ratio": 1.7696629213483146, "no_speech_prob": 4.181164001693105e-07}, {"id": 91, "seek": 48032, "start": 503.38, "end": 505.2, "text": " regression.", "tokens": [50364, 400, 370, 309, 6883, 11221, 281, 341, 6114, 670, 510, 13, 50750, 50750, 400, 2721, 11, 1604, 300, 261, 87, 1804, 272, 307, 283, 295, 2031, 13, 50998, 50998, 400, 370, 291, 393, 28132, 309, 382, 341, 6114, 760, 510, 13, 51249, 51249, 407, 341, 307, 983, 341, 6114, 307, 1143, 281, 14722, 264, 16235, 294, 3890, 1602, 8213, 51517, 51517, 24590, 13, 51608, 51608, 407, 291, 586, 458, 577, 281, 4445, 3890, 1602, 8213, 24590, 13, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.09793425195011092, "compression_ratio": 1.7696629213483146, "no_speech_prob": 4.181164001693105e-07}, {"id": 92, "seek": 48032, "start": 505.2, "end": 509.64, "text": " So you now know how to implement regularized linear regression.", "tokens": [50364, 400, 370, 309, 6883, 11221, 281, 341, 6114, 670, 510, 13, 50750, 50750, 400, 2721, 11, 1604, 300, 261, 87, 1804, 272, 307, 283, 295, 2031, 13, 50998, 50998, 400, 370, 291, 393, 28132, 309, 382, 341, 6114, 760, 510, 13, 51249, 51249, 407, 341, 307, 983, 341, 6114, 307, 1143, 281, 14722, 264, 16235, 294, 3890, 1602, 8213, 51517, 51517, 24590, 13, 51608, 51608, 407, 291, 586, 458, 577, 281, 4445, 3890, 1602, 8213, 24590, 13, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.09793425195011092, "compression_ratio": 1.7696629213483146, "no_speech_prob": 4.181164001693105e-07}, {"id": 93, "seek": 50964, "start": 509.64, "end": 513.6, "text": " Using this, you will reduce overfitting when you have a lot of features and relatively", "tokens": [50364, 11142, 341, 11, 291, 486, 5407, 670, 69, 2414, 562, 291, 362, 257, 688, 295, 4122, 293, 7226, 50562, 50562, 1359, 3097, 992, 13, 50660, 50660, 400, 341, 820, 718, 291, 483, 8213, 24590, 281, 589, 709, 1101, 322, 867, 2740, 13, 50928, 50928, 682, 264, 958, 960, 11, 321, 603, 747, 341, 3890, 2144, 1558, 293, 3079, 309, 281, 3565, 3142, 24590, 281, 51206, 51206, 5042, 670, 69, 2414, 337, 3565, 3142, 24590, 382, 731, 13, 51352, 51352, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51440], "temperature": 0.0, "avg_logprob": -0.12941150462373774, "compression_ratio": 1.6888888888888889, "no_speech_prob": 2.1437050236272626e-05}, {"id": 94, "seek": 50964, "start": 513.6, "end": 515.56, "text": " small training set.", "tokens": [50364, 11142, 341, 11, 291, 486, 5407, 670, 69, 2414, 562, 291, 362, 257, 688, 295, 4122, 293, 7226, 50562, 50562, 1359, 3097, 992, 13, 50660, 50660, 400, 341, 820, 718, 291, 483, 8213, 24590, 281, 589, 709, 1101, 322, 867, 2740, 13, 50928, 50928, 682, 264, 958, 960, 11, 321, 603, 747, 341, 3890, 2144, 1558, 293, 3079, 309, 281, 3565, 3142, 24590, 281, 51206, 51206, 5042, 670, 69, 2414, 337, 3565, 3142, 24590, 382, 731, 13, 51352, 51352, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51440], "temperature": 0.0, "avg_logprob": -0.12941150462373774, "compression_ratio": 1.6888888888888889, "no_speech_prob": 2.1437050236272626e-05}, {"id": 95, "seek": 50964, "start": 515.56, "end": 520.92, "text": " And this should let you get linear regression to work much better on many problems.", "tokens": [50364, 11142, 341, 11, 291, 486, 5407, 670, 69, 2414, 562, 291, 362, 257, 688, 295, 4122, 293, 7226, 50562, 50562, 1359, 3097, 992, 13, 50660, 50660, 400, 341, 820, 718, 291, 483, 8213, 24590, 281, 589, 709, 1101, 322, 867, 2740, 13, 50928, 50928, 682, 264, 958, 960, 11, 321, 603, 747, 341, 3890, 2144, 1558, 293, 3079, 309, 281, 3565, 3142, 24590, 281, 51206, 51206, 5042, 670, 69, 2414, 337, 3565, 3142, 24590, 382, 731, 13, 51352, 51352, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51440], "temperature": 0.0, "avg_logprob": -0.12941150462373774, "compression_ratio": 1.6888888888888889, "no_speech_prob": 2.1437050236272626e-05}, {"id": 96, "seek": 50964, "start": 520.92, "end": 526.48, "text": " In the next video, we'll take this regularization idea and apply it to logistic regression to", "tokens": [50364, 11142, 341, 11, 291, 486, 5407, 670, 69, 2414, 562, 291, 362, 257, 688, 295, 4122, 293, 7226, 50562, 50562, 1359, 3097, 992, 13, 50660, 50660, 400, 341, 820, 718, 291, 483, 8213, 24590, 281, 589, 709, 1101, 322, 867, 2740, 13, 50928, 50928, 682, 264, 958, 960, 11, 321, 603, 747, 341, 3890, 2144, 1558, 293, 3079, 309, 281, 3565, 3142, 24590, 281, 51206, 51206, 5042, 670, 69, 2414, 337, 3565, 3142, 24590, 382, 731, 13, 51352, 51352, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51440], "temperature": 0.0, "avg_logprob": -0.12941150462373774, "compression_ratio": 1.6888888888888889, "no_speech_prob": 2.1437050236272626e-05}, {"id": 97, "seek": 50964, "start": 526.48, "end": 529.4, "text": " avoid overfitting for logistic regression as well.", "tokens": [50364, 11142, 341, 11, 291, 486, 5407, 670, 69, 2414, 562, 291, 362, 257, 688, 295, 4122, 293, 7226, 50562, 50562, 1359, 3097, 992, 13, 50660, 50660, 400, 341, 820, 718, 291, 483, 8213, 24590, 281, 589, 709, 1101, 322, 867, 2740, 13, 50928, 50928, 682, 264, 958, 960, 11, 321, 603, 747, 341, 3890, 2144, 1558, 293, 3079, 309, 281, 3565, 3142, 24590, 281, 51206, 51206, 5042, 670, 69, 2414, 337, 3565, 3142, 24590, 382, 731, 13, 51352, 51352, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51440], "temperature": 0.0, "avg_logprob": -0.12941150462373774, "compression_ratio": 1.6888888888888889, "no_speech_prob": 2.1437050236272626e-05}, {"id": 98, "seek": 52940, "start": 529.4, "end": 540.0799999999999, "text": " Let's take a look at that in the next video.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 50898], "temperature": 0.0, "avg_logprob": -0.3142733573913574, "compression_ratio": 0.8979591836734694, "no_speech_prob": 0.000268907577265054}], "language": "en", "video_id": "yRSKygmsvSI", "entity": "ML Specialization, Andrew Ng (2022)"}}