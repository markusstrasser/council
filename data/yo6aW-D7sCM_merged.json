{"video_id": "yo6aW-D7sCM", "title": "5.11 Additional Neural Network Concepts | Advanced Optimization --[Machine Learning | Andrew Ng]", "description": "Second Course:\nAdvanced Learning Algorithms.\n\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 385, "views": 80, "publish_date": "11/04/2022", "timestamp": 1661472000, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " Gradient descent is an optimization algorithm that is widely used in machine learning and was the foundation of many algorithms like linear regression and logistic regression and early implementations of neural networks. But it turns out that there are now some other optimization algorithms for minimizing the cost function that are even better than gradient descent. In this video, we'll take a look at an algorithm that can help you train your neural network much faster than gradient descent. Recall that this is the expression for one step of gradient descent. A parameter wj is updated as wj minus the learning rate alpha times this partial derivative term. How can we make this work even better? In this example, I plotted the cost function j using a contour plot comprising these ellipses. And so the minimum of this cost function is at the center of these ellipses down here. Now if you were to start gradient descent down here, one step of gradient descent, if alpha is small, may take you a little bit in that direction, then another step, then another step, then another step, then another step. And you notice that every single step of gradient descent is pretty much going in the same direction. And if you see this to be the case, you might wonder, well, why don't we make alpha bigger? Can we have an algorithm to automatically increase alpha to just make it take bigger steps and get to the minimum faster? There's an algorithm called the Adam algorithm that can do that. If it sees that the learning rate is too small, and we are just taking tiny little steps in a similar direction over and over, we should just make the learning rate alpha bigger. In contrast, here again is the same cost function. If we were starting here and had a relatively big learning rate alpha, then maybe one step of gradient descent takes us here, and the second step takes us here, third step, and the fourth step, and the fifth step, and the sixth step. And if you see gradient descent doing this, it's oscillating back and forth, you'd be tempted to say, well, why don't we make the learning rate smaller? And the Adam algorithm can also do that automatically. And with a smaller learning rate, you can then take a more smooth path toward the minimum of the cost function. So depending on how gradient descent is proceeding, sometimes you wish you had a bigger learning rate alpha, and sometimes you wish you had a smaller learning rate alpha. So the Adam algorithm can adjust the learning rate automatically. Adam stands for Adaptive Moment Estimation, or ADAM. And don't worry too much about what this name means, it's just what the authors had called this algorithm. But interestingly, the Adam algorithm doesn't use a single global learning rate alpha, it uses a different learning rates for every single parameter of your model. So if you have parameters w1 through w10, as well as b, then it actually has 11 learning rate parameters, alpha 1, alpha 2, all the way through alpha 10, for w1 through w10, as well as I'll call it alpha 11 for the parameter b. And the intuition behind the Adam algorithm is if a parameter wj or b seems to keep on moving in roughly the same direction, this is what we saw on the first example on the previous slide. But if it seems to keep on moving in roughly the same direction, let's increase the learning rate for that parameter. Let's go faster in that direction. Conversely, if a parameter keeps oscillating back and forth, this is what you saw in the second example on the previous slide, then let's not have it keep on oscillating or bouncing back and forth, let's reduce alpha j for that parameter a little bit. The details of how Adam does this are a bit complicated and beyond the scope of this course. But if you take some more advanced deep learning classes later, you may learn more about the details of this Adam algorithm. But in code, this is how you would implement it. The model is exactly the same as before. And the way you compile the model is very similar to what we had before, except that we now add one extra argument to the compile function, which is that we specify that the optimizer you want to use is tf.keris.optimizers.theAdamOptimizer. So the Adam optimization algorithm does need some default initial learning rate alpha. And in this example, I've set that initial learning rate to be 10 to the negative 3. But when you're using the Adam algorithm in practice, it's worth trying a few values for this initial, this default global learning rate. Try some larger and some smaller values to see what gives you the fastest learning performance. Compared to the original gradient descent algorithm that you had learned in the previous course though, the Adam algorithm, because it can adapt the learning rate a bit automatically, it is more robust to the exact choice of learning rate that you pick. Though it is still worth tuning this parameter a little bit to see if you can get somewhat faster learning. So that's it for the Adam optimization algorithm. It typically works much faster than gradient descent, and it's become a de facto standard in how practitioners train their neural networks. So if you're trying to decide what learning algorithm to use, what optimization algorithm to use to train your neural network, a safe choice would be to just use the Adam optimization algorithm. And most practitioners today will use Adam rather than the original gradient descent algorithm. And with this, I hope that your learning algorithms will be able to learn much more quickly. Now, in the next couple of videos, I'd like to touch on some more advanced concepts for neural networks. And in particular, in the next video, let's take a look at some alternative layer types.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.8, "text": " Gradient descent is an optimization algorithm that is widely used in machine learning and", "tokens": [50364, 16710, 1196, 23475, 307, 364, 19618, 9284, 300, 307, 13371, 1143, 294, 3479, 2539, 293, 50804, 50804, 390, 264, 7030, 295, 867, 14642, 411, 8213, 24590, 293, 3565, 3142, 24590, 51092, 51092, 293, 2440, 4445, 763, 295, 18161, 9590, 13, 51292, 51292, 583, 309, 4523, 484, 300, 456, 366, 586, 512, 661, 19618, 14642, 337, 46608, 264, 51535, 51535, 2063, 2445, 300, 366, 754, 1101, 813, 16235, 23475, 13, 51735, 51735], "temperature": 0.0, "avg_logprob": -0.12220936605375107, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.007933060638606548}, {"id": 1, "seek": 0, "start": 8.8, "end": 14.56, "text": " was the foundation of many algorithms like linear regression and logistic regression", "tokens": [50364, 16710, 1196, 23475, 307, 364, 19618, 9284, 300, 307, 13371, 1143, 294, 3479, 2539, 293, 50804, 50804, 390, 264, 7030, 295, 867, 14642, 411, 8213, 24590, 293, 3565, 3142, 24590, 51092, 51092, 293, 2440, 4445, 763, 295, 18161, 9590, 13, 51292, 51292, 583, 309, 4523, 484, 300, 456, 366, 586, 512, 661, 19618, 14642, 337, 46608, 264, 51535, 51535, 2063, 2445, 300, 366, 754, 1101, 813, 16235, 23475, 13, 51735, 51735], "temperature": 0.0, "avg_logprob": -0.12220936605375107, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.007933060638606548}, {"id": 2, "seek": 0, "start": 14.56, "end": 18.56, "text": " and early implementations of neural networks.", "tokens": [50364, 16710, 1196, 23475, 307, 364, 19618, 9284, 300, 307, 13371, 1143, 294, 3479, 2539, 293, 50804, 50804, 390, 264, 7030, 295, 867, 14642, 411, 8213, 24590, 293, 3565, 3142, 24590, 51092, 51092, 293, 2440, 4445, 763, 295, 18161, 9590, 13, 51292, 51292, 583, 309, 4523, 484, 300, 456, 366, 586, 512, 661, 19618, 14642, 337, 46608, 264, 51535, 51535, 2063, 2445, 300, 366, 754, 1101, 813, 16235, 23475, 13, 51735, 51735], "temperature": 0.0, "avg_logprob": -0.12220936605375107, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.007933060638606548}, {"id": 3, "seek": 0, "start": 18.56, "end": 23.42, "text": " But it turns out that there are now some other optimization algorithms for minimizing the", "tokens": [50364, 16710, 1196, 23475, 307, 364, 19618, 9284, 300, 307, 13371, 1143, 294, 3479, 2539, 293, 50804, 50804, 390, 264, 7030, 295, 867, 14642, 411, 8213, 24590, 293, 3565, 3142, 24590, 51092, 51092, 293, 2440, 4445, 763, 295, 18161, 9590, 13, 51292, 51292, 583, 309, 4523, 484, 300, 456, 366, 586, 512, 661, 19618, 14642, 337, 46608, 264, 51535, 51535, 2063, 2445, 300, 366, 754, 1101, 813, 16235, 23475, 13, 51735, 51735], "temperature": 0.0, "avg_logprob": -0.12220936605375107, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.007933060638606548}, {"id": 4, "seek": 0, "start": 23.42, "end": 27.42, "text": " cost function that are even better than gradient descent.", "tokens": [50364, 16710, 1196, 23475, 307, 364, 19618, 9284, 300, 307, 13371, 1143, 294, 3479, 2539, 293, 50804, 50804, 390, 264, 7030, 295, 867, 14642, 411, 8213, 24590, 293, 3565, 3142, 24590, 51092, 51092, 293, 2440, 4445, 763, 295, 18161, 9590, 13, 51292, 51292, 583, 309, 4523, 484, 300, 456, 366, 586, 512, 661, 19618, 14642, 337, 46608, 264, 51535, 51535, 2063, 2445, 300, 366, 754, 1101, 813, 16235, 23475, 13, 51735, 51735], "temperature": 0.0, "avg_logprob": -0.12220936605375107, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.007933060638606548}, {"id": 5, "seek": 2742, "start": 27.42, "end": 32.400000000000006, "text": " In this video, we'll take a look at an algorithm that can help you train your neural network", "tokens": [50364, 682, 341, 960, 11, 321, 603, 747, 257, 574, 412, 364, 9284, 300, 393, 854, 291, 3847, 428, 18161, 3209, 50613, 50613, 709, 4663, 813, 16235, 23475, 13, 50731, 50731, 9647, 336, 300, 341, 307, 264, 6114, 337, 472, 1823, 295, 16235, 23475, 13, 50999, 50999, 316, 13075, 261, 73, 307, 10588, 382, 261, 73, 3175, 264, 2539, 3314, 8961, 1413, 341, 14641, 13760, 51355, 51355, 1433, 13, 51425, 51425, 1012, 393, 321, 652, 341, 589, 754, 1101, 30, 51571, 51571], "temperature": 0.0, "avg_logprob": -0.12810964469450065, "compression_ratio": 1.5633802816901408, "no_speech_prob": 4.00601165893022e-05}, {"id": 6, "seek": 2742, "start": 32.400000000000006, "end": 34.760000000000005, "text": " much faster than gradient descent.", "tokens": [50364, 682, 341, 960, 11, 321, 603, 747, 257, 574, 412, 364, 9284, 300, 393, 854, 291, 3847, 428, 18161, 3209, 50613, 50613, 709, 4663, 813, 16235, 23475, 13, 50731, 50731, 9647, 336, 300, 341, 307, 264, 6114, 337, 472, 1823, 295, 16235, 23475, 13, 50999, 50999, 316, 13075, 261, 73, 307, 10588, 382, 261, 73, 3175, 264, 2539, 3314, 8961, 1413, 341, 14641, 13760, 51355, 51355, 1433, 13, 51425, 51425, 1012, 393, 321, 652, 341, 589, 754, 1101, 30, 51571, 51571], "temperature": 0.0, "avg_logprob": -0.12810964469450065, "compression_ratio": 1.5633802816901408, "no_speech_prob": 4.00601165893022e-05}, {"id": 7, "seek": 2742, "start": 34.760000000000005, "end": 40.120000000000005, "text": " Recall that this is the expression for one step of gradient descent.", "tokens": [50364, 682, 341, 960, 11, 321, 603, 747, 257, 574, 412, 364, 9284, 300, 393, 854, 291, 3847, 428, 18161, 3209, 50613, 50613, 709, 4663, 813, 16235, 23475, 13, 50731, 50731, 9647, 336, 300, 341, 307, 264, 6114, 337, 472, 1823, 295, 16235, 23475, 13, 50999, 50999, 316, 13075, 261, 73, 307, 10588, 382, 261, 73, 3175, 264, 2539, 3314, 8961, 1413, 341, 14641, 13760, 51355, 51355, 1433, 13, 51425, 51425, 1012, 393, 321, 652, 341, 589, 754, 1101, 30, 51571, 51571], "temperature": 0.0, "avg_logprob": -0.12810964469450065, "compression_ratio": 1.5633802816901408, "no_speech_prob": 4.00601165893022e-05}, {"id": 8, "seek": 2742, "start": 40.120000000000005, "end": 47.24, "text": " A parameter wj is updated as wj minus the learning rate alpha times this partial derivative", "tokens": [50364, 682, 341, 960, 11, 321, 603, 747, 257, 574, 412, 364, 9284, 300, 393, 854, 291, 3847, 428, 18161, 3209, 50613, 50613, 709, 4663, 813, 16235, 23475, 13, 50731, 50731, 9647, 336, 300, 341, 307, 264, 6114, 337, 472, 1823, 295, 16235, 23475, 13, 50999, 50999, 316, 13075, 261, 73, 307, 10588, 382, 261, 73, 3175, 264, 2539, 3314, 8961, 1413, 341, 14641, 13760, 51355, 51355, 1433, 13, 51425, 51425, 1012, 393, 321, 652, 341, 589, 754, 1101, 30, 51571, 51571], "temperature": 0.0, "avg_logprob": -0.12810964469450065, "compression_ratio": 1.5633802816901408, "no_speech_prob": 4.00601165893022e-05}, {"id": 9, "seek": 2742, "start": 47.24, "end": 48.64, "text": " term.", "tokens": [50364, 682, 341, 960, 11, 321, 603, 747, 257, 574, 412, 364, 9284, 300, 393, 854, 291, 3847, 428, 18161, 3209, 50613, 50613, 709, 4663, 813, 16235, 23475, 13, 50731, 50731, 9647, 336, 300, 341, 307, 264, 6114, 337, 472, 1823, 295, 16235, 23475, 13, 50999, 50999, 316, 13075, 261, 73, 307, 10588, 382, 261, 73, 3175, 264, 2539, 3314, 8961, 1413, 341, 14641, 13760, 51355, 51355, 1433, 13, 51425, 51425, 1012, 393, 321, 652, 341, 589, 754, 1101, 30, 51571, 51571], "temperature": 0.0, "avg_logprob": -0.12810964469450065, "compression_ratio": 1.5633802816901408, "no_speech_prob": 4.00601165893022e-05}, {"id": 10, "seek": 2742, "start": 48.64, "end": 51.56, "text": " How can we make this work even better?", "tokens": [50364, 682, 341, 960, 11, 321, 603, 747, 257, 574, 412, 364, 9284, 300, 393, 854, 291, 3847, 428, 18161, 3209, 50613, 50613, 709, 4663, 813, 16235, 23475, 13, 50731, 50731, 9647, 336, 300, 341, 307, 264, 6114, 337, 472, 1823, 295, 16235, 23475, 13, 50999, 50999, 316, 13075, 261, 73, 307, 10588, 382, 261, 73, 3175, 264, 2539, 3314, 8961, 1413, 341, 14641, 13760, 51355, 51355, 1433, 13, 51425, 51425, 1012, 393, 321, 652, 341, 589, 754, 1101, 30, 51571, 51571], "temperature": 0.0, "avg_logprob": -0.12810964469450065, "compression_ratio": 1.5633802816901408, "no_speech_prob": 4.00601165893022e-05}, {"id": 11, "seek": 5156, "start": 51.56, "end": 58.760000000000005, "text": " In this example, I plotted the cost function j using a contour plot comprising these ellipses.", "tokens": [50364, 682, 341, 1365, 11, 286, 43288, 264, 2063, 2445, 361, 1228, 257, 21234, 7542, 16802, 3436, 613, 8284, 2600, 279, 13, 50724, 50724, 400, 370, 264, 7285, 295, 341, 2063, 2445, 307, 412, 264, 3056, 295, 613, 8284, 2600, 279, 760, 510, 13, 51046, 51046, 823, 498, 291, 645, 281, 722, 16235, 23475, 760, 510, 11, 472, 1823, 295, 16235, 23475, 11, 498, 51386, 51386, 8961, 307, 1359, 11, 815, 747, 291, 257, 707, 857, 294, 300, 3513, 11, 550, 1071, 1823, 11, 550, 51600, 51600, 1071, 1823, 11, 550, 1071, 1823, 11, 550, 1071, 1823, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.1203299324111183, "compression_ratio": 1.9516908212560387, "no_speech_prob": 2.3551630874862894e-05}, {"id": 12, "seek": 5156, "start": 58.760000000000005, "end": 65.2, "text": " And so the minimum of this cost function is at the center of these ellipses down here.", "tokens": [50364, 682, 341, 1365, 11, 286, 43288, 264, 2063, 2445, 361, 1228, 257, 21234, 7542, 16802, 3436, 613, 8284, 2600, 279, 13, 50724, 50724, 400, 370, 264, 7285, 295, 341, 2063, 2445, 307, 412, 264, 3056, 295, 613, 8284, 2600, 279, 760, 510, 13, 51046, 51046, 823, 498, 291, 645, 281, 722, 16235, 23475, 760, 510, 11, 472, 1823, 295, 16235, 23475, 11, 498, 51386, 51386, 8961, 307, 1359, 11, 815, 747, 291, 257, 707, 857, 294, 300, 3513, 11, 550, 1071, 1823, 11, 550, 51600, 51600, 1071, 1823, 11, 550, 1071, 1823, 11, 550, 1071, 1823, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.1203299324111183, "compression_ratio": 1.9516908212560387, "no_speech_prob": 2.3551630874862894e-05}, {"id": 13, "seek": 5156, "start": 65.2, "end": 72.0, "text": " Now if you were to start gradient descent down here, one step of gradient descent, if", "tokens": [50364, 682, 341, 1365, 11, 286, 43288, 264, 2063, 2445, 361, 1228, 257, 21234, 7542, 16802, 3436, 613, 8284, 2600, 279, 13, 50724, 50724, 400, 370, 264, 7285, 295, 341, 2063, 2445, 307, 412, 264, 3056, 295, 613, 8284, 2600, 279, 760, 510, 13, 51046, 51046, 823, 498, 291, 645, 281, 722, 16235, 23475, 760, 510, 11, 472, 1823, 295, 16235, 23475, 11, 498, 51386, 51386, 8961, 307, 1359, 11, 815, 747, 291, 257, 707, 857, 294, 300, 3513, 11, 550, 1071, 1823, 11, 550, 51600, 51600, 1071, 1823, 11, 550, 1071, 1823, 11, 550, 1071, 1823, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.1203299324111183, "compression_ratio": 1.9516908212560387, "no_speech_prob": 2.3551630874862894e-05}, {"id": 14, "seek": 5156, "start": 72.0, "end": 76.28, "text": " alpha is small, may take you a little bit in that direction, then another step, then", "tokens": [50364, 682, 341, 1365, 11, 286, 43288, 264, 2063, 2445, 361, 1228, 257, 21234, 7542, 16802, 3436, 613, 8284, 2600, 279, 13, 50724, 50724, 400, 370, 264, 7285, 295, 341, 2063, 2445, 307, 412, 264, 3056, 295, 613, 8284, 2600, 279, 760, 510, 13, 51046, 51046, 823, 498, 291, 645, 281, 722, 16235, 23475, 760, 510, 11, 472, 1823, 295, 16235, 23475, 11, 498, 51386, 51386, 8961, 307, 1359, 11, 815, 747, 291, 257, 707, 857, 294, 300, 3513, 11, 550, 1071, 1823, 11, 550, 51600, 51600, 1071, 1823, 11, 550, 1071, 1823, 11, 550, 1071, 1823, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.1203299324111183, "compression_ratio": 1.9516908212560387, "no_speech_prob": 2.3551630874862894e-05}, {"id": 15, "seek": 5156, "start": 76.28, "end": 79.72, "text": " another step, then another step, then another step.", "tokens": [50364, 682, 341, 1365, 11, 286, 43288, 264, 2063, 2445, 361, 1228, 257, 21234, 7542, 16802, 3436, 613, 8284, 2600, 279, 13, 50724, 50724, 400, 370, 264, 7285, 295, 341, 2063, 2445, 307, 412, 264, 3056, 295, 613, 8284, 2600, 279, 760, 510, 13, 51046, 51046, 823, 498, 291, 645, 281, 722, 16235, 23475, 760, 510, 11, 472, 1823, 295, 16235, 23475, 11, 498, 51386, 51386, 8961, 307, 1359, 11, 815, 747, 291, 257, 707, 857, 294, 300, 3513, 11, 550, 1071, 1823, 11, 550, 51600, 51600, 1071, 1823, 11, 550, 1071, 1823, 11, 550, 1071, 1823, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.1203299324111183, "compression_ratio": 1.9516908212560387, "no_speech_prob": 2.3551630874862894e-05}, {"id": 16, "seek": 7972, "start": 79.72, "end": 85.24, "text": " And you notice that every single step of gradient descent is pretty much going in the same direction.", "tokens": [50364, 400, 291, 3449, 300, 633, 2167, 1823, 295, 16235, 23475, 307, 1238, 709, 516, 294, 264, 912, 3513, 13, 50640, 50640, 400, 498, 291, 536, 341, 281, 312, 264, 1389, 11, 291, 1062, 2441, 11, 731, 11, 983, 500, 380, 321, 652, 8961, 3801, 30, 50932, 50932, 1664, 321, 362, 364, 9284, 281, 6772, 3488, 8961, 281, 445, 652, 309, 747, 3801, 51158, 51158, 4439, 293, 483, 281, 264, 7285, 4663, 30, 51332, 51332, 821, 311, 364, 9284, 1219, 264, 7938, 9284, 300, 393, 360, 300, 13, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.1045479512476659, "compression_ratio": 1.6309012875536482, "no_speech_prob": 8.530227205483243e-06}, {"id": 17, "seek": 7972, "start": 85.24, "end": 91.08, "text": " And if you see this to be the case, you might wonder, well, why don't we make alpha bigger?", "tokens": [50364, 400, 291, 3449, 300, 633, 2167, 1823, 295, 16235, 23475, 307, 1238, 709, 516, 294, 264, 912, 3513, 13, 50640, 50640, 400, 498, 291, 536, 341, 281, 312, 264, 1389, 11, 291, 1062, 2441, 11, 731, 11, 983, 500, 380, 321, 652, 8961, 3801, 30, 50932, 50932, 1664, 321, 362, 364, 9284, 281, 6772, 3488, 8961, 281, 445, 652, 309, 747, 3801, 51158, 51158, 4439, 293, 483, 281, 264, 7285, 4663, 30, 51332, 51332, 821, 311, 364, 9284, 1219, 264, 7938, 9284, 300, 393, 360, 300, 13, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.1045479512476659, "compression_ratio": 1.6309012875536482, "no_speech_prob": 8.530227205483243e-06}, {"id": 18, "seek": 7972, "start": 91.08, "end": 95.6, "text": " Can we have an algorithm to automatically increase alpha to just make it take bigger", "tokens": [50364, 400, 291, 3449, 300, 633, 2167, 1823, 295, 16235, 23475, 307, 1238, 709, 516, 294, 264, 912, 3513, 13, 50640, 50640, 400, 498, 291, 536, 341, 281, 312, 264, 1389, 11, 291, 1062, 2441, 11, 731, 11, 983, 500, 380, 321, 652, 8961, 3801, 30, 50932, 50932, 1664, 321, 362, 364, 9284, 281, 6772, 3488, 8961, 281, 445, 652, 309, 747, 3801, 51158, 51158, 4439, 293, 483, 281, 264, 7285, 4663, 30, 51332, 51332, 821, 311, 364, 9284, 1219, 264, 7938, 9284, 300, 393, 360, 300, 13, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.1045479512476659, "compression_ratio": 1.6309012875536482, "no_speech_prob": 8.530227205483243e-06}, {"id": 19, "seek": 7972, "start": 95.6, "end": 99.08, "text": " steps and get to the minimum faster?", "tokens": [50364, 400, 291, 3449, 300, 633, 2167, 1823, 295, 16235, 23475, 307, 1238, 709, 516, 294, 264, 912, 3513, 13, 50640, 50640, 400, 498, 291, 536, 341, 281, 312, 264, 1389, 11, 291, 1062, 2441, 11, 731, 11, 983, 500, 380, 321, 652, 8961, 3801, 30, 50932, 50932, 1664, 321, 362, 364, 9284, 281, 6772, 3488, 8961, 281, 445, 652, 309, 747, 3801, 51158, 51158, 4439, 293, 483, 281, 264, 7285, 4663, 30, 51332, 51332, 821, 311, 364, 9284, 1219, 264, 7938, 9284, 300, 393, 360, 300, 13, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.1045479512476659, "compression_ratio": 1.6309012875536482, "no_speech_prob": 8.530227205483243e-06}, {"id": 20, "seek": 7972, "start": 99.08, "end": 104.52, "text": " There's an algorithm called the Adam algorithm that can do that.", "tokens": [50364, 400, 291, 3449, 300, 633, 2167, 1823, 295, 16235, 23475, 307, 1238, 709, 516, 294, 264, 912, 3513, 13, 50640, 50640, 400, 498, 291, 536, 341, 281, 312, 264, 1389, 11, 291, 1062, 2441, 11, 731, 11, 983, 500, 380, 321, 652, 8961, 3801, 30, 50932, 50932, 1664, 321, 362, 364, 9284, 281, 6772, 3488, 8961, 281, 445, 652, 309, 747, 3801, 51158, 51158, 4439, 293, 483, 281, 264, 7285, 4663, 30, 51332, 51332, 821, 311, 364, 9284, 1219, 264, 7938, 9284, 300, 393, 360, 300, 13, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.1045479512476659, "compression_ratio": 1.6309012875536482, "no_speech_prob": 8.530227205483243e-06}, {"id": 21, "seek": 10452, "start": 104.52, "end": 110.11999999999999, "text": " If it sees that the learning rate is too small, and we are just taking tiny little steps in", "tokens": [50364, 759, 309, 8194, 300, 264, 2539, 3314, 307, 886, 1359, 11, 293, 321, 366, 445, 1940, 5870, 707, 4439, 294, 50644, 50644, 257, 2531, 3513, 670, 293, 670, 11, 321, 820, 445, 652, 264, 2539, 3314, 8961, 3801, 13, 50954, 50954, 682, 8712, 11, 510, 797, 307, 264, 912, 2063, 2445, 13, 51186, 51186, 759, 321, 645, 2891, 510, 293, 632, 257, 7226, 955, 2539, 3314, 8961, 11, 550, 1310, 472, 1823, 51452, 51452, 295, 16235, 23475, 2516, 505, 510, 11, 293, 264, 1150, 1823, 2516, 505, 510, 11, 2636, 1823, 11, 293, 51668, 51668, 264, 6409, 1823, 11, 293, 264, 9266, 1823, 11, 293, 264, 15102, 1823, 13, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.11277428348507501, "compression_ratio": 1.9372384937238494, "no_speech_prob": 3.844904767902335e-06}, {"id": 22, "seek": 10452, "start": 110.11999999999999, "end": 116.32, "text": " a similar direction over and over, we should just make the learning rate alpha bigger.", "tokens": [50364, 759, 309, 8194, 300, 264, 2539, 3314, 307, 886, 1359, 11, 293, 321, 366, 445, 1940, 5870, 707, 4439, 294, 50644, 50644, 257, 2531, 3513, 670, 293, 670, 11, 321, 820, 445, 652, 264, 2539, 3314, 8961, 3801, 13, 50954, 50954, 682, 8712, 11, 510, 797, 307, 264, 912, 2063, 2445, 13, 51186, 51186, 759, 321, 645, 2891, 510, 293, 632, 257, 7226, 955, 2539, 3314, 8961, 11, 550, 1310, 472, 1823, 51452, 51452, 295, 16235, 23475, 2516, 505, 510, 11, 293, 264, 1150, 1823, 2516, 505, 510, 11, 2636, 1823, 11, 293, 51668, 51668, 264, 6409, 1823, 11, 293, 264, 9266, 1823, 11, 293, 264, 15102, 1823, 13, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.11277428348507501, "compression_ratio": 1.9372384937238494, "no_speech_prob": 3.844904767902335e-06}, {"id": 23, "seek": 10452, "start": 116.32, "end": 120.96, "text": " In contrast, here again is the same cost function.", "tokens": [50364, 759, 309, 8194, 300, 264, 2539, 3314, 307, 886, 1359, 11, 293, 321, 366, 445, 1940, 5870, 707, 4439, 294, 50644, 50644, 257, 2531, 3513, 670, 293, 670, 11, 321, 820, 445, 652, 264, 2539, 3314, 8961, 3801, 13, 50954, 50954, 682, 8712, 11, 510, 797, 307, 264, 912, 2063, 2445, 13, 51186, 51186, 759, 321, 645, 2891, 510, 293, 632, 257, 7226, 955, 2539, 3314, 8961, 11, 550, 1310, 472, 1823, 51452, 51452, 295, 16235, 23475, 2516, 505, 510, 11, 293, 264, 1150, 1823, 2516, 505, 510, 11, 2636, 1823, 11, 293, 51668, 51668, 264, 6409, 1823, 11, 293, 264, 9266, 1823, 11, 293, 264, 15102, 1823, 13, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.11277428348507501, "compression_ratio": 1.9372384937238494, "no_speech_prob": 3.844904767902335e-06}, {"id": 24, "seek": 10452, "start": 120.96, "end": 126.28, "text": " If we were starting here and had a relatively big learning rate alpha, then maybe one step", "tokens": [50364, 759, 309, 8194, 300, 264, 2539, 3314, 307, 886, 1359, 11, 293, 321, 366, 445, 1940, 5870, 707, 4439, 294, 50644, 50644, 257, 2531, 3513, 670, 293, 670, 11, 321, 820, 445, 652, 264, 2539, 3314, 8961, 3801, 13, 50954, 50954, 682, 8712, 11, 510, 797, 307, 264, 912, 2063, 2445, 13, 51186, 51186, 759, 321, 645, 2891, 510, 293, 632, 257, 7226, 955, 2539, 3314, 8961, 11, 550, 1310, 472, 1823, 51452, 51452, 295, 16235, 23475, 2516, 505, 510, 11, 293, 264, 1150, 1823, 2516, 505, 510, 11, 2636, 1823, 11, 293, 51668, 51668, 264, 6409, 1823, 11, 293, 264, 9266, 1823, 11, 293, 264, 15102, 1823, 13, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.11277428348507501, "compression_ratio": 1.9372384937238494, "no_speech_prob": 3.844904767902335e-06}, {"id": 25, "seek": 10452, "start": 126.28, "end": 130.6, "text": " of gradient descent takes us here, and the second step takes us here, third step, and", "tokens": [50364, 759, 309, 8194, 300, 264, 2539, 3314, 307, 886, 1359, 11, 293, 321, 366, 445, 1940, 5870, 707, 4439, 294, 50644, 50644, 257, 2531, 3513, 670, 293, 670, 11, 321, 820, 445, 652, 264, 2539, 3314, 8961, 3801, 13, 50954, 50954, 682, 8712, 11, 510, 797, 307, 264, 912, 2063, 2445, 13, 51186, 51186, 759, 321, 645, 2891, 510, 293, 632, 257, 7226, 955, 2539, 3314, 8961, 11, 550, 1310, 472, 1823, 51452, 51452, 295, 16235, 23475, 2516, 505, 510, 11, 293, 264, 1150, 1823, 2516, 505, 510, 11, 2636, 1823, 11, 293, 51668, 51668, 264, 6409, 1823, 11, 293, 264, 9266, 1823, 11, 293, 264, 15102, 1823, 13, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.11277428348507501, "compression_ratio": 1.9372384937238494, "no_speech_prob": 3.844904767902335e-06}, {"id": 26, "seek": 10452, "start": 130.6, "end": 134.12, "text": " the fourth step, and the fifth step, and the sixth step.", "tokens": [50364, 759, 309, 8194, 300, 264, 2539, 3314, 307, 886, 1359, 11, 293, 321, 366, 445, 1940, 5870, 707, 4439, 294, 50644, 50644, 257, 2531, 3513, 670, 293, 670, 11, 321, 820, 445, 652, 264, 2539, 3314, 8961, 3801, 13, 50954, 50954, 682, 8712, 11, 510, 797, 307, 264, 912, 2063, 2445, 13, 51186, 51186, 759, 321, 645, 2891, 510, 293, 632, 257, 7226, 955, 2539, 3314, 8961, 11, 550, 1310, 472, 1823, 51452, 51452, 295, 16235, 23475, 2516, 505, 510, 11, 293, 264, 1150, 1823, 2516, 505, 510, 11, 2636, 1823, 11, 293, 51668, 51668, 264, 6409, 1823, 11, 293, 264, 9266, 1823, 11, 293, 264, 15102, 1823, 13, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.11277428348507501, "compression_ratio": 1.9372384937238494, "no_speech_prob": 3.844904767902335e-06}, {"id": 27, "seek": 13412, "start": 134.12, "end": 138.76, "text": " And if you see gradient descent doing this, it's oscillating back and forth, you'd be", "tokens": [50364, 400, 498, 291, 536, 16235, 23475, 884, 341, 11, 309, 311, 18225, 990, 646, 293, 5220, 11, 291, 1116, 312, 50596, 50596, 29941, 281, 584, 11, 731, 11, 983, 500, 380, 321, 652, 264, 2539, 3314, 4356, 30, 50778, 50778, 400, 264, 7938, 9284, 393, 611, 360, 300, 6772, 13, 50952, 50952, 400, 365, 257, 4356, 2539, 3314, 11, 291, 393, 550, 747, 257, 544, 5508, 3100, 7361, 264, 7285, 51229, 51229, 295, 264, 2063, 2445, 13, 51324, 51324, 407, 5413, 322, 577, 16235, 23475, 307, 41163, 11, 2171, 291, 3172, 291, 632, 257, 3801, 2539, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.08167988777160645, "compression_ratio": 1.7049180327868851, "no_speech_prob": 3.2887235192902153e-06}, {"id": 28, "seek": 13412, "start": 138.76, "end": 142.4, "text": " tempted to say, well, why don't we make the learning rate smaller?", "tokens": [50364, 400, 498, 291, 536, 16235, 23475, 884, 341, 11, 309, 311, 18225, 990, 646, 293, 5220, 11, 291, 1116, 312, 50596, 50596, 29941, 281, 584, 11, 731, 11, 983, 500, 380, 321, 652, 264, 2539, 3314, 4356, 30, 50778, 50778, 400, 264, 7938, 9284, 393, 611, 360, 300, 6772, 13, 50952, 50952, 400, 365, 257, 4356, 2539, 3314, 11, 291, 393, 550, 747, 257, 544, 5508, 3100, 7361, 264, 7285, 51229, 51229, 295, 264, 2063, 2445, 13, 51324, 51324, 407, 5413, 322, 577, 16235, 23475, 307, 41163, 11, 2171, 291, 3172, 291, 632, 257, 3801, 2539, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.08167988777160645, "compression_ratio": 1.7049180327868851, "no_speech_prob": 3.2887235192902153e-06}, {"id": 29, "seek": 13412, "start": 142.4, "end": 145.88, "text": " And the Adam algorithm can also do that automatically.", "tokens": [50364, 400, 498, 291, 536, 16235, 23475, 884, 341, 11, 309, 311, 18225, 990, 646, 293, 5220, 11, 291, 1116, 312, 50596, 50596, 29941, 281, 584, 11, 731, 11, 983, 500, 380, 321, 652, 264, 2539, 3314, 4356, 30, 50778, 50778, 400, 264, 7938, 9284, 393, 611, 360, 300, 6772, 13, 50952, 50952, 400, 365, 257, 4356, 2539, 3314, 11, 291, 393, 550, 747, 257, 544, 5508, 3100, 7361, 264, 7285, 51229, 51229, 295, 264, 2063, 2445, 13, 51324, 51324, 407, 5413, 322, 577, 16235, 23475, 307, 41163, 11, 2171, 291, 3172, 291, 632, 257, 3801, 2539, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.08167988777160645, "compression_ratio": 1.7049180327868851, "no_speech_prob": 3.2887235192902153e-06}, {"id": 30, "seek": 13412, "start": 145.88, "end": 151.42000000000002, "text": " And with a smaller learning rate, you can then take a more smooth path toward the minimum", "tokens": [50364, 400, 498, 291, 536, 16235, 23475, 884, 341, 11, 309, 311, 18225, 990, 646, 293, 5220, 11, 291, 1116, 312, 50596, 50596, 29941, 281, 584, 11, 731, 11, 983, 500, 380, 321, 652, 264, 2539, 3314, 4356, 30, 50778, 50778, 400, 264, 7938, 9284, 393, 611, 360, 300, 6772, 13, 50952, 50952, 400, 365, 257, 4356, 2539, 3314, 11, 291, 393, 550, 747, 257, 544, 5508, 3100, 7361, 264, 7285, 51229, 51229, 295, 264, 2063, 2445, 13, 51324, 51324, 407, 5413, 322, 577, 16235, 23475, 307, 41163, 11, 2171, 291, 3172, 291, 632, 257, 3801, 2539, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.08167988777160645, "compression_ratio": 1.7049180327868851, "no_speech_prob": 3.2887235192902153e-06}, {"id": 31, "seek": 13412, "start": 151.42000000000002, "end": 153.32, "text": " of the cost function.", "tokens": [50364, 400, 498, 291, 536, 16235, 23475, 884, 341, 11, 309, 311, 18225, 990, 646, 293, 5220, 11, 291, 1116, 312, 50596, 50596, 29941, 281, 584, 11, 731, 11, 983, 500, 380, 321, 652, 264, 2539, 3314, 4356, 30, 50778, 50778, 400, 264, 7938, 9284, 393, 611, 360, 300, 6772, 13, 50952, 50952, 400, 365, 257, 4356, 2539, 3314, 11, 291, 393, 550, 747, 257, 544, 5508, 3100, 7361, 264, 7285, 51229, 51229, 295, 264, 2063, 2445, 13, 51324, 51324, 407, 5413, 322, 577, 16235, 23475, 307, 41163, 11, 2171, 291, 3172, 291, 632, 257, 3801, 2539, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.08167988777160645, "compression_ratio": 1.7049180327868851, "no_speech_prob": 3.2887235192902153e-06}, {"id": 32, "seek": 13412, "start": 153.32, "end": 158.44, "text": " So depending on how gradient descent is proceeding, sometimes you wish you had a bigger learning", "tokens": [50364, 400, 498, 291, 536, 16235, 23475, 884, 341, 11, 309, 311, 18225, 990, 646, 293, 5220, 11, 291, 1116, 312, 50596, 50596, 29941, 281, 584, 11, 731, 11, 983, 500, 380, 321, 652, 264, 2539, 3314, 4356, 30, 50778, 50778, 400, 264, 7938, 9284, 393, 611, 360, 300, 6772, 13, 50952, 50952, 400, 365, 257, 4356, 2539, 3314, 11, 291, 393, 550, 747, 257, 544, 5508, 3100, 7361, 264, 7285, 51229, 51229, 295, 264, 2063, 2445, 13, 51324, 51324, 407, 5413, 322, 577, 16235, 23475, 307, 41163, 11, 2171, 291, 3172, 291, 632, 257, 3801, 2539, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.08167988777160645, "compression_ratio": 1.7049180327868851, "no_speech_prob": 3.2887235192902153e-06}, {"id": 33, "seek": 15844, "start": 158.44, "end": 164.16, "text": " rate alpha, and sometimes you wish you had a smaller learning rate alpha.", "tokens": [50364, 3314, 8961, 11, 293, 2171, 291, 3172, 291, 632, 257, 4356, 2539, 3314, 8961, 13, 50650, 50650, 407, 264, 7938, 9284, 393, 4369, 264, 2539, 3314, 6772, 13, 50844, 50844, 7938, 7382, 337, 49643, 488, 19093, 4410, 332, 399, 11, 420, 9135, 2865, 13, 51166, 51166, 400, 500, 380, 3292, 886, 709, 466, 437, 341, 1315, 1355, 11, 309, 311, 445, 437, 264, 16552, 632, 1219, 51384, 51384, 341, 9284, 13, 51474, 51474, 583, 25873, 11, 264, 7938, 9284, 1177, 380, 764, 257, 2167, 4338, 2539, 3314, 8961, 11, 309, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.1325073850915787, "compression_ratio": 1.7443946188340806, "no_speech_prob": 6.6433312895242125e-06}, {"id": 34, "seek": 15844, "start": 164.16, "end": 168.04, "text": " So the Adam algorithm can adjust the learning rate automatically.", "tokens": [50364, 3314, 8961, 11, 293, 2171, 291, 3172, 291, 632, 257, 4356, 2539, 3314, 8961, 13, 50650, 50650, 407, 264, 7938, 9284, 393, 4369, 264, 2539, 3314, 6772, 13, 50844, 50844, 7938, 7382, 337, 49643, 488, 19093, 4410, 332, 399, 11, 420, 9135, 2865, 13, 51166, 51166, 400, 500, 380, 3292, 886, 709, 466, 437, 341, 1315, 1355, 11, 309, 311, 445, 437, 264, 16552, 632, 1219, 51384, 51384, 341, 9284, 13, 51474, 51474, 583, 25873, 11, 264, 7938, 9284, 1177, 380, 764, 257, 2167, 4338, 2539, 3314, 8961, 11, 309, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.1325073850915787, "compression_ratio": 1.7443946188340806, "no_speech_prob": 6.6433312895242125e-06}, {"id": 35, "seek": 15844, "start": 168.04, "end": 174.48, "text": " Adam stands for Adaptive Moment Estimation, or ADAM.", "tokens": [50364, 3314, 8961, 11, 293, 2171, 291, 3172, 291, 632, 257, 4356, 2539, 3314, 8961, 13, 50650, 50650, 407, 264, 7938, 9284, 393, 4369, 264, 2539, 3314, 6772, 13, 50844, 50844, 7938, 7382, 337, 49643, 488, 19093, 4410, 332, 399, 11, 420, 9135, 2865, 13, 51166, 51166, 400, 500, 380, 3292, 886, 709, 466, 437, 341, 1315, 1355, 11, 309, 311, 445, 437, 264, 16552, 632, 1219, 51384, 51384, 341, 9284, 13, 51474, 51474, 583, 25873, 11, 264, 7938, 9284, 1177, 380, 764, 257, 2167, 4338, 2539, 3314, 8961, 11, 309, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.1325073850915787, "compression_ratio": 1.7443946188340806, "no_speech_prob": 6.6433312895242125e-06}, {"id": 36, "seek": 15844, "start": 174.48, "end": 178.84, "text": " And don't worry too much about what this name means, it's just what the authors had called", "tokens": [50364, 3314, 8961, 11, 293, 2171, 291, 3172, 291, 632, 257, 4356, 2539, 3314, 8961, 13, 50650, 50650, 407, 264, 7938, 9284, 393, 4369, 264, 2539, 3314, 6772, 13, 50844, 50844, 7938, 7382, 337, 49643, 488, 19093, 4410, 332, 399, 11, 420, 9135, 2865, 13, 51166, 51166, 400, 500, 380, 3292, 886, 709, 466, 437, 341, 1315, 1355, 11, 309, 311, 445, 437, 264, 16552, 632, 1219, 51384, 51384, 341, 9284, 13, 51474, 51474, 583, 25873, 11, 264, 7938, 9284, 1177, 380, 764, 257, 2167, 4338, 2539, 3314, 8961, 11, 309, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.1325073850915787, "compression_ratio": 1.7443946188340806, "no_speech_prob": 6.6433312895242125e-06}, {"id": 37, "seek": 15844, "start": 178.84, "end": 180.64, "text": " this algorithm.", "tokens": [50364, 3314, 8961, 11, 293, 2171, 291, 3172, 291, 632, 257, 4356, 2539, 3314, 8961, 13, 50650, 50650, 407, 264, 7938, 9284, 393, 4369, 264, 2539, 3314, 6772, 13, 50844, 50844, 7938, 7382, 337, 49643, 488, 19093, 4410, 332, 399, 11, 420, 9135, 2865, 13, 51166, 51166, 400, 500, 380, 3292, 886, 709, 466, 437, 341, 1315, 1355, 11, 309, 311, 445, 437, 264, 16552, 632, 1219, 51384, 51384, 341, 9284, 13, 51474, 51474, 583, 25873, 11, 264, 7938, 9284, 1177, 380, 764, 257, 2167, 4338, 2539, 3314, 8961, 11, 309, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.1325073850915787, "compression_ratio": 1.7443946188340806, "no_speech_prob": 6.6433312895242125e-06}, {"id": 38, "seek": 15844, "start": 180.64, "end": 186.12, "text": " But interestingly, the Adam algorithm doesn't use a single global learning rate alpha, it", "tokens": [50364, 3314, 8961, 11, 293, 2171, 291, 3172, 291, 632, 257, 4356, 2539, 3314, 8961, 13, 50650, 50650, 407, 264, 7938, 9284, 393, 4369, 264, 2539, 3314, 6772, 13, 50844, 50844, 7938, 7382, 337, 49643, 488, 19093, 4410, 332, 399, 11, 420, 9135, 2865, 13, 51166, 51166, 400, 500, 380, 3292, 886, 709, 466, 437, 341, 1315, 1355, 11, 309, 311, 445, 437, 264, 16552, 632, 1219, 51384, 51384, 341, 9284, 13, 51474, 51474, 583, 25873, 11, 264, 7938, 9284, 1177, 380, 764, 257, 2167, 4338, 2539, 3314, 8961, 11, 309, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.1325073850915787, "compression_ratio": 1.7443946188340806, "no_speech_prob": 6.6433312895242125e-06}, {"id": 39, "seek": 18612, "start": 186.12, "end": 190.96, "text": " uses a different learning rates for every single parameter of your model.", "tokens": [50364, 4960, 257, 819, 2539, 6846, 337, 633, 2167, 13075, 295, 428, 2316, 13, 50606, 50606, 407, 498, 291, 362, 9834, 261, 16, 807, 261, 3279, 11, 382, 731, 382, 272, 11, 550, 309, 767, 575, 2975, 2539, 50916, 50916, 3314, 9834, 11, 8961, 502, 11, 8961, 568, 11, 439, 264, 636, 807, 8961, 1266, 11, 337, 261, 16, 807, 261, 3279, 11, 51258, 51258, 382, 731, 382, 286, 603, 818, 309, 8961, 2975, 337, 264, 13075, 272, 13, 51510, 51510, 400, 264, 24002, 2261, 264, 7938, 9284, 307, 498, 257, 13075, 261, 73, 420, 272, 2544, 281, 1066, 322, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.15700784701745488, "compression_ratio": 1.8130841121495327, "no_speech_prob": 4.785052169609116e-06}, {"id": 40, "seek": 18612, "start": 190.96, "end": 197.16, "text": " So if you have parameters w1 through w10, as well as b, then it actually has 11 learning", "tokens": [50364, 4960, 257, 819, 2539, 6846, 337, 633, 2167, 13075, 295, 428, 2316, 13, 50606, 50606, 407, 498, 291, 362, 9834, 261, 16, 807, 261, 3279, 11, 382, 731, 382, 272, 11, 550, 309, 767, 575, 2975, 2539, 50916, 50916, 3314, 9834, 11, 8961, 502, 11, 8961, 568, 11, 439, 264, 636, 807, 8961, 1266, 11, 337, 261, 16, 807, 261, 3279, 11, 51258, 51258, 382, 731, 382, 286, 603, 818, 309, 8961, 2975, 337, 264, 13075, 272, 13, 51510, 51510, 400, 264, 24002, 2261, 264, 7938, 9284, 307, 498, 257, 13075, 261, 73, 420, 272, 2544, 281, 1066, 322, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.15700784701745488, "compression_ratio": 1.8130841121495327, "no_speech_prob": 4.785052169609116e-06}, {"id": 41, "seek": 18612, "start": 197.16, "end": 204.0, "text": " rate parameters, alpha 1, alpha 2, all the way through alpha 10, for w1 through w10,", "tokens": [50364, 4960, 257, 819, 2539, 6846, 337, 633, 2167, 13075, 295, 428, 2316, 13, 50606, 50606, 407, 498, 291, 362, 9834, 261, 16, 807, 261, 3279, 11, 382, 731, 382, 272, 11, 550, 309, 767, 575, 2975, 2539, 50916, 50916, 3314, 9834, 11, 8961, 502, 11, 8961, 568, 11, 439, 264, 636, 807, 8961, 1266, 11, 337, 261, 16, 807, 261, 3279, 11, 51258, 51258, 382, 731, 382, 286, 603, 818, 309, 8961, 2975, 337, 264, 13075, 272, 13, 51510, 51510, 400, 264, 24002, 2261, 264, 7938, 9284, 307, 498, 257, 13075, 261, 73, 420, 272, 2544, 281, 1066, 322, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.15700784701745488, "compression_ratio": 1.8130841121495327, "no_speech_prob": 4.785052169609116e-06}, {"id": 42, "seek": 18612, "start": 204.0, "end": 209.04000000000002, "text": " as well as I'll call it alpha 11 for the parameter b.", "tokens": [50364, 4960, 257, 819, 2539, 6846, 337, 633, 2167, 13075, 295, 428, 2316, 13, 50606, 50606, 407, 498, 291, 362, 9834, 261, 16, 807, 261, 3279, 11, 382, 731, 382, 272, 11, 550, 309, 767, 575, 2975, 2539, 50916, 50916, 3314, 9834, 11, 8961, 502, 11, 8961, 568, 11, 439, 264, 636, 807, 8961, 1266, 11, 337, 261, 16, 807, 261, 3279, 11, 51258, 51258, 382, 731, 382, 286, 603, 818, 309, 8961, 2975, 337, 264, 13075, 272, 13, 51510, 51510, 400, 264, 24002, 2261, 264, 7938, 9284, 307, 498, 257, 13075, 261, 73, 420, 272, 2544, 281, 1066, 322, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.15700784701745488, "compression_ratio": 1.8130841121495327, "no_speech_prob": 4.785052169609116e-06}, {"id": 43, "seek": 18612, "start": 209.04000000000002, "end": 216.08, "text": " And the intuition behind the Adam algorithm is if a parameter wj or b seems to keep on", "tokens": [50364, 4960, 257, 819, 2539, 6846, 337, 633, 2167, 13075, 295, 428, 2316, 13, 50606, 50606, 407, 498, 291, 362, 9834, 261, 16, 807, 261, 3279, 11, 382, 731, 382, 272, 11, 550, 309, 767, 575, 2975, 2539, 50916, 50916, 3314, 9834, 11, 8961, 502, 11, 8961, 568, 11, 439, 264, 636, 807, 8961, 1266, 11, 337, 261, 16, 807, 261, 3279, 11, 51258, 51258, 382, 731, 382, 286, 603, 818, 309, 8961, 2975, 337, 264, 13075, 272, 13, 51510, 51510, 400, 264, 24002, 2261, 264, 7938, 9284, 307, 498, 257, 13075, 261, 73, 420, 272, 2544, 281, 1066, 322, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.15700784701745488, "compression_ratio": 1.8130841121495327, "no_speech_prob": 4.785052169609116e-06}, {"id": 44, "seek": 21608, "start": 216.08, "end": 221.84, "text": " moving in roughly the same direction, this is what we saw on the first example on the", "tokens": [50364, 2684, 294, 9810, 264, 912, 3513, 11, 341, 307, 437, 321, 1866, 322, 264, 700, 1365, 322, 264, 50652, 50652, 3894, 4137, 13, 50702, 50702, 583, 498, 309, 2544, 281, 1066, 322, 2684, 294, 9810, 264, 912, 3513, 11, 718, 311, 3488, 264, 2539, 50918, 50918, 3314, 337, 300, 13075, 13, 50978, 50978, 961, 311, 352, 4663, 294, 300, 3513, 13, 51094, 51094, 33247, 736, 11, 498, 257, 13075, 5965, 18225, 990, 646, 293, 5220, 11, 341, 307, 437, 291, 1866, 294, 264, 51366, 51366, 1150, 1365, 322, 264, 3894, 4137, 11, 550, 718, 311, 406, 362, 309, 1066, 322, 18225, 990, 420, 27380, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.11102877722846137, "compression_ratio": 1.9464285714285714, "no_speech_prob": 1.112549398385454e-05}, {"id": 45, "seek": 21608, "start": 221.84, "end": 222.84, "text": " previous slide.", "tokens": [50364, 2684, 294, 9810, 264, 912, 3513, 11, 341, 307, 437, 321, 1866, 322, 264, 700, 1365, 322, 264, 50652, 50652, 3894, 4137, 13, 50702, 50702, 583, 498, 309, 2544, 281, 1066, 322, 2684, 294, 9810, 264, 912, 3513, 11, 718, 311, 3488, 264, 2539, 50918, 50918, 3314, 337, 300, 13075, 13, 50978, 50978, 961, 311, 352, 4663, 294, 300, 3513, 13, 51094, 51094, 33247, 736, 11, 498, 257, 13075, 5965, 18225, 990, 646, 293, 5220, 11, 341, 307, 437, 291, 1866, 294, 264, 51366, 51366, 1150, 1365, 322, 264, 3894, 4137, 11, 550, 718, 311, 406, 362, 309, 1066, 322, 18225, 990, 420, 27380, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.11102877722846137, "compression_ratio": 1.9464285714285714, "no_speech_prob": 1.112549398385454e-05}, {"id": 46, "seek": 21608, "start": 222.84, "end": 227.16000000000003, "text": " But if it seems to keep on moving in roughly the same direction, let's increase the learning", "tokens": [50364, 2684, 294, 9810, 264, 912, 3513, 11, 341, 307, 437, 321, 1866, 322, 264, 700, 1365, 322, 264, 50652, 50652, 3894, 4137, 13, 50702, 50702, 583, 498, 309, 2544, 281, 1066, 322, 2684, 294, 9810, 264, 912, 3513, 11, 718, 311, 3488, 264, 2539, 50918, 50918, 3314, 337, 300, 13075, 13, 50978, 50978, 961, 311, 352, 4663, 294, 300, 3513, 13, 51094, 51094, 33247, 736, 11, 498, 257, 13075, 5965, 18225, 990, 646, 293, 5220, 11, 341, 307, 437, 291, 1866, 294, 264, 51366, 51366, 1150, 1365, 322, 264, 3894, 4137, 11, 550, 718, 311, 406, 362, 309, 1066, 322, 18225, 990, 420, 27380, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.11102877722846137, "compression_ratio": 1.9464285714285714, "no_speech_prob": 1.112549398385454e-05}, {"id": 47, "seek": 21608, "start": 227.16000000000003, "end": 228.36, "text": " rate for that parameter.", "tokens": [50364, 2684, 294, 9810, 264, 912, 3513, 11, 341, 307, 437, 321, 1866, 322, 264, 700, 1365, 322, 264, 50652, 50652, 3894, 4137, 13, 50702, 50702, 583, 498, 309, 2544, 281, 1066, 322, 2684, 294, 9810, 264, 912, 3513, 11, 718, 311, 3488, 264, 2539, 50918, 50918, 3314, 337, 300, 13075, 13, 50978, 50978, 961, 311, 352, 4663, 294, 300, 3513, 13, 51094, 51094, 33247, 736, 11, 498, 257, 13075, 5965, 18225, 990, 646, 293, 5220, 11, 341, 307, 437, 291, 1866, 294, 264, 51366, 51366, 1150, 1365, 322, 264, 3894, 4137, 11, 550, 718, 311, 406, 362, 309, 1066, 322, 18225, 990, 420, 27380, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.11102877722846137, "compression_ratio": 1.9464285714285714, "no_speech_prob": 1.112549398385454e-05}, {"id": 48, "seek": 21608, "start": 228.36, "end": 230.68, "text": " Let's go faster in that direction.", "tokens": [50364, 2684, 294, 9810, 264, 912, 3513, 11, 341, 307, 437, 321, 1866, 322, 264, 700, 1365, 322, 264, 50652, 50652, 3894, 4137, 13, 50702, 50702, 583, 498, 309, 2544, 281, 1066, 322, 2684, 294, 9810, 264, 912, 3513, 11, 718, 311, 3488, 264, 2539, 50918, 50918, 3314, 337, 300, 13075, 13, 50978, 50978, 961, 311, 352, 4663, 294, 300, 3513, 13, 51094, 51094, 33247, 736, 11, 498, 257, 13075, 5965, 18225, 990, 646, 293, 5220, 11, 341, 307, 437, 291, 1866, 294, 264, 51366, 51366, 1150, 1365, 322, 264, 3894, 4137, 11, 550, 718, 311, 406, 362, 309, 1066, 322, 18225, 990, 420, 27380, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.11102877722846137, "compression_ratio": 1.9464285714285714, "no_speech_prob": 1.112549398385454e-05}, {"id": 49, "seek": 21608, "start": 230.68, "end": 236.12, "text": " Conversely, if a parameter keeps oscillating back and forth, this is what you saw in the", "tokens": [50364, 2684, 294, 9810, 264, 912, 3513, 11, 341, 307, 437, 321, 1866, 322, 264, 700, 1365, 322, 264, 50652, 50652, 3894, 4137, 13, 50702, 50702, 583, 498, 309, 2544, 281, 1066, 322, 2684, 294, 9810, 264, 912, 3513, 11, 718, 311, 3488, 264, 2539, 50918, 50918, 3314, 337, 300, 13075, 13, 50978, 50978, 961, 311, 352, 4663, 294, 300, 3513, 13, 51094, 51094, 33247, 736, 11, 498, 257, 13075, 5965, 18225, 990, 646, 293, 5220, 11, 341, 307, 437, 291, 1866, 294, 264, 51366, 51366, 1150, 1365, 322, 264, 3894, 4137, 11, 550, 718, 311, 406, 362, 309, 1066, 322, 18225, 990, 420, 27380, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.11102877722846137, "compression_ratio": 1.9464285714285714, "no_speech_prob": 1.112549398385454e-05}, {"id": 50, "seek": 21608, "start": 236.12, "end": 241.84, "text": " second example on the previous slide, then let's not have it keep on oscillating or bouncing", "tokens": [50364, 2684, 294, 9810, 264, 912, 3513, 11, 341, 307, 437, 321, 1866, 322, 264, 700, 1365, 322, 264, 50652, 50652, 3894, 4137, 13, 50702, 50702, 583, 498, 309, 2544, 281, 1066, 322, 2684, 294, 9810, 264, 912, 3513, 11, 718, 311, 3488, 264, 2539, 50918, 50918, 3314, 337, 300, 13075, 13, 50978, 50978, 961, 311, 352, 4663, 294, 300, 3513, 13, 51094, 51094, 33247, 736, 11, 498, 257, 13075, 5965, 18225, 990, 646, 293, 5220, 11, 341, 307, 437, 291, 1866, 294, 264, 51366, 51366, 1150, 1365, 322, 264, 3894, 4137, 11, 550, 718, 311, 406, 362, 309, 1066, 322, 18225, 990, 420, 27380, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.11102877722846137, "compression_ratio": 1.9464285714285714, "no_speech_prob": 1.112549398385454e-05}, {"id": 51, "seek": 24184, "start": 241.84, "end": 248.28, "text": " back and forth, let's reduce alpha j for that parameter a little bit.", "tokens": [50364, 646, 293, 5220, 11, 718, 311, 5407, 8961, 361, 337, 300, 13075, 257, 707, 857, 13, 50686, 50686, 440, 4365, 295, 577, 7938, 775, 341, 366, 257, 857, 6179, 293, 4399, 264, 11923, 295, 341, 1164, 13, 50978, 50978, 583, 498, 291, 747, 512, 544, 7339, 2452, 2539, 5359, 1780, 11, 291, 815, 1466, 544, 466, 264, 51180, 51180, 4365, 295, 341, 7938, 9284, 13, 51320, 51320, 583, 294, 3089, 11, 341, 307, 577, 291, 576, 4445, 309, 13, 51476, 51476, 440, 2316, 307, 2293, 264, 912, 382, 949, 13, 51645, 51645], "temperature": 0.0, "avg_logprob": -0.1104843870122382, "compression_ratio": 1.6462882096069869, "no_speech_prob": 5.594238700723508e-06}, {"id": 52, "seek": 24184, "start": 248.28, "end": 254.12, "text": " The details of how Adam does this are a bit complicated and beyond the scope of this course.", "tokens": [50364, 646, 293, 5220, 11, 718, 311, 5407, 8961, 361, 337, 300, 13075, 257, 707, 857, 13, 50686, 50686, 440, 4365, 295, 577, 7938, 775, 341, 366, 257, 857, 6179, 293, 4399, 264, 11923, 295, 341, 1164, 13, 50978, 50978, 583, 498, 291, 747, 512, 544, 7339, 2452, 2539, 5359, 1780, 11, 291, 815, 1466, 544, 466, 264, 51180, 51180, 4365, 295, 341, 7938, 9284, 13, 51320, 51320, 583, 294, 3089, 11, 341, 307, 577, 291, 576, 4445, 309, 13, 51476, 51476, 440, 2316, 307, 2293, 264, 912, 382, 949, 13, 51645, 51645], "temperature": 0.0, "avg_logprob": -0.1104843870122382, "compression_ratio": 1.6462882096069869, "no_speech_prob": 5.594238700723508e-06}, {"id": 53, "seek": 24184, "start": 254.12, "end": 258.16, "text": " But if you take some more advanced deep learning classes later, you may learn more about the", "tokens": [50364, 646, 293, 5220, 11, 718, 311, 5407, 8961, 361, 337, 300, 13075, 257, 707, 857, 13, 50686, 50686, 440, 4365, 295, 577, 7938, 775, 341, 366, 257, 857, 6179, 293, 4399, 264, 11923, 295, 341, 1164, 13, 50978, 50978, 583, 498, 291, 747, 512, 544, 7339, 2452, 2539, 5359, 1780, 11, 291, 815, 1466, 544, 466, 264, 51180, 51180, 4365, 295, 341, 7938, 9284, 13, 51320, 51320, 583, 294, 3089, 11, 341, 307, 577, 291, 576, 4445, 309, 13, 51476, 51476, 440, 2316, 307, 2293, 264, 912, 382, 949, 13, 51645, 51645], "temperature": 0.0, "avg_logprob": -0.1104843870122382, "compression_ratio": 1.6462882096069869, "no_speech_prob": 5.594238700723508e-06}, {"id": 54, "seek": 24184, "start": 258.16, "end": 260.96, "text": " details of this Adam algorithm.", "tokens": [50364, 646, 293, 5220, 11, 718, 311, 5407, 8961, 361, 337, 300, 13075, 257, 707, 857, 13, 50686, 50686, 440, 4365, 295, 577, 7938, 775, 341, 366, 257, 857, 6179, 293, 4399, 264, 11923, 295, 341, 1164, 13, 50978, 50978, 583, 498, 291, 747, 512, 544, 7339, 2452, 2539, 5359, 1780, 11, 291, 815, 1466, 544, 466, 264, 51180, 51180, 4365, 295, 341, 7938, 9284, 13, 51320, 51320, 583, 294, 3089, 11, 341, 307, 577, 291, 576, 4445, 309, 13, 51476, 51476, 440, 2316, 307, 2293, 264, 912, 382, 949, 13, 51645, 51645], "temperature": 0.0, "avg_logprob": -0.1104843870122382, "compression_ratio": 1.6462882096069869, "no_speech_prob": 5.594238700723508e-06}, {"id": 55, "seek": 24184, "start": 260.96, "end": 264.08, "text": " But in code, this is how you would implement it.", "tokens": [50364, 646, 293, 5220, 11, 718, 311, 5407, 8961, 361, 337, 300, 13075, 257, 707, 857, 13, 50686, 50686, 440, 4365, 295, 577, 7938, 775, 341, 366, 257, 857, 6179, 293, 4399, 264, 11923, 295, 341, 1164, 13, 50978, 50978, 583, 498, 291, 747, 512, 544, 7339, 2452, 2539, 5359, 1780, 11, 291, 815, 1466, 544, 466, 264, 51180, 51180, 4365, 295, 341, 7938, 9284, 13, 51320, 51320, 583, 294, 3089, 11, 341, 307, 577, 291, 576, 4445, 309, 13, 51476, 51476, 440, 2316, 307, 2293, 264, 912, 382, 949, 13, 51645, 51645], "temperature": 0.0, "avg_logprob": -0.1104843870122382, "compression_ratio": 1.6462882096069869, "no_speech_prob": 5.594238700723508e-06}, {"id": 56, "seek": 24184, "start": 264.08, "end": 267.46, "text": " The model is exactly the same as before.", "tokens": [50364, 646, 293, 5220, 11, 718, 311, 5407, 8961, 361, 337, 300, 13075, 257, 707, 857, 13, 50686, 50686, 440, 4365, 295, 577, 7938, 775, 341, 366, 257, 857, 6179, 293, 4399, 264, 11923, 295, 341, 1164, 13, 50978, 50978, 583, 498, 291, 747, 512, 544, 7339, 2452, 2539, 5359, 1780, 11, 291, 815, 1466, 544, 466, 264, 51180, 51180, 4365, 295, 341, 7938, 9284, 13, 51320, 51320, 583, 294, 3089, 11, 341, 307, 577, 291, 576, 4445, 309, 13, 51476, 51476, 440, 2316, 307, 2293, 264, 912, 382, 949, 13, 51645, 51645], "temperature": 0.0, "avg_logprob": -0.1104843870122382, "compression_ratio": 1.6462882096069869, "no_speech_prob": 5.594238700723508e-06}, {"id": 57, "seek": 26746, "start": 267.46, "end": 272.91999999999996, "text": " And the way you compile the model is very similar to what we had before, except that", "tokens": [50364, 400, 264, 636, 291, 31413, 264, 2316, 307, 588, 2531, 281, 437, 321, 632, 949, 11, 3993, 300, 50637, 50637, 321, 586, 909, 472, 2857, 6770, 281, 264, 31413, 2445, 11, 597, 307, 300, 321, 16500, 300, 264, 50983, 50983, 5028, 6545, 291, 528, 281, 764, 307, 256, 69, 13, 5767, 271, 13, 5747, 332, 22525, 13, 3322, 32937, 46, 662, 332, 6545, 13, 51341, 51341, 407, 264, 7938, 19618, 9284, 775, 643, 512, 7576, 5883, 2539, 3314, 8961, 13, 51647, 51647], "temperature": 0.0, "avg_logprob": -0.1431750115894136, "compression_ratio": 1.635, "no_speech_prob": 5.862601483386243e-06}, {"id": 58, "seek": 26746, "start": 272.91999999999996, "end": 279.84, "text": " we now add one extra argument to the compile function, which is that we specify that the", "tokens": [50364, 400, 264, 636, 291, 31413, 264, 2316, 307, 588, 2531, 281, 437, 321, 632, 949, 11, 3993, 300, 50637, 50637, 321, 586, 909, 472, 2857, 6770, 281, 264, 31413, 2445, 11, 597, 307, 300, 321, 16500, 300, 264, 50983, 50983, 5028, 6545, 291, 528, 281, 764, 307, 256, 69, 13, 5767, 271, 13, 5747, 332, 22525, 13, 3322, 32937, 46, 662, 332, 6545, 13, 51341, 51341, 407, 264, 7938, 19618, 9284, 775, 643, 512, 7576, 5883, 2539, 3314, 8961, 13, 51647, 51647], "temperature": 0.0, "avg_logprob": -0.1431750115894136, "compression_ratio": 1.635, "no_speech_prob": 5.862601483386243e-06}, {"id": 59, "seek": 26746, "start": 279.84, "end": 287.0, "text": " optimizer you want to use is tf.keris.optimizers.theAdamOptimizer.", "tokens": [50364, 400, 264, 636, 291, 31413, 264, 2316, 307, 588, 2531, 281, 437, 321, 632, 949, 11, 3993, 300, 50637, 50637, 321, 586, 909, 472, 2857, 6770, 281, 264, 31413, 2445, 11, 597, 307, 300, 321, 16500, 300, 264, 50983, 50983, 5028, 6545, 291, 528, 281, 764, 307, 256, 69, 13, 5767, 271, 13, 5747, 332, 22525, 13, 3322, 32937, 46, 662, 332, 6545, 13, 51341, 51341, 407, 264, 7938, 19618, 9284, 775, 643, 512, 7576, 5883, 2539, 3314, 8961, 13, 51647, 51647], "temperature": 0.0, "avg_logprob": -0.1431750115894136, "compression_ratio": 1.635, "no_speech_prob": 5.862601483386243e-06}, {"id": 60, "seek": 26746, "start": 287.0, "end": 293.12, "text": " So the Adam optimization algorithm does need some default initial learning rate alpha.", "tokens": [50364, 400, 264, 636, 291, 31413, 264, 2316, 307, 588, 2531, 281, 437, 321, 632, 949, 11, 3993, 300, 50637, 50637, 321, 586, 909, 472, 2857, 6770, 281, 264, 31413, 2445, 11, 597, 307, 300, 321, 16500, 300, 264, 50983, 50983, 5028, 6545, 291, 528, 281, 764, 307, 256, 69, 13, 5767, 271, 13, 5747, 332, 22525, 13, 3322, 32937, 46, 662, 332, 6545, 13, 51341, 51341, 407, 264, 7938, 19618, 9284, 775, 643, 512, 7576, 5883, 2539, 3314, 8961, 13, 51647, 51647], "temperature": 0.0, "avg_logprob": -0.1431750115894136, "compression_ratio": 1.635, "no_speech_prob": 5.862601483386243e-06}, {"id": 61, "seek": 29312, "start": 293.12, "end": 300.36, "text": " And in this example, I've set that initial learning rate to be 10 to the negative 3.", "tokens": [50364, 400, 294, 341, 1365, 11, 286, 600, 992, 300, 5883, 2539, 3314, 281, 312, 1266, 281, 264, 3671, 805, 13, 50726, 50726, 583, 562, 291, 434, 1228, 264, 7938, 9284, 294, 3124, 11, 309, 311, 3163, 1382, 257, 1326, 4190, 337, 50942, 50942, 341, 5883, 11, 341, 7576, 4338, 2539, 3314, 13, 51108, 51108, 6526, 512, 4833, 293, 512, 4356, 4190, 281, 536, 437, 2709, 291, 264, 14573, 2539, 3389, 13, 51404, 51404, 30539, 281, 264, 3380, 16235, 23475, 9284, 300, 291, 632, 3264, 294, 264, 3894, 51663, 51663], "temperature": 0.0, "avg_logprob": -0.12294986221816513, "compression_ratio": 1.6411290322580645, "no_speech_prob": 1.2804913239961024e-05}, {"id": 62, "seek": 29312, "start": 300.36, "end": 304.68, "text": " But when you're using the Adam algorithm in practice, it's worth trying a few values for", "tokens": [50364, 400, 294, 341, 1365, 11, 286, 600, 992, 300, 5883, 2539, 3314, 281, 312, 1266, 281, 264, 3671, 805, 13, 50726, 50726, 583, 562, 291, 434, 1228, 264, 7938, 9284, 294, 3124, 11, 309, 311, 3163, 1382, 257, 1326, 4190, 337, 50942, 50942, 341, 5883, 11, 341, 7576, 4338, 2539, 3314, 13, 51108, 51108, 6526, 512, 4833, 293, 512, 4356, 4190, 281, 536, 437, 2709, 291, 264, 14573, 2539, 3389, 13, 51404, 51404, 30539, 281, 264, 3380, 16235, 23475, 9284, 300, 291, 632, 3264, 294, 264, 3894, 51663, 51663], "temperature": 0.0, "avg_logprob": -0.12294986221816513, "compression_ratio": 1.6411290322580645, "no_speech_prob": 1.2804913239961024e-05}, {"id": 63, "seek": 29312, "start": 304.68, "end": 308.0, "text": " this initial, this default global learning rate.", "tokens": [50364, 400, 294, 341, 1365, 11, 286, 600, 992, 300, 5883, 2539, 3314, 281, 312, 1266, 281, 264, 3671, 805, 13, 50726, 50726, 583, 562, 291, 434, 1228, 264, 7938, 9284, 294, 3124, 11, 309, 311, 3163, 1382, 257, 1326, 4190, 337, 50942, 50942, 341, 5883, 11, 341, 7576, 4338, 2539, 3314, 13, 51108, 51108, 6526, 512, 4833, 293, 512, 4356, 4190, 281, 536, 437, 2709, 291, 264, 14573, 2539, 3389, 13, 51404, 51404, 30539, 281, 264, 3380, 16235, 23475, 9284, 300, 291, 632, 3264, 294, 264, 3894, 51663, 51663], "temperature": 0.0, "avg_logprob": -0.12294986221816513, "compression_ratio": 1.6411290322580645, "no_speech_prob": 1.2804913239961024e-05}, {"id": 64, "seek": 29312, "start": 308.0, "end": 313.92, "text": " Try some larger and some smaller values to see what gives you the fastest learning performance.", "tokens": [50364, 400, 294, 341, 1365, 11, 286, 600, 992, 300, 5883, 2539, 3314, 281, 312, 1266, 281, 264, 3671, 805, 13, 50726, 50726, 583, 562, 291, 434, 1228, 264, 7938, 9284, 294, 3124, 11, 309, 311, 3163, 1382, 257, 1326, 4190, 337, 50942, 50942, 341, 5883, 11, 341, 7576, 4338, 2539, 3314, 13, 51108, 51108, 6526, 512, 4833, 293, 512, 4356, 4190, 281, 536, 437, 2709, 291, 264, 14573, 2539, 3389, 13, 51404, 51404, 30539, 281, 264, 3380, 16235, 23475, 9284, 300, 291, 632, 3264, 294, 264, 3894, 51663, 51663], "temperature": 0.0, "avg_logprob": -0.12294986221816513, "compression_ratio": 1.6411290322580645, "no_speech_prob": 1.2804913239961024e-05}, {"id": 65, "seek": 29312, "start": 313.92, "end": 319.1, "text": " Compared to the original gradient descent algorithm that you had learned in the previous", "tokens": [50364, 400, 294, 341, 1365, 11, 286, 600, 992, 300, 5883, 2539, 3314, 281, 312, 1266, 281, 264, 3671, 805, 13, 50726, 50726, 583, 562, 291, 434, 1228, 264, 7938, 9284, 294, 3124, 11, 309, 311, 3163, 1382, 257, 1326, 4190, 337, 50942, 50942, 341, 5883, 11, 341, 7576, 4338, 2539, 3314, 13, 51108, 51108, 6526, 512, 4833, 293, 512, 4356, 4190, 281, 536, 437, 2709, 291, 264, 14573, 2539, 3389, 13, 51404, 51404, 30539, 281, 264, 3380, 16235, 23475, 9284, 300, 291, 632, 3264, 294, 264, 3894, 51663, 51663], "temperature": 0.0, "avg_logprob": -0.12294986221816513, "compression_ratio": 1.6411290322580645, "no_speech_prob": 1.2804913239961024e-05}, {"id": 66, "seek": 31910, "start": 319.1, "end": 325.6, "text": " course though, the Adam algorithm, because it can adapt the learning rate a bit automatically,", "tokens": [50364, 1164, 1673, 11, 264, 7938, 9284, 11, 570, 309, 393, 6231, 264, 2539, 3314, 257, 857, 6772, 11, 50689, 50689, 309, 307, 544, 13956, 281, 264, 1900, 3922, 295, 2539, 3314, 300, 291, 1888, 13, 50911, 50911, 10404, 309, 307, 920, 3163, 15164, 341, 13075, 257, 707, 857, 281, 536, 498, 291, 393, 483, 8344, 51121, 51121, 4663, 2539, 13, 51205, 51205, 407, 300, 311, 309, 337, 264, 7938, 19618, 9284, 13, 51368, 51368, 467, 5850, 1985, 709, 4663, 813, 16235, 23475, 11, 293, 309, 311, 1813, 257, 368, 42225, 3832, 51675, 51675, 294, 577, 25742, 3847, 641, 18161, 9590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.13137008122035435, "compression_ratio": 1.7047970479704797, "no_speech_prob": 5.422056801762665e-06}, {"id": 67, "seek": 31910, "start": 325.6, "end": 330.04, "text": " it is more robust to the exact choice of learning rate that you pick.", "tokens": [50364, 1164, 1673, 11, 264, 7938, 9284, 11, 570, 309, 393, 6231, 264, 2539, 3314, 257, 857, 6772, 11, 50689, 50689, 309, 307, 544, 13956, 281, 264, 1900, 3922, 295, 2539, 3314, 300, 291, 1888, 13, 50911, 50911, 10404, 309, 307, 920, 3163, 15164, 341, 13075, 257, 707, 857, 281, 536, 498, 291, 393, 483, 8344, 51121, 51121, 4663, 2539, 13, 51205, 51205, 407, 300, 311, 309, 337, 264, 7938, 19618, 9284, 13, 51368, 51368, 467, 5850, 1985, 709, 4663, 813, 16235, 23475, 11, 293, 309, 311, 1813, 257, 368, 42225, 3832, 51675, 51675, 294, 577, 25742, 3847, 641, 18161, 9590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.13137008122035435, "compression_ratio": 1.7047970479704797, "no_speech_prob": 5.422056801762665e-06}, {"id": 68, "seek": 31910, "start": 330.04, "end": 334.24, "text": " Though it is still worth tuning this parameter a little bit to see if you can get somewhat", "tokens": [50364, 1164, 1673, 11, 264, 7938, 9284, 11, 570, 309, 393, 6231, 264, 2539, 3314, 257, 857, 6772, 11, 50689, 50689, 309, 307, 544, 13956, 281, 264, 1900, 3922, 295, 2539, 3314, 300, 291, 1888, 13, 50911, 50911, 10404, 309, 307, 920, 3163, 15164, 341, 13075, 257, 707, 857, 281, 536, 498, 291, 393, 483, 8344, 51121, 51121, 4663, 2539, 13, 51205, 51205, 407, 300, 311, 309, 337, 264, 7938, 19618, 9284, 13, 51368, 51368, 467, 5850, 1985, 709, 4663, 813, 16235, 23475, 11, 293, 309, 311, 1813, 257, 368, 42225, 3832, 51675, 51675, 294, 577, 25742, 3847, 641, 18161, 9590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.13137008122035435, "compression_ratio": 1.7047970479704797, "no_speech_prob": 5.422056801762665e-06}, {"id": 69, "seek": 31910, "start": 334.24, "end": 335.92, "text": " faster learning.", "tokens": [50364, 1164, 1673, 11, 264, 7938, 9284, 11, 570, 309, 393, 6231, 264, 2539, 3314, 257, 857, 6772, 11, 50689, 50689, 309, 307, 544, 13956, 281, 264, 1900, 3922, 295, 2539, 3314, 300, 291, 1888, 13, 50911, 50911, 10404, 309, 307, 920, 3163, 15164, 341, 13075, 257, 707, 857, 281, 536, 498, 291, 393, 483, 8344, 51121, 51121, 4663, 2539, 13, 51205, 51205, 407, 300, 311, 309, 337, 264, 7938, 19618, 9284, 13, 51368, 51368, 467, 5850, 1985, 709, 4663, 813, 16235, 23475, 11, 293, 309, 311, 1813, 257, 368, 42225, 3832, 51675, 51675, 294, 577, 25742, 3847, 641, 18161, 9590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.13137008122035435, "compression_ratio": 1.7047970479704797, "no_speech_prob": 5.422056801762665e-06}, {"id": 70, "seek": 31910, "start": 335.92, "end": 339.18, "text": " So that's it for the Adam optimization algorithm.", "tokens": [50364, 1164, 1673, 11, 264, 7938, 9284, 11, 570, 309, 393, 6231, 264, 2539, 3314, 257, 857, 6772, 11, 50689, 50689, 309, 307, 544, 13956, 281, 264, 1900, 3922, 295, 2539, 3314, 300, 291, 1888, 13, 50911, 50911, 10404, 309, 307, 920, 3163, 15164, 341, 13075, 257, 707, 857, 281, 536, 498, 291, 393, 483, 8344, 51121, 51121, 4663, 2539, 13, 51205, 51205, 407, 300, 311, 309, 337, 264, 7938, 19618, 9284, 13, 51368, 51368, 467, 5850, 1985, 709, 4663, 813, 16235, 23475, 11, 293, 309, 311, 1813, 257, 368, 42225, 3832, 51675, 51675, 294, 577, 25742, 3847, 641, 18161, 9590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.13137008122035435, "compression_ratio": 1.7047970479704797, "no_speech_prob": 5.422056801762665e-06}, {"id": 71, "seek": 31910, "start": 339.18, "end": 345.32000000000005, "text": " It typically works much faster than gradient descent, and it's become a de facto standard", "tokens": [50364, 1164, 1673, 11, 264, 7938, 9284, 11, 570, 309, 393, 6231, 264, 2539, 3314, 257, 857, 6772, 11, 50689, 50689, 309, 307, 544, 13956, 281, 264, 1900, 3922, 295, 2539, 3314, 300, 291, 1888, 13, 50911, 50911, 10404, 309, 307, 920, 3163, 15164, 341, 13075, 257, 707, 857, 281, 536, 498, 291, 393, 483, 8344, 51121, 51121, 4663, 2539, 13, 51205, 51205, 407, 300, 311, 309, 337, 264, 7938, 19618, 9284, 13, 51368, 51368, 467, 5850, 1985, 709, 4663, 813, 16235, 23475, 11, 293, 309, 311, 1813, 257, 368, 42225, 3832, 51675, 51675, 294, 577, 25742, 3847, 641, 18161, 9590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.13137008122035435, "compression_ratio": 1.7047970479704797, "no_speech_prob": 5.422056801762665e-06}, {"id": 72, "seek": 31910, "start": 345.32000000000005, "end": 348.74, "text": " in how practitioners train their neural networks.", "tokens": [50364, 1164, 1673, 11, 264, 7938, 9284, 11, 570, 309, 393, 6231, 264, 2539, 3314, 257, 857, 6772, 11, 50689, 50689, 309, 307, 544, 13956, 281, 264, 1900, 3922, 295, 2539, 3314, 300, 291, 1888, 13, 50911, 50911, 10404, 309, 307, 920, 3163, 15164, 341, 13075, 257, 707, 857, 281, 536, 498, 291, 393, 483, 8344, 51121, 51121, 4663, 2539, 13, 51205, 51205, 407, 300, 311, 309, 337, 264, 7938, 19618, 9284, 13, 51368, 51368, 467, 5850, 1985, 709, 4663, 813, 16235, 23475, 11, 293, 309, 311, 1813, 257, 368, 42225, 3832, 51675, 51675, 294, 577, 25742, 3847, 641, 18161, 9590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.13137008122035435, "compression_ratio": 1.7047970479704797, "no_speech_prob": 5.422056801762665e-06}, {"id": 73, "seek": 34874, "start": 348.74, "end": 352.84000000000003, "text": " So if you're trying to decide what learning algorithm to use, what optimization algorithm", "tokens": [50364, 407, 498, 291, 434, 1382, 281, 4536, 437, 2539, 9284, 281, 764, 11, 437, 19618, 9284, 50569, 50569, 281, 764, 281, 3847, 428, 18161, 3209, 11, 257, 3273, 3922, 576, 312, 281, 445, 764, 264, 7938, 19618, 50851, 50851, 9284, 13, 50926, 50926, 400, 881, 25742, 965, 486, 764, 7938, 2831, 813, 264, 3380, 16235, 23475, 51129, 51129, 9284, 13, 51204, 51204, 400, 365, 341, 11, 286, 1454, 300, 428, 2539, 14642, 486, 312, 1075, 281, 1466, 709, 544, 2661, 13, 51503, 51503, 823, 11, 294, 264, 958, 1916, 295, 2145, 11, 286, 1116, 411, 281, 2557, 322, 512, 544, 7339, 10392, 337, 51827, 51827], "temperature": 0.0, "avg_logprob": -0.11279111487843166, "compression_ratio": 1.790874524714829, "no_speech_prob": 4.222704319545301e-06}, {"id": 74, "seek": 34874, "start": 352.84000000000003, "end": 358.48, "text": " to use to train your neural network, a safe choice would be to just use the Adam optimization", "tokens": [50364, 407, 498, 291, 434, 1382, 281, 4536, 437, 2539, 9284, 281, 764, 11, 437, 19618, 9284, 50569, 50569, 281, 764, 281, 3847, 428, 18161, 3209, 11, 257, 3273, 3922, 576, 312, 281, 445, 764, 264, 7938, 19618, 50851, 50851, 9284, 13, 50926, 50926, 400, 881, 25742, 965, 486, 764, 7938, 2831, 813, 264, 3380, 16235, 23475, 51129, 51129, 9284, 13, 51204, 51204, 400, 365, 341, 11, 286, 1454, 300, 428, 2539, 14642, 486, 312, 1075, 281, 1466, 709, 544, 2661, 13, 51503, 51503, 823, 11, 294, 264, 958, 1916, 295, 2145, 11, 286, 1116, 411, 281, 2557, 322, 512, 544, 7339, 10392, 337, 51827, 51827], "temperature": 0.0, "avg_logprob": -0.11279111487843166, "compression_ratio": 1.790874524714829, "no_speech_prob": 4.222704319545301e-06}, {"id": 75, "seek": 34874, "start": 358.48, "end": 359.98, "text": " algorithm.", "tokens": [50364, 407, 498, 291, 434, 1382, 281, 4536, 437, 2539, 9284, 281, 764, 11, 437, 19618, 9284, 50569, 50569, 281, 764, 281, 3847, 428, 18161, 3209, 11, 257, 3273, 3922, 576, 312, 281, 445, 764, 264, 7938, 19618, 50851, 50851, 9284, 13, 50926, 50926, 400, 881, 25742, 965, 486, 764, 7938, 2831, 813, 264, 3380, 16235, 23475, 51129, 51129, 9284, 13, 51204, 51204, 400, 365, 341, 11, 286, 1454, 300, 428, 2539, 14642, 486, 312, 1075, 281, 1466, 709, 544, 2661, 13, 51503, 51503, 823, 11, 294, 264, 958, 1916, 295, 2145, 11, 286, 1116, 411, 281, 2557, 322, 512, 544, 7339, 10392, 337, 51827, 51827], "temperature": 0.0, "avg_logprob": -0.11279111487843166, "compression_ratio": 1.790874524714829, "no_speech_prob": 4.222704319545301e-06}, {"id": 76, "seek": 34874, "start": 359.98, "end": 364.04, "text": " And most practitioners today will use Adam rather than the original gradient descent", "tokens": [50364, 407, 498, 291, 434, 1382, 281, 4536, 437, 2539, 9284, 281, 764, 11, 437, 19618, 9284, 50569, 50569, 281, 764, 281, 3847, 428, 18161, 3209, 11, 257, 3273, 3922, 576, 312, 281, 445, 764, 264, 7938, 19618, 50851, 50851, 9284, 13, 50926, 50926, 400, 881, 25742, 965, 486, 764, 7938, 2831, 813, 264, 3380, 16235, 23475, 51129, 51129, 9284, 13, 51204, 51204, 400, 365, 341, 11, 286, 1454, 300, 428, 2539, 14642, 486, 312, 1075, 281, 1466, 709, 544, 2661, 13, 51503, 51503, 823, 11, 294, 264, 958, 1916, 295, 2145, 11, 286, 1116, 411, 281, 2557, 322, 512, 544, 7339, 10392, 337, 51827, 51827], "temperature": 0.0, "avg_logprob": -0.11279111487843166, "compression_ratio": 1.790874524714829, "no_speech_prob": 4.222704319545301e-06}, {"id": 77, "seek": 34874, "start": 364.04, "end": 365.54, "text": " algorithm.", "tokens": [50364, 407, 498, 291, 434, 1382, 281, 4536, 437, 2539, 9284, 281, 764, 11, 437, 19618, 9284, 50569, 50569, 281, 764, 281, 3847, 428, 18161, 3209, 11, 257, 3273, 3922, 576, 312, 281, 445, 764, 264, 7938, 19618, 50851, 50851, 9284, 13, 50926, 50926, 400, 881, 25742, 965, 486, 764, 7938, 2831, 813, 264, 3380, 16235, 23475, 51129, 51129, 9284, 13, 51204, 51204, 400, 365, 341, 11, 286, 1454, 300, 428, 2539, 14642, 486, 312, 1075, 281, 1466, 709, 544, 2661, 13, 51503, 51503, 823, 11, 294, 264, 958, 1916, 295, 2145, 11, 286, 1116, 411, 281, 2557, 322, 512, 544, 7339, 10392, 337, 51827, 51827], "temperature": 0.0, "avg_logprob": -0.11279111487843166, "compression_ratio": 1.790874524714829, "no_speech_prob": 4.222704319545301e-06}, {"id": 78, "seek": 34874, "start": 365.54, "end": 371.52, "text": " And with this, I hope that your learning algorithms will be able to learn much more quickly.", "tokens": [50364, 407, 498, 291, 434, 1382, 281, 4536, 437, 2539, 9284, 281, 764, 11, 437, 19618, 9284, 50569, 50569, 281, 764, 281, 3847, 428, 18161, 3209, 11, 257, 3273, 3922, 576, 312, 281, 445, 764, 264, 7938, 19618, 50851, 50851, 9284, 13, 50926, 50926, 400, 881, 25742, 965, 486, 764, 7938, 2831, 813, 264, 3380, 16235, 23475, 51129, 51129, 9284, 13, 51204, 51204, 400, 365, 341, 11, 286, 1454, 300, 428, 2539, 14642, 486, 312, 1075, 281, 1466, 709, 544, 2661, 13, 51503, 51503, 823, 11, 294, 264, 958, 1916, 295, 2145, 11, 286, 1116, 411, 281, 2557, 322, 512, 544, 7339, 10392, 337, 51827, 51827], "temperature": 0.0, "avg_logprob": -0.11279111487843166, "compression_ratio": 1.790874524714829, "no_speech_prob": 4.222704319545301e-06}, {"id": 79, "seek": 34874, "start": 371.52, "end": 378.0, "text": " Now, in the next couple of videos, I'd like to touch on some more advanced concepts for", "tokens": [50364, 407, 498, 291, 434, 1382, 281, 4536, 437, 2539, 9284, 281, 764, 11, 437, 19618, 9284, 50569, 50569, 281, 764, 281, 3847, 428, 18161, 3209, 11, 257, 3273, 3922, 576, 312, 281, 445, 764, 264, 7938, 19618, 50851, 50851, 9284, 13, 50926, 50926, 400, 881, 25742, 965, 486, 764, 7938, 2831, 813, 264, 3380, 16235, 23475, 51129, 51129, 9284, 13, 51204, 51204, 400, 365, 341, 11, 286, 1454, 300, 428, 2539, 14642, 486, 312, 1075, 281, 1466, 709, 544, 2661, 13, 51503, 51503, 823, 11, 294, 264, 958, 1916, 295, 2145, 11, 286, 1116, 411, 281, 2557, 322, 512, 544, 7339, 10392, 337, 51827, 51827], "temperature": 0.0, "avg_logprob": -0.11279111487843166, "compression_ratio": 1.790874524714829, "no_speech_prob": 4.222704319545301e-06}, {"id": 80, "seek": 37800, "start": 378.0, "end": 379.0, "text": " neural networks.", "tokens": [50364, 18161, 9590, 13, 50414, 50414, 400, 294, 1729, 11, 294, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 512, 8535, 4583, 3467, 13, 50696], "temperature": 0.0, "avg_logprob": -0.14329159259796143, "compression_ratio": 1.141304347826087, "no_speech_prob": 7.116708729881793e-05}, {"id": 81, "seek": 37900, "start": 379.0, "end": 408.92, "text": " And in particular, in the next video, let's take a look at some alternative layer types.", "tokens": [50364, 400, 294, 1729, 11, 294, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 512, 8535, 4583, 3467, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.1328370968500773, "compression_ratio": 1.1, "no_speech_prob": 1.611417428648565e-05}], "language": "en", "video_id": "yo6aW-D7sCM", "entity": "ML Specialization, Andrew Ng (2022)"}}