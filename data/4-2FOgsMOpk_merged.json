{"video_id": "4-2FOgsMOpk", "title": "4.6 Neural Networks Model | More complex neural networks --[Machine Learning | Andrew Ng]", "description": "Second Course:\nAdvanced Learning Algorithms.\n\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 438, "views": 151, "publish_date": "11/04/2022", "timestamp": 1661817600, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " In the last video, you learned about the neural network layer and how that takes as input a vector of numbers and in turn outputs another vector of numbers. In this video, let's use that layer to build a more complex neural network. And through this, I hope that the notation that we're using for neural networks will become clearer and more concrete as well. Let's take a look. This is the running example that I'm going to use throughout this video as an example of a more complex neural network. This network has four layers, not counting the input layer, which is also called layer zero, where layers one, two, and three are hidden layers and layer four is the output layer and layer zero as usual is the input layer. By convention, when we say that a neural network has four layers, that includes all the hidden layers and the output layer, but we don't count the input layer. So this is a neural network with four layers in the conventional way of counting layers in the network. Let's zoom in to layer three, which is the third and final hidden layer to look at the computations of that layer. Layer three inputs a vector, a superscript square bracket two that was computed by the previous layer and it outputs a three, which is another vector. So what is the computation that layer three does in order to go from a two to a three? If it has three neurons or we call it three hidden units, then it has parameters w1, b1, w2, b2, and w3, b3 and it computes a1 equals sigmoid of w1 dot product with this input to the layer plus b1 and it computes a2 equals sigmoid of w2 dot product with again a2, the input to the layer plus b2 and so on to get a3 and then the output of this layer is a vector comprising a1, a2, and a3. And again by convention if we want to more explicitly denote that all of these are quantities associated with layer three, then we add in all of these superscript square brackets three here to denote that these parameters w and b are the parameters associated with neurons in layer three and that these activations are activations with layer three. Notice that this term here is w1 superscript square bracket three meaning the parameters associated with layer three dot product with a superscript square bracket two, which was the output of layer two, which became the input to layer three. So that's why there's a three here because there's a parameter associated with layer three dot product with and there's a two there because it's the output of layer two. Now let's just do a quick double check of our understanding of this. I'm going to hide the superscripts and subscripts associated with the second neuron and without rewinding this video, go ahead and rewind if you want but you know prefer you not, but without rewinding this video, are you able to think through what are the missing superscripts and subscripts in this equation and fill them in yourself? Why don't you take a look at the end video quiz and see if you can figure out what are the appropriate superscripts and subscripts for this equation over here. So to recap, A3 is activation associated with layer three for the second neuron, hence this is a two. There's a parameter associated with the third layer for the second neuron. This is A2, same as above, and then plus B3, two. So hopefully that makes sense. Here's the more general form of this equation for an arbitrary layer L and for an arbitrary unit J, which is that A deactivation output of layer L unit J like A3, two, that's going to be the sigmoid function applied to this term, which is the weight vector of layer L such as layer three for the Jth unit. So there's two again in the example above. And so that's dot product with A deactivation value of, and notice this is not L, this is L minus one, like the two above here, because you're dot producting with the output from the previous layer, and then plus B, the parameter for this layer for that unit J. And so this gives you the activation of layer L unit J, where the superscript in square brackets L denotes layer L and the subscript J denotes unit J. And when building neural networks, unit J refers to the Jth neuron. So use those terms a little bit interchangeably, where each unit is a single neuron in a layer. G here is the sigmoid function. In the context of a neural network, G has another name, which is also called the activation function because G outputs this activation value. So when I say activation function, I mean this function G here. And so far, the only activation function you've seen is the sigmoid function. But next week, we'll look at when other functions than the sigmoid function can be plugged in in place of G as well. But so the activation function is just that function that outputs these activation values. And just one last piece of notation in order to make all this notation consistent. I'm also doing to give the input vector X another name, which is a zero. So this way, the same equation also works for the first layer, where when L is equal to one deactivations of the first layer that is a one will be sigmoid times the weights dot product with a zero, which is just this input feature vector X. So with this notation, you now know how to compute the activation values of any layer in a neural network as a function of the parameters as well as the activations of the previous layer. So you now know how to compute the activations of any layer given the activations of the previous layer. Let's put this into an inference algorithm for a neural network. In other words, how to get a neural network to make predictions. Let's go see that in the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.2, "text": " In the last video, you learned about the neural network layer and how that takes as input", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 3264, 466, 264, 18161, 3209, 4583, 293, 577, 300, 2516, 382, 4846, 50724, 50724, 257, 8062, 295, 3547, 293, 294, 1261, 23930, 1071, 8062, 295, 3547, 13, 50994, 50994, 682, 341, 960, 11, 718, 311, 764, 300, 4583, 281, 1322, 257, 544, 3997, 18161, 3209, 13, 51244, 51244, 400, 807, 341, 11, 286, 1454, 300, 264, 24657, 300, 321, 434, 1228, 337, 18161, 9590, 486, 51500, 51500, 1813, 26131, 293, 544, 9859, 382, 731, 13, 51630, 51630, 961, 311, 747, 257, 574, 13, 51691, 51691], "temperature": 0.0, "avg_logprob": -0.13467991736627394, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.03306519240140915}, {"id": 1, "seek": 0, "start": 7.2, "end": 12.6, "text": " a vector of numbers and in turn outputs another vector of numbers.", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 3264, 466, 264, 18161, 3209, 4583, 293, 577, 300, 2516, 382, 4846, 50724, 50724, 257, 8062, 295, 3547, 293, 294, 1261, 23930, 1071, 8062, 295, 3547, 13, 50994, 50994, 682, 341, 960, 11, 718, 311, 764, 300, 4583, 281, 1322, 257, 544, 3997, 18161, 3209, 13, 51244, 51244, 400, 807, 341, 11, 286, 1454, 300, 264, 24657, 300, 321, 434, 1228, 337, 18161, 9590, 486, 51500, 51500, 1813, 26131, 293, 544, 9859, 382, 731, 13, 51630, 51630, 961, 311, 747, 257, 574, 13, 51691, 51691], "temperature": 0.0, "avg_logprob": -0.13467991736627394, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.03306519240140915}, {"id": 2, "seek": 0, "start": 12.6, "end": 17.6, "text": " In this video, let's use that layer to build a more complex neural network.", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 3264, 466, 264, 18161, 3209, 4583, 293, 577, 300, 2516, 382, 4846, 50724, 50724, 257, 8062, 295, 3547, 293, 294, 1261, 23930, 1071, 8062, 295, 3547, 13, 50994, 50994, 682, 341, 960, 11, 718, 311, 764, 300, 4583, 281, 1322, 257, 544, 3997, 18161, 3209, 13, 51244, 51244, 400, 807, 341, 11, 286, 1454, 300, 264, 24657, 300, 321, 434, 1228, 337, 18161, 9590, 486, 51500, 51500, 1813, 26131, 293, 544, 9859, 382, 731, 13, 51630, 51630, 961, 311, 747, 257, 574, 13, 51691, 51691], "temperature": 0.0, "avg_logprob": -0.13467991736627394, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.03306519240140915}, {"id": 3, "seek": 0, "start": 17.6, "end": 22.72, "text": " And through this, I hope that the notation that we're using for neural networks will", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 3264, 466, 264, 18161, 3209, 4583, 293, 577, 300, 2516, 382, 4846, 50724, 50724, 257, 8062, 295, 3547, 293, 294, 1261, 23930, 1071, 8062, 295, 3547, 13, 50994, 50994, 682, 341, 960, 11, 718, 311, 764, 300, 4583, 281, 1322, 257, 544, 3997, 18161, 3209, 13, 51244, 51244, 400, 807, 341, 11, 286, 1454, 300, 264, 24657, 300, 321, 434, 1228, 337, 18161, 9590, 486, 51500, 51500, 1813, 26131, 293, 544, 9859, 382, 731, 13, 51630, 51630, 961, 311, 747, 257, 574, 13, 51691, 51691], "temperature": 0.0, "avg_logprob": -0.13467991736627394, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.03306519240140915}, {"id": 4, "seek": 0, "start": 22.72, "end": 25.32, "text": " become clearer and more concrete as well.", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 3264, 466, 264, 18161, 3209, 4583, 293, 577, 300, 2516, 382, 4846, 50724, 50724, 257, 8062, 295, 3547, 293, 294, 1261, 23930, 1071, 8062, 295, 3547, 13, 50994, 50994, 682, 341, 960, 11, 718, 311, 764, 300, 4583, 281, 1322, 257, 544, 3997, 18161, 3209, 13, 51244, 51244, 400, 807, 341, 11, 286, 1454, 300, 264, 24657, 300, 321, 434, 1228, 337, 18161, 9590, 486, 51500, 51500, 1813, 26131, 293, 544, 9859, 382, 731, 13, 51630, 51630, 961, 311, 747, 257, 574, 13, 51691, 51691], "temperature": 0.0, "avg_logprob": -0.13467991736627394, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.03306519240140915}, {"id": 5, "seek": 0, "start": 25.32, "end": 26.54, "text": " Let's take a look.", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 3264, 466, 264, 18161, 3209, 4583, 293, 577, 300, 2516, 382, 4846, 50724, 50724, 257, 8062, 295, 3547, 293, 294, 1261, 23930, 1071, 8062, 295, 3547, 13, 50994, 50994, 682, 341, 960, 11, 718, 311, 764, 300, 4583, 281, 1322, 257, 544, 3997, 18161, 3209, 13, 51244, 51244, 400, 807, 341, 11, 286, 1454, 300, 264, 24657, 300, 321, 434, 1228, 337, 18161, 9590, 486, 51500, 51500, 1813, 26131, 293, 544, 9859, 382, 731, 13, 51630, 51630, 961, 311, 747, 257, 574, 13, 51691, 51691], "temperature": 0.0, "avg_logprob": -0.13467991736627394, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.03306519240140915}, {"id": 6, "seek": 2654, "start": 26.54, "end": 31.2, "text": " This is the running example that I'm going to use throughout this video as an example", "tokens": [50364, 639, 307, 264, 2614, 1365, 300, 286, 478, 516, 281, 764, 3710, 341, 960, 382, 364, 1365, 50597, 50597, 295, 257, 544, 3997, 18161, 3209, 13, 50737, 50737, 639, 3209, 575, 1451, 7914, 11, 406, 13251, 264, 4846, 4583, 11, 597, 307, 611, 1219, 4583, 50992, 50992, 4018, 11, 689, 7914, 472, 11, 732, 11, 293, 1045, 366, 7633, 7914, 293, 4583, 1451, 307, 264, 5598, 51369, 51369, 4583, 293, 4583, 4018, 382, 7713, 307, 264, 4846, 4583, 13, 51585, 51585, 3146, 10286, 11, 562, 321, 584, 300, 257, 18161, 3209, 575, 1451, 7914, 11, 300, 5974, 439, 264, 7633, 51863, 51863], "temperature": 0.0, "avg_logprob": -0.12534580780909613, "compression_ratio": 1.8595744680851063, "no_speech_prob": 3.219063000869937e-05}, {"id": 7, "seek": 2654, "start": 31.2, "end": 34.0, "text": " of a more complex neural network.", "tokens": [50364, 639, 307, 264, 2614, 1365, 300, 286, 478, 516, 281, 764, 3710, 341, 960, 382, 364, 1365, 50597, 50597, 295, 257, 544, 3997, 18161, 3209, 13, 50737, 50737, 639, 3209, 575, 1451, 7914, 11, 406, 13251, 264, 4846, 4583, 11, 597, 307, 611, 1219, 4583, 50992, 50992, 4018, 11, 689, 7914, 472, 11, 732, 11, 293, 1045, 366, 7633, 7914, 293, 4583, 1451, 307, 264, 5598, 51369, 51369, 4583, 293, 4583, 4018, 382, 7713, 307, 264, 4846, 4583, 13, 51585, 51585, 3146, 10286, 11, 562, 321, 584, 300, 257, 18161, 3209, 575, 1451, 7914, 11, 300, 5974, 439, 264, 7633, 51863, 51863], "temperature": 0.0, "avg_logprob": -0.12534580780909613, "compression_ratio": 1.8595744680851063, "no_speech_prob": 3.219063000869937e-05}, {"id": 8, "seek": 2654, "start": 34.0, "end": 39.1, "text": " This network has four layers, not counting the input layer, which is also called layer", "tokens": [50364, 639, 307, 264, 2614, 1365, 300, 286, 478, 516, 281, 764, 3710, 341, 960, 382, 364, 1365, 50597, 50597, 295, 257, 544, 3997, 18161, 3209, 13, 50737, 50737, 639, 3209, 575, 1451, 7914, 11, 406, 13251, 264, 4846, 4583, 11, 597, 307, 611, 1219, 4583, 50992, 50992, 4018, 11, 689, 7914, 472, 11, 732, 11, 293, 1045, 366, 7633, 7914, 293, 4583, 1451, 307, 264, 5598, 51369, 51369, 4583, 293, 4583, 4018, 382, 7713, 307, 264, 4846, 4583, 13, 51585, 51585, 3146, 10286, 11, 562, 321, 584, 300, 257, 18161, 3209, 575, 1451, 7914, 11, 300, 5974, 439, 264, 7633, 51863, 51863], "temperature": 0.0, "avg_logprob": -0.12534580780909613, "compression_ratio": 1.8595744680851063, "no_speech_prob": 3.219063000869937e-05}, {"id": 9, "seek": 2654, "start": 39.1, "end": 46.64, "text": " zero, where layers one, two, and three are hidden layers and layer four is the output", "tokens": [50364, 639, 307, 264, 2614, 1365, 300, 286, 478, 516, 281, 764, 3710, 341, 960, 382, 364, 1365, 50597, 50597, 295, 257, 544, 3997, 18161, 3209, 13, 50737, 50737, 639, 3209, 575, 1451, 7914, 11, 406, 13251, 264, 4846, 4583, 11, 597, 307, 611, 1219, 4583, 50992, 50992, 4018, 11, 689, 7914, 472, 11, 732, 11, 293, 1045, 366, 7633, 7914, 293, 4583, 1451, 307, 264, 5598, 51369, 51369, 4583, 293, 4583, 4018, 382, 7713, 307, 264, 4846, 4583, 13, 51585, 51585, 3146, 10286, 11, 562, 321, 584, 300, 257, 18161, 3209, 575, 1451, 7914, 11, 300, 5974, 439, 264, 7633, 51863, 51863], "temperature": 0.0, "avg_logprob": -0.12534580780909613, "compression_ratio": 1.8595744680851063, "no_speech_prob": 3.219063000869937e-05}, {"id": 10, "seek": 2654, "start": 46.64, "end": 50.96, "text": " layer and layer zero as usual is the input layer.", "tokens": [50364, 639, 307, 264, 2614, 1365, 300, 286, 478, 516, 281, 764, 3710, 341, 960, 382, 364, 1365, 50597, 50597, 295, 257, 544, 3997, 18161, 3209, 13, 50737, 50737, 639, 3209, 575, 1451, 7914, 11, 406, 13251, 264, 4846, 4583, 11, 597, 307, 611, 1219, 4583, 50992, 50992, 4018, 11, 689, 7914, 472, 11, 732, 11, 293, 1045, 366, 7633, 7914, 293, 4583, 1451, 307, 264, 5598, 51369, 51369, 4583, 293, 4583, 4018, 382, 7713, 307, 264, 4846, 4583, 13, 51585, 51585, 3146, 10286, 11, 562, 321, 584, 300, 257, 18161, 3209, 575, 1451, 7914, 11, 300, 5974, 439, 264, 7633, 51863, 51863], "temperature": 0.0, "avg_logprob": -0.12534580780909613, "compression_ratio": 1.8595744680851063, "no_speech_prob": 3.219063000869937e-05}, {"id": 11, "seek": 2654, "start": 50.96, "end": 56.519999999999996, "text": " By convention, when we say that a neural network has four layers, that includes all the hidden", "tokens": [50364, 639, 307, 264, 2614, 1365, 300, 286, 478, 516, 281, 764, 3710, 341, 960, 382, 364, 1365, 50597, 50597, 295, 257, 544, 3997, 18161, 3209, 13, 50737, 50737, 639, 3209, 575, 1451, 7914, 11, 406, 13251, 264, 4846, 4583, 11, 597, 307, 611, 1219, 4583, 50992, 50992, 4018, 11, 689, 7914, 472, 11, 732, 11, 293, 1045, 366, 7633, 7914, 293, 4583, 1451, 307, 264, 5598, 51369, 51369, 4583, 293, 4583, 4018, 382, 7713, 307, 264, 4846, 4583, 13, 51585, 51585, 3146, 10286, 11, 562, 321, 584, 300, 257, 18161, 3209, 575, 1451, 7914, 11, 300, 5974, 439, 264, 7633, 51863, 51863], "temperature": 0.0, "avg_logprob": -0.12534580780909613, "compression_ratio": 1.8595744680851063, "no_speech_prob": 3.219063000869937e-05}, {"id": 12, "seek": 5652, "start": 56.52, "end": 60.28, "text": " layers and the output layer, but we don't count the input layer.", "tokens": [50364, 7914, 293, 264, 5598, 4583, 11, 457, 321, 500, 380, 1207, 264, 4846, 4583, 13, 50552, 50552, 407, 341, 307, 257, 18161, 3209, 365, 1451, 7914, 294, 264, 16011, 636, 295, 13251, 7914, 50792, 50792, 294, 264, 3209, 13, 50932, 50932, 961, 311, 8863, 294, 281, 4583, 1045, 11, 597, 307, 264, 2636, 293, 2572, 7633, 4583, 281, 574, 412, 264, 51240, 51240, 2807, 763, 295, 300, 4583, 13, 51404, 51404, 35166, 1045, 15743, 257, 8062, 11, 257, 37906, 5944, 3732, 16904, 732, 300, 390, 40610, 538, 264, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.12321144601573115, "compression_ratio": 1.7289719626168225, "no_speech_prob": 1.2411142051860224e-05}, {"id": 13, "seek": 5652, "start": 60.28, "end": 65.08, "text": " So this is a neural network with four layers in the conventional way of counting layers", "tokens": [50364, 7914, 293, 264, 5598, 4583, 11, 457, 321, 500, 380, 1207, 264, 4846, 4583, 13, 50552, 50552, 407, 341, 307, 257, 18161, 3209, 365, 1451, 7914, 294, 264, 16011, 636, 295, 13251, 7914, 50792, 50792, 294, 264, 3209, 13, 50932, 50932, 961, 311, 8863, 294, 281, 4583, 1045, 11, 597, 307, 264, 2636, 293, 2572, 7633, 4583, 281, 574, 412, 264, 51240, 51240, 2807, 763, 295, 300, 4583, 13, 51404, 51404, 35166, 1045, 15743, 257, 8062, 11, 257, 37906, 5944, 3732, 16904, 732, 300, 390, 40610, 538, 264, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.12321144601573115, "compression_ratio": 1.7289719626168225, "no_speech_prob": 1.2411142051860224e-05}, {"id": 14, "seek": 5652, "start": 65.08, "end": 67.88, "text": " in the network.", "tokens": [50364, 7914, 293, 264, 5598, 4583, 11, 457, 321, 500, 380, 1207, 264, 4846, 4583, 13, 50552, 50552, 407, 341, 307, 257, 18161, 3209, 365, 1451, 7914, 294, 264, 16011, 636, 295, 13251, 7914, 50792, 50792, 294, 264, 3209, 13, 50932, 50932, 961, 311, 8863, 294, 281, 4583, 1045, 11, 597, 307, 264, 2636, 293, 2572, 7633, 4583, 281, 574, 412, 264, 51240, 51240, 2807, 763, 295, 300, 4583, 13, 51404, 51404, 35166, 1045, 15743, 257, 8062, 11, 257, 37906, 5944, 3732, 16904, 732, 300, 390, 40610, 538, 264, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.12321144601573115, "compression_ratio": 1.7289719626168225, "no_speech_prob": 1.2411142051860224e-05}, {"id": 15, "seek": 5652, "start": 67.88, "end": 74.04, "text": " Let's zoom in to layer three, which is the third and final hidden layer to look at the", "tokens": [50364, 7914, 293, 264, 5598, 4583, 11, 457, 321, 500, 380, 1207, 264, 4846, 4583, 13, 50552, 50552, 407, 341, 307, 257, 18161, 3209, 365, 1451, 7914, 294, 264, 16011, 636, 295, 13251, 7914, 50792, 50792, 294, 264, 3209, 13, 50932, 50932, 961, 311, 8863, 294, 281, 4583, 1045, 11, 597, 307, 264, 2636, 293, 2572, 7633, 4583, 281, 574, 412, 264, 51240, 51240, 2807, 763, 295, 300, 4583, 13, 51404, 51404, 35166, 1045, 15743, 257, 8062, 11, 257, 37906, 5944, 3732, 16904, 732, 300, 390, 40610, 538, 264, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.12321144601573115, "compression_ratio": 1.7289719626168225, "no_speech_prob": 1.2411142051860224e-05}, {"id": 16, "seek": 5652, "start": 74.04, "end": 77.32000000000001, "text": " computations of that layer.", "tokens": [50364, 7914, 293, 264, 5598, 4583, 11, 457, 321, 500, 380, 1207, 264, 4846, 4583, 13, 50552, 50552, 407, 341, 307, 257, 18161, 3209, 365, 1451, 7914, 294, 264, 16011, 636, 295, 13251, 7914, 50792, 50792, 294, 264, 3209, 13, 50932, 50932, 961, 311, 8863, 294, 281, 4583, 1045, 11, 597, 307, 264, 2636, 293, 2572, 7633, 4583, 281, 574, 412, 264, 51240, 51240, 2807, 763, 295, 300, 4583, 13, 51404, 51404, 35166, 1045, 15743, 257, 8062, 11, 257, 37906, 5944, 3732, 16904, 732, 300, 390, 40610, 538, 264, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.12321144601573115, "compression_ratio": 1.7289719626168225, "no_speech_prob": 1.2411142051860224e-05}, {"id": 17, "seek": 5652, "start": 77.32000000000001, "end": 83.92, "text": " Layer three inputs a vector, a superscript square bracket two that was computed by the", "tokens": [50364, 7914, 293, 264, 5598, 4583, 11, 457, 321, 500, 380, 1207, 264, 4846, 4583, 13, 50552, 50552, 407, 341, 307, 257, 18161, 3209, 365, 1451, 7914, 294, 264, 16011, 636, 295, 13251, 7914, 50792, 50792, 294, 264, 3209, 13, 50932, 50932, 961, 311, 8863, 294, 281, 4583, 1045, 11, 597, 307, 264, 2636, 293, 2572, 7633, 4583, 281, 574, 412, 264, 51240, 51240, 2807, 763, 295, 300, 4583, 13, 51404, 51404, 35166, 1045, 15743, 257, 8062, 11, 257, 37906, 5944, 3732, 16904, 732, 300, 390, 40610, 538, 264, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.12321144601573115, "compression_ratio": 1.7289719626168225, "no_speech_prob": 1.2411142051860224e-05}, {"id": 18, "seek": 8392, "start": 83.92, "end": 92.04, "text": " previous layer and it outputs a three, which is another vector.", "tokens": [50364, 3894, 4583, 293, 309, 23930, 257, 1045, 11, 597, 307, 1071, 8062, 13, 50770, 50770, 407, 437, 307, 264, 24903, 300, 4583, 1045, 775, 294, 1668, 281, 352, 490, 257, 732, 281, 257, 1045, 30, 51190, 51190, 759, 309, 575, 1045, 22027, 420, 321, 818, 309, 1045, 7633, 6815, 11, 550, 309, 575, 9834, 261, 16, 11, 272, 16, 11, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.17207109928131104, "compression_ratio": 1.5126582278481013, "no_speech_prob": 7.52777623347356e-06}, {"id": 19, "seek": 8392, "start": 92.04, "end": 100.44, "text": " So what is the computation that layer three does in order to go from a two to a three?", "tokens": [50364, 3894, 4583, 293, 309, 23930, 257, 1045, 11, 597, 307, 1071, 8062, 13, 50770, 50770, 407, 437, 307, 264, 24903, 300, 4583, 1045, 775, 294, 1668, 281, 352, 490, 257, 732, 281, 257, 1045, 30, 51190, 51190, 759, 309, 575, 1045, 22027, 420, 321, 818, 309, 1045, 7633, 6815, 11, 550, 309, 575, 9834, 261, 16, 11, 272, 16, 11, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.17207109928131104, "compression_ratio": 1.5126582278481013, "no_speech_prob": 7.52777623347356e-06}, {"id": 20, "seek": 8392, "start": 100.44, "end": 108.68, "text": " If it has three neurons or we call it three hidden units, then it has parameters w1, b1,", "tokens": [50364, 3894, 4583, 293, 309, 23930, 257, 1045, 11, 597, 307, 1071, 8062, 13, 50770, 50770, 407, 437, 307, 264, 24903, 300, 4583, 1045, 775, 294, 1668, 281, 352, 490, 257, 732, 281, 257, 1045, 30, 51190, 51190, 759, 309, 575, 1045, 22027, 420, 321, 818, 309, 1045, 7633, 6815, 11, 550, 309, 575, 9834, 261, 16, 11, 272, 16, 11, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.17207109928131104, "compression_ratio": 1.5126582278481013, "no_speech_prob": 7.52777623347356e-06}, {"id": 21, "seek": 10868, "start": 108.68, "end": 120.16000000000001, "text": " w2, b2, and w3, b3 and it computes a1 equals sigmoid of w1 dot product with this input", "tokens": [50364, 261, 17, 11, 272, 17, 11, 293, 261, 18, 11, 272, 18, 293, 309, 715, 1819, 257, 16, 6915, 4556, 3280, 327, 295, 261, 16, 5893, 1674, 365, 341, 4846, 50938, 50938, 281, 264, 4583, 1804, 272, 16, 293, 309, 715, 1819, 257, 17, 6915, 4556, 3280, 327, 295, 261, 17, 5893, 1674, 365, 797, 257, 17, 11, 264, 51378, 51378, 4846, 281, 264, 4583, 1804, 272, 17, 293, 370, 322, 281, 483, 257, 18, 293, 550, 264, 5598, 295, 341, 4583, 307, 257, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.1311813701282848, "compression_ratio": 1.8857142857142857, "no_speech_prob": 1.045123553922167e-05}, {"id": 22, "seek": 10868, "start": 120.16000000000001, "end": 128.96, "text": " to the layer plus b1 and it computes a2 equals sigmoid of w2 dot product with again a2, the", "tokens": [50364, 261, 17, 11, 272, 17, 11, 293, 261, 18, 11, 272, 18, 293, 309, 715, 1819, 257, 16, 6915, 4556, 3280, 327, 295, 261, 16, 5893, 1674, 365, 341, 4846, 50938, 50938, 281, 264, 4583, 1804, 272, 16, 293, 309, 715, 1819, 257, 17, 6915, 4556, 3280, 327, 295, 261, 17, 5893, 1674, 365, 797, 257, 17, 11, 264, 51378, 51378, 4846, 281, 264, 4583, 1804, 272, 17, 293, 370, 322, 281, 483, 257, 18, 293, 550, 264, 5598, 295, 341, 4583, 307, 257, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.1311813701282848, "compression_ratio": 1.8857142857142857, "no_speech_prob": 1.045123553922167e-05}, {"id": 23, "seek": 10868, "start": 128.96, "end": 136.52, "text": " input to the layer plus b2 and so on to get a3 and then the output of this layer is a", "tokens": [50364, 261, 17, 11, 272, 17, 11, 293, 261, 18, 11, 272, 18, 293, 309, 715, 1819, 257, 16, 6915, 4556, 3280, 327, 295, 261, 16, 5893, 1674, 365, 341, 4846, 50938, 50938, 281, 264, 4583, 1804, 272, 16, 293, 309, 715, 1819, 257, 17, 6915, 4556, 3280, 327, 295, 261, 17, 5893, 1674, 365, 797, 257, 17, 11, 264, 51378, 51378, 4846, 281, 264, 4583, 1804, 272, 17, 293, 370, 322, 281, 483, 257, 18, 293, 550, 264, 5598, 295, 341, 4583, 307, 257, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.1311813701282848, "compression_ratio": 1.8857142857142857, "no_speech_prob": 1.045123553922167e-05}, {"id": 24, "seek": 13652, "start": 136.52, "end": 142.8, "text": " vector comprising a1, a2, and a3.", "tokens": [50364, 8062, 16802, 3436, 257, 16, 11, 257, 17, 11, 293, 257, 18, 13, 50678, 50678, 400, 797, 538, 10286, 498, 321, 528, 281, 544, 20803, 45708, 300, 439, 295, 613, 366, 22927, 51010, 51010, 6615, 365, 4583, 1045, 11, 550, 321, 909, 294, 439, 295, 613, 37906, 5944, 3732, 26179, 1045, 51310, 51310, 510, 281, 45708, 300, 613, 9834, 261, 293, 272, 366, 264, 9834, 6615, 365, 22027, 51598, 51598], "temperature": 0.0, "avg_logprob": -0.1239067448510064, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.3320560558204306e-06}, {"id": 25, "seek": 13652, "start": 142.8, "end": 149.44, "text": " And again by convention if we want to more explicitly denote that all of these are quantities", "tokens": [50364, 8062, 16802, 3436, 257, 16, 11, 257, 17, 11, 293, 257, 18, 13, 50678, 50678, 400, 797, 538, 10286, 498, 321, 528, 281, 544, 20803, 45708, 300, 439, 295, 613, 366, 22927, 51010, 51010, 6615, 365, 4583, 1045, 11, 550, 321, 909, 294, 439, 295, 613, 37906, 5944, 3732, 26179, 1045, 51310, 51310, 510, 281, 45708, 300, 613, 9834, 261, 293, 272, 366, 264, 9834, 6615, 365, 22027, 51598, 51598], "temperature": 0.0, "avg_logprob": -0.1239067448510064, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.3320560558204306e-06}, {"id": 26, "seek": 13652, "start": 149.44, "end": 155.44, "text": " associated with layer three, then we add in all of these superscript square brackets three", "tokens": [50364, 8062, 16802, 3436, 257, 16, 11, 257, 17, 11, 293, 257, 18, 13, 50678, 50678, 400, 797, 538, 10286, 498, 321, 528, 281, 544, 20803, 45708, 300, 439, 295, 613, 366, 22927, 51010, 51010, 6615, 365, 4583, 1045, 11, 550, 321, 909, 294, 439, 295, 613, 37906, 5944, 3732, 26179, 1045, 51310, 51310, 510, 281, 45708, 300, 613, 9834, 261, 293, 272, 366, 264, 9834, 6615, 365, 22027, 51598, 51598], "temperature": 0.0, "avg_logprob": -0.1239067448510064, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.3320560558204306e-06}, {"id": 27, "seek": 13652, "start": 155.44, "end": 161.20000000000002, "text": " here to denote that these parameters w and b are the parameters associated with neurons", "tokens": [50364, 8062, 16802, 3436, 257, 16, 11, 257, 17, 11, 293, 257, 18, 13, 50678, 50678, 400, 797, 538, 10286, 498, 321, 528, 281, 544, 20803, 45708, 300, 439, 295, 613, 366, 22927, 51010, 51010, 6615, 365, 4583, 1045, 11, 550, 321, 909, 294, 439, 295, 613, 37906, 5944, 3732, 26179, 1045, 51310, 51310, 510, 281, 45708, 300, 613, 9834, 261, 293, 272, 366, 264, 9834, 6615, 365, 22027, 51598, 51598], "temperature": 0.0, "avg_logprob": -0.1239067448510064, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.3320560558204306e-06}, {"id": 28, "seek": 16120, "start": 161.2, "end": 168.6, "text": " in layer three and that these activations are activations with layer three.", "tokens": [50364, 294, 4583, 1045, 293, 300, 613, 2430, 763, 366, 2430, 763, 365, 4583, 1045, 13, 50734, 50734, 13428, 300, 341, 1433, 510, 307, 261, 16, 37906, 5944, 3732, 16904, 1045, 3620, 264, 9834, 50998, 50998, 6615, 365, 4583, 1045, 5893, 1674, 365, 257, 37906, 5944, 3732, 16904, 732, 11, 597, 390, 51332, 51332, 264, 5598, 295, 4583, 732, 11, 597, 3062, 264, 4846, 281, 4583, 1045, 13, 51524, 51524, 407, 300, 311, 983, 456, 311, 257, 1045, 510, 570, 456, 311, 257, 13075, 6615, 365, 4583, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.126934814453125, "compression_ratio": 2.0721649484536084, "no_speech_prob": 8.93954529601615e-06}, {"id": 29, "seek": 16120, "start": 168.6, "end": 173.88, "text": " Notice that this term here is w1 superscript square bracket three meaning the parameters", "tokens": [50364, 294, 4583, 1045, 293, 300, 613, 2430, 763, 366, 2430, 763, 365, 4583, 1045, 13, 50734, 50734, 13428, 300, 341, 1433, 510, 307, 261, 16, 37906, 5944, 3732, 16904, 1045, 3620, 264, 9834, 50998, 50998, 6615, 365, 4583, 1045, 5893, 1674, 365, 257, 37906, 5944, 3732, 16904, 732, 11, 597, 390, 51332, 51332, 264, 5598, 295, 4583, 732, 11, 597, 3062, 264, 4846, 281, 4583, 1045, 13, 51524, 51524, 407, 300, 311, 983, 456, 311, 257, 1045, 510, 570, 456, 311, 257, 13075, 6615, 365, 4583, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.126934814453125, "compression_ratio": 2.0721649484536084, "no_speech_prob": 8.93954529601615e-06}, {"id": 30, "seek": 16120, "start": 173.88, "end": 180.56, "text": " associated with layer three dot product with a superscript square bracket two, which was", "tokens": [50364, 294, 4583, 1045, 293, 300, 613, 2430, 763, 366, 2430, 763, 365, 4583, 1045, 13, 50734, 50734, 13428, 300, 341, 1433, 510, 307, 261, 16, 37906, 5944, 3732, 16904, 1045, 3620, 264, 9834, 50998, 50998, 6615, 365, 4583, 1045, 5893, 1674, 365, 257, 37906, 5944, 3732, 16904, 732, 11, 597, 390, 51332, 51332, 264, 5598, 295, 4583, 732, 11, 597, 3062, 264, 4846, 281, 4583, 1045, 13, 51524, 51524, 407, 300, 311, 983, 456, 311, 257, 1045, 510, 570, 456, 311, 257, 13075, 6615, 365, 4583, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.126934814453125, "compression_ratio": 2.0721649484536084, "no_speech_prob": 8.93954529601615e-06}, {"id": 31, "seek": 16120, "start": 180.56, "end": 184.39999999999998, "text": " the output of layer two, which became the input to layer three.", "tokens": [50364, 294, 4583, 1045, 293, 300, 613, 2430, 763, 366, 2430, 763, 365, 4583, 1045, 13, 50734, 50734, 13428, 300, 341, 1433, 510, 307, 261, 16, 37906, 5944, 3732, 16904, 1045, 3620, 264, 9834, 50998, 50998, 6615, 365, 4583, 1045, 5893, 1674, 365, 257, 37906, 5944, 3732, 16904, 732, 11, 597, 390, 51332, 51332, 264, 5598, 295, 4583, 732, 11, 597, 3062, 264, 4846, 281, 4583, 1045, 13, 51524, 51524, 407, 300, 311, 983, 456, 311, 257, 1045, 510, 570, 456, 311, 257, 13075, 6615, 365, 4583, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.126934814453125, "compression_ratio": 2.0721649484536084, "no_speech_prob": 8.93954529601615e-06}, {"id": 32, "seek": 16120, "start": 184.39999999999998, "end": 188.2, "text": " So that's why there's a three here because there's a parameter associated with layer", "tokens": [50364, 294, 4583, 1045, 293, 300, 613, 2430, 763, 366, 2430, 763, 365, 4583, 1045, 13, 50734, 50734, 13428, 300, 341, 1433, 510, 307, 261, 16, 37906, 5944, 3732, 16904, 1045, 3620, 264, 9834, 50998, 50998, 6615, 365, 4583, 1045, 5893, 1674, 365, 257, 37906, 5944, 3732, 16904, 732, 11, 597, 390, 51332, 51332, 264, 5598, 295, 4583, 732, 11, 597, 3062, 264, 4846, 281, 4583, 1045, 13, 51524, 51524, 407, 300, 311, 983, 456, 311, 257, 1045, 510, 570, 456, 311, 257, 13075, 6615, 365, 4583, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.126934814453125, "compression_ratio": 2.0721649484536084, "no_speech_prob": 8.93954529601615e-06}, {"id": 33, "seek": 18820, "start": 188.2, "end": 194.04, "text": " three dot product with and there's a two there because it's the output of layer two.", "tokens": [50364, 1045, 5893, 1674, 365, 293, 456, 311, 257, 732, 456, 570, 309, 311, 264, 5598, 295, 4583, 732, 13, 50656, 50656, 823, 718, 311, 445, 360, 257, 1702, 3834, 1520, 295, 527, 3701, 295, 341, 13, 50880, 50880, 286, 478, 516, 281, 6479, 264, 37906, 5944, 82, 293, 2325, 39280, 6615, 365, 264, 1150, 34090, 293, 1553, 51318, 51318, 319, 86, 9245, 341, 960, 11, 352, 2286, 293, 41458, 498, 291, 528, 457, 291, 458, 4382, 291, 406, 11, 457, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.14773590224129812, "compression_ratio": 1.6231884057971016, "no_speech_prob": 4.425416591402609e-06}, {"id": 34, "seek": 18820, "start": 194.04, "end": 198.51999999999998, "text": " Now let's just do a quick double check of our understanding of this.", "tokens": [50364, 1045, 5893, 1674, 365, 293, 456, 311, 257, 732, 456, 570, 309, 311, 264, 5598, 295, 4583, 732, 13, 50656, 50656, 823, 718, 311, 445, 360, 257, 1702, 3834, 1520, 295, 527, 3701, 295, 341, 13, 50880, 50880, 286, 478, 516, 281, 6479, 264, 37906, 5944, 82, 293, 2325, 39280, 6615, 365, 264, 1150, 34090, 293, 1553, 51318, 51318, 319, 86, 9245, 341, 960, 11, 352, 2286, 293, 41458, 498, 291, 528, 457, 291, 458, 4382, 291, 406, 11, 457, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.14773590224129812, "compression_ratio": 1.6231884057971016, "no_speech_prob": 4.425416591402609e-06}, {"id": 35, "seek": 18820, "start": 198.51999999999998, "end": 207.28, "text": " I'm going to hide the superscripts and subscripts associated with the second neuron and without", "tokens": [50364, 1045, 5893, 1674, 365, 293, 456, 311, 257, 732, 456, 570, 309, 311, 264, 5598, 295, 4583, 732, 13, 50656, 50656, 823, 718, 311, 445, 360, 257, 1702, 3834, 1520, 295, 527, 3701, 295, 341, 13, 50880, 50880, 286, 478, 516, 281, 6479, 264, 37906, 5944, 82, 293, 2325, 39280, 6615, 365, 264, 1150, 34090, 293, 1553, 51318, 51318, 319, 86, 9245, 341, 960, 11, 352, 2286, 293, 41458, 498, 291, 528, 457, 291, 458, 4382, 291, 406, 11, 457, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.14773590224129812, "compression_ratio": 1.6231884057971016, "no_speech_prob": 4.425416591402609e-06}, {"id": 36, "seek": 18820, "start": 207.28, "end": 212.51999999999998, "text": " rewinding this video, go ahead and rewind if you want but you know prefer you not, but", "tokens": [50364, 1045, 5893, 1674, 365, 293, 456, 311, 257, 732, 456, 570, 309, 311, 264, 5598, 295, 4583, 732, 13, 50656, 50656, 823, 718, 311, 445, 360, 257, 1702, 3834, 1520, 295, 527, 3701, 295, 341, 13, 50880, 50880, 286, 478, 516, 281, 6479, 264, 37906, 5944, 82, 293, 2325, 39280, 6615, 365, 264, 1150, 34090, 293, 1553, 51318, 51318, 319, 86, 9245, 341, 960, 11, 352, 2286, 293, 41458, 498, 291, 528, 457, 291, 458, 4382, 291, 406, 11, 457, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.14773590224129812, "compression_ratio": 1.6231884057971016, "no_speech_prob": 4.425416591402609e-06}, {"id": 37, "seek": 21252, "start": 212.52, "end": 218.4, "text": " without rewinding this video, are you able to think through what are the missing superscripts", "tokens": [50364, 1553, 319, 86, 9245, 341, 960, 11, 366, 291, 1075, 281, 519, 807, 437, 366, 264, 5361, 37906, 5944, 82, 50658, 50658, 293, 2325, 39280, 294, 341, 5367, 293, 2836, 552, 294, 1803, 30, 50854, 50854, 1545, 500, 380, 291, 747, 257, 574, 412, 264, 917, 960, 15450, 293, 536, 498, 291, 393, 2573, 484, 437, 366, 51060, 51060, 264, 6854, 37906, 5944, 82, 293, 2325, 39280, 337, 341, 5367, 670, 510, 13, 51256], "temperature": 0.0, "avg_logprob": -0.14299023778815018, "compression_ratio": 1.723756906077348, "no_speech_prob": 6.475767690972134e-07}, {"id": 38, "seek": 21252, "start": 218.4, "end": 222.32000000000002, "text": " and subscripts in this equation and fill them in yourself?", "tokens": [50364, 1553, 319, 86, 9245, 341, 960, 11, 366, 291, 1075, 281, 519, 807, 437, 366, 264, 5361, 37906, 5944, 82, 50658, 50658, 293, 2325, 39280, 294, 341, 5367, 293, 2836, 552, 294, 1803, 30, 50854, 50854, 1545, 500, 380, 291, 747, 257, 574, 412, 264, 917, 960, 15450, 293, 536, 498, 291, 393, 2573, 484, 437, 366, 51060, 51060, 264, 6854, 37906, 5944, 82, 293, 2325, 39280, 337, 341, 5367, 670, 510, 13, 51256], "temperature": 0.0, "avg_logprob": -0.14299023778815018, "compression_ratio": 1.723756906077348, "no_speech_prob": 6.475767690972134e-07}, {"id": 39, "seek": 21252, "start": 222.32000000000002, "end": 226.44, "text": " Why don't you take a look at the end video quiz and see if you can figure out what are", "tokens": [50364, 1553, 319, 86, 9245, 341, 960, 11, 366, 291, 1075, 281, 519, 807, 437, 366, 264, 5361, 37906, 5944, 82, 50658, 50658, 293, 2325, 39280, 294, 341, 5367, 293, 2836, 552, 294, 1803, 30, 50854, 50854, 1545, 500, 380, 291, 747, 257, 574, 412, 264, 917, 960, 15450, 293, 536, 498, 291, 393, 2573, 484, 437, 366, 51060, 51060, 264, 6854, 37906, 5944, 82, 293, 2325, 39280, 337, 341, 5367, 670, 510, 13, 51256], "temperature": 0.0, "avg_logprob": -0.14299023778815018, "compression_ratio": 1.723756906077348, "no_speech_prob": 6.475767690972134e-07}, {"id": 40, "seek": 22644, "start": 226.44, "end": 248.32, "text": " the appropriate superscripts and subscripts for this equation over here.", "tokens": [50364, 264, 6854, 37906, 5944, 82, 293, 2325, 39280, 337, 341, 5367, 670, 510, 13, 51458, 51458], "temperature": 0.0, "avg_logprob": -0.10104922453562419, "compression_ratio": 1.1076923076923078, "no_speech_prob": 2.2602687295147916e-06}, {"id": 41, "seek": 24832, "start": 248.32, "end": 256.56, "text": " So to recap, A3 is activation associated with layer three for the second neuron, hence this", "tokens": [50364, 407, 281, 20928, 11, 316, 18, 307, 24433, 6615, 365, 4583, 1045, 337, 264, 1150, 34090, 11, 16678, 341, 50776, 50776, 307, 257, 732, 13, 50826, 50826, 821, 311, 257, 13075, 6615, 365, 264, 2636, 4583, 337, 264, 1150, 34090, 13, 51060, 51060, 639, 307, 316, 17, 11, 912, 382, 3673, 11, 293, 550, 1804, 363, 18, 11, 732, 13, 51322, 51322, 407, 4696, 300, 1669, 2020, 13, 51466, 51466, 1692, 311, 264, 544, 2674, 1254, 295, 341, 5367, 337, 364, 23211, 4583, 441, 293, 337, 364, 23211, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.21494709927102792, "compression_ratio": 1.6941747572815533, "no_speech_prob": 1.568902371218428e-05}, {"id": 42, "seek": 24832, "start": 256.56, "end": 257.56, "text": " is a two.", "tokens": [50364, 407, 281, 20928, 11, 316, 18, 307, 24433, 6615, 365, 4583, 1045, 337, 264, 1150, 34090, 11, 16678, 341, 50776, 50776, 307, 257, 732, 13, 50826, 50826, 821, 311, 257, 13075, 6615, 365, 264, 2636, 4583, 337, 264, 1150, 34090, 13, 51060, 51060, 639, 307, 316, 17, 11, 912, 382, 3673, 11, 293, 550, 1804, 363, 18, 11, 732, 13, 51322, 51322, 407, 4696, 300, 1669, 2020, 13, 51466, 51466, 1692, 311, 264, 544, 2674, 1254, 295, 341, 5367, 337, 364, 23211, 4583, 441, 293, 337, 364, 23211, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.21494709927102792, "compression_ratio": 1.6941747572815533, "no_speech_prob": 1.568902371218428e-05}, {"id": 43, "seek": 24832, "start": 257.56, "end": 262.24, "text": " There's a parameter associated with the third layer for the second neuron.", "tokens": [50364, 407, 281, 20928, 11, 316, 18, 307, 24433, 6615, 365, 4583, 1045, 337, 264, 1150, 34090, 11, 16678, 341, 50776, 50776, 307, 257, 732, 13, 50826, 50826, 821, 311, 257, 13075, 6615, 365, 264, 2636, 4583, 337, 264, 1150, 34090, 13, 51060, 51060, 639, 307, 316, 17, 11, 912, 382, 3673, 11, 293, 550, 1804, 363, 18, 11, 732, 13, 51322, 51322, 407, 4696, 300, 1669, 2020, 13, 51466, 51466, 1692, 311, 264, 544, 2674, 1254, 295, 341, 5367, 337, 364, 23211, 4583, 441, 293, 337, 364, 23211, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.21494709927102792, "compression_ratio": 1.6941747572815533, "no_speech_prob": 1.568902371218428e-05}, {"id": 44, "seek": 24832, "start": 262.24, "end": 267.48, "text": " This is A2, same as above, and then plus B3, two.", "tokens": [50364, 407, 281, 20928, 11, 316, 18, 307, 24433, 6615, 365, 4583, 1045, 337, 264, 1150, 34090, 11, 16678, 341, 50776, 50776, 307, 257, 732, 13, 50826, 50826, 821, 311, 257, 13075, 6615, 365, 264, 2636, 4583, 337, 264, 1150, 34090, 13, 51060, 51060, 639, 307, 316, 17, 11, 912, 382, 3673, 11, 293, 550, 1804, 363, 18, 11, 732, 13, 51322, 51322, 407, 4696, 300, 1669, 2020, 13, 51466, 51466, 1692, 311, 264, 544, 2674, 1254, 295, 341, 5367, 337, 364, 23211, 4583, 441, 293, 337, 364, 23211, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.21494709927102792, "compression_ratio": 1.6941747572815533, "no_speech_prob": 1.568902371218428e-05}, {"id": 45, "seek": 24832, "start": 267.48, "end": 270.36, "text": " So hopefully that makes sense.", "tokens": [50364, 407, 281, 20928, 11, 316, 18, 307, 24433, 6615, 365, 4583, 1045, 337, 264, 1150, 34090, 11, 16678, 341, 50776, 50776, 307, 257, 732, 13, 50826, 50826, 821, 311, 257, 13075, 6615, 365, 264, 2636, 4583, 337, 264, 1150, 34090, 13, 51060, 51060, 639, 307, 316, 17, 11, 912, 382, 3673, 11, 293, 550, 1804, 363, 18, 11, 732, 13, 51322, 51322, 407, 4696, 300, 1669, 2020, 13, 51466, 51466, 1692, 311, 264, 544, 2674, 1254, 295, 341, 5367, 337, 364, 23211, 4583, 441, 293, 337, 364, 23211, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.21494709927102792, "compression_ratio": 1.6941747572815533, "no_speech_prob": 1.568902371218428e-05}, {"id": 46, "seek": 24832, "start": 270.36, "end": 275.12, "text": " Here's the more general form of this equation for an arbitrary layer L and for an arbitrary", "tokens": [50364, 407, 281, 20928, 11, 316, 18, 307, 24433, 6615, 365, 4583, 1045, 337, 264, 1150, 34090, 11, 16678, 341, 50776, 50776, 307, 257, 732, 13, 50826, 50826, 821, 311, 257, 13075, 6615, 365, 264, 2636, 4583, 337, 264, 1150, 34090, 13, 51060, 51060, 639, 307, 316, 17, 11, 912, 382, 3673, 11, 293, 550, 1804, 363, 18, 11, 732, 13, 51322, 51322, 407, 4696, 300, 1669, 2020, 13, 51466, 51466, 1692, 311, 264, 544, 2674, 1254, 295, 341, 5367, 337, 364, 23211, 4583, 441, 293, 337, 364, 23211, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.21494709927102792, "compression_ratio": 1.6941747572815533, "no_speech_prob": 1.568902371218428e-05}, {"id": 47, "seek": 27512, "start": 275.12, "end": 285.4, "text": " unit J, which is that A deactivation output of layer L unit J like A3, two, that's going", "tokens": [50364, 4985, 508, 11, 597, 307, 300, 316, 45428, 399, 5598, 295, 4583, 441, 4985, 508, 411, 316, 18, 11, 732, 11, 300, 311, 516, 50878, 50878, 281, 312, 264, 4556, 3280, 327, 2445, 6456, 281, 341, 1433, 11, 597, 307, 264, 3364, 8062, 295, 4583, 51250, 51250, 441, 1270, 382, 4583, 1045, 337, 264, 508, 392, 4985, 13, 51418, 51418, 407, 456, 311, 732, 797, 294, 264, 1365, 3673, 13, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.25518961210508606, "compression_ratio": 1.4883720930232558, "no_speech_prob": 2.8408187063178048e-05}, {"id": 48, "seek": 27512, "start": 285.4, "end": 292.84000000000003, "text": " to be the sigmoid function applied to this term, which is the weight vector of layer", "tokens": [50364, 4985, 508, 11, 597, 307, 300, 316, 45428, 399, 5598, 295, 4583, 441, 4985, 508, 411, 316, 18, 11, 732, 11, 300, 311, 516, 50878, 50878, 281, 312, 264, 4556, 3280, 327, 2445, 6456, 281, 341, 1433, 11, 597, 307, 264, 3364, 8062, 295, 4583, 51250, 51250, 441, 1270, 382, 4583, 1045, 337, 264, 508, 392, 4985, 13, 51418, 51418, 407, 456, 311, 732, 797, 294, 264, 1365, 3673, 13, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.25518961210508606, "compression_ratio": 1.4883720930232558, "no_speech_prob": 2.8408187063178048e-05}, {"id": 49, "seek": 27512, "start": 292.84000000000003, "end": 296.2, "text": " L such as layer three for the Jth unit.", "tokens": [50364, 4985, 508, 11, 597, 307, 300, 316, 45428, 399, 5598, 295, 4583, 441, 4985, 508, 411, 316, 18, 11, 732, 11, 300, 311, 516, 50878, 50878, 281, 312, 264, 4556, 3280, 327, 2445, 6456, 281, 341, 1433, 11, 597, 307, 264, 3364, 8062, 295, 4583, 51250, 51250, 441, 1270, 382, 4583, 1045, 337, 264, 508, 392, 4985, 13, 51418, 51418, 407, 456, 311, 732, 797, 294, 264, 1365, 3673, 13, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.25518961210508606, "compression_ratio": 1.4883720930232558, "no_speech_prob": 2.8408187063178048e-05}, {"id": 50, "seek": 27512, "start": 296.2, "end": 299.04, "text": " So there's two again in the example above.", "tokens": [50364, 4985, 508, 11, 597, 307, 300, 316, 45428, 399, 5598, 295, 4583, 441, 4985, 508, 411, 316, 18, 11, 732, 11, 300, 311, 516, 50878, 50878, 281, 312, 264, 4556, 3280, 327, 2445, 6456, 281, 341, 1433, 11, 597, 307, 264, 3364, 8062, 295, 4583, 51250, 51250, 441, 1270, 382, 4583, 1045, 337, 264, 508, 392, 4985, 13, 51418, 51418, 407, 456, 311, 732, 797, 294, 264, 1365, 3673, 13, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.25518961210508606, "compression_ratio": 1.4883720930232558, "no_speech_prob": 2.8408187063178048e-05}, {"id": 51, "seek": 29904, "start": 299.04, "end": 305.40000000000003, "text": " And so that's dot product with A deactivation value of, and notice this is not L, this is", "tokens": [50364, 400, 370, 300, 311, 5893, 1674, 365, 316, 45428, 399, 2158, 295, 11, 293, 3449, 341, 307, 406, 441, 11, 341, 307, 50682, 50682, 441, 3175, 472, 11, 411, 264, 732, 3673, 510, 11, 570, 291, 434, 5893, 1674, 278, 365, 264, 5598, 490, 51004, 51004, 264, 3894, 4583, 11, 293, 550, 1804, 363, 11, 264, 13075, 337, 341, 4583, 337, 300, 4985, 508, 13, 51334, 51334, 400, 370, 341, 2709, 291, 264, 24433, 295, 4583, 441, 4985, 508, 11, 689, 264, 37906, 5944, 294, 3732, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.14516963958740234, "compression_ratio": 1.6698564593301435, "no_speech_prob": 5.2552800298144575e-06}, {"id": 52, "seek": 29904, "start": 305.40000000000003, "end": 311.84000000000003, "text": " L minus one, like the two above here, because you're dot producting with the output from", "tokens": [50364, 400, 370, 300, 311, 5893, 1674, 365, 316, 45428, 399, 2158, 295, 11, 293, 3449, 341, 307, 406, 441, 11, 341, 307, 50682, 50682, 441, 3175, 472, 11, 411, 264, 732, 3673, 510, 11, 570, 291, 434, 5893, 1674, 278, 365, 264, 5598, 490, 51004, 51004, 264, 3894, 4583, 11, 293, 550, 1804, 363, 11, 264, 13075, 337, 341, 4583, 337, 300, 4985, 508, 13, 51334, 51334, 400, 370, 341, 2709, 291, 264, 24433, 295, 4583, 441, 4985, 508, 11, 689, 264, 37906, 5944, 294, 3732, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.14516963958740234, "compression_ratio": 1.6698564593301435, "no_speech_prob": 5.2552800298144575e-06}, {"id": 53, "seek": 29904, "start": 311.84000000000003, "end": 318.44, "text": " the previous layer, and then plus B, the parameter for this layer for that unit J.", "tokens": [50364, 400, 370, 300, 311, 5893, 1674, 365, 316, 45428, 399, 2158, 295, 11, 293, 3449, 341, 307, 406, 441, 11, 341, 307, 50682, 50682, 441, 3175, 472, 11, 411, 264, 732, 3673, 510, 11, 570, 291, 434, 5893, 1674, 278, 365, 264, 5598, 490, 51004, 51004, 264, 3894, 4583, 11, 293, 550, 1804, 363, 11, 264, 13075, 337, 341, 4583, 337, 300, 4985, 508, 13, 51334, 51334, 400, 370, 341, 2709, 291, 264, 24433, 295, 4583, 441, 4985, 508, 11, 689, 264, 37906, 5944, 294, 3732, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.14516963958740234, "compression_ratio": 1.6698564593301435, "no_speech_prob": 5.2552800298144575e-06}, {"id": 54, "seek": 29904, "start": 318.44, "end": 325.48, "text": " And so this gives you the activation of layer L unit J, where the superscript in square", "tokens": [50364, 400, 370, 300, 311, 5893, 1674, 365, 316, 45428, 399, 2158, 295, 11, 293, 3449, 341, 307, 406, 441, 11, 341, 307, 50682, 50682, 441, 3175, 472, 11, 411, 264, 732, 3673, 510, 11, 570, 291, 434, 5893, 1674, 278, 365, 264, 5598, 490, 51004, 51004, 264, 3894, 4583, 11, 293, 550, 1804, 363, 11, 264, 13075, 337, 341, 4583, 337, 300, 4985, 508, 13, 51334, 51334, 400, 370, 341, 2709, 291, 264, 24433, 295, 4583, 441, 4985, 508, 11, 689, 264, 37906, 5944, 294, 3732, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.14516963958740234, "compression_ratio": 1.6698564593301435, "no_speech_prob": 5.2552800298144575e-06}, {"id": 55, "seek": 32548, "start": 325.48, "end": 330.76, "text": " brackets L denotes layer L and the subscript J denotes unit J.", "tokens": [50364, 26179, 441, 1441, 17251, 4583, 441, 293, 264, 2325, 662, 508, 1441, 17251, 4985, 508, 13, 50628, 50628, 400, 562, 2390, 18161, 9590, 11, 4985, 508, 14942, 281, 264, 508, 392, 34090, 13, 50868, 50868, 407, 764, 729, 2115, 257, 707, 857, 30358, 1188, 11, 689, 1184, 4985, 307, 257, 2167, 34090, 294, 257, 4583, 13, 51152, 51152, 460, 510, 307, 264, 4556, 3280, 327, 2445, 13, 51276, 51276, 682, 264, 4319, 295, 257, 18161, 3209, 11, 460, 575, 1071, 1315, 11, 597, 307, 611, 1219, 264, 24433, 51546, 51546, 2445, 570, 460, 23930, 341, 24433, 2158, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11770802853154201, "compression_ratio": 1.680672268907563, "no_speech_prob": 3.138110969302943e-06}, {"id": 56, "seek": 32548, "start": 330.76, "end": 335.56, "text": " And when building neural networks, unit J refers to the Jth neuron.", "tokens": [50364, 26179, 441, 1441, 17251, 4583, 441, 293, 264, 2325, 662, 508, 1441, 17251, 4985, 508, 13, 50628, 50628, 400, 562, 2390, 18161, 9590, 11, 4985, 508, 14942, 281, 264, 508, 392, 34090, 13, 50868, 50868, 407, 764, 729, 2115, 257, 707, 857, 30358, 1188, 11, 689, 1184, 4985, 307, 257, 2167, 34090, 294, 257, 4583, 13, 51152, 51152, 460, 510, 307, 264, 4556, 3280, 327, 2445, 13, 51276, 51276, 682, 264, 4319, 295, 257, 18161, 3209, 11, 460, 575, 1071, 1315, 11, 597, 307, 611, 1219, 264, 24433, 51546, 51546, 2445, 570, 460, 23930, 341, 24433, 2158, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11770802853154201, "compression_ratio": 1.680672268907563, "no_speech_prob": 3.138110969302943e-06}, {"id": 57, "seek": 32548, "start": 335.56, "end": 341.24, "text": " So use those terms a little bit interchangeably, where each unit is a single neuron in a layer.", "tokens": [50364, 26179, 441, 1441, 17251, 4583, 441, 293, 264, 2325, 662, 508, 1441, 17251, 4985, 508, 13, 50628, 50628, 400, 562, 2390, 18161, 9590, 11, 4985, 508, 14942, 281, 264, 508, 392, 34090, 13, 50868, 50868, 407, 764, 729, 2115, 257, 707, 857, 30358, 1188, 11, 689, 1184, 4985, 307, 257, 2167, 34090, 294, 257, 4583, 13, 51152, 51152, 460, 510, 307, 264, 4556, 3280, 327, 2445, 13, 51276, 51276, 682, 264, 4319, 295, 257, 18161, 3209, 11, 460, 575, 1071, 1315, 11, 597, 307, 611, 1219, 264, 24433, 51546, 51546, 2445, 570, 460, 23930, 341, 24433, 2158, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11770802853154201, "compression_ratio": 1.680672268907563, "no_speech_prob": 3.138110969302943e-06}, {"id": 58, "seek": 32548, "start": 341.24, "end": 343.72, "text": " G here is the sigmoid function.", "tokens": [50364, 26179, 441, 1441, 17251, 4583, 441, 293, 264, 2325, 662, 508, 1441, 17251, 4985, 508, 13, 50628, 50628, 400, 562, 2390, 18161, 9590, 11, 4985, 508, 14942, 281, 264, 508, 392, 34090, 13, 50868, 50868, 407, 764, 729, 2115, 257, 707, 857, 30358, 1188, 11, 689, 1184, 4985, 307, 257, 2167, 34090, 294, 257, 4583, 13, 51152, 51152, 460, 510, 307, 264, 4556, 3280, 327, 2445, 13, 51276, 51276, 682, 264, 4319, 295, 257, 18161, 3209, 11, 460, 575, 1071, 1315, 11, 597, 307, 611, 1219, 264, 24433, 51546, 51546, 2445, 570, 460, 23930, 341, 24433, 2158, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11770802853154201, "compression_ratio": 1.680672268907563, "no_speech_prob": 3.138110969302943e-06}, {"id": 59, "seek": 32548, "start": 343.72, "end": 349.12, "text": " In the context of a neural network, G has another name, which is also called the activation", "tokens": [50364, 26179, 441, 1441, 17251, 4583, 441, 293, 264, 2325, 662, 508, 1441, 17251, 4985, 508, 13, 50628, 50628, 400, 562, 2390, 18161, 9590, 11, 4985, 508, 14942, 281, 264, 508, 392, 34090, 13, 50868, 50868, 407, 764, 729, 2115, 257, 707, 857, 30358, 1188, 11, 689, 1184, 4985, 307, 257, 2167, 34090, 294, 257, 4583, 13, 51152, 51152, 460, 510, 307, 264, 4556, 3280, 327, 2445, 13, 51276, 51276, 682, 264, 4319, 295, 257, 18161, 3209, 11, 460, 575, 1071, 1315, 11, 597, 307, 611, 1219, 264, 24433, 51546, 51546, 2445, 570, 460, 23930, 341, 24433, 2158, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11770802853154201, "compression_ratio": 1.680672268907563, "no_speech_prob": 3.138110969302943e-06}, {"id": 60, "seek": 32548, "start": 349.12, "end": 353.56, "text": " function because G outputs this activation value.", "tokens": [50364, 26179, 441, 1441, 17251, 4583, 441, 293, 264, 2325, 662, 508, 1441, 17251, 4985, 508, 13, 50628, 50628, 400, 562, 2390, 18161, 9590, 11, 4985, 508, 14942, 281, 264, 508, 392, 34090, 13, 50868, 50868, 407, 764, 729, 2115, 257, 707, 857, 30358, 1188, 11, 689, 1184, 4985, 307, 257, 2167, 34090, 294, 257, 4583, 13, 51152, 51152, 460, 510, 307, 264, 4556, 3280, 327, 2445, 13, 51276, 51276, 682, 264, 4319, 295, 257, 18161, 3209, 11, 460, 575, 1071, 1315, 11, 597, 307, 611, 1219, 264, 24433, 51546, 51546, 2445, 570, 460, 23930, 341, 24433, 2158, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11770802853154201, "compression_ratio": 1.680672268907563, "no_speech_prob": 3.138110969302943e-06}, {"id": 61, "seek": 35356, "start": 353.56, "end": 358.8, "text": " So when I say activation function, I mean this function G here.", "tokens": [50364, 407, 562, 286, 584, 24433, 2445, 11, 286, 914, 341, 2445, 460, 510, 13, 50626, 50626, 400, 370, 1400, 11, 264, 787, 24433, 2445, 291, 600, 1612, 307, 264, 4556, 3280, 327, 2445, 13, 50870, 50870, 583, 958, 1243, 11, 321, 603, 574, 412, 562, 661, 6828, 813, 264, 4556, 3280, 327, 2445, 393, 312, 25679, 294, 51102, 51102, 294, 1081, 295, 460, 382, 731, 13, 51176, 51176, 583, 370, 264, 24433, 2445, 307, 445, 300, 2445, 300, 23930, 613, 24433, 4190, 13, 51531, 51531], "temperature": 0.0, "avg_logprob": -0.12850671527029453, "compression_ratio": 1.8177083333333333, "no_speech_prob": 1.963781187441782e-06}, {"id": 62, "seek": 35356, "start": 358.8, "end": 363.68, "text": " And so far, the only activation function you've seen is the sigmoid function.", "tokens": [50364, 407, 562, 286, 584, 24433, 2445, 11, 286, 914, 341, 2445, 460, 510, 13, 50626, 50626, 400, 370, 1400, 11, 264, 787, 24433, 2445, 291, 600, 1612, 307, 264, 4556, 3280, 327, 2445, 13, 50870, 50870, 583, 958, 1243, 11, 321, 603, 574, 412, 562, 661, 6828, 813, 264, 4556, 3280, 327, 2445, 393, 312, 25679, 294, 51102, 51102, 294, 1081, 295, 460, 382, 731, 13, 51176, 51176, 583, 370, 264, 24433, 2445, 307, 445, 300, 2445, 300, 23930, 613, 24433, 4190, 13, 51531, 51531], "temperature": 0.0, "avg_logprob": -0.12850671527029453, "compression_ratio": 1.8177083333333333, "no_speech_prob": 1.963781187441782e-06}, {"id": 63, "seek": 35356, "start": 363.68, "end": 368.32, "text": " But next week, we'll look at when other functions than the sigmoid function can be plugged in", "tokens": [50364, 407, 562, 286, 584, 24433, 2445, 11, 286, 914, 341, 2445, 460, 510, 13, 50626, 50626, 400, 370, 1400, 11, 264, 787, 24433, 2445, 291, 600, 1612, 307, 264, 4556, 3280, 327, 2445, 13, 50870, 50870, 583, 958, 1243, 11, 321, 603, 574, 412, 562, 661, 6828, 813, 264, 4556, 3280, 327, 2445, 393, 312, 25679, 294, 51102, 51102, 294, 1081, 295, 460, 382, 731, 13, 51176, 51176, 583, 370, 264, 24433, 2445, 307, 445, 300, 2445, 300, 23930, 613, 24433, 4190, 13, 51531, 51531], "temperature": 0.0, "avg_logprob": -0.12850671527029453, "compression_ratio": 1.8177083333333333, "no_speech_prob": 1.963781187441782e-06}, {"id": 64, "seek": 35356, "start": 368.32, "end": 369.8, "text": " in place of G as well.", "tokens": [50364, 407, 562, 286, 584, 24433, 2445, 11, 286, 914, 341, 2445, 460, 510, 13, 50626, 50626, 400, 370, 1400, 11, 264, 787, 24433, 2445, 291, 600, 1612, 307, 264, 4556, 3280, 327, 2445, 13, 50870, 50870, 583, 958, 1243, 11, 321, 603, 574, 412, 562, 661, 6828, 813, 264, 4556, 3280, 327, 2445, 393, 312, 25679, 294, 51102, 51102, 294, 1081, 295, 460, 382, 731, 13, 51176, 51176, 583, 370, 264, 24433, 2445, 307, 445, 300, 2445, 300, 23930, 613, 24433, 4190, 13, 51531, 51531], "temperature": 0.0, "avg_logprob": -0.12850671527029453, "compression_ratio": 1.8177083333333333, "no_speech_prob": 1.963781187441782e-06}, {"id": 65, "seek": 35356, "start": 369.8, "end": 376.9, "text": " But so the activation function is just that function that outputs these activation values.", "tokens": [50364, 407, 562, 286, 584, 24433, 2445, 11, 286, 914, 341, 2445, 460, 510, 13, 50626, 50626, 400, 370, 1400, 11, 264, 787, 24433, 2445, 291, 600, 1612, 307, 264, 4556, 3280, 327, 2445, 13, 50870, 50870, 583, 958, 1243, 11, 321, 603, 574, 412, 562, 661, 6828, 813, 264, 4556, 3280, 327, 2445, 393, 312, 25679, 294, 51102, 51102, 294, 1081, 295, 460, 382, 731, 13, 51176, 51176, 583, 370, 264, 24433, 2445, 307, 445, 300, 2445, 300, 23930, 613, 24433, 4190, 13, 51531, 51531], "temperature": 0.0, "avg_logprob": -0.12850671527029453, "compression_ratio": 1.8177083333333333, "no_speech_prob": 1.963781187441782e-06}, {"id": 66, "seek": 37690, "start": 376.9, "end": 384.52, "text": " And just one last piece of notation in order to make all this notation consistent.", "tokens": [50364, 400, 445, 472, 1036, 2522, 295, 24657, 294, 1668, 281, 652, 439, 341, 24657, 8398, 13, 50745, 50745, 286, 478, 611, 884, 281, 976, 264, 4846, 8062, 1783, 1071, 1315, 11, 597, 307, 257, 4018, 13, 51065, 51065, 407, 341, 636, 11, 264, 912, 5367, 611, 1985, 337, 264, 700, 4583, 11, 689, 562, 441, 307, 2681, 51343, 51343, 281, 472, 45428, 763, 295, 264, 700, 4583, 300, 307, 257, 472, 486, 312, 4556, 3280, 327, 1413, 264, 17443, 51657, 51657], "temperature": 0.0, "avg_logprob": -0.16639999021966773, "compression_ratio": 1.5769230769230769, "no_speech_prob": 2.5612528133933665e-06}, {"id": 67, "seek": 37690, "start": 384.52, "end": 390.91999999999996, "text": " I'm also doing to give the input vector X another name, which is a zero.", "tokens": [50364, 400, 445, 472, 1036, 2522, 295, 24657, 294, 1668, 281, 652, 439, 341, 24657, 8398, 13, 50745, 50745, 286, 478, 611, 884, 281, 976, 264, 4846, 8062, 1783, 1071, 1315, 11, 597, 307, 257, 4018, 13, 51065, 51065, 407, 341, 636, 11, 264, 912, 5367, 611, 1985, 337, 264, 700, 4583, 11, 689, 562, 441, 307, 2681, 51343, 51343, 281, 472, 45428, 763, 295, 264, 700, 4583, 300, 307, 257, 472, 486, 312, 4556, 3280, 327, 1413, 264, 17443, 51657, 51657], "temperature": 0.0, "avg_logprob": -0.16639999021966773, "compression_ratio": 1.5769230769230769, "no_speech_prob": 2.5612528133933665e-06}, {"id": 68, "seek": 37690, "start": 390.91999999999996, "end": 396.47999999999996, "text": " So this way, the same equation also works for the first layer, where when L is equal", "tokens": [50364, 400, 445, 472, 1036, 2522, 295, 24657, 294, 1668, 281, 652, 439, 341, 24657, 8398, 13, 50745, 50745, 286, 478, 611, 884, 281, 976, 264, 4846, 8062, 1783, 1071, 1315, 11, 597, 307, 257, 4018, 13, 51065, 51065, 407, 341, 636, 11, 264, 912, 5367, 611, 1985, 337, 264, 700, 4583, 11, 689, 562, 441, 307, 2681, 51343, 51343, 281, 472, 45428, 763, 295, 264, 700, 4583, 300, 307, 257, 472, 486, 312, 4556, 3280, 327, 1413, 264, 17443, 51657, 51657], "temperature": 0.0, "avg_logprob": -0.16639999021966773, "compression_ratio": 1.5769230769230769, "no_speech_prob": 2.5612528133933665e-06}, {"id": 69, "seek": 37690, "start": 396.47999999999996, "end": 402.76, "text": " to one deactivations of the first layer that is a one will be sigmoid times the weights", "tokens": [50364, 400, 445, 472, 1036, 2522, 295, 24657, 294, 1668, 281, 652, 439, 341, 24657, 8398, 13, 50745, 50745, 286, 478, 611, 884, 281, 976, 264, 4846, 8062, 1783, 1071, 1315, 11, 597, 307, 257, 4018, 13, 51065, 51065, 407, 341, 636, 11, 264, 912, 5367, 611, 1985, 337, 264, 700, 4583, 11, 689, 562, 441, 307, 2681, 51343, 51343, 281, 472, 45428, 763, 295, 264, 700, 4583, 300, 307, 257, 472, 486, 312, 4556, 3280, 327, 1413, 264, 17443, 51657, 51657], "temperature": 0.0, "avg_logprob": -0.16639999021966773, "compression_ratio": 1.5769230769230769, "no_speech_prob": 2.5612528133933665e-06}, {"id": 70, "seek": 40276, "start": 402.76, "end": 408.96, "text": " dot product with a zero, which is just this input feature vector X.", "tokens": [50364, 5893, 1674, 365, 257, 4018, 11, 597, 307, 445, 341, 4846, 4111, 8062, 1783, 13, 50674, 50674, 407, 365, 341, 24657, 11, 291, 586, 458, 577, 281, 14722, 264, 24433, 4190, 295, 604, 4583, 50962, 50962, 294, 257, 18161, 3209, 382, 257, 2445, 295, 264, 9834, 382, 731, 382, 264, 2430, 763, 295, 264, 3894, 51256, 51256, 4583, 13, 51306, 51306, 407, 291, 586, 458, 577, 281, 14722, 264, 2430, 763, 295, 604, 4583, 2212, 264, 2430, 763, 295, 264, 51568, 51568, 3894, 4583, 13, 51646, 51646, 961, 311, 829, 341, 666, 364, 38253, 9284, 337, 257, 18161, 3209, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1073295519902156, "compression_ratio": 2.0238095238095237, "no_speech_prob": 1.1726301636372227e-06}, {"id": 71, "seek": 40276, "start": 408.96, "end": 414.71999999999997, "text": " So with this notation, you now know how to compute the activation values of any layer", "tokens": [50364, 5893, 1674, 365, 257, 4018, 11, 597, 307, 445, 341, 4846, 4111, 8062, 1783, 13, 50674, 50674, 407, 365, 341, 24657, 11, 291, 586, 458, 577, 281, 14722, 264, 24433, 4190, 295, 604, 4583, 50962, 50962, 294, 257, 18161, 3209, 382, 257, 2445, 295, 264, 9834, 382, 731, 382, 264, 2430, 763, 295, 264, 3894, 51256, 51256, 4583, 13, 51306, 51306, 407, 291, 586, 458, 577, 281, 14722, 264, 2430, 763, 295, 604, 4583, 2212, 264, 2430, 763, 295, 264, 51568, 51568, 3894, 4583, 13, 51646, 51646, 961, 311, 829, 341, 666, 364, 38253, 9284, 337, 257, 18161, 3209, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1073295519902156, "compression_ratio": 2.0238095238095237, "no_speech_prob": 1.1726301636372227e-06}, {"id": 72, "seek": 40276, "start": 414.71999999999997, "end": 420.59999999999997, "text": " in a neural network as a function of the parameters as well as the activations of the previous", "tokens": [50364, 5893, 1674, 365, 257, 4018, 11, 597, 307, 445, 341, 4846, 4111, 8062, 1783, 13, 50674, 50674, 407, 365, 341, 24657, 11, 291, 586, 458, 577, 281, 14722, 264, 24433, 4190, 295, 604, 4583, 50962, 50962, 294, 257, 18161, 3209, 382, 257, 2445, 295, 264, 9834, 382, 731, 382, 264, 2430, 763, 295, 264, 3894, 51256, 51256, 4583, 13, 51306, 51306, 407, 291, 586, 458, 577, 281, 14722, 264, 2430, 763, 295, 604, 4583, 2212, 264, 2430, 763, 295, 264, 51568, 51568, 3894, 4583, 13, 51646, 51646, 961, 311, 829, 341, 666, 364, 38253, 9284, 337, 257, 18161, 3209, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1073295519902156, "compression_ratio": 2.0238095238095237, "no_speech_prob": 1.1726301636372227e-06}, {"id": 73, "seek": 40276, "start": 420.59999999999997, "end": 421.59999999999997, "text": " layer.", "tokens": [50364, 5893, 1674, 365, 257, 4018, 11, 597, 307, 445, 341, 4846, 4111, 8062, 1783, 13, 50674, 50674, 407, 365, 341, 24657, 11, 291, 586, 458, 577, 281, 14722, 264, 24433, 4190, 295, 604, 4583, 50962, 50962, 294, 257, 18161, 3209, 382, 257, 2445, 295, 264, 9834, 382, 731, 382, 264, 2430, 763, 295, 264, 3894, 51256, 51256, 4583, 13, 51306, 51306, 407, 291, 586, 458, 577, 281, 14722, 264, 2430, 763, 295, 604, 4583, 2212, 264, 2430, 763, 295, 264, 51568, 51568, 3894, 4583, 13, 51646, 51646, 961, 311, 829, 341, 666, 364, 38253, 9284, 337, 257, 18161, 3209, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1073295519902156, "compression_ratio": 2.0238095238095237, "no_speech_prob": 1.1726301636372227e-06}, {"id": 74, "seek": 40276, "start": 421.59999999999997, "end": 426.84, "text": " So you now know how to compute the activations of any layer given the activations of the", "tokens": [50364, 5893, 1674, 365, 257, 4018, 11, 597, 307, 445, 341, 4846, 4111, 8062, 1783, 13, 50674, 50674, 407, 365, 341, 24657, 11, 291, 586, 458, 577, 281, 14722, 264, 24433, 4190, 295, 604, 4583, 50962, 50962, 294, 257, 18161, 3209, 382, 257, 2445, 295, 264, 9834, 382, 731, 382, 264, 2430, 763, 295, 264, 3894, 51256, 51256, 4583, 13, 51306, 51306, 407, 291, 586, 458, 577, 281, 14722, 264, 2430, 763, 295, 604, 4583, 2212, 264, 2430, 763, 295, 264, 51568, 51568, 3894, 4583, 13, 51646, 51646, 961, 311, 829, 341, 666, 364, 38253, 9284, 337, 257, 18161, 3209, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1073295519902156, "compression_ratio": 2.0238095238095237, "no_speech_prob": 1.1726301636372227e-06}, {"id": 75, "seek": 40276, "start": 426.84, "end": 428.4, "text": " previous layer.", "tokens": [50364, 5893, 1674, 365, 257, 4018, 11, 597, 307, 445, 341, 4846, 4111, 8062, 1783, 13, 50674, 50674, 407, 365, 341, 24657, 11, 291, 586, 458, 577, 281, 14722, 264, 24433, 4190, 295, 604, 4583, 50962, 50962, 294, 257, 18161, 3209, 382, 257, 2445, 295, 264, 9834, 382, 731, 382, 264, 2430, 763, 295, 264, 3894, 51256, 51256, 4583, 13, 51306, 51306, 407, 291, 586, 458, 577, 281, 14722, 264, 2430, 763, 295, 604, 4583, 2212, 264, 2430, 763, 295, 264, 51568, 51568, 3894, 4583, 13, 51646, 51646, 961, 311, 829, 341, 666, 364, 38253, 9284, 337, 257, 18161, 3209, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1073295519902156, "compression_ratio": 2.0238095238095237, "no_speech_prob": 1.1726301636372227e-06}, {"id": 76, "seek": 40276, "start": 428.4, "end": 432.44, "text": " Let's put this into an inference algorithm for a neural network.", "tokens": [50364, 5893, 1674, 365, 257, 4018, 11, 597, 307, 445, 341, 4846, 4111, 8062, 1783, 13, 50674, 50674, 407, 365, 341, 24657, 11, 291, 586, 458, 577, 281, 14722, 264, 24433, 4190, 295, 604, 4583, 50962, 50962, 294, 257, 18161, 3209, 382, 257, 2445, 295, 264, 9834, 382, 731, 382, 264, 2430, 763, 295, 264, 3894, 51256, 51256, 4583, 13, 51306, 51306, 407, 291, 586, 458, 577, 281, 14722, 264, 2430, 763, 295, 604, 4583, 2212, 264, 2430, 763, 295, 264, 51568, 51568, 3894, 4583, 13, 51646, 51646, 961, 311, 829, 341, 666, 364, 38253, 9284, 337, 257, 18161, 3209, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1073295519902156, "compression_ratio": 2.0238095238095237, "no_speech_prob": 1.1726301636372227e-06}, {"id": 77, "seek": 43244, "start": 432.44, "end": 436.16, "text": " In other words, how to get a neural network to make predictions.", "tokens": [50364, 682, 661, 2283, 11, 577, 281, 483, 257, 18161, 3209, 281, 652, 21264, 13, 50550, 50550, 961, 311, 352, 536, 300, 294, 264, 958, 960, 13, 50630], "temperature": 0.0, "avg_logprob": -0.13314578450959305, "compression_ratio": 1.1477272727272727, "no_speech_prob": 9.445416071685031e-05}, {"id": 78, "seek": 43616, "start": 436.16, "end": 463.16, "text": " Let's go see that in the next video.", "tokens": [50364, 961, 311, 352, 536, 300, 294, 264, 958, 960, 13, 51714], "temperature": 0.0, "avg_logprob": -0.30962148079505336, "compression_ratio": 0.8571428571428571, "no_speech_prob": 0.00021926728368271142}], "language": "en", "video_id": "4-2FOgsMOpk", "entity": "ML Specialization, Andrew Ng (2022)"}}