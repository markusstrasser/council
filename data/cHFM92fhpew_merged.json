{"video_id": "cHFM92fhpew", "title": "4.3 Neural Networks Intuition | Demand Prediction --[Machine Learning | Andrew Ng]", "description": "Second Course:\nAdvanced Learning Algorithms.\n\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 983, "views": 188, "publish_date": "11/04/2022", "timestamp": 1661817600, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " To illustrate how neural networks work, let's start with an example. We'll use an example from demand prediction in which you look at a product and try to predict, will this product be a top seller or not? Let's take a look. In this example, you're selling t-shirts and you would like to know if a particular t-shirt will be a top seller, you know, yes or no. And you have collected data of different t-shirts that were sold at different prices, as well as which ones became a top seller. This type of application is used by retailers today in order to plan better inventory levels as well as marketing campaigns. If you know what's likely to be a top seller, you would plan, for example, to just purchase more of that stock in advance. So in this example, the input feature X is the price of the t-shirt. And so that's the input to the learning algorithm. And if you apply logistic regression to fit a sigmoid function to the data that might look like that, then the output of your prediction might look like this, 1 over 1 plus e to the negative wx plus b. Previously we had written this as f of x as the output of the learning algorithm. In order to set this up to build a neural network, I'm going to switch the terminology a little bit and use the alphabet a to denounce the output of this logistic regression algorithm. The term a stands for activation, and it's actually a term from neuroscience, and it refers to how much a neuron is sending a high output to other neurons downstream from it. It turns out that this logistic regression unit, or this little logistic regression algorithm, can be thought of as a very simplified model of a single neuron in the brain, where what the neuron does is it takes as input the price x, and then it computes this formula on top, and it outputs the number a, which is computed via this formula, and it outputs the probability of this t-shirt being a top seller. Another way to think of a neuron is as a tiny little computer whose only job is to input one number, or a few numbers, such as a price, and then to output one number or maybe a few other numbers, which in this case is the probability of the t-shirt being a top seller. As I alluded in the previous video, a logistic regression algorithm is much simpler than what any biological neuron in your brain or mind does, which is why the artificial neural network is such a vastly oversimplified model of the human brain, even though in practice, as you know, deep learning algorithms do work very well. Given this description of a single neuron, building a neural network now just requires taking a bunch of these neurons and wiring them together or putting them together. Let's now look at a more complex example of demand prediction. In this example, we're going to have four features to predict whether or not a t-shirt is a top seller. The features are the price of the t-shirt, the shipping costs, the amount of marketing of that particular t-shirt, as well as the material quality. Is this a high quality thick cotton, or is this maybe a lower quality material? Now you might suspect that whether or not a t-shirt becomes a top seller actually depends on a few factors. First, what is the affordability of this t-shirt? Second is what's the degree of awareness of this t-shirt that potential buyers have? And third is perceived quality. Do buyers or potential buyers think this is a high quality t-shirt? And so what I'm going to do is create one artificial neuron to try to estimate the probability that this t-shirt is perceived as highly affordable. And affordability is mainly a function of price and shipping costs because the total amount you have to pay is the sum of the price plus the shipping costs. And so we're going to use a little neuron here, a logistic regression unit, to input price and shipping costs and predict, do people think this is affordable? Second, I'm going to create another artificial neuron here to estimate, is there high awareness of this? And awareness in this case is mainly a function of the marketing of the t-shirt. And finally, going to create another neuron to estimate, do people perceive this to be of high quality? And that may mainly be a function of the price of the t-shirt and of the material quality. Price is a factor here because fortunately or unfortunately, if there's a very high price t-shirt, people will sometimes perceive that to be of high quality because if it's very expensive then maybe people think it's got to be of high quality. Given these estimates of affordability, awareness, and perceived quality, we then wire the outputs of these three neurons to another neuron here on the right that then is another logistic regression unit that finally inputs those three numbers and outputs the probability of this t-shirt being a top seller. So in the terminology of neural networks, we're going to group these three neurons together into what's called a layer, and a layer is a grouping of neurons which take as input the same or similar features and that in turn outputs a few numbers together. So these three neurons on the left form one layer, which is why I drew them on top of each other. And the single neuron on the right is also one layer. The layer on the left has three neurons, so a layer can have multiple neurons, or it can also have a single neuron, as in the case of this layer on the right. This layer on the right is also called the output layer because the output of this final neuron is the output probability predicted by the neural network. In the terminology of neural networks, we're also going to call affordability, awareness, and perceived quality to be activations. The term activations comes from biological neurons, and it refers to the degree that a biological neuron is sending a high output value or sending many electrical impulses to other neurons, to the downstream from it. And so these numbers on affordability, awareness, and perceived quality are the activations of these three neurons in this layer. And also, this output probability is the activation of this neuron shown here on the right. So, this particular neural network, therefore, carries out computations as follows. It inputs four numbers, then this layer of the neural network uses those four numbers to compute three new numbers, also called activation values, and then the final layer, the output layer of the neural network, uses those three numbers to compute one number. And in a neural network, this list of four numbers is also called the input layer, and that's just a list of four numbers. Now, there's one simplification I'd like to make to this neural network, which is the way I've described it so far, we had to go through the neurons one at a time and decide what inputs it would take from the previous layer. So for example, we said affordability is a function of just price and shipping costs and awareness is a function of just marketing and so on. But if you're building a large neural network, it'd be a lot of work to go through and manually decide which neurons should take which features as inputs. The way a neural network is implemented in practice, each neuron in a certain layer, say this layer in the middle, will have access to every feature, to every value from the previous layer, from the input layer, which is why I'm now drawing arrows from every input feature to every one of these neurons shown here in the middle. And you can imagine that if you're trying to predict affordability and it knows what the price shipping costs, marketing and material, maybe you'll learn to ignore marketing and material and just figure out through setting the parameters appropriately to only focus on the subset of features that are most relevant to affordability. To further simplify the notation and the description of this neural network, I'm going to take these four input features and write them as a vector X and we're going to view the neural network as having four features that comprise this feature vector X and this feature vector is fed to this layer in the middle, which then computes three activation values, that is these three numbers, and these three activation values in turn becomes another vector, which is fed to this final output layer that finally outputs the probability of this t-shirt being a top seller. So that's all a neural network is. It has a few layers where each layer inputs a vector and outputs another vector of numbers, where for example, this layer in the middle inputs four numbers X and outputs three numbers corresponding to affordability, awareness and perceived quality. To add a little bit more terminology, you've seen that this layer is called the output layer and this layer is called the input layer. To give the layer in the middle a name as well, this layer in the middle is called a hidden layer. I know that this is maybe not the best or the most intuitive name, but that terminology comes from that when you have a training set, in the training set, you get to observe both X and Y, your data set tells you what is X and what is Y, and so you get data that tells you what are the correct inputs and the correct outputs, but your data set doesn't tell you what are the correct values for affordability, awareness and perceived quality. And so the correct values for those are hidden. You don't see them in the training set, which is why this layer in the middle is called a hidden layer. I'd like to share with you another way of thinking about neural networks that I found useful for building my intuition about it. Just let me cover up the left half of this diagram and see what we're left with. What you see here is that there is a logistic regression algorithm or logistic regression unit that is taking as input affordability, awareness and perceived quality of a t-shirt and using these three features to estimate the probability of the t-shirt being a top seller. So this is just logistic regression. But the cool thing about this is rather than using the original features, price, shipping, cost, marketing and so on, it's using a new, maybe better set of features, affordability, awareness and perceived quality that are hopefully more predictive of whether or not this t-shirt will be a top seller. So one way to think of this neural network is just logistic regression, but it is a version of logistic regression that can learn its own features that makes it easier to make accurate predictions. In fact, you might remember from the previous week, this housing example where we said that if you want to predict the price of the house, you might take the frontage or the width of lots and multiply that by the depth of a lot to construct a more complex feature, X1 times X2, which was the size of the lot. So there we were doing manual feature engineering where we had to look at the features X1 and X2 and decide by hand how to combine them together to come up with better features. What the neural network does is instead of you needing to manually engineer the features, it can learn, as you see later, its own features to make the learning problem easier for itself. So this is what makes neural networks one of the most powerful learning algorithms in the world today. So to summarize, a neural network does this. The input layer has a vector of features, four numbers in this example. It is input to the hidden layer, which outputs three numbers. And I'm going to use a vector to denote this vector of activations that this hidden layer outputs. And then the output layer takes this input, those three numbers and outputs one number, which would be the final activation or the final prediction of the neural network. One note, even though I previously described this neural network as computing affordability, awareness, and perceived quality, one of the really nice properties of a neural network is when you train it from data, you don't need to go and explicitly decide what are the features such as affordability and so on that a neural network should compute. Then it will figure out all by itself what are the features it wants to use in this hidden layer. And that's what makes it such a powerful learning algorithm. So you've seen here one example of a neural network, and this neural network has a single layer that is a hidden layer. Let's take a look at some other examples of neural networks, specifically examples with more than one hidden layer. Here's an example. This neural network has an input feature vector X that is fed to one hidden layer, and I'm going to call this the first hidden layer. And so if this hidden layer has three neurons, it will then output a vector of three activation values. These three numbers can then be input to a second hidden layer. And if the second hidden layer has two neurons, two logistic units, then this second hidden layer will output another vector of now two activation values that maybe goes to the output layer that then outputs the neural network's final prediction. Or here's another example. Here's a neural network that has this input go to the first hidden layer, that the output to the first hidden layer goes to the second hidden layer, goes to the third hidden layer, and then finally to the output layer. When you're building your own neural network, one of the decisions you need to make is how many hidden layers do you want and how many neurons do you want each hidden layer to have. And this question of how many hidden layers and how many neurons per hidden layer is a question of the architecture of the neural network. You learn later in this course some tips for choosing an appropriate architecture for a neural network. But choosing the right number of hidden layers and number of hidden units per layer can have an impact on the performance of your learning algorithm as well. So later in this course, you learn how to choose a good architecture for your neural network as well. Oh, and by the way, in some of the literature, you see this type of neural network with multiple layers like this called a multi-layer perceptron. So if you see that, that just refers to a neural network that looks like what you're seeing here on the slide. So that's a neural network. I know we went through a lot of this video. So thank you for sticking with me, but you now know how a neural network works. In the next video, let's take a look at how these ideas can be applied to other applications as well. In particular, we'll take a look at the computer vision application of face recognition. Let's go on to the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.0, "text": " To illustrate how neural networks work, let's start with an example.", "tokens": [50364, 1407, 23221, 577, 18161, 9590, 589, 11, 718, 311, 722, 365, 364, 1365, 13, 50614, 50614, 492, 603, 764, 364, 1365, 490, 4733, 17630, 294, 597, 291, 574, 412, 257, 1674, 293, 853, 281, 50864, 50864, 6069, 11, 486, 341, 1674, 312, 257, 1192, 23600, 420, 406, 30, 51014, 51014, 961, 311, 747, 257, 574, 13, 51064, 51064, 682, 341, 1365, 11, 291, 434, 6511, 256, 12, 25892, 293, 291, 576, 411, 281, 458, 498, 257, 1729, 51379, 51379, 256, 12, 15313, 486, 312, 257, 1192, 23600, 11, 291, 458, 11, 2086, 420, 572, 13, 51546, 51546, 400, 291, 362, 11087, 1412, 295, 819, 256, 12, 25892, 300, 645, 3718, 412, 819, 7901, 11, 382, 731, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.16585750579833985, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.01970895193517208}, {"id": 1, "seek": 0, "start": 5.0, "end": 10.0, "text": " We'll use an example from demand prediction in which you look at a product and try to", "tokens": [50364, 1407, 23221, 577, 18161, 9590, 589, 11, 718, 311, 722, 365, 364, 1365, 13, 50614, 50614, 492, 603, 764, 364, 1365, 490, 4733, 17630, 294, 597, 291, 574, 412, 257, 1674, 293, 853, 281, 50864, 50864, 6069, 11, 486, 341, 1674, 312, 257, 1192, 23600, 420, 406, 30, 51014, 51014, 961, 311, 747, 257, 574, 13, 51064, 51064, 682, 341, 1365, 11, 291, 434, 6511, 256, 12, 25892, 293, 291, 576, 411, 281, 458, 498, 257, 1729, 51379, 51379, 256, 12, 15313, 486, 312, 257, 1192, 23600, 11, 291, 458, 11, 2086, 420, 572, 13, 51546, 51546, 400, 291, 362, 11087, 1412, 295, 819, 256, 12, 25892, 300, 645, 3718, 412, 819, 7901, 11, 382, 731, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.16585750579833985, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.01970895193517208}, {"id": 2, "seek": 0, "start": 10.0, "end": 13.0, "text": " predict, will this product be a top seller or not?", "tokens": [50364, 1407, 23221, 577, 18161, 9590, 589, 11, 718, 311, 722, 365, 364, 1365, 13, 50614, 50614, 492, 603, 764, 364, 1365, 490, 4733, 17630, 294, 597, 291, 574, 412, 257, 1674, 293, 853, 281, 50864, 50864, 6069, 11, 486, 341, 1674, 312, 257, 1192, 23600, 420, 406, 30, 51014, 51014, 961, 311, 747, 257, 574, 13, 51064, 51064, 682, 341, 1365, 11, 291, 434, 6511, 256, 12, 25892, 293, 291, 576, 411, 281, 458, 498, 257, 1729, 51379, 51379, 256, 12, 15313, 486, 312, 257, 1192, 23600, 11, 291, 458, 11, 2086, 420, 572, 13, 51546, 51546, 400, 291, 362, 11087, 1412, 295, 819, 256, 12, 25892, 300, 645, 3718, 412, 819, 7901, 11, 382, 731, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.16585750579833985, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.01970895193517208}, {"id": 3, "seek": 0, "start": 13.0, "end": 14.0, "text": " Let's take a look.", "tokens": [50364, 1407, 23221, 577, 18161, 9590, 589, 11, 718, 311, 722, 365, 364, 1365, 13, 50614, 50614, 492, 603, 764, 364, 1365, 490, 4733, 17630, 294, 597, 291, 574, 412, 257, 1674, 293, 853, 281, 50864, 50864, 6069, 11, 486, 341, 1674, 312, 257, 1192, 23600, 420, 406, 30, 51014, 51014, 961, 311, 747, 257, 574, 13, 51064, 51064, 682, 341, 1365, 11, 291, 434, 6511, 256, 12, 25892, 293, 291, 576, 411, 281, 458, 498, 257, 1729, 51379, 51379, 256, 12, 15313, 486, 312, 257, 1192, 23600, 11, 291, 458, 11, 2086, 420, 572, 13, 51546, 51546, 400, 291, 362, 11087, 1412, 295, 819, 256, 12, 25892, 300, 645, 3718, 412, 819, 7901, 11, 382, 731, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.16585750579833985, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.01970895193517208}, {"id": 4, "seek": 0, "start": 14.0, "end": 20.3, "text": " In this example, you're selling t-shirts and you would like to know if a particular", "tokens": [50364, 1407, 23221, 577, 18161, 9590, 589, 11, 718, 311, 722, 365, 364, 1365, 13, 50614, 50614, 492, 603, 764, 364, 1365, 490, 4733, 17630, 294, 597, 291, 574, 412, 257, 1674, 293, 853, 281, 50864, 50864, 6069, 11, 486, 341, 1674, 312, 257, 1192, 23600, 420, 406, 30, 51014, 51014, 961, 311, 747, 257, 574, 13, 51064, 51064, 682, 341, 1365, 11, 291, 434, 6511, 256, 12, 25892, 293, 291, 576, 411, 281, 458, 498, 257, 1729, 51379, 51379, 256, 12, 15313, 486, 312, 257, 1192, 23600, 11, 291, 458, 11, 2086, 420, 572, 13, 51546, 51546, 400, 291, 362, 11087, 1412, 295, 819, 256, 12, 25892, 300, 645, 3718, 412, 819, 7901, 11, 382, 731, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.16585750579833985, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.01970895193517208}, {"id": 5, "seek": 0, "start": 20.3, "end": 23.64, "text": " t-shirt will be a top seller, you know, yes or no.", "tokens": [50364, 1407, 23221, 577, 18161, 9590, 589, 11, 718, 311, 722, 365, 364, 1365, 13, 50614, 50614, 492, 603, 764, 364, 1365, 490, 4733, 17630, 294, 597, 291, 574, 412, 257, 1674, 293, 853, 281, 50864, 50864, 6069, 11, 486, 341, 1674, 312, 257, 1192, 23600, 420, 406, 30, 51014, 51014, 961, 311, 747, 257, 574, 13, 51064, 51064, 682, 341, 1365, 11, 291, 434, 6511, 256, 12, 25892, 293, 291, 576, 411, 281, 458, 498, 257, 1729, 51379, 51379, 256, 12, 15313, 486, 312, 257, 1192, 23600, 11, 291, 458, 11, 2086, 420, 572, 13, 51546, 51546, 400, 291, 362, 11087, 1412, 295, 819, 256, 12, 25892, 300, 645, 3718, 412, 819, 7901, 11, 382, 731, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.16585750579833985, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.01970895193517208}, {"id": 6, "seek": 0, "start": 23.64, "end": 28.64, "text": " And you have collected data of different t-shirts that were sold at different prices, as well", "tokens": [50364, 1407, 23221, 577, 18161, 9590, 589, 11, 718, 311, 722, 365, 364, 1365, 13, 50614, 50614, 492, 603, 764, 364, 1365, 490, 4733, 17630, 294, 597, 291, 574, 412, 257, 1674, 293, 853, 281, 50864, 50864, 6069, 11, 486, 341, 1674, 312, 257, 1192, 23600, 420, 406, 30, 51014, 51014, 961, 311, 747, 257, 574, 13, 51064, 51064, 682, 341, 1365, 11, 291, 434, 6511, 256, 12, 25892, 293, 291, 576, 411, 281, 458, 498, 257, 1729, 51379, 51379, 256, 12, 15313, 486, 312, 257, 1192, 23600, 11, 291, 458, 11, 2086, 420, 572, 13, 51546, 51546, 400, 291, 362, 11087, 1412, 295, 819, 256, 12, 25892, 300, 645, 3718, 412, 819, 7901, 11, 382, 731, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.16585750579833985, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.01970895193517208}, {"id": 7, "seek": 2864, "start": 28.64, "end": 31.48, "text": " as which ones became a top seller.", "tokens": [50364, 382, 597, 2306, 3062, 257, 1192, 23600, 13, 50506, 50506, 639, 2010, 295, 3861, 307, 1143, 538, 33519, 965, 294, 1668, 281, 1393, 1101, 14228, 4358, 50856, 50856, 382, 731, 382, 6370, 16840, 13, 50946, 50946, 759, 291, 458, 437, 311, 3700, 281, 312, 257, 1192, 23600, 11, 291, 576, 1393, 11, 337, 1365, 11, 281, 445, 8110, 51177, 51177, 544, 295, 300, 4127, 294, 7295, 13, 51298, 51298, 407, 294, 341, 1365, 11, 264, 4846, 4111, 1783, 307, 264, 3218, 295, 264, 256, 12, 15313, 13, 51616, 51616, 400, 370, 300, 311, 264, 4846, 281, 264, 2539, 9284, 13, 51773, 51773], "temperature": 0.0, "avg_logprob": -0.10701814981607291, "compression_ratio": 1.6144578313253013, "no_speech_prob": 4.936752247886034e-06}, {"id": 8, "seek": 2864, "start": 31.48, "end": 38.480000000000004, "text": " This type of application is used by retailers today in order to plan better inventory levels", "tokens": [50364, 382, 597, 2306, 3062, 257, 1192, 23600, 13, 50506, 50506, 639, 2010, 295, 3861, 307, 1143, 538, 33519, 965, 294, 1668, 281, 1393, 1101, 14228, 4358, 50856, 50856, 382, 731, 382, 6370, 16840, 13, 50946, 50946, 759, 291, 458, 437, 311, 3700, 281, 312, 257, 1192, 23600, 11, 291, 576, 1393, 11, 337, 1365, 11, 281, 445, 8110, 51177, 51177, 544, 295, 300, 4127, 294, 7295, 13, 51298, 51298, 407, 294, 341, 1365, 11, 264, 4846, 4111, 1783, 307, 264, 3218, 295, 264, 256, 12, 15313, 13, 51616, 51616, 400, 370, 300, 311, 264, 4846, 281, 264, 2539, 9284, 13, 51773, 51773], "temperature": 0.0, "avg_logprob": -0.10701814981607291, "compression_ratio": 1.6144578313253013, "no_speech_prob": 4.936752247886034e-06}, {"id": 9, "seek": 2864, "start": 38.480000000000004, "end": 40.28, "text": " as well as marketing campaigns.", "tokens": [50364, 382, 597, 2306, 3062, 257, 1192, 23600, 13, 50506, 50506, 639, 2010, 295, 3861, 307, 1143, 538, 33519, 965, 294, 1668, 281, 1393, 1101, 14228, 4358, 50856, 50856, 382, 731, 382, 6370, 16840, 13, 50946, 50946, 759, 291, 458, 437, 311, 3700, 281, 312, 257, 1192, 23600, 11, 291, 576, 1393, 11, 337, 1365, 11, 281, 445, 8110, 51177, 51177, 544, 295, 300, 4127, 294, 7295, 13, 51298, 51298, 407, 294, 341, 1365, 11, 264, 4846, 4111, 1783, 307, 264, 3218, 295, 264, 256, 12, 15313, 13, 51616, 51616, 400, 370, 300, 311, 264, 4846, 281, 264, 2539, 9284, 13, 51773, 51773], "temperature": 0.0, "avg_logprob": -0.10701814981607291, "compression_ratio": 1.6144578313253013, "no_speech_prob": 4.936752247886034e-06}, {"id": 10, "seek": 2864, "start": 40.28, "end": 44.900000000000006, "text": " If you know what's likely to be a top seller, you would plan, for example, to just purchase", "tokens": [50364, 382, 597, 2306, 3062, 257, 1192, 23600, 13, 50506, 50506, 639, 2010, 295, 3861, 307, 1143, 538, 33519, 965, 294, 1668, 281, 1393, 1101, 14228, 4358, 50856, 50856, 382, 731, 382, 6370, 16840, 13, 50946, 50946, 759, 291, 458, 437, 311, 3700, 281, 312, 257, 1192, 23600, 11, 291, 576, 1393, 11, 337, 1365, 11, 281, 445, 8110, 51177, 51177, 544, 295, 300, 4127, 294, 7295, 13, 51298, 51298, 407, 294, 341, 1365, 11, 264, 4846, 4111, 1783, 307, 264, 3218, 295, 264, 256, 12, 15313, 13, 51616, 51616, 400, 370, 300, 311, 264, 4846, 281, 264, 2539, 9284, 13, 51773, 51773], "temperature": 0.0, "avg_logprob": -0.10701814981607291, "compression_ratio": 1.6144578313253013, "no_speech_prob": 4.936752247886034e-06}, {"id": 11, "seek": 2864, "start": 44.900000000000006, "end": 47.32, "text": " more of that stock in advance.", "tokens": [50364, 382, 597, 2306, 3062, 257, 1192, 23600, 13, 50506, 50506, 639, 2010, 295, 3861, 307, 1143, 538, 33519, 965, 294, 1668, 281, 1393, 1101, 14228, 4358, 50856, 50856, 382, 731, 382, 6370, 16840, 13, 50946, 50946, 759, 291, 458, 437, 311, 3700, 281, 312, 257, 1192, 23600, 11, 291, 576, 1393, 11, 337, 1365, 11, 281, 445, 8110, 51177, 51177, 544, 295, 300, 4127, 294, 7295, 13, 51298, 51298, 407, 294, 341, 1365, 11, 264, 4846, 4111, 1783, 307, 264, 3218, 295, 264, 256, 12, 15313, 13, 51616, 51616, 400, 370, 300, 311, 264, 4846, 281, 264, 2539, 9284, 13, 51773, 51773], "temperature": 0.0, "avg_logprob": -0.10701814981607291, "compression_ratio": 1.6144578313253013, "no_speech_prob": 4.936752247886034e-06}, {"id": 12, "seek": 2864, "start": 47.32, "end": 53.68, "text": " So in this example, the input feature X is the price of the t-shirt.", "tokens": [50364, 382, 597, 2306, 3062, 257, 1192, 23600, 13, 50506, 50506, 639, 2010, 295, 3861, 307, 1143, 538, 33519, 965, 294, 1668, 281, 1393, 1101, 14228, 4358, 50856, 50856, 382, 731, 382, 6370, 16840, 13, 50946, 50946, 759, 291, 458, 437, 311, 3700, 281, 312, 257, 1192, 23600, 11, 291, 576, 1393, 11, 337, 1365, 11, 281, 445, 8110, 51177, 51177, 544, 295, 300, 4127, 294, 7295, 13, 51298, 51298, 407, 294, 341, 1365, 11, 264, 4846, 4111, 1783, 307, 264, 3218, 295, 264, 256, 12, 15313, 13, 51616, 51616, 400, 370, 300, 311, 264, 4846, 281, 264, 2539, 9284, 13, 51773, 51773], "temperature": 0.0, "avg_logprob": -0.10701814981607291, "compression_ratio": 1.6144578313253013, "no_speech_prob": 4.936752247886034e-06}, {"id": 13, "seek": 2864, "start": 53.68, "end": 56.82, "text": " And so that's the input to the learning algorithm.", "tokens": [50364, 382, 597, 2306, 3062, 257, 1192, 23600, 13, 50506, 50506, 639, 2010, 295, 3861, 307, 1143, 538, 33519, 965, 294, 1668, 281, 1393, 1101, 14228, 4358, 50856, 50856, 382, 731, 382, 6370, 16840, 13, 50946, 50946, 759, 291, 458, 437, 311, 3700, 281, 312, 257, 1192, 23600, 11, 291, 576, 1393, 11, 337, 1365, 11, 281, 445, 8110, 51177, 51177, 544, 295, 300, 4127, 294, 7295, 13, 51298, 51298, 407, 294, 341, 1365, 11, 264, 4846, 4111, 1783, 307, 264, 3218, 295, 264, 256, 12, 15313, 13, 51616, 51616, 400, 370, 300, 311, 264, 4846, 281, 264, 2539, 9284, 13, 51773, 51773], "temperature": 0.0, "avg_logprob": -0.10701814981607291, "compression_ratio": 1.6144578313253013, "no_speech_prob": 4.936752247886034e-06}, {"id": 14, "seek": 5682, "start": 56.82, "end": 62.82, "text": " And if you apply logistic regression to fit a sigmoid function to the data that might", "tokens": [50364, 400, 498, 291, 3079, 3565, 3142, 24590, 281, 3318, 257, 4556, 3280, 327, 2445, 281, 264, 1412, 300, 1062, 50664, 50664, 574, 411, 300, 11, 550, 264, 5598, 295, 428, 17630, 1062, 574, 411, 341, 11, 502, 670, 502, 1804, 308, 281, 264, 50971, 50971, 3671, 261, 87, 1804, 272, 13, 51115, 51115, 33606, 321, 632, 3720, 341, 382, 283, 295, 2031, 382, 264, 5598, 295, 264, 2539, 9284, 13, 51415, 51415, 682, 1668, 281, 992, 341, 493, 281, 1322, 257, 18161, 3209, 11, 286, 478, 516, 281, 3679, 264, 27575, 51621, 51621], "temperature": 0.0, "avg_logprob": -0.13776351527163858, "compression_ratio": 1.6444444444444444, "no_speech_prob": 5.593523837887915e-06}, {"id": 15, "seek": 5682, "start": 62.82, "end": 68.96000000000001, "text": " look like that, then the output of your prediction might look like this, 1 over 1 plus e to the", "tokens": [50364, 400, 498, 291, 3079, 3565, 3142, 24590, 281, 3318, 257, 4556, 3280, 327, 2445, 281, 264, 1412, 300, 1062, 50664, 50664, 574, 411, 300, 11, 550, 264, 5598, 295, 428, 17630, 1062, 574, 411, 341, 11, 502, 670, 502, 1804, 308, 281, 264, 50971, 50971, 3671, 261, 87, 1804, 272, 13, 51115, 51115, 33606, 321, 632, 3720, 341, 382, 283, 295, 2031, 382, 264, 5598, 295, 264, 2539, 9284, 13, 51415, 51415, 682, 1668, 281, 992, 341, 493, 281, 1322, 257, 18161, 3209, 11, 286, 478, 516, 281, 3679, 264, 27575, 51621, 51621], "temperature": 0.0, "avg_logprob": -0.13776351527163858, "compression_ratio": 1.6444444444444444, "no_speech_prob": 5.593523837887915e-06}, {"id": 16, "seek": 5682, "start": 68.96000000000001, "end": 71.84, "text": " negative wx plus b.", "tokens": [50364, 400, 498, 291, 3079, 3565, 3142, 24590, 281, 3318, 257, 4556, 3280, 327, 2445, 281, 264, 1412, 300, 1062, 50664, 50664, 574, 411, 300, 11, 550, 264, 5598, 295, 428, 17630, 1062, 574, 411, 341, 11, 502, 670, 502, 1804, 308, 281, 264, 50971, 50971, 3671, 261, 87, 1804, 272, 13, 51115, 51115, 33606, 321, 632, 3720, 341, 382, 283, 295, 2031, 382, 264, 5598, 295, 264, 2539, 9284, 13, 51415, 51415, 682, 1668, 281, 992, 341, 493, 281, 1322, 257, 18161, 3209, 11, 286, 478, 516, 281, 3679, 264, 27575, 51621, 51621], "temperature": 0.0, "avg_logprob": -0.13776351527163858, "compression_ratio": 1.6444444444444444, "no_speech_prob": 5.593523837887915e-06}, {"id": 17, "seek": 5682, "start": 71.84, "end": 77.84, "text": " Previously we had written this as f of x as the output of the learning algorithm.", "tokens": [50364, 400, 498, 291, 3079, 3565, 3142, 24590, 281, 3318, 257, 4556, 3280, 327, 2445, 281, 264, 1412, 300, 1062, 50664, 50664, 574, 411, 300, 11, 550, 264, 5598, 295, 428, 17630, 1062, 574, 411, 341, 11, 502, 670, 502, 1804, 308, 281, 264, 50971, 50971, 3671, 261, 87, 1804, 272, 13, 51115, 51115, 33606, 321, 632, 3720, 341, 382, 283, 295, 2031, 382, 264, 5598, 295, 264, 2539, 9284, 13, 51415, 51415, 682, 1668, 281, 992, 341, 493, 281, 1322, 257, 18161, 3209, 11, 286, 478, 516, 281, 3679, 264, 27575, 51621, 51621], "temperature": 0.0, "avg_logprob": -0.13776351527163858, "compression_ratio": 1.6444444444444444, "no_speech_prob": 5.593523837887915e-06}, {"id": 18, "seek": 5682, "start": 77.84, "end": 81.96000000000001, "text": " In order to set this up to build a neural network, I'm going to switch the terminology", "tokens": [50364, 400, 498, 291, 3079, 3565, 3142, 24590, 281, 3318, 257, 4556, 3280, 327, 2445, 281, 264, 1412, 300, 1062, 50664, 50664, 574, 411, 300, 11, 550, 264, 5598, 295, 428, 17630, 1062, 574, 411, 341, 11, 502, 670, 502, 1804, 308, 281, 264, 50971, 50971, 3671, 261, 87, 1804, 272, 13, 51115, 51115, 33606, 321, 632, 3720, 341, 382, 283, 295, 2031, 382, 264, 5598, 295, 264, 2539, 9284, 13, 51415, 51415, 682, 1668, 281, 992, 341, 493, 281, 1322, 257, 18161, 3209, 11, 286, 478, 516, 281, 3679, 264, 27575, 51621, 51621], "temperature": 0.0, "avg_logprob": -0.13776351527163858, "compression_ratio": 1.6444444444444444, "no_speech_prob": 5.593523837887915e-06}, {"id": 19, "seek": 8196, "start": 81.96, "end": 89.16, "text": " a little bit and use the alphabet a to denounce the output of this logistic regression algorithm.", "tokens": [50364, 257, 707, 857, 293, 764, 264, 23339, 257, 281, 1441, 7826, 264, 5598, 295, 341, 3565, 3142, 24590, 9284, 13, 50724, 50724, 440, 1433, 257, 7382, 337, 24433, 11, 293, 309, 311, 767, 257, 1433, 490, 42762, 11, 293, 309, 50988, 50988, 14942, 281, 577, 709, 257, 34090, 307, 7750, 257, 1090, 5598, 281, 661, 22027, 30621, 490, 309, 13, 51408, 51408, 467, 4523, 484, 300, 341, 3565, 3142, 24590, 4985, 11, 420, 341, 707, 3565, 3142, 24590, 9284, 11, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.1283298447018578, "compression_ratio": 1.816831683168317, "no_speech_prob": 8.139397323247977e-06}, {"id": 20, "seek": 8196, "start": 89.16, "end": 94.44, "text": " The term a stands for activation, and it's actually a term from neuroscience, and it", "tokens": [50364, 257, 707, 857, 293, 764, 264, 23339, 257, 281, 1441, 7826, 264, 5598, 295, 341, 3565, 3142, 24590, 9284, 13, 50724, 50724, 440, 1433, 257, 7382, 337, 24433, 11, 293, 309, 311, 767, 257, 1433, 490, 42762, 11, 293, 309, 50988, 50988, 14942, 281, 577, 709, 257, 34090, 307, 7750, 257, 1090, 5598, 281, 661, 22027, 30621, 490, 309, 13, 51408, 51408, 467, 4523, 484, 300, 341, 3565, 3142, 24590, 4985, 11, 420, 341, 707, 3565, 3142, 24590, 9284, 11, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.1283298447018578, "compression_ratio": 1.816831683168317, "no_speech_prob": 8.139397323247977e-06}, {"id": 21, "seek": 8196, "start": 94.44, "end": 102.83999999999999, "text": " refers to how much a neuron is sending a high output to other neurons downstream from it.", "tokens": [50364, 257, 707, 857, 293, 764, 264, 23339, 257, 281, 1441, 7826, 264, 5598, 295, 341, 3565, 3142, 24590, 9284, 13, 50724, 50724, 440, 1433, 257, 7382, 337, 24433, 11, 293, 309, 311, 767, 257, 1433, 490, 42762, 11, 293, 309, 50988, 50988, 14942, 281, 577, 709, 257, 34090, 307, 7750, 257, 1090, 5598, 281, 661, 22027, 30621, 490, 309, 13, 51408, 51408, 467, 4523, 484, 300, 341, 3565, 3142, 24590, 4985, 11, 420, 341, 707, 3565, 3142, 24590, 9284, 11, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.1283298447018578, "compression_ratio": 1.816831683168317, "no_speech_prob": 8.139397323247977e-06}, {"id": 22, "seek": 8196, "start": 102.83999999999999, "end": 109.0, "text": " It turns out that this logistic regression unit, or this little logistic regression algorithm,", "tokens": [50364, 257, 707, 857, 293, 764, 264, 23339, 257, 281, 1441, 7826, 264, 5598, 295, 341, 3565, 3142, 24590, 9284, 13, 50724, 50724, 440, 1433, 257, 7382, 337, 24433, 11, 293, 309, 311, 767, 257, 1433, 490, 42762, 11, 293, 309, 50988, 50988, 14942, 281, 577, 709, 257, 34090, 307, 7750, 257, 1090, 5598, 281, 661, 22027, 30621, 490, 309, 13, 51408, 51408, 467, 4523, 484, 300, 341, 3565, 3142, 24590, 4985, 11, 420, 341, 707, 3565, 3142, 24590, 9284, 11, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.1283298447018578, "compression_ratio": 1.816831683168317, "no_speech_prob": 8.139397323247977e-06}, {"id": 23, "seek": 10900, "start": 109.0, "end": 115.48, "text": " can be thought of as a very simplified model of a single neuron in the brain, where what", "tokens": [50364, 393, 312, 1194, 295, 382, 257, 588, 26335, 2316, 295, 257, 2167, 34090, 294, 264, 3567, 11, 689, 437, 50688, 50688, 264, 34090, 775, 307, 309, 2516, 382, 4846, 264, 3218, 2031, 11, 293, 550, 309, 715, 1819, 341, 8513, 322, 1192, 11, 51118, 51118, 293, 309, 23930, 264, 1230, 257, 11, 597, 307, 40610, 5766, 341, 8513, 11, 293, 309, 23930, 264, 8482, 51438, 51438, 295, 341, 256, 12, 15313, 885, 257, 1192, 23600, 13, 51586, 51586], "temperature": 0.0, "avg_logprob": -0.11112755537033081, "compression_ratio": 1.6956521739130435, "no_speech_prob": 1.8924025425803848e-05}, {"id": 24, "seek": 10900, "start": 115.48, "end": 124.08, "text": " the neuron does is it takes as input the price x, and then it computes this formula on top,", "tokens": [50364, 393, 312, 1194, 295, 382, 257, 588, 26335, 2316, 295, 257, 2167, 34090, 294, 264, 3567, 11, 689, 437, 50688, 50688, 264, 34090, 775, 307, 309, 2516, 382, 4846, 264, 3218, 2031, 11, 293, 550, 309, 715, 1819, 341, 8513, 322, 1192, 11, 51118, 51118, 293, 309, 23930, 264, 1230, 257, 11, 597, 307, 40610, 5766, 341, 8513, 11, 293, 309, 23930, 264, 8482, 51438, 51438, 295, 341, 256, 12, 15313, 885, 257, 1192, 23600, 13, 51586, 51586], "temperature": 0.0, "avg_logprob": -0.11112755537033081, "compression_ratio": 1.6956521739130435, "no_speech_prob": 1.8924025425803848e-05}, {"id": 25, "seek": 10900, "start": 124.08, "end": 130.48, "text": " and it outputs the number a, which is computed via this formula, and it outputs the probability", "tokens": [50364, 393, 312, 1194, 295, 382, 257, 588, 26335, 2316, 295, 257, 2167, 34090, 294, 264, 3567, 11, 689, 437, 50688, 50688, 264, 34090, 775, 307, 309, 2516, 382, 4846, 264, 3218, 2031, 11, 293, 550, 309, 715, 1819, 341, 8513, 322, 1192, 11, 51118, 51118, 293, 309, 23930, 264, 1230, 257, 11, 597, 307, 40610, 5766, 341, 8513, 11, 293, 309, 23930, 264, 8482, 51438, 51438, 295, 341, 256, 12, 15313, 885, 257, 1192, 23600, 13, 51586, 51586], "temperature": 0.0, "avg_logprob": -0.11112755537033081, "compression_ratio": 1.6956521739130435, "no_speech_prob": 1.8924025425803848e-05}, {"id": 26, "seek": 10900, "start": 130.48, "end": 133.44, "text": " of this t-shirt being a top seller.", "tokens": [50364, 393, 312, 1194, 295, 382, 257, 588, 26335, 2316, 295, 257, 2167, 34090, 294, 264, 3567, 11, 689, 437, 50688, 50688, 264, 34090, 775, 307, 309, 2516, 382, 4846, 264, 3218, 2031, 11, 293, 550, 309, 715, 1819, 341, 8513, 322, 1192, 11, 51118, 51118, 293, 309, 23930, 264, 1230, 257, 11, 597, 307, 40610, 5766, 341, 8513, 11, 293, 309, 23930, 264, 8482, 51438, 51438, 295, 341, 256, 12, 15313, 885, 257, 1192, 23600, 13, 51586, 51586], "temperature": 0.0, "avg_logprob": -0.11112755537033081, "compression_ratio": 1.6956521739130435, "no_speech_prob": 1.8924025425803848e-05}, {"id": 27, "seek": 13344, "start": 133.44, "end": 141.0, "text": " Another way to think of a neuron is as a tiny little computer whose only job is to input", "tokens": [50364, 3996, 636, 281, 519, 295, 257, 34090, 307, 382, 257, 5870, 707, 3820, 6104, 787, 1691, 307, 281, 4846, 50742, 50742, 472, 1230, 11, 420, 257, 1326, 3547, 11, 1270, 382, 257, 3218, 11, 293, 550, 281, 5598, 472, 1230, 420, 1310, 257, 1326, 51066, 51066, 661, 3547, 11, 597, 294, 341, 1389, 307, 264, 8482, 295, 264, 256, 12, 15313, 885, 257, 1192, 23600, 13, 51412, 51412, 1018, 286, 33919, 294, 264, 3894, 960, 11, 257, 3565, 3142, 24590, 9284, 307, 709, 18587, 813, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.09562548090902608, "compression_ratio": 1.6376146788990826, "no_speech_prob": 1.777792385837529e-05}, {"id": 28, "seek": 13344, "start": 141.0, "end": 147.48, "text": " one number, or a few numbers, such as a price, and then to output one number or maybe a few", "tokens": [50364, 3996, 636, 281, 519, 295, 257, 34090, 307, 382, 257, 5870, 707, 3820, 6104, 787, 1691, 307, 281, 4846, 50742, 50742, 472, 1230, 11, 420, 257, 1326, 3547, 11, 1270, 382, 257, 3218, 11, 293, 550, 281, 5598, 472, 1230, 420, 1310, 257, 1326, 51066, 51066, 661, 3547, 11, 597, 294, 341, 1389, 307, 264, 8482, 295, 264, 256, 12, 15313, 885, 257, 1192, 23600, 13, 51412, 51412, 1018, 286, 33919, 294, 264, 3894, 960, 11, 257, 3565, 3142, 24590, 9284, 307, 709, 18587, 813, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.09562548090902608, "compression_ratio": 1.6376146788990826, "no_speech_prob": 1.777792385837529e-05}, {"id": 29, "seek": 13344, "start": 147.48, "end": 154.4, "text": " other numbers, which in this case is the probability of the t-shirt being a top seller.", "tokens": [50364, 3996, 636, 281, 519, 295, 257, 34090, 307, 382, 257, 5870, 707, 3820, 6104, 787, 1691, 307, 281, 4846, 50742, 50742, 472, 1230, 11, 420, 257, 1326, 3547, 11, 1270, 382, 257, 3218, 11, 293, 550, 281, 5598, 472, 1230, 420, 1310, 257, 1326, 51066, 51066, 661, 3547, 11, 597, 294, 341, 1389, 307, 264, 8482, 295, 264, 256, 12, 15313, 885, 257, 1192, 23600, 13, 51412, 51412, 1018, 286, 33919, 294, 264, 3894, 960, 11, 257, 3565, 3142, 24590, 9284, 307, 709, 18587, 813, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.09562548090902608, "compression_ratio": 1.6376146788990826, "no_speech_prob": 1.777792385837529e-05}, {"id": 30, "seek": 13344, "start": 154.4, "end": 160.84, "text": " As I alluded in the previous video, a logistic regression algorithm is much simpler than", "tokens": [50364, 3996, 636, 281, 519, 295, 257, 34090, 307, 382, 257, 5870, 707, 3820, 6104, 787, 1691, 307, 281, 4846, 50742, 50742, 472, 1230, 11, 420, 257, 1326, 3547, 11, 1270, 382, 257, 3218, 11, 293, 550, 281, 5598, 472, 1230, 420, 1310, 257, 1326, 51066, 51066, 661, 3547, 11, 597, 294, 341, 1389, 307, 264, 8482, 295, 264, 256, 12, 15313, 885, 257, 1192, 23600, 13, 51412, 51412, 1018, 286, 33919, 294, 264, 3894, 960, 11, 257, 3565, 3142, 24590, 9284, 307, 709, 18587, 813, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.09562548090902608, "compression_ratio": 1.6376146788990826, "no_speech_prob": 1.777792385837529e-05}, {"id": 31, "seek": 16084, "start": 160.84, "end": 166.16, "text": " what any biological neuron in your brain or mind does, which is why the artificial neural", "tokens": [50364, 437, 604, 13910, 34090, 294, 428, 3567, 420, 1575, 775, 11, 597, 307, 983, 264, 11677, 18161, 50630, 50630, 3209, 307, 1270, 257, 41426, 15488, 332, 564, 2587, 2316, 295, 264, 1952, 3567, 11, 754, 1673, 294, 3124, 11, 50934, 50934, 382, 291, 458, 11, 2452, 2539, 14642, 360, 589, 588, 731, 13, 51128, 51128, 18600, 341, 3855, 295, 257, 2167, 34090, 11, 2390, 257, 18161, 3209, 586, 445, 7029, 51404, 51404, 1940, 257, 3840, 295, 613, 22027, 293, 27520, 552, 1214, 420, 3372, 552, 1214, 13, 51670, 51670, 961, 311, 586, 574, 412, 257, 544, 3997, 1365, 295, 4733, 17630, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.08999323395063293, "compression_ratio": 1.7153284671532847, "no_speech_prob": 5.1736919886025134e-06}, {"id": 32, "seek": 16084, "start": 166.16, "end": 172.24, "text": " network is such a vastly oversimplified model of the human brain, even though in practice,", "tokens": [50364, 437, 604, 13910, 34090, 294, 428, 3567, 420, 1575, 775, 11, 597, 307, 983, 264, 11677, 18161, 50630, 50630, 3209, 307, 1270, 257, 41426, 15488, 332, 564, 2587, 2316, 295, 264, 1952, 3567, 11, 754, 1673, 294, 3124, 11, 50934, 50934, 382, 291, 458, 11, 2452, 2539, 14642, 360, 589, 588, 731, 13, 51128, 51128, 18600, 341, 3855, 295, 257, 2167, 34090, 11, 2390, 257, 18161, 3209, 586, 445, 7029, 51404, 51404, 1940, 257, 3840, 295, 613, 22027, 293, 27520, 552, 1214, 420, 3372, 552, 1214, 13, 51670, 51670, 961, 311, 586, 574, 412, 257, 544, 3997, 1365, 295, 4733, 17630, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.08999323395063293, "compression_ratio": 1.7153284671532847, "no_speech_prob": 5.1736919886025134e-06}, {"id": 33, "seek": 16084, "start": 172.24, "end": 176.12, "text": " as you know, deep learning algorithms do work very well.", "tokens": [50364, 437, 604, 13910, 34090, 294, 428, 3567, 420, 1575, 775, 11, 597, 307, 983, 264, 11677, 18161, 50630, 50630, 3209, 307, 1270, 257, 41426, 15488, 332, 564, 2587, 2316, 295, 264, 1952, 3567, 11, 754, 1673, 294, 3124, 11, 50934, 50934, 382, 291, 458, 11, 2452, 2539, 14642, 360, 589, 588, 731, 13, 51128, 51128, 18600, 341, 3855, 295, 257, 2167, 34090, 11, 2390, 257, 18161, 3209, 586, 445, 7029, 51404, 51404, 1940, 257, 3840, 295, 613, 22027, 293, 27520, 552, 1214, 420, 3372, 552, 1214, 13, 51670, 51670, 961, 311, 586, 574, 412, 257, 544, 3997, 1365, 295, 4733, 17630, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.08999323395063293, "compression_ratio": 1.7153284671532847, "no_speech_prob": 5.1736919886025134e-06}, {"id": 34, "seek": 16084, "start": 176.12, "end": 181.64000000000001, "text": " Given this description of a single neuron, building a neural network now just requires", "tokens": [50364, 437, 604, 13910, 34090, 294, 428, 3567, 420, 1575, 775, 11, 597, 307, 983, 264, 11677, 18161, 50630, 50630, 3209, 307, 1270, 257, 41426, 15488, 332, 564, 2587, 2316, 295, 264, 1952, 3567, 11, 754, 1673, 294, 3124, 11, 50934, 50934, 382, 291, 458, 11, 2452, 2539, 14642, 360, 589, 588, 731, 13, 51128, 51128, 18600, 341, 3855, 295, 257, 2167, 34090, 11, 2390, 257, 18161, 3209, 586, 445, 7029, 51404, 51404, 1940, 257, 3840, 295, 613, 22027, 293, 27520, 552, 1214, 420, 3372, 552, 1214, 13, 51670, 51670, 961, 311, 586, 574, 412, 257, 544, 3997, 1365, 295, 4733, 17630, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.08999323395063293, "compression_ratio": 1.7153284671532847, "no_speech_prob": 5.1736919886025134e-06}, {"id": 35, "seek": 16084, "start": 181.64000000000001, "end": 186.96, "text": " taking a bunch of these neurons and wiring them together or putting them together.", "tokens": [50364, 437, 604, 13910, 34090, 294, 428, 3567, 420, 1575, 775, 11, 597, 307, 983, 264, 11677, 18161, 50630, 50630, 3209, 307, 1270, 257, 41426, 15488, 332, 564, 2587, 2316, 295, 264, 1952, 3567, 11, 754, 1673, 294, 3124, 11, 50934, 50934, 382, 291, 458, 11, 2452, 2539, 14642, 360, 589, 588, 731, 13, 51128, 51128, 18600, 341, 3855, 295, 257, 2167, 34090, 11, 2390, 257, 18161, 3209, 586, 445, 7029, 51404, 51404, 1940, 257, 3840, 295, 613, 22027, 293, 27520, 552, 1214, 420, 3372, 552, 1214, 13, 51670, 51670, 961, 311, 586, 574, 412, 257, 544, 3997, 1365, 295, 4733, 17630, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.08999323395063293, "compression_ratio": 1.7153284671532847, "no_speech_prob": 5.1736919886025134e-06}, {"id": 36, "seek": 16084, "start": 186.96, "end": 190.64000000000001, "text": " Let's now look at a more complex example of demand prediction.", "tokens": [50364, 437, 604, 13910, 34090, 294, 428, 3567, 420, 1575, 775, 11, 597, 307, 983, 264, 11677, 18161, 50630, 50630, 3209, 307, 1270, 257, 41426, 15488, 332, 564, 2587, 2316, 295, 264, 1952, 3567, 11, 754, 1673, 294, 3124, 11, 50934, 50934, 382, 291, 458, 11, 2452, 2539, 14642, 360, 589, 588, 731, 13, 51128, 51128, 18600, 341, 3855, 295, 257, 2167, 34090, 11, 2390, 257, 18161, 3209, 586, 445, 7029, 51404, 51404, 1940, 257, 3840, 295, 613, 22027, 293, 27520, 552, 1214, 420, 3372, 552, 1214, 13, 51670, 51670, 961, 311, 586, 574, 412, 257, 544, 3997, 1365, 295, 4733, 17630, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.08999323395063293, "compression_ratio": 1.7153284671532847, "no_speech_prob": 5.1736919886025134e-06}, {"id": 37, "seek": 19064, "start": 190.64, "end": 196.11999999999998, "text": " In this example, we're going to have four features to predict whether or not a t-shirt", "tokens": [50364, 682, 341, 1365, 11, 321, 434, 516, 281, 362, 1451, 4122, 281, 6069, 1968, 420, 406, 257, 256, 12, 15313, 50638, 50638, 307, 257, 1192, 23600, 13, 50692, 50692, 440, 4122, 366, 264, 3218, 295, 264, 256, 12, 15313, 11, 264, 14122, 5497, 11, 264, 2372, 295, 6370, 50966, 50966, 295, 300, 1729, 256, 12, 15313, 11, 382, 731, 382, 264, 2527, 3125, 13, 51166, 51166, 1119, 341, 257, 1090, 3125, 5060, 13764, 11, 420, 307, 341, 1310, 257, 3126, 3125, 2527, 30, 51474, 51474, 823, 291, 1062, 9091, 300, 1968, 420, 406, 257, 256, 12, 15313, 3643, 257, 1192, 23600, 767, 5946, 51758, 51758, 322, 257, 1326, 6771, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.1342074327301561, "compression_ratio": 1.777327935222672, "no_speech_prob": 8.66440859681461e-06}, {"id": 38, "seek": 19064, "start": 196.11999999999998, "end": 197.2, "text": " is a top seller.", "tokens": [50364, 682, 341, 1365, 11, 321, 434, 516, 281, 362, 1451, 4122, 281, 6069, 1968, 420, 406, 257, 256, 12, 15313, 50638, 50638, 307, 257, 1192, 23600, 13, 50692, 50692, 440, 4122, 366, 264, 3218, 295, 264, 256, 12, 15313, 11, 264, 14122, 5497, 11, 264, 2372, 295, 6370, 50966, 50966, 295, 300, 1729, 256, 12, 15313, 11, 382, 731, 382, 264, 2527, 3125, 13, 51166, 51166, 1119, 341, 257, 1090, 3125, 5060, 13764, 11, 420, 307, 341, 1310, 257, 3126, 3125, 2527, 30, 51474, 51474, 823, 291, 1062, 9091, 300, 1968, 420, 406, 257, 256, 12, 15313, 3643, 257, 1192, 23600, 767, 5946, 51758, 51758, 322, 257, 1326, 6771, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.1342074327301561, "compression_ratio": 1.777327935222672, "no_speech_prob": 8.66440859681461e-06}, {"id": 39, "seek": 19064, "start": 197.2, "end": 202.67999999999998, "text": " The features are the price of the t-shirt, the shipping costs, the amount of marketing", "tokens": [50364, 682, 341, 1365, 11, 321, 434, 516, 281, 362, 1451, 4122, 281, 6069, 1968, 420, 406, 257, 256, 12, 15313, 50638, 50638, 307, 257, 1192, 23600, 13, 50692, 50692, 440, 4122, 366, 264, 3218, 295, 264, 256, 12, 15313, 11, 264, 14122, 5497, 11, 264, 2372, 295, 6370, 50966, 50966, 295, 300, 1729, 256, 12, 15313, 11, 382, 731, 382, 264, 2527, 3125, 13, 51166, 51166, 1119, 341, 257, 1090, 3125, 5060, 13764, 11, 420, 307, 341, 1310, 257, 3126, 3125, 2527, 30, 51474, 51474, 823, 291, 1062, 9091, 300, 1968, 420, 406, 257, 256, 12, 15313, 3643, 257, 1192, 23600, 767, 5946, 51758, 51758, 322, 257, 1326, 6771, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.1342074327301561, "compression_ratio": 1.777327935222672, "no_speech_prob": 8.66440859681461e-06}, {"id": 40, "seek": 19064, "start": 202.67999999999998, "end": 206.67999999999998, "text": " of that particular t-shirt, as well as the material quality.", "tokens": [50364, 682, 341, 1365, 11, 321, 434, 516, 281, 362, 1451, 4122, 281, 6069, 1968, 420, 406, 257, 256, 12, 15313, 50638, 50638, 307, 257, 1192, 23600, 13, 50692, 50692, 440, 4122, 366, 264, 3218, 295, 264, 256, 12, 15313, 11, 264, 14122, 5497, 11, 264, 2372, 295, 6370, 50966, 50966, 295, 300, 1729, 256, 12, 15313, 11, 382, 731, 382, 264, 2527, 3125, 13, 51166, 51166, 1119, 341, 257, 1090, 3125, 5060, 13764, 11, 420, 307, 341, 1310, 257, 3126, 3125, 2527, 30, 51474, 51474, 823, 291, 1062, 9091, 300, 1968, 420, 406, 257, 256, 12, 15313, 3643, 257, 1192, 23600, 767, 5946, 51758, 51758, 322, 257, 1326, 6771, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.1342074327301561, "compression_ratio": 1.777327935222672, "no_speech_prob": 8.66440859681461e-06}, {"id": 41, "seek": 19064, "start": 206.67999999999998, "end": 212.83999999999997, "text": " Is this a high quality thick cotton, or is this maybe a lower quality material?", "tokens": [50364, 682, 341, 1365, 11, 321, 434, 516, 281, 362, 1451, 4122, 281, 6069, 1968, 420, 406, 257, 256, 12, 15313, 50638, 50638, 307, 257, 1192, 23600, 13, 50692, 50692, 440, 4122, 366, 264, 3218, 295, 264, 256, 12, 15313, 11, 264, 14122, 5497, 11, 264, 2372, 295, 6370, 50966, 50966, 295, 300, 1729, 256, 12, 15313, 11, 382, 731, 382, 264, 2527, 3125, 13, 51166, 51166, 1119, 341, 257, 1090, 3125, 5060, 13764, 11, 420, 307, 341, 1310, 257, 3126, 3125, 2527, 30, 51474, 51474, 823, 291, 1062, 9091, 300, 1968, 420, 406, 257, 256, 12, 15313, 3643, 257, 1192, 23600, 767, 5946, 51758, 51758, 322, 257, 1326, 6771, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.1342074327301561, "compression_ratio": 1.777327935222672, "no_speech_prob": 8.66440859681461e-06}, {"id": 42, "seek": 19064, "start": 212.83999999999997, "end": 218.51999999999998, "text": " Now you might suspect that whether or not a t-shirt becomes a top seller actually depends", "tokens": [50364, 682, 341, 1365, 11, 321, 434, 516, 281, 362, 1451, 4122, 281, 6069, 1968, 420, 406, 257, 256, 12, 15313, 50638, 50638, 307, 257, 1192, 23600, 13, 50692, 50692, 440, 4122, 366, 264, 3218, 295, 264, 256, 12, 15313, 11, 264, 14122, 5497, 11, 264, 2372, 295, 6370, 50966, 50966, 295, 300, 1729, 256, 12, 15313, 11, 382, 731, 382, 264, 2527, 3125, 13, 51166, 51166, 1119, 341, 257, 1090, 3125, 5060, 13764, 11, 420, 307, 341, 1310, 257, 3126, 3125, 2527, 30, 51474, 51474, 823, 291, 1062, 9091, 300, 1968, 420, 406, 257, 256, 12, 15313, 3643, 257, 1192, 23600, 767, 5946, 51758, 51758, 322, 257, 1326, 6771, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.1342074327301561, "compression_ratio": 1.777327935222672, "no_speech_prob": 8.66440859681461e-06}, {"id": 43, "seek": 19064, "start": 218.51999999999998, "end": 219.92, "text": " on a few factors.", "tokens": [50364, 682, 341, 1365, 11, 321, 434, 516, 281, 362, 1451, 4122, 281, 6069, 1968, 420, 406, 257, 256, 12, 15313, 50638, 50638, 307, 257, 1192, 23600, 13, 50692, 50692, 440, 4122, 366, 264, 3218, 295, 264, 256, 12, 15313, 11, 264, 14122, 5497, 11, 264, 2372, 295, 6370, 50966, 50966, 295, 300, 1729, 256, 12, 15313, 11, 382, 731, 382, 264, 2527, 3125, 13, 51166, 51166, 1119, 341, 257, 1090, 3125, 5060, 13764, 11, 420, 307, 341, 1310, 257, 3126, 3125, 2527, 30, 51474, 51474, 823, 291, 1062, 9091, 300, 1968, 420, 406, 257, 256, 12, 15313, 3643, 257, 1192, 23600, 767, 5946, 51758, 51758, 322, 257, 1326, 6771, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.1342074327301561, "compression_ratio": 1.777327935222672, "no_speech_prob": 8.66440859681461e-06}, {"id": 44, "seek": 21992, "start": 219.92, "end": 223.95999999999998, "text": " First, what is the affordability of this t-shirt?", "tokens": [50364, 2386, 11, 437, 307, 264, 6157, 2310, 295, 341, 256, 12, 15313, 30, 50566, 50566, 5736, 307, 437, 311, 264, 4314, 295, 8888, 295, 341, 256, 12, 15313, 300, 3995, 23465, 362, 30, 50848, 50848, 400, 2636, 307, 19049, 3125, 13, 50962, 50962, 1144, 23465, 420, 3995, 23465, 519, 341, 307, 257, 1090, 3125, 256, 12, 15313, 30, 51200, 51200, 400, 370, 437, 286, 478, 516, 281, 360, 307, 1884, 472, 11677, 34090, 281, 853, 281, 12539, 264, 8482, 51580, 51580, 300, 341, 256, 12, 15313, 307, 19049, 382, 5405, 12028, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.11348971724510193, "compression_ratio": 1.8104265402843602, "no_speech_prob": 4.157267994742142e-06}, {"id": 45, "seek": 21992, "start": 223.95999999999998, "end": 229.6, "text": " Second is what's the degree of awareness of this t-shirt that potential buyers have?", "tokens": [50364, 2386, 11, 437, 307, 264, 6157, 2310, 295, 341, 256, 12, 15313, 30, 50566, 50566, 5736, 307, 437, 311, 264, 4314, 295, 8888, 295, 341, 256, 12, 15313, 300, 3995, 23465, 362, 30, 50848, 50848, 400, 2636, 307, 19049, 3125, 13, 50962, 50962, 1144, 23465, 420, 3995, 23465, 519, 341, 307, 257, 1090, 3125, 256, 12, 15313, 30, 51200, 51200, 400, 370, 437, 286, 478, 516, 281, 360, 307, 1884, 472, 11677, 34090, 281, 853, 281, 12539, 264, 8482, 51580, 51580, 300, 341, 256, 12, 15313, 307, 19049, 382, 5405, 12028, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.11348971724510193, "compression_ratio": 1.8104265402843602, "no_speech_prob": 4.157267994742142e-06}, {"id": 46, "seek": 21992, "start": 229.6, "end": 231.88, "text": " And third is perceived quality.", "tokens": [50364, 2386, 11, 437, 307, 264, 6157, 2310, 295, 341, 256, 12, 15313, 30, 50566, 50566, 5736, 307, 437, 311, 264, 4314, 295, 8888, 295, 341, 256, 12, 15313, 300, 3995, 23465, 362, 30, 50848, 50848, 400, 2636, 307, 19049, 3125, 13, 50962, 50962, 1144, 23465, 420, 3995, 23465, 519, 341, 307, 257, 1090, 3125, 256, 12, 15313, 30, 51200, 51200, 400, 370, 437, 286, 478, 516, 281, 360, 307, 1884, 472, 11677, 34090, 281, 853, 281, 12539, 264, 8482, 51580, 51580, 300, 341, 256, 12, 15313, 307, 19049, 382, 5405, 12028, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.11348971724510193, "compression_ratio": 1.8104265402843602, "no_speech_prob": 4.157267994742142e-06}, {"id": 47, "seek": 21992, "start": 231.88, "end": 236.64, "text": " Do buyers or potential buyers think this is a high quality t-shirt?", "tokens": [50364, 2386, 11, 437, 307, 264, 6157, 2310, 295, 341, 256, 12, 15313, 30, 50566, 50566, 5736, 307, 437, 311, 264, 4314, 295, 8888, 295, 341, 256, 12, 15313, 300, 3995, 23465, 362, 30, 50848, 50848, 400, 2636, 307, 19049, 3125, 13, 50962, 50962, 1144, 23465, 420, 3995, 23465, 519, 341, 307, 257, 1090, 3125, 256, 12, 15313, 30, 51200, 51200, 400, 370, 437, 286, 478, 516, 281, 360, 307, 1884, 472, 11677, 34090, 281, 853, 281, 12539, 264, 8482, 51580, 51580, 300, 341, 256, 12, 15313, 307, 19049, 382, 5405, 12028, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.11348971724510193, "compression_ratio": 1.8104265402843602, "no_speech_prob": 4.157267994742142e-06}, {"id": 48, "seek": 21992, "start": 236.64, "end": 244.23999999999998, "text": " And so what I'm going to do is create one artificial neuron to try to estimate the probability", "tokens": [50364, 2386, 11, 437, 307, 264, 6157, 2310, 295, 341, 256, 12, 15313, 30, 50566, 50566, 5736, 307, 437, 311, 264, 4314, 295, 8888, 295, 341, 256, 12, 15313, 300, 3995, 23465, 362, 30, 50848, 50848, 400, 2636, 307, 19049, 3125, 13, 50962, 50962, 1144, 23465, 420, 3995, 23465, 519, 341, 307, 257, 1090, 3125, 256, 12, 15313, 30, 51200, 51200, 400, 370, 437, 286, 478, 516, 281, 360, 307, 1884, 472, 11677, 34090, 281, 853, 281, 12539, 264, 8482, 51580, 51580, 300, 341, 256, 12, 15313, 307, 19049, 382, 5405, 12028, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.11348971724510193, "compression_ratio": 1.8104265402843602, "no_speech_prob": 4.157267994742142e-06}, {"id": 49, "seek": 21992, "start": 244.23999999999998, "end": 248.27999999999997, "text": " that this t-shirt is perceived as highly affordable.", "tokens": [50364, 2386, 11, 437, 307, 264, 6157, 2310, 295, 341, 256, 12, 15313, 30, 50566, 50566, 5736, 307, 437, 311, 264, 4314, 295, 8888, 295, 341, 256, 12, 15313, 300, 3995, 23465, 362, 30, 50848, 50848, 400, 2636, 307, 19049, 3125, 13, 50962, 50962, 1144, 23465, 420, 3995, 23465, 519, 341, 307, 257, 1090, 3125, 256, 12, 15313, 30, 51200, 51200, 400, 370, 437, 286, 478, 516, 281, 360, 307, 1884, 472, 11677, 34090, 281, 853, 281, 12539, 264, 8482, 51580, 51580, 300, 341, 256, 12, 15313, 307, 19049, 382, 5405, 12028, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.11348971724510193, "compression_ratio": 1.8104265402843602, "no_speech_prob": 4.157267994742142e-06}, {"id": 50, "seek": 24828, "start": 248.28, "end": 253.04, "text": " And affordability is mainly a function of price and shipping costs because the total", "tokens": [50364, 400, 6157, 2310, 307, 8704, 257, 2445, 295, 3218, 293, 14122, 5497, 570, 264, 3217, 50602, 50602, 2372, 291, 362, 281, 1689, 307, 264, 2408, 295, 264, 3218, 1804, 264, 14122, 5497, 13, 50778, 50778, 400, 370, 321, 434, 516, 281, 764, 257, 707, 34090, 510, 11, 257, 3565, 3142, 24590, 4985, 11, 281, 4846, 50984, 50984, 3218, 293, 14122, 5497, 293, 6069, 11, 360, 561, 519, 341, 307, 12028, 30, 51216, 51216, 5736, 11, 286, 478, 516, 281, 1884, 1071, 11677, 34090, 510, 281, 12539, 11, 307, 456, 1090, 8888, 51528, 51528, 295, 341, 30, 51581, 51581, 400, 8888, 294, 341, 1389, 307, 8704, 257, 2445, 295, 264, 6370, 295, 264, 256, 12, 15313, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.10792128245035808, "compression_ratio": 1.862453531598513, "no_speech_prob": 9.223318556905724e-06}, {"id": 51, "seek": 24828, "start": 253.04, "end": 256.56, "text": " amount you have to pay is the sum of the price plus the shipping costs.", "tokens": [50364, 400, 6157, 2310, 307, 8704, 257, 2445, 295, 3218, 293, 14122, 5497, 570, 264, 3217, 50602, 50602, 2372, 291, 362, 281, 1689, 307, 264, 2408, 295, 264, 3218, 1804, 264, 14122, 5497, 13, 50778, 50778, 400, 370, 321, 434, 516, 281, 764, 257, 707, 34090, 510, 11, 257, 3565, 3142, 24590, 4985, 11, 281, 4846, 50984, 50984, 3218, 293, 14122, 5497, 293, 6069, 11, 360, 561, 519, 341, 307, 12028, 30, 51216, 51216, 5736, 11, 286, 478, 516, 281, 1884, 1071, 11677, 34090, 510, 281, 12539, 11, 307, 456, 1090, 8888, 51528, 51528, 295, 341, 30, 51581, 51581, 400, 8888, 294, 341, 1389, 307, 8704, 257, 2445, 295, 264, 6370, 295, 264, 256, 12, 15313, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.10792128245035808, "compression_ratio": 1.862453531598513, "no_speech_prob": 9.223318556905724e-06}, {"id": 52, "seek": 24828, "start": 256.56, "end": 260.68, "text": " And so we're going to use a little neuron here, a logistic regression unit, to input", "tokens": [50364, 400, 6157, 2310, 307, 8704, 257, 2445, 295, 3218, 293, 14122, 5497, 570, 264, 3217, 50602, 50602, 2372, 291, 362, 281, 1689, 307, 264, 2408, 295, 264, 3218, 1804, 264, 14122, 5497, 13, 50778, 50778, 400, 370, 321, 434, 516, 281, 764, 257, 707, 34090, 510, 11, 257, 3565, 3142, 24590, 4985, 11, 281, 4846, 50984, 50984, 3218, 293, 14122, 5497, 293, 6069, 11, 360, 561, 519, 341, 307, 12028, 30, 51216, 51216, 5736, 11, 286, 478, 516, 281, 1884, 1071, 11677, 34090, 510, 281, 12539, 11, 307, 456, 1090, 8888, 51528, 51528, 295, 341, 30, 51581, 51581, 400, 8888, 294, 341, 1389, 307, 8704, 257, 2445, 295, 264, 6370, 295, 264, 256, 12, 15313, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.10792128245035808, "compression_ratio": 1.862453531598513, "no_speech_prob": 9.223318556905724e-06}, {"id": 53, "seek": 24828, "start": 260.68, "end": 265.32, "text": " price and shipping costs and predict, do people think this is affordable?", "tokens": [50364, 400, 6157, 2310, 307, 8704, 257, 2445, 295, 3218, 293, 14122, 5497, 570, 264, 3217, 50602, 50602, 2372, 291, 362, 281, 1689, 307, 264, 2408, 295, 264, 3218, 1804, 264, 14122, 5497, 13, 50778, 50778, 400, 370, 321, 434, 516, 281, 764, 257, 707, 34090, 510, 11, 257, 3565, 3142, 24590, 4985, 11, 281, 4846, 50984, 50984, 3218, 293, 14122, 5497, 293, 6069, 11, 360, 561, 519, 341, 307, 12028, 30, 51216, 51216, 5736, 11, 286, 478, 516, 281, 1884, 1071, 11677, 34090, 510, 281, 12539, 11, 307, 456, 1090, 8888, 51528, 51528, 295, 341, 30, 51581, 51581, 400, 8888, 294, 341, 1389, 307, 8704, 257, 2445, 295, 264, 6370, 295, 264, 256, 12, 15313, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.10792128245035808, "compression_ratio": 1.862453531598513, "no_speech_prob": 9.223318556905724e-06}, {"id": 54, "seek": 24828, "start": 265.32, "end": 271.56, "text": " Second, I'm going to create another artificial neuron here to estimate, is there high awareness", "tokens": [50364, 400, 6157, 2310, 307, 8704, 257, 2445, 295, 3218, 293, 14122, 5497, 570, 264, 3217, 50602, 50602, 2372, 291, 362, 281, 1689, 307, 264, 2408, 295, 264, 3218, 1804, 264, 14122, 5497, 13, 50778, 50778, 400, 370, 321, 434, 516, 281, 764, 257, 707, 34090, 510, 11, 257, 3565, 3142, 24590, 4985, 11, 281, 4846, 50984, 50984, 3218, 293, 14122, 5497, 293, 6069, 11, 360, 561, 519, 341, 307, 12028, 30, 51216, 51216, 5736, 11, 286, 478, 516, 281, 1884, 1071, 11677, 34090, 510, 281, 12539, 11, 307, 456, 1090, 8888, 51528, 51528, 295, 341, 30, 51581, 51581, 400, 8888, 294, 341, 1389, 307, 8704, 257, 2445, 295, 264, 6370, 295, 264, 256, 12, 15313, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.10792128245035808, "compression_ratio": 1.862453531598513, "no_speech_prob": 9.223318556905724e-06}, {"id": 55, "seek": 24828, "start": 271.56, "end": 272.62, "text": " of this?", "tokens": [50364, 400, 6157, 2310, 307, 8704, 257, 2445, 295, 3218, 293, 14122, 5497, 570, 264, 3217, 50602, 50602, 2372, 291, 362, 281, 1689, 307, 264, 2408, 295, 264, 3218, 1804, 264, 14122, 5497, 13, 50778, 50778, 400, 370, 321, 434, 516, 281, 764, 257, 707, 34090, 510, 11, 257, 3565, 3142, 24590, 4985, 11, 281, 4846, 50984, 50984, 3218, 293, 14122, 5497, 293, 6069, 11, 360, 561, 519, 341, 307, 12028, 30, 51216, 51216, 5736, 11, 286, 478, 516, 281, 1884, 1071, 11677, 34090, 510, 281, 12539, 11, 307, 456, 1090, 8888, 51528, 51528, 295, 341, 30, 51581, 51581, 400, 8888, 294, 341, 1389, 307, 8704, 257, 2445, 295, 264, 6370, 295, 264, 256, 12, 15313, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.10792128245035808, "compression_ratio": 1.862453531598513, "no_speech_prob": 9.223318556905724e-06}, {"id": 56, "seek": 24828, "start": 272.62, "end": 278.16, "text": " And awareness in this case is mainly a function of the marketing of the t-shirt.", "tokens": [50364, 400, 6157, 2310, 307, 8704, 257, 2445, 295, 3218, 293, 14122, 5497, 570, 264, 3217, 50602, 50602, 2372, 291, 362, 281, 1689, 307, 264, 2408, 295, 264, 3218, 1804, 264, 14122, 5497, 13, 50778, 50778, 400, 370, 321, 434, 516, 281, 764, 257, 707, 34090, 510, 11, 257, 3565, 3142, 24590, 4985, 11, 281, 4846, 50984, 50984, 3218, 293, 14122, 5497, 293, 6069, 11, 360, 561, 519, 341, 307, 12028, 30, 51216, 51216, 5736, 11, 286, 478, 516, 281, 1884, 1071, 11677, 34090, 510, 281, 12539, 11, 307, 456, 1090, 8888, 51528, 51528, 295, 341, 30, 51581, 51581, 400, 8888, 294, 341, 1389, 307, 8704, 257, 2445, 295, 264, 6370, 295, 264, 256, 12, 15313, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.10792128245035808, "compression_ratio": 1.862453531598513, "no_speech_prob": 9.223318556905724e-06}, {"id": 57, "seek": 27816, "start": 278.16, "end": 284.04, "text": " And finally, going to create another neuron to estimate, do people perceive this to be", "tokens": [50364, 400, 2721, 11, 516, 281, 1884, 1071, 34090, 281, 12539, 11, 360, 561, 20281, 341, 281, 312, 50658, 50658, 295, 1090, 3125, 30, 50762, 50762, 400, 300, 815, 8704, 312, 257, 2445, 295, 264, 3218, 295, 264, 256, 12, 15313, 293, 295, 264, 2527, 3125, 13, 51090, 51090, 25803, 307, 257, 5952, 510, 570, 25511, 420, 7015, 11, 498, 456, 311, 257, 588, 1090, 3218, 51346, 51346, 256, 12, 15313, 11, 561, 486, 2171, 20281, 300, 281, 312, 295, 1090, 3125, 570, 498, 309, 311, 588, 51602, 51602, 5124, 550, 1310, 561, 519, 309, 311, 658, 281, 312, 295, 1090, 3125, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.12501429611781859, "compression_ratio": 1.8638297872340426, "no_speech_prob": 3.500765842545661e-06}, {"id": 58, "seek": 27816, "start": 284.04, "end": 286.12, "text": " of high quality?", "tokens": [50364, 400, 2721, 11, 516, 281, 1884, 1071, 34090, 281, 12539, 11, 360, 561, 20281, 341, 281, 312, 50658, 50658, 295, 1090, 3125, 30, 50762, 50762, 400, 300, 815, 8704, 312, 257, 2445, 295, 264, 3218, 295, 264, 256, 12, 15313, 293, 295, 264, 2527, 3125, 13, 51090, 51090, 25803, 307, 257, 5952, 510, 570, 25511, 420, 7015, 11, 498, 456, 311, 257, 588, 1090, 3218, 51346, 51346, 256, 12, 15313, 11, 561, 486, 2171, 20281, 300, 281, 312, 295, 1090, 3125, 570, 498, 309, 311, 588, 51602, 51602, 5124, 550, 1310, 561, 519, 309, 311, 658, 281, 312, 295, 1090, 3125, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.12501429611781859, "compression_ratio": 1.8638297872340426, "no_speech_prob": 3.500765842545661e-06}, {"id": 59, "seek": 27816, "start": 286.12, "end": 292.68, "text": " And that may mainly be a function of the price of the t-shirt and of the material quality.", "tokens": [50364, 400, 2721, 11, 516, 281, 1884, 1071, 34090, 281, 12539, 11, 360, 561, 20281, 341, 281, 312, 50658, 50658, 295, 1090, 3125, 30, 50762, 50762, 400, 300, 815, 8704, 312, 257, 2445, 295, 264, 3218, 295, 264, 256, 12, 15313, 293, 295, 264, 2527, 3125, 13, 51090, 51090, 25803, 307, 257, 5952, 510, 570, 25511, 420, 7015, 11, 498, 456, 311, 257, 588, 1090, 3218, 51346, 51346, 256, 12, 15313, 11, 561, 486, 2171, 20281, 300, 281, 312, 295, 1090, 3125, 570, 498, 309, 311, 588, 51602, 51602, 5124, 550, 1310, 561, 519, 309, 311, 658, 281, 312, 295, 1090, 3125, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.12501429611781859, "compression_ratio": 1.8638297872340426, "no_speech_prob": 3.500765842545661e-06}, {"id": 60, "seek": 27816, "start": 292.68, "end": 297.8, "text": " Price is a factor here because fortunately or unfortunately, if there's a very high price", "tokens": [50364, 400, 2721, 11, 516, 281, 1884, 1071, 34090, 281, 12539, 11, 360, 561, 20281, 341, 281, 312, 50658, 50658, 295, 1090, 3125, 30, 50762, 50762, 400, 300, 815, 8704, 312, 257, 2445, 295, 264, 3218, 295, 264, 256, 12, 15313, 293, 295, 264, 2527, 3125, 13, 51090, 51090, 25803, 307, 257, 5952, 510, 570, 25511, 420, 7015, 11, 498, 456, 311, 257, 588, 1090, 3218, 51346, 51346, 256, 12, 15313, 11, 561, 486, 2171, 20281, 300, 281, 312, 295, 1090, 3125, 570, 498, 309, 311, 588, 51602, 51602, 5124, 550, 1310, 561, 519, 309, 311, 658, 281, 312, 295, 1090, 3125, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.12501429611781859, "compression_ratio": 1.8638297872340426, "no_speech_prob": 3.500765842545661e-06}, {"id": 61, "seek": 27816, "start": 297.8, "end": 302.92, "text": " t-shirt, people will sometimes perceive that to be of high quality because if it's very", "tokens": [50364, 400, 2721, 11, 516, 281, 1884, 1071, 34090, 281, 12539, 11, 360, 561, 20281, 341, 281, 312, 50658, 50658, 295, 1090, 3125, 30, 50762, 50762, 400, 300, 815, 8704, 312, 257, 2445, 295, 264, 3218, 295, 264, 256, 12, 15313, 293, 295, 264, 2527, 3125, 13, 51090, 51090, 25803, 307, 257, 5952, 510, 570, 25511, 420, 7015, 11, 498, 456, 311, 257, 588, 1090, 3218, 51346, 51346, 256, 12, 15313, 11, 561, 486, 2171, 20281, 300, 281, 312, 295, 1090, 3125, 570, 498, 309, 311, 588, 51602, 51602, 5124, 550, 1310, 561, 519, 309, 311, 658, 281, 312, 295, 1090, 3125, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.12501429611781859, "compression_ratio": 1.8638297872340426, "no_speech_prob": 3.500765842545661e-06}, {"id": 62, "seek": 27816, "start": 302.92, "end": 307.32000000000005, "text": " expensive then maybe people think it's got to be of high quality.", "tokens": [50364, 400, 2721, 11, 516, 281, 1884, 1071, 34090, 281, 12539, 11, 360, 561, 20281, 341, 281, 312, 50658, 50658, 295, 1090, 3125, 30, 50762, 50762, 400, 300, 815, 8704, 312, 257, 2445, 295, 264, 3218, 295, 264, 256, 12, 15313, 293, 295, 264, 2527, 3125, 13, 51090, 51090, 25803, 307, 257, 5952, 510, 570, 25511, 420, 7015, 11, 498, 456, 311, 257, 588, 1090, 3218, 51346, 51346, 256, 12, 15313, 11, 561, 486, 2171, 20281, 300, 281, 312, 295, 1090, 3125, 570, 498, 309, 311, 588, 51602, 51602, 5124, 550, 1310, 561, 519, 309, 311, 658, 281, 312, 295, 1090, 3125, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.12501429611781859, "compression_ratio": 1.8638297872340426, "no_speech_prob": 3.500765842545661e-06}, {"id": 63, "seek": 30732, "start": 307.32, "end": 313.44, "text": " Given these estimates of affordability, awareness, and perceived quality, we then wire the outputs", "tokens": [50364, 18600, 613, 20561, 295, 6157, 2310, 11, 8888, 11, 293, 19049, 3125, 11, 321, 550, 6234, 264, 23930, 50670, 50670, 295, 613, 1045, 22027, 281, 1071, 34090, 510, 322, 264, 558, 300, 550, 307, 1071, 3565, 3142, 50940, 50940, 24590, 4985, 300, 2721, 15743, 729, 1045, 3547, 293, 23930, 264, 8482, 51229, 51229, 295, 341, 256, 12, 15313, 885, 257, 1192, 23600, 13, 51371, 51371, 407, 294, 264, 27575, 295, 18161, 9590, 11, 321, 434, 516, 281, 1594, 613, 1045, 22027, 51653, 51653], "temperature": 0.0, "avg_logprob": -0.10691817788516773, "compression_ratio": 1.7727272727272727, "no_speech_prob": 8.13920905784471e-06}, {"id": 64, "seek": 30732, "start": 313.44, "end": 318.84, "text": " of these three neurons to another neuron here on the right that then is another logistic", "tokens": [50364, 18600, 613, 20561, 295, 6157, 2310, 11, 8888, 11, 293, 19049, 3125, 11, 321, 550, 6234, 264, 23930, 50670, 50670, 295, 613, 1045, 22027, 281, 1071, 34090, 510, 322, 264, 558, 300, 550, 307, 1071, 3565, 3142, 50940, 50940, 24590, 4985, 300, 2721, 15743, 729, 1045, 3547, 293, 23930, 264, 8482, 51229, 51229, 295, 341, 256, 12, 15313, 885, 257, 1192, 23600, 13, 51371, 51371, 407, 294, 264, 27575, 295, 18161, 9590, 11, 321, 434, 516, 281, 1594, 613, 1045, 22027, 51653, 51653], "temperature": 0.0, "avg_logprob": -0.10691817788516773, "compression_ratio": 1.7727272727272727, "no_speech_prob": 8.13920905784471e-06}, {"id": 65, "seek": 30732, "start": 318.84, "end": 324.62, "text": " regression unit that finally inputs those three numbers and outputs the probability", "tokens": [50364, 18600, 613, 20561, 295, 6157, 2310, 11, 8888, 11, 293, 19049, 3125, 11, 321, 550, 6234, 264, 23930, 50670, 50670, 295, 613, 1045, 22027, 281, 1071, 34090, 510, 322, 264, 558, 300, 550, 307, 1071, 3565, 3142, 50940, 50940, 24590, 4985, 300, 2721, 15743, 729, 1045, 3547, 293, 23930, 264, 8482, 51229, 51229, 295, 341, 256, 12, 15313, 885, 257, 1192, 23600, 13, 51371, 51371, 407, 294, 264, 27575, 295, 18161, 9590, 11, 321, 434, 516, 281, 1594, 613, 1045, 22027, 51653, 51653], "temperature": 0.0, "avg_logprob": -0.10691817788516773, "compression_ratio": 1.7727272727272727, "no_speech_prob": 8.13920905784471e-06}, {"id": 66, "seek": 30732, "start": 324.62, "end": 327.46, "text": " of this t-shirt being a top seller.", "tokens": [50364, 18600, 613, 20561, 295, 6157, 2310, 11, 8888, 11, 293, 19049, 3125, 11, 321, 550, 6234, 264, 23930, 50670, 50670, 295, 613, 1045, 22027, 281, 1071, 34090, 510, 322, 264, 558, 300, 550, 307, 1071, 3565, 3142, 50940, 50940, 24590, 4985, 300, 2721, 15743, 729, 1045, 3547, 293, 23930, 264, 8482, 51229, 51229, 295, 341, 256, 12, 15313, 885, 257, 1192, 23600, 13, 51371, 51371, 407, 294, 264, 27575, 295, 18161, 9590, 11, 321, 434, 516, 281, 1594, 613, 1045, 22027, 51653, 51653], "temperature": 0.0, "avg_logprob": -0.10691817788516773, "compression_ratio": 1.7727272727272727, "no_speech_prob": 8.13920905784471e-06}, {"id": 67, "seek": 30732, "start": 327.46, "end": 333.1, "text": " So in the terminology of neural networks, we're going to group these three neurons", "tokens": [50364, 18600, 613, 20561, 295, 6157, 2310, 11, 8888, 11, 293, 19049, 3125, 11, 321, 550, 6234, 264, 23930, 50670, 50670, 295, 613, 1045, 22027, 281, 1071, 34090, 510, 322, 264, 558, 300, 550, 307, 1071, 3565, 3142, 50940, 50940, 24590, 4985, 300, 2721, 15743, 729, 1045, 3547, 293, 23930, 264, 8482, 51229, 51229, 295, 341, 256, 12, 15313, 885, 257, 1192, 23600, 13, 51371, 51371, 407, 294, 264, 27575, 295, 18161, 9590, 11, 321, 434, 516, 281, 1594, 613, 1045, 22027, 51653, 51653], "temperature": 0.0, "avg_logprob": -0.10691817788516773, "compression_ratio": 1.7727272727272727, "no_speech_prob": 8.13920905784471e-06}, {"id": 68, "seek": 33310, "start": 333.1, "end": 340.52000000000004, "text": " together into what's called a layer, and a layer is a grouping of neurons which take", "tokens": [50364, 1214, 666, 437, 311, 1219, 257, 4583, 11, 293, 257, 4583, 307, 257, 40149, 295, 22027, 597, 747, 50735, 50735, 382, 4846, 264, 912, 420, 2531, 4122, 293, 300, 294, 1261, 23930, 257, 1326, 3547, 1214, 13, 51091, 51091, 407, 613, 1045, 22027, 322, 264, 1411, 1254, 472, 4583, 11, 597, 307, 983, 286, 12804, 552, 322, 1192, 295, 51315, 51315, 1184, 661, 13, 51379, 51379, 400, 264, 2167, 34090, 322, 264, 558, 307, 611, 472, 4583, 13, 51611, 51611, 440, 4583, 322, 264, 1411, 575, 1045, 22027, 11, 370, 257, 4583, 393, 362, 3866, 22027, 11, 420, 309, 393, 51849, 51849], "temperature": 0.0, "avg_logprob": -0.10035817439739521, "compression_ratio": 1.823008849557522, "no_speech_prob": 8.664395863888785e-06}, {"id": 69, "seek": 33310, "start": 340.52000000000004, "end": 347.64000000000004, "text": " as input the same or similar features and that in turn outputs a few numbers together.", "tokens": [50364, 1214, 666, 437, 311, 1219, 257, 4583, 11, 293, 257, 4583, 307, 257, 40149, 295, 22027, 597, 747, 50735, 50735, 382, 4846, 264, 912, 420, 2531, 4122, 293, 300, 294, 1261, 23930, 257, 1326, 3547, 1214, 13, 51091, 51091, 407, 613, 1045, 22027, 322, 264, 1411, 1254, 472, 4583, 11, 597, 307, 983, 286, 12804, 552, 322, 1192, 295, 51315, 51315, 1184, 661, 13, 51379, 51379, 400, 264, 2167, 34090, 322, 264, 558, 307, 611, 472, 4583, 13, 51611, 51611, 440, 4583, 322, 264, 1411, 575, 1045, 22027, 11, 370, 257, 4583, 393, 362, 3866, 22027, 11, 420, 309, 393, 51849, 51849], "temperature": 0.0, "avg_logprob": -0.10035817439739521, "compression_ratio": 1.823008849557522, "no_speech_prob": 8.664395863888785e-06}, {"id": 70, "seek": 33310, "start": 347.64000000000004, "end": 352.12, "text": " So these three neurons on the left form one layer, which is why I drew them on top of", "tokens": [50364, 1214, 666, 437, 311, 1219, 257, 4583, 11, 293, 257, 4583, 307, 257, 40149, 295, 22027, 597, 747, 50735, 50735, 382, 4846, 264, 912, 420, 2531, 4122, 293, 300, 294, 1261, 23930, 257, 1326, 3547, 1214, 13, 51091, 51091, 407, 613, 1045, 22027, 322, 264, 1411, 1254, 472, 4583, 11, 597, 307, 983, 286, 12804, 552, 322, 1192, 295, 51315, 51315, 1184, 661, 13, 51379, 51379, 400, 264, 2167, 34090, 322, 264, 558, 307, 611, 472, 4583, 13, 51611, 51611, 440, 4583, 322, 264, 1411, 575, 1045, 22027, 11, 370, 257, 4583, 393, 362, 3866, 22027, 11, 420, 309, 393, 51849, 51849], "temperature": 0.0, "avg_logprob": -0.10035817439739521, "compression_ratio": 1.823008849557522, "no_speech_prob": 8.664395863888785e-06}, {"id": 71, "seek": 33310, "start": 352.12, "end": 353.40000000000003, "text": " each other.", "tokens": [50364, 1214, 666, 437, 311, 1219, 257, 4583, 11, 293, 257, 4583, 307, 257, 40149, 295, 22027, 597, 747, 50735, 50735, 382, 4846, 264, 912, 420, 2531, 4122, 293, 300, 294, 1261, 23930, 257, 1326, 3547, 1214, 13, 51091, 51091, 407, 613, 1045, 22027, 322, 264, 1411, 1254, 472, 4583, 11, 597, 307, 983, 286, 12804, 552, 322, 1192, 295, 51315, 51315, 1184, 661, 13, 51379, 51379, 400, 264, 2167, 34090, 322, 264, 558, 307, 611, 472, 4583, 13, 51611, 51611, 440, 4583, 322, 264, 1411, 575, 1045, 22027, 11, 370, 257, 4583, 393, 362, 3866, 22027, 11, 420, 309, 393, 51849, 51849], "temperature": 0.0, "avg_logprob": -0.10035817439739521, "compression_ratio": 1.823008849557522, "no_speech_prob": 8.664395863888785e-06}, {"id": 72, "seek": 33310, "start": 353.40000000000003, "end": 358.04, "text": " And the single neuron on the right is also one layer.", "tokens": [50364, 1214, 666, 437, 311, 1219, 257, 4583, 11, 293, 257, 4583, 307, 257, 40149, 295, 22027, 597, 747, 50735, 50735, 382, 4846, 264, 912, 420, 2531, 4122, 293, 300, 294, 1261, 23930, 257, 1326, 3547, 1214, 13, 51091, 51091, 407, 613, 1045, 22027, 322, 264, 1411, 1254, 472, 4583, 11, 597, 307, 983, 286, 12804, 552, 322, 1192, 295, 51315, 51315, 1184, 661, 13, 51379, 51379, 400, 264, 2167, 34090, 322, 264, 558, 307, 611, 472, 4583, 13, 51611, 51611, 440, 4583, 322, 264, 1411, 575, 1045, 22027, 11, 370, 257, 4583, 393, 362, 3866, 22027, 11, 420, 309, 393, 51849, 51849], "temperature": 0.0, "avg_logprob": -0.10035817439739521, "compression_ratio": 1.823008849557522, "no_speech_prob": 8.664395863888785e-06}, {"id": 73, "seek": 33310, "start": 358.04, "end": 362.8, "text": " The layer on the left has three neurons, so a layer can have multiple neurons, or it can", "tokens": [50364, 1214, 666, 437, 311, 1219, 257, 4583, 11, 293, 257, 4583, 307, 257, 40149, 295, 22027, 597, 747, 50735, 50735, 382, 4846, 264, 912, 420, 2531, 4122, 293, 300, 294, 1261, 23930, 257, 1326, 3547, 1214, 13, 51091, 51091, 407, 613, 1045, 22027, 322, 264, 1411, 1254, 472, 4583, 11, 597, 307, 983, 286, 12804, 552, 322, 1192, 295, 51315, 51315, 1184, 661, 13, 51379, 51379, 400, 264, 2167, 34090, 322, 264, 558, 307, 611, 472, 4583, 13, 51611, 51611, 440, 4583, 322, 264, 1411, 575, 1045, 22027, 11, 370, 257, 4583, 393, 362, 3866, 22027, 11, 420, 309, 393, 51849, 51849], "temperature": 0.0, "avg_logprob": -0.10035817439739521, "compression_ratio": 1.823008849557522, "no_speech_prob": 8.664395863888785e-06}, {"id": 74, "seek": 36280, "start": 362.8, "end": 367.68, "text": " also have a single neuron, as in the case of this layer on the right.", "tokens": [50364, 611, 362, 257, 2167, 34090, 11, 382, 294, 264, 1389, 295, 341, 4583, 322, 264, 558, 13, 50608, 50608, 639, 4583, 322, 264, 558, 307, 611, 1219, 264, 5598, 4583, 570, 264, 5598, 295, 341, 2572, 50924, 50924, 34090, 307, 264, 5598, 8482, 19147, 538, 264, 18161, 3209, 13, 51190, 51190, 682, 264, 27575, 295, 18161, 9590, 11, 321, 434, 611, 516, 281, 818, 6157, 2310, 11, 8888, 11, 51464, 51464, 293, 19049, 3125, 281, 312, 2430, 763, 13, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.07375286286135754, "compression_ratio": 1.8020304568527918, "no_speech_prob": 1.1365400496288203e-06}, {"id": 75, "seek": 36280, "start": 367.68, "end": 374.0, "text": " This layer on the right is also called the output layer because the output of this final", "tokens": [50364, 611, 362, 257, 2167, 34090, 11, 382, 294, 264, 1389, 295, 341, 4583, 322, 264, 558, 13, 50608, 50608, 639, 4583, 322, 264, 558, 307, 611, 1219, 264, 5598, 4583, 570, 264, 5598, 295, 341, 2572, 50924, 50924, 34090, 307, 264, 5598, 8482, 19147, 538, 264, 18161, 3209, 13, 51190, 51190, 682, 264, 27575, 295, 18161, 9590, 11, 321, 434, 611, 516, 281, 818, 6157, 2310, 11, 8888, 11, 51464, 51464, 293, 19049, 3125, 281, 312, 2430, 763, 13, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.07375286286135754, "compression_ratio": 1.8020304568527918, "no_speech_prob": 1.1365400496288203e-06}, {"id": 76, "seek": 36280, "start": 374.0, "end": 379.32, "text": " neuron is the output probability predicted by the neural network.", "tokens": [50364, 611, 362, 257, 2167, 34090, 11, 382, 294, 264, 1389, 295, 341, 4583, 322, 264, 558, 13, 50608, 50608, 639, 4583, 322, 264, 558, 307, 611, 1219, 264, 5598, 4583, 570, 264, 5598, 295, 341, 2572, 50924, 50924, 34090, 307, 264, 5598, 8482, 19147, 538, 264, 18161, 3209, 13, 51190, 51190, 682, 264, 27575, 295, 18161, 9590, 11, 321, 434, 611, 516, 281, 818, 6157, 2310, 11, 8888, 11, 51464, 51464, 293, 19049, 3125, 281, 312, 2430, 763, 13, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.07375286286135754, "compression_ratio": 1.8020304568527918, "no_speech_prob": 1.1365400496288203e-06}, {"id": 77, "seek": 36280, "start": 379.32, "end": 384.8, "text": " In the terminology of neural networks, we're also going to call affordability, awareness,", "tokens": [50364, 611, 362, 257, 2167, 34090, 11, 382, 294, 264, 1389, 295, 341, 4583, 322, 264, 558, 13, 50608, 50608, 639, 4583, 322, 264, 558, 307, 611, 1219, 264, 5598, 4583, 570, 264, 5598, 295, 341, 2572, 50924, 50924, 34090, 307, 264, 5598, 8482, 19147, 538, 264, 18161, 3209, 13, 51190, 51190, 682, 264, 27575, 295, 18161, 9590, 11, 321, 434, 611, 516, 281, 818, 6157, 2310, 11, 8888, 11, 51464, 51464, 293, 19049, 3125, 281, 312, 2430, 763, 13, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.07375286286135754, "compression_ratio": 1.8020304568527918, "no_speech_prob": 1.1365400496288203e-06}, {"id": 78, "seek": 36280, "start": 384.8, "end": 388.32, "text": " and perceived quality to be activations.", "tokens": [50364, 611, 362, 257, 2167, 34090, 11, 382, 294, 264, 1389, 295, 341, 4583, 322, 264, 558, 13, 50608, 50608, 639, 4583, 322, 264, 558, 307, 611, 1219, 264, 5598, 4583, 570, 264, 5598, 295, 341, 2572, 50924, 50924, 34090, 307, 264, 5598, 8482, 19147, 538, 264, 18161, 3209, 13, 51190, 51190, 682, 264, 27575, 295, 18161, 9590, 11, 321, 434, 611, 516, 281, 818, 6157, 2310, 11, 8888, 11, 51464, 51464, 293, 19049, 3125, 281, 312, 2430, 763, 13, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.07375286286135754, "compression_ratio": 1.8020304568527918, "no_speech_prob": 1.1365400496288203e-06}, {"id": 79, "seek": 38832, "start": 388.32, "end": 393.24, "text": " The term activations comes from biological neurons, and it refers to the degree that", "tokens": [50364, 440, 1433, 2430, 763, 1487, 490, 13910, 22027, 11, 293, 309, 14942, 281, 264, 4314, 300, 50610, 50610, 257, 13910, 34090, 307, 7750, 257, 1090, 5598, 2158, 420, 7750, 867, 12147, 41767, 6196, 50868, 50868, 281, 661, 22027, 11, 281, 264, 30621, 490, 309, 13, 51016, 51016, 400, 370, 613, 3547, 322, 6157, 2310, 11, 8888, 11, 293, 19049, 3125, 366, 264, 2430, 763, 51284, 51284, 295, 613, 1045, 22027, 294, 341, 4583, 13, 51454, 51454, 400, 611, 11, 341, 5598, 8482, 307, 264, 24433, 295, 341, 34090, 4898, 510, 322, 264, 558, 13, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1380855112659688, "compression_ratio": 1.8559322033898304, "no_speech_prob": 1.6027604488044744e-06}, {"id": 80, "seek": 38832, "start": 393.24, "end": 398.4, "text": " a biological neuron is sending a high output value or sending many electrical impulses", "tokens": [50364, 440, 1433, 2430, 763, 1487, 490, 13910, 22027, 11, 293, 309, 14942, 281, 264, 4314, 300, 50610, 50610, 257, 13910, 34090, 307, 7750, 257, 1090, 5598, 2158, 420, 7750, 867, 12147, 41767, 6196, 50868, 50868, 281, 661, 22027, 11, 281, 264, 30621, 490, 309, 13, 51016, 51016, 400, 370, 613, 3547, 322, 6157, 2310, 11, 8888, 11, 293, 19049, 3125, 366, 264, 2430, 763, 51284, 51284, 295, 613, 1045, 22027, 294, 341, 4583, 13, 51454, 51454, 400, 611, 11, 341, 5598, 8482, 307, 264, 24433, 295, 341, 34090, 4898, 510, 322, 264, 558, 13, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1380855112659688, "compression_ratio": 1.8559322033898304, "no_speech_prob": 1.6027604488044744e-06}, {"id": 81, "seek": 38832, "start": 398.4, "end": 401.36, "text": " to other neurons, to the downstream from it.", "tokens": [50364, 440, 1433, 2430, 763, 1487, 490, 13910, 22027, 11, 293, 309, 14942, 281, 264, 4314, 300, 50610, 50610, 257, 13910, 34090, 307, 7750, 257, 1090, 5598, 2158, 420, 7750, 867, 12147, 41767, 6196, 50868, 50868, 281, 661, 22027, 11, 281, 264, 30621, 490, 309, 13, 51016, 51016, 400, 370, 613, 3547, 322, 6157, 2310, 11, 8888, 11, 293, 19049, 3125, 366, 264, 2430, 763, 51284, 51284, 295, 613, 1045, 22027, 294, 341, 4583, 13, 51454, 51454, 400, 611, 11, 341, 5598, 8482, 307, 264, 24433, 295, 341, 34090, 4898, 510, 322, 264, 558, 13, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1380855112659688, "compression_ratio": 1.8559322033898304, "no_speech_prob": 1.6027604488044744e-06}, {"id": 82, "seek": 38832, "start": 401.36, "end": 406.71999999999997, "text": " And so these numbers on affordability, awareness, and perceived quality are the activations", "tokens": [50364, 440, 1433, 2430, 763, 1487, 490, 13910, 22027, 11, 293, 309, 14942, 281, 264, 4314, 300, 50610, 50610, 257, 13910, 34090, 307, 7750, 257, 1090, 5598, 2158, 420, 7750, 867, 12147, 41767, 6196, 50868, 50868, 281, 661, 22027, 11, 281, 264, 30621, 490, 309, 13, 51016, 51016, 400, 370, 613, 3547, 322, 6157, 2310, 11, 8888, 11, 293, 19049, 3125, 366, 264, 2430, 763, 51284, 51284, 295, 613, 1045, 22027, 294, 341, 4583, 13, 51454, 51454, 400, 611, 11, 341, 5598, 8482, 307, 264, 24433, 295, 341, 34090, 4898, 510, 322, 264, 558, 13, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1380855112659688, "compression_ratio": 1.8559322033898304, "no_speech_prob": 1.6027604488044744e-06}, {"id": 83, "seek": 38832, "start": 406.71999999999997, "end": 410.12, "text": " of these three neurons in this layer.", "tokens": [50364, 440, 1433, 2430, 763, 1487, 490, 13910, 22027, 11, 293, 309, 14942, 281, 264, 4314, 300, 50610, 50610, 257, 13910, 34090, 307, 7750, 257, 1090, 5598, 2158, 420, 7750, 867, 12147, 41767, 6196, 50868, 50868, 281, 661, 22027, 11, 281, 264, 30621, 490, 309, 13, 51016, 51016, 400, 370, 613, 3547, 322, 6157, 2310, 11, 8888, 11, 293, 19049, 3125, 366, 264, 2430, 763, 51284, 51284, 295, 613, 1045, 22027, 294, 341, 4583, 13, 51454, 51454, 400, 611, 11, 341, 5598, 8482, 307, 264, 24433, 295, 341, 34090, 4898, 510, 322, 264, 558, 13, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1380855112659688, "compression_ratio": 1.8559322033898304, "no_speech_prob": 1.6027604488044744e-06}, {"id": 84, "seek": 38832, "start": 410.12, "end": 418.03999999999996, "text": " And also, this output probability is the activation of this neuron shown here on the right.", "tokens": [50364, 440, 1433, 2430, 763, 1487, 490, 13910, 22027, 11, 293, 309, 14942, 281, 264, 4314, 300, 50610, 50610, 257, 13910, 34090, 307, 7750, 257, 1090, 5598, 2158, 420, 7750, 867, 12147, 41767, 6196, 50868, 50868, 281, 661, 22027, 11, 281, 264, 30621, 490, 309, 13, 51016, 51016, 400, 370, 613, 3547, 322, 6157, 2310, 11, 8888, 11, 293, 19049, 3125, 366, 264, 2430, 763, 51284, 51284, 295, 613, 1045, 22027, 294, 341, 4583, 13, 51454, 51454, 400, 611, 11, 341, 5598, 8482, 307, 264, 24433, 295, 341, 34090, 4898, 510, 322, 264, 558, 13, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1380855112659688, "compression_ratio": 1.8559322033898304, "no_speech_prob": 1.6027604488044744e-06}, {"id": 85, "seek": 41804, "start": 418.04, "end": 423.88, "text": " So, this particular neural network, therefore, carries out computations as follows.", "tokens": [50364, 407, 11, 341, 1729, 18161, 3209, 11, 4412, 11, 16402, 484, 2807, 763, 382, 10002, 13, 50656, 50656, 467, 15743, 1451, 3547, 11, 550, 341, 4583, 295, 264, 18161, 3209, 4960, 729, 1451, 3547, 50924, 50924, 281, 14722, 1045, 777, 3547, 11, 611, 1219, 24433, 4190, 11, 293, 550, 264, 2572, 4583, 11, 51244, 51244, 264, 5598, 4583, 295, 264, 18161, 3209, 11, 4960, 729, 1045, 3547, 281, 14722, 472, 1230, 13, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.10247194766998291, "compression_ratio": 1.9770114942528736, "no_speech_prob": 1.7880254290503217e-06}, {"id": 86, "seek": 41804, "start": 423.88, "end": 429.24, "text": " It inputs four numbers, then this layer of the neural network uses those four numbers", "tokens": [50364, 407, 11, 341, 1729, 18161, 3209, 11, 4412, 11, 16402, 484, 2807, 763, 382, 10002, 13, 50656, 50656, 467, 15743, 1451, 3547, 11, 550, 341, 4583, 295, 264, 18161, 3209, 4960, 729, 1451, 3547, 50924, 50924, 281, 14722, 1045, 777, 3547, 11, 611, 1219, 24433, 4190, 11, 293, 550, 264, 2572, 4583, 11, 51244, 51244, 264, 5598, 4583, 295, 264, 18161, 3209, 11, 4960, 729, 1045, 3547, 281, 14722, 472, 1230, 13, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.10247194766998291, "compression_ratio": 1.9770114942528736, "no_speech_prob": 1.7880254290503217e-06}, {"id": 87, "seek": 41804, "start": 429.24, "end": 435.64000000000004, "text": " to compute three new numbers, also called activation values, and then the final layer,", "tokens": [50364, 407, 11, 341, 1729, 18161, 3209, 11, 4412, 11, 16402, 484, 2807, 763, 382, 10002, 13, 50656, 50656, 467, 15743, 1451, 3547, 11, 550, 341, 4583, 295, 264, 18161, 3209, 4960, 729, 1451, 3547, 50924, 50924, 281, 14722, 1045, 777, 3547, 11, 611, 1219, 24433, 4190, 11, 293, 550, 264, 2572, 4583, 11, 51244, 51244, 264, 5598, 4583, 295, 264, 18161, 3209, 11, 4960, 729, 1045, 3547, 281, 14722, 472, 1230, 13, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.10247194766998291, "compression_ratio": 1.9770114942528736, "no_speech_prob": 1.7880254290503217e-06}, {"id": 88, "seek": 41804, "start": 435.64000000000004, "end": 442.24, "text": " the output layer of the neural network, uses those three numbers to compute one number.", "tokens": [50364, 407, 11, 341, 1729, 18161, 3209, 11, 4412, 11, 16402, 484, 2807, 763, 382, 10002, 13, 50656, 50656, 467, 15743, 1451, 3547, 11, 550, 341, 4583, 295, 264, 18161, 3209, 4960, 729, 1451, 3547, 50924, 50924, 281, 14722, 1045, 777, 3547, 11, 611, 1219, 24433, 4190, 11, 293, 550, 264, 2572, 4583, 11, 51244, 51244, 264, 5598, 4583, 295, 264, 18161, 3209, 11, 4960, 729, 1045, 3547, 281, 14722, 472, 1230, 13, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.10247194766998291, "compression_ratio": 1.9770114942528736, "no_speech_prob": 1.7880254290503217e-06}, {"id": 89, "seek": 44224, "start": 442.24, "end": 450.84000000000003, "text": " And in a neural network, this list of four numbers is also called the input layer, and", "tokens": [50364, 400, 294, 257, 18161, 3209, 11, 341, 1329, 295, 1451, 3547, 307, 611, 1219, 264, 4846, 4583, 11, 293, 50794, 50794, 300, 311, 445, 257, 1329, 295, 1451, 3547, 13, 50902, 50902, 823, 11, 456, 311, 472, 6883, 3774, 286, 1116, 411, 281, 652, 281, 341, 18161, 3209, 11, 597, 307, 264, 51248, 51248, 636, 286, 600, 7619, 309, 370, 1400, 11, 321, 632, 281, 352, 807, 264, 22027, 472, 412, 257, 565, 293, 4536, 51506, 51506, 437, 15743, 309, 576, 747, 490, 264, 3894, 4583, 13, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.12497588566371373, "compression_ratio": 1.6602870813397128, "no_speech_prob": 2.9943948902655393e-06}, {"id": 90, "seek": 44224, "start": 450.84000000000003, "end": 453.0, "text": " that's just a list of four numbers.", "tokens": [50364, 400, 294, 257, 18161, 3209, 11, 341, 1329, 295, 1451, 3547, 307, 611, 1219, 264, 4846, 4583, 11, 293, 50794, 50794, 300, 311, 445, 257, 1329, 295, 1451, 3547, 13, 50902, 50902, 823, 11, 456, 311, 472, 6883, 3774, 286, 1116, 411, 281, 652, 281, 341, 18161, 3209, 11, 597, 307, 264, 51248, 51248, 636, 286, 600, 7619, 309, 370, 1400, 11, 321, 632, 281, 352, 807, 264, 22027, 472, 412, 257, 565, 293, 4536, 51506, 51506, 437, 15743, 309, 576, 747, 490, 264, 3894, 4583, 13, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.12497588566371373, "compression_ratio": 1.6602870813397128, "no_speech_prob": 2.9943948902655393e-06}, {"id": 91, "seek": 44224, "start": 453.0, "end": 459.92, "text": " Now, there's one simplification I'd like to make to this neural network, which is the", "tokens": [50364, 400, 294, 257, 18161, 3209, 11, 341, 1329, 295, 1451, 3547, 307, 611, 1219, 264, 4846, 4583, 11, 293, 50794, 50794, 300, 311, 445, 257, 1329, 295, 1451, 3547, 13, 50902, 50902, 823, 11, 456, 311, 472, 6883, 3774, 286, 1116, 411, 281, 652, 281, 341, 18161, 3209, 11, 597, 307, 264, 51248, 51248, 636, 286, 600, 7619, 309, 370, 1400, 11, 321, 632, 281, 352, 807, 264, 22027, 472, 412, 257, 565, 293, 4536, 51506, 51506, 437, 15743, 309, 576, 747, 490, 264, 3894, 4583, 13, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.12497588566371373, "compression_ratio": 1.6602870813397128, "no_speech_prob": 2.9943948902655393e-06}, {"id": 92, "seek": 44224, "start": 459.92, "end": 465.08, "text": " way I've described it so far, we had to go through the neurons one at a time and decide", "tokens": [50364, 400, 294, 257, 18161, 3209, 11, 341, 1329, 295, 1451, 3547, 307, 611, 1219, 264, 4846, 4583, 11, 293, 50794, 50794, 300, 311, 445, 257, 1329, 295, 1451, 3547, 13, 50902, 50902, 823, 11, 456, 311, 472, 6883, 3774, 286, 1116, 411, 281, 652, 281, 341, 18161, 3209, 11, 597, 307, 264, 51248, 51248, 636, 286, 600, 7619, 309, 370, 1400, 11, 321, 632, 281, 352, 807, 264, 22027, 472, 412, 257, 565, 293, 4536, 51506, 51506, 437, 15743, 309, 576, 747, 490, 264, 3894, 4583, 13, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.12497588566371373, "compression_ratio": 1.6602870813397128, "no_speech_prob": 2.9943948902655393e-06}, {"id": 93, "seek": 44224, "start": 465.08, "end": 468.40000000000003, "text": " what inputs it would take from the previous layer.", "tokens": [50364, 400, 294, 257, 18161, 3209, 11, 341, 1329, 295, 1451, 3547, 307, 611, 1219, 264, 4846, 4583, 11, 293, 50794, 50794, 300, 311, 445, 257, 1329, 295, 1451, 3547, 13, 50902, 50902, 823, 11, 456, 311, 472, 6883, 3774, 286, 1116, 411, 281, 652, 281, 341, 18161, 3209, 11, 597, 307, 264, 51248, 51248, 636, 286, 600, 7619, 309, 370, 1400, 11, 321, 632, 281, 352, 807, 264, 22027, 472, 412, 257, 565, 293, 4536, 51506, 51506, 437, 15743, 309, 576, 747, 490, 264, 3894, 4583, 13, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.12497588566371373, "compression_ratio": 1.6602870813397128, "no_speech_prob": 2.9943948902655393e-06}, {"id": 94, "seek": 46840, "start": 468.4, "end": 473.08, "text": " So for example, we said affordability is a function of just price and shipping costs", "tokens": [50364, 407, 337, 1365, 11, 321, 848, 6157, 2310, 307, 257, 2445, 295, 445, 3218, 293, 14122, 5497, 50598, 50598, 293, 8888, 307, 257, 2445, 295, 445, 6370, 293, 370, 322, 13, 50794, 50794, 583, 498, 291, 434, 2390, 257, 2416, 18161, 3209, 11, 309, 1116, 312, 257, 688, 295, 589, 281, 352, 807, 293, 16945, 50998, 50998, 4536, 597, 22027, 820, 747, 597, 4122, 382, 15743, 13, 51240, 51240, 440, 636, 257, 18161, 3209, 307, 12270, 294, 3124, 11, 1184, 34090, 294, 257, 1629, 4583, 11, 51510, 51510, 584, 341, 4583, 294, 264, 2808, 11, 486, 362, 2105, 281, 633, 4111, 11, 281, 633, 2158, 490, 264, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.08432253416594085, "compression_ratio": 1.7216117216117217, "no_speech_prob": 2.5612077934056288e-06}, {"id": 95, "seek": 46840, "start": 473.08, "end": 477.0, "text": " and awareness is a function of just marketing and so on.", "tokens": [50364, 407, 337, 1365, 11, 321, 848, 6157, 2310, 307, 257, 2445, 295, 445, 3218, 293, 14122, 5497, 50598, 50598, 293, 8888, 307, 257, 2445, 295, 445, 6370, 293, 370, 322, 13, 50794, 50794, 583, 498, 291, 434, 2390, 257, 2416, 18161, 3209, 11, 309, 1116, 312, 257, 688, 295, 589, 281, 352, 807, 293, 16945, 50998, 50998, 4536, 597, 22027, 820, 747, 597, 4122, 382, 15743, 13, 51240, 51240, 440, 636, 257, 18161, 3209, 307, 12270, 294, 3124, 11, 1184, 34090, 294, 257, 1629, 4583, 11, 51510, 51510, 584, 341, 4583, 294, 264, 2808, 11, 486, 362, 2105, 281, 633, 4111, 11, 281, 633, 2158, 490, 264, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.08432253416594085, "compression_ratio": 1.7216117216117217, "no_speech_prob": 2.5612077934056288e-06}, {"id": 96, "seek": 46840, "start": 477.0, "end": 481.08, "text": " But if you're building a large neural network, it'd be a lot of work to go through and manually", "tokens": [50364, 407, 337, 1365, 11, 321, 848, 6157, 2310, 307, 257, 2445, 295, 445, 3218, 293, 14122, 5497, 50598, 50598, 293, 8888, 307, 257, 2445, 295, 445, 6370, 293, 370, 322, 13, 50794, 50794, 583, 498, 291, 434, 2390, 257, 2416, 18161, 3209, 11, 309, 1116, 312, 257, 688, 295, 589, 281, 352, 807, 293, 16945, 50998, 50998, 4536, 597, 22027, 820, 747, 597, 4122, 382, 15743, 13, 51240, 51240, 440, 636, 257, 18161, 3209, 307, 12270, 294, 3124, 11, 1184, 34090, 294, 257, 1629, 4583, 11, 51510, 51510, 584, 341, 4583, 294, 264, 2808, 11, 486, 362, 2105, 281, 633, 4111, 11, 281, 633, 2158, 490, 264, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.08432253416594085, "compression_ratio": 1.7216117216117217, "no_speech_prob": 2.5612077934056288e-06}, {"id": 97, "seek": 46840, "start": 481.08, "end": 485.91999999999996, "text": " decide which neurons should take which features as inputs.", "tokens": [50364, 407, 337, 1365, 11, 321, 848, 6157, 2310, 307, 257, 2445, 295, 445, 3218, 293, 14122, 5497, 50598, 50598, 293, 8888, 307, 257, 2445, 295, 445, 6370, 293, 370, 322, 13, 50794, 50794, 583, 498, 291, 434, 2390, 257, 2416, 18161, 3209, 11, 309, 1116, 312, 257, 688, 295, 589, 281, 352, 807, 293, 16945, 50998, 50998, 4536, 597, 22027, 820, 747, 597, 4122, 382, 15743, 13, 51240, 51240, 440, 636, 257, 18161, 3209, 307, 12270, 294, 3124, 11, 1184, 34090, 294, 257, 1629, 4583, 11, 51510, 51510, 584, 341, 4583, 294, 264, 2808, 11, 486, 362, 2105, 281, 633, 4111, 11, 281, 633, 2158, 490, 264, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.08432253416594085, "compression_ratio": 1.7216117216117217, "no_speech_prob": 2.5612077934056288e-06}, {"id": 98, "seek": 46840, "start": 485.91999999999996, "end": 491.32, "text": " The way a neural network is implemented in practice, each neuron in a certain layer,", "tokens": [50364, 407, 337, 1365, 11, 321, 848, 6157, 2310, 307, 257, 2445, 295, 445, 3218, 293, 14122, 5497, 50598, 50598, 293, 8888, 307, 257, 2445, 295, 445, 6370, 293, 370, 322, 13, 50794, 50794, 583, 498, 291, 434, 2390, 257, 2416, 18161, 3209, 11, 309, 1116, 312, 257, 688, 295, 589, 281, 352, 807, 293, 16945, 50998, 50998, 4536, 597, 22027, 820, 747, 597, 4122, 382, 15743, 13, 51240, 51240, 440, 636, 257, 18161, 3209, 307, 12270, 294, 3124, 11, 1184, 34090, 294, 257, 1629, 4583, 11, 51510, 51510, 584, 341, 4583, 294, 264, 2808, 11, 486, 362, 2105, 281, 633, 4111, 11, 281, 633, 2158, 490, 264, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.08432253416594085, "compression_ratio": 1.7216117216117217, "no_speech_prob": 2.5612077934056288e-06}, {"id": 99, "seek": 46840, "start": 491.32, "end": 498.15999999999997, "text": " say this layer in the middle, will have access to every feature, to every value from the", "tokens": [50364, 407, 337, 1365, 11, 321, 848, 6157, 2310, 307, 257, 2445, 295, 445, 3218, 293, 14122, 5497, 50598, 50598, 293, 8888, 307, 257, 2445, 295, 445, 6370, 293, 370, 322, 13, 50794, 50794, 583, 498, 291, 434, 2390, 257, 2416, 18161, 3209, 11, 309, 1116, 312, 257, 688, 295, 589, 281, 352, 807, 293, 16945, 50998, 50998, 4536, 597, 22027, 820, 747, 597, 4122, 382, 15743, 13, 51240, 51240, 440, 636, 257, 18161, 3209, 307, 12270, 294, 3124, 11, 1184, 34090, 294, 257, 1629, 4583, 11, 51510, 51510, 584, 341, 4583, 294, 264, 2808, 11, 486, 362, 2105, 281, 633, 4111, 11, 281, 633, 2158, 490, 264, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.08432253416594085, "compression_ratio": 1.7216117216117217, "no_speech_prob": 2.5612077934056288e-06}, {"id": 100, "seek": 49816, "start": 498.16, "end": 505.16, "text": " previous layer, from the input layer, which is why I'm now drawing arrows from every input", "tokens": [50364, 3894, 4583, 11, 490, 264, 4846, 4583, 11, 597, 307, 983, 286, 478, 586, 6316, 19669, 490, 633, 4846, 50714, 50714, 4111, 281, 633, 472, 295, 613, 22027, 4898, 510, 294, 264, 2808, 13, 50965, 50965, 400, 291, 393, 3811, 300, 498, 291, 434, 1382, 281, 6069, 6157, 2310, 293, 309, 3255, 437, 51220, 51220, 264, 3218, 14122, 5497, 11, 6370, 293, 2527, 11, 1310, 291, 603, 1466, 281, 11200, 6370, 293, 51456, 51456, 2527, 293, 445, 2573, 484, 807, 3287, 264, 9834, 23505, 281, 787, 1879, 51754, 51754], "temperature": 0.0, "avg_logprob": -0.15014244959904596, "compression_ratio": 1.7004048582995952, "no_speech_prob": 2.785191099974327e-07}, {"id": 101, "seek": 49816, "start": 505.16, "end": 510.18, "text": " feature to every one of these neurons shown here in the middle.", "tokens": [50364, 3894, 4583, 11, 490, 264, 4846, 4583, 11, 597, 307, 983, 286, 478, 586, 6316, 19669, 490, 633, 4846, 50714, 50714, 4111, 281, 633, 472, 295, 613, 22027, 4898, 510, 294, 264, 2808, 13, 50965, 50965, 400, 291, 393, 3811, 300, 498, 291, 434, 1382, 281, 6069, 6157, 2310, 293, 309, 3255, 437, 51220, 51220, 264, 3218, 14122, 5497, 11, 6370, 293, 2527, 11, 1310, 291, 603, 1466, 281, 11200, 6370, 293, 51456, 51456, 2527, 293, 445, 2573, 484, 807, 3287, 264, 9834, 23505, 281, 787, 1879, 51754, 51754], "temperature": 0.0, "avg_logprob": -0.15014244959904596, "compression_ratio": 1.7004048582995952, "no_speech_prob": 2.785191099974327e-07}, {"id": 102, "seek": 49816, "start": 510.18, "end": 515.28, "text": " And you can imagine that if you're trying to predict affordability and it knows what", "tokens": [50364, 3894, 4583, 11, 490, 264, 4846, 4583, 11, 597, 307, 983, 286, 478, 586, 6316, 19669, 490, 633, 4846, 50714, 50714, 4111, 281, 633, 472, 295, 613, 22027, 4898, 510, 294, 264, 2808, 13, 50965, 50965, 400, 291, 393, 3811, 300, 498, 291, 434, 1382, 281, 6069, 6157, 2310, 293, 309, 3255, 437, 51220, 51220, 264, 3218, 14122, 5497, 11, 6370, 293, 2527, 11, 1310, 291, 603, 1466, 281, 11200, 6370, 293, 51456, 51456, 2527, 293, 445, 2573, 484, 807, 3287, 264, 9834, 23505, 281, 787, 1879, 51754, 51754], "temperature": 0.0, "avg_logprob": -0.15014244959904596, "compression_ratio": 1.7004048582995952, "no_speech_prob": 2.785191099974327e-07}, {"id": 103, "seek": 49816, "start": 515.28, "end": 520.0, "text": " the price shipping costs, marketing and material, maybe you'll learn to ignore marketing and", "tokens": [50364, 3894, 4583, 11, 490, 264, 4846, 4583, 11, 597, 307, 983, 286, 478, 586, 6316, 19669, 490, 633, 4846, 50714, 50714, 4111, 281, 633, 472, 295, 613, 22027, 4898, 510, 294, 264, 2808, 13, 50965, 50965, 400, 291, 393, 3811, 300, 498, 291, 434, 1382, 281, 6069, 6157, 2310, 293, 309, 3255, 437, 51220, 51220, 264, 3218, 14122, 5497, 11, 6370, 293, 2527, 11, 1310, 291, 603, 1466, 281, 11200, 6370, 293, 51456, 51456, 2527, 293, 445, 2573, 484, 807, 3287, 264, 9834, 23505, 281, 787, 1879, 51754, 51754], "temperature": 0.0, "avg_logprob": -0.15014244959904596, "compression_ratio": 1.7004048582995952, "no_speech_prob": 2.785191099974327e-07}, {"id": 104, "seek": 49816, "start": 520.0, "end": 525.96, "text": " material and just figure out through setting the parameters appropriately to only focus", "tokens": [50364, 3894, 4583, 11, 490, 264, 4846, 4583, 11, 597, 307, 983, 286, 478, 586, 6316, 19669, 490, 633, 4846, 50714, 50714, 4111, 281, 633, 472, 295, 613, 22027, 4898, 510, 294, 264, 2808, 13, 50965, 50965, 400, 291, 393, 3811, 300, 498, 291, 434, 1382, 281, 6069, 6157, 2310, 293, 309, 3255, 437, 51220, 51220, 264, 3218, 14122, 5497, 11, 6370, 293, 2527, 11, 1310, 291, 603, 1466, 281, 11200, 6370, 293, 51456, 51456, 2527, 293, 445, 2573, 484, 807, 3287, 264, 9834, 23505, 281, 787, 1879, 51754, 51754], "temperature": 0.0, "avg_logprob": -0.15014244959904596, "compression_ratio": 1.7004048582995952, "no_speech_prob": 2.785191099974327e-07}, {"id": 105, "seek": 52596, "start": 525.96, "end": 530.9200000000001, "text": " on the subset of features that are most relevant to affordability.", "tokens": [50364, 322, 264, 25993, 295, 4122, 300, 366, 881, 7340, 281, 6157, 2310, 13, 50612, 50612, 1407, 3052, 20460, 264, 24657, 293, 264, 3855, 295, 341, 18161, 3209, 11, 286, 478, 516, 281, 747, 50920, 50920, 613, 1451, 4846, 4122, 293, 2464, 552, 382, 257, 8062, 1783, 293, 321, 434, 516, 281, 1910, 264, 18161, 51296, 51296, 3209, 382, 1419, 1451, 4122, 300, 16802, 908, 341, 4111, 8062, 1783, 293, 341, 4111, 8062, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.08882273498334382, "compression_ratio": 1.7864583333333333, "no_speech_prob": 7.071634172461927e-06}, {"id": 106, "seek": 52596, "start": 530.9200000000001, "end": 537.08, "text": " To further simplify the notation and the description of this neural network, I'm going to take", "tokens": [50364, 322, 264, 25993, 295, 4122, 300, 366, 881, 7340, 281, 6157, 2310, 13, 50612, 50612, 1407, 3052, 20460, 264, 24657, 293, 264, 3855, 295, 341, 18161, 3209, 11, 286, 478, 516, 281, 747, 50920, 50920, 613, 1451, 4846, 4122, 293, 2464, 552, 382, 257, 8062, 1783, 293, 321, 434, 516, 281, 1910, 264, 18161, 51296, 51296, 3209, 382, 1419, 1451, 4122, 300, 16802, 908, 341, 4111, 8062, 1783, 293, 341, 4111, 8062, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.08882273498334382, "compression_ratio": 1.7864583333333333, "no_speech_prob": 7.071634172461927e-06}, {"id": 107, "seek": 52596, "start": 537.08, "end": 544.6, "text": " these four input features and write them as a vector X and we're going to view the neural", "tokens": [50364, 322, 264, 25993, 295, 4122, 300, 366, 881, 7340, 281, 6157, 2310, 13, 50612, 50612, 1407, 3052, 20460, 264, 24657, 293, 264, 3855, 295, 341, 18161, 3209, 11, 286, 478, 516, 281, 747, 50920, 50920, 613, 1451, 4846, 4122, 293, 2464, 552, 382, 257, 8062, 1783, 293, 321, 434, 516, 281, 1910, 264, 18161, 51296, 51296, 3209, 382, 1419, 1451, 4122, 300, 16802, 908, 341, 4111, 8062, 1783, 293, 341, 4111, 8062, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.08882273498334382, "compression_ratio": 1.7864583333333333, "no_speech_prob": 7.071634172461927e-06}, {"id": 108, "seek": 52596, "start": 544.6, "end": 552.6800000000001, "text": " network as having four features that comprise this feature vector X and this feature vector", "tokens": [50364, 322, 264, 25993, 295, 4122, 300, 366, 881, 7340, 281, 6157, 2310, 13, 50612, 50612, 1407, 3052, 20460, 264, 24657, 293, 264, 3855, 295, 341, 18161, 3209, 11, 286, 478, 516, 281, 747, 50920, 50920, 613, 1451, 4846, 4122, 293, 2464, 552, 382, 257, 8062, 1783, 293, 321, 434, 516, 281, 1910, 264, 18161, 51296, 51296, 3209, 382, 1419, 1451, 4122, 300, 16802, 908, 341, 4111, 8062, 1783, 293, 341, 4111, 8062, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.08882273498334382, "compression_ratio": 1.7864583333333333, "no_speech_prob": 7.071634172461927e-06}, {"id": 109, "seek": 55268, "start": 552.68, "end": 559.52, "text": " is fed to this layer in the middle, which then computes three activation values, that", "tokens": [50364, 307, 4636, 281, 341, 4583, 294, 264, 2808, 11, 597, 550, 715, 1819, 1045, 24433, 4190, 11, 300, 50706, 50706, 307, 613, 1045, 3547, 11, 293, 613, 1045, 24433, 4190, 294, 1261, 3643, 1071, 8062, 11, 597, 51128, 51128, 307, 4636, 281, 341, 2572, 5598, 4583, 300, 2721, 23930, 264, 8482, 295, 341, 256, 12, 15313, 885, 51540, 51540, 257, 1192, 23600, 13, 51632, 51632, 407, 300, 311, 439, 257, 18161, 3209, 307, 13, 51747, 51747], "temperature": 0.0, "avg_logprob": -0.13542846532968375, "compression_ratio": 1.7365591397849462, "no_speech_prob": 4.710764642368304e-06}, {"id": 110, "seek": 55268, "start": 559.52, "end": 567.9599999999999, "text": " is these three numbers, and these three activation values in turn becomes another vector, which", "tokens": [50364, 307, 4636, 281, 341, 4583, 294, 264, 2808, 11, 597, 550, 715, 1819, 1045, 24433, 4190, 11, 300, 50706, 50706, 307, 613, 1045, 3547, 11, 293, 613, 1045, 24433, 4190, 294, 1261, 3643, 1071, 8062, 11, 597, 51128, 51128, 307, 4636, 281, 341, 2572, 5598, 4583, 300, 2721, 23930, 264, 8482, 295, 341, 256, 12, 15313, 885, 51540, 51540, 257, 1192, 23600, 13, 51632, 51632, 407, 300, 311, 439, 257, 18161, 3209, 307, 13, 51747, 51747], "temperature": 0.0, "avg_logprob": -0.13542846532968375, "compression_ratio": 1.7365591397849462, "no_speech_prob": 4.710764642368304e-06}, {"id": 111, "seek": 55268, "start": 567.9599999999999, "end": 576.1999999999999, "text": " is fed to this final output layer that finally outputs the probability of this t-shirt being", "tokens": [50364, 307, 4636, 281, 341, 4583, 294, 264, 2808, 11, 597, 550, 715, 1819, 1045, 24433, 4190, 11, 300, 50706, 50706, 307, 613, 1045, 3547, 11, 293, 613, 1045, 24433, 4190, 294, 1261, 3643, 1071, 8062, 11, 597, 51128, 51128, 307, 4636, 281, 341, 2572, 5598, 4583, 300, 2721, 23930, 264, 8482, 295, 341, 256, 12, 15313, 885, 51540, 51540, 257, 1192, 23600, 13, 51632, 51632, 407, 300, 311, 439, 257, 18161, 3209, 307, 13, 51747, 51747], "temperature": 0.0, "avg_logprob": -0.13542846532968375, "compression_ratio": 1.7365591397849462, "no_speech_prob": 4.710764642368304e-06}, {"id": 112, "seek": 55268, "start": 576.1999999999999, "end": 578.04, "text": " a top seller.", "tokens": [50364, 307, 4636, 281, 341, 4583, 294, 264, 2808, 11, 597, 550, 715, 1819, 1045, 24433, 4190, 11, 300, 50706, 50706, 307, 613, 1045, 3547, 11, 293, 613, 1045, 24433, 4190, 294, 1261, 3643, 1071, 8062, 11, 597, 51128, 51128, 307, 4636, 281, 341, 2572, 5598, 4583, 300, 2721, 23930, 264, 8482, 295, 341, 256, 12, 15313, 885, 51540, 51540, 257, 1192, 23600, 13, 51632, 51632, 407, 300, 311, 439, 257, 18161, 3209, 307, 13, 51747, 51747], "temperature": 0.0, "avg_logprob": -0.13542846532968375, "compression_ratio": 1.7365591397849462, "no_speech_prob": 4.710764642368304e-06}, {"id": 113, "seek": 55268, "start": 578.04, "end": 580.3399999999999, "text": " So that's all a neural network is.", "tokens": [50364, 307, 4636, 281, 341, 4583, 294, 264, 2808, 11, 597, 550, 715, 1819, 1045, 24433, 4190, 11, 300, 50706, 50706, 307, 613, 1045, 3547, 11, 293, 613, 1045, 24433, 4190, 294, 1261, 3643, 1071, 8062, 11, 597, 51128, 51128, 307, 4636, 281, 341, 2572, 5598, 4583, 300, 2721, 23930, 264, 8482, 295, 341, 256, 12, 15313, 885, 51540, 51540, 257, 1192, 23600, 13, 51632, 51632, 407, 300, 311, 439, 257, 18161, 3209, 307, 13, 51747, 51747], "temperature": 0.0, "avg_logprob": -0.13542846532968375, "compression_ratio": 1.7365591397849462, "no_speech_prob": 4.710764642368304e-06}, {"id": 114, "seek": 58034, "start": 580.34, "end": 588.5600000000001, "text": " It has a few layers where each layer inputs a vector and outputs another vector of numbers,", "tokens": [50364, 467, 575, 257, 1326, 7914, 689, 1184, 4583, 15743, 257, 8062, 293, 23930, 1071, 8062, 295, 3547, 11, 50775, 50775, 689, 337, 1365, 11, 341, 4583, 294, 264, 2808, 15743, 1451, 3547, 1783, 293, 23930, 1045, 3547, 51104, 51104, 11760, 281, 6157, 2310, 11, 8888, 293, 19049, 3125, 13, 51291, 51291, 1407, 909, 257, 707, 857, 544, 27575, 11, 291, 600, 1612, 300, 341, 4583, 307, 1219, 264, 5598, 51631, 51631], "temperature": 0.0, "avg_logprob": -0.139087585553731, "compression_ratio": 1.6919191919191918, "no_speech_prob": 4.22269886257709e-06}, {"id": 115, "seek": 58034, "start": 588.5600000000001, "end": 595.14, "text": " where for example, this layer in the middle inputs four numbers X and outputs three numbers", "tokens": [50364, 467, 575, 257, 1326, 7914, 689, 1184, 4583, 15743, 257, 8062, 293, 23930, 1071, 8062, 295, 3547, 11, 50775, 50775, 689, 337, 1365, 11, 341, 4583, 294, 264, 2808, 15743, 1451, 3547, 1783, 293, 23930, 1045, 3547, 51104, 51104, 11760, 281, 6157, 2310, 11, 8888, 293, 19049, 3125, 13, 51291, 51291, 1407, 909, 257, 707, 857, 544, 27575, 11, 291, 600, 1612, 300, 341, 4583, 307, 1219, 264, 5598, 51631, 51631], "temperature": 0.0, "avg_logprob": -0.139087585553731, "compression_ratio": 1.6919191919191918, "no_speech_prob": 4.22269886257709e-06}, {"id": 116, "seek": 58034, "start": 595.14, "end": 598.88, "text": " corresponding to affordability, awareness and perceived quality.", "tokens": [50364, 467, 575, 257, 1326, 7914, 689, 1184, 4583, 15743, 257, 8062, 293, 23930, 1071, 8062, 295, 3547, 11, 50775, 50775, 689, 337, 1365, 11, 341, 4583, 294, 264, 2808, 15743, 1451, 3547, 1783, 293, 23930, 1045, 3547, 51104, 51104, 11760, 281, 6157, 2310, 11, 8888, 293, 19049, 3125, 13, 51291, 51291, 1407, 909, 257, 707, 857, 544, 27575, 11, 291, 600, 1612, 300, 341, 4583, 307, 1219, 264, 5598, 51631, 51631], "temperature": 0.0, "avg_logprob": -0.139087585553731, "compression_ratio": 1.6919191919191918, "no_speech_prob": 4.22269886257709e-06}, {"id": 117, "seek": 58034, "start": 598.88, "end": 605.6800000000001, "text": " To add a little bit more terminology, you've seen that this layer is called the output", "tokens": [50364, 467, 575, 257, 1326, 7914, 689, 1184, 4583, 15743, 257, 8062, 293, 23930, 1071, 8062, 295, 3547, 11, 50775, 50775, 689, 337, 1365, 11, 341, 4583, 294, 264, 2808, 15743, 1451, 3547, 1783, 293, 23930, 1045, 3547, 51104, 51104, 11760, 281, 6157, 2310, 11, 8888, 293, 19049, 3125, 13, 51291, 51291, 1407, 909, 257, 707, 857, 544, 27575, 11, 291, 600, 1612, 300, 341, 4583, 307, 1219, 264, 5598, 51631, 51631], "temperature": 0.0, "avg_logprob": -0.139087585553731, "compression_ratio": 1.6919191919191918, "no_speech_prob": 4.22269886257709e-06}, {"id": 118, "seek": 60568, "start": 605.68, "end": 610.0799999999999, "text": " layer and this layer is called the input layer.", "tokens": [50364, 4583, 293, 341, 4583, 307, 1219, 264, 4846, 4583, 13, 50584, 50584, 1407, 976, 264, 4583, 294, 264, 2808, 257, 1315, 382, 731, 11, 341, 4583, 294, 264, 2808, 307, 1219, 257, 50812, 50812, 7633, 4583, 13, 50918, 50918, 286, 458, 300, 341, 307, 1310, 406, 264, 1151, 420, 264, 881, 21769, 1315, 11, 457, 300, 27575, 51192, 51192, 1487, 490, 300, 562, 291, 362, 257, 3097, 992, 11, 294, 264, 3097, 992, 11, 291, 483, 281, 11441, 1293, 51504, 51504, 1783, 293, 398, 11, 428, 1412, 992, 5112, 291, 437, 307, 1783, 293, 437, 307, 398, 11, 293, 370, 291, 483, 1412, 300, 5112, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.12122271476535622, "compression_ratio": 1.9255813953488372, "no_speech_prob": 4.784958946402185e-06}, {"id": 119, "seek": 60568, "start": 610.0799999999999, "end": 614.64, "text": " To give the layer in the middle a name as well, this layer in the middle is called a", "tokens": [50364, 4583, 293, 341, 4583, 307, 1219, 264, 4846, 4583, 13, 50584, 50584, 1407, 976, 264, 4583, 294, 264, 2808, 257, 1315, 382, 731, 11, 341, 4583, 294, 264, 2808, 307, 1219, 257, 50812, 50812, 7633, 4583, 13, 50918, 50918, 286, 458, 300, 341, 307, 1310, 406, 264, 1151, 420, 264, 881, 21769, 1315, 11, 457, 300, 27575, 51192, 51192, 1487, 490, 300, 562, 291, 362, 257, 3097, 992, 11, 294, 264, 3097, 992, 11, 291, 483, 281, 11441, 1293, 51504, 51504, 1783, 293, 398, 11, 428, 1412, 992, 5112, 291, 437, 307, 1783, 293, 437, 307, 398, 11, 293, 370, 291, 483, 1412, 300, 5112, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.12122271476535622, "compression_ratio": 1.9255813953488372, "no_speech_prob": 4.784958946402185e-06}, {"id": 120, "seek": 60568, "start": 614.64, "end": 616.76, "text": " hidden layer.", "tokens": [50364, 4583, 293, 341, 4583, 307, 1219, 264, 4846, 4583, 13, 50584, 50584, 1407, 976, 264, 4583, 294, 264, 2808, 257, 1315, 382, 731, 11, 341, 4583, 294, 264, 2808, 307, 1219, 257, 50812, 50812, 7633, 4583, 13, 50918, 50918, 286, 458, 300, 341, 307, 1310, 406, 264, 1151, 420, 264, 881, 21769, 1315, 11, 457, 300, 27575, 51192, 51192, 1487, 490, 300, 562, 291, 362, 257, 3097, 992, 11, 294, 264, 3097, 992, 11, 291, 483, 281, 11441, 1293, 51504, 51504, 1783, 293, 398, 11, 428, 1412, 992, 5112, 291, 437, 307, 1783, 293, 437, 307, 398, 11, 293, 370, 291, 483, 1412, 300, 5112, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.12122271476535622, "compression_ratio": 1.9255813953488372, "no_speech_prob": 4.784958946402185e-06}, {"id": 121, "seek": 60568, "start": 616.76, "end": 622.2399999999999, "text": " I know that this is maybe not the best or the most intuitive name, but that terminology", "tokens": [50364, 4583, 293, 341, 4583, 307, 1219, 264, 4846, 4583, 13, 50584, 50584, 1407, 976, 264, 4583, 294, 264, 2808, 257, 1315, 382, 731, 11, 341, 4583, 294, 264, 2808, 307, 1219, 257, 50812, 50812, 7633, 4583, 13, 50918, 50918, 286, 458, 300, 341, 307, 1310, 406, 264, 1151, 420, 264, 881, 21769, 1315, 11, 457, 300, 27575, 51192, 51192, 1487, 490, 300, 562, 291, 362, 257, 3097, 992, 11, 294, 264, 3097, 992, 11, 291, 483, 281, 11441, 1293, 51504, 51504, 1783, 293, 398, 11, 428, 1412, 992, 5112, 291, 437, 307, 1783, 293, 437, 307, 398, 11, 293, 370, 291, 483, 1412, 300, 5112, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.12122271476535622, "compression_ratio": 1.9255813953488372, "no_speech_prob": 4.784958946402185e-06}, {"id": 122, "seek": 60568, "start": 622.2399999999999, "end": 628.4799999999999, "text": " comes from that when you have a training set, in the training set, you get to observe both", "tokens": [50364, 4583, 293, 341, 4583, 307, 1219, 264, 4846, 4583, 13, 50584, 50584, 1407, 976, 264, 4583, 294, 264, 2808, 257, 1315, 382, 731, 11, 341, 4583, 294, 264, 2808, 307, 1219, 257, 50812, 50812, 7633, 4583, 13, 50918, 50918, 286, 458, 300, 341, 307, 1310, 406, 264, 1151, 420, 264, 881, 21769, 1315, 11, 457, 300, 27575, 51192, 51192, 1487, 490, 300, 562, 291, 362, 257, 3097, 992, 11, 294, 264, 3097, 992, 11, 291, 483, 281, 11441, 1293, 51504, 51504, 1783, 293, 398, 11, 428, 1412, 992, 5112, 291, 437, 307, 1783, 293, 437, 307, 398, 11, 293, 370, 291, 483, 1412, 300, 5112, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.12122271476535622, "compression_ratio": 1.9255813953488372, "no_speech_prob": 4.784958946402185e-06}, {"id": 123, "seek": 60568, "start": 628.4799999999999, "end": 634.92, "text": " X and Y, your data set tells you what is X and what is Y, and so you get data that tells", "tokens": [50364, 4583, 293, 341, 4583, 307, 1219, 264, 4846, 4583, 13, 50584, 50584, 1407, 976, 264, 4583, 294, 264, 2808, 257, 1315, 382, 731, 11, 341, 4583, 294, 264, 2808, 307, 1219, 257, 50812, 50812, 7633, 4583, 13, 50918, 50918, 286, 458, 300, 341, 307, 1310, 406, 264, 1151, 420, 264, 881, 21769, 1315, 11, 457, 300, 27575, 51192, 51192, 1487, 490, 300, 562, 291, 362, 257, 3097, 992, 11, 294, 264, 3097, 992, 11, 291, 483, 281, 11441, 1293, 51504, 51504, 1783, 293, 398, 11, 428, 1412, 992, 5112, 291, 437, 307, 1783, 293, 437, 307, 398, 11, 293, 370, 291, 483, 1412, 300, 5112, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.12122271476535622, "compression_ratio": 1.9255813953488372, "no_speech_prob": 4.784958946402185e-06}, {"id": 124, "seek": 63492, "start": 634.92, "end": 640.56, "text": " you what are the correct inputs and the correct outputs, but your data set doesn't tell you", "tokens": [50364, 291, 437, 366, 264, 3006, 15743, 293, 264, 3006, 23930, 11, 457, 428, 1412, 992, 1177, 380, 980, 291, 50646, 50646, 437, 366, 264, 3006, 4190, 337, 6157, 2310, 11, 8888, 293, 19049, 3125, 13, 50884, 50884, 400, 370, 264, 3006, 4190, 337, 729, 366, 7633, 13, 50988, 50988, 509, 500, 380, 536, 552, 294, 264, 3097, 992, 11, 597, 307, 983, 341, 4583, 294, 264, 2808, 307, 1219, 51201, 51201, 257, 7633, 4583, 13, 51274, 51274, 286, 1116, 411, 281, 2073, 365, 291, 1071, 636, 295, 1953, 466, 18161, 9590, 300, 286, 1352, 51532, 51532, 4420, 337, 2390, 452, 24002, 466, 309, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.10045297057540328, "compression_ratio": 1.765625, "no_speech_prob": 3.3404078294552164e-06}, {"id": 125, "seek": 63492, "start": 640.56, "end": 645.3199999999999, "text": " what are the correct values for affordability, awareness and perceived quality.", "tokens": [50364, 291, 437, 366, 264, 3006, 15743, 293, 264, 3006, 23930, 11, 457, 428, 1412, 992, 1177, 380, 980, 291, 50646, 50646, 437, 366, 264, 3006, 4190, 337, 6157, 2310, 11, 8888, 293, 19049, 3125, 13, 50884, 50884, 400, 370, 264, 3006, 4190, 337, 729, 366, 7633, 13, 50988, 50988, 509, 500, 380, 536, 552, 294, 264, 3097, 992, 11, 597, 307, 983, 341, 4583, 294, 264, 2808, 307, 1219, 51201, 51201, 257, 7633, 4583, 13, 51274, 51274, 286, 1116, 411, 281, 2073, 365, 291, 1071, 636, 295, 1953, 466, 18161, 9590, 300, 286, 1352, 51532, 51532, 4420, 337, 2390, 452, 24002, 466, 309, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.10045297057540328, "compression_ratio": 1.765625, "no_speech_prob": 3.3404078294552164e-06}, {"id": 126, "seek": 63492, "start": 645.3199999999999, "end": 647.4, "text": " And so the correct values for those are hidden.", "tokens": [50364, 291, 437, 366, 264, 3006, 15743, 293, 264, 3006, 23930, 11, 457, 428, 1412, 992, 1177, 380, 980, 291, 50646, 50646, 437, 366, 264, 3006, 4190, 337, 6157, 2310, 11, 8888, 293, 19049, 3125, 13, 50884, 50884, 400, 370, 264, 3006, 4190, 337, 729, 366, 7633, 13, 50988, 50988, 509, 500, 380, 536, 552, 294, 264, 3097, 992, 11, 597, 307, 983, 341, 4583, 294, 264, 2808, 307, 1219, 51201, 51201, 257, 7633, 4583, 13, 51274, 51274, 286, 1116, 411, 281, 2073, 365, 291, 1071, 636, 295, 1953, 466, 18161, 9590, 300, 286, 1352, 51532, 51532, 4420, 337, 2390, 452, 24002, 466, 309, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.10045297057540328, "compression_ratio": 1.765625, "no_speech_prob": 3.3404078294552164e-06}, {"id": 127, "seek": 63492, "start": 647.4, "end": 651.66, "text": " You don't see them in the training set, which is why this layer in the middle is called", "tokens": [50364, 291, 437, 366, 264, 3006, 15743, 293, 264, 3006, 23930, 11, 457, 428, 1412, 992, 1177, 380, 980, 291, 50646, 50646, 437, 366, 264, 3006, 4190, 337, 6157, 2310, 11, 8888, 293, 19049, 3125, 13, 50884, 50884, 400, 370, 264, 3006, 4190, 337, 729, 366, 7633, 13, 50988, 50988, 509, 500, 380, 536, 552, 294, 264, 3097, 992, 11, 597, 307, 983, 341, 4583, 294, 264, 2808, 307, 1219, 51201, 51201, 257, 7633, 4583, 13, 51274, 51274, 286, 1116, 411, 281, 2073, 365, 291, 1071, 636, 295, 1953, 466, 18161, 9590, 300, 286, 1352, 51532, 51532, 4420, 337, 2390, 452, 24002, 466, 309, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.10045297057540328, "compression_ratio": 1.765625, "no_speech_prob": 3.3404078294552164e-06}, {"id": 128, "seek": 63492, "start": 651.66, "end": 653.12, "text": " a hidden layer.", "tokens": [50364, 291, 437, 366, 264, 3006, 15743, 293, 264, 3006, 23930, 11, 457, 428, 1412, 992, 1177, 380, 980, 291, 50646, 50646, 437, 366, 264, 3006, 4190, 337, 6157, 2310, 11, 8888, 293, 19049, 3125, 13, 50884, 50884, 400, 370, 264, 3006, 4190, 337, 729, 366, 7633, 13, 50988, 50988, 509, 500, 380, 536, 552, 294, 264, 3097, 992, 11, 597, 307, 983, 341, 4583, 294, 264, 2808, 307, 1219, 51201, 51201, 257, 7633, 4583, 13, 51274, 51274, 286, 1116, 411, 281, 2073, 365, 291, 1071, 636, 295, 1953, 466, 18161, 9590, 300, 286, 1352, 51532, 51532, 4420, 337, 2390, 452, 24002, 466, 309, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.10045297057540328, "compression_ratio": 1.765625, "no_speech_prob": 3.3404078294552164e-06}, {"id": 129, "seek": 63492, "start": 653.12, "end": 658.28, "text": " I'd like to share with you another way of thinking about neural networks that I found", "tokens": [50364, 291, 437, 366, 264, 3006, 15743, 293, 264, 3006, 23930, 11, 457, 428, 1412, 992, 1177, 380, 980, 291, 50646, 50646, 437, 366, 264, 3006, 4190, 337, 6157, 2310, 11, 8888, 293, 19049, 3125, 13, 50884, 50884, 400, 370, 264, 3006, 4190, 337, 729, 366, 7633, 13, 50988, 50988, 509, 500, 380, 536, 552, 294, 264, 3097, 992, 11, 597, 307, 983, 341, 4583, 294, 264, 2808, 307, 1219, 51201, 51201, 257, 7633, 4583, 13, 51274, 51274, 286, 1116, 411, 281, 2073, 365, 291, 1071, 636, 295, 1953, 466, 18161, 9590, 300, 286, 1352, 51532, 51532, 4420, 337, 2390, 452, 24002, 466, 309, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.10045297057540328, "compression_ratio": 1.765625, "no_speech_prob": 3.3404078294552164e-06}, {"id": 130, "seek": 63492, "start": 658.28, "end": 661.3199999999999, "text": " useful for building my intuition about it.", "tokens": [50364, 291, 437, 366, 264, 3006, 15743, 293, 264, 3006, 23930, 11, 457, 428, 1412, 992, 1177, 380, 980, 291, 50646, 50646, 437, 366, 264, 3006, 4190, 337, 6157, 2310, 11, 8888, 293, 19049, 3125, 13, 50884, 50884, 400, 370, 264, 3006, 4190, 337, 729, 366, 7633, 13, 50988, 50988, 509, 500, 380, 536, 552, 294, 264, 3097, 992, 11, 597, 307, 983, 341, 4583, 294, 264, 2808, 307, 1219, 51201, 51201, 257, 7633, 4583, 13, 51274, 51274, 286, 1116, 411, 281, 2073, 365, 291, 1071, 636, 295, 1953, 466, 18161, 9590, 300, 286, 1352, 51532, 51532, 4420, 337, 2390, 452, 24002, 466, 309, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.10045297057540328, "compression_ratio": 1.765625, "no_speech_prob": 3.3404078294552164e-06}, {"id": 131, "seek": 66132, "start": 661.32, "end": 666.36, "text": " Just let me cover up the left half of this diagram and see what we're left with.", "tokens": [50364, 1449, 718, 385, 2060, 493, 264, 1411, 1922, 295, 341, 10686, 293, 536, 437, 321, 434, 1411, 365, 13, 50616, 50616, 708, 291, 536, 510, 307, 300, 456, 307, 257, 3565, 3142, 24590, 9284, 420, 3565, 3142, 24590, 50886, 50886, 4985, 300, 307, 1940, 382, 4846, 6157, 2310, 11, 8888, 293, 19049, 3125, 295, 257, 256, 12, 15313, 51206, 51206, 293, 1228, 613, 1045, 4122, 281, 12539, 264, 8482, 295, 264, 256, 12, 15313, 885, 257, 1192, 51478, 51478, 23600, 13, 51532, 51532, 407, 341, 307, 445, 3565, 3142, 24590, 13, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.12899804366262335, "compression_ratio": 1.7410714285714286, "no_speech_prob": 1.6963300367933698e-05}, {"id": 132, "seek": 66132, "start": 666.36, "end": 671.7600000000001, "text": " What you see here is that there is a logistic regression algorithm or logistic regression", "tokens": [50364, 1449, 718, 385, 2060, 493, 264, 1411, 1922, 295, 341, 10686, 293, 536, 437, 321, 434, 1411, 365, 13, 50616, 50616, 708, 291, 536, 510, 307, 300, 456, 307, 257, 3565, 3142, 24590, 9284, 420, 3565, 3142, 24590, 50886, 50886, 4985, 300, 307, 1940, 382, 4846, 6157, 2310, 11, 8888, 293, 19049, 3125, 295, 257, 256, 12, 15313, 51206, 51206, 293, 1228, 613, 1045, 4122, 281, 12539, 264, 8482, 295, 264, 256, 12, 15313, 885, 257, 1192, 51478, 51478, 23600, 13, 51532, 51532, 407, 341, 307, 445, 3565, 3142, 24590, 13, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.12899804366262335, "compression_ratio": 1.7410714285714286, "no_speech_prob": 1.6963300367933698e-05}, {"id": 133, "seek": 66132, "start": 671.7600000000001, "end": 678.1600000000001, "text": " unit that is taking as input affordability, awareness and perceived quality of a t-shirt", "tokens": [50364, 1449, 718, 385, 2060, 493, 264, 1411, 1922, 295, 341, 10686, 293, 536, 437, 321, 434, 1411, 365, 13, 50616, 50616, 708, 291, 536, 510, 307, 300, 456, 307, 257, 3565, 3142, 24590, 9284, 420, 3565, 3142, 24590, 50886, 50886, 4985, 300, 307, 1940, 382, 4846, 6157, 2310, 11, 8888, 293, 19049, 3125, 295, 257, 256, 12, 15313, 51206, 51206, 293, 1228, 613, 1045, 4122, 281, 12539, 264, 8482, 295, 264, 256, 12, 15313, 885, 257, 1192, 51478, 51478, 23600, 13, 51532, 51532, 407, 341, 307, 445, 3565, 3142, 24590, 13, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.12899804366262335, "compression_ratio": 1.7410714285714286, "no_speech_prob": 1.6963300367933698e-05}, {"id": 134, "seek": 66132, "start": 678.1600000000001, "end": 683.6, "text": " and using these three features to estimate the probability of the t-shirt being a top", "tokens": [50364, 1449, 718, 385, 2060, 493, 264, 1411, 1922, 295, 341, 10686, 293, 536, 437, 321, 434, 1411, 365, 13, 50616, 50616, 708, 291, 536, 510, 307, 300, 456, 307, 257, 3565, 3142, 24590, 9284, 420, 3565, 3142, 24590, 50886, 50886, 4985, 300, 307, 1940, 382, 4846, 6157, 2310, 11, 8888, 293, 19049, 3125, 295, 257, 256, 12, 15313, 51206, 51206, 293, 1228, 613, 1045, 4122, 281, 12539, 264, 8482, 295, 264, 256, 12, 15313, 885, 257, 1192, 51478, 51478, 23600, 13, 51532, 51532, 407, 341, 307, 445, 3565, 3142, 24590, 13, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.12899804366262335, "compression_ratio": 1.7410714285714286, "no_speech_prob": 1.6963300367933698e-05}, {"id": 135, "seek": 66132, "start": 683.6, "end": 684.6800000000001, "text": " seller.", "tokens": [50364, 1449, 718, 385, 2060, 493, 264, 1411, 1922, 295, 341, 10686, 293, 536, 437, 321, 434, 1411, 365, 13, 50616, 50616, 708, 291, 536, 510, 307, 300, 456, 307, 257, 3565, 3142, 24590, 9284, 420, 3565, 3142, 24590, 50886, 50886, 4985, 300, 307, 1940, 382, 4846, 6157, 2310, 11, 8888, 293, 19049, 3125, 295, 257, 256, 12, 15313, 51206, 51206, 293, 1228, 613, 1045, 4122, 281, 12539, 264, 8482, 295, 264, 256, 12, 15313, 885, 257, 1192, 51478, 51478, 23600, 13, 51532, 51532, 407, 341, 307, 445, 3565, 3142, 24590, 13, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.12899804366262335, "compression_ratio": 1.7410714285714286, "no_speech_prob": 1.6963300367933698e-05}, {"id": 136, "seek": 66132, "start": 684.6800000000001, "end": 688.48, "text": " So this is just logistic regression.", "tokens": [50364, 1449, 718, 385, 2060, 493, 264, 1411, 1922, 295, 341, 10686, 293, 536, 437, 321, 434, 1411, 365, 13, 50616, 50616, 708, 291, 536, 510, 307, 300, 456, 307, 257, 3565, 3142, 24590, 9284, 420, 3565, 3142, 24590, 50886, 50886, 4985, 300, 307, 1940, 382, 4846, 6157, 2310, 11, 8888, 293, 19049, 3125, 295, 257, 256, 12, 15313, 51206, 51206, 293, 1228, 613, 1045, 4122, 281, 12539, 264, 8482, 295, 264, 256, 12, 15313, 885, 257, 1192, 51478, 51478, 23600, 13, 51532, 51532, 407, 341, 307, 445, 3565, 3142, 24590, 13, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.12899804366262335, "compression_ratio": 1.7410714285714286, "no_speech_prob": 1.6963300367933698e-05}, {"id": 137, "seek": 68848, "start": 688.48, "end": 694.12, "text": " But the cool thing about this is rather than using the original features, price, shipping,", "tokens": [50364, 583, 264, 1627, 551, 466, 341, 307, 2831, 813, 1228, 264, 3380, 4122, 11, 3218, 11, 14122, 11, 50646, 50646, 2063, 11, 6370, 293, 370, 322, 11, 309, 311, 1228, 257, 777, 11, 1310, 1101, 992, 295, 4122, 11, 6157, 2310, 11, 50894, 50894, 8888, 293, 19049, 3125, 300, 366, 4696, 544, 35521, 295, 1968, 420, 406, 341, 256, 12, 15313, 51186, 51186, 486, 312, 257, 1192, 23600, 13, 51266, 51266, 407, 472, 636, 281, 519, 295, 341, 18161, 3209, 307, 445, 3565, 3142, 24590, 11, 457, 309, 307, 257, 3037, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.1377024198833265, "compression_ratio": 1.6131687242798354, "no_speech_prob": 1.6027380524974433e-06}, {"id": 138, "seek": 68848, "start": 694.12, "end": 699.08, "text": " cost, marketing and so on, it's using a new, maybe better set of features, affordability,", "tokens": [50364, 583, 264, 1627, 551, 466, 341, 307, 2831, 813, 1228, 264, 3380, 4122, 11, 3218, 11, 14122, 11, 50646, 50646, 2063, 11, 6370, 293, 370, 322, 11, 309, 311, 1228, 257, 777, 11, 1310, 1101, 992, 295, 4122, 11, 6157, 2310, 11, 50894, 50894, 8888, 293, 19049, 3125, 300, 366, 4696, 544, 35521, 295, 1968, 420, 406, 341, 256, 12, 15313, 51186, 51186, 486, 312, 257, 1192, 23600, 13, 51266, 51266, 407, 472, 636, 281, 519, 295, 341, 18161, 3209, 307, 445, 3565, 3142, 24590, 11, 457, 309, 307, 257, 3037, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.1377024198833265, "compression_ratio": 1.6131687242798354, "no_speech_prob": 1.6027380524974433e-06}, {"id": 139, "seek": 68848, "start": 699.08, "end": 704.9200000000001, "text": " awareness and perceived quality that are hopefully more predictive of whether or not this t-shirt", "tokens": [50364, 583, 264, 1627, 551, 466, 341, 307, 2831, 813, 1228, 264, 3380, 4122, 11, 3218, 11, 14122, 11, 50646, 50646, 2063, 11, 6370, 293, 370, 322, 11, 309, 311, 1228, 257, 777, 11, 1310, 1101, 992, 295, 4122, 11, 6157, 2310, 11, 50894, 50894, 8888, 293, 19049, 3125, 300, 366, 4696, 544, 35521, 295, 1968, 420, 406, 341, 256, 12, 15313, 51186, 51186, 486, 312, 257, 1192, 23600, 13, 51266, 51266, 407, 472, 636, 281, 519, 295, 341, 18161, 3209, 307, 445, 3565, 3142, 24590, 11, 457, 309, 307, 257, 3037, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.1377024198833265, "compression_ratio": 1.6131687242798354, "no_speech_prob": 1.6027380524974433e-06}, {"id": 140, "seek": 68848, "start": 704.9200000000001, "end": 706.52, "text": " will be a top seller.", "tokens": [50364, 583, 264, 1627, 551, 466, 341, 307, 2831, 813, 1228, 264, 3380, 4122, 11, 3218, 11, 14122, 11, 50646, 50646, 2063, 11, 6370, 293, 370, 322, 11, 309, 311, 1228, 257, 777, 11, 1310, 1101, 992, 295, 4122, 11, 6157, 2310, 11, 50894, 50894, 8888, 293, 19049, 3125, 300, 366, 4696, 544, 35521, 295, 1968, 420, 406, 341, 256, 12, 15313, 51186, 51186, 486, 312, 257, 1192, 23600, 13, 51266, 51266, 407, 472, 636, 281, 519, 295, 341, 18161, 3209, 307, 445, 3565, 3142, 24590, 11, 457, 309, 307, 257, 3037, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.1377024198833265, "compression_ratio": 1.6131687242798354, "no_speech_prob": 1.6027380524974433e-06}, {"id": 141, "seek": 68848, "start": 706.52, "end": 713.8000000000001, "text": " So one way to think of this neural network is just logistic regression, but it is a version", "tokens": [50364, 583, 264, 1627, 551, 466, 341, 307, 2831, 813, 1228, 264, 3380, 4122, 11, 3218, 11, 14122, 11, 50646, 50646, 2063, 11, 6370, 293, 370, 322, 11, 309, 311, 1228, 257, 777, 11, 1310, 1101, 992, 295, 4122, 11, 6157, 2310, 11, 50894, 50894, 8888, 293, 19049, 3125, 300, 366, 4696, 544, 35521, 295, 1968, 420, 406, 341, 256, 12, 15313, 51186, 51186, 486, 312, 257, 1192, 23600, 13, 51266, 51266, 407, 472, 636, 281, 519, 295, 341, 18161, 3209, 307, 445, 3565, 3142, 24590, 11, 457, 309, 307, 257, 3037, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.1377024198833265, "compression_ratio": 1.6131687242798354, "no_speech_prob": 1.6027380524974433e-06}, {"id": 142, "seek": 71380, "start": 713.8, "end": 719.16, "text": " of logistic regression that can learn its own features that makes it easier to make", "tokens": [50364, 295, 3565, 3142, 24590, 300, 393, 1466, 1080, 1065, 4122, 300, 1669, 309, 3571, 281, 652, 50632, 50632, 8559, 21264, 13, 50726, 50726, 682, 1186, 11, 291, 1062, 1604, 490, 264, 3894, 1243, 11, 341, 6849, 1365, 689, 321, 848, 300, 51012, 51012, 498, 291, 528, 281, 6069, 264, 3218, 295, 264, 1782, 11, 291, 1062, 747, 264, 1868, 609, 420, 264, 11402, 295, 51238, 51238, 3195, 293, 12972, 300, 538, 264, 7161, 295, 257, 688, 281, 7690, 257, 544, 3997, 4111, 11, 1783, 16, 1413, 51544, 51544, 1783, 17, 11, 597, 390, 264, 2744, 295, 264, 688, 13, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.1252942131560983, "compression_ratio": 1.6693548387096775, "no_speech_prob": 2.0579675492626848e-06}, {"id": 143, "seek": 71380, "start": 719.16, "end": 721.04, "text": " accurate predictions.", "tokens": [50364, 295, 3565, 3142, 24590, 300, 393, 1466, 1080, 1065, 4122, 300, 1669, 309, 3571, 281, 652, 50632, 50632, 8559, 21264, 13, 50726, 50726, 682, 1186, 11, 291, 1062, 1604, 490, 264, 3894, 1243, 11, 341, 6849, 1365, 689, 321, 848, 300, 51012, 51012, 498, 291, 528, 281, 6069, 264, 3218, 295, 264, 1782, 11, 291, 1062, 747, 264, 1868, 609, 420, 264, 11402, 295, 51238, 51238, 3195, 293, 12972, 300, 538, 264, 7161, 295, 257, 688, 281, 7690, 257, 544, 3997, 4111, 11, 1783, 16, 1413, 51544, 51544, 1783, 17, 11, 597, 390, 264, 2744, 295, 264, 688, 13, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.1252942131560983, "compression_ratio": 1.6693548387096775, "no_speech_prob": 2.0579675492626848e-06}, {"id": 144, "seek": 71380, "start": 721.04, "end": 726.76, "text": " In fact, you might remember from the previous week, this housing example where we said that", "tokens": [50364, 295, 3565, 3142, 24590, 300, 393, 1466, 1080, 1065, 4122, 300, 1669, 309, 3571, 281, 652, 50632, 50632, 8559, 21264, 13, 50726, 50726, 682, 1186, 11, 291, 1062, 1604, 490, 264, 3894, 1243, 11, 341, 6849, 1365, 689, 321, 848, 300, 51012, 51012, 498, 291, 528, 281, 6069, 264, 3218, 295, 264, 1782, 11, 291, 1062, 747, 264, 1868, 609, 420, 264, 11402, 295, 51238, 51238, 3195, 293, 12972, 300, 538, 264, 7161, 295, 257, 688, 281, 7690, 257, 544, 3997, 4111, 11, 1783, 16, 1413, 51544, 51544, 1783, 17, 11, 597, 390, 264, 2744, 295, 264, 688, 13, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.1252942131560983, "compression_ratio": 1.6693548387096775, "no_speech_prob": 2.0579675492626848e-06}, {"id": 145, "seek": 71380, "start": 726.76, "end": 731.28, "text": " if you want to predict the price of the house, you might take the frontage or the width of", "tokens": [50364, 295, 3565, 3142, 24590, 300, 393, 1466, 1080, 1065, 4122, 300, 1669, 309, 3571, 281, 652, 50632, 50632, 8559, 21264, 13, 50726, 50726, 682, 1186, 11, 291, 1062, 1604, 490, 264, 3894, 1243, 11, 341, 6849, 1365, 689, 321, 848, 300, 51012, 51012, 498, 291, 528, 281, 6069, 264, 3218, 295, 264, 1782, 11, 291, 1062, 747, 264, 1868, 609, 420, 264, 11402, 295, 51238, 51238, 3195, 293, 12972, 300, 538, 264, 7161, 295, 257, 688, 281, 7690, 257, 544, 3997, 4111, 11, 1783, 16, 1413, 51544, 51544, 1783, 17, 11, 597, 390, 264, 2744, 295, 264, 688, 13, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.1252942131560983, "compression_ratio": 1.6693548387096775, "no_speech_prob": 2.0579675492626848e-06}, {"id": 146, "seek": 71380, "start": 731.28, "end": 737.4, "text": " lots and multiply that by the depth of a lot to construct a more complex feature, X1 times", "tokens": [50364, 295, 3565, 3142, 24590, 300, 393, 1466, 1080, 1065, 4122, 300, 1669, 309, 3571, 281, 652, 50632, 50632, 8559, 21264, 13, 50726, 50726, 682, 1186, 11, 291, 1062, 1604, 490, 264, 3894, 1243, 11, 341, 6849, 1365, 689, 321, 848, 300, 51012, 51012, 498, 291, 528, 281, 6069, 264, 3218, 295, 264, 1782, 11, 291, 1062, 747, 264, 1868, 609, 420, 264, 11402, 295, 51238, 51238, 3195, 293, 12972, 300, 538, 264, 7161, 295, 257, 688, 281, 7690, 257, 544, 3997, 4111, 11, 1783, 16, 1413, 51544, 51544, 1783, 17, 11, 597, 390, 264, 2744, 295, 264, 688, 13, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.1252942131560983, "compression_ratio": 1.6693548387096775, "no_speech_prob": 2.0579675492626848e-06}, {"id": 147, "seek": 71380, "start": 737.4, "end": 740.28, "text": " X2, which was the size of the lot.", "tokens": [50364, 295, 3565, 3142, 24590, 300, 393, 1466, 1080, 1065, 4122, 300, 1669, 309, 3571, 281, 652, 50632, 50632, 8559, 21264, 13, 50726, 50726, 682, 1186, 11, 291, 1062, 1604, 490, 264, 3894, 1243, 11, 341, 6849, 1365, 689, 321, 848, 300, 51012, 51012, 498, 291, 528, 281, 6069, 264, 3218, 295, 264, 1782, 11, 291, 1062, 747, 264, 1868, 609, 420, 264, 11402, 295, 51238, 51238, 3195, 293, 12972, 300, 538, 264, 7161, 295, 257, 688, 281, 7690, 257, 544, 3997, 4111, 11, 1783, 16, 1413, 51544, 51544, 1783, 17, 11, 597, 390, 264, 2744, 295, 264, 688, 13, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.1252942131560983, "compression_ratio": 1.6693548387096775, "no_speech_prob": 2.0579675492626848e-06}, {"id": 148, "seek": 74028, "start": 740.28, "end": 745.6, "text": " So there we were doing manual feature engineering where we had to look at the features X1 and", "tokens": [50364, 407, 456, 321, 645, 884, 9688, 4111, 7043, 689, 321, 632, 281, 574, 412, 264, 4122, 1783, 16, 293, 50630, 50630, 1783, 17, 293, 4536, 538, 1011, 577, 281, 10432, 552, 1214, 281, 808, 493, 365, 1101, 4122, 13, 50902, 50902, 708, 264, 18161, 3209, 775, 307, 2602, 295, 291, 18006, 281, 16945, 11403, 264, 4122, 11, 51178, 51178, 309, 393, 1466, 11, 382, 291, 536, 1780, 11, 1080, 1065, 4122, 281, 652, 264, 2539, 1154, 3571, 337, 2564, 13, 51562, 51562, 407, 341, 307, 437, 1669, 18161, 9590, 472, 295, 264, 881, 4005, 2539, 14642, 294, 51776, 51776, 264, 1002, 965, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.11586197291579202, "compression_ratio": 1.8313725490196078, "no_speech_prob": 5.093383606435964e-06}, {"id": 149, "seek": 74028, "start": 745.6, "end": 751.04, "text": " X2 and decide by hand how to combine them together to come up with better features.", "tokens": [50364, 407, 456, 321, 645, 884, 9688, 4111, 7043, 689, 321, 632, 281, 574, 412, 264, 4122, 1783, 16, 293, 50630, 50630, 1783, 17, 293, 4536, 538, 1011, 577, 281, 10432, 552, 1214, 281, 808, 493, 365, 1101, 4122, 13, 50902, 50902, 708, 264, 18161, 3209, 775, 307, 2602, 295, 291, 18006, 281, 16945, 11403, 264, 4122, 11, 51178, 51178, 309, 393, 1466, 11, 382, 291, 536, 1780, 11, 1080, 1065, 4122, 281, 652, 264, 2539, 1154, 3571, 337, 2564, 13, 51562, 51562, 407, 341, 307, 437, 1669, 18161, 9590, 472, 295, 264, 881, 4005, 2539, 14642, 294, 51776, 51776, 264, 1002, 965, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.11586197291579202, "compression_ratio": 1.8313725490196078, "no_speech_prob": 5.093383606435964e-06}, {"id": 150, "seek": 74028, "start": 751.04, "end": 756.56, "text": " What the neural network does is instead of you needing to manually engineer the features,", "tokens": [50364, 407, 456, 321, 645, 884, 9688, 4111, 7043, 689, 321, 632, 281, 574, 412, 264, 4122, 1783, 16, 293, 50630, 50630, 1783, 17, 293, 4536, 538, 1011, 577, 281, 10432, 552, 1214, 281, 808, 493, 365, 1101, 4122, 13, 50902, 50902, 708, 264, 18161, 3209, 775, 307, 2602, 295, 291, 18006, 281, 16945, 11403, 264, 4122, 11, 51178, 51178, 309, 393, 1466, 11, 382, 291, 536, 1780, 11, 1080, 1065, 4122, 281, 652, 264, 2539, 1154, 3571, 337, 2564, 13, 51562, 51562, 407, 341, 307, 437, 1669, 18161, 9590, 472, 295, 264, 881, 4005, 2539, 14642, 294, 51776, 51776, 264, 1002, 965, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.11586197291579202, "compression_ratio": 1.8313725490196078, "no_speech_prob": 5.093383606435964e-06}, {"id": 151, "seek": 74028, "start": 756.56, "end": 764.24, "text": " it can learn, as you see later, its own features to make the learning problem easier for itself.", "tokens": [50364, 407, 456, 321, 645, 884, 9688, 4111, 7043, 689, 321, 632, 281, 574, 412, 264, 4122, 1783, 16, 293, 50630, 50630, 1783, 17, 293, 4536, 538, 1011, 577, 281, 10432, 552, 1214, 281, 808, 493, 365, 1101, 4122, 13, 50902, 50902, 708, 264, 18161, 3209, 775, 307, 2602, 295, 291, 18006, 281, 16945, 11403, 264, 4122, 11, 51178, 51178, 309, 393, 1466, 11, 382, 291, 536, 1780, 11, 1080, 1065, 4122, 281, 652, 264, 2539, 1154, 3571, 337, 2564, 13, 51562, 51562, 407, 341, 307, 437, 1669, 18161, 9590, 472, 295, 264, 881, 4005, 2539, 14642, 294, 51776, 51776, 264, 1002, 965, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.11586197291579202, "compression_ratio": 1.8313725490196078, "no_speech_prob": 5.093383606435964e-06}, {"id": 152, "seek": 74028, "start": 764.24, "end": 768.52, "text": " So this is what makes neural networks one of the most powerful learning algorithms in", "tokens": [50364, 407, 456, 321, 645, 884, 9688, 4111, 7043, 689, 321, 632, 281, 574, 412, 264, 4122, 1783, 16, 293, 50630, 50630, 1783, 17, 293, 4536, 538, 1011, 577, 281, 10432, 552, 1214, 281, 808, 493, 365, 1101, 4122, 13, 50902, 50902, 708, 264, 18161, 3209, 775, 307, 2602, 295, 291, 18006, 281, 16945, 11403, 264, 4122, 11, 51178, 51178, 309, 393, 1466, 11, 382, 291, 536, 1780, 11, 1080, 1065, 4122, 281, 652, 264, 2539, 1154, 3571, 337, 2564, 13, 51562, 51562, 407, 341, 307, 437, 1669, 18161, 9590, 472, 295, 264, 881, 4005, 2539, 14642, 294, 51776, 51776, 264, 1002, 965, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.11586197291579202, "compression_ratio": 1.8313725490196078, "no_speech_prob": 5.093383606435964e-06}, {"id": 153, "seek": 74028, "start": 768.52, "end": 769.8399999999999, "text": " the world today.", "tokens": [50364, 407, 456, 321, 645, 884, 9688, 4111, 7043, 689, 321, 632, 281, 574, 412, 264, 4122, 1783, 16, 293, 50630, 50630, 1783, 17, 293, 4536, 538, 1011, 577, 281, 10432, 552, 1214, 281, 808, 493, 365, 1101, 4122, 13, 50902, 50902, 708, 264, 18161, 3209, 775, 307, 2602, 295, 291, 18006, 281, 16945, 11403, 264, 4122, 11, 51178, 51178, 309, 393, 1466, 11, 382, 291, 536, 1780, 11, 1080, 1065, 4122, 281, 652, 264, 2539, 1154, 3571, 337, 2564, 13, 51562, 51562, 407, 341, 307, 437, 1669, 18161, 9590, 472, 295, 264, 881, 4005, 2539, 14642, 294, 51776, 51776, 264, 1002, 965, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.11586197291579202, "compression_ratio": 1.8313725490196078, "no_speech_prob": 5.093383606435964e-06}, {"id": 154, "seek": 76984, "start": 769.84, "end": 773.72, "text": " So to summarize, a neural network does this.", "tokens": [50364, 407, 281, 20858, 11, 257, 18161, 3209, 775, 341, 13, 50558, 50558, 440, 4846, 4583, 575, 257, 8062, 295, 4122, 11, 1451, 3547, 294, 341, 1365, 13, 50790, 50790, 467, 307, 4846, 281, 264, 7633, 4583, 11, 597, 23930, 1045, 3547, 13, 51038, 51038, 400, 286, 478, 516, 281, 764, 257, 8062, 281, 45708, 341, 8062, 295, 2430, 763, 300, 341, 7633, 4583, 51398, 51398, 23930, 13, 51462, 51462, 400, 550, 264, 5598, 4583, 2516, 341, 4846, 11, 729, 1045, 3547, 293, 23930, 472, 1230, 11, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.16929860644870334, "compression_ratio": 1.7892156862745099, "no_speech_prob": 3.726510158230667e-06}, {"id": 155, "seek": 76984, "start": 773.72, "end": 778.36, "text": " The input layer has a vector of features, four numbers in this example.", "tokens": [50364, 407, 281, 20858, 11, 257, 18161, 3209, 775, 341, 13, 50558, 50558, 440, 4846, 4583, 575, 257, 8062, 295, 4122, 11, 1451, 3547, 294, 341, 1365, 13, 50790, 50790, 467, 307, 4846, 281, 264, 7633, 4583, 11, 597, 23930, 1045, 3547, 13, 51038, 51038, 400, 286, 478, 516, 281, 764, 257, 8062, 281, 45708, 341, 8062, 295, 2430, 763, 300, 341, 7633, 4583, 51398, 51398, 23930, 13, 51462, 51462, 400, 550, 264, 5598, 4583, 2516, 341, 4846, 11, 729, 1045, 3547, 293, 23930, 472, 1230, 11, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.16929860644870334, "compression_ratio": 1.7892156862745099, "no_speech_prob": 3.726510158230667e-06}, {"id": 156, "seek": 76984, "start": 778.36, "end": 783.32, "text": " It is input to the hidden layer, which outputs three numbers.", "tokens": [50364, 407, 281, 20858, 11, 257, 18161, 3209, 775, 341, 13, 50558, 50558, 440, 4846, 4583, 575, 257, 8062, 295, 4122, 11, 1451, 3547, 294, 341, 1365, 13, 50790, 50790, 467, 307, 4846, 281, 264, 7633, 4583, 11, 597, 23930, 1045, 3547, 13, 51038, 51038, 400, 286, 478, 516, 281, 764, 257, 8062, 281, 45708, 341, 8062, 295, 2430, 763, 300, 341, 7633, 4583, 51398, 51398, 23930, 13, 51462, 51462, 400, 550, 264, 5598, 4583, 2516, 341, 4846, 11, 729, 1045, 3547, 293, 23930, 472, 1230, 11, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.16929860644870334, "compression_ratio": 1.7892156862745099, "no_speech_prob": 3.726510158230667e-06}, {"id": 157, "seek": 76984, "start": 783.32, "end": 790.52, "text": " And I'm going to use a vector to denote this vector of activations that this hidden layer", "tokens": [50364, 407, 281, 20858, 11, 257, 18161, 3209, 775, 341, 13, 50558, 50558, 440, 4846, 4583, 575, 257, 8062, 295, 4122, 11, 1451, 3547, 294, 341, 1365, 13, 50790, 50790, 467, 307, 4846, 281, 264, 7633, 4583, 11, 597, 23930, 1045, 3547, 13, 51038, 51038, 400, 286, 478, 516, 281, 764, 257, 8062, 281, 45708, 341, 8062, 295, 2430, 763, 300, 341, 7633, 4583, 51398, 51398, 23930, 13, 51462, 51462, 400, 550, 264, 5598, 4583, 2516, 341, 4846, 11, 729, 1045, 3547, 293, 23930, 472, 1230, 11, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.16929860644870334, "compression_ratio": 1.7892156862745099, "no_speech_prob": 3.726510158230667e-06}, {"id": 158, "seek": 76984, "start": 790.52, "end": 791.8000000000001, "text": " outputs.", "tokens": [50364, 407, 281, 20858, 11, 257, 18161, 3209, 775, 341, 13, 50558, 50558, 440, 4846, 4583, 575, 257, 8062, 295, 4122, 11, 1451, 3547, 294, 341, 1365, 13, 50790, 50790, 467, 307, 4846, 281, 264, 7633, 4583, 11, 597, 23930, 1045, 3547, 13, 51038, 51038, 400, 286, 478, 516, 281, 764, 257, 8062, 281, 45708, 341, 8062, 295, 2430, 763, 300, 341, 7633, 4583, 51398, 51398, 23930, 13, 51462, 51462, 400, 550, 264, 5598, 4583, 2516, 341, 4846, 11, 729, 1045, 3547, 293, 23930, 472, 1230, 11, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.16929860644870334, "compression_ratio": 1.7892156862745099, "no_speech_prob": 3.726510158230667e-06}, {"id": 159, "seek": 76984, "start": 791.8000000000001, "end": 798.08, "text": " And then the output layer takes this input, those three numbers and outputs one number,", "tokens": [50364, 407, 281, 20858, 11, 257, 18161, 3209, 775, 341, 13, 50558, 50558, 440, 4846, 4583, 575, 257, 8062, 295, 4122, 11, 1451, 3547, 294, 341, 1365, 13, 50790, 50790, 467, 307, 4846, 281, 264, 7633, 4583, 11, 597, 23930, 1045, 3547, 13, 51038, 51038, 400, 286, 478, 516, 281, 764, 257, 8062, 281, 45708, 341, 8062, 295, 2430, 763, 300, 341, 7633, 4583, 51398, 51398, 23930, 13, 51462, 51462, 400, 550, 264, 5598, 4583, 2516, 341, 4846, 11, 729, 1045, 3547, 293, 23930, 472, 1230, 11, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.16929860644870334, "compression_ratio": 1.7892156862745099, "no_speech_prob": 3.726510158230667e-06}, {"id": 160, "seek": 79808, "start": 798.08, "end": 804.44, "text": " which would be the final activation or the final prediction of the neural network.", "tokens": [50364, 597, 576, 312, 264, 2572, 24433, 420, 264, 2572, 17630, 295, 264, 18161, 3209, 13, 50682, 50682, 1485, 3637, 11, 754, 1673, 286, 8046, 7619, 341, 18161, 3209, 382, 15866, 6157, 2310, 11, 50916, 50916, 8888, 11, 293, 19049, 3125, 11, 472, 295, 264, 534, 1481, 7221, 295, 257, 18161, 3209, 51138, 51138, 307, 562, 291, 3847, 309, 490, 1412, 11, 291, 500, 380, 643, 281, 352, 293, 20803, 4536, 437, 366, 51422, 51422, 264, 4122, 1270, 382, 6157, 2310, 293, 370, 322, 300, 257, 18161, 3209, 820, 14722, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.14342128469588908, "compression_ratio": 1.7916666666666667, "no_speech_prob": 7.453612624885864e-07}, {"id": 161, "seek": 79808, "start": 804.44, "end": 809.12, "text": " One note, even though I previously described this neural network as computing affordability,", "tokens": [50364, 597, 576, 312, 264, 2572, 24433, 420, 264, 2572, 17630, 295, 264, 18161, 3209, 13, 50682, 50682, 1485, 3637, 11, 754, 1673, 286, 8046, 7619, 341, 18161, 3209, 382, 15866, 6157, 2310, 11, 50916, 50916, 8888, 11, 293, 19049, 3125, 11, 472, 295, 264, 534, 1481, 7221, 295, 257, 18161, 3209, 51138, 51138, 307, 562, 291, 3847, 309, 490, 1412, 11, 291, 500, 380, 643, 281, 352, 293, 20803, 4536, 437, 366, 51422, 51422, 264, 4122, 1270, 382, 6157, 2310, 293, 370, 322, 300, 257, 18161, 3209, 820, 14722, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.14342128469588908, "compression_ratio": 1.7916666666666667, "no_speech_prob": 7.453612624885864e-07}, {"id": 162, "seek": 79808, "start": 809.12, "end": 813.5600000000001, "text": " awareness, and perceived quality, one of the really nice properties of a neural network", "tokens": [50364, 597, 576, 312, 264, 2572, 24433, 420, 264, 2572, 17630, 295, 264, 18161, 3209, 13, 50682, 50682, 1485, 3637, 11, 754, 1673, 286, 8046, 7619, 341, 18161, 3209, 382, 15866, 6157, 2310, 11, 50916, 50916, 8888, 11, 293, 19049, 3125, 11, 472, 295, 264, 534, 1481, 7221, 295, 257, 18161, 3209, 51138, 51138, 307, 562, 291, 3847, 309, 490, 1412, 11, 291, 500, 380, 643, 281, 352, 293, 20803, 4536, 437, 366, 51422, 51422, 264, 4122, 1270, 382, 6157, 2310, 293, 370, 322, 300, 257, 18161, 3209, 820, 14722, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.14342128469588908, "compression_ratio": 1.7916666666666667, "no_speech_prob": 7.453612624885864e-07}, {"id": 163, "seek": 79808, "start": 813.5600000000001, "end": 819.24, "text": " is when you train it from data, you don't need to go and explicitly decide what are", "tokens": [50364, 597, 576, 312, 264, 2572, 24433, 420, 264, 2572, 17630, 295, 264, 18161, 3209, 13, 50682, 50682, 1485, 3637, 11, 754, 1673, 286, 8046, 7619, 341, 18161, 3209, 382, 15866, 6157, 2310, 11, 50916, 50916, 8888, 11, 293, 19049, 3125, 11, 472, 295, 264, 534, 1481, 7221, 295, 257, 18161, 3209, 51138, 51138, 307, 562, 291, 3847, 309, 490, 1412, 11, 291, 500, 380, 643, 281, 352, 293, 20803, 4536, 437, 366, 51422, 51422, 264, 4122, 1270, 382, 6157, 2310, 293, 370, 322, 300, 257, 18161, 3209, 820, 14722, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.14342128469588908, "compression_ratio": 1.7916666666666667, "no_speech_prob": 7.453612624885864e-07}, {"id": 164, "seek": 79808, "start": 819.24, "end": 824.32, "text": " the features such as affordability and so on that a neural network should compute.", "tokens": [50364, 597, 576, 312, 264, 2572, 24433, 420, 264, 2572, 17630, 295, 264, 18161, 3209, 13, 50682, 50682, 1485, 3637, 11, 754, 1673, 286, 8046, 7619, 341, 18161, 3209, 382, 15866, 6157, 2310, 11, 50916, 50916, 8888, 11, 293, 19049, 3125, 11, 472, 295, 264, 534, 1481, 7221, 295, 257, 18161, 3209, 51138, 51138, 307, 562, 291, 3847, 309, 490, 1412, 11, 291, 500, 380, 643, 281, 352, 293, 20803, 4536, 437, 366, 51422, 51422, 264, 4122, 1270, 382, 6157, 2310, 293, 370, 322, 300, 257, 18161, 3209, 820, 14722, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.14342128469588908, "compression_ratio": 1.7916666666666667, "no_speech_prob": 7.453612624885864e-07}, {"id": 165, "seek": 82432, "start": 824.32, "end": 829.72, "text": " Then it will figure out all by itself what are the features it wants to use in this hidden", "tokens": [50364, 1396, 309, 486, 2573, 484, 439, 538, 2564, 437, 366, 264, 4122, 309, 2738, 281, 764, 294, 341, 7633, 50634, 50634, 4583, 13, 50684, 50684, 400, 300, 311, 437, 1669, 309, 1270, 257, 4005, 2539, 9284, 13, 50852, 50852, 407, 291, 600, 1612, 510, 472, 1365, 295, 257, 18161, 3209, 11, 293, 341, 18161, 3209, 575, 257, 2167, 51122, 51122, 4583, 300, 307, 257, 7633, 4583, 13, 51242, 51242, 961, 311, 747, 257, 574, 412, 512, 661, 5110, 295, 18161, 9590, 11, 4682, 5110, 365, 51486, 51486, 544, 813, 472, 7633, 4583, 13, 51628, 51628, 1692, 311, 364, 1365, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.13045461361224836, "compression_ratio": 1.7725321888412018, "no_speech_prob": 1.3496845667759771e-06}, {"id": 166, "seek": 82432, "start": 829.72, "end": 830.72, "text": " layer.", "tokens": [50364, 1396, 309, 486, 2573, 484, 439, 538, 2564, 437, 366, 264, 4122, 309, 2738, 281, 764, 294, 341, 7633, 50634, 50634, 4583, 13, 50684, 50684, 400, 300, 311, 437, 1669, 309, 1270, 257, 4005, 2539, 9284, 13, 50852, 50852, 407, 291, 600, 1612, 510, 472, 1365, 295, 257, 18161, 3209, 11, 293, 341, 18161, 3209, 575, 257, 2167, 51122, 51122, 4583, 300, 307, 257, 7633, 4583, 13, 51242, 51242, 961, 311, 747, 257, 574, 412, 512, 661, 5110, 295, 18161, 9590, 11, 4682, 5110, 365, 51486, 51486, 544, 813, 472, 7633, 4583, 13, 51628, 51628, 1692, 311, 364, 1365, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.13045461361224836, "compression_ratio": 1.7725321888412018, "no_speech_prob": 1.3496845667759771e-06}, {"id": 167, "seek": 82432, "start": 830.72, "end": 834.08, "text": " And that's what makes it such a powerful learning algorithm.", "tokens": [50364, 1396, 309, 486, 2573, 484, 439, 538, 2564, 437, 366, 264, 4122, 309, 2738, 281, 764, 294, 341, 7633, 50634, 50634, 4583, 13, 50684, 50684, 400, 300, 311, 437, 1669, 309, 1270, 257, 4005, 2539, 9284, 13, 50852, 50852, 407, 291, 600, 1612, 510, 472, 1365, 295, 257, 18161, 3209, 11, 293, 341, 18161, 3209, 575, 257, 2167, 51122, 51122, 4583, 300, 307, 257, 7633, 4583, 13, 51242, 51242, 961, 311, 747, 257, 574, 412, 512, 661, 5110, 295, 18161, 9590, 11, 4682, 5110, 365, 51486, 51486, 544, 813, 472, 7633, 4583, 13, 51628, 51628, 1692, 311, 364, 1365, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.13045461361224836, "compression_ratio": 1.7725321888412018, "no_speech_prob": 1.3496845667759771e-06}, {"id": 168, "seek": 82432, "start": 834.08, "end": 839.48, "text": " So you've seen here one example of a neural network, and this neural network has a single", "tokens": [50364, 1396, 309, 486, 2573, 484, 439, 538, 2564, 437, 366, 264, 4122, 309, 2738, 281, 764, 294, 341, 7633, 50634, 50634, 4583, 13, 50684, 50684, 400, 300, 311, 437, 1669, 309, 1270, 257, 4005, 2539, 9284, 13, 50852, 50852, 407, 291, 600, 1612, 510, 472, 1365, 295, 257, 18161, 3209, 11, 293, 341, 18161, 3209, 575, 257, 2167, 51122, 51122, 4583, 300, 307, 257, 7633, 4583, 13, 51242, 51242, 961, 311, 747, 257, 574, 412, 512, 661, 5110, 295, 18161, 9590, 11, 4682, 5110, 365, 51486, 51486, 544, 813, 472, 7633, 4583, 13, 51628, 51628, 1692, 311, 364, 1365, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.13045461361224836, "compression_ratio": 1.7725321888412018, "no_speech_prob": 1.3496845667759771e-06}, {"id": 169, "seek": 82432, "start": 839.48, "end": 841.88, "text": " layer that is a hidden layer.", "tokens": [50364, 1396, 309, 486, 2573, 484, 439, 538, 2564, 437, 366, 264, 4122, 309, 2738, 281, 764, 294, 341, 7633, 50634, 50634, 4583, 13, 50684, 50684, 400, 300, 311, 437, 1669, 309, 1270, 257, 4005, 2539, 9284, 13, 50852, 50852, 407, 291, 600, 1612, 510, 472, 1365, 295, 257, 18161, 3209, 11, 293, 341, 18161, 3209, 575, 257, 2167, 51122, 51122, 4583, 300, 307, 257, 7633, 4583, 13, 51242, 51242, 961, 311, 747, 257, 574, 412, 512, 661, 5110, 295, 18161, 9590, 11, 4682, 5110, 365, 51486, 51486, 544, 813, 472, 7633, 4583, 13, 51628, 51628, 1692, 311, 364, 1365, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.13045461361224836, "compression_ratio": 1.7725321888412018, "no_speech_prob": 1.3496845667759771e-06}, {"id": 170, "seek": 82432, "start": 841.88, "end": 846.7600000000001, "text": " Let's take a look at some other examples of neural networks, specifically examples with", "tokens": [50364, 1396, 309, 486, 2573, 484, 439, 538, 2564, 437, 366, 264, 4122, 309, 2738, 281, 764, 294, 341, 7633, 50634, 50634, 4583, 13, 50684, 50684, 400, 300, 311, 437, 1669, 309, 1270, 257, 4005, 2539, 9284, 13, 50852, 50852, 407, 291, 600, 1612, 510, 472, 1365, 295, 257, 18161, 3209, 11, 293, 341, 18161, 3209, 575, 257, 2167, 51122, 51122, 4583, 300, 307, 257, 7633, 4583, 13, 51242, 51242, 961, 311, 747, 257, 574, 412, 512, 661, 5110, 295, 18161, 9590, 11, 4682, 5110, 365, 51486, 51486, 544, 813, 472, 7633, 4583, 13, 51628, 51628, 1692, 311, 364, 1365, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.13045461361224836, "compression_ratio": 1.7725321888412018, "no_speech_prob": 1.3496845667759771e-06}, {"id": 171, "seek": 82432, "start": 846.7600000000001, "end": 849.6, "text": " more than one hidden layer.", "tokens": [50364, 1396, 309, 486, 2573, 484, 439, 538, 2564, 437, 366, 264, 4122, 309, 2738, 281, 764, 294, 341, 7633, 50634, 50634, 4583, 13, 50684, 50684, 400, 300, 311, 437, 1669, 309, 1270, 257, 4005, 2539, 9284, 13, 50852, 50852, 407, 291, 600, 1612, 510, 472, 1365, 295, 257, 18161, 3209, 11, 293, 341, 18161, 3209, 575, 257, 2167, 51122, 51122, 4583, 300, 307, 257, 7633, 4583, 13, 51242, 51242, 961, 311, 747, 257, 574, 412, 512, 661, 5110, 295, 18161, 9590, 11, 4682, 5110, 365, 51486, 51486, 544, 813, 472, 7633, 4583, 13, 51628, 51628, 1692, 311, 364, 1365, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.13045461361224836, "compression_ratio": 1.7725321888412018, "no_speech_prob": 1.3496845667759771e-06}, {"id": 172, "seek": 82432, "start": 849.6, "end": 851.2800000000001, "text": " Here's an example.", "tokens": [50364, 1396, 309, 486, 2573, 484, 439, 538, 2564, 437, 366, 264, 4122, 309, 2738, 281, 764, 294, 341, 7633, 50634, 50634, 4583, 13, 50684, 50684, 400, 300, 311, 437, 1669, 309, 1270, 257, 4005, 2539, 9284, 13, 50852, 50852, 407, 291, 600, 1612, 510, 472, 1365, 295, 257, 18161, 3209, 11, 293, 341, 18161, 3209, 575, 257, 2167, 51122, 51122, 4583, 300, 307, 257, 7633, 4583, 13, 51242, 51242, 961, 311, 747, 257, 574, 412, 512, 661, 5110, 295, 18161, 9590, 11, 4682, 5110, 365, 51486, 51486, 544, 813, 472, 7633, 4583, 13, 51628, 51628, 1692, 311, 364, 1365, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.13045461361224836, "compression_ratio": 1.7725321888412018, "no_speech_prob": 1.3496845667759771e-06}, {"id": 173, "seek": 85128, "start": 851.28, "end": 858.92, "text": " This neural network has an input feature vector X that is fed to one hidden layer, and I'm", "tokens": [50364, 639, 18161, 3209, 575, 364, 4846, 4111, 8062, 1783, 300, 307, 4636, 281, 472, 7633, 4583, 11, 293, 286, 478, 50746, 50746, 516, 281, 818, 341, 264, 700, 7633, 4583, 13, 50871, 50871, 400, 370, 498, 341, 7633, 4583, 575, 1045, 22027, 11, 309, 486, 550, 5598, 257, 8062, 295, 1045, 24433, 51218, 51218, 4190, 13, 51294, 51294, 1981, 1045, 3547, 393, 550, 312, 4846, 281, 257, 1150, 7633, 4583, 13, 51556, 51556, 400, 498, 264, 1150, 7633, 4583, 575, 732, 22027, 11, 732, 3565, 3142, 6815, 11, 550, 341, 1150, 7633, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.11637869477272034, "compression_ratio": 1.8803827751196172, "no_speech_prob": 9.972490261134226e-06}, {"id": 174, "seek": 85128, "start": 858.92, "end": 861.42, "text": " going to call this the first hidden layer.", "tokens": [50364, 639, 18161, 3209, 575, 364, 4846, 4111, 8062, 1783, 300, 307, 4636, 281, 472, 7633, 4583, 11, 293, 286, 478, 50746, 50746, 516, 281, 818, 341, 264, 700, 7633, 4583, 13, 50871, 50871, 400, 370, 498, 341, 7633, 4583, 575, 1045, 22027, 11, 309, 486, 550, 5598, 257, 8062, 295, 1045, 24433, 51218, 51218, 4190, 13, 51294, 51294, 1981, 1045, 3547, 393, 550, 312, 4846, 281, 257, 1150, 7633, 4583, 13, 51556, 51556, 400, 498, 264, 1150, 7633, 4583, 575, 732, 22027, 11, 732, 3565, 3142, 6815, 11, 550, 341, 1150, 7633, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.11637869477272034, "compression_ratio": 1.8803827751196172, "no_speech_prob": 9.972490261134226e-06}, {"id": 175, "seek": 85128, "start": 861.42, "end": 868.36, "text": " And so if this hidden layer has three neurons, it will then output a vector of three activation", "tokens": [50364, 639, 18161, 3209, 575, 364, 4846, 4111, 8062, 1783, 300, 307, 4636, 281, 472, 7633, 4583, 11, 293, 286, 478, 50746, 50746, 516, 281, 818, 341, 264, 700, 7633, 4583, 13, 50871, 50871, 400, 370, 498, 341, 7633, 4583, 575, 1045, 22027, 11, 309, 486, 550, 5598, 257, 8062, 295, 1045, 24433, 51218, 51218, 4190, 13, 51294, 51294, 1981, 1045, 3547, 393, 550, 312, 4846, 281, 257, 1150, 7633, 4583, 13, 51556, 51556, 400, 498, 264, 1150, 7633, 4583, 575, 732, 22027, 11, 732, 3565, 3142, 6815, 11, 550, 341, 1150, 7633, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.11637869477272034, "compression_ratio": 1.8803827751196172, "no_speech_prob": 9.972490261134226e-06}, {"id": 176, "seek": 85128, "start": 868.36, "end": 869.88, "text": " values.", "tokens": [50364, 639, 18161, 3209, 575, 364, 4846, 4111, 8062, 1783, 300, 307, 4636, 281, 472, 7633, 4583, 11, 293, 286, 478, 50746, 50746, 516, 281, 818, 341, 264, 700, 7633, 4583, 13, 50871, 50871, 400, 370, 498, 341, 7633, 4583, 575, 1045, 22027, 11, 309, 486, 550, 5598, 257, 8062, 295, 1045, 24433, 51218, 51218, 4190, 13, 51294, 51294, 1981, 1045, 3547, 393, 550, 312, 4846, 281, 257, 1150, 7633, 4583, 13, 51556, 51556, 400, 498, 264, 1150, 7633, 4583, 575, 732, 22027, 11, 732, 3565, 3142, 6815, 11, 550, 341, 1150, 7633, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.11637869477272034, "compression_ratio": 1.8803827751196172, "no_speech_prob": 9.972490261134226e-06}, {"id": 177, "seek": 85128, "start": 869.88, "end": 875.12, "text": " These three numbers can then be input to a second hidden layer.", "tokens": [50364, 639, 18161, 3209, 575, 364, 4846, 4111, 8062, 1783, 300, 307, 4636, 281, 472, 7633, 4583, 11, 293, 286, 478, 50746, 50746, 516, 281, 818, 341, 264, 700, 7633, 4583, 13, 50871, 50871, 400, 370, 498, 341, 7633, 4583, 575, 1045, 22027, 11, 309, 486, 550, 5598, 257, 8062, 295, 1045, 24433, 51218, 51218, 4190, 13, 51294, 51294, 1981, 1045, 3547, 393, 550, 312, 4846, 281, 257, 1150, 7633, 4583, 13, 51556, 51556, 400, 498, 264, 1150, 7633, 4583, 575, 732, 22027, 11, 732, 3565, 3142, 6815, 11, 550, 341, 1150, 7633, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.11637869477272034, "compression_ratio": 1.8803827751196172, "no_speech_prob": 9.972490261134226e-06}, {"id": 178, "seek": 85128, "start": 875.12, "end": 881.04, "text": " And if the second hidden layer has two neurons, two logistic units, then this second hidden", "tokens": [50364, 639, 18161, 3209, 575, 364, 4846, 4111, 8062, 1783, 300, 307, 4636, 281, 472, 7633, 4583, 11, 293, 286, 478, 50746, 50746, 516, 281, 818, 341, 264, 700, 7633, 4583, 13, 50871, 50871, 400, 370, 498, 341, 7633, 4583, 575, 1045, 22027, 11, 309, 486, 550, 5598, 257, 8062, 295, 1045, 24433, 51218, 51218, 4190, 13, 51294, 51294, 1981, 1045, 3547, 393, 550, 312, 4846, 281, 257, 1150, 7633, 4583, 13, 51556, 51556, 400, 498, 264, 1150, 7633, 4583, 575, 732, 22027, 11, 732, 3565, 3142, 6815, 11, 550, 341, 1150, 7633, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.11637869477272034, "compression_ratio": 1.8803827751196172, "no_speech_prob": 9.972490261134226e-06}, {"id": 179, "seek": 88104, "start": 881.04, "end": 887.36, "text": " layer will output another vector of now two activation values that maybe goes to the output", "tokens": [50364, 4583, 486, 5598, 1071, 8062, 295, 586, 732, 24433, 4190, 300, 1310, 1709, 281, 264, 5598, 50680, 50680, 4583, 300, 550, 23930, 264, 18161, 3209, 311, 2572, 17630, 13, 50896, 50896, 1610, 510, 311, 1071, 1365, 13, 50986, 50986, 1692, 311, 257, 18161, 3209, 300, 575, 341, 4846, 352, 281, 264, 700, 7633, 4583, 11, 300, 264, 5598, 51266, 51266, 281, 264, 700, 7633, 4583, 1709, 281, 264, 1150, 7633, 4583, 11, 1709, 281, 264, 2636, 7633, 4583, 11, 51458, 51458, 293, 550, 2721, 281, 264, 5598, 4583, 13, 51617, 51617, 1133, 291, 434, 2390, 428, 1065, 18161, 3209, 11, 472, 295, 264, 5327, 291, 643, 281, 652, 307, 577, 51833, 51833], "temperature": 0.0, "avg_logprob": -0.14046429751212136, "compression_ratio": 2.063025210084034, "no_speech_prob": 8.397877536481246e-06}, {"id": 180, "seek": 88104, "start": 887.36, "end": 891.68, "text": " layer that then outputs the neural network's final prediction.", "tokens": [50364, 4583, 486, 5598, 1071, 8062, 295, 586, 732, 24433, 4190, 300, 1310, 1709, 281, 264, 5598, 50680, 50680, 4583, 300, 550, 23930, 264, 18161, 3209, 311, 2572, 17630, 13, 50896, 50896, 1610, 510, 311, 1071, 1365, 13, 50986, 50986, 1692, 311, 257, 18161, 3209, 300, 575, 341, 4846, 352, 281, 264, 700, 7633, 4583, 11, 300, 264, 5598, 51266, 51266, 281, 264, 700, 7633, 4583, 1709, 281, 264, 1150, 7633, 4583, 11, 1709, 281, 264, 2636, 7633, 4583, 11, 51458, 51458, 293, 550, 2721, 281, 264, 5598, 4583, 13, 51617, 51617, 1133, 291, 434, 2390, 428, 1065, 18161, 3209, 11, 472, 295, 264, 5327, 291, 643, 281, 652, 307, 577, 51833, 51833], "temperature": 0.0, "avg_logprob": -0.14046429751212136, "compression_ratio": 2.063025210084034, "no_speech_prob": 8.397877536481246e-06}, {"id": 181, "seek": 88104, "start": 891.68, "end": 893.48, "text": " Or here's another example.", "tokens": [50364, 4583, 486, 5598, 1071, 8062, 295, 586, 732, 24433, 4190, 300, 1310, 1709, 281, 264, 5598, 50680, 50680, 4583, 300, 550, 23930, 264, 18161, 3209, 311, 2572, 17630, 13, 50896, 50896, 1610, 510, 311, 1071, 1365, 13, 50986, 50986, 1692, 311, 257, 18161, 3209, 300, 575, 341, 4846, 352, 281, 264, 700, 7633, 4583, 11, 300, 264, 5598, 51266, 51266, 281, 264, 700, 7633, 4583, 1709, 281, 264, 1150, 7633, 4583, 11, 1709, 281, 264, 2636, 7633, 4583, 11, 51458, 51458, 293, 550, 2721, 281, 264, 5598, 4583, 13, 51617, 51617, 1133, 291, 434, 2390, 428, 1065, 18161, 3209, 11, 472, 295, 264, 5327, 291, 643, 281, 652, 307, 577, 51833, 51833], "temperature": 0.0, "avg_logprob": -0.14046429751212136, "compression_ratio": 2.063025210084034, "no_speech_prob": 8.397877536481246e-06}, {"id": 182, "seek": 88104, "start": 893.48, "end": 899.0799999999999, "text": " Here's a neural network that has this input go to the first hidden layer, that the output", "tokens": [50364, 4583, 486, 5598, 1071, 8062, 295, 586, 732, 24433, 4190, 300, 1310, 1709, 281, 264, 5598, 50680, 50680, 4583, 300, 550, 23930, 264, 18161, 3209, 311, 2572, 17630, 13, 50896, 50896, 1610, 510, 311, 1071, 1365, 13, 50986, 50986, 1692, 311, 257, 18161, 3209, 300, 575, 341, 4846, 352, 281, 264, 700, 7633, 4583, 11, 300, 264, 5598, 51266, 51266, 281, 264, 700, 7633, 4583, 1709, 281, 264, 1150, 7633, 4583, 11, 1709, 281, 264, 2636, 7633, 4583, 11, 51458, 51458, 293, 550, 2721, 281, 264, 5598, 4583, 13, 51617, 51617, 1133, 291, 434, 2390, 428, 1065, 18161, 3209, 11, 472, 295, 264, 5327, 291, 643, 281, 652, 307, 577, 51833, 51833], "temperature": 0.0, "avg_logprob": -0.14046429751212136, "compression_ratio": 2.063025210084034, "no_speech_prob": 8.397877536481246e-06}, {"id": 183, "seek": 88104, "start": 899.0799999999999, "end": 902.92, "text": " to the first hidden layer goes to the second hidden layer, goes to the third hidden layer,", "tokens": [50364, 4583, 486, 5598, 1071, 8062, 295, 586, 732, 24433, 4190, 300, 1310, 1709, 281, 264, 5598, 50680, 50680, 4583, 300, 550, 23930, 264, 18161, 3209, 311, 2572, 17630, 13, 50896, 50896, 1610, 510, 311, 1071, 1365, 13, 50986, 50986, 1692, 311, 257, 18161, 3209, 300, 575, 341, 4846, 352, 281, 264, 700, 7633, 4583, 11, 300, 264, 5598, 51266, 51266, 281, 264, 700, 7633, 4583, 1709, 281, 264, 1150, 7633, 4583, 11, 1709, 281, 264, 2636, 7633, 4583, 11, 51458, 51458, 293, 550, 2721, 281, 264, 5598, 4583, 13, 51617, 51617, 1133, 291, 434, 2390, 428, 1065, 18161, 3209, 11, 472, 295, 264, 5327, 291, 643, 281, 652, 307, 577, 51833, 51833], "temperature": 0.0, "avg_logprob": -0.14046429751212136, "compression_ratio": 2.063025210084034, "no_speech_prob": 8.397877536481246e-06}, {"id": 184, "seek": 88104, "start": 902.92, "end": 906.0999999999999, "text": " and then finally to the output layer.", "tokens": [50364, 4583, 486, 5598, 1071, 8062, 295, 586, 732, 24433, 4190, 300, 1310, 1709, 281, 264, 5598, 50680, 50680, 4583, 300, 550, 23930, 264, 18161, 3209, 311, 2572, 17630, 13, 50896, 50896, 1610, 510, 311, 1071, 1365, 13, 50986, 50986, 1692, 311, 257, 18161, 3209, 300, 575, 341, 4846, 352, 281, 264, 700, 7633, 4583, 11, 300, 264, 5598, 51266, 51266, 281, 264, 700, 7633, 4583, 1709, 281, 264, 1150, 7633, 4583, 11, 1709, 281, 264, 2636, 7633, 4583, 11, 51458, 51458, 293, 550, 2721, 281, 264, 5598, 4583, 13, 51617, 51617, 1133, 291, 434, 2390, 428, 1065, 18161, 3209, 11, 472, 295, 264, 5327, 291, 643, 281, 652, 307, 577, 51833, 51833], "temperature": 0.0, "avg_logprob": -0.14046429751212136, "compression_ratio": 2.063025210084034, "no_speech_prob": 8.397877536481246e-06}, {"id": 185, "seek": 88104, "start": 906.0999999999999, "end": 910.42, "text": " When you're building your own neural network, one of the decisions you need to make is how", "tokens": [50364, 4583, 486, 5598, 1071, 8062, 295, 586, 732, 24433, 4190, 300, 1310, 1709, 281, 264, 5598, 50680, 50680, 4583, 300, 550, 23930, 264, 18161, 3209, 311, 2572, 17630, 13, 50896, 50896, 1610, 510, 311, 1071, 1365, 13, 50986, 50986, 1692, 311, 257, 18161, 3209, 300, 575, 341, 4846, 352, 281, 264, 700, 7633, 4583, 11, 300, 264, 5598, 51266, 51266, 281, 264, 700, 7633, 4583, 1709, 281, 264, 1150, 7633, 4583, 11, 1709, 281, 264, 2636, 7633, 4583, 11, 51458, 51458, 293, 550, 2721, 281, 264, 5598, 4583, 13, 51617, 51617, 1133, 291, 434, 2390, 428, 1065, 18161, 3209, 11, 472, 295, 264, 5327, 291, 643, 281, 652, 307, 577, 51833, 51833], "temperature": 0.0, "avg_logprob": -0.14046429751212136, "compression_ratio": 2.063025210084034, "no_speech_prob": 8.397877536481246e-06}, {"id": 186, "seek": 91042, "start": 910.42, "end": 916.56, "text": " many hidden layers do you want and how many neurons do you want each hidden layer to have.", "tokens": [50364, 867, 7633, 7914, 360, 291, 528, 293, 577, 867, 22027, 360, 291, 528, 1184, 7633, 4583, 281, 362, 13, 50671, 50671, 400, 341, 1168, 295, 577, 867, 7633, 7914, 293, 577, 867, 22027, 680, 7633, 4583, 307, 257, 50963, 50963, 1168, 295, 264, 9482, 295, 264, 18161, 3209, 13, 51138, 51138, 509, 1466, 1780, 294, 341, 1164, 512, 6082, 337, 10875, 364, 6854, 9482, 337, 257, 51393, 51393, 18161, 3209, 13, 51443, 51443, 583, 10875, 264, 558, 1230, 295, 7633, 7914, 293, 1230, 295, 7633, 6815, 680, 4583, 393, 362, 51719, 51719], "temperature": 0.0, "avg_logprob": -0.12207909847827668, "compression_ratio": 2.1515151515151514, "no_speech_prob": 4.3567747525230516e-06}, {"id": 187, "seek": 91042, "start": 916.56, "end": 922.4, "text": " And this question of how many hidden layers and how many neurons per hidden layer is a", "tokens": [50364, 867, 7633, 7914, 360, 291, 528, 293, 577, 867, 22027, 360, 291, 528, 1184, 7633, 4583, 281, 362, 13, 50671, 50671, 400, 341, 1168, 295, 577, 867, 7633, 7914, 293, 577, 867, 22027, 680, 7633, 4583, 307, 257, 50963, 50963, 1168, 295, 264, 9482, 295, 264, 18161, 3209, 13, 51138, 51138, 509, 1466, 1780, 294, 341, 1164, 512, 6082, 337, 10875, 364, 6854, 9482, 337, 257, 51393, 51393, 18161, 3209, 13, 51443, 51443, 583, 10875, 264, 558, 1230, 295, 7633, 7914, 293, 1230, 295, 7633, 6815, 680, 4583, 393, 362, 51719, 51719], "temperature": 0.0, "avg_logprob": -0.12207909847827668, "compression_ratio": 2.1515151515151514, "no_speech_prob": 4.3567747525230516e-06}, {"id": 188, "seek": 91042, "start": 922.4, "end": 925.9, "text": " question of the architecture of the neural network.", "tokens": [50364, 867, 7633, 7914, 360, 291, 528, 293, 577, 867, 22027, 360, 291, 528, 1184, 7633, 4583, 281, 362, 13, 50671, 50671, 400, 341, 1168, 295, 577, 867, 7633, 7914, 293, 577, 867, 22027, 680, 7633, 4583, 307, 257, 50963, 50963, 1168, 295, 264, 9482, 295, 264, 18161, 3209, 13, 51138, 51138, 509, 1466, 1780, 294, 341, 1164, 512, 6082, 337, 10875, 364, 6854, 9482, 337, 257, 51393, 51393, 18161, 3209, 13, 51443, 51443, 583, 10875, 264, 558, 1230, 295, 7633, 7914, 293, 1230, 295, 7633, 6815, 680, 4583, 393, 362, 51719, 51719], "temperature": 0.0, "avg_logprob": -0.12207909847827668, "compression_ratio": 2.1515151515151514, "no_speech_prob": 4.3567747525230516e-06}, {"id": 189, "seek": 91042, "start": 925.9, "end": 931.0, "text": " You learn later in this course some tips for choosing an appropriate architecture for a", "tokens": [50364, 867, 7633, 7914, 360, 291, 528, 293, 577, 867, 22027, 360, 291, 528, 1184, 7633, 4583, 281, 362, 13, 50671, 50671, 400, 341, 1168, 295, 577, 867, 7633, 7914, 293, 577, 867, 22027, 680, 7633, 4583, 307, 257, 50963, 50963, 1168, 295, 264, 9482, 295, 264, 18161, 3209, 13, 51138, 51138, 509, 1466, 1780, 294, 341, 1164, 512, 6082, 337, 10875, 364, 6854, 9482, 337, 257, 51393, 51393, 18161, 3209, 13, 51443, 51443, 583, 10875, 264, 558, 1230, 295, 7633, 7914, 293, 1230, 295, 7633, 6815, 680, 4583, 393, 362, 51719, 51719], "temperature": 0.0, "avg_logprob": -0.12207909847827668, "compression_ratio": 2.1515151515151514, "no_speech_prob": 4.3567747525230516e-06}, {"id": 190, "seek": 91042, "start": 931.0, "end": 932.0, "text": " neural network.", "tokens": [50364, 867, 7633, 7914, 360, 291, 528, 293, 577, 867, 22027, 360, 291, 528, 1184, 7633, 4583, 281, 362, 13, 50671, 50671, 400, 341, 1168, 295, 577, 867, 7633, 7914, 293, 577, 867, 22027, 680, 7633, 4583, 307, 257, 50963, 50963, 1168, 295, 264, 9482, 295, 264, 18161, 3209, 13, 51138, 51138, 509, 1466, 1780, 294, 341, 1164, 512, 6082, 337, 10875, 364, 6854, 9482, 337, 257, 51393, 51393, 18161, 3209, 13, 51443, 51443, 583, 10875, 264, 558, 1230, 295, 7633, 7914, 293, 1230, 295, 7633, 6815, 680, 4583, 393, 362, 51719, 51719], "temperature": 0.0, "avg_logprob": -0.12207909847827668, "compression_ratio": 2.1515151515151514, "no_speech_prob": 4.3567747525230516e-06}, {"id": 191, "seek": 91042, "start": 932.0, "end": 937.52, "text": " But choosing the right number of hidden layers and number of hidden units per layer can have", "tokens": [50364, 867, 7633, 7914, 360, 291, 528, 293, 577, 867, 22027, 360, 291, 528, 1184, 7633, 4583, 281, 362, 13, 50671, 50671, 400, 341, 1168, 295, 577, 867, 7633, 7914, 293, 577, 867, 22027, 680, 7633, 4583, 307, 257, 50963, 50963, 1168, 295, 264, 9482, 295, 264, 18161, 3209, 13, 51138, 51138, 509, 1466, 1780, 294, 341, 1164, 512, 6082, 337, 10875, 364, 6854, 9482, 337, 257, 51393, 51393, 18161, 3209, 13, 51443, 51443, 583, 10875, 264, 558, 1230, 295, 7633, 7914, 293, 1230, 295, 7633, 6815, 680, 4583, 393, 362, 51719, 51719], "temperature": 0.0, "avg_logprob": -0.12207909847827668, "compression_ratio": 2.1515151515151514, "no_speech_prob": 4.3567747525230516e-06}, {"id": 192, "seek": 93752, "start": 937.52, "end": 940.8, "text": " an impact on the performance of your learning algorithm as well.", "tokens": [50364, 364, 2712, 322, 264, 3389, 295, 428, 2539, 9284, 382, 731, 13, 50528, 50528, 407, 1780, 294, 341, 1164, 11, 291, 1466, 577, 281, 2826, 257, 665, 9482, 337, 428, 18161, 50772, 50772, 3209, 382, 731, 13, 50822, 50822, 876, 11, 293, 538, 264, 636, 11, 294, 512, 295, 264, 10394, 11, 291, 536, 341, 2010, 295, 18161, 3209, 365, 3866, 51058, 51058, 7914, 411, 341, 1219, 257, 4825, 12, 8376, 260, 43276, 2044, 13, 51212, 51212, 407, 498, 291, 536, 300, 11, 300, 445, 14942, 281, 257, 18161, 3209, 300, 1542, 411, 437, 291, 434, 51406, 51406, 2577, 510, 322, 264, 4137, 13, 51531, 51531, 407, 300, 311, 257, 18161, 3209, 13, 51656, 51656, 286, 458, 321, 1437, 807, 257, 688, 295, 341, 960, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13169143383319562, "compression_ratio": 1.837037037037037, "no_speech_prob": 8.939467079471797e-06}, {"id": 193, "seek": 93752, "start": 940.8, "end": 945.68, "text": " So later in this course, you learn how to choose a good architecture for your neural", "tokens": [50364, 364, 2712, 322, 264, 3389, 295, 428, 2539, 9284, 382, 731, 13, 50528, 50528, 407, 1780, 294, 341, 1164, 11, 291, 1466, 577, 281, 2826, 257, 665, 9482, 337, 428, 18161, 50772, 50772, 3209, 382, 731, 13, 50822, 50822, 876, 11, 293, 538, 264, 636, 11, 294, 512, 295, 264, 10394, 11, 291, 536, 341, 2010, 295, 18161, 3209, 365, 3866, 51058, 51058, 7914, 411, 341, 1219, 257, 4825, 12, 8376, 260, 43276, 2044, 13, 51212, 51212, 407, 498, 291, 536, 300, 11, 300, 445, 14942, 281, 257, 18161, 3209, 300, 1542, 411, 437, 291, 434, 51406, 51406, 2577, 510, 322, 264, 4137, 13, 51531, 51531, 407, 300, 311, 257, 18161, 3209, 13, 51656, 51656, 286, 458, 321, 1437, 807, 257, 688, 295, 341, 960, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13169143383319562, "compression_ratio": 1.837037037037037, "no_speech_prob": 8.939467079471797e-06}, {"id": 194, "seek": 93752, "start": 945.68, "end": 946.68, "text": " network as well.", "tokens": [50364, 364, 2712, 322, 264, 3389, 295, 428, 2539, 9284, 382, 731, 13, 50528, 50528, 407, 1780, 294, 341, 1164, 11, 291, 1466, 577, 281, 2826, 257, 665, 9482, 337, 428, 18161, 50772, 50772, 3209, 382, 731, 13, 50822, 50822, 876, 11, 293, 538, 264, 636, 11, 294, 512, 295, 264, 10394, 11, 291, 536, 341, 2010, 295, 18161, 3209, 365, 3866, 51058, 51058, 7914, 411, 341, 1219, 257, 4825, 12, 8376, 260, 43276, 2044, 13, 51212, 51212, 407, 498, 291, 536, 300, 11, 300, 445, 14942, 281, 257, 18161, 3209, 300, 1542, 411, 437, 291, 434, 51406, 51406, 2577, 510, 322, 264, 4137, 13, 51531, 51531, 407, 300, 311, 257, 18161, 3209, 13, 51656, 51656, 286, 458, 321, 1437, 807, 257, 688, 295, 341, 960, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13169143383319562, "compression_ratio": 1.837037037037037, "no_speech_prob": 8.939467079471797e-06}, {"id": 195, "seek": 93752, "start": 946.68, "end": 951.4, "text": " Oh, and by the way, in some of the literature, you see this type of neural network with multiple", "tokens": [50364, 364, 2712, 322, 264, 3389, 295, 428, 2539, 9284, 382, 731, 13, 50528, 50528, 407, 1780, 294, 341, 1164, 11, 291, 1466, 577, 281, 2826, 257, 665, 9482, 337, 428, 18161, 50772, 50772, 3209, 382, 731, 13, 50822, 50822, 876, 11, 293, 538, 264, 636, 11, 294, 512, 295, 264, 10394, 11, 291, 536, 341, 2010, 295, 18161, 3209, 365, 3866, 51058, 51058, 7914, 411, 341, 1219, 257, 4825, 12, 8376, 260, 43276, 2044, 13, 51212, 51212, 407, 498, 291, 536, 300, 11, 300, 445, 14942, 281, 257, 18161, 3209, 300, 1542, 411, 437, 291, 434, 51406, 51406, 2577, 510, 322, 264, 4137, 13, 51531, 51531, 407, 300, 311, 257, 18161, 3209, 13, 51656, 51656, 286, 458, 321, 1437, 807, 257, 688, 295, 341, 960, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13169143383319562, "compression_ratio": 1.837037037037037, "no_speech_prob": 8.939467079471797e-06}, {"id": 196, "seek": 93752, "start": 951.4, "end": 954.48, "text": " layers like this called a multi-layer perceptron.", "tokens": [50364, 364, 2712, 322, 264, 3389, 295, 428, 2539, 9284, 382, 731, 13, 50528, 50528, 407, 1780, 294, 341, 1164, 11, 291, 1466, 577, 281, 2826, 257, 665, 9482, 337, 428, 18161, 50772, 50772, 3209, 382, 731, 13, 50822, 50822, 876, 11, 293, 538, 264, 636, 11, 294, 512, 295, 264, 10394, 11, 291, 536, 341, 2010, 295, 18161, 3209, 365, 3866, 51058, 51058, 7914, 411, 341, 1219, 257, 4825, 12, 8376, 260, 43276, 2044, 13, 51212, 51212, 407, 498, 291, 536, 300, 11, 300, 445, 14942, 281, 257, 18161, 3209, 300, 1542, 411, 437, 291, 434, 51406, 51406, 2577, 510, 322, 264, 4137, 13, 51531, 51531, 407, 300, 311, 257, 18161, 3209, 13, 51656, 51656, 286, 458, 321, 1437, 807, 257, 688, 295, 341, 960, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13169143383319562, "compression_ratio": 1.837037037037037, "no_speech_prob": 8.939467079471797e-06}, {"id": 197, "seek": 93752, "start": 954.48, "end": 958.36, "text": " So if you see that, that just refers to a neural network that looks like what you're", "tokens": [50364, 364, 2712, 322, 264, 3389, 295, 428, 2539, 9284, 382, 731, 13, 50528, 50528, 407, 1780, 294, 341, 1164, 11, 291, 1466, 577, 281, 2826, 257, 665, 9482, 337, 428, 18161, 50772, 50772, 3209, 382, 731, 13, 50822, 50822, 876, 11, 293, 538, 264, 636, 11, 294, 512, 295, 264, 10394, 11, 291, 536, 341, 2010, 295, 18161, 3209, 365, 3866, 51058, 51058, 7914, 411, 341, 1219, 257, 4825, 12, 8376, 260, 43276, 2044, 13, 51212, 51212, 407, 498, 291, 536, 300, 11, 300, 445, 14942, 281, 257, 18161, 3209, 300, 1542, 411, 437, 291, 434, 51406, 51406, 2577, 510, 322, 264, 4137, 13, 51531, 51531, 407, 300, 311, 257, 18161, 3209, 13, 51656, 51656, 286, 458, 321, 1437, 807, 257, 688, 295, 341, 960, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13169143383319562, "compression_ratio": 1.837037037037037, "no_speech_prob": 8.939467079471797e-06}, {"id": 198, "seek": 93752, "start": 958.36, "end": 960.86, "text": " seeing here on the slide.", "tokens": [50364, 364, 2712, 322, 264, 3389, 295, 428, 2539, 9284, 382, 731, 13, 50528, 50528, 407, 1780, 294, 341, 1164, 11, 291, 1466, 577, 281, 2826, 257, 665, 9482, 337, 428, 18161, 50772, 50772, 3209, 382, 731, 13, 50822, 50822, 876, 11, 293, 538, 264, 636, 11, 294, 512, 295, 264, 10394, 11, 291, 536, 341, 2010, 295, 18161, 3209, 365, 3866, 51058, 51058, 7914, 411, 341, 1219, 257, 4825, 12, 8376, 260, 43276, 2044, 13, 51212, 51212, 407, 498, 291, 536, 300, 11, 300, 445, 14942, 281, 257, 18161, 3209, 300, 1542, 411, 437, 291, 434, 51406, 51406, 2577, 510, 322, 264, 4137, 13, 51531, 51531, 407, 300, 311, 257, 18161, 3209, 13, 51656, 51656, 286, 458, 321, 1437, 807, 257, 688, 295, 341, 960, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13169143383319562, "compression_ratio": 1.837037037037037, "no_speech_prob": 8.939467079471797e-06}, {"id": 199, "seek": 93752, "start": 960.86, "end": 963.36, "text": " So that's a neural network.", "tokens": [50364, 364, 2712, 322, 264, 3389, 295, 428, 2539, 9284, 382, 731, 13, 50528, 50528, 407, 1780, 294, 341, 1164, 11, 291, 1466, 577, 281, 2826, 257, 665, 9482, 337, 428, 18161, 50772, 50772, 3209, 382, 731, 13, 50822, 50822, 876, 11, 293, 538, 264, 636, 11, 294, 512, 295, 264, 10394, 11, 291, 536, 341, 2010, 295, 18161, 3209, 365, 3866, 51058, 51058, 7914, 411, 341, 1219, 257, 4825, 12, 8376, 260, 43276, 2044, 13, 51212, 51212, 407, 498, 291, 536, 300, 11, 300, 445, 14942, 281, 257, 18161, 3209, 300, 1542, 411, 437, 291, 434, 51406, 51406, 2577, 510, 322, 264, 4137, 13, 51531, 51531, 407, 300, 311, 257, 18161, 3209, 13, 51656, 51656, 286, 458, 321, 1437, 807, 257, 688, 295, 341, 960, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13169143383319562, "compression_ratio": 1.837037037037037, "no_speech_prob": 8.939467079471797e-06}, {"id": 200, "seek": 93752, "start": 963.36, "end": 965.52, "text": " I know we went through a lot of this video.", "tokens": [50364, 364, 2712, 322, 264, 3389, 295, 428, 2539, 9284, 382, 731, 13, 50528, 50528, 407, 1780, 294, 341, 1164, 11, 291, 1466, 577, 281, 2826, 257, 665, 9482, 337, 428, 18161, 50772, 50772, 3209, 382, 731, 13, 50822, 50822, 876, 11, 293, 538, 264, 636, 11, 294, 512, 295, 264, 10394, 11, 291, 536, 341, 2010, 295, 18161, 3209, 365, 3866, 51058, 51058, 7914, 411, 341, 1219, 257, 4825, 12, 8376, 260, 43276, 2044, 13, 51212, 51212, 407, 498, 291, 536, 300, 11, 300, 445, 14942, 281, 257, 18161, 3209, 300, 1542, 411, 437, 291, 434, 51406, 51406, 2577, 510, 322, 264, 4137, 13, 51531, 51531, 407, 300, 311, 257, 18161, 3209, 13, 51656, 51656, 286, 458, 321, 1437, 807, 257, 688, 295, 341, 960, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13169143383319562, "compression_ratio": 1.837037037037037, "no_speech_prob": 8.939467079471797e-06}, {"id": 201, "seek": 96552, "start": 965.52, "end": 970.52, "text": " So thank you for sticking with me, but you now know how a neural network works.", "tokens": [50364, 407, 1309, 291, 337, 13465, 365, 385, 11, 457, 291, 586, 458, 577, 257, 18161, 3209, 1985, 13, 50614, 50614, 682, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 577, 613, 3487, 393, 312, 6456, 281, 661, 5821, 50838, 50838, 382, 731, 13, 50888, 50888, 682, 1729, 11, 321, 603, 747, 257, 574, 412, 264, 3820, 5201, 3861, 295, 1851, 11150, 13, 51140, 51140, 961, 311, 352, 322, 281, 264, 958, 960, 13, 51212], "temperature": 0.0, "avg_logprob": -0.07132892730908516, "compression_ratio": 1.601063829787234, "no_speech_prob": 2.346995097468607e-05}, {"id": 202, "seek": 96552, "start": 970.52, "end": 975.0, "text": " In the next video, let's take a look at how these ideas can be applied to other applications", "tokens": [50364, 407, 1309, 291, 337, 13465, 365, 385, 11, 457, 291, 586, 458, 577, 257, 18161, 3209, 1985, 13, 50614, 50614, 682, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 577, 613, 3487, 393, 312, 6456, 281, 661, 5821, 50838, 50838, 382, 731, 13, 50888, 50888, 682, 1729, 11, 321, 603, 747, 257, 574, 412, 264, 3820, 5201, 3861, 295, 1851, 11150, 13, 51140, 51140, 961, 311, 352, 322, 281, 264, 958, 960, 13, 51212], "temperature": 0.0, "avg_logprob": -0.07132892730908516, "compression_ratio": 1.601063829787234, "no_speech_prob": 2.346995097468607e-05}, {"id": 203, "seek": 96552, "start": 975.0, "end": 976.0, "text": " as well.", "tokens": [50364, 407, 1309, 291, 337, 13465, 365, 385, 11, 457, 291, 586, 458, 577, 257, 18161, 3209, 1985, 13, 50614, 50614, 682, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 577, 613, 3487, 393, 312, 6456, 281, 661, 5821, 50838, 50838, 382, 731, 13, 50888, 50888, 682, 1729, 11, 321, 603, 747, 257, 574, 412, 264, 3820, 5201, 3861, 295, 1851, 11150, 13, 51140, 51140, 961, 311, 352, 322, 281, 264, 958, 960, 13, 51212], "temperature": 0.0, "avg_logprob": -0.07132892730908516, "compression_ratio": 1.601063829787234, "no_speech_prob": 2.346995097468607e-05}, {"id": 204, "seek": 96552, "start": 976.0, "end": 981.04, "text": " In particular, we'll take a look at the computer vision application of face recognition.", "tokens": [50364, 407, 1309, 291, 337, 13465, 365, 385, 11, 457, 291, 586, 458, 577, 257, 18161, 3209, 1985, 13, 50614, 50614, 682, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 577, 613, 3487, 393, 312, 6456, 281, 661, 5821, 50838, 50838, 382, 731, 13, 50888, 50888, 682, 1729, 11, 321, 603, 747, 257, 574, 412, 264, 3820, 5201, 3861, 295, 1851, 11150, 13, 51140, 51140, 961, 311, 352, 322, 281, 264, 958, 960, 13, 51212], "temperature": 0.0, "avg_logprob": -0.07132892730908516, "compression_ratio": 1.601063829787234, "no_speech_prob": 2.346995097468607e-05}, {"id": 205, "seek": 98104, "start": 981.04, "end": 996.68, "text": " Let's go on to the next video.", "tokens": [50364, 961, 311, 352, 322, 281, 264, 958, 960, 13, 51146], "temperature": 0.0, "avg_logprob": -0.44601257642110187, "compression_ratio": 0.7894736842105263, "no_speech_prob": 0.00036061182618141174}], "language": "en", "video_id": "cHFM92fhpew", "entity": "ML Specialization, Andrew Ng (2022)"}}