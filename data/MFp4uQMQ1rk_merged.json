{"video_id": "MFp4uQMQ1rk", "title": "3.11 Regularization to Reduce Overfitting | Regularized logistic regression-- [ML | Andrew Ng]", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 333, "views": 164, "publish_date": "11/04/2022", "timestamp": 1661299200, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " In this video, you see how to implement regularized logistic regression. Just as the gradient update for logistic regression has seemed surprisingly similar to the gradient update for linear regression, you find that the gradient descent update for regularized logistic regression will also look similar to the update for regularized linear regression. Let's take a look. Here's the idea. We saw earlier that logistic regression can be prone to overfitting if you fit it with very high-order polynomial features like this. Here z is a high-order polynomial that gets passed into the sigmoid function, like so, to compute f. And in particular, you can end up with a decision boundary that is overly complex and overfits this training set. More generally, when you train logistic regression with a lot of features, whether polynomial features or some other features, there can be a higher risk of overfitting. This was the cost function for logistic regression. If you want to modify it to use regularization, all you need to do is add to it the following term. Let's add lambda, the regularization parameter, over 2m, times the sum from j equals 1 through n, where n is the number of features as usual, of wj squared. So when you minimize this cost function as a function of w and b, it has the effect of penalizing parameters w1, w2, through wn and preventing them from being too large. And if you do this, then even though you're fitting a high-order polynomial with a lot of parameters, you still get a decision boundary that looks like this, something that looks more reasonable for separating positive and negative examples while also generalizing, hopefully, to new examples not in the training set. So when using regularization, even when you have a lot of features, how can you actually implement this? How can you actually minimize this cost function j of wb that includes the regularization term? Well, let's use gradient descent as before. So here's the cost function that you want to minimize. And to implement gradient descent, as before, we'll carry out the following simultaneous updates over wj and b. These are the usual update rules for gradient descent. And just like regularized linear regression, when you compute what are these derivative terms, the only thing that changes now is that the derivative with respect to wj gets this additional term, lambda over m times wj added here at the end. And again, it looks a lot like the update for regularized linear regression. In fact, it's the exact same equation, except for the fact that the definition of f is now no longer the linear function, it is the logistic function applied to z. And similar to linear regression, we will regularize only the parameters wj, but not the parameter b, which is why there's no change to the update you would make for b. In the final optional lab of this week, you revisit overfitting. And in the interactive plot in the optional lab, you can now choose to regularize your models, both regression and classification, by enabling regularization during gradient descent, by selecting a value for lambda. Please take a look at the code for implementing regularized logistic regression in particular, because you implement this in a practice lab yourself at the end of this week. So, now you know how to implement regularized logistic regression. When I walk around Silicon Valley, there are many engineers using machine learning to create a ton of value, sometimes making a lot of money for the companies. And I know you've only been studying this stuff for a few weeks. But if you understand and can apply linear regression and logistic regression, that's actually all you need to create some very valuable applications. While the specific learning algorithms you use are important, knowing things like when and how to reduce overfitting turns out to be one of the very valuable skills in the real world as well. So, I want to say, congratulations on how far you've come. And I want to say great job for getting through all the way to the end of this video. I hope you also work through the practice labs and quizzes. Having said that, there's still many more exciting things to learn. In the second course of this specialization, you learn about neural networks, also called deep learning algorithms. Neural networks are responsible for many of the latest breakthroughs in AI today, from practical speech recognition, to computers accurately recognizing objects and images, to self-driving cars. The way a neural network gets built actually uses a lot of what you've already learned, like cost functions and gradient descent and sigmoid functions. So again, congratulations on reaching the end of this third and final week of course one. I hope you have fun in the labs and I will see you in next week's material on neural networks.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.4, "text": " In this video, you see how to implement regularized logistic regression.", "tokens": [50364, 682, 341, 960, 11, 291, 536, 577, 281, 4445, 3890, 1602, 3565, 3142, 24590, 13, 50684, 50684, 1449, 382, 264, 16235, 5623, 337, 3565, 3142, 24590, 575, 6576, 17600, 2531, 281, 264, 16235, 50954, 50954, 5623, 337, 8213, 24590, 11, 291, 915, 300, 264, 16235, 23475, 5623, 337, 3890, 1602, 51186, 51186, 3565, 3142, 24590, 486, 611, 574, 2531, 281, 264, 5623, 337, 3890, 1602, 8213, 24590, 13, 51428, 51428, 961, 311, 747, 257, 574, 13, 51504, 51504, 1692, 311, 264, 1558, 13, 51580, 51580, 492, 1866, 3071, 300, 3565, 3142, 24590, 393, 312, 25806, 281, 670, 69, 2414, 498, 291, 3318, 309, 365, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.13374515816017432, "compression_ratio": 2.1493212669683257, "no_speech_prob": 0.009702973999083042}, {"id": 1, "seek": 0, "start": 6.4, "end": 11.8, "text": " Just as the gradient update for logistic regression has seemed surprisingly similar to the gradient", "tokens": [50364, 682, 341, 960, 11, 291, 536, 577, 281, 4445, 3890, 1602, 3565, 3142, 24590, 13, 50684, 50684, 1449, 382, 264, 16235, 5623, 337, 3565, 3142, 24590, 575, 6576, 17600, 2531, 281, 264, 16235, 50954, 50954, 5623, 337, 8213, 24590, 11, 291, 915, 300, 264, 16235, 23475, 5623, 337, 3890, 1602, 51186, 51186, 3565, 3142, 24590, 486, 611, 574, 2531, 281, 264, 5623, 337, 3890, 1602, 8213, 24590, 13, 51428, 51428, 961, 311, 747, 257, 574, 13, 51504, 51504, 1692, 311, 264, 1558, 13, 51580, 51580, 492, 1866, 3071, 300, 3565, 3142, 24590, 393, 312, 25806, 281, 670, 69, 2414, 498, 291, 3318, 309, 365, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.13374515816017432, "compression_ratio": 2.1493212669683257, "no_speech_prob": 0.009702973999083042}, {"id": 2, "seek": 0, "start": 11.8, "end": 16.44, "text": " update for linear regression, you find that the gradient descent update for regularized", "tokens": [50364, 682, 341, 960, 11, 291, 536, 577, 281, 4445, 3890, 1602, 3565, 3142, 24590, 13, 50684, 50684, 1449, 382, 264, 16235, 5623, 337, 3565, 3142, 24590, 575, 6576, 17600, 2531, 281, 264, 16235, 50954, 50954, 5623, 337, 8213, 24590, 11, 291, 915, 300, 264, 16235, 23475, 5623, 337, 3890, 1602, 51186, 51186, 3565, 3142, 24590, 486, 611, 574, 2531, 281, 264, 5623, 337, 3890, 1602, 8213, 24590, 13, 51428, 51428, 961, 311, 747, 257, 574, 13, 51504, 51504, 1692, 311, 264, 1558, 13, 51580, 51580, 492, 1866, 3071, 300, 3565, 3142, 24590, 393, 312, 25806, 281, 670, 69, 2414, 498, 291, 3318, 309, 365, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.13374515816017432, "compression_ratio": 2.1493212669683257, "no_speech_prob": 0.009702973999083042}, {"id": 3, "seek": 0, "start": 16.44, "end": 21.28, "text": " logistic regression will also look similar to the update for regularized linear regression.", "tokens": [50364, 682, 341, 960, 11, 291, 536, 577, 281, 4445, 3890, 1602, 3565, 3142, 24590, 13, 50684, 50684, 1449, 382, 264, 16235, 5623, 337, 3565, 3142, 24590, 575, 6576, 17600, 2531, 281, 264, 16235, 50954, 50954, 5623, 337, 8213, 24590, 11, 291, 915, 300, 264, 16235, 23475, 5623, 337, 3890, 1602, 51186, 51186, 3565, 3142, 24590, 486, 611, 574, 2531, 281, 264, 5623, 337, 3890, 1602, 8213, 24590, 13, 51428, 51428, 961, 311, 747, 257, 574, 13, 51504, 51504, 1692, 311, 264, 1558, 13, 51580, 51580, 492, 1866, 3071, 300, 3565, 3142, 24590, 393, 312, 25806, 281, 670, 69, 2414, 498, 291, 3318, 309, 365, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.13374515816017432, "compression_ratio": 2.1493212669683257, "no_speech_prob": 0.009702973999083042}, {"id": 4, "seek": 0, "start": 21.28, "end": 22.8, "text": " Let's take a look.", "tokens": [50364, 682, 341, 960, 11, 291, 536, 577, 281, 4445, 3890, 1602, 3565, 3142, 24590, 13, 50684, 50684, 1449, 382, 264, 16235, 5623, 337, 3565, 3142, 24590, 575, 6576, 17600, 2531, 281, 264, 16235, 50954, 50954, 5623, 337, 8213, 24590, 11, 291, 915, 300, 264, 16235, 23475, 5623, 337, 3890, 1602, 51186, 51186, 3565, 3142, 24590, 486, 611, 574, 2531, 281, 264, 5623, 337, 3890, 1602, 8213, 24590, 13, 51428, 51428, 961, 311, 747, 257, 574, 13, 51504, 51504, 1692, 311, 264, 1558, 13, 51580, 51580, 492, 1866, 3071, 300, 3565, 3142, 24590, 393, 312, 25806, 281, 670, 69, 2414, 498, 291, 3318, 309, 365, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.13374515816017432, "compression_ratio": 2.1493212669683257, "no_speech_prob": 0.009702973999083042}, {"id": 5, "seek": 0, "start": 22.8, "end": 24.32, "text": " Here's the idea.", "tokens": [50364, 682, 341, 960, 11, 291, 536, 577, 281, 4445, 3890, 1602, 3565, 3142, 24590, 13, 50684, 50684, 1449, 382, 264, 16235, 5623, 337, 3565, 3142, 24590, 575, 6576, 17600, 2531, 281, 264, 16235, 50954, 50954, 5623, 337, 8213, 24590, 11, 291, 915, 300, 264, 16235, 23475, 5623, 337, 3890, 1602, 51186, 51186, 3565, 3142, 24590, 486, 611, 574, 2531, 281, 264, 5623, 337, 3890, 1602, 8213, 24590, 13, 51428, 51428, 961, 311, 747, 257, 574, 13, 51504, 51504, 1692, 311, 264, 1558, 13, 51580, 51580, 492, 1866, 3071, 300, 3565, 3142, 24590, 393, 312, 25806, 281, 670, 69, 2414, 498, 291, 3318, 309, 365, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.13374515816017432, "compression_ratio": 2.1493212669683257, "no_speech_prob": 0.009702973999083042}, {"id": 6, "seek": 0, "start": 24.32, "end": 28.92, "text": " We saw earlier that logistic regression can be prone to overfitting if you fit it with", "tokens": [50364, 682, 341, 960, 11, 291, 536, 577, 281, 4445, 3890, 1602, 3565, 3142, 24590, 13, 50684, 50684, 1449, 382, 264, 16235, 5623, 337, 3565, 3142, 24590, 575, 6576, 17600, 2531, 281, 264, 16235, 50954, 50954, 5623, 337, 8213, 24590, 11, 291, 915, 300, 264, 16235, 23475, 5623, 337, 3890, 1602, 51186, 51186, 3565, 3142, 24590, 486, 611, 574, 2531, 281, 264, 5623, 337, 3890, 1602, 8213, 24590, 13, 51428, 51428, 961, 311, 747, 257, 574, 13, 51504, 51504, 1692, 311, 264, 1558, 13, 51580, 51580, 492, 1866, 3071, 300, 3565, 3142, 24590, 393, 312, 25806, 281, 670, 69, 2414, 498, 291, 3318, 309, 365, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.13374515816017432, "compression_ratio": 2.1493212669683257, "no_speech_prob": 0.009702973999083042}, {"id": 7, "seek": 2892, "start": 28.92, "end": 32.96, "text": " very high-order polynomial features like this.", "tokens": [50364, 588, 1090, 12, 4687, 26110, 4122, 411, 341, 13, 50566, 50566, 1692, 710, 307, 257, 1090, 12, 4687, 26110, 300, 2170, 4678, 666, 264, 4556, 3280, 327, 2445, 11, 411, 370, 11, 50918, 50918, 281, 14722, 283, 13, 50968, 50968, 400, 294, 1729, 11, 291, 393, 917, 493, 365, 257, 3537, 12866, 300, 307, 24324, 3997, 293, 670, 13979, 51298, 51298, 341, 3097, 992, 13, 51442, 51442, 5048, 5101, 11, 562, 291, 3847, 3565, 3142, 24590, 365, 257, 688, 295, 4122, 11, 1968, 26110, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13649307597767224, "compression_ratio": 1.658878504672897, "no_speech_prob": 2.078442048514262e-05}, {"id": 8, "seek": 2892, "start": 32.96, "end": 40.0, "text": " Here z is a high-order polynomial that gets passed into the sigmoid function, like so,", "tokens": [50364, 588, 1090, 12, 4687, 26110, 4122, 411, 341, 13, 50566, 50566, 1692, 710, 307, 257, 1090, 12, 4687, 26110, 300, 2170, 4678, 666, 264, 4556, 3280, 327, 2445, 11, 411, 370, 11, 50918, 50918, 281, 14722, 283, 13, 50968, 50968, 400, 294, 1729, 11, 291, 393, 917, 493, 365, 257, 3537, 12866, 300, 307, 24324, 3997, 293, 670, 13979, 51298, 51298, 341, 3097, 992, 13, 51442, 51442, 5048, 5101, 11, 562, 291, 3847, 3565, 3142, 24590, 365, 257, 688, 295, 4122, 11, 1968, 26110, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13649307597767224, "compression_ratio": 1.658878504672897, "no_speech_prob": 2.078442048514262e-05}, {"id": 9, "seek": 2892, "start": 40.0, "end": 41.0, "text": " to compute f.", "tokens": [50364, 588, 1090, 12, 4687, 26110, 4122, 411, 341, 13, 50566, 50566, 1692, 710, 307, 257, 1090, 12, 4687, 26110, 300, 2170, 4678, 666, 264, 4556, 3280, 327, 2445, 11, 411, 370, 11, 50918, 50918, 281, 14722, 283, 13, 50968, 50968, 400, 294, 1729, 11, 291, 393, 917, 493, 365, 257, 3537, 12866, 300, 307, 24324, 3997, 293, 670, 13979, 51298, 51298, 341, 3097, 992, 13, 51442, 51442, 5048, 5101, 11, 562, 291, 3847, 3565, 3142, 24590, 365, 257, 688, 295, 4122, 11, 1968, 26110, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13649307597767224, "compression_ratio": 1.658878504672897, "no_speech_prob": 2.078442048514262e-05}, {"id": 10, "seek": 2892, "start": 41.0, "end": 47.6, "text": " And in particular, you can end up with a decision boundary that is overly complex and overfits", "tokens": [50364, 588, 1090, 12, 4687, 26110, 4122, 411, 341, 13, 50566, 50566, 1692, 710, 307, 257, 1090, 12, 4687, 26110, 300, 2170, 4678, 666, 264, 4556, 3280, 327, 2445, 11, 411, 370, 11, 50918, 50918, 281, 14722, 283, 13, 50968, 50968, 400, 294, 1729, 11, 291, 393, 917, 493, 365, 257, 3537, 12866, 300, 307, 24324, 3997, 293, 670, 13979, 51298, 51298, 341, 3097, 992, 13, 51442, 51442, 5048, 5101, 11, 562, 291, 3847, 3565, 3142, 24590, 365, 257, 688, 295, 4122, 11, 1968, 26110, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13649307597767224, "compression_ratio": 1.658878504672897, "no_speech_prob": 2.078442048514262e-05}, {"id": 11, "seek": 2892, "start": 47.6, "end": 50.480000000000004, "text": " this training set.", "tokens": [50364, 588, 1090, 12, 4687, 26110, 4122, 411, 341, 13, 50566, 50566, 1692, 710, 307, 257, 1090, 12, 4687, 26110, 300, 2170, 4678, 666, 264, 4556, 3280, 327, 2445, 11, 411, 370, 11, 50918, 50918, 281, 14722, 283, 13, 50968, 50968, 400, 294, 1729, 11, 291, 393, 917, 493, 365, 257, 3537, 12866, 300, 307, 24324, 3997, 293, 670, 13979, 51298, 51298, 341, 3097, 992, 13, 51442, 51442, 5048, 5101, 11, 562, 291, 3847, 3565, 3142, 24590, 365, 257, 688, 295, 4122, 11, 1968, 26110, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13649307597767224, "compression_ratio": 1.658878504672897, "no_speech_prob": 2.078442048514262e-05}, {"id": 12, "seek": 2892, "start": 50.480000000000004, "end": 55.84, "text": " More generally, when you train logistic regression with a lot of features, whether polynomial", "tokens": [50364, 588, 1090, 12, 4687, 26110, 4122, 411, 341, 13, 50566, 50566, 1692, 710, 307, 257, 1090, 12, 4687, 26110, 300, 2170, 4678, 666, 264, 4556, 3280, 327, 2445, 11, 411, 370, 11, 50918, 50918, 281, 14722, 283, 13, 50968, 50968, 400, 294, 1729, 11, 291, 393, 917, 493, 365, 257, 3537, 12866, 300, 307, 24324, 3997, 293, 670, 13979, 51298, 51298, 341, 3097, 992, 13, 51442, 51442, 5048, 5101, 11, 562, 291, 3847, 3565, 3142, 24590, 365, 257, 688, 295, 4122, 11, 1968, 26110, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13649307597767224, "compression_ratio": 1.658878504672897, "no_speech_prob": 2.078442048514262e-05}, {"id": 13, "seek": 5584, "start": 55.84, "end": 62.28, "text": " features or some other features, there can be a higher risk of overfitting.", "tokens": [50364, 4122, 420, 512, 661, 4122, 11, 456, 393, 312, 257, 2946, 3148, 295, 670, 69, 2414, 13, 50686, 50686, 639, 390, 264, 2063, 2445, 337, 3565, 3142, 24590, 13, 50866, 50866, 759, 291, 528, 281, 16927, 309, 281, 764, 3890, 2144, 11, 439, 291, 643, 281, 360, 307, 909, 281, 309, 264, 3480, 51212, 51212, 1433, 13, 51262, 51262, 961, 311, 909, 13607, 11, 264, 3890, 2144, 13075, 11, 670, 568, 76, 11, 1413, 264, 2408, 490, 361, 6915, 502, 807, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.11333804411046645, "compression_ratio": 1.5631067961165048, "no_speech_prob": 4.092873496119864e-06}, {"id": 14, "seek": 5584, "start": 62.28, "end": 65.88000000000001, "text": " This was the cost function for logistic regression.", "tokens": [50364, 4122, 420, 512, 661, 4122, 11, 456, 393, 312, 257, 2946, 3148, 295, 670, 69, 2414, 13, 50686, 50686, 639, 390, 264, 2063, 2445, 337, 3565, 3142, 24590, 13, 50866, 50866, 759, 291, 528, 281, 16927, 309, 281, 764, 3890, 2144, 11, 439, 291, 643, 281, 360, 307, 909, 281, 309, 264, 3480, 51212, 51212, 1433, 13, 51262, 51262, 961, 311, 909, 13607, 11, 264, 3890, 2144, 13075, 11, 670, 568, 76, 11, 1413, 264, 2408, 490, 361, 6915, 502, 807, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.11333804411046645, "compression_ratio": 1.5631067961165048, "no_speech_prob": 4.092873496119864e-06}, {"id": 15, "seek": 5584, "start": 65.88000000000001, "end": 72.80000000000001, "text": " If you want to modify it to use regularization, all you need to do is add to it the following", "tokens": [50364, 4122, 420, 512, 661, 4122, 11, 456, 393, 312, 257, 2946, 3148, 295, 670, 69, 2414, 13, 50686, 50686, 639, 390, 264, 2063, 2445, 337, 3565, 3142, 24590, 13, 50866, 50866, 759, 291, 528, 281, 16927, 309, 281, 764, 3890, 2144, 11, 439, 291, 643, 281, 360, 307, 909, 281, 309, 264, 3480, 51212, 51212, 1433, 13, 51262, 51262, 961, 311, 909, 13607, 11, 264, 3890, 2144, 13075, 11, 670, 568, 76, 11, 1413, 264, 2408, 490, 361, 6915, 502, 807, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.11333804411046645, "compression_ratio": 1.5631067961165048, "no_speech_prob": 4.092873496119864e-06}, {"id": 16, "seek": 5584, "start": 72.80000000000001, "end": 73.80000000000001, "text": " term.", "tokens": [50364, 4122, 420, 512, 661, 4122, 11, 456, 393, 312, 257, 2946, 3148, 295, 670, 69, 2414, 13, 50686, 50686, 639, 390, 264, 2063, 2445, 337, 3565, 3142, 24590, 13, 50866, 50866, 759, 291, 528, 281, 16927, 309, 281, 764, 3890, 2144, 11, 439, 291, 643, 281, 360, 307, 909, 281, 309, 264, 3480, 51212, 51212, 1433, 13, 51262, 51262, 961, 311, 909, 13607, 11, 264, 3890, 2144, 13075, 11, 670, 568, 76, 11, 1413, 264, 2408, 490, 361, 6915, 502, 807, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.11333804411046645, "compression_ratio": 1.5631067961165048, "no_speech_prob": 4.092873496119864e-06}, {"id": 17, "seek": 5584, "start": 73.80000000000001, "end": 81.2, "text": " Let's add lambda, the regularization parameter, over 2m, times the sum from j equals 1 through", "tokens": [50364, 4122, 420, 512, 661, 4122, 11, 456, 393, 312, 257, 2946, 3148, 295, 670, 69, 2414, 13, 50686, 50686, 639, 390, 264, 2063, 2445, 337, 3565, 3142, 24590, 13, 50866, 50866, 759, 291, 528, 281, 16927, 309, 281, 764, 3890, 2144, 11, 439, 291, 643, 281, 360, 307, 909, 281, 309, 264, 3480, 51212, 51212, 1433, 13, 51262, 51262, 961, 311, 909, 13607, 11, 264, 3890, 2144, 13075, 11, 670, 568, 76, 11, 1413, 264, 2408, 490, 361, 6915, 502, 807, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.11333804411046645, "compression_ratio": 1.5631067961165048, "no_speech_prob": 4.092873496119864e-06}, {"id": 18, "seek": 8120, "start": 81.2, "end": 86.72, "text": " n, where n is the number of features as usual, of wj squared.", "tokens": [50364, 297, 11, 689, 297, 307, 264, 1230, 295, 4122, 382, 7713, 11, 295, 261, 73, 8889, 13, 50640, 50640, 407, 562, 291, 17522, 341, 2063, 2445, 382, 257, 2445, 295, 261, 293, 272, 11, 309, 575, 264, 1802, 295, 50910, 50910, 13661, 3319, 9834, 261, 16, 11, 261, 17, 11, 807, 45368, 293, 19965, 552, 490, 885, 886, 2416, 13, 51290, 51290, 400, 498, 291, 360, 341, 11, 550, 754, 1673, 291, 434, 15669, 257, 1090, 12, 4687, 26110, 365, 257, 688, 51534, 51534, 295, 9834, 11, 291, 920, 483, 257, 3537, 12866, 300, 1542, 411, 341, 11, 746, 300, 1542, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.10381754920596169, "compression_ratio": 1.6334661354581674, "no_speech_prob": 3.3931298730749404e-06}, {"id": 19, "seek": 8120, "start": 86.72, "end": 92.12, "text": " So when you minimize this cost function as a function of w and b, it has the effect of", "tokens": [50364, 297, 11, 689, 297, 307, 264, 1230, 295, 4122, 382, 7713, 11, 295, 261, 73, 8889, 13, 50640, 50640, 407, 562, 291, 17522, 341, 2063, 2445, 382, 257, 2445, 295, 261, 293, 272, 11, 309, 575, 264, 1802, 295, 50910, 50910, 13661, 3319, 9834, 261, 16, 11, 261, 17, 11, 807, 45368, 293, 19965, 552, 490, 885, 886, 2416, 13, 51290, 51290, 400, 498, 291, 360, 341, 11, 550, 754, 1673, 291, 434, 15669, 257, 1090, 12, 4687, 26110, 365, 257, 688, 51534, 51534, 295, 9834, 11, 291, 920, 483, 257, 3537, 12866, 300, 1542, 411, 341, 11, 746, 300, 1542, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.10381754920596169, "compression_ratio": 1.6334661354581674, "no_speech_prob": 3.3931298730749404e-06}, {"id": 20, "seek": 8120, "start": 92.12, "end": 99.72, "text": " penalizing parameters w1, w2, through wn and preventing them from being too large.", "tokens": [50364, 297, 11, 689, 297, 307, 264, 1230, 295, 4122, 382, 7713, 11, 295, 261, 73, 8889, 13, 50640, 50640, 407, 562, 291, 17522, 341, 2063, 2445, 382, 257, 2445, 295, 261, 293, 272, 11, 309, 575, 264, 1802, 295, 50910, 50910, 13661, 3319, 9834, 261, 16, 11, 261, 17, 11, 807, 45368, 293, 19965, 552, 490, 885, 886, 2416, 13, 51290, 51290, 400, 498, 291, 360, 341, 11, 550, 754, 1673, 291, 434, 15669, 257, 1090, 12, 4687, 26110, 365, 257, 688, 51534, 51534, 295, 9834, 11, 291, 920, 483, 257, 3537, 12866, 300, 1542, 411, 341, 11, 746, 300, 1542, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.10381754920596169, "compression_ratio": 1.6334661354581674, "no_speech_prob": 3.3931298730749404e-06}, {"id": 21, "seek": 8120, "start": 99.72, "end": 104.60000000000001, "text": " And if you do this, then even though you're fitting a high-order polynomial with a lot", "tokens": [50364, 297, 11, 689, 297, 307, 264, 1230, 295, 4122, 382, 7713, 11, 295, 261, 73, 8889, 13, 50640, 50640, 407, 562, 291, 17522, 341, 2063, 2445, 382, 257, 2445, 295, 261, 293, 272, 11, 309, 575, 264, 1802, 295, 50910, 50910, 13661, 3319, 9834, 261, 16, 11, 261, 17, 11, 807, 45368, 293, 19965, 552, 490, 885, 886, 2416, 13, 51290, 51290, 400, 498, 291, 360, 341, 11, 550, 754, 1673, 291, 434, 15669, 257, 1090, 12, 4687, 26110, 365, 257, 688, 51534, 51534, 295, 9834, 11, 291, 920, 483, 257, 3537, 12866, 300, 1542, 411, 341, 11, 746, 300, 1542, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.10381754920596169, "compression_ratio": 1.6334661354581674, "no_speech_prob": 3.3931298730749404e-06}, {"id": 22, "seek": 8120, "start": 104.60000000000001, "end": 110.24000000000001, "text": " of parameters, you still get a decision boundary that looks like this, something that looks", "tokens": [50364, 297, 11, 689, 297, 307, 264, 1230, 295, 4122, 382, 7713, 11, 295, 261, 73, 8889, 13, 50640, 50640, 407, 562, 291, 17522, 341, 2063, 2445, 382, 257, 2445, 295, 261, 293, 272, 11, 309, 575, 264, 1802, 295, 50910, 50910, 13661, 3319, 9834, 261, 16, 11, 261, 17, 11, 807, 45368, 293, 19965, 552, 490, 885, 886, 2416, 13, 51290, 51290, 400, 498, 291, 360, 341, 11, 550, 754, 1673, 291, 434, 15669, 257, 1090, 12, 4687, 26110, 365, 257, 688, 51534, 51534, 295, 9834, 11, 291, 920, 483, 257, 3537, 12866, 300, 1542, 411, 341, 11, 746, 300, 1542, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.10381754920596169, "compression_ratio": 1.6334661354581674, "no_speech_prob": 3.3931298730749404e-06}, {"id": 23, "seek": 11024, "start": 110.24, "end": 116.08, "text": " more reasonable for separating positive and negative examples while also generalizing,", "tokens": [50364, 544, 10585, 337, 29279, 3353, 293, 3671, 5110, 1339, 611, 2674, 3319, 11, 50656, 50656, 4696, 11, 281, 777, 5110, 406, 294, 264, 3097, 992, 13, 50874, 50874, 407, 562, 1228, 3890, 2144, 11, 754, 562, 291, 362, 257, 688, 295, 4122, 11, 577, 393, 291, 767, 51164, 51164, 4445, 341, 30, 51220, 51220, 1012, 393, 291, 767, 17522, 341, 2063, 2445, 361, 295, 261, 65, 300, 5974, 264, 3890, 2144, 1433, 30, 51524, 51524, 1042, 11, 718, 311, 764, 16235, 23475, 382, 949, 13, 51741, 51741], "temperature": 0.0, "avg_logprob": -0.12040171462498354, "compression_ratio": 1.6367521367521367, "no_speech_prob": 2.857291519831051e-06}, {"id": 24, "seek": 11024, "start": 116.08, "end": 120.44, "text": " hopefully, to new examples not in the training set.", "tokens": [50364, 544, 10585, 337, 29279, 3353, 293, 3671, 5110, 1339, 611, 2674, 3319, 11, 50656, 50656, 4696, 11, 281, 777, 5110, 406, 294, 264, 3097, 992, 13, 50874, 50874, 407, 562, 1228, 3890, 2144, 11, 754, 562, 291, 362, 257, 688, 295, 4122, 11, 577, 393, 291, 767, 51164, 51164, 4445, 341, 30, 51220, 51220, 1012, 393, 291, 767, 17522, 341, 2063, 2445, 361, 295, 261, 65, 300, 5974, 264, 3890, 2144, 1433, 30, 51524, 51524, 1042, 11, 718, 311, 764, 16235, 23475, 382, 949, 13, 51741, 51741], "temperature": 0.0, "avg_logprob": -0.12040171462498354, "compression_ratio": 1.6367521367521367, "no_speech_prob": 2.857291519831051e-06}, {"id": 25, "seek": 11024, "start": 120.44, "end": 126.24, "text": " So when using regularization, even when you have a lot of features, how can you actually", "tokens": [50364, 544, 10585, 337, 29279, 3353, 293, 3671, 5110, 1339, 611, 2674, 3319, 11, 50656, 50656, 4696, 11, 281, 777, 5110, 406, 294, 264, 3097, 992, 13, 50874, 50874, 407, 562, 1228, 3890, 2144, 11, 754, 562, 291, 362, 257, 688, 295, 4122, 11, 577, 393, 291, 767, 51164, 51164, 4445, 341, 30, 51220, 51220, 1012, 393, 291, 767, 17522, 341, 2063, 2445, 361, 295, 261, 65, 300, 5974, 264, 3890, 2144, 1433, 30, 51524, 51524, 1042, 11, 718, 311, 764, 16235, 23475, 382, 949, 13, 51741, 51741], "temperature": 0.0, "avg_logprob": -0.12040171462498354, "compression_ratio": 1.6367521367521367, "no_speech_prob": 2.857291519831051e-06}, {"id": 26, "seek": 11024, "start": 126.24, "end": 127.36, "text": " implement this?", "tokens": [50364, 544, 10585, 337, 29279, 3353, 293, 3671, 5110, 1339, 611, 2674, 3319, 11, 50656, 50656, 4696, 11, 281, 777, 5110, 406, 294, 264, 3097, 992, 13, 50874, 50874, 407, 562, 1228, 3890, 2144, 11, 754, 562, 291, 362, 257, 688, 295, 4122, 11, 577, 393, 291, 767, 51164, 51164, 4445, 341, 30, 51220, 51220, 1012, 393, 291, 767, 17522, 341, 2063, 2445, 361, 295, 261, 65, 300, 5974, 264, 3890, 2144, 1433, 30, 51524, 51524, 1042, 11, 718, 311, 764, 16235, 23475, 382, 949, 13, 51741, 51741], "temperature": 0.0, "avg_logprob": -0.12040171462498354, "compression_ratio": 1.6367521367521367, "no_speech_prob": 2.857291519831051e-06}, {"id": 27, "seek": 11024, "start": 127.36, "end": 133.44, "text": " How can you actually minimize this cost function j of wb that includes the regularization term?", "tokens": [50364, 544, 10585, 337, 29279, 3353, 293, 3671, 5110, 1339, 611, 2674, 3319, 11, 50656, 50656, 4696, 11, 281, 777, 5110, 406, 294, 264, 3097, 992, 13, 50874, 50874, 407, 562, 1228, 3890, 2144, 11, 754, 562, 291, 362, 257, 688, 295, 4122, 11, 577, 393, 291, 767, 51164, 51164, 4445, 341, 30, 51220, 51220, 1012, 393, 291, 767, 17522, 341, 2063, 2445, 361, 295, 261, 65, 300, 5974, 264, 3890, 2144, 1433, 30, 51524, 51524, 1042, 11, 718, 311, 764, 16235, 23475, 382, 949, 13, 51741, 51741], "temperature": 0.0, "avg_logprob": -0.12040171462498354, "compression_ratio": 1.6367521367521367, "no_speech_prob": 2.857291519831051e-06}, {"id": 28, "seek": 11024, "start": 133.44, "end": 137.78, "text": " Well, let's use gradient descent as before.", "tokens": [50364, 544, 10585, 337, 29279, 3353, 293, 3671, 5110, 1339, 611, 2674, 3319, 11, 50656, 50656, 4696, 11, 281, 777, 5110, 406, 294, 264, 3097, 992, 13, 50874, 50874, 407, 562, 1228, 3890, 2144, 11, 754, 562, 291, 362, 257, 688, 295, 4122, 11, 577, 393, 291, 767, 51164, 51164, 4445, 341, 30, 51220, 51220, 1012, 393, 291, 767, 17522, 341, 2063, 2445, 361, 295, 261, 65, 300, 5974, 264, 3890, 2144, 1433, 30, 51524, 51524, 1042, 11, 718, 311, 764, 16235, 23475, 382, 949, 13, 51741, 51741], "temperature": 0.0, "avg_logprob": -0.12040171462498354, "compression_ratio": 1.6367521367521367, "no_speech_prob": 2.857291519831051e-06}, {"id": 29, "seek": 13778, "start": 137.78, "end": 141.68, "text": " So here's the cost function that you want to minimize.", "tokens": [50364, 407, 510, 311, 264, 2063, 2445, 300, 291, 528, 281, 17522, 13, 50559, 50559, 400, 281, 4445, 16235, 23475, 11, 382, 949, 11, 321, 603, 3985, 484, 264, 3480, 46218, 50795, 50795, 9205, 670, 261, 73, 293, 272, 13, 51059, 51059, 1981, 366, 264, 7713, 5623, 4474, 337, 16235, 23475, 13, 51231, 51231, 400, 445, 411, 3890, 1602, 8213, 24590, 11, 562, 291, 14722, 437, 366, 613, 13760, 51519, 51519, 2115, 11, 264, 787, 551, 300, 2962, 586, 307, 300, 264, 13760, 365, 3104, 281, 261, 73, 2170, 51823, 51823], "temperature": 0.0, "avg_logprob": -0.09321884486986243, "compression_ratio": 1.625514403292181, "no_speech_prob": 1.6536756675122888e-06}, {"id": 30, "seek": 13778, "start": 141.68, "end": 146.4, "text": " And to implement gradient descent, as before, we'll carry out the following simultaneous", "tokens": [50364, 407, 510, 311, 264, 2063, 2445, 300, 291, 528, 281, 17522, 13, 50559, 50559, 400, 281, 4445, 16235, 23475, 11, 382, 949, 11, 321, 603, 3985, 484, 264, 3480, 46218, 50795, 50795, 9205, 670, 261, 73, 293, 272, 13, 51059, 51059, 1981, 366, 264, 7713, 5623, 4474, 337, 16235, 23475, 13, 51231, 51231, 400, 445, 411, 3890, 1602, 8213, 24590, 11, 562, 291, 14722, 437, 366, 613, 13760, 51519, 51519, 2115, 11, 264, 787, 551, 300, 2962, 586, 307, 300, 264, 13760, 365, 3104, 281, 261, 73, 2170, 51823, 51823], "temperature": 0.0, "avg_logprob": -0.09321884486986243, "compression_ratio": 1.625514403292181, "no_speech_prob": 1.6536756675122888e-06}, {"id": 31, "seek": 13778, "start": 146.4, "end": 151.68, "text": " updates over wj and b.", "tokens": [50364, 407, 510, 311, 264, 2063, 2445, 300, 291, 528, 281, 17522, 13, 50559, 50559, 400, 281, 4445, 16235, 23475, 11, 382, 949, 11, 321, 603, 3985, 484, 264, 3480, 46218, 50795, 50795, 9205, 670, 261, 73, 293, 272, 13, 51059, 51059, 1981, 366, 264, 7713, 5623, 4474, 337, 16235, 23475, 13, 51231, 51231, 400, 445, 411, 3890, 1602, 8213, 24590, 11, 562, 291, 14722, 437, 366, 613, 13760, 51519, 51519, 2115, 11, 264, 787, 551, 300, 2962, 586, 307, 300, 264, 13760, 365, 3104, 281, 261, 73, 2170, 51823, 51823], "temperature": 0.0, "avg_logprob": -0.09321884486986243, "compression_ratio": 1.625514403292181, "no_speech_prob": 1.6536756675122888e-06}, {"id": 32, "seek": 13778, "start": 151.68, "end": 155.12, "text": " These are the usual update rules for gradient descent.", "tokens": [50364, 407, 510, 311, 264, 2063, 2445, 300, 291, 528, 281, 17522, 13, 50559, 50559, 400, 281, 4445, 16235, 23475, 11, 382, 949, 11, 321, 603, 3985, 484, 264, 3480, 46218, 50795, 50795, 9205, 670, 261, 73, 293, 272, 13, 51059, 51059, 1981, 366, 264, 7713, 5623, 4474, 337, 16235, 23475, 13, 51231, 51231, 400, 445, 411, 3890, 1602, 8213, 24590, 11, 562, 291, 14722, 437, 366, 613, 13760, 51519, 51519, 2115, 11, 264, 787, 551, 300, 2962, 586, 307, 300, 264, 13760, 365, 3104, 281, 261, 73, 2170, 51823, 51823], "temperature": 0.0, "avg_logprob": -0.09321884486986243, "compression_ratio": 1.625514403292181, "no_speech_prob": 1.6536756675122888e-06}, {"id": 33, "seek": 13778, "start": 155.12, "end": 160.88, "text": " And just like regularized linear regression, when you compute what are these derivative", "tokens": [50364, 407, 510, 311, 264, 2063, 2445, 300, 291, 528, 281, 17522, 13, 50559, 50559, 400, 281, 4445, 16235, 23475, 11, 382, 949, 11, 321, 603, 3985, 484, 264, 3480, 46218, 50795, 50795, 9205, 670, 261, 73, 293, 272, 13, 51059, 51059, 1981, 366, 264, 7713, 5623, 4474, 337, 16235, 23475, 13, 51231, 51231, 400, 445, 411, 3890, 1602, 8213, 24590, 11, 562, 291, 14722, 437, 366, 613, 13760, 51519, 51519, 2115, 11, 264, 787, 551, 300, 2962, 586, 307, 300, 264, 13760, 365, 3104, 281, 261, 73, 2170, 51823, 51823], "temperature": 0.0, "avg_logprob": -0.09321884486986243, "compression_ratio": 1.625514403292181, "no_speech_prob": 1.6536756675122888e-06}, {"id": 34, "seek": 13778, "start": 160.88, "end": 166.96, "text": " terms, the only thing that changes now is that the derivative with respect to wj gets", "tokens": [50364, 407, 510, 311, 264, 2063, 2445, 300, 291, 528, 281, 17522, 13, 50559, 50559, 400, 281, 4445, 16235, 23475, 11, 382, 949, 11, 321, 603, 3985, 484, 264, 3480, 46218, 50795, 50795, 9205, 670, 261, 73, 293, 272, 13, 51059, 51059, 1981, 366, 264, 7713, 5623, 4474, 337, 16235, 23475, 13, 51231, 51231, 400, 445, 411, 3890, 1602, 8213, 24590, 11, 562, 291, 14722, 437, 366, 613, 13760, 51519, 51519, 2115, 11, 264, 787, 551, 300, 2962, 586, 307, 300, 264, 13760, 365, 3104, 281, 261, 73, 2170, 51823, 51823], "temperature": 0.0, "avg_logprob": -0.09321884486986243, "compression_ratio": 1.625514403292181, "no_speech_prob": 1.6536756675122888e-06}, {"id": 35, "seek": 16696, "start": 166.96, "end": 174.96, "text": " this additional term, lambda over m times wj added here at the end.", "tokens": [50364, 341, 4497, 1433, 11, 13607, 670, 275, 1413, 261, 73, 3869, 510, 412, 264, 917, 13, 50764, 50764, 400, 797, 11, 309, 1542, 257, 688, 411, 264, 5623, 337, 3890, 1602, 8213, 24590, 13, 51012, 51012, 682, 1186, 11, 309, 311, 264, 1900, 912, 5367, 11, 3993, 337, 264, 1186, 300, 264, 7123, 295, 283, 307, 586, 51290, 51290, 572, 2854, 264, 8213, 2445, 11, 309, 307, 264, 3565, 3142, 2445, 6456, 281, 710, 13, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.11307630659658698, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.844904313154984e-06}, {"id": 36, "seek": 16696, "start": 174.96, "end": 179.92000000000002, "text": " And again, it looks a lot like the update for regularized linear regression.", "tokens": [50364, 341, 4497, 1433, 11, 13607, 670, 275, 1413, 261, 73, 3869, 510, 412, 264, 917, 13, 50764, 50764, 400, 797, 11, 309, 1542, 257, 688, 411, 264, 5623, 337, 3890, 1602, 8213, 24590, 13, 51012, 51012, 682, 1186, 11, 309, 311, 264, 1900, 912, 5367, 11, 3993, 337, 264, 1186, 300, 264, 7123, 295, 283, 307, 586, 51290, 51290, 572, 2854, 264, 8213, 2445, 11, 309, 307, 264, 3565, 3142, 2445, 6456, 281, 710, 13, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.11307630659658698, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.844904313154984e-06}, {"id": 37, "seek": 16696, "start": 179.92000000000002, "end": 185.48000000000002, "text": " In fact, it's the exact same equation, except for the fact that the definition of f is now", "tokens": [50364, 341, 4497, 1433, 11, 13607, 670, 275, 1413, 261, 73, 3869, 510, 412, 264, 917, 13, 50764, 50764, 400, 797, 11, 309, 1542, 257, 688, 411, 264, 5623, 337, 3890, 1602, 8213, 24590, 13, 51012, 51012, 682, 1186, 11, 309, 311, 264, 1900, 912, 5367, 11, 3993, 337, 264, 1186, 300, 264, 7123, 295, 283, 307, 586, 51290, 51290, 572, 2854, 264, 8213, 2445, 11, 309, 307, 264, 3565, 3142, 2445, 6456, 281, 710, 13, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.11307630659658698, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.844904313154984e-06}, {"id": 38, "seek": 16696, "start": 185.48000000000002, "end": 192.32, "text": " no longer the linear function, it is the logistic function applied to z.", "tokens": [50364, 341, 4497, 1433, 11, 13607, 670, 275, 1413, 261, 73, 3869, 510, 412, 264, 917, 13, 50764, 50764, 400, 797, 11, 309, 1542, 257, 688, 411, 264, 5623, 337, 3890, 1602, 8213, 24590, 13, 51012, 51012, 682, 1186, 11, 309, 311, 264, 1900, 912, 5367, 11, 3993, 337, 264, 1186, 300, 264, 7123, 295, 283, 307, 586, 51290, 51290, 572, 2854, 264, 8213, 2445, 11, 309, 307, 264, 3565, 3142, 2445, 6456, 281, 710, 13, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.11307630659658698, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.844904313154984e-06}, {"id": 39, "seek": 19232, "start": 192.32, "end": 197.64, "text": " And similar to linear regression, we will regularize only the parameters wj, but not", "tokens": [50364, 400, 2531, 281, 8213, 24590, 11, 321, 486, 3890, 1125, 787, 264, 9834, 261, 73, 11, 457, 406, 50630, 50630, 264, 13075, 272, 11, 597, 307, 983, 456, 311, 572, 1319, 281, 264, 5623, 291, 576, 652, 337, 272, 13, 50974, 50974, 682, 264, 2572, 17312, 2715, 295, 341, 1243, 11, 291, 32676, 670, 69, 2414, 13, 51264, 51264, 400, 294, 264, 15141, 7542, 294, 264, 17312, 2715, 11, 291, 393, 586, 2826, 281, 3890, 1125, 428, 51542, 51542], "temperature": 0.0, "avg_logprob": -0.11122039512351707, "compression_ratio": 1.6161616161616161, "no_speech_prob": 3.187521770087187e-06}, {"id": 40, "seek": 19232, "start": 197.64, "end": 204.51999999999998, "text": " the parameter b, which is why there's no change to the update you would make for b.", "tokens": [50364, 400, 2531, 281, 8213, 24590, 11, 321, 486, 3890, 1125, 787, 264, 9834, 261, 73, 11, 457, 406, 50630, 50630, 264, 13075, 272, 11, 597, 307, 983, 456, 311, 572, 1319, 281, 264, 5623, 291, 576, 652, 337, 272, 13, 50974, 50974, 682, 264, 2572, 17312, 2715, 295, 341, 1243, 11, 291, 32676, 670, 69, 2414, 13, 51264, 51264, 400, 294, 264, 15141, 7542, 294, 264, 17312, 2715, 11, 291, 393, 586, 2826, 281, 3890, 1125, 428, 51542, 51542], "temperature": 0.0, "avg_logprob": -0.11122039512351707, "compression_ratio": 1.6161616161616161, "no_speech_prob": 3.187521770087187e-06}, {"id": 41, "seek": 19232, "start": 204.51999999999998, "end": 210.32, "text": " In the final optional lab of this week, you revisit overfitting.", "tokens": [50364, 400, 2531, 281, 8213, 24590, 11, 321, 486, 3890, 1125, 787, 264, 9834, 261, 73, 11, 457, 406, 50630, 50630, 264, 13075, 272, 11, 597, 307, 983, 456, 311, 572, 1319, 281, 264, 5623, 291, 576, 652, 337, 272, 13, 50974, 50974, 682, 264, 2572, 17312, 2715, 295, 341, 1243, 11, 291, 32676, 670, 69, 2414, 13, 51264, 51264, 400, 294, 264, 15141, 7542, 294, 264, 17312, 2715, 11, 291, 393, 586, 2826, 281, 3890, 1125, 428, 51542, 51542], "temperature": 0.0, "avg_logprob": -0.11122039512351707, "compression_ratio": 1.6161616161616161, "no_speech_prob": 3.187521770087187e-06}, {"id": 42, "seek": 19232, "start": 210.32, "end": 215.88, "text": " And in the interactive plot in the optional lab, you can now choose to regularize your", "tokens": [50364, 400, 2531, 281, 8213, 24590, 11, 321, 486, 3890, 1125, 787, 264, 9834, 261, 73, 11, 457, 406, 50630, 50630, 264, 13075, 272, 11, 597, 307, 983, 456, 311, 572, 1319, 281, 264, 5623, 291, 576, 652, 337, 272, 13, 50974, 50974, 682, 264, 2572, 17312, 2715, 295, 341, 1243, 11, 291, 32676, 670, 69, 2414, 13, 51264, 51264, 400, 294, 264, 15141, 7542, 294, 264, 17312, 2715, 11, 291, 393, 586, 2826, 281, 3890, 1125, 428, 51542, 51542], "temperature": 0.0, "avg_logprob": -0.11122039512351707, "compression_ratio": 1.6161616161616161, "no_speech_prob": 3.187521770087187e-06}, {"id": 43, "seek": 21588, "start": 215.88, "end": 222.6, "text": " models, both regression and classification, by enabling regularization during gradient descent,", "tokens": [50364, 5245, 11, 1293, 24590, 293, 21538, 11, 538, 23148, 3890, 2144, 1830, 16235, 23475, 11, 50700, 50700, 538, 18182, 257, 2158, 337, 13607, 13, 50844, 50844, 2555, 747, 257, 574, 412, 264, 3089, 337, 18114, 3890, 1602, 3565, 3142, 24590, 294, 1729, 11, 51100, 51100, 570, 291, 4445, 341, 294, 257, 3124, 2715, 1803, 412, 264, 917, 295, 341, 1243, 13, 51346, 51346, 407, 11, 586, 291, 458, 577, 281, 4445, 3890, 1602, 3565, 3142, 24590, 13, 51648, 51648, 1133, 286, 1792, 926, 25351, 10666, 11, 456, 366, 867, 11955, 1228, 3479, 2539, 281, 1884, 51863, 51863], "temperature": 0.0, "avg_logprob": -0.1332350933190548, "compression_ratio": 1.7769230769230768, "no_speech_prob": 8.013180377020035e-06}, {"id": 44, "seek": 21588, "start": 222.6, "end": 225.48, "text": " by selecting a value for lambda.", "tokens": [50364, 5245, 11, 1293, 24590, 293, 21538, 11, 538, 23148, 3890, 2144, 1830, 16235, 23475, 11, 50700, 50700, 538, 18182, 257, 2158, 337, 13607, 13, 50844, 50844, 2555, 747, 257, 574, 412, 264, 3089, 337, 18114, 3890, 1602, 3565, 3142, 24590, 294, 1729, 11, 51100, 51100, 570, 291, 4445, 341, 294, 257, 3124, 2715, 1803, 412, 264, 917, 295, 341, 1243, 13, 51346, 51346, 407, 11, 586, 291, 458, 577, 281, 4445, 3890, 1602, 3565, 3142, 24590, 13, 51648, 51648, 1133, 286, 1792, 926, 25351, 10666, 11, 456, 366, 867, 11955, 1228, 3479, 2539, 281, 1884, 51863, 51863], "temperature": 0.0, "avg_logprob": -0.1332350933190548, "compression_ratio": 1.7769230769230768, "no_speech_prob": 8.013180377020035e-06}, {"id": 45, "seek": 21588, "start": 225.48, "end": 230.6, "text": " Please take a look at the code for implementing regularized logistic regression in particular,", "tokens": [50364, 5245, 11, 1293, 24590, 293, 21538, 11, 538, 23148, 3890, 2144, 1830, 16235, 23475, 11, 50700, 50700, 538, 18182, 257, 2158, 337, 13607, 13, 50844, 50844, 2555, 747, 257, 574, 412, 264, 3089, 337, 18114, 3890, 1602, 3565, 3142, 24590, 294, 1729, 11, 51100, 51100, 570, 291, 4445, 341, 294, 257, 3124, 2715, 1803, 412, 264, 917, 295, 341, 1243, 13, 51346, 51346, 407, 11, 586, 291, 458, 577, 281, 4445, 3890, 1602, 3565, 3142, 24590, 13, 51648, 51648, 1133, 286, 1792, 926, 25351, 10666, 11, 456, 366, 867, 11955, 1228, 3479, 2539, 281, 1884, 51863, 51863], "temperature": 0.0, "avg_logprob": -0.1332350933190548, "compression_ratio": 1.7769230769230768, "no_speech_prob": 8.013180377020035e-06}, {"id": 46, "seek": 21588, "start": 230.6, "end": 235.51999999999998, "text": " because you implement this in a practice lab yourself at the end of this week.", "tokens": [50364, 5245, 11, 1293, 24590, 293, 21538, 11, 538, 23148, 3890, 2144, 1830, 16235, 23475, 11, 50700, 50700, 538, 18182, 257, 2158, 337, 13607, 13, 50844, 50844, 2555, 747, 257, 574, 412, 264, 3089, 337, 18114, 3890, 1602, 3565, 3142, 24590, 294, 1729, 11, 51100, 51100, 570, 291, 4445, 341, 294, 257, 3124, 2715, 1803, 412, 264, 917, 295, 341, 1243, 13, 51346, 51346, 407, 11, 586, 291, 458, 577, 281, 4445, 3890, 1602, 3565, 3142, 24590, 13, 51648, 51648, 1133, 286, 1792, 926, 25351, 10666, 11, 456, 366, 867, 11955, 1228, 3479, 2539, 281, 1884, 51863, 51863], "temperature": 0.0, "avg_logprob": -0.1332350933190548, "compression_ratio": 1.7769230769230768, "no_speech_prob": 8.013180377020035e-06}, {"id": 47, "seek": 21588, "start": 235.51999999999998, "end": 241.56, "text": " So, now you know how to implement regularized logistic regression.", "tokens": [50364, 5245, 11, 1293, 24590, 293, 21538, 11, 538, 23148, 3890, 2144, 1830, 16235, 23475, 11, 50700, 50700, 538, 18182, 257, 2158, 337, 13607, 13, 50844, 50844, 2555, 747, 257, 574, 412, 264, 3089, 337, 18114, 3890, 1602, 3565, 3142, 24590, 294, 1729, 11, 51100, 51100, 570, 291, 4445, 341, 294, 257, 3124, 2715, 1803, 412, 264, 917, 295, 341, 1243, 13, 51346, 51346, 407, 11, 586, 291, 458, 577, 281, 4445, 3890, 1602, 3565, 3142, 24590, 13, 51648, 51648, 1133, 286, 1792, 926, 25351, 10666, 11, 456, 366, 867, 11955, 1228, 3479, 2539, 281, 1884, 51863, 51863], "temperature": 0.0, "avg_logprob": -0.1332350933190548, "compression_ratio": 1.7769230769230768, "no_speech_prob": 8.013180377020035e-06}, {"id": 48, "seek": 21588, "start": 241.56, "end": 245.85999999999999, "text": " When I walk around Silicon Valley, there are many engineers using machine learning to create", "tokens": [50364, 5245, 11, 1293, 24590, 293, 21538, 11, 538, 23148, 3890, 2144, 1830, 16235, 23475, 11, 50700, 50700, 538, 18182, 257, 2158, 337, 13607, 13, 50844, 50844, 2555, 747, 257, 574, 412, 264, 3089, 337, 18114, 3890, 1602, 3565, 3142, 24590, 294, 1729, 11, 51100, 51100, 570, 291, 4445, 341, 294, 257, 3124, 2715, 1803, 412, 264, 917, 295, 341, 1243, 13, 51346, 51346, 407, 11, 586, 291, 458, 577, 281, 4445, 3890, 1602, 3565, 3142, 24590, 13, 51648, 51648, 1133, 286, 1792, 926, 25351, 10666, 11, 456, 366, 867, 11955, 1228, 3479, 2539, 281, 1884, 51863, 51863], "temperature": 0.0, "avg_logprob": -0.1332350933190548, "compression_ratio": 1.7769230769230768, "no_speech_prob": 8.013180377020035e-06}, {"id": 49, "seek": 24586, "start": 245.86, "end": 249.8, "text": " a ton of value, sometimes making a lot of money for the companies.", "tokens": [50364, 257, 2952, 295, 2158, 11, 2171, 1455, 257, 688, 295, 1460, 337, 264, 3431, 13, 50561, 50561, 400, 286, 458, 291, 600, 787, 668, 7601, 341, 1507, 337, 257, 1326, 3259, 13, 50769, 50769, 583, 498, 291, 1223, 293, 393, 3079, 8213, 24590, 293, 3565, 3142, 24590, 11, 300, 311, 51029, 51029, 767, 439, 291, 643, 281, 1884, 512, 588, 8263, 5821, 13, 51271, 51271, 3987, 264, 2685, 2539, 14642, 291, 764, 366, 1021, 11, 5276, 721, 411, 562, 51499, 51499, 293, 577, 281, 5407, 670, 69, 2414, 4523, 484, 281, 312, 472, 295, 264, 588, 8263, 3942, 294, 264, 51741, 51741, 957, 1002, 382, 731, 13, 51803, 51803], "temperature": 0.0, "avg_logprob": -0.10557481404897329, "compression_ratio": 1.651567944250871, "no_speech_prob": 1.5445813914993778e-05}, {"id": 50, "seek": 24586, "start": 249.8, "end": 253.96, "text": " And I know you've only been studying this stuff for a few weeks.", "tokens": [50364, 257, 2952, 295, 2158, 11, 2171, 1455, 257, 688, 295, 1460, 337, 264, 3431, 13, 50561, 50561, 400, 286, 458, 291, 600, 787, 668, 7601, 341, 1507, 337, 257, 1326, 3259, 13, 50769, 50769, 583, 498, 291, 1223, 293, 393, 3079, 8213, 24590, 293, 3565, 3142, 24590, 11, 300, 311, 51029, 51029, 767, 439, 291, 643, 281, 1884, 512, 588, 8263, 5821, 13, 51271, 51271, 3987, 264, 2685, 2539, 14642, 291, 764, 366, 1021, 11, 5276, 721, 411, 562, 51499, 51499, 293, 577, 281, 5407, 670, 69, 2414, 4523, 484, 281, 312, 472, 295, 264, 588, 8263, 3942, 294, 264, 51741, 51741, 957, 1002, 382, 731, 13, 51803, 51803], "temperature": 0.0, "avg_logprob": -0.10557481404897329, "compression_ratio": 1.651567944250871, "no_speech_prob": 1.5445813914993778e-05}, {"id": 51, "seek": 24586, "start": 253.96, "end": 259.16, "text": " But if you understand and can apply linear regression and logistic regression, that's", "tokens": [50364, 257, 2952, 295, 2158, 11, 2171, 1455, 257, 688, 295, 1460, 337, 264, 3431, 13, 50561, 50561, 400, 286, 458, 291, 600, 787, 668, 7601, 341, 1507, 337, 257, 1326, 3259, 13, 50769, 50769, 583, 498, 291, 1223, 293, 393, 3079, 8213, 24590, 293, 3565, 3142, 24590, 11, 300, 311, 51029, 51029, 767, 439, 291, 643, 281, 1884, 512, 588, 8263, 5821, 13, 51271, 51271, 3987, 264, 2685, 2539, 14642, 291, 764, 366, 1021, 11, 5276, 721, 411, 562, 51499, 51499, 293, 577, 281, 5407, 670, 69, 2414, 4523, 484, 281, 312, 472, 295, 264, 588, 8263, 3942, 294, 264, 51741, 51741, 957, 1002, 382, 731, 13, 51803, 51803], "temperature": 0.0, "avg_logprob": -0.10557481404897329, "compression_ratio": 1.651567944250871, "no_speech_prob": 1.5445813914993778e-05}, {"id": 52, "seek": 24586, "start": 259.16, "end": 264.0, "text": " actually all you need to create some very valuable applications.", "tokens": [50364, 257, 2952, 295, 2158, 11, 2171, 1455, 257, 688, 295, 1460, 337, 264, 3431, 13, 50561, 50561, 400, 286, 458, 291, 600, 787, 668, 7601, 341, 1507, 337, 257, 1326, 3259, 13, 50769, 50769, 583, 498, 291, 1223, 293, 393, 3079, 8213, 24590, 293, 3565, 3142, 24590, 11, 300, 311, 51029, 51029, 767, 439, 291, 643, 281, 1884, 512, 588, 8263, 5821, 13, 51271, 51271, 3987, 264, 2685, 2539, 14642, 291, 764, 366, 1021, 11, 5276, 721, 411, 562, 51499, 51499, 293, 577, 281, 5407, 670, 69, 2414, 4523, 484, 281, 312, 472, 295, 264, 588, 8263, 3942, 294, 264, 51741, 51741, 957, 1002, 382, 731, 13, 51803, 51803], "temperature": 0.0, "avg_logprob": -0.10557481404897329, "compression_ratio": 1.651567944250871, "no_speech_prob": 1.5445813914993778e-05}, {"id": 53, "seek": 24586, "start": 264.0, "end": 268.56, "text": " While the specific learning algorithms you use are important, knowing things like when", "tokens": [50364, 257, 2952, 295, 2158, 11, 2171, 1455, 257, 688, 295, 1460, 337, 264, 3431, 13, 50561, 50561, 400, 286, 458, 291, 600, 787, 668, 7601, 341, 1507, 337, 257, 1326, 3259, 13, 50769, 50769, 583, 498, 291, 1223, 293, 393, 3079, 8213, 24590, 293, 3565, 3142, 24590, 11, 300, 311, 51029, 51029, 767, 439, 291, 643, 281, 1884, 512, 588, 8263, 5821, 13, 51271, 51271, 3987, 264, 2685, 2539, 14642, 291, 764, 366, 1021, 11, 5276, 721, 411, 562, 51499, 51499, 293, 577, 281, 5407, 670, 69, 2414, 4523, 484, 281, 312, 472, 295, 264, 588, 8263, 3942, 294, 264, 51741, 51741, 957, 1002, 382, 731, 13, 51803, 51803], "temperature": 0.0, "avg_logprob": -0.10557481404897329, "compression_ratio": 1.651567944250871, "no_speech_prob": 1.5445813914993778e-05}, {"id": 54, "seek": 24586, "start": 268.56, "end": 273.40000000000003, "text": " and how to reduce overfitting turns out to be one of the very valuable skills in the", "tokens": [50364, 257, 2952, 295, 2158, 11, 2171, 1455, 257, 688, 295, 1460, 337, 264, 3431, 13, 50561, 50561, 400, 286, 458, 291, 600, 787, 668, 7601, 341, 1507, 337, 257, 1326, 3259, 13, 50769, 50769, 583, 498, 291, 1223, 293, 393, 3079, 8213, 24590, 293, 3565, 3142, 24590, 11, 300, 311, 51029, 51029, 767, 439, 291, 643, 281, 1884, 512, 588, 8263, 5821, 13, 51271, 51271, 3987, 264, 2685, 2539, 14642, 291, 764, 366, 1021, 11, 5276, 721, 411, 562, 51499, 51499, 293, 577, 281, 5407, 670, 69, 2414, 4523, 484, 281, 312, 472, 295, 264, 588, 8263, 3942, 294, 264, 51741, 51741, 957, 1002, 382, 731, 13, 51803, 51803], "temperature": 0.0, "avg_logprob": -0.10557481404897329, "compression_ratio": 1.651567944250871, "no_speech_prob": 1.5445813914993778e-05}, {"id": 55, "seek": 24586, "start": 273.40000000000003, "end": 274.64, "text": " real world as well.", "tokens": [50364, 257, 2952, 295, 2158, 11, 2171, 1455, 257, 688, 295, 1460, 337, 264, 3431, 13, 50561, 50561, 400, 286, 458, 291, 600, 787, 668, 7601, 341, 1507, 337, 257, 1326, 3259, 13, 50769, 50769, 583, 498, 291, 1223, 293, 393, 3079, 8213, 24590, 293, 3565, 3142, 24590, 11, 300, 311, 51029, 51029, 767, 439, 291, 643, 281, 1884, 512, 588, 8263, 5821, 13, 51271, 51271, 3987, 264, 2685, 2539, 14642, 291, 764, 366, 1021, 11, 5276, 721, 411, 562, 51499, 51499, 293, 577, 281, 5407, 670, 69, 2414, 4523, 484, 281, 312, 472, 295, 264, 588, 8263, 3942, 294, 264, 51741, 51741, 957, 1002, 382, 731, 13, 51803, 51803], "temperature": 0.0, "avg_logprob": -0.10557481404897329, "compression_ratio": 1.651567944250871, "no_speech_prob": 1.5445813914993778e-05}, {"id": 56, "seek": 27464, "start": 274.64, "end": 279.28, "text": " So, I want to say, congratulations on how far you've come.", "tokens": [50364, 407, 11, 286, 528, 281, 584, 11, 13568, 322, 577, 1400, 291, 600, 808, 13, 50596, 50596, 400, 286, 528, 281, 584, 869, 1691, 337, 1242, 807, 439, 264, 636, 281, 264, 917, 295, 341, 960, 13, 50860, 50860, 286, 1454, 291, 611, 589, 807, 264, 3124, 20339, 293, 48955, 13, 51094, 51094, 10222, 848, 300, 11, 456, 311, 920, 867, 544, 4670, 721, 281, 1466, 13, 51316, 51316, 682, 264, 1150, 1164, 295, 341, 2121, 2144, 11, 291, 1466, 466, 18161, 9590, 11, 611, 1219, 51574, 51574, 2452, 2539, 14642, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.11068740487098694, "compression_ratio": 1.603305785123967, "no_speech_prob": 1.1124941011075862e-05}, {"id": 57, "seek": 27464, "start": 279.28, "end": 284.56, "text": " And I want to say great job for getting through all the way to the end of this video.", "tokens": [50364, 407, 11, 286, 528, 281, 584, 11, 13568, 322, 577, 1400, 291, 600, 808, 13, 50596, 50596, 400, 286, 528, 281, 584, 869, 1691, 337, 1242, 807, 439, 264, 636, 281, 264, 917, 295, 341, 960, 13, 50860, 50860, 286, 1454, 291, 611, 589, 807, 264, 3124, 20339, 293, 48955, 13, 51094, 51094, 10222, 848, 300, 11, 456, 311, 920, 867, 544, 4670, 721, 281, 1466, 13, 51316, 51316, 682, 264, 1150, 1164, 295, 341, 2121, 2144, 11, 291, 1466, 466, 18161, 9590, 11, 611, 1219, 51574, 51574, 2452, 2539, 14642, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.11068740487098694, "compression_ratio": 1.603305785123967, "no_speech_prob": 1.1124941011075862e-05}, {"id": 58, "seek": 27464, "start": 284.56, "end": 289.24, "text": " I hope you also work through the practice labs and quizzes.", "tokens": [50364, 407, 11, 286, 528, 281, 584, 11, 13568, 322, 577, 1400, 291, 600, 808, 13, 50596, 50596, 400, 286, 528, 281, 584, 869, 1691, 337, 1242, 807, 439, 264, 636, 281, 264, 917, 295, 341, 960, 13, 50860, 50860, 286, 1454, 291, 611, 589, 807, 264, 3124, 20339, 293, 48955, 13, 51094, 51094, 10222, 848, 300, 11, 456, 311, 920, 867, 544, 4670, 721, 281, 1466, 13, 51316, 51316, 682, 264, 1150, 1164, 295, 341, 2121, 2144, 11, 291, 1466, 466, 18161, 9590, 11, 611, 1219, 51574, 51574, 2452, 2539, 14642, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.11068740487098694, "compression_ratio": 1.603305785123967, "no_speech_prob": 1.1124941011075862e-05}, {"id": 59, "seek": 27464, "start": 289.24, "end": 293.68, "text": " Having said that, there's still many more exciting things to learn.", "tokens": [50364, 407, 11, 286, 528, 281, 584, 11, 13568, 322, 577, 1400, 291, 600, 808, 13, 50596, 50596, 400, 286, 528, 281, 584, 869, 1691, 337, 1242, 807, 439, 264, 636, 281, 264, 917, 295, 341, 960, 13, 50860, 50860, 286, 1454, 291, 611, 589, 807, 264, 3124, 20339, 293, 48955, 13, 51094, 51094, 10222, 848, 300, 11, 456, 311, 920, 867, 544, 4670, 721, 281, 1466, 13, 51316, 51316, 682, 264, 1150, 1164, 295, 341, 2121, 2144, 11, 291, 1466, 466, 18161, 9590, 11, 611, 1219, 51574, 51574, 2452, 2539, 14642, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.11068740487098694, "compression_ratio": 1.603305785123967, "no_speech_prob": 1.1124941011075862e-05}, {"id": 60, "seek": 27464, "start": 293.68, "end": 298.84, "text": " In the second course of this specialization, you learn about neural networks, also called", "tokens": [50364, 407, 11, 286, 528, 281, 584, 11, 13568, 322, 577, 1400, 291, 600, 808, 13, 50596, 50596, 400, 286, 528, 281, 584, 869, 1691, 337, 1242, 807, 439, 264, 636, 281, 264, 917, 295, 341, 960, 13, 50860, 50860, 286, 1454, 291, 611, 589, 807, 264, 3124, 20339, 293, 48955, 13, 51094, 51094, 10222, 848, 300, 11, 456, 311, 920, 867, 544, 4670, 721, 281, 1466, 13, 51316, 51316, 682, 264, 1150, 1164, 295, 341, 2121, 2144, 11, 291, 1466, 466, 18161, 9590, 11, 611, 1219, 51574, 51574, 2452, 2539, 14642, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.11068740487098694, "compression_ratio": 1.603305785123967, "no_speech_prob": 1.1124941011075862e-05}, {"id": 61, "seek": 27464, "start": 298.84, "end": 301.03999999999996, "text": " deep learning algorithms.", "tokens": [50364, 407, 11, 286, 528, 281, 584, 11, 13568, 322, 577, 1400, 291, 600, 808, 13, 50596, 50596, 400, 286, 528, 281, 584, 869, 1691, 337, 1242, 807, 439, 264, 636, 281, 264, 917, 295, 341, 960, 13, 50860, 50860, 286, 1454, 291, 611, 589, 807, 264, 3124, 20339, 293, 48955, 13, 51094, 51094, 10222, 848, 300, 11, 456, 311, 920, 867, 544, 4670, 721, 281, 1466, 13, 51316, 51316, 682, 264, 1150, 1164, 295, 341, 2121, 2144, 11, 291, 1466, 466, 18161, 9590, 11, 611, 1219, 51574, 51574, 2452, 2539, 14642, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.11068740487098694, "compression_ratio": 1.603305785123967, "no_speech_prob": 1.1124941011075862e-05}, {"id": 62, "seek": 30104, "start": 301.04, "end": 305.20000000000005, "text": " Neural networks are responsible for many of the latest breakthroughs in AI today, from", "tokens": [50364, 1734, 1807, 9590, 366, 6250, 337, 867, 295, 264, 6792, 22397, 82, 294, 7318, 965, 11, 490, 50572, 50572, 8496, 6218, 11150, 11, 281, 10807, 20095, 18538, 6565, 293, 5267, 11, 50830, 50830, 281, 2698, 12, 47094, 5163, 13, 50918, 50918, 440, 636, 257, 18161, 3209, 2170, 3094, 767, 4960, 257, 688, 295, 437, 291, 600, 1217, 3264, 11, 51170, 51170, 411, 2063, 6828, 293, 16235, 23475, 293, 4556, 3280, 327, 6828, 13, 51372, 51372, 407, 797, 11, 13568, 322, 9906, 264, 917, 295, 341, 2636, 293, 2572, 1243, 295, 1164, 51606, 51606, 472, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.14282862345377603, "compression_ratio": 1.5797101449275361, "no_speech_prob": 2.3918963051983155e-05}, {"id": 63, "seek": 30104, "start": 305.20000000000005, "end": 310.36, "text": " practical speech recognition, to computers accurately recognizing objects and images,", "tokens": [50364, 1734, 1807, 9590, 366, 6250, 337, 867, 295, 264, 6792, 22397, 82, 294, 7318, 965, 11, 490, 50572, 50572, 8496, 6218, 11150, 11, 281, 10807, 20095, 18538, 6565, 293, 5267, 11, 50830, 50830, 281, 2698, 12, 47094, 5163, 13, 50918, 50918, 440, 636, 257, 18161, 3209, 2170, 3094, 767, 4960, 257, 688, 295, 437, 291, 600, 1217, 3264, 11, 51170, 51170, 411, 2063, 6828, 293, 16235, 23475, 293, 4556, 3280, 327, 6828, 13, 51372, 51372, 407, 797, 11, 13568, 322, 9906, 264, 917, 295, 341, 2636, 293, 2572, 1243, 295, 1164, 51606, 51606, 472, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.14282862345377603, "compression_ratio": 1.5797101449275361, "no_speech_prob": 2.3918963051983155e-05}, {"id": 64, "seek": 30104, "start": 310.36, "end": 312.12, "text": " to self-driving cars.", "tokens": [50364, 1734, 1807, 9590, 366, 6250, 337, 867, 295, 264, 6792, 22397, 82, 294, 7318, 965, 11, 490, 50572, 50572, 8496, 6218, 11150, 11, 281, 10807, 20095, 18538, 6565, 293, 5267, 11, 50830, 50830, 281, 2698, 12, 47094, 5163, 13, 50918, 50918, 440, 636, 257, 18161, 3209, 2170, 3094, 767, 4960, 257, 688, 295, 437, 291, 600, 1217, 3264, 11, 51170, 51170, 411, 2063, 6828, 293, 16235, 23475, 293, 4556, 3280, 327, 6828, 13, 51372, 51372, 407, 797, 11, 13568, 322, 9906, 264, 917, 295, 341, 2636, 293, 2572, 1243, 295, 1164, 51606, 51606, 472, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.14282862345377603, "compression_ratio": 1.5797101449275361, "no_speech_prob": 2.3918963051983155e-05}, {"id": 65, "seek": 30104, "start": 312.12, "end": 317.16, "text": " The way a neural network gets built actually uses a lot of what you've already learned,", "tokens": [50364, 1734, 1807, 9590, 366, 6250, 337, 867, 295, 264, 6792, 22397, 82, 294, 7318, 965, 11, 490, 50572, 50572, 8496, 6218, 11150, 11, 281, 10807, 20095, 18538, 6565, 293, 5267, 11, 50830, 50830, 281, 2698, 12, 47094, 5163, 13, 50918, 50918, 440, 636, 257, 18161, 3209, 2170, 3094, 767, 4960, 257, 688, 295, 437, 291, 600, 1217, 3264, 11, 51170, 51170, 411, 2063, 6828, 293, 16235, 23475, 293, 4556, 3280, 327, 6828, 13, 51372, 51372, 407, 797, 11, 13568, 322, 9906, 264, 917, 295, 341, 2636, 293, 2572, 1243, 295, 1164, 51606, 51606, 472, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.14282862345377603, "compression_ratio": 1.5797101449275361, "no_speech_prob": 2.3918963051983155e-05}, {"id": 66, "seek": 30104, "start": 317.16, "end": 321.20000000000005, "text": " like cost functions and gradient descent and sigmoid functions.", "tokens": [50364, 1734, 1807, 9590, 366, 6250, 337, 867, 295, 264, 6792, 22397, 82, 294, 7318, 965, 11, 490, 50572, 50572, 8496, 6218, 11150, 11, 281, 10807, 20095, 18538, 6565, 293, 5267, 11, 50830, 50830, 281, 2698, 12, 47094, 5163, 13, 50918, 50918, 440, 636, 257, 18161, 3209, 2170, 3094, 767, 4960, 257, 688, 295, 437, 291, 600, 1217, 3264, 11, 51170, 51170, 411, 2063, 6828, 293, 16235, 23475, 293, 4556, 3280, 327, 6828, 13, 51372, 51372, 407, 797, 11, 13568, 322, 9906, 264, 917, 295, 341, 2636, 293, 2572, 1243, 295, 1164, 51606, 51606, 472, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.14282862345377603, "compression_ratio": 1.5797101449275361, "no_speech_prob": 2.3918963051983155e-05}, {"id": 67, "seek": 30104, "start": 321.20000000000005, "end": 325.88, "text": " So again, congratulations on reaching the end of this third and final week of course", "tokens": [50364, 1734, 1807, 9590, 366, 6250, 337, 867, 295, 264, 6792, 22397, 82, 294, 7318, 965, 11, 490, 50572, 50572, 8496, 6218, 11150, 11, 281, 10807, 20095, 18538, 6565, 293, 5267, 11, 50830, 50830, 281, 2698, 12, 47094, 5163, 13, 50918, 50918, 440, 636, 257, 18161, 3209, 2170, 3094, 767, 4960, 257, 688, 295, 437, 291, 600, 1217, 3264, 11, 51170, 51170, 411, 2063, 6828, 293, 16235, 23475, 293, 4556, 3280, 327, 6828, 13, 51372, 51372, 407, 797, 11, 13568, 322, 9906, 264, 917, 295, 341, 2636, 293, 2572, 1243, 295, 1164, 51606, 51606, 472, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.14282862345377603, "compression_ratio": 1.5797101449275361, "no_speech_prob": 2.3918963051983155e-05}, {"id": 68, "seek": 30104, "start": 325.88, "end": 326.88, "text": " one.", "tokens": [50364, 1734, 1807, 9590, 366, 6250, 337, 867, 295, 264, 6792, 22397, 82, 294, 7318, 965, 11, 490, 50572, 50572, 8496, 6218, 11150, 11, 281, 10807, 20095, 18538, 6565, 293, 5267, 11, 50830, 50830, 281, 2698, 12, 47094, 5163, 13, 50918, 50918, 440, 636, 257, 18161, 3209, 2170, 3094, 767, 4960, 257, 688, 295, 437, 291, 600, 1217, 3264, 11, 51170, 51170, 411, 2063, 6828, 293, 16235, 23475, 293, 4556, 3280, 327, 6828, 13, 51372, 51372, 407, 797, 11, 13568, 322, 9906, 264, 917, 295, 341, 2636, 293, 2572, 1243, 295, 1164, 51606, 51606, 472, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.14282862345377603, "compression_ratio": 1.5797101449275361, "no_speech_prob": 2.3918963051983155e-05}, {"id": 69, "seek": 32688, "start": 326.88, "end": 331.44, "text": " I hope you have fun in the labs and I will see you in next week's material on neural", "tokens": [50364, 286, 1454, 291, 362, 1019, 294, 264, 20339, 293, 286, 486, 536, 291, 294, 958, 1243, 311, 2527, 322, 18161, 50592, 50592, 9590, 13, 50606], "temperature": 0.0, "avg_logprob": -0.1930392759817618, "compression_ratio": 1.1325301204819278, "no_speech_prob": 3.8750389649067074e-05}, {"id": 70, "seek": 33144, "start": 331.44, "end": 358.08, "text": " networks.", "tokens": [50364, 9590, 13, 51696], "temperature": 0.0, "avg_logprob": -0.8894067764282226, "compression_ratio": 0.5294117647058824, "no_speech_prob": 0.0009458315907977521}], "language": "en", "video_id": "MFp4uQMQ1rk", "entity": "ML Specialization, Andrew Ng (2022)"}}