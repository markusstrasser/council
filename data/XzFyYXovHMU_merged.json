{"video_id": "XzFyYXovHMU", "title": "From Seeing to Doing: Understanding & Interacting With The Real World With Fei Fei Li", "description": "In this session (https://scl.ai/3EaRje9), Dr. Li shares how vision is critical for first perceiving the physical world and then interacting with it. She explores how recent advances in AI research help machines perceive the environment around them and then engages with it, to perform both short-horizon and long-horizon tasks.\n\nExecutive summary: Teaching Robots To Perform Complex Tasks - https://scl.ai/3lu2lnn\nFull discussion transcript -  https://scl.ai/3EaRje9\n\nBio: Dr. Fei-Fei Li is the Sequoia Professor of Computer Science at Stanford University and Denning Co-Director of the Stanford Institute for Human-Centered AI (HAI). Her research includes cognitively inspired AI, machine learning, deep learning, computer vision, and AI+healthcare. Before co-founding HAI, she served as Director of Stanford\u2019s AI Lab. During her Stanford sabbatical from 2017 - 2018, Dr. Li was a Vice President at Google and Chief Scientist of AI/ML at Google Cloud.\n\nJoin the leading AI community full of leaders, visionaries, practitioners, and researchers. Get full access to more discussions like this: https://exchange.scale.com/home", "author": "Scale AI", "keywords": [], "channel_url": "https://www.youtube.com/channel/UCAHS4cP-uIXdrpCsSfGq_OA", "length": 2254, "views": 1583, "publish_date": "11/02/2022", "timestamp": 1638403200, "entity": "Fei-Fei Li", "transcript": {"text": " For our next speaker, we are honored to welcome Dr. Fei-Fei Li. Dr. Fei-Fei Li is the Sequoia Professor of Computer Science at Stanford University and Denning Co-Director of the Stanford Institute for Human-Centered AI, HAI. Her research includes cognitively inspired AI, machine learning, deep learning, computer vision, and AI healthcare. Before co-founding HAI, she served as Director of Stanford's AI Lab. Dr. Li was a Vice President at Google and Chief Scientist of AIML at Google Cloud. Dr. Li is co-founder and chairperson of the national non-profit AI for All, which is increasing inclusion and diversity in AI education. Please enjoy Dr. Li's keynote. Hi, everyone. Good morning, good afternoon, and good evening wherever in the world you are. My name is Fei-Fei Li. I'm a professor at Stanford Computer Science Department and also co-director of Stanford's Institute for Human-Centered AI. Today, I'm going to share with you some of the latest work from my lab. And the title of the talk is From Seeing to Doing, Understanding and Interacting with the Real World. I want to take you back 540 million years ago. What was the world like? Most animals, actually all animals, lived in the primordial soup of life. And there aren't that many species on Earth. They mostly float in the water and catch a dinner whenever they float by. But something really mysterious happened around 540 million years ago. In a very short period of time, a matter of 10 million years, fossil studies have revealed that the number of animal species just exploded. Zoologists call this the Cambrian explosion or the Big Bang of evolution. So what made the number of animals, the types of animals just increase exponentially? That has been a mystery for zoologists and biologists for a long time. There's one really prominent theory that emerged in the last couple of decades. And it's a theory that has inspired a lot of my own work. This is proposed by a zoologist from Australia called Andrew Parker. He says that Cambrian explosion is triggered by the sudden evolution of vision, which set off an evolutionary arms race where animals either evolved or died. Basically, the ability to see the world, to see light and to see dinner, is the driving force or one of the major driving forces of evolution. So animals from that point on evolved in all kinds of shapes and forms in order to survive as well as to reproduce. From that point on to today, we have essentially all the animals in the world have some kind of vision. And not only it came vision, animals start to develop intelligence. The nervous system developed more and more complicated apparatus. And now we have humans with one of the most complicated brains in the history of our world. So that is a very, very, very brief history of vision. And that's how I think about my research, that I view vision as a cornerstone of intelligence, whether it's biological or artificial. And in my work in AI and computer vision, I try to use a vision to understand intelligence and to build intelligent machines. So for the rest of the talk, I want to share with you what vision means. To me, it means two very important things. One is to understand the real world. The other is for doing things, interacting and acting in the real world. Let's start by the first, understanding. Psychologists have told us and used studies to show that human vision is remarkable. Humans are capable of perceiving real world objects and things in a really phenomenal way. In this very early study by cognitive scientist Irv Biederman in the 70s, he showed that the ability to recognize a bicycle in two different pictures, one coherent, one incoherent picture, was very dramatically different. Humans are better at seeing bicycles in a coherent scene, even though the bicycle itself hasn't changed locations. Concurrently, Molly Potter and some of her colleagues have shown that humans have a remarkable ability of detecting novel objects. In this video, you'll see there's one frame that contains a person, even though you've never seen this video, you have no problem of detecting where the person is, roughly what he or she is the location on the screen. The gestures, and keep in mind, every frame is only presented for 100 milliseconds. So this, the frame changes at 10 hertz, yet our visual system is very good at detecting these novel objects. Back in 1996, about 25 years ago, psychoneurophysiologists Simon Thorpe and his colleagues have shown through brain EEG study that as early as 150 milliseconds after a picture is shown, our brain shows a differential signal that can tell apart a picture with animals versus a picture without animals. And here we're talking about all kinds of animals among all kinds of destructor images. So it's quite a remarkable ability of human vision. I myself, about 15 years ago have done a experiment. When I was a graduate student, where we put human subjects in front of a computer screen and flash to them, real world photos masked by a wallpaper looking structure. And I asked human subject to type what they see. You can see some of these images are flashed in really really fast way, yet humans are very good at seeing what these things are. If the picture is presented for 500 milliseconds, it's like eternity, people can write novels if you pay them enough. So, there's something special about our visual system, we can use it to understand the world. In fact, Alan Turing, one of the most inspiring person in the history of computer science and inspiring to AI the field of AI has said that it's possible to has conjecture to use a machine and teach it to understand the real world. And this is what I think seeing is for seeing is for understanding is for making sense of what this visual world is about. So back to this experiment. We see that humans are able to understand and make sense and perceive the visual world. But what are the key elements or building blocks of this. If you look at what humans type when presented with a picture like this. They talk a lot about objects like men fist face grass helmet clothing trees at dogs or other things. And indeed, object recognition is a building block of visual understanding or of vision. And for those of you who are not familiar with this, what is object understanding, it's defined by the task of showing a visual system, whether it's a biological system like our own, or a computer, a picture, and the system is able to identify what is the main object in the picture, for example, this is a one that in the picture. Why is it hard or maybe it is not because humans can do this easily. It turns out it's actually quite a difficult task for computers. For one thing, computers have to see this in just numbers, you know color numbers or luminous numbers, but going from numbers to the understanding that there is a one that takes a lot of computation. In fact, objects, even though they can be the same object, they can come in many different kind of shape and form and environment, not to mention, there is a 3D world that renders these objects in very infinite number of possibilities. In fact, this has understanding objects or object recognition has been a quest for more than half a century in computer vision. In early days, people try to use hand designed models to configure geometric shapes to try to express objects in a mathematical language. And there was some heroic efforts in the 60s, 70s, about object recognition. And to fast forward. Shortly before the turn of the century, machine learning as a field became a really important mathematical tool for computer vision and AI, and we computer scientists learned that we don't have to hand design models, we can use the models and the parameters, but we have to rely on hand design features. So we input features, whether it's patches of images, or some kind of encoding of pixels, and then we try to learn through data and through learning models, how these can be configured. And there were a lot of great work that can come out of this. As we start to push towards solving the problem of object recognition, one important aspect of the, of the work or research came about, and that is the design of data sets and benchmarks. In the early days of object recognition one of the most prominent data set was Europeans Pascal VOC data set focus on 20 object categories, and it was released annually between 2006 to 2012 to encourage the field of computer recognition. All the labs worldwide to benchmark against the testing data of this data set to make to assess the progress of the field. But the truth is, the world is a lot larger than 20 categories. It's more than it's in fact psychologists have estimated 10s of thousands if not hundreds of thousands of categories of objects. And here, I want to bring you a quote of one of the most important psychologists that has influenced my thinking in terms of how to working AI and that's JJ Gibson. Gibson has said, or a psychologist has paraphrased Gibson by saying, ask not what's inside your head, but what your head is inside off. And this is a really important concept of encouraging us to think about an ecological approach to perception. So when we are working on, say, object recognition. We know that it's a building block for understanding the world. We really need to emphasize on the scale of the real world, inspired by this concept. Around 2007, my students and I were looking at the size of the data sets towards training object recognition models, and we were deeply unsatisfied because they hover around thousands if not, or 10s of thousands, but truly small compared to the size of the, to the to the visual world that we experience. This is when we built together ImageNet, a data set of 15 million images across 22,000 object categories. And the goal of ImageNet is to really establish object recognition as one of the most important North stars in computer vision, and use the benchmark data set of ImageNet to, to encourage training with real world scale and understanding the real world scale. Of course, a lot of you are already familiar with the rest of the history, ImageNet put together an international challenge annually between 2010 and 2017, and our testing data set become a benchmark data set for the field of computer vision object recognition research community, and 2012, the winner of the ImageNet challenge, especially object classification challenge was a neural net convolutional neural network model. And that was the beginning of deep learnings revolution. And since then, we have seen a lot of different models built upon and benchmarked against the ImageNet, and the field has made tremendous progress. And here's another way to show how the ImageNet accuracy has evolved based on different models. So a lot of progress has been engendered. But the world is more than just discrete object classes. In fact, there's a lot more than recognizing object, different objects. And I'll show two images where object detectors will tell you the same objects existing in these two things, the animal llama and a person. They look similar. And one picture look like this. But if you look at the other picture you realize these are two very very different pictures, because of the relationship between the objects. And psychologists have long conjectured that for to characterize a scene or to understand a visual scene the real visual world relationships between objects must be coded. And that's how we go to the identities of objects. And this brings us to a following work of ImageNet by my students and collaborators on scene graph representation, where we look at not only object identities in the image but also the attributes like the colors and, and, and expressions and so on, as well as the relationship. In fact, every image is a is full of different relationships, we put to put together this, this data set called visual genome, which contains 100,000 images 3.8 million objects, 2.3 million relationships, and also 5.4 million texture descriptions of the things are following work, looked at how we can predict visual relationships using scene graphs and be able to achieve relationship recognition for example, we have a model, we have a model that we can use for modeling, creating a model that can take a picture like this, and call it person riding horse or person wearing hat. So our model can also do one zero shot learning, but looking at new relationships, such as horse wearing hat, which is really rare in real world things but with this compositional representation using scene graph, we're able to achieve this kind of visual learning on novel relationships, and some quantitative numbers show that our scene graph model for relationship estimation, as well as zero shot learning for relationship estimation beats the back than state of the art algorithms. And of course, the community has done a lot more interesting work since then, based on our scene graph representation here I just list a few work by other groups on all kinds of scene graph modeling, and we have also extended this beyond the static scenes into videos and created a new data set, a benchmark called action genome, and using spatial temporal scene graph to represent actions and use this to perform tasks like action recognition or few shot recognition. In fact, we have gone one more step further, and being inspired by Alan Turing's words that understanding the real world scene might connect the machine to also speaking English, in this case, so we have worked on a series of models where you can take a picture and perform image captioning or does image captioning, as well as paragraph captioning. So, that was a very quick, quick overview of one part of visual intelligence which is the perception part, the perception part takes the pixels of the real world, feed it into the AI agent, and the agent is able to do important tasks like object recognition, visual relationship prediction, captioning and so on. And we introduced two data sets one is image net one is visual genome, and a representation called scene graph. But our lab has done more around the problem of perception and around both benchmark learning representation and connecting it to language. So, but I want to now shift gears and ask the question is just passive understanding of the world enough for visual intelligence. My answer would be no. We have gone to Plato's allegory of the cave, where he describes the the the passive perception of the world as prisoners tied to chairs, they're only forced to watch for in front of them, a play that's on full display in the back of their head, and they see are the shadows of the play, and they need to make sense of the real world. So in fact, if we only look at this world in a passive way, we're a little bit like the prisoners of the other of the cave, and that would limit important functions of our visual experience. For example, the. We won't be able to fully understand how to interact with these objects, especially if we view them in angles that won't enable us to interact effectively. So in fact, real visual experience is extreme, extremely dynamic, you and I move around all the time, and animals move around all the time, and they do a lot of things, and that is what I feel what I think visual intelligence is about. Here I'll share with you one favorite quote of mine, which is by philosopher Peter Godfrey Smith, who says, the original and fundamental function of the nervous system is to link perception with action. And this is a very famous experiment done on two kittens, back in the 1960s, where the newborn kittens one is allowed to be active kitten. One is allowed only to be a passive kitten, the active kitten drives the yolk to explore visually with what the world. The world is like, whereas the passive kitten is not allowed to explore by by its pro pro activeness, it only sees the world as the active kitten drives, moves around. And two weeks later, it was demonstrated that the active kitten has a much better developed perceptual visual system that the passive kitten. Not only we find this evidence in kittens we also find evidence in monkeys and humans that we have neurons, neural neurons that are responsible to look at other people's movement, and to respond to that so in a way we're hardwired to perceive movements and want to do the same. So this brings me to the second half of the talk, which is seeing is for doing in the world. And we complete our, our little schema of the world, the where the agent and the world now, not only perceive, but act. And what are the critical ingredients of acting in the world. I think there are several. One is it should be embodied. Moving around in the world is both explorer explorative, as well as exploitative. It's most likely multimodal. A lot of times, it's multitasking, and it's really important. It's really important to allow the agent to be able to generalize, and many times, oftentimes, it's social and interactive with other agents. This brings me to a far reaching dream of AI which is to create robots that can perform a lot of complex human behavior human tasks. This is Rosie the robot. Of course we're not there yet, but the rest of my talk I want to share, share with you some of our efforts towards robotic learning, using vision and in real world things. Like I said, learning in in in active agent or embodied agent is both explorative and exploitative. Let me just start with the explorative, which is really learning to play. There's a huge body of literature in this I won't be able to do justice some of my favorite work come from Allison gopnik and Liz bulky and many others, where we we imitate human newborns or human children, where they spend a lot of time playing without a purpose, yet they're learning and exploring the world. There are flavors of this kind of export it of learning. There is the novelty based motivation there's the skill based motivation and a world model based motivation, and that's where our work is mostly anchored to. And this is also related to previous work on predicting dynamic predicting things and expecting what what to what to expect in future frames of videos and dynamics, but I won't get into the details. Basically the work my colleagues and students collaborators have done is to create a model that work on two kind of model. There is a world model network that predicts the consequences of actions of the actions of the embodied agent exploring a world. And then there's a self model network that predicts errors of the world model and and try to correct those errors. And intrinsic reward is a policy mechanism, where we choose actions that maximize world model loss predicted by self model and this is to maximize the exploration and putting the self model and world model together is our intrinsically motivated self aware agent, and we use that to explore a simulated world 3D world with objects, and you can see that the, the, the agent is able to explore in a similar way the blue line like human babies they start with the self motion, you know motion, and then they look at one single object and then they start to look at two objects. And this lower right panel shows you that the model learned through this self exploration or self motivation can be able to do downstream object recognition tasks, better than a random policy model. So that was a an example of explorative learning. Let's go to a explorative learning which is much more goal based. I'm going to just be very brief. Remind everybody that our eventual goal is to make robots to do long horizon tasks, but most of the work in today's robotic learning is very short punctual skills level tasks. So we need to try to close the gap by encouraging robots to do longer horizon tasks like cleaning up tabletops in a longer horizon way. And here with my students and collaborators we we put together a newer task programming model were inspired by actually the inter vision research, but by enabling robotic learning to be compositional through skill set level tasks and hierarchically stack them together. And I'm not going to get into the details of this competition compositional representation, but here is a result showing that our robots are able to perform longer horizon tasks better than better than a state of the art result. And we, we can perform multiple tasks not just color block stacking but also sorting. In fact, we can also resist some interruptions here and the experimenter is going to disrupt what this color block task is, and the robot is able to compose the the task automatically by itself and reset its goals and complete the task. So, again, I showed you one example of explorative learning towards long horizon tasks. Let me just say that this is has some has been something that my lab has been focusing on in various angles in a new in a newer line of work, we continue to look at long horizon and generalize the realization of long horizon tasks by by training a robot through curriculum learning, where we know the target task, but we know it's really hard for robot to learn at the beginning so we generate a series of simpler target, target tasks to guide the robot. And this is related to a lot of generative models recently we have seen in the AI community, and I'm going to skip the workflow of this model, and to just show you that our robots are capable of of learning different different kind of long horizon tasks by this curriculum training. And, in fact, even generalized to a different simulated desktop. So, um, so that was two example, actually three examples of robotic learning, but we're still not there yet in achieving this kind of real world task. And there is a key missing piece. And that key missing piece brings us back to what today's robotic tasks are still mostly skill level task and short horizon goals. Even if we try to do, try to do some longer horizon tasks, they tend to be small scale and anecdotal, their experimenter will make tasks and lack standard metrics. And, and this, you know, either some of the tasks are in artificially simple environments, or if we bring the, the previously trained robot to a real experiment. It just kind of fails miserably. Yeah, here's an example of that. So this brings us back to JJ Gibson, that we need an ecological approach to perception and robotic learning. And we have seen great progress in vision and NLP and other areas of AI. And we hope that in robotic learning, we can also work towards benchmarks that are large scale and diverse ecological in general complex, as well as standardized the evaluation metrics. We have a new latest work called behavior behavior is a benchmark for everyday household activities in virtual interactive and ecological environments and behavior is enabled by a simulation environment called I Gibson 2.0 it's a object centered model for robotic learning of everyday household. I'll just go over very quickly what I Gibson is, I Gibson is an environment that's very much inspired by a lot of concurrent work like habitat, so in the world, sapien AI to Thor, and its goal is to be realistic in object modeling photo realistic and rendering simulation for both kinematic and non kinematic state changes and full physically simulated action execution, as well as allowing VR interface for human demonstration. I'll just be skip the details of Gibson. I'll just skip the details of the Gibson work on our Stanford website. Gibson enables behavior, this benchmark. As we said, we want to build an embodied AI benchmark that is complex enough, large scale ecological complex and standardized the evaluation metrics. Gibson has 100 so far has 100 different tasks. They are. They are gathered through the American Bureau of Labor Statistics, and, and by sampling what Americans do in their daily life, and we put together this data set of 100 tasks. So in terms of statistics, Gibson is a lot more wider range compared to other data sets, focusing on just a narrower band of tasks, and the statistics of behavior tracks what the general statistics of the ATIS tasks. And it's also ecological in general. Here we show you by one example of clearing table, we show very different object positions environments, the rendering of objects, the textures. We have done extensive, extensive statistics analysis by showing you the diversity of objects and things. And it's also long horizon and complex. We show you that an average behavior task clients is measured by 300 to 20,000 steps, whereas other, other task benchmark are mostly smaller than 100 steps, or between 100 and 1000 steps so Gibson, sorry behavior is really going towards real life complexity in terms of tasks. Last but not least, it tries to standardize evaluation metrics by allowing a logic based representation to score these. The end, the end state, compared to the initial state. I'm just going to skip the details of this and, and, and move on. So, last but not now the least we also allow human VR demo in our behavior benchmark and we can use that for benchmarking against efficiency of execution. So, um, what excites me the most in this graph is behavior is really really hard. We have benchmarked task performance of behavior against a couple of a couple of state of the art algorithm and I want you to look at this, but most leftmost bar where we use default behavior without giving privileged information and you can see that the data set is close to zero. And this is where I think we're starting to, to be on the journey of creating robotic embodied agents that can do really complex household activities and can be benchmark against behavior data set. So for those of you who are interested, you can visit our website to learn more. So in short, in, in, in this scene is for doing. I've shared with you that I get the environment that enable the behavior challenge or behavior data set. And I want to share with you some of our earlier robotic work robotic learning work in curiosity based explorative learning, as well as long horizon task driven learning. This is the work that you can find on our website. And I want to conclude by reminding all of us that vision is a cornerstone of intelligence in it enables us to understand and to do things in this real world, and our research is formulated to support this two goals, especially inspired by JJ Gibson's ecological approach to perception and robotic learning. Thank you everybody this is my awesome team at Stanford, with so many great students and collaborators some of them are not even on this photo, but thank you so much. Bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.8, "text": " For our next speaker, we are honored to welcome Dr. Fei-Fei Li.", "tokens": [50364, 1171, 527, 958, 8145, 11, 321, 366, 14556, 281, 2928, 2491, 13, 39587, 12, 37, 17067, 8349, 13, 50754, 50754, 2491, 13, 39587, 12, 37, 17067, 8349, 307, 264, 46859, 78, 654, 8419, 295, 22289, 8976, 412, 20374, 3535, 51098, 51098, 293, 6458, 773, 3066, 12, 47747, 1672, 295, 264, 20374, 9446, 337, 10294, 12, 34, 317, 4073, 7318, 11, 11979, 40, 13, 51398, 51423, 3204, 2132, 5974, 15605, 356, 7547, 7318, 11, 3479, 2539, 11, 2452, 2539, 11, 51700, 51720, 3820, 5201, 11, 293, 7318, 8884, 13, 51839], "temperature": 0.0, "avg_logprob": -0.24827068454616671, "compression_ratio": 1.5246636771300448, "no_speech_prob": 0.049794215708971024}, {"id": 1, "seek": 0, "start": 7.8, "end": 14.68, "text": " Dr. Fei-Fei Li is the Sequoia Professor of Computer Science at Stanford University", "tokens": [50364, 1171, 527, 958, 8145, 11, 321, 366, 14556, 281, 2928, 2491, 13, 39587, 12, 37, 17067, 8349, 13, 50754, 50754, 2491, 13, 39587, 12, 37, 17067, 8349, 307, 264, 46859, 78, 654, 8419, 295, 22289, 8976, 412, 20374, 3535, 51098, 51098, 293, 6458, 773, 3066, 12, 47747, 1672, 295, 264, 20374, 9446, 337, 10294, 12, 34, 317, 4073, 7318, 11, 11979, 40, 13, 51398, 51423, 3204, 2132, 5974, 15605, 356, 7547, 7318, 11, 3479, 2539, 11, 2452, 2539, 11, 51700, 51720, 3820, 5201, 11, 293, 7318, 8884, 13, 51839], "temperature": 0.0, "avg_logprob": -0.24827068454616671, "compression_ratio": 1.5246636771300448, "no_speech_prob": 0.049794215708971024}, {"id": 2, "seek": 0, "start": 14.68, "end": 20.68, "text": " and Denning Co-Director of the Stanford Institute for Human-Centered AI, HAI.", "tokens": [50364, 1171, 527, 958, 8145, 11, 321, 366, 14556, 281, 2928, 2491, 13, 39587, 12, 37, 17067, 8349, 13, 50754, 50754, 2491, 13, 39587, 12, 37, 17067, 8349, 307, 264, 46859, 78, 654, 8419, 295, 22289, 8976, 412, 20374, 3535, 51098, 51098, 293, 6458, 773, 3066, 12, 47747, 1672, 295, 264, 20374, 9446, 337, 10294, 12, 34, 317, 4073, 7318, 11, 11979, 40, 13, 51398, 51423, 3204, 2132, 5974, 15605, 356, 7547, 7318, 11, 3479, 2539, 11, 2452, 2539, 11, 51700, 51720, 3820, 5201, 11, 293, 7318, 8884, 13, 51839], "temperature": 0.0, "avg_logprob": -0.24827068454616671, "compression_ratio": 1.5246636771300448, "no_speech_prob": 0.049794215708971024}, {"id": 3, "seek": 0, "start": 21.18, "end": 26.72, "text": " Her research includes cognitively inspired AI, machine learning, deep learning,", "tokens": [50364, 1171, 527, 958, 8145, 11, 321, 366, 14556, 281, 2928, 2491, 13, 39587, 12, 37, 17067, 8349, 13, 50754, 50754, 2491, 13, 39587, 12, 37, 17067, 8349, 307, 264, 46859, 78, 654, 8419, 295, 22289, 8976, 412, 20374, 3535, 51098, 51098, 293, 6458, 773, 3066, 12, 47747, 1672, 295, 264, 20374, 9446, 337, 10294, 12, 34, 317, 4073, 7318, 11, 11979, 40, 13, 51398, 51423, 3204, 2132, 5974, 15605, 356, 7547, 7318, 11, 3479, 2539, 11, 2452, 2539, 11, 51700, 51720, 3820, 5201, 11, 293, 7318, 8884, 13, 51839], "temperature": 0.0, "avg_logprob": -0.24827068454616671, "compression_ratio": 1.5246636771300448, "no_speech_prob": 0.049794215708971024}, {"id": 4, "seek": 2672, "start": 26.72, "end": 35.72, "text": " computer vision, and AI healthcare. Before co-founding HAI, she served as Director of Stanford's AI Lab.", "tokens": [50364, 3820, 5201, 11, 293, 7318, 8884, 13, 4546, 598, 12, 17493, 278, 11979, 40, 11, 750, 7584, 382, 7680, 295, 20374, 311, 7318, 10137, 13, 50814, 50814, 2491, 13, 8349, 390, 257, 13276, 3117, 412, 3329, 293, 10068, 18944, 468, 295, 7318, 12683, 412, 3329, 8061, 13, 51114, 51114, 2491, 13, 8349, 307, 598, 12, 33348, 293, 6090, 10813, 295, 264, 4048, 2107, 12, 14583, 7318, 337, 1057, 11, 51414, 51414, 597, 307, 5662, 15874, 293, 8811, 294, 7318, 3309, 13, 51664, 51664, 2555, 2103, 2491, 13, 8349, 311, 33896, 13, 51814], "temperature": 0.0, "avg_logprob": -0.19478580799508602, "compression_ratio": 1.4915966386554622, "no_speech_prob": 0.0003279770608060062}, {"id": 5, "seek": 2672, "start": 35.72, "end": 41.72, "text": " Dr. Li was a Vice President at Google and Chief Scientist of AIML at Google Cloud.", "tokens": [50364, 3820, 5201, 11, 293, 7318, 8884, 13, 4546, 598, 12, 17493, 278, 11979, 40, 11, 750, 7584, 382, 7680, 295, 20374, 311, 7318, 10137, 13, 50814, 50814, 2491, 13, 8349, 390, 257, 13276, 3117, 412, 3329, 293, 10068, 18944, 468, 295, 7318, 12683, 412, 3329, 8061, 13, 51114, 51114, 2491, 13, 8349, 307, 598, 12, 33348, 293, 6090, 10813, 295, 264, 4048, 2107, 12, 14583, 7318, 337, 1057, 11, 51414, 51414, 597, 307, 5662, 15874, 293, 8811, 294, 7318, 3309, 13, 51664, 51664, 2555, 2103, 2491, 13, 8349, 311, 33896, 13, 51814], "temperature": 0.0, "avg_logprob": -0.19478580799508602, "compression_ratio": 1.4915966386554622, "no_speech_prob": 0.0003279770608060062}, {"id": 6, "seek": 2672, "start": 41.72, "end": 47.72, "text": " Dr. Li is co-founder and chairperson of the national non-profit AI for All,", "tokens": [50364, 3820, 5201, 11, 293, 7318, 8884, 13, 4546, 598, 12, 17493, 278, 11979, 40, 11, 750, 7584, 382, 7680, 295, 20374, 311, 7318, 10137, 13, 50814, 50814, 2491, 13, 8349, 390, 257, 13276, 3117, 412, 3329, 293, 10068, 18944, 468, 295, 7318, 12683, 412, 3329, 8061, 13, 51114, 51114, 2491, 13, 8349, 307, 598, 12, 33348, 293, 6090, 10813, 295, 264, 4048, 2107, 12, 14583, 7318, 337, 1057, 11, 51414, 51414, 597, 307, 5662, 15874, 293, 8811, 294, 7318, 3309, 13, 51664, 51664, 2555, 2103, 2491, 13, 8349, 311, 33896, 13, 51814], "temperature": 0.0, "avg_logprob": -0.19478580799508602, "compression_ratio": 1.4915966386554622, "no_speech_prob": 0.0003279770608060062}, {"id": 7, "seek": 2672, "start": 47.72, "end": 52.72, "text": " which is increasing inclusion and diversity in AI education.", "tokens": [50364, 3820, 5201, 11, 293, 7318, 8884, 13, 4546, 598, 12, 17493, 278, 11979, 40, 11, 750, 7584, 382, 7680, 295, 20374, 311, 7318, 10137, 13, 50814, 50814, 2491, 13, 8349, 390, 257, 13276, 3117, 412, 3329, 293, 10068, 18944, 468, 295, 7318, 12683, 412, 3329, 8061, 13, 51114, 51114, 2491, 13, 8349, 307, 598, 12, 33348, 293, 6090, 10813, 295, 264, 4048, 2107, 12, 14583, 7318, 337, 1057, 11, 51414, 51414, 597, 307, 5662, 15874, 293, 8811, 294, 7318, 3309, 13, 51664, 51664, 2555, 2103, 2491, 13, 8349, 311, 33896, 13, 51814], "temperature": 0.0, "avg_logprob": -0.19478580799508602, "compression_ratio": 1.4915966386554622, "no_speech_prob": 0.0003279770608060062}, {"id": 8, "seek": 5272, "start": 52.72, "end": 56.72, "text": " Please enjoy Dr. Li's keynote.", "tokens": [50364, 2555, 2103, 2491, 13, 8349, 311, 33896, 13, 50564, 50564, 2421, 11, 1518, 13, 2205, 2446, 11, 665, 6499, 11, 293, 665, 5634, 8660, 294, 264, 1002, 291, 366, 13, 50914, 50914, 1222, 1315, 307, 39587, 12, 37, 17067, 8349, 13, 286, 478, 257, 8304, 412, 20374, 22289, 8976, 5982, 51164, 51164, 293, 611, 598, 12, 18267, 1672, 295, 20374, 311, 9446, 337, 10294, 12, 34, 317, 4073, 7318, 13, 51414, 51414, 2692, 11, 286, 478, 516, 281, 2073, 365, 291, 512, 295, 264, 6792, 589, 490, 452, 2715, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08511701543280419, "compression_ratio": 1.444915254237288, "no_speech_prob": 0.0006834213272668421}, {"id": 9, "seek": 5272, "start": 56.72, "end": 63.72, "text": " Hi, everyone. Good morning, good afternoon, and good evening wherever in the world you are.", "tokens": [50364, 2555, 2103, 2491, 13, 8349, 311, 33896, 13, 50564, 50564, 2421, 11, 1518, 13, 2205, 2446, 11, 665, 6499, 11, 293, 665, 5634, 8660, 294, 264, 1002, 291, 366, 13, 50914, 50914, 1222, 1315, 307, 39587, 12, 37, 17067, 8349, 13, 286, 478, 257, 8304, 412, 20374, 22289, 8976, 5982, 51164, 51164, 293, 611, 598, 12, 18267, 1672, 295, 20374, 311, 9446, 337, 10294, 12, 34, 317, 4073, 7318, 13, 51414, 51414, 2692, 11, 286, 478, 516, 281, 2073, 365, 291, 512, 295, 264, 6792, 589, 490, 452, 2715, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08511701543280419, "compression_ratio": 1.444915254237288, "no_speech_prob": 0.0006834213272668421}, {"id": 10, "seek": 5272, "start": 63.72, "end": 68.72, "text": " My name is Fei-Fei Li. I'm a professor at Stanford Computer Science Department", "tokens": [50364, 2555, 2103, 2491, 13, 8349, 311, 33896, 13, 50564, 50564, 2421, 11, 1518, 13, 2205, 2446, 11, 665, 6499, 11, 293, 665, 5634, 8660, 294, 264, 1002, 291, 366, 13, 50914, 50914, 1222, 1315, 307, 39587, 12, 37, 17067, 8349, 13, 286, 478, 257, 8304, 412, 20374, 22289, 8976, 5982, 51164, 51164, 293, 611, 598, 12, 18267, 1672, 295, 20374, 311, 9446, 337, 10294, 12, 34, 317, 4073, 7318, 13, 51414, 51414, 2692, 11, 286, 478, 516, 281, 2073, 365, 291, 512, 295, 264, 6792, 589, 490, 452, 2715, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08511701543280419, "compression_ratio": 1.444915254237288, "no_speech_prob": 0.0006834213272668421}, {"id": 11, "seek": 5272, "start": 68.72, "end": 73.72, "text": " and also co-director of Stanford's Institute for Human-Centered AI.", "tokens": [50364, 2555, 2103, 2491, 13, 8349, 311, 33896, 13, 50564, 50564, 2421, 11, 1518, 13, 2205, 2446, 11, 665, 6499, 11, 293, 665, 5634, 8660, 294, 264, 1002, 291, 366, 13, 50914, 50914, 1222, 1315, 307, 39587, 12, 37, 17067, 8349, 13, 286, 478, 257, 8304, 412, 20374, 22289, 8976, 5982, 51164, 51164, 293, 611, 598, 12, 18267, 1672, 295, 20374, 311, 9446, 337, 10294, 12, 34, 317, 4073, 7318, 13, 51414, 51414, 2692, 11, 286, 478, 516, 281, 2073, 365, 291, 512, 295, 264, 6792, 589, 490, 452, 2715, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08511701543280419, "compression_ratio": 1.444915254237288, "no_speech_prob": 0.0006834213272668421}, {"id": 12, "seek": 5272, "start": 73.72, "end": 78.72, "text": " Today, I'm going to share with you some of the latest work from my lab.", "tokens": [50364, 2555, 2103, 2491, 13, 8349, 311, 33896, 13, 50564, 50564, 2421, 11, 1518, 13, 2205, 2446, 11, 665, 6499, 11, 293, 665, 5634, 8660, 294, 264, 1002, 291, 366, 13, 50914, 50914, 1222, 1315, 307, 39587, 12, 37, 17067, 8349, 13, 286, 478, 257, 8304, 412, 20374, 22289, 8976, 5982, 51164, 51164, 293, 611, 598, 12, 18267, 1672, 295, 20374, 311, 9446, 337, 10294, 12, 34, 317, 4073, 7318, 13, 51414, 51414, 2692, 11, 286, 478, 516, 281, 2073, 365, 291, 512, 295, 264, 6792, 589, 490, 452, 2715, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08511701543280419, "compression_ratio": 1.444915254237288, "no_speech_prob": 0.0006834213272668421}, {"id": 13, "seek": 7872, "start": 78.72, "end": 85.72, "text": " And the title of the talk is From Seeing to Doing, Understanding and Interacting with the Real World.", "tokens": [50364, 400, 264, 4876, 295, 264, 751, 307, 3358, 19703, 281, 18496, 11, 36858, 293, 5751, 41090, 365, 264, 8467, 3937, 13, 50714, 50714, 286, 528, 281, 747, 291, 646, 1025, 5254, 2459, 924, 2057, 13, 708, 390, 264, 1002, 411, 30, 51014, 51014, 4534, 4882, 11, 767, 439, 4882, 11, 5152, 294, 264, 2886, 765, 831, 7884, 295, 993, 13, 51264, 51264, 400, 456, 3212, 380, 300, 867, 6172, 322, 4755, 13, 51464, 51464, 814, 5240, 15706, 294, 264, 1281, 293, 3745, 257, 6148, 5699, 436, 15706, 538, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07411920896140478, "compression_ratio": 1.493877551020408, "no_speech_prob": 0.0005752197466790676}, {"id": 14, "seek": 7872, "start": 85.72, "end": 91.72, "text": " I want to take you back 540 million years ago. What was the world like?", "tokens": [50364, 400, 264, 4876, 295, 264, 751, 307, 3358, 19703, 281, 18496, 11, 36858, 293, 5751, 41090, 365, 264, 8467, 3937, 13, 50714, 50714, 286, 528, 281, 747, 291, 646, 1025, 5254, 2459, 924, 2057, 13, 708, 390, 264, 1002, 411, 30, 51014, 51014, 4534, 4882, 11, 767, 439, 4882, 11, 5152, 294, 264, 2886, 765, 831, 7884, 295, 993, 13, 51264, 51264, 400, 456, 3212, 380, 300, 867, 6172, 322, 4755, 13, 51464, 51464, 814, 5240, 15706, 294, 264, 1281, 293, 3745, 257, 6148, 5699, 436, 15706, 538, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07411920896140478, "compression_ratio": 1.493877551020408, "no_speech_prob": 0.0005752197466790676}, {"id": 15, "seek": 7872, "start": 91.72, "end": 96.72, "text": " Most animals, actually all animals, lived in the primordial soup of life.", "tokens": [50364, 400, 264, 4876, 295, 264, 751, 307, 3358, 19703, 281, 18496, 11, 36858, 293, 5751, 41090, 365, 264, 8467, 3937, 13, 50714, 50714, 286, 528, 281, 747, 291, 646, 1025, 5254, 2459, 924, 2057, 13, 708, 390, 264, 1002, 411, 30, 51014, 51014, 4534, 4882, 11, 767, 439, 4882, 11, 5152, 294, 264, 2886, 765, 831, 7884, 295, 993, 13, 51264, 51264, 400, 456, 3212, 380, 300, 867, 6172, 322, 4755, 13, 51464, 51464, 814, 5240, 15706, 294, 264, 1281, 293, 3745, 257, 6148, 5699, 436, 15706, 538, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07411920896140478, "compression_ratio": 1.493877551020408, "no_speech_prob": 0.0005752197466790676}, {"id": 16, "seek": 7872, "start": 96.72, "end": 100.72, "text": " And there aren't that many species on Earth.", "tokens": [50364, 400, 264, 4876, 295, 264, 751, 307, 3358, 19703, 281, 18496, 11, 36858, 293, 5751, 41090, 365, 264, 8467, 3937, 13, 50714, 50714, 286, 528, 281, 747, 291, 646, 1025, 5254, 2459, 924, 2057, 13, 708, 390, 264, 1002, 411, 30, 51014, 51014, 4534, 4882, 11, 767, 439, 4882, 11, 5152, 294, 264, 2886, 765, 831, 7884, 295, 993, 13, 51264, 51264, 400, 456, 3212, 380, 300, 867, 6172, 322, 4755, 13, 51464, 51464, 814, 5240, 15706, 294, 264, 1281, 293, 3745, 257, 6148, 5699, 436, 15706, 538, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07411920896140478, "compression_ratio": 1.493877551020408, "no_speech_prob": 0.0005752197466790676}, {"id": 17, "seek": 7872, "start": 100.72, "end": 106.72, "text": " They mostly float in the water and catch a dinner whenever they float by.", "tokens": [50364, 400, 264, 4876, 295, 264, 751, 307, 3358, 19703, 281, 18496, 11, 36858, 293, 5751, 41090, 365, 264, 8467, 3937, 13, 50714, 50714, 286, 528, 281, 747, 291, 646, 1025, 5254, 2459, 924, 2057, 13, 708, 390, 264, 1002, 411, 30, 51014, 51014, 4534, 4882, 11, 767, 439, 4882, 11, 5152, 294, 264, 2886, 765, 831, 7884, 295, 993, 13, 51264, 51264, 400, 456, 3212, 380, 300, 867, 6172, 322, 4755, 13, 51464, 51464, 814, 5240, 15706, 294, 264, 1281, 293, 3745, 257, 6148, 5699, 436, 15706, 538, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07411920896140478, "compression_ratio": 1.493877551020408, "no_speech_prob": 0.0005752197466790676}, {"id": 18, "seek": 10672, "start": 106.72, "end": 112.72, "text": " But something really mysterious happened around 540 million years ago.", "tokens": [50364, 583, 746, 534, 13831, 2011, 926, 1025, 5254, 2459, 924, 2057, 13, 50664, 50664, 682, 257, 588, 2099, 2896, 295, 565, 11, 257, 1871, 295, 1266, 2459, 924, 11, 50914, 50914, 18737, 5313, 362, 9599, 300, 264, 1230, 295, 5496, 6172, 445, 27049, 13, 51264, 51264, 1176, 1092, 664, 1751, 818, 341, 264, 29287, 5501, 15673, 420, 264, 5429, 11538, 295, 9303, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.04754223396529013, "compression_ratio": 1.4564102564102563, "no_speech_prob": 0.0001311274099862203}, {"id": 19, "seek": 10672, "start": 112.72, "end": 117.72, "text": " In a very short period of time, a matter of 10 million years,", "tokens": [50364, 583, 746, 534, 13831, 2011, 926, 1025, 5254, 2459, 924, 2057, 13, 50664, 50664, 682, 257, 588, 2099, 2896, 295, 565, 11, 257, 1871, 295, 1266, 2459, 924, 11, 50914, 50914, 18737, 5313, 362, 9599, 300, 264, 1230, 295, 5496, 6172, 445, 27049, 13, 51264, 51264, 1176, 1092, 664, 1751, 818, 341, 264, 29287, 5501, 15673, 420, 264, 5429, 11538, 295, 9303, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.04754223396529013, "compression_ratio": 1.4564102564102563, "no_speech_prob": 0.0001311274099862203}, {"id": 20, "seek": 10672, "start": 117.72, "end": 124.72, "text": " fossil studies have revealed that the number of animal species just exploded.", "tokens": [50364, 583, 746, 534, 13831, 2011, 926, 1025, 5254, 2459, 924, 2057, 13, 50664, 50664, 682, 257, 588, 2099, 2896, 295, 565, 11, 257, 1871, 295, 1266, 2459, 924, 11, 50914, 50914, 18737, 5313, 362, 9599, 300, 264, 1230, 295, 5496, 6172, 445, 27049, 13, 51264, 51264, 1176, 1092, 664, 1751, 818, 341, 264, 29287, 5501, 15673, 420, 264, 5429, 11538, 295, 9303, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.04754223396529013, "compression_ratio": 1.4564102564102563, "no_speech_prob": 0.0001311274099862203}, {"id": 21, "seek": 10672, "start": 124.72, "end": 131.72, "text": " Zoologists call this the Cambrian explosion or the Big Bang of evolution.", "tokens": [50364, 583, 746, 534, 13831, 2011, 926, 1025, 5254, 2459, 924, 2057, 13, 50664, 50664, 682, 257, 588, 2099, 2896, 295, 565, 11, 257, 1871, 295, 1266, 2459, 924, 11, 50914, 50914, 18737, 5313, 362, 9599, 300, 264, 1230, 295, 5496, 6172, 445, 27049, 13, 51264, 51264, 1176, 1092, 664, 1751, 818, 341, 264, 29287, 5501, 15673, 420, 264, 5429, 11538, 295, 9303, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.04754223396529013, "compression_ratio": 1.4564102564102563, "no_speech_prob": 0.0001311274099862203}, {"id": 22, "seek": 13172, "start": 131.72, "end": 140.72, "text": " So what made the number of animals, the types of animals just increase exponentially?", "tokens": [50364, 407, 437, 1027, 264, 1230, 295, 4882, 11, 264, 3467, 295, 4882, 445, 3488, 37330, 30, 50814, 50814, 663, 575, 668, 257, 11422, 337, 710, 1092, 664, 1751, 293, 3228, 12256, 337, 257, 938, 565, 13, 51114, 51114, 821, 311, 472, 534, 17034, 5261, 300, 20178, 294, 264, 1036, 1916, 295, 7878, 13, 51414, 51414, 400, 309, 311, 257, 5261, 300, 575, 7547, 257, 688, 295, 452, 1065, 589, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04722363884384568, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.00014622315939050168}, {"id": 23, "seek": 13172, "start": 140.72, "end": 146.72, "text": " That has been a mystery for zoologists and biologists for a long time.", "tokens": [50364, 407, 437, 1027, 264, 1230, 295, 4882, 11, 264, 3467, 295, 4882, 445, 3488, 37330, 30, 50814, 50814, 663, 575, 668, 257, 11422, 337, 710, 1092, 664, 1751, 293, 3228, 12256, 337, 257, 938, 565, 13, 51114, 51114, 821, 311, 472, 534, 17034, 5261, 300, 20178, 294, 264, 1036, 1916, 295, 7878, 13, 51414, 51414, 400, 309, 311, 257, 5261, 300, 575, 7547, 257, 688, 295, 452, 1065, 589, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04722363884384568, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.00014622315939050168}, {"id": 24, "seek": 13172, "start": 146.72, "end": 152.72, "text": " There's one really prominent theory that emerged in the last couple of decades.", "tokens": [50364, 407, 437, 1027, 264, 1230, 295, 4882, 11, 264, 3467, 295, 4882, 445, 3488, 37330, 30, 50814, 50814, 663, 575, 668, 257, 11422, 337, 710, 1092, 664, 1751, 293, 3228, 12256, 337, 257, 938, 565, 13, 51114, 51114, 821, 311, 472, 534, 17034, 5261, 300, 20178, 294, 264, 1036, 1916, 295, 7878, 13, 51414, 51414, 400, 309, 311, 257, 5261, 300, 575, 7547, 257, 688, 295, 452, 1065, 589, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04722363884384568, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.00014622315939050168}, {"id": 25, "seek": 13172, "start": 152.72, "end": 157.72, "text": " And it's a theory that has inspired a lot of my own work.", "tokens": [50364, 407, 437, 1027, 264, 1230, 295, 4882, 11, 264, 3467, 295, 4882, 445, 3488, 37330, 30, 50814, 50814, 663, 575, 668, 257, 11422, 337, 710, 1092, 664, 1751, 293, 3228, 12256, 337, 257, 938, 565, 13, 51114, 51114, 821, 311, 472, 534, 17034, 5261, 300, 20178, 294, 264, 1036, 1916, 295, 7878, 13, 51414, 51414, 400, 309, 311, 257, 5261, 300, 575, 7547, 257, 688, 295, 452, 1065, 589, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04722363884384568, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.00014622315939050168}, {"id": 26, "seek": 15772, "start": 157.72, "end": 162.72, "text": " This is proposed by a zoologist from Australia called Andrew Parker.", "tokens": [50364, 639, 307, 10348, 538, 257, 710, 1092, 664, 468, 490, 7060, 1219, 10110, 20155, 13, 50614, 50614, 634, 1619, 300, 29287, 5501, 15673, 307, 21710, 538, 264, 3990, 9303, 295, 5201, 11, 50964, 50964, 597, 992, 766, 364, 27567, 5812, 4569, 689, 4882, 2139, 14178, 420, 4539, 13, 51314, 51314, 8537, 11, 264, 3485, 281, 536, 264, 1002, 11, 281, 536, 1442, 293, 281, 536, 6148, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.050950486895064234, "compression_ratio": 1.5101010101010102, "no_speech_prob": 0.00026031496236100793}, {"id": 27, "seek": 15772, "start": 162.72, "end": 169.72, "text": " He says that Cambrian explosion is triggered by the sudden evolution of vision,", "tokens": [50364, 639, 307, 10348, 538, 257, 710, 1092, 664, 468, 490, 7060, 1219, 10110, 20155, 13, 50614, 50614, 634, 1619, 300, 29287, 5501, 15673, 307, 21710, 538, 264, 3990, 9303, 295, 5201, 11, 50964, 50964, 597, 992, 766, 364, 27567, 5812, 4569, 689, 4882, 2139, 14178, 420, 4539, 13, 51314, 51314, 8537, 11, 264, 3485, 281, 536, 264, 1002, 11, 281, 536, 1442, 293, 281, 536, 6148, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.050950486895064234, "compression_ratio": 1.5101010101010102, "no_speech_prob": 0.00026031496236100793}, {"id": 28, "seek": 15772, "start": 169.72, "end": 176.72, "text": " which set off an evolutionary arms race where animals either evolved or died.", "tokens": [50364, 639, 307, 10348, 538, 257, 710, 1092, 664, 468, 490, 7060, 1219, 10110, 20155, 13, 50614, 50614, 634, 1619, 300, 29287, 5501, 15673, 307, 21710, 538, 264, 3990, 9303, 295, 5201, 11, 50964, 50964, 597, 992, 766, 364, 27567, 5812, 4569, 689, 4882, 2139, 14178, 420, 4539, 13, 51314, 51314, 8537, 11, 264, 3485, 281, 536, 264, 1002, 11, 281, 536, 1442, 293, 281, 536, 6148, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.050950486895064234, "compression_ratio": 1.5101010101010102, "no_speech_prob": 0.00026031496236100793}, {"id": 29, "seek": 15772, "start": 176.72, "end": 182.72, "text": " Basically, the ability to see the world, to see light and to see dinner,", "tokens": [50364, 639, 307, 10348, 538, 257, 710, 1092, 664, 468, 490, 7060, 1219, 10110, 20155, 13, 50614, 50614, 634, 1619, 300, 29287, 5501, 15673, 307, 21710, 538, 264, 3990, 9303, 295, 5201, 11, 50964, 50964, 597, 992, 766, 364, 27567, 5812, 4569, 689, 4882, 2139, 14178, 420, 4539, 13, 51314, 51314, 8537, 11, 264, 3485, 281, 536, 264, 1002, 11, 281, 536, 1442, 293, 281, 536, 6148, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.050950486895064234, "compression_ratio": 1.5101010101010102, "no_speech_prob": 0.00026031496236100793}, {"id": 30, "seek": 18272, "start": 182.72, "end": 188.72, "text": " is the driving force or one of the major driving forces of evolution.", "tokens": [50364, 307, 264, 4840, 3464, 420, 472, 295, 264, 2563, 4840, 5874, 295, 9303, 13, 50664, 50664, 407, 4882, 490, 300, 935, 322, 14178, 294, 439, 3685, 295, 10854, 293, 6422, 294, 1668, 281, 7867, 382, 731, 382, 281, 29501, 13, 51164, 51164, 3358, 300, 935, 322, 281, 965, 11, 321, 362, 4476, 439, 264, 4882, 294, 264, 1002, 362, 512, 733, 295, 5201, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.058638719951405245, "compression_ratio": 1.660919540229885, "no_speech_prob": 0.00015340153186116368}, {"id": 31, "seek": 18272, "start": 188.72, "end": 198.72, "text": " So animals from that point on evolved in all kinds of shapes and forms in order to survive as well as to reproduce.", "tokens": [50364, 307, 264, 4840, 3464, 420, 472, 295, 264, 2563, 4840, 5874, 295, 9303, 13, 50664, 50664, 407, 4882, 490, 300, 935, 322, 14178, 294, 439, 3685, 295, 10854, 293, 6422, 294, 1668, 281, 7867, 382, 731, 382, 281, 29501, 13, 51164, 51164, 3358, 300, 935, 322, 281, 965, 11, 321, 362, 4476, 439, 264, 4882, 294, 264, 1002, 362, 512, 733, 295, 5201, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.058638719951405245, "compression_ratio": 1.660919540229885, "no_speech_prob": 0.00015340153186116368}, {"id": 32, "seek": 18272, "start": 198.72, "end": 206.72, "text": " From that point on to today, we have essentially all the animals in the world have some kind of vision.", "tokens": [50364, 307, 264, 4840, 3464, 420, 472, 295, 264, 2563, 4840, 5874, 295, 9303, 13, 50664, 50664, 407, 4882, 490, 300, 935, 322, 14178, 294, 439, 3685, 295, 10854, 293, 6422, 294, 1668, 281, 7867, 382, 731, 382, 281, 29501, 13, 51164, 51164, 3358, 300, 935, 322, 281, 965, 11, 321, 362, 4476, 439, 264, 4882, 294, 264, 1002, 362, 512, 733, 295, 5201, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.058638719951405245, "compression_ratio": 1.660919540229885, "no_speech_prob": 0.00015340153186116368}, {"id": 33, "seek": 20672, "start": 206.72, "end": 212.72, "text": " And not only it came vision, animals start to develop intelligence.", "tokens": [50364, 400, 406, 787, 309, 1361, 5201, 11, 4882, 722, 281, 1499, 7599, 13, 50664, 50664, 440, 6296, 1185, 4743, 544, 293, 544, 6179, 38573, 13, 50964, 50964, 400, 586, 321, 362, 6255, 365, 472, 295, 264, 881, 6179, 15442, 294, 264, 2503, 295, 527, 1002, 13, 51464, 51464, 407, 300, 307, 257, 588, 11, 588, 11, 588, 5353, 2503, 295, 5201, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0435105562210083, "compression_ratio": 1.5909090909090908, "no_speech_prob": 9.158322791336104e-05}, {"id": 34, "seek": 20672, "start": 212.72, "end": 218.72, "text": " The nervous system developed more and more complicated apparatus.", "tokens": [50364, 400, 406, 787, 309, 1361, 5201, 11, 4882, 722, 281, 1499, 7599, 13, 50664, 50664, 440, 6296, 1185, 4743, 544, 293, 544, 6179, 38573, 13, 50964, 50964, 400, 586, 321, 362, 6255, 365, 472, 295, 264, 881, 6179, 15442, 294, 264, 2503, 295, 527, 1002, 13, 51464, 51464, 407, 300, 307, 257, 588, 11, 588, 11, 588, 5353, 2503, 295, 5201, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0435105562210083, "compression_ratio": 1.5909090909090908, "no_speech_prob": 9.158322791336104e-05}, {"id": 35, "seek": 20672, "start": 218.72, "end": 228.72, "text": " And now we have humans with one of the most complicated brains in the history of our world.", "tokens": [50364, 400, 406, 787, 309, 1361, 5201, 11, 4882, 722, 281, 1499, 7599, 13, 50664, 50664, 440, 6296, 1185, 4743, 544, 293, 544, 6179, 38573, 13, 50964, 50964, 400, 586, 321, 362, 6255, 365, 472, 295, 264, 881, 6179, 15442, 294, 264, 2503, 295, 527, 1002, 13, 51464, 51464, 407, 300, 307, 257, 588, 11, 588, 11, 588, 5353, 2503, 295, 5201, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0435105562210083, "compression_ratio": 1.5909090909090908, "no_speech_prob": 9.158322791336104e-05}, {"id": 36, "seek": 20672, "start": 228.72, "end": 233.72, "text": " So that is a very, very, very brief history of vision.", "tokens": [50364, 400, 406, 787, 309, 1361, 5201, 11, 4882, 722, 281, 1499, 7599, 13, 50664, 50664, 440, 6296, 1185, 4743, 544, 293, 544, 6179, 38573, 13, 50964, 50964, 400, 586, 321, 362, 6255, 365, 472, 295, 264, 881, 6179, 15442, 294, 264, 2503, 295, 527, 1002, 13, 51464, 51464, 407, 300, 307, 257, 588, 11, 588, 11, 588, 5353, 2503, 295, 5201, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0435105562210083, "compression_ratio": 1.5909090909090908, "no_speech_prob": 9.158322791336104e-05}, {"id": 37, "seek": 23372, "start": 233.72, "end": 245.72, "text": " And that's how I think about my research, that I view vision as a cornerstone of intelligence, whether it's biological or artificial.", "tokens": [50364, 400, 300, 311, 577, 286, 519, 466, 452, 2132, 11, 300, 286, 1910, 5201, 382, 257, 4538, 11243, 295, 7599, 11, 1968, 309, 311, 13910, 420, 11677, 13, 50964, 50964, 400, 294, 452, 589, 294, 7318, 293, 3820, 5201, 11, 286, 853, 281, 764, 257, 5201, 281, 1223, 7599, 293, 281, 1322, 13232, 8379, 13, 51564, 51564, 407, 337, 264, 1472, 295, 264, 751, 11, 286, 528, 281, 2073, 365, 291, 437, 5201, 1355, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.041234049615980706, "compression_ratio": 1.5885167464114833, "no_speech_prob": 7.478864426957443e-05}, {"id": 38, "seek": 23372, "start": 245.72, "end": 257.72, "text": " And in my work in AI and computer vision, I try to use a vision to understand intelligence and to build intelligent machines.", "tokens": [50364, 400, 300, 311, 577, 286, 519, 466, 452, 2132, 11, 300, 286, 1910, 5201, 382, 257, 4538, 11243, 295, 7599, 11, 1968, 309, 311, 13910, 420, 11677, 13, 50964, 50964, 400, 294, 452, 589, 294, 7318, 293, 3820, 5201, 11, 286, 853, 281, 764, 257, 5201, 281, 1223, 7599, 293, 281, 1322, 13232, 8379, 13, 51564, 51564, 407, 337, 264, 1472, 295, 264, 751, 11, 286, 528, 281, 2073, 365, 291, 437, 5201, 1355, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.041234049615980706, "compression_ratio": 1.5885167464114833, "no_speech_prob": 7.478864426957443e-05}, {"id": 39, "seek": 23372, "start": 257.72, "end": 262.72, "text": " So for the rest of the talk, I want to share with you what vision means.", "tokens": [50364, 400, 300, 311, 577, 286, 519, 466, 452, 2132, 11, 300, 286, 1910, 5201, 382, 257, 4538, 11243, 295, 7599, 11, 1968, 309, 311, 13910, 420, 11677, 13, 50964, 50964, 400, 294, 452, 589, 294, 7318, 293, 3820, 5201, 11, 286, 853, 281, 764, 257, 5201, 281, 1223, 7599, 293, 281, 1322, 13232, 8379, 13, 51564, 51564, 407, 337, 264, 1472, 295, 264, 751, 11, 286, 528, 281, 2073, 365, 291, 437, 5201, 1355, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.041234049615980706, "compression_ratio": 1.5885167464114833, "no_speech_prob": 7.478864426957443e-05}, {"id": 40, "seek": 26272, "start": 262.72, "end": 265.72, "text": " To me, it means two very important things.", "tokens": [50364, 1407, 385, 11, 309, 1355, 732, 588, 1021, 721, 13, 50514, 50514, 1485, 307, 281, 1223, 264, 957, 1002, 13, 50664, 50664, 440, 661, 307, 337, 884, 721, 11, 18017, 293, 6577, 294, 264, 957, 1002, 13, 50964, 50964, 961, 311, 722, 538, 264, 700, 11, 3701, 13, 51164, 51164, 17303, 12256, 362, 1907, 505, 293, 1143, 5313, 281, 855, 300, 1952, 5201, 307, 12802, 13, 51514, 51514, 35809, 366, 8189, 295, 9016, 2123, 957, 1002, 6565, 293, 721, 294, 257, 534, 17778, 636, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07695600959692109, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.0001353939005639404}, {"id": 41, "seek": 26272, "start": 265.72, "end": 268.72, "text": " One is to understand the real world.", "tokens": [50364, 1407, 385, 11, 309, 1355, 732, 588, 1021, 721, 13, 50514, 50514, 1485, 307, 281, 1223, 264, 957, 1002, 13, 50664, 50664, 440, 661, 307, 337, 884, 721, 11, 18017, 293, 6577, 294, 264, 957, 1002, 13, 50964, 50964, 961, 311, 722, 538, 264, 700, 11, 3701, 13, 51164, 51164, 17303, 12256, 362, 1907, 505, 293, 1143, 5313, 281, 855, 300, 1952, 5201, 307, 12802, 13, 51514, 51514, 35809, 366, 8189, 295, 9016, 2123, 957, 1002, 6565, 293, 721, 294, 257, 534, 17778, 636, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07695600959692109, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.0001353939005639404}, {"id": 42, "seek": 26272, "start": 268.72, "end": 274.72, "text": " The other is for doing things, interacting and acting in the real world.", "tokens": [50364, 1407, 385, 11, 309, 1355, 732, 588, 1021, 721, 13, 50514, 50514, 1485, 307, 281, 1223, 264, 957, 1002, 13, 50664, 50664, 440, 661, 307, 337, 884, 721, 11, 18017, 293, 6577, 294, 264, 957, 1002, 13, 50964, 50964, 961, 311, 722, 538, 264, 700, 11, 3701, 13, 51164, 51164, 17303, 12256, 362, 1907, 505, 293, 1143, 5313, 281, 855, 300, 1952, 5201, 307, 12802, 13, 51514, 51514, 35809, 366, 8189, 295, 9016, 2123, 957, 1002, 6565, 293, 721, 294, 257, 534, 17778, 636, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07695600959692109, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.0001353939005639404}, {"id": 43, "seek": 26272, "start": 274.72, "end": 278.72, "text": " Let's start by the first, understanding.", "tokens": [50364, 1407, 385, 11, 309, 1355, 732, 588, 1021, 721, 13, 50514, 50514, 1485, 307, 281, 1223, 264, 957, 1002, 13, 50664, 50664, 440, 661, 307, 337, 884, 721, 11, 18017, 293, 6577, 294, 264, 957, 1002, 13, 50964, 50964, 961, 311, 722, 538, 264, 700, 11, 3701, 13, 51164, 51164, 17303, 12256, 362, 1907, 505, 293, 1143, 5313, 281, 855, 300, 1952, 5201, 307, 12802, 13, 51514, 51514, 35809, 366, 8189, 295, 9016, 2123, 957, 1002, 6565, 293, 721, 294, 257, 534, 17778, 636, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07695600959692109, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.0001353939005639404}, {"id": 44, "seek": 26272, "start": 278.72, "end": 285.72, "text": " Psychologists have told us and used studies to show that human vision is remarkable.", "tokens": [50364, 1407, 385, 11, 309, 1355, 732, 588, 1021, 721, 13, 50514, 50514, 1485, 307, 281, 1223, 264, 957, 1002, 13, 50664, 50664, 440, 661, 307, 337, 884, 721, 11, 18017, 293, 6577, 294, 264, 957, 1002, 13, 50964, 50964, 961, 311, 722, 538, 264, 700, 11, 3701, 13, 51164, 51164, 17303, 12256, 362, 1907, 505, 293, 1143, 5313, 281, 855, 300, 1952, 5201, 307, 12802, 13, 51514, 51514, 35809, 366, 8189, 295, 9016, 2123, 957, 1002, 6565, 293, 721, 294, 257, 534, 17778, 636, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07695600959692109, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.0001353939005639404}, {"id": 45, "seek": 26272, "start": 285.72, "end": 291.72, "text": " Humans are capable of perceiving real world objects and things in a really phenomenal way.", "tokens": [50364, 1407, 385, 11, 309, 1355, 732, 588, 1021, 721, 13, 50514, 50514, 1485, 307, 281, 1223, 264, 957, 1002, 13, 50664, 50664, 440, 661, 307, 337, 884, 721, 11, 18017, 293, 6577, 294, 264, 957, 1002, 13, 50964, 50964, 961, 311, 722, 538, 264, 700, 11, 3701, 13, 51164, 51164, 17303, 12256, 362, 1907, 505, 293, 1143, 5313, 281, 855, 300, 1952, 5201, 307, 12802, 13, 51514, 51514, 35809, 366, 8189, 295, 9016, 2123, 957, 1002, 6565, 293, 721, 294, 257, 534, 17778, 636, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07695600959692109, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.0001353939005639404}, {"id": 46, "seek": 29172, "start": 291.72, "end": 309.72, "text": " In this very early study by cognitive scientist Irv Biederman in the 70s, he showed that the ability to recognize a bicycle in two different pictures, one coherent, one incoherent picture, was very dramatically different.", "tokens": [50364, 682, 341, 588, 2440, 2979, 538, 15605, 12662, 9151, 85, 363, 1091, 11821, 294, 264, 5285, 82, 11, 415, 4712, 300, 264, 3485, 281, 5521, 257, 20888, 294, 732, 819, 5242, 11, 472, 36239, 11, 472, 834, 78, 511, 317, 3036, 11, 390, 588, 17548, 819, 13, 51264, 51264, 35809, 366, 1101, 412, 2577, 47913, 294, 257, 36239, 4145, 11, 754, 1673, 264, 20888, 2564, 6132, 380, 3105, 9253, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08406628789128484, "compression_ratio": 1.6231884057971016, "no_speech_prob": 0.0001658537657931447}, {"id": 47, "seek": 29172, "start": 309.72, "end": 318.72, "text": " Humans are better at seeing bicycles in a coherent scene, even though the bicycle itself hasn't changed locations.", "tokens": [50364, 682, 341, 588, 2440, 2979, 538, 15605, 12662, 9151, 85, 363, 1091, 11821, 294, 264, 5285, 82, 11, 415, 4712, 300, 264, 3485, 281, 5521, 257, 20888, 294, 732, 819, 5242, 11, 472, 36239, 11, 472, 834, 78, 511, 317, 3036, 11, 390, 588, 17548, 819, 13, 51264, 51264, 35809, 366, 1101, 412, 2577, 47913, 294, 257, 36239, 4145, 11, 754, 1673, 264, 20888, 2564, 6132, 380, 3105, 9253, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08406628789128484, "compression_ratio": 1.6231884057971016, "no_speech_prob": 0.0001658537657931447}, {"id": 48, "seek": 31872, "start": 318.72, "end": 329.72, "text": " Concurrently, Molly Potter and some of her colleagues have shown that humans have a remarkable ability of detecting novel objects.", "tokens": [50364, 2656, 49827, 356, 11, 26665, 18115, 293, 512, 295, 720, 7734, 362, 4898, 300, 6255, 362, 257, 12802, 3485, 295, 40237, 7613, 6565, 13, 50914, 50914, 682, 341, 960, 11, 291, 603, 536, 456, 311, 472, 3920, 300, 8306, 257, 954, 11, 754, 1673, 291, 600, 1128, 1612, 341, 960, 11, 291, 362, 572, 1154, 295, 40237, 689, 264, 954, 307, 11, 9810, 437, 415, 420, 750, 307, 264, 4914, 322, 264, 2568, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09395481989933895, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.0002956274838652462}, {"id": 49, "seek": 31872, "start": 329.72, "end": 347.72, "text": " In this video, you'll see there's one frame that contains a person, even though you've never seen this video, you have no problem of detecting where the person is, roughly what he or she is the location on the screen.", "tokens": [50364, 2656, 49827, 356, 11, 26665, 18115, 293, 512, 295, 720, 7734, 362, 4898, 300, 6255, 362, 257, 12802, 3485, 295, 40237, 7613, 6565, 13, 50914, 50914, 682, 341, 960, 11, 291, 603, 536, 456, 311, 472, 3920, 300, 8306, 257, 954, 11, 754, 1673, 291, 600, 1128, 1612, 341, 960, 11, 291, 362, 572, 1154, 295, 40237, 689, 264, 954, 307, 11, 9810, 437, 415, 420, 750, 307, 264, 4914, 322, 264, 2568, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09395481989933895, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.0002956274838652462}, {"id": 50, "seek": 34772, "start": 347.72, "end": 365.72, "text": " The gestures, and keep in mind, every frame is only presented for 100 milliseconds. So this, the frame changes at 10 hertz, yet our visual system is very good at detecting these novel objects.", "tokens": [50364, 440, 28475, 11, 293, 1066, 294, 1575, 11, 633, 3920, 307, 787, 8212, 337, 2319, 34184, 13, 407, 341, 11, 264, 3920, 2962, 412, 1266, 45830, 11, 1939, 527, 5056, 1185, 307, 588, 665, 412, 40237, 613, 7613, 6565, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.1831256259571422, "compression_ratio": 1.3617021276595744, "no_speech_prob": 0.00029082701075822115}, {"id": 51, "seek": 36572, "start": 365.72, "end": 389.72, "text": " Back in 1996, about 25 years ago, psychoneurophysiologists Simon Thorpe and his colleagues have shown through brain EEG study that as early as 150 milliseconds after a picture is shown, our brain shows a differential signal that can tell apart a", "tokens": [50364, 5833, 294, 22690, 11, 466, 3552, 924, 2057, 11, 4681, 546, 7052, 950, 749, 72, 12256, 13193, 17777, 494, 293, 702, 7734, 362, 4898, 807, 3567, 33685, 38, 2979, 300, 382, 2440, 382, 8451, 34184, 934, 257, 3036, 307, 4898, 11, 527, 3567, 3110, 257, 15756, 6358, 300, 393, 980, 4936, 257, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.15108985560280935, "compression_ratio": 1.3461538461538463, "no_speech_prob": 0.0003350444894749671}, {"id": 52, "seek": 38972, "start": 389.72, "end": 404.72, "text": " picture with animals versus a picture without animals. And here we're talking about all kinds of animals among all kinds of destructor images. So it's quite a remarkable ability of human vision.", "tokens": [50364, 3036, 365, 4882, 5717, 257, 3036, 1553, 4882, 13, 400, 510, 321, 434, 1417, 466, 439, 3685, 295, 4882, 3654, 439, 3685, 295, 2677, 14535, 5267, 13, 407, 309, 311, 1596, 257, 12802, 3485, 295, 1952, 5201, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.09610639299665179, "compression_ratio": 1.4586466165413534, "no_speech_prob": 0.00042996075353585184}, {"id": 53, "seek": 40472, "start": 404.72, "end": 424.72, "text": " I myself, about 15 years ago have done a experiment. When I was a graduate student, where we put human subjects in front of a computer screen and flash to them, real world photos masked by a wallpaper looking structure.", "tokens": [50364, 286, 2059, 11, 466, 2119, 924, 2057, 362, 1096, 257, 5120, 13, 1133, 286, 390, 257, 8080, 3107, 11, 689, 321, 829, 1952, 13066, 294, 1868, 295, 257, 3820, 2568, 293, 7319, 281, 552, 11, 957, 1002, 5787, 45249, 538, 257, 43293, 1237, 3877, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.14781136415442642, "compression_ratio": 1.3518518518518519, "no_speech_prob": 0.0008027285221032798}, {"id": 54, "seek": 42472, "start": 424.72, "end": 439.72, "text": " And I asked human subject to type what they see. You can see some of these images are flashed in really really fast way, yet humans are very good at seeing what these things are.", "tokens": [50364, 400, 286, 2351, 1952, 3983, 281, 2010, 437, 436, 536, 13, 509, 393, 536, 512, 295, 613, 5267, 366, 7319, 292, 294, 534, 534, 2370, 636, 11, 1939, 6255, 366, 588, 665, 412, 2577, 437, 613, 721, 366, 13, 51114, 51114, 759, 264, 3036, 307, 8212, 337, 5923, 34184, 11, 309, 311, 411, 27162, 11, 561, 393, 2464, 24574, 498, 291, 1689, 552, 1547, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.17016452291737433, "compression_ratio": 1.48, "no_speech_prob": 2.282184323121328e-05}, {"id": 55, "seek": 42472, "start": 439.72, "end": 448.72, "text": " If the picture is presented for 500 milliseconds, it's like eternity, people can write novels if you pay them enough.", "tokens": [50364, 400, 286, 2351, 1952, 3983, 281, 2010, 437, 436, 536, 13, 509, 393, 536, 512, 295, 613, 5267, 366, 7319, 292, 294, 534, 534, 2370, 636, 11, 1939, 6255, 366, 588, 665, 412, 2577, 437, 613, 721, 366, 13, 51114, 51114, 759, 264, 3036, 307, 8212, 337, 5923, 34184, 11, 309, 311, 411, 27162, 11, 561, 393, 2464, 24574, 498, 291, 1689, 552, 1547, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.17016452291737433, "compression_ratio": 1.48, "no_speech_prob": 2.282184323121328e-05}, {"id": 56, "seek": 44872, "start": 448.72, "end": 468.72, "text": " So, there's something special about our visual system, we can use it to understand the world. In fact, Alan Turing, one of the most inspiring person in the history of computer science and inspiring to AI the field of AI has said that it's possible", "tokens": [50364, 407, 11, 456, 311, 746, 2121, 466, 527, 5056, 1185, 11, 321, 393, 764, 309, 281, 1223, 264, 1002, 13, 682, 1186, 11, 16442, 314, 1345, 11, 472, 295, 264, 881, 15883, 954, 294, 264, 2503, 295, 3820, 3497, 293, 15883, 281, 7318, 264, 2519, 295, 7318, 575, 848, 300, 309, 311, 1944, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.07659078062626354, "compression_ratio": 1.4702380952380953, "no_speech_prob": 5.22114678460639e-05}, {"id": 57, "seek": 46872, "start": 468.72, "end": 486.72, "text": " to has conjecture to use a machine and teach it to understand the real world. And this is what I think seeing is for seeing is for understanding is for making sense of what this visual world is about.", "tokens": [50364, 281, 575, 416, 1020, 540, 281, 764, 257, 3479, 293, 2924, 309, 281, 1223, 264, 957, 1002, 13, 400, 341, 307, 437, 286, 519, 2577, 307, 337, 2577, 307, 337, 3701, 307, 337, 1455, 2020, 295, 437, 341, 5056, 1002, 307, 466, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.0734963873599438, "compression_ratio": 1.6129032258064515, "no_speech_prob": 8.74411198310554e-05}, {"id": 58, "seek": 48672, "start": 486.72, "end": 499.72, "text": " So back to this experiment. We see that humans are able to understand and make sense and perceive the visual world. But what are the key elements or building blocks of this.", "tokens": [50364, 407, 646, 281, 341, 5120, 13, 492, 536, 300, 6255, 366, 1075, 281, 1223, 293, 652, 2020, 293, 20281, 264, 5056, 1002, 13, 583, 437, 366, 264, 2141, 4959, 420, 2390, 8474, 295, 341, 13, 51014, 51014], "temperature": 0.0, "avg_logprob": -0.07718184055426182, "compression_ratio": 1.3515625, "no_speech_prob": 4.75540837214794e-05}, {"id": 59, "seek": 49972, "start": 499.72, "end": 518.72, "text": " If you look at what humans type when presented with a picture like this. They talk a lot about objects like men fist face grass helmet clothing trees at dogs or other things.", "tokens": [50364, 759, 291, 574, 412, 437, 6255, 2010, 562, 8212, 365, 257, 3036, 411, 341, 13, 814, 751, 257, 688, 466, 6565, 411, 1706, 21849, 1851, 8054, 15922, 11502, 5852, 412, 7197, 420, 661, 721, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.1253058604705028, "compression_ratio": 1.3384615384615384, "no_speech_prob": 2.17758151848102e-05}, {"id": 60, "seek": 51872, "start": 518.72, "end": 535.72, "text": " And indeed, object recognition is a building block of visual understanding or of vision. And for those of you who are not familiar with this, what is object understanding, it's defined by the task of showing a visual system, whether it's a biological", "tokens": [50364, 400, 6451, 11, 2657, 11150, 307, 257, 2390, 3461, 295, 5056, 3701, 420, 295, 5201, 13, 400, 337, 729, 295, 291, 567, 366, 406, 4963, 365, 341, 11, 437, 307, 2657, 3701, 11, 309, 311, 7642, 538, 264, 5633, 295, 4099, 257, 5056, 1185, 11, 1968, 309, 311, 257, 13910, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.06967822710673015, "compression_ratio": 1.5527950310559007, "no_speech_prob": 6.011566074448638e-05}, {"id": 61, "seek": 53572, "start": 535.72, "end": 549.72, "text": " system like our own, or a computer, a picture, and the system is able to identify what is the main object in the picture, for example, this is a one that in the picture.", "tokens": [50364, 1185, 411, 527, 1065, 11, 420, 257, 3820, 11, 257, 3036, 11, 293, 264, 1185, 307, 1075, 281, 5876, 437, 307, 264, 2135, 2657, 294, 264, 3036, 11, 337, 1365, 11, 341, 307, 257, 472, 300, 294, 264, 3036, 13, 51064, 51064], "temperature": 0.0, "avg_logprob": -0.0997388471256603, "compression_ratio": 1.456896551724138, "no_speech_prob": 9.606988896848634e-05}, {"id": 62, "seek": 54972, "start": 549.72, "end": 570.72, "text": " Why is it hard or maybe it is not because humans can do this easily. It turns out it's actually quite a difficult task for computers. For one thing, computers have to see this in just numbers, you know color numbers or luminous numbers, but", "tokens": [50364, 1545, 307, 309, 1152, 420, 1310, 309, 307, 406, 570, 6255, 393, 360, 341, 3612, 13, 467, 4523, 484, 309, 311, 767, 1596, 257, 2252, 5633, 337, 10807, 13, 1171, 472, 551, 11, 10807, 362, 281, 536, 341, 294, 445, 3547, 11, 291, 458, 2017, 3547, 420, 32476, 563, 3547, 11, 457, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.10518085956573486, "compression_ratio": 1.4634146341463414, "no_speech_prob": 5.142698501003906e-05}, {"id": 63, "seek": 57072, "start": 570.72, "end": 588.72, "text": " going from numbers to the understanding that there is a one that takes a lot of computation. In fact, objects, even though they can be the same object, they can come in many different kind of shape and form and environment, not to mention, there is a 3D world that", "tokens": [50364, 516, 490, 3547, 281, 264, 3701, 300, 456, 307, 257, 472, 300, 2516, 257, 688, 295, 24903, 13, 682, 1186, 11, 6565, 11, 754, 1673, 436, 393, 312, 264, 912, 2657, 11, 436, 393, 808, 294, 867, 819, 733, 295, 3909, 293, 1254, 293, 2823, 11, 406, 281, 2152, 11, 456, 307, 257, 805, 35, 1002, 300, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.12935966741843302, "compression_ratio": 1.5529411764705883, "no_speech_prob": 5.1421462558209896e-05}, {"id": 64, "seek": 58872, "start": 588.72, "end": 602.72, "text": " renders these objects in very infinite number of possibilities. In fact, this has understanding objects or object recognition has been a quest for more than half a century in computer vision.", "tokens": [50364, 6125, 433, 613, 6565, 294, 588, 13785, 1230, 295, 12178, 13, 682, 1186, 11, 341, 575, 3701, 6565, 420, 2657, 11150, 575, 668, 257, 866, 337, 544, 813, 1922, 257, 4901, 294, 3820, 5201, 13, 51064, 51064], "temperature": 0.0, "avg_logprob": -0.06086218051421337, "compression_ratio": 1.4253731343283582, "no_speech_prob": 8.883297414286062e-05}, {"id": 65, "seek": 60272, "start": 602.72, "end": 623.72, "text": " In early days, people try to use hand designed models to configure geometric shapes to try to express objects in a mathematical language. And there was some heroic efforts in the 60s, 70s, about object recognition.", "tokens": [50364, 682, 2440, 1708, 11, 561, 853, 281, 764, 1011, 4761, 5245, 281, 22162, 33246, 10854, 281, 853, 281, 5109, 6565, 294, 257, 18894, 2856, 13, 400, 456, 390, 512, 32915, 6484, 294, 264, 4060, 82, 11, 5285, 82, 11, 466, 2657, 11150, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.16327022998890978, "compression_ratio": 1.4078947368421053, "no_speech_prob": 0.00012718079960905015}, {"id": 66, "seek": 62372, "start": 623.72, "end": 643.72, "text": " And to fast forward. Shortly before the turn of the century, machine learning as a field became a really important mathematical tool for computer vision and AI, and we computer scientists learned that we don't have to hand design models, we can", "tokens": [50364, 400, 281, 2370, 2128, 13, 40109, 949, 264, 1261, 295, 264, 4901, 11, 3479, 2539, 382, 257, 2519, 3062, 257, 534, 1021, 18894, 2290, 337, 3820, 5201, 293, 7318, 11, 293, 321, 3820, 7708, 3264, 300, 321, 500, 380, 362, 281, 1011, 1715, 5245, 11, 321, 393, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.15745232152003868, "compression_ratio": 1.4698795180722892, "no_speech_prob": 5.736377715948038e-05}, {"id": 67, "seek": 64372, "start": 643.72, "end": 662.72, "text": " use the models and the parameters, but we have to rely on hand design features. So we input features, whether it's patches of images, or some kind of encoding of pixels, and then we try to learn through data and through learning models, how these", "tokens": [50364, 764, 264, 5245, 293, 264, 9834, 11, 457, 321, 362, 281, 10687, 322, 1011, 1715, 4122, 13, 407, 321, 4846, 4122, 11, 1968, 309, 311, 26531, 295, 5267, 11, 420, 512, 733, 295, 43430, 295, 18668, 11, 293, 550, 321, 853, 281, 1466, 807, 1412, 293, 807, 2539, 5245, 11, 577, 613, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.207888058253697, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.00010551377636147663}, {"id": 68, "seek": 66272, "start": 662.72, "end": 680.72, "text": " can be configured. And there were a lot of great work that can come out of this. As we start to push towards solving the problem of object recognition, one important aspect of the, of the work or research came about, and that is the design of", "tokens": [50364, 393, 312, 30538, 13, 400, 456, 645, 257, 688, 295, 869, 589, 300, 393, 808, 484, 295, 341, 13, 1018, 321, 722, 281, 2944, 3030, 12606, 264, 1154, 295, 2657, 11150, 11, 472, 1021, 4171, 295, 264, 11, 295, 264, 589, 420, 2132, 1361, 466, 11, 293, 300, 307, 264, 1715, 295, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.12752542325428554, "compression_ratio": 1.5220125786163523, "no_speech_prob": 9.163669164991006e-05}, {"id": 69, "seek": 68072, "start": 680.72, "end": 700.72, "text": " data sets and benchmarks. In the early days of object recognition one of the most prominent data set was Europeans Pascal VOC data set focus on 20 object categories, and it was released annually between 2006 to 2012 to encourage the field of computer", "tokens": [50364, 1412, 6352, 293, 43751, 13, 682, 264, 2440, 1708, 295, 2657, 11150, 472, 295, 264, 881, 17034, 1412, 992, 390, 29746, 41723, 15216, 34, 1412, 992, 1879, 322, 945, 2657, 10479, 11, 293, 309, 390, 4736, 29974, 1296, 14062, 281, 9125, 281, 5373, 264, 2519, 295, 3820, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.1434642006369198, "compression_ratio": 1.4124293785310735, "no_speech_prob": 0.00027755950577557087}, {"id": 70, "seek": 70072, "start": 700.72, "end": 712.72, "text": " recognition. All the labs worldwide to benchmark against the testing data of this data set to make to assess the progress of the field.", "tokens": [50364, 11150, 13, 1057, 264, 20339, 13485, 281, 18927, 1970, 264, 4997, 1412, 295, 341, 1412, 992, 281, 652, 281, 5877, 264, 4205, 295, 264, 2519, 13, 50964, 50964, 583, 264, 3494, 307, 11, 264, 1002, 307, 257, 688, 4833, 813, 945, 10479, 13, 467, 311, 544, 813, 309, 311, 294, 1186, 41562, 362, 14109, 1266, 82, 295, 5383, 498, 406, 6779, 295, 5383, 295, 10479, 295, 6565, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1484396325217353, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.00012714964395854622}, {"id": 71, "seek": 70072, "start": 712.72, "end": 728.72, "text": " But the truth is, the world is a lot larger than 20 categories. It's more than it's in fact psychologists have estimated 10s of thousands if not hundreds of thousands of categories of objects.", "tokens": [50364, 11150, 13, 1057, 264, 20339, 13485, 281, 18927, 1970, 264, 4997, 1412, 295, 341, 1412, 992, 281, 652, 281, 5877, 264, 4205, 295, 264, 2519, 13, 50964, 50964, 583, 264, 3494, 307, 11, 264, 1002, 307, 257, 688, 4833, 813, 945, 10479, 13, 467, 311, 544, 813, 309, 311, 294, 1186, 41562, 362, 14109, 1266, 82, 295, 5383, 498, 406, 6779, 295, 5383, 295, 10479, 295, 6565, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1484396325217353, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.00012714964395854622}, {"id": 72, "seek": 72872, "start": 728.72, "end": 741.72, "text": " And here, I want to bring you a quote of one of the most important psychologists that has influenced my thinking in terms of how to working AI and that's JJ Gibson.", "tokens": [50364, 400, 510, 11, 286, 528, 281, 1565, 291, 257, 6513, 295, 472, 295, 264, 881, 1021, 41562, 300, 575, 15269, 452, 1953, 294, 2115, 295, 577, 281, 1364, 7318, 293, 300, 311, 21386, 42250, 13, 51014, 51014, 42250, 575, 848, 11, 420, 257, 29514, 575, 36992, 1703, 1937, 42250, 538, 1566, 11, 1029, 406, 437, 311, 1854, 428, 1378, 11, 457, 437, 428, 1378, 307, 1854, 766, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11670168240865071, "compression_ratio": 1.5957446808510638, "no_speech_prob": 0.00016333935491275042}, {"id": 73, "seek": 72872, "start": 741.72, "end": 755.72, "text": " Gibson has said, or a psychologist has paraphrased Gibson by saying, ask not what's inside your head, but what your head is inside off.", "tokens": [50364, 400, 510, 11, 286, 528, 281, 1565, 291, 257, 6513, 295, 472, 295, 264, 881, 1021, 41562, 300, 575, 15269, 452, 1953, 294, 2115, 295, 577, 281, 1364, 7318, 293, 300, 311, 21386, 42250, 13, 51014, 51014, 42250, 575, 848, 11, 420, 257, 29514, 575, 36992, 1703, 1937, 42250, 538, 1566, 11, 1029, 406, 437, 311, 1854, 428, 1378, 11, 457, 437, 428, 1378, 307, 1854, 766, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11670168240865071, "compression_ratio": 1.5957446808510638, "no_speech_prob": 0.00016333935491275042}, {"id": 74, "seek": 75572, "start": 755.72, "end": 769.72, "text": " And this is a really important concept of encouraging us to think about an ecological approach to perception. So when we are working on, say, object recognition.", "tokens": [50364, 400, 341, 307, 257, 534, 1021, 3410, 295, 14580, 505, 281, 519, 466, 364, 31054, 3109, 281, 12860, 13, 407, 562, 321, 366, 1364, 322, 11, 584, 11, 2657, 11150, 13, 51064, 51064, 492, 458, 300, 309, 311, 257, 2390, 3461, 337, 3701, 264, 1002, 13, 492, 534, 643, 281, 16078, 322, 264, 4373, 295, 264, 957, 1002, 11, 7547, 538, 341, 3410, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09964538321775548, "compression_ratio": 1.5396039603960396, "no_speech_prob": 7.962221570778638e-05}, {"id": 75, "seek": 75572, "start": 769.72, "end": 782.72, "text": " We know that it's a building block for understanding the world. We really need to emphasize on the scale of the real world, inspired by this concept.", "tokens": [50364, 400, 341, 307, 257, 534, 1021, 3410, 295, 14580, 505, 281, 519, 466, 364, 31054, 3109, 281, 12860, 13, 407, 562, 321, 366, 1364, 322, 11, 584, 11, 2657, 11150, 13, 51064, 51064, 492, 458, 300, 309, 311, 257, 2390, 3461, 337, 3701, 264, 1002, 13, 492, 534, 643, 281, 16078, 322, 264, 4373, 295, 264, 957, 1002, 11, 7547, 538, 341, 3410, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09964538321775548, "compression_ratio": 1.5396039603960396, "no_speech_prob": 7.962221570778638e-05}, {"id": 76, "seek": 78272, "start": 782.72, "end": 799.72, "text": " Around 2007, my students and I were looking at the size of the data sets towards training object recognition models, and we were deeply unsatisfied because they hover around thousands if not, or 10s of thousands, but truly small compared to the", "tokens": [50364, 17633, 12656, 11, 452, 1731, 293, 286, 645, 1237, 412, 264, 2744, 295, 264, 1412, 6352, 3030, 3097, 2657, 11150, 5245, 11, 293, 321, 645, 8760, 2693, 38502, 570, 436, 20076, 926, 5383, 498, 406, 11, 420, 1266, 82, 295, 5383, 11, 457, 4908, 1359, 5347, 281, 264, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.1305367763225849, "compression_ratio": 1.4437869822485208, "no_speech_prob": 5.560203135246411e-05}, {"id": 77, "seek": 79972, "start": 799.72, "end": 816.72, "text": " size of the, to the to the visual world that we experience. This is when we built together ImageNet, a data set of 15 million images across 22,000 object categories.", "tokens": [50364, 2744, 295, 264, 11, 281, 264, 281, 264, 5056, 1002, 300, 321, 1752, 13, 639, 307, 562, 321, 3094, 1214, 29903, 31890, 11, 257, 1412, 992, 295, 2119, 2459, 5267, 2108, 5853, 11, 1360, 2657, 10479, 13, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.14085634743295064, "compression_ratio": 1.2595419847328244, "no_speech_prob": 0.00010884115908993408}, {"id": 78, "seek": 81672, "start": 816.72, "end": 837.72, "text": " And the goal of ImageNet is to really establish object recognition as one of the most important North stars in computer vision, and use the benchmark data set of ImageNet to, to encourage training with real world scale and understanding", "tokens": [50364, 400, 264, 3387, 295, 29903, 31890, 307, 281, 534, 8327, 2657, 11150, 382, 472, 295, 264, 881, 1021, 4067, 6105, 294, 3820, 5201, 11, 293, 764, 264, 18927, 1412, 992, 295, 29903, 31890, 281, 11, 281, 5373, 3097, 365, 957, 1002, 4373, 293, 3701, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.10635990897814433, "compression_ratio": 1.4567901234567902, "no_speech_prob": 9.307778236689046e-05}, {"id": 79, "seek": 83772, "start": 837.72, "end": 855.72, "text": " the real world scale. Of course, a lot of you are already familiar with the rest of the history, ImageNet put together an international challenge annually between 2010 and 2017, and our testing data set become a benchmark data set for the field of", "tokens": [50364, 264, 957, 1002, 4373, 13, 2720, 1164, 11, 257, 688, 295, 291, 366, 1217, 4963, 365, 264, 1472, 295, 264, 2503, 11, 29903, 31890, 829, 1214, 364, 5058, 3430, 29974, 1296, 9657, 293, 6591, 11, 293, 527, 4997, 1412, 992, 1813, 257, 18927, 1412, 992, 337, 264, 2519, 295, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.18942554042024431, "compression_ratio": 1.436046511627907, "no_speech_prob": 0.00012523190525826067}, {"id": 80, "seek": 85572, "start": 855.72, "end": 872.72, "text": " computer vision object recognition research community, and 2012, the winner of the ImageNet challenge, especially object classification challenge was a neural net convolutional neural network model.", "tokens": [50364, 3820, 5201, 2657, 11150, 2132, 1768, 11, 293, 9125, 11, 264, 8507, 295, 264, 29903, 31890, 3430, 11, 2318, 2657, 21538, 3430, 390, 257, 18161, 2533, 45216, 304, 18161, 3209, 2316, 13, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.1817875968085395, "compression_ratio": 1.4142857142857144, "no_speech_prob": 0.00010546540579525754}, {"id": 81, "seek": 87272, "start": 872.72, "end": 894.72, "text": " And that was the beginning of deep learnings revolution. And since then, we have seen a lot of different models built upon and benchmarked against the ImageNet, and the field has made tremendous progress.", "tokens": [50364, 400, 300, 390, 264, 2863, 295, 2452, 2539, 82, 8894, 13, 400, 1670, 550, 11, 321, 362, 1612, 257, 688, 295, 819, 5245, 3094, 3564, 293, 18927, 292, 1970, 264, 29903, 31890, 11, 293, 264, 2519, 575, 1027, 10048, 4205, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.07452837096320258, "compression_ratio": 1.3877551020408163, "no_speech_prob": 4.9819271225715056e-05}, {"id": 82, "seek": 89472, "start": 894.72, "end": 904.72, "text": " And here's another way to show how the ImageNet accuracy has evolved based on different models.", "tokens": [50364, 400, 510, 311, 1071, 636, 281, 855, 577, 264, 29903, 31890, 14170, 575, 14178, 2361, 322, 819, 5245, 13, 50864, 50864, 407, 257, 688, 295, 4205, 575, 668, 1741, 3216, 292, 13, 583, 264, 1002, 307, 544, 813, 445, 27706, 2657, 5359, 13, 682, 1186, 11, 456, 311, 257, 688, 544, 813, 18538, 2657, 11, 819, 6565, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08820554517930554, "compression_ratio": 1.4777777777777779, "no_speech_prob": 5.3047060646349564e-05}, {"id": 83, "seek": 89472, "start": 904.72, "end": 918.72, "text": " So a lot of progress has been engendered. But the world is more than just discrete object classes. In fact, there's a lot more than recognizing object, different objects.", "tokens": [50364, 400, 510, 311, 1071, 636, 281, 855, 577, 264, 29903, 31890, 14170, 575, 14178, 2361, 322, 819, 5245, 13, 50864, 50864, 407, 257, 688, 295, 4205, 575, 668, 1741, 3216, 292, 13, 583, 264, 1002, 307, 544, 813, 445, 27706, 2657, 5359, 13, 682, 1186, 11, 456, 311, 257, 688, 544, 813, 18538, 2657, 11, 819, 6565, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08820554517930554, "compression_ratio": 1.4777777777777779, "no_speech_prob": 5.3047060646349564e-05}, {"id": 84, "seek": 91872, "start": 918.72, "end": 930.72, "text": " And I'll show two images where object detectors will tell you the same objects existing in these two things, the animal llama and a person. They look similar.", "tokens": [50364, 400, 286, 603, 855, 732, 5267, 689, 2657, 46866, 486, 980, 291, 264, 912, 6565, 6741, 294, 613, 732, 721, 11, 264, 5496, 23272, 293, 257, 954, 13, 814, 574, 2531, 13, 50964, 50964, 400, 472, 3036, 574, 411, 341, 13, 583, 498, 291, 574, 412, 264, 661, 3036, 291, 4325, 613, 366, 732, 588, 588, 819, 5242, 11, 570, 295, 264, 2480, 1296, 264, 6565, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.17029054399947047, "compression_ratio": 1.6274509803921569, "no_speech_prob": 0.00014646003546658903}, {"id": 85, "seek": 91872, "start": 930.72, "end": 942.72, "text": " And one picture look like this. But if you look at the other picture you realize these are two very very different pictures, because of the relationship between the objects.", "tokens": [50364, 400, 286, 603, 855, 732, 5267, 689, 2657, 46866, 486, 980, 291, 264, 912, 6565, 6741, 294, 613, 732, 721, 11, 264, 5496, 23272, 293, 257, 954, 13, 814, 574, 2531, 13, 50964, 50964, 400, 472, 3036, 574, 411, 341, 13, 583, 498, 291, 574, 412, 264, 661, 3036, 291, 4325, 613, 366, 732, 588, 588, 819, 5242, 11, 570, 295, 264, 2480, 1296, 264, 6565, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.17029054399947047, "compression_ratio": 1.6274509803921569, "no_speech_prob": 0.00014646003546658903}, {"id": 86, "seek": 94272, "start": 942.72, "end": 958.72, "text": " And psychologists have long conjectured that for to characterize a scene or to understand a visual scene the real visual world relationships between objects must be coded.", "tokens": [50364, 400, 41562, 362, 938, 416, 1020, 3831, 300, 337, 281, 38463, 257, 4145, 420, 281, 1223, 257, 5056, 4145, 264, 957, 5056, 1002, 6159, 1296, 6565, 1633, 312, 34874, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.17699558594647577, "compression_ratio": 1.3790322580645162, "no_speech_prob": 8.088963659247383e-05}, {"id": 87, "seek": 95872, "start": 958.72, "end": 977.72, "text": " And that's how we go to the identities of objects. And this brings us to a following work of ImageNet by my students and collaborators on scene graph representation, where we look at not only object identities in the image but also the attributes", "tokens": [50364, 400, 300, 311, 577, 321, 352, 281, 264, 24239, 295, 6565, 13, 400, 341, 5607, 505, 281, 257, 3480, 589, 295, 29903, 31890, 538, 452, 1731, 293, 39789, 322, 4145, 4295, 10290, 11, 689, 321, 574, 412, 406, 787, 2657, 24239, 294, 264, 3256, 457, 611, 264, 17212, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.25159577222970814, "compression_ratio": 1.4819277108433735, "no_speech_prob": 0.00010716524411691353}, {"id": 88, "seek": 97772, "start": 977.72, "end": 1002.72, "text": " like the colors and, and, and expressions and so on, as well as the relationship. In fact, every image is a is full of different relationships, we put to put together this, this data set called visual genome, which contains 100,000 images 3.8 million", "tokens": [50364, 411, 264, 4577, 293, 11, 293, 11, 293, 15277, 293, 370, 322, 11, 382, 731, 382, 264, 2480, 13, 682, 1186, 11, 633, 3256, 307, 257, 307, 1577, 295, 819, 6159, 11, 321, 829, 281, 829, 1214, 341, 11, 341, 1412, 992, 1219, 5056, 21953, 11, 597, 8306, 2319, 11, 1360, 5267, 805, 13, 23, 2459, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14244551658630372, "compression_ratio": 1.5151515151515151, "no_speech_prob": 7.365603960352018e-05}, {"id": 89, "seek": 100272, "start": 1002.72, "end": 1024.72, "text": " objects, 2.3 million relationships, and also 5.4 million texture descriptions of the things are following work, looked at how we can predict visual relationships using scene graphs and be able to achieve relationship recognition for example,", "tokens": [50364, 6565, 11, 568, 13, 18, 2459, 6159, 11, 293, 611, 1025, 13, 19, 2459, 8091, 24406, 295, 264, 721, 366, 3480, 589, 11, 2956, 412, 577, 321, 393, 6069, 5056, 6159, 1228, 4145, 24877, 293, 312, 1075, 281, 4584, 2480, 11150, 337, 1365, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.17689661184946695, "compression_ratio": 1.535031847133758, "no_speech_prob": 6.500384188257158e-05}, {"id": 90, "seek": 102472, "start": 1024.72, "end": 1036.72, "text": " we have a model, we have a model that we can use for modeling, creating a model that can take a picture like this, and call it person riding horse or person wearing hat.", "tokens": [50364, 321, 362, 257, 2316, 11, 321, 362, 257, 2316, 300, 321, 393, 764, 337, 15983, 11, 4084, 257, 2316, 300, 393, 747, 257, 3036, 411, 341, 11, 293, 818, 309, 954, 9546, 6832, 420, 954, 4769, 2385, 13, 50964, 50964], "temperature": 0.0, "avg_logprob": -0.544414066133045, "compression_ratio": 1.5648148148148149, "no_speech_prob": 0.00018511153757572174}, {"id": 91, "seek": 103672, "start": 1036.72, "end": 1054.72, "text": " So our model can also do one zero shot learning, but looking at new relationships, such as horse wearing hat, which is really rare in real world things but with this compositional representation using scene graph, we're able to achieve this kind of", "tokens": [50364, 407, 527, 2316, 393, 611, 360, 472, 4018, 3347, 2539, 11, 457, 1237, 412, 777, 6159, 11, 1270, 382, 6832, 4769, 2385, 11, 597, 307, 534, 5892, 294, 957, 1002, 721, 457, 365, 341, 10199, 2628, 10290, 1228, 4145, 4295, 11, 321, 434, 1075, 281, 4584, 341, 733, 295, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.14075451077155346, "compression_ratio": 1.503030303030303, "no_speech_prob": 4.610349060385488e-05}, {"id": 92, "seek": 105472, "start": 1054.72, "end": 1069.72, "text": " visual learning on novel relationships, and some quantitative numbers show that our scene graph model for relationship estimation, as well as zero shot", "tokens": [50364, 5056, 2539, 322, 7613, 6159, 11, 293, 512, 27778, 3547, 855, 300, 527, 4145, 4295, 2316, 337, 2480, 35701, 11, 382, 731, 382, 4018, 3347, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.15103673934936523, "compression_ratio": 1.3603603603603605, "no_speech_prob": 5.91792595514562e-05}, {"id": 93, "seek": 106972, "start": 1069.72, "end": 1092.72, "text": " learning for relationship estimation beats the back than state of the art algorithms. And of course, the community has done a lot more interesting work since then, based on our scene graph representation here I just list a few work by other", "tokens": [50364, 2539, 337, 2480, 35701, 16447, 264, 646, 813, 1785, 295, 264, 1523, 14642, 13, 400, 295, 1164, 11, 264, 1768, 575, 1096, 257, 688, 544, 1880, 589, 1670, 550, 11, 2361, 322, 527, 4145, 4295, 10290, 510, 286, 445, 1329, 257, 1326, 589, 538, 661, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13704522775143993, "compression_ratio": 1.4723926380368098, "no_speech_prob": 7.249795453390107e-05}, {"id": 94, "seek": 109272, "start": 1092.72, "end": 1115.72, "text": " groups on all kinds of scene graph modeling, and we have also extended this beyond the static scenes into videos and created a new data set, a benchmark called action genome, and using spatial temporal scene graph to represent actions and use this to", "tokens": [50364, 3935, 322, 439, 3685, 295, 4145, 4295, 15983, 11, 293, 321, 362, 611, 10913, 341, 4399, 264, 13437, 8026, 666, 2145, 293, 2942, 257, 777, 1412, 992, 11, 257, 18927, 1219, 3069, 21953, 11, 293, 1228, 23598, 30881, 4145, 4295, 281, 2906, 5909, 293, 764, 341, 281, 51514, 51514, 2042, 9608, 411, 3069, 11150, 420, 1326, 3347, 11150, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.18260362413194445, "compression_ratio": 1.691891891891892, "no_speech_prob": 3.268680302426219e-05}, {"id": 95, "seek": 109272, "start": 1115.72, "end": 1121.72, "text": " perform tasks like action recognition or few shot recognition.", "tokens": [50364, 3935, 322, 439, 3685, 295, 4145, 4295, 15983, 11, 293, 321, 362, 611, 10913, 341, 4399, 264, 13437, 8026, 666, 2145, 293, 2942, 257, 777, 1412, 992, 11, 257, 18927, 1219, 3069, 21953, 11, 293, 1228, 23598, 30881, 4145, 4295, 281, 2906, 5909, 293, 764, 341, 281, 51514, 51514, 2042, 9608, 411, 3069, 11150, 420, 1326, 3347, 11150, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.18260362413194445, "compression_ratio": 1.691891891891892, "no_speech_prob": 3.268680302426219e-05}, {"id": 96, "seek": 112172, "start": 1121.72, "end": 1140.72, "text": " In fact, we have gone one more step further, and being inspired by Alan Turing's words that understanding the real world scene might connect the machine to also speaking English, in this case, so we have worked on a series of models where you can take a", "tokens": [50364, 682, 1186, 11, 321, 362, 2780, 472, 544, 1823, 3052, 11, 293, 885, 7547, 538, 16442, 314, 1345, 311, 2283, 300, 3701, 264, 957, 1002, 4145, 1062, 1745, 264, 3479, 281, 611, 4124, 3669, 11, 294, 341, 1389, 11, 370, 321, 362, 2732, 322, 257, 2638, 295, 5245, 689, 291, 393, 747, 257, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.1280782515542549, "compression_ratio": 1.4540229885057472, "no_speech_prob": 0.00013126667181495577}, {"id": 97, "seek": 114072, "start": 1140.72, "end": 1162.72, "text": " picture and perform image captioning or does image captioning, as well as paragraph captioning. So, that was a very quick, quick overview of one part of visual intelligence which is the perception part, the perception part takes the pixels of the real", "tokens": [50364, 3036, 293, 2042, 3256, 31974, 278, 420, 775, 3256, 31974, 278, 11, 382, 731, 382, 18865, 31974, 278, 13, 407, 11, 300, 390, 257, 588, 1702, 11, 1702, 12492, 295, 472, 644, 295, 5056, 7599, 597, 307, 264, 12860, 644, 11, 264, 12860, 644, 2516, 264, 18668, 295, 264, 957, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1540284686618381, "compression_ratio": 1.6733333333333333, "no_speech_prob": 0.0001333422987954691}, {"id": 98, "seek": 116272, "start": 1162.72, "end": 1175.72, "text": " world, feed it into the AI agent, and the agent is able to do important tasks like object recognition, visual relationship prediction, captioning and so on.", "tokens": [50364, 1002, 11, 3154, 309, 666, 264, 7318, 9461, 11, 293, 264, 9461, 307, 1075, 281, 360, 1021, 9608, 411, 2657, 11150, 11, 5056, 2480, 17630, 11, 31974, 278, 293, 370, 322, 13, 51014, 51014], "temperature": 0.0, "avg_logprob": -0.09655959076351589, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.00010386468784417957}, {"id": 99, "seek": 117572, "start": 1175.72, "end": 1198.72, "text": " And we introduced two data sets one is image net one is visual genome, and a representation called scene graph. But our lab has done more around the problem of perception and around both benchmark learning representation and connecting it to language.", "tokens": [50364, 400, 321, 7268, 732, 1412, 6352, 472, 307, 3256, 2533, 472, 307, 5056, 21953, 11, 293, 257, 10290, 1219, 4145, 4295, 13, 583, 527, 2715, 575, 1096, 544, 926, 264, 1154, 295, 12860, 293, 926, 1293, 18927, 2539, 10290, 293, 11015, 309, 281, 2856, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.14654794031259966, "compression_ratio": 1.5493827160493827, "no_speech_prob": 9.308753942605108e-05}, {"id": 100, "seek": 119872, "start": 1198.72, "end": 1212.72, "text": " So, but I want to now shift gears and ask the question is just passive understanding of the world enough for visual intelligence. My answer would be no.", "tokens": [50364, 407, 11, 457, 286, 528, 281, 586, 5513, 20915, 293, 1029, 264, 1168, 307, 445, 14975, 3701, 295, 264, 1002, 1547, 337, 5056, 7599, 13, 1222, 1867, 576, 312, 572, 13, 51064, 51064], "temperature": 0.0, "avg_logprob": -0.1766467503138951, "compression_ratio": 1.256198347107438, "no_speech_prob": 8.21243374957703e-05}, {"id": 101, "seek": 121272, "start": 1212.72, "end": 1232.72, "text": " We have gone to Plato's allegory of the cave, where he describes the the the passive perception of the world as prisoners tied to chairs, they're only forced to watch for in front of them, a play that's on full display in the back of their head,", "tokens": [50364, 492, 362, 2780, 281, 43027, 311, 10364, 827, 295, 264, 11730, 11, 689, 415, 15626, 264, 264, 264, 14975, 12860, 295, 264, 1002, 382, 20417, 9601, 281, 18299, 11, 436, 434, 787, 7579, 281, 1159, 337, 294, 1868, 295, 552, 11, 257, 862, 300, 311, 322, 1577, 4674, 294, 264, 646, 295, 641, 1378, 11, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.21996756731453587, "compression_ratio": 1.5506329113924051, "no_speech_prob": 0.00011765982344513759}, {"id": 102, "seek": 123272, "start": 1232.72, "end": 1250.72, "text": " and they see are the shadows of the play, and they need to make sense of the real world. So in fact, if we only look at this world in a passive way, we're a little bit like the prisoners of the other of the cave, and that would limit important functions", "tokens": [50364, 293, 436, 536, 366, 264, 14740, 295, 264, 862, 11, 293, 436, 643, 281, 652, 2020, 295, 264, 957, 1002, 13, 407, 294, 1186, 11, 498, 321, 787, 574, 412, 341, 1002, 294, 257, 14975, 636, 11, 321, 434, 257, 707, 857, 411, 264, 20417, 295, 264, 661, 295, 264, 11730, 11, 293, 300, 576, 4948, 1021, 6828, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.10295199578808199, "compression_ratio": 1.6012658227848102, "no_speech_prob": 8.609664655523375e-05}, {"id": 103, "seek": 125072, "start": 1250.72, "end": 1265.72, "text": " of our visual experience. For example, the. We won't be able to fully understand how to interact with these objects, especially if we view them in angles that won't enable us to interact effectively.", "tokens": [50364, 295, 527, 5056, 1752, 13, 1171, 1365, 11, 264, 13, 492, 1582, 380, 312, 1075, 281, 4498, 1223, 577, 281, 4648, 365, 613, 6565, 11, 2318, 498, 321, 1910, 552, 294, 14708, 300, 1582, 380, 9528, 505, 281, 4648, 8659, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.0665395736694336, "compression_ratio": 1.4113475177304964, "no_speech_prob": 0.00020333802967797965}, {"id": 104, "seek": 126572, "start": 1265.72, "end": 1283.72, "text": " So in fact, real visual experience is extreme, extremely dynamic, you and I move around all the time, and animals move around all the time, and they do a lot of things, and that is what I feel what I think visual intelligence is about.", "tokens": [50364, 407, 294, 1186, 11, 957, 5056, 1752, 307, 8084, 11, 4664, 8546, 11, 291, 293, 286, 1286, 926, 439, 264, 565, 11, 293, 4882, 1286, 926, 439, 264, 565, 11, 293, 436, 360, 257, 688, 295, 721, 11, 293, 300, 307, 437, 286, 841, 437, 286, 519, 5056, 7599, 307, 466, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.11521403278623309, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.00012725729902740568}, {"id": 105, "seek": 128372, "start": 1283.72, "end": 1299.72, "text": " Here I'll share with you one favorite quote of mine, which is by philosopher Peter Godfrey Smith, who says, the original and fundamental function of the nervous system is to link perception with action.", "tokens": [50364, 1692, 286, 603, 2073, 365, 291, 472, 2954, 6513, 295, 3892, 11, 597, 307, 538, 29805, 6508, 1265, 69, 7950, 8538, 11, 567, 1619, 11, 264, 3380, 293, 8088, 2445, 295, 264, 6296, 1185, 307, 281, 2113, 12860, 365, 3069, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.0796736717224121, "compression_ratio": 1.3466666666666667, "no_speech_prob": 0.0002339548955205828}, {"id": 106, "seek": 129972, "start": 1299.72, "end": 1317.72, "text": " And this is a very famous experiment done on two kittens, back in the 1960s, where the newborn kittens one is allowed to be active kitten. One is allowed only to be a passive kitten, the active kitten drives the yolk to explore visually with", "tokens": [50364, 400, 341, 307, 257, 588, 4618, 5120, 1096, 322, 732, 47363, 11, 646, 294, 264, 16157, 82, 11, 689, 264, 32928, 47363, 472, 307, 4350, 281, 312, 4967, 39696, 13, 1485, 307, 4350, 787, 281, 312, 257, 14975, 39696, 11, 264, 4967, 39696, 11754, 264, 32464, 281, 6839, 19622, 365, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.10491856822261104, "compression_ratio": 1.5649350649350648, "no_speech_prob": 0.00019706239982042462}, {"id": 107, "seek": 131772, "start": 1317.72, "end": 1335.72, "text": " what the world. The world is like, whereas the passive kitten is not allowed to explore by by its pro pro activeness, it only sees the world as the active kitten drives, moves around.", "tokens": [50364, 437, 264, 1002, 13, 440, 1002, 307, 411, 11, 9735, 264, 14975, 39696, 307, 406, 4350, 281, 6839, 538, 538, 1080, 447, 447, 605, 8477, 11, 309, 787, 8194, 264, 1002, 382, 264, 4967, 39696, 11754, 11, 6067, 926, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.19000254977833142, "compression_ratio": 1.5, "no_speech_prob": 0.00032481178641319275}, {"id": 108, "seek": 133572, "start": 1335.72, "end": 1354.72, "text": " And two weeks later, it was demonstrated that the active kitten has a much better developed perceptual visual system that the passive kitten. Not only we find this evidence in kittens we also find evidence in monkeys and humans that we have neurons,", "tokens": [50364, 400, 732, 3259, 1780, 11, 309, 390, 18772, 300, 264, 4967, 39696, 575, 257, 709, 1101, 4743, 43276, 901, 5056, 1185, 300, 264, 14975, 39696, 13, 1726, 787, 321, 915, 341, 4467, 294, 47363, 321, 611, 915, 4467, 294, 29534, 293, 6255, 300, 321, 362, 22027, 11, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.11908125409892961, "compression_ratio": 1.55625, "no_speech_prob": 0.00019710222841240466}, {"id": 109, "seek": 135472, "start": 1354.72, "end": 1368.72, "text": " neural neurons that are responsible to look at other people's movement, and to respond to that so in a way we're hardwired to perceive movements and want to do the same.", "tokens": [50364, 18161, 22027, 300, 366, 6250, 281, 574, 412, 661, 561, 311, 3963, 11, 293, 281, 4196, 281, 300, 370, 294, 257, 636, 321, 434, 1152, 86, 1824, 281, 20281, 9981, 293, 528, 281, 360, 264, 912, 13, 51064, 51064], "temperature": 0.0, "avg_logprob": -0.10804244948596489, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.00014192609523888677}, {"id": 110, "seek": 136872, "start": 1368.72, "end": 1387.72, "text": " So this brings me to the second half of the talk, which is seeing is for doing in the world. And we complete our, our little schema of the world, the where the agent and the world now, not only perceive, but act.", "tokens": [50364, 407, 341, 5607, 385, 281, 264, 1150, 1922, 295, 264, 751, 11, 597, 307, 2577, 307, 337, 884, 294, 264, 1002, 13, 400, 321, 3566, 527, 11, 527, 707, 34078, 295, 264, 1002, 11, 264, 689, 264, 9461, 293, 264, 1002, 586, 11, 406, 787, 20281, 11, 457, 605, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.09582695254573116, "compression_ratio": 1.4929577464788732, "no_speech_prob": 9.756781219039112e-05}, {"id": 111, "seek": 138772, "start": 1387.72, "end": 1398.72, "text": " And what are the critical ingredients of acting in the world. I think there are several. One is it should be embodied.", "tokens": [50364, 400, 437, 366, 264, 4924, 6952, 295, 6577, 294, 264, 1002, 13, 286, 519, 456, 366, 2940, 13, 1485, 307, 309, 820, 312, 42046, 13, 50914, 50914, 14242, 926, 294, 264, 1002, 307, 1293, 39680, 24765, 1166, 11, 382, 731, 382, 12382, 14275, 13, 467, 311, 881, 3700, 32972, 378, 304, 13, 316, 688, 295, 1413, 11, 309, 311, 42338, 47211, 11, 293, 309, 311, 534, 1021, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12163106600443523, "compression_ratio": 1.5315789473684212, "no_speech_prob": 0.00013124629913363606}, {"id": 112, "seek": 138772, "start": 1398.72, "end": 1411.72, "text": " Moving around in the world is both explorer explorative, as well as exploitative. It's most likely multimodal. A lot of times, it's multitasking, and it's really important.", "tokens": [50364, 400, 437, 366, 264, 4924, 6952, 295, 6577, 294, 264, 1002, 13, 286, 519, 456, 366, 2940, 13, 1485, 307, 309, 820, 312, 42046, 13, 50914, 50914, 14242, 926, 294, 264, 1002, 307, 1293, 39680, 24765, 1166, 11, 382, 731, 382, 12382, 14275, 13, 467, 311, 881, 3700, 32972, 378, 304, 13, 316, 688, 295, 1413, 11, 309, 311, 42338, 47211, 11, 293, 309, 311, 534, 1021, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12163106600443523, "compression_ratio": 1.5315789473684212, "no_speech_prob": 0.00013124629913363606}, {"id": 113, "seek": 141172, "start": 1411.72, "end": 1421.72, "text": " It's really important to allow the agent to be able to generalize, and many times, oftentimes, it's social and interactive with other agents.", "tokens": [50364, 467, 311, 534, 1021, 281, 2089, 264, 9461, 281, 312, 1075, 281, 2674, 1125, 11, 293, 867, 1413, 11, 18349, 11, 309, 311, 2093, 293, 15141, 365, 661, 12554, 13, 50864, 50864], "temperature": 0.0, "avg_logprob": -0.2619062311509076, "compression_ratio": 1.3177570093457944, "no_speech_prob": 6.500828749267384e-05}, {"id": 114, "seek": 142172, "start": 1421.72, "end": 1442.72, "text": " This brings me to a far reaching dream of AI which is to create robots that can perform a lot of complex human behavior human tasks. This is Rosie the robot. Of course we're not there yet, but the rest of my talk I want to share, share with", "tokens": [50364, 639, 5607, 385, 281, 257, 1400, 9906, 3055, 295, 7318, 597, 307, 281, 1884, 14733, 300, 393, 2042, 257, 688, 295, 3997, 1952, 5223, 1952, 9608, 13, 639, 307, 40521, 264, 7881, 13, 2720, 1164, 321, 434, 406, 456, 1939, 11, 457, 264, 1472, 295, 452, 751, 286, 528, 281, 2073, 11, 2073, 365, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.11057473873269968, "compression_ratio": 1.4545454545454546, "no_speech_prob": 8.345567039214075e-05}, {"id": 115, "seek": 144272, "start": 1442.72, "end": 1451.72, "text": " you some of our efforts towards robotic learning, using vision and in real world things.", "tokens": [50364, 291, 512, 295, 527, 6484, 3030, 30468, 2539, 11, 1228, 5201, 293, 294, 957, 1002, 721, 13, 50814, 50814, 1743, 286, 848, 11, 2539, 294, 294, 294, 4967, 9461, 420, 42046, 9461, 307, 1293, 24765, 1166, 293, 12382, 14275, 13, 961, 385, 445, 722, 365, 264, 24765, 1166, 11, 597, 307, 534, 2539, 281, 862, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09413084189097086, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.0001272605877602473}, {"id": 116, "seek": 144272, "start": 1451.72, "end": 1464.72, "text": " Like I said, learning in in in active agent or embodied agent is both explorative and exploitative. Let me just start with the explorative, which is really learning to play.", "tokens": [50364, 291, 512, 295, 527, 6484, 3030, 30468, 2539, 11, 1228, 5201, 293, 294, 957, 1002, 721, 13, 50814, 50814, 1743, 286, 848, 11, 2539, 294, 294, 294, 4967, 9461, 420, 42046, 9461, 307, 1293, 24765, 1166, 293, 12382, 14275, 13, 961, 385, 445, 722, 365, 264, 24765, 1166, 11, 597, 307, 534, 2539, 281, 862, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09413084189097086, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.0001272605877602473}, {"id": 117, "seek": 146472, "start": 1464.72, "end": 1484.72, "text": " There's a huge body of literature in this I won't be able to do justice some of my favorite work come from Allison gopnik and Liz bulky and many others, where we we imitate human newborns or human children, where they spend a lot of time playing", "tokens": [50364, 821, 311, 257, 2603, 1772, 295, 10394, 294, 341, 286, 1582, 380, 312, 1075, 281, 360, 6118, 512, 295, 452, 2954, 589, 808, 490, 32638, 352, 79, 13123, 293, 16480, 42986, 293, 867, 2357, 11, 689, 321, 321, 35556, 1952, 32928, 82, 420, 1952, 2227, 11, 689, 436, 3496, 257, 688, 295, 565, 2433, 51364, 51364, 1553, 257, 4334, 11, 1939, 436, 434, 2539, 293, 12736, 264, 1002, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14820021799165908, "compression_ratio": 1.4832535885167464, "no_speech_prob": 0.00010066694812849164}, {"id": 118, "seek": 146472, "start": 1484.72, "end": 1489.72, "text": " without a purpose, yet they're learning and exploring the world.", "tokens": [50364, 821, 311, 257, 2603, 1772, 295, 10394, 294, 341, 286, 1582, 380, 312, 1075, 281, 360, 6118, 512, 295, 452, 2954, 589, 808, 490, 32638, 352, 79, 13123, 293, 16480, 42986, 293, 867, 2357, 11, 689, 321, 321, 35556, 1952, 32928, 82, 420, 1952, 2227, 11, 689, 436, 3496, 257, 688, 295, 565, 2433, 51364, 51364, 1553, 257, 4334, 11, 1939, 436, 434, 2539, 293, 12736, 264, 1002, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14820021799165908, "compression_ratio": 1.4832535885167464, "no_speech_prob": 0.00010066694812849164}, {"id": 119, "seek": 148972, "start": 1489.72, "end": 1505.72, "text": " There are flavors of this kind of export it of learning. There is the novelty based motivation there's the skill based motivation and a world model based motivation, and that's where our work is mostly anchored to.", "tokens": [50364, 821, 366, 16303, 295, 341, 733, 295, 10725, 309, 295, 2539, 13, 821, 307, 264, 44805, 2361, 12335, 456, 311, 264, 5389, 2361, 12335, 293, 257, 1002, 2316, 2361, 12335, 11, 293, 300, 311, 689, 527, 589, 307, 5240, 12723, 2769, 281, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.14245429952093897, "compression_ratio": 1.5970149253731343, "no_speech_prob": 0.00022318230185192078}, {"id": 120, "seek": 150572, "start": 1505.72, "end": 1525.72, "text": " And this is also related to previous work on predicting dynamic predicting things and expecting what what to what to expect in future frames of videos and dynamics, but I won't get into the details.", "tokens": [50364, 400, 341, 307, 611, 4077, 281, 3894, 589, 322, 32884, 8546, 32884, 721, 293, 9650, 437, 437, 281, 437, 281, 2066, 294, 2027, 12083, 295, 2145, 293, 15679, 11, 457, 286, 1582, 380, 483, 666, 264, 4365, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.08374022869836717, "compression_ratio": 1.4666666666666666, "no_speech_prob": 5.389441503211856e-05}, {"id": 121, "seek": 152572, "start": 1525.72, "end": 1536.72, "text": " Basically the work my colleagues and students collaborators have done is to create a model that work on two kind of model.", "tokens": [50364, 8537, 264, 589, 452, 7734, 293, 1731, 39789, 362, 1096, 307, 281, 1884, 257, 2316, 300, 589, 322, 732, 733, 295, 2316, 13, 50914, 50914], "temperature": 0.0, "avg_logprob": -0.14042074592025192, "compression_ratio": 1.2577319587628866, "no_speech_prob": 3.320536416140385e-05}, {"id": 122, "seek": 153672, "start": 1536.72, "end": 1556.72, "text": " There is a world model network that predicts the consequences of actions of the actions of the embodied agent exploring a world. And then there's a self model network that predicts errors of the world model and and try to correct those errors.", "tokens": [50364, 821, 307, 257, 1002, 2316, 3209, 300, 6069, 82, 264, 10098, 295, 5909, 295, 264, 5909, 295, 264, 42046, 9461, 12736, 257, 1002, 13, 400, 550, 456, 311, 257, 2698, 2316, 3209, 300, 6069, 82, 13603, 295, 264, 1002, 2316, 293, 293, 853, 281, 3006, 729, 13603, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.08904614815345177, "compression_ratio": 1.7357142857142858, "no_speech_prob": 4.682827420765534e-05}, {"id": 123, "seek": 155672, "start": 1556.72, "end": 1578.72, "text": " And intrinsic reward is a policy mechanism, where we choose actions that maximize world model loss predicted by self model and this is to maximize the exploration and putting the self model and world model together is our intrinsically motivated", "tokens": [50364, 400, 35698, 7782, 307, 257, 3897, 7513, 11, 689, 321, 2826, 5909, 300, 19874, 1002, 2316, 4470, 19147, 538, 2698, 2316, 293, 341, 307, 281, 19874, 264, 16197, 293, 3372, 264, 2698, 2316, 293, 1002, 2316, 1214, 307, 527, 28621, 984, 14515, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09862063242041547, "compression_ratio": 1.611842105263158, "no_speech_prob": 9.167683310806751e-05}, {"id": 124, "seek": 157872, "start": 1578.72, "end": 1597.72, "text": " self aware agent, and we use that to explore a simulated world 3D world with objects, and you can see that the, the, the agent is able to explore in a similar way the blue line like human babies they start with the self motion, you know motion, and then", "tokens": [50364, 2698, 3650, 9461, 11, 293, 321, 764, 300, 281, 6839, 257, 41713, 1002, 805, 35, 1002, 365, 6565, 11, 293, 291, 393, 536, 300, 264, 11, 264, 11, 264, 9461, 307, 1075, 281, 6839, 294, 257, 2531, 636, 264, 3344, 1622, 411, 1952, 10917, 436, 722, 365, 264, 2698, 5394, 11, 291, 458, 5394, 11, 293, 550, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.11373003193589508, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0002233292325399816}, {"id": 125, "seek": 159772, "start": 1597.72, "end": 1618.72, "text": " they look at one single object and then they start to look at two objects. And this lower right panel shows you that the model learned through this self exploration or self motivation can be able to do downstream object recognition tasks, better than a random policy model.", "tokens": [50364, 436, 574, 412, 472, 2167, 2657, 293, 550, 436, 722, 281, 574, 412, 732, 6565, 13, 400, 341, 3126, 558, 4831, 3110, 291, 300, 264, 2316, 3264, 807, 341, 2698, 16197, 420, 2698, 12335, 393, 312, 1075, 281, 360, 30621, 2657, 11150, 9608, 11, 1101, 813, 257, 4974, 3897, 2316, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.09396976124156606, "compression_ratio": 1.5964912280701755, "no_speech_prob": 3.1689003662904724e-05}, {"id": 126, "seek": 161872, "start": 1618.72, "end": 1628.72, "text": " So that was a an example of explorative learning. Let's go to a explorative learning which is much more goal based.", "tokens": [50364, 407, 300, 390, 257, 364, 1365, 295, 24765, 1166, 2539, 13, 961, 311, 352, 281, 257, 24765, 1166, 2539, 597, 307, 709, 544, 3387, 2361, 13, 50864, 50864, 286, 478, 516, 281, 445, 312, 588, 5353, 13, 51014, 51014, 4080, 471, 2201, 300, 527, 33160, 3387, 307, 281, 652, 14733, 281, 360, 938, 18046, 9608, 11, 457, 881, 295, 264, 589, 294, 965, 311, 30468, 2539, 307, 588, 2099, 27006, 901, 3942, 1496, 9608, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09841549547412727, "compression_ratio": 1.5728155339805825, "no_speech_prob": 2.3180749849416316e-05}, {"id": 127, "seek": 161872, "start": 1628.72, "end": 1631.72, "text": " I'm going to just be very brief.", "tokens": [50364, 407, 300, 390, 257, 364, 1365, 295, 24765, 1166, 2539, 13, 961, 311, 352, 281, 257, 24765, 1166, 2539, 597, 307, 709, 544, 3387, 2361, 13, 50864, 50864, 286, 478, 516, 281, 445, 312, 588, 5353, 13, 51014, 51014, 4080, 471, 2201, 300, 527, 33160, 3387, 307, 281, 652, 14733, 281, 360, 938, 18046, 9608, 11, 457, 881, 295, 264, 589, 294, 965, 311, 30468, 2539, 307, 588, 2099, 27006, 901, 3942, 1496, 9608, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09841549547412727, "compression_ratio": 1.5728155339805825, "no_speech_prob": 2.3180749849416316e-05}, {"id": 128, "seek": 161872, "start": 1631.72, "end": 1646.72, "text": " Remind everybody that our eventual goal is to make robots to do long horizon tasks, but most of the work in today's robotic learning is very short punctual skills level tasks.", "tokens": [50364, 407, 300, 390, 257, 364, 1365, 295, 24765, 1166, 2539, 13, 961, 311, 352, 281, 257, 24765, 1166, 2539, 597, 307, 709, 544, 3387, 2361, 13, 50864, 50864, 286, 478, 516, 281, 445, 312, 588, 5353, 13, 51014, 51014, 4080, 471, 2201, 300, 527, 33160, 3387, 307, 281, 652, 14733, 281, 360, 938, 18046, 9608, 11, 457, 881, 295, 264, 589, 294, 965, 311, 30468, 2539, 307, 588, 2099, 27006, 901, 3942, 1496, 9608, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09841549547412727, "compression_ratio": 1.5728155339805825, "no_speech_prob": 2.3180749849416316e-05}, {"id": 129, "seek": 164672, "start": 1646.72, "end": 1667.72, "text": " So we need to try to close the gap by encouraging robots to do longer horizon tasks like cleaning up tabletops in a longer horizon way. And here with my students and collaborators we we put together a newer task programming model were inspired by actually", "tokens": [50364, 407, 321, 643, 281, 853, 281, 1998, 264, 7417, 538, 14580, 14733, 281, 360, 2854, 18046, 9608, 411, 8924, 493, 14136, 3370, 294, 257, 2854, 18046, 636, 13, 400, 510, 365, 452, 1731, 293, 39789, 321, 321, 829, 1214, 257, 17628, 5633, 9410, 2316, 645, 7547, 538, 767, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.1171419070317195, "compression_ratio": 1.5088757396449703, "no_speech_prob": 3.425911927479319e-05}, {"id": 130, "seek": 166772, "start": 1667.72, "end": 1679.72, "text": " the inter vision research, but by enabling robotic learning to be compositional through skill set level tasks and hierarchically stack them together.", "tokens": [50364, 264, 728, 5201, 2132, 11, 457, 538, 23148, 30468, 2539, 281, 312, 10199, 2628, 807, 5389, 992, 1496, 9608, 293, 35250, 984, 8630, 552, 1214, 13, 50964, 50964], "temperature": 0.0, "avg_logprob": -0.2852897644042969, "compression_ratio": 1.3185840707964602, "no_speech_prob": 4.3302556150592864e-05}, {"id": 131, "seek": 167972, "start": 1679.72, "end": 1699.72, "text": " And I'm not going to get into the details of this competition compositional representation, but here is a result showing that our robots are able to perform longer horizon tasks better than better than a state of the art result.", "tokens": [50364, 400, 286, 478, 406, 516, 281, 483, 666, 264, 4365, 295, 341, 6211, 10199, 2628, 10290, 11, 457, 510, 307, 257, 1874, 4099, 300, 527, 14733, 366, 1075, 281, 2042, 2854, 18046, 9608, 1101, 813, 1101, 813, 257, 1785, 295, 264, 1523, 1874, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.1791222095489502, "compression_ratio": 1.509933774834437, "no_speech_prob": 0.00019395703566260636}, {"id": 132, "seek": 169972, "start": 1699.72, "end": 1723.72, "text": " And we, we can perform multiple tasks not just color block stacking but also sorting. In fact, we can also resist some interruptions here and the experimenter is going to disrupt what this color block task is, and the robot is able to compose the the task", "tokens": [50364, 400, 321, 11, 321, 393, 2042, 3866, 9608, 406, 445, 2017, 3461, 41376, 457, 611, 32411, 13, 682, 1186, 11, 321, 393, 611, 4597, 512, 12729, 626, 510, 293, 264, 5120, 260, 307, 516, 281, 14124, 437, 341, 2017, 3461, 5633, 307, 11, 293, 264, 7881, 307, 1075, 281, 35925, 264, 264, 5633, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.14091747685482628, "compression_ratio": 1.5644171779141105, "no_speech_prob": 0.00015347168664447963}, {"id": 133, "seek": 172372, "start": 1723.72, "end": 1730.72, "text": " automatically by itself and reset its goals and complete the task.", "tokens": [50364, 6772, 538, 2564, 293, 14322, 1080, 5493, 293, 3566, 264, 5633, 13, 50714, 50714], "temperature": 0.0, "avg_logprob": -0.14024613797664642, "compression_ratio": 1.0153846153846153, "no_speech_prob": 0.00025299249682575464}, {"id": 134, "seek": 173072, "start": 1730.72, "end": 1753.72, "text": " So, again, I showed you one example of explorative learning towards long horizon tasks. Let me just say that this is has some has been something that my lab has been focusing on in various angles in a new in a newer line of work, we continue to look at long horizon and generalize", "tokens": [50364, 407, 11, 797, 11, 286, 4712, 291, 472, 1365, 295, 24765, 1166, 2539, 3030, 938, 18046, 9608, 13, 961, 385, 445, 584, 300, 341, 307, 575, 512, 575, 668, 746, 300, 452, 2715, 575, 668, 8416, 322, 294, 3683, 14708, 294, 257, 777, 294, 257, 17628, 1622, 295, 589, 11, 321, 2354, 281, 574, 412, 938, 18046, 293, 2674, 1125, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07537759095430374, "compression_ratio": 1.5384615384615385, "no_speech_prob": 6.397045945050195e-05}, {"id": 135, "seek": 175372, "start": 1753.72, "end": 1773.72, "text": " the realization of long horizon tasks by by training a robot through curriculum learning, where we know the target task, but we know it's really hard for robot to learn at the beginning so we generate a series of simpler target, target tasks to guide the robot.", "tokens": [50364, 264, 25138, 295, 938, 18046, 9608, 538, 538, 3097, 257, 7881, 807, 14302, 2539, 11, 689, 321, 458, 264, 3779, 5633, 11, 457, 321, 458, 309, 311, 534, 1152, 337, 7881, 281, 1466, 412, 264, 2863, 370, 321, 8460, 257, 2638, 295, 18587, 3779, 11, 3779, 9608, 281, 5934, 264, 7881, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.13616313253130233, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.00018803907732944936}, {"id": 136, "seek": 177372, "start": 1773.72, "end": 1795.72, "text": " And this is related to a lot of generative models recently we have seen in the AI community, and I'm going to skip the workflow of this model, and to just show you that our robots are capable of of learning different different kind of long horizon tasks by this", "tokens": [50364, 400, 341, 307, 4077, 281, 257, 688, 295, 1337, 1166, 5245, 3938, 321, 362, 1612, 294, 264, 7318, 1768, 11, 293, 286, 478, 516, 281, 10023, 264, 20993, 295, 341, 2316, 11, 293, 281, 445, 855, 291, 300, 527, 14733, 366, 8189, 295, 295, 2539, 819, 819, 733, 295, 938, 18046, 9608, 538, 341, 51464, 51464, 14302, 3097, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12389125521220858, "compression_ratio": 1.5080213903743316, "no_speech_prob": 2.3546705051558092e-05}, {"id": 137, "seek": 177372, "start": 1795.72, "end": 1797.72, "text": " curriculum training.", "tokens": [50364, 400, 341, 307, 4077, 281, 257, 688, 295, 1337, 1166, 5245, 3938, 321, 362, 1612, 294, 264, 7318, 1768, 11, 293, 286, 478, 516, 281, 10023, 264, 20993, 295, 341, 2316, 11, 293, 281, 445, 855, 291, 300, 527, 14733, 366, 8189, 295, 295, 2539, 819, 819, 733, 295, 938, 18046, 9608, 538, 341, 51464, 51464, 14302, 3097, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12389125521220858, "compression_ratio": 1.5080213903743316, "no_speech_prob": 2.3546705051558092e-05}, {"id": 138, "seek": 179772, "start": 1797.72, "end": 1803.72, "text": " And, in fact, even generalized to a different simulated desktop.", "tokens": [50364, 400, 11, 294, 1186, 11, 754, 44498, 281, 257, 819, 41713, 14502, 13, 50664, 50664, 407, 11, 1105, 11, 370, 300, 390, 732, 1365, 11, 767, 1045, 5110, 295, 30468, 2539, 11, 457, 321, 434, 920, 406, 456, 1939, 294, 19626, 341, 733, 295, 957, 1002, 5633, 13, 400, 456, 307, 257, 2141, 5361, 2522, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.10297954082489014, "compression_ratio": 1.4310344827586208, "no_speech_prob": 7.365882629528642e-05}, {"id": 139, "seek": 179772, "start": 1803.72, "end": 1817.72, "text": " So, um, so that was two example, actually three examples of robotic learning, but we're still not there yet in achieving this kind of real world task. And there is a key missing piece.", "tokens": [50364, 400, 11, 294, 1186, 11, 754, 44498, 281, 257, 819, 41713, 14502, 13, 50664, 50664, 407, 11, 1105, 11, 370, 300, 390, 732, 1365, 11, 767, 1045, 5110, 295, 30468, 2539, 11, 457, 321, 434, 920, 406, 456, 1939, 294, 19626, 341, 733, 295, 957, 1002, 5633, 13, 400, 456, 307, 257, 2141, 5361, 2522, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.10297954082489014, "compression_ratio": 1.4310344827586208, "no_speech_prob": 7.365882629528642e-05}, {"id": 140, "seek": 181772, "start": 1817.72, "end": 1840.72, "text": " And that key missing piece brings us back to what today's robotic tasks are still mostly skill level task and short horizon goals. Even if we try to do, try to do some longer horizon tasks, they tend to be small scale and anecdotal, their experimenter", "tokens": [50364, 400, 300, 2141, 5361, 2522, 5607, 505, 646, 281, 437, 965, 311, 30468, 9608, 366, 920, 5240, 5389, 1496, 5633, 293, 2099, 18046, 5493, 13, 2754, 498, 321, 853, 281, 360, 11, 853, 281, 360, 512, 2854, 18046, 9608, 11, 436, 3928, 281, 312, 1359, 4373, 293, 26652, 38180, 11, 641, 5120, 260, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.16181330095257676, "compression_ratio": 1.5029940119760479, "no_speech_prob": 3.7628942664014176e-05}, {"id": 141, "seek": 184072, "start": 1840.72, "end": 1859.72, "text": " will make tasks and lack standard metrics. And, and this, you know, either some of the tasks are in artificially simple environments, or if we bring the, the previously trained robot to a real experiment.", "tokens": [50364, 486, 652, 9608, 293, 5011, 3832, 16367, 13, 400, 11, 293, 341, 11, 291, 458, 11, 2139, 512, 295, 264, 9608, 366, 294, 39905, 2270, 2199, 12388, 11, 420, 498, 321, 1565, 264, 11, 264, 8046, 8895, 7881, 281, 257, 957, 5120, 13, 51314, 51314, 467, 445, 733, 295, 18199, 17725, 1188, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.21818140933388158, "compression_ratio": 1.427710843373494, "no_speech_prob": 3.882317469106056e-05}, {"id": 142, "seek": 184072, "start": 1859.72, "end": 1863.72, "text": " It just kind of fails miserably.", "tokens": [50364, 486, 652, 9608, 293, 5011, 3832, 16367, 13, 400, 11, 293, 341, 11, 291, 458, 11, 2139, 512, 295, 264, 9608, 366, 294, 39905, 2270, 2199, 12388, 11, 420, 498, 321, 1565, 264, 11, 264, 8046, 8895, 7881, 281, 257, 957, 5120, 13, 51314, 51314, 467, 445, 733, 295, 18199, 17725, 1188, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.21818140933388158, "compression_ratio": 1.427710843373494, "no_speech_prob": 3.882317469106056e-05}, {"id": 143, "seek": 186372, "start": 1863.72, "end": 1880.72, "text": " Yeah, here's an example of that. So this brings us back to JJ Gibson, that we need an ecological approach to perception and robotic learning. And we have seen great progress in vision and NLP and other areas of AI.", "tokens": [50364, 865, 11, 510, 311, 364, 1365, 295, 300, 13, 407, 341, 5607, 505, 646, 281, 21386, 42250, 11, 300, 321, 643, 364, 31054, 3109, 281, 12860, 293, 30468, 2539, 13, 400, 321, 362, 1612, 869, 4205, 294, 5201, 293, 426, 45196, 293, 661, 3179, 295, 7318, 13, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.10478004754758348, "compression_ratio": 1.3544303797468353, "no_speech_prob": 4.8306083044735715e-05}, {"id": 144, "seek": 188072, "start": 1880.72, "end": 1895.72, "text": " And we hope that in robotic learning, we can also work towards benchmarks that are large scale and diverse ecological in general complex, as well as standardized the evaluation metrics.", "tokens": [50364, 400, 321, 1454, 300, 294, 30468, 2539, 11, 321, 393, 611, 589, 3030, 43751, 300, 366, 2416, 4373, 293, 9521, 31054, 294, 2674, 3997, 11, 382, 731, 382, 31677, 264, 13344, 16367, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.10803455275458258, "compression_ratio": 1.3703703703703705, "no_speech_prob": 8.474529022350907e-05}, {"id": 145, "seek": 189572, "start": 1895.72, "end": 1916.72, "text": " We have a new latest work called behavior behavior is a benchmark for everyday household activities in virtual interactive and ecological environments and behavior is enabled by a simulation environment called I Gibson 2.0 it's a object centered", "tokens": [50364, 492, 362, 257, 777, 6792, 589, 1219, 5223, 5223, 307, 257, 18927, 337, 7429, 9888, 5354, 294, 6374, 15141, 293, 31054, 12388, 293, 5223, 307, 15172, 538, 257, 16575, 2823, 1219, 286, 42250, 568, 13, 15, 309, 311, 257, 2657, 18988, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.2926567925347222, "compression_ratio": 1.5506329113924051, "no_speech_prob": 0.00021642872889060527}, {"id": 146, "seek": 191672, "start": 1916.72, "end": 1936.72, "text": " model for robotic learning of everyday household. I'll just go over very quickly what I Gibson is, I Gibson is an environment that's very much inspired by a lot of concurrent work like habitat, so in the world, sapien AI to Thor, and its goal is to be realistic", "tokens": [50364, 2316, 337, 30468, 2539, 295, 7429, 9888, 13, 286, 603, 445, 352, 670, 588, 2661, 437, 286, 42250, 307, 11, 286, 42250, 307, 364, 2823, 300, 311, 588, 709, 7547, 538, 257, 688, 295, 37702, 589, 411, 20110, 11, 370, 294, 264, 1002, 11, 18985, 1053, 7318, 281, 17777, 11, 293, 1080, 3387, 307, 281, 312, 12465, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.2340368990038262, "compression_ratio": 1.434065934065934, "no_speech_prob": 9.366734047944192e-06}, {"id": 147, "seek": 193672, "start": 1936.72, "end": 1952.72, "text": " in object modeling photo realistic and rendering simulation for both kinematic and non kinematic state changes and full physically simulated action execution, as well as allowing VR interface for human demonstration.", "tokens": [50364, 294, 2657, 15983, 5052, 12465, 293, 22407, 16575, 337, 1293, 15784, 14911, 293, 2107, 15784, 14911, 1785, 2962, 293, 1577, 9762, 41713, 3069, 15058, 11, 382, 731, 382, 8293, 13722, 9226, 337, 1952, 16520, 13, 51164, 51164, 286, 603, 445, 312, 10023, 264, 4365, 295, 42250, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.14147459291944317, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.00013548029528465122}, {"id": 148, "seek": 193672, "start": 1952.72, "end": 1957.72, "text": " I'll just be skip the details of Gibson.", "tokens": [50364, 294, 2657, 15983, 5052, 12465, 293, 22407, 16575, 337, 1293, 15784, 14911, 293, 2107, 15784, 14911, 1785, 2962, 293, 1577, 9762, 41713, 3069, 15058, 11, 382, 731, 382, 8293, 13722, 9226, 337, 1952, 16520, 13, 51164, 51164, 286, 603, 445, 312, 10023, 264, 4365, 295, 42250, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.14147459291944317, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.00013548029528465122}, {"id": 149, "seek": 195772, "start": 1957.72, "end": 1977.72, "text": " I'll just skip the details of the Gibson work on our Stanford website. Gibson enables behavior, this benchmark. As we said, we want to build an embodied AI benchmark that is complex enough, large scale ecological complex and standardized the evaluation", "tokens": [50364, 286, 603, 445, 10023, 264, 4365, 295, 264, 42250, 589, 322, 527, 20374, 3144, 13, 42250, 17077, 5223, 11, 341, 18927, 13, 1018, 321, 848, 11, 321, 528, 281, 1322, 364, 42046, 7318, 18927, 300, 307, 3997, 1547, 11, 2416, 4373, 31054, 3997, 293, 31677, 264, 13344, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.23426495346368528, "compression_ratio": 1.423728813559322, "no_speech_prob": 8.217020513257012e-05}, {"id": 150, "seek": 197772, "start": 1977.72, "end": 2001.72, "text": " metrics. Gibson has 100 so far has 100 different tasks. They are. They are gathered through the American Bureau of Labor Statistics, and, and by sampling what Americans do in their daily life, and we put together this data set of 100 tasks.", "tokens": [50364, 16367, 13, 42250, 575, 2319, 370, 1400, 575, 2319, 819, 9608, 13, 814, 366, 13, 814, 366, 13032, 807, 264, 2665, 19738, 295, 17250, 49226, 11, 293, 11, 293, 538, 21179, 437, 6280, 360, 294, 641, 5212, 993, 11, 293, 321, 829, 1214, 341, 1412, 992, 295, 2319, 9608, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.14850733015272352, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.00011589402129175141}, {"id": 151, "seek": 200172, "start": 2001.72, "end": 2020.72, "text": " So in terms of statistics, Gibson is a lot more wider range compared to other data sets, focusing on just a narrower band of tasks, and the statistics of behavior tracks what the general statistics of the ATIS tasks.", "tokens": [50364, 407, 294, 2115, 295, 12523, 11, 42250, 307, 257, 688, 544, 11842, 3613, 5347, 281, 661, 1412, 6352, 11, 8416, 322, 445, 257, 46751, 4116, 295, 9608, 11, 293, 264, 12523, 295, 5223, 10218, 437, 264, 2674, 12523, 295, 264, 8872, 2343, 9608, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.11924655238787334, "compression_ratio": 1.4794520547945205, "no_speech_prob": 5.1434348279144615e-05}, {"id": 152, "seek": 202072, "start": 2020.72, "end": 2036.72, "text": " And it's also ecological in general. Here we show you by one example of clearing table, we show very different object positions environments, the rendering of objects, the textures.", "tokens": [50364, 400, 309, 311, 611, 31054, 294, 2674, 13, 1692, 321, 855, 291, 538, 472, 1365, 295, 23937, 3199, 11, 321, 855, 588, 819, 2657, 8432, 12388, 11, 264, 22407, 295, 6565, 11, 264, 24501, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.06841712731581467, "compression_ratio": 1.371212121212121, "no_speech_prob": 0.00011407571582822129}, {"id": 153, "seek": 203672, "start": 2036.72, "end": 2060.7200000000003, "text": " We have done extensive, extensive statistics analysis by showing you the diversity of objects and things. And it's also long horizon and complex. We show you that an average behavior task clients is measured by 300 to 20,000 steps, whereas other, other", "tokens": [50364, 492, 362, 1096, 13246, 11, 13246, 12523, 5215, 538, 4099, 291, 264, 8811, 295, 6565, 293, 721, 13, 400, 309, 311, 611, 938, 18046, 293, 3997, 13, 492, 855, 291, 300, 364, 4274, 5223, 5633, 6982, 307, 12690, 538, 6641, 281, 945, 11, 1360, 4439, 11, 9735, 661, 11, 661, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.17694499757554796, "compression_ratio": 1.44, "no_speech_prob": 5.3902662330074236e-05}, {"id": 154, "seek": 206072, "start": 2060.72, "end": 2077.72, "text": " task benchmark are mostly smaller than 100 steps, or between 100 and 1000 steps so Gibson, sorry behavior is really going towards real life complexity in terms of tasks.", "tokens": [50364, 5633, 18927, 366, 5240, 4356, 813, 2319, 4439, 11, 420, 1296, 2319, 293, 9714, 4439, 370, 42250, 11, 2597, 5223, 307, 534, 516, 3030, 957, 993, 14024, 294, 2115, 295, 9608, 13, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.1385545465681288, "compression_ratio": 1.352, "no_speech_prob": 3.0712250008946285e-05}, {"id": 155, "seek": 207772, "start": 2077.72, "end": 2090.72, "text": " Last but not least, it tries to standardize evaluation metrics by allowing a logic based representation to score these.", "tokens": [50364, 5264, 457, 406, 1935, 11, 309, 9898, 281, 3832, 1125, 13344, 16367, 538, 8293, 257, 9952, 2361, 10290, 281, 6175, 613, 13, 51014, 51014], "temperature": 0.0, "avg_logprob": -0.12610901319063628, "compression_ratio": 1.202020202020202, "no_speech_prob": 2.4679311536601745e-05}, {"id": 156, "seek": 209072, "start": 2090.72, "end": 2113.72, "text": " The end, the end state, compared to the initial state. I'm just going to skip the details of this and, and, and move on. So, last but not now the least we also allow human VR demo in our behavior benchmark and we can use that for benchmarking", "tokens": [50364, 440, 917, 11, 264, 917, 1785, 11, 5347, 281, 264, 5883, 1785, 13, 286, 478, 445, 516, 281, 10023, 264, 4365, 295, 341, 293, 11, 293, 11, 293, 1286, 322, 13, 407, 11, 1036, 457, 406, 586, 264, 1935, 321, 611, 2089, 1952, 13722, 10723, 294, 527, 5223, 18927, 293, 321, 393, 764, 300, 337, 18927, 278, 51514, 51514, 1970, 10493, 295, 15058, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.17174504784976735, "compression_ratio": 1.5449438202247192, "no_speech_prob": 1.3417792615655344e-05}, {"id": 157, "seek": 209072, "start": 2113.72, "end": 2117.72, "text": " against efficiency of execution.", "tokens": [50364, 440, 917, 11, 264, 917, 1785, 11, 5347, 281, 264, 5883, 1785, 13, 286, 478, 445, 516, 281, 10023, 264, 4365, 295, 341, 293, 11, 293, 11, 293, 1286, 322, 13, 407, 11, 1036, 457, 406, 586, 264, 1935, 321, 611, 2089, 1952, 13722, 10723, 294, 527, 5223, 18927, 293, 321, 393, 764, 300, 337, 18927, 278, 51514, 51514, 1970, 10493, 295, 15058, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.17174504784976735, "compression_ratio": 1.5449438202247192, "no_speech_prob": 1.3417792615655344e-05}, {"id": 158, "seek": 211772, "start": 2117.72, "end": 2126.72, "text": " So, um, what excites me the most in this graph is behavior is really really hard.", "tokens": [50364, 407, 11, 1105, 11, 437, 1624, 3324, 385, 264, 881, 294, 341, 4295, 307, 5223, 307, 534, 534, 1152, 13, 50814, 50814], "temperature": 0.0, "avg_logprob": -0.10156844059626262, "compression_ratio": 1.0945945945945945, "no_speech_prob": 4.90740203531459e-05}, {"id": 159, "seek": 212672, "start": 2126.72, "end": 2149.72, "text": " We have benchmarked task performance of behavior against a couple of a couple of state of the art algorithm and I want you to look at this, but most leftmost bar where we use default behavior without giving privileged information and you can see that the", "tokens": [50364, 492, 362, 18927, 292, 5633, 3389, 295, 5223, 1970, 257, 1916, 295, 257, 1916, 295, 1785, 295, 264, 1523, 9284, 293, 286, 528, 291, 281, 574, 412, 341, 11, 457, 881, 1411, 1761, 2159, 689, 321, 764, 7576, 5223, 1553, 2902, 25293, 1589, 293, 291, 393, 536, 300, 264, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.16709306105127875, "compression_ratio": 1.548780487804878, "no_speech_prob": 4.831426849705167e-05}, {"id": 160, "seek": 214972, "start": 2149.72, "end": 2168.72, "text": " data set is close to zero. And this is where I think we're starting to, to be on the journey of creating robotic embodied agents that can do really complex household activities and can be benchmark against behavior data set.", "tokens": [50364, 1412, 992, 307, 1998, 281, 4018, 13, 400, 341, 307, 689, 286, 519, 321, 434, 2891, 281, 11, 281, 312, 322, 264, 4671, 295, 4084, 30468, 42046, 12554, 300, 393, 360, 534, 3997, 9888, 5354, 293, 393, 312, 18927, 1970, 5223, 1412, 992, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.14596105615297952, "compression_ratio": 1.391304347826087, "no_speech_prob": 4.005189111921936e-05}, {"id": 161, "seek": 216872, "start": 2168.72, "end": 2188.72, "text": " So for those of you who are interested, you can visit our website to learn more. So in short, in, in, in this scene is for doing. I've shared with you that I get the environment that enable the behavior challenge or behavior data set.", "tokens": [50364, 407, 337, 729, 295, 291, 567, 366, 3102, 11, 291, 393, 3441, 527, 3144, 281, 1466, 544, 13, 407, 294, 2099, 11, 294, 11, 294, 11, 294, 341, 4145, 307, 337, 884, 13, 286, 600, 5507, 365, 291, 300, 286, 483, 264, 2823, 300, 9528, 264, 5223, 3430, 420, 5223, 1412, 992, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.14782487300404332, "compression_ratio": 1.5, "no_speech_prob": 3.0712086299899966e-05}, {"id": 162, "seek": 218872, "start": 2188.72, "end": 2201.72, "text": " And I want to share with you some of our earlier robotic work robotic learning work in curiosity based explorative learning, as well as long horizon task driven learning.", "tokens": [50364, 400, 286, 528, 281, 2073, 365, 291, 512, 295, 527, 3071, 30468, 589, 30468, 2539, 589, 294, 18769, 2361, 24765, 1166, 2539, 11, 382, 731, 382, 938, 18046, 5633, 9555, 2539, 13, 51014, 51014], "temperature": 0.0, "avg_logprob": -0.175567004415724, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.00023397343466058373}, {"id": 163, "seek": 220172, "start": 2201.72, "end": 2221.72, "text": " This is the work that you can find on our website. And I want to conclude by reminding all of us that vision is a cornerstone of intelligence in it enables us to understand and to do things in this real world, and our research is formulated", "tokens": [50364, 639, 307, 264, 589, 300, 291, 393, 915, 322, 527, 3144, 13, 400, 286, 528, 281, 16886, 538, 27639, 439, 295, 505, 300, 5201, 307, 257, 4538, 11243, 295, 7599, 294, 309, 17077, 505, 281, 1223, 293, 281, 360, 721, 294, 341, 957, 1002, 11, 293, 527, 2132, 307, 48936, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.13753878628766095, "compression_ratio": 1.4723926380368098, "no_speech_prob": 0.0001158453815151006}, {"id": 164, "seek": 222172, "start": 2221.72, "end": 2239.72, "text": " to support this two goals, especially inspired by JJ Gibson's ecological approach to perception and robotic learning. Thank you everybody this is my awesome team at Stanford, with so many great students and collaborators some of them are not even on this", "tokens": [50364, 281, 1406, 341, 732, 5493, 11, 2318, 7547, 538, 21386, 42250, 311, 31054, 3109, 281, 12860, 293, 30468, 2539, 13, 1044, 291, 2201, 341, 307, 452, 3476, 1469, 412, 20374, 11, 365, 370, 867, 869, 1731, 293, 39789, 512, 295, 552, 366, 406, 754, 322, 341, 51264, 51264, 5052, 11, 457, 1309, 291, 370, 709, 13, 4621, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17888549116791272, "compression_ratio": 1.4595959595959596, "no_speech_prob": 5.643181430059485e-05}, {"id": 165, "seek": 223972, "start": 2239.72, "end": 2254.72, "text": " photo, but thank you so much. Bye.", "tokens": [50364, 5052, 11, 457, 1309, 291, 370, 709, 13, 4621, 13, 51114], "temperature": 0.0, "avg_logprob": -0.217486729988685, "compression_ratio": 0.8095238095238095, "no_speech_prob": 0.0003178982005920261}], "language": "en", "video_id": "XzFyYXovHMU", "entity": "Fei-Fei Li"}}