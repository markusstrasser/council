{"video_id": "Hso8Qq3-arc", "title": "6.9 Bias and variance | Bias/variance and neural networks --[Machine Learning | Andrew Ng]", "description": "Second Course:\nAdvanced Learning Algorithms.\n\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 644, "views": 75, "publish_date": "11/04/2022", "timestamp": 1661817600, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " We've seen that high bias or high variance are both bad in the sense that they hurt the performance of your algorithm. One of the reasons that neural networks have been so successful is because neural networks, together with the idea of big data or hopefully having large datasets, is giving us a new way of new ways to address both high bias and high variance. Let's take a look. You saw that if you're fitting different order polynomials to a dataset, then if you were to fit a linear model like this on the left, you have a pretty simple model that can have high bias. Whereas if you were to fit a complex model, then you might suffer from high variance. And there's this trade-off between bias and variance. In our example, it was choosing a second-order polynomial that helps you make a trade-off and pick a model with lowest possible cross-validation error. And so before the days of neural networks, machine learning engineers talked a lot about this bias-variance trade-off in which you had to balance the complexity, that is, the degree of polynomial or the regularization parameter lambda to make bias and variance both not be too high. And if you hear machine learning engineers talk about the bias-variance trade-off, this is what they're referring to, where if you have too simple a model, you have high bias, too complex a model, high variance, and you have to find a trade-off between these two bad things to find hopefully the best possible outcome. But it turns out that neural networks offer us away all of this dilemma of having to trade off bias and variance with some caveats. And it turns out that large neural networks, when trained on small to moderate-sized datasets, are low-bias machines. And what I mean by that is if you make your neural network large enough, you can almost always fit your training set well. So unless your training set is not enormous. And what this means is this gives us a new recipe to try to reduce bias or reduce variance as needed without needing to really trade off between the two of them. So let me share with you a simple recipe that isn't always applicable, but if it applies, can be very powerful for getting an accurate model using a neural network, which is first train your algorithm on your training set and then ask, does it do well on the training set? So measure J train and see if it is high. And by high, I mean, for example, relative to human level performance or some baseline level performance. And if it is not doing well, then you have a high bias problem, high training set error. And one way to reduce bias is to just use a bigger neural network. And by bigger neural network, I mean, either more hidden layers or more hidden units per layer. And you can then keep on going through this loop and make your neural network bigger and bigger until it does well on the training set, meaning it achieves a level of error on your training set that is roughly comparable to the target level of error you hope to get to, which could be human level performance. After it does well on the training set, so the answer to that question is yes, you would then ask, does it do well on the cross validation set? In other words, does it have high variance? And if the answer is no, then you can conclude that the algorithm has high variance because it does well on the training set, does not do well on the cross validation set. So that big gap in JCV and JTrain indicates you probably have a high variance problem. And if you have a high variance problem, then one way to try to fix it is to get more data. So you get more data and go back and retrain the model and just double check that it does well on the training set. If not, have a bigger network or if it does, see if it does well on the cross validation set and if not, get more data. Then if you can keep on going round and round and round this loop until eventually it does well on the cross validation set, then you're probably done because now you have a model that does well on the cross validation set and hopefully will also generalize to new examples as well. Now of course there are limitations to the application of this recipe. Training a bigger neural network does reduce bias, but at some point it does get computationally expensive. That's why the rise of neural networks has been really assisted by the rise of very fast computers, including especially GPUs or graphics processor units. Hardware traditionally used to speed up computer graphics, but that turns out has been very useful for speeding up neural networks as well. But even with hardware accelerators, beyond a certain point, the neural networks are still large and take so long to train, it becomes infeasible. And then of course the other limitation is more data. Sometimes you can only get so much data and beyond a certain point, it's hard to get much more data. But I think this recipe explains a lot of the rise of deep learning in the last several years, which is for applications where you do have access to a lot of data. Then being able to train large neural networks allows you to eventually get pretty good performance on a lot of applications. One thing that was implicit in this slide that may not have been obvious is that as you're developing a learning algorithm, sometimes you find that you have high bias, in which case you do things like increase the neural network. But then after you increase the neural network, you may find that you have high variance, in which case you might do other things like collect more data. Then during the hours or days or weeks you're developing a machine learning algorithm, at different points you may have high bias or high variance and it can change. But it's depending on whether your algorithm has high bias or high variance at that time that that can help give guidance for what you should be trying next. When you're training a neural network, one thing that people have asked me before is, hey Andrew, whether my neural network is too big, will that create a high variance problem? It turns out that a large neural network with well-chosen regularization will usually do as well or better than a smaller one. And so, for example, if you have a small neural network like this and you were to switch to a much larger neural network like this, you would think that the risk of overfitting goes up significantly. But it turns out that if you were to regularize this larger neural network appropriately, then this larger neural network usually will do at least as well or better than the smaller one, so long as the regularization is chosen appropriately. So another way of saying this is that it almost never hurts to go to a larger neural network so long as you regularize appropriately. With one caveat, which is that when you train a larger neural network, it does become more computationally expensive. So the main way it hurts is it will slow down your training and your inference process. And very briefly, to regularize a neural network, this is what you do. If the cost function for your neural network is the average loss, and so the loss here could be squared error or logistic loss, then the regularization term for a neural network looks like pretty much what you expect is lambda over 2m times the sum of w squared, where this is the sum over all weights w in the neural network. And similar to regularization for linear regression and logistic regression, we usually don't regularize the parameters b in a neural network, although in practice it makes very little difference whether you do so or not. And the way you would implement regularization in TensorFlow is, recall that this was the code for implementing an un-regularized handwritten digit classification model. We create three layers like so with number of hidden units, activation, and then create a sequential model with the three layers. If you want to add regularization, then you would just add this extra term kernel regularizer equals L2 and then 0.01, where that's the value of lambda. TensorFlow actually lets you choose different values of lambda for different layers, although for simplicity, you can choose the same value of lambda for all the weights in all of the different layers as follows. And then this will allow you to implement regularization in your neural network. So to summarize, two takeaways I hope you have from this video are, one, it hardly ever hurts to have a larger neural network so long as you regularize appropriately. One caveat being that having a larger neural network can slow down your out-of-room, so maybe that's the one way it hurts, but it shouldn't hurt your out-of-room's performance for the most part. And in fact, it could even help it significantly. And second, so long as your training set isn't too large, then a neural network, especially a large neural network, is often a low-bias machine. It just fits very complicated functions very well, which is why when I'm training neural networks, I find that I'm often fighting various problems rather than bias problems, at least if the neural network is large enough. So the rise of deep learning has really changed the way that machine learning practitioners think about bias and variance. Having said that, even when you're training a neural network, measuring bias and variance and using that to guide what you do next is often a very hopeful thing to do. So that's it for bias and variance. Let's go on to the next video where we'll take all the ideas we've learned and see how they fit in to the development process of machine learning systems. And I hope that we'll tie a lot of these pieces together to give you practical advice for how to quickly move forward in the development of your machine learning systems.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.4, "text": " We've seen that high bias or high variance are both bad in the sense that they hurt the", "tokens": [50364, 492, 600, 1612, 300, 1090, 12577, 420, 1090, 21977, 366, 1293, 1578, 294, 264, 2020, 300, 436, 4607, 264, 50684, 50684, 3389, 295, 428, 9284, 13, 50808, 50808, 1485, 295, 264, 4112, 300, 18161, 9590, 362, 668, 370, 4406, 307, 570, 18161, 9590, 11, 51064, 51064, 1214, 365, 264, 1558, 295, 955, 1412, 420, 4696, 1419, 2416, 42856, 11, 307, 2902, 505, 257, 777, 51335, 51335, 636, 295, 777, 2098, 281, 2985, 1293, 1090, 12577, 293, 1090, 21977, 13, 51592, 51592, 961, 311, 747, 257, 574, 13, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.16334913065145304, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.019113698974251747}, {"id": 1, "seek": 0, "start": 6.4, "end": 8.88, "text": " performance of your algorithm.", "tokens": [50364, 492, 600, 1612, 300, 1090, 12577, 420, 1090, 21977, 366, 1293, 1578, 294, 264, 2020, 300, 436, 4607, 264, 50684, 50684, 3389, 295, 428, 9284, 13, 50808, 50808, 1485, 295, 264, 4112, 300, 18161, 9590, 362, 668, 370, 4406, 307, 570, 18161, 9590, 11, 51064, 51064, 1214, 365, 264, 1558, 295, 955, 1412, 420, 4696, 1419, 2416, 42856, 11, 307, 2902, 505, 257, 777, 51335, 51335, 636, 295, 777, 2098, 281, 2985, 1293, 1090, 12577, 293, 1090, 21977, 13, 51592, 51592, 961, 311, 747, 257, 574, 13, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.16334913065145304, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.019113698974251747}, {"id": 2, "seek": 0, "start": 8.88, "end": 14.0, "text": " One of the reasons that neural networks have been so successful is because neural networks,", "tokens": [50364, 492, 600, 1612, 300, 1090, 12577, 420, 1090, 21977, 366, 1293, 1578, 294, 264, 2020, 300, 436, 4607, 264, 50684, 50684, 3389, 295, 428, 9284, 13, 50808, 50808, 1485, 295, 264, 4112, 300, 18161, 9590, 362, 668, 370, 4406, 307, 570, 18161, 9590, 11, 51064, 51064, 1214, 365, 264, 1558, 295, 955, 1412, 420, 4696, 1419, 2416, 42856, 11, 307, 2902, 505, 257, 777, 51335, 51335, 636, 295, 777, 2098, 281, 2985, 1293, 1090, 12577, 293, 1090, 21977, 13, 51592, 51592, 961, 311, 747, 257, 574, 13, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.16334913065145304, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.019113698974251747}, {"id": 3, "seek": 0, "start": 14.0, "end": 19.42, "text": " together with the idea of big data or hopefully having large datasets, is giving us a new", "tokens": [50364, 492, 600, 1612, 300, 1090, 12577, 420, 1090, 21977, 366, 1293, 1578, 294, 264, 2020, 300, 436, 4607, 264, 50684, 50684, 3389, 295, 428, 9284, 13, 50808, 50808, 1485, 295, 264, 4112, 300, 18161, 9590, 362, 668, 370, 4406, 307, 570, 18161, 9590, 11, 51064, 51064, 1214, 365, 264, 1558, 295, 955, 1412, 420, 4696, 1419, 2416, 42856, 11, 307, 2902, 505, 257, 777, 51335, 51335, 636, 295, 777, 2098, 281, 2985, 1293, 1090, 12577, 293, 1090, 21977, 13, 51592, 51592, 961, 311, 747, 257, 574, 13, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.16334913065145304, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.019113698974251747}, {"id": 4, "seek": 0, "start": 19.42, "end": 24.560000000000002, "text": " way of new ways to address both high bias and high variance.", "tokens": [50364, 492, 600, 1612, 300, 1090, 12577, 420, 1090, 21977, 366, 1293, 1578, 294, 264, 2020, 300, 436, 4607, 264, 50684, 50684, 3389, 295, 428, 9284, 13, 50808, 50808, 1485, 295, 264, 4112, 300, 18161, 9590, 362, 668, 370, 4406, 307, 570, 18161, 9590, 11, 51064, 51064, 1214, 365, 264, 1558, 295, 955, 1412, 420, 4696, 1419, 2416, 42856, 11, 307, 2902, 505, 257, 777, 51335, 51335, 636, 295, 777, 2098, 281, 2985, 1293, 1090, 12577, 293, 1090, 21977, 13, 51592, 51592, 961, 311, 747, 257, 574, 13, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.16334913065145304, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.019113698974251747}, {"id": 5, "seek": 0, "start": 24.560000000000002, "end": 25.6, "text": " Let's take a look.", "tokens": [50364, 492, 600, 1612, 300, 1090, 12577, 420, 1090, 21977, 366, 1293, 1578, 294, 264, 2020, 300, 436, 4607, 264, 50684, 50684, 3389, 295, 428, 9284, 13, 50808, 50808, 1485, 295, 264, 4112, 300, 18161, 9590, 362, 668, 370, 4406, 307, 570, 18161, 9590, 11, 51064, 51064, 1214, 365, 264, 1558, 295, 955, 1412, 420, 4696, 1419, 2416, 42856, 11, 307, 2902, 505, 257, 777, 51335, 51335, 636, 295, 777, 2098, 281, 2985, 1293, 1090, 12577, 293, 1090, 21977, 13, 51592, 51592, 961, 311, 747, 257, 574, 13, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.16334913065145304, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.019113698974251747}, {"id": 6, "seek": 2560, "start": 25.6, "end": 32.28, "text": " You saw that if you're fitting different order polynomials to a dataset, then if you were", "tokens": [50364, 509, 1866, 300, 498, 291, 434, 15669, 819, 1668, 22560, 12356, 281, 257, 28872, 11, 550, 498, 291, 645, 50698, 50698, 281, 3318, 257, 8213, 2316, 411, 341, 322, 264, 1411, 11, 291, 362, 257, 1238, 2199, 2316, 300, 393, 362, 51004, 51004, 1090, 12577, 13, 51054, 51054, 13813, 498, 291, 645, 281, 3318, 257, 3997, 2316, 11, 550, 291, 1062, 9753, 490, 1090, 21977, 13, 51372, 51372, 400, 456, 311, 341, 4923, 12, 4506, 1296, 12577, 293, 21977, 13, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.15904113224574498, "compression_ratio": 1.6751269035532994, "no_speech_prob": 7.646084668522235e-06}, {"id": 7, "seek": 2560, "start": 32.28, "end": 38.400000000000006, "text": " to fit a linear model like this on the left, you have a pretty simple model that can have", "tokens": [50364, 509, 1866, 300, 498, 291, 434, 15669, 819, 1668, 22560, 12356, 281, 257, 28872, 11, 550, 498, 291, 645, 50698, 50698, 281, 3318, 257, 8213, 2316, 411, 341, 322, 264, 1411, 11, 291, 362, 257, 1238, 2199, 2316, 300, 393, 362, 51004, 51004, 1090, 12577, 13, 51054, 51054, 13813, 498, 291, 645, 281, 3318, 257, 3997, 2316, 11, 550, 291, 1062, 9753, 490, 1090, 21977, 13, 51372, 51372, 400, 456, 311, 341, 4923, 12, 4506, 1296, 12577, 293, 21977, 13, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.15904113224574498, "compression_ratio": 1.6751269035532994, "no_speech_prob": 7.646084668522235e-06}, {"id": 8, "seek": 2560, "start": 38.400000000000006, "end": 39.400000000000006, "text": " high bias.", "tokens": [50364, 509, 1866, 300, 498, 291, 434, 15669, 819, 1668, 22560, 12356, 281, 257, 28872, 11, 550, 498, 291, 645, 50698, 50698, 281, 3318, 257, 8213, 2316, 411, 341, 322, 264, 1411, 11, 291, 362, 257, 1238, 2199, 2316, 300, 393, 362, 51004, 51004, 1090, 12577, 13, 51054, 51054, 13813, 498, 291, 645, 281, 3318, 257, 3997, 2316, 11, 550, 291, 1062, 9753, 490, 1090, 21977, 13, 51372, 51372, 400, 456, 311, 341, 4923, 12, 4506, 1296, 12577, 293, 21977, 13, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.15904113224574498, "compression_ratio": 1.6751269035532994, "no_speech_prob": 7.646084668522235e-06}, {"id": 9, "seek": 2560, "start": 39.400000000000006, "end": 45.760000000000005, "text": " Whereas if you were to fit a complex model, then you might suffer from high variance.", "tokens": [50364, 509, 1866, 300, 498, 291, 434, 15669, 819, 1668, 22560, 12356, 281, 257, 28872, 11, 550, 498, 291, 645, 50698, 50698, 281, 3318, 257, 8213, 2316, 411, 341, 322, 264, 1411, 11, 291, 362, 257, 1238, 2199, 2316, 300, 393, 362, 51004, 51004, 1090, 12577, 13, 51054, 51054, 13813, 498, 291, 645, 281, 3318, 257, 3997, 2316, 11, 550, 291, 1062, 9753, 490, 1090, 21977, 13, 51372, 51372, 400, 456, 311, 341, 4923, 12, 4506, 1296, 12577, 293, 21977, 13, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.15904113224574498, "compression_ratio": 1.6751269035532994, "no_speech_prob": 7.646084668522235e-06}, {"id": 10, "seek": 2560, "start": 45.760000000000005, "end": 50.32, "text": " And there's this trade-off between bias and variance.", "tokens": [50364, 509, 1866, 300, 498, 291, 434, 15669, 819, 1668, 22560, 12356, 281, 257, 28872, 11, 550, 498, 291, 645, 50698, 50698, 281, 3318, 257, 8213, 2316, 411, 341, 322, 264, 1411, 11, 291, 362, 257, 1238, 2199, 2316, 300, 393, 362, 51004, 51004, 1090, 12577, 13, 51054, 51054, 13813, 498, 291, 645, 281, 3318, 257, 3997, 2316, 11, 550, 291, 1062, 9753, 490, 1090, 21977, 13, 51372, 51372, 400, 456, 311, 341, 4923, 12, 4506, 1296, 12577, 293, 21977, 13, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.15904113224574498, "compression_ratio": 1.6751269035532994, "no_speech_prob": 7.646084668522235e-06}, {"id": 11, "seek": 5032, "start": 50.32, "end": 57.08, "text": " In our example, it was choosing a second-order polynomial that helps you make a trade-off", "tokens": [50364, 682, 527, 1365, 11, 309, 390, 10875, 257, 1150, 12, 4687, 26110, 300, 3665, 291, 652, 257, 4923, 12, 4506, 50702, 50702, 293, 1888, 257, 2316, 365, 12437, 1944, 3278, 12, 3337, 327, 399, 6713, 13, 50970, 50970, 400, 370, 949, 264, 1708, 295, 18161, 9590, 11, 3479, 2539, 11955, 2825, 257, 688, 466, 51246, 51246, 341, 12577, 12, 34033, 719, 4923, 12, 4506, 294, 597, 291, 632, 281, 4772, 264, 14024, 11, 300, 307, 11, 264, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.16105502328754942, "compression_ratio": 1.5162790697674418, "no_speech_prob": 2.295880221936386e-06}, {"id": 12, "seek": 5032, "start": 57.08, "end": 62.44, "text": " and pick a model with lowest possible cross-validation error.", "tokens": [50364, 682, 527, 1365, 11, 309, 390, 10875, 257, 1150, 12, 4687, 26110, 300, 3665, 291, 652, 257, 4923, 12, 4506, 50702, 50702, 293, 1888, 257, 2316, 365, 12437, 1944, 3278, 12, 3337, 327, 399, 6713, 13, 50970, 50970, 400, 370, 949, 264, 1708, 295, 18161, 9590, 11, 3479, 2539, 11955, 2825, 257, 688, 466, 51246, 51246, 341, 12577, 12, 34033, 719, 4923, 12, 4506, 294, 597, 291, 632, 281, 4772, 264, 14024, 11, 300, 307, 11, 264, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.16105502328754942, "compression_ratio": 1.5162790697674418, "no_speech_prob": 2.295880221936386e-06}, {"id": 13, "seek": 5032, "start": 62.44, "end": 67.96000000000001, "text": " And so before the days of neural networks, machine learning engineers talked a lot about", "tokens": [50364, 682, 527, 1365, 11, 309, 390, 10875, 257, 1150, 12, 4687, 26110, 300, 3665, 291, 652, 257, 4923, 12, 4506, 50702, 50702, 293, 1888, 257, 2316, 365, 12437, 1944, 3278, 12, 3337, 327, 399, 6713, 13, 50970, 50970, 400, 370, 949, 264, 1708, 295, 18161, 9590, 11, 3479, 2539, 11955, 2825, 257, 688, 466, 51246, 51246, 341, 12577, 12, 34033, 719, 4923, 12, 4506, 294, 597, 291, 632, 281, 4772, 264, 14024, 11, 300, 307, 11, 264, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.16105502328754942, "compression_ratio": 1.5162790697674418, "no_speech_prob": 2.295880221936386e-06}, {"id": 14, "seek": 5032, "start": 67.96000000000001, "end": 75.36, "text": " this bias-variance trade-off in which you had to balance the complexity, that is, the", "tokens": [50364, 682, 527, 1365, 11, 309, 390, 10875, 257, 1150, 12, 4687, 26110, 300, 3665, 291, 652, 257, 4923, 12, 4506, 50702, 50702, 293, 1888, 257, 2316, 365, 12437, 1944, 3278, 12, 3337, 327, 399, 6713, 13, 50970, 50970, 400, 370, 949, 264, 1708, 295, 18161, 9590, 11, 3479, 2539, 11955, 2825, 257, 688, 466, 51246, 51246, 341, 12577, 12, 34033, 719, 4923, 12, 4506, 294, 597, 291, 632, 281, 4772, 264, 14024, 11, 300, 307, 11, 264, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.16105502328754942, "compression_ratio": 1.5162790697674418, "no_speech_prob": 2.295880221936386e-06}, {"id": 15, "seek": 7536, "start": 75.36, "end": 80.8, "text": " degree of polynomial or the regularization parameter lambda to make bias and variance", "tokens": [50364, 4314, 295, 26110, 420, 264, 3890, 2144, 13075, 13607, 281, 652, 12577, 293, 21977, 50636, 50636, 1293, 406, 312, 886, 1090, 13, 50768, 50768, 400, 498, 291, 1568, 3479, 2539, 11955, 751, 466, 264, 12577, 12, 34033, 719, 4923, 12, 4506, 11, 341, 51006, 51006, 307, 437, 436, 434, 13761, 281, 11, 689, 498, 291, 362, 886, 2199, 257, 2316, 11, 291, 362, 1090, 12577, 11, 51248, 51248, 886, 3997, 257, 2316, 11, 1090, 21977, 11, 293, 291, 362, 281, 915, 257, 4923, 12, 4506, 1296, 613, 732, 51444, 51444, 1578, 721, 281, 915, 4696, 264, 1151, 1944, 9700, 13, 51691, 51691], "temperature": 0.0, "avg_logprob": -0.11386391749748817, "compression_ratio": 1.7387755102040816, "no_speech_prob": 9.570767360855825e-07}, {"id": 16, "seek": 7536, "start": 80.8, "end": 83.44, "text": " both not be too high.", "tokens": [50364, 4314, 295, 26110, 420, 264, 3890, 2144, 13075, 13607, 281, 652, 12577, 293, 21977, 50636, 50636, 1293, 406, 312, 886, 1090, 13, 50768, 50768, 400, 498, 291, 1568, 3479, 2539, 11955, 751, 466, 264, 12577, 12, 34033, 719, 4923, 12, 4506, 11, 341, 51006, 51006, 307, 437, 436, 434, 13761, 281, 11, 689, 498, 291, 362, 886, 2199, 257, 2316, 11, 291, 362, 1090, 12577, 11, 51248, 51248, 886, 3997, 257, 2316, 11, 1090, 21977, 11, 293, 291, 362, 281, 915, 257, 4923, 12, 4506, 1296, 613, 732, 51444, 51444, 1578, 721, 281, 915, 4696, 264, 1151, 1944, 9700, 13, 51691, 51691], "temperature": 0.0, "avg_logprob": -0.11386391749748817, "compression_ratio": 1.7387755102040816, "no_speech_prob": 9.570767360855825e-07}, {"id": 17, "seek": 7536, "start": 83.44, "end": 88.2, "text": " And if you hear machine learning engineers talk about the bias-variance trade-off, this", "tokens": [50364, 4314, 295, 26110, 420, 264, 3890, 2144, 13075, 13607, 281, 652, 12577, 293, 21977, 50636, 50636, 1293, 406, 312, 886, 1090, 13, 50768, 50768, 400, 498, 291, 1568, 3479, 2539, 11955, 751, 466, 264, 12577, 12, 34033, 719, 4923, 12, 4506, 11, 341, 51006, 51006, 307, 437, 436, 434, 13761, 281, 11, 689, 498, 291, 362, 886, 2199, 257, 2316, 11, 291, 362, 1090, 12577, 11, 51248, 51248, 886, 3997, 257, 2316, 11, 1090, 21977, 11, 293, 291, 362, 281, 915, 257, 4923, 12, 4506, 1296, 613, 732, 51444, 51444, 1578, 721, 281, 915, 4696, 264, 1151, 1944, 9700, 13, 51691, 51691], "temperature": 0.0, "avg_logprob": -0.11386391749748817, "compression_ratio": 1.7387755102040816, "no_speech_prob": 9.570767360855825e-07}, {"id": 18, "seek": 7536, "start": 88.2, "end": 93.03999999999999, "text": " is what they're referring to, where if you have too simple a model, you have high bias,", "tokens": [50364, 4314, 295, 26110, 420, 264, 3890, 2144, 13075, 13607, 281, 652, 12577, 293, 21977, 50636, 50636, 1293, 406, 312, 886, 1090, 13, 50768, 50768, 400, 498, 291, 1568, 3479, 2539, 11955, 751, 466, 264, 12577, 12, 34033, 719, 4923, 12, 4506, 11, 341, 51006, 51006, 307, 437, 436, 434, 13761, 281, 11, 689, 498, 291, 362, 886, 2199, 257, 2316, 11, 291, 362, 1090, 12577, 11, 51248, 51248, 886, 3997, 257, 2316, 11, 1090, 21977, 11, 293, 291, 362, 281, 915, 257, 4923, 12, 4506, 1296, 613, 732, 51444, 51444, 1578, 721, 281, 915, 4696, 264, 1151, 1944, 9700, 13, 51691, 51691], "temperature": 0.0, "avg_logprob": -0.11386391749748817, "compression_ratio": 1.7387755102040816, "no_speech_prob": 9.570767360855825e-07}, {"id": 19, "seek": 7536, "start": 93.03999999999999, "end": 96.96000000000001, "text": " too complex a model, high variance, and you have to find a trade-off between these two", "tokens": [50364, 4314, 295, 26110, 420, 264, 3890, 2144, 13075, 13607, 281, 652, 12577, 293, 21977, 50636, 50636, 1293, 406, 312, 886, 1090, 13, 50768, 50768, 400, 498, 291, 1568, 3479, 2539, 11955, 751, 466, 264, 12577, 12, 34033, 719, 4923, 12, 4506, 11, 341, 51006, 51006, 307, 437, 436, 434, 13761, 281, 11, 689, 498, 291, 362, 886, 2199, 257, 2316, 11, 291, 362, 1090, 12577, 11, 51248, 51248, 886, 3997, 257, 2316, 11, 1090, 21977, 11, 293, 291, 362, 281, 915, 257, 4923, 12, 4506, 1296, 613, 732, 51444, 51444, 1578, 721, 281, 915, 4696, 264, 1151, 1944, 9700, 13, 51691, 51691], "temperature": 0.0, "avg_logprob": -0.11386391749748817, "compression_ratio": 1.7387755102040816, "no_speech_prob": 9.570767360855825e-07}, {"id": 20, "seek": 7536, "start": 96.96000000000001, "end": 101.9, "text": " bad things to find hopefully the best possible outcome.", "tokens": [50364, 4314, 295, 26110, 420, 264, 3890, 2144, 13075, 13607, 281, 652, 12577, 293, 21977, 50636, 50636, 1293, 406, 312, 886, 1090, 13, 50768, 50768, 400, 498, 291, 1568, 3479, 2539, 11955, 751, 466, 264, 12577, 12, 34033, 719, 4923, 12, 4506, 11, 341, 51006, 51006, 307, 437, 436, 434, 13761, 281, 11, 689, 498, 291, 362, 886, 2199, 257, 2316, 11, 291, 362, 1090, 12577, 11, 51248, 51248, 886, 3997, 257, 2316, 11, 1090, 21977, 11, 293, 291, 362, 281, 915, 257, 4923, 12, 4506, 1296, 613, 732, 51444, 51444, 1578, 721, 281, 915, 4696, 264, 1151, 1944, 9700, 13, 51691, 51691], "temperature": 0.0, "avg_logprob": -0.11386391749748817, "compression_ratio": 1.7387755102040816, "no_speech_prob": 9.570767360855825e-07}, {"id": 21, "seek": 10190, "start": 101.9, "end": 108.08000000000001, "text": " But it turns out that neural networks offer us away all of this dilemma of having to trade", "tokens": [50364, 583, 309, 4523, 484, 300, 18161, 9590, 2626, 505, 1314, 439, 295, 341, 34312, 295, 1419, 281, 4923, 50673, 50673, 766, 12577, 293, 21977, 365, 512, 11730, 1720, 13, 50841, 50841, 400, 309, 4523, 484, 300, 2416, 18161, 9590, 11, 562, 8895, 322, 1359, 281, 18174, 12, 20614, 42856, 11, 51243, 51243, 366, 2295, 12, 65, 4609, 8379, 13, 51361, 51361, 400, 437, 286, 914, 538, 300, 307, 498, 291, 652, 428, 18161, 3209, 2416, 1547, 11, 291, 393, 1920, 51686, 51686, 1009, 3318, 428, 3097, 992, 731, 13, 51827, 51827], "temperature": 0.0, "avg_logprob": -0.14482245906706778, "compression_ratio": 1.7142857142857142, "no_speech_prob": 8.186248123820405e-07}, {"id": 22, "seek": 10190, "start": 108.08000000000001, "end": 111.44000000000001, "text": " off bias and variance with some caveats.", "tokens": [50364, 583, 309, 4523, 484, 300, 18161, 9590, 2626, 505, 1314, 439, 295, 341, 34312, 295, 1419, 281, 4923, 50673, 50673, 766, 12577, 293, 21977, 365, 512, 11730, 1720, 13, 50841, 50841, 400, 309, 4523, 484, 300, 2416, 18161, 9590, 11, 562, 8895, 322, 1359, 281, 18174, 12, 20614, 42856, 11, 51243, 51243, 366, 2295, 12, 65, 4609, 8379, 13, 51361, 51361, 400, 437, 286, 914, 538, 300, 307, 498, 291, 652, 428, 18161, 3209, 2416, 1547, 11, 291, 393, 1920, 51686, 51686, 1009, 3318, 428, 3097, 992, 731, 13, 51827, 51827], "temperature": 0.0, "avg_logprob": -0.14482245906706778, "compression_ratio": 1.7142857142857142, "no_speech_prob": 8.186248123820405e-07}, {"id": 23, "seek": 10190, "start": 111.44000000000001, "end": 119.48, "text": " And it turns out that large neural networks, when trained on small to moderate-sized datasets,", "tokens": [50364, 583, 309, 4523, 484, 300, 18161, 9590, 2626, 505, 1314, 439, 295, 341, 34312, 295, 1419, 281, 4923, 50673, 50673, 766, 12577, 293, 21977, 365, 512, 11730, 1720, 13, 50841, 50841, 400, 309, 4523, 484, 300, 2416, 18161, 9590, 11, 562, 8895, 322, 1359, 281, 18174, 12, 20614, 42856, 11, 51243, 51243, 366, 2295, 12, 65, 4609, 8379, 13, 51361, 51361, 400, 437, 286, 914, 538, 300, 307, 498, 291, 652, 428, 18161, 3209, 2416, 1547, 11, 291, 393, 1920, 51686, 51686, 1009, 3318, 428, 3097, 992, 731, 13, 51827, 51827], "temperature": 0.0, "avg_logprob": -0.14482245906706778, "compression_ratio": 1.7142857142857142, "no_speech_prob": 8.186248123820405e-07}, {"id": 24, "seek": 10190, "start": 119.48, "end": 121.84, "text": " are low-bias machines.", "tokens": [50364, 583, 309, 4523, 484, 300, 18161, 9590, 2626, 505, 1314, 439, 295, 341, 34312, 295, 1419, 281, 4923, 50673, 50673, 766, 12577, 293, 21977, 365, 512, 11730, 1720, 13, 50841, 50841, 400, 309, 4523, 484, 300, 2416, 18161, 9590, 11, 562, 8895, 322, 1359, 281, 18174, 12, 20614, 42856, 11, 51243, 51243, 366, 2295, 12, 65, 4609, 8379, 13, 51361, 51361, 400, 437, 286, 914, 538, 300, 307, 498, 291, 652, 428, 18161, 3209, 2416, 1547, 11, 291, 393, 1920, 51686, 51686, 1009, 3318, 428, 3097, 992, 731, 13, 51827, 51827], "temperature": 0.0, "avg_logprob": -0.14482245906706778, "compression_ratio": 1.7142857142857142, "no_speech_prob": 8.186248123820405e-07}, {"id": 25, "seek": 10190, "start": 121.84, "end": 128.34, "text": " And what I mean by that is if you make your neural network large enough, you can almost", "tokens": [50364, 583, 309, 4523, 484, 300, 18161, 9590, 2626, 505, 1314, 439, 295, 341, 34312, 295, 1419, 281, 4923, 50673, 50673, 766, 12577, 293, 21977, 365, 512, 11730, 1720, 13, 50841, 50841, 400, 309, 4523, 484, 300, 2416, 18161, 9590, 11, 562, 8895, 322, 1359, 281, 18174, 12, 20614, 42856, 11, 51243, 51243, 366, 2295, 12, 65, 4609, 8379, 13, 51361, 51361, 400, 437, 286, 914, 538, 300, 307, 498, 291, 652, 428, 18161, 3209, 2416, 1547, 11, 291, 393, 1920, 51686, 51686, 1009, 3318, 428, 3097, 992, 731, 13, 51827, 51827], "temperature": 0.0, "avg_logprob": -0.14482245906706778, "compression_ratio": 1.7142857142857142, "no_speech_prob": 8.186248123820405e-07}, {"id": 26, "seek": 10190, "start": 128.34, "end": 131.16, "text": " always fit your training set well.", "tokens": [50364, 583, 309, 4523, 484, 300, 18161, 9590, 2626, 505, 1314, 439, 295, 341, 34312, 295, 1419, 281, 4923, 50673, 50673, 766, 12577, 293, 21977, 365, 512, 11730, 1720, 13, 50841, 50841, 400, 309, 4523, 484, 300, 2416, 18161, 9590, 11, 562, 8895, 322, 1359, 281, 18174, 12, 20614, 42856, 11, 51243, 51243, 366, 2295, 12, 65, 4609, 8379, 13, 51361, 51361, 400, 437, 286, 914, 538, 300, 307, 498, 291, 652, 428, 18161, 3209, 2416, 1547, 11, 291, 393, 1920, 51686, 51686, 1009, 3318, 428, 3097, 992, 731, 13, 51827, 51827], "temperature": 0.0, "avg_logprob": -0.14482245906706778, "compression_ratio": 1.7142857142857142, "no_speech_prob": 8.186248123820405e-07}, {"id": 27, "seek": 13116, "start": 131.16, "end": 133.79999999999998, "text": " So unless your training set is not enormous.", "tokens": [50364, 407, 5969, 428, 3097, 992, 307, 406, 11322, 13, 50496, 50496, 400, 437, 341, 1355, 307, 341, 2709, 505, 257, 777, 6782, 281, 853, 281, 5407, 12577, 420, 5407, 21977, 50814, 50814, 382, 2978, 1553, 18006, 281, 534, 4923, 766, 1296, 264, 732, 295, 552, 13, 51032, 51032, 407, 718, 385, 2073, 365, 291, 257, 2199, 6782, 300, 1943, 380, 1009, 21142, 11, 457, 498, 309, 13165, 11, 51302, 51302, 393, 312, 588, 4005, 337, 1242, 364, 8559, 2316, 1228, 257, 18161, 3209, 11, 597, 307, 700, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.10191595685351026, "compression_ratio": 1.5950413223140496, "no_speech_prob": 7.183122761489358e-06}, {"id": 28, "seek": 13116, "start": 133.79999999999998, "end": 140.16, "text": " And what this means is this gives us a new recipe to try to reduce bias or reduce variance", "tokens": [50364, 407, 5969, 428, 3097, 992, 307, 406, 11322, 13, 50496, 50496, 400, 437, 341, 1355, 307, 341, 2709, 505, 257, 777, 6782, 281, 853, 281, 5407, 12577, 420, 5407, 21977, 50814, 50814, 382, 2978, 1553, 18006, 281, 534, 4923, 766, 1296, 264, 732, 295, 552, 13, 51032, 51032, 407, 718, 385, 2073, 365, 291, 257, 2199, 6782, 300, 1943, 380, 1009, 21142, 11, 457, 498, 309, 13165, 11, 51302, 51302, 393, 312, 588, 4005, 337, 1242, 364, 8559, 2316, 1228, 257, 18161, 3209, 11, 597, 307, 700, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.10191595685351026, "compression_ratio": 1.5950413223140496, "no_speech_prob": 7.183122761489358e-06}, {"id": 29, "seek": 13116, "start": 140.16, "end": 144.51999999999998, "text": " as needed without needing to really trade off between the two of them.", "tokens": [50364, 407, 5969, 428, 3097, 992, 307, 406, 11322, 13, 50496, 50496, 400, 437, 341, 1355, 307, 341, 2709, 505, 257, 777, 6782, 281, 853, 281, 5407, 12577, 420, 5407, 21977, 50814, 50814, 382, 2978, 1553, 18006, 281, 534, 4923, 766, 1296, 264, 732, 295, 552, 13, 51032, 51032, 407, 718, 385, 2073, 365, 291, 257, 2199, 6782, 300, 1943, 380, 1009, 21142, 11, 457, 498, 309, 13165, 11, 51302, 51302, 393, 312, 588, 4005, 337, 1242, 364, 8559, 2316, 1228, 257, 18161, 3209, 11, 597, 307, 700, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.10191595685351026, "compression_ratio": 1.5950413223140496, "no_speech_prob": 7.183122761489358e-06}, {"id": 30, "seek": 13116, "start": 144.51999999999998, "end": 149.92, "text": " So let me share with you a simple recipe that isn't always applicable, but if it applies,", "tokens": [50364, 407, 5969, 428, 3097, 992, 307, 406, 11322, 13, 50496, 50496, 400, 437, 341, 1355, 307, 341, 2709, 505, 257, 777, 6782, 281, 853, 281, 5407, 12577, 420, 5407, 21977, 50814, 50814, 382, 2978, 1553, 18006, 281, 534, 4923, 766, 1296, 264, 732, 295, 552, 13, 51032, 51032, 407, 718, 385, 2073, 365, 291, 257, 2199, 6782, 300, 1943, 380, 1009, 21142, 11, 457, 498, 309, 13165, 11, 51302, 51302, 393, 312, 588, 4005, 337, 1242, 364, 8559, 2316, 1228, 257, 18161, 3209, 11, 597, 307, 700, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.10191595685351026, "compression_ratio": 1.5950413223140496, "no_speech_prob": 7.183122761489358e-06}, {"id": 31, "seek": 13116, "start": 149.92, "end": 158.2, "text": " can be very powerful for getting an accurate model using a neural network, which is first", "tokens": [50364, 407, 5969, 428, 3097, 992, 307, 406, 11322, 13, 50496, 50496, 400, 437, 341, 1355, 307, 341, 2709, 505, 257, 777, 6782, 281, 853, 281, 5407, 12577, 420, 5407, 21977, 50814, 50814, 382, 2978, 1553, 18006, 281, 534, 4923, 766, 1296, 264, 732, 295, 552, 13, 51032, 51032, 407, 718, 385, 2073, 365, 291, 257, 2199, 6782, 300, 1943, 380, 1009, 21142, 11, 457, 498, 309, 13165, 11, 51302, 51302, 393, 312, 588, 4005, 337, 1242, 364, 8559, 2316, 1228, 257, 18161, 3209, 11, 597, 307, 700, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.10191595685351026, "compression_ratio": 1.5950413223140496, "no_speech_prob": 7.183122761489358e-06}, {"id": 32, "seek": 15820, "start": 158.2, "end": 163.28, "text": " train your algorithm on your training set and then ask, does it do well on the training", "tokens": [50364, 3847, 428, 9284, 322, 428, 3097, 992, 293, 550, 1029, 11, 775, 309, 360, 731, 322, 264, 3097, 50618, 50618, 992, 30, 50668, 50668, 407, 3481, 508, 3847, 293, 536, 498, 309, 307, 1090, 13, 50876, 50876, 400, 538, 1090, 11, 286, 914, 11, 337, 1365, 11, 4972, 281, 1952, 1496, 3389, 420, 512, 20518, 51136, 51136, 1496, 3389, 13, 51258, 51258, 400, 498, 309, 307, 406, 884, 731, 11, 550, 291, 362, 257, 1090, 12577, 1154, 11, 1090, 3097, 992, 6713, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.13318094713934536, "compression_ratio": 1.7135416666666667, "no_speech_prob": 6.853912964288611e-06}, {"id": 33, "seek": 15820, "start": 163.28, "end": 164.28, "text": " set?", "tokens": [50364, 3847, 428, 9284, 322, 428, 3097, 992, 293, 550, 1029, 11, 775, 309, 360, 731, 322, 264, 3097, 50618, 50618, 992, 30, 50668, 50668, 407, 3481, 508, 3847, 293, 536, 498, 309, 307, 1090, 13, 50876, 50876, 400, 538, 1090, 11, 286, 914, 11, 337, 1365, 11, 4972, 281, 1952, 1496, 3389, 420, 512, 20518, 51136, 51136, 1496, 3389, 13, 51258, 51258, 400, 498, 309, 307, 406, 884, 731, 11, 550, 291, 362, 257, 1090, 12577, 1154, 11, 1090, 3097, 992, 6713, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.13318094713934536, "compression_ratio": 1.7135416666666667, "no_speech_prob": 6.853912964288611e-06}, {"id": 34, "seek": 15820, "start": 164.28, "end": 168.44, "text": " So measure J train and see if it is high.", "tokens": [50364, 3847, 428, 9284, 322, 428, 3097, 992, 293, 550, 1029, 11, 775, 309, 360, 731, 322, 264, 3097, 50618, 50618, 992, 30, 50668, 50668, 407, 3481, 508, 3847, 293, 536, 498, 309, 307, 1090, 13, 50876, 50876, 400, 538, 1090, 11, 286, 914, 11, 337, 1365, 11, 4972, 281, 1952, 1496, 3389, 420, 512, 20518, 51136, 51136, 1496, 3389, 13, 51258, 51258, 400, 498, 309, 307, 406, 884, 731, 11, 550, 291, 362, 257, 1090, 12577, 1154, 11, 1090, 3097, 992, 6713, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.13318094713934536, "compression_ratio": 1.7135416666666667, "no_speech_prob": 6.853912964288611e-06}, {"id": 35, "seek": 15820, "start": 168.44, "end": 173.64, "text": " And by high, I mean, for example, relative to human level performance or some baseline", "tokens": [50364, 3847, 428, 9284, 322, 428, 3097, 992, 293, 550, 1029, 11, 775, 309, 360, 731, 322, 264, 3097, 50618, 50618, 992, 30, 50668, 50668, 407, 3481, 508, 3847, 293, 536, 498, 309, 307, 1090, 13, 50876, 50876, 400, 538, 1090, 11, 286, 914, 11, 337, 1365, 11, 4972, 281, 1952, 1496, 3389, 420, 512, 20518, 51136, 51136, 1496, 3389, 13, 51258, 51258, 400, 498, 309, 307, 406, 884, 731, 11, 550, 291, 362, 257, 1090, 12577, 1154, 11, 1090, 3097, 992, 6713, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.13318094713934536, "compression_ratio": 1.7135416666666667, "no_speech_prob": 6.853912964288611e-06}, {"id": 36, "seek": 15820, "start": 173.64, "end": 176.07999999999998, "text": " level performance.", "tokens": [50364, 3847, 428, 9284, 322, 428, 3097, 992, 293, 550, 1029, 11, 775, 309, 360, 731, 322, 264, 3097, 50618, 50618, 992, 30, 50668, 50668, 407, 3481, 508, 3847, 293, 536, 498, 309, 307, 1090, 13, 50876, 50876, 400, 538, 1090, 11, 286, 914, 11, 337, 1365, 11, 4972, 281, 1952, 1496, 3389, 420, 512, 20518, 51136, 51136, 1496, 3389, 13, 51258, 51258, 400, 498, 309, 307, 406, 884, 731, 11, 550, 291, 362, 257, 1090, 12577, 1154, 11, 1090, 3097, 992, 6713, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.13318094713934536, "compression_ratio": 1.7135416666666667, "no_speech_prob": 6.853912964288611e-06}, {"id": 37, "seek": 15820, "start": 176.07999999999998, "end": 182.67999999999998, "text": " And if it is not doing well, then you have a high bias problem, high training set error.", "tokens": [50364, 3847, 428, 9284, 322, 428, 3097, 992, 293, 550, 1029, 11, 775, 309, 360, 731, 322, 264, 3097, 50618, 50618, 992, 30, 50668, 50668, 407, 3481, 508, 3847, 293, 536, 498, 309, 307, 1090, 13, 50876, 50876, 400, 538, 1090, 11, 286, 914, 11, 337, 1365, 11, 4972, 281, 1952, 1496, 3389, 420, 512, 20518, 51136, 51136, 1496, 3389, 13, 51258, 51258, 400, 498, 309, 307, 406, 884, 731, 11, 550, 291, 362, 257, 1090, 12577, 1154, 11, 1090, 3097, 992, 6713, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.13318094713934536, "compression_ratio": 1.7135416666666667, "no_speech_prob": 6.853912964288611e-06}, {"id": 38, "seek": 18268, "start": 182.68, "end": 188.4, "text": " And one way to reduce bias is to just use a bigger neural network.", "tokens": [50364, 400, 472, 636, 281, 5407, 12577, 307, 281, 445, 764, 257, 3801, 18161, 3209, 13, 50650, 50650, 400, 538, 3801, 18161, 3209, 11, 286, 914, 11, 2139, 544, 7633, 7914, 420, 544, 7633, 6815, 680, 50892, 50892, 4583, 13, 50972, 50972, 400, 291, 393, 550, 1066, 322, 516, 807, 341, 6367, 293, 652, 428, 18161, 3209, 3801, 293, 51228, 51228, 3801, 1826, 309, 775, 731, 322, 264, 3097, 992, 11, 3620, 309, 3538, 977, 257, 1496, 295, 6713, 51458, 51458, 322, 428, 3097, 992, 300, 307, 9810, 25323, 281, 264, 3779, 1496, 295, 6713, 291, 1454, 281, 483, 51773, 51773], "temperature": 0.0, "avg_logprob": -0.10882957308900122, "compression_ratio": 1.8771929824561404, "no_speech_prob": 2.9648597887899086e-07}, {"id": 39, "seek": 18268, "start": 188.4, "end": 193.24, "text": " And by bigger neural network, I mean, either more hidden layers or more hidden units per", "tokens": [50364, 400, 472, 636, 281, 5407, 12577, 307, 281, 445, 764, 257, 3801, 18161, 3209, 13, 50650, 50650, 400, 538, 3801, 18161, 3209, 11, 286, 914, 11, 2139, 544, 7633, 7914, 420, 544, 7633, 6815, 680, 50892, 50892, 4583, 13, 50972, 50972, 400, 291, 393, 550, 1066, 322, 516, 807, 341, 6367, 293, 652, 428, 18161, 3209, 3801, 293, 51228, 51228, 3801, 1826, 309, 775, 731, 322, 264, 3097, 992, 11, 3620, 309, 3538, 977, 257, 1496, 295, 6713, 51458, 51458, 322, 428, 3097, 992, 300, 307, 9810, 25323, 281, 264, 3779, 1496, 295, 6713, 291, 1454, 281, 483, 51773, 51773], "temperature": 0.0, "avg_logprob": -0.10882957308900122, "compression_ratio": 1.8771929824561404, "no_speech_prob": 2.9648597887899086e-07}, {"id": 40, "seek": 18268, "start": 193.24, "end": 194.84, "text": " layer.", "tokens": [50364, 400, 472, 636, 281, 5407, 12577, 307, 281, 445, 764, 257, 3801, 18161, 3209, 13, 50650, 50650, 400, 538, 3801, 18161, 3209, 11, 286, 914, 11, 2139, 544, 7633, 7914, 420, 544, 7633, 6815, 680, 50892, 50892, 4583, 13, 50972, 50972, 400, 291, 393, 550, 1066, 322, 516, 807, 341, 6367, 293, 652, 428, 18161, 3209, 3801, 293, 51228, 51228, 3801, 1826, 309, 775, 731, 322, 264, 3097, 992, 11, 3620, 309, 3538, 977, 257, 1496, 295, 6713, 51458, 51458, 322, 428, 3097, 992, 300, 307, 9810, 25323, 281, 264, 3779, 1496, 295, 6713, 291, 1454, 281, 483, 51773, 51773], "temperature": 0.0, "avg_logprob": -0.10882957308900122, "compression_ratio": 1.8771929824561404, "no_speech_prob": 2.9648597887899086e-07}, {"id": 41, "seek": 18268, "start": 194.84, "end": 199.96, "text": " And you can then keep on going through this loop and make your neural network bigger and", "tokens": [50364, 400, 472, 636, 281, 5407, 12577, 307, 281, 445, 764, 257, 3801, 18161, 3209, 13, 50650, 50650, 400, 538, 3801, 18161, 3209, 11, 286, 914, 11, 2139, 544, 7633, 7914, 420, 544, 7633, 6815, 680, 50892, 50892, 4583, 13, 50972, 50972, 400, 291, 393, 550, 1066, 322, 516, 807, 341, 6367, 293, 652, 428, 18161, 3209, 3801, 293, 51228, 51228, 3801, 1826, 309, 775, 731, 322, 264, 3097, 992, 11, 3620, 309, 3538, 977, 257, 1496, 295, 6713, 51458, 51458, 322, 428, 3097, 992, 300, 307, 9810, 25323, 281, 264, 3779, 1496, 295, 6713, 291, 1454, 281, 483, 51773, 51773], "temperature": 0.0, "avg_logprob": -0.10882957308900122, "compression_ratio": 1.8771929824561404, "no_speech_prob": 2.9648597887899086e-07}, {"id": 42, "seek": 18268, "start": 199.96, "end": 204.56, "text": " bigger until it does well on the training set, meaning it achieves a level of error", "tokens": [50364, 400, 472, 636, 281, 5407, 12577, 307, 281, 445, 764, 257, 3801, 18161, 3209, 13, 50650, 50650, 400, 538, 3801, 18161, 3209, 11, 286, 914, 11, 2139, 544, 7633, 7914, 420, 544, 7633, 6815, 680, 50892, 50892, 4583, 13, 50972, 50972, 400, 291, 393, 550, 1066, 322, 516, 807, 341, 6367, 293, 652, 428, 18161, 3209, 3801, 293, 51228, 51228, 3801, 1826, 309, 775, 731, 322, 264, 3097, 992, 11, 3620, 309, 3538, 977, 257, 1496, 295, 6713, 51458, 51458, 322, 428, 3097, 992, 300, 307, 9810, 25323, 281, 264, 3779, 1496, 295, 6713, 291, 1454, 281, 483, 51773, 51773], "temperature": 0.0, "avg_logprob": -0.10882957308900122, "compression_ratio": 1.8771929824561404, "no_speech_prob": 2.9648597887899086e-07}, {"id": 43, "seek": 18268, "start": 204.56, "end": 210.86, "text": " on your training set that is roughly comparable to the target level of error you hope to get", "tokens": [50364, 400, 472, 636, 281, 5407, 12577, 307, 281, 445, 764, 257, 3801, 18161, 3209, 13, 50650, 50650, 400, 538, 3801, 18161, 3209, 11, 286, 914, 11, 2139, 544, 7633, 7914, 420, 544, 7633, 6815, 680, 50892, 50892, 4583, 13, 50972, 50972, 400, 291, 393, 550, 1066, 322, 516, 807, 341, 6367, 293, 652, 428, 18161, 3209, 3801, 293, 51228, 51228, 3801, 1826, 309, 775, 731, 322, 264, 3097, 992, 11, 3620, 309, 3538, 977, 257, 1496, 295, 6713, 51458, 51458, 322, 428, 3097, 992, 300, 307, 9810, 25323, 281, 264, 3779, 1496, 295, 6713, 291, 1454, 281, 483, 51773, 51773], "temperature": 0.0, "avg_logprob": -0.10882957308900122, "compression_ratio": 1.8771929824561404, "no_speech_prob": 2.9648597887899086e-07}, {"id": 44, "seek": 21086, "start": 210.86, "end": 215.0, "text": " to, which could be human level performance.", "tokens": [50364, 281, 11, 597, 727, 312, 1952, 1496, 3389, 13, 50571, 50571, 2381, 309, 775, 731, 322, 264, 3097, 992, 11, 370, 264, 1867, 281, 300, 1168, 307, 2086, 11, 291, 576, 50767, 50767, 550, 1029, 11, 775, 309, 360, 731, 322, 264, 3278, 24071, 992, 30, 50935, 50935, 682, 661, 2283, 11, 775, 309, 362, 1090, 21977, 30, 51147, 51147, 400, 498, 264, 1867, 307, 572, 11, 550, 291, 393, 16886, 300, 264, 9284, 575, 1090, 21977, 570, 51426, 51426, 309, 775, 731, 322, 264, 3097, 992, 11, 775, 406, 360, 731, 322, 264, 3278, 24071, 992, 13, 51601, 51601], "temperature": 0.0, "avg_logprob": -0.12326950185439166, "compression_ratio": 1.995049504950495, "no_speech_prob": 9.080104064196348e-06}, {"id": 45, "seek": 21086, "start": 215.0, "end": 218.92000000000002, "text": " After it does well on the training set, so the answer to that question is yes, you would", "tokens": [50364, 281, 11, 597, 727, 312, 1952, 1496, 3389, 13, 50571, 50571, 2381, 309, 775, 731, 322, 264, 3097, 992, 11, 370, 264, 1867, 281, 300, 1168, 307, 2086, 11, 291, 576, 50767, 50767, 550, 1029, 11, 775, 309, 360, 731, 322, 264, 3278, 24071, 992, 30, 50935, 50935, 682, 661, 2283, 11, 775, 309, 362, 1090, 21977, 30, 51147, 51147, 400, 498, 264, 1867, 307, 572, 11, 550, 291, 393, 16886, 300, 264, 9284, 575, 1090, 21977, 570, 51426, 51426, 309, 775, 731, 322, 264, 3097, 992, 11, 775, 406, 360, 731, 322, 264, 3278, 24071, 992, 13, 51601, 51601], "temperature": 0.0, "avg_logprob": -0.12326950185439166, "compression_ratio": 1.995049504950495, "no_speech_prob": 9.080104064196348e-06}, {"id": 46, "seek": 21086, "start": 218.92000000000002, "end": 222.28, "text": " then ask, does it do well on the cross validation set?", "tokens": [50364, 281, 11, 597, 727, 312, 1952, 1496, 3389, 13, 50571, 50571, 2381, 309, 775, 731, 322, 264, 3097, 992, 11, 370, 264, 1867, 281, 300, 1168, 307, 2086, 11, 291, 576, 50767, 50767, 550, 1029, 11, 775, 309, 360, 731, 322, 264, 3278, 24071, 992, 30, 50935, 50935, 682, 661, 2283, 11, 775, 309, 362, 1090, 21977, 30, 51147, 51147, 400, 498, 264, 1867, 307, 572, 11, 550, 291, 393, 16886, 300, 264, 9284, 575, 1090, 21977, 570, 51426, 51426, 309, 775, 731, 322, 264, 3097, 992, 11, 775, 406, 360, 731, 322, 264, 3278, 24071, 992, 13, 51601, 51601], "temperature": 0.0, "avg_logprob": -0.12326950185439166, "compression_ratio": 1.995049504950495, "no_speech_prob": 9.080104064196348e-06}, {"id": 47, "seek": 21086, "start": 222.28, "end": 226.52, "text": " In other words, does it have high variance?", "tokens": [50364, 281, 11, 597, 727, 312, 1952, 1496, 3389, 13, 50571, 50571, 2381, 309, 775, 731, 322, 264, 3097, 992, 11, 370, 264, 1867, 281, 300, 1168, 307, 2086, 11, 291, 576, 50767, 50767, 550, 1029, 11, 775, 309, 360, 731, 322, 264, 3278, 24071, 992, 30, 50935, 50935, 682, 661, 2283, 11, 775, 309, 362, 1090, 21977, 30, 51147, 51147, 400, 498, 264, 1867, 307, 572, 11, 550, 291, 393, 16886, 300, 264, 9284, 575, 1090, 21977, 570, 51426, 51426, 309, 775, 731, 322, 264, 3097, 992, 11, 775, 406, 360, 731, 322, 264, 3278, 24071, 992, 13, 51601, 51601], "temperature": 0.0, "avg_logprob": -0.12326950185439166, "compression_ratio": 1.995049504950495, "no_speech_prob": 9.080104064196348e-06}, {"id": 48, "seek": 21086, "start": 226.52, "end": 232.10000000000002, "text": " And if the answer is no, then you can conclude that the algorithm has high variance because", "tokens": [50364, 281, 11, 597, 727, 312, 1952, 1496, 3389, 13, 50571, 50571, 2381, 309, 775, 731, 322, 264, 3097, 992, 11, 370, 264, 1867, 281, 300, 1168, 307, 2086, 11, 291, 576, 50767, 50767, 550, 1029, 11, 775, 309, 360, 731, 322, 264, 3278, 24071, 992, 30, 50935, 50935, 682, 661, 2283, 11, 775, 309, 362, 1090, 21977, 30, 51147, 51147, 400, 498, 264, 1867, 307, 572, 11, 550, 291, 393, 16886, 300, 264, 9284, 575, 1090, 21977, 570, 51426, 51426, 309, 775, 731, 322, 264, 3097, 992, 11, 775, 406, 360, 731, 322, 264, 3278, 24071, 992, 13, 51601, 51601], "temperature": 0.0, "avg_logprob": -0.12326950185439166, "compression_ratio": 1.995049504950495, "no_speech_prob": 9.080104064196348e-06}, {"id": 49, "seek": 21086, "start": 232.10000000000002, "end": 235.60000000000002, "text": " it does well on the training set, does not do well on the cross validation set.", "tokens": [50364, 281, 11, 597, 727, 312, 1952, 1496, 3389, 13, 50571, 50571, 2381, 309, 775, 731, 322, 264, 3097, 992, 11, 370, 264, 1867, 281, 300, 1168, 307, 2086, 11, 291, 576, 50767, 50767, 550, 1029, 11, 775, 309, 360, 731, 322, 264, 3278, 24071, 992, 30, 50935, 50935, 682, 661, 2283, 11, 775, 309, 362, 1090, 21977, 30, 51147, 51147, 400, 498, 264, 1867, 307, 572, 11, 550, 291, 393, 16886, 300, 264, 9284, 575, 1090, 21977, 570, 51426, 51426, 309, 775, 731, 322, 264, 3097, 992, 11, 775, 406, 360, 731, 322, 264, 3278, 24071, 992, 13, 51601, 51601], "temperature": 0.0, "avg_logprob": -0.12326950185439166, "compression_ratio": 1.995049504950495, "no_speech_prob": 9.080104064196348e-06}, {"id": 50, "seek": 23560, "start": 235.6, "end": 243.0, "text": " So that big gap in JCV and JTrain indicates you probably have a high variance problem.", "tokens": [50364, 407, 300, 955, 7417, 294, 49802, 53, 293, 508, 51, 7146, 16203, 291, 1391, 362, 257, 1090, 21977, 1154, 13, 50734, 50734, 400, 498, 291, 362, 257, 1090, 21977, 1154, 11, 550, 472, 636, 281, 853, 281, 3191, 309, 307, 281, 483, 544, 1412, 13, 50960, 50960, 407, 291, 483, 544, 1412, 293, 352, 646, 293, 1533, 7146, 264, 2316, 293, 445, 3834, 1520, 300, 309, 775, 51236, 51236, 731, 322, 264, 3097, 992, 13, 51304, 51304, 759, 406, 11, 362, 257, 3801, 3209, 420, 498, 309, 775, 11, 536, 498, 309, 775, 731, 322, 264, 3278, 24071, 51506, 51506, 992, 293, 498, 406, 11, 483, 544, 1412, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.15436690676528797, "compression_ratio": 1.831858407079646, "no_speech_prob": 5.53908137135295e-07}, {"id": 51, "seek": 23560, "start": 243.0, "end": 247.51999999999998, "text": " And if you have a high variance problem, then one way to try to fix it is to get more data.", "tokens": [50364, 407, 300, 955, 7417, 294, 49802, 53, 293, 508, 51, 7146, 16203, 291, 1391, 362, 257, 1090, 21977, 1154, 13, 50734, 50734, 400, 498, 291, 362, 257, 1090, 21977, 1154, 11, 550, 472, 636, 281, 853, 281, 3191, 309, 307, 281, 483, 544, 1412, 13, 50960, 50960, 407, 291, 483, 544, 1412, 293, 352, 646, 293, 1533, 7146, 264, 2316, 293, 445, 3834, 1520, 300, 309, 775, 51236, 51236, 731, 322, 264, 3097, 992, 13, 51304, 51304, 759, 406, 11, 362, 257, 3801, 3209, 420, 498, 309, 775, 11, 536, 498, 309, 775, 731, 322, 264, 3278, 24071, 51506, 51506, 992, 293, 498, 406, 11, 483, 544, 1412, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.15436690676528797, "compression_ratio": 1.831858407079646, "no_speech_prob": 5.53908137135295e-07}, {"id": 52, "seek": 23560, "start": 247.51999999999998, "end": 253.04, "text": " So you get more data and go back and retrain the model and just double check that it does", "tokens": [50364, 407, 300, 955, 7417, 294, 49802, 53, 293, 508, 51, 7146, 16203, 291, 1391, 362, 257, 1090, 21977, 1154, 13, 50734, 50734, 400, 498, 291, 362, 257, 1090, 21977, 1154, 11, 550, 472, 636, 281, 853, 281, 3191, 309, 307, 281, 483, 544, 1412, 13, 50960, 50960, 407, 291, 483, 544, 1412, 293, 352, 646, 293, 1533, 7146, 264, 2316, 293, 445, 3834, 1520, 300, 309, 775, 51236, 51236, 731, 322, 264, 3097, 992, 13, 51304, 51304, 759, 406, 11, 362, 257, 3801, 3209, 420, 498, 309, 775, 11, 536, 498, 309, 775, 731, 322, 264, 3278, 24071, 51506, 51506, 992, 293, 498, 406, 11, 483, 544, 1412, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.15436690676528797, "compression_ratio": 1.831858407079646, "no_speech_prob": 5.53908137135295e-07}, {"id": 53, "seek": 23560, "start": 253.04, "end": 254.4, "text": " well on the training set.", "tokens": [50364, 407, 300, 955, 7417, 294, 49802, 53, 293, 508, 51, 7146, 16203, 291, 1391, 362, 257, 1090, 21977, 1154, 13, 50734, 50734, 400, 498, 291, 362, 257, 1090, 21977, 1154, 11, 550, 472, 636, 281, 853, 281, 3191, 309, 307, 281, 483, 544, 1412, 13, 50960, 50960, 407, 291, 483, 544, 1412, 293, 352, 646, 293, 1533, 7146, 264, 2316, 293, 445, 3834, 1520, 300, 309, 775, 51236, 51236, 731, 322, 264, 3097, 992, 13, 51304, 51304, 759, 406, 11, 362, 257, 3801, 3209, 420, 498, 309, 775, 11, 536, 498, 309, 775, 731, 322, 264, 3278, 24071, 51506, 51506, 992, 293, 498, 406, 11, 483, 544, 1412, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.15436690676528797, "compression_ratio": 1.831858407079646, "no_speech_prob": 5.53908137135295e-07}, {"id": 54, "seek": 23560, "start": 254.4, "end": 258.44, "text": " If not, have a bigger network or if it does, see if it does well on the cross validation", "tokens": [50364, 407, 300, 955, 7417, 294, 49802, 53, 293, 508, 51, 7146, 16203, 291, 1391, 362, 257, 1090, 21977, 1154, 13, 50734, 50734, 400, 498, 291, 362, 257, 1090, 21977, 1154, 11, 550, 472, 636, 281, 853, 281, 3191, 309, 307, 281, 483, 544, 1412, 13, 50960, 50960, 407, 291, 483, 544, 1412, 293, 352, 646, 293, 1533, 7146, 264, 2316, 293, 445, 3834, 1520, 300, 309, 775, 51236, 51236, 731, 322, 264, 3097, 992, 13, 51304, 51304, 759, 406, 11, 362, 257, 3801, 3209, 420, 498, 309, 775, 11, 536, 498, 309, 775, 731, 322, 264, 3278, 24071, 51506, 51506, 992, 293, 498, 406, 11, 483, 544, 1412, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.15436690676528797, "compression_ratio": 1.831858407079646, "no_speech_prob": 5.53908137135295e-07}, {"id": 55, "seek": 23560, "start": 258.44, "end": 261.6, "text": " set and if not, get more data.", "tokens": [50364, 407, 300, 955, 7417, 294, 49802, 53, 293, 508, 51, 7146, 16203, 291, 1391, 362, 257, 1090, 21977, 1154, 13, 50734, 50734, 400, 498, 291, 362, 257, 1090, 21977, 1154, 11, 550, 472, 636, 281, 853, 281, 3191, 309, 307, 281, 483, 544, 1412, 13, 50960, 50960, 407, 291, 483, 544, 1412, 293, 352, 646, 293, 1533, 7146, 264, 2316, 293, 445, 3834, 1520, 300, 309, 775, 51236, 51236, 731, 322, 264, 3097, 992, 13, 51304, 51304, 759, 406, 11, 362, 257, 3801, 3209, 420, 498, 309, 775, 11, 536, 498, 309, 775, 731, 322, 264, 3278, 24071, 51506, 51506, 992, 293, 498, 406, 11, 483, 544, 1412, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.15436690676528797, "compression_ratio": 1.831858407079646, "no_speech_prob": 5.53908137135295e-07}, {"id": 56, "seek": 26160, "start": 261.6, "end": 265.76000000000005, "text": " Then if you can keep on going round and round and round this loop until eventually it does", "tokens": [50364, 1396, 498, 291, 393, 1066, 322, 516, 3098, 293, 3098, 293, 3098, 341, 6367, 1826, 4728, 309, 775, 50572, 50572, 731, 322, 264, 3278, 24071, 992, 11, 550, 291, 434, 1391, 1096, 570, 586, 291, 362, 257, 2316, 50864, 50864, 300, 775, 731, 322, 264, 3278, 24071, 992, 293, 4696, 486, 611, 2674, 1125, 281, 777, 51138, 51138, 5110, 382, 731, 13, 51230, 51230, 823, 295, 1164, 456, 366, 15705, 281, 264, 3861, 295, 341, 6782, 13, 51466, 51466, 20620, 257, 3801, 18161, 3209, 775, 5407, 12577, 11, 457, 412, 512, 935, 309, 775, 483, 24903, 379, 51710, 51710, 5124, 13, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.15041186923072453, "compression_ratio": 1.746212121212121, "no_speech_prob": 1.2098424804207752e-06}, {"id": 57, "seek": 26160, "start": 265.76000000000005, "end": 271.6, "text": " well on the cross validation set, then you're probably done because now you have a model", "tokens": [50364, 1396, 498, 291, 393, 1066, 322, 516, 3098, 293, 3098, 293, 3098, 341, 6367, 1826, 4728, 309, 775, 50572, 50572, 731, 322, 264, 3278, 24071, 992, 11, 550, 291, 434, 1391, 1096, 570, 586, 291, 362, 257, 2316, 50864, 50864, 300, 775, 731, 322, 264, 3278, 24071, 992, 293, 4696, 486, 611, 2674, 1125, 281, 777, 51138, 51138, 5110, 382, 731, 13, 51230, 51230, 823, 295, 1164, 456, 366, 15705, 281, 264, 3861, 295, 341, 6782, 13, 51466, 51466, 20620, 257, 3801, 18161, 3209, 775, 5407, 12577, 11, 457, 412, 512, 935, 309, 775, 483, 24903, 379, 51710, 51710, 5124, 13, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.15041186923072453, "compression_ratio": 1.746212121212121, "no_speech_prob": 1.2098424804207752e-06}, {"id": 58, "seek": 26160, "start": 271.6, "end": 277.08000000000004, "text": " that does well on the cross validation set and hopefully will also generalize to new", "tokens": [50364, 1396, 498, 291, 393, 1066, 322, 516, 3098, 293, 3098, 293, 3098, 341, 6367, 1826, 4728, 309, 775, 50572, 50572, 731, 322, 264, 3278, 24071, 992, 11, 550, 291, 434, 1391, 1096, 570, 586, 291, 362, 257, 2316, 50864, 50864, 300, 775, 731, 322, 264, 3278, 24071, 992, 293, 4696, 486, 611, 2674, 1125, 281, 777, 51138, 51138, 5110, 382, 731, 13, 51230, 51230, 823, 295, 1164, 456, 366, 15705, 281, 264, 3861, 295, 341, 6782, 13, 51466, 51466, 20620, 257, 3801, 18161, 3209, 775, 5407, 12577, 11, 457, 412, 512, 935, 309, 775, 483, 24903, 379, 51710, 51710, 5124, 13, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.15041186923072453, "compression_ratio": 1.746212121212121, "no_speech_prob": 1.2098424804207752e-06}, {"id": 59, "seek": 26160, "start": 277.08000000000004, "end": 278.92, "text": " examples as well.", "tokens": [50364, 1396, 498, 291, 393, 1066, 322, 516, 3098, 293, 3098, 293, 3098, 341, 6367, 1826, 4728, 309, 775, 50572, 50572, 731, 322, 264, 3278, 24071, 992, 11, 550, 291, 434, 1391, 1096, 570, 586, 291, 362, 257, 2316, 50864, 50864, 300, 775, 731, 322, 264, 3278, 24071, 992, 293, 4696, 486, 611, 2674, 1125, 281, 777, 51138, 51138, 5110, 382, 731, 13, 51230, 51230, 823, 295, 1164, 456, 366, 15705, 281, 264, 3861, 295, 341, 6782, 13, 51466, 51466, 20620, 257, 3801, 18161, 3209, 775, 5407, 12577, 11, 457, 412, 512, 935, 309, 775, 483, 24903, 379, 51710, 51710, 5124, 13, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.15041186923072453, "compression_ratio": 1.746212121212121, "no_speech_prob": 1.2098424804207752e-06}, {"id": 60, "seek": 26160, "start": 278.92, "end": 283.64000000000004, "text": " Now of course there are limitations to the application of this recipe.", "tokens": [50364, 1396, 498, 291, 393, 1066, 322, 516, 3098, 293, 3098, 293, 3098, 341, 6367, 1826, 4728, 309, 775, 50572, 50572, 731, 322, 264, 3278, 24071, 992, 11, 550, 291, 434, 1391, 1096, 570, 586, 291, 362, 257, 2316, 50864, 50864, 300, 775, 731, 322, 264, 3278, 24071, 992, 293, 4696, 486, 611, 2674, 1125, 281, 777, 51138, 51138, 5110, 382, 731, 13, 51230, 51230, 823, 295, 1164, 456, 366, 15705, 281, 264, 3861, 295, 341, 6782, 13, 51466, 51466, 20620, 257, 3801, 18161, 3209, 775, 5407, 12577, 11, 457, 412, 512, 935, 309, 775, 483, 24903, 379, 51710, 51710, 5124, 13, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.15041186923072453, "compression_ratio": 1.746212121212121, "no_speech_prob": 1.2098424804207752e-06}, {"id": 61, "seek": 26160, "start": 283.64000000000004, "end": 288.52000000000004, "text": " Training a bigger neural network does reduce bias, but at some point it does get computationally", "tokens": [50364, 1396, 498, 291, 393, 1066, 322, 516, 3098, 293, 3098, 293, 3098, 341, 6367, 1826, 4728, 309, 775, 50572, 50572, 731, 322, 264, 3278, 24071, 992, 11, 550, 291, 434, 1391, 1096, 570, 586, 291, 362, 257, 2316, 50864, 50864, 300, 775, 731, 322, 264, 3278, 24071, 992, 293, 4696, 486, 611, 2674, 1125, 281, 777, 51138, 51138, 5110, 382, 731, 13, 51230, 51230, 823, 295, 1164, 456, 366, 15705, 281, 264, 3861, 295, 341, 6782, 13, 51466, 51466, 20620, 257, 3801, 18161, 3209, 775, 5407, 12577, 11, 457, 412, 512, 935, 309, 775, 483, 24903, 379, 51710, 51710, 5124, 13, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.15041186923072453, "compression_ratio": 1.746212121212121, "no_speech_prob": 1.2098424804207752e-06}, {"id": 62, "seek": 26160, "start": 288.52000000000004, "end": 289.92, "text": " expensive.", "tokens": [50364, 1396, 498, 291, 393, 1066, 322, 516, 3098, 293, 3098, 293, 3098, 341, 6367, 1826, 4728, 309, 775, 50572, 50572, 731, 322, 264, 3278, 24071, 992, 11, 550, 291, 434, 1391, 1096, 570, 586, 291, 362, 257, 2316, 50864, 50864, 300, 775, 731, 322, 264, 3278, 24071, 992, 293, 4696, 486, 611, 2674, 1125, 281, 777, 51138, 51138, 5110, 382, 731, 13, 51230, 51230, 823, 295, 1164, 456, 366, 15705, 281, 264, 3861, 295, 341, 6782, 13, 51466, 51466, 20620, 257, 3801, 18161, 3209, 775, 5407, 12577, 11, 457, 412, 512, 935, 309, 775, 483, 24903, 379, 51710, 51710, 5124, 13, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.15041186923072453, "compression_ratio": 1.746212121212121, "no_speech_prob": 1.2098424804207752e-06}, {"id": 63, "seek": 28992, "start": 289.92, "end": 296.0, "text": " That's why the rise of neural networks has been really assisted by the rise of very fast", "tokens": [50364, 663, 311, 983, 264, 6272, 295, 18161, 9590, 575, 668, 534, 30291, 538, 264, 6272, 295, 588, 2370, 50668, 50668, 10807, 11, 3009, 2318, 18407, 82, 420, 11837, 15321, 6815, 13, 50964, 50964, 11817, 3039, 19067, 1143, 281, 3073, 493, 3820, 11837, 11, 457, 300, 4523, 484, 575, 668, 588, 51174, 51174, 4420, 337, 35593, 493, 18161, 9590, 382, 731, 13, 51312, 51312, 583, 754, 365, 8837, 10172, 3391, 11, 4399, 257, 1629, 935, 11, 264, 18161, 9590, 366, 920, 51534, 51534, 2416, 293, 747, 370, 938, 281, 3847, 11, 309, 3643, 1536, 68, 296, 964, 13, 51717, 51717], "temperature": 0.0, "avg_logprob": -0.1441278552064801, "compression_ratio": 1.7093023255813953, "no_speech_prob": 1.0289091733284295e-05}, {"id": 64, "seek": 28992, "start": 296.0, "end": 301.92, "text": " computers, including especially GPUs or graphics processor units.", "tokens": [50364, 663, 311, 983, 264, 6272, 295, 18161, 9590, 575, 668, 534, 30291, 538, 264, 6272, 295, 588, 2370, 50668, 50668, 10807, 11, 3009, 2318, 18407, 82, 420, 11837, 15321, 6815, 13, 50964, 50964, 11817, 3039, 19067, 1143, 281, 3073, 493, 3820, 11837, 11, 457, 300, 4523, 484, 575, 668, 588, 51174, 51174, 4420, 337, 35593, 493, 18161, 9590, 382, 731, 13, 51312, 51312, 583, 754, 365, 8837, 10172, 3391, 11, 4399, 257, 1629, 935, 11, 264, 18161, 9590, 366, 920, 51534, 51534, 2416, 293, 747, 370, 938, 281, 3847, 11, 309, 3643, 1536, 68, 296, 964, 13, 51717, 51717], "temperature": 0.0, "avg_logprob": -0.1441278552064801, "compression_ratio": 1.7093023255813953, "no_speech_prob": 1.0289091733284295e-05}, {"id": 65, "seek": 28992, "start": 301.92, "end": 306.12, "text": " Hardware traditionally used to speed up computer graphics, but that turns out has been very", "tokens": [50364, 663, 311, 983, 264, 6272, 295, 18161, 9590, 575, 668, 534, 30291, 538, 264, 6272, 295, 588, 2370, 50668, 50668, 10807, 11, 3009, 2318, 18407, 82, 420, 11837, 15321, 6815, 13, 50964, 50964, 11817, 3039, 19067, 1143, 281, 3073, 493, 3820, 11837, 11, 457, 300, 4523, 484, 575, 668, 588, 51174, 51174, 4420, 337, 35593, 493, 18161, 9590, 382, 731, 13, 51312, 51312, 583, 754, 365, 8837, 10172, 3391, 11, 4399, 257, 1629, 935, 11, 264, 18161, 9590, 366, 920, 51534, 51534, 2416, 293, 747, 370, 938, 281, 3847, 11, 309, 3643, 1536, 68, 296, 964, 13, 51717, 51717], "temperature": 0.0, "avg_logprob": -0.1441278552064801, "compression_ratio": 1.7093023255813953, "no_speech_prob": 1.0289091733284295e-05}, {"id": 66, "seek": 28992, "start": 306.12, "end": 308.88, "text": " useful for speeding up neural networks as well.", "tokens": [50364, 663, 311, 983, 264, 6272, 295, 18161, 9590, 575, 668, 534, 30291, 538, 264, 6272, 295, 588, 2370, 50668, 50668, 10807, 11, 3009, 2318, 18407, 82, 420, 11837, 15321, 6815, 13, 50964, 50964, 11817, 3039, 19067, 1143, 281, 3073, 493, 3820, 11837, 11, 457, 300, 4523, 484, 575, 668, 588, 51174, 51174, 4420, 337, 35593, 493, 18161, 9590, 382, 731, 13, 51312, 51312, 583, 754, 365, 8837, 10172, 3391, 11, 4399, 257, 1629, 935, 11, 264, 18161, 9590, 366, 920, 51534, 51534, 2416, 293, 747, 370, 938, 281, 3847, 11, 309, 3643, 1536, 68, 296, 964, 13, 51717, 51717], "temperature": 0.0, "avg_logprob": -0.1441278552064801, "compression_ratio": 1.7093023255813953, "no_speech_prob": 1.0289091733284295e-05}, {"id": 67, "seek": 28992, "start": 308.88, "end": 313.32, "text": " But even with hardware accelerators, beyond a certain point, the neural networks are still", "tokens": [50364, 663, 311, 983, 264, 6272, 295, 18161, 9590, 575, 668, 534, 30291, 538, 264, 6272, 295, 588, 2370, 50668, 50668, 10807, 11, 3009, 2318, 18407, 82, 420, 11837, 15321, 6815, 13, 50964, 50964, 11817, 3039, 19067, 1143, 281, 3073, 493, 3820, 11837, 11, 457, 300, 4523, 484, 575, 668, 588, 51174, 51174, 4420, 337, 35593, 493, 18161, 9590, 382, 731, 13, 51312, 51312, 583, 754, 365, 8837, 10172, 3391, 11, 4399, 257, 1629, 935, 11, 264, 18161, 9590, 366, 920, 51534, 51534, 2416, 293, 747, 370, 938, 281, 3847, 11, 309, 3643, 1536, 68, 296, 964, 13, 51717, 51717], "temperature": 0.0, "avg_logprob": -0.1441278552064801, "compression_ratio": 1.7093023255813953, "no_speech_prob": 1.0289091733284295e-05}, {"id": 68, "seek": 28992, "start": 313.32, "end": 316.98, "text": " large and take so long to train, it becomes infeasible.", "tokens": [50364, 663, 311, 983, 264, 6272, 295, 18161, 9590, 575, 668, 534, 30291, 538, 264, 6272, 295, 588, 2370, 50668, 50668, 10807, 11, 3009, 2318, 18407, 82, 420, 11837, 15321, 6815, 13, 50964, 50964, 11817, 3039, 19067, 1143, 281, 3073, 493, 3820, 11837, 11, 457, 300, 4523, 484, 575, 668, 588, 51174, 51174, 4420, 337, 35593, 493, 18161, 9590, 382, 731, 13, 51312, 51312, 583, 754, 365, 8837, 10172, 3391, 11, 4399, 257, 1629, 935, 11, 264, 18161, 9590, 366, 920, 51534, 51534, 2416, 293, 747, 370, 938, 281, 3847, 11, 309, 3643, 1536, 68, 296, 964, 13, 51717, 51717], "temperature": 0.0, "avg_logprob": -0.1441278552064801, "compression_ratio": 1.7093023255813953, "no_speech_prob": 1.0289091733284295e-05}, {"id": 69, "seek": 31698, "start": 316.98, "end": 321.28000000000003, "text": " And then of course the other limitation is more data.", "tokens": [50364, 400, 550, 295, 1164, 264, 661, 27432, 307, 544, 1412, 13, 50579, 50579, 4803, 291, 393, 787, 483, 370, 709, 1412, 293, 4399, 257, 1629, 935, 11, 309, 311, 1152, 281, 483, 709, 50821, 50821, 544, 1412, 13, 50871, 50871, 583, 286, 519, 341, 6782, 13948, 257, 688, 295, 264, 6272, 295, 2452, 2539, 294, 264, 1036, 2940, 51189, 51189, 924, 11, 597, 307, 337, 5821, 689, 291, 360, 362, 2105, 281, 257, 688, 295, 1412, 13, 51429, 51429, 1396, 885, 1075, 281, 3847, 2416, 18161, 9590, 4045, 291, 281, 4728, 483, 1238, 665, 3389, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.10634018676449554, "compression_ratio": 1.6264591439688716, "no_speech_prob": 7.338060754591424e-07}, {"id": 70, "seek": 31698, "start": 321.28000000000003, "end": 326.12, "text": " Sometimes you can only get so much data and beyond a certain point, it's hard to get much", "tokens": [50364, 400, 550, 295, 1164, 264, 661, 27432, 307, 544, 1412, 13, 50579, 50579, 4803, 291, 393, 787, 483, 370, 709, 1412, 293, 4399, 257, 1629, 935, 11, 309, 311, 1152, 281, 483, 709, 50821, 50821, 544, 1412, 13, 50871, 50871, 583, 286, 519, 341, 6782, 13948, 257, 688, 295, 264, 6272, 295, 2452, 2539, 294, 264, 1036, 2940, 51189, 51189, 924, 11, 597, 307, 337, 5821, 689, 291, 360, 362, 2105, 281, 257, 688, 295, 1412, 13, 51429, 51429, 1396, 885, 1075, 281, 3847, 2416, 18161, 9590, 4045, 291, 281, 4728, 483, 1238, 665, 3389, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.10634018676449554, "compression_ratio": 1.6264591439688716, "no_speech_prob": 7.338060754591424e-07}, {"id": 71, "seek": 31698, "start": 326.12, "end": 327.12, "text": " more data.", "tokens": [50364, 400, 550, 295, 1164, 264, 661, 27432, 307, 544, 1412, 13, 50579, 50579, 4803, 291, 393, 787, 483, 370, 709, 1412, 293, 4399, 257, 1629, 935, 11, 309, 311, 1152, 281, 483, 709, 50821, 50821, 544, 1412, 13, 50871, 50871, 583, 286, 519, 341, 6782, 13948, 257, 688, 295, 264, 6272, 295, 2452, 2539, 294, 264, 1036, 2940, 51189, 51189, 924, 11, 597, 307, 337, 5821, 689, 291, 360, 362, 2105, 281, 257, 688, 295, 1412, 13, 51429, 51429, 1396, 885, 1075, 281, 3847, 2416, 18161, 9590, 4045, 291, 281, 4728, 483, 1238, 665, 3389, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.10634018676449554, "compression_ratio": 1.6264591439688716, "no_speech_prob": 7.338060754591424e-07}, {"id": 72, "seek": 31698, "start": 327.12, "end": 333.48, "text": " But I think this recipe explains a lot of the rise of deep learning in the last several", "tokens": [50364, 400, 550, 295, 1164, 264, 661, 27432, 307, 544, 1412, 13, 50579, 50579, 4803, 291, 393, 787, 483, 370, 709, 1412, 293, 4399, 257, 1629, 935, 11, 309, 311, 1152, 281, 483, 709, 50821, 50821, 544, 1412, 13, 50871, 50871, 583, 286, 519, 341, 6782, 13948, 257, 688, 295, 264, 6272, 295, 2452, 2539, 294, 264, 1036, 2940, 51189, 51189, 924, 11, 597, 307, 337, 5821, 689, 291, 360, 362, 2105, 281, 257, 688, 295, 1412, 13, 51429, 51429, 1396, 885, 1075, 281, 3847, 2416, 18161, 9590, 4045, 291, 281, 4728, 483, 1238, 665, 3389, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.10634018676449554, "compression_ratio": 1.6264591439688716, "no_speech_prob": 7.338060754591424e-07}, {"id": 73, "seek": 31698, "start": 333.48, "end": 338.28000000000003, "text": " years, which is for applications where you do have access to a lot of data.", "tokens": [50364, 400, 550, 295, 1164, 264, 661, 27432, 307, 544, 1412, 13, 50579, 50579, 4803, 291, 393, 787, 483, 370, 709, 1412, 293, 4399, 257, 1629, 935, 11, 309, 311, 1152, 281, 483, 709, 50821, 50821, 544, 1412, 13, 50871, 50871, 583, 286, 519, 341, 6782, 13948, 257, 688, 295, 264, 6272, 295, 2452, 2539, 294, 264, 1036, 2940, 51189, 51189, 924, 11, 597, 307, 337, 5821, 689, 291, 360, 362, 2105, 281, 257, 688, 295, 1412, 13, 51429, 51429, 1396, 885, 1075, 281, 3847, 2416, 18161, 9590, 4045, 291, 281, 4728, 483, 1238, 665, 3389, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.10634018676449554, "compression_ratio": 1.6264591439688716, "no_speech_prob": 7.338060754591424e-07}, {"id": 74, "seek": 31698, "start": 338.28000000000003, "end": 345.26, "text": " Then being able to train large neural networks allows you to eventually get pretty good performance", "tokens": [50364, 400, 550, 295, 1164, 264, 661, 27432, 307, 544, 1412, 13, 50579, 50579, 4803, 291, 393, 787, 483, 370, 709, 1412, 293, 4399, 257, 1629, 935, 11, 309, 311, 1152, 281, 483, 709, 50821, 50821, 544, 1412, 13, 50871, 50871, 583, 286, 519, 341, 6782, 13948, 257, 688, 295, 264, 6272, 295, 2452, 2539, 294, 264, 1036, 2940, 51189, 51189, 924, 11, 597, 307, 337, 5821, 689, 291, 360, 362, 2105, 281, 257, 688, 295, 1412, 13, 51429, 51429, 1396, 885, 1075, 281, 3847, 2416, 18161, 9590, 4045, 291, 281, 4728, 483, 1238, 665, 3389, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.10634018676449554, "compression_ratio": 1.6264591439688716, "no_speech_prob": 7.338060754591424e-07}, {"id": 75, "seek": 34526, "start": 345.26, "end": 347.12, "text": " on a lot of applications.", "tokens": [50364, 322, 257, 688, 295, 5821, 13, 50457, 50457, 1485, 551, 300, 390, 26947, 294, 341, 4137, 300, 815, 406, 362, 668, 6322, 307, 300, 382, 50748, 50748, 291, 434, 6416, 257, 2539, 9284, 11, 2171, 291, 915, 300, 291, 362, 1090, 12577, 11, 294, 597, 51029, 51029, 1389, 291, 360, 721, 411, 3488, 264, 18161, 3209, 13, 51155, 51155, 583, 550, 934, 291, 3488, 264, 18161, 3209, 11, 291, 815, 915, 300, 291, 362, 1090, 21977, 11, 51375, 51375, 294, 597, 1389, 291, 1062, 360, 661, 721, 411, 2500, 544, 1412, 13, 51577, 51577], "temperature": 0.0, "avg_logprob": -0.14159637689590454, "compression_ratio": 1.8675799086757991, "no_speech_prob": 5.9548128774622455e-06}, {"id": 76, "seek": 34526, "start": 347.12, "end": 352.94, "text": " One thing that was implicit in this slide that may not have been obvious is that as", "tokens": [50364, 322, 257, 688, 295, 5821, 13, 50457, 50457, 1485, 551, 300, 390, 26947, 294, 341, 4137, 300, 815, 406, 362, 668, 6322, 307, 300, 382, 50748, 50748, 291, 434, 6416, 257, 2539, 9284, 11, 2171, 291, 915, 300, 291, 362, 1090, 12577, 11, 294, 597, 51029, 51029, 1389, 291, 360, 721, 411, 3488, 264, 18161, 3209, 13, 51155, 51155, 583, 550, 934, 291, 3488, 264, 18161, 3209, 11, 291, 815, 915, 300, 291, 362, 1090, 21977, 11, 51375, 51375, 294, 597, 1389, 291, 1062, 360, 661, 721, 411, 2500, 544, 1412, 13, 51577, 51577], "temperature": 0.0, "avg_logprob": -0.14159637689590454, "compression_ratio": 1.8675799086757991, "no_speech_prob": 5.9548128774622455e-06}, {"id": 77, "seek": 34526, "start": 352.94, "end": 358.56, "text": " you're developing a learning algorithm, sometimes you find that you have high bias, in which", "tokens": [50364, 322, 257, 688, 295, 5821, 13, 50457, 50457, 1485, 551, 300, 390, 26947, 294, 341, 4137, 300, 815, 406, 362, 668, 6322, 307, 300, 382, 50748, 50748, 291, 434, 6416, 257, 2539, 9284, 11, 2171, 291, 915, 300, 291, 362, 1090, 12577, 11, 294, 597, 51029, 51029, 1389, 291, 360, 721, 411, 3488, 264, 18161, 3209, 13, 51155, 51155, 583, 550, 934, 291, 3488, 264, 18161, 3209, 11, 291, 815, 915, 300, 291, 362, 1090, 21977, 11, 51375, 51375, 294, 597, 1389, 291, 1062, 360, 661, 721, 411, 2500, 544, 1412, 13, 51577, 51577], "temperature": 0.0, "avg_logprob": -0.14159637689590454, "compression_ratio": 1.8675799086757991, "no_speech_prob": 5.9548128774622455e-06}, {"id": 78, "seek": 34526, "start": 358.56, "end": 361.08, "text": " case you do things like increase the neural network.", "tokens": [50364, 322, 257, 688, 295, 5821, 13, 50457, 50457, 1485, 551, 300, 390, 26947, 294, 341, 4137, 300, 815, 406, 362, 668, 6322, 307, 300, 382, 50748, 50748, 291, 434, 6416, 257, 2539, 9284, 11, 2171, 291, 915, 300, 291, 362, 1090, 12577, 11, 294, 597, 51029, 51029, 1389, 291, 360, 721, 411, 3488, 264, 18161, 3209, 13, 51155, 51155, 583, 550, 934, 291, 3488, 264, 18161, 3209, 11, 291, 815, 915, 300, 291, 362, 1090, 21977, 11, 51375, 51375, 294, 597, 1389, 291, 1062, 360, 661, 721, 411, 2500, 544, 1412, 13, 51577, 51577], "temperature": 0.0, "avg_logprob": -0.14159637689590454, "compression_ratio": 1.8675799086757991, "no_speech_prob": 5.9548128774622455e-06}, {"id": 79, "seek": 34526, "start": 361.08, "end": 365.48, "text": " But then after you increase the neural network, you may find that you have high variance,", "tokens": [50364, 322, 257, 688, 295, 5821, 13, 50457, 50457, 1485, 551, 300, 390, 26947, 294, 341, 4137, 300, 815, 406, 362, 668, 6322, 307, 300, 382, 50748, 50748, 291, 434, 6416, 257, 2539, 9284, 11, 2171, 291, 915, 300, 291, 362, 1090, 12577, 11, 294, 597, 51029, 51029, 1389, 291, 360, 721, 411, 3488, 264, 18161, 3209, 13, 51155, 51155, 583, 550, 934, 291, 3488, 264, 18161, 3209, 11, 291, 815, 915, 300, 291, 362, 1090, 21977, 11, 51375, 51375, 294, 597, 1389, 291, 1062, 360, 661, 721, 411, 2500, 544, 1412, 13, 51577, 51577], "temperature": 0.0, "avg_logprob": -0.14159637689590454, "compression_ratio": 1.8675799086757991, "no_speech_prob": 5.9548128774622455e-06}, {"id": 80, "seek": 34526, "start": 365.48, "end": 369.52, "text": " in which case you might do other things like collect more data.", "tokens": [50364, 322, 257, 688, 295, 5821, 13, 50457, 50457, 1485, 551, 300, 390, 26947, 294, 341, 4137, 300, 815, 406, 362, 668, 6322, 307, 300, 382, 50748, 50748, 291, 434, 6416, 257, 2539, 9284, 11, 2171, 291, 915, 300, 291, 362, 1090, 12577, 11, 294, 597, 51029, 51029, 1389, 291, 360, 721, 411, 3488, 264, 18161, 3209, 13, 51155, 51155, 583, 550, 934, 291, 3488, 264, 18161, 3209, 11, 291, 815, 915, 300, 291, 362, 1090, 21977, 11, 51375, 51375, 294, 597, 1389, 291, 1062, 360, 661, 721, 411, 2500, 544, 1412, 13, 51577, 51577], "temperature": 0.0, "avg_logprob": -0.14159637689590454, "compression_ratio": 1.8675799086757991, "no_speech_prob": 5.9548128774622455e-06}, {"id": 81, "seek": 36952, "start": 369.52, "end": 375.79999999999995, "text": " Then during the hours or days or weeks you're developing a machine learning algorithm, at", "tokens": [50364, 1396, 1830, 264, 2496, 420, 1708, 420, 3259, 291, 434, 6416, 257, 3479, 2539, 9284, 11, 412, 50678, 50678, 819, 2793, 291, 815, 362, 1090, 12577, 420, 1090, 21977, 293, 309, 393, 1319, 13, 50882, 50882, 583, 309, 311, 5413, 322, 1968, 428, 9284, 575, 1090, 12577, 420, 1090, 21977, 412, 300, 565, 51124, 51124, 300, 300, 393, 854, 976, 10056, 337, 437, 291, 820, 312, 1382, 958, 13, 51358, 51358, 1133, 291, 434, 3097, 257, 18161, 3209, 11, 472, 551, 300, 561, 362, 2351, 385, 949, 307, 11, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.11959715812436995, "compression_ratio": 1.718487394957983, "no_speech_prob": 3.1875019885774236e-06}, {"id": 82, "seek": 36952, "start": 375.79999999999995, "end": 379.88, "text": " different points you may have high bias or high variance and it can change.", "tokens": [50364, 1396, 1830, 264, 2496, 420, 1708, 420, 3259, 291, 434, 6416, 257, 3479, 2539, 9284, 11, 412, 50678, 50678, 819, 2793, 291, 815, 362, 1090, 12577, 420, 1090, 21977, 293, 309, 393, 1319, 13, 50882, 50882, 583, 309, 311, 5413, 322, 1968, 428, 9284, 575, 1090, 12577, 420, 1090, 21977, 412, 300, 565, 51124, 51124, 300, 300, 393, 854, 976, 10056, 337, 437, 291, 820, 312, 1382, 958, 13, 51358, 51358, 1133, 291, 434, 3097, 257, 18161, 3209, 11, 472, 551, 300, 561, 362, 2351, 385, 949, 307, 11, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.11959715812436995, "compression_ratio": 1.718487394957983, "no_speech_prob": 3.1875019885774236e-06}, {"id": 83, "seek": 36952, "start": 379.88, "end": 384.71999999999997, "text": " But it's depending on whether your algorithm has high bias or high variance at that time", "tokens": [50364, 1396, 1830, 264, 2496, 420, 1708, 420, 3259, 291, 434, 6416, 257, 3479, 2539, 9284, 11, 412, 50678, 50678, 819, 2793, 291, 815, 362, 1090, 12577, 420, 1090, 21977, 293, 309, 393, 1319, 13, 50882, 50882, 583, 309, 311, 5413, 322, 1968, 428, 9284, 575, 1090, 12577, 420, 1090, 21977, 412, 300, 565, 51124, 51124, 300, 300, 393, 854, 976, 10056, 337, 437, 291, 820, 312, 1382, 958, 13, 51358, 51358, 1133, 291, 434, 3097, 257, 18161, 3209, 11, 472, 551, 300, 561, 362, 2351, 385, 949, 307, 11, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.11959715812436995, "compression_ratio": 1.718487394957983, "no_speech_prob": 3.1875019885774236e-06}, {"id": 84, "seek": 36952, "start": 384.71999999999997, "end": 389.4, "text": " that that can help give guidance for what you should be trying next.", "tokens": [50364, 1396, 1830, 264, 2496, 420, 1708, 420, 3259, 291, 434, 6416, 257, 3479, 2539, 9284, 11, 412, 50678, 50678, 819, 2793, 291, 815, 362, 1090, 12577, 420, 1090, 21977, 293, 309, 393, 1319, 13, 50882, 50882, 583, 309, 311, 5413, 322, 1968, 428, 9284, 575, 1090, 12577, 420, 1090, 21977, 412, 300, 565, 51124, 51124, 300, 300, 393, 854, 976, 10056, 337, 437, 291, 820, 312, 1382, 958, 13, 51358, 51358, 1133, 291, 434, 3097, 257, 18161, 3209, 11, 472, 551, 300, 561, 362, 2351, 385, 949, 307, 11, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.11959715812436995, "compression_ratio": 1.718487394957983, "no_speech_prob": 3.1875019885774236e-06}, {"id": 85, "seek": 36952, "start": 389.4, "end": 394.28, "text": " When you're training a neural network, one thing that people have asked me before is,", "tokens": [50364, 1396, 1830, 264, 2496, 420, 1708, 420, 3259, 291, 434, 6416, 257, 3479, 2539, 9284, 11, 412, 50678, 50678, 819, 2793, 291, 815, 362, 1090, 12577, 420, 1090, 21977, 293, 309, 393, 1319, 13, 50882, 50882, 583, 309, 311, 5413, 322, 1968, 428, 9284, 575, 1090, 12577, 420, 1090, 21977, 412, 300, 565, 51124, 51124, 300, 300, 393, 854, 976, 10056, 337, 437, 291, 820, 312, 1382, 958, 13, 51358, 51358, 1133, 291, 434, 3097, 257, 18161, 3209, 11, 472, 551, 300, 561, 362, 2351, 385, 949, 307, 11, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.11959715812436995, "compression_ratio": 1.718487394957983, "no_speech_prob": 3.1875019885774236e-06}, {"id": 86, "seek": 39428, "start": 394.28, "end": 401.52, "text": " hey Andrew, whether my neural network is too big, will that create a high variance problem?", "tokens": [50364, 4177, 10110, 11, 1968, 452, 18161, 3209, 307, 886, 955, 11, 486, 300, 1884, 257, 1090, 21977, 1154, 30, 50726, 50726, 467, 4523, 484, 300, 257, 2416, 18161, 3209, 365, 731, 12, 339, 6441, 3890, 2144, 486, 2673, 360, 51080, 51080, 382, 731, 420, 1101, 813, 257, 4356, 472, 13, 51242, 51242, 400, 370, 11, 337, 1365, 11, 498, 291, 362, 257, 1359, 18161, 3209, 411, 341, 293, 291, 645, 281, 3679, 281, 51597, 51597], "temperature": 0.0, "avg_logprob": -0.12677778516496932, "compression_ratio": 1.597938144329897, "no_speech_prob": 2.2603014713240555e-06}, {"id": 87, "seek": 39428, "start": 401.52, "end": 408.59999999999997, "text": " It turns out that a large neural network with well-chosen regularization will usually do", "tokens": [50364, 4177, 10110, 11, 1968, 452, 18161, 3209, 307, 886, 955, 11, 486, 300, 1884, 257, 1090, 21977, 1154, 30, 50726, 50726, 467, 4523, 484, 300, 257, 2416, 18161, 3209, 365, 731, 12, 339, 6441, 3890, 2144, 486, 2673, 360, 51080, 51080, 382, 731, 420, 1101, 813, 257, 4356, 472, 13, 51242, 51242, 400, 370, 11, 337, 1365, 11, 498, 291, 362, 257, 1359, 18161, 3209, 411, 341, 293, 291, 645, 281, 3679, 281, 51597, 51597], "temperature": 0.0, "avg_logprob": -0.12677778516496932, "compression_ratio": 1.597938144329897, "no_speech_prob": 2.2603014713240555e-06}, {"id": 88, "seek": 39428, "start": 408.59999999999997, "end": 411.84, "text": " as well or better than a smaller one.", "tokens": [50364, 4177, 10110, 11, 1968, 452, 18161, 3209, 307, 886, 955, 11, 486, 300, 1884, 257, 1090, 21977, 1154, 30, 50726, 50726, 467, 4523, 484, 300, 257, 2416, 18161, 3209, 365, 731, 12, 339, 6441, 3890, 2144, 486, 2673, 360, 51080, 51080, 382, 731, 420, 1101, 813, 257, 4356, 472, 13, 51242, 51242, 400, 370, 11, 337, 1365, 11, 498, 291, 362, 257, 1359, 18161, 3209, 411, 341, 293, 291, 645, 281, 3679, 281, 51597, 51597], "temperature": 0.0, "avg_logprob": -0.12677778516496932, "compression_ratio": 1.597938144329897, "no_speech_prob": 2.2603014713240555e-06}, {"id": 89, "seek": 39428, "start": 411.84, "end": 418.94, "text": " And so, for example, if you have a small neural network like this and you were to switch to", "tokens": [50364, 4177, 10110, 11, 1968, 452, 18161, 3209, 307, 886, 955, 11, 486, 300, 1884, 257, 1090, 21977, 1154, 30, 50726, 50726, 467, 4523, 484, 300, 257, 2416, 18161, 3209, 365, 731, 12, 339, 6441, 3890, 2144, 486, 2673, 360, 51080, 51080, 382, 731, 420, 1101, 813, 257, 4356, 472, 13, 51242, 51242, 400, 370, 11, 337, 1365, 11, 498, 291, 362, 257, 1359, 18161, 3209, 411, 341, 293, 291, 645, 281, 3679, 281, 51597, 51597], "temperature": 0.0, "avg_logprob": -0.12677778516496932, "compression_ratio": 1.597938144329897, "no_speech_prob": 2.2603014713240555e-06}, {"id": 90, "seek": 41894, "start": 418.94, "end": 424.7, "text": " a much larger neural network like this, you would think that the risk of overfitting goes", "tokens": [50364, 257, 709, 4833, 18161, 3209, 411, 341, 11, 291, 576, 519, 300, 264, 3148, 295, 670, 69, 2414, 1709, 50652, 50652, 493, 10591, 13, 50752, 50752, 583, 309, 4523, 484, 300, 498, 291, 645, 281, 3890, 1125, 341, 4833, 18161, 3209, 23505, 11, 51035, 51035, 550, 341, 4833, 18161, 3209, 2673, 486, 360, 412, 1935, 382, 731, 420, 1101, 813, 264, 4356, 51329, 51329, 472, 11, 370, 938, 382, 264, 3890, 2144, 307, 8614, 23505, 13, 51545, 51545, 407, 1071, 636, 295, 1566, 341, 307, 300, 309, 1920, 1128, 11051, 281, 352, 281, 257, 4833, 18161, 3209, 51823, 51823], "temperature": 0.0, "avg_logprob": -0.08865828561310721, "compression_ratio": 1.8649789029535866, "no_speech_prob": 6.643313099630177e-06}, {"id": 91, "seek": 41894, "start": 424.7, "end": 426.7, "text": " up significantly.", "tokens": [50364, 257, 709, 4833, 18161, 3209, 411, 341, 11, 291, 576, 519, 300, 264, 3148, 295, 670, 69, 2414, 1709, 50652, 50652, 493, 10591, 13, 50752, 50752, 583, 309, 4523, 484, 300, 498, 291, 645, 281, 3890, 1125, 341, 4833, 18161, 3209, 23505, 11, 51035, 51035, 550, 341, 4833, 18161, 3209, 2673, 486, 360, 412, 1935, 382, 731, 420, 1101, 813, 264, 4356, 51329, 51329, 472, 11, 370, 938, 382, 264, 3890, 2144, 307, 8614, 23505, 13, 51545, 51545, 407, 1071, 636, 295, 1566, 341, 307, 300, 309, 1920, 1128, 11051, 281, 352, 281, 257, 4833, 18161, 3209, 51823, 51823], "temperature": 0.0, "avg_logprob": -0.08865828561310721, "compression_ratio": 1.8649789029535866, "no_speech_prob": 6.643313099630177e-06}, {"id": 92, "seek": 41894, "start": 426.7, "end": 432.36, "text": " But it turns out that if you were to regularize this larger neural network appropriately,", "tokens": [50364, 257, 709, 4833, 18161, 3209, 411, 341, 11, 291, 576, 519, 300, 264, 3148, 295, 670, 69, 2414, 1709, 50652, 50652, 493, 10591, 13, 50752, 50752, 583, 309, 4523, 484, 300, 498, 291, 645, 281, 3890, 1125, 341, 4833, 18161, 3209, 23505, 11, 51035, 51035, 550, 341, 4833, 18161, 3209, 2673, 486, 360, 412, 1935, 382, 731, 420, 1101, 813, 264, 4356, 51329, 51329, 472, 11, 370, 938, 382, 264, 3890, 2144, 307, 8614, 23505, 13, 51545, 51545, 407, 1071, 636, 295, 1566, 341, 307, 300, 309, 1920, 1128, 11051, 281, 352, 281, 257, 4833, 18161, 3209, 51823, 51823], "temperature": 0.0, "avg_logprob": -0.08865828561310721, "compression_ratio": 1.8649789029535866, "no_speech_prob": 6.643313099630177e-06}, {"id": 93, "seek": 41894, "start": 432.36, "end": 438.24, "text": " then this larger neural network usually will do at least as well or better than the smaller", "tokens": [50364, 257, 709, 4833, 18161, 3209, 411, 341, 11, 291, 576, 519, 300, 264, 3148, 295, 670, 69, 2414, 1709, 50652, 50652, 493, 10591, 13, 50752, 50752, 583, 309, 4523, 484, 300, 498, 291, 645, 281, 3890, 1125, 341, 4833, 18161, 3209, 23505, 11, 51035, 51035, 550, 341, 4833, 18161, 3209, 2673, 486, 360, 412, 1935, 382, 731, 420, 1101, 813, 264, 4356, 51329, 51329, 472, 11, 370, 938, 382, 264, 3890, 2144, 307, 8614, 23505, 13, 51545, 51545, 407, 1071, 636, 295, 1566, 341, 307, 300, 309, 1920, 1128, 11051, 281, 352, 281, 257, 4833, 18161, 3209, 51823, 51823], "temperature": 0.0, "avg_logprob": -0.08865828561310721, "compression_ratio": 1.8649789029535866, "no_speech_prob": 6.643313099630177e-06}, {"id": 94, "seek": 41894, "start": 438.24, "end": 442.56, "text": " one, so long as the regularization is chosen appropriately.", "tokens": [50364, 257, 709, 4833, 18161, 3209, 411, 341, 11, 291, 576, 519, 300, 264, 3148, 295, 670, 69, 2414, 1709, 50652, 50652, 493, 10591, 13, 50752, 50752, 583, 309, 4523, 484, 300, 498, 291, 645, 281, 3890, 1125, 341, 4833, 18161, 3209, 23505, 11, 51035, 51035, 550, 341, 4833, 18161, 3209, 2673, 486, 360, 412, 1935, 382, 731, 420, 1101, 813, 264, 4356, 51329, 51329, 472, 11, 370, 938, 382, 264, 3890, 2144, 307, 8614, 23505, 13, 51545, 51545, 407, 1071, 636, 295, 1566, 341, 307, 300, 309, 1920, 1128, 11051, 281, 352, 281, 257, 4833, 18161, 3209, 51823, 51823], "temperature": 0.0, "avg_logprob": -0.08865828561310721, "compression_ratio": 1.8649789029535866, "no_speech_prob": 6.643313099630177e-06}, {"id": 95, "seek": 41894, "start": 442.56, "end": 448.12, "text": " So another way of saying this is that it almost never hurts to go to a larger neural network", "tokens": [50364, 257, 709, 4833, 18161, 3209, 411, 341, 11, 291, 576, 519, 300, 264, 3148, 295, 670, 69, 2414, 1709, 50652, 50652, 493, 10591, 13, 50752, 50752, 583, 309, 4523, 484, 300, 498, 291, 645, 281, 3890, 1125, 341, 4833, 18161, 3209, 23505, 11, 51035, 51035, 550, 341, 4833, 18161, 3209, 2673, 486, 360, 412, 1935, 382, 731, 420, 1101, 813, 264, 4356, 51329, 51329, 472, 11, 370, 938, 382, 264, 3890, 2144, 307, 8614, 23505, 13, 51545, 51545, 407, 1071, 636, 295, 1566, 341, 307, 300, 309, 1920, 1128, 11051, 281, 352, 281, 257, 4833, 18161, 3209, 51823, 51823], "temperature": 0.0, "avg_logprob": -0.08865828561310721, "compression_ratio": 1.8649789029535866, "no_speech_prob": 6.643313099630177e-06}, {"id": 96, "seek": 44812, "start": 448.12, "end": 450.98, "text": " so long as you regularize appropriately.", "tokens": [50364, 370, 938, 382, 291, 3890, 1125, 23505, 13, 50507, 50507, 2022, 472, 43012, 11, 597, 307, 300, 562, 291, 3847, 257, 4833, 18161, 3209, 11, 309, 775, 1813, 544, 50758, 50758, 24903, 379, 5124, 13, 50842, 50842, 407, 264, 2135, 636, 309, 11051, 307, 309, 486, 2964, 760, 428, 3097, 293, 428, 38253, 1399, 13, 51112, 51112, 400, 588, 10515, 11, 281, 3890, 1125, 257, 18161, 3209, 11, 341, 307, 437, 291, 360, 13, 51434, 51434, 759, 264, 2063, 2445, 337, 428, 18161, 3209, 307, 264, 4274, 4470, 11, 293, 370, 264, 4470, 510, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.10844171290494958, "compression_ratio": 1.6903765690376569, "no_speech_prob": 5.12286703724385e-07}, {"id": 97, "seek": 44812, "start": 450.98, "end": 456.0, "text": " With one caveat, which is that when you train a larger neural network, it does become more", "tokens": [50364, 370, 938, 382, 291, 3890, 1125, 23505, 13, 50507, 50507, 2022, 472, 43012, 11, 597, 307, 300, 562, 291, 3847, 257, 4833, 18161, 3209, 11, 309, 775, 1813, 544, 50758, 50758, 24903, 379, 5124, 13, 50842, 50842, 407, 264, 2135, 636, 309, 11051, 307, 309, 486, 2964, 760, 428, 3097, 293, 428, 38253, 1399, 13, 51112, 51112, 400, 588, 10515, 11, 281, 3890, 1125, 257, 18161, 3209, 11, 341, 307, 437, 291, 360, 13, 51434, 51434, 759, 264, 2063, 2445, 337, 428, 18161, 3209, 307, 264, 4274, 4470, 11, 293, 370, 264, 4470, 510, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.10844171290494958, "compression_ratio": 1.6903765690376569, "no_speech_prob": 5.12286703724385e-07}, {"id": 98, "seek": 44812, "start": 456.0, "end": 457.68, "text": " computationally expensive.", "tokens": [50364, 370, 938, 382, 291, 3890, 1125, 23505, 13, 50507, 50507, 2022, 472, 43012, 11, 597, 307, 300, 562, 291, 3847, 257, 4833, 18161, 3209, 11, 309, 775, 1813, 544, 50758, 50758, 24903, 379, 5124, 13, 50842, 50842, 407, 264, 2135, 636, 309, 11051, 307, 309, 486, 2964, 760, 428, 3097, 293, 428, 38253, 1399, 13, 51112, 51112, 400, 588, 10515, 11, 281, 3890, 1125, 257, 18161, 3209, 11, 341, 307, 437, 291, 360, 13, 51434, 51434, 759, 264, 2063, 2445, 337, 428, 18161, 3209, 307, 264, 4274, 4470, 11, 293, 370, 264, 4470, 510, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.10844171290494958, "compression_ratio": 1.6903765690376569, "no_speech_prob": 5.12286703724385e-07}, {"id": 99, "seek": 44812, "start": 457.68, "end": 463.08, "text": " So the main way it hurts is it will slow down your training and your inference process.", "tokens": [50364, 370, 938, 382, 291, 3890, 1125, 23505, 13, 50507, 50507, 2022, 472, 43012, 11, 597, 307, 300, 562, 291, 3847, 257, 4833, 18161, 3209, 11, 309, 775, 1813, 544, 50758, 50758, 24903, 379, 5124, 13, 50842, 50842, 407, 264, 2135, 636, 309, 11051, 307, 309, 486, 2964, 760, 428, 3097, 293, 428, 38253, 1399, 13, 51112, 51112, 400, 588, 10515, 11, 281, 3890, 1125, 257, 18161, 3209, 11, 341, 307, 437, 291, 360, 13, 51434, 51434, 759, 264, 2063, 2445, 337, 428, 18161, 3209, 307, 264, 4274, 4470, 11, 293, 370, 264, 4470, 510, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.10844171290494958, "compression_ratio": 1.6903765690376569, "no_speech_prob": 5.12286703724385e-07}, {"id": 100, "seek": 44812, "start": 463.08, "end": 469.52, "text": " And very briefly, to regularize a neural network, this is what you do.", "tokens": [50364, 370, 938, 382, 291, 3890, 1125, 23505, 13, 50507, 50507, 2022, 472, 43012, 11, 597, 307, 300, 562, 291, 3847, 257, 4833, 18161, 3209, 11, 309, 775, 1813, 544, 50758, 50758, 24903, 379, 5124, 13, 50842, 50842, 407, 264, 2135, 636, 309, 11051, 307, 309, 486, 2964, 760, 428, 3097, 293, 428, 38253, 1399, 13, 51112, 51112, 400, 588, 10515, 11, 281, 3890, 1125, 257, 18161, 3209, 11, 341, 307, 437, 291, 360, 13, 51434, 51434, 759, 264, 2063, 2445, 337, 428, 18161, 3209, 307, 264, 4274, 4470, 11, 293, 370, 264, 4470, 510, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.10844171290494958, "compression_ratio": 1.6903765690376569, "no_speech_prob": 5.12286703724385e-07}, {"id": 101, "seek": 44812, "start": 469.52, "end": 475.24, "text": " If the cost function for your neural network is the average loss, and so the loss here", "tokens": [50364, 370, 938, 382, 291, 3890, 1125, 23505, 13, 50507, 50507, 2022, 472, 43012, 11, 597, 307, 300, 562, 291, 3847, 257, 4833, 18161, 3209, 11, 309, 775, 1813, 544, 50758, 50758, 24903, 379, 5124, 13, 50842, 50842, 407, 264, 2135, 636, 309, 11051, 307, 309, 486, 2964, 760, 428, 3097, 293, 428, 38253, 1399, 13, 51112, 51112, 400, 588, 10515, 11, 281, 3890, 1125, 257, 18161, 3209, 11, 341, 307, 437, 291, 360, 13, 51434, 51434, 759, 264, 2063, 2445, 337, 428, 18161, 3209, 307, 264, 4274, 4470, 11, 293, 370, 264, 4470, 510, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.10844171290494958, "compression_ratio": 1.6903765690376569, "no_speech_prob": 5.12286703724385e-07}, {"id": 102, "seek": 47524, "start": 475.24, "end": 482.08, "text": " could be squared error or logistic loss, then the regularization term for a neural network", "tokens": [50364, 727, 312, 8889, 6713, 420, 3565, 3142, 4470, 11, 550, 264, 3890, 2144, 1433, 337, 257, 18161, 3209, 50706, 50706, 1542, 411, 1238, 709, 437, 291, 2066, 307, 13607, 670, 568, 76, 1413, 264, 2408, 295, 261, 8889, 11, 51080, 51080, 689, 341, 307, 264, 2408, 670, 439, 17443, 261, 294, 264, 18161, 3209, 13, 51308, 51308, 400, 2531, 281, 3890, 2144, 337, 8213, 24590, 293, 3565, 3142, 24590, 11, 321, 2673, 500, 380, 51562, 51562, 3890, 1125, 264, 9834, 272, 294, 257, 18161, 3209, 11, 4878, 294, 3124, 309, 1669, 588, 707, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.14951518147262102, "compression_ratio": 1.7740585774058577, "no_speech_prob": 7.646471203770489e-06}, {"id": 103, "seek": 47524, "start": 482.08, "end": 489.56, "text": " looks like pretty much what you expect is lambda over 2m times the sum of w squared,", "tokens": [50364, 727, 312, 8889, 6713, 420, 3565, 3142, 4470, 11, 550, 264, 3890, 2144, 1433, 337, 257, 18161, 3209, 50706, 50706, 1542, 411, 1238, 709, 437, 291, 2066, 307, 13607, 670, 568, 76, 1413, 264, 2408, 295, 261, 8889, 11, 51080, 51080, 689, 341, 307, 264, 2408, 670, 439, 17443, 261, 294, 264, 18161, 3209, 13, 51308, 51308, 400, 2531, 281, 3890, 2144, 337, 8213, 24590, 293, 3565, 3142, 24590, 11, 321, 2673, 500, 380, 51562, 51562, 3890, 1125, 264, 9834, 272, 294, 257, 18161, 3209, 11, 4878, 294, 3124, 309, 1669, 588, 707, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.14951518147262102, "compression_ratio": 1.7740585774058577, "no_speech_prob": 7.646471203770489e-06}, {"id": 104, "seek": 47524, "start": 489.56, "end": 494.12, "text": " where this is the sum over all weights w in the neural network.", "tokens": [50364, 727, 312, 8889, 6713, 420, 3565, 3142, 4470, 11, 550, 264, 3890, 2144, 1433, 337, 257, 18161, 3209, 50706, 50706, 1542, 411, 1238, 709, 437, 291, 2066, 307, 13607, 670, 568, 76, 1413, 264, 2408, 295, 261, 8889, 11, 51080, 51080, 689, 341, 307, 264, 2408, 670, 439, 17443, 261, 294, 264, 18161, 3209, 13, 51308, 51308, 400, 2531, 281, 3890, 2144, 337, 8213, 24590, 293, 3565, 3142, 24590, 11, 321, 2673, 500, 380, 51562, 51562, 3890, 1125, 264, 9834, 272, 294, 257, 18161, 3209, 11, 4878, 294, 3124, 309, 1669, 588, 707, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.14951518147262102, "compression_ratio": 1.7740585774058577, "no_speech_prob": 7.646471203770489e-06}, {"id": 105, "seek": 47524, "start": 494.12, "end": 499.2, "text": " And similar to regularization for linear regression and logistic regression, we usually don't", "tokens": [50364, 727, 312, 8889, 6713, 420, 3565, 3142, 4470, 11, 550, 264, 3890, 2144, 1433, 337, 257, 18161, 3209, 50706, 50706, 1542, 411, 1238, 709, 437, 291, 2066, 307, 13607, 670, 568, 76, 1413, 264, 2408, 295, 261, 8889, 11, 51080, 51080, 689, 341, 307, 264, 2408, 670, 439, 17443, 261, 294, 264, 18161, 3209, 13, 51308, 51308, 400, 2531, 281, 3890, 2144, 337, 8213, 24590, 293, 3565, 3142, 24590, 11, 321, 2673, 500, 380, 51562, 51562, 3890, 1125, 264, 9834, 272, 294, 257, 18161, 3209, 11, 4878, 294, 3124, 309, 1669, 588, 707, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.14951518147262102, "compression_ratio": 1.7740585774058577, "no_speech_prob": 7.646471203770489e-06}, {"id": 106, "seek": 47524, "start": 499.2, "end": 503.92, "text": " regularize the parameters b in a neural network, although in practice it makes very little", "tokens": [50364, 727, 312, 8889, 6713, 420, 3565, 3142, 4470, 11, 550, 264, 3890, 2144, 1433, 337, 257, 18161, 3209, 50706, 50706, 1542, 411, 1238, 709, 437, 291, 2066, 307, 13607, 670, 568, 76, 1413, 264, 2408, 295, 261, 8889, 11, 51080, 51080, 689, 341, 307, 264, 2408, 670, 439, 17443, 261, 294, 264, 18161, 3209, 13, 51308, 51308, 400, 2531, 281, 3890, 2144, 337, 8213, 24590, 293, 3565, 3142, 24590, 11, 321, 2673, 500, 380, 51562, 51562, 3890, 1125, 264, 9834, 272, 294, 257, 18161, 3209, 11, 4878, 294, 3124, 309, 1669, 588, 707, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.14951518147262102, "compression_ratio": 1.7740585774058577, "no_speech_prob": 7.646471203770489e-06}, {"id": 107, "seek": 50392, "start": 503.92, "end": 506.88, "text": " difference whether you do so or not.", "tokens": [50364, 2649, 1968, 291, 360, 370, 420, 406, 13, 50512, 50512, 400, 264, 636, 291, 576, 4445, 3890, 2144, 294, 37624, 307, 11, 9901, 300, 341, 390, 264, 50805, 50805, 3089, 337, 18114, 364, 517, 12, 26713, 1602, 1011, 26859, 14293, 21538, 2316, 13, 51121, 51121, 492, 1884, 1045, 7914, 411, 370, 365, 1230, 295, 7633, 6815, 11, 24433, 11, 293, 550, 1884, 51414, 51414, 257, 42881, 2316, 365, 264, 1045, 7914, 13, 51569, 51569], "temperature": 0.0, "avg_logprob": -0.13098750616374769, "compression_ratio": 1.6076555023923444, "no_speech_prob": 2.0580298496497562e-06}, {"id": 108, "seek": 50392, "start": 506.88, "end": 512.74, "text": " And the way you would implement regularization in TensorFlow is, recall that this was the", "tokens": [50364, 2649, 1968, 291, 360, 370, 420, 406, 13, 50512, 50512, 400, 264, 636, 291, 576, 4445, 3890, 2144, 294, 37624, 307, 11, 9901, 300, 341, 390, 264, 50805, 50805, 3089, 337, 18114, 364, 517, 12, 26713, 1602, 1011, 26859, 14293, 21538, 2316, 13, 51121, 51121, 492, 1884, 1045, 7914, 411, 370, 365, 1230, 295, 7633, 6815, 11, 24433, 11, 293, 550, 1884, 51414, 51414, 257, 42881, 2316, 365, 264, 1045, 7914, 13, 51569, 51569], "temperature": 0.0, "avg_logprob": -0.13098750616374769, "compression_ratio": 1.6076555023923444, "no_speech_prob": 2.0580298496497562e-06}, {"id": 109, "seek": 50392, "start": 512.74, "end": 519.0600000000001, "text": " code for implementing an un-regularized handwritten digit classification model.", "tokens": [50364, 2649, 1968, 291, 360, 370, 420, 406, 13, 50512, 50512, 400, 264, 636, 291, 576, 4445, 3890, 2144, 294, 37624, 307, 11, 9901, 300, 341, 390, 264, 50805, 50805, 3089, 337, 18114, 364, 517, 12, 26713, 1602, 1011, 26859, 14293, 21538, 2316, 13, 51121, 51121, 492, 1884, 1045, 7914, 411, 370, 365, 1230, 295, 7633, 6815, 11, 24433, 11, 293, 550, 1884, 51414, 51414, 257, 42881, 2316, 365, 264, 1045, 7914, 13, 51569, 51569], "temperature": 0.0, "avg_logprob": -0.13098750616374769, "compression_ratio": 1.6076555023923444, "no_speech_prob": 2.0580298496497562e-06}, {"id": 110, "seek": 50392, "start": 519.0600000000001, "end": 524.9200000000001, "text": " We create three layers like so with number of hidden units, activation, and then create", "tokens": [50364, 2649, 1968, 291, 360, 370, 420, 406, 13, 50512, 50512, 400, 264, 636, 291, 576, 4445, 3890, 2144, 294, 37624, 307, 11, 9901, 300, 341, 390, 264, 50805, 50805, 3089, 337, 18114, 364, 517, 12, 26713, 1602, 1011, 26859, 14293, 21538, 2316, 13, 51121, 51121, 492, 1884, 1045, 7914, 411, 370, 365, 1230, 295, 7633, 6815, 11, 24433, 11, 293, 550, 1884, 51414, 51414, 257, 42881, 2316, 365, 264, 1045, 7914, 13, 51569, 51569], "temperature": 0.0, "avg_logprob": -0.13098750616374769, "compression_ratio": 1.6076555023923444, "no_speech_prob": 2.0580298496497562e-06}, {"id": 111, "seek": 50392, "start": 524.9200000000001, "end": 528.02, "text": " a sequential model with the three layers.", "tokens": [50364, 2649, 1968, 291, 360, 370, 420, 406, 13, 50512, 50512, 400, 264, 636, 291, 576, 4445, 3890, 2144, 294, 37624, 307, 11, 9901, 300, 341, 390, 264, 50805, 50805, 3089, 337, 18114, 364, 517, 12, 26713, 1602, 1011, 26859, 14293, 21538, 2316, 13, 51121, 51121, 492, 1884, 1045, 7914, 411, 370, 365, 1230, 295, 7633, 6815, 11, 24433, 11, 293, 550, 1884, 51414, 51414, 257, 42881, 2316, 365, 264, 1045, 7914, 13, 51569, 51569], "temperature": 0.0, "avg_logprob": -0.13098750616374769, "compression_ratio": 1.6076555023923444, "no_speech_prob": 2.0580298496497562e-06}, {"id": 112, "seek": 52802, "start": 528.02, "end": 535.12, "text": " If you want to add regularization, then you would just add this extra term kernel regularizer", "tokens": [50364, 759, 291, 528, 281, 909, 3890, 2144, 11, 550, 291, 576, 445, 909, 341, 2857, 1433, 28256, 3890, 6545, 50719, 50719, 6915, 441, 17, 293, 550, 1958, 13, 10607, 11, 689, 300, 311, 264, 2158, 295, 13607, 13, 51019, 51019, 37624, 767, 6653, 291, 2826, 819, 4190, 295, 13607, 337, 819, 7914, 11, 4878, 51239, 51239, 337, 25632, 11, 291, 393, 2826, 264, 912, 2158, 295, 13607, 337, 439, 264, 17443, 294, 439, 295, 264, 51503, 51503, 819, 7914, 382, 10002, 13, 51601, 51601], "temperature": 0.0, "avg_logprob": -0.13300370061120323, "compression_ratio": 1.7136150234741785, "no_speech_prob": 8.446166930298205e-07}, {"id": 113, "seek": 52802, "start": 535.12, "end": 541.12, "text": " equals L2 and then 0.01, where that's the value of lambda.", "tokens": [50364, 759, 291, 528, 281, 909, 3890, 2144, 11, 550, 291, 576, 445, 909, 341, 2857, 1433, 28256, 3890, 6545, 50719, 50719, 6915, 441, 17, 293, 550, 1958, 13, 10607, 11, 689, 300, 311, 264, 2158, 295, 13607, 13, 51019, 51019, 37624, 767, 6653, 291, 2826, 819, 4190, 295, 13607, 337, 819, 7914, 11, 4878, 51239, 51239, 337, 25632, 11, 291, 393, 2826, 264, 912, 2158, 295, 13607, 337, 439, 264, 17443, 294, 439, 295, 264, 51503, 51503, 819, 7914, 382, 10002, 13, 51601, 51601], "temperature": 0.0, "avg_logprob": -0.13300370061120323, "compression_ratio": 1.7136150234741785, "no_speech_prob": 8.446166930298205e-07}, {"id": 114, "seek": 52802, "start": 541.12, "end": 545.52, "text": " TensorFlow actually lets you choose different values of lambda for different layers, although", "tokens": [50364, 759, 291, 528, 281, 909, 3890, 2144, 11, 550, 291, 576, 445, 909, 341, 2857, 1433, 28256, 3890, 6545, 50719, 50719, 6915, 441, 17, 293, 550, 1958, 13, 10607, 11, 689, 300, 311, 264, 2158, 295, 13607, 13, 51019, 51019, 37624, 767, 6653, 291, 2826, 819, 4190, 295, 13607, 337, 819, 7914, 11, 4878, 51239, 51239, 337, 25632, 11, 291, 393, 2826, 264, 912, 2158, 295, 13607, 337, 439, 264, 17443, 294, 439, 295, 264, 51503, 51503, 819, 7914, 382, 10002, 13, 51601, 51601], "temperature": 0.0, "avg_logprob": -0.13300370061120323, "compression_ratio": 1.7136150234741785, "no_speech_prob": 8.446166930298205e-07}, {"id": 115, "seek": 52802, "start": 545.52, "end": 550.8, "text": " for simplicity, you can choose the same value of lambda for all the weights in all of the", "tokens": [50364, 759, 291, 528, 281, 909, 3890, 2144, 11, 550, 291, 576, 445, 909, 341, 2857, 1433, 28256, 3890, 6545, 50719, 50719, 6915, 441, 17, 293, 550, 1958, 13, 10607, 11, 689, 300, 311, 264, 2158, 295, 13607, 13, 51019, 51019, 37624, 767, 6653, 291, 2826, 819, 4190, 295, 13607, 337, 819, 7914, 11, 4878, 51239, 51239, 337, 25632, 11, 291, 393, 2826, 264, 912, 2158, 295, 13607, 337, 439, 264, 17443, 294, 439, 295, 264, 51503, 51503, 819, 7914, 382, 10002, 13, 51601, 51601], "temperature": 0.0, "avg_logprob": -0.13300370061120323, "compression_ratio": 1.7136150234741785, "no_speech_prob": 8.446166930298205e-07}, {"id": 116, "seek": 52802, "start": 550.8, "end": 552.76, "text": " different layers as follows.", "tokens": [50364, 759, 291, 528, 281, 909, 3890, 2144, 11, 550, 291, 576, 445, 909, 341, 2857, 1433, 28256, 3890, 6545, 50719, 50719, 6915, 441, 17, 293, 550, 1958, 13, 10607, 11, 689, 300, 311, 264, 2158, 295, 13607, 13, 51019, 51019, 37624, 767, 6653, 291, 2826, 819, 4190, 295, 13607, 337, 819, 7914, 11, 4878, 51239, 51239, 337, 25632, 11, 291, 393, 2826, 264, 912, 2158, 295, 13607, 337, 439, 264, 17443, 294, 439, 295, 264, 51503, 51503, 819, 7914, 382, 10002, 13, 51601, 51601], "temperature": 0.0, "avg_logprob": -0.13300370061120323, "compression_ratio": 1.7136150234741785, "no_speech_prob": 8.446166930298205e-07}, {"id": 117, "seek": 55276, "start": 552.76, "end": 558.36, "text": " And then this will allow you to implement regularization in your neural network.", "tokens": [50364, 400, 550, 341, 486, 2089, 291, 281, 4445, 3890, 2144, 294, 428, 18161, 3209, 13, 50644, 50644, 407, 281, 20858, 11, 732, 45584, 286, 1454, 291, 362, 490, 341, 960, 366, 11, 472, 11, 309, 13572, 1562, 50928, 50928, 11051, 281, 362, 257, 4833, 18161, 3209, 370, 938, 382, 291, 3890, 1125, 23505, 13, 51234, 51234, 1485, 43012, 885, 300, 1419, 257, 4833, 18161, 3209, 393, 2964, 760, 428, 484, 12, 2670, 12, 2861, 11, 370, 51458, 51458, 1310, 300, 311, 264, 472, 636, 309, 11051, 11, 457, 309, 4659, 380, 4607, 428, 484, 12, 2670, 12, 2861, 311, 3389, 51648, 51648, 337, 264, 881, 644, 13, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.1254984967343442, "compression_ratio": 1.7401574803149606, "no_speech_prob": 2.2958947738516144e-06}, {"id": 118, "seek": 55276, "start": 558.36, "end": 564.04, "text": " So to summarize, two takeaways I hope you have from this video are, one, it hardly ever", "tokens": [50364, 400, 550, 341, 486, 2089, 291, 281, 4445, 3890, 2144, 294, 428, 18161, 3209, 13, 50644, 50644, 407, 281, 20858, 11, 732, 45584, 286, 1454, 291, 362, 490, 341, 960, 366, 11, 472, 11, 309, 13572, 1562, 50928, 50928, 11051, 281, 362, 257, 4833, 18161, 3209, 370, 938, 382, 291, 3890, 1125, 23505, 13, 51234, 51234, 1485, 43012, 885, 300, 1419, 257, 4833, 18161, 3209, 393, 2964, 760, 428, 484, 12, 2670, 12, 2861, 11, 370, 51458, 51458, 1310, 300, 311, 264, 472, 636, 309, 11051, 11, 457, 309, 4659, 380, 4607, 428, 484, 12, 2670, 12, 2861, 311, 3389, 51648, 51648, 337, 264, 881, 644, 13, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.1254984967343442, "compression_ratio": 1.7401574803149606, "no_speech_prob": 2.2958947738516144e-06}, {"id": 119, "seek": 55276, "start": 564.04, "end": 570.16, "text": " hurts to have a larger neural network so long as you regularize appropriately.", "tokens": [50364, 400, 550, 341, 486, 2089, 291, 281, 4445, 3890, 2144, 294, 428, 18161, 3209, 13, 50644, 50644, 407, 281, 20858, 11, 732, 45584, 286, 1454, 291, 362, 490, 341, 960, 366, 11, 472, 11, 309, 13572, 1562, 50928, 50928, 11051, 281, 362, 257, 4833, 18161, 3209, 370, 938, 382, 291, 3890, 1125, 23505, 13, 51234, 51234, 1485, 43012, 885, 300, 1419, 257, 4833, 18161, 3209, 393, 2964, 760, 428, 484, 12, 2670, 12, 2861, 11, 370, 51458, 51458, 1310, 300, 311, 264, 472, 636, 309, 11051, 11, 457, 309, 4659, 380, 4607, 428, 484, 12, 2670, 12, 2861, 311, 3389, 51648, 51648, 337, 264, 881, 644, 13, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.1254984967343442, "compression_ratio": 1.7401574803149606, "no_speech_prob": 2.2958947738516144e-06}, {"id": 120, "seek": 55276, "start": 570.16, "end": 574.64, "text": " One caveat being that having a larger neural network can slow down your out-of-room, so", "tokens": [50364, 400, 550, 341, 486, 2089, 291, 281, 4445, 3890, 2144, 294, 428, 18161, 3209, 13, 50644, 50644, 407, 281, 20858, 11, 732, 45584, 286, 1454, 291, 362, 490, 341, 960, 366, 11, 472, 11, 309, 13572, 1562, 50928, 50928, 11051, 281, 362, 257, 4833, 18161, 3209, 370, 938, 382, 291, 3890, 1125, 23505, 13, 51234, 51234, 1485, 43012, 885, 300, 1419, 257, 4833, 18161, 3209, 393, 2964, 760, 428, 484, 12, 2670, 12, 2861, 11, 370, 51458, 51458, 1310, 300, 311, 264, 472, 636, 309, 11051, 11, 457, 309, 4659, 380, 4607, 428, 484, 12, 2670, 12, 2861, 311, 3389, 51648, 51648, 337, 264, 881, 644, 13, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.1254984967343442, "compression_ratio": 1.7401574803149606, "no_speech_prob": 2.2958947738516144e-06}, {"id": 121, "seek": 55276, "start": 574.64, "end": 578.4399999999999, "text": " maybe that's the one way it hurts, but it shouldn't hurt your out-of-room's performance", "tokens": [50364, 400, 550, 341, 486, 2089, 291, 281, 4445, 3890, 2144, 294, 428, 18161, 3209, 13, 50644, 50644, 407, 281, 20858, 11, 732, 45584, 286, 1454, 291, 362, 490, 341, 960, 366, 11, 472, 11, 309, 13572, 1562, 50928, 50928, 11051, 281, 362, 257, 4833, 18161, 3209, 370, 938, 382, 291, 3890, 1125, 23505, 13, 51234, 51234, 1485, 43012, 885, 300, 1419, 257, 4833, 18161, 3209, 393, 2964, 760, 428, 484, 12, 2670, 12, 2861, 11, 370, 51458, 51458, 1310, 300, 311, 264, 472, 636, 309, 11051, 11, 457, 309, 4659, 380, 4607, 428, 484, 12, 2670, 12, 2861, 311, 3389, 51648, 51648, 337, 264, 881, 644, 13, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.1254984967343442, "compression_ratio": 1.7401574803149606, "no_speech_prob": 2.2958947738516144e-06}, {"id": 122, "seek": 55276, "start": 578.4399999999999, "end": 579.4399999999999, "text": " for the most part.", "tokens": [50364, 400, 550, 341, 486, 2089, 291, 281, 4445, 3890, 2144, 294, 428, 18161, 3209, 13, 50644, 50644, 407, 281, 20858, 11, 732, 45584, 286, 1454, 291, 362, 490, 341, 960, 366, 11, 472, 11, 309, 13572, 1562, 50928, 50928, 11051, 281, 362, 257, 4833, 18161, 3209, 370, 938, 382, 291, 3890, 1125, 23505, 13, 51234, 51234, 1485, 43012, 885, 300, 1419, 257, 4833, 18161, 3209, 393, 2964, 760, 428, 484, 12, 2670, 12, 2861, 11, 370, 51458, 51458, 1310, 300, 311, 264, 472, 636, 309, 11051, 11, 457, 309, 4659, 380, 4607, 428, 484, 12, 2670, 12, 2861, 311, 3389, 51648, 51648, 337, 264, 881, 644, 13, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.1254984967343442, "compression_ratio": 1.7401574803149606, "no_speech_prob": 2.2958947738516144e-06}, {"id": 123, "seek": 57944, "start": 579.44, "end": 583.0400000000001, "text": " And in fact, it could even help it significantly.", "tokens": [50364, 400, 294, 1186, 11, 309, 727, 754, 854, 309, 10591, 13, 50544, 50544, 400, 1150, 11, 370, 938, 382, 428, 3097, 992, 1943, 380, 886, 2416, 11, 550, 257, 18161, 3209, 11, 2318, 50850, 50850, 257, 2416, 18161, 3209, 11, 307, 2049, 257, 2295, 12, 65, 4609, 3479, 13, 51023, 51023, 467, 445, 9001, 588, 6179, 6828, 588, 731, 11, 597, 307, 983, 562, 286, 478, 3097, 18161, 51300, 51300, 9590, 11, 286, 915, 300, 286, 478, 2049, 5237, 3683, 2740, 2831, 813, 12577, 2740, 11, 412, 1935, 51560, 51560, 498, 264, 18161, 3209, 307, 2416, 1547, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.1427021307103774, "compression_ratio": 1.6979591836734693, "no_speech_prob": 1.9946926386182895e-06}, {"id": 124, "seek": 57944, "start": 583.0400000000001, "end": 589.1600000000001, "text": " And second, so long as your training set isn't too large, then a neural network, especially", "tokens": [50364, 400, 294, 1186, 11, 309, 727, 754, 854, 309, 10591, 13, 50544, 50544, 400, 1150, 11, 370, 938, 382, 428, 3097, 992, 1943, 380, 886, 2416, 11, 550, 257, 18161, 3209, 11, 2318, 50850, 50850, 257, 2416, 18161, 3209, 11, 307, 2049, 257, 2295, 12, 65, 4609, 3479, 13, 51023, 51023, 467, 445, 9001, 588, 6179, 6828, 588, 731, 11, 597, 307, 983, 562, 286, 478, 3097, 18161, 51300, 51300, 9590, 11, 286, 915, 300, 286, 478, 2049, 5237, 3683, 2740, 2831, 813, 12577, 2740, 11, 412, 1935, 51560, 51560, 498, 264, 18161, 3209, 307, 2416, 1547, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.1427021307103774, "compression_ratio": 1.6979591836734693, "no_speech_prob": 1.9946926386182895e-06}, {"id": 125, "seek": 57944, "start": 589.1600000000001, "end": 592.62, "text": " a large neural network, is often a low-bias machine.", "tokens": [50364, 400, 294, 1186, 11, 309, 727, 754, 854, 309, 10591, 13, 50544, 50544, 400, 1150, 11, 370, 938, 382, 428, 3097, 992, 1943, 380, 886, 2416, 11, 550, 257, 18161, 3209, 11, 2318, 50850, 50850, 257, 2416, 18161, 3209, 11, 307, 2049, 257, 2295, 12, 65, 4609, 3479, 13, 51023, 51023, 467, 445, 9001, 588, 6179, 6828, 588, 731, 11, 597, 307, 983, 562, 286, 478, 3097, 18161, 51300, 51300, 9590, 11, 286, 915, 300, 286, 478, 2049, 5237, 3683, 2740, 2831, 813, 12577, 2740, 11, 412, 1935, 51560, 51560, 498, 264, 18161, 3209, 307, 2416, 1547, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.1427021307103774, "compression_ratio": 1.6979591836734693, "no_speech_prob": 1.9946926386182895e-06}, {"id": 126, "seek": 57944, "start": 592.62, "end": 598.1600000000001, "text": " It just fits very complicated functions very well, which is why when I'm training neural", "tokens": [50364, 400, 294, 1186, 11, 309, 727, 754, 854, 309, 10591, 13, 50544, 50544, 400, 1150, 11, 370, 938, 382, 428, 3097, 992, 1943, 380, 886, 2416, 11, 550, 257, 18161, 3209, 11, 2318, 50850, 50850, 257, 2416, 18161, 3209, 11, 307, 2049, 257, 2295, 12, 65, 4609, 3479, 13, 51023, 51023, 467, 445, 9001, 588, 6179, 6828, 588, 731, 11, 597, 307, 983, 562, 286, 478, 3097, 18161, 51300, 51300, 9590, 11, 286, 915, 300, 286, 478, 2049, 5237, 3683, 2740, 2831, 813, 12577, 2740, 11, 412, 1935, 51560, 51560, 498, 264, 18161, 3209, 307, 2416, 1547, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.1427021307103774, "compression_ratio": 1.6979591836734693, "no_speech_prob": 1.9946926386182895e-06}, {"id": 127, "seek": 57944, "start": 598.1600000000001, "end": 603.36, "text": " networks, I find that I'm often fighting various problems rather than bias problems, at least", "tokens": [50364, 400, 294, 1186, 11, 309, 727, 754, 854, 309, 10591, 13, 50544, 50544, 400, 1150, 11, 370, 938, 382, 428, 3097, 992, 1943, 380, 886, 2416, 11, 550, 257, 18161, 3209, 11, 2318, 50850, 50850, 257, 2416, 18161, 3209, 11, 307, 2049, 257, 2295, 12, 65, 4609, 3479, 13, 51023, 51023, 467, 445, 9001, 588, 6179, 6828, 588, 731, 11, 597, 307, 983, 562, 286, 478, 3097, 18161, 51300, 51300, 9590, 11, 286, 915, 300, 286, 478, 2049, 5237, 3683, 2740, 2831, 813, 12577, 2740, 11, 412, 1935, 51560, 51560, 498, 264, 18161, 3209, 307, 2416, 1547, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.1427021307103774, "compression_ratio": 1.6979591836734693, "no_speech_prob": 1.9946926386182895e-06}, {"id": 128, "seek": 57944, "start": 603.36, "end": 605.72, "text": " if the neural network is large enough.", "tokens": [50364, 400, 294, 1186, 11, 309, 727, 754, 854, 309, 10591, 13, 50544, 50544, 400, 1150, 11, 370, 938, 382, 428, 3097, 992, 1943, 380, 886, 2416, 11, 550, 257, 18161, 3209, 11, 2318, 50850, 50850, 257, 2416, 18161, 3209, 11, 307, 2049, 257, 2295, 12, 65, 4609, 3479, 13, 51023, 51023, 467, 445, 9001, 588, 6179, 6828, 588, 731, 11, 597, 307, 983, 562, 286, 478, 3097, 18161, 51300, 51300, 9590, 11, 286, 915, 300, 286, 478, 2049, 5237, 3683, 2740, 2831, 813, 12577, 2740, 11, 412, 1935, 51560, 51560, 498, 264, 18161, 3209, 307, 2416, 1547, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.1427021307103774, "compression_ratio": 1.6979591836734693, "no_speech_prob": 1.9946926386182895e-06}, {"id": 129, "seek": 60572, "start": 605.72, "end": 610.28, "text": " So the rise of deep learning has really changed the way that machine learning practitioners", "tokens": [50364, 407, 264, 6272, 295, 2452, 2539, 575, 534, 3105, 264, 636, 300, 3479, 2539, 25742, 50592, 50592, 519, 466, 12577, 293, 21977, 13, 50702, 50702, 10222, 848, 300, 11, 754, 562, 291, 434, 3097, 257, 18161, 3209, 11, 13389, 12577, 293, 21977, 50903, 50903, 293, 1228, 300, 281, 5934, 437, 291, 360, 958, 307, 2049, 257, 588, 20531, 551, 281, 360, 13, 51244, 51244, 407, 300, 311, 309, 337, 12577, 293, 21977, 13, 51394, 51394, 961, 311, 352, 322, 281, 264, 958, 960, 689, 321, 603, 747, 439, 264, 3487, 321, 600, 3264, 293, 536, 577, 51598, 51598, 436, 3318, 294, 281, 264, 3250, 1399, 295, 3479, 2539, 3652, 13, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.1097843186897144, "compression_ratio": 1.7814814814814814, "no_speech_prob": 5.203492605687643e-07}, {"id": 130, "seek": 60572, "start": 610.28, "end": 612.48, "text": " think about bias and variance.", "tokens": [50364, 407, 264, 6272, 295, 2452, 2539, 575, 534, 3105, 264, 636, 300, 3479, 2539, 25742, 50592, 50592, 519, 466, 12577, 293, 21977, 13, 50702, 50702, 10222, 848, 300, 11, 754, 562, 291, 434, 3097, 257, 18161, 3209, 11, 13389, 12577, 293, 21977, 50903, 50903, 293, 1228, 300, 281, 5934, 437, 291, 360, 958, 307, 2049, 257, 588, 20531, 551, 281, 360, 13, 51244, 51244, 407, 300, 311, 309, 337, 12577, 293, 21977, 13, 51394, 51394, 961, 311, 352, 322, 281, 264, 958, 960, 689, 321, 603, 747, 439, 264, 3487, 321, 600, 3264, 293, 536, 577, 51598, 51598, 436, 3318, 294, 281, 264, 3250, 1399, 295, 3479, 2539, 3652, 13, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.1097843186897144, "compression_ratio": 1.7814814814814814, "no_speech_prob": 5.203492605687643e-07}, {"id": 131, "seek": 60572, "start": 612.48, "end": 616.5, "text": " Having said that, even when you're training a neural network, measuring bias and variance", "tokens": [50364, 407, 264, 6272, 295, 2452, 2539, 575, 534, 3105, 264, 636, 300, 3479, 2539, 25742, 50592, 50592, 519, 466, 12577, 293, 21977, 13, 50702, 50702, 10222, 848, 300, 11, 754, 562, 291, 434, 3097, 257, 18161, 3209, 11, 13389, 12577, 293, 21977, 50903, 50903, 293, 1228, 300, 281, 5934, 437, 291, 360, 958, 307, 2049, 257, 588, 20531, 551, 281, 360, 13, 51244, 51244, 407, 300, 311, 309, 337, 12577, 293, 21977, 13, 51394, 51394, 961, 311, 352, 322, 281, 264, 958, 960, 689, 321, 603, 747, 439, 264, 3487, 321, 600, 3264, 293, 536, 577, 51598, 51598, 436, 3318, 294, 281, 264, 3250, 1399, 295, 3479, 2539, 3652, 13, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.1097843186897144, "compression_ratio": 1.7814814814814814, "no_speech_prob": 5.203492605687643e-07}, {"id": 132, "seek": 60572, "start": 616.5, "end": 623.32, "text": " and using that to guide what you do next is often a very hopeful thing to do.", "tokens": [50364, 407, 264, 6272, 295, 2452, 2539, 575, 534, 3105, 264, 636, 300, 3479, 2539, 25742, 50592, 50592, 519, 466, 12577, 293, 21977, 13, 50702, 50702, 10222, 848, 300, 11, 754, 562, 291, 434, 3097, 257, 18161, 3209, 11, 13389, 12577, 293, 21977, 50903, 50903, 293, 1228, 300, 281, 5934, 437, 291, 360, 958, 307, 2049, 257, 588, 20531, 551, 281, 360, 13, 51244, 51244, 407, 300, 311, 309, 337, 12577, 293, 21977, 13, 51394, 51394, 961, 311, 352, 322, 281, 264, 958, 960, 689, 321, 603, 747, 439, 264, 3487, 321, 600, 3264, 293, 536, 577, 51598, 51598, 436, 3318, 294, 281, 264, 3250, 1399, 295, 3479, 2539, 3652, 13, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.1097843186897144, "compression_ratio": 1.7814814814814814, "no_speech_prob": 5.203492605687643e-07}, {"id": 133, "seek": 60572, "start": 623.32, "end": 626.32, "text": " So that's it for bias and variance.", "tokens": [50364, 407, 264, 6272, 295, 2452, 2539, 575, 534, 3105, 264, 636, 300, 3479, 2539, 25742, 50592, 50592, 519, 466, 12577, 293, 21977, 13, 50702, 50702, 10222, 848, 300, 11, 754, 562, 291, 434, 3097, 257, 18161, 3209, 11, 13389, 12577, 293, 21977, 50903, 50903, 293, 1228, 300, 281, 5934, 437, 291, 360, 958, 307, 2049, 257, 588, 20531, 551, 281, 360, 13, 51244, 51244, 407, 300, 311, 309, 337, 12577, 293, 21977, 13, 51394, 51394, 961, 311, 352, 322, 281, 264, 958, 960, 689, 321, 603, 747, 439, 264, 3487, 321, 600, 3264, 293, 536, 577, 51598, 51598, 436, 3318, 294, 281, 264, 3250, 1399, 295, 3479, 2539, 3652, 13, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.1097843186897144, "compression_ratio": 1.7814814814814814, "no_speech_prob": 5.203492605687643e-07}, {"id": 134, "seek": 60572, "start": 626.32, "end": 630.4, "text": " Let's go on to the next video where we'll take all the ideas we've learned and see how", "tokens": [50364, 407, 264, 6272, 295, 2452, 2539, 575, 534, 3105, 264, 636, 300, 3479, 2539, 25742, 50592, 50592, 519, 466, 12577, 293, 21977, 13, 50702, 50702, 10222, 848, 300, 11, 754, 562, 291, 434, 3097, 257, 18161, 3209, 11, 13389, 12577, 293, 21977, 50903, 50903, 293, 1228, 300, 281, 5934, 437, 291, 360, 958, 307, 2049, 257, 588, 20531, 551, 281, 360, 13, 51244, 51244, 407, 300, 311, 309, 337, 12577, 293, 21977, 13, 51394, 51394, 961, 311, 352, 322, 281, 264, 958, 960, 689, 321, 603, 747, 439, 264, 3487, 321, 600, 3264, 293, 536, 577, 51598, 51598, 436, 3318, 294, 281, 264, 3250, 1399, 295, 3479, 2539, 3652, 13, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.1097843186897144, "compression_ratio": 1.7814814814814814, "no_speech_prob": 5.203492605687643e-07}, {"id": 135, "seek": 60572, "start": 630.4, "end": 634.36, "text": " they fit in to the development process of machine learning systems.", "tokens": [50364, 407, 264, 6272, 295, 2452, 2539, 575, 534, 3105, 264, 636, 300, 3479, 2539, 25742, 50592, 50592, 519, 466, 12577, 293, 21977, 13, 50702, 50702, 10222, 848, 300, 11, 754, 562, 291, 434, 3097, 257, 18161, 3209, 11, 13389, 12577, 293, 21977, 50903, 50903, 293, 1228, 300, 281, 5934, 437, 291, 360, 958, 307, 2049, 257, 588, 20531, 551, 281, 360, 13, 51244, 51244, 407, 300, 311, 309, 337, 12577, 293, 21977, 13, 51394, 51394, 961, 311, 352, 322, 281, 264, 958, 960, 689, 321, 603, 747, 439, 264, 3487, 321, 600, 3264, 293, 536, 577, 51598, 51598, 436, 3318, 294, 281, 264, 3250, 1399, 295, 3479, 2539, 3652, 13, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.1097843186897144, "compression_ratio": 1.7814814814814814, "no_speech_prob": 5.203492605687643e-07}, {"id": 136, "seek": 63436, "start": 634.36, "end": 639.76, "text": " And I hope that we'll tie a lot of these pieces together to give you practical advice for", "tokens": [50364, 400, 286, 1454, 300, 321, 603, 7582, 257, 688, 295, 613, 3755, 1214, 281, 976, 291, 8496, 5192, 337, 50634, 50634, 577, 281, 2661, 1286, 2128, 294, 264, 3250, 295, 428, 3479, 2539, 3652, 13, 50842], "temperature": 0.0, "avg_logprob": -0.14285192991557874, "compression_ratio": 1.317829457364341, "no_speech_prob": 2.5045759684871882e-05}, {"id": 137, "seek": 63976, "start": 639.76, "end": 668.3199999999999, "text": " how to quickly move forward in the development of your machine learning systems.", "tokens": [50364, 577, 281, 2661, 1286, 2128, 294, 264, 3250, 295, 428, 3479, 2539, 3652, 13, 51792], "temperature": 0.0, "avg_logprob": -0.3106385960298426, "compression_ratio": 1.0810810810810811, "no_speech_prob": 4.479533345147502e-06}], "language": "en", "video_id": "Hso8Qq3-arc", "entity": "ML Specialization, Andrew Ng (2022)"}}