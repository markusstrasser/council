{"video_id": "TQ388ISSoI4", "title": "5.3 Activation Functions | Alternatives to the sigmoid activation  --[Machine Learning | Andrew Ng]", "description": "Second Course:\nAdvanced Learning Algorithms.\n\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 329, "views": 99, "publish_date": "11/04/2022", "timestamp": 1661472000, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " So far, we've been using the sigmoid activation function in all the nodes, in the hidden layers, and in the output layer. And we have started that way because we were building up neural networks by taking logistic regression and creating a lot of logistic regression units and stringing them together. But if you use other activation functions, your neural network can become much more powerful. Let's take a look at how to do that. Recall the demand prediction example from last week, where given price, shipping cost, marketing material, you would try to predict if something is highly affordable, if there's good awareness and high perceived quality, and based on that, try to predict if it's a top seller. But this assumes that awareness is maybe binary, it's either people are aware or they are not. But it seems like the degree to which possible buyers are aware of the t-shirt you're selling may not be binary. They can be a little bit aware, somewhat aware, extremely aware, or it could have gone completely viral. So rather than modeling awareness as a binary number, 0 or 1, that you try to estimate the probability of awareness, or rather than modeling awareness as just a number between 0 and 1, maybe awareness should be any non-negative number, because there can be any non-negative value of awareness going from 0 up to very, very large numbers. So whereas previously we had used this equation to calculate the activation of that second hidden unit estimating awareness, where g was the sigmoid function, and thus goes between 0 and 1. If you want to allow a 1, 2 to potentially take on much larger positive values, we can instead swap in a different activation function. It turns out that a very common choice of activation function in neural networks is this function. It looks like this. It goes, if z is this, then g of z is 0 to the left, and then this is straight line, 35 degrees to the right of 0. And so when z is greater than or equal to 0, g of z is just equal to z, that is to the right half of this diagram. And the mathematical equation for this is g of z equals max of 0, z. Be free to verify for yourself that max of 0, z results in this curve that I've drawn over here. And if a 1, 2 is g of z for this value of z, then a, the activation value, can now take on 0 or any non-negative value. This activation function has a name. It goes by the name ReLU with this funny capitalization. And ReLU stands for, again, a somewhat arcane term, but it stands for rectified linear unit. Don't worry too much about what rectified means and what linear unit means. This was just a name that the authors had given to this particular activation function when they came up with it. But most people in deep learning just say ReLU to refer to this g of z. More generally, you have a choice of what to use for g of z. And sometimes we'll use a different choice than the sigmoid activation function. Here are the most commonly used activation functions. You saw the sigmoid activation function, g of z equals the sigmoid function. On the last slide, we just looked at the ReLU, or rectified linear unit, g of z equals max of 0, z. There's one other activation function which is worth mentioning, which is called the linear activation function, which is just g of z equals to z. Sometimes if you use the linear activation function, people will say we're not using any activation function because if a is g of z, where g of z equals z, then a is just equal to this, w dot x plus b, say. And so it's as if there was no g in there at all. So when you are using this linear activation function, g of z, sometimes people will say, well, we're not using any activation function. Although in this clause, I will refer to using the linear activation function rather than no activation function. But if you hear someone else use that terminology, that's what they mean. It just refers to the linear activation function. And these three are probably by far the most commonly used activation functions in neural networks. Later this week, we'll touch on the fourth one called the softmax activation function. But with these activation functions, you'll be able to build a rich variety of powerful neural networks. So when building a neural network, for each neuron, do you want to use the sigmoid activation function or the ReLU activation function or a linear activation function? How do you choose between these different activation functions? Let's take a look at that in the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.76, "text": " So far, we've been using the sigmoid activation function in all the nodes, in the hidden layers,", "tokens": [50364, 407, 1400, 11, 321, 600, 668, 1228, 264, 4556, 3280, 327, 24433, 2445, 294, 439, 264, 13891, 11, 294, 264, 7633, 7914, 11, 50802, 50802, 293, 294, 264, 5598, 4583, 13, 50918, 50918, 400, 321, 362, 1409, 300, 636, 570, 321, 645, 2390, 493, 18161, 9590, 538, 1940, 3565, 3142, 51178, 51178, 24590, 293, 4084, 257, 688, 295, 3565, 3142, 24590, 6815, 293, 6798, 278, 552, 1214, 13, 51506, 51506, 583, 498, 291, 764, 661, 24433, 6828, 11, 428, 18161, 3209, 393, 1813, 709, 544, 4005, 13, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.14122681041340251, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.008056892082095146}, {"id": 1, "seek": 0, "start": 8.76, "end": 11.08, "text": " and in the output layer.", "tokens": [50364, 407, 1400, 11, 321, 600, 668, 1228, 264, 4556, 3280, 327, 24433, 2445, 294, 439, 264, 13891, 11, 294, 264, 7633, 7914, 11, 50802, 50802, 293, 294, 264, 5598, 4583, 13, 50918, 50918, 400, 321, 362, 1409, 300, 636, 570, 321, 645, 2390, 493, 18161, 9590, 538, 1940, 3565, 3142, 51178, 51178, 24590, 293, 4084, 257, 688, 295, 3565, 3142, 24590, 6815, 293, 6798, 278, 552, 1214, 13, 51506, 51506, 583, 498, 291, 764, 661, 24433, 6828, 11, 428, 18161, 3209, 393, 1813, 709, 544, 4005, 13, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.14122681041340251, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.008056892082095146}, {"id": 2, "seek": 0, "start": 11.08, "end": 16.28, "text": " And we have started that way because we were building up neural networks by taking logistic", "tokens": [50364, 407, 1400, 11, 321, 600, 668, 1228, 264, 4556, 3280, 327, 24433, 2445, 294, 439, 264, 13891, 11, 294, 264, 7633, 7914, 11, 50802, 50802, 293, 294, 264, 5598, 4583, 13, 50918, 50918, 400, 321, 362, 1409, 300, 636, 570, 321, 645, 2390, 493, 18161, 9590, 538, 1940, 3565, 3142, 51178, 51178, 24590, 293, 4084, 257, 688, 295, 3565, 3142, 24590, 6815, 293, 6798, 278, 552, 1214, 13, 51506, 51506, 583, 498, 291, 764, 661, 24433, 6828, 11, 428, 18161, 3209, 393, 1813, 709, 544, 4005, 13, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.14122681041340251, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.008056892082095146}, {"id": 3, "seek": 0, "start": 16.28, "end": 22.84, "text": " regression and creating a lot of logistic regression units and stringing them together.", "tokens": [50364, 407, 1400, 11, 321, 600, 668, 1228, 264, 4556, 3280, 327, 24433, 2445, 294, 439, 264, 13891, 11, 294, 264, 7633, 7914, 11, 50802, 50802, 293, 294, 264, 5598, 4583, 13, 50918, 50918, 400, 321, 362, 1409, 300, 636, 570, 321, 645, 2390, 493, 18161, 9590, 538, 1940, 3565, 3142, 51178, 51178, 24590, 293, 4084, 257, 688, 295, 3565, 3142, 24590, 6815, 293, 6798, 278, 552, 1214, 13, 51506, 51506, 583, 498, 291, 764, 661, 24433, 6828, 11, 428, 18161, 3209, 393, 1813, 709, 544, 4005, 13, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.14122681041340251, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.008056892082095146}, {"id": 4, "seek": 0, "start": 22.84, "end": 27.84, "text": " But if you use other activation functions, your neural network can become much more powerful.", "tokens": [50364, 407, 1400, 11, 321, 600, 668, 1228, 264, 4556, 3280, 327, 24433, 2445, 294, 439, 264, 13891, 11, 294, 264, 7633, 7914, 11, 50802, 50802, 293, 294, 264, 5598, 4583, 13, 50918, 50918, 400, 321, 362, 1409, 300, 636, 570, 321, 645, 2390, 493, 18161, 9590, 538, 1940, 3565, 3142, 51178, 51178, 24590, 293, 4084, 257, 688, 295, 3565, 3142, 24590, 6815, 293, 6798, 278, 552, 1214, 13, 51506, 51506, 583, 498, 291, 764, 661, 24433, 6828, 11, 428, 18161, 3209, 393, 1813, 709, 544, 4005, 13, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.14122681041340251, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.008056892082095146}, {"id": 5, "seek": 2784, "start": 27.84, "end": 30.4, "text": " Let's take a look at how to do that.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 577, 281, 360, 300, 13, 50492, 50492, 9647, 336, 264, 4733, 17630, 1365, 490, 1036, 1243, 11, 689, 2212, 3218, 11, 14122, 2063, 11, 50740, 50740, 6370, 2527, 11, 291, 576, 853, 281, 6069, 498, 746, 307, 5405, 12028, 11, 498, 456, 311, 51022, 51022, 665, 8888, 293, 1090, 19049, 3125, 11, 293, 2361, 322, 300, 11, 853, 281, 6069, 498, 309, 311, 51216, 51216, 257, 1192, 23600, 13, 51304, 51304, 583, 341, 37808, 300, 8888, 307, 1310, 17434, 11, 309, 311, 2139, 561, 366, 3650, 420, 436, 366, 406, 13, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.1780904448858582, "compression_ratio": 1.6451612903225807, "no_speech_prob": 1.078150307876058e-05}, {"id": 6, "seek": 2784, "start": 30.4, "end": 35.36, "text": " Recall the demand prediction example from last week, where given price, shipping cost,", "tokens": [50364, 961, 311, 747, 257, 574, 412, 577, 281, 360, 300, 13, 50492, 50492, 9647, 336, 264, 4733, 17630, 1365, 490, 1036, 1243, 11, 689, 2212, 3218, 11, 14122, 2063, 11, 50740, 50740, 6370, 2527, 11, 291, 576, 853, 281, 6069, 498, 746, 307, 5405, 12028, 11, 498, 456, 311, 51022, 51022, 665, 8888, 293, 1090, 19049, 3125, 11, 293, 2361, 322, 300, 11, 853, 281, 6069, 498, 309, 311, 51216, 51216, 257, 1192, 23600, 13, 51304, 51304, 583, 341, 37808, 300, 8888, 307, 1310, 17434, 11, 309, 311, 2139, 561, 366, 3650, 420, 436, 366, 406, 13, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.1780904448858582, "compression_ratio": 1.6451612903225807, "no_speech_prob": 1.078150307876058e-05}, {"id": 7, "seek": 2784, "start": 35.36, "end": 41.0, "text": " marketing material, you would try to predict if something is highly affordable, if there's", "tokens": [50364, 961, 311, 747, 257, 574, 412, 577, 281, 360, 300, 13, 50492, 50492, 9647, 336, 264, 4733, 17630, 1365, 490, 1036, 1243, 11, 689, 2212, 3218, 11, 14122, 2063, 11, 50740, 50740, 6370, 2527, 11, 291, 576, 853, 281, 6069, 498, 746, 307, 5405, 12028, 11, 498, 456, 311, 51022, 51022, 665, 8888, 293, 1090, 19049, 3125, 11, 293, 2361, 322, 300, 11, 853, 281, 6069, 498, 309, 311, 51216, 51216, 257, 1192, 23600, 13, 51304, 51304, 583, 341, 37808, 300, 8888, 307, 1310, 17434, 11, 309, 311, 2139, 561, 366, 3650, 420, 436, 366, 406, 13, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.1780904448858582, "compression_ratio": 1.6451612903225807, "no_speech_prob": 1.078150307876058e-05}, {"id": 8, "seek": 2784, "start": 41.0, "end": 44.879999999999995, "text": " good awareness and high perceived quality, and based on that, try to predict if it's", "tokens": [50364, 961, 311, 747, 257, 574, 412, 577, 281, 360, 300, 13, 50492, 50492, 9647, 336, 264, 4733, 17630, 1365, 490, 1036, 1243, 11, 689, 2212, 3218, 11, 14122, 2063, 11, 50740, 50740, 6370, 2527, 11, 291, 576, 853, 281, 6069, 498, 746, 307, 5405, 12028, 11, 498, 456, 311, 51022, 51022, 665, 8888, 293, 1090, 19049, 3125, 11, 293, 2361, 322, 300, 11, 853, 281, 6069, 498, 309, 311, 51216, 51216, 257, 1192, 23600, 13, 51304, 51304, 583, 341, 37808, 300, 8888, 307, 1310, 17434, 11, 309, 311, 2139, 561, 366, 3650, 420, 436, 366, 406, 13, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.1780904448858582, "compression_ratio": 1.6451612903225807, "no_speech_prob": 1.078150307876058e-05}, {"id": 9, "seek": 2784, "start": 44.879999999999995, "end": 46.64, "text": " a top seller.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 577, 281, 360, 300, 13, 50492, 50492, 9647, 336, 264, 4733, 17630, 1365, 490, 1036, 1243, 11, 689, 2212, 3218, 11, 14122, 2063, 11, 50740, 50740, 6370, 2527, 11, 291, 576, 853, 281, 6069, 498, 746, 307, 5405, 12028, 11, 498, 456, 311, 51022, 51022, 665, 8888, 293, 1090, 19049, 3125, 11, 293, 2361, 322, 300, 11, 853, 281, 6069, 498, 309, 311, 51216, 51216, 257, 1192, 23600, 13, 51304, 51304, 583, 341, 37808, 300, 8888, 307, 1310, 17434, 11, 309, 311, 2139, 561, 366, 3650, 420, 436, 366, 406, 13, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.1780904448858582, "compression_ratio": 1.6451612903225807, "no_speech_prob": 1.078150307876058e-05}, {"id": 10, "seek": 2784, "start": 46.64, "end": 54.16, "text": " But this assumes that awareness is maybe binary, it's either people are aware or they are not.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 577, 281, 360, 300, 13, 50492, 50492, 9647, 336, 264, 4733, 17630, 1365, 490, 1036, 1243, 11, 689, 2212, 3218, 11, 14122, 2063, 11, 50740, 50740, 6370, 2527, 11, 291, 576, 853, 281, 6069, 498, 746, 307, 5405, 12028, 11, 498, 456, 311, 51022, 51022, 665, 8888, 293, 1090, 19049, 3125, 11, 293, 2361, 322, 300, 11, 853, 281, 6069, 498, 309, 311, 51216, 51216, 257, 1192, 23600, 13, 51304, 51304, 583, 341, 37808, 300, 8888, 307, 1310, 17434, 11, 309, 311, 2139, 561, 366, 3650, 420, 436, 366, 406, 13, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.1780904448858582, "compression_ratio": 1.6451612903225807, "no_speech_prob": 1.078150307876058e-05}, {"id": 11, "seek": 5416, "start": 54.16, "end": 60.699999999999996, "text": " But it seems like the degree to which possible buyers are aware of the t-shirt you're selling", "tokens": [50364, 583, 309, 2544, 411, 264, 4314, 281, 597, 1944, 23465, 366, 3650, 295, 264, 256, 12, 15313, 291, 434, 6511, 50691, 50691, 815, 406, 312, 17434, 13, 50756, 50756, 814, 393, 312, 257, 707, 857, 3650, 11, 8344, 3650, 11, 4664, 3650, 11, 420, 309, 727, 362, 2780, 2584, 51002, 51002, 16132, 13, 51068, 51068, 407, 2831, 813, 15983, 8888, 382, 257, 17434, 1230, 11, 1958, 420, 502, 11, 300, 291, 853, 281, 12539, 264, 51396, 51396, 8482, 295, 8888, 11, 420, 2831, 813, 15983, 8888, 382, 445, 257, 1230, 1296, 1958, 293, 502, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.126239092663081, "compression_ratio": 1.7179487179487178, "no_speech_prob": 6.438785931095481e-06}, {"id": 12, "seek": 5416, "start": 60.699999999999996, "end": 62.0, "text": " may not be binary.", "tokens": [50364, 583, 309, 2544, 411, 264, 4314, 281, 597, 1944, 23465, 366, 3650, 295, 264, 256, 12, 15313, 291, 434, 6511, 50691, 50691, 815, 406, 312, 17434, 13, 50756, 50756, 814, 393, 312, 257, 707, 857, 3650, 11, 8344, 3650, 11, 4664, 3650, 11, 420, 309, 727, 362, 2780, 2584, 51002, 51002, 16132, 13, 51068, 51068, 407, 2831, 813, 15983, 8888, 382, 257, 17434, 1230, 11, 1958, 420, 502, 11, 300, 291, 853, 281, 12539, 264, 51396, 51396, 8482, 295, 8888, 11, 420, 2831, 813, 15983, 8888, 382, 445, 257, 1230, 1296, 1958, 293, 502, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.126239092663081, "compression_ratio": 1.7179487179487178, "no_speech_prob": 6.438785931095481e-06}, {"id": 13, "seek": 5416, "start": 62.0, "end": 66.92, "text": " They can be a little bit aware, somewhat aware, extremely aware, or it could have gone completely", "tokens": [50364, 583, 309, 2544, 411, 264, 4314, 281, 597, 1944, 23465, 366, 3650, 295, 264, 256, 12, 15313, 291, 434, 6511, 50691, 50691, 815, 406, 312, 17434, 13, 50756, 50756, 814, 393, 312, 257, 707, 857, 3650, 11, 8344, 3650, 11, 4664, 3650, 11, 420, 309, 727, 362, 2780, 2584, 51002, 51002, 16132, 13, 51068, 51068, 407, 2831, 813, 15983, 8888, 382, 257, 17434, 1230, 11, 1958, 420, 502, 11, 300, 291, 853, 281, 12539, 264, 51396, 51396, 8482, 295, 8888, 11, 420, 2831, 813, 15983, 8888, 382, 445, 257, 1230, 1296, 1958, 293, 502, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.126239092663081, "compression_ratio": 1.7179487179487178, "no_speech_prob": 6.438785931095481e-06}, {"id": 14, "seek": 5416, "start": 66.92, "end": 68.24, "text": " viral.", "tokens": [50364, 583, 309, 2544, 411, 264, 4314, 281, 597, 1944, 23465, 366, 3650, 295, 264, 256, 12, 15313, 291, 434, 6511, 50691, 50691, 815, 406, 312, 17434, 13, 50756, 50756, 814, 393, 312, 257, 707, 857, 3650, 11, 8344, 3650, 11, 4664, 3650, 11, 420, 309, 727, 362, 2780, 2584, 51002, 51002, 16132, 13, 51068, 51068, 407, 2831, 813, 15983, 8888, 382, 257, 17434, 1230, 11, 1958, 420, 502, 11, 300, 291, 853, 281, 12539, 264, 51396, 51396, 8482, 295, 8888, 11, 420, 2831, 813, 15983, 8888, 382, 445, 257, 1230, 1296, 1958, 293, 502, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.126239092663081, "compression_ratio": 1.7179487179487178, "no_speech_prob": 6.438785931095481e-06}, {"id": 15, "seek": 5416, "start": 68.24, "end": 74.8, "text": " So rather than modeling awareness as a binary number, 0 or 1, that you try to estimate the", "tokens": [50364, 583, 309, 2544, 411, 264, 4314, 281, 597, 1944, 23465, 366, 3650, 295, 264, 256, 12, 15313, 291, 434, 6511, 50691, 50691, 815, 406, 312, 17434, 13, 50756, 50756, 814, 393, 312, 257, 707, 857, 3650, 11, 8344, 3650, 11, 4664, 3650, 11, 420, 309, 727, 362, 2780, 2584, 51002, 51002, 16132, 13, 51068, 51068, 407, 2831, 813, 15983, 8888, 382, 257, 17434, 1230, 11, 1958, 420, 502, 11, 300, 291, 853, 281, 12539, 264, 51396, 51396, 8482, 295, 8888, 11, 420, 2831, 813, 15983, 8888, 382, 445, 257, 1230, 1296, 1958, 293, 502, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.126239092663081, "compression_ratio": 1.7179487179487178, "no_speech_prob": 6.438785931095481e-06}, {"id": 16, "seek": 5416, "start": 74.8, "end": 80.72, "text": " probability of awareness, or rather than modeling awareness as just a number between 0 and 1,", "tokens": [50364, 583, 309, 2544, 411, 264, 4314, 281, 597, 1944, 23465, 366, 3650, 295, 264, 256, 12, 15313, 291, 434, 6511, 50691, 50691, 815, 406, 312, 17434, 13, 50756, 50756, 814, 393, 312, 257, 707, 857, 3650, 11, 8344, 3650, 11, 4664, 3650, 11, 420, 309, 727, 362, 2780, 2584, 51002, 51002, 16132, 13, 51068, 51068, 407, 2831, 813, 15983, 8888, 382, 257, 17434, 1230, 11, 1958, 420, 502, 11, 300, 291, 853, 281, 12539, 264, 51396, 51396, 8482, 295, 8888, 11, 420, 2831, 813, 15983, 8888, 382, 445, 257, 1230, 1296, 1958, 293, 502, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.126239092663081, "compression_ratio": 1.7179487179487178, "no_speech_prob": 6.438785931095481e-06}, {"id": 17, "seek": 8072, "start": 80.72, "end": 86.2, "text": " maybe awareness should be any non-negative number, because there can be any non-negative", "tokens": [50364, 1310, 8888, 820, 312, 604, 2107, 12, 28561, 1166, 1230, 11, 570, 456, 393, 312, 604, 2107, 12, 28561, 1166, 50638, 50638, 2158, 295, 8888, 516, 490, 1958, 493, 281, 588, 11, 588, 2416, 3547, 13, 50923, 50923, 407, 9735, 8046, 321, 632, 1143, 341, 5367, 281, 8873, 264, 24433, 295, 300, 1150, 51276, 51276, 7633, 4985, 8017, 990, 8888, 11, 689, 290, 390, 264, 4556, 3280, 327, 2445, 11, 293, 8807, 1709, 1296, 51590, 51590, 1958, 293, 502, 13, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.15070035344078428, "compression_ratio": 1.6442307692307692, "no_speech_prob": 3.966938038502121e-06}, {"id": 18, "seek": 8072, "start": 86.2, "end": 91.9, "text": " value of awareness going from 0 up to very, very large numbers.", "tokens": [50364, 1310, 8888, 820, 312, 604, 2107, 12, 28561, 1166, 1230, 11, 570, 456, 393, 312, 604, 2107, 12, 28561, 1166, 50638, 50638, 2158, 295, 8888, 516, 490, 1958, 493, 281, 588, 11, 588, 2416, 3547, 13, 50923, 50923, 407, 9735, 8046, 321, 632, 1143, 341, 5367, 281, 8873, 264, 24433, 295, 300, 1150, 51276, 51276, 7633, 4985, 8017, 990, 8888, 11, 689, 290, 390, 264, 4556, 3280, 327, 2445, 11, 293, 8807, 1709, 1296, 51590, 51590, 1958, 293, 502, 13, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.15070035344078428, "compression_ratio": 1.6442307692307692, "no_speech_prob": 3.966938038502121e-06}, {"id": 19, "seek": 8072, "start": 91.9, "end": 98.96000000000001, "text": " So whereas previously we had used this equation to calculate the activation of that second", "tokens": [50364, 1310, 8888, 820, 312, 604, 2107, 12, 28561, 1166, 1230, 11, 570, 456, 393, 312, 604, 2107, 12, 28561, 1166, 50638, 50638, 2158, 295, 8888, 516, 490, 1958, 493, 281, 588, 11, 588, 2416, 3547, 13, 50923, 50923, 407, 9735, 8046, 321, 632, 1143, 341, 5367, 281, 8873, 264, 24433, 295, 300, 1150, 51276, 51276, 7633, 4985, 8017, 990, 8888, 11, 689, 290, 390, 264, 4556, 3280, 327, 2445, 11, 293, 8807, 1709, 1296, 51590, 51590, 1958, 293, 502, 13, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.15070035344078428, "compression_ratio": 1.6442307692307692, "no_speech_prob": 3.966938038502121e-06}, {"id": 20, "seek": 8072, "start": 98.96000000000001, "end": 105.24, "text": " hidden unit estimating awareness, where g was the sigmoid function, and thus goes between", "tokens": [50364, 1310, 8888, 820, 312, 604, 2107, 12, 28561, 1166, 1230, 11, 570, 456, 393, 312, 604, 2107, 12, 28561, 1166, 50638, 50638, 2158, 295, 8888, 516, 490, 1958, 493, 281, 588, 11, 588, 2416, 3547, 13, 50923, 50923, 407, 9735, 8046, 321, 632, 1143, 341, 5367, 281, 8873, 264, 24433, 295, 300, 1150, 51276, 51276, 7633, 4985, 8017, 990, 8888, 11, 689, 290, 390, 264, 4556, 3280, 327, 2445, 11, 293, 8807, 1709, 1296, 51590, 51590, 1958, 293, 502, 13, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.15070035344078428, "compression_ratio": 1.6442307692307692, "no_speech_prob": 3.966938038502121e-06}, {"id": 21, "seek": 8072, "start": 105.24, "end": 107.32, "text": " 0 and 1.", "tokens": [50364, 1310, 8888, 820, 312, 604, 2107, 12, 28561, 1166, 1230, 11, 570, 456, 393, 312, 604, 2107, 12, 28561, 1166, 50638, 50638, 2158, 295, 8888, 516, 490, 1958, 493, 281, 588, 11, 588, 2416, 3547, 13, 50923, 50923, 407, 9735, 8046, 321, 632, 1143, 341, 5367, 281, 8873, 264, 24433, 295, 300, 1150, 51276, 51276, 7633, 4985, 8017, 990, 8888, 11, 689, 290, 390, 264, 4556, 3280, 327, 2445, 11, 293, 8807, 1709, 1296, 51590, 51590, 1958, 293, 502, 13, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.15070035344078428, "compression_ratio": 1.6442307692307692, "no_speech_prob": 3.966938038502121e-06}, {"id": 22, "seek": 10732, "start": 107.32, "end": 114.67999999999999, "text": " If you want to allow a 1, 2 to potentially take on much larger positive values, we can", "tokens": [50364, 759, 291, 528, 281, 2089, 257, 502, 11, 568, 281, 7263, 747, 322, 709, 4833, 3353, 4190, 11, 321, 393, 50732, 50732, 2602, 18135, 294, 257, 819, 24433, 2445, 13, 50946, 50946, 467, 4523, 484, 300, 257, 588, 2689, 3922, 295, 24433, 2445, 294, 18161, 9590, 307, 51196, 51196, 341, 2445, 13, 51277, 51277, 467, 1542, 411, 341, 13, 51344, 51344, 467, 1709, 11, 498, 710, 307, 341, 11, 550, 290, 295, 710, 307, 1958, 281, 264, 1411, 11, 293, 550, 341, 307, 2997, 1622, 11, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.1569615258110894, "compression_ratio": 1.645631067961165, "no_speech_prob": 1.9637243440229213e-06}, {"id": 23, "seek": 10732, "start": 114.67999999999999, "end": 118.96, "text": " instead swap in a different activation function.", "tokens": [50364, 759, 291, 528, 281, 2089, 257, 502, 11, 568, 281, 7263, 747, 322, 709, 4833, 3353, 4190, 11, 321, 393, 50732, 50732, 2602, 18135, 294, 257, 819, 24433, 2445, 13, 50946, 50946, 467, 4523, 484, 300, 257, 588, 2689, 3922, 295, 24433, 2445, 294, 18161, 9590, 307, 51196, 51196, 341, 2445, 13, 51277, 51277, 467, 1542, 411, 341, 13, 51344, 51344, 467, 1709, 11, 498, 710, 307, 341, 11, 550, 290, 295, 710, 307, 1958, 281, 264, 1411, 11, 293, 550, 341, 307, 2997, 1622, 11, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.1569615258110894, "compression_ratio": 1.645631067961165, "no_speech_prob": 1.9637243440229213e-06}, {"id": 24, "seek": 10732, "start": 118.96, "end": 123.96, "text": " It turns out that a very common choice of activation function in neural networks is", "tokens": [50364, 759, 291, 528, 281, 2089, 257, 502, 11, 568, 281, 7263, 747, 322, 709, 4833, 3353, 4190, 11, 321, 393, 50732, 50732, 2602, 18135, 294, 257, 819, 24433, 2445, 13, 50946, 50946, 467, 4523, 484, 300, 257, 588, 2689, 3922, 295, 24433, 2445, 294, 18161, 9590, 307, 51196, 51196, 341, 2445, 13, 51277, 51277, 467, 1542, 411, 341, 13, 51344, 51344, 467, 1709, 11, 498, 710, 307, 341, 11, 550, 290, 295, 710, 307, 1958, 281, 264, 1411, 11, 293, 550, 341, 307, 2997, 1622, 11, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.1569615258110894, "compression_ratio": 1.645631067961165, "no_speech_prob": 1.9637243440229213e-06}, {"id": 25, "seek": 10732, "start": 123.96, "end": 125.58, "text": " this function.", "tokens": [50364, 759, 291, 528, 281, 2089, 257, 502, 11, 568, 281, 7263, 747, 322, 709, 4833, 3353, 4190, 11, 321, 393, 50732, 50732, 2602, 18135, 294, 257, 819, 24433, 2445, 13, 50946, 50946, 467, 4523, 484, 300, 257, 588, 2689, 3922, 295, 24433, 2445, 294, 18161, 9590, 307, 51196, 51196, 341, 2445, 13, 51277, 51277, 467, 1542, 411, 341, 13, 51344, 51344, 467, 1709, 11, 498, 710, 307, 341, 11, 550, 290, 295, 710, 307, 1958, 281, 264, 1411, 11, 293, 550, 341, 307, 2997, 1622, 11, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.1569615258110894, "compression_ratio": 1.645631067961165, "no_speech_prob": 1.9637243440229213e-06}, {"id": 26, "seek": 10732, "start": 125.58, "end": 126.91999999999999, "text": " It looks like this.", "tokens": [50364, 759, 291, 528, 281, 2089, 257, 502, 11, 568, 281, 7263, 747, 322, 709, 4833, 3353, 4190, 11, 321, 393, 50732, 50732, 2602, 18135, 294, 257, 819, 24433, 2445, 13, 50946, 50946, 467, 4523, 484, 300, 257, 588, 2689, 3922, 295, 24433, 2445, 294, 18161, 9590, 307, 51196, 51196, 341, 2445, 13, 51277, 51277, 467, 1542, 411, 341, 13, 51344, 51344, 467, 1709, 11, 498, 710, 307, 341, 11, 550, 290, 295, 710, 307, 1958, 281, 264, 1411, 11, 293, 550, 341, 307, 2997, 1622, 11, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.1569615258110894, "compression_ratio": 1.645631067961165, "no_speech_prob": 1.9637243440229213e-06}, {"id": 27, "seek": 10732, "start": 126.91999999999999, "end": 136.28, "text": " It goes, if z is this, then g of z is 0 to the left, and then this is straight line,", "tokens": [50364, 759, 291, 528, 281, 2089, 257, 502, 11, 568, 281, 7263, 747, 322, 709, 4833, 3353, 4190, 11, 321, 393, 50732, 50732, 2602, 18135, 294, 257, 819, 24433, 2445, 13, 50946, 50946, 467, 4523, 484, 300, 257, 588, 2689, 3922, 295, 24433, 2445, 294, 18161, 9590, 307, 51196, 51196, 341, 2445, 13, 51277, 51277, 467, 1542, 411, 341, 13, 51344, 51344, 467, 1709, 11, 498, 710, 307, 341, 11, 550, 290, 295, 710, 307, 1958, 281, 264, 1411, 11, 293, 550, 341, 307, 2997, 1622, 11, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.1569615258110894, "compression_ratio": 1.645631067961165, "no_speech_prob": 1.9637243440229213e-06}, {"id": 28, "seek": 13628, "start": 136.28, "end": 139.84, "text": " 35 degrees to the right of 0.", "tokens": [50364, 6976, 5310, 281, 264, 558, 295, 1958, 13, 50542, 50542, 400, 370, 562, 710, 307, 5044, 813, 420, 2681, 281, 1958, 11, 290, 295, 710, 307, 445, 2681, 281, 710, 11, 300, 307, 281, 264, 50962, 50962, 558, 1922, 295, 341, 10686, 13, 51126, 51126, 400, 264, 18894, 5367, 337, 341, 307, 290, 295, 710, 6915, 11469, 295, 1958, 11, 710, 13, 51506, 51506], "temperature": 0.0, "avg_logprob": -0.14467263944221265, "compression_ratio": 1.5434782608695652, "no_speech_prob": 3.219111022190191e-05}, {"id": 29, "seek": 13628, "start": 139.84, "end": 148.24, "text": " And so when z is greater than or equal to 0, g of z is just equal to z, that is to the", "tokens": [50364, 6976, 5310, 281, 264, 558, 295, 1958, 13, 50542, 50542, 400, 370, 562, 710, 307, 5044, 813, 420, 2681, 281, 1958, 11, 290, 295, 710, 307, 445, 2681, 281, 710, 11, 300, 307, 281, 264, 50962, 50962, 558, 1922, 295, 341, 10686, 13, 51126, 51126, 400, 264, 18894, 5367, 337, 341, 307, 290, 295, 710, 6915, 11469, 295, 1958, 11, 710, 13, 51506, 51506], "temperature": 0.0, "avg_logprob": -0.14467263944221265, "compression_ratio": 1.5434782608695652, "no_speech_prob": 3.219111022190191e-05}, {"id": 30, "seek": 13628, "start": 148.24, "end": 151.52, "text": " right half of this diagram.", "tokens": [50364, 6976, 5310, 281, 264, 558, 295, 1958, 13, 50542, 50542, 400, 370, 562, 710, 307, 5044, 813, 420, 2681, 281, 1958, 11, 290, 295, 710, 307, 445, 2681, 281, 710, 11, 300, 307, 281, 264, 50962, 50962, 558, 1922, 295, 341, 10686, 13, 51126, 51126, 400, 264, 18894, 5367, 337, 341, 307, 290, 295, 710, 6915, 11469, 295, 1958, 11, 710, 13, 51506, 51506], "temperature": 0.0, "avg_logprob": -0.14467263944221265, "compression_ratio": 1.5434782608695652, "no_speech_prob": 3.219111022190191e-05}, {"id": 31, "seek": 13628, "start": 151.52, "end": 159.12, "text": " And the mathematical equation for this is g of z equals max of 0, z.", "tokens": [50364, 6976, 5310, 281, 264, 558, 295, 1958, 13, 50542, 50542, 400, 370, 562, 710, 307, 5044, 813, 420, 2681, 281, 1958, 11, 290, 295, 710, 307, 445, 2681, 281, 710, 11, 300, 307, 281, 264, 50962, 50962, 558, 1922, 295, 341, 10686, 13, 51126, 51126, 400, 264, 18894, 5367, 337, 341, 307, 290, 295, 710, 6915, 11469, 295, 1958, 11, 710, 13, 51506, 51506], "temperature": 0.0, "avg_logprob": -0.14467263944221265, "compression_ratio": 1.5434782608695652, "no_speech_prob": 3.219111022190191e-05}, {"id": 32, "seek": 15912, "start": 159.12, "end": 168.6, "text": " Be free to verify for yourself that max of 0, z results in this curve that I've drawn", "tokens": [50364, 879, 1737, 281, 16888, 337, 1803, 300, 11469, 295, 1958, 11, 710, 3542, 294, 341, 7605, 300, 286, 600, 10117, 50838, 50838, 670, 510, 13, 50913, 50913, 400, 498, 257, 502, 11, 568, 307, 290, 295, 710, 337, 341, 2158, 295, 710, 11, 550, 257, 11, 264, 24433, 2158, 11, 393, 586, 747, 51302, 51302, 322, 1958, 420, 604, 2107, 12, 28561, 1166, 2158, 13, 51553, 51553, 639, 24433, 2445, 575, 257, 1315, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.08827616618229793, "compression_ratio": 1.488235294117647, "no_speech_prob": 1.0451155503687914e-05}, {"id": 33, "seek": 15912, "start": 168.6, "end": 170.1, "text": " over here.", "tokens": [50364, 879, 1737, 281, 16888, 337, 1803, 300, 11469, 295, 1958, 11, 710, 3542, 294, 341, 7605, 300, 286, 600, 10117, 50838, 50838, 670, 510, 13, 50913, 50913, 400, 498, 257, 502, 11, 568, 307, 290, 295, 710, 337, 341, 2158, 295, 710, 11, 550, 257, 11, 264, 24433, 2158, 11, 393, 586, 747, 51302, 51302, 322, 1958, 420, 604, 2107, 12, 28561, 1166, 2158, 13, 51553, 51553, 639, 24433, 2445, 575, 257, 1315, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.08827616618229793, "compression_ratio": 1.488235294117647, "no_speech_prob": 1.0451155503687914e-05}, {"id": 34, "seek": 15912, "start": 170.1, "end": 177.88, "text": " And if a 1, 2 is g of z for this value of z, then a, the activation value, can now take", "tokens": [50364, 879, 1737, 281, 16888, 337, 1803, 300, 11469, 295, 1958, 11, 710, 3542, 294, 341, 7605, 300, 286, 600, 10117, 50838, 50838, 670, 510, 13, 50913, 50913, 400, 498, 257, 502, 11, 568, 307, 290, 295, 710, 337, 341, 2158, 295, 710, 11, 550, 257, 11, 264, 24433, 2158, 11, 393, 586, 747, 51302, 51302, 322, 1958, 420, 604, 2107, 12, 28561, 1166, 2158, 13, 51553, 51553, 639, 24433, 2445, 575, 257, 1315, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.08827616618229793, "compression_ratio": 1.488235294117647, "no_speech_prob": 1.0451155503687914e-05}, {"id": 35, "seek": 15912, "start": 177.88, "end": 182.9, "text": " on 0 or any non-negative value.", "tokens": [50364, 879, 1737, 281, 16888, 337, 1803, 300, 11469, 295, 1958, 11, 710, 3542, 294, 341, 7605, 300, 286, 600, 10117, 50838, 50838, 670, 510, 13, 50913, 50913, 400, 498, 257, 502, 11, 568, 307, 290, 295, 710, 337, 341, 2158, 295, 710, 11, 550, 257, 11, 264, 24433, 2158, 11, 393, 586, 747, 51302, 51302, 322, 1958, 420, 604, 2107, 12, 28561, 1166, 2158, 13, 51553, 51553, 639, 24433, 2445, 575, 257, 1315, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.08827616618229793, "compression_ratio": 1.488235294117647, "no_speech_prob": 1.0451155503687914e-05}, {"id": 36, "seek": 15912, "start": 182.9, "end": 185.52, "text": " This activation function has a name.", "tokens": [50364, 879, 1737, 281, 16888, 337, 1803, 300, 11469, 295, 1958, 11, 710, 3542, 294, 341, 7605, 300, 286, 600, 10117, 50838, 50838, 670, 510, 13, 50913, 50913, 400, 498, 257, 502, 11, 568, 307, 290, 295, 710, 337, 341, 2158, 295, 710, 11, 550, 257, 11, 264, 24433, 2158, 11, 393, 586, 747, 51302, 51302, 322, 1958, 420, 604, 2107, 12, 28561, 1166, 2158, 13, 51553, 51553, 639, 24433, 2445, 575, 257, 1315, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.08827616618229793, "compression_ratio": 1.488235294117647, "no_speech_prob": 1.0451155503687914e-05}, {"id": 37, "seek": 18552, "start": 185.52, "end": 190.12, "text": " It goes by the name ReLU with this funny capitalization.", "tokens": [50364, 467, 1709, 538, 264, 1315, 1300, 43, 52, 365, 341, 4074, 4238, 2144, 13, 50594, 50594, 400, 1300, 43, 52, 7382, 337, 11, 797, 11, 257, 8344, 10346, 1929, 1433, 11, 457, 309, 7382, 337, 11048, 2587, 8213, 4985, 13, 50928, 50928, 1468, 380, 3292, 886, 709, 466, 437, 11048, 2587, 1355, 293, 437, 8213, 4985, 1355, 13, 51097, 51097, 639, 390, 445, 257, 1315, 300, 264, 16552, 632, 2212, 281, 341, 1729, 24433, 2445, 51324, 51324, 562, 436, 1361, 493, 365, 309, 13, 51412, 51412, 583, 881, 561, 294, 2452, 2539, 445, 584, 1300, 43, 52, 281, 2864, 281, 341, 290, 295, 710, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1324452916416553, "compression_ratio": 1.6506024096385543, "no_speech_prob": 2.355074684601277e-05}, {"id": 38, "seek": 18552, "start": 190.12, "end": 196.8, "text": " And ReLU stands for, again, a somewhat arcane term, but it stands for rectified linear unit.", "tokens": [50364, 467, 1709, 538, 264, 1315, 1300, 43, 52, 365, 341, 4074, 4238, 2144, 13, 50594, 50594, 400, 1300, 43, 52, 7382, 337, 11, 797, 11, 257, 8344, 10346, 1929, 1433, 11, 457, 309, 7382, 337, 11048, 2587, 8213, 4985, 13, 50928, 50928, 1468, 380, 3292, 886, 709, 466, 437, 11048, 2587, 1355, 293, 437, 8213, 4985, 1355, 13, 51097, 51097, 639, 390, 445, 257, 1315, 300, 264, 16552, 632, 2212, 281, 341, 1729, 24433, 2445, 51324, 51324, 562, 436, 1361, 493, 365, 309, 13, 51412, 51412, 583, 881, 561, 294, 2452, 2539, 445, 584, 1300, 43, 52, 281, 2864, 281, 341, 290, 295, 710, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1324452916416553, "compression_ratio": 1.6506024096385543, "no_speech_prob": 2.355074684601277e-05}, {"id": 39, "seek": 18552, "start": 196.8, "end": 200.18, "text": " Don't worry too much about what rectified means and what linear unit means.", "tokens": [50364, 467, 1709, 538, 264, 1315, 1300, 43, 52, 365, 341, 4074, 4238, 2144, 13, 50594, 50594, 400, 1300, 43, 52, 7382, 337, 11, 797, 11, 257, 8344, 10346, 1929, 1433, 11, 457, 309, 7382, 337, 11048, 2587, 8213, 4985, 13, 50928, 50928, 1468, 380, 3292, 886, 709, 466, 437, 11048, 2587, 1355, 293, 437, 8213, 4985, 1355, 13, 51097, 51097, 639, 390, 445, 257, 1315, 300, 264, 16552, 632, 2212, 281, 341, 1729, 24433, 2445, 51324, 51324, 562, 436, 1361, 493, 365, 309, 13, 51412, 51412, 583, 881, 561, 294, 2452, 2539, 445, 584, 1300, 43, 52, 281, 2864, 281, 341, 290, 295, 710, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1324452916416553, "compression_ratio": 1.6506024096385543, "no_speech_prob": 2.355074684601277e-05}, {"id": 40, "seek": 18552, "start": 200.18, "end": 204.72, "text": " This was just a name that the authors had given to this particular activation function", "tokens": [50364, 467, 1709, 538, 264, 1315, 1300, 43, 52, 365, 341, 4074, 4238, 2144, 13, 50594, 50594, 400, 1300, 43, 52, 7382, 337, 11, 797, 11, 257, 8344, 10346, 1929, 1433, 11, 457, 309, 7382, 337, 11048, 2587, 8213, 4985, 13, 50928, 50928, 1468, 380, 3292, 886, 709, 466, 437, 11048, 2587, 1355, 293, 437, 8213, 4985, 1355, 13, 51097, 51097, 639, 390, 445, 257, 1315, 300, 264, 16552, 632, 2212, 281, 341, 1729, 24433, 2445, 51324, 51324, 562, 436, 1361, 493, 365, 309, 13, 51412, 51412, 583, 881, 561, 294, 2452, 2539, 445, 584, 1300, 43, 52, 281, 2864, 281, 341, 290, 295, 710, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1324452916416553, "compression_ratio": 1.6506024096385543, "no_speech_prob": 2.355074684601277e-05}, {"id": 41, "seek": 18552, "start": 204.72, "end": 206.48000000000002, "text": " when they came up with it.", "tokens": [50364, 467, 1709, 538, 264, 1315, 1300, 43, 52, 365, 341, 4074, 4238, 2144, 13, 50594, 50594, 400, 1300, 43, 52, 7382, 337, 11, 797, 11, 257, 8344, 10346, 1929, 1433, 11, 457, 309, 7382, 337, 11048, 2587, 8213, 4985, 13, 50928, 50928, 1468, 380, 3292, 886, 709, 466, 437, 11048, 2587, 1355, 293, 437, 8213, 4985, 1355, 13, 51097, 51097, 639, 390, 445, 257, 1315, 300, 264, 16552, 632, 2212, 281, 341, 1729, 24433, 2445, 51324, 51324, 562, 436, 1361, 493, 365, 309, 13, 51412, 51412, 583, 881, 561, 294, 2452, 2539, 445, 584, 1300, 43, 52, 281, 2864, 281, 341, 290, 295, 710, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1324452916416553, "compression_ratio": 1.6506024096385543, "no_speech_prob": 2.355074684601277e-05}, {"id": 42, "seek": 18552, "start": 206.48000000000002, "end": 212.72, "text": " But most people in deep learning just say ReLU to refer to this g of z.", "tokens": [50364, 467, 1709, 538, 264, 1315, 1300, 43, 52, 365, 341, 4074, 4238, 2144, 13, 50594, 50594, 400, 1300, 43, 52, 7382, 337, 11, 797, 11, 257, 8344, 10346, 1929, 1433, 11, 457, 309, 7382, 337, 11048, 2587, 8213, 4985, 13, 50928, 50928, 1468, 380, 3292, 886, 709, 466, 437, 11048, 2587, 1355, 293, 437, 8213, 4985, 1355, 13, 51097, 51097, 639, 390, 445, 257, 1315, 300, 264, 16552, 632, 2212, 281, 341, 1729, 24433, 2445, 51324, 51324, 562, 436, 1361, 493, 365, 309, 13, 51412, 51412, 583, 881, 561, 294, 2452, 2539, 445, 584, 1300, 43, 52, 281, 2864, 281, 341, 290, 295, 710, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1324452916416553, "compression_ratio": 1.6506024096385543, "no_speech_prob": 2.355074684601277e-05}, {"id": 43, "seek": 21272, "start": 212.72, "end": 217.92, "text": " More generally, you have a choice of what to use for g of z.", "tokens": [50364, 5048, 5101, 11, 291, 362, 257, 3922, 295, 437, 281, 764, 337, 290, 295, 710, 13, 50624, 50624, 400, 2171, 321, 603, 764, 257, 819, 3922, 813, 264, 4556, 3280, 327, 24433, 2445, 13, 50882, 50882, 1692, 366, 264, 881, 12719, 1143, 24433, 6828, 13, 51106, 51106, 509, 1866, 264, 4556, 3280, 327, 24433, 2445, 11, 290, 295, 710, 6915, 264, 4556, 3280, 327, 2445, 13, 51370, 51370, 1282, 264, 1036, 4137, 11, 321, 445, 2956, 412, 264, 1300, 43, 52, 11, 420, 11048, 2587, 8213, 4985, 11, 290, 295, 710, 6915, 11469, 51650, 51650, 295, 1958, 11, 710, 13, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.1031562823515672, "compression_ratio": 1.7464788732394365, "no_speech_prob": 7.410987109324196e-06}, {"id": 44, "seek": 21272, "start": 217.92, "end": 223.07999999999998, "text": " And sometimes we'll use a different choice than the sigmoid activation function.", "tokens": [50364, 5048, 5101, 11, 291, 362, 257, 3922, 295, 437, 281, 764, 337, 290, 295, 710, 13, 50624, 50624, 400, 2171, 321, 603, 764, 257, 819, 3922, 813, 264, 4556, 3280, 327, 24433, 2445, 13, 50882, 50882, 1692, 366, 264, 881, 12719, 1143, 24433, 6828, 13, 51106, 51106, 509, 1866, 264, 4556, 3280, 327, 24433, 2445, 11, 290, 295, 710, 6915, 264, 4556, 3280, 327, 2445, 13, 51370, 51370, 1282, 264, 1036, 4137, 11, 321, 445, 2956, 412, 264, 1300, 43, 52, 11, 420, 11048, 2587, 8213, 4985, 11, 290, 295, 710, 6915, 11469, 51650, 51650, 295, 1958, 11, 710, 13, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.1031562823515672, "compression_ratio": 1.7464788732394365, "no_speech_prob": 7.410987109324196e-06}, {"id": 45, "seek": 21272, "start": 223.07999999999998, "end": 227.56, "text": " Here are the most commonly used activation functions.", "tokens": [50364, 5048, 5101, 11, 291, 362, 257, 3922, 295, 437, 281, 764, 337, 290, 295, 710, 13, 50624, 50624, 400, 2171, 321, 603, 764, 257, 819, 3922, 813, 264, 4556, 3280, 327, 24433, 2445, 13, 50882, 50882, 1692, 366, 264, 881, 12719, 1143, 24433, 6828, 13, 51106, 51106, 509, 1866, 264, 4556, 3280, 327, 24433, 2445, 11, 290, 295, 710, 6915, 264, 4556, 3280, 327, 2445, 13, 51370, 51370, 1282, 264, 1036, 4137, 11, 321, 445, 2956, 412, 264, 1300, 43, 52, 11, 420, 11048, 2587, 8213, 4985, 11, 290, 295, 710, 6915, 11469, 51650, 51650, 295, 1958, 11, 710, 13, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.1031562823515672, "compression_ratio": 1.7464788732394365, "no_speech_prob": 7.410987109324196e-06}, {"id": 46, "seek": 21272, "start": 227.56, "end": 232.84, "text": " You saw the sigmoid activation function, g of z equals the sigmoid function.", "tokens": [50364, 5048, 5101, 11, 291, 362, 257, 3922, 295, 437, 281, 764, 337, 290, 295, 710, 13, 50624, 50624, 400, 2171, 321, 603, 764, 257, 819, 3922, 813, 264, 4556, 3280, 327, 24433, 2445, 13, 50882, 50882, 1692, 366, 264, 881, 12719, 1143, 24433, 6828, 13, 51106, 51106, 509, 1866, 264, 4556, 3280, 327, 24433, 2445, 11, 290, 295, 710, 6915, 264, 4556, 3280, 327, 2445, 13, 51370, 51370, 1282, 264, 1036, 4137, 11, 321, 445, 2956, 412, 264, 1300, 43, 52, 11, 420, 11048, 2587, 8213, 4985, 11, 290, 295, 710, 6915, 11469, 51650, 51650, 295, 1958, 11, 710, 13, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.1031562823515672, "compression_ratio": 1.7464788732394365, "no_speech_prob": 7.410987109324196e-06}, {"id": 47, "seek": 21272, "start": 232.84, "end": 238.44, "text": " On the last slide, we just looked at the ReLU, or rectified linear unit, g of z equals max", "tokens": [50364, 5048, 5101, 11, 291, 362, 257, 3922, 295, 437, 281, 764, 337, 290, 295, 710, 13, 50624, 50624, 400, 2171, 321, 603, 764, 257, 819, 3922, 813, 264, 4556, 3280, 327, 24433, 2445, 13, 50882, 50882, 1692, 366, 264, 881, 12719, 1143, 24433, 6828, 13, 51106, 51106, 509, 1866, 264, 4556, 3280, 327, 24433, 2445, 11, 290, 295, 710, 6915, 264, 4556, 3280, 327, 2445, 13, 51370, 51370, 1282, 264, 1036, 4137, 11, 321, 445, 2956, 412, 264, 1300, 43, 52, 11, 420, 11048, 2587, 8213, 4985, 11, 290, 295, 710, 6915, 11469, 51650, 51650, 295, 1958, 11, 710, 13, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.1031562823515672, "compression_ratio": 1.7464788732394365, "no_speech_prob": 7.410987109324196e-06}, {"id": 48, "seek": 21272, "start": 238.44, "end": 240.6, "text": " of 0, z.", "tokens": [50364, 5048, 5101, 11, 291, 362, 257, 3922, 295, 437, 281, 764, 337, 290, 295, 710, 13, 50624, 50624, 400, 2171, 321, 603, 764, 257, 819, 3922, 813, 264, 4556, 3280, 327, 24433, 2445, 13, 50882, 50882, 1692, 366, 264, 881, 12719, 1143, 24433, 6828, 13, 51106, 51106, 509, 1866, 264, 4556, 3280, 327, 24433, 2445, 11, 290, 295, 710, 6915, 264, 4556, 3280, 327, 2445, 13, 51370, 51370, 1282, 264, 1036, 4137, 11, 321, 445, 2956, 412, 264, 1300, 43, 52, 11, 420, 11048, 2587, 8213, 4985, 11, 290, 295, 710, 6915, 11469, 51650, 51650, 295, 1958, 11, 710, 13, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.1031562823515672, "compression_ratio": 1.7464788732394365, "no_speech_prob": 7.410987109324196e-06}, {"id": 49, "seek": 24060, "start": 240.6, "end": 244.92, "text": " There's one other activation function which is worth mentioning, which is called the linear", "tokens": [50364, 821, 311, 472, 661, 24433, 2445, 597, 307, 3163, 18315, 11, 597, 307, 1219, 264, 8213, 50580, 50580, 24433, 2445, 11, 597, 307, 445, 290, 295, 710, 6915, 281, 710, 13, 50856, 50856, 4803, 498, 291, 764, 264, 8213, 24433, 2445, 11, 561, 486, 584, 321, 434, 406, 1228, 51058, 51058, 604, 24433, 2445, 570, 498, 257, 307, 290, 295, 710, 11, 689, 290, 295, 710, 6915, 710, 11, 550, 257, 307, 445, 51452, 51452, 2681, 281, 341, 11, 261, 5893, 2031, 1804, 272, 11, 584, 13, 51652, 51652, 400, 370, 309, 311, 382, 498, 456, 390, 572, 290, 294, 456, 412, 439, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11577689206158673, "compression_ratio": 1.9099526066350712, "no_speech_prob": 2.586661503301002e-05}, {"id": 50, "seek": 24060, "start": 244.92, "end": 250.44, "text": " activation function, which is just g of z equals to z.", "tokens": [50364, 821, 311, 472, 661, 24433, 2445, 597, 307, 3163, 18315, 11, 597, 307, 1219, 264, 8213, 50580, 50580, 24433, 2445, 11, 597, 307, 445, 290, 295, 710, 6915, 281, 710, 13, 50856, 50856, 4803, 498, 291, 764, 264, 8213, 24433, 2445, 11, 561, 486, 584, 321, 434, 406, 1228, 51058, 51058, 604, 24433, 2445, 570, 498, 257, 307, 290, 295, 710, 11, 689, 290, 295, 710, 6915, 710, 11, 550, 257, 307, 445, 51452, 51452, 2681, 281, 341, 11, 261, 5893, 2031, 1804, 272, 11, 584, 13, 51652, 51652, 400, 370, 309, 311, 382, 498, 456, 390, 572, 290, 294, 456, 412, 439, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11577689206158673, "compression_ratio": 1.9099526066350712, "no_speech_prob": 2.586661503301002e-05}, {"id": 51, "seek": 24060, "start": 250.44, "end": 254.48, "text": " Sometimes if you use the linear activation function, people will say we're not using", "tokens": [50364, 821, 311, 472, 661, 24433, 2445, 597, 307, 3163, 18315, 11, 597, 307, 1219, 264, 8213, 50580, 50580, 24433, 2445, 11, 597, 307, 445, 290, 295, 710, 6915, 281, 710, 13, 50856, 50856, 4803, 498, 291, 764, 264, 8213, 24433, 2445, 11, 561, 486, 584, 321, 434, 406, 1228, 51058, 51058, 604, 24433, 2445, 570, 498, 257, 307, 290, 295, 710, 11, 689, 290, 295, 710, 6915, 710, 11, 550, 257, 307, 445, 51452, 51452, 2681, 281, 341, 11, 261, 5893, 2031, 1804, 272, 11, 584, 13, 51652, 51652, 400, 370, 309, 311, 382, 498, 456, 390, 572, 290, 294, 456, 412, 439, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11577689206158673, "compression_ratio": 1.9099526066350712, "no_speech_prob": 2.586661503301002e-05}, {"id": 52, "seek": 24060, "start": 254.48, "end": 262.36, "text": " any activation function because if a is g of z, where g of z equals z, then a is just", "tokens": [50364, 821, 311, 472, 661, 24433, 2445, 597, 307, 3163, 18315, 11, 597, 307, 1219, 264, 8213, 50580, 50580, 24433, 2445, 11, 597, 307, 445, 290, 295, 710, 6915, 281, 710, 13, 50856, 50856, 4803, 498, 291, 764, 264, 8213, 24433, 2445, 11, 561, 486, 584, 321, 434, 406, 1228, 51058, 51058, 604, 24433, 2445, 570, 498, 257, 307, 290, 295, 710, 11, 689, 290, 295, 710, 6915, 710, 11, 550, 257, 307, 445, 51452, 51452, 2681, 281, 341, 11, 261, 5893, 2031, 1804, 272, 11, 584, 13, 51652, 51652, 400, 370, 309, 311, 382, 498, 456, 390, 572, 290, 294, 456, 412, 439, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11577689206158673, "compression_ratio": 1.9099526066350712, "no_speech_prob": 2.586661503301002e-05}, {"id": 53, "seek": 24060, "start": 262.36, "end": 266.36, "text": " equal to this, w dot x plus b, say.", "tokens": [50364, 821, 311, 472, 661, 24433, 2445, 597, 307, 3163, 18315, 11, 597, 307, 1219, 264, 8213, 50580, 50580, 24433, 2445, 11, 597, 307, 445, 290, 295, 710, 6915, 281, 710, 13, 50856, 50856, 4803, 498, 291, 764, 264, 8213, 24433, 2445, 11, 561, 486, 584, 321, 434, 406, 1228, 51058, 51058, 604, 24433, 2445, 570, 498, 257, 307, 290, 295, 710, 11, 689, 290, 295, 710, 6915, 710, 11, 550, 257, 307, 445, 51452, 51452, 2681, 281, 341, 11, 261, 5893, 2031, 1804, 272, 11, 584, 13, 51652, 51652, 400, 370, 309, 311, 382, 498, 456, 390, 572, 290, 294, 456, 412, 439, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11577689206158673, "compression_ratio": 1.9099526066350712, "no_speech_prob": 2.586661503301002e-05}, {"id": 54, "seek": 24060, "start": 266.36, "end": 269.36, "text": " And so it's as if there was no g in there at all.", "tokens": [50364, 821, 311, 472, 661, 24433, 2445, 597, 307, 3163, 18315, 11, 597, 307, 1219, 264, 8213, 50580, 50580, 24433, 2445, 11, 597, 307, 445, 290, 295, 710, 6915, 281, 710, 13, 50856, 50856, 4803, 498, 291, 764, 264, 8213, 24433, 2445, 11, 561, 486, 584, 321, 434, 406, 1228, 51058, 51058, 604, 24433, 2445, 570, 498, 257, 307, 290, 295, 710, 11, 689, 290, 295, 710, 6915, 710, 11, 550, 257, 307, 445, 51452, 51452, 2681, 281, 341, 11, 261, 5893, 2031, 1804, 272, 11, 584, 13, 51652, 51652, 400, 370, 309, 311, 382, 498, 456, 390, 572, 290, 294, 456, 412, 439, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11577689206158673, "compression_ratio": 1.9099526066350712, "no_speech_prob": 2.586661503301002e-05}, {"id": 55, "seek": 26936, "start": 269.36, "end": 275.64, "text": " So when you are using this linear activation function, g of z, sometimes people will say,", "tokens": [50364, 407, 562, 291, 366, 1228, 341, 8213, 24433, 2445, 11, 290, 295, 710, 11, 2171, 561, 486, 584, 11, 50678, 50678, 731, 11, 321, 434, 406, 1228, 604, 24433, 2445, 13, 50826, 50826, 5780, 294, 341, 25925, 11, 286, 486, 2864, 281, 1228, 264, 8213, 24433, 2445, 2831, 813, 51078, 51078, 572, 24433, 2445, 13, 51172, 51172, 583, 498, 291, 1568, 1580, 1646, 764, 300, 27575, 11, 300, 311, 437, 436, 914, 13, 51318, 51318, 467, 445, 14942, 281, 264, 8213, 24433, 2445, 13, 51487, 51487, 400, 613, 1045, 366, 1391, 538, 1400, 264, 881, 12719, 1143, 24433, 6828, 294, 18161, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.13183323996407645, "compression_ratio": 1.974468085106383, "no_speech_prob": 1.034845013236918e-06}, {"id": 56, "seek": 26936, "start": 275.64, "end": 278.6, "text": " well, we're not using any activation function.", "tokens": [50364, 407, 562, 291, 366, 1228, 341, 8213, 24433, 2445, 11, 290, 295, 710, 11, 2171, 561, 486, 584, 11, 50678, 50678, 731, 11, 321, 434, 406, 1228, 604, 24433, 2445, 13, 50826, 50826, 5780, 294, 341, 25925, 11, 286, 486, 2864, 281, 1228, 264, 8213, 24433, 2445, 2831, 813, 51078, 51078, 572, 24433, 2445, 13, 51172, 51172, 583, 498, 291, 1568, 1580, 1646, 764, 300, 27575, 11, 300, 311, 437, 436, 914, 13, 51318, 51318, 467, 445, 14942, 281, 264, 8213, 24433, 2445, 13, 51487, 51487, 400, 613, 1045, 366, 1391, 538, 1400, 264, 881, 12719, 1143, 24433, 6828, 294, 18161, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.13183323996407645, "compression_ratio": 1.974468085106383, "no_speech_prob": 1.034845013236918e-06}, {"id": 57, "seek": 26936, "start": 278.6, "end": 283.64, "text": " Although in this clause, I will refer to using the linear activation function rather than", "tokens": [50364, 407, 562, 291, 366, 1228, 341, 8213, 24433, 2445, 11, 290, 295, 710, 11, 2171, 561, 486, 584, 11, 50678, 50678, 731, 11, 321, 434, 406, 1228, 604, 24433, 2445, 13, 50826, 50826, 5780, 294, 341, 25925, 11, 286, 486, 2864, 281, 1228, 264, 8213, 24433, 2445, 2831, 813, 51078, 51078, 572, 24433, 2445, 13, 51172, 51172, 583, 498, 291, 1568, 1580, 1646, 764, 300, 27575, 11, 300, 311, 437, 436, 914, 13, 51318, 51318, 467, 445, 14942, 281, 264, 8213, 24433, 2445, 13, 51487, 51487, 400, 613, 1045, 366, 1391, 538, 1400, 264, 881, 12719, 1143, 24433, 6828, 294, 18161, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.13183323996407645, "compression_ratio": 1.974468085106383, "no_speech_prob": 1.034845013236918e-06}, {"id": 58, "seek": 26936, "start": 283.64, "end": 285.52000000000004, "text": " no activation function.", "tokens": [50364, 407, 562, 291, 366, 1228, 341, 8213, 24433, 2445, 11, 290, 295, 710, 11, 2171, 561, 486, 584, 11, 50678, 50678, 731, 11, 321, 434, 406, 1228, 604, 24433, 2445, 13, 50826, 50826, 5780, 294, 341, 25925, 11, 286, 486, 2864, 281, 1228, 264, 8213, 24433, 2445, 2831, 813, 51078, 51078, 572, 24433, 2445, 13, 51172, 51172, 583, 498, 291, 1568, 1580, 1646, 764, 300, 27575, 11, 300, 311, 437, 436, 914, 13, 51318, 51318, 467, 445, 14942, 281, 264, 8213, 24433, 2445, 13, 51487, 51487, 400, 613, 1045, 366, 1391, 538, 1400, 264, 881, 12719, 1143, 24433, 6828, 294, 18161, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.13183323996407645, "compression_ratio": 1.974468085106383, "no_speech_prob": 1.034845013236918e-06}, {"id": 59, "seek": 26936, "start": 285.52000000000004, "end": 288.44, "text": " But if you hear someone else use that terminology, that's what they mean.", "tokens": [50364, 407, 562, 291, 366, 1228, 341, 8213, 24433, 2445, 11, 290, 295, 710, 11, 2171, 561, 486, 584, 11, 50678, 50678, 731, 11, 321, 434, 406, 1228, 604, 24433, 2445, 13, 50826, 50826, 5780, 294, 341, 25925, 11, 286, 486, 2864, 281, 1228, 264, 8213, 24433, 2445, 2831, 813, 51078, 51078, 572, 24433, 2445, 13, 51172, 51172, 583, 498, 291, 1568, 1580, 1646, 764, 300, 27575, 11, 300, 311, 437, 436, 914, 13, 51318, 51318, 467, 445, 14942, 281, 264, 8213, 24433, 2445, 13, 51487, 51487, 400, 613, 1045, 366, 1391, 538, 1400, 264, 881, 12719, 1143, 24433, 6828, 294, 18161, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.13183323996407645, "compression_ratio": 1.974468085106383, "no_speech_prob": 1.034845013236918e-06}, {"id": 60, "seek": 26936, "start": 288.44, "end": 291.82, "text": " It just refers to the linear activation function.", "tokens": [50364, 407, 562, 291, 366, 1228, 341, 8213, 24433, 2445, 11, 290, 295, 710, 11, 2171, 561, 486, 584, 11, 50678, 50678, 731, 11, 321, 434, 406, 1228, 604, 24433, 2445, 13, 50826, 50826, 5780, 294, 341, 25925, 11, 286, 486, 2864, 281, 1228, 264, 8213, 24433, 2445, 2831, 813, 51078, 51078, 572, 24433, 2445, 13, 51172, 51172, 583, 498, 291, 1568, 1580, 1646, 764, 300, 27575, 11, 300, 311, 437, 436, 914, 13, 51318, 51318, 467, 445, 14942, 281, 264, 8213, 24433, 2445, 13, 51487, 51487, 400, 613, 1045, 366, 1391, 538, 1400, 264, 881, 12719, 1143, 24433, 6828, 294, 18161, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.13183323996407645, "compression_ratio": 1.974468085106383, "no_speech_prob": 1.034845013236918e-06}, {"id": 61, "seek": 26936, "start": 291.82, "end": 298.32, "text": " And these three are probably by far the most commonly used activation functions in neural", "tokens": [50364, 407, 562, 291, 366, 1228, 341, 8213, 24433, 2445, 11, 290, 295, 710, 11, 2171, 561, 486, 584, 11, 50678, 50678, 731, 11, 321, 434, 406, 1228, 604, 24433, 2445, 13, 50826, 50826, 5780, 294, 341, 25925, 11, 286, 486, 2864, 281, 1228, 264, 8213, 24433, 2445, 2831, 813, 51078, 51078, 572, 24433, 2445, 13, 51172, 51172, 583, 498, 291, 1568, 1580, 1646, 764, 300, 27575, 11, 300, 311, 437, 436, 914, 13, 51318, 51318, 467, 445, 14942, 281, 264, 8213, 24433, 2445, 13, 51487, 51487, 400, 613, 1045, 366, 1391, 538, 1400, 264, 881, 12719, 1143, 24433, 6828, 294, 18161, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.13183323996407645, "compression_ratio": 1.974468085106383, "no_speech_prob": 1.034845013236918e-06}, {"id": 62, "seek": 29832, "start": 298.32, "end": 299.64, "text": " networks.", "tokens": [50364, 9590, 13, 50430, 50430, 11965, 341, 1243, 11, 321, 603, 2557, 322, 264, 6409, 472, 1219, 264, 2787, 41167, 24433, 2445, 13, 50692, 50692, 583, 365, 613, 24433, 6828, 11, 291, 603, 312, 1075, 281, 1322, 257, 4593, 5673, 295, 4005, 50988, 50988, 18161, 9590, 13, 51072, 51072, 407, 562, 2390, 257, 18161, 3209, 11, 337, 1184, 34090, 11, 360, 291, 528, 281, 764, 264, 4556, 3280, 327, 24433, 51360, 51360, 2445, 420, 264, 1300, 43, 52, 24433, 2445, 420, 257, 8213, 24433, 2445, 30, 51631, 51631, 1012, 360, 291, 2826, 1296, 613, 819, 24433, 6828, 30, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.14411573126764582, "compression_ratio": 1.907488986784141, "no_speech_prob": 9.08023866941221e-06}, {"id": 63, "seek": 29832, "start": 299.64, "end": 304.88, "text": " Later this week, we'll touch on the fourth one called the softmax activation function.", "tokens": [50364, 9590, 13, 50430, 50430, 11965, 341, 1243, 11, 321, 603, 2557, 322, 264, 6409, 472, 1219, 264, 2787, 41167, 24433, 2445, 13, 50692, 50692, 583, 365, 613, 24433, 6828, 11, 291, 603, 312, 1075, 281, 1322, 257, 4593, 5673, 295, 4005, 50988, 50988, 18161, 9590, 13, 51072, 51072, 407, 562, 2390, 257, 18161, 3209, 11, 337, 1184, 34090, 11, 360, 291, 528, 281, 764, 264, 4556, 3280, 327, 24433, 51360, 51360, 2445, 420, 264, 1300, 43, 52, 24433, 2445, 420, 257, 8213, 24433, 2445, 30, 51631, 51631, 1012, 360, 291, 2826, 1296, 613, 819, 24433, 6828, 30, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.14411573126764582, "compression_ratio": 1.907488986784141, "no_speech_prob": 9.08023866941221e-06}, {"id": 64, "seek": 29832, "start": 304.88, "end": 310.8, "text": " But with these activation functions, you'll be able to build a rich variety of powerful", "tokens": [50364, 9590, 13, 50430, 50430, 11965, 341, 1243, 11, 321, 603, 2557, 322, 264, 6409, 472, 1219, 264, 2787, 41167, 24433, 2445, 13, 50692, 50692, 583, 365, 613, 24433, 6828, 11, 291, 603, 312, 1075, 281, 1322, 257, 4593, 5673, 295, 4005, 50988, 50988, 18161, 9590, 13, 51072, 51072, 407, 562, 2390, 257, 18161, 3209, 11, 337, 1184, 34090, 11, 360, 291, 528, 281, 764, 264, 4556, 3280, 327, 24433, 51360, 51360, 2445, 420, 264, 1300, 43, 52, 24433, 2445, 420, 257, 8213, 24433, 2445, 30, 51631, 51631, 1012, 360, 291, 2826, 1296, 613, 819, 24433, 6828, 30, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.14411573126764582, "compression_ratio": 1.907488986784141, "no_speech_prob": 9.08023866941221e-06}, {"id": 65, "seek": 29832, "start": 310.8, "end": 312.48, "text": " neural networks.", "tokens": [50364, 9590, 13, 50430, 50430, 11965, 341, 1243, 11, 321, 603, 2557, 322, 264, 6409, 472, 1219, 264, 2787, 41167, 24433, 2445, 13, 50692, 50692, 583, 365, 613, 24433, 6828, 11, 291, 603, 312, 1075, 281, 1322, 257, 4593, 5673, 295, 4005, 50988, 50988, 18161, 9590, 13, 51072, 51072, 407, 562, 2390, 257, 18161, 3209, 11, 337, 1184, 34090, 11, 360, 291, 528, 281, 764, 264, 4556, 3280, 327, 24433, 51360, 51360, 2445, 420, 264, 1300, 43, 52, 24433, 2445, 420, 257, 8213, 24433, 2445, 30, 51631, 51631, 1012, 360, 291, 2826, 1296, 613, 819, 24433, 6828, 30, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.14411573126764582, "compression_ratio": 1.907488986784141, "no_speech_prob": 9.08023866941221e-06}, {"id": 66, "seek": 29832, "start": 312.48, "end": 318.24, "text": " So when building a neural network, for each neuron, do you want to use the sigmoid activation", "tokens": [50364, 9590, 13, 50430, 50430, 11965, 341, 1243, 11, 321, 603, 2557, 322, 264, 6409, 472, 1219, 264, 2787, 41167, 24433, 2445, 13, 50692, 50692, 583, 365, 613, 24433, 6828, 11, 291, 603, 312, 1075, 281, 1322, 257, 4593, 5673, 295, 4005, 50988, 50988, 18161, 9590, 13, 51072, 51072, 407, 562, 2390, 257, 18161, 3209, 11, 337, 1184, 34090, 11, 360, 291, 528, 281, 764, 264, 4556, 3280, 327, 24433, 51360, 51360, 2445, 420, 264, 1300, 43, 52, 24433, 2445, 420, 257, 8213, 24433, 2445, 30, 51631, 51631, 1012, 360, 291, 2826, 1296, 613, 819, 24433, 6828, 30, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.14411573126764582, "compression_ratio": 1.907488986784141, "no_speech_prob": 9.08023866941221e-06}, {"id": 67, "seek": 29832, "start": 318.24, "end": 323.65999999999997, "text": " function or the ReLU activation function or a linear activation function?", "tokens": [50364, 9590, 13, 50430, 50430, 11965, 341, 1243, 11, 321, 603, 2557, 322, 264, 6409, 472, 1219, 264, 2787, 41167, 24433, 2445, 13, 50692, 50692, 583, 365, 613, 24433, 6828, 11, 291, 603, 312, 1075, 281, 1322, 257, 4593, 5673, 295, 4005, 50988, 50988, 18161, 9590, 13, 51072, 51072, 407, 562, 2390, 257, 18161, 3209, 11, 337, 1184, 34090, 11, 360, 291, 528, 281, 764, 264, 4556, 3280, 327, 24433, 51360, 51360, 2445, 420, 264, 1300, 43, 52, 24433, 2445, 420, 257, 8213, 24433, 2445, 30, 51631, 51631, 1012, 360, 291, 2826, 1296, 613, 819, 24433, 6828, 30, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.14411573126764582, "compression_ratio": 1.907488986784141, "no_speech_prob": 9.08023866941221e-06}, {"id": 68, "seek": 29832, "start": 323.65999999999997, "end": 327.24, "text": " How do you choose between these different activation functions?", "tokens": [50364, 9590, 13, 50430, 50430, 11965, 341, 1243, 11, 321, 603, 2557, 322, 264, 6409, 472, 1219, 264, 2787, 41167, 24433, 2445, 13, 50692, 50692, 583, 365, 613, 24433, 6828, 11, 291, 603, 312, 1075, 281, 1322, 257, 4593, 5673, 295, 4005, 50988, 50988, 18161, 9590, 13, 51072, 51072, 407, 562, 2390, 257, 18161, 3209, 11, 337, 1184, 34090, 11, 360, 291, 528, 281, 764, 264, 4556, 3280, 327, 24433, 51360, 51360, 2445, 420, 264, 1300, 43, 52, 24433, 2445, 420, 257, 8213, 24433, 2445, 30, 51631, 51631, 1012, 360, 291, 2826, 1296, 613, 819, 24433, 6828, 30, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.14411573126764582, "compression_ratio": 1.907488986784141, "no_speech_prob": 9.08023866941221e-06}, {"id": 69, "seek": 32724, "start": 327.24, "end": 328.96000000000004, "text": " Let's take a look at that in the next video.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 50450], "temperature": 0.0, "avg_logprob": -0.13729001681009928, "compression_ratio": 0.8979591836734694, "no_speech_prob": 0.0003561834164429456}], "language": "en", "video_id": "TQ388ISSoI4", "entity": "ML Specialization, Andrew Ng (2022)"}}