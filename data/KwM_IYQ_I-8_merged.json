{"video_id": "KwM_IYQ_I-8", "title": "6.3 Evaluating and choosing models | Model selection and training/cross validation/test sets-ML Ng", "description": "Second Course:\nAdvanced Learning Algorithms.\n\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 893, "views": 119, "publish_date": "11/04/2022", "timestamp": 1661817600, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " In the last video, you saw how to use a test set to evaluate the performance of a model. Let's make one further refinement to that idea in this video, which will allow you to use a technique to automatically choose a good model for your machine learning algorithm. One thing we've seen is that once the model's parameters w and b have been fit to the training set, the training error may not be a good indicator of how well the algorithm will do or how well it will generalize to new examples that were not in the training set. And in particular, for this example, the training error will be pretty much zero, and that's likely much lower than the actual generalization error. And by that I mean the average error on new examples that were not in the training set. And what you saw in the last video is that J-Test, the performance of the algorithm on examples is not trained on, that that will be a better indicator of how well the model will likely do on new data. And by that I mean other data that's not in the training set. Let's take a look at how this affects how we might use a test set to choose a model for a given machine learning application. So if fitting a function to predict housing prices or some other regression problem, one model you might consider is to fit a linear model like this. And this is a first order polynomial, and I'm going to use D equals one on this slide to denote fitting a one or first order polynomial. If you were to fit a model like this to your training set, you'd get some parameters W and B, and you can then compute J-Test to estimate how well this will generalize to new data. And on this slide, I'm going to use W1, B1, superscript there, to denote that these are the parameters you get if you were to fit a first order polynomial or degree one, D equals one polynomial. Now you might also consider fitting a second order polynomial or quadratic model. So this is the model. And if you were to fit this to your training set, you would get some parameters W2, B2, and you can then similarly evaluate those parameters on your test set and get J-Test W2, B2. And this would give you a sense of how well the second order polynomial does. And you can go on to try D equals three, that's a third order or a degree three polynomial that looks like this, and fit parameters, and similarly get J-Test. And you might keep doing this until, say, you try up to a 10th order polynomial and you end up with J-Test of W10, B10. That gives you a sense of how well the 10th order polynomial is doing. So one procedure you could try, this turns out not to be the best procedure, but one thing you could try is look at all of these J-Tests and see which one gives you the lowest value. And say you find that J-Test for the fifth order polynomial for W5, B5 turns out to be the lowest. If that's the case, then you might decide that the fifth order polynomial D equals five does best and choose that model for your application. And if you want to estimate how well this model performs, one thing you could do, but this turns out to be a slightly flawed procedure, is to report the test set error J-Test W5, B5. The reason this procedure is flawed is J-Test of W5, B5 is likely to be an optimistic estimate of the generalization error. In other words, it is likely to be lower than the actual generalization error. And the reason is, in the procedure we talked about on the slide, we basically fit one extra parameter, which is D, the degree of polynomial, and we chose this parameter using the test set. So on the previous slide, we saw that if you were to fit WB to the training data, then the training data would be an overly optimistic estimate of generalization error. And it turns out too, that if we were to choose the parameter D using the test set, then the test set J-Test is now an overly optimistic, that is lower than actual estimates of the generalization error. So the procedure on this particular slide is flawed and I don't recommend using this. Instead, if you want to automatically choose a model, such as decide what degree polynomial to use, here's how you modify the training and testing procedure in order to carry out model selection, whereby model selection, I mean, choosing amongst different models, such as these 10 different models that you might contemplate using for your machine learning application. The way we'll modify the procedure is, instead of splitting your data into just two subsets, the training set and the test set, we're going to split your data into three different subsets, which we're going to call the training set, the cross validation set, and then also the test set. So using our example from before of these 10 training examples, we might split it into putting 60% of the data into the training set. And so the notation we'll use for the training set portion will be the same as before, except that now mtrain, the number of training examples will be 6. And we might put 20% of the data into the cross validation set. And the notation I'm going to use is xCV of 1, yCV of 1 for the first cross validation example. So CV stands for cross validation, all the way down to xCV of mCV and yCV of mCV. So here mCV equals 2 in this example is the number of cross validation examples. And then finally, we have the test set same as before. So x1 through xm tests and y1 through ym tests, where m tests here is equal to 2. This is the number of test examples. We'll see on the next slide how to use the cross validation set. So the way we'll modify the procedure is you've already seen the training set and the test set. And we're going to introduce a new subset of the data called the cross validation set. The name cross validation refers to that this is an extra data set that we're going to use to check or cross check the validity or really the accuracy of different models. I don't think it's a great name, but that is what people in machine learning have gotten to call this extra data set. You may also hear people call this the validation set for short, just few syllables then cross validation. Or in some applications, people also call this the development set means basically the same thing. Or for short, sometimes you hear people call this the dev set, but all of these terms mean the same thing as cross validation set. I personally use the term dev set the most often because it's the shortest, fastest way to say it, but cross validation is probably used a little bit more often by machine learning practitioners. So armed with these three subsets of the data, training set, cross validation set and test set, you can then compute the training error, the cross validation error and the test error using these three formulas. Where as usual, none of these terms include the regularization term that is included in the training objective. And this new term in the middle, the cross validation error is just the average over your MCV cross validation examples of the average say squared error. And this term, in addition to being called cross validation error, is also commonly called the validation error for short or even the development set error or the dev error. Armed with these three measures of learning algorithm performance, this is how you can then go about carrying out model selection. You can with the 10 models, same as earlier on the slide with D equals one, D equals two, all the way up to a 10th degree or the 10th order polynomial, you can then fit the parameters W1, B1. But instead of evaluating this on your test set, you would instead evaluate these parameters on your cross validation sets and compute JCV of W1, B1. And similarly for the second model, you get JCV of W2, B2 and all the way down to JCV of W10, B10. Then in order to choose a model, you would look at which model has the lowest cross validation error. And concretely, let's say that JCV of W4, B4 is lowest, then what that means is you would pick this fourth order polynomial as the model you will use for this application. Finally, if you want to report out an estimate of the generalization error of how well this model will do on new data, you would do so using that third subset of your data, the test set and you report out J test of W4, B4. And you notice that throughout this entire procedure, you had fit these parameters using the training set, you then chose the parameter D or chose the degree of polynomial using the cross validation set. And so up until this point, you've not fit any parameters, either W or B or D to the test set. And that's why J test in this example will be a fair estimate of the generalization error of this model that has parameters W4, B4. So this gives a better procedure for model selection. And it lets you automatically make a decision like what order polynomial to choose for your linear regression model. This model selection procedure also works for choosing among other types of models. For example, choosing a neural network architecture. If you are fitting a model for handwritten digit recognition, you might consider three models like these, maybe even a larger set of models than just three, but here are a few different neural networks of small, somewhat larger and then even larger. To help you decide how many layers should your neural network have and how many hidden units per layer should you have. You can then train all three of these models and end up with parameters W1, B1 for the first model, W2, B2 for the second model and W3, B3 for the third model. And you can then evaluate the neural network's performance using JCV using your cross validation set. And with a classification problem, JCV can be the percentage of examples. And since this is a classification problem, JCV, the most common choice would be to compute this as a fraction of cross validation examples that the algorithm has misclassified. And you would compute this using all three models and then pick the model with the lowest cross validation error. So if in this example, this has the lowest cross validation error, you would then pick the second neural network and use parameters trained on this model. And finally, if you want to report out an estimate of the generalization error, you then use the test set to estimate how well the neural network that you just chose will do. So in machine learning practice, it's considered best practice to make all the decisions you want to make regarding your learning algorithm, such as how to choose parameters, what degree polynomial to use, but make decisions only looking at the training set and cross validation set and to not use the test set at all to make decisions about your model. And only after you've made all those decisions, then finally take the model you have designed and evaluated on your test set. And that procedure ensures that you haven't accidentally fit anything to the test set, so that your test set becomes still a fair and not overly optimistic estimate of the generalization error of your algorithm. So it's considered best practice in machine learning that if you have to make decisions about your model, such as fitting parameters or choosing the model architecture, such as neural network architecture or degree of polynomial if you're fitting linear regression, to make all those decisions only using your training set and your cross validation set and to not look at the test set at all while you're still making decisions regarding your learning algorithm. And it's only after you've come up with one model that's your final model to only then evaluate it on the test set. And because you haven't made any decisions using the test set, that ensures that your test set is a fair and not overly optimistic estimate of how well your model will generalize to new data. So that's model selection. And this is actually a very widely used procedure. I use this all the time to automatically choose what model to use for a given machine learning application. Now earlier this week, I mentioned running diagnostics to decide how to improve the performance of a learning algorithm. Now that you have a way to evaluate learning algorithms and even automatically choose a model, let's dive more deeply into examples of some diagnostics. The most powerful diagnostic that I know of and that I use for a lot of machine learning applications is one called bias and variance. Let's take a look at what that means in the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.68, "text": " In the last video, you saw how to use a test set to evaluate the performance of a model.", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 1866, 577, 281, 764, 257, 1500, 992, 281, 13059, 264, 3389, 295, 257, 2316, 13, 50748, 50748, 961, 311, 652, 472, 3052, 1895, 30229, 281, 300, 1558, 294, 341, 960, 11, 597, 486, 2089, 291, 281, 50954, 50954, 764, 257, 6532, 281, 6772, 2826, 257, 665, 2316, 337, 428, 3479, 2539, 9284, 13, 51231, 51231, 1485, 551, 321, 600, 1612, 307, 300, 1564, 264, 2316, 311, 9834, 261, 293, 272, 362, 668, 3318, 281, 264, 3097, 51540, 51540, 992, 11, 264, 3097, 6713, 815, 406, 312, 257, 665, 16961, 295, 577, 731, 264, 9284, 486, 360, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.13471491831653523, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.009122511371970177}, {"id": 1, "seek": 0, "start": 7.68, "end": 11.8, "text": " Let's make one further refinement to that idea in this video, which will allow you to", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 1866, 577, 281, 764, 257, 1500, 992, 281, 13059, 264, 3389, 295, 257, 2316, 13, 50748, 50748, 961, 311, 652, 472, 3052, 1895, 30229, 281, 300, 1558, 294, 341, 960, 11, 597, 486, 2089, 291, 281, 50954, 50954, 764, 257, 6532, 281, 6772, 2826, 257, 665, 2316, 337, 428, 3479, 2539, 9284, 13, 51231, 51231, 1485, 551, 321, 600, 1612, 307, 300, 1564, 264, 2316, 311, 9834, 261, 293, 272, 362, 668, 3318, 281, 264, 3097, 51540, 51540, 992, 11, 264, 3097, 6713, 815, 406, 312, 257, 665, 16961, 295, 577, 731, 264, 9284, 486, 360, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.13471491831653523, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.009122511371970177}, {"id": 2, "seek": 0, "start": 11.8, "end": 17.34, "text": " use a technique to automatically choose a good model for your machine learning algorithm.", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 1866, 577, 281, 764, 257, 1500, 992, 281, 13059, 264, 3389, 295, 257, 2316, 13, 50748, 50748, 961, 311, 652, 472, 3052, 1895, 30229, 281, 300, 1558, 294, 341, 960, 11, 597, 486, 2089, 291, 281, 50954, 50954, 764, 257, 6532, 281, 6772, 2826, 257, 665, 2316, 337, 428, 3479, 2539, 9284, 13, 51231, 51231, 1485, 551, 321, 600, 1612, 307, 300, 1564, 264, 2316, 311, 9834, 261, 293, 272, 362, 668, 3318, 281, 264, 3097, 51540, 51540, 992, 11, 264, 3097, 6713, 815, 406, 312, 257, 665, 16961, 295, 577, 731, 264, 9284, 486, 360, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.13471491831653523, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.009122511371970177}, {"id": 3, "seek": 0, "start": 17.34, "end": 23.52, "text": " One thing we've seen is that once the model's parameters w and b have been fit to the training", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 1866, 577, 281, 764, 257, 1500, 992, 281, 13059, 264, 3389, 295, 257, 2316, 13, 50748, 50748, 961, 311, 652, 472, 3052, 1895, 30229, 281, 300, 1558, 294, 341, 960, 11, 597, 486, 2089, 291, 281, 50954, 50954, 764, 257, 6532, 281, 6772, 2826, 257, 665, 2316, 337, 428, 3479, 2539, 9284, 13, 51231, 51231, 1485, 551, 321, 600, 1612, 307, 300, 1564, 264, 2316, 311, 9834, 261, 293, 272, 362, 668, 3318, 281, 264, 3097, 51540, 51540, 992, 11, 264, 3097, 6713, 815, 406, 312, 257, 665, 16961, 295, 577, 731, 264, 9284, 486, 360, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.13471491831653523, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.009122511371970177}, {"id": 4, "seek": 0, "start": 23.52, "end": 29.52, "text": " set, the training error may not be a good indicator of how well the algorithm will do", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 1866, 577, 281, 764, 257, 1500, 992, 281, 13059, 264, 3389, 295, 257, 2316, 13, 50748, 50748, 961, 311, 652, 472, 3052, 1895, 30229, 281, 300, 1558, 294, 341, 960, 11, 597, 486, 2089, 291, 281, 50954, 50954, 764, 257, 6532, 281, 6772, 2826, 257, 665, 2316, 337, 428, 3479, 2539, 9284, 13, 51231, 51231, 1485, 551, 321, 600, 1612, 307, 300, 1564, 264, 2316, 311, 9834, 261, 293, 272, 362, 668, 3318, 281, 264, 3097, 51540, 51540, 992, 11, 264, 3097, 6713, 815, 406, 312, 257, 665, 16961, 295, 577, 731, 264, 9284, 486, 360, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.13471491831653523, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.009122511371970177}, {"id": 5, "seek": 2952, "start": 29.52, "end": 34.8, "text": " or how well it will generalize to new examples that were not in the training set.", "tokens": [50364, 420, 577, 731, 309, 486, 2674, 1125, 281, 777, 5110, 300, 645, 406, 294, 264, 3097, 992, 13, 50628, 50628, 400, 294, 1729, 11, 337, 341, 1365, 11, 264, 3097, 6713, 486, 312, 1238, 709, 4018, 11, 293, 300, 311, 50916, 50916, 3700, 709, 3126, 813, 264, 3539, 2674, 2144, 6713, 13, 51122, 51122, 400, 538, 300, 286, 914, 264, 4274, 6713, 322, 777, 5110, 300, 645, 406, 294, 264, 3097, 992, 13, 51464, 51464, 400, 437, 291, 1866, 294, 264, 1036, 960, 307, 300, 508, 12, 51, 377, 11, 264, 3389, 295, 264, 9284, 322, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.13607688903808593, "compression_ratio": 1.9103773584905661, "no_speech_prob": 3.5910579754272476e-05}, {"id": 6, "seek": 2952, "start": 34.8, "end": 40.56, "text": " And in particular, for this example, the training error will be pretty much zero, and that's", "tokens": [50364, 420, 577, 731, 309, 486, 2674, 1125, 281, 777, 5110, 300, 645, 406, 294, 264, 3097, 992, 13, 50628, 50628, 400, 294, 1729, 11, 337, 341, 1365, 11, 264, 3097, 6713, 486, 312, 1238, 709, 4018, 11, 293, 300, 311, 50916, 50916, 3700, 709, 3126, 813, 264, 3539, 2674, 2144, 6713, 13, 51122, 51122, 400, 538, 300, 286, 914, 264, 4274, 6713, 322, 777, 5110, 300, 645, 406, 294, 264, 3097, 992, 13, 51464, 51464, 400, 437, 291, 1866, 294, 264, 1036, 960, 307, 300, 508, 12, 51, 377, 11, 264, 3389, 295, 264, 9284, 322, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.13607688903808593, "compression_ratio": 1.9103773584905661, "no_speech_prob": 3.5910579754272476e-05}, {"id": 7, "seek": 2952, "start": 40.56, "end": 44.68, "text": " likely much lower than the actual generalization error.", "tokens": [50364, 420, 577, 731, 309, 486, 2674, 1125, 281, 777, 5110, 300, 645, 406, 294, 264, 3097, 992, 13, 50628, 50628, 400, 294, 1729, 11, 337, 341, 1365, 11, 264, 3097, 6713, 486, 312, 1238, 709, 4018, 11, 293, 300, 311, 50916, 50916, 3700, 709, 3126, 813, 264, 3539, 2674, 2144, 6713, 13, 51122, 51122, 400, 538, 300, 286, 914, 264, 4274, 6713, 322, 777, 5110, 300, 645, 406, 294, 264, 3097, 992, 13, 51464, 51464, 400, 437, 291, 1866, 294, 264, 1036, 960, 307, 300, 508, 12, 51, 377, 11, 264, 3389, 295, 264, 9284, 322, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.13607688903808593, "compression_ratio": 1.9103773584905661, "no_speech_prob": 3.5910579754272476e-05}, {"id": 8, "seek": 2952, "start": 44.68, "end": 51.519999999999996, "text": " And by that I mean the average error on new examples that were not in the training set.", "tokens": [50364, 420, 577, 731, 309, 486, 2674, 1125, 281, 777, 5110, 300, 645, 406, 294, 264, 3097, 992, 13, 50628, 50628, 400, 294, 1729, 11, 337, 341, 1365, 11, 264, 3097, 6713, 486, 312, 1238, 709, 4018, 11, 293, 300, 311, 50916, 50916, 3700, 709, 3126, 813, 264, 3539, 2674, 2144, 6713, 13, 51122, 51122, 400, 538, 300, 286, 914, 264, 4274, 6713, 322, 777, 5110, 300, 645, 406, 294, 264, 3097, 992, 13, 51464, 51464, 400, 437, 291, 1866, 294, 264, 1036, 960, 307, 300, 508, 12, 51, 377, 11, 264, 3389, 295, 264, 9284, 322, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.13607688903808593, "compression_ratio": 1.9103773584905661, "no_speech_prob": 3.5910579754272476e-05}, {"id": 9, "seek": 2952, "start": 51.519999999999996, "end": 55.64, "text": " And what you saw in the last video is that J-Test, the performance of the algorithm on", "tokens": [50364, 420, 577, 731, 309, 486, 2674, 1125, 281, 777, 5110, 300, 645, 406, 294, 264, 3097, 992, 13, 50628, 50628, 400, 294, 1729, 11, 337, 341, 1365, 11, 264, 3097, 6713, 486, 312, 1238, 709, 4018, 11, 293, 300, 311, 50916, 50916, 3700, 709, 3126, 813, 264, 3539, 2674, 2144, 6713, 13, 51122, 51122, 400, 538, 300, 286, 914, 264, 4274, 6713, 322, 777, 5110, 300, 645, 406, 294, 264, 3097, 992, 13, 51464, 51464, 400, 437, 291, 1866, 294, 264, 1036, 960, 307, 300, 508, 12, 51, 377, 11, 264, 3389, 295, 264, 9284, 322, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.13607688903808593, "compression_ratio": 1.9103773584905661, "no_speech_prob": 3.5910579754272476e-05}, {"id": 10, "seek": 5564, "start": 55.64, "end": 60.88, "text": " examples is not trained on, that that will be a better indicator of how well the model", "tokens": [50364, 5110, 307, 406, 8895, 322, 11, 300, 300, 486, 312, 257, 1101, 16961, 295, 577, 731, 264, 2316, 50626, 50626, 486, 3700, 360, 322, 777, 1412, 13, 50750, 50750, 400, 538, 300, 286, 914, 661, 1412, 300, 311, 406, 294, 264, 3097, 992, 13, 50974, 50974, 961, 311, 747, 257, 574, 412, 577, 341, 11807, 577, 321, 1062, 764, 257, 1500, 992, 281, 2826, 257, 2316, 51266, 51266, 337, 257, 2212, 3479, 2539, 3861, 13, 51418, 51418, 407, 498, 15669, 257, 2445, 281, 6069, 6849, 7901, 420, 512, 661, 24590, 1154, 11, 472, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.12394955231971347, "compression_ratio": 1.6359832635983265, "no_speech_prob": 6.438683158194181e-06}, {"id": 11, "seek": 5564, "start": 60.88, "end": 63.36, "text": " will likely do on new data.", "tokens": [50364, 5110, 307, 406, 8895, 322, 11, 300, 300, 486, 312, 257, 1101, 16961, 295, 577, 731, 264, 2316, 50626, 50626, 486, 3700, 360, 322, 777, 1412, 13, 50750, 50750, 400, 538, 300, 286, 914, 661, 1412, 300, 311, 406, 294, 264, 3097, 992, 13, 50974, 50974, 961, 311, 747, 257, 574, 412, 577, 341, 11807, 577, 321, 1062, 764, 257, 1500, 992, 281, 2826, 257, 2316, 51266, 51266, 337, 257, 2212, 3479, 2539, 3861, 13, 51418, 51418, 407, 498, 15669, 257, 2445, 281, 6069, 6849, 7901, 420, 512, 661, 24590, 1154, 11, 472, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.12394955231971347, "compression_ratio": 1.6359832635983265, "no_speech_prob": 6.438683158194181e-06}, {"id": 12, "seek": 5564, "start": 63.36, "end": 67.84, "text": " And by that I mean other data that's not in the training set.", "tokens": [50364, 5110, 307, 406, 8895, 322, 11, 300, 300, 486, 312, 257, 1101, 16961, 295, 577, 731, 264, 2316, 50626, 50626, 486, 3700, 360, 322, 777, 1412, 13, 50750, 50750, 400, 538, 300, 286, 914, 661, 1412, 300, 311, 406, 294, 264, 3097, 992, 13, 50974, 50974, 961, 311, 747, 257, 574, 412, 577, 341, 11807, 577, 321, 1062, 764, 257, 1500, 992, 281, 2826, 257, 2316, 51266, 51266, 337, 257, 2212, 3479, 2539, 3861, 13, 51418, 51418, 407, 498, 15669, 257, 2445, 281, 6069, 6849, 7901, 420, 512, 661, 24590, 1154, 11, 472, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.12394955231971347, "compression_ratio": 1.6359832635983265, "no_speech_prob": 6.438683158194181e-06}, {"id": 13, "seek": 5564, "start": 67.84, "end": 73.68, "text": " Let's take a look at how this affects how we might use a test set to choose a model", "tokens": [50364, 5110, 307, 406, 8895, 322, 11, 300, 300, 486, 312, 257, 1101, 16961, 295, 577, 731, 264, 2316, 50626, 50626, 486, 3700, 360, 322, 777, 1412, 13, 50750, 50750, 400, 538, 300, 286, 914, 661, 1412, 300, 311, 406, 294, 264, 3097, 992, 13, 50974, 50974, 961, 311, 747, 257, 574, 412, 577, 341, 11807, 577, 321, 1062, 764, 257, 1500, 992, 281, 2826, 257, 2316, 51266, 51266, 337, 257, 2212, 3479, 2539, 3861, 13, 51418, 51418, 407, 498, 15669, 257, 2445, 281, 6069, 6849, 7901, 420, 512, 661, 24590, 1154, 11, 472, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.12394955231971347, "compression_ratio": 1.6359832635983265, "no_speech_prob": 6.438683158194181e-06}, {"id": 14, "seek": 5564, "start": 73.68, "end": 76.72, "text": " for a given machine learning application.", "tokens": [50364, 5110, 307, 406, 8895, 322, 11, 300, 300, 486, 312, 257, 1101, 16961, 295, 577, 731, 264, 2316, 50626, 50626, 486, 3700, 360, 322, 777, 1412, 13, 50750, 50750, 400, 538, 300, 286, 914, 661, 1412, 300, 311, 406, 294, 264, 3097, 992, 13, 50974, 50974, 961, 311, 747, 257, 574, 412, 577, 341, 11807, 577, 321, 1062, 764, 257, 1500, 992, 281, 2826, 257, 2316, 51266, 51266, 337, 257, 2212, 3479, 2539, 3861, 13, 51418, 51418, 407, 498, 15669, 257, 2445, 281, 6069, 6849, 7901, 420, 512, 661, 24590, 1154, 11, 472, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.12394955231971347, "compression_ratio": 1.6359832635983265, "no_speech_prob": 6.438683158194181e-06}, {"id": 15, "seek": 5564, "start": 76.72, "end": 82.2, "text": " So if fitting a function to predict housing prices or some other regression problem, one", "tokens": [50364, 5110, 307, 406, 8895, 322, 11, 300, 300, 486, 312, 257, 1101, 16961, 295, 577, 731, 264, 2316, 50626, 50626, 486, 3700, 360, 322, 777, 1412, 13, 50750, 50750, 400, 538, 300, 286, 914, 661, 1412, 300, 311, 406, 294, 264, 3097, 992, 13, 50974, 50974, 961, 311, 747, 257, 574, 412, 577, 341, 11807, 577, 321, 1062, 764, 257, 1500, 992, 281, 2826, 257, 2316, 51266, 51266, 337, 257, 2212, 3479, 2539, 3861, 13, 51418, 51418, 407, 498, 15669, 257, 2445, 281, 6069, 6849, 7901, 420, 512, 661, 24590, 1154, 11, 472, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.12394955231971347, "compression_ratio": 1.6359832635983265, "no_speech_prob": 6.438683158194181e-06}, {"id": 16, "seek": 8220, "start": 82.2, "end": 86.04, "text": " model you might consider is to fit a linear model like this.", "tokens": [50364, 2316, 291, 1062, 1949, 307, 281, 3318, 257, 8213, 2316, 411, 341, 13, 50556, 50556, 400, 341, 307, 257, 700, 1668, 26110, 11, 293, 286, 478, 516, 281, 764, 413, 6915, 472, 322, 341, 4137, 50794, 50794, 281, 45708, 15669, 257, 472, 420, 700, 1668, 26110, 13, 51050, 51050, 759, 291, 645, 281, 3318, 257, 2316, 411, 341, 281, 428, 3097, 992, 11, 291, 1116, 483, 512, 9834, 343, 51264, 51264, 293, 363, 11, 293, 291, 393, 550, 14722, 508, 12, 51, 377, 281, 12539, 577, 731, 341, 486, 2674, 1125, 281, 51600, 51600, 777, 1412, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.1369278407332921, "compression_ratio": 1.68, "no_speech_prob": 7.411183105432428e-06}, {"id": 17, "seek": 8220, "start": 86.04, "end": 90.8, "text": " And this is a first order polynomial, and I'm going to use D equals one on this slide", "tokens": [50364, 2316, 291, 1062, 1949, 307, 281, 3318, 257, 8213, 2316, 411, 341, 13, 50556, 50556, 400, 341, 307, 257, 700, 1668, 26110, 11, 293, 286, 478, 516, 281, 764, 413, 6915, 472, 322, 341, 4137, 50794, 50794, 281, 45708, 15669, 257, 472, 420, 700, 1668, 26110, 13, 51050, 51050, 759, 291, 645, 281, 3318, 257, 2316, 411, 341, 281, 428, 3097, 992, 11, 291, 1116, 483, 512, 9834, 343, 51264, 51264, 293, 363, 11, 293, 291, 393, 550, 14722, 508, 12, 51, 377, 281, 12539, 577, 731, 341, 486, 2674, 1125, 281, 51600, 51600, 777, 1412, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.1369278407332921, "compression_ratio": 1.68, "no_speech_prob": 7.411183105432428e-06}, {"id": 18, "seek": 8220, "start": 90.8, "end": 95.92, "text": " to denote fitting a one or first order polynomial.", "tokens": [50364, 2316, 291, 1062, 1949, 307, 281, 3318, 257, 8213, 2316, 411, 341, 13, 50556, 50556, 400, 341, 307, 257, 700, 1668, 26110, 11, 293, 286, 478, 516, 281, 764, 413, 6915, 472, 322, 341, 4137, 50794, 50794, 281, 45708, 15669, 257, 472, 420, 700, 1668, 26110, 13, 51050, 51050, 759, 291, 645, 281, 3318, 257, 2316, 411, 341, 281, 428, 3097, 992, 11, 291, 1116, 483, 512, 9834, 343, 51264, 51264, 293, 363, 11, 293, 291, 393, 550, 14722, 508, 12, 51, 377, 281, 12539, 577, 731, 341, 486, 2674, 1125, 281, 51600, 51600, 777, 1412, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.1369278407332921, "compression_ratio": 1.68, "no_speech_prob": 7.411183105432428e-06}, {"id": 19, "seek": 8220, "start": 95.92, "end": 100.2, "text": " If you were to fit a model like this to your training set, you'd get some parameters W", "tokens": [50364, 2316, 291, 1062, 1949, 307, 281, 3318, 257, 8213, 2316, 411, 341, 13, 50556, 50556, 400, 341, 307, 257, 700, 1668, 26110, 11, 293, 286, 478, 516, 281, 764, 413, 6915, 472, 322, 341, 4137, 50794, 50794, 281, 45708, 15669, 257, 472, 420, 700, 1668, 26110, 13, 51050, 51050, 759, 291, 645, 281, 3318, 257, 2316, 411, 341, 281, 428, 3097, 992, 11, 291, 1116, 483, 512, 9834, 343, 51264, 51264, 293, 363, 11, 293, 291, 393, 550, 14722, 508, 12, 51, 377, 281, 12539, 577, 731, 341, 486, 2674, 1125, 281, 51600, 51600, 777, 1412, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.1369278407332921, "compression_ratio": 1.68, "no_speech_prob": 7.411183105432428e-06}, {"id": 20, "seek": 8220, "start": 100.2, "end": 106.92, "text": " and B, and you can then compute J-Test to estimate how well this will generalize to", "tokens": [50364, 2316, 291, 1062, 1949, 307, 281, 3318, 257, 8213, 2316, 411, 341, 13, 50556, 50556, 400, 341, 307, 257, 700, 1668, 26110, 11, 293, 286, 478, 516, 281, 764, 413, 6915, 472, 322, 341, 4137, 50794, 50794, 281, 45708, 15669, 257, 472, 420, 700, 1668, 26110, 13, 51050, 51050, 759, 291, 645, 281, 3318, 257, 2316, 411, 341, 281, 428, 3097, 992, 11, 291, 1116, 483, 512, 9834, 343, 51264, 51264, 293, 363, 11, 293, 291, 393, 550, 14722, 508, 12, 51, 377, 281, 12539, 577, 731, 341, 486, 2674, 1125, 281, 51600, 51600, 777, 1412, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.1369278407332921, "compression_ratio": 1.68, "no_speech_prob": 7.411183105432428e-06}, {"id": 21, "seek": 8220, "start": 106.92, "end": 107.96000000000001, "text": " new data.", "tokens": [50364, 2316, 291, 1062, 1949, 307, 281, 3318, 257, 8213, 2316, 411, 341, 13, 50556, 50556, 400, 341, 307, 257, 700, 1668, 26110, 11, 293, 286, 478, 516, 281, 764, 413, 6915, 472, 322, 341, 4137, 50794, 50794, 281, 45708, 15669, 257, 472, 420, 700, 1668, 26110, 13, 51050, 51050, 759, 291, 645, 281, 3318, 257, 2316, 411, 341, 281, 428, 3097, 992, 11, 291, 1116, 483, 512, 9834, 343, 51264, 51264, 293, 363, 11, 293, 291, 393, 550, 14722, 508, 12, 51, 377, 281, 12539, 577, 731, 341, 486, 2674, 1125, 281, 51600, 51600, 777, 1412, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.1369278407332921, "compression_ratio": 1.68, "no_speech_prob": 7.411183105432428e-06}, {"id": 22, "seek": 10796, "start": 107.96, "end": 114.32, "text": " And on this slide, I'm going to use W1, B1, superscript there, to denote that these are", "tokens": [50364, 400, 322, 341, 4137, 11, 286, 478, 516, 281, 764, 343, 16, 11, 363, 16, 11, 37906, 5944, 456, 11, 281, 45708, 300, 613, 366, 50682, 50682, 264, 9834, 291, 483, 498, 291, 645, 281, 3318, 257, 700, 1668, 26110, 420, 4314, 472, 11, 413, 50934, 50934, 6915, 472, 26110, 13, 51108, 51108, 823, 291, 1062, 611, 1949, 15669, 257, 1150, 1668, 26110, 420, 37262, 2316, 13, 51422, 51422, 407, 341, 307, 264, 2316, 13, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.15046639985676052, "compression_ratio": 1.5520833333333333, "no_speech_prob": 4.157317107456038e-06}, {"id": 23, "seek": 10796, "start": 114.32, "end": 119.36, "text": " the parameters you get if you were to fit a first order polynomial or degree one, D", "tokens": [50364, 400, 322, 341, 4137, 11, 286, 478, 516, 281, 764, 343, 16, 11, 363, 16, 11, 37906, 5944, 456, 11, 281, 45708, 300, 613, 366, 50682, 50682, 264, 9834, 291, 483, 498, 291, 645, 281, 3318, 257, 700, 1668, 26110, 420, 4314, 472, 11, 413, 50934, 50934, 6915, 472, 26110, 13, 51108, 51108, 823, 291, 1062, 611, 1949, 15669, 257, 1150, 1668, 26110, 420, 37262, 2316, 13, 51422, 51422, 407, 341, 307, 264, 2316, 13, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.15046639985676052, "compression_ratio": 1.5520833333333333, "no_speech_prob": 4.157317107456038e-06}, {"id": 24, "seek": 10796, "start": 119.36, "end": 122.83999999999999, "text": " equals one polynomial.", "tokens": [50364, 400, 322, 341, 4137, 11, 286, 478, 516, 281, 764, 343, 16, 11, 363, 16, 11, 37906, 5944, 456, 11, 281, 45708, 300, 613, 366, 50682, 50682, 264, 9834, 291, 483, 498, 291, 645, 281, 3318, 257, 700, 1668, 26110, 420, 4314, 472, 11, 413, 50934, 50934, 6915, 472, 26110, 13, 51108, 51108, 823, 291, 1062, 611, 1949, 15669, 257, 1150, 1668, 26110, 420, 37262, 2316, 13, 51422, 51422, 407, 341, 307, 264, 2316, 13, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.15046639985676052, "compression_ratio": 1.5520833333333333, "no_speech_prob": 4.157317107456038e-06}, {"id": 25, "seek": 10796, "start": 122.83999999999999, "end": 129.12, "text": " Now you might also consider fitting a second order polynomial or quadratic model.", "tokens": [50364, 400, 322, 341, 4137, 11, 286, 478, 516, 281, 764, 343, 16, 11, 363, 16, 11, 37906, 5944, 456, 11, 281, 45708, 300, 613, 366, 50682, 50682, 264, 9834, 291, 483, 498, 291, 645, 281, 3318, 257, 700, 1668, 26110, 420, 4314, 472, 11, 413, 50934, 50934, 6915, 472, 26110, 13, 51108, 51108, 823, 291, 1062, 611, 1949, 15669, 257, 1150, 1668, 26110, 420, 37262, 2316, 13, 51422, 51422, 407, 341, 307, 264, 2316, 13, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.15046639985676052, "compression_ratio": 1.5520833333333333, "no_speech_prob": 4.157317107456038e-06}, {"id": 26, "seek": 10796, "start": 129.12, "end": 131.64, "text": " So this is the model.", "tokens": [50364, 400, 322, 341, 4137, 11, 286, 478, 516, 281, 764, 343, 16, 11, 363, 16, 11, 37906, 5944, 456, 11, 281, 45708, 300, 613, 366, 50682, 50682, 264, 9834, 291, 483, 498, 291, 645, 281, 3318, 257, 700, 1668, 26110, 420, 4314, 472, 11, 413, 50934, 50934, 6915, 472, 26110, 13, 51108, 51108, 823, 291, 1062, 611, 1949, 15669, 257, 1150, 1668, 26110, 420, 37262, 2316, 13, 51422, 51422, 407, 341, 307, 264, 2316, 13, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.15046639985676052, "compression_ratio": 1.5520833333333333, "no_speech_prob": 4.157317107456038e-06}, {"id": 27, "seek": 13164, "start": 131.64, "end": 139.11999999999998, "text": " And if you were to fit this to your training set, you would get some parameters W2, B2,", "tokens": [50364, 400, 498, 291, 645, 281, 3318, 341, 281, 428, 3097, 992, 11, 291, 576, 483, 512, 9834, 343, 17, 11, 363, 17, 11, 50738, 50738, 293, 291, 393, 550, 14138, 13059, 729, 9834, 322, 428, 1500, 992, 293, 483, 508, 12, 51, 377, 51090, 51090, 343, 17, 11, 363, 17, 13, 51164, 51164, 400, 341, 576, 976, 291, 257, 2020, 295, 577, 731, 264, 1150, 1668, 26110, 775, 13, 51332, 51332, 400, 291, 393, 352, 322, 281, 853, 413, 6915, 1045, 11, 300, 311, 257, 2636, 1668, 420, 257, 4314, 1045, 26110, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.12997589508692423, "compression_ratio": 1.6384976525821595, "no_speech_prob": 2.058023028439493e-06}, {"id": 28, "seek": 13164, "start": 139.11999999999998, "end": 146.16, "text": " and you can then similarly evaluate those parameters on your test set and get J-Test", "tokens": [50364, 400, 498, 291, 645, 281, 3318, 341, 281, 428, 3097, 992, 11, 291, 576, 483, 512, 9834, 343, 17, 11, 363, 17, 11, 50738, 50738, 293, 291, 393, 550, 14138, 13059, 729, 9834, 322, 428, 1500, 992, 293, 483, 508, 12, 51, 377, 51090, 51090, 343, 17, 11, 363, 17, 13, 51164, 51164, 400, 341, 576, 976, 291, 257, 2020, 295, 577, 731, 264, 1150, 1668, 26110, 775, 13, 51332, 51332, 400, 291, 393, 352, 322, 281, 853, 413, 6915, 1045, 11, 300, 311, 257, 2636, 1668, 420, 257, 4314, 1045, 26110, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.12997589508692423, "compression_ratio": 1.6384976525821595, "no_speech_prob": 2.058023028439493e-06}, {"id": 29, "seek": 13164, "start": 146.16, "end": 147.64, "text": " W2, B2.", "tokens": [50364, 400, 498, 291, 645, 281, 3318, 341, 281, 428, 3097, 992, 11, 291, 576, 483, 512, 9834, 343, 17, 11, 363, 17, 11, 50738, 50738, 293, 291, 393, 550, 14138, 13059, 729, 9834, 322, 428, 1500, 992, 293, 483, 508, 12, 51, 377, 51090, 51090, 343, 17, 11, 363, 17, 13, 51164, 51164, 400, 341, 576, 976, 291, 257, 2020, 295, 577, 731, 264, 1150, 1668, 26110, 775, 13, 51332, 51332, 400, 291, 393, 352, 322, 281, 853, 413, 6915, 1045, 11, 300, 311, 257, 2636, 1668, 420, 257, 4314, 1045, 26110, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.12997589508692423, "compression_ratio": 1.6384976525821595, "no_speech_prob": 2.058023028439493e-06}, {"id": 30, "seek": 13164, "start": 147.64, "end": 151.0, "text": " And this would give you a sense of how well the second order polynomial does.", "tokens": [50364, 400, 498, 291, 645, 281, 3318, 341, 281, 428, 3097, 992, 11, 291, 576, 483, 512, 9834, 343, 17, 11, 363, 17, 11, 50738, 50738, 293, 291, 393, 550, 14138, 13059, 729, 9834, 322, 428, 1500, 992, 293, 483, 508, 12, 51, 377, 51090, 51090, 343, 17, 11, 363, 17, 13, 51164, 51164, 400, 341, 576, 976, 291, 257, 2020, 295, 577, 731, 264, 1150, 1668, 26110, 775, 13, 51332, 51332, 400, 291, 393, 352, 322, 281, 853, 413, 6915, 1045, 11, 300, 311, 257, 2636, 1668, 420, 257, 4314, 1045, 26110, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.12997589508692423, "compression_ratio": 1.6384976525821595, "no_speech_prob": 2.058023028439493e-06}, {"id": 31, "seek": 13164, "start": 151.0, "end": 157.56, "text": " And you can go on to try D equals three, that's a third order or a degree three polynomial", "tokens": [50364, 400, 498, 291, 645, 281, 3318, 341, 281, 428, 3097, 992, 11, 291, 576, 483, 512, 9834, 343, 17, 11, 363, 17, 11, 50738, 50738, 293, 291, 393, 550, 14138, 13059, 729, 9834, 322, 428, 1500, 992, 293, 483, 508, 12, 51, 377, 51090, 51090, 343, 17, 11, 363, 17, 13, 51164, 51164, 400, 341, 576, 976, 291, 257, 2020, 295, 577, 731, 264, 1150, 1668, 26110, 775, 13, 51332, 51332, 400, 291, 393, 352, 322, 281, 853, 413, 6915, 1045, 11, 300, 311, 257, 2636, 1668, 420, 257, 4314, 1045, 26110, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.12997589508692423, "compression_ratio": 1.6384976525821595, "no_speech_prob": 2.058023028439493e-06}, {"id": 32, "seek": 15756, "start": 157.56, "end": 163.56, "text": " that looks like this, and fit parameters, and similarly get J-Test.", "tokens": [50364, 300, 1542, 411, 341, 11, 293, 3318, 9834, 11, 293, 14138, 483, 508, 12, 51, 377, 13, 50664, 50664, 400, 291, 1062, 1066, 884, 341, 1826, 11, 584, 11, 291, 853, 493, 281, 257, 1266, 392, 1668, 26110, 293, 50932, 50932, 291, 917, 493, 365, 508, 12, 51, 377, 295, 343, 3279, 11, 363, 3279, 13, 51158, 51158, 663, 2709, 291, 257, 2020, 295, 577, 731, 264, 1266, 392, 1668, 26110, 307, 884, 13, 51381, 51381, 407, 472, 10747, 291, 727, 853, 11, 341, 4523, 484, 406, 281, 312, 264, 1151, 10747, 11, 457, 472, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.11311421249852036, "compression_ratio": 1.6103286384976525, "no_speech_prob": 5.093580057291547e-06}, {"id": 33, "seek": 15756, "start": 163.56, "end": 168.92000000000002, "text": " And you might keep doing this until, say, you try up to a 10th order polynomial and", "tokens": [50364, 300, 1542, 411, 341, 11, 293, 3318, 9834, 11, 293, 14138, 483, 508, 12, 51, 377, 13, 50664, 50664, 400, 291, 1062, 1066, 884, 341, 1826, 11, 584, 11, 291, 853, 493, 281, 257, 1266, 392, 1668, 26110, 293, 50932, 50932, 291, 917, 493, 365, 508, 12, 51, 377, 295, 343, 3279, 11, 363, 3279, 13, 51158, 51158, 663, 2709, 291, 257, 2020, 295, 577, 731, 264, 1266, 392, 1668, 26110, 307, 884, 13, 51381, 51381, 407, 472, 10747, 291, 727, 853, 11, 341, 4523, 484, 406, 281, 312, 264, 1151, 10747, 11, 457, 472, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.11311421249852036, "compression_ratio": 1.6103286384976525, "no_speech_prob": 5.093580057291547e-06}, {"id": 34, "seek": 15756, "start": 168.92000000000002, "end": 173.44, "text": " you end up with J-Test of W10, B10.", "tokens": [50364, 300, 1542, 411, 341, 11, 293, 3318, 9834, 11, 293, 14138, 483, 508, 12, 51, 377, 13, 50664, 50664, 400, 291, 1062, 1066, 884, 341, 1826, 11, 584, 11, 291, 853, 493, 281, 257, 1266, 392, 1668, 26110, 293, 50932, 50932, 291, 917, 493, 365, 508, 12, 51, 377, 295, 343, 3279, 11, 363, 3279, 13, 51158, 51158, 663, 2709, 291, 257, 2020, 295, 577, 731, 264, 1266, 392, 1668, 26110, 307, 884, 13, 51381, 51381, 407, 472, 10747, 291, 727, 853, 11, 341, 4523, 484, 406, 281, 312, 264, 1151, 10747, 11, 457, 472, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.11311421249852036, "compression_ratio": 1.6103286384976525, "no_speech_prob": 5.093580057291547e-06}, {"id": 35, "seek": 15756, "start": 173.44, "end": 177.9, "text": " That gives you a sense of how well the 10th order polynomial is doing.", "tokens": [50364, 300, 1542, 411, 341, 11, 293, 3318, 9834, 11, 293, 14138, 483, 508, 12, 51, 377, 13, 50664, 50664, 400, 291, 1062, 1066, 884, 341, 1826, 11, 584, 11, 291, 853, 493, 281, 257, 1266, 392, 1668, 26110, 293, 50932, 50932, 291, 917, 493, 365, 508, 12, 51, 377, 295, 343, 3279, 11, 363, 3279, 13, 51158, 51158, 663, 2709, 291, 257, 2020, 295, 577, 731, 264, 1266, 392, 1668, 26110, 307, 884, 13, 51381, 51381, 407, 472, 10747, 291, 727, 853, 11, 341, 4523, 484, 406, 281, 312, 264, 1151, 10747, 11, 457, 472, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.11311421249852036, "compression_ratio": 1.6103286384976525, "no_speech_prob": 5.093580057291547e-06}, {"id": 36, "seek": 15756, "start": 177.9, "end": 183.32, "text": " So one procedure you could try, this turns out not to be the best procedure, but one", "tokens": [50364, 300, 1542, 411, 341, 11, 293, 3318, 9834, 11, 293, 14138, 483, 508, 12, 51, 377, 13, 50664, 50664, 400, 291, 1062, 1066, 884, 341, 1826, 11, 584, 11, 291, 853, 493, 281, 257, 1266, 392, 1668, 26110, 293, 50932, 50932, 291, 917, 493, 365, 508, 12, 51, 377, 295, 343, 3279, 11, 363, 3279, 13, 51158, 51158, 663, 2709, 291, 257, 2020, 295, 577, 731, 264, 1266, 392, 1668, 26110, 307, 884, 13, 51381, 51381, 407, 472, 10747, 291, 727, 853, 11, 341, 4523, 484, 406, 281, 312, 264, 1151, 10747, 11, 457, 472, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.11311421249852036, "compression_ratio": 1.6103286384976525, "no_speech_prob": 5.093580057291547e-06}, {"id": 37, "seek": 18332, "start": 183.32, "end": 190.6, "text": " thing you could try is look at all of these J-Tests and see which one gives you the lowest", "tokens": [50364, 551, 291, 727, 853, 307, 574, 412, 439, 295, 613, 508, 12, 51, 4409, 293, 536, 597, 472, 2709, 291, 264, 12437, 50728, 50728, 2158, 13, 50790, 50790, 400, 584, 291, 915, 300, 508, 12, 51, 377, 337, 264, 9266, 1668, 26110, 337, 343, 20, 11, 363, 20, 4523, 484, 281, 312, 51202, 51202, 264, 12437, 13, 51306, 51306, 759, 300, 311, 264, 1389, 11, 550, 291, 1062, 4536, 300, 264, 9266, 1668, 26110, 413, 6915, 1732, 51556, 51556, 775, 1151, 293, 2826, 300, 2316, 337, 428, 3861, 13, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.09952495944115423, "compression_ratio": 1.625, "no_speech_prob": 4.565913513943087e-06}, {"id": 38, "seek": 18332, "start": 190.6, "end": 191.84, "text": " value.", "tokens": [50364, 551, 291, 727, 853, 307, 574, 412, 439, 295, 613, 508, 12, 51, 4409, 293, 536, 597, 472, 2709, 291, 264, 12437, 50728, 50728, 2158, 13, 50790, 50790, 400, 584, 291, 915, 300, 508, 12, 51, 377, 337, 264, 9266, 1668, 26110, 337, 343, 20, 11, 363, 20, 4523, 484, 281, 312, 51202, 51202, 264, 12437, 13, 51306, 51306, 759, 300, 311, 264, 1389, 11, 550, 291, 1062, 4536, 300, 264, 9266, 1668, 26110, 413, 6915, 1732, 51556, 51556, 775, 1151, 293, 2826, 300, 2316, 337, 428, 3861, 13, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.09952495944115423, "compression_ratio": 1.625, "no_speech_prob": 4.565913513943087e-06}, {"id": 39, "seek": 18332, "start": 191.84, "end": 200.07999999999998, "text": " And say you find that J-Test for the fifth order polynomial for W5, B5 turns out to be", "tokens": [50364, 551, 291, 727, 853, 307, 574, 412, 439, 295, 613, 508, 12, 51, 4409, 293, 536, 597, 472, 2709, 291, 264, 12437, 50728, 50728, 2158, 13, 50790, 50790, 400, 584, 291, 915, 300, 508, 12, 51, 377, 337, 264, 9266, 1668, 26110, 337, 343, 20, 11, 363, 20, 4523, 484, 281, 312, 51202, 51202, 264, 12437, 13, 51306, 51306, 759, 300, 311, 264, 1389, 11, 550, 291, 1062, 4536, 300, 264, 9266, 1668, 26110, 413, 6915, 1732, 51556, 51556, 775, 1151, 293, 2826, 300, 2316, 337, 428, 3861, 13, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.09952495944115423, "compression_ratio": 1.625, "no_speech_prob": 4.565913513943087e-06}, {"id": 40, "seek": 18332, "start": 200.07999999999998, "end": 202.16, "text": " the lowest.", "tokens": [50364, 551, 291, 727, 853, 307, 574, 412, 439, 295, 613, 508, 12, 51, 4409, 293, 536, 597, 472, 2709, 291, 264, 12437, 50728, 50728, 2158, 13, 50790, 50790, 400, 584, 291, 915, 300, 508, 12, 51, 377, 337, 264, 9266, 1668, 26110, 337, 343, 20, 11, 363, 20, 4523, 484, 281, 312, 51202, 51202, 264, 12437, 13, 51306, 51306, 759, 300, 311, 264, 1389, 11, 550, 291, 1062, 4536, 300, 264, 9266, 1668, 26110, 413, 6915, 1732, 51556, 51556, 775, 1151, 293, 2826, 300, 2316, 337, 428, 3861, 13, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.09952495944115423, "compression_ratio": 1.625, "no_speech_prob": 4.565913513943087e-06}, {"id": 41, "seek": 18332, "start": 202.16, "end": 207.16, "text": " If that's the case, then you might decide that the fifth order polynomial D equals five", "tokens": [50364, 551, 291, 727, 853, 307, 574, 412, 439, 295, 613, 508, 12, 51, 4409, 293, 536, 597, 472, 2709, 291, 264, 12437, 50728, 50728, 2158, 13, 50790, 50790, 400, 584, 291, 915, 300, 508, 12, 51, 377, 337, 264, 9266, 1668, 26110, 337, 343, 20, 11, 363, 20, 4523, 484, 281, 312, 51202, 51202, 264, 12437, 13, 51306, 51306, 759, 300, 311, 264, 1389, 11, 550, 291, 1062, 4536, 300, 264, 9266, 1668, 26110, 413, 6915, 1732, 51556, 51556, 775, 1151, 293, 2826, 300, 2316, 337, 428, 3861, 13, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.09952495944115423, "compression_ratio": 1.625, "no_speech_prob": 4.565913513943087e-06}, {"id": 42, "seek": 18332, "start": 207.16, "end": 211.88, "text": " does best and choose that model for your application.", "tokens": [50364, 551, 291, 727, 853, 307, 574, 412, 439, 295, 613, 508, 12, 51, 4409, 293, 536, 597, 472, 2709, 291, 264, 12437, 50728, 50728, 2158, 13, 50790, 50790, 400, 584, 291, 915, 300, 508, 12, 51, 377, 337, 264, 9266, 1668, 26110, 337, 343, 20, 11, 363, 20, 4523, 484, 281, 312, 51202, 51202, 264, 12437, 13, 51306, 51306, 759, 300, 311, 264, 1389, 11, 550, 291, 1062, 4536, 300, 264, 9266, 1668, 26110, 413, 6915, 1732, 51556, 51556, 775, 1151, 293, 2826, 300, 2316, 337, 428, 3861, 13, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.09952495944115423, "compression_ratio": 1.625, "no_speech_prob": 4.565913513943087e-06}, {"id": 43, "seek": 21188, "start": 211.88, "end": 216.2, "text": " And if you want to estimate how well this model performs, one thing you could do, but", "tokens": [50364, 400, 498, 291, 528, 281, 12539, 577, 731, 341, 2316, 26213, 11, 472, 551, 291, 727, 360, 11, 457, 50580, 50580, 341, 4523, 484, 281, 312, 257, 4748, 38823, 10747, 11, 307, 281, 2275, 264, 1500, 992, 6713, 508, 12, 51, 377, 343, 20, 11, 50910, 50910, 363, 20, 13, 51010, 51010, 440, 1778, 341, 10747, 307, 38823, 307, 508, 12, 51, 377, 295, 343, 20, 11, 363, 20, 307, 3700, 281, 312, 364, 19397, 12539, 51432, 51432, 295, 264, 2674, 2144, 6713, 13, 51534, 51534, 682, 661, 2283, 11, 309, 307, 3700, 281, 312, 3126, 813, 264, 3539, 2674, 2144, 6713, 13, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.09554458332953052, "compression_ratio": 1.7342342342342343, "no_speech_prob": 2.561265318945516e-06}, {"id": 44, "seek": 21188, "start": 216.2, "end": 222.79999999999998, "text": " this turns out to be a slightly flawed procedure, is to report the test set error J-Test W5,", "tokens": [50364, 400, 498, 291, 528, 281, 12539, 577, 731, 341, 2316, 26213, 11, 472, 551, 291, 727, 360, 11, 457, 50580, 50580, 341, 4523, 484, 281, 312, 257, 4748, 38823, 10747, 11, 307, 281, 2275, 264, 1500, 992, 6713, 508, 12, 51, 377, 343, 20, 11, 50910, 50910, 363, 20, 13, 51010, 51010, 440, 1778, 341, 10747, 307, 38823, 307, 508, 12, 51, 377, 295, 343, 20, 11, 363, 20, 307, 3700, 281, 312, 364, 19397, 12539, 51432, 51432, 295, 264, 2674, 2144, 6713, 13, 51534, 51534, 682, 661, 2283, 11, 309, 307, 3700, 281, 312, 3126, 813, 264, 3539, 2674, 2144, 6713, 13, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.09554458332953052, "compression_ratio": 1.7342342342342343, "no_speech_prob": 2.561265318945516e-06}, {"id": 45, "seek": 21188, "start": 222.79999999999998, "end": 224.79999999999998, "text": " B5.", "tokens": [50364, 400, 498, 291, 528, 281, 12539, 577, 731, 341, 2316, 26213, 11, 472, 551, 291, 727, 360, 11, 457, 50580, 50580, 341, 4523, 484, 281, 312, 257, 4748, 38823, 10747, 11, 307, 281, 2275, 264, 1500, 992, 6713, 508, 12, 51, 377, 343, 20, 11, 50910, 50910, 363, 20, 13, 51010, 51010, 440, 1778, 341, 10747, 307, 38823, 307, 508, 12, 51, 377, 295, 343, 20, 11, 363, 20, 307, 3700, 281, 312, 364, 19397, 12539, 51432, 51432, 295, 264, 2674, 2144, 6713, 13, 51534, 51534, 682, 661, 2283, 11, 309, 307, 3700, 281, 312, 3126, 813, 264, 3539, 2674, 2144, 6713, 13, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.09554458332953052, "compression_ratio": 1.7342342342342343, "no_speech_prob": 2.561265318945516e-06}, {"id": 46, "seek": 21188, "start": 224.79999999999998, "end": 233.24, "text": " The reason this procedure is flawed is J-Test of W5, B5 is likely to be an optimistic estimate", "tokens": [50364, 400, 498, 291, 528, 281, 12539, 577, 731, 341, 2316, 26213, 11, 472, 551, 291, 727, 360, 11, 457, 50580, 50580, 341, 4523, 484, 281, 312, 257, 4748, 38823, 10747, 11, 307, 281, 2275, 264, 1500, 992, 6713, 508, 12, 51, 377, 343, 20, 11, 50910, 50910, 363, 20, 13, 51010, 51010, 440, 1778, 341, 10747, 307, 38823, 307, 508, 12, 51, 377, 295, 343, 20, 11, 363, 20, 307, 3700, 281, 312, 364, 19397, 12539, 51432, 51432, 295, 264, 2674, 2144, 6713, 13, 51534, 51534, 682, 661, 2283, 11, 309, 307, 3700, 281, 312, 3126, 813, 264, 3539, 2674, 2144, 6713, 13, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.09554458332953052, "compression_ratio": 1.7342342342342343, "no_speech_prob": 2.561265318945516e-06}, {"id": 47, "seek": 21188, "start": 233.24, "end": 235.28, "text": " of the generalization error.", "tokens": [50364, 400, 498, 291, 528, 281, 12539, 577, 731, 341, 2316, 26213, 11, 472, 551, 291, 727, 360, 11, 457, 50580, 50580, 341, 4523, 484, 281, 312, 257, 4748, 38823, 10747, 11, 307, 281, 2275, 264, 1500, 992, 6713, 508, 12, 51, 377, 343, 20, 11, 50910, 50910, 363, 20, 13, 51010, 51010, 440, 1778, 341, 10747, 307, 38823, 307, 508, 12, 51, 377, 295, 343, 20, 11, 363, 20, 307, 3700, 281, 312, 364, 19397, 12539, 51432, 51432, 295, 264, 2674, 2144, 6713, 13, 51534, 51534, 682, 661, 2283, 11, 309, 307, 3700, 281, 312, 3126, 813, 264, 3539, 2674, 2144, 6713, 13, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.09554458332953052, "compression_ratio": 1.7342342342342343, "no_speech_prob": 2.561265318945516e-06}, {"id": 48, "seek": 21188, "start": 235.28, "end": 241.72, "text": " In other words, it is likely to be lower than the actual generalization error.", "tokens": [50364, 400, 498, 291, 528, 281, 12539, 577, 731, 341, 2316, 26213, 11, 472, 551, 291, 727, 360, 11, 457, 50580, 50580, 341, 4523, 484, 281, 312, 257, 4748, 38823, 10747, 11, 307, 281, 2275, 264, 1500, 992, 6713, 508, 12, 51, 377, 343, 20, 11, 50910, 50910, 363, 20, 13, 51010, 51010, 440, 1778, 341, 10747, 307, 38823, 307, 508, 12, 51, 377, 295, 343, 20, 11, 363, 20, 307, 3700, 281, 312, 364, 19397, 12539, 51432, 51432, 295, 264, 2674, 2144, 6713, 13, 51534, 51534, 682, 661, 2283, 11, 309, 307, 3700, 281, 312, 3126, 813, 264, 3539, 2674, 2144, 6713, 13, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.09554458332953052, "compression_ratio": 1.7342342342342343, "no_speech_prob": 2.561265318945516e-06}, {"id": 49, "seek": 24172, "start": 241.72, "end": 247.44, "text": " And the reason is, in the procedure we talked about on the slide, we basically fit one extra", "tokens": [50364, 400, 264, 1778, 307, 11, 294, 264, 10747, 321, 2825, 466, 322, 264, 4137, 11, 321, 1936, 3318, 472, 2857, 50650, 50650, 13075, 11, 597, 307, 413, 11, 264, 4314, 295, 26110, 11, 293, 321, 5111, 341, 13075, 1228, 264, 1500, 51016, 51016, 992, 13, 51086, 51086, 407, 322, 264, 3894, 4137, 11, 321, 1866, 300, 498, 291, 645, 281, 3318, 343, 33, 281, 264, 3097, 1412, 11, 550, 51414, 51414, 264, 3097, 1412, 576, 312, 364, 24324, 19397, 12539, 295, 2674, 2144, 6713, 13, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.10553175679753336, "compression_ratio": 1.634703196347032, "no_speech_prob": 3.4465449516574154e-06}, {"id": 50, "seek": 24172, "start": 247.44, "end": 254.76, "text": " parameter, which is D, the degree of polynomial, and we chose this parameter using the test", "tokens": [50364, 400, 264, 1778, 307, 11, 294, 264, 10747, 321, 2825, 466, 322, 264, 4137, 11, 321, 1936, 3318, 472, 2857, 50650, 50650, 13075, 11, 597, 307, 413, 11, 264, 4314, 295, 26110, 11, 293, 321, 5111, 341, 13075, 1228, 264, 1500, 51016, 51016, 992, 13, 51086, 51086, 407, 322, 264, 3894, 4137, 11, 321, 1866, 300, 498, 291, 645, 281, 3318, 343, 33, 281, 264, 3097, 1412, 11, 550, 51414, 51414, 264, 3097, 1412, 576, 312, 364, 24324, 19397, 12539, 295, 2674, 2144, 6713, 13, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.10553175679753336, "compression_ratio": 1.634703196347032, "no_speech_prob": 3.4465449516574154e-06}, {"id": 51, "seek": 24172, "start": 254.76, "end": 256.16, "text": " set.", "tokens": [50364, 400, 264, 1778, 307, 11, 294, 264, 10747, 321, 2825, 466, 322, 264, 4137, 11, 321, 1936, 3318, 472, 2857, 50650, 50650, 13075, 11, 597, 307, 413, 11, 264, 4314, 295, 26110, 11, 293, 321, 5111, 341, 13075, 1228, 264, 1500, 51016, 51016, 992, 13, 51086, 51086, 407, 322, 264, 3894, 4137, 11, 321, 1866, 300, 498, 291, 645, 281, 3318, 343, 33, 281, 264, 3097, 1412, 11, 550, 51414, 51414, 264, 3097, 1412, 576, 312, 364, 24324, 19397, 12539, 295, 2674, 2144, 6713, 13, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.10553175679753336, "compression_ratio": 1.634703196347032, "no_speech_prob": 3.4465449516574154e-06}, {"id": 52, "seek": 24172, "start": 256.16, "end": 262.72, "text": " So on the previous slide, we saw that if you were to fit WB to the training data, then", "tokens": [50364, 400, 264, 1778, 307, 11, 294, 264, 10747, 321, 2825, 466, 322, 264, 4137, 11, 321, 1936, 3318, 472, 2857, 50650, 50650, 13075, 11, 597, 307, 413, 11, 264, 4314, 295, 26110, 11, 293, 321, 5111, 341, 13075, 1228, 264, 1500, 51016, 51016, 992, 13, 51086, 51086, 407, 322, 264, 3894, 4137, 11, 321, 1866, 300, 498, 291, 645, 281, 3318, 343, 33, 281, 264, 3097, 1412, 11, 550, 51414, 51414, 264, 3097, 1412, 576, 312, 364, 24324, 19397, 12539, 295, 2674, 2144, 6713, 13, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.10553175679753336, "compression_ratio": 1.634703196347032, "no_speech_prob": 3.4465449516574154e-06}, {"id": 53, "seek": 24172, "start": 262.72, "end": 269.36, "text": " the training data would be an overly optimistic estimate of generalization error.", "tokens": [50364, 400, 264, 1778, 307, 11, 294, 264, 10747, 321, 2825, 466, 322, 264, 4137, 11, 321, 1936, 3318, 472, 2857, 50650, 50650, 13075, 11, 597, 307, 413, 11, 264, 4314, 295, 26110, 11, 293, 321, 5111, 341, 13075, 1228, 264, 1500, 51016, 51016, 992, 13, 51086, 51086, 407, 322, 264, 3894, 4137, 11, 321, 1866, 300, 498, 291, 645, 281, 3318, 343, 33, 281, 264, 3097, 1412, 11, 550, 51414, 51414, 264, 3097, 1412, 576, 312, 364, 24324, 19397, 12539, 295, 2674, 2144, 6713, 13, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.10553175679753336, "compression_ratio": 1.634703196347032, "no_speech_prob": 3.4465449516574154e-06}, {"id": 54, "seek": 26936, "start": 269.36, "end": 274.08000000000004, "text": " And it turns out too, that if we were to choose the parameter D using the test set, then the", "tokens": [50364, 400, 309, 4523, 484, 886, 11, 300, 498, 321, 645, 281, 2826, 264, 13075, 413, 1228, 264, 1500, 992, 11, 550, 264, 50600, 50600, 1500, 992, 508, 12, 51, 377, 307, 586, 364, 24324, 19397, 11, 300, 307, 3126, 813, 3539, 20561, 295, 264, 50894, 50894, 2674, 2144, 6713, 13, 51001, 51001, 407, 264, 10747, 322, 341, 1729, 4137, 307, 38823, 293, 286, 500, 380, 2748, 1228, 341, 13, 51250, 51250, 7156, 11, 498, 291, 528, 281, 6772, 2826, 257, 2316, 11, 1270, 382, 4536, 437, 4314, 26110, 51544, 51544, 281, 764, 11, 510, 311, 577, 291, 16927, 264, 3097, 293, 4997, 10747, 294, 1668, 281, 3985, 484, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.12032443284988403, "compression_ratio": 1.6560283687943262, "no_speech_prob": 1.788041231520765e-06}, {"id": 55, "seek": 26936, "start": 274.08000000000004, "end": 279.96000000000004, "text": " test set J-Test is now an overly optimistic, that is lower than actual estimates of the", "tokens": [50364, 400, 309, 4523, 484, 886, 11, 300, 498, 321, 645, 281, 2826, 264, 13075, 413, 1228, 264, 1500, 992, 11, 550, 264, 50600, 50600, 1500, 992, 508, 12, 51, 377, 307, 586, 364, 24324, 19397, 11, 300, 307, 3126, 813, 3539, 20561, 295, 264, 50894, 50894, 2674, 2144, 6713, 13, 51001, 51001, 407, 264, 10747, 322, 341, 1729, 4137, 307, 38823, 293, 286, 500, 380, 2748, 1228, 341, 13, 51250, 51250, 7156, 11, 498, 291, 528, 281, 6772, 2826, 257, 2316, 11, 1270, 382, 4536, 437, 4314, 26110, 51544, 51544, 281, 764, 11, 510, 311, 577, 291, 16927, 264, 3097, 293, 4997, 10747, 294, 1668, 281, 3985, 484, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.12032443284988403, "compression_ratio": 1.6560283687943262, "no_speech_prob": 1.788041231520765e-06}, {"id": 56, "seek": 26936, "start": 279.96000000000004, "end": 282.1, "text": " generalization error.", "tokens": [50364, 400, 309, 4523, 484, 886, 11, 300, 498, 321, 645, 281, 2826, 264, 13075, 413, 1228, 264, 1500, 992, 11, 550, 264, 50600, 50600, 1500, 992, 508, 12, 51, 377, 307, 586, 364, 24324, 19397, 11, 300, 307, 3126, 813, 3539, 20561, 295, 264, 50894, 50894, 2674, 2144, 6713, 13, 51001, 51001, 407, 264, 10747, 322, 341, 1729, 4137, 307, 38823, 293, 286, 500, 380, 2748, 1228, 341, 13, 51250, 51250, 7156, 11, 498, 291, 528, 281, 6772, 2826, 257, 2316, 11, 1270, 382, 4536, 437, 4314, 26110, 51544, 51544, 281, 764, 11, 510, 311, 577, 291, 16927, 264, 3097, 293, 4997, 10747, 294, 1668, 281, 3985, 484, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.12032443284988403, "compression_ratio": 1.6560283687943262, "no_speech_prob": 1.788041231520765e-06}, {"id": 57, "seek": 26936, "start": 282.1, "end": 287.08000000000004, "text": " So the procedure on this particular slide is flawed and I don't recommend using this.", "tokens": [50364, 400, 309, 4523, 484, 886, 11, 300, 498, 321, 645, 281, 2826, 264, 13075, 413, 1228, 264, 1500, 992, 11, 550, 264, 50600, 50600, 1500, 992, 508, 12, 51, 377, 307, 586, 364, 24324, 19397, 11, 300, 307, 3126, 813, 3539, 20561, 295, 264, 50894, 50894, 2674, 2144, 6713, 13, 51001, 51001, 407, 264, 10747, 322, 341, 1729, 4137, 307, 38823, 293, 286, 500, 380, 2748, 1228, 341, 13, 51250, 51250, 7156, 11, 498, 291, 528, 281, 6772, 2826, 257, 2316, 11, 1270, 382, 4536, 437, 4314, 26110, 51544, 51544, 281, 764, 11, 510, 311, 577, 291, 16927, 264, 3097, 293, 4997, 10747, 294, 1668, 281, 3985, 484, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.12032443284988403, "compression_ratio": 1.6560283687943262, "no_speech_prob": 1.788041231520765e-06}, {"id": 58, "seek": 26936, "start": 287.08000000000004, "end": 292.96000000000004, "text": " Instead, if you want to automatically choose a model, such as decide what degree polynomial", "tokens": [50364, 400, 309, 4523, 484, 886, 11, 300, 498, 321, 645, 281, 2826, 264, 13075, 413, 1228, 264, 1500, 992, 11, 550, 264, 50600, 50600, 1500, 992, 508, 12, 51, 377, 307, 586, 364, 24324, 19397, 11, 300, 307, 3126, 813, 3539, 20561, 295, 264, 50894, 50894, 2674, 2144, 6713, 13, 51001, 51001, 407, 264, 10747, 322, 341, 1729, 4137, 307, 38823, 293, 286, 500, 380, 2748, 1228, 341, 13, 51250, 51250, 7156, 11, 498, 291, 528, 281, 6772, 2826, 257, 2316, 11, 1270, 382, 4536, 437, 4314, 26110, 51544, 51544, 281, 764, 11, 510, 311, 577, 291, 16927, 264, 3097, 293, 4997, 10747, 294, 1668, 281, 3985, 484, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.12032443284988403, "compression_ratio": 1.6560283687943262, "no_speech_prob": 1.788041231520765e-06}, {"id": 59, "seek": 26936, "start": 292.96000000000004, "end": 298.96000000000004, "text": " to use, here's how you modify the training and testing procedure in order to carry out", "tokens": [50364, 400, 309, 4523, 484, 886, 11, 300, 498, 321, 645, 281, 2826, 264, 13075, 413, 1228, 264, 1500, 992, 11, 550, 264, 50600, 50600, 1500, 992, 508, 12, 51, 377, 307, 586, 364, 24324, 19397, 11, 300, 307, 3126, 813, 3539, 20561, 295, 264, 50894, 50894, 2674, 2144, 6713, 13, 51001, 51001, 407, 264, 10747, 322, 341, 1729, 4137, 307, 38823, 293, 286, 500, 380, 2748, 1228, 341, 13, 51250, 51250, 7156, 11, 498, 291, 528, 281, 6772, 2826, 257, 2316, 11, 1270, 382, 4536, 437, 4314, 26110, 51544, 51544, 281, 764, 11, 510, 311, 577, 291, 16927, 264, 3097, 293, 4997, 10747, 294, 1668, 281, 3985, 484, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.12032443284988403, "compression_ratio": 1.6560283687943262, "no_speech_prob": 1.788041231520765e-06}, {"id": 60, "seek": 29896, "start": 298.96, "end": 305.08, "text": " model selection, whereby model selection, I mean, choosing amongst different models,", "tokens": [50364, 2316, 9450, 11, 36998, 2316, 9450, 11, 286, 914, 11, 10875, 12918, 819, 5245, 11, 50670, 50670, 1270, 382, 613, 1266, 819, 5245, 300, 291, 1062, 19935, 473, 1228, 337, 428, 3479, 2539, 50896, 50896, 3861, 13, 51008, 51008, 440, 636, 321, 603, 16927, 264, 10747, 307, 11, 2602, 295, 30348, 428, 1412, 666, 445, 732, 2090, 1385, 11, 51262, 51262, 264, 3097, 992, 293, 264, 1500, 992, 11, 321, 434, 516, 281, 7472, 428, 1412, 666, 1045, 819, 2090, 1385, 11, 51506, 51506, 597, 321, 434, 516, 281, 818, 264, 3097, 992, 11, 264, 3278, 24071, 992, 11, 293, 550, 611, 264, 51760, 51760, 1500, 992, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1214576278414045, "compression_ratio": 1.9156626506024097, "no_speech_prob": 1.6441837942693383e-05}, {"id": 61, "seek": 29896, "start": 305.08, "end": 309.59999999999997, "text": " such as these 10 different models that you might contemplate using for your machine learning", "tokens": [50364, 2316, 9450, 11, 36998, 2316, 9450, 11, 286, 914, 11, 10875, 12918, 819, 5245, 11, 50670, 50670, 1270, 382, 613, 1266, 819, 5245, 300, 291, 1062, 19935, 473, 1228, 337, 428, 3479, 2539, 50896, 50896, 3861, 13, 51008, 51008, 440, 636, 321, 603, 16927, 264, 10747, 307, 11, 2602, 295, 30348, 428, 1412, 666, 445, 732, 2090, 1385, 11, 51262, 51262, 264, 3097, 992, 293, 264, 1500, 992, 11, 321, 434, 516, 281, 7472, 428, 1412, 666, 1045, 819, 2090, 1385, 11, 51506, 51506, 597, 321, 434, 516, 281, 818, 264, 3097, 992, 11, 264, 3278, 24071, 992, 11, 293, 550, 611, 264, 51760, 51760, 1500, 992, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1214576278414045, "compression_ratio": 1.9156626506024097, "no_speech_prob": 1.6441837942693383e-05}, {"id": 62, "seek": 29896, "start": 309.59999999999997, "end": 311.84, "text": " application.", "tokens": [50364, 2316, 9450, 11, 36998, 2316, 9450, 11, 286, 914, 11, 10875, 12918, 819, 5245, 11, 50670, 50670, 1270, 382, 613, 1266, 819, 5245, 300, 291, 1062, 19935, 473, 1228, 337, 428, 3479, 2539, 50896, 50896, 3861, 13, 51008, 51008, 440, 636, 321, 603, 16927, 264, 10747, 307, 11, 2602, 295, 30348, 428, 1412, 666, 445, 732, 2090, 1385, 11, 51262, 51262, 264, 3097, 992, 293, 264, 1500, 992, 11, 321, 434, 516, 281, 7472, 428, 1412, 666, 1045, 819, 2090, 1385, 11, 51506, 51506, 597, 321, 434, 516, 281, 818, 264, 3097, 992, 11, 264, 3278, 24071, 992, 11, 293, 550, 611, 264, 51760, 51760, 1500, 992, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1214576278414045, "compression_ratio": 1.9156626506024097, "no_speech_prob": 1.6441837942693383e-05}, {"id": 63, "seek": 29896, "start": 311.84, "end": 316.91999999999996, "text": " The way we'll modify the procedure is, instead of splitting your data into just two subsets,", "tokens": [50364, 2316, 9450, 11, 36998, 2316, 9450, 11, 286, 914, 11, 10875, 12918, 819, 5245, 11, 50670, 50670, 1270, 382, 613, 1266, 819, 5245, 300, 291, 1062, 19935, 473, 1228, 337, 428, 3479, 2539, 50896, 50896, 3861, 13, 51008, 51008, 440, 636, 321, 603, 16927, 264, 10747, 307, 11, 2602, 295, 30348, 428, 1412, 666, 445, 732, 2090, 1385, 11, 51262, 51262, 264, 3097, 992, 293, 264, 1500, 992, 11, 321, 434, 516, 281, 7472, 428, 1412, 666, 1045, 819, 2090, 1385, 11, 51506, 51506, 597, 321, 434, 516, 281, 818, 264, 3097, 992, 11, 264, 3278, 24071, 992, 11, 293, 550, 611, 264, 51760, 51760, 1500, 992, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1214576278414045, "compression_ratio": 1.9156626506024097, "no_speech_prob": 1.6441837942693383e-05}, {"id": 64, "seek": 29896, "start": 316.91999999999996, "end": 321.79999999999995, "text": " the training set and the test set, we're going to split your data into three different subsets,", "tokens": [50364, 2316, 9450, 11, 36998, 2316, 9450, 11, 286, 914, 11, 10875, 12918, 819, 5245, 11, 50670, 50670, 1270, 382, 613, 1266, 819, 5245, 300, 291, 1062, 19935, 473, 1228, 337, 428, 3479, 2539, 50896, 50896, 3861, 13, 51008, 51008, 440, 636, 321, 603, 16927, 264, 10747, 307, 11, 2602, 295, 30348, 428, 1412, 666, 445, 732, 2090, 1385, 11, 51262, 51262, 264, 3097, 992, 293, 264, 1500, 992, 11, 321, 434, 516, 281, 7472, 428, 1412, 666, 1045, 819, 2090, 1385, 11, 51506, 51506, 597, 321, 434, 516, 281, 818, 264, 3097, 992, 11, 264, 3278, 24071, 992, 11, 293, 550, 611, 264, 51760, 51760, 1500, 992, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1214576278414045, "compression_ratio": 1.9156626506024097, "no_speech_prob": 1.6441837942693383e-05}, {"id": 65, "seek": 29896, "start": 321.79999999999995, "end": 326.88, "text": " which we're going to call the training set, the cross validation set, and then also the", "tokens": [50364, 2316, 9450, 11, 36998, 2316, 9450, 11, 286, 914, 11, 10875, 12918, 819, 5245, 11, 50670, 50670, 1270, 382, 613, 1266, 819, 5245, 300, 291, 1062, 19935, 473, 1228, 337, 428, 3479, 2539, 50896, 50896, 3861, 13, 51008, 51008, 440, 636, 321, 603, 16927, 264, 10747, 307, 11, 2602, 295, 30348, 428, 1412, 666, 445, 732, 2090, 1385, 11, 51262, 51262, 264, 3097, 992, 293, 264, 1500, 992, 11, 321, 434, 516, 281, 7472, 428, 1412, 666, 1045, 819, 2090, 1385, 11, 51506, 51506, 597, 321, 434, 516, 281, 818, 264, 3097, 992, 11, 264, 3278, 24071, 992, 11, 293, 550, 611, 264, 51760, 51760, 1500, 992, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1214576278414045, "compression_ratio": 1.9156626506024097, "no_speech_prob": 1.6441837942693383e-05}, {"id": 66, "seek": 29896, "start": 326.88, "end": 328.91999999999996, "text": " test set.", "tokens": [50364, 2316, 9450, 11, 36998, 2316, 9450, 11, 286, 914, 11, 10875, 12918, 819, 5245, 11, 50670, 50670, 1270, 382, 613, 1266, 819, 5245, 300, 291, 1062, 19935, 473, 1228, 337, 428, 3479, 2539, 50896, 50896, 3861, 13, 51008, 51008, 440, 636, 321, 603, 16927, 264, 10747, 307, 11, 2602, 295, 30348, 428, 1412, 666, 445, 732, 2090, 1385, 11, 51262, 51262, 264, 3097, 992, 293, 264, 1500, 992, 11, 321, 434, 516, 281, 7472, 428, 1412, 666, 1045, 819, 2090, 1385, 11, 51506, 51506, 597, 321, 434, 516, 281, 818, 264, 3097, 992, 11, 264, 3278, 24071, 992, 11, 293, 550, 611, 264, 51760, 51760, 1500, 992, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1214576278414045, "compression_ratio": 1.9156626506024097, "no_speech_prob": 1.6441837942693383e-05}, {"id": 67, "seek": 32892, "start": 328.92, "end": 336.96000000000004, "text": " So using our example from before of these 10 training examples, we might split it into", "tokens": [50364, 407, 1228, 527, 1365, 490, 949, 295, 613, 1266, 3097, 5110, 11, 321, 1062, 7472, 309, 666, 50766, 50766, 3372, 4060, 4, 295, 264, 1412, 666, 264, 3097, 992, 13, 51041, 51041, 400, 370, 264, 24657, 321, 603, 764, 337, 264, 3097, 992, 8044, 486, 312, 264, 912, 382, 949, 11, 3993, 51348, 51348, 300, 586, 275, 83, 7146, 11, 264, 1230, 295, 3097, 5110, 486, 312, 1386, 13, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.1334412457191781, "compression_ratio": 1.7083333333333333, "no_speech_prob": 4.495089797273977e-06}, {"id": 68, "seek": 32892, "start": 336.96000000000004, "end": 342.46000000000004, "text": " putting 60% of the data into the training set.", "tokens": [50364, 407, 1228, 527, 1365, 490, 949, 295, 613, 1266, 3097, 5110, 11, 321, 1062, 7472, 309, 666, 50766, 50766, 3372, 4060, 4, 295, 264, 1412, 666, 264, 3097, 992, 13, 51041, 51041, 400, 370, 264, 24657, 321, 603, 764, 337, 264, 3097, 992, 8044, 486, 312, 264, 912, 382, 949, 11, 3993, 51348, 51348, 300, 586, 275, 83, 7146, 11, 264, 1230, 295, 3097, 5110, 486, 312, 1386, 13, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.1334412457191781, "compression_ratio": 1.7083333333333333, "no_speech_prob": 4.495089797273977e-06}, {"id": 69, "seek": 32892, "start": 342.46000000000004, "end": 348.6, "text": " And so the notation we'll use for the training set portion will be the same as before, except", "tokens": [50364, 407, 1228, 527, 1365, 490, 949, 295, 613, 1266, 3097, 5110, 11, 321, 1062, 7472, 309, 666, 50766, 50766, 3372, 4060, 4, 295, 264, 1412, 666, 264, 3097, 992, 13, 51041, 51041, 400, 370, 264, 24657, 321, 603, 764, 337, 264, 3097, 992, 8044, 486, 312, 264, 912, 382, 949, 11, 3993, 51348, 51348, 300, 586, 275, 83, 7146, 11, 264, 1230, 295, 3097, 5110, 486, 312, 1386, 13, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.1334412457191781, "compression_ratio": 1.7083333333333333, "no_speech_prob": 4.495089797273977e-06}, {"id": 70, "seek": 32892, "start": 348.6, "end": 353.68, "text": " that now mtrain, the number of training examples will be 6.", "tokens": [50364, 407, 1228, 527, 1365, 490, 949, 295, 613, 1266, 3097, 5110, 11, 321, 1062, 7472, 309, 666, 50766, 50766, 3372, 4060, 4, 295, 264, 1412, 666, 264, 3097, 992, 13, 51041, 51041, 400, 370, 264, 24657, 321, 603, 764, 337, 264, 3097, 992, 8044, 486, 312, 264, 912, 382, 949, 11, 3993, 51348, 51348, 300, 586, 275, 83, 7146, 11, 264, 1230, 295, 3097, 5110, 486, 312, 1386, 13, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.1334412457191781, "compression_ratio": 1.7083333333333333, "no_speech_prob": 4.495089797273977e-06}, {"id": 71, "seek": 35368, "start": 353.68, "end": 360.32, "text": " And we might put 20% of the data into the cross validation set.", "tokens": [50364, 400, 321, 1062, 829, 945, 4, 295, 264, 1412, 666, 264, 3278, 24071, 992, 13, 50696, 50696, 400, 264, 24657, 286, 478, 516, 281, 764, 307, 2031, 34, 53, 295, 502, 11, 288, 34, 53, 295, 502, 337, 264, 700, 3278, 24071, 51086, 51086, 1365, 13, 51136, 51136, 407, 22995, 7382, 337, 3278, 24071, 11, 439, 264, 636, 760, 281, 2031, 34, 53, 295, 275, 34, 53, 293, 288, 34, 53, 295, 275, 34, 53, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1304099440574646, "compression_ratio": 1.5751633986928104, "no_speech_prob": 2.22525636672799e-06}, {"id": 72, "seek": 35368, "start": 360.32, "end": 368.12, "text": " And the notation I'm going to use is xCV of 1, yCV of 1 for the first cross validation", "tokens": [50364, 400, 321, 1062, 829, 945, 4, 295, 264, 1412, 666, 264, 3278, 24071, 992, 13, 50696, 50696, 400, 264, 24657, 286, 478, 516, 281, 764, 307, 2031, 34, 53, 295, 502, 11, 288, 34, 53, 295, 502, 337, 264, 700, 3278, 24071, 51086, 51086, 1365, 13, 51136, 51136, 407, 22995, 7382, 337, 3278, 24071, 11, 439, 264, 636, 760, 281, 2031, 34, 53, 295, 275, 34, 53, 293, 288, 34, 53, 295, 275, 34, 53, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1304099440574646, "compression_ratio": 1.5751633986928104, "no_speech_prob": 2.22525636672799e-06}, {"id": 73, "seek": 35368, "start": 368.12, "end": 369.12, "text": " example.", "tokens": [50364, 400, 321, 1062, 829, 945, 4, 295, 264, 1412, 666, 264, 3278, 24071, 992, 13, 50696, 50696, 400, 264, 24657, 286, 478, 516, 281, 764, 307, 2031, 34, 53, 295, 502, 11, 288, 34, 53, 295, 502, 337, 264, 700, 3278, 24071, 51086, 51086, 1365, 13, 51136, 51136, 407, 22995, 7382, 337, 3278, 24071, 11, 439, 264, 636, 760, 281, 2031, 34, 53, 295, 275, 34, 53, 293, 288, 34, 53, 295, 275, 34, 53, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1304099440574646, "compression_ratio": 1.5751633986928104, "no_speech_prob": 2.22525636672799e-06}, {"id": 74, "seek": 35368, "start": 369.12, "end": 377.68, "text": " So CV stands for cross validation, all the way down to xCV of mCV and yCV of mCV.", "tokens": [50364, 400, 321, 1062, 829, 945, 4, 295, 264, 1412, 666, 264, 3278, 24071, 992, 13, 50696, 50696, 400, 264, 24657, 286, 478, 516, 281, 764, 307, 2031, 34, 53, 295, 502, 11, 288, 34, 53, 295, 502, 337, 264, 700, 3278, 24071, 51086, 51086, 1365, 13, 51136, 51136, 407, 22995, 7382, 337, 3278, 24071, 11, 439, 264, 636, 760, 281, 2031, 34, 53, 295, 275, 34, 53, 293, 288, 34, 53, 295, 275, 34, 53, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1304099440574646, "compression_ratio": 1.5751633986928104, "no_speech_prob": 2.22525636672799e-06}, {"id": 75, "seek": 37768, "start": 377.68, "end": 383.96, "text": " So here mCV equals 2 in this example is the number of cross validation examples.", "tokens": [50364, 407, 510, 275, 34, 53, 6915, 568, 294, 341, 1365, 307, 264, 1230, 295, 3278, 24071, 5110, 13, 50678, 50678, 400, 550, 2721, 11, 321, 362, 264, 1500, 992, 912, 382, 949, 13, 50848, 50848, 407, 2031, 16, 807, 2031, 76, 6921, 293, 288, 16, 807, 288, 76, 6921, 11, 689, 275, 6921, 510, 307, 2681, 281, 568, 13, 51398, 51398, 639, 307, 264, 1230, 295, 1500, 5110, 13, 51490, 51490, 492, 603, 536, 322, 264, 958, 4137, 577, 281, 764, 264, 3278, 24071, 992, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.15271082984076606, "compression_ratio": 1.687830687830688, "no_speech_prob": 1.653680897106824e-06}, {"id": 76, "seek": 37768, "start": 383.96, "end": 387.36, "text": " And then finally, we have the test set same as before.", "tokens": [50364, 407, 510, 275, 34, 53, 6915, 568, 294, 341, 1365, 307, 264, 1230, 295, 3278, 24071, 5110, 13, 50678, 50678, 400, 550, 2721, 11, 321, 362, 264, 1500, 992, 912, 382, 949, 13, 50848, 50848, 407, 2031, 16, 807, 2031, 76, 6921, 293, 288, 16, 807, 288, 76, 6921, 11, 689, 275, 6921, 510, 307, 2681, 281, 568, 13, 51398, 51398, 639, 307, 264, 1230, 295, 1500, 5110, 13, 51490, 51490, 492, 603, 536, 322, 264, 958, 4137, 577, 281, 764, 264, 3278, 24071, 992, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.15271082984076606, "compression_ratio": 1.687830687830688, "no_speech_prob": 1.653680897106824e-06}, {"id": 77, "seek": 37768, "start": 387.36, "end": 398.36, "text": " So x1 through xm tests and y1 through ym tests, where m tests here is equal to 2.", "tokens": [50364, 407, 510, 275, 34, 53, 6915, 568, 294, 341, 1365, 307, 264, 1230, 295, 3278, 24071, 5110, 13, 50678, 50678, 400, 550, 2721, 11, 321, 362, 264, 1500, 992, 912, 382, 949, 13, 50848, 50848, 407, 2031, 16, 807, 2031, 76, 6921, 293, 288, 16, 807, 288, 76, 6921, 11, 689, 275, 6921, 510, 307, 2681, 281, 568, 13, 51398, 51398, 639, 307, 264, 1230, 295, 1500, 5110, 13, 51490, 51490, 492, 603, 536, 322, 264, 958, 4137, 577, 281, 764, 264, 3278, 24071, 992, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.15271082984076606, "compression_ratio": 1.687830687830688, "no_speech_prob": 1.653680897106824e-06}, {"id": 78, "seek": 37768, "start": 398.36, "end": 400.2, "text": " This is the number of test examples.", "tokens": [50364, 407, 510, 275, 34, 53, 6915, 568, 294, 341, 1365, 307, 264, 1230, 295, 3278, 24071, 5110, 13, 50678, 50678, 400, 550, 2721, 11, 321, 362, 264, 1500, 992, 912, 382, 949, 13, 50848, 50848, 407, 2031, 16, 807, 2031, 76, 6921, 293, 288, 16, 807, 288, 76, 6921, 11, 689, 275, 6921, 510, 307, 2681, 281, 568, 13, 51398, 51398, 639, 307, 264, 1230, 295, 1500, 5110, 13, 51490, 51490, 492, 603, 536, 322, 264, 958, 4137, 577, 281, 764, 264, 3278, 24071, 992, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.15271082984076606, "compression_ratio": 1.687830687830688, "no_speech_prob": 1.653680897106824e-06}, {"id": 79, "seek": 37768, "start": 400.2, "end": 404.48, "text": " We'll see on the next slide how to use the cross validation set.", "tokens": [50364, 407, 510, 275, 34, 53, 6915, 568, 294, 341, 1365, 307, 264, 1230, 295, 3278, 24071, 5110, 13, 50678, 50678, 400, 550, 2721, 11, 321, 362, 264, 1500, 992, 912, 382, 949, 13, 50848, 50848, 407, 2031, 16, 807, 2031, 76, 6921, 293, 288, 16, 807, 288, 76, 6921, 11, 689, 275, 6921, 510, 307, 2681, 281, 568, 13, 51398, 51398, 639, 307, 264, 1230, 295, 1500, 5110, 13, 51490, 51490, 492, 603, 536, 322, 264, 958, 4137, 577, 281, 764, 264, 3278, 24071, 992, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.15271082984076606, "compression_ratio": 1.687830687830688, "no_speech_prob": 1.653680897106824e-06}, {"id": 80, "seek": 40448, "start": 404.48, "end": 411.16, "text": " So the way we'll modify the procedure is you've already seen the training set and the test", "tokens": [50364, 407, 264, 636, 321, 603, 16927, 264, 10747, 307, 291, 600, 1217, 1612, 264, 3097, 992, 293, 264, 1500, 50698, 50698, 992, 13, 50748, 50748, 400, 321, 434, 516, 281, 5366, 257, 777, 25993, 295, 264, 1412, 1219, 264, 3278, 24071, 992, 13, 51080, 51080, 440, 1315, 3278, 24071, 14942, 281, 300, 341, 307, 364, 2857, 1412, 992, 300, 321, 434, 516, 281, 764, 51356, 51356, 281, 1520, 420, 3278, 1520, 264, 40943, 420, 534, 264, 14170, 295, 819, 5245, 13, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.12616512074190028, "compression_ratio": 1.7352941176470589, "no_speech_prob": 8.714168302503822e-07}, {"id": 81, "seek": 40448, "start": 411.16, "end": 412.16, "text": " set.", "tokens": [50364, 407, 264, 636, 321, 603, 16927, 264, 10747, 307, 291, 600, 1217, 1612, 264, 3097, 992, 293, 264, 1500, 50698, 50698, 992, 13, 50748, 50748, 400, 321, 434, 516, 281, 5366, 257, 777, 25993, 295, 264, 1412, 1219, 264, 3278, 24071, 992, 13, 51080, 51080, 440, 1315, 3278, 24071, 14942, 281, 300, 341, 307, 364, 2857, 1412, 992, 300, 321, 434, 516, 281, 764, 51356, 51356, 281, 1520, 420, 3278, 1520, 264, 40943, 420, 534, 264, 14170, 295, 819, 5245, 13, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.12616512074190028, "compression_ratio": 1.7352941176470589, "no_speech_prob": 8.714168302503822e-07}, {"id": 82, "seek": 40448, "start": 412.16, "end": 418.8, "text": " And we're going to introduce a new subset of the data called the cross validation set.", "tokens": [50364, 407, 264, 636, 321, 603, 16927, 264, 10747, 307, 291, 600, 1217, 1612, 264, 3097, 992, 293, 264, 1500, 50698, 50698, 992, 13, 50748, 50748, 400, 321, 434, 516, 281, 5366, 257, 777, 25993, 295, 264, 1412, 1219, 264, 3278, 24071, 992, 13, 51080, 51080, 440, 1315, 3278, 24071, 14942, 281, 300, 341, 307, 364, 2857, 1412, 992, 300, 321, 434, 516, 281, 764, 51356, 51356, 281, 1520, 420, 3278, 1520, 264, 40943, 420, 534, 264, 14170, 295, 819, 5245, 13, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.12616512074190028, "compression_ratio": 1.7352941176470589, "no_speech_prob": 8.714168302503822e-07}, {"id": 83, "seek": 40448, "start": 418.8, "end": 424.32, "text": " The name cross validation refers to that this is an extra data set that we're going to use", "tokens": [50364, 407, 264, 636, 321, 603, 16927, 264, 10747, 307, 291, 600, 1217, 1612, 264, 3097, 992, 293, 264, 1500, 50698, 50698, 992, 13, 50748, 50748, 400, 321, 434, 516, 281, 5366, 257, 777, 25993, 295, 264, 1412, 1219, 264, 3278, 24071, 992, 13, 51080, 51080, 440, 1315, 3278, 24071, 14942, 281, 300, 341, 307, 364, 2857, 1412, 992, 300, 321, 434, 516, 281, 764, 51356, 51356, 281, 1520, 420, 3278, 1520, 264, 40943, 420, 534, 264, 14170, 295, 819, 5245, 13, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.12616512074190028, "compression_ratio": 1.7352941176470589, "no_speech_prob": 8.714168302503822e-07}, {"id": 84, "seek": 40448, "start": 424.32, "end": 430.52000000000004, "text": " to check or cross check the validity or really the accuracy of different models.", "tokens": [50364, 407, 264, 636, 321, 603, 16927, 264, 10747, 307, 291, 600, 1217, 1612, 264, 3097, 992, 293, 264, 1500, 50698, 50698, 992, 13, 50748, 50748, 400, 321, 434, 516, 281, 5366, 257, 777, 25993, 295, 264, 1412, 1219, 264, 3278, 24071, 992, 13, 51080, 51080, 440, 1315, 3278, 24071, 14942, 281, 300, 341, 307, 364, 2857, 1412, 992, 300, 321, 434, 516, 281, 764, 51356, 51356, 281, 1520, 420, 3278, 1520, 264, 40943, 420, 534, 264, 14170, 295, 819, 5245, 13, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.12616512074190028, "compression_ratio": 1.7352941176470589, "no_speech_prob": 8.714168302503822e-07}, {"id": 85, "seek": 43052, "start": 430.52, "end": 435.15999999999997, "text": " I don't think it's a great name, but that is what people in machine learning have gotten", "tokens": [50364, 286, 500, 380, 519, 309, 311, 257, 869, 1315, 11, 457, 300, 307, 437, 561, 294, 3479, 2539, 362, 5768, 50596, 50596, 281, 818, 341, 2857, 1412, 992, 13, 50734, 50734, 509, 815, 611, 1568, 561, 818, 341, 264, 24071, 992, 337, 2099, 11, 445, 1326, 45364, 550, 3278, 51016, 51016, 24071, 13, 51066, 51066, 1610, 294, 512, 5821, 11, 561, 611, 818, 341, 264, 3250, 992, 1355, 1936, 264, 51338, 51338, 912, 551, 13, 51392, 51392, 1610, 337, 2099, 11, 2171, 291, 1568, 561, 818, 341, 264, 1905, 992, 11, 457, 439, 295, 613, 2115, 914, 51634, 51634, 264, 912, 551, 382, 3278, 24071, 992, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.1451517225385786, "compression_ratio": 1.923728813559322, "no_speech_prob": 1.7602980051378836e-06}, {"id": 86, "seek": 43052, "start": 435.15999999999997, "end": 437.91999999999996, "text": " to call this extra data set.", "tokens": [50364, 286, 500, 380, 519, 309, 311, 257, 869, 1315, 11, 457, 300, 307, 437, 561, 294, 3479, 2539, 362, 5768, 50596, 50596, 281, 818, 341, 2857, 1412, 992, 13, 50734, 50734, 509, 815, 611, 1568, 561, 818, 341, 264, 24071, 992, 337, 2099, 11, 445, 1326, 45364, 550, 3278, 51016, 51016, 24071, 13, 51066, 51066, 1610, 294, 512, 5821, 11, 561, 611, 818, 341, 264, 3250, 992, 1355, 1936, 264, 51338, 51338, 912, 551, 13, 51392, 51392, 1610, 337, 2099, 11, 2171, 291, 1568, 561, 818, 341, 264, 1905, 992, 11, 457, 439, 295, 613, 2115, 914, 51634, 51634, 264, 912, 551, 382, 3278, 24071, 992, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.1451517225385786, "compression_ratio": 1.923728813559322, "no_speech_prob": 1.7602980051378836e-06}, {"id": 87, "seek": 43052, "start": 437.91999999999996, "end": 443.56, "text": " You may also hear people call this the validation set for short, just few syllables then cross", "tokens": [50364, 286, 500, 380, 519, 309, 311, 257, 869, 1315, 11, 457, 300, 307, 437, 561, 294, 3479, 2539, 362, 5768, 50596, 50596, 281, 818, 341, 2857, 1412, 992, 13, 50734, 50734, 509, 815, 611, 1568, 561, 818, 341, 264, 24071, 992, 337, 2099, 11, 445, 1326, 45364, 550, 3278, 51016, 51016, 24071, 13, 51066, 51066, 1610, 294, 512, 5821, 11, 561, 611, 818, 341, 264, 3250, 992, 1355, 1936, 264, 51338, 51338, 912, 551, 13, 51392, 51392, 1610, 337, 2099, 11, 2171, 291, 1568, 561, 818, 341, 264, 1905, 992, 11, 457, 439, 295, 613, 2115, 914, 51634, 51634, 264, 912, 551, 382, 3278, 24071, 992, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.1451517225385786, "compression_ratio": 1.923728813559322, "no_speech_prob": 1.7602980051378836e-06}, {"id": 88, "seek": 43052, "start": 443.56, "end": 444.56, "text": " validation.", "tokens": [50364, 286, 500, 380, 519, 309, 311, 257, 869, 1315, 11, 457, 300, 307, 437, 561, 294, 3479, 2539, 362, 5768, 50596, 50596, 281, 818, 341, 2857, 1412, 992, 13, 50734, 50734, 509, 815, 611, 1568, 561, 818, 341, 264, 24071, 992, 337, 2099, 11, 445, 1326, 45364, 550, 3278, 51016, 51016, 24071, 13, 51066, 51066, 1610, 294, 512, 5821, 11, 561, 611, 818, 341, 264, 3250, 992, 1355, 1936, 264, 51338, 51338, 912, 551, 13, 51392, 51392, 1610, 337, 2099, 11, 2171, 291, 1568, 561, 818, 341, 264, 1905, 992, 11, 457, 439, 295, 613, 2115, 914, 51634, 51634, 264, 912, 551, 382, 3278, 24071, 992, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.1451517225385786, "compression_ratio": 1.923728813559322, "no_speech_prob": 1.7602980051378836e-06}, {"id": 89, "seek": 43052, "start": 444.56, "end": 450.0, "text": " Or in some applications, people also call this the development set means basically the", "tokens": [50364, 286, 500, 380, 519, 309, 311, 257, 869, 1315, 11, 457, 300, 307, 437, 561, 294, 3479, 2539, 362, 5768, 50596, 50596, 281, 818, 341, 2857, 1412, 992, 13, 50734, 50734, 509, 815, 611, 1568, 561, 818, 341, 264, 24071, 992, 337, 2099, 11, 445, 1326, 45364, 550, 3278, 51016, 51016, 24071, 13, 51066, 51066, 1610, 294, 512, 5821, 11, 561, 611, 818, 341, 264, 3250, 992, 1355, 1936, 264, 51338, 51338, 912, 551, 13, 51392, 51392, 1610, 337, 2099, 11, 2171, 291, 1568, 561, 818, 341, 264, 1905, 992, 11, 457, 439, 295, 613, 2115, 914, 51634, 51634, 264, 912, 551, 382, 3278, 24071, 992, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.1451517225385786, "compression_ratio": 1.923728813559322, "no_speech_prob": 1.7602980051378836e-06}, {"id": 90, "seek": 43052, "start": 450.0, "end": 451.08, "text": " same thing.", "tokens": [50364, 286, 500, 380, 519, 309, 311, 257, 869, 1315, 11, 457, 300, 307, 437, 561, 294, 3479, 2539, 362, 5768, 50596, 50596, 281, 818, 341, 2857, 1412, 992, 13, 50734, 50734, 509, 815, 611, 1568, 561, 818, 341, 264, 24071, 992, 337, 2099, 11, 445, 1326, 45364, 550, 3278, 51016, 51016, 24071, 13, 51066, 51066, 1610, 294, 512, 5821, 11, 561, 611, 818, 341, 264, 3250, 992, 1355, 1936, 264, 51338, 51338, 912, 551, 13, 51392, 51392, 1610, 337, 2099, 11, 2171, 291, 1568, 561, 818, 341, 264, 1905, 992, 11, 457, 439, 295, 613, 2115, 914, 51634, 51634, 264, 912, 551, 382, 3278, 24071, 992, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.1451517225385786, "compression_ratio": 1.923728813559322, "no_speech_prob": 1.7602980051378836e-06}, {"id": 91, "seek": 43052, "start": 451.08, "end": 455.91999999999996, "text": " Or for short, sometimes you hear people call this the dev set, but all of these terms mean", "tokens": [50364, 286, 500, 380, 519, 309, 311, 257, 869, 1315, 11, 457, 300, 307, 437, 561, 294, 3479, 2539, 362, 5768, 50596, 50596, 281, 818, 341, 2857, 1412, 992, 13, 50734, 50734, 509, 815, 611, 1568, 561, 818, 341, 264, 24071, 992, 337, 2099, 11, 445, 1326, 45364, 550, 3278, 51016, 51016, 24071, 13, 51066, 51066, 1610, 294, 512, 5821, 11, 561, 611, 818, 341, 264, 3250, 992, 1355, 1936, 264, 51338, 51338, 912, 551, 13, 51392, 51392, 1610, 337, 2099, 11, 2171, 291, 1568, 561, 818, 341, 264, 1905, 992, 11, 457, 439, 295, 613, 2115, 914, 51634, 51634, 264, 912, 551, 382, 3278, 24071, 992, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.1451517225385786, "compression_ratio": 1.923728813559322, "no_speech_prob": 1.7602980051378836e-06}, {"id": 92, "seek": 43052, "start": 455.91999999999996, "end": 458.79999999999995, "text": " the same thing as cross validation set.", "tokens": [50364, 286, 500, 380, 519, 309, 311, 257, 869, 1315, 11, 457, 300, 307, 437, 561, 294, 3479, 2539, 362, 5768, 50596, 50596, 281, 818, 341, 2857, 1412, 992, 13, 50734, 50734, 509, 815, 611, 1568, 561, 818, 341, 264, 24071, 992, 337, 2099, 11, 445, 1326, 45364, 550, 3278, 51016, 51016, 24071, 13, 51066, 51066, 1610, 294, 512, 5821, 11, 561, 611, 818, 341, 264, 3250, 992, 1355, 1936, 264, 51338, 51338, 912, 551, 13, 51392, 51392, 1610, 337, 2099, 11, 2171, 291, 1568, 561, 818, 341, 264, 1905, 992, 11, 457, 439, 295, 613, 2115, 914, 51634, 51634, 264, 912, 551, 382, 3278, 24071, 992, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.1451517225385786, "compression_ratio": 1.923728813559322, "no_speech_prob": 1.7602980051378836e-06}, {"id": 93, "seek": 45880, "start": 458.8, "end": 464.2, "text": " I personally use the term dev set the most often because it's the shortest, fastest way", "tokens": [50364, 286, 5665, 764, 264, 1433, 1905, 992, 264, 881, 2049, 570, 309, 311, 264, 31875, 11, 14573, 636, 50634, 50634, 281, 584, 309, 11, 457, 3278, 24071, 307, 1391, 1143, 257, 707, 857, 544, 2049, 538, 3479, 2539, 50850, 50850, 25742, 13, 50944, 50944, 407, 16297, 365, 613, 1045, 2090, 1385, 295, 264, 1412, 11, 3097, 992, 11, 3278, 24071, 992, 293, 1500, 51186, 51186, 992, 11, 291, 393, 550, 14722, 264, 3097, 6713, 11, 264, 3278, 24071, 6713, 293, 264, 1500, 6713, 51518, 51518, 1228, 613, 1045, 30546, 13, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.1645658777115193, "compression_ratio": 1.7964601769911503, "no_speech_prob": 1.101568386729923e-06}, {"id": 94, "seek": 45880, "start": 464.2, "end": 468.52000000000004, "text": " to say it, but cross validation is probably used a little bit more often by machine learning", "tokens": [50364, 286, 5665, 764, 264, 1433, 1905, 992, 264, 881, 2049, 570, 309, 311, 264, 31875, 11, 14573, 636, 50634, 50634, 281, 584, 309, 11, 457, 3278, 24071, 307, 1391, 1143, 257, 707, 857, 544, 2049, 538, 3479, 2539, 50850, 50850, 25742, 13, 50944, 50944, 407, 16297, 365, 613, 1045, 2090, 1385, 295, 264, 1412, 11, 3097, 992, 11, 3278, 24071, 992, 293, 1500, 51186, 51186, 992, 11, 291, 393, 550, 14722, 264, 3097, 6713, 11, 264, 3278, 24071, 6713, 293, 264, 1500, 6713, 51518, 51518, 1228, 613, 1045, 30546, 13, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.1645658777115193, "compression_ratio": 1.7964601769911503, "no_speech_prob": 1.101568386729923e-06}, {"id": 95, "seek": 45880, "start": 468.52000000000004, "end": 470.40000000000003, "text": " practitioners.", "tokens": [50364, 286, 5665, 764, 264, 1433, 1905, 992, 264, 881, 2049, 570, 309, 311, 264, 31875, 11, 14573, 636, 50634, 50634, 281, 584, 309, 11, 457, 3278, 24071, 307, 1391, 1143, 257, 707, 857, 544, 2049, 538, 3479, 2539, 50850, 50850, 25742, 13, 50944, 50944, 407, 16297, 365, 613, 1045, 2090, 1385, 295, 264, 1412, 11, 3097, 992, 11, 3278, 24071, 992, 293, 1500, 51186, 51186, 992, 11, 291, 393, 550, 14722, 264, 3097, 6713, 11, 264, 3278, 24071, 6713, 293, 264, 1500, 6713, 51518, 51518, 1228, 613, 1045, 30546, 13, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.1645658777115193, "compression_ratio": 1.7964601769911503, "no_speech_prob": 1.101568386729923e-06}, {"id": 96, "seek": 45880, "start": 470.40000000000003, "end": 475.24, "text": " So armed with these three subsets of the data, training set, cross validation set and test", "tokens": [50364, 286, 5665, 764, 264, 1433, 1905, 992, 264, 881, 2049, 570, 309, 311, 264, 31875, 11, 14573, 636, 50634, 50634, 281, 584, 309, 11, 457, 3278, 24071, 307, 1391, 1143, 257, 707, 857, 544, 2049, 538, 3479, 2539, 50850, 50850, 25742, 13, 50944, 50944, 407, 16297, 365, 613, 1045, 2090, 1385, 295, 264, 1412, 11, 3097, 992, 11, 3278, 24071, 992, 293, 1500, 51186, 51186, 992, 11, 291, 393, 550, 14722, 264, 3097, 6713, 11, 264, 3278, 24071, 6713, 293, 264, 1500, 6713, 51518, 51518, 1228, 613, 1045, 30546, 13, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.1645658777115193, "compression_ratio": 1.7964601769911503, "no_speech_prob": 1.101568386729923e-06}, {"id": 97, "seek": 45880, "start": 475.24, "end": 481.88, "text": " set, you can then compute the training error, the cross validation error and the test error", "tokens": [50364, 286, 5665, 764, 264, 1433, 1905, 992, 264, 881, 2049, 570, 309, 311, 264, 31875, 11, 14573, 636, 50634, 50634, 281, 584, 309, 11, 457, 3278, 24071, 307, 1391, 1143, 257, 707, 857, 544, 2049, 538, 3479, 2539, 50850, 50850, 25742, 13, 50944, 50944, 407, 16297, 365, 613, 1045, 2090, 1385, 295, 264, 1412, 11, 3097, 992, 11, 3278, 24071, 992, 293, 1500, 51186, 51186, 992, 11, 291, 393, 550, 14722, 264, 3097, 6713, 11, 264, 3278, 24071, 6713, 293, 264, 1500, 6713, 51518, 51518, 1228, 613, 1045, 30546, 13, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.1645658777115193, "compression_ratio": 1.7964601769911503, "no_speech_prob": 1.101568386729923e-06}, {"id": 98, "seek": 45880, "start": 481.88, "end": 484.28000000000003, "text": " using these three formulas.", "tokens": [50364, 286, 5665, 764, 264, 1433, 1905, 992, 264, 881, 2049, 570, 309, 311, 264, 31875, 11, 14573, 636, 50634, 50634, 281, 584, 309, 11, 457, 3278, 24071, 307, 1391, 1143, 257, 707, 857, 544, 2049, 538, 3479, 2539, 50850, 50850, 25742, 13, 50944, 50944, 407, 16297, 365, 613, 1045, 2090, 1385, 295, 264, 1412, 11, 3097, 992, 11, 3278, 24071, 992, 293, 1500, 51186, 51186, 992, 11, 291, 393, 550, 14722, 264, 3097, 6713, 11, 264, 3278, 24071, 6713, 293, 264, 1500, 6713, 51518, 51518, 1228, 613, 1045, 30546, 13, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.1645658777115193, "compression_ratio": 1.7964601769911503, "no_speech_prob": 1.101568386729923e-06}, {"id": 99, "seek": 48428, "start": 484.28, "end": 488.79999999999995, "text": " Where as usual, none of these terms include the regularization term that is included in", "tokens": [50364, 2305, 382, 7713, 11, 6022, 295, 613, 2115, 4090, 264, 3890, 2144, 1433, 300, 307, 5556, 294, 50590, 50590, 264, 3097, 10024, 13, 50676, 50676, 400, 341, 777, 1433, 294, 264, 2808, 11, 264, 3278, 24071, 6713, 307, 445, 264, 4274, 670, 50880, 50880, 428, 8797, 53, 3278, 24071, 5110, 295, 264, 4274, 584, 8889, 6713, 13, 51218, 51218, 400, 341, 1433, 11, 294, 4500, 281, 885, 1219, 3278, 24071, 6713, 11, 307, 611, 12719, 1219, 51526, 51526, 264, 24071, 6713, 337, 2099, 420, 754, 264, 3250, 992, 6713, 420, 264, 1905, 6713, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.124490601675851, "compression_ratio": 1.9004329004329004, "no_speech_prob": 2.6841253202292137e-06}, {"id": 100, "seek": 48428, "start": 488.79999999999995, "end": 490.52, "text": " the training objective.", "tokens": [50364, 2305, 382, 7713, 11, 6022, 295, 613, 2115, 4090, 264, 3890, 2144, 1433, 300, 307, 5556, 294, 50590, 50590, 264, 3097, 10024, 13, 50676, 50676, 400, 341, 777, 1433, 294, 264, 2808, 11, 264, 3278, 24071, 6713, 307, 445, 264, 4274, 670, 50880, 50880, 428, 8797, 53, 3278, 24071, 5110, 295, 264, 4274, 584, 8889, 6713, 13, 51218, 51218, 400, 341, 1433, 11, 294, 4500, 281, 885, 1219, 3278, 24071, 6713, 11, 307, 611, 12719, 1219, 51526, 51526, 264, 24071, 6713, 337, 2099, 420, 754, 264, 3250, 992, 6713, 420, 264, 1905, 6713, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.124490601675851, "compression_ratio": 1.9004329004329004, "no_speech_prob": 2.6841253202292137e-06}, {"id": 101, "seek": 48428, "start": 490.52, "end": 494.59999999999997, "text": " And this new term in the middle, the cross validation error is just the average over", "tokens": [50364, 2305, 382, 7713, 11, 6022, 295, 613, 2115, 4090, 264, 3890, 2144, 1433, 300, 307, 5556, 294, 50590, 50590, 264, 3097, 10024, 13, 50676, 50676, 400, 341, 777, 1433, 294, 264, 2808, 11, 264, 3278, 24071, 6713, 307, 445, 264, 4274, 670, 50880, 50880, 428, 8797, 53, 3278, 24071, 5110, 295, 264, 4274, 584, 8889, 6713, 13, 51218, 51218, 400, 341, 1433, 11, 294, 4500, 281, 885, 1219, 3278, 24071, 6713, 11, 307, 611, 12719, 1219, 51526, 51526, 264, 24071, 6713, 337, 2099, 420, 754, 264, 3250, 992, 6713, 420, 264, 1905, 6713, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.124490601675851, "compression_ratio": 1.9004329004329004, "no_speech_prob": 2.6841253202292137e-06}, {"id": 102, "seek": 48428, "start": 494.59999999999997, "end": 501.35999999999996, "text": " your MCV cross validation examples of the average say squared error.", "tokens": [50364, 2305, 382, 7713, 11, 6022, 295, 613, 2115, 4090, 264, 3890, 2144, 1433, 300, 307, 5556, 294, 50590, 50590, 264, 3097, 10024, 13, 50676, 50676, 400, 341, 777, 1433, 294, 264, 2808, 11, 264, 3278, 24071, 6713, 307, 445, 264, 4274, 670, 50880, 50880, 428, 8797, 53, 3278, 24071, 5110, 295, 264, 4274, 584, 8889, 6713, 13, 51218, 51218, 400, 341, 1433, 11, 294, 4500, 281, 885, 1219, 3278, 24071, 6713, 11, 307, 611, 12719, 1219, 51526, 51526, 264, 24071, 6713, 337, 2099, 420, 754, 264, 3250, 992, 6713, 420, 264, 1905, 6713, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.124490601675851, "compression_ratio": 1.9004329004329004, "no_speech_prob": 2.6841253202292137e-06}, {"id": 103, "seek": 48428, "start": 501.35999999999996, "end": 507.52, "text": " And this term, in addition to being called cross validation error, is also commonly called", "tokens": [50364, 2305, 382, 7713, 11, 6022, 295, 613, 2115, 4090, 264, 3890, 2144, 1433, 300, 307, 5556, 294, 50590, 50590, 264, 3097, 10024, 13, 50676, 50676, 400, 341, 777, 1433, 294, 264, 2808, 11, 264, 3278, 24071, 6713, 307, 445, 264, 4274, 670, 50880, 50880, 428, 8797, 53, 3278, 24071, 5110, 295, 264, 4274, 584, 8889, 6713, 13, 51218, 51218, 400, 341, 1433, 11, 294, 4500, 281, 885, 1219, 3278, 24071, 6713, 11, 307, 611, 12719, 1219, 51526, 51526, 264, 24071, 6713, 337, 2099, 420, 754, 264, 3250, 992, 6713, 420, 264, 1905, 6713, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.124490601675851, "compression_ratio": 1.9004329004329004, "no_speech_prob": 2.6841253202292137e-06}, {"id": 104, "seek": 48428, "start": 507.52, "end": 513.1999999999999, "text": " the validation error for short or even the development set error or the dev error.", "tokens": [50364, 2305, 382, 7713, 11, 6022, 295, 613, 2115, 4090, 264, 3890, 2144, 1433, 300, 307, 5556, 294, 50590, 50590, 264, 3097, 10024, 13, 50676, 50676, 400, 341, 777, 1433, 294, 264, 2808, 11, 264, 3278, 24071, 6713, 307, 445, 264, 4274, 670, 50880, 50880, 428, 8797, 53, 3278, 24071, 5110, 295, 264, 4274, 584, 8889, 6713, 13, 51218, 51218, 400, 341, 1433, 11, 294, 4500, 281, 885, 1219, 3278, 24071, 6713, 11, 307, 611, 12719, 1219, 51526, 51526, 264, 24071, 6713, 337, 2099, 420, 754, 264, 3250, 992, 6713, 420, 264, 1905, 6713, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.124490601675851, "compression_ratio": 1.9004329004329004, "no_speech_prob": 2.6841253202292137e-06}, {"id": 105, "seek": 51320, "start": 513.2, "end": 518.5600000000001, "text": " Armed with these three measures of learning algorithm performance, this is how you can", "tokens": [50364, 42024, 365, 613, 1045, 8000, 295, 2539, 9284, 3389, 11, 341, 307, 577, 291, 393, 50632, 50632, 550, 352, 466, 9792, 484, 2316, 9450, 13, 50814, 50814, 509, 393, 365, 264, 1266, 5245, 11, 912, 382, 3071, 322, 264, 4137, 365, 413, 6915, 472, 11, 413, 6915, 732, 11, 51190, 51190, 439, 264, 636, 493, 281, 257, 1266, 392, 4314, 420, 264, 1266, 392, 1668, 26110, 11, 291, 393, 550, 3318, 264, 9834, 51494, 51494, 343, 16, 11, 363, 16, 13, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1665924072265625, "compression_ratio": 1.5862068965517242, "no_speech_prob": 5.1737997637246735e-06}, {"id": 106, "seek": 51320, "start": 518.5600000000001, "end": 522.2, "text": " then go about carrying out model selection.", "tokens": [50364, 42024, 365, 613, 1045, 8000, 295, 2539, 9284, 3389, 11, 341, 307, 577, 291, 393, 50632, 50632, 550, 352, 466, 9792, 484, 2316, 9450, 13, 50814, 50814, 509, 393, 365, 264, 1266, 5245, 11, 912, 382, 3071, 322, 264, 4137, 365, 413, 6915, 472, 11, 413, 6915, 732, 11, 51190, 51190, 439, 264, 636, 493, 281, 257, 1266, 392, 4314, 420, 264, 1266, 392, 1668, 26110, 11, 291, 393, 550, 3318, 264, 9834, 51494, 51494, 343, 16, 11, 363, 16, 13, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1665924072265625, "compression_ratio": 1.5862068965517242, "no_speech_prob": 5.1737997637246735e-06}, {"id": 107, "seek": 51320, "start": 522.2, "end": 529.72, "text": " You can with the 10 models, same as earlier on the slide with D equals one, D equals two,", "tokens": [50364, 42024, 365, 613, 1045, 8000, 295, 2539, 9284, 3389, 11, 341, 307, 577, 291, 393, 50632, 50632, 550, 352, 466, 9792, 484, 2316, 9450, 13, 50814, 50814, 509, 393, 365, 264, 1266, 5245, 11, 912, 382, 3071, 322, 264, 4137, 365, 413, 6915, 472, 11, 413, 6915, 732, 11, 51190, 51190, 439, 264, 636, 493, 281, 257, 1266, 392, 4314, 420, 264, 1266, 392, 1668, 26110, 11, 291, 393, 550, 3318, 264, 9834, 51494, 51494, 343, 16, 11, 363, 16, 13, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1665924072265625, "compression_ratio": 1.5862068965517242, "no_speech_prob": 5.1737997637246735e-06}, {"id": 108, "seek": 51320, "start": 529.72, "end": 535.8000000000001, "text": " all the way up to a 10th degree or the 10th order polynomial, you can then fit the parameters", "tokens": [50364, 42024, 365, 613, 1045, 8000, 295, 2539, 9284, 3389, 11, 341, 307, 577, 291, 393, 50632, 50632, 550, 352, 466, 9792, 484, 2316, 9450, 13, 50814, 50814, 509, 393, 365, 264, 1266, 5245, 11, 912, 382, 3071, 322, 264, 4137, 365, 413, 6915, 472, 11, 413, 6915, 732, 11, 51190, 51190, 439, 264, 636, 493, 281, 257, 1266, 392, 4314, 420, 264, 1266, 392, 1668, 26110, 11, 291, 393, 550, 3318, 264, 9834, 51494, 51494, 343, 16, 11, 363, 16, 13, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1665924072265625, "compression_ratio": 1.5862068965517242, "no_speech_prob": 5.1737997637246735e-06}, {"id": 109, "seek": 51320, "start": 535.8000000000001, "end": 539.2800000000001, "text": " W1, B1.", "tokens": [50364, 42024, 365, 613, 1045, 8000, 295, 2539, 9284, 3389, 11, 341, 307, 577, 291, 393, 50632, 50632, 550, 352, 466, 9792, 484, 2316, 9450, 13, 50814, 50814, 509, 393, 365, 264, 1266, 5245, 11, 912, 382, 3071, 322, 264, 4137, 365, 413, 6915, 472, 11, 413, 6915, 732, 11, 51190, 51190, 439, 264, 636, 493, 281, 257, 1266, 392, 4314, 420, 264, 1266, 392, 1668, 26110, 11, 291, 393, 550, 3318, 264, 9834, 51494, 51494, 343, 16, 11, 363, 16, 13, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1665924072265625, "compression_ratio": 1.5862068965517242, "no_speech_prob": 5.1737997637246735e-06}, {"id": 110, "seek": 53928, "start": 539.28, "end": 543.9599999999999, "text": " But instead of evaluating this on your test set, you would instead evaluate these parameters", "tokens": [50364, 583, 2602, 295, 27479, 341, 322, 428, 1500, 992, 11, 291, 576, 2602, 13059, 613, 9834, 50598, 50598, 322, 428, 3278, 24071, 6352, 293, 14722, 49802, 53, 295, 343, 16, 11, 363, 16, 13, 50894, 50894, 400, 14138, 337, 264, 1150, 2316, 11, 291, 483, 49802, 53, 295, 343, 17, 11, 363, 17, 293, 439, 264, 636, 760, 281, 49802, 53, 51290, 51290, 295, 343, 3279, 11, 363, 3279, 13, 51486, 51486, 1396, 294, 1668, 281, 2826, 257, 2316, 11, 291, 576, 574, 412, 597, 2316, 575, 264, 12437, 3278, 24071, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.13094168211284438, "compression_ratio": 1.673170731707317, "no_speech_prob": 1.0451312846271321e-05}, {"id": 111, "seek": 53928, "start": 543.9599999999999, "end": 549.88, "text": " on your cross validation sets and compute JCV of W1, B1.", "tokens": [50364, 583, 2602, 295, 27479, 341, 322, 428, 1500, 992, 11, 291, 576, 2602, 13059, 613, 9834, 50598, 50598, 322, 428, 3278, 24071, 6352, 293, 14722, 49802, 53, 295, 343, 16, 11, 363, 16, 13, 50894, 50894, 400, 14138, 337, 264, 1150, 2316, 11, 291, 483, 49802, 53, 295, 343, 17, 11, 363, 17, 293, 439, 264, 636, 760, 281, 49802, 53, 51290, 51290, 295, 343, 3279, 11, 363, 3279, 13, 51486, 51486, 1396, 294, 1668, 281, 2826, 257, 2316, 11, 291, 576, 574, 412, 597, 2316, 575, 264, 12437, 3278, 24071, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.13094168211284438, "compression_ratio": 1.673170731707317, "no_speech_prob": 1.0451312846271321e-05}, {"id": 112, "seek": 53928, "start": 549.88, "end": 557.8, "text": " And similarly for the second model, you get JCV of W2, B2 and all the way down to JCV", "tokens": [50364, 583, 2602, 295, 27479, 341, 322, 428, 1500, 992, 11, 291, 576, 2602, 13059, 613, 9834, 50598, 50598, 322, 428, 3278, 24071, 6352, 293, 14722, 49802, 53, 295, 343, 16, 11, 363, 16, 13, 50894, 50894, 400, 14138, 337, 264, 1150, 2316, 11, 291, 483, 49802, 53, 295, 343, 17, 11, 363, 17, 293, 439, 264, 636, 760, 281, 49802, 53, 51290, 51290, 295, 343, 3279, 11, 363, 3279, 13, 51486, 51486, 1396, 294, 1668, 281, 2826, 257, 2316, 11, 291, 576, 574, 412, 597, 2316, 575, 264, 12437, 3278, 24071, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.13094168211284438, "compression_ratio": 1.673170731707317, "no_speech_prob": 1.0451312846271321e-05}, {"id": 113, "seek": 53928, "start": 557.8, "end": 561.72, "text": " of W10, B10.", "tokens": [50364, 583, 2602, 295, 27479, 341, 322, 428, 1500, 992, 11, 291, 576, 2602, 13059, 613, 9834, 50598, 50598, 322, 428, 3278, 24071, 6352, 293, 14722, 49802, 53, 295, 343, 16, 11, 363, 16, 13, 50894, 50894, 400, 14138, 337, 264, 1150, 2316, 11, 291, 483, 49802, 53, 295, 343, 17, 11, 363, 17, 293, 439, 264, 636, 760, 281, 49802, 53, 51290, 51290, 295, 343, 3279, 11, 363, 3279, 13, 51486, 51486, 1396, 294, 1668, 281, 2826, 257, 2316, 11, 291, 576, 574, 412, 597, 2316, 575, 264, 12437, 3278, 24071, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.13094168211284438, "compression_ratio": 1.673170731707317, "no_speech_prob": 1.0451312846271321e-05}, {"id": 114, "seek": 53928, "start": 561.72, "end": 569.1999999999999, "text": " Then in order to choose a model, you would look at which model has the lowest cross validation", "tokens": [50364, 583, 2602, 295, 27479, 341, 322, 428, 1500, 992, 11, 291, 576, 2602, 13059, 613, 9834, 50598, 50598, 322, 428, 3278, 24071, 6352, 293, 14722, 49802, 53, 295, 343, 16, 11, 363, 16, 13, 50894, 50894, 400, 14138, 337, 264, 1150, 2316, 11, 291, 483, 49802, 53, 295, 343, 17, 11, 363, 17, 293, 439, 264, 636, 760, 281, 49802, 53, 51290, 51290, 295, 343, 3279, 11, 363, 3279, 13, 51486, 51486, 1396, 294, 1668, 281, 2826, 257, 2316, 11, 291, 576, 574, 412, 597, 2316, 575, 264, 12437, 3278, 24071, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.13094168211284438, "compression_ratio": 1.673170731707317, "no_speech_prob": 1.0451312846271321e-05}, {"id": 115, "seek": 56920, "start": 569.2, "end": 570.5600000000001, "text": " error.", "tokens": [50364, 6713, 13, 50432, 50432, 400, 39481, 736, 11, 718, 311, 584, 300, 49802, 53, 295, 343, 19, 11, 363, 19, 307, 12437, 11, 550, 437, 300, 1355, 307, 291, 50836, 50836, 576, 1888, 341, 6409, 1668, 26110, 382, 264, 2316, 291, 486, 764, 337, 341, 3861, 13, 51128, 51128, 6288, 11, 498, 291, 528, 281, 2275, 484, 364, 12539, 295, 264, 2674, 2144, 6713, 295, 577, 731, 341, 51430, 51430, 2316, 486, 360, 322, 777, 1412, 11, 291, 576, 360, 370, 1228, 300, 2636, 25993, 295, 428, 1412, 11, 264, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.12818452145190948, "compression_ratio": 1.5919282511210762, "no_speech_prob": 3.446526307016029e-06}, {"id": 116, "seek": 56920, "start": 570.5600000000001, "end": 578.6400000000001, "text": " And concretely, let's say that JCV of W4, B4 is lowest, then what that means is you", "tokens": [50364, 6713, 13, 50432, 50432, 400, 39481, 736, 11, 718, 311, 584, 300, 49802, 53, 295, 343, 19, 11, 363, 19, 307, 12437, 11, 550, 437, 300, 1355, 307, 291, 50836, 50836, 576, 1888, 341, 6409, 1668, 26110, 382, 264, 2316, 291, 486, 764, 337, 341, 3861, 13, 51128, 51128, 6288, 11, 498, 291, 528, 281, 2275, 484, 364, 12539, 295, 264, 2674, 2144, 6713, 295, 577, 731, 341, 51430, 51430, 2316, 486, 360, 322, 777, 1412, 11, 291, 576, 360, 370, 1228, 300, 2636, 25993, 295, 428, 1412, 11, 264, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.12818452145190948, "compression_ratio": 1.5919282511210762, "no_speech_prob": 3.446526307016029e-06}, {"id": 117, "seek": 56920, "start": 578.6400000000001, "end": 584.48, "text": " would pick this fourth order polynomial as the model you will use for this application.", "tokens": [50364, 6713, 13, 50432, 50432, 400, 39481, 736, 11, 718, 311, 584, 300, 49802, 53, 295, 343, 19, 11, 363, 19, 307, 12437, 11, 550, 437, 300, 1355, 307, 291, 50836, 50836, 576, 1888, 341, 6409, 1668, 26110, 382, 264, 2316, 291, 486, 764, 337, 341, 3861, 13, 51128, 51128, 6288, 11, 498, 291, 528, 281, 2275, 484, 364, 12539, 295, 264, 2674, 2144, 6713, 295, 577, 731, 341, 51430, 51430, 2316, 486, 360, 322, 777, 1412, 11, 291, 576, 360, 370, 1228, 300, 2636, 25993, 295, 428, 1412, 11, 264, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.12818452145190948, "compression_ratio": 1.5919282511210762, "no_speech_prob": 3.446526307016029e-06}, {"id": 118, "seek": 56920, "start": 584.48, "end": 590.5200000000001, "text": " Finally, if you want to report out an estimate of the generalization error of how well this", "tokens": [50364, 6713, 13, 50432, 50432, 400, 39481, 736, 11, 718, 311, 584, 300, 49802, 53, 295, 343, 19, 11, 363, 19, 307, 12437, 11, 550, 437, 300, 1355, 307, 291, 50836, 50836, 576, 1888, 341, 6409, 1668, 26110, 382, 264, 2316, 291, 486, 764, 337, 341, 3861, 13, 51128, 51128, 6288, 11, 498, 291, 528, 281, 2275, 484, 364, 12539, 295, 264, 2674, 2144, 6713, 295, 577, 731, 341, 51430, 51430, 2316, 486, 360, 322, 777, 1412, 11, 291, 576, 360, 370, 1228, 300, 2636, 25993, 295, 428, 1412, 11, 264, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.12818452145190948, "compression_ratio": 1.5919282511210762, "no_speech_prob": 3.446526307016029e-06}, {"id": 119, "seek": 56920, "start": 590.5200000000001, "end": 597.2800000000001, "text": " model will do on new data, you would do so using that third subset of your data, the", "tokens": [50364, 6713, 13, 50432, 50432, 400, 39481, 736, 11, 718, 311, 584, 300, 49802, 53, 295, 343, 19, 11, 363, 19, 307, 12437, 11, 550, 437, 300, 1355, 307, 291, 50836, 50836, 576, 1888, 341, 6409, 1668, 26110, 382, 264, 2316, 291, 486, 764, 337, 341, 3861, 13, 51128, 51128, 6288, 11, 498, 291, 528, 281, 2275, 484, 364, 12539, 295, 264, 2674, 2144, 6713, 295, 577, 731, 341, 51430, 51430, 2316, 486, 360, 322, 777, 1412, 11, 291, 576, 360, 370, 1228, 300, 2636, 25993, 295, 428, 1412, 11, 264, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.12818452145190948, "compression_ratio": 1.5919282511210762, "no_speech_prob": 3.446526307016029e-06}, {"id": 120, "seek": 59728, "start": 597.28, "end": 602.1999999999999, "text": " test set and you report out J test of W4, B4.", "tokens": [50364, 1500, 992, 293, 291, 2275, 484, 508, 1500, 295, 343, 19, 11, 363, 19, 13, 50610, 50610, 400, 291, 3449, 300, 3710, 341, 2302, 10747, 11, 291, 632, 3318, 613, 9834, 1228, 50904, 50904, 264, 3097, 992, 11, 291, 550, 5111, 264, 13075, 413, 420, 5111, 264, 4314, 295, 26110, 1228, 51226, 51226, 264, 3278, 24071, 992, 13, 51326, 51326, 400, 370, 493, 1826, 341, 935, 11, 291, 600, 406, 3318, 604, 9834, 11, 2139, 343, 420, 363, 420, 413, 281, 264, 51592, 51592, 1500, 992, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.13699481251475576, "compression_ratio": 1.6862745098039216, "no_speech_prob": 3.500809953038697e-06}, {"id": 121, "seek": 59728, "start": 602.1999999999999, "end": 608.0799999999999, "text": " And you notice that throughout this entire procedure, you had fit these parameters using", "tokens": [50364, 1500, 992, 293, 291, 2275, 484, 508, 1500, 295, 343, 19, 11, 363, 19, 13, 50610, 50610, 400, 291, 3449, 300, 3710, 341, 2302, 10747, 11, 291, 632, 3318, 613, 9834, 1228, 50904, 50904, 264, 3097, 992, 11, 291, 550, 5111, 264, 13075, 413, 420, 5111, 264, 4314, 295, 26110, 1228, 51226, 51226, 264, 3278, 24071, 992, 13, 51326, 51326, 400, 370, 493, 1826, 341, 935, 11, 291, 600, 406, 3318, 604, 9834, 11, 2139, 343, 420, 363, 420, 413, 281, 264, 51592, 51592, 1500, 992, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.13699481251475576, "compression_ratio": 1.6862745098039216, "no_speech_prob": 3.500809953038697e-06}, {"id": 122, "seek": 59728, "start": 608.0799999999999, "end": 614.52, "text": " the training set, you then chose the parameter D or chose the degree of polynomial using", "tokens": [50364, 1500, 992, 293, 291, 2275, 484, 508, 1500, 295, 343, 19, 11, 363, 19, 13, 50610, 50610, 400, 291, 3449, 300, 3710, 341, 2302, 10747, 11, 291, 632, 3318, 613, 9834, 1228, 50904, 50904, 264, 3097, 992, 11, 291, 550, 5111, 264, 13075, 413, 420, 5111, 264, 4314, 295, 26110, 1228, 51226, 51226, 264, 3278, 24071, 992, 13, 51326, 51326, 400, 370, 493, 1826, 341, 935, 11, 291, 600, 406, 3318, 604, 9834, 11, 2139, 343, 420, 363, 420, 413, 281, 264, 51592, 51592, 1500, 992, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.13699481251475576, "compression_ratio": 1.6862745098039216, "no_speech_prob": 3.500809953038697e-06}, {"id": 123, "seek": 59728, "start": 614.52, "end": 616.52, "text": " the cross validation set.", "tokens": [50364, 1500, 992, 293, 291, 2275, 484, 508, 1500, 295, 343, 19, 11, 363, 19, 13, 50610, 50610, 400, 291, 3449, 300, 3710, 341, 2302, 10747, 11, 291, 632, 3318, 613, 9834, 1228, 50904, 50904, 264, 3097, 992, 11, 291, 550, 5111, 264, 13075, 413, 420, 5111, 264, 4314, 295, 26110, 1228, 51226, 51226, 264, 3278, 24071, 992, 13, 51326, 51326, 400, 370, 493, 1826, 341, 935, 11, 291, 600, 406, 3318, 604, 9834, 11, 2139, 343, 420, 363, 420, 413, 281, 264, 51592, 51592, 1500, 992, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.13699481251475576, "compression_ratio": 1.6862745098039216, "no_speech_prob": 3.500809953038697e-06}, {"id": 124, "seek": 59728, "start": 616.52, "end": 621.8399999999999, "text": " And so up until this point, you've not fit any parameters, either W or B or D to the", "tokens": [50364, 1500, 992, 293, 291, 2275, 484, 508, 1500, 295, 343, 19, 11, 363, 19, 13, 50610, 50610, 400, 291, 3449, 300, 3710, 341, 2302, 10747, 11, 291, 632, 3318, 613, 9834, 1228, 50904, 50904, 264, 3097, 992, 11, 291, 550, 5111, 264, 13075, 413, 420, 5111, 264, 4314, 295, 26110, 1228, 51226, 51226, 264, 3278, 24071, 992, 13, 51326, 51326, 400, 370, 493, 1826, 341, 935, 11, 291, 600, 406, 3318, 604, 9834, 11, 2139, 343, 420, 363, 420, 413, 281, 264, 51592, 51592, 1500, 992, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.13699481251475576, "compression_ratio": 1.6862745098039216, "no_speech_prob": 3.500809953038697e-06}, {"id": 125, "seek": 59728, "start": 621.8399999999999, "end": 623.12, "text": " test set.", "tokens": [50364, 1500, 992, 293, 291, 2275, 484, 508, 1500, 295, 343, 19, 11, 363, 19, 13, 50610, 50610, 400, 291, 3449, 300, 3710, 341, 2302, 10747, 11, 291, 632, 3318, 613, 9834, 1228, 50904, 50904, 264, 3097, 992, 11, 291, 550, 5111, 264, 13075, 413, 420, 5111, 264, 4314, 295, 26110, 1228, 51226, 51226, 264, 3278, 24071, 992, 13, 51326, 51326, 400, 370, 493, 1826, 341, 935, 11, 291, 600, 406, 3318, 604, 9834, 11, 2139, 343, 420, 363, 420, 413, 281, 264, 51592, 51592, 1500, 992, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.13699481251475576, "compression_ratio": 1.6862745098039216, "no_speech_prob": 3.500809953038697e-06}, {"id": 126, "seek": 62312, "start": 623.12, "end": 630.32, "text": " And that's why J test in this example will be a fair estimate of the generalization error", "tokens": [50364, 400, 300, 311, 983, 508, 1500, 294, 341, 1365, 486, 312, 257, 3143, 12539, 295, 264, 2674, 2144, 6713, 50724, 50724, 295, 341, 2316, 300, 575, 9834, 343, 19, 11, 363, 19, 13, 50979, 50979, 407, 341, 2709, 257, 1101, 10747, 337, 2316, 9450, 13, 51200, 51200, 400, 309, 6653, 291, 6772, 652, 257, 3537, 411, 437, 1668, 26110, 281, 2826, 337, 428, 51488, 51488, 8213, 24590, 2316, 13, 51580, 51580, 639, 2316, 9450, 10747, 611, 1985, 337, 10875, 3654, 661, 3467, 295, 5245, 13, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.13383014550369776, "compression_ratio": 1.6083333333333334, "no_speech_prob": 2.090434236379224e-06}, {"id": 127, "seek": 62312, "start": 630.32, "end": 635.42, "text": " of this model that has parameters W4, B4.", "tokens": [50364, 400, 300, 311, 983, 508, 1500, 294, 341, 1365, 486, 312, 257, 3143, 12539, 295, 264, 2674, 2144, 6713, 50724, 50724, 295, 341, 2316, 300, 575, 9834, 343, 19, 11, 363, 19, 13, 50979, 50979, 407, 341, 2709, 257, 1101, 10747, 337, 2316, 9450, 13, 51200, 51200, 400, 309, 6653, 291, 6772, 652, 257, 3537, 411, 437, 1668, 26110, 281, 2826, 337, 428, 51488, 51488, 8213, 24590, 2316, 13, 51580, 51580, 639, 2316, 9450, 10747, 611, 1985, 337, 10875, 3654, 661, 3467, 295, 5245, 13, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.13383014550369776, "compression_ratio": 1.6083333333333334, "no_speech_prob": 2.090434236379224e-06}, {"id": 128, "seek": 62312, "start": 635.42, "end": 639.84, "text": " So this gives a better procedure for model selection.", "tokens": [50364, 400, 300, 311, 983, 508, 1500, 294, 341, 1365, 486, 312, 257, 3143, 12539, 295, 264, 2674, 2144, 6713, 50724, 50724, 295, 341, 2316, 300, 575, 9834, 343, 19, 11, 363, 19, 13, 50979, 50979, 407, 341, 2709, 257, 1101, 10747, 337, 2316, 9450, 13, 51200, 51200, 400, 309, 6653, 291, 6772, 652, 257, 3537, 411, 437, 1668, 26110, 281, 2826, 337, 428, 51488, 51488, 8213, 24590, 2316, 13, 51580, 51580, 639, 2316, 9450, 10747, 611, 1985, 337, 10875, 3654, 661, 3467, 295, 5245, 13, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.13383014550369776, "compression_ratio": 1.6083333333333334, "no_speech_prob": 2.090434236379224e-06}, {"id": 129, "seek": 62312, "start": 639.84, "end": 645.6, "text": " And it lets you automatically make a decision like what order polynomial to choose for your", "tokens": [50364, 400, 300, 311, 983, 508, 1500, 294, 341, 1365, 486, 312, 257, 3143, 12539, 295, 264, 2674, 2144, 6713, 50724, 50724, 295, 341, 2316, 300, 575, 9834, 343, 19, 11, 363, 19, 13, 50979, 50979, 407, 341, 2709, 257, 1101, 10747, 337, 2316, 9450, 13, 51200, 51200, 400, 309, 6653, 291, 6772, 652, 257, 3537, 411, 437, 1668, 26110, 281, 2826, 337, 428, 51488, 51488, 8213, 24590, 2316, 13, 51580, 51580, 639, 2316, 9450, 10747, 611, 1985, 337, 10875, 3654, 661, 3467, 295, 5245, 13, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.13383014550369776, "compression_ratio": 1.6083333333333334, "no_speech_prob": 2.090434236379224e-06}, {"id": 130, "seek": 62312, "start": 645.6, "end": 647.44, "text": " linear regression model.", "tokens": [50364, 400, 300, 311, 983, 508, 1500, 294, 341, 1365, 486, 312, 257, 3143, 12539, 295, 264, 2674, 2144, 6713, 50724, 50724, 295, 341, 2316, 300, 575, 9834, 343, 19, 11, 363, 19, 13, 50979, 50979, 407, 341, 2709, 257, 1101, 10747, 337, 2316, 9450, 13, 51200, 51200, 400, 309, 6653, 291, 6772, 652, 257, 3537, 411, 437, 1668, 26110, 281, 2826, 337, 428, 51488, 51488, 8213, 24590, 2316, 13, 51580, 51580, 639, 2316, 9450, 10747, 611, 1985, 337, 10875, 3654, 661, 3467, 295, 5245, 13, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.13383014550369776, "compression_ratio": 1.6083333333333334, "no_speech_prob": 2.090434236379224e-06}, {"id": 131, "seek": 62312, "start": 647.44, "end": 652.6, "text": " This model selection procedure also works for choosing among other types of models.", "tokens": [50364, 400, 300, 311, 983, 508, 1500, 294, 341, 1365, 486, 312, 257, 3143, 12539, 295, 264, 2674, 2144, 6713, 50724, 50724, 295, 341, 2316, 300, 575, 9834, 343, 19, 11, 363, 19, 13, 50979, 50979, 407, 341, 2709, 257, 1101, 10747, 337, 2316, 9450, 13, 51200, 51200, 400, 309, 6653, 291, 6772, 652, 257, 3537, 411, 437, 1668, 26110, 281, 2826, 337, 428, 51488, 51488, 8213, 24590, 2316, 13, 51580, 51580, 639, 2316, 9450, 10747, 611, 1985, 337, 10875, 3654, 661, 3467, 295, 5245, 13, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.13383014550369776, "compression_ratio": 1.6083333333333334, "no_speech_prob": 2.090434236379224e-06}, {"id": 132, "seek": 65260, "start": 652.6, "end": 655.9200000000001, "text": " For example, choosing a neural network architecture.", "tokens": [50364, 1171, 1365, 11, 10875, 257, 18161, 3209, 9482, 13, 50530, 50530, 759, 291, 366, 15669, 257, 2316, 337, 1011, 26859, 14293, 11150, 11, 291, 1062, 1949, 1045, 50848, 50848, 5245, 411, 613, 11, 1310, 754, 257, 4833, 992, 295, 5245, 813, 445, 1045, 11, 457, 510, 366, 257, 51062, 51062, 1326, 819, 18161, 9590, 295, 1359, 11, 8344, 4833, 293, 550, 754, 4833, 13, 51360, 51360, 1407, 854, 291, 4536, 577, 867, 7914, 820, 428, 18161, 3209, 362, 293, 577, 867, 7633, 51602, 51602, 6815, 680, 4583, 820, 291, 362, 13, 51718, 51718], "temperature": 0.0, "avg_logprob": -0.12024636519582647, "compression_ratio": 1.7295081967213115, "no_speech_prob": 5.173851604922675e-06}, {"id": 133, "seek": 65260, "start": 655.9200000000001, "end": 662.28, "text": " If you are fitting a model for handwritten digit recognition, you might consider three", "tokens": [50364, 1171, 1365, 11, 10875, 257, 18161, 3209, 9482, 13, 50530, 50530, 759, 291, 366, 15669, 257, 2316, 337, 1011, 26859, 14293, 11150, 11, 291, 1062, 1949, 1045, 50848, 50848, 5245, 411, 613, 11, 1310, 754, 257, 4833, 992, 295, 5245, 813, 445, 1045, 11, 457, 510, 366, 257, 51062, 51062, 1326, 819, 18161, 9590, 295, 1359, 11, 8344, 4833, 293, 550, 754, 4833, 13, 51360, 51360, 1407, 854, 291, 4536, 577, 867, 7914, 820, 428, 18161, 3209, 362, 293, 577, 867, 7633, 51602, 51602, 6815, 680, 4583, 820, 291, 362, 13, 51718, 51718], "temperature": 0.0, "avg_logprob": -0.12024636519582647, "compression_ratio": 1.7295081967213115, "no_speech_prob": 5.173851604922675e-06}, {"id": 134, "seek": 65260, "start": 662.28, "end": 666.5600000000001, "text": " models like these, maybe even a larger set of models than just three, but here are a", "tokens": [50364, 1171, 1365, 11, 10875, 257, 18161, 3209, 9482, 13, 50530, 50530, 759, 291, 366, 15669, 257, 2316, 337, 1011, 26859, 14293, 11150, 11, 291, 1062, 1949, 1045, 50848, 50848, 5245, 411, 613, 11, 1310, 754, 257, 4833, 992, 295, 5245, 813, 445, 1045, 11, 457, 510, 366, 257, 51062, 51062, 1326, 819, 18161, 9590, 295, 1359, 11, 8344, 4833, 293, 550, 754, 4833, 13, 51360, 51360, 1407, 854, 291, 4536, 577, 867, 7914, 820, 428, 18161, 3209, 362, 293, 577, 867, 7633, 51602, 51602, 6815, 680, 4583, 820, 291, 362, 13, 51718, 51718], "temperature": 0.0, "avg_logprob": -0.12024636519582647, "compression_ratio": 1.7295081967213115, "no_speech_prob": 5.173851604922675e-06}, {"id": 135, "seek": 65260, "start": 666.5600000000001, "end": 672.52, "text": " few different neural networks of small, somewhat larger and then even larger.", "tokens": [50364, 1171, 1365, 11, 10875, 257, 18161, 3209, 9482, 13, 50530, 50530, 759, 291, 366, 15669, 257, 2316, 337, 1011, 26859, 14293, 11150, 11, 291, 1062, 1949, 1045, 50848, 50848, 5245, 411, 613, 11, 1310, 754, 257, 4833, 992, 295, 5245, 813, 445, 1045, 11, 457, 510, 366, 257, 51062, 51062, 1326, 819, 18161, 9590, 295, 1359, 11, 8344, 4833, 293, 550, 754, 4833, 13, 51360, 51360, 1407, 854, 291, 4536, 577, 867, 7914, 820, 428, 18161, 3209, 362, 293, 577, 867, 7633, 51602, 51602, 6815, 680, 4583, 820, 291, 362, 13, 51718, 51718], "temperature": 0.0, "avg_logprob": -0.12024636519582647, "compression_ratio": 1.7295081967213115, "no_speech_prob": 5.173851604922675e-06}, {"id": 136, "seek": 65260, "start": 672.52, "end": 677.36, "text": " To help you decide how many layers should your neural network have and how many hidden", "tokens": [50364, 1171, 1365, 11, 10875, 257, 18161, 3209, 9482, 13, 50530, 50530, 759, 291, 366, 15669, 257, 2316, 337, 1011, 26859, 14293, 11150, 11, 291, 1062, 1949, 1045, 50848, 50848, 5245, 411, 613, 11, 1310, 754, 257, 4833, 992, 295, 5245, 813, 445, 1045, 11, 457, 510, 366, 257, 51062, 51062, 1326, 819, 18161, 9590, 295, 1359, 11, 8344, 4833, 293, 550, 754, 4833, 13, 51360, 51360, 1407, 854, 291, 4536, 577, 867, 7914, 820, 428, 18161, 3209, 362, 293, 577, 867, 7633, 51602, 51602, 6815, 680, 4583, 820, 291, 362, 13, 51718, 51718], "temperature": 0.0, "avg_logprob": -0.12024636519582647, "compression_ratio": 1.7295081967213115, "no_speech_prob": 5.173851604922675e-06}, {"id": 137, "seek": 65260, "start": 677.36, "end": 679.6800000000001, "text": " units per layer should you have.", "tokens": [50364, 1171, 1365, 11, 10875, 257, 18161, 3209, 9482, 13, 50530, 50530, 759, 291, 366, 15669, 257, 2316, 337, 1011, 26859, 14293, 11150, 11, 291, 1062, 1949, 1045, 50848, 50848, 5245, 411, 613, 11, 1310, 754, 257, 4833, 992, 295, 5245, 813, 445, 1045, 11, 457, 510, 366, 257, 51062, 51062, 1326, 819, 18161, 9590, 295, 1359, 11, 8344, 4833, 293, 550, 754, 4833, 13, 51360, 51360, 1407, 854, 291, 4536, 577, 867, 7914, 820, 428, 18161, 3209, 362, 293, 577, 867, 7633, 51602, 51602, 6815, 680, 4583, 820, 291, 362, 13, 51718, 51718], "temperature": 0.0, "avg_logprob": -0.12024636519582647, "compression_ratio": 1.7295081967213115, "no_speech_prob": 5.173851604922675e-06}, {"id": 138, "seek": 67968, "start": 679.68, "end": 688.8, "text": " You can then train all three of these models and end up with parameters W1, B1 for the", "tokens": [50364, 509, 393, 550, 3847, 439, 1045, 295, 613, 5245, 293, 917, 493, 365, 9834, 343, 16, 11, 363, 16, 337, 264, 50820, 50820, 700, 2316, 11, 343, 17, 11, 363, 17, 337, 264, 1150, 2316, 293, 343, 18, 11, 363, 18, 337, 264, 2636, 2316, 13, 51184, 51184, 400, 291, 393, 550, 13059, 264, 18161, 3209, 311, 3389, 1228, 49802, 53, 1228, 428, 3278, 24071, 51548, 51548, 992, 13, 51617, 51617], "temperature": 0.0, "avg_logprob": -0.12453045257150311, "compression_ratio": 1.5174418604651163, "no_speech_prob": 1.2029209756292403e-05}, {"id": 139, "seek": 67968, "start": 688.8, "end": 696.0799999999999, "text": " first model, W2, B2 for the second model and W3, B3 for the third model.", "tokens": [50364, 509, 393, 550, 3847, 439, 1045, 295, 613, 5245, 293, 917, 493, 365, 9834, 343, 16, 11, 363, 16, 337, 264, 50820, 50820, 700, 2316, 11, 343, 17, 11, 363, 17, 337, 264, 1150, 2316, 293, 343, 18, 11, 363, 18, 337, 264, 2636, 2316, 13, 51184, 51184, 400, 291, 393, 550, 13059, 264, 18161, 3209, 311, 3389, 1228, 49802, 53, 1228, 428, 3278, 24071, 51548, 51548, 992, 13, 51617, 51617], "temperature": 0.0, "avg_logprob": -0.12453045257150311, "compression_ratio": 1.5174418604651163, "no_speech_prob": 1.2029209756292403e-05}, {"id": 140, "seek": 67968, "start": 696.0799999999999, "end": 703.3599999999999, "text": " And you can then evaluate the neural network's performance using JCV using your cross validation", "tokens": [50364, 509, 393, 550, 3847, 439, 1045, 295, 613, 5245, 293, 917, 493, 365, 9834, 343, 16, 11, 363, 16, 337, 264, 50820, 50820, 700, 2316, 11, 343, 17, 11, 363, 17, 337, 264, 1150, 2316, 293, 343, 18, 11, 363, 18, 337, 264, 2636, 2316, 13, 51184, 51184, 400, 291, 393, 550, 13059, 264, 18161, 3209, 311, 3389, 1228, 49802, 53, 1228, 428, 3278, 24071, 51548, 51548, 992, 13, 51617, 51617], "temperature": 0.0, "avg_logprob": -0.12453045257150311, "compression_ratio": 1.5174418604651163, "no_speech_prob": 1.2029209756292403e-05}, {"id": 141, "seek": 67968, "start": 703.3599999999999, "end": 704.74, "text": " set.", "tokens": [50364, 509, 393, 550, 3847, 439, 1045, 295, 613, 5245, 293, 917, 493, 365, 9834, 343, 16, 11, 363, 16, 337, 264, 50820, 50820, 700, 2316, 11, 343, 17, 11, 363, 17, 337, 264, 1150, 2316, 293, 343, 18, 11, 363, 18, 337, 264, 2636, 2316, 13, 51184, 51184, 400, 291, 393, 550, 13059, 264, 18161, 3209, 311, 3389, 1228, 49802, 53, 1228, 428, 3278, 24071, 51548, 51548, 992, 13, 51617, 51617], "temperature": 0.0, "avg_logprob": -0.12453045257150311, "compression_ratio": 1.5174418604651163, "no_speech_prob": 1.2029209756292403e-05}, {"id": 142, "seek": 70474, "start": 704.74, "end": 711.64, "text": " And with a classification problem, JCV can be the percentage of examples.", "tokens": [50364, 400, 365, 257, 21538, 1154, 11, 49802, 53, 393, 312, 264, 9668, 295, 5110, 13, 50709, 50709, 400, 1670, 341, 307, 257, 21538, 1154, 11, 49802, 53, 11, 264, 881, 2689, 3922, 576, 312, 281, 14722, 50987, 50987, 341, 382, 257, 14135, 295, 3278, 24071, 5110, 300, 264, 9284, 575, 3346, 11665, 2587, 13, 51309, 51309, 400, 291, 576, 14722, 341, 1228, 439, 1045, 5245, 293, 550, 1888, 264, 2316, 365, 264, 12437, 51667, 51667, 3278, 24071, 6713, 13, 51785, 51785], "temperature": 0.0, "avg_logprob": -0.08335047457591598, "compression_ratio": 1.7892156862745099, "no_speech_prob": 5.173794761503814e-06}, {"id": 143, "seek": 70474, "start": 711.64, "end": 717.2, "text": " And since this is a classification problem, JCV, the most common choice would be to compute", "tokens": [50364, 400, 365, 257, 21538, 1154, 11, 49802, 53, 393, 312, 264, 9668, 295, 5110, 13, 50709, 50709, 400, 1670, 341, 307, 257, 21538, 1154, 11, 49802, 53, 11, 264, 881, 2689, 3922, 576, 312, 281, 14722, 50987, 50987, 341, 382, 257, 14135, 295, 3278, 24071, 5110, 300, 264, 9284, 575, 3346, 11665, 2587, 13, 51309, 51309, 400, 291, 576, 14722, 341, 1228, 439, 1045, 5245, 293, 550, 1888, 264, 2316, 365, 264, 12437, 51667, 51667, 3278, 24071, 6713, 13, 51785, 51785], "temperature": 0.0, "avg_logprob": -0.08335047457591598, "compression_ratio": 1.7892156862745099, "no_speech_prob": 5.173794761503814e-06}, {"id": 144, "seek": 70474, "start": 717.2, "end": 723.64, "text": " this as a fraction of cross validation examples that the algorithm has misclassified.", "tokens": [50364, 400, 365, 257, 21538, 1154, 11, 49802, 53, 393, 312, 264, 9668, 295, 5110, 13, 50709, 50709, 400, 1670, 341, 307, 257, 21538, 1154, 11, 49802, 53, 11, 264, 881, 2689, 3922, 576, 312, 281, 14722, 50987, 50987, 341, 382, 257, 14135, 295, 3278, 24071, 5110, 300, 264, 9284, 575, 3346, 11665, 2587, 13, 51309, 51309, 400, 291, 576, 14722, 341, 1228, 439, 1045, 5245, 293, 550, 1888, 264, 2316, 365, 264, 12437, 51667, 51667, 3278, 24071, 6713, 13, 51785, 51785], "temperature": 0.0, "avg_logprob": -0.08335047457591598, "compression_ratio": 1.7892156862745099, "no_speech_prob": 5.173794761503814e-06}, {"id": 145, "seek": 70474, "start": 723.64, "end": 730.8, "text": " And you would compute this using all three models and then pick the model with the lowest", "tokens": [50364, 400, 365, 257, 21538, 1154, 11, 49802, 53, 393, 312, 264, 9668, 295, 5110, 13, 50709, 50709, 400, 1670, 341, 307, 257, 21538, 1154, 11, 49802, 53, 11, 264, 881, 2689, 3922, 576, 312, 281, 14722, 50987, 50987, 341, 382, 257, 14135, 295, 3278, 24071, 5110, 300, 264, 9284, 575, 3346, 11665, 2587, 13, 51309, 51309, 400, 291, 576, 14722, 341, 1228, 439, 1045, 5245, 293, 550, 1888, 264, 2316, 365, 264, 12437, 51667, 51667, 3278, 24071, 6713, 13, 51785, 51785], "temperature": 0.0, "avg_logprob": -0.08335047457591598, "compression_ratio": 1.7892156862745099, "no_speech_prob": 5.173794761503814e-06}, {"id": 146, "seek": 70474, "start": 730.8, "end": 733.16, "text": " cross validation error.", "tokens": [50364, 400, 365, 257, 21538, 1154, 11, 49802, 53, 393, 312, 264, 9668, 295, 5110, 13, 50709, 50709, 400, 1670, 341, 307, 257, 21538, 1154, 11, 49802, 53, 11, 264, 881, 2689, 3922, 576, 312, 281, 14722, 50987, 50987, 341, 382, 257, 14135, 295, 3278, 24071, 5110, 300, 264, 9284, 575, 3346, 11665, 2587, 13, 51309, 51309, 400, 291, 576, 14722, 341, 1228, 439, 1045, 5245, 293, 550, 1888, 264, 2316, 365, 264, 12437, 51667, 51667, 3278, 24071, 6713, 13, 51785, 51785], "temperature": 0.0, "avg_logprob": -0.08335047457591598, "compression_ratio": 1.7892156862745099, "no_speech_prob": 5.173794761503814e-06}, {"id": 147, "seek": 73316, "start": 733.16, "end": 739.3199999999999, "text": " So if in this example, this has the lowest cross validation error, you would then pick", "tokens": [50364, 407, 498, 294, 341, 1365, 11, 341, 575, 264, 12437, 3278, 24071, 6713, 11, 291, 576, 550, 1888, 50672, 50672, 264, 1150, 18161, 3209, 293, 764, 9834, 8895, 322, 341, 2316, 13, 51026, 51026, 400, 2721, 11, 498, 291, 528, 281, 2275, 484, 364, 12539, 295, 264, 2674, 2144, 6713, 11, 291, 51242, 51242, 550, 764, 264, 1500, 992, 281, 12539, 577, 731, 264, 18161, 3209, 300, 291, 445, 5111, 486, 51514, 51514, 360, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.10652848738658277, "compression_ratio": 1.6700507614213198, "no_speech_prob": 8.579147561249556e-07}, {"id": 148, "seek": 73316, "start": 739.3199999999999, "end": 746.4, "text": " the second neural network and use parameters trained on this model.", "tokens": [50364, 407, 498, 294, 341, 1365, 11, 341, 575, 264, 12437, 3278, 24071, 6713, 11, 291, 576, 550, 1888, 50672, 50672, 264, 1150, 18161, 3209, 293, 764, 9834, 8895, 322, 341, 2316, 13, 51026, 51026, 400, 2721, 11, 498, 291, 528, 281, 2275, 484, 364, 12539, 295, 264, 2674, 2144, 6713, 11, 291, 51242, 51242, 550, 764, 264, 1500, 992, 281, 12539, 577, 731, 264, 18161, 3209, 300, 291, 445, 5111, 486, 51514, 51514, 360, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.10652848738658277, "compression_ratio": 1.6700507614213198, "no_speech_prob": 8.579147561249556e-07}, {"id": 149, "seek": 73316, "start": 746.4, "end": 750.7199999999999, "text": " And finally, if you want to report out an estimate of the generalization error, you", "tokens": [50364, 407, 498, 294, 341, 1365, 11, 341, 575, 264, 12437, 3278, 24071, 6713, 11, 291, 576, 550, 1888, 50672, 50672, 264, 1150, 18161, 3209, 293, 764, 9834, 8895, 322, 341, 2316, 13, 51026, 51026, 400, 2721, 11, 498, 291, 528, 281, 2275, 484, 364, 12539, 295, 264, 2674, 2144, 6713, 11, 291, 51242, 51242, 550, 764, 264, 1500, 992, 281, 12539, 577, 731, 264, 18161, 3209, 300, 291, 445, 5111, 486, 51514, 51514, 360, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.10652848738658277, "compression_ratio": 1.6700507614213198, "no_speech_prob": 8.579147561249556e-07}, {"id": 150, "seek": 73316, "start": 750.7199999999999, "end": 756.16, "text": " then use the test set to estimate how well the neural network that you just chose will", "tokens": [50364, 407, 498, 294, 341, 1365, 11, 341, 575, 264, 12437, 3278, 24071, 6713, 11, 291, 576, 550, 1888, 50672, 50672, 264, 1150, 18161, 3209, 293, 764, 9834, 8895, 322, 341, 2316, 13, 51026, 51026, 400, 2721, 11, 498, 291, 528, 281, 2275, 484, 364, 12539, 295, 264, 2674, 2144, 6713, 11, 291, 51242, 51242, 550, 764, 264, 1500, 992, 281, 12539, 577, 731, 264, 18161, 3209, 300, 291, 445, 5111, 486, 51514, 51514, 360, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.10652848738658277, "compression_ratio": 1.6700507614213198, "no_speech_prob": 8.579147561249556e-07}, {"id": 151, "seek": 73316, "start": 756.16, "end": 757.4, "text": " do.", "tokens": [50364, 407, 498, 294, 341, 1365, 11, 341, 575, 264, 12437, 3278, 24071, 6713, 11, 291, 576, 550, 1888, 50672, 50672, 264, 1150, 18161, 3209, 293, 764, 9834, 8895, 322, 341, 2316, 13, 51026, 51026, 400, 2721, 11, 498, 291, 528, 281, 2275, 484, 364, 12539, 295, 264, 2674, 2144, 6713, 11, 291, 51242, 51242, 550, 764, 264, 1500, 992, 281, 12539, 577, 731, 264, 18161, 3209, 300, 291, 445, 5111, 486, 51514, 51514, 360, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.10652848738658277, "compression_ratio": 1.6700507614213198, "no_speech_prob": 8.579147561249556e-07}, {"id": 152, "seek": 75740, "start": 757.4, "end": 764.36, "text": " So in machine learning practice, it's considered best practice to make all the decisions you", "tokens": [50364, 407, 294, 3479, 2539, 3124, 11, 309, 311, 4888, 1151, 3124, 281, 652, 439, 264, 5327, 291, 50712, 50712, 528, 281, 652, 8595, 428, 2539, 9284, 11, 1270, 382, 577, 281, 2826, 9834, 11, 437, 4314, 50984, 50984, 26110, 281, 764, 11, 457, 652, 5327, 787, 1237, 412, 264, 3097, 992, 293, 3278, 24071, 51266, 51266, 992, 293, 281, 406, 764, 264, 1500, 992, 412, 439, 281, 652, 5327, 466, 428, 2316, 13, 51542, 51542, 400, 787, 934, 291, 600, 1027, 439, 729, 5327, 11, 550, 2721, 747, 264, 2316, 291, 362, 4761, 51831, 51831], "temperature": 0.0, "avg_logprob": -0.12515149657259284, "compression_ratio": 1.8170731707317074, "no_speech_prob": 5.539082508221327e-07}, {"id": 153, "seek": 75740, "start": 764.36, "end": 769.8, "text": " want to make regarding your learning algorithm, such as how to choose parameters, what degree", "tokens": [50364, 407, 294, 3479, 2539, 3124, 11, 309, 311, 4888, 1151, 3124, 281, 652, 439, 264, 5327, 291, 50712, 50712, 528, 281, 652, 8595, 428, 2539, 9284, 11, 1270, 382, 577, 281, 2826, 9834, 11, 437, 4314, 50984, 50984, 26110, 281, 764, 11, 457, 652, 5327, 787, 1237, 412, 264, 3097, 992, 293, 3278, 24071, 51266, 51266, 992, 293, 281, 406, 764, 264, 1500, 992, 412, 439, 281, 652, 5327, 466, 428, 2316, 13, 51542, 51542, 400, 787, 934, 291, 600, 1027, 439, 729, 5327, 11, 550, 2721, 747, 264, 2316, 291, 362, 4761, 51831, 51831], "temperature": 0.0, "avg_logprob": -0.12515149657259284, "compression_ratio": 1.8170731707317074, "no_speech_prob": 5.539082508221327e-07}, {"id": 154, "seek": 75740, "start": 769.8, "end": 775.4399999999999, "text": " polynomial to use, but make decisions only looking at the training set and cross validation", "tokens": [50364, 407, 294, 3479, 2539, 3124, 11, 309, 311, 4888, 1151, 3124, 281, 652, 439, 264, 5327, 291, 50712, 50712, 528, 281, 652, 8595, 428, 2539, 9284, 11, 1270, 382, 577, 281, 2826, 9834, 11, 437, 4314, 50984, 50984, 26110, 281, 764, 11, 457, 652, 5327, 787, 1237, 412, 264, 3097, 992, 293, 3278, 24071, 51266, 51266, 992, 293, 281, 406, 764, 264, 1500, 992, 412, 439, 281, 652, 5327, 466, 428, 2316, 13, 51542, 51542, 400, 787, 934, 291, 600, 1027, 439, 729, 5327, 11, 550, 2721, 747, 264, 2316, 291, 362, 4761, 51831, 51831], "temperature": 0.0, "avg_logprob": -0.12515149657259284, "compression_ratio": 1.8170731707317074, "no_speech_prob": 5.539082508221327e-07}, {"id": 155, "seek": 75740, "start": 775.4399999999999, "end": 780.9599999999999, "text": " set and to not use the test set at all to make decisions about your model.", "tokens": [50364, 407, 294, 3479, 2539, 3124, 11, 309, 311, 4888, 1151, 3124, 281, 652, 439, 264, 5327, 291, 50712, 50712, 528, 281, 652, 8595, 428, 2539, 9284, 11, 1270, 382, 577, 281, 2826, 9834, 11, 437, 4314, 50984, 50984, 26110, 281, 764, 11, 457, 652, 5327, 787, 1237, 412, 264, 3097, 992, 293, 3278, 24071, 51266, 51266, 992, 293, 281, 406, 764, 264, 1500, 992, 412, 439, 281, 652, 5327, 466, 428, 2316, 13, 51542, 51542, 400, 787, 934, 291, 600, 1027, 439, 729, 5327, 11, 550, 2721, 747, 264, 2316, 291, 362, 4761, 51831, 51831], "temperature": 0.0, "avg_logprob": -0.12515149657259284, "compression_ratio": 1.8170731707317074, "no_speech_prob": 5.539082508221327e-07}, {"id": 156, "seek": 75740, "start": 780.9599999999999, "end": 786.74, "text": " And only after you've made all those decisions, then finally take the model you have designed", "tokens": [50364, 407, 294, 3479, 2539, 3124, 11, 309, 311, 4888, 1151, 3124, 281, 652, 439, 264, 5327, 291, 50712, 50712, 528, 281, 652, 8595, 428, 2539, 9284, 11, 1270, 382, 577, 281, 2826, 9834, 11, 437, 4314, 50984, 50984, 26110, 281, 764, 11, 457, 652, 5327, 787, 1237, 412, 264, 3097, 992, 293, 3278, 24071, 51266, 51266, 992, 293, 281, 406, 764, 264, 1500, 992, 412, 439, 281, 652, 5327, 466, 428, 2316, 13, 51542, 51542, 400, 787, 934, 291, 600, 1027, 439, 729, 5327, 11, 550, 2721, 747, 264, 2316, 291, 362, 4761, 51831, 51831], "temperature": 0.0, "avg_logprob": -0.12515149657259284, "compression_ratio": 1.8170731707317074, "no_speech_prob": 5.539082508221327e-07}, {"id": 157, "seek": 78674, "start": 786.74, "end": 789.72, "text": " and evaluated on your test set.", "tokens": [50364, 293, 25509, 322, 428, 1500, 992, 13, 50513, 50513, 400, 300, 10747, 28111, 300, 291, 2378, 380, 15715, 3318, 1340, 281, 264, 1500, 992, 11, 50825, 50825, 370, 300, 428, 1500, 992, 3643, 920, 257, 3143, 293, 406, 24324, 19397, 12539, 295, 264, 51067, 51067, 2674, 2144, 6713, 295, 428, 9284, 13, 51193, 51193, 407, 309, 311, 4888, 1151, 3124, 294, 3479, 2539, 300, 498, 291, 362, 281, 652, 5327, 51449, 51449, 466, 428, 2316, 11, 1270, 382, 15669, 9834, 420, 10875, 264, 2316, 9482, 11, 1270, 382, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.08497381210327148, "compression_ratio": 1.6935483870967742, "no_speech_prob": 3.46629036585e-07}, {"id": 158, "seek": 78674, "start": 789.72, "end": 795.96, "text": " And that procedure ensures that you haven't accidentally fit anything to the test set,", "tokens": [50364, 293, 25509, 322, 428, 1500, 992, 13, 50513, 50513, 400, 300, 10747, 28111, 300, 291, 2378, 380, 15715, 3318, 1340, 281, 264, 1500, 992, 11, 50825, 50825, 370, 300, 428, 1500, 992, 3643, 920, 257, 3143, 293, 406, 24324, 19397, 12539, 295, 264, 51067, 51067, 2674, 2144, 6713, 295, 428, 9284, 13, 51193, 51193, 407, 309, 311, 4888, 1151, 3124, 294, 3479, 2539, 300, 498, 291, 362, 281, 652, 5327, 51449, 51449, 466, 428, 2316, 11, 1270, 382, 15669, 9834, 420, 10875, 264, 2316, 9482, 11, 1270, 382, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.08497381210327148, "compression_ratio": 1.6935483870967742, "no_speech_prob": 3.46629036585e-07}, {"id": 159, "seek": 78674, "start": 795.96, "end": 800.8, "text": " so that your test set becomes still a fair and not overly optimistic estimate of the", "tokens": [50364, 293, 25509, 322, 428, 1500, 992, 13, 50513, 50513, 400, 300, 10747, 28111, 300, 291, 2378, 380, 15715, 3318, 1340, 281, 264, 1500, 992, 11, 50825, 50825, 370, 300, 428, 1500, 992, 3643, 920, 257, 3143, 293, 406, 24324, 19397, 12539, 295, 264, 51067, 51067, 2674, 2144, 6713, 295, 428, 9284, 13, 51193, 51193, 407, 309, 311, 4888, 1151, 3124, 294, 3479, 2539, 300, 498, 291, 362, 281, 652, 5327, 51449, 51449, 466, 428, 2316, 11, 1270, 382, 15669, 9834, 420, 10875, 264, 2316, 9482, 11, 1270, 382, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.08497381210327148, "compression_ratio": 1.6935483870967742, "no_speech_prob": 3.46629036585e-07}, {"id": 160, "seek": 78674, "start": 800.8, "end": 803.32, "text": " generalization error of your algorithm.", "tokens": [50364, 293, 25509, 322, 428, 1500, 992, 13, 50513, 50513, 400, 300, 10747, 28111, 300, 291, 2378, 380, 15715, 3318, 1340, 281, 264, 1500, 992, 11, 50825, 50825, 370, 300, 428, 1500, 992, 3643, 920, 257, 3143, 293, 406, 24324, 19397, 12539, 295, 264, 51067, 51067, 2674, 2144, 6713, 295, 428, 9284, 13, 51193, 51193, 407, 309, 311, 4888, 1151, 3124, 294, 3479, 2539, 300, 498, 291, 362, 281, 652, 5327, 51449, 51449, 466, 428, 2316, 11, 1270, 382, 15669, 9834, 420, 10875, 264, 2316, 9482, 11, 1270, 382, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.08497381210327148, "compression_ratio": 1.6935483870967742, "no_speech_prob": 3.46629036585e-07}, {"id": 161, "seek": 78674, "start": 803.32, "end": 808.44, "text": " So it's considered best practice in machine learning that if you have to make decisions", "tokens": [50364, 293, 25509, 322, 428, 1500, 992, 13, 50513, 50513, 400, 300, 10747, 28111, 300, 291, 2378, 380, 15715, 3318, 1340, 281, 264, 1500, 992, 11, 50825, 50825, 370, 300, 428, 1500, 992, 3643, 920, 257, 3143, 293, 406, 24324, 19397, 12539, 295, 264, 51067, 51067, 2674, 2144, 6713, 295, 428, 9284, 13, 51193, 51193, 407, 309, 311, 4888, 1151, 3124, 294, 3479, 2539, 300, 498, 291, 362, 281, 652, 5327, 51449, 51449, 466, 428, 2316, 11, 1270, 382, 15669, 9834, 420, 10875, 264, 2316, 9482, 11, 1270, 382, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.08497381210327148, "compression_ratio": 1.6935483870967742, "no_speech_prob": 3.46629036585e-07}, {"id": 162, "seek": 78674, "start": 808.44, "end": 813.32, "text": " about your model, such as fitting parameters or choosing the model architecture, such as", "tokens": [50364, 293, 25509, 322, 428, 1500, 992, 13, 50513, 50513, 400, 300, 10747, 28111, 300, 291, 2378, 380, 15715, 3318, 1340, 281, 264, 1500, 992, 11, 50825, 50825, 370, 300, 428, 1500, 992, 3643, 920, 257, 3143, 293, 406, 24324, 19397, 12539, 295, 264, 51067, 51067, 2674, 2144, 6713, 295, 428, 9284, 13, 51193, 51193, 407, 309, 311, 4888, 1151, 3124, 294, 3479, 2539, 300, 498, 291, 362, 281, 652, 5327, 51449, 51449, 466, 428, 2316, 11, 1270, 382, 15669, 9834, 420, 10875, 264, 2316, 9482, 11, 1270, 382, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.08497381210327148, "compression_ratio": 1.6935483870967742, "no_speech_prob": 3.46629036585e-07}, {"id": 163, "seek": 81332, "start": 813.32, "end": 818.6, "text": " neural network architecture or degree of polynomial if you're fitting linear regression, to make", "tokens": [50364, 18161, 3209, 9482, 420, 4314, 295, 26110, 498, 291, 434, 15669, 8213, 24590, 11, 281, 652, 50628, 50628, 439, 729, 5327, 787, 1228, 428, 3097, 992, 293, 428, 3278, 24071, 992, 293, 281, 406, 50960, 50960, 574, 412, 264, 1500, 992, 412, 439, 1339, 291, 434, 920, 1455, 5327, 8595, 428, 2539, 9284, 13, 51226, 51226, 400, 309, 311, 787, 934, 291, 600, 808, 493, 365, 472, 2316, 300, 311, 428, 2572, 2316, 281, 787, 550, 51502, 51502, 13059, 309, 322, 264, 1500, 992, 13, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.13202336129177822, "compression_ratio": 1.7510917030567685, "no_speech_prob": 5.714926487598859e-07}, {"id": 164, "seek": 81332, "start": 818.6, "end": 825.24, "text": " all those decisions only using your training set and your cross validation set and to not", "tokens": [50364, 18161, 3209, 9482, 420, 4314, 295, 26110, 498, 291, 434, 15669, 8213, 24590, 11, 281, 652, 50628, 50628, 439, 729, 5327, 787, 1228, 428, 3097, 992, 293, 428, 3278, 24071, 992, 293, 281, 406, 50960, 50960, 574, 412, 264, 1500, 992, 412, 439, 1339, 291, 434, 920, 1455, 5327, 8595, 428, 2539, 9284, 13, 51226, 51226, 400, 309, 311, 787, 934, 291, 600, 808, 493, 365, 472, 2316, 300, 311, 428, 2572, 2316, 281, 787, 550, 51502, 51502, 13059, 309, 322, 264, 1500, 992, 13, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.13202336129177822, "compression_ratio": 1.7510917030567685, "no_speech_prob": 5.714926487598859e-07}, {"id": 165, "seek": 81332, "start": 825.24, "end": 830.5600000000001, "text": " look at the test set at all while you're still making decisions regarding your learning algorithm.", "tokens": [50364, 18161, 3209, 9482, 420, 4314, 295, 26110, 498, 291, 434, 15669, 8213, 24590, 11, 281, 652, 50628, 50628, 439, 729, 5327, 787, 1228, 428, 3097, 992, 293, 428, 3278, 24071, 992, 293, 281, 406, 50960, 50960, 574, 412, 264, 1500, 992, 412, 439, 1339, 291, 434, 920, 1455, 5327, 8595, 428, 2539, 9284, 13, 51226, 51226, 400, 309, 311, 787, 934, 291, 600, 808, 493, 365, 472, 2316, 300, 311, 428, 2572, 2316, 281, 787, 550, 51502, 51502, 13059, 309, 322, 264, 1500, 992, 13, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.13202336129177822, "compression_ratio": 1.7510917030567685, "no_speech_prob": 5.714926487598859e-07}, {"id": 166, "seek": 81332, "start": 830.5600000000001, "end": 836.08, "text": " And it's only after you've come up with one model that's your final model to only then", "tokens": [50364, 18161, 3209, 9482, 420, 4314, 295, 26110, 498, 291, 434, 15669, 8213, 24590, 11, 281, 652, 50628, 50628, 439, 729, 5327, 787, 1228, 428, 3097, 992, 293, 428, 3278, 24071, 992, 293, 281, 406, 50960, 50960, 574, 412, 264, 1500, 992, 412, 439, 1339, 291, 434, 920, 1455, 5327, 8595, 428, 2539, 9284, 13, 51226, 51226, 400, 309, 311, 787, 934, 291, 600, 808, 493, 365, 472, 2316, 300, 311, 428, 2572, 2316, 281, 787, 550, 51502, 51502, 13059, 309, 322, 264, 1500, 992, 13, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.13202336129177822, "compression_ratio": 1.7510917030567685, "no_speech_prob": 5.714926487598859e-07}, {"id": 167, "seek": 81332, "start": 836.08, "end": 838.8000000000001, "text": " evaluate it on the test set.", "tokens": [50364, 18161, 3209, 9482, 420, 4314, 295, 26110, 498, 291, 434, 15669, 8213, 24590, 11, 281, 652, 50628, 50628, 439, 729, 5327, 787, 1228, 428, 3097, 992, 293, 428, 3278, 24071, 992, 293, 281, 406, 50960, 50960, 574, 412, 264, 1500, 992, 412, 439, 1339, 291, 434, 920, 1455, 5327, 8595, 428, 2539, 9284, 13, 51226, 51226, 400, 309, 311, 787, 934, 291, 600, 808, 493, 365, 472, 2316, 300, 311, 428, 2572, 2316, 281, 787, 550, 51502, 51502, 13059, 309, 322, 264, 1500, 992, 13, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.13202336129177822, "compression_ratio": 1.7510917030567685, "no_speech_prob": 5.714926487598859e-07}, {"id": 168, "seek": 83880, "start": 838.8, "end": 843.4, "text": " And because you haven't made any decisions using the test set, that ensures that your", "tokens": [50364, 400, 570, 291, 2378, 380, 1027, 604, 5327, 1228, 264, 1500, 992, 11, 300, 28111, 300, 428, 50594, 50594, 1500, 992, 307, 257, 3143, 293, 406, 24324, 19397, 12539, 295, 577, 731, 428, 2316, 486, 2674, 1125, 50936, 50936, 281, 777, 1412, 13, 51002, 51002, 407, 300, 311, 2316, 9450, 13, 51100, 51100, 400, 341, 307, 767, 257, 588, 13371, 1143, 10747, 13, 51262, 51262, 286, 764, 341, 439, 264, 565, 281, 6772, 2826, 437, 2316, 281, 764, 337, 257, 2212, 3479, 2539, 51524, 51524, 3861, 13, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.12400549584692651, "compression_ratio": 1.6111111111111112, "no_speech_prob": 7.81137259764364e-07}, {"id": 169, "seek": 83880, "start": 843.4, "end": 850.24, "text": " test set is a fair and not overly optimistic estimate of how well your model will generalize", "tokens": [50364, 400, 570, 291, 2378, 380, 1027, 604, 5327, 1228, 264, 1500, 992, 11, 300, 28111, 300, 428, 50594, 50594, 1500, 992, 307, 257, 3143, 293, 406, 24324, 19397, 12539, 295, 577, 731, 428, 2316, 486, 2674, 1125, 50936, 50936, 281, 777, 1412, 13, 51002, 51002, 407, 300, 311, 2316, 9450, 13, 51100, 51100, 400, 341, 307, 767, 257, 588, 13371, 1143, 10747, 13, 51262, 51262, 286, 764, 341, 439, 264, 565, 281, 6772, 2826, 437, 2316, 281, 764, 337, 257, 2212, 3479, 2539, 51524, 51524, 3861, 13, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.12400549584692651, "compression_ratio": 1.6111111111111112, "no_speech_prob": 7.81137259764364e-07}, {"id": 170, "seek": 83880, "start": 850.24, "end": 851.56, "text": " to new data.", "tokens": [50364, 400, 570, 291, 2378, 380, 1027, 604, 5327, 1228, 264, 1500, 992, 11, 300, 28111, 300, 428, 50594, 50594, 1500, 992, 307, 257, 3143, 293, 406, 24324, 19397, 12539, 295, 577, 731, 428, 2316, 486, 2674, 1125, 50936, 50936, 281, 777, 1412, 13, 51002, 51002, 407, 300, 311, 2316, 9450, 13, 51100, 51100, 400, 341, 307, 767, 257, 588, 13371, 1143, 10747, 13, 51262, 51262, 286, 764, 341, 439, 264, 565, 281, 6772, 2826, 437, 2316, 281, 764, 337, 257, 2212, 3479, 2539, 51524, 51524, 3861, 13, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.12400549584692651, "compression_ratio": 1.6111111111111112, "no_speech_prob": 7.81137259764364e-07}, {"id": 171, "seek": 83880, "start": 851.56, "end": 853.52, "text": " So that's model selection.", "tokens": [50364, 400, 570, 291, 2378, 380, 1027, 604, 5327, 1228, 264, 1500, 992, 11, 300, 28111, 300, 428, 50594, 50594, 1500, 992, 307, 257, 3143, 293, 406, 24324, 19397, 12539, 295, 577, 731, 428, 2316, 486, 2674, 1125, 50936, 50936, 281, 777, 1412, 13, 51002, 51002, 407, 300, 311, 2316, 9450, 13, 51100, 51100, 400, 341, 307, 767, 257, 588, 13371, 1143, 10747, 13, 51262, 51262, 286, 764, 341, 439, 264, 565, 281, 6772, 2826, 437, 2316, 281, 764, 337, 257, 2212, 3479, 2539, 51524, 51524, 3861, 13, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.12400549584692651, "compression_ratio": 1.6111111111111112, "no_speech_prob": 7.81137259764364e-07}, {"id": 172, "seek": 83880, "start": 853.52, "end": 856.76, "text": " And this is actually a very widely used procedure.", "tokens": [50364, 400, 570, 291, 2378, 380, 1027, 604, 5327, 1228, 264, 1500, 992, 11, 300, 28111, 300, 428, 50594, 50594, 1500, 992, 307, 257, 3143, 293, 406, 24324, 19397, 12539, 295, 577, 731, 428, 2316, 486, 2674, 1125, 50936, 50936, 281, 777, 1412, 13, 51002, 51002, 407, 300, 311, 2316, 9450, 13, 51100, 51100, 400, 341, 307, 767, 257, 588, 13371, 1143, 10747, 13, 51262, 51262, 286, 764, 341, 439, 264, 565, 281, 6772, 2826, 437, 2316, 281, 764, 337, 257, 2212, 3479, 2539, 51524, 51524, 3861, 13, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.12400549584692651, "compression_ratio": 1.6111111111111112, "no_speech_prob": 7.81137259764364e-07}, {"id": 173, "seek": 83880, "start": 856.76, "end": 862.0, "text": " I use this all the time to automatically choose what model to use for a given machine learning", "tokens": [50364, 400, 570, 291, 2378, 380, 1027, 604, 5327, 1228, 264, 1500, 992, 11, 300, 28111, 300, 428, 50594, 50594, 1500, 992, 307, 257, 3143, 293, 406, 24324, 19397, 12539, 295, 577, 731, 428, 2316, 486, 2674, 1125, 50936, 50936, 281, 777, 1412, 13, 51002, 51002, 407, 300, 311, 2316, 9450, 13, 51100, 51100, 400, 341, 307, 767, 257, 588, 13371, 1143, 10747, 13, 51262, 51262, 286, 764, 341, 439, 264, 565, 281, 6772, 2826, 437, 2316, 281, 764, 337, 257, 2212, 3479, 2539, 51524, 51524, 3861, 13, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.12400549584692651, "compression_ratio": 1.6111111111111112, "no_speech_prob": 7.81137259764364e-07}, {"id": 174, "seek": 83880, "start": 862.0, "end": 864.12, "text": " application.", "tokens": [50364, 400, 570, 291, 2378, 380, 1027, 604, 5327, 1228, 264, 1500, 992, 11, 300, 28111, 300, 428, 50594, 50594, 1500, 992, 307, 257, 3143, 293, 406, 24324, 19397, 12539, 295, 577, 731, 428, 2316, 486, 2674, 1125, 50936, 50936, 281, 777, 1412, 13, 51002, 51002, 407, 300, 311, 2316, 9450, 13, 51100, 51100, 400, 341, 307, 767, 257, 588, 13371, 1143, 10747, 13, 51262, 51262, 286, 764, 341, 439, 264, 565, 281, 6772, 2826, 437, 2316, 281, 764, 337, 257, 2212, 3479, 2539, 51524, 51524, 3861, 13, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.12400549584692651, "compression_ratio": 1.6111111111111112, "no_speech_prob": 7.81137259764364e-07}, {"id": 175, "seek": 86412, "start": 864.12, "end": 870.28, "text": " Now earlier this week, I mentioned running diagnostics to decide how to improve the performance", "tokens": [50364, 823, 3071, 341, 1243, 11, 286, 2835, 2614, 43215, 1167, 281, 4536, 577, 281, 3470, 264, 3389, 50672, 50672, 295, 257, 2539, 9284, 13, 50765, 50765, 823, 300, 291, 362, 257, 636, 281, 13059, 2539, 14642, 293, 754, 6772, 2826, 257, 50996, 50996, 2316, 11, 718, 311, 9192, 544, 8760, 666, 5110, 295, 512, 43215, 1167, 13, 51222, 51222, 440, 881, 4005, 27897, 300, 286, 458, 295, 293, 300, 286, 764, 337, 257, 688, 295, 3479, 2539, 51450, 51450, 5821, 307, 472, 1219, 12577, 293, 21977, 13, 51617, 51617], "temperature": 0.0, "avg_logprob": -0.12273258167308765, "compression_ratio": 1.6929460580912863, "no_speech_prob": 7.182355602708412e-06}, {"id": 176, "seek": 86412, "start": 870.28, "end": 872.14, "text": " of a learning algorithm.", "tokens": [50364, 823, 3071, 341, 1243, 11, 286, 2835, 2614, 43215, 1167, 281, 4536, 577, 281, 3470, 264, 3389, 50672, 50672, 295, 257, 2539, 9284, 13, 50765, 50765, 823, 300, 291, 362, 257, 636, 281, 13059, 2539, 14642, 293, 754, 6772, 2826, 257, 50996, 50996, 2316, 11, 718, 311, 9192, 544, 8760, 666, 5110, 295, 512, 43215, 1167, 13, 51222, 51222, 440, 881, 4005, 27897, 300, 286, 458, 295, 293, 300, 286, 764, 337, 257, 688, 295, 3479, 2539, 51450, 51450, 5821, 307, 472, 1219, 12577, 293, 21977, 13, 51617, 51617], "temperature": 0.0, "avg_logprob": -0.12273258167308765, "compression_ratio": 1.6929460580912863, "no_speech_prob": 7.182355602708412e-06}, {"id": 177, "seek": 86412, "start": 872.14, "end": 876.76, "text": " Now that you have a way to evaluate learning algorithms and even automatically choose a", "tokens": [50364, 823, 3071, 341, 1243, 11, 286, 2835, 2614, 43215, 1167, 281, 4536, 577, 281, 3470, 264, 3389, 50672, 50672, 295, 257, 2539, 9284, 13, 50765, 50765, 823, 300, 291, 362, 257, 636, 281, 13059, 2539, 14642, 293, 754, 6772, 2826, 257, 50996, 50996, 2316, 11, 718, 311, 9192, 544, 8760, 666, 5110, 295, 512, 43215, 1167, 13, 51222, 51222, 440, 881, 4005, 27897, 300, 286, 458, 295, 293, 300, 286, 764, 337, 257, 688, 295, 3479, 2539, 51450, 51450, 5821, 307, 472, 1219, 12577, 293, 21977, 13, 51617, 51617], "temperature": 0.0, "avg_logprob": -0.12273258167308765, "compression_ratio": 1.6929460580912863, "no_speech_prob": 7.182355602708412e-06}, {"id": 178, "seek": 86412, "start": 876.76, "end": 881.28, "text": " model, let's dive more deeply into examples of some diagnostics.", "tokens": [50364, 823, 3071, 341, 1243, 11, 286, 2835, 2614, 43215, 1167, 281, 4536, 577, 281, 3470, 264, 3389, 50672, 50672, 295, 257, 2539, 9284, 13, 50765, 50765, 823, 300, 291, 362, 257, 636, 281, 13059, 2539, 14642, 293, 754, 6772, 2826, 257, 50996, 50996, 2316, 11, 718, 311, 9192, 544, 8760, 666, 5110, 295, 512, 43215, 1167, 13, 51222, 51222, 440, 881, 4005, 27897, 300, 286, 458, 295, 293, 300, 286, 764, 337, 257, 688, 295, 3479, 2539, 51450, 51450, 5821, 307, 472, 1219, 12577, 293, 21977, 13, 51617, 51617], "temperature": 0.0, "avg_logprob": -0.12273258167308765, "compression_ratio": 1.6929460580912863, "no_speech_prob": 7.182355602708412e-06}, {"id": 179, "seek": 86412, "start": 881.28, "end": 885.84, "text": " The most powerful diagnostic that I know of and that I use for a lot of machine learning", "tokens": [50364, 823, 3071, 341, 1243, 11, 286, 2835, 2614, 43215, 1167, 281, 4536, 577, 281, 3470, 264, 3389, 50672, 50672, 295, 257, 2539, 9284, 13, 50765, 50765, 823, 300, 291, 362, 257, 636, 281, 13059, 2539, 14642, 293, 754, 6772, 2826, 257, 50996, 50996, 2316, 11, 718, 311, 9192, 544, 8760, 666, 5110, 295, 512, 43215, 1167, 13, 51222, 51222, 440, 881, 4005, 27897, 300, 286, 458, 295, 293, 300, 286, 764, 337, 257, 688, 295, 3479, 2539, 51450, 51450, 5821, 307, 472, 1219, 12577, 293, 21977, 13, 51617, 51617], "temperature": 0.0, "avg_logprob": -0.12273258167308765, "compression_ratio": 1.6929460580912863, "no_speech_prob": 7.182355602708412e-06}, {"id": 180, "seek": 86412, "start": 885.84, "end": 889.1800000000001, "text": " applications is one called bias and variance.", "tokens": [50364, 823, 3071, 341, 1243, 11, 286, 2835, 2614, 43215, 1167, 281, 4536, 577, 281, 3470, 264, 3389, 50672, 50672, 295, 257, 2539, 9284, 13, 50765, 50765, 823, 300, 291, 362, 257, 636, 281, 13059, 2539, 14642, 293, 754, 6772, 2826, 257, 50996, 50996, 2316, 11, 718, 311, 9192, 544, 8760, 666, 5110, 295, 512, 43215, 1167, 13, 51222, 51222, 440, 881, 4005, 27897, 300, 286, 458, 295, 293, 300, 286, 764, 337, 257, 688, 295, 3479, 2539, 51450, 51450, 5821, 307, 472, 1219, 12577, 293, 21977, 13, 51617, 51617], "temperature": 0.0, "avg_logprob": -0.12273258167308765, "compression_ratio": 1.6929460580912863, "no_speech_prob": 7.182355602708412e-06}, {"id": 181, "seek": 88918, "start": 889.18, "end": 894.28, "text": " Let's take a look at what that means in the next video.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 437, 300, 1355, 294, 264, 958, 960, 13, 50619], "temperature": 0.0, "avg_logprob": -0.2883281988256118, "compression_ratio": 0.9482758620689655, "no_speech_prob": 0.0002188767393818125}], "language": "en", "video_id": "KwM_IYQ_I-8", "entity": "ML Specialization, Andrew Ng (2022)"}}