{"video_id": "24QO9iNXvWs", "title": "5.8 Multiclass Classification  | Neural Network with Softmax output --[Machine Learning | Andrew Ng]", "description": "Second Course:\nAdvanced Learning Algorithms.\n\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 444, "views": 86, "publish_date": "11/04/2022", "timestamp": 1661472000, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " In order to build a neural network that can carry out multi-class classification, we're going to take the Softmax regression model and put it into essentially the output layer of a neural network. Let's take a look at how to do that. Previously, when we were doing handwritten digit recognition with just two clauses, we used a neural network with this architecture. If you now want to do handwritten digit classification with 10 clauses, all the digits from 0 to 9, then we're going to change this neural network to have 10 output units, like so. And this new output layer will be a Softmax output layer. So sometimes we'll say this neural network has a Softmax output or that this output layer is a Softmax layer. And the way forward propagation works in this neural network is, given an input x, a1 gets computed exactly the same as before, and then a2, deactivations for the second hidden layer, also get computed exactly the same as before. And we now have to compute deactivations for this output layer. That is a3. This is how it works. If you have 10 output clauses, we will compute z1, z2, through z10 using these expressions. So this is actually very similar to what we had previously for the formula you used to compute z. z1 is w1.product with a2, deactivations from the previous layer, plus b1, and so on, for z1 through z10. Then a1 is equal to e to the z1 divided by e to the z1 plus dot dot dot plus up to e to the z10. And that's our estimate of the chance that y is equal to 1. And similarly for a2, and similarly all the way up to a10. And so this gives you your estimates of the chance of y being equal to 1, 2, and so on up through the 10th possible label for y. And just for completeness, if you want to indicate that these are the quantities associated with layer 3, technically I should add these superscript 3s there. It does make the notation a little bit more cluttered, but this makes explicit that this is, for example, the z31 value and this is the parameters associated with the first unit of layer 3 of this neural network. And with this, your softmax output layer now gives you estimates of the chance of y being any of these 10 possible output labels. I do want to mention that the softmax layer, or sometimes also called the softmax activation function, it is a little bit unusual in one respect compared to the other activation functions we've seen so far like sigmoid, ReLU, and linear, which is that when we're looking at sigmoid or ReLU or linear activation functions, a1 was a function of z1 and a2 was a function of z2 and only z2. In other words, to obtain the activation values, we could apply the activation function g, be it sigmoid or ReLU or something else, element-wise to z1 and z2 and so on to get any of the a1 and a2 and a3 and a4. But with the softmax activation function, notice that a1 is a function of z1 and z2 and z3 all the way up to z10. So each of these activation values depends on all of the values of z. And this is a property that's a bit unique to the softmax output or the softmax activation function. I'll state it differently, if you want to compute a1 through a10, that is a function of z1 all the way up to z10 simultaneously. And this is unlike the other activation functions we've seen so far. Finally, let's look at how you would implement this in TensorFlow. If you want to implement the neural network that I've shown here on this slide, this is the code to do so. Similar as before, there are three steps to specifying and training the model. The first step is to tell TensorFlow to sequentially string together three layers. First layer is this 25 units with ReLU activation function, second layer, 15 units with ReLU activation function, and then the third layer, because there are now 10 output units, you want to output a1 through a10, so there are now 10 output units, and we'll tell TensorFlow to use the softmax activation function. And the cost function that you saw in the last video, TensorFlow calls that the sparse categorical cross entropy function. So I know this name is a bit of a mouthful, whereas for logistic regression, we had the binary cross entropy function. Here we're using the sparse categorical cross entropy function, and what sparse categorical refers to is that you still classify y into categories, so it's categorical, it takes on values from 1 to 10, and sparse refers to that y can only take on one of these 10 values, so each image is either 0 or 1 or 2 or so on up to 9, you're not going to see a picture that is simultaneously the number 2 and the number 7, so sparse refers to that each digit is only one of these categories. So that's why the loss function that you saw in the last video is called, in TensorFlow, the sparse categorical cross entropy loss function. And then the code for training the model is just the same as before, and if you use this code you can train a neural network on a multi-class classification problem. Just one important note, if you use this code exactly as I've written here, it will work, but don't actually use this code, because it turns out that in TensorFlow there's a better version of the code that makes TensorFlow work better. So even though the code shown in this slide works, don't use this code the way I've written it here, because in a later video this week you'll see a different version, there's actually the recommended version of implementing this that will work better, but we'll take a look at that in a later video. So now you know how to train a neural network with a softmax output layer with one caveat. There's a different version of the code that will make TensorFlow able to compute these probabilities much more accurately. Let's take a look at that in the next video, which will also show you the actual code that I recommend you use if you're training a softmax neural network. Let's go on to the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.12, "text": " In order to build a neural network that can carry out multi-class classification, we're", "tokens": [50364, 682, 1668, 281, 1322, 257, 18161, 3209, 300, 393, 3985, 484, 4825, 12, 11665, 21538, 11, 321, 434, 50720, 50720, 516, 281, 747, 264, 16985, 41167, 24590, 2316, 293, 829, 309, 666, 4476, 264, 5598, 4583, 51026, 51026, 295, 257, 18161, 3209, 13, 51076, 51076, 961, 311, 747, 257, 574, 412, 577, 281, 360, 300, 13, 51210, 51210, 33606, 11, 562, 321, 645, 884, 1011, 26859, 14293, 11150, 365, 445, 732, 49072, 11, 321, 51513, 51513, 1143, 257, 18161, 3209, 365, 341, 9482, 13, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.17357481609691272, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.014945278875529766}, {"id": 1, "seek": 0, "start": 7.12, "end": 13.24, "text": " going to take the Softmax regression model and put it into essentially the output layer", "tokens": [50364, 682, 1668, 281, 1322, 257, 18161, 3209, 300, 393, 3985, 484, 4825, 12, 11665, 21538, 11, 321, 434, 50720, 50720, 516, 281, 747, 264, 16985, 41167, 24590, 2316, 293, 829, 309, 666, 4476, 264, 5598, 4583, 51026, 51026, 295, 257, 18161, 3209, 13, 51076, 51076, 961, 311, 747, 257, 574, 412, 577, 281, 360, 300, 13, 51210, 51210, 33606, 11, 562, 321, 645, 884, 1011, 26859, 14293, 11150, 365, 445, 732, 49072, 11, 321, 51513, 51513, 1143, 257, 18161, 3209, 365, 341, 9482, 13, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.17357481609691272, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.014945278875529766}, {"id": 2, "seek": 0, "start": 13.24, "end": 14.24, "text": " of a neural network.", "tokens": [50364, 682, 1668, 281, 1322, 257, 18161, 3209, 300, 393, 3985, 484, 4825, 12, 11665, 21538, 11, 321, 434, 50720, 50720, 516, 281, 747, 264, 16985, 41167, 24590, 2316, 293, 829, 309, 666, 4476, 264, 5598, 4583, 51026, 51026, 295, 257, 18161, 3209, 13, 51076, 51076, 961, 311, 747, 257, 574, 412, 577, 281, 360, 300, 13, 51210, 51210, 33606, 11, 562, 321, 645, 884, 1011, 26859, 14293, 11150, 365, 445, 732, 49072, 11, 321, 51513, 51513, 1143, 257, 18161, 3209, 365, 341, 9482, 13, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.17357481609691272, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.014945278875529766}, {"id": 3, "seek": 0, "start": 14.24, "end": 16.92, "text": " Let's take a look at how to do that.", "tokens": [50364, 682, 1668, 281, 1322, 257, 18161, 3209, 300, 393, 3985, 484, 4825, 12, 11665, 21538, 11, 321, 434, 50720, 50720, 516, 281, 747, 264, 16985, 41167, 24590, 2316, 293, 829, 309, 666, 4476, 264, 5598, 4583, 51026, 51026, 295, 257, 18161, 3209, 13, 51076, 51076, 961, 311, 747, 257, 574, 412, 577, 281, 360, 300, 13, 51210, 51210, 33606, 11, 562, 321, 645, 884, 1011, 26859, 14293, 11150, 365, 445, 732, 49072, 11, 321, 51513, 51513, 1143, 257, 18161, 3209, 365, 341, 9482, 13, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.17357481609691272, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.014945278875529766}, {"id": 4, "seek": 0, "start": 16.92, "end": 22.98, "text": " Previously, when we were doing handwritten digit recognition with just two clauses, we", "tokens": [50364, 682, 1668, 281, 1322, 257, 18161, 3209, 300, 393, 3985, 484, 4825, 12, 11665, 21538, 11, 321, 434, 50720, 50720, 516, 281, 747, 264, 16985, 41167, 24590, 2316, 293, 829, 309, 666, 4476, 264, 5598, 4583, 51026, 51026, 295, 257, 18161, 3209, 13, 51076, 51076, 961, 311, 747, 257, 574, 412, 577, 281, 360, 300, 13, 51210, 51210, 33606, 11, 562, 321, 645, 884, 1011, 26859, 14293, 11150, 365, 445, 732, 49072, 11, 321, 51513, 51513, 1143, 257, 18161, 3209, 365, 341, 9482, 13, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.17357481609691272, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.014945278875529766}, {"id": 5, "seek": 0, "start": 22.98, "end": 26.580000000000002, "text": " used a neural network with this architecture.", "tokens": [50364, 682, 1668, 281, 1322, 257, 18161, 3209, 300, 393, 3985, 484, 4825, 12, 11665, 21538, 11, 321, 434, 50720, 50720, 516, 281, 747, 264, 16985, 41167, 24590, 2316, 293, 829, 309, 666, 4476, 264, 5598, 4583, 51026, 51026, 295, 257, 18161, 3209, 13, 51076, 51076, 961, 311, 747, 257, 574, 412, 577, 281, 360, 300, 13, 51210, 51210, 33606, 11, 562, 321, 645, 884, 1011, 26859, 14293, 11150, 365, 445, 732, 49072, 11, 321, 51513, 51513, 1143, 257, 18161, 3209, 365, 341, 9482, 13, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.17357481609691272, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.014945278875529766}, {"id": 6, "seek": 2658, "start": 26.58, "end": 34.519999999999996, "text": " If you now want to do handwritten digit classification with 10 clauses, all the digits from 0 to", "tokens": [50364, 759, 291, 586, 528, 281, 360, 1011, 26859, 14293, 21538, 365, 1266, 49072, 11, 439, 264, 27011, 490, 1958, 281, 50761, 50761, 1722, 11, 550, 321, 434, 516, 281, 1319, 341, 18161, 3209, 281, 362, 1266, 5598, 6815, 11, 411, 370, 13, 51161, 51161, 400, 341, 777, 5598, 4583, 486, 312, 257, 16985, 41167, 5598, 4583, 13, 51387, 51387, 407, 2171, 321, 603, 584, 341, 18161, 3209, 575, 257, 16985, 41167, 5598, 420, 300, 341, 5598, 4583, 51625, 51625, 307, 257, 16985, 41167, 4583, 13, 51785, 51785], "temperature": 0.0, "avg_logprob": -0.13266176588079903, "compression_ratio": 1.766497461928934, "no_speech_prob": 4.356811587058473e-06}, {"id": 7, "seek": 2658, "start": 34.519999999999996, "end": 42.519999999999996, "text": " 9, then we're going to change this neural network to have 10 output units, like so.", "tokens": [50364, 759, 291, 586, 528, 281, 360, 1011, 26859, 14293, 21538, 365, 1266, 49072, 11, 439, 264, 27011, 490, 1958, 281, 50761, 50761, 1722, 11, 550, 321, 434, 516, 281, 1319, 341, 18161, 3209, 281, 362, 1266, 5598, 6815, 11, 411, 370, 13, 51161, 51161, 400, 341, 777, 5598, 4583, 486, 312, 257, 16985, 41167, 5598, 4583, 13, 51387, 51387, 407, 2171, 321, 603, 584, 341, 18161, 3209, 575, 257, 16985, 41167, 5598, 420, 300, 341, 5598, 4583, 51625, 51625, 307, 257, 16985, 41167, 4583, 13, 51785, 51785], "temperature": 0.0, "avg_logprob": -0.13266176588079903, "compression_ratio": 1.766497461928934, "no_speech_prob": 4.356811587058473e-06}, {"id": 8, "seek": 2658, "start": 42.519999999999996, "end": 47.04, "text": " And this new output layer will be a Softmax output layer.", "tokens": [50364, 759, 291, 586, 528, 281, 360, 1011, 26859, 14293, 21538, 365, 1266, 49072, 11, 439, 264, 27011, 490, 1958, 281, 50761, 50761, 1722, 11, 550, 321, 434, 516, 281, 1319, 341, 18161, 3209, 281, 362, 1266, 5598, 6815, 11, 411, 370, 13, 51161, 51161, 400, 341, 777, 5598, 4583, 486, 312, 257, 16985, 41167, 5598, 4583, 13, 51387, 51387, 407, 2171, 321, 603, 584, 341, 18161, 3209, 575, 257, 16985, 41167, 5598, 420, 300, 341, 5598, 4583, 51625, 51625, 307, 257, 16985, 41167, 4583, 13, 51785, 51785], "temperature": 0.0, "avg_logprob": -0.13266176588079903, "compression_ratio": 1.766497461928934, "no_speech_prob": 4.356811587058473e-06}, {"id": 9, "seek": 2658, "start": 47.04, "end": 51.8, "text": " So sometimes we'll say this neural network has a Softmax output or that this output layer", "tokens": [50364, 759, 291, 586, 528, 281, 360, 1011, 26859, 14293, 21538, 365, 1266, 49072, 11, 439, 264, 27011, 490, 1958, 281, 50761, 50761, 1722, 11, 550, 321, 434, 516, 281, 1319, 341, 18161, 3209, 281, 362, 1266, 5598, 6815, 11, 411, 370, 13, 51161, 51161, 400, 341, 777, 5598, 4583, 486, 312, 257, 16985, 41167, 5598, 4583, 13, 51387, 51387, 407, 2171, 321, 603, 584, 341, 18161, 3209, 575, 257, 16985, 41167, 5598, 420, 300, 341, 5598, 4583, 51625, 51625, 307, 257, 16985, 41167, 4583, 13, 51785, 51785], "temperature": 0.0, "avg_logprob": -0.13266176588079903, "compression_ratio": 1.766497461928934, "no_speech_prob": 4.356811587058473e-06}, {"id": 10, "seek": 2658, "start": 51.8, "end": 55.0, "text": " is a Softmax layer.", "tokens": [50364, 759, 291, 586, 528, 281, 360, 1011, 26859, 14293, 21538, 365, 1266, 49072, 11, 439, 264, 27011, 490, 1958, 281, 50761, 50761, 1722, 11, 550, 321, 434, 516, 281, 1319, 341, 18161, 3209, 281, 362, 1266, 5598, 6815, 11, 411, 370, 13, 51161, 51161, 400, 341, 777, 5598, 4583, 486, 312, 257, 16985, 41167, 5598, 4583, 13, 51387, 51387, 407, 2171, 321, 603, 584, 341, 18161, 3209, 575, 257, 16985, 41167, 5598, 420, 300, 341, 5598, 4583, 51625, 51625, 307, 257, 16985, 41167, 4583, 13, 51785, 51785], "temperature": 0.0, "avg_logprob": -0.13266176588079903, "compression_ratio": 1.766497461928934, "no_speech_prob": 4.356811587058473e-06}, {"id": 11, "seek": 5500, "start": 55.0, "end": 61.08, "text": " And the way forward propagation works in this neural network is, given an input x,", "tokens": [50364, 400, 264, 636, 2128, 38377, 1985, 294, 341, 18161, 3209, 307, 11, 2212, 364, 4846, 2031, 11, 50668, 50668, 257, 16, 2170, 40610, 2293, 264, 912, 382, 949, 11, 293, 550, 257, 17, 11, 45428, 763, 337, 264, 1150, 50974, 50974, 7633, 4583, 11, 611, 483, 40610, 2293, 264, 912, 382, 949, 13, 51244, 51244, 400, 321, 586, 362, 281, 14722, 45428, 763, 337, 341, 5598, 4583, 13, 51494, 51494, 663, 307, 257, 18, 13, 51604, 51604, 639, 307, 577, 309, 1985, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.17032738389640018, "compression_ratio": 1.7301587301587302, "no_speech_prob": 6.438925083784852e-06}, {"id": 12, "seek": 5500, "start": 61.08, "end": 67.2, "text": " a1 gets computed exactly the same as before, and then a2, deactivations for the second", "tokens": [50364, 400, 264, 636, 2128, 38377, 1985, 294, 341, 18161, 3209, 307, 11, 2212, 364, 4846, 2031, 11, 50668, 50668, 257, 16, 2170, 40610, 2293, 264, 912, 382, 949, 11, 293, 550, 257, 17, 11, 45428, 763, 337, 264, 1150, 50974, 50974, 7633, 4583, 11, 611, 483, 40610, 2293, 264, 912, 382, 949, 13, 51244, 51244, 400, 321, 586, 362, 281, 14722, 45428, 763, 337, 341, 5598, 4583, 13, 51494, 51494, 663, 307, 257, 18, 13, 51604, 51604, 639, 307, 577, 309, 1985, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.17032738389640018, "compression_ratio": 1.7301587301587302, "no_speech_prob": 6.438925083784852e-06}, {"id": 13, "seek": 5500, "start": 67.2, "end": 72.6, "text": " hidden layer, also get computed exactly the same as before.", "tokens": [50364, 400, 264, 636, 2128, 38377, 1985, 294, 341, 18161, 3209, 307, 11, 2212, 364, 4846, 2031, 11, 50668, 50668, 257, 16, 2170, 40610, 2293, 264, 912, 382, 949, 11, 293, 550, 257, 17, 11, 45428, 763, 337, 264, 1150, 50974, 50974, 7633, 4583, 11, 611, 483, 40610, 2293, 264, 912, 382, 949, 13, 51244, 51244, 400, 321, 586, 362, 281, 14722, 45428, 763, 337, 341, 5598, 4583, 13, 51494, 51494, 663, 307, 257, 18, 13, 51604, 51604, 639, 307, 577, 309, 1985, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.17032738389640018, "compression_ratio": 1.7301587301587302, "no_speech_prob": 6.438925083784852e-06}, {"id": 14, "seek": 5500, "start": 72.6, "end": 77.6, "text": " And we now have to compute deactivations for this output layer.", "tokens": [50364, 400, 264, 636, 2128, 38377, 1985, 294, 341, 18161, 3209, 307, 11, 2212, 364, 4846, 2031, 11, 50668, 50668, 257, 16, 2170, 40610, 2293, 264, 912, 382, 949, 11, 293, 550, 257, 17, 11, 45428, 763, 337, 264, 1150, 50974, 50974, 7633, 4583, 11, 611, 483, 40610, 2293, 264, 912, 382, 949, 13, 51244, 51244, 400, 321, 586, 362, 281, 14722, 45428, 763, 337, 341, 5598, 4583, 13, 51494, 51494, 663, 307, 257, 18, 13, 51604, 51604, 639, 307, 577, 309, 1985, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.17032738389640018, "compression_ratio": 1.7301587301587302, "no_speech_prob": 6.438925083784852e-06}, {"id": 15, "seek": 5500, "start": 77.6, "end": 79.8, "text": " That is a3.", "tokens": [50364, 400, 264, 636, 2128, 38377, 1985, 294, 341, 18161, 3209, 307, 11, 2212, 364, 4846, 2031, 11, 50668, 50668, 257, 16, 2170, 40610, 2293, 264, 912, 382, 949, 11, 293, 550, 257, 17, 11, 45428, 763, 337, 264, 1150, 50974, 50974, 7633, 4583, 11, 611, 483, 40610, 2293, 264, 912, 382, 949, 13, 51244, 51244, 400, 321, 586, 362, 281, 14722, 45428, 763, 337, 341, 5598, 4583, 13, 51494, 51494, 663, 307, 257, 18, 13, 51604, 51604, 639, 307, 577, 309, 1985, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.17032738389640018, "compression_ratio": 1.7301587301587302, "no_speech_prob": 6.438925083784852e-06}, {"id": 16, "seek": 5500, "start": 79.8, "end": 81.8, "text": " This is how it works.", "tokens": [50364, 400, 264, 636, 2128, 38377, 1985, 294, 341, 18161, 3209, 307, 11, 2212, 364, 4846, 2031, 11, 50668, 50668, 257, 16, 2170, 40610, 2293, 264, 912, 382, 949, 11, 293, 550, 257, 17, 11, 45428, 763, 337, 264, 1150, 50974, 50974, 7633, 4583, 11, 611, 483, 40610, 2293, 264, 912, 382, 949, 13, 51244, 51244, 400, 321, 586, 362, 281, 14722, 45428, 763, 337, 341, 5598, 4583, 13, 51494, 51494, 663, 307, 257, 18, 13, 51604, 51604, 639, 307, 577, 309, 1985, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.17032738389640018, "compression_ratio": 1.7301587301587302, "no_speech_prob": 6.438925083784852e-06}, {"id": 17, "seek": 8180, "start": 81.8, "end": 89.64, "text": " If you have 10 output clauses, we will compute z1, z2, through z10 using these expressions.", "tokens": [50364, 759, 291, 362, 1266, 5598, 49072, 11, 321, 486, 14722, 710, 16, 11, 710, 17, 11, 807, 710, 3279, 1228, 613, 15277, 13, 50756, 50756, 407, 341, 307, 767, 588, 2531, 281, 437, 321, 632, 8046, 337, 264, 8513, 291, 1143, 281, 51006, 51006, 14722, 710, 13, 51056, 51056, 710, 16, 307, 261, 16, 13, 33244, 365, 257, 17, 11, 45428, 763, 490, 264, 3894, 4583, 11, 1804, 272, 16, 11, 293, 370, 322, 11, 337, 51494, 51494, 710, 16, 807, 710, 3279, 13, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.1424801458011974, "compression_ratio": 1.5, "no_speech_prob": 8.530166269338224e-06}, {"id": 18, "seek": 8180, "start": 89.64, "end": 94.64, "text": " So this is actually very similar to what we had previously for the formula you used to", "tokens": [50364, 759, 291, 362, 1266, 5598, 49072, 11, 321, 486, 14722, 710, 16, 11, 710, 17, 11, 807, 710, 3279, 1228, 613, 15277, 13, 50756, 50756, 407, 341, 307, 767, 588, 2531, 281, 437, 321, 632, 8046, 337, 264, 8513, 291, 1143, 281, 51006, 51006, 14722, 710, 13, 51056, 51056, 710, 16, 307, 261, 16, 13, 33244, 365, 257, 17, 11, 45428, 763, 490, 264, 3894, 4583, 11, 1804, 272, 16, 11, 293, 370, 322, 11, 337, 51494, 51494, 710, 16, 807, 710, 3279, 13, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.1424801458011974, "compression_ratio": 1.5, "no_speech_prob": 8.530166269338224e-06}, {"id": 19, "seek": 8180, "start": 94.64, "end": 95.64, "text": " compute z.", "tokens": [50364, 759, 291, 362, 1266, 5598, 49072, 11, 321, 486, 14722, 710, 16, 11, 710, 17, 11, 807, 710, 3279, 1228, 613, 15277, 13, 50756, 50756, 407, 341, 307, 767, 588, 2531, 281, 437, 321, 632, 8046, 337, 264, 8513, 291, 1143, 281, 51006, 51006, 14722, 710, 13, 51056, 51056, 710, 16, 307, 261, 16, 13, 33244, 365, 257, 17, 11, 45428, 763, 490, 264, 3894, 4583, 11, 1804, 272, 16, 11, 293, 370, 322, 11, 337, 51494, 51494, 710, 16, 807, 710, 3279, 13, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.1424801458011974, "compression_ratio": 1.5, "no_speech_prob": 8.530166269338224e-06}, {"id": 20, "seek": 8180, "start": 95.64, "end": 104.4, "text": " z1 is w1.product with a2, deactivations from the previous layer, plus b1, and so on, for", "tokens": [50364, 759, 291, 362, 1266, 5598, 49072, 11, 321, 486, 14722, 710, 16, 11, 710, 17, 11, 807, 710, 3279, 1228, 613, 15277, 13, 50756, 50756, 407, 341, 307, 767, 588, 2531, 281, 437, 321, 632, 8046, 337, 264, 8513, 291, 1143, 281, 51006, 51006, 14722, 710, 13, 51056, 51056, 710, 16, 307, 261, 16, 13, 33244, 365, 257, 17, 11, 45428, 763, 490, 264, 3894, 4583, 11, 1804, 272, 16, 11, 293, 370, 322, 11, 337, 51494, 51494, 710, 16, 807, 710, 3279, 13, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.1424801458011974, "compression_ratio": 1.5, "no_speech_prob": 8.530166269338224e-06}, {"id": 21, "seek": 8180, "start": 104.4, "end": 107.2, "text": " z1 through z10.", "tokens": [50364, 759, 291, 362, 1266, 5598, 49072, 11, 321, 486, 14722, 710, 16, 11, 710, 17, 11, 807, 710, 3279, 1228, 613, 15277, 13, 50756, 50756, 407, 341, 307, 767, 588, 2531, 281, 437, 321, 632, 8046, 337, 264, 8513, 291, 1143, 281, 51006, 51006, 14722, 710, 13, 51056, 51056, 710, 16, 307, 261, 16, 13, 33244, 365, 257, 17, 11, 45428, 763, 490, 264, 3894, 4583, 11, 1804, 272, 16, 11, 293, 370, 322, 11, 337, 51494, 51494, 710, 16, 807, 710, 3279, 13, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.1424801458011974, "compression_ratio": 1.5, "no_speech_prob": 8.530166269338224e-06}, {"id": 22, "seek": 10720, "start": 107.2, "end": 116.24000000000001, "text": " Then a1 is equal to e to the z1 divided by e to the z1 plus dot dot dot plus up to e", "tokens": [50364, 1396, 257, 16, 307, 2681, 281, 308, 281, 264, 710, 16, 6666, 538, 308, 281, 264, 710, 16, 1804, 5893, 5893, 5893, 1804, 493, 281, 308, 50816, 50816, 281, 264, 710, 3279, 13, 50894, 50894, 400, 300, 311, 527, 12539, 295, 264, 2931, 300, 288, 307, 2681, 281, 502, 13, 51142, 51142, 400, 14138, 337, 257, 17, 11, 293, 14138, 439, 264, 636, 493, 281, 257, 3279, 13, 51480, 51480, 400, 370, 341, 2709, 291, 428, 20561, 295, 264, 2931, 295, 288, 885, 2681, 281, 502, 11, 568, 11, 293, 370, 322, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.08321450650691986, "compression_ratio": 1.875776397515528, "no_speech_prob": 3.647664198069833e-05}, {"id": 23, "seek": 10720, "start": 116.24000000000001, "end": 117.8, "text": " to the z10.", "tokens": [50364, 1396, 257, 16, 307, 2681, 281, 308, 281, 264, 710, 16, 6666, 538, 308, 281, 264, 710, 16, 1804, 5893, 5893, 5893, 1804, 493, 281, 308, 50816, 50816, 281, 264, 710, 3279, 13, 50894, 50894, 400, 300, 311, 527, 12539, 295, 264, 2931, 300, 288, 307, 2681, 281, 502, 13, 51142, 51142, 400, 14138, 337, 257, 17, 11, 293, 14138, 439, 264, 636, 493, 281, 257, 3279, 13, 51480, 51480, 400, 370, 341, 2709, 291, 428, 20561, 295, 264, 2931, 295, 288, 885, 2681, 281, 502, 11, 568, 11, 293, 370, 322, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.08321450650691986, "compression_ratio": 1.875776397515528, "no_speech_prob": 3.647664198069833e-05}, {"id": 24, "seek": 10720, "start": 117.8, "end": 122.76, "text": " And that's our estimate of the chance that y is equal to 1.", "tokens": [50364, 1396, 257, 16, 307, 2681, 281, 308, 281, 264, 710, 16, 6666, 538, 308, 281, 264, 710, 16, 1804, 5893, 5893, 5893, 1804, 493, 281, 308, 50816, 50816, 281, 264, 710, 3279, 13, 50894, 50894, 400, 300, 311, 527, 12539, 295, 264, 2931, 300, 288, 307, 2681, 281, 502, 13, 51142, 51142, 400, 14138, 337, 257, 17, 11, 293, 14138, 439, 264, 636, 493, 281, 257, 3279, 13, 51480, 51480, 400, 370, 341, 2709, 291, 428, 20561, 295, 264, 2931, 295, 288, 885, 2681, 281, 502, 11, 568, 11, 293, 370, 322, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.08321450650691986, "compression_ratio": 1.875776397515528, "no_speech_prob": 3.647664198069833e-05}, {"id": 25, "seek": 10720, "start": 122.76, "end": 129.52, "text": " And similarly for a2, and similarly all the way up to a10.", "tokens": [50364, 1396, 257, 16, 307, 2681, 281, 308, 281, 264, 710, 16, 6666, 538, 308, 281, 264, 710, 16, 1804, 5893, 5893, 5893, 1804, 493, 281, 308, 50816, 50816, 281, 264, 710, 3279, 13, 50894, 50894, 400, 300, 311, 527, 12539, 295, 264, 2931, 300, 288, 307, 2681, 281, 502, 13, 51142, 51142, 400, 14138, 337, 257, 17, 11, 293, 14138, 439, 264, 636, 493, 281, 257, 3279, 13, 51480, 51480, 400, 370, 341, 2709, 291, 428, 20561, 295, 264, 2931, 295, 288, 885, 2681, 281, 502, 11, 568, 11, 293, 370, 322, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.08321450650691986, "compression_ratio": 1.875776397515528, "no_speech_prob": 3.647664198069833e-05}, {"id": 26, "seek": 10720, "start": 129.52, "end": 135.44, "text": " And so this gives you your estimates of the chance of y being equal to 1, 2, and so on", "tokens": [50364, 1396, 257, 16, 307, 2681, 281, 308, 281, 264, 710, 16, 6666, 538, 308, 281, 264, 710, 16, 1804, 5893, 5893, 5893, 1804, 493, 281, 308, 50816, 50816, 281, 264, 710, 3279, 13, 50894, 50894, 400, 300, 311, 527, 12539, 295, 264, 2931, 300, 288, 307, 2681, 281, 502, 13, 51142, 51142, 400, 14138, 337, 257, 17, 11, 293, 14138, 439, 264, 636, 493, 281, 257, 3279, 13, 51480, 51480, 400, 370, 341, 2709, 291, 428, 20561, 295, 264, 2931, 295, 288, 885, 2681, 281, 502, 11, 568, 11, 293, 370, 322, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.08321450650691986, "compression_ratio": 1.875776397515528, "no_speech_prob": 3.647664198069833e-05}, {"id": 27, "seek": 13544, "start": 135.44, "end": 139.84, "text": " up through the 10th possible label for y.", "tokens": [50364, 493, 807, 264, 1266, 392, 1944, 7645, 337, 288, 13, 50584, 50584, 400, 445, 337, 1557, 15264, 11, 498, 291, 528, 281, 13330, 300, 613, 366, 264, 22927, 6615, 50878, 50878, 365, 4583, 805, 11, 12120, 286, 820, 909, 613, 37906, 5944, 805, 82, 456, 13, 51157, 51157, 467, 775, 652, 264, 24657, 257, 707, 857, 544, 40614, 292, 11, 457, 341, 1669, 13691, 300, 341, 51364, 51364, 307, 11, 337, 1365, 11, 264, 710, 12967, 2158, 293, 341, 307, 264, 9834, 6615, 365, 264, 700, 4985, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.13142845656845595, "compression_ratio": 1.6085106382978724, "no_speech_prob": 1.0289200872648507e-05}, {"id": 28, "seek": 13544, "start": 139.84, "end": 145.72, "text": " And just for completeness, if you want to indicate that these are the quantities associated", "tokens": [50364, 493, 807, 264, 1266, 392, 1944, 7645, 337, 288, 13, 50584, 50584, 400, 445, 337, 1557, 15264, 11, 498, 291, 528, 281, 13330, 300, 613, 366, 264, 22927, 6615, 50878, 50878, 365, 4583, 805, 11, 12120, 286, 820, 909, 613, 37906, 5944, 805, 82, 456, 13, 51157, 51157, 467, 775, 652, 264, 24657, 257, 707, 857, 544, 40614, 292, 11, 457, 341, 1669, 13691, 300, 341, 51364, 51364, 307, 11, 337, 1365, 11, 264, 710, 12967, 2158, 293, 341, 307, 264, 9834, 6615, 365, 264, 700, 4985, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.13142845656845595, "compression_ratio": 1.6085106382978724, "no_speech_prob": 1.0289200872648507e-05}, {"id": 29, "seek": 13544, "start": 145.72, "end": 151.3, "text": " with layer 3, technically I should add these superscript 3s there.", "tokens": [50364, 493, 807, 264, 1266, 392, 1944, 7645, 337, 288, 13, 50584, 50584, 400, 445, 337, 1557, 15264, 11, 498, 291, 528, 281, 13330, 300, 613, 366, 264, 22927, 6615, 50878, 50878, 365, 4583, 805, 11, 12120, 286, 820, 909, 613, 37906, 5944, 805, 82, 456, 13, 51157, 51157, 467, 775, 652, 264, 24657, 257, 707, 857, 544, 40614, 292, 11, 457, 341, 1669, 13691, 300, 341, 51364, 51364, 307, 11, 337, 1365, 11, 264, 710, 12967, 2158, 293, 341, 307, 264, 9834, 6615, 365, 264, 700, 4985, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.13142845656845595, "compression_ratio": 1.6085106382978724, "no_speech_prob": 1.0289200872648507e-05}, {"id": 30, "seek": 13544, "start": 151.3, "end": 155.44, "text": " It does make the notation a little bit more cluttered, but this makes explicit that this", "tokens": [50364, 493, 807, 264, 1266, 392, 1944, 7645, 337, 288, 13, 50584, 50584, 400, 445, 337, 1557, 15264, 11, 498, 291, 528, 281, 13330, 300, 613, 366, 264, 22927, 6615, 50878, 50878, 365, 4583, 805, 11, 12120, 286, 820, 909, 613, 37906, 5944, 805, 82, 456, 13, 51157, 51157, 467, 775, 652, 264, 24657, 257, 707, 857, 544, 40614, 292, 11, 457, 341, 1669, 13691, 300, 341, 51364, 51364, 307, 11, 337, 1365, 11, 264, 710, 12967, 2158, 293, 341, 307, 264, 9834, 6615, 365, 264, 700, 4985, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.13142845656845595, "compression_ratio": 1.6085106382978724, "no_speech_prob": 1.0289200872648507e-05}, {"id": 31, "seek": 13544, "start": 155.44, "end": 165.07999999999998, "text": " is, for example, the z31 value and this is the parameters associated with the first unit", "tokens": [50364, 493, 807, 264, 1266, 392, 1944, 7645, 337, 288, 13, 50584, 50584, 400, 445, 337, 1557, 15264, 11, 498, 291, 528, 281, 13330, 300, 613, 366, 264, 22927, 6615, 50878, 50878, 365, 4583, 805, 11, 12120, 286, 820, 909, 613, 37906, 5944, 805, 82, 456, 13, 51157, 51157, 467, 775, 652, 264, 24657, 257, 707, 857, 544, 40614, 292, 11, 457, 341, 1669, 13691, 300, 341, 51364, 51364, 307, 11, 337, 1365, 11, 264, 710, 12967, 2158, 293, 341, 307, 264, 9834, 6615, 365, 264, 700, 4985, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.13142845656845595, "compression_ratio": 1.6085106382978724, "no_speech_prob": 1.0289200872648507e-05}, {"id": 32, "seek": 16508, "start": 165.08, "end": 169.16000000000003, "text": " of layer 3 of this neural network.", "tokens": [50364, 295, 4583, 805, 295, 341, 18161, 3209, 13, 50568, 50568, 400, 365, 341, 11, 428, 2787, 41167, 5598, 4583, 586, 2709, 291, 20561, 295, 264, 2931, 295, 288, 885, 50932, 50932, 604, 295, 613, 1266, 1944, 5598, 16949, 13, 51115, 51115, 286, 360, 528, 281, 2152, 300, 264, 2787, 41167, 4583, 11, 420, 2171, 611, 1219, 264, 2787, 41167, 24433, 51362, 51362, 2445, 11, 309, 307, 257, 707, 857, 10901, 294, 472, 3104, 5347, 281, 264, 661, 24433, 6828, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.10280563170651355, "compression_ratio": 1.6761904761904762, "no_speech_prob": 8.530128980055451e-06}, {"id": 33, "seek": 16508, "start": 169.16000000000003, "end": 176.44, "text": " And with this, your softmax output layer now gives you estimates of the chance of y being", "tokens": [50364, 295, 4583, 805, 295, 341, 18161, 3209, 13, 50568, 50568, 400, 365, 341, 11, 428, 2787, 41167, 5598, 4583, 586, 2709, 291, 20561, 295, 264, 2931, 295, 288, 885, 50932, 50932, 604, 295, 613, 1266, 1944, 5598, 16949, 13, 51115, 51115, 286, 360, 528, 281, 2152, 300, 264, 2787, 41167, 4583, 11, 420, 2171, 611, 1219, 264, 2787, 41167, 24433, 51362, 51362, 2445, 11, 309, 307, 257, 707, 857, 10901, 294, 472, 3104, 5347, 281, 264, 661, 24433, 6828, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.10280563170651355, "compression_ratio": 1.6761904761904762, "no_speech_prob": 8.530128980055451e-06}, {"id": 34, "seek": 16508, "start": 176.44, "end": 180.10000000000002, "text": " any of these 10 possible output labels.", "tokens": [50364, 295, 4583, 805, 295, 341, 18161, 3209, 13, 50568, 50568, 400, 365, 341, 11, 428, 2787, 41167, 5598, 4583, 586, 2709, 291, 20561, 295, 264, 2931, 295, 288, 885, 50932, 50932, 604, 295, 613, 1266, 1944, 5598, 16949, 13, 51115, 51115, 286, 360, 528, 281, 2152, 300, 264, 2787, 41167, 4583, 11, 420, 2171, 611, 1219, 264, 2787, 41167, 24433, 51362, 51362, 2445, 11, 309, 307, 257, 707, 857, 10901, 294, 472, 3104, 5347, 281, 264, 661, 24433, 6828, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.10280563170651355, "compression_ratio": 1.6761904761904762, "no_speech_prob": 8.530128980055451e-06}, {"id": 35, "seek": 16508, "start": 180.10000000000002, "end": 185.04000000000002, "text": " I do want to mention that the softmax layer, or sometimes also called the softmax activation", "tokens": [50364, 295, 4583, 805, 295, 341, 18161, 3209, 13, 50568, 50568, 400, 365, 341, 11, 428, 2787, 41167, 5598, 4583, 586, 2709, 291, 20561, 295, 264, 2931, 295, 288, 885, 50932, 50932, 604, 295, 613, 1266, 1944, 5598, 16949, 13, 51115, 51115, 286, 360, 528, 281, 2152, 300, 264, 2787, 41167, 4583, 11, 420, 2171, 611, 1219, 264, 2787, 41167, 24433, 51362, 51362, 2445, 11, 309, 307, 257, 707, 857, 10901, 294, 472, 3104, 5347, 281, 264, 661, 24433, 6828, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.10280563170651355, "compression_ratio": 1.6761904761904762, "no_speech_prob": 8.530128980055451e-06}, {"id": 36, "seek": 16508, "start": 185.04000000000002, "end": 191.0, "text": " function, it is a little bit unusual in one respect compared to the other activation functions", "tokens": [50364, 295, 4583, 805, 295, 341, 18161, 3209, 13, 50568, 50568, 400, 365, 341, 11, 428, 2787, 41167, 5598, 4583, 586, 2709, 291, 20561, 295, 264, 2931, 295, 288, 885, 50932, 50932, 604, 295, 613, 1266, 1944, 5598, 16949, 13, 51115, 51115, 286, 360, 528, 281, 2152, 300, 264, 2787, 41167, 4583, 11, 420, 2171, 611, 1219, 264, 2787, 41167, 24433, 51362, 51362, 2445, 11, 309, 307, 257, 707, 857, 10901, 294, 472, 3104, 5347, 281, 264, 661, 24433, 6828, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.10280563170651355, "compression_ratio": 1.6761904761904762, "no_speech_prob": 8.530128980055451e-06}, {"id": 37, "seek": 19100, "start": 191.0, "end": 196.68, "text": " we've seen so far like sigmoid, ReLU, and linear, which is that when we're looking at sigmoid", "tokens": [50364, 321, 600, 1612, 370, 1400, 411, 4556, 3280, 327, 11, 1300, 43, 52, 11, 293, 8213, 11, 597, 307, 300, 562, 321, 434, 1237, 412, 4556, 3280, 327, 50648, 50648, 420, 1300, 43, 52, 420, 8213, 24433, 6828, 11, 257, 16, 390, 257, 2445, 295, 710, 16, 293, 257, 17, 390, 257, 2445, 51134, 51134, 295, 710, 17, 293, 787, 710, 17, 13, 51316, 51316, 682, 661, 2283, 11, 281, 12701, 264, 24433, 4190, 11, 321, 727, 3079, 264, 24433, 2445, 290, 11, 51588, 51588, 312, 309, 4556, 3280, 327, 420, 1300, 43, 52, 420, 746, 1646, 11, 4478, 12, 3711, 281, 710, 16, 293, 710, 17, 293, 370, 322, 281, 483, 604, 295, 264, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13104419547970556, "compression_ratio": 1.7695852534562213, "no_speech_prob": 5.955076176178409e-06}, {"id": 38, "seek": 19100, "start": 196.68, "end": 206.4, "text": " or ReLU or linear activation functions, a1 was a function of z1 and a2 was a function", "tokens": [50364, 321, 600, 1612, 370, 1400, 411, 4556, 3280, 327, 11, 1300, 43, 52, 11, 293, 8213, 11, 597, 307, 300, 562, 321, 434, 1237, 412, 4556, 3280, 327, 50648, 50648, 420, 1300, 43, 52, 420, 8213, 24433, 6828, 11, 257, 16, 390, 257, 2445, 295, 710, 16, 293, 257, 17, 390, 257, 2445, 51134, 51134, 295, 710, 17, 293, 787, 710, 17, 13, 51316, 51316, 682, 661, 2283, 11, 281, 12701, 264, 24433, 4190, 11, 321, 727, 3079, 264, 24433, 2445, 290, 11, 51588, 51588, 312, 309, 4556, 3280, 327, 420, 1300, 43, 52, 420, 746, 1646, 11, 4478, 12, 3711, 281, 710, 16, 293, 710, 17, 293, 370, 322, 281, 483, 604, 295, 264, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13104419547970556, "compression_ratio": 1.7695852534562213, "no_speech_prob": 5.955076176178409e-06}, {"id": 39, "seek": 19100, "start": 206.4, "end": 210.04, "text": " of z2 and only z2.", "tokens": [50364, 321, 600, 1612, 370, 1400, 411, 4556, 3280, 327, 11, 1300, 43, 52, 11, 293, 8213, 11, 597, 307, 300, 562, 321, 434, 1237, 412, 4556, 3280, 327, 50648, 50648, 420, 1300, 43, 52, 420, 8213, 24433, 6828, 11, 257, 16, 390, 257, 2445, 295, 710, 16, 293, 257, 17, 390, 257, 2445, 51134, 51134, 295, 710, 17, 293, 787, 710, 17, 13, 51316, 51316, 682, 661, 2283, 11, 281, 12701, 264, 24433, 4190, 11, 321, 727, 3079, 264, 24433, 2445, 290, 11, 51588, 51588, 312, 309, 4556, 3280, 327, 420, 1300, 43, 52, 420, 746, 1646, 11, 4478, 12, 3711, 281, 710, 16, 293, 710, 17, 293, 370, 322, 281, 483, 604, 295, 264, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13104419547970556, "compression_ratio": 1.7695852534562213, "no_speech_prob": 5.955076176178409e-06}, {"id": 40, "seek": 19100, "start": 210.04, "end": 215.48, "text": " In other words, to obtain the activation values, we could apply the activation function g,", "tokens": [50364, 321, 600, 1612, 370, 1400, 411, 4556, 3280, 327, 11, 1300, 43, 52, 11, 293, 8213, 11, 597, 307, 300, 562, 321, 434, 1237, 412, 4556, 3280, 327, 50648, 50648, 420, 1300, 43, 52, 420, 8213, 24433, 6828, 11, 257, 16, 390, 257, 2445, 295, 710, 16, 293, 257, 17, 390, 257, 2445, 51134, 51134, 295, 710, 17, 293, 787, 710, 17, 13, 51316, 51316, 682, 661, 2283, 11, 281, 12701, 264, 24433, 4190, 11, 321, 727, 3079, 264, 24433, 2445, 290, 11, 51588, 51588, 312, 309, 4556, 3280, 327, 420, 1300, 43, 52, 420, 746, 1646, 11, 4478, 12, 3711, 281, 710, 16, 293, 710, 17, 293, 370, 322, 281, 483, 604, 295, 264, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13104419547970556, "compression_ratio": 1.7695852534562213, "no_speech_prob": 5.955076176178409e-06}, {"id": 41, "seek": 19100, "start": 215.48, "end": 220.96, "text": " be it sigmoid or ReLU or something else, element-wise to z1 and z2 and so on to get any of the", "tokens": [50364, 321, 600, 1612, 370, 1400, 411, 4556, 3280, 327, 11, 1300, 43, 52, 11, 293, 8213, 11, 597, 307, 300, 562, 321, 434, 1237, 412, 4556, 3280, 327, 50648, 50648, 420, 1300, 43, 52, 420, 8213, 24433, 6828, 11, 257, 16, 390, 257, 2445, 295, 710, 16, 293, 257, 17, 390, 257, 2445, 51134, 51134, 295, 710, 17, 293, 787, 710, 17, 13, 51316, 51316, 682, 661, 2283, 11, 281, 12701, 264, 24433, 4190, 11, 321, 727, 3079, 264, 24433, 2445, 290, 11, 51588, 51588, 312, 309, 4556, 3280, 327, 420, 1300, 43, 52, 420, 746, 1646, 11, 4478, 12, 3711, 281, 710, 16, 293, 710, 17, 293, 370, 322, 281, 483, 604, 295, 264, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.13104419547970556, "compression_ratio": 1.7695852534562213, "no_speech_prob": 5.955076176178409e-06}, {"id": 42, "seek": 22096, "start": 220.96, "end": 224.88, "text": " a1 and a2 and a3 and a4.", "tokens": [50364, 257, 16, 293, 257, 17, 293, 257, 18, 293, 257, 19, 13, 50560, 50560, 583, 365, 264, 2787, 41167, 24433, 2445, 11, 3449, 300, 257, 16, 307, 257, 2445, 295, 710, 16, 293, 710, 17, 50960, 50960, 293, 710, 18, 439, 264, 636, 493, 281, 710, 3279, 13, 51094, 51094, 407, 1184, 295, 613, 24433, 4190, 5946, 322, 439, 295, 264, 4190, 295, 710, 13, 51364, 51364, 400, 341, 307, 257, 4707, 300, 311, 257, 857, 3845, 281, 264, 2787, 41167, 5598, 420, 264, 2787, 41167, 24433, 51628, 51628, 2445, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.1283953716880397, "compression_ratio": 1.7457627118644068, "no_speech_prob": 2.586576738394797e-05}, {"id": 43, "seek": 22096, "start": 224.88, "end": 232.88, "text": " But with the softmax activation function, notice that a1 is a function of z1 and z2", "tokens": [50364, 257, 16, 293, 257, 17, 293, 257, 18, 293, 257, 19, 13, 50560, 50560, 583, 365, 264, 2787, 41167, 24433, 2445, 11, 3449, 300, 257, 16, 307, 257, 2445, 295, 710, 16, 293, 710, 17, 50960, 50960, 293, 710, 18, 439, 264, 636, 493, 281, 710, 3279, 13, 51094, 51094, 407, 1184, 295, 613, 24433, 4190, 5946, 322, 439, 295, 264, 4190, 295, 710, 13, 51364, 51364, 400, 341, 307, 257, 4707, 300, 311, 257, 857, 3845, 281, 264, 2787, 41167, 5598, 420, 264, 2787, 41167, 24433, 51628, 51628, 2445, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.1283953716880397, "compression_ratio": 1.7457627118644068, "no_speech_prob": 2.586576738394797e-05}, {"id": 44, "seek": 22096, "start": 232.88, "end": 235.56, "text": " and z3 all the way up to z10.", "tokens": [50364, 257, 16, 293, 257, 17, 293, 257, 18, 293, 257, 19, 13, 50560, 50560, 583, 365, 264, 2787, 41167, 24433, 2445, 11, 3449, 300, 257, 16, 307, 257, 2445, 295, 710, 16, 293, 710, 17, 50960, 50960, 293, 710, 18, 439, 264, 636, 493, 281, 710, 3279, 13, 51094, 51094, 407, 1184, 295, 613, 24433, 4190, 5946, 322, 439, 295, 264, 4190, 295, 710, 13, 51364, 51364, 400, 341, 307, 257, 4707, 300, 311, 257, 857, 3845, 281, 264, 2787, 41167, 5598, 420, 264, 2787, 41167, 24433, 51628, 51628, 2445, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.1283953716880397, "compression_ratio": 1.7457627118644068, "no_speech_prob": 2.586576738394797e-05}, {"id": 45, "seek": 22096, "start": 235.56, "end": 240.96, "text": " So each of these activation values depends on all of the values of z.", "tokens": [50364, 257, 16, 293, 257, 17, 293, 257, 18, 293, 257, 19, 13, 50560, 50560, 583, 365, 264, 2787, 41167, 24433, 2445, 11, 3449, 300, 257, 16, 307, 257, 2445, 295, 710, 16, 293, 710, 17, 50960, 50960, 293, 710, 18, 439, 264, 636, 493, 281, 710, 3279, 13, 51094, 51094, 407, 1184, 295, 613, 24433, 4190, 5946, 322, 439, 295, 264, 4190, 295, 710, 13, 51364, 51364, 400, 341, 307, 257, 4707, 300, 311, 257, 857, 3845, 281, 264, 2787, 41167, 5598, 420, 264, 2787, 41167, 24433, 51628, 51628, 2445, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.1283953716880397, "compression_ratio": 1.7457627118644068, "no_speech_prob": 2.586576738394797e-05}, {"id": 46, "seek": 22096, "start": 240.96, "end": 246.24, "text": " And this is a property that's a bit unique to the softmax output or the softmax activation", "tokens": [50364, 257, 16, 293, 257, 17, 293, 257, 18, 293, 257, 19, 13, 50560, 50560, 583, 365, 264, 2787, 41167, 24433, 2445, 11, 3449, 300, 257, 16, 307, 257, 2445, 295, 710, 16, 293, 710, 17, 50960, 50960, 293, 710, 18, 439, 264, 636, 493, 281, 710, 3279, 13, 51094, 51094, 407, 1184, 295, 613, 24433, 4190, 5946, 322, 439, 295, 264, 4190, 295, 710, 13, 51364, 51364, 400, 341, 307, 257, 4707, 300, 311, 257, 857, 3845, 281, 264, 2787, 41167, 5598, 420, 264, 2787, 41167, 24433, 51628, 51628, 2445, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.1283953716880397, "compression_ratio": 1.7457627118644068, "no_speech_prob": 2.586576738394797e-05}, {"id": 47, "seek": 22096, "start": 246.24, "end": 247.24, "text": " function.", "tokens": [50364, 257, 16, 293, 257, 17, 293, 257, 18, 293, 257, 19, 13, 50560, 50560, 583, 365, 264, 2787, 41167, 24433, 2445, 11, 3449, 300, 257, 16, 307, 257, 2445, 295, 710, 16, 293, 710, 17, 50960, 50960, 293, 710, 18, 439, 264, 636, 493, 281, 710, 3279, 13, 51094, 51094, 407, 1184, 295, 613, 24433, 4190, 5946, 322, 439, 295, 264, 4190, 295, 710, 13, 51364, 51364, 400, 341, 307, 257, 4707, 300, 311, 257, 857, 3845, 281, 264, 2787, 41167, 5598, 420, 264, 2787, 41167, 24433, 51628, 51628, 2445, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.1283953716880397, "compression_ratio": 1.7457627118644068, "no_speech_prob": 2.586576738394797e-05}, {"id": 48, "seek": 24724, "start": 247.24, "end": 254.68, "text": " I'll state it differently, if you want to compute a1 through a10, that is a function", "tokens": [50364, 286, 603, 1785, 309, 7614, 11, 498, 291, 528, 281, 14722, 257, 16, 807, 257, 3279, 11, 300, 307, 257, 2445, 50736, 50736, 295, 710, 16, 439, 264, 636, 493, 281, 710, 3279, 16561, 13, 51064, 51064, 400, 341, 307, 8343, 264, 661, 24433, 6828, 321, 600, 1612, 370, 1400, 13, 51238, 51238, 6288, 11, 718, 311, 574, 412, 577, 291, 576, 4445, 341, 294, 37624, 13, 51518, 51518, 759, 291, 528, 281, 4445, 264, 18161, 3209, 300, 286, 600, 4898, 510, 322, 341, 4137, 11, 341, 307, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.11137307208517323, "compression_ratio": 1.5784753363228698, "no_speech_prob": 6.64330445943051e-06}, {"id": 49, "seek": 24724, "start": 254.68, "end": 261.24, "text": " of z1 all the way up to z10 simultaneously.", "tokens": [50364, 286, 603, 1785, 309, 7614, 11, 498, 291, 528, 281, 14722, 257, 16, 807, 257, 3279, 11, 300, 307, 257, 2445, 50736, 50736, 295, 710, 16, 439, 264, 636, 493, 281, 710, 3279, 16561, 13, 51064, 51064, 400, 341, 307, 8343, 264, 661, 24433, 6828, 321, 600, 1612, 370, 1400, 13, 51238, 51238, 6288, 11, 718, 311, 574, 412, 577, 291, 576, 4445, 341, 294, 37624, 13, 51518, 51518, 759, 291, 528, 281, 4445, 264, 18161, 3209, 300, 286, 600, 4898, 510, 322, 341, 4137, 11, 341, 307, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.11137307208517323, "compression_ratio": 1.5784753363228698, "no_speech_prob": 6.64330445943051e-06}, {"id": 50, "seek": 24724, "start": 261.24, "end": 264.72, "text": " And this is unlike the other activation functions we've seen so far.", "tokens": [50364, 286, 603, 1785, 309, 7614, 11, 498, 291, 528, 281, 14722, 257, 16, 807, 257, 3279, 11, 300, 307, 257, 2445, 50736, 50736, 295, 710, 16, 439, 264, 636, 493, 281, 710, 3279, 16561, 13, 51064, 51064, 400, 341, 307, 8343, 264, 661, 24433, 6828, 321, 600, 1612, 370, 1400, 13, 51238, 51238, 6288, 11, 718, 311, 574, 412, 577, 291, 576, 4445, 341, 294, 37624, 13, 51518, 51518, 759, 291, 528, 281, 4445, 264, 18161, 3209, 300, 286, 600, 4898, 510, 322, 341, 4137, 11, 341, 307, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.11137307208517323, "compression_ratio": 1.5784753363228698, "no_speech_prob": 6.64330445943051e-06}, {"id": 51, "seek": 24724, "start": 264.72, "end": 270.32, "text": " Finally, let's look at how you would implement this in TensorFlow.", "tokens": [50364, 286, 603, 1785, 309, 7614, 11, 498, 291, 528, 281, 14722, 257, 16, 807, 257, 3279, 11, 300, 307, 257, 2445, 50736, 50736, 295, 710, 16, 439, 264, 636, 493, 281, 710, 3279, 16561, 13, 51064, 51064, 400, 341, 307, 8343, 264, 661, 24433, 6828, 321, 600, 1612, 370, 1400, 13, 51238, 51238, 6288, 11, 718, 311, 574, 412, 577, 291, 576, 4445, 341, 294, 37624, 13, 51518, 51518, 759, 291, 528, 281, 4445, 264, 18161, 3209, 300, 286, 600, 4898, 510, 322, 341, 4137, 11, 341, 307, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.11137307208517323, "compression_ratio": 1.5784753363228698, "no_speech_prob": 6.64330445943051e-06}, {"id": 52, "seek": 24724, "start": 270.32, "end": 276.16, "text": " If you want to implement the neural network that I've shown here on this slide, this is", "tokens": [50364, 286, 603, 1785, 309, 7614, 11, 498, 291, 528, 281, 14722, 257, 16, 807, 257, 3279, 11, 300, 307, 257, 2445, 50736, 50736, 295, 710, 16, 439, 264, 636, 493, 281, 710, 3279, 16561, 13, 51064, 51064, 400, 341, 307, 8343, 264, 661, 24433, 6828, 321, 600, 1612, 370, 1400, 13, 51238, 51238, 6288, 11, 718, 311, 574, 412, 577, 291, 576, 4445, 341, 294, 37624, 13, 51518, 51518, 759, 291, 528, 281, 4445, 264, 18161, 3209, 300, 286, 600, 4898, 510, 322, 341, 4137, 11, 341, 307, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.11137307208517323, "compression_ratio": 1.5784753363228698, "no_speech_prob": 6.64330445943051e-06}, {"id": 53, "seek": 27616, "start": 276.16, "end": 278.32000000000005, "text": " the code to do so.", "tokens": [50364, 264, 3089, 281, 360, 370, 13, 50472, 50472, 10905, 382, 949, 11, 456, 366, 1045, 4439, 281, 1608, 5489, 293, 3097, 264, 2316, 13, 50666, 50666, 440, 700, 1823, 307, 281, 980, 37624, 281, 5123, 3137, 6798, 1214, 1045, 7914, 13, 50966, 50966, 2386, 4583, 307, 341, 3552, 6815, 365, 1300, 43, 52, 24433, 2445, 11, 1150, 4583, 11, 2119, 6815, 365, 1300, 43, 52, 51280, 51280, 24433, 2445, 11, 293, 550, 264, 2636, 4583, 11, 570, 456, 366, 586, 1266, 5598, 6815, 11, 291, 51534, 51534, 528, 281, 5598, 257, 16, 807, 257, 3279, 11, 370, 456, 366, 586, 1266, 5598, 6815, 11, 293, 321, 603, 980, 37624, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1481359701241012, "compression_ratio": 1.868312757201646, "no_speech_prob": 1.3630803550768178e-05}, {"id": 54, "seek": 27616, "start": 278.32000000000005, "end": 282.20000000000005, "text": " Similar as before, there are three steps to specifying and training the model.", "tokens": [50364, 264, 3089, 281, 360, 370, 13, 50472, 50472, 10905, 382, 949, 11, 456, 366, 1045, 4439, 281, 1608, 5489, 293, 3097, 264, 2316, 13, 50666, 50666, 440, 700, 1823, 307, 281, 980, 37624, 281, 5123, 3137, 6798, 1214, 1045, 7914, 13, 50966, 50966, 2386, 4583, 307, 341, 3552, 6815, 365, 1300, 43, 52, 24433, 2445, 11, 1150, 4583, 11, 2119, 6815, 365, 1300, 43, 52, 51280, 51280, 24433, 2445, 11, 293, 550, 264, 2636, 4583, 11, 570, 456, 366, 586, 1266, 5598, 6815, 11, 291, 51534, 51534, 528, 281, 5598, 257, 16, 807, 257, 3279, 11, 370, 456, 366, 586, 1266, 5598, 6815, 11, 293, 321, 603, 980, 37624, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1481359701241012, "compression_ratio": 1.868312757201646, "no_speech_prob": 1.3630803550768178e-05}, {"id": 55, "seek": 27616, "start": 282.20000000000005, "end": 288.20000000000005, "text": " The first step is to tell TensorFlow to sequentially string together three layers.", "tokens": [50364, 264, 3089, 281, 360, 370, 13, 50472, 50472, 10905, 382, 949, 11, 456, 366, 1045, 4439, 281, 1608, 5489, 293, 3097, 264, 2316, 13, 50666, 50666, 440, 700, 1823, 307, 281, 980, 37624, 281, 5123, 3137, 6798, 1214, 1045, 7914, 13, 50966, 50966, 2386, 4583, 307, 341, 3552, 6815, 365, 1300, 43, 52, 24433, 2445, 11, 1150, 4583, 11, 2119, 6815, 365, 1300, 43, 52, 51280, 51280, 24433, 2445, 11, 293, 550, 264, 2636, 4583, 11, 570, 456, 366, 586, 1266, 5598, 6815, 11, 291, 51534, 51534, 528, 281, 5598, 257, 16, 807, 257, 3279, 11, 370, 456, 366, 586, 1266, 5598, 6815, 11, 293, 321, 603, 980, 37624, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1481359701241012, "compression_ratio": 1.868312757201646, "no_speech_prob": 1.3630803550768178e-05}, {"id": 56, "seek": 27616, "start": 288.20000000000005, "end": 294.48, "text": " First layer is this 25 units with ReLU activation function, second layer, 15 units with ReLU", "tokens": [50364, 264, 3089, 281, 360, 370, 13, 50472, 50472, 10905, 382, 949, 11, 456, 366, 1045, 4439, 281, 1608, 5489, 293, 3097, 264, 2316, 13, 50666, 50666, 440, 700, 1823, 307, 281, 980, 37624, 281, 5123, 3137, 6798, 1214, 1045, 7914, 13, 50966, 50966, 2386, 4583, 307, 341, 3552, 6815, 365, 1300, 43, 52, 24433, 2445, 11, 1150, 4583, 11, 2119, 6815, 365, 1300, 43, 52, 51280, 51280, 24433, 2445, 11, 293, 550, 264, 2636, 4583, 11, 570, 456, 366, 586, 1266, 5598, 6815, 11, 291, 51534, 51534, 528, 281, 5598, 257, 16, 807, 257, 3279, 11, 370, 456, 366, 586, 1266, 5598, 6815, 11, 293, 321, 603, 980, 37624, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1481359701241012, "compression_ratio": 1.868312757201646, "no_speech_prob": 1.3630803550768178e-05}, {"id": 57, "seek": 27616, "start": 294.48, "end": 299.56, "text": " activation function, and then the third layer, because there are now 10 output units, you", "tokens": [50364, 264, 3089, 281, 360, 370, 13, 50472, 50472, 10905, 382, 949, 11, 456, 366, 1045, 4439, 281, 1608, 5489, 293, 3097, 264, 2316, 13, 50666, 50666, 440, 700, 1823, 307, 281, 980, 37624, 281, 5123, 3137, 6798, 1214, 1045, 7914, 13, 50966, 50966, 2386, 4583, 307, 341, 3552, 6815, 365, 1300, 43, 52, 24433, 2445, 11, 1150, 4583, 11, 2119, 6815, 365, 1300, 43, 52, 51280, 51280, 24433, 2445, 11, 293, 550, 264, 2636, 4583, 11, 570, 456, 366, 586, 1266, 5598, 6815, 11, 291, 51534, 51534, 528, 281, 5598, 257, 16, 807, 257, 3279, 11, 370, 456, 366, 586, 1266, 5598, 6815, 11, 293, 321, 603, 980, 37624, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1481359701241012, "compression_ratio": 1.868312757201646, "no_speech_prob": 1.3630803550768178e-05}, {"id": 58, "seek": 27616, "start": 299.56, "end": 304.92, "text": " want to output a1 through a10, so there are now 10 output units, and we'll tell TensorFlow", "tokens": [50364, 264, 3089, 281, 360, 370, 13, 50472, 50472, 10905, 382, 949, 11, 456, 366, 1045, 4439, 281, 1608, 5489, 293, 3097, 264, 2316, 13, 50666, 50666, 440, 700, 1823, 307, 281, 980, 37624, 281, 5123, 3137, 6798, 1214, 1045, 7914, 13, 50966, 50966, 2386, 4583, 307, 341, 3552, 6815, 365, 1300, 43, 52, 24433, 2445, 11, 1150, 4583, 11, 2119, 6815, 365, 1300, 43, 52, 51280, 51280, 24433, 2445, 11, 293, 550, 264, 2636, 4583, 11, 570, 456, 366, 586, 1266, 5598, 6815, 11, 291, 51534, 51534, 528, 281, 5598, 257, 16, 807, 257, 3279, 11, 370, 456, 366, 586, 1266, 5598, 6815, 11, 293, 321, 603, 980, 37624, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1481359701241012, "compression_ratio": 1.868312757201646, "no_speech_prob": 1.3630803550768178e-05}, {"id": 59, "seek": 30492, "start": 304.92, "end": 309.2, "text": " to use the softmax activation function.", "tokens": [50364, 281, 764, 264, 2787, 41167, 24433, 2445, 13, 50578, 50578, 400, 264, 2063, 2445, 300, 291, 1866, 294, 264, 1036, 960, 11, 37624, 5498, 300, 264, 637, 11668, 50920, 50920, 19250, 804, 3278, 30867, 2445, 13, 51138, 51138, 407, 286, 458, 341, 1315, 307, 257, 857, 295, 257, 4525, 906, 11, 9735, 337, 3565, 3142, 24590, 11, 321, 632, 264, 51418, 51418, 17434, 3278, 30867, 2445, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1422874692460181, "compression_ratio": 1.5786516853932584, "no_speech_prob": 1.2606669770320877e-05}, {"id": 60, "seek": 30492, "start": 309.2, "end": 316.04, "text": " And the cost function that you saw in the last video, TensorFlow calls that the sparse", "tokens": [50364, 281, 764, 264, 2787, 41167, 24433, 2445, 13, 50578, 50578, 400, 264, 2063, 2445, 300, 291, 1866, 294, 264, 1036, 960, 11, 37624, 5498, 300, 264, 637, 11668, 50920, 50920, 19250, 804, 3278, 30867, 2445, 13, 51138, 51138, 407, 286, 458, 341, 1315, 307, 257, 857, 295, 257, 4525, 906, 11, 9735, 337, 3565, 3142, 24590, 11, 321, 632, 264, 51418, 51418, 17434, 3278, 30867, 2445, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1422874692460181, "compression_ratio": 1.5786516853932584, "no_speech_prob": 1.2606669770320877e-05}, {"id": 61, "seek": 30492, "start": 316.04, "end": 320.40000000000003, "text": " categorical cross entropy function.", "tokens": [50364, 281, 764, 264, 2787, 41167, 24433, 2445, 13, 50578, 50578, 400, 264, 2063, 2445, 300, 291, 1866, 294, 264, 1036, 960, 11, 37624, 5498, 300, 264, 637, 11668, 50920, 50920, 19250, 804, 3278, 30867, 2445, 13, 51138, 51138, 407, 286, 458, 341, 1315, 307, 257, 857, 295, 257, 4525, 906, 11, 9735, 337, 3565, 3142, 24590, 11, 321, 632, 264, 51418, 51418, 17434, 3278, 30867, 2445, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1422874692460181, "compression_ratio": 1.5786516853932584, "no_speech_prob": 1.2606669770320877e-05}, {"id": 62, "seek": 30492, "start": 320.40000000000003, "end": 326.0, "text": " So I know this name is a bit of a mouthful, whereas for logistic regression, we had the", "tokens": [50364, 281, 764, 264, 2787, 41167, 24433, 2445, 13, 50578, 50578, 400, 264, 2063, 2445, 300, 291, 1866, 294, 264, 1036, 960, 11, 37624, 5498, 300, 264, 637, 11668, 50920, 50920, 19250, 804, 3278, 30867, 2445, 13, 51138, 51138, 407, 286, 458, 341, 1315, 307, 257, 857, 295, 257, 4525, 906, 11, 9735, 337, 3565, 3142, 24590, 11, 321, 632, 264, 51418, 51418, 17434, 3278, 30867, 2445, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1422874692460181, "compression_ratio": 1.5786516853932584, "no_speech_prob": 1.2606669770320877e-05}, {"id": 63, "seek": 30492, "start": 326.0, "end": 328.92, "text": " binary cross entropy function.", "tokens": [50364, 281, 764, 264, 2787, 41167, 24433, 2445, 13, 50578, 50578, 400, 264, 2063, 2445, 300, 291, 1866, 294, 264, 1036, 960, 11, 37624, 5498, 300, 264, 637, 11668, 50920, 50920, 19250, 804, 3278, 30867, 2445, 13, 51138, 51138, 407, 286, 458, 341, 1315, 307, 257, 857, 295, 257, 4525, 906, 11, 9735, 337, 3565, 3142, 24590, 11, 321, 632, 264, 51418, 51418, 17434, 3278, 30867, 2445, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1422874692460181, "compression_ratio": 1.5786516853932584, "no_speech_prob": 1.2606669770320877e-05}, {"id": 64, "seek": 32892, "start": 328.92, "end": 335.40000000000003, "text": " Here we're using the sparse categorical cross entropy function, and what sparse categorical", "tokens": [50364, 1692, 321, 434, 1228, 264, 637, 11668, 19250, 804, 3278, 30867, 2445, 11, 293, 437, 637, 11668, 19250, 804, 50688, 50688, 14942, 281, 307, 300, 291, 920, 33872, 288, 666, 10479, 11, 370, 309, 311, 19250, 804, 11, 309, 2516, 51028, 51028, 322, 4190, 490, 502, 281, 1266, 11, 293, 637, 11668, 14942, 281, 300, 288, 393, 787, 747, 322, 472, 295, 613, 1266, 51398, 51398, 4190, 11, 370, 1184, 3256, 307, 2139, 1958, 420, 502, 420, 568, 420, 370, 322, 493, 281, 1722, 11, 291, 434, 406, 516, 281, 536, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.12403291401110197, "compression_ratio": 1.697560975609756, "no_speech_prob": 7.646454832865857e-06}, {"id": 65, "seek": 32892, "start": 335.40000000000003, "end": 342.20000000000005, "text": " refers to is that you still classify y into categories, so it's categorical, it takes", "tokens": [50364, 1692, 321, 434, 1228, 264, 637, 11668, 19250, 804, 3278, 30867, 2445, 11, 293, 437, 637, 11668, 19250, 804, 50688, 50688, 14942, 281, 307, 300, 291, 920, 33872, 288, 666, 10479, 11, 370, 309, 311, 19250, 804, 11, 309, 2516, 51028, 51028, 322, 4190, 490, 502, 281, 1266, 11, 293, 637, 11668, 14942, 281, 300, 288, 393, 787, 747, 322, 472, 295, 613, 1266, 51398, 51398, 4190, 11, 370, 1184, 3256, 307, 2139, 1958, 420, 502, 420, 568, 420, 370, 322, 493, 281, 1722, 11, 291, 434, 406, 516, 281, 536, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.12403291401110197, "compression_ratio": 1.697560975609756, "no_speech_prob": 7.646454832865857e-06}, {"id": 66, "seek": 32892, "start": 342.20000000000005, "end": 349.6, "text": " on values from 1 to 10, and sparse refers to that y can only take on one of these 10", "tokens": [50364, 1692, 321, 434, 1228, 264, 637, 11668, 19250, 804, 3278, 30867, 2445, 11, 293, 437, 637, 11668, 19250, 804, 50688, 50688, 14942, 281, 307, 300, 291, 920, 33872, 288, 666, 10479, 11, 370, 309, 311, 19250, 804, 11, 309, 2516, 51028, 51028, 322, 4190, 490, 502, 281, 1266, 11, 293, 637, 11668, 14942, 281, 300, 288, 393, 787, 747, 322, 472, 295, 613, 1266, 51398, 51398, 4190, 11, 370, 1184, 3256, 307, 2139, 1958, 420, 502, 420, 568, 420, 370, 322, 493, 281, 1722, 11, 291, 434, 406, 516, 281, 536, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.12403291401110197, "compression_ratio": 1.697560975609756, "no_speech_prob": 7.646454832865857e-06}, {"id": 67, "seek": 32892, "start": 349.6, "end": 354.76, "text": " values, so each image is either 0 or 1 or 2 or so on up to 9, you're not going to see", "tokens": [50364, 1692, 321, 434, 1228, 264, 637, 11668, 19250, 804, 3278, 30867, 2445, 11, 293, 437, 637, 11668, 19250, 804, 50688, 50688, 14942, 281, 307, 300, 291, 920, 33872, 288, 666, 10479, 11, 370, 309, 311, 19250, 804, 11, 309, 2516, 51028, 51028, 322, 4190, 490, 502, 281, 1266, 11, 293, 637, 11668, 14942, 281, 300, 288, 393, 787, 747, 322, 472, 295, 613, 1266, 51398, 51398, 4190, 11, 370, 1184, 3256, 307, 2139, 1958, 420, 502, 420, 568, 420, 370, 322, 493, 281, 1722, 11, 291, 434, 406, 516, 281, 536, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.12403291401110197, "compression_ratio": 1.697560975609756, "no_speech_prob": 7.646454832865857e-06}, {"id": 68, "seek": 35476, "start": 354.76, "end": 360.28, "text": " a picture that is simultaneously the number 2 and the number 7, so sparse refers to that", "tokens": [50364, 257, 3036, 300, 307, 16561, 264, 1230, 568, 293, 264, 1230, 1614, 11, 370, 637, 11668, 14942, 281, 300, 50640, 50640, 1184, 14293, 307, 787, 472, 295, 613, 10479, 13, 50826, 50826, 407, 300, 311, 983, 264, 4470, 2445, 300, 291, 1866, 294, 264, 1036, 960, 307, 1219, 11, 294, 37624, 11, 51082, 51082, 264, 637, 11668, 19250, 804, 3278, 30867, 4470, 2445, 13, 51316, 51316, 400, 550, 264, 3089, 337, 3097, 264, 2316, 307, 445, 264, 912, 382, 949, 11, 293, 498, 291, 764, 341, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.12466021643744575, "compression_ratio": 1.6759259259259258, "no_speech_prob": 4.710841949417954e-06}, {"id": 69, "seek": 35476, "start": 360.28, "end": 364.0, "text": " each digit is only one of these categories.", "tokens": [50364, 257, 3036, 300, 307, 16561, 264, 1230, 568, 293, 264, 1230, 1614, 11, 370, 637, 11668, 14942, 281, 300, 50640, 50640, 1184, 14293, 307, 787, 472, 295, 613, 10479, 13, 50826, 50826, 407, 300, 311, 983, 264, 4470, 2445, 300, 291, 1866, 294, 264, 1036, 960, 307, 1219, 11, 294, 37624, 11, 51082, 51082, 264, 637, 11668, 19250, 804, 3278, 30867, 4470, 2445, 13, 51316, 51316, 400, 550, 264, 3089, 337, 3097, 264, 2316, 307, 445, 264, 912, 382, 949, 11, 293, 498, 291, 764, 341, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.12466021643744575, "compression_ratio": 1.6759259259259258, "no_speech_prob": 4.710841949417954e-06}, {"id": 70, "seek": 35476, "start": 364.0, "end": 369.12, "text": " So that's why the loss function that you saw in the last video is called, in TensorFlow,", "tokens": [50364, 257, 3036, 300, 307, 16561, 264, 1230, 568, 293, 264, 1230, 1614, 11, 370, 637, 11668, 14942, 281, 300, 50640, 50640, 1184, 14293, 307, 787, 472, 295, 613, 10479, 13, 50826, 50826, 407, 300, 311, 983, 264, 4470, 2445, 300, 291, 1866, 294, 264, 1036, 960, 307, 1219, 11, 294, 37624, 11, 51082, 51082, 264, 637, 11668, 19250, 804, 3278, 30867, 4470, 2445, 13, 51316, 51316, 400, 550, 264, 3089, 337, 3097, 264, 2316, 307, 445, 264, 912, 382, 949, 11, 293, 498, 291, 764, 341, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.12466021643744575, "compression_ratio": 1.6759259259259258, "no_speech_prob": 4.710841949417954e-06}, {"id": 71, "seek": 35476, "start": 369.12, "end": 373.8, "text": " the sparse categorical cross entropy loss function.", "tokens": [50364, 257, 3036, 300, 307, 16561, 264, 1230, 568, 293, 264, 1230, 1614, 11, 370, 637, 11668, 14942, 281, 300, 50640, 50640, 1184, 14293, 307, 787, 472, 295, 613, 10479, 13, 50826, 50826, 407, 300, 311, 983, 264, 4470, 2445, 300, 291, 1866, 294, 264, 1036, 960, 307, 1219, 11, 294, 37624, 11, 51082, 51082, 264, 637, 11668, 19250, 804, 3278, 30867, 4470, 2445, 13, 51316, 51316, 400, 550, 264, 3089, 337, 3097, 264, 2316, 307, 445, 264, 912, 382, 949, 11, 293, 498, 291, 764, 341, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.12466021643744575, "compression_ratio": 1.6759259259259258, "no_speech_prob": 4.710841949417954e-06}, {"id": 72, "seek": 35476, "start": 373.8, "end": 379.24, "text": " And then the code for training the model is just the same as before, and if you use this", "tokens": [50364, 257, 3036, 300, 307, 16561, 264, 1230, 568, 293, 264, 1230, 1614, 11, 370, 637, 11668, 14942, 281, 300, 50640, 50640, 1184, 14293, 307, 787, 472, 295, 613, 10479, 13, 50826, 50826, 407, 300, 311, 983, 264, 4470, 2445, 300, 291, 1866, 294, 264, 1036, 960, 307, 1219, 11, 294, 37624, 11, 51082, 51082, 264, 637, 11668, 19250, 804, 3278, 30867, 4470, 2445, 13, 51316, 51316, 400, 550, 264, 3089, 337, 3097, 264, 2316, 307, 445, 264, 912, 382, 949, 11, 293, 498, 291, 764, 341, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.12466021643744575, "compression_ratio": 1.6759259259259258, "no_speech_prob": 4.710841949417954e-06}, {"id": 73, "seek": 37924, "start": 379.24, "end": 385.48, "text": " code you can train a neural network on a multi-class classification problem.", "tokens": [50364, 3089, 291, 393, 3847, 257, 18161, 3209, 322, 257, 4825, 12, 11665, 21538, 1154, 13, 50676, 50676, 1449, 472, 1021, 3637, 11, 498, 291, 764, 341, 3089, 2293, 382, 286, 600, 3720, 510, 11, 309, 486, 589, 11, 50956, 50956, 457, 500, 380, 767, 764, 341, 3089, 11, 570, 309, 4523, 484, 300, 294, 37624, 456, 311, 257, 51216, 51216, 1101, 3037, 295, 264, 3089, 300, 1669, 37624, 589, 1101, 13, 51460, 51460, 407, 754, 1673, 264, 3089, 4898, 294, 341, 4137, 1985, 11, 500, 380, 764, 341, 3089, 264, 636, 286, 600, 3720, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.0951924129408233, "compression_ratio": 1.7264957264957266, "no_speech_prob": 2.1444211597554386e-05}, {"id": 74, "seek": 37924, "start": 385.48, "end": 391.08, "text": " Just one important note, if you use this code exactly as I've written here, it will work,", "tokens": [50364, 3089, 291, 393, 3847, 257, 18161, 3209, 322, 257, 4825, 12, 11665, 21538, 1154, 13, 50676, 50676, 1449, 472, 1021, 3637, 11, 498, 291, 764, 341, 3089, 2293, 382, 286, 600, 3720, 510, 11, 309, 486, 589, 11, 50956, 50956, 457, 500, 380, 767, 764, 341, 3089, 11, 570, 309, 4523, 484, 300, 294, 37624, 456, 311, 257, 51216, 51216, 1101, 3037, 295, 264, 3089, 300, 1669, 37624, 589, 1101, 13, 51460, 51460, 407, 754, 1673, 264, 3089, 4898, 294, 341, 4137, 1985, 11, 500, 380, 764, 341, 3089, 264, 636, 286, 600, 3720, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.0951924129408233, "compression_ratio": 1.7264957264957266, "no_speech_prob": 2.1444211597554386e-05}, {"id": 75, "seek": 37924, "start": 391.08, "end": 396.28000000000003, "text": " but don't actually use this code, because it turns out that in TensorFlow there's a", "tokens": [50364, 3089, 291, 393, 3847, 257, 18161, 3209, 322, 257, 4825, 12, 11665, 21538, 1154, 13, 50676, 50676, 1449, 472, 1021, 3637, 11, 498, 291, 764, 341, 3089, 2293, 382, 286, 600, 3720, 510, 11, 309, 486, 589, 11, 50956, 50956, 457, 500, 380, 767, 764, 341, 3089, 11, 570, 309, 4523, 484, 300, 294, 37624, 456, 311, 257, 51216, 51216, 1101, 3037, 295, 264, 3089, 300, 1669, 37624, 589, 1101, 13, 51460, 51460, 407, 754, 1673, 264, 3089, 4898, 294, 341, 4137, 1985, 11, 500, 380, 764, 341, 3089, 264, 636, 286, 600, 3720, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.0951924129408233, "compression_ratio": 1.7264957264957266, "no_speech_prob": 2.1444211597554386e-05}, {"id": 76, "seek": 37924, "start": 396.28000000000003, "end": 401.16, "text": " better version of the code that makes TensorFlow work better.", "tokens": [50364, 3089, 291, 393, 3847, 257, 18161, 3209, 322, 257, 4825, 12, 11665, 21538, 1154, 13, 50676, 50676, 1449, 472, 1021, 3637, 11, 498, 291, 764, 341, 3089, 2293, 382, 286, 600, 3720, 510, 11, 309, 486, 589, 11, 50956, 50956, 457, 500, 380, 767, 764, 341, 3089, 11, 570, 309, 4523, 484, 300, 294, 37624, 456, 311, 257, 51216, 51216, 1101, 3037, 295, 264, 3089, 300, 1669, 37624, 589, 1101, 13, 51460, 51460, 407, 754, 1673, 264, 3089, 4898, 294, 341, 4137, 1985, 11, 500, 380, 764, 341, 3089, 264, 636, 286, 600, 3720, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.0951924129408233, "compression_ratio": 1.7264957264957266, "no_speech_prob": 2.1444211597554386e-05}, {"id": 77, "seek": 37924, "start": 401.16, "end": 406.48, "text": " So even though the code shown in this slide works, don't use this code the way I've written", "tokens": [50364, 3089, 291, 393, 3847, 257, 18161, 3209, 322, 257, 4825, 12, 11665, 21538, 1154, 13, 50676, 50676, 1449, 472, 1021, 3637, 11, 498, 291, 764, 341, 3089, 2293, 382, 286, 600, 3720, 510, 11, 309, 486, 589, 11, 50956, 50956, 457, 500, 380, 767, 764, 341, 3089, 11, 570, 309, 4523, 484, 300, 294, 37624, 456, 311, 257, 51216, 51216, 1101, 3037, 295, 264, 3089, 300, 1669, 37624, 589, 1101, 13, 51460, 51460, 407, 754, 1673, 264, 3089, 4898, 294, 341, 4137, 1985, 11, 500, 380, 764, 341, 3089, 264, 636, 286, 600, 3720, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.0951924129408233, "compression_ratio": 1.7264957264957266, "no_speech_prob": 2.1444211597554386e-05}, {"id": 78, "seek": 40648, "start": 406.48, "end": 411.56, "text": " it here, because in a later video this week you'll see a different version, there's actually", "tokens": [50364, 309, 510, 11, 570, 294, 257, 1780, 960, 341, 1243, 291, 603, 536, 257, 819, 3037, 11, 456, 311, 767, 50618, 50618, 264, 9628, 3037, 295, 18114, 341, 300, 486, 589, 1101, 11, 457, 321, 603, 747, 257, 574, 50846, 50846, 412, 300, 294, 257, 1780, 960, 13, 50975, 50975, 407, 586, 291, 458, 577, 281, 3847, 257, 18161, 3209, 365, 257, 2787, 41167, 5598, 4583, 365, 472, 43012, 13, 51328, 51328, 821, 311, 257, 819, 3037, 295, 264, 3089, 300, 486, 652, 37624, 1075, 281, 14722, 613, 51588, 51588, 33783, 709, 544, 20095, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.11830380950311217, "compression_ratio": 1.6987951807228916, "no_speech_prob": 9.8180153145222e-06}, {"id": 79, "seek": 40648, "start": 411.56, "end": 416.12, "text": " the recommended version of implementing this that will work better, but we'll take a look", "tokens": [50364, 309, 510, 11, 570, 294, 257, 1780, 960, 341, 1243, 291, 603, 536, 257, 819, 3037, 11, 456, 311, 767, 50618, 50618, 264, 9628, 3037, 295, 18114, 341, 300, 486, 589, 1101, 11, 457, 321, 603, 747, 257, 574, 50846, 50846, 412, 300, 294, 257, 1780, 960, 13, 50975, 50975, 407, 586, 291, 458, 577, 281, 3847, 257, 18161, 3209, 365, 257, 2787, 41167, 5598, 4583, 365, 472, 43012, 13, 51328, 51328, 821, 311, 257, 819, 3037, 295, 264, 3089, 300, 486, 652, 37624, 1075, 281, 14722, 613, 51588, 51588, 33783, 709, 544, 20095, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.11830380950311217, "compression_ratio": 1.6987951807228916, "no_speech_prob": 9.8180153145222e-06}, {"id": 80, "seek": 40648, "start": 416.12, "end": 418.70000000000005, "text": " at that in a later video.", "tokens": [50364, 309, 510, 11, 570, 294, 257, 1780, 960, 341, 1243, 291, 603, 536, 257, 819, 3037, 11, 456, 311, 767, 50618, 50618, 264, 9628, 3037, 295, 18114, 341, 300, 486, 589, 1101, 11, 457, 321, 603, 747, 257, 574, 50846, 50846, 412, 300, 294, 257, 1780, 960, 13, 50975, 50975, 407, 586, 291, 458, 577, 281, 3847, 257, 18161, 3209, 365, 257, 2787, 41167, 5598, 4583, 365, 472, 43012, 13, 51328, 51328, 821, 311, 257, 819, 3037, 295, 264, 3089, 300, 486, 652, 37624, 1075, 281, 14722, 613, 51588, 51588, 33783, 709, 544, 20095, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.11830380950311217, "compression_ratio": 1.6987951807228916, "no_speech_prob": 9.8180153145222e-06}, {"id": 81, "seek": 40648, "start": 418.70000000000005, "end": 425.76, "text": " So now you know how to train a neural network with a softmax output layer with one caveat.", "tokens": [50364, 309, 510, 11, 570, 294, 257, 1780, 960, 341, 1243, 291, 603, 536, 257, 819, 3037, 11, 456, 311, 767, 50618, 50618, 264, 9628, 3037, 295, 18114, 341, 300, 486, 589, 1101, 11, 457, 321, 603, 747, 257, 574, 50846, 50846, 412, 300, 294, 257, 1780, 960, 13, 50975, 50975, 407, 586, 291, 458, 577, 281, 3847, 257, 18161, 3209, 365, 257, 2787, 41167, 5598, 4583, 365, 472, 43012, 13, 51328, 51328, 821, 311, 257, 819, 3037, 295, 264, 3089, 300, 486, 652, 37624, 1075, 281, 14722, 613, 51588, 51588, 33783, 709, 544, 20095, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.11830380950311217, "compression_ratio": 1.6987951807228916, "no_speech_prob": 9.8180153145222e-06}, {"id": 82, "seek": 40648, "start": 425.76, "end": 430.96000000000004, "text": " There's a different version of the code that will make TensorFlow able to compute these", "tokens": [50364, 309, 510, 11, 570, 294, 257, 1780, 960, 341, 1243, 291, 603, 536, 257, 819, 3037, 11, 456, 311, 767, 50618, 50618, 264, 9628, 3037, 295, 18114, 341, 300, 486, 589, 1101, 11, 457, 321, 603, 747, 257, 574, 50846, 50846, 412, 300, 294, 257, 1780, 960, 13, 50975, 50975, 407, 586, 291, 458, 577, 281, 3847, 257, 18161, 3209, 365, 257, 2787, 41167, 5598, 4583, 365, 472, 43012, 13, 51328, 51328, 821, 311, 257, 819, 3037, 295, 264, 3089, 300, 486, 652, 37624, 1075, 281, 14722, 613, 51588, 51588, 33783, 709, 544, 20095, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.11830380950311217, "compression_ratio": 1.6987951807228916, "no_speech_prob": 9.8180153145222e-06}, {"id": 83, "seek": 40648, "start": 430.96000000000004, "end": 433.84000000000003, "text": " probabilities much more accurately.", "tokens": [50364, 309, 510, 11, 570, 294, 257, 1780, 960, 341, 1243, 291, 603, 536, 257, 819, 3037, 11, 456, 311, 767, 50618, 50618, 264, 9628, 3037, 295, 18114, 341, 300, 486, 589, 1101, 11, 457, 321, 603, 747, 257, 574, 50846, 50846, 412, 300, 294, 257, 1780, 960, 13, 50975, 50975, 407, 586, 291, 458, 577, 281, 3847, 257, 18161, 3209, 365, 257, 2787, 41167, 5598, 4583, 365, 472, 43012, 13, 51328, 51328, 821, 311, 257, 819, 3037, 295, 264, 3089, 300, 486, 652, 37624, 1075, 281, 14722, 613, 51588, 51588, 33783, 709, 544, 20095, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.11830380950311217, "compression_ratio": 1.6987951807228916, "no_speech_prob": 9.8180153145222e-06}, {"id": 84, "seek": 43384, "start": 433.84, "end": 438.32, "text": " Let's take a look at that in the next video, which will also show you the actual code that", "tokens": [50364, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 11, 597, 486, 611, 855, 291, 264, 3539, 3089, 300, 50588, 50588, 286, 2748, 291, 764, 498, 291, 434, 3097, 257, 2787, 41167, 18161, 3209, 13, 50784, 50784, 961, 311, 352, 322, 281, 264, 958, 960, 13, 50856], "temperature": 0.0, "avg_logprob": -0.11186495949240292, "compression_ratio": 1.3984962406015038, "no_speech_prob": 5.55074293515645e-05}, {"id": 85, "seek": 43384, "start": 438.32, "end": 442.23999999999995, "text": " I recommend you use if you're training a softmax neural network.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 11, 597, 486, 611, 855, 291, 264, 3539, 3089, 300, 50588, 50588, 286, 2748, 291, 764, 498, 291, 434, 3097, 257, 2787, 41167, 18161, 3209, 13, 50784, 50784, 961, 311, 352, 322, 281, 264, 958, 960, 13, 50856], "temperature": 0.0, "avg_logprob": -0.11186495949240292, "compression_ratio": 1.3984962406015038, "no_speech_prob": 5.55074293515645e-05}, {"id": 86, "seek": 44224, "start": 442.24, "end": 464.68, "text": " Let's go on to the next video.", "tokens": [50364, 961, 311, 352, 322, 281, 264, 958, 960, 13, 51486], "temperature": 0.0, "avg_logprob": -0.44409183661142987, "compression_ratio": 0.7894736842105263, "no_speech_prob": 0.0004929409478791058}], "language": "en", "video_id": "24QO9iNXvWs", "entity": "ML Specialization, Andrew Ng (2022)"}}