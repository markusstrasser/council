{"video_id": "jzaF4An03Oc", "title": "5.2 Neural Network Training | Training Details --[Machine Learning | Andrew Ng]", "description": "Second Course:\nAdvanced Learning Algorithms.\n\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 821, "views": 127, "publish_date": "11/04/2022", "timestamp": 1661817600, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " Let's take a look at the details of what the TensorFlow code for training a neural network is actually doing. Let's dive in. Before looking at the details of training a neural network, let's recall how you had trained a logistic regression model in the previous course. Step 1 of building a logistic regression model was you would specify how to compute the output given the input v to the x and the parameters w and b. So in the first course we said the logistic regression function predicts f of x is equal to g, the sigmoid function applied to w dot product x plus b, which was the sigmoid function applied to w dot x plus b. So if z is the dot product of w of x plus b, then f of x is 1 over 1 plus e to the negative z. So that was the first step, where to specify what is the input to output function of logistic regression, and that depends on both the input x and the parameters of the model. The second step we had to do to train the logistic regression model was to specify the loss function and also the cost function. So you may recall that the loss function said if logistic regression outputs f of x and the ground truth label, the actual label in the training set was y, then the loss on that single training example was negative y log f of x minus 1 minus y times log of 1 minus f of x. So this was a measure of how well is logistic regression doing on a single training example x, y. Given this definition of a loss function, we then define the cost function, and the cost function was a function of the parameters w and b, and that was just the average that is taking an average over all m training examples of the loss function computed on the m training examples x1, y1, through xm, ym. And remember that in the condition we're using, the loss function is a function of the output of the learning algorithm and the ground truth label as computed over a single training example, whereas the cost function j is an average of the loss function computed over your entire training set. So that was step two of what we did when building up logistic regression. And then the third and final step to train the logistic regression model was to use an algorithm, specifically gradient descent, to minimize that cost function j of w, b to minimize it as a function of the parameters w and b. And we minimize the cost j as a function of the parameters using gradient descent, where w is updated as w minus the learning rate alpha times the derivative of j with respect to w, and b similarly is updated as b minus the learning rate alpha times the derivative of j with respect to b. So with these three steps, step one, specifying how to compute the outputs given the input x in parameters, step two, specifying loss in cost, and step three, minimize the cost function, we trained logistic regression. The same three steps is how we can train a neural network in TensorFlow. Now let's look at how these three steps map to training a neural network. We'll go over this in greater detail on the next three slides, but really briefly, step one of specifying how to compute the output given the input x in parameters w and b, that's done with this code snippet, which should be familiar from last week of specifying the neural network, and this was actually enough to specify the computations needed in forward propagation or for the inference algorithm, for example. The second step is to compile the model and to tell it what loss you want to use. And here's a code that you use to specify this loss function, which is the binary cross NGP loss function. And once you specify this loss, taking an average over the entire training set also gives you the cost function for the neural network. And then step three is to call function to try to minimize the cost as a function of the parameters of the neural network. Let's look in greater detail in these three steps in the context of training a neural network. The first step, specify how to compute the output given the input x in parameters w and b. This code snippet specifies the entire architecture of the neural network. It tells you that there are 25 hidden units in the first hidden layer, then 15 in the next one, and then one output unit, and that we're using the sig one activation value. And so based on this code snippet, we know also what are the parameters, w1, b1 of the first layer, parameters of the second layer, and parameters of the third layer. So this code snippet specifies the entire architecture of the neural network and therefore tells TensorFlow everything it needs in order to compute the output x as a function. In order to compute the output a3 or f of x as a function of the input x and the parameters here we have written wl and bl. Let's go on to step two. In the second step, you have to specify what is the loss function, and that will also define the cost function we use to train the neural network. So for the MNIST 01 digit classification problem is a binary classification problem, and the most common by far loss function to use is this one. It's actually the same loss function as what we had for logistic regression is negative y log f of x minus one minus y times log one minus f of x, where y is the ground truth label, sometimes also called the target label y, and f of x is now the output of the neural network. So in the terminology of TensorFlow, this loss function is called binary cross entropy, and the syntax is to ask TensorFlow to compile the neural network using this loss function. And another historical note, Keras was originally a library that had developed independently of TensorFlow, it was actually a totally separate project from TensorFlow, but eventually it got merged into TensorFlow, which is why we have tf.keraslibrary.losses.the name of this loss function. And by the way, I don't always remember the names of all the loss functions in TensorFlow, but I just do a quick web search myself to find the right name, and then I plug that into my code. Having specified the loss with respect to a single training example, TensorFlow knows that the costs you want to minimize is then the average, taking the average over all m training examples of the loss on all of the training examples. And optimizing this cost function will result in fitting the neural network to your binary classification data. In case you want to solve a regression problem rather than a classification problem, you can also tell TensorFlow to compile your model using a different loss function. For example, if you have a regression problem, and if you want to minimize the squared error loss, so here is the squared error loss, the loss with respect to if you're learning algorithm outputs f of x with a target or ground truth label of y, that's one half of the squared error, then you can use this loss function in TensorFlow, which is to use the maybe more intuitively named mean squared error loss function, and then TensorFlow will try to minimize the mean squared error. In this expression, I'm using J of capital W comma capital B to denote the cost function. The cost function is a function of all of the parameters in the neural network. So you can think of capital W as including W1, W2, W3, so all the W parameters in the entire neural network and B as including B1, B2, and B3. So if you are optimizing the cost function with respect to W and B, you'd be trying to optimize it with respect to all of the parameters in the neural network. And up on top as well, I had written f of x as the output of the neural network, but you want, you can also write f of WB if we want to emphasize that the output of the neural network as a function of x depends on all the parameters in all the layers of the neural network. So that's the loss function and the cost function. And in TensorFlow, this is called the binary cross entropy loss function. Where does that name come from? Well, it turns out in statistics, this function on top is called the cross entropy loss function. So that's what cross entropy means. And the word binary just re-emphasizes or points out that this is a binary classification problem because each image is either a zero or a one. Finally, you will ask TensorFlow to minimize the cost function. You might remember the gradient descent algorithm from the first course. If you are using gradient descent to train the parameters of a neural network, then you will repeatedly for every layer L and for every unit J, update WLJ according to WLJ minus the learning rate alpha times the partial derivative with respect to that parameter of the cost function J of WB. And similarly for the parameters B as well. And after doing, say, 100 iterations of gradient descent, hopefully you get to a good value of the parameters. So in order to use gradient descent, the key thing you need to compute is these partial derivative terms. And what TensorFlow does, and in fact what is standard in neural network training, is to use an algorithm called backpropagation in order to compute these partial derivative terms. TensorFlow can do all of these things for you. It implements backpropagation all within this function called fit. So all you have to do is call model.fit x, y as your training set and tell it to do so for 100 iterations or 100 epochs. In fact, what you see later is that TensorFlow can use an algorithm that is even a little bit faster than gradient descent. And you see more about that later this week as well. Now I know that we're relying heavily on the TensorFlow library in order to implement a neural network. One pattern I've seen across multiple ideas is as the technology evolves, libraries become more mature and most engineers will use libraries rather than implement code from scratch. And there have been many other examples of this in the history of computing. Once many, many decades ago programmers had to implement their own sorting function from scratch. But now sorting libraries are quite mature that you probably call someone else's sorting function rather than implement it yourself unless you're taking a computing classes, ask you to do it as an exercise. And today if you want to compute the square root of a number, like what is the square root of 7? Well once programmers had to write their own code to compute this, but now pretty much everyone just calls a library to take square roots or matrix operations such as multiplying two matrices together. So when deep learning was younger and less mature, many developers, including me, were implementing things from scratch using Python or C++ or some other library. But today deep learning libraries have matured enough that most developers will use these libraries and in fact most commercial implementations of neural networks today use a library like TensorFlow or PyTorch. But as I've mentioned, it's still useful to understand how they work under the hood so that if something unexpected happens, which still does with today's libraries, you have a better chance of knowing how to fix it. Now that you know how to train a basic neural network, also called a multilayer perceptron, there are some things you can change about the neural network that will make it even more powerful. In the next video, let's take a look at how you can swap in different activation functions as an alternative to the sigmoid activation function we've been using. This will make your neural networks work even much better. So let's go take a look at that in the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.96, "text": " Let's take a look at the details of what the TensorFlow code for training a neural network", "tokens": [50364, 961, 311, 747, 257, 574, 412, 264, 4365, 295, 437, 264, 37624, 3089, 337, 3097, 257, 18161, 3209, 50712, 50712, 307, 767, 884, 13, 50762, 50762, 961, 311, 9192, 294, 13, 50846, 50846, 4546, 1237, 412, 264, 4365, 295, 3097, 257, 18161, 3209, 11, 718, 311, 9901, 577, 291, 632, 51093, 51093, 8895, 257, 3565, 3142, 24590, 2316, 294, 264, 3894, 1164, 13, 51352, 51352, 5470, 502, 295, 2390, 257, 3565, 3142, 24590, 2316, 390, 291, 576, 16500, 577, 281, 14722, 264, 5598, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.11879075806716392, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.011681872420012951}, {"id": 1, "seek": 0, "start": 6.96, "end": 7.96, "text": " is actually doing.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 264, 4365, 295, 437, 264, 37624, 3089, 337, 3097, 257, 18161, 3209, 50712, 50712, 307, 767, 884, 13, 50762, 50762, 961, 311, 9192, 294, 13, 50846, 50846, 4546, 1237, 412, 264, 4365, 295, 3097, 257, 18161, 3209, 11, 718, 311, 9901, 577, 291, 632, 51093, 51093, 8895, 257, 3565, 3142, 24590, 2316, 294, 264, 3894, 1164, 13, 51352, 51352, 5470, 502, 295, 2390, 257, 3565, 3142, 24590, 2316, 390, 291, 576, 16500, 577, 281, 14722, 264, 5598, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.11879075806716392, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.011681872420012951}, {"id": 2, "seek": 0, "start": 7.96, "end": 9.64, "text": " Let's dive in.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 264, 4365, 295, 437, 264, 37624, 3089, 337, 3097, 257, 18161, 3209, 50712, 50712, 307, 767, 884, 13, 50762, 50762, 961, 311, 9192, 294, 13, 50846, 50846, 4546, 1237, 412, 264, 4365, 295, 3097, 257, 18161, 3209, 11, 718, 311, 9901, 577, 291, 632, 51093, 51093, 8895, 257, 3565, 3142, 24590, 2316, 294, 264, 3894, 1164, 13, 51352, 51352, 5470, 502, 295, 2390, 257, 3565, 3142, 24590, 2316, 390, 291, 576, 16500, 577, 281, 14722, 264, 5598, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.11879075806716392, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.011681872420012951}, {"id": 3, "seek": 0, "start": 9.64, "end": 14.58, "text": " Before looking at the details of training a neural network, let's recall how you had", "tokens": [50364, 961, 311, 747, 257, 574, 412, 264, 4365, 295, 437, 264, 37624, 3089, 337, 3097, 257, 18161, 3209, 50712, 50712, 307, 767, 884, 13, 50762, 50762, 961, 311, 9192, 294, 13, 50846, 50846, 4546, 1237, 412, 264, 4365, 295, 3097, 257, 18161, 3209, 11, 718, 311, 9901, 577, 291, 632, 51093, 51093, 8895, 257, 3565, 3142, 24590, 2316, 294, 264, 3894, 1164, 13, 51352, 51352, 5470, 502, 295, 2390, 257, 3565, 3142, 24590, 2316, 390, 291, 576, 16500, 577, 281, 14722, 264, 5598, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.11879075806716392, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.011681872420012951}, {"id": 4, "seek": 0, "start": 14.58, "end": 19.76, "text": " trained a logistic regression model in the previous course.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 264, 4365, 295, 437, 264, 37624, 3089, 337, 3097, 257, 18161, 3209, 50712, 50712, 307, 767, 884, 13, 50762, 50762, 961, 311, 9192, 294, 13, 50846, 50846, 4546, 1237, 412, 264, 4365, 295, 3097, 257, 18161, 3209, 11, 718, 311, 9901, 577, 291, 632, 51093, 51093, 8895, 257, 3565, 3142, 24590, 2316, 294, 264, 3894, 1164, 13, 51352, 51352, 5470, 502, 295, 2390, 257, 3565, 3142, 24590, 2316, 390, 291, 576, 16500, 577, 281, 14722, 264, 5598, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.11879075806716392, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.011681872420012951}, {"id": 5, "seek": 0, "start": 19.76, "end": 26.6, "text": " Step 1 of building a logistic regression model was you would specify how to compute the output", "tokens": [50364, 961, 311, 747, 257, 574, 412, 264, 4365, 295, 437, 264, 37624, 3089, 337, 3097, 257, 18161, 3209, 50712, 50712, 307, 767, 884, 13, 50762, 50762, 961, 311, 9192, 294, 13, 50846, 50846, 4546, 1237, 412, 264, 4365, 295, 3097, 257, 18161, 3209, 11, 718, 311, 9901, 577, 291, 632, 51093, 51093, 8895, 257, 3565, 3142, 24590, 2316, 294, 264, 3894, 1164, 13, 51352, 51352, 5470, 502, 295, 2390, 257, 3565, 3142, 24590, 2316, 390, 291, 576, 16500, 577, 281, 14722, 264, 5598, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.11879075806716392, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.011681872420012951}, {"id": 6, "seek": 2660, "start": 26.6, "end": 31.0, "text": " given the input v to the x and the parameters w and b.", "tokens": [50364, 2212, 264, 4846, 371, 281, 264, 2031, 293, 264, 9834, 261, 293, 272, 13, 50584, 50584, 407, 294, 264, 700, 1164, 321, 848, 264, 3565, 3142, 24590, 2445, 6069, 82, 283, 295, 2031, 307, 2681, 50922, 50922, 281, 290, 11, 264, 4556, 3280, 327, 2445, 6456, 281, 261, 5893, 1674, 2031, 1804, 272, 11, 597, 390, 264, 4556, 3280, 327, 2445, 51230, 51230, 6456, 281, 261, 5893, 2031, 1804, 272, 13, 51376, 51376], "temperature": 0.0, "avg_logprob": -0.1328625996907552, "compression_ratio": 1.7417218543046358, "no_speech_prob": 2.5069888579309918e-05}, {"id": 7, "seek": 2660, "start": 31.0, "end": 37.760000000000005, "text": " So in the first course we said the logistic regression function predicts f of x is equal", "tokens": [50364, 2212, 264, 4846, 371, 281, 264, 2031, 293, 264, 9834, 261, 293, 272, 13, 50584, 50584, 407, 294, 264, 700, 1164, 321, 848, 264, 3565, 3142, 24590, 2445, 6069, 82, 283, 295, 2031, 307, 2681, 50922, 50922, 281, 290, 11, 264, 4556, 3280, 327, 2445, 6456, 281, 261, 5893, 1674, 2031, 1804, 272, 11, 597, 390, 264, 4556, 3280, 327, 2445, 51230, 51230, 6456, 281, 261, 5893, 2031, 1804, 272, 13, 51376, 51376], "temperature": 0.0, "avg_logprob": -0.1328625996907552, "compression_ratio": 1.7417218543046358, "no_speech_prob": 2.5069888579309918e-05}, {"id": 8, "seek": 2660, "start": 37.760000000000005, "end": 43.92, "text": " to g, the sigmoid function applied to w dot product x plus b, which was the sigmoid function", "tokens": [50364, 2212, 264, 4846, 371, 281, 264, 2031, 293, 264, 9834, 261, 293, 272, 13, 50584, 50584, 407, 294, 264, 700, 1164, 321, 848, 264, 3565, 3142, 24590, 2445, 6069, 82, 283, 295, 2031, 307, 2681, 50922, 50922, 281, 290, 11, 264, 4556, 3280, 327, 2445, 6456, 281, 261, 5893, 1674, 2031, 1804, 272, 11, 597, 390, 264, 4556, 3280, 327, 2445, 51230, 51230, 6456, 281, 261, 5893, 2031, 1804, 272, 13, 51376, 51376], "temperature": 0.0, "avg_logprob": -0.1328625996907552, "compression_ratio": 1.7417218543046358, "no_speech_prob": 2.5069888579309918e-05}, {"id": 9, "seek": 2660, "start": 43.92, "end": 46.84, "text": " applied to w dot x plus b.", "tokens": [50364, 2212, 264, 4846, 371, 281, 264, 2031, 293, 264, 9834, 261, 293, 272, 13, 50584, 50584, 407, 294, 264, 700, 1164, 321, 848, 264, 3565, 3142, 24590, 2445, 6069, 82, 283, 295, 2031, 307, 2681, 50922, 50922, 281, 290, 11, 264, 4556, 3280, 327, 2445, 6456, 281, 261, 5893, 1674, 2031, 1804, 272, 11, 597, 390, 264, 4556, 3280, 327, 2445, 51230, 51230, 6456, 281, 261, 5893, 2031, 1804, 272, 13, 51376, 51376], "temperature": 0.0, "avg_logprob": -0.1328625996907552, "compression_ratio": 1.7417218543046358, "no_speech_prob": 2.5069888579309918e-05}, {"id": 10, "seek": 4684, "start": 46.84, "end": 58.92, "text": " So if z is the dot product of w of x plus b, then f of x is 1 over 1 plus e to the negative", "tokens": [50364, 407, 498, 710, 307, 264, 5893, 1674, 295, 261, 295, 2031, 1804, 272, 11, 550, 283, 295, 2031, 307, 502, 670, 502, 1804, 308, 281, 264, 3671, 50968, 50968, 710, 13, 51060, 51060, 407, 300, 390, 264, 700, 1823, 11, 689, 281, 16500, 437, 307, 264, 4846, 281, 5598, 2445, 295, 3565, 3142, 51346, 51346, 24590, 11, 293, 300, 5946, 322, 1293, 264, 4846, 2031, 293, 264, 9834, 295, 264, 2316, 13, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.11803861668235377, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.0676847068680217e-06}, {"id": 11, "seek": 4684, "start": 58.92, "end": 60.760000000000005, "text": " z.", "tokens": [50364, 407, 498, 710, 307, 264, 5893, 1674, 295, 261, 295, 2031, 1804, 272, 11, 550, 283, 295, 2031, 307, 502, 670, 502, 1804, 308, 281, 264, 3671, 50968, 50968, 710, 13, 51060, 51060, 407, 300, 390, 264, 700, 1823, 11, 689, 281, 16500, 437, 307, 264, 4846, 281, 5598, 2445, 295, 3565, 3142, 51346, 51346, 24590, 11, 293, 300, 5946, 322, 1293, 264, 4846, 2031, 293, 264, 9834, 295, 264, 2316, 13, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.11803861668235377, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.0676847068680217e-06}, {"id": 12, "seek": 4684, "start": 60.760000000000005, "end": 66.48, "text": " So that was the first step, where to specify what is the input to output function of logistic", "tokens": [50364, 407, 498, 710, 307, 264, 5893, 1674, 295, 261, 295, 2031, 1804, 272, 11, 550, 283, 295, 2031, 307, 502, 670, 502, 1804, 308, 281, 264, 3671, 50968, 50968, 710, 13, 51060, 51060, 407, 300, 390, 264, 700, 1823, 11, 689, 281, 16500, 437, 307, 264, 4846, 281, 5598, 2445, 295, 3565, 3142, 51346, 51346, 24590, 11, 293, 300, 5946, 322, 1293, 264, 4846, 2031, 293, 264, 9834, 295, 264, 2316, 13, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.11803861668235377, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.0676847068680217e-06}, {"id": 13, "seek": 4684, "start": 66.48, "end": 72.48, "text": " regression, and that depends on both the input x and the parameters of the model.", "tokens": [50364, 407, 498, 710, 307, 264, 5893, 1674, 295, 261, 295, 2031, 1804, 272, 11, 550, 283, 295, 2031, 307, 502, 670, 502, 1804, 308, 281, 264, 3671, 50968, 50968, 710, 13, 51060, 51060, 407, 300, 390, 264, 700, 1823, 11, 689, 281, 16500, 437, 307, 264, 4846, 281, 5598, 2445, 295, 3565, 3142, 51346, 51346, 24590, 11, 293, 300, 5946, 322, 1293, 264, 4846, 2031, 293, 264, 9834, 295, 264, 2316, 13, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.11803861668235377, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.0676847068680217e-06}, {"id": 14, "seek": 7248, "start": 72.48, "end": 78.08, "text": " The second step we had to do to train the logistic regression model was to specify the", "tokens": [50364, 440, 1150, 1823, 321, 632, 281, 360, 281, 3847, 264, 3565, 3142, 24590, 2316, 390, 281, 16500, 264, 50644, 50644, 4470, 2445, 293, 611, 264, 2063, 2445, 13, 50803, 50803, 407, 291, 815, 9901, 300, 264, 4470, 2445, 848, 498, 3565, 3142, 24590, 23930, 283, 295, 2031, 293, 51216, 51216, 264, 2727, 3494, 7645, 11, 264, 3539, 7645, 294, 264, 3097, 992, 390, 288, 11, 550, 264, 4470, 322, 300, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.07046435974739693, "compression_ratio": 1.7894736842105263, "no_speech_prob": 2.6841739781957585e-06}, {"id": 15, "seek": 7248, "start": 78.08, "end": 81.26, "text": " loss function and also the cost function.", "tokens": [50364, 440, 1150, 1823, 321, 632, 281, 360, 281, 3847, 264, 3565, 3142, 24590, 2316, 390, 281, 16500, 264, 50644, 50644, 4470, 2445, 293, 611, 264, 2063, 2445, 13, 50803, 50803, 407, 291, 815, 9901, 300, 264, 4470, 2445, 848, 498, 3565, 3142, 24590, 23930, 283, 295, 2031, 293, 51216, 51216, 264, 2727, 3494, 7645, 11, 264, 3539, 7645, 294, 264, 3097, 992, 390, 288, 11, 550, 264, 4470, 322, 300, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.07046435974739693, "compression_ratio": 1.7894736842105263, "no_speech_prob": 2.6841739781957585e-06}, {"id": 16, "seek": 7248, "start": 81.26, "end": 89.52000000000001, "text": " So you may recall that the loss function said if logistic regression outputs f of x and", "tokens": [50364, 440, 1150, 1823, 321, 632, 281, 360, 281, 3847, 264, 3565, 3142, 24590, 2316, 390, 281, 16500, 264, 50644, 50644, 4470, 2445, 293, 611, 264, 2063, 2445, 13, 50803, 50803, 407, 291, 815, 9901, 300, 264, 4470, 2445, 848, 498, 3565, 3142, 24590, 23930, 283, 295, 2031, 293, 51216, 51216, 264, 2727, 3494, 7645, 11, 264, 3539, 7645, 294, 264, 3097, 992, 390, 288, 11, 550, 264, 4470, 322, 300, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.07046435974739693, "compression_ratio": 1.7894736842105263, "no_speech_prob": 2.6841739781957585e-06}, {"id": 17, "seek": 7248, "start": 89.52000000000001, "end": 95.80000000000001, "text": " the ground truth label, the actual label in the training set was y, then the loss on that", "tokens": [50364, 440, 1150, 1823, 321, 632, 281, 360, 281, 3847, 264, 3565, 3142, 24590, 2316, 390, 281, 16500, 264, 50644, 50644, 4470, 2445, 293, 611, 264, 2063, 2445, 13, 50803, 50803, 407, 291, 815, 9901, 300, 264, 4470, 2445, 848, 498, 3565, 3142, 24590, 23930, 283, 295, 2031, 293, 51216, 51216, 264, 2727, 3494, 7645, 11, 264, 3539, 7645, 294, 264, 3097, 992, 390, 288, 11, 550, 264, 4470, 322, 300, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.07046435974739693, "compression_ratio": 1.7894736842105263, "no_speech_prob": 2.6841739781957585e-06}, {"id": 18, "seek": 9580, "start": 95.8, "end": 103.28, "text": " single training example was negative y log f of x minus 1 minus y times log of 1 minus", "tokens": [50364, 2167, 3097, 1365, 390, 3671, 288, 3565, 283, 295, 2031, 3175, 502, 3175, 288, 1413, 3565, 295, 502, 3175, 50738, 50738, 283, 295, 2031, 13, 50800, 50800, 407, 341, 390, 257, 3481, 295, 577, 731, 307, 3565, 3142, 24590, 884, 322, 257, 2167, 3097, 1365, 51108, 51108, 2031, 11, 288, 13, 51324, 51324, 18600, 341, 7123, 295, 257, 4470, 2445, 11, 321, 550, 6964, 264, 2063, 2445, 11, 293, 264, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.13343679582750476, "compression_ratio": 1.6829268292682926, "no_speech_prob": 5.09366373080411e-06}, {"id": 19, "seek": 9580, "start": 103.28, "end": 104.52, "text": " f of x.", "tokens": [50364, 2167, 3097, 1365, 390, 3671, 288, 3565, 283, 295, 2031, 3175, 502, 3175, 288, 1413, 3565, 295, 502, 3175, 50738, 50738, 283, 295, 2031, 13, 50800, 50800, 407, 341, 390, 257, 3481, 295, 577, 731, 307, 3565, 3142, 24590, 884, 322, 257, 2167, 3097, 1365, 51108, 51108, 2031, 11, 288, 13, 51324, 51324, 18600, 341, 7123, 295, 257, 4470, 2445, 11, 321, 550, 6964, 264, 2063, 2445, 11, 293, 264, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.13343679582750476, "compression_ratio": 1.6829268292682926, "no_speech_prob": 5.09366373080411e-06}, {"id": 20, "seek": 9580, "start": 104.52, "end": 110.67999999999999, "text": " So this was a measure of how well is logistic regression doing on a single training example", "tokens": [50364, 2167, 3097, 1365, 390, 3671, 288, 3565, 283, 295, 2031, 3175, 502, 3175, 288, 1413, 3565, 295, 502, 3175, 50738, 50738, 283, 295, 2031, 13, 50800, 50800, 407, 341, 390, 257, 3481, 295, 577, 731, 307, 3565, 3142, 24590, 884, 322, 257, 2167, 3097, 1365, 51108, 51108, 2031, 11, 288, 13, 51324, 51324, 18600, 341, 7123, 295, 257, 4470, 2445, 11, 321, 550, 6964, 264, 2063, 2445, 11, 293, 264, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.13343679582750476, "compression_ratio": 1.6829268292682926, "no_speech_prob": 5.09366373080411e-06}, {"id": 21, "seek": 9580, "start": 110.67999999999999, "end": 115.0, "text": " x, y.", "tokens": [50364, 2167, 3097, 1365, 390, 3671, 288, 3565, 283, 295, 2031, 3175, 502, 3175, 288, 1413, 3565, 295, 502, 3175, 50738, 50738, 283, 295, 2031, 13, 50800, 50800, 407, 341, 390, 257, 3481, 295, 577, 731, 307, 3565, 3142, 24590, 884, 322, 257, 2167, 3097, 1365, 51108, 51108, 2031, 11, 288, 13, 51324, 51324, 18600, 341, 7123, 295, 257, 4470, 2445, 11, 321, 550, 6964, 264, 2063, 2445, 11, 293, 264, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.13343679582750476, "compression_ratio": 1.6829268292682926, "no_speech_prob": 5.09366373080411e-06}, {"id": 22, "seek": 9580, "start": 115.0, "end": 121.44, "text": " Given this definition of a loss function, we then define the cost function, and the", "tokens": [50364, 2167, 3097, 1365, 390, 3671, 288, 3565, 283, 295, 2031, 3175, 502, 3175, 288, 1413, 3565, 295, 502, 3175, 50738, 50738, 283, 295, 2031, 13, 50800, 50800, 407, 341, 390, 257, 3481, 295, 577, 731, 307, 3565, 3142, 24590, 884, 322, 257, 2167, 3097, 1365, 51108, 51108, 2031, 11, 288, 13, 51324, 51324, 18600, 341, 7123, 295, 257, 4470, 2445, 11, 321, 550, 6964, 264, 2063, 2445, 11, 293, 264, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.13343679582750476, "compression_ratio": 1.6829268292682926, "no_speech_prob": 5.09366373080411e-06}, {"id": 23, "seek": 12144, "start": 121.44, "end": 127.92, "text": " cost function was a function of the parameters w and b, and that was just the average that", "tokens": [50364, 2063, 2445, 390, 257, 2445, 295, 264, 9834, 261, 293, 272, 11, 293, 300, 390, 445, 264, 4274, 300, 50688, 50688, 307, 1940, 364, 4274, 670, 439, 275, 3097, 5110, 295, 264, 4470, 2445, 40610, 322, 264, 275, 3097, 51002, 51002, 5110, 2031, 16, 11, 288, 16, 11, 807, 2031, 76, 11, 288, 76, 13, 51286, 51286, 400, 1604, 300, 294, 264, 4188, 321, 434, 1228, 11, 264, 4470, 2445, 307, 257, 2445, 295, 264, 5598, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.1276252269744873, "compression_ratio": 1.8529411764705883, "no_speech_prob": 1.4593665582651738e-06}, {"id": 24, "seek": 12144, "start": 127.92, "end": 134.2, "text": " is taking an average over all m training examples of the loss function computed on the m training", "tokens": [50364, 2063, 2445, 390, 257, 2445, 295, 264, 9834, 261, 293, 272, 11, 293, 300, 390, 445, 264, 4274, 300, 50688, 50688, 307, 1940, 364, 4274, 670, 439, 275, 3097, 5110, 295, 264, 4470, 2445, 40610, 322, 264, 275, 3097, 51002, 51002, 5110, 2031, 16, 11, 288, 16, 11, 807, 2031, 76, 11, 288, 76, 13, 51286, 51286, 400, 1604, 300, 294, 264, 4188, 321, 434, 1228, 11, 264, 4470, 2445, 307, 257, 2445, 295, 264, 5598, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.1276252269744873, "compression_ratio": 1.8529411764705883, "no_speech_prob": 1.4593665582651738e-06}, {"id": 25, "seek": 12144, "start": 134.2, "end": 139.88, "text": " examples x1, y1, through xm, ym.", "tokens": [50364, 2063, 2445, 390, 257, 2445, 295, 264, 9834, 261, 293, 272, 11, 293, 300, 390, 445, 264, 4274, 300, 50688, 50688, 307, 1940, 364, 4274, 670, 439, 275, 3097, 5110, 295, 264, 4470, 2445, 40610, 322, 264, 275, 3097, 51002, 51002, 5110, 2031, 16, 11, 288, 16, 11, 807, 2031, 76, 11, 288, 76, 13, 51286, 51286, 400, 1604, 300, 294, 264, 4188, 321, 434, 1228, 11, 264, 4470, 2445, 307, 257, 2445, 295, 264, 5598, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.1276252269744873, "compression_ratio": 1.8529411764705883, "no_speech_prob": 1.4593665582651738e-06}, {"id": 26, "seek": 12144, "start": 139.88, "end": 147.76, "text": " And remember that in the condition we're using, the loss function is a function of the output", "tokens": [50364, 2063, 2445, 390, 257, 2445, 295, 264, 9834, 261, 293, 272, 11, 293, 300, 390, 445, 264, 4274, 300, 50688, 50688, 307, 1940, 364, 4274, 670, 439, 275, 3097, 5110, 295, 264, 4470, 2445, 40610, 322, 264, 275, 3097, 51002, 51002, 5110, 2031, 16, 11, 288, 16, 11, 807, 2031, 76, 11, 288, 76, 13, 51286, 51286, 400, 1604, 300, 294, 264, 4188, 321, 434, 1228, 11, 264, 4470, 2445, 307, 257, 2445, 295, 264, 5598, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.1276252269744873, "compression_ratio": 1.8529411764705883, "no_speech_prob": 1.4593665582651738e-06}, {"id": 27, "seek": 14776, "start": 147.76, "end": 154.07999999999998, "text": " of the learning algorithm and the ground truth label as computed over a single training example,", "tokens": [50364, 295, 264, 2539, 9284, 293, 264, 2727, 3494, 7645, 382, 40610, 670, 257, 2167, 3097, 1365, 11, 50680, 50680, 9735, 264, 2063, 2445, 361, 307, 364, 4274, 295, 264, 4470, 2445, 40610, 670, 428, 2302, 50958, 50958, 3097, 992, 13, 51044, 51044, 407, 300, 390, 1823, 732, 295, 437, 321, 630, 562, 2390, 493, 3565, 3142, 24590, 13, 51304, 51304, 400, 550, 264, 2636, 293, 2572, 1823, 281, 3847, 264, 3565, 3142, 24590, 2316, 390, 281, 764, 364, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.09916674218526701, "compression_ratio": 1.7391304347826086, "no_speech_prob": 1.3287476576806512e-06}, {"id": 28, "seek": 14776, "start": 154.07999999999998, "end": 159.64, "text": " whereas the cost function j is an average of the loss function computed over your entire", "tokens": [50364, 295, 264, 2539, 9284, 293, 264, 2727, 3494, 7645, 382, 40610, 670, 257, 2167, 3097, 1365, 11, 50680, 50680, 9735, 264, 2063, 2445, 361, 307, 364, 4274, 295, 264, 4470, 2445, 40610, 670, 428, 2302, 50958, 50958, 3097, 992, 13, 51044, 51044, 407, 300, 390, 1823, 732, 295, 437, 321, 630, 562, 2390, 493, 3565, 3142, 24590, 13, 51304, 51304, 400, 550, 264, 2636, 293, 2572, 1823, 281, 3847, 264, 3565, 3142, 24590, 2316, 390, 281, 764, 364, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.09916674218526701, "compression_ratio": 1.7391304347826086, "no_speech_prob": 1.3287476576806512e-06}, {"id": 29, "seek": 14776, "start": 159.64, "end": 161.35999999999999, "text": " training set.", "tokens": [50364, 295, 264, 2539, 9284, 293, 264, 2727, 3494, 7645, 382, 40610, 670, 257, 2167, 3097, 1365, 11, 50680, 50680, 9735, 264, 2063, 2445, 361, 307, 364, 4274, 295, 264, 4470, 2445, 40610, 670, 428, 2302, 50958, 50958, 3097, 992, 13, 51044, 51044, 407, 300, 390, 1823, 732, 295, 437, 321, 630, 562, 2390, 493, 3565, 3142, 24590, 13, 51304, 51304, 400, 550, 264, 2636, 293, 2572, 1823, 281, 3847, 264, 3565, 3142, 24590, 2316, 390, 281, 764, 364, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.09916674218526701, "compression_ratio": 1.7391304347826086, "no_speech_prob": 1.3287476576806512e-06}, {"id": 30, "seek": 14776, "start": 161.35999999999999, "end": 166.56, "text": " So that was step two of what we did when building up logistic regression.", "tokens": [50364, 295, 264, 2539, 9284, 293, 264, 2727, 3494, 7645, 382, 40610, 670, 257, 2167, 3097, 1365, 11, 50680, 50680, 9735, 264, 2063, 2445, 361, 307, 364, 4274, 295, 264, 4470, 2445, 40610, 670, 428, 2302, 50958, 50958, 3097, 992, 13, 51044, 51044, 407, 300, 390, 1823, 732, 295, 437, 321, 630, 562, 2390, 493, 3565, 3142, 24590, 13, 51304, 51304, 400, 550, 264, 2636, 293, 2572, 1823, 281, 3847, 264, 3565, 3142, 24590, 2316, 390, 281, 764, 364, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.09916674218526701, "compression_ratio": 1.7391304347826086, "no_speech_prob": 1.3287476576806512e-06}, {"id": 31, "seek": 14776, "start": 166.56, "end": 172.32, "text": " And then the third and final step to train the logistic regression model was to use an", "tokens": [50364, 295, 264, 2539, 9284, 293, 264, 2727, 3494, 7645, 382, 40610, 670, 257, 2167, 3097, 1365, 11, 50680, 50680, 9735, 264, 2063, 2445, 361, 307, 364, 4274, 295, 264, 4470, 2445, 40610, 670, 428, 2302, 50958, 50958, 3097, 992, 13, 51044, 51044, 407, 300, 390, 1823, 732, 295, 437, 321, 630, 562, 2390, 493, 3565, 3142, 24590, 13, 51304, 51304, 400, 550, 264, 2636, 293, 2572, 1823, 281, 3847, 264, 3565, 3142, 24590, 2316, 390, 281, 764, 364, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.09916674218526701, "compression_ratio": 1.7391304347826086, "no_speech_prob": 1.3287476576806512e-06}, {"id": 32, "seek": 17232, "start": 172.32, "end": 179.64, "text": " algorithm, specifically gradient descent, to minimize that cost function j of w, b to", "tokens": [50364, 9284, 11, 4682, 16235, 23475, 11, 281, 17522, 300, 2063, 2445, 361, 295, 261, 11, 272, 281, 50730, 50730, 17522, 309, 382, 257, 2445, 295, 264, 9834, 261, 293, 272, 13, 50942, 50942, 400, 321, 17522, 264, 2063, 361, 382, 257, 2445, 295, 264, 9834, 1228, 16235, 23475, 11, 689, 51196, 51196, 261, 307, 10588, 382, 261, 3175, 264, 2539, 3314, 8961, 1413, 264, 13760, 295, 361, 365, 3104, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.11494643067660397, "compression_ratio": 1.8150289017341041, "no_speech_prob": 1.863032412074972e-05}, {"id": 33, "seek": 17232, "start": 179.64, "end": 183.88, "text": " minimize it as a function of the parameters w and b.", "tokens": [50364, 9284, 11, 4682, 16235, 23475, 11, 281, 17522, 300, 2063, 2445, 361, 295, 261, 11, 272, 281, 50730, 50730, 17522, 309, 382, 257, 2445, 295, 264, 9834, 261, 293, 272, 13, 50942, 50942, 400, 321, 17522, 264, 2063, 361, 382, 257, 2445, 295, 264, 9834, 1228, 16235, 23475, 11, 689, 51196, 51196, 261, 307, 10588, 382, 261, 3175, 264, 2539, 3314, 8961, 1413, 264, 13760, 295, 361, 365, 3104, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.11494643067660397, "compression_ratio": 1.8150289017341041, "no_speech_prob": 1.863032412074972e-05}, {"id": 34, "seek": 17232, "start": 183.88, "end": 188.95999999999998, "text": " And we minimize the cost j as a function of the parameters using gradient descent, where", "tokens": [50364, 9284, 11, 4682, 16235, 23475, 11, 281, 17522, 300, 2063, 2445, 361, 295, 261, 11, 272, 281, 50730, 50730, 17522, 309, 382, 257, 2445, 295, 264, 9834, 261, 293, 272, 13, 50942, 50942, 400, 321, 17522, 264, 2063, 361, 382, 257, 2445, 295, 264, 9834, 1228, 16235, 23475, 11, 689, 51196, 51196, 261, 307, 10588, 382, 261, 3175, 264, 2539, 3314, 8961, 1413, 264, 13760, 295, 361, 365, 3104, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.11494643067660397, "compression_ratio": 1.8150289017341041, "no_speech_prob": 1.863032412074972e-05}, {"id": 35, "seek": 17232, "start": 188.95999999999998, "end": 196.24, "text": " w is updated as w minus the learning rate alpha times the derivative of j with respect", "tokens": [50364, 9284, 11, 4682, 16235, 23475, 11, 281, 17522, 300, 2063, 2445, 361, 295, 261, 11, 272, 281, 50730, 50730, 17522, 309, 382, 257, 2445, 295, 264, 9834, 261, 293, 272, 13, 50942, 50942, 400, 321, 17522, 264, 2063, 361, 382, 257, 2445, 295, 264, 9834, 1228, 16235, 23475, 11, 689, 51196, 51196, 261, 307, 10588, 382, 261, 3175, 264, 2539, 3314, 8961, 1413, 264, 13760, 295, 361, 365, 3104, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.11494643067660397, "compression_ratio": 1.8150289017341041, "no_speech_prob": 1.863032412074972e-05}, {"id": 36, "seek": 19624, "start": 196.24, "end": 206.04000000000002, "text": " to w, and b similarly is updated as b minus the learning rate alpha times the derivative", "tokens": [50364, 281, 261, 11, 293, 272, 14138, 307, 10588, 382, 272, 3175, 264, 2539, 3314, 8961, 1413, 264, 13760, 50854, 50854, 295, 361, 365, 3104, 281, 272, 13, 51008, 51008, 407, 365, 613, 1045, 4439, 11, 1823, 472, 11, 1608, 5489, 577, 281, 14722, 264, 23930, 2212, 264, 4846, 51220, 51220, 2031, 294, 9834, 11, 1823, 732, 11, 1608, 5489, 4470, 294, 2063, 11, 293, 1823, 1045, 11, 17522, 264, 2063, 51452, 51452, 2445, 11, 321, 8895, 3565, 3142, 24590, 13, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.17784965605962844, "compression_ratio": 1.6386138613861385, "no_speech_prob": 2.090433326884522e-06}, {"id": 37, "seek": 19624, "start": 206.04000000000002, "end": 209.12, "text": " of j with respect to b.", "tokens": [50364, 281, 261, 11, 293, 272, 14138, 307, 10588, 382, 272, 3175, 264, 2539, 3314, 8961, 1413, 264, 13760, 50854, 50854, 295, 361, 365, 3104, 281, 272, 13, 51008, 51008, 407, 365, 613, 1045, 4439, 11, 1823, 472, 11, 1608, 5489, 577, 281, 14722, 264, 23930, 2212, 264, 4846, 51220, 51220, 2031, 294, 9834, 11, 1823, 732, 11, 1608, 5489, 4470, 294, 2063, 11, 293, 1823, 1045, 11, 17522, 264, 2063, 51452, 51452, 2445, 11, 321, 8895, 3565, 3142, 24590, 13, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.17784965605962844, "compression_ratio": 1.6386138613861385, "no_speech_prob": 2.090433326884522e-06}, {"id": 38, "seek": 19624, "start": 209.12, "end": 213.36, "text": " So with these three steps, step one, specifying how to compute the outputs given the input", "tokens": [50364, 281, 261, 11, 293, 272, 14138, 307, 10588, 382, 272, 3175, 264, 2539, 3314, 8961, 1413, 264, 13760, 50854, 50854, 295, 361, 365, 3104, 281, 272, 13, 51008, 51008, 407, 365, 613, 1045, 4439, 11, 1823, 472, 11, 1608, 5489, 577, 281, 14722, 264, 23930, 2212, 264, 4846, 51220, 51220, 2031, 294, 9834, 11, 1823, 732, 11, 1608, 5489, 4470, 294, 2063, 11, 293, 1823, 1045, 11, 17522, 264, 2063, 51452, 51452, 2445, 11, 321, 8895, 3565, 3142, 24590, 13, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.17784965605962844, "compression_ratio": 1.6386138613861385, "no_speech_prob": 2.090433326884522e-06}, {"id": 39, "seek": 19624, "start": 213.36, "end": 218.0, "text": " x in parameters, step two, specifying loss in cost, and step three, minimize the cost", "tokens": [50364, 281, 261, 11, 293, 272, 14138, 307, 10588, 382, 272, 3175, 264, 2539, 3314, 8961, 1413, 264, 13760, 50854, 50854, 295, 361, 365, 3104, 281, 272, 13, 51008, 51008, 407, 365, 613, 1045, 4439, 11, 1823, 472, 11, 1608, 5489, 577, 281, 14722, 264, 23930, 2212, 264, 4846, 51220, 51220, 2031, 294, 9834, 11, 1823, 732, 11, 1608, 5489, 4470, 294, 2063, 11, 293, 1823, 1045, 11, 17522, 264, 2063, 51452, 51452, 2445, 11, 321, 8895, 3565, 3142, 24590, 13, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.17784965605962844, "compression_ratio": 1.6386138613861385, "no_speech_prob": 2.090433326884522e-06}, {"id": 40, "seek": 19624, "start": 218.0, "end": 221.4, "text": " function, we trained logistic regression.", "tokens": [50364, 281, 261, 11, 293, 272, 14138, 307, 10588, 382, 272, 3175, 264, 2539, 3314, 8961, 1413, 264, 13760, 50854, 50854, 295, 361, 365, 3104, 281, 272, 13, 51008, 51008, 407, 365, 613, 1045, 4439, 11, 1823, 472, 11, 1608, 5489, 577, 281, 14722, 264, 23930, 2212, 264, 4846, 51220, 51220, 2031, 294, 9834, 11, 1823, 732, 11, 1608, 5489, 4470, 294, 2063, 11, 293, 1823, 1045, 11, 17522, 264, 2063, 51452, 51452, 2445, 11, 321, 8895, 3565, 3142, 24590, 13, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.17784965605962844, "compression_ratio": 1.6386138613861385, "no_speech_prob": 2.090433326884522e-06}, {"id": 41, "seek": 22140, "start": 221.4, "end": 227.96, "text": " The same three steps is how we can train a neural network in TensorFlow.", "tokens": [50364, 440, 912, 1045, 4439, 307, 577, 321, 393, 3847, 257, 18161, 3209, 294, 37624, 13, 50692, 50692, 823, 718, 311, 574, 412, 577, 613, 1045, 4439, 4471, 281, 3097, 257, 18161, 3209, 13, 50950, 50950, 492, 603, 352, 670, 341, 294, 5044, 2607, 322, 264, 958, 1045, 9788, 11, 457, 534, 10515, 11, 1823, 51230, 51230, 472, 295, 1608, 5489, 577, 281, 14722, 264, 5598, 2212, 264, 4846, 2031, 294, 9834, 261, 293, 272, 11, 300, 311, 51494, 51494, 1096, 365, 341, 3089, 35623, 302, 11, 597, 820, 312, 4963, 490, 1036, 1243, 295, 1608, 5489, 264, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.1091273846012531, "compression_ratio": 1.6788617886178863, "no_speech_prob": 1.96377231986844e-06}, {"id": 42, "seek": 22140, "start": 227.96, "end": 233.12, "text": " Now let's look at how these three steps map to training a neural network.", "tokens": [50364, 440, 912, 1045, 4439, 307, 577, 321, 393, 3847, 257, 18161, 3209, 294, 37624, 13, 50692, 50692, 823, 718, 311, 574, 412, 577, 613, 1045, 4439, 4471, 281, 3097, 257, 18161, 3209, 13, 50950, 50950, 492, 603, 352, 670, 341, 294, 5044, 2607, 322, 264, 958, 1045, 9788, 11, 457, 534, 10515, 11, 1823, 51230, 51230, 472, 295, 1608, 5489, 577, 281, 14722, 264, 5598, 2212, 264, 4846, 2031, 294, 9834, 261, 293, 272, 11, 300, 311, 51494, 51494, 1096, 365, 341, 3089, 35623, 302, 11, 597, 820, 312, 4963, 490, 1036, 1243, 295, 1608, 5489, 264, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.1091273846012531, "compression_ratio": 1.6788617886178863, "no_speech_prob": 1.96377231986844e-06}, {"id": 43, "seek": 22140, "start": 233.12, "end": 238.72, "text": " We'll go over this in greater detail on the next three slides, but really briefly, step", "tokens": [50364, 440, 912, 1045, 4439, 307, 577, 321, 393, 3847, 257, 18161, 3209, 294, 37624, 13, 50692, 50692, 823, 718, 311, 574, 412, 577, 613, 1045, 4439, 4471, 281, 3097, 257, 18161, 3209, 13, 50950, 50950, 492, 603, 352, 670, 341, 294, 5044, 2607, 322, 264, 958, 1045, 9788, 11, 457, 534, 10515, 11, 1823, 51230, 51230, 472, 295, 1608, 5489, 577, 281, 14722, 264, 5598, 2212, 264, 4846, 2031, 294, 9834, 261, 293, 272, 11, 300, 311, 51494, 51494, 1096, 365, 341, 3089, 35623, 302, 11, 597, 820, 312, 4963, 490, 1036, 1243, 295, 1608, 5489, 264, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.1091273846012531, "compression_ratio": 1.6788617886178863, "no_speech_prob": 1.96377231986844e-06}, {"id": 44, "seek": 22140, "start": 238.72, "end": 244.0, "text": " one of specifying how to compute the output given the input x in parameters w and b, that's", "tokens": [50364, 440, 912, 1045, 4439, 307, 577, 321, 393, 3847, 257, 18161, 3209, 294, 37624, 13, 50692, 50692, 823, 718, 311, 574, 412, 577, 613, 1045, 4439, 4471, 281, 3097, 257, 18161, 3209, 13, 50950, 50950, 492, 603, 352, 670, 341, 294, 5044, 2607, 322, 264, 958, 1045, 9788, 11, 457, 534, 10515, 11, 1823, 51230, 51230, 472, 295, 1608, 5489, 577, 281, 14722, 264, 5598, 2212, 264, 4846, 2031, 294, 9834, 261, 293, 272, 11, 300, 311, 51494, 51494, 1096, 365, 341, 3089, 35623, 302, 11, 597, 820, 312, 4963, 490, 1036, 1243, 295, 1608, 5489, 264, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.1091273846012531, "compression_ratio": 1.6788617886178863, "no_speech_prob": 1.96377231986844e-06}, {"id": 45, "seek": 22140, "start": 244.0, "end": 249.32, "text": " done with this code snippet, which should be familiar from last week of specifying the", "tokens": [50364, 440, 912, 1045, 4439, 307, 577, 321, 393, 3847, 257, 18161, 3209, 294, 37624, 13, 50692, 50692, 823, 718, 311, 574, 412, 577, 613, 1045, 4439, 4471, 281, 3097, 257, 18161, 3209, 13, 50950, 50950, 492, 603, 352, 670, 341, 294, 5044, 2607, 322, 264, 958, 1045, 9788, 11, 457, 534, 10515, 11, 1823, 51230, 51230, 472, 295, 1608, 5489, 577, 281, 14722, 264, 5598, 2212, 264, 4846, 2031, 294, 9834, 261, 293, 272, 11, 300, 311, 51494, 51494, 1096, 365, 341, 3089, 35623, 302, 11, 597, 820, 312, 4963, 490, 1036, 1243, 295, 1608, 5489, 264, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.1091273846012531, "compression_ratio": 1.6788617886178863, "no_speech_prob": 1.96377231986844e-06}, {"id": 46, "seek": 24932, "start": 249.32, "end": 254.6, "text": " neural network, and this was actually enough to specify the computations needed in forward", "tokens": [50364, 18161, 3209, 11, 293, 341, 390, 767, 1547, 281, 16500, 264, 2807, 763, 2978, 294, 2128, 50628, 50628, 38377, 420, 337, 264, 38253, 9284, 11, 337, 1365, 13, 50784, 50784, 440, 1150, 1823, 307, 281, 31413, 264, 2316, 293, 281, 980, 309, 437, 4470, 291, 528, 281, 764, 13, 51086, 51086, 400, 510, 311, 257, 3089, 300, 291, 764, 281, 16500, 341, 4470, 2445, 11, 597, 307, 264, 17434, 3278, 51364, 51364, 426, 38, 47, 4470, 2445, 13, 51558, 51558, 400, 1564, 291, 16500, 341, 4470, 11, 1940, 364, 4274, 670, 264, 2302, 3097, 992, 611, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.11855842590332032, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.854105777165387e-06}, {"id": 47, "seek": 24932, "start": 254.6, "end": 257.71999999999997, "text": " propagation or for the inference algorithm, for example.", "tokens": [50364, 18161, 3209, 11, 293, 341, 390, 767, 1547, 281, 16500, 264, 2807, 763, 2978, 294, 2128, 50628, 50628, 38377, 420, 337, 264, 38253, 9284, 11, 337, 1365, 13, 50784, 50784, 440, 1150, 1823, 307, 281, 31413, 264, 2316, 293, 281, 980, 309, 437, 4470, 291, 528, 281, 764, 13, 51086, 51086, 400, 510, 311, 257, 3089, 300, 291, 764, 281, 16500, 341, 4470, 2445, 11, 597, 307, 264, 17434, 3278, 51364, 51364, 426, 38, 47, 4470, 2445, 13, 51558, 51558, 400, 1564, 291, 16500, 341, 4470, 11, 1940, 364, 4274, 670, 264, 2302, 3097, 992, 611, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.11855842590332032, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.854105777165387e-06}, {"id": 48, "seek": 24932, "start": 257.71999999999997, "end": 263.76, "text": " The second step is to compile the model and to tell it what loss you want to use.", "tokens": [50364, 18161, 3209, 11, 293, 341, 390, 767, 1547, 281, 16500, 264, 2807, 763, 2978, 294, 2128, 50628, 50628, 38377, 420, 337, 264, 38253, 9284, 11, 337, 1365, 13, 50784, 50784, 440, 1150, 1823, 307, 281, 31413, 264, 2316, 293, 281, 980, 309, 437, 4470, 291, 528, 281, 764, 13, 51086, 51086, 400, 510, 311, 257, 3089, 300, 291, 764, 281, 16500, 341, 4470, 2445, 11, 597, 307, 264, 17434, 3278, 51364, 51364, 426, 38, 47, 4470, 2445, 13, 51558, 51558, 400, 1564, 291, 16500, 341, 4470, 11, 1940, 364, 4274, 670, 264, 2302, 3097, 992, 611, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.11855842590332032, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.854105777165387e-06}, {"id": 49, "seek": 24932, "start": 263.76, "end": 269.32, "text": " And here's a code that you use to specify this loss function, which is the binary cross", "tokens": [50364, 18161, 3209, 11, 293, 341, 390, 767, 1547, 281, 16500, 264, 2807, 763, 2978, 294, 2128, 50628, 50628, 38377, 420, 337, 264, 38253, 9284, 11, 337, 1365, 13, 50784, 50784, 440, 1150, 1823, 307, 281, 31413, 264, 2316, 293, 281, 980, 309, 437, 4470, 291, 528, 281, 764, 13, 51086, 51086, 400, 510, 311, 257, 3089, 300, 291, 764, 281, 16500, 341, 4470, 2445, 11, 597, 307, 264, 17434, 3278, 51364, 51364, 426, 38, 47, 4470, 2445, 13, 51558, 51558, 400, 1564, 291, 16500, 341, 4470, 11, 1940, 364, 4274, 670, 264, 2302, 3097, 992, 611, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.11855842590332032, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.854105777165387e-06}, {"id": 50, "seek": 24932, "start": 269.32, "end": 273.2, "text": " NGP loss function.", "tokens": [50364, 18161, 3209, 11, 293, 341, 390, 767, 1547, 281, 16500, 264, 2807, 763, 2978, 294, 2128, 50628, 50628, 38377, 420, 337, 264, 38253, 9284, 11, 337, 1365, 13, 50784, 50784, 440, 1150, 1823, 307, 281, 31413, 264, 2316, 293, 281, 980, 309, 437, 4470, 291, 528, 281, 764, 13, 51086, 51086, 400, 510, 311, 257, 3089, 300, 291, 764, 281, 16500, 341, 4470, 2445, 11, 597, 307, 264, 17434, 3278, 51364, 51364, 426, 38, 47, 4470, 2445, 13, 51558, 51558, 400, 1564, 291, 16500, 341, 4470, 11, 1940, 364, 4274, 670, 264, 2302, 3097, 992, 611, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.11855842590332032, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.854105777165387e-06}, {"id": 51, "seek": 24932, "start": 273.2, "end": 278.15999999999997, "text": " And once you specify this loss, taking an average over the entire training set also", "tokens": [50364, 18161, 3209, 11, 293, 341, 390, 767, 1547, 281, 16500, 264, 2807, 763, 2978, 294, 2128, 50628, 50628, 38377, 420, 337, 264, 38253, 9284, 11, 337, 1365, 13, 50784, 50784, 440, 1150, 1823, 307, 281, 31413, 264, 2316, 293, 281, 980, 309, 437, 4470, 291, 528, 281, 764, 13, 51086, 51086, 400, 510, 311, 257, 3089, 300, 291, 764, 281, 16500, 341, 4470, 2445, 11, 597, 307, 264, 17434, 3278, 51364, 51364, 426, 38, 47, 4470, 2445, 13, 51558, 51558, 400, 1564, 291, 16500, 341, 4470, 11, 1940, 364, 4274, 670, 264, 2302, 3097, 992, 611, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.11855842590332032, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.854105777165387e-06}, {"id": 52, "seek": 27816, "start": 278.16, "end": 281.0, "text": " gives you the cost function for the neural network.", "tokens": [50364, 2709, 291, 264, 2063, 2445, 337, 264, 18161, 3209, 13, 50506, 50506, 400, 550, 1823, 1045, 307, 281, 818, 2445, 281, 853, 281, 17522, 264, 2063, 382, 257, 2445, 295, 50784, 50784, 264, 9834, 295, 264, 18161, 3209, 13, 50904, 50904, 961, 311, 574, 294, 5044, 2607, 294, 613, 1045, 4439, 294, 264, 4319, 295, 3097, 257, 18161, 51132, 51132, 3209, 13, 51258, 51258, 440, 700, 1823, 11, 16500, 577, 281, 14722, 264, 5598, 2212, 264, 4846, 2031, 294, 9834, 261, 293, 51460, 51460, 272, 13, 51510, 51510, 639, 3089, 35623, 302, 1608, 11221, 264, 2302, 9482, 295, 264, 18161, 3209, 13, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.13702138864769126, "compression_ratio": 1.891304347826087, "no_speech_prob": 3.2376108265452785e-06}, {"id": 53, "seek": 27816, "start": 281.0, "end": 286.56, "text": " And then step three is to call function to try to minimize the cost as a function of", "tokens": [50364, 2709, 291, 264, 2063, 2445, 337, 264, 18161, 3209, 13, 50506, 50506, 400, 550, 1823, 1045, 307, 281, 818, 2445, 281, 853, 281, 17522, 264, 2063, 382, 257, 2445, 295, 50784, 50784, 264, 9834, 295, 264, 18161, 3209, 13, 50904, 50904, 961, 311, 574, 294, 5044, 2607, 294, 613, 1045, 4439, 294, 264, 4319, 295, 3097, 257, 18161, 51132, 51132, 3209, 13, 51258, 51258, 440, 700, 1823, 11, 16500, 577, 281, 14722, 264, 5598, 2212, 264, 4846, 2031, 294, 9834, 261, 293, 51460, 51460, 272, 13, 51510, 51510, 639, 3089, 35623, 302, 1608, 11221, 264, 2302, 9482, 295, 264, 18161, 3209, 13, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.13702138864769126, "compression_ratio": 1.891304347826087, "no_speech_prob": 3.2376108265452785e-06}, {"id": 54, "seek": 27816, "start": 286.56, "end": 288.96000000000004, "text": " the parameters of the neural network.", "tokens": [50364, 2709, 291, 264, 2063, 2445, 337, 264, 18161, 3209, 13, 50506, 50506, 400, 550, 1823, 1045, 307, 281, 818, 2445, 281, 853, 281, 17522, 264, 2063, 382, 257, 2445, 295, 50784, 50784, 264, 9834, 295, 264, 18161, 3209, 13, 50904, 50904, 961, 311, 574, 294, 5044, 2607, 294, 613, 1045, 4439, 294, 264, 4319, 295, 3097, 257, 18161, 51132, 51132, 3209, 13, 51258, 51258, 440, 700, 1823, 11, 16500, 577, 281, 14722, 264, 5598, 2212, 264, 4846, 2031, 294, 9834, 261, 293, 51460, 51460, 272, 13, 51510, 51510, 639, 3089, 35623, 302, 1608, 11221, 264, 2302, 9482, 295, 264, 18161, 3209, 13, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.13702138864769126, "compression_ratio": 1.891304347826087, "no_speech_prob": 3.2376108265452785e-06}, {"id": 55, "seek": 27816, "start": 288.96000000000004, "end": 293.52000000000004, "text": " Let's look in greater detail in these three steps in the context of training a neural", "tokens": [50364, 2709, 291, 264, 2063, 2445, 337, 264, 18161, 3209, 13, 50506, 50506, 400, 550, 1823, 1045, 307, 281, 818, 2445, 281, 853, 281, 17522, 264, 2063, 382, 257, 2445, 295, 50784, 50784, 264, 9834, 295, 264, 18161, 3209, 13, 50904, 50904, 961, 311, 574, 294, 5044, 2607, 294, 613, 1045, 4439, 294, 264, 4319, 295, 3097, 257, 18161, 51132, 51132, 3209, 13, 51258, 51258, 440, 700, 1823, 11, 16500, 577, 281, 14722, 264, 5598, 2212, 264, 4846, 2031, 294, 9834, 261, 293, 51460, 51460, 272, 13, 51510, 51510, 639, 3089, 35623, 302, 1608, 11221, 264, 2302, 9482, 295, 264, 18161, 3209, 13, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.13702138864769126, "compression_ratio": 1.891304347826087, "no_speech_prob": 3.2376108265452785e-06}, {"id": 56, "seek": 27816, "start": 293.52000000000004, "end": 296.04, "text": " network.", "tokens": [50364, 2709, 291, 264, 2063, 2445, 337, 264, 18161, 3209, 13, 50506, 50506, 400, 550, 1823, 1045, 307, 281, 818, 2445, 281, 853, 281, 17522, 264, 2063, 382, 257, 2445, 295, 50784, 50784, 264, 9834, 295, 264, 18161, 3209, 13, 50904, 50904, 961, 311, 574, 294, 5044, 2607, 294, 613, 1045, 4439, 294, 264, 4319, 295, 3097, 257, 18161, 51132, 51132, 3209, 13, 51258, 51258, 440, 700, 1823, 11, 16500, 577, 281, 14722, 264, 5598, 2212, 264, 4846, 2031, 294, 9834, 261, 293, 51460, 51460, 272, 13, 51510, 51510, 639, 3089, 35623, 302, 1608, 11221, 264, 2302, 9482, 295, 264, 18161, 3209, 13, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.13702138864769126, "compression_ratio": 1.891304347826087, "no_speech_prob": 3.2376108265452785e-06}, {"id": 57, "seek": 27816, "start": 296.04, "end": 300.08000000000004, "text": " The first step, specify how to compute the output given the input x in parameters w and", "tokens": [50364, 2709, 291, 264, 2063, 2445, 337, 264, 18161, 3209, 13, 50506, 50506, 400, 550, 1823, 1045, 307, 281, 818, 2445, 281, 853, 281, 17522, 264, 2063, 382, 257, 2445, 295, 50784, 50784, 264, 9834, 295, 264, 18161, 3209, 13, 50904, 50904, 961, 311, 574, 294, 5044, 2607, 294, 613, 1045, 4439, 294, 264, 4319, 295, 3097, 257, 18161, 51132, 51132, 3209, 13, 51258, 51258, 440, 700, 1823, 11, 16500, 577, 281, 14722, 264, 5598, 2212, 264, 4846, 2031, 294, 9834, 261, 293, 51460, 51460, 272, 13, 51510, 51510, 639, 3089, 35623, 302, 1608, 11221, 264, 2302, 9482, 295, 264, 18161, 3209, 13, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.13702138864769126, "compression_ratio": 1.891304347826087, "no_speech_prob": 3.2376108265452785e-06}, {"id": 58, "seek": 27816, "start": 300.08000000000004, "end": 301.08000000000004, "text": " b.", "tokens": [50364, 2709, 291, 264, 2063, 2445, 337, 264, 18161, 3209, 13, 50506, 50506, 400, 550, 1823, 1045, 307, 281, 818, 2445, 281, 853, 281, 17522, 264, 2063, 382, 257, 2445, 295, 50784, 50784, 264, 9834, 295, 264, 18161, 3209, 13, 50904, 50904, 961, 311, 574, 294, 5044, 2607, 294, 613, 1045, 4439, 294, 264, 4319, 295, 3097, 257, 18161, 51132, 51132, 3209, 13, 51258, 51258, 440, 700, 1823, 11, 16500, 577, 281, 14722, 264, 5598, 2212, 264, 4846, 2031, 294, 9834, 261, 293, 51460, 51460, 272, 13, 51510, 51510, 639, 3089, 35623, 302, 1608, 11221, 264, 2302, 9482, 295, 264, 18161, 3209, 13, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.13702138864769126, "compression_ratio": 1.891304347826087, "no_speech_prob": 3.2376108265452785e-06}, {"id": 59, "seek": 27816, "start": 301.08000000000004, "end": 305.02000000000004, "text": " This code snippet specifies the entire architecture of the neural network.", "tokens": [50364, 2709, 291, 264, 2063, 2445, 337, 264, 18161, 3209, 13, 50506, 50506, 400, 550, 1823, 1045, 307, 281, 818, 2445, 281, 853, 281, 17522, 264, 2063, 382, 257, 2445, 295, 50784, 50784, 264, 9834, 295, 264, 18161, 3209, 13, 50904, 50904, 961, 311, 574, 294, 5044, 2607, 294, 613, 1045, 4439, 294, 264, 4319, 295, 3097, 257, 18161, 51132, 51132, 3209, 13, 51258, 51258, 440, 700, 1823, 11, 16500, 577, 281, 14722, 264, 5598, 2212, 264, 4846, 2031, 294, 9834, 261, 293, 51460, 51460, 272, 13, 51510, 51510, 639, 3089, 35623, 302, 1608, 11221, 264, 2302, 9482, 295, 264, 18161, 3209, 13, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.13702138864769126, "compression_ratio": 1.891304347826087, "no_speech_prob": 3.2376108265452785e-06}, {"id": 60, "seek": 30502, "start": 305.02, "end": 309.96, "text": " It tells you that there are 25 hidden units in the first hidden layer, then 15 in the", "tokens": [50364, 467, 5112, 291, 300, 456, 366, 3552, 7633, 6815, 294, 264, 700, 7633, 4583, 11, 550, 2119, 294, 264, 50611, 50611, 958, 472, 11, 293, 550, 472, 5598, 4985, 11, 293, 300, 321, 434, 1228, 264, 4556, 472, 24433, 2158, 13, 50879, 50879, 400, 370, 2361, 322, 341, 3089, 35623, 302, 11, 321, 458, 611, 437, 366, 264, 9834, 11, 261, 16, 11, 272, 16, 295, 264, 51115, 51115, 700, 4583, 11, 9834, 295, 264, 1150, 4583, 11, 293, 9834, 295, 264, 2636, 4583, 13, 51352, 51352, 407, 341, 3089, 35623, 302, 1608, 11221, 264, 2302, 9482, 295, 264, 18161, 3209, 293, 4412, 51627, 51627], "temperature": 0.0, "avg_logprob": -0.15414455449469736, "compression_ratio": 1.8220338983050848, "no_speech_prob": 6.540129561471986e-06}, {"id": 61, "seek": 30502, "start": 309.96, "end": 315.32, "text": " next one, and then one output unit, and that we're using the sig one activation value.", "tokens": [50364, 467, 5112, 291, 300, 456, 366, 3552, 7633, 6815, 294, 264, 700, 7633, 4583, 11, 550, 2119, 294, 264, 50611, 50611, 958, 472, 11, 293, 550, 472, 5598, 4985, 11, 293, 300, 321, 434, 1228, 264, 4556, 472, 24433, 2158, 13, 50879, 50879, 400, 370, 2361, 322, 341, 3089, 35623, 302, 11, 321, 458, 611, 437, 366, 264, 9834, 11, 261, 16, 11, 272, 16, 295, 264, 51115, 51115, 700, 4583, 11, 9834, 295, 264, 1150, 4583, 11, 293, 9834, 295, 264, 2636, 4583, 13, 51352, 51352, 407, 341, 3089, 35623, 302, 1608, 11221, 264, 2302, 9482, 295, 264, 18161, 3209, 293, 4412, 51627, 51627], "temperature": 0.0, "avg_logprob": -0.15414455449469736, "compression_ratio": 1.8220338983050848, "no_speech_prob": 6.540129561471986e-06}, {"id": 62, "seek": 30502, "start": 315.32, "end": 320.03999999999996, "text": " And so based on this code snippet, we know also what are the parameters, w1, b1 of the", "tokens": [50364, 467, 5112, 291, 300, 456, 366, 3552, 7633, 6815, 294, 264, 700, 7633, 4583, 11, 550, 2119, 294, 264, 50611, 50611, 958, 472, 11, 293, 550, 472, 5598, 4985, 11, 293, 300, 321, 434, 1228, 264, 4556, 472, 24433, 2158, 13, 50879, 50879, 400, 370, 2361, 322, 341, 3089, 35623, 302, 11, 321, 458, 611, 437, 366, 264, 9834, 11, 261, 16, 11, 272, 16, 295, 264, 51115, 51115, 700, 4583, 11, 9834, 295, 264, 1150, 4583, 11, 293, 9834, 295, 264, 2636, 4583, 13, 51352, 51352, 407, 341, 3089, 35623, 302, 1608, 11221, 264, 2302, 9482, 295, 264, 18161, 3209, 293, 4412, 51627, 51627], "temperature": 0.0, "avg_logprob": -0.15414455449469736, "compression_ratio": 1.8220338983050848, "no_speech_prob": 6.540129561471986e-06}, {"id": 63, "seek": 30502, "start": 320.03999999999996, "end": 324.78, "text": " first layer, parameters of the second layer, and parameters of the third layer.", "tokens": [50364, 467, 5112, 291, 300, 456, 366, 3552, 7633, 6815, 294, 264, 700, 7633, 4583, 11, 550, 2119, 294, 264, 50611, 50611, 958, 472, 11, 293, 550, 472, 5598, 4985, 11, 293, 300, 321, 434, 1228, 264, 4556, 472, 24433, 2158, 13, 50879, 50879, 400, 370, 2361, 322, 341, 3089, 35623, 302, 11, 321, 458, 611, 437, 366, 264, 9834, 11, 261, 16, 11, 272, 16, 295, 264, 51115, 51115, 700, 4583, 11, 9834, 295, 264, 1150, 4583, 11, 293, 9834, 295, 264, 2636, 4583, 13, 51352, 51352, 407, 341, 3089, 35623, 302, 1608, 11221, 264, 2302, 9482, 295, 264, 18161, 3209, 293, 4412, 51627, 51627], "temperature": 0.0, "avg_logprob": -0.15414455449469736, "compression_ratio": 1.8220338983050848, "no_speech_prob": 6.540129561471986e-06}, {"id": 64, "seek": 30502, "start": 324.78, "end": 330.28, "text": " So this code snippet specifies the entire architecture of the neural network and therefore", "tokens": [50364, 467, 5112, 291, 300, 456, 366, 3552, 7633, 6815, 294, 264, 700, 7633, 4583, 11, 550, 2119, 294, 264, 50611, 50611, 958, 472, 11, 293, 550, 472, 5598, 4985, 11, 293, 300, 321, 434, 1228, 264, 4556, 472, 24433, 2158, 13, 50879, 50879, 400, 370, 2361, 322, 341, 3089, 35623, 302, 11, 321, 458, 611, 437, 366, 264, 9834, 11, 261, 16, 11, 272, 16, 295, 264, 51115, 51115, 700, 4583, 11, 9834, 295, 264, 1150, 4583, 11, 293, 9834, 295, 264, 2636, 4583, 13, 51352, 51352, 407, 341, 3089, 35623, 302, 1608, 11221, 264, 2302, 9482, 295, 264, 18161, 3209, 293, 4412, 51627, 51627], "temperature": 0.0, "avg_logprob": -0.15414455449469736, "compression_ratio": 1.8220338983050848, "no_speech_prob": 6.540129561471986e-06}, {"id": 65, "seek": 33028, "start": 330.28, "end": 336.65999999999997, "text": " tells TensorFlow everything it needs in order to compute the output x as a function.", "tokens": [50364, 5112, 37624, 1203, 309, 2203, 294, 1668, 281, 14722, 264, 5598, 2031, 382, 257, 2445, 13, 50683, 50683, 682, 1668, 281, 14722, 264, 5598, 257, 18, 420, 283, 295, 2031, 382, 257, 2445, 295, 264, 4846, 2031, 293, 264, 9834, 51076, 51076, 510, 321, 362, 3720, 261, 75, 293, 888, 13, 51324, 51324, 961, 311, 352, 322, 281, 1823, 732, 13, 51416, 51416, 682, 264, 1150, 1823, 11, 291, 362, 281, 16500, 437, 307, 264, 4470, 2445, 11, 293, 300, 486, 611, 6964, 51652, 51652, 264, 2063, 2445, 321, 764, 281, 3847, 264, 18161, 3209, 13, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.12031970977783203, "compression_ratio": 1.7511520737327189, "no_speech_prob": 3.9669253055762965e-06}, {"id": 66, "seek": 33028, "start": 336.65999999999997, "end": 344.52, "text": " In order to compute the output a3 or f of x as a function of the input x and the parameters", "tokens": [50364, 5112, 37624, 1203, 309, 2203, 294, 1668, 281, 14722, 264, 5598, 2031, 382, 257, 2445, 13, 50683, 50683, 682, 1668, 281, 14722, 264, 5598, 257, 18, 420, 283, 295, 2031, 382, 257, 2445, 295, 264, 4846, 2031, 293, 264, 9834, 51076, 51076, 510, 321, 362, 3720, 261, 75, 293, 888, 13, 51324, 51324, 961, 311, 352, 322, 281, 1823, 732, 13, 51416, 51416, 682, 264, 1150, 1823, 11, 291, 362, 281, 16500, 437, 307, 264, 4470, 2445, 11, 293, 300, 486, 611, 6964, 51652, 51652, 264, 2063, 2445, 321, 764, 281, 3847, 264, 18161, 3209, 13, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.12031970977783203, "compression_ratio": 1.7511520737327189, "no_speech_prob": 3.9669253055762965e-06}, {"id": 67, "seek": 33028, "start": 344.52, "end": 349.47999999999996, "text": " here we have written wl and bl.", "tokens": [50364, 5112, 37624, 1203, 309, 2203, 294, 1668, 281, 14722, 264, 5598, 2031, 382, 257, 2445, 13, 50683, 50683, 682, 1668, 281, 14722, 264, 5598, 257, 18, 420, 283, 295, 2031, 382, 257, 2445, 295, 264, 4846, 2031, 293, 264, 9834, 51076, 51076, 510, 321, 362, 3720, 261, 75, 293, 888, 13, 51324, 51324, 961, 311, 352, 322, 281, 1823, 732, 13, 51416, 51416, 682, 264, 1150, 1823, 11, 291, 362, 281, 16500, 437, 307, 264, 4470, 2445, 11, 293, 300, 486, 611, 6964, 51652, 51652, 264, 2063, 2445, 321, 764, 281, 3847, 264, 18161, 3209, 13, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.12031970977783203, "compression_ratio": 1.7511520737327189, "no_speech_prob": 3.9669253055762965e-06}, {"id": 68, "seek": 33028, "start": 349.47999999999996, "end": 351.32, "text": " Let's go on to step two.", "tokens": [50364, 5112, 37624, 1203, 309, 2203, 294, 1668, 281, 14722, 264, 5598, 2031, 382, 257, 2445, 13, 50683, 50683, 682, 1668, 281, 14722, 264, 5598, 257, 18, 420, 283, 295, 2031, 382, 257, 2445, 295, 264, 4846, 2031, 293, 264, 9834, 51076, 51076, 510, 321, 362, 3720, 261, 75, 293, 888, 13, 51324, 51324, 961, 311, 352, 322, 281, 1823, 732, 13, 51416, 51416, 682, 264, 1150, 1823, 11, 291, 362, 281, 16500, 437, 307, 264, 4470, 2445, 11, 293, 300, 486, 611, 6964, 51652, 51652, 264, 2063, 2445, 321, 764, 281, 3847, 264, 18161, 3209, 13, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.12031970977783203, "compression_ratio": 1.7511520737327189, "no_speech_prob": 3.9669253055762965e-06}, {"id": 69, "seek": 33028, "start": 351.32, "end": 356.03999999999996, "text": " In the second step, you have to specify what is the loss function, and that will also define", "tokens": [50364, 5112, 37624, 1203, 309, 2203, 294, 1668, 281, 14722, 264, 5598, 2031, 382, 257, 2445, 13, 50683, 50683, 682, 1668, 281, 14722, 264, 5598, 257, 18, 420, 283, 295, 2031, 382, 257, 2445, 295, 264, 4846, 2031, 293, 264, 9834, 51076, 51076, 510, 321, 362, 3720, 261, 75, 293, 888, 13, 51324, 51324, 961, 311, 352, 322, 281, 1823, 732, 13, 51416, 51416, 682, 264, 1150, 1823, 11, 291, 362, 281, 16500, 437, 307, 264, 4470, 2445, 11, 293, 300, 486, 611, 6964, 51652, 51652, 264, 2063, 2445, 321, 764, 281, 3847, 264, 18161, 3209, 13, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.12031970977783203, "compression_ratio": 1.7511520737327189, "no_speech_prob": 3.9669253055762965e-06}, {"id": 70, "seek": 33028, "start": 356.03999999999996, "end": 359.88, "text": " the cost function we use to train the neural network.", "tokens": [50364, 5112, 37624, 1203, 309, 2203, 294, 1668, 281, 14722, 264, 5598, 2031, 382, 257, 2445, 13, 50683, 50683, 682, 1668, 281, 14722, 264, 5598, 257, 18, 420, 283, 295, 2031, 382, 257, 2445, 295, 264, 4846, 2031, 293, 264, 9834, 51076, 51076, 510, 321, 362, 3720, 261, 75, 293, 888, 13, 51324, 51324, 961, 311, 352, 322, 281, 1823, 732, 13, 51416, 51416, 682, 264, 1150, 1823, 11, 291, 362, 281, 16500, 437, 307, 264, 4470, 2445, 11, 293, 300, 486, 611, 6964, 51652, 51652, 264, 2063, 2445, 321, 764, 281, 3847, 264, 18161, 3209, 13, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.12031970977783203, "compression_ratio": 1.7511520737327189, "no_speech_prob": 3.9669253055762965e-06}, {"id": 71, "seek": 35988, "start": 359.88, "end": 369.68, "text": " So for the MNIST 01 digit classification problem is a binary classification problem, and the", "tokens": [50364, 407, 337, 264, 376, 45, 19756, 23185, 14293, 21538, 1154, 307, 257, 17434, 21538, 1154, 11, 293, 264, 50854, 50854, 881, 2689, 538, 1400, 4470, 2445, 281, 764, 307, 341, 472, 13, 51052, 51052, 467, 311, 767, 264, 912, 4470, 2445, 382, 437, 321, 632, 337, 3565, 3142, 24590, 307, 3671, 51320, 51320, 288, 3565, 283, 295, 2031, 3175, 472, 3175, 288, 1413, 3565, 472, 3175, 283, 295, 2031, 11, 689, 288, 307, 264, 2727, 3494, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.17545255422592163, "compression_ratio": 1.6326530612244898, "no_speech_prob": 4.425421593623469e-06}, {"id": 72, "seek": 35988, "start": 369.68, "end": 373.64, "text": " most common by far loss function to use is this one.", "tokens": [50364, 407, 337, 264, 376, 45, 19756, 23185, 14293, 21538, 1154, 307, 257, 17434, 21538, 1154, 11, 293, 264, 50854, 50854, 881, 2689, 538, 1400, 4470, 2445, 281, 764, 307, 341, 472, 13, 51052, 51052, 467, 311, 767, 264, 912, 4470, 2445, 382, 437, 321, 632, 337, 3565, 3142, 24590, 307, 3671, 51320, 51320, 288, 3565, 283, 295, 2031, 3175, 472, 3175, 288, 1413, 3565, 472, 3175, 283, 295, 2031, 11, 689, 288, 307, 264, 2727, 3494, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.17545255422592163, "compression_ratio": 1.6326530612244898, "no_speech_prob": 4.425421593623469e-06}, {"id": 73, "seek": 35988, "start": 373.64, "end": 379.0, "text": " It's actually the same loss function as what we had for logistic regression is negative", "tokens": [50364, 407, 337, 264, 376, 45, 19756, 23185, 14293, 21538, 1154, 307, 257, 17434, 21538, 1154, 11, 293, 264, 50854, 50854, 881, 2689, 538, 1400, 4470, 2445, 281, 764, 307, 341, 472, 13, 51052, 51052, 467, 311, 767, 264, 912, 4470, 2445, 382, 437, 321, 632, 337, 3565, 3142, 24590, 307, 3671, 51320, 51320, 288, 3565, 283, 295, 2031, 3175, 472, 3175, 288, 1413, 3565, 472, 3175, 283, 295, 2031, 11, 689, 288, 307, 264, 2727, 3494, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.17545255422592163, "compression_ratio": 1.6326530612244898, "no_speech_prob": 4.425421593623469e-06}, {"id": 74, "seek": 35988, "start": 379.0, "end": 387.0, "text": " y log f of x minus one minus y times log one minus f of x, where y is the ground truth", "tokens": [50364, 407, 337, 264, 376, 45, 19756, 23185, 14293, 21538, 1154, 307, 257, 17434, 21538, 1154, 11, 293, 264, 50854, 50854, 881, 2689, 538, 1400, 4470, 2445, 281, 764, 307, 341, 472, 13, 51052, 51052, 467, 311, 767, 264, 912, 4470, 2445, 382, 437, 321, 632, 337, 3565, 3142, 24590, 307, 3671, 51320, 51320, 288, 3565, 283, 295, 2031, 3175, 472, 3175, 288, 1413, 3565, 472, 3175, 283, 295, 2031, 11, 689, 288, 307, 264, 2727, 3494, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.17545255422592163, "compression_ratio": 1.6326530612244898, "no_speech_prob": 4.425421593623469e-06}, {"id": 75, "seek": 38700, "start": 387.0, "end": 392.72, "text": " label, sometimes also called the target label y, and f of x is now the output of the neural", "tokens": [50364, 7645, 11, 2171, 611, 1219, 264, 3779, 7645, 288, 11, 293, 283, 295, 2031, 307, 586, 264, 5598, 295, 264, 18161, 50650, 50650, 3209, 13, 50712, 50712, 407, 294, 264, 27575, 295, 37624, 11, 341, 4470, 2445, 307, 1219, 17434, 3278, 30867, 11, 51090, 51090, 293, 264, 28431, 307, 281, 1029, 37624, 281, 31413, 264, 18161, 3209, 1228, 341, 4470, 2445, 13, 51422, 51422, 400, 1071, 8584, 3637, 11, 591, 6985, 390, 7993, 257, 6405, 300, 632, 4743, 21761, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.12299977727683194, "compression_ratio": 1.6801801801801801, "no_speech_prob": 3.0415219498536317e-06}, {"id": 76, "seek": 38700, "start": 392.72, "end": 393.96, "text": " network.", "tokens": [50364, 7645, 11, 2171, 611, 1219, 264, 3779, 7645, 288, 11, 293, 283, 295, 2031, 307, 586, 264, 5598, 295, 264, 18161, 50650, 50650, 3209, 13, 50712, 50712, 407, 294, 264, 27575, 295, 37624, 11, 341, 4470, 2445, 307, 1219, 17434, 3278, 30867, 11, 51090, 51090, 293, 264, 28431, 307, 281, 1029, 37624, 281, 31413, 264, 18161, 3209, 1228, 341, 4470, 2445, 13, 51422, 51422, 400, 1071, 8584, 3637, 11, 591, 6985, 390, 7993, 257, 6405, 300, 632, 4743, 21761, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.12299977727683194, "compression_ratio": 1.6801801801801801, "no_speech_prob": 3.0415219498536317e-06}, {"id": 77, "seek": 38700, "start": 393.96, "end": 401.52, "text": " So in the terminology of TensorFlow, this loss function is called binary cross entropy,", "tokens": [50364, 7645, 11, 2171, 611, 1219, 264, 3779, 7645, 288, 11, 293, 283, 295, 2031, 307, 586, 264, 5598, 295, 264, 18161, 50650, 50650, 3209, 13, 50712, 50712, 407, 294, 264, 27575, 295, 37624, 11, 341, 4470, 2445, 307, 1219, 17434, 3278, 30867, 11, 51090, 51090, 293, 264, 28431, 307, 281, 1029, 37624, 281, 31413, 264, 18161, 3209, 1228, 341, 4470, 2445, 13, 51422, 51422, 400, 1071, 8584, 3637, 11, 591, 6985, 390, 7993, 257, 6405, 300, 632, 4743, 21761, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.12299977727683194, "compression_ratio": 1.6801801801801801, "no_speech_prob": 3.0415219498536317e-06}, {"id": 78, "seek": 38700, "start": 401.52, "end": 408.16, "text": " and the syntax is to ask TensorFlow to compile the neural network using this loss function.", "tokens": [50364, 7645, 11, 2171, 611, 1219, 264, 3779, 7645, 288, 11, 293, 283, 295, 2031, 307, 586, 264, 5598, 295, 264, 18161, 50650, 50650, 3209, 13, 50712, 50712, 407, 294, 264, 27575, 295, 37624, 11, 341, 4470, 2445, 307, 1219, 17434, 3278, 30867, 11, 51090, 51090, 293, 264, 28431, 307, 281, 1029, 37624, 281, 31413, 264, 18161, 3209, 1228, 341, 4470, 2445, 13, 51422, 51422, 400, 1071, 8584, 3637, 11, 591, 6985, 390, 7993, 257, 6405, 300, 632, 4743, 21761, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.12299977727683194, "compression_ratio": 1.6801801801801801, "no_speech_prob": 3.0415219498536317e-06}, {"id": 79, "seek": 38700, "start": 408.16, "end": 414.16, "text": " And another historical note, Keras was originally a library that had developed independently", "tokens": [50364, 7645, 11, 2171, 611, 1219, 264, 3779, 7645, 288, 11, 293, 283, 295, 2031, 307, 586, 264, 5598, 295, 264, 18161, 50650, 50650, 3209, 13, 50712, 50712, 407, 294, 264, 27575, 295, 37624, 11, 341, 4470, 2445, 307, 1219, 17434, 3278, 30867, 11, 51090, 51090, 293, 264, 28431, 307, 281, 1029, 37624, 281, 31413, 264, 18161, 3209, 1228, 341, 4470, 2445, 13, 51422, 51422, 400, 1071, 8584, 3637, 11, 591, 6985, 390, 7993, 257, 6405, 300, 632, 4743, 21761, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.12299977727683194, "compression_ratio": 1.6801801801801801, "no_speech_prob": 3.0415219498536317e-06}, {"id": 80, "seek": 41416, "start": 414.16, "end": 418.72, "text": " of TensorFlow, it was actually a totally separate project from TensorFlow, but eventually it", "tokens": [50364, 295, 37624, 11, 309, 390, 767, 257, 3879, 4994, 1716, 490, 37624, 11, 457, 4728, 309, 50592, 50592, 658, 36427, 666, 37624, 11, 597, 307, 983, 321, 362, 256, 69, 13, 5767, 296, 75, 6414, 822, 13, 75, 772, 279, 13, 3322, 1315, 295, 341, 50932, 50932, 4470, 2445, 13, 51028, 51028, 400, 538, 264, 636, 11, 286, 500, 380, 1009, 1604, 264, 5288, 295, 439, 264, 4470, 6828, 294, 37624, 11, 51282, 51282, 457, 286, 445, 360, 257, 1702, 3670, 3164, 2059, 281, 915, 264, 558, 1315, 11, 293, 550, 286, 5452, 300, 51474, 51474, 666, 452, 3089, 13, 51574, 51574, 10222, 22206, 264, 4470, 365, 3104, 281, 257, 2167, 3097, 1365, 11, 37624, 3255, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.09868904749552408, "compression_ratio": 1.7226277372262773, "no_speech_prob": 2.507012504793238e-05}, {"id": 81, "seek": 41416, "start": 418.72, "end": 425.52000000000004, "text": " got merged into TensorFlow, which is why we have tf.keraslibrary.losses.the name of this", "tokens": [50364, 295, 37624, 11, 309, 390, 767, 257, 3879, 4994, 1716, 490, 37624, 11, 457, 4728, 309, 50592, 50592, 658, 36427, 666, 37624, 11, 597, 307, 983, 321, 362, 256, 69, 13, 5767, 296, 75, 6414, 822, 13, 75, 772, 279, 13, 3322, 1315, 295, 341, 50932, 50932, 4470, 2445, 13, 51028, 51028, 400, 538, 264, 636, 11, 286, 500, 380, 1009, 1604, 264, 5288, 295, 439, 264, 4470, 6828, 294, 37624, 11, 51282, 51282, 457, 286, 445, 360, 257, 1702, 3670, 3164, 2059, 281, 915, 264, 558, 1315, 11, 293, 550, 286, 5452, 300, 51474, 51474, 666, 452, 3089, 13, 51574, 51574, 10222, 22206, 264, 4470, 365, 3104, 281, 257, 2167, 3097, 1365, 11, 37624, 3255, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.09868904749552408, "compression_ratio": 1.7226277372262773, "no_speech_prob": 2.507012504793238e-05}, {"id": 82, "seek": 41416, "start": 425.52000000000004, "end": 427.44000000000005, "text": " loss function.", "tokens": [50364, 295, 37624, 11, 309, 390, 767, 257, 3879, 4994, 1716, 490, 37624, 11, 457, 4728, 309, 50592, 50592, 658, 36427, 666, 37624, 11, 597, 307, 983, 321, 362, 256, 69, 13, 5767, 296, 75, 6414, 822, 13, 75, 772, 279, 13, 3322, 1315, 295, 341, 50932, 50932, 4470, 2445, 13, 51028, 51028, 400, 538, 264, 636, 11, 286, 500, 380, 1009, 1604, 264, 5288, 295, 439, 264, 4470, 6828, 294, 37624, 11, 51282, 51282, 457, 286, 445, 360, 257, 1702, 3670, 3164, 2059, 281, 915, 264, 558, 1315, 11, 293, 550, 286, 5452, 300, 51474, 51474, 666, 452, 3089, 13, 51574, 51574, 10222, 22206, 264, 4470, 365, 3104, 281, 257, 2167, 3097, 1365, 11, 37624, 3255, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.09868904749552408, "compression_ratio": 1.7226277372262773, "no_speech_prob": 2.507012504793238e-05}, {"id": 83, "seek": 41416, "start": 427.44000000000005, "end": 432.52000000000004, "text": " And by the way, I don't always remember the names of all the loss functions in TensorFlow,", "tokens": [50364, 295, 37624, 11, 309, 390, 767, 257, 3879, 4994, 1716, 490, 37624, 11, 457, 4728, 309, 50592, 50592, 658, 36427, 666, 37624, 11, 597, 307, 983, 321, 362, 256, 69, 13, 5767, 296, 75, 6414, 822, 13, 75, 772, 279, 13, 3322, 1315, 295, 341, 50932, 50932, 4470, 2445, 13, 51028, 51028, 400, 538, 264, 636, 11, 286, 500, 380, 1009, 1604, 264, 5288, 295, 439, 264, 4470, 6828, 294, 37624, 11, 51282, 51282, 457, 286, 445, 360, 257, 1702, 3670, 3164, 2059, 281, 915, 264, 558, 1315, 11, 293, 550, 286, 5452, 300, 51474, 51474, 666, 452, 3089, 13, 51574, 51574, 10222, 22206, 264, 4470, 365, 3104, 281, 257, 2167, 3097, 1365, 11, 37624, 3255, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.09868904749552408, "compression_ratio": 1.7226277372262773, "no_speech_prob": 2.507012504793238e-05}, {"id": 84, "seek": 41416, "start": 432.52000000000004, "end": 436.36, "text": " but I just do a quick web search myself to find the right name, and then I plug that", "tokens": [50364, 295, 37624, 11, 309, 390, 767, 257, 3879, 4994, 1716, 490, 37624, 11, 457, 4728, 309, 50592, 50592, 658, 36427, 666, 37624, 11, 597, 307, 983, 321, 362, 256, 69, 13, 5767, 296, 75, 6414, 822, 13, 75, 772, 279, 13, 3322, 1315, 295, 341, 50932, 50932, 4470, 2445, 13, 51028, 51028, 400, 538, 264, 636, 11, 286, 500, 380, 1009, 1604, 264, 5288, 295, 439, 264, 4470, 6828, 294, 37624, 11, 51282, 51282, 457, 286, 445, 360, 257, 1702, 3670, 3164, 2059, 281, 915, 264, 558, 1315, 11, 293, 550, 286, 5452, 300, 51474, 51474, 666, 452, 3089, 13, 51574, 51574, 10222, 22206, 264, 4470, 365, 3104, 281, 257, 2167, 3097, 1365, 11, 37624, 3255, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.09868904749552408, "compression_ratio": 1.7226277372262773, "no_speech_prob": 2.507012504793238e-05}, {"id": 85, "seek": 41416, "start": 436.36, "end": 438.36, "text": " into my code.", "tokens": [50364, 295, 37624, 11, 309, 390, 767, 257, 3879, 4994, 1716, 490, 37624, 11, 457, 4728, 309, 50592, 50592, 658, 36427, 666, 37624, 11, 597, 307, 983, 321, 362, 256, 69, 13, 5767, 296, 75, 6414, 822, 13, 75, 772, 279, 13, 3322, 1315, 295, 341, 50932, 50932, 4470, 2445, 13, 51028, 51028, 400, 538, 264, 636, 11, 286, 500, 380, 1009, 1604, 264, 5288, 295, 439, 264, 4470, 6828, 294, 37624, 11, 51282, 51282, 457, 286, 445, 360, 257, 1702, 3670, 3164, 2059, 281, 915, 264, 558, 1315, 11, 293, 550, 286, 5452, 300, 51474, 51474, 666, 452, 3089, 13, 51574, 51574, 10222, 22206, 264, 4470, 365, 3104, 281, 257, 2167, 3097, 1365, 11, 37624, 3255, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.09868904749552408, "compression_ratio": 1.7226277372262773, "no_speech_prob": 2.507012504793238e-05}, {"id": 86, "seek": 41416, "start": 438.36, "end": 443.8, "text": " Having specified the loss with respect to a single training example, TensorFlow knows", "tokens": [50364, 295, 37624, 11, 309, 390, 767, 257, 3879, 4994, 1716, 490, 37624, 11, 457, 4728, 309, 50592, 50592, 658, 36427, 666, 37624, 11, 597, 307, 983, 321, 362, 256, 69, 13, 5767, 296, 75, 6414, 822, 13, 75, 772, 279, 13, 3322, 1315, 295, 341, 50932, 50932, 4470, 2445, 13, 51028, 51028, 400, 538, 264, 636, 11, 286, 500, 380, 1009, 1604, 264, 5288, 295, 439, 264, 4470, 6828, 294, 37624, 11, 51282, 51282, 457, 286, 445, 360, 257, 1702, 3670, 3164, 2059, 281, 915, 264, 558, 1315, 11, 293, 550, 286, 5452, 300, 51474, 51474, 666, 452, 3089, 13, 51574, 51574, 10222, 22206, 264, 4470, 365, 3104, 281, 257, 2167, 3097, 1365, 11, 37624, 3255, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.09868904749552408, "compression_ratio": 1.7226277372262773, "no_speech_prob": 2.507012504793238e-05}, {"id": 87, "seek": 44380, "start": 443.8, "end": 448.92, "text": " that the costs you want to minimize is then the average, taking the average over all m", "tokens": [50364, 300, 264, 5497, 291, 528, 281, 17522, 307, 550, 264, 4274, 11, 1940, 264, 4274, 670, 439, 275, 50620, 50620, 3097, 5110, 295, 264, 4470, 322, 439, 295, 264, 3097, 5110, 13, 50846, 50846, 400, 40425, 341, 2063, 2445, 486, 1874, 294, 15669, 264, 18161, 3209, 281, 428, 17434, 51170, 51170, 21538, 1412, 13, 51246, 51246, 682, 1389, 291, 528, 281, 5039, 257, 24590, 1154, 2831, 813, 257, 21538, 1154, 11, 291, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.11371931276823345, "compression_ratio": 1.7587939698492463, "no_speech_prob": 2.7263565698376624e-06}, {"id": 88, "seek": 44380, "start": 448.92, "end": 453.44, "text": " training examples of the loss on all of the training examples.", "tokens": [50364, 300, 264, 5497, 291, 528, 281, 17522, 307, 550, 264, 4274, 11, 1940, 264, 4274, 670, 439, 275, 50620, 50620, 3097, 5110, 295, 264, 4470, 322, 439, 295, 264, 3097, 5110, 13, 50846, 50846, 400, 40425, 341, 2063, 2445, 486, 1874, 294, 15669, 264, 18161, 3209, 281, 428, 17434, 51170, 51170, 21538, 1412, 13, 51246, 51246, 682, 1389, 291, 528, 281, 5039, 257, 24590, 1154, 2831, 813, 257, 21538, 1154, 11, 291, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.11371931276823345, "compression_ratio": 1.7587939698492463, "no_speech_prob": 2.7263565698376624e-06}, {"id": 89, "seek": 44380, "start": 453.44, "end": 459.92, "text": " And optimizing this cost function will result in fitting the neural network to your binary", "tokens": [50364, 300, 264, 5497, 291, 528, 281, 17522, 307, 550, 264, 4274, 11, 1940, 264, 4274, 670, 439, 275, 50620, 50620, 3097, 5110, 295, 264, 4470, 322, 439, 295, 264, 3097, 5110, 13, 50846, 50846, 400, 40425, 341, 2063, 2445, 486, 1874, 294, 15669, 264, 18161, 3209, 281, 428, 17434, 51170, 51170, 21538, 1412, 13, 51246, 51246, 682, 1389, 291, 528, 281, 5039, 257, 24590, 1154, 2831, 813, 257, 21538, 1154, 11, 291, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.11371931276823345, "compression_ratio": 1.7587939698492463, "no_speech_prob": 2.7263565698376624e-06}, {"id": 90, "seek": 44380, "start": 459.92, "end": 461.44, "text": " classification data.", "tokens": [50364, 300, 264, 5497, 291, 528, 281, 17522, 307, 550, 264, 4274, 11, 1940, 264, 4274, 670, 439, 275, 50620, 50620, 3097, 5110, 295, 264, 4470, 322, 439, 295, 264, 3097, 5110, 13, 50846, 50846, 400, 40425, 341, 2063, 2445, 486, 1874, 294, 15669, 264, 18161, 3209, 281, 428, 17434, 51170, 51170, 21538, 1412, 13, 51246, 51246, 682, 1389, 291, 528, 281, 5039, 257, 24590, 1154, 2831, 813, 257, 21538, 1154, 11, 291, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.11371931276823345, "compression_ratio": 1.7587939698492463, "no_speech_prob": 2.7263565698376624e-06}, {"id": 91, "seek": 44380, "start": 461.44, "end": 468.04, "text": " In case you want to solve a regression problem rather than a classification problem, you", "tokens": [50364, 300, 264, 5497, 291, 528, 281, 17522, 307, 550, 264, 4274, 11, 1940, 264, 4274, 670, 439, 275, 50620, 50620, 3097, 5110, 295, 264, 4470, 322, 439, 295, 264, 3097, 5110, 13, 50846, 50846, 400, 40425, 341, 2063, 2445, 486, 1874, 294, 15669, 264, 18161, 3209, 281, 428, 17434, 51170, 51170, 21538, 1412, 13, 51246, 51246, 682, 1389, 291, 528, 281, 5039, 257, 24590, 1154, 2831, 813, 257, 21538, 1154, 11, 291, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.11371931276823345, "compression_ratio": 1.7587939698492463, "no_speech_prob": 2.7263565698376624e-06}, {"id": 92, "seek": 46804, "start": 468.04, "end": 474.8, "text": " can also tell TensorFlow to compile your model using a different loss function.", "tokens": [50364, 393, 611, 980, 37624, 281, 31413, 428, 2316, 1228, 257, 819, 4470, 2445, 13, 50702, 50702, 1171, 1365, 11, 498, 291, 362, 257, 24590, 1154, 11, 293, 498, 291, 528, 281, 17522, 264, 8889, 6713, 51044, 51044, 4470, 11, 370, 510, 307, 264, 8889, 6713, 4470, 11, 264, 4470, 365, 3104, 281, 498, 291, 434, 2539, 9284, 51324, 51324, 23930, 283, 295, 2031, 365, 257, 3779, 420, 2727, 3494, 7645, 295, 288, 11, 300, 311, 472, 1922, 295, 264, 8889, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.1444853146870931, "compression_ratio": 1.6435185185185186, "no_speech_prob": 4.495119810599135e-06}, {"id": 93, "seek": 46804, "start": 474.8, "end": 481.64000000000004, "text": " For example, if you have a regression problem, and if you want to minimize the squared error", "tokens": [50364, 393, 611, 980, 37624, 281, 31413, 428, 2316, 1228, 257, 819, 4470, 2445, 13, 50702, 50702, 1171, 1365, 11, 498, 291, 362, 257, 24590, 1154, 11, 293, 498, 291, 528, 281, 17522, 264, 8889, 6713, 51044, 51044, 4470, 11, 370, 510, 307, 264, 8889, 6713, 4470, 11, 264, 4470, 365, 3104, 281, 498, 291, 434, 2539, 9284, 51324, 51324, 23930, 283, 295, 2031, 365, 257, 3779, 420, 2727, 3494, 7645, 295, 288, 11, 300, 311, 472, 1922, 295, 264, 8889, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.1444853146870931, "compression_ratio": 1.6435185185185186, "no_speech_prob": 4.495119810599135e-06}, {"id": 94, "seek": 46804, "start": 481.64000000000004, "end": 487.24, "text": " loss, so here is the squared error loss, the loss with respect to if you're learning algorithm", "tokens": [50364, 393, 611, 980, 37624, 281, 31413, 428, 2316, 1228, 257, 819, 4470, 2445, 13, 50702, 50702, 1171, 1365, 11, 498, 291, 362, 257, 24590, 1154, 11, 293, 498, 291, 528, 281, 17522, 264, 8889, 6713, 51044, 51044, 4470, 11, 370, 510, 307, 264, 8889, 6713, 4470, 11, 264, 4470, 365, 3104, 281, 498, 291, 434, 2539, 9284, 51324, 51324, 23930, 283, 295, 2031, 365, 257, 3779, 420, 2727, 3494, 7645, 295, 288, 11, 300, 311, 472, 1922, 295, 264, 8889, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.1444853146870931, "compression_ratio": 1.6435185185185186, "no_speech_prob": 4.495119810599135e-06}, {"id": 95, "seek": 46804, "start": 487.24, "end": 492.84000000000003, "text": " outputs f of x with a target or ground truth label of y, that's one half of the squared", "tokens": [50364, 393, 611, 980, 37624, 281, 31413, 428, 2316, 1228, 257, 819, 4470, 2445, 13, 50702, 50702, 1171, 1365, 11, 498, 291, 362, 257, 24590, 1154, 11, 293, 498, 291, 528, 281, 17522, 264, 8889, 6713, 51044, 51044, 4470, 11, 370, 510, 307, 264, 8889, 6713, 4470, 11, 264, 4470, 365, 3104, 281, 498, 291, 434, 2539, 9284, 51324, 51324, 23930, 283, 295, 2031, 365, 257, 3779, 420, 2727, 3494, 7645, 295, 288, 11, 300, 311, 472, 1922, 295, 264, 8889, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.1444853146870931, "compression_ratio": 1.6435185185185186, "no_speech_prob": 4.495119810599135e-06}, {"id": 96, "seek": 49284, "start": 492.84, "end": 500.56, "text": " error, then you can use this loss function in TensorFlow, which is to use the maybe more", "tokens": [50364, 6713, 11, 550, 291, 393, 764, 341, 4470, 2445, 294, 37624, 11, 597, 307, 281, 764, 264, 1310, 544, 50750, 50750, 46506, 4926, 914, 8889, 6713, 4470, 2445, 11, 293, 550, 37624, 486, 853, 281, 50989, 50989, 17522, 264, 914, 8889, 6713, 13, 51098, 51098, 682, 341, 6114, 11, 286, 478, 1228, 508, 295, 4238, 343, 22117, 4238, 363, 281, 45708, 264, 2063, 2445, 13, 51458, 51458, 440, 2063, 2445, 307, 257, 2445, 295, 439, 295, 264, 9834, 294, 264, 18161, 3209, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1031713814570986, "compression_ratio": 1.7361111111111112, "no_speech_prob": 3.5008254144486273e-06}, {"id": 97, "seek": 49284, "start": 500.56, "end": 505.34, "text": " intuitively named mean squared error loss function, and then TensorFlow will try to", "tokens": [50364, 6713, 11, 550, 291, 393, 764, 341, 4470, 2445, 294, 37624, 11, 597, 307, 281, 764, 264, 1310, 544, 50750, 50750, 46506, 4926, 914, 8889, 6713, 4470, 2445, 11, 293, 550, 37624, 486, 853, 281, 50989, 50989, 17522, 264, 914, 8889, 6713, 13, 51098, 51098, 682, 341, 6114, 11, 286, 478, 1228, 508, 295, 4238, 343, 22117, 4238, 363, 281, 45708, 264, 2063, 2445, 13, 51458, 51458, 440, 2063, 2445, 307, 257, 2445, 295, 439, 295, 264, 9834, 294, 264, 18161, 3209, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1031713814570986, "compression_ratio": 1.7361111111111112, "no_speech_prob": 3.5008254144486273e-06}, {"id": 98, "seek": 49284, "start": 505.34, "end": 507.52, "text": " minimize the mean squared error.", "tokens": [50364, 6713, 11, 550, 291, 393, 764, 341, 4470, 2445, 294, 37624, 11, 597, 307, 281, 764, 264, 1310, 544, 50750, 50750, 46506, 4926, 914, 8889, 6713, 4470, 2445, 11, 293, 550, 37624, 486, 853, 281, 50989, 50989, 17522, 264, 914, 8889, 6713, 13, 51098, 51098, 682, 341, 6114, 11, 286, 478, 1228, 508, 295, 4238, 343, 22117, 4238, 363, 281, 45708, 264, 2063, 2445, 13, 51458, 51458, 440, 2063, 2445, 307, 257, 2445, 295, 439, 295, 264, 9834, 294, 264, 18161, 3209, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1031713814570986, "compression_ratio": 1.7361111111111112, "no_speech_prob": 3.5008254144486273e-06}, {"id": 99, "seek": 49284, "start": 507.52, "end": 514.72, "text": " In this expression, I'm using J of capital W comma capital B to denote the cost function.", "tokens": [50364, 6713, 11, 550, 291, 393, 764, 341, 4470, 2445, 294, 37624, 11, 597, 307, 281, 764, 264, 1310, 544, 50750, 50750, 46506, 4926, 914, 8889, 6713, 4470, 2445, 11, 293, 550, 37624, 486, 853, 281, 50989, 50989, 17522, 264, 914, 8889, 6713, 13, 51098, 51098, 682, 341, 6114, 11, 286, 478, 1228, 508, 295, 4238, 343, 22117, 4238, 363, 281, 45708, 264, 2063, 2445, 13, 51458, 51458, 440, 2063, 2445, 307, 257, 2445, 295, 439, 295, 264, 9834, 294, 264, 18161, 3209, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1031713814570986, "compression_ratio": 1.7361111111111112, "no_speech_prob": 3.5008254144486273e-06}, {"id": 100, "seek": 49284, "start": 514.72, "end": 520.04, "text": " The cost function is a function of all of the parameters in the neural network.", "tokens": [50364, 6713, 11, 550, 291, 393, 764, 341, 4470, 2445, 294, 37624, 11, 597, 307, 281, 764, 264, 1310, 544, 50750, 50750, 46506, 4926, 914, 8889, 6713, 4470, 2445, 11, 293, 550, 37624, 486, 853, 281, 50989, 50989, 17522, 264, 914, 8889, 6713, 13, 51098, 51098, 682, 341, 6114, 11, 286, 478, 1228, 508, 295, 4238, 343, 22117, 4238, 363, 281, 45708, 264, 2063, 2445, 13, 51458, 51458, 440, 2063, 2445, 307, 257, 2445, 295, 439, 295, 264, 9834, 294, 264, 18161, 3209, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1031713814570986, "compression_ratio": 1.7361111111111112, "no_speech_prob": 3.5008254144486273e-06}, {"id": 101, "seek": 52004, "start": 520.04, "end": 529.16, "text": " So you can think of capital W as including W1, W2, W3, so all the W parameters in the", "tokens": [50364, 407, 291, 393, 519, 295, 4238, 343, 382, 3009, 343, 16, 11, 343, 17, 11, 343, 18, 11, 370, 439, 264, 343, 9834, 294, 264, 50820, 50820, 2302, 18161, 3209, 293, 363, 382, 3009, 363, 16, 11, 363, 17, 11, 293, 363, 18, 13, 51206, 51206, 407, 498, 291, 366, 40425, 264, 2063, 2445, 365, 3104, 281, 343, 293, 363, 11, 291, 1116, 312, 1382, 281, 51542, 51542, 19719, 309, 365, 3104, 281, 439, 295, 264, 9834, 294, 264, 18161, 3209, 13, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09382448639980583, "compression_ratio": 1.696629213483146, "no_speech_prob": 1.7603337028049282e-06}, {"id": 102, "seek": 52004, "start": 529.16, "end": 536.88, "text": " entire neural network and B as including B1, B2, and B3.", "tokens": [50364, 407, 291, 393, 519, 295, 4238, 343, 382, 3009, 343, 16, 11, 343, 17, 11, 343, 18, 11, 370, 439, 264, 343, 9834, 294, 264, 50820, 50820, 2302, 18161, 3209, 293, 363, 382, 3009, 363, 16, 11, 363, 17, 11, 293, 363, 18, 13, 51206, 51206, 407, 498, 291, 366, 40425, 264, 2063, 2445, 365, 3104, 281, 343, 293, 363, 11, 291, 1116, 312, 1382, 281, 51542, 51542, 19719, 309, 365, 3104, 281, 439, 295, 264, 9834, 294, 264, 18161, 3209, 13, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09382448639980583, "compression_ratio": 1.696629213483146, "no_speech_prob": 1.7603337028049282e-06}, {"id": 103, "seek": 52004, "start": 536.88, "end": 543.5999999999999, "text": " So if you are optimizing the cost function with respect to W and B, you'd be trying to", "tokens": [50364, 407, 291, 393, 519, 295, 4238, 343, 382, 3009, 343, 16, 11, 343, 17, 11, 343, 18, 11, 370, 439, 264, 343, 9834, 294, 264, 50820, 50820, 2302, 18161, 3209, 293, 363, 382, 3009, 363, 16, 11, 363, 17, 11, 293, 363, 18, 13, 51206, 51206, 407, 498, 291, 366, 40425, 264, 2063, 2445, 365, 3104, 281, 343, 293, 363, 11, 291, 1116, 312, 1382, 281, 51542, 51542, 19719, 309, 365, 3104, 281, 439, 295, 264, 9834, 294, 264, 18161, 3209, 13, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09382448639980583, "compression_ratio": 1.696629213483146, "no_speech_prob": 1.7603337028049282e-06}, {"id": 104, "seek": 52004, "start": 543.5999999999999, "end": 548.36, "text": " optimize it with respect to all of the parameters in the neural network.", "tokens": [50364, 407, 291, 393, 519, 295, 4238, 343, 382, 3009, 343, 16, 11, 343, 17, 11, 343, 18, 11, 370, 439, 264, 343, 9834, 294, 264, 50820, 50820, 2302, 18161, 3209, 293, 363, 382, 3009, 363, 16, 11, 363, 17, 11, 293, 363, 18, 13, 51206, 51206, 407, 498, 291, 366, 40425, 264, 2063, 2445, 365, 3104, 281, 343, 293, 363, 11, 291, 1116, 312, 1382, 281, 51542, 51542, 19719, 309, 365, 3104, 281, 439, 295, 264, 9834, 294, 264, 18161, 3209, 13, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09382448639980583, "compression_ratio": 1.696629213483146, "no_speech_prob": 1.7603337028049282e-06}, {"id": 105, "seek": 54836, "start": 548.36, "end": 554.2, "text": " And up on top as well, I had written f of x as the output of the neural network, but", "tokens": [50364, 400, 493, 322, 1192, 382, 731, 11, 286, 632, 3720, 283, 295, 2031, 382, 264, 5598, 295, 264, 18161, 3209, 11, 457, 50656, 50656, 291, 528, 11, 291, 393, 611, 2464, 283, 295, 343, 33, 498, 321, 528, 281, 16078, 300, 264, 5598, 295, 264, 18161, 50938, 50938, 3209, 382, 257, 2445, 295, 2031, 5946, 322, 439, 264, 9834, 294, 439, 264, 7914, 295, 264, 18161, 51168, 51168, 3209, 13, 51228, 51228, 407, 300, 311, 264, 4470, 2445, 293, 264, 2063, 2445, 13, 51370, 51370, 400, 294, 37624, 11, 341, 307, 1219, 264, 17434, 3278, 30867, 4470, 2445, 13, 51696, 51696, 2305, 775, 300, 1315, 808, 490, 30, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.1667849166052682, "compression_ratio": 1.8220338983050848, "no_speech_prob": 2.5215263121936005e-06}, {"id": 106, "seek": 54836, "start": 554.2, "end": 559.84, "text": " you want, you can also write f of WB if we want to emphasize that the output of the neural", "tokens": [50364, 400, 493, 322, 1192, 382, 731, 11, 286, 632, 3720, 283, 295, 2031, 382, 264, 5598, 295, 264, 18161, 3209, 11, 457, 50656, 50656, 291, 528, 11, 291, 393, 611, 2464, 283, 295, 343, 33, 498, 321, 528, 281, 16078, 300, 264, 5598, 295, 264, 18161, 50938, 50938, 3209, 382, 257, 2445, 295, 2031, 5946, 322, 439, 264, 9834, 294, 439, 264, 7914, 295, 264, 18161, 51168, 51168, 3209, 13, 51228, 51228, 407, 300, 311, 264, 4470, 2445, 293, 264, 2063, 2445, 13, 51370, 51370, 400, 294, 37624, 11, 341, 307, 1219, 264, 17434, 3278, 30867, 4470, 2445, 13, 51696, 51696, 2305, 775, 300, 1315, 808, 490, 30, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.1667849166052682, "compression_ratio": 1.8220338983050848, "no_speech_prob": 2.5215263121936005e-06}, {"id": 107, "seek": 54836, "start": 559.84, "end": 564.44, "text": " network as a function of x depends on all the parameters in all the layers of the neural", "tokens": [50364, 400, 493, 322, 1192, 382, 731, 11, 286, 632, 3720, 283, 295, 2031, 382, 264, 5598, 295, 264, 18161, 3209, 11, 457, 50656, 50656, 291, 528, 11, 291, 393, 611, 2464, 283, 295, 343, 33, 498, 321, 528, 281, 16078, 300, 264, 5598, 295, 264, 18161, 50938, 50938, 3209, 382, 257, 2445, 295, 2031, 5946, 322, 439, 264, 9834, 294, 439, 264, 7914, 295, 264, 18161, 51168, 51168, 3209, 13, 51228, 51228, 407, 300, 311, 264, 4470, 2445, 293, 264, 2063, 2445, 13, 51370, 51370, 400, 294, 37624, 11, 341, 307, 1219, 264, 17434, 3278, 30867, 4470, 2445, 13, 51696, 51696, 2305, 775, 300, 1315, 808, 490, 30, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.1667849166052682, "compression_ratio": 1.8220338983050848, "no_speech_prob": 2.5215263121936005e-06}, {"id": 108, "seek": 54836, "start": 564.44, "end": 565.64, "text": " network.", "tokens": [50364, 400, 493, 322, 1192, 382, 731, 11, 286, 632, 3720, 283, 295, 2031, 382, 264, 5598, 295, 264, 18161, 3209, 11, 457, 50656, 50656, 291, 528, 11, 291, 393, 611, 2464, 283, 295, 343, 33, 498, 321, 528, 281, 16078, 300, 264, 5598, 295, 264, 18161, 50938, 50938, 3209, 382, 257, 2445, 295, 2031, 5946, 322, 439, 264, 9834, 294, 439, 264, 7914, 295, 264, 18161, 51168, 51168, 3209, 13, 51228, 51228, 407, 300, 311, 264, 4470, 2445, 293, 264, 2063, 2445, 13, 51370, 51370, 400, 294, 37624, 11, 341, 307, 1219, 264, 17434, 3278, 30867, 4470, 2445, 13, 51696, 51696, 2305, 775, 300, 1315, 808, 490, 30, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.1667849166052682, "compression_ratio": 1.8220338983050848, "no_speech_prob": 2.5215263121936005e-06}, {"id": 109, "seek": 54836, "start": 565.64, "end": 568.48, "text": " So that's the loss function and the cost function.", "tokens": [50364, 400, 493, 322, 1192, 382, 731, 11, 286, 632, 3720, 283, 295, 2031, 382, 264, 5598, 295, 264, 18161, 3209, 11, 457, 50656, 50656, 291, 528, 11, 291, 393, 611, 2464, 283, 295, 343, 33, 498, 321, 528, 281, 16078, 300, 264, 5598, 295, 264, 18161, 50938, 50938, 3209, 382, 257, 2445, 295, 2031, 5946, 322, 439, 264, 9834, 294, 439, 264, 7914, 295, 264, 18161, 51168, 51168, 3209, 13, 51228, 51228, 407, 300, 311, 264, 4470, 2445, 293, 264, 2063, 2445, 13, 51370, 51370, 400, 294, 37624, 11, 341, 307, 1219, 264, 17434, 3278, 30867, 4470, 2445, 13, 51696, 51696, 2305, 775, 300, 1315, 808, 490, 30, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.1667849166052682, "compression_ratio": 1.8220338983050848, "no_speech_prob": 2.5215263121936005e-06}, {"id": 110, "seek": 54836, "start": 568.48, "end": 575.0, "text": " And in TensorFlow, this is called the binary cross entropy loss function.", "tokens": [50364, 400, 493, 322, 1192, 382, 731, 11, 286, 632, 3720, 283, 295, 2031, 382, 264, 5598, 295, 264, 18161, 3209, 11, 457, 50656, 50656, 291, 528, 11, 291, 393, 611, 2464, 283, 295, 343, 33, 498, 321, 528, 281, 16078, 300, 264, 5598, 295, 264, 18161, 50938, 50938, 3209, 382, 257, 2445, 295, 2031, 5946, 322, 439, 264, 9834, 294, 439, 264, 7914, 295, 264, 18161, 51168, 51168, 3209, 13, 51228, 51228, 407, 300, 311, 264, 4470, 2445, 293, 264, 2063, 2445, 13, 51370, 51370, 400, 294, 37624, 11, 341, 307, 1219, 264, 17434, 3278, 30867, 4470, 2445, 13, 51696, 51696, 2305, 775, 300, 1315, 808, 490, 30, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.1667849166052682, "compression_ratio": 1.8220338983050848, "no_speech_prob": 2.5215263121936005e-06}, {"id": 111, "seek": 54836, "start": 575.0, "end": 576.2, "text": " Where does that name come from?", "tokens": [50364, 400, 493, 322, 1192, 382, 731, 11, 286, 632, 3720, 283, 295, 2031, 382, 264, 5598, 295, 264, 18161, 3209, 11, 457, 50656, 50656, 291, 528, 11, 291, 393, 611, 2464, 283, 295, 343, 33, 498, 321, 528, 281, 16078, 300, 264, 5598, 295, 264, 18161, 50938, 50938, 3209, 382, 257, 2445, 295, 2031, 5946, 322, 439, 264, 9834, 294, 439, 264, 7914, 295, 264, 18161, 51168, 51168, 3209, 13, 51228, 51228, 407, 300, 311, 264, 4470, 2445, 293, 264, 2063, 2445, 13, 51370, 51370, 400, 294, 37624, 11, 341, 307, 1219, 264, 17434, 3278, 30867, 4470, 2445, 13, 51696, 51696, 2305, 775, 300, 1315, 808, 490, 30, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.1667849166052682, "compression_ratio": 1.8220338983050848, "no_speech_prob": 2.5215263121936005e-06}, {"id": 112, "seek": 57620, "start": 576.2, "end": 582.12, "text": " Well, it turns out in statistics, this function on top is called the cross entropy loss function.", "tokens": [50364, 1042, 11, 309, 4523, 484, 294, 12523, 11, 341, 2445, 322, 1192, 307, 1219, 264, 3278, 30867, 4470, 2445, 13, 50660, 50660, 407, 300, 311, 437, 3278, 30867, 1355, 13, 50756, 50756, 400, 264, 1349, 17434, 445, 319, 12, 443, 7485, 5660, 420, 2793, 484, 300, 341, 307, 257, 17434, 21538, 51033, 51033, 1154, 570, 1184, 3256, 307, 2139, 257, 4018, 420, 257, 472, 13, 51296, 51296, 6288, 11, 291, 486, 1029, 37624, 281, 17522, 264, 2063, 2445, 13, 51547, 51547, 509, 1062, 1604, 264, 16235, 23475, 9284, 490, 264, 700, 1164, 13, 51813, 51813], "temperature": 0.0, "avg_logprob": -0.131111282663247, "compression_ratio": 1.6171875, "no_speech_prob": 9.276292303184164e-07}, {"id": 113, "seek": 57620, "start": 582.12, "end": 584.0400000000001, "text": " So that's what cross entropy means.", "tokens": [50364, 1042, 11, 309, 4523, 484, 294, 12523, 11, 341, 2445, 322, 1192, 307, 1219, 264, 3278, 30867, 4470, 2445, 13, 50660, 50660, 407, 300, 311, 437, 3278, 30867, 1355, 13, 50756, 50756, 400, 264, 1349, 17434, 445, 319, 12, 443, 7485, 5660, 420, 2793, 484, 300, 341, 307, 257, 17434, 21538, 51033, 51033, 1154, 570, 1184, 3256, 307, 2139, 257, 4018, 420, 257, 472, 13, 51296, 51296, 6288, 11, 291, 486, 1029, 37624, 281, 17522, 264, 2063, 2445, 13, 51547, 51547, 509, 1062, 1604, 264, 16235, 23475, 9284, 490, 264, 700, 1164, 13, 51813, 51813], "temperature": 0.0, "avg_logprob": -0.131111282663247, "compression_ratio": 1.6171875, "no_speech_prob": 9.276292303184164e-07}, {"id": 114, "seek": 57620, "start": 584.0400000000001, "end": 589.58, "text": " And the word binary just re-emphasizes or points out that this is a binary classification", "tokens": [50364, 1042, 11, 309, 4523, 484, 294, 12523, 11, 341, 2445, 322, 1192, 307, 1219, 264, 3278, 30867, 4470, 2445, 13, 50660, 50660, 407, 300, 311, 437, 3278, 30867, 1355, 13, 50756, 50756, 400, 264, 1349, 17434, 445, 319, 12, 443, 7485, 5660, 420, 2793, 484, 300, 341, 307, 257, 17434, 21538, 51033, 51033, 1154, 570, 1184, 3256, 307, 2139, 257, 4018, 420, 257, 472, 13, 51296, 51296, 6288, 11, 291, 486, 1029, 37624, 281, 17522, 264, 2063, 2445, 13, 51547, 51547, 509, 1062, 1604, 264, 16235, 23475, 9284, 490, 264, 700, 1164, 13, 51813, 51813], "temperature": 0.0, "avg_logprob": -0.131111282663247, "compression_ratio": 1.6171875, "no_speech_prob": 9.276292303184164e-07}, {"id": 115, "seek": 57620, "start": 589.58, "end": 594.84, "text": " problem because each image is either a zero or a one.", "tokens": [50364, 1042, 11, 309, 4523, 484, 294, 12523, 11, 341, 2445, 322, 1192, 307, 1219, 264, 3278, 30867, 4470, 2445, 13, 50660, 50660, 407, 300, 311, 437, 3278, 30867, 1355, 13, 50756, 50756, 400, 264, 1349, 17434, 445, 319, 12, 443, 7485, 5660, 420, 2793, 484, 300, 341, 307, 257, 17434, 21538, 51033, 51033, 1154, 570, 1184, 3256, 307, 2139, 257, 4018, 420, 257, 472, 13, 51296, 51296, 6288, 11, 291, 486, 1029, 37624, 281, 17522, 264, 2063, 2445, 13, 51547, 51547, 509, 1062, 1604, 264, 16235, 23475, 9284, 490, 264, 700, 1164, 13, 51813, 51813], "temperature": 0.0, "avg_logprob": -0.131111282663247, "compression_ratio": 1.6171875, "no_speech_prob": 9.276292303184164e-07}, {"id": 116, "seek": 57620, "start": 594.84, "end": 599.86, "text": " Finally, you will ask TensorFlow to minimize the cost function.", "tokens": [50364, 1042, 11, 309, 4523, 484, 294, 12523, 11, 341, 2445, 322, 1192, 307, 1219, 264, 3278, 30867, 4470, 2445, 13, 50660, 50660, 407, 300, 311, 437, 3278, 30867, 1355, 13, 50756, 50756, 400, 264, 1349, 17434, 445, 319, 12, 443, 7485, 5660, 420, 2793, 484, 300, 341, 307, 257, 17434, 21538, 51033, 51033, 1154, 570, 1184, 3256, 307, 2139, 257, 4018, 420, 257, 472, 13, 51296, 51296, 6288, 11, 291, 486, 1029, 37624, 281, 17522, 264, 2063, 2445, 13, 51547, 51547, 509, 1062, 1604, 264, 16235, 23475, 9284, 490, 264, 700, 1164, 13, 51813, 51813], "temperature": 0.0, "avg_logprob": -0.131111282663247, "compression_ratio": 1.6171875, "no_speech_prob": 9.276292303184164e-07}, {"id": 117, "seek": 57620, "start": 599.86, "end": 605.1800000000001, "text": " You might remember the gradient descent algorithm from the first course.", "tokens": [50364, 1042, 11, 309, 4523, 484, 294, 12523, 11, 341, 2445, 322, 1192, 307, 1219, 264, 3278, 30867, 4470, 2445, 13, 50660, 50660, 407, 300, 311, 437, 3278, 30867, 1355, 13, 50756, 50756, 400, 264, 1349, 17434, 445, 319, 12, 443, 7485, 5660, 420, 2793, 484, 300, 341, 307, 257, 17434, 21538, 51033, 51033, 1154, 570, 1184, 3256, 307, 2139, 257, 4018, 420, 257, 472, 13, 51296, 51296, 6288, 11, 291, 486, 1029, 37624, 281, 17522, 264, 2063, 2445, 13, 51547, 51547, 509, 1062, 1604, 264, 16235, 23475, 9284, 490, 264, 700, 1164, 13, 51813, 51813], "temperature": 0.0, "avg_logprob": -0.131111282663247, "compression_ratio": 1.6171875, "no_speech_prob": 9.276292303184164e-07}, {"id": 118, "seek": 60518, "start": 605.18, "end": 609.56, "text": " If you are using gradient descent to train the parameters of a neural network, then you", "tokens": [50364, 759, 291, 366, 1228, 16235, 23475, 281, 3847, 264, 9834, 295, 257, 18161, 3209, 11, 550, 291, 50583, 50583, 486, 18227, 337, 633, 4583, 441, 293, 337, 633, 4985, 508, 11, 5623, 343, 43, 41, 4650, 281, 343, 43, 41, 51061, 51061, 3175, 264, 2539, 3314, 8961, 1413, 264, 14641, 13760, 365, 3104, 281, 300, 13075, 51427, 51427, 295, 264, 2063, 2445, 508, 295, 343, 33, 13, 51599, 51599, 400, 14138, 337, 264, 9834, 363, 382, 731, 13, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.16112026354161704, "compression_ratio": 1.6028708133971292, "no_speech_prob": 8.26759060146287e-06}, {"id": 119, "seek": 60518, "start": 609.56, "end": 619.12, "text": " will repeatedly for every layer L and for every unit J, update WLJ according to WLJ", "tokens": [50364, 759, 291, 366, 1228, 16235, 23475, 281, 3847, 264, 9834, 295, 257, 18161, 3209, 11, 550, 291, 50583, 50583, 486, 18227, 337, 633, 4583, 441, 293, 337, 633, 4985, 508, 11, 5623, 343, 43, 41, 4650, 281, 343, 43, 41, 51061, 51061, 3175, 264, 2539, 3314, 8961, 1413, 264, 14641, 13760, 365, 3104, 281, 300, 13075, 51427, 51427, 295, 264, 2063, 2445, 508, 295, 343, 33, 13, 51599, 51599, 400, 14138, 337, 264, 9834, 363, 382, 731, 13, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.16112026354161704, "compression_ratio": 1.6028708133971292, "no_speech_prob": 8.26759060146287e-06}, {"id": 120, "seek": 60518, "start": 619.12, "end": 626.4399999999999, "text": " minus the learning rate alpha times the partial derivative with respect to that parameter", "tokens": [50364, 759, 291, 366, 1228, 16235, 23475, 281, 3847, 264, 9834, 295, 257, 18161, 3209, 11, 550, 291, 50583, 50583, 486, 18227, 337, 633, 4583, 441, 293, 337, 633, 4985, 508, 11, 5623, 343, 43, 41, 4650, 281, 343, 43, 41, 51061, 51061, 3175, 264, 2539, 3314, 8961, 1413, 264, 14641, 13760, 365, 3104, 281, 300, 13075, 51427, 51427, 295, 264, 2063, 2445, 508, 295, 343, 33, 13, 51599, 51599, 400, 14138, 337, 264, 9834, 363, 382, 731, 13, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.16112026354161704, "compression_ratio": 1.6028708133971292, "no_speech_prob": 8.26759060146287e-06}, {"id": 121, "seek": 60518, "start": 626.4399999999999, "end": 629.88, "text": " of the cost function J of WB.", "tokens": [50364, 759, 291, 366, 1228, 16235, 23475, 281, 3847, 264, 9834, 295, 257, 18161, 3209, 11, 550, 291, 50583, 50583, 486, 18227, 337, 633, 4583, 441, 293, 337, 633, 4985, 508, 11, 5623, 343, 43, 41, 4650, 281, 343, 43, 41, 51061, 51061, 3175, 264, 2539, 3314, 8961, 1413, 264, 14641, 13760, 365, 3104, 281, 300, 13075, 51427, 51427, 295, 264, 2063, 2445, 508, 295, 343, 33, 13, 51599, 51599, 400, 14138, 337, 264, 9834, 363, 382, 731, 13, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.16112026354161704, "compression_ratio": 1.6028708133971292, "no_speech_prob": 8.26759060146287e-06}, {"id": 122, "seek": 60518, "start": 629.88, "end": 634.38, "text": " And similarly for the parameters B as well.", "tokens": [50364, 759, 291, 366, 1228, 16235, 23475, 281, 3847, 264, 9834, 295, 257, 18161, 3209, 11, 550, 291, 50583, 50583, 486, 18227, 337, 633, 4583, 441, 293, 337, 633, 4985, 508, 11, 5623, 343, 43, 41, 4650, 281, 343, 43, 41, 51061, 51061, 3175, 264, 2539, 3314, 8961, 1413, 264, 14641, 13760, 365, 3104, 281, 300, 13075, 51427, 51427, 295, 264, 2063, 2445, 508, 295, 343, 33, 13, 51599, 51599, 400, 14138, 337, 264, 9834, 363, 382, 731, 13, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.16112026354161704, "compression_ratio": 1.6028708133971292, "no_speech_prob": 8.26759060146287e-06}, {"id": 123, "seek": 63438, "start": 634.38, "end": 641.16, "text": " And after doing, say, 100 iterations of gradient descent, hopefully you get to a good value", "tokens": [50364, 400, 934, 884, 11, 584, 11, 2319, 36540, 295, 16235, 23475, 11, 4696, 291, 483, 281, 257, 665, 2158, 50703, 50703, 295, 264, 9834, 13, 50807, 50807, 407, 294, 1668, 281, 764, 16235, 23475, 11, 264, 2141, 551, 291, 643, 281, 14722, 307, 613, 14641, 51101, 51101, 13760, 2115, 13, 51215, 51215, 400, 437, 37624, 775, 11, 293, 294, 1186, 437, 307, 3832, 294, 18161, 3209, 3097, 11, 307, 51461, 51461, 281, 764, 364, 9284, 1219, 646, 79, 1513, 559, 399, 294, 1668, 281, 14722, 613, 14641, 13760, 51767, 51767, 2115, 13, 51817, 51817], "temperature": 0.0, "avg_logprob": -0.1462738811969757, "compression_ratio": 1.7186147186147187, "no_speech_prob": 2.090399902954232e-06}, {"id": 124, "seek": 63438, "start": 641.16, "end": 643.24, "text": " of the parameters.", "tokens": [50364, 400, 934, 884, 11, 584, 11, 2319, 36540, 295, 16235, 23475, 11, 4696, 291, 483, 281, 257, 665, 2158, 50703, 50703, 295, 264, 9834, 13, 50807, 50807, 407, 294, 1668, 281, 764, 16235, 23475, 11, 264, 2141, 551, 291, 643, 281, 14722, 307, 613, 14641, 51101, 51101, 13760, 2115, 13, 51215, 51215, 400, 437, 37624, 775, 11, 293, 294, 1186, 437, 307, 3832, 294, 18161, 3209, 3097, 11, 307, 51461, 51461, 281, 764, 364, 9284, 1219, 646, 79, 1513, 559, 399, 294, 1668, 281, 14722, 613, 14641, 13760, 51767, 51767, 2115, 13, 51817, 51817], "temperature": 0.0, "avg_logprob": -0.1462738811969757, "compression_ratio": 1.7186147186147187, "no_speech_prob": 2.090399902954232e-06}, {"id": 125, "seek": 63438, "start": 643.24, "end": 649.12, "text": " So in order to use gradient descent, the key thing you need to compute is these partial", "tokens": [50364, 400, 934, 884, 11, 584, 11, 2319, 36540, 295, 16235, 23475, 11, 4696, 291, 483, 281, 257, 665, 2158, 50703, 50703, 295, 264, 9834, 13, 50807, 50807, 407, 294, 1668, 281, 764, 16235, 23475, 11, 264, 2141, 551, 291, 643, 281, 14722, 307, 613, 14641, 51101, 51101, 13760, 2115, 13, 51215, 51215, 400, 437, 37624, 775, 11, 293, 294, 1186, 437, 307, 3832, 294, 18161, 3209, 3097, 11, 307, 51461, 51461, 281, 764, 364, 9284, 1219, 646, 79, 1513, 559, 399, 294, 1668, 281, 14722, 613, 14641, 13760, 51767, 51767, 2115, 13, 51817, 51817], "temperature": 0.0, "avg_logprob": -0.1462738811969757, "compression_ratio": 1.7186147186147187, "no_speech_prob": 2.090399902954232e-06}, {"id": 126, "seek": 63438, "start": 649.12, "end": 651.4, "text": " derivative terms.", "tokens": [50364, 400, 934, 884, 11, 584, 11, 2319, 36540, 295, 16235, 23475, 11, 4696, 291, 483, 281, 257, 665, 2158, 50703, 50703, 295, 264, 9834, 13, 50807, 50807, 407, 294, 1668, 281, 764, 16235, 23475, 11, 264, 2141, 551, 291, 643, 281, 14722, 307, 613, 14641, 51101, 51101, 13760, 2115, 13, 51215, 51215, 400, 437, 37624, 775, 11, 293, 294, 1186, 437, 307, 3832, 294, 18161, 3209, 3097, 11, 307, 51461, 51461, 281, 764, 364, 9284, 1219, 646, 79, 1513, 559, 399, 294, 1668, 281, 14722, 613, 14641, 13760, 51767, 51767, 2115, 13, 51817, 51817], "temperature": 0.0, "avg_logprob": -0.1462738811969757, "compression_ratio": 1.7186147186147187, "no_speech_prob": 2.090399902954232e-06}, {"id": 127, "seek": 63438, "start": 651.4, "end": 656.32, "text": " And what TensorFlow does, and in fact what is standard in neural network training, is", "tokens": [50364, 400, 934, 884, 11, 584, 11, 2319, 36540, 295, 16235, 23475, 11, 4696, 291, 483, 281, 257, 665, 2158, 50703, 50703, 295, 264, 9834, 13, 50807, 50807, 407, 294, 1668, 281, 764, 16235, 23475, 11, 264, 2141, 551, 291, 643, 281, 14722, 307, 613, 14641, 51101, 51101, 13760, 2115, 13, 51215, 51215, 400, 437, 37624, 775, 11, 293, 294, 1186, 437, 307, 3832, 294, 18161, 3209, 3097, 11, 307, 51461, 51461, 281, 764, 364, 9284, 1219, 646, 79, 1513, 559, 399, 294, 1668, 281, 14722, 613, 14641, 13760, 51767, 51767, 2115, 13, 51817, 51817], "temperature": 0.0, "avg_logprob": -0.1462738811969757, "compression_ratio": 1.7186147186147187, "no_speech_prob": 2.090399902954232e-06}, {"id": 128, "seek": 63438, "start": 656.32, "end": 662.44, "text": " to use an algorithm called backpropagation in order to compute these partial derivative", "tokens": [50364, 400, 934, 884, 11, 584, 11, 2319, 36540, 295, 16235, 23475, 11, 4696, 291, 483, 281, 257, 665, 2158, 50703, 50703, 295, 264, 9834, 13, 50807, 50807, 407, 294, 1668, 281, 764, 16235, 23475, 11, 264, 2141, 551, 291, 643, 281, 14722, 307, 613, 14641, 51101, 51101, 13760, 2115, 13, 51215, 51215, 400, 437, 37624, 775, 11, 293, 294, 1186, 437, 307, 3832, 294, 18161, 3209, 3097, 11, 307, 51461, 51461, 281, 764, 364, 9284, 1219, 646, 79, 1513, 559, 399, 294, 1668, 281, 14722, 613, 14641, 13760, 51767, 51767, 2115, 13, 51817, 51817], "temperature": 0.0, "avg_logprob": -0.1462738811969757, "compression_ratio": 1.7186147186147187, "no_speech_prob": 2.090399902954232e-06}, {"id": 129, "seek": 63438, "start": 662.44, "end": 663.44, "text": " terms.", "tokens": [50364, 400, 934, 884, 11, 584, 11, 2319, 36540, 295, 16235, 23475, 11, 4696, 291, 483, 281, 257, 665, 2158, 50703, 50703, 295, 264, 9834, 13, 50807, 50807, 407, 294, 1668, 281, 764, 16235, 23475, 11, 264, 2141, 551, 291, 643, 281, 14722, 307, 613, 14641, 51101, 51101, 13760, 2115, 13, 51215, 51215, 400, 437, 37624, 775, 11, 293, 294, 1186, 437, 307, 3832, 294, 18161, 3209, 3097, 11, 307, 51461, 51461, 281, 764, 364, 9284, 1219, 646, 79, 1513, 559, 399, 294, 1668, 281, 14722, 613, 14641, 13760, 51767, 51767, 2115, 13, 51817, 51817], "temperature": 0.0, "avg_logprob": -0.1462738811969757, "compression_ratio": 1.7186147186147187, "no_speech_prob": 2.090399902954232e-06}, {"id": 130, "seek": 66344, "start": 663.44, "end": 666.7600000000001, "text": " TensorFlow can do all of these things for you.", "tokens": [50364, 37624, 393, 360, 439, 295, 613, 721, 337, 291, 13, 50530, 50530, 467, 704, 17988, 646, 79, 1513, 559, 399, 439, 1951, 341, 2445, 1219, 3318, 13, 50750, 50750, 407, 439, 291, 362, 281, 360, 307, 818, 2316, 13, 6845, 2031, 11, 288, 382, 428, 3097, 992, 293, 980, 309, 281, 360, 370, 51080, 51080, 337, 2319, 36540, 420, 2319, 30992, 28346, 13, 51286, 51286, 682, 1186, 11, 437, 291, 536, 1780, 307, 300, 37624, 393, 764, 364, 9284, 300, 307, 754, 257, 707, 51524, 51524, 857, 4663, 813, 16235, 23475, 13, 51622, 51622, 400, 291, 536, 544, 466, 300, 1780, 341, 1243, 382, 731, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.13735583045265892, "compression_ratio": 1.644, "no_speech_prob": 3.4465174394426867e-06}, {"id": 131, "seek": 66344, "start": 666.7600000000001, "end": 671.1600000000001, "text": " It implements backpropagation all within this function called fit.", "tokens": [50364, 37624, 393, 360, 439, 295, 613, 721, 337, 291, 13, 50530, 50530, 467, 704, 17988, 646, 79, 1513, 559, 399, 439, 1951, 341, 2445, 1219, 3318, 13, 50750, 50750, 407, 439, 291, 362, 281, 360, 307, 818, 2316, 13, 6845, 2031, 11, 288, 382, 428, 3097, 992, 293, 980, 309, 281, 360, 370, 51080, 51080, 337, 2319, 36540, 420, 2319, 30992, 28346, 13, 51286, 51286, 682, 1186, 11, 437, 291, 536, 1780, 307, 300, 37624, 393, 764, 364, 9284, 300, 307, 754, 257, 707, 51524, 51524, 857, 4663, 813, 16235, 23475, 13, 51622, 51622, 400, 291, 536, 544, 466, 300, 1780, 341, 1243, 382, 731, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.13735583045265892, "compression_ratio": 1.644, "no_speech_prob": 3.4465174394426867e-06}, {"id": 132, "seek": 66344, "start": 671.1600000000001, "end": 677.7600000000001, "text": " So all you have to do is call model.fit x, y as your training set and tell it to do so", "tokens": [50364, 37624, 393, 360, 439, 295, 613, 721, 337, 291, 13, 50530, 50530, 467, 704, 17988, 646, 79, 1513, 559, 399, 439, 1951, 341, 2445, 1219, 3318, 13, 50750, 50750, 407, 439, 291, 362, 281, 360, 307, 818, 2316, 13, 6845, 2031, 11, 288, 382, 428, 3097, 992, 293, 980, 309, 281, 360, 370, 51080, 51080, 337, 2319, 36540, 420, 2319, 30992, 28346, 13, 51286, 51286, 682, 1186, 11, 437, 291, 536, 1780, 307, 300, 37624, 393, 764, 364, 9284, 300, 307, 754, 257, 707, 51524, 51524, 857, 4663, 813, 16235, 23475, 13, 51622, 51622, 400, 291, 536, 544, 466, 300, 1780, 341, 1243, 382, 731, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.13735583045265892, "compression_ratio": 1.644, "no_speech_prob": 3.4465174394426867e-06}, {"id": 133, "seek": 66344, "start": 677.7600000000001, "end": 681.8800000000001, "text": " for 100 iterations or 100 epochs.", "tokens": [50364, 37624, 393, 360, 439, 295, 613, 721, 337, 291, 13, 50530, 50530, 467, 704, 17988, 646, 79, 1513, 559, 399, 439, 1951, 341, 2445, 1219, 3318, 13, 50750, 50750, 407, 439, 291, 362, 281, 360, 307, 818, 2316, 13, 6845, 2031, 11, 288, 382, 428, 3097, 992, 293, 980, 309, 281, 360, 370, 51080, 51080, 337, 2319, 36540, 420, 2319, 30992, 28346, 13, 51286, 51286, 682, 1186, 11, 437, 291, 536, 1780, 307, 300, 37624, 393, 764, 364, 9284, 300, 307, 754, 257, 707, 51524, 51524, 857, 4663, 813, 16235, 23475, 13, 51622, 51622, 400, 291, 536, 544, 466, 300, 1780, 341, 1243, 382, 731, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.13735583045265892, "compression_ratio": 1.644, "no_speech_prob": 3.4465174394426867e-06}, {"id": 134, "seek": 66344, "start": 681.8800000000001, "end": 686.6400000000001, "text": " In fact, what you see later is that TensorFlow can use an algorithm that is even a little", "tokens": [50364, 37624, 393, 360, 439, 295, 613, 721, 337, 291, 13, 50530, 50530, 467, 704, 17988, 646, 79, 1513, 559, 399, 439, 1951, 341, 2445, 1219, 3318, 13, 50750, 50750, 407, 439, 291, 362, 281, 360, 307, 818, 2316, 13, 6845, 2031, 11, 288, 382, 428, 3097, 992, 293, 980, 309, 281, 360, 370, 51080, 51080, 337, 2319, 36540, 420, 2319, 30992, 28346, 13, 51286, 51286, 682, 1186, 11, 437, 291, 536, 1780, 307, 300, 37624, 393, 764, 364, 9284, 300, 307, 754, 257, 707, 51524, 51524, 857, 4663, 813, 16235, 23475, 13, 51622, 51622, 400, 291, 536, 544, 466, 300, 1780, 341, 1243, 382, 731, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.13735583045265892, "compression_ratio": 1.644, "no_speech_prob": 3.4465174394426867e-06}, {"id": 135, "seek": 66344, "start": 686.6400000000001, "end": 688.6, "text": " bit faster than gradient descent.", "tokens": [50364, 37624, 393, 360, 439, 295, 613, 721, 337, 291, 13, 50530, 50530, 467, 704, 17988, 646, 79, 1513, 559, 399, 439, 1951, 341, 2445, 1219, 3318, 13, 50750, 50750, 407, 439, 291, 362, 281, 360, 307, 818, 2316, 13, 6845, 2031, 11, 288, 382, 428, 3097, 992, 293, 980, 309, 281, 360, 370, 51080, 51080, 337, 2319, 36540, 420, 2319, 30992, 28346, 13, 51286, 51286, 682, 1186, 11, 437, 291, 536, 1780, 307, 300, 37624, 393, 764, 364, 9284, 300, 307, 754, 257, 707, 51524, 51524, 857, 4663, 813, 16235, 23475, 13, 51622, 51622, 400, 291, 536, 544, 466, 300, 1780, 341, 1243, 382, 731, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.13735583045265892, "compression_ratio": 1.644, "no_speech_prob": 3.4465174394426867e-06}, {"id": 136, "seek": 66344, "start": 688.6, "end": 691.5200000000001, "text": " And you see more about that later this week as well.", "tokens": [50364, 37624, 393, 360, 439, 295, 613, 721, 337, 291, 13, 50530, 50530, 467, 704, 17988, 646, 79, 1513, 559, 399, 439, 1951, 341, 2445, 1219, 3318, 13, 50750, 50750, 407, 439, 291, 362, 281, 360, 307, 818, 2316, 13, 6845, 2031, 11, 288, 382, 428, 3097, 992, 293, 980, 309, 281, 360, 370, 51080, 51080, 337, 2319, 36540, 420, 2319, 30992, 28346, 13, 51286, 51286, 682, 1186, 11, 437, 291, 536, 1780, 307, 300, 37624, 393, 764, 364, 9284, 300, 307, 754, 257, 707, 51524, 51524, 857, 4663, 813, 16235, 23475, 13, 51622, 51622, 400, 291, 536, 544, 466, 300, 1780, 341, 1243, 382, 731, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.13735583045265892, "compression_ratio": 1.644, "no_speech_prob": 3.4465174394426867e-06}, {"id": 137, "seek": 69152, "start": 691.52, "end": 697.28, "text": " Now I know that we're relying heavily on the TensorFlow library in order to implement a", "tokens": [50364, 823, 286, 458, 300, 321, 434, 24140, 10950, 322, 264, 37624, 6405, 294, 1668, 281, 4445, 257, 50652, 50652, 18161, 3209, 13, 50740, 50740, 1485, 5102, 286, 600, 1612, 2108, 3866, 3487, 307, 382, 264, 2899, 43737, 11, 15148, 1813, 51080, 51080, 544, 14442, 293, 881, 11955, 486, 764, 15148, 2831, 813, 4445, 3089, 490, 8459, 13, 51418, 51418, 400, 456, 362, 668, 867, 661, 5110, 295, 341, 294, 264, 2503, 295, 15866, 13, 51662, 51662], "temperature": 0.0, "avg_logprob": -0.12108102211585411, "compression_ratio": 1.547008547008547, "no_speech_prob": 1.61851985467365e-05}, {"id": 138, "seek": 69152, "start": 697.28, "end": 699.04, "text": " neural network.", "tokens": [50364, 823, 286, 458, 300, 321, 434, 24140, 10950, 322, 264, 37624, 6405, 294, 1668, 281, 4445, 257, 50652, 50652, 18161, 3209, 13, 50740, 50740, 1485, 5102, 286, 600, 1612, 2108, 3866, 3487, 307, 382, 264, 2899, 43737, 11, 15148, 1813, 51080, 51080, 544, 14442, 293, 881, 11955, 486, 764, 15148, 2831, 813, 4445, 3089, 490, 8459, 13, 51418, 51418, 400, 456, 362, 668, 867, 661, 5110, 295, 341, 294, 264, 2503, 295, 15866, 13, 51662, 51662], "temperature": 0.0, "avg_logprob": -0.12108102211585411, "compression_ratio": 1.547008547008547, "no_speech_prob": 1.61851985467365e-05}, {"id": 139, "seek": 69152, "start": 699.04, "end": 705.84, "text": " One pattern I've seen across multiple ideas is as the technology evolves, libraries become", "tokens": [50364, 823, 286, 458, 300, 321, 434, 24140, 10950, 322, 264, 37624, 6405, 294, 1668, 281, 4445, 257, 50652, 50652, 18161, 3209, 13, 50740, 50740, 1485, 5102, 286, 600, 1612, 2108, 3866, 3487, 307, 382, 264, 2899, 43737, 11, 15148, 1813, 51080, 51080, 544, 14442, 293, 881, 11955, 486, 764, 15148, 2831, 813, 4445, 3089, 490, 8459, 13, 51418, 51418, 400, 456, 362, 668, 867, 661, 5110, 295, 341, 294, 264, 2503, 295, 15866, 13, 51662, 51662], "temperature": 0.0, "avg_logprob": -0.12108102211585411, "compression_ratio": 1.547008547008547, "no_speech_prob": 1.61851985467365e-05}, {"id": 140, "seek": 69152, "start": 705.84, "end": 712.6, "text": " more mature and most engineers will use libraries rather than implement code from scratch.", "tokens": [50364, 823, 286, 458, 300, 321, 434, 24140, 10950, 322, 264, 37624, 6405, 294, 1668, 281, 4445, 257, 50652, 50652, 18161, 3209, 13, 50740, 50740, 1485, 5102, 286, 600, 1612, 2108, 3866, 3487, 307, 382, 264, 2899, 43737, 11, 15148, 1813, 51080, 51080, 544, 14442, 293, 881, 11955, 486, 764, 15148, 2831, 813, 4445, 3089, 490, 8459, 13, 51418, 51418, 400, 456, 362, 668, 867, 661, 5110, 295, 341, 294, 264, 2503, 295, 15866, 13, 51662, 51662], "temperature": 0.0, "avg_logprob": -0.12108102211585411, "compression_ratio": 1.547008547008547, "no_speech_prob": 1.61851985467365e-05}, {"id": 141, "seek": 69152, "start": 712.6, "end": 717.48, "text": " And there have been many other examples of this in the history of computing.", "tokens": [50364, 823, 286, 458, 300, 321, 434, 24140, 10950, 322, 264, 37624, 6405, 294, 1668, 281, 4445, 257, 50652, 50652, 18161, 3209, 13, 50740, 50740, 1485, 5102, 286, 600, 1612, 2108, 3866, 3487, 307, 382, 264, 2899, 43737, 11, 15148, 1813, 51080, 51080, 544, 14442, 293, 881, 11955, 486, 764, 15148, 2831, 813, 4445, 3089, 490, 8459, 13, 51418, 51418, 400, 456, 362, 668, 867, 661, 5110, 295, 341, 294, 264, 2503, 295, 15866, 13, 51662, 51662], "temperature": 0.0, "avg_logprob": -0.12108102211585411, "compression_ratio": 1.547008547008547, "no_speech_prob": 1.61851985467365e-05}, {"id": 142, "seek": 71748, "start": 717.48, "end": 722.4, "text": " Once many, many decades ago programmers had to implement their own sorting function from", "tokens": [50364, 3443, 867, 11, 867, 7878, 2057, 41504, 632, 281, 4445, 641, 1065, 32411, 2445, 490, 50610, 50610, 8459, 13, 50686, 50686, 583, 586, 32411, 15148, 366, 1596, 14442, 300, 291, 1391, 818, 1580, 1646, 311, 32411, 50928, 50928, 2445, 2831, 813, 4445, 309, 1803, 5969, 291, 434, 1940, 257, 15866, 5359, 11, 51146, 51146, 1029, 291, 281, 360, 309, 382, 364, 5380, 13, 51292, 51292, 400, 965, 498, 291, 528, 281, 14722, 264, 3732, 5593, 295, 257, 1230, 11, 411, 437, 307, 264, 3732, 51528, 51528, 5593, 295, 1614, 30, 51606, 51606, 1042, 1564, 41504, 632, 281, 2464, 641, 1065, 3089, 281, 14722, 341, 11, 457, 586, 1238, 709, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.16824523115580062, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.4970479242037982e-05}, {"id": 143, "seek": 71748, "start": 722.4, "end": 723.9200000000001, "text": " scratch.", "tokens": [50364, 3443, 867, 11, 867, 7878, 2057, 41504, 632, 281, 4445, 641, 1065, 32411, 2445, 490, 50610, 50610, 8459, 13, 50686, 50686, 583, 586, 32411, 15148, 366, 1596, 14442, 300, 291, 1391, 818, 1580, 1646, 311, 32411, 50928, 50928, 2445, 2831, 813, 4445, 309, 1803, 5969, 291, 434, 1940, 257, 15866, 5359, 11, 51146, 51146, 1029, 291, 281, 360, 309, 382, 364, 5380, 13, 51292, 51292, 400, 965, 498, 291, 528, 281, 14722, 264, 3732, 5593, 295, 257, 1230, 11, 411, 437, 307, 264, 3732, 51528, 51528, 5593, 295, 1614, 30, 51606, 51606, 1042, 1564, 41504, 632, 281, 2464, 641, 1065, 3089, 281, 14722, 341, 11, 457, 586, 1238, 709, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.16824523115580062, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.4970479242037982e-05}, {"id": 144, "seek": 71748, "start": 723.9200000000001, "end": 728.76, "text": " But now sorting libraries are quite mature that you probably call someone else's sorting", "tokens": [50364, 3443, 867, 11, 867, 7878, 2057, 41504, 632, 281, 4445, 641, 1065, 32411, 2445, 490, 50610, 50610, 8459, 13, 50686, 50686, 583, 586, 32411, 15148, 366, 1596, 14442, 300, 291, 1391, 818, 1580, 1646, 311, 32411, 50928, 50928, 2445, 2831, 813, 4445, 309, 1803, 5969, 291, 434, 1940, 257, 15866, 5359, 11, 51146, 51146, 1029, 291, 281, 360, 309, 382, 364, 5380, 13, 51292, 51292, 400, 965, 498, 291, 528, 281, 14722, 264, 3732, 5593, 295, 257, 1230, 11, 411, 437, 307, 264, 3732, 51528, 51528, 5593, 295, 1614, 30, 51606, 51606, 1042, 1564, 41504, 632, 281, 2464, 641, 1065, 3089, 281, 14722, 341, 11, 457, 586, 1238, 709, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.16824523115580062, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.4970479242037982e-05}, {"id": 145, "seek": 71748, "start": 728.76, "end": 733.12, "text": " function rather than implement it yourself unless you're taking a computing classes,", "tokens": [50364, 3443, 867, 11, 867, 7878, 2057, 41504, 632, 281, 4445, 641, 1065, 32411, 2445, 490, 50610, 50610, 8459, 13, 50686, 50686, 583, 586, 32411, 15148, 366, 1596, 14442, 300, 291, 1391, 818, 1580, 1646, 311, 32411, 50928, 50928, 2445, 2831, 813, 4445, 309, 1803, 5969, 291, 434, 1940, 257, 15866, 5359, 11, 51146, 51146, 1029, 291, 281, 360, 309, 382, 364, 5380, 13, 51292, 51292, 400, 965, 498, 291, 528, 281, 14722, 264, 3732, 5593, 295, 257, 1230, 11, 411, 437, 307, 264, 3732, 51528, 51528, 5593, 295, 1614, 30, 51606, 51606, 1042, 1564, 41504, 632, 281, 2464, 641, 1065, 3089, 281, 14722, 341, 11, 457, 586, 1238, 709, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.16824523115580062, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.4970479242037982e-05}, {"id": 146, "seek": 71748, "start": 733.12, "end": 736.04, "text": " ask you to do it as an exercise.", "tokens": [50364, 3443, 867, 11, 867, 7878, 2057, 41504, 632, 281, 4445, 641, 1065, 32411, 2445, 490, 50610, 50610, 8459, 13, 50686, 50686, 583, 586, 32411, 15148, 366, 1596, 14442, 300, 291, 1391, 818, 1580, 1646, 311, 32411, 50928, 50928, 2445, 2831, 813, 4445, 309, 1803, 5969, 291, 434, 1940, 257, 15866, 5359, 11, 51146, 51146, 1029, 291, 281, 360, 309, 382, 364, 5380, 13, 51292, 51292, 400, 965, 498, 291, 528, 281, 14722, 264, 3732, 5593, 295, 257, 1230, 11, 411, 437, 307, 264, 3732, 51528, 51528, 5593, 295, 1614, 30, 51606, 51606, 1042, 1564, 41504, 632, 281, 2464, 641, 1065, 3089, 281, 14722, 341, 11, 457, 586, 1238, 709, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.16824523115580062, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.4970479242037982e-05}, {"id": 147, "seek": 71748, "start": 736.04, "end": 740.76, "text": " And today if you want to compute the square root of a number, like what is the square", "tokens": [50364, 3443, 867, 11, 867, 7878, 2057, 41504, 632, 281, 4445, 641, 1065, 32411, 2445, 490, 50610, 50610, 8459, 13, 50686, 50686, 583, 586, 32411, 15148, 366, 1596, 14442, 300, 291, 1391, 818, 1580, 1646, 311, 32411, 50928, 50928, 2445, 2831, 813, 4445, 309, 1803, 5969, 291, 434, 1940, 257, 15866, 5359, 11, 51146, 51146, 1029, 291, 281, 360, 309, 382, 364, 5380, 13, 51292, 51292, 400, 965, 498, 291, 528, 281, 14722, 264, 3732, 5593, 295, 257, 1230, 11, 411, 437, 307, 264, 3732, 51528, 51528, 5593, 295, 1614, 30, 51606, 51606, 1042, 1564, 41504, 632, 281, 2464, 641, 1065, 3089, 281, 14722, 341, 11, 457, 586, 1238, 709, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.16824523115580062, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.4970479242037982e-05}, {"id": 148, "seek": 71748, "start": 740.76, "end": 742.32, "text": " root of 7?", "tokens": [50364, 3443, 867, 11, 867, 7878, 2057, 41504, 632, 281, 4445, 641, 1065, 32411, 2445, 490, 50610, 50610, 8459, 13, 50686, 50686, 583, 586, 32411, 15148, 366, 1596, 14442, 300, 291, 1391, 818, 1580, 1646, 311, 32411, 50928, 50928, 2445, 2831, 813, 4445, 309, 1803, 5969, 291, 434, 1940, 257, 15866, 5359, 11, 51146, 51146, 1029, 291, 281, 360, 309, 382, 364, 5380, 13, 51292, 51292, 400, 965, 498, 291, 528, 281, 14722, 264, 3732, 5593, 295, 257, 1230, 11, 411, 437, 307, 264, 3732, 51528, 51528, 5593, 295, 1614, 30, 51606, 51606, 1042, 1564, 41504, 632, 281, 2464, 641, 1065, 3089, 281, 14722, 341, 11, 457, 586, 1238, 709, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.16824523115580062, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.4970479242037982e-05}, {"id": 149, "seek": 71748, "start": 742.32, "end": 747.28, "text": " Well once programmers had to write their own code to compute this, but now pretty much", "tokens": [50364, 3443, 867, 11, 867, 7878, 2057, 41504, 632, 281, 4445, 641, 1065, 32411, 2445, 490, 50610, 50610, 8459, 13, 50686, 50686, 583, 586, 32411, 15148, 366, 1596, 14442, 300, 291, 1391, 818, 1580, 1646, 311, 32411, 50928, 50928, 2445, 2831, 813, 4445, 309, 1803, 5969, 291, 434, 1940, 257, 15866, 5359, 11, 51146, 51146, 1029, 291, 281, 360, 309, 382, 364, 5380, 13, 51292, 51292, 400, 965, 498, 291, 528, 281, 14722, 264, 3732, 5593, 295, 257, 1230, 11, 411, 437, 307, 264, 3732, 51528, 51528, 5593, 295, 1614, 30, 51606, 51606, 1042, 1564, 41504, 632, 281, 2464, 641, 1065, 3089, 281, 14722, 341, 11, 457, 586, 1238, 709, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.16824523115580062, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.4970479242037982e-05}, {"id": 150, "seek": 74728, "start": 747.28, "end": 754.4, "text": " everyone just calls a library to take square roots or matrix operations such as multiplying", "tokens": [50364, 1518, 445, 5498, 257, 6405, 281, 747, 3732, 10669, 420, 8141, 7705, 1270, 382, 30955, 50720, 50720, 732, 32284, 1214, 13, 50858, 50858, 407, 562, 2452, 2539, 390, 7037, 293, 1570, 14442, 11, 867, 8849, 11, 3009, 385, 11, 645, 51130, 51130, 18114, 721, 490, 8459, 1228, 15329, 420, 383, 25472, 420, 512, 661, 6405, 13, 51398, 51398, 583, 965, 2452, 2539, 15148, 362, 14442, 67, 1547, 300, 881, 8849, 486, 764, 613, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.09684314975490818, "compression_ratio": 1.6026200873362446, "no_speech_prob": 2.046074951067567e-05}, {"id": 151, "seek": 74728, "start": 754.4, "end": 757.16, "text": " two matrices together.", "tokens": [50364, 1518, 445, 5498, 257, 6405, 281, 747, 3732, 10669, 420, 8141, 7705, 1270, 382, 30955, 50720, 50720, 732, 32284, 1214, 13, 50858, 50858, 407, 562, 2452, 2539, 390, 7037, 293, 1570, 14442, 11, 867, 8849, 11, 3009, 385, 11, 645, 51130, 51130, 18114, 721, 490, 8459, 1228, 15329, 420, 383, 25472, 420, 512, 661, 6405, 13, 51398, 51398, 583, 965, 2452, 2539, 15148, 362, 14442, 67, 1547, 300, 881, 8849, 486, 764, 613, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.09684314975490818, "compression_ratio": 1.6026200873362446, "no_speech_prob": 2.046074951067567e-05}, {"id": 152, "seek": 74728, "start": 757.16, "end": 762.6, "text": " So when deep learning was younger and less mature, many developers, including me, were", "tokens": [50364, 1518, 445, 5498, 257, 6405, 281, 747, 3732, 10669, 420, 8141, 7705, 1270, 382, 30955, 50720, 50720, 732, 32284, 1214, 13, 50858, 50858, 407, 562, 2452, 2539, 390, 7037, 293, 1570, 14442, 11, 867, 8849, 11, 3009, 385, 11, 645, 51130, 51130, 18114, 721, 490, 8459, 1228, 15329, 420, 383, 25472, 420, 512, 661, 6405, 13, 51398, 51398, 583, 965, 2452, 2539, 15148, 362, 14442, 67, 1547, 300, 881, 8849, 486, 764, 613, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.09684314975490818, "compression_ratio": 1.6026200873362446, "no_speech_prob": 2.046074951067567e-05}, {"id": 153, "seek": 74728, "start": 762.6, "end": 767.9599999999999, "text": " implementing things from scratch using Python or C++ or some other library.", "tokens": [50364, 1518, 445, 5498, 257, 6405, 281, 747, 3732, 10669, 420, 8141, 7705, 1270, 382, 30955, 50720, 50720, 732, 32284, 1214, 13, 50858, 50858, 407, 562, 2452, 2539, 390, 7037, 293, 1570, 14442, 11, 867, 8849, 11, 3009, 385, 11, 645, 51130, 51130, 18114, 721, 490, 8459, 1228, 15329, 420, 383, 25472, 420, 512, 661, 6405, 13, 51398, 51398, 583, 965, 2452, 2539, 15148, 362, 14442, 67, 1547, 300, 881, 8849, 486, 764, 613, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.09684314975490818, "compression_ratio": 1.6026200873362446, "no_speech_prob": 2.046074951067567e-05}, {"id": 154, "seek": 74728, "start": 767.9599999999999, "end": 773.36, "text": " But today deep learning libraries have matured enough that most developers will use these", "tokens": [50364, 1518, 445, 5498, 257, 6405, 281, 747, 3732, 10669, 420, 8141, 7705, 1270, 382, 30955, 50720, 50720, 732, 32284, 1214, 13, 50858, 50858, 407, 562, 2452, 2539, 390, 7037, 293, 1570, 14442, 11, 867, 8849, 11, 3009, 385, 11, 645, 51130, 51130, 18114, 721, 490, 8459, 1228, 15329, 420, 383, 25472, 420, 512, 661, 6405, 13, 51398, 51398, 583, 965, 2452, 2539, 15148, 362, 14442, 67, 1547, 300, 881, 8849, 486, 764, 613, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.09684314975490818, "compression_ratio": 1.6026200873362446, "no_speech_prob": 2.046074951067567e-05}, {"id": 155, "seek": 77336, "start": 773.36, "end": 779.0, "text": " libraries and in fact most commercial implementations of neural networks today use a library like", "tokens": [50364, 15148, 293, 294, 1186, 881, 6841, 4445, 763, 295, 18161, 9590, 965, 764, 257, 6405, 411, 50646, 50646, 37624, 420, 9953, 51, 284, 339, 13, 50740, 50740, 583, 382, 286, 600, 2835, 11, 309, 311, 920, 4420, 281, 1223, 577, 436, 589, 833, 264, 13376, 370, 50946, 50946, 300, 498, 746, 13106, 2314, 11, 597, 920, 775, 365, 965, 311, 15148, 11, 291, 362, 51167, 51167, 257, 1101, 2931, 295, 5276, 577, 281, 3191, 309, 13, 51288, 51288, 823, 300, 291, 458, 577, 281, 3847, 257, 3875, 18161, 3209, 11, 611, 1219, 257, 2120, 388, 11167, 43276, 2044, 11, 51618, 51618, 456, 366, 512, 721, 291, 393, 1319, 466, 264, 18161, 3209, 300, 486, 652, 309, 754, 51825, 51825], "temperature": 0.0, "avg_logprob": -0.09533551113664611, "compression_ratio": 1.6742671009771988, "no_speech_prob": 4.8602432798361406e-06}, {"id": 156, "seek": 77336, "start": 779.0, "end": 780.88, "text": " TensorFlow or PyTorch.", "tokens": [50364, 15148, 293, 294, 1186, 881, 6841, 4445, 763, 295, 18161, 9590, 965, 764, 257, 6405, 411, 50646, 50646, 37624, 420, 9953, 51, 284, 339, 13, 50740, 50740, 583, 382, 286, 600, 2835, 11, 309, 311, 920, 4420, 281, 1223, 577, 436, 589, 833, 264, 13376, 370, 50946, 50946, 300, 498, 746, 13106, 2314, 11, 597, 920, 775, 365, 965, 311, 15148, 11, 291, 362, 51167, 51167, 257, 1101, 2931, 295, 5276, 577, 281, 3191, 309, 13, 51288, 51288, 823, 300, 291, 458, 577, 281, 3847, 257, 3875, 18161, 3209, 11, 611, 1219, 257, 2120, 388, 11167, 43276, 2044, 11, 51618, 51618, 456, 366, 512, 721, 291, 393, 1319, 466, 264, 18161, 3209, 300, 486, 652, 309, 754, 51825, 51825], "temperature": 0.0, "avg_logprob": -0.09533551113664611, "compression_ratio": 1.6742671009771988, "no_speech_prob": 4.8602432798361406e-06}, {"id": 157, "seek": 77336, "start": 780.88, "end": 785.0, "text": " But as I've mentioned, it's still useful to understand how they work under the hood so", "tokens": [50364, 15148, 293, 294, 1186, 881, 6841, 4445, 763, 295, 18161, 9590, 965, 764, 257, 6405, 411, 50646, 50646, 37624, 420, 9953, 51, 284, 339, 13, 50740, 50740, 583, 382, 286, 600, 2835, 11, 309, 311, 920, 4420, 281, 1223, 577, 436, 589, 833, 264, 13376, 370, 50946, 50946, 300, 498, 746, 13106, 2314, 11, 597, 920, 775, 365, 965, 311, 15148, 11, 291, 362, 51167, 51167, 257, 1101, 2931, 295, 5276, 577, 281, 3191, 309, 13, 51288, 51288, 823, 300, 291, 458, 577, 281, 3847, 257, 3875, 18161, 3209, 11, 611, 1219, 257, 2120, 388, 11167, 43276, 2044, 11, 51618, 51618, 456, 366, 512, 721, 291, 393, 1319, 466, 264, 18161, 3209, 300, 486, 652, 309, 754, 51825, 51825], "temperature": 0.0, "avg_logprob": -0.09533551113664611, "compression_ratio": 1.6742671009771988, "no_speech_prob": 4.8602432798361406e-06}, {"id": 158, "seek": 77336, "start": 785.0, "end": 789.42, "text": " that if something unexpected happens, which still does with today's libraries, you have", "tokens": [50364, 15148, 293, 294, 1186, 881, 6841, 4445, 763, 295, 18161, 9590, 965, 764, 257, 6405, 411, 50646, 50646, 37624, 420, 9953, 51, 284, 339, 13, 50740, 50740, 583, 382, 286, 600, 2835, 11, 309, 311, 920, 4420, 281, 1223, 577, 436, 589, 833, 264, 13376, 370, 50946, 50946, 300, 498, 746, 13106, 2314, 11, 597, 920, 775, 365, 965, 311, 15148, 11, 291, 362, 51167, 51167, 257, 1101, 2931, 295, 5276, 577, 281, 3191, 309, 13, 51288, 51288, 823, 300, 291, 458, 577, 281, 3847, 257, 3875, 18161, 3209, 11, 611, 1219, 257, 2120, 388, 11167, 43276, 2044, 11, 51618, 51618, 456, 366, 512, 721, 291, 393, 1319, 466, 264, 18161, 3209, 300, 486, 652, 309, 754, 51825, 51825], "temperature": 0.0, "avg_logprob": -0.09533551113664611, "compression_ratio": 1.6742671009771988, "no_speech_prob": 4.8602432798361406e-06}, {"id": 159, "seek": 77336, "start": 789.42, "end": 791.84, "text": " a better chance of knowing how to fix it.", "tokens": [50364, 15148, 293, 294, 1186, 881, 6841, 4445, 763, 295, 18161, 9590, 965, 764, 257, 6405, 411, 50646, 50646, 37624, 420, 9953, 51, 284, 339, 13, 50740, 50740, 583, 382, 286, 600, 2835, 11, 309, 311, 920, 4420, 281, 1223, 577, 436, 589, 833, 264, 13376, 370, 50946, 50946, 300, 498, 746, 13106, 2314, 11, 597, 920, 775, 365, 965, 311, 15148, 11, 291, 362, 51167, 51167, 257, 1101, 2931, 295, 5276, 577, 281, 3191, 309, 13, 51288, 51288, 823, 300, 291, 458, 577, 281, 3847, 257, 3875, 18161, 3209, 11, 611, 1219, 257, 2120, 388, 11167, 43276, 2044, 11, 51618, 51618, 456, 366, 512, 721, 291, 393, 1319, 466, 264, 18161, 3209, 300, 486, 652, 309, 754, 51825, 51825], "temperature": 0.0, "avg_logprob": -0.09533551113664611, "compression_ratio": 1.6742671009771988, "no_speech_prob": 4.8602432798361406e-06}, {"id": 160, "seek": 77336, "start": 791.84, "end": 798.44, "text": " Now that you know how to train a basic neural network, also called a multilayer perceptron,", "tokens": [50364, 15148, 293, 294, 1186, 881, 6841, 4445, 763, 295, 18161, 9590, 965, 764, 257, 6405, 411, 50646, 50646, 37624, 420, 9953, 51, 284, 339, 13, 50740, 50740, 583, 382, 286, 600, 2835, 11, 309, 311, 920, 4420, 281, 1223, 577, 436, 589, 833, 264, 13376, 370, 50946, 50946, 300, 498, 746, 13106, 2314, 11, 597, 920, 775, 365, 965, 311, 15148, 11, 291, 362, 51167, 51167, 257, 1101, 2931, 295, 5276, 577, 281, 3191, 309, 13, 51288, 51288, 823, 300, 291, 458, 577, 281, 3847, 257, 3875, 18161, 3209, 11, 611, 1219, 257, 2120, 388, 11167, 43276, 2044, 11, 51618, 51618, 456, 366, 512, 721, 291, 393, 1319, 466, 264, 18161, 3209, 300, 486, 652, 309, 754, 51825, 51825], "temperature": 0.0, "avg_logprob": -0.09533551113664611, "compression_ratio": 1.6742671009771988, "no_speech_prob": 4.8602432798361406e-06}, {"id": 161, "seek": 77336, "start": 798.44, "end": 802.58, "text": " there are some things you can change about the neural network that will make it even", "tokens": [50364, 15148, 293, 294, 1186, 881, 6841, 4445, 763, 295, 18161, 9590, 965, 764, 257, 6405, 411, 50646, 50646, 37624, 420, 9953, 51, 284, 339, 13, 50740, 50740, 583, 382, 286, 600, 2835, 11, 309, 311, 920, 4420, 281, 1223, 577, 436, 589, 833, 264, 13376, 370, 50946, 50946, 300, 498, 746, 13106, 2314, 11, 597, 920, 775, 365, 965, 311, 15148, 11, 291, 362, 51167, 51167, 257, 1101, 2931, 295, 5276, 577, 281, 3191, 309, 13, 51288, 51288, 823, 300, 291, 458, 577, 281, 3847, 257, 3875, 18161, 3209, 11, 611, 1219, 257, 2120, 388, 11167, 43276, 2044, 11, 51618, 51618, 456, 366, 512, 721, 291, 393, 1319, 466, 264, 18161, 3209, 300, 486, 652, 309, 754, 51825, 51825], "temperature": 0.0, "avg_logprob": -0.09533551113664611, "compression_ratio": 1.6742671009771988, "no_speech_prob": 4.8602432798361406e-06}, {"id": 162, "seek": 80258, "start": 802.58, "end": 804.48, "text": " more powerful.", "tokens": [50364, 544, 4005, 13, 50459, 50459, 682, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 577, 291, 393, 18135, 294, 819, 24433, 6828, 50710, 50710, 382, 364, 8535, 281, 264, 4556, 3280, 327, 24433, 2445, 321, 600, 668, 1228, 13, 50963, 50963, 639, 486, 652, 428, 18161, 9590, 589, 754, 709, 1101, 13, 51141, 51141, 407, 718, 311, 352, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51259], "temperature": 0.0, "avg_logprob": -0.11513738762842465, "compression_ratio": 1.6436781609195403, "no_speech_prob": 0.00020940699323546141}, {"id": 163, "seek": 80258, "start": 804.48, "end": 809.5, "text": " In the next video, let's take a look at how you can swap in different activation functions", "tokens": [50364, 544, 4005, 13, 50459, 50459, 682, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 577, 291, 393, 18135, 294, 819, 24433, 6828, 50710, 50710, 382, 364, 8535, 281, 264, 4556, 3280, 327, 24433, 2445, 321, 600, 668, 1228, 13, 50963, 50963, 639, 486, 652, 428, 18161, 9590, 589, 754, 709, 1101, 13, 51141, 51141, 407, 718, 311, 352, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51259], "temperature": 0.0, "avg_logprob": -0.11513738762842465, "compression_ratio": 1.6436781609195403, "no_speech_prob": 0.00020940699323546141}, {"id": 164, "seek": 80258, "start": 809.5, "end": 814.5600000000001, "text": " as an alternative to the sigmoid activation function we've been using.", "tokens": [50364, 544, 4005, 13, 50459, 50459, 682, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 577, 291, 393, 18135, 294, 819, 24433, 6828, 50710, 50710, 382, 364, 8535, 281, 264, 4556, 3280, 327, 24433, 2445, 321, 600, 668, 1228, 13, 50963, 50963, 639, 486, 652, 428, 18161, 9590, 589, 754, 709, 1101, 13, 51141, 51141, 407, 718, 311, 352, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51259], "temperature": 0.0, "avg_logprob": -0.11513738762842465, "compression_ratio": 1.6436781609195403, "no_speech_prob": 0.00020940699323546141}, {"id": 165, "seek": 80258, "start": 814.5600000000001, "end": 818.12, "text": " This will make your neural networks work even much better.", "tokens": [50364, 544, 4005, 13, 50459, 50459, 682, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 577, 291, 393, 18135, 294, 819, 24433, 6828, 50710, 50710, 382, 364, 8535, 281, 264, 4556, 3280, 327, 24433, 2445, 321, 600, 668, 1228, 13, 50963, 50963, 639, 486, 652, 428, 18161, 9590, 589, 754, 709, 1101, 13, 51141, 51141, 407, 718, 311, 352, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51259], "temperature": 0.0, "avg_logprob": -0.11513738762842465, "compression_ratio": 1.6436781609195403, "no_speech_prob": 0.00020940699323546141}, {"id": 166, "seek": 81812, "start": 818.12, "end": 833.76, "text": " So let's go take a look at that in the next video.", "tokens": [50364, 407, 718, 311, 352, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 51146], "temperature": 0.0, "avg_logprob": -0.3202861337100758, "compression_ratio": 0.9090909090909091, "no_speech_prob": 0.00012205462553538382}], "language": "en", "video_id": "jzaF4An03Oc", "entity": "ML Specialization, Andrew Ng (2022)"}}