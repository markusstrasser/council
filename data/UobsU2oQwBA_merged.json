{"video_id": "UobsU2oQwBA", "title": "2.10 Practical Tips for Linear Regression | Polynomial regression --[Machine Learning | Andrew Ng]", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 352, "views": 229, "publish_date": "11/04/2022", "timestamp": 1661126400, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " So far, we've just been fitting straight lines to our data. Let's take the ideas of multiple linear regression and feature engineering to come up with a new algorithm called polynomial regression, which will let you fit curves, nonlinear functions, to your data. Let's say you have a housing data set that looks like this, where feature x is the size in square feet. It doesn't look like a straight line fits this data set very well, so maybe you want to fit a curve, maybe a quadratic function, to the data like this, which includes a size x and also x squared, which is the size raised to the power of 2. And maybe that will give you a better fit to the data. But then you may decide that your quadratic model doesn't really make sense, because a quadratic function eventually comes back down, and well, we wouldn't really expect housing prices to go down when the size increases, right? Big houses seem like they should usually cost more. So then you may choose a cubic function, where we now have not only x squared, but x cubed. So maybe this model produces this curve here, which is a somewhat better fit to the data, because the size does eventually come back up as the size increases. These are both examples of polynomial regression, because you took your optional feature x and raised it to the power of 2, or 3, or any other power. And in the case of the cubic function, the first feature is the size, the second feature is the size squared, and the third feature is the size cubed. I just want to point out one more thing, which is that if you create features that are these powers, like the square of the original features like this, then feature scaling becomes increasingly important. So if the size of the house ranges from say, 1 to 1000 square feet, then the second feature, which is the size squared, would range from 1 to a million, and the third feature, which is size cubed, ranges from 1 to a billion. So these two features, x squared and x cubed, take on very different ranges of values compared to the original feature x. And if you're using gradient descent, it's important to apply feature scaling to get your features into comparable ranges of values. Finally, here's one last example of how you really have a wide range of choices of features to use. Another reasonable alternative to taking the size squared and size cubed is to say use the square root of x. So your model may look like w1 times x plus w2 times the square root of x plus b. The square root function looks like this, and it becomes a bit less steep as x increases, but it doesn't ever completely flatten out, and it certainly never ever comes back down. So this would be another choice of features that might work well for this dataset as well. So you may ask yourself, how do I decide what features to use? Later, in the second course in this specialization, you see how you can choose different features and different models that include or don't include these features, and you have a process for measuring how well these different models perform to help you decide which features to include or not include. For now, I just want you to be aware that you have a choice in what features you use, and by using feature engineering and polynomial functions, you can potentially get a much better model for your data. In the optional lab that follows this video, you will see some code that implements polynomial regression using features like x, x squared, and x cubed. So please take a look and run the code and see how it works. There's also another optional lab after that one that shows how to use a popular open source toolkit that implements linear regression. Scikit-learn is a very widely used open source machine learning library that is used by many practitioners in many of the top AI, internet, machine learning companies in the world. So if either now or in the future you're using machine learning in your job, there's a very good chance you'll be using tools like Scikit-learn to train your models. And so working through that optional lab will give you a chance to not only better understand linear regression, but also see how this can be done in just a few lines of code using a library like Scikit-learn. For you to have a solid understanding of these algorithms and be able to apply them, I do think it's important that you know how to implement linear regression yourself, and not just call some Scikit-learn function that is a black box. But Scikit-learn also has an important role in the way machine learning is done in practice today. So, we're just about at the end of this week. Congratulations on finishing all of this week's videos. Please do take a look at the practice quizzes and also the practice lab, which I hope will let you try out and practice ideas that we've discussed. In this week's practice lab, you implement linear regression. I hope you have a lot of fun getting this learning algorithm to work for yourself. Best of luck with that, and I also look forward to seeing you in next week's videos, where we'll go beyond regression, that is predicting numbers, to talk about our first classification algorithm, which can predict categories. I'll see you next week.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.54, "text": " So far, we've just been fitting straight lines to our data.", "tokens": [50364, 407, 1400, 11, 321, 600, 445, 668, 15669, 2997, 3876, 281, 527, 1412, 13, 50741, 50741, 961, 311, 747, 264, 3487, 295, 3866, 8213, 24590, 293, 4111, 7043, 281, 808, 493, 365, 257, 50992, 50992, 777, 9284, 1219, 26110, 24590, 11, 597, 486, 718, 291, 3318, 19490, 11, 2107, 28263, 6828, 11, 51266, 51266, 281, 428, 1412, 13, 51316, 51316, 961, 311, 584, 291, 362, 257, 6849, 1412, 992, 300, 1542, 411, 341, 11, 689, 4111, 2031, 307, 264, 2744, 51594, 51594, 294, 3732, 3521, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.17451651891072592, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.003763299435377121}, {"id": 1, "seek": 0, "start": 7.54, "end": 12.56, "text": " Let's take the ideas of multiple linear regression and feature engineering to come up with a", "tokens": [50364, 407, 1400, 11, 321, 600, 445, 668, 15669, 2997, 3876, 281, 527, 1412, 13, 50741, 50741, 961, 311, 747, 264, 3487, 295, 3866, 8213, 24590, 293, 4111, 7043, 281, 808, 493, 365, 257, 50992, 50992, 777, 9284, 1219, 26110, 24590, 11, 597, 486, 718, 291, 3318, 19490, 11, 2107, 28263, 6828, 11, 51266, 51266, 281, 428, 1412, 13, 51316, 51316, 961, 311, 584, 291, 362, 257, 6849, 1412, 992, 300, 1542, 411, 341, 11, 689, 4111, 2031, 307, 264, 2744, 51594, 51594, 294, 3732, 3521, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.17451651891072592, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.003763299435377121}, {"id": 2, "seek": 0, "start": 12.56, "end": 18.04, "text": " new algorithm called polynomial regression, which will let you fit curves, nonlinear functions,", "tokens": [50364, 407, 1400, 11, 321, 600, 445, 668, 15669, 2997, 3876, 281, 527, 1412, 13, 50741, 50741, 961, 311, 747, 264, 3487, 295, 3866, 8213, 24590, 293, 4111, 7043, 281, 808, 493, 365, 257, 50992, 50992, 777, 9284, 1219, 26110, 24590, 11, 597, 486, 718, 291, 3318, 19490, 11, 2107, 28263, 6828, 11, 51266, 51266, 281, 428, 1412, 13, 51316, 51316, 961, 311, 584, 291, 362, 257, 6849, 1412, 992, 300, 1542, 411, 341, 11, 689, 4111, 2031, 307, 264, 2744, 51594, 51594, 294, 3732, 3521, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.17451651891072592, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.003763299435377121}, {"id": 3, "seek": 0, "start": 18.04, "end": 19.04, "text": " to your data.", "tokens": [50364, 407, 1400, 11, 321, 600, 445, 668, 15669, 2997, 3876, 281, 527, 1412, 13, 50741, 50741, 961, 311, 747, 264, 3487, 295, 3866, 8213, 24590, 293, 4111, 7043, 281, 808, 493, 365, 257, 50992, 50992, 777, 9284, 1219, 26110, 24590, 11, 597, 486, 718, 291, 3318, 19490, 11, 2107, 28263, 6828, 11, 51266, 51266, 281, 428, 1412, 13, 51316, 51316, 961, 311, 584, 291, 362, 257, 6849, 1412, 992, 300, 1542, 411, 341, 11, 689, 4111, 2031, 307, 264, 2744, 51594, 51594, 294, 3732, 3521, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.17451651891072592, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.003763299435377121}, {"id": 4, "seek": 0, "start": 19.04, "end": 24.6, "text": " Let's say you have a housing data set that looks like this, where feature x is the size", "tokens": [50364, 407, 1400, 11, 321, 600, 445, 668, 15669, 2997, 3876, 281, 527, 1412, 13, 50741, 50741, 961, 311, 747, 264, 3487, 295, 3866, 8213, 24590, 293, 4111, 7043, 281, 808, 493, 365, 257, 50992, 50992, 777, 9284, 1219, 26110, 24590, 11, 597, 486, 718, 291, 3318, 19490, 11, 2107, 28263, 6828, 11, 51266, 51266, 281, 428, 1412, 13, 51316, 51316, 961, 311, 584, 291, 362, 257, 6849, 1412, 992, 300, 1542, 411, 341, 11, 689, 4111, 2031, 307, 264, 2744, 51594, 51594, 294, 3732, 3521, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.17451651891072592, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.003763299435377121}, {"id": 5, "seek": 0, "start": 24.6, "end": 26.400000000000002, "text": " in square feet.", "tokens": [50364, 407, 1400, 11, 321, 600, 445, 668, 15669, 2997, 3876, 281, 527, 1412, 13, 50741, 50741, 961, 311, 747, 264, 3487, 295, 3866, 8213, 24590, 293, 4111, 7043, 281, 808, 493, 365, 257, 50992, 50992, 777, 9284, 1219, 26110, 24590, 11, 597, 486, 718, 291, 3318, 19490, 11, 2107, 28263, 6828, 11, 51266, 51266, 281, 428, 1412, 13, 51316, 51316, 961, 311, 584, 291, 362, 257, 6849, 1412, 992, 300, 1542, 411, 341, 11, 689, 4111, 2031, 307, 264, 2744, 51594, 51594, 294, 3732, 3521, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.17451651891072592, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.003763299435377121}, {"id": 6, "seek": 2640, "start": 26.4, "end": 31.2, "text": " It doesn't look like a straight line fits this data set very well, so maybe you want", "tokens": [50364, 467, 1177, 380, 574, 411, 257, 2997, 1622, 9001, 341, 1412, 992, 588, 731, 11, 370, 1310, 291, 528, 50604, 50604, 281, 3318, 257, 7605, 11, 1310, 257, 37262, 2445, 11, 281, 264, 1412, 411, 341, 11, 597, 5974, 257, 2744, 50974, 50974, 2031, 293, 611, 2031, 8889, 11, 597, 307, 264, 2744, 6005, 281, 264, 1347, 295, 568, 13, 51272, 51272, 400, 1310, 300, 486, 976, 291, 257, 1101, 3318, 281, 264, 1412, 13, 51456, 51456, 583, 550, 291, 815, 4536, 300, 428, 37262, 2316, 1177, 380, 534, 652, 2020, 11, 570, 257, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.11786064809682417, "compression_ratio": 1.6784140969162995, "no_speech_prob": 4.222776169626741e-06}, {"id": 7, "seek": 2640, "start": 31.2, "end": 38.6, "text": " to fit a curve, maybe a quadratic function, to the data like this, which includes a size", "tokens": [50364, 467, 1177, 380, 574, 411, 257, 2997, 1622, 9001, 341, 1412, 992, 588, 731, 11, 370, 1310, 291, 528, 50604, 50604, 281, 3318, 257, 7605, 11, 1310, 257, 37262, 2445, 11, 281, 264, 1412, 411, 341, 11, 597, 5974, 257, 2744, 50974, 50974, 2031, 293, 611, 2031, 8889, 11, 597, 307, 264, 2744, 6005, 281, 264, 1347, 295, 568, 13, 51272, 51272, 400, 1310, 300, 486, 976, 291, 257, 1101, 3318, 281, 264, 1412, 13, 51456, 51456, 583, 550, 291, 815, 4536, 300, 428, 37262, 2316, 1177, 380, 534, 652, 2020, 11, 570, 257, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.11786064809682417, "compression_ratio": 1.6784140969162995, "no_speech_prob": 4.222776169626741e-06}, {"id": 8, "seek": 2640, "start": 38.6, "end": 44.56, "text": " x and also x squared, which is the size raised to the power of 2.", "tokens": [50364, 467, 1177, 380, 574, 411, 257, 2997, 1622, 9001, 341, 1412, 992, 588, 731, 11, 370, 1310, 291, 528, 50604, 50604, 281, 3318, 257, 7605, 11, 1310, 257, 37262, 2445, 11, 281, 264, 1412, 411, 341, 11, 597, 5974, 257, 2744, 50974, 50974, 2031, 293, 611, 2031, 8889, 11, 597, 307, 264, 2744, 6005, 281, 264, 1347, 295, 568, 13, 51272, 51272, 400, 1310, 300, 486, 976, 291, 257, 1101, 3318, 281, 264, 1412, 13, 51456, 51456, 583, 550, 291, 815, 4536, 300, 428, 37262, 2316, 1177, 380, 534, 652, 2020, 11, 570, 257, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.11786064809682417, "compression_ratio": 1.6784140969162995, "no_speech_prob": 4.222776169626741e-06}, {"id": 9, "seek": 2640, "start": 44.56, "end": 48.239999999999995, "text": " And maybe that will give you a better fit to the data.", "tokens": [50364, 467, 1177, 380, 574, 411, 257, 2997, 1622, 9001, 341, 1412, 992, 588, 731, 11, 370, 1310, 291, 528, 50604, 50604, 281, 3318, 257, 7605, 11, 1310, 257, 37262, 2445, 11, 281, 264, 1412, 411, 341, 11, 597, 5974, 257, 2744, 50974, 50974, 2031, 293, 611, 2031, 8889, 11, 597, 307, 264, 2744, 6005, 281, 264, 1347, 295, 568, 13, 51272, 51272, 400, 1310, 300, 486, 976, 291, 257, 1101, 3318, 281, 264, 1412, 13, 51456, 51456, 583, 550, 291, 815, 4536, 300, 428, 37262, 2316, 1177, 380, 534, 652, 2020, 11, 570, 257, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.11786064809682417, "compression_ratio": 1.6784140969162995, "no_speech_prob": 4.222776169626741e-06}, {"id": 10, "seek": 2640, "start": 48.239999999999995, "end": 51.84, "text": " But then you may decide that your quadratic model doesn't really make sense, because a", "tokens": [50364, 467, 1177, 380, 574, 411, 257, 2997, 1622, 9001, 341, 1412, 992, 588, 731, 11, 370, 1310, 291, 528, 50604, 50604, 281, 3318, 257, 7605, 11, 1310, 257, 37262, 2445, 11, 281, 264, 1412, 411, 341, 11, 597, 5974, 257, 2744, 50974, 50974, 2031, 293, 611, 2031, 8889, 11, 597, 307, 264, 2744, 6005, 281, 264, 1347, 295, 568, 13, 51272, 51272, 400, 1310, 300, 486, 976, 291, 257, 1101, 3318, 281, 264, 1412, 13, 51456, 51456, 583, 550, 291, 815, 4536, 300, 428, 37262, 2316, 1177, 380, 534, 652, 2020, 11, 570, 257, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.11786064809682417, "compression_ratio": 1.6784140969162995, "no_speech_prob": 4.222776169626741e-06}, {"id": 11, "seek": 5184, "start": 51.84, "end": 57.400000000000006, "text": " quadratic function eventually comes back down, and well, we wouldn't really expect housing", "tokens": [50364, 37262, 2445, 4728, 1487, 646, 760, 11, 293, 731, 11, 321, 2759, 380, 534, 2066, 6849, 50642, 50642, 7901, 281, 352, 760, 562, 264, 2744, 8637, 11, 558, 30, 50822, 50822, 5429, 8078, 1643, 411, 436, 820, 2673, 2063, 544, 13, 51016, 51016, 407, 550, 291, 815, 2826, 257, 28733, 2445, 11, 689, 321, 586, 362, 406, 787, 2031, 8889, 11, 457, 2031, 36510, 13, 51405, 51405, 407, 1310, 341, 2316, 14725, 341, 7605, 510, 11, 597, 307, 257, 8344, 1101, 3318, 281, 264, 1412, 11, 51705, 51705], "temperature": 0.0, "avg_logprob": -0.09573114183213975, "compression_ratio": 1.5780590717299579, "no_speech_prob": 3.7852846617170144e-06}, {"id": 12, "seek": 5184, "start": 57.400000000000006, "end": 61.0, "text": " prices to go down when the size increases, right?", "tokens": [50364, 37262, 2445, 4728, 1487, 646, 760, 11, 293, 731, 11, 321, 2759, 380, 534, 2066, 6849, 50642, 50642, 7901, 281, 352, 760, 562, 264, 2744, 8637, 11, 558, 30, 50822, 50822, 5429, 8078, 1643, 411, 436, 820, 2673, 2063, 544, 13, 51016, 51016, 407, 550, 291, 815, 2826, 257, 28733, 2445, 11, 689, 321, 586, 362, 406, 787, 2031, 8889, 11, 457, 2031, 36510, 13, 51405, 51405, 407, 1310, 341, 2316, 14725, 341, 7605, 510, 11, 597, 307, 257, 8344, 1101, 3318, 281, 264, 1412, 11, 51705, 51705], "temperature": 0.0, "avg_logprob": -0.09573114183213975, "compression_ratio": 1.5780590717299579, "no_speech_prob": 3.7852846617170144e-06}, {"id": 13, "seek": 5184, "start": 61.0, "end": 64.88000000000001, "text": " Big houses seem like they should usually cost more.", "tokens": [50364, 37262, 2445, 4728, 1487, 646, 760, 11, 293, 731, 11, 321, 2759, 380, 534, 2066, 6849, 50642, 50642, 7901, 281, 352, 760, 562, 264, 2744, 8637, 11, 558, 30, 50822, 50822, 5429, 8078, 1643, 411, 436, 820, 2673, 2063, 544, 13, 51016, 51016, 407, 550, 291, 815, 2826, 257, 28733, 2445, 11, 689, 321, 586, 362, 406, 787, 2031, 8889, 11, 457, 2031, 36510, 13, 51405, 51405, 407, 1310, 341, 2316, 14725, 341, 7605, 510, 11, 597, 307, 257, 8344, 1101, 3318, 281, 264, 1412, 11, 51705, 51705], "temperature": 0.0, "avg_logprob": -0.09573114183213975, "compression_ratio": 1.5780590717299579, "no_speech_prob": 3.7852846617170144e-06}, {"id": 14, "seek": 5184, "start": 64.88000000000001, "end": 72.66, "text": " So then you may choose a cubic function, where we now have not only x squared, but x cubed.", "tokens": [50364, 37262, 2445, 4728, 1487, 646, 760, 11, 293, 731, 11, 321, 2759, 380, 534, 2066, 6849, 50642, 50642, 7901, 281, 352, 760, 562, 264, 2744, 8637, 11, 558, 30, 50822, 50822, 5429, 8078, 1643, 411, 436, 820, 2673, 2063, 544, 13, 51016, 51016, 407, 550, 291, 815, 2826, 257, 28733, 2445, 11, 689, 321, 586, 362, 406, 787, 2031, 8889, 11, 457, 2031, 36510, 13, 51405, 51405, 407, 1310, 341, 2316, 14725, 341, 7605, 510, 11, 597, 307, 257, 8344, 1101, 3318, 281, 264, 1412, 11, 51705, 51705], "temperature": 0.0, "avg_logprob": -0.09573114183213975, "compression_ratio": 1.5780590717299579, "no_speech_prob": 3.7852846617170144e-06}, {"id": 15, "seek": 5184, "start": 72.66, "end": 78.66, "text": " So maybe this model produces this curve here, which is a somewhat better fit to the data,", "tokens": [50364, 37262, 2445, 4728, 1487, 646, 760, 11, 293, 731, 11, 321, 2759, 380, 534, 2066, 6849, 50642, 50642, 7901, 281, 352, 760, 562, 264, 2744, 8637, 11, 558, 30, 50822, 50822, 5429, 8078, 1643, 411, 436, 820, 2673, 2063, 544, 13, 51016, 51016, 407, 550, 291, 815, 2826, 257, 28733, 2445, 11, 689, 321, 586, 362, 406, 787, 2031, 8889, 11, 457, 2031, 36510, 13, 51405, 51405, 407, 1310, 341, 2316, 14725, 341, 7605, 510, 11, 597, 307, 257, 8344, 1101, 3318, 281, 264, 1412, 11, 51705, 51705], "temperature": 0.0, "avg_logprob": -0.09573114183213975, "compression_ratio": 1.5780590717299579, "no_speech_prob": 3.7852846617170144e-06}, {"id": 16, "seek": 7866, "start": 78.66, "end": 83.8, "text": " because the size does eventually come back up as the size increases.", "tokens": [50364, 570, 264, 2744, 775, 4728, 808, 646, 493, 382, 264, 2744, 8637, 13, 50621, 50621, 1981, 366, 1293, 5110, 295, 26110, 24590, 11, 570, 291, 1890, 428, 17312, 4111, 2031, 293, 50919, 50919, 6005, 309, 281, 264, 1347, 295, 568, 11, 420, 805, 11, 420, 604, 661, 1347, 13, 51173, 51173, 400, 294, 264, 1389, 295, 264, 28733, 2445, 11, 264, 700, 4111, 307, 264, 2744, 11, 264, 1150, 4111, 51406, 51406, 307, 264, 2744, 8889, 11, 293, 264, 2636, 4111, 307, 264, 2744, 36510, 13, 51653, 51653], "temperature": 0.0, "avg_logprob": -0.09532601038614909, "compression_ratio": 1.7488151658767772, "no_speech_prob": 2.123360673067509e-06}, {"id": 17, "seek": 7866, "start": 83.8, "end": 89.75999999999999, "text": " These are both examples of polynomial regression, because you took your optional feature x and", "tokens": [50364, 570, 264, 2744, 775, 4728, 808, 646, 493, 382, 264, 2744, 8637, 13, 50621, 50621, 1981, 366, 1293, 5110, 295, 26110, 24590, 11, 570, 291, 1890, 428, 17312, 4111, 2031, 293, 50919, 50919, 6005, 309, 281, 264, 1347, 295, 568, 11, 420, 805, 11, 420, 604, 661, 1347, 13, 51173, 51173, 400, 294, 264, 1389, 295, 264, 28733, 2445, 11, 264, 700, 4111, 307, 264, 2744, 11, 264, 1150, 4111, 51406, 51406, 307, 264, 2744, 8889, 11, 293, 264, 2636, 4111, 307, 264, 2744, 36510, 13, 51653, 51653], "temperature": 0.0, "avg_logprob": -0.09532601038614909, "compression_ratio": 1.7488151658767772, "no_speech_prob": 2.123360673067509e-06}, {"id": 18, "seek": 7866, "start": 89.75999999999999, "end": 94.84, "text": " raised it to the power of 2, or 3, or any other power.", "tokens": [50364, 570, 264, 2744, 775, 4728, 808, 646, 493, 382, 264, 2744, 8637, 13, 50621, 50621, 1981, 366, 1293, 5110, 295, 26110, 24590, 11, 570, 291, 1890, 428, 17312, 4111, 2031, 293, 50919, 50919, 6005, 309, 281, 264, 1347, 295, 568, 11, 420, 805, 11, 420, 604, 661, 1347, 13, 51173, 51173, 400, 294, 264, 1389, 295, 264, 28733, 2445, 11, 264, 700, 4111, 307, 264, 2744, 11, 264, 1150, 4111, 51406, 51406, 307, 264, 2744, 8889, 11, 293, 264, 2636, 4111, 307, 264, 2744, 36510, 13, 51653, 51653], "temperature": 0.0, "avg_logprob": -0.09532601038614909, "compression_ratio": 1.7488151658767772, "no_speech_prob": 2.123360673067509e-06}, {"id": 19, "seek": 7866, "start": 94.84, "end": 99.5, "text": " And in the case of the cubic function, the first feature is the size, the second feature", "tokens": [50364, 570, 264, 2744, 775, 4728, 808, 646, 493, 382, 264, 2744, 8637, 13, 50621, 50621, 1981, 366, 1293, 5110, 295, 26110, 24590, 11, 570, 291, 1890, 428, 17312, 4111, 2031, 293, 50919, 50919, 6005, 309, 281, 264, 1347, 295, 568, 11, 420, 805, 11, 420, 604, 661, 1347, 13, 51173, 51173, 400, 294, 264, 1389, 295, 264, 28733, 2445, 11, 264, 700, 4111, 307, 264, 2744, 11, 264, 1150, 4111, 51406, 51406, 307, 264, 2744, 8889, 11, 293, 264, 2636, 4111, 307, 264, 2744, 36510, 13, 51653, 51653], "temperature": 0.0, "avg_logprob": -0.09532601038614909, "compression_ratio": 1.7488151658767772, "no_speech_prob": 2.123360673067509e-06}, {"id": 20, "seek": 7866, "start": 99.5, "end": 104.44, "text": " is the size squared, and the third feature is the size cubed.", "tokens": [50364, 570, 264, 2744, 775, 4728, 808, 646, 493, 382, 264, 2744, 8637, 13, 50621, 50621, 1981, 366, 1293, 5110, 295, 26110, 24590, 11, 570, 291, 1890, 428, 17312, 4111, 2031, 293, 50919, 50919, 6005, 309, 281, 264, 1347, 295, 568, 11, 420, 805, 11, 420, 604, 661, 1347, 13, 51173, 51173, 400, 294, 264, 1389, 295, 264, 28733, 2445, 11, 264, 700, 4111, 307, 264, 2744, 11, 264, 1150, 4111, 51406, 51406, 307, 264, 2744, 8889, 11, 293, 264, 2636, 4111, 307, 264, 2744, 36510, 13, 51653, 51653], "temperature": 0.0, "avg_logprob": -0.09532601038614909, "compression_ratio": 1.7488151658767772, "no_speech_prob": 2.123360673067509e-06}, {"id": 21, "seek": 10444, "start": 104.44, "end": 109.48, "text": " I just want to point out one more thing, which is that if you create features that are these", "tokens": [50364, 286, 445, 528, 281, 935, 484, 472, 544, 551, 11, 597, 307, 300, 498, 291, 1884, 4122, 300, 366, 613, 50616, 50616, 8674, 11, 411, 264, 3732, 295, 264, 3380, 4122, 411, 341, 11, 550, 4111, 21589, 3643, 12980, 50958, 50958, 1021, 13, 51026, 51026, 407, 498, 264, 2744, 295, 264, 1782, 22526, 490, 584, 11, 502, 281, 9714, 3732, 3521, 11, 550, 264, 1150, 4111, 11, 51378, 51378, 597, 307, 264, 2744, 8889, 11, 576, 3613, 490, 502, 281, 257, 2459, 11, 293, 264, 2636, 4111, 11, 597, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.10417280914962933, "compression_ratio": 1.808411214953271, "no_speech_prob": 4.092865310667548e-06}, {"id": 22, "seek": 10444, "start": 109.48, "end": 116.32, "text": " powers, like the square of the original features like this, then feature scaling becomes increasingly", "tokens": [50364, 286, 445, 528, 281, 935, 484, 472, 544, 551, 11, 597, 307, 300, 498, 291, 1884, 4122, 300, 366, 613, 50616, 50616, 8674, 11, 411, 264, 3732, 295, 264, 3380, 4122, 411, 341, 11, 550, 4111, 21589, 3643, 12980, 50958, 50958, 1021, 13, 51026, 51026, 407, 498, 264, 2744, 295, 264, 1782, 22526, 490, 584, 11, 502, 281, 9714, 3732, 3521, 11, 550, 264, 1150, 4111, 11, 51378, 51378, 597, 307, 264, 2744, 8889, 11, 576, 3613, 490, 502, 281, 257, 2459, 11, 293, 264, 2636, 4111, 11, 597, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.10417280914962933, "compression_ratio": 1.808411214953271, "no_speech_prob": 4.092865310667548e-06}, {"id": 23, "seek": 10444, "start": 116.32, "end": 117.67999999999999, "text": " important.", "tokens": [50364, 286, 445, 528, 281, 935, 484, 472, 544, 551, 11, 597, 307, 300, 498, 291, 1884, 4122, 300, 366, 613, 50616, 50616, 8674, 11, 411, 264, 3732, 295, 264, 3380, 4122, 411, 341, 11, 550, 4111, 21589, 3643, 12980, 50958, 50958, 1021, 13, 51026, 51026, 407, 498, 264, 2744, 295, 264, 1782, 22526, 490, 584, 11, 502, 281, 9714, 3732, 3521, 11, 550, 264, 1150, 4111, 11, 51378, 51378, 597, 307, 264, 2744, 8889, 11, 576, 3613, 490, 502, 281, 257, 2459, 11, 293, 264, 2636, 4111, 11, 597, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.10417280914962933, "compression_ratio": 1.808411214953271, "no_speech_prob": 4.092865310667548e-06}, {"id": 24, "seek": 10444, "start": 117.67999999999999, "end": 124.72, "text": " So if the size of the house ranges from say, 1 to 1000 square feet, then the second feature,", "tokens": [50364, 286, 445, 528, 281, 935, 484, 472, 544, 551, 11, 597, 307, 300, 498, 291, 1884, 4122, 300, 366, 613, 50616, 50616, 8674, 11, 411, 264, 3732, 295, 264, 3380, 4122, 411, 341, 11, 550, 4111, 21589, 3643, 12980, 50958, 50958, 1021, 13, 51026, 51026, 407, 498, 264, 2744, 295, 264, 1782, 22526, 490, 584, 11, 502, 281, 9714, 3732, 3521, 11, 550, 264, 1150, 4111, 11, 51378, 51378, 597, 307, 264, 2744, 8889, 11, 576, 3613, 490, 502, 281, 257, 2459, 11, 293, 264, 2636, 4111, 11, 597, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.10417280914962933, "compression_ratio": 1.808411214953271, "no_speech_prob": 4.092865310667548e-06}, {"id": 25, "seek": 10444, "start": 124.72, "end": 130.72, "text": " which is the size squared, would range from 1 to a million, and the third feature, which", "tokens": [50364, 286, 445, 528, 281, 935, 484, 472, 544, 551, 11, 597, 307, 300, 498, 291, 1884, 4122, 300, 366, 613, 50616, 50616, 8674, 11, 411, 264, 3732, 295, 264, 3380, 4122, 411, 341, 11, 550, 4111, 21589, 3643, 12980, 50958, 50958, 1021, 13, 51026, 51026, 407, 498, 264, 2744, 295, 264, 1782, 22526, 490, 584, 11, 502, 281, 9714, 3732, 3521, 11, 550, 264, 1150, 4111, 11, 51378, 51378, 597, 307, 264, 2744, 8889, 11, 576, 3613, 490, 502, 281, 257, 2459, 11, 293, 264, 2636, 4111, 11, 597, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.10417280914962933, "compression_ratio": 1.808411214953271, "no_speech_prob": 4.092865310667548e-06}, {"id": 26, "seek": 13072, "start": 130.72, "end": 135.24, "text": " is size cubed, ranges from 1 to a billion.", "tokens": [50364, 307, 2744, 36510, 11, 22526, 490, 502, 281, 257, 5218, 13, 50590, 50590, 407, 613, 732, 4122, 11, 2031, 8889, 293, 2031, 36510, 11, 747, 322, 588, 819, 22526, 295, 4190, 5347, 50900, 50900, 281, 264, 3380, 4111, 2031, 13, 50992, 50992, 400, 498, 291, 434, 1228, 16235, 23475, 11, 309, 311, 1021, 281, 3079, 4111, 21589, 281, 483, 51232, 51232, 428, 4122, 666, 25323, 22526, 295, 4190, 13, 51424, 51424, 6288, 11, 510, 311, 472, 1036, 1365, 295, 577, 291, 534, 362, 257, 4874, 3613, 295, 7994, 295, 4122, 51726, 51726, 281, 764, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.11000618790135239, "compression_ratio": 1.6680672268907564, "no_speech_prob": 1.5779378372826613e-06}, {"id": 27, "seek": 13072, "start": 135.24, "end": 141.44, "text": " So these two features, x squared and x cubed, take on very different ranges of values compared", "tokens": [50364, 307, 2744, 36510, 11, 22526, 490, 502, 281, 257, 5218, 13, 50590, 50590, 407, 613, 732, 4122, 11, 2031, 8889, 293, 2031, 36510, 11, 747, 322, 588, 819, 22526, 295, 4190, 5347, 50900, 50900, 281, 264, 3380, 4111, 2031, 13, 50992, 50992, 400, 498, 291, 434, 1228, 16235, 23475, 11, 309, 311, 1021, 281, 3079, 4111, 21589, 281, 483, 51232, 51232, 428, 4122, 666, 25323, 22526, 295, 4190, 13, 51424, 51424, 6288, 11, 510, 311, 472, 1036, 1365, 295, 577, 291, 534, 362, 257, 4874, 3613, 295, 7994, 295, 4122, 51726, 51726, 281, 764, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.11000618790135239, "compression_ratio": 1.6680672268907564, "no_speech_prob": 1.5779378372826613e-06}, {"id": 28, "seek": 13072, "start": 141.44, "end": 143.28, "text": " to the original feature x.", "tokens": [50364, 307, 2744, 36510, 11, 22526, 490, 502, 281, 257, 5218, 13, 50590, 50590, 407, 613, 732, 4122, 11, 2031, 8889, 293, 2031, 36510, 11, 747, 322, 588, 819, 22526, 295, 4190, 5347, 50900, 50900, 281, 264, 3380, 4111, 2031, 13, 50992, 50992, 400, 498, 291, 434, 1228, 16235, 23475, 11, 309, 311, 1021, 281, 3079, 4111, 21589, 281, 483, 51232, 51232, 428, 4122, 666, 25323, 22526, 295, 4190, 13, 51424, 51424, 6288, 11, 510, 311, 472, 1036, 1365, 295, 577, 291, 534, 362, 257, 4874, 3613, 295, 7994, 295, 4122, 51726, 51726, 281, 764, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.11000618790135239, "compression_ratio": 1.6680672268907564, "no_speech_prob": 1.5779378372826613e-06}, {"id": 29, "seek": 13072, "start": 143.28, "end": 148.07999999999998, "text": " And if you're using gradient descent, it's important to apply feature scaling to get", "tokens": [50364, 307, 2744, 36510, 11, 22526, 490, 502, 281, 257, 5218, 13, 50590, 50590, 407, 613, 732, 4122, 11, 2031, 8889, 293, 2031, 36510, 11, 747, 322, 588, 819, 22526, 295, 4190, 5347, 50900, 50900, 281, 264, 3380, 4111, 2031, 13, 50992, 50992, 400, 498, 291, 434, 1228, 16235, 23475, 11, 309, 311, 1021, 281, 3079, 4111, 21589, 281, 483, 51232, 51232, 428, 4122, 666, 25323, 22526, 295, 4190, 13, 51424, 51424, 6288, 11, 510, 311, 472, 1036, 1365, 295, 577, 291, 534, 362, 257, 4874, 3613, 295, 7994, 295, 4122, 51726, 51726, 281, 764, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.11000618790135239, "compression_ratio": 1.6680672268907564, "no_speech_prob": 1.5779378372826613e-06}, {"id": 30, "seek": 13072, "start": 148.07999999999998, "end": 151.92, "text": " your features into comparable ranges of values.", "tokens": [50364, 307, 2744, 36510, 11, 22526, 490, 502, 281, 257, 5218, 13, 50590, 50590, 407, 613, 732, 4122, 11, 2031, 8889, 293, 2031, 36510, 11, 747, 322, 588, 819, 22526, 295, 4190, 5347, 50900, 50900, 281, 264, 3380, 4111, 2031, 13, 50992, 50992, 400, 498, 291, 434, 1228, 16235, 23475, 11, 309, 311, 1021, 281, 3079, 4111, 21589, 281, 483, 51232, 51232, 428, 4122, 666, 25323, 22526, 295, 4190, 13, 51424, 51424, 6288, 11, 510, 311, 472, 1036, 1365, 295, 577, 291, 534, 362, 257, 4874, 3613, 295, 7994, 295, 4122, 51726, 51726, 281, 764, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.11000618790135239, "compression_ratio": 1.6680672268907564, "no_speech_prob": 1.5779378372826613e-06}, {"id": 31, "seek": 13072, "start": 151.92, "end": 157.96, "text": " Finally, here's one last example of how you really have a wide range of choices of features", "tokens": [50364, 307, 2744, 36510, 11, 22526, 490, 502, 281, 257, 5218, 13, 50590, 50590, 407, 613, 732, 4122, 11, 2031, 8889, 293, 2031, 36510, 11, 747, 322, 588, 819, 22526, 295, 4190, 5347, 50900, 50900, 281, 264, 3380, 4111, 2031, 13, 50992, 50992, 400, 498, 291, 434, 1228, 16235, 23475, 11, 309, 311, 1021, 281, 3079, 4111, 21589, 281, 483, 51232, 51232, 428, 4122, 666, 25323, 22526, 295, 4190, 13, 51424, 51424, 6288, 11, 510, 311, 472, 1036, 1365, 295, 577, 291, 534, 362, 257, 4874, 3613, 295, 7994, 295, 4122, 51726, 51726, 281, 764, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.11000618790135239, "compression_ratio": 1.6680672268907564, "no_speech_prob": 1.5779378372826613e-06}, {"id": 32, "seek": 13072, "start": 157.96, "end": 159.64, "text": " to use.", "tokens": [50364, 307, 2744, 36510, 11, 22526, 490, 502, 281, 257, 5218, 13, 50590, 50590, 407, 613, 732, 4122, 11, 2031, 8889, 293, 2031, 36510, 11, 747, 322, 588, 819, 22526, 295, 4190, 5347, 50900, 50900, 281, 264, 3380, 4111, 2031, 13, 50992, 50992, 400, 498, 291, 434, 1228, 16235, 23475, 11, 309, 311, 1021, 281, 3079, 4111, 21589, 281, 483, 51232, 51232, 428, 4122, 666, 25323, 22526, 295, 4190, 13, 51424, 51424, 6288, 11, 510, 311, 472, 1036, 1365, 295, 577, 291, 534, 362, 257, 4874, 3613, 295, 7994, 295, 4122, 51726, 51726, 281, 764, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.11000618790135239, "compression_ratio": 1.6680672268907564, "no_speech_prob": 1.5779378372826613e-06}, {"id": 33, "seek": 15964, "start": 159.64, "end": 164.79999999999998, "text": " Another reasonable alternative to taking the size squared and size cubed is to say use", "tokens": [50364, 3996, 10585, 8535, 281, 1940, 264, 2744, 8889, 293, 2744, 36510, 307, 281, 584, 764, 50622, 50622, 264, 3732, 5593, 295, 2031, 13, 50722, 50722, 407, 428, 2316, 815, 574, 411, 261, 16, 1413, 2031, 1804, 261, 17, 1413, 264, 3732, 5593, 295, 2031, 1804, 272, 13, 51176, 51176, 440, 3732, 5593, 2445, 1542, 411, 341, 11, 293, 309, 3643, 257, 857, 1570, 16841, 382, 2031, 8637, 11, 51486, 51486, 457, 309, 1177, 380, 1562, 2584, 24183, 484, 11, 293, 309, 3297, 1128, 1562, 1487, 646, 760, 13, 51775, 51775], "temperature": 0.0, "avg_logprob": -0.12076431771983272, "compression_ratio": 1.6926605504587156, "no_speech_prob": 2.684160335775232e-06}, {"id": 34, "seek": 15964, "start": 164.79999999999998, "end": 166.79999999999998, "text": " the square root of x.", "tokens": [50364, 3996, 10585, 8535, 281, 1940, 264, 2744, 8889, 293, 2744, 36510, 307, 281, 584, 764, 50622, 50622, 264, 3732, 5593, 295, 2031, 13, 50722, 50722, 407, 428, 2316, 815, 574, 411, 261, 16, 1413, 2031, 1804, 261, 17, 1413, 264, 3732, 5593, 295, 2031, 1804, 272, 13, 51176, 51176, 440, 3732, 5593, 2445, 1542, 411, 341, 11, 293, 309, 3643, 257, 857, 1570, 16841, 382, 2031, 8637, 11, 51486, 51486, 457, 309, 1177, 380, 1562, 2584, 24183, 484, 11, 293, 309, 3297, 1128, 1562, 1487, 646, 760, 13, 51775, 51775], "temperature": 0.0, "avg_logprob": -0.12076431771983272, "compression_ratio": 1.6926605504587156, "no_speech_prob": 2.684160335775232e-06}, {"id": 35, "seek": 15964, "start": 166.79999999999998, "end": 175.88, "text": " So your model may look like w1 times x plus w2 times the square root of x plus b.", "tokens": [50364, 3996, 10585, 8535, 281, 1940, 264, 2744, 8889, 293, 2744, 36510, 307, 281, 584, 764, 50622, 50622, 264, 3732, 5593, 295, 2031, 13, 50722, 50722, 407, 428, 2316, 815, 574, 411, 261, 16, 1413, 2031, 1804, 261, 17, 1413, 264, 3732, 5593, 295, 2031, 1804, 272, 13, 51176, 51176, 440, 3732, 5593, 2445, 1542, 411, 341, 11, 293, 309, 3643, 257, 857, 1570, 16841, 382, 2031, 8637, 11, 51486, 51486, 457, 309, 1177, 380, 1562, 2584, 24183, 484, 11, 293, 309, 3297, 1128, 1562, 1487, 646, 760, 13, 51775, 51775], "temperature": 0.0, "avg_logprob": -0.12076431771983272, "compression_ratio": 1.6926605504587156, "no_speech_prob": 2.684160335775232e-06}, {"id": 36, "seek": 15964, "start": 175.88, "end": 182.07999999999998, "text": " The square root function looks like this, and it becomes a bit less steep as x increases,", "tokens": [50364, 3996, 10585, 8535, 281, 1940, 264, 2744, 8889, 293, 2744, 36510, 307, 281, 584, 764, 50622, 50622, 264, 3732, 5593, 295, 2031, 13, 50722, 50722, 407, 428, 2316, 815, 574, 411, 261, 16, 1413, 2031, 1804, 261, 17, 1413, 264, 3732, 5593, 295, 2031, 1804, 272, 13, 51176, 51176, 440, 3732, 5593, 2445, 1542, 411, 341, 11, 293, 309, 3643, 257, 857, 1570, 16841, 382, 2031, 8637, 11, 51486, 51486, 457, 309, 1177, 380, 1562, 2584, 24183, 484, 11, 293, 309, 3297, 1128, 1562, 1487, 646, 760, 13, 51775, 51775], "temperature": 0.0, "avg_logprob": -0.12076431771983272, "compression_ratio": 1.6926605504587156, "no_speech_prob": 2.684160335775232e-06}, {"id": 37, "seek": 15964, "start": 182.07999999999998, "end": 187.85999999999999, "text": " but it doesn't ever completely flatten out, and it certainly never ever comes back down.", "tokens": [50364, 3996, 10585, 8535, 281, 1940, 264, 2744, 8889, 293, 2744, 36510, 307, 281, 584, 764, 50622, 50622, 264, 3732, 5593, 295, 2031, 13, 50722, 50722, 407, 428, 2316, 815, 574, 411, 261, 16, 1413, 2031, 1804, 261, 17, 1413, 264, 3732, 5593, 295, 2031, 1804, 272, 13, 51176, 51176, 440, 3732, 5593, 2445, 1542, 411, 341, 11, 293, 309, 3643, 257, 857, 1570, 16841, 382, 2031, 8637, 11, 51486, 51486, 457, 309, 1177, 380, 1562, 2584, 24183, 484, 11, 293, 309, 3297, 1128, 1562, 1487, 646, 760, 13, 51775, 51775], "temperature": 0.0, "avg_logprob": -0.12076431771983272, "compression_ratio": 1.6926605504587156, "no_speech_prob": 2.684160335775232e-06}, {"id": 38, "seek": 18786, "start": 187.86, "end": 193.20000000000002, "text": " So this would be another choice of features that might work well for this dataset as well.", "tokens": [50364, 407, 341, 576, 312, 1071, 3922, 295, 4122, 300, 1062, 589, 731, 337, 341, 28872, 382, 731, 13, 50631, 50631, 407, 291, 815, 1029, 1803, 11, 577, 360, 286, 4536, 437, 4122, 281, 764, 30, 50859, 50859, 11965, 11, 294, 264, 1150, 1164, 294, 341, 2121, 2144, 11, 291, 536, 577, 291, 393, 2826, 819, 4122, 51146, 51146, 293, 819, 5245, 300, 4090, 420, 500, 380, 4090, 613, 4122, 11, 293, 291, 362, 257, 1399, 51421, 51421, 337, 13389, 577, 731, 613, 819, 5245, 2042, 281, 854, 291, 4536, 597, 4122, 51647, 51647, 281, 4090, 420, 406, 4090, 13, 51765, 51765], "temperature": 0.0, "avg_logprob": -0.10758591624139582, "compression_ratio": 1.8461538461538463, "no_speech_prob": 3.28868236465496e-06}, {"id": 39, "seek": 18786, "start": 193.20000000000002, "end": 197.76000000000002, "text": " So you may ask yourself, how do I decide what features to use?", "tokens": [50364, 407, 341, 576, 312, 1071, 3922, 295, 4122, 300, 1062, 589, 731, 337, 341, 28872, 382, 731, 13, 50631, 50631, 407, 291, 815, 1029, 1803, 11, 577, 360, 286, 4536, 437, 4122, 281, 764, 30, 50859, 50859, 11965, 11, 294, 264, 1150, 1164, 294, 341, 2121, 2144, 11, 291, 536, 577, 291, 393, 2826, 819, 4122, 51146, 51146, 293, 819, 5245, 300, 4090, 420, 500, 380, 4090, 613, 4122, 11, 293, 291, 362, 257, 1399, 51421, 51421, 337, 13389, 577, 731, 613, 819, 5245, 2042, 281, 854, 291, 4536, 597, 4122, 51647, 51647, 281, 4090, 420, 406, 4090, 13, 51765, 51765], "temperature": 0.0, "avg_logprob": -0.10758591624139582, "compression_ratio": 1.8461538461538463, "no_speech_prob": 3.28868236465496e-06}, {"id": 40, "seek": 18786, "start": 197.76000000000002, "end": 203.5, "text": " Later, in the second course in this specialization, you see how you can choose different features", "tokens": [50364, 407, 341, 576, 312, 1071, 3922, 295, 4122, 300, 1062, 589, 731, 337, 341, 28872, 382, 731, 13, 50631, 50631, 407, 291, 815, 1029, 1803, 11, 577, 360, 286, 4536, 437, 4122, 281, 764, 30, 50859, 50859, 11965, 11, 294, 264, 1150, 1164, 294, 341, 2121, 2144, 11, 291, 536, 577, 291, 393, 2826, 819, 4122, 51146, 51146, 293, 819, 5245, 300, 4090, 420, 500, 380, 4090, 613, 4122, 11, 293, 291, 362, 257, 1399, 51421, 51421, 337, 13389, 577, 731, 613, 819, 5245, 2042, 281, 854, 291, 4536, 597, 4122, 51647, 51647, 281, 4090, 420, 406, 4090, 13, 51765, 51765], "temperature": 0.0, "avg_logprob": -0.10758591624139582, "compression_ratio": 1.8461538461538463, "no_speech_prob": 3.28868236465496e-06}, {"id": 41, "seek": 18786, "start": 203.5, "end": 209.0, "text": " and different models that include or don't include these features, and you have a process", "tokens": [50364, 407, 341, 576, 312, 1071, 3922, 295, 4122, 300, 1062, 589, 731, 337, 341, 28872, 382, 731, 13, 50631, 50631, 407, 291, 815, 1029, 1803, 11, 577, 360, 286, 4536, 437, 4122, 281, 764, 30, 50859, 50859, 11965, 11, 294, 264, 1150, 1164, 294, 341, 2121, 2144, 11, 291, 536, 577, 291, 393, 2826, 819, 4122, 51146, 51146, 293, 819, 5245, 300, 4090, 420, 500, 380, 4090, 613, 4122, 11, 293, 291, 362, 257, 1399, 51421, 51421, 337, 13389, 577, 731, 613, 819, 5245, 2042, 281, 854, 291, 4536, 597, 4122, 51647, 51647, 281, 4090, 420, 406, 4090, 13, 51765, 51765], "temperature": 0.0, "avg_logprob": -0.10758591624139582, "compression_ratio": 1.8461538461538463, "no_speech_prob": 3.28868236465496e-06}, {"id": 42, "seek": 18786, "start": 209.0, "end": 213.52, "text": " for measuring how well these different models perform to help you decide which features", "tokens": [50364, 407, 341, 576, 312, 1071, 3922, 295, 4122, 300, 1062, 589, 731, 337, 341, 28872, 382, 731, 13, 50631, 50631, 407, 291, 815, 1029, 1803, 11, 577, 360, 286, 4536, 437, 4122, 281, 764, 30, 50859, 50859, 11965, 11, 294, 264, 1150, 1164, 294, 341, 2121, 2144, 11, 291, 536, 577, 291, 393, 2826, 819, 4122, 51146, 51146, 293, 819, 5245, 300, 4090, 420, 500, 380, 4090, 613, 4122, 11, 293, 291, 362, 257, 1399, 51421, 51421, 337, 13389, 577, 731, 613, 819, 5245, 2042, 281, 854, 291, 4536, 597, 4122, 51647, 51647, 281, 4090, 420, 406, 4090, 13, 51765, 51765], "temperature": 0.0, "avg_logprob": -0.10758591624139582, "compression_ratio": 1.8461538461538463, "no_speech_prob": 3.28868236465496e-06}, {"id": 43, "seek": 18786, "start": 213.52, "end": 215.88000000000002, "text": " to include or not include.", "tokens": [50364, 407, 341, 576, 312, 1071, 3922, 295, 4122, 300, 1062, 589, 731, 337, 341, 28872, 382, 731, 13, 50631, 50631, 407, 291, 815, 1029, 1803, 11, 577, 360, 286, 4536, 437, 4122, 281, 764, 30, 50859, 50859, 11965, 11, 294, 264, 1150, 1164, 294, 341, 2121, 2144, 11, 291, 536, 577, 291, 393, 2826, 819, 4122, 51146, 51146, 293, 819, 5245, 300, 4090, 420, 500, 380, 4090, 613, 4122, 11, 293, 291, 362, 257, 1399, 51421, 51421, 337, 13389, 577, 731, 613, 819, 5245, 2042, 281, 854, 291, 4536, 597, 4122, 51647, 51647, 281, 4090, 420, 406, 4090, 13, 51765, 51765], "temperature": 0.0, "avg_logprob": -0.10758591624139582, "compression_ratio": 1.8461538461538463, "no_speech_prob": 3.28868236465496e-06}, {"id": 44, "seek": 21588, "start": 215.88, "end": 221.07999999999998, "text": " For now, I just want you to be aware that you have a choice in what features you use,", "tokens": [50364, 1171, 586, 11, 286, 445, 528, 291, 281, 312, 3650, 300, 291, 362, 257, 3922, 294, 437, 4122, 291, 764, 11, 50624, 50624, 293, 538, 1228, 4111, 7043, 293, 26110, 6828, 11, 291, 393, 7263, 483, 257, 709, 50860, 50860, 1101, 2316, 337, 428, 1412, 13, 51020, 51020, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 486, 536, 512, 3089, 300, 704, 17988, 26110, 51294, 51294, 24590, 1228, 4122, 411, 2031, 11, 2031, 8889, 11, 293, 2031, 36510, 13, 51506, 51506, 407, 1767, 747, 257, 574, 293, 1190, 264, 3089, 293, 536, 577, 309, 1985, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.08092112634696212, "compression_ratio": 1.668, "no_speech_prob": 3.288665311629302e-06}, {"id": 45, "seek": 21588, "start": 221.07999999999998, "end": 225.79999999999998, "text": " and by using feature engineering and polynomial functions, you can potentially get a much", "tokens": [50364, 1171, 586, 11, 286, 445, 528, 291, 281, 312, 3650, 300, 291, 362, 257, 3922, 294, 437, 4122, 291, 764, 11, 50624, 50624, 293, 538, 1228, 4111, 7043, 293, 26110, 6828, 11, 291, 393, 7263, 483, 257, 709, 50860, 50860, 1101, 2316, 337, 428, 1412, 13, 51020, 51020, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 486, 536, 512, 3089, 300, 704, 17988, 26110, 51294, 51294, 24590, 1228, 4122, 411, 2031, 11, 2031, 8889, 11, 293, 2031, 36510, 13, 51506, 51506, 407, 1767, 747, 257, 574, 293, 1190, 264, 3089, 293, 536, 577, 309, 1985, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.08092112634696212, "compression_ratio": 1.668, "no_speech_prob": 3.288665311629302e-06}, {"id": 46, "seek": 21588, "start": 225.79999999999998, "end": 229.0, "text": " better model for your data.", "tokens": [50364, 1171, 586, 11, 286, 445, 528, 291, 281, 312, 3650, 300, 291, 362, 257, 3922, 294, 437, 4122, 291, 764, 11, 50624, 50624, 293, 538, 1228, 4111, 7043, 293, 26110, 6828, 11, 291, 393, 7263, 483, 257, 709, 50860, 50860, 1101, 2316, 337, 428, 1412, 13, 51020, 51020, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 486, 536, 512, 3089, 300, 704, 17988, 26110, 51294, 51294, 24590, 1228, 4122, 411, 2031, 11, 2031, 8889, 11, 293, 2031, 36510, 13, 51506, 51506, 407, 1767, 747, 257, 574, 293, 1190, 264, 3089, 293, 536, 577, 309, 1985, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.08092112634696212, "compression_ratio": 1.668, "no_speech_prob": 3.288665311629302e-06}, {"id": 47, "seek": 21588, "start": 229.0, "end": 234.48, "text": " In the optional lab that follows this video, you will see some code that implements polynomial", "tokens": [50364, 1171, 586, 11, 286, 445, 528, 291, 281, 312, 3650, 300, 291, 362, 257, 3922, 294, 437, 4122, 291, 764, 11, 50624, 50624, 293, 538, 1228, 4111, 7043, 293, 26110, 6828, 11, 291, 393, 7263, 483, 257, 709, 50860, 50860, 1101, 2316, 337, 428, 1412, 13, 51020, 51020, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 486, 536, 512, 3089, 300, 704, 17988, 26110, 51294, 51294, 24590, 1228, 4122, 411, 2031, 11, 2031, 8889, 11, 293, 2031, 36510, 13, 51506, 51506, 407, 1767, 747, 257, 574, 293, 1190, 264, 3089, 293, 536, 577, 309, 1985, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.08092112634696212, "compression_ratio": 1.668, "no_speech_prob": 3.288665311629302e-06}, {"id": 48, "seek": 21588, "start": 234.48, "end": 238.72, "text": " regression using features like x, x squared, and x cubed.", "tokens": [50364, 1171, 586, 11, 286, 445, 528, 291, 281, 312, 3650, 300, 291, 362, 257, 3922, 294, 437, 4122, 291, 764, 11, 50624, 50624, 293, 538, 1228, 4111, 7043, 293, 26110, 6828, 11, 291, 393, 7263, 483, 257, 709, 50860, 50860, 1101, 2316, 337, 428, 1412, 13, 51020, 51020, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 486, 536, 512, 3089, 300, 704, 17988, 26110, 51294, 51294, 24590, 1228, 4122, 411, 2031, 11, 2031, 8889, 11, 293, 2031, 36510, 13, 51506, 51506, 407, 1767, 747, 257, 574, 293, 1190, 264, 3089, 293, 536, 577, 309, 1985, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.08092112634696212, "compression_ratio": 1.668, "no_speech_prob": 3.288665311629302e-06}, {"id": 49, "seek": 21588, "start": 238.72, "end": 243.84, "text": " So please take a look and run the code and see how it works.", "tokens": [50364, 1171, 586, 11, 286, 445, 528, 291, 281, 312, 3650, 300, 291, 362, 257, 3922, 294, 437, 4122, 291, 764, 11, 50624, 50624, 293, 538, 1228, 4111, 7043, 293, 26110, 6828, 11, 291, 393, 7263, 483, 257, 709, 50860, 50860, 1101, 2316, 337, 428, 1412, 13, 51020, 51020, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 486, 536, 512, 3089, 300, 704, 17988, 26110, 51294, 51294, 24590, 1228, 4122, 411, 2031, 11, 2031, 8889, 11, 293, 2031, 36510, 13, 51506, 51506, 407, 1767, 747, 257, 574, 293, 1190, 264, 3089, 293, 536, 577, 309, 1985, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.08092112634696212, "compression_ratio": 1.668, "no_speech_prob": 3.288665311629302e-06}, {"id": 50, "seek": 24384, "start": 243.84, "end": 248.72, "text": " There's also another optional lab after that one that shows how to use a popular open source", "tokens": [50364, 821, 311, 611, 1071, 17312, 2715, 934, 300, 472, 300, 3110, 577, 281, 764, 257, 3743, 1269, 4009, 50608, 50608, 40167, 300, 704, 17988, 8213, 24590, 13, 50868, 50868, 16942, 22681, 12, 306, 1083, 307, 257, 588, 13371, 1143, 1269, 4009, 3479, 2539, 6405, 300, 307, 1143, 538, 867, 51136, 51136, 25742, 294, 867, 295, 264, 1192, 7318, 11, 4705, 11, 3479, 2539, 3431, 294, 264, 1002, 13, 51518, 51518, 407, 498, 2139, 586, 420, 294, 264, 2027, 291, 434, 1228, 3479, 2539, 294, 428, 1691, 11, 456, 311, 257, 588, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.09013809404875103, "compression_ratio": 1.7361702127659575, "no_speech_prob": 1.2606354175659362e-05}, {"id": 51, "seek": 24384, "start": 248.72, "end": 253.92000000000002, "text": " toolkit that implements linear regression.", "tokens": [50364, 821, 311, 611, 1071, 17312, 2715, 934, 300, 472, 300, 3110, 577, 281, 764, 257, 3743, 1269, 4009, 50608, 50608, 40167, 300, 704, 17988, 8213, 24590, 13, 50868, 50868, 16942, 22681, 12, 306, 1083, 307, 257, 588, 13371, 1143, 1269, 4009, 3479, 2539, 6405, 300, 307, 1143, 538, 867, 51136, 51136, 25742, 294, 867, 295, 264, 1192, 7318, 11, 4705, 11, 3479, 2539, 3431, 294, 264, 1002, 13, 51518, 51518, 407, 498, 2139, 586, 420, 294, 264, 2027, 291, 434, 1228, 3479, 2539, 294, 428, 1691, 11, 456, 311, 257, 588, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.09013809404875103, "compression_ratio": 1.7361702127659575, "no_speech_prob": 1.2606354175659362e-05}, {"id": 52, "seek": 24384, "start": 253.92000000000002, "end": 259.28000000000003, "text": " Scikit-learn is a very widely used open source machine learning library that is used by many", "tokens": [50364, 821, 311, 611, 1071, 17312, 2715, 934, 300, 472, 300, 3110, 577, 281, 764, 257, 3743, 1269, 4009, 50608, 50608, 40167, 300, 704, 17988, 8213, 24590, 13, 50868, 50868, 16942, 22681, 12, 306, 1083, 307, 257, 588, 13371, 1143, 1269, 4009, 3479, 2539, 6405, 300, 307, 1143, 538, 867, 51136, 51136, 25742, 294, 867, 295, 264, 1192, 7318, 11, 4705, 11, 3479, 2539, 3431, 294, 264, 1002, 13, 51518, 51518, 407, 498, 2139, 586, 420, 294, 264, 2027, 291, 434, 1228, 3479, 2539, 294, 428, 1691, 11, 456, 311, 257, 588, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.09013809404875103, "compression_ratio": 1.7361702127659575, "no_speech_prob": 1.2606354175659362e-05}, {"id": 53, "seek": 24384, "start": 259.28000000000003, "end": 266.92, "text": " practitioners in many of the top AI, internet, machine learning companies in the world.", "tokens": [50364, 821, 311, 611, 1071, 17312, 2715, 934, 300, 472, 300, 3110, 577, 281, 764, 257, 3743, 1269, 4009, 50608, 50608, 40167, 300, 704, 17988, 8213, 24590, 13, 50868, 50868, 16942, 22681, 12, 306, 1083, 307, 257, 588, 13371, 1143, 1269, 4009, 3479, 2539, 6405, 300, 307, 1143, 538, 867, 51136, 51136, 25742, 294, 867, 295, 264, 1192, 7318, 11, 4705, 11, 3479, 2539, 3431, 294, 264, 1002, 13, 51518, 51518, 407, 498, 2139, 586, 420, 294, 264, 2027, 291, 434, 1228, 3479, 2539, 294, 428, 1691, 11, 456, 311, 257, 588, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.09013809404875103, "compression_ratio": 1.7361702127659575, "no_speech_prob": 1.2606354175659362e-05}, {"id": 54, "seek": 24384, "start": 266.92, "end": 271.68, "text": " So if either now or in the future you're using machine learning in your job, there's a very", "tokens": [50364, 821, 311, 611, 1071, 17312, 2715, 934, 300, 472, 300, 3110, 577, 281, 764, 257, 3743, 1269, 4009, 50608, 50608, 40167, 300, 704, 17988, 8213, 24590, 13, 50868, 50868, 16942, 22681, 12, 306, 1083, 307, 257, 588, 13371, 1143, 1269, 4009, 3479, 2539, 6405, 300, 307, 1143, 538, 867, 51136, 51136, 25742, 294, 867, 295, 264, 1192, 7318, 11, 4705, 11, 3479, 2539, 3431, 294, 264, 1002, 13, 51518, 51518, 407, 498, 2139, 586, 420, 294, 264, 2027, 291, 434, 1228, 3479, 2539, 294, 428, 1691, 11, 456, 311, 257, 588, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.09013809404875103, "compression_ratio": 1.7361702127659575, "no_speech_prob": 1.2606354175659362e-05}, {"id": 55, "seek": 27168, "start": 271.68, "end": 277.76, "text": " good chance you'll be using tools like Scikit-learn to train your models.", "tokens": [50364, 665, 2931, 291, 603, 312, 1228, 3873, 411, 16942, 22681, 12, 306, 1083, 281, 3847, 428, 5245, 13, 50668, 50668, 400, 370, 1364, 807, 300, 17312, 2715, 486, 976, 291, 257, 2931, 281, 406, 787, 1101, 1223, 50910, 50910, 8213, 24590, 11, 457, 611, 536, 577, 341, 393, 312, 1096, 294, 445, 257, 1326, 3876, 295, 3089, 1228, 51180, 51180, 257, 6405, 411, 16942, 22681, 12, 306, 1083, 13, 51346, 51346, 1171, 291, 281, 362, 257, 5100, 3701, 295, 613, 14642, 293, 312, 1075, 281, 3079, 552, 11, 286, 360, 51606, 51606, 519, 309, 311, 1021, 300, 291, 458, 577, 281, 4445, 8213, 24590, 1803, 11, 293, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.06915764336113457, "compression_ratio": 1.6988847583643123, "no_speech_prob": 4.356796125648543e-06}, {"id": 56, "seek": 27168, "start": 277.76, "end": 282.6, "text": " And so working through that optional lab will give you a chance to not only better understand", "tokens": [50364, 665, 2931, 291, 603, 312, 1228, 3873, 411, 16942, 22681, 12, 306, 1083, 281, 3847, 428, 5245, 13, 50668, 50668, 400, 370, 1364, 807, 300, 17312, 2715, 486, 976, 291, 257, 2931, 281, 406, 787, 1101, 1223, 50910, 50910, 8213, 24590, 11, 457, 611, 536, 577, 341, 393, 312, 1096, 294, 445, 257, 1326, 3876, 295, 3089, 1228, 51180, 51180, 257, 6405, 411, 16942, 22681, 12, 306, 1083, 13, 51346, 51346, 1171, 291, 281, 362, 257, 5100, 3701, 295, 613, 14642, 293, 312, 1075, 281, 3079, 552, 11, 286, 360, 51606, 51606, 519, 309, 311, 1021, 300, 291, 458, 577, 281, 4445, 8213, 24590, 1803, 11, 293, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.06915764336113457, "compression_ratio": 1.6988847583643123, "no_speech_prob": 4.356796125648543e-06}, {"id": 57, "seek": 27168, "start": 282.6, "end": 288.0, "text": " linear regression, but also see how this can be done in just a few lines of code using", "tokens": [50364, 665, 2931, 291, 603, 312, 1228, 3873, 411, 16942, 22681, 12, 306, 1083, 281, 3847, 428, 5245, 13, 50668, 50668, 400, 370, 1364, 807, 300, 17312, 2715, 486, 976, 291, 257, 2931, 281, 406, 787, 1101, 1223, 50910, 50910, 8213, 24590, 11, 457, 611, 536, 577, 341, 393, 312, 1096, 294, 445, 257, 1326, 3876, 295, 3089, 1228, 51180, 51180, 257, 6405, 411, 16942, 22681, 12, 306, 1083, 13, 51346, 51346, 1171, 291, 281, 362, 257, 5100, 3701, 295, 613, 14642, 293, 312, 1075, 281, 3079, 552, 11, 286, 360, 51606, 51606, 519, 309, 311, 1021, 300, 291, 458, 577, 281, 4445, 8213, 24590, 1803, 11, 293, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.06915764336113457, "compression_ratio": 1.6988847583643123, "no_speech_prob": 4.356796125648543e-06}, {"id": 58, "seek": 27168, "start": 288.0, "end": 291.32, "text": " a library like Scikit-learn.", "tokens": [50364, 665, 2931, 291, 603, 312, 1228, 3873, 411, 16942, 22681, 12, 306, 1083, 281, 3847, 428, 5245, 13, 50668, 50668, 400, 370, 1364, 807, 300, 17312, 2715, 486, 976, 291, 257, 2931, 281, 406, 787, 1101, 1223, 50910, 50910, 8213, 24590, 11, 457, 611, 536, 577, 341, 393, 312, 1096, 294, 445, 257, 1326, 3876, 295, 3089, 1228, 51180, 51180, 257, 6405, 411, 16942, 22681, 12, 306, 1083, 13, 51346, 51346, 1171, 291, 281, 362, 257, 5100, 3701, 295, 613, 14642, 293, 312, 1075, 281, 3079, 552, 11, 286, 360, 51606, 51606, 519, 309, 311, 1021, 300, 291, 458, 577, 281, 4445, 8213, 24590, 1803, 11, 293, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.06915764336113457, "compression_ratio": 1.6988847583643123, "no_speech_prob": 4.356796125648543e-06}, {"id": 59, "seek": 27168, "start": 291.32, "end": 296.52, "text": " For you to have a solid understanding of these algorithms and be able to apply them, I do", "tokens": [50364, 665, 2931, 291, 603, 312, 1228, 3873, 411, 16942, 22681, 12, 306, 1083, 281, 3847, 428, 5245, 13, 50668, 50668, 400, 370, 1364, 807, 300, 17312, 2715, 486, 976, 291, 257, 2931, 281, 406, 787, 1101, 1223, 50910, 50910, 8213, 24590, 11, 457, 611, 536, 577, 341, 393, 312, 1096, 294, 445, 257, 1326, 3876, 295, 3089, 1228, 51180, 51180, 257, 6405, 411, 16942, 22681, 12, 306, 1083, 13, 51346, 51346, 1171, 291, 281, 362, 257, 5100, 3701, 295, 613, 14642, 293, 312, 1075, 281, 3079, 552, 11, 286, 360, 51606, 51606, 519, 309, 311, 1021, 300, 291, 458, 577, 281, 4445, 8213, 24590, 1803, 11, 293, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.06915764336113457, "compression_ratio": 1.6988847583643123, "no_speech_prob": 4.356796125648543e-06}, {"id": 60, "seek": 27168, "start": 296.52, "end": 301.12, "text": " think it's important that you know how to implement linear regression yourself, and", "tokens": [50364, 665, 2931, 291, 603, 312, 1228, 3873, 411, 16942, 22681, 12, 306, 1083, 281, 3847, 428, 5245, 13, 50668, 50668, 400, 370, 1364, 807, 300, 17312, 2715, 486, 976, 291, 257, 2931, 281, 406, 787, 1101, 1223, 50910, 50910, 8213, 24590, 11, 457, 611, 536, 577, 341, 393, 312, 1096, 294, 445, 257, 1326, 3876, 295, 3089, 1228, 51180, 51180, 257, 6405, 411, 16942, 22681, 12, 306, 1083, 13, 51346, 51346, 1171, 291, 281, 362, 257, 5100, 3701, 295, 613, 14642, 293, 312, 1075, 281, 3079, 552, 11, 286, 360, 51606, 51606, 519, 309, 311, 1021, 300, 291, 458, 577, 281, 4445, 8213, 24590, 1803, 11, 293, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.06915764336113457, "compression_ratio": 1.6988847583643123, "no_speech_prob": 4.356796125648543e-06}, {"id": 61, "seek": 30112, "start": 301.12, "end": 305.12, "text": " not just call some Scikit-learn function that is a black box.", "tokens": [50364, 406, 445, 818, 512, 16942, 22681, 12, 306, 1083, 2445, 300, 307, 257, 2211, 2424, 13, 50564, 50564, 583, 16942, 22681, 12, 306, 1083, 611, 575, 364, 1021, 3090, 294, 264, 636, 3479, 2539, 307, 1096, 294, 3124, 50804, 50804, 965, 13, 50854, 50854, 407, 11, 321, 434, 445, 466, 412, 264, 917, 295, 341, 1243, 13, 51060, 51060, 9694, 322, 12693, 439, 295, 341, 1243, 311, 2145, 13, 51228, 51228, 2555, 360, 747, 257, 574, 412, 264, 3124, 48955, 293, 611, 264, 3124, 2715, 11, 597, 286, 1454, 486, 51470, 51470, 718, 291, 853, 484, 293, 3124, 3487, 300, 321, 600, 7152, 13, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.12247745196024577, "compression_ratio": 1.6599190283400809, "no_speech_prob": 7.646219273738097e-06}, {"id": 62, "seek": 30112, "start": 305.12, "end": 309.92, "text": " But Scikit-learn also has an important role in the way machine learning is done in practice", "tokens": [50364, 406, 445, 818, 512, 16942, 22681, 12, 306, 1083, 2445, 300, 307, 257, 2211, 2424, 13, 50564, 50564, 583, 16942, 22681, 12, 306, 1083, 611, 575, 364, 1021, 3090, 294, 264, 636, 3479, 2539, 307, 1096, 294, 3124, 50804, 50804, 965, 13, 50854, 50854, 407, 11, 321, 434, 445, 466, 412, 264, 917, 295, 341, 1243, 13, 51060, 51060, 9694, 322, 12693, 439, 295, 341, 1243, 311, 2145, 13, 51228, 51228, 2555, 360, 747, 257, 574, 412, 264, 3124, 48955, 293, 611, 264, 3124, 2715, 11, 597, 286, 1454, 486, 51470, 51470, 718, 291, 853, 484, 293, 3124, 3487, 300, 321, 600, 7152, 13, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.12247745196024577, "compression_ratio": 1.6599190283400809, "no_speech_prob": 7.646219273738097e-06}, {"id": 63, "seek": 30112, "start": 309.92, "end": 310.92, "text": " today.", "tokens": [50364, 406, 445, 818, 512, 16942, 22681, 12, 306, 1083, 2445, 300, 307, 257, 2211, 2424, 13, 50564, 50564, 583, 16942, 22681, 12, 306, 1083, 611, 575, 364, 1021, 3090, 294, 264, 636, 3479, 2539, 307, 1096, 294, 3124, 50804, 50804, 965, 13, 50854, 50854, 407, 11, 321, 434, 445, 466, 412, 264, 917, 295, 341, 1243, 13, 51060, 51060, 9694, 322, 12693, 439, 295, 341, 1243, 311, 2145, 13, 51228, 51228, 2555, 360, 747, 257, 574, 412, 264, 3124, 48955, 293, 611, 264, 3124, 2715, 11, 597, 286, 1454, 486, 51470, 51470, 718, 291, 853, 484, 293, 3124, 3487, 300, 321, 600, 7152, 13, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.12247745196024577, "compression_ratio": 1.6599190283400809, "no_speech_prob": 7.646219273738097e-06}, {"id": 64, "seek": 30112, "start": 310.92, "end": 315.04, "text": " So, we're just about at the end of this week.", "tokens": [50364, 406, 445, 818, 512, 16942, 22681, 12, 306, 1083, 2445, 300, 307, 257, 2211, 2424, 13, 50564, 50564, 583, 16942, 22681, 12, 306, 1083, 611, 575, 364, 1021, 3090, 294, 264, 636, 3479, 2539, 307, 1096, 294, 3124, 50804, 50804, 965, 13, 50854, 50854, 407, 11, 321, 434, 445, 466, 412, 264, 917, 295, 341, 1243, 13, 51060, 51060, 9694, 322, 12693, 439, 295, 341, 1243, 311, 2145, 13, 51228, 51228, 2555, 360, 747, 257, 574, 412, 264, 3124, 48955, 293, 611, 264, 3124, 2715, 11, 597, 286, 1454, 486, 51470, 51470, 718, 291, 853, 484, 293, 3124, 3487, 300, 321, 600, 7152, 13, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.12247745196024577, "compression_ratio": 1.6599190283400809, "no_speech_prob": 7.646219273738097e-06}, {"id": 65, "seek": 30112, "start": 315.04, "end": 318.4, "text": " Congratulations on finishing all of this week's videos.", "tokens": [50364, 406, 445, 818, 512, 16942, 22681, 12, 306, 1083, 2445, 300, 307, 257, 2211, 2424, 13, 50564, 50564, 583, 16942, 22681, 12, 306, 1083, 611, 575, 364, 1021, 3090, 294, 264, 636, 3479, 2539, 307, 1096, 294, 3124, 50804, 50804, 965, 13, 50854, 50854, 407, 11, 321, 434, 445, 466, 412, 264, 917, 295, 341, 1243, 13, 51060, 51060, 9694, 322, 12693, 439, 295, 341, 1243, 311, 2145, 13, 51228, 51228, 2555, 360, 747, 257, 574, 412, 264, 3124, 48955, 293, 611, 264, 3124, 2715, 11, 597, 286, 1454, 486, 51470, 51470, 718, 291, 853, 484, 293, 3124, 3487, 300, 321, 600, 7152, 13, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.12247745196024577, "compression_ratio": 1.6599190283400809, "no_speech_prob": 7.646219273738097e-06}, {"id": 66, "seek": 30112, "start": 318.4, "end": 323.24, "text": " Please do take a look at the practice quizzes and also the practice lab, which I hope will", "tokens": [50364, 406, 445, 818, 512, 16942, 22681, 12, 306, 1083, 2445, 300, 307, 257, 2211, 2424, 13, 50564, 50564, 583, 16942, 22681, 12, 306, 1083, 611, 575, 364, 1021, 3090, 294, 264, 636, 3479, 2539, 307, 1096, 294, 3124, 50804, 50804, 965, 13, 50854, 50854, 407, 11, 321, 434, 445, 466, 412, 264, 917, 295, 341, 1243, 13, 51060, 51060, 9694, 322, 12693, 439, 295, 341, 1243, 311, 2145, 13, 51228, 51228, 2555, 360, 747, 257, 574, 412, 264, 3124, 48955, 293, 611, 264, 3124, 2715, 11, 597, 286, 1454, 486, 51470, 51470, 718, 291, 853, 484, 293, 3124, 3487, 300, 321, 600, 7152, 13, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.12247745196024577, "compression_ratio": 1.6599190283400809, "no_speech_prob": 7.646219273738097e-06}, {"id": 67, "seek": 30112, "start": 323.24, "end": 327.68, "text": " let you try out and practice ideas that we've discussed.", "tokens": [50364, 406, 445, 818, 512, 16942, 22681, 12, 306, 1083, 2445, 300, 307, 257, 2211, 2424, 13, 50564, 50564, 583, 16942, 22681, 12, 306, 1083, 611, 575, 364, 1021, 3090, 294, 264, 636, 3479, 2539, 307, 1096, 294, 3124, 50804, 50804, 965, 13, 50854, 50854, 407, 11, 321, 434, 445, 466, 412, 264, 917, 295, 341, 1243, 13, 51060, 51060, 9694, 322, 12693, 439, 295, 341, 1243, 311, 2145, 13, 51228, 51228, 2555, 360, 747, 257, 574, 412, 264, 3124, 48955, 293, 611, 264, 3124, 2715, 11, 597, 286, 1454, 486, 51470, 51470, 718, 291, 853, 484, 293, 3124, 3487, 300, 321, 600, 7152, 13, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.12247745196024577, "compression_ratio": 1.6599190283400809, "no_speech_prob": 7.646219273738097e-06}, {"id": 68, "seek": 32768, "start": 327.68, "end": 331.54, "text": " In this week's practice lab, you implement linear regression.", "tokens": [50364, 682, 341, 1243, 311, 3124, 2715, 11, 291, 4445, 8213, 24590, 13, 50557, 50557, 286, 1454, 291, 362, 257, 688, 295, 1019, 1242, 341, 2539, 9284, 281, 589, 337, 1803, 13, 50810, 50810, 9752, 295, 3668, 365, 300, 11, 293, 286, 611, 574, 2128, 281, 2577, 291, 294, 958, 1243, 311, 2145, 11, 689, 51080, 51080, 321, 603, 352, 4399, 24590, 11, 300, 307, 32884, 3547, 11, 281, 751, 466, 527, 700, 21538, 51328, 51328, 9284, 11, 597, 393, 6069, 10479, 13, 51466, 51466, 286, 603, 536, 291, 958, 1243, 13, 51532], "temperature": 0.0, "avg_logprob": -0.15057812345788835, "compression_ratio": 1.6596638655462186, "no_speech_prob": 2.353019590373151e-05}, {"id": 69, "seek": 32768, "start": 331.54, "end": 336.6, "text": " I hope you have a lot of fun getting this learning algorithm to work for yourself.", "tokens": [50364, 682, 341, 1243, 311, 3124, 2715, 11, 291, 4445, 8213, 24590, 13, 50557, 50557, 286, 1454, 291, 362, 257, 688, 295, 1019, 1242, 341, 2539, 9284, 281, 589, 337, 1803, 13, 50810, 50810, 9752, 295, 3668, 365, 300, 11, 293, 286, 611, 574, 2128, 281, 2577, 291, 294, 958, 1243, 311, 2145, 11, 689, 51080, 51080, 321, 603, 352, 4399, 24590, 11, 300, 307, 32884, 3547, 11, 281, 751, 466, 527, 700, 21538, 51328, 51328, 9284, 11, 597, 393, 6069, 10479, 13, 51466, 51466, 286, 603, 536, 291, 958, 1243, 13, 51532], "temperature": 0.0, "avg_logprob": -0.15057812345788835, "compression_ratio": 1.6596638655462186, "no_speech_prob": 2.353019590373151e-05}, {"id": 70, "seek": 32768, "start": 336.6, "end": 342.0, "text": " Best of luck with that, and I also look forward to seeing you in next week's videos, where", "tokens": [50364, 682, 341, 1243, 311, 3124, 2715, 11, 291, 4445, 8213, 24590, 13, 50557, 50557, 286, 1454, 291, 362, 257, 688, 295, 1019, 1242, 341, 2539, 9284, 281, 589, 337, 1803, 13, 50810, 50810, 9752, 295, 3668, 365, 300, 11, 293, 286, 611, 574, 2128, 281, 2577, 291, 294, 958, 1243, 311, 2145, 11, 689, 51080, 51080, 321, 603, 352, 4399, 24590, 11, 300, 307, 32884, 3547, 11, 281, 751, 466, 527, 700, 21538, 51328, 51328, 9284, 11, 597, 393, 6069, 10479, 13, 51466, 51466, 286, 603, 536, 291, 958, 1243, 13, 51532], "temperature": 0.0, "avg_logprob": -0.15057812345788835, "compression_ratio": 1.6596638655462186, "no_speech_prob": 2.353019590373151e-05}, {"id": 71, "seek": 32768, "start": 342.0, "end": 346.96000000000004, "text": " we'll go beyond regression, that is predicting numbers, to talk about our first classification", "tokens": [50364, 682, 341, 1243, 311, 3124, 2715, 11, 291, 4445, 8213, 24590, 13, 50557, 50557, 286, 1454, 291, 362, 257, 688, 295, 1019, 1242, 341, 2539, 9284, 281, 589, 337, 1803, 13, 50810, 50810, 9752, 295, 3668, 365, 300, 11, 293, 286, 611, 574, 2128, 281, 2577, 291, 294, 958, 1243, 311, 2145, 11, 689, 51080, 51080, 321, 603, 352, 4399, 24590, 11, 300, 307, 32884, 3547, 11, 281, 751, 466, 527, 700, 21538, 51328, 51328, 9284, 11, 597, 393, 6069, 10479, 13, 51466, 51466, 286, 603, 536, 291, 958, 1243, 13, 51532], "temperature": 0.0, "avg_logprob": -0.15057812345788835, "compression_ratio": 1.6596638655462186, "no_speech_prob": 2.353019590373151e-05}, {"id": 72, "seek": 32768, "start": 346.96000000000004, "end": 349.72, "text": " algorithm, which can predict categories.", "tokens": [50364, 682, 341, 1243, 311, 3124, 2715, 11, 291, 4445, 8213, 24590, 13, 50557, 50557, 286, 1454, 291, 362, 257, 688, 295, 1019, 1242, 341, 2539, 9284, 281, 589, 337, 1803, 13, 50810, 50810, 9752, 295, 3668, 365, 300, 11, 293, 286, 611, 574, 2128, 281, 2577, 291, 294, 958, 1243, 311, 2145, 11, 689, 51080, 51080, 321, 603, 352, 4399, 24590, 11, 300, 307, 32884, 3547, 11, 281, 751, 466, 527, 700, 21538, 51328, 51328, 9284, 11, 597, 393, 6069, 10479, 13, 51466, 51466, 286, 603, 536, 291, 958, 1243, 13, 51532], "temperature": 0.0, "avg_logprob": -0.15057812345788835, "compression_ratio": 1.6596638655462186, "no_speech_prob": 2.353019590373151e-05}, {"id": 73, "seek": 34972, "start": 349.72, "end": 358.12, "text": " I'll see you next week.", "tokens": [50364, 286, 603, 536, 291, 958, 1243, 13, 50784], "temperature": 0.0, "avg_logprob": -0.5387880325317382, "compression_ratio": 0.7419354838709677, "no_speech_prob": 0.00042749964632093906}], "language": "en", "video_id": "UobsU2oQwBA", "entity": "ML Specialization, Andrew Ng (2022)"}}