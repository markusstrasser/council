{"video_id": "wB08Jlmhi24", "title": "3.5 Cost Function | Simplified Cost Function for Logistic Regression --[Machine Learning|Andrew Ng]", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 345, "views": 184, "publish_date": "11/04/2022", "timestamp": 1661126400, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " In the last video, you saw the loss function and the cost function for logistic regression. In this video, you'll see a slightly simpler way to write out the loss and cost functions so that the implementation can be a bit simpler when we get to gradient descent for fitting the parameters of a logistic regression model. Let's take a look. As a reminder, here's the loss function that we had defined in the previous video for logistic regression. Now, because we're still working on the binary classification problem, y is either 0 or 1. Because y is either 0 or 1 and cannot take on any value other than 0 or 1, we'll be able to come up with a simpler way to write this loss function. You can write the loss function as follows. Given the prediction f of x and the target label y, the loss equals negative y times log of f minus 1 minus y times log of 1 minus f. And it turns out this equation, which we just wrote in one line, is completely equivalent to this more complex formula up here. Let's see why this is the case. Now remember, y can only take on the values of either 1 or 0. In the first case, let's say y equals 1. This first y over here is 1, and this 1 minus y is 1 minus 1, which is therefore equal to 0. And so the loss becomes negative 1 times log of f of x minus 0 times a bunch of stuff that becomes 0 and goes away. And so when y is equal to 1, the loss is indeed the first term on top, negative log of f of x. Let's look at the second case, when y is equal to 0. In this case, this y here is equal to 0, so this first term goes away. And the second term is 1 minus 0 times that logarithmic term. So the loss becomes this negative 1 times log of 1 minus f of x. And that's just equal to this second term up here. And so in the case of y equals 0, we also get back the original loss function as defined above. So what you see is that whether y is 1 or 0, the single expression here is equivalent to the more complex expression up here, which is why this gives us a simpler way to write the loss with just one equation without separating out these two cases like we did on top. Using this simplified loss function, let's go back and write out the cost function for logistic regression. So here again is the simplified loss function. And recall that the cost j is just the average loss, average across the entire training set of m examples. So it's 1 over m times the sum of the loss from i equals 1 to m. Now if you plug in a definition for the simplified loss from above, then it looks like this. 1 over m times the sum of this term above. And if you bring the negative signs and move them outside, then you end up with this expression over here. And this is the cost function. The cost function that pretty much everyone uses to train logistic regression. Now you might be wondering, why do we choose this particular function when there could be tons of other cost functions we could have chosen? Although we won't have time to go into great detail on this in this class, I'd just like to mention that this particular cost function is derived from statistics using a statistical principle called maximum likelihood estimation, which is an idea from statistics on how to efficiently find parameters for different models. And this cost function has the nice property that it is convex. But don't worry about learning the details of maximum likelihood. This puts a deeper rationale and justification behind this particular cost function. The upcoming optional lab will show you how the logistic cost function is implemented in code. I recommend taking a look at it because you implement this later in the practice lab at the end of the week. This upcoming optional lab also shows you how two different choices of the parameters will lead to different cost calculations. So you can see in the plot that the better fitting blue decision boundary has a lower cost relative to the magenta decision boundary. So with the simplified cost function, we're now ready to jump into applying gradient descent to logistic regression. Let's go see that in the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.16, "text": " In the last video, you saw the loss function and the cost function for logistic regression.", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 1866, 264, 4470, 2445, 293, 264, 2063, 2445, 337, 3565, 3142, 24590, 13, 50772, 50772, 682, 341, 960, 11, 291, 603, 536, 257, 4748, 18587, 636, 281, 2464, 484, 264, 4470, 293, 2063, 6828, 51059, 51059, 370, 300, 264, 11420, 393, 312, 257, 857, 18587, 562, 321, 483, 281, 16235, 23475, 337, 15669, 51314, 51314, 264, 9834, 295, 257, 3565, 3142, 24590, 2316, 13, 51492, 51492, 961, 311, 747, 257, 574, 13, 51552, 51552, 1018, 257, 13548, 11, 510, 311, 264, 4470, 2445, 300, 321, 632, 7642, 294, 264, 3894, 960, 337, 3565, 3142, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.10805957133953388, "compression_ratio": 1.8547008547008548, "no_speech_prob": 0.0074570560827851295}, {"id": 1, "seek": 0, "start": 8.16, "end": 13.9, "text": " In this video, you'll see a slightly simpler way to write out the loss and cost functions", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 1866, 264, 4470, 2445, 293, 264, 2063, 2445, 337, 3565, 3142, 24590, 13, 50772, 50772, 682, 341, 960, 11, 291, 603, 536, 257, 4748, 18587, 636, 281, 2464, 484, 264, 4470, 293, 2063, 6828, 51059, 51059, 370, 300, 264, 11420, 393, 312, 257, 857, 18587, 562, 321, 483, 281, 16235, 23475, 337, 15669, 51314, 51314, 264, 9834, 295, 257, 3565, 3142, 24590, 2316, 13, 51492, 51492, 961, 311, 747, 257, 574, 13, 51552, 51552, 1018, 257, 13548, 11, 510, 311, 264, 4470, 2445, 300, 321, 632, 7642, 294, 264, 3894, 960, 337, 3565, 3142, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.10805957133953388, "compression_ratio": 1.8547008547008548, "no_speech_prob": 0.0074570560827851295}, {"id": 2, "seek": 0, "start": 13.9, "end": 19.0, "text": " so that the implementation can be a bit simpler when we get to gradient descent for fitting", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 1866, 264, 4470, 2445, 293, 264, 2063, 2445, 337, 3565, 3142, 24590, 13, 50772, 50772, 682, 341, 960, 11, 291, 603, 536, 257, 4748, 18587, 636, 281, 2464, 484, 264, 4470, 293, 2063, 6828, 51059, 51059, 370, 300, 264, 11420, 393, 312, 257, 857, 18587, 562, 321, 483, 281, 16235, 23475, 337, 15669, 51314, 51314, 264, 9834, 295, 257, 3565, 3142, 24590, 2316, 13, 51492, 51492, 961, 311, 747, 257, 574, 13, 51552, 51552, 1018, 257, 13548, 11, 510, 311, 264, 4470, 2445, 300, 321, 632, 7642, 294, 264, 3894, 960, 337, 3565, 3142, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.10805957133953388, "compression_ratio": 1.8547008547008548, "no_speech_prob": 0.0074570560827851295}, {"id": 3, "seek": 0, "start": 19.0, "end": 22.56, "text": " the parameters of a logistic regression model.", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 1866, 264, 4470, 2445, 293, 264, 2063, 2445, 337, 3565, 3142, 24590, 13, 50772, 50772, 682, 341, 960, 11, 291, 603, 536, 257, 4748, 18587, 636, 281, 2464, 484, 264, 4470, 293, 2063, 6828, 51059, 51059, 370, 300, 264, 11420, 393, 312, 257, 857, 18587, 562, 321, 483, 281, 16235, 23475, 337, 15669, 51314, 51314, 264, 9834, 295, 257, 3565, 3142, 24590, 2316, 13, 51492, 51492, 961, 311, 747, 257, 574, 13, 51552, 51552, 1018, 257, 13548, 11, 510, 311, 264, 4470, 2445, 300, 321, 632, 7642, 294, 264, 3894, 960, 337, 3565, 3142, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.10805957133953388, "compression_ratio": 1.8547008547008548, "no_speech_prob": 0.0074570560827851295}, {"id": 4, "seek": 0, "start": 22.56, "end": 23.76, "text": " Let's take a look.", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 1866, 264, 4470, 2445, 293, 264, 2063, 2445, 337, 3565, 3142, 24590, 13, 50772, 50772, 682, 341, 960, 11, 291, 603, 536, 257, 4748, 18587, 636, 281, 2464, 484, 264, 4470, 293, 2063, 6828, 51059, 51059, 370, 300, 264, 11420, 393, 312, 257, 857, 18587, 562, 321, 483, 281, 16235, 23475, 337, 15669, 51314, 51314, 264, 9834, 295, 257, 3565, 3142, 24590, 2316, 13, 51492, 51492, 961, 311, 747, 257, 574, 13, 51552, 51552, 1018, 257, 13548, 11, 510, 311, 264, 4470, 2445, 300, 321, 632, 7642, 294, 264, 3894, 960, 337, 3565, 3142, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.10805957133953388, "compression_ratio": 1.8547008547008548, "no_speech_prob": 0.0074570560827851295}, {"id": 5, "seek": 0, "start": 23.76, "end": 29.8, "text": " As a reminder, here's the loss function that we had defined in the previous video for logistic", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 1866, 264, 4470, 2445, 293, 264, 2063, 2445, 337, 3565, 3142, 24590, 13, 50772, 50772, 682, 341, 960, 11, 291, 603, 536, 257, 4748, 18587, 636, 281, 2464, 484, 264, 4470, 293, 2063, 6828, 51059, 51059, 370, 300, 264, 11420, 393, 312, 257, 857, 18587, 562, 321, 483, 281, 16235, 23475, 337, 15669, 51314, 51314, 264, 9834, 295, 257, 3565, 3142, 24590, 2316, 13, 51492, 51492, 961, 311, 747, 257, 574, 13, 51552, 51552, 1018, 257, 13548, 11, 510, 311, 264, 4470, 2445, 300, 321, 632, 7642, 294, 264, 3894, 960, 337, 3565, 3142, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.10805957133953388, "compression_ratio": 1.8547008547008548, "no_speech_prob": 0.0074570560827851295}, {"id": 6, "seek": 2980, "start": 29.8, "end": 30.8, "text": " regression.", "tokens": [50364, 24590, 13, 50414, 50414, 823, 11, 570, 321, 434, 920, 1364, 322, 264, 17434, 21538, 1154, 11, 288, 307, 2139, 1958, 420, 502, 13, 50810, 50810, 1436, 288, 307, 2139, 1958, 420, 502, 293, 2644, 747, 322, 604, 2158, 661, 813, 1958, 420, 502, 11, 321, 603, 312, 1075, 51190, 51190, 281, 808, 493, 365, 257, 18587, 636, 281, 2464, 341, 4470, 2445, 13, 51410, 51410, 509, 393, 2464, 264, 4470, 2445, 382, 10002, 13, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.10789576663246638, "compression_ratio": 1.5891891891891892, "no_speech_prob": 3.0240504202083685e-05}, {"id": 7, "seek": 2980, "start": 30.8, "end": 38.72, "text": " Now, because we're still working on the binary classification problem, y is either 0 or 1.", "tokens": [50364, 24590, 13, 50414, 50414, 823, 11, 570, 321, 434, 920, 1364, 322, 264, 17434, 21538, 1154, 11, 288, 307, 2139, 1958, 420, 502, 13, 50810, 50810, 1436, 288, 307, 2139, 1958, 420, 502, 293, 2644, 747, 322, 604, 2158, 661, 813, 1958, 420, 502, 11, 321, 603, 312, 1075, 51190, 51190, 281, 808, 493, 365, 257, 18587, 636, 281, 2464, 341, 4470, 2445, 13, 51410, 51410, 509, 393, 2464, 264, 4470, 2445, 382, 10002, 13, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.10789576663246638, "compression_ratio": 1.5891891891891892, "no_speech_prob": 3.0240504202083685e-05}, {"id": 8, "seek": 2980, "start": 38.72, "end": 46.32, "text": " Because y is either 0 or 1 and cannot take on any value other than 0 or 1, we'll be able", "tokens": [50364, 24590, 13, 50414, 50414, 823, 11, 570, 321, 434, 920, 1364, 322, 264, 17434, 21538, 1154, 11, 288, 307, 2139, 1958, 420, 502, 13, 50810, 50810, 1436, 288, 307, 2139, 1958, 420, 502, 293, 2644, 747, 322, 604, 2158, 661, 813, 1958, 420, 502, 11, 321, 603, 312, 1075, 51190, 51190, 281, 808, 493, 365, 257, 18587, 636, 281, 2464, 341, 4470, 2445, 13, 51410, 51410, 509, 393, 2464, 264, 4470, 2445, 382, 10002, 13, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.10789576663246638, "compression_ratio": 1.5891891891891892, "no_speech_prob": 3.0240504202083685e-05}, {"id": 9, "seek": 2980, "start": 46.32, "end": 50.72, "text": " to come up with a simpler way to write this loss function.", "tokens": [50364, 24590, 13, 50414, 50414, 823, 11, 570, 321, 434, 920, 1364, 322, 264, 17434, 21538, 1154, 11, 288, 307, 2139, 1958, 420, 502, 13, 50810, 50810, 1436, 288, 307, 2139, 1958, 420, 502, 293, 2644, 747, 322, 604, 2158, 661, 813, 1958, 420, 502, 11, 321, 603, 312, 1075, 51190, 51190, 281, 808, 493, 365, 257, 18587, 636, 281, 2464, 341, 4470, 2445, 13, 51410, 51410, 509, 393, 2464, 264, 4470, 2445, 382, 10002, 13, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.10789576663246638, "compression_ratio": 1.5891891891891892, "no_speech_prob": 3.0240504202083685e-05}, {"id": 10, "seek": 2980, "start": 50.72, "end": 53.96, "text": " You can write the loss function as follows.", "tokens": [50364, 24590, 13, 50414, 50414, 823, 11, 570, 321, 434, 920, 1364, 322, 264, 17434, 21538, 1154, 11, 288, 307, 2139, 1958, 420, 502, 13, 50810, 50810, 1436, 288, 307, 2139, 1958, 420, 502, 293, 2644, 747, 322, 604, 2158, 661, 813, 1958, 420, 502, 11, 321, 603, 312, 1075, 51190, 51190, 281, 808, 493, 365, 257, 18587, 636, 281, 2464, 341, 4470, 2445, 13, 51410, 51410, 509, 393, 2464, 264, 4470, 2445, 382, 10002, 13, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.10789576663246638, "compression_ratio": 1.5891891891891892, "no_speech_prob": 3.0240504202083685e-05}, {"id": 11, "seek": 5396, "start": 53.96, "end": 61.72, "text": " Given the prediction f of x and the target label y, the loss equals negative y times", "tokens": [50364, 18600, 264, 17630, 283, 295, 2031, 293, 264, 3779, 7645, 288, 11, 264, 4470, 6915, 3671, 288, 1413, 50752, 50752, 3565, 295, 283, 3175, 502, 3175, 288, 1413, 3565, 295, 502, 3175, 283, 13, 51196, 51196, 400, 309, 4523, 484, 341, 5367, 11, 597, 321, 445, 4114, 294, 472, 1622, 11, 307, 2584, 10344, 51512, 51512, 281, 341, 544, 3997, 8513, 493, 510, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.10786930953755099, "compression_ratio": 1.535294117647059, "no_speech_prob": 7.571107971671154e-07}, {"id": 12, "seek": 5396, "start": 61.72, "end": 70.6, "text": " log of f minus 1 minus y times log of 1 minus f.", "tokens": [50364, 18600, 264, 17630, 283, 295, 2031, 293, 264, 3779, 7645, 288, 11, 264, 4470, 6915, 3671, 288, 1413, 50752, 50752, 3565, 295, 283, 3175, 502, 3175, 288, 1413, 3565, 295, 502, 3175, 283, 13, 51196, 51196, 400, 309, 4523, 484, 341, 5367, 11, 597, 321, 445, 4114, 294, 472, 1622, 11, 307, 2584, 10344, 51512, 51512, 281, 341, 544, 3997, 8513, 493, 510, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.10786930953755099, "compression_ratio": 1.535294117647059, "no_speech_prob": 7.571107971671154e-07}, {"id": 13, "seek": 5396, "start": 70.6, "end": 76.92, "text": " And it turns out this equation, which we just wrote in one line, is completely equivalent", "tokens": [50364, 18600, 264, 17630, 283, 295, 2031, 293, 264, 3779, 7645, 288, 11, 264, 4470, 6915, 3671, 288, 1413, 50752, 50752, 3565, 295, 283, 3175, 502, 3175, 288, 1413, 3565, 295, 502, 3175, 283, 13, 51196, 51196, 400, 309, 4523, 484, 341, 5367, 11, 597, 321, 445, 4114, 294, 472, 1622, 11, 307, 2584, 10344, 51512, 51512, 281, 341, 544, 3997, 8513, 493, 510, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.10786930953755099, "compression_ratio": 1.535294117647059, "no_speech_prob": 7.571107971671154e-07}, {"id": 14, "seek": 5396, "start": 76.92, "end": 80.88, "text": " to this more complex formula up here.", "tokens": [50364, 18600, 264, 17630, 283, 295, 2031, 293, 264, 3779, 7645, 288, 11, 264, 4470, 6915, 3671, 288, 1413, 50752, 50752, 3565, 295, 283, 3175, 502, 3175, 288, 1413, 3565, 295, 502, 3175, 283, 13, 51196, 51196, 400, 309, 4523, 484, 341, 5367, 11, 597, 321, 445, 4114, 294, 472, 1622, 11, 307, 2584, 10344, 51512, 51512, 281, 341, 544, 3997, 8513, 493, 510, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.10786930953755099, "compression_ratio": 1.535294117647059, "no_speech_prob": 7.571107971671154e-07}, {"id": 15, "seek": 8088, "start": 80.88, "end": 84.08, "text": " Let's see why this is the case.", "tokens": [50364, 961, 311, 536, 983, 341, 307, 264, 1389, 13, 50524, 50524, 823, 1604, 11, 288, 393, 787, 747, 322, 264, 4190, 295, 2139, 502, 420, 1958, 13, 50930, 50930, 682, 264, 700, 1389, 11, 718, 311, 584, 288, 6915, 502, 13, 51136, 51136, 639, 700, 288, 670, 510, 307, 502, 11, 293, 341, 502, 3175, 288, 307, 502, 3175, 502, 11, 597, 307, 4412, 2681, 281, 51582, 51582, 1958, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.10919916307603991, "compression_ratio": 1.474025974025974, "no_speech_prob": 1.903378347378748e-06}, {"id": 16, "seek": 8088, "start": 84.08, "end": 92.19999999999999, "text": " Now remember, y can only take on the values of either 1 or 0.", "tokens": [50364, 961, 311, 536, 983, 341, 307, 264, 1389, 13, 50524, 50524, 823, 1604, 11, 288, 393, 787, 747, 322, 264, 4190, 295, 2139, 502, 420, 1958, 13, 50930, 50930, 682, 264, 700, 1389, 11, 718, 311, 584, 288, 6915, 502, 13, 51136, 51136, 639, 700, 288, 670, 510, 307, 502, 11, 293, 341, 502, 3175, 288, 307, 502, 3175, 502, 11, 597, 307, 4412, 2681, 281, 51582, 51582, 1958, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.10919916307603991, "compression_ratio": 1.474025974025974, "no_speech_prob": 1.903378347378748e-06}, {"id": 17, "seek": 8088, "start": 92.19999999999999, "end": 96.32, "text": " In the first case, let's say y equals 1.", "tokens": [50364, 961, 311, 536, 983, 341, 307, 264, 1389, 13, 50524, 50524, 823, 1604, 11, 288, 393, 787, 747, 322, 264, 4190, 295, 2139, 502, 420, 1958, 13, 50930, 50930, 682, 264, 700, 1389, 11, 718, 311, 584, 288, 6915, 502, 13, 51136, 51136, 639, 700, 288, 670, 510, 307, 502, 11, 293, 341, 502, 3175, 288, 307, 502, 3175, 502, 11, 597, 307, 4412, 2681, 281, 51582, 51582, 1958, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.10919916307603991, "compression_ratio": 1.474025974025974, "no_speech_prob": 1.903378347378748e-06}, {"id": 18, "seek": 8088, "start": 96.32, "end": 105.24, "text": " This first y over here is 1, and this 1 minus y is 1 minus 1, which is therefore equal to", "tokens": [50364, 961, 311, 536, 983, 341, 307, 264, 1389, 13, 50524, 50524, 823, 1604, 11, 288, 393, 787, 747, 322, 264, 4190, 295, 2139, 502, 420, 1958, 13, 50930, 50930, 682, 264, 700, 1389, 11, 718, 311, 584, 288, 6915, 502, 13, 51136, 51136, 639, 700, 288, 670, 510, 307, 502, 11, 293, 341, 502, 3175, 288, 307, 502, 3175, 502, 11, 597, 307, 4412, 2681, 281, 51582, 51582, 1958, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.10919916307603991, "compression_ratio": 1.474025974025974, "no_speech_prob": 1.903378347378748e-06}, {"id": 19, "seek": 8088, "start": 105.24, "end": 106.64, "text": " 0.", "tokens": [50364, 961, 311, 536, 983, 341, 307, 264, 1389, 13, 50524, 50524, 823, 1604, 11, 288, 393, 787, 747, 322, 264, 4190, 295, 2139, 502, 420, 1958, 13, 50930, 50930, 682, 264, 700, 1389, 11, 718, 311, 584, 288, 6915, 502, 13, 51136, 51136, 639, 700, 288, 670, 510, 307, 502, 11, 293, 341, 502, 3175, 288, 307, 502, 3175, 502, 11, 597, 307, 4412, 2681, 281, 51582, 51582, 1958, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.10919916307603991, "compression_ratio": 1.474025974025974, "no_speech_prob": 1.903378347378748e-06}, {"id": 20, "seek": 10664, "start": 106.64, "end": 115.44, "text": " And so the loss becomes negative 1 times log of f of x minus 0 times a bunch of stuff that", "tokens": [50364, 400, 370, 264, 4470, 3643, 3671, 502, 1413, 3565, 295, 283, 295, 2031, 3175, 1958, 1413, 257, 3840, 295, 1507, 300, 50804, 50804, 3643, 1958, 293, 1709, 1314, 13, 50976, 50976, 400, 370, 562, 288, 307, 2681, 281, 502, 11, 264, 4470, 307, 6451, 264, 700, 1433, 322, 1192, 11, 3671, 3565, 295, 283, 295, 51344, 51344, 2031, 13, 51458, 51458, 961, 311, 574, 412, 264, 1150, 1389, 11, 562, 288, 307, 2681, 281, 1958, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.07747602462768555, "compression_ratio": 1.6335403726708075, "no_speech_prob": 5.285507995722583e-07}, {"id": 21, "seek": 10664, "start": 115.44, "end": 118.88, "text": " becomes 0 and goes away.", "tokens": [50364, 400, 370, 264, 4470, 3643, 3671, 502, 1413, 3565, 295, 283, 295, 2031, 3175, 1958, 1413, 257, 3840, 295, 1507, 300, 50804, 50804, 3643, 1958, 293, 1709, 1314, 13, 50976, 50976, 400, 370, 562, 288, 307, 2681, 281, 502, 11, 264, 4470, 307, 6451, 264, 700, 1433, 322, 1192, 11, 3671, 3565, 295, 283, 295, 51344, 51344, 2031, 13, 51458, 51458, 961, 311, 574, 412, 264, 1150, 1389, 11, 562, 288, 307, 2681, 281, 1958, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.07747602462768555, "compression_ratio": 1.6335403726708075, "no_speech_prob": 5.285507995722583e-07}, {"id": 22, "seek": 10664, "start": 118.88, "end": 126.24000000000001, "text": " And so when y is equal to 1, the loss is indeed the first term on top, negative log of f of", "tokens": [50364, 400, 370, 264, 4470, 3643, 3671, 502, 1413, 3565, 295, 283, 295, 2031, 3175, 1958, 1413, 257, 3840, 295, 1507, 300, 50804, 50804, 3643, 1958, 293, 1709, 1314, 13, 50976, 50976, 400, 370, 562, 288, 307, 2681, 281, 502, 11, 264, 4470, 307, 6451, 264, 700, 1433, 322, 1192, 11, 3671, 3565, 295, 283, 295, 51344, 51344, 2031, 13, 51458, 51458, 961, 311, 574, 412, 264, 1150, 1389, 11, 562, 288, 307, 2681, 281, 1958, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.07747602462768555, "compression_ratio": 1.6335403726708075, "no_speech_prob": 5.285507995722583e-07}, {"id": 23, "seek": 10664, "start": 126.24000000000001, "end": 128.52, "text": " x.", "tokens": [50364, 400, 370, 264, 4470, 3643, 3671, 502, 1413, 3565, 295, 283, 295, 2031, 3175, 1958, 1413, 257, 3840, 295, 1507, 300, 50804, 50804, 3643, 1958, 293, 1709, 1314, 13, 50976, 50976, 400, 370, 562, 288, 307, 2681, 281, 502, 11, 264, 4470, 307, 6451, 264, 700, 1433, 322, 1192, 11, 3671, 3565, 295, 283, 295, 51344, 51344, 2031, 13, 51458, 51458, 961, 311, 574, 412, 264, 1150, 1389, 11, 562, 288, 307, 2681, 281, 1958, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.07747602462768555, "compression_ratio": 1.6335403726708075, "no_speech_prob": 5.285507995722583e-07}, {"id": 24, "seek": 10664, "start": 128.52, "end": 134.6, "text": " Let's look at the second case, when y is equal to 0.", "tokens": [50364, 400, 370, 264, 4470, 3643, 3671, 502, 1413, 3565, 295, 283, 295, 2031, 3175, 1958, 1413, 257, 3840, 295, 1507, 300, 50804, 50804, 3643, 1958, 293, 1709, 1314, 13, 50976, 50976, 400, 370, 562, 288, 307, 2681, 281, 502, 11, 264, 4470, 307, 6451, 264, 700, 1433, 322, 1192, 11, 3671, 3565, 295, 283, 295, 51344, 51344, 2031, 13, 51458, 51458, 961, 311, 574, 412, 264, 1150, 1389, 11, 562, 288, 307, 2681, 281, 1958, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.07747602462768555, "compression_ratio": 1.6335403726708075, "no_speech_prob": 5.285507995722583e-07}, {"id": 25, "seek": 13460, "start": 134.6, "end": 142.12, "text": " In this case, this y here is equal to 0, so this first term goes away.", "tokens": [50364, 682, 341, 1389, 11, 341, 288, 510, 307, 2681, 281, 1958, 11, 370, 341, 700, 1433, 1709, 1314, 13, 50740, 50740, 400, 264, 1150, 1433, 307, 502, 3175, 1958, 1413, 300, 41473, 355, 13195, 1433, 13, 51146, 51146, 407, 264, 4470, 3643, 341, 3671, 502, 1413, 3565, 295, 502, 3175, 283, 295, 2031, 13, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.09431955732148269, "compression_ratio": 1.4485294117647058, "no_speech_prob": 8.059429887907754e-07}, {"id": 26, "seek": 13460, "start": 142.12, "end": 150.24, "text": " And the second term is 1 minus 0 times that logarithmic term.", "tokens": [50364, 682, 341, 1389, 11, 341, 288, 510, 307, 2681, 281, 1958, 11, 370, 341, 700, 1433, 1709, 1314, 13, 50740, 50740, 400, 264, 1150, 1433, 307, 502, 3175, 1958, 1413, 300, 41473, 355, 13195, 1433, 13, 51146, 51146, 407, 264, 4470, 3643, 341, 3671, 502, 1413, 3565, 295, 502, 3175, 283, 295, 2031, 13, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.09431955732148269, "compression_ratio": 1.4485294117647058, "no_speech_prob": 8.059429887907754e-07}, {"id": 27, "seek": 13460, "start": 150.24, "end": 159.24, "text": " So the loss becomes this negative 1 times log of 1 minus f of x.", "tokens": [50364, 682, 341, 1389, 11, 341, 288, 510, 307, 2681, 281, 1958, 11, 370, 341, 700, 1433, 1709, 1314, 13, 50740, 50740, 400, 264, 1150, 1433, 307, 502, 3175, 1958, 1413, 300, 41473, 355, 13195, 1433, 13, 51146, 51146, 407, 264, 4470, 3643, 341, 3671, 502, 1413, 3565, 295, 502, 3175, 283, 295, 2031, 13, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.09431955732148269, "compression_ratio": 1.4485294117647058, "no_speech_prob": 8.059429887907754e-07}, {"id": 28, "seek": 15924, "start": 159.24, "end": 164.76000000000002, "text": " And that's just equal to this second term up here.", "tokens": [50364, 400, 300, 311, 445, 2681, 281, 341, 1150, 1433, 493, 510, 13, 50640, 50640, 400, 370, 294, 264, 1389, 295, 288, 6915, 1958, 11, 321, 611, 483, 646, 264, 3380, 4470, 2445, 382, 7642, 50974, 50974, 3673, 13, 51080, 51080, 407, 437, 291, 536, 307, 300, 1968, 288, 307, 502, 420, 1958, 11, 264, 2167, 6114, 510, 307, 10344, 51436, 51436, 281, 264, 544, 3997, 6114, 493, 510, 11, 597, 307, 983, 341, 2709, 505, 257, 18587, 636, 281, 2464, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.058276193482535224, "compression_ratio": 1.5555555555555556, "no_speech_prob": 5.043467581344885e-07}, {"id": 29, "seek": 15924, "start": 164.76000000000002, "end": 171.44, "text": " And so in the case of y equals 0, we also get back the original loss function as defined", "tokens": [50364, 400, 300, 311, 445, 2681, 281, 341, 1150, 1433, 493, 510, 13, 50640, 50640, 400, 370, 294, 264, 1389, 295, 288, 6915, 1958, 11, 321, 611, 483, 646, 264, 3380, 4470, 2445, 382, 7642, 50974, 50974, 3673, 13, 51080, 51080, 407, 437, 291, 536, 307, 300, 1968, 288, 307, 502, 420, 1958, 11, 264, 2167, 6114, 510, 307, 10344, 51436, 51436, 281, 264, 544, 3997, 6114, 493, 510, 11, 597, 307, 983, 341, 2709, 505, 257, 18587, 636, 281, 2464, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.058276193482535224, "compression_ratio": 1.5555555555555556, "no_speech_prob": 5.043467581344885e-07}, {"id": 30, "seek": 15924, "start": 171.44, "end": 173.56, "text": " above.", "tokens": [50364, 400, 300, 311, 445, 2681, 281, 341, 1150, 1433, 493, 510, 13, 50640, 50640, 400, 370, 294, 264, 1389, 295, 288, 6915, 1958, 11, 321, 611, 483, 646, 264, 3380, 4470, 2445, 382, 7642, 50974, 50974, 3673, 13, 51080, 51080, 407, 437, 291, 536, 307, 300, 1968, 288, 307, 502, 420, 1958, 11, 264, 2167, 6114, 510, 307, 10344, 51436, 51436, 281, 264, 544, 3997, 6114, 493, 510, 11, 597, 307, 983, 341, 2709, 505, 257, 18587, 636, 281, 2464, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.058276193482535224, "compression_ratio": 1.5555555555555556, "no_speech_prob": 5.043467581344885e-07}, {"id": 31, "seek": 15924, "start": 173.56, "end": 180.68, "text": " So what you see is that whether y is 1 or 0, the single expression here is equivalent", "tokens": [50364, 400, 300, 311, 445, 2681, 281, 341, 1150, 1433, 493, 510, 13, 50640, 50640, 400, 370, 294, 264, 1389, 295, 288, 6915, 1958, 11, 321, 611, 483, 646, 264, 3380, 4470, 2445, 382, 7642, 50974, 50974, 3673, 13, 51080, 51080, 407, 437, 291, 536, 307, 300, 1968, 288, 307, 502, 420, 1958, 11, 264, 2167, 6114, 510, 307, 10344, 51436, 51436, 281, 264, 544, 3997, 6114, 493, 510, 11, 597, 307, 983, 341, 2709, 505, 257, 18587, 636, 281, 2464, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.058276193482535224, "compression_ratio": 1.5555555555555556, "no_speech_prob": 5.043467581344885e-07}, {"id": 32, "seek": 15924, "start": 180.68, "end": 186.44, "text": " to the more complex expression up here, which is why this gives us a simpler way to write", "tokens": [50364, 400, 300, 311, 445, 2681, 281, 341, 1150, 1433, 493, 510, 13, 50640, 50640, 400, 370, 294, 264, 1389, 295, 288, 6915, 1958, 11, 321, 611, 483, 646, 264, 3380, 4470, 2445, 382, 7642, 50974, 50974, 3673, 13, 51080, 51080, 407, 437, 291, 536, 307, 300, 1968, 288, 307, 502, 420, 1958, 11, 264, 2167, 6114, 510, 307, 10344, 51436, 51436, 281, 264, 544, 3997, 6114, 493, 510, 11, 597, 307, 983, 341, 2709, 505, 257, 18587, 636, 281, 2464, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.058276193482535224, "compression_ratio": 1.5555555555555556, "no_speech_prob": 5.043467581344885e-07}, {"id": 33, "seek": 18644, "start": 186.44, "end": 194.4, "text": " the loss with just one equation without separating out these two cases like we did on top.", "tokens": [50364, 264, 4470, 365, 445, 472, 5367, 1553, 29279, 484, 613, 732, 3331, 411, 321, 630, 322, 1192, 13, 50762, 50762, 11142, 341, 26335, 4470, 2445, 11, 718, 311, 352, 646, 293, 2464, 484, 264, 2063, 2445, 337, 50992, 50992, 3565, 3142, 24590, 13, 51175, 51175, 407, 510, 797, 307, 264, 26335, 4470, 2445, 13, 51410, 51410, 400, 9901, 300, 264, 2063, 361, 307, 445, 264, 4274, 4470, 11, 4274, 2108, 264, 2302, 3097, 992, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.08845253479786408, "compression_ratio": 1.702020202020202, "no_speech_prob": 1.6028053551053745e-06}, {"id": 34, "seek": 18644, "start": 194.4, "end": 199.0, "text": " Using this simplified loss function, let's go back and write out the cost function for", "tokens": [50364, 264, 4470, 365, 445, 472, 5367, 1553, 29279, 484, 613, 732, 3331, 411, 321, 630, 322, 1192, 13, 50762, 50762, 11142, 341, 26335, 4470, 2445, 11, 718, 311, 352, 646, 293, 2464, 484, 264, 2063, 2445, 337, 50992, 50992, 3565, 3142, 24590, 13, 51175, 51175, 407, 510, 797, 307, 264, 26335, 4470, 2445, 13, 51410, 51410, 400, 9901, 300, 264, 2063, 361, 307, 445, 264, 4274, 4470, 11, 4274, 2108, 264, 2302, 3097, 992, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.08845253479786408, "compression_ratio": 1.702020202020202, "no_speech_prob": 1.6028053551053745e-06}, {"id": 35, "seek": 18644, "start": 199.0, "end": 202.66, "text": " logistic regression.", "tokens": [50364, 264, 4470, 365, 445, 472, 5367, 1553, 29279, 484, 613, 732, 3331, 411, 321, 630, 322, 1192, 13, 50762, 50762, 11142, 341, 26335, 4470, 2445, 11, 718, 311, 352, 646, 293, 2464, 484, 264, 2063, 2445, 337, 50992, 50992, 3565, 3142, 24590, 13, 51175, 51175, 407, 510, 797, 307, 264, 26335, 4470, 2445, 13, 51410, 51410, 400, 9901, 300, 264, 2063, 361, 307, 445, 264, 4274, 4470, 11, 4274, 2108, 264, 2302, 3097, 992, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.08845253479786408, "compression_ratio": 1.702020202020202, "no_speech_prob": 1.6028053551053745e-06}, {"id": 36, "seek": 18644, "start": 202.66, "end": 207.36, "text": " So here again is the simplified loss function.", "tokens": [50364, 264, 4470, 365, 445, 472, 5367, 1553, 29279, 484, 613, 732, 3331, 411, 321, 630, 322, 1192, 13, 50762, 50762, 11142, 341, 26335, 4470, 2445, 11, 718, 311, 352, 646, 293, 2464, 484, 264, 2063, 2445, 337, 50992, 50992, 3565, 3142, 24590, 13, 51175, 51175, 407, 510, 797, 307, 264, 26335, 4470, 2445, 13, 51410, 51410, 400, 9901, 300, 264, 2063, 361, 307, 445, 264, 4274, 4470, 11, 4274, 2108, 264, 2302, 3097, 992, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.08845253479786408, "compression_ratio": 1.702020202020202, "no_speech_prob": 1.6028053551053745e-06}, {"id": 37, "seek": 18644, "start": 207.36, "end": 214.16, "text": " And recall that the cost j is just the average loss, average across the entire training set", "tokens": [50364, 264, 4470, 365, 445, 472, 5367, 1553, 29279, 484, 613, 732, 3331, 411, 321, 630, 322, 1192, 13, 50762, 50762, 11142, 341, 26335, 4470, 2445, 11, 718, 311, 352, 646, 293, 2464, 484, 264, 2063, 2445, 337, 50992, 50992, 3565, 3142, 24590, 13, 51175, 51175, 407, 510, 797, 307, 264, 26335, 4470, 2445, 13, 51410, 51410, 400, 9901, 300, 264, 2063, 361, 307, 445, 264, 4274, 4470, 11, 4274, 2108, 264, 2302, 3097, 992, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.08845253479786408, "compression_ratio": 1.702020202020202, "no_speech_prob": 1.6028053551053745e-06}, {"id": 38, "seek": 21416, "start": 214.16, "end": 216.48, "text": " of m examples.", "tokens": [50364, 295, 275, 5110, 13, 50480, 50480, 407, 309, 311, 502, 670, 275, 1413, 264, 2408, 295, 264, 4470, 490, 741, 6915, 502, 281, 275, 13, 50824, 50824, 823, 498, 291, 5452, 294, 257, 7123, 337, 264, 26335, 4470, 490, 3673, 11, 550, 309, 1542, 411, 341, 13, 51098, 51098, 502, 670, 275, 1413, 264, 2408, 295, 341, 1433, 3673, 13, 51344, 51344, 400, 498, 291, 1565, 264, 3671, 7880, 293, 1286, 552, 2380, 11, 550, 291, 917, 493, 365, 341, 6114, 51632, 51632, 670, 510, 13, 51712, 51712, 400, 341, 307, 264, 2063, 2445, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.13052610917524857, "compression_ratio": 1.7135922330097086, "no_speech_prob": 3.966959411627613e-06}, {"id": 39, "seek": 21416, "start": 216.48, "end": 223.35999999999999, "text": " So it's 1 over m times the sum of the loss from i equals 1 to m.", "tokens": [50364, 295, 275, 5110, 13, 50480, 50480, 407, 309, 311, 502, 670, 275, 1413, 264, 2408, 295, 264, 4470, 490, 741, 6915, 502, 281, 275, 13, 50824, 50824, 823, 498, 291, 5452, 294, 257, 7123, 337, 264, 26335, 4470, 490, 3673, 11, 550, 309, 1542, 411, 341, 13, 51098, 51098, 502, 670, 275, 1413, 264, 2408, 295, 341, 1433, 3673, 13, 51344, 51344, 400, 498, 291, 1565, 264, 3671, 7880, 293, 1286, 552, 2380, 11, 550, 291, 917, 493, 365, 341, 6114, 51632, 51632, 670, 510, 13, 51712, 51712, 400, 341, 307, 264, 2063, 2445, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.13052610917524857, "compression_ratio": 1.7135922330097086, "no_speech_prob": 3.966959411627613e-06}, {"id": 40, "seek": 21416, "start": 223.35999999999999, "end": 228.84, "text": " Now if you plug in a definition for the simplified loss from above, then it looks like this.", "tokens": [50364, 295, 275, 5110, 13, 50480, 50480, 407, 309, 311, 502, 670, 275, 1413, 264, 2408, 295, 264, 4470, 490, 741, 6915, 502, 281, 275, 13, 50824, 50824, 823, 498, 291, 5452, 294, 257, 7123, 337, 264, 26335, 4470, 490, 3673, 11, 550, 309, 1542, 411, 341, 13, 51098, 51098, 502, 670, 275, 1413, 264, 2408, 295, 341, 1433, 3673, 13, 51344, 51344, 400, 498, 291, 1565, 264, 3671, 7880, 293, 1286, 552, 2380, 11, 550, 291, 917, 493, 365, 341, 6114, 51632, 51632, 670, 510, 13, 51712, 51712, 400, 341, 307, 264, 2063, 2445, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.13052610917524857, "compression_ratio": 1.7135922330097086, "no_speech_prob": 3.966959411627613e-06}, {"id": 41, "seek": 21416, "start": 228.84, "end": 233.76, "text": " 1 over m times the sum of this term above.", "tokens": [50364, 295, 275, 5110, 13, 50480, 50480, 407, 309, 311, 502, 670, 275, 1413, 264, 2408, 295, 264, 4470, 490, 741, 6915, 502, 281, 275, 13, 50824, 50824, 823, 498, 291, 5452, 294, 257, 7123, 337, 264, 26335, 4470, 490, 3673, 11, 550, 309, 1542, 411, 341, 13, 51098, 51098, 502, 670, 275, 1413, 264, 2408, 295, 341, 1433, 3673, 13, 51344, 51344, 400, 498, 291, 1565, 264, 3671, 7880, 293, 1286, 552, 2380, 11, 550, 291, 917, 493, 365, 341, 6114, 51632, 51632, 670, 510, 13, 51712, 51712, 400, 341, 307, 264, 2063, 2445, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.13052610917524857, "compression_ratio": 1.7135922330097086, "no_speech_prob": 3.966959411627613e-06}, {"id": 42, "seek": 21416, "start": 233.76, "end": 239.51999999999998, "text": " And if you bring the negative signs and move them outside, then you end up with this expression", "tokens": [50364, 295, 275, 5110, 13, 50480, 50480, 407, 309, 311, 502, 670, 275, 1413, 264, 2408, 295, 264, 4470, 490, 741, 6915, 502, 281, 275, 13, 50824, 50824, 823, 498, 291, 5452, 294, 257, 7123, 337, 264, 26335, 4470, 490, 3673, 11, 550, 309, 1542, 411, 341, 13, 51098, 51098, 502, 670, 275, 1413, 264, 2408, 295, 341, 1433, 3673, 13, 51344, 51344, 400, 498, 291, 1565, 264, 3671, 7880, 293, 1286, 552, 2380, 11, 550, 291, 917, 493, 365, 341, 6114, 51632, 51632, 670, 510, 13, 51712, 51712, 400, 341, 307, 264, 2063, 2445, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.13052610917524857, "compression_ratio": 1.7135922330097086, "no_speech_prob": 3.966959411627613e-06}, {"id": 43, "seek": 21416, "start": 239.51999999999998, "end": 241.12, "text": " over here.", "tokens": [50364, 295, 275, 5110, 13, 50480, 50480, 407, 309, 311, 502, 670, 275, 1413, 264, 2408, 295, 264, 4470, 490, 741, 6915, 502, 281, 275, 13, 50824, 50824, 823, 498, 291, 5452, 294, 257, 7123, 337, 264, 26335, 4470, 490, 3673, 11, 550, 309, 1542, 411, 341, 13, 51098, 51098, 502, 670, 275, 1413, 264, 2408, 295, 341, 1433, 3673, 13, 51344, 51344, 400, 498, 291, 1565, 264, 3671, 7880, 293, 1286, 552, 2380, 11, 550, 291, 917, 493, 365, 341, 6114, 51632, 51632, 670, 510, 13, 51712, 51712, 400, 341, 307, 264, 2063, 2445, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.13052610917524857, "compression_ratio": 1.7135922330097086, "no_speech_prob": 3.966959411627613e-06}, {"id": 44, "seek": 21416, "start": 241.12, "end": 243.4, "text": " And this is the cost function.", "tokens": [50364, 295, 275, 5110, 13, 50480, 50480, 407, 309, 311, 502, 670, 275, 1413, 264, 2408, 295, 264, 4470, 490, 741, 6915, 502, 281, 275, 13, 50824, 50824, 823, 498, 291, 5452, 294, 257, 7123, 337, 264, 26335, 4470, 490, 3673, 11, 550, 309, 1542, 411, 341, 13, 51098, 51098, 502, 670, 275, 1413, 264, 2408, 295, 341, 1433, 3673, 13, 51344, 51344, 400, 498, 291, 1565, 264, 3671, 7880, 293, 1286, 552, 2380, 11, 550, 291, 917, 493, 365, 341, 6114, 51632, 51632, 670, 510, 13, 51712, 51712, 400, 341, 307, 264, 2063, 2445, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.13052610917524857, "compression_ratio": 1.7135922330097086, "no_speech_prob": 3.966959411627613e-06}, {"id": 45, "seek": 24340, "start": 243.4, "end": 249.12, "text": " The cost function that pretty much everyone uses to train logistic regression.", "tokens": [50364, 440, 2063, 2445, 300, 1238, 709, 1518, 4960, 281, 3847, 3565, 3142, 24590, 13, 50650, 50650, 823, 291, 1062, 312, 6359, 11, 983, 360, 321, 2826, 341, 1729, 2445, 562, 456, 727, 50918, 50918, 312, 9131, 295, 661, 2063, 6828, 321, 727, 362, 8614, 30, 51120, 51120, 5780, 321, 1582, 380, 362, 565, 281, 352, 666, 869, 2607, 322, 341, 294, 341, 1508, 11, 286, 1116, 445, 411, 51363, 51363, 281, 2152, 300, 341, 1729, 2063, 2445, 307, 18949, 490, 12523, 1228, 257, 22820, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.09427131306041371, "compression_ratio": 1.728448275862069, "no_speech_prob": 4.222700226819143e-06}, {"id": 46, "seek": 24340, "start": 249.12, "end": 254.48000000000002, "text": " Now you might be wondering, why do we choose this particular function when there could", "tokens": [50364, 440, 2063, 2445, 300, 1238, 709, 1518, 4960, 281, 3847, 3565, 3142, 24590, 13, 50650, 50650, 823, 291, 1062, 312, 6359, 11, 983, 360, 321, 2826, 341, 1729, 2445, 562, 456, 727, 50918, 50918, 312, 9131, 295, 661, 2063, 6828, 321, 727, 362, 8614, 30, 51120, 51120, 5780, 321, 1582, 380, 362, 565, 281, 352, 666, 869, 2607, 322, 341, 294, 341, 1508, 11, 286, 1116, 445, 411, 51363, 51363, 281, 2152, 300, 341, 1729, 2063, 2445, 307, 18949, 490, 12523, 1228, 257, 22820, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.09427131306041371, "compression_ratio": 1.728448275862069, "no_speech_prob": 4.222700226819143e-06}, {"id": 47, "seek": 24340, "start": 254.48000000000002, "end": 258.52, "text": " be tons of other cost functions we could have chosen?", "tokens": [50364, 440, 2063, 2445, 300, 1238, 709, 1518, 4960, 281, 3847, 3565, 3142, 24590, 13, 50650, 50650, 823, 291, 1062, 312, 6359, 11, 983, 360, 321, 2826, 341, 1729, 2445, 562, 456, 727, 50918, 50918, 312, 9131, 295, 661, 2063, 6828, 321, 727, 362, 8614, 30, 51120, 51120, 5780, 321, 1582, 380, 362, 565, 281, 352, 666, 869, 2607, 322, 341, 294, 341, 1508, 11, 286, 1116, 445, 411, 51363, 51363, 281, 2152, 300, 341, 1729, 2063, 2445, 307, 18949, 490, 12523, 1228, 257, 22820, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.09427131306041371, "compression_ratio": 1.728448275862069, "no_speech_prob": 4.222700226819143e-06}, {"id": 48, "seek": 24340, "start": 258.52, "end": 263.38, "text": " Although we won't have time to go into great detail on this in this class, I'd just like", "tokens": [50364, 440, 2063, 2445, 300, 1238, 709, 1518, 4960, 281, 3847, 3565, 3142, 24590, 13, 50650, 50650, 823, 291, 1062, 312, 6359, 11, 983, 360, 321, 2826, 341, 1729, 2445, 562, 456, 727, 50918, 50918, 312, 9131, 295, 661, 2063, 6828, 321, 727, 362, 8614, 30, 51120, 51120, 5780, 321, 1582, 380, 362, 565, 281, 352, 666, 869, 2607, 322, 341, 294, 341, 1508, 11, 286, 1116, 445, 411, 51363, 51363, 281, 2152, 300, 341, 1729, 2063, 2445, 307, 18949, 490, 12523, 1228, 257, 22820, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.09427131306041371, "compression_ratio": 1.728448275862069, "no_speech_prob": 4.222700226819143e-06}, {"id": 49, "seek": 24340, "start": 263.38, "end": 269.88, "text": " to mention that this particular cost function is derived from statistics using a statistical", "tokens": [50364, 440, 2063, 2445, 300, 1238, 709, 1518, 4960, 281, 3847, 3565, 3142, 24590, 13, 50650, 50650, 823, 291, 1062, 312, 6359, 11, 983, 360, 321, 2826, 341, 1729, 2445, 562, 456, 727, 50918, 50918, 312, 9131, 295, 661, 2063, 6828, 321, 727, 362, 8614, 30, 51120, 51120, 5780, 321, 1582, 380, 362, 565, 281, 352, 666, 869, 2607, 322, 341, 294, 341, 1508, 11, 286, 1116, 445, 411, 51363, 51363, 281, 2152, 300, 341, 1729, 2063, 2445, 307, 18949, 490, 12523, 1228, 257, 22820, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.09427131306041371, "compression_ratio": 1.728448275862069, "no_speech_prob": 4.222700226819143e-06}, {"id": 50, "seek": 26988, "start": 269.88, "end": 276.6, "text": " principle called maximum likelihood estimation, which is an idea from statistics on how to", "tokens": [50364, 8665, 1219, 6674, 22119, 35701, 11, 597, 307, 364, 1558, 490, 12523, 322, 577, 281, 50700, 50700, 19621, 915, 9834, 337, 819, 5245, 13, 50920, 50920, 400, 341, 2063, 2445, 575, 264, 1481, 4707, 300, 309, 307, 42432, 13, 51218, 51218, 583, 500, 380, 3292, 466, 2539, 264, 4365, 295, 6674, 22119, 13, 51412, 51412, 639, 8137, 257, 7731, 41989, 293, 31591, 2261, 341, 1729, 2063, 2445, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.1261546081966824, "compression_ratio": 1.6063348416289593, "no_speech_prob": 2.8129861675552092e-06}, {"id": 51, "seek": 26988, "start": 276.6, "end": 281.0, "text": " efficiently find parameters for different models.", "tokens": [50364, 8665, 1219, 6674, 22119, 35701, 11, 597, 307, 364, 1558, 490, 12523, 322, 577, 281, 50700, 50700, 19621, 915, 9834, 337, 819, 5245, 13, 50920, 50920, 400, 341, 2063, 2445, 575, 264, 1481, 4707, 300, 309, 307, 42432, 13, 51218, 51218, 583, 500, 380, 3292, 466, 2539, 264, 4365, 295, 6674, 22119, 13, 51412, 51412, 639, 8137, 257, 7731, 41989, 293, 31591, 2261, 341, 1729, 2063, 2445, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.1261546081966824, "compression_ratio": 1.6063348416289593, "no_speech_prob": 2.8129861675552092e-06}, {"id": 52, "seek": 26988, "start": 281.0, "end": 286.96, "text": " And this cost function has the nice property that it is convex.", "tokens": [50364, 8665, 1219, 6674, 22119, 35701, 11, 597, 307, 364, 1558, 490, 12523, 322, 577, 281, 50700, 50700, 19621, 915, 9834, 337, 819, 5245, 13, 50920, 50920, 400, 341, 2063, 2445, 575, 264, 1481, 4707, 300, 309, 307, 42432, 13, 51218, 51218, 583, 500, 380, 3292, 466, 2539, 264, 4365, 295, 6674, 22119, 13, 51412, 51412, 639, 8137, 257, 7731, 41989, 293, 31591, 2261, 341, 1729, 2063, 2445, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.1261546081966824, "compression_ratio": 1.6063348416289593, "no_speech_prob": 2.8129861675552092e-06}, {"id": 53, "seek": 26988, "start": 286.96, "end": 290.84, "text": " But don't worry about learning the details of maximum likelihood.", "tokens": [50364, 8665, 1219, 6674, 22119, 35701, 11, 597, 307, 364, 1558, 490, 12523, 322, 577, 281, 50700, 50700, 19621, 915, 9834, 337, 819, 5245, 13, 50920, 50920, 400, 341, 2063, 2445, 575, 264, 1481, 4707, 300, 309, 307, 42432, 13, 51218, 51218, 583, 500, 380, 3292, 466, 2539, 264, 4365, 295, 6674, 22119, 13, 51412, 51412, 639, 8137, 257, 7731, 41989, 293, 31591, 2261, 341, 1729, 2063, 2445, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.1261546081966824, "compression_ratio": 1.6063348416289593, "no_speech_prob": 2.8129861675552092e-06}, {"id": 54, "seek": 26988, "start": 290.84, "end": 298.24, "text": " This puts a deeper rationale and justification behind this particular cost function.", "tokens": [50364, 8665, 1219, 6674, 22119, 35701, 11, 597, 307, 364, 1558, 490, 12523, 322, 577, 281, 50700, 50700, 19621, 915, 9834, 337, 819, 5245, 13, 50920, 50920, 400, 341, 2063, 2445, 575, 264, 1481, 4707, 300, 309, 307, 42432, 13, 51218, 51218, 583, 500, 380, 3292, 466, 2539, 264, 4365, 295, 6674, 22119, 13, 51412, 51412, 639, 8137, 257, 7731, 41989, 293, 31591, 2261, 341, 1729, 2063, 2445, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.1261546081966824, "compression_ratio": 1.6063348416289593, "no_speech_prob": 2.8129861675552092e-06}, {"id": 55, "seek": 29824, "start": 298.24, "end": 303.88, "text": " The upcoming optional lab will show you how the logistic cost function is implemented", "tokens": [50364, 440, 11500, 17312, 2715, 486, 855, 291, 577, 264, 3565, 3142, 2063, 2445, 307, 12270, 50646, 50646, 294, 3089, 13, 50702, 50702, 286, 2748, 1940, 257, 574, 412, 309, 570, 291, 4445, 341, 1780, 294, 264, 3124, 2715, 412, 51008, 51008, 264, 917, 295, 264, 1243, 13, 51161, 51161, 639, 11500, 17312, 2715, 611, 3110, 291, 577, 732, 819, 7994, 295, 264, 9834, 51436, 51436, 486, 1477, 281, 819, 2063, 20448, 13, 51594, 51594], "temperature": 0.0, "avg_logprob": -0.0868682861328125, "compression_ratio": 1.7329842931937174, "no_speech_prob": 2.482449644958251e-06}, {"id": 56, "seek": 29824, "start": 303.88, "end": 305.0, "text": " in code.", "tokens": [50364, 440, 11500, 17312, 2715, 486, 855, 291, 577, 264, 3565, 3142, 2063, 2445, 307, 12270, 50646, 50646, 294, 3089, 13, 50702, 50702, 286, 2748, 1940, 257, 574, 412, 309, 570, 291, 4445, 341, 1780, 294, 264, 3124, 2715, 412, 51008, 51008, 264, 917, 295, 264, 1243, 13, 51161, 51161, 639, 11500, 17312, 2715, 611, 3110, 291, 577, 732, 819, 7994, 295, 264, 9834, 51436, 51436, 486, 1477, 281, 819, 2063, 20448, 13, 51594, 51594], "temperature": 0.0, "avg_logprob": -0.0868682861328125, "compression_ratio": 1.7329842931937174, "no_speech_prob": 2.482449644958251e-06}, {"id": 57, "seek": 29824, "start": 305.0, "end": 311.12, "text": " I recommend taking a look at it because you implement this later in the practice lab at", "tokens": [50364, 440, 11500, 17312, 2715, 486, 855, 291, 577, 264, 3565, 3142, 2063, 2445, 307, 12270, 50646, 50646, 294, 3089, 13, 50702, 50702, 286, 2748, 1940, 257, 574, 412, 309, 570, 291, 4445, 341, 1780, 294, 264, 3124, 2715, 412, 51008, 51008, 264, 917, 295, 264, 1243, 13, 51161, 51161, 639, 11500, 17312, 2715, 611, 3110, 291, 577, 732, 819, 7994, 295, 264, 9834, 51436, 51436, 486, 1477, 281, 819, 2063, 20448, 13, 51594, 51594], "temperature": 0.0, "avg_logprob": -0.0868682861328125, "compression_ratio": 1.7329842931937174, "no_speech_prob": 2.482449644958251e-06}, {"id": 58, "seek": 29824, "start": 311.12, "end": 314.18, "text": " the end of the week.", "tokens": [50364, 440, 11500, 17312, 2715, 486, 855, 291, 577, 264, 3565, 3142, 2063, 2445, 307, 12270, 50646, 50646, 294, 3089, 13, 50702, 50702, 286, 2748, 1940, 257, 574, 412, 309, 570, 291, 4445, 341, 1780, 294, 264, 3124, 2715, 412, 51008, 51008, 264, 917, 295, 264, 1243, 13, 51161, 51161, 639, 11500, 17312, 2715, 611, 3110, 291, 577, 732, 819, 7994, 295, 264, 9834, 51436, 51436, 486, 1477, 281, 819, 2063, 20448, 13, 51594, 51594], "temperature": 0.0, "avg_logprob": -0.0868682861328125, "compression_ratio": 1.7329842931937174, "no_speech_prob": 2.482449644958251e-06}, {"id": 59, "seek": 29824, "start": 314.18, "end": 319.68, "text": " This upcoming optional lab also shows you how two different choices of the parameters", "tokens": [50364, 440, 11500, 17312, 2715, 486, 855, 291, 577, 264, 3565, 3142, 2063, 2445, 307, 12270, 50646, 50646, 294, 3089, 13, 50702, 50702, 286, 2748, 1940, 257, 574, 412, 309, 570, 291, 4445, 341, 1780, 294, 264, 3124, 2715, 412, 51008, 51008, 264, 917, 295, 264, 1243, 13, 51161, 51161, 639, 11500, 17312, 2715, 611, 3110, 291, 577, 732, 819, 7994, 295, 264, 9834, 51436, 51436, 486, 1477, 281, 819, 2063, 20448, 13, 51594, 51594], "temperature": 0.0, "avg_logprob": -0.0868682861328125, "compression_ratio": 1.7329842931937174, "no_speech_prob": 2.482449644958251e-06}, {"id": 60, "seek": 29824, "start": 319.68, "end": 322.84000000000003, "text": " will lead to different cost calculations.", "tokens": [50364, 440, 11500, 17312, 2715, 486, 855, 291, 577, 264, 3565, 3142, 2063, 2445, 307, 12270, 50646, 50646, 294, 3089, 13, 50702, 50702, 286, 2748, 1940, 257, 574, 412, 309, 570, 291, 4445, 341, 1780, 294, 264, 3124, 2715, 412, 51008, 51008, 264, 917, 295, 264, 1243, 13, 51161, 51161, 639, 11500, 17312, 2715, 611, 3110, 291, 577, 732, 819, 7994, 295, 264, 9834, 51436, 51436, 486, 1477, 281, 819, 2063, 20448, 13, 51594, 51594], "temperature": 0.0, "avg_logprob": -0.0868682861328125, "compression_ratio": 1.7329842931937174, "no_speech_prob": 2.482449644958251e-06}, {"id": 61, "seek": 32284, "start": 322.84, "end": 329.0, "text": " So you can see in the plot that the better fitting blue decision boundary has a lower", "tokens": [50364, 407, 291, 393, 536, 294, 264, 7542, 300, 264, 1101, 15669, 3344, 3537, 12866, 575, 257, 3126, 50672, 50672, 2063, 4972, 281, 264, 2258, 8938, 3537, 12866, 13, 50942, 50942, 407, 365, 264, 26335, 2063, 2445, 11, 321, 434, 586, 1919, 281, 3012, 666, 9275, 16235, 23475, 51216, 51216, 281, 3565, 3142, 24590, 13, 51324, 51324, 961, 311, 352, 536, 300, 294, 264, 958, 960, 13, 51408], "temperature": 0.0, "avg_logprob": -0.11874425583991452, "compression_ratio": 1.5513513513513513, "no_speech_prob": 2.2589333639189135e-06}, {"id": 62, "seek": 32284, "start": 329.0, "end": 334.4, "text": " cost relative to the magenta decision boundary.", "tokens": [50364, 407, 291, 393, 536, 294, 264, 7542, 300, 264, 1101, 15669, 3344, 3537, 12866, 575, 257, 3126, 50672, 50672, 2063, 4972, 281, 264, 2258, 8938, 3537, 12866, 13, 50942, 50942, 407, 365, 264, 26335, 2063, 2445, 11, 321, 434, 586, 1919, 281, 3012, 666, 9275, 16235, 23475, 51216, 51216, 281, 3565, 3142, 24590, 13, 51324, 51324, 961, 311, 352, 536, 300, 294, 264, 958, 960, 13, 51408], "temperature": 0.0, "avg_logprob": -0.11874425583991452, "compression_ratio": 1.5513513513513513, "no_speech_prob": 2.2589333639189135e-06}, {"id": 63, "seek": 32284, "start": 334.4, "end": 339.88, "text": " So with the simplified cost function, we're now ready to jump into applying gradient descent", "tokens": [50364, 407, 291, 393, 536, 294, 264, 7542, 300, 264, 1101, 15669, 3344, 3537, 12866, 575, 257, 3126, 50672, 50672, 2063, 4972, 281, 264, 2258, 8938, 3537, 12866, 13, 50942, 50942, 407, 365, 264, 26335, 2063, 2445, 11, 321, 434, 586, 1919, 281, 3012, 666, 9275, 16235, 23475, 51216, 51216, 281, 3565, 3142, 24590, 13, 51324, 51324, 961, 311, 352, 536, 300, 294, 264, 958, 960, 13, 51408], "temperature": 0.0, "avg_logprob": -0.11874425583991452, "compression_ratio": 1.5513513513513513, "no_speech_prob": 2.2589333639189135e-06}, {"id": 64, "seek": 32284, "start": 339.88, "end": 342.03999999999996, "text": " to logistic regression.", "tokens": [50364, 407, 291, 393, 536, 294, 264, 7542, 300, 264, 1101, 15669, 3344, 3537, 12866, 575, 257, 3126, 50672, 50672, 2063, 4972, 281, 264, 2258, 8938, 3537, 12866, 13, 50942, 50942, 407, 365, 264, 26335, 2063, 2445, 11, 321, 434, 586, 1919, 281, 3012, 666, 9275, 16235, 23475, 51216, 51216, 281, 3565, 3142, 24590, 13, 51324, 51324, 961, 311, 352, 536, 300, 294, 264, 958, 960, 13, 51408], "temperature": 0.0, "avg_logprob": -0.11874425583991452, "compression_ratio": 1.5513513513513513, "no_speech_prob": 2.2589333639189135e-06}, {"id": 65, "seek": 34204, "start": 342.04, "end": 353.68, "text": " Let's go see that in the next video.", "tokens": [50364, 961, 311, 352, 536, 300, 294, 264, 958, 960, 13, 50946], "temperature": 0.0, "avg_logprob": -0.3915248650770921, "compression_ratio": 0.8571428571428571, "no_speech_prob": 0.0001682970905676484}], "language": "en", "video_id": "wB08Jlmhi24", "entity": "ML Specialization, Andrew Ng (2022)"}}