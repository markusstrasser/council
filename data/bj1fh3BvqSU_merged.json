{"video_id": "bj1fh3BvqSU", "title": "Week 11 \u2013 Lecture: PyTorch activation and loss functions", "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Yann LeCun\nWeek 11: http://bit.ly/pDL-en-11\n\n0:00:00 \u2013 Week 11 \u2013 Lecture\n\nLECTURE Part A: http://bit.ly/pDL-en-11-1\nIn this section, we discussed about the common activation functions in Pytorch. In particular, we compared activations with kink(s) versus smooth activations - the former is preferred in a deep neural network as the latter might suffer with gradient vanishing problem. We then learned about the common loss functions in Pytorch.\n0:00:15 \u2013 Activation Functions\n0:14:21 \u2013 Q&A of activation\n0:33:10 \u2013 Loss Functions (until AdaptiveLogSoftMax)\n\nLECTURE Part B: http://bit.ly/pDL-en-11-2\nIn this section, we continued to learn about loss functions - in particular, margin-based losses and their applications. We then discussed how to design a good loss function for EBMs as well as examples of well-known EBM loss functions. We gave particular attention to margin-based loss function here, as well as explaining the idea of \u201cmost offending incorrect answer.\n0:53:27 \u2013 Loss Functions (until CosineEmbeddingLoss)\n1:08:23 \u2013 Loss Functions and Loss Functions for Energy Based Models\n1:23:18 \u2013 Loss Functions for Energy Based Models", "author": "Alfredo Canziani", "keywords": ["Yann LeCun", "Deep Learning", "PyTorch", "NYU", "activation functions", "loss functions"], "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw", "length": 6824, "views": 10550, "publish_date": "11/02/2022", "timestamp": 1593907200, "entity": "Yann LeCun", "transcript": {"text": " All right. So we're going to talk about two or three topics today. And the first one is going to be kind of a review of some of the functions that exist in PyTorch and kind of when and how to use them. So the first set of topics is about activation functions. And there's a whole bunch of them defined in PyTorch. And they basically come from various papers that people have written where they claim that this or that particular objective function or activation function works better for their problem. So of course everybody knows the ReLU. That's a restarted one. But there's lots of variations of ReLUs. These ReLUs where the bottom part is not constant and set to zero, but it can be allowed to change either only with a positive slope or forced to have a negative slope or sometimes being random in the case of the randomized leaky ReLU. So they have nice names like leaky ReLU, PreLU, re-ReLU, random ReLU, et cetera. So leaky ReLU is one where you allow the bottom part to have a slight negative slope. And that kind of prevents the issue that sometimes pops up that when a ReLU is off, it doesn't get any gradient. So here you get a chance for that system that function to actually propagate gradient and perhaps do something useful. It can go all the way to kind of complete full rectification of the signal, kind of like an absolute value if you want. PreLU is one where you... Yeah, go ahead. The previous activation was usually using the discriminator in a GAN such that we always have gradients going backwards for the generator. And also this activation was necessary in order to train the very skinny network I show at the beginning of the class, because again, having like a very, very skinny network, it was basically impossible to get gradients flowing back because we were like ending up in one of the quadrants without, you know, where everything was zero out and then nothing would have been actually trained if you wouldn't have used, you know, this activation function that allows me to get some kind of gradients even if we are in the regions where we are trying to suppress the output. So, yeah. Right. So PreLU is fairly similar except that now the slope and the negative side can be just about anything. And okay, what's interesting about all those functions that we just saw is that there are scaling variant in the sense that you can multiply the signal by two and the output will not be changed. I mean, it would be multiplied by two, but otherwise unchanged. So they are equivalent to scale. There's no sort of intrinsic scale in those functions, right? Because there's only one non-linearity and it's a sharp one. So now we're getting into functions where the scale matters. So the amplitude of the incoming signal will affect the type of non-linearity that you're going to get. And one of those is the soft plus. So soft plus is sort of a differentiable version of ReLU if you want. It's kind of the soft version of positive part. And it's usually parameterized as you can see at the top here, one over beta log one plus exponential beta x. So it's kind of like the log sum exponential that we've been using a lot for sort of various purpose, except here one of the terms in the sum is equal to one, which is kind of like exponential zero if you want. So that looks like kind of a function that sort of asymptotically is the identity function for large positive values and asymptotically zero for negative values. So it approximates the ReLU. It has a scale parameter though. This beta parameter is a parameter. The larger beta, the more the function will look like a ReLU. So the kick will be kind of the corner will be kind of sharper if beta goes to infinity. But that function has a scale. Now you can parameterize those functions in various ways. And this is sort of another example of kind of a soft version of ReLU if you want, where here you use ReLU as a basis and then you add a small constant to it that kind of makes it smooth. I can't tell you that any of those has any particular advantage over the others. It really depends on the problem, but they all have kind of similar properties if you want. This also you can make sort of continuously closer to ReLU. So one difference here in this case is that this guy actually goes negative. So unlike the ReLU that has its minimum at zero, its horizontal asymptote at zero, this guy goes below zero. And that may or may not be advantageous depending on the application you have. Sometimes it's advantageous because it allows the system to basically make the average of the output zero, which is advantageous for certain types of for gradient descent convergence. The weights that are connected to units like this will see both positive and negative values, which will then converge faster than if they only see positive values. So it's a bit the same here and it's just a kind of a differently, a different parameterization of the same thing if you want, with different properties. So of course there's tons of variations of this with various parameters with different properties. And some of them that have particular properties that kind of relate them to Gaussian distributions, for example. This is not the cumulative distribution of a Gaussian. But okay, so those were things that have one kinks in them. And if the kink is sharp, there's no scale. If the kink has some scale in it, there is some scale, but it's still sort of a single kink non-linearity. Now we're getting into non-linearities that have two kinks. So this one is basically a saturating value. I'm not sure why saturates at six. Why not? But why not parameterize this a little better? So here's a smooth function that you're familiar with because it's used in recurrent nets, in gated recurrent nets, in an STM, in softmax. Basically this is a two-way softmax you can think of. Basically this is a two-way softmax. You can think of it this way. And this is just a function that goes kind of smoothly between zero and one. It's sometimes called a Fermi-derived function as well because it derives from some work in physics. It's the SQL physics. And then there is the hard probability tangent that we also talked about. It's basically identical to the sigmoid, except it's centered. So it goes between minus one and plus one, and it's a little, it's twice the amplitude and the gain is a little different. But it plays the same role. The advantage of hard probability tangent is that the output is, you can expect the output to not have zero mean, but be close to having zero mean. And again, that's advantageous for the weights that follow because they see positive and negative values and they tend to converge faster, if that's the case. I used to be a big fan of those. Unfortunately, if you stack a lot of sigmoids in many layers in a neural net, you can tend to not learn very efficiently. You have to be very careful about normalization if you want the system to converge if you have many layers. So in that sense, the single key functions are better for deeper networks. So his soft sign, this is basically a bit like the sigmoid, except that it doesn't get to the asymptote as fast. So it doesn't get stuck towards the asymptote as quickly. So one problem with high probability tangent and the sigmoid is that when you get close to the asymptote, the gradient goes to zero fairly quickly. And so if the weights of a unit become too large, they saturate this unit and the gradients get very small and then the unit doesn't run very quickly anymore. It's a problem that exists both in sigmoids and hyperbolic tangent. And so soft sign is a function that was proposed by Yoshio Benjiu and some of his collaborators and it saturates slower. So it doesn't have that same problem. I mean, it has the problem also, but not to the same extent. And this is the opposite, hard tangent, hard 10H. I don't know if it deserves that name, but it's basically just a ramp. And that works surprisingly well, particularly if your weights are somehow kept within a small value so the units don't saturate too much. It's surprising how well it works. And people have used this in various contexts. But that's sort of non-standard. So hard threshold is very rarely used because you can't really propagate gradient through it. And this is really what kept people from inventing backprop in the 60s and 70s, which is that they were using binary neurons. And so they didn't think of the whole idea of gradients because of that. Those other functions are rarely used in the context of neural nets or at least for activation function in a traditional neural net. They're used mostly for sometimes for things like sparse coding. So one step in sparse coding consists in to compute the value of the latent variable, consists in shrinking all the values in the latent variable, in the latent vector by some value. And you do this with a shrink function, a shrinkage function. This is kind of a soft version of a shrinkage function. The hard version is here. I mean, it's called soft shrink, but it actually has corners in it. The reason it's called soft shrink is because there is a hard shrink that looks different that I'll show you in a minute. So this basically just changes a variable by a constant towards zero. And if it goes below zero, it's clumped at zero if it's brought too long. And so this is basically just the identity function to which you subtract hyperbolic tangent to make it look like a shrink. Basically, if we try to get whatever value close to zero, they actually are forced to zero, basically. Right. So small values are forced to zero. Others are shrunk towards zero. But since they're large enough, they're not going to get to zero. So again, that's used mostly as you can think of it as a step of gradient for an L1 criterion. So if you have a variable, you have an L1 cost function on it. And you take a step in a negative gradient of the L1. So L1 cost is an absolute value. This will cause the variable to go towards zero by a constant, which is the slope of that L1 criterion. And to stay at zero coming from either side, it doesn't overshoot if you want. And so that's the non-linear function you use. And that's one of the steps in the ISTA algorithm that is used for inference in sparse coding. But again, it's rarely used in regular neural nets. Unless you encoder is kind of used as kind of an estimation of sparse coding. This is the hard shrink. So hard shrink basically clamps every value smaller than lambda to zero. So if a value is smaller than lambda or larger than minus lambda, it's sort of between minus lambda and lambda. When lambda is some constant, you just set it to zero. Again, it's used for things like certain types of sparse coding, but rarely as an activation function in the neural net. So a log-seq mode is mostly used in cost functions. Not really as an activation function either. But it's a useful function to have if you want to plug this into a loss function. And we'll see that in a minute. So something we've seen, this is the same as softmax, except you have minus signs. So those are multidimensional non-linearities. You have a vector in and you get a vector out, which is the same size as the input vector. And we know that softmax is exponential xi divided by sum over j of exponential xj. This is softmin, where you put the minus sign in front of the x. So you view the x's if you want as energies instead of scores, as penalties instead of scores. And it's a good way of turning a bunch of numbers to something that looks a bit like a probability distribution, which means numbers between zero and one that sum to one. And that's the softmax, which we all know. So log-softmax, again, is not very much used as a non-linearity within the neural net, but it's used a lot at the output as kind of one piece of a loss function. And we'll see this in a minute. OK, so those questions. We have a question. So for preload, I'm not sure I understand number one, why we want the same value for all channels. And number two, how learning A would actually be advantageous. You could have a different A for different channels. So different units can have a different A. You could use this as a parameter of every unit. Or not. It could be shared. That's kind of up to you. It could be shared at the level of a feature map in a convolutional net, or it could be shared on all feature maps, or it could be individual to every unit. If you really want to preserve the convolutional nature of a convolutional net, you probably want to have the same A for every unit in the feature map. But you can have different A's for different feature maps. OK, what was the second question? Why learning actually, a specific value would be advantageous? Like, why are we learning A? You can learn it or not. You can fix it. The reason for fixing it would be not necessarily to have sort of more powerful non-linearity, but to kind of ensure that the non-linearity gives you a non-zero gradient, even if it's in the negative region. So, you know, learnable, not learnable. So to make it learnable allows the system to basically turn a non-linearity into either a linear mapping, which of course is not particularly interesting, but why not, a value, or something like a full rectification, OK, where A would be minus 1 in the negative part, which can be interesting for certain types of applications. So for example, if you have a convolutional net that has an edge detector, an edge detector has a polarity, right? It's got plus coefficients on one side, minus coefficients on the other side. And so it's going to react. So if you have an edge in an image that goes from, say, dark to bright, the convolution will react positively to this one. But if you have another edge from, you know, in the opposite direction, then the filter will react negatively. Now, if you want your filter to react to an edge regardless of its polarity, you rectify it. OK, so that would be kind of just absolute value. Now, you could, of course, bake this in. You don't have to use a prelude. You can just use the absolute value. Probably a better idea is to use a square, actually. So if you take the square nonlinearity, it's not implemented as kind of a neural net nonlinearity. But, you know, in the functional form of PyTorch, you just write square, and that's it. Hope I answered the question. Any other question on this topic? I have a question. It seems to me like these nonlinearities are trying to basically make a linear function nonlinear, and the tweak in the lines denote the change in that function. So can we think of this as if we want to model a curve in the line, should we have learnable parameters on both, like before the zero and after the zero on the x-axis? Well, yeah, I mean, there is diminishing return. So the question is, you know, how complex do you want your nonlinearity to be? So you could imagine, of course, parameterizing an entire nonlinear function, you know, with spline parameters or Bezier curves or something like this, right? Or, I don't know, Chibychev polynomials. You know, I mean, you can parameterize any mapping you want, right? You can imagine those parameters could be part of the learning process. However, you know, what is the advantage of doing this versus just, you know, having more units in your system and relying on the fact that multiple units will be added in the end to approximate the function you want? Generally, it really depends on what, like, if you want to do regression in a fairly low dimensional space, so perhaps you want some parameterized nonlinearities, that might help. You might have like, you might want to have a collection of different nonlinearities with maybe things like Chibychev polynomials if you want to do good approximations. But for, like, you know, high dimensional tasks like image recognition or things like this, you just want a nonlinearity. And it works better if the nonlinearity is monotonic. Otherwise, it creates all kinds of issues because you can have two points that will produce a nonlinear function. So, you know, you can have the same output, and so it's a little ambiguous for the system to learn the right function there. So, you want it, it's much better if the function is monotonic, and almost all the functions here are monotonic, except if you have a negative a here in the in the prior case. So, there's a big advantage to having monotonic functions. But in principle, you could parameterize, you know, any function you want. People have played with this, you know, they're not very popular because mostly they don't seem to be bringing a huge advantage in the kind of applications that people use large neural nets for. Other questions? Another question is going to be King vs. Smooth. Yeah. So, the question I've had is, can you think of any application where the choice of nonlinearities made a big impact? The only thing I'm aware of is using a single function instead of a double King for deep neural networks helps it train better. Well, so here's the problem with double King. Double King has a built-in scale in it, which means if the weights of the incoming layer are multiplied by two, or if the signal amplitude is multiplied by two, the result on the output would be completely different, right? Because you will be, you know, the signal would be more in the nonlinearity. So, you'll get completely different behavior of your layer. Whereas if you have a function with only one King, if you multiply the input by two, the output gets also multiplied by two, you know, modular bias, but the signal bias is fine. So, but what I mean to ask is, can you think of a situation where the choice of activation function made a big difference in the performance of the model, except for deep networks using Prud'Innu instead of Sigmoid? There is no sort of general answer to this. Like, if you're going to use attention, you have to use softmax. I mean, you have no choice, right? I mean, it's not like you have to use softmax, but you want to have something where you get coefficients, right, to kind of focus the attention of the system on, or to kind of spread the attention of the system and not allow it to cheat, which is to pay attention to multiple things at one time, you have to have some sort of normalization of the coefficients that come out of the attention system, right? So, normally in most attention systems, like in transformers and stuff, the coefficients are passed through softmax. So, you get a bunch of coefficients that are between 0 and 1 and sum to 1. And so, that causes the system to have to pay attention to a small number of things, right? It can only concentrate the coefficients on a small number of items and it has to spread it, right? There are other ways to do normalization. You can do, and in fact, there is something that's wrong with softmax normalization for transformers or for attention, which is that if you want a coefficient coming out of a softmax to be close to 0, you need the input to be close to minus infinity, okay, or to be considerably smaller than the largest one, right? When you go into a softmax, one output, the largest input is going to cause the corresponding output to be large. But if you want that output to be close to 1 and all the other ones to be close to 0, you basically want this input to be extremely large and all the other ones to be large and negative, okay? Now, that can be a problem when what you are computing at the input are dot products, because the result is that the easiest way for a system to produce a small dot product is to have two vectors that are orthogonal to each other, in which case the dot product is 0. If you insist that the dot product should be very, very small, then either you have to make the two vectors basically point in opposite directions and you have to make them very long. And that's not so great. And so, using softmax for attention basically limits the contrast that you're going to have between the coefficients, which is not necessarily a good thing. So, same thing for LSTM, gated, recurrent nets, et cetera. You need sigmoids there, because you need coefficients that are between 0 and 1 that either reset the memory cell or make it a pass through so that it keeps its previous memory or kind of write the new input in it. So, there it's nice to have an output that varies continuously between 0 and 1. There you have no choice. So, I mean, I don't think you can say just, you know, if you have a very small dot product, you have no choice. So, I mean, I don't think you can say just, you know, in generic term, you know, this non-linearity is better than this other one. There are certain cases where it learns better. There are certain cases where it relieves you from having to initialize properly. There are certain cases where it works better if you have lots of layers, like, you know, single-kick functions work better if you have lots of layers, better than sigmoid-like functions. There's no kind of, there's no simple answer, basically. I had a question just regarding the general differences between a non-linear activation that has kinks versus a smooth non-linear activation. Is there sort of any general reason or rule to why we would prefer to have kinks in the function or not? It's a matter of scaling or scale equivalent. So, if the kink is hard, again, you multiply the input by two, the output is multiplied by two, but otherwise unchanged. Okay. If you have a smooth transition, if you multiply the input by, let's say, 100, the output now will look like you had a hard kink, okay, because the smooth part now has become shrunk by a factor of 100. If you divide the input by 100, now the kink becomes a very, very smooth sort of convex function. Okay. So, it changes the behavior by changing the scale of the input, you change the behavior of the of the unit. And that might be a problem sometimes because when you train a multilayer neural net and you have two layers that are one after the other, you don't have a good control for, like, how big the weights of this layer are relative to that other weight. So, imagine you have a two-layer network where you don't have a non-linearity in the middle. So, the system is completely linear, right? If the network has arrived at a solution, you can multiply the incoming, the first layer weight matrix by two, divide the second weight matrix by two, and overall the network will have exactly the same output. Okay. You won't have to change anything. What that means is that when you do training, there is nothing that forces the system to have a particular scale for the weight matrices. So, now if you put a non-linearity in the middle and you still don't have any constraint for the system to kind of have scales for the first layer weight versus the second layer weight, you'd better have a non-linearity that doesn't care about scale. Okay. So, if you have a non-linearity that does care about scale, then your network doesn't have the choice of what size weight matrix it can use in the first layer because that will completely change the behavior. And it may want to have large weights for some other reason, which will saturate the non-linearity and then kind of create vanishing gradient issues. So, it's not entirely clear why is it that deep networks work better with single king functions, but it's probably due to that scaling variance property or scale equivalence property. Now, there would be other ways of fixing this problem, which would be to basically set a hard scale on the weights of every layer. So, you could normalize the weights of the layers so that the variance of things that go into a unit is always constant. In fact, that's a little bit what batch normalization does or the various normalization schemes. They do that to some extent. They put the mean at zero and the variance is constant. So, now the variance of the amplitude of the output doesn't depend on the size of the weights because it's normalized. So, that is partially why things like batch norm and group norm and things like this help. It's because they can fix the scale a little bit. But then if you fix the scale, then with something like batch norm, the system now doesn't have any way of changing the behavior of the layers. So, with something like batch norm, the system now doesn't have any way of choosing which part of the non-linearity is going to use in the two-kink function system. So, things like group normalization or batch normalization are incompatible with sigmoids if you want. If you have a sigmoid, you don't want normalization just before it. I see that provides some really good intuition. Thank you. Okay. Any other questions? I have one more question. I noticed in a softmax function, some people use the temperature coefficient. So, in what cases would we want to use the temperature and why would we use it? Well, to some extent, the temperature is redundant with incoming weights. So, if you have weighted sums coming into your softmax, having a beta parameter in your softmax equal to two instead of one is the same as just making your weights twice as big. It has exactly the same effect. Okay. So, that beta parameter is redundant with the size of the weights. But again, if you were or the size of the weighted sum, the variance of the weighted sums if you want. But again, if you have a batch normalization in there, then the temperature parameter matters because now the input variances are fixed. So, now the temperature matters. The temperature basically controls how hard the distribution on the output will be. So, with a very, very large beta, you basically will have one of the outputs equal to one and all the other ones very close to zero. I mean, very close to one and very close to zero. Where beta is small, then it's softer. In the limit of beta equal to zero, it's more like an average actually that you get. Softmax behaves a little bit like an average. So, beta goes to infinity, it behaves a bit like argmax and beta goes to zero, it behaves a bit like an average. So, if you have some sort of normalization before the softmax, then tuning this parameter allows you to control this kind of hardness. And what people do sometimes in certain scenarios is that they start with a relatively low beta so that the numbers that are produced are kind of soft. So, you get gradients everywhere. It's kind of well-behaved in terms of gradient descent. And then as running proceeds, if you want harder decisions in your attention mechanism or whatever, you increase beta. You increase beta. And so, that makes the system kind of make harder decisions. It doesn't run as well anymore, but presumably after a few iterations, it's kind of in the right boat park. So, you can sort of sharpen the decisions there by kind of increasing beta. It's useful, for example, in a mixture of experts and self-attention systems are kind of, you can think of as sort of a weird form of mixture of experts. So, in a mixture of experts, you have multiple sub-networks, and their outputs are kind of linearly combined with coefficients that are the output of a softmax itself controlled by another neural net. So, if you want kind of a soft mixture, you have a low beta. And as you increase beta to infinity, basically, you're going to select one of the experts and ignore all the other ones. That might be useful, for example, if you want to train a mixture of experts or an attention mechanism. But in the end, you want to save computation by just determining which expert do I need to compute and just not computing the other ones. So, in that case, you want those coefficients to be basically either one or zero. And you can train the system progressively to do this by increasing beta. This is cool. The physicists have a name for this because the use is going to trick so various other things. That's called annealing. It has the same meaning as, so annealing comes from metalwork, right? You're making a steel or something, and you make a sword or something, right? And you heat it up, and then you cool it. And depending on whether you cool it quickly or slowly, you change the crystalline structure of the metal. So, this idea of annealing, of progressively lowering the temperature, corresponds to this increasing this beta. Beta is like an inverse temperature. It's akin to an inverse temperature. Any other question? I think we are good. All right. Okay. So, next topic is loss functions. So, PyTorch has a whole bunch of loss functions, as you might have seen. And, of course, there are things that are simple ones like mean squared error. So, I don't need to explain to you what it is. You know, compute the square of the error between the desired output, y, and the actual output, x. And if it's over a mini-batch with n samples, then you have, you know, n losses, one for each of the samples in the batch. And you can tell this loss function to either keep that vector or to kind of reduce it by computing a mean or a sum. Okay. Very simple. Here's a different loss. That's the L1 loss. So, this is basically the absolute value of the difference between the desired output and the actual output. And you want to use this to do what's called robust regression. So, if you want small errors to count a lot and large errors to count, but, you know, not as much as if you use the square, perhaps because you have noise in your data. So, you know that you have a bunch of data points. You have a bunch of data points. You have a bunch of data points. You're trying to kind of train a neural net or something to kind of, you know, fit a curve or, you know, do regression. But you know that you have a few outliers. So, you have a few points that are, you know, very far away from where they should be just because, you know, the system has noise or something, or the data was collected with some noise. So, you want the system to be robust to that noise. You don't want the cost function to increase too quickly as the points are far away from, you know, the kind of the general curve. So, L1 loss would be more robust. Now, the problem with L1 loss is that it's not differentiable at the bottom. And so, you know, you have to kind of be careful when you get to the bottom of how you do the gradient. That's basically done with a soft shrink, essentially. That's the gradient of the L1 loss. Now, to correct for that, people have come up with various ways of kind of making the L1 loss robust for large losses, but then still smooth at the bottom, kind of behaving like a squared error. So, you know, you have to be careful when you are at the bottom. So, an example of this is this particular function, smooth L1 loss. It's basically L1 far away, and it's sort of L2 nearby. And that presents, sometimes that's called a Huber loss. Some people call this also elastic network because on this old paper from the 1980s or 1990s, that kind of proposed this kind of objective function for a different purpose. So, that's useful. That was advertised by Was Gorsuch in the Fatsop CNN paper for, and it's used quite a bit in computer vision for various purposes. Again, it's for protecting against outliers. Also, Giza is sharper. Giza is also sharper, or so it's not when we do like image prediction. Sharper than using the MSC? Not particularly. I mean, it's just like the MSC for small errors. Okay. So, that doesn't make any difference, but it doesn't, or maybe I misunderstood what your point was. Sorry, I was trying to compare the L1 versus the L2. The L1 is a little bit more blurry predictions whenever we try to do a prediction by using the L2, minimizing the L2, whereas people are minimizing the L1 in order to have sharper overall predictions. Okay. So, if you take a bunch of points, if you take a bunch of Y values, and you ask the question, what value does the L1 give to the L2? If you take a bunch of points, if you take a bunch of Y values, and you ask the question, what value, so you take a bunch of points on Y, and you ask the question, what value of Y minimizes the square loss? The answer is the average of all the Ys. Okay. Okay. So, if for a single X, you have a whole bunch of Ys, which means you have noise in your data, your system will want to produce the average of all the Ys that you're observing. Okay. And if the Y you're observing is not a single value, but is, I don't know, an image, the average of a bunch of images is a blurry image. Okay. That's why you get those blurry effects. Now, with L1, the value of Y that minimizes the L1 norm, the L1 distance, so basically the sum of the absolute values of the differences between the value you're considering and all the points, all the Y points, that's the median. Okay. So, it's a given point. All right. Uh-huh. I see. The median, of course, is not blurry. The median of the image is not blurry. It's just an image. Although it's kind of difficult to define in multiple dimensions. But so one problem with this loss is that it has a scale, right? So here, the transition here is at 0.5, but why should it be at 0.5? You know, it could be, it depends what the scale of your errors are. Okay. Negative log-like-a-good loss. This is really not the negative log-like-a-good loss. I'm not sure why it's called this way. It's not the negative log-like-a-good loss. It's the negative log-like-a-good loss. Okay. Negative log-like-a-good loss. This is really not the negative log-like-a-good loss. I'm not sure why it's called this way in PyTorch. But basically, here, imagine that you have an X vector coming out. Okay. And your loss function is there is one correct X. Okay. So imagine each X correspond to a score for, let's say, multi-class classification. One particular index in that vector. Okay. And what you want is you want to make that score as large as possible. Okay. If those scores are likelihoods, then this is minimum negative log likelihood. If those scores are log likelihoods, then this is maximum likelihood or minimum negative log likelihood. Okay. But there is nothing in this module that actually specifies that the L's have to be log likelihoods. So this is just, you know, make my desired component as large as possible. That's it. If you put negative signs in front, so now you can interpret the X's as energies as opposed to scores. Okay. They're not positive scores. They're like penalties, if you want. But it's the same. So the formula here says, you know, just pick the X that happens to be the correct one for one sample in the batch and make that score as large as possible. Now, this particular one allows you to give a different weight to different categories, which is those W's. It's a weight vector that gives a weight to each of the categories. It's useful in a lot of cases, particularly if you have widely different frequencies for the categories. You might want to increase the weight of samples for which you have a small number of examples. I mean, for categories for which you have a small number of samples. However, I'm actually not a big fan of this. I think it's a much better idea to just increase the frequency of the samples from the class that appears rarely so that you equalize the frequencies of the classes when you train. It's much better because it exploits stochastic gradient in a better way. The bottom line of that is, let me actually draw a picture of this. Let's say you have a problem where you have tons of samples for category one and then a small number of samples for category two and a tiny number of samples for category three. Let's say here you have, I don't know, a thousand samples and here you have 500 samples and here you have, I don't know, 200 samples. What you could do is, using this kind of weight function, you could give this a weight of one and this guy a weight of two and this guy a weight of five. Then you can equalize the weights if you want. It's probably better to make sure that the weights normalize to one. That would be probably a better idea. What I recommend is not that. What I recommend is when you pick your samples, you basically pick one sample from class one and then one sample from class two and sample from class three. Then you keep doing this during your training session. When you get to the end of class three, you go back to the beginning. You keep going here, but here you go back to the first sample. Keep going here, go back and now you have the second sample. Now you get to the end of class two, go back to the start. The next sample is going to be here, here and here and then the next one here, here and here, here, here and then this guy wraps around again, etc. You basically have equal probability, equal frequencies for all the categories by just going through those kind of circular buffers more often for categories for which you have fewer samples. One thing you should absolutely never do, is equalize the frequencies by just not using all the samples in categories that are frequent. I mean that's horrible. You should never let any data on the floor. There's never any reason to leave data on the floor. Now here's a problem with this. The problem with this is that after you've trained your neural net to do this, your neural net does not know about the relative likelihood, the relative frequencies of the samples. Let's say this is a system that does medical diagnosis. It doesn't know that the common cold is way, way more frequent than lung cancer or something. What you need to do in the end is do a pass, a few passes perhaps, where you can fine-tune your system with the actual frequencies of the categories. The effect of this is going to be for the system to adapt the biases at the output layer so that the likelihood of a diagnosis corresponds to the frequency of it. It's going to favor things that are more frequent. The reason why you don't want to do this during the entire training is because if you train a multilayer net, it's going to be able to adapt the frequencies that the system basically never develops the right features for rare cases. I may have spoken about this already in the class in past weeks. To recycle the example of medical school, when you go to medical school, you don't spend time studying the flu that is proportional to the of the flu with respect to very rare diseases, for example. You spend basically the same time studying all the diseases. In fact, you spend more time studying complicated one, which usually tend to be rare. And that's because you need to develop the features for it. And then you need to kind of correct for the fact that those rare diseases are rare. So you don't suspect the diagnosis for rare diseases very often, because it's rare. Okay. So that's all for weights. Cross-entropy loss. So you've been using this a lot, of course. And cross-entropy loss is a kind of merging of two things. The merging of log softmax function and negative likelihood loss. And the reason why you want to have this is for numerical reasons. So the log softmax is basically a softmax followed by a log. So you first compute the softmax, then you do the log. If you do softmax, then log, and you back propagate through this, you might have gradients in the middle between the log and the softmax that end up being infinite. So for example, if the maximum value of one of the softmax is close to one, and some of the other ones are close to zero, you take the log, you get something that's close to minus infinity. You back propagate through the log, you get something that's close to infinity. Because the slope of log close to zero is very, very close to infinity. But now you multiply this by a softmax that is saturated. So it's multiplied by something that's very close to zero. So in the end, you get a reasonable number. But because the intermediate numbers are close to infinity or zero, you multiply something that's close to plus infinity by something that's close to zero, you get numerical issues. So you don't want to separate log and softmax. You want to do log softmax in one go. It simplifies the formula. It makes the whole thing much more stable numerically. And for similar reasons, you also want to merge log softmax and negative log likelihood loss. So basically, if you have log softmax and negative log likelihood loss, it says, I got a bunch of weighted sums. I'm going to pass them to the softmax. I'm going to take the log of those. And then I want to make the output of the log softmax for the correct class. As large as possible. That's what the negative log likelihood loss does. It wants to make the score of the correct class as large as possible. We saw that just a minute ago. When you back propagate through the log softmax, as a consequence, it's going to make the score of all the other classes as small as possible because of the normalization. And so that's why sometimes the whole idea of building a network by modules, sometimes there is an advantage instead of merging the modules into a single one by hand. Right. So the cross entropy loss, in fact, this explains a little bit those numerical simplifications. So the loss takes an x vector and a desired category, a class, and computes the negative log of the softmax applied to the vector of scores. But the one that's on the numerator here is the x of the index of the correct class. So that's your loss. The negative log of exponential, the score of the correct class, divided by the sum of the exponentials of all the scores. You can think of the x's as negative energies. It's completely equivalent. Now, when you do the math and you simplify, the log and the exponentials kind of simplify. And so you just get the score of the correct class, the negative score of the correct class. So to make that small, you make the score large. And then plus the log of the sum of the exponentials of the scores of all the other class to make that small, you make all the xj's small, negative, as far as, you know, as negative as possible. Okay. So this will make the score of the correct class large, make the score of everything else small. Again, like in the NLL, you can have a weight per category. Also, there is a physical interpretation of the cross entropy. Right. Okay. So why is it called cross entropy? Because it is the cross entropy between two distributions. It's the KL divergence really between two distributions. It doesn't appear clearly here in this formula, but think of the softmax applied to the x vector as a distribution. Okay. So we'll take the x factors, the scores, rather than to a softmax, you get a bunch of numbers between 0 and 1 that's onto 1. And now you have a desired distribution and the desired distribution, the target distribution, if you want, is one in which all the wrong categories have 0 and the correct category has 1. Okay. Now compute the KL divergence between those two distributions. Okay. It's the sum over indices of the correct probability, okay, which is 0, except for one term, times the ratio between the log of the probability that the system produces and the correct probability, which is 1. Okay. So all of those terms reduce to kind of a single term, which is just the one for which the correct probability term is 1. Okay. So we end up with this term. It's just the negative log of the softmax output for the correct class. Okay. We can view this as a cross entropy between the distribution produced by the system and the one hot vector corresponding to the desired distribution, if you want. Okay. So now there would be another kind of more sophisticated version of this, which would be the actual KL divergence between the distribution produced by the system and a distribution that you propose, whatever it is, a target distribution, which now is not binary. It's not the one hot vector anymore, but it's just a vector of numbers. And that's called the KL divergence class. In fact, it's, we'll see it in a minute. So KL divergence is a kind of, you know, it's not a distance because it's not symmetric, but it's sort of a divergence between, between distributions, discrete distributions. Okay. So this one is a bit of a kind of an extension if you want of log softmax. And it's a version of it that is applicable for very, very large categorization. So if you have many, many, many categories, what you might want to do is kind of cut some corners. You don't want to compute a giant softmax over say a million categories or maybe even more. So there you can sort of basically ignore the ones that are small and, you know, kind of use tricks to kind of, you know, improve the speed of the, of the computation. And this is what this does. I'm not going to go into the details exactly what it does because actually I don't know the details, but it's basically an efficient approximation of softmax for a very, very large number of categories. So this is a special case of cross entropy when you only have two categories. And in that case, it kind of reduces to something simple. So this does not include softmax. This is just a cross entropy when you have two categories. And as I said before, the cross entropy loss is the sum over categories of the probability, I mean, sum over indices or some of the categories of the probability for the target, the target probability for that category times the ratio between the log of the probability for produced by the system divided by the probability of the target category. And if you work it out for two categories, necessarily one score is one minus the other one. If you have two exclusive categories and it comes down to this. Okay. Now, this supposes that x and y are the same. And it comes down to this. Okay. Now, this supposes that x and y are kind of probabilities. They have to be between strictly between zero and one. I mean, not strictly, but well, kind of strictly because otherwise the logs kind of blow up. Here is the KL divergence loss I was telling you about earlier. So here it's written here in a funny form, but it's basically the... Here again, it sort of assumes... This is another one I was telling you about earlier, actually. This one is also a simplified one when you have a one-hot distribution for the target. So y is a category. But it has a disadvantage of not being merged with something like softmax or lux softmax. So it may reach... I mean, it may have kind of numerical issues. Again, it assumes x and y are distributions. This is barely used, Poisson loss. Okay. So this version of the binary cross entropy here takes scores that haven't gone through a sigmoid. So this one does not assume that the x's are between zero and one. It just takes values, whatever they are, and it passes them through a sigmoid to make sure they are between zero and one strictly. Okay. And so that is more likely to be numerically stable. It's a bit the same idea as kind of merging lux softmax and negative log likelihood. Very... Yeah, same thing here. That's what I was talking. Okay. Margin losses. So this is sort of an important category of losses. Those losses basically say, if I have, in this case, two inputs, the loss function here says, I want one input to be larger than the other one by at least a margin. Okay. So imagine the two inputs are scores for two categories. You want the score for the correct category to be larger than the score for the incorrect category, by at least some margin that you pass to the system. And then the last function here says, I want the last input to be larger than the other one by at least a margin. And that's the formula you see down there. So it's basically a hinge. Okay. And it takes the difference between the two scores. And so y is a binary variable. This plus one or minus one, and it controls whether you want x to be larger than x1 to be larger than x2, or whether you want x2 to be larger than x1. Okay. We basically give it two scores, and you tell it which one you want to be the largest score. And then the cost function says, you know, if this one is larger than that one by at least a margin, then the cost is zero. If it's smaller than the margin, or if it's in the other direction, then the cost increases linearly. Okay. So that's called a hinge loss. Okay. So that's very useful for a number of different things. We've seen an example of this in... So, yeah, for example, so this is sort of a margin ranking loss. So you have two values, but there are sort of... There's a simplified version of it. I mean, there's a simpler version of it, which I don't have here for some reason. We only have an x. Okay. So basically the loss is max of zero and minus x times the margin. And it just wants to make x smaller than the margin. Right. And so this is sort of a special case where you have a ranking between two scores, of two categories. So here is how you would use this for classification. You would basically run your classifier. You would get scores. Okay. So before you do any non-linearity weighted sums, and then you know the correct category. So you say, I want this correct category to have a high score. And then what you do is you take another category that has the most offending score. So either another category, so a category that is incorrect, that has a higher score than the correct one, or that has a lower score, but the lower score is too close. Okay. So you take the category that whose score is the closest to the correct one, or whose score is higher than the correct one. And you feed those two scores to a loss function like this. So basically it's going to push up the score of the correct category, push down the score of the incorrect category, until the difference is at least equal to the margin. Okay. And that's a perfectly good way of training something in the context of an energy-based model. For example, that's one of the things you might want to do. You might want to say x1, or minus x1, is the energy, I mean, minus x1 would be the energy of the correct answer, and minus x2 would be the energy of the incorrect answer, like a contrastive term, an incorrect answer. And you want to push down the energy of the correct answer, push up the energy of incorrect answer, so that the difference is at least some margin. Okay. You can use this kind of loss for that. The triplet loss is going to be a refinement on this. So this is used a lot for metric learning, for the kind of Samir's nets that Ishan Mishra was talking about last week. And there the idea is, let's say I have a distance, or let's say I have three samples. I have one sample and another sample that's very similar to it. I run them through two convolutional nets. I get two vectors. I compute the distance between those two vectors, d of a i p i, for example. Okay. I want to make this distance as small as possible, because that's the correct sample. And then I take two samples that I know are semantically different. Okay. The image of a cat and one of a table. And I want to make the vectors far away from each other. So I compute the distance, and I want to make this distance large. All right. Now, I can insist that the first distance be zero, and I can insist that the second distance be larger than the margin. That would be kind of a margin loss type thing. But what I can do is one of those triplet margin loss, where I say the only thing I care about is that the distance that I get for the good pair is smaller than the distance that I get for the bad pair. I don't care if the distance is small. I just want it to be smaller than the distance for the bad pair. Okay. And that's what those ranking laws do. A bunch of those were, one of the first, I think, that was proposed was by Jason Weston and Sami Benjo, back when Jason Weston was still at Google. And they used this to train kind of an image search system for Google. So back then, I'm not sure it's true anymore, but back then you would type a query on Google. Google would encode that query into a vector. Then we compare this to a whole bunch of vectors, describing images that have been previously indexed. And then we kind of retrieve the images whose vector were close to the one that you had. And the way you train those networks that compute those vectors, in that case, back then it was linear networks, actually, is you train them with those triplet loss. Okay. So you said, good hits for my search should be less than one. So the search should have a distance between the vectors that is smaller than any bad hit. And I don't care if the distance is small. I just want it to be smaller than for bad hits. Any question? That's kind of a graphical explanation of this, where P is a positive sample. So it's similar to A. So A is the sample you considered. P is kind of a positive sample. And N is a negative sample, a contrastive sample. You want to push N away and bring P closer. And as soon as P is closer than N by some margin, you stop pushing and pulling. You have soft versions of this. And in fact, you can think of NCE, the kind of loss function that Ishan was talking about, as kind of a soft version of that, where you basically, you have a bunch of positives and a bunch of negatives, or you have one positive and a bunch of negatives, and you run them through a softmax. And you say, I want the e to the minus distance for the correct one to be smaller than e to the minus the other one. So it kind of pushes the positive closer to you and pushes the other ones further to you, but now with some sort of softmaxy exponential decay, as opposed to sort of a hard margin. So in PyTorch, you have things that allow you to have multilabel. So this allows you to basically give multiple correct outputs. So instead of, this is a ranking loss, but instead of insisting that there is only one correct category, and you want a high score for the correct category and a bad score for everything else, here you can have a number of categories for which you want high scores, and then all the other ones will get pushed away. All right, we'll get, their scores will be pushed down. So here it's a hinge loss, but you do a sum of this hinge loss over all categories. And for each category, if the category is a desired one, you push it up. If it's a non-desired one, you push it down, which is what the Cedula formula says. And of course you have the soft version of this, which I'm not going to go into the details of, and the multi-margin version of it. So this pushing and pulling for metric learning for embedding for sine these nets that I was telling you about, it's actually kind of all implemented if you want in one of those hinge embedding laws. So hinge embedding laws is a loss for sine these nets that kind of pushes things that are symmetrically similar to you and push away things that are not. Okay, so the Y variable indicates whether the pair you are, or whether the score you are giving to the system is one that should be pushed up or one that should be pushed down. And it chooses a hinge loss that makes the score positive if Y is plus one, and it makes the score negative by some margin delta if Y is minus one. So this is a very simple example of how to do this. So let's go ahead and if Y is minus one. Very often when you are doing sine these nets, the way you compute the similarity between two vectors is not through a Euclidean distance but through a cosine distance. So V1 minus the cosine of the angle between the two vectors. This is basically a normalized Euclidean distance if you want. You can think of it this way. The advantage of this is that whenever you kind of push the distance, whenever you have two vectors and you want to make the distance as large as possible, there's a very easy way for the system to get away with it by making the the two vectors very large, very long, you know, not pointing in the same direction and make them very very long. So now the distance would be large. But of course that's not what you want. You don't want the system to just make the vectors bigger. You want it to actually rotate the vector in the right direction. So you normalize the vectors and then computer normalize your clean distance and that's basically what this does. And what this does is that it for positive cases it tries to make vectors as aligned with each other as possible. And for negative pairs it tries to make the cosine smaller than the particular margin. The margin in that case should probably be something that kind of is close to zero. So you want the cosine in a high dimensional space. There's a lot of space near the equator of the sphere, of the high dimensional sphere. So all your points now are normalized on the sphere. What you want is samples that are symmetrically similar to you should be close to you. The samples that are dissimilar should be orthogonal. You don't want them to be opposed because there is only one point in the south pole. Whereas on the equator is a very very high large space, the entire sphere minus one dimension basically. Okay. So you can make the margin just, you know, some small positive value and then you get the entire equator essentially of the sphere which contains almost the entire volume of the sphere in high dimension. CTC loss. This is a little more complicated because that's a loss that is basically uses structure prediction, what's called structure prediction. So this is, I sort of briefly talked about it very quickly a few weeks ago on something very similar to this. So this is a loss that is applicable when your output is a sequence of vectors of scores where the vectors correspond to scores of categories. Okay. And so you have, so your system computes a vector of such scores. So imagine for example, a speech recognition system. Speech recognition system every 10 milliseconds gives you a vector of probabilities for what the sound being pronounced right now is. And the number of categories usually is quite large on the order of a few thousand. Okay. So it gives you basically a softmax vector of a size, you know, typically 3000 let's say. One of those every 10 milliseconds. All right. And what you'd like, you know, you have a desired output and the desired output is what word was being pronounced and the word that's being pronounced that corresponds to kind of a particular sequence of sounds if you want that you might know. So what you need now is a cost that basically is low if that sequence looks like that sequence. But what you might allow is for the input sequence to repeat some of the sounds if you want. Right. So for example, you know, my cost to the target might be the word seven, let's say, and it's pronounced really quickly seven. So you basically have, you know, a very small number of samples of each sound in the sequence. But then perhaps the person who is pronouncing the word now that you use as a sample pronounced it very slowly, like seven. Right. So now the first, the first takes, you know, several, several frames of 10 milliseconds that should all be mapped to the same instance of the in the, in the output. And I do that picture before, but I'm going to do it again. Right. So the, you have, let's see, you have a sequence of scores coming out of soft maxes, let's say, it's actually better if there are energies, but for CTC they need to be, and then you have the target sequence. And I think of this as some sort of matrix and each entry in that matrix basically measures the distance between the two vectors that are here. Okay. So when I'm treating the matrix indicates how this vector looks like that vector, for example, the cross entropy or something like that. Okay. Or square error. It doesn't matter what the loss function is. So now if this is the word seven pronounced slowly, okay. And this has perhaps only one instance of each sound. You want all of those, you know, you would want all of those vectors corresponding to the E to be mapped to that E vector here. Okay. So you want to compute that so you want to compute that cost of, you know, confusing that the, those, all of those, I mean, matching those E's to that E. Now, of course, here the system produced the correct answer. So you don't have much of a problem. But if the target is seven, but the word that was pronounced here, or the output that was produced by the system does not correspond to seven, that's when you run into trouble. So here what you do is you find the best mapping from the input sequence to the output sequence. Okay. So the S gets mapped to the S, the E to the E, the V to the V, the E's to the E and the N to the N. So you get this kind of path if you want that think of this as a path in a graph. And the way you determine this is basically by using a dynamic programming algorithm, a short-cut path algorithm that figures out how do I get from here to here, in a path that minimizes the sum of the distance distances between the, all the vectors, all the distances between the vectors of, you know, all the points I'm going through. Okay. So there's a optimization with respect to a latent variable if you want. Okay. And CTC basically does that for you. Right. So you give it two sequences and it computes the distance between them and, you know, kind of the best kind of mapping between the two by allowing basically to map multiple input vectors to kind of a single one on the output. It cannot expand. It can only kind of reduce if you want. And then that's done in a way that you can back propagate gradient to it. We'll come back to this, to more things like this at the end if we can. So this is what this, the target is assumed to be many to one. The alignment of the input to the target is assumed to be many to one, which means the length of the target sequence such that it must be smaller than the length of the input. That's for the reason I just explained. Okay. So it's basically differentiable time warping. You could think of it this way. Or sort of a module that does dynamic time warping or dynamic programming and is still differentiable. The idea for this goes back in the early 90s in the Leon Boutrous PhD thesis actually. That's very old. Is there a good paper or resource to learn more about that dynamic programming algorithm there? Yeah, actually that's kind of what I'm going to talk about next. I may not have time to go through it, but I'll try to. But basically the last part of the energy-based model tutorial. Energy-based model tutorial, the 2006 paper that we gave you a reference to. A tutorial on energy-based models. The second part is all about this kind of stuff essentially. Okay, so it's more energy-based models, but now in kind of more of the supervised context if you want. So before I get to this, I want to come back to the more general formulation of energy-based models. And the idea that if you want to kind of define energy-based models in the proper way, these are the conditional versions. You have a training set, a bunch of pairs, a training set, a bunch of pairs, x, y, y, i for i equals 1 to p. You have a loss functional. So the loss functional L of E and S. So it takes the energy function computed by the system and the training set and it gives you a scalar value. Now you can think of this as a functional. A functional is a function of a function. But in fact, because the energy function itself is parameterized by parameter w, you can turn this loss functional into a loss function, which is not just a function of w, not a function of the energy function. And of course, the set of energy functions is called epsilon here. It's parameterized by the parameter w, which is taken within the set. So training consists in, of course, minimizing the loss functional with respect to w and finding the w that minimizes it. And so one question you might ask yourself, you know, I went through a whole bunch of objective function loss functions here. And the question is, if you are in an energy based framework, what loss functions are good ones and what loss functions are bad ones? How do you characterize a loss function that actually will do something useful for you? Okay. So here is a general formulation of a loss function. It's an average over training samples. So here I'm kind of assuming that it's invariant under permutation of the samples. So an average is as good as any other aggregation aggregating function. So it's the average over training samples of a per sample loss function, capital L. And it takes the desired answer, y, which could be just a category or it could be a whole image or whatever. And it takes the energy function where x, the x variable xi is equal to xi, the ice training sample. But the y variable is undetermined. So E of w, y and xi is basically the entire shape of the energy function for all values of y, over values of y for a given x, x equal to xi. And you can have a regularizer if you want. So here this is a loss functional again. And again, of course, we have to design this loss function also that it makes the energy of correct answer small and the energy of incorrect answers large in some ways. Now we're going to go through a bunch of different types of loss functions. So one thing we could do is say my loss function is just going to be the energy of the correct answer. So I'm going to place myself in the context of an energy-based model. My system produces scores. I interpret those scores as energies. So I'm going to take my low as good as opposed to positive scores. And what I'm just going to do is define my energy functional as a function of the energy function and a function of y. That's simply the energy that my model gives to the correct answer. So basically, I'm going to define my energy function as a function of the energy function and a function of y. So basically, I give it an x and I give it the correct answer y and I ask the system, what energy do you give to that pair? And then I try to make that energy as small as possible. So you have this landscape of energies here. Now I showed you this slide in the context of unsupervised self-supervised learning. Here I'm showing to you in the context of supervised learning. So imagine that one of the variables is x and the other variable is y. And the blue beads are training samples and you want to make the energy of the blue beads as small as possible. So you're pulling down on the blue beads, but you're not doing anything else. And so as a result, depending on the architecture of your network, if your network is not designed properly, or if it's designed in no particular way, it could very well be that the energy function is going to become flat everywhere. You're just trying to make the energy of the correct answer small and you're not telling the system the energy of everything else should be higher. And so the system might just collapse. So energy loss is not good in that sense, but there are certain situations where it's applicable because if the shape of the energy function is such that it can only make the energy of a single answer small, all the other ones being larger, they don't need to have a contrastive term. And we've seen this in the context of self-supervised learning. People are completely lost about the loss function. Right. Okay. So there's a function L and it's a function of another function E. So it's called a functional because it's a function of a function. It's not a function of a point, it's a function of a function. Now, if that second function is parameterized by parameter W, then you can say that the loss function is actually a function of that parameter W and now it becomes a regular function. Okay. That's what I had in the... Can you write it down? It's basically written here. Okay. You can either write the functional as L of E and S. So that's a functional because it's a function of E, which itself is a function. Okay. But E itself is a function of W. And so if I write the loss function directly as a function of W, now it's just a regular function. Okay. Yeah. I mean, I asked the question that was asked in the chat. Yeah. I know. I know. Okay. We've seen the negative log. Like a good loss before. I talked about this. So this is a loss function. So it's a function of W. And it's a function of W. And it's a function of W. This is basically the Doblin yes in addition to the please give the question over here. So this is a function of W, right? Of W. And then there you have a wider Dirac- \u043f\u043e\u043c value. There's And this one is trying to make the energy of all y's for this given x as large as possible. Because the best way to make this term small is to make those energies large because they enter in there as a negative exponential. So this has this kind of pushing down on the correct answer, pushing up on incorrect answer behavior. And we've seen before, we just talked about margin loss and other types of losses. Here is something that's called a perceptron loss because it's basically very similar to, I mean, it's exactly the same as the loss that was used for the perceptron 60 years ago, over 60 years ago. So this one says, I want to make the energy of the correct answer small and at the same time, I want to make the energy of the smallest energy for all answers as large as possible. So pick the y that has the smallest energy in your system, make that as large as you can. At the same time, picks the correct energy, make that as small as you can. Now there is a point at which the answer with the correct energy is gonna be equal to the correct answer. And so that difference can never be negative, okay? Because the first term is necessarily one term in that minimum. And so the difference is at best zero and for every other cases is positive. It's only zero when the system gives you the correct answer. Okay? But this objective function does not prevent the system from giving the very same energy to every answer. Okay? So in that sense, it's a bad energy, it's a bad loss function. It's a bad loss function because it says, I want the energy of the correct answer to be small. I want the energy of all the other answers to be large, but I don't insist that there is any difference between them. So the system can choose to make every answer the same energy. And that's a collapse. Okay? So perceptron loss is not good. It's actually only good for linear systems, but it's not good for, as an objective function for nonlinear systems. So here's a way to design an objective function that will always be good. And you take the energy of the correct answer and you take the energy of the most of finding incorrect answer, which means the value of Y that is incorrect, but at the same time, that is the lowest energy of all the incorrect answers. Okay? And your system will work if that difference is negative. In other words, if the energy of the correct answer is smaller than the energy of the most of finding incorrect answer, but at least some quantity, some margin. Okay? So as long as your objective function, when you design it, ensures that the energy of the correct answer is smaller than the energy of the most of finding incorrect answer by at least some margin, non-zero margin, then you're fine. Your loss function is good. Okay? So things like hinge loss are good. The hinge loss basically says, and we talked about this just before, I want the energy of the correct answer to be smaller than the energy of the most of finding incorrect answer, which is denoted Y I bar here, by at least M. Okay? This is what this loss function does. It's a hinge loss. And it wants to push down the energy of this guy below the energy of that guy by at least this margin. So this has a margin M, and this will, if you train a system with this loss, it will, and it can learn the task, it will learn the task and probably produce the good answers. The hinge loss, the soft hinge loss, which is in the context of energy of these models, is expressed this way. Basically, instead of feeding the difference between the energies of the correct answer and the most of finding incorrect one, into the energy of the correct answer, and the most of finding incorrect one into a hinge, it feeds it to a soft hinge, which we talked about just a few minutes ago. And there, this one also has a margin. The margin is different. The question would be how to pick M? Say again? The question would be how to pick M? It's arbitrary. You can set M to one. You can set M to one-tenth. I mean, it's kind of arbitrary because it will just determine the size of the weights of your last layer. That's all it does. Okay. So it's basically up to you. Yeah, so the soft hinge loss has an infinite margin. It wants the difference between those two energies to be infinite, but the stroke sort of decreases exponentially, so it's never gonna get there because the gradients get very small as the difference increases. Here's another example of a margin loss, the square loss, the square square loss. Okay, so this is a loss that tries to make the energy of the correct answer squared as small as possible, and then it has a square hinge to push away, to push up the energy of the most of finding incorrect answer. Okay, and again, that works. And this is very similar to the kind of loss that people use in Siamese nets and stuff like that that you've heard about. There's a whole menagerie of such losses which I'm not gonna go through. There's actually a whole table here, which is also in this paper, the tutorial on energy-based models. And what's indicated on the right side is whether they have a margin or not. So the energy loss does not have a margin. It doesn't push up anything, so no margin. It doesn't work always. You have to design the machine so that this loss may work for that system. The perceptron loss does not work in general. It only works if you have a linear parameterization of your energy as a function of the parameters, but that's a special case, and that's the case for the perceptron. And then some of them have a finite margin, like the hinge loss, and some of them have an infinite margin, like the log, the soft hinge, if you want. A whole bunch of those losses, some of those were used, were invented in the context of discriminative learning for speech recognition systems. But they were invented before people in machine learning actually got interested in this. The question would be how you find the y bar. So if you have a discrete code, we can find simply the minimum value, but otherwise, are we running gradient descent? Right, so if y is continuous, then there is no clear definition for what is the most offending incorrect answer. You would have to define some sort of distance around the correct answer, above which you consider an answer to be incorrect. Okay, so for example, you are in a continuous energy landscape. There's one training sample here. You wanna make the energy of that training sample small, easy enough, compute the energy through your neural net, push it down, back propagate, update the weight so that the energy goes down, easy enough. Now the incorrect answer, if you take an answer that's just kind of etched on that side of that, and you push up, your energy surface might be a little stiff because it's computed by a parameterized neural net, so that may not be possible. So you probably want to have a incorrect answer that's quite a bit outside that you're gonna push up. And so that's how you define, the whole question is how you define a contrastive sample that you're gonna push up. And a lot of those questions are just and a lot of those objective functions here, those last functions use a single, Y bar, negative sample, but there is no simple, single correct way of picking this Y bar. You can imagine, particularly in the kind of, in the sort of continuous case or in the case where Y is either very, very large or continuous and high dimensional, there's no simple way to pick Y bar. A lot of discussions we've had about contrastive methods that Ishan talked about for Siamese nets and that we talked about before, where basically how do you pick a Y bar in the self-supervised case? So self-supervised you don't have an X, right? And there's many ways you can pick it up. It's only obvious how to pick it up in kind of small cases. I just wanna point out the formula here at the bottom. So this is a kind of, you can think of this as sort of a general form of sort of hinge type contrastive losses where you have an H function here, you think of it as a hinge for some type. And instead of that hinge, you have the energy of the correct answer. So that's the energy of the W, Y, I, X, I. So this is your training sample. That's the energy your system gives to the training sample. The second term is the energy of some other answer Y, for the same X training sample. And then there is a margin, that margin C is actually a function of Y, I and Y. And you might imagine the margin is actually also a function of X and X, I. So basically you determine a margin as a function of the distance between the Ys. And you feed that to, let's say a hinge. Now the thing is this loss function is summed over all Ys. Here is a discrete sum because Y is discrete, but you can imagine an integral. So this kind of loss says, I have an energy for my correct answer. I have energies for every other answer in my space. And I wanna push up the energy of all other answers, but the amount by which I wanna make them higher, the margin depends on the distance between Y and Y bar. Or in this case between Y, I, which is this, and Y, which is the other Ys. Okay, so you can imagine that this margin will, will become smaller and smaller as the two Ys is gonna get closer to each other. In this case, you don't push up too much for things that are too close. And you push up in proportion to the distance of the Y, whatever distance you think is appropriate. This is of course a more difficult loss function to optimize. And at a time, so I might talk about the structural prediction issue that I said I was gonna talk about at a later time. Any more question? The contrast of methods used for the as the self supervised learning papers, that are usually take random, take random images as a negative examples. Do you have any idea if they use these functions and once tried experimented with these? They use what kind of function? These loss functions that you explained to us now. So most of them use the basically the negative likelihood loss here, which in this panel is called NLL MMI. Okay, so NCE that you heard about from Yishan, that's what they use, right? They're trying to make the distance between the samples as much as possible. And then the contrastive term is, it's basically a log softmax of the distances. So when you compute the log softmax, you think of distance as an energy and then you compute the log softmax of those energies. You get this formula here in the second last line called NLL MMI. Okay, so the integral is approximated by taking random images. A random what? They think random images is negative example. So that is used to approximate the integral. Well, so basically you can't compute this integral of all Y or this sum if Y is discrete. And so you basically approximate the sum by a few terms that you pick randomly, right? Let's go Monte Carlo. I mean, basically, if you wanna do this properly, you have to pick those samples according to the rule of Monte Carlo sampling, but it doesn't matter. I mean, that's why hard negative mining is hard, okay? That's why what makes the difference between MoCo, Perl, SimClear, et cetera, is how you pick those negative samples. That's why I said there is no kind of, in cases where Y space is high dimensional, there's no predefined way of picking negative samples essentially. It's only in classification that it's easy. Okay. But have other people experimented with the other losses? Yeah, I mean, a lot of people are using the square square or the sort of hinge, you know, with the difference of energies. So some of the systems that we use by, at least at some point, the system that DeepFace, which is the face recognition system that used by Facebook to tag people, it used a convolutional net trained in supervised mode with a certain number of categories, basically images from, I don't know, a million people or something. But then there was a fine tuning phase that used metric learning, basically Siamese nets, where you show two photos of the same person, and you say those are the same person, and then two photos of different people, and you push them apart. And that used, they tried different objective functions, but I think they were using the square square loss at some point, maybe the square exponential. And I'm not entirely sure what they're using now, but you know, it's one of those. Professor, what topics will you cover in the next lecture? Okay, so we're gonna have two guest lectures. So next week is Michael Lewis. Michael Lewis is a research scientist at Facebook Air Research in Seattle. And he is a specialist of natural language processing and translation. So he's gonna tell you all the interesting tidbits about sequence to sequence, about transformers, about NLP, and about translation. Okay. And he knows much better the details about this than I do, so he's the right person to talk about this. We're gonna have another guest lecture. It's gonna be Xavier Bresson. He's one of the world specialists of graph neural nets. And so this is the whole idea of, how do you apply neural nets? You can think of an image as a function on a regular grid. Every pixel is a location on a regular grid. You can think of an image as a function on that grid. So grid is a graph of a particular type, and an image is just a function on the graph. You can think of, I don't know, video as a regular 3D grid where you have space and time, and most natural signals, you can think of them as functions on regular graphs. Okay. What about the case where the function you're interested in is not on a Euclidean graph, if you want. So let's imagine, for example, you take a photo with a panoramic camera, a 360 camera, right? So it's a camera that basically takes a spherical image. Okay. So now your pixels live on the sphere. How do you compute a convolution on a sphere? Okay. So you want to run your convolutional net on this image that now lives on the sphere. You can't use the standard ways of computing convolutions. So you have to figure out how to compute convolutions on the sphere. Right. So that's an example. Now, here's something a little more complicated. Imagine now that you have a 3D scanner and you're capturing, I don't know, a dancer, someone kind of in front of a 3D scanner, and that person has a particular pose, let's say like this, okay? And then you take another 3D picture, 3D data from another person, and that other person is in another pose. That person has a different body shape. She's in a different body pose. And now what you want is you want to be able to map one onto the other. You want to be able to say like, what is the hand in the first person? Where is the hand in the second person? So what you have to do now is basically have a neural net that takes into account a 3D mesh that represents the geometry of a hand and train it to tell you it's a hand. So that when you apply it to the hand, it tells you it's a hand. When you apply it to the other parts of the body, it tells you it's something else. But the data you have is not an image. It's a 3D mesh, okay? The mesh may have different resolutions. The triangles may occur at different places. So how you define your convolutions on a domain like this that is independent of the resolution of the mesh and only kind of depends on the shape so that you can classify your hand regardless of the orientation, the size, the conformation and the body shape of the person, things like that, right? So here's another example that's perhaps more interesting. You want to train something like a Samy's net, but you want to train the Samy's net to tell you whether one molecule is gonna stick to another molecule, right? So you give two molecules to your neural net and your neural net produces two vectors. If those two molecules stick together, it gives you two vectors whose distance is small, okay? And if they don't stick together, then the distance is large. Okay, so you can think of the distance as kind of the negative free energy of the binding, the binding energy of the two molecules, right? Or the free energy minus the constant, which you want. So you would train this as a Samy's net, but then the problem is how you represent a molecule to a network knowing that it's the same network you're going to apply to this molecule and that molecule and the two molecules don't have the same shape. They don't have the same length. They don't have the same number of atoms. They don't have the same, like how do you represent a molecule? The best way to represent a molecule is as a graph. It's basically a graph whose structure changes with the molecule. And this graph is annotated by the identity of the atoms at each site, maybe by their location in 3D space or their relative location, maybe by the angle of the bonds between two successive atoms or the binding energy of that particular bond and things like this. So the best way to represent a molecule is by representing as a graph, basically. And here's another example, perhaps more relevant to something like Facebook. Let's say I want to kind of infer, or let's say Amazon or something. I want to infer what type of, let's say I'm Amazon, right? And I have a customer and that customer has bought a whole bunch of different things. And that customer has commented a whole bunch of different things. I could think of kind of encoding this as a vector, but it would be a vector of variable size because people buy different numbers of things and stuff like that. So I would need to sort of find a way to aggregate the data so that everybody can be represented by the same fixed size vector. But what if instead I represent the person and all the things that that person has bought and all the reviews that person has made, et cetera, as a graph, essentially. And then I represent, what I feed to the neural net is the graph with values on the nodes and perhaps the arcs. If I have a way of representing a graph so that I can connect a neural net independently of the shape of the graph, then I can do this kind of application. And so this is what graph neural nets are about. It's a very, very hot topic at the moment. It's extremely promising for a lot of applications, particularly in biomedicine, in chemistry, in material science, but also in social science for social network analysis and all kinds of applications, computer graphics, all kinds of stuff. So it's really cool. Xavier is really one of the experts on this topic. So I'm really happy that he accepted to give us a talk. It's not gonna be easy for him because he's in Singapore. Yeah, it's gonna be fine in the morning for him. Yeah, that's right. I'm giving a lecture in a couple of days in Hong Kong. So it's the same thing for me. I see. So actually he's from Nanyang Technological University. Oh, NUS. That's right. NTU, right? NTU, NTU, yeah. Yeah. I was confused, correct. All right, so that was it. Oh, sorry, there was one more question. Yeah, that was really interesting, Professor. I had one more question. I was reading this term called normalizing flows. And I don't understand what they are. Could you just give some intuition into why people are excited about it? Right, so normalizing flows. So it's not a technique I have a lot of experience with, but I've read the papers. So it was proposed by Danino Rosendi and Shakir Mohamed at DeepMind a while ago and a while back, maybe five years ago or so. And it's basically a sort of a density estimation method. So it's a little bit like GANs. It has a bit of the same spirit as GANs. And it gets inspiration from ICA, independent component analysis, although it's not kind of explicit in the original paper. But here's the basic idea. The basic idea is you want to train a neural net to transform a known distribution from which you can sample into a distribution that happens to be the distribution of your data. So let's imagine that you have a little variable z that you sample from a Gaussian distribution and you run it through a function or uniform distribution over a domain. You run it through a function implemented by a neural net and you want to train this neural net so that the distribution you get at the output is the one you want that corresponds to your data. Okay. And so let me give you a very simple example. So let's say I have a variable z and I have observed variable y and I sample my variable z with a uniform distribution between say zero and one. And what I want on the output is, I don't know, say a Gaussian. It's kind of stupid to want a Gaussian, but let's say I want a Gaussian because I could sample from a Gaussian easily. So what I need to do is kind of transform this uniform distribution into a Gaussian by a mapping. And the mapping is gonna be a function like, it's gonna be a function, okay, zero is here. Kind of like this, if you want. Okay. And this is the inverse of the integral of the Gaussian distribution. Okay. So if I take the derivative of this function, okay, so now let me kind of draw this. It's a little difficult to see, but if I map, okay, the derivative of this function here will indicate how much I stretch a little piece here into a piece here, right? So the larger the derivative, the more I stretch. If the slope here is one, then this piece of the distribution here is not gonna be stretched, it's gonna be kind of passed unchanged. Okay. And the larger the slope, the more I stretch the distribution, I stretch a little piece here, and therefore I kind of distribute all the samples that fall into this little location here, I stretch them over a large region, right? And so what I need to do is design this function in such a way that it stretches my input distribution so that that distribution get transformed into the output distribution I want. All right. So there is a formula that says, so in multi-dimension, it's a little more complicated than this, but it says that the distribution you're gonna get on y is gonna be equal to the distribution that you started with in z, multiplied by the inverse of the determinant of the Jacobian of this f function, so this is f. Minus one. So it's actually the original formula is this one. But those two things are equal. Okay. So if you take, so this is for a multi-dimensional vector function, right? So it has a Jacobian to map z to y. And so if you take the determinant of the inverse Jacobian of that function, which is a scalar value, indicates by how much the distribution gets stretched or compressed in that case at q. So in that case here is the compression ratio. It's the inverse of the derivative, so it's the compression, right? And so the more you compress here, the more the probability will be high, more p of y will be large. The density p of y for this y will be large for a given y. For a given q. So this is for y equals f of z. Okay. So the big question of normalizing flow methods is how you do this, right? Given a number of samples of p of y, and given that you sample your distribution q, you have your distribution q, you sample from it, how do you kind of minimize an objective function that knowing that the p you get at the output is equal to the q you put at the input multiplied by this sort of inverse determinant of the Jacobian of the f function. What you have to find is the f function. So you basically have to differentiate. So basically you compute the distance between those divergence, KL divergence for example, between p of y and the thing on the right side of the equal sign. And you have to differentiate this with respect to the parameters of f. So you have to basically propagate through the inverse gradient of the Jacobian of f, right? It's not easy. Very often what people do is that they write f as a succession of very simple f's that only modify the distribution just a little bit. So f very often is something like the identity plus some deviation, a bit like ResNet if you want. And then you stack lots and lots of layers of that. And the problem becomes simpler because when those functions do a little bit of modification then a lot of those kind of issues kind of become simple. The determinant here kind of simplifies. Okay, that's a very sort of abstract high level description of normalizing flow. There's interesting papers about this in recent years on even recent months on using this for like particle physics and stuff like that. Carl Kranmer at NYU is actually kind of a specialist of that. Thank you so much, professor. All right, any other question? Okay, that was it. Great, thank you very much everyone. Yeah, see you tomorrow guys. All right, bye bye. Take care.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.44, "text": " All right. So we're going to talk about two or three topics today. And the first one is going to be", "tokens": [50364, 1057, 558, 13, 407, 321, 434, 516, 281, 751, 466, 732, 420, 1045, 8378, 965, 13, 400, 264, 700, 472, 307, 516, 281, 312, 50636, 50684, 733, 295, 257, 3131, 295, 512, 295, 264, 6828, 300, 2514, 294, 9953, 51, 284, 339, 293, 733, 295, 562, 293, 577, 281, 764, 50968, 50968, 552, 13, 407, 264, 700, 992, 295, 8378, 307, 466, 24433, 6828, 13, 400, 456, 311, 257, 1379, 51440, 51440, 3840, 295, 552, 7642, 294, 9953, 51, 284, 339, 13, 400, 436, 1936, 808, 490, 3683, 10577, 300, 561, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.11443558743125513, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.0006359665421769023}, {"id": 1, "seek": 0, "start": 6.4, "end": 12.08, "text": " kind of a review of some of the functions that exist in PyTorch and kind of when and how to use", "tokens": [50364, 1057, 558, 13, 407, 321, 434, 516, 281, 751, 466, 732, 420, 1045, 8378, 965, 13, 400, 264, 700, 472, 307, 516, 281, 312, 50636, 50684, 733, 295, 257, 3131, 295, 512, 295, 264, 6828, 300, 2514, 294, 9953, 51, 284, 339, 293, 733, 295, 562, 293, 577, 281, 764, 50968, 50968, 552, 13, 407, 264, 700, 992, 295, 8378, 307, 466, 24433, 6828, 13, 400, 456, 311, 257, 1379, 51440, 51440, 3840, 295, 552, 7642, 294, 9953, 51, 284, 339, 13, 400, 436, 1936, 808, 490, 3683, 10577, 300, 561, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.11443558743125513, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.0006359665421769023}, {"id": 2, "seek": 0, "start": 12.08, "end": 21.52, "text": " them. So the first set of topics is about activation functions. And there's a whole", "tokens": [50364, 1057, 558, 13, 407, 321, 434, 516, 281, 751, 466, 732, 420, 1045, 8378, 965, 13, 400, 264, 700, 472, 307, 516, 281, 312, 50636, 50684, 733, 295, 257, 3131, 295, 512, 295, 264, 6828, 300, 2514, 294, 9953, 51, 284, 339, 293, 733, 295, 562, 293, 577, 281, 764, 50968, 50968, 552, 13, 407, 264, 700, 992, 295, 8378, 307, 466, 24433, 6828, 13, 400, 456, 311, 257, 1379, 51440, 51440, 3840, 295, 552, 7642, 294, 9953, 51, 284, 339, 13, 400, 436, 1936, 808, 490, 3683, 10577, 300, 561, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.11443558743125513, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.0006359665421769023}, {"id": 3, "seek": 0, "start": 21.52, "end": 27.2, "text": " bunch of them defined in PyTorch. And they basically come from various papers that people", "tokens": [50364, 1057, 558, 13, 407, 321, 434, 516, 281, 751, 466, 732, 420, 1045, 8378, 965, 13, 400, 264, 700, 472, 307, 516, 281, 312, 50636, 50684, 733, 295, 257, 3131, 295, 512, 295, 264, 6828, 300, 2514, 294, 9953, 51, 284, 339, 293, 733, 295, 562, 293, 577, 281, 764, 50968, 50968, 552, 13, 407, 264, 700, 992, 295, 8378, 307, 466, 24433, 6828, 13, 400, 456, 311, 257, 1379, 51440, 51440, 3840, 295, 552, 7642, 294, 9953, 51, 284, 339, 13, 400, 436, 1936, 808, 490, 3683, 10577, 300, 561, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.11443558743125513, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.0006359665421769023}, {"id": 4, "seek": 2720, "start": 27.2, "end": 32.0, "text": " have written where they claim that this or that particular objective function or activation function", "tokens": [50364, 362, 3720, 689, 436, 3932, 300, 341, 420, 300, 1729, 10024, 2445, 420, 24433, 2445, 50604, 50604, 1985, 1101, 337, 641, 1154, 13, 407, 295, 1164, 2201, 3255, 264, 1300, 43, 52, 13, 663, 311, 257, 21022, 292, 472, 13, 50936, 50972, 583, 456, 311, 3195, 295, 17840, 295, 1300, 43, 29211, 13, 1981, 1300, 43, 29211, 689, 264, 2767, 644, 307, 406, 5754, 293, 51332, 51332, 992, 281, 4018, 11, 457, 309, 393, 312, 4350, 281, 1319, 2139, 787, 365, 257, 3353, 13525, 420, 7579, 281, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.11515768281706087, "compression_ratio": 1.6050420168067228, "no_speech_prob": 7.9612887930125e-05}, {"id": 5, "seek": 2720, "start": 32.0, "end": 38.64, "text": " works better for their problem. So of course everybody knows the ReLU. That's a restarted one.", "tokens": [50364, 362, 3720, 689, 436, 3932, 300, 341, 420, 300, 1729, 10024, 2445, 420, 24433, 2445, 50604, 50604, 1985, 1101, 337, 641, 1154, 13, 407, 295, 1164, 2201, 3255, 264, 1300, 43, 52, 13, 663, 311, 257, 21022, 292, 472, 13, 50936, 50972, 583, 456, 311, 3195, 295, 17840, 295, 1300, 43, 29211, 13, 1981, 1300, 43, 29211, 689, 264, 2767, 644, 307, 406, 5754, 293, 51332, 51332, 992, 281, 4018, 11, 457, 309, 393, 312, 4350, 281, 1319, 2139, 787, 365, 257, 3353, 13525, 420, 7579, 281, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.11515768281706087, "compression_ratio": 1.6050420168067228, "no_speech_prob": 7.9612887930125e-05}, {"id": 6, "seek": 2720, "start": 39.36, "end": 46.56, "text": " But there's lots of variations of ReLUs. These ReLUs where the bottom part is not constant and", "tokens": [50364, 362, 3720, 689, 436, 3932, 300, 341, 420, 300, 1729, 10024, 2445, 420, 24433, 2445, 50604, 50604, 1985, 1101, 337, 641, 1154, 13, 407, 295, 1164, 2201, 3255, 264, 1300, 43, 52, 13, 663, 311, 257, 21022, 292, 472, 13, 50936, 50972, 583, 456, 311, 3195, 295, 17840, 295, 1300, 43, 29211, 13, 1981, 1300, 43, 29211, 689, 264, 2767, 644, 307, 406, 5754, 293, 51332, 51332, 992, 281, 4018, 11, 457, 309, 393, 312, 4350, 281, 1319, 2139, 787, 365, 257, 3353, 13525, 420, 7579, 281, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.11515768281706087, "compression_ratio": 1.6050420168067228, "no_speech_prob": 7.9612887930125e-05}, {"id": 7, "seek": 2720, "start": 46.56, "end": 51.68, "text": " set to zero, but it can be allowed to change either only with a positive slope or forced to", "tokens": [50364, 362, 3720, 689, 436, 3932, 300, 341, 420, 300, 1729, 10024, 2445, 420, 24433, 2445, 50604, 50604, 1985, 1101, 337, 641, 1154, 13, 407, 295, 1164, 2201, 3255, 264, 1300, 43, 52, 13, 663, 311, 257, 21022, 292, 472, 13, 50936, 50972, 583, 456, 311, 3195, 295, 17840, 295, 1300, 43, 29211, 13, 1981, 1300, 43, 29211, 689, 264, 2767, 644, 307, 406, 5754, 293, 51332, 51332, 992, 281, 4018, 11, 457, 309, 393, 312, 4350, 281, 1319, 2139, 787, 365, 257, 3353, 13525, 420, 7579, 281, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.11515768281706087, "compression_ratio": 1.6050420168067228, "no_speech_prob": 7.9612887930125e-05}, {"id": 8, "seek": 5168, "start": 51.68, "end": 58.72, "text": " have a negative slope or sometimes being random in the case of the randomized leaky ReLU. So they", "tokens": [50364, 362, 257, 3671, 13525, 420, 2171, 885, 4974, 294, 264, 1389, 295, 264, 38513, 476, 15681, 1300, 43, 52, 13, 407, 436, 50716, 50716, 362, 1481, 5288, 411, 476, 15681, 1300, 43, 52, 11, 6001, 43, 52, 11, 319, 12, 8524, 43, 52, 11, 4974, 1300, 43, 52, 11, 1030, 11458, 13, 407, 476, 15681, 1300, 43, 52, 307, 472, 689, 51180, 51180, 291, 2089, 264, 2767, 644, 281, 362, 257, 4036, 3671, 13525, 13, 400, 300, 733, 295, 22367, 264, 51576, 51576, 2734, 300, 2171, 16795, 493, 300, 562, 257, 1300, 43, 52, 307, 766, 11, 309, 1177, 380, 483, 604, 16235, 13, 407, 510, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.0807906757701527, "compression_ratio": 1.7136363636363636, "no_speech_prob": 2.014270648942329e-05}, {"id": 9, "seek": 5168, "start": 58.72, "end": 68.0, "text": " have nice names like leaky ReLU, PreLU, re-ReLU, random ReLU, et cetera. So leaky ReLU is one where", "tokens": [50364, 362, 257, 3671, 13525, 420, 2171, 885, 4974, 294, 264, 1389, 295, 264, 38513, 476, 15681, 1300, 43, 52, 13, 407, 436, 50716, 50716, 362, 1481, 5288, 411, 476, 15681, 1300, 43, 52, 11, 6001, 43, 52, 11, 319, 12, 8524, 43, 52, 11, 4974, 1300, 43, 52, 11, 1030, 11458, 13, 407, 476, 15681, 1300, 43, 52, 307, 472, 689, 51180, 51180, 291, 2089, 264, 2767, 644, 281, 362, 257, 4036, 3671, 13525, 13, 400, 300, 733, 295, 22367, 264, 51576, 51576, 2734, 300, 2171, 16795, 493, 300, 562, 257, 1300, 43, 52, 307, 766, 11, 309, 1177, 380, 483, 604, 16235, 13, 407, 510, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.0807906757701527, "compression_ratio": 1.7136363636363636, "no_speech_prob": 2.014270648942329e-05}, {"id": 10, "seek": 5168, "start": 68.0, "end": 75.92, "text": " you allow the bottom part to have a slight negative slope. And that kind of prevents the", "tokens": [50364, 362, 257, 3671, 13525, 420, 2171, 885, 4974, 294, 264, 1389, 295, 264, 38513, 476, 15681, 1300, 43, 52, 13, 407, 436, 50716, 50716, 362, 1481, 5288, 411, 476, 15681, 1300, 43, 52, 11, 6001, 43, 52, 11, 319, 12, 8524, 43, 52, 11, 4974, 1300, 43, 52, 11, 1030, 11458, 13, 407, 476, 15681, 1300, 43, 52, 307, 472, 689, 51180, 51180, 291, 2089, 264, 2767, 644, 281, 362, 257, 4036, 3671, 13525, 13, 400, 300, 733, 295, 22367, 264, 51576, 51576, 2734, 300, 2171, 16795, 493, 300, 562, 257, 1300, 43, 52, 307, 766, 11, 309, 1177, 380, 483, 604, 16235, 13, 407, 510, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.0807906757701527, "compression_ratio": 1.7136363636363636, "no_speech_prob": 2.014270648942329e-05}, {"id": 11, "seek": 5168, "start": 75.92, "end": 81.2, "text": " issue that sometimes pops up that when a ReLU is off, it doesn't get any gradient. So here", "tokens": [50364, 362, 257, 3671, 13525, 420, 2171, 885, 4974, 294, 264, 1389, 295, 264, 38513, 476, 15681, 1300, 43, 52, 13, 407, 436, 50716, 50716, 362, 1481, 5288, 411, 476, 15681, 1300, 43, 52, 11, 6001, 43, 52, 11, 319, 12, 8524, 43, 52, 11, 4974, 1300, 43, 52, 11, 1030, 11458, 13, 407, 476, 15681, 1300, 43, 52, 307, 472, 689, 51180, 51180, 291, 2089, 264, 2767, 644, 281, 362, 257, 4036, 3671, 13525, 13, 400, 300, 733, 295, 22367, 264, 51576, 51576, 2734, 300, 2171, 16795, 493, 300, 562, 257, 1300, 43, 52, 307, 766, 11, 309, 1177, 380, 483, 604, 16235, 13, 407, 510, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.0807906757701527, "compression_ratio": 1.7136363636363636, "no_speech_prob": 2.014270648942329e-05}, {"id": 12, "seek": 8120, "start": 81.2, "end": 88.8, "text": " you get a chance for that system that function to actually propagate gradient and perhaps do", "tokens": [50364, 291, 483, 257, 2931, 337, 300, 1185, 300, 2445, 281, 767, 48256, 16235, 293, 4317, 360, 50744, 50744, 746, 4420, 13, 467, 393, 352, 439, 264, 636, 281, 733, 295, 3566, 1577, 11048, 3774, 295, 264, 6358, 11, 50936, 50980, 733, 295, 411, 364, 8236, 2158, 498, 291, 528, 13, 6001, 43, 52, 307, 472, 689, 291, 485, 865, 11, 352, 2286, 13, 51220, 51296, 440, 3894, 24433, 390, 2673, 1228, 264, 20828, 1639, 294, 257, 460, 1770, 1270, 300, 321, 1009, 362, 51572, 51608], "temperature": 0.0, "avg_logprob": -0.1517786047924524, "compression_ratio": 1.5061728395061729, "no_speech_prob": 2.0365276213851757e-05}, {"id": 13, "seek": 8120, "start": 88.8, "end": 92.64, "text": " something useful. It can go all the way to kind of complete full rectification of the signal,", "tokens": [50364, 291, 483, 257, 2931, 337, 300, 1185, 300, 2445, 281, 767, 48256, 16235, 293, 4317, 360, 50744, 50744, 746, 4420, 13, 467, 393, 352, 439, 264, 636, 281, 733, 295, 3566, 1577, 11048, 3774, 295, 264, 6358, 11, 50936, 50980, 733, 295, 411, 364, 8236, 2158, 498, 291, 528, 13, 6001, 43, 52, 307, 472, 689, 291, 485, 865, 11, 352, 2286, 13, 51220, 51296, 440, 3894, 24433, 390, 2673, 1228, 264, 20828, 1639, 294, 257, 460, 1770, 1270, 300, 321, 1009, 362, 51572, 51608], "temperature": 0.0, "avg_logprob": -0.1517786047924524, "compression_ratio": 1.5061728395061729, "no_speech_prob": 2.0365276213851757e-05}, {"id": 14, "seek": 8120, "start": 93.52000000000001, "end": 98.32000000000001, "text": " kind of like an absolute value if you want. PreLU is one where you... Yeah, go ahead.", "tokens": [50364, 291, 483, 257, 2931, 337, 300, 1185, 300, 2445, 281, 767, 48256, 16235, 293, 4317, 360, 50744, 50744, 746, 4420, 13, 467, 393, 352, 439, 264, 636, 281, 733, 295, 3566, 1577, 11048, 3774, 295, 264, 6358, 11, 50936, 50980, 733, 295, 411, 364, 8236, 2158, 498, 291, 528, 13, 6001, 43, 52, 307, 472, 689, 291, 485, 865, 11, 352, 2286, 13, 51220, 51296, 440, 3894, 24433, 390, 2673, 1228, 264, 20828, 1639, 294, 257, 460, 1770, 1270, 300, 321, 1009, 362, 51572, 51608], "temperature": 0.0, "avg_logprob": -0.1517786047924524, "compression_ratio": 1.5061728395061729, "no_speech_prob": 2.0365276213851757e-05}, {"id": 15, "seek": 8120, "start": 99.84, "end": 105.36, "text": " The previous activation was usually using the discriminator in a GAN such that we always have", "tokens": [50364, 291, 483, 257, 2931, 337, 300, 1185, 300, 2445, 281, 767, 48256, 16235, 293, 4317, 360, 50744, 50744, 746, 4420, 13, 467, 393, 352, 439, 264, 636, 281, 733, 295, 3566, 1577, 11048, 3774, 295, 264, 6358, 11, 50936, 50980, 733, 295, 411, 364, 8236, 2158, 498, 291, 528, 13, 6001, 43, 52, 307, 472, 689, 291, 485, 865, 11, 352, 2286, 13, 51220, 51296, 440, 3894, 24433, 390, 2673, 1228, 264, 20828, 1639, 294, 257, 460, 1770, 1270, 300, 321, 1009, 362, 51572, 51608], "temperature": 0.0, "avg_logprob": -0.1517786047924524, "compression_ratio": 1.5061728395061729, "no_speech_prob": 2.0365276213851757e-05}, {"id": 16, "seek": 10536, "start": 105.36, "end": 111.76, "text": " gradients going backwards for the generator. And also this activation was necessary in order to", "tokens": [50364, 2771, 2448, 516, 12204, 337, 264, 19265, 13, 400, 611, 341, 24433, 390, 4818, 294, 1668, 281, 50684, 50684, 3847, 264, 588, 25193, 3209, 286, 855, 412, 264, 2863, 295, 264, 1508, 11, 570, 797, 11, 1419, 50960, 50960, 411, 257, 588, 11, 588, 25193, 3209, 11, 309, 390, 1936, 6243, 281, 483, 2771, 2448, 13974, 646, 570, 51252, 51252, 321, 645, 411, 8121, 493, 294, 472, 295, 264, 10787, 10968, 1553, 11, 291, 458, 11, 689, 1203, 390, 4018, 484, 51532, 51532, 293, 550, 1825, 576, 362, 668, 767, 8895, 498, 291, 2759, 380, 362, 1143, 11, 291, 458, 11, 51780, 51808], "temperature": 0.0, "avg_logprob": -0.10872548421223958, "compression_ratio": 1.7660377358490567, "no_speech_prob": 9.442530426895246e-05}, {"id": 17, "seek": 10536, "start": 111.76, "end": 117.28, "text": " train the very skinny network I show at the beginning of the class, because again, having", "tokens": [50364, 2771, 2448, 516, 12204, 337, 264, 19265, 13, 400, 611, 341, 24433, 390, 4818, 294, 1668, 281, 50684, 50684, 3847, 264, 588, 25193, 3209, 286, 855, 412, 264, 2863, 295, 264, 1508, 11, 570, 797, 11, 1419, 50960, 50960, 411, 257, 588, 11, 588, 25193, 3209, 11, 309, 390, 1936, 6243, 281, 483, 2771, 2448, 13974, 646, 570, 51252, 51252, 321, 645, 411, 8121, 493, 294, 472, 295, 264, 10787, 10968, 1553, 11, 291, 458, 11, 689, 1203, 390, 4018, 484, 51532, 51532, 293, 550, 1825, 576, 362, 668, 767, 8895, 498, 291, 2759, 380, 362, 1143, 11, 291, 458, 11, 51780, 51808], "temperature": 0.0, "avg_logprob": -0.10872548421223958, "compression_ratio": 1.7660377358490567, "no_speech_prob": 9.442530426895246e-05}, {"id": 18, "seek": 10536, "start": 117.28, "end": 123.12, "text": " like a very, very skinny network, it was basically impossible to get gradients flowing back because", "tokens": [50364, 2771, 2448, 516, 12204, 337, 264, 19265, 13, 400, 611, 341, 24433, 390, 4818, 294, 1668, 281, 50684, 50684, 3847, 264, 588, 25193, 3209, 286, 855, 412, 264, 2863, 295, 264, 1508, 11, 570, 797, 11, 1419, 50960, 50960, 411, 257, 588, 11, 588, 25193, 3209, 11, 309, 390, 1936, 6243, 281, 483, 2771, 2448, 13974, 646, 570, 51252, 51252, 321, 645, 411, 8121, 493, 294, 472, 295, 264, 10787, 10968, 1553, 11, 291, 458, 11, 689, 1203, 390, 4018, 484, 51532, 51532, 293, 550, 1825, 576, 362, 668, 767, 8895, 498, 291, 2759, 380, 362, 1143, 11, 291, 458, 11, 51780, 51808], "temperature": 0.0, "avg_logprob": -0.10872548421223958, "compression_ratio": 1.7660377358490567, "no_speech_prob": 9.442530426895246e-05}, {"id": 19, "seek": 10536, "start": 123.12, "end": 128.72, "text": " we were like ending up in one of the quadrants without, you know, where everything was zero out", "tokens": [50364, 2771, 2448, 516, 12204, 337, 264, 19265, 13, 400, 611, 341, 24433, 390, 4818, 294, 1668, 281, 50684, 50684, 3847, 264, 588, 25193, 3209, 286, 855, 412, 264, 2863, 295, 264, 1508, 11, 570, 797, 11, 1419, 50960, 50960, 411, 257, 588, 11, 588, 25193, 3209, 11, 309, 390, 1936, 6243, 281, 483, 2771, 2448, 13974, 646, 570, 51252, 51252, 321, 645, 411, 8121, 493, 294, 472, 295, 264, 10787, 10968, 1553, 11, 291, 458, 11, 689, 1203, 390, 4018, 484, 51532, 51532, 293, 550, 1825, 576, 362, 668, 767, 8895, 498, 291, 2759, 380, 362, 1143, 11, 291, 458, 11, 51780, 51808], "temperature": 0.0, "avg_logprob": -0.10872548421223958, "compression_ratio": 1.7660377358490567, "no_speech_prob": 9.442530426895246e-05}, {"id": 20, "seek": 10536, "start": 128.72, "end": 133.68, "text": " and then nothing would have been actually trained if you wouldn't have used, you know,", "tokens": [50364, 2771, 2448, 516, 12204, 337, 264, 19265, 13, 400, 611, 341, 24433, 390, 4818, 294, 1668, 281, 50684, 50684, 3847, 264, 588, 25193, 3209, 286, 855, 412, 264, 2863, 295, 264, 1508, 11, 570, 797, 11, 1419, 50960, 50960, 411, 257, 588, 11, 588, 25193, 3209, 11, 309, 390, 1936, 6243, 281, 483, 2771, 2448, 13974, 646, 570, 51252, 51252, 321, 645, 411, 8121, 493, 294, 472, 295, 264, 10787, 10968, 1553, 11, 291, 458, 11, 689, 1203, 390, 4018, 484, 51532, 51532, 293, 550, 1825, 576, 362, 668, 767, 8895, 498, 291, 2759, 380, 362, 1143, 11, 291, 458, 11, 51780, 51808], "temperature": 0.0, "avg_logprob": -0.10872548421223958, "compression_ratio": 1.7660377358490567, "no_speech_prob": 9.442530426895246e-05}, {"id": 21, "seek": 13368, "start": 133.68, "end": 138.4, "text": " this activation function that allows me to get some kind of gradients even if we are", "tokens": [50364, 341, 24433, 2445, 300, 4045, 385, 281, 483, 512, 733, 295, 2771, 2448, 754, 498, 321, 366, 50600, 50600, 294, 264, 10682, 689, 321, 366, 1382, 281, 26835, 264, 5598, 13, 407, 11, 1338, 13, 50796, 50820, 1779, 13, 407, 6001, 43, 52, 307, 6457, 2531, 3993, 300, 586, 264, 13525, 293, 264, 3671, 1252, 393, 312, 445, 51364, 51364, 466, 1340, 13, 400, 1392, 11, 437, 311, 1880, 466, 439, 729, 6828, 300, 321, 445, 1866, 307, 300, 456, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.21194140116373697, "compression_ratio": 1.5650224215246638, "no_speech_prob": 3.966599251725711e-06}, {"id": 22, "seek": 13368, "start": 138.4, "end": 142.32, "text": " in the regions where we are trying to suppress the output. So, yeah.", "tokens": [50364, 341, 24433, 2445, 300, 4045, 385, 281, 483, 512, 733, 295, 2771, 2448, 754, 498, 321, 366, 50600, 50600, 294, 264, 10682, 689, 321, 366, 1382, 281, 26835, 264, 5598, 13, 407, 11, 1338, 13, 50796, 50820, 1779, 13, 407, 6001, 43, 52, 307, 6457, 2531, 3993, 300, 586, 264, 13525, 293, 264, 3671, 1252, 393, 312, 445, 51364, 51364, 466, 1340, 13, 400, 1392, 11, 437, 311, 1880, 466, 439, 729, 6828, 300, 321, 445, 1866, 307, 300, 456, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.21194140116373697, "compression_ratio": 1.5650224215246638, "no_speech_prob": 3.966599251725711e-06}, {"id": 23, "seek": 13368, "start": 142.8, "end": 153.68, "text": " Right. So PreLU is fairly similar except that now the slope and the negative side can be just", "tokens": [50364, 341, 24433, 2445, 300, 4045, 385, 281, 483, 512, 733, 295, 2771, 2448, 754, 498, 321, 366, 50600, 50600, 294, 264, 10682, 689, 321, 366, 1382, 281, 26835, 264, 5598, 13, 407, 11, 1338, 13, 50796, 50820, 1779, 13, 407, 6001, 43, 52, 307, 6457, 2531, 3993, 300, 586, 264, 13525, 293, 264, 3671, 1252, 393, 312, 445, 51364, 51364, 466, 1340, 13, 400, 1392, 11, 437, 311, 1880, 466, 439, 729, 6828, 300, 321, 445, 1866, 307, 300, 456, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.21194140116373697, "compression_ratio": 1.5650224215246638, "no_speech_prob": 3.966599251725711e-06}, {"id": 24, "seek": 13368, "start": 153.68, "end": 161.84, "text": " about anything. And okay, what's interesting about all those functions that we just saw is that there", "tokens": [50364, 341, 24433, 2445, 300, 4045, 385, 281, 483, 512, 733, 295, 2771, 2448, 754, 498, 321, 366, 50600, 50600, 294, 264, 10682, 689, 321, 366, 1382, 281, 26835, 264, 5598, 13, 407, 11, 1338, 13, 50796, 50820, 1779, 13, 407, 6001, 43, 52, 307, 6457, 2531, 3993, 300, 586, 264, 13525, 293, 264, 3671, 1252, 393, 312, 445, 51364, 51364, 466, 1340, 13, 400, 1392, 11, 437, 311, 1880, 466, 439, 729, 6828, 300, 321, 445, 1866, 307, 300, 456, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.21194140116373697, "compression_ratio": 1.5650224215246638, "no_speech_prob": 3.966599251725711e-06}, {"id": 25, "seek": 16184, "start": 161.84, "end": 169.84, "text": " are scaling variant in the sense that you can multiply the signal by two and the output will", "tokens": [50364, 366, 21589, 17501, 294, 264, 2020, 300, 291, 393, 12972, 264, 6358, 538, 732, 293, 264, 5598, 486, 50764, 50764, 406, 312, 3105, 13, 286, 914, 11, 309, 576, 312, 17207, 538, 732, 11, 457, 5911, 44553, 13, 407, 436, 366, 51000, 51000, 10344, 281, 4373, 13, 821, 311, 572, 1333, 295, 35698, 4373, 294, 729, 6828, 11, 558, 30, 1436, 456, 311, 51328, 51328, 787, 472, 2107, 12, 1889, 17409, 293, 309, 311, 257, 8199, 472, 13, 407, 586, 321, 434, 1242, 666, 6828, 689, 264, 4373, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.16479906828507132, "compression_ratio": 1.6309012875536482, "no_speech_prob": 4.356761110102525e-06}, {"id": 26, "seek": 16184, "start": 169.84, "end": 174.56, "text": " not be changed. I mean, it would be multiplied by two, but otherwise unchanged. So they are", "tokens": [50364, 366, 21589, 17501, 294, 264, 2020, 300, 291, 393, 12972, 264, 6358, 538, 732, 293, 264, 5598, 486, 50764, 50764, 406, 312, 3105, 13, 286, 914, 11, 309, 576, 312, 17207, 538, 732, 11, 457, 5911, 44553, 13, 407, 436, 366, 51000, 51000, 10344, 281, 4373, 13, 821, 311, 572, 1333, 295, 35698, 4373, 294, 729, 6828, 11, 558, 30, 1436, 456, 311, 51328, 51328, 787, 472, 2107, 12, 1889, 17409, 293, 309, 311, 257, 8199, 472, 13, 407, 586, 321, 434, 1242, 666, 6828, 689, 264, 4373, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.16479906828507132, "compression_ratio": 1.6309012875536482, "no_speech_prob": 4.356761110102525e-06}, {"id": 27, "seek": 16184, "start": 174.56, "end": 181.12, "text": " equivalent to scale. There's no sort of intrinsic scale in those functions, right? Because there's", "tokens": [50364, 366, 21589, 17501, 294, 264, 2020, 300, 291, 393, 12972, 264, 6358, 538, 732, 293, 264, 5598, 486, 50764, 50764, 406, 312, 3105, 13, 286, 914, 11, 309, 576, 312, 17207, 538, 732, 11, 457, 5911, 44553, 13, 407, 436, 366, 51000, 51000, 10344, 281, 4373, 13, 821, 311, 572, 1333, 295, 35698, 4373, 294, 729, 6828, 11, 558, 30, 1436, 456, 311, 51328, 51328, 787, 472, 2107, 12, 1889, 17409, 293, 309, 311, 257, 8199, 472, 13, 407, 586, 321, 434, 1242, 666, 6828, 689, 264, 4373, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.16479906828507132, "compression_ratio": 1.6309012875536482, "no_speech_prob": 4.356761110102525e-06}, {"id": 28, "seek": 16184, "start": 181.12, "end": 187.28, "text": " only one non-linearity and it's a sharp one. So now we're getting into functions where the scale", "tokens": [50364, 366, 21589, 17501, 294, 264, 2020, 300, 291, 393, 12972, 264, 6358, 538, 732, 293, 264, 5598, 486, 50764, 50764, 406, 312, 3105, 13, 286, 914, 11, 309, 576, 312, 17207, 538, 732, 11, 457, 5911, 44553, 13, 407, 436, 366, 51000, 51000, 10344, 281, 4373, 13, 821, 311, 572, 1333, 295, 35698, 4373, 294, 729, 6828, 11, 558, 30, 1436, 456, 311, 51328, 51328, 787, 472, 2107, 12, 1889, 17409, 293, 309, 311, 257, 8199, 472, 13, 407, 586, 321, 434, 1242, 666, 6828, 689, 264, 4373, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.16479906828507132, "compression_ratio": 1.6309012875536482, "no_speech_prob": 4.356761110102525e-06}, {"id": 29, "seek": 18728, "start": 187.28, "end": 194.24, "text": " matters. So the amplitude of the incoming signal will affect the type of non-linearity that you're", "tokens": [50364, 7001, 13, 407, 264, 27433, 295, 264, 22341, 6358, 486, 3345, 264, 2010, 295, 2107, 12, 1889, 17409, 300, 291, 434, 50712, 50712, 516, 281, 483, 13, 400, 472, 295, 729, 307, 264, 2787, 1804, 13, 407, 2787, 1804, 307, 1333, 295, 257, 819, 9364, 3037, 51012, 51012, 295, 1300, 43, 52, 498, 291, 528, 13, 467, 311, 733, 295, 264, 2787, 3037, 295, 3353, 644, 13, 400, 309, 311, 2673, 51276, 51316, 13075, 1602, 382, 291, 393, 536, 412, 264, 1192, 510, 11, 472, 670, 9861, 3565, 472, 1804, 21510, 9861, 2031, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.15743599132615693, "compression_ratio": 1.6592920353982301, "no_speech_prob": 2.355146898480598e-05}, {"id": 30, "seek": 18728, "start": 194.24, "end": 200.24, "text": " going to get. And one of those is the soft plus. So soft plus is sort of a differentiable version", "tokens": [50364, 7001, 13, 407, 264, 27433, 295, 264, 22341, 6358, 486, 3345, 264, 2010, 295, 2107, 12, 1889, 17409, 300, 291, 434, 50712, 50712, 516, 281, 483, 13, 400, 472, 295, 729, 307, 264, 2787, 1804, 13, 407, 2787, 1804, 307, 1333, 295, 257, 819, 9364, 3037, 51012, 51012, 295, 1300, 43, 52, 498, 291, 528, 13, 467, 311, 733, 295, 264, 2787, 3037, 295, 3353, 644, 13, 400, 309, 311, 2673, 51276, 51316, 13075, 1602, 382, 291, 393, 536, 412, 264, 1192, 510, 11, 472, 670, 9861, 3565, 472, 1804, 21510, 9861, 2031, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.15743599132615693, "compression_ratio": 1.6592920353982301, "no_speech_prob": 2.355146898480598e-05}, {"id": 31, "seek": 18728, "start": 200.24, "end": 205.52, "text": " of ReLU if you want. It's kind of the soft version of positive part. And it's usually", "tokens": [50364, 7001, 13, 407, 264, 27433, 295, 264, 22341, 6358, 486, 3345, 264, 2010, 295, 2107, 12, 1889, 17409, 300, 291, 434, 50712, 50712, 516, 281, 483, 13, 400, 472, 295, 729, 307, 264, 2787, 1804, 13, 407, 2787, 1804, 307, 1333, 295, 257, 819, 9364, 3037, 51012, 51012, 295, 1300, 43, 52, 498, 291, 528, 13, 467, 311, 733, 295, 264, 2787, 3037, 295, 3353, 644, 13, 400, 309, 311, 2673, 51276, 51316, 13075, 1602, 382, 291, 393, 536, 412, 264, 1192, 510, 11, 472, 670, 9861, 3565, 472, 1804, 21510, 9861, 2031, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.15743599132615693, "compression_ratio": 1.6592920353982301, "no_speech_prob": 2.355146898480598e-05}, {"id": 32, "seek": 18728, "start": 206.32, "end": 211.76, "text": " parameterized as you can see at the top here, one over beta log one plus exponential beta x.", "tokens": [50364, 7001, 13, 407, 264, 27433, 295, 264, 22341, 6358, 486, 3345, 264, 2010, 295, 2107, 12, 1889, 17409, 300, 291, 434, 50712, 50712, 516, 281, 483, 13, 400, 472, 295, 729, 307, 264, 2787, 1804, 13, 407, 2787, 1804, 307, 1333, 295, 257, 819, 9364, 3037, 51012, 51012, 295, 1300, 43, 52, 498, 291, 528, 13, 467, 311, 733, 295, 264, 2787, 3037, 295, 3353, 644, 13, 400, 309, 311, 2673, 51276, 51316, 13075, 1602, 382, 291, 393, 536, 412, 264, 1192, 510, 11, 472, 670, 9861, 3565, 472, 1804, 21510, 9861, 2031, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.15743599132615693, "compression_ratio": 1.6592920353982301, "no_speech_prob": 2.355146898480598e-05}, {"id": 33, "seek": 21176, "start": 211.76, "end": 218.95999999999998, "text": " So it's kind of like the log sum exponential that we've been using a lot for sort of various", "tokens": [50364, 407, 309, 311, 733, 295, 411, 264, 3565, 2408, 21510, 300, 321, 600, 668, 1228, 257, 688, 337, 1333, 295, 3683, 50724, 50724, 4334, 11, 3993, 510, 472, 295, 264, 2115, 294, 264, 2408, 307, 2681, 281, 472, 11, 597, 307, 733, 295, 411, 21510, 50992, 50992, 4018, 498, 291, 528, 13, 407, 300, 1542, 411, 733, 295, 257, 2445, 300, 1333, 295, 35114, 310, 984, 307, 264, 51372, 51372, 6575, 2445, 337, 2416, 3353, 4190, 293, 35114, 310, 984, 4018, 337, 3671, 4190, 13, 51588, 51588, 407, 309, 8542, 1024, 264, 1300, 43, 52, 13, 467, 575, 257, 4373, 13075, 1673, 13, 639, 9861, 13075, 307, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1699486122475014, "compression_ratio": 1.7713178294573644, "no_speech_prob": 1.4284288226917852e-05}, {"id": 34, "seek": 21176, "start": 218.95999999999998, "end": 224.32, "text": " purpose, except here one of the terms in the sum is equal to one, which is kind of like exponential", "tokens": [50364, 407, 309, 311, 733, 295, 411, 264, 3565, 2408, 21510, 300, 321, 600, 668, 1228, 257, 688, 337, 1333, 295, 3683, 50724, 50724, 4334, 11, 3993, 510, 472, 295, 264, 2115, 294, 264, 2408, 307, 2681, 281, 472, 11, 597, 307, 733, 295, 411, 21510, 50992, 50992, 4018, 498, 291, 528, 13, 407, 300, 1542, 411, 733, 295, 257, 2445, 300, 1333, 295, 35114, 310, 984, 307, 264, 51372, 51372, 6575, 2445, 337, 2416, 3353, 4190, 293, 35114, 310, 984, 4018, 337, 3671, 4190, 13, 51588, 51588, 407, 309, 8542, 1024, 264, 1300, 43, 52, 13, 467, 575, 257, 4373, 13075, 1673, 13, 639, 9861, 13075, 307, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1699486122475014, "compression_ratio": 1.7713178294573644, "no_speech_prob": 1.4284288226917852e-05}, {"id": 35, "seek": 21176, "start": 224.32, "end": 231.92, "text": " zero if you want. So that looks like kind of a function that sort of asymptotically is the", "tokens": [50364, 407, 309, 311, 733, 295, 411, 264, 3565, 2408, 21510, 300, 321, 600, 668, 1228, 257, 688, 337, 1333, 295, 3683, 50724, 50724, 4334, 11, 3993, 510, 472, 295, 264, 2115, 294, 264, 2408, 307, 2681, 281, 472, 11, 597, 307, 733, 295, 411, 21510, 50992, 50992, 4018, 498, 291, 528, 13, 407, 300, 1542, 411, 733, 295, 257, 2445, 300, 1333, 295, 35114, 310, 984, 307, 264, 51372, 51372, 6575, 2445, 337, 2416, 3353, 4190, 293, 35114, 310, 984, 4018, 337, 3671, 4190, 13, 51588, 51588, 407, 309, 8542, 1024, 264, 1300, 43, 52, 13, 467, 575, 257, 4373, 13075, 1673, 13, 639, 9861, 13075, 307, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1699486122475014, "compression_ratio": 1.7713178294573644, "no_speech_prob": 1.4284288226917852e-05}, {"id": 36, "seek": 21176, "start": 231.92, "end": 236.23999999999998, "text": " identity function for large positive values and asymptotically zero for negative values.", "tokens": [50364, 407, 309, 311, 733, 295, 411, 264, 3565, 2408, 21510, 300, 321, 600, 668, 1228, 257, 688, 337, 1333, 295, 3683, 50724, 50724, 4334, 11, 3993, 510, 472, 295, 264, 2115, 294, 264, 2408, 307, 2681, 281, 472, 11, 597, 307, 733, 295, 411, 21510, 50992, 50992, 4018, 498, 291, 528, 13, 407, 300, 1542, 411, 733, 295, 257, 2445, 300, 1333, 295, 35114, 310, 984, 307, 264, 51372, 51372, 6575, 2445, 337, 2416, 3353, 4190, 293, 35114, 310, 984, 4018, 337, 3671, 4190, 13, 51588, 51588, 407, 309, 8542, 1024, 264, 1300, 43, 52, 13, 467, 575, 257, 4373, 13075, 1673, 13, 639, 9861, 13075, 307, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1699486122475014, "compression_ratio": 1.7713178294573644, "no_speech_prob": 1.4284288226917852e-05}, {"id": 37, "seek": 21176, "start": 236.23999999999998, "end": 241.6, "text": " So it approximates the ReLU. It has a scale parameter though. This beta parameter is", "tokens": [50364, 407, 309, 311, 733, 295, 411, 264, 3565, 2408, 21510, 300, 321, 600, 668, 1228, 257, 688, 337, 1333, 295, 3683, 50724, 50724, 4334, 11, 3993, 510, 472, 295, 264, 2115, 294, 264, 2408, 307, 2681, 281, 472, 11, 597, 307, 733, 295, 411, 21510, 50992, 50992, 4018, 498, 291, 528, 13, 407, 300, 1542, 411, 733, 295, 257, 2445, 300, 1333, 295, 35114, 310, 984, 307, 264, 51372, 51372, 6575, 2445, 337, 2416, 3353, 4190, 293, 35114, 310, 984, 4018, 337, 3671, 4190, 13, 51588, 51588, 407, 309, 8542, 1024, 264, 1300, 43, 52, 13, 467, 575, 257, 4373, 13075, 1673, 13, 639, 9861, 13075, 307, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1699486122475014, "compression_ratio": 1.7713178294573644, "no_speech_prob": 1.4284288226917852e-05}, {"id": 38, "seek": 24160, "start": 241.6, "end": 248.16, "text": " a parameter. The larger beta, the more the function will look like a ReLU. So the kick will be kind of", "tokens": [50364, 257, 13075, 13, 440, 4833, 9861, 11, 264, 544, 264, 2445, 486, 574, 411, 257, 1300, 43, 52, 13, 407, 264, 4437, 486, 312, 733, 295, 50692, 50692, 264, 4538, 486, 312, 733, 295, 44670, 498, 9861, 1709, 281, 13202, 13, 583, 300, 2445, 575, 257, 4373, 13, 51056, 51240, 823, 291, 393, 13075, 1125, 729, 6828, 294, 3683, 2098, 13, 400, 341, 307, 1333, 295, 1071, 1365, 295, 51584, 51616], "temperature": 0.0, "avg_logprob": -0.1510960043293156, "compression_ratio": 1.564516129032258, "no_speech_prob": 5.862695161340525e-06}, {"id": 39, "seek": 24160, "start": 248.16, "end": 255.44, "text": " the corner will be kind of sharper if beta goes to infinity. But that function has a scale.", "tokens": [50364, 257, 13075, 13, 440, 4833, 9861, 11, 264, 544, 264, 2445, 486, 574, 411, 257, 1300, 43, 52, 13, 407, 264, 4437, 486, 312, 733, 295, 50692, 50692, 264, 4538, 486, 312, 733, 295, 44670, 498, 9861, 1709, 281, 13202, 13, 583, 300, 2445, 575, 257, 4373, 13, 51056, 51240, 823, 291, 393, 13075, 1125, 729, 6828, 294, 3683, 2098, 13, 400, 341, 307, 1333, 295, 1071, 1365, 295, 51584, 51616], "temperature": 0.0, "avg_logprob": -0.1510960043293156, "compression_ratio": 1.564516129032258, "no_speech_prob": 5.862695161340525e-06}, {"id": 40, "seek": 24160, "start": 259.12, "end": 266.0, "text": " Now you can parameterize those functions in various ways. And this is sort of another example of", "tokens": [50364, 257, 13075, 13, 440, 4833, 9861, 11, 264, 544, 264, 2445, 486, 574, 411, 257, 1300, 43, 52, 13, 407, 264, 4437, 486, 312, 733, 295, 50692, 50692, 264, 4538, 486, 312, 733, 295, 44670, 498, 9861, 1709, 281, 13202, 13, 583, 300, 2445, 575, 257, 4373, 13, 51056, 51240, 823, 291, 393, 13075, 1125, 729, 6828, 294, 3683, 2098, 13, 400, 341, 307, 1333, 295, 1071, 1365, 295, 51584, 51616], "temperature": 0.0, "avg_logprob": -0.1510960043293156, "compression_ratio": 1.564516129032258, "no_speech_prob": 5.862695161340525e-06}, {"id": 41, "seek": 26600, "start": 266.0, "end": 273.92, "text": " kind of a soft version of ReLU if you want, where here you use ReLU as a basis and then you add", "tokens": [50364, 733, 295, 257, 2787, 3037, 295, 1300, 43, 52, 498, 291, 528, 11, 689, 510, 291, 764, 1300, 43, 52, 382, 257, 5143, 293, 550, 291, 909, 50760, 50760, 257, 1359, 5754, 281, 309, 300, 733, 295, 1669, 309, 5508, 13, 286, 393, 380, 980, 291, 300, 604, 295, 729, 51112, 51112, 575, 604, 1729, 5002, 670, 264, 2357, 13, 467, 534, 5946, 322, 264, 1154, 11, 457, 436, 439, 51420, 51420, 362, 733, 295, 2531, 7221, 498, 291, 528, 13, 639, 611, 291, 393, 652, 1333, 295, 15684, 51644, 51708], "temperature": 0.0, "avg_logprob": -0.14393208616523331, "compression_ratio": 1.6222222222222222, "no_speech_prob": 4.092722065252019e-06}, {"id": 42, "seek": 26600, "start": 273.92, "end": 280.96, "text": " a small constant to it that kind of makes it smooth. I can't tell you that any of those", "tokens": [50364, 733, 295, 257, 2787, 3037, 295, 1300, 43, 52, 498, 291, 528, 11, 689, 510, 291, 764, 1300, 43, 52, 382, 257, 5143, 293, 550, 291, 909, 50760, 50760, 257, 1359, 5754, 281, 309, 300, 733, 295, 1669, 309, 5508, 13, 286, 393, 380, 980, 291, 300, 604, 295, 729, 51112, 51112, 575, 604, 1729, 5002, 670, 264, 2357, 13, 467, 534, 5946, 322, 264, 1154, 11, 457, 436, 439, 51420, 51420, 362, 733, 295, 2531, 7221, 498, 291, 528, 13, 639, 611, 291, 393, 652, 1333, 295, 15684, 51644, 51708], "temperature": 0.0, "avg_logprob": -0.14393208616523331, "compression_ratio": 1.6222222222222222, "no_speech_prob": 4.092722065252019e-06}, {"id": 43, "seek": 26600, "start": 280.96, "end": 287.12, "text": " has any particular advantage over the others. It really depends on the problem, but they all", "tokens": [50364, 733, 295, 257, 2787, 3037, 295, 1300, 43, 52, 498, 291, 528, 11, 689, 510, 291, 764, 1300, 43, 52, 382, 257, 5143, 293, 550, 291, 909, 50760, 50760, 257, 1359, 5754, 281, 309, 300, 733, 295, 1669, 309, 5508, 13, 286, 393, 380, 980, 291, 300, 604, 295, 729, 51112, 51112, 575, 604, 1729, 5002, 670, 264, 2357, 13, 467, 534, 5946, 322, 264, 1154, 11, 457, 436, 439, 51420, 51420, 362, 733, 295, 2531, 7221, 498, 291, 528, 13, 639, 611, 291, 393, 652, 1333, 295, 15684, 51644, 51708], "temperature": 0.0, "avg_logprob": -0.14393208616523331, "compression_ratio": 1.6222222222222222, "no_speech_prob": 4.092722065252019e-06}, {"id": 44, "seek": 26600, "start": 287.12, "end": 291.6, "text": " have kind of similar properties if you want. This also you can make sort of continuously", "tokens": [50364, 733, 295, 257, 2787, 3037, 295, 1300, 43, 52, 498, 291, 528, 11, 689, 510, 291, 764, 1300, 43, 52, 382, 257, 5143, 293, 550, 291, 909, 50760, 50760, 257, 1359, 5754, 281, 309, 300, 733, 295, 1669, 309, 5508, 13, 286, 393, 380, 980, 291, 300, 604, 295, 729, 51112, 51112, 575, 604, 1729, 5002, 670, 264, 2357, 13, 467, 534, 5946, 322, 264, 1154, 11, 457, 436, 439, 51420, 51420, 362, 733, 295, 2531, 7221, 498, 291, 528, 13, 639, 611, 291, 393, 652, 1333, 295, 15684, 51644, 51708], "temperature": 0.0, "avg_logprob": -0.14393208616523331, "compression_ratio": 1.6222222222222222, "no_speech_prob": 4.092722065252019e-06}, {"id": 45, "seek": 29160, "start": 291.6, "end": 303.36, "text": " closer to ReLU. So one difference here in this case is that this guy actually goes negative.", "tokens": [50364, 4966, 281, 1300, 43, 52, 13, 407, 472, 2649, 510, 294, 341, 1389, 307, 300, 341, 2146, 767, 1709, 3671, 13, 50952, 50952, 407, 8343, 264, 1300, 43, 52, 300, 575, 1080, 7285, 412, 4018, 11, 1080, 12750, 35114, 1370, 412, 4018, 11, 51260, 51300, 341, 2146, 1709, 2507, 4018, 13, 400, 300, 815, 420, 815, 406, 312, 5002, 563, 5413, 322, 264, 3861, 291, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.18370845352393994, "compression_ratio": 1.5480225988700564, "no_speech_prob": 7.766016096866224e-06}, {"id": 46, "seek": 29160, "start": 303.36, "end": 309.52000000000004, "text": " So unlike the ReLU that has its minimum at zero, its horizontal asymptote at zero,", "tokens": [50364, 4966, 281, 1300, 43, 52, 13, 407, 472, 2649, 510, 294, 341, 1389, 307, 300, 341, 2146, 767, 1709, 3671, 13, 50952, 50952, 407, 8343, 264, 1300, 43, 52, 300, 575, 1080, 7285, 412, 4018, 11, 1080, 12750, 35114, 1370, 412, 4018, 11, 51260, 51300, 341, 2146, 1709, 2507, 4018, 13, 400, 300, 815, 420, 815, 406, 312, 5002, 563, 5413, 322, 264, 3861, 291, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.18370845352393994, "compression_ratio": 1.5480225988700564, "no_speech_prob": 7.766016096866224e-06}, {"id": 47, "seek": 29160, "start": 310.32000000000005, "end": 317.68, "text": " this guy goes below zero. And that may or may not be advantageous depending on the application you", "tokens": [50364, 4966, 281, 1300, 43, 52, 13, 407, 472, 2649, 510, 294, 341, 1389, 307, 300, 341, 2146, 767, 1709, 3671, 13, 50952, 50952, 407, 8343, 264, 1300, 43, 52, 300, 575, 1080, 7285, 412, 4018, 11, 1080, 12750, 35114, 1370, 412, 4018, 11, 51260, 51300, 341, 2146, 1709, 2507, 4018, 13, 400, 300, 815, 420, 815, 406, 312, 5002, 563, 5413, 322, 264, 3861, 291, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.18370845352393994, "compression_ratio": 1.5480225988700564, "no_speech_prob": 7.766016096866224e-06}, {"id": 48, "seek": 31768, "start": 317.68, "end": 323.92, "text": " have. Sometimes it's advantageous because it allows the system to basically make the average of the", "tokens": [50364, 362, 13, 4803, 309, 311, 5002, 563, 570, 309, 4045, 264, 1185, 281, 1936, 652, 264, 4274, 295, 264, 50676, 50676, 5598, 4018, 11, 597, 307, 5002, 563, 337, 1629, 3467, 295, 337, 16235, 23475, 32181, 13, 440, 51020, 51020, 17443, 300, 366, 4582, 281, 6815, 411, 341, 486, 536, 1293, 3353, 293, 3671, 4190, 11, 51260, 51260, 597, 486, 550, 41881, 4663, 813, 498, 436, 787, 536, 3353, 4190, 13, 51420, 51504], "temperature": 0.0, "avg_logprob": -0.18692002614339193, "compression_ratio": 1.6465116279069767, "no_speech_prob": 1.4062297850614414e-05}, {"id": 49, "seek": 31768, "start": 323.92, "end": 330.8, "text": " output zero, which is advantageous for certain types of for gradient descent convergence. The", "tokens": [50364, 362, 13, 4803, 309, 311, 5002, 563, 570, 309, 4045, 264, 1185, 281, 1936, 652, 264, 4274, 295, 264, 50676, 50676, 5598, 4018, 11, 597, 307, 5002, 563, 337, 1629, 3467, 295, 337, 16235, 23475, 32181, 13, 440, 51020, 51020, 17443, 300, 366, 4582, 281, 6815, 411, 341, 486, 536, 1293, 3353, 293, 3671, 4190, 11, 51260, 51260, 597, 486, 550, 41881, 4663, 813, 498, 436, 787, 536, 3353, 4190, 13, 51420, 51504], "temperature": 0.0, "avg_logprob": -0.18692002614339193, "compression_ratio": 1.6465116279069767, "no_speech_prob": 1.4062297850614414e-05}, {"id": 50, "seek": 31768, "start": 330.8, "end": 335.6, "text": " weights that are connected to units like this will see both positive and negative values,", "tokens": [50364, 362, 13, 4803, 309, 311, 5002, 563, 570, 309, 4045, 264, 1185, 281, 1936, 652, 264, 4274, 295, 264, 50676, 50676, 5598, 4018, 11, 597, 307, 5002, 563, 337, 1629, 3467, 295, 337, 16235, 23475, 32181, 13, 440, 51020, 51020, 17443, 300, 366, 4582, 281, 6815, 411, 341, 486, 536, 1293, 3353, 293, 3671, 4190, 11, 51260, 51260, 597, 486, 550, 41881, 4663, 813, 498, 436, 787, 536, 3353, 4190, 13, 51420, 51504], "temperature": 0.0, "avg_logprob": -0.18692002614339193, "compression_ratio": 1.6465116279069767, "no_speech_prob": 1.4062297850614414e-05}, {"id": 51, "seek": 31768, "start": 335.6, "end": 338.8, "text": " which will then converge faster than if they only see positive values.", "tokens": [50364, 362, 13, 4803, 309, 311, 5002, 563, 570, 309, 4045, 264, 1185, 281, 1936, 652, 264, 4274, 295, 264, 50676, 50676, 5598, 4018, 11, 597, 307, 5002, 563, 337, 1629, 3467, 295, 337, 16235, 23475, 32181, 13, 440, 51020, 51020, 17443, 300, 366, 4582, 281, 6815, 411, 341, 486, 536, 1293, 3353, 293, 3671, 4190, 11, 51260, 51260, 597, 486, 550, 41881, 4663, 813, 498, 436, 787, 536, 3353, 4190, 13, 51420, 51504], "temperature": 0.0, "avg_logprob": -0.18692002614339193, "compression_ratio": 1.6465116279069767, "no_speech_prob": 1.4062297850614414e-05}, {"id": 52, "seek": 33880, "start": 338.8, "end": 346.48, "text": " So it's a bit the same here and it's just a kind of a differently, a different parameterization of", "tokens": [50364, 407, 309, 311, 257, 857, 264, 912, 510, 293, 309, 311, 445, 257, 733, 295, 257, 7614, 11, 257, 819, 13075, 2144, 295, 50748, 50748, 264, 912, 551, 498, 291, 528, 11, 365, 819, 7221, 13, 407, 295, 1164, 456, 311, 9131, 295, 51096, 51096, 17840, 295, 341, 365, 3683, 9834, 365, 819, 7221, 13, 400, 512, 295, 552, 300, 362, 51476, 51476], "temperature": 0.0, "avg_logprob": -0.2546238238994892, "compression_ratio": 1.7658227848101267, "no_speech_prob": 3.089420033575152e-06}, {"id": 53, "seek": 33880, "start": 346.48, "end": 353.44, "text": " the same thing if you want, with different properties. So of course there's tons of", "tokens": [50364, 407, 309, 311, 257, 857, 264, 912, 510, 293, 309, 311, 445, 257, 733, 295, 257, 7614, 11, 257, 819, 13075, 2144, 295, 50748, 50748, 264, 912, 551, 498, 291, 528, 11, 365, 819, 7221, 13, 407, 295, 1164, 456, 311, 9131, 295, 51096, 51096, 17840, 295, 341, 365, 3683, 9834, 365, 819, 7221, 13, 400, 512, 295, 552, 300, 362, 51476, 51476], "temperature": 0.0, "avg_logprob": -0.2546238238994892, "compression_ratio": 1.7658227848101267, "no_speech_prob": 3.089420033575152e-06}, {"id": 54, "seek": 33880, "start": 353.44, "end": 361.04, "text": " variations of this with various parameters with different properties. And some of them that have", "tokens": [50364, 407, 309, 311, 257, 857, 264, 912, 510, 293, 309, 311, 445, 257, 733, 295, 257, 7614, 11, 257, 819, 13075, 2144, 295, 50748, 50748, 264, 912, 551, 498, 291, 528, 11, 365, 819, 7221, 13, 407, 295, 1164, 456, 311, 9131, 295, 51096, 51096, 17840, 295, 341, 365, 3683, 9834, 365, 819, 7221, 13, 400, 512, 295, 552, 300, 362, 51476, 51476], "temperature": 0.0, "avg_logprob": -0.2546238238994892, "compression_ratio": 1.7658227848101267, "no_speech_prob": 3.089420033575152e-06}, {"id": 55, "seek": 36104, "start": 361.04, "end": 369.28000000000003, "text": " particular properties that kind of relate them to Gaussian distributions, for example.", "tokens": [50364, 1729, 7221, 300, 733, 295, 10961, 552, 281, 39148, 37870, 11, 337, 1365, 13, 50776, 50816, 639, 307, 406, 264, 38379, 7316, 295, 257, 39148, 13, 583, 1392, 11, 370, 729, 645, 721, 300, 51132, 51132, 362, 472, 350, 16431, 294, 552, 13, 400, 498, 264, 350, 475, 307, 8199, 11, 456, 311, 572, 4373, 13, 759, 264, 350, 475, 575, 512, 4373, 294, 51404, 51404, 309, 11, 456, 307, 512, 4373, 11, 457, 309, 311, 920, 1333, 295, 257, 2167, 350, 475, 2107, 12, 1889, 17409, 13, 823, 321, 434, 1242, 666, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.1861264730237194, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.2218885785841849e-05}, {"id": 56, "seek": 36104, "start": 370.08000000000004, "end": 376.40000000000003, "text": " This is not the cumulative distribution of a Gaussian. But okay, so those were things that", "tokens": [50364, 1729, 7221, 300, 733, 295, 10961, 552, 281, 39148, 37870, 11, 337, 1365, 13, 50776, 50816, 639, 307, 406, 264, 38379, 7316, 295, 257, 39148, 13, 583, 1392, 11, 370, 729, 645, 721, 300, 51132, 51132, 362, 472, 350, 16431, 294, 552, 13, 400, 498, 264, 350, 475, 307, 8199, 11, 456, 311, 572, 4373, 13, 759, 264, 350, 475, 575, 512, 4373, 294, 51404, 51404, 309, 11, 456, 307, 512, 4373, 11, 457, 309, 311, 920, 1333, 295, 257, 2167, 350, 475, 2107, 12, 1889, 17409, 13, 823, 321, 434, 1242, 666, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.1861264730237194, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.2218885785841849e-05}, {"id": 57, "seek": 36104, "start": 376.40000000000003, "end": 381.84000000000003, "text": " have one kinks in them. And if the kink is sharp, there's no scale. If the kink has some scale in", "tokens": [50364, 1729, 7221, 300, 733, 295, 10961, 552, 281, 39148, 37870, 11, 337, 1365, 13, 50776, 50816, 639, 307, 406, 264, 38379, 7316, 295, 257, 39148, 13, 583, 1392, 11, 370, 729, 645, 721, 300, 51132, 51132, 362, 472, 350, 16431, 294, 552, 13, 400, 498, 264, 350, 475, 307, 8199, 11, 456, 311, 572, 4373, 13, 759, 264, 350, 475, 575, 512, 4373, 294, 51404, 51404, 309, 11, 456, 307, 512, 4373, 11, 457, 309, 311, 920, 1333, 295, 257, 2167, 350, 475, 2107, 12, 1889, 17409, 13, 823, 321, 434, 1242, 666, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.1861264730237194, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.2218885785841849e-05}, {"id": 58, "seek": 36104, "start": 381.84000000000003, "end": 386.56, "text": " it, there is some scale, but it's still sort of a single kink non-linearity. Now we're getting into", "tokens": [50364, 1729, 7221, 300, 733, 295, 10961, 552, 281, 39148, 37870, 11, 337, 1365, 13, 50776, 50816, 639, 307, 406, 264, 38379, 7316, 295, 257, 39148, 13, 583, 1392, 11, 370, 729, 645, 721, 300, 51132, 51132, 362, 472, 350, 16431, 294, 552, 13, 400, 498, 264, 350, 475, 307, 8199, 11, 456, 311, 572, 4373, 13, 759, 264, 350, 475, 575, 512, 4373, 294, 51404, 51404, 309, 11, 456, 307, 512, 4373, 11, 457, 309, 311, 920, 1333, 295, 257, 2167, 350, 475, 2107, 12, 1889, 17409, 13, 823, 321, 434, 1242, 666, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.1861264730237194, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.2218885785841849e-05}, {"id": 59, "seek": 38656, "start": 386.56, "end": 392.72, "text": " non-linearities that have two kinks. So this one is basically a saturating value. I'm not sure why", "tokens": [50364, 2107, 12, 28263, 1088, 300, 362, 732, 350, 16431, 13, 407, 341, 472, 307, 1936, 257, 21160, 990, 2158, 13, 286, 478, 406, 988, 983, 50672, 50672, 21160, 1024, 412, 2309, 13, 1545, 406, 30, 583, 983, 406, 13075, 1125, 341, 257, 707, 1101, 30, 407, 510, 311, 257, 5508, 51068, 51100, 2445, 300, 291, 434, 4963, 365, 570, 309, 311, 1143, 294, 18680, 1753, 36170, 11, 294, 290, 770, 18680, 1753, 36170, 11, 51444, 51444, 294, 364, 4904, 44, 11, 294, 2787, 41167, 13, 8537, 341, 307, 257, 732, 12, 676, 2787, 41167, 291, 393, 519, 295, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.21302182466081046, "compression_ratio": 1.5869565217391304, "no_speech_prob": 1.7778018445824273e-05}, {"id": 60, "seek": 38656, "start": 392.72, "end": 400.64, "text": " saturates at six. Why not? But why not parameterize this a little better? So here's a smooth", "tokens": [50364, 2107, 12, 28263, 1088, 300, 362, 732, 350, 16431, 13, 407, 341, 472, 307, 1936, 257, 21160, 990, 2158, 13, 286, 478, 406, 988, 983, 50672, 50672, 21160, 1024, 412, 2309, 13, 1545, 406, 30, 583, 983, 406, 13075, 1125, 341, 257, 707, 1101, 30, 407, 510, 311, 257, 5508, 51068, 51100, 2445, 300, 291, 434, 4963, 365, 570, 309, 311, 1143, 294, 18680, 1753, 36170, 11, 294, 290, 770, 18680, 1753, 36170, 11, 51444, 51444, 294, 364, 4904, 44, 11, 294, 2787, 41167, 13, 8537, 341, 307, 257, 732, 12, 676, 2787, 41167, 291, 393, 519, 295, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.21302182466081046, "compression_ratio": 1.5869565217391304, "no_speech_prob": 1.7778018445824273e-05}, {"id": 61, "seek": 38656, "start": 401.28000000000003, "end": 408.16, "text": " function that you're familiar with because it's used in recurrent nets, in gated recurrent nets,", "tokens": [50364, 2107, 12, 28263, 1088, 300, 362, 732, 350, 16431, 13, 407, 341, 472, 307, 1936, 257, 21160, 990, 2158, 13, 286, 478, 406, 988, 983, 50672, 50672, 21160, 1024, 412, 2309, 13, 1545, 406, 30, 583, 983, 406, 13075, 1125, 341, 257, 707, 1101, 30, 407, 510, 311, 257, 5508, 51068, 51100, 2445, 300, 291, 434, 4963, 365, 570, 309, 311, 1143, 294, 18680, 1753, 36170, 11, 294, 290, 770, 18680, 1753, 36170, 11, 51444, 51444, 294, 364, 4904, 44, 11, 294, 2787, 41167, 13, 8537, 341, 307, 257, 732, 12, 676, 2787, 41167, 291, 393, 519, 295, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.21302182466081046, "compression_ratio": 1.5869565217391304, "no_speech_prob": 1.7778018445824273e-05}, {"id": 62, "seek": 38656, "start": 408.16, "end": 413.92, "text": " in an STM, in softmax. Basically this is a two-way softmax you can think of.", "tokens": [50364, 2107, 12, 28263, 1088, 300, 362, 732, 350, 16431, 13, 407, 341, 472, 307, 1936, 257, 21160, 990, 2158, 13, 286, 478, 406, 988, 983, 50672, 50672, 21160, 1024, 412, 2309, 13, 1545, 406, 30, 583, 983, 406, 13075, 1125, 341, 257, 707, 1101, 30, 407, 510, 311, 257, 5508, 51068, 51100, 2445, 300, 291, 434, 4963, 365, 570, 309, 311, 1143, 294, 18680, 1753, 36170, 11, 294, 290, 770, 18680, 1753, 36170, 11, 51444, 51444, 294, 364, 4904, 44, 11, 294, 2787, 41167, 13, 8537, 341, 307, 257, 732, 12, 676, 2787, 41167, 291, 393, 519, 295, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.21302182466081046, "compression_ratio": 1.5869565217391304, "no_speech_prob": 1.7778018445824273e-05}, {"id": 63, "seek": 41392, "start": 413.92, "end": 420.96000000000004, "text": " Basically this is a two-way softmax. You can think of it this way. And this is just a function that", "tokens": [50364, 8537, 341, 307, 257, 732, 12, 676, 2787, 41167, 13, 509, 393, 519, 295, 309, 341, 636, 13, 400, 341, 307, 445, 257, 2445, 300, 50716, 50716, 1709, 733, 295, 19565, 1296, 4018, 293, 472, 13, 467, 311, 2171, 1219, 257, 43261, 72, 12, 1068, 3194, 2445, 382, 731, 51020, 51020, 570, 309, 1163, 1539, 490, 512, 589, 294, 10649, 13, 467, 311, 264, 19200, 10649, 13, 400, 550, 456, 307, 264, 51384, 51384, 1152, 8482, 27747, 300, 321, 611, 2825, 466, 13, 467, 311, 1936, 14800, 281, 264, 4556, 3280, 327, 11, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.12113777632565842, "compression_ratio": 1.6212765957446809, "no_speech_prob": 1.2028742276015691e-05}, {"id": 64, "seek": 41392, "start": 420.96000000000004, "end": 427.04, "text": " goes kind of smoothly between zero and one. It's sometimes called a Fermi-derived function as well", "tokens": [50364, 8537, 341, 307, 257, 732, 12, 676, 2787, 41167, 13, 509, 393, 519, 295, 309, 341, 636, 13, 400, 341, 307, 445, 257, 2445, 300, 50716, 50716, 1709, 733, 295, 19565, 1296, 4018, 293, 472, 13, 467, 311, 2171, 1219, 257, 43261, 72, 12, 1068, 3194, 2445, 382, 731, 51020, 51020, 570, 309, 1163, 1539, 490, 512, 589, 294, 10649, 13, 467, 311, 264, 19200, 10649, 13, 400, 550, 456, 307, 264, 51384, 51384, 1152, 8482, 27747, 300, 321, 611, 2825, 466, 13, 467, 311, 1936, 14800, 281, 264, 4556, 3280, 327, 11, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.12113777632565842, "compression_ratio": 1.6212765957446809, "no_speech_prob": 1.2028742276015691e-05}, {"id": 65, "seek": 41392, "start": 427.04, "end": 434.32, "text": " because it derives from some work in physics. It's the SQL physics. And then there is the", "tokens": [50364, 8537, 341, 307, 257, 732, 12, 676, 2787, 41167, 13, 509, 393, 519, 295, 309, 341, 636, 13, 400, 341, 307, 445, 257, 2445, 300, 50716, 50716, 1709, 733, 295, 19565, 1296, 4018, 293, 472, 13, 467, 311, 2171, 1219, 257, 43261, 72, 12, 1068, 3194, 2445, 382, 731, 51020, 51020, 570, 309, 1163, 1539, 490, 512, 589, 294, 10649, 13, 467, 311, 264, 19200, 10649, 13, 400, 550, 456, 307, 264, 51384, 51384, 1152, 8482, 27747, 300, 321, 611, 2825, 466, 13, 467, 311, 1936, 14800, 281, 264, 4556, 3280, 327, 11, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.12113777632565842, "compression_ratio": 1.6212765957446809, "no_speech_prob": 1.2028742276015691e-05}, {"id": 66, "seek": 41392, "start": 434.32, "end": 439.04, "text": " hard probability tangent that we also talked about. It's basically identical to the sigmoid,", "tokens": [50364, 8537, 341, 307, 257, 732, 12, 676, 2787, 41167, 13, 509, 393, 519, 295, 309, 341, 636, 13, 400, 341, 307, 445, 257, 2445, 300, 50716, 50716, 1709, 733, 295, 19565, 1296, 4018, 293, 472, 13, 467, 311, 2171, 1219, 257, 43261, 72, 12, 1068, 3194, 2445, 382, 731, 51020, 51020, 570, 309, 1163, 1539, 490, 512, 589, 294, 10649, 13, 467, 311, 264, 19200, 10649, 13, 400, 550, 456, 307, 264, 51384, 51384, 1152, 8482, 27747, 300, 321, 611, 2825, 466, 13, 467, 311, 1936, 14800, 281, 264, 4556, 3280, 327, 11, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.12113777632565842, "compression_ratio": 1.6212765957446809, "no_speech_prob": 1.2028742276015691e-05}, {"id": 67, "seek": 43904, "start": 439.04, "end": 444.56, "text": " except it's centered. So it goes between minus one and plus one, and it's a little, it's twice", "tokens": [50364, 3993, 309, 311, 18988, 13, 407, 309, 1709, 1296, 3175, 472, 293, 1804, 472, 11, 293, 309, 311, 257, 707, 11, 309, 311, 6091, 50640, 50640, 264, 27433, 293, 264, 6052, 307, 257, 707, 819, 13, 583, 309, 5749, 264, 912, 3090, 13, 440, 5002, 295, 50912, 50912, 1152, 8482, 27747, 307, 300, 264, 5598, 307, 11, 291, 393, 2066, 264, 5598, 281, 406, 362, 4018, 914, 11, 51224, 51224, 457, 312, 1998, 281, 1419, 4018, 914, 13, 400, 797, 11, 300, 311, 5002, 563, 337, 264, 17443, 300, 1524, 51612, 51612, 570, 436, 536, 3353, 293, 3671, 4190, 293, 436, 3928, 281, 41881, 4663, 13, 663, 311, 264, 1389, 13, 51820], "temperature": 0.0, "avg_logprob": -0.10255441331026847, "compression_ratio": 1.730909090909091, "no_speech_prob": 7.811439672877896e-07}, {"id": 68, "seek": 43904, "start": 444.56, "end": 450.0, "text": " the amplitude and the gain is a little different. But it plays the same role. The advantage of", "tokens": [50364, 3993, 309, 311, 18988, 13, 407, 309, 1709, 1296, 3175, 472, 293, 1804, 472, 11, 293, 309, 311, 257, 707, 11, 309, 311, 6091, 50640, 50640, 264, 27433, 293, 264, 6052, 307, 257, 707, 819, 13, 583, 309, 5749, 264, 912, 3090, 13, 440, 5002, 295, 50912, 50912, 1152, 8482, 27747, 307, 300, 264, 5598, 307, 11, 291, 393, 2066, 264, 5598, 281, 406, 362, 4018, 914, 11, 51224, 51224, 457, 312, 1998, 281, 1419, 4018, 914, 13, 400, 797, 11, 300, 311, 5002, 563, 337, 264, 17443, 300, 1524, 51612, 51612, 570, 436, 536, 3353, 293, 3671, 4190, 293, 436, 3928, 281, 41881, 4663, 13, 663, 311, 264, 1389, 13, 51820], "temperature": 0.0, "avg_logprob": -0.10255441331026847, "compression_ratio": 1.730909090909091, "no_speech_prob": 7.811439672877896e-07}, {"id": 69, "seek": 43904, "start": 450.0, "end": 456.24, "text": " hard probability tangent is that the output is, you can expect the output to not have zero mean,", "tokens": [50364, 3993, 309, 311, 18988, 13, 407, 309, 1709, 1296, 3175, 472, 293, 1804, 472, 11, 293, 309, 311, 257, 707, 11, 309, 311, 6091, 50640, 50640, 264, 27433, 293, 264, 6052, 307, 257, 707, 819, 13, 583, 309, 5749, 264, 912, 3090, 13, 440, 5002, 295, 50912, 50912, 1152, 8482, 27747, 307, 300, 264, 5598, 307, 11, 291, 393, 2066, 264, 5598, 281, 406, 362, 4018, 914, 11, 51224, 51224, 457, 312, 1998, 281, 1419, 4018, 914, 13, 400, 797, 11, 300, 311, 5002, 563, 337, 264, 17443, 300, 1524, 51612, 51612, 570, 436, 536, 3353, 293, 3671, 4190, 293, 436, 3928, 281, 41881, 4663, 13, 663, 311, 264, 1389, 13, 51820], "temperature": 0.0, "avg_logprob": -0.10255441331026847, "compression_ratio": 1.730909090909091, "no_speech_prob": 7.811439672877896e-07}, {"id": 70, "seek": 43904, "start": 456.24, "end": 464.0, "text": " but be close to having zero mean. And again, that's advantageous for the weights that follow", "tokens": [50364, 3993, 309, 311, 18988, 13, 407, 309, 1709, 1296, 3175, 472, 293, 1804, 472, 11, 293, 309, 311, 257, 707, 11, 309, 311, 6091, 50640, 50640, 264, 27433, 293, 264, 6052, 307, 257, 707, 819, 13, 583, 309, 5749, 264, 912, 3090, 13, 440, 5002, 295, 50912, 50912, 1152, 8482, 27747, 307, 300, 264, 5598, 307, 11, 291, 393, 2066, 264, 5598, 281, 406, 362, 4018, 914, 11, 51224, 51224, 457, 312, 1998, 281, 1419, 4018, 914, 13, 400, 797, 11, 300, 311, 5002, 563, 337, 264, 17443, 300, 1524, 51612, 51612, 570, 436, 536, 3353, 293, 3671, 4190, 293, 436, 3928, 281, 41881, 4663, 13, 663, 311, 264, 1389, 13, 51820], "temperature": 0.0, "avg_logprob": -0.10255441331026847, "compression_ratio": 1.730909090909091, "no_speech_prob": 7.811439672877896e-07}, {"id": 71, "seek": 46400, "start": 464.0, "end": 469.68, "text": " because they see positive and negative values and they tend to converge faster, if that's the case.", "tokens": [50364, 570, 436, 536, 3353, 293, 3671, 4190, 293, 436, 3928, 281, 41881, 4663, 11, 498, 300, 311, 264, 1389, 13, 50648, 50676, 286, 1143, 281, 312, 257, 955, 3429, 295, 729, 13, 8590, 11, 498, 291, 8630, 257, 688, 295, 4556, 3280, 3742, 294, 867, 7914, 294, 257, 51028, 51028, 18161, 2533, 11, 291, 393, 3928, 281, 406, 1466, 588, 19621, 13, 509, 362, 281, 312, 588, 5026, 466, 51456, 51456, 2710, 2144, 498, 291, 528, 264, 1185, 281, 41881, 498, 291, 362, 867, 7914, 13, 407, 294, 300, 2020, 11, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.12336671728836862, "compression_ratio": 1.6814159292035398, "no_speech_prob": 3.2376922263210872e-06}, {"id": 72, "seek": 46400, "start": 470.24, "end": 477.28, "text": " I used to be a big fan of those. Unfortunately, if you stack a lot of sigmoids in many layers in a", "tokens": [50364, 570, 436, 536, 3353, 293, 3671, 4190, 293, 436, 3928, 281, 41881, 4663, 11, 498, 300, 311, 264, 1389, 13, 50648, 50676, 286, 1143, 281, 312, 257, 955, 3429, 295, 729, 13, 8590, 11, 498, 291, 8630, 257, 688, 295, 4556, 3280, 3742, 294, 867, 7914, 294, 257, 51028, 51028, 18161, 2533, 11, 291, 393, 3928, 281, 406, 1466, 588, 19621, 13, 509, 362, 281, 312, 588, 5026, 466, 51456, 51456, 2710, 2144, 498, 291, 528, 264, 1185, 281, 41881, 498, 291, 362, 867, 7914, 13, 407, 294, 300, 2020, 11, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.12336671728836862, "compression_ratio": 1.6814159292035398, "no_speech_prob": 3.2376922263210872e-06}, {"id": 73, "seek": 46400, "start": 477.28, "end": 485.84, "text": " neural net, you can tend to not learn very efficiently. You have to be very careful about", "tokens": [50364, 570, 436, 536, 3353, 293, 3671, 4190, 293, 436, 3928, 281, 41881, 4663, 11, 498, 300, 311, 264, 1389, 13, 50648, 50676, 286, 1143, 281, 312, 257, 955, 3429, 295, 729, 13, 8590, 11, 498, 291, 8630, 257, 688, 295, 4556, 3280, 3742, 294, 867, 7914, 294, 257, 51028, 51028, 18161, 2533, 11, 291, 393, 3928, 281, 406, 1466, 588, 19621, 13, 509, 362, 281, 312, 588, 5026, 466, 51456, 51456, 2710, 2144, 498, 291, 528, 264, 1185, 281, 41881, 498, 291, 362, 867, 7914, 13, 407, 294, 300, 2020, 11, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.12336671728836862, "compression_ratio": 1.6814159292035398, "no_speech_prob": 3.2376922263210872e-06}, {"id": 74, "seek": 46400, "start": 485.84, "end": 490.8, "text": " normalization if you want the system to converge if you have many layers. So in that sense,", "tokens": [50364, 570, 436, 536, 3353, 293, 3671, 4190, 293, 436, 3928, 281, 41881, 4663, 11, 498, 300, 311, 264, 1389, 13, 50648, 50676, 286, 1143, 281, 312, 257, 955, 3429, 295, 729, 13, 8590, 11, 498, 291, 8630, 257, 688, 295, 4556, 3280, 3742, 294, 867, 7914, 294, 257, 51028, 51028, 18161, 2533, 11, 291, 393, 3928, 281, 406, 1466, 588, 19621, 13, 509, 362, 281, 312, 588, 5026, 466, 51456, 51456, 2710, 2144, 498, 291, 528, 264, 1185, 281, 41881, 498, 291, 362, 867, 7914, 13, 407, 294, 300, 2020, 11, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.12336671728836862, "compression_ratio": 1.6814159292035398, "no_speech_prob": 3.2376922263210872e-06}, {"id": 75, "seek": 49080, "start": 490.8, "end": 499.2, "text": " the single key functions are better for deeper networks. So his soft sign, this is basically", "tokens": [50364, 264, 2167, 2141, 6828, 366, 1101, 337, 7731, 9590, 13, 407, 702, 2787, 1465, 11, 341, 307, 1936, 50784, 50820, 257, 857, 411, 264, 4556, 3280, 327, 11, 3993, 300, 309, 1177, 380, 483, 281, 264, 35114, 1370, 382, 2370, 13, 407, 309, 1177, 380, 483, 51108, 51108, 5541, 3030, 264, 35114, 1370, 382, 2661, 13, 407, 472, 1154, 365, 1090, 8482, 27747, 293, 264, 51472, 51472, 4556, 3280, 327, 307, 300, 562, 291, 483, 1998, 281, 264, 35114, 1370, 11, 264, 16235, 1709, 281, 4018, 6457, 2661, 13, 51772, 51804], "temperature": 0.0, "avg_logprob": -0.10936427372758106, "compression_ratio": 1.744186046511628, "no_speech_prob": 1.5688980056438595e-05}, {"id": 76, "seek": 49080, "start": 499.92, "end": 505.68, "text": " a bit like the sigmoid, except that it doesn't get to the asymptote as fast. So it doesn't get", "tokens": [50364, 264, 2167, 2141, 6828, 366, 1101, 337, 7731, 9590, 13, 407, 702, 2787, 1465, 11, 341, 307, 1936, 50784, 50820, 257, 857, 411, 264, 4556, 3280, 327, 11, 3993, 300, 309, 1177, 380, 483, 281, 264, 35114, 1370, 382, 2370, 13, 407, 309, 1177, 380, 483, 51108, 51108, 5541, 3030, 264, 35114, 1370, 382, 2661, 13, 407, 472, 1154, 365, 1090, 8482, 27747, 293, 264, 51472, 51472, 4556, 3280, 327, 307, 300, 562, 291, 483, 1998, 281, 264, 35114, 1370, 11, 264, 16235, 1709, 281, 4018, 6457, 2661, 13, 51772, 51804], "temperature": 0.0, "avg_logprob": -0.10936427372758106, "compression_ratio": 1.744186046511628, "no_speech_prob": 1.5688980056438595e-05}, {"id": 77, "seek": 49080, "start": 505.68, "end": 512.96, "text": " stuck towards the asymptote as quickly. So one problem with high probability tangent and the", "tokens": [50364, 264, 2167, 2141, 6828, 366, 1101, 337, 7731, 9590, 13, 407, 702, 2787, 1465, 11, 341, 307, 1936, 50784, 50820, 257, 857, 411, 264, 4556, 3280, 327, 11, 3993, 300, 309, 1177, 380, 483, 281, 264, 35114, 1370, 382, 2370, 13, 407, 309, 1177, 380, 483, 51108, 51108, 5541, 3030, 264, 35114, 1370, 382, 2661, 13, 407, 472, 1154, 365, 1090, 8482, 27747, 293, 264, 51472, 51472, 4556, 3280, 327, 307, 300, 562, 291, 483, 1998, 281, 264, 35114, 1370, 11, 264, 16235, 1709, 281, 4018, 6457, 2661, 13, 51772, 51804], "temperature": 0.0, "avg_logprob": -0.10936427372758106, "compression_ratio": 1.744186046511628, "no_speech_prob": 1.5688980056438595e-05}, {"id": 78, "seek": 49080, "start": 512.96, "end": 518.96, "text": " sigmoid is that when you get close to the asymptote, the gradient goes to zero fairly quickly.", "tokens": [50364, 264, 2167, 2141, 6828, 366, 1101, 337, 7731, 9590, 13, 407, 702, 2787, 1465, 11, 341, 307, 1936, 50784, 50820, 257, 857, 411, 264, 4556, 3280, 327, 11, 3993, 300, 309, 1177, 380, 483, 281, 264, 35114, 1370, 382, 2370, 13, 407, 309, 1177, 380, 483, 51108, 51108, 5541, 3030, 264, 35114, 1370, 382, 2661, 13, 407, 472, 1154, 365, 1090, 8482, 27747, 293, 264, 51472, 51472, 4556, 3280, 327, 307, 300, 562, 291, 483, 1998, 281, 264, 35114, 1370, 11, 264, 16235, 1709, 281, 4018, 6457, 2661, 13, 51772, 51804], "temperature": 0.0, "avg_logprob": -0.10936427372758106, "compression_ratio": 1.744186046511628, "no_speech_prob": 1.5688980056438595e-05}, {"id": 79, "seek": 51896, "start": 518.96, "end": 527.2800000000001, "text": " And so if the weights of a unit become too large, they saturate this unit and the gradients get very", "tokens": [50364, 400, 370, 498, 264, 17443, 295, 257, 4985, 1813, 886, 2416, 11, 436, 21160, 473, 341, 4985, 293, 264, 2771, 2448, 483, 588, 50780, 50780, 1359, 293, 550, 264, 4985, 1177, 380, 1190, 588, 2661, 3602, 13, 467, 311, 257, 1154, 300, 8198, 1293, 294, 51196, 51196, 4556, 3280, 3742, 293, 9848, 65, 7940, 27747, 13, 400, 370, 2787, 1465, 307, 257, 2445, 300, 390, 10348, 538, 38949, 1004, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.19099128409607768, "compression_ratio": 1.507936507936508, "no_speech_prob": 4.1572220652597025e-06}, {"id": 80, "seek": 51896, "start": 527.2800000000001, "end": 535.6, "text": " small and then the unit doesn't run very quickly anymore. It's a problem that exists both in", "tokens": [50364, 400, 370, 498, 264, 17443, 295, 257, 4985, 1813, 886, 2416, 11, 436, 21160, 473, 341, 4985, 293, 264, 2771, 2448, 483, 588, 50780, 50780, 1359, 293, 550, 264, 4985, 1177, 380, 1190, 588, 2661, 3602, 13, 467, 311, 257, 1154, 300, 8198, 1293, 294, 51196, 51196, 4556, 3280, 3742, 293, 9848, 65, 7940, 27747, 13, 400, 370, 2787, 1465, 307, 257, 2445, 300, 390, 10348, 538, 38949, 1004, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.19099128409607768, "compression_ratio": 1.507936507936508, "no_speech_prob": 4.1572220652597025e-06}, {"id": 81, "seek": 51896, "start": 535.6, "end": 542.88, "text": " sigmoids and hyperbolic tangent. And so soft sign is a function that was proposed by Yoshio", "tokens": [50364, 400, 370, 498, 264, 17443, 295, 257, 4985, 1813, 886, 2416, 11, 436, 21160, 473, 341, 4985, 293, 264, 2771, 2448, 483, 588, 50780, 50780, 1359, 293, 550, 264, 4985, 1177, 380, 1190, 588, 2661, 3602, 13, 467, 311, 257, 1154, 300, 8198, 1293, 294, 51196, 51196, 4556, 3280, 3742, 293, 9848, 65, 7940, 27747, 13, 400, 370, 2787, 1465, 307, 257, 2445, 300, 390, 10348, 538, 38949, 1004, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.19099128409607768, "compression_ratio": 1.507936507936508, "no_speech_prob": 4.1572220652597025e-06}, {"id": 82, "seek": 54288, "start": 542.88, "end": 549.84, "text": " Benjiu and some of his collaborators and it saturates slower. So it doesn't have that same", "tokens": [50364, 3964, 4013, 84, 293, 512, 295, 702, 39789, 293, 309, 21160, 1024, 14009, 13, 407, 309, 1177, 380, 362, 300, 912, 50712, 50712, 1154, 13, 286, 914, 11, 309, 575, 264, 1154, 611, 11, 457, 406, 281, 264, 912, 8396, 13, 400, 341, 307, 264, 6182, 11, 51116, 51116, 1152, 27747, 11, 1152, 1266, 39, 13, 286, 500, 380, 458, 498, 309, 17037, 300, 1315, 11, 457, 309, 311, 1936, 445, 257, 12428, 13, 51484, 51560], "temperature": 0.0, "avg_logprob": -0.23407312539907602, "compression_ratio": 1.518918918918919, "no_speech_prob": 6.143953214632347e-06}, {"id": 83, "seek": 54288, "start": 549.84, "end": 557.92, "text": " problem. I mean, it has the problem also, but not to the same extent. And this is the opposite,", "tokens": [50364, 3964, 4013, 84, 293, 512, 295, 702, 39789, 293, 309, 21160, 1024, 14009, 13, 407, 309, 1177, 380, 362, 300, 912, 50712, 50712, 1154, 13, 286, 914, 11, 309, 575, 264, 1154, 611, 11, 457, 406, 281, 264, 912, 8396, 13, 400, 341, 307, 264, 6182, 11, 51116, 51116, 1152, 27747, 11, 1152, 1266, 39, 13, 286, 500, 380, 458, 498, 309, 17037, 300, 1315, 11, 457, 309, 311, 1936, 445, 257, 12428, 13, 51484, 51560], "temperature": 0.0, "avg_logprob": -0.23407312539907602, "compression_ratio": 1.518918918918919, "no_speech_prob": 6.143953214632347e-06}, {"id": 84, "seek": 54288, "start": 557.92, "end": 565.28, "text": " hard tangent, hard 10H. I don't know if it deserves that name, but it's basically just a ramp.", "tokens": [50364, 3964, 4013, 84, 293, 512, 295, 702, 39789, 293, 309, 21160, 1024, 14009, 13, 407, 309, 1177, 380, 362, 300, 912, 50712, 50712, 1154, 13, 286, 914, 11, 309, 575, 264, 1154, 611, 11, 457, 406, 281, 264, 912, 8396, 13, 400, 341, 307, 264, 6182, 11, 51116, 51116, 1152, 27747, 11, 1152, 1266, 39, 13, 286, 500, 380, 458, 498, 309, 17037, 300, 1315, 11, 457, 309, 311, 1936, 445, 257, 12428, 13, 51484, 51560], "temperature": 0.0, "avg_logprob": -0.23407312539907602, "compression_ratio": 1.518918918918919, "no_speech_prob": 6.143953214632347e-06}, {"id": 85, "seek": 56528, "start": 565.28, "end": 573.28, "text": " And that works surprisingly well, particularly if your weights are somehow kept within a", "tokens": [50364, 400, 300, 1985, 17600, 731, 11, 4098, 498, 428, 17443, 366, 6063, 4305, 1951, 257, 50764, 50816, 1359, 2158, 370, 264, 6815, 500, 380, 21160, 473, 886, 709, 13, 467, 311, 8830, 577, 731, 309, 1985, 13, 51160, 51204, 400, 561, 362, 1143, 341, 294, 3683, 30628, 13, 583, 300, 311, 1333, 295, 2107, 12, 1115, 515, 13, 407, 1152, 14678, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.16405096787672777, "compression_ratio": 1.4648648648648648, "no_speech_prob": 8.13918359199306e-06}, {"id": 86, "seek": 56528, "start": 574.3199999999999, "end": 581.1999999999999, "text": " small value so the units don't saturate too much. It's surprising how well it works.", "tokens": [50364, 400, 300, 1985, 17600, 731, 11, 4098, 498, 428, 17443, 366, 6063, 4305, 1951, 257, 50764, 50816, 1359, 2158, 370, 264, 6815, 500, 380, 21160, 473, 886, 709, 13, 467, 311, 8830, 577, 731, 309, 1985, 13, 51160, 51204, 400, 561, 362, 1143, 341, 294, 3683, 30628, 13, 583, 300, 311, 1333, 295, 2107, 12, 1115, 515, 13, 407, 1152, 14678, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.16405096787672777, "compression_ratio": 1.4648648648648648, "no_speech_prob": 8.13918359199306e-06}, {"id": 87, "seek": 56528, "start": 582.0799999999999, "end": 590.48, "text": " And people have used this in various contexts. But that's sort of non-standard. So hard threshold", "tokens": [50364, 400, 300, 1985, 17600, 731, 11, 4098, 498, 428, 17443, 366, 6063, 4305, 1951, 257, 50764, 50816, 1359, 2158, 370, 264, 6815, 500, 380, 21160, 473, 886, 709, 13, 467, 311, 8830, 577, 731, 309, 1985, 13, 51160, 51204, 400, 561, 362, 1143, 341, 294, 3683, 30628, 13, 583, 300, 311, 1333, 295, 2107, 12, 1115, 515, 13, 407, 1152, 14678, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.16405096787672777, "compression_ratio": 1.4648648648648648, "no_speech_prob": 8.13918359199306e-06}, {"id": 88, "seek": 59048, "start": 590.48, "end": 596.24, "text": " is very rarely used because you can't really propagate gradient through it. And this is", "tokens": [50364, 307, 588, 13752, 1143, 570, 291, 393, 380, 534, 48256, 16235, 807, 309, 13, 400, 341, 307, 50652, 50652, 534, 437, 4305, 561, 490, 7962, 278, 646, 79, 1513, 294, 264, 4060, 82, 293, 5285, 82, 11, 597, 307, 300, 436, 645, 1228, 50924, 50924, 17434, 22027, 13, 400, 370, 436, 994, 380, 519, 295, 264, 1379, 1558, 295, 2771, 2448, 570, 295, 300, 13, 51212, 51328, 3950, 661, 6828, 366, 13752, 1143, 294, 264, 4319, 295, 18161, 36170, 420, 412, 1935, 337, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.07771649305847869, "compression_ratio": 1.6053811659192825, "no_speech_prob": 1.2797117960872129e-05}, {"id": 89, "seek": 59048, "start": 596.24, "end": 601.6800000000001, "text": " really what kept people from inventing backprop in the 60s and 70s, which is that they were using", "tokens": [50364, 307, 588, 13752, 1143, 570, 291, 393, 380, 534, 48256, 16235, 807, 309, 13, 400, 341, 307, 50652, 50652, 534, 437, 4305, 561, 490, 7962, 278, 646, 79, 1513, 294, 264, 4060, 82, 293, 5285, 82, 11, 597, 307, 300, 436, 645, 1228, 50924, 50924, 17434, 22027, 13, 400, 370, 436, 994, 380, 519, 295, 264, 1379, 1558, 295, 2771, 2448, 570, 295, 300, 13, 51212, 51328, 3950, 661, 6828, 366, 13752, 1143, 294, 264, 4319, 295, 18161, 36170, 420, 412, 1935, 337, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.07771649305847869, "compression_ratio": 1.6053811659192825, "no_speech_prob": 1.2797117960872129e-05}, {"id": 90, "seek": 59048, "start": 601.6800000000001, "end": 607.44, "text": " binary neurons. And so they didn't think of the whole idea of gradients because of that.", "tokens": [50364, 307, 588, 13752, 1143, 570, 291, 393, 380, 534, 48256, 16235, 807, 309, 13, 400, 341, 307, 50652, 50652, 534, 437, 4305, 561, 490, 7962, 278, 646, 79, 1513, 294, 264, 4060, 82, 293, 5285, 82, 11, 597, 307, 300, 436, 645, 1228, 50924, 50924, 17434, 22027, 13, 400, 370, 436, 994, 380, 519, 295, 264, 1379, 1558, 295, 2771, 2448, 570, 295, 300, 13, 51212, 51328, 3950, 661, 6828, 366, 13752, 1143, 294, 264, 4319, 295, 18161, 36170, 420, 412, 1935, 337, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.07771649305847869, "compression_ratio": 1.6053811659192825, "no_speech_prob": 1.2797117960872129e-05}, {"id": 91, "seek": 59048, "start": 609.76, "end": 616.8000000000001, "text": " Those other functions are rarely used in the context of neural nets or at least for", "tokens": [50364, 307, 588, 13752, 1143, 570, 291, 393, 380, 534, 48256, 16235, 807, 309, 13, 400, 341, 307, 50652, 50652, 534, 437, 4305, 561, 490, 7962, 278, 646, 79, 1513, 294, 264, 4060, 82, 293, 5285, 82, 11, 597, 307, 300, 436, 645, 1228, 50924, 50924, 17434, 22027, 13, 400, 370, 436, 994, 380, 519, 295, 264, 1379, 1558, 295, 2771, 2448, 570, 295, 300, 13, 51212, 51328, 3950, 661, 6828, 366, 13752, 1143, 294, 264, 4319, 295, 18161, 36170, 420, 412, 1935, 337, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.07771649305847869, "compression_ratio": 1.6053811659192825, "no_speech_prob": 1.2797117960872129e-05}, {"id": 92, "seek": 61680, "start": 616.8, "end": 621.12, "text": " activation function in a traditional neural net. They're used mostly for", "tokens": [50364, 24433, 2445, 294, 257, 5164, 18161, 2533, 13, 814, 434, 1143, 5240, 337, 50580, 50652, 2171, 337, 721, 411, 637, 11668, 17720, 13, 407, 472, 1823, 294, 637, 11668, 17720, 14689, 294, 50880, 50912, 281, 14722, 264, 2158, 295, 264, 48994, 7006, 11, 14689, 294, 41684, 439, 264, 4190, 294, 264, 51168, 51168, 48994, 7006, 11, 294, 264, 48994, 8062, 538, 512, 2158, 13, 400, 291, 360, 341, 365, 257, 23060, 2445, 11, 51408, 51408, 257, 23060, 609, 2445, 13, 639, 307, 733, 295, 257, 2787, 3037, 295, 257, 23060, 609, 2445, 13, 440, 1152, 3037, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.12127906799316407, "compression_ratio": 1.959090909090909, "no_speech_prob": 5.337751645129174e-06}, {"id": 93, "seek": 61680, "start": 622.56, "end": 627.12, "text": " sometimes for things like sparse coding. So one step in sparse coding consists in", "tokens": [50364, 24433, 2445, 294, 257, 5164, 18161, 2533, 13, 814, 434, 1143, 5240, 337, 50580, 50652, 2171, 337, 721, 411, 637, 11668, 17720, 13, 407, 472, 1823, 294, 637, 11668, 17720, 14689, 294, 50880, 50912, 281, 14722, 264, 2158, 295, 264, 48994, 7006, 11, 14689, 294, 41684, 439, 264, 4190, 294, 264, 51168, 51168, 48994, 7006, 11, 294, 264, 48994, 8062, 538, 512, 2158, 13, 400, 291, 360, 341, 365, 257, 23060, 2445, 11, 51408, 51408, 257, 23060, 609, 2445, 13, 639, 307, 733, 295, 257, 2787, 3037, 295, 257, 23060, 609, 2445, 13, 440, 1152, 3037, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.12127906799316407, "compression_ratio": 1.959090909090909, "no_speech_prob": 5.337751645129174e-06}, {"id": 94, "seek": 61680, "start": 627.76, "end": 632.88, "text": " to compute the value of the latent variable, consists in shrinking all the values in the", "tokens": [50364, 24433, 2445, 294, 257, 5164, 18161, 2533, 13, 814, 434, 1143, 5240, 337, 50580, 50652, 2171, 337, 721, 411, 637, 11668, 17720, 13, 407, 472, 1823, 294, 637, 11668, 17720, 14689, 294, 50880, 50912, 281, 14722, 264, 2158, 295, 264, 48994, 7006, 11, 14689, 294, 41684, 439, 264, 4190, 294, 264, 51168, 51168, 48994, 7006, 11, 294, 264, 48994, 8062, 538, 512, 2158, 13, 400, 291, 360, 341, 365, 257, 23060, 2445, 11, 51408, 51408, 257, 23060, 609, 2445, 13, 639, 307, 733, 295, 257, 2787, 3037, 295, 257, 23060, 609, 2445, 13, 440, 1152, 3037, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.12127906799316407, "compression_ratio": 1.959090909090909, "no_speech_prob": 5.337751645129174e-06}, {"id": 95, "seek": 61680, "start": 632.88, "end": 637.68, "text": " latent variable, in the latent vector by some value. And you do this with a shrink function,", "tokens": [50364, 24433, 2445, 294, 257, 5164, 18161, 2533, 13, 814, 434, 1143, 5240, 337, 50580, 50652, 2171, 337, 721, 411, 637, 11668, 17720, 13, 407, 472, 1823, 294, 637, 11668, 17720, 14689, 294, 50880, 50912, 281, 14722, 264, 2158, 295, 264, 48994, 7006, 11, 14689, 294, 41684, 439, 264, 4190, 294, 264, 51168, 51168, 48994, 7006, 11, 294, 264, 48994, 8062, 538, 512, 2158, 13, 400, 291, 360, 341, 365, 257, 23060, 2445, 11, 51408, 51408, 257, 23060, 609, 2445, 13, 639, 307, 733, 295, 257, 2787, 3037, 295, 257, 23060, 609, 2445, 13, 440, 1152, 3037, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.12127906799316407, "compression_ratio": 1.959090909090909, "no_speech_prob": 5.337751645129174e-06}, {"id": 96, "seek": 61680, "start": 637.68, "end": 641.92, "text": " a shrinkage function. This is kind of a soft version of a shrinkage function. The hard version", "tokens": [50364, 24433, 2445, 294, 257, 5164, 18161, 2533, 13, 814, 434, 1143, 5240, 337, 50580, 50652, 2171, 337, 721, 411, 637, 11668, 17720, 13, 407, 472, 1823, 294, 637, 11668, 17720, 14689, 294, 50880, 50912, 281, 14722, 264, 2158, 295, 264, 48994, 7006, 11, 14689, 294, 41684, 439, 264, 4190, 294, 264, 51168, 51168, 48994, 7006, 11, 294, 264, 48994, 8062, 538, 512, 2158, 13, 400, 291, 360, 341, 365, 257, 23060, 2445, 11, 51408, 51408, 257, 23060, 609, 2445, 13, 639, 307, 733, 295, 257, 2787, 3037, 295, 257, 23060, 609, 2445, 13, 440, 1152, 3037, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.12127906799316407, "compression_ratio": 1.959090909090909, "no_speech_prob": 5.337751645129174e-06}, {"id": 97, "seek": 64192, "start": 641.92, "end": 650.0, "text": " is here. I mean, it's called soft shrink, but it actually has corners in it. The reason it's", "tokens": [50364, 307, 510, 13, 286, 914, 11, 309, 311, 1219, 2787, 23060, 11, 457, 309, 767, 575, 12413, 294, 309, 13, 440, 1778, 309, 311, 50768, 50768, 1219, 2787, 23060, 307, 570, 456, 307, 257, 1152, 23060, 300, 1542, 819, 300, 286, 603, 855, 291, 50960, 50960, 294, 257, 3456, 13, 407, 341, 1936, 445, 2962, 257, 7006, 538, 257, 5754, 3030, 4018, 13, 400, 498, 51436, 51436], "temperature": 0.0, "avg_logprob": -0.09079874425694562, "compression_ratio": 1.5449438202247192, "no_speech_prob": 1.7061580592780956e-06}, {"id": 98, "seek": 64192, "start": 650.0, "end": 653.8399999999999, "text": " called soft shrink is because there is a hard shrink that looks different that I'll show you", "tokens": [50364, 307, 510, 13, 286, 914, 11, 309, 311, 1219, 2787, 23060, 11, 457, 309, 767, 575, 12413, 294, 309, 13, 440, 1778, 309, 311, 50768, 50768, 1219, 2787, 23060, 307, 570, 456, 307, 257, 1152, 23060, 300, 1542, 819, 300, 286, 603, 855, 291, 50960, 50960, 294, 257, 3456, 13, 407, 341, 1936, 445, 2962, 257, 7006, 538, 257, 5754, 3030, 4018, 13, 400, 498, 51436, 51436], "temperature": 0.0, "avg_logprob": -0.09079874425694562, "compression_ratio": 1.5449438202247192, "no_speech_prob": 1.7061580592780956e-06}, {"id": 99, "seek": 64192, "start": 653.8399999999999, "end": 663.36, "text": " in a minute. So this basically just changes a variable by a constant towards zero. And if", "tokens": [50364, 307, 510, 13, 286, 914, 11, 309, 311, 1219, 2787, 23060, 11, 457, 309, 767, 575, 12413, 294, 309, 13, 440, 1778, 309, 311, 50768, 50768, 1219, 2787, 23060, 307, 570, 456, 307, 257, 1152, 23060, 300, 1542, 819, 300, 286, 603, 855, 291, 50960, 50960, 294, 257, 3456, 13, 407, 341, 1936, 445, 2962, 257, 7006, 538, 257, 5754, 3030, 4018, 13, 400, 498, 51436, 51436], "temperature": 0.0, "avg_logprob": -0.09079874425694562, "compression_ratio": 1.5449438202247192, "no_speech_prob": 1.7061580592780956e-06}, {"id": 100, "seek": 66336, "start": 663.36, "end": 677.12, "text": " it goes below zero, it's clumped at zero if it's brought too long. And so this is basically just", "tokens": [50364, 309, 1709, 2507, 4018, 11, 309, 311, 596, 1420, 292, 412, 4018, 498, 309, 311, 3038, 886, 938, 13, 400, 370, 341, 307, 1936, 445, 51052, 51052, 264, 6575, 2445, 281, 597, 291, 16390, 9848, 65, 7940, 27747, 281, 652, 309, 574, 411, 257, 23060, 13, 51264, 51364, 8537, 11, 498, 321, 853, 281, 483, 2035, 2158, 1998, 281, 4018, 11, 436, 767, 366, 7579, 281, 4018, 11, 1936, 13, 51652, 51704], "temperature": 0.0, "avg_logprob": -0.2056449168437236, "compression_ratio": 1.5340314136125655, "no_speech_prob": 8.799523129709996e-06}, {"id": 101, "seek": 66336, "start": 677.12, "end": 681.36, "text": " the identity function to which you subtract hyperbolic tangent to make it look like a shrink.", "tokens": [50364, 309, 1709, 2507, 4018, 11, 309, 311, 596, 1420, 292, 412, 4018, 498, 309, 311, 3038, 886, 938, 13, 400, 370, 341, 307, 1936, 445, 51052, 51052, 264, 6575, 2445, 281, 597, 291, 16390, 9848, 65, 7940, 27747, 281, 652, 309, 574, 411, 257, 23060, 13, 51264, 51364, 8537, 11, 498, 321, 853, 281, 483, 2035, 2158, 1998, 281, 4018, 11, 436, 767, 366, 7579, 281, 4018, 11, 1936, 13, 51652, 51704], "temperature": 0.0, "avg_logprob": -0.2056449168437236, "compression_ratio": 1.5340314136125655, "no_speech_prob": 8.799523129709996e-06}, {"id": 102, "seek": 66336, "start": 683.36, "end": 689.12, "text": " Basically, if we try to get whatever value close to zero, they actually are forced to zero, basically.", "tokens": [50364, 309, 1709, 2507, 4018, 11, 309, 311, 596, 1420, 292, 412, 4018, 498, 309, 311, 3038, 886, 938, 13, 400, 370, 341, 307, 1936, 445, 51052, 51052, 264, 6575, 2445, 281, 597, 291, 16390, 9848, 65, 7940, 27747, 281, 652, 309, 574, 411, 257, 23060, 13, 51264, 51364, 8537, 11, 498, 321, 853, 281, 483, 2035, 2158, 1998, 281, 4018, 11, 436, 767, 366, 7579, 281, 4018, 11, 1936, 13, 51652, 51704], "temperature": 0.0, "avg_logprob": -0.2056449168437236, "compression_ratio": 1.5340314136125655, "no_speech_prob": 8.799523129709996e-06}, {"id": 103, "seek": 68912, "start": 689.12, "end": 695.12, "text": " Right. So small values are forced to zero. Others are shrunk towards zero. But since they're", "tokens": [50364, 1779, 13, 407, 1359, 4190, 366, 7579, 281, 4018, 13, 20277, 366, 9884, 3197, 3030, 4018, 13, 583, 1670, 436, 434, 50664, 50664, 2416, 1547, 11, 436, 434, 406, 516, 281, 483, 281, 4018, 13, 407, 797, 11, 300, 311, 1143, 5240, 382, 291, 393, 519, 295, 51012, 51012, 309, 382, 257, 1823, 295, 16235, 337, 364, 441, 16, 46691, 13, 407, 498, 291, 362, 257, 7006, 11, 291, 362, 364, 441, 16, 2063, 2445, 51380, 51380, 322, 309, 13, 400, 291, 747, 257, 1823, 294, 257, 3671, 16235, 295, 264, 441, 16, 13, 407, 441, 16, 2063, 307, 364, 8236, 2158, 13, 51664, 51700], "temperature": 0.0, "avg_logprob": -0.10920019238908714, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.962087354622781e-06}, {"id": 104, "seek": 68912, "start": 695.12, "end": 702.08, "text": " large enough, they're not going to get to zero. So again, that's used mostly as you can think of", "tokens": [50364, 1779, 13, 407, 1359, 4190, 366, 7579, 281, 4018, 13, 20277, 366, 9884, 3197, 3030, 4018, 13, 583, 1670, 436, 434, 50664, 50664, 2416, 1547, 11, 436, 434, 406, 516, 281, 483, 281, 4018, 13, 407, 797, 11, 300, 311, 1143, 5240, 382, 291, 393, 519, 295, 51012, 51012, 309, 382, 257, 1823, 295, 16235, 337, 364, 441, 16, 46691, 13, 407, 498, 291, 362, 257, 7006, 11, 291, 362, 364, 441, 16, 2063, 2445, 51380, 51380, 322, 309, 13, 400, 291, 747, 257, 1823, 294, 257, 3671, 16235, 295, 264, 441, 16, 13, 407, 441, 16, 2063, 307, 364, 8236, 2158, 13, 51664, 51700], "temperature": 0.0, "avg_logprob": -0.10920019238908714, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.962087354622781e-06}, {"id": 105, "seek": 68912, "start": 702.08, "end": 709.44, "text": " it as a step of gradient for an L1 criterion. So if you have a variable, you have an L1 cost function", "tokens": [50364, 1779, 13, 407, 1359, 4190, 366, 7579, 281, 4018, 13, 20277, 366, 9884, 3197, 3030, 4018, 13, 583, 1670, 436, 434, 50664, 50664, 2416, 1547, 11, 436, 434, 406, 516, 281, 483, 281, 4018, 13, 407, 797, 11, 300, 311, 1143, 5240, 382, 291, 393, 519, 295, 51012, 51012, 309, 382, 257, 1823, 295, 16235, 337, 364, 441, 16, 46691, 13, 407, 498, 291, 362, 257, 7006, 11, 291, 362, 364, 441, 16, 2063, 2445, 51380, 51380, 322, 309, 13, 400, 291, 747, 257, 1823, 294, 257, 3671, 16235, 295, 264, 441, 16, 13, 407, 441, 16, 2063, 307, 364, 8236, 2158, 13, 51664, 51700], "temperature": 0.0, "avg_logprob": -0.10920019238908714, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.962087354622781e-06}, {"id": 106, "seek": 68912, "start": 709.44, "end": 715.12, "text": " on it. And you take a step in a negative gradient of the L1. So L1 cost is an absolute value.", "tokens": [50364, 1779, 13, 407, 1359, 4190, 366, 7579, 281, 4018, 13, 20277, 366, 9884, 3197, 3030, 4018, 13, 583, 1670, 436, 434, 50664, 50664, 2416, 1547, 11, 436, 434, 406, 516, 281, 483, 281, 4018, 13, 407, 797, 11, 300, 311, 1143, 5240, 382, 291, 393, 519, 295, 51012, 51012, 309, 382, 257, 1823, 295, 16235, 337, 364, 441, 16, 46691, 13, 407, 498, 291, 362, 257, 7006, 11, 291, 362, 364, 441, 16, 2063, 2445, 51380, 51380, 322, 309, 13, 400, 291, 747, 257, 1823, 294, 257, 3671, 16235, 295, 264, 441, 16, 13, 407, 441, 16, 2063, 307, 364, 8236, 2158, 13, 51664, 51700], "temperature": 0.0, "avg_logprob": -0.10920019238908714, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.962087354622781e-06}, {"id": 107, "seek": 71512, "start": 715.12, "end": 721.44, "text": " This will cause the variable to go towards zero by a constant, which is the slope of that L1 criterion.", "tokens": [50364, 639, 486, 3082, 264, 7006, 281, 352, 3030, 4018, 538, 257, 5754, 11, 597, 307, 264, 13525, 295, 300, 441, 16, 46691, 13, 50680, 50680, 400, 281, 1754, 412, 4018, 1348, 490, 2139, 1252, 11, 309, 1177, 380, 15488, 24467, 498, 291, 528, 13, 400, 370, 300, 311, 51076, 51076, 264, 2107, 12, 28263, 2445, 291, 764, 13, 400, 300, 311, 472, 295, 264, 4439, 294, 264, 6205, 8241, 9284, 300, 307, 1143, 51300, 51328, 337, 38253, 294, 637, 11668, 17720, 13, 583, 797, 11, 309, 311, 13752, 1143, 294, 3890, 18161, 36170, 13, 16581, 291, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.2572429084777832, "compression_ratio": 1.579591836734694, "no_speech_prob": 4.495020220929291e-06}, {"id": 108, "seek": 71512, "start": 721.44, "end": 729.36, "text": " And to stay at zero coming from either side, it doesn't overshoot if you want. And so that's", "tokens": [50364, 639, 486, 3082, 264, 7006, 281, 352, 3030, 4018, 538, 257, 5754, 11, 597, 307, 264, 13525, 295, 300, 441, 16, 46691, 13, 50680, 50680, 400, 281, 1754, 412, 4018, 1348, 490, 2139, 1252, 11, 309, 1177, 380, 15488, 24467, 498, 291, 528, 13, 400, 370, 300, 311, 51076, 51076, 264, 2107, 12, 28263, 2445, 291, 764, 13, 400, 300, 311, 472, 295, 264, 4439, 294, 264, 6205, 8241, 9284, 300, 307, 1143, 51300, 51328, 337, 38253, 294, 637, 11668, 17720, 13, 583, 797, 11, 309, 311, 13752, 1143, 294, 3890, 18161, 36170, 13, 16581, 291, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.2572429084777832, "compression_ratio": 1.579591836734694, "no_speech_prob": 4.495020220929291e-06}, {"id": 109, "seek": 71512, "start": 729.36, "end": 733.84, "text": " the non-linear function you use. And that's one of the steps in the ISTA algorithm that is used", "tokens": [50364, 639, 486, 3082, 264, 7006, 281, 352, 3030, 4018, 538, 257, 5754, 11, 597, 307, 264, 13525, 295, 300, 441, 16, 46691, 13, 50680, 50680, 400, 281, 1754, 412, 4018, 1348, 490, 2139, 1252, 11, 309, 1177, 380, 15488, 24467, 498, 291, 528, 13, 400, 370, 300, 311, 51076, 51076, 264, 2107, 12, 28263, 2445, 291, 764, 13, 400, 300, 311, 472, 295, 264, 4439, 294, 264, 6205, 8241, 9284, 300, 307, 1143, 51300, 51328, 337, 38253, 294, 637, 11668, 17720, 13, 583, 797, 11, 309, 311, 13752, 1143, 294, 3890, 18161, 36170, 13, 16581, 291, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.2572429084777832, "compression_ratio": 1.579591836734694, "no_speech_prob": 4.495020220929291e-06}, {"id": 110, "seek": 71512, "start": 734.4, "end": 743.84, "text": " for inference in sparse coding. But again, it's rarely used in regular neural nets. Unless you", "tokens": [50364, 639, 486, 3082, 264, 7006, 281, 352, 3030, 4018, 538, 257, 5754, 11, 597, 307, 264, 13525, 295, 300, 441, 16, 46691, 13, 50680, 50680, 400, 281, 1754, 412, 4018, 1348, 490, 2139, 1252, 11, 309, 1177, 380, 15488, 24467, 498, 291, 528, 13, 400, 370, 300, 311, 51076, 51076, 264, 2107, 12, 28263, 2445, 291, 764, 13, 400, 300, 311, 472, 295, 264, 4439, 294, 264, 6205, 8241, 9284, 300, 307, 1143, 51300, 51328, 337, 38253, 294, 637, 11668, 17720, 13, 583, 797, 11, 309, 311, 13752, 1143, 294, 3890, 18161, 36170, 13, 16581, 291, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.2572429084777832, "compression_ratio": 1.579591836734694, "no_speech_prob": 4.495020220929291e-06}, {"id": 111, "seek": 74384, "start": 743.84, "end": 755.2800000000001, "text": " encoder is kind of used as kind of an estimation of sparse coding. This is the hard shrink. So hard", "tokens": [50364, 2058, 19866, 307, 733, 295, 1143, 382, 733, 295, 364, 35701, 295, 637, 11668, 17720, 13, 639, 307, 264, 1152, 23060, 13, 407, 1152, 50936, 50936, 23060, 1936, 44423, 633, 2158, 4356, 813, 13607, 281, 4018, 13, 407, 498, 257, 2158, 307, 4356, 813, 51420, 51420, 13607, 420, 4833, 813, 3175, 13607, 11, 309, 311, 1333, 295, 1296, 3175, 13607, 293, 13607, 13, 1133, 13607, 307, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.21131824765886578, "compression_ratio": 1.7425149700598803, "no_speech_prob": 7.0717042035539635e-06}, {"id": 112, "seek": 74384, "start": 755.2800000000001, "end": 764.96, "text": " shrink basically clamps every value smaller than lambda to zero. So if a value is smaller than", "tokens": [50364, 2058, 19866, 307, 733, 295, 1143, 382, 733, 295, 364, 35701, 295, 637, 11668, 17720, 13, 639, 307, 264, 1152, 23060, 13, 407, 1152, 50936, 50936, 23060, 1936, 44423, 633, 2158, 4356, 813, 13607, 281, 4018, 13, 407, 498, 257, 2158, 307, 4356, 813, 51420, 51420, 13607, 420, 4833, 813, 3175, 13607, 11, 309, 311, 1333, 295, 1296, 3175, 13607, 293, 13607, 13, 1133, 13607, 307, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.21131824765886578, "compression_ratio": 1.7425149700598803, "no_speech_prob": 7.0717042035539635e-06}, {"id": 113, "seek": 74384, "start": 764.96, "end": 769.76, "text": " lambda or larger than minus lambda, it's sort of between minus lambda and lambda. When lambda is", "tokens": [50364, 2058, 19866, 307, 733, 295, 1143, 382, 733, 295, 364, 35701, 295, 637, 11668, 17720, 13, 639, 307, 264, 1152, 23060, 13, 407, 1152, 50936, 50936, 23060, 1936, 44423, 633, 2158, 4356, 813, 13607, 281, 4018, 13, 407, 498, 257, 2158, 307, 4356, 813, 51420, 51420, 13607, 420, 4833, 813, 3175, 13607, 11, 309, 311, 1333, 295, 1296, 3175, 13607, 293, 13607, 13, 1133, 13607, 307, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.21131824765886578, "compression_ratio": 1.7425149700598803, "no_speech_prob": 7.0717042035539635e-06}, {"id": 114, "seek": 76976, "start": 769.76, "end": 777.12, "text": " some constant, you just set it to zero. Again, it's used for things like certain types of", "tokens": [50364, 512, 5754, 11, 291, 445, 992, 309, 281, 4018, 13, 3764, 11, 309, 311, 1143, 337, 721, 411, 1629, 3467, 295, 50732, 50732, 637, 11668, 17720, 11, 457, 13752, 382, 364, 24433, 2445, 294, 264, 18161, 2533, 13, 50888, 51008, 407, 257, 3565, 12, 405, 80, 4391, 307, 5240, 1143, 294, 2063, 6828, 13, 1726, 534, 382, 364, 24433, 2445, 2139, 13, 583, 51276, 51308, 309, 311, 257, 4420, 2445, 281, 362, 498, 291, 528, 281, 5452, 341, 666, 257, 4470, 2445, 13, 400, 321, 603, 536, 300, 294, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1846995815154045, "compression_ratio": 1.663594470046083, "no_speech_prob": 1.4144438864605036e-06}, {"id": 115, "seek": 76976, "start": 777.12, "end": 780.24, "text": " sparse coding, but rarely as an activation function in the neural net.", "tokens": [50364, 512, 5754, 11, 291, 445, 992, 309, 281, 4018, 13, 3764, 11, 309, 311, 1143, 337, 721, 411, 1629, 3467, 295, 50732, 50732, 637, 11668, 17720, 11, 457, 13752, 382, 364, 24433, 2445, 294, 264, 18161, 2533, 13, 50888, 51008, 407, 257, 3565, 12, 405, 80, 4391, 307, 5240, 1143, 294, 2063, 6828, 13, 1726, 534, 382, 364, 24433, 2445, 2139, 13, 583, 51276, 51308, 309, 311, 257, 4420, 2445, 281, 362, 498, 291, 528, 281, 5452, 341, 666, 257, 4470, 2445, 13, 400, 321, 603, 536, 300, 294, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1846995815154045, "compression_ratio": 1.663594470046083, "no_speech_prob": 1.4144438864605036e-06}, {"id": 116, "seek": 76976, "start": 782.64, "end": 788.0, "text": " So a log-seq mode is mostly used in cost functions. Not really as an activation function either. But", "tokens": [50364, 512, 5754, 11, 291, 445, 992, 309, 281, 4018, 13, 3764, 11, 309, 311, 1143, 337, 721, 411, 1629, 3467, 295, 50732, 50732, 637, 11668, 17720, 11, 457, 13752, 382, 364, 24433, 2445, 294, 264, 18161, 2533, 13, 50888, 51008, 407, 257, 3565, 12, 405, 80, 4391, 307, 5240, 1143, 294, 2063, 6828, 13, 1726, 534, 382, 364, 24433, 2445, 2139, 13, 583, 51276, 51308, 309, 311, 257, 4420, 2445, 281, 362, 498, 291, 528, 281, 5452, 341, 666, 257, 4470, 2445, 13, 400, 321, 603, 536, 300, 294, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1846995815154045, "compression_ratio": 1.663594470046083, "no_speech_prob": 1.4144438864605036e-06}, {"id": 117, "seek": 76976, "start": 788.64, "end": 796.96, "text": " it's a useful function to have if you want to plug this into a loss function. And we'll see that in", "tokens": [50364, 512, 5754, 11, 291, 445, 992, 309, 281, 4018, 13, 3764, 11, 309, 311, 1143, 337, 721, 411, 1629, 3467, 295, 50732, 50732, 637, 11668, 17720, 11, 457, 13752, 382, 364, 24433, 2445, 294, 264, 18161, 2533, 13, 50888, 51008, 407, 257, 3565, 12, 405, 80, 4391, 307, 5240, 1143, 294, 2063, 6828, 13, 1726, 534, 382, 364, 24433, 2445, 2139, 13, 583, 51276, 51308, 309, 311, 257, 4420, 2445, 281, 362, 498, 291, 528, 281, 5452, 341, 666, 257, 4470, 2445, 13, 400, 321, 603, 536, 300, 294, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1846995815154045, "compression_ratio": 1.663594470046083, "no_speech_prob": 1.4144438864605036e-06}, {"id": 118, "seek": 79696, "start": 796.96, "end": 803.2800000000001, "text": " a minute. So something we've seen, this is the same as softmax, except you have minus signs.", "tokens": [50364, 257, 3456, 13, 407, 746, 321, 600, 1612, 11, 341, 307, 264, 912, 382, 2787, 41167, 11, 3993, 291, 362, 3175, 7880, 13, 50680, 50880, 407, 729, 366, 2120, 327, 332, 11075, 2107, 12, 28263, 1088, 13, 509, 362, 257, 8062, 294, 293, 291, 483, 257, 8062, 484, 11, 51128, 51128, 597, 307, 264, 912, 2744, 382, 264, 4846, 8062, 13, 400, 321, 458, 300, 2787, 41167, 307, 21510, 36800, 6666, 51472, 51472, 538, 2408, 670, 361, 295, 21510, 2031, 73, 13, 639, 307, 2787, 2367, 11, 689, 291, 829, 264, 3175, 1465, 294, 1868, 295, 264, 2031, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.11427278426087019, "compression_ratio": 1.6919642857142858, "no_speech_prob": 2.8408579964889213e-05}, {"id": 119, "seek": 79696, "start": 807.2800000000001, "end": 812.24, "text": " So those are multidimensional non-linearities. You have a vector in and you get a vector out,", "tokens": [50364, 257, 3456, 13, 407, 746, 321, 600, 1612, 11, 341, 307, 264, 912, 382, 2787, 41167, 11, 3993, 291, 362, 3175, 7880, 13, 50680, 50880, 407, 729, 366, 2120, 327, 332, 11075, 2107, 12, 28263, 1088, 13, 509, 362, 257, 8062, 294, 293, 291, 483, 257, 8062, 484, 11, 51128, 51128, 597, 307, 264, 912, 2744, 382, 264, 4846, 8062, 13, 400, 321, 458, 300, 2787, 41167, 307, 21510, 36800, 6666, 51472, 51472, 538, 2408, 670, 361, 295, 21510, 2031, 73, 13, 639, 307, 2787, 2367, 11, 689, 291, 829, 264, 3175, 1465, 294, 1868, 295, 264, 2031, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.11427278426087019, "compression_ratio": 1.6919642857142858, "no_speech_prob": 2.8408579964889213e-05}, {"id": 120, "seek": 79696, "start": 812.24, "end": 819.12, "text": " which is the same size as the input vector. And we know that softmax is exponential xi divided", "tokens": [50364, 257, 3456, 13, 407, 746, 321, 600, 1612, 11, 341, 307, 264, 912, 382, 2787, 41167, 11, 3993, 291, 362, 3175, 7880, 13, 50680, 50880, 407, 729, 366, 2120, 327, 332, 11075, 2107, 12, 28263, 1088, 13, 509, 362, 257, 8062, 294, 293, 291, 483, 257, 8062, 484, 11, 51128, 51128, 597, 307, 264, 912, 2744, 382, 264, 4846, 8062, 13, 400, 321, 458, 300, 2787, 41167, 307, 21510, 36800, 6666, 51472, 51472, 538, 2408, 670, 361, 295, 21510, 2031, 73, 13, 639, 307, 2787, 2367, 11, 689, 291, 829, 264, 3175, 1465, 294, 1868, 295, 264, 2031, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.11427278426087019, "compression_ratio": 1.6919642857142858, "no_speech_prob": 2.8408579964889213e-05}, {"id": 121, "seek": 79696, "start": 819.12, "end": 825.2, "text": " by sum over j of exponential xj. This is softmin, where you put the minus sign in front of the x.", "tokens": [50364, 257, 3456, 13, 407, 746, 321, 600, 1612, 11, 341, 307, 264, 912, 382, 2787, 41167, 11, 3993, 291, 362, 3175, 7880, 13, 50680, 50880, 407, 729, 366, 2120, 327, 332, 11075, 2107, 12, 28263, 1088, 13, 509, 362, 257, 8062, 294, 293, 291, 483, 257, 8062, 484, 11, 51128, 51128, 597, 307, 264, 912, 2744, 382, 264, 4846, 8062, 13, 400, 321, 458, 300, 2787, 41167, 307, 21510, 36800, 6666, 51472, 51472, 538, 2408, 670, 361, 295, 21510, 2031, 73, 13, 639, 307, 2787, 2367, 11, 689, 291, 829, 264, 3175, 1465, 294, 1868, 295, 264, 2031, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.11427278426087019, "compression_ratio": 1.6919642857142858, "no_speech_prob": 2.8408579964889213e-05}, {"id": 122, "seek": 82520, "start": 825.2, "end": 830.4000000000001, "text": " So you view the x's if you want as energies instead of scores, as penalties instead of scores.", "tokens": [50364, 407, 291, 1910, 264, 2031, 311, 498, 291, 528, 382, 25737, 2602, 295, 13444, 11, 382, 35389, 2602, 295, 13444, 13, 50624, 50728, 400, 309, 311, 257, 665, 636, 295, 6246, 257, 3840, 295, 3547, 281, 746, 300, 1542, 257, 857, 411, 257, 8482, 51048, 51048, 7316, 11, 597, 1355, 3547, 1296, 4018, 293, 472, 300, 2408, 281, 472, 13, 400, 300, 311, 264, 2787, 41167, 11, 51376, 51376, 597, 321, 439, 458, 13, 407, 3565, 12, 13908, 41167, 11, 797, 11, 307, 406, 588, 709, 1143, 382, 257, 2107, 12, 1889, 17409, 1951, 264, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.10778319233595723, "compression_ratio": 1.658008658008658, "no_speech_prob": 7.645448931725696e-06}, {"id": 123, "seek": 82520, "start": 832.48, "end": 838.88, "text": " And it's a good way of turning a bunch of numbers to something that looks a bit like a probability", "tokens": [50364, 407, 291, 1910, 264, 2031, 311, 498, 291, 528, 382, 25737, 2602, 295, 13444, 11, 382, 35389, 2602, 295, 13444, 13, 50624, 50728, 400, 309, 311, 257, 665, 636, 295, 6246, 257, 3840, 295, 3547, 281, 746, 300, 1542, 257, 857, 411, 257, 8482, 51048, 51048, 7316, 11, 597, 1355, 3547, 1296, 4018, 293, 472, 300, 2408, 281, 472, 13, 400, 300, 311, 264, 2787, 41167, 11, 51376, 51376, 597, 321, 439, 458, 13, 407, 3565, 12, 13908, 41167, 11, 797, 11, 307, 406, 588, 709, 1143, 382, 257, 2107, 12, 1889, 17409, 1951, 264, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.10778319233595723, "compression_ratio": 1.658008658008658, "no_speech_prob": 7.645448931725696e-06}, {"id": 124, "seek": 82520, "start": 838.88, "end": 845.44, "text": " distribution, which means numbers between zero and one that sum to one. And that's the softmax,", "tokens": [50364, 407, 291, 1910, 264, 2031, 311, 498, 291, 528, 382, 25737, 2602, 295, 13444, 11, 382, 35389, 2602, 295, 13444, 13, 50624, 50728, 400, 309, 311, 257, 665, 636, 295, 6246, 257, 3840, 295, 3547, 281, 746, 300, 1542, 257, 857, 411, 257, 8482, 51048, 51048, 7316, 11, 597, 1355, 3547, 1296, 4018, 293, 472, 300, 2408, 281, 472, 13, 400, 300, 311, 264, 2787, 41167, 11, 51376, 51376, 597, 321, 439, 458, 13, 407, 3565, 12, 13908, 41167, 11, 797, 11, 307, 406, 588, 709, 1143, 382, 257, 2107, 12, 1889, 17409, 1951, 264, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.10778319233595723, "compression_ratio": 1.658008658008658, "no_speech_prob": 7.645448931725696e-06}, {"id": 125, "seek": 82520, "start": 845.44, "end": 853.0400000000001, "text": " which we all know. So log-softmax, again, is not very much used as a non-linearity within the", "tokens": [50364, 407, 291, 1910, 264, 2031, 311, 498, 291, 528, 382, 25737, 2602, 295, 13444, 11, 382, 35389, 2602, 295, 13444, 13, 50624, 50728, 400, 309, 311, 257, 665, 636, 295, 6246, 257, 3840, 295, 3547, 281, 746, 300, 1542, 257, 857, 411, 257, 8482, 51048, 51048, 7316, 11, 597, 1355, 3547, 1296, 4018, 293, 472, 300, 2408, 281, 472, 13, 400, 300, 311, 264, 2787, 41167, 11, 51376, 51376, 597, 321, 439, 458, 13, 407, 3565, 12, 13908, 41167, 11, 797, 11, 307, 406, 588, 709, 1143, 382, 257, 2107, 12, 1889, 17409, 1951, 264, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.10778319233595723, "compression_ratio": 1.658008658008658, "no_speech_prob": 7.645448931725696e-06}, {"id": 126, "seek": 85304, "start": 853.04, "end": 858.4, "text": " neural net, but it's used a lot at the output as kind of one piece of a loss function. And we'll", "tokens": [50364, 18161, 2533, 11, 457, 309, 311, 1143, 257, 688, 412, 264, 5598, 382, 733, 295, 472, 2522, 295, 257, 4470, 2445, 13, 400, 321, 603, 50632, 50632, 536, 341, 294, 257, 3456, 13, 2264, 11, 370, 729, 1651, 13, 492, 362, 257, 1168, 13, 407, 337, 659, 2907, 11, 286, 478, 406, 988, 51052, 51052, 286, 1223, 1230, 472, 11, 983, 321, 528, 264, 912, 2158, 337, 439, 9235, 13, 400, 1230, 732, 11, 51336, 51336, 577, 2539, 316, 576, 767, 312, 5002, 563, 13, 509, 727, 362, 257, 819, 316, 337, 819, 9235, 13, 51644, 51676], "temperature": 0.0, "avg_logprob": -0.13076894933527167, "compression_ratio": 1.5774058577405858, "no_speech_prob": 1.184119264507899e-05}, {"id": 127, "seek": 85304, "start": 858.4, "end": 866.8, "text": " see this in a minute. OK, so those questions. We have a question. So for preload, I'm not sure", "tokens": [50364, 18161, 2533, 11, 457, 309, 311, 1143, 257, 688, 412, 264, 5598, 382, 733, 295, 472, 2522, 295, 257, 4470, 2445, 13, 400, 321, 603, 50632, 50632, 536, 341, 294, 257, 3456, 13, 2264, 11, 370, 729, 1651, 13, 492, 362, 257, 1168, 13, 407, 337, 659, 2907, 11, 286, 478, 406, 988, 51052, 51052, 286, 1223, 1230, 472, 11, 983, 321, 528, 264, 912, 2158, 337, 439, 9235, 13, 400, 1230, 732, 11, 51336, 51336, 577, 2539, 316, 576, 767, 312, 5002, 563, 13, 509, 727, 362, 257, 819, 316, 337, 819, 9235, 13, 51644, 51676], "temperature": 0.0, "avg_logprob": -0.13076894933527167, "compression_ratio": 1.5774058577405858, "no_speech_prob": 1.184119264507899e-05}, {"id": 128, "seek": 85304, "start": 866.8, "end": 872.48, "text": " I understand number one, why we want the same value for all channels. And number two,", "tokens": [50364, 18161, 2533, 11, 457, 309, 311, 1143, 257, 688, 412, 264, 5598, 382, 733, 295, 472, 2522, 295, 257, 4470, 2445, 13, 400, 321, 603, 50632, 50632, 536, 341, 294, 257, 3456, 13, 2264, 11, 370, 729, 1651, 13, 492, 362, 257, 1168, 13, 407, 337, 659, 2907, 11, 286, 478, 406, 988, 51052, 51052, 286, 1223, 1230, 472, 11, 983, 321, 528, 264, 912, 2158, 337, 439, 9235, 13, 400, 1230, 732, 11, 51336, 51336, 577, 2539, 316, 576, 767, 312, 5002, 563, 13, 509, 727, 362, 257, 819, 316, 337, 819, 9235, 13, 51644, 51676], "temperature": 0.0, "avg_logprob": -0.13076894933527167, "compression_ratio": 1.5774058577405858, "no_speech_prob": 1.184119264507899e-05}, {"id": 129, "seek": 85304, "start": 872.48, "end": 878.64, "text": " how learning A would actually be advantageous. You could have a different A for different channels.", "tokens": [50364, 18161, 2533, 11, 457, 309, 311, 1143, 257, 688, 412, 264, 5598, 382, 733, 295, 472, 2522, 295, 257, 4470, 2445, 13, 400, 321, 603, 50632, 50632, 536, 341, 294, 257, 3456, 13, 2264, 11, 370, 729, 1651, 13, 492, 362, 257, 1168, 13, 407, 337, 659, 2907, 11, 286, 478, 406, 988, 51052, 51052, 286, 1223, 1230, 472, 11, 983, 321, 528, 264, 912, 2158, 337, 439, 9235, 13, 400, 1230, 732, 11, 51336, 51336, 577, 2539, 316, 576, 767, 312, 5002, 563, 13, 509, 727, 362, 257, 819, 316, 337, 819, 9235, 13, 51644, 51676], "temperature": 0.0, "avg_logprob": -0.13076894933527167, "compression_ratio": 1.5774058577405858, "no_speech_prob": 1.184119264507899e-05}, {"id": 130, "seek": 87864, "start": 878.64, "end": 884.56, "text": " So different units can have a different A. You could use this as a parameter of every unit.", "tokens": [50364, 407, 819, 6815, 393, 362, 257, 819, 316, 13, 509, 727, 764, 341, 382, 257, 13075, 295, 633, 4985, 13, 50660, 50748, 1610, 406, 13, 467, 727, 312, 5507, 13, 663, 311, 733, 295, 493, 281, 291, 13, 467, 727, 312, 5507, 412, 264, 1496, 295, 257, 4111, 51008, 51008, 4471, 294, 257, 45216, 304, 2533, 11, 420, 309, 727, 312, 5507, 322, 439, 4111, 11317, 11, 420, 309, 727, 312, 2609, 281, 51212, 51212, 633, 4985, 13, 759, 291, 534, 528, 281, 15665, 264, 45216, 304, 3687, 295, 257, 45216, 304, 2533, 11, 51424, 51424, 291, 1391, 528, 281, 362, 264, 912, 316, 337, 633, 4985, 294, 264, 4111, 4471, 13, 583, 291, 393, 362, 819, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.13763194438839746, "compression_ratio": 2.01255230125523, "no_speech_prob": 2.2824984625913203e-05}, {"id": 131, "seek": 87864, "start": 886.3199999999999, "end": 891.52, "text": " Or not. It could be shared. That's kind of up to you. It could be shared at the level of a feature", "tokens": [50364, 407, 819, 6815, 393, 362, 257, 819, 316, 13, 509, 727, 764, 341, 382, 257, 13075, 295, 633, 4985, 13, 50660, 50748, 1610, 406, 13, 467, 727, 312, 5507, 13, 663, 311, 733, 295, 493, 281, 291, 13, 467, 727, 312, 5507, 412, 264, 1496, 295, 257, 4111, 51008, 51008, 4471, 294, 257, 45216, 304, 2533, 11, 420, 309, 727, 312, 5507, 322, 439, 4111, 11317, 11, 420, 309, 727, 312, 2609, 281, 51212, 51212, 633, 4985, 13, 759, 291, 534, 528, 281, 15665, 264, 45216, 304, 3687, 295, 257, 45216, 304, 2533, 11, 51424, 51424, 291, 1391, 528, 281, 362, 264, 912, 316, 337, 633, 4985, 294, 264, 4111, 4471, 13, 583, 291, 393, 362, 819, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.13763194438839746, "compression_ratio": 2.01255230125523, "no_speech_prob": 2.2824984625913203e-05}, {"id": 132, "seek": 87864, "start": 891.52, "end": 895.6, "text": " map in a convolutional net, or it could be shared on all feature maps, or it could be individual to", "tokens": [50364, 407, 819, 6815, 393, 362, 257, 819, 316, 13, 509, 727, 764, 341, 382, 257, 13075, 295, 633, 4985, 13, 50660, 50748, 1610, 406, 13, 467, 727, 312, 5507, 13, 663, 311, 733, 295, 493, 281, 291, 13, 467, 727, 312, 5507, 412, 264, 1496, 295, 257, 4111, 51008, 51008, 4471, 294, 257, 45216, 304, 2533, 11, 420, 309, 727, 312, 5507, 322, 439, 4111, 11317, 11, 420, 309, 727, 312, 2609, 281, 51212, 51212, 633, 4985, 13, 759, 291, 534, 528, 281, 15665, 264, 45216, 304, 3687, 295, 257, 45216, 304, 2533, 11, 51424, 51424, 291, 1391, 528, 281, 362, 264, 912, 316, 337, 633, 4985, 294, 264, 4111, 4471, 13, 583, 291, 393, 362, 819, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.13763194438839746, "compression_ratio": 2.01255230125523, "no_speech_prob": 2.2824984625913203e-05}, {"id": 133, "seek": 87864, "start": 895.6, "end": 899.84, "text": " every unit. If you really want to preserve the convolutional nature of a convolutional net,", "tokens": [50364, 407, 819, 6815, 393, 362, 257, 819, 316, 13, 509, 727, 764, 341, 382, 257, 13075, 295, 633, 4985, 13, 50660, 50748, 1610, 406, 13, 467, 727, 312, 5507, 13, 663, 311, 733, 295, 493, 281, 291, 13, 467, 727, 312, 5507, 412, 264, 1496, 295, 257, 4111, 51008, 51008, 4471, 294, 257, 45216, 304, 2533, 11, 420, 309, 727, 312, 5507, 322, 439, 4111, 11317, 11, 420, 309, 727, 312, 2609, 281, 51212, 51212, 633, 4985, 13, 759, 291, 534, 528, 281, 15665, 264, 45216, 304, 3687, 295, 257, 45216, 304, 2533, 11, 51424, 51424, 291, 1391, 528, 281, 362, 264, 912, 316, 337, 633, 4985, 294, 264, 4111, 4471, 13, 583, 291, 393, 362, 819, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.13763194438839746, "compression_ratio": 2.01255230125523, "no_speech_prob": 2.2824984625913203e-05}, {"id": 134, "seek": 87864, "start": 899.84, "end": 904.08, "text": " you probably want to have the same A for every unit in the feature map. But you can have different", "tokens": [50364, 407, 819, 6815, 393, 362, 257, 819, 316, 13, 509, 727, 764, 341, 382, 257, 13075, 295, 633, 4985, 13, 50660, 50748, 1610, 406, 13, 467, 727, 312, 5507, 13, 663, 311, 733, 295, 493, 281, 291, 13, 467, 727, 312, 5507, 412, 264, 1496, 295, 257, 4111, 51008, 51008, 4471, 294, 257, 45216, 304, 2533, 11, 420, 309, 727, 312, 5507, 322, 439, 4111, 11317, 11, 420, 309, 727, 312, 2609, 281, 51212, 51212, 633, 4985, 13, 759, 291, 534, 528, 281, 15665, 264, 45216, 304, 3687, 295, 257, 45216, 304, 2533, 11, 51424, 51424, 291, 1391, 528, 281, 362, 264, 912, 316, 337, 633, 4985, 294, 264, 4111, 4471, 13, 583, 291, 393, 362, 819, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.13763194438839746, "compression_ratio": 2.01255230125523, "no_speech_prob": 2.2824984625913203e-05}, {"id": 135, "seek": 90408, "start": 904.08, "end": 910.8000000000001, "text": " A's for different feature maps. OK, what was the second question? Why learning actually,", "tokens": [50364, 316, 311, 337, 819, 4111, 11317, 13, 2264, 11, 437, 390, 264, 1150, 1168, 30, 1545, 2539, 767, 11, 50700, 50740, 257, 2685, 2158, 576, 312, 5002, 563, 30, 1743, 11, 983, 366, 321, 2539, 316, 30, 509, 393, 1466, 309, 420, 406, 13, 509, 51016, 51016, 393, 3191, 309, 13, 440, 1778, 337, 19442, 309, 576, 312, 406, 4725, 281, 362, 1333, 295, 544, 4005, 51388, 51456, 2107, 12, 1889, 17409, 11, 457, 281, 733, 295, 5586, 300, 264, 2107, 12, 1889, 17409, 2709, 291, 257, 2107, 12, 32226, 16235, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.17324572801589966, "compression_ratio": 1.550420168067227, "no_speech_prob": 6.961239250813378e-06}, {"id": 136, "seek": 90408, "start": 911.6, "end": 917.12, "text": " a specific value would be advantageous? Like, why are we learning A? You can learn it or not. You", "tokens": [50364, 316, 311, 337, 819, 4111, 11317, 13, 2264, 11, 437, 390, 264, 1150, 1168, 30, 1545, 2539, 767, 11, 50700, 50740, 257, 2685, 2158, 576, 312, 5002, 563, 30, 1743, 11, 983, 366, 321, 2539, 316, 30, 509, 393, 1466, 309, 420, 406, 13, 509, 51016, 51016, 393, 3191, 309, 13, 440, 1778, 337, 19442, 309, 576, 312, 406, 4725, 281, 362, 1333, 295, 544, 4005, 51388, 51456, 2107, 12, 1889, 17409, 11, 457, 281, 733, 295, 5586, 300, 264, 2107, 12, 1889, 17409, 2709, 291, 257, 2107, 12, 32226, 16235, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.17324572801589966, "compression_ratio": 1.550420168067227, "no_speech_prob": 6.961239250813378e-06}, {"id": 137, "seek": 90408, "start": 917.12, "end": 924.5600000000001, "text": " can fix it. The reason for fixing it would be not necessarily to have sort of more powerful", "tokens": [50364, 316, 311, 337, 819, 4111, 11317, 13, 2264, 11, 437, 390, 264, 1150, 1168, 30, 1545, 2539, 767, 11, 50700, 50740, 257, 2685, 2158, 576, 312, 5002, 563, 30, 1743, 11, 983, 366, 321, 2539, 316, 30, 509, 393, 1466, 309, 420, 406, 13, 509, 51016, 51016, 393, 3191, 309, 13, 440, 1778, 337, 19442, 309, 576, 312, 406, 4725, 281, 362, 1333, 295, 544, 4005, 51388, 51456, 2107, 12, 1889, 17409, 11, 457, 281, 733, 295, 5586, 300, 264, 2107, 12, 1889, 17409, 2709, 291, 257, 2107, 12, 32226, 16235, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.17324572801589966, "compression_ratio": 1.550420168067227, "no_speech_prob": 6.961239250813378e-06}, {"id": 138, "seek": 90408, "start": 925.9200000000001, "end": 930.6400000000001, "text": " non-linearity, but to kind of ensure that the non-linearity gives you a non-zero gradient,", "tokens": [50364, 316, 311, 337, 819, 4111, 11317, 13, 2264, 11, 437, 390, 264, 1150, 1168, 30, 1545, 2539, 767, 11, 50700, 50740, 257, 2685, 2158, 576, 312, 5002, 563, 30, 1743, 11, 983, 366, 321, 2539, 316, 30, 509, 393, 1466, 309, 420, 406, 13, 509, 51016, 51016, 393, 3191, 309, 13, 440, 1778, 337, 19442, 309, 576, 312, 406, 4725, 281, 362, 1333, 295, 544, 4005, 51388, 51456, 2107, 12, 1889, 17409, 11, 457, 281, 733, 295, 5586, 300, 264, 2107, 12, 1889, 17409, 2709, 291, 257, 2107, 12, 32226, 16235, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.17324572801589966, "compression_ratio": 1.550420168067227, "no_speech_prob": 6.961239250813378e-06}, {"id": 139, "seek": 93064, "start": 930.64, "end": 939.84, "text": " even if it's in the negative region. So, you know, learnable, not learnable. So", "tokens": [50364, 754, 498, 309, 311, 294, 264, 3671, 4458, 13, 407, 11, 291, 458, 11, 1466, 712, 11, 406, 1466, 712, 13, 407, 50824, 50892, 281, 652, 309, 1466, 712, 4045, 264, 1185, 281, 1936, 1261, 257, 2107, 12, 1889, 17409, 666, 2139, 257, 8213, 51204, 51240, 18350, 11, 597, 295, 1164, 307, 406, 4098, 1880, 11, 457, 983, 406, 11, 51408, 51456, 257, 2158, 11, 420, 746, 411, 257, 1577, 11048, 3774, 11, 2264, 11, 689, 316, 576, 312, 3175, 502, 294, 264, 3671, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.16288060491735284, "compression_ratio": 1.572093023255814, "no_speech_prob": 2.482428953953786e-06}, {"id": 140, "seek": 93064, "start": 941.1999999999999, "end": 947.4399999999999, "text": " to make it learnable allows the system to basically turn a non-linearity into either a linear", "tokens": [50364, 754, 498, 309, 311, 294, 264, 3671, 4458, 13, 407, 11, 291, 458, 11, 1466, 712, 11, 406, 1466, 712, 13, 407, 50824, 50892, 281, 652, 309, 1466, 712, 4045, 264, 1185, 281, 1936, 1261, 257, 2107, 12, 1889, 17409, 666, 2139, 257, 8213, 51204, 51240, 18350, 11, 597, 295, 1164, 307, 406, 4098, 1880, 11, 457, 983, 406, 11, 51408, 51456, 257, 2158, 11, 420, 746, 411, 257, 1577, 11048, 3774, 11, 2264, 11, 689, 316, 576, 312, 3175, 502, 294, 264, 3671, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.16288060491735284, "compression_ratio": 1.572093023255814, "no_speech_prob": 2.482428953953786e-06}, {"id": 141, "seek": 93064, "start": 948.16, "end": 951.52, "text": " mapping, which of course is not particularly interesting, but why not,", "tokens": [50364, 754, 498, 309, 311, 294, 264, 3671, 4458, 13, 407, 11, 291, 458, 11, 1466, 712, 11, 406, 1466, 712, 13, 407, 50824, 50892, 281, 652, 309, 1466, 712, 4045, 264, 1185, 281, 1936, 1261, 257, 2107, 12, 1889, 17409, 666, 2139, 257, 8213, 51204, 51240, 18350, 11, 597, 295, 1164, 307, 406, 4098, 1880, 11, 457, 983, 406, 11, 51408, 51456, 257, 2158, 11, 420, 746, 411, 257, 1577, 11048, 3774, 11, 2264, 11, 689, 316, 576, 312, 3175, 502, 294, 264, 3671, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.16288060491735284, "compression_ratio": 1.572093023255814, "no_speech_prob": 2.482428953953786e-06}, {"id": 142, "seek": 93064, "start": 952.48, "end": 960.3199999999999, "text": " a value, or something like a full rectification, OK, where A would be minus 1 in the negative", "tokens": [50364, 754, 498, 309, 311, 294, 264, 3671, 4458, 13, 407, 11, 291, 458, 11, 1466, 712, 11, 406, 1466, 712, 13, 407, 50824, 50892, 281, 652, 309, 1466, 712, 4045, 264, 1185, 281, 1936, 1261, 257, 2107, 12, 1889, 17409, 666, 2139, 257, 8213, 51204, 51240, 18350, 11, 597, 295, 1164, 307, 406, 4098, 1880, 11, 457, 983, 406, 11, 51408, 51456, 257, 2158, 11, 420, 746, 411, 257, 1577, 11048, 3774, 11, 2264, 11, 689, 316, 576, 312, 3175, 502, 294, 264, 3671, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.16288060491735284, "compression_ratio": 1.572093023255814, "no_speech_prob": 2.482428953953786e-06}, {"id": 143, "seek": 96032, "start": 960.32, "end": 966.1600000000001, "text": " part, which can be interesting for certain types of applications. So for example, if you have a", "tokens": [50364, 644, 11, 597, 393, 312, 1880, 337, 1629, 3467, 295, 5821, 13, 407, 337, 1365, 11, 498, 291, 362, 257, 50656, 50656, 45216, 304, 2533, 300, 575, 364, 4691, 25712, 11, 364, 4691, 25712, 575, 257, 12367, 507, 11, 558, 30, 467, 311, 658, 1804, 50972, 50972, 31994, 322, 472, 1252, 11, 3175, 31994, 322, 264, 661, 1252, 13, 400, 370, 309, 311, 516, 281, 4515, 13, 407, 498, 51224, 51224, 291, 362, 364, 4691, 294, 364, 3256, 300, 1709, 490, 11, 584, 11, 2877, 281, 4730, 11, 264, 45216, 486, 4515, 25795, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.10106875232814513, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.769222419374273e-06}, {"id": 144, "seek": 96032, "start": 966.1600000000001, "end": 972.48, "text": " convolutional net that has an edge detector, an edge detector has a polarity, right? It's got plus", "tokens": [50364, 644, 11, 597, 393, 312, 1880, 337, 1629, 3467, 295, 5821, 13, 407, 337, 1365, 11, 498, 291, 362, 257, 50656, 50656, 45216, 304, 2533, 300, 575, 364, 4691, 25712, 11, 364, 4691, 25712, 575, 257, 12367, 507, 11, 558, 30, 467, 311, 658, 1804, 50972, 50972, 31994, 322, 472, 1252, 11, 3175, 31994, 322, 264, 661, 1252, 13, 400, 370, 309, 311, 516, 281, 4515, 13, 407, 498, 51224, 51224, 291, 362, 364, 4691, 294, 364, 3256, 300, 1709, 490, 11, 584, 11, 2877, 281, 4730, 11, 264, 45216, 486, 4515, 25795, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.10106875232814513, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.769222419374273e-06}, {"id": 145, "seek": 96032, "start": 972.48, "end": 977.5200000000001, "text": " coefficients on one side, minus coefficients on the other side. And so it's going to react. So if", "tokens": [50364, 644, 11, 597, 393, 312, 1880, 337, 1629, 3467, 295, 5821, 13, 407, 337, 1365, 11, 498, 291, 362, 257, 50656, 50656, 45216, 304, 2533, 300, 575, 364, 4691, 25712, 11, 364, 4691, 25712, 575, 257, 12367, 507, 11, 558, 30, 467, 311, 658, 1804, 50972, 50972, 31994, 322, 472, 1252, 11, 3175, 31994, 322, 264, 661, 1252, 13, 400, 370, 309, 311, 516, 281, 4515, 13, 407, 498, 51224, 51224, 291, 362, 364, 4691, 294, 364, 3256, 300, 1709, 490, 11, 584, 11, 2877, 281, 4730, 11, 264, 45216, 486, 4515, 25795, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.10106875232814513, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.769222419374273e-06}, {"id": 146, "seek": 96032, "start": 977.5200000000001, "end": 985.12, "text": " you have an edge in an image that goes from, say, dark to bright, the convolution will react positively", "tokens": [50364, 644, 11, 597, 393, 312, 1880, 337, 1629, 3467, 295, 5821, 13, 407, 337, 1365, 11, 498, 291, 362, 257, 50656, 50656, 45216, 304, 2533, 300, 575, 364, 4691, 25712, 11, 364, 4691, 25712, 575, 257, 12367, 507, 11, 558, 30, 467, 311, 658, 1804, 50972, 50972, 31994, 322, 472, 1252, 11, 3175, 31994, 322, 264, 661, 1252, 13, 400, 370, 309, 311, 516, 281, 4515, 13, 407, 498, 51224, 51224, 291, 362, 364, 4691, 294, 364, 3256, 300, 1709, 490, 11, 584, 11, 2877, 281, 4730, 11, 264, 45216, 486, 4515, 25795, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.10106875232814513, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.769222419374273e-06}, {"id": 147, "seek": 98512, "start": 985.12, "end": 992.48, "text": " to this one. But if you have another edge from, you know, in the opposite direction, then the", "tokens": [50364, 281, 341, 472, 13, 583, 498, 291, 362, 1071, 4691, 490, 11, 291, 458, 11, 294, 264, 6182, 3513, 11, 550, 264, 50732, 50824, 6608, 486, 4515, 29519, 13, 823, 11, 498, 291, 528, 428, 6608, 281, 4515, 281, 364, 4691, 10060, 295, 1080, 51108, 51108, 12367, 507, 11, 291, 11048, 2505, 309, 13, 2264, 11, 370, 300, 576, 312, 733, 295, 445, 8236, 2158, 13, 823, 11, 291, 727, 11, 51416, 51416, 295, 1164, 11, 16562, 341, 294, 13, 509, 500, 380, 362, 281, 764, 257, 659, 32334, 13, 509, 393, 445, 764, 264, 8236, 2158, 13, 51656, 51692], "temperature": 0.0, "avg_logprob": -0.1215685863120883, "compression_ratio": 1.6607929515418502, "no_speech_prob": 1.1124990123789757e-05}, {"id": 148, "seek": 98512, "start": 994.32, "end": 1000.0, "text": " filter will react negatively. Now, if you want your filter to react to an edge regardless of its", "tokens": [50364, 281, 341, 472, 13, 583, 498, 291, 362, 1071, 4691, 490, 11, 291, 458, 11, 294, 264, 6182, 3513, 11, 550, 264, 50732, 50824, 6608, 486, 4515, 29519, 13, 823, 11, 498, 291, 528, 428, 6608, 281, 4515, 281, 364, 4691, 10060, 295, 1080, 51108, 51108, 12367, 507, 11, 291, 11048, 2505, 309, 13, 2264, 11, 370, 300, 576, 312, 733, 295, 445, 8236, 2158, 13, 823, 11, 291, 727, 11, 51416, 51416, 295, 1164, 11, 16562, 341, 294, 13, 509, 500, 380, 362, 281, 764, 257, 659, 32334, 13, 509, 393, 445, 764, 264, 8236, 2158, 13, 51656, 51692], "temperature": 0.0, "avg_logprob": -0.1215685863120883, "compression_ratio": 1.6607929515418502, "no_speech_prob": 1.1124990123789757e-05}, {"id": 149, "seek": 98512, "start": 1000.0, "end": 1006.16, "text": " polarity, you rectify it. OK, so that would be kind of just absolute value. Now, you could,", "tokens": [50364, 281, 341, 472, 13, 583, 498, 291, 362, 1071, 4691, 490, 11, 291, 458, 11, 294, 264, 6182, 3513, 11, 550, 264, 50732, 50824, 6608, 486, 4515, 29519, 13, 823, 11, 498, 291, 528, 428, 6608, 281, 4515, 281, 364, 4691, 10060, 295, 1080, 51108, 51108, 12367, 507, 11, 291, 11048, 2505, 309, 13, 2264, 11, 370, 300, 576, 312, 733, 295, 445, 8236, 2158, 13, 823, 11, 291, 727, 11, 51416, 51416, 295, 1164, 11, 16562, 341, 294, 13, 509, 500, 380, 362, 281, 764, 257, 659, 32334, 13, 509, 393, 445, 764, 264, 8236, 2158, 13, 51656, 51692], "temperature": 0.0, "avg_logprob": -0.1215685863120883, "compression_ratio": 1.6607929515418502, "no_speech_prob": 1.1124990123789757e-05}, {"id": 150, "seek": 98512, "start": 1006.16, "end": 1010.96, "text": " of course, bake this in. You don't have to use a prelude. You can just use the absolute value.", "tokens": [50364, 281, 341, 472, 13, 583, 498, 291, 362, 1071, 4691, 490, 11, 291, 458, 11, 294, 264, 6182, 3513, 11, 550, 264, 50732, 50824, 6608, 486, 4515, 29519, 13, 823, 11, 498, 291, 528, 428, 6608, 281, 4515, 281, 364, 4691, 10060, 295, 1080, 51108, 51108, 12367, 507, 11, 291, 11048, 2505, 309, 13, 2264, 11, 370, 300, 576, 312, 733, 295, 445, 8236, 2158, 13, 823, 11, 291, 727, 11, 51416, 51416, 295, 1164, 11, 16562, 341, 294, 13, 509, 500, 380, 362, 281, 764, 257, 659, 32334, 13, 509, 393, 445, 764, 264, 8236, 2158, 13, 51656, 51692], "temperature": 0.0, "avg_logprob": -0.1215685863120883, "compression_ratio": 1.6607929515418502, "no_speech_prob": 1.1124990123789757e-05}, {"id": 151, "seek": 101096, "start": 1010.96, "end": 1016.24, "text": " Probably a better idea is to use a square, actually. So if you take the square nonlinearity,", "tokens": [50364, 9210, 257, 1101, 1558, 307, 281, 764, 257, 3732, 11, 767, 13, 407, 498, 291, 747, 264, 3732, 2107, 1889, 17409, 11, 50628, 50628, 309, 311, 406, 12270, 382, 733, 295, 257, 18161, 2533, 2107, 1889, 17409, 13, 583, 11, 291, 458, 11, 294, 264, 11745, 50848, 50848, 1254, 295, 9953, 51, 284, 339, 11, 291, 445, 2464, 3732, 11, 293, 300, 311, 309, 13, 6483, 286, 10103, 264, 1168, 13, 2639, 661, 51132, 51132, 1168, 322, 341, 4829, 30, 286, 362, 257, 1168, 13, 467, 2544, 281, 385, 411, 613, 2107, 28263, 1088, 366, 1382, 281, 51604, 51632], "temperature": 0.0, "avg_logprob": -0.18808740672498647, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.0783012839965522e-05}, {"id": 152, "seek": 101096, "start": 1016.24, "end": 1020.64, "text": " it's not implemented as kind of a neural net nonlinearity. But, you know, in the functional", "tokens": [50364, 9210, 257, 1101, 1558, 307, 281, 764, 257, 3732, 11, 767, 13, 407, 498, 291, 747, 264, 3732, 2107, 1889, 17409, 11, 50628, 50628, 309, 311, 406, 12270, 382, 733, 295, 257, 18161, 2533, 2107, 1889, 17409, 13, 583, 11, 291, 458, 11, 294, 264, 11745, 50848, 50848, 1254, 295, 9953, 51, 284, 339, 11, 291, 445, 2464, 3732, 11, 293, 300, 311, 309, 13, 6483, 286, 10103, 264, 1168, 13, 2639, 661, 51132, 51132, 1168, 322, 341, 4829, 30, 286, 362, 257, 1168, 13, 467, 2544, 281, 385, 411, 613, 2107, 28263, 1088, 366, 1382, 281, 51604, 51632], "temperature": 0.0, "avg_logprob": -0.18808740672498647, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.0783012839965522e-05}, {"id": 153, "seek": 101096, "start": 1020.64, "end": 1026.32, "text": " form of PyTorch, you just write square, and that's it. Hope I answered the question. Any other", "tokens": [50364, 9210, 257, 1101, 1558, 307, 281, 764, 257, 3732, 11, 767, 13, 407, 498, 291, 747, 264, 3732, 2107, 1889, 17409, 11, 50628, 50628, 309, 311, 406, 12270, 382, 733, 295, 257, 18161, 2533, 2107, 1889, 17409, 13, 583, 11, 291, 458, 11, 294, 264, 11745, 50848, 50848, 1254, 295, 9953, 51, 284, 339, 11, 291, 445, 2464, 3732, 11, 293, 300, 311, 309, 13, 6483, 286, 10103, 264, 1168, 13, 2639, 661, 51132, 51132, 1168, 322, 341, 4829, 30, 286, 362, 257, 1168, 13, 467, 2544, 281, 385, 411, 613, 2107, 28263, 1088, 366, 1382, 281, 51604, 51632], "temperature": 0.0, "avg_logprob": -0.18808740672498647, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.0783012839965522e-05}, {"id": 154, "seek": 101096, "start": 1026.32, "end": 1035.76, "text": " question on this topic? I have a question. It seems to me like these nonlinearities are trying to", "tokens": [50364, 9210, 257, 1101, 1558, 307, 281, 764, 257, 3732, 11, 767, 13, 407, 498, 291, 747, 264, 3732, 2107, 1889, 17409, 11, 50628, 50628, 309, 311, 406, 12270, 382, 733, 295, 257, 18161, 2533, 2107, 1889, 17409, 13, 583, 11, 291, 458, 11, 294, 264, 11745, 50848, 50848, 1254, 295, 9953, 51, 284, 339, 11, 291, 445, 2464, 3732, 11, 293, 300, 311, 309, 13, 6483, 286, 10103, 264, 1168, 13, 2639, 661, 51132, 51132, 1168, 322, 341, 4829, 30, 286, 362, 257, 1168, 13, 467, 2544, 281, 385, 411, 613, 2107, 28263, 1088, 366, 1382, 281, 51604, 51632], "temperature": 0.0, "avg_logprob": -0.18808740672498647, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.0783012839965522e-05}, {"id": 155, "seek": 103576, "start": 1035.76, "end": 1045.36, "text": " basically make a linear function nonlinear, and the tweak in the lines denote the change in that", "tokens": [50364, 1936, 652, 257, 8213, 2445, 2107, 28263, 11, 293, 264, 29879, 294, 264, 3876, 45708, 264, 1319, 294, 300, 50844, 50844, 2445, 13, 407, 393, 321, 519, 295, 341, 382, 498, 321, 528, 281, 2316, 257, 7605, 294, 264, 1622, 11, 820, 321, 362, 51404, 51404, 1466, 712, 9834, 322, 1293, 11, 411, 949, 264, 4018, 293, 934, 264, 4018, 322, 264, 2031, 12, 24633, 30, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.20213315146309987, "compression_ratio": 1.5930232558139534, "no_speech_prob": 1.0129392649105284e-05}, {"id": 156, "seek": 103576, "start": 1045.36, "end": 1056.56, "text": " function. So can we think of this as if we want to model a curve in the line, should we have", "tokens": [50364, 1936, 652, 257, 8213, 2445, 2107, 28263, 11, 293, 264, 29879, 294, 264, 3876, 45708, 264, 1319, 294, 300, 50844, 50844, 2445, 13, 407, 393, 321, 519, 295, 341, 382, 498, 321, 528, 281, 2316, 257, 7605, 294, 264, 1622, 11, 820, 321, 362, 51404, 51404, 1466, 712, 9834, 322, 1293, 11, 411, 949, 264, 4018, 293, 934, 264, 4018, 322, 264, 2031, 12, 24633, 30, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.20213315146309987, "compression_ratio": 1.5930232558139534, "no_speech_prob": 1.0129392649105284e-05}, {"id": 157, "seek": 103576, "start": 1056.56, "end": 1064.08, "text": " learnable parameters on both, like before the zero and after the zero on the x-axis?", "tokens": [50364, 1936, 652, 257, 8213, 2445, 2107, 28263, 11, 293, 264, 29879, 294, 264, 3876, 45708, 264, 1319, 294, 300, 50844, 50844, 2445, 13, 407, 393, 321, 519, 295, 341, 382, 498, 321, 528, 281, 2316, 257, 7605, 294, 264, 1622, 11, 820, 321, 362, 51404, 51404, 1466, 712, 9834, 322, 1293, 11, 411, 949, 264, 4018, 293, 934, 264, 4018, 322, 264, 2031, 12, 24633, 30, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.20213315146309987, "compression_ratio": 1.5930232558139534, "no_speech_prob": 1.0129392649105284e-05}, {"id": 158, "seek": 106408, "start": 1064.08, "end": 1070.08, "text": " Well, yeah, I mean, there is diminishing return. So the question is, you know, how complex do you", "tokens": [50364, 1042, 11, 1338, 11, 286, 914, 11, 456, 307, 15739, 3807, 2736, 13, 407, 264, 1168, 307, 11, 291, 458, 11, 577, 3997, 360, 291, 50664, 50664, 528, 428, 2107, 1889, 17409, 281, 312, 30, 407, 291, 727, 3811, 11, 295, 1164, 11, 13075, 3319, 364, 2302, 50900, 50960, 2107, 28263, 2445, 11, 291, 458, 11, 365, 4732, 533, 9834, 420, 879, 33352, 19490, 420, 746, 411, 341, 11, 51188, 51188, 558, 30, 1610, 11, 286, 500, 380, 458, 11, 761, 897, 88, 1876, 85, 22560, 12356, 13, 509, 458, 11, 286, 914, 11, 291, 393, 13075, 1125, 604, 51504, 51504, 18350, 291, 528, 11, 558, 30, 509, 393, 3811, 729, 9834, 727, 312, 644, 295, 264, 2539, 1399, 13, 51780, 51828], "temperature": 0.0, "avg_logprob": -0.2579998047121109, "compression_ratio": 1.7296296296296296, "no_speech_prob": 1.1125195669592358e-05}, {"id": 159, "seek": 106408, "start": 1070.08, "end": 1074.8, "text": " want your nonlinearity to be? So you could imagine, of course, parameterizing an entire", "tokens": [50364, 1042, 11, 1338, 11, 286, 914, 11, 456, 307, 15739, 3807, 2736, 13, 407, 264, 1168, 307, 11, 291, 458, 11, 577, 3997, 360, 291, 50664, 50664, 528, 428, 2107, 1889, 17409, 281, 312, 30, 407, 291, 727, 3811, 11, 295, 1164, 11, 13075, 3319, 364, 2302, 50900, 50960, 2107, 28263, 2445, 11, 291, 458, 11, 365, 4732, 533, 9834, 420, 879, 33352, 19490, 420, 746, 411, 341, 11, 51188, 51188, 558, 30, 1610, 11, 286, 500, 380, 458, 11, 761, 897, 88, 1876, 85, 22560, 12356, 13, 509, 458, 11, 286, 914, 11, 291, 393, 13075, 1125, 604, 51504, 51504, 18350, 291, 528, 11, 558, 30, 509, 393, 3811, 729, 9834, 727, 312, 644, 295, 264, 2539, 1399, 13, 51780, 51828], "temperature": 0.0, "avg_logprob": -0.2579998047121109, "compression_ratio": 1.7296296296296296, "no_speech_prob": 1.1125195669592358e-05}, {"id": 160, "seek": 106408, "start": 1076.0, "end": 1080.56, "text": " nonlinear function, you know, with spline parameters or Bezier curves or something like this,", "tokens": [50364, 1042, 11, 1338, 11, 286, 914, 11, 456, 307, 15739, 3807, 2736, 13, 407, 264, 1168, 307, 11, 291, 458, 11, 577, 3997, 360, 291, 50664, 50664, 528, 428, 2107, 1889, 17409, 281, 312, 30, 407, 291, 727, 3811, 11, 295, 1164, 11, 13075, 3319, 364, 2302, 50900, 50960, 2107, 28263, 2445, 11, 291, 458, 11, 365, 4732, 533, 9834, 420, 879, 33352, 19490, 420, 746, 411, 341, 11, 51188, 51188, 558, 30, 1610, 11, 286, 500, 380, 458, 11, 761, 897, 88, 1876, 85, 22560, 12356, 13, 509, 458, 11, 286, 914, 11, 291, 393, 13075, 1125, 604, 51504, 51504, 18350, 291, 528, 11, 558, 30, 509, 393, 3811, 729, 9834, 727, 312, 644, 295, 264, 2539, 1399, 13, 51780, 51828], "temperature": 0.0, "avg_logprob": -0.2579998047121109, "compression_ratio": 1.7296296296296296, "no_speech_prob": 1.1125195669592358e-05}, {"id": 161, "seek": 106408, "start": 1080.56, "end": 1086.8799999999999, "text": " right? Or, I don't know, Chibychev polynomials. You know, I mean, you can parameterize any", "tokens": [50364, 1042, 11, 1338, 11, 286, 914, 11, 456, 307, 15739, 3807, 2736, 13, 407, 264, 1168, 307, 11, 291, 458, 11, 577, 3997, 360, 291, 50664, 50664, 528, 428, 2107, 1889, 17409, 281, 312, 30, 407, 291, 727, 3811, 11, 295, 1164, 11, 13075, 3319, 364, 2302, 50900, 50960, 2107, 28263, 2445, 11, 291, 458, 11, 365, 4732, 533, 9834, 420, 879, 33352, 19490, 420, 746, 411, 341, 11, 51188, 51188, 558, 30, 1610, 11, 286, 500, 380, 458, 11, 761, 897, 88, 1876, 85, 22560, 12356, 13, 509, 458, 11, 286, 914, 11, 291, 393, 13075, 1125, 604, 51504, 51504, 18350, 291, 528, 11, 558, 30, 509, 393, 3811, 729, 9834, 727, 312, 644, 295, 264, 2539, 1399, 13, 51780, 51828], "temperature": 0.0, "avg_logprob": -0.2579998047121109, "compression_ratio": 1.7296296296296296, "no_speech_prob": 1.1125195669592358e-05}, {"id": 162, "seek": 106408, "start": 1086.8799999999999, "end": 1092.3999999999999, "text": " mapping you want, right? You can imagine those parameters could be part of the learning process.", "tokens": [50364, 1042, 11, 1338, 11, 286, 914, 11, 456, 307, 15739, 3807, 2736, 13, 407, 264, 1168, 307, 11, 291, 458, 11, 577, 3997, 360, 291, 50664, 50664, 528, 428, 2107, 1889, 17409, 281, 312, 30, 407, 291, 727, 3811, 11, 295, 1164, 11, 13075, 3319, 364, 2302, 50900, 50960, 2107, 28263, 2445, 11, 291, 458, 11, 365, 4732, 533, 9834, 420, 879, 33352, 19490, 420, 746, 411, 341, 11, 51188, 51188, 558, 30, 1610, 11, 286, 500, 380, 458, 11, 761, 897, 88, 1876, 85, 22560, 12356, 13, 509, 458, 11, 286, 914, 11, 291, 393, 13075, 1125, 604, 51504, 51504, 18350, 291, 528, 11, 558, 30, 509, 393, 3811, 729, 9834, 727, 312, 644, 295, 264, 2539, 1399, 13, 51780, 51828], "temperature": 0.0, "avg_logprob": -0.2579998047121109, "compression_ratio": 1.7296296296296296, "no_speech_prob": 1.1125195669592358e-05}, {"id": 163, "seek": 109240, "start": 1092.4, "end": 1098.72, "text": " However, you know, what is the advantage of doing this versus just, you know, having more units in", "tokens": [50364, 2908, 11, 291, 458, 11, 437, 307, 264, 5002, 295, 884, 341, 5717, 445, 11, 291, 458, 11, 1419, 544, 6815, 294, 50680, 50680, 428, 1185, 293, 24140, 322, 264, 1186, 300, 3866, 6815, 486, 312, 3869, 294, 264, 917, 281, 30874, 264, 51032, 51032, 2445, 291, 528, 30, 21082, 11, 309, 534, 5946, 322, 437, 11, 411, 11, 498, 291, 528, 281, 360, 24590, 294, 257, 51384, 51384, 6457, 2295, 18795, 1901, 11, 370, 4317, 291, 528, 512, 13075, 1602, 2107, 28263, 1088, 11, 300, 1062, 854, 13, 51620, 51696], "temperature": 0.0, "avg_logprob": -0.17095623221448672, "compression_ratio": 1.6610878661087867, "no_speech_prob": 1.2098296338081127e-06}, {"id": 164, "seek": 109240, "start": 1098.72, "end": 1105.76, "text": " your system and relying on the fact that multiple units will be added in the end to approximate the", "tokens": [50364, 2908, 11, 291, 458, 11, 437, 307, 264, 5002, 295, 884, 341, 5717, 445, 11, 291, 458, 11, 1419, 544, 6815, 294, 50680, 50680, 428, 1185, 293, 24140, 322, 264, 1186, 300, 3866, 6815, 486, 312, 3869, 294, 264, 917, 281, 30874, 264, 51032, 51032, 2445, 291, 528, 30, 21082, 11, 309, 534, 5946, 322, 437, 11, 411, 11, 498, 291, 528, 281, 360, 24590, 294, 257, 51384, 51384, 6457, 2295, 18795, 1901, 11, 370, 4317, 291, 528, 512, 13075, 1602, 2107, 28263, 1088, 11, 300, 1062, 854, 13, 51620, 51696], "temperature": 0.0, "avg_logprob": -0.17095623221448672, "compression_ratio": 1.6610878661087867, "no_speech_prob": 1.2098296338081127e-06}, {"id": 165, "seek": 109240, "start": 1105.76, "end": 1112.8000000000002, "text": " function you want? Generally, it really depends on what, like, if you want to do regression in a", "tokens": [50364, 2908, 11, 291, 458, 11, 437, 307, 264, 5002, 295, 884, 341, 5717, 445, 11, 291, 458, 11, 1419, 544, 6815, 294, 50680, 50680, 428, 1185, 293, 24140, 322, 264, 1186, 300, 3866, 6815, 486, 312, 3869, 294, 264, 917, 281, 30874, 264, 51032, 51032, 2445, 291, 528, 30, 21082, 11, 309, 534, 5946, 322, 437, 11, 411, 11, 498, 291, 528, 281, 360, 24590, 294, 257, 51384, 51384, 6457, 2295, 18795, 1901, 11, 370, 4317, 291, 528, 512, 13075, 1602, 2107, 28263, 1088, 11, 300, 1062, 854, 13, 51620, 51696], "temperature": 0.0, "avg_logprob": -0.17095623221448672, "compression_ratio": 1.6610878661087867, "no_speech_prob": 1.2098296338081127e-06}, {"id": 166, "seek": 109240, "start": 1112.8000000000002, "end": 1117.52, "text": " fairly low dimensional space, so perhaps you want some parameterized nonlinearities, that might help.", "tokens": [50364, 2908, 11, 291, 458, 11, 437, 307, 264, 5002, 295, 884, 341, 5717, 445, 11, 291, 458, 11, 1419, 544, 6815, 294, 50680, 50680, 428, 1185, 293, 24140, 322, 264, 1186, 300, 3866, 6815, 486, 312, 3869, 294, 264, 917, 281, 30874, 264, 51032, 51032, 2445, 291, 528, 30, 21082, 11, 309, 534, 5946, 322, 437, 11, 411, 11, 498, 291, 528, 281, 360, 24590, 294, 257, 51384, 51384, 6457, 2295, 18795, 1901, 11, 370, 4317, 291, 528, 512, 13075, 1602, 2107, 28263, 1088, 11, 300, 1062, 854, 13, 51620, 51696], "temperature": 0.0, "avg_logprob": -0.17095623221448672, "compression_ratio": 1.6610878661087867, "no_speech_prob": 1.2098296338081127e-06}, {"id": 167, "seek": 111752, "start": 1117.52, "end": 1123.92, "text": " You might have like, you might want to have a collection of different nonlinearities with maybe", "tokens": [50364, 509, 1062, 362, 411, 11, 291, 1062, 528, 281, 362, 257, 5765, 295, 819, 2107, 28263, 1088, 365, 1310, 50684, 50684, 721, 411, 761, 897, 88, 1876, 85, 22560, 12356, 498, 291, 528, 281, 360, 665, 8542, 763, 13, 583, 337, 11, 411, 11, 291, 458, 11, 50996, 50996, 1090, 18795, 9608, 411, 3256, 11150, 420, 721, 411, 341, 11, 291, 445, 528, 257, 2107, 1889, 17409, 13, 400, 51260, 51288, 309, 1985, 1101, 498, 264, 2107, 1889, 17409, 307, 1108, 310, 11630, 13, 10328, 11, 309, 7829, 439, 3685, 295, 2663, 570, 51552, 51552, 291, 393, 362, 732, 2793, 300, 486, 5258, 257, 2107, 28263, 2445, 13, 407, 11, 291, 458, 11, 291, 393, 362, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.32357937494913735, "compression_ratio": 1.7962962962962963, "no_speech_prob": 7.18294450052781e-06}, {"id": 168, "seek": 111752, "start": 1123.92, "end": 1130.16, "text": " things like Chibychev polynomials if you want to do good approximations. But for, like, you know,", "tokens": [50364, 509, 1062, 362, 411, 11, 291, 1062, 528, 281, 362, 257, 5765, 295, 819, 2107, 28263, 1088, 365, 1310, 50684, 50684, 721, 411, 761, 897, 88, 1876, 85, 22560, 12356, 498, 291, 528, 281, 360, 665, 8542, 763, 13, 583, 337, 11, 411, 11, 291, 458, 11, 50996, 50996, 1090, 18795, 9608, 411, 3256, 11150, 420, 721, 411, 341, 11, 291, 445, 528, 257, 2107, 1889, 17409, 13, 400, 51260, 51288, 309, 1985, 1101, 498, 264, 2107, 1889, 17409, 307, 1108, 310, 11630, 13, 10328, 11, 309, 7829, 439, 3685, 295, 2663, 570, 51552, 51552, 291, 393, 362, 732, 2793, 300, 486, 5258, 257, 2107, 28263, 2445, 13, 407, 11, 291, 458, 11, 291, 393, 362, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.32357937494913735, "compression_ratio": 1.7962962962962963, "no_speech_prob": 7.18294450052781e-06}, {"id": 169, "seek": 111752, "start": 1130.16, "end": 1135.44, "text": " high dimensional tasks like image recognition or things like this, you just want a nonlinearity. And", "tokens": [50364, 509, 1062, 362, 411, 11, 291, 1062, 528, 281, 362, 257, 5765, 295, 819, 2107, 28263, 1088, 365, 1310, 50684, 50684, 721, 411, 761, 897, 88, 1876, 85, 22560, 12356, 498, 291, 528, 281, 360, 665, 8542, 763, 13, 583, 337, 11, 411, 11, 291, 458, 11, 50996, 50996, 1090, 18795, 9608, 411, 3256, 11150, 420, 721, 411, 341, 11, 291, 445, 528, 257, 2107, 1889, 17409, 13, 400, 51260, 51288, 309, 1985, 1101, 498, 264, 2107, 1889, 17409, 307, 1108, 310, 11630, 13, 10328, 11, 309, 7829, 439, 3685, 295, 2663, 570, 51552, 51552, 291, 393, 362, 732, 2793, 300, 486, 5258, 257, 2107, 28263, 2445, 13, 407, 11, 291, 458, 11, 291, 393, 362, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.32357937494913735, "compression_ratio": 1.7962962962962963, "no_speech_prob": 7.18294450052781e-06}, {"id": 170, "seek": 111752, "start": 1136.0, "end": 1141.28, "text": " it works better if the nonlinearity is monotonic. Otherwise, it creates all kinds of issues because", "tokens": [50364, 509, 1062, 362, 411, 11, 291, 1062, 528, 281, 362, 257, 5765, 295, 819, 2107, 28263, 1088, 365, 1310, 50684, 50684, 721, 411, 761, 897, 88, 1876, 85, 22560, 12356, 498, 291, 528, 281, 360, 665, 8542, 763, 13, 583, 337, 11, 411, 11, 291, 458, 11, 50996, 50996, 1090, 18795, 9608, 411, 3256, 11150, 420, 721, 411, 341, 11, 291, 445, 528, 257, 2107, 1889, 17409, 13, 400, 51260, 51288, 309, 1985, 1101, 498, 264, 2107, 1889, 17409, 307, 1108, 310, 11630, 13, 10328, 11, 309, 7829, 439, 3685, 295, 2663, 570, 51552, 51552, 291, 393, 362, 732, 2793, 300, 486, 5258, 257, 2107, 28263, 2445, 13, 407, 11, 291, 458, 11, 291, 393, 362, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.32357937494913735, "compression_ratio": 1.7962962962962963, "no_speech_prob": 7.18294450052781e-06}, {"id": 171, "seek": 111752, "start": 1141.28, "end": 1147.44, "text": " you can have two points that will produce a nonlinear function. So, you know, you can have", "tokens": [50364, 509, 1062, 362, 411, 11, 291, 1062, 528, 281, 362, 257, 5765, 295, 819, 2107, 28263, 1088, 365, 1310, 50684, 50684, 721, 411, 761, 897, 88, 1876, 85, 22560, 12356, 498, 291, 528, 281, 360, 665, 8542, 763, 13, 583, 337, 11, 411, 11, 291, 458, 11, 50996, 50996, 1090, 18795, 9608, 411, 3256, 11150, 420, 721, 411, 341, 11, 291, 445, 528, 257, 2107, 1889, 17409, 13, 400, 51260, 51288, 309, 1985, 1101, 498, 264, 2107, 1889, 17409, 307, 1108, 310, 11630, 13, 10328, 11, 309, 7829, 439, 3685, 295, 2663, 570, 51552, 51552, 291, 393, 362, 732, 2793, 300, 486, 5258, 257, 2107, 28263, 2445, 13, 407, 11, 291, 458, 11, 291, 393, 362, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.32357937494913735, "compression_ratio": 1.7962962962962963, "no_speech_prob": 7.18294450052781e-06}, {"id": 172, "seek": 114744, "start": 1147.44, "end": 1151.92, "text": " the same output, and so it's a little ambiguous for the system to learn the right function there.", "tokens": [50364, 264, 912, 5598, 11, 293, 370, 309, 311, 257, 707, 39465, 337, 264, 1185, 281, 1466, 264, 558, 2445, 456, 13, 50588, 50656, 407, 11, 291, 528, 309, 11, 309, 311, 709, 1101, 498, 264, 2445, 307, 1108, 310, 11630, 11, 293, 1920, 439, 264, 6828, 510, 50860, 50860, 366, 1108, 310, 11630, 11, 3993, 498, 291, 362, 257, 3671, 257, 510, 294, 264, 294, 264, 4059, 1389, 13, 407, 11, 456, 311, 257, 955, 51172, 51172, 5002, 281, 1419, 1108, 310, 11630, 6828, 13, 583, 294, 8665, 11, 291, 727, 13075, 1125, 11, 291, 458, 11, 51548, 51548, 604, 2445, 291, 528, 13, 3432, 362, 3737, 365, 341, 11, 291, 458, 11, 436, 434, 406, 588, 3743, 570, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.13336609631049923, "compression_ratio": 1.8181818181818181, "no_speech_prob": 1.6441259504063055e-05}, {"id": 173, "seek": 114744, "start": 1153.28, "end": 1157.3600000000001, "text": " So, you want it, it's much better if the function is monotonic, and almost all the functions here", "tokens": [50364, 264, 912, 5598, 11, 293, 370, 309, 311, 257, 707, 39465, 337, 264, 1185, 281, 1466, 264, 558, 2445, 456, 13, 50588, 50656, 407, 11, 291, 528, 309, 11, 309, 311, 709, 1101, 498, 264, 2445, 307, 1108, 310, 11630, 11, 293, 1920, 439, 264, 6828, 510, 50860, 50860, 366, 1108, 310, 11630, 11, 3993, 498, 291, 362, 257, 3671, 257, 510, 294, 264, 294, 264, 4059, 1389, 13, 407, 11, 456, 311, 257, 955, 51172, 51172, 5002, 281, 1419, 1108, 310, 11630, 6828, 13, 583, 294, 8665, 11, 291, 727, 13075, 1125, 11, 291, 458, 11, 51548, 51548, 604, 2445, 291, 528, 13, 3432, 362, 3737, 365, 341, 11, 291, 458, 11, 436, 434, 406, 588, 3743, 570, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.13336609631049923, "compression_ratio": 1.8181818181818181, "no_speech_prob": 1.6441259504063055e-05}, {"id": 174, "seek": 114744, "start": 1157.3600000000001, "end": 1163.6000000000001, "text": " are monotonic, except if you have a negative a here in the in the prior case. So, there's a big", "tokens": [50364, 264, 912, 5598, 11, 293, 370, 309, 311, 257, 707, 39465, 337, 264, 1185, 281, 1466, 264, 558, 2445, 456, 13, 50588, 50656, 407, 11, 291, 528, 309, 11, 309, 311, 709, 1101, 498, 264, 2445, 307, 1108, 310, 11630, 11, 293, 1920, 439, 264, 6828, 510, 50860, 50860, 366, 1108, 310, 11630, 11, 3993, 498, 291, 362, 257, 3671, 257, 510, 294, 264, 294, 264, 4059, 1389, 13, 407, 11, 456, 311, 257, 955, 51172, 51172, 5002, 281, 1419, 1108, 310, 11630, 6828, 13, 583, 294, 8665, 11, 291, 727, 13075, 1125, 11, 291, 458, 11, 51548, 51548, 604, 2445, 291, 528, 13, 3432, 362, 3737, 365, 341, 11, 291, 458, 11, 436, 434, 406, 588, 3743, 570, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.13336609631049923, "compression_ratio": 1.8181818181818181, "no_speech_prob": 1.6441259504063055e-05}, {"id": 175, "seek": 114744, "start": 1163.6000000000001, "end": 1171.1200000000001, "text": " advantage to having monotonic functions. But in principle, you could parameterize, you know,", "tokens": [50364, 264, 912, 5598, 11, 293, 370, 309, 311, 257, 707, 39465, 337, 264, 1185, 281, 1466, 264, 558, 2445, 456, 13, 50588, 50656, 407, 11, 291, 528, 309, 11, 309, 311, 709, 1101, 498, 264, 2445, 307, 1108, 310, 11630, 11, 293, 1920, 439, 264, 6828, 510, 50860, 50860, 366, 1108, 310, 11630, 11, 3993, 498, 291, 362, 257, 3671, 257, 510, 294, 264, 294, 264, 4059, 1389, 13, 407, 11, 456, 311, 257, 955, 51172, 51172, 5002, 281, 1419, 1108, 310, 11630, 6828, 13, 583, 294, 8665, 11, 291, 727, 13075, 1125, 11, 291, 458, 11, 51548, 51548, 604, 2445, 291, 528, 13, 3432, 362, 3737, 365, 341, 11, 291, 458, 11, 436, 434, 406, 588, 3743, 570, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.13336609631049923, "compression_ratio": 1.8181818181818181, "no_speech_prob": 1.6441259504063055e-05}, {"id": 176, "seek": 114744, "start": 1171.1200000000001, "end": 1175.3600000000001, "text": " any function you want. People have played with this, you know, they're not very popular because", "tokens": [50364, 264, 912, 5598, 11, 293, 370, 309, 311, 257, 707, 39465, 337, 264, 1185, 281, 1466, 264, 558, 2445, 456, 13, 50588, 50656, 407, 11, 291, 528, 309, 11, 309, 311, 709, 1101, 498, 264, 2445, 307, 1108, 310, 11630, 11, 293, 1920, 439, 264, 6828, 510, 50860, 50860, 366, 1108, 310, 11630, 11, 3993, 498, 291, 362, 257, 3671, 257, 510, 294, 264, 294, 264, 4059, 1389, 13, 407, 11, 456, 311, 257, 955, 51172, 51172, 5002, 281, 1419, 1108, 310, 11630, 6828, 13, 583, 294, 8665, 11, 291, 727, 13075, 1125, 11, 291, 458, 11, 51548, 51548, 604, 2445, 291, 528, 13, 3432, 362, 3737, 365, 341, 11, 291, 458, 11, 436, 434, 406, 588, 3743, 570, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.13336609631049923, "compression_ratio": 1.8181818181818181, "no_speech_prob": 1.6441259504063055e-05}, {"id": 177, "seek": 117536, "start": 1175.36, "end": 1184.0, "text": " mostly they don't seem to be bringing a huge advantage in the kind of applications that people", "tokens": [50364, 5240, 436, 500, 380, 1643, 281, 312, 5062, 257, 2603, 5002, 294, 264, 733, 295, 5821, 300, 561, 50796, 50796, 764, 2416, 18161, 36170, 337, 13, 5358, 1651, 30, 3996, 1168, 307, 516, 281, 312, 3819, 12041, 13, 42404, 13, 51148, 51180, 865, 13, 407, 11, 264, 1168, 286, 600, 632, 307, 11, 393, 291, 519, 295, 604, 3861, 689, 264, 3922, 295, 51456, 51456], "temperature": 0.0, "avg_logprob": -0.18148889114607625, "compression_ratio": 1.5081967213114753, "no_speech_prob": 7.292545888049062e-06}, {"id": 178, "seek": 117536, "start": 1184.0, "end": 1191.04, "text": " use large neural nets for. Other questions? Another question is going to be King vs. Smooth.", "tokens": [50364, 5240, 436, 500, 380, 1643, 281, 312, 5062, 257, 2603, 5002, 294, 264, 733, 295, 5821, 300, 561, 50796, 50796, 764, 2416, 18161, 36170, 337, 13, 5358, 1651, 30, 3996, 1168, 307, 516, 281, 312, 3819, 12041, 13, 42404, 13, 51148, 51180, 865, 13, 407, 11, 264, 1168, 286, 600, 632, 307, 11, 393, 291, 519, 295, 604, 3861, 689, 264, 3922, 295, 51456, 51456], "temperature": 0.0, "avg_logprob": -0.18148889114607625, "compression_ratio": 1.5081967213114753, "no_speech_prob": 7.292545888049062e-06}, {"id": 179, "seek": 117536, "start": 1191.6799999999998, "end": 1197.1999999999998, "text": " Yeah. So, the question I've had is, can you think of any application where the choice of", "tokens": [50364, 5240, 436, 500, 380, 1643, 281, 312, 5062, 257, 2603, 5002, 294, 264, 733, 295, 5821, 300, 561, 50796, 50796, 764, 2416, 18161, 36170, 337, 13, 5358, 1651, 30, 3996, 1168, 307, 516, 281, 312, 3819, 12041, 13, 42404, 13, 51148, 51180, 865, 13, 407, 11, 264, 1168, 286, 600, 632, 307, 11, 393, 291, 519, 295, 604, 3861, 689, 264, 3922, 295, 51456, 51456], "temperature": 0.0, "avg_logprob": -0.18148889114607625, "compression_ratio": 1.5081967213114753, "no_speech_prob": 7.292545888049062e-06}, {"id": 180, "seek": 119720, "start": 1197.2, "end": 1206.0, "text": " nonlinearities made a big impact? The only thing I'm aware of is using a single function instead of", "tokens": [50364, 2107, 28263, 1088, 1027, 257, 955, 2712, 30, 440, 787, 551, 286, 478, 3650, 295, 307, 1228, 257, 2167, 2445, 2602, 295, 50804, 50804, 257, 3834, 3819, 337, 2452, 18161, 9590, 3665, 309, 3847, 1101, 13, 1042, 11, 370, 510, 311, 264, 1154, 51128, 51128, 365, 3834, 3819, 13, 16633, 3819, 575, 257, 3094, 12, 259, 4373, 294, 309, 11, 597, 1355, 498, 264, 17443, 295, 264, 51436, 51436, 22341, 4583, 366, 17207, 538, 732, 11, 420, 498, 264, 6358, 27433, 307, 17207, 538, 732, 11, 264, 1874, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.16014133328976837, "compression_ratio": 1.6196581196581197, "no_speech_prob": 1.1124422599095851e-05}, {"id": 181, "seek": 119720, "start": 1206.0, "end": 1212.48, "text": " a double King for deep neural networks helps it train better. Well, so here's the problem", "tokens": [50364, 2107, 28263, 1088, 1027, 257, 955, 2712, 30, 440, 787, 551, 286, 478, 3650, 295, 307, 1228, 257, 2167, 2445, 2602, 295, 50804, 50804, 257, 3834, 3819, 337, 2452, 18161, 9590, 3665, 309, 3847, 1101, 13, 1042, 11, 370, 510, 311, 264, 1154, 51128, 51128, 365, 3834, 3819, 13, 16633, 3819, 575, 257, 3094, 12, 259, 4373, 294, 309, 11, 597, 1355, 498, 264, 17443, 295, 264, 51436, 51436, 22341, 4583, 366, 17207, 538, 732, 11, 420, 498, 264, 6358, 27433, 307, 17207, 538, 732, 11, 264, 1874, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.16014133328976837, "compression_ratio": 1.6196581196581197, "no_speech_prob": 1.1124422599095851e-05}, {"id": 182, "seek": 119720, "start": 1212.48, "end": 1218.64, "text": " with double King. Double King has a built-in scale in it, which means if the weights of the", "tokens": [50364, 2107, 28263, 1088, 1027, 257, 955, 2712, 30, 440, 787, 551, 286, 478, 3650, 295, 307, 1228, 257, 2167, 2445, 2602, 295, 50804, 50804, 257, 3834, 3819, 337, 2452, 18161, 9590, 3665, 309, 3847, 1101, 13, 1042, 11, 370, 510, 311, 264, 1154, 51128, 51128, 365, 3834, 3819, 13, 16633, 3819, 575, 257, 3094, 12, 259, 4373, 294, 309, 11, 597, 1355, 498, 264, 17443, 295, 264, 51436, 51436, 22341, 4583, 366, 17207, 538, 732, 11, 420, 498, 264, 6358, 27433, 307, 17207, 538, 732, 11, 264, 1874, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.16014133328976837, "compression_ratio": 1.6196581196581197, "no_speech_prob": 1.1124422599095851e-05}, {"id": 183, "seek": 119720, "start": 1218.64, "end": 1224.88, "text": " incoming layer are multiplied by two, or if the signal amplitude is multiplied by two, the result", "tokens": [50364, 2107, 28263, 1088, 1027, 257, 955, 2712, 30, 440, 787, 551, 286, 478, 3650, 295, 307, 1228, 257, 2167, 2445, 2602, 295, 50804, 50804, 257, 3834, 3819, 337, 2452, 18161, 9590, 3665, 309, 3847, 1101, 13, 1042, 11, 370, 510, 311, 264, 1154, 51128, 51128, 365, 3834, 3819, 13, 16633, 3819, 575, 257, 3094, 12, 259, 4373, 294, 309, 11, 597, 1355, 498, 264, 17443, 295, 264, 51436, 51436, 22341, 4583, 366, 17207, 538, 732, 11, 420, 498, 264, 6358, 27433, 307, 17207, 538, 732, 11, 264, 1874, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.16014133328976837, "compression_ratio": 1.6196581196581197, "no_speech_prob": 1.1124422599095851e-05}, {"id": 184, "seek": 122488, "start": 1224.88, "end": 1229.7600000000002, "text": " on the output would be completely different, right? Because you will be, you know, the signal", "tokens": [50364, 322, 264, 5598, 576, 312, 2584, 819, 11, 558, 30, 1436, 291, 486, 312, 11, 291, 458, 11, 264, 6358, 50608, 50608, 576, 312, 544, 294, 264, 2107, 1889, 17409, 13, 407, 11, 291, 603, 483, 2584, 819, 5223, 295, 428, 4583, 13, 50908, 50968, 13813, 498, 291, 362, 257, 2445, 365, 787, 472, 3819, 11, 498, 291, 12972, 264, 4846, 538, 732, 11, 264, 5598, 2170, 51172, 51172, 611, 17207, 538, 732, 11, 291, 458, 11, 31111, 12577, 11, 457, 264, 6358, 12577, 307, 2489, 13, 407, 11, 457, 437, 286, 914, 51532, 51532, 281, 1029, 307, 11, 393, 291, 519, 295, 257, 2590, 689, 264, 3922, 295, 24433, 2445, 1027, 257, 955, 2649, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.14118967136415114, "compression_ratio": 1.8074074074074074, "no_speech_prob": 3.535340147209354e-05}, {"id": 185, "seek": 122488, "start": 1229.7600000000002, "end": 1235.7600000000002, "text": " would be more in the nonlinearity. So, you'll get completely different behavior of your layer.", "tokens": [50364, 322, 264, 5598, 576, 312, 2584, 819, 11, 558, 30, 1436, 291, 486, 312, 11, 291, 458, 11, 264, 6358, 50608, 50608, 576, 312, 544, 294, 264, 2107, 1889, 17409, 13, 407, 11, 291, 603, 483, 2584, 819, 5223, 295, 428, 4583, 13, 50908, 50968, 13813, 498, 291, 362, 257, 2445, 365, 787, 472, 3819, 11, 498, 291, 12972, 264, 4846, 538, 732, 11, 264, 5598, 2170, 51172, 51172, 611, 17207, 538, 732, 11, 291, 458, 11, 31111, 12577, 11, 457, 264, 6358, 12577, 307, 2489, 13, 407, 11, 457, 437, 286, 914, 51532, 51532, 281, 1029, 307, 11, 393, 291, 519, 295, 257, 2590, 689, 264, 3922, 295, 24433, 2445, 1027, 257, 955, 2649, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.14118967136415114, "compression_ratio": 1.8074074074074074, "no_speech_prob": 3.535340147209354e-05}, {"id": 186, "seek": 122488, "start": 1236.96, "end": 1241.0400000000002, "text": " Whereas if you have a function with only one King, if you multiply the input by two, the output gets", "tokens": [50364, 322, 264, 5598, 576, 312, 2584, 819, 11, 558, 30, 1436, 291, 486, 312, 11, 291, 458, 11, 264, 6358, 50608, 50608, 576, 312, 544, 294, 264, 2107, 1889, 17409, 13, 407, 11, 291, 603, 483, 2584, 819, 5223, 295, 428, 4583, 13, 50908, 50968, 13813, 498, 291, 362, 257, 2445, 365, 787, 472, 3819, 11, 498, 291, 12972, 264, 4846, 538, 732, 11, 264, 5598, 2170, 51172, 51172, 611, 17207, 538, 732, 11, 291, 458, 11, 31111, 12577, 11, 457, 264, 6358, 12577, 307, 2489, 13, 407, 11, 457, 437, 286, 914, 51532, 51532, 281, 1029, 307, 11, 393, 291, 519, 295, 257, 2590, 689, 264, 3922, 295, 24433, 2445, 1027, 257, 955, 2649, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.14118967136415114, "compression_ratio": 1.8074074074074074, "no_speech_prob": 3.535340147209354e-05}, {"id": 187, "seek": 122488, "start": 1241.0400000000002, "end": 1248.24, "text": " also multiplied by two, you know, modular bias, but the signal bias is fine. So, but what I mean", "tokens": [50364, 322, 264, 5598, 576, 312, 2584, 819, 11, 558, 30, 1436, 291, 486, 312, 11, 291, 458, 11, 264, 6358, 50608, 50608, 576, 312, 544, 294, 264, 2107, 1889, 17409, 13, 407, 11, 291, 603, 483, 2584, 819, 5223, 295, 428, 4583, 13, 50908, 50968, 13813, 498, 291, 362, 257, 2445, 365, 787, 472, 3819, 11, 498, 291, 12972, 264, 4846, 538, 732, 11, 264, 5598, 2170, 51172, 51172, 611, 17207, 538, 732, 11, 291, 458, 11, 31111, 12577, 11, 457, 264, 6358, 12577, 307, 2489, 13, 407, 11, 457, 437, 286, 914, 51532, 51532, 281, 1029, 307, 11, 393, 291, 519, 295, 257, 2590, 689, 264, 3922, 295, 24433, 2445, 1027, 257, 955, 2649, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.14118967136415114, "compression_ratio": 1.8074074074074074, "no_speech_prob": 3.535340147209354e-05}, {"id": 188, "seek": 122488, "start": 1248.24, "end": 1254.3200000000002, "text": " to ask is, can you think of a situation where the choice of activation function made a big difference", "tokens": [50364, 322, 264, 5598, 576, 312, 2584, 819, 11, 558, 30, 1436, 291, 486, 312, 11, 291, 458, 11, 264, 6358, 50608, 50608, 576, 312, 544, 294, 264, 2107, 1889, 17409, 13, 407, 11, 291, 603, 483, 2584, 819, 5223, 295, 428, 4583, 13, 50908, 50968, 13813, 498, 291, 362, 257, 2445, 365, 787, 472, 3819, 11, 498, 291, 12972, 264, 4846, 538, 732, 11, 264, 5598, 2170, 51172, 51172, 611, 17207, 538, 732, 11, 291, 458, 11, 31111, 12577, 11, 457, 264, 6358, 12577, 307, 2489, 13, 407, 11, 457, 437, 286, 914, 51532, 51532, 281, 1029, 307, 11, 393, 291, 519, 295, 257, 2590, 689, 264, 3922, 295, 24433, 2445, 1027, 257, 955, 2649, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.14118967136415114, "compression_ratio": 1.8074074074074074, "no_speech_prob": 3.535340147209354e-05}, {"id": 189, "seek": 125432, "start": 1254.32, "end": 1260.08, "text": " in the performance of the model, except for deep networks using Prud'Innu instead of Sigmoid?", "tokens": [50364, 294, 264, 3389, 295, 264, 2316, 11, 3993, 337, 2452, 9590, 1228, 2114, 532, 6, 4575, 16241, 2602, 295, 37763, 3280, 327, 30, 50652, 50852, 821, 307, 572, 1333, 295, 2674, 1867, 281, 341, 13, 1743, 11, 498, 291, 434, 516, 281, 764, 3202, 11, 51172, 51240, 291, 362, 281, 764, 2787, 41167, 13, 286, 914, 11, 291, 362, 572, 3922, 11, 558, 30, 286, 914, 11, 309, 311, 406, 411, 291, 362, 51452, 51452, 281, 764, 2787, 41167, 11, 457, 291, 528, 281, 362, 746, 689, 291, 483, 31994, 11, 558, 11, 281, 733, 295, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.18760099411010742, "compression_ratio": 1.6484018264840183, "no_speech_prob": 1.3630756257043686e-05}, {"id": 190, "seek": 125432, "start": 1264.08, "end": 1270.48, "text": " There is no sort of general answer to this. Like, if you're going to use attention,", "tokens": [50364, 294, 264, 3389, 295, 264, 2316, 11, 3993, 337, 2452, 9590, 1228, 2114, 532, 6, 4575, 16241, 2602, 295, 37763, 3280, 327, 30, 50652, 50852, 821, 307, 572, 1333, 295, 2674, 1867, 281, 341, 13, 1743, 11, 498, 291, 434, 516, 281, 764, 3202, 11, 51172, 51240, 291, 362, 281, 764, 2787, 41167, 13, 286, 914, 11, 291, 362, 572, 3922, 11, 558, 30, 286, 914, 11, 309, 311, 406, 411, 291, 362, 51452, 51452, 281, 764, 2787, 41167, 11, 457, 291, 528, 281, 362, 746, 689, 291, 483, 31994, 11, 558, 11, 281, 733, 295, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.18760099411010742, "compression_ratio": 1.6484018264840183, "no_speech_prob": 1.3630756257043686e-05}, {"id": 191, "seek": 125432, "start": 1271.84, "end": 1276.08, "text": " you have to use softmax. I mean, you have no choice, right? I mean, it's not like you have", "tokens": [50364, 294, 264, 3389, 295, 264, 2316, 11, 3993, 337, 2452, 9590, 1228, 2114, 532, 6, 4575, 16241, 2602, 295, 37763, 3280, 327, 30, 50652, 50852, 821, 307, 572, 1333, 295, 2674, 1867, 281, 341, 13, 1743, 11, 498, 291, 434, 516, 281, 764, 3202, 11, 51172, 51240, 291, 362, 281, 764, 2787, 41167, 13, 286, 914, 11, 291, 362, 572, 3922, 11, 558, 30, 286, 914, 11, 309, 311, 406, 411, 291, 362, 51452, 51452, 281, 764, 2787, 41167, 11, 457, 291, 528, 281, 362, 746, 689, 291, 483, 31994, 11, 558, 11, 281, 733, 295, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.18760099411010742, "compression_ratio": 1.6484018264840183, "no_speech_prob": 1.3630756257043686e-05}, {"id": 192, "seek": 125432, "start": 1276.08, "end": 1281.04, "text": " to use softmax, but you want to have something where you get coefficients, right, to kind of", "tokens": [50364, 294, 264, 3389, 295, 264, 2316, 11, 3993, 337, 2452, 9590, 1228, 2114, 532, 6, 4575, 16241, 2602, 295, 37763, 3280, 327, 30, 50652, 50852, 821, 307, 572, 1333, 295, 2674, 1867, 281, 341, 13, 1743, 11, 498, 291, 434, 516, 281, 764, 3202, 11, 51172, 51240, 291, 362, 281, 764, 2787, 41167, 13, 286, 914, 11, 291, 362, 572, 3922, 11, 558, 30, 286, 914, 11, 309, 311, 406, 411, 291, 362, 51452, 51452, 281, 764, 2787, 41167, 11, 457, 291, 528, 281, 362, 746, 689, 291, 483, 31994, 11, 558, 11, 281, 733, 295, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.18760099411010742, "compression_ratio": 1.6484018264840183, "no_speech_prob": 1.3630756257043686e-05}, {"id": 193, "seek": 128104, "start": 1281.04, "end": 1286.8799999999999, "text": " focus the attention of the system on, or to kind of spread the attention of the system", "tokens": [50364, 1879, 264, 3202, 295, 264, 1185, 322, 11, 420, 281, 733, 295, 3974, 264, 3202, 295, 264, 1185, 50656, 50728, 293, 406, 2089, 309, 281, 17470, 11, 597, 307, 281, 1689, 3202, 281, 3866, 721, 412, 472, 565, 11, 50924, 50924, 291, 362, 281, 362, 512, 1333, 295, 2710, 2144, 295, 264, 31994, 300, 808, 484, 295, 264, 3202, 51200, 51200, 1185, 11, 558, 30, 407, 11, 5646, 294, 881, 3202, 3652, 11, 411, 294, 4088, 433, 293, 1507, 11, 51496, 51524, 264, 31994, 366, 4678, 807, 2787, 41167, 13, 407, 11, 291, 483, 257, 3840, 295, 31994, 300, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.07988564019064301, "compression_ratio": 1.951111111111111, "no_speech_prob": 1.643973337195348e-05}, {"id": 194, "seek": 128104, "start": 1288.32, "end": 1292.24, "text": " and not allow it to cheat, which is to pay attention to multiple things at one time,", "tokens": [50364, 1879, 264, 3202, 295, 264, 1185, 322, 11, 420, 281, 733, 295, 3974, 264, 3202, 295, 264, 1185, 50656, 50728, 293, 406, 2089, 309, 281, 17470, 11, 597, 307, 281, 1689, 3202, 281, 3866, 721, 412, 472, 565, 11, 50924, 50924, 291, 362, 281, 362, 512, 1333, 295, 2710, 2144, 295, 264, 31994, 300, 808, 484, 295, 264, 3202, 51200, 51200, 1185, 11, 558, 30, 407, 11, 5646, 294, 881, 3202, 3652, 11, 411, 294, 4088, 433, 293, 1507, 11, 51496, 51524, 264, 31994, 366, 4678, 807, 2787, 41167, 13, 407, 11, 291, 483, 257, 3840, 295, 31994, 300, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.07988564019064301, "compression_ratio": 1.951111111111111, "no_speech_prob": 1.643973337195348e-05}, {"id": 195, "seek": 128104, "start": 1292.24, "end": 1297.76, "text": " you have to have some sort of normalization of the coefficients that come out of the attention", "tokens": [50364, 1879, 264, 3202, 295, 264, 1185, 322, 11, 420, 281, 733, 295, 3974, 264, 3202, 295, 264, 1185, 50656, 50728, 293, 406, 2089, 309, 281, 17470, 11, 597, 307, 281, 1689, 3202, 281, 3866, 721, 412, 472, 565, 11, 50924, 50924, 291, 362, 281, 362, 512, 1333, 295, 2710, 2144, 295, 264, 31994, 300, 808, 484, 295, 264, 3202, 51200, 51200, 1185, 11, 558, 30, 407, 11, 5646, 294, 881, 3202, 3652, 11, 411, 294, 4088, 433, 293, 1507, 11, 51496, 51524, 264, 31994, 366, 4678, 807, 2787, 41167, 13, 407, 11, 291, 483, 257, 3840, 295, 31994, 300, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.07988564019064301, "compression_ratio": 1.951111111111111, "no_speech_prob": 1.643973337195348e-05}, {"id": 196, "seek": 128104, "start": 1297.76, "end": 1303.68, "text": " system, right? So, normally in most attention systems, like in transformers and stuff,", "tokens": [50364, 1879, 264, 3202, 295, 264, 1185, 322, 11, 420, 281, 733, 295, 3974, 264, 3202, 295, 264, 1185, 50656, 50728, 293, 406, 2089, 309, 281, 17470, 11, 597, 307, 281, 1689, 3202, 281, 3866, 721, 412, 472, 565, 11, 50924, 50924, 291, 362, 281, 362, 512, 1333, 295, 2710, 2144, 295, 264, 31994, 300, 808, 484, 295, 264, 3202, 51200, 51200, 1185, 11, 558, 30, 407, 11, 5646, 294, 881, 3202, 3652, 11, 411, 294, 4088, 433, 293, 1507, 11, 51496, 51524, 264, 31994, 366, 4678, 807, 2787, 41167, 13, 407, 11, 291, 483, 257, 3840, 295, 31994, 300, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.07988564019064301, "compression_ratio": 1.951111111111111, "no_speech_prob": 1.643973337195348e-05}, {"id": 197, "seek": 128104, "start": 1304.24, "end": 1309.28, "text": " the coefficients are passed through softmax. So, you get a bunch of coefficients that", "tokens": [50364, 1879, 264, 3202, 295, 264, 1185, 322, 11, 420, 281, 733, 295, 3974, 264, 3202, 295, 264, 1185, 50656, 50728, 293, 406, 2089, 309, 281, 17470, 11, 597, 307, 281, 1689, 3202, 281, 3866, 721, 412, 472, 565, 11, 50924, 50924, 291, 362, 281, 362, 512, 1333, 295, 2710, 2144, 295, 264, 31994, 300, 808, 484, 295, 264, 3202, 51200, 51200, 1185, 11, 558, 30, 407, 11, 5646, 294, 881, 3202, 3652, 11, 411, 294, 4088, 433, 293, 1507, 11, 51496, 51524, 264, 31994, 366, 4678, 807, 2787, 41167, 13, 407, 11, 291, 483, 257, 3840, 295, 31994, 300, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.07988564019064301, "compression_ratio": 1.951111111111111, "no_speech_prob": 1.643973337195348e-05}, {"id": 198, "seek": 130928, "start": 1309.28, "end": 1315.2, "text": " are between 0 and 1 and sum to 1. And so, that causes the system to have to pay attention to", "tokens": [50364, 366, 1296, 1958, 293, 502, 293, 2408, 281, 502, 13, 400, 370, 11, 300, 7700, 264, 1185, 281, 362, 281, 1689, 3202, 281, 50660, 50772, 257, 1359, 1230, 295, 721, 11, 558, 30, 467, 393, 787, 18089, 264, 31994, 322, 257, 1359, 1230, 50952, 50952, 295, 4754, 293, 309, 575, 281, 3974, 309, 11, 558, 30, 821, 366, 661, 2098, 281, 360, 2710, 2144, 13, 51324, 51356, 509, 393, 360, 11, 293, 294, 1186, 11, 456, 307, 746, 300, 311, 2085, 365, 2787, 41167, 2710, 2144, 337, 51612, 51712], "temperature": 0.0, "avg_logprob": -0.12427923181554773, "compression_ratio": 1.628440366972477, "no_speech_prob": 6.961846793274162e-06}, {"id": 199, "seek": 130928, "start": 1317.44, "end": 1321.04, "text": " a small number of things, right? It can only concentrate the coefficients on a small number", "tokens": [50364, 366, 1296, 1958, 293, 502, 293, 2408, 281, 502, 13, 400, 370, 11, 300, 7700, 264, 1185, 281, 362, 281, 1689, 3202, 281, 50660, 50772, 257, 1359, 1230, 295, 721, 11, 558, 30, 467, 393, 787, 18089, 264, 31994, 322, 257, 1359, 1230, 50952, 50952, 295, 4754, 293, 309, 575, 281, 3974, 309, 11, 558, 30, 821, 366, 661, 2098, 281, 360, 2710, 2144, 13, 51324, 51356, 509, 393, 360, 11, 293, 294, 1186, 11, 456, 307, 746, 300, 311, 2085, 365, 2787, 41167, 2710, 2144, 337, 51612, 51712], "temperature": 0.0, "avg_logprob": -0.12427923181554773, "compression_ratio": 1.628440366972477, "no_speech_prob": 6.961846793274162e-06}, {"id": 200, "seek": 130928, "start": 1321.04, "end": 1328.48, "text": " of items and it has to spread it, right? There are other ways to do normalization.", "tokens": [50364, 366, 1296, 1958, 293, 502, 293, 2408, 281, 502, 13, 400, 370, 11, 300, 7700, 264, 1185, 281, 362, 281, 1689, 3202, 281, 50660, 50772, 257, 1359, 1230, 295, 721, 11, 558, 30, 467, 393, 787, 18089, 264, 31994, 322, 257, 1359, 1230, 50952, 50952, 295, 4754, 293, 309, 575, 281, 3974, 309, 11, 558, 30, 821, 366, 661, 2098, 281, 360, 2710, 2144, 13, 51324, 51356, 509, 393, 360, 11, 293, 294, 1186, 11, 456, 307, 746, 300, 311, 2085, 365, 2787, 41167, 2710, 2144, 337, 51612, 51712], "temperature": 0.0, "avg_logprob": -0.12427923181554773, "compression_ratio": 1.628440366972477, "no_speech_prob": 6.961846793274162e-06}, {"id": 201, "seek": 130928, "start": 1329.12, "end": 1334.24, "text": " You can do, and in fact, there is something that's wrong with softmax normalization for", "tokens": [50364, 366, 1296, 1958, 293, 502, 293, 2408, 281, 502, 13, 400, 370, 11, 300, 7700, 264, 1185, 281, 362, 281, 1689, 3202, 281, 50660, 50772, 257, 1359, 1230, 295, 721, 11, 558, 30, 467, 393, 787, 18089, 264, 31994, 322, 257, 1359, 1230, 50952, 50952, 295, 4754, 293, 309, 575, 281, 3974, 309, 11, 558, 30, 821, 366, 661, 2098, 281, 360, 2710, 2144, 13, 51324, 51356, 509, 393, 360, 11, 293, 294, 1186, 11, 456, 307, 746, 300, 311, 2085, 365, 2787, 41167, 2710, 2144, 337, 51612, 51712], "temperature": 0.0, "avg_logprob": -0.12427923181554773, "compression_ratio": 1.628440366972477, "no_speech_prob": 6.961846793274162e-06}, {"id": 202, "seek": 133424, "start": 1334.24, "end": 1340.8, "text": " transformers or for attention, which is that if you want a coefficient coming out of a softmax to be", "tokens": [50364, 4088, 433, 420, 337, 3202, 11, 597, 307, 300, 498, 291, 528, 257, 17619, 1348, 484, 295, 257, 2787, 41167, 281, 312, 50692, 50692, 1998, 281, 1958, 11, 291, 643, 264, 4846, 281, 312, 1998, 281, 3175, 13202, 11, 1392, 11, 420, 281, 312, 31308, 4356, 51076, 51076, 813, 264, 6443, 472, 11, 558, 30, 1133, 291, 352, 666, 257, 2787, 41167, 11, 472, 5598, 11, 264, 6443, 4846, 307, 516, 51420, 51420, 281, 3082, 264, 11760, 5598, 281, 312, 2416, 13, 583, 498, 291, 528, 300, 5598, 281, 312, 1998, 281, 502, 293, 439, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.17073945324830334, "compression_ratio": 1.790909090909091, "no_speech_prob": 4.936843652103562e-06}, {"id": 203, "seek": 133424, "start": 1340.8, "end": 1348.48, "text": " close to 0, you need the input to be close to minus infinity, okay, or to be considerably smaller", "tokens": [50364, 4088, 433, 420, 337, 3202, 11, 597, 307, 300, 498, 291, 528, 257, 17619, 1348, 484, 295, 257, 2787, 41167, 281, 312, 50692, 50692, 1998, 281, 1958, 11, 291, 643, 264, 4846, 281, 312, 1998, 281, 3175, 13202, 11, 1392, 11, 420, 281, 312, 31308, 4356, 51076, 51076, 813, 264, 6443, 472, 11, 558, 30, 1133, 291, 352, 666, 257, 2787, 41167, 11, 472, 5598, 11, 264, 6443, 4846, 307, 516, 51420, 51420, 281, 3082, 264, 11760, 5598, 281, 312, 2416, 13, 583, 498, 291, 528, 300, 5598, 281, 312, 1998, 281, 502, 293, 439, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.17073945324830334, "compression_ratio": 1.790909090909091, "no_speech_prob": 4.936843652103562e-06}, {"id": 204, "seek": 133424, "start": 1348.48, "end": 1355.36, "text": " than the largest one, right? When you go into a softmax, one output, the largest input is going", "tokens": [50364, 4088, 433, 420, 337, 3202, 11, 597, 307, 300, 498, 291, 528, 257, 17619, 1348, 484, 295, 257, 2787, 41167, 281, 312, 50692, 50692, 1998, 281, 1958, 11, 291, 643, 264, 4846, 281, 312, 1998, 281, 3175, 13202, 11, 1392, 11, 420, 281, 312, 31308, 4356, 51076, 51076, 813, 264, 6443, 472, 11, 558, 30, 1133, 291, 352, 666, 257, 2787, 41167, 11, 472, 5598, 11, 264, 6443, 4846, 307, 516, 51420, 51420, 281, 3082, 264, 11760, 5598, 281, 312, 2416, 13, 583, 498, 291, 528, 300, 5598, 281, 312, 1998, 281, 502, 293, 439, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.17073945324830334, "compression_ratio": 1.790909090909091, "no_speech_prob": 4.936843652103562e-06}, {"id": 205, "seek": 133424, "start": 1355.36, "end": 1361.04, "text": " to cause the corresponding output to be large. But if you want that output to be close to 1 and all", "tokens": [50364, 4088, 433, 420, 337, 3202, 11, 597, 307, 300, 498, 291, 528, 257, 17619, 1348, 484, 295, 257, 2787, 41167, 281, 312, 50692, 50692, 1998, 281, 1958, 11, 291, 643, 264, 4846, 281, 312, 1998, 281, 3175, 13202, 11, 1392, 11, 420, 281, 312, 31308, 4356, 51076, 51076, 813, 264, 6443, 472, 11, 558, 30, 1133, 291, 352, 666, 257, 2787, 41167, 11, 472, 5598, 11, 264, 6443, 4846, 307, 516, 51420, 51420, 281, 3082, 264, 11760, 5598, 281, 312, 2416, 13, 583, 498, 291, 528, 300, 5598, 281, 312, 1998, 281, 502, 293, 439, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.17073945324830334, "compression_ratio": 1.790909090909091, "no_speech_prob": 4.936843652103562e-06}, {"id": 206, "seek": 136104, "start": 1361.04, "end": 1366.08, "text": " the other ones to be close to 0, you basically want this input to be extremely large and all the", "tokens": [50364, 264, 661, 2306, 281, 312, 1998, 281, 1958, 11, 291, 1936, 528, 341, 4846, 281, 312, 4664, 2416, 293, 439, 264, 50616, 50616, 661, 2306, 281, 312, 2416, 293, 3671, 11, 1392, 30, 823, 11, 300, 393, 312, 257, 1154, 562, 437, 291, 366, 15866, 51328, 51328, 412, 264, 4846, 366, 5893, 3383, 11, 570, 264, 1874, 307, 300, 264, 12889, 636, 337, 257, 1185, 51668, 51728], "temperature": 0.0, "avg_logprob": -0.1787732304006383, "compression_ratio": 1.5698324022346368, "no_speech_prob": 7.571042601739464e-07}, {"id": 207, "seek": 136104, "start": 1366.08, "end": 1380.32, "text": " other ones to be large and negative, okay? Now, that can be a problem when what you are computing", "tokens": [50364, 264, 661, 2306, 281, 312, 1998, 281, 1958, 11, 291, 1936, 528, 341, 4846, 281, 312, 4664, 2416, 293, 439, 264, 50616, 50616, 661, 2306, 281, 312, 2416, 293, 3671, 11, 1392, 30, 823, 11, 300, 393, 312, 257, 1154, 562, 437, 291, 366, 15866, 51328, 51328, 412, 264, 4846, 366, 5893, 3383, 11, 570, 264, 1874, 307, 300, 264, 12889, 636, 337, 257, 1185, 51668, 51728], "temperature": 0.0, "avg_logprob": -0.1787732304006383, "compression_ratio": 1.5698324022346368, "no_speech_prob": 7.571042601739464e-07}, {"id": 208, "seek": 136104, "start": 1380.32, "end": 1387.12, "text": " at the input are dot products, because the result is that the easiest way for a system", "tokens": [50364, 264, 661, 2306, 281, 312, 1998, 281, 1958, 11, 291, 1936, 528, 341, 4846, 281, 312, 4664, 2416, 293, 439, 264, 50616, 50616, 661, 2306, 281, 312, 2416, 293, 3671, 11, 1392, 30, 823, 11, 300, 393, 312, 257, 1154, 562, 437, 291, 366, 15866, 51328, 51328, 412, 264, 4846, 366, 5893, 3383, 11, 570, 264, 1874, 307, 300, 264, 12889, 636, 337, 257, 1185, 51668, 51728], "temperature": 0.0, "avg_logprob": -0.1787732304006383, "compression_ratio": 1.5698324022346368, "no_speech_prob": 7.571042601739464e-07}, {"id": 209, "seek": 138712, "start": 1387.12, "end": 1392.08, "text": " to produce a small dot product is to have two vectors that are orthogonal to each other,", "tokens": [50364, 281, 5258, 257, 1359, 5893, 1674, 307, 281, 362, 732, 18875, 300, 366, 41488, 281, 1184, 661, 11, 50612, 50612, 294, 597, 1389, 264, 5893, 1674, 307, 1958, 13, 759, 291, 13466, 300, 264, 5893, 1674, 820, 312, 588, 11, 588, 1359, 11, 50832, 50860, 550, 2139, 291, 362, 281, 652, 264, 732, 18875, 1936, 935, 294, 6182, 11095, 51196, 51196, 293, 291, 362, 281, 652, 552, 588, 938, 13, 400, 300, 311, 406, 370, 869, 13, 400, 370, 11, 51476, 51576], "temperature": 0.0, "avg_logprob": -0.2064319338117327, "compression_ratio": 1.7853403141361257, "no_speech_prob": 1.1124793672934175e-05}, {"id": 210, "seek": 138712, "start": 1392.08, "end": 1396.4799999999998, "text": " in which case the dot product is 0. If you insist that the dot product should be very, very small,", "tokens": [50364, 281, 5258, 257, 1359, 5893, 1674, 307, 281, 362, 732, 18875, 300, 366, 41488, 281, 1184, 661, 11, 50612, 50612, 294, 597, 1389, 264, 5893, 1674, 307, 1958, 13, 759, 291, 13466, 300, 264, 5893, 1674, 820, 312, 588, 11, 588, 1359, 11, 50832, 50860, 550, 2139, 291, 362, 281, 652, 264, 732, 18875, 1936, 935, 294, 6182, 11095, 51196, 51196, 293, 291, 362, 281, 652, 552, 588, 938, 13, 400, 300, 311, 406, 370, 869, 13, 400, 370, 11, 51476, 51576], "temperature": 0.0, "avg_logprob": -0.2064319338117327, "compression_ratio": 1.7853403141361257, "no_speech_prob": 1.1124793672934175e-05}, {"id": 211, "seek": 138712, "start": 1397.04, "end": 1403.76, "text": " then either you have to make the two vectors basically point in opposite directions", "tokens": [50364, 281, 5258, 257, 1359, 5893, 1674, 307, 281, 362, 732, 18875, 300, 366, 41488, 281, 1184, 661, 11, 50612, 50612, 294, 597, 1389, 264, 5893, 1674, 307, 1958, 13, 759, 291, 13466, 300, 264, 5893, 1674, 820, 312, 588, 11, 588, 1359, 11, 50832, 50860, 550, 2139, 291, 362, 281, 652, 264, 732, 18875, 1936, 935, 294, 6182, 11095, 51196, 51196, 293, 291, 362, 281, 652, 552, 588, 938, 13, 400, 300, 311, 406, 370, 869, 13, 400, 370, 11, 51476, 51576], "temperature": 0.0, "avg_logprob": -0.2064319338117327, "compression_ratio": 1.7853403141361257, "no_speech_prob": 1.1124793672934175e-05}, {"id": 212, "seek": 138712, "start": 1403.76, "end": 1409.36, "text": " and you have to make them very long. And that's not so great. And so,", "tokens": [50364, 281, 5258, 257, 1359, 5893, 1674, 307, 281, 362, 732, 18875, 300, 366, 41488, 281, 1184, 661, 11, 50612, 50612, 294, 597, 1389, 264, 5893, 1674, 307, 1958, 13, 759, 291, 13466, 300, 264, 5893, 1674, 820, 312, 588, 11, 588, 1359, 11, 50832, 50860, 550, 2139, 291, 362, 281, 652, 264, 732, 18875, 1936, 935, 294, 6182, 11095, 51196, 51196, 293, 291, 362, 281, 652, 552, 588, 938, 13, 400, 300, 311, 406, 370, 869, 13, 400, 370, 11, 51476, 51576], "temperature": 0.0, "avg_logprob": -0.2064319338117327, "compression_ratio": 1.7853403141361257, "no_speech_prob": 1.1124793672934175e-05}, {"id": 213, "seek": 140936, "start": 1409.36, "end": 1416.1599999999999, "text": " using softmax for attention basically limits the contrast that you're going to have between", "tokens": [50364, 1228, 2787, 41167, 337, 3202, 1936, 10406, 264, 8712, 300, 291, 434, 516, 281, 362, 1296, 50704, 50704, 264, 31994, 11, 597, 307, 406, 4725, 257, 665, 551, 13, 407, 11, 912, 551, 337, 441, 6840, 44, 11, 290, 770, 11, 51172, 51244, 18680, 1753, 36170, 11, 1030, 11458, 13, 509, 643, 4556, 3280, 3742, 456, 11, 570, 291, 643, 31994, 300, 366, 51504, 51504], "temperature": 0.0, "avg_logprob": -0.20874581408144824, "compression_ratio": 1.5055555555555555, "no_speech_prob": 6.240667062229477e-06}, {"id": 214, "seek": 140936, "start": 1416.1599999999999, "end": 1425.52, "text": " the coefficients, which is not necessarily a good thing. So, same thing for LSTM, gated,", "tokens": [50364, 1228, 2787, 41167, 337, 3202, 1936, 10406, 264, 8712, 300, 291, 434, 516, 281, 362, 1296, 50704, 50704, 264, 31994, 11, 597, 307, 406, 4725, 257, 665, 551, 13, 407, 11, 912, 551, 337, 441, 6840, 44, 11, 290, 770, 11, 51172, 51244, 18680, 1753, 36170, 11, 1030, 11458, 13, 509, 643, 4556, 3280, 3742, 456, 11, 570, 291, 643, 31994, 300, 366, 51504, 51504], "temperature": 0.0, "avg_logprob": -0.20874581408144824, "compression_ratio": 1.5055555555555555, "no_speech_prob": 6.240667062229477e-06}, {"id": 215, "seek": 140936, "start": 1426.9599999999998, "end": 1432.1599999999999, "text": " recurrent nets, et cetera. You need sigmoids there, because you need coefficients that are", "tokens": [50364, 1228, 2787, 41167, 337, 3202, 1936, 10406, 264, 8712, 300, 291, 434, 516, 281, 362, 1296, 50704, 50704, 264, 31994, 11, 597, 307, 406, 4725, 257, 665, 551, 13, 407, 11, 912, 551, 337, 441, 6840, 44, 11, 290, 770, 11, 51172, 51244, 18680, 1753, 36170, 11, 1030, 11458, 13, 509, 643, 4556, 3280, 3742, 456, 11, 570, 291, 643, 31994, 300, 366, 51504, 51504], "temperature": 0.0, "avg_logprob": -0.20874581408144824, "compression_ratio": 1.5055555555555555, "no_speech_prob": 6.240667062229477e-06}, {"id": 216, "seek": 143216, "start": 1432.16, "end": 1439.8400000000001, "text": " between 0 and 1 that either reset the memory cell or make it a pass through so that it keeps", "tokens": [50364, 1296, 1958, 293, 502, 300, 2139, 14322, 264, 4675, 2815, 420, 652, 309, 257, 1320, 807, 370, 300, 309, 5965, 50748, 50748, 1080, 3894, 4675, 420, 733, 295, 2464, 264, 777, 4846, 294, 309, 13, 407, 11, 456, 309, 311, 1481, 281, 362, 51076, 51124, 364, 5598, 300, 21716, 15684, 1296, 1958, 293, 502, 13, 821, 291, 362, 572, 3922, 13, 407, 11, 286, 914, 11, 51428, 51460, 286, 500, 380, 519, 291, 393, 584, 445, 11, 291, 458, 11, 498, 291, 362, 257, 588, 1359, 5893, 1674, 11, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.33583934332734794, "compression_ratio": 1.6036866359447004, "no_speech_prob": 1.99470309780736e-06}, {"id": 217, "seek": 143216, "start": 1439.8400000000001, "end": 1446.4, "text": " its previous memory or kind of write the new input in it. So, there it's nice to have", "tokens": [50364, 1296, 1958, 293, 502, 300, 2139, 14322, 264, 4675, 2815, 420, 652, 309, 257, 1320, 807, 370, 300, 309, 5965, 50748, 50748, 1080, 3894, 4675, 420, 733, 295, 2464, 264, 777, 4846, 294, 309, 13, 407, 11, 456, 309, 311, 1481, 281, 362, 51076, 51124, 364, 5598, 300, 21716, 15684, 1296, 1958, 293, 502, 13, 821, 291, 362, 572, 3922, 13, 407, 11, 286, 914, 11, 51428, 51460, 286, 500, 380, 519, 291, 393, 584, 445, 11, 291, 458, 11, 498, 291, 362, 257, 588, 1359, 5893, 1674, 11, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.33583934332734794, "compression_ratio": 1.6036866359447004, "no_speech_prob": 1.99470309780736e-06}, {"id": 218, "seek": 143216, "start": 1447.3600000000001, "end": 1453.44, "text": " an output that varies continuously between 0 and 1. There you have no choice. So, I mean,", "tokens": [50364, 1296, 1958, 293, 502, 300, 2139, 14322, 264, 4675, 2815, 420, 652, 309, 257, 1320, 807, 370, 300, 309, 5965, 50748, 50748, 1080, 3894, 4675, 420, 733, 295, 2464, 264, 777, 4846, 294, 309, 13, 407, 11, 456, 309, 311, 1481, 281, 362, 51076, 51124, 364, 5598, 300, 21716, 15684, 1296, 1958, 293, 502, 13, 821, 291, 362, 572, 3922, 13, 407, 11, 286, 914, 11, 51428, 51460, 286, 500, 380, 519, 291, 393, 584, 445, 11, 291, 458, 11, 498, 291, 362, 257, 588, 1359, 5893, 1674, 11, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.33583934332734794, "compression_ratio": 1.6036866359447004, "no_speech_prob": 1.99470309780736e-06}, {"id": 219, "seek": 143216, "start": 1454.0800000000002, "end": 1457.52, "text": " I don't think you can say just, you know, if you have a very small dot product,", "tokens": [50364, 1296, 1958, 293, 502, 300, 2139, 14322, 264, 4675, 2815, 420, 652, 309, 257, 1320, 807, 370, 300, 309, 5965, 50748, 50748, 1080, 3894, 4675, 420, 733, 295, 2464, 264, 777, 4846, 294, 309, 13, 407, 11, 456, 309, 311, 1481, 281, 362, 51076, 51124, 364, 5598, 300, 21716, 15684, 1296, 1958, 293, 502, 13, 821, 291, 362, 572, 3922, 13, 407, 11, 286, 914, 11, 51428, 51460, 286, 500, 380, 519, 291, 393, 584, 445, 11, 291, 458, 11, 498, 291, 362, 257, 588, 1359, 5893, 1674, 11, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.33583934332734794, "compression_ratio": 1.6036866359447004, "no_speech_prob": 1.99470309780736e-06}, {"id": 220, "seek": 145752, "start": 1457.52, "end": 1463.04, "text": " you have no choice. So, I mean, I don't think you can say just, you know, in generic term,", "tokens": [50364, 291, 362, 572, 3922, 13, 407, 11, 286, 914, 11, 286, 500, 380, 519, 291, 393, 584, 445, 11, 291, 458, 11, 294, 19577, 1433, 11, 50640, 50704, 291, 458, 11, 341, 2107, 12, 1889, 17409, 307, 1101, 813, 341, 661, 472, 13, 821, 366, 1629, 3331, 689, 309, 50900, 50900, 27152, 1101, 13, 821, 366, 1629, 3331, 689, 309, 21680, 977, 291, 490, 1419, 281, 5883, 1125, 6108, 13, 51128, 51180, 821, 366, 1629, 3331, 689, 309, 1985, 1101, 498, 291, 362, 3195, 295, 7914, 11, 411, 11, 291, 458, 11, 51376, 51376, 2167, 12, 42427, 6828, 589, 1101, 498, 291, 362, 3195, 295, 7914, 11, 1101, 813, 4556, 3280, 327, 12, 4092, 6828, 13, 51596, 51644], "temperature": 0.0, "avg_logprob": -0.11533232529958089, "compression_ratio": 2.0616740088105727, "no_speech_prob": 8.80073548614746e-06}, {"id": 221, "seek": 145752, "start": 1464.32, "end": 1468.24, "text": " you know, this non-linearity is better than this other one. There are certain cases where it", "tokens": [50364, 291, 362, 572, 3922, 13, 407, 11, 286, 914, 11, 286, 500, 380, 519, 291, 393, 584, 445, 11, 291, 458, 11, 294, 19577, 1433, 11, 50640, 50704, 291, 458, 11, 341, 2107, 12, 1889, 17409, 307, 1101, 813, 341, 661, 472, 13, 821, 366, 1629, 3331, 689, 309, 50900, 50900, 27152, 1101, 13, 821, 366, 1629, 3331, 689, 309, 21680, 977, 291, 490, 1419, 281, 5883, 1125, 6108, 13, 51128, 51180, 821, 366, 1629, 3331, 689, 309, 1985, 1101, 498, 291, 362, 3195, 295, 7914, 11, 411, 11, 291, 458, 11, 51376, 51376, 2167, 12, 42427, 6828, 589, 1101, 498, 291, 362, 3195, 295, 7914, 11, 1101, 813, 4556, 3280, 327, 12, 4092, 6828, 13, 51596, 51644], "temperature": 0.0, "avg_logprob": -0.11533232529958089, "compression_ratio": 2.0616740088105727, "no_speech_prob": 8.80073548614746e-06}, {"id": 222, "seek": 145752, "start": 1468.24, "end": 1472.8, "text": " learns better. There are certain cases where it relieves you from having to initialize properly.", "tokens": [50364, 291, 362, 572, 3922, 13, 407, 11, 286, 914, 11, 286, 500, 380, 519, 291, 393, 584, 445, 11, 291, 458, 11, 294, 19577, 1433, 11, 50640, 50704, 291, 458, 11, 341, 2107, 12, 1889, 17409, 307, 1101, 813, 341, 661, 472, 13, 821, 366, 1629, 3331, 689, 309, 50900, 50900, 27152, 1101, 13, 821, 366, 1629, 3331, 689, 309, 21680, 977, 291, 490, 1419, 281, 5883, 1125, 6108, 13, 51128, 51180, 821, 366, 1629, 3331, 689, 309, 1985, 1101, 498, 291, 362, 3195, 295, 7914, 11, 411, 11, 291, 458, 11, 51376, 51376, 2167, 12, 42427, 6828, 589, 1101, 498, 291, 362, 3195, 295, 7914, 11, 1101, 813, 4556, 3280, 327, 12, 4092, 6828, 13, 51596, 51644], "temperature": 0.0, "avg_logprob": -0.11533232529958089, "compression_ratio": 2.0616740088105727, "no_speech_prob": 8.80073548614746e-06}, {"id": 223, "seek": 145752, "start": 1473.84, "end": 1477.76, "text": " There are certain cases where it works better if you have lots of layers, like, you know,", "tokens": [50364, 291, 362, 572, 3922, 13, 407, 11, 286, 914, 11, 286, 500, 380, 519, 291, 393, 584, 445, 11, 291, 458, 11, 294, 19577, 1433, 11, 50640, 50704, 291, 458, 11, 341, 2107, 12, 1889, 17409, 307, 1101, 813, 341, 661, 472, 13, 821, 366, 1629, 3331, 689, 309, 50900, 50900, 27152, 1101, 13, 821, 366, 1629, 3331, 689, 309, 21680, 977, 291, 490, 1419, 281, 5883, 1125, 6108, 13, 51128, 51180, 821, 366, 1629, 3331, 689, 309, 1985, 1101, 498, 291, 362, 3195, 295, 7914, 11, 411, 11, 291, 458, 11, 51376, 51376, 2167, 12, 42427, 6828, 589, 1101, 498, 291, 362, 3195, 295, 7914, 11, 1101, 813, 4556, 3280, 327, 12, 4092, 6828, 13, 51596, 51644], "temperature": 0.0, "avg_logprob": -0.11533232529958089, "compression_ratio": 2.0616740088105727, "no_speech_prob": 8.80073548614746e-06}, {"id": 224, "seek": 145752, "start": 1477.76, "end": 1482.16, "text": " single-kick functions work better if you have lots of layers, better than sigmoid-like functions.", "tokens": [50364, 291, 362, 572, 3922, 13, 407, 11, 286, 914, 11, 286, 500, 380, 519, 291, 393, 584, 445, 11, 291, 458, 11, 294, 19577, 1433, 11, 50640, 50704, 291, 458, 11, 341, 2107, 12, 1889, 17409, 307, 1101, 813, 341, 661, 472, 13, 821, 366, 1629, 3331, 689, 309, 50900, 50900, 27152, 1101, 13, 821, 366, 1629, 3331, 689, 309, 21680, 977, 291, 490, 1419, 281, 5883, 1125, 6108, 13, 51128, 51180, 821, 366, 1629, 3331, 689, 309, 1985, 1101, 498, 291, 362, 3195, 295, 7914, 11, 411, 11, 291, 458, 11, 51376, 51376, 2167, 12, 42427, 6828, 589, 1101, 498, 291, 362, 3195, 295, 7914, 11, 1101, 813, 4556, 3280, 327, 12, 4092, 6828, 13, 51596, 51644], "temperature": 0.0, "avg_logprob": -0.11533232529958089, "compression_ratio": 2.0616740088105727, "no_speech_prob": 8.80073548614746e-06}, {"id": 225, "seek": 148216, "start": 1482.16, "end": 1486.8000000000002, "text": " There's no kind of, there's no simple answer, basically.", "tokens": [50364, 821, 311, 572, 733, 295, 11, 456, 311, 572, 2199, 1867, 11, 1936, 13, 50596, 50596, 286, 632, 257, 1168, 445, 8595, 264, 2674, 7300, 1296, 257, 2107, 12, 28263, 24433, 50932, 50932, 300, 575, 350, 16431, 5717, 257, 5508, 2107, 12, 28263, 24433, 13, 1119, 456, 1333, 295, 604, 2674, 51316, 51316, 1778, 420, 4978, 281, 983, 321, 576, 4382, 281, 362, 350, 16431, 294, 264, 2445, 420, 406, 30, 51588, 51620], "temperature": 0.0, "avg_logprob": -0.11364014943440755, "compression_ratio": 1.5863874345549738, "no_speech_prob": 6.107983062975109e-05}, {"id": 226, "seek": 148216, "start": 1486.8000000000002, "end": 1493.52, "text": " I had a question just regarding the general differences between a non-linear activation", "tokens": [50364, 821, 311, 572, 733, 295, 11, 456, 311, 572, 2199, 1867, 11, 1936, 13, 50596, 50596, 286, 632, 257, 1168, 445, 8595, 264, 2674, 7300, 1296, 257, 2107, 12, 28263, 24433, 50932, 50932, 300, 575, 350, 16431, 5717, 257, 5508, 2107, 12, 28263, 24433, 13, 1119, 456, 1333, 295, 604, 2674, 51316, 51316, 1778, 420, 4978, 281, 983, 321, 576, 4382, 281, 362, 350, 16431, 294, 264, 2445, 420, 406, 30, 51588, 51620], "temperature": 0.0, "avg_logprob": -0.11364014943440755, "compression_ratio": 1.5863874345549738, "no_speech_prob": 6.107983062975109e-05}, {"id": 227, "seek": 148216, "start": 1493.52, "end": 1501.2, "text": " that has kinks versus a smooth non-linear activation. Is there sort of any general", "tokens": [50364, 821, 311, 572, 733, 295, 11, 456, 311, 572, 2199, 1867, 11, 1936, 13, 50596, 50596, 286, 632, 257, 1168, 445, 8595, 264, 2674, 7300, 1296, 257, 2107, 12, 28263, 24433, 50932, 50932, 300, 575, 350, 16431, 5717, 257, 5508, 2107, 12, 28263, 24433, 13, 1119, 456, 1333, 295, 604, 2674, 51316, 51316, 1778, 420, 4978, 281, 983, 321, 576, 4382, 281, 362, 350, 16431, 294, 264, 2445, 420, 406, 30, 51588, 51620], "temperature": 0.0, "avg_logprob": -0.11364014943440755, "compression_ratio": 1.5863874345549738, "no_speech_prob": 6.107983062975109e-05}, {"id": 228, "seek": 148216, "start": 1501.2, "end": 1506.64, "text": " reason or rule to why we would prefer to have kinks in the function or not?", "tokens": [50364, 821, 311, 572, 733, 295, 11, 456, 311, 572, 2199, 1867, 11, 1936, 13, 50596, 50596, 286, 632, 257, 1168, 445, 8595, 264, 2674, 7300, 1296, 257, 2107, 12, 28263, 24433, 50932, 50932, 300, 575, 350, 16431, 5717, 257, 5508, 2107, 12, 28263, 24433, 13, 1119, 456, 1333, 295, 604, 2674, 51316, 51316, 1778, 420, 4978, 281, 983, 321, 576, 4382, 281, 362, 350, 16431, 294, 264, 2445, 420, 406, 30, 51588, 51620], "temperature": 0.0, "avg_logprob": -0.11364014943440755, "compression_ratio": 1.5863874345549738, "no_speech_prob": 6.107983062975109e-05}, {"id": 229, "seek": 150664, "start": 1506.64, "end": 1513.76, "text": " It's a matter of scaling or scale equivalent. So, if the kink is hard, again, you multiply the", "tokens": [50364, 467, 311, 257, 1871, 295, 21589, 420, 4373, 10344, 13, 407, 11, 498, 264, 350, 475, 307, 1152, 11, 797, 11, 291, 12972, 264, 50720, 50720, 4846, 538, 732, 11, 264, 5598, 307, 17207, 538, 732, 11, 457, 5911, 44553, 13, 1033, 13, 759, 291, 362, 257, 50972, 50972, 5508, 6034, 11, 498, 291, 12972, 264, 4846, 538, 11, 718, 311, 584, 11, 2319, 11, 264, 5598, 586, 486, 574, 411, 291, 51504, 51504, 632, 257, 1152, 350, 475, 11, 1392, 11, 570, 264, 5508, 644, 586, 575, 1813, 9884, 3197, 538, 257, 5952, 295, 2319, 13, 759, 291, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.14569259384303418, "compression_ratio": 1.6933333333333334, "no_speech_prob": 9.515842975815758e-06}, {"id": 230, "seek": 150664, "start": 1513.76, "end": 1518.8000000000002, "text": " input by two, the output is multiplied by two, but otherwise unchanged. Okay. If you have a", "tokens": [50364, 467, 311, 257, 1871, 295, 21589, 420, 4373, 10344, 13, 407, 11, 498, 264, 350, 475, 307, 1152, 11, 797, 11, 291, 12972, 264, 50720, 50720, 4846, 538, 732, 11, 264, 5598, 307, 17207, 538, 732, 11, 457, 5911, 44553, 13, 1033, 13, 759, 291, 362, 257, 50972, 50972, 5508, 6034, 11, 498, 291, 12972, 264, 4846, 538, 11, 718, 311, 584, 11, 2319, 11, 264, 5598, 586, 486, 574, 411, 291, 51504, 51504, 632, 257, 1152, 350, 475, 11, 1392, 11, 570, 264, 5508, 644, 586, 575, 1813, 9884, 3197, 538, 257, 5952, 295, 2319, 13, 759, 291, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.14569259384303418, "compression_ratio": 1.6933333333333334, "no_speech_prob": 9.515842975815758e-06}, {"id": 231, "seek": 150664, "start": 1518.8000000000002, "end": 1529.44, "text": " smooth transition, if you multiply the input by, let's say, 100, the output now will look like you", "tokens": [50364, 467, 311, 257, 1871, 295, 21589, 420, 4373, 10344, 13, 407, 11, 498, 264, 350, 475, 307, 1152, 11, 797, 11, 291, 12972, 264, 50720, 50720, 4846, 538, 732, 11, 264, 5598, 307, 17207, 538, 732, 11, 457, 5911, 44553, 13, 1033, 13, 759, 291, 362, 257, 50972, 50972, 5508, 6034, 11, 498, 291, 12972, 264, 4846, 538, 11, 718, 311, 584, 11, 2319, 11, 264, 5598, 586, 486, 574, 411, 291, 51504, 51504, 632, 257, 1152, 350, 475, 11, 1392, 11, 570, 264, 5508, 644, 586, 575, 1813, 9884, 3197, 538, 257, 5952, 295, 2319, 13, 759, 291, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.14569259384303418, "compression_ratio": 1.6933333333333334, "no_speech_prob": 9.515842975815758e-06}, {"id": 232, "seek": 150664, "start": 1529.44, "end": 1535.8400000000001, "text": " had a hard kink, okay, because the smooth part now has become shrunk by a factor of 100. If you", "tokens": [50364, 467, 311, 257, 1871, 295, 21589, 420, 4373, 10344, 13, 407, 11, 498, 264, 350, 475, 307, 1152, 11, 797, 11, 291, 12972, 264, 50720, 50720, 4846, 538, 732, 11, 264, 5598, 307, 17207, 538, 732, 11, 457, 5911, 44553, 13, 1033, 13, 759, 291, 362, 257, 50972, 50972, 5508, 6034, 11, 498, 291, 12972, 264, 4846, 538, 11, 718, 311, 584, 11, 2319, 11, 264, 5598, 586, 486, 574, 411, 291, 51504, 51504, 632, 257, 1152, 350, 475, 11, 1392, 11, 570, 264, 5508, 644, 586, 575, 1813, 9884, 3197, 538, 257, 5952, 295, 2319, 13, 759, 291, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.14569259384303418, "compression_ratio": 1.6933333333333334, "no_speech_prob": 9.515842975815758e-06}, {"id": 233, "seek": 153584, "start": 1535.84, "end": 1543.04, "text": " divide the input by 100, now the kink becomes a very, very smooth sort of convex function.", "tokens": [50364, 9845, 264, 4846, 538, 2319, 11, 586, 264, 350, 475, 3643, 257, 588, 11, 588, 5508, 1333, 295, 42432, 2445, 13, 50724, 50764, 1033, 13, 407, 11, 309, 2962, 264, 5223, 538, 4473, 264, 4373, 295, 264, 4846, 11, 291, 1319, 264, 5223, 295, 264, 51032, 51068, 295, 264, 4985, 13, 400, 300, 1062, 312, 257, 1154, 2171, 570, 562, 291, 3847, 257, 2120, 388, 11167, 18161, 2533, 51476, 51476, 293, 291, 362, 732, 7914, 300, 366, 472, 934, 264, 661, 11, 291, 500, 380, 362, 257, 665, 1969, 337, 11, 51788, 51844], "temperature": 0.0, "avg_logprob": -0.1521414606194747, "compression_ratio": 1.6434782608695653, "no_speech_prob": 1.3709347967960639e-06}, {"id": 234, "seek": 153584, "start": 1543.84, "end": 1549.1999999999998, "text": " Okay. So, it changes the behavior by changing the scale of the input, you change the behavior of the", "tokens": [50364, 9845, 264, 4846, 538, 2319, 11, 586, 264, 350, 475, 3643, 257, 588, 11, 588, 5508, 1333, 295, 42432, 2445, 13, 50724, 50764, 1033, 13, 407, 11, 309, 2962, 264, 5223, 538, 4473, 264, 4373, 295, 264, 4846, 11, 291, 1319, 264, 5223, 295, 264, 51032, 51068, 295, 264, 4985, 13, 400, 300, 1062, 312, 257, 1154, 2171, 570, 562, 291, 3847, 257, 2120, 388, 11167, 18161, 2533, 51476, 51476, 293, 291, 362, 732, 7914, 300, 366, 472, 934, 264, 661, 11, 291, 500, 380, 362, 257, 665, 1969, 337, 11, 51788, 51844], "temperature": 0.0, "avg_logprob": -0.1521414606194747, "compression_ratio": 1.6434782608695653, "no_speech_prob": 1.3709347967960639e-06}, {"id": 235, "seek": 153584, "start": 1549.9199999999998, "end": 1558.08, "text": " of the unit. And that might be a problem sometimes because when you train a multilayer neural net", "tokens": [50364, 9845, 264, 4846, 538, 2319, 11, 586, 264, 350, 475, 3643, 257, 588, 11, 588, 5508, 1333, 295, 42432, 2445, 13, 50724, 50764, 1033, 13, 407, 11, 309, 2962, 264, 5223, 538, 4473, 264, 4373, 295, 264, 4846, 11, 291, 1319, 264, 5223, 295, 264, 51032, 51068, 295, 264, 4985, 13, 400, 300, 1062, 312, 257, 1154, 2171, 570, 562, 291, 3847, 257, 2120, 388, 11167, 18161, 2533, 51476, 51476, 293, 291, 362, 732, 7914, 300, 366, 472, 934, 264, 661, 11, 291, 500, 380, 362, 257, 665, 1969, 337, 11, 51788, 51844], "temperature": 0.0, "avg_logprob": -0.1521414606194747, "compression_ratio": 1.6434782608695653, "no_speech_prob": 1.3709347967960639e-06}, {"id": 236, "seek": 153584, "start": 1558.08, "end": 1564.32, "text": " and you have two layers that are one after the other, you don't have a good control for,", "tokens": [50364, 9845, 264, 4846, 538, 2319, 11, 586, 264, 350, 475, 3643, 257, 588, 11, 588, 5508, 1333, 295, 42432, 2445, 13, 50724, 50764, 1033, 13, 407, 11, 309, 2962, 264, 5223, 538, 4473, 264, 4373, 295, 264, 4846, 11, 291, 1319, 264, 5223, 295, 264, 51032, 51068, 295, 264, 4985, 13, 400, 300, 1062, 312, 257, 1154, 2171, 570, 562, 291, 3847, 257, 2120, 388, 11167, 18161, 2533, 51476, 51476, 293, 291, 362, 732, 7914, 300, 366, 472, 934, 264, 661, 11, 291, 500, 380, 362, 257, 665, 1969, 337, 11, 51788, 51844], "temperature": 0.0, "avg_logprob": -0.1521414606194747, "compression_ratio": 1.6434782608695653, "no_speech_prob": 1.3709347967960639e-06}, {"id": 237, "seek": 156432, "start": 1564.32, "end": 1569.12, "text": " like, how big the weights of this layer are relative to that other weight. So, imagine", "tokens": [50364, 411, 11, 577, 955, 264, 17443, 295, 341, 4583, 366, 4972, 281, 300, 661, 3364, 13, 407, 11, 3811, 50604, 50604, 291, 362, 257, 732, 12, 8376, 260, 3209, 689, 291, 500, 380, 362, 257, 2107, 12, 1889, 17409, 294, 264, 2808, 13, 407, 11, 264, 1185, 307, 50800, 50800, 2584, 8213, 11, 558, 30, 759, 264, 3209, 575, 6678, 412, 257, 3827, 11, 291, 393, 12972, 264, 22341, 11, 51244, 51244, 264, 700, 4583, 3364, 8141, 538, 732, 11, 9845, 264, 1150, 3364, 8141, 538, 732, 11, 293, 4787, 264, 51532, 51532, 3209, 486, 362, 2293, 264, 912, 5598, 13, 1033, 13, 509, 1582, 380, 362, 281, 1319, 1340, 13, 51704, 51780], "temperature": 0.0, "avg_logprob": -0.16589440055515456, "compression_ratio": 1.736842105263158, "no_speech_prob": 6.143881364550907e-06}, {"id": 238, "seek": 156432, "start": 1569.12, "end": 1573.04, "text": " you have a two-layer network where you don't have a non-linearity in the middle. So, the system is", "tokens": [50364, 411, 11, 577, 955, 264, 17443, 295, 341, 4583, 366, 4972, 281, 300, 661, 3364, 13, 407, 11, 3811, 50604, 50604, 291, 362, 257, 732, 12, 8376, 260, 3209, 689, 291, 500, 380, 362, 257, 2107, 12, 1889, 17409, 294, 264, 2808, 13, 407, 11, 264, 1185, 307, 50800, 50800, 2584, 8213, 11, 558, 30, 759, 264, 3209, 575, 6678, 412, 257, 3827, 11, 291, 393, 12972, 264, 22341, 11, 51244, 51244, 264, 700, 4583, 3364, 8141, 538, 732, 11, 9845, 264, 1150, 3364, 8141, 538, 732, 11, 293, 4787, 264, 51532, 51532, 3209, 486, 362, 2293, 264, 912, 5598, 13, 1033, 13, 509, 1582, 380, 362, 281, 1319, 1340, 13, 51704, 51780], "temperature": 0.0, "avg_logprob": -0.16589440055515456, "compression_ratio": 1.736842105263158, "no_speech_prob": 6.143881364550907e-06}, {"id": 239, "seek": 156432, "start": 1573.04, "end": 1581.9199999999998, "text": " completely linear, right? If the network has arrived at a solution, you can multiply the incoming,", "tokens": [50364, 411, 11, 577, 955, 264, 17443, 295, 341, 4583, 366, 4972, 281, 300, 661, 3364, 13, 407, 11, 3811, 50604, 50604, 291, 362, 257, 732, 12, 8376, 260, 3209, 689, 291, 500, 380, 362, 257, 2107, 12, 1889, 17409, 294, 264, 2808, 13, 407, 11, 264, 1185, 307, 50800, 50800, 2584, 8213, 11, 558, 30, 759, 264, 3209, 575, 6678, 412, 257, 3827, 11, 291, 393, 12972, 264, 22341, 11, 51244, 51244, 264, 700, 4583, 3364, 8141, 538, 732, 11, 9845, 264, 1150, 3364, 8141, 538, 732, 11, 293, 4787, 264, 51532, 51532, 3209, 486, 362, 2293, 264, 912, 5598, 13, 1033, 13, 509, 1582, 380, 362, 281, 1319, 1340, 13, 51704, 51780], "temperature": 0.0, "avg_logprob": -0.16589440055515456, "compression_ratio": 1.736842105263158, "no_speech_prob": 6.143881364550907e-06}, {"id": 240, "seek": 156432, "start": 1581.9199999999998, "end": 1587.6799999999998, "text": " the first layer weight matrix by two, divide the second weight matrix by two, and overall the", "tokens": [50364, 411, 11, 577, 955, 264, 17443, 295, 341, 4583, 366, 4972, 281, 300, 661, 3364, 13, 407, 11, 3811, 50604, 50604, 291, 362, 257, 732, 12, 8376, 260, 3209, 689, 291, 500, 380, 362, 257, 2107, 12, 1889, 17409, 294, 264, 2808, 13, 407, 11, 264, 1185, 307, 50800, 50800, 2584, 8213, 11, 558, 30, 759, 264, 3209, 575, 6678, 412, 257, 3827, 11, 291, 393, 12972, 264, 22341, 11, 51244, 51244, 264, 700, 4583, 3364, 8141, 538, 732, 11, 9845, 264, 1150, 3364, 8141, 538, 732, 11, 293, 4787, 264, 51532, 51532, 3209, 486, 362, 2293, 264, 912, 5598, 13, 1033, 13, 509, 1582, 380, 362, 281, 1319, 1340, 13, 51704, 51780], "temperature": 0.0, "avg_logprob": -0.16589440055515456, "compression_ratio": 1.736842105263158, "no_speech_prob": 6.143881364550907e-06}, {"id": 241, "seek": 156432, "start": 1587.6799999999998, "end": 1591.12, "text": " network will have exactly the same output. Okay. You won't have to change anything.", "tokens": [50364, 411, 11, 577, 955, 264, 17443, 295, 341, 4583, 366, 4972, 281, 300, 661, 3364, 13, 407, 11, 3811, 50604, 50604, 291, 362, 257, 732, 12, 8376, 260, 3209, 689, 291, 500, 380, 362, 257, 2107, 12, 1889, 17409, 294, 264, 2808, 13, 407, 11, 264, 1185, 307, 50800, 50800, 2584, 8213, 11, 558, 30, 759, 264, 3209, 575, 6678, 412, 257, 3827, 11, 291, 393, 12972, 264, 22341, 11, 51244, 51244, 264, 700, 4583, 3364, 8141, 538, 732, 11, 9845, 264, 1150, 3364, 8141, 538, 732, 11, 293, 4787, 264, 51532, 51532, 3209, 486, 362, 2293, 264, 912, 5598, 13, 1033, 13, 509, 1582, 380, 362, 281, 1319, 1340, 13, 51704, 51780], "temperature": 0.0, "avg_logprob": -0.16589440055515456, "compression_ratio": 1.736842105263158, "no_speech_prob": 6.143881364550907e-06}, {"id": 242, "seek": 159112, "start": 1591.12, "end": 1596.7199999999998, "text": " What that means is that when you do training, there is nothing that forces the system to", "tokens": [50364, 708, 300, 1355, 307, 300, 562, 291, 360, 3097, 11, 456, 307, 1825, 300, 5874, 264, 1185, 281, 50644, 50672, 362, 257, 1729, 4373, 337, 264, 3364, 32284, 13, 407, 11, 586, 498, 291, 829, 257, 2107, 12, 1889, 17409, 294, 264, 2808, 50956, 50996, 293, 291, 920, 500, 380, 362, 604, 25534, 337, 264, 1185, 281, 733, 295, 362, 17408, 337, 264, 51244, 51244, 700, 4583, 3364, 5717, 264, 1150, 4583, 3364, 11, 291, 1116, 1101, 362, 257, 2107, 12, 1889, 17409, 300, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.1958487250588157, "compression_ratio": 1.7832512315270936, "no_speech_prob": 7.646245649084449e-06}, {"id": 243, "seek": 159112, "start": 1597.28, "end": 1602.9599999999998, "text": " have a particular scale for the weight matrices. So, now if you put a non-linearity in the middle", "tokens": [50364, 708, 300, 1355, 307, 300, 562, 291, 360, 3097, 11, 456, 307, 1825, 300, 5874, 264, 1185, 281, 50644, 50672, 362, 257, 1729, 4373, 337, 264, 3364, 32284, 13, 407, 11, 586, 498, 291, 829, 257, 2107, 12, 1889, 17409, 294, 264, 2808, 50956, 50996, 293, 291, 920, 500, 380, 362, 604, 25534, 337, 264, 1185, 281, 733, 295, 362, 17408, 337, 264, 51244, 51244, 700, 4583, 3364, 5717, 264, 1150, 4583, 3364, 11, 291, 1116, 1101, 362, 257, 2107, 12, 1889, 17409, 300, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.1958487250588157, "compression_ratio": 1.7832512315270936, "no_speech_prob": 7.646245649084449e-06}, {"id": 244, "seek": 159112, "start": 1603.76, "end": 1608.7199999999998, "text": " and you still don't have any constraint for the system to kind of have scales for the", "tokens": [50364, 708, 300, 1355, 307, 300, 562, 291, 360, 3097, 11, 456, 307, 1825, 300, 5874, 264, 1185, 281, 50644, 50672, 362, 257, 1729, 4373, 337, 264, 3364, 32284, 13, 407, 11, 586, 498, 291, 829, 257, 2107, 12, 1889, 17409, 294, 264, 2808, 50956, 50996, 293, 291, 920, 500, 380, 362, 604, 25534, 337, 264, 1185, 281, 733, 295, 362, 17408, 337, 264, 51244, 51244, 700, 4583, 3364, 5717, 264, 1150, 4583, 3364, 11, 291, 1116, 1101, 362, 257, 2107, 12, 1889, 17409, 300, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.1958487250588157, "compression_ratio": 1.7832512315270936, "no_speech_prob": 7.646245649084449e-06}, {"id": 245, "seek": 159112, "start": 1608.7199999999998, "end": 1615.76, "text": " first layer weight versus the second layer weight, you'd better have a non-linearity that", "tokens": [50364, 708, 300, 1355, 307, 300, 562, 291, 360, 3097, 11, 456, 307, 1825, 300, 5874, 264, 1185, 281, 50644, 50672, 362, 257, 1729, 4373, 337, 264, 3364, 32284, 13, 407, 11, 586, 498, 291, 829, 257, 2107, 12, 1889, 17409, 294, 264, 2808, 50956, 50996, 293, 291, 920, 500, 380, 362, 604, 25534, 337, 264, 1185, 281, 733, 295, 362, 17408, 337, 264, 51244, 51244, 700, 4583, 3364, 5717, 264, 1150, 4583, 3364, 11, 291, 1116, 1101, 362, 257, 2107, 12, 1889, 17409, 300, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.1958487250588157, "compression_ratio": 1.7832512315270936, "no_speech_prob": 7.646245649084449e-06}, {"id": 246, "seek": 161576, "start": 1615.76, "end": 1621.68, "text": " doesn't care about scale. Okay. So, if you have a non-linearity that does care about scale,", "tokens": [50364, 1177, 380, 1127, 466, 4373, 13, 1033, 13, 407, 11, 498, 291, 362, 257, 2107, 12, 1889, 17409, 300, 775, 1127, 466, 4373, 11, 50660, 50712, 550, 428, 3209, 1177, 380, 362, 264, 3922, 295, 437, 2744, 3364, 8141, 309, 393, 764, 294, 264, 700, 4583, 50976, 50976, 570, 300, 486, 2584, 1319, 264, 5223, 13, 400, 309, 815, 528, 281, 362, 2416, 17443, 337, 512, 51228, 51228, 661, 1778, 11, 597, 486, 21160, 473, 264, 2107, 12, 1889, 17409, 293, 550, 733, 295, 1884, 3161, 3807, 16235, 2663, 13, 51556, 51604], "temperature": 0.0, "avg_logprob": -0.16076540439686876, "compression_ratio": 1.6595744680851063, "no_speech_prob": 4.9368854888598435e-06}, {"id": 247, "seek": 161576, "start": 1622.72, "end": 1628.0, "text": " then your network doesn't have the choice of what size weight matrix it can use in the first layer", "tokens": [50364, 1177, 380, 1127, 466, 4373, 13, 1033, 13, 407, 11, 498, 291, 362, 257, 2107, 12, 1889, 17409, 300, 775, 1127, 466, 4373, 11, 50660, 50712, 550, 428, 3209, 1177, 380, 362, 264, 3922, 295, 437, 2744, 3364, 8141, 309, 393, 764, 294, 264, 700, 4583, 50976, 50976, 570, 300, 486, 2584, 1319, 264, 5223, 13, 400, 309, 815, 528, 281, 362, 2416, 17443, 337, 512, 51228, 51228, 661, 1778, 11, 597, 486, 21160, 473, 264, 2107, 12, 1889, 17409, 293, 550, 733, 295, 1884, 3161, 3807, 16235, 2663, 13, 51556, 51604], "temperature": 0.0, "avg_logprob": -0.16076540439686876, "compression_ratio": 1.6595744680851063, "no_speech_prob": 4.9368854888598435e-06}, {"id": 248, "seek": 161576, "start": 1628.0, "end": 1633.04, "text": " because that will completely change the behavior. And it may want to have large weights for some", "tokens": [50364, 1177, 380, 1127, 466, 4373, 13, 1033, 13, 407, 11, 498, 291, 362, 257, 2107, 12, 1889, 17409, 300, 775, 1127, 466, 4373, 11, 50660, 50712, 550, 428, 3209, 1177, 380, 362, 264, 3922, 295, 437, 2744, 3364, 8141, 309, 393, 764, 294, 264, 700, 4583, 50976, 50976, 570, 300, 486, 2584, 1319, 264, 5223, 13, 400, 309, 815, 528, 281, 362, 2416, 17443, 337, 512, 51228, 51228, 661, 1778, 11, 597, 486, 21160, 473, 264, 2107, 12, 1889, 17409, 293, 550, 733, 295, 1884, 3161, 3807, 16235, 2663, 13, 51556, 51604], "temperature": 0.0, "avg_logprob": -0.16076540439686876, "compression_ratio": 1.6595744680851063, "no_speech_prob": 4.9368854888598435e-06}, {"id": 249, "seek": 161576, "start": 1633.04, "end": 1639.6, "text": " other reason, which will saturate the non-linearity and then kind of create vanishing gradient issues.", "tokens": [50364, 1177, 380, 1127, 466, 4373, 13, 1033, 13, 407, 11, 498, 291, 362, 257, 2107, 12, 1889, 17409, 300, 775, 1127, 466, 4373, 11, 50660, 50712, 550, 428, 3209, 1177, 380, 362, 264, 3922, 295, 437, 2744, 3364, 8141, 309, 393, 764, 294, 264, 700, 4583, 50976, 50976, 570, 300, 486, 2584, 1319, 264, 5223, 13, 400, 309, 815, 528, 281, 362, 2416, 17443, 337, 512, 51228, 51228, 661, 1778, 11, 597, 486, 21160, 473, 264, 2107, 12, 1889, 17409, 293, 550, 733, 295, 1884, 3161, 3807, 16235, 2663, 13, 51556, 51604], "temperature": 0.0, "avg_logprob": -0.16076540439686876, "compression_ratio": 1.6595744680851063, "no_speech_prob": 4.9368854888598435e-06}, {"id": 250, "seek": 163960, "start": 1639.6, "end": 1647.36, "text": " So, it's not entirely clear why is it that deep networks work better with single king functions,", "tokens": [50364, 407, 11, 309, 311, 406, 7696, 1850, 983, 307, 309, 300, 2452, 9590, 589, 1101, 365, 2167, 4867, 6828, 11, 50752, 50752, 457, 309, 311, 1391, 3462, 281, 300, 21589, 21977, 4707, 420, 4373, 9052, 655, 4707, 13, 823, 11, 51068, 51068, 456, 576, 312, 661, 2098, 295, 19442, 341, 1154, 11, 597, 576, 312, 281, 1936, 992, 257, 1152, 4373, 322, 51340, 51340, 264, 17443, 295, 633, 4583, 13, 407, 11, 291, 727, 2710, 1125, 264, 17443, 295, 264, 7914, 370, 300, 264, 51556, 51632], "temperature": 0.0, "avg_logprob": -0.20663371953097256, "compression_ratio": 1.6785714285714286, "no_speech_prob": 3.1874922115093796e-06}, {"id": 251, "seek": 163960, "start": 1647.36, "end": 1653.6799999999998, "text": " but it's probably due to that scaling variance property or scale equivalence property. Now,", "tokens": [50364, 407, 11, 309, 311, 406, 7696, 1850, 983, 307, 309, 300, 2452, 9590, 589, 1101, 365, 2167, 4867, 6828, 11, 50752, 50752, 457, 309, 311, 1391, 3462, 281, 300, 21589, 21977, 4707, 420, 4373, 9052, 655, 4707, 13, 823, 11, 51068, 51068, 456, 576, 312, 661, 2098, 295, 19442, 341, 1154, 11, 597, 576, 312, 281, 1936, 992, 257, 1152, 4373, 322, 51340, 51340, 264, 17443, 295, 633, 4583, 13, 407, 11, 291, 727, 2710, 1125, 264, 17443, 295, 264, 7914, 370, 300, 264, 51556, 51632], "temperature": 0.0, "avg_logprob": -0.20663371953097256, "compression_ratio": 1.6785714285714286, "no_speech_prob": 3.1874922115093796e-06}, {"id": 252, "seek": 163960, "start": 1653.6799999999998, "end": 1659.12, "text": " there would be other ways of fixing this problem, which would be to basically set a hard scale on", "tokens": [50364, 407, 11, 309, 311, 406, 7696, 1850, 983, 307, 309, 300, 2452, 9590, 589, 1101, 365, 2167, 4867, 6828, 11, 50752, 50752, 457, 309, 311, 1391, 3462, 281, 300, 21589, 21977, 4707, 420, 4373, 9052, 655, 4707, 13, 823, 11, 51068, 51068, 456, 576, 312, 661, 2098, 295, 19442, 341, 1154, 11, 597, 576, 312, 281, 1936, 992, 257, 1152, 4373, 322, 51340, 51340, 264, 17443, 295, 633, 4583, 13, 407, 11, 291, 727, 2710, 1125, 264, 17443, 295, 264, 7914, 370, 300, 264, 51556, 51632], "temperature": 0.0, "avg_logprob": -0.20663371953097256, "compression_ratio": 1.6785714285714286, "no_speech_prob": 3.1874922115093796e-06}, {"id": 253, "seek": 163960, "start": 1659.12, "end": 1663.4399999999998, "text": " the weights of every layer. So, you could normalize the weights of the layers so that the", "tokens": [50364, 407, 11, 309, 311, 406, 7696, 1850, 983, 307, 309, 300, 2452, 9590, 589, 1101, 365, 2167, 4867, 6828, 11, 50752, 50752, 457, 309, 311, 1391, 3462, 281, 300, 21589, 21977, 4707, 420, 4373, 9052, 655, 4707, 13, 823, 11, 51068, 51068, 456, 576, 312, 661, 2098, 295, 19442, 341, 1154, 11, 597, 576, 312, 281, 1936, 992, 257, 1152, 4373, 322, 51340, 51340, 264, 17443, 295, 633, 4583, 13, 407, 11, 291, 727, 2710, 1125, 264, 17443, 295, 264, 7914, 370, 300, 264, 51556, 51632], "temperature": 0.0, "avg_logprob": -0.20663371953097256, "compression_ratio": 1.6785714285714286, "no_speech_prob": 3.1874922115093796e-06}, {"id": 254, "seek": 166344, "start": 1663.44, "end": 1668.24, "text": " variance of things that go into a unit is always constant. In fact, that's a little bit what batch", "tokens": [50364, 21977, 295, 721, 300, 352, 666, 257, 4985, 307, 1009, 5754, 13, 682, 1186, 11, 300, 311, 257, 707, 857, 437, 15245, 50604, 50604, 2710, 2144, 775, 420, 264, 3683, 2710, 2144, 26954, 13, 814, 360, 300, 281, 512, 8396, 13, 814, 829, 50836, 50836, 264, 914, 412, 4018, 293, 264, 21977, 307, 5754, 13, 407, 11, 586, 264, 21977, 295, 264, 27433, 295, 264, 51188, 51188, 5598, 1177, 380, 5672, 322, 264, 2744, 295, 264, 17443, 570, 309, 311, 48704, 13, 407, 11, 300, 307, 18886, 983, 51544, 51612], "temperature": 0.0, "avg_logprob": -0.15397872095522674, "compression_ratio": 1.7342342342342343, "no_speech_prob": 5.592515208263649e-06}, {"id": 255, "seek": 166344, "start": 1668.24, "end": 1672.88, "text": " normalization does or the various normalization schemes. They do that to some extent. They put", "tokens": [50364, 21977, 295, 721, 300, 352, 666, 257, 4985, 307, 1009, 5754, 13, 682, 1186, 11, 300, 311, 257, 707, 857, 437, 15245, 50604, 50604, 2710, 2144, 775, 420, 264, 3683, 2710, 2144, 26954, 13, 814, 360, 300, 281, 512, 8396, 13, 814, 829, 50836, 50836, 264, 914, 412, 4018, 293, 264, 21977, 307, 5754, 13, 407, 11, 586, 264, 21977, 295, 264, 27433, 295, 264, 51188, 51188, 5598, 1177, 380, 5672, 322, 264, 2744, 295, 264, 17443, 570, 309, 311, 48704, 13, 407, 11, 300, 307, 18886, 983, 51544, 51612], "temperature": 0.0, "avg_logprob": -0.15397872095522674, "compression_ratio": 1.7342342342342343, "no_speech_prob": 5.592515208263649e-06}, {"id": 256, "seek": 166344, "start": 1672.88, "end": 1679.92, "text": " the mean at zero and the variance is constant. So, now the variance of the amplitude of the", "tokens": [50364, 21977, 295, 721, 300, 352, 666, 257, 4985, 307, 1009, 5754, 13, 682, 1186, 11, 300, 311, 257, 707, 857, 437, 15245, 50604, 50604, 2710, 2144, 775, 420, 264, 3683, 2710, 2144, 26954, 13, 814, 360, 300, 281, 512, 8396, 13, 814, 829, 50836, 50836, 264, 914, 412, 4018, 293, 264, 21977, 307, 5754, 13, 407, 11, 586, 264, 21977, 295, 264, 27433, 295, 264, 51188, 51188, 5598, 1177, 380, 5672, 322, 264, 2744, 295, 264, 17443, 570, 309, 311, 48704, 13, 407, 11, 300, 307, 18886, 983, 51544, 51612], "temperature": 0.0, "avg_logprob": -0.15397872095522674, "compression_ratio": 1.7342342342342343, "no_speech_prob": 5.592515208263649e-06}, {"id": 257, "seek": 166344, "start": 1679.92, "end": 1687.04, "text": " output doesn't depend on the size of the weights because it's normalized. So, that is partially why", "tokens": [50364, 21977, 295, 721, 300, 352, 666, 257, 4985, 307, 1009, 5754, 13, 682, 1186, 11, 300, 311, 257, 707, 857, 437, 15245, 50604, 50604, 2710, 2144, 775, 420, 264, 3683, 2710, 2144, 26954, 13, 814, 360, 300, 281, 512, 8396, 13, 814, 829, 50836, 50836, 264, 914, 412, 4018, 293, 264, 21977, 307, 5754, 13, 407, 11, 586, 264, 21977, 295, 264, 27433, 295, 264, 51188, 51188, 5598, 1177, 380, 5672, 322, 264, 2744, 295, 264, 17443, 570, 309, 311, 48704, 13, 407, 11, 300, 307, 18886, 983, 51544, 51612], "temperature": 0.0, "avg_logprob": -0.15397872095522674, "compression_ratio": 1.7342342342342343, "no_speech_prob": 5.592515208263649e-06}, {"id": 258, "seek": 168704, "start": 1687.04, "end": 1692.8, "text": " things like batch norm and group norm and things like this help. It's because they can fix the", "tokens": [50364, 721, 411, 15245, 2026, 293, 1594, 2026, 293, 721, 411, 341, 854, 13, 467, 311, 570, 436, 393, 3191, 264, 50652, 50704, 4373, 257, 707, 857, 13, 583, 550, 498, 291, 3191, 264, 4373, 11, 550, 365, 746, 411, 15245, 2026, 11, 51136, 51172, 264, 1185, 586, 1177, 380, 362, 604, 636, 295, 4473, 264, 5223, 295, 264, 7914, 13, 407, 11, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.3920948433153557, "compression_ratio": 1.6273291925465838, "no_speech_prob": 3.3930427889572456e-06}, {"id": 259, "seek": 168704, "start": 1693.84, "end": 1702.48, "text": " scale a little bit. But then if you fix the scale, then with something like batch norm,", "tokens": [50364, 721, 411, 15245, 2026, 293, 1594, 2026, 293, 721, 411, 341, 854, 13, 467, 311, 570, 436, 393, 3191, 264, 50652, 50704, 4373, 257, 707, 857, 13, 583, 550, 498, 291, 3191, 264, 4373, 11, 550, 365, 746, 411, 15245, 2026, 11, 51136, 51172, 264, 1185, 586, 1177, 380, 362, 604, 636, 295, 4473, 264, 5223, 295, 264, 7914, 13, 407, 11, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.3920948433153557, "compression_ratio": 1.6273291925465838, "no_speech_prob": 3.3930427889572456e-06}, {"id": 260, "seek": 168704, "start": 1703.2, "end": 1710.96, "text": " the system now doesn't have any way of changing the behavior of the layers. So,", "tokens": [50364, 721, 411, 15245, 2026, 293, 1594, 2026, 293, 721, 411, 341, 854, 13, 467, 311, 570, 436, 393, 3191, 264, 50652, 50704, 4373, 257, 707, 857, 13, 583, 550, 498, 291, 3191, 264, 4373, 11, 550, 365, 746, 411, 15245, 2026, 11, 51136, 51172, 264, 1185, 586, 1177, 380, 362, 604, 636, 295, 4473, 264, 5223, 295, 264, 7914, 13, 407, 11, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.3920948433153557, "compression_ratio": 1.6273291925465838, "no_speech_prob": 3.3930427889572456e-06}, {"id": 261, "seek": 171096, "start": 1710.96, "end": 1718.0, "text": " with something like batch norm, the system now doesn't have any way of choosing which part of the", "tokens": [50364, 365, 746, 411, 15245, 2026, 11, 264, 1185, 586, 1177, 380, 362, 604, 636, 295, 10875, 597, 644, 295, 264, 50716, 50716, 2107, 12, 1889, 17409, 307, 516, 281, 764, 294, 264, 732, 12, 74, 475, 2445, 1185, 13, 407, 11, 721, 411, 1594, 2710, 2144, 51036, 51036, 420, 15245, 2710, 2144, 366, 40393, 267, 964, 365, 4556, 3280, 3742, 498, 291, 528, 13, 759, 291, 362, 257, 4556, 3280, 327, 11, 51308, 51308, 291, 500, 380, 528, 2710, 2144, 445, 949, 309, 13, 286, 536, 300, 6417, 512, 534, 665, 24002, 13, 1044, 291, 13, 51608, 51636], "temperature": 0.0, "avg_logprob": -0.1667998504638672, "compression_ratio": 1.6810344827586208, "no_speech_prob": 3.393097813386703e-06}, {"id": 262, "seek": 171096, "start": 1718.0, "end": 1724.4, "text": " non-linearity is going to use in the two-kink function system. So, things like group normalization", "tokens": [50364, 365, 746, 411, 15245, 2026, 11, 264, 1185, 586, 1177, 380, 362, 604, 636, 295, 10875, 597, 644, 295, 264, 50716, 50716, 2107, 12, 1889, 17409, 307, 516, 281, 764, 294, 264, 732, 12, 74, 475, 2445, 1185, 13, 407, 11, 721, 411, 1594, 2710, 2144, 51036, 51036, 420, 15245, 2710, 2144, 366, 40393, 267, 964, 365, 4556, 3280, 3742, 498, 291, 528, 13, 759, 291, 362, 257, 4556, 3280, 327, 11, 51308, 51308, 291, 500, 380, 528, 2710, 2144, 445, 949, 309, 13, 286, 536, 300, 6417, 512, 534, 665, 24002, 13, 1044, 291, 13, 51608, 51636], "temperature": 0.0, "avg_logprob": -0.1667998504638672, "compression_ratio": 1.6810344827586208, "no_speech_prob": 3.393097813386703e-06}, {"id": 263, "seek": 171096, "start": 1724.4, "end": 1729.8400000000001, "text": " or batch normalization are incompatible with sigmoids if you want. If you have a sigmoid,", "tokens": [50364, 365, 746, 411, 15245, 2026, 11, 264, 1185, 586, 1177, 380, 362, 604, 636, 295, 10875, 597, 644, 295, 264, 50716, 50716, 2107, 12, 1889, 17409, 307, 516, 281, 764, 294, 264, 732, 12, 74, 475, 2445, 1185, 13, 407, 11, 721, 411, 1594, 2710, 2144, 51036, 51036, 420, 15245, 2710, 2144, 366, 40393, 267, 964, 365, 4556, 3280, 3742, 498, 291, 528, 13, 759, 291, 362, 257, 4556, 3280, 327, 11, 51308, 51308, 291, 500, 380, 528, 2710, 2144, 445, 949, 309, 13, 286, 536, 300, 6417, 512, 534, 665, 24002, 13, 1044, 291, 13, 51608, 51636], "temperature": 0.0, "avg_logprob": -0.1667998504638672, "compression_ratio": 1.6810344827586208, "no_speech_prob": 3.393097813386703e-06}, {"id": 264, "seek": 171096, "start": 1729.8400000000001, "end": 1735.8400000000001, "text": " you don't want normalization just before it. I see that provides some really good intuition. Thank you.", "tokens": [50364, 365, 746, 411, 15245, 2026, 11, 264, 1185, 586, 1177, 380, 362, 604, 636, 295, 10875, 597, 644, 295, 264, 50716, 50716, 2107, 12, 1889, 17409, 307, 516, 281, 764, 294, 264, 732, 12, 74, 475, 2445, 1185, 13, 407, 11, 721, 411, 1594, 2710, 2144, 51036, 51036, 420, 15245, 2710, 2144, 366, 40393, 267, 964, 365, 4556, 3280, 3742, 498, 291, 528, 13, 759, 291, 362, 257, 4556, 3280, 327, 11, 51308, 51308, 291, 500, 380, 528, 2710, 2144, 445, 949, 309, 13, 286, 536, 300, 6417, 512, 534, 665, 24002, 13, 1044, 291, 13, 51608, 51636], "temperature": 0.0, "avg_logprob": -0.1667998504638672, "compression_ratio": 1.6810344827586208, "no_speech_prob": 3.393097813386703e-06}, {"id": 265, "seek": 173584, "start": 1735.84, "end": 1742.6399999999999, "text": " Okay. Any other questions? I have one more question. I noticed in a softmax function,", "tokens": [50364, 1033, 13, 2639, 661, 1651, 30, 286, 362, 472, 544, 1168, 13, 286, 5694, 294, 257, 2787, 41167, 2445, 11, 50704, 50704, 512, 561, 764, 264, 4292, 17619, 13, 407, 11, 294, 437, 3331, 576, 321, 528, 281, 764, 264, 4292, 50964, 50964, 293, 983, 576, 321, 764, 309, 30, 1042, 11, 281, 512, 8396, 11, 264, 4292, 307, 40997, 365, 22341, 17443, 13, 407, 11, 51268, 51268, 498, 291, 362, 32807, 34499, 1348, 666, 428, 2787, 41167, 11, 1419, 257, 9861, 13075, 294, 428, 2787, 41167, 2681, 281, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.13375863721293788, "compression_ratio": 1.7212389380530972, "no_speech_prob": 1.3845539797330275e-05}, {"id": 266, "seek": 173584, "start": 1742.6399999999999, "end": 1747.84, "text": " some people use the temperature coefficient. So, in what cases would we want to use the temperature", "tokens": [50364, 1033, 13, 2639, 661, 1651, 30, 286, 362, 472, 544, 1168, 13, 286, 5694, 294, 257, 2787, 41167, 2445, 11, 50704, 50704, 512, 561, 764, 264, 4292, 17619, 13, 407, 11, 294, 437, 3331, 576, 321, 528, 281, 764, 264, 4292, 50964, 50964, 293, 983, 576, 321, 764, 309, 30, 1042, 11, 281, 512, 8396, 11, 264, 4292, 307, 40997, 365, 22341, 17443, 13, 407, 11, 51268, 51268, 498, 291, 362, 32807, 34499, 1348, 666, 428, 2787, 41167, 11, 1419, 257, 9861, 13075, 294, 428, 2787, 41167, 2681, 281, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.13375863721293788, "compression_ratio": 1.7212389380530972, "no_speech_prob": 1.3845539797330275e-05}, {"id": 267, "seek": 173584, "start": 1747.84, "end": 1753.9199999999998, "text": " and why would we use it? Well, to some extent, the temperature is redundant with incoming weights. So,", "tokens": [50364, 1033, 13, 2639, 661, 1651, 30, 286, 362, 472, 544, 1168, 13, 286, 5694, 294, 257, 2787, 41167, 2445, 11, 50704, 50704, 512, 561, 764, 264, 4292, 17619, 13, 407, 11, 294, 437, 3331, 576, 321, 528, 281, 764, 264, 4292, 50964, 50964, 293, 983, 576, 321, 764, 309, 30, 1042, 11, 281, 512, 8396, 11, 264, 4292, 307, 40997, 365, 22341, 17443, 13, 407, 11, 51268, 51268, 498, 291, 362, 32807, 34499, 1348, 666, 428, 2787, 41167, 11, 1419, 257, 9861, 13075, 294, 428, 2787, 41167, 2681, 281, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.13375863721293788, "compression_ratio": 1.7212389380530972, "no_speech_prob": 1.3845539797330275e-05}, {"id": 268, "seek": 173584, "start": 1753.9199999999998, "end": 1761.6799999999998, "text": " if you have weighted sums coming into your softmax, having a beta parameter in your softmax equal to", "tokens": [50364, 1033, 13, 2639, 661, 1651, 30, 286, 362, 472, 544, 1168, 13, 286, 5694, 294, 257, 2787, 41167, 2445, 11, 50704, 50704, 512, 561, 764, 264, 4292, 17619, 13, 407, 11, 294, 437, 3331, 576, 321, 528, 281, 764, 264, 4292, 50964, 50964, 293, 983, 576, 321, 764, 309, 30, 1042, 11, 281, 512, 8396, 11, 264, 4292, 307, 40997, 365, 22341, 17443, 13, 407, 11, 51268, 51268, 498, 291, 362, 32807, 34499, 1348, 666, 428, 2787, 41167, 11, 1419, 257, 9861, 13075, 294, 428, 2787, 41167, 2681, 281, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.13375863721293788, "compression_ratio": 1.7212389380530972, "no_speech_prob": 1.3845539797330275e-05}, {"id": 269, "seek": 176168, "start": 1761.68, "end": 1766.64, "text": " two instead of one is the same as just making your weights twice as big. It has exactly the same effect.", "tokens": [50364, 732, 2602, 295, 472, 307, 264, 912, 382, 445, 1455, 428, 17443, 6091, 382, 955, 13, 467, 575, 2293, 264, 912, 1802, 13, 50612, 50652, 1033, 13, 407, 11, 300, 9861, 13075, 307, 40997, 365, 264, 2744, 295, 264, 17443, 13, 583, 797, 11, 498, 291, 645, 50920, 50952, 420, 264, 2744, 295, 264, 32807, 2408, 11, 264, 21977, 295, 264, 32807, 34499, 498, 291, 528, 13, 583, 797, 11, 51132, 51132, 498, 291, 362, 257, 15245, 2710, 2144, 294, 456, 11, 550, 264, 4292, 13075, 7001, 570, 586, 264, 51372, 51404, 4846, 1374, 21518, 366, 6806, 13, 407, 11, 586, 264, 4292, 7001, 13, 440, 4292, 1936, 9003, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.11902900290700187, "compression_ratio": 1.9019607843137254, "no_speech_prob": 9.223248525813688e-06}, {"id": 270, "seek": 176168, "start": 1767.44, "end": 1772.8, "text": " Okay. So, that beta parameter is redundant with the size of the weights. But again, if you were", "tokens": [50364, 732, 2602, 295, 472, 307, 264, 912, 382, 445, 1455, 428, 17443, 6091, 382, 955, 13, 467, 575, 2293, 264, 912, 1802, 13, 50612, 50652, 1033, 13, 407, 11, 300, 9861, 13075, 307, 40997, 365, 264, 2744, 295, 264, 17443, 13, 583, 797, 11, 498, 291, 645, 50920, 50952, 420, 264, 2744, 295, 264, 32807, 2408, 11, 264, 21977, 295, 264, 32807, 34499, 498, 291, 528, 13, 583, 797, 11, 51132, 51132, 498, 291, 362, 257, 15245, 2710, 2144, 294, 456, 11, 550, 264, 4292, 13075, 7001, 570, 586, 264, 51372, 51404, 4846, 1374, 21518, 366, 6806, 13, 407, 11, 586, 264, 4292, 7001, 13, 440, 4292, 1936, 9003, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.11902900290700187, "compression_ratio": 1.9019607843137254, "no_speech_prob": 9.223248525813688e-06}, {"id": 271, "seek": 176168, "start": 1773.44, "end": 1777.04, "text": " or the size of the weighted sum, the variance of the weighted sums if you want. But again,", "tokens": [50364, 732, 2602, 295, 472, 307, 264, 912, 382, 445, 1455, 428, 17443, 6091, 382, 955, 13, 467, 575, 2293, 264, 912, 1802, 13, 50612, 50652, 1033, 13, 407, 11, 300, 9861, 13075, 307, 40997, 365, 264, 2744, 295, 264, 17443, 13, 583, 797, 11, 498, 291, 645, 50920, 50952, 420, 264, 2744, 295, 264, 32807, 2408, 11, 264, 21977, 295, 264, 32807, 34499, 498, 291, 528, 13, 583, 797, 11, 51132, 51132, 498, 291, 362, 257, 15245, 2710, 2144, 294, 456, 11, 550, 264, 4292, 13075, 7001, 570, 586, 264, 51372, 51404, 4846, 1374, 21518, 366, 6806, 13, 407, 11, 586, 264, 4292, 7001, 13, 440, 4292, 1936, 9003, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.11902900290700187, "compression_ratio": 1.9019607843137254, "no_speech_prob": 9.223248525813688e-06}, {"id": 272, "seek": 176168, "start": 1777.04, "end": 1781.8400000000001, "text": " if you have a batch normalization in there, then the temperature parameter matters because now the", "tokens": [50364, 732, 2602, 295, 472, 307, 264, 912, 382, 445, 1455, 428, 17443, 6091, 382, 955, 13, 467, 575, 2293, 264, 912, 1802, 13, 50612, 50652, 1033, 13, 407, 11, 300, 9861, 13075, 307, 40997, 365, 264, 2744, 295, 264, 17443, 13, 583, 797, 11, 498, 291, 645, 50920, 50952, 420, 264, 2744, 295, 264, 32807, 2408, 11, 264, 21977, 295, 264, 32807, 34499, 498, 291, 528, 13, 583, 797, 11, 51132, 51132, 498, 291, 362, 257, 15245, 2710, 2144, 294, 456, 11, 550, 264, 4292, 13075, 7001, 570, 586, 264, 51372, 51404, 4846, 1374, 21518, 366, 6806, 13, 407, 11, 586, 264, 4292, 7001, 13, 440, 4292, 1936, 9003, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.11902900290700187, "compression_ratio": 1.9019607843137254, "no_speech_prob": 9.223248525813688e-06}, {"id": 273, "seek": 176168, "start": 1782.48, "end": 1791.3600000000001, "text": " input variances are fixed. So, now the temperature matters. The temperature basically controls", "tokens": [50364, 732, 2602, 295, 472, 307, 264, 912, 382, 445, 1455, 428, 17443, 6091, 382, 955, 13, 467, 575, 2293, 264, 912, 1802, 13, 50612, 50652, 1033, 13, 407, 11, 300, 9861, 13075, 307, 40997, 365, 264, 2744, 295, 264, 17443, 13, 583, 797, 11, 498, 291, 645, 50920, 50952, 420, 264, 2744, 295, 264, 32807, 2408, 11, 264, 21977, 295, 264, 32807, 34499, 498, 291, 528, 13, 583, 797, 11, 51132, 51132, 498, 291, 362, 257, 15245, 2710, 2144, 294, 456, 11, 550, 264, 4292, 13075, 7001, 570, 586, 264, 51372, 51404, 4846, 1374, 21518, 366, 6806, 13, 407, 11, 586, 264, 4292, 7001, 13, 440, 4292, 1936, 9003, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.11902900290700187, "compression_ratio": 1.9019607843137254, "no_speech_prob": 9.223248525813688e-06}, {"id": 274, "seek": 179136, "start": 1791.36, "end": 1800.56, "text": " how hard the distribution on the output will be. So, with a very, very large beta, you basically", "tokens": [50364, 577, 1152, 264, 7316, 322, 264, 5598, 486, 312, 13, 407, 11, 365, 257, 588, 11, 588, 2416, 9861, 11, 291, 1936, 50824, 50824, 486, 362, 472, 295, 264, 23930, 2681, 281, 472, 293, 439, 264, 661, 2306, 588, 1998, 281, 4018, 13, 286, 914, 11, 51048, 51048, 588, 1998, 281, 472, 293, 588, 1998, 281, 4018, 13, 2305, 9861, 307, 1359, 11, 550, 309, 311, 23119, 13, 682, 264, 4948, 295, 51292, 51292, 9861, 2681, 281, 4018, 11, 309, 311, 544, 411, 364, 4274, 767, 300, 291, 483, 13, 16985, 41167, 36896, 257, 707, 51504, 51504], "temperature": 0.0, "avg_logprob": -0.07196780888721196, "compression_ratio": 1.7592592592592593, "no_speech_prob": 9.516030331724323e-06}, {"id": 275, "seek": 179136, "start": 1800.56, "end": 1805.04, "text": " will have one of the outputs equal to one and all the other ones very close to zero. I mean,", "tokens": [50364, 577, 1152, 264, 7316, 322, 264, 5598, 486, 312, 13, 407, 11, 365, 257, 588, 11, 588, 2416, 9861, 11, 291, 1936, 50824, 50824, 486, 362, 472, 295, 264, 23930, 2681, 281, 472, 293, 439, 264, 661, 2306, 588, 1998, 281, 4018, 13, 286, 914, 11, 51048, 51048, 588, 1998, 281, 472, 293, 588, 1998, 281, 4018, 13, 2305, 9861, 307, 1359, 11, 550, 309, 311, 23119, 13, 682, 264, 4948, 295, 51292, 51292, 9861, 2681, 281, 4018, 11, 309, 311, 544, 411, 364, 4274, 767, 300, 291, 483, 13, 16985, 41167, 36896, 257, 707, 51504, 51504], "temperature": 0.0, "avg_logprob": -0.07196780888721196, "compression_ratio": 1.7592592592592593, "no_speech_prob": 9.516030331724323e-06}, {"id": 276, "seek": 179136, "start": 1805.04, "end": 1809.9199999999998, "text": " very close to one and very close to zero. Where beta is small, then it's softer. In the limit of", "tokens": [50364, 577, 1152, 264, 7316, 322, 264, 5598, 486, 312, 13, 407, 11, 365, 257, 588, 11, 588, 2416, 9861, 11, 291, 1936, 50824, 50824, 486, 362, 472, 295, 264, 23930, 2681, 281, 472, 293, 439, 264, 661, 2306, 588, 1998, 281, 4018, 13, 286, 914, 11, 51048, 51048, 588, 1998, 281, 472, 293, 588, 1998, 281, 4018, 13, 2305, 9861, 307, 1359, 11, 550, 309, 311, 23119, 13, 682, 264, 4948, 295, 51292, 51292, 9861, 2681, 281, 4018, 11, 309, 311, 544, 411, 364, 4274, 767, 300, 291, 483, 13, 16985, 41167, 36896, 257, 707, 51504, 51504], "temperature": 0.0, "avg_logprob": -0.07196780888721196, "compression_ratio": 1.7592592592592593, "no_speech_prob": 9.516030331724323e-06}, {"id": 277, "seek": 179136, "start": 1809.9199999999998, "end": 1814.1599999999999, "text": " beta equal to zero, it's more like an average actually that you get. Softmax behaves a little", "tokens": [50364, 577, 1152, 264, 7316, 322, 264, 5598, 486, 312, 13, 407, 11, 365, 257, 588, 11, 588, 2416, 9861, 11, 291, 1936, 50824, 50824, 486, 362, 472, 295, 264, 23930, 2681, 281, 472, 293, 439, 264, 661, 2306, 588, 1998, 281, 4018, 13, 286, 914, 11, 51048, 51048, 588, 1998, 281, 472, 293, 588, 1998, 281, 4018, 13, 2305, 9861, 307, 1359, 11, 550, 309, 311, 23119, 13, 682, 264, 4948, 295, 51292, 51292, 9861, 2681, 281, 4018, 11, 309, 311, 544, 411, 364, 4274, 767, 300, 291, 483, 13, 16985, 41167, 36896, 257, 707, 51504, 51504], "temperature": 0.0, "avg_logprob": -0.07196780888721196, "compression_ratio": 1.7592592592592593, "no_speech_prob": 9.516030331724323e-06}, {"id": 278, "seek": 181416, "start": 1814.16, "end": 1822.64, "text": " bit like an average. So, beta goes to infinity, it behaves a bit like argmax and beta goes to zero,", "tokens": [50364, 857, 411, 364, 4274, 13, 407, 11, 9861, 1709, 281, 13202, 11, 309, 36896, 257, 857, 411, 3882, 41167, 293, 9861, 1709, 281, 4018, 11, 50788, 50788, 309, 36896, 257, 857, 411, 364, 4274, 13, 407, 11, 498, 291, 362, 512, 1333, 295, 2710, 2144, 949, 264, 2787, 41167, 11, 51268, 51268, 550, 15164, 341, 13075, 4045, 291, 281, 1969, 341, 733, 295, 44019, 13, 51432, 51480, 400, 437, 561, 360, 2171, 294, 1629, 15077, 307, 300, 436, 722, 365, 257, 7226, 2295, 9861, 51856], "temperature": 0.0, "avg_logprob": -0.08067350826044192, "compression_ratio": 1.7009345794392523, "no_speech_prob": 1.3925326811659033e-06}, {"id": 279, "seek": 181416, "start": 1822.64, "end": 1832.24, "text": " it behaves a bit like an average. So, if you have some sort of normalization before the softmax,", "tokens": [50364, 857, 411, 364, 4274, 13, 407, 11, 9861, 1709, 281, 13202, 11, 309, 36896, 257, 857, 411, 3882, 41167, 293, 9861, 1709, 281, 4018, 11, 50788, 50788, 309, 36896, 257, 857, 411, 364, 4274, 13, 407, 11, 498, 291, 362, 512, 1333, 295, 2710, 2144, 949, 264, 2787, 41167, 11, 51268, 51268, 550, 15164, 341, 13075, 4045, 291, 281, 1969, 341, 733, 295, 44019, 13, 51432, 51480, 400, 437, 561, 360, 2171, 294, 1629, 15077, 307, 300, 436, 722, 365, 257, 7226, 2295, 9861, 51856], "temperature": 0.0, "avg_logprob": -0.08067350826044192, "compression_ratio": 1.7009345794392523, "no_speech_prob": 1.3925326811659033e-06}, {"id": 280, "seek": 181416, "start": 1832.24, "end": 1835.52, "text": " then tuning this parameter allows you to control this kind of hardness.", "tokens": [50364, 857, 411, 364, 4274, 13, 407, 11, 9861, 1709, 281, 13202, 11, 309, 36896, 257, 857, 411, 3882, 41167, 293, 9861, 1709, 281, 4018, 11, 50788, 50788, 309, 36896, 257, 857, 411, 364, 4274, 13, 407, 11, 498, 291, 362, 512, 1333, 295, 2710, 2144, 949, 264, 2787, 41167, 11, 51268, 51268, 550, 15164, 341, 13075, 4045, 291, 281, 1969, 341, 733, 295, 44019, 13, 51432, 51480, 400, 437, 561, 360, 2171, 294, 1629, 15077, 307, 300, 436, 722, 365, 257, 7226, 2295, 9861, 51856], "temperature": 0.0, "avg_logprob": -0.08067350826044192, "compression_ratio": 1.7009345794392523, "no_speech_prob": 1.3925326811659033e-06}, {"id": 281, "seek": 183552, "start": 1835.52, "end": 1843.44, "text": " And what people do sometimes in certain scenarios is that they start with a relatively low beta", "tokens": [50364, 400, 437, 561, 360, 2171, 294, 1629, 15077, 307, 300, 436, 722, 365, 257, 7226, 2295, 9861, 50760, 50808, 370, 300, 264, 3547, 300, 366, 7126, 366, 733, 295, 2787, 13, 407, 11, 291, 483, 2771, 2448, 5315, 13, 467, 311, 51164, 51164, 733, 295, 731, 12, 29437, 12865, 294, 2115, 295, 16235, 23475, 13, 400, 550, 382, 2614, 32280, 11, 51388, 51388, 498, 291, 528, 6081, 5327, 294, 428, 3202, 7513, 420, 2035, 11, 291, 3488, 9861, 13, 51640, 51684], "temperature": 0.0, "avg_logprob": -0.16954549536647567, "compression_ratio": 1.592920353982301, "no_speech_prob": 4.0927361624198966e-06}, {"id": 282, "seek": 183552, "start": 1844.4, "end": 1851.52, "text": " so that the numbers that are produced are kind of soft. So, you get gradients everywhere. It's", "tokens": [50364, 400, 437, 561, 360, 2171, 294, 1629, 15077, 307, 300, 436, 722, 365, 257, 7226, 2295, 9861, 50760, 50808, 370, 300, 264, 3547, 300, 366, 7126, 366, 733, 295, 2787, 13, 407, 11, 291, 483, 2771, 2448, 5315, 13, 467, 311, 51164, 51164, 733, 295, 731, 12, 29437, 12865, 294, 2115, 295, 16235, 23475, 13, 400, 550, 382, 2614, 32280, 11, 51388, 51388, 498, 291, 528, 6081, 5327, 294, 428, 3202, 7513, 420, 2035, 11, 291, 3488, 9861, 13, 51640, 51684], "temperature": 0.0, "avg_logprob": -0.16954549536647567, "compression_ratio": 1.592920353982301, "no_speech_prob": 4.0927361624198966e-06}, {"id": 283, "seek": 183552, "start": 1851.52, "end": 1856.0, "text": " kind of well-behaved in terms of gradient descent. And then as running proceeds,", "tokens": [50364, 400, 437, 561, 360, 2171, 294, 1629, 15077, 307, 300, 436, 722, 365, 257, 7226, 2295, 9861, 50760, 50808, 370, 300, 264, 3547, 300, 366, 7126, 366, 733, 295, 2787, 13, 407, 11, 291, 483, 2771, 2448, 5315, 13, 467, 311, 51164, 51164, 733, 295, 731, 12, 29437, 12865, 294, 2115, 295, 16235, 23475, 13, 400, 550, 382, 2614, 32280, 11, 51388, 51388, 498, 291, 528, 6081, 5327, 294, 428, 3202, 7513, 420, 2035, 11, 291, 3488, 9861, 13, 51640, 51684], "temperature": 0.0, "avg_logprob": -0.16954549536647567, "compression_ratio": 1.592920353982301, "no_speech_prob": 4.0927361624198966e-06}, {"id": 284, "seek": 183552, "start": 1856.0, "end": 1861.04, "text": " if you want harder decisions in your attention mechanism or whatever, you increase beta.", "tokens": [50364, 400, 437, 561, 360, 2171, 294, 1629, 15077, 307, 300, 436, 722, 365, 257, 7226, 2295, 9861, 50760, 50808, 370, 300, 264, 3547, 300, 366, 7126, 366, 733, 295, 2787, 13, 407, 11, 291, 483, 2771, 2448, 5315, 13, 467, 311, 51164, 51164, 733, 295, 731, 12, 29437, 12865, 294, 2115, 295, 16235, 23475, 13, 400, 550, 382, 2614, 32280, 11, 51388, 51388, 498, 291, 528, 6081, 5327, 294, 428, 3202, 7513, 420, 2035, 11, 291, 3488, 9861, 13, 51640, 51684], "temperature": 0.0, "avg_logprob": -0.16954549536647567, "compression_ratio": 1.592920353982301, "no_speech_prob": 4.0927361624198966e-06}, {"id": 285, "seek": 186104, "start": 1861.04, "end": 1865.44, "text": " You increase beta. And so, that makes the system kind of make harder decisions.", "tokens": [50364, 509, 3488, 9861, 13, 400, 370, 11, 300, 1669, 264, 1185, 733, 295, 652, 6081, 5327, 13, 50584, 50584, 467, 1177, 380, 1190, 382, 731, 3602, 11, 457, 26742, 934, 257, 1326, 36540, 11, 309, 311, 733, 295, 294, 264, 50796, 50796, 558, 6582, 3884, 13, 407, 11, 291, 393, 1333, 295, 31570, 264, 5327, 456, 538, 733, 295, 5662, 9861, 13, 51120, 51168, 467, 311, 4420, 11, 337, 1365, 11, 294, 257, 9925, 295, 8572, 293, 2698, 12, 1591, 1251, 3652, 366, 733, 295, 11, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13197163475884333, "compression_ratio": 1.5972850678733033, "no_speech_prob": 8.800659998087212e-06}, {"id": 286, "seek": 186104, "start": 1865.44, "end": 1869.68, "text": " It doesn't run as well anymore, but presumably after a few iterations, it's kind of in the", "tokens": [50364, 509, 3488, 9861, 13, 400, 370, 11, 300, 1669, 264, 1185, 733, 295, 652, 6081, 5327, 13, 50584, 50584, 467, 1177, 380, 1190, 382, 731, 3602, 11, 457, 26742, 934, 257, 1326, 36540, 11, 309, 311, 733, 295, 294, 264, 50796, 50796, 558, 6582, 3884, 13, 407, 11, 291, 393, 1333, 295, 31570, 264, 5327, 456, 538, 733, 295, 5662, 9861, 13, 51120, 51168, 467, 311, 4420, 11, 337, 1365, 11, 294, 257, 9925, 295, 8572, 293, 2698, 12, 1591, 1251, 3652, 366, 733, 295, 11, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13197163475884333, "compression_ratio": 1.5972850678733033, "no_speech_prob": 8.800659998087212e-06}, {"id": 287, "seek": 186104, "start": 1869.68, "end": 1876.1599999999999, "text": " right boat park. So, you can sort of sharpen the decisions there by kind of increasing beta.", "tokens": [50364, 509, 3488, 9861, 13, 400, 370, 11, 300, 1669, 264, 1185, 733, 295, 652, 6081, 5327, 13, 50584, 50584, 467, 1177, 380, 1190, 382, 731, 3602, 11, 457, 26742, 934, 257, 1326, 36540, 11, 309, 311, 733, 295, 294, 264, 50796, 50796, 558, 6582, 3884, 13, 407, 11, 291, 393, 1333, 295, 31570, 264, 5327, 456, 538, 733, 295, 5662, 9861, 13, 51120, 51168, 467, 311, 4420, 11, 337, 1365, 11, 294, 257, 9925, 295, 8572, 293, 2698, 12, 1591, 1251, 3652, 366, 733, 295, 11, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13197163475884333, "compression_ratio": 1.5972850678733033, "no_speech_prob": 8.800659998087212e-06}, {"id": 288, "seek": 186104, "start": 1877.12, "end": 1885.04, "text": " It's useful, for example, in a mixture of experts and self-attention systems are kind of,", "tokens": [50364, 509, 3488, 9861, 13, 400, 370, 11, 300, 1669, 264, 1185, 733, 295, 652, 6081, 5327, 13, 50584, 50584, 467, 1177, 380, 1190, 382, 731, 3602, 11, 457, 26742, 934, 257, 1326, 36540, 11, 309, 311, 733, 295, 294, 264, 50796, 50796, 558, 6582, 3884, 13, 407, 11, 291, 393, 1333, 295, 31570, 264, 5327, 456, 538, 733, 295, 5662, 9861, 13, 51120, 51168, 467, 311, 4420, 11, 337, 1365, 11, 294, 257, 9925, 295, 8572, 293, 2698, 12, 1591, 1251, 3652, 366, 733, 295, 11, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13197163475884333, "compression_ratio": 1.5972850678733033, "no_speech_prob": 8.800659998087212e-06}, {"id": 289, "seek": 188504, "start": 1885.04, "end": 1892.08, "text": " you can think of as sort of a weird form of mixture of experts. So, in a mixture of experts,", "tokens": [50364, 291, 393, 519, 295, 382, 1333, 295, 257, 3657, 1254, 295, 9925, 295, 8572, 13, 407, 11, 294, 257, 9925, 295, 8572, 11, 50716, 50716, 291, 362, 3866, 1422, 12, 7129, 18357, 11, 293, 641, 23930, 366, 733, 295, 43586, 9354, 365, 31994, 50972, 50972, 300, 366, 264, 5598, 295, 257, 2787, 41167, 2564, 10164, 538, 1071, 18161, 2533, 13, 407, 11, 498, 291, 528, 733, 295, 257, 51396, 51396, 2787, 9925, 11, 291, 362, 257, 2295, 9861, 13, 400, 382, 291, 3488, 9861, 281, 13202, 11, 1936, 11, 291, 434, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.11155507940995066, "compression_ratio": 1.6858407079646018, "no_speech_prob": 2.726415914366953e-06}, {"id": 290, "seek": 188504, "start": 1892.08, "end": 1897.2, "text": " you have multiple sub-networks, and their outputs are kind of linearly combined with coefficients", "tokens": [50364, 291, 393, 519, 295, 382, 1333, 295, 257, 3657, 1254, 295, 9925, 295, 8572, 13, 407, 11, 294, 257, 9925, 295, 8572, 11, 50716, 50716, 291, 362, 3866, 1422, 12, 7129, 18357, 11, 293, 641, 23930, 366, 733, 295, 43586, 9354, 365, 31994, 50972, 50972, 300, 366, 264, 5598, 295, 257, 2787, 41167, 2564, 10164, 538, 1071, 18161, 2533, 13, 407, 11, 498, 291, 528, 733, 295, 257, 51396, 51396, 2787, 9925, 11, 291, 362, 257, 2295, 9861, 13, 400, 382, 291, 3488, 9861, 281, 13202, 11, 1936, 11, 291, 434, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.11155507940995066, "compression_ratio": 1.6858407079646018, "no_speech_prob": 2.726415914366953e-06}, {"id": 291, "seek": 188504, "start": 1897.2, "end": 1905.68, "text": " that are the output of a softmax itself controlled by another neural net. So, if you want kind of a", "tokens": [50364, 291, 393, 519, 295, 382, 1333, 295, 257, 3657, 1254, 295, 9925, 295, 8572, 13, 407, 11, 294, 257, 9925, 295, 8572, 11, 50716, 50716, 291, 362, 3866, 1422, 12, 7129, 18357, 11, 293, 641, 23930, 366, 733, 295, 43586, 9354, 365, 31994, 50972, 50972, 300, 366, 264, 5598, 295, 257, 2787, 41167, 2564, 10164, 538, 1071, 18161, 2533, 13, 407, 11, 498, 291, 528, 733, 295, 257, 51396, 51396, 2787, 9925, 11, 291, 362, 257, 2295, 9861, 13, 400, 382, 291, 3488, 9861, 281, 13202, 11, 1936, 11, 291, 434, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.11155507940995066, "compression_ratio": 1.6858407079646018, "no_speech_prob": 2.726415914366953e-06}, {"id": 292, "seek": 188504, "start": 1905.68, "end": 1910.0, "text": " soft mixture, you have a low beta. And as you increase beta to infinity, basically, you're", "tokens": [50364, 291, 393, 519, 295, 382, 1333, 295, 257, 3657, 1254, 295, 9925, 295, 8572, 13, 407, 11, 294, 257, 9925, 295, 8572, 11, 50716, 50716, 291, 362, 3866, 1422, 12, 7129, 18357, 11, 293, 641, 23930, 366, 733, 295, 43586, 9354, 365, 31994, 50972, 50972, 300, 366, 264, 5598, 295, 257, 2787, 41167, 2564, 10164, 538, 1071, 18161, 2533, 13, 407, 11, 498, 291, 528, 733, 295, 257, 51396, 51396, 2787, 9925, 11, 291, 362, 257, 2295, 9861, 13, 400, 382, 291, 3488, 9861, 281, 13202, 11, 1936, 11, 291, 434, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.11155507940995066, "compression_ratio": 1.6858407079646018, "no_speech_prob": 2.726415914366953e-06}, {"id": 293, "seek": 191000, "start": 1910.0, "end": 1915.36, "text": " going to select one of the experts and ignore all the other ones. That might be useful, for example,", "tokens": [50364, 516, 281, 3048, 472, 295, 264, 8572, 293, 11200, 439, 264, 661, 2306, 13, 663, 1062, 312, 4420, 11, 337, 1365, 11, 50632, 50632, 498, 291, 528, 281, 3847, 257, 9925, 295, 8572, 420, 364, 3202, 7513, 13, 583, 294, 264, 917, 11, 50852, 50852, 291, 528, 281, 3155, 24903, 538, 445, 23751, 597, 5844, 360, 286, 643, 281, 14722, 293, 445, 406, 51100, 51100, 15866, 264, 661, 2306, 13, 407, 11, 294, 300, 1389, 11, 291, 528, 729, 31994, 281, 312, 1936, 2139, 51300, 51300, 472, 420, 4018, 13, 400, 291, 393, 3847, 264, 1185, 46667, 281, 360, 341, 538, 5662, 9861, 13, 51700, 51780], "temperature": 0.0, "avg_logprob": -0.05874389189260977, "compression_ratio": 1.7340823970037453, "no_speech_prob": 2.225228627139586e-06}, {"id": 294, "seek": 191000, "start": 1915.36, "end": 1919.76, "text": " if you want to train a mixture of experts or an attention mechanism. But in the end,", "tokens": [50364, 516, 281, 3048, 472, 295, 264, 8572, 293, 11200, 439, 264, 661, 2306, 13, 663, 1062, 312, 4420, 11, 337, 1365, 11, 50632, 50632, 498, 291, 528, 281, 3847, 257, 9925, 295, 8572, 420, 364, 3202, 7513, 13, 583, 294, 264, 917, 11, 50852, 50852, 291, 528, 281, 3155, 24903, 538, 445, 23751, 597, 5844, 360, 286, 643, 281, 14722, 293, 445, 406, 51100, 51100, 15866, 264, 661, 2306, 13, 407, 11, 294, 300, 1389, 11, 291, 528, 729, 31994, 281, 312, 1936, 2139, 51300, 51300, 472, 420, 4018, 13, 400, 291, 393, 3847, 264, 1185, 46667, 281, 360, 341, 538, 5662, 9861, 13, 51700, 51780], "temperature": 0.0, "avg_logprob": -0.05874389189260977, "compression_ratio": 1.7340823970037453, "no_speech_prob": 2.225228627139586e-06}, {"id": 295, "seek": 191000, "start": 1919.76, "end": 1924.72, "text": " you want to save computation by just determining which expert do I need to compute and just not", "tokens": [50364, 516, 281, 3048, 472, 295, 264, 8572, 293, 11200, 439, 264, 661, 2306, 13, 663, 1062, 312, 4420, 11, 337, 1365, 11, 50632, 50632, 498, 291, 528, 281, 3847, 257, 9925, 295, 8572, 420, 364, 3202, 7513, 13, 583, 294, 264, 917, 11, 50852, 50852, 291, 528, 281, 3155, 24903, 538, 445, 23751, 597, 5844, 360, 286, 643, 281, 14722, 293, 445, 406, 51100, 51100, 15866, 264, 661, 2306, 13, 407, 11, 294, 300, 1389, 11, 291, 528, 729, 31994, 281, 312, 1936, 2139, 51300, 51300, 472, 420, 4018, 13, 400, 291, 393, 3847, 264, 1185, 46667, 281, 360, 341, 538, 5662, 9861, 13, 51700, 51780], "temperature": 0.0, "avg_logprob": -0.05874389189260977, "compression_ratio": 1.7340823970037453, "no_speech_prob": 2.225228627139586e-06}, {"id": 296, "seek": 191000, "start": 1924.72, "end": 1928.72, "text": " computing the other ones. So, in that case, you want those coefficients to be basically either", "tokens": [50364, 516, 281, 3048, 472, 295, 264, 8572, 293, 11200, 439, 264, 661, 2306, 13, 663, 1062, 312, 4420, 11, 337, 1365, 11, 50632, 50632, 498, 291, 528, 281, 3847, 257, 9925, 295, 8572, 420, 364, 3202, 7513, 13, 583, 294, 264, 917, 11, 50852, 50852, 291, 528, 281, 3155, 24903, 538, 445, 23751, 597, 5844, 360, 286, 643, 281, 14722, 293, 445, 406, 51100, 51100, 15866, 264, 661, 2306, 13, 407, 11, 294, 300, 1389, 11, 291, 528, 729, 31994, 281, 312, 1936, 2139, 51300, 51300, 472, 420, 4018, 13, 400, 291, 393, 3847, 264, 1185, 46667, 281, 360, 341, 538, 5662, 9861, 13, 51700, 51780], "temperature": 0.0, "avg_logprob": -0.05874389189260977, "compression_ratio": 1.7340823970037453, "no_speech_prob": 2.225228627139586e-06}, {"id": 297, "seek": 191000, "start": 1928.72, "end": 1936.72, "text": " one or zero. And you can train the system progressively to do this by increasing beta.", "tokens": [50364, 516, 281, 3048, 472, 295, 264, 8572, 293, 11200, 439, 264, 661, 2306, 13, 663, 1062, 312, 4420, 11, 337, 1365, 11, 50632, 50632, 498, 291, 528, 281, 3847, 257, 9925, 295, 8572, 420, 364, 3202, 7513, 13, 583, 294, 264, 917, 11, 50852, 50852, 291, 528, 281, 3155, 24903, 538, 445, 23751, 597, 5844, 360, 286, 643, 281, 14722, 293, 445, 406, 51100, 51100, 15866, 264, 661, 2306, 13, 407, 11, 294, 300, 1389, 11, 291, 528, 729, 31994, 281, 312, 1936, 2139, 51300, 51300, 472, 420, 4018, 13, 400, 291, 393, 3847, 264, 1185, 46667, 281, 360, 341, 538, 5662, 9861, 13, 51700, 51780], "temperature": 0.0, "avg_logprob": -0.05874389189260977, "compression_ratio": 1.7340823970037453, "no_speech_prob": 2.225228627139586e-06}, {"id": 298, "seek": 193672, "start": 1936.72, "end": 1941.6000000000001, "text": " This is cool. The physicists have a name for this because the use is going to trick", "tokens": [50364, 639, 307, 1627, 13, 440, 48716, 362, 257, 1315, 337, 341, 570, 264, 764, 307, 516, 281, 4282, 50608, 50608, 370, 3683, 661, 721, 13, 663, 311, 1219, 22256, 4270, 13, 467, 575, 264, 912, 3620, 382, 11, 50800, 50884, 370, 22256, 4270, 1487, 490, 5760, 1902, 11, 558, 30, 509, 434, 1455, 257, 8269, 420, 746, 11, 293, 291, 652, 257, 51340, 51340, 10576, 420, 746, 11, 558, 30, 400, 291, 3738, 309, 493, 11, 293, 550, 291, 1627, 309, 13, 400, 5413, 322, 1968, 291, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1559029044685783, "compression_ratio": 1.6296296296296295, "no_speech_prob": 5.682209121005144e-06}, {"id": 299, "seek": 193672, "start": 1941.6000000000001, "end": 1945.44, "text": " so various other things. That's called annealing. It has the same meaning as,", "tokens": [50364, 639, 307, 1627, 13, 440, 48716, 362, 257, 1315, 337, 341, 570, 264, 764, 307, 516, 281, 4282, 50608, 50608, 370, 3683, 661, 721, 13, 663, 311, 1219, 22256, 4270, 13, 467, 575, 264, 912, 3620, 382, 11, 50800, 50884, 370, 22256, 4270, 1487, 490, 5760, 1902, 11, 558, 30, 509, 434, 1455, 257, 8269, 420, 746, 11, 293, 291, 652, 257, 51340, 51340, 10576, 420, 746, 11, 558, 30, 400, 291, 3738, 309, 493, 11, 293, 550, 291, 1627, 309, 13, 400, 5413, 322, 1968, 291, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1559029044685783, "compression_ratio": 1.6296296296296295, "no_speech_prob": 5.682209121005144e-06}, {"id": 300, "seek": 193672, "start": 1947.1200000000001, "end": 1956.24, "text": " so annealing comes from metalwork, right? You're making a steel or something, and you make a", "tokens": [50364, 639, 307, 1627, 13, 440, 48716, 362, 257, 1315, 337, 341, 570, 264, 764, 307, 516, 281, 4282, 50608, 50608, 370, 3683, 661, 721, 13, 663, 311, 1219, 22256, 4270, 13, 467, 575, 264, 912, 3620, 382, 11, 50800, 50884, 370, 22256, 4270, 1487, 490, 5760, 1902, 11, 558, 30, 509, 434, 1455, 257, 8269, 420, 746, 11, 293, 291, 652, 257, 51340, 51340, 10576, 420, 746, 11, 558, 30, 400, 291, 3738, 309, 493, 11, 293, 550, 291, 1627, 309, 13, 400, 5413, 322, 1968, 291, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1559029044685783, "compression_ratio": 1.6296296296296295, "no_speech_prob": 5.682209121005144e-06}, {"id": 301, "seek": 193672, "start": 1956.24, "end": 1963.92, "text": " sword or something, right? And you heat it up, and then you cool it. And depending on whether you", "tokens": [50364, 639, 307, 1627, 13, 440, 48716, 362, 257, 1315, 337, 341, 570, 264, 764, 307, 516, 281, 4282, 50608, 50608, 370, 3683, 661, 721, 13, 663, 311, 1219, 22256, 4270, 13, 467, 575, 264, 912, 3620, 382, 11, 50800, 50884, 370, 22256, 4270, 1487, 490, 5760, 1902, 11, 558, 30, 509, 434, 1455, 257, 8269, 420, 746, 11, 293, 291, 652, 257, 51340, 51340, 10576, 420, 746, 11, 558, 30, 400, 291, 3738, 309, 493, 11, 293, 550, 291, 1627, 309, 13, 400, 5413, 322, 1968, 291, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1559029044685783, "compression_ratio": 1.6296296296296295, "no_speech_prob": 5.682209121005144e-06}, {"id": 302, "seek": 196392, "start": 1963.92, "end": 1972.3200000000002, "text": " cool it quickly or slowly, you change the crystalline structure of the metal. So, this idea", "tokens": [50364, 1627, 309, 2661, 420, 5692, 11, 291, 1319, 264, 31924, 533, 3877, 295, 264, 5760, 13, 407, 11, 341, 1558, 50784, 50784, 295, 22256, 4270, 11, 295, 46667, 28124, 264, 4292, 11, 23249, 281, 341, 5662, 51032, 51032, 341, 9861, 13, 33286, 307, 411, 364, 17340, 4292, 13, 467, 311, 47540, 281, 364, 17340, 4292, 13, 2639, 661, 51328, 51328, 1168, 30, 286, 519, 321, 366, 665, 13, 1057, 558, 13, 1033, 13, 407, 11, 958, 4829, 307, 4470, 6828, 13, 51808], "temperature": 0.0, "avg_logprob": -0.09956128256661552, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.6267451175954193e-05}, {"id": 303, "seek": 196392, "start": 1972.3200000000002, "end": 1977.28, "text": " of annealing, of progressively lowering the temperature, corresponds to this increasing", "tokens": [50364, 1627, 309, 2661, 420, 5692, 11, 291, 1319, 264, 31924, 533, 3877, 295, 264, 5760, 13, 407, 11, 341, 1558, 50784, 50784, 295, 22256, 4270, 11, 295, 46667, 28124, 264, 4292, 11, 23249, 281, 341, 5662, 51032, 51032, 341, 9861, 13, 33286, 307, 411, 364, 17340, 4292, 13, 467, 311, 47540, 281, 364, 17340, 4292, 13, 2639, 661, 51328, 51328, 1168, 30, 286, 519, 321, 366, 665, 13, 1057, 558, 13, 1033, 13, 407, 11, 958, 4829, 307, 4470, 6828, 13, 51808], "temperature": 0.0, "avg_logprob": -0.09956128256661552, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.6267451175954193e-05}, {"id": 304, "seek": 196392, "start": 1977.28, "end": 1983.2, "text": " this beta. Beta is like an inverse temperature. It's akin to an inverse temperature. Any other", "tokens": [50364, 1627, 309, 2661, 420, 5692, 11, 291, 1319, 264, 31924, 533, 3877, 295, 264, 5760, 13, 407, 11, 341, 1558, 50784, 50784, 295, 22256, 4270, 11, 295, 46667, 28124, 264, 4292, 11, 23249, 281, 341, 5662, 51032, 51032, 341, 9861, 13, 33286, 307, 411, 364, 17340, 4292, 13, 467, 311, 47540, 281, 364, 17340, 4292, 13, 2639, 661, 51328, 51328, 1168, 30, 286, 519, 321, 366, 665, 13, 1057, 558, 13, 1033, 13, 407, 11, 958, 4829, 307, 4470, 6828, 13, 51808], "temperature": 0.0, "avg_logprob": -0.09956128256661552, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.6267451175954193e-05}, {"id": 305, "seek": 198320, "start": 1983.2, "end": 1994.72, "text": " question? I think we are good. All right. Okay. So, next topic is loss functions.", "tokens": [50364, 1168, 30, 286, 519, 321, 366, 665, 13, 1057, 558, 13, 1033, 13, 407, 11, 958, 4829, 307, 4470, 6828, 13, 50940, 51116, 407, 11, 9953, 51, 284, 339, 575, 257, 1379, 3840, 295, 4470, 6828, 11, 382, 291, 1062, 362, 1612, 13, 400, 11, 295, 1164, 11, 456, 366, 51480, 51480, 721, 300, 366, 2199, 2306, 411, 914, 8889, 6713, 13, 407, 11, 286, 500, 380, 643, 281, 2903, 281, 291, 437, 309, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1389941435593825, "compression_ratio": 1.4680851063829787, "no_speech_prob": 6.338548246276332e-06}, {"id": 306, "seek": 198320, "start": 1998.24, "end": 2005.52, "text": " So, PyTorch has a whole bunch of loss functions, as you might have seen. And, of course, there are", "tokens": [50364, 1168, 30, 286, 519, 321, 366, 665, 13, 1057, 558, 13, 1033, 13, 407, 11, 958, 4829, 307, 4470, 6828, 13, 50940, 51116, 407, 11, 9953, 51, 284, 339, 575, 257, 1379, 3840, 295, 4470, 6828, 11, 382, 291, 1062, 362, 1612, 13, 400, 11, 295, 1164, 11, 456, 366, 51480, 51480, 721, 300, 366, 2199, 2306, 411, 914, 8889, 6713, 13, 407, 11, 286, 500, 380, 643, 281, 2903, 281, 291, 437, 309, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1389941435593825, "compression_ratio": 1.4680851063829787, "no_speech_prob": 6.338548246276332e-06}, {"id": 307, "seek": 198320, "start": 2005.52, "end": 2012.88, "text": " things that are simple ones like mean squared error. So, I don't need to explain to you what it", "tokens": [50364, 1168, 30, 286, 519, 321, 366, 665, 13, 1057, 558, 13, 1033, 13, 407, 11, 958, 4829, 307, 4470, 6828, 13, 50940, 51116, 407, 11, 9953, 51, 284, 339, 575, 257, 1379, 3840, 295, 4470, 6828, 11, 382, 291, 1062, 362, 1612, 13, 400, 11, 295, 1164, 11, 456, 366, 51480, 51480, 721, 300, 366, 2199, 2306, 411, 914, 8889, 6713, 13, 407, 11, 286, 500, 380, 643, 281, 2903, 281, 291, 437, 309, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1389941435593825, "compression_ratio": 1.4680851063829787, "no_speech_prob": 6.338548246276332e-06}, {"id": 308, "seek": 201288, "start": 2012.88, "end": 2018.48, "text": " is. You know, compute the square of the error between the desired output, y, and the actual", "tokens": [50364, 307, 13, 509, 458, 11, 14722, 264, 3732, 295, 264, 6713, 1296, 264, 14721, 5598, 11, 288, 11, 293, 264, 3539, 50644, 50644, 5598, 11, 2031, 13, 400, 498, 309, 311, 670, 257, 8382, 12, 65, 852, 365, 297, 10938, 11, 550, 291, 362, 11, 291, 458, 11, 297, 15352, 11, 51032, 51032, 472, 337, 1184, 295, 264, 10938, 294, 264, 15245, 13, 400, 291, 393, 980, 341, 4470, 2445, 281, 2139, 1066, 300, 51304, 51304, 8062, 420, 281, 733, 295, 5407, 309, 538, 15866, 257, 914, 420, 257, 2408, 13, 1033, 13, 4372, 2199, 13, 51568, 51824], "temperature": 0.0, "avg_logprob": -0.10293656349182129, "compression_ratio": 1.5973451327433628, "no_speech_prob": 8.397505553148221e-06}, {"id": 309, "seek": 201288, "start": 2018.48, "end": 2026.24, "text": " output, x. And if it's over a mini-batch with n samples, then you have, you know, n losses,", "tokens": [50364, 307, 13, 509, 458, 11, 14722, 264, 3732, 295, 264, 6713, 1296, 264, 14721, 5598, 11, 288, 11, 293, 264, 3539, 50644, 50644, 5598, 11, 2031, 13, 400, 498, 309, 311, 670, 257, 8382, 12, 65, 852, 365, 297, 10938, 11, 550, 291, 362, 11, 291, 458, 11, 297, 15352, 11, 51032, 51032, 472, 337, 1184, 295, 264, 10938, 294, 264, 15245, 13, 400, 291, 393, 980, 341, 4470, 2445, 281, 2139, 1066, 300, 51304, 51304, 8062, 420, 281, 733, 295, 5407, 309, 538, 15866, 257, 914, 420, 257, 2408, 13, 1033, 13, 4372, 2199, 13, 51568, 51824], "temperature": 0.0, "avg_logprob": -0.10293656349182129, "compression_ratio": 1.5973451327433628, "no_speech_prob": 8.397505553148221e-06}, {"id": 310, "seek": 201288, "start": 2026.24, "end": 2031.68, "text": " one for each of the samples in the batch. And you can tell this loss function to either keep that", "tokens": [50364, 307, 13, 509, 458, 11, 14722, 264, 3732, 295, 264, 6713, 1296, 264, 14721, 5598, 11, 288, 11, 293, 264, 3539, 50644, 50644, 5598, 11, 2031, 13, 400, 498, 309, 311, 670, 257, 8382, 12, 65, 852, 365, 297, 10938, 11, 550, 291, 362, 11, 291, 458, 11, 297, 15352, 11, 51032, 51032, 472, 337, 1184, 295, 264, 10938, 294, 264, 15245, 13, 400, 291, 393, 980, 341, 4470, 2445, 281, 2139, 1066, 300, 51304, 51304, 8062, 420, 281, 733, 295, 5407, 309, 538, 15866, 257, 914, 420, 257, 2408, 13, 1033, 13, 4372, 2199, 13, 51568, 51824], "temperature": 0.0, "avg_logprob": -0.10293656349182129, "compression_ratio": 1.5973451327433628, "no_speech_prob": 8.397505553148221e-06}, {"id": 311, "seek": 201288, "start": 2031.68, "end": 2036.96, "text": " vector or to kind of reduce it by computing a mean or a sum. Okay. Very simple.", "tokens": [50364, 307, 13, 509, 458, 11, 14722, 264, 3732, 295, 264, 6713, 1296, 264, 14721, 5598, 11, 288, 11, 293, 264, 3539, 50644, 50644, 5598, 11, 2031, 13, 400, 498, 309, 311, 670, 257, 8382, 12, 65, 852, 365, 297, 10938, 11, 550, 291, 362, 11, 291, 458, 11, 297, 15352, 11, 51032, 51032, 472, 337, 1184, 295, 264, 10938, 294, 264, 15245, 13, 400, 291, 393, 980, 341, 4470, 2445, 281, 2139, 1066, 300, 51304, 51304, 8062, 420, 281, 733, 295, 5407, 309, 538, 15866, 257, 914, 420, 257, 2408, 13, 1033, 13, 4372, 2199, 13, 51568, 51824], "temperature": 0.0, "avg_logprob": -0.10293656349182129, "compression_ratio": 1.5973451327433628, "no_speech_prob": 8.397505553148221e-06}, {"id": 312, "seek": 203696, "start": 2036.96, "end": 2040.64, "text": " Here's a different loss. That's the L1 loss. So, this is basically the absolute value of the", "tokens": [50364, 1692, 311, 257, 819, 4470, 13, 663, 311, 264, 441, 16, 4470, 13, 407, 11, 341, 307, 1936, 264, 8236, 2158, 295, 264, 50548, 50548, 2649, 1296, 264, 14721, 5598, 293, 264, 3539, 5598, 13, 400, 291, 528, 281, 764, 341, 281, 360, 50848, 50848, 437, 311, 1219, 13956, 24590, 13, 407, 11, 498, 291, 528, 1359, 13603, 281, 1207, 257, 688, 293, 2416, 13603, 281, 51192, 51192, 1207, 11, 457, 11, 291, 458, 11, 406, 382, 709, 382, 498, 291, 764, 264, 3732, 11, 4317, 570, 291, 362, 5658, 294, 428, 51440, 51440, 1412, 13, 407, 11, 291, 458, 300, 291, 362, 257, 3840, 295, 1412, 2793, 13, 509, 362, 257, 3840, 295, 1412, 2793, 13, 509, 362, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.1991804169445503, "compression_ratio": 1.8858267716535433, "no_speech_prob": 5.014598627894884e-06}, {"id": 313, "seek": 203696, "start": 2040.64, "end": 2046.64, "text": " difference between the desired output and the actual output. And you want to use this to do", "tokens": [50364, 1692, 311, 257, 819, 4470, 13, 663, 311, 264, 441, 16, 4470, 13, 407, 11, 341, 307, 1936, 264, 8236, 2158, 295, 264, 50548, 50548, 2649, 1296, 264, 14721, 5598, 293, 264, 3539, 5598, 13, 400, 291, 528, 281, 764, 341, 281, 360, 50848, 50848, 437, 311, 1219, 13956, 24590, 13, 407, 11, 498, 291, 528, 1359, 13603, 281, 1207, 257, 688, 293, 2416, 13603, 281, 51192, 51192, 1207, 11, 457, 11, 291, 458, 11, 406, 382, 709, 382, 498, 291, 764, 264, 3732, 11, 4317, 570, 291, 362, 5658, 294, 428, 51440, 51440, 1412, 13, 407, 11, 291, 458, 300, 291, 362, 257, 3840, 295, 1412, 2793, 13, 509, 362, 257, 3840, 295, 1412, 2793, 13, 509, 362, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.1991804169445503, "compression_ratio": 1.8858267716535433, "no_speech_prob": 5.014598627894884e-06}, {"id": 314, "seek": 203696, "start": 2046.64, "end": 2053.52, "text": " what's called robust regression. So, if you want small errors to count a lot and large errors to", "tokens": [50364, 1692, 311, 257, 819, 4470, 13, 663, 311, 264, 441, 16, 4470, 13, 407, 11, 341, 307, 1936, 264, 8236, 2158, 295, 264, 50548, 50548, 2649, 1296, 264, 14721, 5598, 293, 264, 3539, 5598, 13, 400, 291, 528, 281, 764, 341, 281, 360, 50848, 50848, 437, 311, 1219, 13956, 24590, 13, 407, 11, 498, 291, 528, 1359, 13603, 281, 1207, 257, 688, 293, 2416, 13603, 281, 51192, 51192, 1207, 11, 457, 11, 291, 458, 11, 406, 382, 709, 382, 498, 291, 764, 264, 3732, 11, 4317, 570, 291, 362, 5658, 294, 428, 51440, 51440, 1412, 13, 407, 11, 291, 458, 300, 291, 362, 257, 3840, 295, 1412, 2793, 13, 509, 362, 257, 3840, 295, 1412, 2793, 13, 509, 362, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.1991804169445503, "compression_ratio": 1.8858267716535433, "no_speech_prob": 5.014598627894884e-06}, {"id": 315, "seek": 203696, "start": 2053.52, "end": 2058.48, "text": " count, but, you know, not as much as if you use the square, perhaps because you have noise in your", "tokens": [50364, 1692, 311, 257, 819, 4470, 13, 663, 311, 264, 441, 16, 4470, 13, 407, 11, 341, 307, 1936, 264, 8236, 2158, 295, 264, 50548, 50548, 2649, 1296, 264, 14721, 5598, 293, 264, 3539, 5598, 13, 400, 291, 528, 281, 764, 341, 281, 360, 50848, 50848, 437, 311, 1219, 13956, 24590, 13, 407, 11, 498, 291, 528, 1359, 13603, 281, 1207, 257, 688, 293, 2416, 13603, 281, 51192, 51192, 1207, 11, 457, 11, 291, 458, 11, 406, 382, 709, 382, 498, 291, 764, 264, 3732, 11, 4317, 570, 291, 362, 5658, 294, 428, 51440, 51440, 1412, 13, 407, 11, 291, 458, 300, 291, 362, 257, 3840, 295, 1412, 2793, 13, 509, 362, 257, 3840, 295, 1412, 2793, 13, 509, 362, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.1991804169445503, "compression_ratio": 1.8858267716535433, "no_speech_prob": 5.014598627894884e-06}, {"id": 316, "seek": 203696, "start": 2058.48, "end": 2062.4, "text": " data. So, you know that you have a bunch of data points. You have a bunch of data points. You have", "tokens": [50364, 1692, 311, 257, 819, 4470, 13, 663, 311, 264, 441, 16, 4470, 13, 407, 11, 341, 307, 1936, 264, 8236, 2158, 295, 264, 50548, 50548, 2649, 1296, 264, 14721, 5598, 293, 264, 3539, 5598, 13, 400, 291, 528, 281, 764, 341, 281, 360, 50848, 50848, 437, 311, 1219, 13956, 24590, 13, 407, 11, 498, 291, 528, 1359, 13603, 281, 1207, 257, 688, 293, 2416, 13603, 281, 51192, 51192, 1207, 11, 457, 11, 291, 458, 11, 406, 382, 709, 382, 498, 291, 764, 264, 3732, 11, 4317, 570, 291, 362, 5658, 294, 428, 51440, 51440, 1412, 13, 407, 11, 291, 458, 300, 291, 362, 257, 3840, 295, 1412, 2793, 13, 509, 362, 257, 3840, 295, 1412, 2793, 13, 509, 362, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.1991804169445503, "compression_ratio": 1.8858267716535433, "no_speech_prob": 5.014598627894884e-06}, {"id": 317, "seek": 206240, "start": 2062.4, "end": 2067.2000000000003, "text": " a bunch of data points. You're trying to kind of train a neural net or something to kind of, you", "tokens": [50364, 257, 3840, 295, 1412, 2793, 13, 509, 434, 1382, 281, 733, 295, 3847, 257, 18161, 2533, 420, 746, 281, 733, 295, 11, 291, 50604, 50604, 458, 11, 3318, 257, 7605, 420, 11, 291, 458, 11, 360, 24590, 13, 583, 291, 458, 300, 291, 362, 257, 1326, 484, 23646, 13, 407, 11, 50900, 50900, 291, 362, 257, 1326, 2793, 300, 366, 11, 291, 458, 11, 588, 1400, 1314, 490, 689, 436, 820, 312, 445, 570, 11, 51064, 51064, 291, 458, 11, 264, 1185, 575, 5658, 420, 746, 11, 420, 264, 1412, 390, 11087, 365, 512, 5658, 13, 407, 11, 51332, 51332, 291, 528, 264, 1185, 281, 312, 13956, 281, 300, 5658, 13, 509, 500, 380, 528, 264, 2063, 2445, 281, 3488, 886, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.13019906616210938, "compression_ratio": 1.9, "no_speech_prob": 1.4736579032614827e-05}, {"id": 318, "seek": 206240, "start": 2067.2000000000003, "end": 2073.12, "text": " know, fit a curve or, you know, do regression. But you know that you have a few outliers. So,", "tokens": [50364, 257, 3840, 295, 1412, 2793, 13, 509, 434, 1382, 281, 733, 295, 3847, 257, 18161, 2533, 420, 746, 281, 733, 295, 11, 291, 50604, 50604, 458, 11, 3318, 257, 7605, 420, 11, 291, 458, 11, 360, 24590, 13, 583, 291, 458, 300, 291, 362, 257, 1326, 484, 23646, 13, 407, 11, 50900, 50900, 291, 362, 257, 1326, 2793, 300, 366, 11, 291, 458, 11, 588, 1400, 1314, 490, 689, 436, 820, 312, 445, 570, 11, 51064, 51064, 291, 458, 11, 264, 1185, 575, 5658, 420, 746, 11, 420, 264, 1412, 390, 11087, 365, 512, 5658, 13, 407, 11, 51332, 51332, 291, 528, 264, 1185, 281, 312, 13956, 281, 300, 5658, 13, 509, 500, 380, 528, 264, 2063, 2445, 281, 3488, 886, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.13019906616210938, "compression_ratio": 1.9, "no_speech_prob": 1.4736579032614827e-05}, {"id": 319, "seek": 206240, "start": 2073.12, "end": 2076.4, "text": " you have a few points that are, you know, very far away from where they should be just because,", "tokens": [50364, 257, 3840, 295, 1412, 2793, 13, 509, 434, 1382, 281, 733, 295, 3847, 257, 18161, 2533, 420, 746, 281, 733, 295, 11, 291, 50604, 50604, 458, 11, 3318, 257, 7605, 420, 11, 291, 458, 11, 360, 24590, 13, 583, 291, 458, 300, 291, 362, 257, 1326, 484, 23646, 13, 407, 11, 50900, 50900, 291, 362, 257, 1326, 2793, 300, 366, 11, 291, 458, 11, 588, 1400, 1314, 490, 689, 436, 820, 312, 445, 570, 11, 51064, 51064, 291, 458, 11, 264, 1185, 575, 5658, 420, 746, 11, 420, 264, 1412, 390, 11087, 365, 512, 5658, 13, 407, 11, 51332, 51332, 291, 528, 264, 1185, 281, 312, 13956, 281, 300, 5658, 13, 509, 500, 380, 528, 264, 2063, 2445, 281, 3488, 886, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.13019906616210938, "compression_ratio": 1.9, "no_speech_prob": 1.4736579032614827e-05}, {"id": 320, "seek": 206240, "start": 2076.4, "end": 2081.76, "text": " you know, the system has noise or something, or the data was collected with some noise. So,", "tokens": [50364, 257, 3840, 295, 1412, 2793, 13, 509, 434, 1382, 281, 733, 295, 3847, 257, 18161, 2533, 420, 746, 281, 733, 295, 11, 291, 50604, 50604, 458, 11, 3318, 257, 7605, 420, 11, 291, 458, 11, 360, 24590, 13, 583, 291, 458, 300, 291, 362, 257, 1326, 484, 23646, 13, 407, 11, 50900, 50900, 291, 362, 257, 1326, 2793, 300, 366, 11, 291, 458, 11, 588, 1400, 1314, 490, 689, 436, 820, 312, 445, 570, 11, 51064, 51064, 291, 458, 11, 264, 1185, 575, 5658, 420, 746, 11, 420, 264, 1412, 390, 11087, 365, 512, 5658, 13, 407, 11, 51332, 51332, 291, 528, 264, 1185, 281, 312, 13956, 281, 300, 5658, 13, 509, 500, 380, 528, 264, 2063, 2445, 281, 3488, 886, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.13019906616210938, "compression_ratio": 1.9, "no_speech_prob": 1.4736579032614827e-05}, {"id": 321, "seek": 206240, "start": 2081.76, "end": 2085.84, "text": " you want the system to be robust to that noise. You don't want the cost function to increase too", "tokens": [50364, 257, 3840, 295, 1412, 2793, 13, 509, 434, 1382, 281, 733, 295, 3847, 257, 18161, 2533, 420, 746, 281, 733, 295, 11, 291, 50604, 50604, 458, 11, 3318, 257, 7605, 420, 11, 291, 458, 11, 360, 24590, 13, 583, 291, 458, 300, 291, 362, 257, 1326, 484, 23646, 13, 407, 11, 50900, 50900, 291, 362, 257, 1326, 2793, 300, 366, 11, 291, 458, 11, 588, 1400, 1314, 490, 689, 436, 820, 312, 445, 570, 11, 51064, 51064, 291, 458, 11, 264, 1185, 575, 5658, 420, 746, 11, 420, 264, 1412, 390, 11087, 365, 512, 5658, 13, 407, 11, 51332, 51332, 291, 528, 264, 1185, 281, 312, 13956, 281, 300, 5658, 13, 509, 500, 380, 528, 264, 2063, 2445, 281, 3488, 886, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.13019906616210938, "compression_ratio": 1.9, "no_speech_prob": 1.4736579032614827e-05}, {"id": 322, "seek": 208584, "start": 2085.84, "end": 2096.08, "text": " quickly as the points are far away from, you know, the kind of the general curve. So, L1 loss would", "tokens": [50364, 2661, 382, 264, 2793, 366, 1400, 1314, 490, 11, 291, 458, 11, 264, 733, 295, 264, 2674, 7605, 13, 407, 11, 441, 16, 4470, 576, 50876, 50876, 312, 544, 13956, 13, 823, 11, 264, 1154, 365, 441, 16, 4470, 307, 300, 309, 311, 406, 819, 9364, 412, 264, 2767, 13, 400, 370, 11, 51176, 51176, 291, 458, 11, 291, 362, 281, 733, 295, 312, 5026, 562, 291, 483, 281, 264, 2767, 295, 577, 291, 360, 264, 16235, 13, 51512, 51576], "temperature": 0.0, "avg_logprob": -0.1630245418083377, "compression_ratio": 1.566137566137566, "no_speech_prob": 3.78495656150335e-06}, {"id": 323, "seek": 208584, "start": 2096.08, "end": 2102.08, "text": " be more robust. Now, the problem with L1 loss is that it's not differentiable at the bottom. And so,", "tokens": [50364, 2661, 382, 264, 2793, 366, 1400, 1314, 490, 11, 291, 458, 11, 264, 733, 295, 264, 2674, 7605, 13, 407, 11, 441, 16, 4470, 576, 50876, 50876, 312, 544, 13956, 13, 823, 11, 264, 1154, 365, 441, 16, 4470, 307, 300, 309, 311, 406, 819, 9364, 412, 264, 2767, 13, 400, 370, 11, 51176, 51176, 291, 458, 11, 291, 362, 281, 733, 295, 312, 5026, 562, 291, 483, 281, 264, 2767, 295, 577, 291, 360, 264, 16235, 13, 51512, 51576], "temperature": 0.0, "avg_logprob": -0.1630245418083377, "compression_ratio": 1.566137566137566, "no_speech_prob": 3.78495656150335e-06}, {"id": 324, "seek": 208584, "start": 2102.08, "end": 2108.8, "text": " you know, you have to kind of be careful when you get to the bottom of how you do the gradient.", "tokens": [50364, 2661, 382, 264, 2793, 366, 1400, 1314, 490, 11, 291, 458, 11, 264, 733, 295, 264, 2674, 7605, 13, 407, 11, 441, 16, 4470, 576, 50876, 50876, 312, 544, 13956, 13, 823, 11, 264, 1154, 365, 441, 16, 4470, 307, 300, 309, 311, 406, 819, 9364, 412, 264, 2767, 13, 400, 370, 11, 51176, 51176, 291, 458, 11, 291, 362, 281, 733, 295, 312, 5026, 562, 291, 483, 281, 264, 2767, 295, 577, 291, 360, 264, 16235, 13, 51512, 51576], "temperature": 0.0, "avg_logprob": -0.1630245418083377, "compression_ratio": 1.566137566137566, "no_speech_prob": 3.78495656150335e-06}, {"id": 325, "seek": 210880, "start": 2108.8, "end": 2116.1600000000003, "text": " That's basically done with a soft shrink, essentially. That's the gradient of the L1 loss.", "tokens": [50364, 663, 311, 1936, 1096, 365, 257, 2787, 23060, 11, 4476, 13, 663, 311, 264, 16235, 295, 264, 441, 16, 4470, 13, 50732, 50936, 823, 11, 281, 3006, 337, 300, 11, 561, 362, 808, 493, 365, 51104, 51204, 3683, 2098, 295, 733, 295, 1455, 264, 441, 16, 4470, 13956, 337, 2416, 15352, 11, 457, 550, 920, 5508, 412, 264, 51520, 51520, 2767, 11, 733, 295, 35263, 411, 257, 8889, 6713, 13, 407, 11, 291, 458, 11, 291, 362, 281, 312, 5026, 562, 291, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.3483460226724314, "compression_ratio": 1.5687203791469195, "no_speech_prob": 1.9637222976598423e-06}, {"id": 326, "seek": 210880, "start": 2120.2400000000002, "end": 2123.6000000000004, "text": " Now, to correct for that, people have come up with", "tokens": [50364, 663, 311, 1936, 1096, 365, 257, 2787, 23060, 11, 4476, 13, 663, 311, 264, 16235, 295, 264, 441, 16, 4470, 13, 50732, 50936, 823, 11, 281, 3006, 337, 300, 11, 561, 362, 808, 493, 365, 51104, 51204, 3683, 2098, 295, 733, 295, 1455, 264, 441, 16, 4470, 13956, 337, 2416, 15352, 11, 457, 550, 920, 5508, 412, 264, 51520, 51520, 2767, 11, 733, 295, 35263, 411, 257, 8889, 6713, 13, 407, 11, 291, 458, 11, 291, 362, 281, 312, 5026, 562, 291, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.3483460226724314, "compression_ratio": 1.5687203791469195, "no_speech_prob": 1.9637222976598423e-06}, {"id": 327, "seek": 210880, "start": 2125.6000000000004, "end": 2131.92, "text": " various ways of kind of making the L1 loss robust for large losses, but then still smooth at the", "tokens": [50364, 663, 311, 1936, 1096, 365, 257, 2787, 23060, 11, 4476, 13, 663, 311, 264, 16235, 295, 264, 441, 16, 4470, 13, 50732, 50936, 823, 11, 281, 3006, 337, 300, 11, 561, 362, 808, 493, 365, 51104, 51204, 3683, 2098, 295, 733, 295, 1455, 264, 441, 16, 4470, 13956, 337, 2416, 15352, 11, 457, 550, 920, 5508, 412, 264, 51520, 51520, 2767, 11, 733, 295, 35263, 411, 257, 8889, 6713, 13, 407, 11, 291, 458, 11, 291, 362, 281, 312, 5026, 562, 291, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.3483460226724314, "compression_ratio": 1.5687203791469195, "no_speech_prob": 1.9637222976598423e-06}, {"id": 328, "seek": 210880, "start": 2131.92, "end": 2138.5600000000004, "text": " bottom, kind of behaving like a squared error. So, you know, you have to be careful when you", "tokens": [50364, 663, 311, 1936, 1096, 365, 257, 2787, 23060, 11, 4476, 13, 663, 311, 264, 16235, 295, 264, 441, 16, 4470, 13, 50732, 50936, 823, 11, 281, 3006, 337, 300, 11, 561, 362, 808, 493, 365, 51104, 51204, 3683, 2098, 295, 733, 295, 1455, 264, 441, 16, 4470, 13956, 337, 2416, 15352, 11, 457, 550, 920, 5508, 412, 264, 51520, 51520, 2767, 11, 733, 295, 35263, 411, 257, 8889, 6713, 13, 407, 11, 291, 458, 11, 291, 362, 281, 312, 5026, 562, 291, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.3483460226724314, "compression_ratio": 1.5687203791469195, "no_speech_prob": 1.9637222976598423e-06}, {"id": 329, "seek": 213856, "start": 2138.56, "end": 2145.7599999999998, "text": " are at the bottom. So, an example of this is this particular function, smooth L1 loss. It's basically", "tokens": [50364, 366, 412, 264, 2767, 13, 407, 11, 364, 1365, 295, 341, 307, 341, 1729, 2445, 11, 5508, 441, 16, 4470, 13, 467, 311, 1936, 50724, 50724, 441, 16, 1400, 1314, 11, 293, 309, 311, 1333, 295, 441, 17, 11184, 13, 400, 300, 13533, 11, 2171, 300, 311, 1219, 257, 389, 10261, 4470, 13, 51008, 51068, 2188, 561, 818, 341, 611, 17115, 3209, 570, 322, 341, 1331, 3035, 490, 264, 13626, 82, 420, 13384, 82, 11, 51532, 51532, 300, 733, 295, 10348, 341, 733, 295, 10024, 2445, 337, 257, 819, 4334, 13, 51728, 51832], "temperature": 0.0, "avg_logprob": -0.1591678820158306, "compression_ratio": 1.5308641975308641, "no_speech_prob": 1.3419035894912668e-05}, {"id": 330, "seek": 213856, "start": 2145.7599999999998, "end": 2151.44, "text": " L1 far away, and it's sort of L2 nearby. And that presents, sometimes that's called a Huber loss.", "tokens": [50364, 366, 412, 264, 2767, 13, 407, 11, 364, 1365, 295, 341, 307, 341, 1729, 2445, 11, 5508, 441, 16, 4470, 13, 467, 311, 1936, 50724, 50724, 441, 16, 1400, 1314, 11, 293, 309, 311, 1333, 295, 441, 17, 11184, 13, 400, 300, 13533, 11, 2171, 300, 311, 1219, 257, 389, 10261, 4470, 13, 51008, 51068, 2188, 561, 818, 341, 611, 17115, 3209, 570, 322, 341, 1331, 3035, 490, 264, 13626, 82, 420, 13384, 82, 11, 51532, 51532, 300, 733, 295, 10348, 341, 733, 295, 10024, 2445, 337, 257, 819, 4334, 13, 51728, 51832], "temperature": 0.0, "avg_logprob": -0.1591678820158306, "compression_ratio": 1.5308641975308641, "no_speech_prob": 1.3419035894912668e-05}, {"id": 331, "seek": 213856, "start": 2152.64, "end": 2161.92, "text": " Some people call this also elastic network because on this old paper from the 1980s or 1990s,", "tokens": [50364, 366, 412, 264, 2767, 13, 407, 11, 364, 1365, 295, 341, 307, 341, 1729, 2445, 11, 5508, 441, 16, 4470, 13, 467, 311, 1936, 50724, 50724, 441, 16, 1400, 1314, 11, 293, 309, 311, 1333, 295, 441, 17, 11184, 13, 400, 300, 13533, 11, 2171, 300, 311, 1219, 257, 389, 10261, 4470, 13, 51008, 51068, 2188, 561, 818, 341, 611, 17115, 3209, 570, 322, 341, 1331, 3035, 490, 264, 13626, 82, 420, 13384, 82, 11, 51532, 51532, 300, 733, 295, 10348, 341, 733, 295, 10024, 2445, 337, 257, 819, 4334, 13, 51728, 51832], "temperature": 0.0, "avg_logprob": -0.1591678820158306, "compression_ratio": 1.5308641975308641, "no_speech_prob": 1.3419035894912668e-05}, {"id": 332, "seek": 213856, "start": 2161.92, "end": 2165.84, "text": " that kind of proposed this kind of objective function for a different purpose.", "tokens": [50364, 366, 412, 264, 2767, 13, 407, 11, 364, 1365, 295, 341, 307, 341, 1729, 2445, 11, 5508, 441, 16, 4470, 13, 467, 311, 1936, 50724, 50724, 441, 16, 1400, 1314, 11, 293, 309, 311, 1333, 295, 441, 17, 11184, 13, 400, 300, 13533, 11, 2171, 300, 311, 1219, 257, 389, 10261, 4470, 13, 51008, 51068, 2188, 561, 818, 341, 611, 17115, 3209, 570, 322, 341, 1331, 3035, 490, 264, 13626, 82, 420, 13384, 82, 11, 51532, 51532, 300, 733, 295, 10348, 341, 733, 295, 10024, 2445, 337, 257, 819, 4334, 13, 51728, 51832], "temperature": 0.0, "avg_logprob": -0.1591678820158306, "compression_ratio": 1.5308641975308641, "no_speech_prob": 1.3419035894912668e-05}, {"id": 333, "seek": 216584, "start": 2165.84, "end": 2171.1200000000003, "text": " So, that's useful. That was advertised by Was Gorsuch in the Fatsop CNN paper for,", "tokens": [50364, 407, 11, 300, 311, 4420, 13, 663, 390, 42310, 538, 3027, 460, 830, 625, 294, 264, 479, 1720, 404, 24859, 3035, 337, 11, 50628, 50768, 293, 309, 311, 1143, 1596, 257, 857, 294, 3820, 5201, 337, 3683, 9932, 13, 3764, 11, 50948, 50948, 309, 311, 337, 12316, 1970, 484, 23646, 13, 51068, 51228, 2743, 11, 460, 13427, 307, 44670, 13, 460, 13427, 307, 611, 44670, 11, 420, 370, 309, 311, 406, 562, 321, 360, 411, 3256, 17630, 13, 51536, 51676], "temperature": 0.0, "avg_logprob": -0.4880360393989377, "compression_ratio": 1.469387755102041, "no_speech_prob": 1.4969479707360733e-05}, {"id": 334, "seek": 216584, "start": 2173.92, "end": 2177.52, "text": " and it's used quite a bit in computer vision for various purposes. Again,", "tokens": [50364, 407, 11, 300, 311, 4420, 13, 663, 390, 42310, 538, 3027, 460, 830, 625, 294, 264, 479, 1720, 404, 24859, 3035, 337, 11, 50628, 50768, 293, 309, 311, 1143, 1596, 257, 857, 294, 3820, 5201, 337, 3683, 9932, 13, 3764, 11, 50948, 50948, 309, 311, 337, 12316, 1970, 484, 23646, 13, 51068, 51228, 2743, 11, 460, 13427, 307, 44670, 13, 460, 13427, 307, 611, 44670, 11, 420, 370, 309, 311, 406, 562, 321, 360, 411, 3256, 17630, 13, 51536, 51676], "temperature": 0.0, "avg_logprob": -0.4880360393989377, "compression_ratio": 1.469387755102041, "no_speech_prob": 1.4969479707360733e-05}, {"id": 335, "seek": 216584, "start": 2177.52, "end": 2179.92, "text": " it's for protecting against outliers.", "tokens": [50364, 407, 11, 300, 311, 4420, 13, 663, 390, 42310, 538, 3027, 460, 830, 625, 294, 264, 479, 1720, 404, 24859, 3035, 337, 11, 50628, 50768, 293, 309, 311, 1143, 1596, 257, 857, 294, 3820, 5201, 337, 3683, 9932, 13, 3764, 11, 50948, 50948, 309, 311, 337, 12316, 1970, 484, 23646, 13, 51068, 51228, 2743, 11, 460, 13427, 307, 44670, 13, 460, 13427, 307, 611, 44670, 11, 420, 370, 309, 311, 406, 562, 321, 360, 411, 3256, 17630, 13, 51536, 51676], "temperature": 0.0, "avg_logprob": -0.4880360393989377, "compression_ratio": 1.469387755102041, "no_speech_prob": 1.4969479707360733e-05}, {"id": 336, "seek": 216584, "start": 2183.1200000000003, "end": 2189.28, "text": " Also, Giza is sharper. Giza is also sharper, or so it's not when we do like image prediction.", "tokens": [50364, 407, 11, 300, 311, 4420, 13, 663, 390, 42310, 538, 3027, 460, 830, 625, 294, 264, 479, 1720, 404, 24859, 3035, 337, 11, 50628, 50768, 293, 309, 311, 1143, 1596, 257, 857, 294, 3820, 5201, 337, 3683, 9932, 13, 3764, 11, 50948, 50948, 309, 311, 337, 12316, 1970, 484, 23646, 13, 51068, 51228, 2743, 11, 460, 13427, 307, 44670, 13, 460, 13427, 307, 611, 44670, 11, 420, 370, 309, 311, 406, 562, 321, 360, 411, 3256, 17630, 13, 51536, 51676], "temperature": 0.0, "avg_logprob": -0.4880360393989377, "compression_ratio": 1.469387755102041, "no_speech_prob": 1.4969479707360733e-05}, {"id": 337, "seek": 218928, "start": 2189.28, "end": 2193.28, "text": " Sharper than using the MSC?", "tokens": [50364, 22030, 610, 813, 1228, 264, 7395, 34, 30, 50564, 50716, 1726, 4098, 13, 286, 914, 11, 309, 311, 445, 411, 264, 7395, 34, 337, 1359, 13603, 13, 1033, 13, 407, 11, 51040, 51040, 300, 1177, 380, 652, 604, 2649, 11, 457, 309, 1177, 380, 11, 420, 1310, 286, 33870, 437, 428, 935, 390, 13, 51396, 51396, 4919, 11, 286, 390, 1382, 281, 6794, 264, 441, 16, 5717, 264, 441, 17, 13, 440, 441, 16, 307, 257, 707, 857, 544, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.4570073047316218, "compression_ratio": 1.433673469387755, "no_speech_prob": 6.438207947212504e-06}, {"id": 338, "seek": 218928, "start": 2196.32, "end": 2202.8, "text": " Not particularly. I mean, it's just like the MSC for small errors. Okay. So,", "tokens": [50364, 22030, 610, 813, 1228, 264, 7395, 34, 30, 50564, 50716, 1726, 4098, 13, 286, 914, 11, 309, 311, 445, 411, 264, 7395, 34, 337, 1359, 13603, 13, 1033, 13, 407, 11, 51040, 51040, 300, 1177, 380, 652, 604, 2649, 11, 457, 309, 1177, 380, 11, 420, 1310, 286, 33870, 437, 428, 935, 390, 13, 51396, 51396, 4919, 11, 286, 390, 1382, 281, 6794, 264, 441, 16, 5717, 264, 441, 17, 13, 440, 441, 16, 307, 257, 707, 857, 544, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.4570073047316218, "compression_ratio": 1.433673469387755, "no_speech_prob": 6.438207947212504e-06}, {"id": 339, "seek": 218928, "start": 2202.8, "end": 2209.92, "text": " that doesn't make any difference, but it doesn't, or maybe I misunderstood what your point was.", "tokens": [50364, 22030, 610, 813, 1228, 264, 7395, 34, 30, 50564, 50716, 1726, 4098, 13, 286, 914, 11, 309, 311, 445, 411, 264, 7395, 34, 337, 1359, 13603, 13, 1033, 13, 407, 11, 51040, 51040, 300, 1177, 380, 652, 604, 2649, 11, 457, 309, 1177, 380, 11, 420, 1310, 286, 33870, 437, 428, 935, 390, 13, 51396, 51396, 4919, 11, 286, 390, 1382, 281, 6794, 264, 441, 16, 5717, 264, 441, 17, 13, 440, 441, 16, 307, 257, 707, 857, 544, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.4570073047316218, "compression_ratio": 1.433673469387755, "no_speech_prob": 6.438207947212504e-06}, {"id": 340, "seek": 218928, "start": 2209.92, "end": 2214.2400000000002, "text": " Sorry, I was trying to compare the L1 versus the L2. The L1 is a little bit more", "tokens": [50364, 22030, 610, 813, 1228, 264, 7395, 34, 30, 50564, 50716, 1726, 4098, 13, 286, 914, 11, 309, 311, 445, 411, 264, 7395, 34, 337, 1359, 13603, 13, 1033, 13, 407, 11, 51040, 51040, 300, 1177, 380, 652, 604, 2649, 11, 457, 309, 1177, 380, 11, 420, 1310, 286, 33870, 437, 428, 935, 390, 13, 51396, 51396, 4919, 11, 286, 390, 1382, 281, 6794, 264, 441, 16, 5717, 264, 441, 17, 13, 440, 441, 16, 307, 257, 707, 857, 544, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.4570073047316218, "compression_ratio": 1.433673469387755, "no_speech_prob": 6.438207947212504e-06}, {"id": 341, "seek": 221424, "start": 2214.24, "end": 2219.52, "text": " blurry predictions whenever we try to do a prediction by using the L2, minimizing the L2,", "tokens": [50364, 37644, 21264, 5699, 321, 853, 281, 360, 257, 17630, 538, 1228, 264, 441, 17, 11, 46608, 264, 441, 17, 11, 50628, 50628, 9735, 561, 366, 46608, 264, 441, 16, 294, 1668, 281, 362, 44670, 4787, 21264, 13, 50936, 50936, 1033, 13, 407, 11, 498, 291, 747, 257, 3840, 295, 2793, 11, 498, 291, 747, 257, 3840, 295, 398, 4190, 11, 293, 291, 1029, 264, 1168, 11, 51312, 51364, 437, 2158, 775, 264, 441, 16, 976, 281, 264, 441, 17, 30, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.4350192206246512, "compression_ratio": 1.7087912087912087, "no_speech_prob": 1.8340329916100018e-05}, {"id": 342, "seek": 221424, "start": 2219.52, "end": 2225.68, "text": " whereas people are minimizing the L1 in order to have sharper overall predictions.", "tokens": [50364, 37644, 21264, 5699, 321, 853, 281, 360, 257, 17630, 538, 1228, 264, 441, 17, 11, 46608, 264, 441, 17, 11, 50628, 50628, 9735, 561, 366, 46608, 264, 441, 16, 294, 1668, 281, 362, 44670, 4787, 21264, 13, 50936, 50936, 1033, 13, 407, 11, 498, 291, 747, 257, 3840, 295, 2793, 11, 498, 291, 747, 257, 3840, 295, 398, 4190, 11, 293, 291, 1029, 264, 1168, 11, 51312, 51364, 437, 2158, 775, 264, 441, 16, 976, 281, 264, 441, 17, 30, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.4350192206246512, "compression_ratio": 1.7087912087912087, "no_speech_prob": 1.8340329916100018e-05}, {"id": 343, "seek": 221424, "start": 2225.68, "end": 2233.2, "text": " Okay. So, if you take a bunch of points, if you take a bunch of Y values, and you ask the question,", "tokens": [50364, 37644, 21264, 5699, 321, 853, 281, 360, 257, 17630, 538, 1228, 264, 441, 17, 11, 46608, 264, 441, 17, 11, 50628, 50628, 9735, 561, 366, 46608, 264, 441, 16, 294, 1668, 281, 362, 44670, 4787, 21264, 13, 50936, 50936, 1033, 13, 407, 11, 498, 291, 747, 257, 3840, 295, 2793, 11, 498, 291, 747, 257, 3840, 295, 398, 4190, 11, 293, 291, 1029, 264, 1168, 11, 51312, 51364, 437, 2158, 775, 264, 441, 16, 976, 281, 264, 441, 17, 30, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.4350192206246512, "compression_ratio": 1.7087912087912087, "no_speech_prob": 1.8340329916100018e-05}, {"id": 344, "seek": 221424, "start": 2234.24, "end": 2237.4399999999996, "text": " what value does the L1 give to the L2?", "tokens": [50364, 37644, 21264, 5699, 321, 853, 281, 360, 257, 17630, 538, 1228, 264, 441, 17, 11, 46608, 264, 441, 17, 11, 50628, 50628, 9735, 561, 366, 46608, 264, 441, 16, 294, 1668, 281, 362, 44670, 4787, 21264, 13, 50936, 50936, 1033, 13, 407, 11, 498, 291, 747, 257, 3840, 295, 2793, 11, 498, 291, 747, 257, 3840, 295, 398, 4190, 11, 293, 291, 1029, 264, 1168, 11, 51312, 51364, 437, 2158, 775, 264, 441, 16, 976, 281, 264, 441, 17, 30, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.4350192206246512, "compression_ratio": 1.7087912087912087, "no_speech_prob": 1.8340329916100018e-05}, {"id": 345, "seek": 223744, "start": 2237.44, "end": 2244.08, "text": " If you take a bunch of points, if you take a bunch of Y values, and you ask the question, what value,", "tokens": [50364, 759, 291, 747, 257, 3840, 295, 2793, 11, 498, 291, 747, 257, 3840, 295, 398, 4190, 11, 293, 291, 1029, 264, 1168, 11, 437, 2158, 11, 50696, 50748, 370, 291, 747, 257, 3840, 295, 2793, 322, 398, 11, 293, 291, 1029, 264, 1168, 11, 437, 2158, 295, 398, 4464, 5660, 51024, 51024, 264, 3732, 4470, 30, 440, 1867, 307, 264, 4274, 295, 439, 264, 398, 82, 13, 1033, 13, 51224, 51312, 1033, 13, 407, 11, 498, 337, 257, 2167, 1783, 11, 291, 362, 257, 1379, 3840, 295, 398, 82, 11, 597, 1355, 291, 362, 5658, 294, 428, 1412, 11, 51568, 51688], "temperature": 0.0, "avg_logprob": -0.13095489057522375, "compression_ratio": 1.9081081081081082, "no_speech_prob": 8.139441888488363e-06}, {"id": 346, "seek": 223744, "start": 2245.12, "end": 2250.64, "text": " so you take a bunch of points on Y, and you ask the question, what value of Y minimizes", "tokens": [50364, 759, 291, 747, 257, 3840, 295, 2793, 11, 498, 291, 747, 257, 3840, 295, 398, 4190, 11, 293, 291, 1029, 264, 1168, 11, 437, 2158, 11, 50696, 50748, 370, 291, 747, 257, 3840, 295, 2793, 322, 398, 11, 293, 291, 1029, 264, 1168, 11, 437, 2158, 295, 398, 4464, 5660, 51024, 51024, 264, 3732, 4470, 30, 440, 1867, 307, 264, 4274, 295, 439, 264, 398, 82, 13, 1033, 13, 51224, 51312, 1033, 13, 407, 11, 498, 337, 257, 2167, 1783, 11, 291, 362, 257, 1379, 3840, 295, 398, 82, 11, 597, 1355, 291, 362, 5658, 294, 428, 1412, 11, 51568, 51688], "temperature": 0.0, "avg_logprob": -0.13095489057522375, "compression_ratio": 1.9081081081081082, "no_speech_prob": 8.139441888488363e-06}, {"id": 347, "seek": 223744, "start": 2250.64, "end": 2254.64, "text": " the square loss? The answer is the average of all the Ys. Okay.", "tokens": [50364, 759, 291, 747, 257, 3840, 295, 2793, 11, 498, 291, 747, 257, 3840, 295, 398, 4190, 11, 293, 291, 1029, 264, 1168, 11, 437, 2158, 11, 50696, 50748, 370, 291, 747, 257, 3840, 295, 2793, 322, 398, 11, 293, 291, 1029, 264, 1168, 11, 437, 2158, 295, 398, 4464, 5660, 51024, 51024, 264, 3732, 4470, 30, 440, 1867, 307, 264, 4274, 295, 439, 264, 398, 82, 13, 1033, 13, 51224, 51312, 1033, 13, 407, 11, 498, 337, 257, 2167, 1783, 11, 291, 362, 257, 1379, 3840, 295, 398, 82, 11, 597, 1355, 291, 362, 5658, 294, 428, 1412, 11, 51568, 51688], "temperature": 0.0, "avg_logprob": -0.13095489057522375, "compression_ratio": 1.9081081081081082, "no_speech_prob": 8.139441888488363e-06}, {"id": 348, "seek": 223744, "start": 2256.4, "end": 2261.52, "text": " Okay. So, if for a single X, you have a whole bunch of Ys, which means you have noise in your data,", "tokens": [50364, 759, 291, 747, 257, 3840, 295, 2793, 11, 498, 291, 747, 257, 3840, 295, 398, 4190, 11, 293, 291, 1029, 264, 1168, 11, 437, 2158, 11, 50696, 50748, 370, 291, 747, 257, 3840, 295, 2793, 322, 398, 11, 293, 291, 1029, 264, 1168, 11, 437, 2158, 295, 398, 4464, 5660, 51024, 51024, 264, 3732, 4470, 30, 440, 1867, 307, 264, 4274, 295, 439, 264, 398, 82, 13, 1033, 13, 51224, 51312, 1033, 13, 407, 11, 498, 337, 257, 2167, 1783, 11, 291, 362, 257, 1379, 3840, 295, 398, 82, 11, 597, 1355, 291, 362, 5658, 294, 428, 1412, 11, 51568, 51688], "temperature": 0.0, "avg_logprob": -0.13095489057522375, "compression_ratio": 1.9081081081081082, "no_speech_prob": 8.139441888488363e-06}, {"id": 349, "seek": 226152, "start": 2261.52, "end": 2265.92, "text": " your system will want to produce the average of all the Ys that you're observing.", "tokens": [50364, 428, 1185, 486, 528, 281, 5258, 264, 4274, 295, 439, 264, 398, 82, 300, 291, 434, 22107, 13, 50584, 50656, 1033, 13, 400, 498, 264, 398, 291, 434, 22107, 307, 406, 257, 2167, 2158, 11, 457, 307, 11, 286, 500, 380, 458, 11, 364, 3256, 11, 50928, 50980, 264, 4274, 295, 257, 3840, 295, 5267, 307, 257, 37644, 3256, 13, 1033, 13, 663, 311, 983, 291, 483, 729, 37644, 5065, 13, 51216, 51252, 823, 11, 365, 441, 16, 11, 264, 2158, 295, 398, 300, 4464, 5660, 264, 441, 16, 2026, 11, 264, 441, 16, 4560, 11, 370, 1936, 264, 2408, 295, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.16225013732910157, "compression_ratio": 1.64, "no_speech_prob": 1.9333340333105298e-06}, {"id": 350, "seek": 226152, "start": 2267.36, "end": 2272.8, "text": " Okay. And if the Y you're observing is not a single value, but is, I don't know, an image,", "tokens": [50364, 428, 1185, 486, 528, 281, 5258, 264, 4274, 295, 439, 264, 398, 82, 300, 291, 434, 22107, 13, 50584, 50656, 1033, 13, 400, 498, 264, 398, 291, 434, 22107, 307, 406, 257, 2167, 2158, 11, 457, 307, 11, 286, 500, 380, 458, 11, 364, 3256, 11, 50928, 50980, 264, 4274, 295, 257, 3840, 295, 5267, 307, 257, 37644, 3256, 13, 1033, 13, 663, 311, 983, 291, 483, 729, 37644, 5065, 13, 51216, 51252, 823, 11, 365, 441, 16, 11, 264, 2158, 295, 398, 300, 4464, 5660, 264, 441, 16, 2026, 11, 264, 441, 16, 4560, 11, 370, 1936, 264, 2408, 295, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.16225013732910157, "compression_ratio": 1.64, "no_speech_prob": 1.9333340333105298e-06}, {"id": 351, "seek": 226152, "start": 2273.84, "end": 2278.56, "text": " the average of a bunch of images is a blurry image. Okay. That's why you get those blurry effects.", "tokens": [50364, 428, 1185, 486, 528, 281, 5258, 264, 4274, 295, 439, 264, 398, 82, 300, 291, 434, 22107, 13, 50584, 50656, 1033, 13, 400, 498, 264, 398, 291, 434, 22107, 307, 406, 257, 2167, 2158, 11, 457, 307, 11, 286, 500, 380, 458, 11, 364, 3256, 11, 50928, 50980, 264, 4274, 295, 257, 3840, 295, 5267, 307, 257, 37644, 3256, 13, 1033, 13, 663, 311, 983, 291, 483, 729, 37644, 5065, 13, 51216, 51252, 823, 11, 365, 441, 16, 11, 264, 2158, 295, 398, 300, 4464, 5660, 264, 441, 16, 2026, 11, 264, 441, 16, 4560, 11, 370, 1936, 264, 2408, 295, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.16225013732910157, "compression_ratio": 1.64, "no_speech_prob": 1.9333340333105298e-06}, {"id": 352, "seek": 226152, "start": 2279.28, "end": 2288.64, "text": " Now, with L1, the value of Y that minimizes the L1 norm, the L1 distance, so basically the sum of", "tokens": [50364, 428, 1185, 486, 528, 281, 5258, 264, 4274, 295, 439, 264, 398, 82, 300, 291, 434, 22107, 13, 50584, 50656, 1033, 13, 400, 498, 264, 398, 291, 434, 22107, 307, 406, 257, 2167, 2158, 11, 457, 307, 11, 286, 500, 380, 458, 11, 364, 3256, 11, 50928, 50980, 264, 4274, 295, 257, 3840, 295, 5267, 307, 257, 37644, 3256, 13, 1033, 13, 663, 311, 983, 291, 483, 729, 37644, 5065, 13, 51216, 51252, 823, 11, 365, 441, 16, 11, 264, 2158, 295, 398, 300, 4464, 5660, 264, 441, 16, 2026, 11, 264, 441, 16, 4560, 11, 370, 1936, 264, 2408, 295, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.16225013732910157, "compression_ratio": 1.64, "no_speech_prob": 1.9333340333105298e-06}, {"id": 353, "seek": 228864, "start": 2288.64, "end": 2292.8799999999997, "text": " the absolute values of the differences between the value you're considering and all the points,", "tokens": [50364, 264, 8236, 4190, 295, 264, 7300, 1296, 264, 2158, 291, 434, 8079, 293, 439, 264, 2793, 11, 50576, 50576, 439, 264, 398, 2793, 11, 300, 311, 264, 26779, 13, 1033, 13, 407, 11, 309, 311, 257, 2212, 935, 13, 1057, 558, 13, 51004, 51004, 4019, 12, 18710, 13, 286, 536, 13, 51080, 51080, 440, 26779, 11, 295, 1164, 11, 307, 406, 37644, 13, 440, 26779, 295, 264, 3256, 307, 406, 37644, 13, 467, 311, 445, 364, 3256, 13, 51452, 51452, 5780, 309, 311, 733, 295, 2252, 281, 6964, 294, 3866, 12819, 13, 583, 51600, 51760], "temperature": 0.0, "avg_logprob": -0.2476743127881866, "compression_ratio": 1.6210045662100456, "no_speech_prob": 3.2885968721529935e-06}, {"id": 354, "seek": 228864, "start": 2292.8799999999997, "end": 2301.44, "text": " all the Y points, that's the median. Okay. So, it's a given point. All right.", "tokens": [50364, 264, 8236, 4190, 295, 264, 7300, 1296, 264, 2158, 291, 434, 8079, 293, 439, 264, 2793, 11, 50576, 50576, 439, 264, 398, 2793, 11, 300, 311, 264, 26779, 13, 1033, 13, 407, 11, 309, 311, 257, 2212, 935, 13, 1057, 558, 13, 51004, 51004, 4019, 12, 18710, 13, 286, 536, 13, 51080, 51080, 440, 26779, 11, 295, 1164, 11, 307, 406, 37644, 13, 440, 26779, 295, 264, 3256, 307, 406, 37644, 13, 467, 311, 445, 364, 3256, 13, 51452, 51452, 5780, 309, 311, 733, 295, 2252, 281, 6964, 294, 3866, 12819, 13, 583, 51600, 51760], "temperature": 0.0, "avg_logprob": -0.2476743127881866, "compression_ratio": 1.6210045662100456, "no_speech_prob": 3.2885968721529935e-06}, {"id": 355, "seek": 228864, "start": 2301.44, "end": 2302.96, "text": " Uh-huh. I see.", "tokens": [50364, 264, 8236, 4190, 295, 264, 7300, 1296, 264, 2158, 291, 434, 8079, 293, 439, 264, 2793, 11, 50576, 50576, 439, 264, 398, 2793, 11, 300, 311, 264, 26779, 13, 1033, 13, 407, 11, 309, 311, 257, 2212, 935, 13, 1057, 558, 13, 51004, 51004, 4019, 12, 18710, 13, 286, 536, 13, 51080, 51080, 440, 26779, 11, 295, 1164, 11, 307, 406, 37644, 13, 440, 26779, 295, 264, 3256, 307, 406, 37644, 13, 467, 311, 445, 364, 3256, 13, 51452, 51452, 5780, 309, 311, 733, 295, 2252, 281, 6964, 294, 3866, 12819, 13, 583, 51600, 51760], "temperature": 0.0, "avg_logprob": -0.2476743127881866, "compression_ratio": 1.6210045662100456, "no_speech_prob": 3.2885968721529935e-06}, {"id": 356, "seek": 228864, "start": 2302.96, "end": 2310.4, "text": " The median, of course, is not blurry. The median of the image is not blurry. It's just an image.", "tokens": [50364, 264, 8236, 4190, 295, 264, 7300, 1296, 264, 2158, 291, 434, 8079, 293, 439, 264, 2793, 11, 50576, 50576, 439, 264, 398, 2793, 11, 300, 311, 264, 26779, 13, 1033, 13, 407, 11, 309, 311, 257, 2212, 935, 13, 1057, 558, 13, 51004, 51004, 4019, 12, 18710, 13, 286, 536, 13, 51080, 51080, 440, 26779, 11, 295, 1164, 11, 307, 406, 37644, 13, 440, 26779, 295, 264, 3256, 307, 406, 37644, 13, 467, 311, 445, 364, 3256, 13, 51452, 51452, 5780, 309, 311, 733, 295, 2252, 281, 6964, 294, 3866, 12819, 13, 583, 51600, 51760], "temperature": 0.0, "avg_logprob": -0.2476743127881866, "compression_ratio": 1.6210045662100456, "no_speech_prob": 3.2885968721529935e-06}, {"id": 357, "seek": 228864, "start": 2310.4, "end": 2313.3599999999997, "text": " Although it's kind of difficult to define in multiple dimensions. But", "tokens": [50364, 264, 8236, 4190, 295, 264, 7300, 1296, 264, 2158, 291, 434, 8079, 293, 439, 264, 2793, 11, 50576, 50576, 439, 264, 398, 2793, 11, 300, 311, 264, 26779, 13, 1033, 13, 407, 11, 309, 311, 257, 2212, 935, 13, 1057, 558, 13, 51004, 51004, 4019, 12, 18710, 13, 286, 536, 13, 51080, 51080, 440, 26779, 11, 295, 1164, 11, 307, 406, 37644, 13, 440, 26779, 295, 264, 3256, 307, 406, 37644, 13, 467, 311, 445, 364, 3256, 13, 51452, 51452, 5780, 309, 311, 733, 295, 2252, 281, 6964, 294, 3866, 12819, 13, 583, 51600, 51760], "temperature": 0.0, "avg_logprob": -0.2476743127881866, "compression_ratio": 1.6210045662100456, "no_speech_prob": 3.2885968721529935e-06}, {"id": 358, "seek": 231336, "start": 2313.36, "end": 2318.8, "text": " so one problem with this loss is that it has a scale, right? So here, the transition here is", "tokens": [50364, 370, 472, 1154, 365, 341, 4470, 307, 300, 309, 575, 257, 4373, 11, 558, 30, 407, 510, 11, 264, 6034, 510, 307, 50636, 50636, 412, 1958, 13, 20, 11, 457, 983, 820, 309, 312, 412, 1958, 13, 20, 30, 509, 458, 11, 309, 727, 312, 11, 309, 5946, 437, 264, 4373, 295, 428, 13603, 50976, 51012, 366, 13, 1033, 13, 43230, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 639, 307, 534, 406, 264, 3671, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 51280, 51280, 286, 478, 406, 988, 983, 309, 311, 1219, 341, 636, 13, 467, 311, 406, 264, 3671, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 467, 311, 264, 51508, 51508], "temperature": 0.0, "avg_logprob": -0.37714751427914917, "compression_ratio": 1.7638888888888888, "no_speech_prob": 5.4220854508457705e-06}, {"id": 359, "seek": 231336, "start": 2318.8, "end": 2325.6, "text": " at 0.5, but why should it be at 0.5? You know, it could be, it depends what the scale of your errors", "tokens": [50364, 370, 472, 1154, 365, 341, 4470, 307, 300, 309, 575, 257, 4373, 11, 558, 30, 407, 510, 11, 264, 6034, 510, 307, 50636, 50636, 412, 1958, 13, 20, 11, 457, 983, 820, 309, 312, 412, 1958, 13, 20, 30, 509, 458, 11, 309, 727, 312, 11, 309, 5946, 437, 264, 4373, 295, 428, 13603, 50976, 51012, 366, 13, 1033, 13, 43230, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 639, 307, 534, 406, 264, 3671, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 51280, 51280, 286, 478, 406, 988, 983, 309, 311, 1219, 341, 636, 13, 467, 311, 406, 264, 3671, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 467, 311, 264, 51508, 51508], "temperature": 0.0, "avg_logprob": -0.37714751427914917, "compression_ratio": 1.7638888888888888, "no_speech_prob": 5.4220854508457705e-06}, {"id": 360, "seek": 231336, "start": 2326.32, "end": 2331.6800000000003, "text": " are. Okay. Negative log-like-a-good loss. This is really not the negative log-like-a-good loss.", "tokens": [50364, 370, 472, 1154, 365, 341, 4470, 307, 300, 309, 575, 257, 4373, 11, 558, 30, 407, 510, 11, 264, 6034, 510, 307, 50636, 50636, 412, 1958, 13, 20, 11, 457, 983, 820, 309, 312, 412, 1958, 13, 20, 30, 509, 458, 11, 309, 727, 312, 11, 309, 5946, 437, 264, 4373, 295, 428, 13603, 50976, 51012, 366, 13, 1033, 13, 43230, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 639, 307, 534, 406, 264, 3671, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 51280, 51280, 286, 478, 406, 988, 983, 309, 311, 1219, 341, 636, 13, 467, 311, 406, 264, 3671, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 467, 311, 264, 51508, 51508], "temperature": 0.0, "avg_logprob": -0.37714751427914917, "compression_ratio": 1.7638888888888888, "no_speech_prob": 5.4220854508457705e-06}, {"id": 361, "seek": 231336, "start": 2331.6800000000003, "end": 2336.2400000000002, "text": " I'm not sure why it's called this way. It's not the negative log-like-a-good loss. It's the", "tokens": [50364, 370, 472, 1154, 365, 341, 4470, 307, 300, 309, 575, 257, 4373, 11, 558, 30, 407, 510, 11, 264, 6034, 510, 307, 50636, 50636, 412, 1958, 13, 20, 11, 457, 983, 820, 309, 312, 412, 1958, 13, 20, 30, 509, 458, 11, 309, 727, 312, 11, 309, 5946, 437, 264, 4373, 295, 428, 13603, 50976, 51012, 366, 13, 1033, 13, 43230, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 639, 307, 534, 406, 264, 3671, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 51280, 51280, 286, 478, 406, 988, 983, 309, 311, 1219, 341, 636, 13, 467, 311, 406, 264, 3671, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 467, 311, 264, 51508, 51508], "temperature": 0.0, "avg_logprob": -0.37714751427914917, "compression_ratio": 1.7638888888888888, "no_speech_prob": 5.4220854508457705e-06}, {"id": 362, "seek": 233624, "start": 2336.24, "end": 2343.52, "text": " negative log-like-a-good loss. Okay. Negative log-like-a-good loss. This is really not the", "tokens": [50364, 3671, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 1033, 13, 43230, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 639, 307, 534, 406, 264, 50728, 50728, 3671, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 286, 478, 406, 988, 983, 309, 311, 1219, 341, 636, 294, 9953, 51, 284, 339, 13, 583, 1936, 11, 50956, 51020, 510, 11, 3811, 300, 291, 362, 364, 1783, 8062, 1348, 484, 13, 1033, 13, 400, 428, 4470, 2445, 307, 456, 307, 472, 51336, 51336, 3006, 1783, 13, 1033, 13, 407, 3811, 1184, 1783, 6805, 281, 257, 6175, 337, 11, 718, 311, 584, 11, 4825, 12, 11665, 21538, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.21050119400024414, "compression_ratio": 1.7219730941704037, "no_speech_prob": 4.222767984174425e-06}, {"id": 363, "seek": 233624, "start": 2343.52, "end": 2348.08, "text": " negative log-like-a-good loss. I'm not sure why it's called this way in PyTorch. But basically,", "tokens": [50364, 3671, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 1033, 13, 43230, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 639, 307, 534, 406, 264, 50728, 50728, 3671, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 286, 478, 406, 988, 983, 309, 311, 1219, 341, 636, 294, 9953, 51, 284, 339, 13, 583, 1936, 11, 50956, 51020, 510, 11, 3811, 300, 291, 362, 364, 1783, 8062, 1348, 484, 13, 1033, 13, 400, 428, 4470, 2445, 307, 456, 307, 472, 51336, 51336, 3006, 1783, 13, 1033, 13, 407, 3811, 1184, 1783, 6805, 281, 257, 6175, 337, 11, 718, 311, 584, 11, 4825, 12, 11665, 21538, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.21050119400024414, "compression_ratio": 1.7219730941704037, "no_speech_prob": 4.222767984174425e-06}, {"id": 364, "seek": 233624, "start": 2349.3599999999997, "end": 2355.68, "text": " here, imagine that you have an X vector coming out. Okay. And your loss function is there is one", "tokens": [50364, 3671, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 1033, 13, 43230, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 639, 307, 534, 406, 264, 50728, 50728, 3671, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 286, 478, 406, 988, 983, 309, 311, 1219, 341, 636, 294, 9953, 51, 284, 339, 13, 583, 1936, 11, 50956, 51020, 510, 11, 3811, 300, 291, 362, 364, 1783, 8062, 1348, 484, 13, 1033, 13, 400, 428, 4470, 2445, 307, 456, 307, 472, 51336, 51336, 3006, 1783, 13, 1033, 13, 407, 3811, 1184, 1783, 6805, 281, 257, 6175, 337, 11, 718, 311, 584, 11, 4825, 12, 11665, 21538, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.21050119400024414, "compression_ratio": 1.7219730941704037, "no_speech_prob": 4.222767984174425e-06}, {"id": 365, "seek": 233624, "start": 2355.68, "end": 2363.04, "text": " correct X. Okay. So imagine each X correspond to a score for, let's say, multi-class classification.", "tokens": [50364, 3671, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 1033, 13, 43230, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 639, 307, 534, 406, 264, 50728, 50728, 3671, 3565, 12, 4092, 12, 64, 12, 21753, 4470, 13, 286, 478, 406, 988, 983, 309, 311, 1219, 341, 636, 294, 9953, 51, 284, 339, 13, 583, 1936, 11, 50956, 51020, 510, 11, 3811, 300, 291, 362, 364, 1783, 8062, 1348, 484, 13, 1033, 13, 400, 428, 4470, 2445, 307, 456, 307, 472, 51336, 51336, 3006, 1783, 13, 1033, 13, 407, 3811, 1184, 1783, 6805, 281, 257, 6175, 337, 11, 718, 311, 584, 11, 4825, 12, 11665, 21538, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.21050119400024414, "compression_ratio": 1.7219730941704037, "no_speech_prob": 4.222767984174425e-06}, {"id": 366, "seek": 236304, "start": 2363.04, "end": 2368.32, "text": " One particular index in that vector. Okay. And what you want is you want to make that score as", "tokens": [50364, 1485, 1729, 8186, 294, 300, 8062, 13, 1033, 13, 400, 437, 291, 528, 307, 291, 528, 281, 652, 300, 6175, 382, 50628, 50628, 2416, 382, 1944, 13, 1033, 13, 759, 729, 13444, 366, 22119, 82, 11, 550, 341, 307, 7285, 3671, 3565, 51012, 51012, 22119, 13, 759, 729, 13444, 366, 3565, 22119, 82, 11, 550, 341, 307, 6674, 22119, 420, 7285, 3671, 51324, 51324, 3565, 22119, 13, 1033, 13, 583, 456, 307, 1825, 294, 341, 10088, 300, 767, 1608, 11221, 300, 264, 441, 311, 362, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.08681972910848897, "compression_ratio": 1.9253731343283582, "no_speech_prob": 1.0451357411511708e-05}, {"id": 367, "seek": 236304, "start": 2368.32, "end": 2376.0, "text": " large as possible. Okay. If those scores are likelihoods, then this is minimum negative log", "tokens": [50364, 1485, 1729, 8186, 294, 300, 8062, 13, 1033, 13, 400, 437, 291, 528, 307, 291, 528, 281, 652, 300, 6175, 382, 50628, 50628, 2416, 382, 1944, 13, 1033, 13, 759, 729, 13444, 366, 22119, 82, 11, 550, 341, 307, 7285, 3671, 3565, 51012, 51012, 22119, 13, 759, 729, 13444, 366, 3565, 22119, 82, 11, 550, 341, 307, 6674, 22119, 420, 7285, 3671, 51324, 51324, 3565, 22119, 13, 1033, 13, 583, 456, 307, 1825, 294, 341, 10088, 300, 767, 1608, 11221, 300, 264, 441, 311, 362, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.08681972910848897, "compression_ratio": 1.9253731343283582, "no_speech_prob": 1.0451357411511708e-05}, {"id": 368, "seek": 236304, "start": 2376.0, "end": 2382.24, "text": " likelihood. If those scores are log likelihoods, then this is maximum likelihood or minimum negative", "tokens": [50364, 1485, 1729, 8186, 294, 300, 8062, 13, 1033, 13, 400, 437, 291, 528, 307, 291, 528, 281, 652, 300, 6175, 382, 50628, 50628, 2416, 382, 1944, 13, 1033, 13, 759, 729, 13444, 366, 22119, 82, 11, 550, 341, 307, 7285, 3671, 3565, 51012, 51012, 22119, 13, 759, 729, 13444, 366, 3565, 22119, 82, 11, 550, 341, 307, 6674, 22119, 420, 7285, 3671, 51324, 51324, 3565, 22119, 13, 1033, 13, 583, 456, 307, 1825, 294, 341, 10088, 300, 767, 1608, 11221, 300, 264, 441, 311, 362, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.08681972910848897, "compression_ratio": 1.9253731343283582, "no_speech_prob": 1.0451357411511708e-05}, {"id": 369, "seek": 236304, "start": 2382.24, "end": 2387.7599999999998, "text": " log likelihood. Okay. But there is nothing in this module that actually specifies that the L's have", "tokens": [50364, 1485, 1729, 8186, 294, 300, 8062, 13, 1033, 13, 400, 437, 291, 528, 307, 291, 528, 281, 652, 300, 6175, 382, 50628, 50628, 2416, 382, 1944, 13, 1033, 13, 759, 729, 13444, 366, 22119, 82, 11, 550, 341, 307, 7285, 3671, 3565, 51012, 51012, 22119, 13, 759, 729, 13444, 366, 3565, 22119, 82, 11, 550, 341, 307, 6674, 22119, 420, 7285, 3671, 51324, 51324, 3565, 22119, 13, 1033, 13, 583, 456, 307, 1825, 294, 341, 10088, 300, 767, 1608, 11221, 300, 264, 441, 311, 362, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.08681972910848897, "compression_ratio": 1.9253731343283582, "no_speech_prob": 1.0451357411511708e-05}, {"id": 370, "seek": 238776, "start": 2387.76, "end": 2393.36, "text": " to be log likelihoods. So this is just, you know, make my desired component as large as possible.", "tokens": [50364, 281, 312, 3565, 22119, 82, 13, 407, 341, 307, 445, 11, 291, 458, 11, 652, 452, 14721, 6542, 382, 2416, 382, 1944, 13, 50644, 50644, 663, 311, 309, 13, 759, 291, 829, 3671, 7880, 294, 1868, 11, 370, 586, 291, 393, 7302, 264, 1783, 311, 382, 25737, 382, 51116, 51116, 8851, 281, 13444, 13, 1033, 13, 814, 434, 406, 3353, 13444, 13, 814, 434, 411, 35389, 11, 498, 291, 528, 13, 51404, 51544, 583, 309, 311, 264, 912, 13, 407, 264, 8513, 510, 1619, 11, 291, 458, 11, 445, 1888, 264, 1783, 300, 2314, 281, 312, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.09005365371704102, "compression_ratio": 1.5872340425531914, "no_speech_prob": 3.5007674341613892e-06}, {"id": 371, "seek": 238776, "start": 2393.36, "end": 2402.8, "text": " That's it. If you put negative signs in front, so now you can interpret the X's as energies as", "tokens": [50364, 281, 312, 3565, 22119, 82, 13, 407, 341, 307, 445, 11, 291, 458, 11, 652, 452, 14721, 6542, 382, 2416, 382, 1944, 13, 50644, 50644, 663, 311, 309, 13, 759, 291, 829, 3671, 7880, 294, 1868, 11, 370, 586, 291, 393, 7302, 264, 1783, 311, 382, 25737, 382, 51116, 51116, 8851, 281, 13444, 13, 1033, 13, 814, 434, 406, 3353, 13444, 13, 814, 434, 411, 35389, 11, 498, 291, 528, 13, 51404, 51544, 583, 309, 311, 264, 912, 13, 407, 264, 8513, 510, 1619, 11, 291, 458, 11, 445, 1888, 264, 1783, 300, 2314, 281, 312, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.09005365371704102, "compression_ratio": 1.5872340425531914, "no_speech_prob": 3.5007674341613892e-06}, {"id": 372, "seek": 238776, "start": 2402.8, "end": 2408.5600000000004, "text": " opposed to scores. Okay. They're not positive scores. They're like penalties, if you want.", "tokens": [50364, 281, 312, 3565, 22119, 82, 13, 407, 341, 307, 445, 11, 291, 458, 11, 652, 452, 14721, 6542, 382, 2416, 382, 1944, 13, 50644, 50644, 663, 311, 309, 13, 759, 291, 829, 3671, 7880, 294, 1868, 11, 370, 586, 291, 393, 7302, 264, 1783, 311, 382, 25737, 382, 51116, 51116, 8851, 281, 13444, 13, 1033, 13, 814, 434, 406, 3353, 13444, 13, 814, 434, 411, 35389, 11, 498, 291, 528, 13, 51404, 51544, 583, 309, 311, 264, 912, 13, 407, 264, 8513, 510, 1619, 11, 291, 458, 11, 445, 1888, 264, 1783, 300, 2314, 281, 312, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.09005365371704102, "compression_ratio": 1.5872340425531914, "no_speech_prob": 3.5007674341613892e-06}, {"id": 373, "seek": 238776, "start": 2411.36, "end": 2417.5200000000004, "text": " But it's the same. So the formula here says, you know, just pick the X that happens to be", "tokens": [50364, 281, 312, 3565, 22119, 82, 13, 407, 341, 307, 445, 11, 291, 458, 11, 652, 452, 14721, 6542, 382, 2416, 382, 1944, 13, 50644, 50644, 663, 311, 309, 13, 759, 291, 829, 3671, 7880, 294, 1868, 11, 370, 586, 291, 393, 7302, 264, 1783, 311, 382, 25737, 382, 51116, 51116, 8851, 281, 13444, 13, 1033, 13, 814, 434, 406, 3353, 13444, 13, 814, 434, 411, 35389, 11, 498, 291, 528, 13, 51404, 51544, 583, 309, 311, 264, 912, 13, 407, 264, 8513, 510, 1619, 11, 291, 458, 11, 445, 1888, 264, 1783, 300, 2314, 281, 312, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.09005365371704102, "compression_ratio": 1.5872340425531914, "no_speech_prob": 3.5007674341613892e-06}, {"id": 374, "seek": 241752, "start": 2417.52, "end": 2424.08, "text": " the correct one for one sample in the batch and make that score as large as possible.", "tokens": [50364, 264, 3006, 472, 337, 472, 6889, 294, 264, 15245, 293, 652, 300, 6175, 382, 2416, 382, 1944, 13, 50692, 50740, 823, 11, 341, 1729, 472, 4045, 291, 281, 976, 257, 819, 3364, 281, 819, 10479, 11, 51108, 51160, 597, 307, 729, 343, 311, 13, 467, 311, 257, 3364, 8062, 300, 2709, 257, 3364, 281, 1184, 295, 264, 51416, 51476, 10479, 13, 467, 311, 4420, 294, 257, 688, 295, 3331, 11, 4098, 498, 291, 362, 13371, 51732, 51812], "temperature": 0.0, "avg_logprob": -0.11194454869137535, "compression_ratio": 1.653061224489796, "no_speech_prob": 3.7265883747750195e-06}, {"id": 375, "seek": 241752, "start": 2425.04, "end": 2432.4, "text": " Now, this particular one allows you to give a different weight to different categories,", "tokens": [50364, 264, 3006, 472, 337, 472, 6889, 294, 264, 15245, 293, 652, 300, 6175, 382, 2416, 382, 1944, 13, 50692, 50740, 823, 11, 341, 1729, 472, 4045, 291, 281, 976, 257, 819, 3364, 281, 819, 10479, 11, 51108, 51160, 597, 307, 729, 343, 311, 13, 467, 311, 257, 3364, 8062, 300, 2709, 257, 3364, 281, 1184, 295, 264, 51416, 51476, 10479, 13, 467, 311, 4420, 294, 257, 688, 295, 3331, 11, 4098, 498, 291, 362, 13371, 51732, 51812], "temperature": 0.0, "avg_logprob": -0.11194454869137535, "compression_ratio": 1.653061224489796, "no_speech_prob": 3.7265883747750195e-06}, {"id": 376, "seek": 241752, "start": 2433.44, "end": 2438.56, "text": " which is those W's. It's a weight vector that gives a weight to each of the", "tokens": [50364, 264, 3006, 472, 337, 472, 6889, 294, 264, 15245, 293, 652, 300, 6175, 382, 2416, 382, 1944, 13, 50692, 50740, 823, 11, 341, 1729, 472, 4045, 291, 281, 976, 257, 819, 3364, 281, 819, 10479, 11, 51108, 51160, 597, 307, 729, 343, 311, 13, 467, 311, 257, 3364, 8062, 300, 2709, 257, 3364, 281, 1184, 295, 264, 51416, 51476, 10479, 13, 467, 311, 4420, 294, 257, 688, 295, 3331, 11, 4098, 498, 291, 362, 13371, 51732, 51812], "temperature": 0.0, "avg_logprob": -0.11194454869137535, "compression_ratio": 1.653061224489796, "no_speech_prob": 3.7265883747750195e-06}, {"id": 377, "seek": 241752, "start": 2439.7599999999998, "end": 2444.88, "text": " categories. It's useful in a lot of cases, particularly if you have widely", "tokens": [50364, 264, 3006, 472, 337, 472, 6889, 294, 264, 15245, 293, 652, 300, 6175, 382, 2416, 382, 1944, 13, 50692, 50740, 823, 11, 341, 1729, 472, 4045, 291, 281, 976, 257, 819, 3364, 281, 819, 10479, 11, 51108, 51160, 597, 307, 729, 343, 311, 13, 467, 311, 257, 3364, 8062, 300, 2709, 257, 3364, 281, 1184, 295, 264, 51416, 51476, 10479, 13, 467, 311, 4420, 294, 257, 688, 295, 3331, 11, 4098, 498, 291, 362, 13371, 51732, 51812], "temperature": 0.0, "avg_logprob": -0.11194454869137535, "compression_ratio": 1.653061224489796, "no_speech_prob": 3.7265883747750195e-06}, {"id": 378, "seek": 244488, "start": 2444.88, "end": 2451.44, "text": " different frequencies for the categories. You might want to increase the weight of", "tokens": [50364, 819, 20250, 337, 264, 10479, 13, 509, 1062, 528, 281, 3488, 264, 3364, 295, 50692, 50732, 10938, 337, 597, 291, 362, 257, 1359, 1230, 295, 5110, 13, 286, 914, 11, 337, 10479, 337, 597, 291, 51100, 51100, 362, 257, 1359, 1230, 295, 10938, 13, 2908, 11, 286, 478, 767, 406, 257, 955, 3429, 295, 341, 13, 286, 519, 309, 311, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1756071001291275, "compression_ratio": 1.6815286624203822, "no_speech_prob": 6.643164851993788e-06}, {"id": 379, "seek": 244488, "start": 2452.2400000000002, "end": 2459.6, "text": " samples for which you have a small number of examples. I mean, for categories for which you", "tokens": [50364, 819, 20250, 337, 264, 10479, 13, 509, 1062, 528, 281, 3488, 264, 3364, 295, 50692, 50732, 10938, 337, 597, 291, 362, 257, 1359, 1230, 295, 5110, 13, 286, 914, 11, 337, 10479, 337, 597, 291, 51100, 51100, 362, 257, 1359, 1230, 295, 10938, 13, 2908, 11, 286, 478, 767, 406, 257, 955, 3429, 295, 341, 13, 286, 519, 309, 311, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1756071001291275, "compression_ratio": 1.6815286624203822, "no_speech_prob": 6.643164851993788e-06}, {"id": 380, "seek": 244488, "start": 2459.6, "end": 2466.88, "text": " have a small number of samples. However, I'm actually not a big fan of this. I think it's", "tokens": [50364, 819, 20250, 337, 264, 10479, 13, 509, 1062, 528, 281, 3488, 264, 3364, 295, 50692, 50732, 10938, 337, 597, 291, 362, 257, 1359, 1230, 295, 5110, 13, 286, 914, 11, 337, 10479, 337, 597, 291, 51100, 51100, 362, 257, 1359, 1230, 295, 10938, 13, 2908, 11, 286, 478, 767, 406, 257, 955, 3429, 295, 341, 13, 286, 519, 309, 311, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1756071001291275, "compression_ratio": 1.6815286624203822, "no_speech_prob": 6.643164851993788e-06}, {"id": 381, "seek": 246688, "start": 2466.88, "end": 2477.6, "text": " a much better idea to just increase the frequency of the samples from the class that appears rarely", "tokens": [50364, 257, 709, 1101, 1558, 281, 445, 3488, 264, 7893, 295, 264, 10938, 490, 264, 1508, 300, 7038, 13752, 50900, 50900, 370, 300, 291, 2681, 1125, 264, 20250, 295, 264, 5359, 562, 291, 3847, 13, 51108, 51228, 467, 311, 709, 1101, 570, 309, 12382, 1208, 342, 8997, 2750, 16235, 294, 257, 1101, 636, 13, 51460, 51460], "temperature": 0.0, "avg_logprob": -0.25545329378362286, "compression_ratio": 1.5855263157894737, "no_speech_prob": 1.1842384083138313e-05}, {"id": 382, "seek": 246688, "start": 2477.6, "end": 2481.76, "text": " so that you equalize the frequencies of the classes when you train.", "tokens": [50364, 257, 709, 1101, 1558, 281, 445, 3488, 264, 7893, 295, 264, 10938, 490, 264, 1508, 300, 7038, 13752, 50900, 50900, 370, 300, 291, 2681, 1125, 264, 20250, 295, 264, 5359, 562, 291, 3847, 13, 51108, 51228, 467, 311, 709, 1101, 570, 309, 12382, 1208, 342, 8997, 2750, 16235, 294, 257, 1101, 636, 13, 51460, 51460], "temperature": 0.0, "avg_logprob": -0.25545329378362286, "compression_ratio": 1.5855263157894737, "no_speech_prob": 1.1842384083138313e-05}, {"id": 383, "seek": 246688, "start": 2484.1600000000003, "end": 2488.8, "text": " It's much better because it exploits stochastic gradient in a better way.", "tokens": [50364, 257, 709, 1101, 1558, 281, 445, 3488, 264, 7893, 295, 264, 10938, 490, 264, 1508, 300, 7038, 13752, 50900, 50900, 370, 300, 291, 2681, 1125, 264, 20250, 295, 264, 5359, 562, 291, 3847, 13, 51108, 51228, 467, 311, 709, 1101, 570, 309, 12382, 1208, 342, 8997, 2750, 16235, 294, 257, 1101, 636, 13, 51460, 51460], "temperature": 0.0, "avg_logprob": -0.25545329378362286, "compression_ratio": 1.5855263157894737, "no_speech_prob": 1.1842384083138313e-05}, {"id": 384, "seek": 248880, "start": 2488.8, "end": 2497.92, "text": " The bottom line of that is, let me actually draw a picture of this. Let's say you have a problem", "tokens": [50364, 440, 2767, 1622, 295, 300, 307, 11, 718, 385, 767, 2642, 257, 3036, 295, 341, 13, 961, 311, 584, 291, 362, 257, 1154, 50820, 50820, 689, 291, 362, 9131, 295, 10938, 337, 7719, 472, 293, 550, 257, 1359, 1230, 295, 10938, 337, 7719, 51088, 51088, 732, 293, 257, 5870, 1230, 295, 10938, 337, 7719, 1045, 13, 961, 311, 584, 510, 291, 362, 11, 286, 500, 380, 458, 11, 51520, 51520], "temperature": 0.0, "avg_logprob": -0.1847569677564833, "compression_ratio": 1.8089171974522293, "no_speech_prob": 2.601587993922294e-06}, {"id": 385, "seek": 248880, "start": 2497.92, "end": 2503.28, "text": " where you have tons of samples for category one and then a small number of samples for category", "tokens": [50364, 440, 2767, 1622, 295, 300, 307, 11, 718, 385, 767, 2642, 257, 3036, 295, 341, 13, 961, 311, 584, 291, 362, 257, 1154, 50820, 50820, 689, 291, 362, 9131, 295, 10938, 337, 7719, 472, 293, 550, 257, 1359, 1230, 295, 10938, 337, 7719, 51088, 51088, 732, 293, 257, 5870, 1230, 295, 10938, 337, 7719, 1045, 13, 961, 311, 584, 510, 291, 362, 11, 286, 500, 380, 458, 11, 51520, 51520], "temperature": 0.0, "avg_logprob": -0.1847569677564833, "compression_ratio": 1.8089171974522293, "no_speech_prob": 2.601587993922294e-06}, {"id": 386, "seek": 248880, "start": 2503.28, "end": 2511.92, "text": " two and a tiny number of samples for category three. Let's say here you have, I don't know,", "tokens": [50364, 440, 2767, 1622, 295, 300, 307, 11, 718, 385, 767, 2642, 257, 3036, 295, 341, 13, 961, 311, 584, 291, 362, 257, 1154, 50820, 50820, 689, 291, 362, 9131, 295, 10938, 337, 7719, 472, 293, 550, 257, 1359, 1230, 295, 10938, 337, 7719, 51088, 51088, 732, 293, 257, 5870, 1230, 295, 10938, 337, 7719, 1045, 13, 961, 311, 584, 510, 291, 362, 11, 286, 500, 380, 458, 11, 51520, 51520], "temperature": 0.0, "avg_logprob": -0.1847569677564833, "compression_ratio": 1.8089171974522293, "no_speech_prob": 2.601587993922294e-06}, {"id": 387, "seek": 251192, "start": 2511.92, "end": 2521.04, "text": " a thousand samples and here you have 500 samples and here you have, I don't know, 200 samples.", "tokens": [50364, 257, 4714, 10938, 293, 510, 291, 362, 5923, 10938, 293, 510, 291, 362, 11, 286, 500, 380, 458, 11, 2331, 10938, 13, 50820, 50884, 708, 291, 727, 360, 307, 11, 1228, 341, 733, 295, 3364, 2445, 11, 291, 727, 976, 341, 257, 3364, 295, 51176, 51288, 472, 293, 341, 2146, 257, 3364, 295, 732, 293, 341, 2146, 257, 3364, 295, 1732, 13, 1396, 291, 393, 2681, 1125, 264, 17443, 51532, 51532, 498, 291, 528, 13, 467, 311, 1391, 1101, 281, 652, 988, 300, 264, 17443, 2710, 1125, 281, 472, 13, 663, 576, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.12726983428001404, "compression_ratio": 1.8472906403940887, "no_speech_prob": 4.6377158469113056e-06}, {"id": 388, "seek": 251192, "start": 2522.32, "end": 2528.16, "text": " What you could do is, using this kind of weight function, you could give this a weight of", "tokens": [50364, 257, 4714, 10938, 293, 510, 291, 362, 5923, 10938, 293, 510, 291, 362, 11, 286, 500, 380, 458, 11, 2331, 10938, 13, 50820, 50884, 708, 291, 727, 360, 307, 11, 1228, 341, 733, 295, 3364, 2445, 11, 291, 727, 976, 341, 257, 3364, 295, 51176, 51288, 472, 293, 341, 2146, 257, 3364, 295, 732, 293, 341, 2146, 257, 3364, 295, 1732, 13, 1396, 291, 393, 2681, 1125, 264, 17443, 51532, 51532, 498, 291, 528, 13, 467, 311, 1391, 1101, 281, 652, 988, 300, 264, 17443, 2710, 1125, 281, 472, 13, 663, 576, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.12726983428001404, "compression_ratio": 1.8472906403940887, "no_speech_prob": 4.6377158469113056e-06}, {"id": 389, "seek": 251192, "start": 2530.4, "end": 2535.28, "text": " one and this guy a weight of two and this guy a weight of five. Then you can equalize the weights", "tokens": [50364, 257, 4714, 10938, 293, 510, 291, 362, 5923, 10938, 293, 510, 291, 362, 11, 286, 500, 380, 458, 11, 2331, 10938, 13, 50820, 50884, 708, 291, 727, 360, 307, 11, 1228, 341, 733, 295, 3364, 2445, 11, 291, 727, 976, 341, 257, 3364, 295, 51176, 51288, 472, 293, 341, 2146, 257, 3364, 295, 732, 293, 341, 2146, 257, 3364, 295, 1732, 13, 1396, 291, 393, 2681, 1125, 264, 17443, 51532, 51532, 498, 291, 528, 13, 467, 311, 1391, 1101, 281, 652, 988, 300, 264, 17443, 2710, 1125, 281, 472, 13, 663, 576, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.12726983428001404, "compression_ratio": 1.8472906403940887, "no_speech_prob": 4.6377158469113056e-06}, {"id": 390, "seek": 251192, "start": 2535.28, "end": 2539.12, "text": " if you want. It's probably better to make sure that the weights normalize to one. That would", "tokens": [50364, 257, 4714, 10938, 293, 510, 291, 362, 5923, 10938, 293, 510, 291, 362, 11, 286, 500, 380, 458, 11, 2331, 10938, 13, 50820, 50884, 708, 291, 727, 360, 307, 11, 1228, 341, 733, 295, 3364, 2445, 11, 291, 727, 976, 341, 257, 3364, 295, 51176, 51288, 472, 293, 341, 2146, 257, 3364, 295, 732, 293, 341, 2146, 257, 3364, 295, 1732, 13, 1396, 291, 393, 2681, 1125, 264, 17443, 51532, 51532, 498, 291, 528, 13, 467, 311, 1391, 1101, 281, 652, 988, 300, 264, 17443, 2710, 1125, 281, 472, 13, 663, 576, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.12726983428001404, "compression_ratio": 1.8472906403940887, "no_speech_prob": 4.6377158469113056e-06}, {"id": 391, "seek": 253912, "start": 2539.12, "end": 2547.6, "text": " be probably a better idea. What I recommend is not that. What I recommend is when you pick your samples,", "tokens": [50364, 312, 1391, 257, 1101, 1558, 13, 708, 286, 2748, 307, 406, 300, 13, 708, 286, 2748, 307, 562, 291, 1888, 428, 10938, 11, 50788, 51056, 291, 1936, 1888, 472, 6889, 490, 1508, 472, 293, 550, 472, 6889, 490, 1508, 732, 293, 51272, 51272, 6889, 490, 1508, 1045, 13, 1396, 291, 1066, 884, 341, 1830, 428, 3097, 5481, 13, 51536, 51588], "temperature": 0.0, "avg_logprob": -0.17098997485253117, "compression_ratio": 1.7986577181208054, "no_speech_prob": 6.681406148345559e-07}, {"id": 392, "seek": 253912, "start": 2552.96, "end": 2557.2799999999997, "text": " you basically pick one sample from class one and then one sample from class two and", "tokens": [50364, 312, 1391, 257, 1101, 1558, 13, 708, 286, 2748, 307, 406, 300, 13, 708, 286, 2748, 307, 562, 291, 1888, 428, 10938, 11, 50788, 51056, 291, 1936, 1888, 472, 6889, 490, 1508, 472, 293, 550, 472, 6889, 490, 1508, 732, 293, 51272, 51272, 6889, 490, 1508, 1045, 13, 1396, 291, 1066, 884, 341, 1830, 428, 3097, 5481, 13, 51536, 51588], "temperature": 0.0, "avg_logprob": -0.17098997485253117, "compression_ratio": 1.7986577181208054, "no_speech_prob": 6.681406148345559e-07}, {"id": 393, "seek": 253912, "start": 2557.2799999999997, "end": 2562.56, "text": " sample from class three. Then you keep doing this during your training session.", "tokens": [50364, 312, 1391, 257, 1101, 1558, 13, 708, 286, 2748, 307, 406, 300, 13, 708, 286, 2748, 307, 562, 291, 1888, 428, 10938, 11, 50788, 51056, 291, 1936, 1888, 472, 6889, 490, 1508, 472, 293, 550, 472, 6889, 490, 1508, 732, 293, 51272, 51272, 6889, 490, 1508, 1045, 13, 1396, 291, 1066, 884, 341, 1830, 428, 3097, 5481, 13, 51536, 51588], "temperature": 0.0, "avg_logprob": -0.17098997485253117, "compression_ratio": 1.7986577181208054, "no_speech_prob": 6.681406148345559e-07}, {"id": 394, "seek": 256256, "start": 2562.56, "end": 2569.36, "text": " When you get to the end of class three, you go back to the beginning. You keep going here,", "tokens": [50364, 1133, 291, 483, 281, 264, 917, 295, 1508, 1045, 11, 291, 352, 646, 281, 264, 2863, 13, 509, 1066, 516, 510, 11, 50704, 50704, 457, 510, 291, 352, 646, 281, 264, 700, 6889, 13, 5527, 516, 510, 11, 352, 646, 293, 586, 291, 362, 264, 1150, 6889, 13, 50980, 51048, 823, 291, 483, 281, 264, 917, 295, 1508, 732, 11, 352, 646, 281, 264, 722, 13, 440, 958, 6889, 307, 516, 281, 312, 510, 11, 51472, 51520], "temperature": 0.0, "avg_logprob": -0.20793634728540347, "compression_ratio": 1.9266666666666667, "no_speech_prob": 3.187514948876924e-06}, {"id": 395, "seek": 256256, "start": 2569.36, "end": 2574.88, "text": " but here you go back to the first sample. Keep going here, go back and now you have the second sample.", "tokens": [50364, 1133, 291, 483, 281, 264, 917, 295, 1508, 1045, 11, 291, 352, 646, 281, 264, 2863, 13, 509, 1066, 516, 510, 11, 50704, 50704, 457, 510, 291, 352, 646, 281, 264, 700, 6889, 13, 5527, 516, 510, 11, 352, 646, 293, 586, 291, 362, 264, 1150, 6889, 13, 50980, 51048, 823, 291, 483, 281, 264, 917, 295, 1508, 732, 11, 352, 646, 281, 264, 722, 13, 440, 958, 6889, 307, 516, 281, 312, 510, 11, 51472, 51520], "temperature": 0.0, "avg_logprob": -0.20793634728540347, "compression_ratio": 1.9266666666666667, "no_speech_prob": 3.187514948876924e-06}, {"id": 396, "seek": 256256, "start": 2576.24, "end": 2584.72, "text": " Now you get to the end of class two, go back to the start. The next sample is going to be here,", "tokens": [50364, 1133, 291, 483, 281, 264, 917, 295, 1508, 1045, 11, 291, 352, 646, 281, 264, 2863, 13, 509, 1066, 516, 510, 11, 50704, 50704, 457, 510, 291, 352, 646, 281, 264, 700, 6889, 13, 5527, 516, 510, 11, 352, 646, 293, 586, 291, 362, 264, 1150, 6889, 13, 50980, 51048, 823, 291, 483, 281, 264, 917, 295, 1508, 732, 11, 352, 646, 281, 264, 722, 13, 440, 958, 6889, 307, 516, 281, 312, 510, 11, 51472, 51520], "temperature": 0.0, "avg_logprob": -0.20793634728540347, "compression_ratio": 1.9266666666666667, "no_speech_prob": 3.187514948876924e-06}, {"id": 397, "seek": 258472, "start": 2584.72, "end": 2592.3199999999997, "text": " here and here and then the next one here, here and here, here, here and then this guy wraps around", "tokens": [50364, 510, 293, 510, 293, 550, 264, 958, 472, 510, 11, 510, 293, 510, 11, 510, 11, 510, 293, 550, 341, 2146, 25831, 926, 50744, 50744, 797, 11, 5183, 13, 509, 1936, 362, 2681, 8482, 11, 2681, 20250, 337, 439, 264, 10479, 51136, 51136, 538, 445, 516, 807, 729, 733, 295, 16476, 9204, 433, 544, 2049, 337, 10479, 337, 597, 291, 51512, 51512, 362, 13366, 10938, 13, 1485, 551, 291, 820, 3122, 1128, 360, 11, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.22816320566030648, "compression_ratio": 1.6538461538461537, "no_speech_prob": 4.6377840590139385e-06}, {"id": 398, "seek": 258472, "start": 2592.3199999999997, "end": 2600.16, "text": " again, etc. You basically have equal probability, equal frequencies for all the categories", "tokens": [50364, 510, 293, 510, 293, 550, 264, 958, 472, 510, 11, 510, 293, 510, 11, 510, 11, 510, 293, 550, 341, 2146, 25831, 926, 50744, 50744, 797, 11, 5183, 13, 509, 1936, 362, 2681, 8482, 11, 2681, 20250, 337, 439, 264, 10479, 51136, 51136, 538, 445, 516, 807, 729, 733, 295, 16476, 9204, 433, 544, 2049, 337, 10479, 337, 597, 291, 51512, 51512, 362, 13366, 10938, 13, 1485, 551, 291, 820, 3122, 1128, 360, 11, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.22816320566030648, "compression_ratio": 1.6538461538461537, "no_speech_prob": 4.6377840590139385e-06}, {"id": 399, "seek": 258472, "start": 2600.16, "end": 2607.68, "text": " by just going through those kind of circular buffers more often for categories for which you", "tokens": [50364, 510, 293, 510, 293, 550, 264, 958, 472, 510, 11, 510, 293, 510, 11, 510, 11, 510, 293, 550, 341, 2146, 25831, 926, 50744, 50744, 797, 11, 5183, 13, 509, 1936, 362, 2681, 8482, 11, 2681, 20250, 337, 439, 264, 10479, 51136, 51136, 538, 445, 516, 807, 729, 733, 295, 16476, 9204, 433, 544, 2049, 337, 10479, 337, 597, 291, 51512, 51512, 362, 13366, 10938, 13, 1485, 551, 291, 820, 3122, 1128, 360, 11, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.22816320566030648, "compression_ratio": 1.6538461538461537, "no_speech_prob": 4.6377840590139385e-06}, {"id": 400, "seek": 258472, "start": 2607.68, "end": 2612.8799999999997, "text": " have fewer samples. One thing you should absolutely never do,", "tokens": [50364, 510, 293, 510, 293, 550, 264, 958, 472, 510, 11, 510, 293, 510, 11, 510, 11, 510, 293, 550, 341, 2146, 25831, 926, 50744, 50744, 797, 11, 5183, 13, 509, 1936, 362, 2681, 8482, 11, 2681, 20250, 337, 439, 264, 10479, 51136, 51136, 538, 445, 516, 807, 729, 733, 295, 16476, 9204, 433, 544, 2049, 337, 10479, 337, 597, 291, 51512, 51512, 362, 13366, 10938, 13, 1485, 551, 291, 820, 3122, 1128, 360, 11, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.22816320566030648, "compression_ratio": 1.6538461538461537, "no_speech_prob": 4.6377840590139385e-06}, {"id": 401, "seek": 261288, "start": 2612.88, "end": 2620.08, "text": " is equalize the frequencies by just not using all the samples in categories that are frequent.", "tokens": [50364, 307, 2681, 1125, 264, 20250, 538, 445, 406, 1228, 439, 264, 10938, 294, 10479, 300, 366, 18004, 13, 50724, 50724, 286, 914, 300, 311, 9263, 13, 509, 820, 1128, 718, 604, 1412, 322, 264, 4123, 13, 821, 311, 1128, 604, 1778, 281, 50940, 50940, 1856, 1412, 322, 264, 4123, 13, 823, 510, 311, 257, 1154, 365, 341, 13, 440, 1154, 365, 341, 307, 300, 934, 51288, 51288, 291, 600, 8895, 428, 18161, 2533, 281, 360, 341, 11, 428, 18161, 2533, 775, 406, 458, 466, 264, 4972, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.1975688722398546, "compression_ratio": 1.7523364485981308, "no_speech_prob": 5.681404672941426e-06}, {"id": 402, "seek": 261288, "start": 2620.08, "end": 2624.4, "text": " I mean that's horrible. You should never let any data on the floor. There's never any reason to", "tokens": [50364, 307, 2681, 1125, 264, 20250, 538, 445, 406, 1228, 439, 264, 10938, 294, 10479, 300, 366, 18004, 13, 50724, 50724, 286, 914, 300, 311, 9263, 13, 509, 820, 1128, 718, 604, 1412, 322, 264, 4123, 13, 821, 311, 1128, 604, 1778, 281, 50940, 50940, 1856, 1412, 322, 264, 4123, 13, 823, 510, 311, 257, 1154, 365, 341, 13, 440, 1154, 365, 341, 307, 300, 934, 51288, 51288, 291, 600, 8895, 428, 18161, 2533, 281, 360, 341, 11, 428, 18161, 2533, 775, 406, 458, 466, 264, 4972, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.1975688722398546, "compression_ratio": 1.7523364485981308, "no_speech_prob": 5.681404672941426e-06}, {"id": 403, "seek": 261288, "start": 2624.4, "end": 2631.36, "text": " leave data on the floor. Now here's a problem with this. The problem with this is that after", "tokens": [50364, 307, 2681, 1125, 264, 20250, 538, 445, 406, 1228, 439, 264, 10938, 294, 10479, 300, 366, 18004, 13, 50724, 50724, 286, 914, 300, 311, 9263, 13, 509, 820, 1128, 718, 604, 1412, 322, 264, 4123, 13, 821, 311, 1128, 604, 1778, 281, 50940, 50940, 1856, 1412, 322, 264, 4123, 13, 823, 510, 311, 257, 1154, 365, 341, 13, 440, 1154, 365, 341, 307, 300, 934, 51288, 51288, 291, 600, 8895, 428, 18161, 2533, 281, 360, 341, 11, 428, 18161, 2533, 775, 406, 458, 466, 264, 4972, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.1975688722398546, "compression_ratio": 1.7523364485981308, "no_speech_prob": 5.681404672941426e-06}, {"id": 404, "seek": 261288, "start": 2631.36, "end": 2637.6, "text": " you've trained your neural net to do this, your neural net does not know about the relative", "tokens": [50364, 307, 2681, 1125, 264, 20250, 538, 445, 406, 1228, 439, 264, 10938, 294, 10479, 300, 366, 18004, 13, 50724, 50724, 286, 914, 300, 311, 9263, 13, 509, 820, 1128, 718, 604, 1412, 322, 264, 4123, 13, 821, 311, 1128, 604, 1778, 281, 50940, 50940, 1856, 1412, 322, 264, 4123, 13, 823, 510, 311, 257, 1154, 365, 341, 13, 440, 1154, 365, 341, 307, 300, 934, 51288, 51288, 291, 600, 8895, 428, 18161, 2533, 281, 360, 341, 11, 428, 18161, 2533, 775, 406, 458, 466, 264, 4972, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.1975688722398546, "compression_ratio": 1.7523364485981308, "no_speech_prob": 5.681404672941426e-06}, {"id": 405, "seek": 263760, "start": 2637.6, "end": 2643.2799999999997, "text": " likelihood, the relative frequencies of the samples. Let's say this is a system that does", "tokens": [50364, 22119, 11, 264, 4972, 20250, 295, 264, 10938, 13, 961, 311, 584, 341, 307, 257, 1185, 300, 775, 50648, 50648, 4625, 15217, 13, 467, 1177, 380, 458, 300, 264, 2689, 3554, 307, 636, 11, 636, 544, 18004, 813, 50972, 51040, 16730, 5592, 420, 746, 13, 708, 291, 643, 281, 360, 294, 264, 917, 307, 360, 257, 1320, 11, 257, 1326, 11335, 4317, 11, 51548, 51580], "temperature": 0.0, "avg_logprob": -0.21795052201000611, "compression_ratio": 1.4944444444444445, "no_speech_prob": 1.2801267985196318e-05}, {"id": 406, "seek": 263760, "start": 2643.2799999999997, "end": 2649.7599999999998, "text": " medical diagnosis. It doesn't know that the common cold is way, way more frequent than", "tokens": [50364, 22119, 11, 264, 4972, 20250, 295, 264, 10938, 13, 961, 311, 584, 341, 307, 257, 1185, 300, 775, 50648, 50648, 4625, 15217, 13, 467, 1177, 380, 458, 300, 264, 2689, 3554, 307, 636, 11, 636, 544, 18004, 813, 50972, 51040, 16730, 5592, 420, 746, 13, 708, 291, 643, 281, 360, 294, 264, 917, 307, 360, 257, 1320, 11, 257, 1326, 11335, 4317, 11, 51548, 51580], "temperature": 0.0, "avg_logprob": -0.21795052201000611, "compression_ratio": 1.4944444444444445, "no_speech_prob": 1.2801267985196318e-05}, {"id": 407, "seek": 263760, "start": 2651.12, "end": 2661.2799999999997, "text": " lung cancer or something. What you need to do in the end is do a pass, a few passes perhaps,", "tokens": [50364, 22119, 11, 264, 4972, 20250, 295, 264, 10938, 13, 961, 311, 584, 341, 307, 257, 1185, 300, 775, 50648, 50648, 4625, 15217, 13, 467, 1177, 380, 458, 300, 264, 2689, 3554, 307, 636, 11, 636, 544, 18004, 813, 50972, 51040, 16730, 5592, 420, 746, 13, 708, 291, 643, 281, 360, 294, 264, 917, 307, 360, 257, 1320, 11, 257, 1326, 11335, 4317, 11, 51548, 51580], "temperature": 0.0, "avg_logprob": -0.21795052201000611, "compression_ratio": 1.4944444444444445, "no_speech_prob": 1.2801267985196318e-05}, {"id": 408, "seek": 266128, "start": 2661.28, "end": 2666.6400000000003, "text": " where you can fine-tune your system with the actual frequencies of the categories.", "tokens": [50364, 689, 291, 393, 2489, 12, 83, 2613, 428, 1185, 365, 264, 3539, 20250, 295, 264, 10479, 13, 50632, 50632, 440, 1802, 295, 341, 307, 516, 281, 312, 337, 264, 1185, 281, 6231, 264, 32152, 412, 264, 5598, 4583, 370, 300, 50900, 50900, 264, 22119, 295, 257, 15217, 23249, 281, 264, 7893, 295, 309, 13, 467, 311, 516, 281, 2294, 51320, 51320, 721, 300, 366, 544, 18004, 13, 440, 1778, 983, 291, 500, 380, 528, 281, 360, 341, 1830, 264, 2302, 3097, 51536, 51564, 307, 570, 498, 291, 3847, 257, 2120, 388, 11167, 2533, 11, 309, 311, 516, 281, 312, 1075, 281, 6231, 264, 20250, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.2774900507043909, "compression_ratio": 1.834008097165992, "no_speech_prob": 1.061463262885809e-05}, {"id": 409, "seek": 266128, "start": 2666.6400000000003, "end": 2672.0, "text": " The effect of this is going to be for the system to adapt the biases at the output layer so that", "tokens": [50364, 689, 291, 393, 2489, 12, 83, 2613, 428, 1185, 365, 264, 3539, 20250, 295, 264, 10479, 13, 50632, 50632, 440, 1802, 295, 341, 307, 516, 281, 312, 337, 264, 1185, 281, 6231, 264, 32152, 412, 264, 5598, 4583, 370, 300, 50900, 50900, 264, 22119, 295, 257, 15217, 23249, 281, 264, 7893, 295, 309, 13, 467, 311, 516, 281, 2294, 51320, 51320, 721, 300, 366, 544, 18004, 13, 440, 1778, 983, 291, 500, 380, 528, 281, 360, 341, 1830, 264, 2302, 3097, 51536, 51564, 307, 570, 498, 291, 3847, 257, 2120, 388, 11167, 2533, 11, 309, 311, 516, 281, 312, 1075, 281, 6231, 264, 20250, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.2774900507043909, "compression_ratio": 1.834008097165992, "no_speech_prob": 1.061463262885809e-05}, {"id": 410, "seek": 266128, "start": 2672.0, "end": 2680.4, "text": " the likelihood of a diagnosis corresponds to the frequency of it. It's going to favor", "tokens": [50364, 689, 291, 393, 2489, 12, 83, 2613, 428, 1185, 365, 264, 3539, 20250, 295, 264, 10479, 13, 50632, 50632, 440, 1802, 295, 341, 307, 516, 281, 312, 337, 264, 1185, 281, 6231, 264, 32152, 412, 264, 5598, 4583, 370, 300, 50900, 50900, 264, 22119, 295, 257, 15217, 23249, 281, 264, 7893, 295, 309, 13, 467, 311, 516, 281, 2294, 51320, 51320, 721, 300, 366, 544, 18004, 13, 440, 1778, 983, 291, 500, 380, 528, 281, 360, 341, 1830, 264, 2302, 3097, 51536, 51564, 307, 570, 498, 291, 3847, 257, 2120, 388, 11167, 2533, 11, 309, 311, 516, 281, 312, 1075, 281, 6231, 264, 20250, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.2774900507043909, "compression_ratio": 1.834008097165992, "no_speech_prob": 1.061463262885809e-05}, {"id": 411, "seek": 266128, "start": 2680.4, "end": 2684.7200000000003, "text": " things that are more frequent. The reason why you don't want to do this during the entire training", "tokens": [50364, 689, 291, 393, 2489, 12, 83, 2613, 428, 1185, 365, 264, 3539, 20250, 295, 264, 10479, 13, 50632, 50632, 440, 1802, 295, 341, 307, 516, 281, 312, 337, 264, 1185, 281, 6231, 264, 32152, 412, 264, 5598, 4583, 370, 300, 50900, 50900, 264, 22119, 295, 257, 15217, 23249, 281, 264, 7893, 295, 309, 13, 467, 311, 516, 281, 2294, 51320, 51320, 721, 300, 366, 544, 18004, 13, 440, 1778, 983, 291, 500, 380, 528, 281, 360, 341, 1830, 264, 2302, 3097, 51536, 51564, 307, 570, 498, 291, 3847, 257, 2120, 388, 11167, 2533, 11, 309, 311, 516, 281, 312, 1075, 281, 6231, 264, 20250, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.2774900507043909, "compression_ratio": 1.834008097165992, "no_speech_prob": 1.061463262885809e-05}, {"id": 412, "seek": 266128, "start": 2685.28, "end": 2691.2000000000003, "text": " is because if you train a multilayer net, it's going to be able to adapt the frequencies", "tokens": [50364, 689, 291, 393, 2489, 12, 83, 2613, 428, 1185, 365, 264, 3539, 20250, 295, 264, 10479, 13, 50632, 50632, 440, 1802, 295, 341, 307, 516, 281, 312, 337, 264, 1185, 281, 6231, 264, 32152, 412, 264, 5598, 4583, 370, 300, 50900, 50900, 264, 22119, 295, 257, 15217, 23249, 281, 264, 7893, 295, 309, 13, 467, 311, 516, 281, 2294, 51320, 51320, 721, 300, 366, 544, 18004, 13, 440, 1778, 983, 291, 500, 380, 528, 281, 360, 341, 1830, 264, 2302, 3097, 51536, 51564, 307, 570, 498, 291, 3847, 257, 2120, 388, 11167, 2533, 11, 309, 311, 516, 281, 312, 1075, 281, 6231, 264, 20250, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.2774900507043909, "compression_ratio": 1.834008097165992, "no_speech_prob": 1.061463262885809e-05}, {"id": 413, "seek": 269120, "start": 2691.2, "end": 2698.56, "text": " that the system basically never develops the right features for rare cases. I may have", "tokens": [50364, 300, 264, 1185, 1936, 1128, 25453, 264, 558, 4122, 337, 5892, 3331, 13, 286, 815, 362, 50732, 50732, 10759, 466, 341, 1217, 294, 264, 1508, 294, 1791, 3259, 13, 1407, 32162, 264, 1365, 295, 4625, 1395, 11, 51152, 51296, 562, 291, 352, 281, 4625, 1395, 11, 291, 500, 380, 3496, 565, 7601, 264, 5029, 300, 307, 24969, 281, 264, 51580, 51580], "temperature": 0.6, "avg_logprob": -0.26666141691662015, "compression_ratio": 1.5245901639344261, "no_speech_prob": 1.3627125554194208e-05}, {"id": 414, "seek": 269120, "start": 2698.56, "end": 2706.96, "text": " spoken about this already in the class in past weeks. To recycle the example of medical school,", "tokens": [50364, 300, 264, 1185, 1936, 1128, 25453, 264, 558, 4122, 337, 5892, 3331, 13, 286, 815, 362, 50732, 50732, 10759, 466, 341, 1217, 294, 264, 1508, 294, 1791, 3259, 13, 1407, 32162, 264, 1365, 295, 4625, 1395, 11, 51152, 51296, 562, 291, 352, 281, 4625, 1395, 11, 291, 500, 380, 3496, 565, 7601, 264, 5029, 300, 307, 24969, 281, 264, 51580, 51580], "temperature": 0.6, "avg_logprob": -0.26666141691662015, "compression_ratio": 1.5245901639344261, "no_speech_prob": 1.3627125554194208e-05}, {"id": 415, "seek": 269120, "start": 2709.8399999999997, "end": 2715.52, "text": " when you go to medical school, you don't spend time studying the flu that is proportional to the", "tokens": [50364, 300, 264, 1185, 1936, 1128, 25453, 264, 558, 4122, 337, 5892, 3331, 13, 286, 815, 362, 50732, 50732, 10759, 466, 341, 1217, 294, 264, 1508, 294, 1791, 3259, 13, 1407, 32162, 264, 1365, 295, 4625, 1395, 11, 51152, 51296, 562, 291, 352, 281, 4625, 1395, 11, 291, 500, 380, 3496, 565, 7601, 264, 5029, 300, 307, 24969, 281, 264, 51580, 51580], "temperature": 0.6, "avg_logprob": -0.26666141691662015, "compression_ratio": 1.5245901639344261, "no_speech_prob": 1.3627125554194208e-05}, {"id": 416, "seek": 271552, "start": 2715.52, "end": 2719.42, "text": " of the flu with respect to very rare diseases, for example.", "tokens": [50364, 295, 264, 5029, 365, 3104, 281, 588, 5892, 11044, 11, 337, 1365, 13, 50559, 50620, 509, 3496, 1936, 264, 912, 565, 7601, 439, 264, 11044, 13, 50747, 50747, 682, 1186, 11, 291, 3496, 544, 565, 7601, 6179, 472, 11, 50883, 50883, 597, 2673, 3928, 281, 312, 5892, 13, 50965, 51007, 400, 300, 311, 570, 291, 643, 281, 1499, 264, 4122, 337, 309, 13, 51199, 51199, 400, 550, 291, 643, 281, 733, 295, 3006, 337, 264, 1186, 300, 51331, 51331, 729, 5892, 11044, 366, 5892, 13, 51485, 51485, 407, 291, 500, 380, 9091, 264, 15217, 337, 5892, 11044, 51735], "temperature": 0.0, "avg_logprob": -0.20535848617553712, "compression_ratio": 1.8198198198198199, "no_speech_prob": 0.0053832014091312885}, {"id": 417, "seek": 271552, "start": 2720.64, "end": 2723.18, "text": " You spend basically the same time studying all the diseases.", "tokens": [50364, 295, 264, 5029, 365, 3104, 281, 588, 5892, 11044, 11, 337, 1365, 13, 50559, 50620, 509, 3496, 1936, 264, 912, 565, 7601, 439, 264, 11044, 13, 50747, 50747, 682, 1186, 11, 291, 3496, 544, 565, 7601, 6179, 472, 11, 50883, 50883, 597, 2673, 3928, 281, 312, 5892, 13, 50965, 51007, 400, 300, 311, 570, 291, 643, 281, 1499, 264, 4122, 337, 309, 13, 51199, 51199, 400, 550, 291, 643, 281, 733, 295, 3006, 337, 264, 1186, 300, 51331, 51331, 729, 5892, 11044, 366, 5892, 13, 51485, 51485, 407, 291, 500, 380, 9091, 264, 15217, 337, 5892, 11044, 51735], "temperature": 0.0, "avg_logprob": -0.20535848617553712, "compression_ratio": 1.8198198198198199, "no_speech_prob": 0.0053832014091312885}, {"id": 418, "seek": 271552, "start": 2723.18, "end": 2725.9, "text": " In fact, you spend more time studying complicated one,", "tokens": [50364, 295, 264, 5029, 365, 3104, 281, 588, 5892, 11044, 11, 337, 1365, 13, 50559, 50620, 509, 3496, 1936, 264, 912, 565, 7601, 439, 264, 11044, 13, 50747, 50747, 682, 1186, 11, 291, 3496, 544, 565, 7601, 6179, 472, 11, 50883, 50883, 597, 2673, 3928, 281, 312, 5892, 13, 50965, 51007, 400, 300, 311, 570, 291, 643, 281, 1499, 264, 4122, 337, 309, 13, 51199, 51199, 400, 550, 291, 643, 281, 733, 295, 3006, 337, 264, 1186, 300, 51331, 51331, 729, 5892, 11044, 366, 5892, 13, 51485, 51485, 407, 291, 500, 380, 9091, 264, 15217, 337, 5892, 11044, 51735], "temperature": 0.0, "avg_logprob": -0.20535848617553712, "compression_ratio": 1.8198198198198199, "no_speech_prob": 0.0053832014091312885}, {"id": 419, "seek": 271552, "start": 2725.9, "end": 2727.54, "text": " which usually tend to be rare.", "tokens": [50364, 295, 264, 5029, 365, 3104, 281, 588, 5892, 11044, 11, 337, 1365, 13, 50559, 50620, 509, 3496, 1936, 264, 912, 565, 7601, 439, 264, 11044, 13, 50747, 50747, 682, 1186, 11, 291, 3496, 544, 565, 7601, 6179, 472, 11, 50883, 50883, 597, 2673, 3928, 281, 312, 5892, 13, 50965, 51007, 400, 300, 311, 570, 291, 643, 281, 1499, 264, 4122, 337, 309, 13, 51199, 51199, 400, 550, 291, 643, 281, 733, 295, 3006, 337, 264, 1186, 300, 51331, 51331, 729, 5892, 11044, 366, 5892, 13, 51485, 51485, 407, 291, 500, 380, 9091, 264, 15217, 337, 5892, 11044, 51735], "temperature": 0.0, "avg_logprob": -0.20535848617553712, "compression_ratio": 1.8198198198198199, "no_speech_prob": 0.0053832014091312885}, {"id": 420, "seek": 271552, "start": 2728.38, "end": 2732.22, "text": " And that's because you need to develop the features for it.", "tokens": [50364, 295, 264, 5029, 365, 3104, 281, 588, 5892, 11044, 11, 337, 1365, 13, 50559, 50620, 509, 3496, 1936, 264, 912, 565, 7601, 439, 264, 11044, 13, 50747, 50747, 682, 1186, 11, 291, 3496, 544, 565, 7601, 6179, 472, 11, 50883, 50883, 597, 2673, 3928, 281, 312, 5892, 13, 50965, 51007, 400, 300, 311, 570, 291, 643, 281, 1499, 264, 4122, 337, 309, 13, 51199, 51199, 400, 550, 291, 643, 281, 733, 295, 3006, 337, 264, 1186, 300, 51331, 51331, 729, 5892, 11044, 366, 5892, 13, 51485, 51485, 407, 291, 500, 380, 9091, 264, 15217, 337, 5892, 11044, 51735], "temperature": 0.0, "avg_logprob": -0.20535848617553712, "compression_ratio": 1.8198198198198199, "no_speech_prob": 0.0053832014091312885}, {"id": 421, "seek": 271552, "start": 2732.22, "end": 2734.86, "text": " And then you need to kind of correct for the fact that", "tokens": [50364, 295, 264, 5029, 365, 3104, 281, 588, 5892, 11044, 11, 337, 1365, 13, 50559, 50620, 509, 3496, 1936, 264, 912, 565, 7601, 439, 264, 11044, 13, 50747, 50747, 682, 1186, 11, 291, 3496, 544, 565, 7601, 6179, 472, 11, 50883, 50883, 597, 2673, 3928, 281, 312, 5892, 13, 50965, 51007, 400, 300, 311, 570, 291, 643, 281, 1499, 264, 4122, 337, 309, 13, 51199, 51199, 400, 550, 291, 643, 281, 733, 295, 3006, 337, 264, 1186, 300, 51331, 51331, 729, 5892, 11044, 366, 5892, 13, 51485, 51485, 407, 291, 500, 380, 9091, 264, 15217, 337, 5892, 11044, 51735], "temperature": 0.0, "avg_logprob": -0.20535848617553712, "compression_ratio": 1.8198198198198199, "no_speech_prob": 0.0053832014091312885}, {"id": 422, "seek": 271552, "start": 2734.86, "end": 2737.94, "text": " those rare diseases are rare.", "tokens": [50364, 295, 264, 5029, 365, 3104, 281, 588, 5892, 11044, 11, 337, 1365, 13, 50559, 50620, 509, 3496, 1936, 264, 912, 565, 7601, 439, 264, 11044, 13, 50747, 50747, 682, 1186, 11, 291, 3496, 544, 565, 7601, 6179, 472, 11, 50883, 50883, 597, 2673, 3928, 281, 312, 5892, 13, 50965, 51007, 400, 300, 311, 570, 291, 643, 281, 1499, 264, 4122, 337, 309, 13, 51199, 51199, 400, 550, 291, 643, 281, 733, 295, 3006, 337, 264, 1186, 300, 51331, 51331, 729, 5892, 11044, 366, 5892, 13, 51485, 51485, 407, 291, 500, 380, 9091, 264, 15217, 337, 5892, 11044, 51735], "temperature": 0.0, "avg_logprob": -0.20535848617553712, "compression_ratio": 1.8198198198198199, "no_speech_prob": 0.0053832014091312885}, {"id": 423, "seek": 273794, "start": 2737.94, "end": 2746.28, "text": " So you don't suspect the diagnosis for rare diseases very often,", "tokens": [50364, 407, 291, 500, 380, 9091, 264, 15217, 337, 5892, 11044, 588, 2049, 11, 50781, 50781, 570, 309, 311, 5892, 13, 50856, 51046, 1033, 13, 51071, 51184, 407, 300, 311, 439, 337, 17443, 13, 51272, 51336, 11623, 12, 317, 27514, 4470, 13, 51372, 51372, 407, 291, 600, 668, 1228, 341, 257, 688, 11, 295, 1164, 13, 51468, 51518, 400, 3278, 12, 317, 27514, 4470, 307, 257, 733, 295, 44559, 295, 732, 721, 13, 51792, 51828], "temperature": 0.0, "avg_logprob": -0.35368771302072627, "compression_ratio": 1.4753086419753085, "no_speech_prob": 3.500372258713469e-06}, {"id": 424, "seek": 273794, "start": 2746.28, "end": 2747.78, "text": " because it's rare.", "tokens": [50364, 407, 291, 500, 380, 9091, 264, 15217, 337, 5892, 11044, 588, 2049, 11, 50781, 50781, 570, 309, 311, 5892, 13, 50856, 51046, 1033, 13, 51071, 51184, 407, 300, 311, 439, 337, 17443, 13, 51272, 51336, 11623, 12, 317, 27514, 4470, 13, 51372, 51372, 407, 291, 600, 668, 1228, 341, 257, 688, 11, 295, 1164, 13, 51468, 51518, 400, 3278, 12, 317, 27514, 4470, 307, 257, 733, 295, 44559, 295, 732, 721, 13, 51792, 51828], "temperature": 0.0, "avg_logprob": -0.35368771302072627, "compression_ratio": 1.4753086419753085, "no_speech_prob": 3.500372258713469e-06}, {"id": 425, "seek": 273794, "start": 2751.58, "end": 2752.08, "text": " Okay.", "tokens": [50364, 407, 291, 500, 380, 9091, 264, 15217, 337, 5892, 11044, 588, 2049, 11, 50781, 50781, 570, 309, 311, 5892, 13, 50856, 51046, 1033, 13, 51071, 51184, 407, 300, 311, 439, 337, 17443, 13, 51272, 51336, 11623, 12, 317, 27514, 4470, 13, 51372, 51372, 407, 291, 600, 668, 1228, 341, 257, 688, 11, 295, 1164, 13, 51468, 51518, 400, 3278, 12, 317, 27514, 4470, 307, 257, 733, 295, 44559, 295, 732, 721, 13, 51792, 51828], "temperature": 0.0, "avg_logprob": -0.35368771302072627, "compression_ratio": 1.4753086419753085, "no_speech_prob": 3.500372258713469e-06}, {"id": 426, "seek": 273794, "start": 2754.34, "end": 2756.1, "text": " So that's all for weights.", "tokens": [50364, 407, 291, 500, 380, 9091, 264, 15217, 337, 5892, 11044, 588, 2049, 11, 50781, 50781, 570, 309, 311, 5892, 13, 50856, 51046, 1033, 13, 51071, 51184, 407, 300, 311, 439, 337, 17443, 13, 51272, 51336, 11623, 12, 317, 27514, 4470, 13, 51372, 51372, 407, 291, 600, 668, 1228, 341, 257, 688, 11, 295, 1164, 13, 51468, 51518, 400, 3278, 12, 317, 27514, 4470, 307, 257, 733, 295, 44559, 295, 732, 721, 13, 51792, 51828], "temperature": 0.0, "avg_logprob": -0.35368771302072627, "compression_ratio": 1.4753086419753085, "no_speech_prob": 3.500372258713469e-06}, {"id": 427, "seek": 273794, "start": 2757.38, "end": 2758.1, "text": " Cross-entropy loss.", "tokens": [50364, 407, 291, 500, 380, 9091, 264, 15217, 337, 5892, 11044, 588, 2049, 11, 50781, 50781, 570, 309, 311, 5892, 13, 50856, 51046, 1033, 13, 51071, 51184, 407, 300, 311, 439, 337, 17443, 13, 51272, 51336, 11623, 12, 317, 27514, 4470, 13, 51372, 51372, 407, 291, 600, 668, 1228, 341, 257, 688, 11, 295, 1164, 13, 51468, 51518, 400, 3278, 12, 317, 27514, 4470, 307, 257, 733, 295, 44559, 295, 732, 721, 13, 51792, 51828], "temperature": 0.0, "avg_logprob": -0.35368771302072627, "compression_ratio": 1.4753086419753085, "no_speech_prob": 3.500372258713469e-06}, {"id": 428, "seek": 273794, "start": 2758.1, "end": 2760.02, "text": " So you've been using this a lot, of course.", "tokens": [50364, 407, 291, 500, 380, 9091, 264, 15217, 337, 5892, 11044, 588, 2049, 11, 50781, 50781, 570, 309, 311, 5892, 13, 50856, 51046, 1033, 13, 51071, 51184, 407, 300, 311, 439, 337, 17443, 13, 51272, 51336, 11623, 12, 317, 27514, 4470, 13, 51372, 51372, 407, 291, 600, 668, 1228, 341, 257, 688, 11, 295, 1164, 13, 51468, 51518, 400, 3278, 12, 317, 27514, 4470, 307, 257, 733, 295, 44559, 295, 732, 721, 13, 51792, 51828], "temperature": 0.0, "avg_logprob": -0.35368771302072627, "compression_ratio": 1.4753086419753085, "no_speech_prob": 3.500372258713469e-06}, {"id": 429, "seek": 273794, "start": 2761.02, "end": 2766.5, "text": " And cross-entropy loss is a kind of merging of two things.", "tokens": [50364, 407, 291, 500, 380, 9091, 264, 15217, 337, 5892, 11044, 588, 2049, 11, 50781, 50781, 570, 309, 311, 5892, 13, 50856, 51046, 1033, 13, 51071, 51184, 407, 300, 311, 439, 337, 17443, 13, 51272, 51336, 11623, 12, 317, 27514, 4470, 13, 51372, 51372, 407, 291, 600, 668, 1228, 341, 257, 688, 11, 295, 1164, 13, 51468, 51518, 400, 3278, 12, 317, 27514, 4470, 307, 257, 733, 295, 44559, 295, 732, 721, 13, 51792, 51828], "temperature": 0.0, "avg_logprob": -0.35368771302072627, "compression_ratio": 1.4753086419753085, "no_speech_prob": 3.500372258713469e-06}, {"id": 430, "seek": 276650, "start": 2766.5, "end": 2771.3, "text": " The merging of log softmax function and negative likelihood loss.", "tokens": [50364, 440, 44559, 295, 3565, 2787, 41167, 2445, 293, 3671, 22119, 4470, 13, 50604, 50604, 400, 264, 1778, 983, 291, 528, 281, 362, 341, 307, 337, 29054, 4112, 13, 50864, 51024, 407, 264, 3565, 2787, 41167, 307, 1936, 257, 2787, 41167, 6263, 538, 257, 3565, 13, 51280, 51280, 407, 291, 700, 14722, 264, 2787, 41167, 11, 550, 291, 360, 264, 3565, 13, 51440, 51440, 759, 291, 360, 2787, 41167, 11, 550, 3565, 11, 293, 291, 646, 48256, 807, 341, 11, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.24274821453783885, "compression_ratio": 1.679144385026738, "no_speech_prob": 6.960026439628564e-06}, {"id": 431, "seek": 276650, "start": 2771.3, "end": 2776.5, "text": " And the reason why you want to have this is for numerical reasons.", "tokens": [50364, 440, 44559, 295, 3565, 2787, 41167, 2445, 293, 3671, 22119, 4470, 13, 50604, 50604, 400, 264, 1778, 983, 291, 528, 281, 362, 341, 307, 337, 29054, 4112, 13, 50864, 51024, 407, 264, 3565, 2787, 41167, 307, 1936, 257, 2787, 41167, 6263, 538, 257, 3565, 13, 51280, 51280, 407, 291, 700, 14722, 264, 2787, 41167, 11, 550, 291, 360, 264, 3565, 13, 51440, 51440, 759, 291, 360, 2787, 41167, 11, 550, 3565, 11, 293, 291, 646, 48256, 807, 341, 11, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.24274821453783885, "compression_ratio": 1.679144385026738, "no_speech_prob": 6.960026439628564e-06}, {"id": 432, "seek": 276650, "start": 2779.7, "end": 2784.82, "text": " So the log softmax is basically a softmax followed by a log.", "tokens": [50364, 440, 44559, 295, 3565, 2787, 41167, 2445, 293, 3671, 22119, 4470, 13, 50604, 50604, 400, 264, 1778, 983, 291, 528, 281, 362, 341, 307, 337, 29054, 4112, 13, 50864, 51024, 407, 264, 3565, 2787, 41167, 307, 1936, 257, 2787, 41167, 6263, 538, 257, 3565, 13, 51280, 51280, 407, 291, 700, 14722, 264, 2787, 41167, 11, 550, 291, 360, 264, 3565, 13, 51440, 51440, 759, 291, 360, 2787, 41167, 11, 550, 3565, 11, 293, 291, 646, 48256, 807, 341, 11, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.24274821453783885, "compression_ratio": 1.679144385026738, "no_speech_prob": 6.960026439628564e-06}, {"id": 433, "seek": 276650, "start": 2784.82, "end": 2788.02, "text": " So you first compute the softmax, then you do the log.", "tokens": [50364, 440, 44559, 295, 3565, 2787, 41167, 2445, 293, 3671, 22119, 4470, 13, 50604, 50604, 400, 264, 1778, 983, 291, 528, 281, 362, 341, 307, 337, 29054, 4112, 13, 50864, 51024, 407, 264, 3565, 2787, 41167, 307, 1936, 257, 2787, 41167, 6263, 538, 257, 3565, 13, 51280, 51280, 407, 291, 700, 14722, 264, 2787, 41167, 11, 550, 291, 360, 264, 3565, 13, 51440, 51440, 759, 291, 360, 2787, 41167, 11, 550, 3565, 11, 293, 291, 646, 48256, 807, 341, 11, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.24274821453783885, "compression_ratio": 1.679144385026738, "no_speech_prob": 6.960026439628564e-06}, {"id": 434, "seek": 276650, "start": 2788.02, "end": 2791.86, "text": " If you do softmax, then log, and you back propagate through this,", "tokens": [50364, 440, 44559, 295, 3565, 2787, 41167, 2445, 293, 3671, 22119, 4470, 13, 50604, 50604, 400, 264, 1778, 983, 291, 528, 281, 362, 341, 307, 337, 29054, 4112, 13, 50864, 51024, 407, 264, 3565, 2787, 41167, 307, 1936, 257, 2787, 41167, 6263, 538, 257, 3565, 13, 51280, 51280, 407, 291, 700, 14722, 264, 2787, 41167, 11, 550, 291, 360, 264, 3565, 13, 51440, 51440, 759, 291, 360, 2787, 41167, 11, 550, 3565, 11, 293, 291, 646, 48256, 807, 341, 11, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.24274821453783885, "compression_ratio": 1.679144385026738, "no_speech_prob": 6.960026439628564e-06}, {"id": 435, "seek": 279186, "start": 2791.86, "end": 2797.1400000000003, "text": " you might have gradients in the middle between the log and the softmax", "tokens": [50364, 291, 1062, 362, 2771, 2448, 294, 264, 2808, 1296, 264, 3565, 293, 264, 2787, 41167, 50628, 50628, 300, 917, 493, 885, 13785, 13, 50700, 50816, 407, 337, 1365, 11, 498, 264, 6674, 2158, 295, 472, 295, 264, 2787, 41167, 307, 1998, 281, 472, 11, 51084, 51252, 293, 512, 295, 264, 661, 2306, 366, 1998, 281, 4018, 11, 51332, 51332, 291, 747, 264, 3565, 11, 291, 483, 746, 300, 311, 1998, 281, 3175, 13202, 13, 51524, 51524, 509, 646, 48256, 807, 264, 3565, 11, 291, 483, 746, 300, 311, 1998, 281, 13202, 13, 51676, 51708], "temperature": 0.0, "avg_logprob": -0.1077451507250468, "compression_ratio": 1.8871794871794871, "no_speech_prob": 7.1826593739388045e-06}, {"id": 436, "seek": 279186, "start": 2797.1400000000003, "end": 2798.58, "text": " that end up being infinite.", "tokens": [50364, 291, 1062, 362, 2771, 2448, 294, 264, 2808, 1296, 264, 3565, 293, 264, 2787, 41167, 50628, 50628, 300, 917, 493, 885, 13785, 13, 50700, 50816, 407, 337, 1365, 11, 498, 264, 6674, 2158, 295, 472, 295, 264, 2787, 41167, 307, 1998, 281, 472, 11, 51084, 51252, 293, 512, 295, 264, 661, 2306, 366, 1998, 281, 4018, 11, 51332, 51332, 291, 747, 264, 3565, 11, 291, 483, 746, 300, 311, 1998, 281, 3175, 13202, 13, 51524, 51524, 509, 646, 48256, 807, 264, 3565, 11, 291, 483, 746, 300, 311, 1998, 281, 13202, 13, 51676, 51708], "temperature": 0.0, "avg_logprob": -0.1077451507250468, "compression_ratio": 1.8871794871794871, "no_speech_prob": 7.1826593739388045e-06}, {"id": 437, "seek": 279186, "start": 2800.9, "end": 2806.26, "text": " So for example, if the maximum value of one of the softmax is close to one,", "tokens": [50364, 291, 1062, 362, 2771, 2448, 294, 264, 2808, 1296, 264, 3565, 293, 264, 2787, 41167, 50628, 50628, 300, 917, 493, 885, 13785, 13, 50700, 50816, 407, 337, 1365, 11, 498, 264, 6674, 2158, 295, 472, 295, 264, 2787, 41167, 307, 1998, 281, 472, 11, 51084, 51252, 293, 512, 295, 264, 661, 2306, 366, 1998, 281, 4018, 11, 51332, 51332, 291, 747, 264, 3565, 11, 291, 483, 746, 300, 311, 1998, 281, 3175, 13202, 13, 51524, 51524, 509, 646, 48256, 807, 264, 3565, 11, 291, 483, 746, 300, 311, 1998, 281, 13202, 13, 51676, 51708], "temperature": 0.0, "avg_logprob": -0.1077451507250468, "compression_ratio": 1.8871794871794871, "no_speech_prob": 7.1826593739388045e-06}, {"id": 438, "seek": 279186, "start": 2809.6200000000003, "end": 2811.2200000000003, "text": " and some of the other ones are close to zero,", "tokens": [50364, 291, 1062, 362, 2771, 2448, 294, 264, 2808, 1296, 264, 3565, 293, 264, 2787, 41167, 50628, 50628, 300, 917, 493, 885, 13785, 13, 50700, 50816, 407, 337, 1365, 11, 498, 264, 6674, 2158, 295, 472, 295, 264, 2787, 41167, 307, 1998, 281, 472, 11, 51084, 51252, 293, 512, 295, 264, 661, 2306, 366, 1998, 281, 4018, 11, 51332, 51332, 291, 747, 264, 3565, 11, 291, 483, 746, 300, 311, 1998, 281, 3175, 13202, 13, 51524, 51524, 509, 646, 48256, 807, 264, 3565, 11, 291, 483, 746, 300, 311, 1998, 281, 13202, 13, 51676, 51708], "temperature": 0.0, "avg_logprob": -0.1077451507250468, "compression_ratio": 1.8871794871794871, "no_speech_prob": 7.1826593739388045e-06}, {"id": 439, "seek": 279186, "start": 2811.2200000000003, "end": 2815.06, "text": " you take the log, you get something that's close to minus infinity.", "tokens": [50364, 291, 1062, 362, 2771, 2448, 294, 264, 2808, 1296, 264, 3565, 293, 264, 2787, 41167, 50628, 50628, 300, 917, 493, 885, 13785, 13, 50700, 50816, 407, 337, 1365, 11, 498, 264, 6674, 2158, 295, 472, 295, 264, 2787, 41167, 307, 1998, 281, 472, 11, 51084, 51252, 293, 512, 295, 264, 661, 2306, 366, 1998, 281, 4018, 11, 51332, 51332, 291, 747, 264, 3565, 11, 291, 483, 746, 300, 311, 1998, 281, 3175, 13202, 13, 51524, 51524, 509, 646, 48256, 807, 264, 3565, 11, 291, 483, 746, 300, 311, 1998, 281, 13202, 13, 51676, 51708], "temperature": 0.0, "avg_logprob": -0.1077451507250468, "compression_ratio": 1.8871794871794871, "no_speech_prob": 7.1826593739388045e-06}, {"id": 440, "seek": 279186, "start": 2815.06, "end": 2818.1, "text": " You back propagate through the log, you get something that's close to infinity.", "tokens": [50364, 291, 1062, 362, 2771, 2448, 294, 264, 2808, 1296, 264, 3565, 293, 264, 2787, 41167, 50628, 50628, 300, 917, 493, 885, 13785, 13, 50700, 50816, 407, 337, 1365, 11, 498, 264, 6674, 2158, 295, 472, 295, 264, 2787, 41167, 307, 1998, 281, 472, 11, 51084, 51252, 293, 512, 295, 264, 661, 2306, 366, 1998, 281, 4018, 11, 51332, 51332, 291, 747, 264, 3565, 11, 291, 483, 746, 300, 311, 1998, 281, 3175, 13202, 13, 51524, 51524, 509, 646, 48256, 807, 264, 3565, 11, 291, 483, 746, 300, 311, 1998, 281, 13202, 13, 51676, 51708], "temperature": 0.0, "avg_logprob": -0.1077451507250468, "compression_ratio": 1.8871794871794871, "no_speech_prob": 7.1826593739388045e-06}, {"id": 441, "seek": 281810, "start": 2818.1, "end": 2825.62, "text": " Because the slope of log close to zero is very, very close to infinity.", "tokens": [50364, 1436, 264, 13525, 295, 3565, 1998, 281, 4018, 307, 588, 11, 588, 1998, 281, 13202, 13, 50740, 50780, 583, 586, 291, 12972, 341, 538, 257, 2787, 41167, 300, 307, 25408, 13, 50936, 50936, 407, 309, 311, 17207, 538, 746, 300, 311, 588, 1998, 281, 4018, 13, 51040, 51040, 407, 294, 264, 917, 11, 291, 483, 257, 10585, 1230, 13, 51124, 51164, 583, 570, 264, 19376, 3547, 366, 1998, 281, 13202, 420, 4018, 11, 51348, 51348, 291, 12972, 746, 300, 311, 1998, 281, 1804, 13202, 538, 746, 300, 311, 1998, 281, 4018, 11, 51568, 51568, 291, 483, 29054, 2663, 13, 51644, 51644, 407, 291, 500, 380, 528, 281, 4994, 3565, 293, 2787, 41167, 13, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.14976074349167, "compression_ratio": 2.0219298245614037, "no_speech_prob": 1.0451115485921036e-05}, {"id": 442, "seek": 281810, "start": 2826.42, "end": 2829.54, "text": " But now you multiply this by a softmax that is saturated.", "tokens": [50364, 1436, 264, 13525, 295, 3565, 1998, 281, 4018, 307, 588, 11, 588, 1998, 281, 13202, 13, 50740, 50780, 583, 586, 291, 12972, 341, 538, 257, 2787, 41167, 300, 307, 25408, 13, 50936, 50936, 407, 309, 311, 17207, 538, 746, 300, 311, 588, 1998, 281, 4018, 13, 51040, 51040, 407, 294, 264, 917, 11, 291, 483, 257, 10585, 1230, 13, 51124, 51164, 583, 570, 264, 19376, 3547, 366, 1998, 281, 13202, 420, 4018, 11, 51348, 51348, 291, 12972, 746, 300, 311, 1998, 281, 1804, 13202, 538, 746, 300, 311, 1998, 281, 4018, 11, 51568, 51568, 291, 483, 29054, 2663, 13, 51644, 51644, 407, 291, 500, 380, 528, 281, 4994, 3565, 293, 2787, 41167, 13, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.14976074349167, "compression_ratio": 2.0219298245614037, "no_speech_prob": 1.0451115485921036e-05}, {"id": 443, "seek": 281810, "start": 2829.54, "end": 2831.62, "text": " So it's multiplied by something that's very close to zero.", "tokens": [50364, 1436, 264, 13525, 295, 3565, 1998, 281, 4018, 307, 588, 11, 588, 1998, 281, 13202, 13, 50740, 50780, 583, 586, 291, 12972, 341, 538, 257, 2787, 41167, 300, 307, 25408, 13, 50936, 50936, 407, 309, 311, 17207, 538, 746, 300, 311, 588, 1998, 281, 4018, 13, 51040, 51040, 407, 294, 264, 917, 11, 291, 483, 257, 10585, 1230, 13, 51124, 51164, 583, 570, 264, 19376, 3547, 366, 1998, 281, 13202, 420, 4018, 11, 51348, 51348, 291, 12972, 746, 300, 311, 1998, 281, 1804, 13202, 538, 746, 300, 311, 1998, 281, 4018, 11, 51568, 51568, 291, 483, 29054, 2663, 13, 51644, 51644, 407, 291, 500, 380, 528, 281, 4994, 3565, 293, 2787, 41167, 13, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.14976074349167, "compression_ratio": 2.0219298245614037, "no_speech_prob": 1.0451115485921036e-05}, {"id": 444, "seek": 281810, "start": 2831.62, "end": 2833.2999999999997, "text": " So in the end, you get a reasonable number.", "tokens": [50364, 1436, 264, 13525, 295, 3565, 1998, 281, 4018, 307, 588, 11, 588, 1998, 281, 13202, 13, 50740, 50780, 583, 586, 291, 12972, 341, 538, 257, 2787, 41167, 300, 307, 25408, 13, 50936, 50936, 407, 309, 311, 17207, 538, 746, 300, 311, 588, 1998, 281, 4018, 13, 51040, 51040, 407, 294, 264, 917, 11, 291, 483, 257, 10585, 1230, 13, 51124, 51164, 583, 570, 264, 19376, 3547, 366, 1998, 281, 13202, 420, 4018, 11, 51348, 51348, 291, 12972, 746, 300, 311, 1998, 281, 1804, 13202, 538, 746, 300, 311, 1998, 281, 4018, 11, 51568, 51568, 291, 483, 29054, 2663, 13, 51644, 51644, 407, 291, 500, 380, 528, 281, 4994, 3565, 293, 2787, 41167, 13, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.14976074349167, "compression_ratio": 2.0219298245614037, "no_speech_prob": 1.0451115485921036e-05}, {"id": 445, "seek": 281810, "start": 2834.1, "end": 2837.7799999999997, "text": " But because the intermediate numbers are close to infinity or zero,", "tokens": [50364, 1436, 264, 13525, 295, 3565, 1998, 281, 4018, 307, 588, 11, 588, 1998, 281, 13202, 13, 50740, 50780, 583, 586, 291, 12972, 341, 538, 257, 2787, 41167, 300, 307, 25408, 13, 50936, 50936, 407, 309, 311, 17207, 538, 746, 300, 311, 588, 1998, 281, 4018, 13, 51040, 51040, 407, 294, 264, 917, 11, 291, 483, 257, 10585, 1230, 13, 51124, 51164, 583, 570, 264, 19376, 3547, 366, 1998, 281, 13202, 420, 4018, 11, 51348, 51348, 291, 12972, 746, 300, 311, 1998, 281, 1804, 13202, 538, 746, 300, 311, 1998, 281, 4018, 11, 51568, 51568, 291, 483, 29054, 2663, 13, 51644, 51644, 407, 291, 500, 380, 528, 281, 4994, 3565, 293, 2787, 41167, 13, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.14976074349167, "compression_ratio": 2.0219298245614037, "no_speech_prob": 1.0451115485921036e-05}, {"id": 446, "seek": 281810, "start": 2837.7799999999997, "end": 2842.18, "text": " you multiply something that's close to plus infinity by something that's close to zero,", "tokens": [50364, 1436, 264, 13525, 295, 3565, 1998, 281, 4018, 307, 588, 11, 588, 1998, 281, 13202, 13, 50740, 50780, 583, 586, 291, 12972, 341, 538, 257, 2787, 41167, 300, 307, 25408, 13, 50936, 50936, 407, 309, 311, 17207, 538, 746, 300, 311, 588, 1998, 281, 4018, 13, 51040, 51040, 407, 294, 264, 917, 11, 291, 483, 257, 10585, 1230, 13, 51124, 51164, 583, 570, 264, 19376, 3547, 366, 1998, 281, 13202, 420, 4018, 11, 51348, 51348, 291, 12972, 746, 300, 311, 1998, 281, 1804, 13202, 538, 746, 300, 311, 1998, 281, 4018, 11, 51568, 51568, 291, 483, 29054, 2663, 13, 51644, 51644, 407, 291, 500, 380, 528, 281, 4994, 3565, 293, 2787, 41167, 13, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.14976074349167, "compression_ratio": 2.0219298245614037, "no_speech_prob": 1.0451115485921036e-05}, {"id": 447, "seek": 281810, "start": 2842.18, "end": 2843.7, "text": " you get numerical issues.", "tokens": [50364, 1436, 264, 13525, 295, 3565, 1998, 281, 4018, 307, 588, 11, 588, 1998, 281, 13202, 13, 50740, 50780, 583, 586, 291, 12972, 341, 538, 257, 2787, 41167, 300, 307, 25408, 13, 50936, 50936, 407, 309, 311, 17207, 538, 746, 300, 311, 588, 1998, 281, 4018, 13, 51040, 51040, 407, 294, 264, 917, 11, 291, 483, 257, 10585, 1230, 13, 51124, 51164, 583, 570, 264, 19376, 3547, 366, 1998, 281, 13202, 420, 4018, 11, 51348, 51348, 291, 12972, 746, 300, 311, 1998, 281, 1804, 13202, 538, 746, 300, 311, 1998, 281, 4018, 11, 51568, 51568, 291, 483, 29054, 2663, 13, 51644, 51644, 407, 291, 500, 380, 528, 281, 4994, 3565, 293, 2787, 41167, 13, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.14976074349167, "compression_ratio": 2.0219298245614037, "no_speech_prob": 1.0451115485921036e-05}, {"id": 448, "seek": 281810, "start": 2843.7, "end": 2845.7799999999997, "text": " So you don't want to separate log and softmax.", "tokens": [50364, 1436, 264, 13525, 295, 3565, 1998, 281, 4018, 307, 588, 11, 588, 1998, 281, 13202, 13, 50740, 50780, 583, 586, 291, 12972, 341, 538, 257, 2787, 41167, 300, 307, 25408, 13, 50936, 50936, 407, 309, 311, 17207, 538, 746, 300, 311, 588, 1998, 281, 4018, 13, 51040, 51040, 407, 294, 264, 917, 11, 291, 483, 257, 10585, 1230, 13, 51124, 51164, 583, 570, 264, 19376, 3547, 366, 1998, 281, 13202, 420, 4018, 11, 51348, 51348, 291, 12972, 746, 300, 311, 1998, 281, 1804, 13202, 538, 746, 300, 311, 1998, 281, 4018, 11, 51568, 51568, 291, 483, 29054, 2663, 13, 51644, 51644, 407, 291, 500, 380, 528, 281, 4994, 3565, 293, 2787, 41167, 13, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.14976074349167, "compression_ratio": 2.0219298245614037, "no_speech_prob": 1.0451115485921036e-05}, {"id": 449, "seek": 284578, "start": 2845.78, "end": 2848.5800000000004, "text": " You want to do log softmax in one go.", "tokens": [50364, 509, 528, 281, 360, 3565, 2787, 41167, 294, 472, 352, 13, 50504, 50504, 467, 6883, 11221, 264, 8513, 13, 50584, 50584, 467, 1669, 264, 1379, 551, 709, 544, 8351, 7866, 984, 13, 50728, 50832, 400, 337, 2531, 4112, 11, 291, 611, 528, 281, 22183, 3565, 2787, 41167, 51052, 51052, 293, 3671, 3565, 22119, 4470, 13, 51124, 51124, 407, 1936, 11, 498, 291, 362, 3565, 2787, 41167, 293, 3671, 3565, 22119, 4470, 11, 51296, 51296, 309, 1619, 11, 286, 658, 257, 3840, 295, 32807, 34499, 13, 51408, 51408, 286, 478, 516, 281, 1320, 552, 281, 264, 2787, 41167, 13, 51484, 51484, 286, 478, 516, 281, 747, 264, 3565, 295, 729, 13, 51592, 51592, 400, 550, 286, 528, 281, 652, 264, 5598, 295, 264, 3565, 2787, 41167, 337, 264, 3006, 1508, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.10590381764653903, "compression_ratio": 1.904382470119522, "no_speech_prob": 3.02392818412045e-05}, {"id": 450, "seek": 284578, "start": 2848.5800000000004, "end": 2850.1800000000003, "text": " It simplifies the formula.", "tokens": [50364, 509, 528, 281, 360, 3565, 2787, 41167, 294, 472, 352, 13, 50504, 50504, 467, 6883, 11221, 264, 8513, 13, 50584, 50584, 467, 1669, 264, 1379, 551, 709, 544, 8351, 7866, 984, 13, 50728, 50832, 400, 337, 2531, 4112, 11, 291, 611, 528, 281, 22183, 3565, 2787, 41167, 51052, 51052, 293, 3671, 3565, 22119, 4470, 13, 51124, 51124, 407, 1936, 11, 498, 291, 362, 3565, 2787, 41167, 293, 3671, 3565, 22119, 4470, 11, 51296, 51296, 309, 1619, 11, 286, 658, 257, 3840, 295, 32807, 34499, 13, 51408, 51408, 286, 478, 516, 281, 1320, 552, 281, 264, 2787, 41167, 13, 51484, 51484, 286, 478, 516, 281, 747, 264, 3565, 295, 729, 13, 51592, 51592, 400, 550, 286, 528, 281, 652, 264, 5598, 295, 264, 3565, 2787, 41167, 337, 264, 3006, 1508, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.10590381764653903, "compression_ratio": 1.904382470119522, "no_speech_prob": 3.02392818412045e-05}, {"id": 451, "seek": 284578, "start": 2850.1800000000003, "end": 2853.0600000000004, "text": " It makes the whole thing much more stable numerically.", "tokens": [50364, 509, 528, 281, 360, 3565, 2787, 41167, 294, 472, 352, 13, 50504, 50504, 467, 6883, 11221, 264, 8513, 13, 50584, 50584, 467, 1669, 264, 1379, 551, 709, 544, 8351, 7866, 984, 13, 50728, 50832, 400, 337, 2531, 4112, 11, 291, 611, 528, 281, 22183, 3565, 2787, 41167, 51052, 51052, 293, 3671, 3565, 22119, 4470, 13, 51124, 51124, 407, 1936, 11, 498, 291, 362, 3565, 2787, 41167, 293, 3671, 3565, 22119, 4470, 11, 51296, 51296, 309, 1619, 11, 286, 658, 257, 3840, 295, 32807, 34499, 13, 51408, 51408, 286, 478, 516, 281, 1320, 552, 281, 264, 2787, 41167, 13, 51484, 51484, 286, 478, 516, 281, 747, 264, 3565, 295, 729, 13, 51592, 51592, 400, 550, 286, 528, 281, 652, 264, 5598, 295, 264, 3565, 2787, 41167, 337, 264, 3006, 1508, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.10590381764653903, "compression_ratio": 1.904382470119522, "no_speech_prob": 3.02392818412045e-05}, {"id": 452, "seek": 284578, "start": 2855.1400000000003, "end": 2859.5400000000004, "text": " And for similar reasons, you also want to merge log softmax", "tokens": [50364, 509, 528, 281, 360, 3565, 2787, 41167, 294, 472, 352, 13, 50504, 50504, 467, 6883, 11221, 264, 8513, 13, 50584, 50584, 467, 1669, 264, 1379, 551, 709, 544, 8351, 7866, 984, 13, 50728, 50832, 400, 337, 2531, 4112, 11, 291, 611, 528, 281, 22183, 3565, 2787, 41167, 51052, 51052, 293, 3671, 3565, 22119, 4470, 13, 51124, 51124, 407, 1936, 11, 498, 291, 362, 3565, 2787, 41167, 293, 3671, 3565, 22119, 4470, 11, 51296, 51296, 309, 1619, 11, 286, 658, 257, 3840, 295, 32807, 34499, 13, 51408, 51408, 286, 478, 516, 281, 1320, 552, 281, 264, 2787, 41167, 13, 51484, 51484, 286, 478, 516, 281, 747, 264, 3565, 295, 729, 13, 51592, 51592, 400, 550, 286, 528, 281, 652, 264, 5598, 295, 264, 3565, 2787, 41167, 337, 264, 3006, 1508, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.10590381764653903, "compression_ratio": 1.904382470119522, "no_speech_prob": 3.02392818412045e-05}, {"id": 453, "seek": 284578, "start": 2859.5400000000004, "end": 2860.98, "text": " and negative log likelihood loss.", "tokens": [50364, 509, 528, 281, 360, 3565, 2787, 41167, 294, 472, 352, 13, 50504, 50504, 467, 6883, 11221, 264, 8513, 13, 50584, 50584, 467, 1669, 264, 1379, 551, 709, 544, 8351, 7866, 984, 13, 50728, 50832, 400, 337, 2531, 4112, 11, 291, 611, 528, 281, 22183, 3565, 2787, 41167, 51052, 51052, 293, 3671, 3565, 22119, 4470, 13, 51124, 51124, 407, 1936, 11, 498, 291, 362, 3565, 2787, 41167, 293, 3671, 3565, 22119, 4470, 11, 51296, 51296, 309, 1619, 11, 286, 658, 257, 3840, 295, 32807, 34499, 13, 51408, 51408, 286, 478, 516, 281, 1320, 552, 281, 264, 2787, 41167, 13, 51484, 51484, 286, 478, 516, 281, 747, 264, 3565, 295, 729, 13, 51592, 51592, 400, 550, 286, 528, 281, 652, 264, 5598, 295, 264, 3565, 2787, 41167, 337, 264, 3006, 1508, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.10590381764653903, "compression_ratio": 1.904382470119522, "no_speech_prob": 3.02392818412045e-05}, {"id": 454, "seek": 284578, "start": 2860.98, "end": 2864.42, "text": " So basically, if you have log softmax and negative log likelihood loss,", "tokens": [50364, 509, 528, 281, 360, 3565, 2787, 41167, 294, 472, 352, 13, 50504, 50504, 467, 6883, 11221, 264, 8513, 13, 50584, 50584, 467, 1669, 264, 1379, 551, 709, 544, 8351, 7866, 984, 13, 50728, 50832, 400, 337, 2531, 4112, 11, 291, 611, 528, 281, 22183, 3565, 2787, 41167, 51052, 51052, 293, 3671, 3565, 22119, 4470, 13, 51124, 51124, 407, 1936, 11, 498, 291, 362, 3565, 2787, 41167, 293, 3671, 3565, 22119, 4470, 11, 51296, 51296, 309, 1619, 11, 286, 658, 257, 3840, 295, 32807, 34499, 13, 51408, 51408, 286, 478, 516, 281, 1320, 552, 281, 264, 2787, 41167, 13, 51484, 51484, 286, 478, 516, 281, 747, 264, 3565, 295, 729, 13, 51592, 51592, 400, 550, 286, 528, 281, 652, 264, 5598, 295, 264, 3565, 2787, 41167, 337, 264, 3006, 1508, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.10590381764653903, "compression_ratio": 1.904382470119522, "no_speech_prob": 3.02392818412045e-05}, {"id": 455, "seek": 284578, "start": 2864.42, "end": 2866.6600000000003, "text": " it says, I got a bunch of weighted sums.", "tokens": [50364, 509, 528, 281, 360, 3565, 2787, 41167, 294, 472, 352, 13, 50504, 50504, 467, 6883, 11221, 264, 8513, 13, 50584, 50584, 467, 1669, 264, 1379, 551, 709, 544, 8351, 7866, 984, 13, 50728, 50832, 400, 337, 2531, 4112, 11, 291, 611, 528, 281, 22183, 3565, 2787, 41167, 51052, 51052, 293, 3671, 3565, 22119, 4470, 13, 51124, 51124, 407, 1936, 11, 498, 291, 362, 3565, 2787, 41167, 293, 3671, 3565, 22119, 4470, 11, 51296, 51296, 309, 1619, 11, 286, 658, 257, 3840, 295, 32807, 34499, 13, 51408, 51408, 286, 478, 516, 281, 1320, 552, 281, 264, 2787, 41167, 13, 51484, 51484, 286, 478, 516, 281, 747, 264, 3565, 295, 729, 13, 51592, 51592, 400, 550, 286, 528, 281, 652, 264, 5598, 295, 264, 3565, 2787, 41167, 337, 264, 3006, 1508, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.10590381764653903, "compression_ratio": 1.904382470119522, "no_speech_prob": 3.02392818412045e-05}, {"id": 456, "seek": 284578, "start": 2866.6600000000003, "end": 2868.1800000000003, "text": " I'm going to pass them to the softmax.", "tokens": [50364, 509, 528, 281, 360, 3565, 2787, 41167, 294, 472, 352, 13, 50504, 50504, 467, 6883, 11221, 264, 8513, 13, 50584, 50584, 467, 1669, 264, 1379, 551, 709, 544, 8351, 7866, 984, 13, 50728, 50832, 400, 337, 2531, 4112, 11, 291, 611, 528, 281, 22183, 3565, 2787, 41167, 51052, 51052, 293, 3671, 3565, 22119, 4470, 13, 51124, 51124, 407, 1936, 11, 498, 291, 362, 3565, 2787, 41167, 293, 3671, 3565, 22119, 4470, 11, 51296, 51296, 309, 1619, 11, 286, 658, 257, 3840, 295, 32807, 34499, 13, 51408, 51408, 286, 478, 516, 281, 1320, 552, 281, 264, 2787, 41167, 13, 51484, 51484, 286, 478, 516, 281, 747, 264, 3565, 295, 729, 13, 51592, 51592, 400, 550, 286, 528, 281, 652, 264, 5598, 295, 264, 3565, 2787, 41167, 337, 264, 3006, 1508, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.10590381764653903, "compression_ratio": 1.904382470119522, "no_speech_prob": 3.02392818412045e-05}, {"id": 457, "seek": 284578, "start": 2868.1800000000003, "end": 2870.34, "text": " I'm going to take the log of those.", "tokens": [50364, 509, 528, 281, 360, 3565, 2787, 41167, 294, 472, 352, 13, 50504, 50504, 467, 6883, 11221, 264, 8513, 13, 50584, 50584, 467, 1669, 264, 1379, 551, 709, 544, 8351, 7866, 984, 13, 50728, 50832, 400, 337, 2531, 4112, 11, 291, 611, 528, 281, 22183, 3565, 2787, 41167, 51052, 51052, 293, 3671, 3565, 22119, 4470, 13, 51124, 51124, 407, 1936, 11, 498, 291, 362, 3565, 2787, 41167, 293, 3671, 3565, 22119, 4470, 11, 51296, 51296, 309, 1619, 11, 286, 658, 257, 3840, 295, 32807, 34499, 13, 51408, 51408, 286, 478, 516, 281, 1320, 552, 281, 264, 2787, 41167, 13, 51484, 51484, 286, 478, 516, 281, 747, 264, 3565, 295, 729, 13, 51592, 51592, 400, 550, 286, 528, 281, 652, 264, 5598, 295, 264, 3565, 2787, 41167, 337, 264, 3006, 1508, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.10590381764653903, "compression_ratio": 1.904382470119522, "no_speech_prob": 3.02392818412045e-05}, {"id": 458, "seek": 284578, "start": 2870.34, "end": 2875.7000000000003, "text": " And then I want to make the output of the log softmax for the correct class.", "tokens": [50364, 509, 528, 281, 360, 3565, 2787, 41167, 294, 472, 352, 13, 50504, 50504, 467, 6883, 11221, 264, 8513, 13, 50584, 50584, 467, 1669, 264, 1379, 551, 709, 544, 8351, 7866, 984, 13, 50728, 50832, 400, 337, 2531, 4112, 11, 291, 611, 528, 281, 22183, 3565, 2787, 41167, 51052, 51052, 293, 3671, 3565, 22119, 4470, 13, 51124, 51124, 407, 1936, 11, 498, 291, 362, 3565, 2787, 41167, 293, 3671, 3565, 22119, 4470, 11, 51296, 51296, 309, 1619, 11, 286, 658, 257, 3840, 295, 32807, 34499, 13, 51408, 51408, 286, 478, 516, 281, 1320, 552, 281, 264, 2787, 41167, 13, 51484, 51484, 286, 478, 516, 281, 747, 264, 3565, 295, 729, 13, 51592, 51592, 400, 550, 286, 528, 281, 652, 264, 5598, 295, 264, 3565, 2787, 41167, 337, 264, 3006, 1508, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.10590381764653903, "compression_ratio": 1.904382470119522, "no_speech_prob": 3.02392818412045e-05}, {"id": 459, "seek": 287570, "start": 2875.7, "end": 2876.66, "text": " As large as possible.", "tokens": [50364, 1018, 2416, 382, 1944, 13, 50412, 50484, 663, 311, 437, 264, 3671, 3565, 22119, 4470, 775, 13, 50592, 50592, 467, 2738, 281, 652, 264, 6175, 295, 264, 3006, 1508, 382, 2416, 382, 1944, 13, 50724, 50772, 492, 1866, 300, 445, 257, 3456, 2057, 13, 50832, 50904, 1133, 291, 646, 48256, 807, 264, 3565, 2787, 41167, 11, 51056, 51056, 382, 257, 18326, 11, 309, 311, 516, 281, 652, 264, 6175, 295, 439, 264, 661, 5359, 51208, 51208, 382, 1359, 382, 1944, 570, 295, 264, 2710, 2144, 13, 51380, 51572, 400, 370, 300, 311, 983, 2171, 264, 1379, 1558, 295, 2390, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.10547518498689226, "compression_ratio": 1.702127659574468, "no_speech_prob": 2.586447590147145e-05}, {"id": 460, "seek": 287570, "start": 2878.1, "end": 2880.2599999999998, "text": " That's what the negative log likelihood loss does.", "tokens": [50364, 1018, 2416, 382, 1944, 13, 50412, 50484, 663, 311, 437, 264, 3671, 3565, 22119, 4470, 775, 13, 50592, 50592, 467, 2738, 281, 652, 264, 6175, 295, 264, 3006, 1508, 382, 2416, 382, 1944, 13, 50724, 50772, 492, 1866, 300, 445, 257, 3456, 2057, 13, 50832, 50904, 1133, 291, 646, 48256, 807, 264, 3565, 2787, 41167, 11, 51056, 51056, 382, 257, 18326, 11, 309, 311, 516, 281, 652, 264, 6175, 295, 439, 264, 661, 5359, 51208, 51208, 382, 1359, 382, 1944, 570, 295, 264, 2710, 2144, 13, 51380, 51572, 400, 370, 300, 311, 983, 2171, 264, 1379, 1558, 295, 2390, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.10547518498689226, "compression_ratio": 1.702127659574468, "no_speech_prob": 2.586447590147145e-05}, {"id": 461, "seek": 287570, "start": 2880.2599999999998, "end": 2882.8999999999996, "text": " It wants to make the score of the correct class as large as possible.", "tokens": [50364, 1018, 2416, 382, 1944, 13, 50412, 50484, 663, 311, 437, 264, 3671, 3565, 22119, 4470, 775, 13, 50592, 50592, 467, 2738, 281, 652, 264, 6175, 295, 264, 3006, 1508, 382, 2416, 382, 1944, 13, 50724, 50772, 492, 1866, 300, 445, 257, 3456, 2057, 13, 50832, 50904, 1133, 291, 646, 48256, 807, 264, 3565, 2787, 41167, 11, 51056, 51056, 382, 257, 18326, 11, 309, 311, 516, 281, 652, 264, 6175, 295, 439, 264, 661, 5359, 51208, 51208, 382, 1359, 382, 1944, 570, 295, 264, 2710, 2144, 13, 51380, 51572, 400, 370, 300, 311, 983, 2171, 264, 1379, 1558, 295, 2390, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.10547518498689226, "compression_ratio": 1.702127659574468, "no_speech_prob": 2.586447590147145e-05}, {"id": 462, "seek": 287570, "start": 2883.8599999999997, "end": 2885.06, "text": " We saw that just a minute ago.", "tokens": [50364, 1018, 2416, 382, 1944, 13, 50412, 50484, 663, 311, 437, 264, 3671, 3565, 22119, 4470, 775, 13, 50592, 50592, 467, 2738, 281, 652, 264, 6175, 295, 264, 3006, 1508, 382, 2416, 382, 1944, 13, 50724, 50772, 492, 1866, 300, 445, 257, 3456, 2057, 13, 50832, 50904, 1133, 291, 646, 48256, 807, 264, 3565, 2787, 41167, 11, 51056, 51056, 382, 257, 18326, 11, 309, 311, 516, 281, 652, 264, 6175, 295, 439, 264, 661, 5359, 51208, 51208, 382, 1359, 382, 1944, 570, 295, 264, 2710, 2144, 13, 51380, 51572, 400, 370, 300, 311, 983, 2171, 264, 1379, 1558, 295, 2390, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.10547518498689226, "compression_ratio": 1.702127659574468, "no_speech_prob": 2.586447590147145e-05}, {"id": 463, "seek": 287570, "start": 2886.5, "end": 2889.54, "text": " When you back propagate through the log softmax,", "tokens": [50364, 1018, 2416, 382, 1944, 13, 50412, 50484, 663, 311, 437, 264, 3671, 3565, 22119, 4470, 775, 13, 50592, 50592, 467, 2738, 281, 652, 264, 6175, 295, 264, 3006, 1508, 382, 2416, 382, 1944, 13, 50724, 50772, 492, 1866, 300, 445, 257, 3456, 2057, 13, 50832, 50904, 1133, 291, 646, 48256, 807, 264, 3565, 2787, 41167, 11, 51056, 51056, 382, 257, 18326, 11, 309, 311, 516, 281, 652, 264, 6175, 295, 439, 264, 661, 5359, 51208, 51208, 382, 1359, 382, 1944, 570, 295, 264, 2710, 2144, 13, 51380, 51572, 400, 370, 300, 311, 983, 2171, 264, 1379, 1558, 295, 2390, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.10547518498689226, "compression_ratio": 1.702127659574468, "no_speech_prob": 2.586447590147145e-05}, {"id": 464, "seek": 287570, "start": 2889.54, "end": 2892.58, "text": " as a consequence, it's going to make the score of all the other classes", "tokens": [50364, 1018, 2416, 382, 1944, 13, 50412, 50484, 663, 311, 437, 264, 3671, 3565, 22119, 4470, 775, 13, 50592, 50592, 467, 2738, 281, 652, 264, 6175, 295, 264, 3006, 1508, 382, 2416, 382, 1944, 13, 50724, 50772, 492, 1866, 300, 445, 257, 3456, 2057, 13, 50832, 50904, 1133, 291, 646, 48256, 807, 264, 3565, 2787, 41167, 11, 51056, 51056, 382, 257, 18326, 11, 309, 311, 516, 281, 652, 264, 6175, 295, 439, 264, 661, 5359, 51208, 51208, 382, 1359, 382, 1944, 570, 295, 264, 2710, 2144, 13, 51380, 51572, 400, 370, 300, 311, 983, 2171, 264, 1379, 1558, 295, 2390, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.10547518498689226, "compression_ratio": 1.702127659574468, "no_speech_prob": 2.586447590147145e-05}, {"id": 465, "seek": 287570, "start": 2892.58, "end": 2896.02, "text": " as small as possible because of the normalization.", "tokens": [50364, 1018, 2416, 382, 1944, 13, 50412, 50484, 663, 311, 437, 264, 3671, 3565, 22119, 4470, 775, 13, 50592, 50592, 467, 2738, 281, 652, 264, 6175, 295, 264, 3006, 1508, 382, 2416, 382, 1944, 13, 50724, 50772, 492, 1866, 300, 445, 257, 3456, 2057, 13, 50832, 50904, 1133, 291, 646, 48256, 807, 264, 3565, 2787, 41167, 11, 51056, 51056, 382, 257, 18326, 11, 309, 311, 516, 281, 652, 264, 6175, 295, 439, 264, 661, 5359, 51208, 51208, 382, 1359, 382, 1944, 570, 295, 264, 2710, 2144, 13, 51380, 51572, 400, 370, 300, 311, 983, 2171, 264, 1379, 1558, 295, 2390, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.10547518498689226, "compression_ratio": 1.702127659574468, "no_speech_prob": 2.586447590147145e-05}, {"id": 466, "seek": 287570, "start": 2899.8599999999997, "end": 2904.5, "text": " And so that's why sometimes the whole idea of building", "tokens": [50364, 1018, 2416, 382, 1944, 13, 50412, 50484, 663, 311, 437, 264, 3671, 3565, 22119, 4470, 775, 13, 50592, 50592, 467, 2738, 281, 652, 264, 6175, 295, 264, 3006, 1508, 382, 2416, 382, 1944, 13, 50724, 50772, 492, 1866, 300, 445, 257, 3456, 2057, 13, 50832, 50904, 1133, 291, 646, 48256, 807, 264, 3565, 2787, 41167, 11, 51056, 51056, 382, 257, 18326, 11, 309, 311, 516, 281, 652, 264, 6175, 295, 439, 264, 661, 5359, 51208, 51208, 382, 1359, 382, 1944, 570, 295, 264, 2710, 2144, 13, 51380, 51572, 400, 370, 300, 311, 983, 2171, 264, 1379, 1558, 295, 2390, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.10547518498689226, "compression_ratio": 1.702127659574468, "no_speech_prob": 2.586447590147145e-05}, {"id": 467, "seek": 290450, "start": 2904.5, "end": 2910.18, "text": " a network by modules, sometimes there is an advantage", "tokens": [50364, 257, 3209, 538, 16679, 11, 2171, 456, 307, 364, 5002, 50648, 50648, 2602, 295, 44559, 264, 16679, 666, 257, 2167, 472, 538, 1011, 13, 50804, 51196, 1779, 13, 51221, 51324, 407, 264, 3278, 30867, 4470, 11, 294, 1186, 11, 341, 13948, 257, 707, 857, 729, 51536, 51624, 29054, 6883, 7833, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.17215985384854404, "compression_ratio": 1.4266666666666667, "no_speech_prob": 2.467816557327751e-05}, {"id": 468, "seek": 290450, "start": 2910.18, "end": 2913.3, "text": " instead of merging the modules into a single one by hand.", "tokens": [50364, 257, 3209, 538, 16679, 11, 2171, 456, 307, 364, 5002, 50648, 50648, 2602, 295, 44559, 264, 16679, 666, 257, 2167, 472, 538, 1011, 13, 50804, 51196, 1779, 13, 51221, 51324, 407, 264, 3278, 30867, 4470, 11, 294, 1186, 11, 341, 13948, 257, 707, 857, 729, 51536, 51624, 29054, 6883, 7833, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.17215985384854404, "compression_ratio": 1.4266666666666667, "no_speech_prob": 2.467816557327751e-05}, {"id": 469, "seek": 290450, "start": 2921.14, "end": 2921.64, "text": " Right.", "tokens": [50364, 257, 3209, 538, 16679, 11, 2171, 456, 307, 364, 5002, 50648, 50648, 2602, 295, 44559, 264, 16679, 666, 257, 2167, 472, 538, 1011, 13, 50804, 51196, 1779, 13, 51221, 51324, 407, 264, 3278, 30867, 4470, 11, 294, 1186, 11, 341, 13948, 257, 707, 857, 729, 51536, 51624, 29054, 6883, 7833, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.17215985384854404, "compression_ratio": 1.4266666666666667, "no_speech_prob": 2.467816557327751e-05}, {"id": 470, "seek": 290450, "start": 2923.7, "end": 2927.94, "text": " So the cross entropy loss, in fact, this explains a little bit those", "tokens": [50364, 257, 3209, 538, 16679, 11, 2171, 456, 307, 364, 5002, 50648, 50648, 2602, 295, 44559, 264, 16679, 666, 257, 2167, 472, 538, 1011, 13, 50804, 51196, 1779, 13, 51221, 51324, 407, 264, 3278, 30867, 4470, 11, 294, 1186, 11, 341, 13948, 257, 707, 857, 729, 51536, 51624, 29054, 6883, 7833, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.17215985384854404, "compression_ratio": 1.4266666666666667, "no_speech_prob": 2.467816557327751e-05}, {"id": 471, "seek": 290450, "start": 2929.7, "end": 2931.3, "text": " numerical simplifications.", "tokens": [50364, 257, 3209, 538, 16679, 11, 2171, 456, 307, 364, 5002, 50648, 50648, 2602, 295, 44559, 264, 16679, 666, 257, 2167, 472, 538, 1011, 13, 50804, 51196, 1779, 13, 51221, 51324, 407, 264, 3278, 30867, 4470, 11, 294, 1186, 11, 341, 13948, 257, 707, 857, 729, 51536, 51624, 29054, 6883, 7833, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.17215985384854404, "compression_ratio": 1.4266666666666667, "no_speech_prob": 2.467816557327751e-05}, {"id": 472, "seek": 293130, "start": 2931.3, "end": 2939.7000000000003, "text": " So the loss takes an x vector and a desired category, a class,", "tokens": [50364, 407, 264, 4470, 2516, 364, 2031, 8062, 293, 257, 14721, 7719, 11, 257, 1508, 11, 50784, 50856, 293, 715, 1819, 264, 3671, 3565, 295, 264, 2787, 41167, 6456, 281, 264, 8062, 295, 13444, 13, 51084, 51120, 583, 264, 472, 300, 311, 322, 264, 30380, 510, 307, 264, 2031, 295, 264, 8186, 295, 264, 3006, 1508, 13, 51596, 51656, 407, 300, 311, 428, 4470, 13, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.12584804086124196, "compression_ratio": 1.5870967741935484, "no_speech_prob": 6.961757208046038e-06}, {"id": 473, "seek": 293130, "start": 2941.1400000000003, "end": 2945.7000000000003, "text": " and computes the negative log of the softmax applied to the vector of scores.", "tokens": [50364, 407, 264, 4470, 2516, 364, 2031, 8062, 293, 257, 14721, 7719, 11, 257, 1508, 11, 50784, 50856, 293, 715, 1819, 264, 3671, 3565, 295, 264, 2787, 41167, 6456, 281, 264, 8062, 295, 13444, 13, 51084, 51120, 583, 264, 472, 300, 311, 322, 264, 30380, 510, 307, 264, 2031, 295, 264, 8186, 295, 264, 3006, 1508, 13, 51596, 51656, 407, 300, 311, 428, 4470, 13, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.12584804086124196, "compression_ratio": 1.5870967741935484, "no_speech_prob": 6.961757208046038e-06}, {"id": 474, "seek": 293130, "start": 2946.42, "end": 2955.94, "text": " But the one that's on the numerator here is the x of the index of the correct class.", "tokens": [50364, 407, 264, 4470, 2516, 364, 2031, 8062, 293, 257, 14721, 7719, 11, 257, 1508, 11, 50784, 50856, 293, 715, 1819, 264, 3671, 3565, 295, 264, 2787, 41167, 6456, 281, 264, 8062, 295, 13444, 13, 51084, 51120, 583, 264, 472, 300, 311, 322, 264, 30380, 510, 307, 264, 2031, 295, 264, 8186, 295, 264, 3006, 1508, 13, 51596, 51656, 407, 300, 311, 428, 4470, 13, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.12584804086124196, "compression_ratio": 1.5870967741935484, "no_speech_prob": 6.961757208046038e-06}, {"id": 475, "seek": 293130, "start": 2957.1400000000003, "end": 2958.1800000000003, "text": " So that's your loss.", "tokens": [50364, 407, 264, 4470, 2516, 364, 2031, 8062, 293, 257, 14721, 7719, 11, 257, 1508, 11, 50784, 50856, 293, 715, 1819, 264, 3671, 3565, 295, 264, 2787, 41167, 6456, 281, 264, 8062, 295, 13444, 13, 51084, 51120, 583, 264, 472, 300, 311, 322, 264, 30380, 510, 307, 264, 2031, 295, 264, 8186, 295, 264, 3006, 1508, 13, 51596, 51656, 407, 300, 311, 428, 4470, 13, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.12584804086124196, "compression_ratio": 1.5870967741935484, "no_speech_prob": 6.961757208046038e-06}, {"id": 476, "seek": 295818, "start": 2958.18, "end": 2961.94, "text": " The negative log of exponential, the score of the correct class,", "tokens": [50364, 440, 3671, 3565, 295, 21510, 11, 264, 6175, 295, 264, 3006, 1508, 11, 50552, 50552, 6666, 538, 264, 2408, 295, 264, 21510, 82, 295, 439, 264, 13444, 13, 50664, 50792, 509, 393, 519, 295, 264, 2031, 311, 382, 3671, 25737, 13, 50892, 51012, 467, 311, 2584, 10344, 13, 51060, 51176, 823, 11, 562, 291, 360, 264, 5221, 293, 291, 20460, 11, 51424, 51452, 264, 3565, 293, 264, 21510, 82, 733, 295, 20460, 13, 51556, 51556, 400, 370, 291, 445, 483, 264, 6175, 295, 264, 3006, 1508, 11, 51664, 51664, 264, 3671, 6175, 295, 264, 3006, 1508, 13, 51736, 51792], "temperature": 0.0, "avg_logprob": -0.11613652257636042, "compression_ratio": 1.9242424242424243, "no_speech_prob": 8.800555406196509e-06}, {"id": 477, "seek": 295818, "start": 2961.94, "end": 2964.18, "text": " divided by the sum of the exponentials of all the scores.", "tokens": [50364, 440, 3671, 3565, 295, 21510, 11, 264, 6175, 295, 264, 3006, 1508, 11, 50552, 50552, 6666, 538, 264, 2408, 295, 264, 21510, 82, 295, 439, 264, 13444, 13, 50664, 50792, 509, 393, 519, 295, 264, 2031, 311, 382, 3671, 25737, 13, 50892, 51012, 467, 311, 2584, 10344, 13, 51060, 51176, 823, 11, 562, 291, 360, 264, 5221, 293, 291, 20460, 11, 51424, 51452, 264, 3565, 293, 264, 21510, 82, 733, 295, 20460, 13, 51556, 51556, 400, 370, 291, 445, 483, 264, 6175, 295, 264, 3006, 1508, 11, 51664, 51664, 264, 3671, 6175, 295, 264, 3006, 1508, 13, 51736, 51792], "temperature": 0.0, "avg_logprob": -0.11613652257636042, "compression_ratio": 1.9242424242424243, "no_speech_prob": 8.800555406196509e-06}, {"id": 478, "seek": 295818, "start": 2966.74, "end": 2968.74, "text": " You can think of the x's as negative energies.", "tokens": [50364, 440, 3671, 3565, 295, 21510, 11, 264, 6175, 295, 264, 3006, 1508, 11, 50552, 50552, 6666, 538, 264, 2408, 295, 264, 21510, 82, 295, 439, 264, 13444, 13, 50664, 50792, 509, 393, 519, 295, 264, 2031, 311, 382, 3671, 25737, 13, 50892, 51012, 467, 311, 2584, 10344, 13, 51060, 51176, 823, 11, 562, 291, 360, 264, 5221, 293, 291, 20460, 11, 51424, 51452, 264, 3565, 293, 264, 21510, 82, 733, 295, 20460, 13, 51556, 51556, 400, 370, 291, 445, 483, 264, 6175, 295, 264, 3006, 1508, 11, 51664, 51664, 264, 3671, 6175, 295, 264, 3006, 1508, 13, 51736, 51792], "temperature": 0.0, "avg_logprob": -0.11613652257636042, "compression_ratio": 1.9242424242424243, "no_speech_prob": 8.800555406196509e-06}, {"id": 479, "seek": 295818, "start": 2971.14, "end": 2972.1, "text": " It's completely equivalent.", "tokens": [50364, 440, 3671, 3565, 295, 21510, 11, 264, 6175, 295, 264, 3006, 1508, 11, 50552, 50552, 6666, 538, 264, 2408, 295, 264, 21510, 82, 295, 439, 264, 13444, 13, 50664, 50792, 509, 393, 519, 295, 264, 2031, 311, 382, 3671, 25737, 13, 50892, 51012, 467, 311, 2584, 10344, 13, 51060, 51176, 823, 11, 562, 291, 360, 264, 5221, 293, 291, 20460, 11, 51424, 51452, 264, 3565, 293, 264, 21510, 82, 733, 295, 20460, 13, 51556, 51556, 400, 370, 291, 445, 483, 264, 6175, 295, 264, 3006, 1508, 11, 51664, 51664, 264, 3671, 6175, 295, 264, 3006, 1508, 13, 51736, 51792], "temperature": 0.0, "avg_logprob": -0.11613652257636042, "compression_ratio": 1.9242424242424243, "no_speech_prob": 8.800555406196509e-06}, {"id": 480, "seek": 295818, "start": 2974.4199999999996, "end": 2979.3799999999997, "text": " Now, when you do the math and you simplify,", "tokens": [50364, 440, 3671, 3565, 295, 21510, 11, 264, 6175, 295, 264, 3006, 1508, 11, 50552, 50552, 6666, 538, 264, 2408, 295, 264, 21510, 82, 295, 439, 264, 13444, 13, 50664, 50792, 509, 393, 519, 295, 264, 2031, 311, 382, 3671, 25737, 13, 50892, 51012, 467, 311, 2584, 10344, 13, 51060, 51176, 823, 11, 562, 291, 360, 264, 5221, 293, 291, 20460, 11, 51424, 51452, 264, 3565, 293, 264, 21510, 82, 733, 295, 20460, 13, 51556, 51556, 400, 370, 291, 445, 483, 264, 6175, 295, 264, 3006, 1508, 11, 51664, 51664, 264, 3671, 6175, 295, 264, 3006, 1508, 13, 51736, 51792], "temperature": 0.0, "avg_logprob": -0.11613652257636042, "compression_ratio": 1.9242424242424243, "no_speech_prob": 8.800555406196509e-06}, {"id": 481, "seek": 295818, "start": 2979.94, "end": 2982.02, "text": " the log and the exponentials kind of simplify.", "tokens": [50364, 440, 3671, 3565, 295, 21510, 11, 264, 6175, 295, 264, 3006, 1508, 11, 50552, 50552, 6666, 538, 264, 2408, 295, 264, 21510, 82, 295, 439, 264, 13444, 13, 50664, 50792, 509, 393, 519, 295, 264, 2031, 311, 382, 3671, 25737, 13, 50892, 51012, 467, 311, 2584, 10344, 13, 51060, 51176, 823, 11, 562, 291, 360, 264, 5221, 293, 291, 20460, 11, 51424, 51452, 264, 3565, 293, 264, 21510, 82, 733, 295, 20460, 13, 51556, 51556, 400, 370, 291, 445, 483, 264, 6175, 295, 264, 3006, 1508, 11, 51664, 51664, 264, 3671, 6175, 295, 264, 3006, 1508, 13, 51736, 51792], "temperature": 0.0, "avg_logprob": -0.11613652257636042, "compression_ratio": 1.9242424242424243, "no_speech_prob": 8.800555406196509e-06}, {"id": 482, "seek": 295818, "start": 2982.02, "end": 2984.18, "text": " And so you just get the score of the correct class,", "tokens": [50364, 440, 3671, 3565, 295, 21510, 11, 264, 6175, 295, 264, 3006, 1508, 11, 50552, 50552, 6666, 538, 264, 2408, 295, 264, 21510, 82, 295, 439, 264, 13444, 13, 50664, 50792, 509, 393, 519, 295, 264, 2031, 311, 382, 3671, 25737, 13, 50892, 51012, 467, 311, 2584, 10344, 13, 51060, 51176, 823, 11, 562, 291, 360, 264, 5221, 293, 291, 20460, 11, 51424, 51452, 264, 3565, 293, 264, 21510, 82, 733, 295, 20460, 13, 51556, 51556, 400, 370, 291, 445, 483, 264, 6175, 295, 264, 3006, 1508, 11, 51664, 51664, 264, 3671, 6175, 295, 264, 3006, 1508, 13, 51736, 51792], "temperature": 0.0, "avg_logprob": -0.11613652257636042, "compression_ratio": 1.9242424242424243, "no_speech_prob": 8.800555406196509e-06}, {"id": 483, "seek": 295818, "start": 2984.18, "end": 2985.62, "text": " the negative score of the correct class.", "tokens": [50364, 440, 3671, 3565, 295, 21510, 11, 264, 6175, 295, 264, 3006, 1508, 11, 50552, 50552, 6666, 538, 264, 2408, 295, 264, 21510, 82, 295, 439, 264, 13444, 13, 50664, 50792, 509, 393, 519, 295, 264, 2031, 311, 382, 3671, 25737, 13, 50892, 51012, 467, 311, 2584, 10344, 13, 51060, 51176, 823, 11, 562, 291, 360, 264, 5221, 293, 291, 20460, 11, 51424, 51452, 264, 3565, 293, 264, 21510, 82, 733, 295, 20460, 13, 51556, 51556, 400, 370, 291, 445, 483, 264, 6175, 295, 264, 3006, 1508, 11, 51664, 51664, 264, 3671, 6175, 295, 264, 3006, 1508, 13, 51736, 51792], "temperature": 0.0, "avg_logprob": -0.11613652257636042, "compression_ratio": 1.9242424242424243, "no_speech_prob": 8.800555406196509e-06}, {"id": 484, "seek": 298562, "start": 2985.62, "end": 2987.8599999999997, "text": " So to make that small, you make the score large.", "tokens": [50364, 407, 281, 652, 300, 1359, 11, 291, 652, 264, 6175, 2416, 13, 50476, 50520, 400, 550, 1804, 264, 3565, 295, 264, 2408, 295, 264, 21510, 82, 295, 264, 13444, 295, 439, 264, 661, 50704, 50704, 1508, 281, 652, 300, 1359, 11, 291, 652, 439, 264, 2031, 73, 311, 1359, 11, 3671, 11, 382, 1400, 382, 11, 51064, 51064, 291, 458, 11, 382, 3671, 382, 1944, 13, 51164, 51216, 1033, 13, 51241, 51241, 407, 341, 486, 652, 264, 6175, 295, 264, 3006, 1508, 2416, 11, 652, 264, 6175, 295, 1203, 1646, 1359, 13, 51416, 51600, 3764, 11, 411, 294, 264, 426, 24010, 11, 291, 393, 362, 257, 3364, 680, 7719, 13, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.29976787901761237, "compression_ratio": 1.8883720930232557, "no_speech_prob": 1.2411255738697946e-05}, {"id": 485, "seek": 298562, "start": 2988.74, "end": 2992.42, "text": " And then plus the log of the sum of the exponentials of the scores of all the other", "tokens": [50364, 407, 281, 652, 300, 1359, 11, 291, 652, 264, 6175, 2416, 13, 50476, 50520, 400, 550, 1804, 264, 3565, 295, 264, 2408, 295, 264, 21510, 82, 295, 264, 13444, 295, 439, 264, 661, 50704, 50704, 1508, 281, 652, 300, 1359, 11, 291, 652, 439, 264, 2031, 73, 311, 1359, 11, 3671, 11, 382, 1400, 382, 11, 51064, 51064, 291, 458, 11, 382, 3671, 382, 1944, 13, 51164, 51216, 1033, 13, 51241, 51241, 407, 341, 486, 652, 264, 6175, 295, 264, 3006, 1508, 2416, 11, 652, 264, 6175, 295, 1203, 1646, 1359, 13, 51416, 51600, 3764, 11, 411, 294, 264, 426, 24010, 11, 291, 393, 362, 257, 3364, 680, 7719, 13, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.29976787901761237, "compression_ratio": 1.8883720930232557, "no_speech_prob": 1.2411255738697946e-05}, {"id": 486, "seek": 298562, "start": 2992.42, "end": 2999.62, "text": " class to make that small, you make all the xj's small, negative, as far as,", "tokens": [50364, 407, 281, 652, 300, 1359, 11, 291, 652, 264, 6175, 2416, 13, 50476, 50520, 400, 550, 1804, 264, 3565, 295, 264, 2408, 295, 264, 21510, 82, 295, 264, 13444, 295, 439, 264, 661, 50704, 50704, 1508, 281, 652, 300, 1359, 11, 291, 652, 439, 264, 2031, 73, 311, 1359, 11, 3671, 11, 382, 1400, 382, 11, 51064, 51064, 291, 458, 11, 382, 3671, 382, 1944, 13, 51164, 51216, 1033, 13, 51241, 51241, 407, 341, 486, 652, 264, 6175, 295, 264, 3006, 1508, 2416, 11, 652, 264, 6175, 295, 1203, 1646, 1359, 13, 51416, 51600, 3764, 11, 411, 294, 264, 426, 24010, 11, 291, 393, 362, 257, 3364, 680, 7719, 13, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.29976787901761237, "compression_ratio": 1.8883720930232557, "no_speech_prob": 1.2411255738697946e-05}, {"id": 487, "seek": 298562, "start": 2999.62, "end": 3001.62, "text": " you know, as negative as possible.", "tokens": [50364, 407, 281, 652, 300, 1359, 11, 291, 652, 264, 6175, 2416, 13, 50476, 50520, 400, 550, 1804, 264, 3565, 295, 264, 2408, 295, 264, 21510, 82, 295, 264, 13444, 295, 439, 264, 661, 50704, 50704, 1508, 281, 652, 300, 1359, 11, 291, 652, 439, 264, 2031, 73, 311, 1359, 11, 3671, 11, 382, 1400, 382, 11, 51064, 51064, 291, 458, 11, 382, 3671, 382, 1944, 13, 51164, 51216, 1033, 13, 51241, 51241, 407, 341, 486, 652, 264, 6175, 295, 264, 3006, 1508, 2416, 11, 652, 264, 6175, 295, 1203, 1646, 1359, 13, 51416, 51600, 3764, 11, 411, 294, 264, 426, 24010, 11, 291, 393, 362, 257, 3364, 680, 7719, 13, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.29976787901761237, "compression_ratio": 1.8883720930232557, "no_speech_prob": 1.2411255738697946e-05}, {"id": 488, "seek": 298562, "start": 3002.66, "end": 3003.16, "text": " Okay.", "tokens": [50364, 407, 281, 652, 300, 1359, 11, 291, 652, 264, 6175, 2416, 13, 50476, 50520, 400, 550, 1804, 264, 3565, 295, 264, 2408, 295, 264, 21510, 82, 295, 264, 13444, 295, 439, 264, 661, 50704, 50704, 1508, 281, 652, 300, 1359, 11, 291, 652, 439, 264, 2031, 73, 311, 1359, 11, 3671, 11, 382, 1400, 382, 11, 51064, 51064, 291, 458, 11, 382, 3671, 382, 1944, 13, 51164, 51216, 1033, 13, 51241, 51241, 407, 341, 486, 652, 264, 6175, 295, 264, 3006, 1508, 2416, 11, 652, 264, 6175, 295, 1203, 1646, 1359, 13, 51416, 51600, 3764, 11, 411, 294, 264, 426, 24010, 11, 291, 393, 362, 257, 3364, 680, 7719, 13, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.29976787901761237, "compression_ratio": 1.8883720930232557, "no_speech_prob": 1.2411255738697946e-05}, {"id": 489, "seek": 298562, "start": 3003.16, "end": 3006.66, "text": " So this will make the score of the correct class large, make the score of everything else small.", "tokens": [50364, 407, 281, 652, 300, 1359, 11, 291, 652, 264, 6175, 2416, 13, 50476, 50520, 400, 550, 1804, 264, 3565, 295, 264, 2408, 295, 264, 21510, 82, 295, 264, 13444, 295, 439, 264, 661, 50704, 50704, 1508, 281, 652, 300, 1359, 11, 291, 652, 439, 264, 2031, 73, 311, 1359, 11, 3671, 11, 382, 1400, 382, 11, 51064, 51064, 291, 458, 11, 382, 3671, 382, 1944, 13, 51164, 51216, 1033, 13, 51241, 51241, 407, 341, 486, 652, 264, 6175, 295, 264, 3006, 1508, 2416, 11, 652, 264, 6175, 295, 1203, 1646, 1359, 13, 51416, 51600, 3764, 11, 411, 294, 264, 426, 24010, 11, 291, 393, 362, 257, 3364, 680, 7719, 13, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.29976787901761237, "compression_ratio": 1.8883720930232557, "no_speech_prob": 1.2411255738697946e-05}, {"id": 490, "seek": 298562, "start": 3010.3399999999997, "end": 3014.3399999999997, "text": " Again, like in the NLL, you can have a weight per category.", "tokens": [50364, 407, 281, 652, 300, 1359, 11, 291, 652, 264, 6175, 2416, 13, 50476, 50520, 400, 550, 1804, 264, 3565, 295, 264, 2408, 295, 264, 21510, 82, 295, 264, 13444, 295, 439, 264, 661, 50704, 50704, 1508, 281, 652, 300, 1359, 11, 291, 652, 439, 264, 2031, 73, 311, 1359, 11, 3671, 11, 382, 1400, 382, 11, 51064, 51064, 291, 458, 11, 382, 3671, 382, 1944, 13, 51164, 51216, 1033, 13, 51241, 51241, 407, 341, 486, 652, 264, 6175, 295, 264, 3006, 1508, 2416, 11, 652, 264, 6175, 295, 1203, 1646, 1359, 13, 51416, 51600, 3764, 11, 411, 294, 264, 426, 24010, 11, 291, 393, 362, 257, 3364, 680, 7719, 13, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.29976787901761237, "compression_ratio": 1.8883720930232557, "no_speech_prob": 1.2411255738697946e-05}, {"id": 491, "seek": 301434, "start": 3014.34, "end": 3018.6600000000003, "text": " Also, there is a physical interpretation of the cross entropy.", "tokens": [50364, 2743, 11, 456, 307, 257, 4001, 14174, 295, 264, 3278, 30867, 13, 50580, 50680, 1779, 13, 50705, 50705, 1033, 13, 50730, 50730, 407, 983, 307, 309, 1219, 3278, 30867, 30, 50832, 50832, 1436, 309, 307, 264, 3278, 30867, 1296, 732, 37870, 13, 50980, 50980, 467, 311, 264, 47991, 47387, 534, 1296, 732, 37870, 13, 51100, 51160, 467, 1177, 380, 4204, 4448, 510, 294, 341, 8513, 11, 51280, 51280, 457, 519, 295, 264, 2787, 41167, 6456, 281, 264, 2031, 8062, 382, 257, 7316, 13, 51540, 51540, 1033, 13, 51565, 51565, 407, 321, 603, 747, 264, 2031, 6771, 11, 264, 13444, 11, 2831, 813, 281, 257, 2787, 41167, 11, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.26700172767982827, "compression_ratio": 1.7735042735042734, "no_speech_prob": 9.665517609391827e-06}, {"id": 492, "seek": 301434, "start": 3020.6600000000003, "end": 3021.1600000000003, "text": " Right.", "tokens": [50364, 2743, 11, 456, 307, 257, 4001, 14174, 295, 264, 3278, 30867, 13, 50580, 50680, 1779, 13, 50705, 50705, 1033, 13, 50730, 50730, 407, 983, 307, 309, 1219, 3278, 30867, 30, 50832, 50832, 1436, 309, 307, 264, 3278, 30867, 1296, 732, 37870, 13, 50980, 50980, 467, 311, 264, 47991, 47387, 534, 1296, 732, 37870, 13, 51100, 51160, 467, 1177, 380, 4204, 4448, 510, 294, 341, 8513, 11, 51280, 51280, 457, 519, 295, 264, 2787, 41167, 6456, 281, 264, 2031, 8062, 382, 257, 7316, 13, 51540, 51540, 1033, 13, 51565, 51565, 407, 321, 603, 747, 264, 2031, 6771, 11, 264, 13444, 11, 2831, 813, 281, 257, 2787, 41167, 11, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.26700172767982827, "compression_ratio": 1.7735042735042734, "no_speech_prob": 9.665517609391827e-06}, {"id": 493, "seek": 301434, "start": 3021.1600000000003, "end": 3021.6600000000003, "text": " Okay.", "tokens": [50364, 2743, 11, 456, 307, 257, 4001, 14174, 295, 264, 3278, 30867, 13, 50580, 50680, 1779, 13, 50705, 50705, 1033, 13, 50730, 50730, 407, 983, 307, 309, 1219, 3278, 30867, 30, 50832, 50832, 1436, 309, 307, 264, 3278, 30867, 1296, 732, 37870, 13, 50980, 50980, 467, 311, 264, 47991, 47387, 534, 1296, 732, 37870, 13, 51100, 51160, 467, 1177, 380, 4204, 4448, 510, 294, 341, 8513, 11, 51280, 51280, 457, 519, 295, 264, 2787, 41167, 6456, 281, 264, 2031, 8062, 382, 257, 7316, 13, 51540, 51540, 1033, 13, 51565, 51565, 407, 321, 603, 747, 264, 2031, 6771, 11, 264, 13444, 11, 2831, 813, 281, 257, 2787, 41167, 11, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.26700172767982827, "compression_ratio": 1.7735042735042734, "no_speech_prob": 9.665517609391827e-06}, {"id": 494, "seek": 301434, "start": 3021.6600000000003, "end": 3023.7000000000003, "text": " So why is it called cross entropy?", "tokens": [50364, 2743, 11, 456, 307, 257, 4001, 14174, 295, 264, 3278, 30867, 13, 50580, 50680, 1779, 13, 50705, 50705, 1033, 13, 50730, 50730, 407, 983, 307, 309, 1219, 3278, 30867, 30, 50832, 50832, 1436, 309, 307, 264, 3278, 30867, 1296, 732, 37870, 13, 50980, 50980, 467, 311, 264, 47991, 47387, 534, 1296, 732, 37870, 13, 51100, 51160, 467, 1177, 380, 4204, 4448, 510, 294, 341, 8513, 11, 51280, 51280, 457, 519, 295, 264, 2787, 41167, 6456, 281, 264, 2031, 8062, 382, 257, 7316, 13, 51540, 51540, 1033, 13, 51565, 51565, 407, 321, 603, 747, 264, 2031, 6771, 11, 264, 13444, 11, 2831, 813, 281, 257, 2787, 41167, 11, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.26700172767982827, "compression_ratio": 1.7735042735042734, "no_speech_prob": 9.665517609391827e-06}, {"id": 495, "seek": 301434, "start": 3023.7000000000003, "end": 3026.6600000000003, "text": " Because it is the cross entropy between two distributions.", "tokens": [50364, 2743, 11, 456, 307, 257, 4001, 14174, 295, 264, 3278, 30867, 13, 50580, 50680, 1779, 13, 50705, 50705, 1033, 13, 50730, 50730, 407, 983, 307, 309, 1219, 3278, 30867, 30, 50832, 50832, 1436, 309, 307, 264, 3278, 30867, 1296, 732, 37870, 13, 50980, 50980, 467, 311, 264, 47991, 47387, 534, 1296, 732, 37870, 13, 51100, 51160, 467, 1177, 380, 4204, 4448, 510, 294, 341, 8513, 11, 51280, 51280, 457, 519, 295, 264, 2787, 41167, 6456, 281, 264, 2031, 8062, 382, 257, 7316, 13, 51540, 51540, 1033, 13, 51565, 51565, 407, 321, 603, 747, 264, 2031, 6771, 11, 264, 13444, 11, 2831, 813, 281, 257, 2787, 41167, 11, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.26700172767982827, "compression_ratio": 1.7735042735042734, "no_speech_prob": 9.665517609391827e-06}, {"id": 496, "seek": 301434, "start": 3026.6600000000003, "end": 3029.06, "text": " It's the KL divergence really between two distributions.", "tokens": [50364, 2743, 11, 456, 307, 257, 4001, 14174, 295, 264, 3278, 30867, 13, 50580, 50680, 1779, 13, 50705, 50705, 1033, 13, 50730, 50730, 407, 983, 307, 309, 1219, 3278, 30867, 30, 50832, 50832, 1436, 309, 307, 264, 3278, 30867, 1296, 732, 37870, 13, 50980, 50980, 467, 311, 264, 47991, 47387, 534, 1296, 732, 37870, 13, 51100, 51160, 467, 1177, 380, 4204, 4448, 510, 294, 341, 8513, 11, 51280, 51280, 457, 519, 295, 264, 2787, 41167, 6456, 281, 264, 2031, 8062, 382, 257, 7316, 13, 51540, 51540, 1033, 13, 51565, 51565, 407, 321, 603, 747, 264, 2031, 6771, 11, 264, 13444, 11, 2831, 813, 281, 257, 2787, 41167, 11, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.26700172767982827, "compression_ratio": 1.7735042735042734, "no_speech_prob": 9.665517609391827e-06}, {"id": 497, "seek": 301434, "start": 3030.26, "end": 3032.6600000000003, "text": " It doesn't appear clearly here in this formula,", "tokens": [50364, 2743, 11, 456, 307, 257, 4001, 14174, 295, 264, 3278, 30867, 13, 50580, 50680, 1779, 13, 50705, 50705, 1033, 13, 50730, 50730, 407, 983, 307, 309, 1219, 3278, 30867, 30, 50832, 50832, 1436, 309, 307, 264, 3278, 30867, 1296, 732, 37870, 13, 50980, 50980, 467, 311, 264, 47991, 47387, 534, 1296, 732, 37870, 13, 51100, 51160, 467, 1177, 380, 4204, 4448, 510, 294, 341, 8513, 11, 51280, 51280, 457, 519, 295, 264, 2787, 41167, 6456, 281, 264, 2031, 8062, 382, 257, 7316, 13, 51540, 51540, 1033, 13, 51565, 51565, 407, 321, 603, 747, 264, 2031, 6771, 11, 264, 13444, 11, 2831, 813, 281, 257, 2787, 41167, 11, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.26700172767982827, "compression_ratio": 1.7735042735042734, "no_speech_prob": 9.665517609391827e-06}, {"id": 498, "seek": 301434, "start": 3032.6600000000003, "end": 3037.86, "text": " but think of the softmax applied to the x vector as a distribution.", "tokens": [50364, 2743, 11, 456, 307, 257, 4001, 14174, 295, 264, 3278, 30867, 13, 50580, 50680, 1779, 13, 50705, 50705, 1033, 13, 50730, 50730, 407, 983, 307, 309, 1219, 3278, 30867, 30, 50832, 50832, 1436, 309, 307, 264, 3278, 30867, 1296, 732, 37870, 13, 50980, 50980, 467, 311, 264, 47991, 47387, 534, 1296, 732, 37870, 13, 51100, 51160, 467, 1177, 380, 4204, 4448, 510, 294, 341, 8513, 11, 51280, 51280, 457, 519, 295, 264, 2787, 41167, 6456, 281, 264, 2031, 8062, 382, 257, 7316, 13, 51540, 51540, 1033, 13, 51565, 51565, 407, 321, 603, 747, 264, 2031, 6771, 11, 264, 13444, 11, 2831, 813, 281, 257, 2787, 41167, 11, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.26700172767982827, "compression_ratio": 1.7735042735042734, "no_speech_prob": 9.665517609391827e-06}, {"id": 499, "seek": 301434, "start": 3037.86, "end": 3038.36, "text": " Okay.", "tokens": [50364, 2743, 11, 456, 307, 257, 4001, 14174, 295, 264, 3278, 30867, 13, 50580, 50680, 1779, 13, 50705, 50705, 1033, 13, 50730, 50730, 407, 983, 307, 309, 1219, 3278, 30867, 30, 50832, 50832, 1436, 309, 307, 264, 3278, 30867, 1296, 732, 37870, 13, 50980, 50980, 467, 311, 264, 47991, 47387, 534, 1296, 732, 37870, 13, 51100, 51160, 467, 1177, 380, 4204, 4448, 510, 294, 341, 8513, 11, 51280, 51280, 457, 519, 295, 264, 2787, 41167, 6456, 281, 264, 2031, 8062, 382, 257, 7316, 13, 51540, 51540, 1033, 13, 51565, 51565, 407, 321, 603, 747, 264, 2031, 6771, 11, 264, 13444, 11, 2831, 813, 281, 257, 2787, 41167, 11, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.26700172767982827, "compression_ratio": 1.7735042735042734, "no_speech_prob": 9.665517609391827e-06}, {"id": 500, "seek": 301434, "start": 3038.36, "end": 3041.78, "text": " So we'll take the x factors, the scores, rather than to a softmax,", "tokens": [50364, 2743, 11, 456, 307, 257, 4001, 14174, 295, 264, 3278, 30867, 13, 50580, 50680, 1779, 13, 50705, 50705, 1033, 13, 50730, 50730, 407, 983, 307, 309, 1219, 3278, 30867, 30, 50832, 50832, 1436, 309, 307, 264, 3278, 30867, 1296, 732, 37870, 13, 50980, 50980, 467, 311, 264, 47991, 47387, 534, 1296, 732, 37870, 13, 51100, 51160, 467, 1177, 380, 4204, 4448, 510, 294, 341, 8513, 11, 51280, 51280, 457, 519, 295, 264, 2787, 41167, 6456, 281, 264, 2031, 8062, 382, 257, 7316, 13, 51540, 51540, 1033, 13, 51565, 51565, 407, 321, 603, 747, 264, 2031, 6771, 11, 264, 13444, 11, 2831, 813, 281, 257, 2787, 41167, 11, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.26700172767982827, "compression_ratio": 1.7735042735042734, "no_speech_prob": 9.665517609391827e-06}, {"id": 501, "seek": 304178, "start": 3041.78, "end": 3045.38, "text": " you get a bunch of numbers between 0 and 1 that's onto 1.", "tokens": [50364, 291, 483, 257, 3840, 295, 3547, 1296, 1958, 293, 502, 300, 311, 3911, 502, 13, 50544, 50636, 400, 586, 291, 362, 257, 14721, 7316, 293, 264, 14721, 7316, 11, 50804, 50848, 264, 3779, 7316, 11, 498, 291, 528, 11, 307, 472, 294, 597, 439, 264, 2085, 10479, 362, 1958, 51104, 51104, 293, 264, 3006, 7719, 575, 502, 13, 51172, 51200, 1033, 13, 51225, 51244, 823, 14722, 264, 47991, 47387, 1296, 729, 732, 37870, 13, 51396, 51436, 1033, 13, 51461, 51461], "temperature": 0.0, "avg_logprob": -0.20752832366199028, "compression_ratio": 1.702127659574468, "no_speech_prob": 8.186199806914374e-07}, {"id": 502, "seek": 304178, "start": 3047.2200000000003, "end": 3050.5800000000004, "text": " And now you have a desired distribution and the desired distribution,", "tokens": [50364, 291, 483, 257, 3840, 295, 3547, 1296, 1958, 293, 502, 300, 311, 3911, 502, 13, 50544, 50636, 400, 586, 291, 362, 257, 14721, 7316, 293, 264, 14721, 7316, 11, 50804, 50848, 264, 3779, 7316, 11, 498, 291, 528, 11, 307, 472, 294, 597, 439, 264, 2085, 10479, 362, 1958, 51104, 51104, 293, 264, 3006, 7719, 575, 502, 13, 51172, 51200, 1033, 13, 51225, 51244, 823, 14722, 264, 47991, 47387, 1296, 729, 732, 37870, 13, 51396, 51436, 1033, 13, 51461, 51461], "temperature": 0.0, "avg_logprob": -0.20752832366199028, "compression_ratio": 1.702127659574468, "no_speech_prob": 8.186199806914374e-07}, {"id": 503, "seek": 304178, "start": 3051.46, "end": 3056.5800000000004, "text": " the target distribution, if you want, is one in which all the wrong categories have 0", "tokens": [50364, 291, 483, 257, 3840, 295, 3547, 1296, 1958, 293, 502, 300, 311, 3911, 502, 13, 50544, 50636, 400, 586, 291, 362, 257, 14721, 7316, 293, 264, 14721, 7316, 11, 50804, 50848, 264, 3779, 7316, 11, 498, 291, 528, 11, 307, 472, 294, 597, 439, 264, 2085, 10479, 362, 1958, 51104, 51104, 293, 264, 3006, 7719, 575, 502, 13, 51172, 51200, 1033, 13, 51225, 51244, 823, 14722, 264, 47991, 47387, 1296, 729, 732, 37870, 13, 51396, 51436, 1033, 13, 51461, 51461], "temperature": 0.0, "avg_logprob": -0.20752832366199028, "compression_ratio": 1.702127659574468, "no_speech_prob": 8.186199806914374e-07}, {"id": 504, "seek": 304178, "start": 3056.5800000000004, "end": 3057.94, "text": " and the correct category has 1.", "tokens": [50364, 291, 483, 257, 3840, 295, 3547, 1296, 1958, 293, 502, 300, 311, 3911, 502, 13, 50544, 50636, 400, 586, 291, 362, 257, 14721, 7316, 293, 264, 14721, 7316, 11, 50804, 50848, 264, 3779, 7316, 11, 498, 291, 528, 11, 307, 472, 294, 597, 439, 264, 2085, 10479, 362, 1958, 51104, 51104, 293, 264, 3006, 7719, 575, 502, 13, 51172, 51200, 1033, 13, 51225, 51244, 823, 14722, 264, 47991, 47387, 1296, 729, 732, 37870, 13, 51396, 51436, 1033, 13, 51461, 51461], "temperature": 0.0, "avg_logprob": -0.20752832366199028, "compression_ratio": 1.702127659574468, "no_speech_prob": 8.186199806914374e-07}, {"id": 505, "seek": 304178, "start": 3058.5, "end": 3059.0, "text": " Okay.", "tokens": [50364, 291, 483, 257, 3840, 295, 3547, 1296, 1958, 293, 502, 300, 311, 3911, 502, 13, 50544, 50636, 400, 586, 291, 362, 257, 14721, 7316, 293, 264, 14721, 7316, 11, 50804, 50848, 264, 3779, 7316, 11, 498, 291, 528, 11, 307, 472, 294, 597, 439, 264, 2085, 10479, 362, 1958, 51104, 51104, 293, 264, 3006, 7719, 575, 502, 13, 51172, 51200, 1033, 13, 51225, 51244, 823, 14722, 264, 47991, 47387, 1296, 729, 732, 37870, 13, 51396, 51436, 1033, 13, 51461, 51461], "temperature": 0.0, "avg_logprob": -0.20752832366199028, "compression_ratio": 1.702127659574468, "no_speech_prob": 8.186199806914374e-07}, {"id": 506, "seek": 304178, "start": 3059.38, "end": 3062.42, "text": " Now compute the KL divergence between those two distributions.", "tokens": [50364, 291, 483, 257, 3840, 295, 3547, 1296, 1958, 293, 502, 300, 311, 3911, 502, 13, 50544, 50636, 400, 586, 291, 362, 257, 14721, 7316, 293, 264, 14721, 7316, 11, 50804, 50848, 264, 3779, 7316, 11, 498, 291, 528, 11, 307, 472, 294, 597, 439, 264, 2085, 10479, 362, 1958, 51104, 51104, 293, 264, 3006, 7719, 575, 502, 13, 51172, 51200, 1033, 13, 51225, 51244, 823, 14722, 264, 47991, 47387, 1296, 729, 732, 37870, 13, 51396, 51436, 1033, 13, 51461, 51461], "temperature": 0.0, "avg_logprob": -0.20752832366199028, "compression_ratio": 1.702127659574468, "no_speech_prob": 8.186199806914374e-07}, {"id": 507, "seek": 304178, "start": 3063.2200000000003, "end": 3063.7200000000003, "text": " Okay.", "tokens": [50364, 291, 483, 257, 3840, 295, 3547, 1296, 1958, 293, 502, 300, 311, 3911, 502, 13, 50544, 50636, 400, 586, 291, 362, 257, 14721, 7316, 293, 264, 14721, 7316, 11, 50804, 50848, 264, 3779, 7316, 11, 498, 291, 528, 11, 307, 472, 294, 597, 439, 264, 2085, 10479, 362, 1958, 51104, 51104, 293, 264, 3006, 7719, 575, 502, 13, 51172, 51200, 1033, 13, 51225, 51244, 823, 14722, 264, 47991, 47387, 1296, 729, 732, 37870, 13, 51396, 51436, 1033, 13, 51461, 51461], "temperature": 0.0, "avg_logprob": -0.20752832366199028, "compression_ratio": 1.702127659574468, "no_speech_prob": 8.186199806914374e-07}, {"id": 508, "seek": 306372, "start": 3063.72, "end": 3074.9399999999996, "text": " It's the sum over indices of the correct probability, okay, which is 0, except for one term,", "tokens": [50364, 467, 311, 264, 2408, 670, 43840, 295, 264, 3006, 8482, 11, 1392, 11, 597, 307, 1958, 11, 3993, 337, 472, 1433, 11, 50925, 51037, 1413, 264, 8509, 1296, 264, 3565, 295, 264, 8482, 300, 264, 1185, 14725, 51377, 51377, 293, 264, 3006, 8482, 11, 597, 307, 502, 13, 51501, 51501, 1033, 13, 51526, 51526, 407, 439, 295, 729, 2115, 5407, 281, 733, 295, 257, 2167, 1433, 11, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.1481654744752696, "compression_ratio": 1.646341463414634, "no_speech_prob": 5.422079539130209e-06}, {"id": 509, "seek": 306372, "start": 3077.18, "end": 3083.98, "text": " times the ratio between the log of the probability that the system produces", "tokens": [50364, 467, 311, 264, 2408, 670, 43840, 295, 264, 3006, 8482, 11, 1392, 11, 597, 307, 1958, 11, 3993, 337, 472, 1433, 11, 50925, 51037, 1413, 264, 8509, 1296, 264, 3565, 295, 264, 8482, 300, 264, 1185, 14725, 51377, 51377, 293, 264, 3006, 8482, 11, 597, 307, 502, 13, 51501, 51501, 1033, 13, 51526, 51526, 407, 439, 295, 729, 2115, 5407, 281, 733, 295, 257, 2167, 1433, 11, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.1481654744752696, "compression_ratio": 1.646341463414634, "no_speech_prob": 5.422079539130209e-06}, {"id": 510, "seek": 306372, "start": 3083.98, "end": 3086.4599999999996, "text": " and the correct probability, which is 1.", "tokens": [50364, 467, 311, 264, 2408, 670, 43840, 295, 264, 3006, 8482, 11, 1392, 11, 597, 307, 1958, 11, 3993, 337, 472, 1433, 11, 50925, 51037, 1413, 264, 8509, 1296, 264, 3565, 295, 264, 8482, 300, 264, 1185, 14725, 51377, 51377, 293, 264, 3006, 8482, 11, 597, 307, 502, 13, 51501, 51501, 1033, 13, 51526, 51526, 407, 439, 295, 729, 2115, 5407, 281, 733, 295, 257, 2167, 1433, 11, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.1481654744752696, "compression_ratio": 1.646341463414634, "no_speech_prob": 5.422079539130209e-06}, {"id": 511, "seek": 306372, "start": 3086.4599999999996, "end": 3086.9599999999996, "text": " Okay.", "tokens": [50364, 467, 311, 264, 2408, 670, 43840, 295, 264, 3006, 8482, 11, 1392, 11, 597, 307, 1958, 11, 3993, 337, 472, 1433, 11, 50925, 51037, 1413, 264, 8509, 1296, 264, 3565, 295, 264, 8482, 300, 264, 1185, 14725, 51377, 51377, 293, 264, 3006, 8482, 11, 597, 307, 502, 13, 51501, 51501, 1033, 13, 51526, 51526, 407, 439, 295, 729, 2115, 5407, 281, 733, 295, 257, 2167, 1433, 11, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.1481654744752696, "compression_ratio": 1.646341463414634, "no_speech_prob": 5.422079539130209e-06}, {"id": 512, "seek": 306372, "start": 3086.9599999999996, "end": 3091.5, "text": " So all of those terms reduce to kind of a single term,", "tokens": [50364, 467, 311, 264, 2408, 670, 43840, 295, 264, 3006, 8482, 11, 1392, 11, 597, 307, 1958, 11, 3993, 337, 472, 1433, 11, 50925, 51037, 1413, 264, 8509, 1296, 264, 3565, 295, 264, 8482, 300, 264, 1185, 14725, 51377, 51377, 293, 264, 3006, 8482, 11, 597, 307, 502, 13, 51501, 51501, 1033, 13, 51526, 51526, 407, 439, 295, 729, 2115, 5407, 281, 733, 295, 257, 2167, 1433, 11, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.1481654744752696, "compression_ratio": 1.646341463414634, "no_speech_prob": 5.422079539130209e-06}, {"id": 513, "seek": 309150, "start": 3091.5, "end": 3095.1, "text": " which is just the one for which the correct probability term is 1.", "tokens": [50364, 597, 307, 445, 264, 472, 337, 597, 264, 3006, 8482, 1433, 307, 502, 13, 50544, 50596, 1033, 13, 50621, 50621, 407, 321, 917, 493, 365, 341, 1433, 13, 50708, 50708, 467, 311, 445, 264, 3671, 3565, 295, 264, 2787, 41167, 5598, 337, 264, 3006, 1508, 13, 50912, 50960, 1033, 13, 50985, 50985, 492, 393, 1910, 341, 382, 257, 3278, 30867, 1296, 264, 7316, 7126, 538, 264, 1185, 51168, 51196, 293, 264, 472, 2368, 8062, 11760, 281, 264, 14721, 7316, 11, 498, 291, 528, 13, 51420, 51456, 1033, 13, 51481, 51481, 407, 586, 456, 576, 312, 1071, 733, 295, 544, 16950, 3037, 295, 341, 11, 51712, 51740], "temperature": 0.0, "avg_logprob": -0.10455809699164496, "compression_ratio": 1.708502024291498, "no_speech_prob": 4.8602314564050175e-06}, {"id": 514, "seek": 309150, "start": 3096.14, "end": 3096.64, "text": " Okay.", "tokens": [50364, 597, 307, 445, 264, 472, 337, 597, 264, 3006, 8482, 1433, 307, 502, 13, 50544, 50596, 1033, 13, 50621, 50621, 407, 321, 917, 493, 365, 341, 1433, 13, 50708, 50708, 467, 311, 445, 264, 3671, 3565, 295, 264, 2787, 41167, 5598, 337, 264, 3006, 1508, 13, 50912, 50960, 1033, 13, 50985, 50985, 492, 393, 1910, 341, 382, 257, 3278, 30867, 1296, 264, 7316, 7126, 538, 264, 1185, 51168, 51196, 293, 264, 472, 2368, 8062, 11760, 281, 264, 14721, 7316, 11, 498, 291, 528, 13, 51420, 51456, 1033, 13, 51481, 51481, 407, 586, 456, 576, 312, 1071, 733, 295, 544, 16950, 3037, 295, 341, 11, 51712, 51740], "temperature": 0.0, "avg_logprob": -0.10455809699164496, "compression_ratio": 1.708502024291498, "no_speech_prob": 4.8602314564050175e-06}, {"id": 515, "seek": 309150, "start": 3096.64, "end": 3098.38, "text": " So we end up with this term.", "tokens": [50364, 597, 307, 445, 264, 472, 337, 597, 264, 3006, 8482, 1433, 307, 502, 13, 50544, 50596, 1033, 13, 50621, 50621, 407, 321, 917, 493, 365, 341, 1433, 13, 50708, 50708, 467, 311, 445, 264, 3671, 3565, 295, 264, 2787, 41167, 5598, 337, 264, 3006, 1508, 13, 50912, 50960, 1033, 13, 50985, 50985, 492, 393, 1910, 341, 382, 257, 3278, 30867, 1296, 264, 7316, 7126, 538, 264, 1185, 51168, 51196, 293, 264, 472, 2368, 8062, 11760, 281, 264, 14721, 7316, 11, 498, 291, 528, 13, 51420, 51456, 1033, 13, 51481, 51481, 407, 586, 456, 576, 312, 1071, 733, 295, 544, 16950, 3037, 295, 341, 11, 51712, 51740], "temperature": 0.0, "avg_logprob": -0.10455809699164496, "compression_ratio": 1.708502024291498, "no_speech_prob": 4.8602314564050175e-06}, {"id": 516, "seek": 309150, "start": 3098.38, "end": 3102.46, "text": " It's just the negative log of the softmax output for the correct class.", "tokens": [50364, 597, 307, 445, 264, 472, 337, 597, 264, 3006, 8482, 1433, 307, 502, 13, 50544, 50596, 1033, 13, 50621, 50621, 407, 321, 917, 493, 365, 341, 1433, 13, 50708, 50708, 467, 311, 445, 264, 3671, 3565, 295, 264, 2787, 41167, 5598, 337, 264, 3006, 1508, 13, 50912, 50960, 1033, 13, 50985, 50985, 492, 393, 1910, 341, 382, 257, 3278, 30867, 1296, 264, 7316, 7126, 538, 264, 1185, 51168, 51196, 293, 264, 472, 2368, 8062, 11760, 281, 264, 14721, 7316, 11, 498, 291, 528, 13, 51420, 51456, 1033, 13, 51481, 51481, 407, 586, 456, 576, 312, 1071, 733, 295, 544, 16950, 3037, 295, 341, 11, 51712, 51740], "temperature": 0.0, "avg_logprob": -0.10455809699164496, "compression_ratio": 1.708502024291498, "no_speech_prob": 4.8602314564050175e-06}, {"id": 517, "seek": 309150, "start": 3103.42, "end": 3103.92, "text": " Okay.", "tokens": [50364, 597, 307, 445, 264, 472, 337, 597, 264, 3006, 8482, 1433, 307, 502, 13, 50544, 50596, 1033, 13, 50621, 50621, 407, 321, 917, 493, 365, 341, 1433, 13, 50708, 50708, 467, 311, 445, 264, 3671, 3565, 295, 264, 2787, 41167, 5598, 337, 264, 3006, 1508, 13, 50912, 50960, 1033, 13, 50985, 50985, 492, 393, 1910, 341, 382, 257, 3278, 30867, 1296, 264, 7316, 7126, 538, 264, 1185, 51168, 51196, 293, 264, 472, 2368, 8062, 11760, 281, 264, 14721, 7316, 11, 498, 291, 528, 13, 51420, 51456, 1033, 13, 51481, 51481, 407, 586, 456, 576, 312, 1071, 733, 295, 544, 16950, 3037, 295, 341, 11, 51712, 51740], "temperature": 0.0, "avg_logprob": -0.10455809699164496, "compression_ratio": 1.708502024291498, "no_speech_prob": 4.8602314564050175e-06}, {"id": 518, "seek": 309150, "start": 3103.92, "end": 3107.58, "text": " We can view this as a cross entropy between the distribution produced by the system", "tokens": [50364, 597, 307, 445, 264, 472, 337, 597, 264, 3006, 8482, 1433, 307, 502, 13, 50544, 50596, 1033, 13, 50621, 50621, 407, 321, 917, 493, 365, 341, 1433, 13, 50708, 50708, 467, 311, 445, 264, 3671, 3565, 295, 264, 2787, 41167, 5598, 337, 264, 3006, 1508, 13, 50912, 50960, 1033, 13, 50985, 50985, 492, 393, 1910, 341, 382, 257, 3278, 30867, 1296, 264, 7316, 7126, 538, 264, 1185, 51168, 51196, 293, 264, 472, 2368, 8062, 11760, 281, 264, 14721, 7316, 11, 498, 291, 528, 13, 51420, 51456, 1033, 13, 51481, 51481, 407, 586, 456, 576, 312, 1071, 733, 295, 544, 16950, 3037, 295, 341, 11, 51712, 51740], "temperature": 0.0, "avg_logprob": -0.10455809699164496, "compression_ratio": 1.708502024291498, "no_speech_prob": 4.8602314564050175e-06}, {"id": 519, "seek": 309150, "start": 3108.14, "end": 3112.62, "text": " and the one hot vector corresponding to the desired distribution, if you want.", "tokens": [50364, 597, 307, 445, 264, 472, 337, 597, 264, 3006, 8482, 1433, 307, 502, 13, 50544, 50596, 1033, 13, 50621, 50621, 407, 321, 917, 493, 365, 341, 1433, 13, 50708, 50708, 467, 311, 445, 264, 3671, 3565, 295, 264, 2787, 41167, 5598, 337, 264, 3006, 1508, 13, 50912, 50960, 1033, 13, 50985, 50985, 492, 393, 1910, 341, 382, 257, 3278, 30867, 1296, 264, 7316, 7126, 538, 264, 1185, 51168, 51196, 293, 264, 472, 2368, 8062, 11760, 281, 264, 14721, 7316, 11, 498, 291, 528, 13, 51420, 51456, 1033, 13, 51481, 51481, 407, 586, 456, 576, 312, 1071, 733, 295, 544, 16950, 3037, 295, 341, 11, 51712, 51740], "temperature": 0.0, "avg_logprob": -0.10455809699164496, "compression_ratio": 1.708502024291498, "no_speech_prob": 4.8602314564050175e-06}, {"id": 520, "seek": 309150, "start": 3113.34, "end": 3113.84, "text": " Okay.", "tokens": [50364, 597, 307, 445, 264, 472, 337, 597, 264, 3006, 8482, 1433, 307, 502, 13, 50544, 50596, 1033, 13, 50621, 50621, 407, 321, 917, 493, 365, 341, 1433, 13, 50708, 50708, 467, 311, 445, 264, 3671, 3565, 295, 264, 2787, 41167, 5598, 337, 264, 3006, 1508, 13, 50912, 50960, 1033, 13, 50985, 50985, 492, 393, 1910, 341, 382, 257, 3278, 30867, 1296, 264, 7316, 7126, 538, 264, 1185, 51168, 51196, 293, 264, 472, 2368, 8062, 11760, 281, 264, 14721, 7316, 11, 498, 291, 528, 13, 51420, 51456, 1033, 13, 51481, 51481, 407, 586, 456, 576, 312, 1071, 733, 295, 544, 16950, 3037, 295, 341, 11, 51712, 51740], "temperature": 0.0, "avg_logprob": -0.10455809699164496, "compression_ratio": 1.708502024291498, "no_speech_prob": 4.8602314564050175e-06}, {"id": 521, "seek": 309150, "start": 3113.84, "end": 3118.46, "text": " So now there would be another kind of more sophisticated version of this,", "tokens": [50364, 597, 307, 445, 264, 472, 337, 597, 264, 3006, 8482, 1433, 307, 502, 13, 50544, 50596, 1033, 13, 50621, 50621, 407, 321, 917, 493, 365, 341, 1433, 13, 50708, 50708, 467, 311, 445, 264, 3671, 3565, 295, 264, 2787, 41167, 5598, 337, 264, 3006, 1508, 13, 50912, 50960, 1033, 13, 50985, 50985, 492, 393, 1910, 341, 382, 257, 3278, 30867, 1296, 264, 7316, 7126, 538, 264, 1185, 51168, 51196, 293, 264, 472, 2368, 8062, 11760, 281, 264, 14721, 7316, 11, 498, 291, 528, 13, 51420, 51456, 1033, 13, 51481, 51481, 407, 586, 456, 576, 312, 1071, 733, 295, 544, 16950, 3037, 295, 341, 11, 51712, 51740], "temperature": 0.0, "avg_logprob": -0.10455809699164496, "compression_ratio": 1.708502024291498, "no_speech_prob": 4.8602314564050175e-06}, {"id": 522, "seek": 311846, "start": 3118.46, "end": 3122.78, "text": " which would be the actual KL divergence between the distribution produced by the system", "tokens": [50364, 597, 576, 312, 264, 3539, 47991, 47387, 1296, 264, 7316, 7126, 538, 264, 1185, 50580, 50580, 293, 257, 7316, 300, 291, 17421, 11, 2035, 309, 307, 11, 257, 3779, 7316, 11, 50760, 50760, 597, 586, 307, 406, 17434, 13, 50816, 50816, 467, 311, 406, 264, 472, 2368, 8062, 3602, 11, 457, 309, 311, 445, 257, 8062, 295, 3547, 13, 50984, 50984, 400, 300, 311, 1219, 264, 47991, 47387, 1508, 13, 51084, 51084, 682, 1186, 11, 309, 311, 11, 321, 603, 536, 309, 294, 257, 3456, 13, 51160, 51476, 407, 47991, 47387, 307, 257, 733, 295, 11, 291, 458, 11, 309, 311, 406, 257, 4560, 570, 309, 311, 406, 32330, 11, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.16581301103558457, "compression_ratio": 1.7530364372469636, "no_speech_prob": 8.800801879260689e-06}, {"id": 523, "seek": 311846, "start": 3122.78, "end": 3126.38, "text": " and a distribution that you propose, whatever it is, a target distribution,", "tokens": [50364, 597, 576, 312, 264, 3539, 47991, 47387, 1296, 264, 7316, 7126, 538, 264, 1185, 50580, 50580, 293, 257, 7316, 300, 291, 17421, 11, 2035, 309, 307, 11, 257, 3779, 7316, 11, 50760, 50760, 597, 586, 307, 406, 17434, 13, 50816, 50816, 467, 311, 406, 264, 472, 2368, 8062, 3602, 11, 457, 309, 311, 445, 257, 8062, 295, 3547, 13, 50984, 50984, 400, 300, 311, 1219, 264, 47991, 47387, 1508, 13, 51084, 51084, 682, 1186, 11, 309, 311, 11, 321, 603, 536, 309, 294, 257, 3456, 13, 51160, 51476, 407, 47991, 47387, 307, 257, 733, 295, 11, 291, 458, 11, 309, 311, 406, 257, 4560, 570, 309, 311, 406, 32330, 11, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.16581301103558457, "compression_ratio": 1.7530364372469636, "no_speech_prob": 8.800801879260689e-06}, {"id": 524, "seek": 311846, "start": 3126.38, "end": 3127.5, "text": " which now is not binary.", "tokens": [50364, 597, 576, 312, 264, 3539, 47991, 47387, 1296, 264, 7316, 7126, 538, 264, 1185, 50580, 50580, 293, 257, 7316, 300, 291, 17421, 11, 2035, 309, 307, 11, 257, 3779, 7316, 11, 50760, 50760, 597, 586, 307, 406, 17434, 13, 50816, 50816, 467, 311, 406, 264, 472, 2368, 8062, 3602, 11, 457, 309, 311, 445, 257, 8062, 295, 3547, 13, 50984, 50984, 400, 300, 311, 1219, 264, 47991, 47387, 1508, 13, 51084, 51084, 682, 1186, 11, 309, 311, 11, 321, 603, 536, 309, 294, 257, 3456, 13, 51160, 51476, 407, 47991, 47387, 307, 257, 733, 295, 11, 291, 458, 11, 309, 311, 406, 257, 4560, 570, 309, 311, 406, 32330, 11, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.16581301103558457, "compression_ratio": 1.7530364372469636, "no_speech_prob": 8.800801879260689e-06}, {"id": 525, "seek": 311846, "start": 3127.5, "end": 3130.86, "text": " It's not the one hot vector anymore, but it's just a vector of numbers.", "tokens": [50364, 597, 576, 312, 264, 3539, 47991, 47387, 1296, 264, 7316, 7126, 538, 264, 1185, 50580, 50580, 293, 257, 7316, 300, 291, 17421, 11, 2035, 309, 307, 11, 257, 3779, 7316, 11, 50760, 50760, 597, 586, 307, 406, 17434, 13, 50816, 50816, 467, 311, 406, 264, 472, 2368, 8062, 3602, 11, 457, 309, 311, 445, 257, 8062, 295, 3547, 13, 50984, 50984, 400, 300, 311, 1219, 264, 47991, 47387, 1508, 13, 51084, 51084, 682, 1186, 11, 309, 311, 11, 321, 603, 536, 309, 294, 257, 3456, 13, 51160, 51476, 407, 47991, 47387, 307, 257, 733, 295, 11, 291, 458, 11, 309, 311, 406, 257, 4560, 570, 309, 311, 406, 32330, 11, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.16581301103558457, "compression_ratio": 1.7530364372469636, "no_speech_prob": 8.800801879260689e-06}, {"id": 526, "seek": 311846, "start": 3130.86, "end": 3132.86, "text": " And that's called the KL divergence class.", "tokens": [50364, 597, 576, 312, 264, 3539, 47991, 47387, 1296, 264, 7316, 7126, 538, 264, 1185, 50580, 50580, 293, 257, 7316, 300, 291, 17421, 11, 2035, 309, 307, 11, 257, 3779, 7316, 11, 50760, 50760, 597, 586, 307, 406, 17434, 13, 50816, 50816, 467, 311, 406, 264, 472, 2368, 8062, 3602, 11, 457, 309, 311, 445, 257, 8062, 295, 3547, 13, 50984, 50984, 400, 300, 311, 1219, 264, 47991, 47387, 1508, 13, 51084, 51084, 682, 1186, 11, 309, 311, 11, 321, 603, 536, 309, 294, 257, 3456, 13, 51160, 51476, 407, 47991, 47387, 307, 257, 733, 295, 11, 291, 458, 11, 309, 311, 406, 257, 4560, 570, 309, 311, 406, 32330, 11, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.16581301103558457, "compression_ratio": 1.7530364372469636, "no_speech_prob": 8.800801879260689e-06}, {"id": 527, "seek": 311846, "start": 3132.86, "end": 3134.38, "text": " In fact, it's, we'll see it in a minute.", "tokens": [50364, 597, 576, 312, 264, 3539, 47991, 47387, 1296, 264, 7316, 7126, 538, 264, 1185, 50580, 50580, 293, 257, 7316, 300, 291, 17421, 11, 2035, 309, 307, 11, 257, 3779, 7316, 11, 50760, 50760, 597, 586, 307, 406, 17434, 13, 50816, 50816, 467, 311, 406, 264, 472, 2368, 8062, 3602, 11, 457, 309, 311, 445, 257, 8062, 295, 3547, 13, 50984, 50984, 400, 300, 311, 1219, 264, 47991, 47387, 1508, 13, 51084, 51084, 682, 1186, 11, 309, 311, 11, 321, 603, 536, 309, 294, 257, 3456, 13, 51160, 51476, 407, 47991, 47387, 307, 257, 733, 295, 11, 291, 458, 11, 309, 311, 406, 257, 4560, 570, 309, 311, 406, 32330, 11, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.16581301103558457, "compression_ratio": 1.7530364372469636, "no_speech_prob": 8.800801879260689e-06}, {"id": 528, "seek": 311846, "start": 3140.7, "end": 3144.86, "text": " So KL divergence is a kind of, you know, it's not a distance because it's not symmetric,", "tokens": [50364, 597, 576, 312, 264, 3539, 47991, 47387, 1296, 264, 7316, 7126, 538, 264, 1185, 50580, 50580, 293, 257, 7316, 300, 291, 17421, 11, 2035, 309, 307, 11, 257, 3779, 7316, 11, 50760, 50760, 597, 586, 307, 406, 17434, 13, 50816, 50816, 467, 311, 406, 264, 472, 2368, 8062, 3602, 11, 457, 309, 311, 445, 257, 8062, 295, 3547, 13, 50984, 50984, 400, 300, 311, 1219, 264, 47991, 47387, 1508, 13, 51084, 51084, 682, 1186, 11, 309, 311, 11, 321, 603, 536, 309, 294, 257, 3456, 13, 51160, 51476, 407, 47991, 47387, 307, 257, 733, 295, 11, 291, 458, 11, 309, 311, 406, 257, 4560, 570, 309, 311, 406, 32330, 11, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.16581301103558457, "compression_ratio": 1.7530364372469636, "no_speech_prob": 8.800801879260689e-06}, {"id": 529, "seek": 314486, "start": 3144.86, "end": 3151.6600000000003, "text": " but it's sort of a divergence between, between distributions, discrete distributions.", "tokens": [50364, 457, 309, 311, 1333, 295, 257, 47387, 1296, 11, 1296, 37870, 11, 27706, 37870, 13, 50704, 50748, 1033, 13, 50773, 50773, 407, 341, 472, 307, 257, 857, 295, 257, 733, 295, 364, 10320, 498, 291, 528, 295, 3565, 2787, 41167, 13, 51180, 51280, 400, 309, 311, 257, 3037, 295, 309, 300, 307, 21142, 337, 588, 11, 588, 2416, 19250, 2144, 13, 51556, 51556, 407, 498, 291, 362, 867, 11, 867, 11, 867, 10479, 11, 51656, 51720], "temperature": 0.0, "avg_logprob": -0.24493535359700522, "compression_ratio": 1.6256983240223464, "no_speech_prob": 3.5002581171283964e-06}, {"id": 530, "seek": 314486, "start": 3152.54, "end": 3153.04, "text": " Okay.", "tokens": [50364, 457, 309, 311, 1333, 295, 257, 47387, 1296, 11, 1296, 37870, 11, 27706, 37870, 13, 50704, 50748, 1033, 13, 50773, 50773, 407, 341, 472, 307, 257, 857, 295, 257, 733, 295, 364, 10320, 498, 291, 528, 295, 3565, 2787, 41167, 13, 51180, 51280, 400, 309, 311, 257, 3037, 295, 309, 300, 307, 21142, 337, 588, 11, 588, 2416, 19250, 2144, 13, 51556, 51556, 407, 498, 291, 362, 867, 11, 867, 11, 867, 10479, 11, 51656, 51720], "temperature": 0.0, "avg_logprob": -0.24493535359700522, "compression_ratio": 1.6256983240223464, "no_speech_prob": 3.5002581171283964e-06}, {"id": 531, "seek": 314486, "start": 3153.04, "end": 3161.1800000000003, "text": " So this one is a bit of a kind of an extension if you want of log softmax.", "tokens": [50364, 457, 309, 311, 1333, 295, 257, 47387, 1296, 11, 1296, 37870, 11, 27706, 37870, 13, 50704, 50748, 1033, 13, 50773, 50773, 407, 341, 472, 307, 257, 857, 295, 257, 733, 295, 364, 10320, 498, 291, 528, 295, 3565, 2787, 41167, 13, 51180, 51280, 400, 309, 311, 257, 3037, 295, 309, 300, 307, 21142, 337, 588, 11, 588, 2416, 19250, 2144, 13, 51556, 51556, 407, 498, 291, 362, 867, 11, 867, 11, 867, 10479, 11, 51656, 51720], "temperature": 0.0, "avg_logprob": -0.24493535359700522, "compression_ratio": 1.6256983240223464, "no_speech_prob": 3.5002581171283964e-06}, {"id": 532, "seek": 314486, "start": 3163.1800000000003, "end": 3168.7000000000003, "text": " And it's a version of it that is applicable for very, very large categorization.", "tokens": [50364, 457, 309, 311, 1333, 295, 257, 47387, 1296, 11, 1296, 37870, 11, 27706, 37870, 13, 50704, 50748, 1033, 13, 50773, 50773, 407, 341, 472, 307, 257, 857, 295, 257, 733, 295, 364, 10320, 498, 291, 528, 295, 3565, 2787, 41167, 13, 51180, 51280, 400, 309, 311, 257, 3037, 295, 309, 300, 307, 21142, 337, 588, 11, 588, 2416, 19250, 2144, 13, 51556, 51556, 407, 498, 291, 362, 867, 11, 867, 11, 867, 10479, 11, 51656, 51720], "temperature": 0.0, "avg_logprob": -0.24493535359700522, "compression_ratio": 1.6256983240223464, "no_speech_prob": 3.5002581171283964e-06}, {"id": 533, "seek": 314486, "start": 3168.7000000000003, "end": 3170.7000000000003, "text": " So if you have many, many, many categories,", "tokens": [50364, 457, 309, 311, 1333, 295, 257, 47387, 1296, 11, 1296, 37870, 11, 27706, 37870, 13, 50704, 50748, 1033, 13, 50773, 50773, 407, 341, 472, 307, 257, 857, 295, 257, 733, 295, 364, 10320, 498, 291, 528, 295, 3565, 2787, 41167, 13, 51180, 51280, 400, 309, 311, 257, 3037, 295, 309, 300, 307, 21142, 337, 588, 11, 588, 2416, 19250, 2144, 13, 51556, 51556, 407, 498, 291, 362, 867, 11, 867, 11, 867, 10479, 11, 51656, 51720], "temperature": 0.0, "avg_logprob": -0.24493535359700522, "compression_ratio": 1.6256983240223464, "no_speech_prob": 3.5002581171283964e-06}, {"id": 534, "seek": 317070, "start": 3170.7, "end": 3173.8199999999997, "text": " what you might want to do is kind of cut some corners.", "tokens": [50364, 437, 291, 1062, 528, 281, 360, 307, 733, 295, 1723, 512, 12413, 13, 50520, 50520, 509, 500, 380, 528, 281, 14722, 257, 7410, 2787, 41167, 670, 584, 257, 2459, 10479, 420, 1310, 754, 544, 13, 50808, 50856, 407, 456, 291, 393, 1333, 295, 1936, 11200, 264, 2306, 300, 366, 1359, 293, 11, 291, 458, 11, 51020, 51020, 733, 295, 764, 11733, 281, 733, 295, 11, 291, 458, 11, 3470, 264, 3073, 295, 264, 11, 295, 264, 24903, 13, 51496, 51496, 400, 341, 307, 437, 341, 775, 13, 51552, 51552, 286, 478, 406, 516, 281, 352, 666, 264, 4365, 2293, 437, 309, 775, 570, 767, 286, 500, 380, 458, 264, 4365, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.17479614589525305, "compression_ratio": 1.7170542635658914, "no_speech_prob": 9.36812830332201e-06}, {"id": 535, "seek": 317070, "start": 3173.8199999999997, "end": 3179.58, "text": " You don't want to compute a giant softmax over say a million categories or maybe even more.", "tokens": [50364, 437, 291, 1062, 528, 281, 360, 307, 733, 295, 1723, 512, 12413, 13, 50520, 50520, 509, 500, 380, 528, 281, 14722, 257, 7410, 2787, 41167, 670, 584, 257, 2459, 10479, 420, 1310, 754, 544, 13, 50808, 50856, 407, 456, 291, 393, 1333, 295, 1936, 11200, 264, 2306, 300, 366, 1359, 293, 11, 291, 458, 11, 51020, 51020, 733, 295, 764, 11733, 281, 733, 295, 11, 291, 458, 11, 3470, 264, 3073, 295, 264, 11, 295, 264, 24903, 13, 51496, 51496, 400, 341, 307, 437, 341, 775, 13, 51552, 51552, 286, 478, 406, 516, 281, 352, 666, 264, 4365, 2293, 437, 309, 775, 570, 767, 286, 500, 380, 458, 264, 4365, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.17479614589525305, "compression_ratio": 1.7170542635658914, "no_speech_prob": 9.36812830332201e-06}, {"id": 536, "seek": 317070, "start": 3180.54, "end": 3183.8199999999997, "text": " So there you can sort of basically ignore the ones that are small and, you know,", "tokens": [50364, 437, 291, 1062, 528, 281, 360, 307, 733, 295, 1723, 512, 12413, 13, 50520, 50520, 509, 500, 380, 528, 281, 14722, 257, 7410, 2787, 41167, 670, 584, 257, 2459, 10479, 420, 1310, 754, 544, 13, 50808, 50856, 407, 456, 291, 393, 1333, 295, 1936, 11200, 264, 2306, 300, 366, 1359, 293, 11, 291, 458, 11, 51020, 51020, 733, 295, 764, 11733, 281, 733, 295, 11, 291, 458, 11, 3470, 264, 3073, 295, 264, 11, 295, 264, 24903, 13, 51496, 51496, 400, 341, 307, 437, 341, 775, 13, 51552, 51552, 286, 478, 406, 516, 281, 352, 666, 264, 4365, 2293, 437, 309, 775, 570, 767, 286, 500, 380, 458, 264, 4365, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.17479614589525305, "compression_ratio": 1.7170542635658914, "no_speech_prob": 9.36812830332201e-06}, {"id": 537, "seek": 317070, "start": 3183.8199999999997, "end": 3193.3399999999997, "text": " kind of use tricks to kind of, you know, improve the speed of the, of the computation.", "tokens": [50364, 437, 291, 1062, 528, 281, 360, 307, 733, 295, 1723, 512, 12413, 13, 50520, 50520, 509, 500, 380, 528, 281, 14722, 257, 7410, 2787, 41167, 670, 584, 257, 2459, 10479, 420, 1310, 754, 544, 13, 50808, 50856, 407, 456, 291, 393, 1333, 295, 1936, 11200, 264, 2306, 300, 366, 1359, 293, 11, 291, 458, 11, 51020, 51020, 733, 295, 764, 11733, 281, 733, 295, 11, 291, 458, 11, 3470, 264, 3073, 295, 264, 11, 295, 264, 24903, 13, 51496, 51496, 400, 341, 307, 437, 341, 775, 13, 51552, 51552, 286, 478, 406, 516, 281, 352, 666, 264, 4365, 2293, 437, 309, 775, 570, 767, 286, 500, 380, 458, 264, 4365, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.17479614589525305, "compression_ratio": 1.7170542635658914, "no_speech_prob": 9.36812830332201e-06}, {"id": 538, "seek": 317070, "start": 3193.3399999999997, "end": 3194.46, "text": " And this is what this does.", "tokens": [50364, 437, 291, 1062, 528, 281, 360, 307, 733, 295, 1723, 512, 12413, 13, 50520, 50520, 509, 500, 380, 528, 281, 14722, 257, 7410, 2787, 41167, 670, 584, 257, 2459, 10479, 420, 1310, 754, 544, 13, 50808, 50856, 407, 456, 291, 393, 1333, 295, 1936, 11200, 264, 2306, 300, 366, 1359, 293, 11, 291, 458, 11, 51020, 51020, 733, 295, 764, 11733, 281, 733, 295, 11, 291, 458, 11, 3470, 264, 3073, 295, 264, 11, 295, 264, 24903, 13, 51496, 51496, 400, 341, 307, 437, 341, 775, 13, 51552, 51552, 286, 478, 406, 516, 281, 352, 666, 264, 4365, 2293, 437, 309, 775, 570, 767, 286, 500, 380, 458, 264, 4365, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.17479614589525305, "compression_ratio": 1.7170542635658914, "no_speech_prob": 9.36812830332201e-06}, {"id": 539, "seek": 317070, "start": 3194.46, "end": 3197.98, "text": " I'm not going to go into the details exactly what it does because actually I don't know the details,", "tokens": [50364, 437, 291, 1062, 528, 281, 360, 307, 733, 295, 1723, 512, 12413, 13, 50520, 50520, 509, 500, 380, 528, 281, 14722, 257, 7410, 2787, 41167, 670, 584, 257, 2459, 10479, 420, 1310, 754, 544, 13, 50808, 50856, 407, 456, 291, 393, 1333, 295, 1936, 11200, 264, 2306, 300, 366, 1359, 293, 11, 291, 458, 11, 51020, 51020, 733, 295, 764, 11733, 281, 733, 295, 11, 291, 458, 11, 3470, 264, 3073, 295, 264, 11, 295, 264, 24903, 13, 51496, 51496, 400, 341, 307, 437, 341, 775, 13, 51552, 51552, 286, 478, 406, 516, 281, 352, 666, 264, 4365, 2293, 437, 309, 775, 570, 767, 286, 500, 380, 458, 264, 4365, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.17479614589525305, "compression_ratio": 1.7170542635658914, "no_speech_prob": 9.36812830332201e-06}, {"id": 540, "seek": 319798, "start": 3197.98, "end": 3202.7, "text": " but it's basically an efficient approximation of softmax for a very, very large number of categories.", "tokens": [50364, 457, 309, 311, 1936, 364, 7148, 28023, 295, 2787, 41167, 337, 257, 588, 11, 588, 2416, 1230, 295, 10479, 13, 50600, 50916, 407, 341, 307, 257, 2121, 1389, 295, 3278, 30867, 562, 291, 787, 362, 732, 10479, 13, 51260, 51288, 400, 294, 300, 1389, 11, 309, 733, 295, 18081, 281, 746, 2199, 13, 51428, 51492, 407, 341, 775, 406, 4090, 2787, 41167, 13, 51580, 51580, 639, 307, 445, 257, 3278, 30867, 562, 291, 362, 732, 10479, 13, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.2740596959620346, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.494969289225992e-06}, {"id": 541, "seek": 319798, "start": 3209.02, "end": 3215.9, "text": " So this is a special case of cross entropy when you only have two categories.", "tokens": [50364, 457, 309, 311, 1936, 364, 7148, 28023, 295, 2787, 41167, 337, 257, 588, 11, 588, 2416, 1230, 295, 10479, 13, 50600, 50916, 407, 341, 307, 257, 2121, 1389, 295, 3278, 30867, 562, 291, 787, 362, 732, 10479, 13, 51260, 51288, 400, 294, 300, 1389, 11, 309, 733, 295, 18081, 281, 746, 2199, 13, 51428, 51492, 407, 341, 775, 406, 4090, 2787, 41167, 13, 51580, 51580, 639, 307, 445, 257, 3278, 30867, 562, 291, 362, 732, 10479, 13, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.2740596959620346, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.494969289225992e-06}, {"id": 542, "seek": 319798, "start": 3216.46, "end": 3219.26, "text": " And in that case, it kind of reduces to something simple.", "tokens": [50364, 457, 309, 311, 1936, 364, 7148, 28023, 295, 2787, 41167, 337, 257, 588, 11, 588, 2416, 1230, 295, 10479, 13, 50600, 50916, 407, 341, 307, 257, 2121, 1389, 295, 3278, 30867, 562, 291, 787, 362, 732, 10479, 13, 51260, 51288, 400, 294, 300, 1389, 11, 309, 733, 295, 18081, 281, 746, 2199, 13, 51428, 51492, 407, 341, 775, 406, 4090, 2787, 41167, 13, 51580, 51580, 639, 307, 445, 257, 3278, 30867, 562, 291, 362, 732, 10479, 13, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.2740596959620346, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.494969289225992e-06}, {"id": 543, "seek": 319798, "start": 3220.54, "end": 3222.3, "text": " So this does not include softmax.", "tokens": [50364, 457, 309, 311, 1936, 364, 7148, 28023, 295, 2787, 41167, 337, 257, 588, 11, 588, 2416, 1230, 295, 10479, 13, 50600, 50916, 407, 341, 307, 257, 2121, 1389, 295, 3278, 30867, 562, 291, 787, 362, 732, 10479, 13, 51260, 51288, 400, 294, 300, 1389, 11, 309, 733, 295, 18081, 281, 746, 2199, 13, 51428, 51492, 407, 341, 775, 406, 4090, 2787, 41167, 13, 51580, 51580, 639, 307, 445, 257, 3278, 30867, 562, 291, 362, 732, 10479, 13, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.2740596959620346, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.494969289225992e-06}, {"id": 544, "seek": 319798, "start": 3222.3, "end": 3225.58, "text": " This is just a cross entropy when you have two categories.", "tokens": [50364, 457, 309, 311, 1936, 364, 7148, 28023, 295, 2787, 41167, 337, 257, 588, 11, 588, 2416, 1230, 295, 10479, 13, 50600, 50916, 407, 341, 307, 257, 2121, 1389, 295, 3278, 30867, 562, 291, 787, 362, 732, 10479, 13, 51260, 51288, 400, 294, 300, 1389, 11, 309, 733, 295, 18081, 281, 746, 2199, 13, 51428, 51492, 407, 341, 775, 406, 4090, 2787, 41167, 13, 51580, 51580, 639, 307, 445, 257, 3278, 30867, 562, 291, 362, 732, 10479, 13, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.2740596959620346, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.494969289225992e-06}, {"id": 545, "seek": 322558, "start": 3225.58, "end": 3235.66, "text": " And as I said before, the cross entropy loss is the sum over categories of the probability,", "tokens": [50364, 400, 382, 286, 848, 949, 11, 264, 3278, 30867, 4470, 307, 264, 2408, 670, 10479, 295, 264, 8482, 11, 50868, 50960, 286, 914, 11, 2408, 670, 43840, 420, 512, 295, 264, 10479, 295, 264, 8482, 337, 264, 3779, 11, 51180, 51180, 264, 3779, 8482, 337, 300, 7719, 1413, 264, 8509, 1296, 264, 3565, 295, 264, 51560, 51612], "temperature": 0.0, "avg_logprob": -0.3452374409821074, "compression_ratio": 1.8489208633093526, "no_speech_prob": 2.2119649656815454e-05}, {"id": 546, "seek": 322558, "start": 3237.5, "end": 3241.9, "text": " I mean, sum over indices or some of the categories of the probability for the target,", "tokens": [50364, 400, 382, 286, 848, 949, 11, 264, 3278, 30867, 4470, 307, 264, 2408, 670, 10479, 295, 264, 8482, 11, 50868, 50960, 286, 914, 11, 2408, 670, 43840, 420, 512, 295, 264, 10479, 295, 264, 8482, 337, 264, 3779, 11, 51180, 51180, 264, 3779, 8482, 337, 300, 7719, 1413, 264, 8509, 1296, 264, 3565, 295, 264, 51560, 51612], "temperature": 0.0, "avg_logprob": -0.3452374409821074, "compression_ratio": 1.8489208633093526, "no_speech_prob": 2.2119649656815454e-05}, {"id": 547, "seek": 322558, "start": 3241.9, "end": 3249.5, "text": " the target probability for that category times the ratio between the log of the", "tokens": [50364, 400, 382, 286, 848, 949, 11, 264, 3278, 30867, 4470, 307, 264, 2408, 670, 10479, 295, 264, 8482, 11, 50868, 50960, 286, 914, 11, 2408, 670, 43840, 420, 512, 295, 264, 10479, 295, 264, 8482, 337, 264, 3779, 11, 51180, 51180, 264, 3779, 8482, 337, 300, 7719, 1413, 264, 8509, 1296, 264, 3565, 295, 264, 51560, 51612], "temperature": 0.0, "avg_logprob": -0.3452374409821074, "compression_ratio": 1.8489208633093526, "no_speech_prob": 2.2119649656815454e-05}, {"id": 548, "seek": 324950, "start": 3249.5, "end": 3255.66, "text": " probability for produced by the system divided by the probability of the target category.", "tokens": [50364, 8482, 337, 7126, 538, 264, 1185, 6666, 538, 264, 8482, 295, 264, 3779, 7719, 13, 50672, 50672, 400, 498, 291, 589, 309, 484, 337, 732, 10479, 11, 4725, 472, 6175, 307, 472, 3175, 264, 661, 472, 13, 50912, 50912, 759, 291, 362, 732, 13005, 10479, 293, 309, 1487, 760, 281, 341, 13, 51192, 51192, 1033, 13, 823, 11, 341, 1003, 4201, 300, 2031, 293, 288, 366, 264, 912, 13, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.3628198022711767, "compression_ratio": 1.5935828877005347, "no_speech_prob": 1.095179595722584e-05}, {"id": 549, "seek": 324950, "start": 3255.66, "end": 3260.46, "text": " And if you work it out for two categories, necessarily one score is one minus the other one.", "tokens": [50364, 8482, 337, 7126, 538, 264, 1185, 6666, 538, 264, 8482, 295, 264, 3779, 7719, 13, 50672, 50672, 400, 498, 291, 589, 309, 484, 337, 732, 10479, 11, 4725, 472, 6175, 307, 472, 3175, 264, 661, 472, 13, 50912, 50912, 759, 291, 362, 732, 13005, 10479, 293, 309, 1487, 760, 281, 341, 13, 51192, 51192, 1033, 13, 823, 11, 341, 1003, 4201, 300, 2031, 293, 288, 366, 264, 912, 13, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.3628198022711767, "compression_ratio": 1.5935828877005347, "no_speech_prob": 1.095179595722584e-05}, {"id": 550, "seek": 324950, "start": 3260.46, "end": 3266.06, "text": " If you have two exclusive categories and it comes down to this.", "tokens": [50364, 8482, 337, 7126, 538, 264, 1185, 6666, 538, 264, 8482, 295, 264, 3779, 7719, 13, 50672, 50672, 400, 498, 291, 589, 309, 484, 337, 732, 10479, 11, 4725, 472, 6175, 307, 472, 3175, 264, 661, 472, 13, 50912, 50912, 759, 291, 362, 732, 13005, 10479, 293, 309, 1487, 760, 281, 341, 13, 51192, 51192, 1033, 13, 823, 11, 341, 1003, 4201, 300, 2031, 293, 288, 366, 264, 912, 13, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.3628198022711767, "compression_ratio": 1.5935828877005347, "no_speech_prob": 1.095179595722584e-05}, {"id": 551, "seek": 324950, "start": 3266.06, "end": 3272.7, "text": " Okay. Now, this supposes that x and y are the same.", "tokens": [50364, 8482, 337, 7126, 538, 264, 1185, 6666, 538, 264, 8482, 295, 264, 3779, 7719, 13, 50672, 50672, 400, 498, 291, 589, 309, 484, 337, 732, 10479, 11, 4725, 472, 6175, 307, 472, 3175, 264, 661, 472, 13, 50912, 50912, 759, 291, 362, 732, 13005, 10479, 293, 309, 1487, 760, 281, 341, 13, 51192, 51192, 1033, 13, 823, 11, 341, 1003, 4201, 300, 2031, 293, 288, 366, 264, 912, 13, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.3628198022711767, "compression_ratio": 1.5935828877005347, "no_speech_prob": 1.095179595722584e-05}, {"id": 552, "seek": 327270, "start": 3272.7, "end": 3283.66, "text": " And it comes down to this. Okay. Now, this supposes that x and y are kind of probabilities.", "tokens": [50364, 400, 309, 1487, 760, 281, 341, 13, 1033, 13, 823, 11, 341, 1003, 4201, 300, 2031, 293, 288, 366, 733, 295, 33783, 13, 50912, 50912, 814, 362, 281, 312, 1296, 20792, 1296, 4018, 293, 472, 13, 286, 914, 11, 406, 20792, 11, 457, 731, 11, 51096, 51096, 733, 295, 20792, 570, 5911, 264, 20820, 733, 295, 6327, 493, 13, 51208, 51468, 1692, 307, 264, 47991, 47387, 4470, 286, 390, 3585, 291, 466, 3071, 13, 51608, 51676], "temperature": 0.0, "avg_logprob": -0.1869509648054074, "compression_ratio": 1.5303030303030303, "no_speech_prob": 8.139442797983065e-06}, {"id": 553, "seek": 327270, "start": 3283.66, "end": 3287.3399999999997, "text": " They have to be between strictly between zero and one. I mean, not strictly, but well,", "tokens": [50364, 400, 309, 1487, 760, 281, 341, 13, 1033, 13, 823, 11, 341, 1003, 4201, 300, 2031, 293, 288, 366, 733, 295, 33783, 13, 50912, 50912, 814, 362, 281, 312, 1296, 20792, 1296, 4018, 293, 472, 13, 286, 914, 11, 406, 20792, 11, 457, 731, 11, 51096, 51096, 733, 295, 20792, 570, 5911, 264, 20820, 733, 295, 6327, 493, 13, 51208, 51468, 1692, 307, 264, 47991, 47387, 4470, 286, 390, 3585, 291, 466, 3071, 13, 51608, 51676], "temperature": 0.0, "avg_logprob": -0.1869509648054074, "compression_ratio": 1.5303030303030303, "no_speech_prob": 8.139442797983065e-06}, {"id": 554, "seek": 327270, "start": 3287.3399999999997, "end": 3289.58, "text": " kind of strictly because otherwise the logs kind of blow up.", "tokens": [50364, 400, 309, 1487, 760, 281, 341, 13, 1033, 13, 823, 11, 341, 1003, 4201, 300, 2031, 293, 288, 366, 733, 295, 33783, 13, 50912, 50912, 814, 362, 281, 312, 1296, 20792, 1296, 4018, 293, 472, 13, 286, 914, 11, 406, 20792, 11, 457, 731, 11, 51096, 51096, 733, 295, 20792, 570, 5911, 264, 20820, 733, 295, 6327, 493, 13, 51208, 51468, 1692, 307, 264, 47991, 47387, 4470, 286, 390, 3585, 291, 466, 3071, 13, 51608, 51676], "temperature": 0.0, "avg_logprob": -0.1869509648054074, "compression_ratio": 1.5303030303030303, "no_speech_prob": 8.139442797983065e-06}, {"id": 555, "seek": 327270, "start": 3294.7799999999997, "end": 3297.58, "text": " Here is the KL divergence loss I was telling you about earlier.", "tokens": [50364, 400, 309, 1487, 760, 281, 341, 13, 1033, 13, 823, 11, 341, 1003, 4201, 300, 2031, 293, 288, 366, 733, 295, 33783, 13, 50912, 50912, 814, 362, 281, 312, 1296, 20792, 1296, 4018, 293, 472, 13, 286, 914, 11, 406, 20792, 11, 457, 731, 11, 51096, 51096, 733, 295, 20792, 570, 5911, 264, 20820, 733, 295, 6327, 493, 13, 51208, 51468, 1692, 307, 264, 47991, 47387, 4470, 286, 390, 3585, 291, 466, 3071, 13, 51608, 51676], "temperature": 0.0, "avg_logprob": -0.1869509648054074, "compression_ratio": 1.5303030303030303, "no_speech_prob": 8.139442797983065e-06}, {"id": 556, "seek": 329758, "start": 3297.58, "end": 3305.98, "text": " So here it's written here in a funny form, but it's basically the...", "tokens": [50364, 407, 510, 309, 311, 3720, 510, 294, 257, 4074, 1254, 11, 457, 309, 311, 1936, 264, 485, 50784, 50884, 1692, 797, 11, 309, 1333, 295, 37808, 485, 639, 307, 1071, 472, 286, 390, 3585, 291, 466, 3071, 11, 767, 13, 51136, 51136, 639, 472, 307, 611, 257, 26335, 472, 562, 291, 362, 257, 472, 12, 12194, 7316, 337, 264, 3779, 13, 51472, 51472, 407, 288, 307, 257, 7719, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.34421225769879066, "compression_ratio": 1.4863387978142077, "no_speech_prob": 7.071525487845065e-06}, {"id": 557, "seek": 329758, "start": 3307.98, "end": 3313.02, "text": " Here again, it sort of assumes... This is another one I was telling you about earlier, actually.", "tokens": [50364, 407, 510, 309, 311, 3720, 510, 294, 257, 4074, 1254, 11, 457, 309, 311, 1936, 264, 485, 50784, 50884, 1692, 797, 11, 309, 1333, 295, 37808, 485, 639, 307, 1071, 472, 286, 390, 3585, 291, 466, 3071, 11, 767, 13, 51136, 51136, 639, 472, 307, 611, 257, 26335, 472, 562, 291, 362, 257, 472, 12, 12194, 7316, 337, 264, 3779, 13, 51472, 51472, 407, 288, 307, 257, 7719, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.34421225769879066, "compression_ratio": 1.4863387978142077, "no_speech_prob": 7.071525487845065e-06}, {"id": 558, "seek": 329758, "start": 3313.02, "end": 3319.74, "text": " This one is also a simplified one when you have a one-hot distribution for the target.", "tokens": [50364, 407, 510, 309, 311, 3720, 510, 294, 257, 4074, 1254, 11, 457, 309, 311, 1936, 264, 485, 50784, 50884, 1692, 797, 11, 309, 1333, 295, 37808, 485, 639, 307, 1071, 472, 286, 390, 3585, 291, 466, 3071, 11, 767, 13, 51136, 51136, 639, 472, 307, 611, 257, 26335, 472, 562, 291, 362, 257, 472, 12, 12194, 7316, 337, 264, 3779, 13, 51472, 51472, 407, 288, 307, 257, 7719, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.34421225769879066, "compression_ratio": 1.4863387978142077, "no_speech_prob": 7.071525487845065e-06}, {"id": 559, "seek": 329758, "start": 3319.74, "end": 3323.8199999999997, "text": " So y is a category.", "tokens": [50364, 407, 510, 309, 311, 3720, 510, 294, 257, 4074, 1254, 11, 457, 309, 311, 1936, 264, 485, 50784, 50884, 1692, 797, 11, 309, 1333, 295, 37808, 485, 639, 307, 1071, 472, 286, 390, 3585, 291, 466, 3071, 11, 767, 13, 51136, 51136, 639, 472, 307, 611, 257, 26335, 472, 562, 291, 362, 257, 472, 12, 12194, 7316, 337, 264, 3779, 13, 51472, 51472, 407, 288, 307, 257, 7719, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.34421225769879066, "compression_ratio": 1.4863387978142077, "no_speech_prob": 7.071525487845065e-06}, {"id": 560, "seek": 332382, "start": 3323.82, "end": 3331.9, "text": " But it has a disadvantage of not being merged with something like softmax or lux softmax.", "tokens": [50364, 583, 309, 575, 257, 24292, 295, 406, 885, 36427, 365, 746, 411, 2787, 41167, 420, 11363, 2787, 41167, 13, 50768, 50768, 407, 309, 815, 2524, 485, 286, 914, 11, 309, 815, 362, 733, 295, 29054, 2663, 13, 51112, 51212, 3764, 11, 309, 37808, 2031, 293, 288, 366, 37870, 13, 51424, 51620, 639, 307, 10268, 1143, 11, 6165, 30472, 4470, 13, 51780, 51840], "temperature": 0.0, "avg_logprob": -0.20594513416290283, "compression_ratio": 1.4181818181818182, "no_speech_prob": 1.5779218074385426e-06}, {"id": 561, "seek": 332382, "start": 3331.9, "end": 3338.78, "text": " So it may reach... I mean, it may have kind of numerical issues.", "tokens": [50364, 583, 309, 575, 257, 24292, 295, 406, 885, 36427, 365, 746, 411, 2787, 41167, 420, 11363, 2787, 41167, 13, 50768, 50768, 407, 309, 815, 2524, 485, 286, 914, 11, 309, 815, 362, 733, 295, 29054, 2663, 13, 51112, 51212, 3764, 11, 309, 37808, 2031, 293, 288, 366, 37870, 13, 51424, 51620, 639, 307, 10268, 1143, 11, 6165, 30472, 4470, 13, 51780, 51840], "temperature": 0.0, "avg_logprob": -0.20594513416290283, "compression_ratio": 1.4181818181818182, "no_speech_prob": 1.5779218074385426e-06}, {"id": 562, "seek": 332382, "start": 3340.78, "end": 3345.02, "text": " Again, it assumes x and y are distributions.", "tokens": [50364, 583, 309, 575, 257, 24292, 295, 406, 885, 36427, 365, 746, 411, 2787, 41167, 420, 11363, 2787, 41167, 13, 50768, 50768, 407, 309, 815, 2524, 485, 286, 914, 11, 309, 815, 362, 733, 295, 29054, 2663, 13, 51112, 51212, 3764, 11, 309, 37808, 2031, 293, 288, 366, 37870, 13, 51424, 51620, 639, 307, 10268, 1143, 11, 6165, 30472, 4470, 13, 51780, 51840], "temperature": 0.0, "avg_logprob": -0.20594513416290283, "compression_ratio": 1.4181818181818182, "no_speech_prob": 1.5779218074385426e-06}, {"id": 563, "seek": 332382, "start": 3348.94, "end": 3352.1400000000003, "text": " This is barely used, Poisson loss.", "tokens": [50364, 583, 309, 575, 257, 24292, 295, 406, 885, 36427, 365, 746, 411, 2787, 41167, 420, 11363, 2787, 41167, 13, 50768, 50768, 407, 309, 815, 2524, 485, 286, 914, 11, 309, 815, 362, 733, 295, 29054, 2663, 13, 51112, 51212, 3764, 11, 309, 37808, 2031, 293, 288, 366, 37870, 13, 51424, 51620, 639, 307, 10268, 1143, 11, 6165, 30472, 4470, 13, 51780, 51840], "temperature": 0.0, "avg_logprob": -0.20594513416290283, "compression_ratio": 1.4181818181818182, "no_speech_prob": 1.5779218074385426e-06}, {"id": 564, "seek": 335214, "start": 3352.14, "end": 3359.66, "text": " Okay. So this version of the binary cross entropy here takes scores that haven't gone", "tokens": [50364, 1033, 13, 407, 341, 3037, 295, 264, 17434, 3278, 30867, 510, 2516, 13444, 300, 2378, 380, 2780, 50740, 50740, 807, 257, 4556, 3280, 327, 13, 407, 341, 472, 775, 406, 6552, 300, 264, 2031, 311, 366, 1296, 4018, 293, 472, 13, 51044, 51088, 467, 445, 2516, 4190, 11, 2035, 436, 366, 11, 293, 309, 11335, 552, 807, 257, 4556, 3280, 327, 281, 652, 988, 51360, 51360, 436, 366, 1296, 4018, 293, 472, 20792, 13, 1033, 13, 400, 370, 300, 307, 544, 3700, 281, 312, 7866, 984, 8351, 13, 51636, 51768], "temperature": 0.0, "avg_logprob": -0.20214856189230215, "compression_ratio": 1.6712962962962963, "no_speech_prob": 4.356817044026684e-06}, {"id": 565, "seek": 335214, "start": 3359.66, "end": 3365.74, "text": " through a sigmoid. So this one does not assume that the x's are between zero and one.", "tokens": [50364, 1033, 13, 407, 341, 3037, 295, 264, 17434, 3278, 30867, 510, 2516, 13444, 300, 2378, 380, 2780, 50740, 50740, 807, 257, 4556, 3280, 327, 13, 407, 341, 472, 775, 406, 6552, 300, 264, 2031, 311, 366, 1296, 4018, 293, 472, 13, 51044, 51088, 467, 445, 2516, 4190, 11, 2035, 436, 366, 11, 293, 309, 11335, 552, 807, 257, 4556, 3280, 327, 281, 652, 988, 51360, 51360, 436, 366, 1296, 4018, 293, 472, 20792, 13, 1033, 13, 400, 370, 300, 307, 544, 3700, 281, 312, 7866, 984, 8351, 13, 51636, 51768], "temperature": 0.0, "avg_logprob": -0.20214856189230215, "compression_ratio": 1.6712962962962963, "no_speech_prob": 4.356817044026684e-06}, {"id": 566, "seek": 335214, "start": 3366.62, "end": 3372.06, "text": " It just takes values, whatever they are, and it passes them through a sigmoid to make sure", "tokens": [50364, 1033, 13, 407, 341, 3037, 295, 264, 17434, 3278, 30867, 510, 2516, 13444, 300, 2378, 380, 2780, 50740, 50740, 807, 257, 4556, 3280, 327, 13, 407, 341, 472, 775, 406, 6552, 300, 264, 2031, 311, 366, 1296, 4018, 293, 472, 13, 51044, 51088, 467, 445, 2516, 4190, 11, 2035, 436, 366, 11, 293, 309, 11335, 552, 807, 257, 4556, 3280, 327, 281, 652, 988, 51360, 51360, 436, 366, 1296, 4018, 293, 472, 20792, 13, 1033, 13, 400, 370, 300, 307, 544, 3700, 281, 312, 7866, 984, 8351, 13, 51636, 51768], "temperature": 0.0, "avg_logprob": -0.20214856189230215, "compression_ratio": 1.6712962962962963, "no_speech_prob": 4.356817044026684e-06}, {"id": 567, "seek": 335214, "start": 3372.06, "end": 3377.58, "text": " they are between zero and one strictly. Okay. And so that is more likely to be numerically stable.", "tokens": [50364, 1033, 13, 407, 341, 3037, 295, 264, 17434, 3278, 30867, 510, 2516, 13444, 300, 2378, 380, 2780, 50740, 50740, 807, 257, 4556, 3280, 327, 13, 407, 341, 472, 775, 406, 6552, 300, 264, 2031, 311, 366, 1296, 4018, 293, 472, 13, 51044, 51088, 467, 445, 2516, 4190, 11, 2035, 436, 366, 11, 293, 309, 11335, 552, 807, 257, 4556, 3280, 327, 281, 652, 988, 51360, 51360, 436, 366, 1296, 4018, 293, 472, 20792, 13, 1033, 13, 400, 370, 300, 307, 544, 3700, 281, 312, 7866, 984, 8351, 13, 51636, 51768], "temperature": 0.0, "avg_logprob": -0.20214856189230215, "compression_ratio": 1.6712962962962963, "no_speech_prob": 4.356817044026684e-06}, {"id": 568, "seek": 337758, "start": 3377.58, "end": 3385.18, "text": " It's a bit the same idea as kind of merging lux softmax and negative log likelihood.", "tokens": [50364, 467, 311, 257, 857, 264, 912, 1558, 382, 733, 295, 44559, 11363, 2787, 41167, 293, 3671, 3565, 22119, 13, 50744, 50844, 4372, 485, 865, 11, 912, 551, 510, 13, 663, 311, 437, 286, 390, 1417, 13, 1033, 13, 2039, 1494, 15352, 13, 51080, 51080, 407, 341, 307, 1333, 295, 364, 1021, 7719, 295, 15352, 13, 3950, 15352, 1936, 584, 11, 51364, 51464, 498, 286, 362, 11, 294, 341, 1389, 11, 732, 15743, 11, 51576, 51692], "temperature": 0.0, "avg_logprob": -0.36926254668793124, "compression_ratio": 1.450777202072539, "no_speech_prob": 2.331988525838824e-06}, {"id": 569, "seek": 337758, "start": 3387.18, "end": 3391.9, "text": " Very... Yeah, same thing here. That's what I was talking. Okay. Margin losses.", "tokens": [50364, 467, 311, 257, 857, 264, 912, 1558, 382, 733, 295, 44559, 11363, 2787, 41167, 293, 3671, 3565, 22119, 13, 50744, 50844, 4372, 485, 865, 11, 912, 551, 510, 13, 663, 311, 437, 286, 390, 1417, 13, 1033, 13, 2039, 1494, 15352, 13, 51080, 51080, 407, 341, 307, 1333, 295, 364, 1021, 7719, 295, 15352, 13, 3950, 15352, 1936, 584, 11, 51364, 51464, 498, 286, 362, 11, 294, 341, 1389, 11, 732, 15743, 11, 51576, 51692], "temperature": 0.0, "avg_logprob": -0.36926254668793124, "compression_ratio": 1.450777202072539, "no_speech_prob": 2.331988525838824e-06}, {"id": 570, "seek": 337758, "start": 3391.9, "end": 3397.58, "text": " So this is sort of an important category of losses. Those losses basically say,", "tokens": [50364, 467, 311, 257, 857, 264, 912, 1558, 382, 733, 295, 44559, 11363, 2787, 41167, 293, 3671, 3565, 22119, 13, 50744, 50844, 4372, 485, 865, 11, 912, 551, 510, 13, 663, 311, 437, 286, 390, 1417, 13, 1033, 13, 2039, 1494, 15352, 13, 51080, 51080, 407, 341, 307, 1333, 295, 364, 1021, 7719, 295, 15352, 13, 3950, 15352, 1936, 584, 11, 51364, 51464, 498, 286, 362, 11, 294, 341, 1389, 11, 732, 15743, 11, 51576, 51692], "temperature": 0.0, "avg_logprob": -0.36926254668793124, "compression_ratio": 1.450777202072539, "no_speech_prob": 2.331988525838824e-06}, {"id": 571, "seek": 337758, "start": 3399.58, "end": 3401.8199999999997, "text": " if I have, in this case, two inputs,", "tokens": [50364, 467, 311, 257, 857, 264, 912, 1558, 382, 733, 295, 44559, 11363, 2787, 41167, 293, 3671, 3565, 22119, 13, 50744, 50844, 4372, 485, 865, 11, 912, 551, 510, 13, 663, 311, 437, 286, 390, 1417, 13, 1033, 13, 2039, 1494, 15352, 13, 51080, 51080, 407, 341, 307, 1333, 295, 364, 1021, 7719, 295, 15352, 13, 3950, 15352, 1936, 584, 11, 51364, 51464, 498, 286, 362, 11, 294, 341, 1389, 11, 732, 15743, 11, 51576, 51692], "temperature": 0.0, "avg_logprob": -0.36926254668793124, "compression_ratio": 1.450777202072539, "no_speech_prob": 2.331988525838824e-06}, {"id": 572, "seek": 340182, "start": 3401.82, "end": 3408.86, "text": " the loss function here says, I want one input to be larger than the other one by at least a margin.", "tokens": [50364, 264, 4470, 2445, 510, 1619, 11, 286, 528, 472, 4846, 281, 312, 4833, 813, 264, 661, 472, 538, 412, 1935, 257, 10270, 13, 50716, 50760, 1033, 13, 407, 3811, 264, 732, 15743, 366, 13444, 337, 732, 10479, 13, 509, 528, 264, 6175, 337, 50996, 50996, 264, 3006, 7719, 281, 312, 4833, 813, 264, 6175, 337, 264, 18424, 7719, 11, 51144, 51144, 538, 412, 1935, 512, 10270, 300, 291, 1320, 281, 264, 1185, 13, 51308, 51428, 400, 550, 264, 1036, 2445, 510, 1619, 11, 286, 528, 264, 1036, 4846, 281, 312, 4833, 813, 264, 661, 472, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.3459248398289536, "compression_ratio": 2.130890052356021, "no_speech_prob": 6.338618277368369e-06}, {"id": 573, "seek": 340182, "start": 3409.7400000000002, "end": 3414.46, "text": " Okay. So imagine the two inputs are scores for two categories. You want the score for", "tokens": [50364, 264, 4470, 2445, 510, 1619, 11, 286, 528, 472, 4846, 281, 312, 4833, 813, 264, 661, 472, 538, 412, 1935, 257, 10270, 13, 50716, 50760, 1033, 13, 407, 3811, 264, 732, 15743, 366, 13444, 337, 732, 10479, 13, 509, 528, 264, 6175, 337, 50996, 50996, 264, 3006, 7719, 281, 312, 4833, 813, 264, 6175, 337, 264, 18424, 7719, 11, 51144, 51144, 538, 412, 1935, 512, 10270, 300, 291, 1320, 281, 264, 1185, 13, 51308, 51428, 400, 550, 264, 1036, 2445, 510, 1619, 11, 286, 528, 264, 1036, 4846, 281, 312, 4833, 813, 264, 661, 472, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.3459248398289536, "compression_ratio": 2.130890052356021, "no_speech_prob": 6.338618277368369e-06}, {"id": 574, "seek": 340182, "start": 3414.46, "end": 3417.42, "text": " the correct category to be larger than the score for the incorrect category,", "tokens": [50364, 264, 4470, 2445, 510, 1619, 11, 286, 528, 472, 4846, 281, 312, 4833, 813, 264, 661, 472, 538, 412, 1935, 257, 10270, 13, 50716, 50760, 1033, 13, 407, 3811, 264, 732, 15743, 366, 13444, 337, 732, 10479, 13, 509, 528, 264, 6175, 337, 50996, 50996, 264, 3006, 7719, 281, 312, 4833, 813, 264, 6175, 337, 264, 18424, 7719, 11, 51144, 51144, 538, 412, 1935, 512, 10270, 300, 291, 1320, 281, 264, 1185, 13, 51308, 51428, 400, 550, 264, 1036, 2445, 510, 1619, 11, 286, 528, 264, 1036, 4846, 281, 312, 4833, 813, 264, 661, 472, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.3459248398289536, "compression_ratio": 2.130890052356021, "no_speech_prob": 6.338618277368369e-06}, {"id": 575, "seek": 340182, "start": 3417.42, "end": 3420.7000000000003, "text": " by at least some margin that you pass to the system.", "tokens": [50364, 264, 4470, 2445, 510, 1619, 11, 286, 528, 472, 4846, 281, 312, 4833, 813, 264, 661, 472, 538, 412, 1935, 257, 10270, 13, 50716, 50760, 1033, 13, 407, 3811, 264, 732, 15743, 366, 13444, 337, 732, 10479, 13, 509, 528, 264, 6175, 337, 50996, 50996, 264, 3006, 7719, 281, 312, 4833, 813, 264, 6175, 337, 264, 18424, 7719, 11, 51144, 51144, 538, 412, 1935, 512, 10270, 300, 291, 1320, 281, 264, 1185, 13, 51308, 51428, 400, 550, 264, 1036, 2445, 510, 1619, 11, 286, 528, 264, 1036, 4846, 281, 312, 4833, 813, 264, 661, 472, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.3459248398289536, "compression_ratio": 2.130890052356021, "no_speech_prob": 6.338618277368369e-06}, {"id": 576, "seek": 340182, "start": 3423.1000000000004, "end": 3429.02, "text": " And then the last function here says, I want the last input to be larger than the other one", "tokens": [50364, 264, 4470, 2445, 510, 1619, 11, 286, 528, 472, 4846, 281, 312, 4833, 813, 264, 661, 472, 538, 412, 1935, 257, 10270, 13, 50716, 50760, 1033, 13, 407, 3811, 264, 732, 15743, 366, 13444, 337, 732, 10479, 13, 509, 528, 264, 6175, 337, 50996, 50996, 264, 3006, 7719, 281, 312, 4833, 813, 264, 6175, 337, 264, 18424, 7719, 11, 51144, 51144, 538, 412, 1935, 512, 10270, 300, 291, 1320, 281, 264, 1185, 13, 51308, 51428, 400, 550, 264, 1036, 2445, 510, 1619, 11, 286, 528, 264, 1036, 4846, 281, 312, 4833, 813, 264, 661, 472, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.3459248398289536, "compression_ratio": 2.130890052356021, "no_speech_prob": 6.338618277368369e-06}, {"id": 577, "seek": 342902, "start": 3429.02, "end": 3435.58, "text": " by at least a margin. And that's the formula you see down there. So it's basically a hinge.", "tokens": [50364, 538, 412, 1935, 257, 10270, 13, 400, 300, 311, 264, 8513, 291, 536, 760, 456, 13, 407, 309, 311, 1936, 257, 28822, 13, 50692, 50692, 1033, 13, 400, 309, 2516, 264, 2649, 1296, 264, 732, 13444, 13, 400, 370, 288, 307, 257, 17434, 7006, 13, 639, 1804, 50912, 50912, 472, 420, 3175, 472, 11, 293, 309, 9003, 1968, 291, 528, 2031, 281, 312, 4833, 813, 2031, 16, 281, 312, 4833, 813, 2031, 17, 11, 51184, 51236, 420, 1968, 291, 528, 2031, 17, 281, 312, 4833, 813, 2031, 16, 13, 1033, 13, 492, 1936, 976, 309, 732, 13444, 11, 51484, 51484, 293, 291, 980, 309, 597, 472, 291, 528, 281, 312, 264, 6443, 6175, 13, 400, 550, 264, 2063, 2445, 1619, 11, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.15737571716308593, "compression_ratio": 1.852, "no_speech_prob": 4.2227529775118455e-06}, {"id": 578, "seek": 342902, "start": 3435.58, "end": 3439.98, "text": " Okay. And it takes the difference between the two scores. And so y is a binary variable. This plus", "tokens": [50364, 538, 412, 1935, 257, 10270, 13, 400, 300, 311, 264, 8513, 291, 536, 760, 456, 13, 407, 309, 311, 1936, 257, 28822, 13, 50692, 50692, 1033, 13, 400, 309, 2516, 264, 2649, 1296, 264, 732, 13444, 13, 400, 370, 288, 307, 257, 17434, 7006, 13, 639, 1804, 50912, 50912, 472, 420, 3175, 472, 11, 293, 309, 9003, 1968, 291, 528, 2031, 281, 312, 4833, 813, 2031, 16, 281, 312, 4833, 813, 2031, 17, 11, 51184, 51236, 420, 1968, 291, 528, 2031, 17, 281, 312, 4833, 813, 2031, 16, 13, 1033, 13, 492, 1936, 976, 309, 732, 13444, 11, 51484, 51484, 293, 291, 980, 309, 597, 472, 291, 528, 281, 312, 264, 6443, 6175, 13, 400, 550, 264, 2063, 2445, 1619, 11, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.15737571716308593, "compression_ratio": 1.852, "no_speech_prob": 4.2227529775118455e-06}, {"id": 579, "seek": 342902, "start": 3439.98, "end": 3445.42, "text": " one or minus one, and it controls whether you want x to be larger than x1 to be larger than x2,", "tokens": [50364, 538, 412, 1935, 257, 10270, 13, 400, 300, 311, 264, 8513, 291, 536, 760, 456, 13, 407, 309, 311, 1936, 257, 28822, 13, 50692, 50692, 1033, 13, 400, 309, 2516, 264, 2649, 1296, 264, 732, 13444, 13, 400, 370, 288, 307, 257, 17434, 7006, 13, 639, 1804, 50912, 50912, 472, 420, 3175, 472, 11, 293, 309, 9003, 1968, 291, 528, 2031, 281, 312, 4833, 813, 2031, 16, 281, 312, 4833, 813, 2031, 17, 11, 51184, 51236, 420, 1968, 291, 528, 2031, 17, 281, 312, 4833, 813, 2031, 16, 13, 1033, 13, 492, 1936, 976, 309, 732, 13444, 11, 51484, 51484, 293, 291, 980, 309, 597, 472, 291, 528, 281, 312, 264, 6443, 6175, 13, 400, 550, 264, 2063, 2445, 1619, 11, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.15737571716308593, "compression_ratio": 1.852, "no_speech_prob": 4.2227529775118455e-06}, {"id": 580, "seek": 342902, "start": 3446.46, "end": 3451.42, "text": " or whether you want x2 to be larger than x1. Okay. We basically give it two scores,", "tokens": [50364, 538, 412, 1935, 257, 10270, 13, 400, 300, 311, 264, 8513, 291, 536, 760, 456, 13, 407, 309, 311, 1936, 257, 28822, 13, 50692, 50692, 1033, 13, 400, 309, 2516, 264, 2649, 1296, 264, 732, 13444, 13, 400, 370, 288, 307, 257, 17434, 7006, 13, 639, 1804, 50912, 50912, 472, 420, 3175, 472, 11, 293, 309, 9003, 1968, 291, 528, 2031, 281, 312, 4833, 813, 2031, 16, 281, 312, 4833, 813, 2031, 17, 11, 51184, 51236, 420, 1968, 291, 528, 2031, 17, 281, 312, 4833, 813, 2031, 16, 13, 1033, 13, 492, 1936, 976, 309, 732, 13444, 11, 51484, 51484, 293, 291, 980, 309, 597, 472, 291, 528, 281, 312, 264, 6443, 6175, 13, 400, 550, 264, 2063, 2445, 1619, 11, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.15737571716308593, "compression_ratio": 1.852, "no_speech_prob": 4.2227529775118455e-06}, {"id": 581, "seek": 342902, "start": 3451.42, "end": 3457.74, "text": " and you tell it which one you want to be the largest score. And then the cost function says,", "tokens": [50364, 538, 412, 1935, 257, 10270, 13, 400, 300, 311, 264, 8513, 291, 536, 760, 456, 13, 407, 309, 311, 1936, 257, 28822, 13, 50692, 50692, 1033, 13, 400, 309, 2516, 264, 2649, 1296, 264, 732, 13444, 13, 400, 370, 288, 307, 257, 17434, 7006, 13, 639, 1804, 50912, 50912, 472, 420, 3175, 472, 11, 293, 309, 9003, 1968, 291, 528, 2031, 281, 312, 4833, 813, 2031, 16, 281, 312, 4833, 813, 2031, 17, 11, 51184, 51236, 420, 1968, 291, 528, 2031, 17, 281, 312, 4833, 813, 2031, 16, 13, 1033, 13, 492, 1936, 976, 309, 732, 13444, 11, 51484, 51484, 293, 291, 980, 309, 597, 472, 291, 528, 281, 312, 264, 6443, 6175, 13, 400, 550, 264, 2063, 2445, 1619, 11, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.15737571716308593, "compression_ratio": 1.852, "no_speech_prob": 4.2227529775118455e-06}, {"id": 582, "seek": 345774, "start": 3457.74, "end": 3462.7, "text": " you know, if this one is larger than that one by at least a margin, then the cost is zero.", "tokens": [50364, 291, 458, 11, 498, 341, 472, 307, 4833, 813, 300, 472, 538, 412, 1935, 257, 10270, 11, 550, 264, 2063, 307, 4018, 13, 50612, 50612, 759, 309, 311, 4356, 813, 264, 10270, 11, 420, 498, 309, 311, 294, 264, 661, 3513, 11, 50832, 50832, 550, 264, 2063, 8637, 43586, 13, 1033, 13, 407, 300, 311, 1219, 257, 28822, 4470, 13, 1033, 13, 51044, 51352, 407, 300, 311, 588, 4420, 337, 257, 1230, 295, 819, 721, 13, 492, 600, 1612, 364, 1365, 295, 341, 294, 51692], "temperature": 0.0, "avg_logprob": -0.07358589939687443, "compression_ratio": 1.6615384615384616, "no_speech_prob": 2.6425377654959448e-06}, {"id": 583, "seek": 345774, "start": 3462.7, "end": 3467.1, "text": " If it's smaller than the margin, or if it's in the other direction,", "tokens": [50364, 291, 458, 11, 498, 341, 472, 307, 4833, 813, 300, 472, 538, 412, 1935, 257, 10270, 11, 550, 264, 2063, 307, 4018, 13, 50612, 50612, 759, 309, 311, 4356, 813, 264, 10270, 11, 420, 498, 309, 311, 294, 264, 661, 3513, 11, 50832, 50832, 550, 264, 2063, 8637, 43586, 13, 1033, 13, 407, 300, 311, 1219, 257, 28822, 4470, 13, 1033, 13, 51044, 51352, 407, 300, 311, 588, 4420, 337, 257, 1230, 295, 819, 721, 13, 492, 600, 1612, 364, 1365, 295, 341, 294, 51692], "temperature": 0.0, "avg_logprob": -0.07358589939687443, "compression_ratio": 1.6615384615384616, "no_speech_prob": 2.6425377654959448e-06}, {"id": 584, "seek": 345774, "start": 3467.1, "end": 3471.3399999999997, "text": " then the cost increases linearly. Okay. So that's called a hinge loss. Okay.", "tokens": [50364, 291, 458, 11, 498, 341, 472, 307, 4833, 813, 300, 472, 538, 412, 1935, 257, 10270, 11, 550, 264, 2063, 307, 4018, 13, 50612, 50612, 759, 309, 311, 4356, 813, 264, 10270, 11, 420, 498, 309, 311, 294, 264, 661, 3513, 11, 50832, 50832, 550, 264, 2063, 8637, 43586, 13, 1033, 13, 407, 300, 311, 1219, 257, 28822, 4470, 13, 1033, 13, 51044, 51352, 407, 300, 311, 588, 4420, 337, 257, 1230, 295, 819, 721, 13, 492, 600, 1612, 364, 1365, 295, 341, 294, 51692], "temperature": 0.0, "avg_logprob": -0.07358589939687443, "compression_ratio": 1.6615384615384616, "no_speech_prob": 2.6425377654959448e-06}, {"id": 585, "seek": 347134, "start": 3471.34, "end": 3484.3, "text": " So that's very useful for a number of different things. We've seen an example of this in...", "tokens": [50364, 407, 300, 311, 588, 4420, 337, 257, 1230, 295, 819, 721, 13, 492, 600, 1612, 364, 1365, 295, 341, 294, 485, 51012, 51212, 407, 11, 1338, 11, 337, 1365, 11, 370, 341, 307, 1333, 295, 257, 10270, 17833, 4470, 13, 407, 291, 362, 732, 4190, 11, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.14325930595397948, "compression_ratio": 1.3636363636363635, "no_speech_prob": 2.1565856513916515e-06}, {"id": 586, "seek": 347134, "start": 3488.3, "end": 3499.7400000000002, "text": " So, yeah, for example, so this is sort of a margin ranking loss. So you have two values,", "tokens": [50364, 407, 300, 311, 588, 4420, 337, 257, 1230, 295, 819, 721, 13, 492, 600, 1612, 364, 1365, 295, 341, 294, 485, 51012, 51212, 407, 11, 1338, 11, 337, 1365, 11, 370, 341, 307, 1333, 295, 257, 10270, 17833, 4470, 13, 407, 291, 362, 732, 4190, 11, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.14325930595397948, "compression_ratio": 1.3636363636363635, "no_speech_prob": 2.1565856513916515e-06}, {"id": 587, "seek": 349974, "start": 3499.74, "end": 3504.9399999999996, "text": " but there are sort of... There's a simplified version of it. I mean, there's a simpler version", "tokens": [50364, 457, 456, 366, 1333, 295, 485, 821, 311, 257, 26335, 3037, 295, 309, 13, 286, 914, 11, 456, 311, 257, 18587, 3037, 50624, 50624, 295, 309, 11, 597, 286, 500, 380, 362, 510, 337, 512, 1778, 13, 492, 787, 362, 364, 2031, 13, 1033, 13, 407, 1936, 264, 4470, 307, 50944, 50944, 11469, 295, 4018, 293, 3175, 2031, 1413, 264, 10270, 13, 400, 309, 445, 2738, 281, 652, 2031, 4356, 813, 264, 10270, 13, 51524, 51568, 1779, 13, 400, 370, 341, 307, 1333, 295, 257, 2121, 1389, 689, 291, 362, 257, 17833, 1296, 732, 13444, 11, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10434062957763672, "compression_ratio": 1.6111111111111112, "no_speech_prob": 6.144081908132648e-06}, {"id": 588, "seek": 349974, "start": 3504.9399999999996, "end": 3511.3399999999997, "text": " of it, which I don't have here for some reason. We only have an x. Okay. So basically the loss is", "tokens": [50364, 457, 456, 366, 1333, 295, 485, 821, 311, 257, 26335, 3037, 295, 309, 13, 286, 914, 11, 456, 311, 257, 18587, 3037, 50624, 50624, 295, 309, 11, 597, 286, 500, 380, 362, 510, 337, 512, 1778, 13, 492, 787, 362, 364, 2031, 13, 1033, 13, 407, 1936, 264, 4470, 307, 50944, 50944, 11469, 295, 4018, 293, 3175, 2031, 1413, 264, 10270, 13, 400, 309, 445, 2738, 281, 652, 2031, 4356, 813, 264, 10270, 13, 51524, 51568, 1779, 13, 400, 370, 341, 307, 1333, 295, 257, 2121, 1389, 689, 291, 362, 257, 17833, 1296, 732, 13444, 11, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10434062957763672, "compression_ratio": 1.6111111111111112, "no_speech_prob": 6.144081908132648e-06}, {"id": 589, "seek": 349974, "start": 3511.3399999999997, "end": 3522.9399999999996, "text": " max of zero and minus x times the margin. And it just wants to make x smaller than the margin.", "tokens": [50364, 457, 456, 366, 1333, 295, 485, 821, 311, 257, 26335, 3037, 295, 309, 13, 286, 914, 11, 456, 311, 257, 18587, 3037, 50624, 50624, 295, 309, 11, 597, 286, 500, 380, 362, 510, 337, 512, 1778, 13, 492, 787, 362, 364, 2031, 13, 1033, 13, 407, 1936, 264, 4470, 307, 50944, 50944, 11469, 295, 4018, 293, 3175, 2031, 1413, 264, 10270, 13, 400, 309, 445, 2738, 281, 652, 2031, 4356, 813, 264, 10270, 13, 51524, 51568, 1779, 13, 400, 370, 341, 307, 1333, 295, 257, 2121, 1389, 689, 291, 362, 257, 17833, 1296, 732, 13444, 11, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10434062957763672, "compression_ratio": 1.6111111111111112, "no_speech_prob": 6.144081908132648e-06}, {"id": 590, "seek": 349974, "start": 3523.8199999999997, "end": 3529.5, "text": " Right. And so this is sort of a special case where you have a ranking between two scores,", "tokens": [50364, 457, 456, 366, 1333, 295, 485, 821, 311, 257, 26335, 3037, 295, 309, 13, 286, 914, 11, 456, 311, 257, 18587, 3037, 50624, 50624, 295, 309, 11, 597, 286, 500, 380, 362, 510, 337, 512, 1778, 13, 492, 787, 362, 364, 2031, 13, 1033, 13, 407, 1936, 264, 4470, 307, 50944, 50944, 11469, 295, 4018, 293, 3175, 2031, 1413, 264, 10270, 13, 400, 309, 445, 2738, 281, 652, 2031, 4356, 813, 264, 10270, 13, 51524, 51568, 1779, 13, 400, 370, 341, 307, 1333, 295, 257, 2121, 1389, 689, 291, 362, 257, 17833, 1296, 732, 13444, 11, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10434062957763672, "compression_ratio": 1.6111111111111112, "no_speech_prob": 6.144081908132648e-06}, {"id": 591, "seek": 352950, "start": 3529.5, "end": 3535.18, "text": " of two categories. So here is how you would use this for classification. You would basically", "tokens": [50364, 295, 732, 10479, 13, 407, 510, 307, 577, 291, 576, 764, 341, 337, 21538, 13, 509, 576, 1936, 50648, 50692, 1190, 428, 1508, 9902, 13, 509, 576, 483, 13444, 13, 1033, 13, 407, 949, 291, 360, 604, 2107, 12, 1889, 17409, 32807, 34499, 11, 51020, 51068, 293, 550, 291, 458, 264, 3006, 7719, 13, 407, 291, 584, 11, 286, 528, 341, 3006, 7719, 281, 362, 51316, 51316, 257, 1090, 6175, 13, 400, 550, 437, 291, 360, 307, 291, 747, 1071, 7719, 300, 575, 264, 881, 766, 2029, 51664, 51696], "temperature": 0.0, "avg_logprob": -0.07479998305603698, "compression_ratio": 1.7464788732394365, "no_speech_prob": 8.664420420245733e-06}, {"id": 592, "seek": 352950, "start": 3536.06, "end": 3542.62, "text": " run your classifier. You would get scores. Okay. So before you do any non-linearity weighted sums,", "tokens": [50364, 295, 732, 10479, 13, 407, 510, 307, 577, 291, 576, 764, 341, 337, 21538, 13, 509, 576, 1936, 50648, 50692, 1190, 428, 1508, 9902, 13, 509, 576, 483, 13444, 13, 1033, 13, 407, 949, 291, 360, 604, 2107, 12, 1889, 17409, 32807, 34499, 11, 51020, 51068, 293, 550, 291, 458, 264, 3006, 7719, 13, 407, 291, 584, 11, 286, 528, 341, 3006, 7719, 281, 362, 51316, 51316, 257, 1090, 6175, 13, 400, 550, 437, 291, 360, 307, 291, 747, 1071, 7719, 300, 575, 264, 881, 766, 2029, 51664, 51696], "temperature": 0.0, "avg_logprob": -0.07479998305603698, "compression_ratio": 1.7464788732394365, "no_speech_prob": 8.664420420245733e-06}, {"id": 593, "seek": 352950, "start": 3543.58, "end": 3548.54, "text": " and then you know the correct category. So you say, I want this correct category to have", "tokens": [50364, 295, 732, 10479, 13, 407, 510, 307, 577, 291, 576, 764, 341, 337, 21538, 13, 509, 576, 1936, 50648, 50692, 1190, 428, 1508, 9902, 13, 509, 576, 483, 13444, 13, 1033, 13, 407, 949, 291, 360, 604, 2107, 12, 1889, 17409, 32807, 34499, 11, 51020, 51068, 293, 550, 291, 458, 264, 3006, 7719, 13, 407, 291, 584, 11, 286, 528, 341, 3006, 7719, 281, 362, 51316, 51316, 257, 1090, 6175, 13, 400, 550, 437, 291, 360, 307, 291, 747, 1071, 7719, 300, 575, 264, 881, 766, 2029, 51664, 51696], "temperature": 0.0, "avg_logprob": -0.07479998305603698, "compression_ratio": 1.7464788732394365, "no_speech_prob": 8.664420420245733e-06}, {"id": 594, "seek": 352950, "start": 3548.54, "end": 3555.5, "text": " a high score. And then what you do is you take another category that has the most offending", "tokens": [50364, 295, 732, 10479, 13, 407, 510, 307, 577, 291, 576, 764, 341, 337, 21538, 13, 509, 576, 1936, 50648, 50692, 1190, 428, 1508, 9902, 13, 509, 576, 483, 13444, 13, 1033, 13, 407, 949, 291, 360, 604, 2107, 12, 1889, 17409, 32807, 34499, 11, 51020, 51068, 293, 550, 291, 458, 264, 3006, 7719, 13, 407, 291, 584, 11, 286, 528, 341, 3006, 7719, 281, 362, 51316, 51316, 257, 1090, 6175, 13, 400, 550, 437, 291, 360, 307, 291, 747, 1071, 7719, 300, 575, 264, 881, 766, 2029, 51664, 51696], "temperature": 0.0, "avg_logprob": -0.07479998305603698, "compression_ratio": 1.7464788732394365, "no_speech_prob": 8.664420420245733e-06}, {"id": 595, "seek": 355550, "start": 3555.5, "end": 3561.74, "text": " score. So either another category, so a category that is incorrect, that has a higher score than", "tokens": [50364, 6175, 13, 407, 2139, 1071, 7719, 11, 370, 257, 7719, 300, 307, 18424, 11, 300, 575, 257, 2946, 6175, 813, 50676, 50676, 264, 3006, 472, 11, 420, 300, 575, 257, 3126, 6175, 11, 457, 264, 3126, 6175, 307, 886, 1998, 13, 1033, 13, 407, 291, 747, 50924, 50988, 264, 7719, 300, 6104, 6175, 307, 264, 13699, 281, 264, 3006, 472, 11, 420, 6104, 6175, 307, 2946, 813, 51364, 51364, 264, 3006, 472, 13, 400, 291, 3154, 729, 732, 13444, 281, 257, 4470, 2445, 411, 341, 13, 407, 1936, 309, 311, 516, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.15745392849570827, "compression_ratio": 2.031413612565445, "no_speech_prob": 1.8924854884971865e-05}, {"id": 596, "seek": 355550, "start": 3561.74, "end": 3566.7, "text": " the correct one, or that has a lower score, but the lower score is too close. Okay. So you take", "tokens": [50364, 6175, 13, 407, 2139, 1071, 7719, 11, 370, 257, 7719, 300, 307, 18424, 11, 300, 575, 257, 2946, 6175, 813, 50676, 50676, 264, 3006, 472, 11, 420, 300, 575, 257, 3126, 6175, 11, 457, 264, 3126, 6175, 307, 886, 1998, 13, 1033, 13, 407, 291, 747, 50924, 50988, 264, 7719, 300, 6104, 6175, 307, 264, 13699, 281, 264, 3006, 472, 11, 420, 6104, 6175, 307, 2946, 813, 51364, 51364, 264, 3006, 472, 13, 400, 291, 3154, 729, 732, 13444, 281, 257, 4470, 2445, 411, 341, 13, 407, 1936, 309, 311, 516, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.15745392849570827, "compression_ratio": 2.031413612565445, "no_speech_prob": 1.8924854884971865e-05}, {"id": 597, "seek": 355550, "start": 3567.98, "end": 3575.5, "text": " the category that whose score is the closest to the correct one, or whose score is higher than", "tokens": [50364, 6175, 13, 407, 2139, 1071, 7719, 11, 370, 257, 7719, 300, 307, 18424, 11, 300, 575, 257, 2946, 6175, 813, 50676, 50676, 264, 3006, 472, 11, 420, 300, 575, 257, 3126, 6175, 11, 457, 264, 3126, 6175, 307, 886, 1998, 13, 1033, 13, 407, 291, 747, 50924, 50988, 264, 7719, 300, 6104, 6175, 307, 264, 13699, 281, 264, 3006, 472, 11, 420, 6104, 6175, 307, 2946, 813, 51364, 51364, 264, 3006, 472, 13, 400, 291, 3154, 729, 732, 13444, 281, 257, 4470, 2445, 411, 341, 13, 407, 1936, 309, 311, 516, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.15745392849570827, "compression_ratio": 2.031413612565445, "no_speech_prob": 1.8924854884971865e-05}, {"id": 598, "seek": 355550, "start": 3575.5, "end": 3582.14, "text": " the correct one. And you feed those two scores to a loss function like this. So basically it's going", "tokens": [50364, 6175, 13, 407, 2139, 1071, 7719, 11, 370, 257, 7719, 300, 307, 18424, 11, 300, 575, 257, 2946, 6175, 813, 50676, 50676, 264, 3006, 472, 11, 420, 300, 575, 257, 3126, 6175, 11, 457, 264, 3126, 6175, 307, 886, 1998, 13, 1033, 13, 407, 291, 747, 50924, 50988, 264, 7719, 300, 6104, 6175, 307, 264, 13699, 281, 264, 3006, 472, 11, 420, 6104, 6175, 307, 2946, 813, 51364, 51364, 264, 3006, 472, 13, 400, 291, 3154, 729, 732, 13444, 281, 257, 4470, 2445, 411, 341, 13, 407, 1936, 309, 311, 516, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.15745392849570827, "compression_ratio": 2.031413612565445, "no_speech_prob": 1.8924854884971865e-05}, {"id": 599, "seek": 358214, "start": 3582.14, "end": 3585.8199999999997, "text": " to push up the score of the correct category, push down the score of the incorrect category,", "tokens": [50364, 281, 2944, 493, 264, 6175, 295, 264, 3006, 7719, 11, 2944, 760, 264, 6175, 295, 264, 18424, 7719, 11, 50548, 50548, 1826, 264, 2649, 307, 412, 1935, 2681, 281, 264, 10270, 13, 1033, 13, 400, 300, 311, 257, 6239, 665, 636, 295, 50912, 50996, 3097, 746, 294, 264, 4319, 295, 364, 2281, 12, 6032, 2316, 13, 1171, 1365, 11, 300, 311, 472, 295, 51164, 51164, 264, 721, 291, 1062, 528, 281, 360, 13, 509, 1062, 528, 281, 584, 2031, 16, 11, 420, 3175, 2031, 16, 11, 307, 264, 2281, 11, 286, 914, 11, 51544, 51572, 3175, 2031, 16, 576, 312, 264, 2281, 295, 264, 3006, 1867, 11, 293, 3175, 2031, 17, 576, 312, 264, 2281, 295, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.14902114868164062, "compression_ratio": 1.896265560165975, "no_speech_prob": 1.221836282638833e-05}, {"id": 600, "seek": 358214, "start": 3585.8199999999997, "end": 3593.1, "text": " until the difference is at least equal to the margin. Okay. And that's a perfectly good way of", "tokens": [50364, 281, 2944, 493, 264, 6175, 295, 264, 3006, 7719, 11, 2944, 760, 264, 6175, 295, 264, 18424, 7719, 11, 50548, 50548, 1826, 264, 2649, 307, 412, 1935, 2681, 281, 264, 10270, 13, 1033, 13, 400, 300, 311, 257, 6239, 665, 636, 295, 50912, 50996, 3097, 746, 294, 264, 4319, 295, 364, 2281, 12, 6032, 2316, 13, 1171, 1365, 11, 300, 311, 472, 295, 51164, 51164, 264, 721, 291, 1062, 528, 281, 360, 13, 509, 1062, 528, 281, 584, 2031, 16, 11, 420, 3175, 2031, 16, 11, 307, 264, 2281, 11, 286, 914, 11, 51544, 51572, 3175, 2031, 16, 576, 312, 264, 2281, 295, 264, 3006, 1867, 11, 293, 3175, 2031, 17, 576, 312, 264, 2281, 295, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.14902114868164062, "compression_ratio": 1.896265560165975, "no_speech_prob": 1.221836282638833e-05}, {"id": 601, "seek": 358214, "start": 3594.7799999999997, "end": 3598.14, "text": " training something in the context of an energy-based model. For example, that's one of", "tokens": [50364, 281, 2944, 493, 264, 6175, 295, 264, 3006, 7719, 11, 2944, 760, 264, 6175, 295, 264, 18424, 7719, 11, 50548, 50548, 1826, 264, 2649, 307, 412, 1935, 2681, 281, 264, 10270, 13, 1033, 13, 400, 300, 311, 257, 6239, 665, 636, 295, 50912, 50996, 3097, 746, 294, 264, 4319, 295, 364, 2281, 12, 6032, 2316, 13, 1171, 1365, 11, 300, 311, 472, 295, 51164, 51164, 264, 721, 291, 1062, 528, 281, 360, 13, 509, 1062, 528, 281, 584, 2031, 16, 11, 420, 3175, 2031, 16, 11, 307, 264, 2281, 11, 286, 914, 11, 51544, 51572, 3175, 2031, 16, 576, 312, 264, 2281, 295, 264, 3006, 1867, 11, 293, 3175, 2031, 17, 576, 312, 264, 2281, 295, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.14902114868164062, "compression_ratio": 1.896265560165975, "no_speech_prob": 1.221836282638833e-05}, {"id": 602, "seek": 358214, "start": 3598.14, "end": 3605.74, "text": " the things you might want to do. You might want to say x1, or minus x1, is the energy, I mean,", "tokens": [50364, 281, 2944, 493, 264, 6175, 295, 264, 3006, 7719, 11, 2944, 760, 264, 6175, 295, 264, 18424, 7719, 11, 50548, 50548, 1826, 264, 2649, 307, 412, 1935, 2681, 281, 264, 10270, 13, 1033, 13, 400, 300, 311, 257, 6239, 665, 636, 295, 50912, 50996, 3097, 746, 294, 264, 4319, 295, 364, 2281, 12, 6032, 2316, 13, 1171, 1365, 11, 300, 311, 472, 295, 51164, 51164, 264, 721, 291, 1062, 528, 281, 360, 13, 509, 1062, 528, 281, 584, 2031, 16, 11, 420, 3175, 2031, 16, 11, 307, 264, 2281, 11, 286, 914, 11, 51544, 51572, 3175, 2031, 16, 576, 312, 264, 2281, 295, 264, 3006, 1867, 11, 293, 3175, 2031, 17, 576, 312, 264, 2281, 295, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.14902114868164062, "compression_ratio": 1.896265560165975, "no_speech_prob": 1.221836282638833e-05}, {"id": 603, "seek": 358214, "start": 3606.2999999999997, "end": 3611.58, "text": " minus x1 would be the energy of the correct answer, and minus x2 would be the energy of", "tokens": [50364, 281, 2944, 493, 264, 6175, 295, 264, 3006, 7719, 11, 2944, 760, 264, 6175, 295, 264, 18424, 7719, 11, 50548, 50548, 1826, 264, 2649, 307, 412, 1935, 2681, 281, 264, 10270, 13, 1033, 13, 400, 300, 311, 257, 6239, 665, 636, 295, 50912, 50996, 3097, 746, 294, 264, 4319, 295, 364, 2281, 12, 6032, 2316, 13, 1171, 1365, 11, 300, 311, 472, 295, 51164, 51164, 264, 721, 291, 1062, 528, 281, 360, 13, 509, 1062, 528, 281, 584, 2031, 16, 11, 420, 3175, 2031, 16, 11, 307, 264, 2281, 11, 286, 914, 11, 51544, 51572, 3175, 2031, 16, 576, 312, 264, 2281, 295, 264, 3006, 1867, 11, 293, 3175, 2031, 17, 576, 312, 264, 2281, 295, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.14902114868164062, "compression_ratio": 1.896265560165975, "no_speech_prob": 1.221836282638833e-05}, {"id": 604, "seek": 361158, "start": 3611.58, "end": 3619.02, "text": " the incorrect answer, like a contrastive term, an incorrect answer. And you want to push down", "tokens": [50364, 264, 18424, 1867, 11, 411, 257, 8712, 488, 1433, 11, 364, 18424, 1867, 13, 400, 291, 528, 281, 2944, 760, 50736, 50736, 264, 2281, 295, 264, 3006, 1867, 11, 2944, 493, 264, 2281, 295, 18424, 1867, 11, 370, 300, 264, 2649, 50908, 50908, 307, 412, 1935, 512, 10270, 13, 1033, 13, 509, 393, 764, 341, 733, 295, 4470, 337, 300, 13, 440, 1376, 14657, 4470, 307, 516, 51272, 51272, 281, 312, 257, 1895, 30229, 322, 341, 13, 407, 341, 307, 1143, 257, 688, 337, 20678, 2539, 11, 337, 264, 733, 295, 4832, 347, 311, 36170, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.10232920598502111, "compression_ratio": 1.759090909090909, "no_speech_prob": 4.710628218163038e-06}, {"id": 605, "seek": 361158, "start": 3619.02, "end": 3622.46, "text": " the energy of the correct answer, push up the energy of incorrect answer, so that the difference", "tokens": [50364, 264, 18424, 1867, 11, 411, 257, 8712, 488, 1433, 11, 364, 18424, 1867, 13, 400, 291, 528, 281, 2944, 760, 50736, 50736, 264, 2281, 295, 264, 3006, 1867, 11, 2944, 493, 264, 2281, 295, 18424, 1867, 11, 370, 300, 264, 2649, 50908, 50908, 307, 412, 1935, 512, 10270, 13, 1033, 13, 509, 393, 764, 341, 733, 295, 4470, 337, 300, 13, 440, 1376, 14657, 4470, 307, 516, 51272, 51272, 281, 312, 257, 1895, 30229, 322, 341, 13, 407, 341, 307, 1143, 257, 688, 337, 20678, 2539, 11, 337, 264, 733, 295, 4832, 347, 311, 36170, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.10232920598502111, "compression_ratio": 1.759090909090909, "no_speech_prob": 4.710628218163038e-06}, {"id": 606, "seek": 361158, "start": 3622.46, "end": 3629.74, "text": " is at least some margin. Okay. You can use this kind of loss for that. The triplet loss is going", "tokens": [50364, 264, 18424, 1867, 11, 411, 257, 8712, 488, 1433, 11, 364, 18424, 1867, 13, 400, 291, 528, 281, 2944, 760, 50736, 50736, 264, 2281, 295, 264, 3006, 1867, 11, 2944, 493, 264, 2281, 295, 18424, 1867, 11, 370, 300, 264, 2649, 50908, 50908, 307, 412, 1935, 512, 10270, 13, 1033, 13, 509, 393, 764, 341, 733, 295, 4470, 337, 300, 13, 440, 1376, 14657, 4470, 307, 516, 51272, 51272, 281, 312, 257, 1895, 30229, 322, 341, 13, 407, 341, 307, 1143, 257, 688, 337, 20678, 2539, 11, 337, 264, 733, 295, 4832, 347, 311, 36170, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.10232920598502111, "compression_ratio": 1.759090909090909, "no_speech_prob": 4.710628218163038e-06}, {"id": 607, "seek": 361158, "start": 3629.74, "end": 3635.2599999999998, "text": " to be a refinement on this. So this is used a lot for metric learning, for the kind of Samir's nets", "tokens": [50364, 264, 18424, 1867, 11, 411, 257, 8712, 488, 1433, 11, 364, 18424, 1867, 13, 400, 291, 528, 281, 2944, 760, 50736, 50736, 264, 2281, 295, 264, 3006, 1867, 11, 2944, 493, 264, 2281, 295, 18424, 1867, 11, 370, 300, 264, 2649, 50908, 50908, 307, 412, 1935, 512, 10270, 13, 1033, 13, 509, 393, 764, 341, 733, 295, 4470, 337, 300, 13, 440, 1376, 14657, 4470, 307, 516, 51272, 51272, 281, 312, 257, 1895, 30229, 322, 341, 13, 407, 341, 307, 1143, 257, 688, 337, 20678, 2539, 11, 337, 264, 733, 295, 4832, 347, 311, 36170, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.10232920598502111, "compression_ratio": 1.759090909090909, "no_speech_prob": 4.710628218163038e-06}, {"id": 608, "seek": 363526, "start": 3635.26, "end": 3643.26, "text": " that Ishan Mishra was talking about last week. And there the idea is, let's say I have", "tokens": [50364, 300, 1119, 3451, 376, 742, 424, 390, 1417, 466, 1036, 1243, 13, 400, 456, 264, 1558, 307, 11, 718, 311, 584, 286, 362, 50764, 50972, 257, 4560, 11, 420, 718, 311, 584, 286, 362, 1045, 10938, 13, 286, 362, 472, 6889, 293, 1071, 6889, 300, 311, 588, 51312, 51312, 2531, 281, 309, 13, 286, 1190, 552, 807, 732, 45216, 304, 36170, 13, 286, 483, 732, 18875, 13, 286, 14722, 264, 4560, 51536, 51536, 1296, 729, 732, 18875, 11, 274, 295, 257, 741, 280, 741, 11, 337, 1365, 13, 1033, 13, 286, 528, 281, 652, 341, 4560, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.130400915145874, "compression_ratio": 1.701834862385321, "no_speech_prob": 5.33717502548825e-06}, {"id": 609, "seek": 363526, "start": 3647.42, "end": 3654.2200000000003, "text": " a distance, or let's say I have three samples. I have one sample and another sample that's very", "tokens": [50364, 300, 1119, 3451, 376, 742, 424, 390, 1417, 466, 1036, 1243, 13, 400, 456, 264, 1558, 307, 11, 718, 311, 584, 286, 362, 50764, 50972, 257, 4560, 11, 420, 718, 311, 584, 286, 362, 1045, 10938, 13, 286, 362, 472, 6889, 293, 1071, 6889, 300, 311, 588, 51312, 51312, 2531, 281, 309, 13, 286, 1190, 552, 807, 732, 45216, 304, 36170, 13, 286, 483, 732, 18875, 13, 286, 14722, 264, 4560, 51536, 51536, 1296, 729, 732, 18875, 11, 274, 295, 257, 741, 280, 741, 11, 337, 1365, 13, 1033, 13, 286, 528, 281, 652, 341, 4560, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.130400915145874, "compression_ratio": 1.701834862385321, "no_speech_prob": 5.33717502548825e-06}, {"id": 610, "seek": 363526, "start": 3654.2200000000003, "end": 3658.7000000000003, "text": " similar to it. I run them through two convolutional nets. I get two vectors. I compute the distance", "tokens": [50364, 300, 1119, 3451, 376, 742, 424, 390, 1417, 466, 1036, 1243, 13, 400, 456, 264, 1558, 307, 11, 718, 311, 584, 286, 362, 50764, 50972, 257, 4560, 11, 420, 718, 311, 584, 286, 362, 1045, 10938, 13, 286, 362, 472, 6889, 293, 1071, 6889, 300, 311, 588, 51312, 51312, 2531, 281, 309, 13, 286, 1190, 552, 807, 732, 45216, 304, 36170, 13, 286, 483, 732, 18875, 13, 286, 14722, 264, 4560, 51536, 51536, 1296, 729, 732, 18875, 11, 274, 295, 257, 741, 280, 741, 11, 337, 1365, 13, 1033, 13, 286, 528, 281, 652, 341, 4560, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.130400915145874, "compression_ratio": 1.701834862385321, "no_speech_prob": 5.33717502548825e-06}, {"id": 611, "seek": 363526, "start": 3658.7000000000003, "end": 3664.7000000000003, "text": " between those two vectors, d of a i p i, for example. Okay. I want to make this distance", "tokens": [50364, 300, 1119, 3451, 376, 742, 424, 390, 1417, 466, 1036, 1243, 13, 400, 456, 264, 1558, 307, 11, 718, 311, 584, 286, 362, 50764, 50972, 257, 4560, 11, 420, 718, 311, 584, 286, 362, 1045, 10938, 13, 286, 362, 472, 6889, 293, 1071, 6889, 300, 311, 588, 51312, 51312, 2531, 281, 309, 13, 286, 1190, 552, 807, 732, 45216, 304, 36170, 13, 286, 483, 732, 18875, 13, 286, 14722, 264, 4560, 51536, 51536, 1296, 729, 732, 18875, 11, 274, 295, 257, 741, 280, 741, 11, 337, 1365, 13, 1033, 13, 286, 528, 281, 652, 341, 4560, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.130400915145874, "compression_ratio": 1.701834862385321, "no_speech_prob": 5.33717502548825e-06}, {"id": 612, "seek": 366470, "start": 3664.7, "end": 3669.8999999999996, "text": " as small as possible, because that's the correct sample. And then I take two samples that I know", "tokens": [50364, 382, 1359, 382, 1944, 11, 570, 300, 311, 264, 3006, 6889, 13, 400, 550, 286, 747, 732, 10938, 300, 286, 458, 50624, 50624, 366, 4361, 49505, 819, 13, 1033, 13, 440, 3256, 295, 257, 3857, 293, 472, 295, 257, 3199, 13, 400, 286, 528, 281, 652, 264, 50900, 50900, 18875, 1400, 1314, 490, 1184, 661, 13, 407, 286, 14722, 264, 4560, 11, 293, 286, 528, 281, 652, 341, 4560, 2416, 13, 51108, 51140, 1057, 558, 13, 823, 11, 286, 393, 13466, 300, 264, 700, 4560, 312, 4018, 11, 293, 286, 393, 13466, 300, 264, 51516, 51516, 1150, 4560, 312, 4833, 813, 264, 10270, 13, 663, 576, 312, 733, 295, 257, 10270, 4470, 2010, 551, 13, 51748, 51808], "temperature": 0.0, "avg_logprob": -0.08485850967279002, "compression_ratio": 1.7946768060836502, "no_speech_prob": 8.939405233832076e-06}, {"id": 613, "seek": 366470, "start": 3669.8999999999996, "end": 3675.4199999999996, "text": " are semantically different. Okay. The image of a cat and one of a table. And I want to make the", "tokens": [50364, 382, 1359, 382, 1944, 11, 570, 300, 311, 264, 3006, 6889, 13, 400, 550, 286, 747, 732, 10938, 300, 286, 458, 50624, 50624, 366, 4361, 49505, 819, 13, 1033, 13, 440, 3256, 295, 257, 3857, 293, 472, 295, 257, 3199, 13, 400, 286, 528, 281, 652, 264, 50900, 50900, 18875, 1400, 1314, 490, 1184, 661, 13, 407, 286, 14722, 264, 4560, 11, 293, 286, 528, 281, 652, 341, 4560, 2416, 13, 51108, 51140, 1057, 558, 13, 823, 11, 286, 393, 13466, 300, 264, 700, 4560, 312, 4018, 11, 293, 286, 393, 13466, 300, 264, 51516, 51516, 1150, 4560, 312, 4833, 813, 264, 10270, 13, 663, 576, 312, 733, 295, 257, 10270, 4470, 2010, 551, 13, 51748, 51808], "temperature": 0.0, "avg_logprob": -0.08485850967279002, "compression_ratio": 1.7946768060836502, "no_speech_prob": 8.939405233832076e-06}, {"id": 614, "seek": 366470, "start": 3675.4199999999996, "end": 3679.58, "text": " vectors far away from each other. So I compute the distance, and I want to make this distance large.", "tokens": [50364, 382, 1359, 382, 1944, 11, 570, 300, 311, 264, 3006, 6889, 13, 400, 550, 286, 747, 732, 10938, 300, 286, 458, 50624, 50624, 366, 4361, 49505, 819, 13, 1033, 13, 440, 3256, 295, 257, 3857, 293, 472, 295, 257, 3199, 13, 400, 286, 528, 281, 652, 264, 50900, 50900, 18875, 1400, 1314, 490, 1184, 661, 13, 407, 286, 14722, 264, 4560, 11, 293, 286, 528, 281, 652, 341, 4560, 2416, 13, 51108, 51140, 1057, 558, 13, 823, 11, 286, 393, 13466, 300, 264, 700, 4560, 312, 4018, 11, 293, 286, 393, 13466, 300, 264, 51516, 51516, 1150, 4560, 312, 4833, 813, 264, 10270, 13, 663, 576, 312, 733, 295, 257, 10270, 4470, 2010, 551, 13, 51748, 51808], "temperature": 0.0, "avg_logprob": -0.08485850967279002, "compression_ratio": 1.7946768060836502, "no_speech_prob": 8.939405233832076e-06}, {"id": 615, "seek": 366470, "start": 3680.22, "end": 3687.74, "text": " All right. Now, I can insist that the first distance be zero, and I can insist that the", "tokens": [50364, 382, 1359, 382, 1944, 11, 570, 300, 311, 264, 3006, 6889, 13, 400, 550, 286, 747, 732, 10938, 300, 286, 458, 50624, 50624, 366, 4361, 49505, 819, 13, 1033, 13, 440, 3256, 295, 257, 3857, 293, 472, 295, 257, 3199, 13, 400, 286, 528, 281, 652, 264, 50900, 50900, 18875, 1400, 1314, 490, 1184, 661, 13, 407, 286, 14722, 264, 4560, 11, 293, 286, 528, 281, 652, 341, 4560, 2416, 13, 51108, 51140, 1057, 558, 13, 823, 11, 286, 393, 13466, 300, 264, 700, 4560, 312, 4018, 11, 293, 286, 393, 13466, 300, 264, 51516, 51516, 1150, 4560, 312, 4833, 813, 264, 10270, 13, 663, 576, 312, 733, 295, 257, 10270, 4470, 2010, 551, 13, 51748, 51808], "temperature": 0.0, "avg_logprob": -0.08485850967279002, "compression_ratio": 1.7946768060836502, "no_speech_prob": 8.939405233832076e-06}, {"id": 616, "seek": 366470, "start": 3687.74, "end": 3692.3799999999997, "text": " second distance be larger than the margin. That would be kind of a margin loss type thing.", "tokens": [50364, 382, 1359, 382, 1944, 11, 570, 300, 311, 264, 3006, 6889, 13, 400, 550, 286, 747, 732, 10938, 300, 286, 458, 50624, 50624, 366, 4361, 49505, 819, 13, 1033, 13, 440, 3256, 295, 257, 3857, 293, 472, 295, 257, 3199, 13, 400, 286, 528, 281, 652, 264, 50900, 50900, 18875, 1400, 1314, 490, 1184, 661, 13, 407, 286, 14722, 264, 4560, 11, 293, 286, 528, 281, 652, 341, 4560, 2416, 13, 51108, 51140, 1057, 558, 13, 823, 11, 286, 393, 13466, 300, 264, 700, 4560, 312, 4018, 11, 293, 286, 393, 13466, 300, 264, 51516, 51516, 1150, 4560, 312, 4833, 813, 264, 10270, 13, 663, 576, 312, 733, 295, 257, 10270, 4470, 2010, 551, 13, 51748, 51808], "temperature": 0.0, "avg_logprob": -0.08485850967279002, "compression_ratio": 1.7946768060836502, "no_speech_prob": 8.939405233832076e-06}, {"id": 617, "seek": 369238, "start": 3692.38, "end": 3697.6600000000003, "text": " But what I can do is one of those triplet margin loss, where I say the only thing I care about is", "tokens": [50364, 583, 437, 286, 393, 360, 307, 472, 295, 729, 1376, 14657, 10270, 4470, 11, 689, 286, 584, 264, 787, 551, 286, 1127, 466, 307, 50628, 50628, 300, 264, 4560, 300, 286, 483, 337, 264, 665, 6119, 307, 4356, 813, 264, 4560, 300, 286, 483, 337, 264, 1578, 50888, 50888, 6119, 13, 286, 500, 380, 1127, 498, 264, 4560, 307, 1359, 13, 286, 445, 528, 309, 281, 312, 4356, 813, 264, 4560, 337, 51156, 51156, 264, 1578, 6119, 13, 1033, 13, 400, 300, 311, 437, 729, 17833, 6064, 360, 13, 316, 3840, 295, 729, 645, 11, 51588, 51708], "temperature": 0.0, "avg_logprob": -0.17719929627697878, "compression_ratio": 1.9479166666666667, "no_speech_prob": 3.726559498318238e-06}, {"id": 618, "seek": 369238, "start": 3697.6600000000003, "end": 3702.86, "text": " that the distance that I get for the good pair is smaller than the distance that I get for the bad", "tokens": [50364, 583, 437, 286, 393, 360, 307, 472, 295, 729, 1376, 14657, 10270, 4470, 11, 689, 286, 584, 264, 787, 551, 286, 1127, 466, 307, 50628, 50628, 300, 264, 4560, 300, 286, 483, 337, 264, 665, 6119, 307, 4356, 813, 264, 4560, 300, 286, 483, 337, 264, 1578, 50888, 50888, 6119, 13, 286, 500, 380, 1127, 498, 264, 4560, 307, 1359, 13, 286, 445, 528, 309, 281, 312, 4356, 813, 264, 4560, 337, 51156, 51156, 264, 1578, 6119, 13, 1033, 13, 400, 300, 311, 437, 729, 17833, 6064, 360, 13, 316, 3840, 295, 729, 645, 11, 51588, 51708], "temperature": 0.0, "avg_logprob": -0.17719929627697878, "compression_ratio": 1.9479166666666667, "no_speech_prob": 3.726559498318238e-06}, {"id": 619, "seek": 369238, "start": 3702.86, "end": 3708.2200000000003, "text": " pair. I don't care if the distance is small. I just want it to be smaller than the distance for", "tokens": [50364, 583, 437, 286, 393, 360, 307, 472, 295, 729, 1376, 14657, 10270, 4470, 11, 689, 286, 584, 264, 787, 551, 286, 1127, 466, 307, 50628, 50628, 300, 264, 4560, 300, 286, 483, 337, 264, 665, 6119, 307, 4356, 813, 264, 4560, 300, 286, 483, 337, 264, 1578, 50888, 50888, 6119, 13, 286, 500, 380, 1127, 498, 264, 4560, 307, 1359, 13, 286, 445, 528, 309, 281, 312, 4356, 813, 264, 4560, 337, 51156, 51156, 264, 1578, 6119, 13, 1033, 13, 400, 300, 311, 437, 729, 17833, 6064, 360, 13, 316, 3840, 295, 729, 645, 11, 51588, 51708], "temperature": 0.0, "avg_logprob": -0.17719929627697878, "compression_ratio": 1.9479166666666667, "no_speech_prob": 3.726559498318238e-06}, {"id": 620, "seek": 369238, "start": 3708.2200000000003, "end": 3716.86, "text": " the bad pair. Okay. And that's what those ranking laws do. A bunch of those were,", "tokens": [50364, 583, 437, 286, 393, 360, 307, 472, 295, 729, 1376, 14657, 10270, 4470, 11, 689, 286, 584, 264, 787, 551, 286, 1127, 466, 307, 50628, 50628, 300, 264, 4560, 300, 286, 483, 337, 264, 665, 6119, 307, 4356, 813, 264, 4560, 300, 286, 483, 337, 264, 1578, 50888, 50888, 6119, 13, 286, 500, 380, 1127, 498, 264, 4560, 307, 1359, 13, 286, 445, 528, 309, 281, 312, 4356, 813, 264, 4560, 337, 51156, 51156, 264, 1578, 6119, 13, 1033, 13, 400, 300, 311, 437, 729, 17833, 6064, 360, 13, 316, 3840, 295, 729, 645, 11, 51588, 51708], "temperature": 0.0, "avg_logprob": -0.17719929627697878, "compression_ratio": 1.9479166666666667, "no_speech_prob": 3.726559498318238e-06}, {"id": 621, "seek": 371686, "start": 3716.86, "end": 3724.46, "text": " one of the first, I think, that was proposed was by Jason Weston and Sami Benjo, back when Jason", "tokens": [50364, 472, 295, 264, 700, 11, 286, 519, 11, 300, 390, 10348, 390, 538, 11181, 4055, 266, 293, 44029, 3964, 5134, 11, 646, 562, 11181, 50744, 50744, 4055, 266, 390, 920, 412, 3329, 13, 400, 436, 1143, 341, 281, 3847, 733, 295, 364, 3256, 3164, 1185, 337, 3329, 13, 51068, 51108, 407, 646, 550, 11, 286, 478, 406, 988, 309, 311, 2074, 3602, 11, 457, 646, 550, 291, 576, 2010, 257, 14581, 322, 3329, 13, 51348, 51348, 3329, 576, 2058, 1429, 300, 14581, 666, 257, 8062, 13, 1396, 321, 6794, 341, 281, 257, 1379, 3840, 295, 18875, 11, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.18309562985259709, "compression_ratio": 1.6452991452991452, "no_speech_prob": 2.026119773290702e-06}, {"id": 622, "seek": 371686, "start": 3724.46, "end": 3730.94, "text": " Weston was still at Google. And they used this to train kind of an image search system for Google.", "tokens": [50364, 472, 295, 264, 700, 11, 286, 519, 11, 300, 390, 10348, 390, 538, 11181, 4055, 266, 293, 44029, 3964, 5134, 11, 646, 562, 11181, 50744, 50744, 4055, 266, 390, 920, 412, 3329, 13, 400, 436, 1143, 341, 281, 3847, 733, 295, 364, 3256, 3164, 1185, 337, 3329, 13, 51068, 51108, 407, 646, 550, 11, 286, 478, 406, 988, 309, 311, 2074, 3602, 11, 457, 646, 550, 291, 576, 2010, 257, 14581, 322, 3329, 13, 51348, 51348, 3329, 576, 2058, 1429, 300, 14581, 666, 257, 8062, 13, 1396, 321, 6794, 341, 281, 257, 1379, 3840, 295, 18875, 11, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.18309562985259709, "compression_ratio": 1.6452991452991452, "no_speech_prob": 2.026119773290702e-06}, {"id": 623, "seek": 371686, "start": 3731.7400000000002, "end": 3736.54, "text": " So back then, I'm not sure it's true anymore, but back then you would type a query on Google.", "tokens": [50364, 472, 295, 264, 700, 11, 286, 519, 11, 300, 390, 10348, 390, 538, 11181, 4055, 266, 293, 44029, 3964, 5134, 11, 646, 562, 11181, 50744, 50744, 4055, 266, 390, 920, 412, 3329, 13, 400, 436, 1143, 341, 281, 3847, 733, 295, 364, 3256, 3164, 1185, 337, 3329, 13, 51068, 51108, 407, 646, 550, 11, 286, 478, 406, 988, 309, 311, 2074, 3602, 11, 457, 646, 550, 291, 576, 2010, 257, 14581, 322, 3329, 13, 51348, 51348, 3329, 576, 2058, 1429, 300, 14581, 666, 257, 8062, 13, 1396, 321, 6794, 341, 281, 257, 1379, 3840, 295, 18875, 11, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.18309562985259709, "compression_ratio": 1.6452991452991452, "no_speech_prob": 2.026119773290702e-06}, {"id": 624, "seek": 371686, "start": 3736.54, "end": 3742.3, "text": " Google would encode that query into a vector. Then we compare this to a whole bunch of vectors,", "tokens": [50364, 472, 295, 264, 700, 11, 286, 519, 11, 300, 390, 10348, 390, 538, 11181, 4055, 266, 293, 44029, 3964, 5134, 11, 646, 562, 11181, 50744, 50744, 4055, 266, 390, 920, 412, 3329, 13, 400, 436, 1143, 341, 281, 3847, 733, 295, 364, 3256, 3164, 1185, 337, 3329, 13, 51068, 51108, 407, 646, 550, 11, 286, 478, 406, 988, 309, 311, 2074, 3602, 11, 457, 646, 550, 291, 576, 2010, 257, 14581, 322, 3329, 13, 51348, 51348, 3329, 576, 2058, 1429, 300, 14581, 666, 257, 8062, 13, 1396, 321, 6794, 341, 281, 257, 1379, 3840, 295, 18875, 11, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.18309562985259709, "compression_ratio": 1.6452991452991452, "no_speech_prob": 2.026119773290702e-06}, {"id": 625, "seek": 374230, "start": 3742.3, "end": 3750.46, "text": " describing images that have been previously indexed. And then we kind of retrieve the images", "tokens": [50364, 16141, 5267, 300, 362, 668, 8046, 8186, 292, 13, 400, 550, 321, 733, 295, 30254, 264, 5267, 50772, 50772, 6104, 8062, 645, 1998, 281, 264, 472, 300, 291, 632, 13, 400, 264, 636, 291, 3847, 729, 9590, 300, 14722, 51108, 51108, 729, 18875, 11, 294, 300, 1389, 11, 646, 550, 309, 390, 8213, 9590, 11, 767, 11, 307, 291, 3847, 552, 365, 51372, 51372, 729, 1376, 14657, 4470, 13, 1033, 13, 407, 291, 848, 11, 665, 8664, 337, 452, 3164, 820, 312, 1570, 813, 472, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.30172551472981773, "compression_ratio": 1.7045454545454546, "no_speech_prob": 3.089455958615872e-06}, {"id": 626, "seek": 374230, "start": 3750.46, "end": 3757.1800000000003, "text": " whose vector were close to the one that you had. And the way you train those networks that compute", "tokens": [50364, 16141, 5267, 300, 362, 668, 8046, 8186, 292, 13, 400, 550, 321, 733, 295, 30254, 264, 5267, 50772, 50772, 6104, 8062, 645, 1998, 281, 264, 472, 300, 291, 632, 13, 400, 264, 636, 291, 3847, 729, 9590, 300, 14722, 51108, 51108, 729, 18875, 11, 294, 300, 1389, 11, 646, 550, 309, 390, 8213, 9590, 11, 767, 11, 307, 291, 3847, 552, 365, 51372, 51372, 729, 1376, 14657, 4470, 13, 1033, 13, 407, 291, 848, 11, 665, 8664, 337, 452, 3164, 820, 312, 1570, 813, 472, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.30172551472981773, "compression_ratio": 1.7045454545454546, "no_speech_prob": 3.089455958615872e-06}, {"id": 627, "seek": 374230, "start": 3757.1800000000003, "end": 3762.46, "text": " those vectors, in that case, back then it was linear networks, actually, is you train them with", "tokens": [50364, 16141, 5267, 300, 362, 668, 8046, 8186, 292, 13, 400, 550, 321, 733, 295, 30254, 264, 5267, 50772, 50772, 6104, 8062, 645, 1998, 281, 264, 472, 300, 291, 632, 13, 400, 264, 636, 291, 3847, 729, 9590, 300, 14722, 51108, 51108, 729, 18875, 11, 294, 300, 1389, 11, 646, 550, 309, 390, 8213, 9590, 11, 767, 11, 307, 291, 3847, 552, 365, 51372, 51372, 729, 1376, 14657, 4470, 13, 1033, 13, 407, 291, 848, 11, 665, 8664, 337, 452, 3164, 820, 312, 1570, 813, 472, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.30172551472981773, "compression_ratio": 1.7045454545454546, "no_speech_prob": 3.089455958615872e-06}, {"id": 628, "seek": 374230, "start": 3762.46, "end": 3770.46, "text": " those triplet loss. Okay. So you said, good hits for my search should be less than one.", "tokens": [50364, 16141, 5267, 300, 362, 668, 8046, 8186, 292, 13, 400, 550, 321, 733, 295, 30254, 264, 5267, 50772, 50772, 6104, 8062, 645, 1998, 281, 264, 472, 300, 291, 632, 13, 400, 264, 636, 291, 3847, 729, 9590, 300, 14722, 51108, 51108, 729, 18875, 11, 294, 300, 1389, 11, 646, 550, 309, 390, 8213, 9590, 11, 767, 11, 307, 291, 3847, 552, 365, 51372, 51372, 729, 1376, 14657, 4470, 13, 1033, 13, 407, 291, 848, 11, 665, 8664, 337, 452, 3164, 820, 312, 1570, 813, 472, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.30172551472981773, "compression_ratio": 1.7045454545454546, "no_speech_prob": 3.089455958615872e-06}, {"id": 629, "seek": 377046, "start": 3770.46, "end": 3775.82, "text": " So the search should have a distance between the vectors that is smaller than any bad hit.", "tokens": [50364, 407, 264, 3164, 820, 362, 257, 4560, 1296, 264, 18875, 300, 307, 4356, 813, 604, 1578, 2045, 13, 50632, 50632, 400, 286, 500, 380, 1127, 498, 264, 4560, 307, 1359, 13, 286, 445, 528, 309, 281, 312, 4356, 813, 337, 1578, 8664, 13, 50816, 50920, 2639, 1168, 30, 50948, 51200, 663, 311, 733, 295, 257, 35942, 10835, 295, 341, 11, 689, 430, 307, 257, 3353, 6889, 13, 407, 309, 311, 2531, 281, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.17431062146237022, "compression_ratio": 1.5129533678756477, "no_speech_prob": 2.1443933292175643e-05}, {"id": 630, "seek": 377046, "start": 3775.82, "end": 3779.5, "text": " And I don't care if the distance is small. I just want it to be smaller than for bad hits.", "tokens": [50364, 407, 264, 3164, 820, 362, 257, 4560, 1296, 264, 18875, 300, 307, 4356, 813, 604, 1578, 2045, 13, 50632, 50632, 400, 286, 500, 380, 1127, 498, 264, 4560, 307, 1359, 13, 286, 445, 528, 309, 281, 312, 4356, 813, 337, 1578, 8664, 13, 50816, 50920, 2639, 1168, 30, 50948, 51200, 663, 311, 733, 295, 257, 35942, 10835, 295, 341, 11, 689, 430, 307, 257, 3353, 6889, 13, 407, 309, 311, 2531, 281, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.17431062146237022, "compression_ratio": 1.5129533678756477, "no_speech_prob": 2.1443933292175643e-05}, {"id": 631, "seek": 377046, "start": 3781.58, "end": 3782.14, "text": " Any question?", "tokens": [50364, 407, 264, 3164, 820, 362, 257, 4560, 1296, 264, 18875, 300, 307, 4356, 813, 604, 1578, 2045, 13, 50632, 50632, 400, 286, 500, 380, 1127, 498, 264, 4560, 307, 1359, 13, 286, 445, 528, 309, 281, 312, 4356, 813, 337, 1578, 8664, 13, 50816, 50920, 2639, 1168, 30, 50948, 51200, 663, 311, 733, 295, 257, 35942, 10835, 295, 341, 11, 689, 430, 307, 257, 3353, 6889, 13, 407, 309, 311, 2531, 281, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.17431062146237022, "compression_ratio": 1.5129533678756477, "no_speech_prob": 2.1443933292175643e-05}, {"id": 632, "seek": 377046, "start": 3787.18, "end": 3794.7, "text": " That's kind of a graphical explanation of this, where P is a positive sample. So it's similar to", "tokens": [50364, 407, 264, 3164, 820, 362, 257, 4560, 1296, 264, 18875, 300, 307, 4356, 813, 604, 1578, 2045, 13, 50632, 50632, 400, 286, 500, 380, 1127, 498, 264, 4560, 307, 1359, 13, 286, 445, 528, 309, 281, 312, 4356, 813, 337, 1578, 8664, 13, 50816, 50920, 2639, 1168, 30, 50948, 51200, 663, 311, 733, 295, 257, 35942, 10835, 295, 341, 11, 689, 430, 307, 257, 3353, 6889, 13, 407, 309, 311, 2531, 281, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.17431062146237022, "compression_ratio": 1.5129533678756477, "no_speech_prob": 2.1443933292175643e-05}, {"id": 633, "seek": 379470, "start": 3794.7, "end": 3801.18, "text": " A. So A is the sample you considered. P is kind of a positive sample. And N is a negative sample,", "tokens": [50364, 316, 13, 407, 316, 307, 264, 6889, 291, 4888, 13, 430, 307, 733, 295, 257, 3353, 6889, 13, 400, 426, 307, 257, 3671, 6889, 11, 50688, 50688, 257, 8712, 488, 6889, 13, 509, 528, 281, 2944, 426, 1314, 293, 1565, 430, 4966, 13, 400, 382, 2321, 382, 50968, 51004, 430, 307, 4966, 813, 426, 538, 512, 10270, 11, 291, 1590, 7380, 293, 8407, 13, 51180, 51512, 509, 362, 2787, 9606, 295, 341, 13, 400, 294, 1186, 11, 291, 393, 519, 295, 426, 4969, 11, 264, 733, 295, 4470, 2445, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.10040549821751092, "compression_ratio": 1.6231884057971016, "no_speech_prob": 3.3404915029677795e-06}, {"id": 634, "seek": 379470, "start": 3801.18, "end": 3806.7799999999997, "text": " a contrastive sample. You want to push N away and bring P closer. And as soon as", "tokens": [50364, 316, 13, 407, 316, 307, 264, 6889, 291, 4888, 13, 430, 307, 733, 295, 257, 3353, 6889, 13, 400, 426, 307, 257, 3671, 6889, 11, 50688, 50688, 257, 8712, 488, 6889, 13, 509, 528, 281, 2944, 426, 1314, 293, 1565, 430, 4966, 13, 400, 382, 2321, 382, 50968, 51004, 430, 307, 4966, 813, 426, 538, 512, 10270, 11, 291, 1590, 7380, 293, 8407, 13, 51180, 51512, 509, 362, 2787, 9606, 295, 341, 13, 400, 294, 1186, 11, 291, 393, 519, 295, 426, 4969, 11, 264, 733, 295, 4470, 2445, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.10040549821751092, "compression_ratio": 1.6231884057971016, "no_speech_prob": 3.3404915029677795e-06}, {"id": 635, "seek": 379470, "start": 3807.5, "end": 3811.02, "text": " P is closer than N by some margin, you stop pushing and pulling.", "tokens": [50364, 316, 13, 407, 316, 307, 264, 6889, 291, 4888, 13, 430, 307, 733, 295, 257, 3353, 6889, 13, 400, 426, 307, 257, 3671, 6889, 11, 50688, 50688, 257, 8712, 488, 6889, 13, 509, 528, 281, 2944, 426, 1314, 293, 1565, 430, 4966, 13, 400, 382, 2321, 382, 50968, 51004, 430, 307, 4966, 813, 426, 538, 512, 10270, 11, 291, 1590, 7380, 293, 8407, 13, 51180, 51512, 509, 362, 2787, 9606, 295, 341, 13, 400, 294, 1186, 11, 291, 393, 519, 295, 426, 4969, 11, 264, 733, 295, 4470, 2445, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.10040549821751092, "compression_ratio": 1.6231884057971016, "no_speech_prob": 3.3404915029677795e-06}, {"id": 636, "seek": 379470, "start": 3817.66, "end": 3822.06, "text": " You have soft versions of this. And in fact, you can think of NCE, the kind of loss function", "tokens": [50364, 316, 13, 407, 316, 307, 264, 6889, 291, 4888, 13, 430, 307, 733, 295, 257, 3353, 6889, 13, 400, 426, 307, 257, 3671, 6889, 11, 50688, 50688, 257, 8712, 488, 6889, 13, 509, 528, 281, 2944, 426, 1314, 293, 1565, 430, 4966, 13, 400, 382, 2321, 382, 50968, 51004, 430, 307, 4966, 813, 426, 538, 512, 10270, 11, 291, 1590, 7380, 293, 8407, 13, 51180, 51512, 509, 362, 2787, 9606, 295, 341, 13, 400, 294, 1186, 11, 291, 393, 519, 295, 426, 4969, 11, 264, 733, 295, 4470, 2445, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.10040549821751092, "compression_ratio": 1.6231884057971016, "no_speech_prob": 3.3404915029677795e-06}, {"id": 637, "seek": 382206, "start": 3822.06, "end": 3827.34, "text": " that Ishan was talking about, as kind of a soft version of that, where you basically,", "tokens": [50364, 300, 1119, 3451, 390, 1417, 466, 11, 382, 733, 295, 257, 2787, 3037, 295, 300, 11, 689, 291, 1936, 11, 50628, 50700, 291, 362, 257, 3840, 295, 35127, 293, 257, 3840, 295, 40019, 11, 420, 291, 362, 472, 3353, 293, 257, 3840, 50868, 50868, 295, 40019, 11, 293, 291, 1190, 552, 807, 257, 2787, 41167, 13, 400, 291, 584, 11, 286, 528, 264, 308, 281, 264, 3175, 4560, 51340, 51340, 337, 264, 3006, 472, 281, 312, 4356, 813, 308, 281, 264, 3175, 264, 661, 472, 13, 407, 309, 733, 295, 21020, 264, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.12987041473388672, "compression_ratio": 1.8137254901960784, "no_speech_prob": 8.397812052862719e-06}, {"id": 638, "seek": 382206, "start": 3828.7799999999997, "end": 3832.14, "text": " you have a bunch of positives and a bunch of negatives, or you have one positive and a bunch", "tokens": [50364, 300, 1119, 3451, 390, 1417, 466, 11, 382, 733, 295, 257, 2787, 3037, 295, 300, 11, 689, 291, 1936, 11, 50628, 50700, 291, 362, 257, 3840, 295, 35127, 293, 257, 3840, 295, 40019, 11, 420, 291, 362, 472, 3353, 293, 257, 3840, 50868, 50868, 295, 40019, 11, 293, 291, 1190, 552, 807, 257, 2787, 41167, 13, 400, 291, 584, 11, 286, 528, 264, 308, 281, 264, 3175, 4560, 51340, 51340, 337, 264, 3006, 472, 281, 312, 4356, 813, 308, 281, 264, 3175, 264, 661, 472, 13, 407, 309, 733, 295, 21020, 264, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.12987041473388672, "compression_ratio": 1.8137254901960784, "no_speech_prob": 8.397812052862719e-06}, {"id": 639, "seek": 382206, "start": 3832.14, "end": 3841.58, "text": " of negatives, and you run them through a softmax. And you say, I want the e to the minus distance", "tokens": [50364, 300, 1119, 3451, 390, 1417, 466, 11, 382, 733, 295, 257, 2787, 3037, 295, 300, 11, 689, 291, 1936, 11, 50628, 50700, 291, 362, 257, 3840, 295, 35127, 293, 257, 3840, 295, 40019, 11, 420, 291, 362, 472, 3353, 293, 257, 3840, 50868, 50868, 295, 40019, 11, 293, 291, 1190, 552, 807, 257, 2787, 41167, 13, 400, 291, 584, 11, 286, 528, 264, 308, 281, 264, 3175, 4560, 51340, 51340, 337, 264, 3006, 472, 281, 312, 4356, 813, 308, 281, 264, 3175, 264, 661, 472, 13, 407, 309, 733, 295, 21020, 264, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.12987041473388672, "compression_ratio": 1.8137254901960784, "no_speech_prob": 8.397812052862719e-06}, {"id": 640, "seek": 382206, "start": 3841.58, "end": 3851.42, "text": " for the correct one to be smaller than e to the minus the other one. So it kind of pushes the", "tokens": [50364, 300, 1119, 3451, 390, 1417, 466, 11, 382, 733, 295, 257, 2787, 3037, 295, 300, 11, 689, 291, 1936, 11, 50628, 50700, 291, 362, 257, 3840, 295, 35127, 293, 257, 3840, 295, 40019, 11, 420, 291, 362, 472, 3353, 293, 257, 3840, 50868, 50868, 295, 40019, 11, 293, 291, 1190, 552, 807, 257, 2787, 41167, 13, 400, 291, 584, 11, 286, 528, 264, 308, 281, 264, 3175, 4560, 51340, 51340, 337, 264, 3006, 472, 281, 312, 4356, 813, 308, 281, 264, 3175, 264, 661, 472, 13, 407, 309, 733, 295, 21020, 264, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.12987041473388672, "compression_ratio": 1.8137254901960784, "no_speech_prob": 8.397812052862719e-06}, {"id": 641, "seek": 385142, "start": 3851.42, "end": 3856.78, "text": " positive closer to you and pushes the other ones further to you, but now with some sort of softmaxy", "tokens": [50364, 3353, 4966, 281, 291, 293, 21020, 264, 661, 2306, 3052, 281, 291, 11, 457, 586, 365, 512, 1333, 295, 2787, 41167, 88, 50632, 50704, 21510, 21039, 11, 382, 8851, 281, 1333, 295, 257, 1152, 10270, 13, 50808, 51040, 407, 294, 9953, 51, 284, 339, 11, 291, 362, 721, 300, 2089, 291, 281, 362, 2120, 388, 18657, 13, 407, 341, 4045, 291, 281, 1936, 51304, 51304, 976, 3866, 3006, 23930, 13, 407, 2602, 295, 11, 341, 307, 257, 17833, 4470, 11, 457, 2602, 295, 13466, 278, 300, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.14764198727077907, "compression_ratio": 1.6572769953051643, "no_speech_prob": 7.295498562598368e-06}, {"id": 642, "seek": 385142, "start": 3858.2200000000003, "end": 3860.3, "text": " exponential decay, as opposed to sort of a hard margin.", "tokens": [50364, 3353, 4966, 281, 291, 293, 21020, 264, 661, 2306, 3052, 281, 291, 11, 457, 586, 365, 512, 1333, 295, 2787, 41167, 88, 50632, 50704, 21510, 21039, 11, 382, 8851, 281, 1333, 295, 257, 1152, 10270, 13, 50808, 51040, 407, 294, 9953, 51, 284, 339, 11, 291, 362, 721, 300, 2089, 291, 281, 362, 2120, 388, 18657, 13, 407, 341, 4045, 291, 281, 1936, 51304, 51304, 976, 3866, 3006, 23930, 13, 407, 2602, 295, 11, 341, 307, 257, 17833, 4470, 11, 457, 2602, 295, 13466, 278, 300, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.14764198727077907, "compression_ratio": 1.6572769953051643, "no_speech_prob": 7.295498562598368e-06}, {"id": 643, "seek": 385142, "start": 3864.94, "end": 3870.2200000000003, "text": " So in PyTorch, you have things that allow you to have multilabel. So this allows you to basically", "tokens": [50364, 3353, 4966, 281, 291, 293, 21020, 264, 661, 2306, 3052, 281, 291, 11, 457, 586, 365, 512, 1333, 295, 2787, 41167, 88, 50632, 50704, 21510, 21039, 11, 382, 8851, 281, 1333, 295, 257, 1152, 10270, 13, 50808, 51040, 407, 294, 9953, 51, 284, 339, 11, 291, 362, 721, 300, 2089, 291, 281, 362, 2120, 388, 18657, 13, 407, 341, 4045, 291, 281, 1936, 51304, 51304, 976, 3866, 3006, 23930, 13, 407, 2602, 295, 11, 341, 307, 257, 17833, 4470, 11, 457, 2602, 295, 13466, 278, 300, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.14764198727077907, "compression_ratio": 1.6572769953051643, "no_speech_prob": 7.295498562598368e-06}, {"id": 644, "seek": 385142, "start": 3870.2200000000003, "end": 3878.38, "text": " give multiple correct outputs. So instead of, this is a ranking loss, but instead of insisting that", "tokens": [50364, 3353, 4966, 281, 291, 293, 21020, 264, 661, 2306, 3052, 281, 291, 11, 457, 586, 365, 512, 1333, 295, 2787, 41167, 88, 50632, 50704, 21510, 21039, 11, 382, 8851, 281, 1333, 295, 257, 1152, 10270, 13, 50808, 51040, 407, 294, 9953, 51, 284, 339, 11, 291, 362, 721, 300, 2089, 291, 281, 362, 2120, 388, 18657, 13, 407, 341, 4045, 291, 281, 1936, 51304, 51304, 976, 3866, 3006, 23930, 13, 407, 2602, 295, 11, 341, 307, 257, 17833, 4470, 11, 457, 2602, 295, 13466, 278, 300, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.14764198727077907, "compression_ratio": 1.6572769953051643, "no_speech_prob": 7.295498562598368e-06}, {"id": 645, "seek": 387838, "start": 3878.38, "end": 3883.6600000000003, "text": " there is only one correct category, and you want a high score for the correct category and a bad", "tokens": [50364, 456, 307, 787, 472, 3006, 7719, 11, 293, 291, 528, 257, 1090, 6175, 337, 264, 3006, 7719, 293, 257, 1578, 50628, 50628, 6175, 337, 1203, 1646, 11, 510, 291, 393, 362, 257, 1230, 295, 10479, 337, 597, 291, 528, 1090, 13444, 11, 50876, 50916, 293, 550, 439, 264, 661, 2306, 486, 483, 9152, 1314, 13, 1057, 558, 11, 321, 603, 483, 11, 641, 13444, 486, 312, 9152, 51164, 51164, 760, 13, 407, 510, 309, 311, 257, 28822, 4470, 11, 457, 291, 360, 257, 2408, 295, 341, 28822, 4470, 670, 439, 10479, 13, 51516, 51684], "temperature": 0.0, "avg_logprob": -0.12865005930264792, "compression_ratio": 1.820754716981132, "no_speech_prob": 1.0782969184219837e-05}, {"id": 646, "seek": 387838, "start": 3883.6600000000003, "end": 3888.62, "text": " score for everything else, here you can have a number of categories for which you want high scores,", "tokens": [50364, 456, 307, 787, 472, 3006, 7719, 11, 293, 291, 528, 257, 1090, 6175, 337, 264, 3006, 7719, 293, 257, 1578, 50628, 50628, 6175, 337, 1203, 1646, 11, 510, 291, 393, 362, 257, 1230, 295, 10479, 337, 597, 291, 528, 1090, 13444, 11, 50876, 50916, 293, 550, 439, 264, 661, 2306, 486, 483, 9152, 1314, 13, 1057, 558, 11, 321, 603, 483, 11, 641, 13444, 486, 312, 9152, 51164, 51164, 760, 13, 407, 510, 309, 311, 257, 28822, 4470, 11, 457, 291, 360, 257, 2408, 295, 341, 28822, 4470, 670, 439, 10479, 13, 51516, 51684], "temperature": 0.0, "avg_logprob": -0.12865005930264792, "compression_ratio": 1.820754716981132, "no_speech_prob": 1.0782969184219837e-05}, {"id": 647, "seek": 387838, "start": 3889.42, "end": 3894.38, "text": " and then all the other ones will get pushed away. All right, we'll get, their scores will be pushed", "tokens": [50364, 456, 307, 787, 472, 3006, 7719, 11, 293, 291, 528, 257, 1090, 6175, 337, 264, 3006, 7719, 293, 257, 1578, 50628, 50628, 6175, 337, 1203, 1646, 11, 510, 291, 393, 362, 257, 1230, 295, 10479, 337, 597, 291, 528, 1090, 13444, 11, 50876, 50916, 293, 550, 439, 264, 661, 2306, 486, 483, 9152, 1314, 13, 1057, 558, 11, 321, 603, 483, 11, 641, 13444, 486, 312, 9152, 51164, 51164, 760, 13, 407, 510, 309, 311, 257, 28822, 4470, 11, 457, 291, 360, 257, 2408, 295, 341, 28822, 4470, 670, 439, 10479, 13, 51516, 51684], "temperature": 0.0, "avg_logprob": -0.12865005930264792, "compression_ratio": 1.820754716981132, "no_speech_prob": 1.0782969184219837e-05}, {"id": 648, "seek": 387838, "start": 3894.38, "end": 3901.42, "text": " down. So here it's a hinge loss, but you do a sum of this hinge loss over all categories.", "tokens": [50364, 456, 307, 787, 472, 3006, 7719, 11, 293, 291, 528, 257, 1090, 6175, 337, 264, 3006, 7719, 293, 257, 1578, 50628, 50628, 6175, 337, 1203, 1646, 11, 510, 291, 393, 362, 257, 1230, 295, 10479, 337, 597, 291, 528, 1090, 13444, 11, 50876, 50916, 293, 550, 439, 264, 661, 2306, 486, 483, 9152, 1314, 13, 1057, 558, 11, 321, 603, 483, 11, 641, 13444, 486, 312, 9152, 51164, 51164, 760, 13, 407, 510, 309, 311, 257, 28822, 4470, 11, 457, 291, 360, 257, 2408, 295, 341, 28822, 4470, 670, 439, 10479, 13, 51516, 51684], "temperature": 0.0, "avg_logprob": -0.12865005930264792, "compression_ratio": 1.820754716981132, "no_speech_prob": 1.0782969184219837e-05}, {"id": 649, "seek": 390142, "start": 3901.42, "end": 3907.66, "text": " And for each category, if the category is a desired one, you push it up. If it's a non-desired", "tokens": [50364, 400, 337, 1184, 7719, 11, 498, 264, 7719, 307, 257, 14721, 472, 11, 291, 2944, 309, 493, 13, 759, 309, 311, 257, 2107, 12, 14792, 1824, 50676, 50676, 472, 11, 291, 2944, 309, 760, 11, 597, 307, 437, 264, 383, 292, 3780, 8513, 1619, 13, 400, 295, 1164, 291, 362, 264, 2787, 51140, 51140, 3037, 295, 341, 11, 597, 286, 478, 406, 516, 281, 352, 666, 264, 4365, 295, 11, 293, 264, 4825, 12, 6209, 1494, 3037, 295, 309, 13, 51520, 51760], "temperature": 0.0, "avg_logprob": -0.2197443644205729, "compression_ratio": 1.6179775280898876, "no_speech_prob": 3.236886641388992e-06}, {"id": 650, "seek": 390142, "start": 3907.66, "end": 3916.94, "text": " one, you push it down, which is what the Cedula formula says. And of course you have the soft", "tokens": [50364, 400, 337, 1184, 7719, 11, 498, 264, 7719, 307, 257, 14721, 472, 11, 291, 2944, 309, 493, 13, 759, 309, 311, 257, 2107, 12, 14792, 1824, 50676, 50676, 472, 11, 291, 2944, 309, 760, 11, 597, 307, 437, 264, 383, 292, 3780, 8513, 1619, 13, 400, 295, 1164, 291, 362, 264, 2787, 51140, 51140, 3037, 295, 341, 11, 597, 286, 478, 406, 516, 281, 352, 666, 264, 4365, 295, 11, 293, 264, 4825, 12, 6209, 1494, 3037, 295, 309, 13, 51520, 51760], "temperature": 0.0, "avg_logprob": -0.2197443644205729, "compression_ratio": 1.6179775280898876, "no_speech_prob": 3.236886641388992e-06}, {"id": 651, "seek": 390142, "start": 3916.94, "end": 3924.54, "text": " version of this, which I'm not going to go into the details of, and the multi-margin version of it.", "tokens": [50364, 400, 337, 1184, 7719, 11, 498, 264, 7719, 307, 257, 14721, 472, 11, 291, 2944, 309, 493, 13, 759, 309, 311, 257, 2107, 12, 14792, 1824, 50676, 50676, 472, 11, 291, 2944, 309, 760, 11, 597, 307, 437, 264, 383, 292, 3780, 8513, 1619, 13, 400, 295, 1164, 291, 362, 264, 2787, 51140, 51140, 3037, 295, 341, 11, 597, 286, 478, 406, 516, 281, 352, 666, 264, 4365, 295, 11, 293, 264, 4825, 12, 6209, 1494, 3037, 295, 309, 13, 51520, 51760], "temperature": 0.0, "avg_logprob": -0.2197443644205729, "compression_ratio": 1.6179775280898876, "no_speech_prob": 3.236886641388992e-06}, {"id": 652, "seek": 392454, "start": 3924.54, "end": 3929.34, "text": " So this pushing and pulling for metric learning for embedding for sine these nets that I was", "tokens": [50364, 407, 341, 7380, 293, 8407, 337, 20678, 2539, 337, 12240, 3584, 337, 18609, 613, 36170, 300, 286, 390, 50604, 50604, 3585, 291, 466, 11, 309, 311, 767, 733, 295, 439, 12270, 498, 291, 528, 294, 472, 295, 729, 28822, 50928, 50928, 12240, 3584, 6064, 13, 407, 28822, 12240, 3584, 6064, 307, 257, 4470, 337, 18609, 613, 36170, 300, 733, 295, 21020, 51264, 51312, 721, 300, 366, 14232, 27965, 984, 2531, 281, 291, 293, 2944, 1314, 721, 300, 366, 406, 13, 1033, 11, 370, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.25790563122979526, "compression_ratio": 1.8564102564102565, "no_speech_prob": 4.710577741207089e-06}, {"id": 653, "seek": 392454, "start": 3929.34, "end": 3935.82, "text": " telling you about, it's actually kind of all implemented if you want in one of those hinge", "tokens": [50364, 407, 341, 7380, 293, 8407, 337, 20678, 2539, 337, 12240, 3584, 337, 18609, 613, 36170, 300, 286, 390, 50604, 50604, 3585, 291, 466, 11, 309, 311, 767, 733, 295, 439, 12270, 498, 291, 528, 294, 472, 295, 729, 28822, 50928, 50928, 12240, 3584, 6064, 13, 407, 28822, 12240, 3584, 6064, 307, 257, 4470, 337, 18609, 613, 36170, 300, 733, 295, 21020, 51264, 51312, 721, 300, 366, 14232, 27965, 984, 2531, 281, 291, 293, 2944, 1314, 721, 300, 366, 406, 13, 1033, 11, 370, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.25790563122979526, "compression_ratio": 1.8564102564102565, "no_speech_prob": 4.710577741207089e-06}, {"id": 654, "seek": 392454, "start": 3935.82, "end": 3942.54, "text": " embedding laws. So hinge embedding laws is a loss for sine these nets that kind of pushes", "tokens": [50364, 407, 341, 7380, 293, 8407, 337, 20678, 2539, 337, 12240, 3584, 337, 18609, 613, 36170, 300, 286, 390, 50604, 50604, 3585, 291, 466, 11, 309, 311, 767, 733, 295, 439, 12270, 498, 291, 528, 294, 472, 295, 729, 28822, 50928, 50928, 12240, 3584, 6064, 13, 407, 28822, 12240, 3584, 6064, 307, 257, 4470, 337, 18609, 613, 36170, 300, 733, 295, 21020, 51264, 51312, 721, 300, 366, 14232, 27965, 984, 2531, 281, 291, 293, 2944, 1314, 721, 300, 366, 406, 13, 1033, 11, 370, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.25790563122979526, "compression_ratio": 1.8564102564102565, "no_speech_prob": 4.710577741207089e-06}, {"id": 655, "seek": 392454, "start": 3943.5, "end": 3948.46, "text": " things that are symmetrically similar to you and push away things that are not. Okay, so", "tokens": [50364, 407, 341, 7380, 293, 8407, 337, 20678, 2539, 337, 12240, 3584, 337, 18609, 613, 36170, 300, 286, 390, 50604, 50604, 3585, 291, 466, 11, 309, 311, 767, 733, 295, 439, 12270, 498, 291, 528, 294, 472, 295, 729, 28822, 50928, 50928, 12240, 3584, 6064, 13, 407, 28822, 12240, 3584, 6064, 307, 257, 4470, 337, 18609, 613, 36170, 300, 733, 295, 21020, 51264, 51312, 721, 300, 366, 14232, 27965, 984, 2531, 281, 291, 293, 2944, 1314, 721, 300, 366, 406, 13, 1033, 11, 370, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.25790563122979526, "compression_ratio": 1.8564102564102565, "no_speech_prob": 4.710577741207089e-06}, {"id": 656, "seek": 394846, "start": 3948.46, "end": 3954.78, "text": " the Y variable indicates whether the pair you are, or whether the score you are giving to the system", "tokens": [50364, 264, 398, 7006, 16203, 1968, 264, 6119, 291, 366, 11, 420, 1968, 264, 6175, 291, 366, 2902, 281, 264, 1185, 50680, 50680, 307, 472, 300, 820, 312, 9152, 493, 420, 472, 300, 820, 312, 9152, 760, 13, 400, 309, 25963, 257, 28822, 4470, 300, 50960, 50960, 1669, 264, 6175, 3353, 498, 398, 307, 1804, 472, 11, 293, 309, 1669, 264, 6175, 3671, 538, 512, 10270, 8289, 51428, 51428, 498, 398, 307, 3175, 472, 13, 407, 341, 307, 257, 588, 2199, 1365, 295, 577, 281, 360, 341, 13, 407, 718, 311, 352, 2286, 293, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.4230827252889417, "compression_ratio": 1.7844036697247707, "no_speech_prob": 4.936885943607194e-06}, {"id": 657, "seek": 394846, "start": 3954.78, "end": 3960.38, "text": " is one that should be pushed up or one that should be pushed down. And it chooses a hinge loss that", "tokens": [50364, 264, 398, 7006, 16203, 1968, 264, 6119, 291, 366, 11, 420, 1968, 264, 6175, 291, 366, 2902, 281, 264, 1185, 50680, 50680, 307, 472, 300, 820, 312, 9152, 493, 420, 472, 300, 820, 312, 9152, 760, 13, 400, 309, 25963, 257, 28822, 4470, 300, 50960, 50960, 1669, 264, 6175, 3353, 498, 398, 307, 1804, 472, 11, 293, 309, 1669, 264, 6175, 3671, 538, 512, 10270, 8289, 51428, 51428, 498, 398, 307, 3175, 472, 13, 407, 341, 307, 257, 588, 2199, 1365, 295, 577, 281, 360, 341, 13, 407, 718, 311, 352, 2286, 293, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.4230827252889417, "compression_ratio": 1.7844036697247707, "no_speech_prob": 4.936885943607194e-06}, {"id": 658, "seek": 394846, "start": 3960.38, "end": 3969.7400000000002, "text": " makes the score positive if Y is plus one, and it makes the score negative by some margin delta", "tokens": [50364, 264, 398, 7006, 16203, 1968, 264, 6119, 291, 366, 11, 420, 1968, 264, 6175, 291, 366, 2902, 281, 264, 1185, 50680, 50680, 307, 472, 300, 820, 312, 9152, 493, 420, 472, 300, 820, 312, 9152, 760, 13, 400, 309, 25963, 257, 28822, 4470, 300, 50960, 50960, 1669, 264, 6175, 3353, 498, 398, 307, 1804, 472, 11, 293, 309, 1669, 264, 6175, 3671, 538, 512, 10270, 8289, 51428, 51428, 498, 398, 307, 3175, 472, 13, 407, 341, 307, 257, 588, 2199, 1365, 295, 577, 281, 360, 341, 13, 407, 718, 311, 352, 2286, 293, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.4230827252889417, "compression_ratio": 1.7844036697247707, "no_speech_prob": 4.936885943607194e-06}, {"id": 659, "seek": 394846, "start": 3969.7400000000002, "end": 3976.46, "text": " if Y is minus one. So this is a very simple example of how to do this. So let's go ahead and", "tokens": [50364, 264, 398, 7006, 16203, 1968, 264, 6119, 291, 366, 11, 420, 1968, 264, 6175, 291, 366, 2902, 281, 264, 1185, 50680, 50680, 307, 472, 300, 820, 312, 9152, 493, 420, 472, 300, 820, 312, 9152, 760, 13, 400, 309, 25963, 257, 28822, 4470, 300, 50960, 50960, 1669, 264, 6175, 3353, 498, 398, 307, 1804, 472, 11, 293, 309, 1669, 264, 6175, 3671, 538, 512, 10270, 8289, 51428, 51428, 498, 398, 307, 3175, 472, 13, 407, 341, 307, 257, 588, 2199, 1365, 295, 577, 281, 360, 341, 13, 407, 718, 311, 352, 2286, 293, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.4230827252889417, "compression_ratio": 1.7844036697247707, "no_speech_prob": 4.936885943607194e-06}, {"id": 660, "seek": 397646, "start": 3976.46, "end": 3990.54, "text": " if Y is minus one. Very often when you are doing sine these nets, the way you compute the similarity", "tokens": [50364, 498, 398, 307, 3175, 472, 13, 4372, 2049, 562, 291, 366, 884, 18609, 613, 36170, 11, 264, 636, 291, 14722, 264, 32194, 51068, 51068, 1296, 732, 18875, 307, 406, 807, 257, 462, 1311, 31264, 282, 4560, 457, 807, 257, 23565, 4560, 13, 407, 51312, 51312, 691, 16, 3175, 264, 23565, 295, 264, 5802, 1296, 264, 732, 18875, 13, 639, 307, 1936, 257, 48704, 51628, 51628], "temperature": 0.0, "avg_logprob": -0.1593320761153947, "compression_ratio": 1.576271186440678, "no_speech_prob": 3.986576120951213e-05}, {"id": 661, "seek": 397646, "start": 3990.54, "end": 3995.42, "text": " between two vectors is not through a Euclidean distance but through a cosine distance. So", "tokens": [50364, 498, 398, 307, 3175, 472, 13, 4372, 2049, 562, 291, 366, 884, 18609, 613, 36170, 11, 264, 636, 291, 14722, 264, 32194, 51068, 51068, 1296, 732, 18875, 307, 406, 807, 257, 462, 1311, 31264, 282, 4560, 457, 807, 257, 23565, 4560, 13, 407, 51312, 51312, 691, 16, 3175, 264, 23565, 295, 264, 5802, 1296, 264, 732, 18875, 13, 639, 307, 1936, 257, 48704, 51628, 51628], "temperature": 0.0, "avg_logprob": -0.1593320761153947, "compression_ratio": 1.576271186440678, "no_speech_prob": 3.986576120951213e-05}, {"id": 662, "seek": 397646, "start": 3995.42, "end": 4001.7400000000002, "text": " V1 minus the cosine of the angle between the two vectors. This is basically a normalized", "tokens": [50364, 498, 398, 307, 3175, 472, 13, 4372, 2049, 562, 291, 366, 884, 18609, 613, 36170, 11, 264, 636, 291, 14722, 264, 32194, 51068, 51068, 1296, 732, 18875, 307, 406, 807, 257, 462, 1311, 31264, 282, 4560, 457, 807, 257, 23565, 4560, 13, 407, 51312, 51312, 691, 16, 3175, 264, 23565, 295, 264, 5802, 1296, 264, 732, 18875, 13, 639, 307, 1936, 257, 48704, 51628, 51628], "temperature": 0.0, "avg_logprob": -0.1593320761153947, "compression_ratio": 1.576271186440678, "no_speech_prob": 3.986576120951213e-05}, {"id": 663, "seek": 400174, "start": 4001.74, "end": 4007.8999999999996, "text": " Euclidean distance if you want. You can think of it this way. The advantage of this is that whenever", "tokens": [50364, 462, 1311, 31264, 282, 4560, 498, 291, 528, 13, 509, 393, 519, 295, 309, 341, 636, 13, 440, 5002, 295, 341, 307, 300, 5699, 50672, 50672, 291, 733, 295, 2944, 264, 4560, 11, 5699, 291, 362, 732, 18875, 293, 291, 528, 281, 652, 264, 4560, 382, 50912, 50912, 2416, 382, 1944, 11, 456, 311, 257, 588, 1858, 636, 337, 264, 1185, 281, 483, 1314, 365, 309, 538, 1455, 264, 51108, 51140, 264, 732, 18875, 588, 2416, 11, 588, 938, 11, 291, 458, 11, 406, 12166, 294, 264, 912, 3513, 293, 652, 552, 51416, 51416, 588, 588, 938, 13, 407, 586, 264, 4560, 576, 312, 2416, 13, 583, 295, 1164, 300, 311, 406, 437, 291, 528, 13, 509, 500, 380, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.07387519464260195, "compression_ratio": 1.8702290076335877, "no_speech_prob": 4.5391258026938885e-05}, {"id": 664, "seek": 400174, "start": 4007.8999999999996, "end": 4012.7, "text": " you kind of push the distance, whenever you have two vectors and you want to make the distance as", "tokens": [50364, 462, 1311, 31264, 282, 4560, 498, 291, 528, 13, 509, 393, 519, 295, 309, 341, 636, 13, 440, 5002, 295, 341, 307, 300, 5699, 50672, 50672, 291, 733, 295, 2944, 264, 4560, 11, 5699, 291, 362, 732, 18875, 293, 291, 528, 281, 652, 264, 4560, 382, 50912, 50912, 2416, 382, 1944, 11, 456, 311, 257, 588, 1858, 636, 337, 264, 1185, 281, 483, 1314, 365, 309, 538, 1455, 264, 51108, 51140, 264, 732, 18875, 588, 2416, 11, 588, 938, 11, 291, 458, 11, 406, 12166, 294, 264, 912, 3513, 293, 652, 552, 51416, 51416, 588, 588, 938, 13, 407, 586, 264, 4560, 576, 312, 2416, 13, 583, 295, 1164, 300, 311, 406, 437, 291, 528, 13, 509, 500, 380, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.07387519464260195, "compression_ratio": 1.8702290076335877, "no_speech_prob": 4.5391258026938885e-05}, {"id": 665, "seek": 400174, "start": 4012.7, "end": 4016.62, "text": " large as possible, there's a very easy way for the system to get away with it by making the", "tokens": [50364, 462, 1311, 31264, 282, 4560, 498, 291, 528, 13, 509, 393, 519, 295, 309, 341, 636, 13, 440, 5002, 295, 341, 307, 300, 5699, 50672, 50672, 291, 733, 295, 2944, 264, 4560, 11, 5699, 291, 362, 732, 18875, 293, 291, 528, 281, 652, 264, 4560, 382, 50912, 50912, 2416, 382, 1944, 11, 456, 311, 257, 588, 1858, 636, 337, 264, 1185, 281, 483, 1314, 365, 309, 538, 1455, 264, 51108, 51140, 264, 732, 18875, 588, 2416, 11, 588, 938, 11, 291, 458, 11, 406, 12166, 294, 264, 912, 3513, 293, 652, 552, 51416, 51416, 588, 588, 938, 13, 407, 586, 264, 4560, 576, 312, 2416, 13, 583, 295, 1164, 300, 311, 406, 437, 291, 528, 13, 509, 500, 380, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.07387519464260195, "compression_ratio": 1.8702290076335877, "no_speech_prob": 4.5391258026938885e-05}, {"id": 666, "seek": 400174, "start": 4017.2599999999998, "end": 4022.7799999999997, "text": " the two vectors very large, very long, you know, not pointing in the same direction and make them", "tokens": [50364, 462, 1311, 31264, 282, 4560, 498, 291, 528, 13, 509, 393, 519, 295, 309, 341, 636, 13, 440, 5002, 295, 341, 307, 300, 5699, 50672, 50672, 291, 733, 295, 2944, 264, 4560, 11, 5699, 291, 362, 732, 18875, 293, 291, 528, 281, 652, 264, 4560, 382, 50912, 50912, 2416, 382, 1944, 11, 456, 311, 257, 588, 1858, 636, 337, 264, 1185, 281, 483, 1314, 365, 309, 538, 1455, 264, 51108, 51140, 264, 732, 18875, 588, 2416, 11, 588, 938, 11, 291, 458, 11, 406, 12166, 294, 264, 912, 3513, 293, 652, 552, 51416, 51416, 588, 588, 938, 13, 407, 586, 264, 4560, 576, 312, 2416, 13, 583, 295, 1164, 300, 311, 406, 437, 291, 528, 13, 509, 500, 380, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.07387519464260195, "compression_ratio": 1.8702290076335877, "no_speech_prob": 4.5391258026938885e-05}, {"id": 667, "seek": 400174, "start": 4022.7799999999997, "end": 4027.5, "text": " very very long. So now the distance would be large. But of course that's not what you want. You don't", "tokens": [50364, 462, 1311, 31264, 282, 4560, 498, 291, 528, 13, 509, 393, 519, 295, 309, 341, 636, 13, 440, 5002, 295, 341, 307, 300, 5699, 50672, 50672, 291, 733, 295, 2944, 264, 4560, 11, 5699, 291, 362, 732, 18875, 293, 291, 528, 281, 652, 264, 4560, 382, 50912, 50912, 2416, 382, 1944, 11, 456, 311, 257, 588, 1858, 636, 337, 264, 1185, 281, 483, 1314, 365, 309, 538, 1455, 264, 51108, 51140, 264, 732, 18875, 588, 2416, 11, 588, 938, 11, 291, 458, 11, 406, 12166, 294, 264, 912, 3513, 293, 652, 552, 51416, 51416, 588, 588, 938, 13, 407, 586, 264, 4560, 576, 312, 2416, 13, 583, 295, 1164, 300, 311, 406, 437, 291, 528, 13, 509, 500, 380, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.07387519464260195, "compression_ratio": 1.8702290076335877, "no_speech_prob": 4.5391258026938885e-05}, {"id": 668, "seek": 402750, "start": 4027.5, "end": 4031.98, "text": " want the system to just make the vectors bigger. You want it to actually rotate the vector in the", "tokens": [50364, 528, 264, 1185, 281, 445, 652, 264, 18875, 3801, 13, 509, 528, 309, 281, 767, 13121, 264, 8062, 294, 264, 50588, 50588, 558, 3513, 13, 407, 291, 2710, 1125, 264, 18875, 293, 550, 3820, 2710, 1125, 428, 2541, 4560, 293, 50820, 50820, 300, 311, 1936, 437, 341, 775, 13, 400, 437, 341, 775, 307, 300, 309, 337, 3353, 3331, 309, 9898, 281, 652, 51108, 51108, 18875, 382, 17962, 365, 1184, 661, 382, 1944, 13, 400, 337, 3671, 15494, 309, 9898, 281, 652, 264, 23565, 51504, 51548, 4356, 813, 264, 1729, 10270, 13, 440, 10270, 294, 300, 1389, 820, 1391, 312, 746, 300, 733, 295, 51820], "temperature": 0.0, "avg_logprob": -0.10339780611412547, "compression_ratio": 1.8198529411764706, "no_speech_prob": 1.241027803189354e-05}, {"id": 669, "seek": 402750, "start": 4031.98, "end": 4036.62, "text": " right direction. So you normalize the vectors and then computer normalize your clean distance and", "tokens": [50364, 528, 264, 1185, 281, 445, 652, 264, 18875, 3801, 13, 509, 528, 309, 281, 767, 13121, 264, 8062, 294, 264, 50588, 50588, 558, 3513, 13, 407, 291, 2710, 1125, 264, 18875, 293, 550, 3820, 2710, 1125, 428, 2541, 4560, 293, 50820, 50820, 300, 311, 1936, 437, 341, 775, 13, 400, 437, 341, 775, 307, 300, 309, 337, 3353, 3331, 309, 9898, 281, 652, 51108, 51108, 18875, 382, 17962, 365, 1184, 661, 382, 1944, 13, 400, 337, 3671, 15494, 309, 9898, 281, 652, 264, 23565, 51504, 51548, 4356, 813, 264, 1729, 10270, 13, 440, 10270, 294, 300, 1389, 820, 1391, 312, 746, 300, 733, 295, 51820], "temperature": 0.0, "avg_logprob": -0.10339780611412547, "compression_ratio": 1.8198529411764706, "no_speech_prob": 1.241027803189354e-05}, {"id": 670, "seek": 402750, "start": 4036.62, "end": 4042.38, "text": " that's basically what this does. And what this does is that it for positive cases it tries to make", "tokens": [50364, 528, 264, 1185, 281, 445, 652, 264, 18875, 3801, 13, 509, 528, 309, 281, 767, 13121, 264, 8062, 294, 264, 50588, 50588, 558, 3513, 13, 407, 291, 2710, 1125, 264, 18875, 293, 550, 3820, 2710, 1125, 428, 2541, 4560, 293, 50820, 50820, 300, 311, 1936, 437, 341, 775, 13, 400, 437, 341, 775, 307, 300, 309, 337, 3353, 3331, 309, 9898, 281, 652, 51108, 51108, 18875, 382, 17962, 365, 1184, 661, 382, 1944, 13, 400, 337, 3671, 15494, 309, 9898, 281, 652, 264, 23565, 51504, 51548, 4356, 813, 264, 1729, 10270, 13, 440, 10270, 294, 300, 1389, 820, 1391, 312, 746, 300, 733, 295, 51820], "temperature": 0.0, "avg_logprob": -0.10339780611412547, "compression_ratio": 1.8198529411764706, "no_speech_prob": 1.241027803189354e-05}, {"id": 671, "seek": 402750, "start": 4042.38, "end": 4050.3, "text": " vectors as aligned with each other as possible. And for negative pairs it tries to make the cosine", "tokens": [50364, 528, 264, 1185, 281, 445, 652, 264, 18875, 3801, 13, 509, 528, 309, 281, 767, 13121, 264, 8062, 294, 264, 50588, 50588, 558, 3513, 13, 407, 291, 2710, 1125, 264, 18875, 293, 550, 3820, 2710, 1125, 428, 2541, 4560, 293, 50820, 50820, 300, 311, 1936, 437, 341, 775, 13, 400, 437, 341, 775, 307, 300, 309, 337, 3353, 3331, 309, 9898, 281, 652, 51108, 51108, 18875, 382, 17962, 365, 1184, 661, 382, 1944, 13, 400, 337, 3671, 15494, 309, 9898, 281, 652, 264, 23565, 51504, 51548, 4356, 813, 264, 1729, 10270, 13, 440, 10270, 294, 300, 1389, 820, 1391, 312, 746, 300, 733, 295, 51820], "temperature": 0.0, "avg_logprob": -0.10339780611412547, "compression_ratio": 1.8198529411764706, "no_speech_prob": 1.241027803189354e-05}, {"id": 672, "seek": 405030, "start": 4050.3, "end": 4056.94, "text": " smaller than the particular margin. The margin in that case should probably be something that kind of", "tokens": [50364, 4356, 813, 264, 1729, 10270, 13, 440, 10270, 294, 300, 1389, 820, 1391, 312, 746, 300, 733, 295, 50696, 50696, 307, 1998, 281, 4018, 13, 407, 291, 528, 264, 23565, 294, 257, 1090, 18795, 1901, 13, 821, 311, 257, 688, 295, 1901, 2651, 264, 51096, 51096, 45544, 295, 264, 16687, 11, 295, 264, 1090, 18795, 16687, 13, 407, 439, 428, 2793, 586, 366, 48704, 322, 51336, 51336, 264, 16687, 13, 708, 291, 528, 307, 10938, 300, 366, 14232, 27965, 984, 2531, 281, 291, 820, 312, 1998, 281, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.18172190739558294, "compression_ratio": 1.7312775330396475, "no_speech_prob": 2.6682620955398306e-05}, {"id": 673, "seek": 405030, "start": 4056.94, "end": 4064.94, "text": " is close to zero. So you want the cosine in a high dimensional space. There's a lot of space near the", "tokens": [50364, 4356, 813, 264, 1729, 10270, 13, 440, 10270, 294, 300, 1389, 820, 1391, 312, 746, 300, 733, 295, 50696, 50696, 307, 1998, 281, 4018, 13, 407, 291, 528, 264, 23565, 294, 257, 1090, 18795, 1901, 13, 821, 311, 257, 688, 295, 1901, 2651, 264, 51096, 51096, 45544, 295, 264, 16687, 11, 295, 264, 1090, 18795, 16687, 13, 407, 439, 428, 2793, 586, 366, 48704, 322, 51336, 51336, 264, 16687, 13, 708, 291, 528, 307, 10938, 300, 366, 14232, 27965, 984, 2531, 281, 291, 820, 312, 1998, 281, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.18172190739558294, "compression_ratio": 1.7312775330396475, "no_speech_prob": 2.6682620955398306e-05}, {"id": 674, "seek": 405030, "start": 4064.94, "end": 4069.7400000000002, "text": " equator of the sphere, of the high dimensional sphere. So all your points now are normalized on", "tokens": [50364, 4356, 813, 264, 1729, 10270, 13, 440, 10270, 294, 300, 1389, 820, 1391, 312, 746, 300, 733, 295, 50696, 50696, 307, 1998, 281, 4018, 13, 407, 291, 528, 264, 23565, 294, 257, 1090, 18795, 1901, 13, 821, 311, 257, 688, 295, 1901, 2651, 264, 51096, 51096, 45544, 295, 264, 16687, 11, 295, 264, 1090, 18795, 16687, 13, 407, 439, 428, 2793, 586, 366, 48704, 322, 51336, 51336, 264, 16687, 13, 708, 291, 528, 307, 10938, 300, 366, 14232, 27965, 984, 2531, 281, 291, 820, 312, 1998, 281, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.18172190739558294, "compression_ratio": 1.7312775330396475, "no_speech_prob": 2.6682620955398306e-05}, {"id": 675, "seek": 405030, "start": 4069.7400000000002, "end": 4075.42, "text": " the sphere. What you want is samples that are symmetrically similar to you should be close to", "tokens": [50364, 4356, 813, 264, 1729, 10270, 13, 440, 10270, 294, 300, 1389, 820, 1391, 312, 746, 300, 733, 295, 50696, 50696, 307, 1998, 281, 4018, 13, 407, 291, 528, 264, 23565, 294, 257, 1090, 18795, 1901, 13, 821, 311, 257, 688, 295, 1901, 2651, 264, 51096, 51096, 45544, 295, 264, 16687, 11, 295, 264, 1090, 18795, 16687, 13, 407, 439, 428, 2793, 586, 366, 48704, 322, 51336, 51336, 264, 16687, 13, 708, 291, 528, 307, 10938, 300, 366, 14232, 27965, 984, 2531, 281, 291, 820, 312, 1998, 281, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.18172190739558294, "compression_ratio": 1.7312775330396475, "no_speech_prob": 2.6682620955398306e-05}, {"id": 676, "seek": 407542, "start": 4075.42, "end": 4080.62, "text": " you. The samples that are dissimilar should be orthogonal. You don't want them to be opposed", "tokens": [50364, 291, 13, 440, 10938, 300, 366, 7802, 332, 2202, 820, 312, 41488, 13, 509, 500, 380, 528, 552, 281, 312, 8851, 50624, 50680, 570, 456, 307, 787, 472, 935, 294, 264, 7377, 13208, 13, 13813, 322, 264, 45544, 307, 257, 588, 588, 1090, 50920, 50980, 2416, 1901, 11, 264, 2302, 16687, 3175, 472, 10139, 1936, 13, 1033, 13, 407, 291, 393, 652, 264, 10270, 445, 11, 51348, 51348, 291, 458, 11, 512, 1359, 3353, 2158, 293, 550, 291, 483, 264, 2302, 45544, 4476, 295, 264, 16687, 51616, 51616, 597, 8306, 1920, 264, 2302, 5523, 295, 264, 16687, 294, 1090, 10139, 13, 51760], "temperature": 0.0, "avg_logprob": -0.14563343158135048, "compression_ratio": 1.741444866920152, "no_speech_prob": 6.144083727122052e-06}, {"id": 677, "seek": 407542, "start": 4081.7400000000002, "end": 4086.54, "text": " because there is only one point in the south pole. Whereas on the equator is a very very high", "tokens": [50364, 291, 13, 440, 10938, 300, 366, 7802, 332, 2202, 820, 312, 41488, 13, 509, 500, 380, 528, 552, 281, 312, 8851, 50624, 50680, 570, 456, 307, 787, 472, 935, 294, 264, 7377, 13208, 13, 13813, 322, 264, 45544, 307, 257, 588, 588, 1090, 50920, 50980, 2416, 1901, 11, 264, 2302, 16687, 3175, 472, 10139, 1936, 13, 1033, 13, 407, 291, 393, 652, 264, 10270, 445, 11, 51348, 51348, 291, 458, 11, 512, 1359, 3353, 2158, 293, 550, 291, 483, 264, 2302, 45544, 4476, 295, 264, 16687, 51616, 51616, 597, 8306, 1920, 264, 2302, 5523, 295, 264, 16687, 294, 1090, 10139, 13, 51760], "temperature": 0.0, "avg_logprob": -0.14563343158135048, "compression_ratio": 1.741444866920152, "no_speech_prob": 6.144083727122052e-06}, {"id": 678, "seek": 407542, "start": 4087.7400000000002, "end": 4095.1, "text": " large space, the entire sphere minus one dimension basically. Okay. So you can make the margin just,", "tokens": [50364, 291, 13, 440, 10938, 300, 366, 7802, 332, 2202, 820, 312, 41488, 13, 509, 500, 380, 528, 552, 281, 312, 8851, 50624, 50680, 570, 456, 307, 787, 472, 935, 294, 264, 7377, 13208, 13, 13813, 322, 264, 45544, 307, 257, 588, 588, 1090, 50920, 50980, 2416, 1901, 11, 264, 2302, 16687, 3175, 472, 10139, 1936, 13, 1033, 13, 407, 291, 393, 652, 264, 10270, 445, 11, 51348, 51348, 291, 458, 11, 512, 1359, 3353, 2158, 293, 550, 291, 483, 264, 2302, 45544, 4476, 295, 264, 16687, 51616, 51616, 597, 8306, 1920, 264, 2302, 5523, 295, 264, 16687, 294, 1090, 10139, 13, 51760], "temperature": 0.0, "avg_logprob": -0.14563343158135048, "compression_ratio": 1.741444866920152, "no_speech_prob": 6.144083727122052e-06}, {"id": 679, "seek": 407542, "start": 4095.1, "end": 4100.46, "text": " you know, some small positive value and then you get the entire equator essentially of the sphere", "tokens": [50364, 291, 13, 440, 10938, 300, 366, 7802, 332, 2202, 820, 312, 41488, 13, 509, 500, 380, 528, 552, 281, 312, 8851, 50624, 50680, 570, 456, 307, 787, 472, 935, 294, 264, 7377, 13208, 13, 13813, 322, 264, 45544, 307, 257, 588, 588, 1090, 50920, 50980, 2416, 1901, 11, 264, 2302, 16687, 3175, 472, 10139, 1936, 13, 1033, 13, 407, 291, 393, 652, 264, 10270, 445, 11, 51348, 51348, 291, 458, 11, 512, 1359, 3353, 2158, 293, 550, 291, 483, 264, 2302, 45544, 4476, 295, 264, 16687, 51616, 51616, 597, 8306, 1920, 264, 2302, 5523, 295, 264, 16687, 294, 1090, 10139, 13, 51760], "temperature": 0.0, "avg_logprob": -0.14563343158135048, "compression_ratio": 1.741444866920152, "no_speech_prob": 6.144083727122052e-06}, {"id": 680, "seek": 410046, "start": 4100.46, "end": 4105.66, "text": " which contains almost the entire volume of the sphere in high dimension.", "tokens": [50364, 597, 8306, 1920, 264, 2302, 5523, 295, 264, 16687, 294, 1090, 10139, 13, 50624, 50808, 383, 18238, 4470, 13, 639, 307, 257, 707, 544, 6179, 570, 300, 311, 257, 4470, 300, 307, 1936, 4960, 51100, 51176, 3877, 17630, 11, 437, 311, 1219, 3877, 17630, 13, 407, 341, 307, 11, 286, 1333, 295, 10515, 2825, 51476, 51476, 466, 309, 588, 2661, 257, 1326, 3259, 2057, 322, 746, 588, 2531, 281, 341, 13, 407, 341, 307, 51796], "temperature": 0.0, "avg_logprob": -0.1589711672299868, "compression_ratio": 1.6037735849056605, "no_speech_prob": 8.66323352965992e-06}, {"id": 681, "seek": 410046, "start": 4109.34, "end": 4115.18, "text": " CTC loss. This is a little more complicated because that's a loss that is basically uses", "tokens": [50364, 597, 8306, 1920, 264, 2302, 5523, 295, 264, 16687, 294, 1090, 10139, 13, 50624, 50808, 383, 18238, 4470, 13, 639, 307, 257, 707, 544, 6179, 570, 300, 311, 257, 4470, 300, 307, 1936, 4960, 51100, 51176, 3877, 17630, 11, 437, 311, 1219, 3877, 17630, 13, 407, 341, 307, 11, 286, 1333, 295, 10515, 2825, 51476, 51476, 466, 309, 588, 2661, 257, 1326, 3259, 2057, 322, 746, 588, 2531, 281, 341, 13, 407, 341, 307, 51796], "temperature": 0.0, "avg_logprob": -0.1589711672299868, "compression_ratio": 1.6037735849056605, "no_speech_prob": 8.66323352965992e-06}, {"id": 682, "seek": 410046, "start": 4116.7, "end": 4122.7, "text": " structure prediction, what's called structure prediction. So this is, I sort of briefly talked", "tokens": [50364, 597, 8306, 1920, 264, 2302, 5523, 295, 264, 16687, 294, 1090, 10139, 13, 50624, 50808, 383, 18238, 4470, 13, 639, 307, 257, 707, 544, 6179, 570, 300, 311, 257, 4470, 300, 307, 1936, 4960, 51100, 51176, 3877, 17630, 11, 437, 311, 1219, 3877, 17630, 13, 407, 341, 307, 11, 286, 1333, 295, 10515, 2825, 51476, 51476, 466, 309, 588, 2661, 257, 1326, 3259, 2057, 322, 746, 588, 2531, 281, 341, 13, 407, 341, 307, 51796], "temperature": 0.0, "avg_logprob": -0.1589711672299868, "compression_ratio": 1.6037735849056605, "no_speech_prob": 8.66323352965992e-06}, {"id": 683, "seek": 412270, "start": 4122.7, "end": 4131.099999999999, "text": " about it very quickly a few weeks ago on something very similar to this. So this is a loss that is", "tokens": [50364, 466, 309, 588, 2661, 257, 1326, 3259, 2057, 322, 746, 588, 2531, 281, 341, 13, 407, 341, 307, 257, 4470, 300, 307, 50784, 50784, 21142, 562, 428, 5598, 307, 257, 8310, 295, 18875, 295, 13444, 689, 264, 18875, 6805, 51272, 51272, 281, 13444, 295, 10479, 13, 1033, 13, 400, 370, 291, 362, 11, 370, 428, 1185, 715, 1819, 257, 8062, 295, 1270, 13444, 13, 407, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.10910336176554362, "compression_ratio": 1.5846994535519126, "no_speech_prob": 5.255133146420121e-06}, {"id": 684, "seek": 412270, "start": 4131.099999999999, "end": 4140.86, "text": " applicable when your output is a sequence of vectors of scores where the vectors correspond", "tokens": [50364, 466, 309, 588, 2661, 257, 1326, 3259, 2057, 322, 746, 588, 2531, 281, 341, 13, 407, 341, 307, 257, 4470, 300, 307, 50784, 50784, 21142, 562, 428, 5598, 307, 257, 8310, 295, 18875, 295, 13444, 689, 264, 18875, 6805, 51272, 51272, 281, 13444, 295, 10479, 13, 1033, 13, 400, 370, 291, 362, 11, 370, 428, 1185, 715, 1819, 257, 8062, 295, 1270, 13444, 13, 407, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.10910336176554362, "compression_ratio": 1.5846994535519126, "no_speech_prob": 5.255133146420121e-06}, {"id": 685, "seek": 412270, "start": 4140.86, "end": 4148.54, "text": " to scores of categories. Okay. And so you have, so your system computes a vector of such scores. So", "tokens": [50364, 466, 309, 588, 2661, 257, 1326, 3259, 2057, 322, 746, 588, 2531, 281, 341, 13, 407, 341, 307, 257, 4470, 300, 307, 50784, 50784, 21142, 562, 428, 5598, 307, 257, 8310, 295, 18875, 295, 13444, 689, 264, 18875, 6805, 51272, 51272, 281, 13444, 295, 10479, 13, 1033, 13, 400, 370, 291, 362, 11, 370, 428, 1185, 715, 1819, 257, 8062, 295, 1270, 13444, 13, 407, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.10910336176554362, "compression_ratio": 1.5846994535519126, "no_speech_prob": 5.255133146420121e-06}, {"id": 686, "seek": 414854, "start": 4148.54, "end": 4154.38, "text": " imagine for example, a speech recognition system. Speech recognition system every 10 milliseconds", "tokens": [50364, 3811, 337, 1365, 11, 257, 6218, 11150, 1185, 13, 48385, 11150, 1185, 633, 1266, 34184, 50656, 50656, 2709, 291, 257, 8062, 295, 33783, 337, 437, 264, 1626, 885, 23155, 558, 586, 307, 13, 400, 264, 50988, 50988, 1230, 295, 10479, 2673, 307, 1596, 2416, 322, 264, 1668, 295, 257, 1326, 4714, 13, 1033, 13, 407, 309, 2709, 291, 51208, 51236, 1936, 257, 2787, 41167, 8062, 295, 257, 2744, 11, 291, 458, 11, 5850, 20984, 718, 311, 584, 13, 1485, 295, 729, 633, 1266, 51556, 51556], "temperature": 0.0, "avg_logprob": -0.15714536864182044, "compression_ratio": 1.610878661087866, "no_speech_prob": 9.079863048100378e-06}, {"id": 687, "seek": 414854, "start": 4154.38, "end": 4161.0199999999995, "text": " gives you a vector of probabilities for what the sound being pronounced right now is. And the", "tokens": [50364, 3811, 337, 1365, 11, 257, 6218, 11150, 1185, 13, 48385, 11150, 1185, 633, 1266, 34184, 50656, 50656, 2709, 291, 257, 8062, 295, 33783, 337, 437, 264, 1626, 885, 23155, 558, 586, 307, 13, 400, 264, 50988, 50988, 1230, 295, 10479, 2673, 307, 1596, 2416, 322, 264, 1668, 295, 257, 1326, 4714, 13, 1033, 13, 407, 309, 2709, 291, 51208, 51236, 1936, 257, 2787, 41167, 8062, 295, 257, 2744, 11, 291, 458, 11, 5850, 20984, 718, 311, 584, 13, 1485, 295, 729, 633, 1266, 51556, 51556], "temperature": 0.0, "avg_logprob": -0.15714536864182044, "compression_ratio": 1.610878661087866, "no_speech_prob": 9.079863048100378e-06}, {"id": 688, "seek": 414854, "start": 4161.0199999999995, "end": 4165.42, "text": " number of categories usually is quite large on the order of a few thousand. Okay. So it gives you", "tokens": [50364, 3811, 337, 1365, 11, 257, 6218, 11150, 1185, 13, 48385, 11150, 1185, 633, 1266, 34184, 50656, 50656, 2709, 291, 257, 8062, 295, 33783, 337, 437, 264, 1626, 885, 23155, 558, 586, 307, 13, 400, 264, 50988, 50988, 1230, 295, 10479, 2673, 307, 1596, 2416, 322, 264, 1668, 295, 257, 1326, 4714, 13, 1033, 13, 407, 309, 2709, 291, 51208, 51236, 1936, 257, 2787, 41167, 8062, 295, 257, 2744, 11, 291, 458, 11, 5850, 20984, 718, 311, 584, 13, 1485, 295, 729, 633, 1266, 51556, 51556], "temperature": 0.0, "avg_logprob": -0.15714536864182044, "compression_ratio": 1.610878661087866, "no_speech_prob": 9.079863048100378e-06}, {"id": 689, "seek": 414854, "start": 4165.98, "end": 4172.38, "text": " basically a softmax vector of a size, you know, typically 3000 let's say. One of those every 10", "tokens": [50364, 3811, 337, 1365, 11, 257, 6218, 11150, 1185, 13, 48385, 11150, 1185, 633, 1266, 34184, 50656, 50656, 2709, 291, 257, 8062, 295, 33783, 337, 437, 264, 1626, 885, 23155, 558, 586, 307, 13, 400, 264, 50988, 50988, 1230, 295, 10479, 2673, 307, 1596, 2416, 322, 264, 1668, 295, 257, 1326, 4714, 13, 1033, 13, 407, 309, 2709, 291, 51208, 51236, 1936, 257, 2787, 41167, 8062, 295, 257, 2744, 11, 291, 458, 11, 5850, 20984, 718, 311, 584, 13, 1485, 295, 729, 633, 1266, 51556, 51556], "temperature": 0.0, "avg_logprob": -0.15714536864182044, "compression_ratio": 1.610878661087866, "no_speech_prob": 9.079863048100378e-06}, {"id": 690, "seek": 417238, "start": 4172.38, "end": 4178.62, "text": " milliseconds. All right. And what you'd like, you know, you have a desired output and the desired", "tokens": [50364, 34184, 13, 1057, 558, 13, 400, 437, 291, 1116, 411, 11, 291, 458, 11, 291, 362, 257, 14721, 5598, 293, 264, 14721, 50676, 50676, 5598, 307, 437, 1349, 390, 885, 23155, 293, 264, 1349, 300, 311, 885, 23155, 300, 23249, 281, 50936, 50936, 733, 295, 257, 1729, 8310, 295, 3263, 498, 291, 528, 300, 291, 1062, 458, 13, 407, 437, 291, 643, 586, 51304, 51304, 307, 257, 2063, 300, 1936, 307, 2295, 498, 300, 8310, 1542, 411, 300, 8310, 13, 583, 437, 291, 1062, 2089, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.13015583124053612, "compression_ratio": 1.8523809523809525, "no_speech_prob": 6.74762077323976e-06}, {"id": 691, "seek": 417238, "start": 4178.62, "end": 4183.82, "text": " output is what word was being pronounced and the word that's being pronounced that corresponds to", "tokens": [50364, 34184, 13, 1057, 558, 13, 400, 437, 291, 1116, 411, 11, 291, 458, 11, 291, 362, 257, 14721, 5598, 293, 264, 14721, 50676, 50676, 5598, 307, 437, 1349, 390, 885, 23155, 293, 264, 1349, 300, 311, 885, 23155, 300, 23249, 281, 50936, 50936, 733, 295, 257, 1729, 8310, 295, 3263, 498, 291, 528, 300, 291, 1062, 458, 13, 407, 437, 291, 643, 586, 51304, 51304, 307, 257, 2063, 300, 1936, 307, 2295, 498, 300, 8310, 1542, 411, 300, 8310, 13, 583, 437, 291, 1062, 2089, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.13015583124053612, "compression_ratio": 1.8523809523809525, "no_speech_prob": 6.74762077323976e-06}, {"id": 692, "seek": 417238, "start": 4183.82, "end": 4191.18, "text": " kind of a particular sequence of sounds if you want that you might know. So what you need now", "tokens": [50364, 34184, 13, 1057, 558, 13, 400, 437, 291, 1116, 411, 11, 291, 458, 11, 291, 362, 257, 14721, 5598, 293, 264, 14721, 50676, 50676, 5598, 307, 437, 1349, 390, 885, 23155, 293, 264, 1349, 300, 311, 885, 23155, 300, 23249, 281, 50936, 50936, 733, 295, 257, 1729, 8310, 295, 3263, 498, 291, 528, 300, 291, 1062, 458, 13, 407, 437, 291, 643, 586, 51304, 51304, 307, 257, 2063, 300, 1936, 307, 2295, 498, 300, 8310, 1542, 411, 300, 8310, 13, 583, 437, 291, 1062, 2089, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.13015583124053612, "compression_ratio": 1.8523809523809525, "no_speech_prob": 6.74762077323976e-06}, {"id": 693, "seek": 417238, "start": 4191.18, "end": 4198.78, "text": " is a cost that basically is low if that sequence looks like that sequence. But what you might allow", "tokens": [50364, 34184, 13, 1057, 558, 13, 400, 437, 291, 1116, 411, 11, 291, 458, 11, 291, 362, 257, 14721, 5598, 293, 264, 14721, 50676, 50676, 5598, 307, 437, 1349, 390, 885, 23155, 293, 264, 1349, 300, 311, 885, 23155, 300, 23249, 281, 50936, 50936, 733, 295, 257, 1729, 8310, 295, 3263, 498, 291, 528, 300, 291, 1062, 458, 13, 407, 437, 291, 643, 586, 51304, 51304, 307, 257, 2063, 300, 1936, 307, 2295, 498, 300, 8310, 1542, 411, 300, 8310, 13, 583, 437, 291, 1062, 2089, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.13015583124053612, "compression_ratio": 1.8523809523809525, "no_speech_prob": 6.74762077323976e-06}, {"id": 694, "seek": 419878, "start": 4198.78, "end": 4211.42, "text": " is for the input sequence to repeat some of the sounds if you want. Right. So for example, you", "tokens": [50364, 307, 337, 264, 4846, 8310, 281, 7149, 512, 295, 264, 3263, 498, 291, 528, 13, 1779, 13, 407, 337, 1365, 11, 291, 50996, 50996, 458, 11, 452, 2063, 281, 264, 3779, 1062, 312, 264, 1349, 3407, 11, 718, 311, 584, 11, 293, 309, 311, 23155, 534, 51324, 51324, 2661, 3407, 13, 407, 291, 1936, 362, 11, 291, 458, 11, 257, 588, 1359, 1230, 295, 10938, 295, 1184, 1626, 51552, 51552, 294, 264, 8310, 13, 583, 550, 4317, 264, 954, 567, 307, 14144, 2175, 264, 1349, 586, 300, 291, 764, 382, 257, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.149314358359889, "compression_ratio": 1.6801801801801801, "no_speech_prob": 6.240773473109584e-06}, {"id": 695, "seek": 419878, "start": 4211.42, "end": 4217.98, "text": " know, my cost to the target might be the word seven, let's say, and it's pronounced really", "tokens": [50364, 307, 337, 264, 4846, 8310, 281, 7149, 512, 295, 264, 3263, 498, 291, 528, 13, 1779, 13, 407, 337, 1365, 11, 291, 50996, 50996, 458, 11, 452, 2063, 281, 264, 3779, 1062, 312, 264, 1349, 3407, 11, 718, 311, 584, 11, 293, 309, 311, 23155, 534, 51324, 51324, 2661, 3407, 13, 407, 291, 1936, 362, 11, 291, 458, 11, 257, 588, 1359, 1230, 295, 10938, 295, 1184, 1626, 51552, 51552, 294, 264, 8310, 13, 583, 550, 4317, 264, 954, 567, 307, 14144, 2175, 264, 1349, 586, 300, 291, 764, 382, 257, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.149314358359889, "compression_ratio": 1.6801801801801801, "no_speech_prob": 6.240773473109584e-06}, {"id": 696, "seek": 419878, "start": 4217.98, "end": 4222.54, "text": " quickly seven. So you basically have, you know, a very small number of samples of each sound", "tokens": [50364, 307, 337, 264, 4846, 8310, 281, 7149, 512, 295, 264, 3263, 498, 291, 528, 13, 1779, 13, 407, 337, 1365, 11, 291, 50996, 50996, 458, 11, 452, 2063, 281, 264, 3779, 1062, 312, 264, 1349, 3407, 11, 718, 311, 584, 11, 293, 309, 311, 23155, 534, 51324, 51324, 2661, 3407, 13, 407, 291, 1936, 362, 11, 291, 458, 11, 257, 588, 1359, 1230, 295, 10938, 295, 1184, 1626, 51552, 51552, 294, 264, 8310, 13, 583, 550, 4317, 264, 954, 567, 307, 14144, 2175, 264, 1349, 586, 300, 291, 764, 382, 257, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.149314358359889, "compression_ratio": 1.6801801801801801, "no_speech_prob": 6.240773473109584e-06}, {"id": 697, "seek": 419878, "start": 4222.54, "end": 4228.0599999999995, "text": " in the sequence. But then perhaps the person who is pronouncing the word now that you use as a", "tokens": [50364, 307, 337, 264, 4846, 8310, 281, 7149, 512, 295, 264, 3263, 498, 291, 528, 13, 1779, 13, 407, 337, 1365, 11, 291, 50996, 50996, 458, 11, 452, 2063, 281, 264, 3779, 1062, 312, 264, 1349, 3407, 11, 718, 311, 584, 11, 293, 309, 311, 23155, 534, 51324, 51324, 2661, 3407, 13, 407, 291, 1936, 362, 11, 291, 458, 11, 257, 588, 1359, 1230, 295, 10938, 295, 1184, 1626, 51552, 51552, 294, 264, 8310, 13, 583, 550, 4317, 264, 954, 567, 307, 14144, 2175, 264, 1349, 586, 300, 291, 764, 382, 257, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.149314358359889, "compression_ratio": 1.6801801801801801, "no_speech_prob": 6.240773473109584e-06}, {"id": 698, "seek": 422806, "start": 4228.06, "end": 4235.660000000001, "text": " sample pronounced it very slowly, like seven. Right. So now the first, the first takes, you", "tokens": [50364, 6889, 23155, 309, 588, 5692, 11, 411, 3407, 13, 1779, 13, 407, 586, 264, 700, 11, 264, 700, 2516, 11, 291, 50744, 50744, 458, 11, 2940, 11, 2940, 12083, 295, 1266, 34184, 300, 820, 439, 312, 33318, 281, 264, 912, 5197, 295, 51252, 51252, 264, 294, 264, 11, 294, 264, 5598, 13, 400, 286, 360, 300, 3036, 949, 11, 457, 286, 478, 516, 281, 360, 309, 797, 13, 1779, 13, 407, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.23346343994140625, "compression_ratio": 1.5026178010471205, "no_speech_prob": 3.6687827105197357e-06}, {"id": 699, "seek": 422806, "start": 4235.660000000001, "end": 4245.820000000001, "text": " know, several, several frames of 10 milliseconds that should all be mapped to the same instance of", "tokens": [50364, 6889, 23155, 309, 588, 5692, 11, 411, 3407, 13, 1779, 13, 407, 586, 264, 700, 11, 264, 700, 2516, 11, 291, 50744, 50744, 458, 11, 2940, 11, 2940, 12083, 295, 1266, 34184, 300, 820, 439, 312, 33318, 281, 264, 912, 5197, 295, 51252, 51252, 264, 294, 264, 11, 294, 264, 5598, 13, 400, 286, 360, 300, 3036, 949, 11, 457, 286, 478, 516, 281, 360, 309, 797, 13, 1779, 13, 407, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.23346343994140625, "compression_ratio": 1.5026178010471205, "no_speech_prob": 3.6687827105197357e-06}, {"id": 700, "seek": 422806, "start": 4245.820000000001, "end": 4252.620000000001, "text": " the in the, in the output. And I do that picture before, but I'm going to do it again. Right. So", "tokens": [50364, 6889, 23155, 309, 588, 5692, 11, 411, 3407, 13, 1779, 13, 407, 586, 264, 700, 11, 264, 700, 2516, 11, 291, 50744, 50744, 458, 11, 2940, 11, 2940, 12083, 295, 1266, 34184, 300, 820, 439, 312, 33318, 281, 264, 912, 5197, 295, 51252, 51252, 264, 294, 264, 11, 294, 264, 5598, 13, 400, 286, 360, 300, 3036, 949, 11, 457, 286, 478, 516, 281, 360, 309, 797, 13, 1779, 13, 407, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.23346343994140625, "compression_ratio": 1.5026178010471205, "no_speech_prob": 3.6687827105197357e-06}, {"id": 701, "seek": 425262, "start": 4252.62, "end": 4266.54, "text": " the, you have, let's see, you have a sequence of scores coming out of soft maxes, let's say,", "tokens": [50364, 264, 11, 291, 362, 11, 718, 311, 536, 11, 291, 362, 257, 8310, 295, 13444, 1348, 484, 295, 2787, 11469, 279, 11, 718, 311, 584, 11, 51060, 51060, 309, 311, 767, 1101, 498, 456, 366, 25737, 11, 457, 337, 383, 18238, 436, 643, 281, 312, 11, 293, 550, 291, 362, 264, 51400, 51400], "temperature": 0.0, "avg_logprob": -0.17042354236949575, "compression_ratio": 1.449612403100775, "no_speech_prob": 3.088711991949822e-06}, {"id": 702, "seek": 425262, "start": 4266.54, "end": 4273.34, "text": " it's actually better if there are energies, but for CTC they need to be, and then you have the", "tokens": [50364, 264, 11, 291, 362, 11, 718, 311, 536, 11, 291, 362, 257, 8310, 295, 13444, 1348, 484, 295, 2787, 11469, 279, 11, 718, 311, 584, 11, 51060, 51060, 309, 311, 767, 1101, 498, 456, 366, 25737, 11, 457, 337, 383, 18238, 436, 643, 281, 312, 11, 293, 550, 291, 362, 264, 51400, 51400], "temperature": 0.0, "avg_logprob": -0.17042354236949575, "compression_ratio": 1.449612403100775, "no_speech_prob": 3.088711991949822e-06}, {"id": 703, "seek": 427334, "start": 4273.34, "end": 4287.26, "text": " target sequence. And I think of this as some sort of matrix and each entry in that matrix", "tokens": [50364, 3779, 8310, 13, 400, 286, 519, 295, 341, 382, 512, 1333, 295, 8141, 293, 1184, 8729, 294, 300, 8141, 51060, 51104, 1936, 8000, 264, 4560, 1296, 264, 732, 18875, 300, 366, 510, 13, 1033, 13, 407, 562, 286, 478, 51436, 51436, 15083, 264, 8141, 16203, 577, 341, 8062, 1542, 411, 300, 8062, 11, 337, 1365, 11, 264, 3278, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.12017823034717191, "compression_ratio": 1.5055555555555555, "no_speech_prob": 2.902137566707097e-06}, {"id": 704, "seek": 427334, "start": 4288.14, "end": 4294.78, "text": " basically measures the distance between the two vectors that are here. Okay. So when I'm", "tokens": [50364, 3779, 8310, 13, 400, 286, 519, 295, 341, 382, 512, 1333, 295, 8141, 293, 1184, 8729, 294, 300, 8141, 51060, 51104, 1936, 8000, 264, 4560, 1296, 264, 732, 18875, 300, 366, 510, 13, 1033, 13, 407, 562, 286, 478, 51436, 51436, 15083, 264, 8141, 16203, 577, 341, 8062, 1542, 411, 300, 8062, 11, 337, 1365, 11, 264, 3278, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.12017823034717191, "compression_ratio": 1.5055555555555555, "no_speech_prob": 2.902137566707097e-06}, {"id": 705, "seek": 427334, "start": 4294.78, "end": 4300.38, "text": " treating the matrix indicates how this vector looks like that vector, for example, the cross", "tokens": [50364, 3779, 8310, 13, 400, 286, 519, 295, 341, 382, 512, 1333, 295, 8141, 293, 1184, 8729, 294, 300, 8141, 51060, 51104, 1936, 8000, 264, 4560, 1296, 264, 732, 18875, 300, 366, 510, 13, 1033, 13, 407, 562, 286, 478, 51436, 51436, 15083, 264, 8141, 16203, 577, 341, 8062, 1542, 411, 300, 8062, 11, 337, 1365, 11, 264, 3278, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.12017823034717191, "compression_ratio": 1.5055555555555555, "no_speech_prob": 2.902137566707097e-06}, {"id": 706, "seek": 430038, "start": 4300.38, "end": 4305.74, "text": " entropy or something like that. Okay. Or square error. It doesn't matter what the loss function", "tokens": [50364, 30867, 420, 746, 411, 300, 13, 1033, 13, 1610, 3732, 6713, 13, 467, 1177, 380, 1871, 437, 264, 4470, 2445, 50632, 50632, 307, 13, 407, 586, 498, 341, 307, 264, 1349, 3407, 23155, 5692, 11, 1392, 13, 400, 341, 575, 4317, 787, 472, 51632, 51720], "temperature": 0.0, "avg_logprob": -0.13867392438523313, "compression_ratio": 1.3357142857142856, "no_speech_prob": 3.288678271928802e-06}, {"id": 707, "seek": 430038, "start": 4305.74, "end": 4325.74, "text": " is. So now if this is the word seven pronounced slowly, okay. And this has perhaps only one", "tokens": [50364, 30867, 420, 746, 411, 300, 13, 1033, 13, 1610, 3732, 6713, 13, 467, 1177, 380, 1871, 437, 264, 4470, 2445, 50632, 50632, 307, 13, 407, 586, 498, 341, 307, 264, 1349, 3407, 23155, 5692, 11, 1392, 13, 400, 341, 575, 4317, 787, 472, 51632, 51720], "temperature": 0.0, "avg_logprob": -0.13867392438523313, "compression_ratio": 1.3357142857142856, "no_speech_prob": 3.288678271928802e-06}, {"id": 708, "seek": 432574, "start": 4325.74, "end": 4334.94, "text": " instance of each sound. You want all of those, you know, you would want all of those vectors", "tokens": [50364, 5197, 295, 1184, 1626, 13, 509, 528, 439, 295, 729, 11, 291, 458, 11, 291, 576, 528, 439, 295, 729, 18875, 50824, 50960, 11760, 281, 264, 462, 281, 312, 33318, 281, 300, 462, 8062, 510, 13, 1033, 13, 407, 291, 528, 281, 14722, 300, 51432, 51432], "temperature": 0.0, "avg_logprob": -0.1903582215309143, "compression_ratio": 1.4566929133858268, "no_speech_prob": 6.747818588337395e-06}, {"id": 709, "seek": 432574, "start": 4337.66, "end": 4347.099999999999, "text": " corresponding to the E to be mapped to that E vector here. Okay. So you want to compute that", "tokens": [50364, 5197, 295, 1184, 1626, 13, 509, 528, 439, 295, 729, 11, 291, 458, 11, 291, 576, 528, 439, 295, 729, 18875, 50824, 50960, 11760, 281, 264, 462, 281, 312, 33318, 281, 300, 462, 8062, 510, 13, 1033, 13, 407, 291, 528, 281, 14722, 300, 51432, 51432], "temperature": 0.0, "avg_logprob": -0.1903582215309143, "compression_ratio": 1.4566929133858268, "no_speech_prob": 6.747818588337395e-06}, {"id": 710, "seek": 434710, "start": 4347.1, "end": 4356.700000000001, "text": " so you want to compute that cost of, you know, confusing that the, those, all of those, I mean,", "tokens": [50364, 370, 291, 528, 281, 14722, 300, 2063, 295, 11, 291, 458, 11, 13181, 300, 264, 11, 729, 11, 439, 295, 729, 11, 286, 914, 11, 50844, 50844, 14324, 729, 462, 311, 281, 300, 462, 13, 823, 11, 295, 1164, 11, 510, 264, 1185, 7126, 264, 3006, 1867, 13, 407, 291, 500, 380, 362, 709, 295, 257, 1154, 13, 51000, 51028, 583, 498, 264, 3779, 307, 3407, 11, 457, 264, 1349, 300, 390, 23155, 510, 11, 51196, 51308, 420, 264, 5598, 300, 390, 7126, 538, 264, 1185, 775, 406, 6805, 281, 3407, 11, 51552, 51640], "temperature": 0.0, "avg_logprob": -0.2188392480214437, "compression_ratio": 1.6712328767123288, "no_speech_prob": 3.0894402698322665e-06}, {"id": 711, "seek": 434710, "start": 4356.700000000001, "end": 4359.820000000001, "text": " matching those E's to that E. Now, of course, here the system produced the correct answer. So you don't have much of a problem.", "tokens": [50364, 370, 291, 528, 281, 14722, 300, 2063, 295, 11, 291, 458, 11, 13181, 300, 264, 11, 729, 11, 439, 295, 729, 11, 286, 914, 11, 50844, 50844, 14324, 729, 462, 311, 281, 300, 462, 13, 823, 11, 295, 1164, 11, 510, 264, 1185, 7126, 264, 3006, 1867, 13, 407, 291, 500, 380, 362, 709, 295, 257, 1154, 13, 51000, 51028, 583, 498, 264, 3779, 307, 3407, 11, 457, 264, 1349, 300, 390, 23155, 510, 11, 51196, 51308, 420, 264, 5598, 300, 390, 7126, 538, 264, 1185, 775, 406, 6805, 281, 3407, 11, 51552, 51640], "temperature": 0.0, "avg_logprob": -0.2188392480214437, "compression_ratio": 1.6712328767123288, "no_speech_prob": 3.0894402698322665e-06}, {"id": 712, "seek": 434710, "start": 4360.38, "end": 4363.740000000001, "text": " But if the target is seven, but the word that was pronounced here,", "tokens": [50364, 370, 291, 528, 281, 14722, 300, 2063, 295, 11, 291, 458, 11, 13181, 300, 264, 11, 729, 11, 439, 295, 729, 11, 286, 914, 11, 50844, 50844, 14324, 729, 462, 311, 281, 300, 462, 13, 823, 11, 295, 1164, 11, 510, 264, 1185, 7126, 264, 3006, 1867, 13, 407, 291, 500, 380, 362, 709, 295, 257, 1154, 13, 51000, 51028, 583, 498, 264, 3779, 307, 3407, 11, 457, 264, 1349, 300, 390, 23155, 510, 11, 51196, 51308, 420, 264, 5598, 300, 390, 7126, 538, 264, 1185, 775, 406, 6805, 281, 3407, 11, 51552, 51640], "temperature": 0.0, "avg_logprob": -0.2188392480214437, "compression_ratio": 1.6712328767123288, "no_speech_prob": 3.0894402698322665e-06}, {"id": 713, "seek": 434710, "start": 4365.9800000000005, "end": 4370.860000000001, "text": " or the output that was produced by the system does not correspond to seven,", "tokens": [50364, 370, 291, 528, 281, 14722, 300, 2063, 295, 11, 291, 458, 11, 13181, 300, 264, 11, 729, 11, 439, 295, 729, 11, 286, 914, 11, 50844, 50844, 14324, 729, 462, 311, 281, 300, 462, 13, 823, 11, 295, 1164, 11, 510, 264, 1185, 7126, 264, 3006, 1867, 13, 407, 291, 500, 380, 362, 709, 295, 257, 1154, 13, 51000, 51028, 583, 498, 264, 3779, 307, 3407, 11, 457, 264, 1349, 300, 390, 23155, 510, 11, 51196, 51308, 420, 264, 5598, 300, 390, 7126, 538, 264, 1185, 775, 406, 6805, 281, 3407, 11, 51552, 51640], "temperature": 0.0, "avg_logprob": -0.2188392480214437, "compression_ratio": 1.6712328767123288, "no_speech_prob": 3.0894402698322665e-06}, {"id": 714, "seek": 437086, "start": 4370.86, "end": 4380.46, "text": " that's when you run into trouble. So here what you do is you find the best mapping from the input sequence to the output sequence.", "tokens": [50364, 300, 311, 562, 291, 1190, 666, 5253, 13, 407, 510, 437, 291, 360, 307, 291, 915, 264, 1151, 18350, 490, 264, 4846, 8310, 281, 264, 5598, 8310, 13, 50844, 50844, 1033, 13, 407, 264, 318, 2170, 33318, 281, 264, 318, 11, 264, 462, 281, 264, 462, 11, 264, 691, 281, 264, 691, 11, 264, 462, 311, 281, 264, 462, 293, 264, 426, 281, 264, 426, 13, 51244, 51244, 407, 291, 483, 341, 733, 295, 3100, 498, 291, 528, 300, 519, 295, 341, 382, 257, 3100, 294, 257, 4295, 13, 51444, 51520], "temperature": 0.0, "avg_logprob": -0.2572846156294628, "compression_ratio": 1.7569060773480663, "no_speech_prob": 1.3709377526538447e-06}, {"id": 715, "seek": 437086, "start": 4380.46, "end": 4388.46, "text": " Okay. So the S gets mapped to the S, the E to the E, the V to the V, the E's to the E and the N to the N.", "tokens": [50364, 300, 311, 562, 291, 1190, 666, 5253, 13, 407, 510, 437, 291, 360, 307, 291, 915, 264, 1151, 18350, 490, 264, 4846, 8310, 281, 264, 5598, 8310, 13, 50844, 50844, 1033, 13, 407, 264, 318, 2170, 33318, 281, 264, 318, 11, 264, 462, 281, 264, 462, 11, 264, 691, 281, 264, 691, 11, 264, 462, 311, 281, 264, 462, 293, 264, 426, 281, 264, 426, 13, 51244, 51244, 407, 291, 483, 341, 733, 295, 3100, 498, 291, 528, 300, 519, 295, 341, 382, 257, 3100, 294, 257, 4295, 13, 51444, 51520], "temperature": 0.0, "avg_logprob": -0.2572846156294628, "compression_ratio": 1.7569060773480663, "no_speech_prob": 1.3709377526538447e-06}, {"id": 716, "seek": 437086, "start": 4388.46, "end": 4392.46, "text": " So you get this kind of path if you want that think of this as a path in a graph.", "tokens": [50364, 300, 311, 562, 291, 1190, 666, 5253, 13, 407, 510, 437, 291, 360, 307, 291, 915, 264, 1151, 18350, 490, 264, 4846, 8310, 281, 264, 5598, 8310, 13, 50844, 50844, 1033, 13, 407, 264, 318, 2170, 33318, 281, 264, 318, 11, 264, 462, 281, 264, 462, 11, 264, 691, 281, 264, 691, 11, 264, 462, 311, 281, 264, 462, 293, 264, 426, 281, 264, 426, 13, 51244, 51244, 407, 291, 483, 341, 733, 295, 3100, 498, 291, 528, 300, 519, 295, 341, 382, 257, 3100, 294, 257, 4295, 13, 51444, 51520], "temperature": 0.0, "avg_logprob": -0.2572846156294628, "compression_ratio": 1.7569060773480663, "no_speech_prob": 1.3709377526538447e-06}, {"id": 717, "seek": 439246, "start": 4392.46, "end": 4401.34, "text": " And the way you determine this is basically by using a dynamic programming algorithm, a short-cut path algorithm that figures out how do I get from here to here,", "tokens": [50364, 400, 264, 636, 291, 6997, 341, 307, 1936, 538, 1228, 257, 8546, 9410, 9284, 11, 257, 2099, 12, 6672, 3100, 9284, 300, 9624, 484, 577, 360, 286, 483, 490, 510, 281, 510, 11, 50808, 50808, 294, 257, 3100, 300, 4464, 5660, 264, 2408, 295, 264, 4560, 22182, 1296, 264, 11, 439, 264, 18875, 11, 51228, 51228, 439, 264, 22182, 1296, 264, 18875, 295, 11, 291, 458, 11, 439, 264, 2793, 286, 478, 516, 807, 13, 51428, 51496, 1033, 13, 407, 456, 311, 257, 19618, 365, 3104, 281, 257, 48994, 7006, 498, 291, 528, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.3209464404047752, "compression_ratio": 1.6938775510204083, "no_speech_prob": 1.0844873941096012e-06}, {"id": 718, "seek": 439246, "start": 4401.34, "end": 4409.74, "text": " in a path that minimizes the sum of the distance distances between the, all the vectors,", "tokens": [50364, 400, 264, 636, 291, 6997, 341, 307, 1936, 538, 1228, 257, 8546, 9410, 9284, 11, 257, 2099, 12, 6672, 3100, 9284, 300, 9624, 484, 577, 360, 286, 483, 490, 510, 281, 510, 11, 50808, 50808, 294, 257, 3100, 300, 4464, 5660, 264, 2408, 295, 264, 4560, 22182, 1296, 264, 11, 439, 264, 18875, 11, 51228, 51228, 439, 264, 22182, 1296, 264, 18875, 295, 11, 291, 458, 11, 439, 264, 2793, 286, 478, 516, 807, 13, 51428, 51496, 1033, 13, 407, 456, 311, 257, 19618, 365, 3104, 281, 257, 48994, 7006, 498, 291, 528, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.3209464404047752, "compression_ratio": 1.6938775510204083, "no_speech_prob": 1.0844873941096012e-06}, {"id": 719, "seek": 439246, "start": 4409.74, "end": 4413.74, "text": " all the distances between the vectors of, you know, all the points I'm going through.", "tokens": [50364, 400, 264, 636, 291, 6997, 341, 307, 1936, 538, 1228, 257, 8546, 9410, 9284, 11, 257, 2099, 12, 6672, 3100, 9284, 300, 9624, 484, 577, 360, 286, 483, 490, 510, 281, 510, 11, 50808, 50808, 294, 257, 3100, 300, 4464, 5660, 264, 2408, 295, 264, 4560, 22182, 1296, 264, 11, 439, 264, 18875, 11, 51228, 51228, 439, 264, 22182, 1296, 264, 18875, 295, 11, 291, 458, 11, 439, 264, 2793, 286, 478, 516, 807, 13, 51428, 51496, 1033, 13, 407, 456, 311, 257, 19618, 365, 3104, 281, 257, 48994, 7006, 498, 291, 528, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.3209464404047752, "compression_ratio": 1.6938775510204083, "no_speech_prob": 1.0844873941096012e-06}, {"id": 720, "seek": 439246, "start": 4415.1, "end": 4420.62, "text": " Okay. So there's a optimization with respect to a latent variable if you want.", "tokens": [50364, 400, 264, 636, 291, 6997, 341, 307, 1936, 538, 1228, 257, 8546, 9410, 9284, 11, 257, 2099, 12, 6672, 3100, 9284, 300, 9624, 484, 577, 360, 286, 483, 490, 510, 281, 510, 11, 50808, 50808, 294, 257, 3100, 300, 4464, 5660, 264, 2408, 295, 264, 4560, 22182, 1296, 264, 11, 439, 264, 18875, 11, 51228, 51228, 439, 264, 22182, 1296, 264, 18875, 295, 11, 291, 458, 11, 439, 264, 2793, 286, 478, 516, 807, 13, 51428, 51496, 1033, 13, 407, 456, 311, 257, 19618, 365, 3104, 281, 257, 48994, 7006, 498, 291, 528, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.3209464404047752, "compression_ratio": 1.6938775510204083, "no_speech_prob": 1.0844873941096012e-06}, {"id": 721, "seek": 442062, "start": 4420.62, "end": 4426.38, "text": " Okay. And CTC basically does that for you. Right. So you give it two sequences and it computes the distance between them and,", "tokens": [50364, 1033, 13, 400, 383, 18238, 1936, 775, 300, 337, 291, 13, 1779, 13, 407, 291, 976, 309, 732, 22978, 293, 309, 715, 1819, 264, 4560, 1296, 552, 293, 11, 50652, 50652, 291, 458, 11, 733, 295, 264, 1151, 733, 295, 18350, 1296, 264, 732, 538, 8293, 50896, 51080, 1936, 281, 4471, 3866, 4846, 18875, 281, 733, 295, 257, 2167, 472, 322, 264, 5598, 13, 467, 2644, 5268, 13, 467, 393, 787, 733, 295, 5407, 498, 291, 528, 13, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.26919523099573645, "compression_ratio": 1.6243902439024391, "no_speech_prob": 1.1298272511339746e-05}, {"id": 722, "seek": 442062, "start": 4426.38, "end": 4431.26, "text": " you know, kind of the best kind of mapping between the two by allowing", "tokens": [50364, 1033, 13, 400, 383, 18238, 1936, 775, 300, 337, 291, 13, 1779, 13, 407, 291, 976, 309, 732, 22978, 293, 309, 715, 1819, 264, 4560, 1296, 552, 293, 11, 50652, 50652, 291, 458, 11, 733, 295, 264, 1151, 733, 295, 18350, 1296, 264, 732, 538, 8293, 50896, 51080, 1936, 281, 4471, 3866, 4846, 18875, 281, 733, 295, 257, 2167, 472, 322, 264, 5598, 13, 467, 2644, 5268, 13, 467, 393, 787, 733, 295, 5407, 498, 291, 528, 13, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.26919523099573645, "compression_ratio": 1.6243902439024391, "no_speech_prob": 1.1298272511339746e-05}, {"id": 723, "seek": 442062, "start": 4434.94, "end": 4443.98, "text": " basically to map multiple input vectors to kind of a single one on the output. It cannot expand. It can only kind of reduce if you want.", "tokens": [50364, 1033, 13, 400, 383, 18238, 1936, 775, 300, 337, 291, 13, 1779, 13, 407, 291, 976, 309, 732, 22978, 293, 309, 715, 1819, 264, 4560, 1296, 552, 293, 11, 50652, 50652, 291, 458, 11, 733, 295, 264, 1151, 733, 295, 18350, 1296, 264, 732, 538, 8293, 50896, 51080, 1936, 281, 4471, 3866, 4846, 18875, 281, 733, 295, 257, 2167, 472, 322, 264, 5598, 13, 467, 2644, 5268, 13, 467, 393, 787, 733, 295, 5407, 498, 291, 528, 13, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.26919523099573645, "compression_ratio": 1.6243902439024391, "no_speech_prob": 1.1298272511339746e-05}, {"id": 724, "seek": 444398, "start": 4443.98, "end": 4450.86, "text": " And then that's done in a way that you can back propagate gradient to it.", "tokens": [50364, 400, 550, 300, 311, 1096, 294, 257, 636, 300, 291, 393, 646, 48256, 16235, 281, 309, 13, 50708, 50772, 492, 603, 808, 646, 281, 341, 11, 281, 544, 721, 411, 341, 412, 264, 917, 498, 321, 393, 13, 50932, 51120, 407, 341, 307, 437, 341, 11, 264, 3779, 307, 15895, 281, 312, 867, 281, 472, 13, 51312, 51364, 440, 18515, 295, 264, 4846, 281, 264, 3779, 307, 15895, 281, 312, 867, 281, 472, 11, 597, 1355, 264, 4641, 295, 264, 3779, 8310, 1270, 300, 309, 1633, 312, 51648, 51684, 4356, 813, 264, 4641, 295, 264, 4846, 13, 663, 311, 337, 264, 1778, 286, 445, 8825, 13, 51836], "temperature": 0.0, "avg_logprob": -0.14288781542296802, "compression_ratio": 1.804255319148936, "no_speech_prob": 6.238964033400407e-06}, {"id": 725, "seek": 444398, "start": 4452.139999999999, "end": 4455.339999999999, "text": " We'll come back to this, to more things like this at the end if we can.", "tokens": [50364, 400, 550, 300, 311, 1096, 294, 257, 636, 300, 291, 393, 646, 48256, 16235, 281, 309, 13, 50708, 50772, 492, 603, 808, 646, 281, 341, 11, 281, 544, 721, 411, 341, 412, 264, 917, 498, 321, 393, 13, 50932, 51120, 407, 341, 307, 437, 341, 11, 264, 3779, 307, 15895, 281, 312, 867, 281, 472, 13, 51312, 51364, 440, 18515, 295, 264, 4846, 281, 264, 3779, 307, 15895, 281, 312, 867, 281, 472, 11, 597, 1355, 264, 4641, 295, 264, 3779, 8310, 1270, 300, 309, 1633, 312, 51648, 51684, 4356, 813, 264, 4641, 295, 264, 4846, 13, 663, 311, 337, 264, 1778, 286, 445, 8825, 13, 51836], "temperature": 0.0, "avg_logprob": -0.14288781542296802, "compression_ratio": 1.804255319148936, "no_speech_prob": 6.238964033400407e-06}, {"id": 726, "seek": 444398, "start": 4459.099999999999, "end": 4462.94, "text": " So this is what this, the target is assumed to be many to one.", "tokens": [50364, 400, 550, 300, 311, 1096, 294, 257, 636, 300, 291, 393, 646, 48256, 16235, 281, 309, 13, 50708, 50772, 492, 603, 808, 646, 281, 341, 11, 281, 544, 721, 411, 341, 412, 264, 917, 498, 321, 393, 13, 50932, 51120, 407, 341, 307, 437, 341, 11, 264, 3779, 307, 15895, 281, 312, 867, 281, 472, 13, 51312, 51364, 440, 18515, 295, 264, 4846, 281, 264, 3779, 307, 15895, 281, 312, 867, 281, 472, 11, 597, 1355, 264, 4641, 295, 264, 3779, 8310, 1270, 300, 309, 1633, 312, 51648, 51684, 4356, 813, 264, 4641, 295, 264, 4846, 13, 663, 311, 337, 264, 1778, 286, 445, 8825, 13, 51836], "temperature": 0.0, "avg_logprob": -0.14288781542296802, "compression_ratio": 1.804255319148936, "no_speech_prob": 6.238964033400407e-06}, {"id": 727, "seek": 444398, "start": 4463.98, "end": 4469.66, "text": " The alignment of the input to the target is assumed to be many to one, which means the length of the target sequence such that it must be", "tokens": [50364, 400, 550, 300, 311, 1096, 294, 257, 636, 300, 291, 393, 646, 48256, 16235, 281, 309, 13, 50708, 50772, 492, 603, 808, 646, 281, 341, 11, 281, 544, 721, 411, 341, 412, 264, 917, 498, 321, 393, 13, 50932, 51120, 407, 341, 307, 437, 341, 11, 264, 3779, 307, 15895, 281, 312, 867, 281, 472, 13, 51312, 51364, 440, 18515, 295, 264, 4846, 281, 264, 3779, 307, 15895, 281, 312, 867, 281, 472, 11, 597, 1355, 264, 4641, 295, 264, 3779, 8310, 1270, 300, 309, 1633, 312, 51648, 51684, 4356, 813, 264, 4641, 295, 264, 4846, 13, 663, 311, 337, 264, 1778, 286, 445, 8825, 13, 51836], "temperature": 0.0, "avg_logprob": -0.14288781542296802, "compression_ratio": 1.804255319148936, "no_speech_prob": 6.238964033400407e-06}, {"id": 728, "seek": 446966, "start": 4469.66, "end": 4473.42, "text": " smaller than the length of the input. That's for the reason I just explained.", "tokens": [50364, 4356, 813, 264, 4641, 295, 264, 4846, 13, 663, 311, 337, 264, 1778, 286, 445, 8825, 13, 50552, 50652, 1033, 13, 407, 309, 311, 1936, 819, 9364, 565, 1516, 3381, 13, 509, 727, 519, 295, 309, 341, 636, 13, 50808, 50880, 1610, 1333, 295, 257, 10088, 300, 775, 8546, 565, 1516, 3381, 420, 8546, 9410, 293, 307, 920, 819, 9364, 13, 51224, 51256, 440, 1558, 337, 341, 1709, 646, 294, 264, 2440, 4289, 82, 294, 264, 13244, 363, 346, 21189, 14476, 22288, 767, 13, 663, 311, 588, 1331, 13, 51508, 51556], "temperature": 0.0, "avg_logprob": -0.17936620404643397, "compression_ratio": 1.5836909871244635, "no_speech_prob": 9.08021775103407e-06}, {"id": 729, "seek": 446966, "start": 4475.42, "end": 4478.54, "text": " Okay. So it's basically differentiable time warping. You could think of it this way.", "tokens": [50364, 4356, 813, 264, 4641, 295, 264, 4846, 13, 663, 311, 337, 264, 1778, 286, 445, 8825, 13, 50552, 50652, 1033, 13, 407, 309, 311, 1936, 819, 9364, 565, 1516, 3381, 13, 509, 727, 519, 295, 309, 341, 636, 13, 50808, 50880, 1610, 1333, 295, 257, 10088, 300, 775, 8546, 565, 1516, 3381, 420, 8546, 9410, 293, 307, 920, 819, 9364, 13, 51224, 51256, 440, 1558, 337, 341, 1709, 646, 294, 264, 2440, 4289, 82, 294, 264, 13244, 363, 346, 21189, 14476, 22288, 767, 13, 663, 311, 588, 1331, 13, 51508, 51556], "temperature": 0.0, "avg_logprob": -0.17936620404643397, "compression_ratio": 1.5836909871244635, "no_speech_prob": 9.08021775103407e-06}, {"id": 730, "seek": 446966, "start": 4479.98, "end": 4486.86, "text": " Or sort of a module that does dynamic time warping or dynamic programming and is still differentiable.", "tokens": [50364, 4356, 813, 264, 4641, 295, 264, 4846, 13, 663, 311, 337, 264, 1778, 286, 445, 8825, 13, 50552, 50652, 1033, 13, 407, 309, 311, 1936, 819, 9364, 565, 1516, 3381, 13, 509, 727, 519, 295, 309, 341, 636, 13, 50808, 50880, 1610, 1333, 295, 257, 10088, 300, 775, 8546, 565, 1516, 3381, 420, 8546, 9410, 293, 307, 920, 819, 9364, 13, 51224, 51256, 440, 1558, 337, 341, 1709, 646, 294, 264, 2440, 4289, 82, 294, 264, 13244, 363, 346, 21189, 14476, 22288, 767, 13, 663, 311, 588, 1331, 13, 51508, 51556], "temperature": 0.0, "avg_logprob": -0.17936620404643397, "compression_ratio": 1.5836909871244635, "no_speech_prob": 9.08021775103407e-06}, {"id": 731, "seek": 446966, "start": 4487.5, "end": 4492.54, "text": " The idea for this goes back in the early 90s in the Leon Boutrous PhD thesis actually. That's very old.", "tokens": [50364, 4356, 813, 264, 4641, 295, 264, 4846, 13, 663, 311, 337, 264, 1778, 286, 445, 8825, 13, 50552, 50652, 1033, 13, 407, 309, 311, 1936, 819, 9364, 565, 1516, 3381, 13, 509, 727, 519, 295, 309, 341, 636, 13, 50808, 50880, 1610, 1333, 295, 257, 10088, 300, 775, 8546, 565, 1516, 3381, 420, 8546, 9410, 293, 307, 920, 819, 9364, 13, 51224, 51256, 440, 1558, 337, 341, 1709, 646, 294, 264, 2440, 4289, 82, 294, 264, 13244, 363, 346, 21189, 14476, 22288, 767, 13, 663, 311, 588, 1331, 13, 51508, 51556], "temperature": 0.0, "avg_logprob": -0.17936620404643397, "compression_ratio": 1.5836909871244635, "no_speech_prob": 9.08021775103407e-06}, {"id": 732, "seek": 449254, "start": 4492.54, "end": 4500.14, "text": " Is there a good paper or resource to learn more about that dynamic programming algorithm there?", "tokens": [50364, 1119, 456, 257, 665, 3035, 420, 7684, 281, 1466, 544, 466, 300, 8546, 9410, 9284, 456, 30, 50744, 50744, 865, 11, 767, 300, 311, 733, 295, 437, 286, 478, 516, 281, 751, 466, 958, 13, 50900, 50956, 286, 815, 406, 362, 565, 281, 352, 807, 309, 11, 457, 286, 603, 853, 281, 13, 51156, 51216, 583, 1936, 264, 1036, 644, 295, 264, 2281, 12, 6032, 2316, 7073, 13, 51472, 51556], "temperature": 0.0, "avg_logprob": -0.34897883733113605, "compression_ratio": 1.4684210526315788, "no_speech_prob": 7.295943760254886e-06}, {"id": 733, "seek": 449254, "start": 4500.14, "end": 4503.26, "text": " Yeah, actually that's kind of what I'm going to talk about next.", "tokens": [50364, 1119, 456, 257, 665, 3035, 420, 7684, 281, 1466, 544, 466, 300, 8546, 9410, 9284, 456, 30, 50744, 50744, 865, 11, 767, 300, 311, 733, 295, 437, 286, 478, 516, 281, 751, 466, 958, 13, 50900, 50956, 286, 815, 406, 362, 565, 281, 352, 807, 309, 11, 457, 286, 603, 853, 281, 13, 51156, 51216, 583, 1936, 264, 1036, 644, 295, 264, 2281, 12, 6032, 2316, 7073, 13, 51472, 51556], "temperature": 0.0, "avg_logprob": -0.34897883733113605, "compression_ratio": 1.4684210526315788, "no_speech_prob": 7.295943760254886e-06}, {"id": 734, "seek": 449254, "start": 4504.38, "end": 4508.38, "text": " I may not have time to go through it, but I'll try to.", "tokens": [50364, 1119, 456, 257, 665, 3035, 420, 7684, 281, 1466, 544, 466, 300, 8546, 9410, 9284, 456, 30, 50744, 50744, 865, 11, 767, 300, 311, 733, 295, 437, 286, 478, 516, 281, 751, 466, 958, 13, 50900, 50956, 286, 815, 406, 362, 565, 281, 352, 807, 309, 11, 457, 286, 603, 853, 281, 13, 51156, 51216, 583, 1936, 264, 1036, 644, 295, 264, 2281, 12, 6032, 2316, 7073, 13, 51472, 51556], "temperature": 0.0, "avg_logprob": -0.34897883733113605, "compression_ratio": 1.4684210526315788, "no_speech_prob": 7.295943760254886e-06}, {"id": 735, "seek": 449254, "start": 4509.58, "end": 4514.7, "text": " But basically the last part of the energy-based model tutorial.", "tokens": [50364, 1119, 456, 257, 665, 3035, 420, 7684, 281, 1466, 544, 466, 300, 8546, 9410, 9284, 456, 30, 50744, 50744, 865, 11, 767, 300, 311, 733, 295, 437, 286, 478, 516, 281, 751, 466, 958, 13, 50900, 50956, 286, 815, 406, 362, 565, 281, 352, 807, 309, 11, 457, 286, 603, 853, 281, 13, 51156, 51216, 583, 1936, 264, 1036, 644, 295, 264, 2281, 12, 6032, 2316, 7073, 13, 51472, 51556], "temperature": 0.0, "avg_logprob": -0.34897883733113605, "compression_ratio": 1.4684210526315788, "no_speech_prob": 7.295943760254886e-06}, {"id": 736, "seek": 451470, "start": 4514.7, "end": 4525.099999999999, "text": " Energy-based model tutorial, the 2006 paper that we gave you a reference to.", "tokens": [50364, 14939, 12, 6032, 2316, 7073, 11, 264, 14062, 3035, 300, 321, 2729, 291, 257, 6408, 281, 13, 50884, 50948, 316, 7073, 322, 2281, 12, 6032, 5245, 13, 440, 1150, 644, 307, 439, 466, 341, 733, 295, 1507, 4476, 13, 51380, 51432, 1033, 11, 370, 309, 311, 544, 2281, 12, 6032, 5245, 11, 457, 586, 294, 733, 295, 544, 295, 264, 46533, 4319, 498, 291, 528, 13, 51688, 51792], "temperature": 0.0, "avg_logprob": -0.20366615567888532, "compression_ratio": 1.5449438202247192, "no_speech_prob": 1.1121261195512488e-05}, {"id": 737, "seek": 451470, "start": 4526.38, "end": 4535.0199999999995, "text": " A tutorial on energy-based models. The second part is all about this kind of stuff essentially.", "tokens": [50364, 14939, 12, 6032, 2316, 7073, 11, 264, 14062, 3035, 300, 321, 2729, 291, 257, 6408, 281, 13, 50884, 50948, 316, 7073, 322, 2281, 12, 6032, 5245, 13, 440, 1150, 644, 307, 439, 466, 341, 733, 295, 1507, 4476, 13, 51380, 51432, 1033, 11, 370, 309, 311, 544, 2281, 12, 6032, 5245, 11, 457, 586, 294, 733, 295, 544, 295, 264, 46533, 4319, 498, 291, 528, 13, 51688, 51792], "temperature": 0.0, "avg_logprob": -0.20366615567888532, "compression_ratio": 1.5449438202247192, "no_speech_prob": 1.1121261195512488e-05}, {"id": 738, "seek": 451470, "start": 4536.0599999999995, "end": 4541.179999999999, "text": " Okay, so it's more energy-based models, but now in kind of more of the supervised context if you want.", "tokens": [50364, 14939, 12, 6032, 2316, 7073, 11, 264, 14062, 3035, 300, 321, 2729, 291, 257, 6408, 281, 13, 50884, 50948, 316, 7073, 322, 2281, 12, 6032, 5245, 13, 440, 1150, 644, 307, 439, 466, 341, 733, 295, 1507, 4476, 13, 51380, 51432, 1033, 11, 370, 309, 311, 544, 2281, 12, 6032, 5245, 11, 457, 586, 294, 733, 295, 544, 295, 264, 46533, 4319, 498, 291, 528, 13, 51688, 51792], "temperature": 0.0, "avg_logprob": -0.20366615567888532, "compression_ratio": 1.5449438202247192, "no_speech_prob": 1.1121261195512488e-05}, {"id": 739, "seek": 454118, "start": 4541.18, "end": 4551.900000000001, "text": " So before I get to this, I want to come back to the more general formulation of energy-based models.", "tokens": [50364, 407, 949, 286, 483, 281, 341, 11, 286, 528, 281, 808, 646, 281, 264, 544, 2674, 37642, 295, 2281, 12, 6032, 5245, 13, 50900, 50900, 400, 264, 1558, 300, 498, 291, 528, 281, 733, 295, 6964, 2281, 12, 6032, 5245, 294, 264, 2296, 636, 11, 51452, 51452, 613, 366, 264, 27708, 9606, 13, 509, 362, 257, 3097, 992, 11, 257, 3840, 295, 15494, 11, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.25765348883236155, "compression_ratio": 1.5739644970414202, "no_speech_prob": 6.4383830249425955e-06}, {"id": 740, "seek": 454118, "start": 4551.900000000001, "end": 4562.9400000000005, "text": " And the idea that if you want to kind of define energy-based models in the proper way,", "tokens": [50364, 407, 949, 286, 483, 281, 341, 11, 286, 528, 281, 808, 646, 281, 264, 544, 2674, 37642, 295, 2281, 12, 6032, 5245, 13, 50900, 50900, 400, 264, 1558, 300, 498, 291, 528, 281, 733, 295, 6964, 2281, 12, 6032, 5245, 294, 264, 2296, 636, 11, 51452, 51452, 613, 366, 264, 27708, 9606, 13, 509, 362, 257, 3097, 992, 11, 257, 3840, 295, 15494, 11, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.25765348883236155, "compression_ratio": 1.5739644970414202, "no_speech_prob": 6.4383830249425955e-06}, {"id": 741, "seek": 454118, "start": 4562.9400000000005, "end": 4569.5, "text": " these are the conditional versions. You have a training set, a bunch of pairs,", "tokens": [50364, 407, 949, 286, 483, 281, 341, 11, 286, 528, 281, 808, 646, 281, 264, 544, 2674, 37642, 295, 2281, 12, 6032, 5245, 13, 50900, 50900, 400, 264, 1558, 300, 498, 291, 528, 281, 733, 295, 6964, 2281, 12, 6032, 5245, 294, 264, 2296, 636, 11, 51452, 51452, 613, 366, 264, 27708, 9606, 13, 509, 362, 257, 3097, 992, 11, 257, 3840, 295, 15494, 11, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.25765348883236155, "compression_ratio": 1.5739644970414202, "no_speech_prob": 6.4383830249425955e-06}, {"id": 742, "seek": 456950, "start": 4569.5, "end": 4575.26, "text": " a training set, a bunch of pairs, x, y, y, i for i equals 1 to p. You have a loss functional.", "tokens": [50364, 257, 3097, 992, 11, 257, 3840, 295, 15494, 11, 2031, 11, 288, 11, 288, 11, 741, 337, 741, 6915, 502, 281, 280, 13, 509, 362, 257, 4470, 11745, 13, 50652, 50652, 407, 264, 4470, 11745, 441, 295, 462, 293, 318, 13, 407, 309, 2516, 264, 2281, 2445, 40610, 538, 264, 1185, 50924, 50988, 293, 264, 3097, 992, 293, 309, 2709, 291, 257, 39684, 2158, 13, 823, 291, 393, 519, 295, 341, 382, 257, 11745, 13, 51272, 51272, 316, 11745, 307, 257, 2445, 295, 257, 2445, 13, 583, 294, 1186, 11, 570, 264, 2281, 2445, 2564, 307, 51508, 51508, 13075, 1602, 538, 13075, 261, 11, 291, 393, 1261, 341, 4470, 11745, 666, 257, 4470, 2445, 11, 597, 307, 406, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.17105802942494877, "compression_ratio": 1.9268292682926829, "no_speech_prob": 1.6435877114417963e-05}, {"id": 743, "seek": 456950, "start": 4575.26, "end": 4580.7, "text": " So the loss functional L of E and S. So it takes the energy function computed by the system", "tokens": [50364, 257, 3097, 992, 11, 257, 3840, 295, 15494, 11, 2031, 11, 288, 11, 288, 11, 741, 337, 741, 6915, 502, 281, 280, 13, 509, 362, 257, 4470, 11745, 13, 50652, 50652, 407, 264, 4470, 11745, 441, 295, 462, 293, 318, 13, 407, 309, 2516, 264, 2281, 2445, 40610, 538, 264, 1185, 50924, 50988, 293, 264, 3097, 992, 293, 309, 2709, 291, 257, 39684, 2158, 13, 823, 291, 393, 519, 295, 341, 382, 257, 11745, 13, 51272, 51272, 316, 11745, 307, 257, 2445, 295, 257, 2445, 13, 583, 294, 1186, 11, 570, 264, 2281, 2445, 2564, 307, 51508, 51508, 13075, 1602, 538, 13075, 261, 11, 291, 393, 1261, 341, 4470, 11745, 666, 257, 4470, 2445, 11, 597, 307, 406, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.17105802942494877, "compression_ratio": 1.9268292682926829, "no_speech_prob": 1.6435877114417963e-05}, {"id": 744, "seek": 456950, "start": 4581.98, "end": 4587.66, "text": " and the training set and it gives you a scalar value. Now you can think of this as a functional.", "tokens": [50364, 257, 3097, 992, 11, 257, 3840, 295, 15494, 11, 2031, 11, 288, 11, 288, 11, 741, 337, 741, 6915, 502, 281, 280, 13, 509, 362, 257, 4470, 11745, 13, 50652, 50652, 407, 264, 4470, 11745, 441, 295, 462, 293, 318, 13, 407, 309, 2516, 264, 2281, 2445, 40610, 538, 264, 1185, 50924, 50988, 293, 264, 3097, 992, 293, 309, 2709, 291, 257, 39684, 2158, 13, 823, 291, 393, 519, 295, 341, 382, 257, 11745, 13, 51272, 51272, 316, 11745, 307, 257, 2445, 295, 257, 2445, 13, 583, 294, 1186, 11, 570, 264, 2281, 2445, 2564, 307, 51508, 51508, 13075, 1602, 538, 13075, 261, 11, 291, 393, 1261, 341, 4470, 11745, 666, 257, 4470, 2445, 11, 597, 307, 406, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.17105802942494877, "compression_ratio": 1.9268292682926829, "no_speech_prob": 1.6435877114417963e-05}, {"id": 745, "seek": 456950, "start": 4587.66, "end": 4592.38, "text": " A functional is a function of a function. But in fact, because the energy function itself is", "tokens": [50364, 257, 3097, 992, 11, 257, 3840, 295, 15494, 11, 2031, 11, 288, 11, 288, 11, 741, 337, 741, 6915, 502, 281, 280, 13, 509, 362, 257, 4470, 11745, 13, 50652, 50652, 407, 264, 4470, 11745, 441, 295, 462, 293, 318, 13, 407, 309, 2516, 264, 2281, 2445, 40610, 538, 264, 1185, 50924, 50988, 293, 264, 3097, 992, 293, 309, 2709, 291, 257, 39684, 2158, 13, 823, 291, 393, 519, 295, 341, 382, 257, 11745, 13, 51272, 51272, 316, 11745, 307, 257, 2445, 295, 257, 2445, 13, 583, 294, 1186, 11, 570, 264, 2281, 2445, 2564, 307, 51508, 51508, 13075, 1602, 538, 13075, 261, 11, 291, 393, 1261, 341, 4470, 11745, 666, 257, 4470, 2445, 11, 597, 307, 406, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.17105802942494877, "compression_ratio": 1.9268292682926829, "no_speech_prob": 1.6435877114417963e-05}, {"id": 746, "seek": 456950, "start": 4592.38, "end": 4597.66, "text": " parameterized by parameter w, you can turn this loss functional into a loss function, which is not", "tokens": [50364, 257, 3097, 992, 11, 257, 3840, 295, 15494, 11, 2031, 11, 288, 11, 288, 11, 741, 337, 741, 6915, 502, 281, 280, 13, 509, 362, 257, 4470, 11745, 13, 50652, 50652, 407, 264, 4470, 11745, 441, 295, 462, 293, 318, 13, 407, 309, 2516, 264, 2281, 2445, 40610, 538, 264, 1185, 50924, 50988, 293, 264, 3097, 992, 293, 309, 2709, 291, 257, 39684, 2158, 13, 823, 291, 393, 519, 295, 341, 382, 257, 11745, 13, 51272, 51272, 316, 11745, 307, 257, 2445, 295, 257, 2445, 13, 583, 294, 1186, 11, 570, 264, 2281, 2445, 2564, 307, 51508, 51508, 13075, 1602, 538, 13075, 261, 11, 291, 393, 1261, 341, 4470, 11745, 666, 257, 4470, 2445, 11, 597, 307, 406, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.17105802942494877, "compression_ratio": 1.9268292682926829, "no_speech_prob": 1.6435877114417963e-05}, {"id": 747, "seek": 459766, "start": 4597.66, "end": 4606.22, "text": " just a function of w, not a function of the energy function. And of course, the set of energy functions", "tokens": [50364, 445, 257, 2445, 295, 261, 11, 406, 257, 2445, 295, 264, 2281, 2445, 13, 400, 295, 1164, 11, 264, 992, 295, 2281, 6828, 50792, 50792, 307, 1219, 17889, 510, 13, 467, 311, 13075, 1602, 538, 264, 13075, 261, 11, 597, 307, 2726, 1951, 264, 992, 13, 51044, 51168, 407, 3097, 14689, 294, 11, 295, 1164, 11, 46608, 264, 4470, 11745, 365, 3104, 281, 261, 293, 5006, 51400, 51400, 264, 261, 300, 4464, 5660, 309, 13, 400, 370, 472, 1168, 291, 1062, 1029, 1803, 11, 291, 458, 11, 286, 1437, 807, 257, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.10877857208251954, "compression_ratio": 1.7647058823529411, "no_speech_prob": 1.0614901839289814e-05}, {"id": 748, "seek": 459766, "start": 4606.22, "end": 4611.26, "text": " is called epsilon here. It's parameterized by the parameter w, which is taken within the set.", "tokens": [50364, 445, 257, 2445, 295, 261, 11, 406, 257, 2445, 295, 264, 2281, 2445, 13, 400, 295, 1164, 11, 264, 992, 295, 2281, 6828, 50792, 50792, 307, 1219, 17889, 510, 13, 467, 311, 13075, 1602, 538, 264, 13075, 261, 11, 597, 307, 2726, 1951, 264, 992, 13, 51044, 51168, 407, 3097, 14689, 294, 11, 295, 1164, 11, 46608, 264, 4470, 11745, 365, 3104, 281, 261, 293, 5006, 51400, 51400, 264, 261, 300, 4464, 5660, 309, 13, 400, 370, 472, 1168, 291, 1062, 1029, 1803, 11, 291, 458, 11, 286, 1437, 807, 257, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.10877857208251954, "compression_ratio": 1.7647058823529411, "no_speech_prob": 1.0614901839289814e-05}, {"id": 749, "seek": 459766, "start": 4613.74, "end": 4618.38, "text": " So training consists in, of course, minimizing the loss functional with respect to w and finding", "tokens": [50364, 445, 257, 2445, 295, 261, 11, 406, 257, 2445, 295, 264, 2281, 2445, 13, 400, 295, 1164, 11, 264, 992, 295, 2281, 6828, 50792, 50792, 307, 1219, 17889, 510, 13, 467, 311, 13075, 1602, 538, 264, 13075, 261, 11, 597, 307, 2726, 1951, 264, 992, 13, 51044, 51168, 407, 3097, 14689, 294, 11, 295, 1164, 11, 46608, 264, 4470, 11745, 365, 3104, 281, 261, 293, 5006, 51400, 51400, 264, 261, 300, 4464, 5660, 309, 13, 400, 370, 472, 1168, 291, 1062, 1029, 1803, 11, 291, 458, 11, 286, 1437, 807, 257, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.10877857208251954, "compression_ratio": 1.7647058823529411, "no_speech_prob": 1.0614901839289814e-05}, {"id": 750, "seek": 459766, "start": 4618.38, "end": 4623.82, "text": " the w that minimizes it. And so one question you might ask yourself, you know, I went through a", "tokens": [50364, 445, 257, 2445, 295, 261, 11, 406, 257, 2445, 295, 264, 2281, 2445, 13, 400, 295, 1164, 11, 264, 992, 295, 2281, 6828, 50792, 50792, 307, 1219, 17889, 510, 13, 467, 311, 13075, 1602, 538, 264, 13075, 261, 11, 597, 307, 2726, 1951, 264, 992, 13, 51044, 51168, 407, 3097, 14689, 294, 11, 295, 1164, 11, 46608, 264, 4470, 11745, 365, 3104, 281, 261, 293, 5006, 51400, 51400, 264, 261, 300, 4464, 5660, 309, 13, 400, 370, 472, 1168, 291, 1062, 1029, 1803, 11, 291, 458, 11, 286, 1437, 807, 257, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.10877857208251954, "compression_ratio": 1.7647058823529411, "no_speech_prob": 1.0614901839289814e-05}, {"id": 751, "seek": 462382, "start": 4623.82, "end": 4628.299999999999, "text": " whole bunch of objective function loss functions here. And the question is, if you are in an energy", "tokens": [50364, 1379, 3840, 295, 10024, 2445, 4470, 6828, 510, 13, 400, 264, 1168, 307, 11, 498, 291, 366, 294, 364, 2281, 50588, 50588, 2361, 8388, 11, 437, 4470, 6828, 366, 665, 2306, 293, 437, 4470, 6828, 366, 1578, 2306, 30, 50844, 50892, 1012, 360, 291, 38463, 257, 4470, 2445, 300, 767, 486, 360, 746, 4420, 337, 291, 30, 1033, 13, 51172, 51244, 407, 510, 307, 257, 2674, 37642, 295, 257, 4470, 2445, 13, 467, 311, 364, 4274, 670, 3097, 10938, 13, 51524, 51596, 407, 510, 286, 478, 733, 295, 11926, 300, 309, 311, 33270, 394, 833, 4784, 11380, 295, 264, 10938, 13, 407, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10484908661752376, "compression_ratio": 1.7898832684824904, "no_speech_prob": 5.954921562079107e-06}, {"id": 752, "seek": 462382, "start": 4628.299999999999, "end": 4633.42, "text": " based framework, what loss functions are good ones and what loss functions are bad ones?", "tokens": [50364, 1379, 3840, 295, 10024, 2445, 4470, 6828, 510, 13, 400, 264, 1168, 307, 11, 498, 291, 366, 294, 364, 2281, 50588, 50588, 2361, 8388, 11, 437, 4470, 6828, 366, 665, 2306, 293, 437, 4470, 6828, 366, 1578, 2306, 30, 50844, 50892, 1012, 360, 291, 38463, 257, 4470, 2445, 300, 767, 486, 360, 746, 4420, 337, 291, 30, 1033, 13, 51172, 51244, 407, 510, 307, 257, 2674, 37642, 295, 257, 4470, 2445, 13, 467, 311, 364, 4274, 670, 3097, 10938, 13, 51524, 51596, 407, 510, 286, 478, 733, 295, 11926, 300, 309, 311, 33270, 394, 833, 4784, 11380, 295, 264, 10938, 13, 407, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10484908661752376, "compression_ratio": 1.7898832684824904, "no_speech_prob": 5.954921562079107e-06}, {"id": 753, "seek": 462382, "start": 4634.38, "end": 4639.98, "text": " How do you characterize a loss function that actually will do something useful for you? Okay.", "tokens": [50364, 1379, 3840, 295, 10024, 2445, 4470, 6828, 510, 13, 400, 264, 1168, 307, 11, 498, 291, 366, 294, 364, 2281, 50588, 50588, 2361, 8388, 11, 437, 4470, 6828, 366, 665, 2306, 293, 437, 4470, 6828, 366, 1578, 2306, 30, 50844, 50892, 1012, 360, 291, 38463, 257, 4470, 2445, 300, 767, 486, 360, 746, 4420, 337, 291, 30, 1033, 13, 51172, 51244, 407, 510, 307, 257, 2674, 37642, 295, 257, 4470, 2445, 13, 467, 311, 364, 4274, 670, 3097, 10938, 13, 51524, 51596, 407, 510, 286, 478, 733, 295, 11926, 300, 309, 311, 33270, 394, 833, 4784, 11380, 295, 264, 10938, 13, 407, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10484908661752376, "compression_ratio": 1.7898832684824904, "no_speech_prob": 5.954921562079107e-06}, {"id": 754, "seek": 462382, "start": 4641.42, "end": 4647.0199999999995, "text": " So here is a general formulation of a loss function. It's an average over training samples.", "tokens": [50364, 1379, 3840, 295, 10024, 2445, 4470, 6828, 510, 13, 400, 264, 1168, 307, 11, 498, 291, 366, 294, 364, 2281, 50588, 50588, 2361, 8388, 11, 437, 4470, 6828, 366, 665, 2306, 293, 437, 4470, 6828, 366, 1578, 2306, 30, 50844, 50892, 1012, 360, 291, 38463, 257, 4470, 2445, 300, 767, 486, 360, 746, 4420, 337, 291, 30, 1033, 13, 51172, 51244, 407, 510, 307, 257, 2674, 37642, 295, 257, 4470, 2445, 13, 467, 311, 364, 4274, 670, 3097, 10938, 13, 51524, 51596, 407, 510, 286, 478, 733, 295, 11926, 300, 309, 311, 33270, 394, 833, 4784, 11380, 295, 264, 10938, 13, 407, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10484908661752376, "compression_ratio": 1.7898832684824904, "no_speech_prob": 5.954921562079107e-06}, {"id": 755, "seek": 462382, "start": 4648.46, "end": 4653.58, "text": " So here I'm kind of assuming that it's invariant under permutation of the samples. So", "tokens": [50364, 1379, 3840, 295, 10024, 2445, 4470, 6828, 510, 13, 400, 264, 1168, 307, 11, 498, 291, 366, 294, 364, 2281, 50588, 50588, 2361, 8388, 11, 437, 4470, 6828, 366, 665, 2306, 293, 437, 4470, 6828, 366, 1578, 2306, 30, 50844, 50892, 1012, 360, 291, 38463, 257, 4470, 2445, 300, 767, 486, 360, 746, 4420, 337, 291, 30, 1033, 13, 51172, 51244, 407, 510, 307, 257, 2674, 37642, 295, 257, 4470, 2445, 13, 467, 311, 364, 4274, 670, 3097, 10938, 13, 51524, 51596, 407, 510, 286, 478, 733, 295, 11926, 300, 309, 311, 33270, 394, 833, 4784, 11380, 295, 264, 10938, 13, 407, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10484908661752376, "compression_ratio": 1.7898832684824904, "no_speech_prob": 5.954921562079107e-06}, {"id": 756, "seek": 465358, "start": 4653.58, "end": 4658.78, "text": " an average is as good as any other aggregation aggregating function. So it's the average over", "tokens": [50364, 364, 4274, 307, 382, 665, 382, 604, 661, 16743, 399, 16743, 990, 2445, 13, 407, 309, 311, 264, 4274, 670, 50624, 50624, 3097, 10938, 295, 257, 680, 6889, 4470, 2445, 11, 4238, 441, 13, 400, 309, 2516, 264, 14721, 1867, 11, 50932, 50932, 288, 11, 597, 727, 312, 445, 257, 7719, 420, 309, 727, 312, 257, 1379, 3256, 420, 2035, 13, 400, 309, 2516, 264, 51296, 51296, 2281, 2445, 689, 2031, 11, 264, 2031, 7006, 36800, 307, 2681, 281, 36800, 11, 264, 4435, 3097, 6889, 13, 51624, 51700], "temperature": 0.0, "avg_logprob": -0.1426585939195421, "compression_ratio": 1.7403846153846154, "no_speech_prob": 2.4299028154928237e-05}, {"id": 757, "seek": 465358, "start": 4658.78, "end": 4664.94, "text": " training samples of a per sample loss function, capital L. And it takes the desired answer,", "tokens": [50364, 364, 4274, 307, 382, 665, 382, 604, 661, 16743, 399, 16743, 990, 2445, 13, 407, 309, 311, 264, 4274, 670, 50624, 50624, 3097, 10938, 295, 257, 680, 6889, 4470, 2445, 11, 4238, 441, 13, 400, 309, 2516, 264, 14721, 1867, 11, 50932, 50932, 288, 11, 597, 727, 312, 445, 257, 7719, 420, 309, 727, 312, 257, 1379, 3256, 420, 2035, 13, 400, 309, 2516, 264, 51296, 51296, 2281, 2445, 689, 2031, 11, 264, 2031, 7006, 36800, 307, 2681, 281, 36800, 11, 264, 4435, 3097, 6889, 13, 51624, 51700], "temperature": 0.0, "avg_logprob": -0.1426585939195421, "compression_ratio": 1.7403846153846154, "no_speech_prob": 2.4299028154928237e-05}, {"id": 758, "seek": 465358, "start": 4664.94, "end": 4672.22, "text": " y, which could be just a category or it could be a whole image or whatever. And it takes the", "tokens": [50364, 364, 4274, 307, 382, 665, 382, 604, 661, 16743, 399, 16743, 990, 2445, 13, 407, 309, 311, 264, 4274, 670, 50624, 50624, 3097, 10938, 295, 257, 680, 6889, 4470, 2445, 11, 4238, 441, 13, 400, 309, 2516, 264, 14721, 1867, 11, 50932, 50932, 288, 11, 597, 727, 312, 445, 257, 7719, 420, 309, 727, 312, 257, 1379, 3256, 420, 2035, 13, 400, 309, 2516, 264, 51296, 51296, 2281, 2445, 689, 2031, 11, 264, 2031, 7006, 36800, 307, 2681, 281, 36800, 11, 264, 4435, 3097, 6889, 13, 51624, 51700], "temperature": 0.0, "avg_logprob": -0.1426585939195421, "compression_ratio": 1.7403846153846154, "no_speech_prob": 2.4299028154928237e-05}, {"id": 759, "seek": 465358, "start": 4672.22, "end": 4678.78, "text": " energy function where x, the x variable xi is equal to xi, the ice training sample.", "tokens": [50364, 364, 4274, 307, 382, 665, 382, 604, 661, 16743, 399, 16743, 990, 2445, 13, 407, 309, 311, 264, 4274, 670, 50624, 50624, 3097, 10938, 295, 257, 680, 6889, 4470, 2445, 11, 4238, 441, 13, 400, 309, 2516, 264, 14721, 1867, 11, 50932, 50932, 288, 11, 597, 727, 312, 445, 257, 7719, 420, 309, 727, 312, 257, 1379, 3256, 420, 2035, 13, 400, 309, 2516, 264, 51296, 51296, 2281, 2445, 689, 2031, 11, 264, 2031, 7006, 36800, 307, 2681, 281, 36800, 11, 264, 4435, 3097, 6889, 13, 51624, 51700], "temperature": 0.0, "avg_logprob": -0.1426585939195421, "compression_ratio": 1.7403846153846154, "no_speech_prob": 2.4299028154928237e-05}, {"id": 760, "seek": 467878, "start": 4678.78, "end": 4687.58, "text": " But the y variable is undetermined. So E of w, y and xi is basically the entire shape of the energy", "tokens": [50364, 583, 264, 288, 7006, 307, 674, 35344, 2001, 13, 407, 462, 295, 261, 11, 288, 293, 36800, 307, 1936, 264, 2302, 3909, 295, 264, 2281, 50804, 50804, 2445, 337, 439, 4190, 295, 288, 11, 670, 4190, 295, 288, 337, 257, 2212, 2031, 11, 2031, 2681, 281, 36800, 13, 400, 291, 393, 362, 51112, 51112, 257, 3890, 6545, 498, 291, 528, 13, 407, 510, 341, 307, 257, 4470, 11745, 797, 13, 400, 797, 11, 295, 1164, 11, 51512, 51540, 321, 362, 281, 1715, 341, 4470, 2445, 611, 300, 309, 1669, 264, 2281, 295, 3006, 1867, 1359, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.21885661886195945, "compression_ratio": 1.654867256637168, "no_speech_prob": 1.7502194168628193e-05}, {"id": 761, "seek": 467878, "start": 4687.58, "end": 4693.74, "text": " function for all values of y, over values of y for a given x, x equal to xi. And you can have", "tokens": [50364, 583, 264, 288, 7006, 307, 674, 35344, 2001, 13, 407, 462, 295, 261, 11, 288, 293, 36800, 307, 1936, 264, 2302, 3909, 295, 264, 2281, 50804, 50804, 2445, 337, 439, 4190, 295, 288, 11, 670, 4190, 295, 288, 337, 257, 2212, 2031, 11, 2031, 2681, 281, 36800, 13, 400, 291, 393, 362, 51112, 51112, 257, 3890, 6545, 498, 291, 528, 13, 407, 510, 341, 307, 257, 4470, 11745, 797, 13, 400, 797, 11, 295, 1164, 11, 51512, 51540, 321, 362, 281, 1715, 341, 4470, 2445, 611, 300, 309, 1669, 264, 2281, 295, 3006, 1867, 1359, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.21885661886195945, "compression_ratio": 1.654867256637168, "no_speech_prob": 1.7502194168628193e-05}, {"id": 762, "seek": 467878, "start": 4693.74, "end": 4701.74, "text": " a regularizer if you want. So here this is a loss functional again. And again, of course,", "tokens": [50364, 583, 264, 288, 7006, 307, 674, 35344, 2001, 13, 407, 462, 295, 261, 11, 288, 293, 36800, 307, 1936, 264, 2302, 3909, 295, 264, 2281, 50804, 50804, 2445, 337, 439, 4190, 295, 288, 11, 670, 4190, 295, 288, 337, 257, 2212, 2031, 11, 2031, 2681, 281, 36800, 13, 400, 291, 393, 362, 51112, 51112, 257, 3890, 6545, 498, 291, 528, 13, 407, 510, 341, 307, 257, 4470, 11745, 797, 13, 400, 797, 11, 295, 1164, 11, 51512, 51540, 321, 362, 281, 1715, 341, 4470, 2445, 611, 300, 309, 1669, 264, 2281, 295, 3006, 1867, 1359, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.21885661886195945, "compression_ratio": 1.654867256637168, "no_speech_prob": 1.7502194168628193e-05}, {"id": 763, "seek": 467878, "start": 4702.3, "end": 4705.74, "text": " we have to design this loss function also that it makes the energy of correct answer small", "tokens": [50364, 583, 264, 288, 7006, 307, 674, 35344, 2001, 13, 407, 462, 295, 261, 11, 288, 293, 36800, 307, 1936, 264, 2302, 3909, 295, 264, 2281, 50804, 50804, 2445, 337, 439, 4190, 295, 288, 11, 670, 4190, 295, 288, 337, 257, 2212, 2031, 11, 2031, 2681, 281, 36800, 13, 400, 291, 393, 362, 51112, 51112, 257, 3890, 6545, 498, 291, 528, 13, 407, 510, 341, 307, 257, 4470, 11745, 797, 13, 400, 797, 11, 295, 1164, 11, 51512, 51540, 321, 362, 281, 1715, 341, 4470, 2445, 611, 300, 309, 1669, 264, 2281, 295, 3006, 1867, 1359, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.21885661886195945, "compression_ratio": 1.654867256637168, "no_speech_prob": 1.7502194168628193e-05}, {"id": 764, "seek": 470574, "start": 4705.74, "end": 4711.74, "text": " and the energy of incorrect answers large in some ways. Now we're going to go through a bunch of", "tokens": [50364, 293, 264, 2281, 295, 18424, 6338, 2416, 294, 512, 2098, 13, 823, 321, 434, 516, 281, 352, 807, 257, 3840, 295, 50664, 50664, 819, 3467, 295, 4470, 6828, 13, 407, 472, 551, 321, 727, 360, 307, 584, 452, 4470, 2445, 307, 445, 516, 281, 51060, 51060, 312, 264, 2281, 295, 264, 3006, 1867, 13, 407, 286, 478, 516, 281, 1081, 2059, 294, 264, 4319, 295, 364, 2281, 12, 6032, 51348, 51348, 2316, 13, 1222, 1185, 14725, 13444, 13, 286, 7302, 729, 13444, 382, 25737, 13, 407, 286, 478, 516, 281, 747, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.2237681740208676, "compression_ratio": 1.7889908256880733, "no_speech_prob": 2.0143457732046954e-05}, {"id": 765, "seek": 470574, "start": 4711.74, "end": 4719.66, "text": " different types of loss functions. So one thing we could do is say my loss function is just going to", "tokens": [50364, 293, 264, 2281, 295, 18424, 6338, 2416, 294, 512, 2098, 13, 823, 321, 434, 516, 281, 352, 807, 257, 3840, 295, 50664, 50664, 819, 3467, 295, 4470, 6828, 13, 407, 472, 551, 321, 727, 360, 307, 584, 452, 4470, 2445, 307, 445, 516, 281, 51060, 51060, 312, 264, 2281, 295, 264, 3006, 1867, 13, 407, 286, 478, 516, 281, 1081, 2059, 294, 264, 4319, 295, 364, 2281, 12, 6032, 51348, 51348, 2316, 13, 1222, 1185, 14725, 13444, 13, 286, 7302, 729, 13444, 382, 25737, 13, 407, 286, 478, 516, 281, 747, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.2237681740208676, "compression_ratio": 1.7889908256880733, "no_speech_prob": 2.0143457732046954e-05}, {"id": 766, "seek": 470574, "start": 4719.66, "end": 4725.42, "text": " be the energy of the correct answer. So I'm going to place myself in the context of an energy-based", "tokens": [50364, 293, 264, 2281, 295, 18424, 6338, 2416, 294, 512, 2098, 13, 823, 321, 434, 516, 281, 352, 807, 257, 3840, 295, 50664, 50664, 819, 3467, 295, 4470, 6828, 13, 407, 472, 551, 321, 727, 360, 307, 584, 452, 4470, 2445, 307, 445, 516, 281, 51060, 51060, 312, 264, 2281, 295, 264, 3006, 1867, 13, 407, 286, 478, 516, 281, 1081, 2059, 294, 264, 4319, 295, 364, 2281, 12, 6032, 51348, 51348, 2316, 13, 1222, 1185, 14725, 13444, 13, 286, 7302, 729, 13444, 382, 25737, 13, 407, 286, 478, 516, 281, 747, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.2237681740208676, "compression_ratio": 1.7889908256880733, "no_speech_prob": 2.0143457732046954e-05}, {"id": 767, "seek": 470574, "start": 4725.42, "end": 4731.179999999999, "text": " model. My system produces scores. I interpret those scores as energies. So I'm going to take", "tokens": [50364, 293, 264, 2281, 295, 18424, 6338, 2416, 294, 512, 2098, 13, 823, 321, 434, 516, 281, 352, 807, 257, 3840, 295, 50664, 50664, 819, 3467, 295, 4470, 6828, 13, 407, 472, 551, 321, 727, 360, 307, 584, 452, 4470, 2445, 307, 445, 516, 281, 51060, 51060, 312, 264, 2281, 295, 264, 3006, 1867, 13, 407, 286, 478, 516, 281, 1081, 2059, 294, 264, 4319, 295, 364, 2281, 12, 6032, 51348, 51348, 2316, 13, 1222, 1185, 14725, 13444, 13, 286, 7302, 729, 13444, 382, 25737, 13, 407, 286, 478, 516, 281, 747, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.2237681740208676, "compression_ratio": 1.7889908256880733, "no_speech_prob": 2.0143457732046954e-05}, {"id": 768, "seek": 473118, "start": 4731.18, "end": 4740.860000000001, "text": " my low as good as opposed to positive scores. And what I'm just going to do is", "tokens": [50364, 452, 2295, 382, 665, 382, 8851, 281, 3353, 13444, 13, 400, 437, 286, 478, 445, 516, 281, 360, 307, 50848, 50916, 6964, 452, 2281, 11745, 382, 257, 2445, 295, 264, 2281, 2445, 293, 257, 2445, 295, 288, 13, 51196, 51196, 663, 311, 2935, 264, 2281, 300, 452, 2316, 2709, 281, 264, 3006, 1867, 13, 407, 1936, 11, 51476, 51476, 286, 478, 516, 281, 6964, 452, 2281, 2445, 382, 257, 2445, 295, 264, 2281, 2445, 293, 257, 2445, 295, 288, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.5312604450044178, "compression_ratio": 2.029585798816568, "no_speech_prob": 8.529658771294635e-06}, {"id": 769, "seek": 473118, "start": 4742.22, "end": 4747.820000000001, "text": " define my energy functional as a function of the energy function and a function of y.", "tokens": [50364, 452, 2295, 382, 665, 382, 8851, 281, 3353, 13444, 13, 400, 437, 286, 478, 445, 516, 281, 360, 307, 50848, 50916, 6964, 452, 2281, 11745, 382, 257, 2445, 295, 264, 2281, 2445, 293, 257, 2445, 295, 288, 13, 51196, 51196, 663, 311, 2935, 264, 2281, 300, 452, 2316, 2709, 281, 264, 3006, 1867, 13, 407, 1936, 11, 51476, 51476, 286, 478, 516, 281, 6964, 452, 2281, 2445, 382, 257, 2445, 295, 264, 2281, 2445, 293, 257, 2445, 295, 288, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.5312604450044178, "compression_ratio": 2.029585798816568, "no_speech_prob": 8.529658771294635e-06}, {"id": 770, "seek": 473118, "start": 4747.820000000001, "end": 4753.42, "text": " That's simply the energy that my model gives to the correct answer. So basically,", "tokens": [50364, 452, 2295, 382, 665, 382, 8851, 281, 3353, 13444, 13, 400, 437, 286, 478, 445, 516, 281, 360, 307, 50848, 50916, 6964, 452, 2281, 11745, 382, 257, 2445, 295, 264, 2281, 2445, 293, 257, 2445, 295, 288, 13, 51196, 51196, 663, 311, 2935, 264, 2281, 300, 452, 2316, 2709, 281, 264, 3006, 1867, 13, 407, 1936, 11, 51476, 51476, 286, 478, 516, 281, 6964, 452, 2281, 2445, 382, 257, 2445, 295, 264, 2281, 2445, 293, 257, 2445, 295, 288, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.5312604450044178, "compression_ratio": 2.029585798816568, "no_speech_prob": 8.529658771294635e-06}, {"id": 771, "seek": 473118, "start": 4753.42, "end": 4758.38, "text": " I'm going to define my energy function as a function of the energy function and a function of y.", "tokens": [50364, 452, 2295, 382, 665, 382, 8851, 281, 3353, 13444, 13, 400, 437, 286, 478, 445, 516, 281, 360, 307, 50848, 50916, 6964, 452, 2281, 11745, 382, 257, 2445, 295, 264, 2281, 2445, 293, 257, 2445, 295, 288, 13, 51196, 51196, 663, 311, 2935, 264, 2281, 300, 452, 2316, 2709, 281, 264, 3006, 1867, 13, 407, 1936, 11, 51476, 51476, 286, 478, 516, 281, 6964, 452, 2281, 2445, 382, 257, 2445, 295, 264, 2281, 2445, 293, 257, 2445, 295, 288, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.5312604450044178, "compression_ratio": 2.029585798816568, "no_speech_prob": 8.529658771294635e-06}, {"id": 772, "seek": 475838, "start": 4758.38, "end": 4766.86, "text": " So basically, I give it an x and I give it the correct answer y and I ask the system,", "tokens": [50364, 407, 1936, 11, 286, 976, 309, 364, 2031, 293, 286, 976, 309, 264, 3006, 1867, 288, 293, 286, 1029, 264, 1185, 11, 50788, 50788, 437, 2281, 360, 291, 976, 281, 300, 6119, 30, 400, 550, 286, 853, 281, 652, 300, 2281, 382, 1359, 382, 1944, 13, 50992, 51052, 407, 291, 362, 341, 9661, 295, 25737, 510, 13, 823, 286, 4712, 291, 341, 4137, 294, 264, 4319, 295, 51376, 51376, 2693, 12879, 24420, 2698, 12, 48172, 24420, 2539, 13, 1692, 286, 478, 4099, 281, 291, 294, 264, 4319, 295, 46533, 51532, 51532, 2539, 13, 407, 3811, 300, 472, 295, 264, 9102, 307, 2031, 293, 264, 661, 7006, 307, 288, 13, 400, 264, 3344, 20369, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10783483228112897, "compression_ratio": 1.8300395256916997, "no_speech_prob": 1.1478265150799416e-05}, {"id": 773, "seek": 475838, "start": 4766.86, "end": 4770.9400000000005, "text": " what energy do you give to that pair? And then I try to make that energy as small as possible.", "tokens": [50364, 407, 1936, 11, 286, 976, 309, 364, 2031, 293, 286, 976, 309, 264, 3006, 1867, 288, 293, 286, 1029, 264, 1185, 11, 50788, 50788, 437, 2281, 360, 291, 976, 281, 300, 6119, 30, 400, 550, 286, 853, 281, 652, 300, 2281, 382, 1359, 382, 1944, 13, 50992, 51052, 407, 291, 362, 341, 9661, 295, 25737, 510, 13, 823, 286, 4712, 291, 341, 4137, 294, 264, 4319, 295, 51376, 51376, 2693, 12879, 24420, 2698, 12, 48172, 24420, 2539, 13, 1692, 286, 478, 4099, 281, 291, 294, 264, 4319, 295, 46533, 51532, 51532, 2539, 13, 407, 3811, 300, 472, 295, 264, 9102, 307, 2031, 293, 264, 661, 7006, 307, 288, 13, 400, 264, 3344, 20369, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10783483228112897, "compression_ratio": 1.8300395256916997, "no_speech_prob": 1.1478265150799416e-05}, {"id": 774, "seek": 475838, "start": 4772.14, "end": 4778.62, "text": " So you have this landscape of energies here. Now I showed you this slide in the context of", "tokens": [50364, 407, 1936, 11, 286, 976, 309, 364, 2031, 293, 286, 976, 309, 264, 3006, 1867, 288, 293, 286, 1029, 264, 1185, 11, 50788, 50788, 437, 2281, 360, 291, 976, 281, 300, 6119, 30, 400, 550, 286, 853, 281, 652, 300, 2281, 382, 1359, 382, 1944, 13, 50992, 51052, 407, 291, 362, 341, 9661, 295, 25737, 510, 13, 823, 286, 4712, 291, 341, 4137, 294, 264, 4319, 295, 51376, 51376, 2693, 12879, 24420, 2698, 12, 48172, 24420, 2539, 13, 1692, 286, 478, 4099, 281, 291, 294, 264, 4319, 295, 46533, 51532, 51532, 2539, 13, 407, 3811, 300, 472, 295, 264, 9102, 307, 2031, 293, 264, 661, 7006, 307, 288, 13, 400, 264, 3344, 20369, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10783483228112897, "compression_ratio": 1.8300395256916997, "no_speech_prob": 1.1478265150799416e-05}, {"id": 775, "seek": 475838, "start": 4778.62, "end": 4781.74, "text": " unsupervised self-supervised learning. Here I'm showing to you in the context of supervised", "tokens": [50364, 407, 1936, 11, 286, 976, 309, 364, 2031, 293, 286, 976, 309, 264, 3006, 1867, 288, 293, 286, 1029, 264, 1185, 11, 50788, 50788, 437, 2281, 360, 291, 976, 281, 300, 6119, 30, 400, 550, 286, 853, 281, 652, 300, 2281, 382, 1359, 382, 1944, 13, 50992, 51052, 407, 291, 362, 341, 9661, 295, 25737, 510, 13, 823, 286, 4712, 291, 341, 4137, 294, 264, 4319, 295, 51376, 51376, 2693, 12879, 24420, 2698, 12, 48172, 24420, 2539, 13, 1692, 286, 478, 4099, 281, 291, 294, 264, 4319, 295, 46533, 51532, 51532, 2539, 13, 407, 3811, 300, 472, 295, 264, 9102, 307, 2031, 293, 264, 661, 7006, 307, 288, 13, 400, 264, 3344, 20369, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10783483228112897, "compression_ratio": 1.8300395256916997, "no_speech_prob": 1.1478265150799416e-05}, {"id": 776, "seek": 475838, "start": 4781.74, "end": 4788.14, "text": " learning. So imagine that one of the variables is x and the other variable is y. And the blue beads", "tokens": [50364, 407, 1936, 11, 286, 976, 309, 364, 2031, 293, 286, 976, 309, 264, 3006, 1867, 288, 293, 286, 1029, 264, 1185, 11, 50788, 50788, 437, 2281, 360, 291, 976, 281, 300, 6119, 30, 400, 550, 286, 853, 281, 652, 300, 2281, 382, 1359, 382, 1944, 13, 50992, 51052, 407, 291, 362, 341, 9661, 295, 25737, 510, 13, 823, 286, 4712, 291, 341, 4137, 294, 264, 4319, 295, 51376, 51376, 2693, 12879, 24420, 2698, 12, 48172, 24420, 2539, 13, 1692, 286, 478, 4099, 281, 291, 294, 264, 4319, 295, 46533, 51532, 51532, 2539, 13, 407, 3811, 300, 472, 295, 264, 9102, 307, 2031, 293, 264, 661, 7006, 307, 288, 13, 400, 264, 3344, 20369, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10783483228112897, "compression_ratio": 1.8300395256916997, "no_speech_prob": 1.1478265150799416e-05}, {"id": 777, "seek": 478814, "start": 4788.14, "end": 4791.42, "text": " are training samples and you want to make the energy of the blue beads as small as possible.", "tokens": [50364, 366, 3097, 10938, 293, 291, 528, 281, 652, 264, 2281, 295, 264, 3344, 20369, 382, 1359, 382, 1944, 13, 50528, 50688, 407, 291, 434, 8407, 760, 322, 264, 3344, 20369, 11, 457, 291, 434, 406, 884, 1340, 1646, 13, 400, 370, 382, 257, 1874, 11, 50892, 50892, 5413, 322, 264, 9482, 295, 428, 3209, 11, 498, 428, 3209, 307, 406, 4761, 6108, 11, 420, 498, 309, 311, 51100, 51100, 4761, 294, 572, 1729, 636, 11, 309, 727, 588, 731, 312, 300, 264, 2281, 2445, 307, 516, 281, 1813, 51416, 51416, 4962, 5315, 13, 509, 434, 445, 1382, 281, 652, 264, 2281, 295, 264, 3006, 1867, 1359, 293, 291, 434, 406, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.08218553610015333, "compression_ratio": 1.7962962962962963, "no_speech_prob": 1.9827984942821786e-05}, {"id": 778, "seek": 478814, "start": 4794.62, "end": 4798.700000000001, "text": " So you're pulling down on the blue beads, but you're not doing anything else. And so as a result,", "tokens": [50364, 366, 3097, 10938, 293, 291, 528, 281, 652, 264, 2281, 295, 264, 3344, 20369, 382, 1359, 382, 1944, 13, 50528, 50688, 407, 291, 434, 8407, 760, 322, 264, 3344, 20369, 11, 457, 291, 434, 406, 884, 1340, 1646, 13, 400, 370, 382, 257, 1874, 11, 50892, 50892, 5413, 322, 264, 9482, 295, 428, 3209, 11, 498, 428, 3209, 307, 406, 4761, 6108, 11, 420, 498, 309, 311, 51100, 51100, 4761, 294, 572, 1729, 636, 11, 309, 727, 588, 731, 312, 300, 264, 2281, 2445, 307, 516, 281, 1813, 51416, 51416, 4962, 5315, 13, 509, 434, 445, 1382, 281, 652, 264, 2281, 295, 264, 3006, 1867, 1359, 293, 291, 434, 406, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.08218553610015333, "compression_ratio": 1.7962962962962963, "no_speech_prob": 1.9827984942821786e-05}, {"id": 779, "seek": 478814, "start": 4798.700000000001, "end": 4802.860000000001, "text": " depending on the architecture of your network, if your network is not designed properly, or if it's", "tokens": [50364, 366, 3097, 10938, 293, 291, 528, 281, 652, 264, 2281, 295, 264, 3344, 20369, 382, 1359, 382, 1944, 13, 50528, 50688, 407, 291, 434, 8407, 760, 322, 264, 3344, 20369, 11, 457, 291, 434, 406, 884, 1340, 1646, 13, 400, 370, 382, 257, 1874, 11, 50892, 50892, 5413, 322, 264, 9482, 295, 428, 3209, 11, 498, 428, 3209, 307, 406, 4761, 6108, 11, 420, 498, 309, 311, 51100, 51100, 4761, 294, 572, 1729, 636, 11, 309, 727, 588, 731, 312, 300, 264, 2281, 2445, 307, 516, 281, 1813, 51416, 51416, 4962, 5315, 13, 509, 434, 445, 1382, 281, 652, 264, 2281, 295, 264, 3006, 1867, 1359, 293, 291, 434, 406, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.08218553610015333, "compression_ratio": 1.7962962962962963, "no_speech_prob": 1.9827984942821786e-05}, {"id": 780, "seek": 478814, "start": 4802.860000000001, "end": 4809.18, "text": " designed in no particular way, it could very well be that the energy function is going to become", "tokens": [50364, 366, 3097, 10938, 293, 291, 528, 281, 652, 264, 2281, 295, 264, 3344, 20369, 382, 1359, 382, 1944, 13, 50528, 50688, 407, 291, 434, 8407, 760, 322, 264, 3344, 20369, 11, 457, 291, 434, 406, 884, 1340, 1646, 13, 400, 370, 382, 257, 1874, 11, 50892, 50892, 5413, 322, 264, 9482, 295, 428, 3209, 11, 498, 428, 3209, 307, 406, 4761, 6108, 11, 420, 498, 309, 311, 51100, 51100, 4761, 294, 572, 1729, 636, 11, 309, 727, 588, 731, 312, 300, 264, 2281, 2445, 307, 516, 281, 1813, 51416, 51416, 4962, 5315, 13, 509, 434, 445, 1382, 281, 652, 264, 2281, 295, 264, 3006, 1867, 1359, 293, 291, 434, 406, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.08218553610015333, "compression_ratio": 1.7962962962962963, "no_speech_prob": 1.9827984942821786e-05}, {"id": 781, "seek": 478814, "start": 4809.18, "end": 4813.58, "text": " flat everywhere. You're just trying to make the energy of the correct answer small and you're not", "tokens": [50364, 366, 3097, 10938, 293, 291, 528, 281, 652, 264, 2281, 295, 264, 3344, 20369, 382, 1359, 382, 1944, 13, 50528, 50688, 407, 291, 434, 8407, 760, 322, 264, 3344, 20369, 11, 457, 291, 434, 406, 884, 1340, 1646, 13, 400, 370, 382, 257, 1874, 11, 50892, 50892, 5413, 322, 264, 9482, 295, 428, 3209, 11, 498, 428, 3209, 307, 406, 4761, 6108, 11, 420, 498, 309, 311, 51100, 51100, 4761, 294, 572, 1729, 636, 11, 309, 727, 588, 731, 312, 300, 264, 2281, 2445, 307, 516, 281, 1813, 51416, 51416, 4962, 5315, 13, 509, 434, 445, 1382, 281, 652, 264, 2281, 295, 264, 3006, 1867, 1359, 293, 291, 434, 406, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.08218553610015333, "compression_ratio": 1.7962962962962963, "no_speech_prob": 1.9827984942821786e-05}, {"id": 782, "seek": 481358, "start": 4813.58, "end": 4818.62, "text": " telling the system the energy of everything else should be higher. And so the system might just", "tokens": [50364, 3585, 264, 1185, 264, 2281, 295, 1203, 1646, 820, 312, 2946, 13, 400, 370, 264, 1185, 1062, 445, 50616, 50648, 15584, 13, 407, 2281, 4470, 307, 406, 665, 294, 300, 2020, 11, 457, 456, 366, 1629, 6851, 689, 51044, 51044, 309, 311, 21142, 570, 498, 264, 3909, 295, 264, 2281, 2445, 307, 1270, 300, 309, 393, 787, 652, 264, 2281, 51504, 51504, 295, 257, 2167, 1867, 1359, 11, 439, 264, 661, 2306, 885, 4833, 11, 436, 500, 380, 643, 281, 362, 257, 8712, 488, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.08422013304450295, "compression_ratio": 1.6551724137931034, "no_speech_prob": 4.936884124617791e-06}, {"id": 783, "seek": 481358, "start": 4819.26, "end": 4827.18, "text": " collapse. So energy loss is not good in that sense, but there are certain situations where", "tokens": [50364, 3585, 264, 1185, 264, 2281, 295, 1203, 1646, 820, 312, 2946, 13, 400, 370, 264, 1185, 1062, 445, 50616, 50648, 15584, 13, 407, 2281, 4470, 307, 406, 665, 294, 300, 2020, 11, 457, 456, 366, 1629, 6851, 689, 51044, 51044, 309, 311, 21142, 570, 498, 264, 3909, 295, 264, 2281, 2445, 307, 1270, 300, 309, 393, 787, 652, 264, 2281, 51504, 51504, 295, 257, 2167, 1867, 1359, 11, 439, 264, 661, 2306, 885, 4833, 11, 436, 500, 380, 643, 281, 362, 257, 8712, 488, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.08422013304450295, "compression_ratio": 1.6551724137931034, "no_speech_prob": 4.936884124617791e-06}, {"id": 784, "seek": 481358, "start": 4827.18, "end": 4836.38, "text": " it's applicable because if the shape of the energy function is such that it can only make the energy", "tokens": [50364, 3585, 264, 1185, 264, 2281, 295, 1203, 1646, 820, 312, 2946, 13, 400, 370, 264, 1185, 1062, 445, 50616, 50648, 15584, 13, 407, 2281, 4470, 307, 406, 665, 294, 300, 2020, 11, 457, 456, 366, 1629, 6851, 689, 51044, 51044, 309, 311, 21142, 570, 498, 264, 3909, 295, 264, 2281, 2445, 307, 1270, 300, 309, 393, 787, 652, 264, 2281, 51504, 51504, 295, 257, 2167, 1867, 1359, 11, 439, 264, 661, 2306, 885, 4833, 11, 436, 500, 380, 643, 281, 362, 257, 8712, 488, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.08422013304450295, "compression_ratio": 1.6551724137931034, "no_speech_prob": 4.936884124617791e-06}, {"id": 785, "seek": 481358, "start": 4836.38, "end": 4843.18, "text": " of a single answer small, all the other ones being larger, they don't need to have a contrastive", "tokens": [50364, 3585, 264, 1185, 264, 2281, 295, 1203, 1646, 820, 312, 2946, 13, 400, 370, 264, 1185, 1062, 445, 50616, 50648, 15584, 13, 407, 2281, 4470, 307, 406, 665, 294, 300, 2020, 11, 457, 456, 366, 1629, 6851, 689, 51044, 51044, 309, 311, 21142, 570, 498, 264, 3909, 295, 264, 2281, 2445, 307, 1270, 300, 309, 393, 787, 652, 264, 2281, 51504, 51504, 295, 257, 2167, 1867, 1359, 11, 439, 264, 661, 2306, 885, 4833, 11, 436, 500, 380, 643, 281, 362, 257, 8712, 488, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.08422013304450295, "compression_ratio": 1.6551724137931034, "no_speech_prob": 4.936884124617791e-06}, {"id": 786, "seek": 484318, "start": 4843.18, "end": 4848.38, "text": " term. And we've seen this in the context of self-supervised learning. People are completely", "tokens": [50364, 1433, 13, 400, 321, 600, 1612, 341, 294, 264, 4319, 295, 2698, 12, 48172, 24420, 2539, 13, 3432, 366, 2584, 50624, 50624, 2731, 466, 264, 4470, 2445, 13, 1779, 13, 1033, 13, 407, 456, 311, 257, 2445, 441, 293, 309, 311, 257, 2445, 295, 51100, 51100, 1071, 2445, 462, 13, 407, 309, 311, 1219, 257, 11745, 570, 309, 311, 257, 2445, 295, 257, 2445, 13, 51416, 51496, 467, 311, 406, 257, 2445, 295, 257, 935, 11, 309, 311, 257, 2445, 295, 257, 2445, 13, 823, 11, 498, 300, 1150, 2445, 51676, 51784], "temperature": 0.0, "avg_logprob": -0.12859672181149748, "compression_ratio": 1.8461538461538463, "no_speech_prob": 6.853557715658098e-05}, {"id": 787, "seek": 484318, "start": 4848.38, "end": 4857.900000000001, "text": " lost about the loss function. Right. Okay. So there's a function L and it's a function of", "tokens": [50364, 1433, 13, 400, 321, 600, 1612, 341, 294, 264, 4319, 295, 2698, 12, 48172, 24420, 2539, 13, 3432, 366, 2584, 50624, 50624, 2731, 466, 264, 4470, 2445, 13, 1779, 13, 1033, 13, 407, 456, 311, 257, 2445, 441, 293, 309, 311, 257, 2445, 295, 51100, 51100, 1071, 2445, 462, 13, 407, 309, 311, 1219, 257, 11745, 570, 309, 311, 257, 2445, 295, 257, 2445, 13, 51416, 51496, 467, 311, 406, 257, 2445, 295, 257, 935, 11, 309, 311, 257, 2445, 295, 257, 2445, 13, 823, 11, 498, 300, 1150, 2445, 51676, 51784], "temperature": 0.0, "avg_logprob": -0.12859672181149748, "compression_ratio": 1.8461538461538463, "no_speech_prob": 6.853557715658098e-05}, {"id": 788, "seek": 484318, "start": 4857.900000000001, "end": 4864.22, "text": " another function E. So it's called a functional because it's a function of a function.", "tokens": [50364, 1433, 13, 400, 321, 600, 1612, 341, 294, 264, 4319, 295, 2698, 12, 48172, 24420, 2539, 13, 3432, 366, 2584, 50624, 50624, 2731, 466, 264, 4470, 2445, 13, 1779, 13, 1033, 13, 407, 456, 311, 257, 2445, 441, 293, 309, 311, 257, 2445, 295, 51100, 51100, 1071, 2445, 462, 13, 407, 309, 311, 1219, 257, 11745, 570, 309, 311, 257, 2445, 295, 257, 2445, 13, 51416, 51496, 467, 311, 406, 257, 2445, 295, 257, 935, 11, 309, 311, 257, 2445, 295, 257, 2445, 13, 823, 11, 498, 300, 1150, 2445, 51676, 51784], "temperature": 0.0, "avg_logprob": -0.12859672181149748, "compression_ratio": 1.8461538461538463, "no_speech_prob": 6.853557715658098e-05}, {"id": 789, "seek": 484318, "start": 4865.820000000001, "end": 4869.42, "text": " It's not a function of a point, it's a function of a function. Now, if that second function", "tokens": [50364, 1433, 13, 400, 321, 600, 1612, 341, 294, 264, 4319, 295, 2698, 12, 48172, 24420, 2539, 13, 3432, 366, 2584, 50624, 50624, 2731, 466, 264, 4470, 2445, 13, 1779, 13, 1033, 13, 407, 456, 311, 257, 2445, 441, 293, 309, 311, 257, 2445, 295, 51100, 51100, 1071, 2445, 462, 13, 407, 309, 311, 1219, 257, 11745, 570, 309, 311, 257, 2445, 295, 257, 2445, 13, 51416, 51496, 467, 311, 406, 257, 2445, 295, 257, 935, 11, 309, 311, 257, 2445, 295, 257, 2445, 13, 823, 11, 498, 300, 1150, 2445, 51676, 51784], "temperature": 0.0, "avg_logprob": -0.12859672181149748, "compression_ratio": 1.8461538461538463, "no_speech_prob": 6.853557715658098e-05}, {"id": 790, "seek": 486942, "start": 4869.42, "end": 4875.66, "text": " is parameterized by parameter W, then you can say that the loss function is actually a function of", "tokens": [50364, 307, 13075, 1602, 538, 13075, 343, 11, 550, 291, 393, 584, 300, 264, 4470, 2445, 307, 767, 257, 2445, 295, 50676, 50676, 300, 13075, 343, 293, 586, 309, 3643, 257, 3890, 2445, 13, 1033, 13, 663, 311, 437, 286, 632, 294, 264, 485, 50900, 50900, 1664, 291, 2464, 309, 760, 30, 467, 311, 1936, 3720, 510, 13, 1033, 13, 509, 393, 2139, 2464, 264, 11745, 382, 51228, 51328, 441, 295, 462, 293, 318, 13, 407, 300, 311, 257, 11745, 570, 309, 311, 257, 2445, 295, 462, 11, 597, 2564, 307, 257, 2445, 13, 51608, 51700], "temperature": 0.0, "avg_logprob": -0.26849424224538904, "compression_ratio": 1.7431192660550459, "no_speech_prob": 1.2606004929693881e-05}, {"id": 791, "seek": 486942, "start": 4875.66, "end": 4880.14, "text": " that parameter W and now it becomes a regular function. Okay. That's what I had in the...", "tokens": [50364, 307, 13075, 1602, 538, 13075, 343, 11, 550, 291, 393, 584, 300, 264, 4470, 2445, 307, 767, 257, 2445, 295, 50676, 50676, 300, 13075, 343, 293, 586, 309, 3643, 257, 3890, 2445, 13, 1033, 13, 663, 311, 437, 286, 632, 294, 264, 485, 50900, 50900, 1664, 291, 2464, 309, 760, 30, 467, 311, 1936, 3720, 510, 13, 1033, 13, 509, 393, 2139, 2464, 264, 11745, 382, 51228, 51328, 441, 295, 462, 293, 318, 13, 407, 300, 311, 257, 11745, 570, 309, 311, 257, 2445, 295, 462, 11, 597, 2564, 307, 257, 2445, 13, 51608, 51700], "temperature": 0.0, "avg_logprob": -0.26849424224538904, "compression_ratio": 1.7431192660550459, "no_speech_prob": 1.2606004929693881e-05}, {"id": 792, "seek": 486942, "start": 4880.14, "end": 4886.7, "text": " Can you write it down? It's basically written here. Okay. You can either write the functional as", "tokens": [50364, 307, 13075, 1602, 538, 13075, 343, 11, 550, 291, 393, 584, 300, 264, 4470, 2445, 307, 767, 257, 2445, 295, 50676, 50676, 300, 13075, 343, 293, 586, 309, 3643, 257, 3890, 2445, 13, 1033, 13, 663, 311, 437, 286, 632, 294, 264, 485, 50900, 50900, 1664, 291, 2464, 309, 760, 30, 467, 311, 1936, 3720, 510, 13, 1033, 13, 509, 393, 2139, 2464, 264, 11745, 382, 51228, 51328, 441, 295, 462, 293, 318, 13, 407, 300, 311, 257, 11745, 570, 309, 311, 257, 2445, 295, 462, 11, 597, 2564, 307, 257, 2445, 13, 51608, 51700], "temperature": 0.0, "avg_logprob": -0.26849424224538904, "compression_ratio": 1.7431192660550459, "no_speech_prob": 1.2606004929693881e-05}, {"id": 793, "seek": 486942, "start": 4888.7, "end": 4894.3, "text": " L of E and S. So that's a functional because it's a function of E, which itself is a function.", "tokens": [50364, 307, 13075, 1602, 538, 13075, 343, 11, 550, 291, 393, 584, 300, 264, 4470, 2445, 307, 767, 257, 2445, 295, 50676, 50676, 300, 13075, 343, 293, 586, 309, 3643, 257, 3890, 2445, 13, 1033, 13, 663, 311, 437, 286, 632, 294, 264, 485, 50900, 50900, 1664, 291, 2464, 309, 760, 30, 467, 311, 1936, 3720, 510, 13, 1033, 13, 509, 393, 2139, 2464, 264, 11745, 382, 51228, 51328, 441, 295, 462, 293, 318, 13, 407, 300, 311, 257, 11745, 570, 309, 311, 257, 2445, 295, 462, 11, 597, 2564, 307, 257, 2445, 13, 51608, 51700], "temperature": 0.0, "avg_logprob": -0.26849424224538904, "compression_ratio": 1.7431192660550459, "no_speech_prob": 1.2606004929693881e-05}, {"id": 794, "seek": 489430, "start": 4894.3, "end": 4902.54, "text": " Okay. But E itself is a function of W. And so if I write the loss function directly as a function", "tokens": [50364, 1033, 13, 583, 462, 2564, 307, 257, 2445, 295, 343, 13, 400, 370, 498, 286, 2464, 264, 4470, 2445, 3838, 382, 257, 2445, 50776, 50776, 295, 343, 11, 586, 309, 311, 445, 257, 3890, 2445, 13, 1033, 13, 865, 13, 286, 914, 11, 286, 2351, 264, 1168, 300, 390, 2351, 294, 264, 5081, 13, 51240, 51240, 865, 13, 286, 458, 13, 286, 458, 13, 51340, 51440], "temperature": 0.0, "avg_logprob": -0.3787983726052677, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.7775551896193065e-05}, {"id": 795, "seek": 489430, "start": 4902.54, "end": 4911.820000000001, "text": " of W, now it's just a regular function. Okay. Yeah. I mean, I asked the question that was asked in the chat.", "tokens": [50364, 1033, 13, 583, 462, 2564, 307, 257, 2445, 295, 343, 13, 400, 370, 498, 286, 2464, 264, 4470, 2445, 3838, 382, 257, 2445, 50776, 50776, 295, 343, 11, 586, 309, 311, 445, 257, 3890, 2445, 13, 1033, 13, 865, 13, 286, 914, 11, 286, 2351, 264, 1168, 300, 390, 2351, 294, 264, 5081, 13, 51240, 51240, 865, 13, 286, 458, 13, 286, 458, 13, 51340, 51440], "temperature": 0.0, "avg_logprob": -0.3787983726052677, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.7775551896193065e-05}, {"id": 796, "seek": 489430, "start": 4911.820000000001, "end": 4913.820000000001, "text": " Yeah. I know. I know.", "tokens": [50364, 1033, 13, 583, 462, 2564, 307, 257, 2445, 295, 343, 13, 400, 370, 498, 286, 2464, 264, 4470, 2445, 3838, 382, 257, 2445, 50776, 50776, 295, 343, 11, 586, 309, 311, 445, 257, 3890, 2445, 13, 1033, 13, 865, 13, 286, 914, 11, 286, 2351, 264, 1168, 300, 390, 2351, 294, 264, 5081, 13, 51240, 51240, 865, 13, 286, 458, 13, 286, 458, 13, 51340, 51440], "temperature": 0.0, "avg_logprob": -0.3787983726052677, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.7775551896193065e-05}, {"id": 797, "seek": 491382, "start": 4913.82, "end": 4933.42, "text": " Okay. We've seen the negative log. Like a good loss before. I talked about this. So this is a loss", "tokens": [50364, 1033, 13, 492, 600, 1612, 264, 3671, 3565, 13, 1743, 257, 665, 4470, 949, 13, 286, 2825, 466, 341, 13, 407, 341, 307, 257, 4470, 51344, 51344, 2445, 13, 407, 309, 311, 257, 2445, 295, 343, 13, 400, 309, 311, 257, 2445, 295, 343, 13, 400, 309, 311, 257, 2445, 295, 343, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.6817560028611568, "compression_ratio": 1.608695652173913, "no_speech_prob": 8.472747867926955e-05}, {"id": 798, "seek": 491382, "start": 4933.42, "end": 4940.0599999999995, "text": " function. So it's a function of W. And it's a function of W. And it's a function of W.", "tokens": [50364, 1033, 13, 492, 600, 1612, 264, 3671, 3565, 13, 1743, 257, 665, 4470, 949, 13, 286, 2825, 466, 341, 13, 407, 341, 307, 257, 4470, 51344, 51344, 2445, 13, 407, 309, 311, 257, 2445, 295, 343, 13, 400, 309, 311, 257, 2445, 295, 343, 13, 400, 309, 311, 257, 2445, 295, 343, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.6817560028611568, "compression_ratio": 1.608695652173913, "no_speech_prob": 8.472747867926955e-05}, {"id": 799, "seek": 494006, "start": 4940.06, "end": 4944.860000000001, "text": " This is basically the Doblin yes in addition to the please give the question over here.", "tokens": [50364, 639, 307, 1936, 264, 29679, 5045, 2086, 294, 4500, 281, 264, 1767, 976, 264, 1168, 670, 510, 13, 50604, 50604, 407, 341, 307, 257, 2445, 295, 343, 11, 558, 30, 2720, 343, 13, 400, 550, 456, 291, 362, 257, 11842, 34422, 326, 12, 8613, 51132, 51132, 2158, 13, 821, 311, 51596, 51696], "temperature": 1.0, "avg_logprob": -3.607229162145544, "compression_ratio": 1.2653061224489797, "no_speech_prob": 2.6687561330618337e-05}, {"id": 800, "seek": 494006, "start": 4944.860000000001, "end": 4955.42, "text": " So this is a function of W, right? Of W. And then there you have a wider Dirac- \u043f\u043e\u043c", "tokens": [50364, 639, 307, 1936, 264, 29679, 5045, 2086, 294, 4500, 281, 264, 1767, 976, 264, 1168, 670, 510, 13, 50604, 50604, 407, 341, 307, 257, 2445, 295, 343, 11, 558, 30, 2720, 343, 13, 400, 550, 456, 291, 362, 257, 11842, 34422, 326, 12, 8613, 51132, 51132, 2158, 13, 821, 311, 51596, 51696], "temperature": 1.0, "avg_logprob": -3.607229162145544, "compression_ratio": 1.2653061224489797, "no_speech_prob": 2.6687561330618337e-05}, {"id": 801, "seek": 494006, "start": 4955.42, "end": 4964.700000000001, "text": " value. There's", "tokens": [50364, 639, 307, 1936, 264, 29679, 5045, 2086, 294, 4500, 281, 264, 1767, 976, 264, 1168, 670, 510, 13, 50604, 50604, 407, 341, 307, 257, 2445, 295, 343, 11, 558, 30, 2720, 343, 13, 400, 550, 456, 291, 362, 257, 11842, 34422, 326, 12, 8613, 51132, 51132, 2158, 13, 821, 311, 51596, 51696], "temperature": 1.0, "avg_logprob": -3.607229162145544, "compression_ratio": 1.2653061224489797, "no_speech_prob": 2.6687561330618337e-05}, {"id": 802, "seek": 496470, "start": 4964.7, "end": 4969.099999999999, "text": " And this one is trying to make the energy of all y's", "tokens": [50364, 400, 341, 472, 307, 1382, 281, 652, 264, 2281, 295, 439, 288, 311, 50584, 50584, 337, 341, 2212, 2031, 382, 2416, 382, 1944, 13, 50736, 50736, 1436, 264, 1151, 636, 281, 652, 341, 1433, 1359, 50862, 50862, 307, 281, 652, 729, 25737, 2416, 570, 436, 3242, 294, 456, 51034, 51034, 382, 257, 3671, 21510, 13, 51166, 51308, 407, 341, 575, 341, 733, 295, 7380, 760, 322, 264, 3006, 1867, 11, 51500, 51500, 7380, 493, 322, 18424, 1867, 5223, 13, 51614, 51750], "temperature": 0.0, "avg_logprob": -0.21088271543204065, "compression_ratio": 1.7015706806282722, "no_speech_prob": 0.00045822118408977985}, {"id": 803, "seek": 496470, "start": 4969.099999999999, "end": 4972.139999999999, "text": " for this given x as large as possible.", "tokens": [50364, 400, 341, 472, 307, 1382, 281, 652, 264, 2281, 295, 439, 288, 311, 50584, 50584, 337, 341, 2212, 2031, 382, 2416, 382, 1944, 13, 50736, 50736, 1436, 264, 1151, 636, 281, 652, 341, 1433, 1359, 50862, 50862, 307, 281, 652, 729, 25737, 2416, 570, 436, 3242, 294, 456, 51034, 51034, 382, 257, 3671, 21510, 13, 51166, 51308, 407, 341, 575, 341, 733, 295, 7380, 760, 322, 264, 3006, 1867, 11, 51500, 51500, 7380, 493, 322, 18424, 1867, 5223, 13, 51614, 51750], "temperature": 0.0, "avg_logprob": -0.21088271543204065, "compression_ratio": 1.7015706806282722, "no_speech_prob": 0.00045822118408977985}, {"id": 804, "seek": 496470, "start": 4972.139999999999, "end": 4974.66, "text": " Because the best way to make this term small", "tokens": [50364, 400, 341, 472, 307, 1382, 281, 652, 264, 2281, 295, 439, 288, 311, 50584, 50584, 337, 341, 2212, 2031, 382, 2416, 382, 1944, 13, 50736, 50736, 1436, 264, 1151, 636, 281, 652, 341, 1433, 1359, 50862, 50862, 307, 281, 652, 729, 25737, 2416, 570, 436, 3242, 294, 456, 51034, 51034, 382, 257, 3671, 21510, 13, 51166, 51308, 407, 341, 575, 341, 733, 295, 7380, 760, 322, 264, 3006, 1867, 11, 51500, 51500, 7380, 493, 322, 18424, 1867, 5223, 13, 51614, 51750], "temperature": 0.0, "avg_logprob": -0.21088271543204065, "compression_ratio": 1.7015706806282722, "no_speech_prob": 0.00045822118408977985}, {"id": 805, "seek": 496470, "start": 4974.66, "end": 4978.099999999999, "text": " is to make those energies large because they enter in there", "tokens": [50364, 400, 341, 472, 307, 1382, 281, 652, 264, 2281, 295, 439, 288, 311, 50584, 50584, 337, 341, 2212, 2031, 382, 2416, 382, 1944, 13, 50736, 50736, 1436, 264, 1151, 636, 281, 652, 341, 1433, 1359, 50862, 50862, 307, 281, 652, 729, 25737, 2416, 570, 436, 3242, 294, 456, 51034, 51034, 382, 257, 3671, 21510, 13, 51166, 51308, 407, 341, 575, 341, 733, 295, 7380, 760, 322, 264, 3006, 1867, 11, 51500, 51500, 7380, 493, 322, 18424, 1867, 5223, 13, 51614, 51750], "temperature": 0.0, "avg_logprob": -0.21088271543204065, "compression_ratio": 1.7015706806282722, "no_speech_prob": 0.00045822118408977985}, {"id": 806, "seek": 496470, "start": 4978.099999999999, "end": 4980.74, "text": " as a negative exponential.", "tokens": [50364, 400, 341, 472, 307, 1382, 281, 652, 264, 2281, 295, 439, 288, 311, 50584, 50584, 337, 341, 2212, 2031, 382, 2416, 382, 1944, 13, 50736, 50736, 1436, 264, 1151, 636, 281, 652, 341, 1433, 1359, 50862, 50862, 307, 281, 652, 729, 25737, 2416, 570, 436, 3242, 294, 456, 51034, 51034, 382, 257, 3671, 21510, 13, 51166, 51308, 407, 341, 575, 341, 733, 295, 7380, 760, 322, 264, 3006, 1867, 11, 51500, 51500, 7380, 493, 322, 18424, 1867, 5223, 13, 51614, 51750], "temperature": 0.0, "avg_logprob": -0.21088271543204065, "compression_ratio": 1.7015706806282722, "no_speech_prob": 0.00045822118408977985}, {"id": 807, "seek": 496470, "start": 4983.58, "end": 4987.42, "text": " So this has this kind of pushing down on the correct answer,", "tokens": [50364, 400, 341, 472, 307, 1382, 281, 652, 264, 2281, 295, 439, 288, 311, 50584, 50584, 337, 341, 2212, 2031, 382, 2416, 382, 1944, 13, 50736, 50736, 1436, 264, 1151, 636, 281, 652, 341, 1433, 1359, 50862, 50862, 307, 281, 652, 729, 25737, 2416, 570, 436, 3242, 294, 456, 51034, 51034, 382, 257, 3671, 21510, 13, 51166, 51308, 407, 341, 575, 341, 733, 295, 7380, 760, 322, 264, 3006, 1867, 11, 51500, 51500, 7380, 493, 322, 18424, 1867, 5223, 13, 51614, 51750], "temperature": 0.0, "avg_logprob": -0.21088271543204065, "compression_ratio": 1.7015706806282722, "no_speech_prob": 0.00045822118408977985}, {"id": 808, "seek": 496470, "start": 4987.42, "end": 4989.7, "text": " pushing up on incorrect answer behavior.", "tokens": [50364, 400, 341, 472, 307, 1382, 281, 652, 264, 2281, 295, 439, 288, 311, 50584, 50584, 337, 341, 2212, 2031, 382, 2416, 382, 1944, 13, 50736, 50736, 1436, 264, 1151, 636, 281, 652, 341, 1433, 1359, 50862, 50862, 307, 281, 652, 729, 25737, 2416, 570, 436, 3242, 294, 456, 51034, 51034, 382, 257, 3671, 21510, 13, 51166, 51308, 407, 341, 575, 341, 733, 295, 7380, 760, 322, 264, 3006, 1867, 11, 51500, 51500, 7380, 493, 322, 18424, 1867, 5223, 13, 51614, 51750], "temperature": 0.0, "avg_logprob": -0.21088271543204065, "compression_ratio": 1.7015706806282722, "no_speech_prob": 0.00045822118408977985}, {"id": 809, "seek": 498970, "start": 4989.7, "end": 4994.7, "text": " And we've seen before, we just talked about margin loss", "tokens": [50364, 400, 321, 600, 1612, 949, 11, 321, 445, 2825, 466, 10270, 4470, 50614, 50716, 293, 661, 3467, 295, 15352, 13, 50806, 50876, 1692, 307, 746, 300, 311, 1219, 257, 43276, 2044, 4470, 50962, 50962, 570, 309, 311, 1936, 588, 2531, 281, 11, 51050, 51050, 286, 914, 11, 309, 311, 2293, 264, 912, 382, 264, 4470, 300, 390, 1143, 51238, 51238, 337, 264, 43276, 2044, 4060, 924, 2057, 11, 670, 4060, 924, 2057, 13, 51434, 51434, 407, 341, 472, 1619, 11, 286, 528, 281, 652, 264, 2281, 51592, 51592, 295, 264, 3006, 1867, 1359, 13, 51733], "temperature": 0.0, "avg_logprob": -0.17352338181328528, "compression_ratio": 1.5829596412556053, "no_speech_prob": 3.0239718398661353e-05}, {"id": 810, "seek": 498970, "start": 4996.74, "end": 4998.54, "text": " and other types of losses.", "tokens": [50364, 400, 321, 600, 1612, 949, 11, 321, 445, 2825, 466, 10270, 4470, 50614, 50716, 293, 661, 3467, 295, 15352, 13, 50806, 50876, 1692, 307, 746, 300, 311, 1219, 257, 43276, 2044, 4470, 50962, 50962, 570, 309, 311, 1936, 588, 2531, 281, 11, 51050, 51050, 286, 914, 11, 309, 311, 2293, 264, 912, 382, 264, 4470, 300, 390, 1143, 51238, 51238, 337, 264, 43276, 2044, 4060, 924, 2057, 11, 670, 4060, 924, 2057, 13, 51434, 51434, 407, 341, 472, 1619, 11, 286, 528, 281, 652, 264, 2281, 51592, 51592, 295, 264, 3006, 1867, 1359, 13, 51733], "temperature": 0.0, "avg_logprob": -0.17352338181328528, "compression_ratio": 1.5829596412556053, "no_speech_prob": 3.0239718398661353e-05}, {"id": 811, "seek": 498970, "start": 4999.94, "end": 5001.66, "text": " Here is something that's called a perceptron loss", "tokens": [50364, 400, 321, 600, 1612, 949, 11, 321, 445, 2825, 466, 10270, 4470, 50614, 50716, 293, 661, 3467, 295, 15352, 13, 50806, 50876, 1692, 307, 746, 300, 311, 1219, 257, 43276, 2044, 4470, 50962, 50962, 570, 309, 311, 1936, 588, 2531, 281, 11, 51050, 51050, 286, 914, 11, 309, 311, 2293, 264, 912, 382, 264, 4470, 300, 390, 1143, 51238, 51238, 337, 264, 43276, 2044, 4060, 924, 2057, 11, 670, 4060, 924, 2057, 13, 51434, 51434, 407, 341, 472, 1619, 11, 286, 528, 281, 652, 264, 2281, 51592, 51592, 295, 264, 3006, 1867, 1359, 13, 51733], "temperature": 0.0, "avg_logprob": -0.17352338181328528, "compression_ratio": 1.5829596412556053, "no_speech_prob": 3.0239718398661353e-05}, {"id": 812, "seek": 498970, "start": 5001.66, "end": 5003.42, "text": " because it's basically very similar to,", "tokens": [50364, 400, 321, 600, 1612, 949, 11, 321, 445, 2825, 466, 10270, 4470, 50614, 50716, 293, 661, 3467, 295, 15352, 13, 50806, 50876, 1692, 307, 746, 300, 311, 1219, 257, 43276, 2044, 4470, 50962, 50962, 570, 309, 311, 1936, 588, 2531, 281, 11, 51050, 51050, 286, 914, 11, 309, 311, 2293, 264, 912, 382, 264, 4470, 300, 390, 1143, 51238, 51238, 337, 264, 43276, 2044, 4060, 924, 2057, 11, 670, 4060, 924, 2057, 13, 51434, 51434, 407, 341, 472, 1619, 11, 286, 528, 281, 652, 264, 2281, 51592, 51592, 295, 264, 3006, 1867, 1359, 13, 51733], "temperature": 0.0, "avg_logprob": -0.17352338181328528, "compression_ratio": 1.5829596412556053, "no_speech_prob": 3.0239718398661353e-05}, {"id": 813, "seek": 498970, "start": 5003.42, "end": 5007.179999999999, "text": " I mean, it's exactly the same as the loss that was used", "tokens": [50364, 400, 321, 600, 1612, 949, 11, 321, 445, 2825, 466, 10270, 4470, 50614, 50716, 293, 661, 3467, 295, 15352, 13, 50806, 50876, 1692, 307, 746, 300, 311, 1219, 257, 43276, 2044, 4470, 50962, 50962, 570, 309, 311, 1936, 588, 2531, 281, 11, 51050, 51050, 286, 914, 11, 309, 311, 2293, 264, 912, 382, 264, 4470, 300, 390, 1143, 51238, 51238, 337, 264, 43276, 2044, 4060, 924, 2057, 11, 670, 4060, 924, 2057, 13, 51434, 51434, 407, 341, 472, 1619, 11, 286, 528, 281, 652, 264, 2281, 51592, 51592, 295, 264, 3006, 1867, 1359, 13, 51733], "temperature": 0.0, "avg_logprob": -0.17352338181328528, "compression_ratio": 1.5829596412556053, "no_speech_prob": 3.0239718398661353e-05}, {"id": 814, "seek": 498970, "start": 5007.179999999999, "end": 5011.099999999999, "text": " for the perceptron 60 years ago, over 60 years ago.", "tokens": [50364, 400, 321, 600, 1612, 949, 11, 321, 445, 2825, 466, 10270, 4470, 50614, 50716, 293, 661, 3467, 295, 15352, 13, 50806, 50876, 1692, 307, 746, 300, 311, 1219, 257, 43276, 2044, 4470, 50962, 50962, 570, 309, 311, 1936, 588, 2531, 281, 11, 51050, 51050, 286, 914, 11, 309, 311, 2293, 264, 912, 382, 264, 4470, 300, 390, 1143, 51238, 51238, 337, 264, 43276, 2044, 4060, 924, 2057, 11, 670, 4060, 924, 2057, 13, 51434, 51434, 407, 341, 472, 1619, 11, 286, 528, 281, 652, 264, 2281, 51592, 51592, 295, 264, 3006, 1867, 1359, 13, 51733], "temperature": 0.0, "avg_logprob": -0.17352338181328528, "compression_ratio": 1.5829596412556053, "no_speech_prob": 3.0239718398661353e-05}, {"id": 815, "seek": 498970, "start": 5011.099999999999, "end": 5014.26, "text": " So this one says, I want to make the energy", "tokens": [50364, 400, 321, 600, 1612, 949, 11, 321, 445, 2825, 466, 10270, 4470, 50614, 50716, 293, 661, 3467, 295, 15352, 13, 50806, 50876, 1692, 307, 746, 300, 311, 1219, 257, 43276, 2044, 4470, 50962, 50962, 570, 309, 311, 1936, 588, 2531, 281, 11, 51050, 51050, 286, 914, 11, 309, 311, 2293, 264, 912, 382, 264, 4470, 300, 390, 1143, 51238, 51238, 337, 264, 43276, 2044, 4060, 924, 2057, 11, 670, 4060, 924, 2057, 13, 51434, 51434, 407, 341, 472, 1619, 11, 286, 528, 281, 652, 264, 2281, 51592, 51592, 295, 264, 3006, 1867, 1359, 13, 51733], "temperature": 0.0, "avg_logprob": -0.17352338181328528, "compression_ratio": 1.5829596412556053, "no_speech_prob": 3.0239718398661353e-05}, {"id": 816, "seek": 501426, "start": 5014.26, "end": 5019.26, "text": " of the correct answer small and at the same time,", "tokens": [50364, 295, 264, 3006, 1867, 1359, 293, 412, 264, 912, 565, 11, 50614, 50680, 286, 528, 281, 652, 264, 2281, 295, 264, 16998, 2281, 50910, 50910, 337, 439, 6338, 382, 2416, 382, 1944, 13, 51078, 51078, 407, 1888, 264, 288, 300, 575, 264, 16998, 2281, 294, 428, 1185, 11, 51274, 51274, 652, 300, 382, 2416, 382, 291, 393, 13, 51364, 51364, 1711, 264, 912, 565, 11, 16137, 264, 3006, 2281, 11, 51448, 51448, 652, 300, 382, 1359, 382, 291, 393, 13, 51524, 51524, 823, 456, 307, 257, 935, 412, 597, 264, 1867, 51724, 51724, 365, 264, 3006, 2281, 307, 799, 312, 2681, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.11217602321079799, "compression_ratio": 1.9793814432989691, "no_speech_prob": 3.0240587875596248e-05}, {"id": 817, "seek": 501426, "start": 5020.58, "end": 5025.18, "text": " I want to make the energy of the smallest energy", "tokens": [50364, 295, 264, 3006, 1867, 1359, 293, 412, 264, 912, 565, 11, 50614, 50680, 286, 528, 281, 652, 264, 2281, 295, 264, 16998, 2281, 50910, 50910, 337, 439, 6338, 382, 2416, 382, 1944, 13, 51078, 51078, 407, 1888, 264, 288, 300, 575, 264, 16998, 2281, 294, 428, 1185, 11, 51274, 51274, 652, 300, 382, 2416, 382, 291, 393, 13, 51364, 51364, 1711, 264, 912, 565, 11, 16137, 264, 3006, 2281, 11, 51448, 51448, 652, 300, 382, 1359, 382, 291, 393, 13, 51524, 51524, 823, 456, 307, 257, 935, 412, 597, 264, 1867, 51724, 51724, 365, 264, 3006, 2281, 307, 799, 312, 2681, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.11217602321079799, "compression_ratio": 1.9793814432989691, "no_speech_prob": 3.0240587875596248e-05}, {"id": 818, "seek": 501426, "start": 5025.18, "end": 5028.54, "text": " for all answers as large as possible.", "tokens": [50364, 295, 264, 3006, 1867, 1359, 293, 412, 264, 912, 565, 11, 50614, 50680, 286, 528, 281, 652, 264, 2281, 295, 264, 16998, 2281, 50910, 50910, 337, 439, 6338, 382, 2416, 382, 1944, 13, 51078, 51078, 407, 1888, 264, 288, 300, 575, 264, 16998, 2281, 294, 428, 1185, 11, 51274, 51274, 652, 300, 382, 2416, 382, 291, 393, 13, 51364, 51364, 1711, 264, 912, 565, 11, 16137, 264, 3006, 2281, 11, 51448, 51448, 652, 300, 382, 1359, 382, 291, 393, 13, 51524, 51524, 823, 456, 307, 257, 935, 412, 597, 264, 1867, 51724, 51724, 365, 264, 3006, 2281, 307, 799, 312, 2681, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.11217602321079799, "compression_ratio": 1.9793814432989691, "no_speech_prob": 3.0240587875596248e-05}, {"id": 819, "seek": 501426, "start": 5028.54, "end": 5032.46, "text": " So pick the y that has the smallest energy in your system,", "tokens": [50364, 295, 264, 3006, 1867, 1359, 293, 412, 264, 912, 565, 11, 50614, 50680, 286, 528, 281, 652, 264, 2281, 295, 264, 16998, 2281, 50910, 50910, 337, 439, 6338, 382, 2416, 382, 1944, 13, 51078, 51078, 407, 1888, 264, 288, 300, 575, 264, 16998, 2281, 294, 428, 1185, 11, 51274, 51274, 652, 300, 382, 2416, 382, 291, 393, 13, 51364, 51364, 1711, 264, 912, 565, 11, 16137, 264, 3006, 2281, 11, 51448, 51448, 652, 300, 382, 1359, 382, 291, 393, 13, 51524, 51524, 823, 456, 307, 257, 935, 412, 597, 264, 1867, 51724, 51724, 365, 264, 3006, 2281, 307, 799, 312, 2681, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.11217602321079799, "compression_ratio": 1.9793814432989691, "no_speech_prob": 3.0240587875596248e-05}, {"id": 820, "seek": 501426, "start": 5032.46, "end": 5034.26, "text": " make that as large as you can.", "tokens": [50364, 295, 264, 3006, 1867, 1359, 293, 412, 264, 912, 565, 11, 50614, 50680, 286, 528, 281, 652, 264, 2281, 295, 264, 16998, 2281, 50910, 50910, 337, 439, 6338, 382, 2416, 382, 1944, 13, 51078, 51078, 407, 1888, 264, 288, 300, 575, 264, 16998, 2281, 294, 428, 1185, 11, 51274, 51274, 652, 300, 382, 2416, 382, 291, 393, 13, 51364, 51364, 1711, 264, 912, 565, 11, 16137, 264, 3006, 2281, 11, 51448, 51448, 652, 300, 382, 1359, 382, 291, 393, 13, 51524, 51524, 823, 456, 307, 257, 935, 412, 597, 264, 1867, 51724, 51724, 365, 264, 3006, 2281, 307, 799, 312, 2681, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.11217602321079799, "compression_ratio": 1.9793814432989691, "no_speech_prob": 3.0240587875596248e-05}, {"id": 821, "seek": 501426, "start": 5034.26, "end": 5035.9400000000005, "text": " At the same time, picks the correct energy,", "tokens": [50364, 295, 264, 3006, 1867, 1359, 293, 412, 264, 912, 565, 11, 50614, 50680, 286, 528, 281, 652, 264, 2281, 295, 264, 16998, 2281, 50910, 50910, 337, 439, 6338, 382, 2416, 382, 1944, 13, 51078, 51078, 407, 1888, 264, 288, 300, 575, 264, 16998, 2281, 294, 428, 1185, 11, 51274, 51274, 652, 300, 382, 2416, 382, 291, 393, 13, 51364, 51364, 1711, 264, 912, 565, 11, 16137, 264, 3006, 2281, 11, 51448, 51448, 652, 300, 382, 1359, 382, 291, 393, 13, 51524, 51524, 823, 456, 307, 257, 935, 412, 597, 264, 1867, 51724, 51724, 365, 264, 3006, 2281, 307, 799, 312, 2681, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.11217602321079799, "compression_ratio": 1.9793814432989691, "no_speech_prob": 3.0240587875596248e-05}, {"id": 822, "seek": 501426, "start": 5035.9400000000005, "end": 5037.46, "text": " make that as small as you can.", "tokens": [50364, 295, 264, 3006, 1867, 1359, 293, 412, 264, 912, 565, 11, 50614, 50680, 286, 528, 281, 652, 264, 2281, 295, 264, 16998, 2281, 50910, 50910, 337, 439, 6338, 382, 2416, 382, 1944, 13, 51078, 51078, 407, 1888, 264, 288, 300, 575, 264, 16998, 2281, 294, 428, 1185, 11, 51274, 51274, 652, 300, 382, 2416, 382, 291, 393, 13, 51364, 51364, 1711, 264, 912, 565, 11, 16137, 264, 3006, 2281, 11, 51448, 51448, 652, 300, 382, 1359, 382, 291, 393, 13, 51524, 51524, 823, 456, 307, 257, 935, 412, 597, 264, 1867, 51724, 51724, 365, 264, 3006, 2281, 307, 799, 312, 2681, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.11217602321079799, "compression_ratio": 1.9793814432989691, "no_speech_prob": 3.0240587875596248e-05}, {"id": 823, "seek": 501426, "start": 5037.46, "end": 5041.46, "text": " Now there is a point at which the answer", "tokens": [50364, 295, 264, 3006, 1867, 1359, 293, 412, 264, 912, 565, 11, 50614, 50680, 286, 528, 281, 652, 264, 2281, 295, 264, 16998, 2281, 50910, 50910, 337, 439, 6338, 382, 2416, 382, 1944, 13, 51078, 51078, 407, 1888, 264, 288, 300, 575, 264, 16998, 2281, 294, 428, 1185, 11, 51274, 51274, 652, 300, 382, 2416, 382, 291, 393, 13, 51364, 51364, 1711, 264, 912, 565, 11, 16137, 264, 3006, 2281, 11, 51448, 51448, 652, 300, 382, 1359, 382, 291, 393, 13, 51524, 51524, 823, 456, 307, 257, 935, 412, 597, 264, 1867, 51724, 51724, 365, 264, 3006, 2281, 307, 799, 312, 2681, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.11217602321079799, "compression_ratio": 1.9793814432989691, "no_speech_prob": 3.0240587875596248e-05}, {"id": 824, "seek": 501426, "start": 5041.46, "end": 5042.860000000001, "text": " with the correct energy is gonna be equal", "tokens": [50364, 295, 264, 3006, 1867, 1359, 293, 412, 264, 912, 565, 11, 50614, 50680, 286, 528, 281, 652, 264, 2281, 295, 264, 16998, 2281, 50910, 50910, 337, 439, 6338, 382, 2416, 382, 1944, 13, 51078, 51078, 407, 1888, 264, 288, 300, 575, 264, 16998, 2281, 294, 428, 1185, 11, 51274, 51274, 652, 300, 382, 2416, 382, 291, 393, 13, 51364, 51364, 1711, 264, 912, 565, 11, 16137, 264, 3006, 2281, 11, 51448, 51448, 652, 300, 382, 1359, 382, 291, 393, 13, 51524, 51524, 823, 456, 307, 257, 935, 412, 597, 264, 1867, 51724, 51724, 365, 264, 3006, 2281, 307, 799, 312, 2681, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.11217602321079799, "compression_ratio": 1.9793814432989691, "no_speech_prob": 3.0240587875596248e-05}, {"id": 825, "seek": 504286, "start": 5042.86, "end": 5045.179999999999, "text": " to the correct answer.", "tokens": [50364, 281, 264, 3006, 1867, 13, 50480, 50526, 400, 370, 300, 2649, 393, 1128, 312, 3671, 11, 1392, 30, 50684, 50684, 1436, 264, 700, 1433, 307, 4725, 472, 1433, 50876, 50876, 294, 300, 7285, 13, 50942, 50942, 400, 370, 264, 2649, 307, 412, 1151, 4018, 51124, 51124, 293, 337, 633, 661, 3331, 307, 3353, 13, 51374, 51448, 467, 311, 787, 4018, 562, 264, 1185, 2709, 291, 264, 3006, 1867, 13, 51626, 51626, 1033, 30, 51684, 51684, 583, 341, 10024, 2445, 775, 406, 4871, 264, 1185, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.12430065328424628, "compression_ratio": 1.6732673267326732, "no_speech_prob": 5.50746199223795e-06}, {"id": 826, "seek": 504286, "start": 5046.099999999999, "end": 5049.259999999999, "text": " And so that difference can never be negative, okay?", "tokens": [50364, 281, 264, 3006, 1867, 13, 50480, 50526, 400, 370, 300, 2649, 393, 1128, 312, 3671, 11, 1392, 30, 50684, 50684, 1436, 264, 700, 1433, 307, 4725, 472, 1433, 50876, 50876, 294, 300, 7285, 13, 50942, 50942, 400, 370, 264, 2649, 307, 412, 1151, 4018, 51124, 51124, 293, 337, 633, 661, 3331, 307, 3353, 13, 51374, 51448, 467, 311, 787, 4018, 562, 264, 1185, 2709, 291, 264, 3006, 1867, 13, 51626, 51626, 1033, 30, 51684, 51684, 583, 341, 10024, 2445, 775, 406, 4871, 264, 1185, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.12430065328424628, "compression_ratio": 1.6732673267326732, "no_speech_prob": 5.50746199223795e-06}, {"id": 827, "seek": 504286, "start": 5049.259999999999, "end": 5053.099999999999, "text": " Because the first term is necessarily one term", "tokens": [50364, 281, 264, 3006, 1867, 13, 50480, 50526, 400, 370, 300, 2649, 393, 1128, 312, 3671, 11, 1392, 30, 50684, 50684, 1436, 264, 700, 1433, 307, 4725, 472, 1433, 50876, 50876, 294, 300, 7285, 13, 50942, 50942, 400, 370, 264, 2649, 307, 412, 1151, 4018, 51124, 51124, 293, 337, 633, 661, 3331, 307, 3353, 13, 51374, 51448, 467, 311, 787, 4018, 562, 264, 1185, 2709, 291, 264, 3006, 1867, 13, 51626, 51626, 1033, 30, 51684, 51684, 583, 341, 10024, 2445, 775, 406, 4871, 264, 1185, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.12430065328424628, "compression_ratio": 1.6732673267326732, "no_speech_prob": 5.50746199223795e-06}, {"id": 828, "seek": 504286, "start": 5053.099999999999, "end": 5054.42, "text": " in that minimum.", "tokens": [50364, 281, 264, 3006, 1867, 13, 50480, 50526, 400, 370, 300, 2649, 393, 1128, 312, 3671, 11, 1392, 30, 50684, 50684, 1436, 264, 700, 1433, 307, 4725, 472, 1433, 50876, 50876, 294, 300, 7285, 13, 50942, 50942, 400, 370, 264, 2649, 307, 412, 1151, 4018, 51124, 51124, 293, 337, 633, 661, 3331, 307, 3353, 13, 51374, 51448, 467, 311, 787, 4018, 562, 264, 1185, 2709, 291, 264, 3006, 1867, 13, 51626, 51626, 1033, 30, 51684, 51684, 583, 341, 10024, 2445, 775, 406, 4871, 264, 1185, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.12430065328424628, "compression_ratio": 1.6732673267326732, "no_speech_prob": 5.50746199223795e-06}, {"id": 829, "seek": 504286, "start": 5054.42, "end": 5058.0599999999995, "text": " And so the difference is at best zero", "tokens": [50364, 281, 264, 3006, 1867, 13, 50480, 50526, 400, 370, 300, 2649, 393, 1128, 312, 3671, 11, 1392, 30, 50684, 50684, 1436, 264, 700, 1433, 307, 4725, 472, 1433, 50876, 50876, 294, 300, 7285, 13, 50942, 50942, 400, 370, 264, 2649, 307, 412, 1151, 4018, 51124, 51124, 293, 337, 633, 661, 3331, 307, 3353, 13, 51374, 51448, 467, 311, 787, 4018, 562, 264, 1185, 2709, 291, 264, 3006, 1867, 13, 51626, 51626, 1033, 30, 51684, 51684, 583, 341, 10024, 2445, 775, 406, 4871, 264, 1185, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.12430065328424628, "compression_ratio": 1.6732673267326732, "no_speech_prob": 5.50746199223795e-06}, {"id": 830, "seek": 504286, "start": 5058.0599999999995, "end": 5063.0599999999995, "text": " and for every other cases is positive.", "tokens": [50364, 281, 264, 3006, 1867, 13, 50480, 50526, 400, 370, 300, 2649, 393, 1128, 312, 3671, 11, 1392, 30, 50684, 50684, 1436, 264, 700, 1433, 307, 4725, 472, 1433, 50876, 50876, 294, 300, 7285, 13, 50942, 50942, 400, 370, 264, 2649, 307, 412, 1151, 4018, 51124, 51124, 293, 337, 633, 661, 3331, 307, 3353, 13, 51374, 51448, 467, 311, 787, 4018, 562, 264, 1185, 2709, 291, 264, 3006, 1867, 13, 51626, 51626, 1033, 30, 51684, 51684, 583, 341, 10024, 2445, 775, 406, 4871, 264, 1185, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.12430065328424628, "compression_ratio": 1.6732673267326732, "no_speech_prob": 5.50746199223795e-06}, {"id": 831, "seek": 504286, "start": 5064.54, "end": 5068.099999999999, "text": " It's only zero when the system gives you the correct answer.", "tokens": [50364, 281, 264, 3006, 1867, 13, 50480, 50526, 400, 370, 300, 2649, 393, 1128, 312, 3671, 11, 1392, 30, 50684, 50684, 1436, 264, 700, 1433, 307, 4725, 472, 1433, 50876, 50876, 294, 300, 7285, 13, 50942, 50942, 400, 370, 264, 2649, 307, 412, 1151, 4018, 51124, 51124, 293, 337, 633, 661, 3331, 307, 3353, 13, 51374, 51448, 467, 311, 787, 4018, 562, 264, 1185, 2709, 291, 264, 3006, 1867, 13, 51626, 51626, 1033, 30, 51684, 51684, 583, 341, 10024, 2445, 775, 406, 4871, 264, 1185, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.12430065328424628, "compression_ratio": 1.6732673267326732, "no_speech_prob": 5.50746199223795e-06}, {"id": 832, "seek": 504286, "start": 5068.099999999999, "end": 5069.259999999999, "text": " Okay?", "tokens": [50364, 281, 264, 3006, 1867, 13, 50480, 50526, 400, 370, 300, 2649, 393, 1128, 312, 3671, 11, 1392, 30, 50684, 50684, 1436, 264, 700, 1433, 307, 4725, 472, 1433, 50876, 50876, 294, 300, 7285, 13, 50942, 50942, 400, 370, 264, 2649, 307, 412, 1151, 4018, 51124, 51124, 293, 337, 633, 661, 3331, 307, 3353, 13, 51374, 51448, 467, 311, 787, 4018, 562, 264, 1185, 2709, 291, 264, 3006, 1867, 13, 51626, 51626, 1033, 30, 51684, 51684, 583, 341, 10024, 2445, 775, 406, 4871, 264, 1185, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.12430065328424628, "compression_ratio": 1.6732673267326732, "no_speech_prob": 5.50746199223795e-06}, {"id": 833, "seek": 504286, "start": 5069.259999999999, "end": 5072.74, "text": " But this objective function does not prevent the system", "tokens": [50364, 281, 264, 3006, 1867, 13, 50480, 50526, 400, 370, 300, 2649, 393, 1128, 312, 3671, 11, 1392, 30, 50684, 50684, 1436, 264, 700, 1433, 307, 4725, 472, 1433, 50876, 50876, 294, 300, 7285, 13, 50942, 50942, 400, 370, 264, 2649, 307, 412, 1151, 4018, 51124, 51124, 293, 337, 633, 661, 3331, 307, 3353, 13, 51374, 51448, 467, 311, 787, 4018, 562, 264, 1185, 2709, 291, 264, 3006, 1867, 13, 51626, 51626, 1033, 30, 51684, 51684, 583, 341, 10024, 2445, 775, 406, 4871, 264, 1185, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.12430065328424628, "compression_ratio": 1.6732673267326732, "no_speech_prob": 5.50746199223795e-06}, {"id": 834, "seek": 507274, "start": 5072.74, "end": 5076.34, "text": " from giving the very same energy to every answer.", "tokens": [50364, 490, 2902, 264, 588, 912, 2281, 281, 633, 1867, 13, 50544, 50544, 1033, 30, 50586, 50586, 407, 294, 300, 2020, 11, 309, 311, 257, 1578, 2281, 11, 50646, 50646, 309, 311, 257, 1578, 4470, 2445, 13, 50736, 50736, 467, 311, 257, 1578, 4470, 2445, 570, 309, 1619, 11, 50880, 50880, 286, 528, 264, 2281, 295, 264, 3006, 1867, 281, 312, 1359, 13, 50996, 50996, 286, 528, 264, 2281, 295, 439, 264, 661, 6338, 281, 312, 2416, 11, 51166, 51166, 457, 286, 500, 380, 13466, 300, 456, 307, 604, 2649, 51244, 51244, 1296, 552, 13, 51286, 51286, 407, 264, 1185, 393, 2826, 281, 652, 633, 1867, 51388, 51388, 264, 912, 2281, 13, 51430, 51504, 400, 300, 311, 257, 15584, 13, 51580, 51670, 1033, 30, 51712, 51712, 407, 43276, 2044, 4470, 307, 406, 665, 13, 51780, 51780, 467, 311, 767, 787, 665, 337, 8213, 3652, 11, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1007447049424455, "compression_ratio": 1.8830188679245283, "no_speech_prob": 1.0615202882036101e-05}, {"id": 835, "seek": 507274, "start": 5076.34, "end": 5077.179999999999, "text": " Okay?", "tokens": [50364, 490, 2902, 264, 588, 912, 2281, 281, 633, 1867, 13, 50544, 50544, 1033, 30, 50586, 50586, 407, 294, 300, 2020, 11, 309, 311, 257, 1578, 2281, 11, 50646, 50646, 309, 311, 257, 1578, 4470, 2445, 13, 50736, 50736, 467, 311, 257, 1578, 4470, 2445, 570, 309, 1619, 11, 50880, 50880, 286, 528, 264, 2281, 295, 264, 3006, 1867, 281, 312, 1359, 13, 50996, 50996, 286, 528, 264, 2281, 295, 439, 264, 661, 6338, 281, 312, 2416, 11, 51166, 51166, 457, 286, 500, 380, 13466, 300, 456, 307, 604, 2649, 51244, 51244, 1296, 552, 13, 51286, 51286, 407, 264, 1185, 393, 2826, 281, 652, 633, 1867, 51388, 51388, 264, 912, 2281, 13, 51430, 51504, 400, 300, 311, 257, 15584, 13, 51580, 51670, 1033, 30, 51712, 51712, 407, 43276, 2044, 4470, 307, 406, 665, 13, 51780, 51780, 467, 311, 767, 787, 665, 337, 8213, 3652, 11, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1007447049424455, "compression_ratio": 1.8830188679245283, "no_speech_prob": 1.0615202882036101e-05}, {"id": 836, "seek": 507274, "start": 5077.179999999999, "end": 5078.38, "text": " So in that sense, it's a bad energy,", "tokens": [50364, 490, 2902, 264, 588, 912, 2281, 281, 633, 1867, 13, 50544, 50544, 1033, 30, 50586, 50586, 407, 294, 300, 2020, 11, 309, 311, 257, 1578, 2281, 11, 50646, 50646, 309, 311, 257, 1578, 4470, 2445, 13, 50736, 50736, 467, 311, 257, 1578, 4470, 2445, 570, 309, 1619, 11, 50880, 50880, 286, 528, 264, 2281, 295, 264, 3006, 1867, 281, 312, 1359, 13, 50996, 50996, 286, 528, 264, 2281, 295, 439, 264, 661, 6338, 281, 312, 2416, 11, 51166, 51166, 457, 286, 500, 380, 13466, 300, 456, 307, 604, 2649, 51244, 51244, 1296, 552, 13, 51286, 51286, 407, 264, 1185, 393, 2826, 281, 652, 633, 1867, 51388, 51388, 264, 912, 2281, 13, 51430, 51504, 400, 300, 311, 257, 15584, 13, 51580, 51670, 1033, 30, 51712, 51712, 407, 43276, 2044, 4470, 307, 406, 665, 13, 51780, 51780, 467, 311, 767, 787, 665, 337, 8213, 3652, 11, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1007447049424455, "compression_ratio": 1.8830188679245283, "no_speech_prob": 1.0615202882036101e-05}, {"id": 837, "seek": 507274, "start": 5078.38, "end": 5080.179999999999, "text": " it's a bad loss function.", "tokens": [50364, 490, 2902, 264, 588, 912, 2281, 281, 633, 1867, 13, 50544, 50544, 1033, 30, 50586, 50586, 407, 294, 300, 2020, 11, 309, 311, 257, 1578, 2281, 11, 50646, 50646, 309, 311, 257, 1578, 4470, 2445, 13, 50736, 50736, 467, 311, 257, 1578, 4470, 2445, 570, 309, 1619, 11, 50880, 50880, 286, 528, 264, 2281, 295, 264, 3006, 1867, 281, 312, 1359, 13, 50996, 50996, 286, 528, 264, 2281, 295, 439, 264, 661, 6338, 281, 312, 2416, 11, 51166, 51166, 457, 286, 500, 380, 13466, 300, 456, 307, 604, 2649, 51244, 51244, 1296, 552, 13, 51286, 51286, 407, 264, 1185, 393, 2826, 281, 652, 633, 1867, 51388, 51388, 264, 912, 2281, 13, 51430, 51504, 400, 300, 311, 257, 15584, 13, 51580, 51670, 1033, 30, 51712, 51712, 407, 43276, 2044, 4470, 307, 406, 665, 13, 51780, 51780, 467, 311, 767, 787, 665, 337, 8213, 3652, 11, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1007447049424455, "compression_ratio": 1.8830188679245283, "no_speech_prob": 1.0615202882036101e-05}, {"id": 838, "seek": 507274, "start": 5080.179999999999, "end": 5083.0599999999995, "text": " It's a bad loss function because it says,", "tokens": [50364, 490, 2902, 264, 588, 912, 2281, 281, 633, 1867, 13, 50544, 50544, 1033, 30, 50586, 50586, 407, 294, 300, 2020, 11, 309, 311, 257, 1578, 2281, 11, 50646, 50646, 309, 311, 257, 1578, 4470, 2445, 13, 50736, 50736, 467, 311, 257, 1578, 4470, 2445, 570, 309, 1619, 11, 50880, 50880, 286, 528, 264, 2281, 295, 264, 3006, 1867, 281, 312, 1359, 13, 50996, 50996, 286, 528, 264, 2281, 295, 439, 264, 661, 6338, 281, 312, 2416, 11, 51166, 51166, 457, 286, 500, 380, 13466, 300, 456, 307, 604, 2649, 51244, 51244, 1296, 552, 13, 51286, 51286, 407, 264, 1185, 393, 2826, 281, 652, 633, 1867, 51388, 51388, 264, 912, 2281, 13, 51430, 51504, 400, 300, 311, 257, 15584, 13, 51580, 51670, 1033, 30, 51712, 51712, 407, 43276, 2044, 4470, 307, 406, 665, 13, 51780, 51780, 467, 311, 767, 787, 665, 337, 8213, 3652, 11, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1007447049424455, "compression_ratio": 1.8830188679245283, "no_speech_prob": 1.0615202882036101e-05}, {"id": 839, "seek": 507274, "start": 5083.0599999999995, "end": 5085.38, "text": " I want the energy of the correct answer to be small.", "tokens": [50364, 490, 2902, 264, 588, 912, 2281, 281, 633, 1867, 13, 50544, 50544, 1033, 30, 50586, 50586, 407, 294, 300, 2020, 11, 309, 311, 257, 1578, 2281, 11, 50646, 50646, 309, 311, 257, 1578, 4470, 2445, 13, 50736, 50736, 467, 311, 257, 1578, 4470, 2445, 570, 309, 1619, 11, 50880, 50880, 286, 528, 264, 2281, 295, 264, 3006, 1867, 281, 312, 1359, 13, 50996, 50996, 286, 528, 264, 2281, 295, 439, 264, 661, 6338, 281, 312, 2416, 11, 51166, 51166, 457, 286, 500, 380, 13466, 300, 456, 307, 604, 2649, 51244, 51244, 1296, 552, 13, 51286, 51286, 407, 264, 1185, 393, 2826, 281, 652, 633, 1867, 51388, 51388, 264, 912, 2281, 13, 51430, 51504, 400, 300, 311, 257, 15584, 13, 51580, 51670, 1033, 30, 51712, 51712, 407, 43276, 2044, 4470, 307, 406, 665, 13, 51780, 51780, 467, 311, 767, 787, 665, 337, 8213, 3652, 11, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1007447049424455, "compression_ratio": 1.8830188679245283, "no_speech_prob": 1.0615202882036101e-05}, {"id": 840, "seek": 507274, "start": 5085.38, "end": 5088.78, "text": " I want the energy of all the other answers to be large,", "tokens": [50364, 490, 2902, 264, 588, 912, 2281, 281, 633, 1867, 13, 50544, 50544, 1033, 30, 50586, 50586, 407, 294, 300, 2020, 11, 309, 311, 257, 1578, 2281, 11, 50646, 50646, 309, 311, 257, 1578, 4470, 2445, 13, 50736, 50736, 467, 311, 257, 1578, 4470, 2445, 570, 309, 1619, 11, 50880, 50880, 286, 528, 264, 2281, 295, 264, 3006, 1867, 281, 312, 1359, 13, 50996, 50996, 286, 528, 264, 2281, 295, 439, 264, 661, 6338, 281, 312, 2416, 11, 51166, 51166, 457, 286, 500, 380, 13466, 300, 456, 307, 604, 2649, 51244, 51244, 1296, 552, 13, 51286, 51286, 407, 264, 1185, 393, 2826, 281, 652, 633, 1867, 51388, 51388, 264, 912, 2281, 13, 51430, 51504, 400, 300, 311, 257, 15584, 13, 51580, 51670, 1033, 30, 51712, 51712, 407, 43276, 2044, 4470, 307, 406, 665, 13, 51780, 51780, 467, 311, 767, 787, 665, 337, 8213, 3652, 11, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1007447049424455, "compression_ratio": 1.8830188679245283, "no_speech_prob": 1.0615202882036101e-05}, {"id": 841, "seek": 507274, "start": 5088.78, "end": 5090.34, "text": " but I don't insist that there is any difference", "tokens": [50364, 490, 2902, 264, 588, 912, 2281, 281, 633, 1867, 13, 50544, 50544, 1033, 30, 50586, 50586, 407, 294, 300, 2020, 11, 309, 311, 257, 1578, 2281, 11, 50646, 50646, 309, 311, 257, 1578, 4470, 2445, 13, 50736, 50736, 467, 311, 257, 1578, 4470, 2445, 570, 309, 1619, 11, 50880, 50880, 286, 528, 264, 2281, 295, 264, 3006, 1867, 281, 312, 1359, 13, 50996, 50996, 286, 528, 264, 2281, 295, 439, 264, 661, 6338, 281, 312, 2416, 11, 51166, 51166, 457, 286, 500, 380, 13466, 300, 456, 307, 604, 2649, 51244, 51244, 1296, 552, 13, 51286, 51286, 407, 264, 1185, 393, 2826, 281, 652, 633, 1867, 51388, 51388, 264, 912, 2281, 13, 51430, 51504, 400, 300, 311, 257, 15584, 13, 51580, 51670, 1033, 30, 51712, 51712, 407, 43276, 2044, 4470, 307, 406, 665, 13, 51780, 51780, 467, 311, 767, 787, 665, 337, 8213, 3652, 11, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1007447049424455, "compression_ratio": 1.8830188679245283, "no_speech_prob": 1.0615202882036101e-05}, {"id": 842, "seek": 507274, "start": 5090.34, "end": 5091.179999999999, "text": " between them.", "tokens": [50364, 490, 2902, 264, 588, 912, 2281, 281, 633, 1867, 13, 50544, 50544, 1033, 30, 50586, 50586, 407, 294, 300, 2020, 11, 309, 311, 257, 1578, 2281, 11, 50646, 50646, 309, 311, 257, 1578, 4470, 2445, 13, 50736, 50736, 467, 311, 257, 1578, 4470, 2445, 570, 309, 1619, 11, 50880, 50880, 286, 528, 264, 2281, 295, 264, 3006, 1867, 281, 312, 1359, 13, 50996, 50996, 286, 528, 264, 2281, 295, 439, 264, 661, 6338, 281, 312, 2416, 11, 51166, 51166, 457, 286, 500, 380, 13466, 300, 456, 307, 604, 2649, 51244, 51244, 1296, 552, 13, 51286, 51286, 407, 264, 1185, 393, 2826, 281, 652, 633, 1867, 51388, 51388, 264, 912, 2281, 13, 51430, 51504, 400, 300, 311, 257, 15584, 13, 51580, 51670, 1033, 30, 51712, 51712, 407, 43276, 2044, 4470, 307, 406, 665, 13, 51780, 51780, 467, 311, 767, 787, 665, 337, 8213, 3652, 11, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1007447049424455, "compression_ratio": 1.8830188679245283, "no_speech_prob": 1.0615202882036101e-05}, {"id": 843, "seek": 507274, "start": 5091.179999999999, "end": 5093.219999999999, "text": " So the system can choose to make every answer", "tokens": [50364, 490, 2902, 264, 588, 912, 2281, 281, 633, 1867, 13, 50544, 50544, 1033, 30, 50586, 50586, 407, 294, 300, 2020, 11, 309, 311, 257, 1578, 2281, 11, 50646, 50646, 309, 311, 257, 1578, 4470, 2445, 13, 50736, 50736, 467, 311, 257, 1578, 4470, 2445, 570, 309, 1619, 11, 50880, 50880, 286, 528, 264, 2281, 295, 264, 3006, 1867, 281, 312, 1359, 13, 50996, 50996, 286, 528, 264, 2281, 295, 439, 264, 661, 6338, 281, 312, 2416, 11, 51166, 51166, 457, 286, 500, 380, 13466, 300, 456, 307, 604, 2649, 51244, 51244, 1296, 552, 13, 51286, 51286, 407, 264, 1185, 393, 2826, 281, 652, 633, 1867, 51388, 51388, 264, 912, 2281, 13, 51430, 51504, 400, 300, 311, 257, 15584, 13, 51580, 51670, 1033, 30, 51712, 51712, 407, 43276, 2044, 4470, 307, 406, 665, 13, 51780, 51780, 467, 311, 767, 787, 665, 337, 8213, 3652, 11, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1007447049424455, "compression_ratio": 1.8830188679245283, "no_speech_prob": 1.0615202882036101e-05}, {"id": 844, "seek": 507274, "start": 5093.219999999999, "end": 5094.0599999999995, "text": " the same energy.", "tokens": [50364, 490, 2902, 264, 588, 912, 2281, 281, 633, 1867, 13, 50544, 50544, 1033, 30, 50586, 50586, 407, 294, 300, 2020, 11, 309, 311, 257, 1578, 2281, 11, 50646, 50646, 309, 311, 257, 1578, 4470, 2445, 13, 50736, 50736, 467, 311, 257, 1578, 4470, 2445, 570, 309, 1619, 11, 50880, 50880, 286, 528, 264, 2281, 295, 264, 3006, 1867, 281, 312, 1359, 13, 50996, 50996, 286, 528, 264, 2281, 295, 439, 264, 661, 6338, 281, 312, 2416, 11, 51166, 51166, 457, 286, 500, 380, 13466, 300, 456, 307, 604, 2649, 51244, 51244, 1296, 552, 13, 51286, 51286, 407, 264, 1185, 393, 2826, 281, 652, 633, 1867, 51388, 51388, 264, 912, 2281, 13, 51430, 51504, 400, 300, 311, 257, 15584, 13, 51580, 51670, 1033, 30, 51712, 51712, 407, 43276, 2044, 4470, 307, 406, 665, 13, 51780, 51780, 467, 311, 767, 787, 665, 337, 8213, 3652, 11, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1007447049424455, "compression_ratio": 1.8830188679245283, "no_speech_prob": 1.0615202882036101e-05}, {"id": 845, "seek": 507274, "start": 5095.54, "end": 5097.0599999999995, "text": " And that's a collapse.", "tokens": [50364, 490, 2902, 264, 588, 912, 2281, 281, 633, 1867, 13, 50544, 50544, 1033, 30, 50586, 50586, 407, 294, 300, 2020, 11, 309, 311, 257, 1578, 2281, 11, 50646, 50646, 309, 311, 257, 1578, 4470, 2445, 13, 50736, 50736, 467, 311, 257, 1578, 4470, 2445, 570, 309, 1619, 11, 50880, 50880, 286, 528, 264, 2281, 295, 264, 3006, 1867, 281, 312, 1359, 13, 50996, 50996, 286, 528, 264, 2281, 295, 439, 264, 661, 6338, 281, 312, 2416, 11, 51166, 51166, 457, 286, 500, 380, 13466, 300, 456, 307, 604, 2649, 51244, 51244, 1296, 552, 13, 51286, 51286, 407, 264, 1185, 393, 2826, 281, 652, 633, 1867, 51388, 51388, 264, 912, 2281, 13, 51430, 51504, 400, 300, 311, 257, 15584, 13, 51580, 51670, 1033, 30, 51712, 51712, 407, 43276, 2044, 4470, 307, 406, 665, 13, 51780, 51780, 467, 311, 767, 787, 665, 337, 8213, 3652, 11, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1007447049424455, "compression_ratio": 1.8830188679245283, "no_speech_prob": 1.0615202882036101e-05}, {"id": 846, "seek": 507274, "start": 5098.86, "end": 5099.7, "text": " Okay?", "tokens": [50364, 490, 2902, 264, 588, 912, 2281, 281, 633, 1867, 13, 50544, 50544, 1033, 30, 50586, 50586, 407, 294, 300, 2020, 11, 309, 311, 257, 1578, 2281, 11, 50646, 50646, 309, 311, 257, 1578, 4470, 2445, 13, 50736, 50736, 467, 311, 257, 1578, 4470, 2445, 570, 309, 1619, 11, 50880, 50880, 286, 528, 264, 2281, 295, 264, 3006, 1867, 281, 312, 1359, 13, 50996, 50996, 286, 528, 264, 2281, 295, 439, 264, 661, 6338, 281, 312, 2416, 11, 51166, 51166, 457, 286, 500, 380, 13466, 300, 456, 307, 604, 2649, 51244, 51244, 1296, 552, 13, 51286, 51286, 407, 264, 1185, 393, 2826, 281, 652, 633, 1867, 51388, 51388, 264, 912, 2281, 13, 51430, 51504, 400, 300, 311, 257, 15584, 13, 51580, 51670, 1033, 30, 51712, 51712, 407, 43276, 2044, 4470, 307, 406, 665, 13, 51780, 51780, 467, 311, 767, 787, 665, 337, 8213, 3652, 11, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1007447049424455, "compression_ratio": 1.8830188679245283, "no_speech_prob": 1.0615202882036101e-05}, {"id": 847, "seek": 507274, "start": 5099.7, "end": 5101.0599999999995, "text": " So perceptron loss is not good.", "tokens": [50364, 490, 2902, 264, 588, 912, 2281, 281, 633, 1867, 13, 50544, 50544, 1033, 30, 50586, 50586, 407, 294, 300, 2020, 11, 309, 311, 257, 1578, 2281, 11, 50646, 50646, 309, 311, 257, 1578, 4470, 2445, 13, 50736, 50736, 467, 311, 257, 1578, 4470, 2445, 570, 309, 1619, 11, 50880, 50880, 286, 528, 264, 2281, 295, 264, 3006, 1867, 281, 312, 1359, 13, 50996, 50996, 286, 528, 264, 2281, 295, 439, 264, 661, 6338, 281, 312, 2416, 11, 51166, 51166, 457, 286, 500, 380, 13466, 300, 456, 307, 604, 2649, 51244, 51244, 1296, 552, 13, 51286, 51286, 407, 264, 1185, 393, 2826, 281, 652, 633, 1867, 51388, 51388, 264, 912, 2281, 13, 51430, 51504, 400, 300, 311, 257, 15584, 13, 51580, 51670, 1033, 30, 51712, 51712, 407, 43276, 2044, 4470, 307, 406, 665, 13, 51780, 51780, 467, 311, 767, 787, 665, 337, 8213, 3652, 11, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1007447049424455, "compression_ratio": 1.8830188679245283, "no_speech_prob": 1.0615202882036101e-05}, {"id": 848, "seek": 507274, "start": 5101.0599999999995, "end": 5102.7, "text": " It's actually only good for linear systems,", "tokens": [50364, 490, 2902, 264, 588, 912, 2281, 281, 633, 1867, 13, 50544, 50544, 1033, 30, 50586, 50586, 407, 294, 300, 2020, 11, 309, 311, 257, 1578, 2281, 11, 50646, 50646, 309, 311, 257, 1578, 4470, 2445, 13, 50736, 50736, 467, 311, 257, 1578, 4470, 2445, 570, 309, 1619, 11, 50880, 50880, 286, 528, 264, 2281, 295, 264, 3006, 1867, 281, 312, 1359, 13, 50996, 50996, 286, 528, 264, 2281, 295, 439, 264, 661, 6338, 281, 312, 2416, 11, 51166, 51166, 457, 286, 500, 380, 13466, 300, 456, 307, 604, 2649, 51244, 51244, 1296, 552, 13, 51286, 51286, 407, 264, 1185, 393, 2826, 281, 652, 633, 1867, 51388, 51388, 264, 912, 2281, 13, 51430, 51504, 400, 300, 311, 257, 15584, 13, 51580, 51670, 1033, 30, 51712, 51712, 407, 43276, 2044, 4470, 307, 406, 665, 13, 51780, 51780, 467, 311, 767, 787, 665, 337, 8213, 3652, 11, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1007447049424455, "compression_ratio": 1.8830188679245283, "no_speech_prob": 1.0615202882036101e-05}, {"id": 849, "seek": 510270, "start": 5102.7, "end": 5103.78, "text": " but it's not good for,", "tokens": [50364, 457, 309, 311, 406, 665, 337, 11, 50418, 50500, 382, 364, 10024, 2445, 337, 2107, 28263, 3652, 13, 50664, 50758, 407, 510, 311, 257, 636, 281, 1715, 364, 10024, 2445, 50872, 50872, 300, 486, 1009, 312, 665, 13, 50962, 51008, 400, 291, 747, 264, 2281, 295, 264, 3006, 1867, 51166, 51166, 293, 291, 747, 264, 2281, 295, 264, 881, 51226, 51226, 295, 5006, 18424, 1867, 11, 51278, 51278, 597, 1355, 264, 2158, 295, 398, 300, 307, 18424, 11, 51438, 51438, 457, 412, 264, 912, 565, 11, 51480, 51480, 300, 307, 264, 12437, 2281, 295, 439, 264, 18424, 6338, 13, 51682, 51682, 1033, 30, 51762, 51762, 400, 428, 1185, 486, 589, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.1389033083330121, "compression_ratio": 1.885321100917431, "no_speech_prob": 9.817876161832828e-06}, {"id": 850, "seek": 510270, "start": 5105.42, "end": 5108.7, "text": " as an objective function for nonlinear systems.", "tokens": [50364, 457, 309, 311, 406, 665, 337, 11, 50418, 50500, 382, 364, 10024, 2445, 337, 2107, 28263, 3652, 13, 50664, 50758, 407, 510, 311, 257, 636, 281, 1715, 364, 10024, 2445, 50872, 50872, 300, 486, 1009, 312, 665, 13, 50962, 51008, 400, 291, 747, 264, 2281, 295, 264, 3006, 1867, 51166, 51166, 293, 291, 747, 264, 2281, 295, 264, 881, 51226, 51226, 295, 5006, 18424, 1867, 11, 51278, 51278, 597, 1355, 264, 2158, 295, 398, 300, 307, 18424, 11, 51438, 51438, 457, 412, 264, 912, 565, 11, 51480, 51480, 300, 307, 264, 12437, 2281, 295, 439, 264, 18424, 6338, 13, 51682, 51682, 1033, 30, 51762, 51762, 400, 428, 1185, 486, 589, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.1389033083330121, "compression_ratio": 1.885321100917431, "no_speech_prob": 9.817876161832828e-06}, {"id": 851, "seek": 510270, "start": 5110.58, "end": 5112.86, "text": " So here's a way to design an objective function", "tokens": [50364, 457, 309, 311, 406, 665, 337, 11, 50418, 50500, 382, 364, 10024, 2445, 337, 2107, 28263, 3652, 13, 50664, 50758, 407, 510, 311, 257, 636, 281, 1715, 364, 10024, 2445, 50872, 50872, 300, 486, 1009, 312, 665, 13, 50962, 51008, 400, 291, 747, 264, 2281, 295, 264, 3006, 1867, 51166, 51166, 293, 291, 747, 264, 2281, 295, 264, 881, 51226, 51226, 295, 5006, 18424, 1867, 11, 51278, 51278, 597, 1355, 264, 2158, 295, 398, 300, 307, 18424, 11, 51438, 51438, 457, 412, 264, 912, 565, 11, 51480, 51480, 300, 307, 264, 12437, 2281, 295, 439, 264, 18424, 6338, 13, 51682, 51682, 1033, 30, 51762, 51762, 400, 428, 1185, 486, 589, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.1389033083330121, "compression_ratio": 1.885321100917431, "no_speech_prob": 9.817876161832828e-06}, {"id": 852, "seek": 510270, "start": 5112.86, "end": 5114.66, "text": " that will always be good.", "tokens": [50364, 457, 309, 311, 406, 665, 337, 11, 50418, 50500, 382, 364, 10024, 2445, 337, 2107, 28263, 3652, 13, 50664, 50758, 407, 510, 311, 257, 636, 281, 1715, 364, 10024, 2445, 50872, 50872, 300, 486, 1009, 312, 665, 13, 50962, 51008, 400, 291, 747, 264, 2281, 295, 264, 3006, 1867, 51166, 51166, 293, 291, 747, 264, 2281, 295, 264, 881, 51226, 51226, 295, 5006, 18424, 1867, 11, 51278, 51278, 597, 1355, 264, 2158, 295, 398, 300, 307, 18424, 11, 51438, 51438, 457, 412, 264, 912, 565, 11, 51480, 51480, 300, 307, 264, 12437, 2281, 295, 439, 264, 18424, 6338, 13, 51682, 51682, 1033, 30, 51762, 51762, 400, 428, 1185, 486, 589, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.1389033083330121, "compression_ratio": 1.885321100917431, "no_speech_prob": 9.817876161832828e-06}, {"id": 853, "seek": 510270, "start": 5115.58, "end": 5118.74, "text": " And you take the energy of the correct answer", "tokens": [50364, 457, 309, 311, 406, 665, 337, 11, 50418, 50500, 382, 364, 10024, 2445, 337, 2107, 28263, 3652, 13, 50664, 50758, 407, 510, 311, 257, 636, 281, 1715, 364, 10024, 2445, 50872, 50872, 300, 486, 1009, 312, 665, 13, 50962, 51008, 400, 291, 747, 264, 2281, 295, 264, 3006, 1867, 51166, 51166, 293, 291, 747, 264, 2281, 295, 264, 881, 51226, 51226, 295, 5006, 18424, 1867, 11, 51278, 51278, 597, 1355, 264, 2158, 295, 398, 300, 307, 18424, 11, 51438, 51438, 457, 412, 264, 912, 565, 11, 51480, 51480, 300, 307, 264, 12437, 2281, 295, 439, 264, 18424, 6338, 13, 51682, 51682, 1033, 30, 51762, 51762, 400, 428, 1185, 486, 589, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.1389033083330121, "compression_ratio": 1.885321100917431, "no_speech_prob": 9.817876161832828e-06}, {"id": 854, "seek": 510270, "start": 5118.74, "end": 5119.94, "text": " and you take the energy of the most", "tokens": [50364, 457, 309, 311, 406, 665, 337, 11, 50418, 50500, 382, 364, 10024, 2445, 337, 2107, 28263, 3652, 13, 50664, 50758, 407, 510, 311, 257, 636, 281, 1715, 364, 10024, 2445, 50872, 50872, 300, 486, 1009, 312, 665, 13, 50962, 51008, 400, 291, 747, 264, 2281, 295, 264, 3006, 1867, 51166, 51166, 293, 291, 747, 264, 2281, 295, 264, 881, 51226, 51226, 295, 5006, 18424, 1867, 11, 51278, 51278, 597, 1355, 264, 2158, 295, 398, 300, 307, 18424, 11, 51438, 51438, 457, 412, 264, 912, 565, 11, 51480, 51480, 300, 307, 264, 12437, 2281, 295, 439, 264, 18424, 6338, 13, 51682, 51682, 1033, 30, 51762, 51762, 400, 428, 1185, 486, 589, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.1389033083330121, "compression_ratio": 1.885321100917431, "no_speech_prob": 9.817876161832828e-06}, {"id": 855, "seek": 510270, "start": 5119.94, "end": 5120.98, "text": " of finding incorrect answer,", "tokens": [50364, 457, 309, 311, 406, 665, 337, 11, 50418, 50500, 382, 364, 10024, 2445, 337, 2107, 28263, 3652, 13, 50664, 50758, 407, 510, 311, 257, 636, 281, 1715, 364, 10024, 2445, 50872, 50872, 300, 486, 1009, 312, 665, 13, 50962, 51008, 400, 291, 747, 264, 2281, 295, 264, 3006, 1867, 51166, 51166, 293, 291, 747, 264, 2281, 295, 264, 881, 51226, 51226, 295, 5006, 18424, 1867, 11, 51278, 51278, 597, 1355, 264, 2158, 295, 398, 300, 307, 18424, 11, 51438, 51438, 457, 412, 264, 912, 565, 11, 51480, 51480, 300, 307, 264, 12437, 2281, 295, 439, 264, 18424, 6338, 13, 51682, 51682, 1033, 30, 51762, 51762, 400, 428, 1185, 486, 589, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.1389033083330121, "compression_ratio": 1.885321100917431, "no_speech_prob": 9.817876161832828e-06}, {"id": 856, "seek": 510270, "start": 5120.98, "end": 5124.179999999999, "text": " which means the value of Y that is incorrect,", "tokens": [50364, 457, 309, 311, 406, 665, 337, 11, 50418, 50500, 382, 364, 10024, 2445, 337, 2107, 28263, 3652, 13, 50664, 50758, 407, 510, 311, 257, 636, 281, 1715, 364, 10024, 2445, 50872, 50872, 300, 486, 1009, 312, 665, 13, 50962, 51008, 400, 291, 747, 264, 2281, 295, 264, 3006, 1867, 51166, 51166, 293, 291, 747, 264, 2281, 295, 264, 881, 51226, 51226, 295, 5006, 18424, 1867, 11, 51278, 51278, 597, 1355, 264, 2158, 295, 398, 300, 307, 18424, 11, 51438, 51438, 457, 412, 264, 912, 565, 11, 51480, 51480, 300, 307, 264, 12437, 2281, 295, 439, 264, 18424, 6338, 13, 51682, 51682, 1033, 30, 51762, 51762, 400, 428, 1185, 486, 589, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.1389033083330121, "compression_ratio": 1.885321100917431, "no_speech_prob": 9.817876161832828e-06}, {"id": 857, "seek": 510270, "start": 5124.179999999999, "end": 5125.0199999999995, "text": " but at the same time,", "tokens": [50364, 457, 309, 311, 406, 665, 337, 11, 50418, 50500, 382, 364, 10024, 2445, 337, 2107, 28263, 3652, 13, 50664, 50758, 407, 510, 311, 257, 636, 281, 1715, 364, 10024, 2445, 50872, 50872, 300, 486, 1009, 312, 665, 13, 50962, 51008, 400, 291, 747, 264, 2281, 295, 264, 3006, 1867, 51166, 51166, 293, 291, 747, 264, 2281, 295, 264, 881, 51226, 51226, 295, 5006, 18424, 1867, 11, 51278, 51278, 597, 1355, 264, 2158, 295, 398, 300, 307, 18424, 11, 51438, 51438, 457, 412, 264, 912, 565, 11, 51480, 51480, 300, 307, 264, 12437, 2281, 295, 439, 264, 18424, 6338, 13, 51682, 51682, 1033, 30, 51762, 51762, 400, 428, 1185, 486, 589, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.1389033083330121, "compression_ratio": 1.885321100917431, "no_speech_prob": 9.817876161832828e-06}, {"id": 858, "seek": 510270, "start": 5125.0199999999995, "end": 5129.0599999999995, "text": " that is the lowest energy of all the incorrect answers.", "tokens": [50364, 457, 309, 311, 406, 665, 337, 11, 50418, 50500, 382, 364, 10024, 2445, 337, 2107, 28263, 3652, 13, 50664, 50758, 407, 510, 311, 257, 636, 281, 1715, 364, 10024, 2445, 50872, 50872, 300, 486, 1009, 312, 665, 13, 50962, 51008, 400, 291, 747, 264, 2281, 295, 264, 3006, 1867, 51166, 51166, 293, 291, 747, 264, 2281, 295, 264, 881, 51226, 51226, 295, 5006, 18424, 1867, 11, 51278, 51278, 597, 1355, 264, 2158, 295, 398, 300, 307, 18424, 11, 51438, 51438, 457, 412, 264, 912, 565, 11, 51480, 51480, 300, 307, 264, 12437, 2281, 295, 439, 264, 18424, 6338, 13, 51682, 51682, 1033, 30, 51762, 51762, 400, 428, 1185, 486, 589, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.1389033083330121, "compression_ratio": 1.885321100917431, "no_speech_prob": 9.817876161832828e-06}, {"id": 859, "seek": 510270, "start": 5129.0599999999995, "end": 5130.66, "text": " Okay?", "tokens": [50364, 457, 309, 311, 406, 665, 337, 11, 50418, 50500, 382, 364, 10024, 2445, 337, 2107, 28263, 3652, 13, 50664, 50758, 407, 510, 311, 257, 636, 281, 1715, 364, 10024, 2445, 50872, 50872, 300, 486, 1009, 312, 665, 13, 50962, 51008, 400, 291, 747, 264, 2281, 295, 264, 3006, 1867, 51166, 51166, 293, 291, 747, 264, 2281, 295, 264, 881, 51226, 51226, 295, 5006, 18424, 1867, 11, 51278, 51278, 597, 1355, 264, 2158, 295, 398, 300, 307, 18424, 11, 51438, 51438, 457, 412, 264, 912, 565, 11, 51480, 51480, 300, 307, 264, 12437, 2281, 295, 439, 264, 18424, 6338, 13, 51682, 51682, 1033, 30, 51762, 51762, 400, 428, 1185, 486, 589, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.1389033083330121, "compression_ratio": 1.885321100917431, "no_speech_prob": 9.817876161832828e-06}, {"id": 860, "seek": 510270, "start": 5130.66, "end": 5132.0599999999995, "text": " And your system will work", "tokens": [50364, 457, 309, 311, 406, 665, 337, 11, 50418, 50500, 382, 364, 10024, 2445, 337, 2107, 28263, 3652, 13, 50664, 50758, 407, 510, 311, 257, 636, 281, 1715, 364, 10024, 2445, 50872, 50872, 300, 486, 1009, 312, 665, 13, 50962, 51008, 400, 291, 747, 264, 2281, 295, 264, 3006, 1867, 51166, 51166, 293, 291, 747, 264, 2281, 295, 264, 881, 51226, 51226, 295, 5006, 18424, 1867, 11, 51278, 51278, 597, 1355, 264, 2158, 295, 398, 300, 307, 18424, 11, 51438, 51438, 457, 412, 264, 912, 565, 11, 51480, 51480, 300, 307, 264, 12437, 2281, 295, 439, 264, 18424, 6338, 13, 51682, 51682, 1033, 30, 51762, 51762, 400, 428, 1185, 486, 589, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.1389033083330121, "compression_ratio": 1.885321100917431, "no_speech_prob": 9.817876161832828e-06}, {"id": 861, "seek": 513206, "start": 5132.06, "end": 5134.38, "text": " if that difference is negative.", "tokens": [50364, 498, 300, 2649, 307, 3671, 13, 50480, 50480, 682, 661, 2283, 11, 50522, 50522, 498, 264, 2281, 295, 264, 3006, 1867, 307, 4356, 50632, 50632, 813, 264, 2281, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50758, 50758, 457, 412, 1935, 512, 11275, 11, 512, 10270, 13, 50878, 50932, 1033, 30, 50974, 50974, 407, 382, 938, 382, 428, 10024, 2445, 11, 51070, 51070, 562, 291, 1715, 309, 11, 51120, 51120, 28111, 300, 264, 2281, 295, 264, 3006, 1867, 51240, 51240, 307, 4356, 813, 264, 2281, 51288, 51288, 295, 264, 881, 295, 5006, 18424, 1867, 51354, 51354, 538, 412, 1935, 512, 10270, 11, 51418, 51418, 2107, 12, 32226, 10270, 11, 51492, 51492, 550, 291, 434, 2489, 13, 51564, 51564, 2260, 4470, 2445, 307, 665, 13, 51722, 51820], "temperature": 0.0, "avg_logprob": -0.09236443787813187, "compression_ratio": 2.1203703703703702, "no_speech_prob": 1.892288855742663e-05}, {"id": 862, "seek": 513206, "start": 5134.38, "end": 5135.22, "text": " In other words,", "tokens": [50364, 498, 300, 2649, 307, 3671, 13, 50480, 50480, 682, 661, 2283, 11, 50522, 50522, 498, 264, 2281, 295, 264, 3006, 1867, 307, 4356, 50632, 50632, 813, 264, 2281, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50758, 50758, 457, 412, 1935, 512, 11275, 11, 512, 10270, 13, 50878, 50932, 1033, 30, 50974, 50974, 407, 382, 938, 382, 428, 10024, 2445, 11, 51070, 51070, 562, 291, 1715, 309, 11, 51120, 51120, 28111, 300, 264, 2281, 295, 264, 3006, 1867, 51240, 51240, 307, 4356, 813, 264, 2281, 51288, 51288, 295, 264, 881, 295, 5006, 18424, 1867, 51354, 51354, 538, 412, 1935, 512, 10270, 11, 51418, 51418, 2107, 12, 32226, 10270, 11, 51492, 51492, 550, 291, 434, 2489, 13, 51564, 51564, 2260, 4470, 2445, 307, 665, 13, 51722, 51820], "temperature": 0.0, "avg_logprob": -0.09236443787813187, "compression_ratio": 2.1203703703703702, "no_speech_prob": 1.892288855742663e-05}, {"id": 863, "seek": 513206, "start": 5135.22, "end": 5137.42, "text": " if the energy of the correct answer is smaller", "tokens": [50364, 498, 300, 2649, 307, 3671, 13, 50480, 50480, 682, 661, 2283, 11, 50522, 50522, 498, 264, 2281, 295, 264, 3006, 1867, 307, 4356, 50632, 50632, 813, 264, 2281, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50758, 50758, 457, 412, 1935, 512, 11275, 11, 512, 10270, 13, 50878, 50932, 1033, 30, 50974, 50974, 407, 382, 938, 382, 428, 10024, 2445, 11, 51070, 51070, 562, 291, 1715, 309, 11, 51120, 51120, 28111, 300, 264, 2281, 295, 264, 3006, 1867, 51240, 51240, 307, 4356, 813, 264, 2281, 51288, 51288, 295, 264, 881, 295, 5006, 18424, 1867, 51354, 51354, 538, 412, 1935, 512, 10270, 11, 51418, 51418, 2107, 12, 32226, 10270, 11, 51492, 51492, 550, 291, 434, 2489, 13, 51564, 51564, 2260, 4470, 2445, 307, 665, 13, 51722, 51820], "temperature": 0.0, "avg_logprob": -0.09236443787813187, "compression_ratio": 2.1203703703703702, "no_speech_prob": 1.892288855742663e-05}, {"id": 864, "seek": 513206, "start": 5137.42, "end": 5139.9400000000005, "text": " than the energy of the most of finding incorrect answer,", "tokens": [50364, 498, 300, 2649, 307, 3671, 13, 50480, 50480, 682, 661, 2283, 11, 50522, 50522, 498, 264, 2281, 295, 264, 3006, 1867, 307, 4356, 50632, 50632, 813, 264, 2281, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50758, 50758, 457, 412, 1935, 512, 11275, 11, 512, 10270, 13, 50878, 50932, 1033, 30, 50974, 50974, 407, 382, 938, 382, 428, 10024, 2445, 11, 51070, 51070, 562, 291, 1715, 309, 11, 51120, 51120, 28111, 300, 264, 2281, 295, 264, 3006, 1867, 51240, 51240, 307, 4356, 813, 264, 2281, 51288, 51288, 295, 264, 881, 295, 5006, 18424, 1867, 51354, 51354, 538, 412, 1935, 512, 10270, 11, 51418, 51418, 2107, 12, 32226, 10270, 11, 51492, 51492, 550, 291, 434, 2489, 13, 51564, 51564, 2260, 4470, 2445, 307, 665, 13, 51722, 51820], "temperature": 0.0, "avg_logprob": -0.09236443787813187, "compression_ratio": 2.1203703703703702, "no_speech_prob": 1.892288855742663e-05}, {"id": 865, "seek": 513206, "start": 5139.9400000000005, "end": 5142.34, "text": " but at least some quantity, some margin.", "tokens": [50364, 498, 300, 2649, 307, 3671, 13, 50480, 50480, 682, 661, 2283, 11, 50522, 50522, 498, 264, 2281, 295, 264, 3006, 1867, 307, 4356, 50632, 50632, 813, 264, 2281, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50758, 50758, 457, 412, 1935, 512, 11275, 11, 512, 10270, 13, 50878, 50932, 1033, 30, 50974, 50974, 407, 382, 938, 382, 428, 10024, 2445, 11, 51070, 51070, 562, 291, 1715, 309, 11, 51120, 51120, 28111, 300, 264, 2281, 295, 264, 3006, 1867, 51240, 51240, 307, 4356, 813, 264, 2281, 51288, 51288, 295, 264, 881, 295, 5006, 18424, 1867, 51354, 51354, 538, 412, 1935, 512, 10270, 11, 51418, 51418, 2107, 12, 32226, 10270, 11, 51492, 51492, 550, 291, 434, 2489, 13, 51564, 51564, 2260, 4470, 2445, 307, 665, 13, 51722, 51820], "temperature": 0.0, "avg_logprob": -0.09236443787813187, "compression_ratio": 2.1203703703703702, "no_speech_prob": 1.892288855742663e-05}, {"id": 866, "seek": 513206, "start": 5143.42, "end": 5144.26, "text": " Okay?", "tokens": [50364, 498, 300, 2649, 307, 3671, 13, 50480, 50480, 682, 661, 2283, 11, 50522, 50522, 498, 264, 2281, 295, 264, 3006, 1867, 307, 4356, 50632, 50632, 813, 264, 2281, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50758, 50758, 457, 412, 1935, 512, 11275, 11, 512, 10270, 13, 50878, 50932, 1033, 30, 50974, 50974, 407, 382, 938, 382, 428, 10024, 2445, 11, 51070, 51070, 562, 291, 1715, 309, 11, 51120, 51120, 28111, 300, 264, 2281, 295, 264, 3006, 1867, 51240, 51240, 307, 4356, 813, 264, 2281, 51288, 51288, 295, 264, 881, 295, 5006, 18424, 1867, 51354, 51354, 538, 412, 1935, 512, 10270, 11, 51418, 51418, 2107, 12, 32226, 10270, 11, 51492, 51492, 550, 291, 434, 2489, 13, 51564, 51564, 2260, 4470, 2445, 307, 665, 13, 51722, 51820], "temperature": 0.0, "avg_logprob": -0.09236443787813187, "compression_ratio": 2.1203703703703702, "no_speech_prob": 1.892288855742663e-05}, {"id": 867, "seek": 513206, "start": 5144.26, "end": 5146.18, "text": " So as long as your objective function,", "tokens": [50364, 498, 300, 2649, 307, 3671, 13, 50480, 50480, 682, 661, 2283, 11, 50522, 50522, 498, 264, 2281, 295, 264, 3006, 1867, 307, 4356, 50632, 50632, 813, 264, 2281, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50758, 50758, 457, 412, 1935, 512, 11275, 11, 512, 10270, 13, 50878, 50932, 1033, 30, 50974, 50974, 407, 382, 938, 382, 428, 10024, 2445, 11, 51070, 51070, 562, 291, 1715, 309, 11, 51120, 51120, 28111, 300, 264, 2281, 295, 264, 3006, 1867, 51240, 51240, 307, 4356, 813, 264, 2281, 51288, 51288, 295, 264, 881, 295, 5006, 18424, 1867, 51354, 51354, 538, 412, 1935, 512, 10270, 11, 51418, 51418, 2107, 12, 32226, 10270, 11, 51492, 51492, 550, 291, 434, 2489, 13, 51564, 51564, 2260, 4470, 2445, 307, 665, 13, 51722, 51820], "temperature": 0.0, "avg_logprob": -0.09236443787813187, "compression_ratio": 2.1203703703703702, "no_speech_prob": 1.892288855742663e-05}, {"id": 868, "seek": 513206, "start": 5146.18, "end": 5147.18, "text": " when you design it,", "tokens": [50364, 498, 300, 2649, 307, 3671, 13, 50480, 50480, 682, 661, 2283, 11, 50522, 50522, 498, 264, 2281, 295, 264, 3006, 1867, 307, 4356, 50632, 50632, 813, 264, 2281, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50758, 50758, 457, 412, 1935, 512, 11275, 11, 512, 10270, 13, 50878, 50932, 1033, 30, 50974, 50974, 407, 382, 938, 382, 428, 10024, 2445, 11, 51070, 51070, 562, 291, 1715, 309, 11, 51120, 51120, 28111, 300, 264, 2281, 295, 264, 3006, 1867, 51240, 51240, 307, 4356, 813, 264, 2281, 51288, 51288, 295, 264, 881, 295, 5006, 18424, 1867, 51354, 51354, 538, 412, 1935, 512, 10270, 11, 51418, 51418, 2107, 12, 32226, 10270, 11, 51492, 51492, 550, 291, 434, 2489, 13, 51564, 51564, 2260, 4470, 2445, 307, 665, 13, 51722, 51820], "temperature": 0.0, "avg_logprob": -0.09236443787813187, "compression_ratio": 2.1203703703703702, "no_speech_prob": 1.892288855742663e-05}, {"id": 869, "seek": 513206, "start": 5147.18, "end": 5149.580000000001, "text": " ensures that the energy of the correct answer", "tokens": [50364, 498, 300, 2649, 307, 3671, 13, 50480, 50480, 682, 661, 2283, 11, 50522, 50522, 498, 264, 2281, 295, 264, 3006, 1867, 307, 4356, 50632, 50632, 813, 264, 2281, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50758, 50758, 457, 412, 1935, 512, 11275, 11, 512, 10270, 13, 50878, 50932, 1033, 30, 50974, 50974, 407, 382, 938, 382, 428, 10024, 2445, 11, 51070, 51070, 562, 291, 1715, 309, 11, 51120, 51120, 28111, 300, 264, 2281, 295, 264, 3006, 1867, 51240, 51240, 307, 4356, 813, 264, 2281, 51288, 51288, 295, 264, 881, 295, 5006, 18424, 1867, 51354, 51354, 538, 412, 1935, 512, 10270, 11, 51418, 51418, 2107, 12, 32226, 10270, 11, 51492, 51492, 550, 291, 434, 2489, 13, 51564, 51564, 2260, 4470, 2445, 307, 665, 13, 51722, 51820], "temperature": 0.0, "avg_logprob": -0.09236443787813187, "compression_ratio": 2.1203703703703702, "no_speech_prob": 1.892288855742663e-05}, {"id": 870, "seek": 513206, "start": 5149.580000000001, "end": 5150.54, "text": " is smaller than the energy", "tokens": [50364, 498, 300, 2649, 307, 3671, 13, 50480, 50480, 682, 661, 2283, 11, 50522, 50522, 498, 264, 2281, 295, 264, 3006, 1867, 307, 4356, 50632, 50632, 813, 264, 2281, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50758, 50758, 457, 412, 1935, 512, 11275, 11, 512, 10270, 13, 50878, 50932, 1033, 30, 50974, 50974, 407, 382, 938, 382, 428, 10024, 2445, 11, 51070, 51070, 562, 291, 1715, 309, 11, 51120, 51120, 28111, 300, 264, 2281, 295, 264, 3006, 1867, 51240, 51240, 307, 4356, 813, 264, 2281, 51288, 51288, 295, 264, 881, 295, 5006, 18424, 1867, 51354, 51354, 538, 412, 1935, 512, 10270, 11, 51418, 51418, 2107, 12, 32226, 10270, 11, 51492, 51492, 550, 291, 434, 2489, 13, 51564, 51564, 2260, 4470, 2445, 307, 665, 13, 51722, 51820], "temperature": 0.0, "avg_logprob": -0.09236443787813187, "compression_ratio": 2.1203703703703702, "no_speech_prob": 1.892288855742663e-05}, {"id": 871, "seek": 513206, "start": 5150.54, "end": 5151.860000000001, "text": " of the most of finding incorrect answer", "tokens": [50364, 498, 300, 2649, 307, 3671, 13, 50480, 50480, 682, 661, 2283, 11, 50522, 50522, 498, 264, 2281, 295, 264, 3006, 1867, 307, 4356, 50632, 50632, 813, 264, 2281, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50758, 50758, 457, 412, 1935, 512, 11275, 11, 512, 10270, 13, 50878, 50932, 1033, 30, 50974, 50974, 407, 382, 938, 382, 428, 10024, 2445, 11, 51070, 51070, 562, 291, 1715, 309, 11, 51120, 51120, 28111, 300, 264, 2281, 295, 264, 3006, 1867, 51240, 51240, 307, 4356, 813, 264, 2281, 51288, 51288, 295, 264, 881, 295, 5006, 18424, 1867, 51354, 51354, 538, 412, 1935, 512, 10270, 11, 51418, 51418, 2107, 12, 32226, 10270, 11, 51492, 51492, 550, 291, 434, 2489, 13, 51564, 51564, 2260, 4470, 2445, 307, 665, 13, 51722, 51820], "temperature": 0.0, "avg_logprob": -0.09236443787813187, "compression_ratio": 2.1203703703703702, "no_speech_prob": 1.892288855742663e-05}, {"id": 872, "seek": 513206, "start": 5151.860000000001, "end": 5153.14, "text": " by at least some margin,", "tokens": [50364, 498, 300, 2649, 307, 3671, 13, 50480, 50480, 682, 661, 2283, 11, 50522, 50522, 498, 264, 2281, 295, 264, 3006, 1867, 307, 4356, 50632, 50632, 813, 264, 2281, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50758, 50758, 457, 412, 1935, 512, 11275, 11, 512, 10270, 13, 50878, 50932, 1033, 30, 50974, 50974, 407, 382, 938, 382, 428, 10024, 2445, 11, 51070, 51070, 562, 291, 1715, 309, 11, 51120, 51120, 28111, 300, 264, 2281, 295, 264, 3006, 1867, 51240, 51240, 307, 4356, 813, 264, 2281, 51288, 51288, 295, 264, 881, 295, 5006, 18424, 1867, 51354, 51354, 538, 412, 1935, 512, 10270, 11, 51418, 51418, 2107, 12, 32226, 10270, 11, 51492, 51492, 550, 291, 434, 2489, 13, 51564, 51564, 2260, 4470, 2445, 307, 665, 13, 51722, 51820], "temperature": 0.0, "avg_logprob": -0.09236443787813187, "compression_ratio": 2.1203703703703702, "no_speech_prob": 1.892288855742663e-05}, {"id": 873, "seek": 513206, "start": 5153.14, "end": 5154.620000000001, "text": " non-zero margin,", "tokens": [50364, 498, 300, 2649, 307, 3671, 13, 50480, 50480, 682, 661, 2283, 11, 50522, 50522, 498, 264, 2281, 295, 264, 3006, 1867, 307, 4356, 50632, 50632, 813, 264, 2281, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50758, 50758, 457, 412, 1935, 512, 11275, 11, 512, 10270, 13, 50878, 50932, 1033, 30, 50974, 50974, 407, 382, 938, 382, 428, 10024, 2445, 11, 51070, 51070, 562, 291, 1715, 309, 11, 51120, 51120, 28111, 300, 264, 2281, 295, 264, 3006, 1867, 51240, 51240, 307, 4356, 813, 264, 2281, 51288, 51288, 295, 264, 881, 295, 5006, 18424, 1867, 51354, 51354, 538, 412, 1935, 512, 10270, 11, 51418, 51418, 2107, 12, 32226, 10270, 11, 51492, 51492, 550, 291, 434, 2489, 13, 51564, 51564, 2260, 4470, 2445, 307, 665, 13, 51722, 51820], "temperature": 0.0, "avg_logprob": -0.09236443787813187, "compression_ratio": 2.1203703703703702, "no_speech_prob": 1.892288855742663e-05}, {"id": 874, "seek": 513206, "start": 5154.620000000001, "end": 5156.06, "text": " then you're fine.", "tokens": [50364, 498, 300, 2649, 307, 3671, 13, 50480, 50480, 682, 661, 2283, 11, 50522, 50522, 498, 264, 2281, 295, 264, 3006, 1867, 307, 4356, 50632, 50632, 813, 264, 2281, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50758, 50758, 457, 412, 1935, 512, 11275, 11, 512, 10270, 13, 50878, 50932, 1033, 30, 50974, 50974, 407, 382, 938, 382, 428, 10024, 2445, 11, 51070, 51070, 562, 291, 1715, 309, 11, 51120, 51120, 28111, 300, 264, 2281, 295, 264, 3006, 1867, 51240, 51240, 307, 4356, 813, 264, 2281, 51288, 51288, 295, 264, 881, 295, 5006, 18424, 1867, 51354, 51354, 538, 412, 1935, 512, 10270, 11, 51418, 51418, 2107, 12, 32226, 10270, 11, 51492, 51492, 550, 291, 434, 2489, 13, 51564, 51564, 2260, 4470, 2445, 307, 665, 13, 51722, 51820], "temperature": 0.0, "avg_logprob": -0.09236443787813187, "compression_ratio": 2.1203703703703702, "no_speech_prob": 1.892288855742663e-05}, {"id": 875, "seek": 513206, "start": 5156.06, "end": 5159.22, "text": " Your loss function is good.", "tokens": [50364, 498, 300, 2649, 307, 3671, 13, 50480, 50480, 682, 661, 2283, 11, 50522, 50522, 498, 264, 2281, 295, 264, 3006, 1867, 307, 4356, 50632, 50632, 813, 264, 2281, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50758, 50758, 457, 412, 1935, 512, 11275, 11, 512, 10270, 13, 50878, 50932, 1033, 30, 50974, 50974, 407, 382, 938, 382, 428, 10024, 2445, 11, 51070, 51070, 562, 291, 1715, 309, 11, 51120, 51120, 28111, 300, 264, 2281, 295, 264, 3006, 1867, 51240, 51240, 307, 4356, 813, 264, 2281, 51288, 51288, 295, 264, 881, 295, 5006, 18424, 1867, 51354, 51354, 538, 412, 1935, 512, 10270, 11, 51418, 51418, 2107, 12, 32226, 10270, 11, 51492, 51492, 550, 291, 434, 2489, 13, 51564, 51564, 2260, 4470, 2445, 307, 665, 13, 51722, 51820], "temperature": 0.0, "avg_logprob": -0.09236443787813187, "compression_ratio": 2.1203703703703702, "no_speech_prob": 1.892288855742663e-05}, {"id": 876, "seek": 515922, "start": 5159.22, "end": 5160.06, "text": " Okay?", "tokens": [50364, 1033, 30, 50406, 50406, 407, 721, 411, 28822, 4470, 366, 665, 13, 50512, 50512, 440, 28822, 4470, 1936, 1619, 11, 50606, 50606, 293, 321, 2825, 466, 341, 445, 949, 11, 50754, 50754, 286, 528, 264, 2281, 295, 264, 3006, 1867, 50832, 50832, 281, 312, 4356, 813, 264, 2281, 50874, 50874, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50952, 50952, 597, 307, 1441, 23325, 398, 286, 2159, 510, 11, 51102, 51102, 538, 412, 1935, 376, 13, 51184, 51184, 1033, 30, 51226, 51226, 639, 307, 437, 341, 4470, 2445, 775, 13, 51268, 51268, 467, 311, 257, 28822, 4470, 13, 51348, 51348, 400, 309, 2738, 281, 2944, 760, 264, 2281, 295, 341, 2146, 51472, 51528, 2507, 264, 2281, 295, 300, 2146, 51598, 51598, 538, 412, 1935, 341, 10270, 13, 51668, 51762], "temperature": 0.0, "avg_logprob": -0.32721657789390507, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.3418363778328057e-05}, {"id": 877, "seek": 515922, "start": 5160.06, "end": 5162.18, "text": " So things like hinge loss are good.", "tokens": [50364, 1033, 30, 50406, 50406, 407, 721, 411, 28822, 4470, 366, 665, 13, 50512, 50512, 440, 28822, 4470, 1936, 1619, 11, 50606, 50606, 293, 321, 2825, 466, 341, 445, 949, 11, 50754, 50754, 286, 528, 264, 2281, 295, 264, 3006, 1867, 50832, 50832, 281, 312, 4356, 813, 264, 2281, 50874, 50874, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50952, 50952, 597, 307, 1441, 23325, 398, 286, 2159, 510, 11, 51102, 51102, 538, 412, 1935, 376, 13, 51184, 51184, 1033, 30, 51226, 51226, 639, 307, 437, 341, 4470, 2445, 775, 13, 51268, 51268, 467, 311, 257, 28822, 4470, 13, 51348, 51348, 400, 309, 2738, 281, 2944, 760, 264, 2281, 295, 341, 2146, 51472, 51528, 2507, 264, 2281, 295, 300, 2146, 51598, 51598, 538, 412, 1935, 341, 10270, 13, 51668, 51762], "temperature": 0.0, "avg_logprob": -0.32721657789390507, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.3418363778328057e-05}, {"id": 878, "seek": 515922, "start": 5162.18, "end": 5164.06, "text": " The hinge loss basically says,", "tokens": [50364, 1033, 30, 50406, 50406, 407, 721, 411, 28822, 4470, 366, 665, 13, 50512, 50512, 440, 28822, 4470, 1936, 1619, 11, 50606, 50606, 293, 321, 2825, 466, 341, 445, 949, 11, 50754, 50754, 286, 528, 264, 2281, 295, 264, 3006, 1867, 50832, 50832, 281, 312, 4356, 813, 264, 2281, 50874, 50874, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50952, 50952, 597, 307, 1441, 23325, 398, 286, 2159, 510, 11, 51102, 51102, 538, 412, 1935, 376, 13, 51184, 51184, 1033, 30, 51226, 51226, 639, 307, 437, 341, 4470, 2445, 775, 13, 51268, 51268, 467, 311, 257, 28822, 4470, 13, 51348, 51348, 400, 309, 2738, 281, 2944, 760, 264, 2281, 295, 341, 2146, 51472, 51528, 2507, 264, 2281, 295, 300, 2146, 51598, 51598, 538, 412, 1935, 341, 10270, 13, 51668, 51762], "temperature": 0.0, "avg_logprob": -0.32721657789390507, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.3418363778328057e-05}, {"id": 879, "seek": 515922, "start": 5164.06, "end": 5167.02, "text": " and we talked about this just before,", "tokens": [50364, 1033, 30, 50406, 50406, 407, 721, 411, 28822, 4470, 366, 665, 13, 50512, 50512, 440, 28822, 4470, 1936, 1619, 11, 50606, 50606, 293, 321, 2825, 466, 341, 445, 949, 11, 50754, 50754, 286, 528, 264, 2281, 295, 264, 3006, 1867, 50832, 50832, 281, 312, 4356, 813, 264, 2281, 50874, 50874, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50952, 50952, 597, 307, 1441, 23325, 398, 286, 2159, 510, 11, 51102, 51102, 538, 412, 1935, 376, 13, 51184, 51184, 1033, 30, 51226, 51226, 639, 307, 437, 341, 4470, 2445, 775, 13, 51268, 51268, 467, 311, 257, 28822, 4470, 13, 51348, 51348, 400, 309, 2738, 281, 2944, 760, 264, 2281, 295, 341, 2146, 51472, 51528, 2507, 264, 2281, 295, 300, 2146, 51598, 51598, 538, 412, 1935, 341, 10270, 13, 51668, 51762], "temperature": 0.0, "avg_logprob": -0.32721657789390507, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.3418363778328057e-05}, {"id": 880, "seek": 515922, "start": 5167.02, "end": 5168.58, "text": " I want the energy of the correct answer", "tokens": [50364, 1033, 30, 50406, 50406, 407, 721, 411, 28822, 4470, 366, 665, 13, 50512, 50512, 440, 28822, 4470, 1936, 1619, 11, 50606, 50606, 293, 321, 2825, 466, 341, 445, 949, 11, 50754, 50754, 286, 528, 264, 2281, 295, 264, 3006, 1867, 50832, 50832, 281, 312, 4356, 813, 264, 2281, 50874, 50874, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50952, 50952, 597, 307, 1441, 23325, 398, 286, 2159, 510, 11, 51102, 51102, 538, 412, 1935, 376, 13, 51184, 51184, 1033, 30, 51226, 51226, 639, 307, 437, 341, 4470, 2445, 775, 13, 51268, 51268, 467, 311, 257, 28822, 4470, 13, 51348, 51348, 400, 309, 2738, 281, 2944, 760, 264, 2281, 295, 341, 2146, 51472, 51528, 2507, 264, 2281, 295, 300, 2146, 51598, 51598, 538, 412, 1935, 341, 10270, 13, 51668, 51762], "temperature": 0.0, "avg_logprob": -0.32721657789390507, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.3418363778328057e-05}, {"id": 881, "seek": 515922, "start": 5168.58, "end": 5169.42, "text": " to be smaller than the energy", "tokens": [50364, 1033, 30, 50406, 50406, 407, 721, 411, 28822, 4470, 366, 665, 13, 50512, 50512, 440, 28822, 4470, 1936, 1619, 11, 50606, 50606, 293, 321, 2825, 466, 341, 445, 949, 11, 50754, 50754, 286, 528, 264, 2281, 295, 264, 3006, 1867, 50832, 50832, 281, 312, 4356, 813, 264, 2281, 50874, 50874, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50952, 50952, 597, 307, 1441, 23325, 398, 286, 2159, 510, 11, 51102, 51102, 538, 412, 1935, 376, 13, 51184, 51184, 1033, 30, 51226, 51226, 639, 307, 437, 341, 4470, 2445, 775, 13, 51268, 51268, 467, 311, 257, 28822, 4470, 13, 51348, 51348, 400, 309, 2738, 281, 2944, 760, 264, 2281, 295, 341, 2146, 51472, 51528, 2507, 264, 2281, 295, 300, 2146, 51598, 51598, 538, 412, 1935, 341, 10270, 13, 51668, 51762], "temperature": 0.0, "avg_logprob": -0.32721657789390507, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.3418363778328057e-05}, {"id": 882, "seek": 515922, "start": 5169.42, "end": 5170.9800000000005, "text": " of the most of finding incorrect answer,", "tokens": [50364, 1033, 30, 50406, 50406, 407, 721, 411, 28822, 4470, 366, 665, 13, 50512, 50512, 440, 28822, 4470, 1936, 1619, 11, 50606, 50606, 293, 321, 2825, 466, 341, 445, 949, 11, 50754, 50754, 286, 528, 264, 2281, 295, 264, 3006, 1867, 50832, 50832, 281, 312, 4356, 813, 264, 2281, 50874, 50874, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50952, 50952, 597, 307, 1441, 23325, 398, 286, 2159, 510, 11, 51102, 51102, 538, 412, 1935, 376, 13, 51184, 51184, 1033, 30, 51226, 51226, 639, 307, 437, 341, 4470, 2445, 775, 13, 51268, 51268, 467, 311, 257, 28822, 4470, 13, 51348, 51348, 400, 309, 2738, 281, 2944, 760, 264, 2281, 295, 341, 2146, 51472, 51528, 2507, 264, 2281, 295, 300, 2146, 51598, 51598, 538, 412, 1935, 341, 10270, 13, 51668, 51762], "temperature": 0.0, "avg_logprob": -0.32721657789390507, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.3418363778328057e-05}, {"id": 883, "seek": 515922, "start": 5170.9800000000005, "end": 5173.9800000000005, "text": " which is denoted Y I bar here,", "tokens": [50364, 1033, 30, 50406, 50406, 407, 721, 411, 28822, 4470, 366, 665, 13, 50512, 50512, 440, 28822, 4470, 1936, 1619, 11, 50606, 50606, 293, 321, 2825, 466, 341, 445, 949, 11, 50754, 50754, 286, 528, 264, 2281, 295, 264, 3006, 1867, 50832, 50832, 281, 312, 4356, 813, 264, 2281, 50874, 50874, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50952, 50952, 597, 307, 1441, 23325, 398, 286, 2159, 510, 11, 51102, 51102, 538, 412, 1935, 376, 13, 51184, 51184, 1033, 30, 51226, 51226, 639, 307, 437, 341, 4470, 2445, 775, 13, 51268, 51268, 467, 311, 257, 28822, 4470, 13, 51348, 51348, 400, 309, 2738, 281, 2944, 760, 264, 2281, 295, 341, 2146, 51472, 51528, 2507, 264, 2281, 295, 300, 2146, 51598, 51598, 538, 412, 1935, 341, 10270, 13, 51668, 51762], "temperature": 0.0, "avg_logprob": -0.32721657789390507, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.3418363778328057e-05}, {"id": 884, "seek": 515922, "start": 5173.9800000000005, "end": 5175.62, "text": " by at least M.", "tokens": [50364, 1033, 30, 50406, 50406, 407, 721, 411, 28822, 4470, 366, 665, 13, 50512, 50512, 440, 28822, 4470, 1936, 1619, 11, 50606, 50606, 293, 321, 2825, 466, 341, 445, 949, 11, 50754, 50754, 286, 528, 264, 2281, 295, 264, 3006, 1867, 50832, 50832, 281, 312, 4356, 813, 264, 2281, 50874, 50874, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50952, 50952, 597, 307, 1441, 23325, 398, 286, 2159, 510, 11, 51102, 51102, 538, 412, 1935, 376, 13, 51184, 51184, 1033, 30, 51226, 51226, 639, 307, 437, 341, 4470, 2445, 775, 13, 51268, 51268, 467, 311, 257, 28822, 4470, 13, 51348, 51348, 400, 309, 2738, 281, 2944, 760, 264, 2281, 295, 341, 2146, 51472, 51528, 2507, 264, 2281, 295, 300, 2146, 51598, 51598, 538, 412, 1935, 341, 10270, 13, 51668, 51762], "temperature": 0.0, "avg_logprob": -0.32721657789390507, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.3418363778328057e-05}, {"id": 885, "seek": 515922, "start": 5175.62, "end": 5176.46, "text": " Okay?", "tokens": [50364, 1033, 30, 50406, 50406, 407, 721, 411, 28822, 4470, 366, 665, 13, 50512, 50512, 440, 28822, 4470, 1936, 1619, 11, 50606, 50606, 293, 321, 2825, 466, 341, 445, 949, 11, 50754, 50754, 286, 528, 264, 2281, 295, 264, 3006, 1867, 50832, 50832, 281, 312, 4356, 813, 264, 2281, 50874, 50874, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50952, 50952, 597, 307, 1441, 23325, 398, 286, 2159, 510, 11, 51102, 51102, 538, 412, 1935, 376, 13, 51184, 51184, 1033, 30, 51226, 51226, 639, 307, 437, 341, 4470, 2445, 775, 13, 51268, 51268, 467, 311, 257, 28822, 4470, 13, 51348, 51348, 400, 309, 2738, 281, 2944, 760, 264, 2281, 295, 341, 2146, 51472, 51528, 2507, 264, 2281, 295, 300, 2146, 51598, 51598, 538, 412, 1935, 341, 10270, 13, 51668, 51762], "temperature": 0.0, "avg_logprob": -0.32721657789390507, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.3418363778328057e-05}, {"id": 886, "seek": 515922, "start": 5176.46, "end": 5177.3, "text": " This is what this loss function does.", "tokens": [50364, 1033, 30, 50406, 50406, 407, 721, 411, 28822, 4470, 366, 665, 13, 50512, 50512, 440, 28822, 4470, 1936, 1619, 11, 50606, 50606, 293, 321, 2825, 466, 341, 445, 949, 11, 50754, 50754, 286, 528, 264, 2281, 295, 264, 3006, 1867, 50832, 50832, 281, 312, 4356, 813, 264, 2281, 50874, 50874, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50952, 50952, 597, 307, 1441, 23325, 398, 286, 2159, 510, 11, 51102, 51102, 538, 412, 1935, 376, 13, 51184, 51184, 1033, 30, 51226, 51226, 639, 307, 437, 341, 4470, 2445, 775, 13, 51268, 51268, 467, 311, 257, 28822, 4470, 13, 51348, 51348, 400, 309, 2738, 281, 2944, 760, 264, 2281, 295, 341, 2146, 51472, 51528, 2507, 264, 2281, 295, 300, 2146, 51598, 51598, 538, 412, 1935, 341, 10270, 13, 51668, 51762], "temperature": 0.0, "avg_logprob": -0.32721657789390507, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.3418363778328057e-05}, {"id": 887, "seek": 515922, "start": 5177.3, "end": 5178.900000000001, "text": " It's a hinge loss.", "tokens": [50364, 1033, 30, 50406, 50406, 407, 721, 411, 28822, 4470, 366, 665, 13, 50512, 50512, 440, 28822, 4470, 1936, 1619, 11, 50606, 50606, 293, 321, 2825, 466, 341, 445, 949, 11, 50754, 50754, 286, 528, 264, 2281, 295, 264, 3006, 1867, 50832, 50832, 281, 312, 4356, 813, 264, 2281, 50874, 50874, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50952, 50952, 597, 307, 1441, 23325, 398, 286, 2159, 510, 11, 51102, 51102, 538, 412, 1935, 376, 13, 51184, 51184, 1033, 30, 51226, 51226, 639, 307, 437, 341, 4470, 2445, 775, 13, 51268, 51268, 467, 311, 257, 28822, 4470, 13, 51348, 51348, 400, 309, 2738, 281, 2944, 760, 264, 2281, 295, 341, 2146, 51472, 51528, 2507, 264, 2281, 295, 300, 2146, 51598, 51598, 538, 412, 1935, 341, 10270, 13, 51668, 51762], "temperature": 0.0, "avg_logprob": -0.32721657789390507, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.3418363778328057e-05}, {"id": 888, "seek": 515922, "start": 5178.900000000001, "end": 5181.38, "text": " And it wants to push down the energy of this guy", "tokens": [50364, 1033, 30, 50406, 50406, 407, 721, 411, 28822, 4470, 366, 665, 13, 50512, 50512, 440, 28822, 4470, 1936, 1619, 11, 50606, 50606, 293, 321, 2825, 466, 341, 445, 949, 11, 50754, 50754, 286, 528, 264, 2281, 295, 264, 3006, 1867, 50832, 50832, 281, 312, 4356, 813, 264, 2281, 50874, 50874, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50952, 50952, 597, 307, 1441, 23325, 398, 286, 2159, 510, 11, 51102, 51102, 538, 412, 1935, 376, 13, 51184, 51184, 1033, 30, 51226, 51226, 639, 307, 437, 341, 4470, 2445, 775, 13, 51268, 51268, 467, 311, 257, 28822, 4470, 13, 51348, 51348, 400, 309, 2738, 281, 2944, 760, 264, 2281, 295, 341, 2146, 51472, 51528, 2507, 264, 2281, 295, 300, 2146, 51598, 51598, 538, 412, 1935, 341, 10270, 13, 51668, 51762], "temperature": 0.0, "avg_logprob": -0.32721657789390507, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.3418363778328057e-05}, {"id": 889, "seek": 515922, "start": 5182.5, "end": 5183.900000000001, "text": " below the energy of that guy", "tokens": [50364, 1033, 30, 50406, 50406, 407, 721, 411, 28822, 4470, 366, 665, 13, 50512, 50512, 440, 28822, 4470, 1936, 1619, 11, 50606, 50606, 293, 321, 2825, 466, 341, 445, 949, 11, 50754, 50754, 286, 528, 264, 2281, 295, 264, 3006, 1867, 50832, 50832, 281, 312, 4356, 813, 264, 2281, 50874, 50874, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50952, 50952, 597, 307, 1441, 23325, 398, 286, 2159, 510, 11, 51102, 51102, 538, 412, 1935, 376, 13, 51184, 51184, 1033, 30, 51226, 51226, 639, 307, 437, 341, 4470, 2445, 775, 13, 51268, 51268, 467, 311, 257, 28822, 4470, 13, 51348, 51348, 400, 309, 2738, 281, 2944, 760, 264, 2281, 295, 341, 2146, 51472, 51528, 2507, 264, 2281, 295, 300, 2146, 51598, 51598, 538, 412, 1935, 341, 10270, 13, 51668, 51762], "temperature": 0.0, "avg_logprob": -0.32721657789390507, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.3418363778328057e-05}, {"id": 890, "seek": 515922, "start": 5183.900000000001, "end": 5185.3, "text": " by at least this margin.", "tokens": [50364, 1033, 30, 50406, 50406, 407, 721, 411, 28822, 4470, 366, 665, 13, 50512, 50512, 440, 28822, 4470, 1936, 1619, 11, 50606, 50606, 293, 321, 2825, 466, 341, 445, 949, 11, 50754, 50754, 286, 528, 264, 2281, 295, 264, 3006, 1867, 50832, 50832, 281, 312, 4356, 813, 264, 2281, 50874, 50874, 295, 264, 881, 295, 5006, 18424, 1867, 11, 50952, 50952, 597, 307, 1441, 23325, 398, 286, 2159, 510, 11, 51102, 51102, 538, 412, 1935, 376, 13, 51184, 51184, 1033, 30, 51226, 51226, 639, 307, 437, 341, 4470, 2445, 775, 13, 51268, 51268, 467, 311, 257, 28822, 4470, 13, 51348, 51348, 400, 309, 2738, 281, 2944, 760, 264, 2281, 295, 341, 2146, 51472, 51528, 2507, 264, 2281, 295, 300, 2146, 51598, 51598, 538, 412, 1935, 341, 10270, 13, 51668, 51762], "temperature": 0.0, "avg_logprob": -0.32721657789390507, "compression_ratio": 1.7745901639344261, "no_speech_prob": 1.3418363778328057e-05}, {"id": 891, "seek": 518530, "start": 5185.3, "end": 5187.38, "text": " So this has a margin M,", "tokens": [50364, 407, 341, 575, 257, 10270, 376, 11, 50468, 50468, 293, 341, 486, 11, 50546, 50546, 498, 291, 3847, 257, 1185, 365, 341, 4470, 11, 50646, 50646, 309, 486, 11, 293, 309, 393, 1466, 264, 5633, 11, 50740, 50740, 309, 486, 1466, 264, 5633, 50806, 50806, 293, 1391, 5258, 264, 665, 6338, 13, 50946, 50998, 440, 28822, 4470, 11, 51050, 51050, 264, 2787, 28822, 4470, 11, 51106, 51106, 597, 307, 294, 264, 4319, 295, 2281, 295, 613, 5245, 11, 51244, 51244, 307, 12675, 341, 636, 13, 51322, 51322, 8537, 11, 2602, 295, 12919, 264, 2649, 51456, 51456, 1296, 264, 25737, 295, 264, 3006, 1867, 51536, 51536, 293, 264, 881, 295, 5006, 18424, 472, 11, 51626, 51626, 666, 264, 2281, 295, 264, 3006, 1867, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.4078474570447066, "compression_ratio": 1.8949579831932772, "no_speech_prob": 1.8341195755056106e-05}, {"id": 892, "seek": 518530, "start": 5187.38, "end": 5188.9400000000005, "text": " and this will,", "tokens": [50364, 407, 341, 575, 257, 10270, 376, 11, 50468, 50468, 293, 341, 486, 11, 50546, 50546, 498, 291, 3847, 257, 1185, 365, 341, 4470, 11, 50646, 50646, 309, 486, 11, 293, 309, 393, 1466, 264, 5633, 11, 50740, 50740, 309, 486, 1466, 264, 5633, 50806, 50806, 293, 1391, 5258, 264, 665, 6338, 13, 50946, 50998, 440, 28822, 4470, 11, 51050, 51050, 264, 2787, 28822, 4470, 11, 51106, 51106, 597, 307, 294, 264, 4319, 295, 2281, 295, 613, 5245, 11, 51244, 51244, 307, 12675, 341, 636, 13, 51322, 51322, 8537, 11, 2602, 295, 12919, 264, 2649, 51456, 51456, 1296, 264, 25737, 295, 264, 3006, 1867, 51536, 51536, 293, 264, 881, 295, 5006, 18424, 472, 11, 51626, 51626, 666, 264, 2281, 295, 264, 3006, 1867, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.4078474570447066, "compression_ratio": 1.8949579831932772, "no_speech_prob": 1.8341195755056106e-05}, {"id": 893, "seek": 518530, "start": 5188.9400000000005, "end": 5190.9400000000005, "text": " if you train a system with this loss,", "tokens": [50364, 407, 341, 575, 257, 10270, 376, 11, 50468, 50468, 293, 341, 486, 11, 50546, 50546, 498, 291, 3847, 257, 1185, 365, 341, 4470, 11, 50646, 50646, 309, 486, 11, 293, 309, 393, 1466, 264, 5633, 11, 50740, 50740, 309, 486, 1466, 264, 5633, 50806, 50806, 293, 1391, 5258, 264, 665, 6338, 13, 50946, 50998, 440, 28822, 4470, 11, 51050, 51050, 264, 2787, 28822, 4470, 11, 51106, 51106, 597, 307, 294, 264, 4319, 295, 2281, 295, 613, 5245, 11, 51244, 51244, 307, 12675, 341, 636, 13, 51322, 51322, 8537, 11, 2602, 295, 12919, 264, 2649, 51456, 51456, 1296, 264, 25737, 295, 264, 3006, 1867, 51536, 51536, 293, 264, 881, 295, 5006, 18424, 472, 11, 51626, 51626, 666, 264, 2281, 295, 264, 3006, 1867, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.4078474570447066, "compression_ratio": 1.8949579831932772, "no_speech_prob": 1.8341195755056106e-05}, {"id": 894, "seek": 518530, "start": 5190.9400000000005, "end": 5192.820000000001, "text": " it will, and it can learn the task,", "tokens": [50364, 407, 341, 575, 257, 10270, 376, 11, 50468, 50468, 293, 341, 486, 11, 50546, 50546, 498, 291, 3847, 257, 1185, 365, 341, 4470, 11, 50646, 50646, 309, 486, 11, 293, 309, 393, 1466, 264, 5633, 11, 50740, 50740, 309, 486, 1466, 264, 5633, 50806, 50806, 293, 1391, 5258, 264, 665, 6338, 13, 50946, 50998, 440, 28822, 4470, 11, 51050, 51050, 264, 2787, 28822, 4470, 11, 51106, 51106, 597, 307, 294, 264, 4319, 295, 2281, 295, 613, 5245, 11, 51244, 51244, 307, 12675, 341, 636, 13, 51322, 51322, 8537, 11, 2602, 295, 12919, 264, 2649, 51456, 51456, 1296, 264, 25737, 295, 264, 3006, 1867, 51536, 51536, 293, 264, 881, 295, 5006, 18424, 472, 11, 51626, 51626, 666, 264, 2281, 295, 264, 3006, 1867, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.4078474570447066, "compression_ratio": 1.8949579831932772, "no_speech_prob": 1.8341195755056106e-05}, {"id": 895, "seek": 518530, "start": 5192.820000000001, "end": 5194.14, "text": " it will learn the task", "tokens": [50364, 407, 341, 575, 257, 10270, 376, 11, 50468, 50468, 293, 341, 486, 11, 50546, 50546, 498, 291, 3847, 257, 1185, 365, 341, 4470, 11, 50646, 50646, 309, 486, 11, 293, 309, 393, 1466, 264, 5633, 11, 50740, 50740, 309, 486, 1466, 264, 5633, 50806, 50806, 293, 1391, 5258, 264, 665, 6338, 13, 50946, 50998, 440, 28822, 4470, 11, 51050, 51050, 264, 2787, 28822, 4470, 11, 51106, 51106, 597, 307, 294, 264, 4319, 295, 2281, 295, 613, 5245, 11, 51244, 51244, 307, 12675, 341, 636, 13, 51322, 51322, 8537, 11, 2602, 295, 12919, 264, 2649, 51456, 51456, 1296, 264, 25737, 295, 264, 3006, 1867, 51536, 51536, 293, 264, 881, 295, 5006, 18424, 472, 11, 51626, 51626, 666, 264, 2281, 295, 264, 3006, 1867, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.4078474570447066, "compression_ratio": 1.8949579831932772, "no_speech_prob": 1.8341195755056106e-05}, {"id": 896, "seek": 518530, "start": 5194.14, "end": 5196.9400000000005, "text": " and probably produce the good answers.", "tokens": [50364, 407, 341, 575, 257, 10270, 376, 11, 50468, 50468, 293, 341, 486, 11, 50546, 50546, 498, 291, 3847, 257, 1185, 365, 341, 4470, 11, 50646, 50646, 309, 486, 11, 293, 309, 393, 1466, 264, 5633, 11, 50740, 50740, 309, 486, 1466, 264, 5633, 50806, 50806, 293, 1391, 5258, 264, 665, 6338, 13, 50946, 50998, 440, 28822, 4470, 11, 51050, 51050, 264, 2787, 28822, 4470, 11, 51106, 51106, 597, 307, 294, 264, 4319, 295, 2281, 295, 613, 5245, 11, 51244, 51244, 307, 12675, 341, 636, 13, 51322, 51322, 8537, 11, 2602, 295, 12919, 264, 2649, 51456, 51456, 1296, 264, 25737, 295, 264, 3006, 1867, 51536, 51536, 293, 264, 881, 295, 5006, 18424, 472, 11, 51626, 51626, 666, 264, 2281, 295, 264, 3006, 1867, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.4078474570447066, "compression_ratio": 1.8949579831932772, "no_speech_prob": 1.8341195755056106e-05}, {"id": 897, "seek": 518530, "start": 5197.9800000000005, "end": 5199.02, "text": " The hinge loss,", "tokens": [50364, 407, 341, 575, 257, 10270, 376, 11, 50468, 50468, 293, 341, 486, 11, 50546, 50546, 498, 291, 3847, 257, 1185, 365, 341, 4470, 11, 50646, 50646, 309, 486, 11, 293, 309, 393, 1466, 264, 5633, 11, 50740, 50740, 309, 486, 1466, 264, 5633, 50806, 50806, 293, 1391, 5258, 264, 665, 6338, 13, 50946, 50998, 440, 28822, 4470, 11, 51050, 51050, 264, 2787, 28822, 4470, 11, 51106, 51106, 597, 307, 294, 264, 4319, 295, 2281, 295, 613, 5245, 11, 51244, 51244, 307, 12675, 341, 636, 13, 51322, 51322, 8537, 11, 2602, 295, 12919, 264, 2649, 51456, 51456, 1296, 264, 25737, 295, 264, 3006, 1867, 51536, 51536, 293, 264, 881, 295, 5006, 18424, 472, 11, 51626, 51626, 666, 264, 2281, 295, 264, 3006, 1867, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.4078474570447066, "compression_ratio": 1.8949579831932772, "no_speech_prob": 1.8341195755056106e-05}, {"id": 898, "seek": 518530, "start": 5199.02, "end": 5200.14, "text": " the soft hinge loss,", "tokens": [50364, 407, 341, 575, 257, 10270, 376, 11, 50468, 50468, 293, 341, 486, 11, 50546, 50546, 498, 291, 3847, 257, 1185, 365, 341, 4470, 11, 50646, 50646, 309, 486, 11, 293, 309, 393, 1466, 264, 5633, 11, 50740, 50740, 309, 486, 1466, 264, 5633, 50806, 50806, 293, 1391, 5258, 264, 665, 6338, 13, 50946, 50998, 440, 28822, 4470, 11, 51050, 51050, 264, 2787, 28822, 4470, 11, 51106, 51106, 597, 307, 294, 264, 4319, 295, 2281, 295, 613, 5245, 11, 51244, 51244, 307, 12675, 341, 636, 13, 51322, 51322, 8537, 11, 2602, 295, 12919, 264, 2649, 51456, 51456, 1296, 264, 25737, 295, 264, 3006, 1867, 51536, 51536, 293, 264, 881, 295, 5006, 18424, 472, 11, 51626, 51626, 666, 264, 2281, 295, 264, 3006, 1867, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.4078474570447066, "compression_ratio": 1.8949579831932772, "no_speech_prob": 1.8341195755056106e-05}, {"id": 899, "seek": 518530, "start": 5200.14, "end": 5202.900000000001, "text": " which is in the context of energy of these models,", "tokens": [50364, 407, 341, 575, 257, 10270, 376, 11, 50468, 50468, 293, 341, 486, 11, 50546, 50546, 498, 291, 3847, 257, 1185, 365, 341, 4470, 11, 50646, 50646, 309, 486, 11, 293, 309, 393, 1466, 264, 5633, 11, 50740, 50740, 309, 486, 1466, 264, 5633, 50806, 50806, 293, 1391, 5258, 264, 665, 6338, 13, 50946, 50998, 440, 28822, 4470, 11, 51050, 51050, 264, 2787, 28822, 4470, 11, 51106, 51106, 597, 307, 294, 264, 4319, 295, 2281, 295, 613, 5245, 11, 51244, 51244, 307, 12675, 341, 636, 13, 51322, 51322, 8537, 11, 2602, 295, 12919, 264, 2649, 51456, 51456, 1296, 264, 25737, 295, 264, 3006, 1867, 51536, 51536, 293, 264, 881, 295, 5006, 18424, 472, 11, 51626, 51626, 666, 264, 2281, 295, 264, 3006, 1867, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.4078474570447066, "compression_ratio": 1.8949579831932772, "no_speech_prob": 1.8341195755056106e-05}, {"id": 900, "seek": 518530, "start": 5202.900000000001, "end": 5204.46, "text": " is expressed this way.", "tokens": [50364, 407, 341, 575, 257, 10270, 376, 11, 50468, 50468, 293, 341, 486, 11, 50546, 50546, 498, 291, 3847, 257, 1185, 365, 341, 4470, 11, 50646, 50646, 309, 486, 11, 293, 309, 393, 1466, 264, 5633, 11, 50740, 50740, 309, 486, 1466, 264, 5633, 50806, 50806, 293, 1391, 5258, 264, 665, 6338, 13, 50946, 50998, 440, 28822, 4470, 11, 51050, 51050, 264, 2787, 28822, 4470, 11, 51106, 51106, 597, 307, 294, 264, 4319, 295, 2281, 295, 613, 5245, 11, 51244, 51244, 307, 12675, 341, 636, 13, 51322, 51322, 8537, 11, 2602, 295, 12919, 264, 2649, 51456, 51456, 1296, 264, 25737, 295, 264, 3006, 1867, 51536, 51536, 293, 264, 881, 295, 5006, 18424, 472, 11, 51626, 51626, 666, 264, 2281, 295, 264, 3006, 1867, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.4078474570447066, "compression_ratio": 1.8949579831932772, "no_speech_prob": 1.8341195755056106e-05}, {"id": 901, "seek": 518530, "start": 5204.46, "end": 5207.14, "text": " Basically, instead of feeding the difference", "tokens": [50364, 407, 341, 575, 257, 10270, 376, 11, 50468, 50468, 293, 341, 486, 11, 50546, 50546, 498, 291, 3847, 257, 1185, 365, 341, 4470, 11, 50646, 50646, 309, 486, 11, 293, 309, 393, 1466, 264, 5633, 11, 50740, 50740, 309, 486, 1466, 264, 5633, 50806, 50806, 293, 1391, 5258, 264, 665, 6338, 13, 50946, 50998, 440, 28822, 4470, 11, 51050, 51050, 264, 2787, 28822, 4470, 11, 51106, 51106, 597, 307, 294, 264, 4319, 295, 2281, 295, 613, 5245, 11, 51244, 51244, 307, 12675, 341, 636, 13, 51322, 51322, 8537, 11, 2602, 295, 12919, 264, 2649, 51456, 51456, 1296, 264, 25737, 295, 264, 3006, 1867, 51536, 51536, 293, 264, 881, 295, 5006, 18424, 472, 11, 51626, 51626, 666, 264, 2281, 295, 264, 3006, 1867, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.4078474570447066, "compression_ratio": 1.8949579831932772, "no_speech_prob": 1.8341195755056106e-05}, {"id": 902, "seek": 518530, "start": 5207.14, "end": 5208.74, "text": " between the energies of the correct answer", "tokens": [50364, 407, 341, 575, 257, 10270, 376, 11, 50468, 50468, 293, 341, 486, 11, 50546, 50546, 498, 291, 3847, 257, 1185, 365, 341, 4470, 11, 50646, 50646, 309, 486, 11, 293, 309, 393, 1466, 264, 5633, 11, 50740, 50740, 309, 486, 1466, 264, 5633, 50806, 50806, 293, 1391, 5258, 264, 665, 6338, 13, 50946, 50998, 440, 28822, 4470, 11, 51050, 51050, 264, 2787, 28822, 4470, 11, 51106, 51106, 597, 307, 294, 264, 4319, 295, 2281, 295, 613, 5245, 11, 51244, 51244, 307, 12675, 341, 636, 13, 51322, 51322, 8537, 11, 2602, 295, 12919, 264, 2649, 51456, 51456, 1296, 264, 25737, 295, 264, 3006, 1867, 51536, 51536, 293, 264, 881, 295, 5006, 18424, 472, 11, 51626, 51626, 666, 264, 2281, 295, 264, 3006, 1867, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.4078474570447066, "compression_ratio": 1.8949579831932772, "no_speech_prob": 1.8341195755056106e-05}, {"id": 903, "seek": 518530, "start": 5208.74, "end": 5210.54, "text": " and the most of finding incorrect one,", "tokens": [50364, 407, 341, 575, 257, 10270, 376, 11, 50468, 50468, 293, 341, 486, 11, 50546, 50546, 498, 291, 3847, 257, 1185, 365, 341, 4470, 11, 50646, 50646, 309, 486, 11, 293, 309, 393, 1466, 264, 5633, 11, 50740, 50740, 309, 486, 1466, 264, 5633, 50806, 50806, 293, 1391, 5258, 264, 665, 6338, 13, 50946, 50998, 440, 28822, 4470, 11, 51050, 51050, 264, 2787, 28822, 4470, 11, 51106, 51106, 597, 307, 294, 264, 4319, 295, 2281, 295, 613, 5245, 11, 51244, 51244, 307, 12675, 341, 636, 13, 51322, 51322, 8537, 11, 2602, 295, 12919, 264, 2649, 51456, 51456, 1296, 264, 25737, 295, 264, 3006, 1867, 51536, 51536, 293, 264, 881, 295, 5006, 18424, 472, 11, 51626, 51626, 666, 264, 2281, 295, 264, 3006, 1867, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.4078474570447066, "compression_ratio": 1.8949579831932772, "no_speech_prob": 1.8341195755056106e-05}, {"id": 904, "seek": 518530, "start": 5210.54, "end": 5212.58, "text": " into the energy of the correct answer,", "tokens": [50364, 407, 341, 575, 257, 10270, 376, 11, 50468, 50468, 293, 341, 486, 11, 50546, 50546, 498, 291, 3847, 257, 1185, 365, 341, 4470, 11, 50646, 50646, 309, 486, 11, 293, 309, 393, 1466, 264, 5633, 11, 50740, 50740, 309, 486, 1466, 264, 5633, 50806, 50806, 293, 1391, 5258, 264, 665, 6338, 13, 50946, 50998, 440, 28822, 4470, 11, 51050, 51050, 264, 2787, 28822, 4470, 11, 51106, 51106, 597, 307, 294, 264, 4319, 295, 2281, 295, 613, 5245, 11, 51244, 51244, 307, 12675, 341, 636, 13, 51322, 51322, 8537, 11, 2602, 295, 12919, 264, 2649, 51456, 51456, 1296, 264, 25737, 295, 264, 3006, 1867, 51536, 51536, 293, 264, 881, 295, 5006, 18424, 472, 11, 51626, 51626, 666, 264, 2281, 295, 264, 3006, 1867, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.4078474570447066, "compression_ratio": 1.8949579831932772, "no_speech_prob": 1.8341195755056106e-05}, {"id": 905, "seek": 521258, "start": 5212.58, "end": 5216.3, "text": " and the most of finding incorrect one into a hinge,", "tokens": [50364, 293, 264, 881, 295, 5006, 18424, 472, 666, 257, 28822, 11, 50550, 50550, 309, 23712, 309, 281, 257, 2787, 28822, 11, 50708, 50708, 597, 321, 2825, 466, 445, 257, 1326, 2077, 2057, 13, 50876, 50958, 400, 456, 11, 341, 472, 611, 575, 257, 10270, 13, 51064, 51064, 440, 10270, 307, 819, 13, 51106, 51106, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51220, 51220, 6463, 797, 30, 51262, 51262, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51332, 51436, 467, 311, 23211, 13, 51478, 51532, 509, 393, 992, 376, 281, 472, 13, 51632, 51632, 509, 393, 992, 376, 281, 472, 12, 83, 17966, 13, 51732, 51732, 286, 914, 11, 309, 311, 733, 295, 23211, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.3294947624206543, "compression_ratio": 1.7302325581395348, "no_speech_prob": 4.2226811274304055e-06}, {"id": 906, "seek": 521258, "start": 5216.3, "end": 5219.46, "text": " it feeds it to a soft hinge,", "tokens": [50364, 293, 264, 881, 295, 5006, 18424, 472, 666, 257, 28822, 11, 50550, 50550, 309, 23712, 309, 281, 257, 2787, 28822, 11, 50708, 50708, 597, 321, 2825, 466, 445, 257, 1326, 2077, 2057, 13, 50876, 50958, 400, 456, 11, 341, 472, 611, 575, 257, 10270, 13, 51064, 51064, 440, 10270, 307, 819, 13, 51106, 51106, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51220, 51220, 6463, 797, 30, 51262, 51262, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51332, 51436, 467, 311, 23211, 13, 51478, 51532, 509, 393, 992, 376, 281, 472, 13, 51632, 51632, 509, 393, 992, 376, 281, 472, 12, 83, 17966, 13, 51732, 51732, 286, 914, 11, 309, 311, 733, 295, 23211, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.3294947624206543, "compression_ratio": 1.7302325581395348, "no_speech_prob": 4.2226811274304055e-06}, {"id": 907, "seek": 521258, "start": 5219.46, "end": 5222.82, "text": " which we talked about just a few minutes ago.", "tokens": [50364, 293, 264, 881, 295, 5006, 18424, 472, 666, 257, 28822, 11, 50550, 50550, 309, 23712, 309, 281, 257, 2787, 28822, 11, 50708, 50708, 597, 321, 2825, 466, 445, 257, 1326, 2077, 2057, 13, 50876, 50958, 400, 456, 11, 341, 472, 611, 575, 257, 10270, 13, 51064, 51064, 440, 10270, 307, 819, 13, 51106, 51106, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51220, 51220, 6463, 797, 30, 51262, 51262, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51332, 51436, 467, 311, 23211, 13, 51478, 51532, 509, 393, 992, 376, 281, 472, 13, 51632, 51632, 509, 393, 992, 376, 281, 472, 12, 83, 17966, 13, 51732, 51732, 286, 914, 11, 309, 311, 733, 295, 23211, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.3294947624206543, "compression_ratio": 1.7302325581395348, "no_speech_prob": 4.2226811274304055e-06}, {"id": 908, "seek": 521258, "start": 5224.46, "end": 5226.58, "text": " And there, this one also has a margin.", "tokens": [50364, 293, 264, 881, 295, 5006, 18424, 472, 666, 257, 28822, 11, 50550, 50550, 309, 23712, 309, 281, 257, 2787, 28822, 11, 50708, 50708, 597, 321, 2825, 466, 445, 257, 1326, 2077, 2057, 13, 50876, 50958, 400, 456, 11, 341, 472, 611, 575, 257, 10270, 13, 51064, 51064, 440, 10270, 307, 819, 13, 51106, 51106, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51220, 51220, 6463, 797, 30, 51262, 51262, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51332, 51436, 467, 311, 23211, 13, 51478, 51532, 509, 393, 992, 376, 281, 472, 13, 51632, 51632, 509, 393, 992, 376, 281, 472, 12, 83, 17966, 13, 51732, 51732, 286, 914, 11, 309, 311, 733, 295, 23211, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.3294947624206543, "compression_ratio": 1.7302325581395348, "no_speech_prob": 4.2226811274304055e-06}, {"id": 909, "seek": 521258, "start": 5226.58, "end": 5227.42, "text": " The margin is different.", "tokens": [50364, 293, 264, 881, 295, 5006, 18424, 472, 666, 257, 28822, 11, 50550, 50550, 309, 23712, 309, 281, 257, 2787, 28822, 11, 50708, 50708, 597, 321, 2825, 466, 445, 257, 1326, 2077, 2057, 13, 50876, 50958, 400, 456, 11, 341, 472, 611, 575, 257, 10270, 13, 51064, 51064, 440, 10270, 307, 819, 13, 51106, 51106, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51220, 51220, 6463, 797, 30, 51262, 51262, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51332, 51436, 467, 311, 23211, 13, 51478, 51532, 509, 393, 992, 376, 281, 472, 13, 51632, 51632, 509, 393, 992, 376, 281, 472, 12, 83, 17966, 13, 51732, 51732, 286, 914, 11, 309, 311, 733, 295, 23211, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.3294947624206543, "compression_ratio": 1.7302325581395348, "no_speech_prob": 4.2226811274304055e-06}, {"id": 910, "seek": 521258, "start": 5227.42, "end": 5229.7, "text": " The question would be how to pick M?", "tokens": [50364, 293, 264, 881, 295, 5006, 18424, 472, 666, 257, 28822, 11, 50550, 50550, 309, 23712, 309, 281, 257, 2787, 28822, 11, 50708, 50708, 597, 321, 2825, 466, 445, 257, 1326, 2077, 2057, 13, 50876, 50958, 400, 456, 11, 341, 472, 611, 575, 257, 10270, 13, 51064, 51064, 440, 10270, 307, 819, 13, 51106, 51106, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51220, 51220, 6463, 797, 30, 51262, 51262, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51332, 51436, 467, 311, 23211, 13, 51478, 51532, 509, 393, 992, 376, 281, 472, 13, 51632, 51632, 509, 393, 992, 376, 281, 472, 12, 83, 17966, 13, 51732, 51732, 286, 914, 11, 309, 311, 733, 295, 23211, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.3294947624206543, "compression_ratio": 1.7302325581395348, "no_speech_prob": 4.2226811274304055e-06}, {"id": 911, "seek": 521258, "start": 5229.7, "end": 5230.54, "text": " Say again?", "tokens": [50364, 293, 264, 881, 295, 5006, 18424, 472, 666, 257, 28822, 11, 50550, 50550, 309, 23712, 309, 281, 257, 2787, 28822, 11, 50708, 50708, 597, 321, 2825, 466, 445, 257, 1326, 2077, 2057, 13, 50876, 50958, 400, 456, 11, 341, 472, 611, 575, 257, 10270, 13, 51064, 51064, 440, 10270, 307, 819, 13, 51106, 51106, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51220, 51220, 6463, 797, 30, 51262, 51262, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51332, 51436, 467, 311, 23211, 13, 51478, 51532, 509, 393, 992, 376, 281, 472, 13, 51632, 51632, 509, 393, 992, 376, 281, 472, 12, 83, 17966, 13, 51732, 51732, 286, 914, 11, 309, 311, 733, 295, 23211, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.3294947624206543, "compression_ratio": 1.7302325581395348, "no_speech_prob": 4.2226811274304055e-06}, {"id": 912, "seek": 521258, "start": 5230.54, "end": 5231.94, "text": " The question would be how to pick M?", "tokens": [50364, 293, 264, 881, 295, 5006, 18424, 472, 666, 257, 28822, 11, 50550, 50550, 309, 23712, 309, 281, 257, 2787, 28822, 11, 50708, 50708, 597, 321, 2825, 466, 445, 257, 1326, 2077, 2057, 13, 50876, 50958, 400, 456, 11, 341, 472, 611, 575, 257, 10270, 13, 51064, 51064, 440, 10270, 307, 819, 13, 51106, 51106, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51220, 51220, 6463, 797, 30, 51262, 51262, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51332, 51436, 467, 311, 23211, 13, 51478, 51532, 509, 393, 992, 376, 281, 472, 13, 51632, 51632, 509, 393, 992, 376, 281, 472, 12, 83, 17966, 13, 51732, 51732, 286, 914, 11, 309, 311, 733, 295, 23211, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.3294947624206543, "compression_ratio": 1.7302325581395348, "no_speech_prob": 4.2226811274304055e-06}, {"id": 913, "seek": 521258, "start": 5234.0199999999995, "end": 5234.86, "text": " It's arbitrary.", "tokens": [50364, 293, 264, 881, 295, 5006, 18424, 472, 666, 257, 28822, 11, 50550, 50550, 309, 23712, 309, 281, 257, 2787, 28822, 11, 50708, 50708, 597, 321, 2825, 466, 445, 257, 1326, 2077, 2057, 13, 50876, 50958, 400, 456, 11, 341, 472, 611, 575, 257, 10270, 13, 51064, 51064, 440, 10270, 307, 819, 13, 51106, 51106, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51220, 51220, 6463, 797, 30, 51262, 51262, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51332, 51436, 467, 311, 23211, 13, 51478, 51532, 509, 393, 992, 376, 281, 472, 13, 51632, 51632, 509, 393, 992, 376, 281, 472, 12, 83, 17966, 13, 51732, 51732, 286, 914, 11, 309, 311, 733, 295, 23211, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.3294947624206543, "compression_ratio": 1.7302325581395348, "no_speech_prob": 4.2226811274304055e-06}, {"id": 914, "seek": 521258, "start": 5235.94, "end": 5237.94, "text": " You can set M to one.", "tokens": [50364, 293, 264, 881, 295, 5006, 18424, 472, 666, 257, 28822, 11, 50550, 50550, 309, 23712, 309, 281, 257, 2787, 28822, 11, 50708, 50708, 597, 321, 2825, 466, 445, 257, 1326, 2077, 2057, 13, 50876, 50958, 400, 456, 11, 341, 472, 611, 575, 257, 10270, 13, 51064, 51064, 440, 10270, 307, 819, 13, 51106, 51106, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51220, 51220, 6463, 797, 30, 51262, 51262, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51332, 51436, 467, 311, 23211, 13, 51478, 51532, 509, 393, 992, 376, 281, 472, 13, 51632, 51632, 509, 393, 992, 376, 281, 472, 12, 83, 17966, 13, 51732, 51732, 286, 914, 11, 309, 311, 733, 295, 23211, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.3294947624206543, "compression_ratio": 1.7302325581395348, "no_speech_prob": 4.2226811274304055e-06}, {"id": 915, "seek": 521258, "start": 5237.94, "end": 5239.94, "text": " You can set M to one-tenth.", "tokens": [50364, 293, 264, 881, 295, 5006, 18424, 472, 666, 257, 28822, 11, 50550, 50550, 309, 23712, 309, 281, 257, 2787, 28822, 11, 50708, 50708, 597, 321, 2825, 466, 445, 257, 1326, 2077, 2057, 13, 50876, 50958, 400, 456, 11, 341, 472, 611, 575, 257, 10270, 13, 51064, 51064, 440, 10270, 307, 819, 13, 51106, 51106, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51220, 51220, 6463, 797, 30, 51262, 51262, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51332, 51436, 467, 311, 23211, 13, 51478, 51532, 509, 393, 992, 376, 281, 472, 13, 51632, 51632, 509, 393, 992, 376, 281, 472, 12, 83, 17966, 13, 51732, 51732, 286, 914, 11, 309, 311, 733, 295, 23211, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.3294947624206543, "compression_ratio": 1.7302325581395348, "no_speech_prob": 4.2226811274304055e-06}, {"id": 916, "seek": 521258, "start": 5239.94, "end": 5241.26, "text": " I mean, it's kind of arbitrary", "tokens": [50364, 293, 264, 881, 295, 5006, 18424, 472, 666, 257, 28822, 11, 50550, 50550, 309, 23712, 309, 281, 257, 2787, 28822, 11, 50708, 50708, 597, 321, 2825, 466, 445, 257, 1326, 2077, 2057, 13, 50876, 50958, 400, 456, 11, 341, 472, 611, 575, 257, 10270, 13, 51064, 51064, 440, 10270, 307, 819, 13, 51106, 51106, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51220, 51220, 6463, 797, 30, 51262, 51262, 440, 1168, 576, 312, 577, 281, 1888, 376, 30, 51332, 51436, 467, 311, 23211, 13, 51478, 51532, 509, 393, 992, 376, 281, 472, 13, 51632, 51632, 509, 393, 992, 376, 281, 472, 12, 83, 17966, 13, 51732, 51732, 286, 914, 11, 309, 311, 733, 295, 23211, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.3294947624206543, "compression_ratio": 1.7302325581395348, "no_speech_prob": 4.2226811274304055e-06}, {"id": 917, "seek": 524126, "start": 5241.26, "end": 5243.34, "text": " because it will just determine the size", "tokens": [50364, 570, 309, 486, 445, 6997, 264, 2744, 50468, 50468, 295, 264, 17443, 295, 428, 1036, 4583, 13, 50546, 50546, 663, 311, 439, 309, 775, 13, 50588, 50588, 1033, 13, 50630, 50792, 407, 309, 311, 1936, 493, 281, 291, 13, 50856, 51022, 865, 11, 370, 264, 2787, 28822, 4470, 575, 364, 13785, 10270, 13, 51170, 51170, 467, 2738, 264, 2649, 1296, 729, 732, 25737, 51258, 51258, 281, 312, 13785, 11, 51306, 51306, 457, 264, 12403, 1333, 295, 24108, 37330, 11, 51458, 51458, 370, 309, 311, 1128, 799, 483, 456, 51544, 51544, 570, 264, 2771, 2448, 483, 588, 1359, 51766, 51766, 382, 264, 2649, 8637, 13, 51836], "temperature": 0.0, "avg_logprob": -0.273004014915395, "compression_ratio": 1.6307053941908713, "no_speech_prob": 1.07818732431042e-05}, {"id": 918, "seek": 524126, "start": 5243.34, "end": 5244.900000000001, "text": " of the weights of your last layer.", "tokens": [50364, 570, 309, 486, 445, 6997, 264, 2744, 50468, 50468, 295, 264, 17443, 295, 428, 1036, 4583, 13, 50546, 50546, 663, 311, 439, 309, 775, 13, 50588, 50588, 1033, 13, 50630, 50792, 407, 309, 311, 1936, 493, 281, 291, 13, 50856, 51022, 865, 11, 370, 264, 2787, 28822, 4470, 575, 364, 13785, 10270, 13, 51170, 51170, 467, 2738, 264, 2649, 1296, 729, 732, 25737, 51258, 51258, 281, 312, 13785, 11, 51306, 51306, 457, 264, 12403, 1333, 295, 24108, 37330, 11, 51458, 51458, 370, 309, 311, 1128, 799, 483, 456, 51544, 51544, 570, 264, 2771, 2448, 483, 588, 1359, 51766, 51766, 382, 264, 2649, 8637, 13, 51836], "temperature": 0.0, "avg_logprob": -0.273004014915395, "compression_ratio": 1.6307053941908713, "no_speech_prob": 1.07818732431042e-05}, {"id": 919, "seek": 524126, "start": 5244.900000000001, "end": 5245.74, "text": " That's all it does.", "tokens": [50364, 570, 309, 486, 445, 6997, 264, 2744, 50468, 50468, 295, 264, 17443, 295, 428, 1036, 4583, 13, 50546, 50546, 663, 311, 439, 309, 775, 13, 50588, 50588, 1033, 13, 50630, 50792, 407, 309, 311, 1936, 493, 281, 291, 13, 50856, 51022, 865, 11, 370, 264, 2787, 28822, 4470, 575, 364, 13785, 10270, 13, 51170, 51170, 467, 2738, 264, 2649, 1296, 729, 732, 25737, 51258, 51258, 281, 312, 13785, 11, 51306, 51306, 457, 264, 12403, 1333, 295, 24108, 37330, 11, 51458, 51458, 370, 309, 311, 1128, 799, 483, 456, 51544, 51544, 570, 264, 2771, 2448, 483, 588, 1359, 51766, 51766, 382, 264, 2649, 8637, 13, 51836], "temperature": 0.0, "avg_logprob": -0.273004014915395, "compression_ratio": 1.6307053941908713, "no_speech_prob": 1.07818732431042e-05}, {"id": 920, "seek": 524126, "start": 5245.74, "end": 5246.58, "text": " Okay.", "tokens": [50364, 570, 309, 486, 445, 6997, 264, 2744, 50468, 50468, 295, 264, 17443, 295, 428, 1036, 4583, 13, 50546, 50546, 663, 311, 439, 309, 775, 13, 50588, 50588, 1033, 13, 50630, 50792, 407, 309, 311, 1936, 493, 281, 291, 13, 50856, 51022, 865, 11, 370, 264, 2787, 28822, 4470, 575, 364, 13785, 10270, 13, 51170, 51170, 467, 2738, 264, 2649, 1296, 729, 732, 25737, 51258, 51258, 281, 312, 13785, 11, 51306, 51306, 457, 264, 12403, 1333, 295, 24108, 37330, 11, 51458, 51458, 370, 309, 311, 1128, 799, 483, 456, 51544, 51544, 570, 264, 2771, 2448, 483, 588, 1359, 51766, 51766, 382, 264, 2649, 8637, 13, 51836], "temperature": 0.0, "avg_logprob": -0.273004014915395, "compression_ratio": 1.6307053941908713, "no_speech_prob": 1.07818732431042e-05}, {"id": 921, "seek": 524126, "start": 5249.820000000001, "end": 5251.1, "text": " So it's basically up to you.", "tokens": [50364, 570, 309, 486, 445, 6997, 264, 2744, 50468, 50468, 295, 264, 17443, 295, 428, 1036, 4583, 13, 50546, 50546, 663, 311, 439, 309, 775, 13, 50588, 50588, 1033, 13, 50630, 50792, 407, 309, 311, 1936, 493, 281, 291, 13, 50856, 51022, 865, 11, 370, 264, 2787, 28822, 4470, 575, 364, 13785, 10270, 13, 51170, 51170, 467, 2738, 264, 2649, 1296, 729, 732, 25737, 51258, 51258, 281, 312, 13785, 11, 51306, 51306, 457, 264, 12403, 1333, 295, 24108, 37330, 11, 51458, 51458, 370, 309, 311, 1128, 799, 483, 456, 51544, 51544, 570, 264, 2771, 2448, 483, 588, 1359, 51766, 51766, 382, 264, 2649, 8637, 13, 51836], "temperature": 0.0, "avg_logprob": -0.273004014915395, "compression_ratio": 1.6307053941908713, "no_speech_prob": 1.07818732431042e-05}, {"id": 922, "seek": 524126, "start": 5254.42, "end": 5257.38, "text": " Yeah, so the soft hinge loss has an infinite margin.", "tokens": [50364, 570, 309, 486, 445, 6997, 264, 2744, 50468, 50468, 295, 264, 17443, 295, 428, 1036, 4583, 13, 50546, 50546, 663, 311, 439, 309, 775, 13, 50588, 50588, 1033, 13, 50630, 50792, 407, 309, 311, 1936, 493, 281, 291, 13, 50856, 51022, 865, 11, 370, 264, 2787, 28822, 4470, 575, 364, 13785, 10270, 13, 51170, 51170, 467, 2738, 264, 2649, 1296, 729, 732, 25737, 51258, 51258, 281, 312, 13785, 11, 51306, 51306, 457, 264, 12403, 1333, 295, 24108, 37330, 11, 51458, 51458, 370, 309, 311, 1128, 799, 483, 456, 51544, 51544, 570, 264, 2771, 2448, 483, 588, 1359, 51766, 51766, 382, 264, 2649, 8637, 13, 51836], "temperature": 0.0, "avg_logprob": -0.273004014915395, "compression_ratio": 1.6307053941908713, "no_speech_prob": 1.07818732431042e-05}, {"id": 923, "seek": 524126, "start": 5257.38, "end": 5259.14, "text": " It wants the difference between those two energies", "tokens": [50364, 570, 309, 486, 445, 6997, 264, 2744, 50468, 50468, 295, 264, 17443, 295, 428, 1036, 4583, 13, 50546, 50546, 663, 311, 439, 309, 775, 13, 50588, 50588, 1033, 13, 50630, 50792, 407, 309, 311, 1936, 493, 281, 291, 13, 50856, 51022, 865, 11, 370, 264, 2787, 28822, 4470, 575, 364, 13785, 10270, 13, 51170, 51170, 467, 2738, 264, 2649, 1296, 729, 732, 25737, 51258, 51258, 281, 312, 13785, 11, 51306, 51306, 457, 264, 12403, 1333, 295, 24108, 37330, 11, 51458, 51458, 370, 309, 311, 1128, 799, 483, 456, 51544, 51544, 570, 264, 2771, 2448, 483, 588, 1359, 51766, 51766, 382, 264, 2649, 8637, 13, 51836], "temperature": 0.0, "avg_logprob": -0.273004014915395, "compression_ratio": 1.6307053941908713, "no_speech_prob": 1.07818732431042e-05}, {"id": 924, "seek": 524126, "start": 5259.14, "end": 5260.1, "text": " to be infinite,", "tokens": [50364, 570, 309, 486, 445, 6997, 264, 2744, 50468, 50468, 295, 264, 17443, 295, 428, 1036, 4583, 13, 50546, 50546, 663, 311, 439, 309, 775, 13, 50588, 50588, 1033, 13, 50630, 50792, 407, 309, 311, 1936, 493, 281, 291, 13, 50856, 51022, 865, 11, 370, 264, 2787, 28822, 4470, 575, 364, 13785, 10270, 13, 51170, 51170, 467, 2738, 264, 2649, 1296, 729, 732, 25737, 51258, 51258, 281, 312, 13785, 11, 51306, 51306, 457, 264, 12403, 1333, 295, 24108, 37330, 11, 51458, 51458, 370, 309, 311, 1128, 799, 483, 456, 51544, 51544, 570, 264, 2771, 2448, 483, 588, 1359, 51766, 51766, 382, 264, 2649, 8637, 13, 51836], "temperature": 0.0, "avg_logprob": -0.273004014915395, "compression_ratio": 1.6307053941908713, "no_speech_prob": 1.07818732431042e-05}, {"id": 925, "seek": 524126, "start": 5260.1, "end": 5263.14, "text": " but the stroke sort of decreases exponentially,", "tokens": [50364, 570, 309, 486, 445, 6997, 264, 2744, 50468, 50468, 295, 264, 17443, 295, 428, 1036, 4583, 13, 50546, 50546, 663, 311, 439, 309, 775, 13, 50588, 50588, 1033, 13, 50630, 50792, 407, 309, 311, 1936, 493, 281, 291, 13, 50856, 51022, 865, 11, 370, 264, 2787, 28822, 4470, 575, 364, 13785, 10270, 13, 51170, 51170, 467, 2738, 264, 2649, 1296, 729, 732, 25737, 51258, 51258, 281, 312, 13785, 11, 51306, 51306, 457, 264, 12403, 1333, 295, 24108, 37330, 11, 51458, 51458, 370, 309, 311, 1128, 799, 483, 456, 51544, 51544, 570, 264, 2771, 2448, 483, 588, 1359, 51766, 51766, 382, 264, 2649, 8637, 13, 51836], "temperature": 0.0, "avg_logprob": -0.273004014915395, "compression_ratio": 1.6307053941908713, "no_speech_prob": 1.07818732431042e-05}, {"id": 926, "seek": 524126, "start": 5263.14, "end": 5264.860000000001, "text": " so it's never gonna get there", "tokens": [50364, 570, 309, 486, 445, 6997, 264, 2744, 50468, 50468, 295, 264, 17443, 295, 428, 1036, 4583, 13, 50546, 50546, 663, 311, 439, 309, 775, 13, 50588, 50588, 1033, 13, 50630, 50792, 407, 309, 311, 1936, 493, 281, 291, 13, 50856, 51022, 865, 11, 370, 264, 2787, 28822, 4470, 575, 364, 13785, 10270, 13, 51170, 51170, 467, 2738, 264, 2649, 1296, 729, 732, 25737, 51258, 51258, 281, 312, 13785, 11, 51306, 51306, 457, 264, 12403, 1333, 295, 24108, 37330, 11, 51458, 51458, 370, 309, 311, 1128, 799, 483, 456, 51544, 51544, 570, 264, 2771, 2448, 483, 588, 1359, 51766, 51766, 382, 264, 2649, 8637, 13, 51836], "temperature": 0.0, "avg_logprob": -0.273004014915395, "compression_ratio": 1.6307053941908713, "no_speech_prob": 1.07818732431042e-05}, {"id": 927, "seek": 524126, "start": 5264.860000000001, "end": 5269.3, "text": " because the gradients get very small", "tokens": [50364, 570, 309, 486, 445, 6997, 264, 2744, 50468, 50468, 295, 264, 17443, 295, 428, 1036, 4583, 13, 50546, 50546, 663, 311, 439, 309, 775, 13, 50588, 50588, 1033, 13, 50630, 50792, 407, 309, 311, 1936, 493, 281, 291, 13, 50856, 51022, 865, 11, 370, 264, 2787, 28822, 4470, 575, 364, 13785, 10270, 13, 51170, 51170, 467, 2738, 264, 2649, 1296, 729, 732, 25737, 51258, 51258, 281, 312, 13785, 11, 51306, 51306, 457, 264, 12403, 1333, 295, 24108, 37330, 11, 51458, 51458, 370, 309, 311, 1128, 799, 483, 456, 51544, 51544, 570, 264, 2771, 2448, 483, 588, 1359, 51766, 51766, 382, 264, 2649, 8637, 13, 51836], "temperature": 0.0, "avg_logprob": -0.273004014915395, "compression_ratio": 1.6307053941908713, "no_speech_prob": 1.07818732431042e-05}, {"id": 928, "seek": 526930, "start": 5269.3, "end": 5271.5, "text": " as the difference increases.", "tokens": [50364, 382, 264, 2649, 8637, 13, 50474, 50696, 1692, 311, 1071, 1365, 295, 257, 10270, 4470, 11, 50820, 50820, 264, 3732, 4470, 11, 264, 3732, 3732, 4470, 13, 50976, 50976, 1033, 11, 370, 341, 307, 257, 4470, 300, 9898, 281, 652, 51164, 51164, 264, 2281, 295, 264, 3006, 1867, 8889, 51328, 51328, 382, 1359, 382, 1944, 11, 51382, 51382, 293, 550, 309, 575, 257, 3732, 28822, 281, 2944, 1314, 11, 51568, 51568, 281, 2944, 493, 264, 2281, 295, 264, 881, 51656, 51656, 295, 5006, 18424, 1867, 13, 51726, 51784], "temperature": 0.0, "avg_logprob": -0.16163775126139324, "compression_ratio": 1.7608695652173914, "no_speech_prob": 8.800760042504407e-06}, {"id": 929, "seek": 526930, "start": 5275.9400000000005, "end": 5278.42, "text": " Here's another example of a margin loss,", "tokens": [50364, 382, 264, 2649, 8637, 13, 50474, 50696, 1692, 311, 1071, 1365, 295, 257, 10270, 4470, 11, 50820, 50820, 264, 3732, 4470, 11, 264, 3732, 3732, 4470, 13, 50976, 50976, 1033, 11, 370, 341, 307, 257, 4470, 300, 9898, 281, 652, 51164, 51164, 264, 2281, 295, 264, 3006, 1867, 8889, 51328, 51328, 382, 1359, 382, 1944, 11, 51382, 51382, 293, 550, 309, 575, 257, 3732, 28822, 281, 2944, 1314, 11, 51568, 51568, 281, 2944, 493, 264, 2281, 295, 264, 881, 51656, 51656, 295, 5006, 18424, 1867, 13, 51726, 51784], "temperature": 0.0, "avg_logprob": -0.16163775126139324, "compression_ratio": 1.7608695652173914, "no_speech_prob": 8.800760042504407e-06}, {"id": 930, "seek": 526930, "start": 5278.42, "end": 5281.54, "text": " the square loss, the square square loss.", "tokens": [50364, 382, 264, 2649, 8637, 13, 50474, 50696, 1692, 311, 1071, 1365, 295, 257, 10270, 4470, 11, 50820, 50820, 264, 3732, 4470, 11, 264, 3732, 3732, 4470, 13, 50976, 50976, 1033, 11, 370, 341, 307, 257, 4470, 300, 9898, 281, 652, 51164, 51164, 264, 2281, 295, 264, 3006, 1867, 8889, 51328, 51328, 382, 1359, 382, 1944, 11, 51382, 51382, 293, 550, 309, 575, 257, 3732, 28822, 281, 2944, 1314, 11, 51568, 51568, 281, 2944, 493, 264, 2281, 295, 264, 881, 51656, 51656, 295, 5006, 18424, 1867, 13, 51726, 51784], "temperature": 0.0, "avg_logprob": -0.16163775126139324, "compression_ratio": 1.7608695652173914, "no_speech_prob": 8.800760042504407e-06}, {"id": 931, "seek": 526930, "start": 5281.54, "end": 5285.3, "text": " Okay, so this is a loss that tries to make", "tokens": [50364, 382, 264, 2649, 8637, 13, 50474, 50696, 1692, 311, 1071, 1365, 295, 257, 10270, 4470, 11, 50820, 50820, 264, 3732, 4470, 11, 264, 3732, 3732, 4470, 13, 50976, 50976, 1033, 11, 370, 341, 307, 257, 4470, 300, 9898, 281, 652, 51164, 51164, 264, 2281, 295, 264, 3006, 1867, 8889, 51328, 51328, 382, 1359, 382, 1944, 11, 51382, 51382, 293, 550, 309, 575, 257, 3732, 28822, 281, 2944, 1314, 11, 51568, 51568, 281, 2944, 493, 264, 2281, 295, 264, 881, 51656, 51656, 295, 5006, 18424, 1867, 13, 51726, 51784], "temperature": 0.0, "avg_logprob": -0.16163775126139324, "compression_ratio": 1.7608695652173914, "no_speech_prob": 8.800760042504407e-06}, {"id": 932, "seek": 526930, "start": 5285.3, "end": 5288.58, "text": " the energy of the correct answer squared", "tokens": [50364, 382, 264, 2649, 8637, 13, 50474, 50696, 1692, 311, 1071, 1365, 295, 257, 10270, 4470, 11, 50820, 50820, 264, 3732, 4470, 11, 264, 3732, 3732, 4470, 13, 50976, 50976, 1033, 11, 370, 341, 307, 257, 4470, 300, 9898, 281, 652, 51164, 51164, 264, 2281, 295, 264, 3006, 1867, 8889, 51328, 51328, 382, 1359, 382, 1944, 11, 51382, 51382, 293, 550, 309, 575, 257, 3732, 28822, 281, 2944, 1314, 11, 51568, 51568, 281, 2944, 493, 264, 2281, 295, 264, 881, 51656, 51656, 295, 5006, 18424, 1867, 13, 51726, 51784], "temperature": 0.0, "avg_logprob": -0.16163775126139324, "compression_ratio": 1.7608695652173914, "no_speech_prob": 8.800760042504407e-06}, {"id": 933, "seek": 526930, "start": 5288.58, "end": 5289.66, "text": " as small as possible,", "tokens": [50364, 382, 264, 2649, 8637, 13, 50474, 50696, 1692, 311, 1071, 1365, 295, 257, 10270, 4470, 11, 50820, 50820, 264, 3732, 4470, 11, 264, 3732, 3732, 4470, 13, 50976, 50976, 1033, 11, 370, 341, 307, 257, 4470, 300, 9898, 281, 652, 51164, 51164, 264, 2281, 295, 264, 3006, 1867, 8889, 51328, 51328, 382, 1359, 382, 1944, 11, 51382, 51382, 293, 550, 309, 575, 257, 3732, 28822, 281, 2944, 1314, 11, 51568, 51568, 281, 2944, 493, 264, 2281, 295, 264, 881, 51656, 51656, 295, 5006, 18424, 1867, 13, 51726, 51784], "temperature": 0.0, "avg_logprob": -0.16163775126139324, "compression_ratio": 1.7608695652173914, "no_speech_prob": 8.800760042504407e-06}, {"id": 934, "seek": 526930, "start": 5289.66, "end": 5293.38, "text": " and then it has a square hinge to push away,", "tokens": [50364, 382, 264, 2649, 8637, 13, 50474, 50696, 1692, 311, 1071, 1365, 295, 257, 10270, 4470, 11, 50820, 50820, 264, 3732, 4470, 11, 264, 3732, 3732, 4470, 13, 50976, 50976, 1033, 11, 370, 341, 307, 257, 4470, 300, 9898, 281, 652, 51164, 51164, 264, 2281, 295, 264, 3006, 1867, 8889, 51328, 51328, 382, 1359, 382, 1944, 11, 51382, 51382, 293, 550, 309, 575, 257, 3732, 28822, 281, 2944, 1314, 11, 51568, 51568, 281, 2944, 493, 264, 2281, 295, 264, 881, 51656, 51656, 295, 5006, 18424, 1867, 13, 51726, 51784], "temperature": 0.0, "avg_logprob": -0.16163775126139324, "compression_ratio": 1.7608695652173914, "no_speech_prob": 8.800760042504407e-06}, {"id": 935, "seek": 526930, "start": 5293.38, "end": 5295.14, "text": " to push up the energy of the most", "tokens": [50364, 382, 264, 2649, 8637, 13, 50474, 50696, 1692, 311, 1071, 1365, 295, 257, 10270, 4470, 11, 50820, 50820, 264, 3732, 4470, 11, 264, 3732, 3732, 4470, 13, 50976, 50976, 1033, 11, 370, 341, 307, 257, 4470, 300, 9898, 281, 652, 51164, 51164, 264, 2281, 295, 264, 3006, 1867, 8889, 51328, 51328, 382, 1359, 382, 1944, 11, 51382, 51382, 293, 550, 309, 575, 257, 3732, 28822, 281, 2944, 1314, 11, 51568, 51568, 281, 2944, 493, 264, 2281, 295, 264, 881, 51656, 51656, 295, 5006, 18424, 1867, 13, 51726, 51784], "temperature": 0.0, "avg_logprob": -0.16163775126139324, "compression_ratio": 1.7608695652173914, "no_speech_prob": 8.800760042504407e-06}, {"id": 936, "seek": 526930, "start": 5295.14, "end": 5296.54, "text": " of finding incorrect answer.", "tokens": [50364, 382, 264, 2649, 8637, 13, 50474, 50696, 1692, 311, 1071, 1365, 295, 257, 10270, 4470, 11, 50820, 50820, 264, 3732, 4470, 11, 264, 3732, 3732, 4470, 13, 50976, 50976, 1033, 11, 370, 341, 307, 257, 4470, 300, 9898, 281, 652, 51164, 51164, 264, 2281, 295, 264, 3006, 1867, 8889, 51328, 51328, 382, 1359, 382, 1944, 11, 51382, 51382, 293, 550, 309, 575, 257, 3732, 28822, 281, 2944, 1314, 11, 51568, 51568, 281, 2944, 493, 264, 2281, 295, 264, 881, 51656, 51656, 295, 5006, 18424, 1867, 13, 51726, 51784], "temperature": 0.0, "avg_logprob": -0.16163775126139324, "compression_ratio": 1.7608695652173914, "no_speech_prob": 8.800760042504407e-06}, {"id": 937, "seek": 529654, "start": 5296.54, "end": 5298.38, "text": " Okay, and again, that works.", "tokens": [50364, 1033, 11, 293, 797, 11, 300, 1985, 13, 50456, 50500, 400, 341, 307, 588, 2531, 281, 264, 733, 295, 4470, 50572, 50572, 300, 561, 764, 294, 318, 2918, 1130, 36170, 50640, 50640, 293, 1507, 411, 300, 300, 291, 600, 2198, 466, 13, 50744, 50792, 821, 311, 257, 1379, 1706, 3557, 414, 295, 1270, 15352, 50878, 50878, 597, 286, 478, 406, 799, 352, 807, 13, 50976, 50976, 821, 311, 767, 257, 1379, 3199, 510, 11, 51066, 51066, 597, 307, 611, 294, 341, 3035, 11, 51124, 51124, 264, 7073, 322, 2281, 12, 6032, 5245, 13, 51234, 51234, 400, 437, 311, 16176, 322, 264, 558, 1252, 51422, 51466, 307, 1968, 436, 362, 257, 10270, 420, 406, 13, 51568, 51568, 407, 264, 2281, 4470, 775, 406, 362, 257, 10270, 13, 51668, 51668, 467, 1177, 380, 2944, 493, 1340, 11, 370, 572, 10270, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.29364605883618333, "compression_ratio": 1.686206896551724, "no_speech_prob": 1.3630669855047017e-05}, {"id": 938, "seek": 529654, "start": 5299.26, "end": 5300.7, "text": " And this is very similar to the kind of loss", "tokens": [50364, 1033, 11, 293, 797, 11, 300, 1985, 13, 50456, 50500, 400, 341, 307, 588, 2531, 281, 264, 733, 295, 4470, 50572, 50572, 300, 561, 764, 294, 318, 2918, 1130, 36170, 50640, 50640, 293, 1507, 411, 300, 300, 291, 600, 2198, 466, 13, 50744, 50792, 821, 311, 257, 1379, 1706, 3557, 414, 295, 1270, 15352, 50878, 50878, 597, 286, 478, 406, 799, 352, 807, 13, 50976, 50976, 821, 311, 767, 257, 1379, 3199, 510, 11, 51066, 51066, 597, 307, 611, 294, 341, 3035, 11, 51124, 51124, 264, 7073, 322, 2281, 12, 6032, 5245, 13, 51234, 51234, 400, 437, 311, 16176, 322, 264, 558, 1252, 51422, 51466, 307, 1968, 436, 362, 257, 10270, 420, 406, 13, 51568, 51568, 407, 264, 2281, 4470, 775, 406, 362, 257, 10270, 13, 51668, 51668, 467, 1177, 380, 2944, 493, 1340, 11, 370, 572, 10270, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.29364605883618333, "compression_ratio": 1.686206896551724, "no_speech_prob": 1.3630669855047017e-05}, {"id": 939, "seek": 529654, "start": 5300.7, "end": 5302.06, "text": " that people use in Siamese nets", "tokens": [50364, 1033, 11, 293, 797, 11, 300, 1985, 13, 50456, 50500, 400, 341, 307, 588, 2531, 281, 264, 733, 295, 4470, 50572, 50572, 300, 561, 764, 294, 318, 2918, 1130, 36170, 50640, 50640, 293, 1507, 411, 300, 300, 291, 600, 2198, 466, 13, 50744, 50792, 821, 311, 257, 1379, 1706, 3557, 414, 295, 1270, 15352, 50878, 50878, 597, 286, 478, 406, 799, 352, 807, 13, 50976, 50976, 821, 311, 767, 257, 1379, 3199, 510, 11, 51066, 51066, 597, 307, 611, 294, 341, 3035, 11, 51124, 51124, 264, 7073, 322, 2281, 12, 6032, 5245, 13, 51234, 51234, 400, 437, 311, 16176, 322, 264, 558, 1252, 51422, 51466, 307, 1968, 436, 362, 257, 10270, 420, 406, 13, 51568, 51568, 407, 264, 2281, 4470, 775, 406, 362, 257, 10270, 13, 51668, 51668, 467, 1177, 380, 2944, 493, 1340, 11, 370, 572, 10270, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.29364605883618333, "compression_ratio": 1.686206896551724, "no_speech_prob": 1.3630669855047017e-05}, {"id": 940, "seek": 529654, "start": 5302.06, "end": 5304.14, "text": " and stuff like that that you've heard about.", "tokens": [50364, 1033, 11, 293, 797, 11, 300, 1985, 13, 50456, 50500, 400, 341, 307, 588, 2531, 281, 264, 733, 295, 4470, 50572, 50572, 300, 561, 764, 294, 318, 2918, 1130, 36170, 50640, 50640, 293, 1507, 411, 300, 300, 291, 600, 2198, 466, 13, 50744, 50792, 821, 311, 257, 1379, 1706, 3557, 414, 295, 1270, 15352, 50878, 50878, 597, 286, 478, 406, 799, 352, 807, 13, 50976, 50976, 821, 311, 767, 257, 1379, 3199, 510, 11, 51066, 51066, 597, 307, 611, 294, 341, 3035, 11, 51124, 51124, 264, 7073, 322, 2281, 12, 6032, 5245, 13, 51234, 51234, 400, 437, 311, 16176, 322, 264, 558, 1252, 51422, 51466, 307, 1968, 436, 362, 257, 10270, 420, 406, 13, 51568, 51568, 407, 264, 2281, 4470, 775, 406, 362, 257, 10270, 13, 51668, 51668, 467, 1177, 380, 2944, 493, 1340, 11, 370, 572, 10270, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.29364605883618333, "compression_ratio": 1.686206896551724, "no_speech_prob": 1.3630669855047017e-05}, {"id": 941, "seek": 529654, "start": 5305.1, "end": 5306.82, "text": " There's a whole menagerie of such losses", "tokens": [50364, 1033, 11, 293, 797, 11, 300, 1985, 13, 50456, 50500, 400, 341, 307, 588, 2531, 281, 264, 733, 295, 4470, 50572, 50572, 300, 561, 764, 294, 318, 2918, 1130, 36170, 50640, 50640, 293, 1507, 411, 300, 300, 291, 600, 2198, 466, 13, 50744, 50792, 821, 311, 257, 1379, 1706, 3557, 414, 295, 1270, 15352, 50878, 50878, 597, 286, 478, 406, 799, 352, 807, 13, 50976, 50976, 821, 311, 767, 257, 1379, 3199, 510, 11, 51066, 51066, 597, 307, 611, 294, 341, 3035, 11, 51124, 51124, 264, 7073, 322, 2281, 12, 6032, 5245, 13, 51234, 51234, 400, 437, 311, 16176, 322, 264, 558, 1252, 51422, 51466, 307, 1968, 436, 362, 257, 10270, 420, 406, 13, 51568, 51568, 407, 264, 2281, 4470, 775, 406, 362, 257, 10270, 13, 51668, 51668, 467, 1177, 380, 2944, 493, 1340, 11, 370, 572, 10270, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.29364605883618333, "compression_ratio": 1.686206896551724, "no_speech_prob": 1.3630669855047017e-05}, {"id": 942, "seek": 529654, "start": 5306.82, "end": 5308.78, "text": " which I'm not gonna go through.", "tokens": [50364, 1033, 11, 293, 797, 11, 300, 1985, 13, 50456, 50500, 400, 341, 307, 588, 2531, 281, 264, 733, 295, 4470, 50572, 50572, 300, 561, 764, 294, 318, 2918, 1130, 36170, 50640, 50640, 293, 1507, 411, 300, 300, 291, 600, 2198, 466, 13, 50744, 50792, 821, 311, 257, 1379, 1706, 3557, 414, 295, 1270, 15352, 50878, 50878, 597, 286, 478, 406, 799, 352, 807, 13, 50976, 50976, 821, 311, 767, 257, 1379, 3199, 510, 11, 51066, 51066, 597, 307, 611, 294, 341, 3035, 11, 51124, 51124, 264, 7073, 322, 2281, 12, 6032, 5245, 13, 51234, 51234, 400, 437, 311, 16176, 322, 264, 558, 1252, 51422, 51466, 307, 1968, 436, 362, 257, 10270, 420, 406, 13, 51568, 51568, 407, 264, 2281, 4470, 775, 406, 362, 257, 10270, 13, 51668, 51668, 467, 1177, 380, 2944, 493, 1340, 11, 370, 572, 10270, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.29364605883618333, "compression_ratio": 1.686206896551724, "no_speech_prob": 1.3630669855047017e-05}, {"id": 943, "seek": 529654, "start": 5308.78, "end": 5310.58, "text": " There's actually a whole table here,", "tokens": [50364, 1033, 11, 293, 797, 11, 300, 1985, 13, 50456, 50500, 400, 341, 307, 588, 2531, 281, 264, 733, 295, 4470, 50572, 50572, 300, 561, 764, 294, 318, 2918, 1130, 36170, 50640, 50640, 293, 1507, 411, 300, 300, 291, 600, 2198, 466, 13, 50744, 50792, 821, 311, 257, 1379, 1706, 3557, 414, 295, 1270, 15352, 50878, 50878, 597, 286, 478, 406, 799, 352, 807, 13, 50976, 50976, 821, 311, 767, 257, 1379, 3199, 510, 11, 51066, 51066, 597, 307, 611, 294, 341, 3035, 11, 51124, 51124, 264, 7073, 322, 2281, 12, 6032, 5245, 13, 51234, 51234, 400, 437, 311, 16176, 322, 264, 558, 1252, 51422, 51466, 307, 1968, 436, 362, 257, 10270, 420, 406, 13, 51568, 51568, 407, 264, 2281, 4470, 775, 406, 362, 257, 10270, 13, 51668, 51668, 467, 1177, 380, 2944, 493, 1340, 11, 370, 572, 10270, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.29364605883618333, "compression_ratio": 1.686206896551724, "no_speech_prob": 1.3630669855047017e-05}, {"id": 944, "seek": 529654, "start": 5310.58, "end": 5311.74, "text": " which is also in this paper,", "tokens": [50364, 1033, 11, 293, 797, 11, 300, 1985, 13, 50456, 50500, 400, 341, 307, 588, 2531, 281, 264, 733, 295, 4470, 50572, 50572, 300, 561, 764, 294, 318, 2918, 1130, 36170, 50640, 50640, 293, 1507, 411, 300, 300, 291, 600, 2198, 466, 13, 50744, 50792, 821, 311, 257, 1379, 1706, 3557, 414, 295, 1270, 15352, 50878, 50878, 597, 286, 478, 406, 799, 352, 807, 13, 50976, 50976, 821, 311, 767, 257, 1379, 3199, 510, 11, 51066, 51066, 597, 307, 611, 294, 341, 3035, 11, 51124, 51124, 264, 7073, 322, 2281, 12, 6032, 5245, 13, 51234, 51234, 400, 437, 311, 16176, 322, 264, 558, 1252, 51422, 51466, 307, 1968, 436, 362, 257, 10270, 420, 406, 13, 51568, 51568, 407, 264, 2281, 4470, 775, 406, 362, 257, 10270, 13, 51668, 51668, 467, 1177, 380, 2944, 493, 1340, 11, 370, 572, 10270, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.29364605883618333, "compression_ratio": 1.686206896551724, "no_speech_prob": 1.3630669855047017e-05}, {"id": 945, "seek": 529654, "start": 5311.74, "end": 5313.94, "text": " the tutorial on energy-based models.", "tokens": [50364, 1033, 11, 293, 797, 11, 300, 1985, 13, 50456, 50500, 400, 341, 307, 588, 2531, 281, 264, 733, 295, 4470, 50572, 50572, 300, 561, 764, 294, 318, 2918, 1130, 36170, 50640, 50640, 293, 1507, 411, 300, 300, 291, 600, 2198, 466, 13, 50744, 50792, 821, 311, 257, 1379, 1706, 3557, 414, 295, 1270, 15352, 50878, 50878, 597, 286, 478, 406, 799, 352, 807, 13, 50976, 50976, 821, 311, 767, 257, 1379, 3199, 510, 11, 51066, 51066, 597, 307, 611, 294, 341, 3035, 11, 51124, 51124, 264, 7073, 322, 2281, 12, 6032, 5245, 13, 51234, 51234, 400, 437, 311, 16176, 322, 264, 558, 1252, 51422, 51466, 307, 1968, 436, 362, 257, 10270, 420, 406, 13, 51568, 51568, 407, 264, 2281, 4470, 775, 406, 362, 257, 10270, 13, 51668, 51668, 467, 1177, 380, 2944, 493, 1340, 11, 370, 572, 10270, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.29364605883618333, "compression_ratio": 1.686206896551724, "no_speech_prob": 1.3630669855047017e-05}, {"id": 946, "seek": 529654, "start": 5313.94, "end": 5317.7, "text": " And what's indicated on the right side", "tokens": [50364, 1033, 11, 293, 797, 11, 300, 1985, 13, 50456, 50500, 400, 341, 307, 588, 2531, 281, 264, 733, 295, 4470, 50572, 50572, 300, 561, 764, 294, 318, 2918, 1130, 36170, 50640, 50640, 293, 1507, 411, 300, 300, 291, 600, 2198, 466, 13, 50744, 50792, 821, 311, 257, 1379, 1706, 3557, 414, 295, 1270, 15352, 50878, 50878, 597, 286, 478, 406, 799, 352, 807, 13, 50976, 50976, 821, 311, 767, 257, 1379, 3199, 510, 11, 51066, 51066, 597, 307, 611, 294, 341, 3035, 11, 51124, 51124, 264, 7073, 322, 2281, 12, 6032, 5245, 13, 51234, 51234, 400, 437, 311, 16176, 322, 264, 558, 1252, 51422, 51466, 307, 1968, 436, 362, 257, 10270, 420, 406, 13, 51568, 51568, 407, 264, 2281, 4470, 775, 406, 362, 257, 10270, 13, 51668, 51668, 467, 1177, 380, 2944, 493, 1340, 11, 370, 572, 10270, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.29364605883618333, "compression_ratio": 1.686206896551724, "no_speech_prob": 1.3630669855047017e-05}, {"id": 947, "seek": 529654, "start": 5318.58, "end": 5320.62, "text": " is whether they have a margin or not.", "tokens": [50364, 1033, 11, 293, 797, 11, 300, 1985, 13, 50456, 50500, 400, 341, 307, 588, 2531, 281, 264, 733, 295, 4470, 50572, 50572, 300, 561, 764, 294, 318, 2918, 1130, 36170, 50640, 50640, 293, 1507, 411, 300, 300, 291, 600, 2198, 466, 13, 50744, 50792, 821, 311, 257, 1379, 1706, 3557, 414, 295, 1270, 15352, 50878, 50878, 597, 286, 478, 406, 799, 352, 807, 13, 50976, 50976, 821, 311, 767, 257, 1379, 3199, 510, 11, 51066, 51066, 597, 307, 611, 294, 341, 3035, 11, 51124, 51124, 264, 7073, 322, 2281, 12, 6032, 5245, 13, 51234, 51234, 400, 437, 311, 16176, 322, 264, 558, 1252, 51422, 51466, 307, 1968, 436, 362, 257, 10270, 420, 406, 13, 51568, 51568, 407, 264, 2281, 4470, 775, 406, 362, 257, 10270, 13, 51668, 51668, 467, 1177, 380, 2944, 493, 1340, 11, 370, 572, 10270, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.29364605883618333, "compression_ratio": 1.686206896551724, "no_speech_prob": 1.3630669855047017e-05}, {"id": 948, "seek": 529654, "start": 5320.62, "end": 5322.62, "text": " So the energy loss does not have a margin.", "tokens": [50364, 1033, 11, 293, 797, 11, 300, 1985, 13, 50456, 50500, 400, 341, 307, 588, 2531, 281, 264, 733, 295, 4470, 50572, 50572, 300, 561, 764, 294, 318, 2918, 1130, 36170, 50640, 50640, 293, 1507, 411, 300, 300, 291, 600, 2198, 466, 13, 50744, 50792, 821, 311, 257, 1379, 1706, 3557, 414, 295, 1270, 15352, 50878, 50878, 597, 286, 478, 406, 799, 352, 807, 13, 50976, 50976, 821, 311, 767, 257, 1379, 3199, 510, 11, 51066, 51066, 597, 307, 611, 294, 341, 3035, 11, 51124, 51124, 264, 7073, 322, 2281, 12, 6032, 5245, 13, 51234, 51234, 400, 437, 311, 16176, 322, 264, 558, 1252, 51422, 51466, 307, 1968, 436, 362, 257, 10270, 420, 406, 13, 51568, 51568, 407, 264, 2281, 4470, 775, 406, 362, 257, 10270, 13, 51668, 51668, 467, 1177, 380, 2944, 493, 1340, 11, 370, 572, 10270, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.29364605883618333, "compression_ratio": 1.686206896551724, "no_speech_prob": 1.3630669855047017e-05}, {"id": 949, "seek": 529654, "start": 5322.62, "end": 5325.46, "text": " It doesn't push up anything, so no margin.", "tokens": [50364, 1033, 11, 293, 797, 11, 300, 1985, 13, 50456, 50500, 400, 341, 307, 588, 2531, 281, 264, 733, 295, 4470, 50572, 50572, 300, 561, 764, 294, 318, 2918, 1130, 36170, 50640, 50640, 293, 1507, 411, 300, 300, 291, 600, 2198, 466, 13, 50744, 50792, 821, 311, 257, 1379, 1706, 3557, 414, 295, 1270, 15352, 50878, 50878, 597, 286, 478, 406, 799, 352, 807, 13, 50976, 50976, 821, 311, 767, 257, 1379, 3199, 510, 11, 51066, 51066, 597, 307, 611, 294, 341, 3035, 11, 51124, 51124, 264, 7073, 322, 2281, 12, 6032, 5245, 13, 51234, 51234, 400, 437, 311, 16176, 322, 264, 558, 1252, 51422, 51466, 307, 1968, 436, 362, 257, 10270, 420, 406, 13, 51568, 51568, 407, 264, 2281, 4470, 775, 406, 362, 257, 10270, 13, 51668, 51668, 467, 1177, 380, 2944, 493, 1340, 11, 370, 572, 10270, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.29364605883618333, "compression_ratio": 1.686206896551724, "no_speech_prob": 1.3630669855047017e-05}, {"id": 950, "seek": 532546, "start": 5325.46, "end": 5327.82, "text": " It doesn't work always.", "tokens": [50364, 467, 1177, 380, 589, 1009, 13, 50482, 50482, 509, 362, 281, 1715, 264, 3479, 50524, 50524, 370, 300, 341, 4470, 815, 589, 337, 300, 1185, 13, 50700, 50700, 440, 43276, 2044, 4470, 775, 406, 589, 294, 2674, 13, 50878, 50878, 467, 787, 1985, 498, 291, 362, 257, 8213, 13075, 2144, 51006, 51006, 295, 428, 2281, 382, 257, 2445, 295, 264, 9834, 11, 51132, 51132, 457, 300, 311, 257, 2121, 1389, 11, 51200, 51200, 293, 300, 311, 264, 1389, 337, 264, 43276, 2044, 13, 51280, 51356, 400, 550, 512, 295, 552, 362, 257, 19362, 10270, 11, 51504, 51504, 411, 264, 28822, 4470, 11, 51560, 51560, 293, 512, 295, 552, 362, 364, 13785, 10270, 11, 51640, 51640, 411, 264, 3565, 295, 264, 2281, 13, 51840], "temperature": 0.0, "avg_logprob": -0.3328608703613281, "compression_ratio": 1.9059829059829059, "no_speech_prob": 7.645546247658785e-06}, {"id": 951, "seek": 532546, "start": 5327.82, "end": 5328.66, "text": " You have to design the machine", "tokens": [50364, 467, 1177, 380, 589, 1009, 13, 50482, 50482, 509, 362, 281, 1715, 264, 3479, 50524, 50524, 370, 300, 341, 4470, 815, 589, 337, 300, 1185, 13, 50700, 50700, 440, 43276, 2044, 4470, 775, 406, 589, 294, 2674, 13, 50878, 50878, 467, 787, 1985, 498, 291, 362, 257, 8213, 13075, 2144, 51006, 51006, 295, 428, 2281, 382, 257, 2445, 295, 264, 9834, 11, 51132, 51132, 457, 300, 311, 257, 2121, 1389, 11, 51200, 51200, 293, 300, 311, 264, 1389, 337, 264, 43276, 2044, 13, 51280, 51356, 400, 550, 512, 295, 552, 362, 257, 19362, 10270, 11, 51504, 51504, 411, 264, 28822, 4470, 11, 51560, 51560, 293, 512, 295, 552, 362, 364, 13785, 10270, 11, 51640, 51640, 411, 264, 3565, 295, 264, 2281, 13, 51840], "temperature": 0.0, "avg_logprob": -0.3328608703613281, "compression_ratio": 1.9059829059829059, "no_speech_prob": 7.645546247658785e-06}, {"id": 952, "seek": 532546, "start": 5328.66, "end": 5332.18, "text": " so that this loss may work for that system.", "tokens": [50364, 467, 1177, 380, 589, 1009, 13, 50482, 50482, 509, 362, 281, 1715, 264, 3479, 50524, 50524, 370, 300, 341, 4470, 815, 589, 337, 300, 1185, 13, 50700, 50700, 440, 43276, 2044, 4470, 775, 406, 589, 294, 2674, 13, 50878, 50878, 467, 787, 1985, 498, 291, 362, 257, 8213, 13075, 2144, 51006, 51006, 295, 428, 2281, 382, 257, 2445, 295, 264, 9834, 11, 51132, 51132, 457, 300, 311, 257, 2121, 1389, 11, 51200, 51200, 293, 300, 311, 264, 1389, 337, 264, 43276, 2044, 13, 51280, 51356, 400, 550, 512, 295, 552, 362, 257, 19362, 10270, 11, 51504, 51504, 411, 264, 28822, 4470, 11, 51560, 51560, 293, 512, 295, 552, 362, 364, 13785, 10270, 11, 51640, 51640, 411, 264, 3565, 295, 264, 2281, 13, 51840], "temperature": 0.0, "avg_logprob": -0.3328608703613281, "compression_ratio": 1.9059829059829059, "no_speech_prob": 7.645546247658785e-06}, {"id": 953, "seek": 532546, "start": 5332.18, "end": 5335.74, "text": " The perceptron loss does not work in general.", "tokens": [50364, 467, 1177, 380, 589, 1009, 13, 50482, 50482, 509, 362, 281, 1715, 264, 3479, 50524, 50524, 370, 300, 341, 4470, 815, 589, 337, 300, 1185, 13, 50700, 50700, 440, 43276, 2044, 4470, 775, 406, 589, 294, 2674, 13, 50878, 50878, 467, 787, 1985, 498, 291, 362, 257, 8213, 13075, 2144, 51006, 51006, 295, 428, 2281, 382, 257, 2445, 295, 264, 9834, 11, 51132, 51132, 457, 300, 311, 257, 2121, 1389, 11, 51200, 51200, 293, 300, 311, 264, 1389, 337, 264, 43276, 2044, 13, 51280, 51356, 400, 550, 512, 295, 552, 362, 257, 19362, 10270, 11, 51504, 51504, 411, 264, 28822, 4470, 11, 51560, 51560, 293, 512, 295, 552, 362, 364, 13785, 10270, 11, 51640, 51640, 411, 264, 3565, 295, 264, 2281, 13, 51840], "temperature": 0.0, "avg_logprob": -0.3328608703613281, "compression_ratio": 1.9059829059829059, "no_speech_prob": 7.645546247658785e-06}, {"id": 954, "seek": 532546, "start": 5335.74, "end": 5338.3, "text": " It only works if you have a linear parameterization", "tokens": [50364, 467, 1177, 380, 589, 1009, 13, 50482, 50482, 509, 362, 281, 1715, 264, 3479, 50524, 50524, 370, 300, 341, 4470, 815, 589, 337, 300, 1185, 13, 50700, 50700, 440, 43276, 2044, 4470, 775, 406, 589, 294, 2674, 13, 50878, 50878, 467, 787, 1985, 498, 291, 362, 257, 8213, 13075, 2144, 51006, 51006, 295, 428, 2281, 382, 257, 2445, 295, 264, 9834, 11, 51132, 51132, 457, 300, 311, 257, 2121, 1389, 11, 51200, 51200, 293, 300, 311, 264, 1389, 337, 264, 43276, 2044, 13, 51280, 51356, 400, 550, 512, 295, 552, 362, 257, 19362, 10270, 11, 51504, 51504, 411, 264, 28822, 4470, 11, 51560, 51560, 293, 512, 295, 552, 362, 364, 13785, 10270, 11, 51640, 51640, 411, 264, 3565, 295, 264, 2281, 13, 51840], "temperature": 0.0, "avg_logprob": -0.3328608703613281, "compression_ratio": 1.9059829059829059, "no_speech_prob": 7.645546247658785e-06}, {"id": 955, "seek": 532546, "start": 5338.3, "end": 5340.82, "text": " of your energy as a function of the parameters,", "tokens": [50364, 467, 1177, 380, 589, 1009, 13, 50482, 50482, 509, 362, 281, 1715, 264, 3479, 50524, 50524, 370, 300, 341, 4470, 815, 589, 337, 300, 1185, 13, 50700, 50700, 440, 43276, 2044, 4470, 775, 406, 589, 294, 2674, 13, 50878, 50878, 467, 787, 1985, 498, 291, 362, 257, 8213, 13075, 2144, 51006, 51006, 295, 428, 2281, 382, 257, 2445, 295, 264, 9834, 11, 51132, 51132, 457, 300, 311, 257, 2121, 1389, 11, 51200, 51200, 293, 300, 311, 264, 1389, 337, 264, 43276, 2044, 13, 51280, 51356, 400, 550, 512, 295, 552, 362, 257, 19362, 10270, 11, 51504, 51504, 411, 264, 28822, 4470, 11, 51560, 51560, 293, 512, 295, 552, 362, 364, 13785, 10270, 11, 51640, 51640, 411, 264, 3565, 295, 264, 2281, 13, 51840], "temperature": 0.0, "avg_logprob": -0.3328608703613281, "compression_ratio": 1.9059829059829059, "no_speech_prob": 7.645546247658785e-06}, {"id": 956, "seek": 532546, "start": 5340.82, "end": 5342.18, "text": " but that's a special case,", "tokens": [50364, 467, 1177, 380, 589, 1009, 13, 50482, 50482, 509, 362, 281, 1715, 264, 3479, 50524, 50524, 370, 300, 341, 4470, 815, 589, 337, 300, 1185, 13, 50700, 50700, 440, 43276, 2044, 4470, 775, 406, 589, 294, 2674, 13, 50878, 50878, 467, 787, 1985, 498, 291, 362, 257, 8213, 13075, 2144, 51006, 51006, 295, 428, 2281, 382, 257, 2445, 295, 264, 9834, 11, 51132, 51132, 457, 300, 311, 257, 2121, 1389, 11, 51200, 51200, 293, 300, 311, 264, 1389, 337, 264, 43276, 2044, 13, 51280, 51356, 400, 550, 512, 295, 552, 362, 257, 19362, 10270, 11, 51504, 51504, 411, 264, 28822, 4470, 11, 51560, 51560, 293, 512, 295, 552, 362, 364, 13785, 10270, 11, 51640, 51640, 411, 264, 3565, 295, 264, 2281, 13, 51840], "temperature": 0.0, "avg_logprob": -0.3328608703613281, "compression_ratio": 1.9059829059829059, "no_speech_prob": 7.645546247658785e-06}, {"id": 957, "seek": 532546, "start": 5342.18, "end": 5343.78, "text": " and that's the case for the perceptron.", "tokens": [50364, 467, 1177, 380, 589, 1009, 13, 50482, 50482, 509, 362, 281, 1715, 264, 3479, 50524, 50524, 370, 300, 341, 4470, 815, 589, 337, 300, 1185, 13, 50700, 50700, 440, 43276, 2044, 4470, 775, 406, 589, 294, 2674, 13, 50878, 50878, 467, 787, 1985, 498, 291, 362, 257, 8213, 13075, 2144, 51006, 51006, 295, 428, 2281, 382, 257, 2445, 295, 264, 9834, 11, 51132, 51132, 457, 300, 311, 257, 2121, 1389, 11, 51200, 51200, 293, 300, 311, 264, 1389, 337, 264, 43276, 2044, 13, 51280, 51356, 400, 550, 512, 295, 552, 362, 257, 19362, 10270, 11, 51504, 51504, 411, 264, 28822, 4470, 11, 51560, 51560, 293, 512, 295, 552, 362, 364, 13785, 10270, 11, 51640, 51640, 411, 264, 3565, 295, 264, 2281, 13, 51840], "temperature": 0.0, "avg_logprob": -0.3328608703613281, "compression_ratio": 1.9059829059829059, "no_speech_prob": 7.645546247658785e-06}, {"id": 958, "seek": 532546, "start": 5345.3, "end": 5348.26, "text": " And then some of them have a finite margin,", "tokens": [50364, 467, 1177, 380, 589, 1009, 13, 50482, 50482, 509, 362, 281, 1715, 264, 3479, 50524, 50524, 370, 300, 341, 4470, 815, 589, 337, 300, 1185, 13, 50700, 50700, 440, 43276, 2044, 4470, 775, 406, 589, 294, 2674, 13, 50878, 50878, 467, 787, 1985, 498, 291, 362, 257, 8213, 13075, 2144, 51006, 51006, 295, 428, 2281, 382, 257, 2445, 295, 264, 9834, 11, 51132, 51132, 457, 300, 311, 257, 2121, 1389, 11, 51200, 51200, 293, 300, 311, 264, 1389, 337, 264, 43276, 2044, 13, 51280, 51356, 400, 550, 512, 295, 552, 362, 257, 19362, 10270, 11, 51504, 51504, 411, 264, 28822, 4470, 11, 51560, 51560, 293, 512, 295, 552, 362, 364, 13785, 10270, 11, 51640, 51640, 411, 264, 3565, 295, 264, 2281, 13, 51840], "temperature": 0.0, "avg_logprob": -0.3328608703613281, "compression_ratio": 1.9059829059829059, "no_speech_prob": 7.645546247658785e-06}, {"id": 959, "seek": 532546, "start": 5348.26, "end": 5349.38, "text": " like the hinge loss,", "tokens": [50364, 467, 1177, 380, 589, 1009, 13, 50482, 50482, 509, 362, 281, 1715, 264, 3479, 50524, 50524, 370, 300, 341, 4470, 815, 589, 337, 300, 1185, 13, 50700, 50700, 440, 43276, 2044, 4470, 775, 406, 589, 294, 2674, 13, 50878, 50878, 467, 787, 1985, 498, 291, 362, 257, 8213, 13075, 2144, 51006, 51006, 295, 428, 2281, 382, 257, 2445, 295, 264, 9834, 11, 51132, 51132, 457, 300, 311, 257, 2121, 1389, 11, 51200, 51200, 293, 300, 311, 264, 1389, 337, 264, 43276, 2044, 13, 51280, 51356, 400, 550, 512, 295, 552, 362, 257, 19362, 10270, 11, 51504, 51504, 411, 264, 28822, 4470, 11, 51560, 51560, 293, 512, 295, 552, 362, 364, 13785, 10270, 11, 51640, 51640, 411, 264, 3565, 295, 264, 2281, 13, 51840], "temperature": 0.0, "avg_logprob": -0.3328608703613281, "compression_ratio": 1.9059829059829059, "no_speech_prob": 7.645546247658785e-06}, {"id": 960, "seek": 532546, "start": 5349.38, "end": 5350.9800000000005, "text": " and some of them have an infinite margin,", "tokens": [50364, 467, 1177, 380, 589, 1009, 13, 50482, 50482, 509, 362, 281, 1715, 264, 3479, 50524, 50524, 370, 300, 341, 4470, 815, 589, 337, 300, 1185, 13, 50700, 50700, 440, 43276, 2044, 4470, 775, 406, 589, 294, 2674, 13, 50878, 50878, 467, 787, 1985, 498, 291, 362, 257, 8213, 13075, 2144, 51006, 51006, 295, 428, 2281, 382, 257, 2445, 295, 264, 9834, 11, 51132, 51132, 457, 300, 311, 257, 2121, 1389, 11, 51200, 51200, 293, 300, 311, 264, 1389, 337, 264, 43276, 2044, 13, 51280, 51356, 400, 550, 512, 295, 552, 362, 257, 19362, 10270, 11, 51504, 51504, 411, 264, 28822, 4470, 11, 51560, 51560, 293, 512, 295, 552, 362, 364, 13785, 10270, 11, 51640, 51640, 411, 264, 3565, 295, 264, 2281, 13, 51840], "temperature": 0.0, "avg_logprob": -0.3328608703613281, "compression_ratio": 1.9059829059829059, "no_speech_prob": 7.645546247658785e-06}, {"id": 961, "seek": 535098, "start": 5350.98, "end": 5355.98, "text": " like the log, the soft hinge, if you want.", "tokens": [50364, 411, 264, 3565, 11, 264, 2787, 28822, 11, 498, 291, 528, 13, 50614, 50854, 316, 1379, 3840, 295, 729, 15352, 11, 50968, 50968, 512, 295, 729, 645, 1143, 11, 51048, 51048, 645, 14479, 294, 264, 4319, 295, 20828, 1166, 2539, 51200, 51200, 337, 6218, 11150, 3652, 13, 51284, 51284, 583, 436, 645, 14479, 949, 561, 51484, 51484, 294, 3479, 2539, 767, 658, 3102, 294, 341, 13, 51656, 51704], "temperature": 0.0, "avg_logprob": -0.38988922664097375, "compression_ratio": 1.5953757225433527, "no_speech_prob": 6.852834758319659e-06}, {"id": 962, "seek": 535098, "start": 5360.78, "end": 5363.0599999999995, "text": " A whole bunch of those losses,", "tokens": [50364, 411, 264, 3565, 11, 264, 2787, 28822, 11, 498, 291, 528, 13, 50614, 50854, 316, 1379, 3840, 295, 729, 15352, 11, 50968, 50968, 512, 295, 729, 645, 1143, 11, 51048, 51048, 645, 14479, 294, 264, 4319, 295, 20828, 1166, 2539, 51200, 51200, 337, 6218, 11150, 3652, 13, 51284, 51284, 583, 436, 645, 14479, 949, 561, 51484, 51484, 294, 3479, 2539, 767, 658, 3102, 294, 341, 13, 51656, 51704], "temperature": 0.0, "avg_logprob": -0.38988922664097375, "compression_ratio": 1.5953757225433527, "no_speech_prob": 6.852834758319659e-06}, {"id": 963, "seek": 535098, "start": 5363.0599999999995, "end": 5364.66, "text": " some of those were used,", "tokens": [50364, 411, 264, 3565, 11, 264, 2787, 28822, 11, 498, 291, 528, 13, 50614, 50854, 316, 1379, 3840, 295, 729, 15352, 11, 50968, 50968, 512, 295, 729, 645, 1143, 11, 51048, 51048, 645, 14479, 294, 264, 4319, 295, 20828, 1166, 2539, 51200, 51200, 337, 6218, 11150, 3652, 13, 51284, 51284, 583, 436, 645, 14479, 949, 561, 51484, 51484, 294, 3479, 2539, 767, 658, 3102, 294, 341, 13, 51656, 51704], "temperature": 0.0, "avg_logprob": -0.38988922664097375, "compression_ratio": 1.5953757225433527, "no_speech_prob": 6.852834758319659e-06}, {"id": 964, "seek": 535098, "start": 5364.66, "end": 5367.7, "text": " were invented in the context of discriminative learning", "tokens": [50364, 411, 264, 3565, 11, 264, 2787, 28822, 11, 498, 291, 528, 13, 50614, 50854, 316, 1379, 3840, 295, 729, 15352, 11, 50968, 50968, 512, 295, 729, 645, 1143, 11, 51048, 51048, 645, 14479, 294, 264, 4319, 295, 20828, 1166, 2539, 51200, 51200, 337, 6218, 11150, 3652, 13, 51284, 51284, 583, 436, 645, 14479, 949, 561, 51484, 51484, 294, 3479, 2539, 767, 658, 3102, 294, 341, 13, 51656, 51704], "temperature": 0.0, "avg_logprob": -0.38988922664097375, "compression_ratio": 1.5953757225433527, "no_speech_prob": 6.852834758319659e-06}, {"id": 965, "seek": 535098, "start": 5367.7, "end": 5369.379999999999, "text": " for speech recognition systems.", "tokens": [50364, 411, 264, 3565, 11, 264, 2787, 28822, 11, 498, 291, 528, 13, 50614, 50854, 316, 1379, 3840, 295, 729, 15352, 11, 50968, 50968, 512, 295, 729, 645, 1143, 11, 51048, 51048, 645, 14479, 294, 264, 4319, 295, 20828, 1166, 2539, 51200, 51200, 337, 6218, 11150, 3652, 13, 51284, 51284, 583, 436, 645, 14479, 949, 561, 51484, 51484, 294, 3479, 2539, 767, 658, 3102, 294, 341, 13, 51656, 51704], "temperature": 0.0, "avg_logprob": -0.38988922664097375, "compression_ratio": 1.5953757225433527, "no_speech_prob": 6.852834758319659e-06}, {"id": 966, "seek": 535098, "start": 5369.379999999999, "end": 5373.379999999999, "text": " But they were invented before people", "tokens": [50364, 411, 264, 3565, 11, 264, 2787, 28822, 11, 498, 291, 528, 13, 50614, 50854, 316, 1379, 3840, 295, 729, 15352, 11, 50968, 50968, 512, 295, 729, 645, 1143, 11, 51048, 51048, 645, 14479, 294, 264, 4319, 295, 20828, 1166, 2539, 51200, 51200, 337, 6218, 11150, 3652, 13, 51284, 51284, 583, 436, 645, 14479, 949, 561, 51484, 51484, 294, 3479, 2539, 767, 658, 3102, 294, 341, 13, 51656, 51704], "temperature": 0.0, "avg_logprob": -0.38988922664097375, "compression_ratio": 1.5953757225433527, "no_speech_prob": 6.852834758319659e-06}, {"id": 967, "seek": 535098, "start": 5373.379999999999, "end": 5376.82, "text": " in machine learning actually got interested in this.", "tokens": [50364, 411, 264, 3565, 11, 264, 2787, 28822, 11, 498, 291, 528, 13, 50614, 50854, 316, 1379, 3840, 295, 729, 15352, 11, 50968, 50968, 512, 295, 729, 645, 1143, 11, 51048, 51048, 645, 14479, 294, 264, 4319, 295, 20828, 1166, 2539, 51200, 51200, 337, 6218, 11150, 3652, 13, 51284, 51284, 583, 436, 645, 14479, 949, 561, 51484, 51484, 294, 3479, 2539, 767, 658, 3102, 294, 341, 13, 51656, 51704], "temperature": 0.0, "avg_logprob": -0.38988922664097375, "compression_ratio": 1.5953757225433527, "no_speech_prob": 6.852834758319659e-06}, {"id": 968, "seek": 537682, "start": 5376.82, "end": 5380.139999999999, "text": " The question would be how you find the y bar.", "tokens": [50364, 440, 1168, 576, 312, 577, 291, 915, 264, 288, 2159, 13, 50530, 50530, 407, 498, 291, 362, 257, 27706, 3089, 11, 50628, 50628, 321, 393, 915, 2935, 264, 7285, 2158, 11, 50776, 50776, 457, 5911, 11, 366, 321, 2614, 16235, 23475, 30, 50896, 51144, 1779, 11, 370, 498, 288, 307, 10957, 11, 51298, 51298, 550, 456, 307, 572, 1850, 7123, 51450, 51450, 337, 437, 307, 264, 881, 766, 2029, 18424, 1867, 13, 51582, 51582, 509, 576, 362, 281, 6964, 512, 1333, 295, 4560, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.34287321156468886, "compression_ratio": 1.5603864734299517, "no_speech_prob": 1.496965796832228e-05}, {"id": 969, "seek": 537682, "start": 5380.139999999999, "end": 5382.099999999999, "text": " So if you have a discrete code,", "tokens": [50364, 440, 1168, 576, 312, 577, 291, 915, 264, 288, 2159, 13, 50530, 50530, 407, 498, 291, 362, 257, 27706, 3089, 11, 50628, 50628, 321, 393, 915, 2935, 264, 7285, 2158, 11, 50776, 50776, 457, 5911, 11, 366, 321, 2614, 16235, 23475, 30, 50896, 51144, 1779, 11, 370, 498, 288, 307, 10957, 11, 51298, 51298, 550, 456, 307, 572, 1850, 7123, 51450, 51450, 337, 437, 307, 264, 881, 766, 2029, 18424, 1867, 13, 51582, 51582, 509, 576, 362, 281, 6964, 512, 1333, 295, 4560, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.34287321156468886, "compression_ratio": 1.5603864734299517, "no_speech_prob": 1.496965796832228e-05}, {"id": 970, "seek": 537682, "start": 5382.099999999999, "end": 5385.0599999999995, "text": " we can find simply the minimum value,", "tokens": [50364, 440, 1168, 576, 312, 577, 291, 915, 264, 288, 2159, 13, 50530, 50530, 407, 498, 291, 362, 257, 27706, 3089, 11, 50628, 50628, 321, 393, 915, 2935, 264, 7285, 2158, 11, 50776, 50776, 457, 5911, 11, 366, 321, 2614, 16235, 23475, 30, 50896, 51144, 1779, 11, 370, 498, 288, 307, 10957, 11, 51298, 51298, 550, 456, 307, 572, 1850, 7123, 51450, 51450, 337, 437, 307, 264, 881, 766, 2029, 18424, 1867, 13, 51582, 51582, 509, 576, 362, 281, 6964, 512, 1333, 295, 4560, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.34287321156468886, "compression_ratio": 1.5603864734299517, "no_speech_prob": 1.496965796832228e-05}, {"id": 971, "seek": 537682, "start": 5385.0599999999995, "end": 5387.46, "text": " but otherwise, are we running gradient descent?", "tokens": [50364, 440, 1168, 576, 312, 577, 291, 915, 264, 288, 2159, 13, 50530, 50530, 407, 498, 291, 362, 257, 27706, 3089, 11, 50628, 50628, 321, 393, 915, 2935, 264, 7285, 2158, 11, 50776, 50776, 457, 5911, 11, 366, 321, 2614, 16235, 23475, 30, 50896, 51144, 1779, 11, 370, 498, 288, 307, 10957, 11, 51298, 51298, 550, 456, 307, 572, 1850, 7123, 51450, 51450, 337, 437, 307, 264, 881, 766, 2029, 18424, 1867, 13, 51582, 51582, 509, 576, 362, 281, 6964, 512, 1333, 295, 4560, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.34287321156468886, "compression_ratio": 1.5603864734299517, "no_speech_prob": 1.496965796832228e-05}, {"id": 972, "seek": 537682, "start": 5392.42, "end": 5395.5, "text": " Right, so if y is continuous,", "tokens": [50364, 440, 1168, 576, 312, 577, 291, 915, 264, 288, 2159, 13, 50530, 50530, 407, 498, 291, 362, 257, 27706, 3089, 11, 50628, 50628, 321, 393, 915, 2935, 264, 7285, 2158, 11, 50776, 50776, 457, 5911, 11, 366, 321, 2614, 16235, 23475, 30, 50896, 51144, 1779, 11, 370, 498, 288, 307, 10957, 11, 51298, 51298, 550, 456, 307, 572, 1850, 7123, 51450, 51450, 337, 437, 307, 264, 881, 766, 2029, 18424, 1867, 13, 51582, 51582, 509, 576, 362, 281, 6964, 512, 1333, 295, 4560, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.34287321156468886, "compression_ratio": 1.5603864734299517, "no_speech_prob": 1.496965796832228e-05}, {"id": 973, "seek": 537682, "start": 5395.5, "end": 5398.54, "text": " then there is no clear definition", "tokens": [50364, 440, 1168, 576, 312, 577, 291, 915, 264, 288, 2159, 13, 50530, 50530, 407, 498, 291, 362, 257, 27706, 3089, 11, 50628, 50628, 321, 393, 915, 2935, 264, 7285, 2158, 11, 50776, 50776, 457, 5911, 11, 366, 321, 2614, 16235, 23475, 30, 50896, 51144, 1779, 11, 370, 498, 288, 307, 10957, 11, 51298, 51298, 550, 456, 307, 572, 1850, 7123, 51450, 51450, 337, 437, 307, 264, 881, 766, 2029, 18424, 1867, 13, 51582, 51582, 509, 576, 362, 281, 6964, 512, 1333, 295, 4560, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.34287321156468886, "compression_ratio": 1.5603864734299517, "no_speech_prob": 1.496965796832228e-05}, {"id": 974, "seek": 537682, "start": 5398.54, "end": 5401.179999999999, "text": " for what is the most offending incorrect answer.", "tokens": [50364, 440, 1168, 576, 312, 577, 291, 915, 264, 288, 2159, 13, 50530, 50530, 407, 498, 291, 362, 257, 27706, 3089, 11, 50628, 50628, 321, 393, 915, 2935, 264, 7285, 2158, 11, 50776, 50776, 457, 5911, 11, 366, 321, 2614, 16235, 23475, 30, 50896, 51144, 1779, 11, 370, 498, 288, 307, 10957, 11, 51298, 51298, 550, 456, 307, 572, 1850, 7123, 51450, 51450, 337, 437, 307, 264, 881, 766, 2029, 18424, 1867, 13, 51582, 51582, 509, 576, 362, 281, 6964, 512, 1333, 295, 4560, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.34287321156468886, "compression_ratio": 1.5603864734299517, "no_speech_prob": 1.496965796832228e-05}, {"id": 975, "seek": 537682, "start": 5401.179999999999, "end": 5405.099999999999, "text": " You would have to define some sort of distance", "tokens": [50364, 440, 1168, 576, 312, 577, 291, 915, 264, 288, 2159, 13, 50530, 50530, 407, 498, 291, 362, 257, 27706, 3089, 11, 50628, 50628, 321, 393, 915, 2935, 264, 7285, 2158, 11, 50776, 50776, 457, 5911, 11, 366, 321, 2614, 16235, 23475, 30, 50896, 51144, 1779, 11, 370, 498, 288, 307, 10957, 11, 51298, 51298, 550, 456, 307, 572, 1850, 7123, 51450, 51450, 337, 437, 307, 264, 881, 766, 2029, 18424, 1867, 13, 51582, 51582, 509, 576, 362, 281, 6964, 512, 1333, 295, 4560, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.34287321156468886, "compression_ratio": 1.5603864734299517, "no_speech_prob": 1.496965796832228e-05}, {"id": 976, "seek": 540510, "start": 5405.1, "end": 5407.06, "text": " around the correct answer,", "tokens": [50364, 926, 264, 3006, 1867, 11, 50462, 50462, 3673, 597, 291, 1949, 364, 1867, 281, 312, 18424, 13, 50624, 50668, 1033, 11, 370, 337, 1365, 11, 50710, 50710, 291, 366, 294, 257, 10957, 2281, 9661, 13, 50854, 50854, 821, 311, 472, 3097, 6889, 510, 13, 50958, 50958, 509, 1948, 652, 264, 2281, 295, 300, 3097, 6889, 1359, 11, 51082, 51082, 1858, 1547, 11, 14722, 264, 2281, 807, 428, 18161, 2533, 11, 51258, 51258, 2944, 309, 760, 11, 646, 48256, 11, 5623, 264, 3364, 51366, 51366, 370, 300, 264, 2281, 1709, 760, 11, 1858, 1547, 13, 51466, 51466, 823, 264, 18424, 1867, 11, 51548, 51548, 498, 291, 747, 364, 1867, 300, 311, 445, 733, 295, 51654, 51654, 1030, 19318, 322, 300, 1252, 295, 300, 11, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.3246746513787217, "compression_ratio": 1.8106060606060606, "no_speech_prob": 4.710575467470335e-06}, {"id": 977, "seek": 540510, "start": 5407.06, "end": 5410.3, "text": " above which you consider an answer to be incorrect.", "tokens": [50364, 926, 264, 3006, 1867, 11, 50462, 50462, 3673, 597, 291, 1949, 364, 1867, 281, 312, 18424, 13, 50624, 50668, 1033, 11, 370, 337, 1365, 11, 50710, 50710, 291, 366, 294, 257, 10957, 2281, 9661, 13, 50854, 50854, 821, 311, 472, 3097, 6889, 510, 13, 50958, 50958, 509, 1948, 652, 264, 2281, 295, 300, 3097, 6889, 1359, 11, 51082, 51082, 1858, 1547, 11, 14722, 264, 2281, 807, 428, 18161, 2533, 11, 51258, 51258, 2944, 309, 760, 11, 646, 48256, 11, 5623, 264, 3364, 51366, 51366, 370, 300, 264, 2281, 1709, 760, 11, 1858, 1547, 13, 51466, 51466, 823, 264, 18424, 1867, 11, 51548, 51548, 498, 291, 747, 364, 1867, 300, 311, 445, 733, 295, 51654, 51654, 1030, 19318, 322, 300, 1252, 295, 300, 11, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.3246746513787217, "compression_ratio": 1.8106060606060606, "no_speech_prob": 4.710575467470335e-06}, {"id": 978, "seek": 540510, "start": 5411.18, "end": 5412.02, "text": " Okay, so for example,", "tokens": [50364, 926, 264, 3006, 1867, 11, 50462, 50462, 3673, 597, 291, 1949, 364, 1867, 281, 312, 18424, 13, 50624, 50668, 1033, 11, 370, 337, 1365, 11, 50710, 50710, 291, 366, 294, 257, 10957, 2281, 9661, 13, 50854, 50854, 821, 311, 472, 3097, 6889, 510, 13, 50958, 50958, 509, 1948, 652, 264, 2281, 295, 300, 3097, 6889, 1359, 11, 51082, 51082, 1858, 1547, 11, 14722, 264, 2281, 807, 428, 18161, 2533, 11, 51258, 51258, 2944, 309, 760, 11, 646, 48256, 11, 5623, 264, 3364, 51366, 51366, 370, 300, 264, 2281, 1709, 760, 11, 1858, 1547, 13, 51466, 51466, 823, 264, 18424, 1867, 11, 51548, 51548, 498, 291, 747, 364, 1867, 300, 311, 445, 733, 295, 51654, 51654, 1030, 19318, 322, 300, 1252, 295, 300, 11, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.3246746513787217, "compression_ratio": 1.8106060606060606, "no_speech_prob": 4.710575467470335e-06}, {"id": 979, "seek": 540510, "start": 5412.02, "end": 5414.900000000001, "text": " you are in a continuous energy landscape.", "tokens": [50364, 926, 264, 3006, 1867, 11, 50462, 50462, 3673, 597, 291, 1949, 364, 1867, 281, 312, 18424, 13, 50624, 50668, 1033, 11, 370, 337, 1365, 11, 50710, 50710, 291, 366, 294, 257, 10957, 2281, 9661, 13, 50854, 50854, 821, 311, 472, 3097, 6889, 510, 13, 50958, 50958, 509, 1948, 652, 264, 2281, 295, 300, 3097, 6889, 1359, 11, 51082, 51082, 1858, 1547, 11, 14722, 264, 2281, 807, 428, 18161, 2533, 11, 51258, 51258, 2944, 309, 760, 11, 646, 48256, 11, 5623, 264, 3364, 51366, 51366, 370, 300, 264, 2281, 1709, 760, 11, 1858, 1547, 13, 51466, 51466, 823, 264, 18424, 1867, 11, 51548, 51548, 498, 291, 747, 364, 1867, 300, 311, 445, 733, 295, 51654, 51654, 1030, 19318, 322, 300, 1252, 295, 300, 11, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.3246746513787217, "compression_ratio": 1.8106060606060606, "no_speech_prob": 4.710575467470335e-06}, {"id": 980, "seek": 540510, "start": 5414.900000000001, "end": 5416.9800000000005, "text": " There's one training sample here.", "tokens": [50364, 926, 264, 3006, 1867, 11, 50462, 50462, 3673, 597, 291, 1949, 364, 1867, 281, 312, 18424, 13, 50624, 50668, 1033, 11, 370, 337, 1365, 11, 50710, 50710, 291, 366, 294, 257, 10957, 2281, 9661, 13, 50854, 50854, 821, 311, 472, 3097, 6889, 510, 13, 50958, 50958, 509, 1948, 652, 264, 2281, 295, 300, 3097, 6889, 1359, 11, 51082, 51082, 1858, 1547, 11, 14722, 264, 2281, 807, 428, 18161, 2533, 11, 51258, 51258, 2944, 309, 760, 11, 646, 48256, 11, 5623, 264, 3364, 51366, 51366, 370, 300, 264, 2281, 1709, 760, 11, 1858, 1547, 13, 51466, 51466, 823, 264, 18424, 1867, 11, 51548, 51548, 498, 291, 747, 364, 1867, 300, 311, 445, 733, 295, 51654, 51654, 1030, 19318, 322, 300, 1252, 295, 300, 11, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.3246746513787217, "compression_ratio": 1.8106060606060606, "no_speech_prob": 4.710575467470335e-06}, {"id": 981, "seek": 540510, "start": 5416.9800000000005, "end": 5419.46, "text": " You wanna make the energy of that training sample small,", "tokens": [50364, 926, 264, 3006, 1867, 11, 50462, 50462, 3673, 597, 291, 1949, 364, 1867, 281, 312, 18424, 13, 50624, 50668, 1033, 11, 370, 337, 1365, 11, 50710, 50710, 291, 366, 294, 257, 10957, 2281, 9661, 13, 50854, 50854, 821, 311, 472, 3097, 6889, 510, 13, 50958, 50958, 509, 1948, 652, 264, 2281, 295, 300, 3097, 6889, 1359, 11, 51082, 51082, 1858, 1547, 11, 14722, 264, 2281, 807, 428, 18161, 2533, 11, 51258, 51258, 2944, 309, 760, 11, 646, 48256, 11, 5623, 264, 3364, 51366, 51366, 370, 300, 264, 2281, 1709, 760, 11, 1858, 1547, 13, 51466, 51466, 823, 264, 18424, 1867, 11, 51548, 51548, 498, 291, 747, 364, 1867, 300, 311, 445, 733, 295, 51654, 51654, 1030, 19318, 322, 300, 1252, 295, 300, 11, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.3246746513787217, "compression_ratio": 1.8106060606060606, "no_speech_prob": 4.710575467470335e-06}, {"id": 982, "seek": 540510, "start": 5419.46, "end": 5422.9800000000005, "text": " easy enough, compute the energy through your neural net,", "tokens": [50364, 926, 264, 3006, 1867, 11, 50462, 50462, 3673, 597, 291, 1949, 364, 1867, 281, 312, 18424, 13, 50624, 50668, 1033, 11, 370, 337, 1365, 11, 50710, 50710, 291, 366, 294, 257, 10957, 2281, 9661, 13, 50854, 50854, 821, 311, 472, 3097, 6889, 510, 13, 50958, 50958, 509, 1948, 652, 264, 2281, 295, 300, 3097, 6889, 1359, 11, 51082, 51082, 1858, 1547, 11, 14722, 264, 2281, 807, 428, 18161, 2533, 11, 51258, 51258, 2944, 309, 760, 11, 646, 48256, 11, 5623, 264, 3364, 51366, 51366, 370, 300, 264, 2281, 1709, 760, 11, 1858, 1547, 13, 51466, 51466, 823, 264, 18424, 1867, 11, 51548, 51548, 498, 291, 747, 364, 1867, 300, 311, 445, 733, 295, 51654, 51654, 1030, 19318, 322, 300, 1252, 295, 300, 11, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.3246746513787217, "compression_ratio": 1.8106060606060606, "no_speech_prob": 4.710575467470335e-06}, {"id": 983, "seek": 540510, "start": 5422.9800000000005, "end": 5425.14, "text": " push it down, back propagate, update the weight", "tokens": [50364, 926, 264, 3006, 1867, 11, 50462, 50462, 3673, 597, 291, 1949, 364, 1867, 281, 312, 18424, 13, 50624, 50668, 1033, 11, 370, 337, 1365, 11, 50710, 50710, 291, 366, 294, 257, 10957, 2281, 9661, 13, 50854, 50854, 821, 311, 472, 3097, 6889, 510, 13, 50958, 50958, 509, 1948, 652, 264, 2281, 295, 300, 3097, 6889, 1359, 11, 51082, 51082, 1858, 1547, 11, 14722, 264, 2281, 807, 428, 18161, 2533, 11, 51258, 51258, 2944, 309, 760, 11, 646, 48256, 11, 5623, 264, 3364, 51366, 51366, 370, 300, 264, 2281, 1709, 760, 11, 1858, 1547, 13, 51466, 51466, 823, 264, 18424, 1867, 11, 51548, 51548, 498, 291, 747, 364, 1867, 300, 311, 445, 733, 295, 51654, 51654, 1030, 19318, 322, 300, 1252, 295, 300, 11, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.3246746513787217, "compression_ratio": 1.8106060606060606, "no_speech_prob": 4.710575467470335e-06}, {"id": 984, "seek": 540510, "start": 5425.14, "end": 5427.14, "text": " so that the energy goes down, easy enough.", "tokens": [50364, 926, 264, 3006, 1867, 11, 50462, 50462, 3673, 597, 291, 1949, 364, 1867, 281, 312, 18424, 13, 50624, 50668, 1033, 11, 370, 337, 1365, 11, 50710, 50710, 291, 366, 294, 257, 10957, 2281, 9661, 13, 50854, 50854, 821, 311, 472, 3097, 6889, 510, 13, 50958, 50958, 509, 1948, 652, 264, 2281, 295, 300, 3097, 6889, 1359, 11, 51082, 51082, 1858, 1547, 11, 14722, 264, 2281, 807, 428, 18161, 2533, 11, 51258, 51258, 2944, 309, 760, 11, 646, 48256, 11, 5623, 264, 3364, 51366, 51366, 370, 300, 264, 2281, 1709, 760, 11, 1858, 1547, 13, 51466, 51466, 823, 264, 18424, 1867, 11, 51548, 51548, 498, 291, 747, 364, 1867, 300, 311, 445, 733, 295, 51654, 51654, 1030, 19318, 322, 300, 1252, 295, 300, 11, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.3246746513787217, "compression_ratio": 1.8106060606060606, "no_speech_prob": 4.710575467470335e-06}, {"id": 985, "seek": 540510, "start": 5427.14, "end": 5428.780000000001, "text": " Now the incorrect answer,", "tokens": [50364, 926, 264, 3006, 1867, 11, 50462, 50462, 3673, 597, 291, 1949, 364, 1867, 281, 312, 18424, 13, 50624, 50668, 1033, 11, 370, 337, 1365, 11, 50710, 50710, 291, 366, 294, 257, 10957, 2281, 9661, 13, 50854, 50854, 821, 311, 472, 3097, 6889, 510, 13, 50958, 50958, 509, 1948, 652, 264, 2281, 295, 300, 3097, 6889, 1359, 11, 51082, 51082, 1858, 1547, 11, 14722, 264, 2281, 807, 428, 18161, 2533, 11, 51258, 51258, 2944, 309, 760, 11, 646, 48256, 11, 5623, 264, 3364, 51366, 51366, 370, 300, 264, 2281, 1709, 760, 11, 1858, 1547, 13, 51466, 51466, 823, 264, 18424, 1867, 11, 51548, 51548, 498, 291, 747, 364, 1867, 300, 311, 445, 733, 295, 51654, 51654, 1030, 19318, 322, 300, 1252, 295, 300, 11, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.3246746513787217, "compression_ratio": 1.8106060606060606, "no_speech_prob": 4.710575467470335e-06}, {"id": 986, "seek": 540510, "start": 5428.780000000001, "end": 5430.900000000001, "text": " if you take an answer that's just kind of", "tokens": [50364, 926, 264, 3006, 1867, 11, 50462, 50462, 3673, 597, 291, 1949, 364, 1867, 281, 312, 18424, 13, 50624, 50668, 1033, 11, 370, 337, 1365, 11, 50710, 50710, 291, 366, 294, 257, 10957, 2281, 9661, 13, 50854, 50854, 821, 311, 472, 3097, 6889, 510, 13, 50958, 50958, 509, 1948, 652, 264, 2281, 295, 300, 3097, 6889, 1359, 11, 51082, 51082, 1858, 1547, 11, 14722, 264, 2281, 807, 428, 18161, 2533, 11, 51258, 51258, 2944, 309, 760, 11, 646, 48256, 11, 5623, 264, 3364, 51366, 51366, 370, 300, 264, 2281, 1709, 760, 11, 1858, 1547, 13, 51466, 51466, 823, 264, 18424, 1867, 11, 51548, 51548, 498, 291, 747, 364, 1867, 300, 311, 445, 733, 295, 51654, 51654, 1030, 19318, 322, 300, 1252, 295, 300, 11, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.3246746513787217, "compression_ratio": 1.8106060606060606, "no_speech_prob": 4.710575467470335e-06}, {"id": 987, "seek": 540510, "start": 5430.900000000001, "end": 5433.02, "text": " etched on that side of that,", "tokens": [50364, 926, 264, 3006, 1867, 11, 50462, 50462, 3673, 597, 291, 1949, 364, 1867, 281, 312, 18424, 13, 50624, 50668, 1033, 11, 370, 337, 1365, 11, 50710, 50710, 291, 366, 294, 257, 10957, 2281, 9661, 13, 50854, 50854, 821, 311, 472, 3097, 6889, 510, 13, 50958, 50958, 509, 1948, 652, 264, 2281, 295, 300, 3097, 6889, 1359, 11, 51082, 51082, 1858, 1547, 11, 14722, 264, 2281, 807, 428, 18161, 2533, 11, 51258, 51258, 2944, 309, 760, 11, 646, 48256, 11, 5623, 264, 3364, 51366, 51366, 370, 300, 264, 2281, 1709, 760, 11, 1858, 1547, 13, 51466, 51466, 823, 264, 18424, 1867, 11, 51548, 51548, 498, 291, 747, 364, 1867, 300, 311, 445, 733, 295, 51654, 51654, 1030, 19318, 322, 300, 1252, 295, 300, 11, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.3246746513787217, "compression_ratio": 1.8106060606060606, "no_speech_prob": 4.710575467470335e-06}, {"id": 988, "seek": 543302, "start": 5433.02, "end": 5438.02, "text": " and you push up, your energy surface might be a little stiff", "tokens": [50364, 293, 291, 2944, 493, 11, 428, 2281, 3753, 1062, 312, 257, 707, 15451, 50614, 50618, 570, 309, 311, 40610, 538, 257, 13075, 1602, 18161, 2533, 11, 50754, 50754, 370, 300, 815, 406, 312, 1944, 13, 50828, 50828, 407, 291, 1391, 528, 281, 362, 257, 18424, 1867, 51008, 51008, 300, 311, 1596, 257, 857, 2380, 300, 291, 434, 799, 2944, 493, 13, 51206, 51286, 400, 370, 300, 311, 577, 291, 6964, 11, 51358, 51358, 264, 1379, 1168, 307, 577, 291, 6964, 257, 8712, 488, 6889, 51504, 51504, 300, 291, 434, 799, 2944, 493, 13, 51574, 51574, 400, 257, 688, 295, 729, 1651, 366, 445, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.380561008631626, "compression_ratio": 1.7112068965517242, "no_speech_prob": 1.1841552804980893e-05}, {"id": 989, "seek": 543302, "start": 5438.1, "end": 5440.820000000001, "text": " because it's computed by a parameterized neural net,", "tokens": [50364, 293, 291, 2944, 493, 11, 428, 2281, 3753, 1062, 312, 257, 707, 15451, 50614, 50618, 570, 309, 311, 40610, 538, 257, 13075, 1602, 18161, 2533, 11, 50754, 50754, 370, 300, 815, 406, 312, 1944, 13, 50828, 50828, 407, 291, 1391, 528, 281, 362, 257, 18424, 1867, 51008, 51008, 300, 311, 1596, 257, 857, 2380, 300, 291, 434, 799, 2944, 493, 13, 51206, 51286, 400, 370, 300, 311, 577, 291, 6964, 11, 51358, 51358, 264, 1379, 1168, 307, 577, 291, 6964, 257, 8712, 488, 6889, 51504, 51504, 300, 291, 434, 799, 2944, 493, 13, 51574, 51574, 400, 257, 688, 295, 729, 1651, 366, 445, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.380561008631626, "compression_ratio": 1.7112068965517242, "no_speech_prob": 1.1841552804980893e-05}, {"id": 990, "seek": 543302, "start": 5440.820000000001, "end": 5442.3, "text": " so that may not be possible.", "tokens": [50364, 293, 291, 2944, 493, 11, 428, 2281, 3753, 1062, 312, 257, 707, 15451, 50614, 50618, 570, 309, 311, 40610, 538, 257, 13075, 1602, 18161, 2533, 11, 50754, 50754, 370, 300, 815, 406, 312, 1944, 13, 50828, 50828, 407, 291, 1391, 528, 281, 362, 257, 18424, 1867, 51008, 51008, 300, 311, 1596, 257, 857, 2380, 300, 291, 434, 799, 2944, 493, 13, 51206, 51286, 400, 370, 300, 311, 577, 291, 6964, 11, 51358, 51358, 264, 1379, 1168, 307, 577, 291, 6964, 257, 8712, 488, 6889, 51504, 51504, 300, 291, 434, 799, 2944, 493, 13, 51574, 51574, 400, 257, 688, 295, 729, 1651, 366, 445, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.380561008631626, "compression_ratio": 1.7112068965517242, "no_speech_prob": 1.1841552804980893e-05}, {"id": 991, "seek": 543302, "start": 5442.3, "end": 5445.900000000001, "text": " So you probably want to have a incorrect answer", "tokens": [50364, 293, 291, 2944, 493, 11, 428, 2281, 3753, 1062, 312, 257, 707, 15451, 50614, 50618, 570, 309, 311, 40610, 538, 257, 13075, 1602, 18161, 2533, 11, 50754, 50754, 370, 300, 815, 406, 312, 1944, 13, 50828, 50828, 407, 291, 1391, 528, 281, 362, 257, 18424, 1867, 51008, 51008, 300, 311, 1596, 257, 857, 2380, 300, 291, 434, 799, 2944, 493, 13, 51206, 51286, 400, 370, 300, 311, 577, 291, 6964, 11, 51358, 51358, 264, 1379, 1168, 307, 577, 291, 6964, 257, 8712, 488, 6889, 51504, 51504, 300, 291, 434, 799, 2944, 493, 13, 51574, 51574, 400, 257, 688, 295, 729, 1651, 366, 445, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.380561008631626, "compression_ratio": 1.7112068965517242, "no_speech_prob": 1.1841552804980893e-05}, {"id": 992, "seek": 543302, "start": 5445.900000000001, "end": 5449.860000000001, "text": " that's quite a bit outside that you're gonna push up.", "tokens": [50364, 293, 291, 2944, 493, 11, 428, 2281, 3753, 1062, 312, 257, 707, 15451, 50614, 50618, 570, 309, 311, 40610, 538, 257, 13075, 1602, 18161, 2533, 11, 50754, 50754, 370, 300, 815, 406, 312, 1944, 13, 50828, 50828, 407, 291, 1391, 528, 281, 362, 257, 18424, 1867, 51008, 51008, 300, 311, 1596, 257, 857, 2380, 300, 291, 434, 799, 2944, 493, 13, 51206, 51286, 400, 370, 300, 311, 577, 291, 6964, 11, 51358, 51358, 264, 1379, 1168, 307, 577, 291, 6964, 257, 8712, 488, 6889, 51504, 51504, 300, 291, 434, 799, 2944, 493, 13, 51574, 51574, 400, 257, 688, 295, 729, 1651, 366, 445, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.380561008631626, "compression_ratio": 1.7112068965517242, "no_speech_prob": 1.1841552804980893e-05}, {"id": 993, "seek": 543302, "start": 5451.46, "end": 5452.900000000001, "text": " And so that's how you define,", "tokens": [50364, 293, 291, 2944, 493, 11, 428, 2281, 3753, 1062, 312, 257, 707, 15451, 50614, 50618, 570, 309, 311, 40610, 538, 257, 13075, 1602, 18161, 2533, 11, 50754, 50754, 370, 300, 815, 406, 312, 1944, 13, 50828, 50828, 407, 291, 1391, 528, 281, 362, 257, 18424, 1867, 51008, 51008, 300, 311, 1596, 257, 857, 2380, 300, 291, 434, 799, 2944, 493, 13, 51206, 51286, 400, 370, 300, 311, 577, 291, 6964, 11, 51358, 51358, 264, 1379, 1168, 307, 577, 291, 6964, 257, 8712, 488, 6889, 51504, 51504, 300, 291, 434, 799, 2944, 493, 13, 51574, 51574, 400, 257, 688, 295, 729, 1651, 366, 445, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.380561008631626, "compression_ratio": 1.7112068965517242, "no_speech_prob": 1.1841552804980893e-05}, {"id": 994, "seek": 543302, "start": 5452.900000000001, "end": 5455.820000000001, "text": " the whole question is how you define a contrastive sample", "tokens": [50364, 293, 291, 2944, 493, 11, 428, 2281, 3753, 1062, 312, 257, 707, 15451, 50614, 50618, 570, 309, 311, 40610, 538, 257, 13075, 1602, 18161, 2533, 11, 50754, 50754, 370, 300, 815, 406, 312, 1944, 13, 50828, 50828, 407, 291, 1391, 528, 281, 362, 257, 18424, 1867, 51008, 51008, 300, 311, 1596, 257, 857, 2380, 300, 291, 434, 799, 2944, 493, 13, 51206, 51286, 400, 370, 300, 311, 577, 291, 6964, 11, 51358, 51358, 264, 1379, 1168, 307, 577, 291, 6964, 257, 8712, 488, 6889, 51504, 51504, 300, 291, 434, 799, 2944, 493, 13, 51574, 51574, 400, 257, 688, 295, 729, 1651, 366, 445, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.380561008631626, "compression_ratio": 1.7112068965517242, "no_speech_prob": 1.1841552804980893e-05}, {"id": 995, "seek": 543302, "start": 5455.820000000001, "end": 5457.22, "text": " that you're gonna push up.", "tokens": [50364, 293, 291, 2944, 493, 11, 428, 2281, 3753, 1062, 312, 257, 707, 15451, 50614, 50618, 570, 309, 311, 40610, 538, 257, 13075, 1602, 18161, 2533, 11, 50754, 50754, 370, 300, 815, 406, 312, 1944, 13, 50828, 50828, 407, 291, 1391, 528, 281, 362, 257, 18424, 1867, 51008, 51008, 300, 311, 1596, 257, 857, 2380, 300, 291, 434, 799, 2944, 493, 13, 51206, 51286, 400, 370, 300, 311, 577, 291, 6964, 11, 51358, 51358, 264, 1379, 1168, 307, 577, 291, 6964, 257, 8712, 488, 6889, 51504, 51504, 300, 291, 434, 799, 2944, 493, 13, 51574, 51574, 400, 257, 688, 295, 729, 1651, 366, 445, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.380561008631626, "compression_ratio": 1.7112068965517242, "no_speech_prob": 1.1841552804980893e-05}, {"id": 996, "seek": 543302, "start": 5457.22, "end": 5460.5, "text": " And a lot of those questions are just", "tokens": [50364, 293, 291, 2944, 493, 11, 428, 2281, 3753, 1062, 312, 257, 707, 15451, 50614, 50618, 570, 309, 311, 40610, 538, 257, 13075, 1602, 18161, 2533, 11, 50754, 50754, 370, 300, 815, 406, 312, 1944, 13, 50828, 50828, 407, 291, 1391, 528, 281, 362, 257, 18424, 1867, 51008, 51008, 300, 311, 1596, 257, 857, 2380, 300, 291, 434, 799, 2944, 493, 13, 51206, 51286, 400, 370, 300, 311, 577, 291, 6964, 11, 51358, 51358, 264, 1379, 1168, 307, 577, 291, 6964, 257, 8712, 488, 6889, 51504, 51504, 300, 291, 434, 799, 2944, 493, 13, 51574, 51574, 400, 257, 688, 295, 729, 1651, 366, 445, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.380561008631626, "compression_ratio": 1.7112068965517242, "no_speech_prob": 1.1841552804980893e-05}, {"id": 997, "seek": 546050, "start": 5460.5, "end": 5464.02, "text": " and a lot of those objective functions here,", "tokens": [50364, 293, 257, 688, 295, 729, 10024, 6828, 510, 11, 50540, 50540, 729, 1036, 6828, 764, 257, 2167, 11, 50684, 50810, 398, 2159, 11, 3671, 6889, 11, 50958, 50958, 457, 456, 307, 572, 2199, 11, 51154, 51154, 2167, 3006, 636, 295, 8867, 341, 398, 2159, 13, 51298, 51298, 509, 393, 3811, 11, 4098, 294, 264, 733, 295, 11, 51492, 51492, 294, 264, 1333, 295, 10957, 1389, 420, 294, 264, 1389, 689, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.2845062307409338, "compression_ratio": 1.6280487804878048, "no_speech_prob": 4.71074235974811e-06}, {"id": 998, "seek": 546050, "start": 5464.02, "end": 5466.9, "text": " those last functions use a single,", "tokens": [50364, 293, 257, 688, 295, 729, 10024, 6828, 510, 11, 50540, 50540, 729, 1036, 6828, 764, 257, 2167, 11, 50684, 50810, 398, 2159, 11, 3671, 6889, 11, 50958, 50958, 457, 456, 307, 572, 2199, 11, 51154, 51154, 2167, 3006, 636, 295, 8867, 341, 398, 2159, 13, 51298, 51298, 509, 393, 3811, 11, 4098, 294, 264, 733, 295, 11, 51492, 51492, 294, 264, 1333, 295, 10957, 1389, 420, 294, 264, 1389, 689, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.2845062307409338, "compression_ratio": 1.6280487804878048, "no_speech_prob": 4.71074235974811e-06}, {"id": 999, "seek": 546050, "start": 5469.42, "end": 5472.38, "text": " Y bar, negative sample,", "tokens": [50364, 293, 257, 688, 295, 729, 10024, 6828, 510, 11, 50540, 50540, 729, 1036, 6828, 764, 257, 2167, 11, 50684, 50810, 398, 2159, 11, 3671, 6889, 11, 50958, 50958, 457, 456, 307, 572, 2199, 11, 51154, 51154, 2167, 3006, 636, 295, 8867, 341, 398, 2159, 13, 51298, 51298, 509, 393, 3811, 11, 4098, 294, 264, 733, 295, 11, 51492, 51492, 294, 264, 1333, 295, 10957, 1389, 420, 294, 264, 1389, 689, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.2845062307409338, "compression_ratio": 1.6280487804878048, "no_speech_prob": 4.71074235974811e-06}, {"id": 1000, "seek": 546050, "start": 5472.38, "end": 5476.3, "text": " but there is no simple,", "tokens": [50364, 293, 257, 688, 295, 729, 10024, 6828, 510, 11, 50540, 50540, 729, 1036, 6828, 764, 257, 2167, 11, 50684, 50810, 398, 2159, 11, 3671, 6889, 11, 50958, 50958, 457, 456, 307, 572, 2199, 11, 51154, 51154, 2167, 3006, 636, 295, 8867, 341, 398, 2159, 13, 51298, 51298, 509, 393, 3811, 11, 4098, 294, 264, 733, 295, 11, 51492, 51492, 294, 264, 1333, 295, 10957, 1389, 420, 294, 264, 1389, 689, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.2845062307409338, "compression_ratio": 1.6280487804878048, "no_speech_prob": 4.71074235974811e-06}, {"id": 1001, "seek": 546050, "start": 5476.3, "end": 5479.18, "text": " single correct way of picking this Y bar.", "tokens": [50364, 293, 257, 688, 295, 729, 10024, 6828, 510, 11, 50540, 50540, 729, 1036, 6828, 764, 257, 2167, 11, 50684, 50810, 398, 2159, 11, 3671, 6889, 11, 50958, 50958, 457, 456, 307, 572, 2199, 11, 51154, 51154, 2167, 3006, 636, 295, 8867, 341, 398, 2159, 13, 51298, 51298, 509, 393, 3811, 11, 4098, 294, 264, 733, 295, 11, 51492, 51492, 294, 264, 1333, 295, 10957, 1389, 420, 294, 264, 1389, 689, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.2845062307409338, "compression_ratio": 1.6280487804878048, "no_speech_prob": 4.71074235974811e-06}, {"id": 1002, "seek": 546050, "start": 5479.18, "end": 5483.06, "text": " You can imagine, particularly in the kind of,", "tokens": [50364, 293, 257, 688, 295, 729, 10024, 6828, 510, 11, 50540, 50540, 729, 1036, 6828, 764, 257, 2167, 11, 50684, 50810, 398, 2159, 11, 3671, 6889, 11, 50958, 50958, 457, 456, 307, 572, 2199, 11, 51154, 51154, 2167, 3006, 636, 295, 8867, 341, 398, 2159, 13, 51298, 51298, 509, 393, 3811, 11, 4098, 294, 264, 733, 295, 11, 51492, 51492, 294, 264, 1333, 295, 10957, 1389, 420, 294, 264, 1389, 689, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.2845062307409338, "compression_ratio": 1.6280487804878048, "no_speech_prob": 4.71074235974811e-06}, {"id": 1003, "seek": 546050, "start": 5483.06, "end": 5487.1, "text": " in the sort of continuous case or in the case where", "tokens": [50364, 293, 257, 688, 295, 729, 10024, 6828, 510, 11, 50540, 50540, 729, 1036, 6828, 764, 257, 2167, 11, 50684, 50810, 398, 2159, 11, 3671, 6889, 11, 50958, 50958, 457, 456, 307, 572, 2199, 11, 51154, 51154, 2167, 3006, 636, 295, 8867, 341, 398, 2159, 13, 51298, 51298, 509, 393, 3811, 11, 4098, 294, 264, 733, 295, 11, 51492, 51492, 294, 264, 1333, 295, 10957, 1389, 420, 294, 264, 1389, 689, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.2845062307409338, "compression_ratio": 1.6280487804878048, "no_speech_prob": 4.71074235974811e-06}, {"id": 1004, "seek": 548710, "start": 5487.1, "end": 5491.5, "text": " Y is either very, very large", "tokens": [50364, 398, 307, 2139, 588, 11, 588, 2416, 50584, 50584, 420, 10957, 293, 1090, 18795, 11, 50724, 50724, 456, 311, 572, 2199, 636, 281, 1888, 398, 2159, 13, 50950, 50950, 316, 688, 295, 11088, 321, 600, 632, 466, 8712, 488, 7150, 51090, 51090, 300, 1119, 3451, 2825, 466, 337, 318, 2918, 1130, 36170, 51204, 51204, 293, 300, 321, 2825, 466, 949, 11, 51316, 51316, 689, 1936, 577, 360, 291, 1888, 257, 398, 2159, 51390, 51390, 294, 264, 2698, 12, 48172, 24420, 1389, 30, 51506, 51506, 407, 2698, 12, 48172, 24420, 291, 500, 380, 362, 364, 1783, 11, 558, 30, 51620, 51696], "temperature": 0.0, "avg_logprob": -0.1836874718759574, "compression_ratio": 1.5818181818181818, "no_speech_prob": 1.0128514077223372e-05}, {"id": 1005, "seek": 548710, "start": 5491.5, "end": 5494.3, "text": " or continuous and high dimensional,", "tokens": [50364, 398, 307, 2139, 588, 11, 588, 2416, 50584, 50584, 420, 10957, 293, 1090, 18795, 11, 50724, 50724, 456, 311, 572, 2199, 636, 281, 1888, 398, 2159, 13, 50950, 50950, 316, 688, 295, 11088, 321, 600, 632, 466, 8712, 488, 7150, 51090, 51090, 300, 1119, 3451, 2825, 466, 337, 318, 2918, 1130, 36170, 51204, 51204, 293, 300, 321, 2825, 466, 949, 11, 51316, 51316, 689, 1936, 577, 360, 291, 1888, 257, 398, 2159, 51390, 51390, 294, 264, 2698, 12, 48172, 24420, 1389, 30, 51506, 51506, 407, 2698, 12, 48172, 24420, 291, 500, 380, 362, 364, 1783, 11, 558, 30, 51620, 51696], "temperature": 0.0, "avg_logprob": -0.1836874718759574, "compression_ratio": 1.5818181818181818, "no_speech_prob": 1.0128514077223372e-05}, {"id": 1006, "seek": 548710, "start": 5494.3, "end": 5498.820000000001, "text": " there's no simple way to pick Y bar.", "tokens": [50364, 398, 307, 2139, 588, 11, 588, 2416, 50584, 50584, 420, 10957, 293, 1090, 18795, 11, 50724, 50724, 456, 311, 572, 2199, 636, 281, 1888, 398, 2159, 13, 50950, 50950, 316, 688, 295, 11088, 321, 600, 632, 466, 8712, 488, 7150, 51090, 51090, 300, 1119, 3451, 2825, 466, 337, 318, 2918, 1130, 36170, 51204, 51204, 293, 300, 321, 2825, 466, 949, 11, 51316, 51316, 689, 1936, 577, 360, 291, 1888, 257, 398, 2159, 51390, 51390, 294, 264, 2698, 12, 48172, 24420, 1389, 30, 51506, 51506, 407, 2698, 12, 48172, 24420, 291, 500, 380, 362, 364, 1783, 11, 558, 30, 51620, 51696], "temperature": 0.0, "avg_logprob": -0.1836874718759574, "compression_ratio": 1.5818181818181818, "no_speech_prob": 1.0128514077223372e-05}, {"id": 1007, "seek": 548710, "start": 5498.820000000001, "end": 5501.620000000001, "text": " A lot of discussions we've had about contrastive methods", "tokens": [50364, 398, 307, 2139, 588, 11, 588, 2416, 50584, 50584, 420, 10957, 293, 1090, 18795, 11, 50724, 50724, 456, 311, 572, 2199, 636, 281, 1888, 398, 2159, 13, 50950, 50950, 316, 688, 295, 11088, 321, 600, 632, 466, 8712, 488, 7150, 51090, 51090, 300, 1119, 3451, 2825, 466, 337, 318, 2918, 1130, 36170, 51204, 51204, 293, 300, 321, 2825, 466, 949, 11, 51316, 51316, 689, 1936, 577, 360, 291, 1888, 257, 398, 2159, 51390, 51390, 294, 264, 2698, 12, 48172, 24420, 1389, 30, 51506, 51506, 407, 2698, 12, 48172, 24420, 291, 500, 380, 362, 364, 1783, 11, 558, 30, 51620, 51696], "temperature": 0.0, "avg_logprob": -0.1836874718759574, "compression_ratio": 1.5818181818181818, "no_speech_prob": 1.0128514077223372e-05}, {"id": 1008, "seek": 548710, "start": 5501.620000000001, "end": 5503.900000000001, "text": " that Ishan talked about for Siamese nets", "tokens": [50364, 398, 307, 2139, 588, 11, 588, 2416, 50584, 50584, 420, 10957, 293, 1090, 18795, 11, 50724, 50724, 456, 311, 572, 2199, 636, 281, 1888, 398, 2159, 13, 50950, 50950, 316, 688, 295, 11088, 321, 600, 632, 466, 8712, 488, 7150, 51090, 51090, 300, 1119, 3451, 2825, 466, 337, 318, 2918, 1130, 36170, 51204, 51204, 293, 300, 321, 2825, 466, 949, 11, 51316, 51316, 689, 1936, 577, 360, 291, 1888, 257, 398, 2159, 51390, 51390, 294, 264, 2698, 12, 48172, 24420, 1389, 30, 51506, 51506, 407, 2698, 12, 48172, 24420, 291, 500, 380, 362, 364, 1783, 11, 558, 30, 51620, 51696], "temperature": 0.0, "avg_logprob": -0.1836874718759574, "compression_ratio": 1.5818181818181818, "no_speech_prob": 1.0128514077223372e-05}, {"id": 1009, "seek": 548710, "start": 5503.900000000001, "end": 5506.14, "text": " and that we talked about before,", "tokens": [50364, 398, 307, 2139, 588, 11, 588, 2416, 50584, 50584, 420, 10957, 293, 1090, 18795, 11, 50724, 50724, 456, 311, 572, 2199, 636, 281, 1888, 398, 2159, 13, 50950, 50950, 316, 688, 295, 11088, 321, 600, 632, 466, 8712, 488, 7150, 51090, 51090, 300, 1119, 3451, 2825, 466, 337, 318, 2918, 1130, 36170, 51204, 51204, 293, 300, 321, 2825, 466, 949, 11, 51316, 51316, 689, 1936, 577, 360, 291, 1888, 257, 398, 2159, 51390, 51390, 294, 264, 2698, 12, 48172, 24420, 1389, 30, 51506, 51506, 407, 2698, 12, 48172, 24420, 291, 500, 380, 362, 364, 1783, 11, 558, 30, 51620, 51696], "temperature": 0.0, "avg_logprob": -0.1836874718759574, "compression_ratio": 1.5818181818181818, "no_speech_prob": 1.0128514077223372e-05}, {"id": 1010, "seek": 548710, "start": 5506.14, "end": 5507.620000000001, "text": " where basically how do you pick a Y bar", "tokens": [50364, 398, 307, 2139, 588, 11, 588, 2416, 50584, 50584, 420, 10957, 293, 1090, 18795, 11, 50724, 50724, 456, 311, 572, 2199, 636, 281, 1888, 398, 2159, 13, 50950, 50950, 316, 688, 295, 11088, 321, 600, 632, 466, 8712, 488, 7150, 51090, 51090, 300, 1119, 3451, 2825, 466, 337, 318, 2918, 1130, 36170, 51204, 51204, 293, 300, 321, 2825, 466, 949, 11, 51316, 51316, 689, 1936, 577, 360, 291, 1888, 257, 398, 2159, 51390, 51390, 294, 264, 2698, 12, 48172, 24420, 1389, 30, 51506, 51506, 407, 2698, 12, 48172, 24420, 291, 500, 380, 362, 364, 1783, 11, 558, 30, 51620, 51696], "temperature": 0.0, "avg_logprob": -0.1836874718759574, "compression_ratio": 1.5818181818181818, "no_speech_prob": 1.0128514077223372e-05}, {"id": 1011, "seek": 548710, "start": 5507.620000000001, "end": 5509.9400000000005, "text": " in the self-supervised case?", "tokens": [50364, 398, 307, 2139, 588, 11, 588, 2416, 50584, 50584, 420, 10957, 293, 1090, 18795, 11, 50724, 50724, 456, 311, 572, 2199, 636, 281, 1888, 398, 2159, 13, 50950, 50950, 316, 688, 295, 11088, 321, 600, 632, 466, 8712, 488, 7150, 51090, 51090, 300, 1119, 3451, 2825, 466, 337, 318, 2918, 1130, 36170, 51204, 51204, 293, 300, 321, 2825, 466, 949, 11, 51316, 51316, 689, 1936, 577, 360, 291, 1888, 257, 398, 2159, 51390, 51390, 294, 264, 2698, 12, 48172, 24420, 1389, 30, 51506, 51506, 407, 2698, 12, 48172, 24420, 291, 500, 380, 362, 364, 1783, 11, 558, 30, 51620, 51696], "temperature": 0.0, "avg_logprob": -0.1836874718759574, "compression_ratio": 1.5818181818181818, "no_speech_prob": 1.0128514077223372e-05}, {"id": 1012, "seek": 548710, "start": 5509.9400000000005, "end": 5512.22, "text": " So self-supervised you don't have an X, right?", "tokens": [50364, 398, 307, 2139, 588, 11, 588, 2416, 50584, 50584, 420, 10957, 293, 1090, 18795, 11, 50724, 50724, 456, 311, 572, 2199, 636, 281, 1888, 398, 2159, 13, 50950, 50950, 316, 688, 295, 11088, 321, 600, 632, 466, 8712, 488, 7150, 51090, 51090, 300, 1119, 3451, 2825, 466, 337, 318, 2918, 1130, 36170, 51204, 51204, 293, 300, 321, 2825, 466, 949, 11, 51316, 51316, 689, 1936, 577, 360, 291, 1888, 257, 398, 2159, 51390, 51390, 294, 264, 2698, 12, 48172, 24420, 1389, 30, 51506, 51506, 407, 2698, 12, 48172, 24420, 291, 500, 380, 362, 364, 1783, 11, 558, 30, 51620, 51696], "temperature": 0.0, "avg_logprob": -0.1836874718759574, "compression_ratio": 1.5818181818181818, "no_speech_prob": 1.0128514077223372e-05}, {"id": 1013, "seek": 551222, "start": 5512.22, "end": 5517.22, "text": " And there's many ways you can pick it up.", "tokens": [50364, 400, 456, 311, 867, 2098, 291, 393, 1888, 309, 493, 13, 50614, 50798, 467, 311, 787, 6322, 577, 281, 1888, 309, 493, 294, 733, 295, 1359, 3331, 13, 50948, 50948, 286, 445, 1948, 935, 484, 264, 8513, 510, 412, 264, 2767, 13, 51110, 51110, 407, 341, 307, 257, 733, 295, 11, 291, 393, 519, 295, 341, 51234, 51234, 382, 1333, 295, 257, 2674, 1254, 295, 1333, 295, 28822, 2010, 51484, 51622, 8712, 488, 15352, 689, 291, 362, 364, 389, 2445, 510, 11, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.13588265485541764, "compression_ratio": 1.581151832460733, "no_speech_prob": 1.1841802006529178e-05}, {"id": 1014, "seek": 551222, "start": 5520.900000000001, "end": 5523.900000000001, "text": " It's only obvious how to pick it up in kind of small cases.", "tokens": [50364, 400, 456, 311, 867, 2098, 291, 393, 1888, 309, 493, 13, 50614, 50798, 467, 311, 787, 6322, 577, 281, 1888, 309, 493, 294, 733, 295, 1359, 3331, 13, 50948, 50948, 286, 445, 1948, 935, 484, 264, 8513, 510, 412, 264, 2767, 13, 51110, 51110, 407, 341, 307, 257, 733, 295, 11, 291, 393, 519, 295, 341, 51234, 51234, 382, 1333, 295, 257, 2674, 1254, 295, 1333, 295, 28822, 2010, 51484, 51622, 8712, 488, 15352, 689, 291, 362, 364, 389, 2445, 510, 11, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.13588265485541764, "compression_ratio": 1.581151832460733, "no_speech_prob": 1.1841802006529178e-05}, {"id": 1015, "seek": 551222, "start": 5523.900000000001, "end": 5527.14, "text": " I just wanna point out the formula here at the bottom.", "tokens": [50364, 400, 456, 311, 867, 2098, 291, 393, 1888, 309, 493, 13, 50614, 50798, 467, 311, 787, 6322, 577, 281, 1888, 309, 493, 294, 733, 295, 1359, 3331, 13, 50948, 50948, 286, 445, 1948, 935, 484, 264, 8513, 510, 412, 264, 2767, 13, 51110, 51110, 407, 341, 307, 257, 733, 295, 11, 291, 393, 519, 295, 341, 51234, 51234, 382, 1333, 295, 257, 2674, 1254, 295, 1333, 295, 28822, 2010, 51484, 51622, 8712, 488, 15352, 689, 291, 362, 364, 389, 2445, 510, 11, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.13588265485541764, "compression_ratio": 1.581151832460733, "no_speech_prob": 1.1841802006529178e-05}, {"id": 1016, "seek": 551222, "start": 5527.14, "end": 5529.62, "text": " So this is a kind of, you can think of this", "tokens": [50364, 400, 456, 311, 867, 2098, 291, 393, 1888, 309, 493, 13, 50614, 50798, 467, 311, 787, 6322, 577, 281, 1888, 309, 493, 294, 733, 295, 1359, 3331, 13, 50948, 50948, 286, 445, 1948, 935, 484, 264, 8513, 510, 412, 264, 2767, 13, 51110, 51110, 407, 341, 307, 257, 733, 295, 11, 291, 393, 519, 295, 341, 51234, 51234, 382, 1333, 295, 257, 2674, 1254, 295, 1333, 295, 28822, 2010, 51484, 51622, 8712, 488, 15352, 689, 291, 362, 364, 389, 2445, 510, 11, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.13588265485541764, "compression_ratio": 1.581151832460733, "no_speech_prob": 1.1841802006529178e-05}, {"id": 1017, "seek": 551222, "start": 5529.62, "end": 5534.62, "text": " as sort of a general form of sort of hinge type", "tokens": [50364, 400, 456, 311, 867, 2098, 291, 393, 1888, 309, 493, 13, 50614, 50798, 467, 311, 787, 6322, 577, 281, 1888, 309, 493, 294, 733, 295, 1359, 3331, 13, 50948, 50948, 286, 445, 1948, 935, 484, 264, 8513, 510, 412, 264, 2767, 13, 51110, 51110, 407, 341, 307, 257, 733, 295, 11, 291, 393, 519, 295, 341, 51234, 51234, 382, 1333, 295, 257, 2674, 1254, 295, 1333, 295, 28822, 2010, 51484, 51622, 8712, 488, 15352, 689, 291, 362, 364, 389, 2445, 510, 11, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.13588265485541764, "compression_ratio": 1.581151832460733, "no_speech_prob": 1.1841802006529178e-05}, {"id": 1018, "seek": 551222, "start": 5537.38, "end": 5541.9800000000005, "text": " contrastive losses where you have an H function here,", "tokens": [50364, 400, 456, 311, 867, 2098, 291, 393, 1888, 309, 493, 13, 50614, 50798, 467, 311, 787, 6322, 577, 281, 1888, 309, 493, 294, 733, 295, 1359, 3331, 13, 50948, 50948, 286, 445, 1948, 935, 484, 264, 8513, 510, 412, 264, 2767, 13, 51110, 51110, 407, 341, 307, 257, 733, 295, 11, 291, 393, 519, 295, 341, 51234, 51234, 382, 1333, 295, 257, 2674, 1254, 295, 1333, 295, 28822, 2010, 51484, 51622, 8712, 488, 15352, 689, 291, 362, 364, 389, 2445, 510, 11, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.13588265485541764, "compression_ratio": 1.581151832460733, "no_speech_prob": 1.1841802006529178e-05}, {"id": 1019, "seek": 554198, "start": 5541.98, "end": 5544.5, "text": " you think of it as a hinge for some type.", "tokens": [50364, 291, 519, 295, 309, 382, 257, 28822, 337, 512, 2010, 13, 50490, 50556, 400, 2602, 295, 300, 28822, 11, 50634, 50634, 291, 362, 264, 2281, 295, 264, 3006, 1867, 13, 50816, 50816, 407, 300, 311, 264, 2281, 295, 264, 343, 11, 398, 11, 286, 11, 1783, 11, 286, 13, 50954, 50954, 407, 341, 307, 428, 3097, 6889, 13, 51012, 51012, 663, 311, 264, 2281, 428, 1185, 2709, 281, 264, 3097, 6889, 13, 51168, 51168, 440, 1150, 1433, 307, 264, 2281, 295, 512, 661, 1867, 398, 11, 51400, 51520, 337, 264, 912, 1783, 3097, 6889, 13, 51658, 51658, 400, 550, 456, 307, 257, 10270, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.15282798696447303, "compression_ratio": 1.8608247422680413, "no_speech_prob": 1.6440202671219595e-05}, {"id": 1020, "seek": 554198, "start": 5545.82, "end": 5547.379999999999, "text": " And instead of that hinge,", "tokens": [50364, 291, 519, 295, 309, 382, 257, 28822, 337, 512, 2010, 13, 50490, 50556, 400, 2602, 295, 300, 28822, 11, 50634, 50634, 291, 362, 264, 2281, 295, 264, 3006, 1867, 13, 50816, 50816, 407, 300, 311, 264, 2281, 295, 264, 343, 11, 398, 11, 286, 11, 1783, 11, 286, 13, 50954, 50954, 407, 341, 307, 428, 3097, 6889, 13, 51012, 51012, 663, 311, 264, 2281, 428, 1185, 2709, 281, 264, 3097, 6889, 13, 51168, 51168, 440, 1150, 1433, 307, 264, 2281, 295, 512, 661, 1867, 398, 11, 51400, 51520, 337, 264, 912, 1783, 3097, 6889, 13, 51658, 51658, 400, 550, 456, 307, 257, 10270, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.15282798696447303, "compression_ratio": 1.8608247422680413, "no_speech_prob": 1.6440202671219595e-05}, {"id": 1021, "seek": 554198, "start": 5547.379999999999, "end": 5551.0199999999995, "text": " you have the energy of the correct answer.", "tokens": [50364, 291, 519, 295, 309, 382, 257, 28822, 337, 512, 2010, 13, 50490, 50556, 400, 2602, 295, 300, 28822, 11, 50634, 50634, 291, 362, 264, 2281, 295, 264, 3006, 1867, 13, 50816, 50816, 407, 300, 311, 264, 2281, 295, 264, 343, 11, 398, 11, 286, 11, 1783, 11, 286, 13, 50954, 50954, 407, 341, 307, 428, 3097, 6889, 13, 51012, 51012, 663, 311, 264, 2281, 428, 1185, 2709, 281, 264, 3097, 6889, 13, 51168, 51168, 440, 1150, 1433, 307, 264, 2281, 295, 512, 661, 1867, 398, 11, 51400, 51520, 337, 264, 912, 1783, 3097, 6889, 13, 51658, 51658, 400, 550, 456, 307, 257, 10270, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.15282798696447303, "compression_ratio": 1.8608247422680413, "no_speech_prob": 1.6440202671219595e-05}, {"id": 1022, "seek": 554198, "start": 5551.0199999999995, "end": 5553.78, "text": " So that's the energy of the W, Y, I, X, I.", "tokens": [50364, 291, 519, 295, 309, 382, 257, 28822, 337, 512, 2010, 13, 50490, 50556, 400, 2602, 295, 300, 28822, 11, 50634, 50634, 291, 362, 264, 2281, 295, 264, 3006, 1867, 13, 50816, 50816, 407, 300, 311, 264, 2281, 295, 264, 343, 11, 398, 11, 286, 11, 1783, 11, 286, 13, 50954, 50954, 407, 341, 307, 428, 3097, 6889, 13, 51012, 51012, 663, 311, 264, 2281, 428, 1185, 2709, 281, 264, 3097, 6889, 13, 51168, 51168, 440, 1150, 1433, 307, 264, 2281, 295, 512, 661, 1867, 398, 11, 51400, 51520, 337, 264, 912, 1783, 3097, 6889, 13, 51658, 51658, 400, 550, 456, 307, 257, 10270, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.15282798696447303, "compression_ratio": 1.8608247422680413, "no_speech_prob": 1.6440202671219595e-05}, {"id": 1023, "seek": 554198, "start": 5553.78, "end": 5554.94, "text": " So this is your training sample.", "tokens": [50364, 291, 519, 295, 309, 382, 257, 28822, 337, 512, 2010, 13, 50490, 50556, 400, 2602, 295, 300, 28822, 11, 50634, 50634, 291, 362, 264, 2281, 295, 264, 3006, 1867, 13, 50816, 50816, 407, 300, 311, 264, 2281, 295, 264, 343, 11, 398, 11, 286, 11, 1783, 11, 286, 13, 50954, 50954, 407, 341, 307, 428, 3097, 6889, 13, 51012, 51012, 663, 311, 264, 2281, 428, 1185, 2709, 281, 264, 3097, 6889, 13, 51168, 51168, 440, 1150, 1433, 307, 264, 2281, 295, 512, 661, 1867, 398, 11, 51400, 51520, 337, 264, 912, 1783, 3097, 6889, 13, 51658, 51658, 400, 550, 456, 307, 257, 10270, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.15282798696447303, "compression_ratio": 1.8608247422680413, "no_speech_prob": 1.6440202671219595e-05}, {"id": 1024, "seek": 554198, "start": 5554.94, "end": 5558.0599999999995, "text": " That's the energy your system gives to the training sample.", "tokens": [50364, 291, 519, 295, 309, 382, 257, 28822, 337, 512, 2010, 13, 50490, 50556, 400, 2602, 295, 300, 28822, 11, 50634, 50634, 291, 362, 264, 2281, 295, 264, 3006, 1867, 13, 50816, 50816, 407, 300, 311, 264, 2281, 295, 264, 343, 11, 398, 11, 286, 11, 1783, 11, 286, 13, 50954, 50954, 407, 341, 307, 428, 3097, 6889, 13, 51012, 51012, 663, 311, 264, 2281, 428, 1185, 2709, 281, 264, 3097, 6889, 13, 51168, 51168, 440, 1150, 1433, 307, 264, 2281, 295, 512, 661, 1867, 398, 11, 51400, 51520, 337, 264, 912, 1783, 3097, 6889, 13, 51658, 51658, 400, 550, 456, 307, 257, 10270, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.15282798696447303, "compression_ratio": 1.8608247422680413, "no_speech_prob": 1.6440202671219595e-05}, {"id": 1025, "seek": 554198, "start": 5558.0599999999995, "end": 5562.7, "text": " The second term is the energy of some other answer Y,", "tokens": [50364, 291, 519, 295, 309, 382, 257, 28822, 337, 512, 2010, 13, 50490, 50556, 400, 2602, 295, 300, 28822, 11, 50634, 50634, 291, 362, 264, 2281, 295, 264, 3006, 1867, 13, 50816, 50816, 407, 300, 311, 264, 2281, 295, 264, 343, 11, 398, 11, 286, 11, 1783, 11, 286, 13, 50954, 50954, 407, 341, 307, 428, 3097, 6889, 13, 51012, 51012, 663, 311, 264, 2281, 428, 1185, 2709, 281, 264, 3097, 6889, 13, 51168, 51168, 440, 1150, 1433, 307, 264, 2281, 295, 512, 661, 1867, 398, 11, 51400, 51520, 337, 264, 912, 1783, 3097, 6889, 13, 51658, 51658, 400, 550, 456, 307, 257, 10270, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.15282798696447303, "compression_ratio": 1.8608247422680413, "no_speech_prob": 1.6440202671219595e-05}, {"id": 1026, "seek": 554198, "start": 5565.099999999999, "end": 5567.86, "text": " for the same X training sample.", "tokens": [50364, 291, 519, 295, 309, 382, 257, 28822, 337, 512, 2010, 13, 50490, 50556, 400, 2602, 295, 300, 28822, 11, 50634, 50634, 291, 362, 264, 2281, 295, 264, 3006, 1867, 13, 50816, 50816, 407, 300, 311, 264, 2281, 295, 264, 343, 11, 398, 11, 286, 11, 1783, 11, 286, 13, 50954, 50954, 407, 341, 307, 428, 3097, 6889, 13, 51012, 51012, 663, 311, 264, 2281, 428, 1185, 2709, 281, 264, 3097, 6889, 13, 51168, 51168, 440, 1150, 1433, 307, 264, 2281, 295, 512, 661, 1867, 398, 11, 51400, 51520, 337, 264, 912, 1783, 3097, 6889, 13, 51658, 51658, 400, 550, 456, 307, 257, 10270, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.15282798696447303, "compression_ratio": 1.8608247422680413, "no_speech_prob": 1.6440202671219595e-05}, {"id": 1027, "seek": 554198, "start": 5567.86, "end": 5569.379999999999, "text": " And then there is a margin,", "tokens": [50364, 291, 519, 295, 309, 382, 257, 28822, 337, 512, 2010, 13, 50490, 50556, 400, 2602, 295, 300, 28822, 11, 50634, 50634, 291, 362, 264, 2281, 295, 264, 3006, 1867, 13, 50816, 50816, 407, 300, 311, 264, 2281, 295, 264, 343, 11, 398, 11, 286, 11, 1783, 11, 286, 13, 50954, 50954, 407, 341, 307, 428, 3097, 6889, 13, 51012, 51012, 663, 311, 264, 2281, 428, 1185, 2709, 281, 264, 3097, 6889, 13, 51168, 51168, 440, 1150, 1433, 307, 264, 2281, 295, 512, 661, 1867, 398, 11, 51400, 51520, 337, 264, 912, 1783, 3097, 6889, 13, 51658, 51658, 400, 550, 456, 307, 257, 10270, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.15282798696447303, "compression_ratio": 1.8608247422680413, "no_speech_prob": 1.6440202671219595e-05}, {"id": 1028, "seek": 556938, "start": 5569.38, "end": 5573.86, "text": " that margin C is actually a function of Y, I and Y.", "tokens": [50364, 300, 10270, 383, 307, 767, 257, 2445, 295, 398, 11, 286, 293, 398, 13, 50588, 50588, 400, 291, 1062, 3811, 264, 10270, 307, 767, 611, 257, 2445, 50698, 50698, 295, 1783, 293, 1783, 11, 286, 13, 50748, 50830, 407, 1936, 291, 6997, 257, 10270, 382, 257, 2445, 50946, 50946, 295, 264, 4560, 1296, 264, 398, 82, 13, 51090, 51236, 400, 291, 3154, 300, 281, 11, 718, 311, 584, 257, 28822, 13, 51386, 51386, 823, 264, 551, 307, 341, 4470, 2445, 307, 2408, 1912, 670, 439, 398, 82, 13, 51556, 51556, 1692, 307, 257, 27706, 2408, 570, 398, 307, 27706, 11, 51652, 51652, 457, 291, 393, 3811, 364, 11573, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.16985597863661506, "compression_ratio": 1.7716894977168949, "no_speech_prob": 5.507329660758842e-06}, {"id": 1029, "seek": 556938, "start": 5573.86, "end": 5576.06, "text": " And you might imagine the margin is actually also a function", "tokens": [50364, 300, 10270, 383, 307, 767, 257, 2445, 295, 398, 11, 286, 293, 398, 13, 50588, 50588, 400, 291, 1062, 3811, 264, 10270, 307, 767, 611, 257, 2445, 50698, 50698, 295, 1783, 293, 1783, 11, 286, 13, 50748, 50830, 407, 1936, 291, 6997, 257, 10270, 382, 257, 2445, 50946, 50946, 295, 264, 4560, 1296, 264, 398, 82, 13, 51090, 51236, 400, 291, 3154, 300, 281, 11, 718, 311, 584, 257, 28822, 13, 51386, 51386, 823, 264, 551, 307, 341, 4470, 2445, 307, 2408, 1912, 670, 439, 398, 82, 13, 51556, 51556, 1692, 307, 257, 27706, 2408, 570, 398, 307, 27706, 11, 51652, 51652, 457, 291, 393, 3811, 364, 11573, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.16985597863661506, "compression_ratio": 1.7716894977168949, "no_speech_prob": 5.507329660758842e-06}, {"id": 1030, "seek": 556938, "start": 5576.06, "end": 5577.06, "text": " of X and X, I.", "tokens": [50364, 300, 10270, 383, 307, 767, 257, 2445, 295, 398, 11, 286, 293, 398, 13, 50588, 50588, 400, 291, 1062, 3811, 264, 10270, 307, 767, 611, 257, 2445, 50698, 50698, 295, 1783, 293, 1783, 11, 286, 13, 50748, 50830, 407, 1936, 291, 6997, 257, 10270, 382, 257, 2445, 50946, 50946, 295, 264, 4560, 1296, 264, 398, 82, 13, 51090, 51236, 400, 291, 3154, 300, 281, 11, 718, 311, 584, 257, 28822, 13, 51386, 51386, 823, 264, 551, 307, 341, 4470, 2445, 307, 2408, 1912, 670, 439, 398, 82, 13, 51556, 51556, 1692, 307, 257, 27706, 2408, 570, 398, 307, 27706, 11, 51652, 51652, 457, 291, 393, 3811, 364, 11573, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.16985597863661506, "compression_ratio": 1.7716894977168949, "no_speech_prob": 5.507329660758842e-06}, {"id": 1031, "seek": 556938, "start": 5578.7, "end": 5581.02, "text": " So basically you determine a margin as a function", "tokens": [50364, 300, 10270, 383, 307, 767, 257, 2445, 295, 398, 11, 286, 293, 398, 13, 50588, 50588, 400, 291, 1062, 3811, 264, 10270, 307, 767, 611, 257, 2445, 50698, 50698, 295, 1783, 293, 1783, 11, 286, 13, 50748, 50830, 407, 1936, 291, 6997, 257, 10270, 382, 257, 2445, 50946, 50946, 295, 264, 4560, 1296, 264, 398, 82, 13, 51090, 51236, 400, 291, 3154, 300, 281, 11, 718, 311, 584, 257, 28822, 13, 51386, 51386, 823, 264, 551, 307, 341, 4470, 2445, 307, 2408, 1912, 670, 439, 398, 82, 13, 51556, 51556, 1692, 307, 257, 27706, 2408, 570, 398, 307, 27706, 11, 51652, 51652, 457, 291, 393, 3811, 364, 11573, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.16985597863661506, "compression_ratio": 1.7716894977168949, "no_speech_prob": 5.507329660758842e-06}, {"id": 1032, "seek": 556938, "start": 5581.02, "end": 5583.900000000001, "text": " of the distance between the Ys.", "tokens": [50364, 300, 10270, 383, 307, 767, 257, 2445, 295, 398, 11, 286, 293, 398, 13, 50588, 50588, 400, 291, 1062, 3811, 264, 10270, 307, 767, 611, 257, 2445, 50698, 50698, 295, 1783, 293, 1783, 11, 286, 13, 50748, 50830, 407, 1936, 291, 6997, 257, 10270, 382, 257, 2445, 50946, 50946, 295, 264, 4560, 1296, 264, 398, 82, 13, 51090, 51236, 400, 291, 3154, 300, 281, 11, 718, 311, 584, 257, 28822, 13, 51386, 51386, 823, 264, 551, 307, 341, 4470, 2445, 307, 2408, 1912, 670, 439, 398, 82, 13, 51556, 51556, 1692, 307, 257, 27706, 2408, 570, 398, 307, 27706, 11, 51652, 51652, 457, 291, 393, 3811, 364, 11573, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.16985597863661506, "compression_ratio": 1.7716894977168949, "no_speech_prob": 5.507329660758842e-06}, {"id": 1033, "seek": 556938, "start": 5586.82, "end": 5589.82, "text": " And you feed that to, let's say a hinge.", "tokens": [50364, 300, 10270, 383, 307, 767, 257, 2445, 295, 398, 11, 286, 293, 398, 13, 50588, 50588, 400, 291, 1062, 3811, 264, 10270, 307, 767, 611, 257, 2445, 50698, 50698, 295, 1783, 293, 1783, 11, 286, 13, 50748, 50830, 407, 1936, 291, 6997, 257, 10270, 382, 257, 2445, 50946, 50946, 295, 264, 4560, 1296, 264, 398, 82, 13, 51090, 51236, 400, 291, 3154, 300, 281, 11, 718, 311, 584, 257, 28822, 13, 51386, 51386, 823, 264, 551, 307, 341, 4470, 2445, 307, 2408, 1912, 670, 439, 398, 82, 13, 51556, 51556, 1692, 307, 257, 27706, 2408, 570, 398, 307, 27706, 11, 51652, 51652, 457, 291, 393, 3811, 364, 11573, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.16985597863661506, "compression_ratio": 1.7716894977168949, "no_speech_prob": 5.507329660758842e-06}, {"id": 1034, "seek": 556938, "start": 5589.82, "end": 5593.22, "text": " Now the thing is this loss function is summed over all Ys.", "tokens": [50364, 300, 10270, 383, 307, 767, 257, 2445, 295, 398, 11, 286, 293, 398, 13, 50588, 50588, 400, 291, 1062, 3811, 264, 10270, 307, 767, 611, 257, 2445, 50698, 50698, 295, 1783, 293, 1783, 11, 286, 13, 50748, 50830, 407, 1936, 291, 6997, 257, 10270, 382, 257, 2445, 50946, 50946, 295, 264, 4560, 1296, 264, 398, 82, 13, 51090, 51236, 400, 291, 3154, 300, 281, 11, 718, 311, 584, 257, 28822, 13, 51386, 51386, 823, 264, 551, 307, 341, 4470, 2445, 307, 2408, 1912, 670, 439, 398, 82, 13, 51556, 51556, 1692, 307, 257, 27706, 2408, 570, 398, 307, 27706, 11, 51652, 51652, 457, 291, 393, 3811, 364, 11573, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.16985597863661506, "compression_ratio": 1.7716894977168949, "no_speech_prob": 5.507329660758842e-06}, {"id": 1035, "seek": 556938, "start": 5593.22, "end": 5595.14, "text": " Here is a discrete sum because Y is discrete,", "tokens": [50364, 300, 10270, 383, 307, 767, 257, 2445, 295, 398, 11, 286, 293, 398, 13, 50588, 50588, 400, 291, 1062, 3811, 264, 10270, 307, 767, 611, 257, 2445, 50698, 50698, 295, 1783, 293, 1783, 11, 286, 13, 50748, 50830, 407, 1936, 291, 6997, 257, 10270, 382, 257, 2445, 50946, 50946, 295, 264, 4560, 1296, 264, 398, 82, 13, 51090, 51236, 400, 291, 3154, 300, 281, 11, 718, 311, 584, 257, 28822, 13, 51386, 51386, 823, 264, 551, 307, 341, 4470, 2445, 307, 2408, 1912, 670, 439, 398, 82, 13, 51556, 51556, 1692, 307, 257, 27706, 2408, 570, 398, 307, 27706, 11, 51652, 51652, 457, 291, 393, 3811, 364, 11573, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.16985597863661506, "compression_ratio": 1.7716894977168949, "no_speech_prob": 5.507329660758842e-06}, {"id": 1036, "seek": 556938, "start": 5595.14, "end": 5597.66, "text": " but you can imagine an integral.", "tokens": [50364, 300, 10270, 383, 307, 767, 257, 2445, 295, 398, 11, 286, 293, 398, 13, 50588, 50588, 400, 291, 1062, 3811, 264, 10270, 307, 767, 611, 257, 2445, 50698, 50698, 295, 1783, 293, 1783, 11, 286, 13, 50748, 50830, 407, 1936, 291, 6997, 257, 10270, 382, 257, 2445, 50946, 50946, 295, 264, 4560, 1296, 264, 398, 82, 13, 51090, 51236, 400, 291, 3154, 300, 281, 11, 718, 311, 584, 257, 28822, 13, 51386, 51386, 823, 264, 551, 307, 341, 4470, 2445, 307, 2408, 1912, 670, 439, 398, 82, 13, 51556, 51556, 1692, 307, 257, 27706, 2408, 570, 398, 307, 27706, 11, 51652, 51652, 457, 291, 393, 3811, 364, 11573, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.16985597863661506, "compression_ratio": 1.7716894977168949, "no_speech_prob": 5.507329660758842e-06}, {"id": 1037, "seek": 559766, "start": 5597.66, "end": 5599.58, "text": " So this kind of loss says,", "tokens": [50364, 407, 341, 733, 295, 4470, 1619, 11, 50460, 50520, 286, 362, 364, 2281, 337, 452, 3006, 1867, 13, 50668, 50668, 286, 362, 25737, 337, 633, 661, 1867, 294, 452, 1901, 13, 50854, 50854, 400, 286, 1948, 2944, 493, 264, 2281, 295, 439, 661, 6338, 11, 51020, 51020, 457, 264, 2372, 538, 597, 286, 1948, 652, 552, 2946, 11, 51204, 51204, 264, 10270, 5946, 322, 264, 4560, 1296, 398, 293, 398, 2159, 13, 51454, 51540, 1610, 294, 341, 1389, 1296, 398, 11, 286, 11, 597, 307, 341, 11, 51672, 51672, 293, 398, 11, 597, 307, 264, 661, 398, 82, 13, 51840], "temperature": 0.0, "avg_logprob": -0.19266530579211666, "compression_ratio": 1.7170731707317073, "no_speech_prob": 8.800548130238894e-06}, {"id": 1038, "seek": 559766, "start": 5600.78, "end": 5603.74, "text": " I have an energy for my correct answer.", "tokens": [50364, 407, 341, 733, 295, 4470, 1619, 11, 50460, 50520, 286, 362, 364, 2281, 337, 452, 3006, 1867, 13, 50668, 50668, 286, 362, 25737, 337, 633, 661, 1867, 294, 452, 1901, 13, 50854, 50854, 400, 286, 1948, 2944, 493, 264, 2281, 295, 439, 661, 6338, 11, 51020, 51020, 457, 264, 2372, 538, 597, 286, 1948, 652, 552, 2946, 11, 51204, 51204, 264, 10270, 5946, 322, 264, 4560, 1296, 398, 293, 398, 2159, 13, 51454, 51540, 1610, 294, 341, 1389, 1296, 398, 11, 286, 11, 597, 307, 341, 11, 51672, 51672, 293, 398, 11, 597, 307, 264, 661, 398, 82, 13, 51840], "temperature": 0.0, "avg_logprob": -0.19266530579211666, "compression_ratio": 1.7170731707317073, "no_speech_prob": 8.800548130238894e-06}, {"id": 1039, "seek": 559766, "start": 5603.74, "end": 5607.46, "text": " I have energies for every other answer in my space.", "tokens": [50364, 407, 341, 733, 295, 4470, 1619, 11, 50460, 50520, 286, 362, 364, 2281, 337, 452, 3006, 1867, 13, 50668, 50668, 286, 362, 25737, 337, 633, 661, 1867, 294, 452, 1901, 13, 50854, 50854, 400, 286, 1948, 2944, 493, 264, 2281, 295, 439, 661, 6338, 11, 51020, 51020, 457, 264, 2372, 538, 597, 286, 1948, 652, 552, 2946, 11, 51204, 51204, 264, 10270, 5946, 322, 264, 4560, 1296, 398, 293, 398, 2159, 13, 51454, 51540, 1610, 294, 341, 1389, 1296, 398, 11, 286, 11, 597, 307, 341, 11, 51672, 51672, 293, 398, 11, 597, 307, 264, 661, 398, 82, 13, 51840], "temperature": 0.0, "avg_logprob": -0.19266530579211666, "compression_ratio": 1.7170731707317073, "no_speech_prob": 8.800548130238894e-06}, {"id": 1040, "seek": 559766, "start": 5607.46, "end": 5610.78, "text": " And I wanna push up the energy of all other answers,", "tokens": [50364, 407, 341, 733, 295, 4470, 1619, 11, 50460, 50520, 286, 362, 364, 2281, 337, 452, 3006, 1867, 13, 50668, 50668, 286, 362, 25737, 337, 633, 661, 1867, 294, 452, 1901, 13, 50854, 50854, 400, 286, 1948, 2944, 493, 264, 2281, 295, 439, 661, 6338, 11, 51020, 51020, 457, 264, 2372, 538, 597, 286, 1948, 652, 552, 2946, 11, 51204, 51204, 264, 10270, 5946, 322, 264, 4560, 1296, 398, 293, 398, 2159, 13, 51454, 51540, 1610, 294, 341, 1389, 1296, 398, 11, 286, 11, 597, 307, 341, 11, 51672, 51672, 293, 398, 11, 597, 307, 264, 661, 398, 82, 13, 51840], "temperature": 0.0, "avg_logprob": -0.19266530579211666, "compression_ratio": 1.7170731707317073, "no_speech_prob": 8.800548130238894e-06}, {"id": 1041, "seek": 559766, "start": 5610.78, "end": 5614.46, "text": " but the amount by which I wanna make them higher,", "tokens": [50364, 407, 341, 733, 295, 4470, 1619, 11, 50460, 50520, 286, 362, 364, 2281, 337, 452, 3006, 1867, 13, 50668, 50668, 286, 362, 25737, 337, 633, 661, 1867, 294, 452, 1901, 13, 50854, 50854, 400, 286, 1948, 2944, 493, 264, 2281, 295, 439, 661, 6338, 11, 51020, 51020, 457, 264, 2372, 538, 597, 286, 1948, 652, 552, 2946, 11, 51204, 51204, 264, 10270, 5946, 322, 264, 4560, 1296, 398, 293, 398, 2159, 13, 51454, 51540, 1610, 294, 341, 1389, 1296, 398, 11, 286, 11, 597, 307, 341, 11, 51672, 51672, 293, 398, 11, 597, 307, 264, 661, 398, 82, 13, 51840], "temperature": 0.0, "avg_logprob": -0.19266530579211666, "compression_ratio": 1.7170731707317073, "no_speech_prob": 8.800548130238894e-06}, {"id": 1042, "seek": 559766, "start": 5614.46, "end": 5619.46, "text": " the margin depends on the distance between Y and Y bar.", "tokens": [50364, 407, 341, 733, 295, 4470, 1619, 11, 50460, 50520, 286, 362, 364, 2281, 337, 452, 3006, 1867, 13, 50668, 50668, 286, 362, 25737, 337, 633, 661, 1867, 294, 452, 1901, 13, 50854, 50854, 400, 286, 1948, 2944, 493, 264, 2281, 295, 439, 661, 6338, 11, 51020, 51020, 457, 264, 2372, 538, 597, 286, 1948, 652, 552, 2946, 11, 51204, 51204, 264, 10270, 5946, 322, 264, 4560, 1296, 398, 293, 398, 2159, 13, 51454, 51540, 1610, 294, 341, 1389, 1296, 398, 11, 286, 11, 597, 307, 341, 11, 51672, 51672, 293, 398, 11, 597, 307, 264, 661, 398, 82, 13, 51840], "temperature": 0.0, "avg_logprob": -0.19266530579211666, "compression_ratio": 1.7170731707317073, "no_speech_prob": 8.800548130238894e-06}, {"id": 1043, "seek": 559766, "start": 5621.18, "end": 5623.82, "text": " Or in this case between Y, I, which is this,", "tokens": [50364, 407, 341, 733, 295, 4470, 1619, 11, 50460, 50520, 286, 362, 364, 2281, 337, 452, 3006, 1867, 13, 50668, 50668, 286, 362, 25737, 337, 633, 661, 1867, 294, 452, 1901, 13, 50854, 50854, 400, 286, 1948, 2944, 493, 264, 2281, 295, 439, 661, 6338, 11, 51020, 51020, 457, 264, 2372, 538, 597, 286, 1948, 652, 552, 2946, 11, 51204, 51204, 264, 10270, 5946, 322, 264, 4560, 1296, 398, 293, 398, 2159, 13, 51454, 51540, 1610, 294, 341, 1389, 1296, 398, 11, 286, 11, 597, 307, 341, 11, 51672, 51672, 293, 398, 11, 597, 307, 264, 661, 398, 82, 13, 51840], "temperature": 0.0, "avg_logprob": -0.19266530579211666, "compression_ratio": 1.7170731707317073, "no_speech_prob": 8.800548130238894e-06}, {"id": 1044, "seek": 562382, "start": 5623.82, "end": 5627.9, "text": " and Y, which is the other Ys.", "tokens": [50364, 293, 398, 11, 597, 307, 264, 661, 398, 82, 13, 50568, 50568, 1033, 11, 370, 291, 393, 3811, 300, 341, 10270, 486, 11, 50708, 50708, 486, 1813, 4356, 293, 4356, 382, 264, 732, 398, 82, 50822, 50822, 307, 799, 483, 4966, 281, 1184, 661, 13, 50876, 50876, 682, 341, 1389, 11, 291, 500, 380, 2944, 493, 886, 709, 50972, 50972, 337, 721, 300, 366, 886, 1998, 13, 51020, 51020, 400, 291, 2944, 493, 294, 16068, 281, 264, 4560, 295, 264, 398, 11, 51218, 51218, 2035, 4560, 291, 519, 307, 6854, 13, 51328, 51414, 639, 307, 295, 1164, 257, 544, 2252, 4470, 2445, 51556, 51556, 281, 19719, 13, 51620, 51722, 400, 412, 257, 565, 11, 370, 286, 1062, 751, 466, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1691251305060658, "compression_ratio": 1.67578125, "no_speech_prob": 8.267443263321184e-06}, {"id": 1045, "seek": 562382, "start": 5627.9, "end": 5630.7, "text": " Okay, so you can imagine that this margin will,", "tokens": [50364, 293, 398, 11, 597, 307, 264, 661, 398, 82, 13, 50568, 50568, 1033, 11, 370, 291, 393, 3811, 300, 341, 10270, 486, 11, 50708, 50708, 486, 1813, 4356, 293, 4356, 382, 264, 732, 398, 82, 50822, 50822, 307, 799, 483, 4966, 281, 1184, 661, 13, 50876, 50876, 682, 341, 1389, 11, 291, 500, 380, 2944, 493, 886, 709, 50972, 50972, 337, 721, 300, 366, 886, 1998, 13, 51020, 51020, 400, 291, 2944, 493, 294, 16068, 281, 264, 4560, 295, 264, 398, 11, 51218, 51218, 2035, 4560, 291, 519, 307, 6854, 13, 51328, 51414, 639, 307, 295, 1164, 257, 544, 2252, 4470, 2445, 51556, 51556, 281, 19719, 13, 51620, 51722, 400, 412, 257, 565, 11, 370, 286, 1062, 751, 466, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1691251305060658, "compression_ratio": 1.67578125, "no_speech_prob": 8.267443263321184e-06}, {"id": 1046, "seek": 562382, "start": 5630.7, "end": 5632.98, "text": " will become smaller and smaller as the two Ys", "tokens": [50364, 293, 398, 11, 597, 307, 264, 661, 398, 82, 13, 50568, 50568, 1033, 11, 370, 291, 393, 3811, 300, 341, 10270, 486, 11, 50708, 50708, 486, 1813, 4356, 293, 4356, 382, 264, 732, 398, 82, 50822, 50822, 307, 799, 483, 4966, 281, 1184, 661, 13, 50876, 50876, 682, 341, 1389, 11, 291, 500, 380, 2944, 493, 886, 709, 50972, 50972, 337, 721, 300, 366, 886, 1998, 13, 51020, 51020, 400, 291, 2944, 493, 294, 16068, 281, 264, 4560, 295, 264, 398, 11, 51218, 51218, 2035, 4560, 291, 519, 307, 6854, 13, 51328, 51414, 639, 307, 295, 1164, 257, 544, 2252, 4470, 2445, 51556, 51556, 281, 19719, 13, 51620, 51722, 400, 412, 257, 565, 11, 370, 286, 1062, 751, 466, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1691251305060658, "compression_ratio": 1.67578125, "no_speech_prob": 8.267443263321184e-06}, {"id": 1047, "seek": 562382, "start": 5632.98, "end": 5634.0599999999995, "text": " is gonna get closer to each other.", "tokens": [50364, 293, 398, 11, 597, 307, 264, 661, 398, 82, 13, 50568, 50568, 1033, 11, 370, 291, 393, 3811, 300, 341, 10270, 486, 11, 50708, 50708, 486, 1813, 4356, 293, 4356, 382, 264, 732, 398, 82, 50822, 50822, 307, 799, 483, 4966, 281, 1184, 661, 13, 50876, 50876, 682, 341, 1389, 11, 291, 500, 380, 2944, 493, 886, 709, 50972, 50972, 337, 721, 300, 366, 886, 1998, 13, 51020, 51020, 400, 291, 2944, 493, 294, 16068, 281, 264, 4560, 295, 264, 398, 11, 51218, 51218, 2035, 4560, 291, 519, 307, 6854, 13, 51328, 51414, 639, 307, 295, 1164, 257, 544, 2252, 4470, 2445, 51556, 51556, 281, 19719, 13, 51620, 51722, 400, 412, 257, 565, 11, 370, 286, 1062, 751, 466, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1691251305060658, "compression_ratio": 1.67578125, "no_speech_prob": 8.267443263321184e-06}, {"id": 1048, "seek": 562382, "start": 5634.0599999999995, "end": 5635.98, "text": " In this case, you don't push up too much", "tokens": [50364, 293, 398, 11, 597, 307, 264, 661, 398, 82, 13, 50568, 50568, 1033, 11, 370, 291, 393, 3811, 300, 341, 10270, 486, 11, 50708, 50708, 486, 1813, 4356, 293, 4356, 382, 264, 732, 398, 82, 50822, 50822, 307, 799, 483, 4966, 281, 1184, 661, 13, 50876, 50876, 682, 341, 1389, 11, 291, 500, 380, 2944, 493, 886, 709, 50972, 50972, 337, 721, 300, 366, 886, 1998, 13, 51020, 51020, 400, 291, 2944, 493, 294, 16068, 281, 264, 4560, 295, 264, 398, 11, 51218, 51218, 2035, 4560, 291, 519, 307, 6854, 13, 51328, 51414, 639, 307, 295, 1164, 257, 544, 2252, 4470, 2445, 51556, 51556, 281, 19719, 13, 51620, 51722, 400, 412, 257, 565, 11, 370, 286, 1062, 751, 466, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1691251305060658, "compression_ratio": 1.67578125, "no_speech_prob": 8.267443263321184e-06}, {"id": 1049, "seek": 562382, "start": 5635.98, "end": 5636.94, "text": " for things that are too close.", "tokens": [50364, 293, 398, 11, 597, 307, 264, 661, 398, 82, 13, 50568, 50568, 1033, 11, 370, 291, 393, 3811, 300, 341, 10270, 486, 11, 50708, 50708, 486, 1813, 4356, 293, 4356, 382, 264, 732, 398, 82, 50822, 50822, 307, 799, 483, 4966, 281, 1184, 661, 13, 50876, 50876, 682, 341, 1389, 11, 291, 500, 380, 2944, 493, 886, 709, 50972, 50972, 337, 721, 300, 366, 886, 1998, 13, 51020, 51020, 400, 291, 2944, 493, 294, 16068, 281, 264, 4560, 295, 264, 398, 11, 51218, 51218, 2035, 4560, 291, 519, 307, 6854, 13, 51328, 51414, 639, 307, 295, 1164, 257, 544, 2252, 4470, 2445, 51556, 51556, 281, 19719, 13, 51620, 51722, 400, 412, 257, 565, 11, 370, 286, 1062, 751, 466, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1691251305060658, "compression_ratio": 1.67578125, "no_speech_prob": 8.267443263321184e-06}, {"id": 1050, "seek": 562382, "start": 5636.94, "end": 5640.9, "text": " And you push up in proportion to the distance of the Y,", "tokens": [50364, 293, 398, 11, 597, 307, 264, 661, 398, 82, 13, 50568, 50568, 1033, 11, 370, 291, 393, 3811, 300, 341, 10270, 486, 11, 50708, 50708, 486, 1813, 4356, 293, 4356, 382, 264, 732, 398, 82, 50822, 50822, 307, 799, 483, 4966, 281, 1184, 661, 13, 50876, 50876, 682, 341, 1389, 11, 291, 500, 380, 2944, 493, 886, 709, 50972, 50972, 337, 721, 300, 366, 886, 1998, 13, 51020, 51020, 400, 291, 2944, 493, 294, 16068, 281, 264, 4560, 295, 264, 398, 11, 51218, 51218, 2035, 4560, 291, 519, 307, 6854, 13, 51328, 51414, 639, 307, 295, 1164, 257, 544, 2252, 4470, 2445, 51556, 51556, 281, 19719, 13, 51620, 51722, 400, 412, 257, 565, 11, 370, 286, 1062, 751, 466, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1691251305060658, "compression_ratio": 1.67578125, "no_speech_prob": 8.267443263321184e-06}, {"id": 1051, "seek": 562382, "start": 5640.9, "end": 5643.099999999999, "text": " whatever distance you think is appropriate.", "tokens": [50364, 293, 398, 11, 597, 307, 264, 661, 398, 82, 13, 50568, 50568, 1033, 11, 370, 291, 393, 3811, 300, 341, 10270, 486, 11, 50708, 50708, 486, 1813, 4356, 293, 4356, 382, 264, 732, 398, 82, 50822, 50822, 307, 799, 483, 4966, 281, 1184, 661, 13, 50876, 50876, 682, 341, 1389, 11, 291, 500, 380, 2944, 493, 886, 709, 50972, 50972, 337, 721, 300, 366, 886, 1998, 13, 51020, 51020, 400, 291, 2944, 493, 294, 16068, 281, 264, 4560, 295, 264, 398, 11, 51218, 51218, 2035, 4560, 291, 519, 307, 6854, 13, 51328, 51414, 639, 307, 295, 1164, 257, 544, 2252, 4470, 2445, 51556, 51556, 281, 19719, 13, 51620, 51722, 400, 412, 257, 565, 11, 370, 286, 1062, 751, 466, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1691251305060658, "compression_ratio": 1.67578125, "no_speech_prob": 8.267443263321184e-06}, {"id": 1052, "seek": 562382, "start": 5644.82, "end": 5647.66, "text": " This is of course a more difficult loss function", "tokens": [50364, 293, 398, 11, 597, 307, 264, 661, 398, 82, 13, 50568, 50568, 1033, 11, 370, 291, 393, 3811, 300, 341, 10270, 486, 11, 50708, 50708, 486, 1813, 4356, 293, 4356, 382, 264, 732, 398, 82, 50822, 50822, 307, 799, 483, 4966, 281, 1184, 661, 13, 50876, 50876, 682, 341, 1389, 11, 291, 500, 380, 2944, 493, 886, 709, 50972, 50972, 337, 721, 300, 366, 886, 1998, 13, 51020, 51020, 400, 291, 2944, 493, 294, 16068, 281, 264, 4560, 295, 264, 398, 11, 51218, 51218, 2035, 4560, 291, 519, 307, 6854, 13, 51328, 51414, 639, 307, 295, 1164, 257, 544, 2252, 4470, 2445, 51556, 51556, 281, 19719, 13, 51620, 51722, 400, 412, 257, 565, 11, 370, 286, 1062, 751, 466, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1691251305060658, "compression_ratio": 1.67578125, "no_speech_prob": 8.267443263321184e-06}, {"id": 1053, "seek": 562382, "start": 5647.66, "end": 5648.94, "text": " to optimize.", "tokens": [50364, 293, 398, 11, 597, 307, 264, 661, 398, 82, 13, 50568, 50568, 1033, 11, 370, 291, 393, 3811, 300, 341, 10270, 486, 11, 50708, 50708, 486, 1813, 4356, 293, 4356, 382, 264, 732, 398, 82, 50822, 50822, 307, 799, 483, 4966, 281, 1184, 661, 13, 50876, 50876, 682, 341, 1389, 11, 291, 500, 380, 2944, 493, 886, 709, 50972, 50972, 337, 721, 300, 366, 886, 1998, 13, 51020, 51020, 400, 291, 2944, 493, 294, 16068, 281, 264, 4560, 295, 264, 398, 11, 51218, 51218, 2035, 4560, 291, 519, 307, 6854, 13, 51328, 51414, 639, 307, 295, 1164, 257, 544, 2252, 4470, 2445, 51556, 51556, 281, 19719, 13, 51620, 51722, 400, 412, 257, 565, 11, 370, 286, 1062, 751, 466, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1691251305060658, "compression_ratio": 1.67578125, "no_speech_prob": 8.267443263321184e-06}, {"id": 1054, "seek": 562382, "start": 5650.98, "end": 5652.98, "text": " And at a time, so I might talk about", "tokens": [50364, 293, 398, 11, 597, 307, 264, 661, 398, 82, 13, 50568, 50568, 1033, 11, 370, 291, 393, 3811, 300, 341, 10270, 486, 11, 50708, 50708, 486, 1813, 4356, 293, 4356, 382, 264, 732, 398, 82, 50822, 50822, 307, 799, 483, 4966, 281, 1184, 661, 13, 50876, 50876, 682, 341, 1389, 11, 291, 500, 380, 2944, 493, 886, 709, 50972, 50972, 337, 721, 300, 366, 886, 1998, 13, 51020, 51020, 400, 291, 2944, 493, 294, 16068, 281, 264, 4560, 295, 264, 398, 11, 51218, 51218, 2035, 4560, 291, 519, 307, 6854, 13, 51328, 51414, 639, 307, 295, 1164, 257, 544, 2252, 4470, 2445, 51556, 51556, 281, 19719, 13, 51620, 51722, 400, 412, 257, 565, 11, 370, 286, 1062, 751, 466, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1691251305060658, "compression_ratio": 1.67578125, "no_speech_prob": 8.267443263321184e-06}, {"id": 1055, "seek": 565298, "start": 5652.98, "end": 5654.98, "text": " the structural prediction issue that I said", "tokens": [50364, 264, 15067, 17630, 2734, 300, 286, 848, 50464, 50464, 286, 390, 799, 751, 466, 412, 257, 1780, 565, 13, 50590, 50674, 2639, 544, 1168, 30, 50724, 51020, 440, 8712, 295, 7150, 1143, 337, 264, 51174, 51174, 382, 264, 2698, 46533, 2539, 10577, 11, 51326, 51326, 300, 366, 2673, 747, 4974, 11, 51480, 51596, 747, 4974, 5267, 382, 257, 3671, 5110, 13, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.38238557668832635, "compression_ratio": 1.44, "no_speech_prob": 1.5206537682388443e-05}, {"id": 1056, "seek": 565298, "start": 5654.98, "end": 5657.5, "text": " I was gonna talk about at a later time.", "tokens": [50364, 264, 15067, 17630, 2734, 300, 286, 848, 50464, 50464, 286, 390, 799, 751, 466, 412, 257, 1780, 565, 13, 50590, 50674, 2639, 544, 1168, 30, 50724, 51020, 440, 8712, 295, 7150, 1143, 337, 264, 51174, 51174, 382, 264, 2698, 46533, 2539, 10577, 11, 51326, 51326, 300, 366, 2673, 747, 4974, 11, 51480, 51596, 747, 4974, 5267, 382, 257, 3671, 5110, 13, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.38238557668832635, "compression_ratio": 1.44, "no_speech_prob": 1.5206537682388443e-05}, {"id": 1057, "seek": 565298, "start": 5659.179999999999, "end": 5660.179999999999, "text": " Any more question?", "tokens": [50364, 264, 15067, 17630, 2734, 300, 286, 848, 50464, 50464, 286, 390, 799, 751, 466, 412, 257, 1780, 565, 13, 50590, 50674, 2639, 544, 1168, 30, 50724, 51020, 440, 8712, 295, 7150, 1143, 337, 264, 51174, 51174, 382, 264, 2698, 46533, 2539, 10577, 11, 51326, 51326, 300, 366, 2673, 747, 4974, 11, 51480, 51596, 747, 4974, 5267, 382, 257, 3671, 5110, 13, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.38238557668832635, "compression_ratio": 1.44, "no_speech_prob": 1.5206537682388443e-05}, {"id": 1058, "seek": 565298, "start": 5666.099999999999, "end": 5669.179999999999, "text": " The contrast of methods used for the", "tokens": [50364, 264, 15067, 17630, 2734, 300, 286, 848, 50464, 50464, 286, 390, 799, 751, 466, 412, 257, 1780, 565, 13, 50590, 50674, 2639, 544, 1168, 30, 50724, 51020, 440, 8712, 295, 7150, 1143, 337, 264, 51174, 51174, 382, 264, 2698, 46533, 2539, 10577, 11, 51326, 51326, 300, 366, 2673, 747, 4974, 11, 51480, 51596, 747, 4974, 5267, 382, 257, 3671, 5110, 13, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.38238557668832635, "compression_ratio": 1.44, "no_speech_prob": 1.5206537682388443e-05}, {"id": 1059, "seek": 565298, "start": 5669.179999999999, "end": 5672.219999999999, "text": " as the self supervised learning papers,", "tokens": [50364, 264, 15067, 17630, 2734, 300, 286, 848, 50464, 50464, 286, 390, 799, 751, 466, 412, 257, 1780, 565, 13, 50590, 50674, 2639, 544, 1168, 30, 50724, 51020, 440, 8712, 295, 7150, 1143, 337, 264, 51174, 51174, 382, 264, 2698, 46533, 2539, 10577, 11, 51326, 51326, 300, 366, 2673, 747, 4974, 11, 51480, 51596, 747, 4974, 5267, 382, 257, 3671, 5110, 13, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.38238557668832635, "compression_ratio": 1.44, "no_speech_prob": 1.5206537682388443e-05}, {"id": 1060, "seek": 565298, "start": 5672.219999999999, "end": 5675.299999999999, "text": " that are usually take random,", "tokens": [50364, 264, 15067, 17630, 2734, 300, 286, 848, 50464, 50464, 286, 390, 799, 751, 466, 412, 257, 1780, 565, 13, 50590, 50674, 2639, 544, 1168, 30, 50724, 51020, 440, 8712, 295, 7150, 1143, 337, 264, 51174, 51174, 382, 264, 2698, 46533, 2539, 10577, 11, 51326, 51326, 300, 366, 2673, 747, 4974, 11, 51480, 51596, 747, 4974, 5267, 382, 257, 3671, 5110, 13, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.38238557668832635, "compression_ratio": 1.44, "no_speech_prob": 1.5206537682388443e-05}, {"id": 1061, "seek": 565298, "start": 5677.62, "end": 5680.58, "text": " take random images as a negative examples.", "tokens": [50364, 264, 15067, 17630, 2734, 300, 286, 848, 50464, 50464, 286, 390, 799, 751, 466, 412, 257, 1780, 565, 13, 50590, 50674, 2639, 544, 1168, 30, 50724, 51020, 440, 8712, 295, 7150, 1143, 337, 264, 51174, 51174, 382, 264, 2698, 46533, 2539, 10577, 11, 51326, 51326, 300, 366, 2673, 747, 4974, 11, 51480, 51596, 747, 4974, 5267, 382, 257, 3671, 5110, 13, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.38238557668832635, "compression_ratio": 1.44, "no_speech_prob": 1.5206537682388443e-05}, {"id": 1062, "seek": 568058, "start": 5680.58, "end": 5683.14, "text": " Do you have any idea if they use these functions", "tokens": [50364, 1144, 291, 362, 604, 1558, 498, 436, 764, 613, 6828, 50492, 50492, 293, 1564, 3031, 5120, 292, 365, 613, 30, 50630, 50630, 814, 764, 437, 733, 295, 2445, 30, 50740, 50740, 1981, 4470, 6828, 300, 291, 8825, 281, 505, 586, 13, 50942, 50942, 407, 881, 295, 552, 764, 264, 1936, 51082, 51082, 264, 3671, 22119, 4470, 510, 11, 51182, 51182, 597, 294, 341, 4831, 307, 1219, 426, 24010, 376, 13808, 13, 51342, 51342, 1033, 11, 370, 426, 4969, 300, 291, 2198, 466, 490, 398, 742, 282, 11, 51520, 51520, 300, 311, 437, 436, 764, 11, 558, 30, 51614, 51614, 814, 434, 1382, 281, 652, 264, 4560, 1296, 264, 10938, 51734, 51734, 382, 709, 382, 1944, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.19862755139668783, "compression_ratio": 1.6179775280898876, "no_speech_prob": 3.534890856826678e-05}, {"id": 1063, "seek": 568058, "start": 5683.14, "end": 5685.9, "text": " and once tried experimented with these?", "tokens": [50364, 1144, 291, 362, 604, 1558, 498, 436, 764, 613, 6828, 50492, 50492, 293, 1564, 3031, 5120, 292, 365, 613, 30, 50630, 50630, 814, 764, 437, 733, 295, 2445, 30, 50740, 50740, 1981, 4470, 6828, 300, 291, 8825, 281, 505, 586, 13, 50942, 50942, 407, 881, 295, 552, 764, 264, 1936, 51082, 51082, 264, 3671, 22119, 4470, 510, 11, 51182, 51182, 597, 294, 341, 4831, 307, 1219, 426, 24010, 376, 13808, 13, 51342, 51342, 1033, 11, 370, 426, 4969, 300, 291, 2198, 466, 490, 398, 742, 282, 11, 51520, 51520, 300, 311, 437, 436, 764, 11, 558, 30, 51614, 51614, 814, 434, 1382, 281, 652, 264, 4560, 1296, 264, 10938, 51734, 51734, 382, 709, 382, 1944, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.19862755139668783, "compression_ratio": 1.6179775280898876, "no_speech_prob": 3.534890856826678e-05}, {"id": 1064, "seek": 568058, "start": 5685.9, "end": 5688.1, "text": " They use what kind of function?", "tokens": [50364, 1144, 291, 362, 604, 1558, 498, 436, 764, 613, 6828, 50492, 50492, 293, 1564, 3031, 5120, 292, 365, 613, 30, 50630, 50630, 814, 764, 437, 733, 295, 2445, 30, 50740, 50740, 1981, 4470, 6828, 300, 291, 8825, 281, 505, 586, 13, 50942, 50942, 407, 881, 295, 552, 764, 264, 1936, 51082, 51082, 264, 3671, 22119, 4470, 510, 11, 51182, 51182, 597, 294, 341, 4831, 307, 1219, 426, 24010, 376, 13808, 13, 51342, 51342, 1033, 11, 370, 426, 4969, 300, 291, 2198, 466, 490, 398, 742, 282, 11, 51520, 51520, 300, 311, 437, 436, 764, 11, 558, 30, 51614, 51614, 814, 434, 1382, 281, 652, 264, 4560, 1296, 264, 10938, 51734, 51734, 382, 709, 382, 1944, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.19862755139668783, "compression_ratio": 1.6179775280898876, "no_speech_prob": 3.534890856826678e-05}, {"id": 1065, "seek": 568058, "start": 5688.1, "end": 5692.14, "text": " These loss functions that you explained to us now.", "tokens": [50364, 1144, 291, 362, 604, 1558, 498, 436, 764, 613, 6828, 50492, 50492, 293, 1564, 3031, 5120, 292, 365, 613, 30, 50630, 50630, 814, 764, 437, 733, 295, 2445, 30, 50740, 50740, 1981, 4470, 6828, 300, 291, 8825, 281, 505, 586, 13, 50942, 50942, 407, 881, 295, 552, 764, 264, 1936, 51082, 51082, 264, 3671, 22119, 4470, 510, 11, 51182, 51182, 597, 294, 341, 4831, 307, 1219, 426, 24010, 376, 13808, 13, 51342, 51342, 1033, 11, 370, 426, 4969, 300, 291, 2198, 466, 490, 398, 742, 282, 11, 51520, 51520, 300, 311, 437, 436, 764, 11, 558, 30, 51614, 51614, 814, 434, 1382, 281, 652, 264, 4560, 1296, 264, 10938, 51734, 51734, 382, 709, 382, 1944, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.19862755139668783, "compression_ratio": 1.6179775280898876, "no_speech_prob": 3.534890856826678e-05}, {"id": 1066, "seek": 568058, "start": 5692.14, "end": 5694.94, "text": " So most of them use the basically", "tokens": [50364, 1144, 291, 362, 604, 1558, 498, 436, 764, 613, 6828, 50492, 50492, 293, 1564, 3031, 5120, 292, 365, 613, 30, 50630, 50630, 814, 764, 437, 733, 295, 2445, 30, 50740, 50740, 1981, 4470, 6828, 300, 291, 8825, 281, 505, 586, 13, 50942, 50942, 407, 881, 295, 552, 764, 264, 1936, 51082, 51082, 264, 3671, 22119, 4470, 510, 11, 51182, 51182, 597, 294, 341, 4831, 307, 1219, 426, 24010, 376, 13808, 13, 51342, 51342, 1033, 11, 370, 426, 4969, 300, 291, 2198, 466, 490, 398, 742, 282, 11, 51520, 51520, 300, 311, 437, 436, 764, 11, 558, 30, 51614, 51614, 814, 434, 1382, 281, 652, 264, 4560, 1296, 264, 10938, 51734, 51734, 382, 709, 382, 1944, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.19862755139668783, "compression_ratio": 1.6179775280898876, "no_speech_prob": 3.534890856826678e-05}, {"id": 1067, "seek": 568058, "start": 5694.94, "end": 5696.94, "text": " the negative likelihood loss here,", "tokens": [50364, 1144, 291, 362, 604, 1558, 498, 436, 764, 613, 6828, 50492, 50492, 293, 1564, 3031, 5120, 292, 365, 613, 30, 50630, 50630, 814, 764, 437, 733, 295, 2445, 30, 50740, 50740, 1981, 4470, 6828, 300, 291, 8825, 281, 505, 586, 13, 50942, 50942, 407, 881, 295, 552, 764, 264, 1936, 51082, 51082, 264, 3671, 22119, 4470, 510, 11, 51182, 51182, 597, 294, 341, 4831, 307, 1219, 426, 24010, 376, 13808, 13, 51342, 51342, 1033, 11, 370, 426, 4969, 300, 291, 2198, 466, 490, 398, 742, 282, 11, 51520, 51520, 300, 311, 437, 436, 764, 11, 558, 30, 51614, 51614, 814, 434, 1382, 281, 652, 264, 4560, 1296, 264, 10938, 51734, 51734, 382, 709, 382, 1944, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.19862755139668783, "compression_ratio": 1.6179775280898876, "no_speech_prob": 3.534890856826678e-05}, {"id": 1068, "seek": 568058, "start": 5696.94, "end": 5700.14, "text": " which in this panel is called NLL MMI.", "tokens": [50364, 1144, 291, 362, 604, 1558, 498, 436, 764, 613, 6828, 50492, 50492, 293, 1564, 3031, 5120, 292, 365, 613, 30, 50630, 50630, 814, 764, 437, 733, 295, 2445, 30, 50740, 50740, 1981, 4470, 6828, 300, 291, 8825, 281, 505, 586, 13, 50942, 50942, 407, 881, 295, 552, 764, 264, 1936, 51082, 51082, 264, 3671, 22119, 4470, 510, 11, 51182, 51182, 597, 294, 341, 4831, 307, 1219, 426, 24010, 376, 13808, 13, 51342, 51342, 1033, 11, 370, 426, 4969, 300, 291, 2198, 466, 490, 398, 742, 282, 11, 51520, 51520, 300, 311, 437, 436, 764, 11, 558, 30, 51614, 51614, 814, 434, 1382, 281, 652, 264, 4560, 1296, 264, 10938, 51734, 51734, 382, 709, 382, 1944, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.19862755139668783, "compression_ratio": 1.6179775280898876, "no_speech_prob": 3.534890856826678e-05}, {"id": 1069, "seek": 568058, "start": 5700.14, "end": 5703.7, "text": " Okay, so NCE that you heard about from Yishan,", "tokens": [50364, 1144, 291, 362, 604, 1558, 498, 436, 764, 613, 6828, 50492, 50492, 293, 1564, 3031, 5120, 292, 365, 613, 30, 50630, 50630, 814, 764, 437, 733, 295, 2445, 30, 50740, 50740, 1981, 4470, 6828, 300, 291, 8825, 281, 505, 586, 13, 50942, 50942, 407, 881, 295, 552, 764, 264, 1936, 51082, 51082, 264, 3671, 22119, 4470, 510, 11, 51182, 51182, 597, 294, 341, 4831, 307, 1219, 426, 24010, 376, 13808, 13, 51342, 51342, 1033, 11, 370, 426, 4969, 300, 291, 2198, 466, 490, 398, 742, 282, 11, 51520, 51520, 300, 311, 437, 436, 764, 11, 558, 30, 51614, 51614, 814, 434, 1382, 281, 652, 264, 4560, 1296, 264, 10938, 51734, 51734, 382, 709, 382, 1944, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.19862755139668783, "compression_ratio": 1.6179775280898876, "no_speech_prob": 3.534890856826678e-05}, {"id": 1070, "seek": 568058, "start": 5703.7, "end": 5705.58, "text": " that's what they use, right?", "tokens": [50364, 1144, 291, 362, 604, 1558, 498, 436, 764, 613, 6828, 50492, 50492, 293, 1564, 3031, 5120, 292, 365, 613, 30, 50630, 50630, 814, 764, 437, 733, 295, 2445, 30, 50740, 50740, 1981, 4470, 6828, 300, 291, 8825, 281, 505, 586, 13, 50942, 50942, 407, 881, 295, 552, 764, 264, 1936, 51082, 51082, 264, 3671, 22119, 4470, 510, 11, 51182, 51182, 597, 294, 341, 4831, 307, 1219, 426, 24010, 376, 13808, 13, 51342, 51342, 1033, 11, 370, 426, 4969, 300, 291, 2198, 466, 490, 398, 742, 282, 11, 51520, 51520, 300, 311, 437, 436, 764, 11, 558, 30, 51614, 51614, 814, 434, 1382, 281, 652, 264, 4560, 1296, 264, 10938, 51734, 51734, 382, 709, 382, 1944, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.19862755139668783, "compression_ratio": 1.6179775280898876, "no_speech_prob": 3.534890856826678e-05}, {"id": 1071, "seek": 568058, "start": 5705.58, "end": 5707.98, "text": " They're trying to make the distance between the samples", "tokens": [50364, 1144, 291, 362, 604, 1558, 498, 436, 764, 613, 6828, 50492, 50492, 293, 1564, 3031, 5120, 292, 365, 613, 30, 50630, 50630, 814, 764, 437, 733, 295, 2445, 30, 50740, 50740, 1981, 4470, 6828, 300, 291, 8825, 281, 505, 586, 13, 50942, 50942, 407, 881, 295, 552, 764, 264, 1936, 51082, 51082, 264, 3671, 22119, 4470, 510, 11, 51182, 51182, 597, 294, 341, 4831, 307, 1219, 426, 24010, 376, 13808, 13, 51342, 51342, 1033, 11, 370, 426, 4969, 300, 291, 2198, 466, 490, 398, 742, 282, 11, 51520, 51520, 300, 311, 437, 436, 764, 11, 558, 30, 51614, 51614, 814, 434, 1382, 281, 652, 264, 4560, 1296, 264, 10938, 51734, 51734, 382, 709, 382, 1944, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.19862755139668783, "compression_ratio": 1.6179775280898876, "no_speech_prob": 3.534890856826678e-05}, {"id": 1072, "seek": 568058, "start": 5707.98, "end": 5708.82, "text": " as much as possible.", "tokens": [50364, 1144, 291, 362, 604, 1558, 498, 436, 764, 613, 6828, 50492, 50492, 293, 1564, 3031, 5120, 292, 365, 613, 30, 50630, 50630, 814, 764, 437, 733, 295, 2445, 30, 50740, 50740, 1981, 4470, 6828, 300, 291, 8825, 281, 505, 586, 13, 50942, 50942, 407, 881, 295, 552, 764, 264, 1936, 51082, 51082, 264, 3671, 22119, 4470, 510, 11, 51182, 51182, 597, 294, 341, 4831, 307, 1219, 426, 24010, 376, 13808, 13, 51342, 51342, 1033, 11, 370, 426, 4969, 300, 291, 2198, 466, 490, 398, 742, 282, 11, 51520, 51520, 300, 311, 437, 436, 764, 11, 558, 30, 51614, 51614, 814, 434, 1382, 281, 652, 264, 4560, 1296, 264, 10938, 51734, 51734, 382, 709, 382, 1944, 13, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.19862755139668783, "compression_ratio": 1.6179775280898876, "no_speech_prob": 3.534890856826678e-05}, {"id": 1073, "seek": 570882, "start": 5708.82, "end": 5712.219999999999, "text": " And then the contrastive term is,", "tokens": [50364, 400, 550, 264, 8712, 488, 1433, 307, 11, 50534, 50534, 309, 311, 1936, 257, 3565, 2787, 41167, 295, 264, 22182, 13, 50654, 50654, 407, 562, 291, 14722, 264, 3565, 2787, 41167, 11, 50752, 50796, 291, 519, 295, 4560, 382, 364, 2281, 50912, 50912, 293, 550, 291, 14722, 264, 3565, 2787, 41167, 295, 729, 25737, 13, 51072, 51072, 509, 483, 341, 8513, 510, 51168, 51218, 294, 264, 1150, 1036, 1622, 1219, 426, 24010, 376, 13808, 13, 51356, 51356, 1033, 11, 370, 264, 11573, 307, 8542, 770, 51472, 51472, 538, 1940, 4974, 5267, 13, 51572, 51688, 316, 4974, 437, 30, 51738, 51784], "temperature": 0.0, "avg_logprob": -0.263119473176844, "compression_ratio": 1.644859813084112, "no_speech_prob": 1.1657883987936657e-05}, {"id": 1074, "seek": 570882, "start": 5712.219999999999, "end": 5714.62, "text": " it's basically a log softmax of the distances.", "tokens": [50364, 400, 550, 264, 8712, 488, 1433, 307, 11, 50534, 50534, 309, 311, 1936, 257, 3565, 2787, 41167, 295, 264, 22182, 13, 50654, 50654, 407, 562, 291, 14722, 264, 3565, 2787, 41167, 11, 50752, 50796, 291, 519, 295, 4560, 382, 364, 2281, 50912, 50912, 293, 550, 291, 14722, 264, 3565, 2787, 41167, 295, 729, 25737, 13, 51072, 51072, 509, 483, 341, 8513, 510, 51168, 51218, 294, 264, 1150, 1036, 1622, 1219, 426, 24010, 376, 13808, 13, 51356, 51356, 1033, 11, 370, 264, 11573, 307, 8542, 770, 51472, 51472, 538, 1940, 4974, 5267, 13, 51572, 51688, 316, 4974, 437, 30, 51738, 51784], "temperature": 0.0, "avg_logprob": -0.263119473176844, "compression_ratio": 1.644859813084112, "no_speech_prob": 1.1657883987936657e-05}, {"id": 1075, "seek": 570882, "start": 5714.62, "end": 5716.58, "text": " So when you compute the log softmax,", "tokens": [50364, 400, 550, 264, 8712, 488, 1433, 307, 11, 50534, 50534, 309, 311, 1936, 257, 3565, 2787, 41167, 295, 264, 22182, 13, 50654, 50654, 407, 562, 291, 14722, 264, 3565, 2787, 41167, 11, 50752, 50796, 291, 519, 295, 4560, 382, 364, 2281, 50912, 50912, 293, 550, 291, 14722, 264, 3565, 2787, 41167, 295, 729, 25737, 13, 51072, 51072, 509, 483, 341, 8513, 510, 51168, 51218, 294, 264, 1150, 1036, 1622, 1219, 426, 24010, 376, 13808, 13, 51356, 51356, 1033, 11, 370, 264, 11573, 307, 8542, 770, 51472, 51472, 538, 1940, 4974, 5267, 13, 51572, 51688, 316, 4974, 437, 30, 51738, 51784], "temperature": 0.0, "avg_logprob": -0.263119473176844, "compression_ratio": 1.644859813084112, "no_speech_prob": 1.1657883987936657e-05}, {"id": 1076, "seek": 570882, "start": 5717.46, "end": 5719.78, "text": " you think of distance as an energy", "tokens": [50364, 400, 550, 264, 8712, 488, 1433, 307, 11, 50534, 50534, 309, 311, 1936, 257, 3565, 2787, 41167, 295, 264, 22182, 13, 50654, 50654, 407, 562, 291, 14722, 264, 3565, 2787, 41167, 11, 50752, 50796, 291, 519, 295, 4560, 382, 364, 2281, 50912, 50912, 293, 550, 291, 14722, 264, 3565, 2787, 41167, 295, 729, 25737, 13, 51072, 51072, 509, 483, 341, 8513, 510, 51168, 51218, 294, 264, 1150, 1036, 1622, 1219, 426, 24010, 376, 13808, 13, 51356, 51356, 1033, 11, 370, 264, 11573, 307, 8542, 770, 51472, 51472, 538, 1940, 4974, 5267, 13, 51572, 51688, 316, 4974, 437, 30, 51738, 51784], "temperature": 0.0, "avg_logprob": -0.263119473176844, "compression_ratio": 1.644859813084112, "no_speech_prob": 1.1657883987936657e-05}, {"id": 1077, "seek": 570882, "start": 5719.78, "end": 5722.98, "text": " and then you compute the log softmax of those energies.", "tokens": [50364, 400, 550, 264, 8712, 488, 1433, 307, 11, 50534, 50534, 309, 311, 1936, 257, 3565, 2787, 41167, 295, 264, 22182, 13, 50654, 50654, 407, 562, 291, 14722, 264, 3565, 2787, 41167, 11, 50752, 50796, 291, 519, 295, 4560, 382, 364, 2281, 50912, 50912, 293, 550, 291, 14722, 264, 3565, 2787, 41167, 295, 729, 25737, 13, 51072, 51072, 509, 483, 341, 8513, 510, 51168, 51218, 294, 264, 1150, 1036, 1622, 1219, 426, 24010, 376, 13808, 13, 51356, 51356, 1033, 11, 370, 264, 11573, 307, 8542, 770, 51472, 51472, 538, 1940, 4974, 5267, 13, 51572, 51688, 316, 4974, 437, 30, 51738, 51784], "temperature": 0.0, "avg_logprob": -0.263119473176844, "compression_ratio": 1.644859813084112, "no_speech_prob": 1.1657883987936657e-05}, {"id": 1078, "seek": 570882, "start": 5722.98, "end": 5724.9, "text": " You get this formula here", "tokens": [50364, 400, 550, 264, 8712, 488, 1433, 307, 11, 50534, 50534, 309, 311, 1936, 257, 3565, 2787, 41167, 295, 264, 22182, 13, 50654, 50654, 407, 562, 291, 14722, 264, 3565, 2787, 41167, 11, 50752, 50796, 291, 519, 295, 4560, 382, 364, 2281, 50912, 50912, 293, 550, 291, 14722, 264, 3565, 2787, 41167, 295, 729, 25737, 13, 51072, 51072, 509, 483, 341, 8513, 510, 51168, 51218, 294, 264, 1150, 1036, 1622, 1219, 426, 24010, 376, 13808, 13, 51356, 51356, 1033, 11, 370, 264, 11573, 307, 8542, 770, 51472, 51472, 538, 1940, 4974, 5267, 13, 51572, 51688, 316, 4974, 437, 30, 51738, 51784], "temperature": 0.0, "avg_logprob": -0.263119473176844, "compression_ratio": 1.644859813084112, "no_speech_prob": 1.1657883987936657e-05}, {"id": 1079, "seek": 570882, "start": 5725.9, "end": 5728.66, "text": " in the second last line called NLL MMI.", "tokens": [50364, 400, 550, 264, 8712, 488, 1433, 307, 11, 50534, 50534, 309, 311, 1936, 257, 3565, 2787, 41167, 295, 264, 22182, 13, 50654, 50654, 407, 562, 291, 14722, 264, 3565, 2787, 41167, 11, 50752, 50796, 291, 519, 295, 4560, 382, 364, 2281, 50912, 50912, 293, 550, 291, 14722, 264, 3565, 2787, 41167, 295, 729, 25737, 13, 51072, 51072, 509, 483, 341, 8513, 510, 51168, 51218, 294, 264, 1150, 1036, 1622, 1219, 426, 24010, 376, 13808, 13, 51356, 51356, 1033, 11, 370, 264, 11573, 307, 8542, 770, 51472, 51472, 538, 1940, 4974, 5267, 13, 51572, 51688, 316, 4974, 437, 30, 51738, 51784], "temperature": 0.0, "avg_logprob": -0.263119473176844, "compression_ratio": 1.644859813084112, "no_speech_prob": 1.1657883987936657e-05}, {"id": 1080, "seek": 570882, "start": 5728.66, "end": 5730.98, "text": " Okay, so the integral is approximated", "tokens": [50364, 400, 550, 264, 8712, 488, 1433, 307, 11, 50534, 50534, 309, 311, 1936, 257, 3565, 2787, 41167, 295, 264, 22182, 13, 50654, 50654, 407, 562, 291, 14722, 264, 3565, 2787, 41167, 11, 50752, 50796, 291, 519, 295, 4560, 382, 364, 2281, 50912, 50912, 293, 550, 291, 14722, 264, 3565, 2787, 41167, 295, 729, 25737, 13, 51072, 51072, 509, 483, 341, 8513, 510, 51168, 51218, 294, 264, 1150, 1036, 1622, 1219, 426, 24010, 376, 13808, 13, 51356, 51356, 1033, 11, 370, 264, 11573, 307, 8542, 770, 51472, 51472, 538, 1940, 4974, 5267, 13, 51572, 51688, 316, 4974, 437, 30, 51738, 51784], "temperature": 0.0, "avg_logprob": -0.263119473176844, "compression_ratio": 1.644859813084112, "no_speech_prob": 1.1657883987936657e-05}, {"id": 1081, "seek": 570882, "start": 5730.98, "end": 5732.98, "text": " by taking random images.", "tokens": [50364, 400, 550, 264, 8712, 488, 1433, 307, 11, 50534, 50534, 309, 311, 1936, 257, 3565, 2787, 41167, 295, 264, 22182, 13, 50654, 50654, 407, 562, 291, 14722, 264, 3565, 2787, 41167, 11, 50752, 50796, 291, 519, 295, 4560, 382, 364, 2281, 50912, 50912, 293, 550, 291, 14722, 264, 3565, 2787, 41167, 295, 729, 25737, 13, 51072, 51072, 509, 483, 341, 8513, 510, 51168, 51218, 294, 264, 1150, 1036, 1622, 1219, 426, 24010, 376, 13808, 13, 51356, 51356, 1033, 11, 370, 264, 11573, 307, 8542, 770, 51472, 51472, 538, 1940, 4974, 5267, 13, 51572, 51688, 316, 4974, 437, 30, 51738, 51784], "temperature": 0.0, "avg_logprob": -0.263119473176844, "compression_ratio": 1.644859813084112, "no_speech_prob": 1.1657883987936657e-05}, {"id": 1082, "seek": 570882, "start": 5735.299999999999, "end": 5736.299999999999, "text": " A random what?", "tokens": [50364, 400, 550, 264, 8712, 488, 1433, 307, 11, 50534, 50534, 309, 311, 1936, 257, 3565, 2787, 41167, 295, 264, 22182, 13, 50654, 50654, 407, 562, 291, 14722, 264, 3565, 2787, 41167, 11, 50752, 50796, 291, 519, 295, 4560, 382, 364, 2281, 50912, 50912, 293, 550, 291, 14722, 264, 3565, 2787, 41167, 295, 729, 25737, 13, 51072, 51072, 509, 483, 341, 8513, 510, 51168, 51218, 294, 264, 1150, 1036, 1622, 1219, 426, 24010, 376, 13808, 13, 51356, 51356, 1033, 11, 370, 264, 11573, 307, 8542, 770, 51472, 51472, 538, 1940, 4974, 5267, 13, 51572, 51688, 316, 4974, 437, 30, 51738, 51784], "temperature": 0.0, "avg_logprob": -0.263119473176844, "compression_ratio": 1.644859813084112, "no_speech_prob": 1.1657883987936657e-05}, {"id": 1083, "seek": 573630, "start": 5736.3, "end": 5739.78, "text": " They think random images is negative example.", "tokens": [50364, 814, 519, 4974, 5267, 307, 3671, 1365, 13, 50538, 50538, 407, 300, 307, 1143, 281, 30874, 264, 11573, 13, 50692, 50692, 1042, 11, 370, 1936, 291, 393, 380, 14722, 341, 11573, 50814, 50814, 295, 439, 398, 420, 341, 2408, 498, 398, 307, 27706, 13, 51064, 51076, 400, 370, 291, 1936, 30874, 264, 2408, 538, 51222, 51372, 257, 1326, 2115, 300, 291, 1888, 16979, 11, 558, 30, 51526, 51526, 961, 311, 352, 38105, 45112, 13, 51576, 51576, 286, 914, 11, 1936, 11, 498, 291, 1948, 360, 341, 6108, 11, 51704, 51704, 291, 362, 281, 1888, 729, 10938, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.24217836380004884, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.450875606678892e-05}, {"id": 1084, "seek": 573630, "start": 5739.78, "end": 5742.860000000001, "text": " So that is used to approximate the integral.", "tokens": [50364, 814, 519, 4974, 5267, 307, 3671, 1365, 13, 50538, 50538, 407, 300, 307, 1143, 281, 30874, 264, 11573, 13, 50692, 50692, 1042, 11, 370, 1936, 291, 393, 380, 14722, 341, 11573, 50814, 50814, 295, 439, 398, 420, 341, 2408, 498, 398, 307, 27706, 13, 51064, 51076, 400, 370, 291, 1936, 30874, 264, 2408, 538, 51222, 51372, 257, 1326, 2115, 300, 291, 1888, 16979, 11, 558, 30, 51526, 51526, 961, 311, 352, 38105, 45112, 13, 51576, 51576, 286, 914, 11, 1936, 11, 498, 291, 1948, 360, 341, 6108, 11, 51704, 51704, 291, 362, 281, 1888, 729, 10938, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.24217836380004884, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.450875606678892e-05}, {"id": 1085, "seek": 573630, "start": 5742.860000000001, "end": 5745.3, "text": " Well, so basically you can't compute this integral", "tokens": [50364, 814, 519, 4974, 5267, 307, 3671, 1365, 13, 50538, 50538, 407, 300, 307, 1143, 281, 30874, 264, 11573, 13, 50692, 50692, 1042, 11, 370, 1936, 291, 393, 380, 14722, 341, 11573, 50814, 50814, 295, 439, 398, 420, 341, 2408, 498, 398, 307, 27706, 13, 51064, 51076, 400, 370, 291, 1936, 30874, 264, 2408, 538, 51222, 51372, 257, 1326, 2115, 300, 291, 1888, 16979, 11, 558, 30, 51526, 51526, 961, 311, 352, 38105, 45112, 13, 51576, 51576, 286, 914, 11, 1936, 11, 498, 291, 1948, 360, 341, 6108, 11, 51704, 51704, 291, 362, 281, 1888, 729, 10938, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.24217836380004884, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.450875606678892e-05}, {"id": 1086, "seek": 573630, "start": 5745.3, "end": 5750.3, "text": " of all Y or this sum if Y is discrete.", "tokens": [50364, 814, 519, 4974, 5267, 307, 3671, 1365, 13, 50538, 50538, 407, 300, 307, 1143, 281, 30874, 264, 11573, 13, 50692, 50692, 1042, 11, 370, 1936, 291, 393, 380, 14722, 341, 11573, 50814, 50814, 295, 439, 398, 420, 341, 2408, 498, 398, 307, 27706, 13, 51064, 51076, 400, 370, 291, 1936, 30874, 264, 2408, 538, 51222, 51372, 257, 1326, 2115, 300, 291, 1888, 16979, 11, 558, 30, 51526, 51526, 961, 311, 352, 38105, 45112, 13, 51576, 51576, 286, 914, 11, 1936, 11, 498, 291, 1948, 360, 341, 6108, 11, 51704, 51704, 291, 362, 281, 1888, 729, 10938, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.24217836380004884, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.450875606678892e-05}, {"id": 1087, "seek": 573630, "start": 5750.54, "end": 5753.46, "text": " And so you basically approximate the sum by", "tokens": [50364, 814, 519, 4974, 5267, 307, 3671, 1365, 13, 50538, 50538, 407, 300, 307, 1143, 281, 30874, 264, 11573, 13, 50692, 50692, 1042, 11, 370, 1936, 291, 393, 380, 14722, 341, 11573, 50814, 50814, 295, 439, 398, 420, 341, 2408, 498, 398, 307, 27706, 13, 51064, 51076, 400, 370, 291, 1936, 30874, 264, 2408, 538, 51222, 51372, 257, 1326, 2115, 300, 291, 1888, 16979, 11, 558, 30, 51526, 51526, 961, 311, 352, 38105, 45112, 13, 51576, 51576, 286, 914, 11, 1936, 11, 498, 291, 1948, 360, 341, 6108, 11, 51704, 51704, 291, 362, 281, 1888, 729, 10938, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.24217836380004884, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.450875606678892e-05}, {"id": 1088, "seek": 573630, "start": 5756.46, "end": 5759.54, "text": " a few terms that you pick randomly, right?", "tokens": [50364, 814, 519, 4974, 5267, 307, 3671, 1365, 13, 50538, 50538, 407, 300, 307, 1143, 281, 30874, 264, 11573, 13, 50692, 50692, 1042, 11, 370, 1936, 291, 393, 380, 14722, 341, 11573, 50814, 50814, 295, 439, 398, 420, 341, 2408, 498, 398, 307, 27706, 13, 51064, 51076, 400, 370, 291, 1936, 30874, 264, 2408, 538, 51222, 51372, 257, 1326, 2115, 300, 291, 1888, 16979, 11, 558, 30, 51526, 51526, 961, 311, 352, 38105, 45112, 13, 51576, 51576, 286, 914, 11, 1936, 11, 498, 291, 1948, 360, 341, 6108, 11, 51704, 51704, 291, 362, 281, 1888, 729, 10938, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.24217836380004884, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.450875606678892e-05}, {"id": 1089, "seek": 573630, "start": 5759.54, "end": 5760.54, "text": " Let's go Monte Carlo.", "tokens": [50364, 814, 519, 4974, 5267, 307, 3671, 1365, 13, 50538, 50538, 407, 300, 307, 1143, 281, 30874, 264, 11573, 13, 50692, 50692, 1042, 11, 370, 1936, 291, 393, 380, 14722, 341, 11573, 50814, 50814, 295, 439, 398, 420, 341, 2408, 498, 398, 307, 27706, 13, 51064, 51076, 400, 370, 291, 1936, 30874, 264, 2408, 538, 51222, 51372, 257, 1326, 2115, 300, 291, 1888, 16979, 11, 558, 30, 51526, 51526, 961, 311, 352, 38105, 45112, 13, 51576, 51576, 286, 914, 11, 1936, 11, 498, 291, 1948, 360, 341, 6108, 11, 51704, 51704, 291, 362, 281, 1888, 729, 10938, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.24217836380004884, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.450875606678892e-05}, {"id": 1090, "seek": 573630, "start": 5760.54, "end": 5763.1, "text": " I mean, basically, if you wanna do this properly,", "tokens": [50364, 814, 519, 4974, 5267, 307, 3671, 1365, 13, 50538, 50538, 407, 300, 307, 1143, 281, 30874, 264, 11573, 13, 50692, 50692, 1042, 11, 370, 1936, 291, 393, 380, 14722, 341, 11573, 50814, 50814, 295, 439, 398, 420, 341, 2408, 498, 398, 307, 27706, 13, 51064, 51076, 400, 370, 291, 1936, 30874, 264, 2408, 538, 51222, 51372, 257, 1326, 2115, 300, 291, 1888, 16979, 11, 558, 30, 51526, 51526, 961, 311, 352, 38105, 45112, 13, 51576, 51576, 286, 914, 11, 1936, 11, 498, 291, 1948, 360, 341, 6108, 11, 51704, 51704, 291, 362, 281, 1888, 729, 10938, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.24217836380004884, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.450875606678892e-05}, {"id": 1091, "seek": 573630, "start": 5763.1, "end": 5765.18, "text": " you have to pick those samples", "tokens": [50364, 814, 519, 4974, 5267, 307, 3671, 1365, 13, 50538, 50538, 407, 300, 307, 1143, 281, 30874, 264, 11573, 13, 50692, 50692, 1042, 11, 370, 1936, 291, 393, 380, 14722, 341, 11573, 50814, 50814, 295, 439, 398, 420, 341, 2408, 498, 398, 307, 27706, 13, 51064, 51076, 400, 370, 291, 1936, 30874, 264, 2408, 538, 51222, 51372, 257, 1326, 2115, 300, 291, 1888, 16979, 11, 558, 30, 51526, 51526, 961, 311, 352, 38105, 45112, 13, 51576, 51576, 286, 914, 11, 1936, 11, 498, 291, 1948, 360, 341, 6108, 11, 51704, 51704, 291, 362, 281, 1888, 729, 10938, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.24217836380004884, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.450875606678892e-05}, {"id": 1092, "seek": 576518, "start": 5765.18, "end": 5768.02, "text": " according to the rule of Monte Carlo sampling,", "tokens": [50364, 4650, 281, 264, 4978, 295, 38105, 45112, 21179, 11, 50506, 50506, 457, 309, 1177, 380, 1871, 13, 50596, 50596, 286, 914, 11, 300, 311, 983, 1152, 3671, 15512, 307, 1152, 11, 1392, 30, 50746, 50746, 663, 311, 983, 437, 1669, 264, 2649, 50842, 50842, 1296, 3335, 21141, 11, 3026, 75, 11, 3998, 34, 5797, 11, 1030, 11458, 11, 50984, 50984, 307, 577, 291, 1888, 729, 3671, 10938, 13, 51102, 51102, 663, 311, 983, 286, 848, 456, 307, 572, 733, 295, 11, 51216, 51216, 294, 3331, 689, 398, 1901, 307, 1090, 18795, 11, 51412, 51412, 456, 311, 572, 659, 37716, 636, 51556, 51556, 295, 8867, 3671, 10938, 4476, 13, 51686, 51686, 467, 311, 787, 294, 21538, 300, 309, 311, 1858, 13, 51794, 51860], "temperature": 0.0, "avg_logprob": -0.1436144151995259, "compression_ratio": 1.6329588014981273, "no_speech_prob": 2.8406719138729386e-05}, {"id": 1093, "seek": 576518, "start": 5768.02, "end": 5769.820000000001, "text": " but it doesn't matter.", "tokens": [50364, 4650, 281, 264, 4978, 295, 38105, 45112, 21179, 11, 50506, 50506, 457, 309, 1177, 380, 1871, 13, 50596, 50596, 286, 914, 11, 300, 311, 983, 1152, 3671, 15512, 307, 1152, 11, 1392, 30, 50746, 50746, 663, 311, 983, 437, 1669, 264, 2649, 50842, 50842, 1296, 3335, 21141, 11, 3026, 75, 11, 3998, 34, 5797, 11, 1030, 11458, 11, 50984, 50984, 307, 577, 291, 1888, 729, 3671, 10938, 13, 51102, 51102, 663, 311, 983, 286, 848, 456, 307, 572, 733, 295, 11, 51216, 51216, 294, 3331, 689, 398, 1901, 307, 1090, 18795, 11, 51412, 51412, 456, 311, 572, 659, 37716, 636, 51556, 51556, 295, 8867, 3671, 10938, 4476, 13, 51686, 51686, 467, 311, 787, 294, 21538, 300, 309, 311, 1858, 13, 51794, 51860], "temperature": 0.0, "avg_logprob": -0.1436144151995259, "compression_ratio": 1.6329588014981273, "no_speech_prob": 2.8406719138729386e-05}, {"id": 1094, "seek": 576518, "start": 5769.820000000001, "end": 5772.820000000001, "text": " I mean, that's why hard negative mining is hard, okay?", "tokens": [50364, 4650, 281, 264, 4978, 295, 38105, 45112, 21179, 11, 50506, 50506, 457, 309, 1177, 380, 1871, 13, 50596, 50596, 286, 914, 11, 300, 311, 983, 1152, 3671, 15512, 307, 1152, 11, 1392, 30, 50746, 50746, 663, 311, 983, 437, 1669, 264, 2649, 50842, 50842, 1296, 3335, 21141, 11, 3026, 75, 11, 3998, 34, 5797, 11, 1030, 11458, 11, 50984, 50984, 307, 577, 291, 1888, 729, 3671, 10938, 13, 51102, 51102, 663, 311, 983, 286, 848, 456, 307, 572, 733, 295, 11, 51216, 51216, 294, 3331, 689, 398, 1901, 307, 1090, 18795, 11, 51412, 51412, 456, 311, 572, 659, 37716, 636, 51556, 51556, 295, 8867, 3671, 10938, 4476, 13, 51686, 51686, 467, 311, 787, 294, 21538, 300, 309, 311, 1858, 13, 51794, 51860], "temperature": 0.0, "avg_logprob": -0.1436144151995259, "compression_ratio": 1.6329588014981273, "no_speech_prob": 2.8406719138729386e-05}, {"id": 1095, "seek": 576518, "start": 5772.820000000001, "end": 5774.740000000001, "text": " That's why what makes the difference", "tokens": [50364, 4650, 281, 264, 4978, 295, 38105, 45112, 21179, 11, 50506, 50506, 457, 309, 1177, 380, 1871, 13, 50596, 50596, 286, 914, 11, 300, 311, 983, 1152, 3671, 15512, 307, 1152, 11, 1392, 30, 50746, 50746, 663, 311, 983, 437, 1669, 264, 2649, 50842, 50842, 1296, 3335, 21141, 11, 3026, 75, 11, 3998, 34, 5797, 11, 1030, 11458, 11, 50984, 50984, 307, 577, 291, 1888, 729, 3671, 10938, 13, 51102, 51102, 663, 311, 983, 286, 848, 456, 307, 572, 733, 295, 11, 51216, 51216, 294, 3331, 689, 398, 1901, 307, 1090, 18795, 11, 51412, 51412, 456, 311, 572, 659, 37716, 636, 51556, 51556, 295, 8867, 3671, 10938, 4476, 13, 51686, 51686, 467, 311, 787, 294, 21538, 300, 309, 311, 1858, 13, 51794, 51860], "temperature": 0.0, "avg_logprob": -0.1436144151995259, "compression_ratio": 1.6329588014981273, "no_speech_prob": 2.8406719138729386e-05}, {"id": 1096, "seek": 576518, "start": 5774.740000000001, "end": 5777.58, "text": " between MoCo, Perl, SimClear, et cetera,", "tokens": [50364, 4650, 281, 264, 4978, 295, 38105, 45112, 21179, 11, 50506, 50506, 457, 309, 1177, 380, 1871, 13, 50596, 50596, 286, 914, 11, 300, 311, 983, 1152, 3671, 15512, 307, 1152, 11, 1392, 30, 50746, 50746, 663, 311, 983, 437, 1669, 264, 2649, 50842, 50842, 1296, 3335, 21141, 11, 3026, 75, 11, 3998, 34, 5797, 11, 1030, 11458, 11, 50984, 50984, 307, 577, 291, 1888, 729, 3671, 10938, 13, 51102, 51102, 663, 311, 983, 286, 848, 456, 307, 572, 733, 295, 11, 51216, 51216, 294, 3331, 689, 398, 1901, 307, 1090, 18795, 11, 51412, 51412, 456, 311, 572, 659, 37716, 636, 51556, 51556, 295, 8867, 3671, 10938, 4476, 13, 51686, 51686, 467, 311, 787, 294, 21538, 300, 309, 311, 1858, 13, 51794, 51860], "temperature": 0.0, "avg_logprob": -0.1436144151995259, "compression_ratio": 1.6329588014981273, "no_speech_prob": 2.8406719138729386e-05}, {"id": 1097, "seek": 576518, "start": 5777.58, "end": 5779.9400000000005, "text": " is how you pick those negative samples.", "tokens": [50364, 4650, 281, 264, 4978, 295, 38105, 45112, 21179, 11, 50506, 50506, 457, 309, 1177, 380, 1871, 13, 50596, 50596, 286, 914, 11, 300, 311, 983, 1152, 3671, 15512, 307, 1152, 11, 1392, 30, 50746, 50746, 663, 311, 983, 437, 1669, 264, 2649, 50842, 50842, 1296, 3335, 21141, 11, 3026, 75, 11, 3998, 34, 5797, 11, 1030, 11458, 11, 50984, 50984, 307, 577, 291, 1888, 729, 3671, 10938, 13, 51102, 51102, 663, 311, 983, 286, 848, 456, 307, 572, 733, 295, 11, 51216, 51216, 294, 3331, 689, 398, 1901, 307, 1090, 18795, 11, 51412, 51412, 456, 311, 572, 659, 37716, 636, 51556, 51556, 295, 8867, 3671, 10938, 4476, 13, 51686, 51686, 467, 311, 787, 294, 21538, 300, 309, 311, 1858, 13, 51794, 51860], "temperature": 0.0, "avg_logprob": -0.1436144151995259, "compression_ratio": 1.6329588014981273, "no_speech_prob": 2.8406719138729386e-05}, {"id": 1098, "seek": 576518, "start": 5779.9400000000005, "end": 5782.22, "text": " That's why I said there is no kind of,", "tokens": [50364, 4650, 281, 264, 4978, 295, 38105, 45112, 21179, 11, 50506, 50506, 457, 309, 1177, 380, 1871, 13, 50596, 50596, 286, 914, 11, 300, 311, 983, 1152, 3671, 15512, 307, 1152, 11, 1392, 30, 50746, 50746, 663, 311, 983, 437, 1669, 264, 2649, 50842, 50842, 1296, 3335, 21141, 11, 3026, 75, 11, 3998, 34, 5797, 11, 1030, 11458, 11, 50984, 50984, 307, 577, 291, 1888, 729, 3671, 10938, 13, 51102, 51102, 663, 311, 983, 286, 848, 456, 307, 572, 733, 295, 11, 51216, 51216, 294, 3331, 689, 398, 1901, 307, 1090, 18795, 11, 51412, 51412, 456, 311, 572, 659, 37716, 636, 51556, 51556, 295, 8867, 3671, 10938, 4476, 13, 51686, 51686, 467, 311, 787, 294, 21538, 300, 309, 311, 1858, 13, 51794, 51860], "temperature": 0.0, "avg_logprob": -0.1436144151995259, "compression_ratio": 1.6329588014981273, "no_speech_prob": 2.8406719138729386e-05}, {"id": 1099, "seek": 576518, "start": 5782.22, "end": 5786.14, "text": " in cases where Y space is high dimensional,", "tokens": [50364, 4650, 281, 264, 4978, 295, 38105, 45112, 21179, 11, 50506, 50506, 457, 309, 1177, 380, 1871, 13, 50596, 50596, 286, 914, 11, 300, 311, 983, 1152, 3671, 15512, 307, 1152, 11, 1392, 30, 50746, 50746, 663, 311, 983, 437, 1669, 264, 2649, 50842, 50842, 1296, 3335, 21141, 11, 3026, 75, 11, 3998, 34, 5797, 11, 1030, 11458, 11, 50984, 50984, 307, 577, 291, 1888, 729, 3671, 10938, 13, 51102, 51102, 663, 311, 983, 286, 848, 456, 307, 572, 733, 295, 11, 51216, 51216, 294, 3331, 689, 398, 1901, 307, 1090, 18795, 11, 51412, 51412, 456, 311, 572, 659, 37716, 636, 51556, 51556, 295, 8867, 3671, 10938, 4476, 13, 51686, 51686, 467, 311, 787, 294, 21538, 300, 309, 311, 1858, 13, 51794, 51860], "temperature": 0.0, "avg_logprob": -0.1436144151995259, "compression_ratio": 1.6329588014981273, "no_speech_prob": 2.8406719138729386e-05}, {"id": 1100, "seek": 576518, "start": 5786.14, "end": 5789.02, "text": " there's no predefined way", "tokens": [50364, 4650, 281, 264, 4978, 295, 38105, 45112, 21179, 11, 50506, 50506, 457, 309, 1177, 380, 1871, 13, 50596, 50596, 286, 914, 11, 300, 311, 983, 1152, 3671, 15512, 307, 1152, 11, 1392, 30, 50746, 50746, 663, 311, 983, 437, 1669, 264, 2649, 50842, 50842, 1296, 3335, 21141, 11, 3026, 75, 11, 3998, 34, 5797, 11, 1030, 11458, 11, 50984, 50984, 307, 577, 291, 1888, 729, 3671, 10938, 13, 51102, 51102, 663, 311, 983, 286, 848, 456, 307, 572, 733, 295, 11, 51216, 51216, 294, 3331, 689, 398, 1901, 307, 1090, 18795, 11, 51412, 51412, 456, 311, 572, 659, 37716, 636, 51556, 51556, 295, 8867, 3671, 10938, 4476, 13, 51686, 51686, 467, 311, 787, 294, 21538, 300, 309, 311, 1858, 13, 51794, 51860], "temperature": 0.0, "avg_logprob": -0.1436144151995259, "compression_ratio": 1.6329588014981273, "no_speech_prob": 2.8406719138729386e-05}, {"id": 1101, "seek": 576518, "start": 5789.02, "end": 5791.62, "text": " of picking negative samples essentially.", "tokens": [50364, 4650, 281, 264, 4978, 295, 38105, 45112, 21179, 11, 50506, 50506, 457, 309, 1177, 380, 1871, 13, 50596, 50596, 286, 914, 11, 300, 311, 983, 1152, 3671, 15512, 307, 1152, 11, 1392, 30, 50746, 50746, 663, 311, 983, 437, 1669, 264, 2649, 50842, 50842, 1296, 3335, 21141, 11, 3026, 75, 11, 3998, 34, 5797, 11, 1030, 11458, 11, 50984, 50984, 307, 577, 291, 1888, 729, 3671, 10938, 13, 51102, 51102, 663, 311, 983, 286, 848, 456, 307, 572, 733, 295, 11, 51216, 51216, 294, 3331, 689, 398, 1901, 307, 1090, 18795, 11, 51412, 51412, 456, 311, 572, 659, 37716, 636, 51556, 51556, 295, 8867, 3671, 10938, 4476, 13, 51686, 51686, 467, 311, 787, 294, 21538, 300, 309, 311, 1858, 13, 51794, 51860], "temperature": 0.0, "avg_logprob": -0.1436144151995259, "compression_ratio": 1.6329588014981273, "no_speech_prob": 2.8406719138729386e-05}, {"id": 1102, "seek": 576518, "start": 5791.62, "end": 5793.780000000001, "text": " It's only in classification that it's easy.", "tokens": [50364, 4650, 281, 264, 4978, 295, 38105, 45112, 21179, 11, 50506, 50506, 457, 309, 1177, 380, 1871, 13, 50596, 50596, 286, 914, 11, 300, 311, 983, 1152, 3671, 15512, 307, 1152, 11, 1392, 30, 50746, 50746, 663, 311, 983, 437, 1669, 264, 2649, 50842, 50842, 1296, 3335, 21141, 11, 3026, 75, 11, 3998, 34, 5797, 11, 1030, 11458, 11, 50984, 50984, 307, 577, 291, 1888, 729, 3671, 10938, 13, 51102, 51102, 663, 311, 983, 286, 848, 456, 307, 572, 733, 295, 11, 51216, 51216, 294, 3331, 689, 398, 1901, 307, 1090, 18795, 11, 51412, 51412, 456, 311, 572, 659, 37716, 636, 51556, 51556, 295, 8867, 3671, 10938, 4476, 13, 51686, 51686, 467, 311, 787, 294, 21538, 300, 309, 311, 1858, 13, 51794, 51860], "temperature": 0.0, "avg_logprob": -0.1436144151995259, "compression_ratio": 1.6329588014981273, "no_speech_prob": 2.8406719138729386e-05}, {"id": 1103, "seek": 579378, "start": 5793.78, "end": 5794.62, "text": " Okay.", "tokens": [50364, 1033, 13, 50406, 50406, 583, 362, 661, 561, 5120, 292, 365, 264, 661, 15352, 30, 50656, 50766, 865, 11, 286, 914, 11, 257, 688, 295, 561, 366, 1228, 264, 3732, 3732, 50940, 50940, 420, 264, 1333, 295, 28822, 11, 291, 458, 11, 51088, 51088, 365, 264, 2649, 295, 25737, 13, 51218, 51218, 407, 512, 295, 264, 3652, 300, 321, 764, 538, 11, 51340, 51340, 412, 1935, 412, 512, 935, 11, 264, 1185, 300, 14895, 37, 617, 11, 51542, 51542, 597, 307, 264, 1851, 11150, 1185, 51628, 51628, 300, 1143, 538, 4384, 281, 6162, 561, 11, 51748, 51818], "temperature": 0.0, "avg_logprob": -0.21268750200367936, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.4289602151839063e-05}, {"id": 1104, "seek": 579378, "start": 5794.62, "end": 5799.62, "text": " But have other people experimented with the other losses?", "tokens": [50364, 1033, 13, 50406, 50406, 583, 362, 661, 561, 5120, 292, 365, 264, 661, 15352, 30, 50656, 50766, 865, 11, 286, 914, 11, 257, 688, 295, 561, 366, 1228, 264, 3732, 3732, 50940, 50940, 420, 264, 1333, 295, 28822, 11, 291, 458, 11, 51088, 51088, 365, 264, 2649, 295, 25737, 13, 51218, 51218, 407, 512, 295, 264, 3652, 300, 321, 764, 538, 11, 51340, 51340, 412, 1935, 412, 512, 935, 11, 264, 1185, 300, 14895, 37, 617, 11, 51542, 51542, 597, 307, 264, 1851, 11150, 1185, 51628, 51628, 300, 1143, 538, 4384, 281, 6162, 561, 11, 51748, 51818], "temperature": 0.0, "avg_logprob": -0.21268750200367936, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.4289602151839063e-05}, {"id": 1105, "seek": 579378, "start": 5801.82, "end": 5805.3, "text": " Yeah, I mean, a lot of people are using the square square", "tokens": [50364, 1033, 13, 50406, 50406, 583, 362, 661, 561, 5120, 292, 365, 264, 661, 15352, 30, 50656, 50766, 865, 11, 286, 914, 11, 257, 688, 295, 561, 366, 1228, 264, 3732, 3732, 50940, 50940, 420, 264, 1333, 295, 28822, 11, 291, 458, 11, 51088, 51088, 365, 264, 2649, 295, 25737, 13, 51218, 51218, 407, 512, 295, 264, 3652, 300, 321, 764, 538, 11, 51340, 51340, 412, 1935, 412, 512, 935, 11, 264, 1185, 300, 14895, 37, 617, 11, 51542, 51542, 597, 307, 264, 1851, 11150, 1185, 51628, 51628, 300, 1143, 538, 4384, 281, 6162, 561, 11, 51748, 51818], "temperature": 0.0, "avg_logprob": -0.21268750200367936, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.4289602151839063e-05}, {"id": 1106, "seek": 579378, "start": 5805.3, "end": 5808.259999999999, "text": " or the sort of hinge, you know,", "tokens": [50364, 1033, 13, 50406, 50406, 583, 362, 661, 561, 5120, 292, 365, 264, 661, 15352, 30, 50656, 50766, 865, 11, 286, 914, 11, 257, 688, 295, 561, 366, 1228, 264, 3732, 3732, 50940, 50940, 420, 264, 1333, 295, 28822, 11, 291, 458, 11, 51088, 51088, 365, 264, 2649, 295, 25737, 13, 51218, 51218, 407, 512, 295, 264, 3652, 300, 321, 764, 538, 11, 51340, 51340, 412, 1935, 412, 512, 935, 11, 264, 1185, 300, 14895, 37, 617, 11, 51542, 51542, 597, 307, 264, 1851, 11150, 1185, 51628, 51628, 300, 1143, 538, 4384, 281, 6162, 561, 11, 51748, 51818], "temperature": 0.0, "avg_logprob": -0.21268750200367936, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.4289602151839063e-05}, {"id": 1107, "seek": 579378, "start": 5808.259999999999, "end": 5810.86, "text": " with the difference of energies.", "tokens": [50364, 1033, 13, 50406, 50406, 583, 362, 661, 561, 5120, 292, 365, 264, 661, 15352, 30, 50656, 50766, 865, 11, 286, 914, 11, 257, 688, 295, 561, 366, 1228, 264, 3732, 3732, 50940, 50940, 420, 264, 1333, 295, 28822, 11, 291, 458, 11, 51088, 51088, 365, 264, 2649, 295, 25737, 13, 51218, 51218, 407, 512, 295, 264, 3652, 300, 321, 764, 538, 11, 51340, 51340, 412, 1935, 412, 512, 935, 11, 264, 1185, 300, 14895, 37, 617, 11, 51542, 51542, 597, 307, 264, 1851, 11150, 1185, 51628, 51628, 300, 1143, 538, 4384, 281, 6162, 561, 11, 51748, 51818], "temperature": 0.0, "avg_logprob": -0.21268750200367936, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.4289602151839063e-05}, {"id": 1108, "seek": 579378, "start": 5810.86, "end": 5813.3, "text": " So some of the systems that we use by,", "tokens": [50364, 1033, 13, 50406, 50406, 583, 362, 661, 561, 5120, 292, 365, 264, 661, 15352, 30, 50656, 50766, 865, 11, 286, 914, 11, 257, 688, 295, 561, 366, 1228, 264, 3732, 3732, 50940, 50940, 420, 264, 1333, 295, 28822, 11, 291, 458, 11, 51088, 51088, 365, 264, 2649, 295, 25737, 13, 51218, 51218, 407, 512, 295, 264, 3652, 300, 321, 764, 538, 11, 51340, 51340, 412, 1935, 412, 512, 935, 11, 264, 1185, 300, 14895, 37, 617, 11, 51542, 51542, 597, 307, 264, 1851, 11150, 1185, 51628, 51628, 300, 1143, 538, 4384, 281, 6162, 561, 11, 51748, 51818], "temperature": 0.0, "avg_logprob": -0.21268750200367936, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.4289602151839063e-05}, {"id": 1109, "seek": 579378, "start": 5813.3, "end": 5817.34, "text": " at least at some point, the system that DeepFace,", "tokens": [50364, 1033, 13, 50406, 50406, 583, 362, 661, 561, 5120, 292, 365, 264, 661, 15352, 30, 50656, 50766, 865, 11, 286, 914, 11, 257, 688, 295, 561, 366, 1228, 264, 3732, 3732, 50940, 50940, 420, 264, 1333, 295, 28822, 11, 291, 458, 11, 51088, 51088, 365, 264, 2649, 295, 25737, 13, 51218, 51218, 407, 512, 295, 264, 3652, 300, 321, 764, 538, 11, 51340, 51340, 412, 1935, 412, 512, 935, 11, 264, 1185, 300, 14895, 37, 617, 11, 51542, 51542, 597, 307, 264, 1851, 11150, 1185, 51628, 51628, 300, 1143, 538, 4384, 281, 6162, 561, 11, 51748, 51818], "temperature": 0.0, "avg_logprob": -0.21268750200367936, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.4289602151839063e-05}, {"id": 1110, "seek": 579378, "start": 5817.34, "end": 5819.0599999999995, "text": " which is the face recognition system", "tokens": [50364, 1033, 13, 50406, 50406, 583, 362, 661, 561, 5120, 292, 365, 264, 661, 15352, 30, 50656, 50766, 865, 11, 286, 914, 11, 257, 688, 295, 561, 366, 1228, 264, 3732, 3732, 50940, 50940, 420, 264, 1333, 295, 28822, 11, 291, 458, 11, 51088, 51088, 365, 264, 2649, 295, 25737, 13, 51218, 51218, 407, 512, 295, 264, 3652, 300, 321, 764, 538, 11, 51340, 51340, 412, 1935, 412, 512, 935, 11, 264, 1185, 300, 14895, 37, 617, 11, 51542, 51542, 597, 307, 264, 1851, 11150, 1185, 51628, 51628, 300, 1143, 538, 4384, 281, 6162, 561, 11, 51748, 51818], "temperature": 0.0, "avg_logprob": -0.21268750200367936, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.4289602151839063e-05}, {"id": 1111, "seek": 579378, "start": 5819.0599999999995, "end": 5821.46, "text": " that used by Facebook to tag people,", "tokens": [50364, 1033, 13, 50406, 50406, 583, 362, 661, 561, 5120, 292, 365, 264, 661, 15352, 30, 50656, 50766, 865, 11, 286, 914, 11, 257, 688, 295, 561, 366, 1228, 264, 3732, 3732, 50940, 50940, 420, 264, 1333, 295, 28822, 11, 291, 458, 11, 51088, 51088, 365, 264, 2649, 295, 25737, 13, 51218, 51218, 407, 512, 295, 264, 3652, 300, 321, 764, 538, 11, 51340, 51340, 412, 1935, 412, 512, 935, 11, 264, 1185, 300, 14895, 37, 617, 11, 51542, 51542, 597, 307, 264, 1851, 11150, 1185, 51628, 51628, 300, 1143, 538, 4384, 281, 6162, 561, 11, 51748, 51818], "temperature": 0.0, "avg_logprob": -0.21268750200367936, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.4289602151839063e-05}, {"id": 1112, "seek": 582146, "start": 5821.46, "end": 5826.46, "text": " it used a convolutional net trained in supervised mode", "tokens": [50364, 309, 1143, 257, 45216, 304, 2533, 8895, 294, 46533, 4391, 50614, 50616, 365, 257, 1629, 1230, 295, 10479, 11, 50758, 50758, 1936, 5267, 490, 11, 286, 500, 380, 458, 11, 50838, 50838, 257, 2459, 561, 420, 746, 13, 50938, 50938, 583, 550, 456, 390, 257, 2489, 15164, 5574, 51030, 51030, 300, 1143, 20678, 2539, 11, 1936, 318, 2918, 1130, 36170, 11, 51198, 51198, 689, 291, 855, 732, 5787, 295, 264, 912, 954, 11, 51372, 51372, 293, 291, 584, 729, 366, 264, 912, 954, 11, 51482, 51482, 293, 550, 732, 5787, 295, 819, 561, 11, 51542, 51542, 293, 291, 2944, 552, 4936, 13, 51610, 51610, 400, 300, 1143, 11, 436, 3031, 819, 10024, 6828, 11, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.13126673132686292, "compression_ratio": 1.7576923076923077, "no_speech_prob": 5.5939099183888175e-06}, {"id": 1113, "seek": 582146, "start": 5826.5, "end": 5829.34, "text": " with a certain number of categories,", "tokens": [50364, 309, 1143, 257, 45216, 304, 2533, 8895, 294, 46533, 4391, 50614, 50616, 365, 257, 1629, 1230, 295, 10479, 11, 50758, 50758, 1936, 5267, 490, 11, 286, 500, 380, 458, 11, 50838, 50838, 257, 2459, 561, 420, 746, 13, 50938, 50938, 583, 550, 456, 390, 257, 2489, 15164, 5574, 51030, 51030, 300, 1143, 20678, 2539, 11, 1936, 318, 2918, 1130, 36170, 11, 51198, 51198, 689, 291, 855, 732, 5787, 295, 264, 912, 954, 11, 51372, 51372, 293, 291, 584, 729, 366, 264, 912, 954, 11, 51482, 51482, 293, 550, 732, 5787, 295, 819, 561, 11, 51542, 51542, 293, 291, 2944, 552, 4936, 13, 51610, 51610, 400, 300, 1143, 11, 436, 3031, 819, 10024, 6828, 11, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.13126673132686292, "compression_ratio": 1.7576923076923077, "no_speech_prob": 5.5939099183888175e-06}, {"id": 1114, "seek": 582146, "start": 5829.34, "end": 5830.94, "text": " basically images from, I don't know,", "tokens": [50364, 309, 1143, 257, 45216, 304, 2533, 8895, 294, 46533, 4391, 50614, 50616, 365, 257, 1629, 1230, 295, 10479, 11, 50758, 50758, 1936, 5267, 490, 11, 286, 500, 380, 458, 11, 50838, 50838, 257, 2459, 561, 420, 746, 13, 50938, 50938, 583, 550, 456, 390, 257, 2489, 15164, 5574, 51030, 51030, 300, 1143, 20678, 2539, 11, 1936, 318, 2918, 1130, 36170, 11, 51198, 51198, 689, 291, 855, 732, 5787, 295, 264, 912, 954, 11, 51372, 51372, 293, 291, 584, 729, 366, 264, 912, 954, 11, 51482, 51482, 293, 550, 732, 5787, 295, 819, 561, 11, 51542, 51542, 293, 291, 2944, 552, 4936, 13, 51610, 51610, 400, 300, 1143, 11, 436, 3031, 819, 10024, 6828, 11, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.13126673132686292, "compression_ratio": 1.7576923076923077, "no_speech_prob": 5.5939099183888175e-06}, {"id": 1115, "seek": 582146, "start": 5830.94, "end": 5832.94, "text": " a million people or something.", "tokens": [50364, 309, 1143, 257, 45216, 304, 2533, 8895, 294, 46533, 4391, 50614, 50616, 365, 257, 1629, 1230, 295, 10479, 11, 50758, 50758, 1936, 5267, 490, 11, 286, 500, 380, 458, 11, 50838, 50838, 257, 2459, 561, 420, 746, 13, 50938, 50938, 583, 550, 456, 390, 257, 2489, 15164, 5574, 51030, 51030, 300, 1143, 20678, 2539, 11, 1936, 318, 2918, 1130, 36170, 11, 51198, 51198, 689, 291, 855, 732, 5787, 295, 264, 912, 954, 11, 51372, 51372, 293, 291, 584, 729, 366, 264, 912, 954, 11, 51482, 51482, 293, 550, 732, 5787, 295, 819, 561, 11, 51542, 51542, 293, 291, 2944, 552, 4936, 13, 51610, 51610, 400, 300, 1143, 11, 436, 3031, 819, 10024, 6828, 11, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.13126673132686292, "compression_ratio": 1.7576923076923077, "no_speech_prob": 5.5939099183888175e-06}, {"id": 1116, "seek": 582146, "start": 5832.94, "end": 5834.78, "text": " But then there was a fine tuning phase", "tokens": [50364, 309, 1143, 257, 45216, 304, 2533, 8895, 294, 46533, 4391, 50614, 50616, 365, 257, 1629, 1230, 295, 10479, 11, 50758, 50758, 1936, 5267, 490, 11, 286, 500, 380, 458, 11, 50838, 50838, 257, 2459, 561, 420, 746, 13, 50938, 50938, 583, 550, 456, 390, 257, 2489, 15164, 5574, 51030, 51030, 300, 1143, 20678, 2539, 11, 1936, 318, 2918, 1130, 36170, 11, 51198, 51198, 689, 291, 855, 732, 5787, 295, 264, 912, 954, 11, 51372, 51372, 293, 291, 584, 729, 366, 264, 912, 954, 11, 51482, 51482, 293, 550, 732, 5787, 295, 819, 561, 11, 51542, 51542, 293, 291, 2944, 552, 4936, 13, 51610, 51610, 400, 300, 1143, 11, 436, 3031, 819, 10024, 6828, 11, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.13126673132686292, "compression_ratio": 1.7576923076923077, "no_speech_prob": 5.5939099183888175e-06}, {"id": 1117, "seek": 582146, "start": 5834.78, "end": 5838.14, "text": " that used metric learning, basically Siamese nets,", "tokens": [50364, 309, 1143, 257, 45216, 304, 2533, 8895, 294, 46533, 4391, 50614, 50616, 365, 257, 1629, 1230, 295, 10479, 11, 50758, 50758, 1936, 5267, 490, 11, 286, 500, 380, 458, 11, 50838, 50838, 257, 2459, 561, 420, 746, 13, 50938, 50938, 583, 550, 456, 390, 257, 2489, 15164, 5574, 51030, 51030, 300, 1143, 20678, 2539, 11, 1936, 318, 2918, 1130, 36170, 11, 51198, 51198, 689, 291, 855, 732, 5787, 295, 264, 912, 954, 11, 51372, 51372, 293, 291, 584, 729, 366, 264, 912, 954, 11, 51482, 51482, 293, 550, 732, 5787, 295, 819, 561, 11, 51542, 51542, 293, 291, 2944, 552, 4936, 13, 51610, 51610, 400, 300, 1143, 11, 436, 3031, 819, 10024, 6828, 11, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.13126673132686292, "compression_ratio": 1.7576923076923077, "no_speech_prob": 5.5939099183888175e-06}, {"id": 1118, "seek": 582146, "start": 5838.14, "end": 5841.62, "text": " where you show two photos of the same person,", "tokens": [50364, 309, 1143, 257, 45216, 304, 2533, 8895, 294, 46533, 4391, 50614, 50616, 365, 257, 1629, 1230, 295, 10479, 11, 50758, 50758, 1936, 5267, 490, 11, 286, 500, 380, 458, 11, 50838, 50838, 257, 2459, 561, 420, 746, 13, 50938, 50938, 583, 550, 456, 390, 257, 2489, 15164, 5574, 51030, 51030, 300, 1143, 20678, 2539, 11, 1936, 318, 2918, 1130, 36170, 11, 51198, 51198, 689, 291, 855, 732, 5787, 295, 264, 912, 954, 11, 51372, 51372, 293, 291, 584, 729, 366, 264, 912, 954, 11, 51482, 51482, 293, 550, 732, 5787, 295, 819, 561, 11, 51542, 51542, 293, 291, 2944, 552, 4936, 13, 51610, 51610, 400, 300, 1143, 11, 436, 3031, 819, 10024, 6828, 11, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.13126673132686292, "compression_ratio": 1.7576923076923077, "no_speech_prob": 5.5939099183888175e-06}, {"id": 1119, "seek": 582146, "start": 5841.62, "end": 5843.82, "text": " and you say those are the same person,", "tokens": [50364, 309, 1143, 257, 45216, 304, 2533, 8895, 294, 46533, 4391, 50614, 50616, 365, 257, 1629, 1230, 295, 10479, 11, 50758, 50758, 1936, 5267, 490, 11, 286, 500, 380, 458, 11, 50838, 50838, 257, 2459, 561, 420, 746, 13, 50938, 50938, 583, 550, 456, 390, 257, 2489, 15164, 5574, 51030, 51030, 300, 1143, 20678, 2539, 11, 1936, 318, 2918, 1130, 36170, 11, 51198, 51198, 689, 291, 855, 732, 5787, 295, 264, 912, 954, 11, 51372, 51372, 293, 291, 584, 729, 366, 264, 912, 954, 11, 51482, 51482, 293, 550, 732, 5787, 295, 819, 561, 11, 51542, 51542, 293, 291, 2944, 552, 4936, 13, 51610, 51610, 400, 300, 1143, 11, 436, 3031, 819, 10024, 6828, 11, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.13126673132686292, "compression_ratio": 1.7576923076923077, "no_speech_prob": 5.5939099183888175e-06}, {"id": 1120, "seek": 582146, "start": 5843.82, "end": 5845.02, "text": " and then two photos of different people,", "tokens": [50364, 309, 1143, 257, 45216, 304, 2533, 8895, 294, 46533, 4391, 50614, 50616, 365, 257, 1629, 1230, 295, 10479, 11, 50758, 50758, 1936, 5267, 490, 11, 286, 500, 380, 458, 11, 50838, 50838, 257, 2459, 561, 420, 746, 13, 50938, 50938, 583, 550, 456, 390, 257, 2489, 15164, 5574, 51030, 51030, 300, 1143, 20678, 2539, 11, 1936, 318, 2918, 1130, 36170, 11, 51198, 51198, 689, 291, 855, 732, 5787, 295, 264, 912, 954, 11, 51372, 51372, 293, 291, 584, 729, 366, 264, 912, 954, 11, 51482, 51482, 293, 550, 732, 5787, 295, 819, 561, 11, 51542, 51542, 293, 291, 2944, 552, 4936, 13, 51610, 51610, 400, 300, 1143, 11, 436, 3031, 819, 10024, 6828, 11, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.13126673132686292, "compression_ratio": 1.7576923076923077, "no_speech_prob": 5.5939099183888175e-06}, {"id": 1121, "seek": 582146, "start": 5845.02, "end": 5846.38, "text": " and you push them apart.", "tokens": [50364, 309, 1143, 257, 45216, 304, 2533, 8895, 294, 46533, 4391, 50614, 50616, 365, 257, 1629, 1230, 295, 10479, 11, 50758, 50758, 1936, 5267, 490, 11, 286, 500, 380, 458, 11, 50838, 50838, 257, 2459, 561, 420, 746, 13, 50938, 50938, 583, 550, 456, 390, 257, 2489, 15164, 5574, 51030, 51030, 300, 1143, 20678, 2539, 11, 1936, 318, 2918, 1130, 36170, 11, 51198, 51198, 689, 291, 855, 732, 5787, 295, 264, 912, 954, 11, 51372, 51372, 293, 291, 584, 729, 366, 264, 912, 954, 11, 51482, 51482, 293, 550, 732, 5787, 295, 819, 561, 11, 51542, 51542, 293, 291, 2944, 552, 4936, 13, 51610, 51610, 400, 300, 1143, 11, 436, 3031, 819, 10024, 6828, 11, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.13126673132686292, "compression_ratio": 1.7576923076923077, "no_speech_prob": 5.5939099183888175e-06}, {"id": 1122, "seek": 582146, "start": 5846.38, "end": 5849.5, "text": " And that used, they tried different objective functions,", "tokens": [50364, 309, 1143, 257, 45216, 304, 2533, 8895, 294, 46533, 4391, 50614, 50616, 365, 257, 1629, 1230, 295, 10479, 11, 50758, 50758, 1936, 5267, 490, 11, 286, 500, 380, 458, 11, 50838, 50838, 257, 2459, 561, 420, 746, 13, 50938, 50938, 583, 550, 456, 390, 257, 2489, 15164, 5574, 51030, 51030, 300, 1143, 20678, 2539, 11, 1936, 318, 2918, 1130, 36170, 11, 51198, 51198, 689, 291, 855, 732, 5787, 295, 264, 912, 954, 11, 51372, 51372, 293, 291, 584, 729, 366, 264, 912, 954, 11, 51482, 51482, 293, 550, 732, 5787, 295, 819, 561, 11, 51542, 51542, 293, 291, 2944, 552, 4936, 13, 51610, 51610, 400, 300, 1143, 11, 436, 3031, 819, 10024, 6828, 11, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.13126673132686292, "compression_ratio": 1.7576923076923077, "no_speech_prob": 5.5939099183888175e-06}, {"id": 1123, "seek": 584950, "start": 5849.5, "end": 5852.78, "text": " but I think they were using the square square loss", "tokens": [50364, 457, 286, 519, 436, 645, 1228, 264, 3732, 3732, 4470, 50528, 50528, 412, 512, 935, 11, 1310, 264, 3732, 21510, 13, 50642, 50642, 400, 286, 478, 406, 7696, 988, 437, 436, 434, 1228, 586, 11, 50744, 50744, 457, 291, 458, 11, 309, 311, 472, 295, 729, 13, 50830, 50990, 8419, 11, 437, 8378, 486, 291, 2060, 294, 264, 958, 7991, 30, 51142, 51262, 1033, 11, 370, 321, 434, 799, 362, 732, 8341, 16564, 13, 51438, 51438, 407, 958, 1243, 307, 5116, 17412, 13, 51530, 51530, 5116, 17412, 307, 257, 2132, 12662, 51640, 51640, 412, 4384, 5774, 10303, 294, 15721, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1220552004300631, "compression_ratio": 1.6141078838174274, "no_speech_prob": 5.014369889977388e-06}, {"id": 1124, "seek": 584950, "start": 5852.78, "end": 5855.06, "text": " at some point, maybe the square exponential.", "tokens": [50364, 457, 286, 519, 436, 645, 1228, 264, 3732, 3732, 4470, 50528, 50528, 412, 512, 935, 11, 1310, 264, 3732, 21510, 13, 50642, 50642, 400, 286, 478, 406, 7696, 988, 437, 436, 434, 1228, 586, 11, 50744, 50744, 457, 291, 458, 11, 309, 311, 472, 295, 729, 13, 50830, 50990, 8419, 11, 437, 8378, 486, 291, 2060, 294, 264, 958, 7991, 30, 51142, 51262, 1033, 11, 370, 321, 434, 799, 362, 732, 8341, 16564, 13, 51438, 51438, 407, 958, 1243, 307, 5116, 17412, 13, 51530, 51530, 5116, 17412, 307, 257, 2132, 12662, 51640, 51640, 412, 4384, 5774, 10303, 294, 15721, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1220552004300631, "compression_ratio": 1.6141078838174274, "no_speech_prob": 5.014369889977388e-06}, {"id": 1125, "seek": 584950, "start": 5855.06, "end": 5857.1, "text": " And I'm not entirely sure what they're using now,", "tokens": [50364, 457, 286, 519, 436, 645, 1228, 264, 3732, 3732, 4470, 50528, 50528, 412, 512, 935, 11, 1310, 264, 3732, 21510, 13, 50642, 50642, 400, 286, 478, 406, 7696, 988, 437, 436, 434, 1228, 586, 11, 50744, 50744, 457, 291, 458, 11, 309, 311, 472, 295, 729, 13, 50830, 50990, 8419, 11, 437, 8378, 486, 291, 2060, 294, 264, 958, 7991, 30, 51142, 51262, 1033, 11, 370, 321, 434, 799, 362, 732, 8341, 16564, 13, 51438, 51438, 407, 958, 1243, 307, 5116, 17412, 13, 51530, 51530, 5116, 17412, 307, 257, 2132, 12662, 51640, 51640, 412, 4384, 5774, 10303, 294, 15721, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1220552004300631, "compression_ratio": 1.6141078838174274, "no_speech_prob": 5.014369889977388e-06}, {"id": 1126, "seek": 584950, "start": 5857.1, "end": 5858.82, "text": " but you know, it's one of those.", "tokens": [50364, 457, 286, 519, 436, 645, 1228, 264, 3732, 3732, 4470, 50528, 50528, 412, 512, 935, 11, 1310, 264, 3732, 21510, 13, 50642, 50642, 400, 286, 478, 406, 7696, 988, 437, 436, 434, 1228, 586, 11, 50744, 50744, 457, 291, 458, 11, 309, 311, 472, 295, 729, 13, 50830, 50990, 8419, 11, 437, 8378, 486, 291, 2060, 294, 264, 958, 7991, 30, 51142, 51262, 1033, 11, 370, 321, 434, 799, 362, 732, 8341, 16564, 13, 51438, 51438, 407, 958, 1243, 307, 5116, 17412, 13, 51530, 51530, 5116, 17412, 307, 257, 2132, 12662, 51640, 51640, 412, 4384, 5774, 10303, 294, 15721, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1220552004300631, "compression_ratio": 1.6141078838174274, "no_speech_prob": 5.014369889977388e-06}, {"id": 1127, "seek": 584950, "start": 5862.02, "end": 5865.06, "text": " Professor, what topics will you cover in the next lecture?", "tokens": [50364, 457, 286, 519, 436, 645, 1228, 264, 3732, 3732, 4470, 50528, 50528, 412, 512, 935, 11, 1310, 264, 3732, 21510, 13, 50642, 50642, 400, 286, 478, 406, 7696, 988, 437, 436, 434, 1228, 586, 11, 50744, 50744, 457, 291, 458, 11, 309, 311, 472, 295, 729, 13, 50830, 50990, 8419, 11, 437, 8378, 486, 291, 2060, 294, 264, 958, 7991, 30, 51142, 51262, 1033, 11, 370, 321, 434, 799, 362, 732, 8341, 16564, 13, 51438, 51438, 407, 958, 1243, 307, 5116, 17412, 13, 51530, 51530, 5116, 17412, 307, 257, 2132, 12662, 51640, 51640, 412, 4384, 5774, 10303, 294, 15721, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1220552004300631, "compression_ratio": 1.6141078838174274, "no_speech_prob": 5.014369889977388e-06}, {"id": 1128, "seek": 584950, "start": 5867.46, "end": 5870.98, "text": " Okay, so we're gonna have two guest lectures.", "tokens": [50364, 457, 286, 519, 436, 645, 1228, 264, 3732, 3732, 4470, 50528, 50528, 412, 512, 935, 11, 1310, 264, 3732, 21510, 13, 50642, 50642, 400, 286, 478, 406, 7696, 988, 437, 436, 434, 1228, 586, 11, 50744, 50744, 457, 291, 458, 11, 309, 311, 472, 295, 729, 13, 50830, 50990, 8419, 11, 437, 8378, 486, 291, 2060, 294, 264, 958, 7991, 30, 51142, 51262, 1033, 11, 370, 321, 434, 799, 362, 732, 8341, 16564, 13, 51438, 51438, 407, 958, 1243, 307, 5116, 17412, 13, 51530, 51530, 5116, 17412, 307, 257, 2132, 12662, 51640, 51640, 412, 4384, 5774, 10303, 294, 15721, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1220552004300631, "compression_ratio": 1.6141078838174274, "no_speech_prob": 5.014369889977388e-06}, {"id": 1129, "seek": 584950, "start": 5870.98, "end": 5872.82, "text": " So next week is Michael Lewis.", "tokens": [50364, 457, 286, 519, 436, 645, 1228, 264, 3732, 3732, 4470, 50528, 50528, 412, 512, 935, 11, 1310, 264, 3732, 21510, 13, 50642, 50642, 400, 286, 478, 406, 7696, 988, 437, 436, 434, 1228, 586, 11, 50744, 50744, 457, 291, 458, 11, 309, 311, 472, 295, 729, 13, 50830, 50990, 8419, 11, 437, 8378, 486, 291, 2060, 294, 264, 958, 7991, 30, 51142, 51262, 1033, 11, 370, 321, 434, 799, 362, 732, 8341, 16564, 13, 51438, 51438, 407, 958, 1243, 307, 5116, 17412, 13, 51530, 51530, 5116, 17412, 307, 257, 2132, 12662, 51640, 51640, 412, 4384, 5774, 10303, 294, 15721, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1220552004300631, "compression_ratio": 1.6141078838174274, "no_speech_prob": 5.014369889977388e-06}, {"id": 1130, "seek": 584950, "start": 5872.82, "end": 5875.02, "text": " Michael Lewis is a research scientist", "tokens": [50364, 457, 286, 519, 436, 645, 1228, 264, 3732, 3732, 4470, 50528, 50528, 412, 512, 935, 11, 1310, 264, 3732, 21510, 13, 50642, 50642, 400, 286, 478, 406, 7696, 988, 437, 436, 434, 1228, 586, 11, 50744, 50744, 457, 291, 458, 11, 309, 311, 472, 295, 729, 13, 50830, 50990, 8419, 11, 437, 8378, 486, 291, 2060, 294, 264, 958, 7991, 30, 51142, 51262, 1033, 11, 370, 321, 434, 799, 362, 732, 8341, 16564, 13, 51438, 51438, 407, 958, 1243, 307, 5116, 17412, 13, 51530, 51530, 5116, 17412, 307, 257, 2132, 12662, 51640, 51640, 412, 4384, 5774, 10303, 294, 15721, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1220552004300631, "compression_ratio": 1.6141078838174274, "no_speech_prob": 5.014369889977388e-06}, {"id": 1131, "seek": 584950, "start": 5875.02, "end": 5879.06, "text": " at Facebook Air Research in Seattle.", "tokens": [50364, 457, 286, 519, 436, 645, 1228, 264, 3732, 3732, 4470, 50528, 50528, 412, 512, 935, 11, 1310, 264, 3732, 21510, 13, 50642, 50642, 400, 286, 478, 406, 7696, 988, 437, 436, 434, 1228, 586, 11, 50744, 50744, 457, 291, 458, 11, 309, 311, 472, 295, 729, 13, 50830, 50990, 8419, 11, 437, 8378, 486, 291, 2060, 294, 264, 958, 7991, 30, 51142, 51262, 1033, 11, 370, 321, 434, 799, 362, 732, 8341, 16564, 13, 51438, 51438, 407, 958, 1243, 307, 5116, 17412, 13, 51530, 51530, 5116, 17412, 307, 257, 2132, 12662, 51640, 51640, 412, 4384, 5774, 10303, 294, 15721, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1220552004300631, "compression_ratio": 1.6141078838174274, "no_speech_prob": 5.014369889977388e-06}, {"id": 1132, "seek": 587906, "start": 5879.06, "end": 5882.1, "text": " And he is a specialist", "tokens": [50364, 400, 415, 307, 257, 17008, 50516, 50516, 295, 3303, 2856, 9007, 293, 12853, 13, 50622, 50622, 407, 415, 311, 799, 980, 291, 439, 264, 1880, 9422, 34010, 50854, 50854, 466, 8310, 281, 8310, 11, 466, 4088, 433, 11, 51058, 51058, 466, 426, 45196, 11, 293, 466, 12853, 13, 51172, 51172, 1033, 13, 51214, 51306, 400, 415, 3255, 709, 1101, 264, 4365, 466, 341, 813, 286, 360, 11, 51522, 51522, 370, 415, 311, 264, 558, 954, 281, 751, 466, 341, 13, 51664, 51664, 492, 434, 799, 362, 1071, 8341, 7991, 13, 51750, 51750, 467, 311, 799, 312, 44653, 363, 735, 266, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.12271660396030971, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.6670435545383953e-05}, {"id": 1133, "seek": 587906, "start": 5882.1, "end": 5884.22, "text": " of natural language processing and translation.", "tokens": [50364, 400, 415, 307, 257, 17008, 50516, 50516, 295, 3303, 2856, 9007, 293, 12853, 13, 50622, 50622, 407, 415, 311, 799, 980, 291, 439, 264, 1880, 9422, 34010, 50854, 50854, 466, 8310, 281, 8310, 11, 466, 4088, 433, 11, 51058, 51058, 466, 426, 45196, 11, 293, 466, 12853, 13, 51172, 51172, 1033, 13, 51214, 51306, 400, 415, 3255, 709, 1101, 264, 4365, 466, 341, 813, 286, 360, 11, 51522, 51522, 370, 415, 311, 264, 558, 954, 281, 751, 466, 341, 13, 51664, 51664, 492, 434, 799, 362, 1071, 8341, 7991, 13, 51750, 51750, 467, 311, 799, 312, 44653, 363, 735, 266, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.12271660396030971, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.6670435545383953e-05}, {"id": 1134, "seek": 587906, "start": 5884.22, "end": 5888.860000000001, "text": " So he's gonna tell you all the interesting tidbits", "tokens": [50364, 400, 415, 307, 257, 17008, 50516, 50516, 295, 3303, 2856, 9007, 293, 12853, 13, 50622, 50622, 407, 415, 311, 799, 980, 291, 439, 264, 1880, 9422, 34010, 50854, 50854, 466, 8310, 281, 8310, 11, 466, 4088, 433, 11, 51058, 51058, 466, 426, 45196, 11, 293, 466, 12853, 13, 51172, 51172, 1033, 13, 51214, 51306, 400, 415, 3255, 709, 1101, 264, 4365, 466, 341, 813, 286, 360, 11, 51522, 51522, 370, 415, 311, 264, 558, 954, 281, 751, 466, 341, 13, 51664, 51664, 492, 434, 799, 362, 1071, 8341, 7991, 13, 51750, 51750, 467, 311, 799, 312, 44653, 363, 735, 266, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.12271660396030971, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.6670435545383953e-05}, {"id": 1135, "seek": 587906, "start": 5888.860000000001, "end": 5892.9400000000005, "text": " about sequence to sequence, about transformers,", "tokens": [50364, 400, 415, 307, 257, 17008, 50516, 50516, 295, 3303, 2856, 9007, 293, 12853, 13, 50622, 50622, 407, 415, 311, 799, 980, 291, 439, 264, 1880, 9422, 34010, 50854, 50854, 466, 8310, 281, 8310, 11, 466, 4088, 433, 11, 51058, 51058, 466, 426, 45196, 11, 293, 466, 12853, 13, 51172, 51172, 1033, 13, 51214, 51306, 400, 415, 3255, 709, 1101, 264, 4365, 466, 341, 813, 286, 360, 11, 51522, 51522, 370, 415, 311, 264, 558, 954, 281, 751, 466, 341, 13, 51664, 51664, 492, 434, 799, 362, 1071, 8341, 7991, 13, 51750, 51750, 467, 311, 799, 312, 44653, 363, 735, 266, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.12271660396030971, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.6670435545383953e-05}, {"id": 1136, "seek": 587906, "start": 5892.9400000000005, "end": 5895.22, "text": " about NLP, and about translation.", "tokens": [50364, 400, 415, 307, 257, 17008, 50516, 50516, 295, 3303, 2856, 9007, 293, 12853, 13, 50622, 50622, 407, 415, 311, 799, 980, 291, 439, 264, 1880, 9422, 34010, 50854, 50854, 466, 8310, 281, 8310, 11, 466, 4088, 433, 11, 51058, 51058, 466, 426, 45196, 11, 293, 466, 12853, 13, 51172, 51172, 1033, 13, 51214, 51306, 400, 415, 3255, 709, 1101, 264, 4365, 466, 341, 813, 286, 360, 11, 51522, 51522, 370, 415, 311, 264, 558, 954, 281, 751, 466, 341, 13, 51664, 51664, 492, 434, 799, 362, 1071, 8341, 7991, 13, 51750, 51750, 467, 311, 799, 312, 44653, 363, 735, 266, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.12271660396030971, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.6670435545383953e-05}, {"id": 1137, "seek": 587906, "start": 5895.22, "end": 5896.06, "text": " Okay.", "tokens": [50364, 400, 415, 307, 257, 17008, 50516, 50516, 295, 3303, 2856, 9007, 293, 12853, 13, 50622, 50622, 407, 415, 311, 799, 980, 291, 439, 264, 1880, 9422, 34010, 50854, 50854, 466, 8310, 281, 8310, 11, 466, 4088, 433, 11, 51058, 51058, 466, 426, 45196, 11, 293, 466, 12853, 13, 51172, 51172, 1033, 13, 51214, 51306, 400, 415, 3255, 709, 1101, 264, 4365, 466, 341, 813, 286, 360, 11, 51522, 51522, 370, 415, 311, 264, 558, 954, 281, 751, 466, 341, 13, 51664, 51664, 492, 434, 799, 362, 1071, 8341, 7991, 13, 51750, 51750, 467, 311, 799, 312, 44653, 363, 735, 266, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.12271660396030971, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.6670435545383953e-05}, {"id": 1138, "seek": 587906, "start": 5897.900000000001, "end": 5902.22, "text": " And he knows much better the details about this than I do,", "tokens": [50364, 400, 415, 307, 257, 17008, 50516, 50516, 295, 3303, 2856, 9007, 293, 12853, 13, 50622, 50622, 407, 415, 311, 799, 980, 291, 439, 264, 1880, 9422, 34010, 50854, 50854, 466, 8310, 281, 8310, 11, 466, 4088, 433, 11, 51058, 51058, 466, 426, 45196, 11, 293, 466, 12853, 13, 51172, 51172, 1033, 13, 51214, 51306, 400, 415, 3255, 709, 1101, 264, 4365, 466, 341, 813, 286, 360, 11, 51522, 51522, 370, 415, 311, 264, 558, 954, 281, 751, 466, 341, 13, 51664, 51664, 492, 434, 799, 362, 1071, 8341, 7991, 13, 51750, 51750, 467, 311, 799, 312, 44653, 363, 735, 266, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.12271660396030971, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.6670435545383953e-05}, {"id": 1139, "seek": 587906, "start": 5902.22, "end": 5905.06, "text": " so he's the right person to talk about this.", "tokens": [50364, 400, 415, 307, 257, 17008, 50516, 50516, 295, 3303, 2856, 9007, 293, 12853, 13, 50622, 50622, 407, 415, 311, 799, 980, 291, 439, 264, 1880, 9422, 34010, 50854, 50854, 466, 8310, 281, 8310, 11, 466, 4088, 433, 11, 51058, 51058, 466, 426, 45196, 11, 293, 466, 12853, 13, 51172, 51172, 1033, 13, 51214, 51306, 400, 415, 3255, 709, 1101, 264, 4365, 466, 341, 813, 286, 360, 11, 51522, 51522, 370, 415, 311, 264, 558, 954, 281, 751, 466, 341, 13, 51664, 51664, 492, 434, 799, 362, 1071, 8341, 7991, 13, 51750, 51750, 467, 311, 799, 312, 44653, 363, 735, 266, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.12271660396030971, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.6670435545383953e-05}, {"id": 1140, "seek": 587906, "start": 5905.06, "end": 5906.780000000001, "text": " We're gonna have another guest lecture.", "tokens": [50364, 400, 415, 307, 257, 17008, 50516, 50516, 295, 3303, 2856, 9007, 293, 12853, 13, 50622, 50622, 407, 415, 311, 799, 980, 291, 439, 264, 1880, 9422, 34010, 50854, 50854, 466, 8310, 281, 8310, 11, 466, 4088, 433, 11, 51058, 51058, 466, 426, 45196, 11, 293, 466, 12853, 13, 51172, 51172, 1033, 13, 51214, 51306, 400, 415, 3255, 709, 1101, 264, 4365, 466, 341, 813, 286, 360, 11, 51522, 51522, 370, 415, 311, 264, 558, 954, 281, 751, 466, 341, 13, 51664, 51664, 492, 434, 799, 362, 1071, 8341, 7991, 13, 51750, 51750, 467, 311, 799, 312, 44653, 363, 735, 266, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.12271660396030971, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.6670435545383953e-05}, {"id": 1141, "seek": 587906, "start": 5906.780000000001, "end": 5908.02, "text": " It's gonna be Xavier Bresson.", "tokens": [50364, 400, 415, 307, 257, 17008, 50516, 50516, 295, 3303, 2856, 9007, 293, 12853, 13, 50622, 50622, 407, 415, 311, 799, 980, 291, 439, 264, 1880, 9422, 34010, 50854, 50854, 466, 8310, 281, 8310, 11, 466, 4088, 433, 11, 51058, 51058, 466, 426, 45196, 11, 293, 466, 12853, 13, 51172, 51172, 1033, 13, 51214, 51306, 400, 415, 3255, 709, 1101, 264, 4365, 466, 341, 813, 286, 360, 11, 51522, 51522, 370, 415, 311, 264, 558, 954, 281, 751, 466, 341, 13, 51664, 51664, 492, 434, 799, 362, 1071, 8341, 7991, 13, 51750, 51750, 467, 311, 799, 312, 44653, 363, 735, 266, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.12271660396030971, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.6670435545383953e-05}, {"id": 1142, "seek": 590802, "start": 5908.02, "end": 5913.02, "text": " He's one of the world specialists of graph neural nets.", "tokens": [50364, 634, 311, 472, 295, 264, 1002, 25476, 295, 4295, 18161, 36170, 13, 50614, 50638, 400, 370, 341, 307, 264, 1379, 1558, 295, 11, 50846, 50846, 577, 360, 291, 3079, 18161, 36170, 30, 50956, 50956, 509, 393, 519, 295, 364, 3256, 382, 257, 2445, 322, 257, 3890, 10748, 13, 51188, 51188, 2048, 19261, 307, 257, 4914, 322, 257, 3890, 10748, 13, 51332, 51332, 509, 393, 519, 295, 364, 3256, 382, 257, 2445, 322, 300, 10748, 13, 51488, 51488, 407, 10748, 307, 257, 4295, 295, 257, 1729, 2010, 11, 51608, 51608, 293, 364, 3256, 307, 445, 257, 2445, 322, 264, 4295, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11420859740330623, "compression_ratio": 1.8762886597938144, "no_speech_prob": 5.647572470479645e-05}, {"id": 1143, "seek": 590802, "start": 5913.5, "end": 5917.660000000001, "text": " And so this is the whole idea of,", "tokens": [50364, 634, 311, 472, 295, 264, 1002, 25476, 295, 4295, 18161, 36170, 13, 50614, 50638, 400, 370, 341, 307, 264, 1379, 1558, 295, 11, 50846, 50846, 577, 360, 291, 3079, 18161, 36170, 30, 50956, 50956, 509, 393, 519, 295, 364, 3256, 382, 257, 2445, 322, 257, 3890, 10748, 13, 51188, 51188, 2048, 19261, 307, 257, 4914, 322, 257, 3890, 10748, 13, 51332, 51332, 509, 393, 519, 295, 364, 3256, 382, 257, 2445, 322, 300, 10748, 13, 51488, 51488, 407, 10748, 307, 257, 4295, 295, 257, 1729, 2010, 11, 51608, 51608, 293, 364, 3256, 307, 445, 257, 2445, 322, 264, 4295, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11420859740330623, "compression_ratio": 1.8762886597938144, "no_speech_prob": 5.647572470479645e-05}, {"id": 1144, "seek": 590802, "start": 5917.660000000001, "end": 5919.860000000001, "text": " how do you apply neural nets?", "tokens": [50364, 634, 311, 472, 295, 264, 1002, 25476, 295, 4295, 18161, 36170, 13, 50614, 50638, 400, 370, 341, 307, 264, 1379, 1558, 295, 11, 50846, 50846, 577, 360, 291, 3079, 18161, 36170, 30, 50956, 50956, 509, 393, 519, 295, 364, 3256, 382, 257, 2445, 322, 257, 3890, 10748, 13, 51188, 51188, 2048, 19261, 307, 257, 4914, 322, 257, 3890, 10748, 13, 51332, 51332, 509, 393, 519, 295, 364, 3256, 382, 257, 2445, 322, 300, 10748, 13, 51488, 51488, 407, 10748, 307, 257, 4295, 295, 257, 1729, 2010, 11, 51608, 51608, 293, 364, 3256, 307, 445, 257, 2445, 322, 264, 4295, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11420859740330623, "compression_ratio": 1.8762886597938144, "no_speech_prob": 5.647572470479645e-05}, {"id": 1145, "seek": 590802, "start": 5919.860000000001, "end": 5924.5, "text": " You can think of an image as a function on a regular grid.", "tokens": [50364, 634, 311, 472, 295, 264, 1002, 25476, 295, 4295, 18161, 36170, 13, 50614, 50638, 400, 370, 341, 307, 264, 1379, 1558, 295, 11, 50846, 50846, 577, 360, 291, 3079, 18161, 36170, 30, 50956, 50956, 509, 393, 519, 295, 364, 3256, 382, 257, 2445, 322, 257, 3890, 10748, 13, 51188, 51188, 2048, 19261, 307, 257, 4914, 322, 257, 3890, 10748, 13, 51332, 51332, 509, 393, 519, 295, 364, 3256, 382, 257, 2445, 322, 300, 10748, 13, 51488, 51488, 407, 10748, 307, 257, 4295, 295, 257, 1729, 2010, 11, 51608, 51608, 293, 364, 3256, 307, 445, 257, 2445, 322, 264, 4295, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11420859740330623, "compression_ratio": 1.8762886597938144, "no_speech_prob": 5.647572470479645e-05}, {"id": 1146, "seek": 590802, "start": 5924.5, "end": 5927.38, "text": " Every pixel is a location on a regular grid.", "tokens": [50364, 634, 311, 472, 295, 264, 1002, 25476, 295, 4295, 18161, 36170, 13, 50614, 50638, 400, 370, 341, 307, 264, 1379, 1558, 295, 11, 50846, 50846, 577, 360, 291, 3079, 18161, 36170, 30, 50956, 50956, 509, 393, 519, 295, 364, 3256, 382, 257, 2445, 322, 257, 3890, 10748, 13, 51188, 51188, 2048, 19261, 307, 257, 4914, 322, 257, 3890, 10748, 13, 51332, 51332, 509, 393, 519, 295, 364, 3256, 382, 257, 2445, 322, 300, 10748, 13, 51488, 51488, 407, 10748, 307, 257, 4295, 295, 257, 1729, 2010, 11, 51608, 51608, 293, 364, 3256, 307, 445, 257, 2445, 322, 264, 4295, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11420859740330623, "compression_ratio": 1.8762886597938144, "no_speech_prob": 5.647572470479645e-05}, {"id": 1147, "seek": 590802, "start": 5927.38, "end": 5930.5, "text": " You can think of an image as a function on that grid.", "tokens": [50364, 634, 311, 472, 295, 264, 1002, 25476, 295, 4295, 18161, 36170, 13, 50614, 50638, 400, 370, 341, 307, 264, 1379, 1558, 295, 11, 50846, 50846, 577, 360, 291, 3079, 18161, 36170, 30, 50956, 50956, 509, 393, 519, 295, 364, 3256, 382, 257, 2445, 322, 257, 3890, 10748, 13, 51188, 51188, 2048, 19261, 307, 257, 4914, 322, 257, 3890, 10748, 13, 51332, 51332, 509, 393, 519, 295, 364, 3256, 382, 257, 2445, 322, 300, 10748, 13, 51488, 51488, 407, 10748, 307, 257, 4295, 295, 257, 1729, 2010, 11, 51608, 51608, 293, 364, 3256, 307, 445, 257, 2445, 322, 264, 4295, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11420859740330623, "compression_ratio": 1.8762886597938144, "no_speech_prob": 5.647572470479645e-05}, {"id": 1148, "seek": 590802, "start": 5930.5, "end": 5932.900000000001, "text": " So grid is a graph of a particular type,", "tokens": [50364, 634, 311, 472, 295, 264, 1002, 25476, 295, 4295, 18161, 36170, 13, 50614, 50638, 400, 370, 341, 307, 264, 1379, 1558, 295, 11, 50846, 50846, 577, 360, 291, 3079, 18161, 36170, 30, 50956, 50956, 509, 393, 519, 295, 364, 3256, 382, 257, 2445, 322, 257, 3890, 10748, 13, 51188, 51188, 2048, 19261, 307, 257, 4914, 322, 257, 3890, 10748, 13, 51332, 51332, 509, 393, 519, 295, 364, 3256, 382, 257, 2445, 322, 300, 10748, 13, 51488, 51488, 407, 10748, 307, 257, 4295, 295, 257, 1729, 2010, 11, 51608, 51608, 293, 364, 3256, 307, 445, 257, 2445, 322, 264, 4295, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11420859740330623, "compression_ratio": 1.8762886597938144, "no_speech_prob": 5.647572470479645e-05}, {"id": 1149, "seek": 590802, "start": 5932.900000000001, "end": 5936.02, "text": " and an image is just a function on the graph.", "tokens": [50364, 634, 311, 472, 295, 264, 1002, 25476, 295, 4295, 18161, 36170, 13, 50614, 50638, 400, 370, 341, 307, 264, 1379, 1558, 295, 11, 50846, 50846, 577, 360, 291, 3079, 18161, 36170, 30, 50956, 50956, 509, 393, 519, 295, 364, 3256, 382, 257, 2445, 322, 257, 3890, 10748, 13, 51188, 51188, 2048, 19261, 307, 257, 4914, 322, 257, 3890, 10748, 13, 51332, 51332, 509, 393, 519, 295, 364, 3256, 382, 257, 2445, 322, 300, 10748, 13, 51488, 51488, 407, 10748, 307, 257, 4295, 295, 257, 1729, 2010, 11, 51608, 51608, 293, 364, 3256, 307, 445, 257, 2445, 322, 264, 4295, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11420859740330623, "compression_ratio": 1.8762886597938144, "no_speech_prob": 5.647572470479645e-05}, {"id": 1150, "seek": 593602, "start": 5936.02, "end": 5937.9800000000005, "text": " You can think of, I don't know,", "tokens": [50364, 509, 393, 519, 295, 11, 286, 500, 380, 458, 11, 50462, 50514, 960, 382, 257, 3890, 805, 35, 10748, 689, 291, 362, 1901, 293, 565, 11, 50718, 50762, 293, 881, 3303, 12354, 11, 50878, 50878, 291, 393, 519, 295, 552, 382, 6828, 322, 3890, 24877, 13, 51106, 51106, 1033, 13, 51148, 51148, 708, 466, 264, 1389, 689, 264, 2445, 291, 434, 3102, 294, 51332, 51332, 307, 406, 322, 257, 462, 1311, 31264, 282, 4295, 11, 498, 291, 528, 13, 51524, 51524, 407, 718, 311, 3811, 11, 337, 1365, 11, 51582, 51582, 291, 747, 257, 5052, 365, 257, 2462, 284, 20726, 2799, 11, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.14985726014623102, "compression_ratio": 1.5418502202643172, "no_speech_prob": 5.1736719797190744e-06}, {"id": 1151, "seek": 593602, "start": 5939.02, "end": 5943.1, "text": " video as a regular 3D grid where you have space and time,", "tokens": [50364, 509, 393, 519, 295, 11, 286, 500, 380, 458, 11, 50462, 50514, 960, 382, 257, 3890, 805, 35, 10748, 689, 291, 362, 1901, 293, 565, 11, 50718, 50762, 293, 881, 3303, 12354, 11, 50878, 50878, 291, 393, 519, 295, 552, 382, 6828, 322, 3890, 24877, 13, 51106, 51106, 1033, 13, 51148, 51148, 708, 466, 264, 1389, 689, 264, 2445, 291, 434, 3102, 294, 51332, 51332, 307, 406, 322, 257, 462, 1311, 31264, 282, 4295, 11, 498, 291, 528, 13, 51524, 51524, 407, 718, 311, 3811, 11, 337, 1365, 11, 51582, 51582, 291, 747, 257, 5052, 365, 257, 2462, 284, 20726, 2799, 11, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.14985726014623102, "compression_ratio": 1.5418502202643172, "no_speech_prob": 5.1736719797190744e-06}, {"id": 1152, "seek": 593602, "start": 5943.9800000000005, "end": 5946.3, "text": " and most natural signals,", "tokens": [50364, 509, 393, 519, 295, 11, 286, 500, 380, 458, 11, 50462, 50514, 960, 382, 257, 3890, 805, 35, 10748, 689, 291, 362, 1901, 293, 565, 11, 50718, 50762, 293, 881, 3303, 12354, 11, 50878, 50878, 291, 393, 519, 295, 552, 382, 6828, 322, 3890, 24877, 13, 51106, 51106, 1033, 13, 51148, 51148, 708, 466, 264, 1389, 689, 264, 2445, 291, 434, 3102, 294, 51332, 51332, 307, 406, 322, 257, 462, 1311, 31264, 282, 4295, 11, 498, 291, 528, 13, 51524, 51524, 407, 718, 311, 3811, 11, 337, 1365, 11, 51582, 51582, 291, 747, 257, 5052, 365, 257, 2462, 284, 20726, 2799, 11, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.14985726014623102, "compression_ratio": 1.5418502202643172, "no_speech_prob": 5.1736719797190744e-06}, {"id": 1153, "seek": 593602, "start": 5946.3, "end": 5950.860000000001, "text": " you can think of them as functions on regular graphs.", "tokens": [50364, 509, 393, 519, 295, 11, 286, 500, 380, 458, 11, 50462, 50514, 960, 382, 257, 3890, 805, 35, 10748, 689, 291, 362, 1901, 293, 565, 11, 50718, 50762, 293, 881, 3303, 12354, 11, 50878, 50878, 291, 393, 519, 295, 552, 382, 6828, 322, 3890, 24877, 13, 51106, 51106, 1033, 13, 51148, 51148, 708, 466, 264, 1389, 689, 264, 2445, 291, 434, 3102, 294, 51332, 51332, 307, 406, 322, 257, 462, 1311, 31264, 282, 4295, 11, 498, 291, 528, 13, 51524, 51524, 407, 718, 311, 3811, 11, 337, 1365, 11, 51582, 51582, 291, 747, 257, 5052, 365, 257, 2462, 284, 20726, 2799, 11, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.14985726014623102, "compression_ratio": 1.5418502202643172, "no_speech_prob": 5.1736719797190744e-06}, {"id": 1154, "seek": 593602, "start": 5950.860000000001, "end": 5951.700000000001, "text": " Okay.", "tokens": [50364, 509, 393, 519, 295, 11, 286, 500, 380, 458, 11, 50462, 50514, 960, 382, 257, 3890, 805, 35, 10748, 689, 291, 362, 1901, 293, 565, 11, 50718, 50762, 293, 881, 3303, 12354, 11, 50878, 50878, 291, 393, 519, 295, 552, 382, 6828, 322, 3890, 24877, 13, 51106, 51106, 1033, 13, 51148, 51148, 708, 466, 264, 1389, 689, 264, 2445, 291, 434, 3102, 294, 51332, 51332, 307, 406, 322, 257, 462, 1311, 31264, 282, 4295, 11, 498, 291, 528, 13, 51524, 51524, 407, 718, 311, 3811, 11, 337, 1365, 11, 51582, 51582, 291, 747, 257, 5052, 365, 257, 2462, 284, 20726, 2799, 11, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.14985726014623102, "compression_ratio": 1.5418502202643172, "no_speech_prob": 5.1736719797190744e-06}, {"id": 1155, "seek": 593602, "start": 5951.700000000001, "end": 5955.38, "text": " What about the case where the function you're interested in", "tokens": [50364, 509, 393, 519, 295, 11, 286, 500, 380, 458, 11, 50462, 50514, 960, 382, 257, 3890, 805, 35, 10748, 689, 291, 362, 1901, 293, 565, 11, 50718, 50762, 293, 881, 3303, 12354, 11, 50878, 50878, 291, 393, 519, 295, 552, 382, 6828, 322, 3890, 24877, 13, 51106, 51106, 1033, 13, 51148, 51148, 708, 466, 264, 1389, 689, 264, 2445, 291, 434, 3102, 294, 51332, 51332, 307, 406, 322, 257, 462, 1311, 31264, 282, 4295, 11, 498, 291, 528, 13, 51524, 51524, 407, 718, 311, 3811, 11, 337, 1365, 11, 51582, 51582, 291, 747, 257, 5052, 365, 257, 2462, 284, 20726, 2799, 11, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.14985726014623102, "compression_ratio": 1.5418502202643172, "no_speech_prob": 5.1736719797190744e-06}, {"id": 1156, "seek": 593602, "start": 5955.38, "end": 5959.22, "text": " is not on a Euclidean graph, if you want.", "tokens": [50364, 509, 393, 519, 295, 11, 286, 500, 380, 458, 11, 50462, 50514, 960, 382, 257, 3890, 805, 35, 10748, 689, 291, 362, 1901, 293, 565, 11, 50718, 50762, 293, 881, 3303, 12354, 11, 50878, 50878, 291, 393, 519, 295, 552, 382, 6828, 322, 3890, 24877, 13, 51106, 51106, 1033, 13, 51148, 51148, 708, 466, 264, 1389, 689, 264, 2445, 291, 434, 3102, 294, 51332, 51332, 307, 406, 322, 257, 462, 1311, 31264, 282, 4295, 11, 498, 291, 528, 13, 51524, 51524, 407, 718, 311, 3811, 11, 337, 1365, 11, 51582, 51582, 291, 747, 257, 5052, 365, 257, 2462, 284, 20726, 2799, 11, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.14985726014623102, "compression_ratio": 1.5418502202643172, "no_speech_prob": 5.1736719797190744e-06}, {"id": 1157, "seek": 593602, "start": 5959.22, "end": 5960.38, "text": " So let's imagine, for example,", "tokens": [50364, 509, 393, 519, 295, 11, 286, 500, 380, 458, 11, 50462, 50514, 960, 382, 257, 3890, 805, 35, 10748, 689, 291, 362, 1901, 293, 565, 11, 50718, 50762, 293, 881, 3303, 12354, 11, 50878, 50878, 291, 393, 519, 295, 552, 382, 6828, 322, 3890, 24877, 13, 51106, 51106, 1033, 13, 51148, 51148, 708, 466, 264, 1389, 689, 264, 2445, 291, 434, 3102, 294, 51332, 51332, 307, 406, 322, 257, 462, 1311, 31264, 282, 4295, 11, 498, 291, 528, 13, 51524, 51524, 407, 718, 311, 3811, 11, 337, 1365, 11, 51582, 51582, 291, 747, 257, 5052, 365, 257, 2462, 284, 20726, 2799, 11, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.14985726014623102, "compression_ratio": 1.5418502202643172, "no_speech_prob": 5.1736719797190744e-06}, {"id": 1158, "seek": 593602, "start": 5960.38, "end": 5964.42, "text": " you take a photo with a panoramic camera,", "tokens": [50364, 509, 393, 519, 295, 11, 286, 500, 380, 458, 11, 50462, 50514, 960, 382, 257, 3890, 805, 35, 10748, 689, 291, 362, 1901, 293, 565, 11, 50718, 50762, 293, 881, 3303, 12354, 11, 50878, 50878, 291, 393, 519, 295, 552, 382, 6828, 322, 3890, 24877, 13, 51106, 51106, 1033, 13, 51148, 51148, 708, 466, 264, 1389, 689, 264, 2445, 291, 434, 3102, 294, 51332, 51332, 307, 406, 322, 257, 462, 1311, 31264, 282, 4295, 11, 498, 291, 528, 13, 51524, 51524, 407, 718, 311, 3811, 11, 337, 1365, 11, 51582, 51582, 291, 747, 257, 5052, 365, 257, 2462, 284, 20726, 2799, 11, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.14985726014623102, "compression_ratio": 1.5418502202643172, "no_speech_prob": 5.1736719797190744e-06}, {"id": 1159, "seek": 596442, "start": 5964.42, "end": 5966.18, "text": " a 360 camera, right?", "tokens": [50364, 257, 13898, 2799, 11, 558, 30, 50452, 50452, 407, 309, 311, 257, 2799, 300, 1936, 2516, 257, 37300, 3256, 13, 50642, 50642, 1033, 13, 50684, 50684, 407, 586, 428, 18668, 1621, 322, 264, 16687, 13, 50780, 50824, 1012, 360, 291, 14722, 257, 45216, 322, 257, 16687, 30, 50934, 50976, 1033, 13, 51018, 51070, 407, 291, 528, 281, 1190, 428, 45216, 304, 2533, 51166, 51166, 322, 341, 3256, 300, 586, 2909, 322, 264, 16687, 13, 51276, 51332, 509, 393, 380, 764, 264, 3832, 2098, 295, 15866, 3754, 15892, 13, 51582, 51660, 407, 291, 362, 281, 2573, 484, 51702, 51702, 577, 281, 14722, 3754, 15892, 322, 264, 16687, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.13013340546204163, "compression_ratio": 1.8215962441314555, "no_speech_prob": 9.51568472373765e-06}, {"id": 1160, "seek": 596442, "start": 5966.18, "end": 5969.9800000000005, "text": " So it's a camera that basically takes a spherical image.", "tokens": [50364, 257, 13898, 2799, 11, 558, 30, 50452, 50452, 407, 309, 311, 257, 2799, 300, 1936, 2516, 257, 37300, 3256, 13, 50642, 50642, 1033, 13, 50684, 50684, 407, 586, 428, 18668, 1621, 322, 264, 16687, 13, 50780, 50824, 1012, 360, 291, 14722, 257, 45216, 322, 257, 16687, 30, 50934, 50976, 1033, 13, 51018, 51070, 407, 291, 528, 281, 1190, 428, 45216, 304, 2533, 51166, 51166, 322, 341, 3256, 300, 586, 2909, 322, 264, 16687, 13, 51276, 51332, 509, 393, 380, 764, 264, 3832, 2098, 295, 15866, 3754, 15892, 13, 51582, 51660, 407, 291, 362, 281, 2573, 484, 51702, 51702, 577, 281, 14722, 3754, 15892, 322, 264, 16687, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.13013340546204163, "compression_ratio": 1.8215962441314555, "no_speech_prob": 9.51568472373765e-06}, {"id": 1161, "seek": 596442, "start": 5969.9800000000005, "end": 5970.82, "text": " Okay.", "tokens": [50364, 257, 13898, 2799, 11, 558, 30, 50452, 50452, 407, 309, 311, 257, 2799, 300, 1936, 2516, 257, 37300, 3256, 13, 50642, 50642, 1033, 13, 50684, 50684, 407, 586, 428, 18668, 1621, 322, 264, 16687, 13, 50780, 50824, 1012, 360, 291, 14722, 257, 45216, 322, 257, 16687, 30, 50934, 50976, 1033, 13, 51018, 51070, 407, 291, 528, 281, 1190, 428, 45216, 304, 2533, 51166, 51166, 322, 341, 3256, 300, 586, 2909, 322, 264, 16687, 13, 51276, 51332, 509, 393, 380, 764, 264, 3832, 2098, 295, 15866, 3754, 15892, 13, 51582, 51660, 407, 291, 362, 281, 2573, 484, 51702, 51702, 577, 281, 14722, 3754, 15892, 322, 264, 16687, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.13013340546204163, "compression_ratio": 1.8215962441314555, "no_speech_prob": 9.51568472373765e-06}, {"id": 1162, "seek": 596442, "start": 5970.82, "end": 5972.74, "text": " So now your pixels live on the sphere.", "tokens": [50364, 257, 13898, 2799, 11, 558, 30, 50452, 50452, 407, 309, 311, 257, 2799, 300, 1936, 2516, 257, 37300, 3256, 13, 50642, 50642, 1033, 13, 50684, 50684, 407, 586, 428, 18668, 1621, 322, 264, 16687, 13, 50780, 50824, 1012, 360, 291, 14722, 257, 45216, 322, 257, 16687, 30, 50934, 50976, 1033, 13, 51018, 51070, 407, 291, 528, 281, 1190, 428, 45216, 304, 2533, 51166, 51166, 322, 341, 3256, 300, 586, 2909, 322, 264, 16687, 13, 51276, 51332, 509, 393, 380, 764, 264, 3832, 2098, 295, 15866, 3754, 15892, 13, 51582, 51660, 407, 291, 362, 281, 2573, 484, 51702, 51702, 577, 281, 14722, 3754, 15892, 322, 264, 16687, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.13013340546204163, "compression_ratio": 1.8215962441314555, "no_speech_prob": 9.51568472373765e-06}, {"id": 1163, "seek": 596442, "start": 5973.62, "end": 5975.82, "text": " How do you compute a convolution on a sphere?", "tokens": [50364, 257, 13898, 2799, 11, 558, 30, 50452, 50452, 407, 309, 311, 257, 2799, 300, 1936, 2516, 257, 37300, 3256, 13, 50642, 50642, 1033, 13, 50684, 50684, 407, 586, 428, 18668, 1621, 322, 264, 16687, 13, 50780, 50824, 1012, 360, 291, 14722, 257, 45216, 322, 257, 16687, 30, 50934, 50976, 1033, 13, 51018, 51070, 407, 291, 528, 281, 1190, 428, 45216, 304, 2533, 51166, 51166, 322, 341, 3256, 300, 586, 2909, 322, 264, 16687, 13, 51276, 51332, 509, 393, 380, 764, 264, 3832, 2098, 295, 15866, 3754, 15892, 13, 51582, 51660, 407, 291, 362, 281, 2573, 484, 51702, 51702, 577, 281, 14722, 3754, 15892, 322, 264, 16687, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.13013340546204163, "compression_ratio": 1.8215962441314555, "no_speech_prob": 9.51568472373765e-06}, {"id": 1164, "seek": 596442, "start": 5976.66, "end": 5977.5, "text": " Okay.", "tokens": [50364, 257, 13898, 2799, 11, 558, 30, 50452, 50452, 407, 309, 311, 257, 2799, 300, 1936, 2516, 257, 37300, 3256, 13, 50642, 50642, 1033, 13, 50684, 50684, 407, 586, 428, 18668, 1621, 322, 264, 16687, 13, 50780, 50824, 1012, 360, 291, 14722, 257, 45216, 322, 257, 16687, 30, 50934, 50976, 1033, 13, 51018, 51070, 407, 291, 528, 281, 1190, 428, 45216, 304, 2533, 51166, 51166, 322, 341, 3256, 300, 586, 2909, 322, 264, 16687, 13, 51276, 51332, 509, 393, 380, 764, 264, 3832, 2098, 295, 15866, 3754, 15892, 13, 51582, 51660, 407, 291, 362, 281, 2573, 484, 51702, 51702, 577, 281, 14722, 3754, 15892, 322, 264, 16687, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.13013340546204163, "compression_ratio": 1.8215962441314555, "no_speech_prob": 9.51568472373765e-06}, {"id": 1165, "seek": 596442, "start": 5978.54, "end": 5980.46, "text": " So you want to run your convolutional net", "tokens": [50364, 257, 13898, 2799, 11, 558, 30, 50452, 50452, 407, 309, 311, 257, 2799, 300, 1936, 2516, 257, 37300, 3256, 13, 50642, 50642, 1033, 13, 50684, 50684, 407, 586, 428, 18668, 1621, 322, 264, 16687, 13, 50780, 50824, 1012, 360, 291, 14722, 257, 45216, 322, 257, 16687, 30, 50934, 50976, 1033, 13, 51018, 51070, 407, 291, 528, 281, 1190, 428, 45216, 304, 2533, 51166, 51166, 322, 341, 3256, 300, 586, 2909, 322, 264, 16687, 13, 51276, 51332, 509, 393, 380, 764, 264, 3832, 2098, 295, 15866, 3754, 15892, 13, 51582, 51660, 407, 291, 362, 281, 2573, 484, 51702, 51702, 577, 281, 14722, 3754, 15892, 322, 264, 16687, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.13013340546204163, "compression_ratio": 1.8215962441314555, "no_speech_prob": 9.51568472373765e-06}, {"id": 1166, "seek": 596442, "start": 5980.46, "end": 5982.66, "text": " on this image that now lives on the sphere.", "tokens": [50364, 257, 13898, 2799, 11, 558, 30, 50452, 50452, 407, 309, 311, 257, 2799, 300, 1936, 2516, 257, 37300, 3256, 13, 50642, 50642, 1033, 13, 50684, 50684, 407, 586, 428, 18668, 1621, 322, 264, 16687, 13, 50780, 50824, 1012, 360, 291, 14722, 257, 45216, 322, 257, 16687, 30, 50934, 50976, 1033, 13, 51018, 51070, 407, 291, 528, 281, 1190, 428, 45216, 304, 2533, 51166, 51166, 322, 341, 3256, 300, 586, 2909, 322, 264, 16687, 13, 51276, 51332, 509, 393, 380, 764, 264, 3832, 2098, 295, 15866, 3754, 15892, 13, 51582, 51660, 407, 291, 362, 281, 2573, 484, 51702, 51702, 577, 281, 14722, 3754, 15892, 322, 264, 16687, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.13013340546204163, "compression_ratio": 1.8215962441314555, "no_speech_prob": 9.51568472373765e-06}, {"id": 1167, "seek": 596442, "start": 5983.78, "end": 5988.78, "text": " You can't use the standard ways of computing convolutions.", "tokens": [50364, 257, 13898, 2799, 11, 558, 30, 50452, 50452, 407, 309, 311, 257, 2799, 300, 1936, 2516, 257, 37300, 3256, 13, 50642, 50642, 1033, 13, 50684, 50684, 407, 586, 428, 18668, 1621, 322, 264, 16687, 13, 50780, 50824, 1012, 360, 291, 14722, 257, 45216, 322, 257, 16687, 30, 50934, 50976, 1033, 13, 51018, 51070, 407, 291, 528, 281, 1190, 428, 45216, 304, 2533, 51166, 51166, 322, 341, 3256, 300, 586, 2909, 322, 264, 16687, 13, 51276, 51332, 509, 393, 380, 764, 264, 3832, 2098, 295, 15866, 3754, 15892, 13, 51582, 51660, 407, 291, 362, 281, 2573, 484, 51702, 51702, 577, 281, 14722, 3754, 15892, 322, 264, 16687, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.13013340546204163, "compression_ratio": 1.8215962441314555, "no_speech_prob": 9.51568472373765e-06}, {"id": 1168, "seek": 596442, "start": 5990.34, "end": 5991.18, "text": " So you have to figure out", "tokens": [50364, 257, 13898, 2799, 11, 558, 30, 50452, 50452, 407, 309, 311, 257, 2799, 300, 1936, 2516, 257, 37300, 3256, 13, 50642, 50642, 1033, 13, 50684, 50684, 407, 586, 428, 18668, 1621, 322, 264, 16687, 13, 50780, 50824, 1012, 360, 291, 14722, 257, 45216, 322, 257, 16687, 30, 50934, 50976, 1033, 13, 51018, 51070, 407, 291, 528, 281, 1190, 428, 45216, 304, 2533, 51166, 51166, 322, 341, 3256, 300, 586, 2909, 322, 264, 16687, 13, 51276, 51332, 509, 393, 380, 764, 264, 3832, 2098, 295, 15866, 3754, 15892, 13, 51582, 51660, 407, 291, 362, 281, 2573, 484, 51702, 51702, 577, 281, 14722, 3754, 15892, 322, 264, 16687, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.13013340546204163, "compression_ratio": 1.8215962441314555, "no_speech_prob": 9.51568472373765e-06}, {"id": 1169, "seek": 596442, "start": 5991.18, "end": 5993.5, "text": " how to compute convolutions on the sphere.", "tokens": [50364, 257, 13898, 2799, 11, 558, 30, 50452, 50452, 407, 309, 311, 257, 2799, 300, 1936, 2516, 257, 37300, 3256, 13, 50642, 50642, 1033, 13, 50684, 50684, 407, 586, 428, 18668, 1621, 322, 264, 16687, 13, 50780, 50824, 1012, 360, 291, 14722, 257, 45216, 322, 257, 16687, 30, 50934, 50976, 1033, 13, 51018, 51070, 407, 291, 528, 281, 1190, 428, 45216, 304, 2533, 51166, 51166, 322, 341, 3256, 300, 586, 2909, 322, 264, 16687, 13, 51276, 51332, 509, 393, 380, 764, 264, 3832, 2098, 295, 15866, 3754, 15892, 13, 51582, 51660, 407, 291, 362, 281, 2573, 484, 51702, 51702, 577, 281, 14722, 3754, 15892, 322, 264, 16687, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.13013340546204163, "compression_ratio": 1.8215962441314555, "no_speech_prob": 9.51568472373765e-06}, {"id": 1170, "seek": 599350, "start": 5993.5, "end": 5995.7, "text": " Right. So that's an example.", "tokens": [50364, 1779, 13, 407, 300, 311, 364, 1365, 13, 50474, 50474, 823, 11, 510, 311, 746, 257, 707, 544, 6179, 13, 50610, 50610, 11739, 586, 300, 291, 362, 257, 805, 35, 30211, 50822, 50822, 293, 291, 434, 23384, 11, 286, 500, 380, 458, 11, 257, 21621, 11, 50980, 50980, 1580, 733, 295, 294, 1868, 295, 257, 805, 35, 30211, 11, 51090, 51210, 293, 300, 954, 575, 257, 1729, 10774, 11, 51302, 51302, 718, 311, 584, 411, 341, 11, 1392, 30, 51372, 51372, 400, 550, 291, 747, 1071, 805, 35, 3036, 11, 51622, 51658, 805, 35, 1412, 490, 1071, 954, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.13381576538085938, "compression_ratio": 1.5318181818181817, "no_speech_prob": 4.2892252167803235e-06}, {"id": 1171, "seek": 599350, "start": 5995.7, "end": 5998.42, "text": " Now, here's something a little more complicated.", "tokens": [50364, 1779, 13, 407, 300, 311, 364, 1365, 13, 50474, 50474, 823, 11, 510, 311, 746, 257, 707, 544, 6179, 13, 50610, 50610, 11739, 586, 300, 291, 362, 257, 805, 35, 30211, 50822, 50822, 293, 291, 434, 23384, 11, 286, 500, 380, 458, 11, 257, 21621, 11, 50980, 50980, 1580, 733, 295, 294, 1868, 295, 257, 805, 35, 30211, 11, 51090, 51210, 293, 300, 954, 575, 257, 1729, 10774, 11, 51302, 51302, 718, 311, 584, 411, 341, 11, 1392, 30, 51372, 51372, 400, 550, 291, 747, 1071, 805, 35, 3036, 11, 51622, 51658, 805, 35, 1412, 490, 1071, 954, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.13381576538085938, "compression_ratio": 1.5318181818181817, "no_speech_prob": 4.2892252167803235e-06}, {"id": 1172, "seek": 599350, "start": 5998.42, "end": 6002.66, "text": " Imagine now that you have a 3D scanner", "tokens": [50364, 1779, 13, 407, 300, 311, 364, 1365, 13, 50474, 50474, 823, 11, 510, 311, 746, 257, 707, 544, 6179, 13, 50610, 50610, 11739, 586, 300, 291, 362, 257, 805, 35, 30211, 50822, 50822, 293, 291, 434, 23384, 11, 286, 500, 380, 458, 11, 257, 21621, 11, 50980, 50980, 1580, 733, 295, 294, 1868, 295, 257, 805, 35, 30211, 11, 51090, 51210, 293, 300, 954, 575, 257, 1729, 10774, 11, 51302, 51302, 718, 311, 584, 411, 341, 11, 1392, 30, 51372, 51372, 400, 550, 291, 747, 1071, 805, 35, 3036, 11, 51622, 51658, 805, 35, 1412, 490, 1071, 954, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.13381576538085938, "compression_ratio": 1.5318181818181817, "no_speech_prob": 4.2892252167803235e-06}, {"id": 1173, "seek": 599350, "start": 6002.66, "end": 6005.82, "text": " and you're capturing, I don't know, a dancer,", "tokens": [50364, 1779, 13, 407, 300, 311, 364, 1365, 13, 50474, 50474, 823, 11, 510, 311, 746, 257, 707, 544, 6179, 13, 50610, 50610, 11739, 586, 300, 291, 362, 257, 805, 35, 30211, 50822, 50822, 293, 291, 434, 23384, 11, 286, 500, 380, 458, 11, 257, 21621, 11, 50980, 50980, 1580, 733, 295, 294, 1868, 295, 257, 805, 35, 30211, 11, 51090, 51210, 293, 300, 954, 575, 257, 1729, 10774, 11, 51302, 51302, 718, 311, 584, 411, 341, 11, 1392, 30, 51372, 51372, 400, 550, 291, 747, 1071, 805, 35, 3036, 11, 51622, 51658, 805, 35, 1412, 490, 1071, 954, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.13381576538085938, "compression_ratio": 1.5318181818181817, "no_speech_prob": 4.2892252167803235e-06}, {"id": 1174, "seek": 599350, "start": 6005.82, "end": 6008.02, "text": " someone kind of in front of a 3D scanner,", "tokens": [50364, 1779, 13, 407, 300, 311, 364, 1365, 13, 50474, 50474, 823, 11, 510, 311, 746, 257, 707, 544, 6179, 13, 50610, 50610, 11739, 586, 300, 291, 362, 257, 805, 35, 30211, 50822, 50822, 293, 291, 434, 23384, 11, 286, 500, 380, 458, 11, 257, 21621, 11, 50980, 50980, 1580, 733, 295, 294, 1868, 295, 257, 805, 35, 30211, 11, 51090, 51210, 293, 300, 954, 575, 257, 1729, 10774, 11, 51302, 51302, 718, 311, 584, 411, 341, 11, 1392, 30, 51372, 51372, 400, 550, 291, 747, 1071, 805, 35, 3036, 11, 51622, 51658, 805, 35, 1412, 490, 1071, 954, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.13381576538085938, "compression_ratio": 1.5318181818181817, "no_speech_prob": 4.2892252167803235e-06}, {"id": 1175, "seek": 599350, "start": 6010.42, "end": 6012.26, "text": " and that person has a particular pose,", "tokens": [50364, 1779, 13, 407, 300, 311, 364, 1365, 13, 50474, 50474, 823, 11, 510, 311, 746, 257, 707, 544, 6179, 13, 50610, 50610, 11739, 586, 300, 291, 362, 257, 805, 35, 30211, 50822, 50822, 293, 291, 434, 23384, 11, 286, 500, 380, 458, 11, 257, 21621, 11, 50980, 50980, 1580, 733, 295, 294, 1868, 295, 257, 805, 35, 30211, 11, 51090, 51210, 293, 300, 954, 575, 257, 1729, 10774, 11, 51302, 51302, 718, 311, 584, 411, 341, 11, 1392, 30, 51372, 51372, 400, 550, 291, 747, 1071, 805, 35, 3036, 11, 51622, 51658, 805, 35, 1412, 490, 1071, 954, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.13381576538085938, "compression_ratio": 1.5318181818181817, "no_speech_prob": 4.2892252167803235e-06}, {"id": 1176, "seek": 599350, "start": 6012.26, "end": 6013.66, "text": " let's say like this, okay?", "tokens": [50364, 1779, 13, 407, 300, 311, 364, 1365, 13, 50474, 50474, 823, 11, 510, 311, 746, 257, 707, 544, 6179, 13, 50610, 50610, 11739, 586, 300, 291, 362, 257, 805, 35, 30211, 50822, 50822, 293, 291, 434, 23384, 11, 286, 500, 380, 458, 11, 257, 21621, 11, 50980, 50980, 1580, 733, 295, 294, 1868, 295, 257, 805, 35, 30211, 11, 51090, 51210, 293, 300, 954, 575, 257, 1729, 10774, 11, 51302, 51302, 718, 311, 584, 411, 341, 11, 1392, 30, 51372, 51372, 400, 550, 291, 747, 1071, 805, 35, 3036, 11, 51622, 51658, 805, 35, 1412, 490, 1071, 954, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.13381576538085938, "compression_ratio": 1.5318181818181817, "no_speech_prob": 4.2892252167803235e-06}, {"id": 1177, "seek": 599350, "start": 6013.66, "end": 6018.66, "text": " And then you take another 3D picture,", "tokens": [50364, 1779, 13, 407, 300, 311, 364, 1365, 13, 50474, 50474, 823, 11, 510, 311, 746, 257, 707, 544, 6179, 13, 50610, 50610, 11739, 586, 300, 291, 362, 257, 805, 35, 30211, 50822, 50822, 293, 291, 434, 23384, 11, 286, 500, 380, 458, 11, 257, 21621, 11, 50980, 50980, 1580, 733, 295, 294, 1868, 295, 257, 805, 35, 30211, 11, 51090, 51210, 293, 300, 954, 575, 257, 1729, 10774, 11, 51302, 51302, 718, 311, 584, 411, 341, 11, 1392, 30, 51372, 51372, 400, 550, 291, 747, 1071, 805, 35, 3036, 11, 51622, 51658, 805, 35, 1412, 490, 1071, 954, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.13381576538085938, "compression_ratio": 1.5318181818181817, "no_speech_prob": 4.2892252167803235e-06}, {"id": 1178, "seek": 599350, "start": 6019.38, "end": 6020.78, "text": " 3D data from another person,", "tokens": [50364, 1779, 13, 407, 300, 311, 364, 1365, 13, 50474, 50474, 823, 11, 510, 311, 746, 257, 707, 544, 6179, 13, 50610, 50610, 11739, 586, 300, 291, 362, 257, 805, 35, 30211, 50822, 50822, 293, 291, 434, 23384, 11, 286, 500, 380, 458, 11, 257, 21621, 11, 50980, 50980, 1580, 733, 295, 294, 1868, 295, 257, 805, 35, 30211, 11, 51090, 51210, 293, 300, 954, 575, 257, 1729, 10774, 11, 51302, 51302, 718, 311, 584, 411, 341, 11, 1392, 30, 51372, 51372, 400, 550, 291, 747, 1071, 805, 35, 3036, 11, 51622, 51658, 805, 35, 1412, 490, 1071, 954, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.13381576538085938, "compression_ratio": 1.5318181818181817, "no_speech_prob": 4.2892252167803235e-06}, {"id": 1179, "seek": 602078, "start": 6020.78, "end": 6024.3, "text": " and that other person is in another pose.", "tokens": [50364, 293, 300, 661, 954, 307, 294, 1071, 10774, 13, 50540, 50540, 663, 954, 575, 257, 819, 1772, 3909, 13, 50676, 50676, 1240, 311, 294, 257, 819, 1772, 10774, 13, 50756, 50818, 400, 586, 437, 291, 528, 307, 291, 528, 281, 312, 1075, 281, 4471, 472, 50994, 50994, 3911, 264, 661, 13, 51036, 51036, 509, 528, 281, 312, 1075, 281, 584, 411, 11, 51138, 51138, 437, 307, 264, 1011, 294, 264, 700, 954, 30, 51230, 51230, 2305, 307, 264, 1011, 294, 264, 1150, 954, 30, 51340, 51340, 407, 437, 291, 362, 281, 360, 586, 307, 1936, 362, 257, 18161, 2533, 51516, 51516, 300, 2516, 666, 2696, 257, 805, 35, 17407, 51652, 51652, 300, 8855, 264, 18426, 295, 257, 1011, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1101495867869893, "compression_ratio": 1.8448275862068966, "no_speech_prob": 9.223108463629615e-06}, {"id": 1180, "seek": 602078, "start": 6024.3, "end": 6027.0199999999995, "text": " That person has a different body shape.", "tokens": [50364, 293, 300, 661, 954, 307, 294, 1071, 10774, 13, 50540, 50540, 663, 954, 575, 257, 819, 1772, 3909, 13, 50676, 50676, 1240, 311, 294, 257, 819, 1772, 10774, 13, 50756, 50818, 400, 586, 437, 291, 528, 307, 291, 528, 281, 312, 1075, 281, 4471, 472, 50994, 50994, 3911, 264, 661, 13, 51036, 51036, 509, 528, 281, 312, 1075, 281, 584, 411, 11, 51138, 51138, 437, 307, 264, 1011, 294, 264, 700, 954, 30, 51230, 51230, 2305, 307, 264, 1011, 294, 264, 1150, 954, 30, 51340, 51340, 407, 437, 291, 362, 281, 360, 586, 307, 1936, 362, 257, 18161, 2533, 51516, 51516, 300, 2516, 666, 2696, 257, 805, 35, 17407, 51652, 51652, 300, 8855, 264, 18426, 295, 257, 1011, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1101495867869893, "compression_ratio": 1.8448275862068966, "no_speech_prob": 9.223108463629615e-06}, {"id": 1181, "seek": 602078, "start": 6027.0199999999995, "end": 6028.62, "text": " She's in a different body pose.", "tokens": [50364, 293, 300, 661, 954, 307, 294, 1071, 10774, 13, 50540, 50540, 663, 954, 575, 257, 819, 1772, 3909, 13, 50676, 50676, 1240, 311, 294, 257, 819, 1772, 10774, 13, 50756, 50818, 400, 586, 437, 291, 528, 307, 291, 528, 281, 312, 1075, 281, 4471, 472, 50994, 50994, 3911, 264, 661, 13, 51036, 51036, 509, 528, 281, 312, 1075, 281, 584, 411, 11, 51138, 51138, 437, 307, 264, 1011, 294, 264, 700, 954, 30, 51230, 51230, 2305, 307, 264, 1011, 294, 264, 1150, 954, 30, 51340, 51340, 407, 437, 291, 362, 281, 360, 586, 307, 1936, 362, 257, 18161, 2533, 51516, 51516, 300, 2516, 666, 2696, 257, 805, 35, 17407, 51652, 51652, 300, 8855, 264, 18426, 295, 257, 1011, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1101495867869893, "compression_ratio": 1.8448275862068966, "no_speech_prob": 9.223108463629615e-06}, {"id": 1182, "seek": 602078, "start": 6029.86, "end": 6033.38, "text": " And now what you want is you want to be able to map one", "tokens": [50364, 293, 300, 661, 954, 307, 294, 1071, 10774, 13, 50540, 50540, 663, 954, 575, 257, 819, 1772, 3909, 13, 50676, 50676, 1240, 311, 294, 257, 819, 1772, 10774, 13, 50756, 50818, 400, 586, 437, 291, 528, 307, 291, 528, 281, 312, 1075, 281, 4471, 472, 50994, 50994, 3911, 264, 661, 13, 51036, 51036, 509, 528, 281, 312, 1075, 281, 584, 411, 11, 51138, 51138, 437, 307, 264, 1011, 294, 264, 700, 954, 30, 51230, 51230, 2305, 307, 264, 1011, 294, 264, 1150, 954, 30, 51340, 51340, 407, 437, 291, 362, 281, 360, 586, 307, 1936, 362, 257, 18161, 2533, 51516, 51516, 300, 2516, 666, 2696, 257, 805, 35, 17407, 51652, 51652, 300, 8855, 264, 18426, 295, 257, 1011, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1101495867869893, "compression_ratio": 1.8448275862068966, "no_speech_prob": 9.223108463629615e-06}, {"id": 1183, "seek": 602078, "start": 6033.38, "end": 6034.219999999999, "text": " onto the other.", "tokens": [50364, 293, 300, 661, 954, 307, 294, 1071, 10774, 13, 50540, 50540, 663, 954, 575, 257, 819, 1772, 3909, 13, 50676, 50676, 1240, 311, 294, 257, 819, 1772, 10774, 13, 50756, 50818, 400, 586, 437, 291, 528, 307, 291, 528, 281, 312, 1075, 281, 4471, 472, 50994, 50994, 3911, 264, 661, 13, 51036, 51036, 509, 528, 281, 312, 1075, 281, 584, 411, 11, 51138, 51138, 437, 307, 264, 1011, 294, 264, 700, 954, 30, 51230, 51230, 2305, 307, 264, 1011, 294, 264, 1150, 954, 30, 51340, 51340, 407, 437, 291, 362, 281, 360, 586, 307, 1936, 362, 257, 18161, 2533, 51516, 51516, 300, 2516, 666, 2696, 257, 805, 35, 17407, 51652, 51652, 300, 8855, 264, 18426, 295, 257, 1011, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1101495867869893, "compression_ratio": 1.8448275862068966, "no_speech_prob": 9.223108463629615e-06}, {"id": 1184, "seek": 602078, "start": 6034.219999999999, "end": 6036.259999999999, "text": " You want to be able to say like,", "tokens": [50364, 293, 300, 661, 954, 307, 294, 1071, 10774, 13, 50540, 50540, 663, 954, 575, 257, 819, 1772, 3909, 13, 50676, 50676, 1240, 311, 294, 257, 819, 1772, 10774, 13, 50756, 50818, 400, 586, 437, 291, 528, 307, 291, 528, 281, 312, 1075, 281, 4471, 472, 50994, 50994, 3911, 264, 661, 13, 51036, 51036, 509, 528, 281, 312, 1075, 281, 584, 411, 11, 51138, 51138, 437, 307, 264, 1011, 294, 264, 700, 954, 30, 51230, 51230, 2305, 307, 264, 1011, 294, 264, 1150, 954, 30, 51340, 51340, 407, 437, 291, 362, 281, 360, 586, 307, 1936, 362, 257, 18161, 2533, 51516, 51516, 300, 2516, 666, 2696, 257, 805, 35, 17407, 51652, 51652, 300, 8855, 264, 18426, 295, 257, 1011, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1101495867869893, "compression_ratio": 1.8448275862068966, "no_speech_prob": 9.223108463629615e-06}, {"id": 1185, "seek": 602078, "start": 6036.259999999999, "end": 6038.099999999999, "text": " what is the hand in the first person?", "tokens": [50364, 293, 300, 661, 954, 307, 294, 1071, 10774, 13, 50540, 50540, 663, 954, 575, 257, 819, 1772, 3909, 13, 50676, 50676, 1240, 311, 294, 257, 819, 1772, 10774, 13, 50756, 50818, 400, 586, 437, 291, 528, 307, 291, 528, 281, 312, 1075, 281, 4471, 472, 50994, 50994, 3911, 264, 661, 13, 51036, 51036, 509, 528, 281, 312, 1075, 281, 584, 411, 11, 51138, 51138, 437, 307, 264, 1011, 294, 264, 700, 954, 30, 51230, 51230, 2305, 307, 264, 1011, 294, 264, 1150, 954, 30, 51340, 51340, 407, 437, 291, 362, 281, 360, 586, 307, 1936, 362, 257, 18161, 2533, 51516, 51516, 300, 2516, 666, 2696, 257, 805, 35, 17407, 51652, 51652, 300, 8855, 264, 18426, 295, 257, 1011, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1101495867869893, "compression_ratio": 1.8448275862068966, "no_speech_prob": 9.223108463629615e-06}, {"id": 1186, "seek": 602078, "start": 6038.099999999999, "end": 6040.3, "text": " Where is the hand in the second person?", "tokens": [50364, 293, 300, 661, 954, 307, 294, 1071, 10774, 13, 50540, 50540, 663, 954, 575, 257, 819, 1772, 3909, 13, 50676, 50676, 1240, 311, 294, 257, 819, 1772, 10774, 13, 50756, 50818, 400, 586, 437, 291, 528, 307, 291, 528, 281, 312, 1075, 281, 4471, 472, 50994, 50994, 3911, 264, 661, 13, 51036, 51036, 509, 528, 281, 312, 1075, 281, 584, 411, 11, 51138, 51138, 437, 307, 264, 1011, 294, 264, 700, 954, 30, 51230, 51230, 2305, 307, 264, 1011, 294, 264, 1150, 954, 30, 51340, 51340, 407, 437, 291, 362, 281, 360, 586, 307, 1936, 362, 257, 18161, 2533, 51516, 51516, 300, 2516, 666, 2696, 257, 805, 35, 17407, 51652, 51652, 300, 8855, 264, 18426, 295, 257, 1011, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1101495867869893, "compression_ratio": 1.8448275862068966, "no_speech_prob": 9.223108463629615e-06}, {"id": 1187, "seek": 602078, "start": 6040.3, "end": 6043.82, "text": " So what you have to do now is basically have a neural net", "tokens": [50364, 293, 300, 661, 954, 307, 294, 1071, 10774, 13, 50540, 50540, 663, 954, 575, 257, 819, 1772, 3909, 13, 50676, 50676, 1240, 311, 294, 257, 819, 1772, 10774, 13, 50756, 50818, 400, 586, 437, 291, 528, 307, 291, 528, 281, 312, 1075, 281, 4471, 472, 50994, 50994, 3911, 264, 661, 13, 51036, 51036, 509, 528, 281, 312, 1075, 281, 584, 411, 11, 51138, 51138, 437, 307, 264, 1011, 294, 264, 700, 954, 30, 51230, 51230, 2305, 307, 264, 1011, 294, 264, 1150, 954, 30, 51340, 51340, 407, 437, 291, 362, 281, 360, 586, 307, 1936, 362, 257, 18161, 2533, 51516, 51516, 300, 2516, 666, 2696, 257, 805, 35, 17407, 51652, 51652, 300, 8855, 264, 18426, 295, 257, 1011, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1101495867869893, "compression_ratio": 1.8448275862068966, "no_speech_prob": 9.223108463629615e-06}, {"id": 1188, "seek": 602078, "start": 6043.82, "end": 6046.54, "text": " that takes into account a 3D mesh", "tokens": [50364, 293, 300, 661, 954, 307, 294, 1071, 10774, 13, 50540, 50540, 663, 954, 575, 257, 819, 1772, 3909, 13, 50676, 50676, 1240, 311, 294, 257, 819, 1772, 10774, 13, 50756, 50818, 400, 586, 437, 291, 528, 307, 291, 528, 281, 312, 1075, 281, 4471, 472, 50994, 50994, 3911, 264, 661, 13, 51036, 51036, 509, 528, 281, 312, 1075, 281, 584, 411, 11, 51138, 51138, 437, 307, 264, 1011, 294, 264, 700, 954, 30, 51230, 51230, 2305, 307, 264, 1011, 294, 264, 1150, 954, 30, 51340, 51340, 407, 437, 291, 362, 281, 360, 586, 307, 1936, 362, 257, 18161, 2533, 51516, 51516, 300, 2516, 666, 2696, 257, 805, 35, 17407, 51652, 51652, 300, 8855, 264, 18426, 295, 257, 1011, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1101495867869893, "compression_ratio": 1.8448275862068966, "no_speech_prob": 9.223108463629615e-06}, {"id": 1189, "seek": 604654, "start": 6046.54, "end": 6051.54, "text": " that represents the geometry of a hand and train it", "tokens": [50364, 300, 8855, 264, 18426, 295, 257, 1011, 293, 3847, 309, 50614, 50682, 281, 980, 291, 309, 311, 257, 1011, 13, 50760, 50760, 407, 300, 562, 291, 3079, 309, 281, 264, 1011, 11, 50836, 50836, 309, 5112, 291, 309, 311, 257, 1011, 13, 50878, 50878, 1133, 291, 3079, 309, 281, 264, 661, 3166, 295, 264, 1772, 11, 50976, 50976, 309, 5112, 291, 309, 311, 746, 1646, 13, 51078, 51078, 583, 264, 1412, 291, 362, 307, 406, 364, 3256, 13, 51150, 51150, 467, 311, 257, 805, 35, 17407, 11, 1392, 30, 51250, 51250, 440, 17407, 815, 362, 819, 32179, 13, 51342, 51342, 440, 29896, 815, 5160, 412, 819, 3190, 13, 51496, 51496, 407, 577, 291, 6964, 428, 3754, 15892, 322, 257, 9274, 411, 341, 51630, 51630, 300, 307, 6695, 295, 264, 8669, 295, 264, 17407, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.08842378463188227, "compression_ratio": 1.867704280155642, "no_speech_prob": 5.6822027545422316e-06}, {"id": 1190, "seek": 604654, "start": 6052.9, "end": 6054.46, "text": " to tell you it's a hand.", "tokens": [50364, 300, 8855, 264, 18426, 295, 257, 1011, 293, 3847, 309, 50614, 50682, 281, 980, 291, 309, 311, 257, 1011, 13, 50760, 50760, 407, 300, 562, 291, 3079, 309, 281, 264, 1011, 11, 50836, 50836, 309, 5112, 291, 309, 311, 257, 1011, 13, 50878, 50878, 1133, 291, 3079, 309, 281, 264, 661, 3166, 295, 264, 1772, 11, 50976, 50976, 309, 5112, 291, 309, 311, 746, 1646, 13, 51078, 51078, 583, 264, 1412, 291, 362, 307, 406, 364, 3256, 13, 51150, 51150, 467, 311, 257, 805, 35, 17407, 11, 1392, 30, 51250, 51250, 440, 17407, 815, 362, 819, 32179, 13, 51342, 51342, 440, 29896, 815, 5160, 412, 819, 3190, 13, 51496, 51496, 407, 577, 291, 6964, 428, 3754, 15892, 322, 257, 9274, 411, 341, 51630, 51630, 300, 307, 6695, 295, 264, 8669, 295, 264, 17407, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.08842378463188227, "compression_ratio": 1.867704280155642, "no_speech_prob": 5.6822027545422316e-06}, {"id": 1191, "seek": 604654, "start": 6054.46, "end": 6055.98, "text": " So that when you apply it to the hand,", "tokens": [50364, 300, 8855, 264, 18426, 295, 257, 1011, 293, 3847, 309, 50614, 50682, 281, 980, 291, 309, 311, 257, 1011, 13, 50760, 50760, 407, 300, 562, 291, 3079, 309, 281, 264, 1011, 11, 50836, 50836, 309, 5112, 291, 309, 311, 257, 1011, 13, 50878, 50878, 1133, 291, 3079, 309, 281, 264, 661, 3166, 295, 264, 1772, 11, 50976, 50976, 309, 5112, 291, 309, 311, 746, 1646, 13, 51078, 51078, 583, 264, 1412, 291, 362, 307, 406, 364, 3256, 13, 51150, 51150, 467, 311, 257, 805, 35, 17407, 11, 1392, 30, 51250, 51250, 440, 17407, 815, 362, 819, 32179, 13, 51342, 51342, 440, 29896, 815, 5160, 412, 819, 3190, 13, 51496, 51496, 407, 577, 291, 6964, 428, 3754, 15892, 322, 257, 9274, 411, 341, 51630, 51630, 300, 307, 6695, 295, 264, 8669, 295, 264, 17407, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.08842378463188227, "compression_ratio": 1.867704280155642, "no_speech_prob": 5.6822027545422316e-06}, {"id": 1192, "seek": 604654, "start": 6055.98, "end": 6056.82, "text": " it tells you it's a hand.", "tokens": [50364, 300, 8855, 264, 18426, 295, 257, 1011, 293, 3847, 309, 50614, 50682, 281, 980, 291, 309, 311, 257, 1011, 13, 50760, 50760, 407, 300, 562, 291, 3079, 309, 281, 264, 1011, 11, 50836, 50836, 309, 5112, 291, 309, 311, 257, 1011, 13, 50878, 50878, 1133, 291, 3079, 309, 281, 264, 661, 3166, 295, 264, 1772, 11, 50976, 50976, 309, 5112, 291, 309, 311, 746, 1646, 13, 51078, 51078, 583, 264, 1412, 291, 362, 307, 406, 364, 3256, 13, 51150, 51150, 467, 311, 257, 805, 35, 17407, 11, 1392, 30, 51250, 51250, 440, 17407, 815, 362, 819, 32179, 13, 51342, 51342, 440, 29896, 815, 5160, 412, 819, 3190, 13, 51496, 51496, 407, 577, 291, 6964, 428, 3754, 15892, 322, 257, 9274, 411, 341, 51630, 51630, 300, 307, 6695, 295, 264, 8669, 295, 264, 17407, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.08842378463188227, "compression_ratio": 1.867704280155642, "no_speech_prob": 5.6822027545422316e-06}, {"id": 1193, "seek": 604654, "start": 6056.82, "end": 6058.78, "text": " When you apply it to the other parts of the body,", "tokens": [50364, 300, 8855, 264, 18426, 295, 257, 1011, 293, 3847, 309, 50614, 50682, 281, 980, 291, 309, 311, 257, 1011, 13, 50760, 50760, 407, 300, 562, 291, 3079, 309, 281, 264, 1011, 11, 50836, 50836, 309, 5112, 291, 309, 311, 257, 1011, 13, 50878, 50878, 1133, 291, 3079, 309, 281, 264, 661, 3166, 295, 264, 1772, 11, 50976, 50976, 309, 5112, 291, 309, 311, 746, 1646, 13, 51078, 51078, 583, 264, 1412, 291, 362, 307, 406, 364, 3256, 13, 51150, 51150, 467, 311, 257, 805, 35, 17407, 11, 1392, 30, 51250, 51250, 440, 17407, 815, 362, 819, 32179, 13, 51342, 51342, 440, 29896, 815, 5160, 412, 819, 3190, 13, 51496, 51496, 407, 577, 291, 6964, 428, 3754, 15892, 322, 257, 9274, 411, 341, 51630, 51630, 300, 307, 6695, 295, 264, 8669, 295, 264, 17407, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.08842378463188227, "compression_ratio": 1.867704280155642, "no_speech_prob": 5.6822027545422316e-06}, {"id": 1194, "seek": 604654, "start": 6058.78, "end": 6060.82, "text": " it tells you it's something else.", "tokens": [50364, 300, 8855, 264, 18426, 295, 257, 1011, 293, 3847, 309, 50614, 50682, 281, 980, 291, 309, 311, 257, 1011, 13, 50760, 50760, 407, 300, 562, 291, 3079, 309, 281, 264, 1011, 11, 50836, 50836, 309, 5112, 291, 309, 311, 257, 1011, 13, 50878, 50878, 1133, 291, 3079, 309, 281, 264, 661, 3166, 295, 264, 1772, 11, 50976, 50976, 309, 5112, 291, 309, 311, 746, 1646, 13, 51078, 51078, 583, 264, 1412, 291, 362, 307, 406, 364, 3256, 13, 51150, 51150, 467, 311, 257, 805, 35, 17407, 11, 1392, 30, 51250, 51250, 440, 17407, 815, 362, 819, 32179, 13, 51342, 51342, 440, 29896, 815, 5160, 412, 819, 3190, 13, 51496, 51496, 407, 577, 291, 6964, 428, 3754, 15892, 322, 257, 9274, 411, 341, 51630, 51630, 300, 307, 6695, 295, 264, 8669, 295, 264, 17407, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.08842378463188227, "compression_ratio": 1.867704280155642, "no_speech_prob": 5.6822027545422316e-06}, {"id": 1195, "seek": 604654, "start": 6060.82, "end": 6062.26, "text": " But the data you have is not an image.", "tokens": [50364, 300, 8855, 264, 18426, 295, 257, 1011, 293, 3847, 309, 50614, 50682, 281, 980, 291, 309, 311, 257, 1011, 13, 50760, 50760, 407, 300, 562, 291, 3079, 309, 281, 264, 1011, 11, 50836, 50836, 309, 5112, 291, 309, 311, 257, 1011, 13, 50878, 50878, 1133, 291, 3079, 309, 281, 264, 661, 3166, 295, 264, 1772, 11, 50976, 50976, 309, 5112, 291, 309, 311, 746, 1646, 13, 51078, 51078, 583, 264, 1412, 291, 362, 307, 406, 364, 3256, 13, 51150, 51150, 467, 311, 257, 805, 35, 17407, 11, 1392, 30, 51250, 51250, 440, 17407, 815, 362, 819, 32179, 13, 51342, 51342, 440, 29896, 815, 5160, 412, 819, 3190, 13, 51496, 51496, 407, 577, 291, 6964, 428, 3754, 15892, 322, 257, 9274, 411, 341, 51630, 51630, 300, 307, 6695, 295, 264, 8669, 295, 264, 17407, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.08842378463188227, "compression_ratio": 1.867704280155642, "no_speech_prob": 5.6822027545422316e-06}, {"id": 1196, "seek": 604654, "start": 6062.26, "end": 6064.26, "text": " It's a 3D mesh, okay?", "tokens": [50364, 300, 8855, 264, 18426, 295, 257, 1011, 293, 3847, 309, 50614, 50682, 281, 980, 291, 309, 311, 257, 1011, 13, 50760, 50760, 407, 300, 562, 291, 3079, 309, 281, 264, 1011, 11, 50836, 50836, 309, 5112, 291, 309, 311, 257, 1011, 13, 50878, 50878, 1133, 291, 3079, 309, 281, 264, 661, 3166, 295, 264, 1772, 11, 50976, 50976, 309, 5112, 291, 309, 311, 746, 1646, 13, 51078, 51078, 583, 264, 1412, 291, 362, 307, 406, 364, 3256, 13, 51150, 51150, 467, 311, 257, 805, 35, 17407, 11, 1392, 30, 51250, 51250, 440, 17407, 815, 362, 819, 32179, 13, 51342, 51342, 440, 29896, 815, 5160, 412, 819, 3190, 13, 51496, 51496, 407, 577, 291, 6964, 428, 3754, 15892, 322, 257, 9274, 411, 341, 51630, 51630, 300, 307, 6695, 295, 264, 8669, 295, 264, 17407, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.08842378463188227, "compression_ratio": 1.867704280155642, "no_speech_prob": 5.6822027545422316e-06}, {"id": 1197, "seek": 604654, "start": 6064.26, "end": 6066.1, "text": " The mesh may have different resolutions.", "tokens": [50364, 300, 8855, 264, 18426, 295, 257, 1011, 293, 3847, 309, 50614, 50682, 281, 980, 291, 309, 311, 257, 1011, 13, 50760, 50760, 407, 300, 562, 291, 3079, 309, 281, 264, 1011, 11, 50836, 50836, 309, 5112, 291, 309, 311, 257, 1011, 13, 50878, 50878, 1133, 291, 3079, 309, 281, 264, 661, 3166, 295, 264, 1772, 11, 50976, 50976, 309, 5112, 291, 309, 311, 746, 1646, 13, 51078, 51078, 583, 264, 1412, 291, 362, 307, 406, 364, 3256, 13, 51150, 51150, 467, 311, 257, 805, 35, 17407, 11, 1392, 30, 51250, 51250, 440, 17407, 815, 362, 819, 32179, 13, 51342, 51342, 440, 29896, 815, 5160, 412, 819, 3190, 13, 51496, 51496, 407, 577, 291, 6964, 428, 3754, 15892, 322, 257, 9274, 411, 341, 51630, 51630, 300, 307, 6695, 295, 264, 8669, 295, 264, 17407, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.08842378463188227, "compression_ratio": 1.867704280155642, "no_speech_prob": 5.6822027545422316e-06}, {"id": 1198, "seek": 604654, "start": 6066.1, "end": 6069.18, "text": " The triangles may occur at different places.", "tokens": [50364, 300, 8855, 264, 18426, 295, 257, 1011, 293, 3847, 309, 50614, 50682, 281, 980, 291, 309, 311, 257, 1011, 13, 50760, 50760, 407, 300, 562, 291, 3079, 309, 281, 264, 1011, 11, 50836, 50836, 309, 5112, 291, 309, 311, 257, 1011, 13, 50878, 50878, 1133, 291, 3079, 309, 281, 264, 661, 3166, 295, 264, 1772, 11, 50976, 50976, 309, 5112, 291, 309, 311, 746, 1646, 13, 51078, 51078, 583, 264, 1412, 291, 362, 307, 406, 364, 3256, 13, 51150, 51150, 467, 311, 257, 805, 35, 17407, 11, 1392, 30, 51250, 51250, 440, 17407, 815, 362, 819, 32179, 13, 51342, 51342, 440, 29896, 815, 5160, 412, 819, 3190, 13, 51496, 51496, 407, 577, 291, 6964, 428, 3754, 15892, 322, 257, 9274, 411, 341, 51630, 51630, 300, 307, 6695, 295, 264, 8669, 295, 264, 17407, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.08842378463188227, "compression_ratio": 1.867704280155642, "no_speech_prob": 5.6822027545422316e-06}, {"id": 1199, "seek": 604654, "start": 6069.18, "end": 6071.86, "text": " So how you define your convolutions on a domain like this", "tokens": [50364, 300, 8855, 264, 18426, 295, 257, 1011, 293, 3847, 309, 50614, 50682, 281, 980, 291, 309, 311, 257, 1011, 13, 50760, 50760, 407, 300, 562, 291, 3079, 309, 281, 264, 1011, 11, 50836, 50836, 309, 5112, 291, 309, 311, 257, 1011, 13, 50878, 50878, 1133, 291, 3079, 309, 281, 264, 661, 3166, 295, 264, 1772, 11, 50976, 50976, 309, 5112, 291, 309, 311, 746, 1646, 13, 51078, 51078, 583, 264, 1412, 291, 362, 307, 406, 364, 3256, 13, 51150, 51150, 467, 311, 257, 805, 35, 17407, 11, 1392, 30, 51250, 51250, 440, 17407, 815, 362, 819, 32179, 13, 51342, 51342, 440, 29896, 815, 5160, 412, 819, 3190, 13, 51496, 51496, 407, 577, 291, 6964, 428, 3754, 15892, 322, 257, 9274, 411, 341, 51630, 51630, 300, 307, 6695, 295, 264, 8669, 295, 264, 17407, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.08842378463188227, "compression_ratio": 1.867704280155642, "no_speech_prob": 5.6822027545422316e-06}, {"id": 1200, "seek": 604654, "start": 6071.86, "end": 6076.14, "text": " that is independent of the resolution of the mesh", "tokens": [50364, 300, 8855, 264, 18426, 295, 257, 1011, 293, 3847, 309, 50614, 50682, 281, 980, 291, 309, 311, 257, 1011, 13, 50760, 50760, 407, 300, 562, 291, 3079, 309, 281, 264, 1011, 11, 50836, 50836, 309, 5112, 291, 309, 311, 257, 1011, 13, 50878, 50878, 1133, 291, 3079, 309, 281, 264, 661, 3166, 295, 264, 1772, 11, 50976, 50976, 309, 5112, 291, 309, 311, 746, 1646, 13, 51078, 51078, 583, 264, 1412, 291, 362, 307, 406, 364, 3256, 13, 51150, 51150, 467, 311, 257, 805, 35, 17407, 11, 1392, 30, 51250, 51250, 440, 17407, 815, 362, 819, 32179, 13, 51342, 51342, 440, 29896, 815, 5160, 412, 819, 3190, 13, 51496, 51496, 407, 577, 291, 6964, 428, 3754, 15892, 322, 257, 9274, 411, 341, 51630, 51630, 300, 307, 6695, 295, 264, 8669, 295, 264, 17407, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.08842378463188227, "compression_ratio": 1.867704280155642, "no_speech_prob": 5.6822027545422316e-06}, {"id": 1201, "seek": 607614, "start": 6076.14, "end": 6078.9400000000005, "text": " and only kind of depends on the shape", "tokens": [50364, 293, 787, 733, 295, 5946, 322, 264, 3909, 50504, 50504, 370, 300, 291, 393, 33872, 428, 1011, 10060, 50592, 50592, 295, 264, 14764, 11, 264, 2744, 11, 264, 416, 8663, 50842, 50854, 293, 264, 1772, 3909, 295, 264, 954, 11, 721, 411, 300, 11, 558, 30, 51024, 51024, 407, 510, 311, 1071, 1365, 300, 311, 4317, 544, 1880, 13, 51274, 51336, 509, 528, 281, 3847, 746, 411, 257, 4832, 88, 311, 2533, 11, 51586, 51658, 457, 291, 528, 281, 3847, 264, 4832, 88, 311, 2533, 281, 980, 291, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.1401723467785379, "compression_ratio": 1.6699029126213591, "no_speech_prob": 4.092782546649687e-06}, {"id": 1202, "seek": 607614, "start": 6078.9400000000005, "end": 6080.700000000001, "text": " so that you can classify your hand regardless", "tokens": [50364, 293, 787, 733, 295, 5946, 322, 264, 3909, 50504, 50504, 370, 300, 291, 393, 33872, 428, 1011, 10060, 50592, 50592, 295, 264, 14764, 11, 264, 2744, 11, 264, 416, 8663, 50842, 50854, 293, 264, 1772, 3909, 295, 264, 954, 11, 721, 411, 300, 11, 558, 30, 51024, 51024, 407, 510, 311, 1071, 1365, 300, 311, 4317, 544, 1880, 13, 51274, 51336, 509, 528, 281, 3847, 746, 411, 257, 4832, 88, 311, 2533, 11, 51586, 51658, 457, 291, 528, 281, 3847, 264, 4832, 88, 311, 2533, 281, 980, 291, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.1401723467785379, "compression_ratio": 1.6699029126213591, "no_speech_prob": 4.092782546649687e-06}, {"id": 1203, "seek": 607614, "start": 6080.700000000001, "end": 6085.700000000001, "text": " of the orientation, the size, the conformation", "tokens": [50364, 293, 787, 733, 295, 5946, 322, 264, 3909, 50504, 50504, 370, 300, 291, 393, 33872, 428, 1011, 10060, 50592, 50592, 295, 264, 14764, 11, 264, 2744, 11, 264, 416, 8663, 50842, 50854, 293, 264, 1772, 3909, 295, 264, 954, 11, 721, 411, 300, 11, 558, 30, 51024, 51024, 407, 510, 311, 1071, 1365, 300, 311, 4317, 544, 1880, 13, 51274, 51336, 509, 528, 281, 3847, 746, 411, 257, 4832, 88, 311, 2533, 11, 51586, 51658, 457, 291, 528, 281, 3847, 264, 4832, 88, 311, 2533, 281, 980, 291, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.1401723467785379, "compression_ratio": 1.6699029126213591, "no_speech_prob": 4.092782546649687e-06}, {"id": 1204, "seek": 607614, "start": 6085.9400000000005, "end": 6089.34, "text": " and the body shape of the person, things like that, right?", "tokens": [50364, 293, 787, 733, 295, 5946, 322, 264, 3909, 50504, 50504, 370, 300, 291, 393, 33872, 428, 1011, 10060, 50592, 50592, 295, 264, 14764, 11, 264, 2744, 11, 264, 416, 8663, 50842, 50854, 293, 264, 1772, 3909, 295, 264, 954, 11, 721, 411, 300, 11, 558, 30, 51024, 51024, 407, 510, 311, 1071, 1365, 300, 311, 4317, 544, 1880, 13, 51274, 51336, 509, 528, 281, 3847, 746, 411, 257, 4832, 88, 311, 2533, 11, 51586, 51658, 457, 291, 528, 281, 3847, 264, 4832, 88, 311, 2533, 281, 980, 291, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.1401723467785379, "compression_ratio": 1.6699029126213591, "no_speech_prob": 4.092782546649687e-06}, {"id": 1205, "seek": 607614, "start": 6089.34, "end": 6094.34, "text": " So here's another example that's perhaps more interesting.", "tokens": [50364, 293, 787, 733, 295, 5946, 322, 264, 3909, 50504, 50504, 370, 300, 291, 393, 33872, 428, 1011, 10060, 50592, 50592, 295, 264, 14764, 11, 264, 2744, 11, 264, 416, 8663, 50842, 50854, 293, 264, 1772, 3909, 295, 264, 954, 11, 721, 411, 300, 11, 558, 30, 51024, 51024, 407, 510, 311, 1071, 1365, 300, 311, 4317, 544, 1880, 13, 51274, 51336, 509, 528, 281, 3847, 746, 411, 257, 4832, 88, 311, 2533, 11, 51586, 51658, 457, 291, 528, 281, 3847, 264, 4832, 88, 311, 2533, 281, 980, 291, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.1401723467785379, "compression_ratio": 1.6699029126213591, "no_speech_prob": 4.092782546649687e-06}, {"id": 1206, "seek": 607614, "start": 6095.58, "end": 6100.58, "text": " You want to train something like a Samy's net,", "tokens": [50364, 293, 787, 733, 295, 5946, 322, 264, 3909, 50504, 50504, 370, 300, 291, 393, 33872, 428, 1011, 10060, 50592, 50592, 295, 264, 14764, 11, 264, 2744, 11, 264, 416, 8663, 50842, 50854, 293, 264, 1772, 3909, 295, 264, 954, 11, 721, 411, 300, 11, 558, 30, 51024, 51024, 407, 510, 311, 1071, 1365, 300, 311, 4317, 544, 1880, 13, 51274, 51336, 509, 528, 281, 3847, 746, 411, 257, 4832, 88, 311, 2533, 11, 51586, 51658, 457, 291, 528, 281, 3847, 264, 4832, 88, 311, 2533, 281, 980, 291, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.1401723467785379, "compression_ratio": 1.6699029126213591, "no_speech_prob": 4.092782546649687e-06}, {"id": 1207, "seek": 607614, "start": 6102.02, "end": 6105.06, "text": " but you want to train the Samy's net to tell you", "tokens": [50364, 293, 787, 733, 295, 5946, 322, 264, 3909, 50504, 50504, 370, 300, 291, 393, 33872, 428, 1011, 10060, 50592, 50592, 295, 264, 14764, 11, 264, 2744, 11, 264, 416, 8663, 50842, 50854, 293, 264, 1772, 3909, 295, 264, 954, 11, 721, 411, 300, 11, 558, 30, 51024, 51024, 407, 510, 311, 1071, 1365, 300, 311, 4317, 544, 1880, 13, 51274, 51336, 509, 528, 281, 3847, 746, 411, 257, 4832, 88, 311, 2533, 11, 51586, 51658, 457, 291, 528, 281, 3847, 264, 4832, 88, 311, 2533, 281, 980, 291, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.1401723467785379, "compression_ratio": 1.6699029126213591, "no_speech_prob": 4.092782546649687e-06}, {"id": 1208, "seek": 610506, "start": 6105.06, "end": 6109.780000000001, "text": " whether one molecule is gonna stick to another molecule,", "tokens": [50364, 1968, 472, 15582, 307, 799, 2897, 281, 1071, 15582, 11, 50600, 50600, 558, 30, 50642, 50642, 407, 291, 976, 732, 13093, 281, 428, 18161, 2533, 50814, 50814, 293, 428, 18161, 2533, 14725, 732, 18875, 13, 50948, 50948, 759, 729, 732, 13093, 2897, 1214, 11, 51190, 51190, 309, 2709, 291, 732, 18875, 6104, 4560, 307, 1359, 11, 1392, 30, 51424, 51424, 400, 498, 436, 500, 380, 2897, 1214, 11, 550, 264, 4560, 307, 2416, 13, 51540, 51540, 1033, 11, 370, 291, 393, 519, 295, 264, 4560, 51608, 51608, 382, 733, 295, 264, 3671, 1737, 2281, 295, 264, 17359, 11, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.09562468996234968, "compression_ratio": 1.8685446009389672, "no_speech_prob": 1.834014074120205e-05}, {"id": 1209, "seek": 610506, "start": 6109.780000000001, "end": 6110.620000000001, "text": " right?", "tokens": [50364, 1968, 472, 15582, 307, 799, 2897, 281, 1071, 15582, 11, 50600, 50600, 558, 30, 50642, 50642, 407, 291, 976, 732, 13093, 281, 428, 18161, 2533, 50814, 50814, 293, 428, 18161, 2533, 14725, 732, 18875, 13, 50948, 50948, 759, 729, 732, 13093, 2897, 1214, 11, 51190, 51190, 309, 2709, 291, 732, 18875, 6104, 4560, 307, 1359, 11, 1392, 30, 51424, 51424, 400, 498, 436, 500, 380, 2897, 1214, 11, 550, 264, 4560, 307, 2416, 13, 51540, 51540, 1033, 11, 370, 291, 393, 519, 295, 264, 4560, 51608, 51608, 382, 733, 295, 264, 3671, 1737, 2281, 295, 264, 17359, 11, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.09562468996234968, "compression_ratio": 1.8685446009389672, "no_speech_prob": 1.834014074120205e-05}, {"id": 1210, "seek": 610506, "start": 6110.620000000001, "end": 6114.06, "text": " So you give two molecules to your neural net", "tokens": [50364, 1968, 472, 15582, 307, 799, 2897, 281, 1071, 15582, 11, 50600, 50600, 558, 30, 50642, 50642, 407, 291, 976, 732, 13093, 281, 428, 18161, 2533, 50814, 50814, 293, 428, 18161, 2533, 14725, 732, 18875, 13, 50948, 50948, 759, 729, 732, 13093, 2897, 1214, 11, 51190, 51190, 309, 2709, 291, 732, 18875, 6104, 4560, 307, 1359, 11, 1392, 30, 51424, 51424, 400, 498, 436, 500, 380, 2897, 1214, 11, 550, 264, 4560, 307, 2416, 13, 51540, 51540, 1033, 11, 370, 291, 393, 519, 295, 264, 4560, 51608, 51608, 382, 733, 295, 264, 3671, 1737, 2281, 295, 264, 17359, 11, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.09562468996234968, "compression_ratio": 1.8685446009389672, "no_speech_prob": 1.834014074120205e-05}, {"id": 1211, "seek": 610506, "start": 6114.06, "end": 6116.740000000001, "text": " and your neural net produces two vectors.", "tokens": [50364, 1968, 472, 15582, 307, 799, 2897, 281, 1071, 15582, 11, 50600, 50600, 558, 30, 50642, 50642, 407, 291, 976, 732, 13093, 281, 428, 18161, 2533, 50814, 50814, 293, 428, 18161, 2533, 14725, 732, 18875, 13, 50948, 50948, 759, 729, 732, 13093, 2897, 1214, 11, 51190, 51190, 309, 2709, 291, 732, 18875, 6104, 4560, 307, 1359, 11, 1392, 30, 51424, 51424, 400, 498, 436, 500, 380, 2897, 1214, 11, 550, 264, 4560, 307, 2416, 13, 51540, 51540, 1033, 11, 370, 291, 393, 519, 295, 264, 4560, 51608, 51608, 382, 733, 295, 264, 3671, 1737, 2281, 295, 264, 17359, 11, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.09562468996234968, "compression_ratio": 1.8685446009389672, "no_speech_prob": 1.834014074120205e-05}, {"id": 1212, "seek": 610506, "start": 6116.740000000001, "end": 6121.580000000001, "text": " If those two molecules stick together,", "tokens": [50364, 1968, 472, 15582, 307, 799, 2897, 281, 1071, 15582, 11, 50600, 50600, 558, 30, 50642, 50642, 407, 291, 976, 732, 13093, 281, 428, 18161, 2533, 50814, 50814, 293, 428, 18161, 2533, 14725, 732, 18875, 13, 50948, 50948, 759, 729, 732, 13093, 2897, 1214, 11, 51190, 51190, 309, 2709, 291, 732, 18875, 6104, 4560, 307, 1359, 11, 1392, 30, 51424, 51424, 400, 498, 436, 500, 380, 2897, 1214, 11, 550, 264, 4560, 307, 2416, 13, 51540, 51540, 1033, 11, 370, 291, 393, 519, 295, 264, 4560, 51608, 51608, 382, 733, 295, 264, 3671, 1737, 2281, 295, 264, 17359, 11, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.09562468996234968, "compression_ratio": 1.8685446009389672, "no_speech_prob": 1.834014074120205e-05}, {"id": 1213, "seek": 610506, "start": 6121.580000000001, "end": 6126.26, "text": " it gives you two vectors whose distance is small, okay?", "tokens": [50364, 1968, 472, 15582, 307, 799, 2897, 281, 1071, 15582, 11, 50600, 50600, 558, 30, 50642, 50642, 407, 291, 976, 732, 13093, 281, 428, 18161, 2533, 50814, 50814, 293, 428, 18161, 2533, 14725, 732, 18875, 13, 50948, 50948, 759, 729, 732, 13093, 2897, 1214, 11, 51190, 51190, 309, 2709, 291, 732, 18875, 6104, 4560, 307, 1359, 11, 1392, 30, 51424, 51424, 400, 498, 436, 500, 380, 2897, 1214, 11, 550, 264, 4560, 307, 2416, 13, 51540, 51540, 1033, 11, 370, 291, 393, 519, 295, 264, 4560, 51608, 51608, 382, 733, 295, 264, 3671, 1737, 2281, 295, 264, 17359, 11, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.09562468996234968, "compression_ratio": 1.8685446009389672, "no_speech_prob": 1.834014074120205e-05}, {"id": 1214, "seek": 610506, "start": 6126.26, "end": 6128.580000000001, "text": " And if they don't stick together, then the distance is large.", "tokens": [50364, 1968, 472, 15582, 307, 799, 2897, 281, 1071, 15582, 11, 50600, 50600, 558, 30, 50642, 50642, 407, 291, 976, 732, 13093, 281, 428, 18161, 2533, 50814, 50814, 293, 428, 18161, 2533, 14725, 732, 18875, 13, 50948, 50948, 759, 729, 732, 13093, 2897, 1214, 11, 51190, 51190, 309, 2709, 291, 732, 18875, 6104, 4560, 307, 1359, 11, 1392, 30, 51424, 51424, 400, 498, 436, 500, 380, 2897, 1214, 11, 550, 264, 4560, 307, 2416, 13, 51540, 51540, 1033, 11, 370, 291, 393, 519, 295, 264, 4560, 51608, 51608, 382, 733, 295, 264, 3671, 1737, 2281, 295, 264, 17359, 11, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.09562468996234968, "compression_ratio": 1.8685446009389672, "no_speech_prob": 1.834014074120205e-05}, {"id": 1215, "seek": 610506, "start": 6128.580000000001, "end": 6129.9400000000005, "text": " Okay, so you can think of the distance", "tokens": [50364, 1968, 472, 15582, 307, 799, 2897, 281, 1071, 15582, 11, 50600, 50600, 558, 30, 50642, 50642, 407, 291, 976, 732, 13093, 281, 428, 18161, 2533, 50814, 50814, 293, 428, 18161, 2533, 14725, 732, 18875, 13, 50948, 50948, 759, 729, 732, 13093, 2897, 1214, 11, 51190, 51190, 309, 2709, 291, 732, 18875, 6104, 4560, 307, 1359, 11, 1392, 30, 51424, 51424, 400, 498, 436, 500, 380, 2897, 1214, 11, 550, 264, 4560, 307, 2416, 13, 51540, 51540, 1033, 11, 370, 291, 393, 519, 295, 264, 4560, 51608, 51608, 382, 733, 295, 264, 3671, 1737, 2281, 295, 264, 17359, 11, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.09562468996234968, "compression_ratio": 1.8685446009389672, "no_speech_prob": 1.834014074120205e-05}, {"id": 1216, "seek": 610506, "start": 6129.9400000000005, "end": 6132.3, "text": " as kind of the negative free energy of the binding,", "tokens": [50364, 1968, 472, 15582, 307, 799, 2897, 281, 1071, 15582, 11, 50600, 50600, 558, 30, 50642, 50642, 407, 291, 976, 732, 13093, 281, 428, 18161, 2533, 50814, 50814, 293, 428, 18161, 2533, 14725, 732, 18875, 13, 50948, 50948, 759, 729, 732, 13093, 2897, 1214, 11, 51190, 51190, 309, 2709, 291, 732, 18875, 6104, 4560, 307, 1359, 11, 1392, 30, 51424, 51424, 400, 498, 436, 500, 380, 2897, 1214, 11, 550, 264, 4560, 307, 2416, 13, 51540, 51540, 1033, 11, 370, 291, 393, 519, 295, 264, 4560, 51608, 51608, 382, 733, 295, 264, 3671, 1737, 2281, 295, 264, 17359, 11, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.09562468996234968, "compression_ratio": 1.8685446009389672, "no_speech_prob": 1.834014074120205e-05}, {"id": 1217, "seek": 613230, "start": 6132.3, "end": 6137.3, "text": " the binding energy of the two molecules, right?", "tokens": [50364, 264, 17359, 2281, 295, 264, 732, 13093, 11, 558, 30, 50614, 50814, 1610, 264, 1737, 2281, 3175, 264, 5754, 11, 597, 291, 528, 13, 51020, 51268, 407, 291, 576, 3847, 341, 382, 257, 4832, 88, 311, 2533, 11, 51358, 51358, 457, 550, 264, 1154, 307, 577, 291, 2906, 257, 15582, 51522, 51522, 281, 257, 3209, 5276, 300, 309, 311, 264, 912, 3209, 51628, 51628, 291, 434, 516, 281, 3079, 281, 341, 15582, 293, 300, 15582, 51726, 51726, 293, 264, 732, 13093, 500, 380, 362, 264, 912, 3909, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.13751008199608844, "compression_ratio": 1.7241379310344827, "no_speech_prob": 8.800180694379378e-06}, {"id": 1218, "seek": 613230, "start": 6141.3, "end": 6145.42, "text": " Or the free energy minus the constant, which you want.", "tokens": [50364, 264, 17359, 2281, 295, 264, 732, 13093, 11, 558, 30, 50614, 50814, 1610, 264, 1737, 2281, 3175, 264, 5754, 11, 597, 291, 528, 13, 51020, 51268, 407, 291, 576, 3847, 341, 382, 257, 4832, 88, 311, 2533, 11, 51358, 51358, 457, 550, 264, 1154, 307, 577, 291, 2906, 257, 15582, 51522, 51522, 281, 257, 3209, 5276, 300, 309, 311, 264, 912, 3209, 51628, 51628, 291, 434, 516, 281, 3079, 281, 341, 15582, 293, 300, 15582, 51726, 51726, 293, 264, 732, 13093, 500, 380, 362, 264, 912, 3909, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.13751008199608844, "compression_ratio": 1.7241379310344827, "no_speech_prob": 8.800180694379378e-06}, {"id": 1219, "seek": 613230, "start": 6150.38, "end": 6152.18, "text": " So you would train this as a Samy's net,", "tokens": [50364, 264, 17359, 2281, 295, 264, 732, 13093, 11, 558, 30, 50614, 50814, 1610, 264, 1737, 2281, 3175, 264, 5754, 11, 597, 291, 528, 13, 51020, 51268, 407, 291, 576, 3847, 341, 382, 257, 4832, 88, 311, 2533, 11, 51358, 51358, 457, 550, 264, 1154, 307, 577, 291, 2906, 257, 15582, 51522, 51522, 281, 257, 3209, 5276, 300, 309, 311, 264, 912, 3209, 51628, 51628, 291, 434, 516, 281, 3079, 281, 341, 15582, 293, 300, 15582, 51726, 51726, 293, 264, 732, 13093, 500, 380, 362, 264, 912, 3909, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.13751008199608844, "compression_ratio": 1.7241379310344827, "no_speech_prob": 8.800180694379378e-06}, {"id": 1220, "seek": 613230, "start": 6152.18, "end": 6155.46, "text": " but then the problem is how you represent a molecule", "tokens": [50364, 264, 17359, 2281, 295, 264, 732, 13093, 11, 558, 30, 50614, 50814, 1610, 264, 1737, 2281, 3175, 264, 5754, 11, 597, 291, 528, 13, 51020, 51268, 407, 291, 576, 3847, 341, 382, 257, 4832, 88, 311, 2533, 11, 51358, 51358, 457, 550, 264, 1154, 307, 577, 291, 2906, 257, 15582, 51522, 51522, 281, 257, 3209, 5276, 300, 309, 311, 264, 912, 3209, 51628, 51628, 291, 434, 516, 281, 3079, 281, 341, 15582, 293, 300, 15582, 51726, 51726, 293, 264, 732, 13093, 500, 380, 362, 264, 912, 3909, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.13751008199608844, "compression_ratio": 1.7241379310344827, "no_speech_prob": 8.800180694379378e-06}, {"id": 1221, "seek": 613230, "start": 6155.46, "end": 6157.58, "text": " to a network knowing that it's the same network", "tokens": [50364, 264, 17359, 2281, 295, 264, 732, 13093, 11, 558, 30, 50614, 50814, 1610, 264, 1737, 2281, 3175, 264, 5754, 11, 597, 291, 528, 13, 51020, 51268, 407, 291, 576, 3847, 341, 382, 257, 4832, 88, 311, 2533, 11, 51358, 51358, 457, 550, 264, 1154, 307, 577, 291, 2906, 257, 15582, 51522, 51522, 281, 257, 3209, 5276, 300, 309, 311, 264, 912, 3209, 51628, 51628, 291, 434, 516, 281, 3079, 281, 341, 15582, 293, 300, 15582, 51726, 51726, 293, 264, 732, 13093, 500, 380, 362, 264, 912, 3909, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.13751008199608844, "compression_ratio": 1.7241379310344827, "no_speech_prob": 8.800180694379378e-06}, {"id": 1222, "seek": 613230, "start": 6157.58, "end": 6159.54, "text": " you're going to apply to this molecule and that molecule", "tokens": [50364, 264, 17359, 2281, 295, 264, 732, 13093, 11, 558, 30, 50614, 50814, 1610, 264, 1737, 2281, 3175, 264, 5754, 11, 597, 291, 528, 13, 51020, 51268, 407, 291, 576, 3847, 341, 382, 257, 4832, 88, 311, 2533, 11, 51358, 51358, 457, 550, 264, 1154, 307, 577, 291, 2906, 257, 15582, 51522, 51522, 281, 257, 3209, 5276, 300, 309, 311, 264, 912, 3209, 51628, 51628, 291, 434, 516, 281, 3079, 281, 341, 15582, 293, 300, 15582, 51726, 51726, 293, 264, 732, 13093, 500, 380, 362, 264, 912, 3909, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.13751008199608844, "compression_ratio": 1.7241379310344827, "no_speech_prob": 8.800180694379378e-06}, {"id": 1223, "seek": 613230, "start": 6159.54, "end": 6161.34, "text": " and the two molecules don't have the same shape.", "tokens": [50364, 264, 17359, 2281, 295, 264, 732, 13093, 11, 558, 30, 50614, 50814, 1610, 264, 1737, 2281, 3175, 264, 5754, 11, 597, 291, 528, 13, 51020, 51268, 407, 291, 576, 3847, 341, 382, 257, 4832, 88, 311, 2533, 11, 51358, 51358, 457, 550, 264, 1154, 307, 577, 291, 2906, 257, 15582, 51522, 51522, 281, 257, 3209, 5276, 300, 309, 311, 264, 912, 3209, 51628, 51628, 291, 434, 516, 281, 3079, 281, 341, 15582, 293, 300, 15582, 51726, 51726, 293, 264, 732, 13093, 500, 380, 362, 264, 912, 3909, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.13751008199608844, "compression_ratio": 1.7241379310344827, "no_speech_prob": 8.800180694379378e-06}, {"id": 1224, "seek": 616134, "start": 6161.34, "end": 6163.22, "text": " They don't have the same length.", "tokens": [50364, 814, 500, 380, 362, 264, 912, 4641, 13, 50458, 50458, 814, 500, 380, 362, 264, 912, 1230, 295, 16871, 13, 50518, 50518, 814, 500, 380, 362, 264, 912, 11, 50562, 50562, 411, 577, 360, 291, 2906, 257, 15582, 30, 50698, 50698, 440, 1151, 636, 281, 2906, 257, 15582, 307, 382, 257, 4295, 13, 50820, 50820, 467, 311, 1936, 257, 4295, 6104, 3877, 2962, 50932, 50932, 365, 264, 15582, 13, 51000, 51000, 400, 341, 4295, 307, 25339, 770, 538, 264, 6575, 51250, 51250, 295, 264, 16871, 412, 1184, 3621, 11, 51354, 51354, 1310, 538, 641, 4914, 294, 805, 35, 1901, 51466, 51466, 420, 641, 4972, 4914, 11, 51528, 51528, 1310, 538, 264, 5802, 295, 264, 14713, 51652, 51652, 1296, 732, 48043, 16871, 51756, 51830], "temperature": 0.0, "avg_logprob": -0.08774785614013672, "compression_ratio": 1.860082304526749, "no_speech_prob": 1.1840696970466524e-05}, {"id": 1225, "seek": 616134, "start": 6163.22, "end": 6164.42, "text": " They don't have the same number of atoms.", "tokens": [50364, 814, 500, 380, 362, 264, 912, 4641, 13, 50458, 50458, 814, 500, 380, 362, 264, 912, 1230, 295, 16871, 13, 50518, 50518, 814, 500, 380, 362, 264, 912, 11, 50562, 50562, 411, 577, 360, 291, 2906, 257, 15582, 30, 50698, 50698, 440, 1151, 636, 281, 2906, 257, 15582, 307, 382, 257, 4295, 13, 50820, 50820, 467, 311, 1936, 257, 4295, 6104, 3877, 2962, 50932, 50932, 365, 264, 15582, 13, 51000, 51000, 400, 341, 4295, 307, 25339, 770, 538, 264, 6575, 51250, 51250, 295, 264, 16871, 412, 1184, 3621, 11, 51354, 51354, 1310, 538, 641, 4914, 294, 805, 35, 1901, 51466, 51466, 420, 641, 4972, 4914, 11, 51528, 51528, 1310, 538, 264, 5802, 295, 264, 14713, 51652, 51652, 1296, 732, 48043, 16871, 51756, 51830], "temperature": 0.0, "avg_logprob": -0.08774785614013672, "compression_ratio": 1.860082304526749, "no_speech_prob": 1.1840696970466524e-05}, {"id": 1226, "seek": 616134, "start": 6164.42, "end": 6165.3, "text": " They don't have the same,", "tokens": [50364, 814, 500, 380, 362, 264, 912, 4641, 13, 50458, 50458, 814, 500, 380, 362, 264, 912, 1230, 295, 16871, 13, 50518, 50518, 814, 500, 380, 362, 264, 912, 11, 50562, 50562, 411, 577, 360, 291, 2906, 257, 15582, 30, 50698, 50698, 440, 1151, 636, 281, 2906, 257, 15582, 307, 382, 257, 4295, 13, 50820, 50820, 467, 311, 1936, 257, 4295, 6104, 3877, 2962, 50932, 50932, 365, 264, 15582, 13, 51000, 51000, 400, 341, 4295, 307, 25339, 770, 538, 264, 6575, 51250, 51250, 295, 264, 16871, 412, 1184, 3621, 11, 51354, 51354, 1310, 538, 641, 4914, 294, 805, 35, 1901, 51466, 51466, 420, 641, 4972, 4914, 11, 51528, 51528, 1310, 538, 264, 5802, 295, 264, 14713, 51652, 51652, 1296, 732, 48043, 16871, 51756, 51830], "temperature": 0.0, "avg_logprob": -0.08774785614013672, "compression_ratio": 1.860082304526749, "no_speech_prob": 1.1840696970466524e-05}, {"id": 1227, "seek": 616134, "start": 6165.3, "end": 6168.02, "text": " like how do you represent a molecule?", "tokens": [50364, 814, 500, 380, 362, 264, 912, 4641, 13, 50458, 50458, 814, 500, 380, 362, 264, 912, 1230, 295, 16871, 13, 50518, 50518, 814, 500, 380, 362, 264, 912, 11, 50562, 50562, 411, 577, 360, 291, 2906, 257, 15582, 30, 50698, 50698, 440, 1151, 636, 281, 2906, 257, 15582, 307, 382, 257, 4295, 13, 50820, 50820, 467, 311, 1936, 257, 4295, 6104, 3877, 2962, 50932, 50932, 365, 264, 15582, 13, 51000, 51000, 400, 341, 4295, 307, 25339, 770, 538, 264, 6575, 51250, 51250, 295, 264, 16871, 412, 1184, 3621, 11, 51354, 51354, 1310, 538, 641, 4914, 294, 805, 35, 1901, 51466, 51466, 420, 641, 4972, 4914, 11, 51528, 51528, 1310, 538, 264, 5802, 295, 264, 14713, 51652, 51652, 1296, 732, 48043, 16871, 51756, 51830], "temperature": 0.0, "avg_logprob": -0.08774785614013672, "compression_ratio": 1.860082304526749, "no_speech_prob": 1.1840696970466524e-05}, {"id": 1228, "seek": 616134, "start": 6168.02, "end": 6170.46, "text": " The best way to represent a molecule is as a graph.", "tokens": [50364, 814, 500, 380, 362, 264, 912, 4641, 13, 50458, 50458, 814, 500, 380, 362, 264, 912, 1230, 295, 16871, 13, 50518, 50518, 814, 500, 380, 362, 264, 912, 11, 50562, 50562, 411, 577, 360, 291, 2906, 257, 15582, 30, 50698, 50698, 440, 1151, 636, 281, 2906, 257, 15582, 307, 382, 257, 4295, 13, 50820, 50820, 467, 311, 1936, 257, 4295, 6104, 3877, 2962, 50932, 50932, 365, 264, 15582, 13, 51000, 51000, 400, 341, 4295, 307, 25339, 770, 538, 264, 6575, 51250, 51250, 295, 264, 16871, 412, 1184, 3621, 11, 51354, 51354, 1310, 538, 641, 4914, 294, 805, 35, 1901, 51466, 51466, 420, 641, 4972, 4914, 11, 51528, 51528, 1310, 538, 264, 5802, 295, 264, 14713, 51652, 51652, 1296, 732, 48043, 16871, 51756, 51830], "temperature": 0.0, "avg_logprob": -0.08774785614013672, "compression_ratio": 1.860082304526749, "no_speech_prob": 1.1840696970466524e-05}, {"id": 1229, "seek": 616134, "start": 6170.46, "end": 6172.7, "text": " It's basically a graph whose structure changes", "tokens": [50364, 814, 500, 380, 362, 264, 912, 4641, 13, 50458, 50458, 814, 500, 380, 362, 264, 912, 1230, 295, 16871, 13, 50518, 50518, 814, 500, 380, 362, 264, 912, 11, 50562, 50562, 411, 577, 360, 291, 2906, 257, 15582, 30, 50698, 50698, 440, 1151, 636, 281, 2906, 257, 15582, 307, 382, 257, 4295, 13, 50820, 50820, 467, 311, 1936, 257, 4295, 6104, 3877, 2962, 50932, 50932, 365, 264, 15582, 13, 51000, 51000, 400, 341, 4295, 307, 25339, 770, 538, 264, 6575, 51250, 51250, 295, 264, 16871, 412, 1184, 3621, 11, 51354, 51354, 1310, 538, 641, 4914, 294, 805, 35, 1901, 51466, 51466, 420, 641, 4972, 4914, 11, 51528, 51528, 1310, 538, 264, 5802, 295, 264, 14713, 51652, 51652, 1296, 732, 48043, 16871, 51756, 51830], "temperature": 0.0, "avg_logprob": -0.08774785614013672, "compression_ratio": 1.860082304526749, "no_speech_prob": 1.1840696970466524e-05}, {"id": 1230, "seek": 616134, "start": 6172.7, "end": 6174.06, "text": " with the molecule.", "tokens": [50364, 814, 500, 380, 362, 264, 912, 4641, 13, 50458, 50458, 814, 500, 380, 362, 264, 912, 1230, 295, 16871, 13, 50518, 50518, 814, 500, 380, 362, 264, 912, 11, 50562, 50562, 411, 577, 360, 291, 2906, 257, 15582, 30, 50698, 50698, 440, 1151, 636, 281, 2906, 257, 15582, 307, 382, 257, 4295, 13, 50820, 50820, 467, 311, 1936, 257, 4295, 6104, 3877, 2962, 50932, 50932, 365, 264, 15582, 13, 51000, 51000, 400, 341, 4295, 307, 25339, 770, 538, 264, 6575, 51250, 51250, 295, 264, 16871, 412, 1184, 3621, 11, 51354, 51354, 1310, 538, 641, 4914, 294, 805, 35, 1901, 51466, 51466, 420, 641, 4972, 4914, 11, 51528, 51528, 1310, 538, 264, 5802, 295, 264, 14713, 51652, 51652, 1296, 732, 48043, 16871, 51756, 51830], "temperature": 0.0, "avg_logprob": -0.08774785614013672, "compression_ratio": 1.860082304526749, "no_speech_prob": 1.1840696970466524e-05}, {"id": 1231, "seek": 616134, "start": 6174.06, "end": 6179.06, "text": " And this graph is annotated by the identity", "tokens": [50364, 814, 500, 380, 362, 264, 912, 4641, 13, 50458, 50458, 814, 500, 380, 362, 264, 912, 1230, 295, 16871, 13, 50518, 50518, 814, 500, 380, 362, 264, 912, 11, 50562, 50562, 411, 577, 360, 291, 2906, 257, 15582, 30, 50698, 50698, 440, 1151, 636, 281, 2906, 257, 15582, 307, 382, 257, 4295, 13, 50820, 50820, 467, 311, 1936, 257, 4295, 6104, 3877, 2962, 50932, 50932, 365, 264, 15582, 13, 51000, 51000, 400, 341, 4295, 307, 25339, 770, 538, 264, 6575, 51250, 51250, 295, 264, 16871, 412, 1184, 3621, 11, 51354, 51354, 1310, 538, 641, 4914, 294, 805, 35, 1901, 51466, 51466, 420, 641, 4972, 4914, 11, 51528, 51528, 1310, 538, 264, 5802, 295, 264, 14713, 51652, 51652, 1296, 732, 48043, 16871, 51756, 51830], "temperature": 0.0, "avg_logprob": -0.08774785614013672, "compression_ratio": 1.860082304526749, "no_speech_prob": 1.1840696970466524e-05}, {"id": 1232, "seek": 616134, "start": 6179.06, "end": 6181.14, "text": " of the atoms at each site,", "tokens": [50364, 814, 500, 380, 362, 264, 912, 4641, 13, 50458, 50458, 814, 500, 380, 362, 264, 912, 1230, 295, 16871, 13, 50518, 50518, 814, 500, 380, 362, 264, 912, 11, 50562, 50562, 411, 577, 360, 291, 2906, 257, 15582, 30, 50698, 50698, 440, 1151, 636, 281, 2906, 257, 15582, 307, 382, 257, 4295, 13, 50820, 50820, 467, 311, 1936, 257, 4295, 6104, 3877, 2962, 50932, 50932, 365, 264, 15582, 13, 51000, 51000, 400, 341, 4295, 307, 25339, 770, 538, 264, 6575, 51250, 51250, 295, 264, 16871, 412, 1184, 3621, 11, 51354, 51354, 1310, 538, 641, 4914, 294, 805, 35, 1901, 51466, 51466, 420, 641, 4972, 4914, 11, 51528, 51528, 1310, 538, 264, 5802, 295, 264, 14713, 51652, 51652, 1296, 732, 48043, 16871, 51756, 51830], "temperature": 0.0, "avg_logprob": -0.08774785614013672, "compression_ratio": 1.860082304526749, "no_speech_prob": 1.1840696970466524e-05}, {"id": 1233, "seek": 616134, "start": 6181.14, "end": 6183.38, "text": " maybe by their location in 3D space", "tokens": [50364, 814, 500, 380, 362, 264, 912, 4641, 13, 50458, 50458, 814, 500, 380, 362, 264, 912, 1230, 295, 16871, 13, 50518, 50518, 814, 500, 380, 362, 264, 912, 11, 50562, 50562, 411, 577, 360, 291, 2906, 257, 15582, 30, 50698, 50698, 440, 1151, 636, 281, 2906, 257, 15582, 307, 382, 257, 4295, 13, 50820, 50820, 467, 311, 1936, 257, 4295, 6104, 3877, 2962, 50932, 50932, 365, 264, 15582, 13, 51000, 51000, 400, 341, 4295, 307, 25339, 770, 538, 264, 6575, 51250, 51250, 295, 264, 16871, 412, 1184, 3621, 11, 51354, 51354, 1310, 538, 641, 4914, 294, 805, 35, 1901, 51466, 51466, 420, 641, 4972, 4914, 11, 51528, 51528, 1310, 538, 264, 5802, 295, 264, 14713, 51652, 51652, 1296, 732, 48043, 16871, 51756, 51830], "temperature": 0.0, "avg_logprob": -0.08774785614013672, "compression_ratio": 1.860082304526749, "no_speech_prob": 1.1840696970466524e-05}, {"id": 1234, "seek": 616134, "start": 6183.38, "end": 6184.62, "text": " or their relative location,", "tokens": [50364, 814, 500, 380, 362, 264, 912, 4641, 13, 50458, 50458, 814, 500, 380, 362, 264, 912, 1230, 295, 16871, 13, 50518, 50518, 814, 500, 380, 362, 264, 912, 11, 50562, 50562, 411, 577, 360, 291, 2906, 257, 15582, 30, 50698, 50698, 440, 1151, 636, 281, 2906, 257, 15582, 307, 382, 257, 4295, 13, 50820, 50820, 467, 311, 1936, 257, 4295, 6104, 3877, 2962, 50932, 50932, 365, 264, 15582, 13, 51000, 51000, 400, 341, 4295, 307, 25339, 770, 538, 264, 6575, 51250, 51250, 295, 264, 16871, 412, 1184, 3621, 11, 51354, 51354, 1310, 538, 641, 4914, 294, 805, 35, 1901, 51466, 51466, 420, 641, 4972, 4914, 11, 51528, 51528, 1310, 538, 264, 5802, 295, 264, 14713, 51652, 51652, 1296, 732, 48043, 16871, 51756, 51830], "temperature": 0.0, "avg_logprob": -0.08774785614013672, "compression_ratio": 1.860082304526749, "no_speech_prob": 1.1840696970466524e-05}, {"id": 1235, "seek": 616134, "start": 6184.62, "end": 6187.1, "text": " maybe by the angle of the bonds", "tokens": [50364, 814, 500, 380, 362, 264, 912, 4641, 13, 50458, 50458, 814, 500, 380, 362, 264, 912, 1230, 295, 16871, 13, 50518, 50518, 814, 500, 380, 362, 264, 912, 11, 50562, 50562, 411, 577, 360, 291, 2906, 257, 15582, 30, 50698, 50698, 440, 1151, 636, 281, 2906, 257, 15582, 307, 382, 257, 4295, 13, 50820, 50820, 467, 311, 1936, 257, 4295, 6104, 3877, 2962, 50932, 50932, 365, 264, 15582, 13, 51000, 51000, 400, 341, 4295, 307, 25339, 770, 538, 264, 6575, 51250, 51250, 295, 264, 16871, 412, 1184, 3621, 11, 51354, 51354, 1310, 538, 641, 4914, 294, 805, 35, 1901, 51466, 51466, 420, 641, 4972, 4914, 11, 51528, 51528, 1310, 538, 264, 5802, 295, 264, 14713, 51652, 51652, 1296, 732, 48043, 16871, 51756, 51830], "temperature": 0.0, "avg_logprob": -0.08774785614013672, "compression_ratio": 1.860082304526749, "no_speech_prob": 1.1840696970466524e-05}, {"id": 1236, "seek": 616134, "start": 6187.1, "end": 6189.18, "text": " between two successive atoms", "tokens": [50364, 814, 500, 380, 362, 264, 912, 4641, 13, 50458, 50458, 814, 500, 380, 362, 264, 912, 1230, 295, 16871, 13, 50518, 50518, 814, 500, 380, 362, 264, 912, 11, 50562, 50562, 411, 577, 360, 291, 2906, 257, 15582, 30, 50698, 50698, 440, 1151, 636, 281, 2906, 257, 15582, 307, 382, 257, 4295, 13, 50820, 50820, 467, 311, 1936, 257, 4295, 6104, 3877, 2962, 50932, 50932, 365, 264, 15582, 13, 51000, 51000, 400, 341, 4295, 307, 25339, 770, 538, 264, 6575, 51250, 51250, 295, 264, 16871, 412, 1184, 3621, 11, 51354, 51354, 1310, 538, 641, 4914, 294, 805, 35, 1901, 51466, 51466, 420, 641, 4972, 4914, 11, 51528, 51528, 1310, 538, 264, 5802, 295, 264, 14713, 51652, 51652, 1296, 732, 48043, 16871, 51756, 51830], "temperature": 0.0, "avg_logprob": -0.08774785614013672, "compression_ratio": 1.860082304526749, "no_speech_prob": 1.1840696970466524e-05}, {"id": 1237, "seek": 618918, "start": 6189.18, "end": 6192.38, "text": " or the binding energy of that particular bond", "tokens": [50364, 420, 264, 17359, 2281, 295, 300, 1729, 6086, 50524, 50524, 293, 721, 411, 341, 13, 50566, 50566, 407, 264, 1151, 636, 281, 2906, 257, 15582, 50750, 50750, 307, 538, 13460, 382, 257, 4295, 11, 1936, 13, 50900, 50900, 400, 510, 311, 1071, 1365, 11, 51018, 51018, 4317, 544, 7340, 281, 746, 411, 4384, 13, 51140, 51184, 961, 311, 584, 286, 528, 281, 733, 295, 13596, 11, 51434, 51504, 420, 718, 311, 584, 6795, 420, 746, 13, 51614, 51614, 286, 528, 281, 13596, 437, 2010, 295, 11, 51752, 51816], "temperature": 0.0, "avg_logprob": -0.19218775431315105, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.4728202586411498e-05}, {"id": 1238, "seek": 618918, "start": 6192.38, "end": 6193.22, "text": " and things like this.", "tokens": [50364, 420, 264, 17359, 2281, 295, 300, 1729, 6086, 50524, 50524, 293, 721, 411, 341, 13, 50566, 50566, 407, 264, 1151, 636, 281, 2906, 257, 15582, 50750, 50750, 307, 538, 13460, 382, 257, 4295, 11, 1936, 13, 50900, 50900, 400, 510, 311, 1071, 1365, 11, 51018, 51018, 4317, 544, 7340, 281, 746, 411, 4384, 13, 51140, 51184, 961, 311, 584, 286, 528, 281, 733, 295, 13596, 11, 51434, 51504, 420, 718, 311, 584, 6795, 420, 746, 13, 51614, 51614, 286, 528, 281, 13596, 437, 2010, 295, 11, 51752, 51816], "temperature": 0.0, "avg_logprob": -0.19218775431315105, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.4728202586411498e-05}, {"id": 1239, "seek": 618918, "start": 6193.22, "end": 6196.900000000001, "text": " So the best way to represent a molecule", "tokens": [50364, 420, 264, 17359, 2281, 295, 300, 1729, 6086, 50524, 50524, 293, 721, 411, 341, 13, 50566, 50566, 407, 264, 1151, 636, 281, 2906, 257, 15582, 50750, 50750, 307, 538, 13460, 382, 257, 4295, 11, 1936, 13, 50900, 50900, 400, 510, 311, 1071, 1365, 11, 51018, 51018, 4317, 544, 7340, 281, 746, 411, 4384, 13, 51140, 51184, 961, 311, 584, 286, 528, 281, 733, 295, 13596, 11, 51434, 51504, 420, 718, 311, 584, 6795, 420, 746, 13, 51614, 51614, 286, 528, 281, 13596, 437, 2010, 295, 11, 51752, 51816], "temperature": 0.0, "avg_logprob": -0.19218775431315105, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.4728202586411498e-05}, {"id": 1240, "seek": 618918, "start": 6196.900000000001, "end": 6199.900000000001, "text": " is by representing as a graph, basically.", "tokens": [50364, 420, 264, 17359, 2281, 295, 300, 1729, 6086, 50524, 50524, 293, 721, 411, 341, 13, 50566, 50566, 407, 264, 1151, 636, 281, 2906, 257, 15582, 50750, 50750, 307, 538, 13460, 382, 257, 4295, 11, 1936, 13, 50900, 50900, 400, 510, 311, 1071, 1365, 11, 51018, 51018, 4317, 544, 7340, 281, 746, 411, 4384, 13, 51140, 51184, 961, 311, 584, 286, 528, 281, 733, 295, 13596, 11, 51434, 51504, 420, 718, 311, 584, 6795, 420, 746, 13, 51614, 51614, 286, 528, 281, 13596, 437, 2010, 295, 11, 51752, 51816], "temperature": 0.0, "avg_logprob": -0.19218775431315105, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.4728202586411498e-05}, {"id": 1241, "seek": 618918, "start": 6199.900000000001, "end": 6202.26, "text": " And here's another example,", "tokens": [50364, 420, 264, 17359, 2281, 295, 300, 1729, 6086, 50524, 50524, 293, 721, 411, 341, 13, 50566, 50566, 407, 264, 1151, 636, 281, 2906, 257, 15582, 50750, 50750, 307, 538, 13460, 382, 257, 4295, 11, 1936, 13, 50900, 50900, 400, 510, 311, 1071, 1365, 11, 51018, 51018, 4317, 544, 7340, 281, 746, 411, 4384, 13, 51140, 51184, 961, 311, 584, 286, 528, 281, 733, 295, 13596, 11, 51434, 51504, 420, 718, 311, 584, 6795, 420, 746, 13, 51614, 51614, 286, 528, 281, 13596, 437, 2010, 295, 11, 51752, 51816], "temperature": 0.0, "avg_logprob": -0.19218775431315105, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.4728202586411498e-05}, {"id": 1242, "seek": 618918, "start": 6202.26, "end": 6204.700000000001, "text": " perhaps more relevant to something like Facebook.", "tokens": [50364, 420, 264, 17359, 2281, 295, 300, 1729, 6086, 50524, 50524, 293, 721, 411, 341, 13, 50566, 50566, 407, 264, 1151, 636, 281, 2906, 257, 15582, 50750, 50750, 307, 538, 13460, 382, 257, 4295, 11, 1936, 13, 50900, 50900, 400, 510, 311, 1071, 1365, 11, 51018, 51018, 4317, 544, 7340, 281, 746, 411, 4384, 13, 51140, 51184, 961, 311, 584, 286, 528, 281, 733, 295, 13596, 11, 51434, 51504, 420, 718, 311, 584, 6795, 420, 746, 13, 51614, 51614, 286, 528, 281, 13596, 437, 2010, 295, 11, 51752, 51816], "temperature": 0.0, "avg_logprob": -0.19218775431315105, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.4728202586411498e-05}, {"id": 1243, "seek": 618918, "start": 6205.58, "end": 6210.58, "text": " Let's say I want to kind of infer,", "tokens": [50364, 420, 264, 17359, 2281, 295, 300, 1729, 6086, 50524, 50524, 293, 721, 411, 341, 13, 50566, 50566, 407, 264, 1151, 636, 281, 2906, 257, 15582, 50750, 50750, 307, 538, 13460, 382, 257, 4295, 11, 1936, 13, 50900, 50900, 400, 510, 311, 1071, 1365, 11, 51018, 51018, 4317, 544, 7340, 281, 746, 411, 4384, 13, 51140, 51184, 961, 311, 584, 286, 528, 281, 733, 295, 13596, 11, 51434, 51504, 420, 718, 311, 584, 6795, 420, 746, 13, 51614, 51614, 286, 528, 281, 13596, 437, 2010, 295, 11, 51752, 51816], "temperature": 0.0, "avg_logprob": -0.19218775431315105, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.4728202586411498e-05}, {"id": 1244, "seek": 618918, "start": 6211.9800000000005, "end": 6214.18, "text": " or let's say Amazon or something.", "tokens": [50364, 420, 264, 17359, 2281, 295, 300, 1729, 6086, 50524, 50524, 293, 721, 411, 341, 13, 50566, 50566, 407, 264, 1151, 636, 281, 2906, 257, 15582, 50750, 50750, 307, 538, 13460, 382, 257, 4295, 11, 1936, 13, 50900, 50900, 400, 510, 311, 1071, 1365, 11, 51018, 51018, 4317, 544, 7340, 281, 746, 411, 4384, 13, 51140, 51184, 961, 311, 584, 286, 528, 281, 733, 295, 13596, 11, 51434, 51504, 420, 718, 311, 584, 6795, 420, 746, 13, 51614, 51614, 286, 528, 281, 13596, 437, 2010, 295, 11, 51752, 51816], "temperature": 0.0, "avg_logprob": -0.19218775431315105, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.4728202586411498e-05}, {"id": 1245, "seek": 618918, "start": 6214.18, "end": 6216.9400000000005, "text": " I want to infer what type of,", "tokens": [50364, 420, 264, 17359, 2281, 295, 300, 1729, 6086, 50524, 50524, 293, 721, 411, 341, 13, 50566, 50566, 407, 264, 1151, 636, 281, 2906, 257, 15582, 50750, 50750, 307, 538, 13460, 382, 257, 4295, 11, 1936, 13, 50900, 50900, 400, 510, 311, 1071, 1365, 11, 51018, 51018, 4317, 544, 7340, 281, 746, 411, 4384, 13, 51140, 51184, 961, 311, 584, 286, 528, 281, 733, 295, 13596, 11, 51434, 51504, 420, 718, 311, 584, 6795, 420, 746, 13, 51614, 51614, 286, 528, 281, 13596, 437, 2010, 295, 11, 51752, 51816], "temperature": 0.0, "avg_logprob": -0.19218775431315105, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.4728202586411498e-05}, {"id": 1246, "seek": 621694, "start": 6216.94, "end": 6219.74, "text": " let's say I'm Amazon, right?", "tokens": [50364, 718, 311, 584, 286, 478, 6795, 11, 558, 30, 50504, 50504, 400, 286, 362, 257, 5474, 50566, 50566, 293, 300, 5474, 575, 4243, 257, 1379, 3840, 50648, 50648, 295, 819, 721, 13, 50722, 50722, 400, 300, 5474, 575, 26940, 50818, 50818, 257, 1379, 3840, 295, 819, 721, 13, 50884, 50884, 286, 727, 519, 295, 733, 295, 43430, 341, 382, 257, 8062, 11, 50996, 50996, 457, 309, 576, 312, 257, 8062, 295, 7006, 2744, 51224, 51224, 570, 561, 2256, 819, 3547, 295, 721, 51376, 51376, 293, 1507, 411, 300, 13, 51418, 51418, 407, 286, 576, 643, 281, 1333, 295, 915, 257, 636, 281, 26118, 264, 1412, 51542, 51542, 370, 300, 2201, 393, 312, 10379, 51632, 51632, 538, 264, 912, 6806, 2744, 8062, 13, 51708, 51708, 583, 437, 498, 2602, 286, 2906, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11648141804026134, "compression_ratio": 1.8224637681159421, "no_speech_prob": 3.0413818876695586e-06}, {"id": 1247, "seek": 621694, "start": 6219.74, "end": 6220.98, "text": " And I have a customer", "tokens": [50364, 718, 311, 584, 286, 478, 6795, 11, 558, 30, 50504, 50504, 400, 286, 362, 257, 5474, 50566, 50566, 293, 300, 5474, 575, 4243, 257, 1379, 3840, 50648, 50648, 295, 819, 721, 13, 50722, 50722, 400, 300, 5474, 575, 26940, 50818, 50818, 257, 1379, 3840, 295, 819, 721, 13, 50884, 50884, 286, 727, 519, 295, 733, 295, 43430, 341, 382, 257, 8062, 11, 50996, 50996, 457, 309, 576, 312, 257, 8062, 295, 7006, 2744, 51224, 51224, 570, 561, 2256, 819, 3547, 295, 721, 51376, 51376, 293, 1507, 411, 300, 13, 51418, 51418, 407, 286, 576, 643, 281, 1333, 295, 915, 257, 636, 281, 26118, 264, 1412, 51542, 51542, 370, 300, 2201, 393, 312, 10379, 51632, 51632, 538, 264, 912, 6806, 2744, 8062, 13, 51708, 51708, 583, 437, 498, 2602, 286, 2906, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11648141804026134, "compression_ratio": 1.8224637681159421, "no_speech_prob": 3.0413818876695586e-06}, {"id": 1248, "seek": 621694, "start": 6220.98, "end": 6222.62, "text": " and that customer has bought a whole bunch", "tokens": [50364, 718, 311, 584, 286, 478, 6795, 11, 558, 30, 50504, 50504, 400, 286, 362, 257, 5474, 50566, 50566, 293, 300, 5474, 575, 4243, 257, 1379, 3840, 50648, 50648, 295, 819, 721, 13, 50722, 50722, 400, 300, 5474, 575, 26940, 50818, 50818, 257, 1379, 3840, 295, 819, 721, 13, 50884, 50884, 286, 727, 519, 295, 733, 295, 43430, 341, 382, 257, 8062, 11, 50996, 50996, 457, 309, 576, 312, 257, 8062, 295, 7006, 2744, 51224, 51224, 570, 561, 2256, 819, 3547, 295, 721, 51376, 51376, 293, 1507, 411, 300, 13, 51418, 51418, 407, 286, 576, 643, 281, 1333, 295, 915, 257, 636, 281, 26118, 264, 1412, 51542, 51542, 370, 300, 2201, 393, 312, 10379, 51632, 51632, 538, 264, 912, 6806, 2744, 8062, 13, 51708, 51708, 583, 437, 498, 2602, 286, 2906, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11648141804026134, "compression_ratio": 1.8224637681159421, "no_speech_prob": 3.0413818876695586e-06}, {"id": 1249, "seek": 621694, "start": 6222.62, "end": 6224.099999999999, "text": " of different things.", "tokens": [50364, 718, 311, 584, 286, 478, 6795, 11, 558, 30, 50504, 50504, 400, 286, 362, 257, 5474, 50566, 50566, 293, 300, 5474, 575, 4243, 257, 1379, 3840, 50648, 50648, 295, 819, 721, 13, 50722, 50722, 400, 300, 5474, 575, 26940, 50818, 50818, 257, 1379, 3840, 295, 819, 721, 13, 50884, 50884, 286, 727, 519, 295, 733, 295, 43430, 341, 382, 257, 8062, 11, 50996, 50996, 457, 309, 576, 312, 257, 8062, 295, 7006, 2744, 51224, 51224, 570, 561, 2256, 819, 3547, 295, 721, 51376, 51376, 293, 1507, 411, 300, 13, 51418, 51418, 407, 286, 576, 643, 281, 1333, 295, 915, 257, 636, 281, 26118, 264, 1412, 51542, 51542, 370, 300, 2201, 393, 312, 10379, 51632, 51632, 538, 264, 912, 6806, 2744, 8062, 13, 51708, 51708, 583, 437, 498, 2602, 286, 2906, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11648141804026134, "compression_ratio": 1.8224637681159421, "no_speech_prob": 3.0413818876695586e-06}, {"id": 1250, "seek": 621694, "start": 6224.099999999999, "end": 6226.0199999999995, "text": " And that customer has commented", "tokens": [50364, 718, 311, 584, 286, 478, 6795, 11, 558, 30, 50504, 50504, 400, 286, 362, 257, 5474, 50566, 50566, 293, 300, 5474, 575, 4243, 257, 1379, 3840, 50648, 50648, 295, 819, 721, 13, 50722, 50722, 400, 300, 5474, 575, 26940, 50818, 50818, 257, 1379, 3840, 295, 819, 721, 13, 50884, 50884, 286, 727, 519, 295, 733, 295, 43430, 341, 382, 257, 8062, 11, 50996, 50996, 457, 309, 576, 312, 257, 8062, 295, 7006, 2744, 51224, 51224, 570, 561, 2256, 819, 3547, 295, 721, 51376, 51376, 293, 1507, 411, 300, 13, 51418, 51418, 407, 286, 576, 643, 281, 1333, 295, 915, 257, 636, 281, 26118, 264, 1412, 51542, 51542, 370, 300, 2201, 393, 312, 10379, 51632, 51632, 538, 264, 912, 6806, 2744, 8062, 13, 51708, 51708, 583, 437, 498, 2602, 286, 2906, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11648141804026134, "compression_ratio": 1.8224637681159421, "no_speech_prob": 3.0413818876695586e-06}, {"id": 1251, "seek": 621694, "start": 6226.0199999999995, "end": 6227.339999999999, "text": " a whole bunch of different things.", "tokens": [50364, 718, 311, 584, 286, 478, 6795, 11, 558, 30, 50504, 50504, 400, 286, 362, 257, 5474, 50566, 50566, 293, 300, 5474, 575, 4243, 257, 1379, 3840, 50648, 50648, 295, 819, 721, 13, 50722, 50722, 400, 300, 5474, 575, 26940, 50818, 50818, 257, 1379, 3840, 295, 819, 721, 13, 50884, 50884, 286, 727, 519, 295, 733, 295, 43430, 341, 382, 257, 8062, 11, 50996, 50996, 457, 309, 576, 312, 257, 8062, 295, 7006, 2744, 51224, 51224, 570, 561, 2256, 819, 3547, 295, 721, 51376, 51376, 293, 1507, 411, 300, 13, 51418, 51418, 407, 286, 576, 643, 281, 1333, 295, 915, 257, 636, 281, 26118, 264, 1412, 51542, 51542, 370, 300, 2201, 393, 312, 10379, 51632, 51632, 538, 264, 912, 6806, 2744, 8062, 13, 51708, 51708, 583, 437, 498, 2602, 286, 2906, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11648141804026134, "compression_ratio": 1.8224637681159421, "no_speech_prob": 3.0413818876695586e-06}, {"id": 1252, "seek": 621694, "start": 6227.339999999999, "end": 6229.58, "text": " I could think of kind of encoding this as a vector,", "tokens": [50364, 718, 311, 584, 286, 478, 6795, 11, 558, 30, 50504, 50504, 400, 286, 362, 257, 5474, 50566, 50566, 293, 300, 5474, 575, 4243, 257, 1379, 3840, 50648, 50648, 295, 819, 721, 13, 50722, 50722, 400, 300, 5474, 575, 26940, 50818, 50818, 257, 1379, 3840, 295, 819, 721, 13, 50884, 50884, 286, 727, 519, 295, 733, 295, 43430, 341, 382, 257, 8062, 11, 50996, 50996, 457, 309, 576, 312, 257, 8062, 295, 7006, 2744, 51224, 51224, 570, 561, 2256, 819, 3547, 295, 721, 51376, 51376, 293, 1507, 411, 300, 13, 51418, 51418, 407, 286, 576, 643, 281, 1333, 295, 915, 257, 636, 281, 26118, 264, 1412, 51542, 51542, 370, 300, 2201, 393, 312, 10379, 51632, 51632, 538, 264, 912, 6806, 2744, 8062, 13, 51708, 51708, 583, 437, 498, 2602, 286, 2906, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11648141804026134, "compression_ratio": 1.8224637681159421, "no_speech_prob": 3.0413818876695586e-06}, {"id": 1253, "seek": 621694, "start": 6229.58, "end": 6234.139999999999, "text": " but it would be a vector of variable size", "tokens": [50364, 718, 311, 584, 286, 478, 6795, 11, 558, 30, 50504, 50504, 400, 286, 362, 257, 5474, 50566, 50566, 293, 300, 5474, 575, 4243, 257, 1379, 3840, 50648, 50648, 295, 819, 721, 13, 50722, 50722, 400, 300, 5474, 575, 26940, 50818, 50818, 257, 1379, 3840, 295, 819, 721, 13, 50884, 50884, 286, 727, 519, 295, 733, 295, 43430, 341, 382, 257, 8062, 11, 50996, 50996, 457, 309, 576, 312, 257, 8062, 295, 7006, 2744, 51224, 51224, 570, 561, 2256, 819, 3547, 295, 721, 51376, 51376, 293, 1507, 411, 300, 13, 51418, 51418, 407, 286, 576, 643, 281, 1333, 295, 915, 257, 636, 281, 26118, 264, 1412, 51542, 51542, 370, 300, 2201, 393, 312, 10379, 51632, 51632, 538, 264, 912, 6806, 2744, 8062, 13, 51708, 51708, 583, 437, 498, 2602, 286, 2906, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11648141804026134, "compression_ratio": 1.8224637681159421, "no_speech_prob": 3.0413818876695586e-06}, {"id": 1254, "seek": 621694, "start": 6234.139999999999, "end": 6237.179999999999, "text": " because people buy different numbers of things", "tokens": [50364, 718, 311, 584, 286, 478, 6795, 11, 558, 30, 50504, 50504, 400, 286, 362, 257, 5474, 50566, 50566, 293, 300, 5474, 575, 4243, 257, 1379, 3840, 50648, 50648, 295, 819, 721, 13, 50722, 50722, 400, 300, 5474, 575, 26940, 50818, 50818, 257, 1379, 3840, 295, 819, 721, 13, 50884, 50884, 286, 727, 519, 295, 733, 295, 43430, 341, 382, 257, 8062, 11, 50996, 50996, 457, 309, 576, 312, 257, 8062, 295, 7006, 2744, 51224, 51224, 570, 561, 2256, 819, 3547, 295, 721, 51376, 51376, 293, 1507, 411, 300, 13, 51418, 51418, 407, 286, 576, 643, 281, 1333, 295, 915, 257, 636, 281, 26118, 264, 1412, 51542, 51542, 370, 300, 2201, 393, 312, 10379, 51632, 51632, 538, 264, 912, 6806, 2744, 8062, 13, 51708, 51708, 583, 437, 498, 2602, 286, 2906, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11648141804026134, "compression_ratio": 1.8224637681159421, "no_speech_prob": 3.0413818876695586e-06}, {"id": 1255, "seek": 621694, "start": 6237.179999999999, "end": 6238.0199999999995, "text": " and stuff like that.", "tokens": [50364, 718, 311, 584, 286, 478, 6795, 11, 558, 30, 50504, 50504, 400, 286, 362, 257, 5474, 50566, 50566, 293, 300, 5474, 575, 4243, 257, 1379, 3840, 50648, 50648, 295, 819, 721, 13, 50722, 50722, 400, 300, 5474, 575, 26940, 50818, 50818, 257, 1379, 3840, 295, 819, 721, 13, 50884, 50884, 286, 727, 519, 295, 733, 295, 43430, 341, 382, 257, 8062, 11, 50996, 50996, 457, 309, 576, 312, 257, 8062, 295, 7006, 2744, 51224, 51224, 570, 561, 2256, 819, 3547, 295, 721, 51376, 51376, 293, 1507, 411, 300, 13, 51418, 51418, 407, 286, 576, 643, 281, 1333, 295, 915, 257, 636, 281, 26118, 264, 1412, 51542, 51542, 370, 300, 2201, 393, 312, 10379, 51632, 51632, 538, 264, 912, 6806, 2744, 8062, 13, 51708, 51708, 583, 437, 498, 2602, 286, 2906, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11648141804026134, "compression_ratio": 1.8224637681159421, "no_speech_prob": 3.0413818876695586e-06}, {"id": 1256, "seek": 621694, "start": 6238.0199999999995, "end": 6240.5, "text": " So I would need to sort of find a way to aggregate the data", "tokens": [50364, 718, 311, 584, 286, 478, 6795, 11, 558, 30, 50504, 50504, 400, 286, 362, 257, 5474, 50566, 50566, 293, 300, 5474, 575, 4243, 257, 1379, 3840, 50648, 50648, 295, 819, 721, 13, 50722, 50722, 400, 300, 5474, 575, 26940, 50818, 50818, 257, 1379, 3840, 295, 819, 721, 13, 50884, 50884, 286, 727, 519, 295, 733, 295, 43430, 341, 382, 257, 8062, 11, 50996, 50996, 457, 309, 576, 312, 257, 8062, 295, 7006, 2744, 51224, 51224, 570, 561, 2256, 819, 3547, 295, 721, 51376, 51376, 293, 1507, 411, 300, 13, 51418, 51418, 407, 286, 576, 643, 281, 1333, 295, 915, 257, 636, 281, 26118, 264, 1412, 51542, 51542, 370, 300, 2201, 393, 312, 10379, 51632, 51632, 538, 264, 912, 6806, 2744, 8062, 13, 51708, 51708, 583, 437, 498, 2602, 286, 2906, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11648141804026134, "compression_ratio": 1.8224637681159421, "no_speech_prob": 3.0413818876695586e-06}, {"id": 1257, "seek": 621694, "start": 6240.5, "end": 6242.299999999999, "text": " so that everybody can be represented", "tokens": [50364, 718, 311, 584, 286, 478, 6795, 11, 558, 30, 50504, 50504, 400, 286, 362, 257, 5474, 50566, 50566, 293, 300, 5474, 575, 4243, 257, 1379, 3840, 50648, 50648, 295, 819, 721, 13, 50722, 50722, 400, 300, 5474, 575, 26940, 50818, 50818, 257, 1379, 3840, 295, 819, 721, 13, 50884, 50884, 286, 727, 519, 295, 733, 295, 43430, 341, 382, 257, 8062, 11, 50996, 50996, 457, 309, 576, 312, 257, 8062, 295, 7006, 2744, 51224, 51224, 570, 561, 2256, 819, 3547, 295, 721, 51376, 51376, 293, 1507, 411, 300, 13, 51418, 51418, 407, 286, 576, 643, 281, 1333, 295, 915, 257, 636, 281, 26118, 264, 1412, 51542, 51542, 370, 300, 2201, 393, 312, 10379, 51632, 51632, 538, 264, 912, 6806, 2744, 8062, 13, 51708, 51708, 583, 437, 498, 2602, 286, 2906, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11648141804026134, "compression_ratio": 1.8224637681159421, "no_speech_prob": 3.0413818876695586e-06}, {"id": 1258, "seek": 621694, "start": 6242.299999999999, "end": 6243.82, "text": " by the same fixed size vector.", "tokens": [50364, 718, 311, 584, 286, 478, 6795, 11, 558, 30, 50504, 50504, 400, 286, 362, 257, 5474, 50566, 50566, 293, 300, 5474, 575, 4243, 257, 1379, 3840, 50648, 50648, 295, 819, 721, 13, 50722, 50722, 400, 300, 5474, 575, 26940, 50818, 50818, 257, 1379, 3840, 295, 819, 721, 13, 50884, 50884, 286, 727, 519, 295, 733, 295, 43430, 341, 382, 257, 8062, 11, 50996, 50996, 457, 309, 576, 312, 257, 8062, 295, 7006, 2744, 51224, 51224, 570, 561, 2256, 819, 3547, 295, 721, 51376, 51376, 293, 1507, 411, 300, 13, 51418, 51418, 407, 286, 576, 643, 281, 1333, 295, 915, 257, 636, 281, 26118, 264, 1412, 51542, 51542, 370, 300, 2201, 393, 312, 10379, 51632, 51632, 538, 264, 912, 6806, 2744, 8062, 13, 51708, 51708, 583, 437, 498, 2602, 286, 2906, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11648141804026134, "compression_ratio": 1.8224637681159421, "no_speech_prob": 3.0413818876695586e-06}, {"id": 1259, "seek": 621694, "start": 6243.82, "end": 6246.82, "text": " But what if instead I represent", "tokens": [50364, 718, 311, 584, 286, 478, 6795, 11, 558, 30, 50504, 50504, 400, 286, 362, 257, 5474, 50566, 50566, 293, 300, 5474, 575, 4243, 257, 1379, 3840, 50648, 50648, 295, 819, 721, 13, 50722, 50722, 400, 300, 5474, 575, 26940, 50818, 50818, 257, 1379, 3840, 295, 819, 721, 13, 50884, 50884, 286, 727, 519, 295, 733, 295, 43430, 341, 382, 257, 8062, 11, 50996, 50996, 457, 309, 576, 312, 257, 8062, 295, 7006, 2744, 51224, 51224, 570, 561, 2256, 819, 3547, 295, 721, 51376, 51376, 293, 1507, 411, 300, 13, 51418, 51418, 407, 286, 576, 643, 281, 1333, 295, 915, 257, 636, 281, 26118, 264, 1412, 51542, 51542, 370, 300, 2201, 393, 312, 10379, 51632, 51632, 538, 264, 912, 6806, 2744, 8062, 13, 51708, 51708, 583, 437, 498, 2602, 286, 2906, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11648141804026134, "compression_ratio": 1.8224637681159421, "no_speech_prob": 3.0413818876695586e-06}, {"id": 1260, "seek": 624682, "start": 6246.82, "end": 6249.9, "text": " the person and all the things that that person has bought", "tokens": [50364, 264, 954, 293, 439, 264, 721, 300, 300, 954, 575, 4243, 50518, 50518, 293, 439, 264, 10229, 300, 954, 575, 1027, 11, 1030, 11458, 11, 50712, 50712, 382, 257, 4295, 11, 4476, 13, 50804, 50804, 400, 550, 286, 2906, 11, 437, 286, 3154, 281, 264, 18161, 2533, 50978, 50978, 307, 264, 4295, 365, 4190, 322, 264, 13891, 293, 4317, 264, 10346, 82, 13, 51168, 51210, 759, 286, 362, 257, 636, 295, 13460, 257, 4295, 51298, 51298, 370, 300, 286, 393, 1745, 257, 18161, 2533, 51398, 51398, 21761, 295, 264, 3909, 295, 264, 4295, 11, 51490, 51490, 550, 286, 393, 360, 341, 733, 295, 3861, 13, 51614, 51614, 400, 370, 341, 307, 437, 4295, 18161, 36170, 366, 466, 13, 51718, 51718, 467, 311, 257, 588, 11, 588, 2368, 4829, 412, 264, 1623, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.08708614461562213, "compression_ratio": 1.8893129770992367, "no_speech_prob": 8.799893294053618e-06}, {"id": 1261, "seek": 624682, "start": 6249.9, "end": 6253.78, "text": " and all the reviews that person has made, et cetera,", "tokens": [50364, 264, 954, 293, 439, 264, 721, 300, 300, 954, 575, 4243, 50518, 50518, 293, 439, 264, 10229, 300, 954, 575, 1027, 11, 1030, 11458, 11, 50712, 50712, 382, 257, 4295, 11, 4476, 13, 50804, 50804, 400, 550, 286, 2906, 11, 437, 286, 3154, 281, 264, 18161, 2533, 50978, 50978, 307, 264, 4295, 365, 4190, 322, 264, 13891, 293, 4317, 264, 10346, 82, 13, 51168, 51210, 759, 286, 362, 257, 636, 295, 13460, 257, 4295, 51298, 51298, 370, 300, 286, 393, 1745, 257, 18161, 2533, 51398, 51398, 21761, 295, 264, 3909, 295, 264, 4295, 11, 51490, 51490, 550, 286, 393, 360, 341, 733, 295, 3861, 13, 51614, 51614, 400, 370, 341, 307, 437, 4295, 18161, 36170, 366, 466, 13, 51718, 51718, 467, 311, 257, 588, 11, 588, 2368, 4829, 412, 264, 1623, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.08708614461562213, "compression_ratio": 1.8893129770992367, "no_speech_prob": 8.799893294053618e-06}, {"id": 1262, "seek": 624682, "start": 6253.78, "end": 6255.62, "text": " as a graph, essentially.", "tokens": [50364, 264, 954, 293, 439, 264, 721, 300, 300, 954, 575, 4243, 50518, 50518, 293, 439, 264, 10229, 300, 954, 575, 1027, 11, 1030, 11458, 11, 50712, 50712, 382, 257, 4295, 11, 4476, 13, 50804, 50804, 400, 550, 286, 2906, 11, 437, 286, 3154, 281, 264, 18161, 2533, 50978, 50978, 307, 264, 4295, 365, 4190, 322, 264, 13891, 293, 4317, 264, 10346, 82, 13, 51168, 51210, 759, 286, 362, 257, 636, 295, 13460, 257, 4295, 51298, 51298, 370, 300, 286, 393, 1745, 257, 18161, 2533, 51398, 51398, 21761, 295, 264, 3909, 295, 264, 4295, 11, 51490, 51490, 550, 286, 393, 360, 341, 733, 295, 3861, 13, 51614, 51614, 400, 370, 341, 307, 437, 4295, 18161, 36170, 366, 466, 13, 51718, 51718, 467, 311, 257, 588, 11, 588, 2368, 4829, 412, 264, 1623, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.08708614461562213, "compression_ratio": 1.8893129770992367, "no_speech_prob": 8.799893294053618e-06}, {"id": 1263, "seek": 624682, "start": 6255.62, "end": 6259.099999999999, "text": " And then I represent, what I feed to the neural net", "tokens": [50364, 264, 954, 293, 439, 264, 721, 300, 300, 954, 575, 4243, 50518, 50518, 293, 439, 264, 10229, 300, 954, 575, 1027, 11, 1030, 11458, 11, 50712, 50712, 382, 257, 4295, 11, 4476, 13, 50804, 50804, 400, 550, 286, 2906, 11, 437, 286, 3154, 281, 264, 18161, 2533, 50978, 50978, 307, 264, 4295, 365, 4190, 322, 264, 13891, 293, 4317, 264, 10346, 82, 13, 51168, 51210, 759, 286, 362, 257, 636, 295, 13460, 257, 4295, 51298, 51298, 370, 300, 286, 393, 1745, 257, 18161, 2533, 51398, 51398, 21761, 295, 264, 3909, 295, 264, 4295, 11, 51490, 51490, 550, 286, 393, 360, 341, 733, 295, 3861, 13, 51614, 51614, 400, 370, 341, 307, 437, 4295, 18161, 36170, 366, 466, 13, 51718, 51718, 467, 311, 257, 588, 11, 588, 2368, 4829, 412, 264, 1623, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.08708614461562213, "compression_ratio": 1.8893129770992367, "no_speech_prob": 8.799893294053618e-06}, {"id": 1264, "seek": 624682, "start": 6259.099999999999, "end": 6262.9, "text": " is the graph with values on the nodes and perhaps the arcs.", "tokens": [50364, 264, 954, 293, 439, 264, 721, 300, 300, 954, 575, 4243, 50518, 50518, 293, 439, 264, 10229, 300, 954, 575, 1027, 11, 1030, 11458, 11, 50712, 50712, 382, 257, 4295, 11, 4476, 13, 50804, 50804, 400, 550, 286, 2906, 11, 437, 286, 3154, 281, 264, 18161, 2533, 50978, 50978, 307, 264, 4295, 365, 4190, 322, 264, 13891, 293, 4317, 264, 10346, 82, 13, 51168, 51210, 759, 286, 362, 257, 636, 295, 13460, 257, 4295, 51298, 51298, 370, 300, 286, 393, 1745, 257, 18161, 2533, 51398, 51398, 21761, 295, 264, 3909, 295, 264, 4295, 11, 51490, 51490, 550, 286, 393, 360, 341, 733, 295, 3861, 13, 51614, 51614, 400, 370, 341, 307, 437, 4295, 18161, 36170, 366, 466, 13, 51718, 51718, 467, 311, 257, 588, 11, 588, 2368, 4829, 412, 264, 1623, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.08708614461562213, "compression_ratio": 1.8893129770992367, "no_speech_prob": 8.799893294053618e-06}, {"id": 1265, "seek": 624682, "start": 6263.74, "end": 6265.5, "text": " If I have a way of representing a graph", "tokens": [50364, 264, 954, 293, 439, 264, 721, 300, 300, 954, 575, 4243, 50518, 50518, 293, 439, 264, 10229, 300, 954, 575, 1027, 11, 1030, 11458, 11, 50712, 50712, 382, 257, 4295, 11, 4476, 13, 50804, 50804, 400, 550, 286, 2906, 11, 437, 286, 3154, 281, 264, 18161, 2533, 50978, 50978, 307, 264, 4295, 365, 4190, 322, 264, 13891, 293, 4317, 264, 10346, 82, 13, 51168, 51210, 759, 286, 362, 257, 636, 295, 13460, 257, 4295, 51298, 51298, 370, 300, 286, 393, 1745, 257, 18161, 2533, 51398, 51398, 21761, 295, 264, 3909, 295, 264, 4295, 11, 51490, 51490, 550, 286, 393, 360, 341, 733, 295, 3861, 13, 51614, 51614, 400, 370, 341, 307, 437, 4295, 18161, 36170, 366, 466, 13, 51718, 51718, 467, 311, 257, 588, 11, 588, 2368, 4829, 412, 264, 1623, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.08708614461562213, "compression_ratio": 1.8893129770992367, "no_speech_prob": 8.799893294053618e-06}, {"id": 1266, "seek": 624682, "start": 6265.5, "end": 6267.5, "text": " so that I can connect a neural net", "tokens": [50364, 264, 954, 293, 439, 264, 721, 300, 300, 954, 575, 4243, 50518, 50518, 293, 439, 264, 10229, 300, 954, 575, 1027, 11, 1030, 11458, 11, 50712, 50712, 382, 257, 4295, 11, 4476, 13, 50804, 50804, 400, 550, 286, 2906, 11, 437, 286, 3154, 281, 264, 18161, 2533, 50978, 50978, 307, 264, 4295, 365, 4190, 322, 264, 13891, 293, 4317, 264, 10346, 82, 13, 51168, 51210, 759, 286, 362, 257, 636, 295, 13460, 257, 4295, 51298, 51298, 370, 300, 286, 393, 1745, 257, 18161, 2533, 51398, 51398, 21761, 295, 264, 3909, 295, 264, 4295, 11, 51490, 51490, 550, 286, 393, 360, 341, 733, 295, 3861, 13, 51614, 51614, 400, 370, 341, 307, 437, 4295, 18161, 36170, 366, 466, 13, 51718, 51718, 467, 311, 257, 588, 11, 588, 2368, 4829, 412, 264, 1623, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.08708614461562213, "compression_ratio": 1.8893129770992367, "no_speech_prob": 8.799893294053618e-06}, {"id": 1267, "seek": 624682, "start": 6267.5, "end": 6269.34, "text": " independently of the shape of the graph,", "tokens": [50364, 264, 954, 293, 439, 264, 721, 300, 300, 954, 575, 4243, 50518, 50518, 293, 439, 264, 10229, 300, 954, 575, 1027, 11, 1030, 11458, 11, 50712, 50712, 382, 257, 4295, 11, 4476, 13, 50804, 50804, 400, 550, 286, 2906, 11, 437, 286, 3154, 281, 264, 18161, 2533, 50978, 50978, 307, 264, 4295, 365, 4190, 322, 264, 13891, 293, 4317, 264, 10346, 82, 13, 51168, 51210, 759, 286, 362, 257, 636, 295, 13460, 257, 4295, 51298, 51298, 370, 300, 286, 393, 1745, 257, 18161, 2533, 51398, 51398, 21761, 295, 264, 3909, 295, 264, 4295, 11, 51490, 51490, 550, 286, 393, 360, 341, 733, 295, 3861, 13, 51614, 51614, 400, 370, 341, 307, 437, 4295, 18161, 36170, 366, 466, 13, 51718, 51718, 467, 311, 257, 588, 11, 588, 2368, 4829, 412, 264, 1623, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.08708614461562213, "compression_ratio": 1.8893129770992367, "no_speech_prob": 8.799893294053618e-06}, {"id": 1268, "seek": 624682, "start": 6269.34, "end": 6271.82, "text": " then I can do this kind of application.", "tokens": [50364, 264, 954, 293, 439, 264, 721, 300, 300, 954, 575, 4243, 50518, 50518, 293, 439, 264, 10229, 300, 954, 575, 1027, 11, 1030, 11458, 11, 50712, 50712, 382, 257, 4295, 11, 4476, 13, 50804, 50804, 400, 550, 286, 2906, 11, 437, 286, 3154, 281, 264, 18161, 2533, 50978, 50978, 307, 264, 4295, 365, 4190, 322, 264, 13891, 293, 4317, 264, 10346, 82, 13, 51168, 51210, 759, 286, 362, 257, 636, 295, 13460, 257, 4295, 51298, 51298, 370, 300, 286, 393, 1745, 257, 18161, 2533, 51398, 51398, 21761, 295, 264, 3909, 295, 264, 4295, 11, 51490, 51490, 550, 286, 393, 360, 341, 733, 295, 3861, 13, 51614, 51614, 400, 370, 341, 307, 437, 4295, 18161, 36170, 366, 466, 13, 51718, 51718, 467, 311, 257, 588, 11, 588, 2368, 4829, 412, 264, 1623, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.08708614461562213, "compression_ratio": 1.8893129770992367, "no_speech_prob": 8.799893294053618e-06}, {"id": 1269, "seek": 624682, "start": 6271.82, "end": 6273.9, "text": " And so this is what graph neural nets are about.", "tokens": [50364, 264, 954, 293, 439, 264, 721, 300, 300, 954, 575, 4243, 50518, 50518, 293, 439, 264, 10229, 300, 954, 575, 1027, 11, 1030, 11458, 11, 50712, 50712, 382, 257, 4295, 11, 4476, 13, 50804, 50804, 400, 550, 286, 2906, 11, 437, 286, 3154, 281, 264, 18161, 2533, 50978, 50978, 307, 264, 4295, 365, 4190, 322, 264, 13891, 293, 4317, 264, 10346, 82, 13, 51168, 51210, 759, 286, 362, 257, 636, 295, 13460, 257, 4295, 51298, 51298, 370, 300, 286, 393, 1745, 257, 18161, 2533, 51398, 51398, 21761, 295, 264, 3909, 295, 264, 4295, 11, 51490, 51490, 550, 286, 393, 360, 341, 733, 295, 3861, 13, 51614, 51614, 400, 370, 341, 307, 437, 4295, 18161, 36170, 366, 466, 13, 51718, 51718, 467, 311, 257, 588, 11, 588, 2368, 4829, 412, 264, 1623, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.08708614461562213, "compression_ratio": 1.8893129770992367, "no_speech_prob": 8.799893294053618e-06}, {"id": 1270, "seek": 624682, "start": 6273.9, "end": 6276.38, "text": " It's a very, very hot topic at the moment.", "tokens": [50364, 264, 954, 293, 439, 264, 721, 300, 300, 954, 575, 4243, 50518, 50518, 293, 439, 264, 10229, 300, 954, 575, 1027, 11, 1030, 11458, 11, 50712, 50712, 382, 257, 4295, 11, 4476, 13, 50804, 50804, 400, 550, 286, 2906, 11, 437, 286, 3154, 281, 264, 18161, 2533, 50978, 50978, 307, 264, 4295, 365, 4190, 322, 264, 13891, 293, 4317, 264, 10346, 82, 13, 51168, 51210, 759, 286, 362, 257, 636, 295, 13460, 257, 4295, 51298, 51298, 370, 300, 286, 393, 1745, 257, 18161, 2533, 51398, 51398, 21761, 295, 264, 3909, 295, 264, 4295, 11, 51490, 51490, 550, 286, 393, 360, 341, 733, 295, 3861, 13, 51614, 51614, 400, 370, 341, 307, 437, 4295, 18161, 36170, 366, 466, 13, 51718, 51718, 467, 311, 257, 588, 11, 588, 2368, 4829, 412, 264, 1623, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.08708614461562213, "compression_ratio": 1.8893129770992367, "no_speech_prob": 8.799893294053618e-06}, {"id": 1271, "seek": 627638, "start": 6276.38, "end": 6278.34, "text": " It's extremely promising for a lot of applications,", "tokens": [50364, 467, 311, 4664, 20257, 337, 257, 688, 295, 5821, 11, 50462, 50462, 4098, 294, 3228, 17671, 299, 533, 11, 294, 12558, 11, 50634, 50634, 294, 2527, 3497, 11, 457, 611, 294, 2093, 3497, 50834, 50834, 337, 2093, 3209, 5215, 50930, 50930, 293, 439, 3685, 295, 5821, 11, 3820, 11837, 11, 51180, 51180, 439, 3685, 295, 1507, 13, 51222, 51222, 407, 309, 311, 534, 1627, 13, 51306, 51306, 44653, 307, 534, 472, 295, 264, 8572, 322, 341, 4829, 13, 51434, 51434, 407, 286, 478, 534, 2055, 300, 415, 9035, 281, 976, 505, 257, 751, 13, 51580, 51580, 467, 311, 406, 799, 312, 1858, 337, 796, 570, 415, 311, 294, 14491, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.128272391202157, "compression_ratio": 1.683794466403162, "no_speech_prob": 4.005115624750033e-05}, {"id": 1272, "seek": 627638, "start": 6278.34, "end": 6281.78, "text": " particularly in biomedicine, in chemistry,", "tokens": [50364, 467, 311, 4664, 20257, 337, 257, 688, 295, 5821, 11, 50462, 50462, 4098, 294, 3228, 17671, 299, 533, 11, 294, 12558, 11, 50634, 50634, 294, 2527, 3497, 11, 457, 611, 294, 2093, 3497, 50834, 50834, 337, 2093, 3209, 5215, 50930, 50930, 293, 439, 3685, 295, 5821, 11, 3820, 11837, 11, 51180, 51180, 439, 3685, 295, 1507, 13, 51222, 51222, 407, 309, 311, 534, 1627, 13, 51306, 51306, 44653, 307, 534, 472, 295, 264, 8572, 322, 341, 4829, 13, 51434, 51434, 407, 286, 478, 534, 2055, 300, 415, 9035, 281, 976, 505, 257, 751, 13, 51580, 51580, 467, 311, 406, 799, 312, 1858, 337, 796, 570, 415, 311, 294, 14491, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.128272391202157, "compression_ratio": 1.683794466403162, "no_speech_prob": 4.005115624750033e-05}, {"id": 1273, "seek": 627638, "start": 6281.78, "end": 6285.78, "text": " in material science, but also in social science", "tokens": [50364, 467, 311, 4664, 20257, 337, 257, 688, 295, 5821, 11, 50462, 50462, 4098, 294, 3228, 17671, 299, 533, 11, 294, 12558, 11, 50634, 50634, 294, 2527, 3497, 11, 457, 611, 294, 2093, 3497, 50834, 50834, 337, 2093, 3209, 5215, 50930, 50930, 293, 439, 3685, 295, 5821, 11, 3820, 11837, 11, 51180, 51180, 439, 3685, 295, 1507, 13, 51222, 51222, 407, 309, 311, 534, 1627, 13, 51306, 51306, 44653, 307, 534, 472, 295, 264, 8572, 322, 341, 4829, 13, 51434, 51434, 407, 286, 478, 534, 2055, 300, 415, 9035, 281, 976, 505, 257, 751, 13, 51580, 51580, 467, 311, 406, 799, 312, 1858, 337, 796, 570, 415, 311, 294, 14491, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.128272391202157, "compression_ratio": 1.683794466403162, "no_speech_prob": 4.005115624750033e-05}, {"id": 1274, "seek": 627638, "start": 6285.78, "end": 6287.7, "text": " for social network analysis", "tokens": [50364, 467, 311, 4664, 20257, 337, 257, 688, 295, 5821, 11, 50462, 50462, 4098, 294, 3228, 17671, 299, 533, 11, 294, 12558, 11, 50634, 50634, 294, 2527, 3497, 11, 457, 611, 294, 2093, 3497, 50834, 50834, 337, 2093, 3209, 5215, 50930, 50930, 293, 439, 3685, 295, 5821, 11, 3820, 11837, 11, 51180, 51180, 439, 3685, 295, 1507, 13, 51222, 51222, 407, 309, 311, 534, 1627, 13, 51306, 51306, 44653, 307, 534, 472, 295, 264, 8572, 322, 341, 4829, 13, 51434, 51434, 407, 286, 478, 534, 2055, 300, 415, 9035, 281, 976, 505, 257, 751, 13, 51580, 51580, 467, 311, 406, 799, 312, 1858, 337, 796, 570, 415, 311, 294, 14491, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.128272391202157, "compression_ratio": 1.683794466403162, "no_speech_prob": 4.005115624750033e-05}, {"id": 1275, "seek": 627638, "start": 6287.7, "end": 6292.7, "text": " and all kinds of applications, computer graphics,", "tokens": [50364, 467, 311, 4664, 20257, 337, 257, 688, 295, 5821, 11, 50462, 50462, 4098, 294, 3228, 17671, 299, 533, 11, 294, 12558, 11, 50634, 50634, 294, 2527, 3497, 11, 457, 611, 294, 2093, 3497, 50834, 50834, 337, 2093, 3209, 5215, 50930, 50930, 293, 439, 3685, 295, 5821, 11, 3820, 11837, 11, 51180, 51180, 439, 3685, 295, 1507, 13, 51222, 51222, 407, 309, 311, 534, 1627, 13, 51306, 51306, 44653, 307, 534, 472, 295, 264, 8572, 322, 341, 4829, 13, 51434, 51434, 407, 286, 478, 534, 2055, 300, 415, 9035, 281, 976, 505, 257, 751, 13, 51580, 51580, 467, 311, 406, 799, 312, 1858, 337, 796, 570, 415, 311, 294, 14491, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.128272391202157, "compression_ratio": 1.683794466403162, "no_speech_prob": 4.005115624750033e-05}, {"id": 1276, "seek": 627638, "start": 6292.7, "end": 6293.54, "text": " all kinds of stuff.", "tokens": [50364, 467, 311, 4664, 20257, 337, 257, 688, 295, 5821, 11, 50462, 50462, 4098, 294, 3228, 17671, 299, 533, 11, 294, 12558, 11, 50634, 50634, 294, 2527, 3497, 11, 457, 611, 294, 2093, 3497, 50834, 50834, 337, 2093, 3209, 5215, 50930, 50930, 293, 439, 3685, 295, 5821, 11, 3820, 11837, 11, 51180, 51180, 439, 3685, 295, 1507, 13, 51222, 51222, 407, 309, 311, 534, 1627, 13, 51306, 51306, 44653, 307, 534, 472, 295, 264, 8572, 322, 341, 4829, 13, 51434, 51434, 407, 286, 478, 534, 2055, 300, 415, 9035, 281, 976, 505, 257, 751, 13, 51580, 51580, 467, 311, 406, 799, 312, 1858, 337, 796, 570, 415, 311, 294, 14491, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.128272391202157, "compression_ratio": 1.683794466403162, "no_speech_prob": 4.005115624750033e-05}, {"id": 1277, "seek": 627638, "start": 6293.54, "end": 6295.22, "text": " So it's really cool.", "tokens": [50364, 467, 311, 4664, 20257, 337, 257, 688, 295, 5821, 11, 50462, 50462, 4098, 294, 3228, 17671, 299, 533, 11, 294, 12558, 11, 50634, 50634, 294, 2527, 3497, 11, 457, 611, 294, 2093, 3497, 50834, 50834, 337, 2093, 3209, 5215, 50930, 50930, 293, 439, 3685, 295, 5821, 11, 3820, 11837, 11, 51180, 51180, 439, 3685, 295, 1507, 13, 51222, 51222, 407, 309, 311, 534, 1627, 13, 51306, 51306, 44653, 307, 534, 472, 295, 264, 8572, 322, 341, 4829, 13, 51434, 51434, 407, 286, 478, 534, 2055, 300, 415, 9035, 281, 976, 505, 257, 751, 13, 51580, 51580, 467, 311, 406, 799, 312, 1858, 337, 796, 570, 415, 311, 294, 14491, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.128272391202157, "compression_ratio": 1.683794466403162, "no_speech_prob": 4.005115624750033e-05}, {"id": 1278, "seek": 627638, "start": 6295.22, "end": 6297.78, "text": " Xavier is really one of the experts on this topic.", "tokens": [50364, 467, 311, 4664, 20257, 337, 257, 688, 295, 5821, 11, 50462, 50462, 4098, 294, 3228, 17671, 299, 533, 11, 294, 12558, 11, 50634, 50634, 294, 2527, 3497, 11, 457, 611, 294, 2093, 3497, 50834, 50834, 337, 2093, 3209, 5215, 50930, 50930, 293, 439, 3685, 295, 5821, 11, 3820, 11837, 11, 51180, 51180, 439, 3685, 295, 1507, 13, 51222, 51222, 407, 309, 311, 534, 1627, 13, 51306, 51306, 44653, 307, 534, 472, 295, 264, 8572, 322, 341, 4829, 13, 51434, 51434, 407, 286, 478, 534, 2055, 300, 415, 9035, 281, 976, 505, 257, 751, 13, 51580, 51580, 467, 311, 406, 799, 312, 1858, 337, 796, 570, 415, 311, 294, 14491, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.128272391202157, "compression_ratio": 1.683794466403162, "no_speech_prob": 4.005115624750033e-05}, {"id": 1279, "seek": 627638, "start": 6297.78, "end": 6300.7, "text": " So I'm really happy that he accepted to give us a talk.", "tokens": [50364, 467, 311, 4664, 20257, 337, 257, 688, 295, 5821, 11, 50462, 50462, 4098, 294, 3228, 17671, 299, 533, 11, 294, 12558, 11, 50634, 50634, 294, 2527, 3497, 11, 457, 611, 294, 2093, 3497, 50834, 50834, 337, 2093, 3209, 5215, 50930, 50930, 293, 439, 3685, 295, 5821, 11, 3820, 11837, 11, 51180, 51180, 439, 3685, 295, 1507, 13, 51222, 51222, 407, 309, 311, 534, 1627, 13, 51306, 51306, 44653, 307, 534, 472, 295, 264, 8572, 322, 341, 4829, 13, 51434, 51434, 407, 286, 478, 534, 2055, 300, 415, 9035, 281, 976, 505, 257, 751, 13, 51580, 51580, 467, 311, 406, 799, 312, 1858, 337, 796, 570, 415, 311, 294, 14491, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.128272391202157, "compression_ratio": 1.683794466403162, "no_speech_prob": 4.005115624750033e-05}, {"id": 1280, "seek": 627638, "start": 6300.7, "end": 6304.3, "text": " It's not gonna be easy for him because he's in Singapore.", "tokens": [50364, 467, 311, 4664, 20257, 337, 257, 688, 295, 5821, 11, 50462, 50462, 4098, 294, 3228, 17671, 299, 533, 11, 294, 12558, 11, 50634, 50634, 294, 2527, 3497, 11, 457, 611, 294, 2093, 3497, 50834, 50834, 337, 2093, 3209, 5215, 50930, 50930, 293, 439, 3685, 295, 5821, 11, 3820, 11837, 11, 51180, 51180, 439, 3685, 295, 1507, 13, 51222, 51222, 407, 309, 311, 534, 1627, 13, 51306, 51306, 44653, 307, 534, 472, 295, 264, 8572, 322, 341, 4829, 13, 51434, 51434, 407, 286, 478, 534, 2055, 300, 415, 9035, 281, 976, 505, 257, 751, 13, 51580, 51580, 467, 311, 406, 799, 312, 1858, 337, 796, 570, 415, 311, 294, 14491, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.128272391202157, "compression_ratio": 1.683794466403162, "no_speech_prob": 4.005115624750033e-05}, {"id": 1281, "seek": 630430, "start": 6304.3, "end": 6306.54, "text": " Yeah, it's gonna be fine in the morning for him.", "tokens": [50364, 865, 11, 309, 311, 799, 312, 2489, 294, 264, 2446, 337, 796, 13, 50476, 50476, 865, 11, 300, 311, 558, 13, 50518, 50518, 286, 478, 2902, 257, 7991, 294, 257, 1916, 295, 1708, 294, 8868, 9832, 13, 50666, 50666, 407, 309, 311, 264, 912, 551, 337, 385, 13, 50744, 50798, 286, 536, 13, 50840, 50840, 407, 767, 415, 311, 490, 426, 1325, 656, 8337, 4383, 3535, 13, 51038, 51038, 876, 11, 426, 3447, 13, 51080, 51080, 663, 311, 558, 13, 51122, 51122, 43452, 52, 11, 558, 30, 51164, 51164, 43452, 52, 11, 43452, 52, 11, 1338, 13, 51230, 51230, 865, 13, 51272, 51272, 286, 390, 9019, 11, 3006, 13, 51432, 51480, 1057, 558, 11, 370, 300, 390, 309, 13, 51548, 51548, 876, 11, 2597, 11, 456, 390, 472, 544, 1168, 13, 51670, 51670, 865, 11, 300, 390, 534, 1880, 11, 8419, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.21557142963148143, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.587998071452603e-05}, {"id": 1282, "seek": 630430, "start": 6306.54, "end": 6307.38, "text": " Yeah, that's right.", "tokens": [50364, 865, 11, 309, 311, 799, 312, 2489, 294, 264, 2446, 337, 796, 13, 50476, 50476, 865, 11, 300, 311, 558, 13, 50518, 50518, 286, 478, 2902, 257, 7991, 294, 257, 1916, 295, 1708, 294, 8868, 9832, 13, 50666, 50666, 407, 309, 311, 264, 912, 551, 337, 385, 13, 50744, 50798, 286, 536, 13, 50840, 50840, 407, 767, 415, 311, 490, 426, 1325, 656, 8337, 4383, 3535, 13, 51038, 51038, 876, 11, 426, 3447, 13, 51080, 51080, 663, 311, 558, 13, 51122, 51122, 43452, 52, 11, 558, 30, 51164, 51164, 43452, 52, 11, 43452, 52, 11, 1338, 13, 51230, 51230, 865, 13, 51272, 51272, 286, 390, 9019, 11, 3006, 13, 51432, 51480, 1057, 558, 11, 370, 300, 390, 309, 13, 51548, 51548, 876, 11, 2597, 11, 456, 390, 472, 544, 1168, 13, 51670, 51670, 865, 11, 300, 390, 534, 1880, 11, 8419, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.21557142963148143, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.587998071452603e-05}, {"id": 1283, "seek": 630430, "start": 6307.38, "end": 6310.34, "text": " I'm giving a lecture in a couple of days in Hong Kong.", "tokens": [50364, 865, 11, 309, 311, 799, 312, 2489, 294, 264, 2446, 337, 796, 13, 50476, 50476, 865, 11, 300, 311, 558, 13, 50518, 50518, 286, 478, 2902, 257, 7991, 294, 257, 1916, 295, 1708, 294, 8868, 9832, 13, 50666, 50666, 407, 309, 311, 264, 912, 551, 337, 385, 13, 50744, 50798, 286, 536, 13, 50840, 50840, 407, 767, 415, 311, 490, 426, 1325, 656, 8337, 4383, 3535, 13, 51038, 51038, 876, 11, 426, 3447, 13, 51080, 51080, 663, 311, 558, 13, 51122, 51122, 43452, 52, 11, 558, 30, 51164, 51164, 43452, 52, 11, 43452, 52, 11, 1338, 13, 51230, 51230, 865, 13, 51272, 51272, 286, 390, 9019, 11, 3006, 13, 51432, 51480, 1057, 558, 11, 370, 300, 390, 309, 13, 51548, 51548, 876, 11, 2597, 11, 456, 390, 472, 544, 1168, 13, 51670, 51670, 865, 11, 300, 390, 534, 1880, 11, 8419, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.21557142963148143, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.587998071452603e-05}, {"id": 1284, "seek": 630430, "start": 6310.34, "end": 6311.900000000001, "text": " So it's the same thing for me.", "tokens": [50364, 865, 11, 309, 311, 799, 312, 2489, 294, 264, 2446, 337, 796, 13, 50476, 50476, 865, 11, 300, 311, 558, 13, 50518, 50518, 286, 478, 2902, 257, 7991, 294, 257, 1916, 295, 1708, 294, 8868, 9832, 13, 50666, 50666, 407, 309, 311, 264, 912, 551, 337, 385, 13, 50744, 50798, 286, 536, 13, 50840, 50840, 407, 767, 415, 311, 490, 426, 1325, 656, 8337, 4383, 3535, 13, 51038, 51038, 876, 11, 426, 3447, 13, 51080, 51080, 663, 311, 558, 13, 51122, 51122, 43452, 52, 11, 558, 30, 51164, 51164, 43452, 52, 11, 43452, 52, 11, 1338, 13, 51230, 51230, 865, 13, 51272, 51272, 286, 390, 9019, 11, 3006, 13, 51432, 51480, 1057, 558, 11, 370, 300, 390, 309, 13, 51548, 51548, 876, 11, 2597, 11, 456, 390, 472, 544, 1168, 13, 51670, 51670, 865, 11, 300, 390, 534, 1880, 11, 8419, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.21557142963148143, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.587998071452603e-05}, {"id": 1285, "seek": 630430, "start": 6312.9800000000005, "end": 6313.820000000001, "text": " I see.", "tokens": [50364, 865, 11, 309, 311, 799, 312, 2489, 294, 264, 2446, 337, 796, 13, 50476, 50476, 865, 11, 300, 311, 558, 13, 50518, 50518, 286, 478, 2902, 257, 7991, 294, 257, 1916, 295, 1708, 294, 8868, 9832, 13, 50666, 50666, 407, 309, 311, 264, 912, 551, 337, 385, 13, 50744, 50798, 286, 536, 13, 50840, 50840, 407, 767, 415, 311, 490, 426, 1325, 656, 8337, 4383, 3535, 13, 51038, 51038, 876, 11, 426, 3447, 13, 51080, 51080, 663, 311, 558, 13, 51122, 51122, 43452, 52, 11, 558, 30, 51164, 51164, 43452, 52, 11, 43452, 52, 11, 1338, 13, 51230, 51230, 865, 13, 51272, 51272, 286, 390, 9019, 11, 3006, 13, 51432, 51480, 1057, 558, 11, 370, 300, 390, 309, 13, 51548, 51548, 876, 11, 2597, 11, 456, 390, 472, 544, 1168, 13, 51670, 51670, 865, 11, 300, 390, 534, 1880, 11, 8419, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.21557142963148143, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.587998071452603e-05}, {"id": 1286, "seek": 630430, "start": 6313.820000000001, "end": 6317.78, "text": " So actually he's from Nanyang Technological University.", "tokens": [50364, 865, 11, 309, 311, 799, 312, 2489, 294, 264, 2446, 337, 796, 13, 50476, 50476, 865, 11, 300, 311, 558, 13, 50518, 50518, 286, 478, 2902, 257, 7991, 294, 257, 1916, 295, 1708, 294, 8868, 9832, 13, 50666, 50666, 407, 309, 311, 264, 912, 551, 337, 385, 13, 50744, 50798, 286, 536, 13, 50840, 50840, 407, 767, 415, 311, 490, 426, 1325, 656, 8337, 4383, 3535, 13, 51038, 51038, 876, 11, 426, 3447, 13, 51080, 51080, 663, 311, 558, 13, 51122, 51122, 43452, 52, 11, 558, 30, 51164, 51164, 43452, 52, 11, 43452, 52, 11, 1338, 13, 51230, 51230, 865, 13, 51272, 51272, 286, 390, 9019, 11, 3006, 13, 51432, 51480, 1057, 558, 11, 370, 300, 390, 309, 13, 51548, 51548, 876, 11, 2597, 11, 456, 390, 472, 544, 1168, 13, 51670, 51670, 865, 11, 300, 390, 534, 1880, 11, 8419, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.21557142963148143, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.587998071452603e-05}, {"id": 1287, "seek": 630430, "start": 6317.78, "end": 6318.62, "text": " Oh, NUS.", "tokens": [50364, 865, 11, 309, 311, 799, 312, 2489, 294, 264, 2446, 337, 796, 13, 50476, 50476, 865, 11, 300, 311, 558, 13, 50518, 50518, 286, 478, 2902, 257, 7991, 294, 257, 1916, 295, 1708, 294, 8868, 9832, 13, 50666, 50666, 407, 309, 311, 264, 912, 551, 337, 385, 13, 50744, 50798, 286, 536, 13, 50840, 50840, 407, 767, 415, 311, 490, 426, 1325, 656, 8337, 4383, 3535, 13, 51038, 51038, 876, 11, 426, 3447, 13, 51080, 51080, 663, 311, 558, 13, 51122, 51122, 43452, 52, 11, 558, 30, 51164, 51164, 43452, 52, 11, 43452, 52, 11, 1338, 13, 51230, 51230, 865, 13, 51272, 51272, 286, 390, 9019, 11, 3006, 13, 51432, 51480, 1057, 558, 11, 370, 300, 390, 309, 13, 51548, 51548, 876, 11, 2597, 11, 456, 390, 472, 544, 1168, 13, 51670, 51670, 865, 11, 300, 390, 534, 1880, 11, 8419, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.21557142963148143, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.587998071452603e-05}, {"id": 1288, "seek": 630430, "start": 6318.62, "end": 6319.46, "text": " That's right.", "tokens": [50364, 865, 11, 309, 311, 799, 312, 2489, 294, 264, 2446, 337, 796, 13, 50476, 50476, 865, 11, 300, 311, 558, 13, 50518, 50518, 286, 478, 2902, 257, 7991, 294, 257, 1916, 295, 1708, 294, 8868, 9832, 13, 50666, 50666, 407, 309, 311, 264, 912, 551, 337, 385, 13, 50744, 50798, 286, 536, 13, 50840, 50840, 407, 767, 415, 311, 490, 426, 1325, 656, 8337, 4383, 3535, 13, 51038, 51038, 876, 11, 426, 3447, 13, 51080, 51080, 663, 311, 558, 13, 51122, 51122, 43452, 52, 11, 558, 30, 51164, 51164, 43452, 52, 11, 43452, 52, 11, 1338, 13, 51230, 51230, 865, 13, 51272, 51272, 286, 390, 9019, 11, 3006, 13, 51432, 51480, 1057, 558, 11, 370, 300, 390, 309, 13, 51548, 51548, 876, 11, 2597, 11, 456, 390, 472, 544, 1168, 13, 51670, 51670, 865, 11, 300, 390, 534, 1880, 11, 8419, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.21557142963148143, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.587998071452603e-05}, {"id": 1289, "seek": 630430, "start": 6319.46, "end": 6320.3, "text": " NTU, right?", "tokens": [50364, 865, 11, 309, 311, 799, 312, 2489, 294, 264, 2446, 337, 796, 13, 50476, 50476, 865, 11, 300, 311, 558, 13, 50518, 50518, 286, 478, 2902, 257, 7991, 294, 257, 1916, 295, 1708, 294, 8868, 9832, 13, 50666, 50666, 407, 309, 311, 264, 912, 551, 337, 385, 13, 50744, 50798, 286, 536, 13, 50840, 50840, 407, 767, 415, 311, 490, 426, 1325, 656, 8337, 4383, 3535, 13, 51038, 51038, 876, 11, 426, 3447, 13, 51080, 51080, 663, 311, 558, 13, 51122, 51122, 43452, 52, 11, 558, 30, 51164, 51164, 43452, 52, 11, 43452, 52, 11, 1338, 13, 51230, 51230, 865, 13, 51272, 51272, 286, 390, 9019, 11, 3006, 13, 51432, 51480, 1057, 558, 11, 370, 300, 390, 309, 13, 51548, 51548, 876, 11, 2597, 11, 456, 390, 472, 544, 1168, 13, 51670, 51670, 865, 11, 300, 390, 534, 1880, 11, 8419, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.21557142963148143, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.587998071452603e-05}, {"id": 1290, "seek": 630430, "start": 6320.3, "end": 6321.62, "text": " NTU, NTU, yeah.", "tokens": [50364, 865, 11, 309, 311, 799, 312, 2489, 294, 264, 2446, 337, 796, 13, 50476, 50476, 865, 11, 300, 311, 558, 13, 50518, 50518, 286, 478, 2902, 257, 7991, 294, 257, 1916, 295, 1708, 294, 8868, 9832, 13, 50666, 50666, 407, 309, 311, 264, 912, 551, 337, 385, 13, 50744, 50798, 286, 536, 13, 50840, 50840, 407, 767, 415, 311, 490, 426, 1325, 656, 8337, 4383, 3535, 13, 51038, 51038, 876, 11, 426, 3447, 13, 51080, 51080, 663, 311, 558, 13, 51122, 51122, 43452, 52, 11, 558, 30, 51164, 51164, 43452, 52, 11, 43452, 52, 11, 1338, 13, 51230, 51230, 865, 13, 51272, 51272, 286, 390, 9019, 11, 3006, 13, 51432, 51480, 1057, 558, 11, 370, 300, 390, 309, 13, 51548, 51548, 876, 11, 2597, 11, 456, 390, 472, 544, 1168, 13, 51670, 51670, 865, 11, 300, 390, 534, 1880, 11, 8419, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.21557142963148143, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.587998071452603e-05}, {"id": 1291, "seek": 630430, "start": 6321.62, "end": 6322.46, "text": " Yeah.", "tokens": [50364, 865, 11, 309, 311, 799, 312, 2489, 294, 264, 2446, 337, 796, 13, 50476, 50476, 865, 11, 300, 311, 558, 13, 50518, 50518, 286, 478, 2902, 257, 7991, 294, 257, 1916, 295, 1708, 294, 8868, 9832, 13, 50666, 50666, 407, 309, 311, 264, 912, 551, 337, 385, 13, 50744, 50798, 286, 536, 13, 50840, 50840, 407, 767, 415, 311, 490, 426, 1325, 656, 8337, 4383, 3535, 13, 51038, 51038, 876, 11, 426, 3447, 13, 51080, 51080, 663, 311, 558, 13, 51122, 51122, 43452, 52, 11, 558, 30, 51164, 51164, 43452, 52, 11, 43452, 52, 11, 1338, 13, 51230, 51230, 865, 13, 51272, 51272, 286, 390, 9019, 11, 3006, 13, 51432, 51480, 1057, 558, 11, 370, 300, 390, 309, 13, 51548, 51548, 876, 11, 2597, 11, 456, 390, 472, 544, 1168, 13, 51670, 51670, 865, 11, 300, 390, 534, 1880, 11, 8419, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.21557142963148143, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.587998071452603e-05}, {"id": 1292, "seek": 630430, "start": 6322.46, "end": 6325.66, "text": " I was confused, correct.", "tokens": [50364, 865, 11, 309, 311, 799, 312, 2489, 294, 264, 2446, 337, 796, 13, 50476, 50476, 865, 11, 300, 311, 558, 13, 50518, 50518, 286, 478, 2902, 257, 7991, 294, 257, 1916, 295, 1708, 294, 8868, 9832, 13, 50666, 50666, 407, 309, 311, 264, 912, 551, 337, 385, 13, 50744, 50798, 286, 536, 13, 50840, 50840, 407, 767, 415, 311, 490, 426, 1325, 656, 8337, 4383, 3535, 13, 51038, 51038, 876, 11, 426, 3447, 13, 51080, 51080, 663, 311, 558, 13, 51122, 51122, 43452, 52, 11, 558, 30, 51164, 51164, 43452, 52, 11, 43452, 52, 11, 1338, 13, 51230, 51230, 865, 13, 51272, 51272, 286, 390, 9019, 11, 3006, 13, 51432, 51480, 1057, 558, 11, 370, 300, 390, 309, 13, 51548, 51548, 876, 11, 2597, 11, 456, 390, 472, 544, 1168, 13, 51670, 51670, 865, 11, 300, 390, 534, 1880, 11, 8419, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.21557142963148143, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.587998071452603e-05}, {"id": 1293, "seek": 630430, "start": 6326.62, "end": 6327.9800000000005, "text": " All right, so that was it.", "tokens": [50364, 865, 11, 309, 311, 799, 312, 2489, 294, 264, 2446, 337, 796, 13, 50476, 50476, 865, 11, 300, 311, 558, 13, 50518, 50518, 286, 478, 2902, 257, 7991, 294, 257, 1916, 295, 1708, 294, 8868, 9832, 13, 50666, 50666, 407, 309, 311, 264, 912, 551, 337, 385, 13, 50744, 50798, 286, 536, 13, 50840, 50840, 407, 767, 415, 311, 490, 426, 1325, 656, 8337, 4383, 3535, 13, 51038, 51038, 876, 11, 426, 3447, 13, 51080, 51080, 663, 311, 558, 13, 51122, 51122, 43452, 52, 11, 558, 30, 51164, 51164, 43452, 52, 11, 43452, 52, 11, 1338, 13, 51230, 51230, 865, 13, 51272, 51272, 286, 390, 9019, 11, 3006, 13, 51432, 51480, 1057, 558, 11, 370, 300, 390, 309, 13, 51548, 51548, 876, 11, 2597, 11, 456, 390, 472, 544, 1168, 13, 51670, 51670, 865, 11, 300, 390, 534, 1880, 11, 8419, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.21557142963148143, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.587998071452603e-05}, {"id": 1294, "seek": 630430, "start": 6327.9800000000005, "end": 6330.42, "text": " Oh, sorry, there was one more question.", "tokens": [50364, 865, 11, 309, 311, 799, 312, 2489, 294, 264, 2446, 337, 796, 13, 50476, 50476, 865, 11, 300, 311, 558, 13, 50518, 50518, 286, 478, 2902, 257, 7991, 294, 257, 1916, 295, 1708, 294, 8868, 9832, 13, 50666, 50666, 407, 309, 311, 264, 912, 551, 337, 385, 13, 50744, 50798, 286, 536, 13, 50840, 50840, 407, 767, 415, 311, 490, 426, 1325, 656, 8337, 4383, 3535, 13, 51038, 51038, 876, 11, 426, 3447, 13, 51080, 51080, 663, 311, 558, 13, 51122, 51122, 43452, 52, 11, 558, 30, 51164, 51164, 43452, 52, 11, 43452, 52, 11, 1338, 13, 51230, 51230, 865, 13, 51272, 51272, 286, 390, 9019, 11, 3006, 13, 51432, 51480, 1057, 558, 11, 370, 300, 390, 309, 13, 51548, 51548, 876, 11, 2597, 11, 456, 390, 472, 544, 1168, 13, 51670, 51670, 865, 11, 300, 390, 534, 1880, 11, 8419, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.21557142963148143, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.587998071452603e-05}, {"id": 1295, "seek": 630430, "start": 6330.42, "end": 6333.3, "text": " Yeah, that was really interesting, Professor.", "tokens": [50364, 865, 11, 309, 311, 799, 312, 2489, 294, 264, 2446, 337, 796, 13, 50476, 50476, 865, 11, 300, 311, 558, 13, 50518, 50518, 286, 478, 2902, 257, 7991, 294, 257, 1916, 295, 1708, 294, 8868, 9832, 13, 50666, 50666, 407, 309, 311, 264, 912, 551, 337, 385, 13, 50744, 50798, 286, 536, 13, 50840, 50840, 407, 767, 415, 311, 490, 426, 1325, 656, 8337, 4383, 3535, 13, 51038, 51038, 876, 11, 426, 3447, 13, 51080, 51080, 663, 311, 558, 13, 51122, 51122, 43452, 52, 11, 558, 30, 51164, 51164, 43452, 52, 11, 43452, 52, 11, 1338, 13, 51230, 51230, 865, 13, 51272, 51272, 286, 390, 9019, 11, 3006, 13, 51432, 51480, 1057, 558, 11, 370, 300, 390, 309, 13, 51548, 51548, 876, 11, 2597, 11, 456, 390, 472, 544, 1168, 13, 51670, 51670, 865, 11, 300, 390, 534, 1880, 11, 8419, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.21557142963148143, "compression_ratio": 1.6220472440944882, "no_speech_prob": 3.587998071452603e-05}, {"id": 1296, "seek": 633330, "start": 6333.3, "end": 6334.34, "text": " I had one more question.", "tokens": [50364, 286, 632, 472, 544, 1168, 13, 50416, 50416, 286, 390, 3760, 341, 1433, 1219, 2710, 3319, 12867, 13, 50600, 50600, 400, 286, 500, 380, 1223, 437, 436, 366, 13, 50682, 50682, 7497, 291, 445, 976, 512, 24002, 50768, 50768, 666, 983, 561, 366, 2919, 466, 309, 30, 50916, 50974, 1779, 11, 370, 2710, 3319, 12867, 13, 51138, 51138, 407, 309, 311, 406, 257, 6532, 286, 362, 257, 688, 295, 1752, 365, 11, 51232, 51232, 457, 286, 600, 1401, 264, 10577, 13, 51298, 51412, 407, 309, 390, 10348, 538, 3394, 2982, 11144, 13021, 51526, 51526, 293, 47459, 347, 16123, 3475, 412, 14895, 44, 471, 257, 1339, 2057, 51776, 51832], "temperature": 0.0, "avg_logprob": -0.15862532528963955, "compression_ratio": 1.5450819672131149, "no_speech_prob": 2.0458484868868254e-05}, {"id": 1297, "seek": 633330, "start": 6334.34, "end": 6338.02, "text": " I was reading this term called normalizing flows.", "tokens": [50364, 286, 632, 472, 544, 1168, 13, 50416, 50416, 286, 390, 3760, 341, 1433, 1219, 2710, 3319, 12867, 13, 50600, 50600, 400, 286, 500, 380, 1223, 437, 436, 366, 13, 50682, 50682, 7497, 291, 445, 976, 512, 24002, 50768, 50768, 666, 983, 561, 366, 2919, 466, 309, 30, 50916, 50974, 1779, 11, 370, 2710, 3319, 12867, 13, 51138, 51138, 407, 309, 311, 406, 257, 6532, 286, 362, 257, 688, 295, 1752, 365, 11, 51232, 51232, 457, 286, 600, 1401, 264, 10577, 13, 51298, 51412, 407, 309, 390, 10348, 538, 3394, 2982, 11144, 13021, 51526, 51526, 293, 47459, 347, 16123, 3475, 412, 14895, 44, 471, 257, 1339, 2057, 51776, 51832], "temperature": 0.0, "avg_logprob": -0.15862532528963955, "compression_ratio": 1.5450819672131149, "no_speech_prob": 2.0458484868868254e-05}, {"id": 1298, "seek": 633330, "start": 6338.02, "end": 6339.66, "text": " And I don't understand what they are.", "tokens": [50364, 286, 632, 472, 544, 1168, 13, 50416, 50416, 286, 390, 3760, 341, 1433, 1219, 2710, 3319, 12867, 13, 50600, 50600, 400, 286, 500, 380, 1223, 437, 436, 366, 13, 50682, 50682, 7497, 291, 445, 976, 512, 24002, 50768, 50768, 666, 983, 561, 366, 2919, 466, 309, 30, 50916, 50974, 1779, 11, 370, 2710, 3319, 12867, 13, 51138, 51138, 407, 309, 311, 406, 257, 6532, 286, 362, 257, 688, 295, 1752, 365, 11, 51232, 51232, 457, 286, 600, 1401, 264, 10577, 13, 51298, 51412, 407, 309, 390, 10348, 538, 3394, 2982, 11144, 13021, 51526, 51526, 293, 47459, 347, 16123, 3475, 412, 14895, 44, 471, 257, 1339, 2057, 51776, 51832], "temperature": 0.0, "avg_logprob": -0.15862532528963955, "compression_ratio": 1.5450819672131149, "no_speech_prob": 2.0458484868868254e-05}, {"id": 1299, "seek": 633330, "start": 6339.66, "end": 6341.38, "text": " Could you just give some intuition", "tokens": [50364, 286, 632, 472, 544, 1168, 13, 50416, 50416, 286, 390, 3760, 341, 1433, 1219, 2710, 3319, 12867, 13, 50600, 50600, 400, 286, 500, 380, 1223, 437, 436, 366, 13, 50682, 50682, 7497, 291, 445, 976, 512, 24002, 50768, 50768, 666, 983, 561, 366, 2919, 466, 309, 30, 50916, 50974, 1779, 11, 370, 2710, 3319, 12867, 13, 51138, 51138, 407, 309, 311, 406, 257, 6532, 286, 362, 257, 688, 295, 1752, 365, 11, 51232, 51232, 457, 286, 600, 1401, 264, 10577, 13, 51298, 51412, 407, 309, 390, 10348, 538, 3394, 2982, 11144, 13021, 51526, 51526, 293, 47459, 347, 16123, 3475, 412, 14895, 44, 471, 257, 1339, 2057, 51776, 51832], "temperature": 0.0, "avg_logprob": -0.15862532528963955, "compression_ratio": 1.5450819672131149, "no_speech_prob": 2.0458484868868254e-05}, {"id": 1300, "seek": 633330, "start": 6341.38, "end": 6344.34, "text": " into why people are excited about it?", "tokens": [50364, 286, 632, 472, 544, 1168, 13, 50416, 50416, 286, 390, 3760, 341, 1433, 1219, 2710, 3319, 12867, 13, 50600, 50600, 400, 286, 500, 380, 1223, 437, 436, 366, 13, 50682, 50682, 7497, 291, 445, 976, 512, 24002, 50768, 50768, 666, 983, 561, 366, 2919, 466, 309, 30, 50916, 50974, 1779, 11, 370, 2710, 3319, 12867, 13, 51138, 51138, 407, 309, 311, 406, 257, 6532, 286, 362, 257, 688, 295, 1752, 365, 11, 51232, 51232, 457, 286, 600, 1401, 264, 10577, 13, 51298, 51412, 407, 309, 390, 10348, 538, 3394, 2982, 11144, 13021, 51526, 51526, 293, 47459, 347, 16123, 3475, 412, 14895, 44, 471, 257, 1339, 2057, 51776, 51832], "temperature": 0.0, "avg_logprob": -0.15862532528963955, "compression_ratio": 1.5450819672131149, "no_speech_prob": 2.0458484868868254e-05}, {"id": 1301, "seek": 633330, "start": 6345.5, "end": 6348.78, "text": " Right, so normalizing flows.", "tokens": [50364, 286, 632, 472, 544, 1168, 13, 50416, 50416, 286, 390, 3760, 341, 1433, 1219, 2710, 3319, 12867, 13, 50600, 50600, 400, 286, 500, 380, 1223, 437, 436, 366, 13, 50682, 50682, 7497, 291, 445, 976, 512, 24002, 50768, 50768, 666, 983, 561, 366, 2919, 466, 309, 30, 50916, 50974, 1779, 11, 370, 2710, 3319, 12867, 13, 51138, 51138, 407, 309, 311, 406, 257, 6532, 286, 362, 257, 688, 295, 1752, 365, 11, 51232, 51232, 457, 286, 600, 1401, 264, 10577, 13, 51298, 51412, 407, 309, 390, 10348, 538, 3394, 2982, 11144, 13021, 51526, 51526, 293, 47459, 347, 16123, 3475, 412, 14895, 44, 471, 257, 1339, 2057, 51776, 51832], "temperature": 0.0, "avg_logprob": -0.15862532528963955, "compression_ratio": 1.5450819672131149, "no_speech_prob": 2.0458484868868254e-05}, {"id": 1302, "seek": 633330, "start": 6348.78, "end": 6350.66, "text": " So it's not a technique I have a lot of experience with,", "tokens": [50364, 286, 632, 472, 544, 1168, 13, 50416, 50416, 286, 390, 3760, 341, 1433, 1219, 2710, 3319, 12867, 13, 50600, 50600, 400, 286, 500, 380, 1223, 437, 436, 366, 13, 50682, 50682, 7497, 291, 445, 976, 512, 24002, 50768, 50768, 666, 983, 561, 366, 2919, 466, 309, 30, 50916, 50974, 1779, 11, 370, 2710, 3319, 12867, 13, 51138, 51138, 407, 309, 311, 406, 257, 6532, 286, 362, 257, 688, 295, 1752, 365, 11, 51232, 51232, 457, 286, 600, 1401, 264, 10577, 13, 51298, 51412, 407, 309, 390, 10348, 538, 3394, 2982, 11144, 13021, 51526, 51526, 293, 47459, 347, 16123, 3475, 412, 14895, 44, 471, 257, 1339, 2057, 51776, 51832], "temperature": 0.0, "avg_logprob": -0.15862532528963955, "compression_ratio": 1.5450819672131149, "no_speech_prob": 2.0458484868868254e-05}, {"id": 1303, "seek": 633330, "start": 6350.66, "end": 6351.9800000000005, "text": " but I've read the papers.", "tokens": [50364, 286, 632, 472, 544, 1168, 13, 50416, 50416, 286, 390, 3760, 341, 1433, 1219, 2710, 3319, 12867, 13, 50600, 50600, 400, 286, 500, 380, 1223, 437, 436, 366, 13, 50682, 50682, 7497, 291, 445, 976, 512, 24002, 50768, 50768, 666, 983, 561, 366, 2919, 466, 309, 30, 50916, 50974, 1779, 11, 370, 2710, 3319, 12867, 13, 51138, 51138, 407, 309, 311, 406, 257, 6532, 286, 362, 257, 688, 295, 1752, 365, 11, 51232, 51232, 457, 286, 600, 1401, 264, 10577, 13, 51298, 51412, 407, 309, 390, 10348, 538, 3394, 2982, 11144, 13021, 51526, 51526, 293, 47459, 347, 16123, 3475, 412, 14895, 44, 471, 257, 1339, 2057, 51776, 51832], "temperature": 0.0, "avg_logprob": -0.15862532528963955, "compression_ratio": 1.5450819672131149, "no_speech_prob": 2.0458484868868254e-05}, {"id": 1304, "seek": 633330, "start": 6354.26, "end": 6356.54, "text": " So it was proposed by Danino Rosendi", "tokens": [50364, 286, 632, 472, 544, 1168, 13, 50416, 50416, 286, 390, 3760, 341, 1433, 1219, 2710, 3319, 12867, 13, 50600, 50600, 400, 286, 500, 380, 1223, 437, 436, 366, 13, 50682, 50682, 7497, 291, 445, 976, 512, 24002, 50768, 50768, 666, 983, 561, 366, 2919, 466, 309, 30, 50916, 50974, 1779, 11, 370, 2710, 3319, 12867, 13, 51138, 51138, 407, 309, 311, 406, 257, 6532, 286, 362, 257, 688, 295, 1752, 365, 11, 51232, 51232, 457, 286, 600, 1401, 264, 10577, 13, 51298, 51412, 407, 309, 390, 10348, 538, 3394, 2982, 11144, 13021, 51526, 51526, 293, 47459, 347, 16123, 3475, 412, 14895, 44, 471, 257, 1339, 2057, 51776, 51832], "temperature": 0.0, "avg_logprob": -0.15862532528963955, "compression_ratio": 1.5450819672131149, "no_speech_prob": 2.0458484868868254e-05}, {"id": 1305, "seek": 633330, "start": 6356.54, "end": 6361.54, "text": " and Shakir Mohamed at DeepMind a while ago", "tokens": [50364, 286, 632, 472, 544, 1168, 13, 50416, 50416, 286, 390, 3760, 341, 1433, 1219, 2710, 3319, 12867, 13, 50600, 50600, 400, 286, 500, 380, 1223, 437, 436, 366, 13, 50682, 50682, 7497, 291, 445, 976, 512, 24002, 50768, 50768, 666, 983, 561, 366, 2919, 466, 309, 30, 50916, 50974, 1779, 11, 370, 2710, 3319, 12867, 13, 51138, 51138, 407, 309, 311, 406, 257, 6532, 286, 362, 257, 688, 295, 1752, 365, 11, 51232, 51232, 457, 286, 600, 1401, 264, 10577, 13, 51298, 51412, 407, 309, 390, 10348, 538, 3394, 2982, 11144, 13021, 51526, 51526, 293, 47459, 347, 16123, 3475, 412, 14895, 44, 471, 257, 1339, 2057, 51776, 51832], "temperature": 0.0, "avg_logprob": -0.15862532528963955, "compression_ratio": 1.5450819672131149, "no_speech_prob": 2.0458484868868254e-05}, {"id": 1306, "seek": 636154, "start": 6361.54, "end": 6364.06, "text": " and a while back, maybe five years ago or so.", "tokens": [50364, 293, 257, 1339, 646, 11, 1310, 1732, 924, 2057, 420, 370, 13, 50490, 50542, 400, 309, 311, 1936, 257, 1333, 295, 257, 10305, 35701, 3170, 13, 50708, 50708, 407, 309, 311, 257, 707, 857, 411, 460, 1770, 82, 13, 50818, 50818, 467, 575, 257, 857, 295, 264, 912, 3797, 382, 460, 1770, 82, 13, 50946, 51028, 400, 309, 2170, 10249, 490, 14360, 32, 11, 51210, 51210, 6695, 6542, 5215, 11, 51306, 51306, 4878, 309, 311, 406, 733, 295, 13691, 294, 264, 3380, 3035, 13, 51454, 51454, 583, 510, 311, 264, 3875, 1558, 13, 51496, 51496, 440, 3875, 1558, 307, 291, 528, 281, 3847, 257, 18161, 2533, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.2939099051735618, "compression_ratio": 1.5798319327731092, "no_speech_prob": 1.1298031495243777e-05}, {"id": 1307, "seek": 636154, "start": 6365.1, "end": 6368.42, "text": " And it's basically a sort of a density estimation method.", "tokens": [50364, 293, 257, 1339, 646, 11, 1310, 1732, 924, 2057, 420, 370, 13, 50490, 50542, 400, 309, 311, 1936, 257, 1333, 295, 257, 10305, 35701, 3170, 13, 50708, 50708, 407, 309, 311, 257, 707, 857, 411, 460, 1770, 82, 13, 50818, 50818, 467, 575, 257, 857, 295, 264, 912, 3797, 382, 460, 1770, 82, 13, 50946, 51028, 400, 309, 2170, 10249, 490, 14360, 32, 11, 51210, 51210, 6695, 6542, 5215, 11, 51306, 51306, 4878, 309, 311, 406, 733, 295, 13691, 294, 264, 3380, 3035, 13, 51454, 51454, 583, 510, 311, 264, 3875, 1558, 13, 51496, 51496, 440, 3875, 1558, 307, 291, 528, 281, 3847, 257, 18161, 2533, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.2939099051735618, "compression_ratio": 1.5798319327731092, "no_speech_prob": 1.1298031495243777e-05}, {"id": 1308, "seek": 636154, "start": 6368.42, "end": 6370.62, "text": " So it's a little bit like GANs.", "tokens": [50364, 293, 257, 1339, 646, 11, 1310, 1732, 924, 2057, 420, 370, 13, 50490, 50542, 400, 309, 311, 1936, 257, 1333, 295, 257, 10305, 35701, 3170, 13, 50708, 50708, 407, 309, 311, 257, 707, 857, 411, 460, 1770, 82, 13, 50818, 50818, 467, 575, 257, 857, 295, 264, 912, 3797, 382, 460, 1770, 82, 13, 50946, 51028, 400, 309, 2170, 10249, 490, 14360, 32, 11, 51210, 51210, 6695, 6542, 5215, 11, 51306, 51306, 4878, 309, 311, 406, 733, 295, 13691, 294, 264, 3380, 3035, 13, 51454, 51454, 583, 510, 311, 264, 3875, 1558, 13, 51496, 51496, 440, 3875, 1558, 307, 291, 528, 281, 3847, 257, 18161, 2533, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.2939099051735618, "compression_ratio": 1.5798319327731092, "no_speech_prob": 1.1298031495243777e-05}, {"id": 1309, "seek": 636154, "start": 6370.62, "end": 6373.18, "text": " It has a bit of the same spirit as GANs.", "tokens": [50364, 293, 257, 1339, 646, 11, 1310, 1732, 924, 2057, 420, 370, 13, 50490, 50542, 400, 309, 311, 1936, 257, 1333, 295, 257, 10305, 35701, 3170, 13, 50708, 50708, 407, 309, 311, 257, 707, 857, 411, 460, 1770, 82, 13, 50818, 50818, 467, 575, 257, 857, 295, 264, 912, 3797, 382, 460, 1770, 82, 13, 50946, 51028, 400, 309, 2170, 10249, 490, 14360, 32, 11, 51210, 51210, 6695, 6542, 5215, 11, 51306, 51306, 4878, 309, 311, 406, 733, 295, 13691, 294, 264, 3380, 3035, 13, 51454, 51454, 583, 510, 311, 264, 3875, 1558, 13, 51496, 51496, 440, 3875, 1558, 307, 291, 528, 281, 3847, 257, 18161, 2533, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.2939099051735618, "compression_ratio": 1.5798319327731092, "no_speech_prob": 1.1298031495243777e-05}, {"id": 1310, "seek": 636154, "start": 6374.82, "end": 6378.46, "text": " And it gets inspiration from ICA,", "tokens": [50364, 293, 257, 1339, 646, 11, 1310, 1732, 924, 2057, 420, 370, 13, 50490, 50542, 400, 309, 311, 1936, 257, 1333, 295, 257, 10305, 35701, 3170, 13, 50708, 50708, 407, 309, 311, 257, 707, 857, 411, 460, 1770, 82, 13, 50818, 50818, 467, 575, 257, 857, 295, 264, 912, 3797, 382, 460, 1770, 82, 13, 50946, 51028, 400, 309, 2170, 10249, 490, 14360, 32, 11, 51210, 51210, 6695, 6542, 5215, 11, 51306, 51306, 4878, 309, 311, 406, 733, 295, 13691, 294, 264, 3380, 3035, 13, 51454, 51454, 583, 510, 311, 264, 3875, 1558, 13, 51496, 51496, 440, 3875, 1558, 307, 291, 528, 281, 3847, 257, 18161, 2533, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.2939099051735618, "compression_ratio": 1.5798319327731092, "no_speech_prob": 1.1298031495243777e-05}, {"id": 1311, "seek": 636154, "start": 6378.46, "end": 6380.38, "text": " independent component analysis,", "tokens": [50364, 293, 257, 1339, 646, 11, 1310, 1732, 924, 2057, 420, 370, 13, 50490, 50542, 400, 309, 311, 1936, 257, 1333, 295, 257, 10305, 35701, 3170, 13, 50708, 50708, 407, 309, 311, 257, 707, 857, 411, 460, 1770, 82, 13, 50818, 50818, 467, 575, 257, 857, 295, 264, 912, 3797, 382, 460, 1770, 82, 13, 50946, 51028, 400, 309, 2170, 10249, 490, 14360, 32, 11, 51210, 51210, 6695, 6542, 5215, 11, 51306, 51306, 4878, 309, 311, 406, 733, 295, 13691, 294, 264, 3380, 3035, 13, 51454, 51454, 583, 510, 311, 264, 3875, 1558, 13, 51496, 51496, 440, 3875, 1558, 307, 291, 528, 281, 3847, 257, 18161, 2533, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.2939099051735618, "compression_ratio": 1.5798319327731092, "no_speech_prob": 1.1298031495243777e-05}, {"id": 1312, "seek": 636154, "start": 6380.38, "end": 6383.34, "text": " although it's not kind of explicit in the original paper.", "tokens": [50364, 293, 257, 1339, 646, 11, 1310, 1732, 924, 2057, 420, 370, 13, 50490, 50542, 400, 309, 311, 1936, 257, 1333, 295, 257, 10305, 35701, 3170, 13, 50708, 50708, 407, 309, 311, 257, 707, 857, 411, 460, 1770, 82, 13, 50818, 50818, 467, 575, 257, 857, 295, 264, 912, 3797, 382, 460, 1770, 82, 13, 50946, 51028, 400, 309, 2170, 10249, 490, 14360, 32, 11, 51210, 51210, 6695, 6542, 5215, 11, 51306, 51306, 4878, 309, 311, 406, 733, 295, 13691, 294, 264, 3380, 3035, 13, 51454, 51454, 583, 510, 311, 264, 3875, 1558, 13, 51496, 51496, 440, 3875, 1558, 307, 291, 528, 281, 3847, 257, 18161, 2533, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.2939099051735618, "compression_ratio": 1.5798319327731092, "no_speech_prob": 1.1298031495243777e-05}, {"id": 1313, "seek": 636154, "start": 6383.34, "end": 6384.18, "text": " But here's the basic idea.", "tokens": [50364, 293, 257, 1339, 646, 11, 1310, 1732, 924, 2057, 420, 370, 13, 50490, 50542, 400, 309, 311, 1936, 257, 1333, 295, 257, 10305, 35701, 3170, 13, 50708, 50708, 407, 309, 311, 257, 707, 857, 411, 460, 1770, 82, 13, 50818, 50818, 467, 575, 257, 857, 295, 264, 912, 3797, 382, 460, 1770, 82, 13, 50946, 51028, 400, 309, 2170, 10249, 490, 14360, 32, 11, 51210, 51210, 6695, 6542, 5215, 11, 51306, 51306, 4878, 309, 311, 406, 733, 295, 13691, 294, 264, 3380, 3035, 13, 51454, 51454, 583, 510, 311, 264, 3875, 1558, 13, 51496, 51496, 440, 3875, 1558, 307, 291, 528, 281, 3847, 257, 18161, 2533, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.2939099051735618, "compression_ratio": 1.5798319327731092, "no_speech_prob": 1.1298031495243777e-05}, {"id": 1314, "seek": 636154, "start": 6384.18, "end": 6388.7, "text": " The basic idea is you want to train a neural net", "tokens": [50364, 293, 257, 1339, 646, 11, 1310, 1732, 924, 2057, 420, 370, 13, 50490, 50542, 400, 309, 311, 1936, 257, 1333, 295, 257, 10305, 35701, 3170, 13, 50708, 50708, 407, 309, 311, 257, 707, 857, 411, 460, 1770, 82, 13, 50818, 50818, 467, 575, 257, 857, 295, 264, 912, 3797, 382, 460, 1770, 82, 13, 50946, 51028, 400, 309, 2170, 10249, 490, 14360, 32, 11, 51210, 51210, 6695, 6542, 5215, 11, 51306, 51306, 4878, 309, 311, 406, 733, 295, 13691, 294, 264, 3380, 3035, 13, 51454, 51454, 583, 510, 311, 264, 3875, 1558, 13, 51496, 51496, 440, 3875, 1558, 307, 291, 528, 281, 3847, 257, 18161, 2533, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.2939099051735618, "compression_ratio": 1.5798319327731092, "no_speech_prob": 1.1298031495243777e-05}, {"id": 1315, "seek": 638870, "start": 6388.7, "end": 6391.78, "text": " to transform a known distribution from which you can sample", "tokens": [50364, 281, 4088, 257, 2570, 7316, 490, 597, 291, 393, 6889, 50518, 50518, 666, 257, 7316, 300, 2314, 281, 312, 50600, 50600, 264, 7316, 295, 428, 1412, 13, 50676, 50720, 407, 718, 311, 3811, 300, 291, 362, 257, 707, 7006, 710, 50902, 50902, 300, 291, 6889, 490, 257, 39148, 7316, 51008, 51056, 293, 291, 1190, 309, 807, 257, 2445, 51146, 51146, 420, 9452, 7316, 670, 257, 9274, 13, 51280, 51280, 509, 1190, 309, 807, 257, 2445, 12270, 538, 257, 18161, 2533, 51430, 51430, 293, 291, 528, 281, 3847, 341, 18161, 2533, 51500, 51500, 370, 300, 264, 7316, 291, 483, 412, 264, 5598, 51622, 51622, 307, 264, 472, 291, 528, 300, 23249, 281, 428, 1412, 13, 51742, 51792], "temperature": 0.0, "avg_logprob": -0.32447688862428825, "compression_ratio": 2.0246913580246915, "no_speech_prob": 1.7496196960564703e-05}, {"id": 1316, "seek": 638870, "start": 6391.78, "end": 6393.42, "text": " into a distribution that happens to be", "tokens": [50364, 281, 4088, 257, 2570, 7316, 490, 597, 291, 393, 6889, 50518, 50518, 666, 257, 7316, 300, 2314, 281, 312, 50600, 50600, 264, 7316, 295, 428, 1412, 13, 50676, 50720, 407, 718, 311, 3811, 300, 291, 362, 257, 707, 7006, 710, 50902, 50902, 300, 291, 6889, 490, 257, 39148, 7316, 51008, 51056, 293, 291, 1190, 309, 807, 257, 2445, 51146, 51146, 420, 9452, 7316, 670, 257, 9274, 13, 51280, 51280, 509, 1190, 309, 807, 257, 2445, 12270, 538, 257, 18161, 2533, 51430, 51430, 293, 291, 528, 281, 3847, 341, 18161, 2533, 51500, 51500, 370, 300, 264, 7316, 291, 483, 412, 264, 5598, 51622, 51622, 307, 264, 472, 291, 528, 300, 23249, 281, 428, 1412, 13, 51742, 51792], "temperature": 0.0, "avg_logprob": -0.32447688862428825, "compression_ratio": 2.0246913580246915, "no_speech_prob": 1.7496196960564703e-05}, {"id": 1317, "seek": 638870, "start": 6393.42, "end": 6394.94, "text": " the distribution of your data.", "tokens": [50364, 281, 4088, 257, 2570, 7316, 490, 597, 291, 393, 6889, 50518, 50518, 666, 257, 7316, 300, 2314, 281, 312, 50600, 50600, 264, 7316, 295, 428, 1412, 13, 50676, 50720, 407, 718, 311, 3811, 300, 291, 362, 257, 707, 7006, 710, 50902, 50902, 300, 291, 6889, 490, 257, 39148, 7316, 51008, 51056, 293, 291, 1190, 309, 807, 257, 2445, 51146, 51146, 420, 9452, 7316, 670, 257, 9274, 13, 51280, 51280, 509, 1190, 309, 807, 257, 2445, 12270, 538, 257, 18161, 2533, 51430, 51430, 293, 291, 528, 281, 3847, 341, 18161, 2533, 51500, 51500, 370, 300, 264, 7316, 291, 483, 412, 264, 5598, 51622, 51622, 307, 264, 472, 291, 528, 300, 23249, 281, 428, 1412, 13, 51742, 51792], "temperature": 0.0, "avg_logprob": -0.32447688862428825, "compression_ratio": 2.0246913580246915, "no_speech_prob": 1.7496196960564703e-05}, {"id": 1318, "seek": 638870, "start": 6395.82, "end": 6399.46, "text": " So let's imagine that you have a little variable z", "tokens": [50364, 281, 4088, 257, 2570, 7316, 490, 597, 291, 393, 6889, 50518, 50518, 666, 257, 7316, 300, 2314, 281, 312, 50600, 50600, 264, 7316, 295, 428, 1412, 13, 50676, 50720, 407, 718, 311, 3811, 300, 291, 362, 257, 707, 7006, 710, 50902, 50902, 300, 291, 6889, 490, 257, 39148, 7316, 51008, 51056, 293, 291, 1190, 309, 807, 257, 2445, 51146, 51146, 420, 9452, 7316, 670, 257, 9274, 13, 51280, 51280, 509, 1190, 309, 807, 257, 2445, 12270, 538, 257, 18161, 2533, 51430, 51430, 293, 291, 528, 281, 3847, 341, 18161, 2533, 51500, 51500, 370, 300, 264, 7316, 291, 483, 412, 264, 5598, 51622, 51622, 307, 264, 472, 291, 528, 300, 23249, 281, 428, 1412, 13, 51742, 51792], "temperature": 0.0, "avg_logprob": -0.32447688862428825, "compression_ratio": 2.0246913580246915, "no_speech_prob": 1.7496196960564703e-05}, {"id": 1319, "seek": 638870, "start": 6399.46, "end": 6401.58, "text": " that you sample from a Gaussian distribution", "tokens": [50364, 281, 4088, 257, 2570, 7316, 490, 597, 291, 393, 6889, 50518, 50518, 666, 257, 7316, 300, 2314, 281, 312, 50600, 50600, 264, 7316, 295, 428, 1412, 13, 50676, 50720, 407, 718, 311, 3811, 300, 291, 362, 257, 707, 7006, 710, 50902, 50902, 300, 291, 6889, 490, 257, 39148, 7316, 51008, 51056, 293, 291, 1190, 309, 807, 257, 2445, 51146, 51146, 420, 9452, 7316, 670, 257, 9274, 13, 51280, 51280, 509, 1190, 309, 807, 257, 2445, 12270, 538, 257, 18161, 2533, 51430, 51430, 293, 291, 528, 281, 3847, 341, 18161, 2533, 51500, 51500, 370, 300, 264, 7316, 291, 483, 412, 264, 5598, 51622, 51622, 307, 264, 472, 291, 528, 300, 23249, 281, 428, 1412, 13, 51742, 51792], "temperature": 0.0, "avg_logprob": -0.32447688862428825, "compression_ratio": 2.0246913580246915, "no_speech_prob": 1.7496196960564703e-05}, {"id": 1320, "seek": 638870, "start": 6402.54, "end": 6404.34, "text": " and you run it through a function", "tokens": [50364, 281, 4088, 257, 2570, 7316, 490, 597, 291, 393, 6889, 50518, 50518, 666, 257, 7316, 300, 2314, 281, 312, 50600, 50600, 264, 7316, 295, 428, 1412, 13, 50676, 50720, 407, 718, 311, 3811, 300, 291, 362, 257, 707, 7006, 710, 50902, 50902, 300, 291, 6889, 490, 257, 39148, 7316, 51008, 51056, 293, 291, 1190, 309, 807, 257, 2445, 51146, 51146, 420, 9452, 7316, 670, 257, 9274, 13, 51280, 51280, 509, 1190, 309, 807, 257, 2445, 12270, 538, 257, 18161, 2533, 51430, 51430, 293, 291, 528, 281, 3847, 341, 18161, 2533, 51500, 51500, 370, 300, 264, 7316, 291, 483, 412, 264, 5598, 51622, 51622, 307, 264, 472, 291, 528, 300, 23249, 281, 428, 1412, 13, 51742, 51792], "temperature": 0.0, "avg_logprob": -0.32447688862428825, "compression_ratio": 2.0246913580246915, "no_speech_prob": 1.7496196960564703e-05}, {"id": 1321, "seek": 638870, "start": 6404.34, "end": 6407.0199999999995, "text": " or uniform distribution over a domain.", "tokens": [50364, 281, 4088, 257, 2570, 7316, 490, 597, 291, 393, 6889, 50518, 50518, 666, 257, 7316, 300, 2314, 281, 312, 50600, 50600, 264, 7316, 295, 428, 1412, 13, 50676, 50720, 407, 718, 311, 3811, 300, 291, 362, 257, 707, 7006, 710, 50902, 50902, 300, 291, 6889, 490, 257, 39148, 7316, 51008, 51056, 293, 291, 1190, 309, 807, 257, 2445, 51146, 51146, 420, 9452, 7316, 670, 257, 9274, 13, 51280, 51280, 509, 1190, 309, 807, 257, 2445, 12270, 538, 257, 18161, 2533, 51430, 51430, 293, 291, 528, 281, 3847, 341, 18161, 2533, 51500, 51500, 370, 300, 264, 7316, 291, 483, 412, 264, 5598, 51622, 51622, 307, 264, 472, 291, 528, 300, 23249, 281, 428, 1412, 13, 51742, 51792], "temperature": 0.0, "avg_logprob": -0.32447688862428825, "compression_ratio": 2.0246913580246915, "no_speech_prob": 1.7496196960564703e-05}, {"id": 1322, "seek": 638870, "start": 6407.0199999999995, "end": 6410.0199999999995, "text": " You run it through a function implemented by a neural net", "tokens": [50364, 281, 4088, 257, 2570, 7316, 490, 597, 291, 393, 6889, 50518, 50518, 666, 257, 7316, 300, 2314, 281, 312, 50600, 50600, 264, 7316, 295, 428, 1412, 13, 50676, 50720, 407, 718, 311, 3811, 300, 291, 362, 257, 707, 7006, 710, 50902, 50902, 300, 291, 6889, 490, 257, 39148, 7316, 51008, 51056, 293, 291, 1190, 309, 807, 257, 2445, 51146, 51146, 420, 9452, 7316, 670, 257, 9274, 13, 51280, 51280, 509, 1190, 309, 807, 257, 2445, 12270, 538, 257, 18161, 2533, 51430, 51430, 293, 291, 528, 281, 3847, 341, 18161, 2533, 51500, 51500, 370, 300, 264, 7316, 291, 483, 412, 264, 5598, 51622, 51622, 307, 264, 472, 291, 528, 300, 23249, 281, 428, 1412, 13, 51742, 51792], "temperature": 0.0, "avg_logprob": -0.32447688862428825, "compression_ratio": 2.0246913580246915, "no_speech_prob": 1.7496196960564703e-05}, {"id": 1323, "seek": 638870, "start": 6410.0199999999995, "end": 6411.42, "text": " and you want to train this neural net", "tokens": [50364, 281, 4088, 257, 2570, 7316, 490, 597, 291, 393, 6889, 50518, 50518, 666, 257, 7316, 300, 2314, 281, 312, 50600, 50600, 264, 7316, 295, 428, 1412, 13, 50676, 50720, 407, 718, 311, 3811, 300, 291, 362, 257, 707, 7006, 710, 50902, 50902, 300, 291, 6889, 490, 257, 39148, 7316, 51008, 51056, 293, 291, 1190, 309, 807, 257, 2445, 51146, 51146, 420, 9452, 7316, 670, 257, 9274, 13, 51280, 51280, 509, 1190, 309, 807, 257, 2445, 12270, 538, 257, 18161, 2533, 51430, 51430, 293, 291, 528, 281, 3847, 341, 18161, 2533, 51500, 51500, 370, 300, 264, 7316, 291, 483, 412, 264, 5598, 51622, 51622, 307, 264, 472, 291, 528, 300, 23249, 281, 428, 1412, 13, 51742, 51792], "temperature": 0.0, "avg_logprob": -0.32447688862428825, "compression_ratio": 2.0246913580246915, "no_speech_prob": 1.7496196960564703e-05}, {"id": 1324, "seek": 638870, "start": 6411.42, "end": 6413.86, "text": " so that the distribution you get at the output", "tokens": [50364, 281, 4088, 257, 2570, 7316, 490, 597, 291, 393, 6889, 50518, 50518, 666, 257, 7316, 300, 2314, 281, 312, 50600, 50600, 264, 7316, 295, 428, 1412, 13, 50676, 50720, 407, 718, 311, 3811, 300, 291, 362, 257, 707, 7006, 710, 50902, 50902, 300, 291, 6889, 490, 257, 39148, 7316, 51008, 51056, 293, 291, 1190, 309, 807, 257, 2445, 51146, 51146, 420, 9452, 7316, 670, 257, 9274, 13, 51280, 51280, 509, 1190, 309, 807, 257, 2445, 12270, 538, 257, 18161, 2533, 51430, 51430, 293, 291, 528, 281, 3847, 341, 18161, 2533, 51500, 51500, 370, 300, 264, 7316, 291, 483, 412, 264, 5598, 51622, 51622, 307, 264, 472, 291, 528, 300, 23249, 281, 428, 1412, 13, 51742, 51792], "temperature": 0.0, "avg_logprob": -0.32447688862428825, "compression_ratio": 2.0246913580246915, "no_speech_prob": 1.7496196960564703e-05}, {"id": 1325, "seek": 638870, "start": 6413.86, "end": 6416.26, "text": " is the one you want that corresponds to your data.", "tokens": [50364, 281, 4088, 257, 2570, 7316, 490, 597, 291, 393, 6889, 50518, 50518, 666, 257, 7316, 300, 2314, 281, 312, 50600, 50600, 264, 7316, 295, 428, 1412, 13, 50676, 50720, 407, 718, 311, 3811, 300, 291, 362, 257, 707, 7006, 710, 50902, 50902, 300, 291, 6889, 490, 257, 39148, 7316, 51008, 51056, 293, 291, 1190, 309, 807, 257, 2445, 51146, 51146, 420, 9452, 7316, 670, 257, 9274, 13, 51280, 51280, 509, 1190, 309, 807, 257, 2445, 12270, 538, 257, 18161, 2533, 51430, 51430, 293, 291, 528, 281, 3847, 341, 18161, 2533, 51500, 51500, 370, 300, 264, 7316, 291, 483, 412, 264, 5598, 51622, 51622, 307, 264, 472, 291, 528, 300, 23249, 281, 428, 1412, 13, 51742, 51792], "temperature": 0.0, "avg_logprob": -0.32447688862428825, "compression_ratio": 2.0246913580246915, "no_speech_prob": 1.7496196960564703e-05}, {"id": 1326, "seek": 641626, "start": 6416.26, "end": 6421.26, "text": " Okay. And so let me give you a very simple example.", "tokens": [50364, 1033, 13, 400, 370, 718, 385, 976, 291, 257, 588, 2199, 1365, 13, 50614, 51144, 407, 718, 311, 584, 286, 362, 257, 7006, 710, 51294, 51452, 293, 286, 362, 13095, 7006, 288, 51574, 51574, 293, 286, 6889, 452, 7006, 710, 365, 257, 9452, 7316, 51774, 51830], "temperature": 0.0, "avg_logprob": -0.3162050247192383, "compression_ratio": 1.3934426229508197, "no_speech_prob": 2.1905045741732465e-06}, {"id": 1327, "seek": 641626, "start": 6431.860000000001, "end": 6434.860000000001, "text": " So let's say I have a variable z", "tokens": [50364, 1033, 13, 400, 370, 718, 385, 976, 291, 257, 588, 2199, 1365, 13, 50614, 51144, 407, 718, 311, 584, 286, 362, 257, 7006, 710, 51294, 51452, 293, 286, 362, 13095, 7006, 288, 51574, 51574, 293, 286, 6889, 452, 7006, 710, 365, 257, 9452, 7316, 51774, 51830], "temperature": 0.0, "avg_logprob": -0.3162050247192383, "compression_ratio": 1.3934426229508197, "no_speech_prob": 2.1905045741732465e-06}, {"id": 1328, "seek": 641626, "start": 6438.02, "end": 6440.46, "text": " and I have observed variable y", "tokens": [50364, 1033, 13, 400, 370, 718, 385, 976, 291, 257, 588, 2199, 1365, 13, 50614, 51144, 407, 718, 311, 584, 286, 362, 257, 7006, 710, 51294, 51452, 293, 286, 362, 13095, 7006, 288, 51574, 51574, 293, 286, 6889, 452, 7006, 710, 365, 257, 9452, 7316, 51774, 51830], "temperature": 0.0, "avg_logprob": -0.3162050247192383, "compression_ratio": 1.3934426229508197, "no_speech_prob": 2.1905045741732465e-06}, {"id": 1329, "seek": 641626, "start": 6440.46, "end": 6444.46, "text": " and I sample my variable z with a uniform distribution", "tokens": [50364, 1033, 13, 400, 370, 718, 385, 976, 291, 257, 588, 2199, 1365, 13, 50614, 51144, 407, 718, 311, 584, 286, 362, 257, 7006, 710, 51294, 51452, 293, 286, 362, 13095, 7006, 288, 51574, 51574, 293, 286, 6889, 452, 7006, 710, 365, 257, 9452, 7316, 51774, 51830], "temperature": 0.0, "avg_logprob": -0.3162050247192383, "compression_ratio": 1.3934426229508197, "no_speech_prob": 2.1905045741732465e-06}, {"id": 1330, "seek": 644446, "start": 6444.46, "end": 6447.58, "text": " between say zero and one.", "tokens": [50364, 1296, 584, 4018, 293, 472, 13, 50520, 50690, 400, 437, 286, 528, 322, 264, 5598, 307, 11, 286, 500, 380, 458, 11, 50902, 50994, 584, 257, 39148, 13, 51050, 51050, 467, 311, 733, 295, 6631, 281, 528, 257, 39148, 11, 51120, 51120, 457, 718, 311, 584, 286, 528, 257, 39148, 51186, 51186, 570, 286, 727, 6889, 490, 257, 39148, 3612, 13, 51298, 51366, 407, 437, 286, 643, 281, 360, 307, 733, 295, 4088, 51516, 51516, 341, 9452, 7316, 666, 257, 39148, 538, 257, 18350, 13, 51706, 51706, 400, 264, 18350, 307, 516, 281, 312, 257, 2445, 411, 51818], "temperature": 0.0, "avg_logprob": -0.16092292785644532, "compression_ratio": 1.6889952153110048, "no_speech_prob": 3.089179472226533e-06}, {"id": 1331, "seek": 644446, "start": 6450.9800000000005, "end": 6455.22, "text": " And what I want on the output is, I don't know,", "tokens": [50364, 1296, 584, 4018, 293, 472, 13, 50520, 50690, 400, 437, 286, 528, 322, 264, 5598, 307, 11, 286, 500, 380, 458, 11, 50902, 50994, 584, 257, 39148, 13, 51050, 51050, 467, 311, 733, 295, 6631, 281, 528, 257, 39148, 11, 51120, 51120, 457, 718, 311, 584, 286, 528, 257, 39148, 51186, 51186, 570, 286, 727, 6889, 490, 257, 39148, 3612, 13, 51298, 51366, 407, 437, 286, 643, 281, 360, 307, 733, 295, 4088, 51516, 51516, 341, 9452, 7316, 666, 257, 39148, 538, 257, 18350, 13, 51706, 51706, 400, 264, 18350, 307, 516, 281, 312, 257, 2445, 411, 51818], "temperature": 0.0, "avg_logprob": -0.16092292785644532, "compression_ratio": 1.6889952153110048, "no_speech_prob": 3.089179472226533e-06}, {"id": 1332, "seek": 644446, "start": 6457.06, "end": 6458.18, "text": " say a Gaussian.", "tokens": [50364, 1296, 584, 4018, 293, 472, 13, 50520, 50690, 400, 437, 286, 528, 322, 264, 5598, 307, 11, 286, 500, 380, 458, 11, 50902, 50994, 584, 257, 39148, 13, 51050, 51050, 467, 311, 733, 295, 6631, 281, 528, 257, 39148, 11, 51120, 51120, 457, 718, 311, 584, 286, 528, 257, 39148, 51186, 51186, 570, 286, 727, 6889, 490, 257, 39148, 3612, 13, 51298, 51366, 407, 437, 286, 643, 281, 360, 307, 733, 295, 4088, 51516, 51516, 341, 9452, 7316, 666, 257, 39148, 538, 257, 18350, 13, 51706, 51706, 400, 264, 18350, 307, 516, 281, 312, 257, 2445, 411, 51818], "temperature": 0.0, "avg_logprob": -0.16092292785644532, "compression_ratio": 1.6889952153110048, "no_speech_prob": 3.089179472226533e-06}, {"id": 1333, "seek": 644446, "start": 6458.18, "end": 6459.58, "text": " It's kind of stupid to want a Gaussian,", "tokens": [50364, 1296, 584, 4018, 293, 472, 13, 50520, 50690, 400, 437, 286, 528, 322, 264, 5598, 307, 11, 286, 500, 380, 458, 11, 50902, 50994, 584, 257, 39148, 13, 51050, 51050, 467, 311, 733, 295, 6631, 281, 528, 257, 39148, 11, 51120, 51120, 457, 718, 311, 584, 286, 528, 257, 39148, 51186, 51186, 570, 286, 727, 6889, 490, 257, 39148, 3612, 13, 51298, 51366, 407, 437, 286, 643, 281, 360, 307, 733, 295, 4088, 51516, 51516, 341, 9452, 7316, 666, 257, 39148, 538, 257, 18350, 13, 51706, 51706, 400, 264, 18350, 307, 516, 281, 312, 257, 2445, 411, 51818], "temperature": 0.0, "avg_logprob": -0.16092292785644532, "compression_ratio": 1.6889952153110048, "no_speech_prob": 3.089179472226533e-06}, {"id": 1334, "seek": 644446, "start": 6459.58, "end": 6460.9, "text": " but let's say I want a Gaussian", "tokens": [50364, 1296, 584, 4018, 293, 472, 13, 50520, 50690, 400, 437, 286, 528, 322, 264, 5598, 307, 11, 286, 500, 380, 458, 11, 50902, 50994, 584, 257, 39148, 13, 51050, 51050, 467, 311, 733, 295, 6631, 281, 528, 257, 39148, 11, 51120, 51120, 457, 718, 311, 584, 286, 528, 257, 39148, 51186, 51186, 570, 286, 727, 6889, 490, 257, 39148, 3612, 13, 51298, 51366, 407, 437, 286, 643, 281, 360, 307, 733, 295, 4088, 51516, 51516, 341, 9452, 7316, 666, 257, 39148, 538, 257, 18350, 13, 51706, 51706, 400, 264, 18350, 307, 516, 281, 312, 257, 2445, 411, 51818], "temperature": 0.0, "avg_logprob": -0.16092292785644532, "compression_ratio": 1.6889952153110048, "no_speech_prob": 3.089179472226533e-06}, {"id": 1335, "seek": 644446, "start": 6460.9, "end": 6463.14, "text": " because I could sample from a Gaussian easily.", "tokens": [50364, 1296, 584, 4018, 293, 472, 13, 50520, 50690, 400, 437, 286, 528, 322, 264, 5598, 307, 11, 286, 500, 380, 458, 11, 50902, 50994, 584, 257, 39148, 13, 51050, 51050, 467, 311, 733, 295, 6631, 281, 528, 257, 39148, 11, 51120, 51120, 457, 718, 311, 584, 286, 528, 257, 39148, 51186, 51186, 570, 286, 727, 6889, 490, 257, 39148, 3612, 13, 51298, 51366, 407, 437, 286, 643, 281, 360, 307, 733, 295, 4088, 51516, 51516, 341, 9452, 7316, 666, 257, 39148, 538, 257, 18350, 13, 51706, 51706, 400, 264, 18350, 307, 516, 281, 312, 257, 2445, 411, 51818], "temperature": 0.0, "avg_logprob": -0.16092292785644532, "compression_ratio": 1.6889952153110048, "no_speech_prob": 3.089179472226533e-06}, {"id": 1336, "seek": 644446, "start": 6464.5, "end": 6467.5, "text": " So what I need to do is kind of transform", "tokens": [50364, 1296, 584, 4018, 293, 472, 13, 50520, 50690, 400, 437, 286, 528, 322, 264, 5598, 307, 11, 286, 500, 380, 458, 11, 50902, 50994, 584, 257, 39148, 13, 51050, 51050, 467, 311, 733, 295, 6631, 281, 528, 257, 39148, 11, 51120, 51120, 457, 718, 311, 584, 286, 528, 257, 39148, 51186, 51186, 570, 286, 727, 6889, 490, 257, 39148, 3612, 13, 51298, 51366, 407, 437, 286, 643, 281, 360, 307, 733, 295, 4088, 51516, 51516, 341, 9452, 7316, 666, 257, 39148, 538, 257, 18350, 13, 51706, 51706, 400, 264, 18350, 307, 516, 281, 312, 257, 2445, 411, 51818], "temperature": 0.0, "avg_logprob": -0.16092292785644532, "compression_ratio": 1.6889952153110048, "no_speech_prob": 3.089179472226533e-06}, {"id": 1337, "seek": 644446, "start": 6467.5, "end": 6471.3, "text": " this uniform distribution into a Gaussian by a mapping.", "tokens": [50364, 1296, 584, 4018, 293, 472, 13, 50520, 50690, 400, 437, 286, 528, 322, 264, 5598, 307, 11, 286, 500, 380, 458, 11, 50902, 50994, 584, 257, 39148, 13, 51050, 51050, 467, 311, 733, 295, 6631, 281, 528, 257, 39148, 11, 51120, 51120, 457, 718, 311, 584, 286, 528, 257, 39148, 51186, 51186, 570, 286, 727, 6889, 490, 257, 39148, 3612, 13, 51298, 51366, 407, 437, 286, 643, 281, 360, 307, 733, 295, 4088, 51516, 51516, 341, 9452, 7316, 666, 257, 39148, 538, 257, 18350, 13, 51706, 51706, 400, 264, 18350, 307, 516, 281, 312, 257, 2445, 411, 51818], "temperature": 0.0, "avg_logprob": -0.16092292785644532, "compression_ratio": 1.6889952153110048, "no_speech_prob": 3.089179472226533e-06}, {"id": 1338, "seek": 647130, "start": 6471.3, "end": 6476.3, "text": " And the mapping is gonna be a function like,", "tokens": [50364, 400, 264, 18350, 307, 799, 312, 257, 2445, 411, 11, 50614, 50688, 309, 311, 799, 312, 257, 2445, 11, 1392, 11, 4018, 307, 510, 13, 50938, 51206, 9242, 295, 411, 341, 11, 498, 291, 528, 13, 51278, 51340, 1033, 13, 51382, 51490, 400, 341, 307, 264, 17340, 295, 264, 11573, 51688, 51688, 295, 264, 39148, 7316, 13, 51758, 51840], "temperature": 0.0, "avg_logprob": -0.2808127794109407, "compression_ratio": 1.434782608695652, "no_speech_prob": 4.381809617370891e-07}, {"id": 1339, "seek": 647130, "start": 6477.78, "end": 6482.78, "text": " it's gonna be a function, okay, zero is here.", "tokens": [50364, 400, 264, 18350, 307, 799, 312, 257, 2445, 411, 11, 50614, 50688, 309, 311, 799, 312, 257, 2445, 11, 1392, 11, 4018, 307, 510, 13, 50938, 51206, 9242, 295, 411, 341, 11, 498, 291, 528, 13, 51278, 51340, 1033, 13, 51382, 51490, 400, 341, 307, 264, 17340, 295, 264, 11573, 51688, 51688, 295, 264, 39148, 7316, 13, 51758, 51840], "temperature": 0.0, "avg_logprob": -0.2808127794109407, "compression_ratio": 1.434782608695652, "no_speech_prob": 4.381809617370891e-07}, {"id": 1340, "seek": 647130, "start": 6488.14, "end": 6489.58, "text": " Kind of like this, if you want.", "tokens": [50364, 400, 264, 18350, 307, 799, 312, 257, 2445, 411, 11, 50614, 50688, 309, 311, 799, 312, 257, 2445, 11, 1392, 11, 4018, 307, 510, 13, 50938, 51206, 9242, 295, 411, 341, 11, 498, 291, 528, 13, 51278, 51340, 1033, 13, 51382, 51490, 400, 341, 307, 264, 17340, 295, 264, 11573, 51688, 51688, 295, 264, 39148, 7316, 13, 51758, 51840], "temperature": 0.0, "avg_logprob": -0.2808127794109407, "compression_ratio": 1.434782608695652, "no_speech_prob": 4.381809617370891e-07}, {"id": 1341, "seek": 647130, "start": 6490.820000000001, "end": 6491.66, "text": " Okay.", "tokens": [50364, 400, 264, 18350, 307, 799, 312, 257, 2445, 411, 11, 50614, 50688, 309, 311, 799, 312, 257, 2445, 11, 1392, 11, 4018, 307, 510, 13, 50938, 51206, 9242, 295, 411, 341, 11, 498, 291, 528, 13, 51278, 51340, 1033, 13, 51382, 51490, 400, 341, 307, 264, 17340, 295, 264, 11573, 51688, 51688, 295, 264, 39148, 7316, 13, 51758, 51840], "temperature": 0.0, "avg_logprob": -0.2808127794109407, "compression_ratio": 1.434782608695652, "no_speech_prob": 4.381809617370891e-07}, {"id": 1342, "seek": 647130, "start": 6493.820000000001, "end": 6497.78, "text": " And this is the inverse of the integral", "tokens": [50364, 400, 264, 18350, 307, 799, 312, 257, 2445, 411, 11, 50614, 50688, 309, 311, 799, 312, 257, 2445, 11, 1392, 11, 4018, 307, 510, 13, 50938, 51206, 9242, 295, 411, 341, 11, 498, 291, 528, 13, 51278, 51340, 1033, 13, 51382, 51490, 400, 341, 307, 264, 17340, 295, 264, 11573, 51688, 51688, 295, 264, 39148, 7316, 13, 51758, 51840], "temperature": 0.0, "avg_logprob": -0.2808127794109407, "compression_ratio": 1.434782608695652, "no_speech_prob": 4.381809617370891e-07}, {"id": 1343, "seek": 647130, "start": 6497.78, "end": 6499.18, "text": " of the Gaussian distribution.", "tokens": [50364, 400, 264, 18350, 307, 799, 312, 257, 2445, 411, 11, 50614, 50688, 309, 311, 799, 312, 257, 2445, 11, 1392, 11, 4018, 307, 510, 13, 50938, 51206, 9242, 295, 411, 341, 11, 498, 291, 528, 13, 51278, 51340, 1033, 13, 51382, 51490, 400, 341, 307, 264, 17340, 295, 264, 11573, 51688, 51688, 295, 264, 39148, 7316, 13, 51758, 51840], "temperature": 0.0, "avg_logprob": -0.2808127794109407, "compression_ratio": 1.434782608695652, "no_speech_prob": 4.381809617370891e-07}, {"id": 1344, "seek": 649918, "start": 6499.18, "end": 6500.02, "text": " Okay.", "tokens": [50364, 1033, 13, 50406, 50406, 407, 498, 286, 747, 264, 13760, 295, 341, 2445, 11, 50606, 50606, 1392, 11, 370, 586, 718, 385, 733, 295, 2642, 341, 13, 50744, 50970, 467, 311, 257, 707, 2252, 281, 536, 11, 457, 498, 286, 4471, 11, 51184, 51276, 1392, 11, 264, 13760, 295, 341, 2445, 510, 51384, 51384, 486, 13330, 577, 709, 286, 5985, 257, 707, 2522, 510, 51578, 51578, 666, 257, 2522, 510, 11, 558, 30, 51668, 51668, 407, 264, 4833, 264, 13760, 11, 264, 544, 286, 5985, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.30411754184299045, "compression_ratio": 1.7401129943502824, "no_speech_prob": 4.157260264037177e-06}, {"id": 1345, "seek": 649918, "start": 6500.02, "end": 6504.02, "text": " So if I take the derivative of this function,", "tokens": [50364, 1033, 13, 50406, 50406, 407, 498, 286, 747, 264, 13760, 295, 341, 2445, 11, 50606, 50606, 1392, 11, 370, 586, 718, 385, 733, 295, 2642, 341, 13, 50744, 50970, 467, 311, 257, 707, 2252, 281, 536, 11, 457, 498, 286, 4471, 11, 51184, 51276, 1392, 11, 264, 13760, 295, 341, 2445, 510, 51384, 51384, 486, 13330, 577, 709, 286, 5985, 257, 707, 2522, 510, 51578, 51578, 666, 257, 2522, 510, 11, 558, 30, 51668, 51668, 407, 264, 4833, 264, 13760, 11, 264, 544, 286, 5985, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.30411754184299045, "compression_ratio": 1.7401129943502824, "no_speech_prob": 4.157260264037177e-06}, {"id": 1346, "seek": 649918, "start": 6504.02, "end": 6506.780000000001, "text": " okay, so now let me kind of draw this.", "tokens": [50364, 1033, 13, 50406, 50406, 407, 498, 286, 747, 264, 13760, 295, 341, 2445, 11, 50606, 50606, 1392, 11, 370, 586, 718, 385, 733, 295, 2642, 341, 13, 50744, 50970, 467, 311, 257, 707, 2252, 281, 536, 11, 457, 498, 286, 4471, 11, 51184, 51276, 1392, 11, 264, 13760, 295, 341, 2445, 510, 51384, 51384, 486, 13330, 577, 709, 286, 5985, 257, 707, 2522, 510, 51578, 51578, 666, 257, 2522, 510, 11, 558, 30, 51668, 51668, 407, 264, 4833, 264, 13760, 11, 264, 544, 286, 5985, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.30411754184299045, "compression_ratio": 1.7401129943502824, "no_speech_prob": 4.157260264037177e-06}, {"id": 1347, "seek": 649918, "start": 6511.3, "end": 6515.58, "text": " It's a little difficult to see, but if I map,", "tokens": [50364, 1033, 13, 50406, 50406, 407, 498, 286, 747, 264, 13760, 295, 341, 2445, 11, 50606, 50606, 1392, 11, 370, 586, 718, 385, 733, 295, 2642, 341, 13, 50744, 50970, 467, 311, 257, 707, 2252, 281, 536, 11, 457, 498, 286, 4471, 11, 51184, 51276, 1392, 11, 264, 13760, 295, 341, 2445, 510, 51384, 51384, 486, 13330, 577, 709, 286, 5985, 257, 707, 2522, 510, 51578, 51578, 666, 257, 2522, 510, 11, 558, 30, 51668, 51668, 407, 264, 4833, 264, 13760, 11, 264, 544, 286, 5985, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.30411754184299045, "compression_ratio": 1.7401129943502824, "no_speech_prob": 4.157260264037177e-06}, {"id": 1348, "seek": 649918, "start": 6517.42, "end": 6519.58, "text": " okay, the derivative of this function here", "tokens": [50364, 1033, 13, 50406, 50406, 407, 498, 286, 747, 264, 13760, 295, 341, 2445, 11, 50606, 50606, 1392, 11, 370, 586, 718, 385, 733, 295, 2642, 341, 13, 50744, 50970, 467, 311, 257, 707, 2252, 281, 536, 11, 457, 498, 286, 4471, 11, 51184, 51276, 1392, 11, 264, 13760, 295, 341, 2445, 510, 51384, 51384, 486, 13330, 577, 709, 286, 5985, 257, 707, 2522, 510, 51578, 51578, 666, 257, 2522, 510, 11, 558, 30, 51668, 51668, 407, 264, 4833, 264, 13760, 11, 264, 544, 286, 5985, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.30411754184299045, "compression_ratio": 1.7401129943502824, "no_speech_prob": 4.157260264037177e-06}, {"id": 1349, "seek": 649918, "start": 6519.58, "end": 6523.46, "text": " will indicate how much I stretch a little piece here", "tokens": [50364, 1033, 13, 50406, 50406, 407, 498, 286, 747, 264, 13760, 295, 341, 2445, 11, 50606, 50606, 1392, 11, 370, 586, 718, 385, 733, 295, 2642, 341, 13, 50744, 50970, 467, 311, 257, 707, 2252, 281, 536, 11, 457, 498, 286, 4471, 11, 51184, 51276, 1392, 11, 264, 13760, 295, 341, 2445, 510, 51384, 51384, 486, 13330, 577, 709, 286, 5985, 257, 707, 2522, 510, 51578, 51578, 666, 257, 2522, 510, 11, 558, 30, 51668, 51668, 407, 264, 4833, 264, 13760, 11, 264, 544, 286, 5985, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.30411754184299045, "compression_ratio": 1.7401129943502824, "no_speech_prob": 4.157260264037177e-06}, {"id": 1350, "seek": 649918, "start": 6523.46, "end": 6525.26, "text": " into a piece here, right?", "tokens": [50364, 1033, 13, 50406, 50406, 407, 498, 286, 747, 264, 13760, 295, 341, 2445, 11, 50606, 50606, 1392, 11, 370, 586, 718, 385, 733, 295, 2642, 341, 13, 50744, 50970, 467, 311, 257, 707, 2252, 281, 536, 11, 457, 498, 286, 4471, 11, 51184, 51276, 1392, 11, 264, 13760, 295, 341, 2445, 510, 51384, 51384, 486, 13330, 577, 709, 286, 5985, 257, 707, 2522, 510, 51578, 51578, 666, 257, 2522, 510, 11, 558, 30, 51668, 51668, 407, 264, 4833, 264, 13760, 11, 264, 544, 286, 5985, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.30411754184299045, "compression_ratio": 1.7401129943502824, "no_speech_prob": 4.157260264037177e-06}, {"id": 1351, "seek": 649918, "start": 6525.26, "end": 6528.34, "text": " So the larger the derivative, the more I stretch.", "tokens": [50364, 1033, 13, 50406, 50406, 407, 498, 286, 747, 264, 13760, 295, 341, 2445, 11, 50606, 50606, 1392, 11, 370, 586, 718, 385, 733, 295, 2642, 341, 13, 50744, 50970, 467, 311, 257, 707, 2252, 281, 536, 11, 457, 498, 286, 4471, 11, 51184, 51276, 1392, 11, 264, 13760, 295, 341, 2445, 510, 51384, 51384, 486, 13330, 577, 709, 286, 5985, 257, 707, 2522, 510, 51578, 51578, 666, 257, 2522, 510, 11, 558, 30, 51668, 51668, 407, 264, 4833, 264, 13760, 11, 264, 544, 286, 5985, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.30411754184299045, "compression_ratio": 1.7401129943502824, "no_speech_prob": 4.157260264037177e-06}, {"id": 1352, "seek": 652834, "start": 6528.34, "end": 6530.3, "text": " If the slope here is one,", "tokens": [50364, 759, 264, 13525, 510, 307, 472, 11, 50462, 50462, 550, 341, 2522, 295, 264, 7316, 510, 50588, 50588, 307, 406, 799, 312, 23563, 11, 50630, 50630, 309, 311, 799, 312, 733, 295, 4678, 44553, 13, 50794, 50794, 1033, 13, 50836, 50906, 400, 264, 4833, 264, 13525, 11, 51026, 51026, 264, 544, 286, 5985, 264, 7316, 11, 51136, 51136, 286, 5985, 257, 707, 2522, 510, 11, 51228, 51228, 293, 4412, 286, 733, 295, 20594, 439, 264, 10938, 51340, 51340, 300, 2100, 666, 341, 707, 4914, 510, 11, 51470, 51470, 286, 5985, 552, 670, 257, 2416, 4458, 11, 558, 30, 51720, 51800], "temperature": 0.0, "avg_logprob": -0.2860080120610256, "compression_ratio": 1.8636363636363635, "no_speech_prob": 3.6117719446338015e-06}, {"id": 1353, "seek": 652834, "start": 6530.3, "end": 6532.82, "text": " then this piece of the distribution here", "tokens": [50364, 759, 264, 13525, 510, 307, 472, 11, 50462, 50462, 550, 341, 2522, 295, 264, 7316, 510, 50588, 50588, 307, 406, 799, 312, 23563, 11, 50630, 50630, 309, 311, 799, 312, 733, 295, 4678, 44553, 13, 50794, 50794, 1033, 13, 50836, 50906, 400, 264, 4833, 264, 13525, 11, 51026, 51026, 264, 544, 286, 5985, 264, 7316, 11, 51136, 51136, 286, 5985, 257, 707, 2522, 510, 11, 51228, 51228, 293, 4412, 286, 733, 295, 20594, 439, 264, 10938, 51340, 51340, 300, 2100, 666, 341, 707, 4914, 510, 11, 51470, 51470, 286, 5985, 552, 670, 257, 2416, 4458, 11, 558, 30, 51720, 51800], "temperature": 0.0, "avg_logprob": -0.2860080120610256, "compression_ratio": 1.8636363636363635, "no_speech_prob": 3.6117719446338015e-06}, {"id": 1354, "seek": 652834, "start": 6532.82, "end": 6533.66, "text": " is not gonna be stretched,", "tokens": [50364, 759, 264, 13525, 510, 307, 472, 11, 50462, 50462, 550, 341, 2522, 295, 264, 7316, 510, 50588, 50588, 307, 406, 799, 312, 23563, 11, 50630, 50630, 309, 311, 799, 312, 733, 295, 4678, 44553, 13, 50794, 50794, 1033, 13, 50836, 50906, 400, 264, 4833, 264, 13525, 11, 51026, 51026, 264, 544, 286, 5985, 264, 7316, 11, 51136, 51136, 286, 5985, 257, 707, 2522, 510, 11, 51228, 51228, 293, 4412, 286, 733, 295, 20594, 439, 264, 10938, 51340, 51340, 300, 2100, 666, 341, 707, 4914, 510, 11, 51470, 51470, 286, 5985, 552, 670, 257, 2416, 4458, 11, 558, 30, 51720, 51800], "temperature": 0.0, "avg_logprob": -0.2860080120610256, "compression_ratio": 1.8636363636363635, "no_speech_prob": 3.6117719446338015e-06}, {"id": 1355, "seek": 652834, "start": 6533.66, "end": 6536.9400000000005, "text": " it's gonna be kind of passed unchanged.", "tokens": [50364, 759, 264, 13525, 510, 307, 472, 11, 50462, 50462, 550, 341, 2522, 295, 264, 7316, 510, 50588, 50588, 307, 406, 799, 312, 23563, 11, 50630, 50630, 309, 311, 799, 312, 733, 295, 4678, 44553, 13, 50794, 50794, 1033, 13, 50836, 50906, 400, 264, 4833, 264, 13525, 11, 51026, 51026, 264, 544, 286, 5985, 264, 7316, 11, 51136, 51136, 286, 5985, 257, 707, 2522, 510, 11, 51228, 51228, 293, 4412, 286, 733, 295, 20594, 439, 264, 10938, 51340, 51340, 300, 2100, 666, 341, 707, 4914, 510, 11, 51470, 51470, 286, 5985, 552, 670, 257, 2416, 4458, 11, 558, 30, 51720, 51800], "temperature": 0.0, "avg_logprob": -0.2860080120610256, "compression_ratio": 1.8636363636363635, "no_speech_prob": 3.6117719446338015e-06}, {"id": 1356, "seek": 652834, "start": 6536.9400000000005, "end": 6537.78, "text": " Okay.", "tokens": [50364, 759, 264, 13525, 510, 307, 472, 11, 50462, 50462, 550, 341, 2522, 295, 264, 7316, 510, 50588, 50588, 307, 406, 799, 312, 23563, 11, 50630, 50630, 309, 311, 799, 312, 733, 295, 4678, 44553, 13, 50794, 50794, 1033, 13, 50836, 50906, 400, 264, 4833, 264, 13525, 11, 51026, 51026, 264, 544, 286, 5985, 264, 7316, 11, 51136, 51136, 286, 5985, 257, 707, 2522, 510, 11, 51228, 51228, 293, 4412, 286, 733, 295, 20594, 439, 264, 10938, 51340, 51340, 300, 2100, 666, 341, 707, 4914, 510, 11, 51470, 51470, 286, 5985, 552, 670, 257, 2416, 4458, 11, 558, 30, 51720, 51800], "temperature": 0.0, "avg_logprob": -0.2860080120610256, "compression_ratio": 1.8636363636363635, "no_speech_prob": 3.6117719446338015e-06}, {"id": 1357, "seek": 652834, "start": 6539.18, "end": 6541.58, "text": " And the larger the slope,", "tokens": [50364, 759, 264, 13525, 510, 307, 472, 11, 50462, 50462, 550, 341, 2522, 295, 264, 7316, 510, 50588, 50588, 307, 406, 799, 312, 23563, 11, 50630, 50630, 309, 311, 799, 312, 733, 295, 4678, 44553, 13, 50794, 50794, 1033, 13, 50836, 50906, 400, 264, 4833, 264, 13525, 11, 51026, 51026, 264, 544, 286, 5985, 264, 7316, 11, 51136, 51136, 286, 5985, 257, 707, 2522, 510, 11, 51228, 51228, 293, 4412, 286, 733, 295, 20594, 439, 264, 10938, 51340, 51340, 300, 2100, 666, 341, 707, 4914, 510, 11, 51470, 51470, 286, 5985, 552, 670, 257, 2416, 4458, 11, 558, 30, 51720, 51800], "temperature": 0.0, "avg_logprob": -0.2860080120610256, "compression_ratio": 1.8636363636363635, "no_speech_prob": 3.6117719446338015e-06}, {"id": 1358, "seek": 652834, "start": 6541.58, "end": 6543.78, "text": " the more I stretch the distribution,", "tokens": [50364, 759, 264, 13525, 510, 307, 472, 11, 50462, 50462, 550, 341, 2522, 295, 264, 7316, 510, 50588, 50588, 307, 406, 799, 312, 23563, 11, 50630, 50630, 309, 311, 799, 312, 733, 295, 4678, 44553, 13, 50794, 50794, 1033, 13, 50836, 50906, 400, 264, 4833, 264, 13525, 11, 51026, 51026, 264, 544, 286, 5985, 264, 7316, 11, 51136, 51136, 286, 5985, 257, 707, 2522, 510, 11, 51228, 51228, 293, 4412, 286, 733, 295, 20594, 439, 264, 10938, 51340, 51340, 300, 2100, 666, 341, 707, 4914, 510, 11, 51470, 51470, 286, 5985, 552, 670, 257, 2416, 4458, 11, 558, 30, 51720, 51800], "temperature": 0.0, "avg_logprob": -0.2860080120610256, "compression_ratio": 1.8636363636363635, "no_speech_prob": 3.6117719446338015e-06}, {"id": 1359, "seek": 652834, "start": 6543.78, "end": 6545.62, "text": " I stretch a little piece here,", "tokens": [50364, 759, 264, 13525, 510, 307, 472, 11, 50462, 50462, 550, 341, 2522, 295, 264, 7316, 510, 50588, 50588, 307, 406, 799, 312, 23563, 11, 50630, 50630, 309, 311, 799, 312, 733, 295, 4678, 44553, 13, 50794, 50794, 1033, 13, 50836, 50906, 400, 264, 4833, 264, 13525, 11, 51026, 51026, 264, 544, 286, 5985, 264, 7316, 11, 51136, 51136, 286, 5985, 257, 707, 2522, 510, 11, 51228, 51228, 293, 4412, 286, 733, 295, 20594, 439, 264, 10938, 51340, 51340, 300, 2100, 666, 341, 707, 4914, 510, 11, 51470, 51470, 286, 5985, 552, 670, 257, 2416, 4458, 11, 558, 30, 51720, 51800], "temperature": 0.0, "avg_logprob": -0.2860080120610256, "compression_ratio": 1.8636363636363635, "no_speech_prob": 3.6117719446338015e-06}, {"id": 1360, "seek": 652834, "start": 6545.62, "end": 6547.860000000001, "text": " and therefore I kind of distribute all the samples", "tokens": [50364, 759, 264, 13525, 510, 307, 472, 11, 50462, 50462, 550, 341, 2522, 295, 264, 7316, 510, 50588, 50588, 307, 406, 799, 312, 23563, 11, 50630, 50630, 309, 311, 799, 312, 733, 295, 4678, 44553, 13, 50794, 50794, 1033, 13, 50836, 50906, 400, 264, 4833, 264, 13525, 11, 51026, 51026, 264, 544, 286, 5985, 264, 7316, 11, 51136, 51136, 286, 5985, 257, 707, 2522, 510, 11, 51228, 51228, 293, 4412, 286, 733, 295, 20594, 439, 264, 10938, 51340, 51340, 300, 2100, 666, 341, 707, 4914, 510, 11, 51470, 51470, 286, 5985, 552, 670, 257, 2416, 4458, 11, 558, 30, 51720, 51800], "temperature": 0.0, "avg_logprob": -0.2860080120610256, "compression_ratio": 1.8636363636363635, "no_speech_prob": 3.6117719446338015e-06}, {"id": 1361, "seek": 652834, "start": 6547.860000000001, "end": 6550.46, "text": " that fall into this little location here,", "tokens": [50364, 759, 264, 13525, 510, 307, 472, 11, 50462, 50462, 550, 341, 2522, 295, 264, 7316, 510, 50588, 50588, 307, 406, 799, 312, 23563, 11, 50630, 50630, 309, 311, 799, 312, 733, 295, 4678, 44553, 13, 50794, 50794, 1033, 13, 50836, 50906, 400, 264, 4833, 264, 13525, 11, 51026, 51026, 264, 544, 286, 5985, 264, 7316, 11, 51136, 51136, 286, 5985, 257, 707, 2522, 510, 11, 51228, 51228, 293, 4412, 286, 733, 295, 20594, 439, 264, 10938, 51340, 51340, 300, 2100, 666, 341, 707, 4914, 510, 11, 51470, 51470, 286, 5985, 552, 670, 257, 2416, 4458, 11, 558, 30, 51720, 51800], "temperature": 0.0, "avg_logprob": -0.2860080120610256, "compression_ratio": 1.8636363636363635, "no_speech_prob": 3.6117719446338015e-06}, {"id": 1362, "seek": 652834, "start": 6550.46, "end": 6555.46, "text": " I stretch them over a large region, right?", "tokens": [50364, 759, 264, 13525, 510, 307, 472, 11, 50462, 50462, 550, 341, 2522, 295, 264, 7316, 510, 50588, 50588, 307, 406, 799, 312, 23563, 11, 50630, 50630, 309, 311, 799, 312, 733, 295, 4678, 44553, 13, 50794, 50794, 1033, 13, 50836, 50906, 400, 264, 4833, 264, 13525, 11, 51026, 51026, 264, 544, 286, 5985, 264, 7316, 11, 51136, 51136, 286, 5985, 257, 707, 2522, 510, 11, 51228, 51228, 293, 4412, 286, 733, 295, 20594, 439, 264, 10938, 51340, 51340, 300, 2100, 666, 341, 707, 4914, 510, 11, 51470, 51470, 286, 5985, 552, 670, 257, 2416, 4458, 11, 558, 30, 51720, 51800], "temperature": 0.0, "avg_logprob": -0.2860080120610256, "compression_ratio": 1.8636363636363635, "no_speech_prob": 3.6117719446338015e-06}, {"id": 1363, "seek": 655546, "start": 6555.46, "end": 6558.34, "text": " And so what I need to do is design this function", "tokens": [50364, 400, 370, 437, 286, 643, 281, 360, 307, 1715, 341, 2445, 50508, 50508, 294, 1270, 257, 636, 300, 309, 29058, 452, 4846, 7316, 50686, 50686, 370, 300, 300, 7316, 483, 16894, 50814, 50814, 666, 264, 5598, 7316, 286, 528, 13, 50924, 50974, 1057, 558, 13, 51016, 51066, 407, 456, 307, 257, 8513, 300, 1619, 11, 51136, 51136, 370, 294, 4825, 12, 13595, 3378, 11, 51178, 51178, 309, 311, 257, 707, 544, 6179, 813, 341, 11, 51244, 51244, 457, 309, 1619, 300, 264, 7316, 291, 434, 799, 483, 51406, 51480, 322, 288, 307, 799, 312, 2681, 281, 264, 7316, 51628, 51628, 300, 291, 1409, 365, 294, 710, 11, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.31335542008683487, "compression_ratio": 1.7835497835497836, "no_speech_prob": 1.7880192899610847e-06}, {"id": 1364, "seek": 655546, "start": 6558.34, "end": 6561.9, "text": " in such a way that it stretches my input distribution", "tokens": [50364, 400, 370, 437, 286, 643, 281, 360, 307, 1715, 341, 2445, 50508, 50508, 294, 1270, 257, 636, 300, 309, 29058, 452, 4846, 7316, 50686, 50686, 370, 300, 300, 7316, 483, 16894, 50814, 50814, 666, 264, 5598, 7316, 286, 528, 13, 50924, 50974, 1057, 558, 13, 51016, 51066, 407, 456, 307, 257, 8513, 300, 1619, 11, 51136, 51136, 370, 294, 4825, 12, 13595, 3378, 11, 51178, 51178, 309, 311, 257, 707, 544, 6179, 813, 341, 11, 51244, 51244, 457, 309, 1619, 300, 264, 7316, 291, 434, 799, 483, 51406, 51480, 322, 288, 307, 799, 312, 2681, 281, 264, 7316, 51628, 51628, 300, 291, 1409, 365, 294, 710, 11, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.31335542008683487, "compression_ratio": 1.7835497835497836, "no_speech_prob": 1.7880192899610847e-06}, {"id": 1365, "seek": 655546, "start": 6561.9, "end": 6564.46, "text": " so that that distribution get transformed", "tokens": [50364, 400, 370, 437, 286, 643, 281, 360, 307, 1715, 341, 2445, 50508, 50508, 294, 1270, 257, 636, 300, 309, 29058, 452, 4846, 7316, 50686, 50686, 370, 300, 300, 7316, 483, 16894, 50814, 50814, 666, 264, 5598, 7316, 286, 528, 13, 50924, 50974, 1057, 558, 13, 51016, 51066, 407, 456, 307, 257, 8513, 300, 1619, 11, 51136, 51136, 370, 294, 4825, 12, 13595, 3378, 11, 51178, 51178, 309, 311, 257, 707, 544, 6179, 813, 341, 11, 51244, 51244, 457, 309, 1619, 300, 264, 7316, 291, 434, 799, 483, 51406, 51480, 322, 288, 307, 799, 312, 2681, 281, 264, 7316, 51628, 51628, 300, 291, 1409, 365, 294, 710, 11, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.31335542008683487, "compression_ratio": 1.7835497835497836, "no_speech_prob": 1.7880192899610847e-06}, {"id": 1366, "seek": 655546, "start": 6564.46, "end": 6566.66, "text": " into the output distribution I want.", "tokens": [50364, 400, 370, 437, 286, 643, 281, 360, 307, 1715, 341, 2445, 50508, 50508, 294, 1270, 257, 636, 300, 309, 29058, 452, 4846, 7316, 50686, 50686, 370, 300, 300, 7316, 483, 16894, 50814, 50814, 666, 264, 5598, 7316, 286, 528, 13, 50924, 50974, 1057, 558, 13, 51016, 51066, 407, 456, 307, 257, 8513, 300, 1619, 11, 51136, 51136, 370, 294, 4825, 12, 13595, 3378, 11, 51178, 51178, 309, 311, 257, 707, 544, 6179, 813, 341, 11, 51244, 51244, 457, 309, 1619, 300, 264, 7316, 291, 434, 799, 483, 51406, 51480, 322, 288, 307, 799, 312, 2681, 281, 264, 7316, 51628, 51628, 300, 291, 1409, 365, 294, 710, 11, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.31335542008683487, "compression_ratio": 1.7835497835497836, "no_speech_prob": 1.7880192899610847e-06}, {"id": 1367, "seek": 655546, "start": 6567.66, "end": 6568.5, "text": " All right.", "tokens": [50364, 400, 370, 437, 286, 643, 281, 360, 307, 1715, 341, 2445, 50508, 50508, 294, 1270, 257, 636, 300, 309, 29058, 452, 4846, 7316, 50686, 50686, 370, 300, 300, 7316, 483, 16894, 50814, 50814, 666, 264, 5598, 7316, 286, 528, 13, 50924, 50974, 1057, 558, 13, 51016, 51066, 407, 456, 307, 257, 8513, 300, 1619, 11, 51136, 51136, 370, 294, 4825, 12, 13595, 3378, 11, 51178, 51178, 309, 311, 257, 707, 544, 6179, 813, 341, 11, 51244, 51244, 457, 309, 1619, 300, 264, 7316, 291, 434, 799, 483, 51406, 51480, 322, 288, 307, 799, 312, 2681, 281, 264, 7316, 51628, 51628, 300, 291, 1409, 365, 294, 710, 11, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.31335542008683487, "compression_ratio": 1.7835497835497836, "no_speech_prob": 1.7880192899610847e-06}, {"id": 1368, "seek": 655546, "start": 6569.5, "end": 6570.9, "text": " So there is a formula that says,", "tokens": [50364, 400, 370, 437, 286, 643, 281, 360, 307, 1715, 341, 2445, 50508, 50508, 294, 1270, 257, 636, 300, 309, 29058, 452, 4846, 7316, 50686, 50686, 370, 300, 300, 7316, 483, 16894, 50814, 50814, 666, 264, 5598, 7316, 286, 528, 13, 50924, 50974, 1057, 558, 13, 51016, 51066, 407, 456, 307, 257, 8513, 300, 1619, 11, 51136, 51136, 370, 294, 4825, 12, 13595, 3378, 11, 51178, 51178, 309, 311, 257, 707, 544, 6179, 813, 341, 11, 51244, 51244, 457, 309, 1619, 300, 264, 7316, 291, 434, 799, 483, 51406, 51480, 322, 288, 307, 799, 312, 2681, 281, 264, 7316, 51628, 51628, 300, 291, 1409, 365, 294, 710, 11, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.31335542008683487, "compression_ratio": 1.7835497835497836, "no_speech_prob": 1.7880192899610847e-06}, {"id": 1369, "seek": 655546, "start": 6570.9, "end": 6571.74, "text": " so in multi-dimension,", "tokens": [50364, 400, 370, 437, 286, 643, 281, 360, 307, 1715, 341, 2445, 50508, 50508, 294, 1270, 257, 636, 300, 309, 29058, 452, 4846, 7316, 50686, 50686, 370, 300, 300, 7316, 483, 16894, 50814, 50814, 666, 264, 5598, 7316, 286, 528, 13, 50924, 50974, 1057, 558, 13, 51016, 51066, 407, 456, 307, 257, 8513, 300, 1619, 11, 51136, 51136, 370, 294, 4825, 12, 13595, 3378, 11, 51178, 51178, 309, 311, 257, 707, 544, 6179, 813, 341, 11, 51244, 51244, 457, 309, 1619, 300, 264, 7316, 291, 434, 799, 483, 51406, 51480, 322, 288, 307, 799, 312, 2681, 281, 264, 7316, 51628, 51628, 300, 291, 1409, 365, 294, 710, 11, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.31335542008683487, "compression_ratio": 1.7835497835497836, "no_speech_prob": 1.7880192899610847e-06}, {"id": 1370, "seek": 655546, "start": 6571.74, "end": 6573.06, "text": " it's a little more complicated than this,", "tokens": [50364, 400, 370, 437, 286, 643, 281, 360, 307, 1715, 341, 2445, 50508, 50508, 294, 1270, 257, 636, 300, 309, 29058, 452, 4846, 7316, 50686, 50686, 370, 300, 300, 7316, 483, 16894, 50814, 50814, 666, 264, 5598, 7316, 286, 528, 13, 50924, 50974, 1057, 558, 13, 51016, 51066, 407, 456, 307, 257, 8513, 300, 1619, 11, 51136, 51136, 370, 294, 4825, 12, 13595, 3378, 11, 51178, 51178, 309, 311, 257, 707, 544, 6179, 813, 341, 11, 51244, 51244, 457, 309, 1619, 300, 264, 7316, 291, 434, 799, 483, 51406, 51480, 322, 288, 307, 799, 312, 2681, 281, 264, 7316, 51628, 51628, 300, 291, 1409, 365, 294, 710, 11, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.31335542008683487, "compression_ratio": 1.7835497835497836, "no_speech_prob": 1.7880192899610847e-06}, {"id": 1371, "seek": 655546, "start": 6573.06, "end": 6576.3, "text": " but it says that the distribution you're gonna get", "tokens": [50364, 400, 370, 437, 286, 643, 281, 360, 307, 1715, 341, 2445, 50508, 50508, 294, 1270, 257, 636, 300, 309, 29058, 452, 4846, 7316, 50686, 50686, 370, 300, 300, 7316, 483, 16894, 50814, 50814, 666, 264, 5598, 7316, 286, 528, 13, 50924, 50974, 1057, 558, 13, 51016, 51066, 407, 456, 307, 257, 8513, 300, 1619, 11, 51136, 51136, 370, 294, 4825, 12, 13595, 3378, 11, 51178, 51178, 309, 311, 257, 707, 544, 6179, 813, 341, 11, 51244, 51244, 457, 309, 1619, 300, 264, 7316, 291, 434, 799, 483, 51406, 51480, 322, 288, 307, 799, 312, 2681, 281, 264, 7316, 51628, 51628, 300, 291, 1409, 365, 294, 710, 11, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.31335542008683487, "compression_ratio": 1.7835497835497836, "no_speech_prob": 1.7880192899610847e-06}, {"id": 1372, "seek": 655546, "start": 6577.78, "end": 6580.74, "text": " on y is gonna be equal to the distribution", "tokens": [50364, 400, 370, 437, 286, 643, 281, 360, 307, 1715, 341, 2445, 50508, 50508, 294, 1270, 257, 636, 300, 309, 29058, 452, 4846, 7316, 50686, 50686, 370, 300, 300, 7316, 483, 16894, 50814, 50814, 666, 264, 5598, 7316, 286, 528, 13, 50924, 50974, 1057, 558, 13, 51016, 51066, 407, 456, 307, 257, 8513, 300, 1619, 11, 51136, 51136, 370, 294, 4825, 12, 13595, 3378, 11, 51178, 51178, 309, 311, 257, 707, 544, 6179, 813, 341, 11, 51244, 51244, 457, 309, 1619, 300, 264, 7316, 291, 434, 799, 483, 51406, 51480, 322, 288, 307, 799, 312, 2681, 281, 264, 7316, 51628, 51628, 300, 291, 1409, 365, 294, 710, 11, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.31335542008683487, "compression_ratio": 1.7835497835497836, "no_speech_prob": 1.7880192899610847e-06}, {"id": 1373, "seek": 655546, "start": 6580.74, "end": 6582.62, "text": " that you started with in z,", "tokens": [50364, 400, 370, 437, 286, 643, 281, 360, 307, 1715, 341, 2445, 50508, 50508, 294, 1270, 257, 636, 300, 309, 29058, 452, 4846, 7316, 50686, 50686, 370, 300, 300, 7316, 483, 16894, 50814, 50814, 666, 264, 5598, 7316, 286, 528, 13, 50924, 50974, 1057, 558, 13, 51016, 51066, 407, 456, 307, 257, 8513, 300, 1619, 11, 51136, 51136, 370, 294, 4825, 12, 13595, 3378, 11, 51178, 51178, 309, 311, 257, 707, 544, 6179, 813, 341, 11, 51244, 51244, 457, 309, 1619, 300, 264, 7316, 291, 434, 799, 483, 51406, 51480, 322, 288, 307, 799, 312, 2681, 281, 264, 7316, 51628, 51628, 300, 291, 1409, 365, 294, 710, 11, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.31335542008683487, "compression_ratio": 1.7835497835497836, "no_speech_prob": 1.7880192899610847e-06}, {"id": 1374, "seek": 658262, "start": 6582.62, "end": 6586.26, "text": " multiplied by the inverse of the determinant", "tokens": [50364, 17207, 538, 264, 17340, 295, 264, 41296, 50546, 50546, 295, 264, 14117, 952, 295, 341, 283, 2445, 11, 50734, 50734, 370, 341, 307, 283, 13, 50776, 51236, 2829, 301, 472, 13, 51278, 51354, 407, 309, 311, 767, 264, 3380, 8513, 307, 341, 472, 13, 51576, 51714], "temperature": 0.0, "avg_logprob": -0.43339284261067706, "compression_ratio": 1.3565217391304347, "no_speech_prob": 8.012656508071814e-06}, {"id": 1375, "seek": 658262, "start": 6586.26, "end": 6590.0199999999995, "text": " of the Jacobian of this f function,", "tokens": [50364, 17207, 538, 264, 17340, 295, 264, 41296, 50546, 50546, 295, 264, 14117, 952, 295, 341, 283, 2445, 11, 50734, 50734, 370, 341, 307, 283, 13, 50776, 51236, 2829, 301, 472, 13, 51278, 51354, 407, 309, 311, 767, 264, 3380, 8513, 307, 341, 472, 13, 51576, 51714], "temperature": 0.0, "avg_logprob": -0.43339284261067706, "compression_ratio": 1.3565217391304347, "no_speech_prob": 8.012656508071814e-06}, {"id": 1376, "seek": 658262, "start": 6590.0199999999995, "end": 6590.86, "text": " so this is f.", "tokens": [50364, 17207, 538, 264, 17340, 295, 264, 41296, 50546, 50546, 295, 264, 14117, 952, 295, 341, 283, 2445, 11, 50734, 50734, 370, 341, 307, 283, 13, 50776, 51236, 2829, 301, 472, 13, 51278, 51354, 407, 309, 311, 767, 264, 3380, 8513, 307, 341, 472, 13, 51576, 51714], "temperature": 0.0, "avg_logprob": -0.43339284261067706, "compression_ratio": 1.3565217391304347, "no_speech_prob": 8.012656508071814e-06}, {"id": 1377, "seek": 658262, "start": 6600.0599999999995, "end": 6600.9, "text": " Minus one.", "tokens": [50364, 17207, 538, 264, 17340, 295, 264, 41296, 50546, 50546, 295, 264, 14117, 952, 295, 341, 283, 2445, 11, 50734, 50734, 370, 341, 307, 283, 13, 50776, 51236, 2829, 301, 472, 13, 51278, 51354, 407, 309, 311, 767, 264, 3380, 8513, 307, 341, 472, 13, 51576, 51714], "temperature": 0.0, "avg_logprob": -0.43339284261067706, "compression_ratio": 1.3565217391304347, "no_speech_prob": 8.012656508071814e-06}, {"id": 1378, "seek": 658262, "start": 6602.42, "end": 6606.86, "text": " So it's actually the original formula is this one.", "tokens": [50364, 17207, 538, 264, 17340, 295, 264, 41296, 50546, 50546, 295, 264, 14117, 952, 295, 341, 283, 2445, 11, 50734, 50734, 370, 341, 307, 283, 13, 50776, 51236, 2829, 301, 472, 13, 51278, 51354, 407, 309, 311, 767, 264, 3380, 8513, 307, 341, 472, 13, 51576, 51714], "temperature": 0.0, "avg_logprob": -0.43339284261067706, "compression_ratio": 1.3565217391304347, "no_speech_prob": 8.012656508071814e-06}, {"id": 1379, "seek": 660686, "start": 6606.86, "end": 6611.86, "text": " But those two things are equal.", "tokens": [50364, 583, 729, 732, 721, 366, 2681, 13, 50614, 50614, 1033, 13, 50670, 50670, 407, 498, 291, 747, 11, 50712, 50712, 370, 341, 307, 337, 257, 4825, 12, 18759, 8062, 2445, 11, 558, 30, 50814, 50814, 407, 309, 575, 257, 14117, 952, 281, 4471, 710, 281, 288, 13, 51004, 51052, 400, 370, 498, 291, 747, 264, 41296, 51254, 51254, 295, 264, 17340, 14117, 952, 295, 300, 2445, 11, 51454, 51538, 597, 307, 257, 39684, 2158, 11, 51616, 51616, 16203, 538, 577, 709, 264, 7316, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.4445264092807112, "compression_ratio": 1.5052083333333333, "no_speech_prob": 2.4060143459792016e-06}, {"id": 1380, "seek": 660686, "start": 6611.86, "end": 6612.98, "text": " Okay.", "tokens": [50364, 583, 729, 732, 721, 366, 2681, 13, 50614, 50614, 1033, 13, 50670, 50670, 407, 498, 291, 747, 11, 50712, 50712, 370, 341, 307, 337, 257, 4825, 12, 18759, 8062, 2445, 11, 558, 30, 50814, 50814, 407, 309, 575, 257, 14117, 952, 281, 4471, 710, 281, 288, 13, 51004, 51052, 400, 370, 498, 291, 747, 264, 41296, 51254, 51254, 295, 264, 17340, 14117, 952, 295, 300, 2445, 11, 51454, 51538, 597, 307, 257, 39684, 2158, 11, 51616, 51616, 16203, 538, 577, 709, 264, 7316, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.4445264092807112, "compression_ratio": 1.5052083333333333, "no_speech_prob": 2.4060143459792016e-06}, {"id": 1381, "seek": 660686, "start": 6612.98, "end": 6613.82, "text": " So if you take,", "tokens": [50364, 583, 729, 732, 721, 366, 2681, 13, 50614, 50614, 1033, 13, 50670, 50670, 407, 498, 291, 747, 11, 50712, 50712, 370, 341, 307, 337, 257, 4825, 12, 18759, 8062, 2445, 11, 558, 30, 50814, 50814, 407, 309, 575, 257, 14117, 952, 281, 4471, 710, 281, 288, 13, 51004, 51052, 400, 370, 498, 291, 747, 264, 41296, 51254, 51254, 295, 264, 17340, 14117, 952, 295, 300, 2445, 11, 51454, 51538, 597, 307, 257, 39684, 2158, 11, 51616, 51616, 16203, 538, 577, 709, 264, 7316, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.4445264092807112, "compression_ratio": 1.5052083333333333, "no_speech_prob": 2.4060143459792016e-06}, {"id": 1382, "seek": 660686, "start": 6613.82, "end": 6615.86, "text": " so this is for a multi-dimensional vector function, right?", "tokens": [50364, 583, 729, 732, 721, 366, 2681, 13, 50614, 50614, 1033, 13, 50670, 50670, 407, 498, 291, 747, 11, 50712, 50712, 370, 341, 307, 337, 257, 4825, 12, 18759, 8062, 2445, 11, 558, 30, 50814, 50814, 407, 309, 575, 257, 14117, 952, 281, 4471, 710, 281, 288, 13, 51004, 51052, 400, 370, 498, 291, 747, 264, 41296, 51254, 51254, 295, 264, 17340, 14117, 952, 295, 300, 2445, 11, 51454, 51538, 597, 307, 257, 39684, 2158, 11, 51616, 51616, 16203, 538, 577, 709, 264, 7316, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.4445264092807112, "compression_ratio": 1.5052083333333333, "no_speech_prob": 2.4060143459792016e-06}, {"id": 1383, "seek": 660686, "start": 6615.86, "end": 6619.66, "text": " So it has a Jacobian to map z to y.", "tokens": [50364, 583, 729, 732, 721, 366, 2681, 13, 50614, 50614, 1033, 13, 50670, 50670, 407, 498, 291, 747, 11, 50712, 50712, 370, 341, 307, 337, 257, 4825, 12, 18759, 8062, 2445, 11, 558, 30, 50814, 50814, 407, 309, 575, 257, 14117, 952, 281, 4471, 710, 281, 288, 13, 51004, 51052, 400, 370, 498, 291, 747, 264, 41296, 51254, 51254, 295, 264, 17340, 14117, 952, 295, 300, 2445, 11, 51454, 51538, 597, 307, 257, 39684, 2158, 11, 51616, 51616, 16203, 538, 577, 709, 264, 7316, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.4445264092807112, "compression_ratio": 1.5052083333333333, "no_speech_prob": 2.4060143459792016e-06}, {"id": 1384, "seek": 660686, "start": 6620.62, "end": 6624.66, "text": " And so if you take the determinant", "tokens": [50364, 583, 729, 732, 721, 366, 2681, 13, 50614, 50614, 1033, 13, 50670, 50670, 407, 498, 291, 747, 11, 50712, 50712, 370, 341, 307, 337, 257, 4825, 12, 18759, 8062, 2445, 11, 558, 30, 50814, 50814, 407, 309, 575, 257, 14117, 952, 281, 4471, 710, 281, 288, 13, 51004, 51052, 400, 370, 498, 291, 747, 264, 41296, 51254, 51254, 295, 264, 17340, 14117, 952, 295, 300, 2445, 11, 51454, 51538, 597, 307, 257, 39684, 2158, 11, 51616, 51616, 16203, 538, 577, 709, 264, 7316, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.4445264092807112, "compression_ratio": 1.5052083333333333, "no_speech_prob": 2.4060143459792016e-06}, {"id": 1385, "seek": 660686, "start": 6624.66, "end": 6628.66, "text": " of the inverse Jacobian of that function,", "tokens": [50364, 583, 729, 732, 721, 366, 2681, 13, 50614, 50614, 1033, 13, 50670, 50670, 407, 498, 291, 747, 11, 50712, 50712, 370, 341, 307, 337, 257, 4825, 12, 18759, 8062, 2445, 11, 558, 30, 50814, 50814, 407, 309, 575, 257, 14117, 952, 281, 4471, 710, 281, 288, 13, 51004, 51052, 400, 370, 498, 291, 747, 264, 41296, 51254, 51254, 295, 264, 17340, 14117, 952, 295, 300, 2445, 11, 51454, 51538, 597, 307, 257, 39684, 2158, 11, 51616, 51616, 16203, 538, 577, 709, 264, 7316, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.4445264092807112, "compression_ratio": 1.5052083333333333, "no_speech_prob": 2.4060143459792016e-06}, {"id": 1386, "seek": 660686, "start": 6630.339999999999, "end": 6631.9, "text": " which is a scalar value,", "tokens": [50364, 583, 729, 732, 721, 366, 2681, 13, 50614, 50614, 1033, 13, 50670, 50670, 407, 498, 291, 747, 11, 50712, 50712, 370, 341, 307, 337, 257, 4825, 12, 18759, 8062, 2445, 11, 558, 30, 50814, 50814, 407, 309, 575, 257, 14117, 952, 281, 4471, 710, 281, 288, 13, 51004, 51052, 400, 370, 498, 291, 747, 264, 41296, 51254, 51254, 295, 264, 17340, 14117, 952, 295, 300, 2445, 11, 51454, 51538, 597, 307, 257, 39684, 2158, 11, 51616, 51616, 16203, 538, 577, 709, 264, 7316, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.4445264092807112, "compression_ratio": 1.5052083333333333, "no_speech_prob": 2.4060143459792016e-06}, {"id": 1387, "seek": 660686, "start": 6631.9, "end": 6635.62, "text": " indicates by how much the distribution", "tokens": [50364, 583, 729, 732, 721, 366, 2681, 13, 50614, 50614, 1033, 13, 50670, 50670, 407, 498, 291, 747, 11, 50712, 50712, 370, 341, 307, 337, 257, 4825, 12, 18759, 8062, 2445, 11, 558, 30, 50814, 50814, 407, 309, 575, 257, 14117, 952, 281, 4471, 710, 281, 288, 13, 51004, 51052, 400, 370, 498, 291, 747, 264, 41296, 51254, 51254, 295, 264, 17340, 14117, 952, 295, 300, 2445, 11, 51454, 51538, 597, 307, 257, 39684, 2158, 11, 51616, 51616, 16203, 538, 577, 709, 264, 7316, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.4445264092807112, "compression_ratio": 1.5052083333333333, "no_speech_prob": 2.4060143459792016e-06}, {"id": 1388, "seek": 663562, "start": 6635.62, "end": 6640.62, "text": " gets stretched or compressed in that case at q.", "tokens": [50364, 2170, 23563, 420, 30353, 294, 300, 1389, 412, 9505, 13, 50614, 50750, 407, 294, 300, 1389, 510, 307, 264, 19355, 8509, 13, 50866, 50866, 467, 311, 264, 17340, 295, 264, 13760, 11, 50932, 50932, 370, 309, 311, 264, 19355, 11, 558, 30, 51080, 51080, 400, 370, 264, 544, 291, 14778, 510, 11, 51212, 51316, 264, 544, 264, 8482, 486, 312, 1090, 11, 51466, 51466, 544, 280, 295, 288, 486, 312, 2416, 13, 51588, 51588, 440, 10305, 280, 295, 288, 337, 341, 288, 486, 312, 2416, 51704, 51704, 337, 257, 2212, 288, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.3813149929046631, "compression_ratio": 1.7692307692307692, "no_speech_prob": 8.013286787900142e-06}, {"id": 1389, "seek": 663562, "start": 6643.34, "end": 6645.66, "text": " So in that case here is the compression ratio.", "tokens": [50364, 2170, 23563, 420, 30353, 294, 300, 1389, 412, 9505, 13, 50614, 50750, 407, 294, 300, 1389, 510, 307, 264, 19355, 8509, 13, 50866, 50866, 467, 311, 264, 17340, 295, 264, 13760, 11, 50932, 50932, 370, 309, 311, 264, 19355, 11, 558, 30, 51080, 51080, 400, 370, 264, 544, 291, 14778, 510, 11, 51212, 51316, 264, 544, 264, 8482, 486, 312, 1090, 11, 51466, 51466, 544, 280, 295, 288, 486, 312, 2416, 13, 51588, 51588, 440, 10305, 280, 295, 288, 337, 341, 288, 486, 312, 2416, 51704, 51704, 337, 257, 2212, 288, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.3813149929046631, "compression_ratio": 1.7692307692307692, "no_speech_prob": 8.013286787900142e-06}, {"id": 1390, "seek": 663562, "start": 6645.66, "end": 6646.98, "text": " It's the inverse of the derivative,", "tokens": [50364, 2170, 23563, 420, 30353, 294, 300, 1389, 412, 9505, 13, 50614, 50750, 407, 294, 300, 1389, 510, 307, 264, 19355, 8509, 13, 50866, 50866, 467, 311, 264, 17340, 295, 264, 13760, 11, 50932, 50932, 370, 309, 311, 264, 19355, 11, 558, 30, 51080, 51080, 400, 370, 264, 544, 291, 14778, 510, 11, 51212, 51316, 264, 544, 264, 8482, 486, 312, 1090, 11, 51466, 51466, 544, 280, 295, 288, 486, 312, 2416, 13, 51588, 51588, 440, 10305, 280, 295, 288, 337, 341, 288, 486, 312, 2416, 51704, 51704, 337, 257, 2212, 288, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.3813149929046631, "compression_ratio": 1.7692307692307692, "no_speech_prob": 8.013286787900142e-06}, {"id": 1391, "seek": 663562, "start": 6646.98, "end": 6649.94, "text": " so it's the compression, right?", "tokens": [50364, 2170, 23563, 420, 30353, 294, 300, 1389, 412, 9505, 13, 50614, 50750, 407, 294, 300, 1389, 510, 307, 264, 19355, 8509, 13, 50866, 50866, 467, 311, 264, 17340, 295, 264, 13760, 11, 50932, 50932, 370, 309, 311, 264, 19355, 11, 558, 30, 51080, 51080, 400, 370, 264, 544, 291, 14778, 510, 11, 51212, 51316, 264, 544, 264, 8482, 486, 312, 1090, 11, 51466, 51466, 544, 280, 295, 288, 486, 312, 2416, 13, 51588, 51588, 440, 10305, 280, 295, 288, 337, 341, 288, 486, 312, 2416, 51704, 51704, 337, 257, 2212, 288, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.3813149929046631, "compression_ratio": 1.7692307692307692, "no_speech_prob": 8.013286787900142e-06}, {"id": 1392, "seek": 663562, "start": 6649.94, "end": 6652.58, "text": " And so the more you compress here,", "tokens": [50364, 2170, 23563, 420, 30353, 294, 300, 1389, 412, 9505, 13, 50614, 50750, 407, 294, 300, 1389, 510, 307, 264, 19355, 8509, 13, 50866, 50866, 467, 311, 264, 17340, 295, 264, 13760, 11, 50932, 50932, 370, 309, 311, 264, 19355, 11, 558, 30, 51080, 51080, 400, 370, 264, 544, 291, 14778, 510, 11, 51212, 51316, 264, 544, 264, 8482, 486, 312, 1090, 11, 51466, 51466, 544, 280, 295, 288, 486, 312, 2416, 13, 51588, 51588, 440, 10305, 280, 295, 288, 337, 341, 288, 486, 312, 2416, 51704, 51704, 337, 257, 2212, 288, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.3813149929046631, "compression_ratio": 1.7692307692307692, "no_speech_prob": 8.013286787900142e-06}, {"id": 1393, "seek": 663562, "start": 6654.66, "end": 6657.66, "text": " the more the probability will be high,", "tokens": [50364, 2170, 23563, 420, 30353, 294, 300, 1389, 412, 9505, 13, 50614, 50750, 407, 294, 300, 1389, 510, 307, 264, 19355, 8509, 13, 50866, 50866, 467, 311, 264, 17340, 295, 264, 13760, 11, 50932, 50932, 370, 309, 311, 264, 19355, 11, 558, 30, 51080, 51080, 400, 370, 264, 544, 291, 14778, 510, 11, 51212, 51316, 264, 544, 264, 8482, 486, 312, 1090, 11, 51466, 51466, 544, 280, 295, 288, 486, 312, 2416, 13, 51588, 51588, 440, 10305, 280, 295, 288, 337, 341, 288, 486, 312, 2416, 51704, 51704, 337, 257, 2212, 288, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.3813149929046631, "compression_ratio": 1.7692307692307692, "no_speech_prob": 8.013286787900142e-06}, {"id": 1394, "seek": 663562, "start": 6657.66, "end": 6660.099999999999, "text": " more p of y will be large.", "tokens": [50364, 2170, 23563, 420, 30353, 294, 300, 1389, 412, 9505, 13, 50614, 50750, 407, 294, 300, 1389, 510, 307, 264, 19355, 8509, 13, 50866, 50866, 467, 311, 264, 17340, 295, 264, 13760, 11, 50932, 50932, 370, 309, 311, 264, 19355, 11, 558, 30, 51080, 51080, 400, 370, 264, 544, 291, 14778, 510, 11, 51212, 51316, 264, 544, 264, 8482, 486, 312, 1090, 11, 51466, 51466, 544, 280, 295, 288, 486, 312, 2416, 13, 51588, 51588, 440, 10305, 280, 295, 288, 337, 341, 288, 486, 312, 2416, 51704, 51704, 337, 257, 2212, 288, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.3813149929046631, "compression_ratio": 1.7692307692307692, "no_speech_prob": 8.013286787900142e-06}, {"id": 1395, "seek": 663562, "start": 6660.099999999999, "end": 6662.42, "text": " The density p of y for this y will be large", "tokens": [50364, 2170, 23563, 420, 30353, 294, 300, 1389, 412, 9505, 13, 50614, 50750, 407, 294, 300, 1389, 510, 307, 264, 19355, 8509, 13, 50866, 50866, 467, 311, 264, 17340, 295, 264, 13760, 11, 50932, 50932, 370, 309, 311, 264, 19355, 11, 558, 30, 51080, 51080, 400, 370, 264, 544, 291, 14778, 510, 11, 51212, 51316, 264, 544, 264, 8482, 486, 312, 1090, 11, 51466, 51466, 544, 280, 295, 288, 486, 312, 2416, 13, 51588, 51588, 440, 10305, 280, 295, 288, 337, 341, 288, 486, 312, 2416, 51704, 51704, 337, 257, 2212, 288, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.3813149929046631, "compression_ratio": 1.7692307692307692, "no_speech_prob": 8.013286787900142e-06}, {"id": 1396, "seek": 663562, "start": 6662.42, "end": 6664.54, "text": " for a given y.", "tokens": [50364, 2170, 23563, 420, 30353, 294, 300, 1389, 412, 9505, 13, 50614, 50750, 407, 294, 300, 1389, 510, 307, 264, 19355, 8509, 13, 50866, 50866, 467, 311, 264, 17340, 295, 264, 13760, 11, 50932, 50932, 370, 309, 311, 264, 19355, 11, 558, 30, 51080, 51080, 400, 370, 264, 544, 291, 14778, 510, 11, 51212, 51316, 264, 544, 264, 8482, 486, 312, 1090, 11, 51466, 51466, 544, 280, 295, 288, 486, 312, 2416, 13, 51588, 51588, 440, 10305, 280, 295, 288, 337, 341, 288, 486, 312, 2416, 51704, 51704, 337, 257, 2212, 288, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.3813149929046631, "compression_ratio": 1.7692307692307692, "no_speech_prob": 8.013286787900142e-06}, {"id": 1397, "seek": 666454, "start": 6664.54, "end": 6667.7, "text": " For a given q.", "tokens": [50364, 1171, 257, 2212, 9505, 13, 50522, 50522, 407, 341, 307, 337, 288, 6915, 283, 295, 710, 13, 50772, 50936, 1033, 13, 50978, 51062, 407, 264, 955, 1168, 295, 2710, 3319, 3095, 7150, 51312, 51422, 307, 577, 291, 360, 341, 11, 558, 30, 51588, 51588, 18600, 257, 1230, 295, 10938, 295, 280, 295, 288, 11, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.13263022488561169, "compression_ratio": 1.2913385826771653, "no_speech_prob": 1.3287400406625238e-06}, {"id": 1398, "seek": 666454, "start": 6667.7, "end": 6672.7, "text": " So this is for y equals f of z.", "tokens": [50364, 1171, 257, 2212, 9505, 13, 50522, 50522, 407, 341, 307, 337, 288, 6915, 283, 295, 710, 13, 50772, 50936, 1033, 13, 50978, 51062, 407, 264, 955, 1168, 295, 2710, 3319, 3095, 7150, 51312, 51422, 307, 577, 291, 360, 341, 11, 558, 30, 51588, 51588, 18600, 257, 1230, 295, 10938, 295, 280, 295, 288, 11, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.13263022488561169, "compression_ratio": 1.2913385826771653, "no_speech_prob": 1.3287400406625238e-06}, {"id": 1399, "seek": 666454, "start": 6675.98, "end": 6676.82, "text": " Okay.", "tokens": [50364, 1171, 257, 2212, 9505, 13, 50522, 50522, 407, 341, 307, 337, 288, 6915, 283, 295, 710, 13, 50772, 50936, 1033, 13, 50978, 51062, 407, 264, 955, 1168, 295, 2710, 3319, 3095, 7150, 51312, 51422, 307, 577, 291, 360, 341, 11, 558, 30, 51588, 51588, 18600, 257, 1230, 295, 10938, 295, 280, 295, 288, 11, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.13263022488561169, "compression_ratio": 1.2913385826771653, "no_speech_prob": 1.3287400406625238e-06}, {"id": 1400, "seek": 666454, "start": 6678.5, "end": 6683.5, "text": " So the big question of normalizing flow methods", "tokens": [50364, 1171, 257, 2212, 9505, 13, 50522, 50522, 407, 341, 307, 337, 288, 6915, 283, 295, 710, 13, 50772, 50936, 1033, 13, 50978, 51062, 407, 264, 955, 1168, 295, 2710, 3319, 3095, 7150, 51312, 51422, 307, 577, 291, 360, 341, 11, 558, 30, 51588, 51588, 18600, 257, 1230, 295, 10938, 295, 280, 295, 288, 11, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.13263022488561169, "compression_ratio": 1.2913385826771653, "no_speech_prob": 1.3287400406625238e-06}, {"id": 1401, "seek": 666454, "start": 6685.7, "end": 6689.0199999999995, "text": " is how you do this, right?", "tokens": [50364, 1171, 257, 2212, 9505, 13, 50522, 50522, 407, 341, 307, 337, 288, 6915, 283, 295, 710, 13, 50772, 50936, 1033, 13, 50978, 51062, 407, 264, 955, 1168, 295, 2710, 3319, 3095, 7150, 51312, 51422, 307, 577, 291, 360, 341, 11, 558, 30, 51588, 51588, 18600, 257, 1230, 295, 10938, 295, 280, 295, 288, 11, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.13263022488561169, "compression_ratio": 1.2913385826771653, "no_speech_prob": 1.3287400406625238e-06}, {"id": 1402, "seek": 666454, "start": 6689.0199999999995, "end": 6691.86, "text": " Given a number of samples of p of y,", "tokens": [50364, 1171, 257, 2212, 9505, 13, 50522, 50522, 407, 341, 307, 337, 288, 6915, 283, 295, 710, 13, 50772, 50936, 1033, 13, 50978, 51062, 407, 264, 955, 1168, 295, 2710, 3319, 3095, 7150, 51312, 51422, 307, 577, 291, 360, 341, 11, 558, 30, 51588, 51588, 18600, 257, 1230, 295, 10938, 295, 280, 295, 288, 11, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.13263022488561169, "compression_ratio": 1.2913385826771653, "no_speech_prob": 1.3287400406625238e-06}, {"id": 1403, "seek": 669186, "start": 6691.86, "end": 6695.74, "text": " and given that you sample your distribution q,", "tokens": [50364, 293, 2212, 300, 291, 6889, 428, 7316, 9505, 11, 50558, 50558, 291, 362, 428, 7316, 9505, 11, 291, 6889, 490, 309, 11, 50692, 50762, 577, 360, 291, 733, 295, 17522, 364, 10024, 2445, 50888, 50888, 300, 5276, 300, 264, 280, 291, 483, 412, 264, 5598, 51138, 51158, 307, 2681, 281, 264, 9505, 291, 829, 412, 264, 4846, 51270, 51270, 17207, 538, 341, 1333, 295, 17340, 41296, 51410, 51410, 295, 264, 14117, 952, 295, 264, 283, 2445, 13, 51520, 51520, 708, 291, 362, 281, 915, 307, 264, 283, 2445, 13, 51602, 51602, 407, 291, 1936, 362, 281, 23203, 13, 51690, 51690, 407, 1936, 291, 14722, 264, 4560, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.13152232603593306, "compression_ratio": 1.8864628820960698, "no_speech_prob": 3.340438979648752e-06}, {"id": 1404, "seek": 669186, "start": 6695.74, "end": 6698.42, "text": " you have your distribution q, you sample from it,", "tokens": [50364, 293, 2212, 300, 291, 6889, 428, 7316, 9505, 11, 50558, 50558, 291, 362, 428, 7316, 9505, 11, 291, 6889, 490, 309, 11, 50692, 50762, 577, 360, 291, 733, 295, 17522, 364, 10024, 2445, 50888, 50888, 300, 5276, 300, 264, 280, 291, 483, 412, 264, 5598, 51138, 51158, 307, 2681, 281, 264, 9505, 291, 829, 412, 264, 4846, 51270, 51270, 17207, 538, 341, 1333, 295, 17340, 41296, 51410, 51410, 295, 264, 14117, 952, 295, 264, 283, 2445, 13, 51520, 51520, 708, 291, 362, 281, 915, 307, 264, 283, 2445, 13, 51602, 51602, 407, 291, 1936, 362, 281, 23203, 13, 51690, 51690, 407, 1936, 291, 14722, 264, 4560, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.13152232603593306, "compression_ratio": 1.8864628820960698, "no_speech_prob": 3.340438979648752e-06}, {"id": 1405, "seek": 669186, "start": 6699.82, "end": 6702.339999999999, "text": " how do you kind of minimize an objective function", "tokens": [50364, 293, 2212, 300, 291, 6889, 428, 7316, 9505, 11, 50558, 50558, 291, 362, 428, 7316, 9505, 11, 291, 6889, 490, 309, 11, 50692, 50762, 577, 360, 291, 733, 295, 17522, 364, 10024, 2445, 50888, 50888, 300, 5276, 300, 264, 280, 291, 483, 412, 264, 5598, 51138, 51158, 307, 2681, 281, 264, 9505, 291, 829, 412, 264, 4846, 51270, 51270, 17207, 538, 341, 1333, 295, 17340, 41296, 51410, 51410, 295, 264, 14117, 952, 295, 264, 283, 2445, 13, 51520, 51520, 708, 291, 362, 281, 915, 307, 264, 283, 2445, 13, 51602, 51602, 407, 291, 1936, 362, 281, 23203, 13, 51690, 51690, 407, 1936, 291, 14722, 264, 4560, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.13152232603593306, "compression_ratio": 1.8864628820960698, "no_speech_prob": 3.340438979648752e-06}, {"id": 1406, "seek": 669186, "start": 6702.339999999999, "end": 6707.339999999999, "text": " that knowing that the p you get at the output", "tokens": [50364, 293, 2212, 300, 291, 6889, 428, 7316, 9505, 11, 50558, 50558, 291, 362, 428, 7316, 9505, 11, 291, 6889, 490, 309, 11, 50692, 50762, 577, 360, 291, 733, 295, 17522, 364, 10024, 2445, 50888, 50888, 300, 5276, 300, 264, 280, 291, 483, 412, 264, 5598, 51138, 51158, 307, 2681, 281, 264, 9505, 291, 829, 412, 264, 4846, 51270, 51270, 17207, 538, 341, 1333, 295, 17340, 41296, 51410, 51410, 295, 264, 14117, 952, 295, 264, 283, 2445, 13, 51520, 51520, 708, 291, 362, 281, 915, 307, 264, 283, 2445, 13, 51602, 51602, 407, 291, 1936, 362, 281, 23203, 13, 51690, 51690, 407, 1936, 291, 14722, 264, 4560, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.13152232603593306, "compression_ratio": 1.8864628820960698, "no_speech_prob": 3.340438979648752e-06}, {"id": 1407, "seek": 669186, "start": 6707.74, "end": 6709.98, "text": " is equal to the q you put at the input", "tokens": [50364, 293, 2212, 300, 291, 6889, 428, 7316, 9505, 11, 50558, 50558, 291, 362, 428, 7316, 9505, 11, 291, 6889, 490, 309, 11, 50692, 50762, 577, 360, 291, 733, 295, 17522, 364, 10024, 2445, 50888, 50888, 300, 5276, 300, 264, 280, 291, 483, 412, 264, 5598, 51138, 51158, 307, 2681, 281, 264, 9505, 291, 829, 412, 264, 4846, 51270, 51270, 17207, 538, 341, 1333, 295, 17340, 41296, 51410, 51410, 295, 264, 14117, 952, 295, 264, 283, 2445, 13, 51520, 51520, 708, 291, 362, 281, 915, 307, 264, 283, 2445, 13, 51602, 51602, 407, 291, 1936, 362, 281, 23203, 13, 51690, 51690, 407, 1936, 291, 14722, 264, 4560, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.13152232603593306, "compression_ratio": 1.8864628820960698, "no_speech_prob": 3.340438979648752e-06}, {"id": 1408, "seek": 669186, "start": 6709.98, "end": 6712.78, "text": " multiplied by this sort of inverse determinant", "tokens": [50364, 293, 2212, 300, 291, 6889, 428, 7316, 9505, 11, 50558, 50558, 291, 362, 428, 7316, 9505, 11, 291, 6889, 490, 309, 11, 50692, 50762, 577, 360, 291, 733, 295, 17522, 364, 10024, 2445, 50888, 50888, 300, 5276, 300, 264, 280, 291, 483, 412, 264, 5598, 51138, 51158, 307, 2681, 281, 264, 9505, 291, 829, 412, 264, 4846, 51270, 51270, 17207, 538, 341, 1333, 295, 17340, 41296, 51410, 51410, 295, 264, 14117, 952, 295, 264, 283, 2445, 13, 51520, 51520, 708, 291, 362, 281, 915, 307, 264, 283, 2445, 13, 51602, 51602, 407, 291, 1936, 362, 281, 23203, 13, 51690, 51690, 407, 1936, 291, 14722, 264, 4560, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.13152232603593306, "compression_ratio": 1.8864628820960698, "no_speech_prob": 3.340438979648752e-06}, {"id": 1409, "seek": 669186, "start": 6712.78, "end": 6714.98, "text": " of the Jacobian of the f function.", "tokens": [50364, 293, 2212, 300, 291, 6889, 428, 7316, 9505, 11, 50558, 50558, 291, 362, 428, 7316, 9505, 11, 291, 6889, 490, 309, 11, 50692, 50762, 577, 360, 291, 733, 295, 17522, 364, 10024, 2445, 50888, 50888, 300, 5276, 300, 264, 280, 291, 483, 412, 264, 5598, 51138, 51158, 307, 2681, 281, 264, 9505, 291, 829, 412, 264, 4846, 51270, 51270, 17207, 538, 341, 1333, 295, 17340, 41296, 51410, 51410, 295, 264, 14117, 952, 295, 264, 283, 2445, 13, 51520, 51520, 708, 291, 362, 281, 915, 307, 264, 283, 2445, 13, 51602, 51602, 407, 291, 1936, 362, 281, 23203, 13, 51690, 51690, 407, 1936, 291, 14722, 264, 4560, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.13152232603593306, "compression_ratio": 1.8864628820960698, "no_speech_prob": 3.340438979648752e-06}, {"id": 1410, "seek": 669186, "start": 6714.98, "end": 6716.62, "text": " What you have to find is the f function.", "tokens": [50364, 293, 2212, 300, 291, 6889, 428, 7316, 9505, 11, 50558, 50558, 291, 362, 428, 7316, 9505, 11, 291, 6889, 490, 309, 11, 50692, 50762, 577, 360, 291, 733, 295, 17522, 364, 10024, 2445, 50888, 50888, 300, 5276, 300, 264, 280, 291, 483, 412, 264, 5598, 51138, 51158, 307, 2681, 281, 264, 9505, 291, 829, 412, 264, 4846, 51270, 51270, 17207, 538, 341, 1333, 295, 17340, 41296, 51410, 51410, 295, 264, 14117, 952, 295, 264, 283, 2445, 13, 51520, 51520, 708, 291, 362, 281, 915, 307, 264, 283, 2445, 13, 51602, 51602, 407, 291, 1936, 362, 281, 23203, 13, 51690, 51690, 407, 1936, 291, 14722, 264, 4560, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.13152232603593306, "compression_ratio": 1.8864628820960698, "no_speech_prob": 3.340438979648752e-06}, {"id": 1411, "seek": 669186, "start": 6716.62, "end": 6718.38, "text": " So you basically have to differentiate.", "tokens": [50364, 293, 2212, 300, 291, 6889, 428, 7316, 9505, 11, 50558, 50558, 291, 362, 428, 7316, 9505, 11, 291, 6889, 490, 309, 11, 50692, 50762, 577, 360, 291, 733, 295, 17522, 364, 10024, 2445, 50888, 50888, 300, 5276, 300, 264, 280, 291, 483, 412, 264, 5598, 51138, 51158, 307, 2681, 281, 264, 9505, 291, 829, 412, 264, 4846, 51270, 51270, 17207, 538, 341, 1333, 295, 17340, 41296, 51410, 51410, 295, 264, 14117, 952, 295, 264, 283, 2445, 13, 51520, 51520, 708, 291, 362, 281, 915, 307, 264, 283, 2445, 13, 51602, 51602, 407, 291, 1936, 362, 281, 23203, 13, 51690, 51690, 407, 1936, 291, 14722, 264, 4560, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.13152232603593306, "compression_ratio": 1.8864628820960698, "no_speech_prob": 3.340438979648752e-06}, {"id": 1412, "seek": 669186, "start": 6718.38, "end": 6720.54, "text": " So basically you compute the distance", "tokens": [50364, 293, 2212, 300, 291, 6889, 428, 7316, 9505, 11, 50558, 50558, 291, 362, 428, 7316, 9505, 11, 291, 6889, 490, 309, 11, 50692, 50762, 577, 360, 291, 733, 295, 17522, 364, 10024, 2445, 50888, 50888, 300, 5276, 300, 264, 280, 291, 483, 412, 264, 5598, 51138, 51158, 307, 2681, 281, 264, 9505, 291, 829, 412, 264, 4846, 51270, 51270, 17207, 538, 341, 1333, 295, 17340, 41296, 51410, 51410, 295, 264, 14117, 952, 295, 264, 283, 2445, 13, 51520, 51520, 708, 291, 362, 281, 915, 307, 264, 283, 2445, 13, 51602, 51602, 407, 291, 1936, 362, 281, 23203, 13, 51690, 51690, 407, 1936, 291, 14722, 264, 4560, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.13152232603593306, "compression_ratio": 1.8864628820960698, "no_speech_prob": 3.340438979648752e-06}, {"id": 1413, "seek": 672054, "start": 6720.54, "end": 6723.5, "text": " between those divergence, KL divergence for example,", "tokens": [50364, 1296, 729, 47387, 11, 47991, 47387, 337, 1365, 11, 50512, 50512, 1296, 280, 295, 288, 293, 264, 551, 322, 264, 558, 1252, 50644, 50644, 295, 264, 2681, 1465, 13, 50740, 50788, 400, 291, 362, 281, 23203, 341, 50862, 50862, 365, 3104, 281, 264, 9834, 295, 283, 13, 50970, 50970, 407, 291, 362, 281, 1936, 48256, 51040, 51040, 807, 264, 17340, 16235, 295, 264, 14117, 952, 295, 283, 11, 558, 30, 51256, 51300, 467, 311, 406, 1858, 13, 51342, 51390, 4372, 2049, 437, 561, 360, 307, 300, 436, 2464, 283, 51590, 51590, 382, 257, 36624, 295, 588, 2199, 283, 311, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.12521283603408961, "compression_ratio": 1.5899581589958158, "no_speech_prob": 7.526767603849294e-06}, {"id": 1414, "seek": 672054, "start": 6723.5, "end": 6726.14, "text": " between p of y and the thing on the right side", "tokens": [50364, 1296, 729, 47387, 11, 47991, 47387, 337, 1365, 11, 50512, 50512, 1296, 280, 295, 288, 293, 264, 551, 322, 264, 558, 1252, 50644, 50644, 295, 264, 2681, 1465, 13, 50740, 50788, 400, 291, 362, 281, 23203, 341, 50862, 50862, 365, 3104, 281, 264, 9834, 295, 283, 13, 50970, 50970, 407, 291, 362, 281, 1936, 48256, 51040, 51040, 807, 264, 17340, 16235, 295, 264, 14117, 952, 295, 283, 11, 558, 30, 51256, 51300, 467, 311, 406, 1858, 13, 51342, 51390, 4372, 2049, 437, 561, 360, 307, 300, 436, 2464, 283, 51590, 51590, 382, 257, 36624, 295, 588, 2199, 283, 311, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.12521283603408961, "compression_ratio": 1.5899581589958158, "no_speech_prob": 7.526767603849294e-06}, {"id": 1415, "seek": 672054, "start": 6726.14, "end": 6728.06, "text": " of the equal sign.", "tokens": [50364, 1296, 729, 47387, 11, 47991, 47387, 337, 1365, 11, 50512, 50512, 1296, 280, 295, 288, 293, 264, 551, 322, 264, 558, 1252, 50644, 50644, 295, 264, 2681, 1465, 13, 50740, 50788, 400, 291, 362, 281, 23203, 341, 50862, 50862, 365, 3104, 281, 264, 9834, 295, 283, 13, 50970, 50970, 407, 291, 362, 281, 1936, 48256, 51040, 51040, 807, 264, 17340, 16235, 295, 264, 14117, 952, 295, 283, 11, 558, 30, 51256, 51300, 467, 311, 406, 1858, 13, 51342, 51390, 4372, 2049, 437, 561, 360, 307, 300, 436, 2464, 283, 51590, 51590, 382, 257, 36624, 295, 588, 2199, 283, 311, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.12521283603408961, "compression_ratio": 1.5899581589958158, "no_speech_prob": 7.526767603849294e-06}, {"id": 1416, "seek": 672054, "start": 6729.0199999999995, "end": 6730.5, "text": " And you have to differentiate this", "tokens": [50364, 1296, 729, 47387, 11, 47991, 47387, 337, 1365, 11, 50512, 50512, 1296, 280, 295, 288, 293, 264, 551, 322, 264, 558, 1252, 50644, 50644, 295, 264, 2681, 1465, 13, 50740, 50788, 400, 291, 362, 281, 23203, 341, 50862, 50862, 365, 3104, 281, 264, 9834, 295, 283, 13, 50970, 50970, 407, 291, 362, 281, 1936, 48256, 51040, 51040, 807, 264, 17340, 16235, 295, 264, 14117, 952, 295, 283, 11, 558, 30, 51256, 51300, 467, 311, 406, 1858, 13, 51342, 51390, 4372, 2049, 437, 561, 360, 307, 300, 436, 2464, 283, 51590, 51590, 382, 257, 36624, 295, 588, 2199, 283, 311, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.12521283603408961, "compression_ratio": 1.5899581589958158, "no_speech_prob": 7.526767603849294e-06}, {"id": 1417, "seek": 672054, "start": 6730.5, "end": 6732.66, "text": " with respect to the parameters of f.", "tokens": [50364, 1296, 729, 47387, 11, 47991, 47387, 337, 1365, 11, 50512, 50512, 1296, 280, 295, 288, 293, 264, 551, 322, 264, 558, 1252, 50644, 50644, 295, 264, 2681, 1465, 13, 50740, 50788, 400, 291, 362, 281, 23203, 341, 50862, 50862, 365, 3104, 281, 264, 9834, 295, 283, 13, 50970, 50970, 407, 291, 362, 281, 1936, 48256, 51040, 51040, 807, 264, 17340, 16235, 295, 264, 14117, 952, 295, 283, 11, 558, 30, 51256, 51300, 467, 311, 406, 1858, 13, 51342, 51390, 4372, 2049, 437, 561, 360, 307, 300, 436, 2464, 283, 51590, 51590, 382, 257, 36624, 295, 588, 2199, 283, 311, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.12521283603408961, "compression_ratio": 1.5899581589958158, "no_speech_prob": 7.526767603849294e-06}, {"id": 1418, "seek": 672054, "start": 6732.66, "end": 6734.06, "text": " So you have to basically propagate", "tokens": [50364, 1296, 729, 47387, 11, 47991, 47387, 337, 1365, 11, 50512, 50512, 1296, 280, 295, 288, 293, 264, 551, 322, 264, 558, 1252, 50644, 50644, 295, 264, 2681, 1465, 13, 50740, 50788, 400, 291, 362, 281, 23203, 341, 50862, 50862, 365, 3104, 281, 264, 9834, 295, 283, 13, 50970, 50970, 407, 291, 362, 281, 1936, 48256, 51040, 51040, 807, 264, 17340, 16235, 295, 264, 14117, 952, 295, 283, 11, 558, 30, 51256, 51300, 467, 311, 406, 1858, 13, 51342, 51390, 4372, 2049, 437, 561, 360, 307, 300, 436, 2464, 283, 51590, 51590, 382, 257, 36624, 295, 588, 2199, 283, 311, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.12521283603408961, "compression_ratio": 1.5899581589958158, "no_speech_prob": 7.526767603849294e-06}, {"id": 1419, "seek": 672054, "start": 6734.06, "end": 6738.38, "text": " through the inverse gradient of the Jacobian of f, right?", "tokens": [50364, 1296, 729, 47387, 11, 47991, 47387, 337, 1365, 11, 50512, 50512, 1296, 280, 295, 288, 293, 264, 551, 322, 264, 558, 1252, 50644, 50644, 295, 264, 2681, 1465, 13, 50740, 50788, 400, 291, 362, 281, 23203, 341, 50862, 50862, 365, 3104, 281, 264, 9834, 295, 283, 13, 50970, 50970, 407, 291, 362, 281, 1936, 48256, 51040, 51040, 807, 264, 17340, 16235, 295, 264, 14117, 952, 295, 283, 11, 558, 30, 51256, 51300, 467, 311, 406, 1858, 13, 51342, 51390, 4372, 2049, 437, 561, 360, 307, 300, 436, 2464, 283, 51590, 51590, 382, 257, 36624, 295, 588, 2199, 283, 311, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.12521283603408961, "compression_ratio": 1.5899581589958158, "no_speech_prob": 7.526767603849294e-06}, {"id": 1420, "seek": 672054, "start": 6739.26, "end": 6740.1, "text": " It's not easy.", "tokens": [50364, 1296, 729, 47387, 11, 47991, 47387, 337, 1365, 11, 50512, 50512, 1296, 280, 295, 288, 293, 264, 551, 322, 264, 558, 1252, 50644, 50644, 295, 264, 2681, 1465, 13, 50740, 50788, 400, 291, 362, 281, 23203, 341, 50862, 50862, 365, 3104, 281, 264, 9834, 295, 283, 13, 50970, 50970, 407, 291, 362, 281, 1936, 48256, 51040, 51040, 807, 264, 17340, 16235, 295, 264, 14117, 952, 295, 283, 11, 558, 30, 51256, 51300, 467, 311, 406, 1858, 13, 51342, 51390, 4372, 2049, 437, 561, 360, 307, 300, 436, 2464, 283, 51590, 51590, 382, 257, 36624, 295, 588, 2199, 283, 311, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.12521283603408961, "compression_ratio": 1.5899581589958158, "no_speech_prob": 7.526767603849294e-06}, {"id": 1421, "seek": 672054, "start": 6741.06, "end": 6745.06, "text": " Very often what people do is that they write f", "tokens": [50364, 1296, 729, 47387, 11, 47991, 47387, 337, 1365, 11, 50512, 50512, 1296, 280, 295, 288, 293, 264, 551, 322, 264, 558, 1252, 50644, 50644, 295, 264, 2681, 1465, 13, 50740, 50788, 400, 291, 362, 281, 23203, 341, 50862, 50862, 365, 3104, 281, 264, 9834, 295, 283, 13, 50970, 50970, 407, 291, 362, 281, 1936, 48256, 51040, 51040, 807, 264, 17340, 16235, 295, 264, 14117, 952, 295, 283, 11, 558, 30, 51256, 51300, 467, 311, 406, 1858, 13, 51342, 51390, 4372, 2049, 437, 561, 360, 307, 300, 436, 2464, 283, 51590, 51590, 382, 257, 36624, 295, 588, 2199, 283, 311, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.12521283603408961, "compression_ratio": 1.5899581589958158, "no_speech_prob": 7.526767603849294e-06}, {"id": 1422, "seek": 672054, "start": 6745.06, "end": 6747.66, "text": " as a succession of very simple f's", "tokens": [50364, 1296, 729, 47387, 11, 47991, 47387, 337, 1365, 11, 50512, 50512, 1296, 280, 295, 288, 293, 264, 551, 322, 264, 558, 1252, 50644, 50644, 295, 264, 2681, 1465, 13, 50740, 50788, 400, 291, 362, 281, 23203, 341, 50862, 50862, 365, 3104, 281, 264, 9834, 295, 283, 13, 50970, 50970, 407, 291, 362, 281, 1936, 48256, 51040, 51040, 807, 264, 17340, 16235, 295, 264, 14117, 952, 295, 283, 11, 558, 30, 51256, 51300, 467, 311, 406, 1858, 13, 51342, 51390, 4372, 2049, 437, 561, 360, 307, 300, 436, 2464, 283, 51590, 51590, 382, 257, 36624, 295, 588, 2199, 283, 311, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.12521283603408961, "compression_ratio": 1.5899581589958158, "no_speech_prob": 7.526767603849294e-06}, {"id": 1423, "seek": 674766, "start": 6747.66, "end": 6751.62, "text": " that only modify the distribution just a little bit.", "tokens": [50364, 300, 787, 16927, 264, 7316, 445, 257, 707, 857, 13, 50562, 50672, 407, 283, 588, 2049, 307, 746, 411, 264, 6575, 50890, 50890, 1804, 512, 25163, 11, 257, 857, 411, 5015, 31890, 498, 291, 528, 13, 51058, 51058, 400, 550, 291, 8630, 3195, 293, 3195, 295, 7914, 295, 300, 13, 51250, 51250, 400, 264, 1154, 3643, 18587, 51338, 51338, 570, 562, 729, 6828, 360, 257, 707, 857, 295, 26747, 51588, 51670], "temperature": 0.0, "avg_logprob": -0.10229475204258749, "compression_ratio": 1.566137566137566, "no_speech_prob": 8.527893442078494e-06}, {"id": 1424, "seek": 674766, "start": 6753.82, "end": 6758.18, "text": " So f very often is something like the identity", "tokens": [50364, 300, 787, 16927, 264, 7316, 445, 257, 707, 857, 13, 50562, 50672, 407, 283, 588, 2049, 307, 746, 411, 264, 6575, 50890, 50890, 1804, 512, 25163, 11, 257, 857, 411, 5015, 31890, 498, 291, 528, 13, 51058, 51058, 400, 550, 291, 8630, 3195, 293, 3195, 295, 7914, 295, 300, 13, 51250, 51250, 400, 264, 1154, 3643, 18587, 51338, 51338, 570, 562, 729, 6828, 360, 257, 707, 857, 295, 26747, 51588, 51670], "temperature": 0.0, "avg_logprob": -0.10229475204258749, "compression_ratio": 1.566137566137566, "no_speech_prob": 8.527893442078494e-06}, {"id": 1425, "seek": 674766, "start": 6758.18, "end": 6761.54, "text": " plus some deviation, a bit like ResNet if you want.", "tokens": [50364, 300, 787, 16927, 264, 7316, 445, 257, 707, 857, 13, 50562, 50672, 407, 283, 588, 2049, 307, 746, 411, 264, 6575, 50890, 50890, 1804, 512, 25163, 11, 257, 857, 411, 5015, 31890, 498, 291, 528, 13, 51058, 51058, 400, 550, 291, 8630, 3195, 293, 3195, 295, 7914, 295, 300, 13, 51250, 51250, 400, 264, 1154, 3643, 18587, 51338, 51338, 570, 562, 729, 6828, 360, 257, 707, 857, 295, 26747, 51588, 51670], "temperature": 0.0, "avg_logprob": -0.10229475204258749, "compression_ratio": 1.566137566137566, "no_speech_prob": 8.527893442078494e-06}, {"id": 1426, "seek": 674766, "start": 6761.54, "end": 6765.38, "text": " And then you stack lots and lots of layers of that.", "tokens": [50364, 300, 787, 16927, 264, 7316, 445, 257, 707, 857, 13, 50562, 50672, 407, 283, 588, 2049, 307, 746, 411, 264, 6575, 50890, 50890, 1804, 512, 25163, 11, 257, 857, 411, 5015, 31890, 498, 291, 528, 13, 51058, 51058, 400, 550, 291, 8630, 3195, 293, 3195, 295, 7914, 295, 300, 13, 51250, 51250, 400, 264, 1154, 3643, 18587, 51338, 51338, 570, 562, 729, 6828, 360, 257, 707, 857, 295, 26747, 51588, 51670], "temperature": 0.0, "avg_logprob": -0.10229475204258749, "compression_ratio": 1.566137566137566, "no_speech_prob": 8.527893442078494e-06}, {"id": 1427, "seek": 674766, "start": 6765.38, "end": 6767.139999999999, "text": " And the problem becomes simpler", "tokens": [50364, 300, 787, 16927, 264, 7316, 445, 257, 707, 857, 13, 50562, 50672, 407, 283, 588, 2049, 307, 746, 411, 264, 6575, 50890, 50890, 1804, 512, 25163, 11, 257, 857, 411, 5015, 31890, 498, 291, 528, 13, 51058, 51058, 400, 550, 291, 8630, 3195, 293, 3195, 295, 7914, 295, 300, 13, 51250, 51250, 400, 264, 1154, 3643, 18587, 51338, 51338, 570, 562, 729, 6828, 360, 257, 707, 857, 295, 26747, 51588, 51670], "temperature": 0.0, "avg_logprob": -0.10229475204258749, "compression_ratio": 1.566137566137566, "no_speech_prob": 8.527893442078494e-06}, {"id": 1428, "seek": 674766, "start": 6767.139999999999, "end": 6772.139999999999, "text": " because when those functions do a little bit of modification", "tokens": [50364, 300, 787, 16927, 264, 7316, 445, 257, 707, 857, 13, 50562, 50672, 407, 283, 588, 2049, 307, 746, 411, 264, 6575, 50890, 50890, 1804, 512, 25163, 11, 257, 857, 411, 5015, 31890, 498, 291, 528, 13, 51058, 51058, 400, 550, 291, 8630, 3195, 293, 3195, 295, 7914, 295, 300, 13, 51250, 51250, 400, 264, 1154, 3643, 18587, 51338, 51338, 570, 562, 729, 6828, 360, 257, 707, 857, 295, 26747, 51588, 51670], "temperature": 0.0, "avg_logprob": -0.10229475204258749, "compression_ratio": 1.566137566137566, "no_speech_prob": 8.527893442078494e-06}, {"id": 1429, "seek": 677214, "start": 6772.14, "end": 6777.14, "text": " then a lot of those kind of issues kind of become simple.", "tokens": [50364, 550, 257, 688, 295, 729, 733, 295, 2663, 733, 295, 1813, 2199, 13, 50614, 50628, 440, 41296, 510, 733, 295, 6883, 11221, 13, 50782, 50782, 1033, 11, 300, 311, 257, 588, 1333, 295, 12649, 50878, 50878, 1090, 1496, 3855, 295, 2710, 3319, 3095, 13, 51034, 51136, 821, 311, 1880, 10577, 466, 341, 294, 5162, 924, 51386, 51472, 322, 754, 5162, 2493, 322, 1228, 341, 51628, 51628, 337, 411, 12359, 10649, 293, 1507, 411, 300, 13, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.36097483091716526, "compression_ratio": 1.595959595959596, "no_speech_prob": 6.338897946989164e-06}, {"id": 1430, "seek": 677214, "start": 6777.42, "end": 6780.5, "text": " The determinant here kind of simplifies.", "tokens": [50364, 550, 257, 688, 295, 729, 733, 295, 2663, 733, 295, 1813, 2199, 13, 50614, 50628, 440, 41296, 510, 733, 295, 6883, 11221, 13, 50782, 50782, 1033, 11, 300, 311, 257, 588, 1333, 295, 12649, 50878, 50878, 1090, 1496, 3855, 295, 2710, 3319, 3095, 13, 51034, 51136, 821, 311, 1880, 10577, 466, 341, 294, 5162, 924, 51386, 51472, 322, 754, 5162, 2493, 322, 1228, 341, 51628, 51628, 337, 411, 12359, 10649, 293, 1507, 411, 300, 13, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.36097483091716526, "compression_ratio": 1.595959595959596, "no_speech_prob": 6.338897946989164e-06}, {"id": 1431, "seek": 677214, "start": 6780.5, "end": 6782.42, "text": " Okay, that's a very sort of abstract", "tokens": [50364, 550, 257, 688, 295, 729, 733, 295, 2663, 733, 295, 1813, 2199, 13, 50614, 50628, 440, 41296, 510, 733, 295, 6883, 11221, 13, 50782, 50782, 1033, 11, 300, 311, 257, 588, 1333, 295, 12649, 50878, 50878, 1090, 1496, 3855, 295, 2710, 3319, 3095, 13, 51034, 51136, 821, 311, 1880, 10577, 466, 341, 294, 5162, 924, 51386, 51472, 322, 754, 5162, 2493, 322, 1228, 341, 51628, 51628, 337, 411, 12359, 10649, 293, 1507, 411, 300, 13, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.36097483091716526, "compression_ratio": 1.595959595959596, "no_speech_prob": 6.338897946989164e-06}, {"id": 1432, "seek": 677214, "start": 6782.42, "end": 6785.54, "text": " high level description of normalizing flow.", "tokens": [50364, 550, 257, 688, 295, 729, 733, 295, 2663, 733, 295, 1813, 2199, 13, 50614, 50628, 440, 41296, 510, 733, 295, 6883, 11221, 13, 50782, 50782, 1033, 11, 300, 311, 257, 588, 1333, 295, 12649, 50878, 50878, 1090, 1496, 3855, 295, 2710, 3319, 3095, 13, 51034, 51136, 821, 311, 1880, 10577, 466, 341, 294, 5162, 924, 51386, 51472, 322, 754, 5162, 2493, 322, 1228, 341, 51628, 51628, 337, 411, 12359, 10649, 293, 1507, 411, 300, 13, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.36097483091716526, "compression_ratio": 1.595959595959596, "no_speech_prob": 6.338897946989164e-06}, {"id": 1433, "seek": 677214, "start": 6787.58, "end": 6792.58, "text": " There's interesting papers about this in recent years", "tokens": [50364, 550, 257, 688, 295, 729, 733, 295, 2663, 733, 295, 1813, 2199, 13, 50614, 50628, 440, 41296, 510, 733, 295, 6883, 11221, 13, 50782, 50782, 1033, 11, 300, 311, 257, 588, 1333, 295, 12649, 50878, 50878, 1090, 1496, 3855, 295, 2710, 3319, 3095, 13, 51034, 51136, 821, 311, 1880, 10577, 466, 341, 294, 5162, 924, 51386, 51472, 322, 754, 5162, 2493, 322, 1228, 341, 51628, 51628, 337, 411, 12359, 10649, 293, 1507, 411, 300, 13, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.36097483091716526, "compression_ratio": 1.595959595959596, "no_speech_prob": 6.338897946989164e-06}, {"id": 1434, "seek": 677214, "start": 6794.3, "end": 6797.42, "text": " on even recent months on using this", "tokens": [50364, 550, 257, 688, 295, 729, 733, 295, 2663, 733, 295, 1813, 2199, 13, 50614, 50628, 440, 41296, 510, 733, 295, 6883, 11221, 13, 50782, 50782, 1033, 11, 300, 311, 257, 588, 1333, 295, 12649, 50878, 50878, 1090, 1496, 3855, 295, 2710, 3319, 3095, 13, 51034, 51136, 821, 311, 1880, 10577, 466, 341, 294, 5162, 924, 51386, 51472, 322, 754, 5162, 2493, 322, 1228, 341, 51628, 51628, 337, 411, 12359, 10649, 293, 1507, 411, 300, 13, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.36097483091716526, "compression_ratio": 1.595959595959596, "no_speech_prob": 6.338897946989164e-06}, {"id": 1435, "seek": 677214, "start": 6797.42, "end": 6799.860000000001, "text": " for like particle physics and stuff like that.", "tokens": [50364, 550, 257, 688, 295, 729, 733, 295, 2663, 733, 295, 1813, 2199, 13, 50614, 50628, 440, 41296, 510, 733, 295, 6883, 11221, 13, 50782, 50782, 1033, 11, 300, 311, 257, 588, 1333, 295, 12649, 50878, 50878, 1090, 1496, 3855, 295, 2710, 3319, 3095, 13, 51034, 51136, 821, 311, 1880, 10577, 466, 341, 294, 5162, 924, 51386, 51472, 322, 754, 5162, 2493, 322, 1228, 341, 51628, 51628, 337, 411, 12359, 10649, 293, 1507, 411, 300, 13, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.36097483091716526, "compression_ratio": 1.595959595959596, "no_speech_prob": 6.338897946989164e-06}, {"id": 1436, "seek": 679986, "start": 6799.86, "end": 6802.86, "text": " Carl Kranmer at NYU is actually kind of a specialist of that.", "tokens": [50364, 14256, 591, 4257, 936, 412, 42682, 307, 767, 733, 295, 257, 17008, 295, 300, 13, 50514, 50606, 1044, 291, 370, 709, 11, 8304, 13, 50680, 50776, 1057, 558, 11, 604, 661, 1168, 30, 50862, 50934, 1033, 11, 300, 390, 309, 13, 51000, 51000, 3769, 11, 1309, 291, 588, 709, 1518, 13, 51136, 51136, 865, 11, 536, 291, 4153, 1074, 13, 51206, 51252, 1057, 558, 11, 6543, 6543, 13, 51318, 51318, 3664, 1127, 13, 51360], "temperature": 0.0, "avg_logprob": -0.4714139135260331, "compression_ratio": 1.4, "no_speech_prob": 0.0001633728388696909}, {"id": 1437, "seek": 679986, "start": 6804.7, "end": 6806.179999999999, "text": " Thank you so much, professor.", "tokens": [50364, 14256, 591, 4257, 936, 412, 42682, 307, 767, 733, 295, 257, 17008, 295, 300, 13, 50514, 50606, 1044, 291, 370, 709, 11, 8304, 13, 50680, 50776, 1057, 558, 11, 604, 661, 1168, 30, 50862, 50934, 1033, 11, 300, 390, 309, 13, 51000, 51000, 3769, 11, 1309, 291, 588, 709, 1518, 13, 51136, 51136, 865, 11, 536, 291, 4153, 1074, 13, 51206, 51252, 1057, 558, 11, 6543, 6543, 13, 51318, 51318, 3664, 1127, 13, 51360], "temperature": 0.0, "avg_logprob": -0.4714139135260331, "compression_ratio": 1.4, "no_speech_prob": 0.0001633728388696909}, {"id": 1438, "seek": 679986, "start": 6808.099999999999, "end": 6809.82, "text": " All right, any other question?", "tokens": [50364, 14256, 591, 4257, 936, 412, 42682, 307, 767, 733, 295, 257, 17008, 295, 300, 13, 50514, 50606, 1044, 291, 370, 709, 11, 8304, 13, 50680, 50776, 1057, 558, 11, 604, 661, 1168, 30, 50862, 50934, 1033, 11, 300, 390, 309, 13, 51000, 51000, 3769, 11, 1309, 291, 588, 709, 1518, 13, 51136, 51136, 865, 11, 536, 291, 4153, 1074, 13, 51206, 51252, 1057, 558, 11, 6543, 6543, 13, 51318, 51318, 3664, 1127, 13, 51360], "temperature": 0.0, "avg_logprob": -0.4714139135260331, "compression_ratio": 1.4, "no_speech_prob": 0.0001633728388696909}, {"id": 1439, "seek": 679986, "start": 6811.259999999999, "end": 6812.58, "text": " Okay, that was it.", "tokens": [50364, 14256, 591, 4257, 936, 412, 42682, 307, 767, 733, 295, 257, 17008, 295, 300, 13, 50514, 50606, 1044, 291, 370, 709, 11, 8304, 13, 50680, 50776, 1057, 558, 11, 604, 661, 1168, 30, 50862, 50934, 1033, 11, 300, 390, 309, 13, 51000, 51000, 3769, 11, 1309, 291, 588, 709, 1518, 13, 51136, 51136, 865, 11, 536, 291, 4153, 1074, 13, 51206, 51252, 1057, 558, 11, 6543, 6543, 13, 51318, 51318, 3664, 1127, 13, 51360], "temperature": 0.0, "avg_logprob": -0.4714139135260331, "compression_ratio": 1.4, "no_speech_prob": 0.0001633728388696909}, {"id": 1440, "seek": 679986, "start": 6812.58, "end": 6815.299999999999, "text": " Great, thank you very much everyone.", "tokens": [50364, 14256, 591, 4257, 936, 412, 42682, 307, 767, 733, 295, 257, 17008, 295, 300, 13, 50514, 50606, 1044, 291, 370, 709, 11, 8304, 13, 50680, 50776, 1057, 558, 11, 604, 661, 1168, 30, 50862, 50934, 1033, 11, 300, 390, 309, 13, 51000, 51000, 3769, 11, 1309, 291, 588, 709, 1518, 13, 51136, 51136, 865, 11, 536, 291, 4153, 1074, 13, 51206, 51252, 1057, 558, 11, 6543, 6543, 13, 51318, 51318, 3664, 1127, 13, 51360], "temperature": 0.0, "avg_logprob": -0.4714139135260331, "compression_ratio": 1.4, "no_speech_prob": 0.0001633728388696909}, {"id": 1441, "seek": 679986, "start": 6815.299999999999, "end": 6816.7, "text": " Yeah, see you tomorrow guys.", "tokens": [50364, 14256, 591, 4257, 936, 412, 42682, 307, 767, 733, 295, 257, 17008, 295, 300, 13, 50514, 50606, 1044, 291, 370, 709, 11, 8304, 13, 50680, 50776, 1057, 558, 11, 604, 661, 1168, 30, 50862, 50934, 1033, 11, 300, 390, 309, 13, 51000, 51000, 3769, 11, 1309, 291, 588, 709, 1518, 13, 51136, 51136, 865, 11, 536, 291, 4153, 1074, 13, 51206, 51252, 1057, 558, 11, 6543, 6543, 13, 51318, 51318, 3664, 1127, 13, 51360], "temperature": 0.0, "avg_logprob": -0.4714139135260331, "compression_ratio": 1.4, "no_speech_prob": 0.0001633728388696909}, {"id": 1442, "seek": 679986, "start": 6817.62, "end": 6818.94, "text": " All right, bye bye.", "tokens": [50364, 14256, 591, 4257, 936, 412, 42682, 307, 767, 733, 295, 257, 17008, 295, 300, 13, 50514, 50606, 1044, 291, 370, 709, 11, 8304, 13, 50680, 50776, 1057, 558, 11, 604, 661, 1168, 30, 50862, 50934, 1033, 11, 300, 390, 309, 13, 51000, 51000, 3769, 11, 1309, 291, 588, 709, 1518, 13, 51136, 51136, 865, 11, 536, 291, 4153, 1074, 13, 51206, 51252, 1057, 558, 11, 6543, 6543, 13, 51318, 51318, 3664, 1127, 13, 51360], "temperature": 0.0, "avg_logprob": -0.4714139135260331, "compression_ratio": 1.4, "no_speech_prob": 0.0001633728388696909}, {"id": 1443, "seek": 681894, "start": 6818.94, "end": 6833.94, "text": " Take care.", "tokens": [50364, 3664, 1127, 13, 51114], "temperature": 0.0, "avg_logprob": -0.5702426433563232, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.00022157754574436694}], "language": "en", "video_id": "bj1fh3BvqSU", "entity": "Yann LeCun"}}