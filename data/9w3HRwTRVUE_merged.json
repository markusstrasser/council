{"video_id": "9w3HRwTRVUE", "title": "2.7 Practical Tips for Linear Regression | Checking gradient descent for convergence -- ML Andrew Ng", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 340, "views": 247, "publish_date": "11/04/2022", "timestamp": 1661040000, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " When running gradient descent, how can you tell if it is converging? That is, whether it's helping you to find parameters close to the global minimum of the cost function. By learning to recognize what a well-running implementation of gradient descent looks like, we will also, in a later video, be better able to choose a good learning rate alpha. Let's take a look. As a reminder, here's the gradient descent rule, and one of the key choices is the choice of the learning rate alpha. Here's something that I often do to make sure that gradient descent is working well. Recall that the job of gradient descent is to find parameters w and b that hopefully minimize the cost function j. So what I'll often do is plot the cost function j, which is calculated on the training set, and I'll plot the value of j at each iteration of gradient descent. Remember that each iteration means after each simultaneous update of the parameters w and b, so in this plot, the horizontal axis is the number of iterations of gradient descent that you've run so far. And so you may get a curve that looks like this. Notice that the horizontal axis is the number of iterations of gradient descent, and not a parameter like w or b. This differs from previous graphs you've seen, where the vertical axis was cost j, and the horizontal axis was a single parameter like w or b. This curve is also called a learning curve. Note that there are a few different types of learning curves used in machine learning, and you'll see some of the types later in this course as well. Secondly, if you look at this point on the curve, this means that after you've run gradient descent for 100 iterations, meaning 100 simultaneous updates of the parameters, you have some learned values for w and b. And if you compute the cost, jwb, for those values of w and b, the ones you got after 100 iterations, you get this value for the cost j, that is, this point on the vertical axis. And this point here corresponds to the value of j for the parameters that you got after 200 iterations of gradient descent. So looking at this graph helps you to see how your cost j changes after each iteration of gradient descent. If gradient descent is working properly, then the cost j should decrease after every single iteration. If j ever increases after one iteration, that means either alpha is chosen poorly, and it usually means alpha is too large, or there could be a bug in the code. Another useful thing that this plot can tell you is that if you look at this curve, by the time you reach maybe 300 iterations or so, the cost j is leveling off and is no longer decreasing much. And by 400 iterations, it looks like the curve has flattened out. So this means that gradient descent has more or less converged because the curve is no longer decreasing. So looking at this learning curve, you can try to spot whether or not gradient descent is converging. By the way, the number of iterations that gradient descent takes to converge can vary a lot between different applications. In one application, it may converge after just 30 iterations. For a different application, it could take a thousand or a hundred thousand iterations. It turns out to be very difficult to tell in advance how many iterations gradient descent needs to converge, which is why you can create a graph like this, a learning curve, to try to find out when you can stop training your particular model. Another way to decide when your model is done training is with an automatic convergence test. So let's let epsilon, this here is the Greek alphabet epsilon, let's let epsilon be a variable representing a small number, such as 0.001 or 10 to the power of negative three. If the cost J decreases by less than this number epsilon on one iteration, then you're likely on this flattened part of the curve that you see on the left and you can declare convergence. Remember, convergence hopefully indicates that you found parameters W and B that are close to the minimum possible value of J. I usually find that choosing the right threshold epsilon is pretty difficult. So I actually tend to look at graphs like this one on the left, rather than rely on automatic convergence tests. Looking at this other figure can tell you, I'll give you some advanced warning if maybe gradient descent is not working correctly as well. So you've now seen what the learning curve should look like when gradient descent is running well. Let's take these insights and in the next video, take a look at how to choose an appropriate learning rate.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.96, "text": " When running gradient descent, how can you tell if it is converging?", "tokens": [50364, 1133, 2614, 16235, 23475, 11, 577, 393, 291, 980, 498, 309, 307, 9652, 3249, 30, 50662, 50662, 663, 307, 11, 1968, 309, 311, 4315, 291, 281, 915, 9834, 1998, 281, 264, 4338, 7285, 295, 50880, 50880, 264, 2063, 2445, 13, 50972, 50972, 3146, 2539, 281, 5521, 437, 257, 731, 12, 45482, 11420, 295, 16235, 23475, 1542, 411, 11, 51204, 51204, 321, 486, 611, 11, 294, 257, 1780, 960, 11, 312, 1101, 1075, 281, 2826, 257, 665, 2539, 3314, 8961, 13, 51500, 51500, 961, 311, 747, 257, 574, 13, 51563, 51563, 1018, 257, 13548, 11, 510, 311, 264, 16235, 23475, 4978, 11, 293, 472, 295, 264, 2141, 7994, 307, 264, 3922, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.13307532092981172, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.007813142612576485}, {"id": 1, "seek": 0, "start": 5.96, "end": 10.32, "text": " That is, whether it's helping you to find parameters close to the global minimum of", "tokens": [50364, 1133, 2614, 16235, 23475, 11, 577, 393, 291, 980, 498, 309, 307, 9652, 3249, 30, 50662, 50662, 663, 307, 11, 1968, 309, 311, 4315, 291, 281, 915, 9834, 1998, 281, 264, 4338, 7285, 295, 50880, 50880, 264, 2063, 2445, 13, 50972, 50972, 3146, 2539, 281, 5521, 437, 257, 731, 12, 45482, 11420, 295, 16235, 23475, 1542, 411, 11, 51204, 51204, 321, 486, 611, 11, 294, 257, 1780, 960, 11, 312, 1101, 1075, 281, 2826, 257, 665, 2539, 3314, 8961, 13, 51500, 51500, 961, 311, 747, 257, 574, 13, 51563, 51563, 1018, 257, 13548, 11, 510, 311, 264, 16235, 23475, 4978, 11, 293, 472, 295, 264, 2141, 7994, 307, 264, 3922, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.13307532092981172, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.007813142612576485}, {"id": 2, "seek": 0, "start": 10.32, "end": 12.16, "text": " the cost function.", "tokens": [50364, 1133, 2614, 16235, 23475, 11, 577, 393, 291, 980, 498, 309, 307, 9652, 3249, 30, 50662, 50662, 663, 307, 11, 1968, 309, 311, 4315, 291, 281, 915, 9834, 1998, 281, 264, 4338, 7285, 295, 50880, 50880, 264, 2063, 2445, 13, 50972, 50972, 3146, 2539, 281, 5521, 437, 257, 731, 12, 45482, 11420, 295, 16235, 23475, 1542, 411, 11, 51204, 51204, 321, 486, 611, 11, 294, 257, 1780, 960, 11, 312, 1101, 1075, 281, 2826, 257, 665, 2539, 3314, 8961, 13, 51500, 51500, 961, 311, 747, 257, 574, 13, 51563, 51563, 1018, 257, 13548, 11, 510, 311, 264, 16235, 23475, 4978, 11, 293, 472, 295, 264, 2141, 7994, 307, 264, 3922, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.13307532092981172, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.007813142612576485}, {"id": 3, "seek": 0, "start": 12.16, "end": 16.8, "text": " By learning to recognize what a well-running implementation of gradient descent looks like,", "tokens": [50364, 1133, 2614, 16235, 23475, 11, 577, 393, 291, 980, 498, 309, 307, 9652, 3249, 30, 50662, 50662, 663, 307, 11, 1968, 309, 311, 4315, 291, 281, 915, 9834, 1998, 281, 264, 4338, 7285, 295, 50880, 50880, 264, 2063, 2445, 13, 50972, 50972, 3146, 2539, 281, 5521, 437, 257, 731, 12, 45482, 11420, 295, 16235, 23475, 1542, 411, 11, 51204, 51204, 321, 486, 611, 11, 294, 257, 1780, 960, 11, 312, 1101, 1075, 281, 2826, 257, 665, 2539, 3314, 8961, 13, 51500, 51500, 961, 311, 747, 257, 574, 13, 51563, 51563, 1018, 257, 13548, 11, 510, 311, 264, 16235, 23475, 4978, 11, 293, 472, 295, 264, 2141, 7994, 307, 264, 3922, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.13307532092981172, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.007813142612576485}, {"id": 4, "seek": 0, "start": 16.8, "end": 22.72, "text": " we will also, in a later video, be better able to choose a good learning rate alpha.", "tokens": [50364, 1133, 2614, 16235, 23475, 11, 577, 393, 291, 980, 498, 309, 307, 9652, 3249, 30, 50662, 50662, 663, 307, 11, 1968, 309, 311, 4315, 291, 281, 915, 9834, 1998, 281, 264, 4338, 7285, 295, 50880, 50880, 264, 2063, 2445, 13, 50972, 50972, 3146, 2539, 281, 5521, 437, 257, 731, 12, 45482, 11420, 295, 16235, 23475, 1542, 411, 11, 51204, 51204, 321, 486, 611, 11, 294, 257, 1780, 960, 11, 312, 1101, 1075, 281, 2826, 257, 665, 2539, 3314, 8961, 13, 51500, 51500, 961, 311, 747, 257, 574, 13, 51563, 51563, 1018, 257, 13548, 11, 510, 311, 264, 16235, 23475, 4978, 11, 293, 472, 295, 264, 2141, 7994, 307, 264, 3922, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.13307532092981172, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.007813142612576485}, {"id": 5, "seek": 0, "start": 22.72, "end": 23.98, "text": " Let's take a look.", "tokens": [50364, 1133, 2614, 16235, 23475, 11, 577, 393, 291, 980, 498, 309, 307, 9652, 3249, 30, 50662, 50662, 663, 307, 11, 1968, 309, 311, 4315, 291, 281, 915, 9834, 1998, 281, 264, 4338, 7285, 295, 50880, 50880, 264, 2063, 2445, 13, 50972, 50972, 3146, 2539, 281, 5521, 437, 257, 731, 12, 45482, 11420, 295, 16235, 23475, 1542, 411, 11, 51204, 51204, 321, 486, 611, 11, 294, 257, 1780, 960, 11, 312, 1101, 1075, 281, 2826, 257, 665, 2539, 3314, 8961, 13, 51500, 51500, 961, 311, 747, 257, 574, 13, 51563, 51563, 1018, 257, 13548, 11, 510, 311, 264, 16235, 23475, 4978, 11, 293, 472, 295, 264, 2141, 7994, 307, 264, 3922, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.13307532092981172, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.007813142612576485}, {"id": 6, "seek": 0, "start": 23.98, "end": 29.68, "text": " As a reminder, here's the gradient descent rule, and one of the key choices is the choice", "tokens": [50364, 1133, 2614, 16235, 23475, 11, 577, 393, 291, 980, 498, 309, 307, 9652, 3249, 30, 50662, 50662, 663, 307, 11, 1968, 309, 311, 4315, 291, 281, 915, 9834, 1998, 281, 264, 4338, 7285, 295, 50880, 50880, 264, 2063, 2445, 13, 50972, 50972, 3146, 2539, 281, 5521, 437, 257, 731, 12, 45482, 11420, 295, 16235, 23475, 1542, 411, 11, 51204, 51204, 321, 486, 611, 11, 294, 257, 1780, 960, 11, 312, 1101, 1075, 281, 2826, 257, 665, 2539, 3314, 8961, 13, 51500, 51500, 961, 311, 747, 257, 574, 13, 51563, 51563, 1018, 257, 13548, 11, 510, 311, 264, 16235, 23475, 4978, 11, 293, 472, 295, 264, 2141, 7994, 307, 264, 3922, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.13307532092981172, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.007813142612576485}, {"id": 7, "seek": 2968, "start": 29.68, "end": 33.12, "text": " of the learning rate alpha.", "tokens": [50364, 295, 264, 2539, 3314, 8961, 13, 50536, 50536, 1692, 311, 746, 300, 286, 2049, 360, 281, 652, 988, 300, 16235, 23475, 307, 1364, 731, 13, 50796, 50796, 9647, 336, 300, 264, 1691, 295, 16235, 23475, 307, 281, 915, 9834, 261, 293, 272, 300, 4696, 51044, 51044, 17522, 264, 2063, 2445, 361, 13, 51188, 51188, 407, 437, 286, 603, 2049, 360, 307, 7542, 264, 2063, 2445, 361, 11, 597, 307, 15598, 322, 264, 3097, 992, 11, 51556, 51556, 293, 286, 603, 7542, 264, 2158, 295, 361, 412, 1184, 24784, 295, 16235, 23475, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.15251843134562174, "compression_ratio": 1.7916666666666667, "no_speech_prob": 2.8855378332082182e-05}, {"id": 8, "seek": 2968, "start": 33.12, "end": 38.32, "text": " Here's something that I often do to make sure that gradient descent is working well.", "tokens": [50364, 295, 264, 2539, 3314, 8961, 13, 50536, 50536, 1692, 311, 746, 300, 286, 2049, 360, 281, 652, 988, 300, 16235, 23475, 307, 1364, 731, 13, 50796, 50796, 9647, 336, 300, 264, 1691, 295, 16235, 23475, 307, 281, 915, 9834, 261, 293, 272, 300, 4696, 51044, 51044, 17522, 264, 2063, 2445, 361, 13, 51188, 51188, 407, 437, 286, 603, 2049, 360, 307, 7542, 264, 2063, 2445, 361, 11, 597, 307, 15598, 322, 264, 3097, 992, 11, 51556, 51556, 293, 286, 603, 7542, 264, 2158, 295, 361, 412, 1184, 24784, 295, 16235, 23475, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.15251843134562174, "compression_ratio": 1.7916666666666667, "no_speech_prob": 2.8855378332082182e-05}, {"id": 9, "seek": 2968, "start": 38.32, "end": 43.28, "text": " Recall that the job of gradient descent is to find parameters w and b that hopefully", "tokens": [50364, 295, 264, 2539, 3314, 8961, 13, 50536, 50536, 1692, 311, 746, 300, 286, 2049, 360, 281, 652, 988, 300, 16235, 23475, 307, 1364, 731, 13, 50796, 50796, 9647, 336, 300, 264, 1691, 295, 16235, 23475, 307, 281, 915, 9834, 261, 293, 272, 300, 4696, 51044, 51044, 17522, 264, 2063, 2445, 361, 13, 51188, 51188, 407, 437, 286, 603, 2049, 360, 307, 7542, 264, 2063, 2445, 361, 11, 597, 307, 15598, 322, 264, 3097, 992, 11, 51556, 51556, 293, 286, 603, 7542, 264, 2158, 295, 361, 412, 1184, 24784, 295, 16235, 23475, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.15251843134562174, "compression_ratio": 1.7916666666666667, "no_speech_prob": 2.8855378332082182e-05}, {"id": 10, "seek": 2968, "start": 43.28, "end": 46.16, "text": " minimize the cost function j.", "tokens": [50364, 295, 264, 2539, 3314, 8961, 13, 50536, 50536, 1692, 311, 746, 300, 286, 2049, 360, 281, 652, 988, 300, 16235, 23475, 307, 1364, 731, 13, 50796, 50796, 9647, 336, 300, 264, 1691, 295, 16235, 23475, 307, 281, 915, 9834, 261, 293, 272, 300, 4696, 51044, 51044, 17522, 264, 2063, 2445, 361, 13, 51188, 51188, 407, 437, 286, 603, 2049, 360, 307, 7542, 264, 2063, 2445, 361, 11, 597, 307, 15598, 322, 264, 3097, 992, 11, 51556, 51556, 293, 286, 603, 7542, 264, 2158, 295, 361, 412, 1184, 24784, 295, 16235, 23475, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.15251843134562174, "compression_ratio": 1.7916666666666667, "no_speech_prob": 2.8855378332082182e-05}, {"id": 11, "seek": 2968, "start": 46.16, "end": 53.519999999999996, "text": " So what I'll often do is plot the cost function j, which is calculated on the training set,", "tokens": [50364, 295, 264, 2539, 3314, 8961, 13, 50536, 50536, 1692, 311, 746, 300, 286, 2049, 360, 281, 652, 988, 300, 16235, 23475, 307, 1364, 731, 13, 50796, 50796, 9647, 336, 300, 264, 1691, 295, 16235, 23475, 307, 281, 915, 9834, 261, 293, 272, 300, 4696, 51044, 51044, 17522, 264, 2063, 2445, 361, 13, 51188, 51188, 407, 437, 286, 603, 2049, 360, 307, 7542, 264, 2063, 2445, 361, 11, 597, 307, 15598, 322, 264, 3097, 992, 11, 51556, 51556, 293, 286, 603, 7542, 264, 2158, 295, 361, 412, 1184, 24784, 295, 16235, 23475, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.15251843134562174, "compression_ratio": 1.7916666666666667, "no_speech_prob": 2.8855378332082182e-05}, {"id": 12, "seek": 2968, "start": 53.519999999999996, "end": 59.480000000000004, "text": " and I'll plot the value of j at each iteration of gradient descent.", "tokens": [50364, 295, 264, 2539, 3314, 8961, 13, 50536, 50536, 1692, 311, 746, 300, 286, 2049, 360, 281, 652, 988, 300, 16235, 23475, 307, 1364, 731, 13, 50796, 50796, 9647, 336, 300, 264, 1691, 295, 16235, 23475, 307, 281, 915, 9834, 261, 293, 272, 300, 4696, 51044, 51044, 17522, 264, 2063, 2445, 361, 13, 51188, 51188, 407, 437, 286, 603, 2049, 360, 307, 7542, 264, 2063, 2445, 361, 11, 597, 307, 15598, 322, 264, 3097, 992, 11, 51556, 51556, 293, 286, 603, 7542, 264, 2158, 295, 361, 412, 1184, 24784, 295, 16235, 23475, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.15251843134562174, "compression_ratio": 1.7916666666666667, "no_speech_prob": 2.8855378332082182e-05}, {"id": 13, "seek": 5948, "start": 59.48, "end": 66.36, "text": " Remember that each iteration means after each simultaneous update of the parameters w and", "tokens": [50364, 5459, 300, 1184, 24784, 1355, 934, 1184, 46218, 5623, 295, 264, 9834, 261, 293, 50708, 50708, 272, 11, 370, 294, 341, 7542, 11, 264, 12750, 10298, 307, 264, 1230, 295, 36540, 295, 16235, 23475, 51121, 51121, 300, 291, 600, 1190, 370, 1400, 13, 51248, 51248, 400, 370, 291, 815, 483, 257, 7605, 300, 1542, 411, 341, 13, 51436, 51436, 13428, 300, 264, 12750, 10298, 307, 264, 1230, 295, 36540, 295, 16235, 23475, 11, 293, 406, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.0989670632760736, "compression_ratio": 1.8225806451612903, "no_speech_prob": 9.972790394385811e-06}, {"id": 14, "seek": 5948, "start": 66.36, "end": 74.62, "text": " b, so in this plot, the horizontal axis is the number of iterations of gradient descent", "tokens": [50364, 5459, 300, 1184, 24784, 1355, 934, 1184, 46218, 5623, 295, 264, 9834, 261, 293, 50708, 50708, 272, 11, 370, 294, 341, 7542, 11, 264, 12750, 10298, 307, 264, 1230, 295, 36540, 295, 16235, 23475, 51121, 51121, 300, 291, 600, 1190, 370, 1400, 13, 51248, 51248, 400, 370, 291, 815, 483, 257, 7605, 300, 1542, 411, 341, 13, 51436, 51436, 13428, 300, 264, 12750, 10298, 307, 264, 1230, 295, 36540, 295, 16235, 23475, 11, 293, 406, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.0989670632760736, "compression_ratio": 1.8225806451612903, "no_speech_prob": 9.972790394385811e-06}, {"id": 15, "seek": 5948, "start": 74.62, "end": 77.16, "text": " that you've run so far.", "tokens": [50364, 5459, 300, 1184, 24784, 1355, 934, 1184, 46218, 5623, 295, 264, 9834, 261, 293, 50708, 50708, 272, 11, 370, 294, 341, 7542, 11, 264, 12750, 10298, 307, 264, 1230, 295, 36540, 295, 16235, 23475, 51121, 51121, 300, 291, 600, 1190, 370, 1400, 13, 51248, 51248, 400, 370, 291, 815, 483, 257, 7605, 300, 1542, 411, 341, 13, 51436, 51436, 13428, 300, 264, 12750, 10298, 307, 264, 1230, 295, 36540, 295, 16235, 23475, 11, 293, 406, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.0989670632760736, "compression_ratio": 1.8225806451612903, "no_speech_prob": 9.972790394385811e-06}, {"id": 16, "seek": 5948, "start": 77.16, "end": 80.92, "text": " And so you may get a curve that looks like this.", "tokens": [50364, 5459, 300, 1184, 24784, 1355, 934, 1184, 46218, 5623, 295, 264, 9834, 261, 293, 50708, 50708, 272, 11, 370, 294, 341, 7542, 11, 264, 12750, 10298, 307, 264, 1230, 295, 36540, 295, 16235, 23475, 51121, 51121, 300, 291, 600, 1190, 370, 1400, 13, 51248, 51248, 400, 370, 291, 815, 483, 257, 7605, 300, 1542, 411, 341, 13, 51436, 51436, 13428, 300, 264, 12750, 10298, 307, 264, 1230, 295, 36540, 295, 16235, 23475, 11, 293, 406, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.0989670632760736, "compression_ratio": 1.8225806451612903, "no_speech_prob": 9.972790394385811e-06}, {"id": 17, "seek": 5948, "start": 80.92, "end": 86.44, "text": " Notice that the horizontal axis is the number of iterations of gradient descent, and not", "tokens": [50364, 5459, 300, 1184, 24784, 1355, 934, 1184, 46218, 5623, 295, 264, 9834, 261, 293, 50708, 50708, 272, 11, 370, 294, 341, 7542, 11, 264, 12750, 10298, 307, 264, 1230, 295, 36540, 295, 16235, 23475, 51121, 51121, 300, 291, 600, 1190, 370, 1400, 13, 51248, 51248, 400, 370, 291, 815, 483, 257, 7605, 300, 1542, 411, 341, 13, 51436, 51436, 13428, 300, 264, 12750, 10298, 307, 264, 1230, 295, 36540, 295, 16235, 23475, 11, 293, 406, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.0989670632760736, "compression_ratio": 1.8225806451612903, "no_speech_prob": 9.972790394385811e-06}, {"id": 18, "seek": 8644, "start": 86.44, "end": 90.16, "text": " a parameter like w or b.", "tokens": [50364, 257, 13075, 411, 261, 420, 272, 13, 50550, 50550, 639, 37761, 490, 3894, 24877, 291, 600, 1612, 11, 689, 264, 9429, 10298, 390, 2063, 361, 11, 293, 264, 50882, 50882, 12750, 10298, 390, 257, 2167, 13075, 411, 261, 420, 272, 13, 51194, 51194, 639, 7605, 307, 611, 1219, 257, 2539, 7605, 13, 51390, 51390, 11633, 300, 456, 366, 257, 1326, 819, 3467, 295, 2539, 19490, 1143, 294, 3479, 2539, 11, 51644, 51644, 293, 291, 603, 536, 512, 295, 264, 3467, 1780, 294, 341, 1164, 382, 731, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.12463647192651099, "compression_ratio": 1.7355769230769231, "no_speech_prob": 3.3931305551959667e-06}, {"id": 19, "seek": 8644, "start": 90.16, "end": 96.8, "text": " This differs from previous graphs you've seen, where the vertical axis was cost j, and the", "tokens": [50364, 257, 13075, 411, 261, 420, 272, 13, 50550, 50550, 639, 37761, 490, 3894, 24877, 291, 600, 1612, 11, 689, 264, 9429, 10298, 390, 2063, 361, 11, 293, 264, 50882, 50882, 12750, 10298, 390, 257, 2167, 13075, 411, 261, 420, 272, 13, 51194, 51194, 639, 7605, 307, 611, 1219, 257, 2539, 7605, 13, 51390, 51390, 11633, 300, 456, 366, 257, 1326, 819, 3467, 295, 2539, 19490, 1143, 294, 3479, 2539, 11, 51644, 51644, 293, 291, 603, 536, 512, 295, 264, 3467, 1780, 294, 341, 1164, 382, 731, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.12463647192651099, "compression_ratio": 1.7355769230769231, "no_speech_prob": 3.3931305551959667e-06}, {"id": 20, "seek": 8644, "start": 96.8, "end": 103.03999999999999, "text": " horizontal axis was a single parameter like w or b.", "tokens": [50364, 257, 13075, 411, 261, 420, 272, 13, 50550, 50550, 639, 37761, 490, 3894, 24877, 291, 600, 1612, 11, 689, 264, 9429, 10298, 390, 2063, 361, 11, 293, 264, 50882, 50882, 12750, 10298, 390, 257, 2167, 13075, 411, 261, 420, 272, 13, 51194, 51194, 639, 7605, 307, 611, 1219, 257, 2539, 7605, 13, 51390, 51390, 11633, 300, 456, 366, 257, 1326, 819, 3467, 295, 2539, 19490, 1143, 294, 3479, 2539, 11, 51644, 51644, 293, 291, 603, 536, 512, 295, 264, 3467, 1780, 294, 341, 1164, 382, 731, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.12463647192651099, "compression_ratio": 1.7355769230769231, "no_speech_prob": 3.3931305551959667e-06}, {"id": 21, "seek": 8644, "start": 103.03999999999999, "end": 106.96, "text": " This curve is also called a learning curve.", "tokens": [50364, 257, 13075, 411, 261, 420, 272, 13, 50550, 50550, 639, 37761, 490, 3894, 24877, 291, 600, 1612, 11, 689, 264, 9429, 10298, 390, 2063, 361, 11, 293, 264, 50882, 50882, 12750, 10298, 390, 257, 2167, 13075, 411, 261, 420, 272, 13, 51194, 51194, 639, 7605, 307, 611, 1219, 257, 2539, 7605, 13, 51390, 51390, 11633, 300, 456, 366, 257, 1326, 819, 3467, 295, 2539, 19490, 1143, 294, 3479, 2539, 11, 51644, 51644, 293, 291, 603, 536, 512, 295, 264, 3467, 1780, 294, 341, 1164, 382, 731, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.12463647192651099, "compression_ratio": 1.7355769230769231, "no_speech_prob": 3.3931305551959667e-06}, {"id": 22, "seek": 8644, "start": 106.96, "end": 112.03999999999999, "text": " Note that there are a few different types of learning curves used in machine learning,", "tokens": [50364, 257, 13075, 411, 261, 420, 272, 13, 50550, 50550, 639, 37761, 490, 3894, 24877, 291, 600, 1612, 11, 689, 264, 9429, 10298, 390, 2063, 361, 11, 293, 264, 50882, 50882, 12750, 10298, 390, 257, 2167, 13075, 411, 261, 420, 272, 13, 51194, 51194, 639, 7605, 307, 611, 1219, 257, 2539, 7605, 13, 51390, 51390, 11633, 300, 456, 366, 257, 1326, 819, 3467, 295, 2539, 19490, 1143, 294, 3479, 2539, 11, 51644, 51644, 293, 291, 603, 536, 512, 295, 264, 3467, 1780, 294, 341, 1164, 382, 731, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.12463647192651099, "compression_ratio": 1.7355769230769231, "no_speech_prob": 3.3931305551959667e-06}, {"id": 23, "seek": 8644, "start": 112.03999999999999, "end": 116.36, "text": " and you'll see some of the types later in this course as well.", "tokens": [50364, 257, 13075, 411, 261, 420, 272, 13, 50550, 50550, 639, 37761, 490, 3894, 24877, 291, 600, 1612, 11, 689, 264, 9429, 10298, 390, 2063, 361, 11, 293, 264, 50882, 50882, 12750, 10298, 390, 257, 2167, 13075, 411, 261, 420, 272, 13, 51194, 51194, 639, 7605, 307, 611, 1219, 257, 2539, 7605, 13, 51390, 51390, 11633, 300, 456, 366, 257, 1326, 819, 3467, 295, 2539, 19490, 1143, 294, 3479, 2539, 11, 51644, 51644, 293, 291, 603, 536, 512, 295, 264, 3467, 1780, 294, 341, 1164, 382, 731, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.12463647192651099, "compression_ratio": 1.7355769230769231, "no_speech_prob": 3.3931305551959667e-06}, {"id": 24, "seek": 11636, "start": 116.36, "end": 122.68, "text": " Secondly, if you look at this point on the curve, this means that after you've run gradient", "tokens": [50364, 19483, 11, 498, 291, 574, 412, 341, 935, 322, 264, 7605, 11, 341, 1355, 300, 934, 291, 600, 1190, 16235, 50680, 50680, 23475, 337, 2319, 36540, 11, 3620, 2319, 46218, 9205, 295, 264, 9834, 11, 291, 362, 512, 3264, 51046, 51046, 4190, 337, 261, 293, 272, 13, 51200, 51200, 400, 498, 291, 14722, 264, 2063, 11, 361, 86, 65, 11, 337, 729, 4190, 295, 261, 293, 272, 11, 264, 2306, 291, 658, 934, 51586, 51586], "temperature": 0.0, "avg_logprob": -0.14760042165780995, "compression_ratio": 1.5572916666666667, "no_speech_prob": 5.093666459288215e-06}, {"id": 25, "seek": 11636, "start": 122.68, "end": 130.0, "text": " descent for 100 iterations, meaning 100 simultaneous updates of the parameters, you have some learned", "tokens": [50364, 19483, 11, 498, 291, 574, 412, 341, 935, 322, 264, 7605, 11, 341, 1355, 300, 934, 291, 600, 1190, 16235, 50680, 50680, 23475, 337, 2319, 36540, 11, 3620, 2319, 46218, 9205, 295, 264, 9834, 11, 291, 362, 512, 3264, 51046, 51046, 4190, 337, 261, 293, 272, 13, 51200, 51200, 400, 498, 291, 14722, 264, 2063, 11, 361, 86, 65, 11, 337, 729, 4190, 295, 261, 293, 272, 11, 264, 2306, 291, 658, 934, 51586, 51586], "temperature": 0.0, "avg_logprob": -0.14760042165780995, "compression_ratio": 1.5572916666666667, "no_speech_prob": 5.093666459288215e-06}, {"id": 26, "seek": 11636, "start": 130.0, "end": 133.07999999999998, "text": " values for w and b.", "tokens": [50364, 19483, 11, 498, 291, 574, 412, 341, 935, 322, 264, 7605, 11, 341, 1355, 300, 934, 291, 600, 1190, 16235, 50680, 50680, 23475, 337, 2319, 36540, 11, 3620, 2319, 46218, 9205, 295, 264, 9834, 11, 291, 362, 512, 3264, 51046, 51046, 4190, 337, 261, 293, 272, 13, 51200, 51200, 400, 498, 291, 14722, 264, 2063, 11, 361, 86, 65, 11, 337, 729, 4190, 295, 261, 293, 272, 11, 264, 2306, 291, 658, 934, 51586, 51586], "temperature": 0.0, "avg_logprob": -0.14760042165780995, "compression_ratio": 1.5572916666666667, "no_speech_prob": 5.093666459288215e-06}, {"id": 27, "seek": 11636, "start": 133.07999999999998, "end": 140.8, "text": " And if you compute the cost, jwb, for those values of w and b, the ones you got after", "tokens": [50364, 19483, 11, 498, 291, 574, 412, 341, 935, 322, 264, 7605, 11, 341, 1355, 300, 934, 291, 600, 1190, 16235, 50680, 50680, 23475, 337, 2319, 36540, 11, 3620, 2319, 46218, 9205, 295, 264, 9834, 11, 291, 362, 512, 3264, 51046, 51046, 4190, 337, 261, 293, 272, 13, 51200, 51200, 400, 498, 291, 14722, 264, 2063, 11, 361, 86, 65, 11, 337, 729, 4190, 295, 261, 293, 272, 11, 264, 2306, 291, 658, 934, 51586, 51586], "temperature": 0.0, "avg_logprob": -0.14760042165780995, "compression_ratio": 1.5572916666666667, "no_speech_prob": 5.093666459288215e-06}, {"id": 28, "seek": 14080, "start": 140.8, "end": 148.4, "text": " 100 iterations, you get this value for the cost j, that is, this point on the vertical", "tokens": [50364, 2319, 36540, 11, 291, 483, 341, 2158, 337, 264, 2063, 361, 11, 300, 307, 11, 341, 935, 322, 264, 9429, 50744, 50744, 10298, 13, 50826, 50826, 400, 341, 935, 510, 23249, 281, 264, 2158, 295, 361, 337, 264, 9834, 300, 291, 658, 934, 51152, 51152, 2331, 36540, 295, 16235, 23475, 13, 51329, 51329, 407, 1237, 412, 341, 4295, 3665, 291, 281, 536, 577, 428, 2063, 361, 2962, 934, 1184, 24784, 51637, 51637, 295, 16235, 23475, 13, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.1224367618560791, "compression_ratio": 1.7513513513513514, "no_speech_prob": 7.112417961252504e-07}, {"id": 29, "seek": 14080, "start": 148.4, "end": 150.04000000000002, "text": " axis.", "tokens": [50364, 2319, 36540, 11, 291, 483, 341, 2158, 337, 264, 2063, 361, 11, 300, 307, 11, 341, 935, 322, 264, 9429, 50744, 50744, 10298, 13, 50826, 50826, 400, 341, 935, 510, 23249, 281, 264, 2158, 295, 361, 337, 264, 9834, 300, 291, 658, 934, 51152, 51152, 2331, 36540, 295, 16235, 23475, 13, 51329, 51329, 407, 1237, 412, 341, 4295, 3665, 291, 281, 536, 577, 428, 2063, 361, 2962, 934, 1184, 24784, 51637, 51637, 295, 16235, 23475, 13, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.1224367618560791, "compression_ratio": 1.7513513513513514, "no_speech_prob": 7.112417961252504e-07}, {"id": 30, "seek": 14080, "start": 150.04000000000002, "end": 156.56, "text": " And this point here corresponds to the value of j for the parameters that you got after", "tokens": [50364, 2319, 36540, 11, 291, 483, 341, 2158, 337, 264, 2063, 361, 11, 300, 307, 11, 341, 935, 322, 264, 9429, 50744, 50744, 10298, 13, 50826, 50826, 400, 341, 935, 510, 23249, 281, 264, 2158, 295, 361, 337, 264, 9834, 300, 291, 658, 934, 51152, 51152, 2331, 36540, 295, 16235, 23475, 13, 51329, 51329, 407, 1237, 412, 341, 4295, 3665, 291, 281, 536, 577, 428, 2063, 361, 2962, 934, 1184, 24784, 51637, 51637, 295, 16235, 23475, 13, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.1224367618560791, "compression_ratio": 1.7513513513513514, "no_speech_prob": 7.112417961252504e-07}, {"id": 31, "seek": 14080, "start": 156.56, "end": 160.10000000000002, "text": " 200 iterations of gradient descent.", "tokens": [50364, 2319, 36540, 11, 291, 483, 341, 2158, 337, 264, 2063, 361, 11, 300, 307, 11, 341, 935, 322, 264, 9429, 50744, 50744, 10298, 13, 50826, 50826, 400, 341, 935, 510, 23249, 281, 264, 2158, 295, 361, 337, 264, 9834, 300, 291, 658, 934, 51152, 51152, 2331, 36540, 295, 16235, 23475, 13, 51329, 51329, 407, 1237, 412, 341, 4295, 3665, 291, 281, 536, 577, 428, 2063, 361, 2962, 934, 1184, 24784, 51637, 51637, 295, 16235, 23475, 13, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.1224367618560791, "compression_ratio": 1.7513513513513514, "no_speech_prob": 7.112417961252504e-07}, {"id": 32, "seek": 14080, "start": 160.10000000000002, "end": 166.26000000000002, "text": " So looking at this graph helps you to see how your cost j changes after each iteration", "tokens": [50364, 2319, 36540, 11, 291, 483, 341, 2158, 337, 264, 2063, 361, 11, 300, 307, 11, 341, 935, 322, 264, 9429, 50744, 50744, 10298, 13, 50826, 50826, 400, 341, 935, 510, 23249, 281, 264, 2158, 295, 361, 337, 264, 9834, 300, 291, 658, 934, 51152, 51152, 2331, 36540, 295, 16235, 23475, 13, 51329, 51329, 407, 1237, 412, 341, 4295, 3665, 291, 281, 536, 577, 428, 2063, 361, 2962, 934, 1184, 24784, 51637, 51637, 295, 16235, 23475, 13, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.1224367618560791, "compression_ratio": 1.7513513513513514, "no_speech_prob": 7.112417961252504e-07}, {"id": 33, "seek": 14080, "start": 166.26000000000002, "end": 168.18, "text": " of gradient descent.", "tokens": [50364, 2319, 36540, 11, 291, 483, 341, 2158, 337, 264, 2063, 361, 11, 300, 307, 11, 341, 935, 322, 264, 9429, 50744, 50744, 10298, 13, 50826, 50826, 400, 341, 935, 510, 23249, 281, 264, 2158, 295, 361, 337, 264, 9834, 300, 291, 658, 934, 51152, 51152, 2331, 36540, 295, 16235, 23475, 13, 51329, 51329, 407, 1237, 412, 341, 4295, 3665, 291, 281, 536, 577, 428, 2063, 361, 2962, 934, 1184, 24784, 51637, 51637, 295, 16235, 23475, 13, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.1224367618560791, "compression_ratio": 1.7513513513513514, "no_speech_prob": 7.112417961252504e-07}, {"id": 34, "seek": 16818, "start": 168.18, "end": 173.48000000000002, "text": " If gradient descent is working properly, then the cost j should decrease after every single", "tokens": [50364, 759, 16235, 23475, 307, 1364, 6108, 11, 550, 264, 2063, 361, 820, 11514, 934, 633, 2167, 50629, 50629, 24784, 13, 50715, 50715, 759, 361, 1562, 8637, 934, 472, 24784, 11, 300, 1355, 2139, 8961, 307, 8614, 22271, 11, 293, 309, 51097, 51097, 2673, 1355, 8961, 307, 886, 2416, 11, 420, 456, 727, 312, 257, 7426, 294, 264, 3089, 13, 51369, 51369, 3996, 4420, 551, 300, 341, 7542, 393, 980, 291, 307, 300, 498, 291, 574, 412, 341, 7605, 11, 538, 51627, 51627], "temperature": 0.0, "avg_logprob": -0.09963514691307432, "compression_ratio": 1.674641148325359, "no_speech_prob": 4.785049895872362e-06}, {"id": 35, "seek": 16818, "start": 173.48000000000002, "end": 175.20000000000002, "text": " iteration.", "tokens": [50364, 759, 16235, 23475, 307, 1364, 6108, 11, 550, 264, 2063, 361, 820, 11514, 934, 633, 2167, 50629, 50629, 24784, 13, 50715, 50715, 759, 361, 1562, 8637, 934, 472, 24784, 11, 300, 1355, 2139, 8961, 307, 8614, 22271, 11, 293, 309, 51097, 51097, 2673, 1355, 8961, 307, 886, 2416, 11, 420, 456, 727, 312, 257, 7426, 294, 264, 3089, 13, 51369, 51369, 3996, 4420, 551, 300, 341, 7542, 393, 980, 291, 307, 300, 498, 291, 574, 412, 341, 7605, 11, 538, 51627, 51627], "temperature": 0.0, "avg_logprob": -0.09963514691307432, "compression_ratio": 1.674641148325359, "no_speech_prob": 4.785049895872362e-06}, {"id": 36, "seek": 16818, "start": 175.20000000000002, "end": 182.84, "text": " If j ever increases after one iteration, that means either alpha is chosen poorly, and it", "tokens": [50364, 759, 16235, 23475, 307, 1364, 6108, 11, 550, 264, 2063, 361, 820, 11514, 934, 633, 2167, 50629, 50629, 24784, 13, 50715, 50715, 759, 361, 1562, 8637, 934, 472, 24784, 11, 300, 1355, 2139, 8961, 307, 8614, 22271, 11, 293, 309, 51097, 51097, 2673, 1355, 8961, 307, 886, 2416, 11, 420, 456, 727, 312, 257, 7426, 294, 264, 3089, 13, 51369, 51369, 3996, 4420, 551, 300, 341, 7542, 393, 980, 291, 307, 300, 498, 291, 574, 412, 341, 7605, 11, 538, 51627, 51627], "temperature": 0.0, "avg_logprob": -0.09963514691307432, "compression_ratio": 1.674641148325359, "no_speech_prob": 4.785049895872362e-06}, {"id": 37, "seek": 16818, "start": 182.84, "end": 188.28, "text": " usually means alpha is too large, or there could be a bug in the code.", "tokens": [50364, 759, 16235, 23475, 307, 1364, 6108, 11, 550, 264, 2063, 361, 820, 11514, 934, 633, 2167, 50629, 50629, 24784, 13, 50715, 50715, 759, 361, 1562, 8637, 934, 472, 24784, 11, 300, 1355, 2139, 8961, 307, 8614, 22271, 11, 293, 309, 51097, 51097, 2673, 1355, 8961, 307, 886, 2416, 11, 420, 456, 727, 312, 257, 7426, 294, 264, 3089, 13, 51369, 51369, 3996, 4420, 551, 300, 341, 7542, 393, 980, 291, 307, 300, 498, 291, 574, 412, 341, 7605, 11, 538, 51627, 51627], "temperature": 0.0, "avg_logprob": -0.09963514691307432, "compression_ratio": 1.674641148325359, "no_speech_prob": 4.785049895872362e-06}, {"id": 38, "seek": 16818, "start": 188.28, "end": 193.44, "text": " Another useful thing that this plot can tell you is that if you look at this curve, by", "tokens": [50364, 759, 16235, 23475, 307, 1364, 6108, 11, 550, 264, 2063, 361, 820, 11514, 934, 633, 2167, 50629, 50629, 24784, 13, 50715, 50715, 759, 361, 1562, 8637, 934, 472, 24784, 11, 300, 1355, 2139, 8961, 307, 8614, 22271, 11, 293, 309, 51097, 51097, 2673, 1355, 8961, 307, 886, 2416, 11, 420, 456, 727, 312, 257, 7426, 294, 264, 3089, 13, 51369, 51369, 3996, 4420, 551, 300, 341, 7542, 393, 980, 291, 307, 300, 498, 291, 574, 412, 341, 7605, 11, 538, 51627, 51627], "temperature": 0.0, "avg_logprob": -0.09963514691307432, "compression_ratio": 1.674641148325359, "no_speech_prob": 4.785049895872362e-06}, {"id": 39, "seek": 19344, "start": 193.44, "end": 201.28, "text": " the time you reach maybe 300 iterations or so, the cost j is leveling off and is no longer", "tokens": [50364, 264, 565, 291, 2524, 1310, 6641, 36540, 420, 370, 11, 264, 2063, 361, 307, 40617, 766, 293, 307, 572, 2854, 50756, 50756, 23223, 709, 13, 50850, 50850, 400, 538, 8423, 36540, 11, 309, 1542, 411, 264, 7605, 575, 24183, 292, 484, 13, 51096, 51096, 407, 341, 1355, 300, 16235, 23475, 575, 544, 420, 1570, 9652, 3004, 570, 264, 7605, 307, 572, 51414, 51414, 2854, 23223, 13, 51548, 51548, 407, 1237, 412, 341, 2539, 7605, 11, 291, 393, 853, 281, 4008, 1968, 420, 406, 16235, 23475, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.08950644396664051, "compression_ratio": 1.7183098591549295, "no_speech_prob": 1.2679191740971874e-06}, {"id": 40, "seek": 19344, "start": 201.28, "end": 203.16, "text": " decreasing much.", "tokens": [50364, 264, 565, 291, 2524, 1310, 6641, 36540, 420, 370, 11, 264, 2063, 361, 307, 40617, 766, 293, 307, 572, 2854, 50756, 50756, 23223, 709, 13, 50850, 50850, 400, 538, 8423, 36540, 11, 309, 1542, 411, 264, 7605, 575, 24183, 292, 484, 13, 51096, 51096, 407, 341, 1355, 300, 16235, 23475, 575, 544, 420, 1570, 9652, 3004, 570, 264, 7605, 307, 572, 51414, 51414, 2854, 23223, 13, 51548, 51548, 407, 1237, 412, 341, 2539, 7605, 11, 291, 393, 853, 281, 4008, 1968, 420, 406, 16235, 23475, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.08950644396664051, "compression_ratio": 1.7183098591549295, "no_speech_prob": 1.2679191740971874e-06}, {"id": 41, "seek": 19344, "start": 203.16, "end": 208.07999999999998, "text": " And by 400 iterations, it looks like the curve has flattened out.", "tokens": [50364, 264, 565, 291, 2524, 1310, 6641, 36540, 420, 370, 11, 264, 2063, 361, 307, 40617, 766, 293, 307, 572, 2854, 50756, 50756, 23223, 709, 13, 50850, 50850, 400, 538, 8423, 36540, 11, 309, 1542, 411, 264, 7605, 575, 24183, 292, 484, 13, 51096, 51096, 407, 341, 1355, 300, 16235, 23475, 575, 544, 420, 1570, 9652, 3004, 570, 264, 7605, 307, 572, 51414, 51414, 2854, 23223, 13, 51548, 51548, 407, 1237, 412, 341, 2539, 7605, 11, 291, 393, 853, 281, 4008, 1968, 420, 406, 16235, 23475, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.08950644396664051, "compression_ratio": 1.7183098591549295, "no_speech_prob": 1.2679191740971874e-06}, {"id": 42, "seek": 19344, "start": 208.07999999999998, "end": 214.44, "text": " So this means that gradient descent has more or less converged because the curve is no", "tokens": [50364, 264, 565, 291, 2524, 1310, 6641, 36540, 420, 370, 11, 264, 2063, 361, 307, 40617, 766, 293, 307, 572, 2854, 50756, 50756, 23223, 709, 13, 50850, 50850, 400, 538, 8423, 36540, 11, 309, 1542, 411, 264, 7605, 575, 24183, 292, 484, 13, 51096, 51096, 407, 341, 1355, 300, 16235, 23475, 575, 544, 420, 1570, 9652, 3004, 570, 264, 7605, 307, 572, 51414, 51414, 2854, 23223, 13, 51548, 51548, 407, 1237, 412, 341, 2539, 7605, 11, 291, 393, 853, 281, 4008, 1968, 420, 406, 16235, 23475, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.08950644396664051, "compression_ratio": 1.7183098591549295, "no_speech_prob": 1.2679191740971874e-06}, {"id": 43, "seek": 19344, "start": 214.44, "end": 217.12, "text": " longer decreasing.", "tokens": [50364, 264, 565, 291, 2524, 1310, 6641, 36540, 420, 370, 11, 264, 2063, 361, 307, 40617, 766, 293, 307, 572, 2854, 50756, 50756, 23223, 709, 13, 50850, 50850, 400, 538, 8423, 36540, 11, 309, 1542, 411, 264, 7605, 575, 24183, 292, 484, 13, 51096, 51096, 407, 341, 1355, 300, 16235, 23475, 575, 544, 420, 1570, 9652, 3004, 570, 264, 7605, 307, 572, 51414, 51414, 2854, 23223, 13, 51548, 51548, 407, 1237, 412, 341, 2539, 7605, 11, 291, 393, 853, 281, 4008, 1968, 420, 406, 16235, 23475, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.08950644396664051, "compression_ratio": 1.7183098591549295, "no_speech_prob": 1.2679191740971874e-06}, {"id": 44, "seek": 19344, "start": 217.12, "end": 222.48, "text": " So looking at this learning curve, you can try to spot whether or not gradient descent", "tokens": [50364, 264, 565, 291, 2524, 1310, 6641, 36540, 420, 370, 11, 264, 2063, 361, 307, 40617, 766, 293, 307, 572, 2854, 50756, 50756, 23223, 709, 13, 50850, 50850, 400, 538, 8423, 36540, 11, 309, 1542, 411, 264, 7605, 575, 24183, 292, 484, 13, 51096, 51096, 407, 341, 1355, 300, 16235, 23475, 575, 544, 420, 1570, 9652, 3004, 570, 264, 7605, 307, 572, 51414, 51414, 2854, 23223, 13, 51548, 51548, 407, 1237, 412, 341, 2539, 7605, 11, 291, 393, 853, 281, 4008, 1968, 420, 406, 16235, 23475, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.08950644396664051, "compression_ratio": 1.7183098591549295, "no_speech_prob": 1.2679191740971874e-06}, {"id": 45, "seek": 22248, "start": 222.48, "end": 224.6, "text": " is converging.", "tokens": [50364, 307, 9652, 3249, 13, 50470, 50470, 3146, 264, 636, 11, 264, 1230, 295, 36540, 300, 16235, 23475, 2516, 281, 41881, 393, 10559, 50736, 50736, 257, 688, 1296, 819, 5821, 13, 50870, 50870, 682, 472, 3861, 11, 309, 815, 41881, 934, 445, 2217, 36540, 13, 51090, 51090, 1171, 257, 819, 3861, 11, 309, 727, 747, 257, 4714, 420, 257, 3262, 4714, 36540, 13, 51384, 51384, 467, 4523, 484, 281, 312, 588, 2252, 281, 980, 294, 7295, 577, 867, 36540, 16235, 23475, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.09188437461853027, "compression_ratio": 1.8086124401913874, "no_speech_prob": 1.6280407635349547e-06}, {"id": 46, "seek": 22248, "start": 224.6, "end": 229.92, "text": " By the way, the number of iterations that gradient descent takes to converge can vary", "tokens": [50364, 307, 9652, 3249, 13, 50470, 50470, 3146, 264, 636, 11, 264, 1230, 295, 36540, 300, 16235, 23475, 2516, 281, 41881, 393, 10559, 50736, 50736, 257, 688, 1296, 819, 5821, 13, 50870, 50870, 682, 472, 3861, 11, 309, 815, 41881, 934, 445, 2217, 36540, 13, 51090, 51090, 1171, 257, 819, 3861, 11, 309, 727, 747, 257, 4714, 420, 257, 3262, 4714, 36540, 13, 51384, 51384, 467, 4523, 484, 281, 312, 588, 2252, 281, 980, 294, 7295, 577, 867, 36540, 16235, 23475, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.09188437461853027, "compression_ratio": 1.8086124401913874, "no_speech_prob": 1.6280407635349547e-06}, {"id": 47, "seek": 22248, "start": 229.92, "end": 232.6, "text": " a lot between different applications.", "tokens": [50364, 307, 9652, 3249, 13, 50470, 50470, 3146, 264, 636, 11, 264, 1230, 295, 36540, 300, 16235, 23475, 2516, 281, 41881, 393, 10559, 50736, 50736, 257, 688, 1296, 819, 5821, 13, 50870, 50870, 682, 472, 3861, 11, 309, 815, 41881, 934, 445, 2217, 36540, 13, 51090, 51090, 1171, 257, 819, 3861, 11, 309, 727, 747, 257, 4714, 420, 257, 3262, 4714, 36540, 13, 51384, 51384, 467, 4523, 484, 281, 312, 588, 2252, 281, 980, 294, 7295, 577, 867, 36540, 16235, 23475, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.09188437461853027, "compression_ratio": 1.8086124401913874, "no_speech_prob": 1.6280407635349547e-06}, {"id": 48, "seek": 22248, "start": 232.6, "end": 237.0, "text": " In one application, it may converge after just 30 iterations.", "tokens": [50364, 307, 9652, 3249, 13, 50470, 50470, 3146, 264, 636, 11, 264, 1230, 295, 36540, 300, 16235, 23475, 2516, 281, 41881, 393, 10559, 50736, 50736, 257, 688, 1296, 819, 5821, 13, 50870, 50870, 682, 472, 3861, 11, 309, 815, 41881, 934, 445, 2217, 36540, 13, 51090, 51090, 1171, 257, 819, 3861, 11, 309, 727, 747, 257, 4714, 420, 257, 3262, 4714, 36540, 13, 51384, 51384, 467, 4523, 484, 281, 312, 588, 2252, 281, 980, 294, 7295, 577, 867, 36540, 16235, 23475, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.09188437461853027, "compression_ratio": 1.8086124401913874, "no_speech_prob": 1.6280407635349547e-06}, {"id": 49, "seek": 22248, "start": 237.0, "end": 242.88, "text": " For a different application, it could take a thousand or a hundred thousand iterations.", "tokens": [50364, 307, 9652, 3249, 13, 50470, 50470, 3146, 264, 636, 11, 264, 1230, 295, 36540, 300, 16235, 23475, 2516, 281, 41881, 393, 10559, 50736, 50736, 257, 688, 1296, 819, 5821, 13, 50870, 50870, 682, 472, 3861, 11, 309, 815, 41881, 934, 445, 2217, 36540, 13, 51090, 51090, 1171, 257, 819, 3861, 11, 309, 727, 747, 257, 4714, 420, 257, 3262, 4714, 36540, 13, 51384, 51384, 467, 4523, 484, 281, 312, 588, 2252, 281, 980, 294, 7295, 577, 867, 36540, 16235, 23475, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.09188437461853027, "compression_ratio": 1.8086124401913874, "no_speech_prob": 1.6280407635349547e-06}, {"id": 50, "seek": 22248, "start": 242.88, "end": 249.23999999999998, "text": " It turns out to be very difficult to tell in advance how many iterations gradient descent", "tokens": [50364, 307, 9652, 3249, 13, 50470, 50470, 3146, 264, 636, 11, 264, 1230, 295, 36540, 300, 16235, 23475, 2516, 281, 41881, 393, 10559, 50736, 50736, 257, 688, 1296, 819, 5821, 13, 50870, 50870, 682, 472, 3861, 11, 309, 815, 41881, 934, 445, 2217, 36540, 13, 51090, 51090, 1171, 257, 819, 3861, 11, 309, 727, 747, 257, 4714, 420, 257, 3262, 4714, 36540, 13, 51384, 51384, 467, 4523, 484, 281, 312, 588, 2252, 281, 980, 294, 7295, 577, 867, 36540, 16235, 23475, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.09188437461853027, "compression_ratio": 1.8086124401913874, "no_speech_prob": 1.6280407635349547e-06}, {"id": 51, "seek": 24924, "start": 249.24, "end": 255.8, "text": " needs to converge, which is why you can create a graph like this, a learning curve, to try", "tokens": [50364, 2203, 281, 41881, 11, 597, 307, 983, 291, 393, 1884, 257, 4295, 411, 341, 11, 257, 2539, 7605, 11, 281, 853, 50692, 50692, 281, 915, 484, 562, 291, 393, 1590, 3097, 428, 1729, 2316, 13, 50958, 50958, 3996, 636, 281, 4536, 562, 428, 2316, 307, 1096, 3097, 307, 365, 364, 12509, 32181, 51236, 51236, 1500, 13, 51314, 51314, 407, 718, 311, 718, 17889, 11, 341, 510, 307, 264, 10281, 23339, 17889, 11, 718, 311, 718, 17889, 312, 257, 7006, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.10050584034747388, "compression_ratio": 1.6715686274509804, "no_speech_prob": 1.0952925549645443e-05}, {"id": 52, "seek": 24924, "start": 255.8, "end": 261.12, "text": " to find out when you can stop training your particular model.", "tokens": [50364, 2203, 281, 41881, 11, 597, 307, 983, 291, 393, 1884, 257, 4295, 411, 341, 11, 257, 2539, 7605, 11, 281, 853, 50692, 50692, 281, 915, 484, 562, 291, 393, 1590, 3097, 428, 1729, 2316, 13, 50958, 50958, 3996, 636, 281, 4536, 562, 428, 2316, 307, 1096, 3097, 307, 365, 364, 12509, 32181, 51236, 51236, 1500, 13, 51314, 51314, 407, 718, 311, 718, 17889, 11, 341, 510, 307, 264, 10281, 23339, 17889, 11, 718, 311, 718, 17889, 312, 257, 7006, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.10050584034747388, "compression_ratio": 1.6715686274509804, "no_speech_prob": 1.0952925549645443e-05}, {"id": 53, "seek": 24924, "start": 261.12, "end": 266.68, "text": " Another way to decide when your model is done training is with an automatic convergence", "tokens": [50364, 2203, 281, 41881, 11, 597, 307, 983, 291, 393, 1884, 257, 4295, 411, 341, 11, 257, 2539, 7605, 11, 281, 853, 50692, 50692, 281, 915, 484, 562, 291, 393, 1590, 3097, 428, 1729, 2316, 13, 50958, 50958, 3996, 636, 281, 4536, 562, 428, 2316, 307, 1096, 3097, 307, 365, 364, 12509, 32181, 51236, 51236, 1500, 13, 51314, 51314, 407, 718, 311, 718, 17889, 11, 341, 510, 307, 264, 10281, 23339, 17889, 11, 718, 311, 718, 17889, 312, 257, 7006, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.10050584034747388, "compression_ratio": 1.6715686274509804, "no_speech_prob": 1.0952925549645443e-05}, {"id": 54, "seek": 24924, "start": 266.68, "end": 268.24, "text": " test.", "tokens": [50364, 2203, 281, 41881, 11, 597, 307, 983, 291, 393, 1884, 257, 4295, 411, 341, 11, 257, 2539, 7605, 11, 281, 853, 50692, 50692, 281, 915, 484, 562, 291, 393, 1590, 3097, 428, 1729, 2316, 13, 50958, 50958, 3996, 636, 281, 4536, 562, 428, 2316, 307, 1096, 3097, 307, 365, 364, 12509, 32181, 51236, 51236, 1500, 13, 51314, 51314, 407, 718, 311, 718, 17889, 11, 341, 510, 307, 264, 10281, 23339, 17889, 11, 718, 311, 718, 17889, 312, 257, 7006, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.10050584034747388, "compression_ratio": 1.6715686274509804, "no_speech_prob": 1.0952925549645443e-05}, {"id": 55, "seek": 24924, "start": 268.24, "end": 276.16, "text": " So let's let epsilon, this here is the Greek alphabet epsilon, let's let epsilon be a variable", "tokens": [50364, 2203, 281, 41881, 11, 597, 307, 983, 291, 393, 1884, 257, 4295, 411, 341, 11, 257, 2539, 7605, 11, 281, 853, 50692, 50692, 281, 915, 484, 562, 291, 393, 1590, 3097, 428, 1729, 2316, 13, 50958, 50958, 3996, 636, 281, 4536, 562, 428, 2316, 307, 1096, 3097, 307, 365, 364, 12509, 32181, 51236, 51236, 1500, 13, 51314, 51314, 407, 718, 311, 718, 17889, 11, 341, 510, 307, 264, 10281, 23339, 17889, 11, 718, 311, 718, 17889, 312, 257, 7006, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.10050584034747388, "compression_ratio": 1.6715686274509804, "no_speech_prob": 1.0952925549645443e-05}, {"id": 56, "seek": 27616, "start": 276.16, "end": 283.92, "text": " representing a small number, such as 0.001 or 10 to the power of negative three.", "tokens": [50364, 13460, 257, 1359, 1230, 11, 1270, 382, 1958, 13, 628, 16, 420, 1266, 281, 264, 1347, 295, 3671, 1045, 13, 50752, 50752, 759, 264, 2063, 508, 24108, 538, 1570, 813, 341, 1230, 17889, 322, 472, 24784, 11, 550, 291, 434, 51022, 51022, 3700, 322, 341, 24183, 292, 644, 295, 264, 7605, 300, 291, 536, 322, 264, 1411, 293, 291, 393, 19710, 51294, 51294, 32181, 13, 51364, 51364, 5459, 11, 32181, 4696, 16203, 300, 291, 1352, 9834, 343, 293, 363, 300, 366, 51646, 51646, 1998, 281, 264, 7285, 1944, 2158, 295, 508, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.14625183741251627, "compression_ratio": 1.6322314049586777, "no_speech_prob": 6.540252798004076e-06}, {"id": 57, "seek": 27616, "start": 283.92, "end": 289.32000000000005, "text": " If the cost J decreases by less than this number epsilon on one iteration, then you're", "tokens": [50364, 13460, 257, 1359, 1230, 11, 1270, 382, 1958, 13, 628, 16, 420, 1266, 281, 264, 1347, 295, 3671, 1045, 13, 50752, 50752, 759, 264, 2063, 508, 24108, 538, 1570, 813, 341, 1230, 17889, 322, 472, 24784, 11, 550, 291, 434, 51022, 51022, 3700, 322, 341, 24183, 292, 644, 295, 264, 7605, 300, 291, 536, 322, 264, 1411, 293, 291, 393, 19710, 51294, 51294, 32181, 13, 51364, 51364, 5459, 11, 32181, 4696, 16203, 300, 291, 1352, 9834, 343, 293, 363, 300, 366, 51646, 51646, 1998, 281, 264, 7285, 1944, 2158, 295, 508, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.14625183741251627, "compression_ratio": 1.6322314049586777, "no_speech_prob": 6.540252798004076e-06}, {"id": 58, "seek": 27616, "start": 289.32000000000005, "end": 294.76000000000005, "text": " likely on this flattened part of the curve that you see on the left and you can declare", "tokens": [50364, 13460, 257, 1359, 1230, 11, 1270, 382, 1958, 13, 628, 16, 420, 1266, 281, 264, 1347, 295, 3671, 1045, 13, 50752, 50752, 759, 264, 2063, 508, 24108, 538, 1570, 813, 341, 1230, 17889, 322, 472, 24784, 11, 550, 291, 434, 51022, 51022, 3700, 322, 341, 24183, 292, 644, 295, 264, 7605, 300, 291, 536, 322, 264, 1411, 293, 291, 393, 19710, 51294, 51294, 32181, 13, 51364, 51364, 5459, 11, 32181, 4696, 16203, 300, 291, 1352, 9834, 343, 293, 363, 300, 366, 51646, 51646, 1998, 281, 264, 7285, 1944, 2158, 295, 508, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.14625183741251627, "compression_ratio": 1.6322314049586777, "no_speech_prob": 6.540252798004076e-06}, {"id": 59, "seek": 27616, "start": 294.76000000000005, "end": 296.16, "text": " convergence.", "tokens": [50364, 13460, 257, 1359, 1230, 11, 1270, 382, 1958, 13, 628, 16, 420, 1266, 281, 264, 1347, 295, 3671, 1045, 13, 50752, 50752, 759, 264, 2063, 508, 24108, 538, 1570, 813, 341, 1230, 17889, 322, 472, 24784, 11, 550, 291, 434, 51022, 51022, 3700, 322, 341, 24183, 292, 644, 295, 264, 7605, 300, 291, 536, 322, 264, 1411, 293, 291, 393, 19710, 51294, 51294, 32181, 13, 51364, 51364, 5459, 11, 32181, 4696, 16203, 300, 291, 1352, 9834, 343, 293, 363, 300, 366, 51646, 51646, 1998, 281, 264, 7285, 1944, 2158, 295, 508, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.14625183741251627, "compression_ratio": 1.6322314049586777, "no_speech_prob": 6.540252798004076e-06}, {"id": 60, "seek": 27616, "start": 296.16, "end": 301.8, "text": " Remember, convergence hopefully indicates that you found parameters W and B that are", "tokens": [50364, 13460, 257, 1359, 1230, 11, 1270, 382, 1958, 13, 628, 16, 420, 1266, 281, 264, 1347, 295, 3671, 1045, 13, 50752, 50752, 759, 264, 2063, 508, 24108, 538, 1570, 813, 341, 1230, 17889, 322, 472, 24784, 11, 550, 291, 434, 51022, 51022, 3700, 322, 341, 24183, 292, 644, 295, 264, 7605, 300, 291, 536, 322, 264, 1411, 293, 291, 393, 19710, 51294, 51294, 32181, 13, 51364, 51364, 5459, 11, 32181, 4696, 16203, 300, 291, 1352, 9834, 343, 293, 363, 300, 366, 51646, 51646, 1998, 281, 264, 7285, 1944, 2158, 295, 508, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.14625183741251627, "compression_ratio": 1.6322314049586777, "no_speech_prob": 6.540252798004076e-06}, {"id": 61, "seek": 27616, "start": 301.8, "end": 306.04, "text": " close to the minimum possible value of J.", "tokens": [50364, 13460, 257, 1359, 1230, 11, 1270, 382, 1958, 13, 628, 16, 420, 1266, 281, 264, 1347, 295, 3671, 1045, 13, 50752, 50752, 759, 264, 2063, 508, 24108, 538, 1570, 813, 341, 1230, 17889, 322, 472, 24784, 11, 550, 291, 434, 51022, 51022, 3700, 322, 341, 24183, 292, 644, 295, 264, 7605, 300, 291, 536, 322, 264, 1411, 293, 291, 393, 19710, 51294, 51294, 32181, 13, 51364, 51364, 5459, 11, 32181, 4696, 16203, 300, 291, 1352, 9834, 343, 293, 363, 300, 366, 51646, 51646, 1998, 281, 264, 7285, 1944, 2158, 295, 508, 13, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.14625183741251627, "compression_ratio": 1.6322314049586777, "no_speech_prob": 6.540252798004076e-06}, {"id": 62, "seek": 30604, "start": 306.04, "end": 310.44, "text": " I usually find that choosing the right threshold epsilon is pretty difficult.", "tokens": [50364, 286, 2673, 915, 300, 10875, 264, 558, 14678, 17889, 307, 1238, 2252, 13, 50584, 50584, 407, 286, 767, 3928, 281, 574, 412, 24877, 411, 341, 472, 322, 264, 1411, 11, 2831, 813, 10687, 322, 50796, 50796, 12509, 32181, 6921, 13, 50938, 50938, 11053, 412, 341, 661, 2573, 393, 980, 291, 11, 286, 603, 976, 291, 512, 7339, 9164, 498, 1310, 51216, 51216, 16235, 23475, 307, 406, 1364, 8944, 382, 731, 13, 51422, 51422, 407, 291, 600, 586, 1612, 437, 264, 2539, 7605, 820, 574, 411, 562, 16235, 23475, 307, 51641, 51641, 2614, 731, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.1348378123069296, "compression_ratio": 1.671875, "no_speech_prob": 6.048789146007039e-06}, {"id": 63, "seek": 30604, "start": 310.44, "end": 314.68, "text": " So I actually tend to look at graphs like this one on the left, rather than rely on", "tokens": [50364, 286, 2673, 915, 300, 10875, 264, 558, 14678, 17889, 307, 1238, 2252, 13, 50584, 50584, 407, 286, 767, 3928, 281, 574, 412, 24877, 411, 341, 472, 322, 264, 1411, 11, 2831, 813, 10687, 322, 50796, 50796, 12509, 32181, 6921, 13, 50938, 50938, 11053, 412, 341, 661, 2573, 393, 980, 291, 11, 286, 603, 976, 291, 512, 7339, 9164, 498, 1310, 51216, 51216, 16235, 23475, 307, 406, 1364, 8944, 382, 731, 13, 51422, 51422, 407, 291, 600, 586, 1612, 437, 264, 2539, 7605, 820, 574, 411, 562, 16235, 23475, 307, 51641, 51641, 2614, 731, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.1348378123069296, "compression_ratio": 1.671875, "no_speech_prob": 6.048789146007039e-06}, {"id": 64, "seek": 30604, "start": 314.68, "end": 317.52000000000004, "text": " automatic convergence tests.", "tokens": [50364, 286, 2673, 915, 300, 10875, 264, 558, 14678, 17889, 307, 1238, 2252, 13, 50584, 50584, 407, 286, 767, 3928, 281, 574, 412, 24877, 411, 341, 472, 322, 264, 1411, 11, 2831, 813, 10687, 322, 50796, 50796, 12509, 32181, 6921, 13, 50938, 50938, 11053, 412, 341, 661, 2573, 393, 980, 291, 11, 286, 603, 976, 291, 512, 7339, 9164, 498, 1310, 51216, 51216, 16235, 23475, 307, 406, 1364, 8944, 382, 731, 13, 51422, 51422, 407, 291, 600, 586, 1612, 437, 264, 2539, 7605, 820, 574, 411, 562, 16235, 23475, 307, 51641, 51641, 2614, 731, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.1348378123069296, "compression_ratio": 1.671875, "no_speech_prob": 6.048789146007039e-06}, {"id": 65, "seek": 30604, "start": 317.52000000000004, "end": 323.08000000000004, "text": " Looking at this other figure can tell you, I'll give you some advanced warning if maybe", "tokens": [50364, 286, 2673, 915, 300, 10875, 264, 558, 14678, 17889, 307, 1238, 2252, 13, 50584, 50584, 407, 286, 767, 3928, 281, 574, 412, 24877, 411, 341, 472, 322, 264, 1411, 11, 2831, 813, 10687, 322, 50796, 50796, 12509, 32181, 6921, 13, 50938, 50938, 11053, 412, 341, 661, 2573, 393, 980, 291, 11, 286, 603, 976, 291, 512, 7339, 9164, 498, 1310, 51216, 51216, 16235, 23475, 307, 406, 1364, 8944, 382, 731, 13, 51422, 51422, 407, 291, 600, 586, 1612, 437, 264, 2539, 7605, 820, 574, 411, 562, 16235, 23475, 307, 51641, 51641, 2614, 731, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.1348378123069296, "compression_ratio": 1.671875, "no_speech_prob": 6.048789146007039e-06}, {"id": 66, "seek": 30604, "start": 323.08000000000004, "end": 327.20000000000005, "text": " gradient descent is not working correctly as well.", "tokens": [50364, 286, 2673, 915, 300, 10875, 264, 558, 14678, 17889, 307, 1238, 2252, 13, 50584, 50584, 407, 286, 767, 3928, 281, 574, 412, 24877, 411, 341, 472, 322, 264, 1411, 11, 2831, 813, 10687, 322, 50796, 50796, 12509, 32181, 6921, 13, 50938, 50938, 11053, 412, 341, 661, 2573, 393, 980, 291, 11, 286, 603, 976, 291, 512, 7339, 9164, 498, 1310, 51216, 51216, 16235, 23475, 307, 406, 1364, 8944, 382, 731, 13, 51422, 51422, 407, 291, 600, 586, 1612, 437, 264, 2539, 7605, 820, 574, 411, 562, 16235, 23475, 307, 51641, 51641, 2614, 731, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.1348378123069296, "compression_ratio": 1.671875, "no_speech_prob": 6.048789146007039e-06}, {"id": 67, "seek": 30604, "start": 327.20000000000005, "end": 331.58000000000004, "text": " So you've now seen what the learning curve should look like when gradient descent is", "tokens": [50364, 286, 2673, 915, 300, 10875, 264, 558, 14678, 17889, 307, 1238, 2252, 13, 50584, 50584, 407, 286, 767, 3928, 281, 574, 412, 24877, 411, 341, 472, 322, 264, 1411, 11, 2831, 813, 10687, 322, 50796, 50796, 12509, 32181, 6921, 13, 50938, 50938, 11053, 412, 341, 661, 2573, 393, 980, 291, 11, 286, 603, 976, 291, 512, 7339, 9164, 498, 1310, 51216, 51216, 16235, 23475, 307, 406, 1364, 8944, 382, 731, 13, 51422, 51422, 407, 291, 600, 586, 1612, 437, 264, 2539, 7605, 820, 574, 411, 562, 16235, 23475, 307, 51641, 51641, 2614, 731, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.1348378123069296, "compression_ratio": 1.671875, "no_speech_prob": 6.048789146007039e-06}, {"id": 68, "seek": 30604, "start": 331.58000000000004, "end": 332.96000000000004, "text": " running well.", "tokens": [50364, 286, 2673, 915, 300, 10875, 264, 558, 14678, 17889, 307, 1238, 2252, 13, 50584, 50584, 407, 286, 767, 3928, 281, 574, 412, 24877, 411, 341, 472, 322, 264, 1411, 11, 2831, 813, 10687, 322, 50796, 50796, 12509, 32181, 6921, 13, 50938, 50938, 11053, 412, 341, 661, 2573, 393, 980, 291, 11, 286, 603, 976, 291, 512, 7339, 9164, 498, 1310, 51216, 51216, 16235, 23475, 307, 406, 1364, 8944, 382, 731, 13, 51422, 51422, 407, 291, 600, 586, 1612, 437, 264, 2539, 7605, 820, 574, 411, 562, 16235, 23475, 307, 51641, 51641, 2614, 731, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.1348378123069296, "compression_ratio": 1.671875, "no_speech_prob": 6.048789146007039e-06}, {"id": 69, "seek": 33296, "start": 332.96, "end": 338.03999999999996, "text": " Let's take these insights and in the next video, take a look at how to choose an appropriate", "tokens": [50364, 961, 311, 747, 613, 14310, 293, 294, 264, 958, 960, 11, 747, 257, 574, 412, 577, 281, 2826, 364, 6854, 50618, 50618, 2539, 3314, 13, 50660], "temperature": 0.0, "avg_logprob": -0.11110272577830724, "compression_ratio": 1.202247191011236, "no_speech_prob": 7.715676474617794e-05}, {"id": 70, "seek": 33804, "start": 338.04, "end": 363.92, "text": " learning rate.", "tokens": [50364, 2539, 3314, 13, 51658], "temperature": 0.0, "avg_logprob": -0.7508056163787842, "compression_ratio": 0.6363636363636364, "no_speech_prob": 0.00029884447576478124}], "language": "en", "video_id": "9w3HRwTRVUE", "entity": "ML Specialization, Andrew Ng (2022)"}}