{"video_id": "Pgct8PKV7iw", "title": "Week 9 \u2013 Lecture: Group sparsity, world model, and generative adversarial networks (GANs)", "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Yann LeCun\nWeek 9: http://bit.ly/pDL-en-09\n\n0:00:00 \u2013 Week 9 \u2013 Lecture\n\nLECTURE Part A: http://bit.ly/pDL-en-09-1\nWe discussed discriminative recurrent sparse auto-encoders and group sparsity. The main idea was how to combine sparse coding with discriminative training. We went through how to structure a network with a recurrent autoencoder similar to LISTA and a decoder. Then we discussed how to use group sparsity to extract invariant features.\n0:00:35 \u2013 Discriminative Recurrent Sparse Auto-Encoder and Group Sparsity\n0:15:18 \u2013 AE With Group Sparsity: Questions and Clarification\n0:30:34 \u2013 Convolutional RELU with Group Sparsity\n\nLECTURE Part B: http://bit.ly/pDL-en-09-2\nIn this section, we talked about the World Models for autonomous control including the neural network architecture and training schema. Then, we discussed the difference between World Models and Reinforcement Learning (RL). Finally, we studied Generative Adversarial Networks (GANs) in terms of energy-based model with the contrastive method.\n0:42:06 \u2013 Learning World Models for Autonomous Control\n1:06:33 \u2013 Reinforcement Learning\n1:30:30 \u2013 Generative Adversarial Network", "author": "Alfredo Canziani", "keywords": ["Yann LeCun", "Deep Learning", "PyTorch", "NYU", "EBM", "Energy Based Models", "contrastive methods", "Regularised Latent Variables", "Generative Adversarial Network", "GAN", "World model", "control", "Reinforcement Learning", "RL", "sparsity"], "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw", "length": 7104, "views": 8101, "publish_date": "11/02/2022", "timestamp": 1590883200, "entity": "Yann LeCun", "transcript": {"text": " So I guess we can get started. This is the third part of the lecture on energy-based models, which we are going to continue a little bit what we talked about last time on sparse coding and talk about GANs very briefly. You'll hear more about it tomorrow from Alfredo, and then talk about learning world models and similar things. Also a little bit about exotic self-supervised and unsupervised learning algorithms that are kind of active research topics at the moment. So one thing I talked about last time was sparse coding. I'm going to mention just a very simple idea which consists of combining sparse coding or the idea of sparse autoencoder with discriminative training. So imagine that the architecture I'm showing you here, the encoder if you will, the first part on the left is mostly similar to the encoder I talked about for the lista method. So you start with the x variable, you run it through a matrix, then you run that through a non-linearity, it could be a value for example, this is the case here, and then you take the result, multiply it by some matrix which we're going to learn, add this with the product of the input by the encoding matrix we, and then pass this to a non-linearity. You can repeat this little block, this green block here multiple times, each of those is a layer basically that consists in a matrix or a bunch of convolutions, an addition with some pre-existing variable and a non-linearity. So this is a funny kind of neural network where you have skipping connections and then we're going to train this neural network to do three different things or with three different criteria. One criterion is going to be just reconstruct x, okay, so there's going to be a decoding matrix that is going to reproduce the input on the output and we're going to do this by just minimizing square error. So this is what's indicated by the decoding filters here. And again this could be convolutional or not depending on which version you like. There's going to be an L1 criterion on the feature vector that makes it sparse, so this is very much like a sparse autoencoder of the type that we talked about last week. But then we're also going to add a third term and this third term is going to be basically a simple linear classifier which is going to try to predict a category, okay, and we're going to train the system to minimize all three criteria at the same time. So this is a sparse autoencoder that also tries to find codes that do a good job at prediction. And this is sort of a good way, you can see this in two different ways. You can see this as an autoencoder that is biased towards producing good labels or you can see this as a classifier, multi-layer classifier that is regularized by an autoencoder. What's the advantage of this? Well the advantage is that by forcing the system to find representations here at the second last layer that can reconstruct the input, then you're basically biasing the system towards extracting features that contain as much information about the input as possible. So that sort of makes the features richer if you want. It forces the system to not generate degenerate features but to generate features that contain as much information as possible about the input. That works pretty well. I think it's an under-explored method for training neural nets because very often we don't have enough label training data or when the training data is such that you don't have a lot of categories to work with, maybe it's a two or three or ten class problem which we know tend to produce very degenerate features in a neural net as we discussed last time. Then forcing the system to reconstruct basically tells it you can't generate features that are to degenerate. We'll also degenerate that you can't reconstruct the input from it. So that's sort of a good, you could think of it as a good regularizer. Group sparsity and structure sparsity. So there's some work going back about 10 years, maybe a little more. In fact, the first work on this are about 20 years old. On the idea of group sparsity, what does that mean? Here is the idea. The idea is to train a system to generate sparse features but not just normal features that are extracted, say, by a bunch of convolutions and values but to basically produce sparse features that are sparse after the pooling. So you essentially have a system that consists of convolutions, non-linearity, and pooling. You try to make those features sparse. There's a number of different works. The idea goes back to Ivarinon and Hoyer in 2001 in the context of ICA, independent component analysis. And then there were a few other papers, one by Ozyndero in Jeff Eaton's group. And then Corai Cabotrullo, who was a student of mine back in the late 2000s. Carl Greger, who was posed up with me. Julien Meral, who is in France, and a bunch of other people, on this idea of structure sparsity. So the idea basically is you take... So some of those models only have an encoder. Some of them only have a decoder. And some of them are autoencoders. So the one on the left, Ozyndero's model, is an encoder-only model. Julien Meral's model is a decoder-only model. And Corai Cabotrullo's model is basically an autoencoder, a sparse autoencoder of the type that we talked about last time. So how does that work? Let's take, say, an encoder-only model. You have a feature extractor, which consists of convolutions or maybe just free-connected matrices over an image patch, for example. And then instead of forcing the output of this to be after a non-linearity, instead of forcing that to be sparse, you put a pooling layer and you force the pooling to be sparse. And this applies to all three of those. So here is a more specific example. This is the version that Corai Cabotrullo did for his PhD thesis, where he had a sparse autoencoder. So you have an encoding function, gE of weyi. It could be multiple layers. In this case, it was basically just two layers with one non-linearity. You have a decoder, which in this case was linear, wD times z. You have a latent variable z. And that latent variable, instead of going to a L1, it goes through basically an L2, but it's L2 over groups. So you take a group of components of z. You compute the L2 norm, not the square of the L2 norm, but the L2 norm, which means the square root of the sum of the values of those components, of the square of those components. So take each component, compute the square, and then compute the sum of the values of the components. So take each component, compute the square, and then compute the sum of a group of those squares, and then compute the square root of that. So that's the L2 norm within that group. And then you do this for multiple groups. The groups can be overlapping or non-overlapping. And you compute their sum. And that's your regularizer. That's your sparsity regularizer. So what does that tend to do? It tends to basically turn off the maximum number of groups. The system basically is sparsity on groups. So it wants the smallest number of groups to be on at any one time. But within a group, because it's an L2 norm within a group, it doesn't care how many units are on within a group. So many units can be on within a group. So what does that do? It forces the system basically to group within a pool features that turn on simultaneously. So if you have features that are very similar, feature extractors that are very similar, filters that are very similar in a convolutional net, then those features will tend to kind of, when you do the training, they'll tend to group themselves within a group, because they will tend to be activated together. And that's the best way to minimize the number of groups that are activated at any one time. So to get those interesting kind of pictures here, the way this was obtained is by here the groups. So what you're looking at here are the, either the, I think it's a decoding matrix. So these are the columns of the WD matrix that we can reconstruct an image patch from the sparse code by multiplying by that matrix. But what we do here is that we group those features into blocks of 36. So we arrange all the features in a 2D map that has nothing to do with the topology of the image. We can choose any topology we want. In fact, this is not actually a 2D topology. It's a Torridel topology. So the left side touches the right side and the top touches the bottom. So it's topologically identical to a torus. And what we do is we group sets of 36 features within a group. And those groups of 36 features overlap by three columns and three rows. Okay, so we have multiple groups of 36 features, six by six, shifted by three. You could think of this as kind of pooling over feature, but not pooling over space, because there's no space here. It's a fully connected network. But it has a bit of the flavor, the same flavor as pooling, except here you pool over 36 features. You don't pool over space. So then you compute the sum of the L2 norm of the features that are within each group. And that's the regularizer you use when you train your Sparsity encoder. So what the system wants to do is minimize the number of groups that are on at any one time. And so as I said before, it basically regroups all the features that are similar and likely to fire simultaneously into groups. And because the groups overlap, then it creates those kind of slowly evolving sets of features that sort of seem to kind of swirl around a point. So the features you get as a result of this have some sort of invariance, and they have some invariance not to shift, but to things like rotation and scale and things like that, whatever the system decides. So here the reason for choosing a 2D topology is basically just to make it look beautiful. But you could choose any kind of topology you want. What is on the x-axis and the y-axis here in this diagram? Those are arbitrary axes. I have, I don't even remember how many features there are here. This might be 256 features, I think. It's 16 by 16. So there's 256 features. So you can see that the x-axis is 16 by 16. So there's 256 hidden units. So imagine a network that has a 12 by 12 input patch, an input image. It's a patch from an image. And 256 hidden units with full connection, non-linearity, and there is another layer on top. Then that's the encoder. And then you have this group sparsity, and then the decoder is linear. And what you're seeing here are the columns of the decoder. And they are organized in a 2D topology, but it's arbitrary. Each of these squares is a column of the decoder. Each of these squares is a column of the decoder, but also corresponds to a component of z, a component of the feature vector. And so they are organized in a 16 by 16 matrix, but it's kind of arbitrary. We just put them in a matrix. And then we train. And because the groups take kind of 6 by 6 neighborhoods in this topology, the system naturally learns features that are similar when they are nearby within this topology. But again, I could have chosen any kind of topology, 1D, 2D, 3D, or even some graph neighborhood of some kind, as long as the pooling is between neighbors and the graph. That will work. So what I've done here is kind of repeat this little pattern to kind of show, because it's, you know, it's toroidal, to show how those patterns kind of repeat and are sort of periodical. And the reason for visualizing it this way is that this is the kind of stuff that neuroscientists observe when they poke electrodes in the primary visual context of mammals, but most animals that have good vision. They see kind of those kind of swirling patterns where neighboring neurons detect similar features, which means similar oriented edges. They are sensitive to oriented edges and neighboring neurons are sensitive to similar angles, or the same angles at similar scale or things like that. And so perhaps this is how the brain organizes its neurons. It's by kind of basically having some sort of criterion on the complex cells, which are the equivalent of the pooling units that we're seeing here. Here is another example here. So this one is not at the patch level, but it uses local connections, but it's not convolutional in the sense that it doesn't use shared weights. The reason for doing this is to have some semi-realistic correspondence to a sort of biological learning where, of course, neurons in the brain can share weights. They end up being similar because they train using some sort of unsupervised learning, but there is no such thing as weight sharing in the brain, as far as we know. So it was asked if a similar strategy of the training of the autoencoder with the classifier and the regularizer can be applied for a variational autoencoder and whether this has been explored, if it works as well for the first slide you show. Yeah, so basically adding noise in a variational autoencoder and forcing sparsity are basically two ways to achieve the same purpose, which is reduce the capacity of the latent variable, reduce the capacity of the code that is extracted by the autoencoder. And this is what prevents the system from running a trivial identity function, which would not be useful. And what we talked about the last couple times is the fact that if you reduce the information capacity of the latent variable of the code, as a consequence, you also minimize the volume of space that can take low energy. Okay, because you limit the number of configurations of the code. And so as a consequence, you kind of limit the volume of space that can take low energy. So essentially this idea of regularizing with L1 or sparsity or something like this, or adding noise to a code while limiting the norm of the code achieve the same purpose, which is limiting the capacity of the code for the purpose of limiting the volume of space that can take low energy. And as a consequence, if you train part of the space to have low energy by minimizing the reconstruction error on your training samples, automatically the rest of the space will have higher energy because the capacity, the volume that can take low energy is limited. So this is just to recap what we talked about last time and a couple of weeks ago. This is sort of the alternative. So those kind of architectural methods are alternatives to the contrastive methods where you explicitly push up on the energy of bad samples, which means you have to come up with a good way of generating bad samples in that case. So again, remember those two types of methods, contrastive methods, you push down the energy of the training samples, you push out the energy of stuff outside either by corrupting the original samples or by doing noisy gradient descent, contrastive divergence, things like this, or by generating contrastive points in some way. We've seen a bunch of different contrastive methods. And then the alternative is limiting the capacity of code or kind of limiting the volume of stuff that can take low energy in the context of autoencoder or predictor. This means limiting the capacity of the code. And there are many ways to do this. One way is to sparsity, one way is through adding noise while limiting the norm, that's the AEs. And there are other ways that we'll talk about in a minute. Whenever you were talking before about the group sparsity, you were summing just a few samples, like a few indexes within a small range. What is that PJ? Maybe I didn't- PJ is a group. It's a pool. So imagine this is a pool, like in a convolution net, but the pool instead of pulling just over space, it pulls over features as well. For a fully connected network, it just pulls over components of z, just features. So PJ is like a set of indexes? PJ is a subset of indices of z, of components of z. Yeah. Okay, thanks. Right, so here PJ is a group of six components of z that happen to be neighbors in this topology. And that's one P and the next P is a similar square, six by six square, shifted by three pixels to the left, to the top or bottom. Okay, okay. And that's right, top, bottom. Okay, thanks. So the overlapping between the groups is what kind of represents this topology, if you want. Okay, so in this experiment, you know, is very similar to the one we just talked about. Except here, we have local connections. So we have an input. It's a two-dimensional input here. We kind of only represent a 1D version of it. And we have units, possibly multiple units at one location looking at a piece of the input, kind of a local patch on the input. And then those sets of units are kind of replicated multiple times, but there's no shared weights. So the units, these kind of units everywhere on the input, but they the weights are not shared. Okay, they're just locally connected. So I guess I'm not quite understanding the overall concept of the feature pooling. I mean, if I think about it in terms of like pooling that we used in convolution, networks, then it's straightforward. But I don't really understand how feature pooling works. Okay, let me draw a picture. Maybe that'll be clear. Okay, so you start with an input vector, multiplied by a matrix or pass it through some sort of encoder. Which may have radios and whatever, or multiple matrices inside. Okay, maybe multiple layers. And you get a feature vector. Okay, so let's call that z. And now you do pooling essentially. So you divide this into groups. In this case, they are non-overlapping. And you compute the within one of those groups, you compute the square root of the sum of the squares of those zis where i belong to the group, the pool. Okay, it's called p because it's a pool. Okay, and you do this for all the groups. All right, so what you get here, this output here looks very much like the output of a pooling on a computer. So you get the output of a pool. Looks very much like the output of a pooling layer in a convolutional net. This is not a convolutional net. Okay, it's a fully connected network here. But the result is the same. And that's your regularizer. Now, in the example I just showed, you take the z, and this is what you send to a decoder matrix from which you reconstruct the input. Okay, so this is y, this is y bar. That's a prediction for the reconstruction. And this pooled layer here is only used to compute the regularizer. It's not actually used for reconstruction. You reconstruct from the sparse code directly. But it looks very much like a pooling layer. Now, if this were a convolutional net, then that dimension or feature here would be features, but you would have multiple feature maps. Okay, so I'm representing the feature dimension vertically. Then the encoder would do multiple convolutions and would also generate multiple convolutions and would also generate multiple feature maps, perhaps a larger number. And then the kind of pooling we would do here is a pooling where, so each, after pooling, we would take a window over space as well as over features and compute the square root of some square there. And that gives us one output in our pooling output. And then we have multiple groups of features like this that go into different pooling. So it doesn't matter whether this is convolutional or not. In convolutions, you would pool over space as well as feature type. But if you don't have convolutions, you just pool over features. And that builds invariants to whatever it is that the system thinks makes sense. Is that clear? Does that answer your question? Yeah, I think it's more clear. Thank you. Professor, I have a question for when you split the z into groups and do the pooling, do those groups overlap? Right. So in the example I showed here, they do not overlap. But you can make them overlap. OK. So let's say we have a feature vector z. I can take a pool here and a pool here and a pool here. And here those groups overlap. And if I do this and I do group sparsity where these are the groups, what's going to happen is that I'm going to have a continuously varying set of features here that vary from one end to the other because the system is going to want to group within a pool features that are similar. And so because of the overlap, it's going to continuously vary them so that they change slowly over the vector. Now, in the pictures that I showed in the slides, instead of organizing the z features here in a 1D topology, I organized them in a 2D topology. And I made the groups two dimensional. So I take a 6 by 6 block. That's one group. And then the next group will be another 6 by 6 block with some overlap. And then the next group will be yet another 6 by 6 block. And maybe I have another one because I have a toroidal topology that takes these guys and these guys. And then there is a similar thing sliding up, et cetera. So the groups basically are those 6 by 6 windows that are shifted by 3 and overlapping. And so that's how you get those continuously varying features along the two dimensions. I could have equally well chosen to organize this in a 3D topology or into some sort of tree. So I take all the components of z and I organize them in some sort of graph, perhaps a tree. So this is called structure sparsity, not group sparsity anymore. Well, it depends how you do it, I guess. And then the groups would be things like this would be a group. And then perhaps this would be a group as well. And I can organize a group in sort of Russian dolls like this. And what's going to happen there is that the groups that the units that are in the topology are going to be sparse. The units that are in many groups will tend to be very sparse, whereas the units that are in a few groups will tend to be less sparse. And so if you do something like this with a tree, what happens here is that the feature in the center tends to be not sparse at all. It's going to be something that really sort of detects just very sort of generic features. And then at the first level in the tree, they're going to be a little sparse. So they're going to be sort of very sort of smooth edge extractors or something like that. And then the more you go inside of the tree, the more each feature enters in a large number of pools, and therefore they get more pressure to be sparse. And so they end up being much sparser, which means they end up being more selective for particular features. And what happens there is that when you show an image, it tends to favor activating features that are along one particular branch in that tree, because that's the best way to sort of minimize the number of pools that are on at any one time. So let's call structures varsity. And there's a number of papers on this by Julien Meral. So this goes back about 10 years ago, and Rodolphe Genaton. I mean, they co-authored this. Senior author was Francis Back. I put the reference in one of the slides. And there's a paper by my group also by Arthur Schlamm, which I'll go to in a minute. Can you explain why grouping regularization actually helps in grouping similar features? Well, so that's a good question. First of all, does it help? And the answer is not clear. So those experiments were done quite a while ago before the computation was really available, and the data was available for this to really kind of work at a big scale. This was mostly viewed as the people interested in this were interested in two things. They were either interested in unsupervised learning for things like image restoration and stuff like that. This was what Julien Meral was doing. All they were interested in was the self-supervised pre-training, because at the time the data sets were very small for training convolutional nets. They were too small. So there had to be some sort of pre-training procedure, which is what I was interested in. And so it's the same motivation that we now have again for self-supervised learning. But a lot of those methods haven't been brought back to the fore. They tended to work very well when the data set was small. So they tended to kind of improve performance of, let's say, a convolutional net if you pre-train this using a method. So using a method very similar to the one I showed earlier, so something a bit like this, but convolutional. So make the encoder and the decoder convolutional and train with good sparsity on complex cells. And then after you're done pre-training the system, you get rid of the decoder. You only use the encoder as a feature extractor, say the first layer of a convolutional net, and you stick a second layer on top of it. So let me go through this a little bit. So you start with a set of images. You have an encoder which is basically convolutional radio. Not much more than that, okay, just convolutional radio. There needs to be some sort of scaling layer afterwards for this particular case. And you train with groups sparsity. So you have a linear decoder and you kind of reconstruct the input and you have a you have a criterion here which is this group L1. Okay, so it's sum over group, sorry, I call the group P, right? Sum over group of square root of sum for i in the group of z i squared. Okay, so that's group sparsity. So you train this little sparse autoencoder with good sparsity. And then what you do is you take the this group sparsity layer that you just used as a regularizer. And so you basically eliminate, you cut this part out of the network, you take the group sparsity, which is really a pooling layer, an L2 pooling layer, and you stick it here. Okay, so this is basically L2 pooling. But it has the same architecture as the one you use for the for the groups sparsity. And then you use that as a feature extractor, which is like the first pair of layers of a convolutional net, convolutional value pooling. Okay, but this is L2 pooling, not max pooling. And then you can repeat the process. You can train another instance of this network, have a couple layers here. I'm going to have a decoder, have this L2 pooling and sparsity criterion, train this to reconstruct its input, and then stick the pooling on top, eliminate this, and now you have a pre-trained two layer convolutional net. This is a procedure that some people call stacked autoencoder. So you train an autoencoder to extract features, and then you generate features with the encoder of that part of that autoencoder, and you stick another layer on top, train that as an autoencoder, and then keep going. And the only characteristic here is that this autoencoder is trained with to produce invariant features through group sparsity. Do we use all possible sub-trees as groups in the previous example? No, that's kind of up to you, really. What structure you use here, you can use multiple trees, you can use if you want multiple features to kind of be to represent an input even at low frequency. So that's really up to you. It could be like what you can afford. What you can do also is train the system with a bigger tree that is more compact, that is more compact, that is more compact, that is more compact, and then kind of cut down on the tree than necessary, and then sort of prune the tree whenever there are branches that are not used, or used very rarely. Okay, so this is, the experiment I showed here is similar but is only local connections and no weight sharing. And what you see here is this, again, disorganization of the features in terms of what neuroscientists call pinwheel patterns. So pinwheel patterns are those patterns where the orientation selectivity varies continuously as you go around one of those red dots. So you take one of those red dots, and if you kind of do a little circle around the red dots, what you notice is that the orientation of the edge extractor kind of varies continuously as you move around. And those are called pinwheel patterns. And they are observed in the brain. In fact, those pictures here on the right come from neuroscience papers that describe this where the color here encodes the orientation selectivity. The little stars indicate those kind of the singularities here, the center of the pinwheels. Is the group sparsity term train to have a small value? Well, it's a regularizer, right? Let me go back to the... It's a cost function during training or during inference depending on whether you use the sort of predictive version of it where you have a variable or not. But it's basically just a term of the energy, right? So the term itself is not trained, it's fixed, right? It's just the L2 norm over groups, and the groups are predetermined. But because it's a criterion, it sort of determines what the encoder and the decoders will do, what type of features will be extracted. Here is another example of sort of exotic way of doing sparse coding through lateral inhibition. And there's a bunch of examples of this. So this is a linear decoder with a square reconstruction error. This is WZ minus X, where X is the input here in this case. And then there is a criterion in the energy, which is the vector formed by the absolute values of Z transpose times some matrix times some vector. And then there is a linear decoder which is a linear decoder with a square reconstruction error. And then there is a linear decoder with a square representation error of Z transpose times some matrix times the vector itself. So it's a kind of a quadratic form that involves Z and this matrix S. And the matrix S is either determined by hand or learned so as to kind of maximize this term. Okay. If the terms in S are positive and large, if one particular term, Sij is large, what that means is that the system does not want Zi and Zj to be on at the same time. Okay. If Zi is on and Sij is large, then it wants Zj to be off and vice versa. Okay. And so it's sort of a mutual inhibition. People call this lateral inhibition in neuroscience. It's basically all your feature vectors basically inhibit other feature vectors through this matrix S. You can decide that the matrix S a priori is structured. So you can decide that only some terms are on zero. You can decide that some terms, those terms are fixed or can be trained. And the way you train them is by actually maximizing. So it's kind of adversarial training a little bit. You try to find the value of S that sort of, is as large as possible if you want within limits. Above a certain value of Sij, one of the Zi or Zj is going to go to zero and that term is going to disappear. So the system is going to maximize the Sijs until it's large enough to kind of do the mutual inhibition between Zi and Zj. And it's not going to go any further because it doesn't need to. And again, if you organize S in terms of a tree, so here the lines represent the zero terms in the S matrix. And whenever you don't have a line between two features, there's a non-zero term in the S matrix, right? So every feature inhibits all other features except the ones that are up the tree or down the tree from it. And this is very much like group sparsity a little bit. It's kind of the converse, if you want, of group sparsity. Instead of saying features within a branch of the tree need to be activated together by minimizing L2, minimizing the number of such groups that are on, here you explicitly have a sort of inhibition term that for every feature inhibits all other features in all the other branches of the tree. And what you see again is that you see this system sort of organizing the features in a more or less continuous fashion and in such a way that features along a branch of the tree correspond to basically the same feature but with sort of different levels of selectivity and then features along the periphery sort of vary more or less continuously because there is inhibition not just at the bottom level but also at the middle level. OK, so to go back to this, the way you train the system is at each iteration you give an x, you find a z that minimizes this energy function. So you find a z that reconstructs but also minimizes the second term, which means that if you have an Sij term that is non-zero, it wants either zi or zj to be zero or at least very small. You do one step of gradient descent now to to kind of update w so as to minimize the reconstruction error. And you do also, if you want, you can do one step of gradient ascent to make the terms in S larger by kind of computing the gradient of this energy with respect to S and then going up the energy not down. Again, if you use not a tree but some sort of 2D topology you also get those kind of patterns. And more complex ones if there are kind of multiple scales for the features. OK, so much for sparse coding and structured sparse coding. And the reason I'm telling you about this is because although those don't have a huge amount of practical applications the sparse coding, structured sparse coding they, in my opinion, will be the basis for kind of self-supervised learning methods of the next few years. As I told you, I think self-supervised learning right now is the hardest topic in NLP and it's becoming kind of a bit of a hot topic in computer vision as well. And it's mostly now dominated by contrasting methods but I think the architectural methods are going to take over because contrasting methods don't scale very well. So this is sort of giving you weapons for the future, if you want. Understanding what this is all about. OK, now for something completely different. This is something that Alfredo will like because he works on this project. And it's one of the users, probably one of the most important uses of self-supervised learning, is the idea of learning world models for control systems or for other purposes. So when we, when humans or animals learn a task we quite obviously have a kind of good internal model of how the world works, of intuitive physics, of the fact that when an object is not supported, it falls. We've learned gravity when we were babies, probably around the age of nine months or so. Eight or nine months. That's when it pops up in babies. And we learn this mostly by observation. So how is it that we can learn how the world works and all the concepts about the world by observation? And there are two reasons for this. So one I already explained is the idea of self-supervised learning. So you can train yourself to predict, maybe you will spontaneously kind of learn abstract concepts about the world. That might be useful in preparation for learning a particular task or set of tasks. But there's another reason, which is that you actually want to build models of the world if you want to be able to act on the world. So I'm holding this pen and I know that if I move my hand up the pen will move with it because it's between my fingers. I know that if I open my fingers, the pen will fall. I know by gravity. I know by grasping. I've learned all that stuff. And I've learned mostly by observation. I've learned also by experimentation. But a lot of what I've learned, I've learned just by observation. So the big question is can we use what we've learned about self-supervised learning to train a system to learn world models? What is a world model? So if you want to sort of give an idea of the architecture of an autonomous intelligent system it would be a system that is composed of essentially four major blocks here that are represented on the left. So it's an intelligent agent or maybe not so intelligent, we'll see. It has a perception module and the perception module basically observes the world and then computes a representation of the state of the world. Called ST. At time t, S of t is the idea that the system has of the state of the world. This is necessarily an incomplete representation of the world because we can't observe the entire universe at once. We only observe what's immediately around us. And even that we can't see through occlusions and there is a lot of internal states of the world that we can't observe well enough. Even if you can observe, your accuracy of observation may not be good enough. So if I put this pen in my hand and it appears to be vertical and I let it go, it's going to fall but you can't really predict in what direction. I've used that example before to describe the problem of aleatoric uncertainty, which is the world is non-deterministic and you can't predict exactly what's going to happen because you don't have a perfect reading of the state of the world. And maybe the world is intrinsically stochastic. We don't know that actually. Okay, so a forward model is a model that given the current state of the world S of t or your idea of your current state of the world and an action that you're taking or that someone else is taking, something that you can choose or at least observe, and perhaps an auxiliary latent variable Z of t, which represents what you don't know about the world. So the part of the world, the state of the world that you don't know or the thing that's unpredictable about what's going to go on in the world. The forward model predicts the next state of the world, S t plus one. You discretize time in some way. So if you have a model of the world of that type, you can simulate in your head what's going to happen as a consequence of your actions. So you have this model in your head. You know the current state of the world or some idea of the current state of the world. You run your internal model of the world forward with a sequence of A of t, which is a sequence of action that you imagine taking. And your model of the world, as you imagine it, will predict what's going to happen in the world. If you could do this, then you could plan a sequence of actions that will arrive at a particular goal. So for example, what sequence of action should I do to grab this pen? I should follow a particular trajectory, actuate my muscles in a particular way. So I grab this pen. And the cost function I can measure is whether I've grabbed the pen, whether the pen is in my grasp. I could measure this with some function, perhaps. And the question is, can I plan a sequence of actions that, given my model of the world, which in this case is the model of my hand and the model of where the pen is, will allow me to grab it? It's a little more complicated if I throw the pen and I have to catch it in the air, because I have to predict the trajectory of the pen. So I have to have an intuitive model of physics to be able to grab that pen, which of course I've learned through experience as well. People are surprised you like so much reinforcement learning. This is not reinforcement learning. This has absolutely nothing to do with reinforcement learning. Let me be very clear. This has nothing to do with reinforcement learning. This may have to do in the future, OK? But right now it doesn't. Model-based reinforcement learning. No, it doesn't. It has nothing to do with reinforcement learning. OK, let me go through this a little bit. Can you explain the difference then? Yes, I will. In a minute. OK, so now, so on the left here you have this little agent. It has this model of the world that it can run forward, OK? It has an actor, or you can think of it as a policy that produces a sequence of actions, which it is going to feed to the model. And then a critic, which is going to predict what the cost of the final state or the trajectory is going to be according to the criterion. So the critic here computes basically the cost of not fulfilling the goal that I set myself. So if my task is to reach for this pen, and I kind of miss the pen by a few centimeters, my cost is a few centimeters. If I grab it, the cost is zero. If I miss it by a lot, the cost is higher. OK, that would be an example of a cost. Now, OK, so there is a number of different things you can do with this sort of basic model of intelligent agent. So the first one is you start from an initial state that you observe in the world. You run your forward model. You give a proposal for a sequence of actions. You measure the cost. What you can do here, ignoring the P here, which represents a policy, let's imagine it doesn't exist. By gradient descent or by some sort of optimization algorithm, you could try to find a sequence of actions that will minimize the overall cost over the trajectory. I start from a state. I run my forward model. And it takes an action. OK, let me just call this A1. This is S1. And this is going to give me S2. And I'm going to measure the cost of S2 through some cost function, C. OK, the next time step, running my forward model again, make an action proposal A2. This is all simulated. This is all in my head, right? Because this model, this forward model is in my head. It's in my frontal cortex. So I'm not actually doing this in the world, et cetera, right? So I can enroll this for a few time steps. Those time steps can be milliseconds if I control muscles. They can be seconds if I control high level actions. They can be hours. So if I want to plan how to, I don't know, go to San Francisco, I need to get to the airports and then catch a plane. And then when I arrive there, catch a taxi or something, et cetera. OK, so this is independent of the level of description of the thing. OK, so what I can do with this is I can do a very classical method called model predictive control. So it's a classical method of optimal control, which is a whole discipline that has been around since the 50s, if not earlier. And some of the methods or method predictive controls go back to the 1960s. There is something called the Kelly-Brierson algorithm. I think it's Kelly with an E, I'm not sure. So this is a method very similar to the one I'm describing at the moment. And this was used primarily by NASA, let's say, to compute trajectories for rockets. So when they started having computers in the 60s at NASA, they started computing trajectories with computers and they were basically using things like this. Before that, they had to do it by hand. And if you haven't seen the movie Hidden Figures, it describes how people were computing this by hand. This was mostly done by black women, black mathematicians, women mathematicians, who also ended up kind of programming those computers. Watch that movie, it's really great. OK, so here is a basic idea here. This looks very much like a recurrent net, because your forward model is basically the same network replicated over time. And this is like an unworld recurrent network. And so what you do here is you back propagate the value of the cost through this entire network all the way to the actions. And you don't use this for training, you use this for inference. You think of the actions as latent variables. And you basically by gradient descent or some other optimization method, you find a sequence of actions that will minimize the sum of the cost over the trajectory. OK, so basically you have an overall cost. I'm going to call it big C, and that's going to be the sum over time steps of the little c of s t. OK, and what you're going to do is big A, which is the sequence of A, is going to be replaced by its own value minus some step size times the gradient of big C with respect to A. OK, so as long as you can compute the gradient of the sum of those costs over the trajectory with respect to all of the components of A, which means the trajectories of A, you can do this optimization. You don't have to do it necessarily through gradient descent. In some cases, there are more efficient ways to do this optimization using dynamic programming. For example, if A is discrete, that might be more efficient. But if A is continuous and high dimensional, you basically have no choice but to use gradient based methods. OK, so this is inference. There's no learning yet. What is A? Big A is the sequence A1, A2, A3, et cetera. OK, so you have a differentiable objective function and you can minimize it with respect to the variables you're interested in. So what do you get out of this? There are no weights in A. A is a vector, right? So A is a vector, yeah. Yeah, so that was actually because we never use we never minimize vectors so far. We've always been optimizing weights so people are confused. Oh, we have. For latent variables like the z variables, the latent variables of the energy based models, the latent variables, we do minimize the energy with respect to z. So this is the same problem here we're solving. I think, yeah, I think not everyone understood that the latent variables are actually inputs. So that was, I think, like a misunderstanding with the question we had on Piazza about training these latent variable models. You don't want to use the word training for latent variables or for things like this because you want to use inference. OK, you don't use the word to infer or not to train. I want to use the word inference, not training. What's the difference between inference and training? Training, with training, you learn a parameter that is the same for a large number of samples. For inference, you find the value of some variable, a latent variable, A in this case, z in the case of a latent variable energy based model that is specific to one sample. You change the sample, the latent variable changes. So you don't learn it because you don't remember it from one time to the next. There's no memory for it, right? So that's the difference. Conceptually, you're doing the same kind of operation where you do learning and inference. And so at some level of abstraction, they are the same. But inference, you do it per sample. Learning, you do it over a bunch of samples and the parameter is shared across the samples. When we have an energy based model and we'd like to do inference, we still have a minimization to do at every time we perform this, we use it, right? So that was a big difference between... After you've trained the model, when you use it, you still have to do minimization with respect to the latent variables. That's the big difference. Same here. Here, there may or may not be any training. Your forward model may be built by hand or may be trained. But by the time we are here, it's trained. We're not training anything here. We're just doing inference. We're figuring out what is the optimal value of the sequence of A's that will minimize our cost, overall cost. And this is an inference problem, just like energy based models. For example, the FM, the forward model can be just one line of equation of physics, right? It can be just a deterministic equation. So imagine the forward model is the few equations that describe the physics of a rocket. And A is basically the action on the steering, how you orient the nozzles and then the thrust. So that would be the collection of A, would be the collection of those variables. And then there is very simple physics, Newtonian physics, basically. You can write the equations. It will give you the state of the rocket at the next time step as a function of state of the rocket at the previous time step and the actions you're taking. That's how you do simulations, that's how every simulator works. And then your cost function, if you want to shoot a rocket, would be maybe a combination of two things. One would be the energy spent during that time step, the amount of fuel you spent, something like that. And the second term might be the distance to a target you want to reach. Maybe you want to rendezvous with a space station. And the second term in the cost would be the distance to the space station, square of distance to the space station. If you measure the sum over the entire trajectory of the distance to the space station, the system will try to minimize the time it will take to get to the space station because it won't want to minimize the sum of the square of the distances to the space station over the trajectory. But at the same time, it wants to minimize fuel. So you have to balance those two terms. So that's a classical way of doing optimal control. And that's called model predictive control. Is Kalman filtering one type of model predictive control? No, Kalman filtering is a particular forward model, if you want. It's a way of estimating the state of the world. But it's basically given your observation of the state of the world through a perception system, there's going to be some uncertainty about the state of the world. And the Kalman filter basically assumes a Gaussian distribution on this uncertainty. And now when you run through your forward model, you're going to have a resulting uncertainty about the state of the world at the next time step. Because it was uncertain to start with. So given the uncertainty when you started from, what's the uncertainty after one step of physics, if you want? And if you assume linearity of all those steps and Gaussianity of the uncertainty, that's what a Kalman filter is. Most of the uncertainty comes from, OK, so now your forward model produces a prediction. And at the next time step, you might get another reading of the state of the world because your sensors are still working. So now you have two Gaussians. One is your new perception of the world tells you, here is where I think the state of the world is. And your forward model also predicted, here's where I think it is. And you have to combine those two. That's where the complexity of Kalman filtering comes in, which is I've got two Gaussian predictions. So the resulting probability distribution is also a Gaussian if you compute the covariance matrix and et cetera. And that's where the formulas for Kalman filters come from. OK, so Kalman filter is a way to deal with the uncertainty in the reading your perception of the world and in the when you propagate this uncertainty in your forward model. I think there was still a main difference. I think you wanted to address the point that this is different from RL. OK, so what is RL in that context? OK, so OK, I need one more step before I talk about RL. OK, and here is that step. OK, so what we had just a minute ago was a forward model that's unrolled in time. And the system has takes a sequence of actions. A1, A2, A3, S1, S2. And then we have the cost function here coming up. OK, and this could go on, right? Now, what we'd like to be able to do is not have to do this optimization with respect to A1, A2, A3, A4 every time. Every time we need to do a planning, we don't want to have to do the to go through this complex process of back propagating gradient through this entire system to do model predictive control. And so a simple way to get rid of that step is the same trick that we use in autoencoders versus sparse coding. So remember, it's sparse coding. We wanted to reconstruct, but then we had to do inference with respect to the latent variable by optimization. And that turned out to be expensive. So what we talked about last week was the idea of using an encoder that we train to predict the optimal value directly. OK, and we're going to do the same here. That resulted in the idea of sparse autoencoder. We're going to do the same here. We're going to train a network to take the state and directly predict what the optimal value of the action is. And this network, of course, we're going to apply every time step. And this is going to be called a policy network. OK, so the policy network takes the state and produces a guess about the best action to take at this time. So as to minimize the overall cost. And this is going to be a trainable neural net or whatever model parameterized model that we want. The way we're going to train this model is basically just by propagation. So we're going to using our perception module. This is the world here. And we're looking at the world with a camera. And there is a perception module that gives us a guess as to what the state of the world is. OK, this is perception. And this is a forward model applied multiple time steps. And this is a cost. OK, so what we can do is run the system. And to run the system, we first run through the perception. We compute an action, we run this action through the forward model. This forward model gives us here is the next state we're going to be in. Compute the cost and then keep going. Keep doing this just forward prop through this entire system, which is really kind of an unworld recurrent net, if you want. And once you're done, you back propagate gradients from all the terms in the cost function all the way through the network, all the way through the parameters of that policy network. So basically you compute the big C. So big C, remember, is the sum of all the C's in a long time with respect to DW. OK, and that's just going to be the sum over time of the big C over DW. Sorry, yeah, big C over DAT. DAT over DW. OK, I just applied chain rule, right? But I don't need to, right? If I just define this function in pytorch and I just do backprop, it'll just do the right thing. So I can compute the gradient of the overall cost with respect to the parameters of that policy network. And so if I train this over sufficiently many samples, if my forward model is correct, if my cost function does what I want, then my policy network is going to learn a good policy that just looking at the state will minimize the expected cost over a trajectory. OK, the average cost over a trajectory. There's no reinforcement learning here. This is all backdrop. OK, now we can talk about the difference with reinforcement learning. The main difference with reinforcement learning here is twofold. The first one is in reinforcement learning, in most reinforcement learning scenarios at least, the C function is a black box. Well, it's a black box, not a red box. OK, that's the first difference. The second difference is that this is not a forward model of the world. This is the real world. And your measure of the state of the world is imperfect. So inside of this policy network, you might have a perception network here that estimates the state of the world. So you have no control over the real world and your cost function is not known. You can just get the output of the cost function by just trying something. You take an action, you see the effect on the world, and that gives you what reinforcement learning people call a reward, but it's just a negative cost. It's the negative value of your cost. But the cost is not differentiable. You don't know the function of the cost. You have to go through the world to figure out the value of the cost. And that's the main issue with reinforcement learning, which is that the cost function is not differentiable. It's unknown. The only way to estimate it is by trying something and then observing the value, which is what the reward is, really. It's a negative. The negative of the reward is basically your cost. So in that situation, since you cannot evaluate gradients, to minimize your cost, you have to try multiple things. You have to try an action, see the result, and then try another action, see if the result is better, and then try another action, see if the result is better. And if your cost function is very flat, you have to try many, many things before you get a non-zero reward or a non-high cost. And so that's where the complexity goes. There is the additional problem of exploration. So because you don't know the form of the cost and because it's non-differentiable, you might need to kind of try many actions in kind of a smart way to figure out in which part of the space to go to be able to sort of figure out how can I improve my performance. OK, so that's the main issue of exploration. And then there is the issue of exploration versus exploitation. So the fact that when you're on a situation, you don't want to take completely random actions because they're likely to not result in anything interesting. So you want to take actions that are kind of close to what you think might work and sort of occasionally kind of try something else while you're learning and learn your policy as you go. What I'm describing, what I was describing just before, is a situation where you can do all of this in your head because you have a model of the world and you can optimize your sequence of action very efficiently because you have a differentiable cost function. Your cost function is computed by your own brain, if you want, inside of your agent. You can tell if you grab the pen, you can tell the distance between your hand and the pen. So you can compute your own cost function and it is kind of in your internal world model is differentiable. In the real world, it's not. In the real world, you don't know the derivative of the distance of your hand to the pen unless you have some model of that in your head. But by default, you don't. But because everything is in your head, everything is differentiable. Everything is implemented by neural net and everything you can back propagate gradient through everything. So that's the big advantage of this kind of approach versus reinforcement learning. Make everything differentiable. So there's two problems with the world. So there's one big advantage in this kind of scenario, which is you can run this faster than real time because your forward model inside of your agent can run as fast as you want. You don't need to run through the world. That's one advantage. Second advantage is the actions you're taking will not kill you because you can predict using your forward model. Maybe you'll predict that the action will kill you, but you're not going to take it in the real world. So it won't kill you if you have an accurate forward model. Third advantage, because everything takes place in your head, everything is in neural net, everything is differentiable. You can use all kinds of efficient learning or inference algorithms to figure out a good course of actions. OK, so that's the difference with reinforcement learning. In reinforcement learning, you're telling yourself, I have to go through the real world. I don't have a model of the real world. I don't know how to compute the cost function in a differentiable way. That said, a lot of reinforcement learning methods actually work by training a model of the cost function. OK, so actor critic methods. Basically, the role of the critic is to learn to evaluate, to kind of predict the value of the overall objective function, the expected value of the objective function. And because it's a neural net that you're going to train, you can back propagate gradient to it. So you're basically learning an approximation of the cost function of the real world using a neural net. That's the role of a critic. OK, why is it so good to have models when you're learning a skill, like learning to drive, for example? It's basically what allows you to learn quickly and to learn without killing yourself. So if you don't have a good model of the world, you don't know about gravity, you don't know about the dynamics of objects, you don't know anything. And you put an agent at the wheel of a car. The agent has no idea what the physics of a car is. OK, and you put the car next to a cliff. The car is driving at 30 miles an hour next to a cliff. The agent doesn't have a model of the world. It has no idea that by turning the wheel to the right, the car will run off a cliff and will fall into the ravine. It has to actually try it to figure it out. It has to fall into the ravine to figure out that this is a bad idea. OK, and maybe just from one sample, it's not going to be able to learn it. So it's going to have to run into the ravine like thousands of times before it figures out the model of the world that first, turning the wheel to the right makes the car go to the right. And second, that when the car goes above a ravine, it falls into a ravine and destroys itself. OK, if you have a model of the world that understands about gravity and things like this, then you know that turning the wheel to the right is going to make the car via to the ravine. And you don't do it because you know it's going to kill you. OK, so what allows humans and animals to learn quickly, much, much quicker than any model free reinforcement learning methods that has ever been devised is the fact that we have very, very good word models in our head. OK, now, what does that tell us? OK, so here is the problem with the world. The world is not deterministic or if it is deterministic, it's so complex that it equally well could be non deterministic. It doesn't make any difference for us. There's two problems with predicting the next state of the world. The first problem is that the world is not entirely predictable and it could be not entirely predictable for two reasons. Those are called aleatoric uncertainty and epistemic uncertainty. Aleatoric uncertainty is due to the fact that the world is intrinsically unpredictable or the fact that we don't have full information about the state of the world. So we cannot predict exactly what's going to happen next. So you're looking at me right now. You're a pretty good model of the immediate environment of me. OK, but you cannot exactly predict in which way I'm going to move my head next because you don't have an accurate model of what's inside my skull. OK, your perceptual system does not give you a full model of how my brain functions, unfortunately. So you cannot exactly predict what I'm going to do next, what I'm going to say, how I'm going to move my head, et cetera. So that's aleatoric uncertainty. There is also epistemic uncertainty. Epistemic uncertainty is the fact that you can't completely predict the next state of the world because the amount of training data you've had was not enough. Your model hasn't been trained enough to really kind of figure it out. OK, that's kind of a different type of uncertainty. So the big question now is how do we train models of the world under a certainty? I give you an ST. Can you predict ST plus 1? And it's the same problem we encountered before with our supervised learning. I give you an X. Can you predict Y? But the problem is that there are now multiple Ys that are compatible with X. There are multiple ST plus 1s that are compatible with S, even for a given action. So what does that mean? That means that our model here, our forward model, may take the state of the world and an action. But it will also have to take a latent variable, which we don't know the value of, to predict the next state. And this was very much like what we talked about earlier, where we had I'm going to draw this in a different topology, but it's the same idea. So we had X and it was going through a predictor, computing H. And then that was going through a decoder that will take into account a latent variable to predict Y bar. And then we observe Y. OK, this is a prediction for S. And maybe at some time, we might be able to actually take the action and observe the next state of the world. While we are training our model, we'll actually be observing the next state of the world, T plus 1. OK, so to train our forward model here, we take the state ST. We take an action if we have an action. We have a latent variable and our prediction goes into a cost function. That diagram is exactly identical to the one on the right. Right, it's the same. It's exactly the same diagram, except I split the FM into two modules. I've given it a particular architecture. In fact, I could make this more explicit. I think you have the super thick marker selected. I do, yes. You don't like that, huh? So this would be my forward model. OK, so that's what's inside this box. Inside the forward model box here is this. And you know, I renamed ST is now called X and ST plus 1 is now called Y bar. I mean, it's now called Y, but it's the same thing otherwise. So it's the same scenario that we talked about before in latent variable energy based models, essentially. But now we're going to use this to train a forward model to predict what's going to happen in the world. So we may have to play the same tricks that we played, that we talked about last week, which is that last week what we explained was that we can take, OK, the way I drew this last week was slightly different. What I explained last week is that we can, if we have while we are training our forward model, we have a pair X and Y. And the way we find the value of Z is by minimizing the energy with respect to Z, right? So we basically find the star, which is the argmin of C of Y and Y bar, Y bar being the output of our predictor of our system. OK, and then we do one step of gradient descent. So we change the parameters of our entire system according to the gradient of that cost. But for this to work, we had to regularize Z, limit its information content. And we have to do the same here. Why is that? Well, here we're trying to solve a prediction problem. But imagine, and we talked about this a couple of weeks ago, I give you an X and a Y, and you find the Z that minimizes the overall energy and the Z is not regularized. If Z is the same dimension as Y, there's probably going to be a Z for any Y that makes the cost function zero, right? If there's enough capacity in Z, there's always going to be a value of Z that makes the cost function zero. And that's bad because that means my energy function is going to be completely flat. It's going to be zero everywhere. And I need it to be small on the training samples and high outside of the region of high data density. And what we saw in the last couple of weeks is that by regularizing Z, limiting its capacity, either by making it sparse, for example, or making it discrete or by making it noisy, then we can limit its capacity. Why do we need ZT if you already have AT? Well, so AT is the action you take, right? OK, I'm going to tell you I'm going to let this thing go. OK, but you don't know in which direction it's going to go, right? So let's say it goes this way. But I have to predict in advance which way it's going to go. It's like, OK, here's a better situation. You are goalie playing soccer. OK, and it's a penalty kick. So you're in front of the kicker in front of you. And the guy is going to kick the ball and you're going to have to jump one way or the other. And you have to make a choice about jumping left or right. And you have to make that decision based on what you observe from the person. But you don't know exactly what the ball is going to do. A is which direction you jump in. It's basically how you jump. Z is what you don't know about the player in front of you doing. You don't know the state of the world. You don't know the state of the brain of this guy. And so you don't know if he's going to shoot left or right or up or down. OK, that's the difference. Z is what you cannot know about the world that is necessary to make the prediction of the next state. A is the action you take, which in this case has very little influence on the immediate state of the world. Yeah, it seems to be clear now. Right. So you need to regularize Z. And then one of the tricks we described. So one of the things we described to regularize Z was sparsity. Another one was adding noise. But the other trick we described is this idea of having an encoder. So you have X or ST run through the predictor. The predictor goes into the decoder, which makes a prediction about Y. Let's call it Y bar. And you compare Y bar to Y. And here you have Z. And what we talked about is the idea of using an encoder here to predict the optimal value of Z and then basically having a cost function that is a term in the energy that measures the discrepancy between the value of Z you actually use and the value of Z predicted by the encoder. And perhaps this is regularized in some way. And the predictor also has to influence the encoder. So it's pretty clear that you need an information bottleneck between the encoder and the decoder. Otherwise, the system will cheat. It will completely ignore X. It will be able to predict Y exactly by just cheating, by looking at the value of Y, running it through the encoder and then running it through the decoder and then predicting Y. Right. That's just a very simple autoencoder. So unless you restrict the capacity of Z, the system will just cheat and not actually train itself to predict. You have to push down on the information content of Z so as to force the system to use the information from X. OK, to make the best prediction. OK, now we can use that trick to train our forward model. Because again, the forward model is basically just an instance of this. And this is the project for autonomous driving that a former student Mikhail Inav worked on. And Alfredo has worked on this and is still working on this project. And so here you're trying to train a car to drive itself. And what's difficult to predict is what the car around you are going to do. So you place a camera above the highway and you watch the cars kind of go by. And you can track every car and then extract the immediate neighborhood of the car. Basically, a little rectangle around every car that indicates where the other cars are relative to your car. And this is what was represented at the bottom. So at the bottom, you have a little rectangle that's centered around a given car. And then all the cars around are the little rectangle centered on that car where the car is in a standardized location in the middle of that rectangle. You do this for every car. What it gives you is for every car a sequence of what the cars around it are going to do. And we can use this to train a forward model that will predict what the cars around us are going to do. The question is if these forward models predict in all possible futures irrespective of the action taken. Yeah. Where we predict a set of futures. So given one action and given one initial state, one action, and one particular value of the latent variable, it will make a single prediction. And then you can vary the latent variable and it will make multiple predictions. You can change the action, of course. So I've redrawn the little diagram I drew previously here. Here the state basically is a sequence of three frames from this video. There's no abstract state here. It's just the picture itself. The blue car is our car and the green cars are the other cars. So you take kind of three frames from the past, run this through this neural net, which attempts to predict the next frame. Okay. Using basically a big convolutional net as a predictor and a big convolutional net as a decoder. There's a latent variable here. There's also an action here which is not drawn that gets into this. And the system also has an encoder. So it looks more like this. And again, the action here is not represented, but imagine there is one. So X is the past frames. It goes through a predictor that predicts a representation of the input. And then that representation goes into a convolutional net that the decoder that predicts it basically is combined additively with a latent variable. So it's added to a latent variable before going into a decoder that makes a prediction for the next state. And the latent variable itself is a latent variable, but is being predicted by an encoder, which itself is also a convolutional net. It takes the past and the future and tries to predict the ideal value of the latent variable. Now, of course, you have to restrict the information content here. And this is done in this particular project using sort of a VAE-like approach where the... I mean, it's basically a VAE with a few tricks. So Z is sampled from a distribution that is obtained from the output of the encoder. The output of the encoder outputs a prediction for Z bar as well as prediction for variances. And Z is sampled from that distribution. So it's not optimized, it's sampled. But there's also a term that tries to kind of minimize the sum of the squares of the Z's over time, which is the standard technique for VAE. And that goes into the decoder. And so this is trained as a conditional autoencoder, basically. There's another trick that's added to this, which is that half the time Z is simply set to zero. So half the time the system is told you're not allowed to use Z. Just make your best guess as to prediction without a Z. And that drives the system to really use the past in a bigger way than if you just have a noisy Z. If you just use the standard VAE-type training, the system basically ignores the past. It just cheats. It looks at the answer why. I will cover the rest in greater detail in a lab, in a future lab. Perhaps you want to say something about the GANs? I want to say something about GANs. So GANs are a particular form of contrastive learning. Remember that when we talked about energy-based learning, we have data points and a model, which I'm going to draw like this, with a cost function. It could have any kind of structure, but I'm just going to draw it like this. So this would be sort of a reconstruction-type model. So imagine that the model here is an autoencoder or something like this. But you can imagine just about anything. A simplified version, I mean a more general version of this, would be just Y goes into a cost function and I'm not specifying what the cost function looks like. So what the cost function computes is in the space of Y. So let's say Y is two-dimensional. So Y is an energy that we want to be low on the data and high outside the data. And here I deliberately drew a bad energy function. So this energy function is bad because it should be low around this region where we have data. And it should be higher outside, and right now it's pretty low in this region right here. So we talked about contrastive methods, and contrastive methods consist in taking a sample and pushing down on its energy and then taking a contrastive sample, which I'm going to draw in purple. So contrastive samples should be a sample that our model already gives low energy to, but should not give low energy to. We're going to push that up. So push up on the energy of this guy, push down on the energy of that guy. And if you keep picking those samples and those contrastive samples well, by minimizing some objective function that wants to make the energy of the blue point small and the energy of the pink points high, then the system will learn properly. So we've seen several ways of generating contrastive samples. The idea of denoising autoencoder, which is to take a sample and basically corrupt it in some way. We've seen the idea of contrastive divergence, which takes a sample and then you go down the energy with some noise, and that gives you a contrastive sample to push up. And we've seen a number of other methods that are based on prior knowledge about similarity between samples. But here is another idea. The other idea is to train a neural net to produce those contrastive samples intelligently. And that's the basic idea of GANs, at least in a form of GANs that would be called energy-based GANs. There are several formulations of GANs. In fact, there is an entire laundry list of various types of GANs. But the basic idea of GANs is that you train your energy model. So the energy model in the context of GAN is called a discriminator or sometimes a critic, but it's basically just very similar to an energy model. And you train it to take low energy on the data points. And then you train another neural net to generate contrastive data points, and you move their energy up. So the overall diagram is something like this. You have a discriminator, and a discriminator really should be not drawn this way. It could be a large neural net, but in the end, it's just a cost function. So it takes a variable y, and it tells you it's good or bad. Low energy if it's good, high energy if it's bad. So in one phase, you collect a piece of data from your data set and just give it to your discriminator. So this is a real y coming from data. That's a training sample. And you say the output of that should go down. I should really write this as f, because after all, it's just an energy function. So make f of y go down, of course, by changing the parameters. So you do w, replaced by w minus eta df. So f is a neural net. f is a neural net. Some parameterized function, but probably a pretty complicated neural net. That's the first thing. And that will make the energy of data points small. Now, there's a form of this that's conditional. So the form of this is conditional, you have an extra input here, which is an observation. But you can have this or not. That's called conditional again. It doesn't matter. OK, second phase, for contrastive samples, you have a latent variable z that you sample from some distribution, a distribution that's easy to sample from, let's say Gaussian, multivariate Gaussian, or uniform or something. You run this through what's called a generator. So this is a neural net. And that neural net produces something similar to y. It just produces an image, let's say, for IR images. And again, you run this through your discriminator. But now you want to make that large. OK, so in fact, what I told you before is a lie. You don't do this update like that. OK, but here what you want is you want to make FW of this y bar high. OK. And what you're going to do now is train the discriminator and the generator simultaneously. So you're going to first have to come up with a cost function, a loss function. And this loss function is going to be some over sample of a per sample loss function that basically is a function of F of y and F of y bar. Y bar, of course, is generated from the randomly sample latent variable z. Now, this cost function needs to be a decreasing function of F of y and an increasing function of F of y bar. OK, you can use just about any cost function you want as long as it makes F of y decrease and it makes F of y bar increase. Or as long as it makes a difference decrease, F of y minus F of y bar. Good example of this would be kind of a hinge loss, for example. OK, so something that says my loss function is going to be F of y plus some margin minus F of y bar positive part. OK, so this is a hinge. And it says I want to make F of y bar smaller than m. Other than that, I don't care. Bigger than m, I'm sorry. I drew this backwards. So overall, as a function of F of y bar, this function looks like this. So it wants to make F of y bar larger than m. OK, so that's an example. The actual cost function that most the original formulation of GANs used basically plugs each of those terms into a sigmoid and tries to make the sigmoid apply to F of y as close to 1 as possible and the sigmoid apply to F of y bar as close to 0 as possible. It's basically that. Nothing more than that. So it's sigmoid of F of y plus 1 minus sigmoid of F of y bar. And you take logs because this is not the last function. This is kind of what goes before the last function. So this is kind of like a cross entropy. But you have cross entropy that's positive for the positive phase. And so the target is negative for the negative phase. Yeah, I shouldn't write it this way. This is wrong, actually. Sorry about that. But you put it in the logistic loss for each of those. So it's technically log of 1 plus exponential F of y for the correct one and minus further that. Log of 1 plus e to the F of y plus log 1 plus e to the minus F of y bar. But you could imagine a large number of objective functions of this type. OK, so this is the last function you're going to use to train the discriminator. But the generator, this is for the discriminator. But it's going to be a last function for the generator, and that's a different last function. And you're going to optimize those two last functions the same way. So the generator is one that basically wants to make the generator produce outputs that the discriminator thinks are good, but they're not. So basically the generator wants to adapt its weight so that the output that it produces, y bar, produces a low energy for F of y. So you sample a random variable z, you run it through the generator, it produces a y bar, you run through the discriminator, the F of y, you get some value. And then you back propagate the value through the generator and adapt the weights of the generator so that this energy goes down. So basically the generator is trying to find a y bar that has low energy, as low as possible. And it trains itself to kind of produce y's that have low energy. Again, if we're talking about conditional GANs, there's going to be an x variable that's going to enter those two modules, but that makes no difference in the end. So lg is maybe simply an increasing function of F of y bar. I think we are kind of running out of time. We are. We have run out of time. So this would be some objective function of F of g if g is the generator of z where z is sample randomly. So you just do back prop through this and you change the parameters of g, let's call them u, so that this goes down. Now this is called a game in the sense that you have two objective functions that you need to minimize simultaneously and they are incompatible with each other. And so it's not a gradient descent problem. You have to find what's called a Nash equilibrium between those two functions. And gradient descent will not do it by default. So that leads to instabilities. And there is tons of papers on how to make GANs actually work. That's kind of a complicated part. But Alfredo will tell you all about this tomorrow. Maybe you also you want to mention the one with the sigmoid that creates some issues if we have like samples that are close to the true manifold. Yes. And then I think we can close the closed. OK, so let me mention that. So let's imagine that your data. So again, energy based framework. Your data is around so manifold, but it's a thin manifold. So it's an infinitely thin distribution. OK. In the original formulation of GAN, the GAN, the discriminator would need to produce zero probability outside of this. OK, so it needs to produce zero probability here. And it needs to produce on the manifold, it needs to produce infinite probability. In such a way that the integral, if this is really a density estimation, in such a way that the integral of the density over the entire space is one. And this is, of course, very hard. So GANs basically abandon the idea of actually learning a distribution. What they want to do is produce zero, the original formulation, produce zero outside the manifold of data and produce one here. It's the output of a sigmoid that needs to be one, which means the weighted sum going into that sigmoid needs to be infinite, essentially. So it's not that different. And the problem with this is that if you train the system successfully. And you get the energy function, which is zero outside the manifold and one on the manifold, your energy function is completely useless. It's useless because it's a golf course, right? It's flat. So the energy function basically that corresponds to this would be the negative log of that. So it would be infinity here. And the minimum value of your cost function on the manifold, which, for example, could be zero. If it's an autoencoder, the energy can be smaller than zero. And so it's a golf course of infinite altitude, which is really not that useful. What you want, as I said before, for every energy-based model, if you want an energy-based model to be useful, you want the energy function to be smooth. You don't want it to go to infinity in sort of very small steps. You want it to be smooth so that you can do inference. So that if you start from a point here, it's easy to find a point on the manifold that's nearby using gradient descent, for example. So the original formulation of GaN leads to, first of all, infinite weights in the discriminator, instabilities, something called mode collapse, which Alfredo will tell you about. And in the end, a contrast function, an energy function that's essentially useless. So it's not ideally formulated. So people have proposed ways to fix it by regularizing the energy function, basically forcing it to be smooth. So one good example of this is something called Wasserstein GaNs. Proposed by Martin Narzowski, who just graduated from NYU, and Leon Boutroux, and a few other people. And the idea of that is to basically limit the size of the weights of the discriminator so that the function is smooth. And there is various mathematical arguments in probabilistic framework, but that's the basic idea. And there's lots of variations of this also. Questions about today class? It was dense, but at least we were answering every question it was coming through. So I think we follow along today. I wasn't sure if maybe you explained it in a different form and I didn't realize it's the same thing, but I was a little lost on what the policy network is. Okay. What that does. So the policy network takes the estimation of the state of the world and produces an action. And it's trained to minimize the expected cost of the state over the trajectory, but it takes just one action. Right. And there was a part towards the end where I guess you drew a new connection. Right. That from like S to where it goes down, like connected through some module to A. So what is happening there? So the policy network is indicated by pi here on the screen. So it takes S, the state, and it produces an action. Okay. Okay. That's what a policy is, right? You observe the state of the world and you take an action. I see. Okay. In fact, a probabilistic policy is you don't take an action. You give a distribution of actions and then you pick the action in some way from the distribution. But here, you know, you just have to take an action. If the number of actions is discrete, then this pi network is this policy network is basically a classifier. And it produces a bunch of scores for each possible action. And then you take one of the actions probabilistically or deterministically. Deterministically, you just take the action with the highest score. Probabilistically, you can sample according to the score. And then you run through your front model and you keep going. Okay. So without the policy connection, then the action is just kind of... Is a latent variable. So you have to optimize with respect to the latent variable to find its optimal value. So you have this kind of diagram now where the actions are not produced by neural net. There are latent variables that you have to figure out for every time you run your model. You have to figure out what's the best sequence of action to minimize my cost. You have to basically do this, for example, by gradient descent, figuring out the sequence of A that will minimize the sum of the C's over the trajectory. That's called model predictive control. And then the one with the policy network is called direct control, essentially. Professor, you say that during inference, we need to minimize the energy to get the final value. But okay, there are two questions. One, won't it take too much time during inference and would it be useful for real time systems? And the second one is, since it's unrolled and you have to back propagate all the way through the beginning, it would have all the problems that we face in recurrent neural networks. Exactly. So presumably you're not going to get the same problems as you have with recurrent nets because your forward model, presumably implements the dynamics of some real system. So it might not have the issues of sort of non-invertibility that you have. If it's a physical system, it's probably going to be reversible. So you may not have the same issue as with regular recurrent nets. But yeah, you're facing the same problems. Now, in real time situations, you use a form of this called receding horizon planning. So receding horizon planning is when you are in a real time situation, your system will run its forward model for a few steps in the future. I don't know, let's say a few seconds. Okay. Sufficiently many steps to predict for a few seconds. That's your horizon. Then you do this model predictive control by optimizing, finding the optimal A that minimizes your cost, your estimated cost, according to your model. You haven't taken an action yet. You just run your internal model to make that prediction. So through optimization with respect to A, you find the sequence of A that optimizes your cost. And then you take the first action in that A and then you do it again. Okay. So with the A you took, observe the state of the world now. You have a new state, which you observe from your sensors. And now repeat the process. Run your forward model, a number of steps in the future. Optimize the sequence of actions to minimize your cost. Take the first action and do it again. So it can be expensive if your horizon is long, if your forward model is complicated. And so that's when you need a forward model, a policy network. So the policy network basically compiles this whole process into a neural net that directly produces the best action from the state. Okay. Which may or may not be possible, but it gives you a good guess. Now, to give you a concrete example, there's an interesting series of books by a Nobel Prize winning economist that is in New York called Danny Kahneman. And he talks about two systems in the human mind called system one and system two. So system one is the process by which you take an action without thinking. Okay. You're a very experienced driver and you can drive your car without even paying attention by talking to someone next to you. You don't actually need to think about it. Okay. System two is more sort of deliberate planning. So system two is when you use your internal model of the world to kind of predict in advance what's going to happen ahead, sort of foresee what's going to happen and then take a deliberate action that you think is going to be the right one according to your model. So it's more like reasoning. Okay. You can think of this, you know, optimization with respect to actions to minimize an objective as a form of reasoning. And we talked about this before. So basically model predictive control is when you don't have a policy, you haven't learned the skill, you know what your cost function is, you have a pretty good model of the world, but you don't know how to react. Okay. So a beginner chess player would be like that. You look at the chess game and you have to think about all possibilities before you play because, you know, you don't know where to play. So you have to kind of imagine all the possibilities. If you're an expert player and you play against a beginner, you know immediately what to play. You don't have to think about it. I don't know if you've played simultaneous games against a master or grandmaster at chess. A grandmaster can play against 50 people and beat them in a few minutes because the player can go from a chess, you know, one opponent to another and just immediately play. It's completely reactive. You actually don't need to think because, you know, they've kind of compiled that if you want in their knowledge of chess, that they don't need to think when you see this kind of type of easy situation. So that's going from system two to system one. And when you learn a skill, at first you're hesitant and you have to think about it. You know, you're hesitant when you drive, you drive slowly and you look at everything and you pay attention. And then when you're experimented, you can just react really quickly. Basically, you've gone from model predictive control to basically training your own policy network, if you will. And in the process of doing this, your skill has gone from a sort of deliberate planned conscious decision mechanism to a sort of subconscious automatic decision mechanism. That's what acquiring expertise does. And that's that's how you go from this diagram to that diagram where you have a policy that directly predicts the action without having to plan. OK, got it. Thanks.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.0, "text": " So I guess we can get started. This is the third part of the lecture on energy-based models,", "tokens": [50364, 407, 286, 2041, 321, 393, 483, 1409, 13, 639, 307, 264, 2636, 644, 295, 264, 7991, 322, 2281, 12, 6032, 5245, 11, 50664, 50740, 597, 321, 366, 516, 281, 2354, 257, 707, 857, 437, 321, 2825, 466, 1036, 565, 322, 637, 11668, 17720, 293, 51024, 51092, 751, 466, 460, 1770, 82, 588, 10515, 13, 509, 603, 1568, 544, 466, 309, 4153, 490, 28327, 78, 11, 293, 550, 751, 51452, 51452, 466, 2539, 1002, 5245, 293, 2531, 721, 13, 2743, 257, 707, 857, 466, 27063, 2698, 12, 48172, 24420, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.15690718526425568, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.00047997760702855885}, {"id": 1, "seek": 0, "start": 7.5200000000000005, "end": 13.200000000000001, "text": " which we are going to continue a little bit what we talked about last time on sparse coding and", "tokens": [50364, 407, 286, 2041, 321, 393, 483, 1409, 13, 639, 307, 264, 2636, 644, 295, 264, 7991, 322, 2281, 12, 6032, 5245, 11, 50664, 50740, 597, 321, 366, 516, 281, 2354, 257, 707, 857, 437, 321, 2825, 466, 1036, 565, 322, 637, 11668, 17720, 293, 51024, 51092, 751, 466, 460, 1770, 82, 588, 10515, 13, 509, 603, 1568, 544, 466, 309, 4153, 490, 28327, 78, 11, 293, 550, 751, 51452, 51452, 466, 2539, 1002, 5245, 293, 2531, 721, 13, 2743, 257, 707, 857, 466, 27063, 2698, 12, 48172, 24420, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.15690718526425568, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.00047997760702855885}, {"id": 2, "seek": 0, "start": 14.56, "end": 21.76, "text": " talk about GANs very briefly. You'll hear more about it tomorrow from Alfredo, and then talk", "tokens": [50364, 407, 286, 2041, 321, 393, 483, 1409, 13, 639, 307, 264, 2636, 644, 295, 264, 7991, 322, 2281, 12, 6032, 5245, 11, 50664, 50740, 597, 321, 366, 516, 281, 2354, 257, 707, 857, 437, 321, 2825, 466, 1036, 565, 322, 637, 11668, 17720, 293, 51024, 51092, 751, 466, 460, 1770, 82, 588, 10515, 13, 509, 603, 1568, 544, 466, 309, 4153, 490, 28327, 78, 11, 293, 550, 751, 51452, 51452, 466, 2539, 1002, 5245, 293, 2531, 721, 13, 2743, 257, 707, 857, 466, 27063, 2698, 12, 48172, 24420, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.15690718526425568, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.00047997760702855885}, {"id": 3, "seek": 0, "start": 21.76, "end": 29.76, "text": " about learning world models and similar things. Also a little bit about exotic self-supervised", "tokens": [50364, 407, 286, 2041, 321, 393, 483, 1409, 13, 639, 307, 264, 2636, 644, 295, 264, 7991, 322, 2281, 12, 6032, 5245, 11, 50664, 50740, 597, 321, 366, 516, 281, 2354, 257, 707, 857, 437, 321, 2825, 466, 1036, 565, 322, 637, 11668, 17720, 293, 51024, 51092, 751, 466, 460, 1770, 82, 588, 10515, 13, 509, 603, 1568, 544, 466, 309, 4153, 490, 28327, 78, 11, 293, 550, 751, 51452, 51452, 466, 2539, 1002, 5245, 293, 2531, 721, 13, 2743, 257, 707, 857, 466, 27063, 2698, 12, 48172, 24420, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.15690718526425568, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.00047997760702855885}, {"id": 4, "seek": 2976, "start": 29.76, "end": 35.120000000000005, "text": " and unsupervised learning algorithms that are kind of active research topics at the moment.", "tokens": [50364, 293, 2693, 12879, 24420, 2539, 14642, 300, 366, 733, 295, 4967, 2132, 8378, 412, 264, 1623, 13, 50632, 50664, 407, 472, 551, 286, 2825, 466, 1036, 565, 390, 637, 11668, 17720, 13, 286, 478, 516, 281, 2152, 445, 257, 50960, 51016, 588, 2199, 1558, 597, 14689, 295, 21928, 637, 11668, 17720, 420, 264, 1558, 295, 637, 11668, 8399, 22660, 19866, 51492, 51492, 365, 20828, 1166, 3097, 13, 407, 3811, 300, 264, 9482, 286, 478, 4099, 291, 510, 11, 51772, 51828], "temperature": 0.0, "avg_logprob": -0.12927513587765577, "compression_ratio": 1.6238532110091743, "no_speech_prob": 5.821398372063413e-05}, {"id": 5, "seek": 2976, "start": 35.760000000000005, "end": 41.68, "text": " So one thing I talked about last time was sparse coding. I'm going to mention just a", "tokens": [50364, 293, 2693, 12879, 24420, 2539, 14642, 300, 366, 733, 295, 4967, 2132, 8378, 412, 264, 1623, 13, 50632, 50664, 407, 472, 551, 286, 2825, 466, 1036, 565, 390, 637, 11668, 17720, 13, 286, 478, 516, 281, 2152, 445, 257, 50960, 51016, 588, 2199, 1558, 597, 14689, 295, 21928, 637, 11668, 17720, 420, 264, 1558, 295, 637, 11668, 8399, 22660, 19866, 51492, 51492, 365, 20828, 1166, 3097, 13, 407, 3811, 300, 264, 9482, 286, 478, 4099, 291, 510, 11, 51772, 51828], "temperature": 0.0, "avg_logprob": -0.12927513587765577, "compression_ratio": 1.6238532110091743, "no_speech_prob": 5.821398372063413e-05}, {"id": 6, "seek": 2976, "start": 42.800000000000004, "end": 52.32, "text": " very simple idea which consists of combining sparse coding or the idea of sparse autoencoder", "tokens": [50364, 293, 2693, 12879, 24420, 2539, 14642, 300, 366, 733, 295, 4967, 2132, 8378, 412, 264, 1623, 13, 50632, 50664, 407, 472, 551, 286, 2825, 466, 1036, 565, 390, 637, 11668, 17720, 13, 286, 478, 516, 281, 2152, 445, 257, 50960, 51016, 588, 2199, 1558, 597, 14689, 295, 21928, 637, 11668, 17720, 420, 264, 1558, 295, 637, 11668, 8399, 22660, 19866, 51492, 51492, 365, 20828, 1166, 3097, 13, 407, 3811, 300, 264, 9482, 286, 478, 4099, 291, 510, 11, 51772, 51828], "temperature": 0.0, "avg_logprob": -0.12927513587765577, "compression_ratio": 1.6238532110091743, "no_speech_prob": 5.821398372063413e-05}, {"id": 7, "seek": 2976, "start": 52.32, "end": 57.92, "text": " with discriminative training. So imagine that the architecture I'm showing you here,", "tokens": [50364, 293, 2693, 12879, 24420, 2539, 14642, 300, 366, 733, 295, 4967, 2132, 8378, 412, 264, 1623, 13, 50632, 50664, 407, 472, 551, 286, 2825, 466, 1036, 565, 390, 637, 11668, 17720, 13, 286, 478, 516, 281, 2152, 445, 257, 50960, 51016, 588, 2199, 1558, 597, 14689, 295, 21928, 637, 11668, 17720, 420, 264, 1558, 295, 637, 11668, 8399, 22660, 19866, 51492, 51492, 365, 20828, 1166, 3097, 13, 407, 3811, 300, 264, 9482, 286, 478, 4099, 291, 510, 11, 51772, 51828], "temperature": 0.0, "avg_logprob": -0.12927513587765577, "compression_ratio": 1.6238532110091743, "no_speech_prob": 5.821398372063413e-05}, {"id": 8, "seek": 5792, "start": 57.92, "end": 68.48, "text": " the encoder if you will, the first part on the left is mostly similar to the encoder I talked", "tokens": [50364, 264, 2058, 19866, 498, 291, 486, 11, 264, 700, 644, 322, 264, 1411, 307, 5240, 2531, 281, 264, 2058, 19866, 286, 2825, 50892, 50892, 466, 337, 264, 27764, 3170, 13, 407, 291, 722, 365, 264, 2031, 7006, 11, 291, 1190, 309, 807, 257, 8141, 11, 51176, 51228, 550, 291, 1190, 300, 807, 257, 2107, 12, 1889, 17409, 11, 309, 727, 312, 257, 2158, 337, 1365, 11, 341, 307, 264, 1389, 510, 11, 51452, 51512, 293, 550, 291, 747, 264, 1874, 11, 12972, 309, 538, 512, 8141, 597, 321, 434, 516, 281, 1466, 11, 909, 341, 365, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.2226181983947754, "compression_ratio": 1.6608695652173913, "no_speech_prob": 7.936565089039505e-05}, {"id": 9, "seek": 5792, "start": 68.48, "end": 74.16, "text": " about for the lista method. So you start with the x variable, you run it through a matrix,", "tokens": [50364, 264, 2058, 19866, 498, 291, 486, 11, 264, 700, 644, 322, 264, 1411, 307, 5240, 2531, 281, 264, 2058, 19866, 286, 2825, 50892, 50892, 466, 337, 264, 27764, 3170, 13, 407, 291, 722, 365, 264, 2031, 7006, 11, 291, 1190, 309, 807, 257, 8141, 11, 51176, 51228, 550, 291, 1190, 300, 807, 257, 2107, 12, 1889, 17409, 11, 309, 727, 312, 257, 2158, 337, 1365, 11, 341, 307, 264, 1389, 510, 11, 51452, 51512, 293, 550, 291, 747, 264, 1874, 11, 12972, 309, 538, 512, 8141, 597, 321, 434, 516, 281, 1466, 11, 909, 341, 365, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.2226181983947754, "compression_ratio": 1.6608695652173913, "no_speech_prob": 7.936565089039505e-05}, {"id": 10, "seek": 5792, "start": 75.2, "end": 79.68, "text": " then you run that through a non-linearity, it could be a value for example, this is the case here,", "tokens": [50364, 264, 2058, 19866, 498, 291, 486, 11, 264, 700, 644, 322, 264, 1411, 307, 5240, 2531, 281, 264, 2058, 19866, 286, 2825, 50892, 50892, 466, 337, 264, 27764, 3170, 13, 407, 291, 722, 365, 264, 2031, 7006, 11, 291, 1190, 309, 807, 257, 8141, 11, 51176, 51228, 550, 291, 1190, 300, 807, 257, 2107, 12, 1889, 17409, 11, 309, 727, 312, 257, 2158, 337, 1365, 11, 341, 307, 264, 1389, 510, 11, 51452, 51512, 293, 550, 291, 747, 264, 1874, 11, 12972, 309, 538, 512, 8141, 597, 321, 434, 516, 281, 1466, 11, 909, 341, 365, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.2226181983947754, "compression_ratio": 1.6608695652173913, "no_speech_prob": 7.936565089039505e-05}, {"id": 11, "seek": 5792, "start": 80.88, "end": 87.2, "text": " and then you take the result, multiply it by some matrix which we're going to learn, add this with", "tokens": [50364, 264, 2058, 19866, 498, 291, 486, 11, 264, 700, 644, 322, 264, 1411, 307, 5240, 2531, 281, 264, 2058, 19866, 286, 2825, 50892, 50892, 466, 337, 264, 27764, 3170, 13, 407, 291, 722, 365, 264, 2031, 7006, 11, 291, 1190, 309, 807, 257, 8141, 11, 51176, 51228, 550, 291, 1190, 300, 807, 257, 2107, 12, 1889, 17409, 11, 309, 727, 312, 257, 2158, 337, 1365, 11, 341, 307, 264, 1389, 510, 11, 51452, 51512, 293, 550, 291, 747, 264, 1874, 11, 12972, 309, 538, 512, 8141, 597, 321, 434, 516, 281, 1466, 11, 909, 341, 365, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.2226181983947754, "compression_ratio": 1.6608695652173913, "no_speech_prob": 7.936565089039505e-05}, {"id": 12, "seek": 8720, "start": 87.2, "end": 95.04, "text": " the product of the input by the encoding matrix we, and then pass this to a non-linearity.", "tokens": [50364, 264, 1674, 295, 264, 4846, 538, 264, 43430, 8141, 321, 11, 293, 550, 1320, 341, 281, 257, 2107, 12, 1889, 17409, 13, 50756, 50756, 509, 393, 7149, 341, 707, 3461, 11, 341, 3092, 3461, 510, 3866, 1413, 11, 1184, 295, 729, 307, 257, 4583, 51080, 51080, 1936, 300, 14689, 294, 257, 8141, 420, 257, 3840, 295, 3754, 15892, 11, 364, 4500, 365, 512, 659, 12, 36447, 51420, 51420, 7006, 293, 257, 2107, 12, 1889, 17409, 13, 407, 341, 307, 257, 4074, 733, 295, 18161, 3209, 689, 291, 362, 31533, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.12558454082858178, "compression_ratio": 1.646551724137931, "no_speech_prob": 1.861504279077053e-05}, {"id": 13, "seek": 8720, "start": 95.04, "end": 101.52000000000001, "text": " You can repeat this little block, this green block here multiple times, each of those is a layer", "tokens": [50364, 264, 1674, 295, 264, 4846, 538, 264, 43430, 8141, 321, 11, 293, 550, 1320, 341, 281, 257, 2107, 12, 1889, 17409, 13, 50756, 50756, 509, 393, 7149, 341, 707, 3461, 11, 341, 3092, 3461, 510, 3866, 1413, 11, 1184, 295, 729, 307, 257, 4583, 51080, 51080, 1936, 300, 14689, 294, 257, 8141, 420, 257, 3840, 295, 3754, 15892, 11, 364, 4500, 365, 512, 659, 12, 36447, 51420, 51420, 7006, 293, 257, 2107, 12, 1889, 17409, 13, 407, 341, 307, 257, 4074, 733, 295, 18161, 3209, 689, 291, 362, 31533, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.12558454082858178, "compression_ratio": 1.646551724137931, "no_speech_prob": 1.861504279077053e-05}, {"id": 14, "seek": 8720, "start": 101.52000000000001, "end": 108.32000000000001, "text": " basically that consists in a matrix or a bunch of convolutions, an addition with some pre-existing", "tokens": [50364, 264, 1674, 295, 264, 4846, 538, 264, 43430, 8141, 321, 11, 293, 550, 1320, 341, 281, 257, 2107, 12, 1889, 17409, 13, 50756, 50756, 509, 393, 7149, 341, 707, 3461, 11, 341, 3092, 3461, 510, 3866, 1413, 11, 1184, 295, 729, 307, 257, 4583, 51080, 51080, 1936, 300, 14689, 294, 257, 8141, 420, 257, 3840, 295, 3754, 15892, 11, 364, 4500, 365, 512, 659, 12, 36447, 51420, 51420, 7006, 293, 257, 2107, 12, 1889, 17409, 13, 407, 341, 307, 257, 4074, 733, 295, 18161, 3209, 689, 291, 362, 31533, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.12558454082858178, "compression_ratio": 1.646551724137931, "no_speech_prob": 1.861504279077053e-05}, {"id": 15, "seek": 8720, "start": 108.32000000000001, "end": 115.84, "text": " variable and a non-linearity. So this is a funny kind of neural network where you have skipping", "tokens": [50364, 264, 1674, 295, 264, 4846, 538, 264, 43430, 8141, 321, 11, 293, 550, 1320, 341, 281, 257, 2107, 12, 1889, 17409, 13, 50756, 50756, 509, 393, 7149, 341, 707, 3461, 11, 341, 3092, 3461, 510, 3866, 1413, 11, 1184, 295, 729, 307, 257, 4583, 51080, 51080, 1936, 300, 14689, 294, 257, 8141, 420, 257, 3840, 295, 3754, 15892, 11, 364, 4500, 365, 512, 659, 12, 36447, 51420, 51420, 7006, 293, 257, 2107, 12, 1889, 17409, 13, 407, 341, 307, 257, 4074, 733, 295, 18161, 3209, 689, 291, 362, 31533, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.12558454082858178, "compression_ratio": 1.646551724137931, "no_speech_prob": 1.861504279077053e-05}, {"id": 16, "seek": 11584, "start": 115.84, "end": 122.56, "text": " connections and then we're going to train this neural network to do three different things or", "tokens": [50364, 9271, 293, 550, 321, 434, 516, 281, 3847, 341, 18161, 3209, 281, 360, 1045, 819, 721, 420, 50700, 50700, 365, 1045, 819, 11101, 13, 1485, 46691, 307, 516, 281, 312, 445, 31499, 2031, 11, 1392, 11, 370, 456, 311, 51004, 51004, 516, 281, 312, 257, 979, 8616, 8141, 300, 307, 516, 281, 29501, 264, 4846, 322, 264, 5598, 293, 321, 434, 516, 51380, 51380, 281, 360, 341, 538, 445, 46608, 3732, 6713, 13, 407, 341, 307, 437, 311, 16176, 538, 264, 979, 8616, 15995, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.08263189142400568, "compression_ratio": 1.7897196261682242, "no_speech_prob": 6.438610398618039e-06}, {"id": 17, "seek": 11584, "start": 122.56, "end": 128.64000000000001, "text": " with three different criteria. One criterion is going to be just reconstruct x, okay, so there's", "tokens": [50364, 9271, 293, 550, 321, 434, 516, 281, 3847, 341, 18161, 3209, 281, 360, 1045, 819, 721, 420, 50700, 50700, 365, 1045, 819, 11101, 13, 1485, 46691, 307, 516, 281, 312, 445, 31499, 2031, 11, 1392, 11, 370, 456, 311, 51004, 51004, 516, 281, 312, 257, 979, 8616, 8141, 300, 307, 516, 281, 29501, 264, 4846, 322, 264, 5598, 293, 321, 434, 516, 51380, 51380, 281, 360, 341, 538, 445, 46608, 3732, 6713, 13, 407, 341, 307, 437, 311, 16176, 538, 264, 979, 8616, 15995, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.08263189142400568, "compression_ratio": 1.7897196261682242, "no_speech_prob": 6.438610398618039e-06}, {"id": 18, "seek": 11584, "start": 128.64000000000001, "end": 136.16, "text": " going to be a decoding matrix that is going to reproduce the input on the output and we're going", "tokens": [50364, 9271, 293, 550, 321, 434, 516, 281, 3847, 341, 18161, 3209, 281, 360, 1045, 819, 721, 420, 50700, 50700, 365, 1045, 819, 11101, 13, 1485, 46691, 307, 516, 281, 312, 445, 31499, 2031, 11, 1392, 11, 370, 456, 311, 51004, 51004, 516, 281, 312, 257, 979, 8616, 8141, 300, 307, 516, 281, 29501, 264, 4846, 322, 264, 5598, 293, 321, 434, 516, 51380, 51380, 281, 360, 341, 538, 445, 46608, 3732, 6713, 13, 407, 341, 307, 437, 311, 16176, 538, 264, 979, 8616, 15995, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.08263189142400568, "compression_ratio": 1.7897196261682242, "no_speech_prob": 6.438610398618039e-06}, {"id": 19, "seek": 11584, "start": 136.16, "end": 141.52, "text": " to do this by just minimizing square error. So this is what's indicated by the decoding filters", "tokens": [50364, 9271, 293, 550, 321, 434, 516, 281, 3847, 341, 18161, 3209, 281, 360, 1045, 819, 721, 420, 50700, 50700, 365, 1045, 819, 11101, 13, 1485, 46691, 307, 516, 281, 312, 445, 31499, 2031, 11, 1392, 11, 370, 456, 311, 51004, 51004, 516, 281, 312, 257, 979, 8616, 8141, 300, 307, 516, 281, 29501, 264, 4846, 322, 264, 5598, 293, 321, 434, 516, 51380, 51380, 281, 360, 341, 538, 445, 46608, 3732, 6713, 13, 407, 341, 307, 437, 311, 16176, 538, 264, 979, 8616, 15995, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.08263189142400568, "compression_ratio": 1.7897196261682242, "no_speech_prob": 6.438610398618039e-06}, {"id": 20, "seek": 14152, "start": 141.52, "end": 146.88000000000002, "text": " here. And again this could be convolutional or not depending on which version you like.", "tokens": [50364, 510, 13, 400, 797, 341, 727, 312, 45216, 304, 420, 406, 5413, 322, 597, 3037, 291, 411, 13, 50632, 50680, 821, 311, 516, 281, 312, 364, 441, 16, 46691, 322, 264, 4111, 8062, 300, 1669, 309, 637, 11668, 11, 370, 341, 307, 588, 709, 51056, 51056, 411, 257, 637, 11668, 8399, 22660, 19866, 295, 264, 2010, 300, 321, 2825, 466, 1036, 1243, 13, 583, 550, 321, 434, 611, 516, 51368, 51368, 281, 909, 257, 2636, 1433, 293, 341, 2636, 1433, 307, 516, 281, 312, 1936, 257, 2199, 8213, 1508, 9902, 597, 51776, 51824], "temperature": 0.0, "avg_logprob": -0.08569136167827406, "compression_ratio": 1.61864406779661, "no_speech_prob": 1.4367354879141203e-06}, {"id": 21, "seek": 14152, "start": 147.84, "end": 155.36, "text": " There's going to be an L1 criterion on the feature vector that makes it sparse, so this is very much", "tokens": [50364, 510, 13, 400, 797, 341, 727, 312, 45216, 304, 420, 406, 5413, 322, 597, 3037, 291, 411, 13, 50632, 50680, 821, 311, 516, 281, 312, 364, 441, 16, 46691, 322, 264, 4111, 8062, 300, 1669, 309, 637, 11668, 11, 370, 341, 307, 588, 709, 51056, 51056, 411, 257, 637, 11668, 8399, 22660, 19866, 295, 264, 2010, 300, 321, 2825, 466, 1036, 1243, 13, 583, 550, 321, 434, 611, 516, 51368, 51368, 281, 909, 257, 2636, 1433, 293, 341, 2636, 1433, 307, 516, 281, 312, 1936, 257, 2199, 8213, 1508, 9902, 597, 51776, 51824], "temperature": 0.0, "avg_logprob": -0.08569136167827406, "compression_ratio": 1.61864406779661, "no_speech_prob": 1.4367354879141203e-06}, {"id": 22, "seek": 14152, "start": 155.36, "end": 161.60000000000002, "text": " like a sparse autoencoder of the type that we talked about last week. But then we're also going", "tokens": [50364, 510, 13, 400, 797, 341, 727, 312, 45216, 304, 420, 406, 5413, 322, 597, 3037, 291, 411, 13, 50632, 50680, 821, 311, 516, 281, 312, 364, 441, 16, 46691, 322, 264, 4111, 8062, 300, 1669, 309, 637, 11668, 11, 370, 341, 307, 588, 709, 51056, 51056, 411, 257, 637, 11668, 8399, 22660, 19866, 295, 264, 2010, 300, 321, 2825, 466, 1036, 1243, 13, 583, 550, 321, 434, 611, 516, 51368, 51368, 281, 909, 257, 2636, 1433, 293, 341, 2636, 1433, 307, 516, 281, 312, 1936, 257, 2199, 8213, 1508, 9902, 597, 51776, 51824], "temperature": 0.0, "avg_logprob": -0.08569136167827406, "compression_ratio": 1.61864406779661, "no_speech_prob": 1.4367354879141203e-06}, {"id": 23, "seek": 14152, "start": 161.60000000000002, "end": 169.76000000000002, "text": " to add a third term and this third term is going to be basically a simple linear classifier which", "tokens": [50364, 510, 13, 400, 797, 341, 727, 312, 45216, 304, 420, 406, 5413, 322, 597, 3037, 291, 411, 13, 50632, 50680, 821, 311, 516, 281, 312, 364, 441, 16, 46691, 322, 264, 4111, 8062, 300, 1669, 309, 637, 11668, 11, 370, 341, 307, 588, 709, 51056, 51056, 411, 257, 637, 11668, 8399, 22660, 19866, 295, 264, 2010, 300, 321, 2825, 466, 1036, 1243, 13, 583, 550, 321, 434, 611, 516, 51368, 51368, 281, 909, 257, 2636, 1433, 293, 341, 2636, 1433, 307, 516, 281, 312, 1936, 257, 2199, 8213, 1508, 9902, 597, 51776, 51824], "temperature": 0.0, "avg_logprob": -0.08569136167827406, "compression_ratio": 1.61864406779661, "no_speech_prob": 1.4367354879141203e-06}, {"id": 24, "seek": 16976, "start": 169.76, "end": 175.51999999999998, "text": " is going to try to predict a category, okay, and we're going to train the system to minimize all", "tokens": [50364, 307, 516, 281, 853, 281, 6069, 257, 7719, 11, 1392, 11, 293, 321, 434, 516, 281, 3847, 264, 1185, 281, 17522, 439, 50652, 50652, 1045, 11101, 412, 264, 912, 565, 13, 407, 341, 307, 257, 637, 11668, 8399, 22660, 19866, 300, 611, 9898, 281, 915, 14211, 50900, 50900, 300, 360, 257, 665, 1691, 412, 17630, 13, 400, 341, 307, 1333, 295, 257, 665, 636, 11, 291, 393, 536, 341, 294, 732, 819, 51208, 51208, 2098, 13, 509, 393, 536, 341, 382, 364, 8399, 22660, 19866, 300, 307, 28035, 3030, 10501, 665, 16949, 420, 291, 393, 51464, 51464, 536, 341, 382, 257, 1508, 9902, 11, 4825, 12, 8376, 260, 1508, 9902, 300, 307, 3890, 1602, 538, 364, 8399, 22660, 19866, 13, 708, 311, 264, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.12639287513072098, "compression_ratio": 1.8659003831417624, "no_speech_prob": 3.84477380066528e-06}, {"id": 25, "seek": 16976, "start": 175.51999999999998, "end": 180.48, "text": " three criteria at the same time. So this is a sparse autoencoder that also tries to find codes", "tokens": [50364, 307, 516, 281, 853, 281, 6069, 257, 7719, 11, 1392, 11, 293, 321, 434, 516, 281, 3847, 264, 1185, 281, 17522, 439, 50652, 50652, 1045, 11101, 412, 264, 912, 565, 13, 407, 341, 307, 257, 637, 11668, 8399, 22660, 19866, 300, 611, 9898, 281, 915, 14211, 50900, 50900, 300, 360, 257, 665, 1691, 412, 17630, 13, 400, 341, 307, 1333, 295, 257, 665, 636, 11, 291, 393, 536, 341, 294, 732, 819, 51208, 51208, 2098, 13, 509, 393, 536, 341, 382, 364, 8399, 22660, 19866, 300, 307, 28035, 3030, 10501, 665, 16949, 420, 291, 393, 51464, 51464, 536, 341, 382, 257, 1508, 9902, 11, 4825, 12, 8376, 260, 1508, 9902, 300, 307, 3890, 1602, 538, 364, 8399, 22660, 19866, 13, 708, 311, 264, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.12639287513072098, "compression_ratio": 1.8659003831417624, "no_speech_prob": 3.84477380066528e-06}, {"id": 26, "seek": 16976, "start": 180.48, "end": 186.64, "text": " that do a good job at prediction. And this is sort of a good way, you can see this in two different", "tokens": [50364, 307, 516, 281, 853, 281, 6069, 257, 7719, 11, 1392, 11, 293, 321, 434, 516, 281, 3847, 264, 1185, 281, 17522, 439, 50652, 50652, 1045, 11101, 412, 264, 912, 565, 13, 407, 341, 307, 257, 637, 11668, 8399, 22660, 19866, 300, 611, 9898, 281, 915, 14211, 50900, 50900, 300, 360, 257, 665, 1691, 412, 17630, 13, 400, 341, 307, 1333, 295, 257, 665, 636, 11, 291, 393, 536, 341, 294, 732, 819, 51208, 51208, 2098, 13, 509, 393, 536, 341, 382, 364, 8399, 22660, 19866, 300, 307, 28035, 3030, 10501, 665, 16949, 420, 291, 393, 51464, 51464, 536, 341, 382, 257, 1508, 9902, 11, 4825, 12, 8376, 260, 1508, 9902, 300, 307, 3890, 1602, 538, 364, 8399, 22660, 19866, 13, 708, 311, 264, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.12639287513072098, "compression_ratio": 1.8659003831417624, "no_speech_prob": 3.84477380066528e-06}, {"id": 27, "seek": 16976, "start": 186.64, "end": 191.76, "text": " ways. You can see this as an autoencoder that is biased towards producing good labels or you can", "tokens": [50364, 307, 516, 281, 853, 281, 6069, 257, 7719, 11, 1392, 11, 293, 321, 434, 516, 281, 3847, 264, 1185, 281, 17522, 439, 50652, 50652, 1045, 11101, 412, 264, 912, 565, 13, 407, 341, 307, 257, 637, 11668, 8399, 22660, 19866, 300, 611, 9898, 281, 915, 14211, 50900, 50900, 300, 360, 257, 665, 1691, 412, 17630, 13, 400, 341, 307, 1333, 295, 257, 665, 636, 11, 291, 393, 536, 341, 294, 732, 819, 51208, 51208, 2098, 13, 509, 393, 536, 341, 382, 364, 8399, 22660, 19866, 300, 307, 28035, 3030, 10501, 665, 16949, 420, 291, 393, 51464, 51464, 536, 341, 382, 257, 1508, 9902, 11, 4825, 12, 8376, 260, 1508, 9902, 300, 307, 3890, 1602, 538, 364, 8399, 22660, 19866, 13, 708, 311, 264, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.12639287513072098, "compression_ratio": 1.8659003831417624, "no_speech_prob": 3.84477380066528e-06}, {"id": 28, "seek": 16976, "start": 191.76, "end": 198.72, "text": " see this as a classifier, multi-layer classifier that is regularized by an autoencoder. What's the", "tokens": [50364, 307, 516, 281, 853, 281, 6069, 257, 7719, 11, 1392, 11, 293, 321, 434, 516, 281, 3847, 264, 1185, 281, 17522, 439, 50652, 50652, 1045, 11101, 412, 264, 912, 565, 13, 407, 341, 307, 257, 637, 11668, 8399, 22660, 19866, 300, 611, 9898, 281, 915, 14211, 50900, 50900, 300, 360, 257, 665, 1691, 412, 17630, 13, 400, 341, 307, 1333, 295, 257, 665, 636, 11, 291, 393, 536, 341, 294, 732, 819, 51208, 51208, 2098, 13, 509, 393, 536, 341, 382, 364, 8399, 22660, 19866, 300, 307, 28035, 3030, 10501, 665, 16949, 420, 291, 393, 51464, 51464, 536, 341, 382, 257, 1508, 9902, 11, 4825, 12, 8376, 260, 1508, 9902, 300, 307, 3890, 1602, 538, 364, 8399, 22660, 19866, 13, 708, 311, 264, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.12639287513072098, "compression_ratio": 1.8659003831417624, "no_speech_prob": 3.84477380066528e-06}, {"id": 29, "seek": 19872, "start": 198.72, "end": 204.48, "text": " advantage of this? Well the advantage is that by forcing the system to find representations here", "tokens": [50364, 5002, 295, 341, 30, 1042, 264, 5002, 307, 300, 538, 19030, 264, 1185, 281, 915, 33358, 510, 50652, 50652, 412, 264, 1150, 1036, 4583, 300, 393, 31499, 264, 4846, 11, 550, 291, 434, 1936, 3228, 3349, 264, 1185, 51008, 51008, 3030, 49844, 4122, 300, 5304, 382, 709, 1589, 466, 264, 4846, 382, 1944, 13, 51192, 51292, 407, 300, 1333, 295, 1669, 264, 4122, 29021, 498, 291, 528, 13, 467, 5874, 264, 1185, 281, 406, 8460, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.14667799502988405, "compression_ratio": 1.7168949771689497, "no_speech_prob": 4.936711320624454e-06}, {"id": 30, "seek": 19872, "start": 204.48, "end": 211.6, "text": " at the second last layer that can reconstruct the input, then you're basically biasing the system", "tokens": [50364, 5002, 295, 341, 30, 1042, 264, 5002, 307, 300, 538, 19030, 264, 1185, 281, 915, 33358, 510, 50652, 50652, 412, 264, 1150, 1036, 4583, 300, 393, 31499, 264, 4846, 11, 550, 291, 434, 1936, 3228, 3349, 264, 1185, 51008, 51008, 3030, 49844, 4122, 300, 5304, 382, 709, 1589, 466, 264, 4846, 382, 1944, 13, 51192, 51292, 407, 300, 1333, 295, 1669, 264, 4122, 29021, 498, 291, 528, 13, 467, 5874, 264, 1185, 281, 406, 8460, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.14667799502988405, "compression_ratio": 1.7168949771689497, "no_speech_prob": 4.936711320624454e-06}, {"id": 31, "seek": 19872, "start": 211.6, "end": 215.28, "text": " towards extracting features that contain as much information about the input as possible.", "tokens": [50364, 5002, 295, 341, 30, 1042, 264, 5002, 307, 300, 538, 19030, 264, 1185, 281, 915, 33358, 510, 50652, 50652, 412, 264, 1150, 1036, 4583, 300, 393, 31499, 264, 4846, 11, 550, 291, 434, 1936, 3228, 3349, 264, 1185, 51008, 51008, 3030, 49844, 4122, 300, 5304, 382, 709, 1589, 466, 264, 4846, 382, 1944, 13, 51192, 51292, 407, 300, 1333, 295, 1669, 264, 4122, 29021, 498, 291, 528, 13, 467, 5874, 264, 1185, 281, 406, 8460, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.14667799502988405, "compression_ratio": 1.7168949771689497, "no_speech_prob": 4.936711320624454e-06}, {"id": 32, "seek": 19872, "start": 217.28, "end": 225.44, "text": " So that sort of makes the features richer if you want. It forces the system to not generate", "tokens": [50364, 5002, 295, 341, 30, 1042, 264, 5002, 307, 300, 538, 19030, 264, 1185, 281, 915, 33358, 510, 50652, 50652, 412, 264, 1150, 1036, 4583, 300, 393, 31499, 264, 4846, 11, 550, 291, 434, 1936, 3228, 3349, 264, 1185, 51008, 51008, 3030, 49844, 4122, 300, 5304, 382, 709, 1589, 466, 264, 4846, 382, 1944, 13, 51192, 51292, 407, 300, 1333, 295, 1669, 264, 4122, 29021, 498, 291, 528, 13, 467, 5874, 264, 1185, 281, 406, 8460, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.14667799502988405, "compression_ratio": 1.7168949771689497, "no_speech_prob": 4.936711320624454e-06}, {"id": 33, "seek": 22544, "start": 225.44, "end": 230.07999999999998, "text": " degenerate features but to generate features that contain as much information as possible about", "tokens": [50364, 40520, 473, 4122, 457, 281, 8460, 4122, 300, 5304, 382, 709, 1589, 382, 1944, 466, 50596, 50596, 264, 4846, 13, 663, 1985, 1238, 731, 13, 286, 519, 309, 311, 364, 833, 12, 23040, 2769, 3170, 337, 3097, 18161, 36170, 51064, 51156, 570, 588, 2049, 321, 500, 380, 362, 1547, 7645, 3097, 1412, 420, 562, 264, 3097, 1412, 307, 1270, 51516, 51516, 300, 291, 500, 380, 362, 257, 688, 295, 10479, 281, 589, 365, 11, 1310, 309, 311, 257, 732, 420, 1045, 420, 2064, 1508, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.18397682363336737, "compression_ratio": 1.6740088105726871, "no_speech_prob": 6.239818503672723e-06}, {"id": 34, "seek": 22544, "start": 230.07999999999998, "end": 239.44, "text": " the input. That works pretty well. I think it's an under-explored method for training neural nets", "tokens": [50364, 40520, 473, 4122, 457, 281, 8460, 4122, 300, 5304, 382, 709, 1589, 382, 1944, 466, 50596, 50596, 264, 4846, 13, 663, 1985, 1238, 731, 13, 286, 519, 309, 311, 364, 833, 12, 23040, 2769, 3170, 337, 3097, 18161, 36170, 51064, 51156, 570, 588, 2049, 321, 500, 380, 362, 1547, 7645, 3097, 1412, 420, 562, 264, 3097, 1412, 307, 1270, 51516, 51516, 300, 291, 500, 380, 362, 257, 688, 295, 10479, 281, 589, 365, 11, 1310, 309, 311, 257, 732, 420, 1045, 420, 2064, 1508, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.18397682363336737, "compression_ratio": 1.6740088105726871, "no_speech_prob": 6.239818503672723e-06}, {"id": 35, "seek": 22544, "start": 241.28, "end": 248.48, "text": " because very often we don't have enough label training data or when the training data is such", "tokens": [50364, 40520, 473, 4122, 457, 281, 8460, 4122, 300, 5304, 382, 709, 1589, 382, 1944, 466, 50596, 50596, 264, 4846, 13, 663, 1985, 1238, 731, 13, 286, 519, 309, 311, 364, 833, 12, 23040, 2769, 3170, 337, 3097, 18161, 36170, 51064, 51156, 570, 588, 2049, 321, 500, 380, 362, 1547, 7645, 3097, 1412, 420, 562, 264, 3097, 1412, 307, 1270, 51516, 51516, 300, 291, 500, 380, 362, 257, 688, 295, 10479, 281, 589, 365, 11, 1310, 309, 311, 257, 732, 420, 1045, 420, 2064, 1508, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.18397682363336737, "compression_ratio": 1.6740088105726871, "no_speech_prob": 6.239818503672723e-06}, {"id": 36, "seek": 22544, "start": 248.48, "end": 253.12, "text": " that you don't have a lot of categories to work with, maybe it's a two or three or ten class", "tokens": [50364, 40520, 473, 4122, 457, 281, 8460, 4122, 300, 5304, 382, 709, 1589, 382, 1944, 466, 50596, 50596, 264, 4846, 13, 663, 1985, 1238, 731, 13, 286, 519, 309, 311, 364, 833, 12, 23040, 2769, 3170, 337, 3097, 18161, 36170, 51064, 51156, 570, 588, 2049, 321, 500, 380, 362, 1547, 7645, 3097, 1412, 420, 562, 264, 3097, 1412, 307, 1270, 51516, 51516, 300, 291, 500, 380, 362, 257, 688, 295, 10479, 281, 589, 365, 11, 1310, 309, 311, 257, 732, 420, 1045, 420, 2064, 1508, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.18397682363336737, "compression_ratio": 1.6740088105726871, "no_speech_prob": 6.239818503672723e-06}, {"id": 37, "seek": 25312, "start": 253.12, "end": 260.8, "text": " problem which we know tend to produce very degenerate features in a neural net as we discussed last time.", "tokens": [50364, 1154, 597, 321, 458, 3928, 281, 5258, 588, 40520, 473, 4122, 294, 257, 18161, 2533, 382, 321, 7152, 1036, 565, 13, 50748, 50796, 1396, 19030, 264, 1185, 281, 31499, 1936, 5112, 309, 291, 393, 380, 8460, 4122, 300, 366, 281, 51124, 51124, 40520, 473, 13, 492, 603, 611, 40520, 473, 300, 291, 393, 380, 31499, 264, 4846, 490, 309, 13, 407, 300, 311, 1333, 295, 51396, 51396, 257, 665, 11, 291, 727, 519, 295, 309, 382, 257, 665, 3890, 6545, 13, 10500, 637, 685, 507, 293, 3877, 637, 685, 507, 13, 407, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.20846964915593466, "compression_ratio": 1.706896551724138, "no_speech_prob": 3.0412520573008806e-06}, {"id": 38, "seek": 25312, "start": 261.76, "end": 268.32, "text": " Then forcing the system to reconstruct basically tells it you can't generate features that are to", "tokens": [50364, 1154, 597, 321, 458, 3928, 281, 5258, 588, 40520, 473, 4122, 294, 257, 18161, 2533, 382, 321, 7152, 1036, 565, 13, 50748, 50796, 1396, 19030, 264, 1185, 281, 31499, 1936, 5112, 309, 291, 393, 380, 8460, 4122, 300, 366, 281, 51124, 51124, 40520, 473, 13, 492, 603, 611, 40520, 473, 300, 291, 393, 380, 31499, 264, 4846, 490, 309, 13, 407, 300, 311, 1333, 295, 51396, 51396, 257, 665, 11, 291, 727, 519, 295, 309, 382, 257, 665, 3890, 6545, 13, 10500, 637, 685, 507, 293, 3877, 637, 685, 507, 13, 407, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.20846964915593466, "compression_ratio": 1.706896551724138, "no_speech_prob": 3.0412520573008806e-06}, {"id": 39, "seek": 25312, "start": 268.32, "end": 273.76, "text": " degenerate. We'll also degenerate that you can't reconstruct the input from it. So that's sort of", "tokens": [50364, 1154, 597, 321, 458, 3928, 281, 5258, 588, 40520, 473, 4122, 294, 257, 18161, 2533, 382, 321, 7152, 1036, 565, 13, 50748, 50796, 1396, 19030, 264, 1185, 281, 31499, 1936, 5112, 309, 291, 393, 380, 8460, 4122, 300, 366, 281, 51124, 51124, 40520, 473, 13, 492, 603, 611, 40520, 473, 300, 291, 393, 380, 31499, 264, 4846, 490, 309, 13, 407, 300, 311, 1333, 295, 51396, 51396, 257, 665, 11, 291, 727, 519, 295, 309, 382, 257, 665, 3890, 6545, 13, 10500, 637, 685, 507, 293, 3877, 637, 685, 507, 13, 407, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.20846964915593466, "compression_ratio": 1.706896551724138, "no_speech_prob": 3.0412520573008806e-06}, {"id": 40, "seek": 25312, "start": 273.76, "end": 280.24, "text": " a good, you could think of it as a good regularizer. Group sparsity and structure sparsity. So", "tokens": [50364, 1154, 597, 321, 458, 3928, 281, 5258, 588, 40520, 473, 4122, 294, 257, 18161, 2533, 382, 321, 7152, 1036, 565, 13, 50748, 50796, 1396, 19030, 264, 1185, 281, 31499, 1936, 5112, 309, 291, 393, 380, 8460, 4122, 300, 366, 281, 51124, 51124, 40520, 473, 13, 492, 603, 611, 40520, 473, 300, 291, 393, 380, 31499, 264, 4846, 490, 309, 13, 407, 300, 311, 1333, 295, 51396, 51396, 257, 665, 11, 291, 727, 519, 295, 309, 382, 257, 665, 3890, 6545, 13, 10500, 637, 685, 507, 293, 3877, 637, 685, 507, 13, 407, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.20846964915593466, "compression_ratio": 1.706896551724138, "no_speech_prob": 3.0412520573008806e-06}, {"id": 41, "seek": 28024, "start": 280.24, "end": 285.52, "text": " there's some work going back about 10 years, maybe a little more. In fact, the first work on this are", "tokens": [50364, 456, 311, 512, 589, 516, 646, 466, 1266, 924, 11, 1310, 257, 707, 544, 13, 682, 1186, 11, 264, 700, 589, 322, 341, 366, 50628, 50628, 466, 945, 924, 1331, 13, 1282, 264, 1558, 295, 1594, 637, 685, 507, 11, 437, 775, 300, 914, 30, 1692, 307, 264, 1558, 13, 440, 1558, 307, 50988, 50988, 281, 3847, 257, 1185, 281, 8460, 637, 11668, 4122, 457, 406, 445, 2710, 4122, 300, 366, 34086, 11, 584, 11, 51344, 51344, 538, 257, 3840, 295, 3754, 15892, 293, 4190, 457, 281, 1936, 5258, 637, 11668, 4122, 300, 366, 637, 11668, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.10864696502685547, "compression_ratio": 1.6652719665271967, "no_speech_prob": 8.18603780317062e-07}, {"id": 42, "seek": 28024, "start": 285.52, "end": 292.72, "text": " about 20 years old. On the idea of group sparsity, what does that mean? Here is the idea. The idea is", "tokens": [50364, 456, 311, 512, 589, 516, 646, 466, 1266, 924, 11, 1310, 257, 707, 544, 13, 682, 1186, 11, 264, 700, 589, 322, 341, 366, 50628, 50628, 466, 945, 924, 1331, 13, 1282, 264, 1558, 295, 1594, 637, 685, 507, 11, 437, 775, 300, 914, 30, 1692, 307, 264, 1558, 13, 440, 1558, 307, 50988, 50988, 281, 3847, 257, 1185, 281, 8460, 637, 11668, 4122, 457, 406, 445, 2710, 4122, 300, 366, 34086, 11, 584, 11, 51344, 51344, 538, 257, 3840, 295, 3754, 15892, 293, 4190, 457, 281, 1936, 5258, 637, 11668, 4122, 300, 366, 637, 11668, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.10864696502685547, "compression_ratio": 1.6652719665271967, "no_speech_prob": 8.18603780317062e-07}, {"id": 43, "seek": 28024, "start": 292.72, "end": 299.84000000000003, "text": " to train a system to generate sparse features but not just normal features that are extracted, say,", "tokens": [50364, 456, 311, 512, 589, 516, 646, 466, 1266, 924, 11, 1310, 257, 707, 544, 13, 682, 1186, 11, 264, 700, 589, 322, 341, 366, 50628, 50628, 466, 945, 924, 1331, 13, 1282, 264, 1558, 295, 1594, 637, 685, 507, 11, 437, 775, 300, 914, 30, 1692, 307, 264, 1558, 13, 440, 1558, 307, 50988, 50988, 281, 3847, 257, 1185, 281, 8460, 637, 11668, 4122, 457, 406, 445, 2710, 4122, 300, 366, 34086, 11, 584, 11, 51344, 51344, 538, 257, 3840, 295, 3754, 15892, 293, 4190, 457, 281, 1936, 5258, 637, 11668, 4122, 300, 366, 637, 11668, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.10864696502685547, "compression_ratio": 1.6652719665271967, "no_speech_prob": 8.18603780317062e-07}, {"id": 44, "seek": 28024, "start": 299.84000000000003, "end": 306.08, "text": " by a bunch of convolutions and values but to basically produce sparse features that are sparse", "tokens": [50364, 456, 311, 512, 589, 516, 646, 466, 1266, 924, 11, 1310, 257, 707, 544, 13, 682, 1186, 11, 264, 700, 589, 322, 341, 366, 50628, 50628, 466, 945, 924, 1331, 13, 1282, 264, 1558, 295, 1594, 637, 685, 507, 11, 437, 775, 300, 914, 30, 1692, 307, 264, 1558, 13, 440, 1558, 307, 50988, 50988, 281, 3847, 257, 1185, 281, 8460, 637, 11668, 4122, 457, 406, 445, 2710, 4122, 300, 366, 34086, 11, 584, 11, 51344, 51344, 538, 257, 3840, 295, 3754, 15892, 293, 4190, 457, 281, 1936, 5258, 637, 11668, 4122, 300, 366, 637, 11668, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.10864696502685547, "compression_ratio": 1.6652719665271967, "no_speech_prob": 8.18603780317062e-07}, {"id": 45, "seek": 30608, "start": 306.08, "end": 312.88, "text": " after the pooling. So you essentially have a system that consists of convolutions, non-linearity,", "tokens": [50364, 934, 264, 7005, 278, 13, 407, 291, 4476, 362, 257, 1185, 300, 14689, 295, 3754, 15892, 11, 2107, 12, 1889, 17409, 11, 50704, 50704, 293, 7005, 278, 13, 509, 853, 281, 652, 729, 4122, 637, 11668, 13, 821, 311, 257, 1230, 295, 819, 1985, 13, 440, 1558, 1709, 51060, 51060, 646, 281, 286, 8517, 259, 266, 293, 28664, 260, 294, 16382, 294, 264, 4319, 295, 14360, 32, 11, 6695, 6542, 5215, 13, 51392, 51432, 400, 550, 456, 645, 257, 1326, 661, 10577, 11, 472, 538, 422, 1229, 273, 2032, 294, 7506, 14429, 266, 311, 1594, 13, 51636, 51728], "temperature": 0.0, "avg_logprob": -0.18672489166259765, "compression_ratio": 1.476, "no_speech_prob": 7.645518962817732e-06}, {"id": 46, "seek": 30608, "start": 312.88, "end": 320.0, "text": " and pooling. You try to make those features sparse. There's a number of different works. The idea goes", "tokens": [50364, 934, 264, 7005, 278, 13, 407, 291, 4476, 362, 257, 1185, 300, 14689, 295, 3754, 15892, 11, 2107, 12, 1889, 17409, 11, 50704, 50704, 293, 7005, 278, 13, 509, 853, 281, 652, 729, 4122, 637, 11668, 13, 821, 311, 257, 1230, 295, 819, 1985, 13, 440, 1558, 1709, 51060, 51060, 646, 281, 286, 8517, 259, 266, 293, 28664, 260, 294, 16382, 294, 264, 4319, 295, 14360, 32, 11, 6695, 6542, 5215, 13, 51392, 51432, 400, 550, 456, 645, 257, 1326, 661, 10577, 11, 472, 538, 422, 1229, 273, 2032, 294, 7506, 14429, 266, 311, 1594, 13, 51636, 51728], "temperature": 0.0, "avg_logprob": -0.18672489166259765, "compression_ratio": 1.476, "no_speech_prob": 7.645518962817732e-06}, {"id": 47, "seek": 30608, "start": 320.0, "end": 326.64, "text": " back to Ivarinon and Hoyer in 2001 in the context of ICA, independent component analysis.", "tokens": [50364, 934, 264, 7005, 278, 13, 407, 291, 4476, 362, 257, 1185, 300, 14689, 295, 3754, 15892, 11, 2107, 12, 1889, 17409, 11, 50704, 50704, 293, 7005, 278, 13, 509, 853, 281, 652, 729, 4122, 637, 11668, 13, 821, 311, 257, 1230, 295, 819, 1985, 13, 440, 1558, 1709, 51060, 51060, 646, 281, 286, 8517, 259, 266, 293, 28664, 260, 294, 16382, 294, 264, 4319, 295, 14360, 32, 11, 6695, 6542, 5215, 13, 51392, 51432, 400, 550, 456, 645, 257, 1326, 661, 10577, 11, 472, 538, 422, 1229, 273, 2032, 294, 7506, 14429, 266, 311, 1594, 13, 51636, 51728], "temperature": 0.0, "avg_logprob": -0.18672489166259765, "compression_ratio": 1.476, "no_speech_prob": 7.645518962817732e-06}, {"id": 48, "seek": 30608, "start": 327.44, "end": 331.52, "text": " And then there were a few other papers, one by Ozyndero in Jeff Eaton's group.", "tokens": [50364, 934, 264, 7005, 278, 13, 407, 291, 4476, 362, 257, 1185, 300, 14689, 295, 3754, 15892, 11, 2107, 12, 1889, 17409, 11, 50704, 50704, 293, 7005, 278, 13, 509, 853, 281, 652, 729, 4122, 637, 11668, 13, 821, 311, 257, 1230, 295, 819, 1985, 13, 440, 1558, 1709, 51060, 51060, 646, 281, 286, 8517, 259, 266, 293, 28664, 260, 294, 16382, 294, 264, 4319, 295, 14360, 32, 11, 6695, 6542, 5215, 13, 51392, 51432, 400, 550, 456, 645, 257, 1326, 661, 10577, 11, 472, 538, 422, 1229, 273, 2032, 294, 7506, 14429, 266, 311, 1594, 13, 51636, 51728], "temperature": 0.0, "avg_logprob": -0.18672489166259765, "compression_ratio": 1.476, "no_speech_prob": 7.645518962817732e-06}, {"id": 49, "seek": 33152, "start": 331.52, "end": 337.35999999999996, "text": " And then Corai Cabotrullo, who was a student of mine back in the late 2000s. Carl Greger,", "tokens": [50364, 400, 550, 3925, 1301, 14704, 310, 81, 858, 78, 11, 567, 390, 257, 3107, 295, 3892, 646, 294, 264, 3469, 8132, 82, 13, 14256, 14986, 1321, 11, 50656, 50656, 567, 390, 31399, 493, 365, 385, 13, 7174, 1053, 6124, 304, 11, 567, 307, 294, 6190, 11, 293, 257, 3840, 295, 661, 561, 11, 50896, 50924, 322, 341, 1558, 295, 3877, 637, 685, 507, 13, 407, 264, 1558, 1936, 307, 291, 747, 485, 51200, 51276, 407, 512, 295, 729, 5245, 787, 362, 364, 2058, 19866, 13, 2188, 295, 552, 787, 362, 257, 979, 19866, 13, 400, 512, 295, 51480, 51480, 552, 366, 8399, 22660, 378, 433, 13, 407, 264, 472, 322, 264, 1411, 11, 422, 1229, 273, 2032, 311, 2316, 11, 307, 364, 2058, 19866, 12, 25202, 2316, 13, 51724, 51760], "temperature": 0.0, "avg_logprob": -0.3155694441361861, "compression_ratio": 1.6875, "no_speech_prob": 1.2217765288369264e-05}, {"id": 50, "seek": 33152, "start": 337.35999999999996, "end": 342.15999999999997, "text": " who was posed up with me. Julien Meral, who is in France, and a bunch of other people,", "tokens": [50364, 400, 550, 3925, 1301, 14704, 310, 81, 858, 78, 11, 567, 390, 257, 3107, 295, 3892, 646, 294, 264, 3469, 8132, 82, 13, 14256, 14986, 1321, 11, 50656, 50656, 567, 390, 31399, 493, 365, 385, 13, 7174, 1053, 6124, 304, 11, 567, 307, 294, 6190, 11, 293, 257, 3840, 295, 661, 561, 11, 50896, 50924, 322, 341, 1558, 295, 3877, 637, 685, 507, 13, 407, 264, 1558, 1936, 307, 291, 747, 485, 51200, 51276, 407, 512, 295, 729, 5245, 787, 362, 364, 2058, 19866, 13, 2188, 295, 552, 787, 362, 257, 979, 19866, 13, 400, 512, 295, 51480, 51480, 552, 366, 8399, 22660, 378, 433, 13, 407, 264, 472, 322, 264, 1411, 11, 422, 1229, 273, 2032, 311, 2316, 11, 307, 364, 2058, 19866, 12, 25202, 2316, 13, 51724, 51760], "temperature": 0.0, "avg_logprob": -0.3155694441361861, "compression_ratio": 1.6875, "no_speech_prob": 1.2217765288369264e-05}, {"id": 51, "seek": 33152, "start": 342.71999999999997, "end": 348.24, "text": " on this idea of structure sparsity. So the idea basically is you take...", "tokens": [50364, 400, 550, 3925, 1301, 14704, 310, 81, 858, 78, 11, 567, 390, 257, 3107, 295, 3892, 646, 294, 264, 3469, 8132, 82, 13, 14256, 14986, 1321, 11, 50656, 50656, 567, 390, 31399, 493, 365, 385, 13, 7174, 1053, 6124, 304, 11, 567, 307, 294, 6190, 11, 293, 257, 3840, 295, 661, 561, 11, 50896, 50924, 322, 341, 1558, 295, 3877, 637, 685, 507, 13, 407, 264, 1558, 1936, 307, 291, 747, 485, 51200, 51276, 407, 512, 295, 729, 5245, 787, 362, 364, 2058, 19866, 13, 2188, 295, 552, 787, 362, 257, 979, 19866, 13, 400, 512, 295, 51480, 51480, 552, 366, 8399, 22660, 378, 433, 13, 407, 264, 472, 322, 264, 1411, 11, 422, 1229, 273, 2032, 311, 2316, 11, 307, 364, 2058, 19866, 12, 25202, 2316, 13, 51724, 51760], "temperature": 0.0, "avg_logprob": -0.3155694441361861, "compression_ratio": 1.6875, "no_speech_prob": 1.2217765288369264e-05}, {"id": 52, "seek": 33152, "start": 349.76, "end": 353.84, "text": " So some of those models only have an encoder. Some of them only have a decoder. And some of", "tokens": [50364, 400, 550, 3925, 1301, 14704, 310, 81, 858, 78, 11, 567, 390, 257, 3107, 295, 3892, 646, 294, 264, 3469, 8132, 82, 13, 14256, 14986, 1321, 11, 50656, 50656, 567, 390, 31399, 493, 365, 385, 13, 7174, 1053, 6124, 304, 11, 567, 307, 294, 6190, 11, 293, 257, 3840, 295, 661, 561, 11, 50896, 50924, 322, 341, 1558, 295, 3877, 637, 685, 507, 13, 407, 264, 1558, 1936, 307, 291, 747, 485, 51200, 51276, 407, 512, 295, 729, 5245, 787, 362, 364, 2058, 19866, 13, 2188, 295, 552, 787, 362, 257, 979, 19866, 13, 400, 512, 295, 51480, 51480, 552, 366, 8399, 22660, 378, 433, 13, 407, 264, 472, 322, 264, 1411, 11, 422, 1229, 273, 2032, 311, 2316, 11, 307, 364, 2058, 19866, 12, 25202, 2316, 13, 51724, 51760], "temperature": 0.0, "avg_logprob": -0.3155694441361861, "compression_ratio": 1.6875, "no_speech_prob": 1.2217765288369264e-05}, {"id": 53, "seek": 33152, "start": 353.84, "end": 358.71999999999997, "text": " them are autoencoders. So the one on the left, Ozyndero's model, is an encoder-only model.", "tokens": [50364, 400, 550, 3925, 1301, 14704, 310, 81, 858, 78, 11, 567, 390, 257, 3107, 295, 3892, 646, 294, 264, 3469, 8132, 82, 13, 14256, 14986, 1321, 11, 50656, 50656, 567, 390, 31399, 493, 365, 385, 13, 7174, 1053, 6124, 304, 11, 567, 307, 294, 6190, 11, 293, 257, 3840, 295, 661, 561, 11, 50896, 50924, 322, 341, 1558, 295, 3877, 637, 685, 507, 13, 407, 264, 1558, 1936, 307, 291, 747, 485, 51200, 51276, 407, 512, 295, 729, 5245, 787, 362, 364, 2058, 19866, 13, 2188, 295, 552, 787, 362, 257, 979, 19866, 13, 400, 512, 295, 51480, 51480, 552, 366, 8399, 22660, 378, 433, 13, 407, 264, 472, 322, 264, 1411, 11, 422, 1229, 273, 2032, 311, 2316, 11, 307, 364, 2058, 19866, 12, 25202, 2316, 13, 51724, 51760], "temperature": 0.0, "avg_logprob": -0.3155694441361861, "compression_ratio": 1.6875, "no_speech_prob": 1.2217765288369264e-05}, {"id": 54, "seek": 35872, "start": 358.72, "end": 364.0, "text": " Julien Meral's model is a decoder-only model. And Corai Cabotrullo's model is", "tokens": [50364, 7174, 1053, 6124, 304, 311, 2316, 307, 257, 979, 19866, 12, 25202, 2316, 13, 400, 3925, 1301, 14704, 310, 81, 858, 78, 311, 2316, 307, 50628, 50664, 1936, 364, 8399, 22660, 19866, 11, 257, 637, 11668, 8399, 22660, 19866, 295, 264, 2010, 300, 321, 2825, 466, 1036, 565, 13, 50844, 50960, 407, 577, 775, 300, 589, 30, 961, 311, 747, 11, 584, 11, 364, 2058, 19866, 12, 25202, 2316, 13, 509, 362, 257, 4111, 8947, 284, 11, 51356, 51356, 597, 14689, 295, 3754, 15892, 420, 1310, 445, 1737, 12, 9826, 292, 51576, 51696], "temperature": 0.0, "avg_logprob": -0.22025604248046876, "compression_ratio": 1.5432692307692308, "no_speech_prob": 4.936361619911622e-06}, {"id": 55, "seek": 35872, "start": 364.72, "end": 368.32000000000005, "text": " basically an autoencoder, a sparse autoencoder of the type that we talked about last time.", "tokens": [50364, 7174, 1053, 6124, 304, 311, 2316, 307, 257, 979, 19866, 12, 25202, 2316, 13, 400, 3925, 1301, 14704, 310, 81, 858, 78, 311, 2316, 307, 50628, 50664, 1936, 364, 8399, 22660, 19866, 11, 257, 637, 11668, 8399, 22660, 19866, 295, 264, 2010, 300, 321, 2825, 466, 1036, 565, 13, 50844, 50960, 407, 577, 775, 300, 589, 30, 961, 311, 747, 11, 584, 11, 364, 2058, 19866, 12, 25202, 2316, 13, 509, 362, 257, 4111, 8947, 284, 11, 51356, 51356, 597, 14689, 295, 3754, 15892, 420, 1310, 445, 1737, 12, 9826, 292, 51576, 51696], "temperature": 0.0, "avg_logprob": -0.22025604248046876, "compression_ratio": 1.5432692307692308, "no_speech_prob": 4.936361619911622e-06}, {"id": 56, "seek": 35872, "start": 370.64000000000004, "end": 378.56, "text": " So how does that work? Let's take, say, an encoder-only model. You have a feature extractor,", "tokens": [50364, 7174, 1053, 6124, 304, 311, 2316, 307, 257, 979, 19866, 12, 25202, 2316, 13, 400, 3925, 1301, 14704, 310, 81, 858, 78, 311, 2316, 307, 50628, 50664, 1936, 364, 8399, 22660, 19866, 11, 257, 637, 11668, 8399, 22660, 19866, 295, 264, 2010, 300, 321, 2825, 466, 1036, 565, 13, 50844, 50960, 407, 577, 775, 300, 589, 30, 961, 311, 747, 11, 584, 11, 364, 2058, 19866, 12, 25202, 2316, 13, 509, 362, 257, 4111, 8947, 284, 11, 51356, 51356, 597, 14689, 295, 3754, 15892, 420, 1310, 445, 1737, 12, 9826, 292, 51576, 51696], "temperature": 0.0, "avg_logprob": -0.22025604248046876, "compression_ratio": 1.5432692307692308, "no_speech_prob": 4.936361619911622e-06}, {"id": 57, "seek": 35872, "start": 378.56, "end": 382.96000000000004, "text": " which consists of convolutions or maybe just free-connected", "tokens": [50364, 7174, 1053, 6124, 304, 311, 2316, 307, 257, 979, 19866, 12, 25202, 2316, 13, 400, 3925, 1301, 14704, 310, 81, 858, 78, 311, 2316, 307, 50628, 50664, 1936, 364, 8399, 22660, 19866, 11, 257, 637, 11668, 8399, 22660, 19866, 295, 264, 2010, 300, 321, 2825, 466, 1036, 565, 13, 50844, 50960, 407, 577, 775, 300, 589, 30, 961, 311, 747, 11, 584, 11, 364, 2058, 19866, 12, 25202, 2316, 13, 509, 362, 257, 4111, 8947, 284, 11, 51356, 51356, 597, 14689, 295, 3754, 15892, 420, 1310, 445, 1737, 12, 9826, 292, 51576, 51696], "temperature": 0.0, "avg_logprob": -0.22025604248046876, "compression_ratio": 1.5432692307692308, "no_speech_prob": 4.936361619911622e-06}, {"id": 58, "seek": 38296, "start": 382.96, "end": 389.91999999999996, "text": " matrices over an image patch, for example. And then instead of forcing the output of this to be", "tokens": [50364, 32284, 670, 364, 3256, 9972, 11, 337, 1365, 13, 400, 550, 2602, 295, 19030, 264, 5598, 295, 341, 281, 312, 50712, 50712, 934, 257, 2107, 12, 1889, 17409, 11, 2602, 295, 19030, 300, 281, 312, 637, 11668, 11, 291, 829, 257, 7005, 278, 4583, 293, 291, 51000, 51000, 3464, 264, 7005, 278, 281, 312, 637, 11668, 13, 400, 341, 13165, 281, 439, 1045, 295, 729, 13, 51288, 51388, 407, 510, 307, 257, 544, 2685, 1365, 13, 639, 307, 264, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.2465533980404038, "compression_ratio": 1.7401129943502824, "no_speech_prob": 1.1831012670882046e-05}, {"id": 59, "seek": 38296, "start": 389.91999999999996, "end": 395.68, "text": " after a non-linearity, instead of forcing that to be sparse, you put a pooling layer and you", "tokens": [50364, 32284, 670, 364, 3256, 9972, 11, 337, 1365, 13, 400, 550, 2602, 295, 19030, 264, 5598, 295, 341, 281, 312, 50712, 50712, 934, 257, 2107, 12, 1889, 17409, 11, 2602, 295, 19030, 300, 281, 312, 637, 11668, 11, 291, 829, 257, 7005, 278, 4583, 293, 291, 51000, 51000, 3464, 264, 7005, 278, 281, 312, 637, 11668, 13, 400, 341, 13165, 281, 439, 1045, 295, 729, 13, 51288, 51388, 407, 510, 307, 257, 544, 2685, 1365, 13, 639, 307, 264, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.2465533980404038, "compression_ratio": 1.7401129943502824, "no_speech_prob": 1.1831012670882046e-05}, {"id": 60, "seek": 38296, "start": 395.68, "end": 401.44, "text": " force the pooling to be sparse. And this applies to all three of those.", "tokens": [50364, 32284, 670, 364, 3256, 9972, 11, 337, 1365, 13, 400, 550, 2602, 295, 19030, 264, 5598, 295, 341, 281, 312, 50712, 50712, 934, 257, 2107, 12, 1889, 17409, 11, 2602, 295, 19030, 300, 281, 312, 637, 11668, 11, 291, 829, 257, 7005, 278, 4583, 293, 291, 51000, 51000, 3464, 264, 7005, 278, 281, 312, 637, 11668, 13, 400, 341, 13165, 281, 439, 1045, 295, 729, 13, 51288, 51388, 407, 510, 307, 257, 544, 2685, 1365, 13, 639, 307, 264, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.2465533980404038, "compression_ratio": 1.7401129943502824, "no_speech_prob": 1.1831012670882046e-05}, {"id": 61, "seek": 38296, "start": 403.44, "end": 406.71999999999997, "text": " So here is a more specific example. This is the", "tokens": [50364, 32284, 670, 364, 3256, 9972, 11, 337, 1365, 13, 400, 550, 2602, 295, 19030, 264, 5598, 295, 341, 281, 312, 50712, 50712, 934, 257, 2107, 12, 1889, 17409, 11, 2602, 295, 19030, 300, 281, 312, 637, 11668, 11, 291, 829, 257, 7005, 278, 4583, 293, 291, 51000, 51000, 3464, 264, 7005, 278, 281, 312, 637, 11668, 13, 400, 341, 13165, 281, 439, 1045, 295, 729, 13, 51288, 51388, 407, 510, 307, 257, 544, 2685, 1365, 13, 639, 307, 264, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.2465533980404038, "compression_ratio": 1.7401129943502824, "no_speech_prob": 1.1831012670882046e-05}, {"id": 62, "seek": 40672, "start": 406.72, "end": 414.08000000000004, "text": " version that Corai Cabotrullo did for his PhD thesis, where he had a sparse autoencoder. So", "tokens": [50364, 3037, 300, 3925, 1301, 14704, 310, 81, 858, 78, 630, 337, 702, 14476, 22288, 11, 689, 415, 632, 257, 637, 11668, 8399, 22660, 19866, 13, 407, 50732, 50732, 291, 362, 364, 43430, 2445, 11, 290, 36, 295, 321, 8461, 13, 467, 727, 312, 3866, 7914, 13, 682, 341, 1389, 11, 309, 390, 51020, 51020, 1936, 445, 732, 7914, 365, 472, 2107, 12, 1889, 17409, 13, 509, 362, 257, 979, 19866, 11, 597, 294, 341, 1389, 390, 8213, 11, 51308, 51308, 261, 35, 1413, 710, 13, 509, 362, 257, 48994, 7006, 710, 13, 400, 300, 48994, 7006, 11, 2602, 295, 516, 281, 257, 51556, 51556], "temperature": 0.0, "avg_logprob": -0.19411133820155882, "compression_ratio": 1.609442060085837, "no_speech_prob": 2.561092514952179e-06}, {"id": 63, "seek": 40672, "start": 414.08000000000004, "end": 419.84000000000003, "text": " you have an encoding function, gE of weyi. It could be multiple layers. In this case, it was", "tokens": [50364, 3037, 300, 3925, 1301, 14704, 310, 81, 858, 78, 630, 337, 702, 14476, 22288, 11, 689, 415, 632, 257, 637, 11668, 8399, 22660, 19866, 13, 407, 50732, 50732, 291, 362, 364, 43430, 2445, 11, 290, 36, 295, 321, 8461, 13, 467, 727, 312, 3866, 7914, 13, 682, 341, 1389, 11, 309, 390, 51020, 51020, 1936, 445, 732, 7914, 365, 472, 2107, 12, 1889, 17409, 13, 509, 362, 257, 979, 19866, 11, 597, 294, 341, 1389, 390, 8213, 11, 51308, 51308, 261, 35, 1413, 710, 13, 509, 362, 257, 48994, 7006, 710, 13, 400, 300, 48994, 7006, 11, 2602, 295, 516, 281, 257, 51556, 51556], "temperature": 0.0, "avg_logprob": -0.19411133820155882, "compression_ratio": 1.609442060085837, "no_speech_prob": 2.561092514952179e-06}, {"id": 64, "seek": 40672, "start": 419.84000000000003, "end": 425.6, "text": " basically just two layers with one non-linearity. You have a decoder, which in this case was linear,", "tokens": [50364, 3037, 300, 3925, 1301, 14704, 310, 81, 858, 78, 630, 337, 702, 14476, 22288, 11, 689, 415, 632, 257, 637, 11668, 8399, 22660, 19866, 13, 407, 50732, 50732, 291, 362, 364, 43430, 2445, 11, 290, 36, 295, 321, 8461, 13, 467, 727, 312, 3866, 7914, 13, 682, 341, 1389, 11, 309, 390, 51020, 51020, 1936, 445, 732, 7914, 365, 472, 2107, 12, 1889, 17409, 13, 509, 362, 257, 979, 19866, 11, 597, 294, 341, 1389, 390, 8213, 11, 51308, 51308, 261, 35, 1413, 710, 13, 509, 362, 257, 48994, 7006, 710, 13, 400, 300, 48994, 7006, 11, 2602, 295, 516, 281, 257, 51556, 51556], "temperature": 0.0, "avg_logprob": -0.19411133820155882, "compression_ratio": 1.609442060085837, "no_speech_prob": 2.561092514952179e-06}, {"id": 65, "seek": 40672, "start": 425.6, "end": 430.56, "text": " wD times z. You have a latent variable z. And that latent variable, instead of going to a", "tokens": [50364, 3037, 300, 3925, 1301, 14704, 310, 81, 858, 78, 630, 337, 702, 14476, 22288, 11, 689, 415, 632, 257, 637, 11668, 8399, 22660, 19866, 13, 407, 50732, 50732, 291, 362, 364, 43430, 2445, 11, 290, 36, 295, 321, 8461, 13, 467, 727, 312, 3866, 7914, 13, 682, 341, 1389, 11, 309, 390, 51020, 51020, 1936, 445, 732, 7914, 365, 472, 2107, 12, 1889, 17409, 13, 509, 362, 257, 979, 19866, 11, 597, 294, 341, 1389, 390, 8213, 11, 51308, 51308, 261, 35, 1413, 710, 13, 509, 362, 257, 48994, 7006, 710, 13, 400, 300, 48994, 7006, 11, 2602, 295, 516, 281, 257, 51556, 51556], "temperature": 0.0, "avg_logprob": -0.19411133820155882, "compression_ratio": 1.609442060085837, "no_speech_prob": 2.561092514952179e-06}, {"id": 66, "seek": 43056, "start": 430.56, "end": 438.96, "text": " L1, it goes through basically an L2, but it's L2 over groups. So you take a group of components of", "tokens": [50364, 441, 16, 11, 309, 1709, 807, 1936, 364, 441, 17, 11, 457, 309, 311, 441, 17, 670, 3935, 13, 407, 291, 747, 257, 1594, 295, 6677, 295, 50784, 50784, 710, 13, 509, 14722, 264, 441, 17, 2026, 11, 406, 264, 3732, 295, 264, 441, 17, 2026, 11, 457, 264, 441, 17, 2026, 11, 597, 1355, 264, 3732, 51076, 51076, 5593, 295, 264, 2408, 295, 264, 4190, 295, 729, 6677, 11, 295, 264, 3732, 295, 729, 6677, 13, 407, 747, 1184, 51420, 51420, 6542, 11, 14722, 264, 3732, 11, 293, 550, 14722, 264, 2408, 295, 264, 4190, 295, 264, 6677, 13, 407, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.2032108670189267, "compression_ratio": 2.108695652173913, "no_speech_prob": 2.0459481675061397e-05}, {"id": 67, "seek": 43056, "start": 438.96, "end": 444.8, "text": " z. You compute the L2 norm, not the square of the L2 norm, but the L2 norm, which means the square", "tokens": [50364, 441, 16, 11, 309, 1709, 807, 1936, 364, 441, 17, 11, 457, 309, 311, 441, 17, 670, 3935, 13, 407, 291, 747, 257, 1594, 295, 6677, 295, 50784, 50784, 710, 13, 509, 14722, 264, 441, 17, 2026, 11, 406, 264, 3732, 295, 264, 441, 17, 2026, 11, 457, 264, 441, 17, 2026, 11, 597, 1355, 264, 3732, 51076, 51076, 5593, 295, 264, 2408, 295, 264, 4190, 295, 729, 6677, 11, 295, 264, 3732, 295, 729, 6677, 13, 407, 747, 1184, 51420, 51420, 6542, 11, 14722, 264, 3732, 11, 293, 550, 14722, 264, 2408, 295, 264, 4190, 295, 264, 6677, 13, 407, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.2032108670189267, "compression_ratio": 2.108695652173913, "no_speech_prob": 2.0459481675061397e-05}, {"id": 68, "seek": 43056, "start": 444.8, "end": 451.68, "text": " root of the sum of the values of those components, of the square of those components. So take each", "tokens": [50364, 441, 16, 11, 309, 1709, 807, 1936, 364, 441, 17, 11, 457, 309, 311, 441, 17, 670, 3935, 13, 407, 291, 747, 257, 1594, 295, 6677, 295, 50784, 50784, 710, 13, 509, 14722, 264, 441, 17, 2026, 11, 406, 264, 3732, 295, 264, 441, 17, 2026, 11, 457, 264, 441, 17, 2026, 11, 597, 1355, 264, 3732, 51076, 51076, 5593, 295, 264, 2408, 295, 264, 4190, 295, 729, 6677, 11, 295, 264, 3732, 295, 729, 6677, 13, 407, 747, 1184, 51420, 51420, 6542, 11, 14722, 264, 3732, 11, 293, 550, 14722, 264, 2408, 295, 264, 4190, 295, 264, 6677, 13, 407, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.2032108670189267, "compression_ratio": 2.108695652173913, "no_speech_prob": 2.0459481675061397e-05}, {"id": 69, "seek": 43056, "start": 451.68, "end": 458.08, "text": " component, compute the square, and then compute the sum of the values of the components. So", "tokens": [50364, 441, 16, 11, 309, 1709, 807, 1936, 364, 441, 17, 11, 457, 309, 311, 441, 17, 670, 3935, 13, 407, 291, 747, 257, 1594, 295, 6677, 295, 50784, 50784, 710, 13, 509, 14722, 264, 441, 17, 2026, 11, 406, 264, 3732, 295, 264, 441, 17, 2026, 11, 457, 264, 441, 17, 2026, 11, 597, 1355, 264, 3732, 51076, 51076, 5593, 295, 264, 2408, 295, 264, 4190, 295, 729, 6677, 11, 295, 264, 3732, 295, 729, 6677, 13, 407, 747, 1184, 51420, 51420, 6542, 11, 14722, 264, 3732, 11, 293, 550, 14722, 264, 2408, 295, 264, 4190, 295, 264, 6677, 13, 407, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.2032108670189267, "compression_ratio": 2.108695652173913, "no_speech_prob": 2.0459481675061397e-05}, {"id": 70, "seek": 45808, "start": 458.08, "end": 463.28, "text": " take each component, compute the square, and then compute the sum of a group of those squares, and", "tokens": [50364, 747, 1184, 6542, 11, 14722, 264, 3732, 11, 293, 550, 14722, 264, 2408, 295, 257, 1594, 295, 729, 19368, 11, 293, 50624, 50624, 550, 14722, 264, 3732, 5593, 295, 300, 13, 407, 300, 311, 264, 441, 17, 2026, 1951, 300, 1594, 13, 400, 550, 291, 360, 50888, 50888, 341, 337, 3866, 3935, 13, 440, 3935, 393, 312, 33535, 420, 2107, 12, 3570, 15639, 13, 400, 291, 14722, 51132, 51132, 641, 2408, 13, 400, 300, 311, 428, 3890, 6545, 13, 663, 311, 428, 637, 685, 507, 3890, 6545, 13, 407, 437, 775, 300, 3928, 281, 51412, 51412, 360, 30, 467, 12258, 281, 1936, 1261, 766, 264, 6674, 1230, 295, 3935, 13, 440, 1185, 1936, 307, 637, 685, 507, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.09315147872798699, "compression_ratio": 1.9634146341463414, "no_speech_prob": 9.817484169616364e-06}, {"id": 71, "seek": 45808, "start": 463.28, "end": 468.56, "text": " then compute the square root of that. So that's the L2 norm within that group. And then you do", "tokens": [50364, 747, 1184, 6542, 11, 14722, 264, 3732, 11, 293, 550, 14722, 264, 2408, 295, 257, 1594, 295, 729, 19368, 11, 293, 50624, 50624, 550, 14722, 264, 3732, 5593, 295, 300, 13, 407, 300, 311, 264, 441, 17, 2026, 1951, 300, 1594, 13, 400, 550, 291, 360, 50888, 50888, 341, 337, 3866, 3935, 13, 440, 3935, 393, 312, 33535, 420, 2107, 12, 3570, 15639, 13, 400, 291, 14722, 51132, 51132, 641, 2408, 13, 400, 300, 311, 428, 3890, 6545, 13, 663, 311, 428, 637, 685, 507, 3890, 6545, 13, 407, 437, 775, 300, 3928, 281, 51412, 51412, 360, 30, 467, 12258, 281, 1936, 1261, 766, 264, 6674, 1230, 295, 3935, 13, 440, 1185, 1936, 307, 637, 685, 507, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.09315147872798699, "compression_ratio": 1.9634146341463414, "no_speech_prob": 9.817484169616364e-06}, {"id": 72, "seek": 45808, "start": 468.56, "end": 473.44, "text": " this for multiple groups. The groups can be overlapping or non-overlapping. And you compute", "tokens": [50364, 747, 1184, 6542, 11, 14722, 264, 3732, 11, 293, 550, 14722, 264, 2408, 295, 257, 1594, 295, 729, 19368, 11, 293, 50624, 50624, 550, 14722, 264, 3732, 5593, 295, 300, 13, 407, 300, 311, 264, 441, 17, 2026, 1951, 300, 1594, 13, 400, 550, 291, 360, 50888, 50888, 341, 337, 3866, 3935, 13, 440, 3935, 393, 312, 33535, 420, 2107, 12, 3570, 15639, 13, 400, 291, 14722, 51132, 51132, 641, 2408, 13, 400, 300, 311, 428, 3890, 6545, 13, 663, 311, 428, 637, 685, 507, 3890, 6545, 13, 407, 437, 775, 300, 3928, 281, 51412, 51412, 360, 30, 467, 12258, 281, 1936, 1261, 766, 264, 6674, 1230, 295, 3935, 13, 440, 1185, 1936, 307, 637, 685, 507, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.09315147872798699, "compression_ratio": 1.9634146341463414, "no_speech_prob": 9.817484169616364e-06}, {"id": 73, "seek": 45808, "start": 473.44, "end": 479.03999999999996, "text": " their sum. And that's your regularizer. That's your sparsity regularizer. So what does that tend to", "tokens": [50364, 747, 1184, 6542, 11, 14722, 264, 3732, 11, 293, 550, 14722, 264, 2408, 295, 257, 1594, 295, 729, 19368, 11, 293, 50624, 50624, 550, 14722, 264, 3732, 5593, 295, 300, 13, 407, 300, 311, 264, 441, 17, 2026, 1951, 300, 1594, 13, 400, 550, 291, 360, 50888, 50888, 341, 337, 3866, 3935, 13, 440, 3935, 393, 312, 33535, 420, 2107, 12, 3570, 15639, 13, 400, 291, 14722, 51132, 51132, 641, 2408, 13, 400, 300, 311, 428, 3890, 6545, 13, 663, 311, 428, 637, 685, 507, 3890, 6545, 13, 407, 437, 775, 300, 3928, 281, 51412, 51412, 360, 30, 467, 12258, 281, 1936, 1261, 766, 264, 6674, 1230, 295, 3935, 13, 440, 1185, 1936, 307, 637, 685, 507, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.09315147872798699, "compression_ratio": 1.9634146341463414, "no_speech_prob": 9.817484169616364e-06}, {"id": 74, "seek": 45808, "start": 479.03999999999996, "end": 487.76, "text": " do? It tends to basically turn off the maximum number of groups. The system basically is sparsity", "tokens": [50364, 747, 1184, 6542, 11, 14722, 264, 3732, 11, 293, 550, 14722, 264, 2408, 295, 257, 1594, 295, 729, 19368, 11, 293, 50624, 50624, 550, 14722, 264, 3732, 5593, 295, 300, 13, 407, 300, 311, 264, 441, 17, 2026, 1951, 300, 1594, 13, 400, 550, 291, 360, 50888, 50888, 341, 337, 3866, 3935, 13, 440, 3935, 393, 312, 33535, 420, 2107, 12, 3570, 15639, 13, 400, 291, 14722, 51132, 51132, 641, 2408, 13, 400, 300, 311, 428, 3890, 6545, 13, 663, 311, 428, 637, 685, 507, 3890, 6545, 13, 407, 437, 775, 300, 3928, 281, 51412, 51412, 360, 30, 467, 12258, 281, 1936, 1261, 766, 264, 6674, 1230, 295, 3935, 13, 440, 1185, 1936, 307, 637, 685, 507, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.09315147872798699, "compression_ratio": 1.9634146341463414, "no_speech_prob": 9.817484169616364e-06}, {"id": 75, "seek": 48776, "start": 487.76, "end": 492.96, "text": " on groups. So it wants the smallest number of groups to be on at any one time. But within a", "tokens": [50364, 322, 3935, 13, 407, 309, 2738, 264, 16998, 1230, 295, 3935, 281, 312, 322, 412, 604, 472, 565, 13, 583, 1951, 257, 50624, 50624, 1594, 11, 570, 309, 311, 364, 441, 17, 2026, 1951, 257, 1594, 11, 309, 1177, 380, 1127, 577, 867, 6815, 366, 322, 1951, 257, 1594, 13, 50864, 50940, 407, 867, 6815, 393, 312, 322, 1951, 257, 1594, 13, 407, 437, 775, 300, 360, 30, 467, 5874, 264, 1185, 1936, 51276, 51328, 281, 1594, 1951, 257, 7005, 4122, 300, 1261, 322, 16561, 13, 407, 498, 291, 362, 4122, 300, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06714945534865062, "compression_ratio": 1.8203883495145632, "no_speech_prob": 5.0146322791988496e-06}, {"id": 76, "seek": 48776, "start": 492.96, "end": 497.76, "text": " group, because it's an L2 norm within a group, it doesn't care how many units are on within a group.", "tokens": [50364, 322, 3935, 13, 407, 309, 2738, 264, 16998, 1230, 295, 3935, 281, 312, 322, 412, 604, 472, 565, 13, 583, 1951, 257, 50624, 50624, 1594, 11, 570, 309, 311, 364, 441, 17, 2026, 1951, 257, 1594, 11, 309, 1177, 380, 1127, 577, 867, 6815, 366, 322, 1951, 257, 1594, 13, 50864, 50940, 407, 867, 6815, 393, 312, 322, 1951, 257, 1594, 13, 407, 437, 775, 300, 360, 30, 467, 5874, 264, 1185, 1936, 51276, 51328, 281, 1594, 1951, 257, 7005, 4122, 300, 1261, 322, 16561, 13, 407, 498, 291, 362, 4122, 300, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06714945534865062, "compression_ratio": 1.8203883495145632, "no_speech_prob": 5.0146322791988496e-06}, {"id": 77, "seek": 48776, "start": 499.28, "end": 506.0, "text": " So many units can be on within a group. So what does that do? It forces the system basically", "tokens": [50364, 322, 3935, 13, 407, 309, 2738, 264, 16998, 1230, 295, 3935, 281, 312, 322, 412, 604, 472, 565, 13, 583, 1951, 257, 50624, 50624, 1594, 11, 570, 309, 311, 364, 441, 17, 2026, 1951, 257, 1594, 11, 309, 1177, 380, 1127, 577, 867, 6815, 366, 322, 1951, 257, 1594, 13, 50864, 50940, 407, 867, 6815, 393, 312, 322, 1951, 257, 1594, 13, 407, 437, 775, 300, 360, 30, 467, 5874, 264, 1185, 1936, 51276, 51328, 281, 1594, 1951, 257, 7005, 4122, 300, 1261, 322, 16561, 13, 407, 498, 291, 362, 4122, 300, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06714945534865062, "compression_ratio": 1.8203883495145632, "no_speech_prob": 5.0146322791988496e-06}, {"id": 78, "seek": 48776, "start": 507.03999999999996, "end": 513.76, "text": " to group within a pool features that turn on simultaneously. So if you have features that", "tokens": [50364, 322, 3935, 13, 407, 309, 2738, 264, 16998, 1230, 295, 3935, 281, 312, 322, 412, 604, 472, 565, 13, 583, 1951, 257, 50624, 50624, 1594, 11, 570, 309, 311, 364, 441, 17, 2026, 1951, 257, 1594, 11, 309, 1177, 380, 1127, 577, 867, 6815, 366, 322, 1951, 257, 1594, 13, 50864, 50940, 407, 867, 6815, 393, 312, 322, 1951, 257, 1594, 13, 407, 437, 775, 300, 360, 30, 467, 5874, 264, 1185, 1936, 51276, 51328, 281, 1594, 1951, 257, 7005, 4122, 300, 1261, 322, 16561, 13, 407, 498, 291, 362, 4122, 300, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06714945534865062, "compression_ratio": 1.8203883495145632, "no_speech_prob": 5.0146322791988496e-06}, {"id": 79, "seek": 51376, "start": 513.76, "end": 518.48, "text": " are very similar, feature extractors that are very similar, filters that are very similar in", "tokens": [50364, 366, 588, 2531, 11, 4111, 8947, 830, 300, 366, 588, 2531, 11, 15995, 300, 366, 588, 2531, 294, 50600, 50600, 257, 45216, 304, 2533, 11, 550, 729, 4122, 486, 3928, 281, 733, 295, 11, 562, 291, 360, 264, 3097, 11, 436, 603, 50900, 50900, 3928, 281, 1594, 2969, 1951, 257, 1594, 11, 570, 436, 486, 3928, 281, 312, 18157, 1214, 13, 400, 300, 311, 51164, 51164, 264, 1151, 636, 281, 17522, 264, 1230, 295, 3935, 300, 366, 18157, 412, 604, 472, 565, 13, 407, 281, 483, 729, 51588, 51640], "temperature": 0.0, "avg_logprob": -0.08812020899175288, "compression_ratio": 1.8564593301435406, "no_speech_prob": 4.93678635393735e-06}, {"id": 80, "seek": 51376, "start": 518.48, "end": 524.48, "text": " a convolutional net, then those features will tend to kind of, when you do the training, they'll", "tokens": [50364, 366, 588, 2531, 11, 4111, 8947, 830, 300, 366, 588, 2531, 11, 15995, 300, 366, 588, 2531, 294, 50600, 50600, 257, 45216, 304, 2533, 11, 550, 729, 4122, 486, 3928, 281, 733, 295, 11, 562, 291, 360, 264, 3097, 11, 436, 603, 50900, 50900, 3928, 281, 1594, 2969, 1951, 257, 1594, 11, 570, 436, 486, 3928, 281, 312, 18157, 1214, 13, 400, 300, 311, 51164, 51164, 264, 1151, 636, 281, 17522, 264, 1230, 295, 3935, 300, 366, 18157, 412, 604, 472, 565, 13, 407, 281, 483, 729, 51588, 51640], "temperature": 0.0, "avg_logprob": -0.08812020899175288, "compression_ratio": 1.8564593301435406, "no_speech_prob": 4.93678635393735e-06}, {"id": 81, "seek": 51376, "start": 524.48, "end": 529.76, "text": " tend to group themselves within a group, because they will tend to be activated together. And that's", "tokens": [50364, 366, 588, 2531, 11, 4111, 8947, 830, 300, 366, 588, 2531, 11, 15995, 300, 366, 588, 2531, 294, 50600, 50600, 257, 45216, 304, 2533, 11, 550, 729, 4122, 486, 3928, 281, 733, 295, 11, 562, 291, 360, 264, 3097, 11, 436, 603, 50900, 50900, 3928, 281, 1594, 2969, 1951, 257, 1594, 11, 570, 436, 486, 3928, 281, 312, 18157, 1214, 13, 400, 300, 311, 51164, 51164, 264, 1151, 636, 281, 17522, 264, 1230, 295, 3935, 300, 366, 18157, 412, 604, 472, 565, 13, 407, 281, 483, 729, 51588, 51640], "temperature": 0.0, "avg_logprob": -0.08812020899175288, "compression_ratio": 1.8564593301435406, "no_speech_prob": 4.93678635393735e-06}, {"id": 82, "seek": 51376, "start": 529.76, "end": 538.24, "text": " the best way to minimize the number of groups that are activated at any one time. So to get those", "tokens": [50364, 366, 588, 2531, 11, 4111, 8947, 830, 300, 366, 588, 2531, 11, 15995, 300, 366, 588, 2531, 294, 50600, 50600, 257, 45216, 304, 2533, 11, 550, 729, 4122, 486, 3928, 281, 733, 295, 11, 562, 291, 360, 264, 3097, 11, 436, 603, 50900, 50900, 3928, 281, 1594, 2969, 1951, 257, 1594, 11, 570, 436, 486, 3928, 281, 312, 18157, 1214, 13, 400, 300, 311, 51164, 51164, 264, 1151, 636, 281, 17522, 264, 1230, 295, 3935, 300, 366, 18157, 412, 604, 472, 565, 13, 407, 281, 483, 729, 51588, 51640], "temperature": 0.0, "avg_logprob": -0.08812020899175288, "compression_ratio": 1.8564593301435406, "no_speech_prob": 4.93678635393735e-06}, {"id": 83, "seek": 53824, "start": 538.24, "end": 544.48, "text": " interesting kind of pictures here, the way this was obtained is by", "tokens": [50364, 1880, 733, 295, 5242, 510, 11, 264, 636, 341, 390, 14879, 307, 538, 50676, 50788, 510, 264, 3935, 13, 407, 437, 291, 434, 1237, 412, 510, 366, 264, 11, 2139, 264, 11, 286, 519, 309, 311, 257, 979, 8616, 51112, 51112, 8141, 13, 407, 613, 366, 264, 13766, 295, 264, 343, 35, 8141, 300, 321, 393, 31499, 364, 3256, 9972, 490, 51548, 51596], "temperature": 0.0, "avg_logprob": -0.18197533534123347, "compression_ratio": 1.5149700598802396, "no_speech_prob": 1.221852744492935e-05}, {"id": 84, "seek": 53824, "start": 546.72, "end": 553.2, "text": " here the groups. So what you're looking at here are the, either the, I think it's a decoding", "tokens": [50364, 1880, 733, 295, 5242, 510, 11, 264, 636, 341, 390, 14879, 307, 538, 50676, 50788, 510, 264, 3935, 13, 407, 437, 291, 434, 1237, 412, 510, 366, 264, 11, 2139, 264, 11, 286, 519, 309, 311, 257, 979, 8616, 51112, 51112, 8141, 13, 407, 613, 366, 264, 13766, 295, 264, 343, 35, 8141, 300, 321, 393, 31499, 364, 3256, 9972, 490, 51548, 51596], "temperature": 0.0, "avg_logprob": -0.18197533534123347, "compression_ratio": 1.5149700598802396, "no_speech_prob": 1.221852744492935e-05}, {"id": 85, "seek": 53824, "start": 553.2, "end": 561.92, "text": " matrix. So these are the columns of the WD matrix that we can reconstruct an image patch from", "tokens": [50364, 1880, 733, 295, 5242, 510, 11, 264, 636, 341, 390, 14879, 307, 538, 50676, 50788, 510, 264, 3935, 13, 407, 437, 291, 434, 1237, 412, 510, 366, 264, 11, 2139, 264, 11, 286, 519, 309, 311, 257, 979, 8616, 51112, 51112, 8141, 13, 407, 613, 366, 264, 13766, 295, 264, 343, 35, 8141, 300, 321, 393, 31499, 364, 3256, 9972, 490, 51548, 51596], "temperature": 0.0, "avg_logprob": -0.18197533534123347, "compression_ratio": 1.5149700598802396, "no_speech_prob": 1.221852744492935e-05}, {"id": 86, "seek": 56192, "start": 561.92, "end": 572.88, "text": " the sparse code by multiplying by that matrix. But what we do here is that we group those features", "tokens": [50364, 264, 637, 11668, 3089, 538, 30955, 538, 300, 8141, 13, 583, 437, 321, 360, 510, 307, 300, 321, 1594, 729, 4122, 50912, 50912, 666, 8474, 295, 8652, 13, 407, 321, 9424, 439, 264, 4122, 294, 257, 568, 35, 4471, 300, 575, 1825, 281, 360, 365, 264, 51156, 51156, 1192, 1793, 295, 264, 3256, 13, 492, 393, 2826, 604, 1192, 1793, 321, 528, 13, 682, 1186, 11, 341, 307, 406, 767, 257, 568, 35, 51388, 51388, 1192, 1793, 13, 467, 311, 257, 7160, 8558, 338, 1192, 1793, 13, 407, 264, 1411, 1252, 17431, 264, 558, 1252, 293, 264, 1192, 17431, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1650502473405264, "compression_ratio": 1.6828193832599119, "no_speech_prob": 9.364621291751973e-06}, {"id": 87, "seek": 56192, "start": 572.88, "end": 577.76, "text": " into blocks of 36. So we arrange all the features in a 2D map that has nothing to do with the", "tokens": [50364, 264, 637, 11668, 3089, 538, 30955, 538, 300, 8141, 13, 583, 437, 321, 360, 510, 307, 300, 321, 1594, 729, 4122, 50912, 50912, 666, 8474, 295, 8652, 13, 407, 321, 9424, 439, 264, 4122, 294, 257, 568, 35, 4471, 300, 575, 1825, 281, 360, 365, 264, 51156, 51156, 1192, 1793, 295, 264, 3256, 13, 492, 393, 2826, 604, 1192, 1793, 321, 528, 13, 682, 1186, 11, 341, 307, 406, 767, 257, 568, 35, 51388, 51388, 1192, 1793, 13, 467, 311, 257, 7160, 8558, 338, 1192, 1793, 13, 407, 264, 1411, 1252, 17431, 264, 558, 1252, 293, 264, 1192, 17431, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1650502473405264, "compression_ratio": 1.6828193832599119, "no_speech_prob": 9.364621291751973e-06}, {"id": 88, "seek": 56192, "start": 577.76, "end": 582.4, "text": " topology of the image. We can choose any topology we want. In fact, this is not actually a 2D", "tokens": [50364, 264, 637, 11668, 3089, 538, 30955, 538, 300, 8141, 13, 583, 437, 321, 360, 510, 307, 300, 321, 1594, 729, 4122, 50912, 50912, 666, 8474, 295, 8652, 13, 407, 321, 9424, 439, 264, 4122, 294, 257, 568, 35, 4471, 300, 575, 1825, 281, 360, 365, 264, 51156, 51156, 1192, 1793, 295, 264, 3256, 13, 492, 393, 2826, 604, 1192, 1793, 321, 528, 13, 682, 1186, 11, 341, 307, 406, 767, 257, 568, 35, 51388, 51388, 1192, 1793, 13, 467, 311, 257, 7160, 8558, 338, 1192, 1793, 13, 407, 264, 1411, 1252, 17431, 264, 558, 1252, 293, 264, 1192, 17431, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1650502473405264, "compression_ratio": 1.6828193832599119, "no_speech_prob": 9.364621291751973e-06}, {"id": 89, "seek": 56192, "start": 582.4, "end": 587.92, "text": " topology. It's a Torridel topology. So the left side touches the right side and the top touches", "tokens": [50364, 264, 637, 11668, 3089, 538, 30955, 538, 300, 8141, 13, 583, 437, 321, 360, 510, 307, 300, 321, 1594, 729, 4122, 50912, 50912, 666, 8474, 295, 8652, 13, 407, 321, 9424, 439, 264, 4122, 294, 257, 568, 35, 4471, 300, 575, 1825, 281, 360, 365, 264, 51156, 51156, 1192, 1793, 295, 264, 3256, 13, 492, 393, 2826, 604, 1192, 1793, 321, 528, 13, 682, 1186, 11, 341, 307, 406, 767, 257, 568, 35, 51388, 51388, 1192, 1793, 13, 467, 311, 257, 7160, 8558, 338, 1192, 1793, 13, 407, 264, 1411, 1252, 17431, 264, 558, 1252, 293, 264, 1192, 17431, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1650502473405264, "compression_ratio": 1.6828193832599119, "no_speech_prob": 9.364621291751973e-06}, {"id": 90, "seek": 58792, "start": 587.92, "end": 601.92, "text": " the bottom. So it's topologically identical to a torus. And what we do is we group sets of 36", "tokens": [50364, 264, 2767, 13, 407, 309, 311, 1192, 17157, 14800, 281, 257, 3930, 301, 13, 400, 437, 321, 360, 307, 321, 1594, 6352, 295, 8652, 51064, 51064, 4122, 1951, 257, 1594, 13, 400, 729, 3935, 295, 8652, 4122, 19959, 538, 1045, 13766, 293, 1045, 13241, 13, 51512, 51512], "temperature": 0.0, "avg_logprob": -0.10495542993350905, "compression_ratio": 1.446969696969697, "no_speech_prob": 1.1297976016066968e-05}, {"id": 91, "seek": 58792, "start": 601.92, "end": 610.88, "text": " features within a group. And those groups of 36 features overlap by three columns and three rows.", "tokens": [50364, 264, 2767, 13, 407, 309, 311, 1192, 17157, 14800, 281, 257, 3930, 301, 13, 400, 437, 321, 360, 307, 321, 1594, 6352, 295, 8652, 51064, 51064, 4122, 1951, 257, 1594, 13, 400, 729, 3935, 295, 8652, 4122, 19959, 538, 1045, 13766, 293, 1045, 13241, 13, 51512, 51512], "temperature": 0.0, "avg_logprob": -0.10495542993350905, "compression_ratio": 1.446969696969697, "no_speech_prob": 1.1297976016066968e-05}, {"id": 92, "seek": 61088, "start": 610.88, "end": 617.92, "text": " Okay, so we have multiple groups of 36 features, six by six, shifted by three. You could think of", "tokens": [50364, 1033, 11, 370, 321, 362, 3866, 3935, 295, 8652, 4122, 11, 2309, 538, 2309, 11, 18892, 538, 1045, 13, 509, 727, 519, 295, 50716, 50716, 341, 382, 733, 295, 7005, 278, 670, 4111, 11, 457, 406, 7005, 278, 670, 1901, 11, 570, 456, 311, 572, 1901, 510, 13, 50956, 50956, 467, 311, 257, 4498, 4582, 3209, 13, 583, 309, 575, 257, 857, 295, 264, 6813, 11, 264, 912, 6813, 382, 7005, 278, 11, 51252, 51252, 3993, 510, 291, 7005, 670, 8652, 4122, 13, 509, 500, 380, 7005, 670, 1901, 13, 407, 51532, 51720], "temperature": 0.0, "avg_logprob": -0.10263996124267578, "compression_ratio": 1.6435185185185186, "no_speech_prob": 2.5069182811421342e-05}, {"id": 93, "seek": 61088, "start": 617.92, "end": 622.72, "text": " this as kind of pooling over feature, but not pooling over space, because there's no space here.", "tokens": [50364, 1033, 11, 370, 321, 362, 3866, 3935, 295, 8652, 4122, 11, 2309, 538, 2309, 11, 18892, 538, 1045, 13, 509, 727, 519, 295, 50716, 50716, 341, 382, 733, 295, 7005, 278, 670, 4111, 11, 457, 406, 7005, 278, 670, 1901, 11, 570, 456, 311, 572, 1901, 510, 13, 50956, 50956, 467, 311, 257, 4498, 4582, 3209, 13, 583, 309, 575, 257, 857, 295, 264, 6813, 11, 264, 912, 6813, 382, 7005, 278, 11, 51252, 51252, 3993, 510, 291, 7005, 670, 8652, 4122, 13, 509, 500, 380, 7005, 670, 1901, 13, 407, 51532, 51720], "temperature": 0.0, "avg_logprob": -0.10263996124267578, "compression_ratio": 1.6435185185185186, "no_speech_prob": 2.5069182811421342e-05}, {"id": 94, "seek": 61088, "start": 622.72, "end": 628.64, "text": " It's a fully connected network. But it has a bit of the flavor, the same flavor as pooling,", "tokens": [50364, 1033, 11, 370, 321, 362, 3866, 3935, 295, 8652, 4122, 11, 2309, 538, 2309, 11, 18892, 538, 1045, 13, 509, 727, 519, 295, 50716, 50716, 341, 382, 733, 295, 7005, 278, 670, 4111, 11, 457, 406, 7005, 278, 670, 1901, 11, 570, 456, 311, 572, 1901, 510, 13, 50956, 50956, 467, 311, 257, 4498, 4582, 3209, 13, 583, 309, 575, 257, 857, 295, 264, 6813, 11, 264, 912, 6813, 382, 7005, 278, 11, 51252, 51252, 3993, 510, 291, 7005, 670, 8652, 4122, 13, 509, 500, 380, 7005, 670, 1901, 13, 407, 51532, 51720], "temperature": 0.0, "avg_logprob": -0.10263996124267578, "compression_ratio": 1.6435185185185186, "no_speech_prob": 2.5069182811421342e-05}, {"id": 95, "seek": 61088, "start": 628.64, "end": 634.24, "text": " except here you pool over 36 features. You don't pool over space. So", "tokens": [50364, 1033, 11, 370, 321, 362, 3866, 3935, 295, 8652, 4122, 11, 2309, 538, 2309, 11, 18892, 538, 1045, 13, 509, 727, 519, 295, 50716, 50716, 341, 382, 733, 295, 7005, 278, 670, 4111, 11, 457, 406, 7005, 278, 670, 1901, 11, 570, 456, 311, 572, 1901, 510, 13, 50956, 50956, 467, 311, 257, 4498, 4582, 3209, 13, 583, 309, 575, 257, 857, 295, 264, 6813, 11, 264, 912, 6813, 382, 7005, 278, 11, 51252, 51252, 3993, 510, 291, 7005, 670, 8652, 4122, 13, 509, 500, 380, 7005, 670, 1901, 13, 407, 51532, 51720], "temperature": 0.0, "avg_logprob": -0.10263996124267578, "compression_ratio": 1.6435185185185186, "no_speech_prob": 2.5069182811421342e-05}, {"id": 96, "seek": 63424, "start": 634.24, "end": 641.6, "text": " then you compute the sum of the L2 norm of the features that are within each group. And that's", "tokens": [50364, 550, 291, 14722, 264, 2408, 295, 264, 441, 17, 2026, 295, 264, 4122, 300, 366, 1951, 1184, 1594, 13, 400, 300, 311, 50732, 50732, 264, 3890, 6545, 291, 764, 562, 291, 3847, 428, 1738, 685, 507, 2058, 19866, 13, 407, 437, 264, 1185, 2738, 281, 360, 307, 51044, 51044, 17522, 264, 1230, 295, 3935, 300, 366, 322, 412, 604, 472, 565, 13, 400, 370, 382, 286, 848, 949, 11, 309, 1936, 51320, 51364, 319, 17377, 82, 439, 264, 4122, 300, 366, 2531, 293, 3700, 281, 2610, 16561, 666, 3935, 13, 51644, 51684], "temperature": 0.0, "avg_logprob": -0.26106826295243934, "compression_ratio": 1.6981981981981982, "no_speech_prob": 3.2376369745179545e-06}, {"id": 97, "seek": 63424, "start": 641.6, "end": 647.84, "text": " the regularizer you use when you train your Sparsity encoder. So what the system wants to do is", "tokens": [50364, 550, 291, 14722, 264, 2408, 295, 264, 441, 17, 2026, 295, 264, 4122, 300, 366, 1951, 1184, 1594, 13, 400, 300, 311, 50732, 50732, 264, 3890, 6545, 291, 764, 562, 291, 3847, 428, 1738, 685, 507, 2058, 19866, 13, 407, 437, 264, 1185, 2738, 281, 360, 307, 51044, 51044, 17522, 264, 1230, 295, 3935, 300, 366, 322, 412, 604, 472, 565, 13, 400, 370, 382, 286, 848, 949, 11, 309, 1936, 51320, 51364, 319, 17377, 82, 439, 264, 4122, 300, 366, 2531, 293, 3700, 281, 2610, 16561, 666, 3935, 13, 51644, 51684], "temperature": 0.0, "avg_logprob": -0.26106826295243934, "compression_ratio": 1.6981981981981982, "no_speech_prob": 3.2376369745179545e-06}, {"id": 98, "seek": 63424, "start": 647.84, "end": 653.36, "text": " minimize the number of groups that are on at any one time. And so as I said before, it basically", "tokens": [50364, 550, 291, 14722, 264, 2408, 295, 264, 441, 17, 2026, 295, 264, 4122, 300, 366, 1951, 1184, 1594, 13, 400, 300, 311, 50732, 50732, 264, 3890, 6545, 291, 764, 562, 291, 3847, 428, 1738, 685, 507, 2058, 19866, 13, 407, 437, 264, 1185, 2738, 281, 360, 307, 51044, 51044, 17522, 264, 1230, 295, 3935, 300, 366, 322, 412, 604, 472, 565, 13, 400, 370, 382, 286, 848, 949, 11, 309, 1936, 51320, 51364, 319, 17377, 82, 439, 264, 4122, 300, 366, 2531, 293, 3700, 281, 2610, 16561, 666, 3935, 13, 51644, 51684], "temperature": 0.0, "avg_logprob": -0.26106826295243934, "compression_ratio": 1.6981981981981982, "no_speech_prob": 3.2376369745179545e-06}, {"id": 99, "seek": 63424, "start": 654.24, "end": 659.84, "text": " regroups all the features that are similar and likely to fire simultaneously into groups.", "tokens": [50364, 550, 291, 14722, 264, 2408, 295, 264, 441, 17, 2026, 295, 264, 4122, 300, 366, 1951, 1184, 1594, 13, 400, 300, 311, 50732, 50732, 264, 3890, 6545, 291, 764, 562, 291, 3847, 428, 1738, 685, 507, 2058, 19866, 13, 407, 437, 264, 1185, 2738, 281, 360, 307, 51044, 51044, 17522, 264, 1230, 295, 3935, 300, 366, 322, 412, 604, 472, 565, 13, 400, 370, 382, 286, 848, 949, 11, 309, 1936, 51320, 51364, 319, 17377, 82, 439, 264, 4122, 300, 366, 2531, 293, 3700, 281, 2610, 16561, 666, 3935, 13, 51644, 51684], "temperature": 0.0, "avg_logprob": -0.26106826295243934, "compression_ratio": 1.6981981981981982, "no_speech_prob": 3.2376369745179545e-06}, {"id": 100, "seek": 65984, "start": 659.84, "end": 665.44, "text": " And because the groups overlap, then it creates those kind of slowly evolving sets of features", "tokens": [50364, 400, 570, 264, 3935, 19959, 11, 550, 309, 7829, 729, 733, 295, 5692, 21085, 6352, 295, 4122, 50644, 50644, 300, 1333, 295, 1643, 281, 733, 295, 30310, 926, 257, 935, 13, 407, 264, 4122, 291, 483, 382, 257, 1874, 295, 341, 362, 51112, 51112, 512, 1333, 295, 33270, 719, 11, 293, 436, 362, 512, 33270, 719, 406, 281, 5513, 11, 457, 281, 721, 411, 51360, 51360, 12447, 293, 4373, 293, 721, 411, 300, 11, 2035, 264, 1185, 14898, 13, 407, 510, 264, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.13638625034066135, "compression_ratio": 1.7416267942583732, "no_speech_prob": 1.1658137736958452e-05}, {"id": 101, "seek": 65984, "start": 665.44, "end": 674.8000000000001, "text": " that sort of seem to kind of swirl around a point. So the features you get as a result of this have", "tokens": [50364, 400, 570, 264, 3935, 19959, 11, 550, 309, 7829, 729, 733, 295, 5692, 21085, 6352, 295, 4122, 50644, 50644, 300, 1333, 295, 1643, 281, 733, 295, 30310, 926, 257, 935, 13, 407, 264, 4122, 291, 483, 382, 257, 1874, 295, 341, 362, 51112, 51112, 512, 1333, 295, 33270, 719, 11, 293, 436, 362, 512, 33270, 719, 406, 281, 5513, 11, 457, 281, 721, 411, 51360, 51360, 12447, 293, 4373, 293, 721, 411, 300, 11, 2035, 264, 1185, 14898, 13, 407, 510, 264, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.13638625034066135, "compression_ratio": 1.7416267942583732, "no_speech_prob": 1.1658137736958452e-05}, {"id": 102, "seek": 65984, "start": 674.8000000000001, "end": 679.76, "text": " some sort of invariance, and they have some invariance not to shift, but to things like", "tokens": [50364, 400, 570, 264, 3935, 19959, 11, 550, 309, 7829, 729, 733, 295, 5692, 21085, 6352, 295, 4122, 50644, 50644, 300, 1333, 295, 1643, 281, 733, 295, 30310, 926, 257, 935, 13, 407, 264, 4122, 291, 483, 382, 257, 1874, 295, 341, 362, 51112, 51112, 512, 1333, 295, 33270, 719, 11, 293, 436, 362, 512, 33270, 719, 406, 281, 5513, 11, 457, 281, 721, 411, 51360, 51360, 12447, 293, 4373, 293, 721, 411, 300, 11, 2035, 264, 1185, 14898, 13, 407, 510, 264, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.13638625034066135, "compression_ratio": 1.7416267942583732, "no_speech_prob": 1.1658137736958452e-05}, {"id": 103, "seek": 65984, "start": 679.76, "end": 684.88, "text": " rotation and scale and things like that, whatever the system decides. So here the", "tokens": [50364, 400, 570, 264, 3935, 19959, 11, 550, 309, 7829, 729, 733, 295, 5692, 21085, 6352, 295, 4122, 50644, 50644, 300, 1333, 295, 1643, 281, 733, 295, 30310, 926, 257, 935, 13, 407, 264, 4122, 291, 483, 382, 257, 1874, 295, 341, 362, 51112, 51112, 512, 1333, 295, 33270, 719, 11, 293, 436, 362, 512, 33270, 719, 406, 281, 5513, 11, 457, 281, 721, 411, 51360, 51360, 12447, 293, 4373, 293, 721, 411, 300, 11, 2035, 264, 1185, 14898, 13, 407, 510, 264, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.13638625034066135, "compression_ratio": 1.7416267942583732, "no_speech_prob": 1.1658137736958452e-05}, {"id": 104, "seek": 68488, "start": 684.88, "end": 692.48, "text": " reason for choosing a 2D topology is basically just to make it look beautiful. But you could", "tokens": [50364, 1778, 337, 10875, 257, 568, 35, 1192, 1793, 307, 1936, 445, 281, 652, 309, 574, 2238, 13, 583, 291, 727, 50744, 50744, 2826, 604, 733, 295, 1192, 1793, 291, 528, 13, 708, 307, 322, 264, 2031, 12, 24633, 293, 264, 288, 12, 24633, 510, 294, 341, 10686, 30, 51028, 51028, 3950, 366, 23211, 35387, 13, 286, 362, 11, 286, 500, 380, 754, 1604, 577, 867, 4122, 456, 366, 510, 13, 639, 51384, 51384, 1062, 312, 38882, 4122, 11, 286, 519, 13, 467, 311, 3165, 538, 3165, 13, 407, 456, 311, 38882, 4122, 13, 407, 291, 393, 536, 300, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.28197692422305837, "compression_ratio": 1.5537190082644627, "no_speech_prob": 3.321120675536804e-05}, {"id": 105, "seek": 68488, "start": 692.48, "end": 698.16, "text": " choose any kind of topology you want. What is on the x-axis and the y-axis here in this diagram?", "tokens": [50364, 1778, 337, 10875, 257, 568, 35, 1192, 1793, 307, 1936, 445, 281, 652, 309, 574, 2238, 13, 583, 291, 727, 50744, 50744, 2826, 604, 733, 295, 1192, 1793, 291, 528, 13, 708, 307, 322, 264, 2031, 12, 24633, 293, 264, 288, 12, 24633, 510, 294, 341, 10686, 30, 51028, 51028, 3950, 366, 23211, 35387, 13, 286, 362, 11, 286, 500, 380, 754, 1604, 577, 867, 4122, 456, 366, 510, 13, 639, 51384, 51384, 1062, 312, 38882, 4122, 11, 286, 519, 13, 467, 311, 3165, 538, 3165, 13, 407, 456, 311, 38882, 4122, 13, 407, 291, 393, 536, 300, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.28197692422305837, "compression_ratio": 1.5537190082644627, "no_speech_prob": 3.321120675536804e-05}, {"id": 106, "seek": 68488, "start": 698.16, "end": 705.28, "text": " Those are arbitrary axes. I have, I don't even remember how many features there are here. This", "tokens": [50364, 1778, 337, 10875, 257, 568, 35, 1192, 1793, 307, 1936, 445, 281, 652, 309, 574, 2238, 13, 583, 291, 727, 50744, 50744, 2826, 604, 733, 295, 1192, 1793, 291, 528, 13, 708, 307, 322, 264, 2031, 12, 24633, 293, 264, 288, 12, 24633, 510, 294, 341, 10686, 30, 51028, 51028, 3950, 366, 23211, 35387, 13, 286, 362, 11, 286, 500, 380, 754, 1604, 577, 867, 4122, 456, 366, 510, 13, 639, 51384, 51384, 1062, 312, 38882, 4122, 11, 286, 519, 13, 467, 311, 3165, 538, 3165, 13, 407, 456, 311, 38882, 4122, 13, 407, 291, 393, 536, 300, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.28197692422305837, "compression_ratio": 1.5537190082644627, "no_speech_prob": 3.321120675536804e-05}, {"id": 107, "seek": 68488, "start": 705.28, "end": 712.32, "text": " might be 256 features, I think. It's 16 by 16. So there's 256 features. So you can see that", "tokens": [50364, 1778, 337, 10875, 257, 568, 35, 1192, 1793, 307, 1936, 445, 281, 652, 309, 574, 2238, 13, 583, 291, 727, 50744, 50744, 2826, 604, 733, 295, 1192, 1793, 291, 528, 13, 708, 307, 322, 264, 2031, 12, 24633, 293, 264, 288, 12, 24633, 510, 294, 341, 10686, 30, 51028, 51028, 3950, 366, 23211, 35387, 13, 286, 362, 11, 286, 500, 380, 754, 1604, 577, 867, 4122, 456, 366, 510, 13, 639, 51384, 51384, 1062, 312, 38882, 4122, 11, 286, 519, 13, 467, 311, 3165, 538, 3165, 13, 407, 456, 311, 38882, 4122, 13, 407, 291, 393, 536, 300, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.28197692422305837, "compression_ratio": 1.5537190082644627, "no_speech_prob": 3.321120675536804e-05}, {"id": 108, "seek": 71232, "start": 712.32, "end": 719.9200000000001, "text": " the x-axis is 16 by 16. So there's 256 hidden units. So imagine a network that has a 12 by 12", "tokens": [50364, 264, 2031, 12, 24633, 307, 3165, 538, 3165, 13, 407, 456, 311, 38882, 7633, 6815, 13, 407, 3811, 257, 3209, 300, 575, 257, 2272, 538, 2272, 50744, 50744, 4846, 9972, 11, 364, 4846, 3256, 13, 467, 311, 257, 9972, 490, 364, 3256, 13, 400, 38882, 7633, 6815, 365, 1577, 4984, 11, 51280, 51320, 2107, 12, 1889, 17409, 11, 293, 456, 307, 1071, 4583, 322, 1192, 13, 1396, 300, 311, 264, 2058, 19866, 13, 400, 550, 291, 362, 51680, 51716], "temperature": 0.0, "avg_logprob": -0.1616270159497673, "compression_ratio": 1.5132275132275133, "no_speech_prob": 1.669258563197218e-05}, {"id": 109, "seek": 71232, "start": 719.9200000000001, "end": 730.6400000000001, "text": " input patch, an input image. It's a patch from an image. And 256 hidden units with full connection,", "tokens": [50364, 264, 2031, 12, 24633, 307, 3165, 538, 3165, 13, 407, 456, 311, 38882, 7633, 6815, 13, 407, 3811, 257, 3209, 300, 575, 257, 2272, 538, 2272, 50744, 50744, 4846, 9972, 11, 364, 4846, 3256, 13, 467, 311, 257, 9972, 490, 364, 3256, 13, 400, 38882, 7633, 6815, 365, 1577, 4984, 11, 51280, 51320, 2107, 12, 1889, 17409, 11, 293, 456, 307, 1071, 4583, 322, 1192, 13, 1396, 300, 311, 264, 2058, 19866, 13, 400, 550, 291, 362, 51680, 51716], "temperature": 0.0, "avg_logprob": -0.1616270159497673, "compression_ratio": 1.5132275132275133, "no_speech_prob": 1.669258563197218e-05}, {"id": 110, "seek": 71232, "start": 731.44, "end": 738.6400000000001, "text": " non-linearity, and there is another layer on top. Then that's the encoder. And then you have", "tokens": [50364, 264, 2031, 12, 24633, 307, 3165, 538, 3165, 13, 407, 456, 311, 38882, 7633, 6815, 13, 407, 3811, 257, 3209, 300, 575, 257, 2272, 538, 2272, 50744, 50744, 4846, 9972, 11, 364, 4846, 3256, 13, 467, 311, 257, 9972, 490, 364, 3256, 13, 400, 38882, 7633, 6815, 365, 1577, 4984, 11, 51280, 51320, 2107, 12, 1889, 17409, 11, 293, 456, 307, 1071, 4583, 322, 1192, 13, 1396, 300, 311, 264, 2058, 19866, 13, 400, 550, 291, 362, 51680, 51716], "temperature": 0.0, "avg_logprob": -0.1616270159497673, "compression_ratio": 1.5132275132275133, "no_speech_prob": 1.669258563197218e-05}, {"id": 111, "seek": 73864, "start": 738.64, "end": 745.6, "text": " this group sparsity, and then the decoder is linear. And what you're seeing here are the", "tokens": [50364, 341, 1594, 637, 685, 507, 11, 293, 550, 264, 979, 19866, 307, 8213, 13, 400, 437, 291, 434, 2577, 510, 366, 264, 50712, 50712, 13766, 295, 264, 979, 19866, 13, 400, 436, 366, 9983, 294, 257, 568, 35, 1192, 1793, 11, 457, 309, 311, 23211, 13, 51024, 51024, 6947, 295, 613, 19368, 307, 257, 7738, 295, 264, 979, 19866, 13, 51164, 51164, 6947, 295, 613, 19368, 307, 257, 7738, 295, 264, 979, 19866, 11, 457, 611, 23249, 281, 257, 6542, 295, 710, 11, 257, 51388, 51388, 6542, 295, 264, 4111, 8062, 13, 400, 370, 436, 366, 9983, 294, 257, 3165, 538, 3165, 8141, 11, 457, 309, 311, 733, 295, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.17862605204624413, "compression_ratio": 1.966824644549763, "no_speech_prob": 3.535232463036664e-05}, {"id": 112, "seek": 73864, "start": 745.6, "end": 751.84, "text": " columns of the decoder. And they are organized in a 2D topology, but it's arbitrary.", "tokens": [50364, 341, 1594, 637, 685, 507, 11, 293, 550, 264, 979, 19866, 307, 8213, 13, 400, 437, 291, 434, 2577, 510, 366, 264, 50712, 50712, 13766, 295, 264, 979, 19866, 13, 400, 436, 366, 9983, 294, 257, 568, 35, 1192, 1793, 11, 457, 309, 311, 23211, 13, 51024, 51024, 6947, 295, 613, 19368, 307, 257, 7738, 295, 264, 979, 19866, 13, 51164, 51164, 6947, 295, 613, 19368, 307, 257, 7738, 295, 264, 979, 19866, 11, 457, 611, 23249, 281, 257, 6542, 295, 710, 11, 257, 51388, 51388, 6542, 295, 264, 4111, 8062, 13, 400, 370, 436, 366, 9983, 294, 257, 3165, 538, 3165, 8141, 11, 457, 309, 311, 733, 295, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.17862605204624413, "compression_ratio": 1.966824644549763, "no_speech_prob": 3.535232463036664e-05}, {"id": 113, "seek": 73864, "start": 751.84, "end": 754.64, "text": " Each of these squares is a column of the decoder.", "tokens": [50364, 341, 1594, 637, 685, 507, 11, 293, 550, 264, 979, 19866, 307, 8213, 13, 400, 437, 291, 434, 2577, 510, 366, 264, 50712, 50712, 13766, 295, 264, 979, 19866, 13, 400, 436, 366, 9983, 294, 257, 568, 35, 1192, 1793, 11, 457, 309, 311, 23211, 13, 51024, 51024, 6947, 295, 613, 19368, 307, 257, 7738, 295, 264, 979, 19866, 13, 51164, 51164, 6947, 295, 613, 19368, 307, 257, 7738, 295, 264, 979, 19866, 11, 457, 611, 23249, 281, 257, 6542, 295, 710, 11, 257, 51388, 51388, 6542, 295, 264, 4111, 8062, 13, 400, 370, 436, 366, 9983, 294, 257, 3165, 538, 3165, 8141, 11, 457, 309, 311, 733, 295, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.17862605204624413, "compression_ratio": 1.966824644549763, "no_speech_prob": 3.535232463036664e-05}, {"id": 114, "seek": 73864, "start": 754.64, "end": 759.12, "text": " Each of these squares is a column of the decoder, but also corresponds to a component of z, a", "tokens": [50364, 341, 1594, 637, 685, 507, 11, 293, 550, 264, 979, 19866, 307, 8213, 13, 400, 437, 291, 434, 2577, 510, 366, 264, 50712, 50712, 13766, 295, 264, 979, 19866, 13, 400, 436, 366, 9983, 294, 257, 568, 35, 1192, 1793, 11, 457, 309, 311, 23211, 13, 51024, 51024, 6947, 295, 613, 19368, 307, 257, 7738, 295, 264, 979, 19866, 13, 51164, 51164, 6947, 295, 613, 19368, 307, 257, 7738, 295, 264, 979, 19866, 11, 457, 611, 23249, 281, 257, 6542, 295, 710, 11, 257, 51388, 51388, 6542, 295, 264, 4111, 8062, 13, 400, 370, 436, 366, 9983, 294, 257, 3165, 538, 3165, 8141, 11, 457, 309, 311, 733, 295, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.17862605204624413, "compression_ratio": 1.966824644549763, "no_speech_prob": 3.535232463036664e-05}, {"id": 115, "seek": 73864, "start": 759.12, "end": 767.4399999999999, "text": " component of the feature vector. And so they are organized in a 16 by 16 matrix, but it's kind of", "tokens": [50364, 341, 1594, 637, 685, 507, 11, 293, 550, 264, 979, 19866, 307, 8213, 13, 400, 437, 291, 434, 2577, 510, 366, 264, 50712, 50712, 13766, 295, 264, 979, 19866, 13, 400, 436, 366, 9983, 294, 257, 568, 35, 1192, 1793, 11, 457, 309, 311, 23211, 13, 51024, 51024, 6947, 295, 613, 19368, 307, 257, 7738, 295, 264, 979, 19866, 13, 51164, 51164, 6947, 295, 613, 19368, 307, 257, 7738, 295, 264, 979, 19866, 11, 457, 611, 23249, 281, 257, 6542, 295, 710, 11, 257, 51388, 51388, 6542, 295, 264, 4111, 8062, 13, 400, 370, 436, 366, 9983, 294, 257, 3165, 538, 3165, 8141, 11, 457, 309, 311, 733, 295, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.17862605204624413, "compression_ratio": 1.966824644549763, "no_speech_prob": 3.535232463036664e-05}, {"id": 116, "seek": 76744, "start": 767.44, "end": 776.08, "text": " arbitrary. We just put them in a matrix. And then we train. And because the groups take kind of", "tokens": [50364, 23211, 13, 492, 445, 829, 552, 294, 257, 8141, 13, 400, 550, 321, 3847, 13, 400, 570, 264, 3935, 747, 733, 295, 50796, 50796, 1386, 538, 1386, 20052, 294, 341, 1192, 1793, 11, 264, 1185, 8195, 27152, 4122, 300, 366, 2531, 562, 51156, 51156, 436, 366, 11184, 1951, 341, 1192, 1793, 13, 583, 797, 11, 286, 727, 362, 8614, 604, 733, 295, 1192, 1793, 11, 502, 35, 11, 568, 35, 11, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.1336890157063802, "compression_ratio": 1.5129533678756477, "no_speech_prob": 5.507430159923388e-06}, {"id": 117, "seek": 76744, "start": 776.08, "end": 783.2800000000001, "text": " 6 by 6 neighborhoods in this topology, the system naturally learns features that are similar when", "tokens": [50364, 23211, 13, 492, 445, 829, 552, 294, 257, 8141, 13, 400, 550, 321, 3847, 13, 400, 570, 264, 3935, 747, 733, 295, 50796, 50796, 1386, 538, 1386, 20052, 294, 341, 1192, 1793, 11, 264, 1185, 8195, 27152, 4122, 300, 366, 2531, 562, 51156, 51156, 436, 366, 11184, 1951, 341, 1192, 1793, 13, 583, 797, 11, 286, 727, 362, 8614, 604, 733, 295, 1192, 1793, 11, 502, 35, 11, 568, 35, 11, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.1336890157063802, "compression_ratio": 1.5129533678756477, "no_speech_prob": 5.507430159923388e-06}, {"id": 118, "seek": 76744, "start": 783.2800000000001, "end": 791.2, "text": " they are nearby within this topology. But again, I could have chosen any kind of topology, 1D, 2D,", "tokens": [50364, 23211, 13, 492, 445, 829, 552, 294, 257, 8141, 13, 400, 550, 321, 3847, 13, 400, 570, 264, 3935, 747, 733, 295, 50796, 50796, 1386, 538, 1386, 20052, 294, 341, 1192, 1793, 11, 264, 1185, 8195, 27152, 4122, 300, 366, 2531, 562, 51156, 51156, 436, 366, 11184, 1951, 341, 1192, 1793, 13, 583, 797, 11, 286, 727, 362, 8614, 604, 733, 295, 1192, 1793, 11, 502, 35, 11, 568, 35, 11, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.1336890157063802, "compression_ratio": 1.5129533678756477, "no_speech_prob": 5.507430159923388e-06}, {"id": 119, "seek": 79120, "start": 791.2, "end": 800.5600000000001, "text": " 3D, or even some graph neighborhood of some kind, as long as the pooling is between neighbors", "tokens": [50364, 805, 35, 11, 420, 754, 512, 4295, 7630, 295, 512, 733, 11, 382, 938, 382, 264, 7005, 278, 307, 1296, 12512, 50832, 50832, 293, 264, 4295, 13, 663, 486, 589, 13, 407, 437, 286, 600, 1096, 510, 307, 733, 295, 7149, 341, 707, 5102, 51444, 51512], "temperature": 0.0, "avg_logprob": -0.11899847785631816, "compression_ratio": 1.3602941176470589, "no_speech_prob": 1.568807419971563e-05}, {"id": 120, "seek": 79120, "start": 800.5600000000001, "end": 812.8000000000001, "text": " and the graph. That will work. So what I've done here is kind of repeat this little pattern", "tokens": [50364, 805, 35, 11, 420, 754, 512, 4295, 7630, 295, 512, 733, 11, 382, 938, 382, 264, 7005, 278, 307, 1296, 12512, 50832, 50832, 293, 264, 4295, 13, 663, 486, 589, 13, 407, 437, 286, 600, 1096, 510, 307, 733, 295, 7149, 341, 707, 5102, 51444, 51512], "temperature": 0.0, "avg_logprob": -0.11899847785631816, "compression_ratio": 1.3602941176470589, "no_speech_prob": 1.568807419971563e-05}, {"id": 121, "seek": 81280, "start": 812.8, "end": 821.92, "text": " to kind of show, because it's, you know, it's toroidal, to show how those patterns kind of", "tokens": [50364, 281, 733, 295, 855, 11, 570, 309, 311, 11, 291, 458, 11, 309, 311, 281, 6490, 304, 11, 281, 855, 577, 729, 8294, 733, 295, 50820, 50820, 7149, 293, 366, 1333, 295, 2896, 804, 13, 400, 264, 1778, 337, 5056, 3319, 309, 341, 636, 307, 300, 341, 307, 264, 51156, 51156, 733, 295, 1507, 300, 28813, 5412, 1751, 11441, 562, 436, 19712, 47824, 294, 264, 6194, 5056, 4319, 51408, 51408, 295, 35408, 11, 457, 881, 4882, 300, 362, 665, 5201, 13, 814, 536, 733, 295, 729, 733, 295, 30310, 278, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.1828458664265085, "compression_ratio": 1.6844444444444444, "no_speech_prob": 1.9524803064996377e-05}, {"id": 122, "seek": 81280, "start": 821.92, "end": 828.64, "text": " repeat and are sort of periodical. And the reason for visualizing it this way is that this is the", "tokens": [50364, 281, 733, 295, 855, 11, 570, 309, 311, 11, 291, 458, 11, 309, 311, 281, 6490, 304, 11, 281, 855, 577, 729, 8294, 733, 295, 50820, 50820, 7149, 293, 366, 1333, 295, 2896, 804, 13, 400, 264, 1778, 337, 5056, 3319, 309, 341, 636, 307, 300, 341, 307, 264, 51156, 51156, 733, 295, 1507, 300, 28813, 5412, 1751, 11441, 562, 436, 19712, 47824, 294, 264, 6194, 5056, 4319, 51408, 51408, 295, 35408, 11, 457, 881, 4882, 300, 362, 665, 5201, 13, 814, 536, 733, 295, 729, 733, 295, 30310, 278, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.1828458664265085, "compression_ratio": 1.6844444444444444, "no_speech_prob": 1.9524803064996377e-05}, {"id": 123, "seek": 81280, "start": 828.64, "end": 833.68, "text": " kind of stuff that neuroscientists observe when they poke electrodes in the primary visual context", "tokens": [50364, 281, 733, 295, 855, 11, 570, 309, 311, 11, 291, 458, 11, 309, 311, 281, 6490, 304, 11, 281, 855, 577, 729, 8294, 733, 295, 50820, 50820, 7149, 293, 366, 1333, 295, 2896, 804, 13, 400, 264, 1778, 337, 5056, 3319, 309, 341, 636, 307, 300, 341, 307, 264, 51156, 51156, 733, 295, 1507, 300, 28813, 5412, 1751, 11441, 562, 436, 19712, 47824, 294, 264, 6194, 5056, 4319, 51408, 51408, 295, 35408, 11, 457, 881, 4882, 300, 362, 665, 5201, 13, 814, 536, 733, 295, 729, 733, 295, 30310, 278, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.1828458664265085, "compression_ratio": 1.6844444444444444, "no_speech_prob": 1.9524803064996377e-05}, {"id": 124, "seek": 81280, "start": 833.68, "end": 840.7199999999999, "text": " of mammals, but most animals that have good vision. They see kind of those kind of swirling", "tokens": [50364, 281, 733, 295, 855, 11, 570, 309, 311, 11, 291, 458, 11, 309, 311, 281, 6490, 304, 11, 281, 855, 577, 729, 8294, 733, 295, 50820, 50820, 7149, 293, 366, 1333, 295, 2896, 804, 13, 400, 264, 1778, 337, 5056, 3319, 309, 341, 636, 307, 300, 341, 307, 264, 51156, 51156, 733, 295, 1507, 300, 28813, 5412, 1751, 11441, 562, 436, 19712, 47824, 294, 264, 6194, 5056, 4319, 51408, 51408, 295, 35408, 11, 457, 881, 4882, 300, 362, 665, 5201, 13, 814, 536, 733, 295, 729, 733, 295, 30310, 278, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.1828458664265085, "compression_ratio": 1.6844444444444444, "no_speech_prob": 1.9524803064996377e-05}, {"id": 125, "seek": 84072, "start": 840.72, "end": 847.36, "text": " patterns where neighboring neurons detect similar features, which means similar oriented edges.", "tokens": [50364, 8294, 689, 31521, 22027, 5531, 2531, 4122, 11, 597, 1355, 2531, 21841, 8819, 13, 50696, 50696, 814, 366, 9477, 281, 21841, 8819, 293, 31521, 22027, 366, 9477, 281, 2531, 14708, 11, 51076, 51108, 420, 264, 912, 14708, 412, 2531, 4373, 420, 721, 411, 300, 13, 400, 370, 4317, 341, 307, 577, 264, 3567, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.2184976611221046, "compression_ratio": 1.76875, "no_speech_prob": 3.0239883926697075e-05}, {"id": 126, "seek": 84072, "start": 847.36, "end": 854.96, "text": " They are sensitive to oriented edges and neighboring neurons are sensitive to similar angles,", "tokens": [50364, 8294, 689, 31521, 22027, 5531, 2531, 4122, 11, 597, 1355, 2531, 21841, 8819, 13, 50696, 50696, 814, 366, 9477, 281, 21841, 8819, 293, 31521, 22027, 366, 9477, 281, 2531, 14708, 11, 51076, 51108, 420, 264, 912, 14708, 412, 2531, 4373, 420, 721, 411, 300, 13, 400, 370, 4317, 341, 307, 577, 264, 3567, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.2184976611221046, "compression_ratio": 1.76875, "no_speech_prob": 3.0239883926697075e-05}, {"id": 127, "seek": 84072, "start": 855.6, "end": 864.48, "text": " or the same angles at similar scale or things like that. And so perhaps this is how the brain", "tokens": [50364, 8294, 689, 31521, 22027, 5531, 2531, 4122, 11, 597, 1355, 2531, 21841, 8819, 13, 50696, 50696, 814, 366, 9477, 281, 21841, 8819, 293, 31521, 22027, 366, 9477, 281, 2531, 14708, 11, 51076, 51108, 420, 264, 912, 14708, 412, 2531, 4373, 420, 721, 411, 300, 13, 400, 370, 4317, 341, 307, 577, 264, 3567, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.2184976611221046, "compression_ratio": 1.76875, "no_speech_prob": 3.0239883926697075e-05}, {"id": 128, "seek": 86448, "start": 864.48, "end": 871.04, "text": " organizes its neurons. It's by kind of basically having some sort of criterion on the complex", "tokens": [50364, 4645, 279, 1080, 22027, 13, 467, 311, 538, 733, 295, 1936, 1419, 512, 1333, 295, 46691, 322, 264, 3997, 50692, 50692, 5438, 11, 597, 366, 264, 10344, 295, 264, 7005, 278, 6815, 300, 321, 434, 2577, 510, 13, 1692, 307, 1071, 51100, 51100, 1365, 510, 13, 407, 341, 472, 307, 406, 412, 264, 9972, 1496, 11, 457, 309, 4960, 2654, 9271, 11, 457, 309, 311, 51480, 51480, 406, 45216, 304, 294, 264, 2020, 300, 309, 1177, 380, 764, 5507, 17443, 13, 440, 1778, 337, 884, 341, 307, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.1319018039074573, "compression_ratio": 1.609442060085837, "no_speech_prob": 2.4678276531631127e-05}, {"id": 129, "seek": 86448, "start": 871.04, "end": 879.2, "text": " cells, which are the equivalent of the pooling units that we're seeing here. Here is another", "tokens": [50364, 4645, 279, 1080, 22027, 13, 467, 311, 538, 733, 295, 1936, 1419, 512, 1333, 295, 46691, 322, 264, 3997, 50692, 50692, 5438, 11, 597, 366, 264, 10344, 295, 264, 7005, 278, 6815, 300, 321, 434, 2577, 510, 13, 1692, 307, 1071, 51100, 51100, 1365, 510, 13, 407, 341, 472, 307, 406, 412, 264, 9972, 1496, 11, 457, 309, 4960, 2654, 9271, 11, 457, 309, 311, 51480, 51480, 406, 45216, 304, 294, 264, 2020, 300, 309, 1177, 380, 764, 5507, 17443, 13, 440, 1778, 337, 884, 341, 307, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.1319018039074573, "compression_ratio": 1.609442060085837, "no_speech_prob": 2.4678276531631127e-05}, {"id": 130, "seek": 86448, "start": 879.2, "end": 886.8000000000001, "text": " example here. So this one is not at the patch level, but it uses local connections, but it's", "tokens": [50364, 4645, 279, 1080, 22027, 13, 467, 311, 538, 733, 295, 1936, 1419, 512, 1333, 295, 46691, 322, 264, 3997, 50692, 50692, 5438, 11, 597, 366, 264, 10344, 295, 264, 7005, 278, 6815, 300, 321, 434, 2577, 510, 13, 1692, 307, 1071, 51100, 51100, 1365, 510, 13, 407, 341, 472, 307, 406, 412, 264, 9972, 1496, 11, 457, 309, 4960, 2654, 9271, 11, 457, 309, 311, 51480, 51480, 406, 45216, 304, 294, 264, 2020, 300, 309, 1177, 380, 764, 5507, 17443, 13, 440, 1778, 337, 884, 341, 307, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.1319018039074573, "compression_ratio": 1.609442060085837, "no_speech_prob": 2.4678276531631127e-05}, {"id": 131, "seek": 86448, "start": 886.8000000000001, "end": 891.36, "text": " not convolutional in the sense that it doesn't use shared weights. The reason for doing this is", "tokens": [50364, 4645, 279, 1080, 22027, 13, 467, 311, 538, 733, 295, 1936, 1419, 512, 1333, 295, 46691, 322, 264, 3997, 50692, 50692, 5438, 11, 597, 366, 264, 10344, 295, 264, 7005, 278, 6815, 300, 321, 434, 2577, 510, 13, 1692, 307, 1071, 51100, 51100, 1365, 510, 13, 407, 341, 472, 307, 406, 412, 264, 9972, 1496, 11, 457, 309, 4960, 2654, 9271, 11, 457, 309, 311, 51480, 51480, 406, 45216, 304, 294, 264, 2020, 300, 309, 1177, 380, 764, 5507, 17443, 13, 440, 1778, 337, 884, 341, 307, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.1319018039074573, "compression_ratio": 1.609442060085837, "no_speech_prob": 2.4678276531631127e-05}, {"id": 132, "seek": 89136, "start": 891.36, "end": 901.84, "text": " to have some semi-realistic correspondence to a sort of biological learning where, of course,", "tokens": [50364, 281, 362, 512, 12909, 12, 9342, 3142, 38135, 281, 257, 1333, 295, 13910, 2539, 689, 11, 295, 1164, 11, 50888, 50888, 22027, 294, 264, 3567, 393, 2073, 17443, 13, 814, 917, 493, 885, 2531, 570, 436, 3847, 1228, 512, 1333, 51248, 51248, 295, 2693, 12879, 24420, 2539, 11, 457, 456, 307, 572, 1270, 551, 382, 3364, 5414, 294, 264, 3567, 11, 382, 1400, 382, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.19736549433539896, "compression_ratio": 1.6055555555555556, "no_speech_prob": 8.938653991208412e-06}, {"id": 133, "seek": 89136, "start": 901.84, "end": 909.04, "text": " neurons in the brain can share weights. They end up being similar because they train using some sort", "tokens": [50364, 281, 362, 512, 12909, 12, 9342, 3142, 38135, 281, 257, 1333, 295, 13910, 2539, 689, 11, 295, 1164, 11, 50888, 50888, 22027, 294, 264, 3567, 393, 2073, 17443, 13, 814, 917, 493, 885, 2531, 570, 436, 3847, 1228, 512, 1333, 51248, 51248, 295, 2693, 12879, 24420, 2539, 11, 457, 456, 307, 572, 1270, 551, 382, 3364, 5414, 294, 264, 3567, 11, 382, 1400, 382, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.19736549433539896, "compression_ratio": 1.6055555555555556, "no_speech_prob": 8.938653991208412e-06}, {"id": 134, "seek": 89136, "start": 909.04, "end": 915.84, "text": " of unsupervised learning, but there is no such thing as weight sharing in the brain, as far as", "tokens": [50364, 281, 362, 512, 12909, 12, 9342, 3142, 38135, 281, 257, 1333, 295, 13910, 2539, 689, 11, 295, 1164, 11, 50888, 50888, 22027, 294, 264, 3567, 393, 2073, 17443, 13, 814, 917, 493, 885, 2531, 570, 436, 3847, 1228, 512, 1333, 51248, 51248, 295, 2693, 12879, 24420, 2539, 11, 457, 456, 307, 572, 1270, 551, 382, 3364, 5414, 294, 264, 3567, 11, 382, 1400, 382, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.19736549433539896, "compression_ratio": 1.6055555555555556, "no_speech_prob": 8.938653991208412e-06}, {"id": 135, "seek": 91584, "start": 915.84, "end": 922.96, "text": " we know. So it was asked if a similar strategy of the training of the autoencoder with the", "tokens": [50364, 321, 458, 13, 407, 309, 390, 2351, 498, 257, 2531, 5206, 295, 264, 3097, 295, 264, 8399, 22660, 19866, 365, 264, 50720, 50720, 1508, 9902, 293, 264, 3890, 6545, 393, 312, 6456, 337, 257, 3034, 1478, 8399, 22660, 19866, 293, 1968, 341, 575, 668, 51020, 51020, 24016, 11, 498, 309, 1985, 382, 731, 337, 264, 700, 4137, 291, 855, 13, 865, 11, 370, 1936, 5127, 5658, 294, 257, 51464, 51464, 3034, 1478, 8399, 22660, 19866, 293, 19030, 637, 685, 507, 366, 1936, 732, 2098, 281, 4584, 264, 912, 4334, 11, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.1919097697481196, "compression_ratio": 1.7232142857142858, "no_speech_prob": 8.939217877923511e-06}, {"id": 136, "seek": 91584, "start": 922.96, "end": 928.96, "text": " classifier and the regularizer can be applied for a variational autoencoder and whether this has been", "tokens": [50364, 321, 458, 13, 407, 309, 390, 2351, 498, 257, 2531, 5206, 295, 264, 3097, 295, 264, 8399, 22660, 19866, 365, 264, 50720, 50720, 1508, 9902, 293, 264, 3890, 6545, 393, 312, 6456, 337, 257, 3034, 1478, 8399, 22660, 19866, 293, 1968, 341, 575, 668, 51020, 51020, 24016, 11, 498, 309, 1985, 382, 731, 337, 264, 700, 4137, 291, 855, 13, 865, 11, 370, 1936, 5127, 5658, 294, 257, 51464, 51464, 3034, 1478, 8399, 22660, 19866, 293, 19030, 637, 685, 507, 366, 1936, 732, 2098, 281, 4584, 264, 912, 4334, 11, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.1919097697481196, "compression_ratio": 1.7232142857142858, "no_speech_prob": 8.939217877923511e-06}, {"id": 137, "seek": 91584, "start": 928.96, "end": 937.84, "text": " explored, if it works as well for the first slide you show. Yeah, so basically adding noise in a", "tokens": [50364, 321, 458, 13, 407, 309, 390, 2351, 498, 257, 2531, 5206, 295, 264, 3097, 295, 264, 8399, 22660, 19866, 365, 264, 50720, 50720, 1508, 9902, 293, 264, 3890, 6545, 393, 312, 6456, 337, 257, 3034, 1478, 8399, 22660, 19866, 293, 1968, 341, 575, 668, 51020, 51020, 24016, 11, 498, 309, 1985, 382, 731, 337, 264, 700, 4137, 291, 855, 13, 865, 11, 370, 1936, 5127, 5658, 294, 257, 51464, 51464, 3034, 1478, 8399, 22660, 19866, 293, 19030, 637, 685, 507, 366, 1936, 732, 2098, 281, 4584, 264, 912, 4334, 11, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.1919097697481196, "compression_ratio": 1.7232142857142858, "no_speech_prob": 8.939217877923511e-06}, {"id": 138, "seek": 91584, "start": 937.84, "end": 944.1600000000001, "text": " variational autoencoder and forcing sparsity are basically two ways to achieve the same purpose,", "tokens": [50364, 321, 458, 13, 407, 309, 390, 2351, 498, 257, 2531, 5206, 295, 264, 3097, 295, 264, 8399, 22660, 19866, 365, 264, 50720, 50720, 1508, 9902, 293, 264, 3890, 6545, 393, 312, 6456, 337, 257, 3034, 1478, 8399, 22660, 19866, 293, 1968, 341, 575, 668, 51020, 51020, 24016, 11, 498, 309, 1985, 382, 731, 337, 264, 700, 4137, 291, 855, 13, 865, 11, 370, 1936, 5127, 5658, 294, 257, 51464, 51464, 3034, 1478, 8399, 22660, 19866, 293, 19030, 637, 685, 507, 366, 1936, 732, 2098, 281, 4584, 264, 912, 4334, 11, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.1919097697481196, "compression_ratio": 1.7232142857142858, "no_speech_prob": 8.939217877923511e-06}, {"id": 139, "seek": 94416, "start": 944.16, "end": 949.6, "text": " which is reduce the capacity of the latent variable, reduce the capacity of the code that", "tokens": [50364, 597, 307, 5407, 264, 6042, 295, 264, 48994, 7006, 11, 5407, 264, 6042, 295, 264, 3089, 300, 50636, 50636, 307, 34086, 538, 264, 8399, 22660, 19866, 13, 400, 341, 307, 437, 22367, 264, 1185, 490, 2614, 257, 26703, 50880, 50880, 6575, 2445, 11, 597, 576, 406, 312, 4420, 13, 400, 437, 321, 2825, 466, 264, 1036, 1916, 1413, 51140, 51140, 307, 264, 1186, 300, 498, 291, 5407, 264, 1589, 6042, 295, 264, 48994, 7006, 295, 264, 3089, 11, 51428, 51428, 382, 257, 18326, 11, 291, 611, 17522, 264, 5523, 295, 1901, 300, 393, 747, 2295, 2281, 13, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.16002485067537514, "compression_ratio": 1.8786610878661087, "no_speech_prob": 8.792782864475157e-06}, {"id": 140, "seek": 94416, "start": 949.6, "end": 954.48, "text": " is extracted by the autoencoder. And this is what prevents the system from running a trivial", "tokens": [50364, 597, 307, 5407, 264, 6042, 295, 264, 48994, 7006, 11, 5407, 264, 6042, 295, 264, 3089, 300, 50636, 50636, 307, 34086, 538, 264, 8399, 22660, 19866, 13, 400, 341, 307, 437, 22367, 264, 1185, 490, 2614, 257, 26703, 50880, 50880, 6575, 2445, 11, 597, 576, 406, 312, 4420, 13, 400, 437, 321, 2825, 466, 264, 1036, 1916, 1413, 51140, 51140, 307, 264, 1186, 300, 498, 291, 5407, 264, 1589, 6042, 295, 264, 48994, 7006, 295, 264, 3089, 11, 51428, 51428, 382, 257, 18326, 11, 291, 611, 17522, 264, 5523, 295, 1901, 300, 393, 747, 2295, 2281, 13, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.16002485067537514, "compression_ratio": 1.8786610878661087, "no_speech_prob": 8.792782864475157e-06}, {"id": 141, "seek": 94416, "start": 954.48, "end": 959.68, "text": " identity function, which would not be useful. And what we talked about the last couple times", "tokens": [50364, 597, 307, 5407, 264, 6042, 295, 264, 48994, 7006, 11, 5407, 264, 6042, 295, 264, 3089, 300, 50636, 50636, 307, 34086, 538, 264, 8399, 22660, 19866, 13, 400, 341, 307, 437, 22367, 264, 1185, 490, 2614, 257, 26703, 50880, 50880, 6575, 2445, 11, 597, 576, 406, 312, 4420, 13, 400, 437, 321, 2825, 466, 264, 1036, 1916, 1413, 51140, 51140, 307, 264, 1186, 300, 498, 291, 5407, 264, 1589, 6042, 295, 264, 48994, 7006, 295, 264, 3089, 11, 51428, 51428, 382, 257, 18326, 11, 291, 611, 17522, 264, 5523, 295, 1901, 300, 393, 747, 2295, 2281, 13, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.16002485067537514, "compression_ratio": 1.8786610878661087, "no_speech_prob": 8.792782864475157e-06}, {"id": 142, "seek": 94416, "start": 959.68, "end": 965.4399999999999, "text": " is the fact that if you reduce the information capacity of the latent variable of the code,", "tokens": [50364, 597, 307, 5407, 264, 6042, 295, 264, 48994, 7006, 11, 5407, 264, 6042, 295, 264, 3089, 300, 50636, 50636, 307, 34086, 538, 264, 8399, 22660, 19866, 13, 400, 341, 307, 437, 22367, 264, 1185, 490, 2614, 257, 26703, 50880, 50880, 6575, 2445, 11, 597, 576, 406, 312, 4420, 13, 400, 437, 321, 2825, 466, 264, 1036, 1916, 1413, 51140, 51140, 307, 264, 1186, 300, 498, 291, 5407, 264, 1589, 6042, 295, 264, 48994, 7006, 295, 264, 3089, 11, 51428, 51428, 382, 257, 18326, 11, 291, 611, 17522, 264, 5523, 295, 1901, 300, 393, 747, 2295, 2281, 13, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.16002485067537514, "compression_ratio": 1.8786610878661087, "no_speech_prob": 8.792782864475157e-06}, {"id": 143, "seek": 94416, "start": 965.4399999999999, "end": 972.8, "text": " as a consequence, you also minimize the volume of space that can take low energy.", "tokens": [50364, 597, 307, 5407, 264, 6042, 295, 264, 48994, 7006, 11, 5407, 264, 6042, 295, 264, 3089, 300, 50636, 50636, 307, 34086, 538, 264, 8399, 22660, 19866, 13, 400, 341, 307, 437, 22367, 264, 1185, 490, 2614, 257, 26703, 50880, 50880, 6575, 2445, 11, 597, 576, 406, 312, 4420, 13, 400, 437, 321, 2825, 466, 264, 1036, 1916, 1413, 51140, 51140, 307, 264, 1186, 300, 498, 291, 5407, 264, 1589, 6042, 295, 264, 48994, 7006, 295, 264, 3089, 11, 51428, 51428, 382, 257, 18326, 11, 291, 611, 17522, 264, 5523, 295, 1901, 300, 393, 747, 2295, 2281, 13, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.16002485067537514, "compression_ratio": 1.8786610878661087, "no_speech_prob": 8.792782864475157e-06}, {"id": 144, "seek": 97280, "start": 972.8, "end": 977.52, "text": " Okay, because you limit the number of configurations of the code. And so as a consequence,", "tokens": [50364, 1033, 11, 570, 291, 4948, 264, 1230, 295, 31493, 295, 264, 3089, 13, 400, 370, 382, 257, 18326, 11, 50600, 50600, 291, 733, 295, 4948, 264, 5523, 295, 1901, 300, 393, 747, 2295, 2281, 13, 407, 4476, 341, 1558, 295, 50860, 50908, 3890, 3319, 365, 441, 16, 420, 637, 685, 507, 420, 746, 411, 341, 11, 420, 5127, 5658, 281, 257, 3089, 1339, 22083, 51188, 51188, 264, 2026, 295, 264, 3089, 4584, 264, 912, 4334, 11, 597, 307, 22083, 264, 6042, 295, 264, 3089, 337, 51512, 51512, 264, 4334, 295, 22083, 264, 5523, 295, 1901, 300, 393, 747, 2295, 2281, 13, 400, 382, 257, 18326, 11, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.19614810943603517, "compression_ratio": 1.9098360655737705, "no_speech_prob": 1.7215654224855825e-05}, {"id": 145, "seek": 97280, "start": 977.52, "end": 982.7199999999999, "text": " you kind of limit the volume of space that can take low energy. So essentially this idea of", "tokens": [50364, 1033, 11, 570, 291, 4948, 264, 1230, 295, 31493, 295, 264, 3089, 13, 400, 370, 382, 257, 18326, 11, 50600, 50600, 291, 733, 295, 4948, 264, 5523, 295, 1901, 300, 393, 747, 2295, 2281, 13, 407, 4476, 341, 1558, 295, 50860, 50908, 3890, 3319, 365, 441, 16, 420, 637, 685, 507, 420, 746, 411, 341, 11, 420, 5127, 5658, 281, 257, 3089, 1339, 22083, 51188, 51188, 264, 2026, 295, 264, 3089, 4584, 264, 912, 4334, 11, 597, 307, 22083, 264, 6042, 295, 264, 3089, 337, 51512, 51512, 264, 4334, 295, 22083, 264, 5523, 295, 1901, 300, 393, 747, 2295, 2281, 13, 400, 382, 257, 18326, 11, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.19614810943603517, "compression_ratio": 1.9098360655737705, "no_speech_prob": 1.7215654224855825e-05}, {"id": 146, "seek": 97280, "start": 983.68, "end": 989.28, "text": " regularizing with L1 or sparsity or something like this, or adding noise to a code while limiting", "tokens": [50364, 1033, 11, 570, 291, 4948, 264, 1230, 295, 31493, 295, 264, 3089, 13, 400, 370, 382, 257, 18326, 11, 50600, 50600, 291, 733, 295, 4948, 264, 5523, 295, 1901, 300, 393, 747, 2295, 2281, 13, 407, 4476, 341, 1558, 295, 50860, 50908, 3890, 3319, 365, 441, 16, 420, 637, 685, 507, 420, 746, 411, 341, 11, 420, 5127, 5658, 281, 257, 3089, 1339, 22083, 51188, 51188, 264, 2026, 295, 264, 3089, 4584, 264, 912, 4334, 11, 597, 307, 22083, 264, 6042, 295, 264, 3089, 337, 51512, 51512, 264, 4334, 295, 22083, 264, 5523, 295, 1901, 300, 393, 747, 2295, 2281, 13, 400, 382, 257, 18326, 11, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.19614810943603517, "compression_ratio": 1.9098360655737705, "no_speech_prob": 1.7215654224855825e-05}, {"id": 147, "seek": 97280, "start": 989.28, "end": 995.76, "text": " the norm of the code achieve the same purpose, which is limiting the capacity of the code for", "tokens": [50364, 1033, 11, 570, 291, 4948, 264, 1230, 295, 31493, 295, 264, 3089, 13, 400, 370, 382, 257, 18326, 11, 50600, 50600, 291, 733, 295, 4948, 264, 5523, 295, 1901, 300, 393, 747, 2295, 2281, 13, 407, 4476, 341, 1558, 295, 50860, 50908, 3890, 3319, 365, 441, 16, 420, 637, 685, 507, 420, 746, 411, 341, 11, 420, 5127, 5658, 281, 257, 3089, 1339, 22083, 51188, 51188, 264, 2026, 295, 264, 3089, 4584, 264, 912, 4334, 11, 597, 307, 22083, 264, 6042, 295, 264, 3089, 337, 51512, 51512, 264, 4334, 295, 22083, 264, 5523, 295, 1901, 300, 393, 747, 2295, 2281, 13, 400, 382, 257, 18326, 11, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.19614810943603517, "compression_ratio": 1.9098360655737705, "no_speech_prob": 1.7215654224855825e-05}, {"id": 148, "seek": 97280, "start": 995.76, "end": 1001.92, "text": " the purpose of limiting the volume of space that can take low energy. And as a consequence,", "tokens": [50364, 1033, 11, 570, 291, 4948, 264, 1230, 295, 31493, 295, 264, 3089, 13, 400, 370, 382, 257, 18326, 11, 50600, 50600, 291, 733, 295, 4948, 264, 5523, 295, 1901, 300, 393, 747, 2295, 2281, 13, 407, 4476, 341, 1558, 295, 50860, 50908, 3890, 3319, 365, 441, 16, 420, 637, 685, 507, 420, 746, 411, 341, 11, 420, 5127, 5658, 281, 257, 3089, 1339, 22083, 51188, 51188, 264, 2026, 295, 264, 3089, 4584, 264, 912, 4334, 11, 597, 307, 22083, 264, 6042, 295, 264, 3089, 337, 51512, 51512, 264, 4334, 295, 22083, 264, 5523, 295, 1901, 300, 393, 747, 2295, 2281, 13, 400, 382, 257, 18326, 11, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.19614810943603517, "compression_ratio": 1.9098360655737705, "no_speech_prob": 1.7215654224855825e-05}, {"id": 149, "seek": 100192, "start": 1001.92, "end": 1006.24, "text": " if you train part of the space to have low energy by minimizing the reconstruction error", "tokens": [50364, 498, 291, 3847, 644, 295, 264, 1901, 281, 362, 2295, 2281, 538, 46608, 264, 31565, 6713, 50580, 50580, 322, 428, 3097, 10938, 11, 6772, 264, 1472, 295, 264, 1901, 486, 362, 2946, 2281, 50796, 50796, 570, 264, 6042, 11, 264, 5523, 300, 393, 747, 2295, 2281, 307, 5567, 13, 407, 341, 307, 445, 281, 20928, 51136, 51136, 437, 321, 2825, 466, 1036, 565, 293, 257, 1916, 295, 3259, 2057, 13, 639, 307, 1333, 295, 264, 8535, 13, 51512, 51512, 407, 729, 733, 295, 26621, 7150, 366, 20478, 281, 264, 8712, 488, 7150, 689, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.1480201839171734, "compression_ratio": 1.7888446215139442, "no_speech_prob": 3.881840893882327e-05}, {"id": 150, "seek": 100192, "start": 1006.24, "end": 1010.56, "text": " on your training samples, automatically the rest of the space will have higher energy", "tokens": [50364, 498, 291, 3847, 644, 295, 264, 1901, 281, 362, 2295, 2281, 538, 46608, 264, 31565, 6713, 50580, 50580, 322, 428, 3097, 10938, 11, 6772, 264, 1472, 295, 264, 1901, 486, 362, 2946, 2281, 50796, 50796, 570, 264, 6042, 11, 264, 5523, 300, 393, 747, 2295, 2281, 307, 5567, 13, 407, 341, 307, 445, 281, 20928, 51136, 51136, 437, 321, 2825, 466, 1036, 565, 293, 257, 1916, 295, 3259, 2057, 13, 639, 307, 1333, 295, 264, 8535, 13, 51512, 51512, 407, 729, 733, 295, 26621, 7150, 366, 20478, 281, 264, 8712, 488, 7150, 689, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.1480201839171734, "compression_ratio": 1.7888446215139442, "no_speech_prob": 3.881840893882327e-05}, {"id": 151, "seek": 100192, "start": 1010.56, "end": 1017.36, "text": " because the capacity, the volume that can take low energy is limited. So this is just to recap", "tokens": [50364, 498, 291, 3847, 644, 295, 264, 1901, 281, 362, 2295, 2281, 538, 46608, 264, 31565, 6713, 50580, 50580, 322, 428, 3097, 10938, 11, 6772, 264, 1472, 295, 264, 1901, 486, 362, 2946, 2281, 50796, 50796, 570, 264, 6042, 11, 264, 5523, 300, 393, 747, 2295, 2281, 307, 5567, 13, 407, 341, 307, 445, 281, 20928, 51136, 51136, 437, 321, 2825, 466, 1036, 565, 293, 257, 1916, 295, 3259, 2057, 13, 639, 307, 1333, 295, 264, 8535, 13, 51512, 51512, 407, 729, 733, 295, 26621, 7150, 366, 20478, 281, 264, 8712, 488, 7150, 689, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.1480201839171734, "compression_ratio": 1.7888446215139442, "no_speech_prob": 3.881840893882327e-05}, {"id": 152, "seek": 100192, "start": 1017.36, "end": 1024.8799999999999, "text": " what we talked about last time and a couple of weeks ago. This is sort of the alternative.", "tokens": [50364, 498, 291, 3847, 644, 295, 264, 1901, 281, 362, 2295, 2281, 538, 46608, 264, 31565, 6713, 50580, 50580, 322, 428, 3097, 10938, 11, 6772, 264, 1472, 295, 264, 1901, 486, 362, 2946, 2281, 50796, 50796, 570, 264, 6042, 11, 264, 5523, 300, 393, 747, 2295, 2281, 307, 5567, 13, 407, 341, 307, 445, 281, 20928, 51136, 51136, 437, 321, 2825, 466, 1036, 565, 293, 257, 1916, 295, 3259, 2057, 13, 639, 307, 1333, 295, 264, 8535, 13, 51512, 51512, 407, 729, 733, 295, 26621, 7150, 366, 20478, 281, 264, 8712, 488, 7150, 689, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.1480201839171734, "compression_ratio": 1.7888446215139442, "no_speech_prob": 3.881840893882327e-05}, {"id": 153, "seek": 100192, "start": 1024.8799999999999, "end": 1029.44, "text": " So those kind of architectural methods are alternatives to the contrastive methods where", "tokens": [50364, 498, 291, 3847, 644, 295, 264, 1901, 281, 362, 2295, 2281, 538, 46608, 264, 31565, 6713, 50580, 50580, 322, 428, 3097, 10938, 11, 6772, 264, 1472, 295, 264, 1901, 486, 362, 2946, 2281, 50796, 50796, 570, 264, 6042, 11, 264, 5523, 300, 393, 747, 2295, 2281, 307, 5567, 13, 407, 341, 307, 445, 281, 20928, 51136, 51136, 437, 321, 2825, 466, 1036, 565, 293, 257, 1916, 295, 3259, 2057, 13, 639, 307, 1333, 295, 264, 8535, 13, 51512, 51512, 407, 729, 733, 295, 26621, 7150, 366, 20478, 281, 264, 8712, 488, 7150, 689, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.1480201839171734, "compression_ratio": 1.7888446215139442, "no_speech_prob": 3.881840893882327e-05}, {"id": 154, "seek": 102944, "start": 1029.44, "end": 1035.6000000000001, "text": " you explicitly push up on the energy of bad samples, which means you have to come up with", "tokens": [50364, 291, 20803, 2944, 493, 322, 264, 2281, 295, 1578, 10938, 11, 597, 1355, 291, 362, 281, 808, 493, 365, 50672, 50672, 257, 665, 636, 295, 17746, 1578, 10938, 294, 300, 1389, 13, 407, 797, 11, 1604, 729, 732, 3467, 295, 7150, 11, 51020, 51020, 8712, 488, 7150, 11, 291, 2944, 760, 264, 2281, 295, 264, 3097, 10938, 11, 291, 2944, 484, 264, 2281, 295, 51216, 51216, 1507, 2380, 2139, 538, 17366, 278, 264, 3380, 10938, 420, 538, 884, 24518, 16235, 23475, 11, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.15490271324335142, "compression_ratio": 1.7867298578199051, "no_speech_prob": 1.9828696167678572e-05}, {"id": 155, "seek": 102944, "start": 1035.6000000000001, "end": 1042.56, "text": " a good way of generating bad samples in that case. So again, remember those two types of methods,", "tokens": [50364, 291, 20803, 2944, 493, 322, 264, 2281, 295, 1578, 10938, 11, 597, 1355, 291, 362, 281, 808, 493, 365, 50672, 50672, 257, 665, 636, 295, 17746, 1578, 10938, 294, 300, 1389, 13, 407, 797, 11, 1604, 729, 732, 3467, 295, 7150, 11, 51020, 51020, 8712, 488, 7150, 11, 291, 2944, 760, 264, 2281, 295, 264, 3097, 10938, 11, 291, 2944, 484, 264, 2281, 295, 51216, 51216, 1507, 2380, 2139, 538, 17366, 278, 264, 3380, 10938, 420, 538, 884, 24518, 16235, 23475, 11, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.15490271324335142, "compression_ratio": 1.7867298578199051, "no_speech_prob": 1.9828696167678572e-05}, {"id": 156, "seek": 102944, "start": 1042.56, "end": 1046.48, "text": " contrastive methods, you push down the energy of the training samples, you push out the energy of", "tokens": [50364, 291, 20803, 2944, 493, 322, 264, 2281, 295, 1578, 10938, 11, 597, 1355, 291, 362, 281, 808, 493, 365, 50672, 50672, 257, 665, 636, 295, 17746, 1578, 10938, 294, 300, 1389, 13, 407, 797, 11, 1604, 729, 732, 3467, 295, 7150, 11, 51020, 51020, 8712, 488, 7150, 11, 291, 2944, 760, 264, 2281, 295, 264, 3097, 10938, 11, 291, 2944, 484, 264, 2281, 295, 51216, 51216, 1507, 2380, 2139, 538, 17366, 278, 264, 3380, 10938, 420, 538, 884, 24518, 16235, 23475, 11, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.15490271324335142, "compression_ratio": 1.7867298578199051, "no_speech_prob": 1.9828696167678572e-05}, {"id": 157, "seek": 102944, "start": 1046.48, "end": 1055.04, "text": " stuff outside either by corrupting the original samples or by doing noisy gradient descent,", "tokens": [50364, 291, 20803, 2944, 493, 322, 264, 2281, 295, 1578, 10938, 11, 597, 1355, 291, 362, 281, 808, 493, 365, 50672, 50672, 257, 665, 636, 295, 17746, 1578, 10938, 294, 300, 1389, 13, 407, 797, 11, 1604, 729, 732, 3467, 295, 7150, 11, 51020, 51020, 8712, 488, 7150, 11, 291, 2944, 760, 264, 2281, 295, 264, 3097, 10938, 11, 291, 2944, 484, 264, 2281, 295, 51216, 51216, 1507, 2380, 2139, 538, 17366, 278, 264, 3380, 10938, 420, 538, 884, 24518, 16235, 23475, 11, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.15490271324335142, "compression_ratio": 1.7867298578199051, "no_speech_prob": 1.9828696167678572e-05}, {"id": 158, "seek": 105504, "start": 1055.04, "end": 1059.44, "text": " contrastive divergence, things like this, or by generating contrastive points in some way.", "tokens": [50364, 8712, 488, 47387, 11, 721, 411, 341, 11, 420, 538, 17746, 8712, 488, 2793, 294, 512, 636, 13, 50584, 50584, 492, 600, 1612, 257, 3840, 295, 819, 8712, 488, 7150, 13, 400, 550, 264, 8535, 307, 50888, 50960, 22083, 264, 6042, 295, 3089, 420, 733, 295, 22083, 264, 5523, 295, 1507, 300, 393, 747, 2295, 2281, 294, 51288, 51288, 264, 4319, 295, 8399, 22660, 19866, 420, 6069, 284, 13, 639, 1355, 22083, 264, 6042, 295, 264, 3089, 13, 51520, 51588], "temperature": 0.0, "avg_logprob": -0.23784581626333842, "compression_ratio": 1.7198067632850242, "no_speech_prob": 1.6428813978563994e-05}, {"id": 159, "seek": 105504, "start": 1059.44, "end": 1065.52, "text": " We've seen a bunch of different contrastive methods. And then the alternative is", "tokens": [50364, 8712, 488, 47387, 11, 721, 411, 341, 11, 420, 538, 17746, 8712, 488, 2793, 294, 512, 636, 13, 50584, 50584, 492, 600, 1612, 257, 3840, 295, 819, 8712, 488, 7150, 13, 400, 550, 264, 8535, 307, 50888, 50960, 22083, 264, 6042, 295, 3089, 420, 733, 295, 22083, 264, 5523, 295, 1507, 300, 393, 747, 2295, 2281, 294, 51288, 51288, 264, 4319, 295, 8399, 22660, 19866, 420, 6069, 284, 13, 639, 1355, 22083, 264, 6042, 295, 264, 3089, 13, 51520, 51588], "temperature": 0.0, "avg_logprob": -0.23784581626333842, "compression_ratio": 1.7198067632850242, "no_speech_prob": 1.6428813978563994e-05}, {"id": 160, "seek": 105504, "start": 1066.96, "end": 1073.52, "text": " limiting the capacity of code or kind of limiting the volume of stuff that can take low energy in", "tokens": [50364, 8712, 488, 47387, 11, 721, 411, 341, 11, 420, 538, 17746, 8712, 488, 2793, 294, 512, 636, 13, 50584, 50584, 492, 600, 1612, 257, 3840, 295, 819, 8712, 488, 7150, 13, 400, 550, 264, 8535, 307, 50888, 50960, 22083, 264, 6042, 295, 3089, 420, 733, 295, 22083, 264, 5523, 295, 1507, 300, 393, 747, 2295, 2281, 294, 51288, 51288, 264, 4319, 295, 8399, 22660, 19866, 420, 6069, 284, 13, 639, 1355, 22083, 264, 6042, 295, 264, 3089, 13, 51520, 51588], "temperature": 0.0, "avg_logprob": -0.23784581626333842, "compression_ratio": 1.7198067632850242, "no_speech_prob": 1.6428813978563994e-05}, {"id": 161, "seek": 105504, "start": 1073.52, "end": 1078.1599999999999, "text": " the context of autoencoder or predictor. This means limiting the capacity of the code.", "tokens": [50364, 8712, 488, 47387, 11, 721, 411, 341, 11, 420, 538, 17746, 8712, 488, 2793, 294, 512, 636, 13, 50584, 50584, 492, 600, 1612, 257, 3840, 295, 819, 8712, 488, 7150, 13, 400, 550, 264, 8535, 307, 50888, 50960, 22083, 264, 6042, 295, 3089, 420, 733, 295, 22083, 264, 5523, 295, 1507, 300, 393, 747, 2295, 2281, 294, 51288, 51288, 264, 4319, 295, 8399, 22660, 19866, 420, 6069, 284, 13, 639, 1355, 22083, 264, 6042, 295, 264, 3089, 13, 51520, 51588], "temperature": 0.0, "avg_logprob": -0.23784581626333842, "compression_ratio": 1.7198067632850242, "no_speech_prob": 1.6428813978563994e-05}, {"id": 162, "seek": 107816, "start": 1078.16, "end": 1083.76, "text": " And there are many ways to do this. One way is to sparsity, one way is through adding noise while", "tokens": [50364, 400, 456, 366, 867, 2098, 281, 360, 341, 13, 1485, 636, 307, 281, 637, 685, 507, 11, 472, 636, 307, 807, 5127, 5658, 1339, 50644, 50644, 22083, 264, 2026, 11, 300, 311, 264, 316, 20442, 13, 400, 456, 366, 661, 2098, 300, 321, 603, 751, 466, 294, 257, 3456, 13, 50892, 50892, 14159, 291, 645, 1417, 949, 466, 264, 1594, 637, 685, 507, 11, 291, 645, 2408, 2810, 445, 257, 1326, 10938, 11, 51188, 51188, 411, 257, 1326, 8186, 279, 1951, 257, 1359, 3613, 13, 708, 307, 300, 30549, 30, 51404, 51404, 2704, 286, 994, 380, 12, 30549, 307, 257, 1594, 13, 467, 311, 257, 7005, 13, 407, 3811, 341, 307, 257, 7005, 11, 411, 294, 257, 45216, 2533, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.222930662093624, "compression_ratio": 1.6754716981132076, "no_speech_prob": 2.0577806481014704e-06}, {"id": 163, "seek": 107816, "start": 1083.76, "end": 1088.72, "text": " limiting the norm, that's the AEs. And there are other ways that we'll talk about in a minute.", "tokens": [50364, 400, 456, 366, 867, 2098, 281, 360, 341, 13, 1485, 636, 307, 281, 637, 685, 507, 11, 472, 636, 307, 807, 5127, 5658, 1339, 50644, 50644, 22083, 264, 2026, 11, 300, 311, 264, 316, 20442, 13, 400, 456, 366, 661, 2098, 300, 321, 603, 751, 466, 294, 257, 3456, 13, 50892, 50892, 14159, 291, 645, 1417, 949, 466, 264, 1594, 637, 685, 507, 11, 291, 645, 2408, 2810, 445, 257, 1326, 10938, 11, 51188, 51188, 411, 257, 1326, 8186, 279, 1951, 257, 1359, 3613, 13, 708, 307, 300, 30549, 30, 51404, 51404, 2704, 286, 994, 380, 12, 30549, 307, 257, 1594, 13, 467, 311, 257, 7005, 13, 407, 3811, 341, 307, 257, 7005, 11, 411, 294, 257, 45216, 2533, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.222930662093624, "compression_ratio": 1.6754716981132076, "no_speech_prob": 2.0577806481014704e-06}, {"id": 164, "seek": 107816, "start": 1088.72, "end": 1094.64, "text": " Whenever you were talking before about the group sparsity, you were summing just a few samples,", "tokens": [50364, 400, 456, 366, 867, 2098, 281, 360, 341, 13, 1485, 636, 307, 281, 637, 685, 507, 11, 472, 636, 307, 807, 5127, 5658, 1339, 50644, 50644, 22083, 264, 2026, 11, 300, 311, 264, 316, 20442, 13, 400, 456, 366, 661, 2098, 300, 321, 603, 751, 466, 294, 257, 3456, 13, 50892, 50892, 14159, 291, 645, 1417, 949, 466, 264, 1594, 637, 685, 507, 11, 291, 645, 2408, 2810, 445, 257, 1326, 10938, 11, 51188, 51188, 411, 257, 1326, 8186, 279, 1951, 257, 1359, 3613, 13, 708, 307, 300, 30549, 30, 51404, 51404, 2704, 286, 994, 380, 12, 30549, 307, 257, 1594, 13, 467, 311, 257, 7005, 13, 407, 3811, 341, 307, 257, 7005, 11, 411, 294, 257, 45216, 2533, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.222930662093624, "compression_ratio": 1.6754716981132076, "no_speech_prob": 2.0577806481014704e-06}, {"id": 165, "seek": 107816, "start": 1094.64, "end": 1098.96, "text": " like a few indexes within a small range. What is that PJ?", "tokens": [50364, 400, 456, 366, 867, 2098, 281, 360, 341, 13, 1485, 636, 307, 281, 637, 685, 507, 11, 472, 636, 307, 807, 5127, 5658, 1339, 50644, 50644, 22083, 264, 2026, 11, 300, 311, 264, 316, 20442, 13, 400, 456, 366, 661, 2098, 300, 321, 603, 751, 466, 294, 257, 3456, 13, 50892, 50892, 14159, 291, 645, 1417, 949, 466, 264, 1594, 637, 685, 507, 11, 291, 645, 2408, 2810, 445, 257, 1326, 10938, 11, 51188, 51188, 411, 257, 1326, 8186, 279, 1951, 257, 1359, 3613, 13, 708, 307, 300, 30549, 30, 51404, 51404, 2704, 286, 994, 380, 12, 30549, 307, 257, 1594, 13, 467, 311, 257, 7005, 13, 407, 3811, 341, 307, 257, 7005, 11, 411, 294, 257, 45216, 2533, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.222930662093624, "compression_ratio": 1.6754716981132076, "no_speech_prob": 2.0577806481014704e-06}, {"id": 166, "seek": 107816, "start": 1098.96, "end": 1104.72, "text": " Maybe I didn't- PJ is a group. It's a pool. So imagine this is a pool, like in a convolution net,", "tokens": [50364, 400, 456, 366, 867, 2098, 281, 360, 341, 13, 1485, 636, 307, 281, 637, 685, 507, 11, 472, 636, 307, 807, 5127, 5658, 1339, 50644, 50644, 22083, 264, 2026, 11, 300, 311, 264, 316, 20442, 13, 400, 456, 366, 661, 2098, 300, 321, 603, 751, 466, 294, 257, 3456, 13, 50892, 50892, 14159, 291, 645, 1417, 949, 466, 264, 1594, 637, 685, 507, 11, 291, 645, 2408, 2810, 445, 257, 1326, 10938, 11, 51188, 51188, 411, 257, 1326, 8186, 279, 1951, 257, 1359, 3613, 13, 708, 307, 300, 30549, 30, 51404, 51404, 2704, 286, 994, 380, 12, 30549, 307, 257, 1594, 13, 467, 311, 257, 7005, 13, 407, 3811, 341, 307, 257, 7005, 11, 411, 294, 257, 45216, 2533, 11, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.222930662093624, "compression_ratio": 1.6754716981132076, "no_speech_prob": 2.0577806481014704e-06}, {"id": 167, "seek": 110472, "start": 1104.72, "end": 1110.0, "text": " but the pool instead of pulling just over space, it pulls over features as well.", "tokens": [50364, 457, 264, 7005, 2602, 295, 8407, 445, 670, 1901, 11, 309, 16982, 670, 4122, 382, 731, 13, 50628, 50628, 1171, 257, 4498, 4582, 3209, 11, 309, 445, 16982, 670, 6677, 295, 710, 11, 445, 4122, 13, 50904, 50948, 407, 30549, 307, 411, 257, 992, 295, 8186, 279, 30, 30549, 307, 257, 25993, 295, 43840, 295, 710, 11, 51280, 51280, 295, 6677, 295, 710, 13, 865, 13, 1033, 11, 3231, 13, 51416, 51516], "temperature": 0.0, "avg_logprob": -0.3285830987466348, "compression_ratio": 1.5465116279069768, "no_speech_prob": 2.584965477581136e-05}, {"id": 168, "seek": 110472, "start": 1110.0, "end": 1115.52, "text": " For a fully connected network, it just pulls over components of z, just features.", "tokens": [50364, 457, 264, 7005, 2602, 295, 8407, 445, 670, 1901, 11, 309, 16982, 670, 4122, 382, 731, 13, 50628, 50628, 1171, 257, 4498, 4582, 3209, 11, 309, 445, 16982, 670, 6677, 295, 710, 11, 445, 4122, 13, 50904, 50948, 407, 30549, 307, 411, 257, 992, 295, 8186, 279, 30, 30549, 307, 257, 25993, 295, 43840, 295, 710, 11, 51280, 51280, 295, 6677, 295, 710, 13, 865, 13, 1033, 11, 3231, 13, 51416, 51516], "temperature": 0.0, "avg_logprob": -0.3285830987466348, "compression_ratio": 1.5465116279069768, "no_speech_prob": 2.584965477581136e-05}, {"id": 169, "seek": 110472, "start": 1116.4, "end": 1123.04, "text": " So PJ is like a set of indexes? PJ is a subset of indices of z,", "tokens": [50364, 457, 264, 7005, 2602, 295, 8407, 445, 670, 1901, 11, 309, 16982, 670, 4122, 382, 731, 13, 50628, 50628, 1171, 257, 4498, 4582, 3209, 11, 309, 445, 16982, 670, 6677, 295, 710, 11, 445, 4122, 13, 50904, 50948, 407, 30549, 307, 411, 257, 992, 295, 8186, 279, 30, 30549, 307, 257, 25993, 295, 43840, 295, 710, 11, 51280, 51280, 295, 6677, 295, 710, 13, 865, 13, 1033, 11, 3231, 13, 51416, 51516], "temperature": 0.0, "avg_logprob": -0.3285830987466348, "compression_ratio": 1.5465116279069768, "no_speech_prob": 2.584965477581136e-05}, {"id": 170, "seek": 110472, "start": 1123.04, "end": 1125.76, "text": " of components of z. Yeah. Okay, thanks.", "tokens": [50364, 457, 264, 7005, 2602, 295, 8407, 445, 670, 1901, 11, 309, 16982, 670, 4122, 382, 731, 13, 50628, 50628, 1171, 257, 4498, 4582, 3209, 11, 309, 445, 16982, 670, 6677, 295, 710, 11, 445, 4122, 13, 50904, 50948, 407, 30549, 307, 411, 257, 992, 295, 8186, 279, 30, 30549, 307, 257, 25993, 295, 43840, 295, 710, 11, 51280, 51280, 295, 6677, 295, 710, 13, 865, 13, 1033, 11, 3231, 13, 51416, 51516], "temperature": 0.0, "avg_logprob": -0.3285830987466348, "compression_ratio": 1.5465116279069768, "no_speech_prob": 2.584965477581136e-05}, {"id": 171, "seek": 112576, "start": 1125.76, "end": 1134.0, "text": " Right, so here PJ is a group of six components of z that happen to be neighbors in this topology.", "tokens": [50364, 1779, 11, 370, 510, 30549, 307, 257, 1594, 295, 2309, 6677, 295, 710, 300, 1051, 281, 312, 12512, 294, 341, 1192, 1793, 13, 50776, 50888, 400, 300, 311, 472, 430, 293, 264, 958, 430, 307, 257, 2531, 3732, 11, 2309, 538, 2309, 3732, 11, 51200, 51200, 18892, 538, 1045, 18668, 281, 264, 1411, 11, 281, 264, 1192, 420, 2767, 13, 1033, 11, 1392, 13, 51480, 51480, 400, 300, 311, 558, 11, 1192, 11, 2767, 13, 1033, 11, 3231, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.3431448879012142, "compression_ratio": 1.5921787709497206, "no_speech_prob": 4.8316076572518796e-05}, {"id": 172, "seek": 112576, "start": 1136.24, "end": 1142.48, "text": " And that's one P and the next P is a similar square, six by six square,", "tokens": [50364, 1779, 11, 370, 510, 30549, 307, 257, 1594, 295, 2309, 6677, 295, 710, 300, 1051, 281, 312, 12512, 294, 341, 1192, 1793, 13, 50776, 50888, 400, 300, 311, 472, 430, 293, 264, 958, 430, 307, 257, 2531, 3732, 11, 2309, 538, 2309, 3732, 11, 51200, 51200, 18892, 538, 1045, 18668, 281, 264, 1411, 11, 281, 264, 1192, 420, 2767, 13, 1033, 11, 1392, 13, 51480, 51480, 400, 300, 311, 558, 11, 1192, 11, 2767, 13, 1033, 11, 3231, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.3431448879012142, "compression_ratio": 1.5921787709497206, "no_speech_prob": 4.8316076572518796e-05}, {"id": 173, "seek": 112576, "start": 1142.48, "end": 1148.08, "text": " shifted by three pixels to the left, to the top or bottom. Okay, okay.", "tokens": [50364, 1779, 11, 370, 510, 30549, 307, 257, 1594, 295, 2309, 6677, 295, 710, 300, 1051, 281, 312, 12512, 294, 341, 1192, 1793, 13, 50776, 50888, 400, 300, 311, 472, 430, 293, 264, 958, 430, 307, 257, 2531, 3732, 11, 2309, 538, 2309, 3732, 11, 51200, 51200, 18892, 538, 1045, 18668, 281, 264, 1411, 11, 281, 264, 1192, 420, 2767, 13, 1033, 11, 1392, 13, 51480, 51480, 400, 300, 311, 558, 11, 1192, 11, 2767, 13, 1033, 11, 3231, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.3431448879012142, "compression_ratio": 1.5921787709497206, "no_speech_prob": 4.8316076572518796e-05}, {"id": 174, "seek": 112576, "start": 1148.08, "end": 1150.24, "text": " And that's right, top, bottom. Okay, thanks.", "tokens": [50364, 1779, 11, 370, 510, 30549, 307, 257, 1594, 295, 2309, 6677, 295, 710, 300, 1051, 281, 312, 12512, 294, 341, 1192, 1793, 13, 50776, 50888, 400, 300, 311, 472, 430, 293, 264, 958, 430, 307, 257, 2531, 3732, 11, 2309, 538, 2309, 3732, 11, 51200, 51200, 18892, 538, 1045, 18668, 281, 264, 1411, 11, 281, 264, 1192, 420, 2767, 13, 1033, 11, 1392, 13, 51480, 51480, 400, 300, 311, 558, 11, 1192, 11, 2767, 13, 1033, 11, 3231, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.3431448879012142, "compression_ratio": 1.5921787709497206, "no_speech_prob": 4.8316076572518796e-05}, {"id": 175, "seek": 115024, "start": 1150.24, "end": 1160.08, "text": " So the overlapping between the groups is what kind of represents this topology, if you want.", "tokens": [50364, 407, 264, 33535, 1296, 264, 3935, 307, 437, 733, 295, 8855, 341, 1192, 1793, 11, 498, 291, 528, 13, 50856, 51212, 1033, 11, 370, 294, 341, 5120, 11, 291, 458, 11, 307, 588, 2531, 281, 264, 472, 321, 445, 2825, 466, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.34162326481031335, "compression_ratio": 1.3533834586466165, "no_speech_prob": 4.751464803121053e-05}, {"id": 176, "seek": 115024, "start": 1167.2, "end": 1177.04, "text": " Okay, so in this experiment, you know, is very similar to the one we just talked about.", "tokens": [50364, 407, 264, 33535, 1296, 264, 3935, 307, 437, 733, 295, 8855, 341, 1192, 1793, 11, 498, 291, 528, 13, 50856, 51212, 1033, 11, 370, 294, 341, 5120, 11, 291, 458, 11, 307, 588, 2531, 281, 264, 472, 321, 445, 2825, 466, 13, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.34162326481031335, "compression_ratio": 1.3533834586466165, "no_speech_prob": 4.751464803121053e-05}, {"id": 177, "seek": 117704, "start": 1177.04, "end": 1182.32, "text": " Except here, we have local connections. So we have an input. It's a two-dimensional input here.", "tokens": [50364, 16192, 510, 11, 321, 362, 2654, 9271, 13, 407, 321, 362, 364, 4846, 13, 467, 311, 257, 732, 12, 18759, 4846, 510, 13, 50628, 50628, 492, 733, 295, 787, 2906, 257, 502, 35, 3037, 295, 309, 13, 400, 321, 362, 6815, 11, 6264, 3866, 6815, 412, 472, 4914, 51084, 51156, 1237, 412, 257, 2522, 295, 264, 4846, 11, 733, 295, 257, 2654, 9972, 322, 264, 4846, 13, 51304, 51344, 400, 550, 729, 6352, 295, 6815, 366, 733, 295, 46365, 3866, 1413, 11, 457, 456, 311, 572, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.23512390984429254, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.5065044610528275e-05}, {"id": 178, "seek": 117704, "start": 1182.32, "end": 1191.44, "text": " We kind of only represent a 1D version of it. And we have units, possibly multiple units at one location", "tokens": [50364, 16192, 510, 11, 321, 362, 2654, 9271, 13, 407, 321, 362, 364, 4846, 13, 467, 311, 257, 732, 12, 18759, 4846, 510, 13, 50628, 50628, 492, 733, 295, 787, 2906, 257, 502, 35, 3037, 295, 309, 13, 400, 321, 362, 6815, 11, 6264, 3866, 6815, 412, 472, 4914, 51084, 51156, 1237, 412, 257, 2522, 295, 264, 4846, 11, 733, 295, 257, 2654, 9972, 322, 264, 4846, 13, 51304, 51344, 400, 550, 729, 6352, 295, 6815, 366, 733, 295, 46365, 3866, 1413, 11, 457, 456, 311, 572, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.23512390984429254, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.5065044610528275e-05}, {"id": 179, "seek": 117704, "start": 1192.8799999999999, "end": 1195.84, "text": " looking at a piece of the input, kind of a local patch on the input.", "tokens": [50364, 16192, 510, 11, 321, 362, 2654, 9271, 13, 407, 321, 362, 364, 4846, 13, 467, 311, 257, 732, 12, 18759, 4846, 510, 13, 50628, 50628, 492, 733, 295, 787, 2906, 257, 502, 35, 3037, 295, 309, 13, 400, 321, 362, 6815, 11, 6264, 3866, 6815, 412, 472, 4914, 51084, 51156, 1237, 412, 257, 2522, 295, 264, 4846, 11, 733, 295, 257, 2654, 9972, 322, 264, 4846, 13, 51304, 51344, 400, 550, 729, 6352, 295, 6815, 366, 733, 295, 46365, 3866, 1413, 11, 457, 456, 311, 572, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.23512390984429254, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.5065044610528275e-05}, {"id": 180, "seek": 117704, "start": 1196.6399999999999, "end": 1203.92, "text": " And then those sets of units are kind of replicated multiple times, but there's no", "tokens": [50364, 16192, 510, 11, 321, 362, 2654, 9271, 13, 407, 321, 362, 364, 4846, 13, 467, 311, 257, 732, 12, 18759, 4846, 510, 13, 50628, 50628, 492, 733, 295, 787, 2906, 257, 502, 35, 3037, 295, 309, 13, 400, 321, 362, 6815, 11, 6264, 3866, 6815, 412, 472, 4914, 51084, 51156, 1237, 412, 257, 2522, 295, 264, 4846, 11, 733, 295, 257, 2654, 9972, 322, 264, 4846, 13, 51304, 51344, 400, 550, 729, 6352, 295, 6815, 366, 733, 295, 46365, 3866, 1413, 11, 457, 456, 311, 572, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.23512390984429254, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.5065044610528275e-05}, {"id": 181, "seek": 120392, "start": 1203.92, "end": 1210.24, "text": " shared weights. So the units, these kind of units everywhere on the input, but they", "tokens": [50364, 5507, 17443, 13, 407, 264, 6815, 11, 613, 733, 295, 6815, 5315, 322, 264, 4846, 11, 457, 436, 50680, 50680, 264, 17443, 366, 406, 5507, 13, 1033, 11, 436, 434, 445, 16143, 4582, 13, 50820, 50936, 407, 286, 2041, 286, 478, 406, 1596, 3701, 264, 4787, 3410, 295, 264, 4111, 7005, 278, 13, 51416, 51476, 286, 914, 11, 498, 286, 519, 466, 309, 294, 2115, 295, 411, 7005, 278, 300, 321, 1143, 294, 45216, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.272320880165583, "compression_ratio": 1.5392156862745099, "no_speech_prob": 5.014644102629973e-06}, {"id": 182, "seek": 120392, "start": 1210.24, "end": 1213.04, "text": " the weights are not shared. Okay, they're just locally connected.", "tokens": [50364, 5507, 17443, 13, 407, 264, 6815, 11, 613, 733, 295, 6815, 5315, 322, 264, 4846, 11, 457, 436, 50680, 50680, 264, 17443, 366, 406, 5507, 13, 1033, 11, 436, 434, 445, 16143, 4582, 13, 50820, 50936, 407, 286, 2041, 286, 478, 406, 1596, 3701, 264, 4787, 3410, 295, 264, 4111, 7005, 278, 13, 51416, 51476, 286, 914, 11, 498, 286, 519, 466, 309, 294, 2115, 295, 411, 7005, 278, 300, 321, 1143, 294, 45216, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.272320880165583, "compression_ratio": 1.5392156862745099, "no_speech_prob": 5.014644102629973e-06}, {"id": 183, "seek": 120392, "start": 1215.3600000000001, "end": 1224.96, "text": " So I guess I'm not quite understanding the overall concept of the feature pooling.", "tokens": [50364, 5507, 17443, 13, 407, 264, 6815, 11, 613, 733, 295, 6815, 5315, 322, 264, 4846, 11, 457, 436, 50680, 50680, 264, 17443, 366, 406, 5507, 13, 1033, 11, 436, 434, 445, 16143, 4582, 13, 50820, 50936, 407, 286, 2041, 286, 478, 406, 1596, 3701, 264, 4787, 3410, 295, 264, 4111, 7005, 278, 13, 51416, 51476, 286, 914, 11, 498, 286, 519, 466, 309, 294, 2115, 295, 411, 7005, 278, 300, 321, 1143, 294, 45216, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.272320880165583, "compression_ratio": 1.5392156862745099, "no_speech_prob": 5.014644102629973e-06}, {"id": 184, "seek": 120392, "start": 1226.16, "end": 1231.2, "text": " I mean, if I think about it in terms of like pooling that we used in convolution,", "tokens": [50364, 5507, 17443, 13, 407, 264, 6815, 11, 613, 733, 295, 6815, 5315, 322, 264, 4846, 11, 457, 436, 50680, 50680, 264, 17443, 366, 406, 5507, 13, 1033, 11, 436, 434, 445, 16143, 4582, 13, 50820, 50936, 407, 286, 2041, 286, 478, 406, 1596, 3701, 264, 4787, 3410, 295, 264, 4111, 7005, 278, 13, 51416, 51476, 286, 914, 11, 498, 286, 519, 466, 309, 294, 2115, 295, 411, 7005, 278, 300, 321, 1143, 294, 45216, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.272320880165583, "compression_ratio": 1.5392156862745099, "no_speech_prob": 5.014644102629973e-06}, {"id": 185, "seek": 123120, "start": 1231.2, "end": 1240.32, "text": " networks, then it's straightforward. But I don't really understand how feature pooling works.", "tokens": [50364, 9590, 11, 550, 309, 311, 15325, 13, 583, 286, 500, 380, 534, 1223, 577, 4111, 7005, 278, 1985, 13, 50820, 50900, 1033, 11, 718, 385, 2642, 257, 3036, 13, 2704, 300, 603, 312, 1850, 13, 1033, 11, 370, 291, 722, 365, 364, 4846, 8062, 11, 51260, 51296, 17207, 538, 257, 8141, 420, 1320, 309, 807, 512, 1333, 295, 2058, 19866, 13, 51648, 51776], "temperature": 0.0, "avg_logprob": -0.341259032029372, "compression_ratio": 1.3867403314917126, "no_speech_prob": 1.8738006701823906e-06}, {"id": 186, "seek": 123120, "start": 1241.92, "end": 1249.1200000000001, "text": " Okay, let me draw a picture. Maybe that'll be clear. Okay, so you start with an input vector,", "tokens": [50364, 9590, 11, 550, 309, 311, 15325, 13, 583, 286, 500, 380, 534, 1223, 577, 4111, 7005, 278, 1985, 13, 50820, 50900, 1033, 11, 718, 385, 2642, 257, 3036, 13, 2704, 300, 603, 312, 1850, 13, 1033, 11, 370, 291, 722, 365, 364, 4846, 8062, 11, 51260, 51296, 17207, 538, 257, 8141, 420, 1320, 309, 807, 512, 1333, 295, 2058, 19866, 13, 51648, 51776], "temperature": 0.0, "avg_logprob": -0.341259032029372, "compression_ratio": 1.3867403314917126, "no_speech_prob": 1.8738006701823906e-06}, {"id": 187, "seek": 123120, "start": 1249.8400000000001, "end": 1256.88, "text": " multiplied by a matrix or pass it through some sort of encoder.", "tokens": [50364, 9590, 11, 550, 309, 311, 15325, 13, 583, 286, 500, 380, 534, 1223, 577, 4111, 7005, 278, 1985, 13, 50820, 50900, 1033, 11, 718, 385, 2642, 257, 3036, 13, 2704, 300, 603, 312, 1850, 13, 1033, 11, 370, 291, 722, 365, 364, 4846, 8062, 11, 51260, 51296, 17207, 538, 257, 8141, 420, 1320, 309, 807, 512, 1333, 295, 2058, 19866, 13, 51648, 51776], "temperature": 0.0, "avg_logprob": -0.341259032029372, "compression_ratio": 1.3867403314917126, "no_speech_prob": 1.8738006701823906e-06}, {"id": 188, "seek": 125688, "start": 1256.88, "end": 1265.3600000000001, "text": " Which may have radios and whatever, or multiple matrices inside. Okay, maybe multiple layers.", "tokens": [50364, 3013, 815, 362, 2843, 2717, 293, 2035, 11, 420, 3866, 32284, 1854, 13, 1033, 11, 1310, 3866, 7914, 13, 50788, 50816, 400, 291, 483, 257, 4111, 8062, 13, 1033, 11, 370, 718, 311, 818, 300, 710, 13, 400, 586, 291, 360, 7005, 278, 4476, 13, 407, 51356, 51356, 291, 9845, 341, 666, 3935, 13, 682, 341, 1389, 11, 436, 366, 2107, 12, 3570, 15639, 13, 400, 291, 14722, 264, 51636, 51708], "temperature": 0.0, "avg_logprob": -0.2833755963469205, "compression_ratio": 1.4919786096256684, "no_speech_prob": 1.17233889795898e-06}, {"id": 189, "seek": 125688, "start": 1265.92, "end": 1276.72, "text": " And you get a feature vector. Okay, so let's call that z. And now you do pooling essentially. So", "tokens": [50364, 3013, 815, 362, 2843, 2717, 293, 2035, 11, 420, 3866, 32284, 1854, 13, 1033, 11, 1310, 3866, 7914, 13, 50788, 50816, 400, 291, 483, 257, 4111, 8062, 13, 1033, 11, 370, 718, 311, 818, 300, 710, 13, 400, 586, 291, 360, 7005, 278, 4476, 13, 407, 51356, 51356, 291, 9845, 341, 666, 3935, 13, 682, 341, 1389, 11, 436, 366, 2107, 12, 3570, 15639, 13, 400, 291, 14722, 264, 51636, 51708], "temperature": 0.0, "avg_logprob": -0.2833755963469205, "compression_ratio": 1.4919786096256684, "no_speech_prob": 1.17233889795898e-06}, {"id": 190, "seek": 125688, "start": 1276.72, "end": 1282.3200000000002, "text": " you divide this into groups. In this case, they are non-overlapping. And you compute the", "tokens": [50364, 3013, 815, 362, 2843, 2717, 293, 2035, 11, 420, 3866, 32284, 1854, 13, 1033, 11, 1310, 3866, 7914, 13, 50788, 50816, 400, 291, 483, 257, 4111, 8062, 13, 1033, 11, 370, 718, 311, 818, 300, 710, 13, 400, 586, 291, 360, 7005, 278, 4476, 13, 407, 51356, 51356, 291, 9845, 341, 666, 3935, 13, 682, 341, 1389, 11, 436, 366, 2107, 12, 3570, 15639, 13, 400, 291, 14722, 264, 51636, 51708], "temperature": 0.0, "avg_logprob": -0.2833755963469205, "compression_ratio": 1.4919786096256684, "no_speech_prob": 1.17233889795898e-06}, {"id": 191, "seek": 128232, "start": 1282.32, "end": 1288.24, "text": " within one of those groups, you compute the square root of the sum of the squares", "tokens": [50364, 1951, 472, 295, 729, 3935, 11, 291, 14722, 264, 3732, 5593, 295, 264, 2408, 295, 264, 19368, 50660, 50700, 295, 729, 710, 271, 689, 741, 5784, 281, 264, 1594, 11, 264, 7005, 13, 1033, 11, 309, 311, 1219, 280, 570, 309, 311, 257, 7005, 13, 51056, 51204, 1033, 11, 293, 291, 360, 341, 337, 439, 264, 3935, 13, 1057, 558, 11, 370, 437, 291, 483, 510, 11, 341, 5598, 510, 51520, 51520, 1542, 588, 709, 411, 264, 5598, 295, 257, 7005, 278, 322, 257, 3820, 13, 407, 291, 483, 264, 5598, 295, 257, 7005, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.39577919545799795, "compression_ratio": 1.7860696517412935, "no_speech_prob": 6.747761290171184e-06}, {"id": 192, "seek": 128232, "start": 1289.04, "end": 1296.1599999999999, "text": " of those zis where i belong to the group, the pool. Okay, it's called p because it's a pool.", "tokens": [50364, 1951, 472, 295, 729, 3935, 11, 291, 14722, 264, 3732, 5593, 295, 264, 2408, 295, 264, 19368, 50660, 50700, 295, 729, 710, 271, 689, 741, 5784, 281, 264, 1594, 11, 264, 7005, 13, 1033, 11, 309, 311, 1219, 280, 570, 309, 311, 257, 7005, 13, 51056, 51204, 1033, 11, 293, 291, 360, 341, 337, 439, 264, 3935, 13, 1057, 558, 11, 370, 437, 291, 483, 510, 11, 341, 5598, 510, 51520, 51520, 1542, 588, 709, 411, 264, 5598, 295, 257, 7005, 278, 322, 257, 3820, 13, 407, 291, 483, 264, 5598, 295, 257, 7005, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.39577919545799795, "compression_ratio": 1.7860696517412935, "no_speech_prob": 6.747761290171184e-06}, {"id": 193, "seek": 128232, "start": 1299.12, "end": 1305.4399999999998, "text": " Okay, and you do this for all the groups. All right, so what you get here, this output here", "tokens": [50364, 1951, 472, 295, 729, 3935, 11, 291, 14722, 264, 3732, 5593, 295, 264, 2408, 295, 264, 19368, 50660, 50700, 295, 729, 710, 271, 689, 741, 5784, 281, 264, 1594, 11, 264, 7005, 13, 1033, 11, 309, 311, 1219, 280, 570, 309, 311, 257, 7005, 13, 51056, 51204, 1033, 11, 293, 291, 360, 341, 337, 439, 264, 3935, 13, 1057, 558, 11, 370, 437, 291, 483, 510, 11, 341, 5598, 510, 51520, 51520, 1542, 588, 709, 411, 264, 5598, 295, 257, 7005, 278, 322, 257, 3820, 13, 407, 291, 483, 264, 5598, 295, 257, 7005, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.39577919545799795, "compression_ratio": 1.7860696517412935, "no_speech_prob": 6.747761290171184e-06}, {"id": 194, "seek": 128232, "start": 1305.4399999999998, "end": 1310.48, "text": " looks very much like the output of a pooling on a computer. So you get the output of a pool.", "tokens": [50364, 1951, 472, 295, 729, 3935, 11, 291, 14722, 264, 3732, 5593, 295, 264, 2408, 295, 264, 19368, 50660, 50700, 295, 729, 710, 271, 689, 741, 5784, 281, 264, 1594, 11, 264, 7005, 13, 1033, 11, 309, 311, 1219, 280, 570, 309, 311, 257, 7005, 13, 51056, 51204, 1033, 11, 293, 291, 360, 341, 337, 439, 264, 3935, 13, 1057, 558, 11, 370, 437, 291, 483, 510, 11, 341, 5598, 510, 51520, 51520, 1542, 588, 709, 411, 264, 5598, 295, 257, 7005, 278, 322, 257, 3820, 13, 407, 291, 483, 264, 5598, 295, 257, 7005, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.39577919545799795, "compression_ratio": 1.7860696517412935, "no_speech_prob": 6.747761290171184e-06}, {"id": 195, "seek": 131048, "start": 1310.48, "end": 1314.8, "text": " Looks very much like the output of a pooling layer in a convolutional net. This is not a", "tokens": [50364, 10027, 588, 709, 411, 264, 5598, 295, 257, 7005, 278, 4583, 294, 257, 45216, 304, 2533, 13, 639, 307, 406, 257, 50580, 50580, 45216, 304, 2533, 13, 1033, 11, 309, 311, 257, 4498, 4582, 3209, 510, 13, 583, 264, 1874, 307, 264, 912, 13, 50884, 51012, 400, 300, 311, 428, 3890, 6545, 13, 823, 11, 294, 264, 1365, 286, 445, 4712, 11, 291, 747, 264, 710, 11, 293, 341, 307, 437, 51336, 51336, 291, 2845, 281, 257, 979, 19866, 8141, 490, 597, 291, 31499, 264, 4846, 13, 1033, 11, 370, 341, 307, 288, 11, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.10280986708037708, "compression_ratio": 1.5807860262008733, "no_speech_prob": 2.2958195131650427e-06}, {"id": 196, "seek": 131048, "start": 1314.8, "end": 1320.88, "text": " convolutional net. Okay, it's a fully connected network here. But the result is the same.", "tokens": [50364, 10027, 588, 709, 411, 264, 5598, 295, 257, 7005, 278, 4583, 294, 257, 45216, 304, 2533, 13, 639, 307, 406, 257, 50580, 50580, 45216, 304, 2533, 13, 1033, 11, 309, 311, 257, 4498, 4582, 3209, 510, 13, 583, 264, 1874, 307, 264, 912, 13, 50884, 51012, 400, 300, 311, 428, 3890, 6545, 13, 823, 11, 294, 264, 1365, 286, 445, 4712, 11, 291, 747, 264, 710, 11, 293, 341, 307, 437, 51336, 51336, 291, 2845, 281, 257, 979, 19866, 8141, 490, 597, 291, 31499, 264, 4846, 13, 1033, 11, 370, 341, 307, 288, 11, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.10280986708037708, "compression_ratio": 1.5807860262008733, "no_speech_prob": 2.2958195131650427e-06}, {"id": 197, "seek": 131048, "start": 1323.44, "end": 1329.92, "text": " And that's your regularizer. Now, in the example I just showed, you take the z, and this is what", "tokens": [50364, 10027, 588, 709, 411, 264, 5598, 295, 257, 7005, 278, 4583, 294, 257, 45216, 304, 2533, 13, 639, 307, 406, 257, 50580, 50580, 45216, 304, 2533, 13, 1033, 11, 309, 311, 257, 4498, 4582, 3209, 510, 13, 583, 264, 1874, 307, 264, 912, 13, 50884, 51012, 400, 300, 311, 428, 3890, 6545, 13, 823, 11, 294, 264, 1365, 286, 445, 4712, 11, 291, 747, 264, 710, 11, 293, 341, 307, 437, 51336, 51336, 291, 2845, 281, 257, 979, 19866, 8141, 490, 597, 291, 31499, 264, 4846, 13, 1033, 11, 370, 341, 307, 288, 11, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.10280986708037708, "compression_ratio": 1.5807860262008733, "no_speech_prob": 2.2958195131650427e-06}, {"id": 198, "seek": 131048, "start": 1329.92, "end": 1338.56, "text": " you send to a decoder matrix from which you reconstruct the input. Okay, so this is y,", "tokens": [50364, 10027, 588, 709, 411, 264, 5598, 295, 257, 7005, 278, 4583, 294, 257, 45216, 304, 2533, 13, 639, 307, 406, 257, 50580, 50580, 45216, 304, 2533, 13, 1033, 11, 309, 311, 257, 4498, 4582, 3209, 510, 13, 583, 264, 1874, 307, 264, 912, 13, 50884, 51012, 400, 300, 311, 428, 3890, 6545, 13, 823, 11, 294, 264, 1365, 286, 445, 4712, 11, 291, 747, 264, 710, 11, 293, 341, 307, 437, 51336, 51336, 291, 2845, 281, 257, 979, 19866, 8141, 490, 597, 291, 31499, 264, 4846, 13, 1033, 11, 370, 341, 307, 288, 11, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.10280986708037708, "compression_ratio": 1.5807860262008733, "no_speech_prob": 2.2958195131650427e-06}, {"id": 199, "seek": 133856, "start": 1338.56, "end": 1346.48, "text": " this is y bar. That's a prediction for the reconstruction. And this pooled layer here", "tokens": [50364, 341, 307, 288, 2159, 13, 663, 311, 257, 17630, 337, 264, 31565, 13, 400, 341, 7005, 292, 4583, 510, 50760, 50760, 307, 787, 1143, 281, 14722, 264, 3890, 6545, 13, 467, 311, 406, 767, 1143, 337, 31565, 13, 509, 31499, 51156, 51156, 490, 264, 637, 11668, 3089, 3838, 13, 583, 309, 1542, 588, 709, 411, 257, 7005, 278, 4583, 13, 823, 11, 498, 341, 645, 51432, 51632], "temperature": 0.0, "avg_logprob": -0.1357537006986314, "compression_ratio": 1.516304347826087, "no_speech_prob": 8.529952538083307e-06}, {"id": 200, "seek": 133856, "start": 1346.48, "end": 1354.3999999999999, "text": " is only used to compute the regularizer. It's not actually used for reconstruction. You reconstruct", "tokens": [50364, 341, 307, 288, 2159, 13, 663, 311, 257, 17630, 337, 264, 31565, 13, 400, 341, 7005, 292, 4583, 510, 50760, 50760, 307, 787, 1143, 281, 14722, 264, 3890, 6545, 13, 467, 311, 406, 767, 1143, 337, 31565, 13, 509, 31499, 51156, 51156, 490, 264, 637, 11668, 3089, 3838, 13, 583, 309, 1542, 588, 709, 411, 257, 7005, 278, 4583, 13, 823, 11, 498, 341, 645, 51432, 51632], "temperature": 0.0, "avg_logprob": -0.1357537006986314, "compression_ratio": 1.516304347826087, "no_speech_prob": 8.529952538083307e-06}, {"id": 201, "seek": 133856, "start": 1354.3999999999999, "end": 1359.9199999999998, "text": " from the sparse code directly. But it looks very much like a pooling layer. Now, if this were", "tokens": [50364, 341, 307, 288, 2159, 13, 663, 311, 257, 17630, 337, 264, 31565, 13, 400, 341, 7005, 292, 4583, 510, 50760, 50760, 307, 787, 1143, 281, 14722, 264, 3890, 6545, 13, 467, 311, 406, 767, 1143, 337, 31565, 13, 509, 31499, 51156, 51156, 490, 264, 637, 11668, 3089, 3838, 13, 583, 309, 1542, 588, 709, 411, 257, 7005, 278, 4583, 13, 823, 11, 498, 341, 645, 51432, 51632], "temperature": 0.0, "avg_logprob": -0.1357537006986314, "compression_ratio": 1.516304347826087, "no_speech_prob": 8.529952538083307e-06}, {"id": 202, "seek": 135992, "start": 1359.92, "end": 1368.88, "text": " a convolutional net, then that dimension or feature here would be", "tokens": [50364, 257, 45216, 304, 2533, 11, 550, 300, 10139, 420, 4111, 510, 576, 312, 50812, 50956, 4122, 11, 457, 291, 576, 362, 3866, 4111, 11317, 13, 1033, 11, 370, 286, 478, 13460, 264, 4111, 10139, 51308, 51308, 28450, 13, 1396, 264, 2058, 19866, 576, 360, 3866, 3754, 15892, 293, 576, 611, 8460, 3866, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.2562673602785383, "compression_ratio": 1.664516129032258, "no_speech_prob": 3.4462880194041645e-06}, {"id": 203, "seek": 135992, "start": 1371.76, "end": 1378.8000000000002, "text": " features, but you would have multiple feature maps. Okay, so I'm representing the feature dimension", "tokens": [50364, 257, 45216, 304, 2533, 11, 550, 300, 10139, 420, 4111, 510, 576, 312, 50812, 50956, 4122, 11, 457, 291, 576, 362, 3866, 4111, 11317, 13, 1033, 11, 370, 286, 478, 13460, 264, 4111, 10139, 51308, 51308, 28450, 13, 1396, 264, 2058, 19866, 576, 360, 3866, 3754, 15892, 293, 576, 611, 8460, 3866, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.2562673602785383, "compression_ratio": 1.664516129032258, "no_speech_prob": 3.4462880194041645e-06}, {"id": 204, "seek": 135992, "start": 1378.8000000000002, "end": 1384.0800000000002, "text": " vertically. Then the encoder would do multiple convolutions and would also generate multiple", "tokens": [50364, 257, 45216, 304, 2533, 11, 550, 300, 10139, 420, 4111, 510, 576, 312, 50812, 50956, 4122, 11, 457, 291, 576, 362, 3866, 4111, 11317, 13, 1033, 11, 370, 286, 478, 13460, 264, 4111, 10139, 51308, 51308, 28450, 13, 1396, 264, 2058, 19866, 576, 360, 3866, 3754, 15892, 293, 576, 611, 8460, 3866, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.2562673602785383, "compression_ratio": 1.664516129032258, "no_speech_prob": 3.4462880194041645e-06}, {"id": 205, "seek": 138408, "start": 1384.08, "end": 1392.1599999999999, "text": " convolutions and would also generate multiple feature maps, perhaps a larger number.", "tokens": [50364, 3754, 15892, 293, 576, 611, 8460, 3866, 4111, 11317, 11, 4317, 257, 4833, 1230, 13, 50768, 50924, 400, 550, 264, 733, 295, 7005, 278, 321, 576, 360, 510, 51008, 51184, 307, 257, 7005, 278, 689, 11, 370, 1184, 11, 934, 7005, 278, 11, 321, 576, 747, 257, 4910, 670, 1901, 382, 731, 382, 670, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.1751948882793558, "compression_ratio": 1.5342465753424657, "no_speech_prob": 1.6024284832383273e-06}, {"id": 206, "seek": 138408, "start": 1395.28, "end": 1396.96, "text": " And then the kind of pooling we would do here", "tokens": [50364, 3754, 15892, 293, 576, 611, 8460, 3866, 4111, 11317, 11, 4317, 257, 4833, 1230, 13, 50768, 50924, 400, 550, 264, 733, 295, 7005, 278, 321, 576, 360, 510, 51008, 51184, 307, 257, 7005, 278, 689, 11, 370, 1184, 11, 934, 7005, 278, 11, 321, 576, 747, 257, 4910, 670, 1901, 382, 731, 382, 670, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.1751948882793558, "compression_ratio": 1.5342465753424657, "no_speech_prob": 1.6024284832383273e-06}, {"id": 207, "seek": 138408, "start": 1400.48, "end": 1412.96, "text": " is a pooling where, so each, after pooling, we would take a window over space as well as over", "tokens": [50364, 3754, 15892, 293, 576, 611, 8460, 3866, 4111, 11317, 11, 4317, 257, 4833, 1230, 13, 50768, 50924, 400, 550, 264, 733, 295, 7005, 278, 321, 576, 360, 510, 51008, 51184, 307, 257, 7005, 278, 689, 11, 370, 1184, 11, 934, 7005, 278, 11, 321, 576, 747, 257, 4910, 670, 1901, 382, 731, 382, 670, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.1751948882793558, "compression_ratio": 1.5342465753424657, "no_speech_prob": 1.6024284832383273e-06}, {"id": 208, "seek": 141296, "start": 1412.96, "end": 1422.0, "text": " features and compute the square root of some square there. And that gives us one output in our pooling", "tokens": [50364, 4122, 293, 14722, 264, 3732, 5593, 295, 512, 3732, 456, 13, 400, 300, 2709, 505, 472, 5598, 294, 527, 7005, 278, 50816, 50856, 5598, 13, 400, 550, 321, 362, 3866, 3935, 295, 4122, 411, 341, 300, 352, 666, 819, 51096, 51168, 7005, 278, 13, 407, 309, 1177, 380, 1871, 1968, 341, 307, 45216, 304, 420, 406, 13, 682, 3754, 15892, 11, 51380, 51380, 291, 576, 7005, 670, 1901, 382, 731, 382, 4111, 2010, 13, 583, 498, 291, 500, 380, 362, 3754, 15892, 11, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.10650677516542632, "compression_ratio": 1.6981132075471699, "no_speech_prob": 2.726297452682047e-06}, {"id": 209, "seek": 141296, "start": 1422.8, "end": 1427.6000000000001, "text": " output. And then we have multiple groups of features like this that go into different", "tokens": [50364, 4122, 293, 14722, 264, 3732, 5593, 295, 512, 3732, 456, 13, 400, 300, 2709, 505, 472, 5598, 294, 527, 7005, 278, 50816, 50856, 5598, 13, 400, 550, 321, 362, 3866, 3935, 295, 4122, 411, 341, 300, 352, 666, 819, 51096, 51168, 7005, 278, 13, 407, 309, 1177, 380, 1871, 1968, 341, 307, 45216, 304, 420, 406, 13, 682, 3754, 15892, 11, 51380, 51380, 291, 576, 7005, 670, 1901, 382, 731, 382, 4111, 2010, 13, 583, 498, 291, 500, 380, 362, 3754, 15892, 11, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.10650677516542632, "compression_ratio": 1.6981132075471699, "no_speech_prob": 2.726297452682047e-06}, {"id": 210, "seek": 141296, "start": 1429.04, "end": 1433.28, "text": " pooling. So it doesn't matter whether this is convolutional or not. In convolutions,", "tokens": [50364, 4122, 293, 14722, 264, 3732, 5593, 295, 512, 3732, 456, 13, 400, 300, 2709, 505, 472, 5598, 294, 527, 7005, 278, 50816, 50856, 5598, 13, 400, 550, 321, 362, 3866, 3935, 295, 4122, 411, 341, 300, 352, 666, 819, 51096, 51168, 7005, 278, 13, 407, 309, 1177, 380, 1871, 1968, 341, 307, 45216, 304, 420, 406, 13, 682, 3754, 15892, 11, 51380, 51380, 291, 576, 7005, 670, 1901, 382, 731, 382, 4111, 2010, 13, 583, 498, 291, 500, 380, 362, 3754, 15892, 11, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.10650677516542632, "compression_ratio": 1.6981132075471699, "no_speech_prob": 2.726297452682047e-06}, {"id": 211, "seek": 141296, "start": 1433.28, "end": 1440.32, "text": " you would pool over space as well as feature type. But if you don't have convolutions,", "tokens": [50364, 4122, 293, 14722, 264, 3732, 5593, 295, 512, 3732, 456, 13, 400, 300, 2709, 505, 472, 5598, 294, 527, 7005, 278, 50816, 50856, 5598, 13, 400, 550, 321, 362, 3866, 3935, 295, 4122, 411, 341, 300, 352, 666, 819, 51096, 51168, 7005, 278, 13, 407, 309, 1177, 380, 1871, 1968, 341, 307, 45216, 304, 420, 406, 13, 682, 3754, 15892, 11, 51380, 51380, 291, 576, 7005, 670, 1901, 382, 731, 382, 4111, 2010, 13, 583, 498, 291, 500, 380, 362, 3754, 15892, 11, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.10650677516542632, "compression_ratio": 1.6981132075471699, "no_speech_prob": 2.726297452682047e-06}, {"id": 212, "seek": 144032, "start": 1440.32, "end": 1445.04, "text": " you just pool over features. And that builds invariants to whatever it is that", "tokens": [50364, 291, 445, 7005, 670, 4122, 13, 400, 300, 15182, 33270, 1719, 281, 2035, 309, 307, 300, 50600, 50660, 264, 1185, 7309, 1669, 2020, 13, 1119, 300, 1850, 30, 4402, 300, 1867, 428, 1168, 30, 51016, 51076, 865, 11, 286, 519, 309, 311, 544, 1850, 13, 1044, 291, 13, 51220, 51340, 8419, 11, 286, 362, 257, 1168, 337, 562, 291, 7472, 264, 710, 666, 3935, 293, 360, 264, 7005, 278, 11, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.10946349839906434, "compression_ratio": 1.4791666666666667, "no_speech_prob": 6.7479463723429944e-06}, {"id": 213, "seek": 144032, "start": 1446.24, "end": 1453.36, "text": " the system thinks makes sense. Is that clear? Does that answer your question?", "tokens": [50364, 291, 445, 7005, 670, 4122, 13, 400, 300, 15182, 33270, 1719, 281, 2035, 309, 307, 300, 50600, 50660, 264, 1185, 7309, 1669, 2020, 13, 1119, 300, 1850, 30, 4402, 300, 1867, 428, 1168, 30, 51016, 51076, 865, 11, 286, 519, 309, 311, 544, 1850, 13, 1044, 291, 13, 51220, 51340, 8419, 11, 286, 362, 257, 1168, 337, 562, 291, 7472, 264, 710, 666, 3935, 293, 360, 264, 7005, 278, 11, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.10946349839906434, "compression_ratio": 1.4791666666666667, "no_speech_prob": 6.7479463723429944e-06}, {"id": 214, "seek": 144032, "start": 1454.56, "end": 1457.4399999999998, "text": " Yeah, I think it's more clear. Thank you.", "tokens": [50364, 291, 445, 7005, 670, 4122, 13, 400, 300, 15182, 33270, 1719, 281, 2035, 309, 307, 300, 50600, 50660, 264, 1185, 7309, 1669, 2020, 13, 1119, 300, 1850, 30, 4402, 300, 1867, 428, 1168, 30, 51016, 51076, 865, 11, 286, 519, 309, 311, 544, 1850, 13, 1044, 291, 13, 51220, 51340, 8419, 11, 286, 362, 257, 1168, 337, 562, 291, 7472, 264, 710, 666, 3935, 293, 360, 264, 7005, 278, 11, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.10946349839906434, "compression_ratio": 1.4791666666666667, "no_speech_prob": 6.7479463723429944e-06}, {"id": 215, "seek": 144032, "start": 1459.84, "end": 1465.84, "text": " Professor, I have a question for when you split the z into groups and do the pooling,", "tokens": [50364, 291, 445, 7005, 670, 4122, 13, 400, 300, 15182, 33270, 1719, 281, 2035, 309, 307, 300, 50600, 50660, 264, 1185, 7309, 1669, 2020, 13, 1119, 300, 1850, 30, 4402, 300, 1867, 428, 1168, 30, 51016, 51076, 865, 11, 286, 519, 309, 311, 544, 1850, 13, 1044, 291, 13, 51220, 51340, 8419, 11, 286, 362, 257, 1168, 337, 562, 291, 7472, 264, 710, 666, 3935, 293, 360, 264, 7005, 278, 11, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.10946349839906434, "compression_ratio": 1.4791666666666667, "no_speech_prob": 6.7479463723429944e-06}, {"id": 216, "seek": 146584, "start": 1465.84, "end": 1473.28, "text": " do those groups overlap? Right. So in the example I showed here, they do not overlap.", "tokens": [50364, 360, 729, 3935, 19959, 30, 1779, 13, 407, 294, 264, 1365, 286, 4712, 510, 11, 436, 360, 406, 19959, 13, 50736, 50788, 583, 291, 393, 652, 552, 19959, 13, 2264, 13, 407, 718, 311, 584, 321, 362, 257, 4111, 8062, 710, 13, 51104, 51228, 286, 393, 747, 257, 7005, 510, 293, 257, 7005, 510, 293, 257, 7005, 510, 13, 400, 510, 729, 3935, 19959, 13, 51556, 51556, 400, 498, 286, 360, 341, 293, 286, 360, 1594, 637, 685, 507, 689, 613, 366, 264, 3935, 11, 437, 311, 516, 281, 1051, 307, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.12940261238499692, "compression_ratio": 1.7411167512690355, "no_speech_prob": 7.766023372823838e-06}, {"id": 217, "seek": 146584, "start": 1474.32, "end": 1480.6399999999999, "text": " But you can make them overlap. OK. So let's say we have a feature vector z.", "tokens": [50364, 360, 729, 3935, 19959, 30, 1779, 13, 407, 294, 264, 1365, 286, 4712, 510, 11, 436, 360, 406, 19959, 13, 50736, 50788, 583, 291, 393, 652, 552, 19959, 13, 2264, 13, 407, 718, 311, 584, 321, 362, 257, 4111, 8062, 710, 13, 51104, 51228, 286, 393, 747, 257, 7005, 510, 293, 257, 7005, 510, 293, 257, 7005, 510, 13, 400, 510, 729, 3935, 19959, 13, 51556, 51556, 400, 498, 286, 360, 341, 293, 286, 360, 1594, 637, 685, 507, 689, 613, 366, 264, 3935, 11, 437, 311, 516, 281, 1051, 307, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.12940261238499692, "compression_ratio": 1.7411167512690355, "no_speech_prob": 7.766023372823838e-06}, {"id": 218, "seek": 146584, "start": 1483.12, "end": 1489.6799999999998, "text": " I can take a pool here and a pool here and a pool here. And here those groups overlap.", "tokens": [50364, 360, 729, 3935, 19959, 30, 1779, 13, 407, 294, 264, 1365, 286, 4712, 510, 11, 436, 360, 406, 19959, 13, 50736, 50788, 583, 291, 393, 652, 552, 19959, 13, 2264, 13, 407, 718, 311, 584, 321, 362, 257, 4111, 8062, 710, 13, 51104, 51228, 286, 393, 747, 257, 7005, 510, 293, 257, 7005, 510, 293, 257, 7005, 510, 13, 400, 510, 729, 3935, 19959, 13, 51556, 51556, 400, 498, 286, 360, 341, 293, 286, 360, 1594, 637, 685, 507, 689, 613, 366, 264, 3935, 11, 437, 311, 516, 281, 1051, 307, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.12940261238499692, "compression_ratio": 1.7411167512690355, "no_speech_prob": 7.766023372823838e-06}, {"id": 219, "seek": 146584, "start": 1489.6799999999998, "end": 1495.36, "text": " And if I do this and I do group sparsity where these are the groups, what's going to happen is", "tokens": [50364, 360, 729, 3935, 19959, 30, 1779, 13, 407, 294, 264, 1365, 286, 4712, 510, 11, 436, 360, 406, 19959, 13, 50736, 50788, 583, 291, 393, 652, 552, 19959, 13, 2264, 13, 407, 718, 311, 584, 321, 362, 257, 4111, 8062, 710, 13, 51104, 51228, 286, 393, 747, 257, 7005, 510, 293, 257, 7005, 510, 293, 257, 7005, 510, 13, 400, 510, 729, 3935, 19959, 13, 51556, 51556, 400, 498, 286, 360, 341, 293, 286, 360, 1594, 637, 685, 507, 689, 613, 366, 264, 3935, 11, 437, 311, 516, 281, 1051, 307, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.12940261238499692, "compression_ratio": 1.7411167512690355, "no_speech_prob": 7.766023372823838e-06}, {"id": 220, "seek": 149536, "start": 1495.36, "end": 1501.9199999999998, "text": " that I'm going to have a continuously varying set of features here that vary from one end to the", "tokens": [50364, 300, 286, 478, 516, 281, 362, 257, 15684, 22984, 992, 295, 4122, 510, 300, 10559, 490, 472, 917, 281, 264, 50692, 50692, 661, 570, 264, 1185, 307, 516, 281, 528, 281, 1594, 1951, 257, 7005, 4122, 300, 366, 2531, 13, 50972, 50972, 400, 370, 570, 295, 264, 19959, 11, 309, 311, 516, 281, 15684, 10559, 552, 370, 300, 436, 1319, 5692, 51200, 51248, 670, 264, 8062, 13, 823, 11, 294, 264, 5242, 300, 286, 4712, 294, 264, 9788, 11, 2602, 295, 17608, 264, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.08416031146871633, "compression_ratio": 1.7407407407407407, "no_speech_prob": 1.6184370906557888e-05}, {"id": 221, "seek": 149536, "start": 1501.9199999999998, "end": 1507.52, "text": " other because the system is going to want to group within a pool features that are similar.", "tokens": [50364, 300, 286, 478, 516, 281, 362, 257, 15684, 22984, 992, 295, 4122, 510, 300, 10559, 490, 472, 917, 281, 264, 50692, 50692, 661, 570, 264, 1185, 307, 516, 281, 528, 281, 1594, 1951, 257, 7005, 4122, 300, 366, 2531, 13, 50972, 50972, 400, 370, 570, 295, 264, 19959, 11, 309, 311, 516, 281, 15684, 10559, 552, 370, 300, 436, 1319, 5692, 51200, 51248, 670, 264, 8062, 13, 823, 11, 294, 264, 5242, 300, 286, 4712, 294, 264, 9788, 11, 2602, 295, 17608, 264, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.08416031146871633, "compression_ratio": 1.7407407407407407, "no_speech_prob": 1.6184370906557888e-05}, {"id": 222, "seek": 149536, "start": 1507.52, "end": 1512.08, "text": " And so because of the overlap, it's going to continuously vary them so that they change slowly", "tokens": [50364, 300, 286, 478, 516, 281, 362, 257, 15684, 22984, 992, 295, 4122, 510, 300, 10559, 490, 472, 917, 281, 264, 50692, 50692, 661, 570, 264, 1185, 307, 516, 281, 528, 281, 1594, 1951, 257, 7005, 4122, 300, 366, 2531, 13, 50972, 50972, 400, 370, 570, 295, 264, 19959, 11, 309, 311, 516, 281, 15684, 10559, 552, 370, 300, 436, 1319, 5692, 51200, 51248, 670, 264, 8062, 13, 823, 11, 294, 264, 5242, 300, 286, 4712, 294, 264, 9788, 11, 2602, 295, 17608, 264, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.08416031146871633, "compression_ratio": 1.7407407407407407, "no_speech_prob": 1.6184370906557888e-05}, {"id": 223, "seek": 149536, "start": 1513.04, "end": 1521.12, "text": " over the vector. Now, in the pictures that I showed in the slides, instead of organizing the", "tokens": [50364, 300, 286, 478, 516, 281, 362, 257, 15684, 22984, 992, 295, 4122, 510, 300, 10559, 490, 472, 917, 281, 264, 50692, 50692, 661, 570, 264, 1185, 307, 516, 281, 528, 281, 1594, 1951, 257, 7005, 4122, 300, 366, 2531, 13, 50972, 50972, 400, 370, 570, 295, 264, 19959, 11, 309, 311, 516, 281, 15684, 10559, 552, 370, 300, 436, 1319, 5692, 51200, 51248, 670, 264, 8062, 13, 823, 11, 294, 264, 5242, 300, 286, 4712, 294, 264, 9788, 11, 2602, 295, 17608, 264, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.08416031146871633, "compression_ratio": 1.7407407407407407, "no_speech_prob": 1.6184370906557888e-05}, {"id": 224, "seek": 152112, "start": 1521.12, "end": 1526.6399999999999, "text": " z features here in a 1D topology, I organized them in a 2D topology. And I made the groups", "tokens": [50364, 710, 4122, 510, 294, 257, 502, 35, 1192, 1793, 11, 286, 9983, 552, 294, 257, 568, 35, 1192, 1793, 13, 400, 286, 1027, 264, 3935, 50640, 50640, 732, 18795, 13, 407, 286, 747, 257, 1386, 538, 1386, 3461, 13, 663, 311, 472, 1594, 13, 400, 550, 264, 958, 1594, 486, 312, 1071, 51024, 51024, 1386, 538, 1386, 3461, 365, 512, 19959, 13, 400, 550, 264, 958, 1594, 486, 312, 1939, 1071, 1386, 538, 1386, 3461, 13, 51340, 51404, 400, 1310, 286, 362, 1071, 472, 570, 286, 362, 257, 281, 6490, 304, 1192, 1793, 300, 2516, 613, 1074, 293, 613, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.09372404709603023, "compression_ratio": 1.8300970873786409, "no_speech_prob": 1.4732605450262781e-05}, {"id": 225, "seek": 152112, "start": 1526.6399999999999, "end": 1534.32, "text": " two dimensional. So I take a 6 by 6 block. That's one group. And then the next group will be another", "tokens": [50364, 710, 4122, 510, 294, 257, 502, 35, 1192, 1793, 11, 286, 9983, 552, 294, 257, 568, 35, 1192, 1793, 13, 400, 286, 1027, 264, 3935, 50640, 50640, 732, 18795, 13, 407, 286, 747, 257, 1386, 538, 1386, 3461, 13, 663, 311, 472, 1594, 13, 400, 550, 264, 958, 1594, 486, 312, 1071, 51024, 51024, 1386, 538, 1386, 3461, 365, 512, 19959, 13, 400, 550, 264, 958, 1594, 486, 312, 1939, 1071, 1386, 538, 1386, 3461, 13, 51340, 51404, 400, 1310, 286, 362, 1071, 472, 570, 286, 362, 257, 281, 6490, 304, 1192, 1793, 300, 2516, 613, 1074, 293, 613, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.09372404709603023, "compression_ratio": 1.8300970873786409, "no_speech_prob": 1.4732605450262781e-05}, {"id": 226, "seek": 152112, "start": 1534.32, "end": 1540.6399999999999, "text": " 6 by 6 block with some overlap. And then the next group will be yet another 6 by 6 block.", "tokens": [50364, 710, 4122, 510, 294, 257, 502, 35, 1192, 1793, 11, 286, 9983, 552, 294, 257, 568, 35, 1192, 1793, 13, 400, 286, 1027, 264, 3935, 50640, 50640, 732, 18795, 13, 407, 286, 747, 257, 1386, 538, 1386, 3461, 13, 663, 311, 472, 1594, 13, 400, 550, 264, 958, 1594, 486, 312, 1071, 51024, 51024, 1386, 538, 1386, 3461, 365, 512, 19959, 13, 400, 550, 264, 958, 1594, 486, 312, 1939, 1071, 1386, 538, 1386, 3461, 13, 51340, 51404, 400, 1310, 286, 362, 1071, 472, 570, 286, 362, 257, 281, 6490, 304, 1192, 1793, 300, 2516, 613, 1074, 293, 613, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.09372404709603023, "compression_ratio": 1.8300970873786409, "no_speech_prob": 1.4732605450262781e-05}, {"id": 227, "seek": 152112, "start": 1541.9199999999998, "end": 1547.4399999999998, "text": " And maybe I have another one because I have a toroidal topology that takes these guys and these", "tokens": [50364, 710, 4122, 510, 294, 257, 502, 35, 1192, 1793, 11, 286, 9983, 552, 294, 257, 568, 35, 1192, 1793, 13, 400, 286, 1027, 264, 3935, 50640, 50640, 732, 18795, 13, 407, 286, 747, 257, 1386, 538, 1386, 3461, 13, 663, 311, 472, 1594, 13, 400, 550, 264, 958, 1594, 486, 312, 1071, 51024, 51024, 1386, 538, 1386, 3461, 365, 512, 19959, 13, 400, 550, 264, 958, 1594, 486, 312, 1939, 1071, 1386, 538, 1386, 3461, 13, 51340, 51404, 400, 1310, 286, 362, 1071, 472, 570, 286, 362, 257, 281, 6490, 304, 1192, 1793, 300, 2516, 613, 1074, 293, 613, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.09372404709603023, "compression_ratio": 1.8300970873786409, "no_speech_prob": 1.4732605450262781e-05}, {"id": 228, "seek": 154744, "start": 1547.44, "end": 1557.8400000000001, "text": " guys. And then there is a similar thing sliding up, et cetera. So the groups basically are those", "tokens": [50364, 1074, 13, 400, 550, 456, 307, 257, 2531, 551, 21169, 493, 11, 1030, 11458, 13, 407, 264, 3935, 1936, 366, 729, 50884, 50884, 1386, 538, 1386, 9309, 300, 366, 18892, 538, 805, 293, 33535, 13, 400, 370, 300, 311, 577, 291, 483, 729, 51168, 51168, 15684, 22984, 4122, 2051, 264, 732, 12819, 13, 286, 727, 362, 12309, 731, 51556, 51600], "temperature": 0.0, "avg_logprob": -0.08837134607376591, "compression_ratio": 1.434782608695652, "no_speech_prob": 9.366098311147653e-06}, {"id": 229, "seek": 154744, "start": 1557.8400000000001, "end": 1563.52, "text": " 6 by 6 windows that are shifted by 3 and overlapping. And so that's how you get those", "tokens": [50364, 1074, 13, 400, 550, 456, 307, 257, 2531, 551, 21169, 493, 11, 1030, 11458, 13, 407, 264, 3935, 1936, 366, 729, 50884, 50884, 1386, 538, 1386, 9309, 300, 366, 18892, 538, 805, 293, 33535, 13, 400, 370, 300, 311, 577, 291, 483, 729, 51168, 51168, 15684, 22984, 4122, 2051, 264, 732, 12819, 13, 286, 727, 362, 12309, 731, 51556, 51600], "temperature": 0.0, "avg_logprob": -0.08837134607376591, "compression_ratio": 1.434782608695652, "no_speech_prob": 9.366098311147653e-06}, {"id": 230, "seek": 154744, "start": 1563.52, "end": 1571.28, "text": " continuously varying features along the two dimensions. I could have equally well", "tokens": [50364, 1074, 13, 400, 550, 456, 307, 257, 2531, 551, 21169, 493, 11, 1030, 11458, 13, 407, 264, 3935, 1936, 366, 729, 50884, 50884, 1386, 538, 1386, 9309, 300, 366, 18892, 538, 805, 293, 33535, 13, 400, 370, 300, 311, 577, 291, 483, 729, 51168, 51168, 15684, 22984, 4122, 2051, 264, 732, 12819, 13, 286, 727, 362, 12309, 731, 51556, 51600], "temperature": 0.0, "avg_logprob": -0.08837134607376591, "compression_ratio": 1.434782608695652, "no_speech_prob": 9.366098311147653e-06}, {"id": 231, "seek": 157128, "start": 1571.28, "end": 1579.92, "text": " chosen to organize this in a 3D topology or into some sort of tree. So I take all the components", "tokens": [50364, 8614, 281, 13859, 341, 294, 257, 805, 35, 1192, 1793, 420, 666, 512, 1333, 295, 4230, 13, 407, 286, 747, 439, 264, 6677, 50796, 50796, 295, 710, 293, 286, 13859, 552, 294, 512, 1333, 295, 4295, 11, 4317, 257, 4230, 13, 407, 341, 307, 1219, 51240, 51240, 3877, 637, 685, 507, 11, 406, 1594, 637, 685, 507, 3602, 13, 1042, 11, 309, 5946, 577, 291, 360, 309, 11, 286, 2041, 13, 51460, 51732], "temperature": 0.0, "avg_logprob": -0.17038848876953125, "compression_ratio": 1.5344827586206897, "no_speech_prob": 8.012225407583173e-06}, {"id": 232, "seek": 157128, "start": 1579.92, "end": 1588.8, "text": " of z and I organize them in some sort of graph, perhaps a tree. So this is called", "tokens": [50364, 8614, 281, 13859, 341, 294, 257, 805, 35, 1192, 1793, 420, 666, 512, 1333, 295, 4230, 13, 407, 286, 747, 439, 264, 6677, 50796, 50796, 295, 710, 293, 286, 13859, 552, 294, 512, 1333, 295, 4295, 11, 4317, 257, 4230, 13, 407, 341, 307, 1219, 51240, 51240, 3877, 637, 685, 507, 11, 406, 1594, 637, 685, 507, 3602, 13, 1042, 11, 309, 5946, 577, 291, 360, 309, 11, 286, 2041, 13, 51460, 51732], "temperature": 0.0, "avg_logprob": -0.17038848876953125, "compression_ratio": 1.5344827586206897, "no_speech_prob": 8.012225407583173e-06}, {"id": 233, "seek": 157128, "start": 1588.8, "end": 1593.2, "text": " structure sparsity, not group sparsity anymore. Well, it depends how you do it, I guess.", "tokens": [50364, 8614, 281, 13859, 341, 294, 257, 805, 35, 1192, 1793, 420, 666, 512, 1333, 295, 4230, 13, 407, 286, 747, 439, 264, 6677, 50796, 50796, 295, 710, 293, 286, 13859, 552, 294, 512, 1333, 295, 4295, 11, 4317, 257, 4230, 13, 407, 341, 307, 1219, 51240, 51240, 3877, 637, 685, 507, 11, 406, 1594, 637, 685, 507, 3602, 13, 1042, 11, 309, 5946, 577, 291, 360, 309, 11, 286, 2041, 13, 51460, 51732], "temperature": 0.0, "avg_logprob": -0.17038848876953125, "compression_ratio": 1.5344827586206897, "no_speech_prob": 8.012225407583173e-06}, {"id": 234, "seek": 159320, "start": 1593.2, "end": 1600.56, "text": " And then the groups would be things like this would be a group. And then perhaps this would", "tokens": [50364, 400, 550, 264, 3935, 576, 312, 721, 411, 341, 576, 312, 257, 1594, 13, 400, 550, 4317, 341, 576, 50732, 50732, 312, 257, 1594, 382, 731, 13, 400, 286, 393, 13859, 257, 1594, 294, 1333, 295, 7220, 29134, 411, 341, 13, 400, 437, 311, 51328, 51328, 516, 281, 1051, 456, 307, 300, 264, 3935, 300, 264, 6815, 300, 366, 294, 264, 1192, 1793, 366, 516, 281, 312, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.32698171911105306, "compression_ratio": 1.855263157894737, "no_speech_prob": 3.704403206938878e-05}, {"id": 235, "seek": 159320, "start": 1600.56, "end": 1612.48, "text": " be a group as well. And I can organize a group in sort of Russian dolls like this. And what's", "tokens": [50364, 400, 550, 264, 3935, 576, 312, 721, 411, 341, 576, 312, 257, 1594, 13, 400, 550, 4317, 341, 576, 50732, 50732, 312, 257, 1594, 382, 731, 13, 400, 286, 393, 13859, 257, 1594, 294, 1333, 295, 7220, 29134, 411, 341, 13, 400, 437, 311, 51328, 51328, 516, 281, 1051, 456, 307, 300, 264, 3935, 300, 264, 6815, 300, 366, 294, 264, 1192, 1793, 366, 516, 281, 312, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.32698171911105306, "compression_ratio": 1.855263157894737, "no_speech_prob": 3.704403206938878e-05}, {"id": 236, "seek": 159320, "start": 1612.48, "end": 1621.1200000000001, "text": " going to happen there is that the groups that the units that are in the topology are going to be", "tokens": [50364, 400, 550, 264, 3935, 576, 312, 721, 411, 341, 576, 312, 257, 1594, 13, 400, 550, 4317, 341, 576, 50732, 50732, 312, 257, 1594, 382, 731, 13, 400, 286, 393, 13859, 257, 1594, 294, 1333, 295, 7220, 29134, 411, 341, 13, 400, 437, 311, 51328, 51328, 516, 281, 1051, 456, 307, 300, 264, 3935, 300, 264, 6815, 300, 366, 294, 264, 1192, 1793, 366, 516, 281, 312, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.32698171911105306, "compression_ratio": 1.855263157894737, "no_speech_prob": 3.704403206938878e-05}, {"id": 237, "seek": 162112, "start": 1621.12, "end": 1626.1599999999999, "text": " sparse. The units that are in many groups will tend to be very sparse, whereas the units that", "tokens": [50364, 637, 11668, 13, 440, 6815, 300, 366, 294, 867, 3935, 486, 3928, 281, 312, 588, 637, 11668, 11, 9735, 264, 6815, 300, 50616, 50616, 366, 294, 257, 1326, 3935, 486, 3928, 281, 312, 1570, 637, 11668, 13, 400, 370, 498, 291, 360, 746, 411, 341, 365, 257, 4230, 11, 50912, 50936, 437, 2314, 510, 307, 300, 264, 4111, 294, 264, 3056, 12258, 281, 312, 406, 637, 11668, 412, 439, 13, 467, 311, 516, 51188, 51188, 281, 312, 746, 300, 534, 1333, 295, 5531, 82, 445, 588, 1333, 295, 19577, 4122, 13, 400, 550, 412, 264, 51468, 51468, 700, 1496, 294, 264, 4230, 11, 436, 434, 516, 281, 312, 257, 707, 637, 11668, 13, 407, 436, 434, 516, 281, 312, 1333, 295, 588, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.15049262273879277, "compression_ratio": 2.0168067226890756, "no_speech_prob": 1.696212711976841e-05}, {"id": 238, "seek": 162112, "start": 1626.1599999999999, "end": 1632.08, "text": " are in a few groups will tend to be less sparse. And so if you do something like this with a tree,", "tokens": [50364, 637, 11668, 13, 440, 6815, 300, 366, 294, 867, 3935, 486, 3928, 281, 312, 588, 637, 11668, 11, 9735, 264, 6815, 300, 50616, 50616, 366, 294, 257, 1326, 3935, 486, 3928, 281, 312, 1570, 637, 11668, 13, 400, 370, 498, 291, 360, 746, 411, 341, 365, 257, 4230, 11, 50912, 50936, 437, 2314, 510, 307, 300, 264, 4111, 294, 264, 3056, 12258, 281, 312, 406, 637, 11668, 412, 439, 13, 467, 311, 516, 51188, 51188, 281, 312, 746, 300, 534, 1333, 295, 5531, 82, 445, 588, 1333, 295, 19577, 4122, 13, 400, 550, 412, 264, 51468, 51468, 700, 1496, 294, 264, 4230, 11, 436, 434, 516, 281, 312, 257, 707, 637, 11668, 13, 407, 436, 434, 516, 281, 312, 1333, 295, 588, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.15049262273879277, "compression_ratio": 2.0168067226890756, "no_speech_prob": 1.696212711976841e-05}, {"id": 239, "seek": 162112, "start": 1632.56, "end": 1637.6, "text": " what happens here is that the feature in the center tends to be not sparse at all. It's going", "tokens": [50364, 637, 11668, 13, 440, 6815, 300, 366, 294, 867, 3935, 486, 3928, 281, 312, 588, 637, 11668, 11, 9735, 264, 6815, 300, 50616, 50616, 366, 294, 257, 1326, 3935, 486, 3928, 281, 312, 1570, 637, 11668, 13, 400, 370, 498, 291, 360, 746, 411, 341, 365, 257, 4230, 11, 50912, 50936, 437, 2314, 510, 307, 300, 264, 4111, 294, 264, 3056, 12258, 281, 312, 406, 637, 11668, 412, 439, 13, 467, 311, 516, 51188, 51188, 281, 312, 746, 300, 534, 1333, 295, 5531, 82, 445, 588, 1333, 295, 19577, 4122, 13, 400, 550, 412, 264, 51468, 51468, 700, 1496, 294, 264, 4230, 11, 436, 434, 516, 281, 312, 257, 707, 637, 11668, 13, 407, 436, 434, 516, 281, 312, 1333, 295, 588, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.15049262273879277, "compression_ratio": 2.0168067226890756, "no_speech_prob": 1.696212711976841e-05}, {"id": 240, "seek": 162112, "start": 1637.6, "end": 1643.1999999999998, "text": " to be something that really sort of detects just very sort of generic features. And then at the", "tokens": [50364, 637, 11668, 13, 440, 6815, 300, 366, 294, 867, 3935, 486, 3928, 281, 312, 588, 637, 11668, 11, 9735, 264, 6815, 300, 50616, 50616, 366, 294, 257, 1326, 3935, 486, 3928, 281, 312, 1570, 637, 11668, 13, 400, 370, 498, 291, 360, 746, 411, 341, 365, 257, 4230, 11, 50912, 50936, 437, 2314, 510, 307, 300, 264, 4111, 294, 264, 3056, 12258, 281, 312, 406, 637, 11668, 412, 439, 13, 467, 311, 516, 51188, 51188, 281, 312, 746, 300, 534, 1333, 295, 5531, 82, 445, 588, 1333, 295, 19577, 4122, 13, 400, 550, 412, 264, 51468, 51468, 700, 1496, 294, 264, 4230, 11, 436, 434, 516, 281, 312, 257, 707, 637, 11668, 13, 407, 436, 434, 516, 281, 312, 1333, 295, 588, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.15049262273879277, "compression_ratio": 2.0168067226890756, "no_speech_prob": 1.696212711976841e-05}, {"id": 241, "seek": 162112, "start": 1643.1999999999998, "end": 1646.9599999999998, "text": " first level in the tree, they're going to be a little sparse. So they're going to be sort of very", "tokens": [50364, 637, 11668, 13, 440, 6815, 300, 366, 294, 867, 3935, 486, 3928, 281, 312, 588, 637, 11668, 11, 9735, 264, 6815, 300, 50616, 50616, 366, 294, 257, 1326, 3935, 486, 3928, 281, 312, 1570, 637, 11668, 13, 400, 370, 498, 291, 360, 746, 411, 341, 365, 257, 4230, 11, 50912, 50936, 437, 2314, 510, 307, 300, 264, 4111, 294, 264, 3056, 12258, 281, 312, 406, 637, 11668, 412, 439, 13, 467, 311, 516, 51188, 51188, 281, 312, 746, 300, 534, 1333, 295, 5531, 82, 445, 588, 1333, 295, 19577, 4122, 13, 400, 550, 412, 264, 51468, 51468, 700, 1496, 294, 264, 4230, 11, 436, 434, 516, 281, 312, 257, 707, 637, 11668, 13, 407, 436, 434, 516, 281, 312, 1333, 295, 588, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.15049262273879277, "compression_ratio": 2.0168067226890756, "no_speech_prob": 1.696212711976841e-05}, {"id": 242, "seek": 164696, "start": 1646.96, "end": 1654.08, "text": " sort of smooth edge extractors or something like that. And then the more you go inside of the tree,", "tokens": [50364, 1333, 295, 5508, 4691, 8947, 830, 420, 746, 411, 300, 13, 400, 550, 264, 544, 291, 352, 1854, 295, 264, 4230, 11, 50720, 50720, 264, 544, 1184, 4111, 18780, 294, 257, 2416, 1230, 295, 28688, 11, 293, 4412, 436, 483, 544, 3321, 51072, 51072, 281, 312, 637, 11668, 13, 400, 370, 436, 917, 493, 885, 709, 637, 685, 260, 11, 597, 1355, 436, 917, 493, 885, 544, 51268, 51268, 33930, 337, 1729, 4122, 13, 400, 437, 2314, 456, 307, 300, 562, 291, 855, 364, 3256, 11, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.0576796637641059, "compression_ratio": 1.75, "no_speech_prob": 2.0142406356171705e-05}, {"id": 243, "seek": 164696, "start": 1654.08, "end": 1661.1200000000001, "text": " the more each feature enters in a large number of pools, and therefore they get more pressure", "tokens": [50364, 1333, 295, 5508, 4691, 8947, 830, 420, 746, 411, 300, 13, 400, 550, 264, 544, 291, 352, 1854, 295, 264, 4230, 11, 50720, 50720, 264, 544, 1184, 4111, 18780, 294, 257, 2416, 1230, 295, 28688, 11, 293, 4412, 436, 483, 544, 3321, 51072, 51072, 281, 312, 637, 11668, 13, 400, 370, 436, 917, 493, 885, 709, 637, 685, 260, 11, 597, 1355, 436, 917, 493, 885, 544, 51268, 51268, 33930, 337, 1729, 4122, 13, 400, 437, 2314, 456, 307, 300, 562, 291, 855, 364, 3256, 11, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.0576796637641059, "compression_ratio": 1.75, "no_speech_prob": 2.0142406356171705e-05}, {"id": 244, "seek": 164696, "start": 1661.1200000000001, "end": 1665.04, "text": " to be sparse. And so they end up being much sparser, which means they end up being more", "tokens": [50364, 1333, 295, 5508, 4691, 8947, 830, 420, 746, 411, 300, 13, 400, 550, 264, 544, 291, 352, 1854, 295, 264, 4230, 11, 50720, 50720, 264, 544, 1184, 4111, 18780, 294, 257, 2416, 1230, 295, 28688, 11, 293, 4412, 436, 483, 544, 3321, 51072, 51072, 281, 312, 637, 11668, 13, 400, 370, 436, 917, 493, 885, 709, 637, 685, 260, 11, 597, 1355, 436, 917, 493, 885, 544, 51268, 51268, 33930, 337, 1729, 4122, 13, 400, 437, 2314, 456, 307, 300, 562, 291, 855, 364, 3256, 11, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.0576796637641059, "compression_ratio": 1.75, "no_speech_prob": 2.0142406356171705e-05}, {"id": 245, "seek": 164696, "start": 1665.04, "end": 1671.6000000000001, "text": " selective for particular features. And what happens there is that when you show an image,", "tokens": [50364, 1333, 295, 5508, 4691, 8947, 830, 420, 746, 411, 300, 13, 400, 550, 264, 544, 291, 352, 1854, 295, 264, 4230, 11, 50720, 50720, 264, 544, 1184, 4111, 18780, 294, 257, 2416, 1230, 295, 28688, 11, 293, 4412, 436, 483, 544, 3321, 51072, 51072, 281, 312, 637, 11668, 13, 400, 370, 436, 917, 493, 885, 709, 637, 685, 260, 11, 597, 1355, 436, 917, 493, 885, 544, 51268, 51268, 33930, 337, 1729, 4122, 13, 400, 437, 2314, 456, 307, 300, 562, 291, 855, 364, 3256, 11, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.0576796637641059, "compression_ratio": 1.75, "no_speech_prob": 2.0142406356171705e-05}, {"id": 246, "seek": 167160, "start": 1671.6, "end": 1678.32, "text": " it tends to favor activating features that are along one particular branch in that tree,", "tokens": [50364, 309, 12258, 281, 2294, 42481, 4122, 300, 366, 2051, 472, 1729, 9819, 294, 300, 4230, 11, 50700, 50756, 570, 300, 311, 264, 1151, 636, 281, 1333, 295, 17522, 264, 1230, 295, 28688, 300, 366, 322, 412, 604, 472, 565, 13, 50952, 51020, 407, 718, 311, 818, 9227, 46130, 507, 13, 51092, 51532], "temperature": 0.0, "avg_logprob": -0.16406597914519133, "compression_ratio": 1.489795918367347, "no_speech_prob": 1.472573603678029e-05}, {"id": 247, "seek": 167160, "start": 1679.4399999999998, "end": 1683.36, "text": " because that's the best way to sort of minimize the number of pools that are on at any one time.", "tokens": [50364, 309, 12258, 281, 2294, 42481, 4122, 300, 366, 2051, 472, 1729, 9819, 294, 300, 4230, 11, 50700, 50756, 570, 300, 311, 264, 1151, 636, 281, 1333, 295, 17522, 264, 1230, 295, 28688, 300, 366, 322, 412, 604, 472, 565, 13, 50952, 51020, 407, 718, 311, 818, 9227, 46130, 507, 13, 51092, 51532], "temperature": 0.0, "avg_logprob": -0.16406597914519133, "compression_ratio": 1.489795918367347, "no_speech_prob": 1.472573603678029e-05}, {"id": 248, "seek": 167160, "start": 1684.7199999999998, "end": 1686.1599999999999, "text": " So let's call structures varsity.", "tokens": [50364, 309, 12258, 281, 2294, 42481, 4122, 300, 366, 2051, 472, 1729, 9819, 294, 300, 4230, 11, 50700, 50756, 570, 300, 311, 264, 1151, 636, 281, 1333, 295, 17522, 264, 1230, 295, 28688, 300, 366, 322, 412, 604, 472, 565, 13, 50952, 51020, 407, 718, 311, 818, 9227, 46130, 507, 13, 51092, 51532], "temperature": 0.0, "avg_logprob": -0.16406597914519133, "compression_ratio": 1.489795918367347, "no_speech_prob": 1.472573603678029e-05}, {"id": 249, "seek": 168616, "start": 1686.16, "end": 1697.0400000000002, "text": " And there's a number of papers on this by", "tokens": [50364, 400, 456, 311, 257, 1230, 295, 10577, 322, 341, 538, 50908, 50996, 7174, 1053, 6124, 304, 13, 407, 341, 1709, 646, 466, 1266, 924, 2057, 11, 293, 11097, 401, 79, 675, 3632, 25781, 13, 51324, 51420, 286, 914, 11, 436, 598, 12, 40198, 2769, 341, 13, 18370, 3793, 390, 19648, 5833, 13, 51588, 51744], "temperature": 0.0, "avg_logprob": -0.2817319631576538, "compression_ratio": 1.2805755395683454, "no_speech_prob": 2.5699599063955247e-05}, {"id": 250, "seek": 168616, "start": 1698.8000000000002, "end": 1705.3600000000001, "text": " Julien Meral. So this goes back about 10 years ago, and Rodolphe Genaton.", "tokens": [50364, 400, 456, 311, 257, 1230, 295, 10577, 322, 341, 538, 50908, 50996, 7174, 1053, 6124, 304, 13, 407, 341, 1709, 646, 466, 1266, 924, 2057, 11, 293, 11097, 401, 79, 675, 3632, 25781, 13, 51324, 51420, 286, 914, 11, 436, 598, 12, 40198, 2769, 341, 13, 18370, 3793, 390, 19648, 5833, 13, 51588, 51744], "temperature": 0.0, "avg_logprob": -0.2817319631576538, "compression_ratio": 1.2805755395683454, "no_speech_prob": 2.5699599063955247e-05}, {"id": 251, "seek": 168616, "start": 1707.28, "end": 1710.64, "text": " I mean, they co-authored this. Senior author was Francis Back.", "tokens": [50364, 400, 456, 311, 257, 1230, 295, 10577, 322, 341, 538, 50908, 50996, 7174, 1053, 6124, 304, 13, 407, 341, 1709, 646, 466, 1266, 924, 2057, 11, 293, 11097, 401, 79, 675, 3632, 25781, 13, 51324, 51420, 286, 914, 11, 436, 598, 12, 40198, 2769, 341, 13, 18370, 3793, 390, 19648, 5833, 13, 51588, 51744], "temperature": 0.0, "avg_logprob": -0.2817319631576538, "compression_ratio": 1.2805755395683454, "no_speech_prob": 2.5699599063955247e-05}, {"id": 252, "seek": 171064, "start": 1710.64, "end": 1717.44, "text": " I put the reference in one of the slides. And there's a paper by my group also by Arthur Schlamm,", "tokens": [50364, 286, 829, 264, 6408, 294, 472, 295, 264, 9788, 13, 400, 456, 311, 257, 3035, 538, 452, 1594, 611, 538, 19624, 16420, 5136, 11, 50704, 50764, 597, 286, 603, 352, 281, 294, 257, 3456, 13, 50868, 50868, 1664, 291, 2903, 983, 40149, 3890, 2144, 767, 3665, 294, 40149, 2531, 4122, 30, 51180, 51244, 1042, 11, 370, 300, 311, 257, 665, 1168, 13, 2386, 295, 439, 11, 775, 309, 854, 30, 400, 264, 1867, 307, 406, 1850, 13, 51608, 51656], "temperature": 0.0, "avg_logprob": -0.3096756876250844, "compression_ratio": 1.4711538461538463, "no_speech_prob": 3.1144598324317485e-05}, {"id": 253, "seek": 171064, "start": 1718.64, "end": 1720.72, "text": " which I'll go to in a minute.", "tokens": [50364, 286, 829, 264, 6408, 294, 472, 295, 264, 9788, 13, 400, 456, 311, 257, 3035, 538, 452, 1594, 611, 538, 19624, 16420, 5136, 11, 50704, 50764, 597, 286, 603, 352, 281, 294, 257, 3456, 13, 50868, 50868, 1664, 291, 2903, 983, 40149, 3890, 2144, 767, 3665, 294, 40149, 2531, 4122, 30, 51180, 51244, 1042, 11, 370, 300, 311, 257, 665, 1168, 13, 2386, 295, 439, 11, 775, 309, 854, 30, 400, 264, 1867, 307, 406, 1850, 13, 51608, 51656], "temperature": 0.0, "avg_logprob": -0.3096756876250844, "compression_ratio": 1.4711538461538463, "no_speech_prob": 3.1144598324317485e-05}, {"id": 254, "seek": 171064, "start": 1720.72, "end": 1726.96, "text": " Can you explain why grouping regularization actually helps in grouping similar features?", "tokens": [50364, 286, 829, 264, 6408, 294, 472, 295, 264, 9788, 13, 400, 456, 311, 257, 3035, 538, 452, 1594, 611, 538, 19624, 16420, 5136, 11, 50704, 50764, 597, 286, 603, 352, 281, 294, 257, 3456, 13, 50868, 50868, 1664, 291, 2903, 983, 40149, 3890, 2144, 767, 3665, 294, 40149, 2531, 4122, 30, 51180, 51244, 1042, 11, 370, 300, 311, 257, 665, 1168, 13, 2386, 295, 439, 11, 775, 309, 854, 30, 400, 264, 1867, 307, 406, 1850, 13, 51608, 51656], "temperature": 0.0, "avg_logprob": -0.3096756876250844, "compression_ratio": 1.4711538461538463, "no_speech_prob": 3.1144598324317485e-05}, {"id": 255, "seek": 171064, "start": 1728.24, "end": 1735.5200000000002, "text": " Well, so that's a good question. First of all, does it help? And the answer is not clear.", "tokens": [50364, 286, 829, 264, 6408, 294, 472, 295, 264, 9788, 13, 400, 456, 311, 257, 3035, 538, 452, 1594, 611, 538, 19624, 16420, 5136, 11, 50704, 50764, 597, 286, 603, 352, 281, 294, 257, 3456, 13, 50868, 50868, 1664, 291, 2903, 983, 40149, 3890, 2144, 767, 3665, 294, 40149, 2531, 4122, 30, 51180, 51244, 1042, 11, 370, 300, 311, 257, 665, 1168, 13, 2386, 295, 439, 11, 775, 309, 854, 30, 400, 264, 1867, 307, 406, 1850, 13, 51608, 51656], "temperature": 0.0, "avg_logprob": -0.3096756876250844, "compression_ratio": 1.4711538461538463, "no_speech_prob": 3.1144598324317485e-05}, {"id": 256, "seek": 173552, "start": 1735.52, "end": 1742.08, "text": " So those experiments were done quite a while ago before the computation was really available,", "tokens": [50364, 407, 729, 12050, 645, 1096, 1596, 257, 1339, 2057, 949, 264, 24903, 390, 534, 2435, 11, 50692, 50692, 293, 264, 1412, 390, 2435, 337, 341, 281, 534, 733, 295, 589, 412, 257, 955, 4373, 13, 639, 390, 5240, 50980, 51036, 19174, 382, 264, 561, 3102, 294, 341, 645, 3102, 294, 732, 721, 13, 814, 645, 2139, 51212, 51212, 3102, 294, 2693, 12879, 24420, 2539, 337, 721, 411, 3256, 23722, 293, 1507, 411, 300, 13, 639, 51412, 51412, 390, 437, 7174, 1053, 6124, 304, 390, 884, 13, 1057, 436, 645, 3102, 294, 390, 264, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.2662845886859697, "compression_ratio": 1.8057851239669422, "no_speech_prob": 1.0127400855708402e-05}, {"id": 257, "seek": 173552, "start": 1742.08, "end": 1747.84, "text": " and the data was available for this to really kind of work at a big scale. This was mostly", "tokens": [50364, 407, 729, 12050, 645, 1096, 1596, 257, 1339, 2057, 949, 264, 24903, 390, 534, 2435, 11, 50692, 50692, 293, 264, 1412, 390, 2435, 337, 341, 281, 534, 733, 295, 589, 412, 257, 955, 4373, 13, 639, 390, 5240, 50980, 51036, 19174, 382, 264, 561, 3102, 294, 341, 645, 3102, 294, 732, 721, 13, 814, 645, 2139, 51212, 51212, 3102, 294, 2693, 12879, 24420, 2539, 337, 721, 411, 3256, 23722, 293, 1507, 411, 300, 13, 639, 51412, 51412, 390, 437, 7174, 1053, 6124, 304, 390, 884, 13, 1057, 436, 645, 3102, 294, 390, 264, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.2662845886859697, "compression_ratio": 1.8057851239669422, "no_speech_prob": 1.0127400855708402e-05}, {"id": 258, "seek": 173552, "start": 1748.96, "end": 1752.48, "text": " viewed as the people interested in this were interested in two things. They were either", "tokens": [50364, 407, 729, 12050, 645, 1096, 1596, 257, 1339, 2057, 949, 264, 24903, 390, 534, 2435, 11, 50692, 50692, 293, 264, 1412, 390, 2435, 337, 341, 281, 534, 733, 295, 589, 412, 257, 955, 4373, 13, 639, 390, 5240, 50980, 51036, 19174, 382, 264, 561, 3102, 294, 341, 645, 3102, 294, 732, 721, 13, 814, 645, 2139, 51212, 51212, 3102, 294, 2693, 12879, 24420, 2539, 337, 721, 411, 3256, 23722, 293, 1507, 411, 300, 13, 639, 51412, 51412, 390, 437, 7174, 1053, 6124, 304, 390, 884, 13, 1057, 436, 645, 3102, 294, 390, 264, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.2662845886859697, "compression_ratio": 1.8057851239669422, "no_speech_prob": 1.0127400855708402e-05}, {"id": 259, "seek": 173552, "start": 1752.48, "end": 1756.48, "text": " interested in unsupervised learning for things like image restoration and stuff like that. This", "tokens": [50364, 407, 729, 12050, 645, 1096, 1596, 257, 1339, 2057, 949, 264, 24903, 390, 534, 2435, 11, 50692, 50692, 293, 264, 1412, 390, 2435, 337, 341, 281, 534, 733, 295, 589, 412, 257, 955, 4373, 13, 639, 390, 5240, 50980, 51036, 19174, 382, 264, 561, 3102, 294, 341, 645, 3102, 294, 732, 721, 13, 814, 645, 2139, 51212, 51212, 3102, 294, 2693, 12879, 24420, 2539, 337, 721, 411, 3256, 23722, 293, 1507, 411, 300, 13, 639, 51412, 51412, 390, 437, 7174, 1053, 6124, 304, 390, 884, 13, 1057, 436, 645, 3102, 294, 390, 264, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.2662845886859697, "compression_ratio": 1.8057851239669422, "no_speech_prob": 1.0127400855708402e-05}, {"id": 260, "seek": 173552, "start": 1756.48, "end": 1760.96, "text": " was what Julien Meral was doing. All they were interested in was the", "tokens": [50364, 407, 729, 12050, 645, 1096, 1596, 257, 1339, 2057, 949, 264, 24903, 390, 534, 2435, 11, 50692, 50692, 293, 264, 1412, 390, 2435, 337, 341, 281, 534, 733, 295, 589, 412, 257, 955, 4373, 13, 639, 390, 5240, 50980, 51036, 19174, 382, 264, 561, 3102, 294, 341, 645, 3102, 294, 732, 721, 13, 814, 645, 2139, 51212, 51212, 3102, 294, 2693, 12879, 24420, 2539, 337, 721, 411, 3256, 23722, 293, 1507, 411, 300, 13, 639, 51412, 51412, 390, 437, 7174, 1053, 6124, 304, 390, 884, 13, 1057, 436, 645, 3102, 294, 390, 264, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.2662845886859697, "compression_ratio": 1.8057851239669422, "no_speech_prob": 1.0127400855708402e-05}, {"id": 261, "seek": 176096, "start": 1760.96, "end": 1765.76, "text": " self-supervised pre-training, because at the time the data sets were very small for training", "tokens": [50364, 2698, 12, 48172, 24420, 659, 12, 17227, 1760, 11, 570, 412, 264, 565, 264, 1412, 6352, 645, 588, 1359, 337, 3097, 50604, 50672, 45216, 304, 36170, 13, 814, 645, 886, 1359, 13, 407, 456, 632, 281, 312, 512, 1333, 295, 659, 12, 17227, 1760, 10747, 11, 50860, 50888, 597, 307, 437, 286, 390, 3102, 294, 13, 400, 370, 309, 311, 264, 912, 12335, 300, 321, 586, 362, 797, 337, 51180, 51180, 2698, 12, 48172, 24420, 2539, 13, 583, 257, 688, 295, 729, 7150, 2378, 380, 668, 3038, 646, 51532, 51588], "temperature": 0.0, "avg_logprob": -0.22498528853706692, "compression_ratio": 1.5947136563876652, "no_speech_prob": 6.046072030585492e-06}, {"id": 262, "seek": 176096, "start": 1767.1200000000001, "end": 1770.88, "text": " convolutional nets. They were too small. So there had to be some sort of pre-training procedure,", "tokens": [50364, 2698, 12, 48172, 24420, 659, 12, 17227, 1760, 11, 570, 412, 264, 565, 264, 1412, 6352, 645, 588, 1359, 337, 3097, 50604, 50672, 45216, 304, 36170, 13, 814, 645, 886, 1359, 13, 407, 456, 632, 281, 312, 512, 1333, 295, 659, 12, 17227, 1760, 10747, 11, 50860, 50888, 597, 307, 437, 286, 390, 3102, 294, 13, 400, 370, 309, 311, 264, 912, 12335, 300, 321, 586, 362, 797, 337, 51180, 51180, 2698, 12, 48172, 24420, 2539, 13, 583, 257, 688, 295, 729, 7150, 2378, 380, 668, 3038, 646, 51532, 51588], "temperature": 0.0, "avg_logprob": -0.22498528853706692, "compression_ratio": 1.5947136563876652, "no_speech_prob": 6.046072030585492e-06}, {"id": 263, "seek": 176096, "start": 1771.44, "end": 1777.28, "text": " which is what I was interested in. And so it's the same motivation that we now have again for", "tokens": [50364, 2698, 12, 48172, 24420, 659, 12, 17227, 1760, 11, 570, 412, 264, 565, 264, 1412, 6352, 645, 588, 1359, 337, 3097, 50604, 50672, 45216, 304, 36170, 13, 814, 645, 886, 1359, 13, 407, 456, 632, 281, 312, 512, 1333, 295, 659, 12, 17227, 1760, 10747, 11, 50860, 50888, 597, 307, 437, 286, 390, 3102, 294, 13, 400, 370, 309, 311, 264, 912, 12335, 300, 321, 586, 362, 797, 337, 51180, 51180, 2698, 12, 48172, 24420, 2539, 13, 583, 257, 688, 295, 729, 7150, 2378, 380, 668, 3038, 646, 51532, 51588], "temperature": 0.0, "avg_logprob": -0.22498528853706692, "compression_ratio": 1.5947136563876652, "no_speech_prob": 6.046072030585492e-06}, {"id": 264, "seek": 176096, "start": 1777.28, "end": 1784.32, "text": " self-supervised learning. But a lot of those methods haven't been brought back", "tokens": [50364, 2698, 12, 48172, 24420, 659, 12, 17227, 1760, 11, 570, 412, 264, 565, 264, 1412, 6352, 645, 588, 1359, 337, 3097, 50604, 50672, 45216, 304, 36170, 13, 814, 645, 886, 1359, 13, 407, 456, 632, 281, 312, 512, 1333, 295, 659, 12, 17227, 1760, 10747, 11, 50860, 50888, 597, 307, 437, 286, 390, 3102, 294, 13, 400, 370, 309, 311, 264, 912, 12335, 300, 321, 586, 362, 797, 337, 51180, 51180, 2698, 12, 48172, 24420, 2539, 13, 583, 257, 688, 295, 729, 7150, 2378, 380, 668, 3038, 646, 51532, 51588], "temperature": 0.0, "avg_logprob": -0.22498528853706692, "compression_ratio": 1.5947136563876652, "no_speech_prob": 6.046072030585492e-06}, {"id": 265, "seek": 178432, "start": 1784.32, "end": 1790.72, "text": " to the fore. They tended to work very well when the data set was small. So they tended to kind", "tokens": [50364, 281, 264, 2091, 13, 814, 34732, 281, 589, 588, 731, 562, 264, 1412, 992, 390, 1359, 13, 407, 436, 34732, 281, 733, 50684, 50684, 295, 3470, 3389, 295, 11, 718, 311, 584, 11, 257, 45216, 304, 2533, 498, 291, 659, 12, 83, 7146, 341, 1228, 257, 3170, 13, 50924, 50964, 407, 1228, 257, 3170, 588, 2531, 281, 264, 472, 286, 4712, 3071, 11, 370, 746, 257, 857, 411, 341, 11, 51244, 51244, 457, 45216, 304, 13, 407, 652, 264, 2058, 19866, 293, 264, 979, 19866, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.21295446760199044, "compression_ratio": 1.6584158415841583, "no_speech_prob": 2.1560788354690885e-06}, {"id": 266, "seek": 178432, "start": 1790.72, "end": 1795.52, "text": " of improve performance of, let's say, a convolutional net if you pre-train this using a method.", "tokens": [50364, 281, 264, 2091, 13, 814, 34732, 281, 589, 588, 731, 562, 264, 1412, 992, 390, 1359, 13, 407, 436, 34732, 281, 733, 50684, 50684, 295, 3470, 3389, 295, 11, 718, 311, 584, 11, 257, 45216, 304, 2533, 498, 291, 659, 12, 83, 7146, 341, 1228, 257, 3170, 13, 50924, 50964, 407, 1228, 257, 3170, 588, 2531, 281, 264, 472, 286, 4712, 3071, 11, 370, 746, 257, 857, 411, 341, 11, 51244, 51244, 457, 45216, 304, 13, 407, 652, 264, 2058, 19866, 293, 264, 979, 19866, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.21295446760199044, "compression_ratio": 1.6584158415841583, "no_speech_prob": 2.1560788354690885e-06}, {"id": 267, "seek": 178432, "start": 1796.32, "end": 1801.9199999999998, "text": " So using a method very similar to the one I showed earlier, so something a bit like this,", "tokens": [50364, 281, 264, 2091, 13, 814, 34732, 281, 589, 588, 731, 562, 264, 1412, 992, 390, 1359, 13, 407, 436, 34732, 281, 733, 50684, 50684, 295, 3470, 3389, 295, 11, 718, 311, 584, 11, 257, 45216, 304, 2533, 498, 291, 659, 12, 83, 7146, 341, 1228, 257, 3170, 13, 50924, 50964, 407, 1228, 257, 3170, 588, 2531, 281, 264, 472, 286, 4712, 3071, 11, 370, 746, 257, 857, 411, 341, 11, 51244, 51244, 457, 45216, 304, 13, 407, 652, 264, 2058, 19866, 293, 264, 979, 19866, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.21295446760199044, "compression_ratio": 1.6584158415841583, "no_speech_prob": 2.1560788354690885e-06}, {"id": 268, "seek": 178432, "start": 1801.9199999999998, "end": 1808.0, "text": " but convolutional. So make the encoder and the decoder", "tokens": [50364, 281, 264, 2091, 13, 814, 34732, 281, 589, 588, 731, 562, 264, 1412, 992, 390, 1359, 13, 407, 436, 34732, 281, 733, 50684, 50684, 295, 3470, 3389, 295, 11, 718, 311, 584, 11, 257, 45216, 304, 2533, 498, 291, 659, 12, 83, 7146, 341, 1228, 257, 3170, 13, 50924, 50964, 407, 1228, 257, 3170, 588, 2531, 281, 264, 472, 286, 4712, 3071, 11, 370, 746, 257, 857, 411, 341, 11, 51244, 51244, 457, 45216, 304, 13, 407, 652, 264, 2058, 19866, 293, 264, 979, 19866, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.21295446760199044, "compression_ratio": 1.6584158415841583, "no_speech_prob": 2.1560788354690885e-06}, {"id": 269, "seek": 180800, "start": 1808.0, "end": 1816.24, "text": " convolutional and train with good sparsity on complex cells. And then after you're done", "tokens": [50364, 45216, 304, 293, 3847, 365, 665, 637, 685, 507, 322, 3997, 5438, 13, 400, 550, 934, 291, 434, 1096, 50776, 50776, 659, 12, 17227, 1760, 264, 1185, 11, 291, 483, 3973, 295, 264, 979, 19866, 13, 509, 787, 764, 264, 2058, 19866, 382, 257, 4111, 51000, 51000, 8947, 284, 11, 584, 264, 700, 4583, 295, 257, 45216, 304, 2533, 11, 293, 291, 2897, 257, 1150, 4583, 322, 1192, 295, 309, 13, 51308, 51348, 407, 718, 385, 352, 807, 341, 257, 707, 857, 13, 407, 291, 722, 365, 257, 992, 295, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.2694546922724298, "compression_ratio": 1.602803738317757, "no_speech_prob": 1.9357561541255563e-05}, {"id": 270, "seek": 180800, "start": 1816.24, "end": 1820.72, "text": " pre-training the system, you get rid of the decoder. You only use the encoder as a feature", "tokens": [50364, 45216, 304, 293, 3847, 365, 665, 637, 685, 507, 322, 3997, 5438, 13, 400, 550, 934, 291, 434, 1096, 50776, 50776, 659, 12, 17227, 1760, 264, 1185, 11, 291, 483, 3973, 295, 264, 979, 19866, 13, 509, 787, 764, 264, 2058, 19866, 382, 257, 4111, 51000, 51000, 8947, 284, 11, 584, 264, 700, 4583, 295, 257, 45216, 304, 2533, 11, 293, 291, 2897, 257, 1150, 4583, 322, 1192, 295, 309, 13, 51308, 51348, 407, 718, 385, 352, 807, 341, 257, 707, 857, 13, 407, 291, 722, 365, 257, 992, 295, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.2694546922724298, "compression_ratio": 1.602803738317757, "no_speech_prob": 1.9357561541255563e-05}, {"id": 271, "seek": 180800, "start": 1820.72, "end": 1826.88, "text": " extractor, say the first layer of a convolutional net, and you stick a second layer on top of it.", "tokens": [50364, 45216, 304, 293, 3847, 365, 665, 637, 685, 507, 322, 3997, 5438, 13, 400, 550, 934, 291, 434, 1096, 50776, 50776, 659, 12, 17227, 1760, 264, 1185, 11, 291, 483, 3973, 295, 264, 979, 19866, 13, 509, 787, 764, 264, 2058, 19866, 382, 257, 4111, 51000, 51000, 8947, 284, 11, 584, 264, 700, 4583, 295, 257, 45216, 304, 2533, 11, 293, 291, 2897, 257, 1150, 4583, 322, 1192, 295, 309, 13, 51308, 51348, 407, 718, 385, 352, 807, 341, 257, 707, 857, 13, 407, 291, 722, 365, 257, 992, 295, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.2694546922724298, "compression_ratio": 1.602803738317757, "no_speech_prob": 1.9357561541255563e-05}, {"id": 272, "seek": 180800, "start": 1827.68, "end": 1834.86, "text": " So let me go through this a little bit. So you start with a set of", "tokens": [50364, 45216, 304, 293, 3847, 365, 665, 637, 685, 507, 322, 3997, 5438, 13, 400, 550, 934, 291, 434, 1096, 50776, 50776, 659, 12, 17227, 1760, 264, 1185, 11, 291, 483, 3973, 295, 264, 979, 19866, 13, 509, 787, 764, 264, 2058, 19866, 382, 257, 4111, 51000, 51000, 8947, 284, 11, 584, 264, 700, 4583, 295, 257, 45216, 304, 2533, 11, 293, 291, 2897, 257, 1150, 4583, 322, 1192, 295, 309, 13, 51308, 51348, 407, 718, 385, 352, 807, 341, 257, 707, 857, 13, 407, 291, 722, 365, 257, 992, 295, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.2694546922724298, "compression_ratio": 1.602803738317757, "no_speech_prob": 1.9357561541255563e-05}, {"id": 273, "seek": 183486, "start": 1834.86, "end": 1852.3999999999999, "text": " images. You have an encoder which is basically convolutional radio.", "tokens": [50364, 5267, 13, 509, 362, 364, 2058, 19866, 597, 307, 1936, 45216, 304, 6477, 13, 51241, 51401, 1726, 709, 544, 813, 300, 11, 1392, 11, 445, 45216, 304, 6477, 13, 821, 2203, 281, 312, 512, 1333, 295, 21589, 51689, 51689], "temperature": 0.0, "avg_logprob": -0.36615646176221894, "compression_ratio": 1.38135593220339, "no_speech_prob": 3.904527602571761e-06}, {"id": 274, "seek": 183486, "start": 1855.6, "end": 1861.36, "text": " Not much more than that, okay, just convolutional radio. There needs to be some sort of scaling", "tokens": [50364, 5267, 13, 509, 362, 364, 2058, 19866, 597, 307, 1936, 45216, 304, 6477, 13, 51241, 51401, 1726, 709, 544, 813, 300, 11, 1392, 11, 445, 45216, 304, 6477, 13, 821, 2203, 281, 312, 512, 1333, 295, 21589, 51689, 51689], "temperature": 0.0, "avg_logprob": -0.36615646176221894, "compression_ratio": 1.38135593220339, "no_speech_prob": 3.904527602571761e-06}, {"id": 275, "seek": 186136, "start": 1861.36, "end": 1870.08, "text": " layer afterwards for this particular case. And you train with groups sparsity. So you have a", "tokens": [50364, 4583, 10543, 337, 341, 1729, 1389, 13, 400, 291, 3847, 365, 3935, 637, 685, 507, 13, 407, 291, 362, 257, 50800, 50800, 8213, 979, 19866, 293, 291, 733, 295, 31499, 264, 4846, 293, 291, 362, 257, 51164, 51456, 291, 362, 257, 46691, 510, 597, 307, 341, 1594, 441, 16, 13, 51596, 51676], "temperature": 0.0, "avg_logprob": -0.24305945855599861, "compression_ratio": 1.4685314685314685, "no_speech_prob": 1.1473383892735e-05}, {"id": 276, "seek": 186136, "start": 1870.08, "end": 1877.36, "text": " linear decoder and you kind of reconstruct the input and you have a", "tokens": [50364, 4583, 10543, 337, 341, 1729, 1389, 13, 400, 291, 3847, 365, 3935, 637, 685, 507, 13, 407, 291, 362, 257, 50800, 50800, 8213, 979, 19866, 293, 291, 733, 295, 31499, 264, 4846, 293, 291, 362, 257, 51164, 51456, 291, 362, 257, 46691, 510, 597, 307, 341, 1594, 441, 16, 13, 51596, 51676], "temperature": 0.0, "avg_logprob": -0.24305945855599861, "compression_ratio": 1.4685314685314685, "no_speech_prob": 1.1473383892735e-05}, {"id": 277, "seek": 186136, "start": 1883.1999999999998, "end": 1886.0, "text": " you have a criterion here which is this group L1.", "tokens": [50364, 4583, 10543, 337, 341, 1729, 1389, 13, 400, 291, 3847, 365, 3935, 637, 685, 507, 13, 407, 291, 362, 257, 50800, 50800, 8213, 979, 19866, 293, 291, 733, 295, 31499, 264, 4846, 293, 291, 362, 257, 51164, 51456, 291, 362, 257, 46691, 510, 597, 307, 341, 1594, 441, 16, 13, 51596, 51676], "temperature": 0.0, "avg_logprob": -0.24305945855599861, "compression_ratio": 1.4685314685314685, "no_speech_prob": 1.1473383892735e-05}, {"id": 278, "seek": 188600, "start": 1886.0, "end": 1895.92, "text": " Okay, so it's sum over group, sorry, I call the group P, right? Sum over group of square root of sum for", "tokens": [50364, 1033, 11, 370, 309, 311, 2408, 670, 1594, 11, 2597, 11, 286, 818, 264, 1594, 430, 11, 558, 30, 8626, 670, 1594, 295, 3732, 5593, 295, 2408, 337, 50860, 50912, 741, 294, 264, 1594, 295, 710, 741, 8889, 13, 1033, 11, 370, 300, 311, 1594, 637, 685, 507, 13, 407, 291, 3847, 341, 707, 637, 11668, 51388, 51440, 8399, 22660, 19866, 365, 665, 637, 685, 507, 13, 400, 550, 437, 291, 360, 307, 291, 747, 264, 51700, 51772], "temperature": 0.0, "avg_logprob": -0.22327265739440919, "compression_ratio": 1.5952380952380953, "no_speech_prob": 4.935809101880295e-06}, {"id": 279, "seek": 188600, "start": 1896.96, "end": 1906.48, "text": " i in the group of z i squared. Okay, so that's group sparsity. So you train this little sparse", "tokens": [50364, 1033, 11, 370, 309, 311, 2408, 670, 1594, 11, 2597, 11, 286, 818, 264, 1594, 430, 11, 558, 30, 8626, 670, 1594, 295, 3732, 5593, 295, 2408, 337, 50860, 50912, 741, 294, 264, 1594, 295, 710, 741, 8889, 13, 1033, 11, 370, 300, 311, 1594, 637, 685, 507, 13, 407, 291, 3847, 341, 707, 637, 11668, 51388, 51440, 8399, 22660, 19866, 365, 665, 637, 685, 507, 13, 400, 550, 437, 291, 360, 307, 291, 747, 264, 51700, 51772], "temperature": 0.0, "avg_logprob": -0.22327265739440919, "compression_ratio": 1.5952380952380953, "no_speech_prob": 4.935809101880295e-06}, {"id": 280, "seek": 188600, "start": 1907.52, "end": 1912.72, "text": " autoencoder with good sparsity. And then what you do is you take the", "tokens": [50364, 1033, 11, 370, 309, 311, 2408, 670, 1594, 11, 2597, 11, 286, 818, 264, 1594, 430, 11, 558, 30, 8626, 670, 1594, 295, 3732, 5593, 295, 2408, 337, 50860, 50912, 741, 294, 264, 1594, 295, 710, 741, 8889, 13, 1033, 11, 370, 300, 311, 1594, 637, 685, 507, 13, 407, 291, 3847, 341, 707, 637, 11668, 51388, 51440, 8399, 22660, 19866, 365, 665, 637, 685, 507, 13, 400, 550, 437, 291, 360, 307, 291, 747, 264, 51700, 51772], "temperature": 0.0, "avg_logprob": -0.22327265739440919, "compression_ratio": 1.5952380952380953, "no_speech_prob": 4.935809101880295e-06}, {"id": 281, "seek": 191272, "start": 1912.72, "end": 1920.08, "text": " this group sparsity layer that you just used as a regularizer.", "tokens": [50364, 341, 1594, 637, 685, 507, 4583, 300, 291, 445, 1143, 382, 257, 3890, 6545, 13, 50732, 50824, 400, 370, 291, 1936, 13819, 11, 291, 1723, 341, 644, 484, 295, 264, 3209, 11, 291, 747, 264, 1594, 637, 685, 507, 11, 51316, 51316, 597, 307, 534, 257, 7005, 278, 4583, 11, 364, 441, 17, 7005, 278, 4583, 11, 293, 291, 2897, 309, 510, 13, 51556, 51632], "temperature": 0.0, "avg_logprob": -0.19035729365562326, "compression_ratio": 1.5657894736842106, "no_speech_prob": 1.873734390755999e-06}, {"id": 282, "seek": 191272, "start": 1921.92, "end": 1931.76, "text": " And so you basically eliminate, you cut this part out of the network, you take the group sparsity,", "tokens": [50364, 341, 1594, 637, 685, 507, 4583, 300, 291, 445, 1143, 382, 257, 3890, 6545, 13, 50732, 50824, 400, 370, 291, 1936, 13819, 11, 291, 1723, 341, 644, 484, 295, 264, 3209, 11, 291, 747, 264, 1594, 637, 685, 507, 11, 51316, 51316, 597, 307, 534, 257, 7005, 278, 4583, 11, 364, 441, 17, 7005, 278, 4583, 11, 293, 291, 2897, 309, 510, 13, 51556, 51632], "temperature": 0.0, "avg_logprob": -0.19035729365562326, "compression_ratio": 1.5657894736842106, "no_speech_prob": 1.873734390755999e-06}, {"id": 283, "seek": 191272, "start": 1931.76, "end": 1936.56, "text": " which is really a pooling layer, an L2 pooling layer, and you stick it here.", "tokens": [50364, 341, 1594, 637, 685, 507, 4583, 300, 291, 445, 1143, 382, 257, 3890, 6545, 13, 50732, 50824, 400, 370, 291, 1936, 13819, 11, 291, 1723, 341, 644, 484, 295, 264, 3209, 11, 291, 747, 264, 1594, 637, 685, 507, 11, 51316, 51316, 597, 307, 534, 257, 7005, 278, 4583, 11, 364, 441, 17, 7005, 278, 4583, 11, 293, 291, 2897, 309, 510, 13, 51556, 51632], "temperature": 0.0, "avg_logprob": -0.19035729365562326, "compression_ratio": 1.5657894736842106, "no_speech_prob": 1.873734390755999e-06}, {"id": 284, "seek": 193656, "start": 1936.56, "end": 1944.8799999999999, "text": " Okay, so this is basically L2 pooling. But it has the same architecture as the one you use for the", "tokens": [50364, 1033, 11, 370, 341, 307, 1936, 441, 17, 7005, 278, 13, 583, 309, 575, 264, 912, 9482, 382, 264, 472, 291, 764, 337, 264, 50780, 50880, 337, 264, 3935, 637, 685, 507, 13, 400, 550, 291, 764, 300, 382, 257, 4111, 8947, 284, 11, 51084, 51144, 597, 307, 411, 264, 700, 6119, 295, 7914, 295, 257, 45216, 304, 2533, 11, 45216, 304, 2158, 7005, 278, 13, 51396, 51432, 1033, 11, 457, 341, 307, 441, 17, 7005, 278, 11, 406, 11469, 7005, 278, 13, 400, 550, 291, 393, 7149, 264, 1399, 13, 509, 393, 3847, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.27946028417470503, "compression_ratio": 1.7095238095238094, "no_speech_prob": 1.221694765263237e-05}, {"id": 285, "seek": 193656, "start": 1946.8799999999999, "end": 1950.96, "text": " for the groups sparsity. And then you use that as a feature extractor,", "tokens": [50364, 1033, 11, 370, 341, 307, 1936, 441, 17, 7005, 278, 13, 583, 309, 575, 264, 912, 9482, 382, 264, 472, 291, 764, 337, 264, 50780, 50880, 337, 264, 3935, 637, 685, 507, 13, 400, 550, 291, 764, 300, 382, 257, 4111, 8947, 284, 11, 51084, 51144, 597, 307, 411, 264, 700, 6119, 295, 7914, 295, 257, 45216, 304, 2533, 11, 45216, 304, 2158, 7005, 278, 13, 51396, 51432, 1033, 11, 457, 341, 307, 441, 17, 7005, 278, 11, 406, 11469, 7005, 278, 13, 400, 550, 291, 393, 7149, 264, 1399, 13, 509, 393, 3847, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.27946028417470503, "compression_ratio": 1.7095238095238094, "no_speech_prob": 1.221694765263237e-05}, {"id": 286, "seek": 193656, "start": 1952.1599999999999, "end": 1957.2, "text": " which is like the first pair of layers of a convolutional net, convolutional value pooling.", "tokens": [50364, 1033, 11, 370, 341, 307, 1936, 441, 17, 7005, 278, 13, 583, 309, 575, 264, 912, 9482, 382, 264, 472, 291, 764, 337, 264, 50780, 50880, 337, 264, 3935, 637, 685, 507, 13, 400, 550, 291, 764, 300, 382, 257, 4111, 8947, 284, 11, 51084, 51144, 597, 307, 411, 264, 700, 6119, 295, 7914, 295, 257, 45216, 304, 2533, 11, 45216, 304, 2158, 7005, 278, 13, 51396, 51432, 1033, 11, 457, 341, 307, 441, 17, 7005, 278, 11, 406, 11469, 7005, 278, 13, 400, 550, 291, 393, 7149, 264, 1399, 13, 509, 393, 3847, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.27946028417470503, "compression_ratio": 1.7095238095238094, "no_speech_prob": 1.221694765263237e-05}, {"id": 287, "seek": 193656, "start": 1957.9199999999998, "end": 1962.6399999999999, "text": " Okay, but this is L2 pooling, not max pooling. And then you can repeat the process. You can train", "tokens": [50364, 1033, 11, 370, 341, 307, 1936, 441, 17, 7005, 278, 13, 583, 309, 575, 264, 912, 9482, 382, 264, 472, 291, 764, 337, 264, 50780, 50880, 337, 264, 3935, 637, 685, 507, 13, 400, 550, 291, 764, 300, 382, 257, 4111, 8947, 284, 11, 51084, 51144, 597, 307, 411, 264, 700, 6119, 295, 7914, 295, 257, 45216, 304, 2533, 11, 45216, 304, 2158, 7005, 278, 13, 51396, 51432, 1033, 11, 457, 341, 307, 441, 17, 7005, 278, 11, 406, 11469, 7005, 278, 13, 400, 550, 291, 393, 7149, 264, 1399, 13, 509, 393, 3847, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.27946028417470503, "compression_ratio": 1.7095238095238094, "no_speech_prob": 1.221694765263237e-05}, {"id": 288, "seek": 196264, "start": 1962.64, "end": 1974.5600000000002, "text": " another instance of this network, have a couple layers here. I'm going to have a decoder, have this", "tokens": [50364, 1071, 5197, 295, 341, 3209, 11, 362, 257, 1916, 7914, 510, 13, 286, 478, 516, 281, 362, 257, 979, 19866, 11, 362, 341, 50960, 50960, 441, 17, 7005, 278, 293, 637, 685, 507, 46691, 11, 3847, 341, 281, 31499, 1080, 4846, 11, 293, 550, 2897, 264, 7005, 278, 51452, 51452, 322, 1192, 11, 13819, 341, 11, 293, 586, 291, 362, 257, 659, 12, 17227, 2001, 732, 4583, 45216, 304, 2533, 13, 51720, 51772], "temperature": 0.0, "avg_logprob": -0.26694450378417967, "compression_ratio": 1.5666666666666667, "no_speech_prob": 1.8922663002740592e-05}, {"id": 289, "seek": 196264, "start": 1974.5600000000002, "end": 1984.4, "text": " L2 pooling and sparsity criterion, train this to reconstruct its input, and then stick the pooling", "tokens": [50364, 1071, 5197, 295, 341, 3209, 11, 362, 257, 1916, 7914, 510, 13, 286, 478, 516, 281, 362, 257, 979, 19866, 11, 362, 341, 50960, 50960, 441, 17, 7005, 278, 293, 637, 685, 507, 46691, 11, 3847, 341, 281, 31499, 1080, 4846, 11, 293, 550, 2897, 264, 7005, 278, 51452, 51452, 322, 1192, 11, 13819, 341, 11, 293, 586, 291, 362, 257, 659, 12, 17227, 2001, 732, 4583, 45216, 304, 2533, 13, 51720, 51772], "temperature": 0.0, "avg_logprob": -0.26694450378417967, "compression_ratio": 1.5666666666666667, "no_speech_prob": 1.8922663002740592e-05}, {"id": 290, "seek": 196264, "start": 1984.4, "end": 1989.76, "text": " on top, eliminate this, and now you have a pre-trained two layer convolutional net.", "tokens": [50364, 1071, 5197, 295, 341, 3209, 11, 362, 257, 1916, 7914, 510, 13, 286, 478, 516, 281, 362, 257, 979, 19866, 11, 362, 341, 50960, 50960, 441, 17, 7005, 278, 293, 637, 685, 507, 46691, 11, 3847, 341, 281, 31499, 1080, 4846, 11, 293, 550, 2897, 264, 7005, 278, 51452, 51452, 322, 1192, 11, 13819, 341, 11, 293, 586, 291, 362, 257, 659, 12, 17227, 2001, 732, 4583, 45216, 304, 2533, 13, 51720, 51772], "temperature": 0.0, "avg_logprob": -0.26694450378417967, "compression_ratio": 1.5666666666666667, "no_speech_prob": 1.8922663002740592e-05}, {"id": 291, "seek": 198976, "start": 1989.76, "end": 1995.92, "text": " This is a procedure that some people call stacked autoencoder. So you train an autoencoder to extract", "tokens": [50364, 639, 307, 257, 10747, 300, 512, 561, 818, 28867, 8399, 22660, 19866, 13, 407, 291, 3847, 364, 8399, 22660, 19866, 281, 8947, 50672, 50672, 4122, 11, 293, 550, 291, 8460, 4122, 365, 264, 2058, 19866, 295, 300, 644, 295, 300, 8399, 22660, 19866, 11, 51020, 51020, 293, 291, 2897, 1071, 4583, 322, 1192, 11, 3847, 300, 382, 364, 8399, 22660, 19866, 11, 293, 550, 1066, 516, 13, 51216, 51260, 400, 264, 787, 16282, 510, 307, 300, 341, 8399, 22660, 19866, 307, 8895, 365, 51428, 51504, 281, 5258, 33270, 394, 4122, 807, 1594, 637, 685, 507, 13, 51704, 51768], "temperature": 0.0, "avg_logprob": -0.21244579315185547, "compression_ratio": 1.880184331797235, "no_speech_prob": 5.013916052121203e-06}, {"id": 292, "seek": 198976, "start": 1995.92, "end": 2002.8799999999999, "text": " features, and then you generate features with the encoder of that part of that autoencoder,", "tokens": [50364, 639, 307, 257, 10747, 300, 512, 561, 818, 28867, 8399, 22660, 19866, 13, 407, 291, 3847, 364, 8399, 22660, 19866, 281, 8947, 50672, 50672, 4122, 11, 293, 550, 291, 8460, 4122, 365, 264, 2058, 19866, 295, 300, 644, 295, 300, 8399, 22660, 19866, 11, 51020, 51020, 293, 291, 2897, 1071, 4583, 322, 1192, 11, 3847, 300, 382, 364, 8399, 22660, 19866, 11, 293, 550, 1066, 516, 13, 51216, 51260, 400, 264, 787, 16282, 510, 307, 300, 341, 8399, 22660, 19866, 307, 8895, 365, 51428, 51504, 281, 5258, 33270, 394, 4122, 807, 1594, 637, 685, 507, 13, 51704, 51768], "temperature": 0.0, "avg_logprob": -0.21244579315185547, "compression_ratio": 1.880184331797235, "no_speech_prob": 5.013916052121203e-06}, {"id": 293, "seek": 198976, "start": 2002.8799999999999, "end": 2006.8, "text": " and you stick another layer on top, train that as an autoencoder, and then keep going.", "tokens": [50364, 639, 307, 257, 10747, 300, 512, 561, 818, 28867, 8399, 22660, 19866, 13, 407, 291, 3847, 364, 8399, 22660, 19866, 281, 8947, 50672, 50672, 4122, 11, 293, 550, 291, 8460, 4122, 365, 264, 2058, 19866, 295, 300, 644, 295, 300, 8399, 22660, 19866, 11, 51020, 51020, 293, 291, 2897, 1071, 4583, 322, 1192, 11, 3847, 300, 382, 364, 8399, 22660, 19866, 11, 293, 550, 1066, 516, 13, 51216, 51260, 400, 264, 787, 16282, 510, 307, 300, 341, 8399, 22660, 19866, 307, 8895, 365, 51428, 51504, 281, 5258, 33270, 394, 4122, 807, 1594, 637, 685, 507, 13, 51704, 51768], "temperature": 0.0, "avg_logprob": -0.21244579315185547, "compression_ratio": 1.880184331797235, "no_speech_prob": 5.013916052121203e-06}, {"id": 294, "seek": 198976, "start": 2007.68, "end": 2011.04, "text": " And the only characteristic here is that this autoencoder is trained with", "tokens": [50364, 639, 307, 257, 10747, 300, 512, 561, 818, 28867, 8399, 22660, 19866, 13, 407, 291, 3847, 364, 8399, 22660, 19866, 281, 8947, 50672, 50672, 4122, 11, 293, 550, 291, 8460, 4122, 365, 264, 2058, 19866, 295, 300, 644, 295, 300, 8399, 22660, 19866, 11, 51020, 51020, 293, 291, 2897, 1071, 4583, 322, 1192, 11, 3847, 300, 382, 364, 8399, 22660, 19866, 11, 293, 550, 1066, 516, 13, 51216, 51260, 400, 264, 787, 16282, 510, 307, 300, 341, 8399, 22660, 19866, 307, 8895, 365, 51428, 51504, 281, 5258, 33270, 394, 4122, 807, 1594, 637, 685, 507, 13, 51704, 51768], "temperature": 0.0, "avg_logprob": -0.21244579315185547, "compression_ratio": 1.880184331797235, "no_speech_prob": 5.013916052121203e-06}, {"id": 295, "seek": 198976, "start": 2012.56, "end": 2016.56, "text": " to produce invariant features through group sparsity.", "tokens": [50364, 639, 307, 257, 10747, 300, 512, 561, 818, 28867, 8399, 22660, 19866, 13, 407, 291, 3847, 364, 8399, 22660, 19866, 281, 8947, 50672, 50672, 4122, 11, 293, 550, 291, 8460, 4122, 365, 264, 2058, 19866, 295, 300, 644, 295, 300, 8399, 22660, 19866, 11, 51020, 51020, 293, 291, 2897, 1071, 4583, 322, 1192, 11, 3847, 300, 382, 364, 8399, 22660, 19866, 11, 293, 550, 1066, 516, 13, 51216, 51260, 400, 264, 787, 16282, 510, 307, 300, 341, 8399, 22660, 19866, 307, 8895, 365, 51428, 51504, 281, 5258, 33270, 394, 4122, 807, 1594, 637, 685, 507, 13, 51704, 51768], "temperature": 0.0, "avg_logprob": -0.21244579315185547, "compression_ratio": 1.880184331797235, "no_speech_prob": 5.013916052121203e-06}, {"id": 296, "seek": 201656, "start": 2016.56, "end": 2020.3999999999999, "text": " Do we use all possible sub-trees as groups in the previous example?", "tokens": [50364, 1144, 321, 764, 439, 1944, 1422, 12, 3599, 279, 382, 3935, 294, 264, 3894, 1365, 30, 50556, 50604, 883, 11, 300, 311, 733, 295, 493, 281, 291, 11, 534, 13, 708, 3877, 291, 764, 510, 11, 291, 393, 764, 3866, 5852, 11, 50900, 50900, 291, 393, 764, 498, 291, 528, 3866, 4122, 281, 733, 295, 312, 281, 2906, 364, 4846, 754, 412, 2295, 7893, 13, 51200, 51200, 407, 300, 311, 534, 493, 281, 291, 13, 467, 727, 312, 411, 437, 291, 393, 6157, 13, 708, 291, 393, 360, 611, 307, 3847, 264, 51552, 51552, 1185, 365, 257, 3801, 4230, 300, 307, 544, 14679, 11, 300, 307, 544, 14679, 11, 300, 307, 544, 14679, 11, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.36624375036207296, "compression_ratio": 1.8495934959349594, "no_speech_prob": 3.525289866956882e-05}, {"id": 297, "seek": 201656, "start": 2021.36, "end": 2027.28, "text": " No, that's kind of up to you, really. What structure you use here, you can use multiple trees,", "tokens": [50364, 1144, 321, 764, 439, 1944, 1422, 12, 3599, 279, 382, 3935, 294, 264, 3894, 1365, 30, 50556, 50604, 883, 11, 300, 311, 733, 295, 493, 281, 291, 11, 534, 13, 708, 3877, 291, 764, 510, 11, 291, 393, 764, 3866, 5852, 11, 50900, 50900, 291, 393, 764, 498, 291, 528, 3866, 4122, 281, 733, 295, 312, 281, 2906, 364, 4846, 754, 412, 2295, 7893, 13, 51200, 51200, 407, 300, 311, 534, 493, 281, 291, 13, 467, 727, 312, 411, 437, 291, 393, 6157, 13, 708, 291, 393, 360, 611, 307, 3847, 264, 51552, 51552, 1185, 365, 257, 3801, 4230, 300, 307, 544, 14679, 11, 300, 307, 544, 14679, 11, 300, 307, 544, 14679, 11, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.36624375036207296, "compression_ratio": 1.8495934959349594, "no_speech_prob": 3.525289866956882e-05}, {"id": 298, "seek": 201656, "start": 2027.28, "end": 2033.28, "text": " you can use if you want multiple features to kind of be to represent an input even at low frequency.", "tokens": [50364, 1144, 321, 764, 439, 1944, 1422, 12, 3599, 279, 382, 3935, 294, 264, 3894, 1365, 30, 50556, 50604, 883, 11, 300, 311, 733, 295, 493, 281, 291, 11, 534, 13, 708, 3877, 291, 764, 510, 11, 291, 393, 764, 3866, 5852, 11, 50900, 50900, 291, 393, 764, 498, 291, 528, 3866, 4122, 281, 733, 295, 312, 281, 2906, 364, 4846, 754, 412, 2295, 7893, 13, 51200, 51200, 407, 300, 311, 534, 493, 281, 291, 13, 467, 727, 312, 411, 437, 291, 393, 6157, 13, 708, 291, 393, 360, 611, 307, 3847, 264, 51552, 51552, 1185, 365, 257, 3801, 4230, 300, 307, 544, 14679, 11, 300, 307, 544, 14679, 11, 300, 307, 544, 14679, 11, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.36624375036207296, "compression_ratio": 1.8495934959349594, "no_speech_prob": 3.525289866956882e-05}, {"id": 299, "seek": 201656, "start": 2033.28, "end": 2040.32, "text": " So that's really up to you. It could be like what you can afford. What you can do also is train the", "tokens": [50364, 1144, 321, 764, 439, 1944, 1422, 12, 3599, 279, 382, 3935, 294, 264, 3894, 1365, 30, 50556, 50604, 883, 11, 300, 311, 733, 295, 493, 281, 291, 11, 534, 13, 708, 3877, 291, 764, 510, 11, 291, 393, 764, 3866, 5852, 11, 50900, 50900, 291, 393, 764, 498, 291, 528, 3866, 4122, 281, 733, 295, 312, 281, 2906, 364, 4846, 754, 412, 2295, 7893, 13, 51200, 51200, 407, 300, 311, 534, 493, 281, 291, 13, 467, 727, 312, 411, 437, 291, 393, 6157, 13, 708, 291, 393, 360, 611, 307, 3847, 264, 51552, 51552, 1185, 365, 257, 3801, 4230, 300, 307, 544, 14679, 11, 300, 307, 544, 14679, 11, 300, 307, 544, 14679, 11, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.36624375036207296, "compression_ratio": 1.8495934959349594, "no_speech_prob": 3.525289866956882e-05}, {"id": 300, "seek": 201656, "start": 2040.32, "end": 2046.48, "text": " system with a bigger tree that is more compact, that is more compact, that is more compact,", "tokens": [50364, 1144, 321, 764, 439, 1944, 1422, 12, 3599, 279, 382, 3935, 294, 264, 3894, 1365, 30, 50556, 50604, 883, 11, 300, 311, 733, 295, 493, 281, 291, 11, 534, 13, 708, 3877, 291, 764, 510, 11, 291, 393, 764, 3866, 5852, 11, 50900, 50900, 291, 393, 764, 498, 291, 528, 3866, 4122, 281, 733, 295, 312, 281, 2906, 364, 4846, 754, 412, 2295, 7893, 13, 51200, 51200, 407, 300, 311, 534, 493, 281, 291, 13, 467, 727, 312, 411, 437, 291, 393, 6157, 13, 708, 291, 393, 360, 611, 307, 3847, 264, 51552, 51552, 1185, 365, 257, 3801, 4230, 300, 307, 544, 14679, 11, 300, 307, 544, 14679, 11, 300, 307, 544, 14679, 11, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.36624375036207296, "compression_ratio": 1.8495934959349594, "no_speech_prob": 3.525289866956882e-05}, {"id": 301, "seek": 204648, "start": 2046.48, "end": 2048.48, "text": " that is more compact, and then kind of cut down on the tree than necessary, and then sort of prune", "tokens": [50364, 300, 307, 544, 14679, 11, 293, 550, 733, 295, 1723, 760, 322, 264, 4230, 813, 4818, 11, 293, 550, 1333, 295, 582, 2613, 50464, 50464, 264, 4230, 5699, 456, 366, 14770, 300, 366, 406, 1143, 11, 420, 1143, 588, 13752, 13, 50664, 50720, 1033, 11, 370, 341, 307, 11, 264, 5120, 286, 4712, 510, 307, 2531, 457, 307, 787, 2654, 9271, 293, 572, 51012, 51076, 3364, 5414, 13, 400, 437, 291, 536, 510, 307, 341, 11, 797, 11, 717, 12372, 2144, 295, 264, 4122, 51424, 51492, 294, 2115, 295, 437, 28813, 5412, 1751, 818, 5447, 22830, 8294, 13, 407, 5447, 22830, 8294, 366, 729, 8294, 51744, 51744], "temperature": 0.6, "avg_logprob": -0.48783073075320743, "compression_ratio": 1.7224334600760456, "no_speech_prob": 2.1773415937786922e-05}, {"id": 302, "seek": 204648, "start": 2048.48, "end": 2052.48, "text": " the tree whenever there are branches that are not used, or used very rarely.", "tokens": [50364, 300, 307, 544, 14679, 11, 293, 550, 733, 295, 1723, 760, 322, 264, 4230, 813, 4818, 11, 293, 550, 1333, 295, 582, 2613, 50464, 50464, 264, 4230, 5699, 456, 366, 14770, 300, 366, 406, 1143, 11, 420, 1143, 588, 13752, 13, 50664, 50720, 1033, 11, 370, 341, 307, 11, 264, 5120, 286, 4712, 510, 307, 2531, 457, 307, 787, 2654, 9271, 293, 572, 51012, 51076, 3364, 5414, 13, 400, 437, 291, 536, 510, 307, 341, 11, 797, 11, 717, 12372, 2144, 295, 264, 4122, 51424, 51492, 294, 2115, 295, 437, 28813, 5412, 1751, 818, 5447, 22830, 8294, 13, 407, 5447, 22830, 8294, 366, 729, 8294, 51744, 51744], "temperature": 0.6, "avg_logprob": -0.48783073075320743, "compression_ratio": 1.7224334600760456, "no_speech_prob": 2.1773415937786922e-05}, {"id": 303, "seek": 204648, "start": 2053.6, "end": 2059.44, "text": " Okay, so this is, the experiment I showed here is similar but is only local connections and no", "tokens": [50364, 300, 307, 544, 14679, 11, 293, 550, 733, 295, 1723, 760, 322, 264, 4230, 813, 4818, 11, 293, 550, 1333, 295, 582, 2613, 50464, 50464, 264, 4230, 5699, 456, 366, 14770, 300, 366, 406, 1143, 11, 420, 1143, 588, 13752, 13, 50664, 50720, 1033, 11, 370, 341, 307, 11, 264, 5120, 286, 4712, 510, 307, 2531, 457, 307, 787, 2654, 9271, 293, 572, 51012, 51076, 3364, 5414, 13, 400, 437, 291, 536, 510, 307, 341, 11, 797, 11, 717, 12372, 2144, 295, 264, 4122, 51424, 51492, 294, 2115, 295, 437, 28813, 5412, 1751, 818, 5447, 22830, 8294, 13, 407, 5447, 22830, 8294, 366, 729, 8294, 51744, 51744], "temperature": 0.6, "avg_logprob": -0.48783073075320743, "compression_ratio": 1.7224334600760456, "no_speech_prob": 2.1773415937786922e-05}, {"id": 304, "seek": 204648, "start": 2060.72, "end": 2067.68, "text": " weight sharing. And what you see here is this, again, disorganization of the features", "tokens": [50364, 300, 307, 544, 14679, 11, 293, 550, 733, 295, 1723, 760, 322, 264, 4230, 813, 4818, 11, 293, 550, 1333, 295, 582, 2613, 50464, 50464, 264, 4230, 5699, 456, 366, 14770, 300, 366, 406, 1143, 11, 420, 1143, 588, 13752, 13, 50664, 50720, 1033, 11, 370, 341, 307, 11, 264, 5120, 286, 4712, 510, 307, 2531, 457, 307, 787, 2654, 9271, 293, 572, 51012, 51076, 3364, 5414, 13, 400, 437, 291, 536, 510, 307, 341, 11, 797, 11, 717, 12372, 2144, 295, 264, 4122, 51424, 51492, 294, 2115, 295, 437, 28813, 5412, 1751, 818, 5447, 22830, 8294, 13, 407, 5447, 22830, 8294, 366, 729, 8294, 51744, 51744], "temperature": 0.6, "avg_logprob": -0.48783073075320743, "compression_ratio": 1.7224334600760456, "no_speech_prob": 2.1773415937786922e-05}, {"id": 305, "seek": 204648, "start": 2069.04, "end": 2074.08, "text": " in terms of what neuroscientists call pinwheel patterns. So pinwheel patterns are those patterns", "tokens": [50364, 300, 307, 544, 14679, 11, 293, 550, 733, 295, 1723, 760, 322, 264, 4230, 813, 4818, 11, 293, 550, 1333, 295, 582, 2613, 50464, 50464, 264, 4230, 5699, 456, 366, 14770, 300, 366, 406, 1143, 11, 420, 1143, 588, 13752, 13, 50664, 50720, 1033, 11, 370, 341, 307, 11, 264, 5120, 286, 4712, 510, 307, 2531, 457, 307, 787, 2654, 9271, 293, 572, 51012, 51076, 3364, 5414, 13, 400, 437, 291, 536, 510, 307, 341, 11, 797, 11, 717, 12372, 2144, 295, 264, 4122, 51424, 51492, 294, 2115, 295, 437, 28813, 5412, 1751, 818, 5447, 22830, 8294, 13, 407, 5447, 22830, 8294, 366, 729, 8294, 51744, 51744], "temperature": 0.6, "avg_logprob": -0.48783073075320743, "compression_ratio": 1.7224334600760456, "no_speech_prob": 2.1773415937786922e-05}, {"id": 306, "seek": 207408, "start": 2074.08, "end": 2077.92, "text": " where the orientation selectivity varies continuously", "tokens": [50364, 689, 264, 14764, 3048, 4253, 21716, 15684, 50556, 50556, 382, 291, 352, 926, 472, 295, 729, 2182, 15026, 13, 50764, 50764, 407, 291, 747, 472, 295, 729, 2182, 15026, 11, 50842, 50842, 293, 498, 291, 733, 295, 360, 257, 707, 6329, 926, 264, 2182, 15026, 11, 51034, 51034, 437, 291, 3449, 307, 300, 264, 14764, 51150, 51150, 295, 264, 4691, 8947, 284, 733, 295, 21716, 15684, 51319, 51319, 382, 291, 1286, 926, 13, 51398, 51398, 400, 729, 366, 1219, 5447, 22830, 8294, 13, 51500, 51500, 400, 436, 366, 13095, 294, 264, 3567, 13, 51604, 51808], "temperature": 0.0, "avg_logprob": -0.18031661043462066, "compression_ratio": 1.865, "no_speech_prob": 0.00044405439984984696}, {"id": 307, "seek": 207408, "start": 2077.92, "end": 2082.08, "text": " as you go around one of those red dots.", "tokens": [50364, 689, 264, 14764, 3048, 4253, 21716, 15684, 50556, 50556, 382, 291, 352, 926, 472, 295, 729, 2182, 15026, 13, 50764, 50764, 407, 291, 747, 472, 295, 729, 2182, 15026, 11, 50842, 50842, 293, 498, 291, 733, 295, 360, 257, 707, 6329, 926, 264, 2182, 15026, 11, 51034, 51034, 437, 291, 3449, 307, 300, 264, 14764, 51150, 51150, 295, 264, 4691, 8947, 284, 733, 295, 21716, 15684, 51319, 51319, 382, 291, 1286, 926, 13, 51398, 51398, 400, 729, 366, 1219, 5447, 22830, 8294, 13, 51500, 51500, 400, 436, 366, 13095, 294, 264, 3567, 13, 51604, 51808], "temperature": 0.0, "avg_logprob": -0.18031661043462066, "compression_ratio": 1.865, "no_speech_prob": 0.00044405439984984696}, {"id": 308, "seek": 207408, "start": 2082.08, "end": 2083.64, "text": " So you take one of those red dots,", "tokens": [50364, 689, 264, 14764, 3048, 4253, 21716, 15684, 50556, 50556, 382, 291, 352, 926, 472, 295, 729, 2182, 15026, 13, 50764, 50764, 407, 291, 747, 472, 295, 729, 2182, 15026, 11, 50842, 50842, 293, 498, 291, 733, 295, 360, 257, 707, 6329, 926, 264, 2182, 15026, 11, 51034, 51034, 437, 291, 3449, 307, 300, 264, 14764, 51150, 51150, 295, 264, 4691, 8947, 284, 733, 295, 21716, 15684, 51319, 51319, 382, 291, 1286, 926, 13, 51398, 51398, 400, 729, 366, 1219, 5447, 22830, 8294, 13, 51500, 51500, 400, 436, 366, 13095, 294, 264, 3567, 13, 51604, 51808], "temperature": 0.0, "avg_logprob": -0.18031661043462066, "compression_ratio": 1.865, "no_speech_prob": 0.00044405439984984696}, {"id": 309, "seek": 207408, "start": 2083.64, "end": 2087.48, "text": " and if you kind of do a little circle around the red dots,", "tokens": [50364, 689, 264, 14764, 3048, 4253, 21716, 15684, 50556, 50556, 382, 291, 352, 926, 472, 295, 729, 2182, 15026, 13, 50764, 50764, 407, 291, 747, 472, 295, 729, 2182, 15026, 11, 50842, 50842, 293, 498, 291, 733, 295, 360, 257, 707, 6329, 926, 264, 2182, 15026, 11, 51034, 51034, 437, 291, 3449, 307, 300, 264, 14764, 51150, 51150, 295, 264, 4691, 8947, 284, 733, 295, 21716, 15684, 51319, 51319, 382, 291, 1286, 926, 13, 51398, 51398, 400, 729, 366, 1219, 5447, 22830, 8294, 13, 51500, 51500, 400, 436, 366, 13095, 294, 264, 3567, 13, 51604, 51808], "temperature": 0.0, "avg_logprob": -0.18031661043462066, "compression_ratio": 1.865, "no_speech_prob": 0.00044405439984984696}, {"id": 310, "seek": 207408, "start": 2087.48, "end": 2089.7999999999997, "text": " what you notice is that the orientation", "tokens": [50364, 689, 264, 14764, 3048, 4253, 21716, 15684, 50556, 50556, 382, 291, 352, 926, 472, 295, 729, 2182, 15026, 13, 50764, 50764, 407, 291, 747, 472, 295, 729, 2182, 15026, 11, 50842, 50842, 293, 498, 291, 733, 295, 360, 257, 707, 6329, 926, 264, 2182, 15026, 11, 51034, 51034, 437, 291, 3449, 307, 300, 264, 14764, 51150, 51150, 295, 264, 4691, 8947, 284, 733, 295, 21716, 15684, 51319, 51319, 382, 291, 1286, 926, 13, 51398, 51398, 400, 729, 366, 1219, 5447, 22830, 8294, 13, 51500, 51500, 400, 436, 366, 13095, 294, 264, 3567, 13, 51604, 51808], "temperature": 0.0, "avg_logprob": -0.18031661043462066, "compression_ratio": 1.865, "no_speech_prob": 0.00044405439984984696}, {"id": 311, "seek": 207408, "start": 2089.7999999999997, "end": 2093.18, "text": " of the edge extractor kind of varies continuously", "tokens": [50364, 689, 264, 14764, 3048, 4253, 21716, 15684, 50556, 50556, 382, 291, 352, 926, 472, 295, 729, 2182, 15026, 13, 50764, 50764, 407, 291, 747, 472, 295, 729, 2182, 15026, 11, 50842, 50842, 293, 498, 291, 733, 295, 360, 257, 707, 6329, 926, 264, 2182, 15026, 11, 51034, 51034, 437, 291, 3449, 307, 300, 264, 14764, 51150, 51150, 295, 264, 4691, 8947, 284, 733, 295, 21716, 15684, 51319, 51319, 382, 291, 1286, 926, 13, 51398, 51398, 400, 729, 366, 1219, 5447, 22830, 8294, 13, 51500, 51500, 400, 436, 366, 13095, 294, 264, 3567, 13, 51604, 51808], "temperature": 0.0, "avg_logprob": -0.18031661043462066, "compression_ratio": 1.865, "no_speech_prob": 0.00044405439984984696}, {"id": 312, "seek": 207408, "start": 2093.18, "end": 2094.7599999999998, "text": " as you move around.", "tokens": [50364, 689, 264, 14764, 3048, 4253, 21716, 15684, 50556, 50556, 382, 291, 352, 926, 472, 295, 729, 2182, 15026, 13, 50764, 50764, 407, 291, 747, 472, 295, 729, 2182, 15026, 11, 50842, 50842, 293, 498, 291, 733, 295, 360, 257, 707, 6329, 926, 264, 2182, 15026, 11, 51034, 51034, 437, 291, 3449, 307, 300, 264, 14764, 51150, 51150, 295, 264, 4691, 8947, 284, 733, 295, 21716, 15684, 51319, 51319, 382, 291, 1286, 926, 13, 51398, 51398, 400, 729, 366, 1219, 5447, 22830, 8294, 13, 51500, 51500, 400, 436, 366, 13095, 294, 264, 3567, 13, 51604, 51808], "temperature": 0.0, "avg_logprob": -0.18031661043462066, "compression_ratio": 1.865, "no_speech_prob": 0.00044405439984984696}, {"id": 313, "seek": 207408, "start": 2094.7599999999998, "end": 2096.7999999999997, "text": " And those are called pinwheel patterns.", "tokens": [50364, 689, 264, 14764, 3048, 4253, 21716, 15684, 50556, 50556, 382, 291, 352, 926, 472, 295, 729, 2182, 15026, 13, 50764, 50764, 407, 291, 747, 472, 295, 729, 2182, 15026, 11, 50842, 50842, 293, 498, 291, 733, 295, 360, 257, 707, 6329, 926, 264, 2182, 15026, 11, 51034, 51034, 437, 291, 3449, 307, 300, 264, 14764, 51150, 51150, 295, 264, 4691, 8947, 284, 733, 295, 21716, 15684, 51319, 51319, 382, 291, 1286, 926, 13, 51398, 51398, 400, 729, 366, 1219, 5447, 22830, 8294, 13, 51500, 51500, 400, 436, 366, 13095, 294, 264, 3567, 13, 51604, 51808], "temperature": 0.0, "avg_logprob": -0.18031661043462066, "compression_ratio": 1.865, "no_speech_prob": 0.00044405439984984696}, {"id": 314, "seek": 207408, "start": 2096.7999999999997, "end": 2098.88, "text": " And they are observed in the brain.", "tokens": [50364, 689, 264, 14764, 3048, 4253, 21716, 15684, 50556, 50556, 382, 291, 352, 926, 472, 295, 729, 2182, 15026, 13, 50764, 50764, 407, 291, 747, 472, 295, 729, 2182, 15026, 11, 50842, 50842, 293, 498, 291, 733, 295, 360, 257, 707, 6329, 926, 264, 2182, 15026, 11, 51034, 51034, 437, 291, 3449, 307, 300, 264, 14764, 51150, 51150, 295, 264, 4691, 8947, 284, 733, 295, 21716, 15684, 51319, 51319, 382, 291, 1286, 926, 13, 51398, 51398, 400, 729, 366, 1219, 5447, 22830, 8294, 13, 51500, 51500, 400, 436, 366, 13095, 294, 264, 3567, 13, 51604, 51808], "temperature": 0.0, "avg_logprob": -0.18031661043462066, "compression_ratio": 1.865, "no_speech_prob": 0.00044405439984984696}, {"id": 315, "seek": 209888, "start": 2098.88, "end": 2103.88, "text": " In fact, those pictures here on the right", "tokens": [50364, 682, 1186, 11, 729, 5242, 510, 322, 264, 558, 50614, 50674, 808, 490, 42762, 10577, 300, 6786, 341, 50844, 50844, 689, 264, 2017, 510, 2058, 4789, 264, 14764, 3048, 4253, 13, 51020, 51104, 440, 707, 6105, 13330, 729, 733, 295, 264, 20010, 1088, 510, 11, 51354, 51470, 264, 3056, 295, 264, 5447, 17098, 1625, 13, 51572, 51572, 1119, 264, 1594, 637, 685, 507, 1433, 3847, 281, 362, 257, 1359, 2158, 30, 51739, 51782], "temperature": 0.0, "avg_logprob": -0.24758087158203124, "compression_ratio": 1.5714285714285714, "no_speech_prob": 4.5308963308343664e-05}, {"id": 316, "seek": 209888, "start": 2105.08, "end": 2108.48, "text": " come from neuroscience papers that describe this", "tokens": [50364, 682, 1186, 11, 729, 5242, 510, 322, 264, 558, 50614, 50674, 808, 490, 42762, 10577, 300, 6786, 341, 50844, 50844, 689, 264, 2017, 510, 2058, 4789, 264, 14764, 3048, 4253, 13, 51020, 51104, 440, 707, 6105, 13330, 729, 733, 295, 264, 20010, 1088, 510, 11, 51354, 51470, 264, 3056, 295, 264, 5447, 17098, 1625, 13, 51572, 51572, 1119, 264, 1594, 637, 685, 507, 1433, 3847, 281, 362, 257, 1359, 2158, 30, 51739, 51782], "temperature": 0.0, "avg_logprob": -0.24758087158203124, "compression_ratio": 1.5714285714285714, "no_speech_prob": 4.5308963308343664e-05}, {"id": 317, "seek": 209888, "start": 2108.48, "end": 2112.0, "text": " where the color here encodes the orientation selectivity.", "tokens": [50364, 682, 1186, 11, 729, 5242, 510, 322, 264, 558, 50614, 50674, 808, 490, 42762, 10577, 300, 6786, 341, 50844, 50844, 689, 264, 2017, 510, 2058, 4789, 264, 14764, 3048, 4253, 13, 51020, 51104, 440, 707, 6105, 13330, 729, 733, 295, 264, 20010, 1088, 510, 11, 51354, 51470, 264, 3056, 295, 264, 5447, 17098, 1625, 13, 51572, 51572, 1119, 264, 1594, 637, 685, 507, 1433, 3847, 281, 362, 257, 1359, 2158, 30, 51739, 51782], "temperature": 0.0, "avg_logprob": -0.24758087158203124, "compression_ratio": 1.5714285714285714, "no_speech_prob": 4.5308963308343664e-05}, {"id": 318, "seek": 209888, "start": 2113.6800000000003, "end": 2118.6800000000003, "text": " The little stars indicate those kind of the singularities here,", "tokens": [50364, 682, 1186, 11, 729, 5242, 510, 322, 264, 558, 50614, 50674, 808, 490, 42762, 10577, 300, 6786, 341, 50844, 50844, 689, 264, 2017, 510, 2058, 4789, 264, 14764, 3048, 4253, 13, 51020, 51104, 440, 707, 6105, 13330, 729, 733, 295, 264, 20010, 1088, 510, 11, 51354, 51470, 264, 3056, 295, 264, 5447, 17098, 1625, 13, 51572, 51572, 1119, 264, 1594, 637, 685, 507, 1433, 3847, 281, 362, 257, 1359, 2158, 30, 51739, 51782], "temperature": 0.0, "avg_logprob": -0.24758087158203124, "compression_ratio": 1.5714285714285714, "no_speech_prob": 4.5308963308343664e-05}, {"id": 319, "seek": 209888, "start": 2121.0, "end": 2123.04, "text": " the center of the pinwheels.", "tokens": [50364, 682, 1186, 11, 729, 5242, 510, 322, 264, 558, 50614, 50674, 808, 490, 42762, 10577, 300, 6786, 341, 50844, 50844, 689, 264, 2017, 510, 2058, 4789, 264, 14764, 3048, 4253, 13, 51020, 51104, 440, 707, 6105, 13330, 729, 733, 295, 264, 20010, 1088, 510, 11, 51354, 51470, 264, 3056, 295, 264, 5447, 17098, 1625, 13, 51572, 51572, 1119, 264, 1594, 637, 685, 507, 1433, 3847, 281, 362, 257, 1359, 2158, 30, 51739, 51782], "temperature": 0.0, "avg_logprob": -0.24758087158203124, "compression_ratio": 1.5714285714285714, "no_speech_prob": 4.5308963308343664e-05}, {"id": 320, "seek": 209888, "start": 2123.04, "end": 2126.38, "text": " Is the group sparsity term train to have a small value?", "tokens": [50364, 682, 1186, 11, 729, 5242, 510, 322, 264, 558, 50614, 50674, 808, 490, 42762, 10577, 300, 6786, 341, 50844, 50844, 689, 264, 2017, 510, 2058, 4789, 264, 14764, 3048, 4253, 13, 51020, 51104, 440, 707, 6105, 13330, 729, 733, 295, 264, 20010, 1088, 510, 11, 51354, 51470, 264, 3056, 295, 264, 5447, 17098, 1625, 13, 51572, 51572, 1119, 264, 1594, 637, 685, 507, 1433, 3847, 281, 362, 257, 1359, 2158, 30, 51739, 51782], "temperature": 0.0, "avg_logprob": -0.24758087158203124, "compression_ratio": 1.5714285714285714, "no_speech_prob": 4.5308963308343664e-05}, {"id": 321, "seek": 212638, "start": 2126.38, "end": 2129.7000000000003, "text": " Well, it's a regularizer, right?", "tokens": [50364, 1042, 11, 309, 311, 257, 3890, 6545, 11, 558, 30, 50530, 50530, 961, 385, 352, 646, 281, 264, 485, 50660, 50784, 467, 311, 257, 2063, 2445, 1830, 3097, 420, 1830, 38253, 51034, 51034, 5413, 322, 1968, 291, 764, 264, 1333, 295, 35521, 3037, 51284, 51298, 295, 309, 689, 291, 362, 257, 7006, 420, 406, 13, 51384, 51384, 583, 309, 311, 1936, 445, 257, 1433, 295, 264, 2281, 11, 558, 30, 51634, 51810], "temperature": 0.0, "avg_logprob": -0.2570692010827967, "compression_ratio": 1.467032967032967, "no_speech_prob": 4.092298240720993e-06}, {"id": 322, "seek": 212638, "start": 2129.7000000000003, "end": 2132.3, "text": " Let me go back to the...", "tokens": [50364, 1042, 11, 309, 311, 257, 3890, 6545, 11, 558, 30, 50530, 50530, 961, 385, 352, 646, 281, 264, 485, 50660, 50784, 467, 311, 257, 2063, 2445, 1830, 3097, 420, 1830, 38253, 51034, 51034, 5413, 322, 1968, 291, 764, 264, 1333, 295, 35521, 3037, 51284, 51298, 295, 309, 689, 291, 362, 257, 7006, 420, 406, 13, 51384, 51384, 583, 309, 311, 1936, 445, 257, 1433, 295, 264, 2281, 11, 558, 30, 51634, 51810], "temperature": 0.0, "avg_logprob": -0.2570692010827967, "compression_ratio": 1.467032967032967, "no_speech_prob": 4.092298240720993e-06}, {"id": 323, "seek": 212638, "start": 2134.78, "end": 2139.78, "text": " It's a cost function during training or during inference", "tokens": [50364, 1042, 11, 309, 311, 257, 3890, 6545, 11, 558, 30, 50530, 50530, 961, 385, 352, 646, 281, 264, 485, 50660, 50784, 467, 311, 257, 2063, 2445, 1830, 3097, 420, 1830, 38253, 51034, 51034, 5413, 322, 1968, 291, 764, 264, 1333, 295, 35521, 3037, 51284, 51298, 295, 309, 689, 291, 362, 257, 7006, 420, 406, 13, 51384, 51384, 583, 309, 311, 1936, 445, 257, 1433, 295, 264, 2281, 11, 558, 30, 51634, 51810], "temperature": 0.0, "avg_logprob": -0.2570692010827967, "compression_ratio": 1.467032967032967, "no_speech_prob": 4.092298240720993e-06}, {"id": 324, "seek": 212638, "start": 2139.78, "end": 2144.78, "text": " depending on whether you use the sort of predictive version", "tokens": [50364, 1042, 11, 309, 311, 257, 3890, 6545, 11, 558, 30, 50530, 50530, 961, 385, 352, 646, 281, 264, 485, 50660, 50784, 467, 311, 257, 2063, 2445, 1830, 3097, 420, 1830, 38253, 51034, 51034, 5413, 322, 1968, 291, 764, 264, 1333, 295, 35521, 3037, 51284, 51298, 295, 309, 689, 291, 362, 257, 7006, 420, 406, 13, 51384, 51384, 583, 309, 311, 1936, 445, 257, 1433, 295, 264, 2281, 11, 558, 30, 51634, 51810], "temperature": 0.0, "avg_logprob": -0.2570692010827967, "compression_ratio": 1.467032967032967, "no_speech_prob": 4.092298240720993e-06}, {"id": 325, "seek": 212638, "start": 2145.06, "end": 2146.78, "text": " of it where you have a variable or not.", "tokens": [50364, 1042, 11, 309, 311, 257, 3890, 6545, 11, 558, 30, 50530, 50530, 961, 385, 352, 646, 281, 264, 485, 50660, 50784, 467, 311, 257, 2063, 2445, 1830, 3097, 420, 1830, 38253, 51034, 51034, 5413, 322, 1968, 291, 764, 264, 1333, 295, 35521, 3037, 51284, 51298, 295, 309, 689, 291, 362, 257, 7006, 420, 406, 13, 51384, 51384, 583, 309, 311, 1936, 445, 257, 1433, 295, 264, 2281, 11, 558, 30, 51634, 51810], "temperature": 0.0, "avg_logprob": -0.2570692010827967, "compression_ratio": 1.467032967032967, "no_speech_prob": 4.092298240720993e-06}, {"id": 326, "seek": 212638, "start": 2146.78, "end": 2151.78, "text": " But it's basically just a term of the energy, right?", "tokens": [50364, 1042, 11, 309, 311, 257, 3890, 6545, 11, 558, 30, 50530, 50530, 961, 385, 352, 646, 281, 264, 485, 50660, 50784, 467, 311, 257, 2063, 2445, 1830, 3097, 420, 1830, 38253, 51034, 51034, 5413, 322, 1968, 291, 764, 264, 1333, 295, 35521, 3037, 51284, 51298, 295, 309, 689, 291, 362, 257, 7006, 420, 406, 13, 51384, 51384, 583, 309, 311, 1936, 445, 257, 1433, 295, 264, 2281, 11, 558, 30, 51634, 51810], "temperature": 0.0, "avg_logprob": -0.2570692010827967, "compression_ratio": 1.467032967032967, "no_speech_prob": 4.092298240720993e-06}, {"id": 327, "seek": 215178, "start": 2151.78, "end": 2156.78, "text": " So the term itself is not trained, it's fixed, right?", "tokens": [50364, 407, 264, 1433, 2564, 307, 406, 8895, 11, 309, 311, 6806, 11, 558, 30, 50614, 50614, 467, 311, 445, 264, 441, 17, 2026, 670, 3935, 11, 50680, 50680, 293, 264, 3935, 366, 3852, 35344, 2001, 13, 50752, 50832, 583, 570, 309, 311, 257, 46691, 11, 50900, 50900, 309, 1333, 295, 24799, 437, 264, 2058, 19866, 51034, 51034, 293, 264, 979, 378, 433, 486, 360, 11, 51090, 51090, 437, 2010, 295, 4122, 486, 312, 34086, 13, 51190, 51250, 1692, 307, 1071, 1365, 295, 1333, 295, 27063, 636, 51500, 51504, 295, 884, 637, 11668, 17720, 807, 25128, 20406, 849, 13, 51672, 51672, 400, 456, 311, 257, 3840, 295, 5110, 295, 341, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.37776133888646174, "compression_ratio": 1.65, "no_speech_prob": 9.3639164333581e-06}, {"id": 328, "seek": 215178, "start": 2156.78, "end": 2158.1000000000004, "text": " It's just the L2 norm over groups,", "tokens": [50364, 407, 264, 1433, 2564, 307, 406, 8895, 11, 309, 311, 6806, 11, 558, 30, 50614, 50614, 467, 311, 445, 264, 441, 17, 2026, 670, 3935, 11, 50680, 50680, 293, 264, 3935, 366, 3852, 35344, 2001, 13, 50752, 50832, 583, 570, 309, 311, 257, 46691, 11, 50900, 50900, 309, 1333, 295, 24799, 437, 264, 2058, 19866, 51034, 51034, 293, 264, 979, 378, 433, 486, 360, 11, 51090, 51090, 437, 2010, 295, 4122, 486, 312, 34086, 13, 51190, 51250, 1692, 307, 1071, 1365, 295, 1333, 295, 27063, 636, 51500, 51504, 295, 884, 637, 11668, 17720, 807, 25128, 20406, 849, 13, 51672, 51672, 400, 456, 311, 257, 3840, 295, 5110, 295, 341, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.37776133888646174, "compression_ratio": 1.65, "no_speech_prob": 9.3639164333581e-06}, {"id": 329, "seek": 215178, "start": 2158.1000000000004, "end": 2159.5400000000004, "text": " and the groups are predetermined.", "tokens": [50364, 407, 264, 1433, 2564, 307, 406, 8895, 11, 309, 311, 6806, 11, 558, 30, 50614, 50614, 467, 311, 445, 264, 441, 17, 2026, 670, 3935, 11, 50680, 50680, 293, 264, 3935, 366, 3852, 35344, 2001, 13, 50752, 50832, 583, 570, 309, 311, 257, 46691, 11, 50900, 50900, 309, 1333, 295, 24799, 437, 264, 2058, 19866, 51034, 51034, 293, 264, 979, 378, 433, 486, 360, 11, 51090, 51090, 437, 2010, 295, 4122, 486, 312, 34086, 13, 51190, 51250, 1692, 307, 1071, 1365, 295, 1333, 295, 27063, 636, 51500, 51504, 295, 884, 637, 11668, 17720, 807, 25128, 20406, 849, 13, 51672, 51672, 400, 456, 311, 257, 3840, 295, 5110, 295, 341, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.37776133888646174, "compression_ratio": 1.65, "no_speech_prob": 9.3639164333581e-06}, {"id": 330, "seek": 215178, "start": 2161.1400000000003, "end": 2162.5, "text": " But because it's a criterion,", "tokens": [50364, 407, 264, 1433, 2564, 307, 406, 8895, 11, 309, 311, 6806, 11, 558, 30, 50614, 50614, 467, 311, 445, 264, 441, 17, 2026, 670, 3935, 11, 50680, 50680, 293, 264, 3935, 366, 3852, 35344, 2001, 13, 50752, 50832, 583, 570, 309, 311, 257, 46691, 11, 50900, 50900, 309, 1333, 295, 24799, 437, 264, 2058, 19866, 51034, 51034, 293, 264, 979, 378, 433, 486, 360, 11, 51090, 51090, 437, 2010, 295, 4122, 486, 312, 34086, 13, 51190, 51250, 1692, 307, 1071, 1365, 295, 1333, 295, 27063, 636, 51500, 51504, 295, 884, 637, 11668, 17720, 807, 25128, 20406, 849, 13, 51672, 51672, 400, 456, 311, 257, 3840, 295, 5110, 295, 341, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.37776133888646174, "compression_ratio": 1.65, "no_speech_prob": 9.3639164333581e-06}, {"id": 331, "seek": 215178, "start": 2162.5, "end": 2165.1800000000003, "text": " it sort of determines what the encoder", "tokens": [50364, 407, 264, 1433, 2564, 307, 406, 8895, 11, 309, 311, 6806, 11, 558, 30, 50614, 50614, 467, 311, 445, 264, 441, 17, 2026, 670, 3935, 11, 50680, 50680, 293, 264, 3935, 366, 3852, 35344, 2001, 13, 50752, 50832, 583, 570, 309, 311, 257, 46691, 11, 50900, 50900, 309, 1333, 295, 24799, 437, 264, 2058, 19866, 51034, 51034, 293, 264, 979, 378, 433, 486, 360, 11, 51090, 51090, 437, 2010, 295, 4122, 486, 312, 34086, 13, 51190, 51250, 1692, 307, 1071, 1365, 295, 1333, 295, 27063, 636, 51500, 51504, 295, 884, 637, 11668, 17720, 807, 25128, 20406, 849, 13, 51672, 51672, 400, 456, 311, 257, 3840, 295, 5110, 295, 341, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.37776133888646174, "compression_ratio": 1.65, "no_speech_prob": 9.3639164333581e-06}, {"id": 332, "seek": 215178, "start": 2165.1800000000003, "end": 2166.3, "text": " and the decoders will do,", "tokens": [50364, 407, 264, 1433, 2564, 307, 406, 8895, 11, 309, 311, 6806, 11, 558, 30, 50614, 50614, 467, 311, 445, 264, 441, 17, 2026, 670, 3935, 11, 50680, 50680, 293, 264, 3935, 366, 3852, 35344, 2001, 13, 50752, 50832, 583, 570, 309, 311, 257, 46691, 11, 50900, 50900, 309, 1333, 295, 24799, 437, 264, 2058, 19866, 51034, 51034, 293, 264, 979, 378, 433, 486, 360, 11, 51090, 51090, 437, 2010, 295, 4122, 486, 312, 34086, 13, 51190, 51250, 1692, 307, 1071, 1365, 295, 1333, 295, 27063, 636, 51500, 51504, 295, 884, 637, 11668, 17720, 807, 25128, 20406, 849, 13, 51672, 51672, 400, 456, 311, 257, 3840, 295, 5110, 295, 341, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.37776133888646174, "compression_ratio": 1.65, "no_speech_prob": 9.3639164333581e-06}, {"id": 333, "seek": 215178, "start": 2166.3, "end": 2168.3, "text": " what type of features will be extracted.", "tokens": [50364, 407, 264, 1433, 2564, 307, 406, 8895, 11, 309, 311, 6806, 11, 558, 30, 50614, 50614, 467, 311, 445, 264, 441, 17, 2026, 670, 3935, 11, 50680, 50680, 293, 264, 3935, 366, 3852, 35344, 2001, 13, 50752, 50832, 583, 570, 309, 311, 257, 46691, 11, 50900, 50900, 309, 1333, 295, 24799, 437, 264, 2058, 19866, 51034, 51034, 293, 264, 979, 378, 433, 486, 360, 11, 51090, 51090, 437, 2010, 295, 4122, 486, 312, 34086, 13, 51190, 51250, 1692, 307, 1071, 1365, 295, 1333, 295, 27063, 636, 51500, 51504, 295, 884, 637, 11668, 17720, 807, 25128, 20406, 849, 13, 51672, 51672, 400, 456, 311, 257, 3840, 295, 5110, 295, 341, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.37776133888646174, "compression_ratio": 1.65, "no_speech_prob": 9.3639164333581e-06}, {"id": 334, "seek": 215178, "start": 2169.5, "end": 2174.5, "text": " Here is another example of sort of exotic way", "tokens": [50364, 407, 264, 1433, 2564, 307, 406, 8895, 11, 309, 311, 6806, 11, 558, 30, 50614, 50614, 467, 311, 445, 264, 441, 17, 2026, 670, 3935, 11, 50680, 50680, 293, 264, 3935, 366, 3852, 35344, 2001, 13, 50752, 50832, 583, 570, 309, 311, 257, 46691, 11, 50900, 50900, 309, 1333, 295, 24799, 437, 264, 2058, 19866, 51034, 51034, 293, 264, 979, 378, 433, 486, 360, 11, 51090, 51090, 437, 2010, 295, 4122, 486, 312, 34086, 13, 51190, 51250, 1692, 307, 1071, 1365, 295, 1333, 295, 27063, 636, 51500, 51504, 295, 884, 637, 11668, 17720, 807, 25128, 20406, 849, 13, 51672, 51672, 400, 456, 311, 257, 3840, 295, 5110, 295, 341, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.37776133888646174, "compression_ratio": 1.65, "no_speech_prob": 9.3639164333581e-06}, {"id": 335, "seek": 215178, "start": 2174.5800000000004, "end": 2177.94, "text": " of doing sparse coding through lateral inhibition.", "tokens": [50364, 407, 264, 1433, 2564, 307, 406, 8895, 11, 309, 311, 6806, 11, 558, 30, 50614, 50614, 467, 311, 445, 264, 441, 17, 2026, 670, 3935, 11, 50680, 50680, 293, 264, 3935, 366, 3852, 35344, 2001, 13, 50752, 50832, 583, 570, 309, 311, 257, 46691, 11, 50900, 50900, 309, 1333, 295, 24799, 437, 264, 2058, 19866, 51034, 51034, 293, 264, 979, 378, 433, 486, 360, 11, 51090, 51090, 437, 2010, 295, 4122, 486, 312, 34086, 13, 51190, 51250, 1692, 307, 1071, 1365, 295, 1333, 295, 27063, 636, 51500, 51504, 295, 884, 637, 11668, 17720, 807, 25128, 20406, 849, 13, 51672, 51672, 400, 456, 311, 257, 3840, 295, 5110, 295, 341, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.37776133888646174, "compression_ratio": 1.65, "no_speech_prob": 9.3639164333581e-06}, {"id": 336, "seek": 215178, "start": 2177.94, "end": 2180.0600000000004, "text": " And there's a bunch of examples of this.", "tokens": [50364, 407, 264, 1433, 2564, 307, 406, 8895, 11, 309, 311, 6806, 11, 558, 30, 50614, 50614, 467, 311, 445, 264, 441, 17, 2026, 670, 3935, 11, 50680, 50680, 293, 264, 3935, 366, 3852, 35344, 2001, 13, 50752, 50832, 583, 570, 309, 311, 257, 46691, 11, 50900, 50900, 309, 1333, 295, 24799, 437, 264, 2058, 19866, 51034, 51034, 293, 264, 979, 378, 433, 486, 360, 11, 51090, 51090, 437, 2010, 295, 4122, 486, 312, 34086, 13, 51190, 51250, 1692, 307, 1071, 1365, 295, 1333, 295, 27063, 636, 51500, 51504, 295, 884, 637, 11668, 17720, 807, 25128, 20406, 849, 13, 51672, 51672, 400, 456, 311, 257, 3840, 295, 5110, 295, 341, 13, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.37776133888646174, "compression_ratio": 1.65, "no_speech_prob": 9.3639164333581e-06}, {"id": 337, "seek": 218006, "start": 2180.06, "end": 2183.06, "text": " So this is a linear decoder with a square reconstruction error.", "tokens": [50364, 407, 341, 307, 257, 8213, 979, 19866, 365, 257, 3732, 31565, 6713, 13, 50514, 50514, 639, 307, 343, 57, 3175, 1783, 11, 689, 1783, 307, 264, 4846, 510, 294, 341, 1389, 13, 50764, 50764, 400, 550, 456, 307, 257, 46691, 294, 264, 2281, 11, 50914, 50914, 597, 307, 264, 8062, 8693, 538, 264, 8236, 4190, 295, 1176, 51164, 51200, 25167, 1413, 512, 8141, 1413, 512, 8062, 13, 51350, 51350, 400, 550, 456, 307, 257, 8213, 979, 19866, 51464, 51464, 597, 307, 257, 8213, 979, 19866, 365, 257, 3732, 31565, 6713, 13, 51614, 51614, 400, 550, 456, 307, 257, 8213, 979, 19866, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.6506381080264136, "compression_ratio": 2.1157894736842104, "no_speech_prob": 2.5421755708521232e-05}, {"id": 338, "seek": 218006, "start": 2183.06, "end": 2188.06, "text": " This is WZ minus X, where X is the input here in this case.", "tokens": [50364, 407, 341, 307, 257, 8213, 979, 19866, 365, 257, 3732, 31565, 6713, 13, 50514, 50514, 639, 307, 343, 57, 3175, 1783, 11, 689, 1783, 307, 264, 4846, 510, 294, 341, 1389, 13, 50764, 50764, 400, 550, 456, 307, 257, 46691, 294, 264, 2281, 11, 50914, 50914, 597, 307, 264, 8062, 8693, 538, 264, 8236, 4190, 295, 1176, 51164, 51200, 25167, 1413, 512, 8141, 1413, 512, 8062, 13, 51350, 51350, 400, 550, 456, 307, 257, 8213, 979, 19866, 51464, 51464, 597, 307, 257, 8213, 979, 19866, 365, 257, 3732, 31565, 6713, 13, 51614, 51614, 400, 550, 456, 307, 257, 8213, 979, 19866, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.6506381080264136, "compression_ratio": 2.1157894736842104, "no_speech_prob": 2.5421755708521232e-05}, {"id": 339, "seek": 218006, "start": 2188.06, "end": 2191.06, "text": " And then there is a criterion in the energy,", "tokens": [50364, 407, 341, 307, 257, 8213, 979, 19866, 365, 257, 3732, 31565, 6713, 13, 50514, 50514, 639, 307, 343, 57, 3175, 1783, 11, 689, 1783, 307, 264, 4846, 510, 294, 341, 1389, 13, 50764, 50764, 400, 550, 456, 307, 257, 46691, 294, 264, 2281, 11, 50914, 50914, 597, 307, 264, 8062, 8693, 538, 264, 8236, 4190, 295, 1176, 51164, 51200, 25167, 1413, 512, 8141, 1413, 512, 8062, 13, 51350, 51350, 400, 550, 456, 307, 257, 8213, 979, 19866, 51464, 51464, 597, 307, 257, 8213, 979, 19866, 365, 257, 3732, 31565, 6713, 13, 51614, 51614, 400, 550, 456, 307, 257, 8213, 979, 19866, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.6506381080264136, "compression_ratio": 2.1157894736842104, "no_speech_prob": 2.5421755708521232e-05}, {"id": 340, "seek": 218006, "start": 2191.06, "end": 2196.06, "text": " which is the vector formed by the absolute values of Z", "tokens": [50364, 407, 341, 307, 257, 8213, 979, 19866, 365, 257, 3732, 31565, 6713, 13, 50514, 50514, 639, 307, 343, 57, 3175, 1783, 11, 689, 1783, 307, 264, 4846, 510, 294, 341, 1389, 13, 50764, 50764, 400, 550, 456, 307, 257, 46691, 294, 264, 2281, 11, 50914, 50914, 597, 307, 264, 8062, 8693, 538, 264, 8236, 4190, 295, 1176, 51164, 51200, 25167, 1413, 512, 8141, 1413, 512, 8062, 13, 51350, 51350, 400, 550, 456, 307, 257, 8213, 979, 19866, 51464, 51464, 597, 307, 257, 8213, 979, 19866, 365, 257, 3732, 31565, 6713, 13, 51614, 51614, 400, 550, 456, 307, 257, 8213, 979, 19866, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.6506381080264136, "compression_ratio": 2.1157894736842104, "no_speech_prob": 2.5421755708521232e-05}, {"id": 341, "seek": 218006, "start": 2196.7799999999997, "end": 2199.7799999999997, "text": " transpose times some matrix times some vector.", "tokens": [50364, 407, 341, 307, 257, 8213, 979, 19866, 365, 257, 3732, 31565, 6713, 13, 50514, 50514, 639, 307, 343, 57, 3175, 1783, 11, 689, 1783, 307, 264, 4846, 510, 294, 341, 1389, 13, 50764, 50764, 400, 550, 456, 307, 257, 46691, 294, 264, 2281, 11, 50914, 50914, 597, 307, 264, 8062, 8693, 538, 264, 8236, 4190, 295, 1176, 51164, 51200, 25167, 1413, 512, 8141, 1413, 512, 8062, 13, 51350, 51350, 400, 550, 456, 307, 257, 8213, 979, 19866, 51464, 51464, 597, 307, 257, 8213, 979, 19866, 365, 257, 3732, 31565, 6713, 13, 51614, 51614, 400, 550, 456, 307, 257, 8213, 979, 19866, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.6506381080264136, "compression_ratio": 2.1157894736842104, "no_speech_prob": 2.5421755708521232e-05}, {"id": 342, "seek": 218006, "start": 2199.7799999999997, "end": 2202.06, "text": " And then there is a linear decoder", "tokens": [50364, 407, 341, 307, 257, 8213, 979, 19866, 365, 257, 3732, 31565, 6713, 13, 50514, 50514, 639, 307, 343, 57, 3175, 1783, 11, 689, 1783, 307, 264, 4846, 510, 294, 341, 1389, 13, 50764, 50764, 400, 550, 456, 307, 257, 46691, 294, 264, 2281, 11, 50914, 50914, 597, 307, 264, 8062, 8693, 538, 264, 8236, 4190, 295, 1176, 51164, 51200, 25167, 1413, 512, 8141, 1413, 512, 8062, 13, 51350, 51350, 400, 550, 456, 307, 257, 8213, 979, 19866, 51464, 51464, 597, 307, 257, 8213, 979, 19866, 365, 257, 3732, 31565, 6713, 13, 51614, 51614, 400, 550, 456, 307, 257, 8213, 979, 19866, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.6506381080264136, "compression_ratio": 2.1157894736842104, "no_speech_prob": 2.5421755708521232e-05}, {"id": 343, "seek": 218006, "start": 2202.06, "end": 2205.06, "text": " which is a linear decoder with a square reconstruction error.", "tokens": [50364, 407, 341, 307, 257, 8213, 979, 19866, 365, 257, 3732, 31565, 6713, 13, 50514, 50514, 639, 307, 343, 57, 3175, 1783, 11, 689, 1783, 307, 264, 4846, 510, 294, 341, 1389, 13, 50764, 50764, 400, 550, 456, 307, 257, 46691, 294, 264, 2281, 11, 50914, 50914, 597, 307, 264, 8062, 8693, 538, 264, 8236, 4190, 295, 1176, 51164, 51200, 25167, 1413, 512, 8141, 1413, 512, 8062, 13, 51350, 51350, 400, 550, 456, 307, 257, 8213, 979, 19866, 51464, 51464, 597, 307, 257, 8213, 979, 19866, 365, 257, 3732, 31565, 6713, 13, 51614, 51614, 400, 550, 456, 307, 257, 8213, 979, 19866, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.6506381080264136, "compression_ratio": 2.1157894736842104, "no_speech_prob": 2.5421755708521232e-05}, {"id": 344, "seek": 218006, "start": 2205.06, "end": 2207.06, "text": " And then there is a linear decoder", "tokens": [50364, 407, 341, 307, 257, 8213, 979, 19866, 365, 257, 3732, 31565, 6713, 13, 50514, 50514, 639, 307, 343, 57, 3175, 1783, 11, 689, 1783, 307, 264, 4846, 510, 294, 341, 1389, 13, 50764, 50764, 400, 550, 456, 307, 257, 46691, 294, 264, 2281, 11, 50914, 50914, 597, 307, 264, 8062, 8693, 538, 264, 8236, 4190, 295, 1176, 51164, 51200, 25167, 1413, 512, 8141, 1413, 512, 8062, 13, 51350, 51350, 400, 550, 456, 307, 257, 8213, 979, 19866, 51464, 51464, 597, 307, 257, 8213, 979, 19866, 365, 257, 3732, 31565, 6713, 13, 51614, 51614, 400, 550, 456, 307, 257, 8213, 979, 19866, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.6506381080264136, "compression_ratio": 2.1157894736842104, "no_speech_prob": 2.5421755708521232e-05}, {"id": 345, "seek": 220706, "start": 2207.06, "end": 2211.06, "text": " with a square representation error of Z", "tokens": [50364, 365, 257, 3732, 10290, 6713, 295, 1176, 50564, 50564, 25167, 1413, 512, 8141, 1413, 264, 8062, 2564, 13, 50714, 50714, 407, 309, 311, 257, 733, 295, 257, 37262, 1254, 50914, 50914, 300, 11626, 1176, 293, 341, 8141, 318, 13, 51014, 51014, 400, 264, 8141, 318, 307, 2139, 9540, 538, 1011, 51214, 51214, 420, 3264, 370, 382, 281, 733, 295, 19874, 341, 1433, 13, 51464, 51514, 1033, 13, 51564, 51664], "temperature": 0.4, "avg_logprob": -0.263389291897626, "compression_ratio": 1.5144508670520231, "no_speech_prob": 9.817382306209765e-06}, {"id": 346, "seek": 220706, "start": 2211.06, "end": 2214.06, "text": " transpose times some matrix times the vector itself.", "tokens": [50364, 365, 257, 3732, 10290, 6713, 295, 1176, 50564, 50564, 25167, 1413, 512, 8141, 1413, 264, 8062, 2564, 13, 50714, 50714, 407, 309, 311, 257, 733, 295, 257, 37262, 1254, 50914, 50914, 300, 11626, 1176, 293, 341, 8141, 318, 13, 51014, 51014, 400, 264, 8141, 318, 307, 2139, 9540, 538, 1011, 51214, 51214, 420, 3264, 370, 382, 281, 733, 295, 19874, 341, 1433, 13, 51464, 51514, 1033, 13, 51564, 51664], "temperature": 0.4, "avg_logprob": -0.263389291897626, "compression_ratio": 1.5144508670520231, "no_speech_prob": 9.817382306209765e-06}, {"id": 347, "seek": 220706, "start": 2214.06, "end": 2218.06, "text": " So it's a kind of a quadratic form", "tokens": [50364, 365, 257, 3732, 10290, 6713, 295, 1176, 50564, 50564, 25167, 1413, 512, 8141, 1413, 264, 8062, 2564, 13, 50714, 50714, 407, 309, 311, 257, 733, 295, 257, 37262, 1254, 50914, 50914, 300, 11626, 1176, 293, 341, 8141, 318, 13, 51014, 51014, 400, 264, 8141, 318, 307, 2139, 9540, 538, 1011, 51214, 51214, 420, 3264, 370, 382, 281, 733, 295, 19874, 341, 1433, 13, 51464, 51514, 1033, 13, 51564, 51664], "temperature": 0.4, "avg_logprob": -0.263389291897626, "compression_ratio": 1.5144508670520231, "no_speech_prob": 9.817382306209765e-06}, {"id": 348, "seek": 220706, "start": 2218.06, "end": 2220.06, "text": " that involves Z and this matrix S.", "tokens": [50364, 365, 257, 3732, 10290, 6713, 295, 1176, 50564, 50564, 25167, 1413, 512, 8141, 1413, 264, 8062, 2564, 13, 50714, 50714, 407, 309, 311, 257, 733, 295, 257, 37262, 1254, 50914, 50914, 300, 11626, 1176, 293, 341, 8141, 318, 13, 51014, 51014, 400, 264, 8141, 318, 307, 2139, 9540, 538, 1011, 51214, 51214, 420, 3264, 370, 382, 281, 733, 295, 19874, 341, 1433, 13, 51464, 51514, 1033, 13, 51564, 51664], "temperature": 0.4, "avg_logprob": -0.263389291897626, "compression_ratio": 1.5144508670520231, "no_speech_prob": 9.817382306209765e-06}, {"id": 349, "seek": 220706, "start": 2220.06, "end": 2224.06, "text": " And the matrix S is either determined by hand", "tokens": [50364, 365, 257, 3732, 10290, 6713, 295, 1176, 50564, 50564, 25167, 1413, 512, 8141, 1413, 264, 8062, 2564, 13, 50714, 50714, 407, 309, 311, 257, 733, 295, 257, 37262, 1254, 50914, 50914, 300, 11626, 1176, 293, 341, 8141, 318, 13, 51014, 51014, 400, 264, 8141, 318, 307, 2139, 9540, 538, 1011, 51214, 51214, 420, 3264, 370, 382, 281, 733, 295, 19874, 341, 1433, 13, 51464, 51514, 1033, 13, 51564, 51664], "temperature": 0.4, "avg_logprob": -0.263389291897626, "compression_ratio": 1.5144508670520231, "no_speech_prob": 9.817382306209765e-06}, {"id": 350, "seek": 220706, "start": 2224.06, "end": 2229.06, "text": " or learned so as to kind of maximize this term.", "tokens": [50364, 365, 257, 3732, 10290, 6713, 295, 1176, 50564, 50564, 25167, 1413, 512, 8141, 1413, 264, 8062, 2564, 13, 50714, 50714, 407, 309, 311, 257, 733, 295, 257, 37262, 1254, 50914, 50914, 300, 11626, 1176, 293, 341, 8141, 318, 13, 51014, 51014, 400, 264, 8141, 318, 307, 2139, 9540, 538, 1011, 51214, 51214, 420, 3264, 370, 382, 281, 733, 295, 19874, 341, 1433, 13, 51464, 51514, 1033, 13, 51564, 51664], "temperature": 0.4, "avg_logprob": -0.263389291897626, "compression_ratio": 1.5144508670520231, "no_speech_prob": 9.817382306209765e-06}, {"id": 351, "seek": 220706, "start": 2230.06, "end": 2231.06, "text": " Okay.", "tokens": [50364, 365, 257, 3732, 10290, 6713, 295, 1176, 50564, 50564, 25167, 1413, 512, 8141, 1413, 264, 8062, 2564, 13, 50714, 50714, 407, 309, 311, 257, 733, 295, 257, 37262, 1254, 50914, 50914, 300, 11626, 1176, 293, 341, 8141, 318, 13, 51014, 51014, 400, 264, 8141, 318, 307, 2139, 9540, 538, 1011, 51214, 51214, 420, 3264, 370, 382, 281, 733, 295, 19874, 341, 1433, 13, 51464, 51514, 1033, 13, 51564, 51664], "temperature": 0.4, "avg_logprob": -0.263389291897626, "compression_ratio": 1.5144508670520231, "no_speech_prob": 9.817382306209765e-06}, {"id": 352, "seek": 223106, "start": 2231.06, "end": 2236.06, "text": " If the terms in S are positive and large,", "tokens": [50364, 759, 264, 2115, 294, 318, 366, 3353, 293, 2416, 11, 50614, 50614, 498, 472, 1729, 1433, 11, 318, 1718, 307, 2416, 11, 50714, 50714, 437, 300, 1355, 307, 300, 264, 1185, 775, 406, 528, 50864, 50864, 26190, 293, 1176, 73, 281, 312, 322, 412, 264, 912, 565, 13, 50964, 50964, 1033, 13, 51014, 51014, 759, 26190, 307, 322, 293, 318, 1718, 307, 2416, 11, 51264, 51264, 550, 309, 2738, 1176, 73, 281, 312, 766, 293, 11964, 25650, 13, 51414, 51414, 1033, 13, 51464, 51464, 400, 370, 309, 311, 1333, 295, 257, 16917, 20406, 849, 13, 51614, 51614, 3432, 818, 341, 25128, 20406, 849, 294, 42762, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11863052194768732, "compression_ratio": 1.6538461538461537, "no_speech_prob": 2.5050281692529097e-05}, {"id": 353, "seek": 223106, "start": 2236.06, "end": 2238.06, "text": " if one particular term, Sij is large,", "tokens": [50364, 759, 264, 2115, 294, 318, 366, 3353, 293, 2416, 11, 50614, 50614, 498, 472, 1729, 1433, 11, 318, 1718, 307, 2416, 11, 50714, 50714, 437, 300, 1355, 307, 300, 264, 1185, 775, 406, 528, 50864, 50864, 26190, 293, 1176, 73, 281, 312, 322, 412, 264, 912, 565, 13, 50964, 50964, 1033, 13, 51014, 51014, 759, 26190, 307, 322, 293, 318, 1718, 307, 2416, 11, 51264, 51264, 550, 309, 2738, 1176, 73, 281, 312, 766, 293, 11964, 25650, 13, 51414, 51414, 1033, 13, 51464, 51464, 400, 370, 309, 311, 1333, 295, 257, 16917, 20406, 849, 13, 51614, 51614, 3432, 818, 341, 25128, 20406, 849, 294, 42762, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11863052194768732, "compression_ratio": 1.6538461538461537, "no_speech_prob": 2.5050281692529097e-05}, {"id": 354, "seek": 223106, "start": 2238.06, "end": 2241.06, "text": " what that means is that the system does not want", "tokens": [50364, 759, 264, 2115, 294, 318, 366, 3353, 293, 2416, 11, 50614, 50614, 498, 472, 1729, 1433, 11, 318, 1718, 307, 2416, 11, 50714, 50714, 437, 300, 1355, 307, 300, 264, 1185, 775, 406, 528, 50864, 50864, 26190, 293, 1176, 73, 281, 312, 322, 412, 264, 912, 565, 13, 50964, 50964, 1033, 13, 51014, 51014, 759, 26190, 307, 322, 293, 318, 1718, 307, 2416, 11, 51264, 51264, 550, 309, 2738, 1176, 73, 281, 312, 766, 293, 11964, 25650, 13, 51414, 51414, 1033, 13, 51464, 51464, 400, 370, 309, 311, 1333, 295, 257, 16917, 20406, 849, 13, 51614, 51614, 3432, 818, 341, 25128, 20406, 849, 294, 42762, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11863052194768732, "compression_ratio": 1.6538461538461537, "no_speech_prob": 2.5050281692529097e-05}, {"id": 355, "seek": 223106, "start": 2241.06, "end": 2243.06, "text": " Zi and Zj to be on at the same time.", "tokens": [50364, 759, 264, 2115, 294, 318, 366, 3353, 293, 2416, 11, 50614, 50614, 498, 472, 1729, 1433, 11, 318, 1718, 307, 2416, 11, 50714, 50714, 437, 300, 1355, 307, 300, 264, 1185, 775, 406, 528, 50864, 50864, 26190, 293, 1176, 73, 281, 312, 322, 412, 264, 912, 565, 13, 50964, 50964, 1033, 13, 51014, 51014, 759, 26190, 307, 322, 293, 318, 1718, 307, 2416, 11, 51264, 51264, 550, 309, 2738, 1176, 73, 281, 312, 766, 293, 11964, 25650, 13, 51414, 51414, 1033, 13, 51464, 51464, 400, 370, 309, 311, 1333, 295, 257, 16917, 20406, 849, 13, 51614, 51614, 3432, 818, 341, 25128, 20406, 849, 294, 42762, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11863052194768732, "compression_ratio": 1.6538461538461537, "no_speech_prob": 2.5050281692529097e-05}, {"id": 356, "seek": 223106, "start": 2243.06, "end": 2244.06, "text": " Okay.", "tokens": [50364, 759, 264, 2115, 294, 318, 366, 3353, 293, 2416, 11, 50614, 50614, 498, 472, 1729, 1433, 11, 318, 1718, 307, 2416, 11, 50714, 50714, 437, 300, 1355, 307, 300, 264, 1185, 775, 406, 528, 50864, 50864, 26190, 293, 1176, 73, 281, 312, 322, 412, 264, 912, 565, 13, 50964, 50964, 1033, 13, 51014, 51014, 759, 26190, 307, 322, 293, 318, 1718, 307, 2416, 11, 51264, 51264, 550, 309, 2738, 1176, 73, 281, 312, 766, 293, 11964, 25650, 13, 51414, 51414, 1033, 13, 51464, 51464, 400, 370, 309, 311, 1333, 295, 257, 16917, 20406, 849, 13, 51614, 51614, 3432, 818, 341, 25128, 20406, 849, 294, 42762, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11863052194768732, "compression_ratio": 1.6538461538461537, "no_speech_prob": 2.5050281692529097e-05}, {"id": 357, "seek": 223106, "start": 2244.06, "end": 2249.06, "text": " If Zi is on and Sij is large,", "tokens": [50364, 759, 264, 2115, 294, 318, 366, 3353, 293, 2416, 11, 50614, 50614, 498, 472, 1729, 1433, 11, 318, 1718, 307, 2416, 11, 50714, 50714, 437, 300, 1355, 307, 300, 264, 1185, 775, 406, 528, 50864, 50864, 26190, 293, 1176, 73, 281, 312, 322, 412, 264, 912, 565, 13, 50964, 50964, 1033, 13, 51014, 51014, 759, 26190, 307, 322, 293, 318, 1718, 307, 2416, 11, 51264, 51264, 550, 309, 2738, 1176, 73, 281, 312, 766, 293, 11964, 25650, 13, 51414, 51414, 1033, 13, 51464, 51464, 400, 370, 309, 311, 1333, 295, 257, 16917, 20406, 849, 13, 51614, 51614, 3432, 818, 341, 25128, 20406, 849, 294, 42762, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11863052194768732, "compression_ratio": 1.6538461538461537, "no_speech_prob": 2.5050281692529097e-05}, {"id": 358, "seek": 223106, "start": 2249.06, "end": 2252.06, "text": " then it wants Zj to be off and vice versa.", "tokens": [50364, 759, 264, 2115, 294, 318, 366, 3353, 293, 2416, 11, 50614, 50614, 498, 472, 1729, 1433, 11, 318, 1718, 307, 2416, 11, 50714, 50714, 437, 300, 1355, 307, 300, 264, 1185, 775, 406, 528, 50864, 50864, 26190, 293, 1176, 73, 281, 312, 322, 412, 264, 912, 565, 13, 50964, 50964, 1033, 13, 51014, 51014, 759, 26190, 307, 322, 293, 318, 1718, 307, 2416, 11, 51264, 51264, 550, 309, 2738, 1176, 73, 281, 312, 766, 293, 11964, 25650, 13, 51414, 51414, 1033, 13, 51464, 51464, 400, 370, 309, 311, 1333, 295, 257, 16917, 20406, 849, 13, 51614, 51614, 3432, 818, 341, 25128, 20406, 849, 294, 42762, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11863052194768732, "compression_ratio": 1.6538461538461537, "no_speech_prob": 2.5050281692529097e-05}, {"id": 359, "seek": 223106, "start": 2252.06, "end": 2253.06, "text": " Okay.", "tokens": [50364, 759, 264, 2115, 294, 318, 366, 3353, 293, 2416, 11, 50614, 50614, 498, 472, 1729, 1433, 11, 318, 1718, 307, 2416, 11, 50714, 50714, 437, 300, 1355, 307, 300, 264, 1185, 775, 406, 528, 50864, 50864, 26190, 293, 1176, 73, 281, 312, 322, 412, 264, 912, 565, 13, 50964, 50964, 1033, 13, 51014, 51014, 759, 26190, 307, 322, 293, 318, 1718, 307, 2416, 11, 51264, 51264, 550, 309, 2738, 1176, 73, 281, 312, 766, 293, 11964, 25650, 13, 51414, 51414, 1033, 13, 51464, 51464, 400, 370, 309, 311, 1333, 295, 257, 16917, 20406, 849, 13, 51614, 51614, 3432, 818, 341, 25128, 20406, 849, 294, 42762, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11863052194768732, "compression_ratio": 1.6538461538461537, "no_speech_prob": 2.5050281692529097e-05}, {"id": 360, "seek": 223106, "start": 2253.06, "end": 2256.06, "text": " And so it's sort of a mutual inhibition.", "tokens": [50364, 759, 264, 2115, 294, 318, 366, 3353, 293, 2416, 11, 50614, 50614, 498, 472, 1729, 1433, 11, 318, 1718, 307, 2416, 11, 50714, 50714, 437, 300, 1355, 307, 300, 264, 1185, 775, 406, 528, 50864, 50864, 26190, 293, 1176, 73, 281, 312, 322, 412, 264, 912, 565, 13, 50964, 50964, 1033, 13, 51014, 51014, 759, 26190, 307, 322, 293, 318, 1718, 307, 2416, 11, 51264, 51264, 550, 309, 2738, 1176, 73, 281, 312, 766, 293, 11964, 25650, 13, 51414, 51414, 1033, 13, 51464, 51464, 400, 370, 309, 311, 1333, 295, 257, 16917, 20406, 849, 13, 51614, 51614, 3432, 818, 341, 25128, 20406, 849, 294, 42762, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11863052194768732, "compression_ratio": 1.6538461538461537, "no_speech_prob": 2.5050281692529097e-05}, {"id": 361, "seek": 223106, "start": 2256.06, "end": 2260.06, "text": " People call this lateral inhibition in neuroscience.", "tokens": [50364, 759, 264, 2115, 294, 318, 366, 3353, 293, 2416, 11, 50614, 50614, 498, 472, 1729, 1433, 11, 318, 1718, 307, 2416, 11, 50714, 50714, 437, 300, 1355, 307, 300, 264, 1185, 775, 406, 528, 50864, 50864, 26190, 293, 1176, 73, 281, 312, 322, 412, 264, 912, 565, 13, 50964, 50964, 1033, 13, 51014, 51014, 759, 26190, 307, 322, 293, 318, 1718, 307, 2416, 11, 51264, 51264, 550, 309, 2738, 1176, 73, 281, 312, 766, 293, 11964, 25650, 13, 51414, 51414, 1033, 13, 51464, 51464, 400, 370, 309, 311, 1333, 295, 257, 16917, 20406, 849, 13, 51614, 51614, 3432, 818, 341, 25128, 20406, 849, 294, 42762, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11863052194768732, "compression_ratio": 1.6538461538461537, "no_speech_prob": 2.5050281692529097e-05}, {"id": 362, "seek": 226006, "start": 2260.06, "end": 2263.06, "text": " It's basically all your feature vectors", "tokens": [50364, 467, 311, 1936, 439, 428, 4111, 18875, 50514, 50514, 1936, 49858, 661, 4111, 18875, 50614, 50614, 807, 341, 8141, 318, 13, 50664, 50664, 509, 393, 4536, 300, 264, 8141, 318, 257, 4059, 72, 307, 18519, 13, 50814, 50814, 407, 291, 393, 4536, 300, 787, 512, 2115, 366, 322, 4018, 13, 50964, 50964, 509, 393, 4536, 300, 512, 2115, 11, 51064, 51064, 729, 2115, 366, 6806, 420, 393, 312, 8895, 13, 51214, 51214, 400, 264, 636, 291, 3847, 552, 307, 538, 767, 5138, 3319, 13, 51364, 51364, 407, 309, 311, 733, 295, 17641, 44745, 3097, 257, 707, 857, 13, 51464, 51464, 509, 853, 281, 915, 264, 2158, 295, 318, 300, 1333, 295, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07618789015145137, "compression_ratio": 1.8547008547008548, "no_speech_prob": 5.828316352562979e-05}, {"id": 363, "seek": 226006, "start": 2263.06, "end": 2265.06, "text": " basically inhibit other feature vectors", "tokens": [50364, 467, 311, 1936, 439, 428, 4111, 18875, 50514, 50514, 1936, 49858, 661, 4111, 18875, 50614, 50614, 807, 341, 8141, 318, 13, 50664, 50664, 509, 393, 4536, 300, 264, 8141, 318, 257, 4059, 72, 307, 18519, 13, 50814, 50814, 407, 291, 393, 4536, 300, 787, 512, 2115, 366, 322, 4018, 13, 50964, 50964, 509, 393, 4536, 300, 512, 2115, 11, 51064, 51064, 729, 2115, 366, 6806, 420, 393, 312, 8895, 13, 51214, 51214, 400, 264, 636, 291, 3847, 552, 307, 538, 767, 5138, 3319, 13, 51364, 51364, 407, 309, 311, 733, 295, 17641, 44745, 3097, 257, 707, 857, 13, 51464, 51464, 509, 853, 281, 915, 264, 2158, 295, 318, 300, 1333, 295, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07618789015145137, "compression_ratio": 1.8547008547008548, "no_speech_prob": 5.828316352562979e-05}, {"id": 364, "seek": 226006, "start": 2265.06, "end": 2266.06, "text": " through this matrix S.", "tokens": [50364, 467, 311, 1936, 439, 428, 4111, 18875, 50514, 50514, 1936, 49858, 661, 4111, 18875, 50614, 50614, 807, 341, 8141, 318, 13, 50664, 50664, 509, 393, 4536, 300, 264, 8141, 318, 257, 4059, 72, 307, 18519, 13, 50814, 50814, 407, 291, 393, 4536, 300, 787, 512, 2115, 366, 322, 4018, 13, 50964, 50964, 509, 393, 4536, 300, 512, 2115, 11, 51064, 51064, 729, 2115, 366, 6806, 420, 393, 312, 8895, 13, 51214, 51214, 400, 264, 636, 291, 3847, 552, 307, 538, 767, 5138, 3319, 13, 51364, 51364, 407, 309, 311, 733, 295, 17641, 44745, 3097, 257, 707, 857, 13, 51464, 51464, 509, 853, 281, 915, 264, 2158, 295, 318, 300, 1333, 295, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07618789015145137, "compression_ratio": 1.8547008547008548, "no_speech_prob": 5.828316352562979e-05}, {"id": 365, "seek": 226006, "start": 2266.06, "end": 2269.06, "text": " You can decide that the matrix S a priori is structured.", "tokens": [50364, 467, 311, 1936, 439, 428, 4111, 18875, 50514, 50514, 1936, 49858, 661, 4111, 18875, 50614, 50614, 807, 341, 8141, 318, 13, 50664, 50664, 509, 393, 4536, 300, 264, 8141, 318, 257, 4059, 72, 307, 18519, 13, 50814, 50814, 407, 291, 393, 4536, 300, 787, 512, 2115, 366, 322, 4018, 13, 50964, 50964, 509, 393, 4536, 300, 512, 2115, 11, 51064, 51064, 729, 2115, 366, 6806, 420, 393, 312, 8895, 13, 51214, 51214, 400, 264, 636, 291, 3847, 552, 307, 538, 767, 5138, 3319, 13, 51364, 51364, 407, 309, 311, 733, 295, 17641, 44745, 3097, 257, 707, 857, 13, 51464, 51464, 509, 853, 281, 915, 264, 2158, 295, 318, 300, 1333, 295, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07618789015145137, "compression_ratio": 1.8547008547008548, "no_speech_prob": 5.828316352562979e-05}, {"id": 366, "seek": 226006, "start": 2269.06, "end": 2272.06, "text": " So you can decide that only some terms are on zero.", "tokens": [50364, 467, 311, 1936, 439, 428, 4111, 18875, 50514, 50514, 1936, 49858, 661, 4111, 18875, 50614, 50614, 807, 341, 8141, 318, 13, 50664, 50664, 509, 393, 4536, 300, 264, 8141, 318, 257, 4059, 72, 307, 18519, 13, 50814, 50814, 407, 291, 393, 4536, 300, 787, 512, 2115, 366, 322, 4018, 13, 50964, 50964, 509, 393, 4536, 300, 512, 2115, 11, 51064, 51064, 729, 2115, 366, 6806, 420, 393, 312, 8895, 13, 51214, 51214, 400, 264, 636, 291, 3847, 552, 307, 538, 767, 5138, 3319, 13, 51364, 51364, 407, 309, 311, 733, 295, 17641, 44745, 3097, 257, 707, 857, 13, 51464, 51464, 509, 853, 281, 915, 264, 2158, 295, 318, 300, 1333, 295, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07618789015145137, "compression_ratio": 1.8547008547008548, "no_speech_prob": 5.828316352562979e-05}, {"id": 367, "seek": 226006, "start": 2272.06, "end": 2274.06, "text": " You can decide that some terms,", "tokens": [50364, 467, 311, 1936, 439, 428, 4111, 18875, 50514, 50514, 1936, 49858, 661, 4111, 18875, 50614, 50614, 807, 341, 8141, 318, 13, 50664, 50664, 509, 393, 4536, 300, 264, 8141, 318, 257, 4059, 72, 307, 18519, 13, 50814, 50814, 407, 291, 393, 4536, 300, 787, 512, 2115, 366, 322, 4018, 13, 50964, 50964, 509, 393, 4536, 300, 512, 2115, 11, 51064, 51064, 729, 2115, 366, 6806, 420, 393, 312, 8895, 13, 51214, 51214, 400, 264, 636, 291, 3847, 552, 307, 538, 767, 5138, 3319, 13, 51364, 51364, 407, 309, 311, 733, 295, 17641, 44745, 3097, 257, 707, 857, 13, 51464, 51464, 509, 853, 281, 915, 264, 2158, 295, 318, 300, 1333, 295, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07618789015145137, "compression_ratio": 1.8547008547008548, "no_speech_prob": 5.828316352562979e-05}, {"id": 368, "seek": 226006, "start": 2274.06, "end": 2277.06, "text": " those terms are fixed or can be trained.", "tokens": [50364, 467, 311, 1936, 439, 428, 4111, 18875, 50514, 50514, 1936, 49858, 661, 4111, 18875, 50614, 50614, 807, 341, 8141, 318, 13, 50664, 50664, 509, 393, 4536, 300, 264, 8141, 318, 257, 4059, 72, 307, 18519, 13, 50814, 50814, 407, 291, 393, 4536, 300, 787, 512, 2115, 366, 322, 4018, 13, 50964, 50964, 509, 393, 4536, 300, 512, 2115, 11, 51064, 51064, 729, 2115, 366, 6806, 420, 393, 312, 8895, 13, 51214, 51214, 400, 264, 636, 291, 3847, 552, 307, 538, 767, 5138, 3319, 13, 51364, 51364, 407, 309, 311, 733, 295, 17641, 44745, 3097, 257, 707, 857, 13, 51464, 51464, 509, 853, 281, 915, 264, 2158, 295, 318, 300, 1333, 295, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07618789015145137, "compression_ratio": 1.8547008547008548, "no_speech_prob": 5.828316352562979e-05}, {"id": 369, "seek": 226006, "start": 2277.06, "end": 2280.06, "text": " And the way you train them is by actually maximizing.", "tokens": [50364, 467, 311, 1936, 439, 428, 4111, 18875, 50514, 50514, 1936, 49858, 661, 4111, 18875, 50614, 50614, 807, 341, 8141, 318, 13, 50664, 50664, 509, 393, 4536, 300, 264, 8141, 318, 257, 4059, 72, 307, 18519, 13, 50814, 50814, 407, 291, 393, 4536, 300, 787, 512, 2115, 366, 322, 4018, 13, 50964, 50964, 509, 393, 4536, 300, 512, 2115, 11, 51064, 51064, 729, 2115, 366, 6806, 420, 393, 312, 8895, 13, 51214, 51214, 400, 264, 636, 291, 3847, 552, 307, 538, 767, 5138, 3319, 13, 51364, 51364, 407, 309, 311, 733, 295, 17641, 44745, 3097, 257, 707, 857, 13, 51464, 51464, 509, 853, 281, 915, 264, 2158, 295, 318, 300, 1333, 295, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07618789015145137, "compression_ratio": 1.8547008547008548, "no_speech_prob": 5.828316352562979e-05}, {"id": 370, "seek": 226006, "start": 2280.06, "end": 2282.06, "text": " So it's kind of adversarial training a little bit.", "tokens": [50364, 467, 311, 1936, 439, 428, 4111, 18875, 50514, 50514, 1936, 49858, 661, 4111, 18875, 50614, 50614, 807, 341, 8141, 318, 13, 50664, 50664, 509, 393, 4536, 300, 264, 8141, 318, 257, 4059, 72, 307, 18519, 13, 50814, 50814, 407, 291, 393, 4536, 300, 787, 512, 2115, 366, 322, 4018, 13, 50964, 50964, 509, 393, 4536, 300, 512, 2115, 11, 51064, 51064, 729, 2115, 366, 6806, 420, 393, 312, 8895, 13, 51214, 51214, 400, 264, 636, 291, 3847, 552, 307, 538, 767, 5138, 3319, 13, 51364, 51364, 407, 309, 311, 733, 295, 17641, 44745, 3097, 257, 707, 857, 13, 51464, 51464, 509, 853, 281, 915, 264, 2158, 295, 318, 300, 1333, 295, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07618789015145137, "compression_ratio": 1.8547008547008548, "no_speech_prob": 5.828316352562979e-05}, {"id": 371, "seek": 226006, "start": 2282.06, "end": 2286.06, "text": " You try to find the value of S that sort of,", "tokens": [50364, 467, 311, 1936, 439, 428, 4111, 18875, 50514, 50514, 1936, 49858, 661, 4111, 18875, 50614, 50614, 807, 341, 8141, 318, 13, 50664, 50664, 509, 393, 4536, 300, 264, 8141, 318, 257, 4059, 72, 307, 18519, 13, 50814, 50814, 407, 291, 393, 4536, 300, 787, 512, 2115, 366, 322, 4018, 13, 50964, 50964, 509, 393, 4536, 300, 512, 2115, 11, 51064, 51064, 729, 2115, 366, 6806, 420, 393, 312, 8895, 13, 51214, 51214, 400, 264, 636, 291, 3847, 552, 307, 538, 767, 5138, 3319, 13, 51364, 51364, 407, 309, 311, 733, 295, 17641, 44745, 3097, 257, 707, 857, 13, 51464, 51464, 509, 853, 281, 915, 264, 2158, 295, 318, 300, 1333, 295, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07618789015145137, "compression_ratio": 1.8547008547008548, "no_speech_prob": 5.828316352562979e-05}, {"id": 372, "seek": 228606, "start": 2286.06, "end": 2291.06, "text": " is as large as possible if you want within limits.", "tokens": [50364, 307, 382, 2416, 382, 1944, 498, 291, 528, 1951, 10406, 13, 50614, 50714, 32691, 257, 1629, 2158, 295, 318, 1718, 11, 50814, 50814, 472, 295, 264, 26190, 420, 1176, 73, 307, 516, 281, 352, 281, 4018, 51014, 51014, 293, 300, 1433, 307, 516, 281, 11596, 13, 51114, 51114, 407, 264, 1185, 307, 516, 281, 19874, 264, 318, 1718, 82, 51364, 51364, 1826, 309, 311, 2416, 1547, 281, 733, 295, 360, 264, 16917, 20406, 849, 51614, 51614, 1296, 26190, 293, 1176, 73, 13, 51664, 51664, 400, 309, 311, 406, 516, 281, 352, 604, 3052, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11063558047579736, "compression_ratio": 1.6161616161616161, "no_speech_prob": 1.2597390195878688e-05}, {"id": 373, "seek": 228606, "start": 2293.06, "end": 2295.06, "text": " Above a certain value of Sij,", "tokens": [50364, 307, 382, 2416, 382, 1944, 498, 291, 528, 1951, 10406, 13, 50614, 50714, 32691, 257, 1629, 2158, 295, 318, 1718, 11, 50814, 50814, 472, 295, 264, 26190, 420, 1176, 73, 307, 516, 281, 352, 281, 4018, 51014, 51014, 293, 300, 1433, 307, 516, 281, 11596, 13, 51114, 51114, 407, 264, 1185, 307, 516, 281, 19874, 264, 318, 1718, 82, 51364, 51364, 1826, 309, 311, 2416, 1547, 281, 733, 295, 360, 264, 16917, 20406, 849, 51614, 51614, 1296, 26190, 293, 1176, 73, 13, 51664, 51664, 400, 309, 311, 406, 516, 281, 352, 604, 3052, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11063558047579736, "compression_ratio": 1.6161616161616161, "no_speech_prob": 1.2597390195878688e-05}, {"id": 374, "seek": 228606, "start": 2295.06, "end": 2299.06, "text": " one of the Zi or Zj is going to go to zero", "tokens": [50364, 307, 382, 2416, 382, 1944, 498, 291, 528, 1951, 10406, 13, 50614, 50714, 32691, 257, 1629, 2158, 295, 318, 1718, 11, 50814, 50814, 472, 295, 264, 26190, 420, 1176, 73, 307, 516, 281, 352, 281, 4018, 51014, 51014, 293, 300, 1433, 307, 516, 281, 11596, 13, 51114, 51114, 407, 264, 1185, 307, 516, 281, 19874, 264, 318, 1718, 82, 51364, 51364, 1826, 309, 311, 2416, 1547, 281, 733, 295, 360, 264, 16917, 20406, 849, 51614, 51614, 1296, 26190, 293, 1176, 73, 13, 51664, 51664, 400, 309, 311, 406, 516, 281, 352, 604, 3052, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11063558047579736, "compression_ratio": 1.6161616161616161, "no_speech_prob": 1.2597390195878688e-05}, {"id": 375, "seek": 228606, "start": 2299.06, "end": 2301.06, "text": " and that term is going to disappear.", "tokens": [50364, 307, 382, 2416, 382, 1944, 498, 291, 528, 1951, 10406, 13, 50614, 50714, 32691, 257, 1629, 2158, 295, 318, 1718, 11, 50814, 50814, 472, 295, 264, 26190, 420, 1176, 73, 307, 516, 281, 352, 281, 4018, 51014, 51014, 293, 300, 1433, 307, 516, 281, 11596, 13, 51114, 51114, 407, 264, 1185, 307, 516, 281, 19874, 264, 318, 1718, 82, 51364, 51364, 1826, 309, 311, 2416, 1547, 281, 733, 295, 360, 264, 16917, 20406, 849, 51614, 51614, 1296, 26190, 293, 1176, 73, 13, 51664, 51664, 400, 309, 311, 406, 516, 281, 352, 604, 3052, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11063558047579736, "compression_ratio": 1.6161616161616161, "no_speech_prob": 1.2597390195878688e-05}, {"id": 376, "seek": 228606, "start": 2301.06, "end": 2306.06, "text": " So the system is going to maximize the Sijs", "tokens": [50364, 307, 382, 2416, 382, 1944, 498, 291, 528, 1951, 10406, 13, 50614, 50714, 32691, 257, 1629, 2158, 295, 318, 1718, 11, 50814, 50814, 472, 295, 264, 26190, 420, 1176, 73, 307, 516, 281, 352, 281, 4018, 51014, 51014, 293, 300, 1433, 307, 516, 281, 11596, 13, 51114, 51114, 407, 264, 1185, 307, 516, 281, 19874, 264, 318, 1718, 82, 51364, 51364, 1826, 309, 311, 2416, 1547, 281, 733, 295, 360, 264, 16917, 20406, 849, 51614, 51614, 1296, 26190, 293, 1176, 73, 13, 51664, 51664, 400, 309, 311, 406, 516, 281, 352, 604, 3052, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11063558047579736, "compression_ratio": 1.6161616161616161, "no_speech_prob": 1.2597390195878688e-05}, {"id": 377, "seek": 228606, "start": 2306.06, "end": 2311.06, "text": " until it's large enough to kind of do the mutual inhibition", "tokens": [50364, 307, 382, 2416, 382, 1944, 498, 291, 528, 1951, 10406, 13, 50614, 50714, 32691, 257, 1629, 2158, 295, 318, 1718, 11, 50814, 50814, 472, 295, 264, 26190, 420, 1176, 73, 307, 516, 281, 352, 281, 4018, 51014, 51014, 293, 300, 1433, 307, 516, 281, 11596, 13, 51114, 51114, 407, 264, 1185, 307, 516, 281, 19874, 264, 318, 1718, 82, 51364, 51364, 1826, 309, 311, 2416, 1547, 281, 733, 295, 360, 264, 16917, 20406, 849, 51614, 51614, 1296, 26190, 293, 1176, 73, 13, 51664, 51664, 400, 309, 311, 406, 516, 281, 352, 604, 3052, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11063558047579736, "compression_ratio": 1.6161616161616161, "no_speech_prob": 1.2597390195878688e-05}, {"id": 378, "seek": 228606, "start": 2311.06, "end": 2312.06, "text": " between Zi and Zj.", "tokens": [50364, 307, 382, 2416, 382, 1944, 498, 291, 528, 1951, 10406, 13, 50614, 50714, 32691, 257, 1629, 2158, 295, 318, 1718, 11, 50814, 50814, 472, 295, 264, 26190, 420, 1176, 73, 307, 516, 281, 352, 281, 4018, 51014, 51014, 293, 300, 1433, 307, 516, 281, 11596, 13, 51114, 51114, 407, 264, 1185, 307, 516, 281, 19874, 264, 318, 1718, 82, 51364, 51364, 1826, 309, 311, 2416, 1547, 281, 733, 295, 360, 264, 16917, 20406, 849, 51614, 51614, 1296, 26190, 293, 1176, 73, 13, 51664, 51664, 400, 309, 311, 406, 516, 281, 352, 604, 3052, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11063558047579736, "compression_ratio": 1.6161616161616161, "no_speech_prob": 1.2597390195878688e-05}, {"id": 379, "seek": 228606, "start": 2312.06, "end": 2314.06, "text": " And it's not going to go any further", "tokens": [50364, 307, 382, 2416, 382, 1944, 498, 291, 528, 1951, 10406, 13, 50614, 50714, 32691, 257, 1629, 2158, 295, 318, 1718, 11, 50814, 50814, 472, 295, 264, 26190, 420, 1176, 73, 307, 516, 281, 352, 281, 4018, 51014, 51014, 293, 300, 1433, 307, 516, 281, 11596, 13, 51114, 51114, 407, 264, 1185, 307, 516, 281, 19874, 264, 318, 1718, 82, 51364, 51364, 1826, 309, 311, 2416, 1547, 281, 733, 295, 360, 264, 16917, 20406, 849, 51614, 51614, 1296, 26190, 293, 1176, 73, 13, 51664, 51664, 400, 309, 311, 406, 516, 281, 352, 604, 3052, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11063558047579736, "compression_ratio": 1.6161616161616161, "no_speech_prob": 1.2597390195878688e-05}, {"id": 380, "seek": 231406, "start": 2314.06, "end": 2316.06, "text": " because it doesn't need to.", "tokens": [50364, 570, 309, 1177, 380, 643, 281, 13, 50464, 50564, 400, 797, 11, 498, 291, 13859, 318, 294, 2115, 295, 257, 4230, 11, 50814, 50814, 370, 510, 264, 3876, 2906, 264, 4018, 2115, 294, 264, 318, 8141, 13, 51214, 51214, 400, 5699, 291, 500, 380, 362, 257, 1622, 1296, 732, 4122, 11, 51364, 51364, 456, 311, 257, 2107, 12, 32226, 1433, 294, 264, 318, 8141, 11, 558, 30, 51514, 51514, 407, 633, 4111, 20406, 1208, 439, 661, 4122, 51614, 51614, 3993, 264, 2306, 300, 366, 493, 264, 4230, 51714, 51714, 420, 760, 264, 4230, 490, 309, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0720747071083146, "compression_ratio": 1.6859903381642511, "no_speech_prob": 7.88334000390023e-06}, {"id": 381, "seek": 231406, "start": 2318.06, "end": 2323.06, "text": " And again, if you organize S in terms of a tree,", "tokens": [50364, 570, 309, 1177, 380, 643, 281, 13, 50464, 50564, 400, 797, 11, 498, 291, 13859, 318, 294, 2115, 295, 257, 4230, 11, 50814, 50814, 370, 510, 264, 3876, 2906, 264, 4018, 2115, 294, 264, 318, 8141, 13, 51214, 51214, 400, 5699, 291, 500, 380, 362, 257, 1622, 1296, 732, 4122, 11, 51364, 51364, 456, 311, 257, 2107, 12, 32226, 1433, 294, 264, 318, 8141, 11, 558, 30, 51514, 51514, 407, 633, 4111, 20406, 1208, 439, 661, 4122, 51614, 51614, 3993, 264, 2306, 300, 366, 493, 264, 4230, 51714, 51714, 420, 760, 264, 4230, 490, 309, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0720747071083146, "compression_ratio": 1.6859903381642511, "no_speech_prob": 7.88334000390023e-06}, {"id": 382, "seek": 231406, "start": 2323.06, "end": 2331.06, "text": " so here the lines represent the zero terms in the S matrix.", "tokens": [50364, 570, 309, 1177, 380, 643, 281, 13, 50464, 50564, 400, 797, 11, 498, 291, 13859, 318, 294, 2115, 295, 257, 4230, 11, 50814, 50814, 370, 510, 264, 3876, 2906, 264, 4018, 2115, 294, 264, 318, 8141, 13, 51214, 51214, 400, 5699, 291, 500, 380, 362, 257, 1622, 1296, 732, 4122, 11, 51364, 51364, 456, 311, 257, 2107, 12, 32226, 1433, 294, 264, 318, 8141, 11, 558, 30, 51514, 51514, 407, 633, 4111, 20406, 1208, 439, 661, 4122, 51614, 51614, 3993, 264, 2306, 300, 366, 493, 264, 4230, 51714, 51714, 420, 760, 264, 4230, 490, 309, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0720747071083146, "compression_ratio": 1.6859903381642511, "no_speech_prob": 7.88334000390023e-06}, {"id": 383, "seek": 231406, "start": 2331.06, "end": 2334.06, "text": " And whenever you don't have a line between two features,", "tokens": [50364, 570, 309, 1177, 380, 643, 281, 13, 50464, 50564, 400, 797, 11, 498, 291, 13859, 318, 294, 2115, 295, 257, 4230, 11, 50814, 50814, 370, 510, 264, 3876, 2906, 264, 4018, 2115, 294, 264, 318, 8141, 13, 51214, 51214, 400, 5699, 291, 500, 380, 362, 257, 1622, 1296, 732, 4122, 11, 51364, 51364, 456, 311, 257, 2107, 12, 32226, 1433, 294, 264, 318, 8141, 11, 558, 30, 51514, 51514, 407, 633, 4111, 20406, 1208, 439, 661, 4122, 51614, 51614, 3993, 264, 2306, 300, 366, 493, 264, 4230, 51714, 51714, 420, 760, 264, 4230, 490, 309, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0720747071083146, "compression_ratio": 1.6859903381642511, "no_speech_prob": 7.88334000390023e-06}, {"id": 384, "seek": 231406, "start": 2334.06, "end": 2337.06, "text": " there's a non-zero term in the S matrix, right?", "tokens": [50364, 570, 309, 1177, 380, 643, 281, 13, 50464, 50564, 400, 797, 11, 498, 291, 13859, 318, 294, 2115, 295, 257, 4230, 11, 50814, 50814, 370, 510, 264, 3876, 2906, 264, 4018, 2115, 294, 264, 318, 8141, 13, 51214, 51214, 400, 5699, 291, 500, 380, 362, 257, 1622, 1296, 732, 4122, 11, 51364, 51364, 456, 311, 257, 2107, 12, 32226, 1433, 294, 264, 318, 8141, 11, 558, 30, 51514, 51514, 407, 633, 4111, 20406, 1208, 439, 661, 4122, 51614, 51614, 3993, 264, 2306, 300, 366, 493, 264, 4230, 51714, 51714, 420, 760, 264, 4230, 490, 309, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0720747071083146, "compression_ratio": 1.6859903381642511, "no_speech_prob": 7.88334000390023e-06}, {"id": 385, "seek": 231406, "start": 2337.06, "end": 2339.06, "text": " So every feature inhibits all other features", "tokens": [50364, 570, 309, 1177, 380, 643, 281, 13, 50464, 50564, 400, 797, 11, 498, 291, 13859, 318, 294, 2115, 295, 257, 4230, 11, 50814, 50814, 370, 510, 264, 3876, 2906, 264, 4018, 2115, 294, 264, 318, 8141, 13, 51214, 51214, 400, 5699, 291, 500, 380, 362, 257, 1622, 1296, 732, 4122, 11, 51364, 51364, 456, 311, 257, 2107, 12, 32226, 1433, 294, 264, 318, 8141, 11, 558, 30, 51514, 51514, 407, 633, 4111, 20406, 1208, 439, 661, 4122, 51614, 51614, 3993, 264, 2306, 300, 366, 493, 264, 4230, 51714, 51714, 420, 760, 264, 4230, 490, 309, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0720747071083146, "compression_ratio": 1.6859903381642511, "no_speech_prob": 7.88334000390023e-06}, {"id": 386, "seek": 231406, "start": 2339.06, "end": 2341.06, "text": " except the ones that are up the tree", "tokens": [50364, 570, 309, 1177, 380, 643, 281, 13, 50464, 50564, 400, 797, 11, 498, 291, 13859, 318, 294, 2115, 295, 257, 4230, 11, 50814, 50814, 370, 510, 264, 3876, 2906, 264, 4018, 2115, 294, 264, 318, 8141, 13, 51214, 51214, 400, 5699, 291, 500, 380, 362, 257, 1622, 1296, 732, 4122, 11, 51364, 51364, 456, 311, 257, 2107, 12, 32226, 1433, 294, 264, 318, 8141, 11, 558, 30, 51514, 51514, 407, 633, 4111, 20406, 1208, 439, 661, 4122, 51614, 51614, 3993, 264, 2306, 300, 366, 493, 264, 4230, 51714, 51714, 420, 760, 264, 4230, 490, 309, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0720747071083146, "compression_ratio": 1.6859903381642511, "no_speech_prob": 7.88334000390023e-06}, {"id": 387, "seek": 234106, "start": 2341.06, "end": 2344.06, "text": " or down the tree from it.", "tokens": [50364, 420, 760, 264, 4230, 490, 309, 13, 50514, 50514, 400, 341, 307, 588, 709, 411, 1594, 637, 685, 507, 257, 707, 857, 13, 50664, 50664, 467, 311, 733, 295, 264, 416, 4308, 11, 498, 291, 528, 11, 295, 1594, 637, 685, 507, 13, 50914, 50914, 7156, 295, 1566, 4122, 1951, 257, 9819, 295, 264, 4230, 51114, 51114, 643, 281, 312, 18157, 1214, 538, 46608, 441, 17, 11, 51364, 51364, 46608, 264, 1230, 295, 1270, 3935, 300, 366, 322, 11, 51514, 51514, 510, 291, 20803, 362, 257, 1333, 295, 20406, 849, 1433, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.048886791028474506, "compression_ratio": 1.5925925925925926, "no_speech_prob": 8.004893970792182e-06}, {"id": 388, "seek": 234106, "start": 2344.06, "end": 2347.06, "text": " And this is very much like group sparsity a little bit.", "tokens": [50364, 420, 760, 264, 4230, 490, 309, 13, 50514, 50514, 400, 341, 307, 588, 709, 411, 1594, 637, 685, 507, 257, 707, 857, 13, 50664, 50664, 467, 311, 733, 295, 264, 416, 4308, 11, 498, 291, 528, 11, 295, 1594, 637, 685, 507, 13, 50914, 50914, 7156, 295, 1566, 4122, 1951, 257, 9819, 295, 264, 4230, 51114, 51114, 643, 281, 312, 18157, 1214, 538, 46608, 441, 17, 11, 51364, 51364, 46608, 264, 1230, 295, 1270, 3935, 300, 366, 322, 11, 51514, 51514, 510, 291, 20803, 362, 257, 1333, 295, 20406, 849, 1433, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.048886791028474506, "compression_ratio": 1.5925925925925926, "no_speech_prob": 8.004893970792182e-06}, {"id": 389, "seek": 234106, "start": 2347.06, "end": 2352.06, "text": " It's kind of the converse, if you want, of group sparsity.", "tokens": [50364, 420, 760, 264, 4230, 490, 309, 13, 50514, 50514, 400, 341, 307, 588, 709, 411, 1594, 637, 685, 507, 257, 707, 857, 13, 50664, 50664, 467, 311, 733, 295, 264, 416, 4308, 11, 498, 291, 528, 11, 295, 1594, 637, 685, 507, 13, 50914, 50914, 7156, 295, 1566, 4122, 1951, 257, 9819, 295, 264, 4230, 51114, 51114, 643, 281, 312, 18157, 1214, 538, 46608, 441, 17, 11, 51364, 51364, 46608, 264, 1230, 295, 1270, 3935, 300, 366, 322, 11, 51514, 51514, 510, 291, 20803, 362, 257, 1333, 295, 20406, 849, 1433, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.048886791028474506, "compression_ratio": 1.5925925925925926, "no_speech_prob": 8.004893970792182e-06}, {"id": 390, "seek": 234106, "start": 2352.06, "end": 2356.06, "text": " Instead of saying features within a branch of the tree", "tokens": [50364, 420, 760, 264, 4230, 490, 309, 13, 50514, 50514, 400, 341, 307, 588, 709, 411, 1594, 637, 685, 507, 257, 707, 857, 13, 50664, 50664, 467, 311, 733, 295, 264, 416, 4308, 11, 498, 291, 528, 11, 295, 1594, 637, 685, 507, 13, 50914, 50914, 7156, 295, 1566, 4122, 1951, 257, 9819, 295, 264, 4230, 51114, 51114, 643, 281, 312, 18157, 1214, 538, 46608, 441, 17, 11, 51364, 51364, 46608, 264, 1230, 295, 1270, 3935, 300, 366, 322, 11, 51514, 51514, 510, 291, 20803, 362, 257, 1333, 295, 20406, 849, 1433, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.048886791028474506, "compression_ratio": 1.5925925925925926, "no_speech_prob": 8.004893970792182e-06}, {"id": 391, "seek": 234106, "start": 2356.06, "end": 2361.06, "text": " need to be activated together by minimizing L2,", "tokens": [50364, 420, 760, 264, 4230, 490, 309, 13, 50514, 50514, 400, 341, 307, 588, 709, 411, 1594, 637, 685, 507, 257, 707, 857, 13, 50664, 50664, 467, 311, 733, 295, 264, 416, 4308, 11, 498, 291, 528, 11, 295, 1594, 637, 685, 507, 13, 50914, 50914, 7156, 295, 1566, 4122, 1951, 257, 9819, 295, 264, 4230, 51114, 51114, 643, 281, 312, 18157, 1214, 538, 46608, 441, 17, 11, 51364, 51364, 46608, 264, 1230, 295, 1270, 3935, 300, 366, 322, 11, 51514, 51514, 510, 291, 20803, 362, 257, 1333, 295, 20406, 849, 1433, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.048886791028474506, "compression_ratio": 1.5925925925925926, "no_speech_prob": 8.004893970792182e-06}, {"id": 392, "seek": 234106, "start": 2361.06, "end": 2364.06, "text": " minimizing the number of such groups that are on,", "tokens": [50364, 420, 760, 264, 4230, 490, 309, 13, 50514, 50514, 400, 341, 307, 588, 709, 411, 1594, 637, 685, 507, 257, 707, 857, 13, 50664, 50664, 467, 311, 733, 295, 264, 416, 4308, 11, 498, 291, 528, 11, 295, 1594, 637, 685, 507, 13, 50914, 50914, 7156, 295, 1566, 4122, 1951, 257, 9819, 295, 264, 4230, 51114, 51114, 643, 281, 312, 18157, 1214, 538, 46608, 441, 17, 11, 51364, 51364, 46608, 264, 1230, 295, 1270, 3935, 300, 366, 322, 11, 51514, 51514, 510, 291, 20803, 362, 257, 1333, 295, 20406, 849, 1433, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.048886791028474506, "compression_ratio": 1.5925925925925926, "no_speech_prob": 8.004893970792182e-06}, {"id": 393, "seek": 234106, "start": 2364.06, "end": 2368.06, "text": " here you explicitly have a sort of inhibition term", "tokens": [50364, 420, 760, 264, 4230, 490, 309, 13, 50514, 50514, 400, 341, 307, 588, 709, 411, 1594, 637, 685, 507, 257, 707, 857, 13, 50664, 50664, 467, 311, 733, 295, 264, 416, 4308, 11, 498, 291, 528, 11, 295, 1594, 637, 685, 507, 13, 50914, 50914, 7156, 295, 1566, 4122, 1951, 257, 9819, 295, 264, 4230, 51114, 51114, 643, 281, 312, 18157, 1214, 538, 46608, 441, 17, 11, 51364, 51364, 46608, 264, 1230, 295, 1270, 3935, 300, 366, 322, 11, 51514, 51514, 510, 291, 20803, 362, 257, 1333, 295, 20406, 849, 1433, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.048886791028474506, "compression_ratio": 1.5925925925925926, "no_speech_prob": 8.004893970792182e-06}, {"id": 394, "seek": 236806, "start": 2368.06, "end": 2373.06, "text": " that for every feature inhibits all other features", "tokens": [50364, 300, 337, 633, 4111, 20406, 1208, 439, 661, 4122, 50614, 50614, 294, 439, 264, 661, 14770, 295, 264, 4230, 13, 50814, 50814, 400, 437, 291, 536, 797, 307, 300, 291, 536, 341, 1185, 50964, 50964, 1333, 295, 17608, 264, 4122, 294, 257, 544, 420, 1570, 51114, 51114, 10957, 6700, 293, 294, 1270, 257, 636, 300, 4122, 51364, 51364, 2051, 257, 9819, 295, 264, 4230, 6805, 281, 1936, 51514, 51514, 264, 912, 4111, 457, 365, 1333, 295, 819, 4358, 51664, 51664, 295, 3048, 4253, 293, 550, 4122, 2051, 264, 26807, 88, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1125610534180986, "compression_ratio": 1.8679245283018868, "no_speech_prob": 1.831899135140702e-05}, {"id": 395, "seek": 236806, "start": 2373.06, "end": 2377.06, "text": " in all the other branches of the tree.", "tokens": [50364, 300, 337, 633, 4111, 20406, 1208, 439, 661, 4122, 50614, 50614, 294, 439, 264, 661, 14770, 295, 264, 4230, 13, 50814, 50814, 400, 437, 291, 536, 797, 307, 300, 291, 536, 341, 1185, 50964, 50964, 1333, 295, 17608, 264, 4122, 294, 257, 544, 420, 1570, 51114, 51114, 10957, 6700, 293, 294, 1270, 257, 636, 300, 4122, 51364, 51364, 2051, 257, 9819, 295, 264, 4230, 6805, 281, 1936, 51514, 51514, 264, 912, 4111, 457, 365, 1333, 295, 819, 4358, 51664, 51664, 295, 3048, 4253, 293, 550, 4122, 2051, 264, 26807, 88, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1125610534180986, "compression_ratio": 1.8679245283018868, "no_speech_prob": 1.831899135140702e-05}, {"id": 396, "seek": 236806, "start": 2377.06, "end": 2380.06, "text": " And what you see again is that you see this system", "tokens": [50364, 300, 337, 633, 4111, 20406, 1208, 439, 661, 4122, 50614, 50614, 294, 439, 264, 661, 14770, 295, 264, 4230, 13, 50814, 50814, 400, 437, 291, 536, 797, 307, 300, 291, 536, 341, 1185, 50964, 50964, 1333, 295, 17608, 264, 4122, 294, 257, 544, 420, 1570, 51114, 51114, 10957, 6700, 293, 294, 1270, 257, 636, 300, 4122, 51364, 51364, 2051, 257, 9819, 295, 264, 4230, 6805, 281, 1936, 51514, 51514, 264, 912, 4111, 457, 365, 1333, 295, 819, 4358, 51664, 51664, 295, 3048, 4253, 293, 550, 4122, 2051, 264, 26807, 88, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1125610534180986, "compression_ratio": 1.8679245283018868, "no_speech_prob": 1.831899135140702e-05}, {"id": 397, "seek": 236806, "start": 2380.06, "end": 2383.06, "text": " sort of organizing the features in a more or less", "tokens": [50364, 300, 337, 633, 4111, 20406, 1208, 439, 661, 4122, 50614, 50614, 294, 439, 264, 661, 14770, 295, 264, 4230, 13, 50814, 50814, 400, 437, 291, 536, 797, 307, 300, 291, 536, 341, 1185, 50964, 50964, 1333, 295, 17608, 264, 4122, 294, 257, 544, 420, 1570, 51114, 51114, 10957, 6700, 293, 294, 1270, 257, 636, 300, 4122, 51364, 51364, 2051, 257, 9819, 295, 264, 4230, 6805, 281, 1936, 51514, 51514, 264, 912, 4111, 457, 365, 1333, 295, 819, 4358, 51664, 51664, 295, 3048, 4253, 293, 550, 4122, 2051, 264, 26807, 88, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1125610534180986, "compression_ratio": 1.8679245283018868, "no_speech_prob": 1.831899135140702e-05}, {"id": 398, "seek": 236806, "start": 2383.06, "end": 2388.06, "text": " continuous fashion and in such a way that features", "tokens": [50364, 300, 337, 633, 4111, 20406, 1208, 439, 661, 4122, 50614, 50614, 294, 439, 264, 661, 14770, 295, 264, 4230, 13, 50814, 50814, 400, 437, 291, 536, 797, 307, 300, 291, 536, 341, 1185, 50964, 50964, 1333, 295, 17608, 264, 4122, 294, 257, 544, 420, 1570, 51114, 51114, 10957, 6700, 293, 294, 1270, 257, 636, 300, 4122, 51364, 51364, 2051, 257, 9819, 295, 264, 4230, 6805, 281, 1936, 51514, 51514, 264, 912, 4111, 457, 365, 1333, 295, 819, 4358, 51664, 51664, 295, 3048, 4253, 293, 550, 4122, 2051, 264, 26807, 88, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1125610534180986, "compression_ratio": 1.8679245283018868, "no_speech_prob": 1.831899135140702e-05}, {"id": 399, "seek": 236806, "start": 2388.06, "end": 2391.06, "text": " along a branch of the tree correspond to basically", "tokens": [50364, 300, 337, 633, 4111, 20406, 1208, 439, 661, 4122, 50614, 50614, 294, 439, 264, 661, 14770, 295, 264, 4230, 13, 50814, 50814, 400, 437, 291, 536, 797, 307, 300, 291, 536, 341, 1185, 50964, 50964, 1333, 295, 17608, 264, 4122, 294, 257, 544, 420, 1570, 51114, 51114, 10957, 6700, 293, 294, 1270, 257, 636, 300, 4122, 51364, 51364, 2051, 257, 9819, 295, 264, 4230, 6805, 281, 1936, 51514, 51514, 264, 912, 4111, 457, 365, 1333, 295, 819, 4358, 51664, 51664, 295, 3048, 4253, 293, 550, 4122, 2051, 264, 26807, 88, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1125610534180986, "compression_ratio": 1.8679245283018868, "no_speech_prob": 1.831899135140702e-05}, {"id": 400, "seek": 236806, "start": 2391.06, "end": 2394.06, "text": " the same feature but with sort of different levels", "tokens": [50364, 300, 337, 633, 4111, 20406, 1208, 439, 661, 4122, 50614, 50614, 294, 439, 264, 661, 14770, 295, 264, 4230, 13, 50814, 50814, 400, 437, 291, 536, 797, 307, 300, 291, 536, 341, 1185, 50964, 50964, 1333, 295, 17608, 264, 4122, 294, 257, 544, 420, 1570, 51114, 51114, 10957, 6700, 293, 294, 1270, 257, 636, 300, 4122, 51364, 51364, 2051, 257, 9819, 295, 264, 4230, 6805, 281, 1936, 51514, 51514, 264, 912, 4111, 457, 365, 1333, 295, 819, 4358, 51664, 51664, 295, 3048, 4253, 293, 550, 4122, 2051, 264, 26807, 88, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1125610534180986, "compression_ratio": 1.8679245283018868, "no_speech_prob": 1.831899135140702e-05}, {"id": 401, "seek": 236806, "start": 2394.06, "end": 2397.06, "text": " of selectivity and then features along the periphery", "tokens": [50364, 300, 337, 633, 4111, 20406, 1208, 439, 661, 4122, 50614, 50614, 294, 439, 264, 661, 14770, 295, 264, 4230, 13, 50814, 50814, 400, 437, 291, 536, 797, 307, 300, 291, 536, 341, 1185, 50964, 50964, 1333, 295, 17608, 264, 4122, 294, 257, 544, 420, 1570, 51114, 51114, 10957, 6700, 293, 294, 1270, 257, 636, 300, 4122, 51364, 51364, 2051, 257, 9819, 295, 264, 4230, 6805, 281, 1936, 51514, 51514, 264, 912, 4111, 457, 365, 1333, 295, 819, 4358, 51664, 51664, 295, 3048, 4253, 293, 550, 4122, 2051, 264, 26807, 88, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1125610534180986, "compression_ratio": 1.8679245283018868, "no_speech_prob": 1.831899135140702e-05}, {"id": 402, "seek": 239706, "start": 2397.06, "end": 2400.06, "text": " sort of vary more or less continuously because there is", "tokens": [50364, 1333, 295, 10559, 544, 420, 1570, 15684, 570, 456, 307, 50514, 50514, 20406, 849, 406, 445, 412, 264, 2767, 1496, 50664, 50664, 457, 611, 412, 264, 2808, 1496, 13, 50764, 51014, 2264, 11, 370, 281, 352, 646, 281, 341, 11, 264, 636, 291, 3847, 264, 1185, 51214, 51214, 307, 412, 1184, 24784, 291, 976, 364, 2031, 11, 291, 915, 257, 710, 51364, 51364, 300, 4464, 5660, 341, 2281, 2445, 13, 51514, 51514, 407, 291, 915, 257, 710, 300, 31499, 82, 457, 611, 4464, 5660, 51664, 51664, 264, 1150, 1433, 11, 597, 1355, 300, 498, 291, 362, 364, 318, 1718, 1433, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11486370746905987, "compression_ratio": 1.6875, "no_speech_prob": 1.695863829809241e-05}, {"id": 403, "seek": 239706, "start": 2400.06, "end": 2403.06, "text": " inhibition not just at the bottom level", "tokens": [50364, 1333, 295, 10559, 544, 420, 1570, 15684, 570, 456, 307, 50514, 50514, 20406, 849, 406, 445, 412, 264, 2767, 1496, 50664, 50664, 457, 611, 412, 264, 2808, 1496, 13, 50764, 51014, 2264, 11, 370, 281, 352, 646, 281, 341, 11, 264, 636, 291, 3847, 264, 1185, 51214, 51214, 307, 412, 1184, 24784, 291, 976, 364, 2031, 11, 291, 915, 257, 710, 51364, 51364, 300, 4464, 5660, 341, 2281, 2445, 13, 51514, 51514, 407, 291, 915, 257, 710, 300, 31499, 82, 457, 611, 4464, 5660, 51664, 51664, 264, 1150, 1433, 11, 597, 1355, 300, 498, 291, 362, 364, 318, 1718, 1433, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11486370746905987, "compression_ratio": 1.6875, "no_speech_prob": 1.695863829809241e-05}, {"id": 404, "seek": 239706, "start": 2403.06, "end": 2405.06, "text": " but also at the middle level.", "tokens": [50364, 1333, 295, 10559, 544, 420, 1570, 15684, 570, 456, 307, 50514, 50514, 20406, 849, 406, 445, 412, 264, 2767, 1496, 50664, 50664, 457, 611, 412, 264, 2808, 1496, 13, 50764, 51014, 2264, 11, 370, 281, 352, 646, 281, 341, 11, 264, 636, 291, 3847, 264, 1185, 51214, 51214, 307, 412, 1184, 24784, 291, 976, 364, 2031, 11, 291, 915, 257, 710, 51364, 51364, 300, 4464, 5660, 341, 2281, 2445, 13, 51514, 51514, 407, 291, 915, 257, 710, 300, 31499, 82, 457, 611, 4464, 5660, 51664, 51664, 264, 1150, 1433, 11, 597, 1355, 300, 498, 291, 362, 364, 318, 1718, 1433, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11486370746905987, "compression_ratio": 1.6875, "no_speech_prob": 1.695863829809241e-05}, {"id": 405, "seek": 239706, "start": 2410.06, "end": 2414.06, "text": " OK, so to go back to this, the way you train the system", "tokens": [50364, 1333, 295, 10559, 544, 420, 1570, 15684, 570, 456, 307, 50514, 50514, 20406, 849, 406, 445, 412, 264, 2767, 1496, 50664, 50664, 457, 611, 412, 264, 2808, 1496, 13, 50764, 51014, 2264, 11, 370, 281, 352, 646, 281, 341, 11, 264, 636, 291, 3847, 264, 1185, 51214, 51214, 307, 412, 1184, 24784, 291, 976, 364, 2031, 11, 291, 915, 257, 710, 51364, 51364, 300, 4464, 5660, 341, 2281, 2445, 13, 51514, 51514, 407, 291, 915, 257, 710, 300, 31499, 82, 457, 611, 4464, 5660, 51664, 51664, 264, 1150, 1433, 11, 597, 1355, 300, 498, 291, 362, 364, 318, 1718, 1433, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11486370746905987, "compression_ratio": 1.6875, "no_speech_prob": 1.695863829809241e-05}, {"id": 406, "seek": 239706, "start": 2414.06, "end": 2417.06, "text": " is at each iteration you give an x, you find a z", "tokens": [50364, 1333, 295, 10559, 544, 420, 1570, 15684, 570, 456, 307, 50514, 50514, 20406, 849, 406, 445, 412, 264, 2767, 1496, 50664, 50664, 457, 611, 412, 264, 2808, 1496, 13, 50764, 51014, 2264, 11, 370, 281, 352, 646, 281, 341, 11, 264, 636, 291, 3847, 264, 1185, 51214, 51214, 307, 412, 1184, 24784, 291, 976, 364, 2031, 11, 291, 915, 257, 710, 51364, 51364, 300, 4464, 5660, 341, 2281, 2445, 13, 51514, 51514, 407, 291, 915, 257, 710, 300, 31499, 82, 457, 611, 4464, 5660, 51664, 51664, 264, 1150, 1433, 11, 597, 1355, 300, 498, 291, 362, 364, 318, 1718, 1433, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11486370746905987, "compression_ratio": 1.6875, "no_speech_prob": 1.695863829809241e-05}, {"id": 407, "seek": 239706, "start": 2417.06, "end": 2420.06, "text": " that minimizes this energy function.", "tokens": [50364, 1333, 295, 10559, 544, 420, 1570, 15684, 570, 456, 307, 50514, 50514, 20406, 849, 406, 445, 412, 264, 2767, 1496, 50664, 50664, 457, 611, 412, 264, 2808, 1496, 13, 50764, 51014, 2264, 11, 370, 281, 352, 646, 281, 341, 11, 264, 636, 291, 3847, 264, 1185, 51214, 51214, 307, 412, 1184, 24784, 291, 976, 364, 2031, 11, 291, 915, 257, 710, 51364, 51364, 300, 4464, 5660, 341, 2281, 2445, 13, 51514, 51514, 407, 291, 915, 257, 710, 300, 31499, 82, 457, 611, 4464, 5660, 51664, 51664, 264, 1150, 1433, 11, 597, 1355, 300, 498, 291, 362, 364, 318, 1718, 1433, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11486370746905987, "compression_ratio": 1.6875, "no_speech_prob": 1.695863829809241e-05}, {"id": 408, "seek": 239706, "start": 2420.06, "end": 2423.06, "text": " So you find a z that reconstructs but also minimizes", "tokens": [50364, 1333, 295, 10559, 544, 420, 1570, 15684, 570, 456, 307, 50514, 50514, 20406, 849, 406, 445, 412, 264, 2767, 1496, 50664, 50664, 457, 611, 412, 264, 2808, 1496, 13, 50764, 51014, 2264, 11, 370, 281, 352, 646, 281, 341, 11, 264, 636, 291, 3847, 264, 1185, 51214, 51214, 307, 412, 1184, 24784, 291, 976, 364, 2031, 11, 291, 915, 257, 710, 51364, 51364, 300, 4464, 5660, 341, 2281, 2445, 13, 51514, 51514, 407, 291, 915, 257, 710, 300, 31499, 82, 457, 611, 4464, 5660, 51664, 51664, 264, 1150, 1433, 11, 597, 1355, 300, 498, 291, 362, 364, 318, 1718, 1433, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11486370746905987, "compression_ratio": 1.6875, "no_speech_prob": 1.695863829809241e-05}, {"id": 409, "seek": 239706, "start": 2423.06, "end": 2426.06, "text": " the second term, which means that if you have an Sij term", "tokens": [50364, 1333, 295, 10559, 544, 420, 1570, 15684, 570, 456, 307, 50514, 50514, 20406, 849, 406, 445, 412, 264, 2767, 1496, 50664, 50664, 457, 611, 412, 264, 2808, 1496, 13, 50764, 51014, 2264, 11, 370, 281, 352, 646, 281, 341, 11, 264, 636, 291, 3847, 264, 1185, 51214, 51214, 307, 412, 1184, 24784, 291, 976, 364, 2031, 11, 291, 915, 257, 710, 51364, 51364, 300, 4464, 5660, 341, 2281, 2445, 13, 51514, 51514, 407, 291, 915, 257, 710, 300, 31499, 82, 457, 611, 4464, 5660, 51664, 51664, 264, 1150, 1433, 11, 597, 1355, 300, 498, 291, 362, 364, 318, 1718, 1433, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11486370746905987, "compression_ratio": 1.6875, "no_speech_prob": 1.695863829809241e-05}, {"id": 410, "seek": 242606, "start": 2426.06, "end": 2430.06, "text": " that is non-zero, it wants either zi or zj to be", "tokens": [50364, 300, 307, 2107, 12, 32226, 11, 309, 2738, 2139, 710, 72, 420, 710, 73, 281, 312, 50564, 50564, 4018, 420, 412, 1935, 588, 1359, 13, 50714, 50714, 509, 360, 472, 1823, 295, 16235, 23475, 586, 281, 50964, 50964, 281, 733, 295, 5623, 261, 370, 382, 281, 17522, 264, 31565, 6713, 13, 51164, 51164, 400, 291, 360, 611, 11, 498, 291, 528, 11, 291, 393, 360, 472, 1823, 295, 51314, 51314, 16235, 382, 2207, 281, 652, 264, 2115, 294, 318, 4833, 51564, 51564, 538, 733, 295, 15866, 264, 16235, 295, 341, 2281, 365, 3104, 281, 318, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11349344735193734, "compression_ratio": 1.6587677725118484, "no_speech_prob": 3.9420883695129305e-05}, {"id": 411, "seek": 242606, "start": 2430.06, "end": 2433.06, "text": " zero or at least very small.", "tokens": [50364, 300, 307, 2107, 12, 32226, 11, 309, 2738, 2139, 710, 72, 420, 710, 73, 281, 312, 50564, 50564, 4018, 420, 412, 1935, 588, 1359, 13, 50714, 50714, 509, 360, 472, 1823, 295, 16235, 23475, 586, 281, 50964, 50964, 281, 733, 295, 5623, 261, 370, 382, 281, 17522, 264, 31565, 6713, 13, 51164, 51164, 400, 291, 360, 611, 11, 498, 291, 528, 11, 291, 393, 360, 472, 1823, 295, 51314, 51314, 16235, 382, 2207, 281, 652, 264, 2115, 294, 318, 4833, 51564, 51564, 538, 733, 295, 15866, 264, 16235, 295, 341, 2281, 365, 3104, 281, 318, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11349344735193734, "compression_ratio": 1.6587677725118484, "no_speech_prob": 3.9420883695129305e-05}, {"id": 412, "seek": 242606, "start": 2433.06, "end": 2438.06, "text": " You do one step of gradient descent now to", "tokens": [50364, 300, 307, 2107, 12, 32226, 11, 309, 2738, 2139, 710, 72, 420, 710, 73, 281, 312, 50564, 50564, 4018, 420, 412, 1935, 588, 1359, 13, 50714, 50714, 509, 360, 472, 1823, 295, 16235, 23475, 586, 281, 50964, 50964, 281, 733, 295, 5623, 261, 370, 382, 281, 17522, 264, 31565, 6713, 13, 51164, 51164, 400, 291, 360, 611, 11, 498, 291, 528, 11, 291, 393, 360, 472, 1823, 295, 51314, 51314, 16235, 382, 2207, 281, 652, 264, 2115, 294, 318, 4833, 51564, 51564, 538, 733, 295, 15866, 264, 16235, 295, 341, 2281, 365, 3104, 281, 318, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11349344735193734, "compression_ratio": 1.6587677725118484, "no_speech_prob": 3.9420883695129305e-05}, {"id": 413, "seek": 242606, "start": 2438.06, "end": 2442.06, "text": " to kind of update w so as to minimize the reconstruction error.", "tokens": [50364, 300, 307, 2107, 12, 32226, 11, 309, 2738, 2139, 710, 72, 420, 710, 73, 281, 312, 50564, 50564, 4018, 420, 412, 1935, 588, 1359, 13, 50714, 50714, 509, 360, 472, 1823, 295, 16235, 23475, 586, 281, 50964, 50964, 281, 733, 295, 5623, 261, 370, 382, 281, 17522, 264, 31565, 6713, 13, 51164, 51164, 400, 291, 360, 611, 11, 498, 291, 528, 11, 291, 393, 360, 472, 1823, 295, 51314, 51314, 16235, 382, 2207, 281, 652, 264, 2115, 294, 318, 4833, 51564, 51564, 538, 733, 295, 15866, 264, 16235, 295, 341, 2281, 365, 3104, 281, 318, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11349344735193734, "compression_ratio": 1.6587677725118484, "no_speech_prob": 3.9420883695129305e-05}, {"id": 414, "seek": 242606, "start": 2442.06, "end": 2445.06, "text": " And you do also, if you want, you can do one step of", "tokens": [50364, 300, 307, 2107, 12, 32226, 11, 309, 2738, 2139, 710, 72, 420, 710, 73, 281, 312, 50564, 50564, 4018, 420, 412, 1935, 588, 1359, 13, 50714, 50714, 509, 360, 472, 1823, 295, 16235, 23475, 586, 281, 50964, 50964, 281, 733, 295, 5623, 261, 370, 382, 281, 17522, 264, 31565, 6713, 13, 51164, 51164, 400, 291, 360, 611, 11, 498, 291, 528, 11, 291, 393, 360, 472, 1823, 295, 51314, 51314, 16235, 382, 2207, 281, 652, 264, 2115, 294, 318, 4833, 51564, 51564, 538, 733, 295, 15866, 264, 16235, 295, 341, 2281, 365, 3104, 281, 318, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11349344735193734, "compression_ratio": 1.6587677725118484, "no_speech_prob": 3.9420883695129305e-05}, {"id": 415, "seek": 242606, "start": 2445.06, "end": 2450.06, "text": " gradient ascent to make the terms in S larger", "tokens": [50364, 300, 307, 2107, 12, 32226, 11, 309, 2738, 2139, 710, 72, 420, 710, 73, 281, 312, 50564, 50564, 4018, 420, 412, 1935, 588, 1359, 13, 50714, 50714, 509, 360, 472, 1823, 295, 16235, 23475, 586, 281, 50964, 50964, 281, 733, 295, 5623, 261, 370, 382, 281, 17522, 264, 31565, 6713, 13, 51164, 51164, 400, 291, 360, 611, 11, 498, 291, 528, 11, 291, 393, 360, 472, 1823, 295, 51314, 51314, 16235, 382, 2207, 281, 652, 264, 2115, 294, 318, 4833, 51564, 51564, 538, 733, 295, 15866, 264, 16235, 295, 341, 2281, 365, 3104, 281, 318, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11349344735193734, "compression_ratio": 1.6587677725118484, "no_speech_prob": 3.9420883695129305e-05}, {"id": 416, "seek": 242606, "start": 2450.06, "end": 2453.06, "text": " by kind of computing the gradient of this energy with respect to S", "tokens": [50364, 300, 307, 2107, 12, 32226, 11, 309, 2738, 2139, 710, 72, 420, 710, 73, 281, 312, 50564, 50564, 4018, 420, 412, 1935, 588, 1359, 13, 50714, 50714, 509, 360, 472, 1823, 295, 16235, 23475, 586, 281, 50964, 50964, 281, 733, 295, 5623, 261, 370, 382, 281, 17522, 264, 31565, 6713, 13, 51164, 51164, 400, 291, 360, 611, 11, 498, 291, 528, 11, 291, 393, 360, 472, 1823, 295, 51314, 51314, 16235, 382, 2207, 281, 652, 264, 2115, 294, 318, 4833, 51564, 51564, 538, 733, 295, 15866, 264, 16235, 295, 341, 2281, 365, 3104, 281, 318, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11349344735193734, "compression_ratio": 1.6587677725118484, "no_speech_prob": 3.9420883695129305e-05}, {"id": 417, "seek": 245306, "start": 2453.06, "end": 2456.06, "text": " and then going up the energy not down.", "tokens": [50364, 293, 550, 516, 493, 264, 2281, 406, 760, 13, 50514, 50664, 3764, 11, 498, 291, 764, 406, 257, 4230, 457, 512, 1333, 295, 568, 35, 1192, 1793, 50864, 50864, 291, 611, 483, 729, 733, 295, 8294, 13, 51014, 51314, 400, 544, 3997, 2306, 498, 456, 366, 733, 295, 51464, 51464, 3866, 17408, 337, 264, 4122, 13, 51564, 51564, 2264, 11, 370, 709, 337, 637, 11668, 17720, 293, 18519, 637, 11668, 17720, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11773638976247687, "compression_ratio": 1.5083798882681565, "no_speech_prob": 2.8759943234035745e-05}, {"id": 418, "seek": 245306, "start": 2459.06, "end": 2463.06, "text": " Again, if you use not a tree but some sort of 2D topology", "tokens": [50364, 293, 550, 516, 493, 264, 2281, 406, 760, 13, 50514, 50664, 3764, 11, 498, 291, 764, 406, 257, 4230, 457, 512, 1333, 295, 568, 35, 1192, 1793, 50864, 50864, 291, 611, 483, 729, 733, 295, 8294, 13, 51014, 51314, 400, 544, 3997, 2306, 498, 456, 366, 733, 295, 51464, 51464, 3866, 17408, 337, 264, 4122, 13, 51564, 51564, 2264, 11, 370, 709, 337, 637, 11668, 17720, 293, 18519, 637, 11668, 17720, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11773638976247687, "compression_ratio": 1.5083798882681565, "no_speech_prob": 2.8759943234035745e-05}, {"id": 419, "seek": 245306, "start": 2463.06, "end": 2466.06, "text": " you also get those kind of patterns.", "tokens": [50364, 293, 550, 516, 493, 264, 2281, 406, 760, 13, 50514, 50664, 3764, 11, 498, 291, 764, 406, 257, 4230, 457, 512, 1333, 295, 568, 35, 1192, 1793, 50864, 50864, 291, 611, 483, 729, 733, 295, 8294, 13, 51014, 51314, 400, 544, 3997, 2306, 498, 456, 366, 733, 295, 51464, 51464, 3866, 17408, 337, 264, 4122, 13, 51564, 51564, 2264, 11, 370, 709, 337, 637, 11668, 17720, 293, 18519, 637, 11668, 17720, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11773638976247687, "compression_ratio": 1.5083798882681565, "no_speech_prob": 2.8759943234035745e-05}, {"id": 420, "seek": 245306, "start": 2472.06, "end": 2475.06, "text": " And more complex ones if there are kind of", "tokens": [50364, 293, 550, 516, 493, 264, 2281, 406, 760, 13, 50514, 50664, 3764, 11, 498, 291, 764, 406, 257, 4230, 457, 512, 1333, 295, 568, 35, 1192, 1793, 50864, 50864, 291, 611, 483, 729, 733, 295, 8294, 13, 51014, 51314, 400, 544, 3997, 2306, 498, 456, 366, 733, 295, 51464, 51464, 3866, 17408, 337, 264, 4122, 13, 51564, 51564, 2264, 11, 370, 709, 337, 637, 11668, 17720, 293, 18519, 637, 11668, 17720, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11773638976247687, "compression_ratio": 1.5083798882681565, "no_speech_prob": 2.8759943234035745e-05}, {"id": 421, "seek": 245306, "start": 2475.06, "end": 2477.06, "text": " multiple scales for the features.", "tokens": [50364, 293, 550, 516, 493, 264, 2281, 406, 760, 13, 50514, 50664, 3764, 11, 498, 291, 764, 406, 257, 4230, 457, 512, 1333, 295, 568, 35, 1192, 1793, 50864, 50864, 291, 611, 483, 729, 733, 295, 8294, 13, 51014, 51314, 400, 544, 3997, 2306, 498, 456, 366, 733, 295, 51464, 51464, 3866, 17408, 337, 264, 4122, 13, 51564, 51564, 2264, 11, 370, 709, 337, 637, 11668, 17720, 293, 18519, 637, 11668, 17720, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11773638976247687, "compression_ratio": 1.5083798882681565, "no_speech_prob": 2.8759943234035745e-05}, {"id": 422, "seek": 245306, "start": 2477.06, "end": 2481.06, "text": " OK, so much for sparse coding and structured sparse coding.", "tokens": [50364, 293, 550, 516, 493, 264, 2281, 406, 760, 13, 50514, 50664, 3764, 11, 498, 291, 764, 406, 257, 4230, 457, 512, 1333, 295, 568, 35, 1192, 1793, 50864, 50864, 291, 611, 483, 729, 733, 295, 8294, 13, 51014, 51314, 400, 544, 3997, 2306, 498, 456, 366, 733, 295, 51464, 51464, 3866, 17408, 337, 264, 4122, 13, 51564, 51564, 2264, 11, 370, 709, 337, 637, 11668, 17720, 293, 18519, 637, 11668, 17720, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11773638976247687, "compression_ratio": 1.5083798882681565, "no_speech_prob": 2.8759943234035745e-05}, {"id": 423, "seek": 248106, "start": 2481.06, "end": 2484.06, "text": " And the reason I'm telling you about this is because", "tokens": [50364, 400, 264, 1778, 286, 478, 3585, 291, 466, 341, 307, 570, 50514, 50514, 4878, 729, 500, 380, 362, 257, 2603, 2372, 295, 8496, 5821, 50664, 50664, 264, 637, 11668, 17720, 11, 18519, 637, 11668, 17720, 50864, 50864, 436, 11, 294, 452, 4800, 11, 486, 312, 264, 5143, 337, 733, 295, 51114, 51114, 2698, 12, 48172, 24420, 2539, 7150, 295, 264, 958, 1326, 924, 13, 51314, 51314, 1018, 286, 1907, 291, 11, 286, 519, 2698, 12, 48172, 24420, 2539, 558, 586, 307, 264, 13158, 4829, 294, 426, 45196, 51514, 51514, 293, 309, 311, 5617, 733, 295, 257, 857, 295, 257, 2368, 4829, 294, 3820, 5201, 382, 731, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09751115403733812, "compression_ratio": 1.6602316602316602, "no_speech_prob": 2.620595842017792e-05}, {"id": 424, "seek": 248106, "start": 2484.06, "end": 2487.06, "text": " although those don't have a huge amount of practical applications", "tokens": [50364, 400, 264, 1778, 286, 478, 3585, 291, 466, 341, 307, 570, 50514, 50514, 4878, 729, 500, 380, 362, 257, 2603, 2372, 295, 8496, 5821, 50664, 50664, 264, 637, 11668, 17720, 11, 18519, 637, 11668, 17720, 50864, 50864, 436, 11, 294, 452, 4800, 11, 486, 312, 264, 5143, 337, 733, 295, 51114, 51114, 2698, 12, 48172, 24420, 2539, 7150, 295, 264, 958, 1326, 924, 13, 51314, 51314, 1018, 286, 1907, 291, 11, 286, 519, 2698, 12, 48172, 24420, 2539, 558, 586, 307, 264, 13158, 4829, 294, 426, 45196, 51514, 51514, 293, 309, 311, 5617, 733, 295, 257, 857, 295, 257, 2368, 4829, 294, 3820, 5201, 382, 731, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09751115403733812, "compression_ratio": 1.6602316602316602, "no_speech_prob": 2.620595842017792e-05}, {"id": 425, "seek": 248106, "start": 2487.06, "end": 2491.06, "text": " the sparse coding, structured sparse coding", "tokens": [50364, 400, 264, 1778, 286, 478, 3585, 291, 466, 341, 307, 570, 50514, 50514, 4878, 729, 500, 380, 362, 257, 2603, 2372, 295, 8496, 5821, 50664, 50664, 264, 637, 11668, 17720, 11, 18519, 637, 11668, 17720, 50864, 50864, 436, 11, 294, 452, 4800, 11, 486, 312, 264, 5143, 337, 733, 295, 51114, 51114, 2698, 12, 48172, 24420, 2539, 7150, 295, 264, 958, 1326, 924, 13, 51314, 51314, 1018, 286, 1907, 291, 11, 286, 519, 2698, 12, 48172, 24420, 2539, 558, 586, 307, 264, 13158, 4829, 294, 426, 45196, 51514, 51514, 293, 309, 311, 5617, 733, 295, 257, 857, 295, 257, 2368, 4829, 294, 3820, 5201, 382, 731, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09751115403733812, "compression_ratio": 1.6602316602316602, "no_speech_prob": 2.620595842017792e-05}, {"id": 426, "seek": 248106, "start": 2491.06, "end": 2496.06, "text": " they, in my opinion, will be the basis for kind of", "tokens": [50364, 400, 264, 1778, 286, 478, 3585, 291, 466, 341, 307, 570, 50514, 50514, 4878, 729, 500, 380, 362, 257, 2603, 2372, 295, 8496, 5821, 50664, 50664, 264, 637, 11668, 17720, 11, 18519, 637, 11668, 17720, 50864, 50864, 436, 11, 294, 452, 4800, 11, 486, 312, 264, 5143, 337, 733, 295, 51114, 51114, 2698, 12, 48172, 24420, 2539, 7150, 295, 264, 958, 1326, 924, 13, 51314, 51314, 1018, 286, 1907, 291, 11, 286, 519, 2698, 12, 48172, 24420, 2539, 558, 586, 307, 264, 13158, 4829, 294, 426, 45196, 51514, 51514, 293, 309, 311, 5617, 733, 295, 257, 857, 295, 257, 2368, 4829, 294, 3820, 5201, 382, 731, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09751115403733812, "compression_ratio": 1.6602316602316602, "no_speech_prob": 2.620595842017792e-05}, {"id": 427, "seek": 248106, "start": 2496.06, "end": 2500.06, "text": " self-supervised learning methods of the next few years.", "tokens": [50364, 400, 264, 1778, 286, 478, 3585, 291, 466, 341, 307, 570, 50514, 50514, 4878, 729, 500, 380, 362, 257, 2603, 2372, 295, 8496, 5821, 50664, 50664, 264, 637, 11668, 17720, 11, 18519, 637, 11668, 17720, 50864, 50864, 436, 11, 294, 452, 4800, 11, 486, 312, 264, 5143, 337, 733, 295, 51114, 51114, 2698, 12, 48172, 24420, 2539, 7150, 295, 264, 958, 1326, 924, 13, 51314, 51314, 1018, 286, 1907, 291, 11, 286, 519, 2698, 12, 48172, 24420, 2539, 558, 586, 307, 264, 13158, 4829, 294, 426, 45196, 51514, 51514, 293, 309, 311, 5617, 733, 295, 257, 857, 295, 257, 2368, 4829, 294, 3820, 5201, 382, 731, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09751115403733812, "compression_ratio": 1.6602316602316602, "no_speech_prob": 2.620595842017792e-05}, {"id": 428, "seek": 248106, "start": 2500.06, "end": 2504.06, "text": " As I told you, I think self-supervised learning right now is the hardest topic in NLP", "tokens": [50364, 400, 264, 1778, 286, 478, 3585, 291, 466, 341, 307, 570, 50514, 50514, 4878, 729, 500, 380, 362, 257, 2603, 2372, 295, 8496, 5821, 50664, 50664, 264, 637, 11668, 17720, 11, 18519, 637, 11668, 17720, 50864, 50864, 436, 11, 294, 452, 4800, 11, 486, 312, 264, 5143, 337, 733, 295, 51114, 51114, 2698, 12, 48172, 24420, 2539, 7150, 295, 264, 958, 1326, 924, 13, 51314, 51314, 1018, 286, 1907, 291, 11, 286, 519, 2698, 12, 48172, 24420, 2539, 558, 586, 307, 264, 13158, 4829, 294, 426, 45196, 51514, 51514, 293, 309, 311, 5617, 733, 295, 257, 857, 295, 257, 2368, 4829, 294, 3820, 5201, 382, 731, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09751115403733812, "compression_ratio": 1.6602316602316602, "no_speech_prob": 2.620595842017792e-05}, {"id": 429, "seek": 248106, "start": 2504.06, "end": 2509.06, "text": " and it's becoming kind of a bit of a hot topic in computer vision as well.", "tokens": [50364, 400, 264, 1778, 286, 478, 3585, 291, 466, 341, 307, 570, 50514, 50514, 4878, 729, 500, 380, 362, 257, 2603, 2372, 295, 8496, 5821, 50664, 50664, 264, 637, 11668, 17720, 11, 18519, 637, 11668, 17720, 50864, 50864, 436, 11, 294, 452, 4800, 11, 486, 312, 264, 5143, 337, 733, 295, 51114, 51114, 2698, 12, 48172, 24420, 2539, 7150, 295, 264, 958, 1326, 924, 13, 51314, 51314, 1018, 286, 1907, 291, 11, 286, 519, 2698, 12, 48172, 24420, 2539, 558, 586, 307, 264, 13158, 4829, 294, 426, 45196, 51514, 51514, 293, 309, 311, 5617, 733, 295, 257, 857, 295, 257, 2368, 4829, 294, 3820, 5201, 382, 731, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09751115403733812, "compression_ratio": 1.6602316602316602, "no_speech_prob": 2.620595842017792e-05}, {"id": 430, "seek": 250906, "start": 2509.06, "end": 2512.06, "text": " And it's mostly now dominated by contrasting methods", "tokens": [50364, 400, 309, 311, 5240, 586, 23755, 538, 8712, 278, 7150, 50514, 50514, 457, 286, 519, 264, 26621, 7150, 366, 516, 281, 747, 670, 50664, 50664, 570, 8712, 278, 7150, 500, 380, 4373, 588, 731, 13, 50814, 50814, 407, 341, 307, 1333, 295, 2902, 291, 7278, 337, 264, 2027, 11, 498, 291, 528, 13, 51114, 51114, 36858, 437, 341, 307, 439, 466, 13, 51264, 51264, 2264, 11, 586, 337, 746, 2584, 819, 13, 51364, 51364, 639, 307, 746, 300, 28327, 78, 486, 411, 570, 415, 1985, 322, 341, 1716, 13, 51614, 51614, 400, 309, 311, 472, 295, 264, 5022, 11, 1391, 472, 295, 264, 881, 1021, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0886256979146135, "compression_ratio": 1.6981132075471699, "no_speech_prob": 2.74714420811506e-05}, {"id": 431, "seek": 250906, "start": 2512.06, "end": 2515.06, "text": " but I think the architectural methods are going to take over", "tokens": [50364, 400, 309, 311, 5240, 586, 23755, 538, 8712, 278, 7150, 50514, 50514, 457, 286, 519, 264, 26621, 7150, 366, 516, 281, 747, 670, 50664, 50664, 570, 8712, 278, 7150, 500, 380, 4373, 588, 731, 13, 50814, 50814, 407, 341, 307, 1333, 295, 2902, 291, 7278, 337, 264, 2027, 11, 498, 291, 528, 13, 51114, 51114, 36858, 437, 341, 307, 439, 466, 13, 51264, 51264, 2264, 11, 586, 337, 746, 2584, 819, 13, 51364, 51364, 639, 307, 746, 300, 28327, 78, 486, 411, 570, 415, 1985, 322, 341, 1716, 13, 51614, 51614, 400, 309, 311, 472, 295, 264, 5022, 11, 1391, 472, 295, 264, 881, 1021, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0886256979146135, "compression_ratio": 1.6981132075471699, "no_speech_prob": 2.74714420811506e-05}, {"id": 432, "seek": 250906, "start": 2515.06, "end": 2518.06, "text": " because contrasting methods don't scale very well.", "tokens": [50364, 400, 309, 311, 5240, 586, 23755, 538, 8712, 278, 7150, 50514, 50514, 457, 286, 519, 264, 26621, 7150, 366, 516, 281, 747, 670, 50664, 50664, 570, 8712, 278, 7150, 500, 380, 4373, 588, 731, 13, 50814, 50814, 407, 341, 307, 1333, 295, 2902, 291, 7278, 337, 264, 2027, 11, 498, 291, 528, 13, 51114, 51114, 36858, 437, 341, 307, 439, 466, 13, 51264, 51264, 2264, 11, 586, 337, 746, 2584, 819, 13, 51364, 51364, 639, 307, 746, 300, 28327, 78, 486, 411, 570, 415, 1985, 322, 341, 1716, 13, 51614, 51614, 400, 309, 311, 472, 295, 264, 5022, 11, 1391, 472, 295, 264, 881, 1021, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0886256979146135, "compression_ratio": 1.6981132075471699, "no_speech_prob": 2.74714420811506e-05}, {"id": 433, "seek": 250906, "start": 2518.06, "end": 2524.06, "text": " So this is sort of giving you weapons for the future, if you want.", "tokens": [50364, 400, 309, 311, 5240, 586, 23755, 538, 8712, 278, 7150, 50514, 50514, 457, 286, 519, 264, 26621, 7150, 366, 516, 281, 747, 670, 50664, 50664, 570, 8712, 278, 7150, 500, 380, 4373, 588, 731, 13, 50814, 50814, 407, 341, 307, 1333, 295, 2902, 291, 7278, 337, 264, 2027, 11, 498, 291, 528, 13, 51114, 51114, 36858, 437, 341, 307, 439, 466, 13, 51264, 51264, 2264, 11, 586, 337, 746, 2584, 819, 13, 51364, 51364, 639, 307, 746, 300, 28327, 78, 486, 411, 570, 415, 1985, 322, 341, 1716, 13, 51614, 51614, 400, 309, 311, 472, 295, 264, 5022, 11, 1391, 472, 295, 264, 881, 1021, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0886256979146135, "compression_ratio": 1.6981132075471699, "no_speech_prob": 2.74714420811506e-05}, {"id": 434, "seek": 250906, "start": 2524.06, "end": 2527.06, "text": " Understanding what this is all about.", "tokens": [50364, 400, 309, 311, 5240, 586, 23755, 538, 8712, 278, 7150, 50514, 50514, 457, 286, 519, 264, 26621, 7150, 366, 516, 281, 747, 670, 50664, 50664, 570, 8712, 278, 7150, 500, 380, 4373, 588, 731, 13, 50814, 50814, 407, 341, 307, 1333, 295, 2902, 291, 7278, 337, 264, 2027, 11, 498, 291, 528, 13, 51114, 51114, 36858, 437, 341, 307, 439, 466, 13, 51264, 51264, 2264, 11, 586, 337, 746, 2584, 819, 13, 51364, 51364, 639, 307, 746, 300, 28327, 78, 486, 411, 570, 415, 1985, 322, 341, 1716, 13, 51614, 51614, 400, 309, 311, 472, 295, 264, 5022, 11, 1391, 472, 295, 264, 881, 1021, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0886256979146135, "compression_ratio": 1.6981132075471699, "no_speech_prob": 2.74714420811506e-05}, {"id": 435, "seek": 250906, "start": 2527.06, "end": 2529.06, "text": " OK, now for something completely different.", "tokens": [50364, 400, 309, 311, 5240, 586, 23755, 538, 8712, 278, 7150, 50514, 50514, 457, 286, 519, 264, 26621, 7150, 366, 516, 281, 747, 670, 50664, 50664, 570, 8712, 278, 7150, 500, 380, 4373, 588, 731, 13, 50814, 50814, 407, 341, 307, 1333, 295, 2902, 291, 7278, 337, 264, 2027, 11, 498, 291, 528, 13, 51114, 51114, 36858, 437, 341, 307, 439, 466, 13, 51264, 51264, 2264, 11, 586, 337, 746, 2584, 819, 13, 51364, 51364, 639, 307, 746, 300, 28327, 78, 486, 411, 570, 415, 1985, 322, 341, 1716, 13, 51614, 51614, 400, 309, 311, 472, 295, 264, 5022, 11, 1391, 472, 295, 264, 881, 1021, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0886256979146135, "compression_ratio": 1.6981132075471699, "no_speech_prob": 2.74714420811506e-05}, {"id": 436, "seek": 250906, "start": 2529.06, "end": 2534.06, "text": " This is something that Alfredo will like because he works on this project.", "tokens": [50364, 400, 309, 311, 5240, 586, 23755, 538, 8712, 278, 7150, 50514, 50514, 457, 286, 519, 264, 26621, 7150, 366, 516, 281, 747, 670, 50664, 50664, 570, 8712, 278, 7150, 500, 380, 4373, 588, 731, 13, 50814, 50814, 407, 341, 307, 1333, 295, 2902, 291, 7278, 337, 264, 2027, 11, 498, 291, 528, 13, 51114, 51114, 36858, 437, 341, 307, 439, 466, 13, 51264, 51264, 2264, 11, 586, 337, 746, 2584, 819, 13, 51364, 51364, 639, 307, 746, 300, 28327, 78, 486, 411, 570, 415, 1985, 322, 341, 1716, 13, 51614, 51614, 400, 309, 311, 472, 295, 264, 5022, 11, 1391, 472, 295, 264, 881, 1021, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0886256979146135, "compression_ratio": 1.6981132075471699, "no_speech_prob": 2.74714420811506e-05}, {"id": 437, "seek": 250906, "start": 2534.06, "end": 2538.06, "text": " And it's one of the users, probably one of the most important", "tokens": [50364, 400, 309, 311, 5240, 586, 23755, 538, 8712, 278, 7150, 50514, 50514, 457, 286, 519, 264, 26621, 7150, 366, 516, 281, 747, 670, 50664, 50664, 570, 8712, 278, 7150, 500, 380, 4373, 588, 731, 13, 50814, 50814, 407, 341, 307, 1333, 295, 2902, 291, 7278, 337, 264, 2027, 11, 498, 291, 528, 13, 51114, 51114, 36858, 437, 341, 307, 439, 466, 13, 51264, 51264, 2264, 11, 586, 337, 746, 2584, 819, 13, 51364, 51364, 639, 307, 746, 300, 28327, 78, 486, 411, 570, 415, 1985, 322, 341, 1716, 13, 51614, 51614, 400, 309, 311, 472, 295, 264, 5022, 11, 1391, 472, 295, 264, 881, 1021, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0886256979146135, "compression_ratio": 1.6981132075471699, "no_speech_prob": 2.74714420811506e-05}, {"id": 438, "seek": 253806, "start": 2538.06, "end": 2542.06, "text": " uses of self-supervised learning, is the idea of", "tokens": [50364, 4960, 295, 2698, 12, 48172, 24420, 2539, 11, 307, 264, 1558, 295, 50564, 50564, 2539, 1002, 5245, 337, 1969, 3652, 420, 337, 661, 9932, 13, 50914, 50914, 407, 562, 321, 11, 562, 6255, 420, 4882, 1466, 257, 5633, 51264, 51264, 321, 1596, 2745, 362, 257, 733, 295, 665, 6920, 2316, 51464, 51464, 295, 577, 264, 1002, 1985, 11, 295, 21769, 10649, 11, 295, 264, 1186, 300, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12230454853602818, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.00010949296847684309}, {"id": 439, "seek": 253806, "start": 2542.06, "end": 2549.06, "text": " learning world models for control systems or for other purposes.", "tokens": [50364, 4960, 295, 2698, 12, 48172, 24420, 2539, 11, 307, 264, 1558, 295, 50564, 50564, 2539, 1002, 5245, 337, 1969, 3652, 420, 337, 661, 9932, 13, 50914, 50914, 407, 562, 321, 11, 562, 6255, 420, 4882, 1466, 257, 5633, 51264, 51264, 321, 1596, 2745, 362, 257, 733, 295, 665, 6920, 2316, 51464, 51464, 295, 577, 264, 1002, 1985, 11, 295, 21769, 10649, 11, 295, 264, 1186, 300, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12230454853602818, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.00010949296847684309}, {"id": 440, "seek": 253806, "start": 2549.06, "end": 2556.06, "text": " So when we, when humans or animals learn a task", "tokens": [50364, 4960, 295, 2698, 12, 48172, 24420, 2539, 11, 307, 264, 1558, 295, 50564, 50564, 2539, 1002, 5245, 337, 1969, 3652, 420, 337, 661, 9932, 13, 50914, 50914, 407, 562, 321, 11, 562, 6255, 420, 4882, 1466, 257, 5633, 51264, 51264, 321, 1596, 2745, 362, 257, 733, 295, 665, 6920, 2316, 51464, 51464, 295, 577, 264, 1002, 1985, 11, 295, 21769, 10649, 11, 295, 264, 1186, 300, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12230454853602818, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.00010949296847684309}, {"id": 441, "seek": 253806, "start": 2556.06, "end": 2560.06, "text": " we quite obviously have a kind of good internal model", "tokens": [50364, 4960, 295, 2698, 12, 48172, 24420, 2539, 11, 307, 264, 1558, 295, 50564, 50564, 2539, 1002, 5245, 337, 1969, 3652, 420, 337, 661, 9932, 13, 50914, 50914, 407, 562, 321, 11, 562, 6255, 420, 4882, 1466, 257, 5633, 51264, 51264, 321, 1596, 2745, 362, 257, 733, 295, 665, 6920, 2316, 51464, 51464, 295, 577, 264, 1002, 1985, 11, 295, 21769, 10649, 11, 295, 264, 1186, 300, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12230454853602818, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.00010949296847684309}, {"id": 442, "seek": 253806, "start": 2560.06, "end": 2565.06, "text": " of how the world works, of intuitive physics, of the fact that", "tokens": [50364, 4960, 295, 2698, 12, 48172, 24420, 2539, 11, 307, 264, 1558, 295, 50564, 50564, 2539, 1002, 5245, 337, 1969, 3652, 420, 337, 661, 9932, 13, 50914, 50914, 407, 562, 321, 11, 562, 6255, 420, 4882, 1466, 257, 5633, 51264, 51264, 321, 1596, 2745, 362, 257, 733, 295, 665, 6920, 2316, 51464, 51464, 295, 577, 264, 1002, 1985, 11, 295, 21769, 10649, 11, 295, 264, 1186, 300, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12230454853602818, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.00010949296847684309}, {"id": 443, "seek": 256506, "start": 2565.06, "end": 2568.06, "text": " when an object is not supported, it falls.", "tokens": [50364, 562, 364, 2657, 307, 406, 8104, 11, 309, 8804, 13, 50514, 50514, 492, 600, 3264, 12110, 562, 321, 645, 10917, 11, 1391, 926, 264, 3205, 295, 4949, 2493, 420, 370, 13, 50714, 50714, 17708, 420, 4949, 2493, 13, 663, 311, 562, 309, 16795, 493, 294, 10917, 13, 50914, 50914, 400, 321, 1466, 341, 5240, 538, 14816, 13, 51014, 51014, 407, 577, 307, 309, 300, 321, 393, 1466, 577, 264, 1002, 1985, 51164, 51164, 293, 439, 264, 10392, 466, 264, 1002, 538, 14816, 30, 51364, 51364, 400, 456, 366, 732, 4112, 337, 341, 13, 51514, 51514, 407, 472, 286, 1217, 8825, 307, 264, 1558, 295, 2698, 12, 48172, 24420, 2539, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07787118041724489, "compression_ratio": 1.626865671641791, "no_speech_prob": 9.376493835588917e-05}, {"id": 444, "seek": 256506, "start": 2568.06, "end": 2572.06, "text": " We've learned gravity when we were babies, probably around the age of nine months or so.", "tokens": [50364, 562, 364, 2657, 307, 406, 8104, 11, 309, 8804, 13, 50514, 50514, 492, 600, 3264, 12110, 562, 321, 645, 10917, 11, 1391, 926, 264, 3205, 295, 4949, 2493, 420, 370, 13, 50714, 50714, 17708, 420, 4949, 2493, 13, 663, 311, 562, 309, 16795, 493, 294, 10917, 13, 50914, 50914, 400, 321, 1466, 341, 5240, 538, 14816, 13, 51014, 51014, 407, 577, 307, 309, 300, 321, 393, 1466, 577, 264, 1002, 1985, 51164, 51164, 293, 439, 264, 10392, 466, 264, 1002, 538, 14816, 30, 51364, 51364, 400, 456, 366, 732, 4112, 337, 341, 13, 51514, 51514, 407, 472, 286, 1217, 8825, 307, 264, 1558, 295, 2698, 12, 48172, 24420, 2539, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07787118041724489, "compression_ratio": 1.626865671641791, "no_speech_prob": 9.376493835588917e-05}, {"id": 445, "seek": 256506, "start": 2572.06, "end": 2576.06, "text": " Eight or nine months. That's when it pops up in babies.", "tokens": [50364, 562, 364, 2657, 307, 406, 8104, 11, 309, 8804, 13, 50514, 50514, 492, 600, 3264, 12110, 562, 321, 645, 10917, 11, 1391, 926, 264, 3205, 295, 4949, 2493, 420, 370, 13, 50714, 50714, 17708, 420, 4949, 2493, 13, 663, 311, 562, 309, 16795, 493, 294, 10917, 13, 50914, 50914, 400, 321, 1466, 341, 5240, 538, 14816, 13, 51014, 51014, 407, 577, 307, 309, 300, 321, 393, 1466, 577, 264, 1002, 1985, 51164, 51164, 293, 439, 264, 10392, 466, 264, 1002, 538, 14816, 30, 51364, 51364, 400, 456, 366, 732, 4112, 337, 341, 13, 51514, 51514, 407, 472, 286, 1217, 8825, 307, 264, 1558, 295, 2698, 12, 48172, 24420, 2539, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07787118041724489, "compression_ratio": 1.626865671641791, "no_speech_prob": 9.376493835588917e-05}, {"id": 446, "seek": 256506, "start": 2576.06, "end": 2578.06, "text": " And we learn this mostly by observation.", "tokens": [50364, 562, 364, 2657, 307, 406, 8104, 11, 309, 8804, 13, 50514, 50514, 492, 600, 3264, 12110, 562, 321, 645, 10917, 11, 1391, 926, 264, 3205, 295, 4949, 2493, 420, 370, 13, 50714, 50714, 17708, 420, 4949, 2493, 13, 663, 311, 562, 309, 16795, 493, 294, 10917, 13, 50914, 50914, 400, 321, 1466, 341, 5240, 538, 14816, 13, 51014, 51014, 407, 577, 307, 309, 300, 321, 393, 1466, 577, 264, 1002, 1985, 51164, 51164, 293, 439, 264, 10392, 466, 264, 1002, 538, 14816, 30, 51364, 51364, 400, 456, 366, 732, 4112, 337, 341, 13, 51514, 51514, 407, 472, 286, 1217, 8825, 307, 264, 1558, 295, 2698, 12, 48172, 24420, 2539, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07787118041724489, "compression_ratio": 1.626865671641791, "no_speech_prob": 9.376493835588917e-05}, {"id": 447, "seek": 256506, "start": 2578.06, "end": 2581.06, "text": " So how is it that we can learn how the world works", "tokens": [50364, 562, 364, 2657, 307, 406, 8104, 11, 309, 8804, 13, 50514, 50514, 492, 600, 3264, 12110, 562, 321, 645, 10917, 11, 1391, 926, 264, 3205, 295, 4949, 2493, 420, 370, 13, 50714, 50714, 17708, 420, 4949, 2493, 13, 663, 311, 562, 309, 16795, 493, 294, 10917, 13, 50914, 50914, 400, 321, 1466, 341, 5240, 538, 14816, 13, 51014, 51014, 407, 577, 307, 309, 300, 321, 393, 1466, 577, 264, 1002, 1985, 51164, 51164, 293, 439, 264, 10392, 466, 264, 1002, 538, 14816, 30, 51364, 51364, 400, 456, 366, 732, 4112, 337, 341, 13, 51514, 51514, 407, 472, 286, 1217, 8825, 307, 264, 1558, 295, 2698, 12, 48172, 24420, 2539, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07787118041724489, "compression_ratio": 1.626865671641791, "no_speech_prob": 9.376493835588917e-05}, {"id": 448, "seek": 256506, "start": 2581.06, "end": 2585.06, "text": " and all the concepts about the world by observation?", "tokens": [50364, 562, 364, 2657, 307, 406, 8104, 11, 309, 8804, 13, 50514, 50514, 492, 600, 3264, 12110, 562, 321, 645, 10917, 11, 1391, 926, 264, 3205, 295, 4949, 2493, 420, 370, 13, 50714, 50714, 17708, 420, 4949, 2493, 13, 663, 311, 562, 309, 16795, 493, 294, 10917, 13, 50914, 50914, 400, 321, 1466, 341, 5240, 538, 14816, 13, 51014, 51014, 407, 577, 307, 309, 300, 321, 393, 1466, 577, 264, 1002, 1985, 51164, 51164, 293, 439, 264, 10392, 466, 264, 1002, 538, 14816, 30, 51364, 51364, 400, 456, 366, 732, 4112, 337, 341, 13, 51514, 51514, 407, 472, 286, 1217, 8825, 307, 264, 1558, 295, 2698, 12, 48172, 24420, 2539, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07787118041724489, "compression_ratio": 1.626865671641791, "no_speech_prob": 9.376493835588917e-05}, {"id": 449, "seek": 256506, "start": 2585.06, "end": 2588.06, "text": " And there are two reasons for this.", "tokens": [50364, 562, 364, 2657, 307, 406, 8104, 11, 309, 8804, 13, 50514, 50514, 492, 600, 3264, 12110, 562, 321, 645, 10917, 11, 1391, 926, 264, 3205, 295, 4949, 2493, 420, 370, 13, 50714, 50714, 17708, 420, 4949, 2493, 13, 663, 311, 562, 309, 16795, 493, 294, 10917, 13, 50914, 50914, 400, 321, 1466, 341, 5240, 538, 14816, 13, 51014, 51014, 407, 577, 307, 309, 300, 321, 393, 1466, 577, 264, 1002, 1985, 51164, 51164, 293, 439, 264, 10392, 466, 264, 1002, 538, 14816, 30, 51364, 51364, 400, 456, 366, 732, 4112, 337, 341, 13, 51514, 51514, 407, 472, 286, 1217, 8825, 307, 264, 1558, 295, 2698, 12, 48172, 24420, 2539, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07787118041724489, "compression_ratio": 1.626865671641791, "no_speech_prob": 9.376493835588917e-05}, {"id": 450, "seek": 256506, "start": 2588.06, "end": 2591.06, "text": " So one I already explained is the idea of self-supervised learning.", "tokens": [50364, 562, 364, 2657, 307, 406, 8104, 11, 309, 8804, 13, 50514, 50514, 492, 600, 3264, 12110, 562, 321, 645, 10917, 11, 1391, 926, 264, 3205, 295, 4949, 2493, 420, 370, 13, 50714, 50714, 17708, 420, 4949, 2493, 13, 663, 311, 562, 309, 16795, 493, 294, 10917, 13, 50914, 50914, 400, 321, 1466, 341, 5240, 538, 14816, 13, 51014, 51014, 407, 577, 307, 309, 300, 321, 393, 1466, 577, 264, 1002, 1985, 51164, 51164, 293, 439, 264, 10392, 466, 264, 1002, 538, 14816, 30, 51364, 51364, 400, 456, 366, 732, 4112, 337, 341, 13, 51514, 51514, 407, 472, 286, 1217, 8825, 307, 264, 1558, 295, 2698, 12, 48172, 24420, 2539, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07787118041724489, "compression_ratio": 1.626865671641791, "no_speech_prob": 9.376493835588917e-05}, {"id": 451, "seek": 259106, "start": 2591.06, "end": 2595.06, "text": " So you can train yourself to predict, maybe you will spontaneously", "tokens": [50364, 407, 291, 393, 3847, 1803, 281, 6069, 11, 1310, 291, 486, 47632, 50564, 50564, 733, 295, 1466, 12649, 10392, 466, 264, 1002, 13, 50714, 50714, 663, 1062, 312, 4420, 294, 13081, 337, 2539, 257, 1729, 5633, 420, 992, 295, 9608, 13, 51064, 51064, 583, 456, 311, 1071, 1778, 11, 597, 307, 300, 291, 767, 528, 281, 1322, 5245, 295, 264, 1002, 51264, 51264, 498, 291, 528, 281, 312, 1075, 281, 605, 322, 264, 1002, 13, 51414, 51414, 407, 286, 478, 5061, 341, 3435, 293, 286, 458, 300, 498, 286, 1286, 452, 1011, 493, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11972759679420707, "compression_ratio": 1.610655737704918, "no_speech_prob": 2.2815740521764383e-05}, {"id": 452, "seek": 259106, "start": 2595.06, "end": 2598.06, "text": " kind of learn abstract concepts about the world.", "tokens": [50364, 407, 291, 393, 3847, 1803, 281, 6069, 11, 1310, 291, 486, 47632, 50564, 50564, 733, 295, 1466, 12649, 10392, 466, 264, 1002, 13, 50714, 50714, 663, 1062, 312, 4420, 294, 13081, 337, 2539, 257, 1729, 5633, 420, 992, 295, 9608, 13, 51064, 51064, 583, 456, 311, 1071, 1778, 11, 597, 307, 300, 291, 767, 528, 281, 1322, 5245, 295, 264, 1002, 51264, 51264, 498, 291, 528, 281, 312, 1075, 281, 605, 322, 264, 1002, 13, 51414, 51414, 407, 286, 478, 5061, 341, 3435, 293, 286, 458, 300, 498, 286, 1286, 452, 1011, 493, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11972759679420707, "compression_ratio": 1.610655737704918, "no_speech_prob": 2.2815740521764383e-05}, {"id": 453, "seek": 259106, "start": 2598.06, "end": 2605.06, "text": " That might be useful in preparation for learning a particular task or set of tasks.", "tokens": [50364, 407, 291, 393, 3847, 1803, 281, 6069, 11, 1310, 291, 486, 47632, 50564, 50564, 733, 295, 1466, 12649, 10392, 466, 264, 1002, 13, 50714, 50714, 663, 1062, 312, 4420, 294, 13081, 337, 2539, 257, 1729, 5633, 420, 992, 295, 9608, 13, 51064, 51064, 583, 456, 311, 1071, 1778, 11, 597, 307, 300, 291, 767, 528, 281, 1322, 5245, 295, 264, 1002, 51264, 51264, 498, 291, 528, 281, 312, 1075, 281, 605, 322, 264, 1002, 13, 51414, 51414, 407, 286, 478, 5061, 341, 3435, 293, 286, 458, 300, 498, 286, 1286, 452, 1011, 493, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11972759679420707, "compression_ratio": 1.610655737704918, "no_speech_prob": 2.2815740521764383e-05}, {"id": 454, "seek": 259106, "start": 2605.06, "end": 2609.06, "text": " But there's another reason, which is that you actually want to build models of the world", "tokens": [50364, 407, 291, 393, 3847, 1803, 281, 6069, 11, 1310, 291, 486, 47632, 50564, 50564, 733, 295, 1466, 12649, 10392, 466, 264, 1002, 13, 50714, 50714, 663, 1062, 312, 4420, 294, 13081, 337, 2539, 257, 1729, 5633, 420, 992, 295, 9608, 13, 51064, 51064, 583, 456, 311, 1071, 1778, 11, 597, 307, 300, 291, 767, 528, 281, 1322, 5245, 295, 264, 1002, 51264, 51264, 498, 291, 528, 281, 312, 1075, 281, 605, 322, 264, 1002, 13, 51414, 51414, 407, 286, 478, 5061, 341, 3435, 293, 286, 458, 300, 498, 286, 1286, 452, 1011, 493, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11972759679420707, "compression_ratio": 1.610655737704918, "no_speech_prob": 2.2815740521764383e-05}, {"id": 455, "seek": 259106, "start": 2609.06, "end": 2612.06, "text": " if you want to be able to act on the world.", "tokens": [50364, 407, 291, 393, 3847, 1803, 281, 6069, 11, 1310, 291, 486, 47632, 50564, 50564, 733, 295, 1466, 12649, 10392, 466, 264, 1002, 13, 50714, 50714, 663, 1062, 312, 4420, 294, 13081, 337, 2539, 257, 1729, 5633, 420, 992, 295, 9608, 13, 51064, 51064, 583, 456, 311, 1071, 1778, 11, 597, 307, 300, 291, 767, 528, 281, 1322, 5245, 295, 264, 1002, 51264, 51264, 498, 291, 528, 281, 312, 1075, 281, 605, 322, 264, 1002, 13, 51414, 51414, 407, 286, 478, 5061, 341, 3435, 293, 286, 458, 300, 498, 286, 1286, 452, 1011, 493, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11972759679420707, "compression_ratio": 1.610655737704918, "no_speech_prob": 2.2815740521764383e-05}, {"id": 456, "seek": 259106, "start": 2612.06, "end": 2618.06, "text": " So I'm holding this pen and I know that if I move my hand up", "tokens": [50364, 407, 291, 393, 3847, 1803, 281, 6069, 11, 1310, 291, 486, 47632, 50564, 50564, 733, 295, 1466, 12649, 10392, 466, 264, 1002, 13, 50714, 50714, 663, 1062, 312, 4420, 294, 13081, 337, 2539, 257, 1729, 5633, 420, 992, 295, 9608, 13, 51064, 51064, 583, 456, 311, 1071, 1778, 11, 597, 307, 300, 291, 767, 528, 281, 1322, 5245, 295, 264, 1002, 51264, 51264, 498, 291, 528, 281, 312, 1075, 281, 605, 322, 264, 1002, 13, 51414, 51414, 407, 286, 478, 5061, 341, 3435, 293, 286, 458, 300, 498, 286, 1286, 452, 1011, 493, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11972759679420707, "compression_ratio": 1.610655737704918, "no_speech_prob": 2.2815740521764383e-05}, {"id": 457, "seek": 261806, "start": 2618.06, "end": 2623.06, "text": " the pen will move with it because it's between my fingers.", "tokens": [50364, 264, 3435, 486, 1286, 365, 309, 570, 309, 311, 1296, 452, 7350, 13, 50614, 50614, 286, 458, 300, 498, 286, 1269, 452, 7350, 11, 264, 3435, 486, 2100, 13, 50764, 50764, 286, 458, 538, 12110, 13, 286, 458, 538, 29444, 3381, 13, 50864, 50864, 286, 600, 3264, 439, 300, 1507, 13, 400, 286, 600, 3264, 5240, 538, 14816, 13, 51014, 51014, 286, 600, 3264, 611, 538, 37142, 13, 51114, 51114, 583, 257, 688, 295, 437, 286, 600, 3264, 11, 286, 600, 3264, 445, 538, 14816, 13, 51214, 51214, 407, 264, 955, 1168, 307, 393, 321, 764, 437, 321, 600, 3264, 466, 2698, 12, 48172, 24420, 2539, 51514, 51514, 281, 3847, 257, 1185, 281, 1466, 1002, 5245, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09248725638901892, "compression_ratio": 1.9145299145299146, "no_speech_prob": 1.9512459402903914e-05}, {"id": 458, "seek": 261806, "start": 2623.06, "end": 2626.06, "text": " I know that if I open my fingers, the pen will fall.", "tokens": [50364, 264, 3435, 486, 1286, 365, 309, 570, 309, 311, 1296, 452, 7350, 13, 50614, 50614, 286, 458, 300, 498, 286, 1269, 452, 7350, 11, 264, 3435, 486, 2100, 13, 50764, 50764, 286, 458, 538, 12110, 13, 286, 458, 538, 29444, 3381, 13, 50864, 50864, 286, 600, 3264, 439, 300, 1507, 13, 400, 286, 600, 3264, 5240, 538, 14816, 13, 51014, 51014, 286, 600, 3264, 611, 538, 37142, 13, 51114, 51114, 583, 257, 688, 295, 437, 286, 600, 3264, 11, 286, 600, 3264, 445, 538, 14816, 13, 51214, 51214, 407, 264, 955, 1168, 307, 393, 321, 764, 437, 321, 600, 3264, 466, 2698, 12, 48172, 24420, 2539, 51514, 51514, 281, 3847, 257, 1185, 281, 1466, 1002, 5245, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09248725638901892, "compression_ratio": 1.9145299145299146, "no_speech_prob": 1.9512459402903914e-05}, {"id": 459, "seek": 261806, "start": 2626.06, "end": 2628.06, "text": " I know by gravity. I know by grasping.", "tokens": [50364, 264, 3435, 486, 1286, 365, 309, 570, 309, 311, 1296, 452, 7350, 13, 50614, 50614, 286, 458, 300, 498, 286, 1269, 452, 7350, 11, 264, 3435, 486, 2100, 13, 50764, 50764, 286, 458, 538, 12110, 13, 286, 458, 538, 29444, 3381, 13, 50864, 50864, 286, 600, 3264, 439, 300, 1507, 13, 400, 286, 600, 3264, 5240, 538, 14816, 13, 51014, 51014, 286, 600, 3264, 611, 538, 37142, 13, 51114, 51114, 583, 257, 688, 295, 437, 286, 600, 3264, 11, 286, 600, 3264, 445, 538, 14816, 13, 51214, 51214, 407, 264, 955, 1168, 307, 393, 321, 764, 437, 321, 600, 3264, 466, 2698, 12, 48172, 24420, 2539, 51514, 51514, 281, 3847, 257, 1185, 281, 1466, 1002, 5245, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09248725638901892, "compression_ratio": 1.9145299145299146, "no_speech_prob": 1.9512459402903914e-05}, {"id": 460, "seek": 261806, "start": 2628.06, "end": 2631.06, "text": " I've learned all that stuff. And I've learned mostly by observation.", "tokens": [50364, 264, 3435, 486, 1286, 365, 309, 570, 309, 311, 1296, 452, 7350, 13, 50614, 50614, 286, 458, 300, 498, 286, 1269, 452, 7350, 11, 264, 3435, 486, 2100, 13, 50764, 50764, 286, 458, 538, 12110, 13, 286, 458, 538, 29444, 3381, 13, 50864, 50864, 286, 600, 3264, 439, 300, 1507, 13, 400, 286, 600, 3264, 5240, 538, 14816, 13, 51014, 51014, 286, 600, 3264, 611, 538, 37142, 13, 51114, 51114, 583, 257, 688, 295, 437, 286, 600, 3264, 11, 286, 600, 3264, 445, 538, 14816, 13, 51214, 51214, 407, 264, 955, 1168, 307, 393, 321, 764, 437, 321, 600, 3264, 466, 2698, 12, 48172, 24420, 2539, 51514, 51514, 281, 3847, 257, 1185, 281, 1466, 1002, 5245, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09248725638901892, "compression_ratio": 1.9145299145299146, "no_speech_prob": 1.9512459402903914e-05}, {"id": 461, "seek": 261806, "start": 2631.06, "end": 2633.06, "text": " I've learned also by experimentation.", "tokens": [50364, 264, 3435, 486, 1286, 365, 309, 570, 309, 311, 1296, 452, 7350, 13, 50614, 50614, 286, 458, 300, 498, 286, 1269, 452, 7350, 11, 264, 3435, 486, 2100, 13, 50764, 50764, 286, 458, 538, 12110, 13, 286, 458, 538, 29444, 3381, 13, 50864, 50864, 286, 600, 3264, 439, 300, 1507, 13, 400, 286, 600, 3264, 5240, 538, 14816, 13, 51014, 51014, 286, 600, 3264, 611, 538, 37142, 13, 51114, 51114, 583, 257, 688, 295, 437, 286, 600, 3264, 11, 286, 600, 3264, 445, 538, 14816, 13, 51214, 51214, 407, 264, 955, 1168, 307, 393, 321, 764, 437, 321, 600, 3264, 466, 2698, 12, 48172, 24420, 2539, 51514, 51514, 281, 3847, 257, 1185, 281, 1466, 1002, 5245, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09248725638901892, "compression_ratio": 1.9145299145299146, "no_speech_prob": 1.9512459402903914e-05}, {"id": 462, "seek": 261806, "start": 2633.06, "end": 2635.06, "text": " But a lot of what I've learned, I've learned just by observation.", "tokens": [50364, 264, 3435, 486, 1286, 365, 309, 570, 309, 311, 1296, 452, 7350, 13, 50614, 50614, 286, 458, 300, 498, 286, 1269, 452, 7350, 11, 264, 3435, 486, 2100, 13, 50764, 50764, 286, 458, 538, 12110, 13, 286, 458, 538, 29444, 3381, 13, 50864, 50864, 286, 600, 3264, 439, 300, 1507, 13, 400, 286, 600, 3264, 5240, 538, 14816, 13, 51014, 51014, 286, 600, 3264, 611, 538, 37142, 13, 51114, 51114, 583, 257, 688, 295, 437, 286, 600, 3264, 11, 286, 600, 3264, 445, 538, 14816, 13, 51214, 51214, 407, 264, 955, 1168, 307, 393, 321, 764, 437, 321, 600, 3264, 466, 2698, 12, 48172, 24420, 2539, 51514, 51514, 281, 3847, 257, 1185, 281, 1466, 1002, 5245, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09248725638901892, "compression_ratio": 1.9145299145299146, "no_speech_prob": 1.9512459402903914e-05}, {"id": 463, "seek": 261806, "start": 2635.06, "end": 2641.06, "text": " So the big question is can we use what we've learned about self-supervised learning", "tokens": [50364, 264, 3435, 486, 1286, 365, 309, 570, 309, 311, 1296, 452, 7350, 13, 50614, 50614, 286, 458, 300, 498, 286, 1269, 452, 7350, 11, 264, 3435, 486, 2100, 13, 50764, 50764, 286, 458, 538, 12110, 13, 286, 458, 538, 29444, 3381, 13, 50864, 50864, 286, 600, 3264, 439, 300, 1507, 13, 400, 286, 600, 3264, 5240, 538, 14816, 13, 51014, 51014, 286, 600, 3264, 611, 538, 37142, 13, 51114, 51114, 583, 257, 688, 295, 437, 286, 600, 3264, 11, 286, 600, 3264, 445, 538, 14816, 13, 51214, 51214, 407, 264, 955, 1168, 307, 393, 321, 764, 437, 321, 600, 3264, 466, 2698, 12, 48172, 24420, 2539, 51514, 51514, 281, 3847, 257, 1185, 281, 1466, 1002, 5245, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09248725638901892, "compression_ratio": 1.9145299145299146, "no_speech_prob": 1.9512459402903914e-05}, {"id": 464, "seek": 261806, "start": 2641.06, "end": 2647.06, "text": " to train a system to learn world models?", "tokens": [50364, 264, 3435, 486, 1286, 365, 309, 570, 309, 311, 1296, 452, 7350, 13, 50614, 50614, 286, 458, 300, 498, 286, 1269, 452, 7350, 11, 264, 3435, 486, 2100, 13, 50764, 50764, 286, 458, 538, 12110, 13, 286, 458, 538, 29444, 3381, 13, 50864, 50864, 286, 600, 3264, 439, 300, 1507, 13, 400, 286, 600, 3264, 5240, 538, 14816, 13, 51014, 51014, 286, 600, 3264, 611, 538, 37142, 13, 51114, 51114, 583, 257, 688, 295, 437, 286, 600, 3264, 11, 286, 600, 3264, 445, 538, 14816, 13, 51214, 51214, 407, 264, 955, 1168, 307, 393, 321, 764, 437, 321, 600, 3264, 466, 2698, 12, 48172, 24420, 2539, 51514, 51514, 281, 3847, 257, 1185, 281, 1466, 1002, 5245, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09248725638901892, "compression_ratio": 1.9145299145299146, "no_speech_prob": 1.9512459402903914e-05}, {"id": 465, "seek": 264706, "start": 2647.06, "end": 2649.06, "text": " What is a world model?", "tokens": [50364, 708, 307, 257, 1002, 2316, 30, 50464, 50464, 407, 498, 291, 528, 281, 1333, 295, 976, 364, 1558, 295, 264, 9482, 295, 364, 23797, 13232, 1185, 50914, 50914, 309, 576, 312, 257, 1185, 300, 307, 18204, 295, 4476, 1451, 2563, 8474, 510, 51114, 51114, 300, 366, 10379, 322, 264, 1411, 13, 51214, 51214, 407, 309, 311, 364, 13232, 9461, 420, 1310, 406, 370, 13232, 11, 321, 603, 536, 13, 51414, 51414, 467, 575, 257, 12860, 10088, 293, 264, 12860, 10088, 1936, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09321617799646714, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.00018462831212673336}, {"id": 466, "seek": 264706, "start": 2649.06, "end": 2658.06, "text": " So if you want to sort of give an idea of the architecture of an autonomous intelligent system", "tokens": [50364, 708, 307, 257, 1002, 2316, 30, 50464, 50464, 407, 498, 291, 528, 281, 1333, 295, 976, 364, 1558, 295, 264, 9482, 295, 364, 23797, 13232, 1185, 50914, 50914, 309, 576, 312, 257, 1185, 300, 307, 18204, 295, 4476, 1451, 2563, 8474, 510, 51114, 51114, 300, 366, 10379, 322, 264, 1411, 13, 51214, 51214, 407, 309, 311, 364, 13232, 9461, 420, 1310, 406, 370, 13232, 11, 321, 603, 536, 13, 51414, 51414, 467, 575, 257, 12860, 10088, 293, 264, 12860, 10088, 1936, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09321617799646714, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.00018462831212673336}, {"id": 467, "seek": 264706, "start": 2658.06, "end": 2662.06, "text": " it would be a system that is composed of essentially four major blocks here", "tokens": [50364, 708, 307, 257, 1002, 2316, 30, 50464, 50464, 407, 498, 291, 528, 281, 1333, 295, 976, 364, 1558, 295, 264, 9482, 295, 364, 23797, 13232, 1185, 50914, 50914, 309, 576, 312, 257, 1185, 300, 307, 18204, 295, 4476, 1451, 2563, 8474, 510, 51114, 51114, 300, 366, 10379, 322, 264, 1411, 13, 51214, 51214, 407, 309, 311, 364, 13232, 9461, 420, 1310, 406, 370, 13232, 11, 321, 603, 536, 13, 51414, 51414, 467, 575, 257, 12860, 10088, 293, 264, 12860, 10088, 1936, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09321617799646714, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.00018462831212673336}, {"id": 468, "seek": 264706, "start": 2662.06, "end": 2664.06, "text": " that are represented on the left.", "tokens": [50364, 708, 307, 257, 1002, 2316, 30, 50464, 50464, 407, 498, 291, 528, 281, 1333, 295, 976, 364, 1558, 295, 264, 9482, 295, 364, 23797, 13232, 1185, 50914, 50914, 309, 576, 312, 257, 1185, 300, 307, 18204, 295, 4476, 1451, 2563, 8474, 510, 51114, 51114, 300, 366, 10379, 322, 264, 1411, 13, 51214, 51214, 407, 309, 311, 364, 13232, 9461, 420, 1310, 406, 370, 13232, 11, 321, 603, 536, 13, 51414, 51414, 467, 575, 257, 12860, 10088, 293, 264, 12860, 10088, 1936, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09321617799646714, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.00018462831212673336}, {"id": 469, "seek": 264706, "start": 2664.06, "end": 2668.06, "text": " So it's an intelligent agent or maybe not so intelligent, we'll see.", "tokens": [50364, 708, 307, 257, 1002, 2316, 30, 50464, 50464, 407, 498, 291, 528, 281, 1333, 295, 976, 364, 1558, 295, 264, 9482, 295, 364, 23797, 13232, 1185, 50914, 50914, 309, 576, 312, 257, 1185, 300, 307, 18204, 295, 4476, 1451, 2563, 8474, 510, 51114, 51114, 300, 366, 10379, 322, 264, 1411, 13, 51214, 51214, 407, 309, 311, 364, 13232, 9461, 420, 1310, 406, 370, 13232, 11, 321, 603, 536, 13, 51414, 51414, 467, 575, 257, 12860, 10088, 293, 264, 12860, 10088, 1936, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09321617799646714, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.00018462831212673336}, {"id": 470, "seek": 264706, "start": 2668.06, "end": 2671.06, "text": " It has a perception module and the perception module basically", "tokens": [50364, 708, 307, 257, 1002, 2316, 30, 50464, 50464, 407, 498, 291, 528, 281, 1333, 295, 976, 364, 1558, 295, 264, 9482, 295, 364, 23797, 13232, 1185, 50914, 50914, 309, 576, 312, 257, 1185, 300, 307, 18204, 295, 4476, 1451, 2563, 8474, 510, 51114, 51114, 300, 366, 10379, 322, 264, 1411, 13, 51214, 51214, 407, 309, 311, 364, 13232, 9461, 420, 1310, 406, 370, 13232, 11, 321, 603, 536, 13, 51414, 51414, 467, 575, 257, 12860, 10088, 293, 264, 12860, 10088, 1936, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09321617799646714, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.00018462831212673336}, {"id": 471, "seek": 267106, "start": 2671.06, "end": 2677.06, "text": " observes the world and then computes a representation of the state of the world.", "tokens": [50364, 3181, 9054, 264, 1002, 293, 550, 715, 1819, 257, 10290, 295, 264, 1785, 295, 264, 1002, 13, 50664, 50664, 45001, 4904, 13, 1711, 565, 256, 11, 318, 295, 256, 307, 264, 1558, 300, 264, 1185, 575, 295, 264, 1785, 295, 264, 1002, 13, 51114, 51114, 639, 307, 4725, 364, 31709, 10290, 295, 264, 1002, 51264, 51264, 570, 321, 393, 380, 11441, 264, 2302, 6445, 412, 1564, 13, 51414, 51414, 492, 787, 11441, 437, 311, 4258, 926, 505, 13, 51514, 51514, 400, 754, 300, 321, 393, 380, 536, 807, 2678, 3063, 626, 293, 456, 307, 257, 688, 295, 6920, 4368, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10828231144877314, "compression_ratio": 1.8157894736842106, "no_speech_prob": 7.36263464204967e-05}, {"id": 472, "seek": 267106, "start": 2677.06, "end": 2686.06, "text": " Called ST. At time t, S of t is the idea that the system has of the state of the world.", "tokens": [50364, 3181, 9054, 264, 1002, 293, 550, 715, 1819, 257, 10290, 295, 264, 1785, 295, 264, 1002, 13, 50664, 50664, 45001, 4904, 13, 1711, 565, 256, 11, 318, 295, 256, 307, 264, 1558, 300, 264, 1185, 575, 295, 264, 1785, 295, 264, 1002, 13, 51114, 51114, 639, 307, 4725, 364, 31709, 10290, 295, 264, 1002, 51264, 51264, 570, 321, 393, 380, 11441, 264, 2302, 6445, 412, 1564, 13, 51414, 51414, 492, 787, 11441, 437, 311, 4258, 926, 505, 13, 51514, 51514, 400, 754, 300, 321, 393, 380, 536, 807, 2678, 3063, 626, 293, 456, 307, 257, 688, 295, 6920, 4368, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10828231144877314, "compression_ratio": 1.8157894736842106, "no_speech_prob": 7.36263464204967e-05}, {"id": 473, "seek": 267106, "start": 2686.06, "end": 2689.06, "text": " This is necessarily an incomplete representation of the world", "tokens": [50364, 3181, 9054, 264, 1002, 293, 550, 715, 1819, 257, 10290, 295, 264, 1785, 295, 264, 1002, 13, 50664, 50664, 45001, 4904, 13, 1711, 565, 256, 11, 318, 295, 256, 307, 264, 1558, 300, 264, 1185, 575, 295, 264, 1785, 295, 264, 1002, 13, 51114, 51114, 639, 307, 4725, 364, 31709, 10290, 295, 264, 1002, 51264, 51264, 570, 321, 393, 380, 11441, 264, 2302, 6445, 412, 1564, 13, 51414, 51414, 492, 787, 11441, 437, 311, 4258, 926, 505, 13, 51514, 51514, 400, 754, 300, 321, 393, 380, 536, 807, 2678, 3063, 626, 293, 456, 307, 257, 688, 295, 6920, 4368, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10828231144877314, "compression_ratio": 1.8157894736842106, "no_speech_prob": 7.36263464204967e-05}, {"id": 474, "seek": 267106, "start": 2689.06, "end": 2692.06, "text": " because we can't observe the entire universe at once.", "tokens": [50364, 3181, 9054, 264, 1002, 293, 550, 715, 1819, 257, 10290, 295, 264, 1785, 295, 264, 1002, 13, 50664, 50664, 45001, 4904, 13, 1711, 565, 256, 11, 318, 295, 256, 307, 264, 1558, 300, 264, 1185, 575, 295, 264, 1785, 295, 264, 1002, 13, 51114, 51114, 639, 307, 4725, 364, 31709, 10290, 295, 264, 1002, 51264, 51264, 570, 321, 393, 380, 11441, 264, 2302, 6445, 412, 1564, 13, 51414, 51414, 492, 787, 11441, 437, 311, 4258, 926, 505, 13, 51514, 51514, 400, 754, 300, 321, 393, 380, 536, 807, 2678, 3063, 626, 293, 456, 307, 257, 688, 295, 6920, 4368, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10828231144877314, "compression_ratio": 1.8157894736842106, "no_speech_prob": 7.36263464204967e-05}, {"id": 475, "seek": 267106, "start": 2692.06, "end": 2694.06, "text": " We only observe what's immediately around us.", "tokens": [50364, 3181, 9054, 264, 1002, 293, 550, 715, 1819, 257, 10290, 295, 264, 1785, 295, 264, 1002, 13, 50664, 50664, 45001, 4904, 13, 1711, 565, 256, 11, 318, 295, 256, 307, 264, 1558, 300, 264, 1185, 575, 295, 264, 1785, 295, 264, 1002, 13, 51114, 51114, 639, 307, 4725, 364, 31709, 10290, 295, 264, 1002, 51264, 51264, 570, 321, 393, 380, 11441, 264, 2302, 6445, 412, 1564, 13, 51414, 51414, 492, 787, 11441, 437, 311, 4258, 926, 505, 13, 51514, 51514, 400, 754, 300, 321, 393, 380, 536, 807, 2678, 3063, 626, 293, 456, 307, 257, 688, 295, 6920, 4368, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10828231144877314, "compression_ratio": 1.8157894736842106, "no_speech_prob": 7.36263464204967e-05}, {"id": 476, "seek": 267106, "start": 2694.06, "end": 2699.06, "text": " And even that we can't see through occlusions and there is a lot of internal states", "tokens": [50364, 3181, 9054, 264, 1002, 293, 550, 715, 1819, 257, 10290, 295, 264, 1785, 295, 264, 1002, 13, 50664, 50664, 45001, 4904, 13, 1711, 565, 256, 11, 318, 295, 256, 307, 264, 1558, 300, 264, 1185, 575, 295, 264, 1785, 295, 264, 1002, 13, 51114, 51114, 639, 307, 4725, 364, 31709, 10290, 295, 264, 1002, 51264, 51264, 570, 321, 393, 380, 11441, 264, 2302, 6445, 412, 1564, 13, 51414, 51414, 492, 787, 11441, 437, 311, 4258, 926, 505, 13, 51514, 51514, 400, 754, 300, 321, 393, 380, 536, 807, 2678, 3063, 626, 293, 456, 307, 257, 688, 295, 6920, 4368, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10828231144877314, "compression_ratio": 1.8157894736842106, "no_speech_prob": 7.36263464204967e-05}, {"id": 477, "seek": 269906, "start": 2699.06, "end": 2702.06, "text": " of the world that we can't observe well enough.", "tokens": [50364, 295, 264, 1002, 300, 321, 393, 380, 11441, 731, 1547, 13, 50514, 50514, 2754, 498, 291, 393, 11441, 11, 428, 14170, 295, 14816, 815, 406, 312, 665, 1547, 13, 50714, 50714, 407, 498, 286, 829, 341, 3435, 294, 452, 1011, 293, 309, 7038, 281, 312, 9429, 293, 286, 718, 309, 352, 11, 50914, 50914, 309, 311, 516, 281, 2100, 457, 291, 393, 380, 534, 6069, 294, 437, 3513, 13, 51064, 51064, 286, 600, 1143, 300, 1365, 949, 281, 6786, 264, 1154, 295, 6775, 1639, 299, 15697, 11, 51464, 51464, 597, 307, 264, 1002, 307, 2107, 12, 49136, 259, 3142, 293, 291, 393, 380, 6069, 2293, 437, 311, 516, 281, 1051, 51664, 51664, 570, 291, 500, 380, 362, 257, 2176, 3760, 295, 264, 1785, 295, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07819672213255904, "compression_ratio": 1.7534246575342465, "no_speech_prob": 7.834074494894594e-05}, {"id": 478, "seek": 269906, "start": 2702.06, "end": 2706.06, "text": " Even if you can observe, your accuracy of observation may not be good enough.", "tokens": [50364, 295, 264, 1002, 300, 321, 393, 380, 11441, 731, 1547, 13, 50514, 50514, 2754, 498, 291, 393, 11441, 11, 428, 14170, 295, 14816, 815, 406, 312, 665, 1547, 13, 50714, 50714, 407, 498, 286, 829, 341, 3435, 294, 452, 1011, 293, 309, 7038, 281, 312, 9429, 293, 286, 718, 309, 352, 11, 50914, 50914, 309, 311, 516, 281, 2100, 457, 291, 393, 380, 534, 6069, 294, 437, 3513, 13, 51064, 51064, 286, 600, 1143, 300, 1365, 949, 281, 6786, 264, 1154, 295, 6775, 1639, 299, 15697, 11, 51464, 51464, 597, 307, 264, 1002, 307, 2107, 12, 49136, 259, 3142, 293, 291, 393, 380, 6069, 2293, 437, 311, 516, 281, 1051, 51664, 51664, 570, 291, 500, 380, 362, 257, 2176, 3760, 295, 264, 1785, 295, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07819672213255904, "compression_ratio": 1.7534246575342465, "no_speech_prob": 7.834074494894594e-05}, {"id": 479, "seek": 269906, "start": 2706.06, "end": 2710.06, "text": " So if I put this pen in my hand and it appears to be vertical and I let it go,", "tokens": [50364, 295, 264, 1002, 300, 321, 393, 380, 11441, 731, 1547, 13, 50514, 50514, 2754, 498, 291, 393, 11441, 11, 428, 14170, 295, 14816, 815, 406, 312, 665, 1547, 13, 50714, 50714, 407, 498, 286, 829, 341, 3435, 294, 452, 1011, 293, 309, 7038, 281, 312, 9429, 293, 286, 718, 309, 352, 11, 50914, 50914, 309, 311, 516, 281, 2100, 457, 291, 393, 380, 534, 6069, 294, 437, 3513, 13, 51064, 51064, 286, 600, 1143, 300, 1365, 949, 281, 6786, 264, 1154, 295, 6775, 1639, 299, 15697, 11, 51464, 51464, 597, 307, 264, 1002, 307, 2107, 12, 49136, 259, 3142, 293, 291, 393, 380, 6069, 2293, 437, 311, 516, 281, 1051, 51664, 51664, 570, 291, 500, 380, 362, 257, 2176, 3760, 295, 264, 1785, 295, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07819672213255904, "compression_ratio": 1.7534246575342465, "no_speech_prob": 7.834074494894594e-05}, {"id": 480, "seek": 269906, "start": 2710.06, "end": 2713.06, "text": " it's going to fall but you can't really predict in what direction.", "tokens": [50364, 295, 264, 1002, 300, 321, 393, 380, 11441, 731, 1547, 13, 50514, 50514, 2754, 498, 291, 393, 11441, 11, 428, 14170, 295, 14816, 815, 406, 312, 665, 1547, 13, 50714, 50714, 407, 498, 286, 829, 341, 3435, 294, 452, 1011, 293, 309, 7038, 281, 312, 9429, 293, 286, 718, 309, 352, 11, 50914, 50914, 309, 311, 516, 281, 2100, 457, 291, 393, 380, 534, 6069, 294, 437, 3513, 13, 51064, 51064, 286, 600, 1143, 300, 1365, 949, 281, 6786, 264, 1154, 295, 6775, 1639, 299, 15697, 11, 51464, 51464, 597, 307, 264, 1002, 307, 2107, 12, 49136, 259, 3142, 293, 291, 393, 380, 6069, 2293, 437, 311, 516, 281, 1051, 51664, 51664, 570, 291, 500, 380, 362, 257, 2176, 3760, 295, 264, 1785, 295, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07819672213255904, "compression_ratio": 1.7534246575342465, "no_speech_prob": 7.834074494894594e-05}, {"id": 481, "seek": 269906, "start": 2713.06, "end": 2721.06, "text": " I've used that example before to describe the problem of aleatoric uncertainty,", "tokens": [50364, 295, 264, 1002, 300, 321, 393, 380, 11441, 731, 1547, 13, 50514, 50514, 2754, 498, 291, 393, 11441, 11, 428, 14170, 295, 14816, 815, 406, 312, 665, 1547, 13, 50714, 50714, 407, 498, 286, 829, 341, 3435, 294, 452, 1011, 293, 309, 7038, 281, 312, 9429, 293, 286, 718, 309, 352, 11, 50914, 50914, 309, 311, 516, 281, 2100, 457, 291, 393, 380, 534, 6069, 294, 437, 3513, 13, 51064, 51064, 286, 600, 1143, 300, 1365, 949, 281, 6786, 264, 1154, 295, 6775, 1639, 299, 15697, 11, 51464, 51464, 597, 307, 264, 1002, 307, 2107, 12, 49136, 259, 3142, 293, 291, 393, 380, 6069, 2293, 437, 311, 516, 281, 1051, 51664, 51664, 570, 291, 500, 380, 362, 257, 2176, 3760, 295, 264, 1785, 295, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07819672213255904, "compression_ratio": 1.7534246575342465, "no_speech_prob": 7.834074494894594e-05}, {"id": 482, "seek": 269906, "start": 2721.06, "end": 2725.06, "text": " which is the world is non-deterministic and you can't predict exactly what's going to happen", "tokens": [50364, 295, 264, 1002, 300, 321, 393, 380, 11441, 731, 1547, 13, 50514, 50514, 2754, 498, 291, 393, 11441, 11, 428, 14170, 295, 14816, 815, 406, 312, 665, 1547, 13, 50714, 50714, 407, 498, 286, 829, 341, 3435, 294, 452, 1011, 293, 309, 7038, 281, 312, 9429, 293, 286, 718, 309, 352, 11, 50914, 50914, 309, 311, 516, 281, 2100, 457, 291, 393, 380, 534, 6069, 294, 437, 3513, 13, 51064, 51064, 286, 600, 1143, 300, 1365, 949, 281, 6786, 264, 1154, 295, 6775, 1639, 299, 15697, 11, 51464, 51464, 597, 307, 264, 1002, 307, 2107, 12, 49136, 259, 3142, 293, 291, 393, 380, 6069, 2293, 437, 311, 516, 281, 1051, 51664, 51664, 570, 291, 500, 380, 362, 257, 2176, 3760, 295, 264, 1785, 295, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07819672213255904, "compression_ratio": 1.7534246575342465, "no_speech_prob": 7.834074494894594e-05}, {"id": 483, "seek": 269906, "start": 2725.06, "end": 2728.06, "text": " because you don't have a perfect reading of the state of the world.", "tokens": [50364, 295, 264, 1002, 300, 321, 393, 380, 11441, 731, 1547, 13, 50514, 50514, 2754, 498, 291, 393, 11441, 11, 428, 14170, 295, 14816, 815, 406, 312, 665, 1547, 13, 50714, 50714, 407, 498, 286, 829, 341, 3435, 294, 452, 1011, 293, 309, 7038, 281, 312, 9429, 293, 286, 718, 309, 352, 11, 50914, 50914, 309, 311, 516, 281, 2100, 457, 291, 393, 380, 534, 6069, 294, 437, 3513, 13, 51064, 51064, 286, 600, 1143, 300, 1365, 949, 281, 6786, 264, 1154, 295, 6775, 1639, 299, 15697, 11, 51464, 51464, 597, 307, 264, 1002, 307, 2107, 12, 49136, 259, 3142, 293, 291, 393, 380, 6069, 2293, 437, 311, 516, 281, 1051, 51664, 51664, 570, 291, 500, 380, 362, 257, 2176, 3760, 295, 264, 1785, 295, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07819672213255904, "compression_ratio": 1.7534246575342465, "no_speech_prob": 7.834074494894594e-05}, {"id": 484, "seek": 272806, "start": 2728.06, "end": 2732.06, "text": " And maybe the world is intrinsically stochastic.", "tokens": [50364, 400, 1310, 264, 1002, 307, 28621, 984, 342, 8997, 2750, 13, 50564, 50564, 492, 500, 380, 458, 300, 767, 13, 50664, 50664, 1033, 11, 370, 257, 2128, 2316, 307, 257, 2316, 300, 2212, 264, 2190, 1785, 295, 264, 1002, 318, 295, 256, 50964, 50964, 420, 428, 1558, 295, 428, 2190, 1785, 295, 264, 1002, 293, 364, 3069, 300, 291, 434, 1940, 51214, 51214, 420, 300, 1580, 1646, 307, 1940, 11, 746, 300, 291, 393, 2826, 420, 412, 1935, 11441, 11, 51514, 51514, 293, 4317, 364, 43741, 48994, 7006, 1176, 295, 256, 11, 51664, 51664, 597, 8855, 437, 291, 500, 380, 458, 466, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09634801007192069, "compression_ratio": 1.7727272727272727, "no_speech_prob": 2.3907565264380537e-05}, {"id": 485, "seek": 272806, "start": 2732.06, "end": 2734.06, "text": " We don't know that actually.", "tokens": [50364, 400, 1310, 264, 1002, 307, 28621, 984, 342, 8997, 2750, 13, 50564, 50564, 492, 500, 380, 458, 300, 767, 13, 50664, 50664, 1033, 11, 370, 257, 2128, 2316, 307, 257, 2316, 300, 2212, 264, 2190, 1785, 295, 264, 1002, 318, 295, 256, 50964, 50964, 420, 428, 1558, 295, 428, 2190, 1785, 295, 264, 1002, 293, 364, 3069, 300, 291, 434, 1940, 51214, 51214, 420, 300, 1580, 1646, 307, 1940, 11, 746, 300, 291, 393, 2826, 420, 412, 1935, 11441, 11, 51514, 51514, 293, 4317, 364, 43741, 48994, 7006, 1176, 295, 256, 11, 51664, 51664, 597, 8855, 437, 291, 500, 380, 458, 466, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09634801007192069, "compression_ratio": 1.7727272727272727, "no_speech_prob": 2.3907565264380537e-05}, {"id": 486, "seek": 272806, "start": 2734.06, "end": 2740.06, "text": " Okay, so a forward model is a model that given the current state of the world S of t", "tokens": [50364, 400, 1310, 264, 1002, 307, 28621, 984, 342, 8997, 2750, 13, 50564, 50564, 492, 500, 380, 458, 300, 767, 13, 50664, 50664, 1033, 11, 370, 257, 2128, 2316, 307, 257, 2316, 300, 2212, 264, 2190, 1785, 295, 264, 1002, 318, 295, 256, 50964, 50964, 420, 428, 1558, 295, 428, 2190, 1785, 295, 264, 1002, 293, 364, 3069, 300, 291, 434, 1940, 51214, 51214, 420, 300, 1580, 1646, 307, 1940, 11, 746, 300, 291, 393, 2826, 420, 412, 1935, 11441, 11, 51514, 51514, 293, 4317, 364, 43741, 48994, 7006, 1176, 295, 256, 11, 51664, 51664, 597, 8855, 437, 291, 500, 380, 458, 466, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09634801007192069, "compression_ratio": 1.7727272727272727, "no_speech_prob": 2.3907565264380537e-05}, {"id": 487, "seek": 272806, "start": 2740.06, "end": 2745.06, "text": " or your idea of your current state of the world and an action that you're taking", "tokens": [50364, 400, 1310, 264, 1002, 307, 28621, 984, 342, 8997, 2750, 13, 50564, 50564, 492, 500, 380, 458, 300, 767, 13, 50664, 50664, 1033, 11, 370, 257, 2128, 2316, 307, 257, 2316, 300, 2212, 264, 2190, 1785, 295, 264, 1002, 318, 295, 256, 50964, 50964, 420, 428, 1558, 295, 428, 2190, 1785, 295, 264, 1002, 293, 364, 3069, 300, 291, 434, 1940, 51214, 51214, 420, 300, 1580, 1646, 307, 1940, 11, 746, 300, 291, 393, 2826, 420, 412, 1935, 11441, 11, 51514, 51514, 293, 4317, 364, 43741, 48994, 7006, 1176, 295, 256, 11, 51664, 51664, 597, 8855, 437, 291, 500, 380, 458, 466, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09634801007192069, "compression_ratio": 1.7727272727272727, "no_speech_prob": 2.3907565264380537e-05}, {"id": 488, "seek": 272806, "start": 2745.06, "end": 2751.06, "text": " or that someone else is taking, something that you can choose or at least observe,", "tokens": [50364, 400, 1310, 264, 1002, 307, 28621, 984, 342, 8997, 2750, 13, 50564, 50564, 492, 500, 380, 458, 300, 767, 13, 50664, 50664, 1033, 11, 370, 257, 2128, 2316, 307, 257, 2316, 300, 2212, 264, 2190, 1785, 295, 264, 1002, 318, 295, 256, 50964, 50964, 420, 428, 1558, 295, 428, 2190, 1785, 295, 264, 1002, 293, 364, 3069, 300, 291, 434, 1940, 51214, 51214, 420, 300, 1580, 1646, 307, 1940, 11, 746, 300, 291, 393, 2826, 420, 412, 1935, 11441, 11, 51514, 51514, 293, 4317, 364, 43741, 48994, 7006, 1176, 295, 256, 11, 51664, 51664, 597, 8855, 437, 291, 500, 380, 458, 466, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09634801007192069, "compression_ratio": 1.7727272727272727, "no_speech_prob": 2.3907565264380537e-05}, {"id": 489, "seek": 272806, "start": 2751.06, "end": 2754.06, "text": " and perhaps an auxiliary latent variable Z of t,", "tokens": [50364, 400, 1310, 264, 1002, 307, 28621, 984, 342, 8997, 2750, 13, 50564, 50564, 492, 500, 380, 458, 300, 767, 13, 50664, 50664, 1033, 11, 370, 257, 2128, 2316, 307, 257, 2316, 300, 2212, 264, 2190, 1785, 295, 264, 1002, 318, 295, 256, 50964, 50964, 420, 428, 1558, 295, 428, 2190, 1785, 295, 264, 1002, 293, 364, 3069, 300, 291, 434, 1940, 51214, 51214, 420, 300, 1580, 1646, 307, 1940, 11, 746, 300, 291, 393, 2826, 420, 412, 1935, 11441, 11, 51514, 51514, 293, 4317, 364, 43741, 48994, 7006, 1176, 295, 256, 11, 51664, 51664, 597, 8855, 437, 291, 500, 380, 458, 466, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09634801007192069, "compression_ratio": 1.7727272727272727, "no_speech_prob": 2.3907565264380537e-05}, {"id": 490, "seek": 272806, "start": 2754.06, "end": 2757.06, "text": " which represents what you don't know about the world.", "tokens": [50364, 400, 1310, 264, 1002, 307, 28621, 984, 342, 8997, 2750, 13, 50564, 50564, 492, 500, 380, 458, 300, 767, 13, 50664, 50664, 1033, 11, 370, 257, 2128, 2316, 307, 257, 2316, 300, 2212, 264, 2190, 1785, 295, 264, 1002, 318, 295, 256, 50964, 50964, 420, 428, 1558, 295, 428, 2190, 1785, 295, 264, 1002, 293, 364, 3069, 300, 291, 434, 1940, 51214, 51214, 420, 300, 1580, 1646, 307, 1940, 11, 746, 300, 291, 393, 2826, 420, 412, 1935, 11441, 11, 51514, 51514, 293, 4317, 364, 43741, 48994, 7006, 1176, 295, 256, 11, 51664, 51664, 597, 8855, 437, 291, 500, 380, 458, 466, 264, 1002, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09634801007192069, "compression_ratio": 1.7727272727272727, "no_speech_prob": 2.3907565264380537e-05}, {"id": 491, "seek": 275706, "start": 2757.06, "end": 2761.06, "text": " So the part of the world, the state of the world that you don't know", "tokens": [50364, 407, 264, 644, 295, 264, 1002, 11, 264, 1785, 295, 264, 1002, 300, 291, 500, 380, 458, 50564, 50564, 420, 264, 551, 300, 311, 31160, 466, 437, 311, 516, 281, 352, 322, 294, 264, 1002, 13, 50764, 50764, 440, 2128, 2316, 6069, 82, 264, 958, 1785, 295, 264, 1002, 11, 318, 256, 1804, 472, 13, 50964, 50964, 509, 25656, 1125, 565, 294, 512, 636, 13, 51164, 51164, 407, 498, 291, 362, 257, 2316, 295, 264, 1002, 295, 300, 2010, 11, 51414, 51414, 291, 393, 27817, 294, 428, 1378, 437, 311, 516, 281, 1051, 382, 257, 18326, 295, 428, 5909, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08957633605370155, "compression_ratio": 1.8113207547169812, "no_speech_prob": 4.0017977880779654e-05}, {"id": 492, "seek": 275706, "start": 2761.06, "end": 2765.06, "text": " or the thing that's unpredictable about what's going to go on in the world.", "tokens": [50364, 407, 264, 644, 295, 264, 1002, 11, 264, 1785, 295, 264, 1002, 300, 291, 500, 380, 458, 50564, 50564, 420, 264, 551, 300, 311, 31160, 466, 437, 311, 516, 281, 352, 322, 294, 264, 1002, 13, 50764, 50764, 440, 2128, 2316, 6069, 82, 264, 958, 1785, 295, 264, 1002, 11, 318, 256, 1804, 472, 13, 50964, 50964, 509, 25656, 1125, 565, 294, 512, 636, 13, 51164, 51164, 407, 498, 291, 362, 257, 2316, 295, 264, 1002, 295, 300, 2010, 11, 51414, 51414, 291, 393, 27817, 294, 428, 1378, 437, 311, 516, 281, 1051, 382, 257, 18326, 295, 428, 5909, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08957633605370155, "compression_ratio": 1.8113207547169812, "no_speech_prob": 4.0017977880779654e-05}, {"id": 493, "seek": 275706, "start": 2765.06, "end": 2769.06, "text": " The forward model predicts the next state of the world, S t plus one.", "tokens": [50364, 407, 264, 644, 295, 264, 1002, 11, 264, 1785, 295, 264, 1002, 300, 291, 500, 380, 458, 50564, 50564, 420, 264, 551, 300, 311, 31160, 466, 437, 311, 516, 281, 352, 322, 294, 264, 1002, 13, 50764, 50764, 440, 2128, 2316, 6069, 82, 264, 958, 1785, 295, 264, 1002, 11, 318, 256, 1804, 472, 13, 50964, 50964, 509, 25656, 1125, 565, 294, 512, 636, 13, 51164, 51164, 407, 498, 291, 362, 257, 2316, 295, 264, 1002, 295, 300, 2010, 11, 51414, 51414, 291, 393, 27817, 294, 428, 1378, 437, 311, 516, 281, 1051, 382, 257, 18326, 295, 428, 5909, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08957633605370155, "compression_ratio": 1.8113207547169812, "no_speech_prob": 4.0017977880779654e-05}, {"id": 494, "seek": 275706, "start": 2769.06, "end": 2773.06, "text": " You discretize time in some way.", "tokens": [50364, 407, 264, 644, 295, 264, 1002, 11, 264, 1785, 295, 264, 1002, 300, 291, 500, 380, 458, 50564, 50564, 420, 264, 551, 300, 311, 31160, 466, 437, 311, 516, 281, 352, 322, 294, 264, 1002, 13, 50764, 50764, 440, 2128, 2316, 6069, 82, 264, 958, 1785, 295, 264, 1002, 11, 318, 256, 1804, 472, 13, 50964, 50964, 509, 25656, 1125, 565, 294, 512, 636, 13, 51164, 51164, 407, 498, 291, 362, 257, 2316, 295, 264, 1002, 295, 300, 2010, 11, 51414, 51414, 291, 393, 27817, 294, 428, 1378, 437, 311, 516, 281, 1051, 382, 257, 18326, 295, 428, 5909, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08957633605370155, "compression_ratio": 1.8113207547169812, "no_speech_prob": 4.0017977880779654e-05}, {"id": 495, "seek": 275706, "start": 2773.06, "end": 2778.06, "text": " So if you have a model of the world of that type,", "tokens": [50364, 407, 264, 644, 295, 264, 1002, 11, 264, 1785, 295, 264, 1002, 300, 291, 500, 380, 458, 50564, 50564, 420, 264, 551, 300, 311, 31160, 466, 437, 311, 516, 281, 352, 322, 294, 264, 1002, 13, 50764, 50764, 440, 2128, 2316, 6069, 82, 264, 958, 1785, 295, 264, 1002, 11, 318, 256, 1804, 472, 13, 50964, 50964, 509, 25656, 1125, 565, 294, 512, 636, 13, 51164, 51164, 407, 498, 291, 362, 257, 2316, 295, 264, 1002, 295, 300, 2010, 11, 51414, 51414, 291, 393, 27817, 294, 428, 1378, 437, 311, 516, 281, 1051, 382, 257, 18326, 295, 428, 5909, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08957633605370155, "compression_ratio": 1.8113207547169812, "no_speech_prob": 4.0017977880779654e-05}, {"id": 496, "seek": 275706, "start": 2778.06, "end": 2785.06, "text": " you can simulate in your head what's going to happen as a consequence of your actions.", "tokens": [50364, 407, 264, 644, 295, 264, 1002, 11, 264, 1785, 295, 264, 1002, 300, 291, 500, 380, 458, 50564, 50564, 420, 264, 551, 300, 311, 31160, 466, 437, 311, 516, 281, 352, 322, 294, 264, 1002, 13, 50764, 50764, 440, 2128, 2316, 6069, 82, 264, 958, 1785, 295, 264, 1002, 11, 318, 256, 1804, 472, 13, 50964, 50964, 509, 25656, 1125, 565, 294, 512, 636, 13, 51164, 51164, 407, 498, 291, 362, 257, 2316, 295, 264, 1002, 295, 300, 2010, 11, 51414, 51414, 291, 393, 27817, 294, 428, 1378, 437, 311, 516, 281, 1051, 382, 257, 18326, 295, 428, 5909, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08957633605370155, "compression_ratio": 1.8113207547169812, "no_speech_prob": 4.0017977880779654e-05}, {"id": 497, "seek": 278506, "start": 2785.06, "end": 2790.06, "text": " So you have this model in your head.", "tokens": [50364, 407, 291, 362, 341, 2316, 294, 428, 1378, 13, 50614, 50614, 509, 458, 264, 2190, 1785, 295, 264, 1002, 420, 512, 1558, 295, 264, 2190, 1785, 295, 264, 1002, 13, 50814, 50814, 509, 1190, 428, 6920, 2316, 295, 264, 1002, 2128, 365, 257, 8310, 295, 316, 295, 256, 11, 51064, 51064, 597, 307, 257, 8310, 295, 3069, 300, 291, 3811, 1940, 13, 51314, 51314, 400, 428, 2316, 295, 264, 1002, 11, 382, 291, 3811, 309, 11, 486, 6069, 437, 311, 516, 281, 1051, 294, 264, 1002, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07163475895975972, "compression_ratio": 1.8677248677248677, "no_speech_prob": 2.3549908291897736e-05}, {"id": 498, "seek": 278506, "start": 2790.06, "end": 2794.06, "text": " You know the current state of the world or some idea of the current state of the world.", "tokens": [50364, 407, 291, 362, 341, 2316, 294, 428, 1378, 13, 50614, 50614, 509, 458, 264, 2190, 1785, 295, 264, 1002, 420, 512, 1558, 295, 264, 2190, 1785, 295, 264, 1002, 13, 50814, 50814, 509, 1190, 428, 6920, 2316, 295, 264, 1002, 2128, 365, 257, 8310, 295, 316, 295, 256, 11, 51064, 51064, 597, 307, 257, 8310, 295, 3069, 300, 291, 3811, 1940, 13, 51314, 51314, 400, 428, 2316, 295, 264, 1002, 11, 382, 291, 3811, 309, 11, 486, 6069, 437, 311, 516, 281, 1051, 294, 264, 1002, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07163475895975972, "compression_ratio": 1.8677248677248677, "no_speech_prob": 2.3549908291897736e-05}, {"id": 499, "seek": 278506, "start": 2794.06, "end": 2799.06, "text": " You run your internal model of the world forward with a sequence of A of t,", "tokens": [50364, 407, 291, 362, 341, 2316, 294, 428, 1378, 13, 50614, 50614, 509, 458, 264, 2190, 1785, 295, 264, 1002, 420, 512, 1558, 295, 264, 2190, 1785, 295, 264, 1002, 13, 50814, 50814, 509, 1190, 428, 6920, 2316, 295, 264, 1002, 2128, 365, 257, 8310, 295, 316, 295, 256, 11, 51064, 51064, 597, 307, 257, 8310, 295, 3069, 300, 291, 3811, 1940, 13, 51314, 51314, 400, 428, 2316, 295, 264, 1002, 11, 382, 291, 3811, 309, 11, 486, 6069, 437, 311, 516, 281, 1051, 294, 264, 1002, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07163475895975972, "compression_ratio": 1.8677248677248677, "no_speech_prob": 2.3549908291897736e-05}, {"id": 500, "seek": 278506, "start": 2799.06, "end": 2804.06, "text": " which is a sequence of action that you imagine taking.", "tokens": [50364, 407, 291, 362, 341, 2316, 294, 428, 1378, 13, 50614, 50614, 509, 458, 264, 2190, 1785, 295, 264, 1002, 420, 512, 1558, 295, 264, 2190, 1785, 295, 264, 1002, 13, 50814, 50814, 509, 1190, 428, 6920, 2316, 295, 264, 1002, 2128, 365, 257, 8310, 295, 316, 295, 256, 11, 51064, 51064, 597, 307, 257, 8310, 295, 3069, 300, 291, 3811, 1940, 13, 51314, 51314, 400, 428, 2316, 295, 264, 1002, 11, 382, 291, 3811, 309, 11, 486, 6069, 437, 311, 516, 281, 1051, 294, 264, 1002, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07163475895975972, "compression_ratio": 1.8677248677248677, "no_speech_prob": 2.3549908291897736e-05}, {"id": 501, "seek": 278506, "start": 2804.06, "end": 2812.06, "text": " And your model of the world, as you imagine it, will predict what's going to happen in the world.", "tokens": [50364, 407, 291, 362, 341, 2316, 294, 428, 1378, 13, 50614, 50614, 509, 458, 264, 2190, 1785, 295, 264, 1002, 420, 512, 1558, 295, 264, 2190, 1785, 295, 264, 1002, 13, 50814, 50814, 509, 1190, 428, 6920, 2316, 295, 264, 1002, 2128, 365, 257, 8310, 295, 316, 295, 256, 11, 51064, 51064, 597, 307, 257, 8310, 295, 3069, 300, 291, 3811, 1940, 13, 51314, 51314, 400, 428, 2316, 295, 264, 1002, 11, 382, 291, 3811, 309, 11, 486, 6069, 437, 311, 516, 281, 1051, 294, 264, 1002, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07163475895975972, "compression_ratio": 1.8677248677248677, "no_speech_prob": 2.3549908291897736e-05}, {"id": 502, "seek": 281206, "start": 2812.06, "end": 2819.06, "text": " If you could do this, then you could plan a sequence of actions that will arrive at a particular goal.", "tokens": [50364, 759, 291, 727, 360, 341, 11, 550, 291, 727, 1393, 257, 8310, 295, 5909, 300, 486, 8881, 412, 257, 1729, 3387, 13, 50714, 50714, 407, 337, 1365, 11, 437, 8310, 295, 3069, 820, 286, 360, 281, 4444, 341, 3435, 30, 51064, 51064, 286, 820, 1524, 257, 1729, 21512, 11, 605, 10107, 452, 9530, 294, 257, 1729, 636, 13, 51364, 51364, 407, 286, 4444, 341, 3435, 13, 51514, 51514, 400, 264, 2063, 2445, 286, 393, 3481, 307, 1968, 286, 600, 18607, 264, 3435, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08870892141057157, "compression_ratio": 1.71, "no_speech_prob": 2.2820131562184542e-05}, {"id": 503, "seek": 281206, "start": 2819.06, "end": 2826.06, "text": " So for example, what sequence of action should I do to grab this pen?", "tokens": [50364, 759, 291, 727, 360, 341, 11, 550, 291, 727, 1393, 257, 8310, 295, 5909, 300, 486, 8881, 412, 257, 1729, 3387, 13, 50714, 50714, 407, 337, 1365, 11, 437, 8310, 295, 3069, 820, 286, 360, 281, 4444, 341, 3435, 30, 51064, 51064, 286, 820, 1524, 257, 1729, 21512, 11, 605, 10107, 452, 9530, 294, 257, 1729, 636, 13, 51364, 51364, 407, 286, 4444, 341, 3435, 13, 51514, 51514, 400, 264, 2063, 2445, 286, 393, 3481, 307, 1968, 286, 600, 18607, 264, 3435, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08870892141057157, "compression_ratio": 1.71, "no_speech_prob": 2.2820131562184542e-05}, {"id": 504, "seek": 281206, "start": 2826.06, "end": 2832.06, "text": " I should follow a particular trajectory, actuate my muscles in a particular way.", "tokens": [50364, 759, 291, 727, 360, 341, 11, 550, 291, 727, 1393, 257, 8310, 295, 5909, 300, 486, 8881, 412, 257, 1729, 3387, 13, 50714, 50714, 407, 337, 1365, 11, 437, 8310, 295, 3069, 820, 286, 360, 281, 4444, 341, 3435, 30, 51064, 51064, 286, 820, 1524, 257, 1729, 21512, 11, 605, 10107, 452, 9530, 294, 257, 1729, 636, 13, 51364, 51364, 407, 286, 4444, 341, 3435, 13, 51514, 51514, 400, 264, 2063, 2445, 286, 393, 3481, 307, 1968, 286, 600, 18607, 264, 3435, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08870892141057157, "compression_ratio": 1.71, "no_speech_prob": 2.2820131562184542e-05}, {"id": 505, "seek": 281206, "start": 2832.06, "end": 2835.06, "text": " So I grab this pen.", "tokens": [50364, 759, 291, 727, 360, 341, 11, 550, 291, 727, 1393, 257, 8310, 295, 5909, 300, 486, 8881, 412, 257, 1729, 3387, 13, 50714, 50714, 407, 337, 1365, 11, 437, 8310, 295, 3069, 820, 286, 360, 281, 4444, 341, 3435, 30, 51064, 51064, 286, 820, 1524, 257, 1729, 21512, 11, 605, 10107, 452, 9530, 294, 257, 1729, 636, 13, 51364, 51364, 407, 286, 4444, 341, 3435, 13, 51514, 51514, 400, 264, 2063, 2445, 286, 393, 3481, 307, 1968, 286, 600, 18607, 264, 3435, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08870892141057157, "compression_ratio": 1.71, "no_speech_prob": 2.2820131562184542e-05}, {"id": 506, "seek": 281206, "start": 2835.06, "end": 2841.06, "text": " And the cost function I can measure is whether I've grabbed the pen,", "tokens": [50364, 759, 291, 727, 360, 341, 11, 550, 291, 727, 1393, 257, 8310, 295, 5909, 300, 486, 8881, 412, 257, 1729, 3387, 13, 50714, 50714, 407, 337, 1365, 11, 437, 8310, 295, 3069, 820, 286, 360, 281, 4444, 341, 3435, 30, 51064, 51064, 286, 820, 1524, 257, 1729, 21512, 11, 605, 10107, 452, 9530, 294, 257, 1729, 636, 13, 51364, 51364, 407, 286, 4444, 341, 3435, 13, 51514, 51514, 400, 264, 2063, 2445, 286, 393, 3481, 307, 1968, 286, 600, 18607, 264, 3435, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08870892141057157, "compression_ratio": 1.71, "no_speech_prob": 2.2820131562184542e-05}, {"id": 507, "seek": 284106, "start": 2841.06, "end": 2843.06, "text": " whether the pen is in my grasp.", "tokens": [50364, 1968, 264, 3435, 307, 294, 452, 21743, 13, 50464, 50464, 286, 727, 3481, 341, 365, 512, 2445, 11, 4317, 13, 50664, 50664, 400, 264, 1168, 307, 11, 393, 286, 1393, 257, 8310, 295, 5909, 300, 11, 2212, 452, 2316, 295, 264, 1002, 11, 50914, 50914, 597, 294, 341, 1389, 307, 264, 2316, 295, 452, 1011, 293, 264, 2316, 295, 689, 264, 3435, 307, 11, 51114, 51114, 486, 2089, 385, 281, 4444, 309, 30, 51214, 51214, 467, 311, 257, 707, 544, 6179, 498, 286, 3507, 264, 3435, 293, 286, 362, 281, 3745, 309, 294, 264, 1988, 11, 51414, 51414, 570, 286, 362, 281, 6069, 264, 21512, 295, 264, 3435, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0725273368632899, "compression_ratio": 1.7095435684647302, "no_speech_prob": 5.384478936321102e-05}, {"id": 508, "seek": 284106, "start": 2843.06, "end": 2847.06, "text": " I could measure this with some function, perhaps.", "tokens": [50364, 1968, 264, 3435, 307, 294, 452, 21743, 13, 50464, 50464, 286, 727, 3481, 341, 365, 512, 2445, 11, 4317, 13, 50664, 50664, 400, 264, 1168, 307, 11, 393, 286, 1393, 257, 8310, 295, 5909, 300, 11, 2212, 452, 2316, 295, 264, 1002, 11, 50914, 50914, 597, 294, 341, 1389, 307, 264, 2316, 295, 452, 1011, 293, 264, 2316, 295, 689, 264, 3435, 307, 11, 51114, 51114, 486, 2089, 385, 281, 4444, 309, 30, 51214, 51214, 467, 311, 257, 707, 544, 6179, 498, 286, 3507, 264, 3435, 293, 286, 362, 281, 3745, 309, 294, 264, 1988, 11, 51414, 51414, 570, 286, 362, 281, 6069, 264, 21512, 295, 264, 3435, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0725273368632899, "compression_ratio": 1.7095435684647302, "no_speech_prob": 5.384478936321102e-05}, {"id": 509, "seek": 284106, "start": 2847.06, "end": 2852.06, "text": " And the question is, can I plan a sequence of actions that, given my model of the world,", "tokens": [50364, 1968, 264, 3435, 307, 294, 452, 21743, 13, 50464, 50464, 286, 727, 3481, 341, 365, 512, 2445, 11, 4317, 13, 50664, 50664, 400, 264, 1168, 307, 11, 393, 286, 1393, 257, 8310, 295, 5909, 300, 11, 2212, 452, 2316, 295, 264, 1002, 11, 50914, 50914, 597, 294, 341, 1389, 307, 264, 2316, 295, 452, 1011, 293, 264, 2316, 295, 689, 264, 3435, 307, 11, 51114, 51114, 486, 2089, 385, 281, 4444, 309, 30, 51214, 51214, 467, 311, 257, 707, 544, 6179, 498, 286, 3507, 264, 3435, 293, 286, 362, 281, 3745, 309, 294, 264, 1988, 11, 51414, 51414, 570, 286, 362, 281, 6069, 264, 21512, 295, 264, 3435, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0725273368632899, "compression_ratio": 1.7095435684647302, "no_speech_prob": 5.384478936321102e-05}, {"id": 510, "seek": 284106, "start": 2852.06, "end": 2856.06, "text": " which in this case is the model of my hand and the model of where the pen is,", "tokens": [50364, 1968, 264, 3435, 307, 294, 452, 21743, 13, 50464, 50464, 286, 727, 3481, 341, 365, 512, 2445, 11, 4317, 13, 50664, 50664, 400, 264, 1168, 307, 11, 393, 286, 1393, 257, 8310, 295, 5909, 300, 11, 2212, 452, 2316, 295, 264, 1002, 11, 50914, 50914, 597, 294, 341, 1389, 307, 264, 2316, 295, 452, 1011, 293, 264, 2316, 295, 689, 264, 3435, 307, 11, 51114, 51114, 486, 2089, 385, 281, 4444, 309, 30, 51214, 51214, 467, 311, 257, 707, 544, 6179, 498, 286, 3507, 264, 3435, 293, 286, 362, 281, 3745, 309, 294, 264, 1988, 11, 51414, 51414, 570, 286, 362, 281, 6069, 264, 21512, 295, 264, 3435, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0725273368632899, "compression_ratio": 1.7095435684647302, "no_speech_prob": 5.384478936321102e-05}, {"id": 511, "seek": 284106, "start": 2856.06, "end": 2858.06, "text": " will allow me to grab it?", "tokens": [50364, 1968, 264, 3435, 307, 294, 452, 21743, 13, 50464, 50464, 286, 727, 3481, 341, 365, 512, 2445, 11, 4317, 13, 50664, 50664, 400, 264, 1168, 307, 11, 393, 286, 1393, 257, 8310, 295, 5909, 300, 11, 2212, 452, 2316, 295, 264, 1002, 11, 50914, 50914, 597, 294, 341, 1389, 307, 264, 2316, 295, 452, 1011, 293, 264, 2316, 295, 689, 264, 3435, 307, 11, 51114, 51114, 486, 2089, 385, 281, 4444, 309, 30, 51214, 51214, 467, 311, 257, 707, 544, 6179, 498, 286, 3507, 264, 3435, 293, 286, 362, 281, 3745, 309, 294, 264, 1988, 11, 51414, 51414, 570, 286, 362, 281, 6069, 264, 21512, 295, 264, 3435, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0725273368632899, "compression_ratio": 1.7095435684647302, "no_speech_prob": 5.384478936321102e-05}, {"id": 512, "seek": 284106, "start": 2858.06, "end": 2862.06, "text": " It's a little more complicated if I throw the pen and I have to catch it in the air,", "tokens": [50364, 1968, 264, 3435, 307, 294, 452, 21743, 13, 50464, 50464, 286, 727, 3481, 341, 365, 512, 2445, 11, 4317, 13, 50664, 50664, 400, 264, 1168, 307, 11, 393, 286, 1393, 257, 8310, 295, 5909, 300, 11, 2212, 452, 2316, 295, 264, 1002, 11, 50914, 50914, 597, 294, 341, 1389, 307, 264, 2316, 295, 452, 1011, 293, 264, 2316, 295, 689, 264, 3435, 307, 11, 51114, 51114, 486, 2089, 385, 281, 4444, 309, 30, 51214, 51214, 467, 311, 257, 707, 544, 6179, 498, 286, 3507, 264, 3435, 293, 286, 362, 281, 3745, 309, 294, 264, 1988, 11, 51414, 51414, 570, 286, 362, 281, 6069, 264, 21512, 295, 264, 3435, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0725273368632899, "compression_ratio": 1.7095435684647302, "no_speech_prob": 5.384478936321102e-05}, {"id": 513, "seek": 284106, "start": 2862.06, "end": 2865.06, "text": " because I have to predict the trajectory of the pen.", "tokens": [50364, 1968, 264, 3435, 307, 294, 452, 21743, 13, 50464, 50464, 286, 727, 3481, 341, 365, 512, 2445, 11, 4317, 13, 50664, 50664, 400, 264, 1168, 307, 11, 393, 286, 1393, 257, 8310, 295, 5909, 300, 11, 2212, 452, 2316, 295, 264, 1002, 11, 50914, 50914, 597, 294, 341, 1389, 307, 264, 2316, 295, 452, 1011, 293, 264, 2316, 295, 689, 264, 3435, 307, 11, 51114, 51114, 486, 2089, 385, 281, 4444, 309, 30, 51214, 51214, 467, 311, 257, 707, 544, 6179, 498, 286, 3507, 264, 3435, 293, 286, 362, 281, 3745, 309, 294, 264, 1988, 11, 51414, 51414, 570, 286, 362, 281, 6069, 264, 21512, 295, 264, 3435, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0725273368632899, "compression_ratio": 1.7095435684647302, "no_speech_prob": 5.384478936321102e-05}, {"id": 514, "seek": 286506, "start": 2865.06, "end": 2871.06, "text": " So I have to have an intuitive model of physics to be able to grab that pen,", "tokens": [50364, 407, 286, 362, 281, 362, 364, 21769, 2316, 295, 10649, 281, 312, 1075, 281, 4444, 300, 3435, 11, 50664, 50664, 597, 295, 1164, 286, 600, 3264, 807, 1752, 382, 731, 13, 50764, 50764, 3432, 366, 6100, 291, 411, 370, 709, 29280, 2539, 13, 50914, 50914, 639, 307, 406, 29280, 2539, 13, 50964, 50964, 639, 575, 3122, 1825, 281, 360, 365, 29280, 2539, 13, 51114, 51114, 961, 385, 312, 588, 1850, 13, 51164, 51164, 639, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51314, 51314, 639, 815, 362, 281, 360, 294, 264, 2027, 11, 2264, 30, 51414, 51414, 583, 558, 586, 309, 1177, 380, 13, 51564, 51564, 17105, 12, 6032, 29280, 2539, 13, 51664, 51664, 883, 11, 309, 1177, 380, 13, 51714, 51714, 467, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10266247502079716, "compression_ratio": 2.0980392156862746, "no_speech_prob": 2.1778543668915518e-05}, {"id": 515, "seek": 286506, "start": 2871.06, "end": 2873.06, "text": " which of course I've learned through experience as well.", "tokens": [50364, 407, 286, 362, 281, 362, 364, 21769, 2316, 295, 10649, 281, 312, 1075, 281, 4444, 300, 3435, 11, 50664, 50664, 597, 295, 1164, 286, 600, 3264, 807, 1752, 382, 731, 13, 50764, 50764, 3432, 366, 6100, 291, 411, 370, 709, 29280, 2539, 13, 50914, 50914, 639, 307, 406, 29280, 2539, 13, 50964, 50964, 639, 575, 3122, 1825, 281, 360, 365, 29280, 2539, 13, 51114, 51114, 961, 385, 312, 588, 1850, 13, 51164, 51164, 639, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51314, 51314, 639, 815, 362, 281, 360, 294, 264, 2027, 11, 2264, 30, 51414, 51414, 583, 558, 586, 309, 1177, 380, 13, 51564, 51564, 17105, 12, 6032, 29280, 2539, 13, 51664, 51664, 883, 11, 309, 1177, 380, 13, 51714, 51714, 467, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10266247502079716, "compression_ratio": 2.0980392156862746, "no_speech_prob": 2.1778543668915518e-05}, {"id": 516, "seek": 286506, "start": 2873.06, "end": 2876.06, "text": " People are surprised you like so much reinforcement learning.", "tokens": [50364, 407, 286, 362, 281, 362, 364, 21769, 2316, 295, 10649, 281, 312, 1075, 281, 4444, 300, 3435, 11, 50664, 50664, 597, 295, 1164, 286, 600, 3264, 807, 1752, 382, 731, 13, 50764, 50764, 3432, 366, 6100, 291, 411, 370, 709, 29280, 2539, 13, 50914, 50914, 639, 307, 406, 29280, 2539, 13, 50964, 50964, 639, 575, 3122, 1825, 281, 360, 365, 29280, 2539, 13, 51114, 51114, 961, 385, 312, 588, 1850, 13, 51164, 51164, 639, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51314, 51314, 639, 815, 362, 281, 360, 294, 264, 2027, 11, 2264, 30, 51414, 51414, 583, 558, 586, 309, 1177, 380, 13, 51564, 51564, 17105, 12, 6032, 29280, 2539, 13, 51664, 51664, 883, 11, 309, 1177, 380, 13, 51714, 51714, 467, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10266247502079716, "compression_ratio": 2.0980392156862746, "no_speech_prob": 2.1778543668915518e-05}, {"id": 517, "seek": 286506, "start": 2876.06, "end": 2877.06, "text": " This is not reinforcement learning.", "tokens": [50364, 407, 286, 362, 281, 362, 364, 21769, 2316, 295, 10649, 281, 312, 1075, 281, 4444, 300, 3435, 11, 50664, 50664, 597, 295, 1164, 286, 600, 3264, 807, 1752, 382, 731, 13, 50764, 50764, 3432, 366, 6100, 291, 411, 370, 709, 29280, 2539, 13, 50914, 50914, 639, 307, 406, 29280, 2539, 13, 50964, 50964, 639, 575, 3122, 1825, 281, 360, 365, 29280, 2539, 13, 51114, 51114, 961, 385, 312, 588, 1850, 13, 51164, 51164, 639, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51314, 51314, 639, 815, 362, 281, 360, 294, 264, 2027, 11, 2264, 30, 51414, 51414, 583, 558, 586, 309, 1177, 380, 13, 51564, 51564, 17105, 12, 6032, 29280, 2539, 13, 51664, 51664, 883, 11, 309, 1177, 380, 13, 51714, 51714, 467, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10266247502079716, "compression_ratio": 2.0980392156862746, "no_speech_prob": 2.1778543668915518e-05}, {"id": 518, "seek": 286506, "start": 2877.06, "end": 2880.06, "text": " This has absolutely nothing to do with reinforcement learning.", "tokens": [50364, 407, 286, 362, 281, 362, 364, 21769, 2316, 295, 10649, 281, 312, 1075, 281, 4444, 300, 3435, 11, 50664, 50664, 597, 295, 1164, 286, 600, 3264, 807, 1752, 382, 731, 13, 50764, 50764, 3432, 366, 6100, 291, 411, 370, 709, 29280, 2539, 13, 50914, 50914, 639, 307, 406, 29280, 2539, 13, 50964, 50964, 639, 575, 3122, 1825, 281, 360, 365, 29280, 2539, 13, 51114, 51114, 961, 385, 312, 588, 1850, 13, 51164, 51164, 639, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51314, 51314, 639, 815, 362, 281, 360, 294, 264, 2027, 11, 2264, 30, 51414, 51414, 583, 558, 586, 309, 1177, 380, 13, 51564, 51564, 17105, 12, 6032, 29280, 2539, 13, 51664, 51664, 883, 11, 309, 1177, 380, 13, 51714, 51714, 467, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10266247502079716, "compression_ratio": 2.0980392156862746, "no_speech_prob": 2.1778543668915518e-05}, {"id": 519, "seek": 286506, "start": 2880.06, "end": 2881.06, "text": " Let me be very clear.", "tokens": [50364, 407, 286, 362, 281, 362, 364, 21769, 2316, 295, 10649, 281, 312, 1075, 281, 4444, 300, 3435, 11, 50664, 50664, 597, 295, 1164, 286, 600, 3264, 807, 1752, 382, 731, 13, 50764, 50764, 3432, 366, 6100, 291, 411, 370, 709, 29280, 2539, 13, 50914, 50914, 639, 307, 406, 29280, 2539, 13, 50964, 50964, 639, 575, 3122, 1825, 281, 360, 365, 29280, 2539, 13, 51114, 51114, 961, 385, 312, 588, 1850, 13, 51164, 51164, 639, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51314, 51314, 639, 815, 362, 281, 360, 294, 264, 2027, 11, 2264, 30, 51414, 51414, 583, 558, 586, 309, 1177, 380, 13, 51564, 51564, 17105, 12, 6032, 29280, 2539, 13, 51664, 51664, 883, 11, 309, 1177, 380, 13, 51714, 51714, 467, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10266247502079716, "compression_ratio": 2.0980392156862746, "no_speech_prob": 2.1778543668915518e-05}, {"id": 520, "seek": 286506, "start": 2881.06, "end": 2884.06, "text": " This has nothing to do with reinforcement learning.", "tokens": [50364, 407, 286, 362, 281, 362, 364, 21769, 2316, 295, 10649, 281, 312, 1075, 281, 4444, 300, 3435, 11, 50664, 50664, 597, 295, 1164, 286, 600, 3264, 807, 1752, 382, 731, 13, 50764, 50764, 3432, 366, 6100, 291, 411, 370, 709, 29280, 2539, 13, 50914, 50914, 639, 307, 406, 29280, 2539, 13, 50964, 50964, 639, 575, 3122, 1825, 281, 360, 365, 29280, 2539, 13, 51114, 51114, 961, 385, 312, 588, 1850, 13, 51164, 51164, 639, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51314, 51314, 639, 815, 362, 281, 360, 294, 264, 2027, 11, 2264, 30, 51414, 51414, 583, 558, 586, 309, 1177, 380, 13, 51564, 51564, 17105, 12, 6032, 29280, 2539, 13, 51664, 51664, 883, 11, 309, 1177, 380, 13, 51714, 51714, 467, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10266247502079716, "compression_ratio": 2.0980392156862746, "no_speech_prob": 2.1778543668915518e-05}, {"id": 521, "seek": 286506, "start": 2884.06, "end": 2886.06, "text": " This may have to do in the future, OK?", "tokens": [50364, 407, 286, 362, 281, 362, 364, 21769, 2316, 295, 10649, 281, 312, 1075, 281, 4444, 300, 3435, 11, 50664, 50664, 597, 295, 1164, 286, 600, 3264, 807, 1752, 382, 731, 13, 50764, 50764, 3432, 366, 6100, 291, 411, 370, 709, 29280, 2539, 13, 50914, 50914, 639, 307, 406, 29280, 2539, 13, 50964, 50964, 639, 575, 3122, 1825, 281, 360, 365, 29280, 2539, 13, 51114, 51114, 961, 385, 312, 588, 1850, 13, 51164, 51164, 639, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51314, 51314, 639, 815, 362, 281, 360, 294, 264, 2027, 11, 2264, 30, 51414, 51414, 583, 558, 586, 309, 1177, 380, 13, 51564, 51564, 17105, 12, 6032, 29280, 2539, 13, 51664, 51664, 883, 11, 309, 1177, 380, 13, 51714, 51714, 467, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10266247502079716, "compression_ratio": 2.0980392156862746, "no_speech_prob": 2.1778543668915518e-05}, {"id": 522, "seek": 286506, "start": 2886.06, "end": 2889.06, "text": " But right now it doesn't.", "tokens": [50364, 407, 286, 362, 281, 362, 364, 21769, 2316, 295, 10649, 281, 312, 1075, 281, 4444, 300, 3435, 11, 50664, 50664, 597, 295, 1164, 286, 600, 3264, 807, 1752, 382, 731, 13, 50764, 50764, 3432, 366, 6100, 291, 411, 370, 709, 29280, 2539, 13, 50914, 50914, 639, 307, 406, 29280, 2539, 13, 50964, 50964, 639, 575, 3122, 1825, 281, 360, 365, 29280, 2539, 13, 51114, 51114, 961, 385, 312, 588, 1850, 13, 51164, 51164, 639, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51314, 51314, 639, 815, 362, 281, 360, 294, 264, 2027, 11, 2264, 30, 51414, 51414, 583, 558, 586, 309, 1177, 380, 13, 51564, 51564, 17105, 12, 6032, 29280, 2539, 13, 51664, 51664, 883, 11, 309, 1177, 380, 13, 51714, 51714, 467, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10266247502079716, "compression_ratio": 2.0980392156862746, "no_speech_prob": 2.1778543668915518e-05}, {"id": 523, "seek": 286506, "start": 2889.06, "end": 2891.06, "text": " Model-based reinforcement learning.", "tokens": [50364, 407, 286, 362, 281, 362, 364, 21769, 2316, 295, 10649, 281, 312, 1075, 281, 4444, 300, 3435, 11, 50664, 50664, 597, 295, 1164, 286, 600, 3264, 807, 1752, 382, 731, 13, 50764, 50764, 3432, 366, 6100, 291, 411, 370, 709, 29280, 2539, 13, 50914, 50914, 639, 307, 406, 29280, 2539, 13, 50964, 50964, 639, 575, 3122, 1825, 281, 360, 365, 29280, 2539, 13, 51114, 51114, 961, 385, 312, 588, 1850, 13, 51164, 51164, 639, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51314, 51314, 639, 815, 362, 281, 360, 294, 264, 2027, 11, 2264, 30, 51414, 51414, 583, 558, 586, 309, 1177, 380, 13, 51564, 51564, 17105, 12, 6032, 29280, 2539, 13, 51664, 51664, 883, 11, 309, 1177, 380, 13, 51714, 51714, 467, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10266247502079716, "compression_ratio": 2.0980392156862746, "no_speech_prob": 2.1778543668915518e-05}, {"id": 524, "seek": 286506, "start": 2891.06, "end": 2892.06, "text": " No, it doesn't.", "tokens": [50364, 407, 286, 362, 281, 362, 364, 21769, 2316, 295, 10649, 281, 312, 1075, 281, 4444, 300, 3435, 11, 50664, 50664, 597, 295, 1164, 286, 600, 3264, 807, 1752, 382, 731, 13, 50764, 50764, 3432, 366, 6100, 291, 411, 370, 709, 29280, 2539, 13, 50914, 50914, 639, 307, 406, 29280, 2539, 13, 50964, 50964, 639, 575, 3122, 1825, 281, 360, 365, 29280, 2539, 13, 51114, 51114, 961, 385, 312, 588, 1850, 13, 51164, 51164, 639, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51314, 51314, 639, 815, 362, 281, 360, 294, 264, 2027, 11, 2264, 30, 51414, 51414, 583, 558, 586, 309, 1177, 380, 13, 51564, 51564, 17105, 12, 6032, 29280, 2539, 13, 51664, 51664, 883, 11, 309, 1177, 380, 13, 51714, 51714, 467, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10266247502079716, "compression_ratio": 2.0980392156862746, "no_speech_prob": 2.1778543668915518e-05}, {"id": 525, "seek": 286506, "start": 2892.06, "end": 2894.06, "text": " It has nothing to do with reinforcement learning.", "tokens": [50364, 407, 286, 362, 281, 362, 364, 21769, 2316, 295, 10649, 281, 312, 1075, 281, 4444, 300, 3435, 11, 50664, 50664, 597, 295, 1164, 286, 600, 3264, 807, 1752, 382, 731, 13, 50764, 50764, 3432, 366, 6100, 291, 411, 370, 709, 29280, 2539, 13, 50914, 50914, 639, 307, 406, 29280, 2539, 13, 50964, 50964, 639, 575, 3122, 1825, 281, 360, 365, 29280, 2539, 13, 51114, 51114, 961, 385, 312, 588, 1850, 13, 51164, 51164, 639, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51314, 51314, 639, 815, 362, 281, 360, 294, 264, 2027, 11, 2264, 30, 51414, 51414, 583, 558, 586, 309, 1177, 380, 13, 51564, 51564, 17105, 12, 6032, 29280, 2539, 13, 51664, 51664, 883, 11, 309, 1177, 380, 13, 51714, 51714, 467, 575, 1825, 281, 360, 365, 29280, 2539, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10266247502079716, "compression_ratio": 2.0980392156862746, "no_speech_prob": 2.1778543668915518e-05}, {"id": 526, "seek": 289406, "start": 2894.06, "end": 2896.06, "text": " OK, let me go through this a little bit.", "tokens": [50364, 2264, 11, 718, 385, 352, 807, 341, 257, 707, 857, 13, 50464, 50464, 1664, 291, 2903, 264, 2649, 550, 30, 50514, 50514, 1079, 11, 286, 486, 13, 50614, 50614, 682, 257, 3456, 13, 50664, 50664, 2264, 11, 370, 586, 11, 370, 322, 264, 1411, 510, 291, 362, 341, 707, 9461, 13, 50964, 50964, 467, 575, 341, 2316, 295, 264, 1002, 300, 309, 393, 1190, 2128, 11, 2264, 30, 51164, 51164, 467, 575, 364, 8747, 11, 420, 291, 393, 519, 295, 309, 382, 257, 3897, 300, 14725, 257, 8310, 295, 5909, 11, 51464, 51464, 597, 309, 307, 516, 281, 3154, 281, 264, 2316, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12289783442131827, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.211671229219064e-05}, {"id": 527, "seek": 289406, "start": 2896.06, "end": 2897.06, "text": " Can you explain the difference then?", "tokens": [50364, 2264, 11, 718, 385, 352, 807, 341, 257, 707, 857, 13, 50464, 50464, 1664, 291, 2903, 264, 2649, 550, 30, 50514, 50514, 1079, 11, 286, 486, 13, 50614, 50614, 682, 257, 3456, 13, 50664, 50664, 2264, 11, 370, 586, 11, 370, 322, 264, 1411, 510, 291, 362, 341, 707, 9461, 13, 50964, 50964, 467, 575, 341, 2316, 295, 264, 1002, 300, 309, 393, 1190, 2128, 11, 2264, 30, 51164, 51164, 467, 575, 364, 8747, 11, 420, 291, 393, 519, 295, 309, 382, 257, 3897, 300, 14725, 257, 8310, 295, 5909, 11, 51464, 51464, 597, 309, 307, 516, 281, 3154, 281, 264, 2316, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12289783442131827, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.211671229219064e-05}, {"id": 528, "seek": 289406, "start": 2897.06, "end": 2899.06, "text": " Yes, I will.", "tokens": [50364, 2264, 11, 718, 385, 352, 807, 341, 257, 707, 857, 13, 50464, 50464, 1664, 291, 2903, 264, 2649, 550, 30, 50514, 50514, 1079, 11, 286, 486, 13, 50614, 50614, 682, 257, 3456, 13, 50664, 50664, 2264, 11, 370, 586, 11, 370, 322, 264, 1411, 510, 291, 362, 341, 707, 9461, 13, 50964, 50964, 467, 575, 341, 2316, 295, 264, 1002, 300, 309, 393, 1190, 2128, 11, 2264, 30, 51164, 51164, 467, 575, 364, 8747, 11, 420, 291, 393, 519, 295, 309, 382, 257, 3897, 300, 14725, 257, 8310, 295, 5909, 11, 51464, 51464, 597, 309, 307, 516, 281, 3154, 281, 264, 2316, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12289783442131827, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.211671229219064e-05}, {"id": 529, "seek": 289406, "start": 2899.06, "end": 2900.06, "text": " In a minute.", "tokens": [50364, 2264, 11, 718, 385, 352, 807, 341, 257, 707, 857, 13, 50464, 50464, 1664, 291, 2903, 264, 2649, 550, 30, 50514, 50514, 1079, 11, 286, 486, 13, 50614, 50614, 682, 257, 3456, 13, 50664, 50664, 2264, 11, 370, 586, 11, 370, 322, 264, 1411, 510, 291, 362, 341, 707, 9461, 13, 50964, 50964, 467, 575, 341, 2316, 295, 264, 1002, 300, 309, 393, 1190, 2128, 11, 2264, 30, 51164, 51164, 467, 575, 364, 8747, 11, 420, 291, 393, 519, 295, 309, 382, 257, 3897, 300, 14725, 257, 8310, 295, 5909, 11, 51464, 51464, 597, 309, 307, 516, 281, 3154, 281, 264, 2316, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12289783442131827, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.211671229219064e-05}, {"id": 530, "seek": 289406, "start": 2900.06, "end": 2906.06, "text": " OK, so now, so on the left here you have this little agent.", "tokens": [50364, 2264, 11, 718, 385, 352, 807, 341, 257, 707, 857, 13, 50464, 50464, 1664, 291, 2903, 264, 2649, 550, 30, 50514, 50514, 1079, 11, 286, 486, 13, 50614, 50614, 682, 257, 3456, 13, 50664, 50664, 2264, 11, 370, 586, 11, 370, 322, 264, 1411, 510, 291, 362, 341, 707, 9461, 13, 50964, 50964, 467, 575, 341, 2316, 295, 264, 1002, 300, 309, 393, 1190, 2128, 11, 2264, 30, 51164, 51164, 467, 575, 364, 8747, 11, 420, 291, 393, 519, 295, 309, 382, 257, 3897, 300, 14725, 257, 8310, 295, 5909, 11, 51464, 51464, 597, 309, 307, 516, 281, 3154, 281, 264, 2316, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12289783442131827, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.211671229219064e-05}, {"id": 531, "seek": 289406, "start": 2906.06, "end": 2910.06, "text": " It has this model of the world that it can run forward, OK?", "tokens": [50364, 2264, 11, 718, 385, 352, 807, 341, 257, 707, 857, 13, 50464, 50464, 1664, 291, 2903, 264, 2649, 550, 30, 50514, 50514, 1079, 11, 286, 486, 13, 50614, 50614, 682, 257, 3456, 13, 50664, 50664, 2264, 11, 370, 586, 11, 370, 322, 264, 1411, 510, 291, 362, 341, 707, 9461, 13, 50964, 50964, 467, 575, 341, 2316, 295, 264, 1002, 300, 309, 393, 1190, 2128, 11, 2264, 30, 51164, 51164, 467, 575, 364, 8747, 11, 420, 291, 393, 519, 295, 309, 382, 257, 3897, 300, 14725, 257, 8310, 295, 5909, 11, 51464, 51464, 597, 309, 307, 516, 281, 3154, 281, 264, 2316, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12289783442131827, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.211671229219064e-05}, {"id": 532, "seek": 289406, "start": 2910.06, "end": 2916.06, "text": " It has an actor, or you can think of it as a policy that produces a sequence of actions,", "tokens": [50364, 2264, 11, 718, 385, 352, 807, 341, 257, 707, 857, 13, 50464, 50464, 1664, 291, 2903, 264, 2649, 550, 30, 50514, 50514, 1079, 11, 286, 486, 13, 50614, 50614, 682, 257, 3456, 13, 50664, 50664, 2264, 11, 370, 586, 11, 370, 322, 264, 1411, 510, 291, 362, 341, 707, 9461, 13, 50964, 50964, 467, 575, 341, 2316, 295, 264, 1002, 300, 309, 393, 1190, 2128, 11, 2264, 30, 51164, 51164, 467, 575, 364, 8747, 11, 420, 291, 393, 519, 295, 309, 382, 257, 3897, 300, 14725, 257, 8310, 295, 5909, 11, 51464, 51464, 597, 309, 307, 516, 281, 3154, 281, 264, 2316, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12289783442131827, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.211671229219064e-05}, {"id": 533, "seek": 289406, "start": 2916.06, "end": 2918.06, "text": " which it is going to feed to the model.", "tokens": [50364, 2264, 11, 718, 385, 352, 807, 341, 257, 707, 857, 13, 50464, 50464, 1664, 291, 2903, 264, 2649, 550, 30, 50514, 50514, 1079, 11, 286, 486, 13, 50614, 50614, 682, 257, 3456, 13, 50664, 50664, 2264, 11, 370, 586, 11, 370, 322, 264, 1411, 510, 291, 362, 341, 707, 9461, 13, 50964, 50964, 467, 575, 341, 2316, 295, 264, 1002, 300, 309, 393, 1190, 2128, 11, 2264, 30, 51164, 51164, 467, 575, 364, 8747, 11, 420, 291, 393, 519, 295, 309, 382, 257, 3897, 300, 14725, 257, 8310, 295, 5909, 11, 51464, 51464, 597, 309, 307, 516, 281, 3154, 281, 264, 2316, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12289783442131827, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.211671229219064e-05}, {"id": 534, "seek": 291806, "start": 2918.06, "end": 2925.06, "text": " And then a critic, which is going to predict what the cost of the final state", "tokens": [50364, 400, 550, 257, 7850, 11, 597, 307, 516, 281, 6069, 437, 264, 2063, 295, 264, 2572, 1785, 50714, 50714, 420, 264, 21512, 307, 516, 281, 312, 4650, 281, 264, 46691, 13, 50864, 50864, 407, 264, 7850, 510, 715, 1819, 1936, 264, 2063, 295, 406, 25800, 264, 3387, 300, 286, 992, 2059, 13, 51314, 51314, 407, 498, 452, 5633, 307, 281, 2524, 337, 341, 3435, 11, 293, 286, 733, 295, 1713, 264, 3435, 538, 257, 1326, 23300, 11, 51614, 51614, 452, 2063, 307, 257, 1326, 23300, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06157533327738444, "compression_ratio": 1.7073170731707317, "no_speech_prob": 2.8393867978593335e-05}, {"id": 535, "seek": 291806, "start": 2925.06, "end": 2928.06, "text": " or the trajectory is going to be according to the criterion.", "tokens": [50364, 400, 550, 257, 7850, 11, 597, 307, 516, 281, 6069, 437, 264, 2063, 295, 264, 2572, 1785, 50714, 50714, 420, 264, 21512, 307, 516, 281, 312, 4650, 281, 264, 46691, 13, 50864, 50864, 407, 264, 7850, 510, 715, 1819, 1936, 264, 2063, 295, 406, 25800, 264, 3387, 300, 286, 992, 2059, 13, 51314, 51314, 407, 498, 452, 5633, 307, 281, 2524, 337, 341, 3435, 11, 293, 286, 733, 295, 1713, 264, 3435, 538, 257, 1326, 23300, 11, 51614, 51614, 452, 2063, 307, 257, 1326, 23300, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06157533327738444, "compression_ratio": 1.7073170731707317, "no_speech_prob": 2.8393867978593335e-05}, {"id": 536, "seek": 291806, "start": 2928.06, "end": 2937.06, "text": " So the critic here computes basically the cost of not fulfilling the goal that I set myself.", "tokens": [50364, 400, 550, 257, 7850, 11, 597, 307, 516, 281, 6069, 437, 264, 2063, 295, 264, 2572, 1785, 50714, 50714, 420, 264, 21512, 307, 516, 281, 312, 4650, 281, 264, 46691, 13, 50864, 50864, 407, 264, 7850, 510, 715, 1819, 1936, 264, 2063, 295, 406, 25800, 264, 3387, 300, 286, 992, 2059, 13, 51314, 51314, 407, 498, 452, 5633, 307, 281, 2524, 337, 341, 3435, 11, 293, 286, 733, 295, 1713, 264, 3435, 538, 257, 1326, 23300, 11, 51614, 51614, 452, 2063, 307, 257, 1326, 23300, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06157533327738444, "compression_ratio": 1.7073170731707317, "no_speech_prob": 2.8393867978593335e-05}, {"id": 537, "seek": 291806, "start": 2937.06, "end": 2943.06, "text": " So if my task is to reach for this pen, and I kind of miss the pen by a few centimeters,", "tokens": [50364, 400, 550, 257, 7850, 11, 597, 307, 516, 281, 6069, 437, 264, 2063, 295, 264, 2572, 1785, 50714, 50714, 420, 264, 21512, 307, 516, 281, 312, 4650, 281, 264, 46691, 13, 50864, 50864, 407, 264, 7850, 510, 715, 1819, 1936, 264, 2063, 295, 406, 25800, 264, 3387, 300, 286, 992, 2059, 13, 51314, 51314, 407, 498, 452, 5633, 307, 281, 2524, 337, 341, 3435, 11, 293, 286, 733, 295, 1713, 264, 3435, 538, 257, 1326, 23300, 11, 51614, 51614, 452, 2063, 307, 257, 1326, 23300, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06157533327738444, "compression_ratio": 1.7073170731707317, "no_speech_prob": 2.8393867978593335e-05}, {"id": 538, "seek": 291806, "start": 2943.06, "end": 2946.06, "text": " my cost is a few centimeters.", "tokens": [50364, 400, 550, 257, 7850, 11, 597, 307, 516, 281, 6069, 437, 264, 2063, 295, 264, 2572, 1785, 50714, 50714, 420, 264, 21512, 307, 516, 281, 312, 4650, 281, 264, 46691, 13, 50864, 50864, 407, 264, 7850, 510, 715, 1819, 1936, 264, 2063, 295, 406, 25800, 264, 3387, 300, 286, 992, 2059, 13, 51314, 51314, 407, 498, 452, 5633, 307, 281, 2524, 337, 341, 3435, 11, 293, 286, 733, 295, 1713, 264, 3435, 538, 257, 1326, 23300, 11, 51614, 51614, 452, 2063, 307, 257, 1326, 23300, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06157533327738444, "compression_ratio": 1.7073170731707317, "no_speech_prob": 2.8393867978593335e-05}, {"id": 539, "seek": 294606, "start": 2946.06, "end": 2948.06, "text": " If I grab it, the cost is zero.", "tokens": [50364, 759, 286, 4444, 309, 11, 264, 2063, 307, 4018, 13, 50464, 50464, 759, 286, 1713, 309, 538, 257, 688, 11, 264, 2063, 307, 2946, 13, 50564, 50564, 2264, 11, 300, 576, 312, 364, 1365, 295, 257, 2063, 13, 50764, 50764, 823, 11, 2264, 11, 370, 456, 307, 257, 1230, 295, 819, 721, 291, 393, 360, 365, 341, 1333, 295, 3875, 2316, 295, 13232, 9461, 13, 51264, 51264, 407, 264, 700, 472, 307, 291, 722, 490, 364, 5883, 1785, 300, 291, 11441, 294, 264, 1002, 13, 51514, 51514, 509, 1190, 428, 2128, 2316, 13, 51614, 51614, 509, 976, 257, 11494, 337, 257, 8310, 295, 5909, 13, 51764, 51764, 509, 3481, 264, 2063, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06658108892111943, "compression_ratio": 1.6612244897959183, "no_speech_prob": 2.9748609449598007e-05}, {"id": 540, "seek": 294606, "start": 2948.06, "end": 2950.06, "text": " If I miss it by a lot, the cost is higher.", "tokens": [50364, 759, 286, 4444, 309, 11, 264, 2063, 307, 4018, 13, 50464, 50464, 759, 286, 1713, 309, 538, 257, 688, 11, 264, 2063, 307, 2946, 13, 50564, 50564, 2264, 11, 300, 576, 312, 364, 1365, 295, 257, 2063, 13, 50764, 50764, 823, 11, 2264, 11, 370, 456, 307, 257, 1230, 295, 819, 721, 291, 393, 360, 365, 341, 1333, 295, 3875, 2316, 295, 13232, 9461, 13, 51264, 51264, 407, 264, 700, 472, 307, 291, 722, 490, 364, 5883, 1785, 300, 291, 11441, 294, 264, 1002, 13, 51514, 51514, 509, 1190, 428, 2128, 2316, 13, 51614, 51614, 509, 976, 257, 11494, 337, 257, 8310, 295, 5909, 13, 51764, 51764, 509, 3481, 264, 2063, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06658108892111943, "compression_ratio": 1.6612244897959183, "no_speech_prob": 2.9748609449598007e-05}, {"id": 541, "seek": 294606, "start": 2950.06, "end": 2954.06, "text": " OK, that would be an example of a cost.", "tokens": [50364, 759, 286, 4444, 309, 11, 264, 2063, 307, 4018, 13, 50464, 50464, 759, 286, 1713, 309, 538, 257, 688, 11, 264, 2063, 307, 2946, 13, 50564, 50564, 2264, 11, 300, 576, 312, 364, 1365, 295, 257, 2063, 13, 50764, 50764, 823, 11, 2264, 11, 370, 456, 307, 257, 1230, 295, 819, 721, 291, 393, 360, 365, 341, 1333, 295, 3875, 2316, 295, 13232, 9461, 13, 51264, 51264, 407, 264, 700, 472, 307, 291, 722, 490, 364, 5883, 1785, 300, 291, 11441, 294, 264, 1002, 13, 51514, 51514, 509, 1190, 428, 2128, 2316, 13, 51614, 51614, 509, 976, 257, 11494, 337, 257, 8310, 295, 5909, 13, 51764, 51764, 509, 3481, 264, 2063, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06658108892111943, "compression_ratio": 1.6612244897959183, "no_speech_prob": 2.9748609449598007e-05}, {"id": 542, "seek": 294606, "start": 2954.06, "end": 2964.06, "text": " Now, OK, so there is a number of different things you can do with this sort of basic model of intelligent agent.", "tokens": [50364, 759, 286, 4444, 309, 11, 264, 2063, 307, 4018, 13, 50464, 50464, 759, 286, 1713, 309, 538, 257, 688, 11, 264, 2063, 307, 2946, 13, 50564, 50564, 2264, 11, 300, 576, 312, 364, 1365, 295, 257, 2063, 13, 50764, 50764, 823, 11, 2264, 11, 370, 456, 307, 257, 1230, 295, 819, 721, 291, 393, 360, 365, 341, 1333, 295, 3875, 2316, 295, 13232, 9461, 13, 51264, 51264, 407, 264, 700, 472, 307, 291, 722, 490, 364, 5883, 1785, 300, 291, 11441, 294, 264, 1002, 13, 51514, 51514, 509, 1190, 428, 2128, 2316, 13, 51614, 51614, 509, 976, 257, 11494, 337, 257, 8310, 295, 5909, 13, 51764, 51764, 509, 3481, 264, 2063, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06658108892111943, "compression_ratio": 1.6612244897959183, "no_speech_prob": 2.9748609449598007e-05}, {"id": 543, "seek": 294606, "start": 2964.06, "end": 2969.06, "text": " So the first one is you start from an initial state that you observe in the world.", "tokens": [50364, 759, 286, 4444, 309, 11, 264, 2063, 307, 4018, 13, 50464, 50464, 759, 286, 1713, 309, 538, 257, 688, 11, 264, 2063, 307, 2946, 13, 50564, 50564, 2264, 11, 300, 576, 312, 364, 1365, 295, 257, 2063, 13, 50764, 50764, 823, 11, 2264, 11, 370, 456, 307, 257, 1230, 295, 819, 721, 291, 393, 360, 365, 341, 1333, 295, 3875, 2316, 295, 13232, 9461, 13, 51264, 51264, 407, 264, 700, 472, 307, 291, 722, 490, 364, 5883, 1785, 300, 291, 11441, 294, 264, 1002, 13, 51514, 51514, 509, 1190, 428, 2128, 2316, 13, 51614, 51614, 509, 976, 257, 11494, 337, 257, 8310, 295, 5909, 13, 51764, 51764, 509, 3481, 264, 2063, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06658108892111943, "compression_ratio": 1.6612244897959183, "no_speech_prob": 2.9748609449598007e-05}, {"id": 544, "seek": 294606, "start": 2969.06, "end": 2971.06, "text": " You run your forward model.", "tokens": [50364, 759, 286, 4444, 309, 11, 264, 2063, 307, 4018, 13, 50464, 50464, 759, 286, 1713, 309, 538, 257, 688, 11, 264, 2063, 307, 2946, 13, 50564, 50564, 2264, 11, 300, 576, 312, 364, 1365, 295, 257, 2063, 13, 50764, 50764, 823, 11, 2264, 11, 370, 456, 307, 257, 1230, 295, 819, 721, 291, 393, 360, 365, 341, 1333, 295, 3875, 2316, 295, 13232, 9461, 13, 51264, 51264, 407, 264, 700, 472, 307, 291, 722, 490, 364, 5883, 1785, 300, 291, 11441, 294, 264, 1002, 13, 51514, 51514, 509, 1190, 428, 2128, 2316, 13, 51614, 51614, 509, 976, 257, 11494, 337, 257, 8310, 295, 5909, 13, 51764, 51764, 509, 3481, 264, 2063, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06658108892111943, "compression_ratio": 1.6612244897959183, "no_speech_prob": 2.9748609449598007e-05}, {"id": 545, "seek": 294606, "start": 2971.06, "end": 2974.06, "text": " You give a proposal for a sequence of actions.", "tokens": [50364, 759, 286, 4444, 309, 11, 264, 2063, 307, 4018, 13, 50464, 50464, 759, 286, 1713, 309, 538, 257, 688, 11, 264, 2063, 307, 2946, 13, 50564, 50564, 2264, 11, 300, 576, 312, 364, 1365, 295, 257, 2063, 13, 50764, 50764, 823, 11, 2264, 11, 370, 456, 307, 257, 1230, 295, 819, 721, 291, 393, 360, 365, 341, 1333, 295, 3875, 2316, 295, 13232, 9461, 13, 51264, 51264, 407, 264, 700, 472, 307, 291, 722, 490, 364, 5883, 1785, 300, 291, 11441, 294, 264, 1002, 13, 51514, 51514, 509, 1190, 428, 2128, 2316, 13, 51614, 51614, 509, 976, 257, 11494, 337, 257, 8310, 295, 5909, 13, 51764, 51764, 509, 3481, 264, 2063, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06658108892111943, "compression_ratio": 1.6612244897959183, "no_speech_prob": 2.9748609449598007e-05}, {"id": 546, "seek": 294606, "start": 2974.06, "end": 2975.06, "text": " You measure the cost.", "tokens": [50364, 759, 286, 4444, 309, 11, 264, 2063, 307, 4018, 13, 50464, 50464, 759, 286, 1713, 309, 538, 257, 688, 11, 264, 2063, 307, 2946, 13, 50564, 50564, 2264, 11, 300, 576, 312, 364, 1365, 295, 257, 2063, 13, 50764, 50764, 823, 11, 2264, 11, 370, 456, 307, 257, 1230, 295, 819, 721, 291, 393, 360, 365, 341, 1333, 295, 3875, 2316, 295, 13232, 9461, 13, 51264, 51264, 407, 264, 700, 472, 307, 291, 722, 490, 364, 5883, 1785, 300, 291, 11441, 294, 264, 1002, 13, 51514, 51514, 509, 1190, 428, 2128, 2316, 13, 51614, 51614, 509, 976, 257, 11494, 337, 257, 8310, 295, 5909, 13, 51764, 51764, 509, 3481, 264, 2063, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06658108892111943, "compression_ratio": 1.6612244897959183, "no_speech_prob": 2.9748609449598007e-05}, {"id": 547, "seek": 297506, "start": 2975.06, "end": 2983.06, "text": " What you can do here, ignoring the P here, which represents a policy, let's imagine it doesn't exist.", "tokens": [50364, 708, 291, 393, 360, 510, 11, 26258, 264, 430, 510, 11, 597, 8855, 257, 3897, 11, 718, 311, 3811, 309, 1177, 380, 2514, 13, 50764, 50764, 3146, 16235, 23475, 420, 538, 512, 1333, 295, 19618, 9284, 11, 50964, 50964, 291, 727, 853, 281, 915, 257, 8310, 295, 5909, 300, 486, 17522, 264, 4787, 2063, 670, 264, 21512, 13, 51264, 51264, 286, 722, 490, 257, 1785, 13, 51364, 51364, 286, 1190, 452, 2128, 2316, 13, 51564, 51564, 400, 309, 2516, 364, 3069, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09024870118429494, "compression_ratio": 1.5022421524663676, "no_speech_prob": 1.2996358236705419e-05}, {"id": 548, "seek": 297506, "start": 2983.06, "end": 2987.06, "text": " By gradient descent or by some sort of optimization algorithm,", "tokens": [50364, 708, 291, 393, 360, 510, 11, 26258, 264, 430, 510, 11, 597, 8855, 257, 3897, 11, 718, 311, 3811, 309, 1177, 380, 2514, 13, 50764, 50764, 3146, 16235, 23475, 420, 538, 512, 1333, 295, 19618, 9284, 11, 50964, 50964, 291, 727, 853, 281, 915, 257, 8310, 295, 5909, 300, 486, 17522, 264, 4787, 2063, 670, 264, 21512, 13, 51264, 51264, 286, 722, 490, 257, 1785, 13, 51364, 51364, 286, 1190, 452, 2128, 2316, 13, 51564, 51564, 400, 309, 2516, 364, 3069, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09024870118429494, "compression_ratio": 1.5022421524663676, "no_speech_prob": 1.2996358236705419e-05}, {"id": 549, "seek": 297506, "start": 2987.06, "end": 2993.06, "text": " you could try to find a sequence of actions that will minimize the overall cost over the trajectory.", "tokens": [50364, 708, 291, 393, 360, 510, 11, 26258, 264, 430, 510, 11, 597, 8855, 257, 3897, 11, 718, 311, 3811, 309, 1177, 380, 2514, 13, 50764, 50764, 3146, 16235, 23475, 420, 538, 512, 1333, 295, 19618, 9284, 11, 50964, 50964, 291, 727, 853, 281, 915, 257, 8310, 295, 5909, 300, 486, 17522, 264, 4787, 2063, 670, 264, 21512, 13, 51264, 51264, 286, 722, 490, 257, 1785, 13, 51364, 51364, 286, 1190, 452, 2128, 2316, 13, 51564, 51564, 400, 309, 2516, 364, 3069, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09024870118429494, "compression_ratio": 1.5022421524663676, "no_speech_prob": 1.2996358236705419e-05}, {"id": 550, "seek": 297506, "start": 2993.06, "end": 2995.06, "text": " I start from a state.", "tokens": [50364, 708, 291, 393, 360, 510, 11, 26258, 264, 430, 510, 11, 597, 8855, 257, 3897, 11, 718, 311, 3811, 309, 1177, 380, 2514, 13, 50764, 50764, 3146, 16235, 23475, 420, 538, 512, 1333, 295, 19618, 9284, 11, 50964, 50964, 291, 727, 853, 281, 915, 257, 8310, 295, 5909, 300, 486, 17522, 264, 4787, 2063, 670, 264, 21512, 13, 51264, 51264, 286, 722, 490, 257, 1785, 13, 51364, 51364, 286, 1190, 452, 2128, 2316, 13, 51564, 51564, 400, 309, 2516, 364, 3069, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09024870118429494, "compression_ratio": 1.5022421524663676, "no_speech_prob": 1.2996358236705419e-05}, {"id": 551, "seek": 297506, "start": 2995.06, "end": 2999.06, "text": " I run my forward model.", "tokens": [50364, 708, 291, 393, 360, 510, 11, 26258, 264, 430, 510, 11, 597, 8855, 257, 3897, 11, 718, 311, 3811, 309, 1177, 380, 2514, 13, 50764, 50764, 3146, 16235, 23475, 420, 538, 512, 1333, 295, 19618, 9284, 11, 50964, 50964, 291, 727, 853, 281, 915, 257, 8310, 295, 5909, 300, 486, 17522, 264, 4787, 2063, 670, 264, 21512, 13, 51264, 51264, 286, 722, 490, 257, 1785, 13, 51364, 51364, 286, 1190, 452, 2128, 2316, 13, 51564, 51564, 400, 309, 2516, 364, 3069, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09024870118429494, "compression_ratio": 1.5022421524663676, "no_speech_prob": 1.2996358236705419e-05}, {"id": 552, "seek": 297506, "start": 2999.06, "end": 3003.06, "text": " And it takes an action.", "tokens": [50364, 708, 291, 393, 360, 510, 11, 26258, 264, 430, 510, 11, 597, 8855, 257, 3897, 11, 718, 311, 3811, 309, 1177, 380, 2514, 13, 50764, 50764, 3146, 16235, 23475, 420, 538, 512, 1333, 295, 19618, 9284, 11, 50964, 50964, 291, 727, 853, 281, 915, 257, 8310, 295, 5909, 300, 486, 17522, 264, 4787, 2063, 670, 264, 21512, 13, 51264, 51264, 286, 722, 490, 257, 1785, 13, 51364, 51364, 286, 1190, 452, 2128, 2316, 13, 51564, 51564, 400, 309, 2516, 364, 3069, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09024870118429494, "compression_ratio": 1.5022421524663676, "no_speech_prob": 1.2996358236705419e-05}, {"id": 553, "seek": 300306, "start": 3003.06, "end": 3005.06, "text": " OK, let me just call this A1.", "tokens": [50364, 2264, 11, 718, 385, 445, 818, 341, 316, 16, 13, 50464, 50464, 639, 307, 318, 16, 13, 50664, 50664, 400, 341, 307, 516, 281, 976, 385, 318, 17, 13, 50864, 50864, 400, 286, 478, 516, 281, 3481, 264, 2063, 295, 318, 17, 807, 512, 2063, 2445, 11, 383, 13, 51364, 51364, 2264, 11, 264, 958, 565, 1823, 11, 2614, 452, 2128, 2316, 797, 11, 652, 364, 3069, 11494, 316, 17, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1350049591064453, "compression_ratio": 1.3795180722891567, "no_speech_prob": 5.093336312711472e-06}, {"id": 554, "seek": 300306, "start": 3005.06, "end": 3009.06, "text": " This is S1.", "tokens": [50364, 2264, 11, 718, 385, 445, 818, 341, 316, 16, 13, 50464, 50464, 639, 307, 318, 16, 13, 50664, 50664, 400, 341, 307, 516, 281, 976, 385, 318, 17, 13, 50864, 50864, 400, 286, 478, 516, 281, 3481, 264, 2063, 295, 318, 17, 807, 512, 2063, 2445, 11, 383, 13, 51364, 51364, 2264, 11, 264, 958, 565, 1823, 11, 2614, 452, 2128, 2316, 797, 11, 652, 364, 3069, 11494, 316, 17, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1350049591064453, "compression_ratio": 1.3795180722891567, "no_speech_prob": 5.093336312711472e-06}, {"id": 555, "seek": 300306, "start": 3009.06, "end": 3013.06, "text": " And this is going to give me S2.", "tokens": [50364, 2264, 11, 718, 385, 445, 818, 341, 316, 16, 13, 50464, 50464, 639, 307, 318, 16, 13, 50664, 50664, 400, 341, 307, 516, 281, 976, 385, 318, 17, 13, 50864, 50864, 400, 286, 478, 516, 281, 3481, 264, 2063, 295, 318, 17, 807, 512, 2063, 2445, 11, 383, 13, 51364, 51364, 2264, 11, 264, 958, 565, 1823, 11, 2614, 452, 2128, 2316, 797, 11, 652, 364, 3069, 11494, 316, 17, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1350049591064453, "compression_ratio": 1.3795180722891567, "no_speech_prob": 5.093336312711472e-06}, {"id": 556, "seek": 300306, "start": 3013.06, "end": 3023.06, "text": " And I'm going to measure the cost of S2 through some cost function, C.", "tokens": [50364, 2264, 11, 718, 385, 445, 818, 341, 316, 16, 13, 50464, 50464, 639, 307, 318, 16, 13, 50664, 50664, 400, 341, 307, 516, 281, 976, 385, 318, 17, 13, 50864, 50864, 400, 286, 478, 516, 281, 3481, 264, 2063, 295, 318, 17, 807, 512, 2063, 2445, 11, 383, 13, 51364, 51364, 2264, 11, 264, 958, 565, 1823, 11, 2614, 452, 2128, 2316, 797, 11, 652, 364, 3069, 11494, 316, 17, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1350049591064453, "compression_ratio": 1.3795180722891567, "no_speech_prob": 5.093336312711472e-06}, {"id": 557, "seek": 300306, "start": 3023.06, "end": 3032.06, "text": " OK, the next time step, running my forward model again, make an action proposal A2.", "tokens": [50364, 2264, 11, 718, 385, 445, 818, 341, 316, 16, 13, 50464, 50464, 639, 307, 318, 16, 13, 50664, 50664, 400, 341, 307, 516, 281, 976, 385, 318, 17, 13, 50864, 50864, 400, 286, 478, 516, 281, 3481, 264, 2063, 295, 318, 17, 807, 512, 2063, 2445, 11, 383, 13, 51364, 51364, 2264, 11, 264, 958, 565, 1823, 11, 2614, 452, 2128, 2316, 797, 11, 652, 364, 3069, 11494, 316, 17, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1350049591064453, "compression_ratio": 1.3795180722891567, "no_speech_prob": 5.093336312711472e-06}, {"id": 558, "seek": 303206, "start": 3032.06, "end": 3033.06, "text": " This is all simulated.", "tokens": [50364, 639, 307, 439, 41713, 13, 50414, 50414, 639, 307, 439, 294, 452, 1378, 11, 558, 30, 50464, 50464, 1436, 341, 2316, 11, 341, 2128, 2316, 307, 294, 452, 1378, 13, 50614, 50614, 467, 311, 294, 452, 34647, 33312, 13, 50764, 50764, 407, 286, 478, 406, 767, 884, 341, 294, 264, 1002, 11, 1030, 11458, 11, 558, 30, 51114, 51114, 407, 286, 393, 12266, 341, 337, 257, 1326, 565, 4439, 13, 51364, 51364, 3950, 565, 4439, 393, 312, 34184, 498, 286, 1969, 9530, 13, 51514, 51514, 814, 393, 312, 3949, 498, 286, 1969, 1090, 1496, 5909, 13, 51714, 51714, 814, 393, 312, 2496, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08856481926463475, "compression_ratio": 1.7666666666666666, "no_speech_prob": 2.076243436022196e-05}, {"id": 559, "seek": 303206, "start": 3033.06, "end": 3034.06, "text": " This is all in my head, right?", "tokens": [50364, 639, 307, 439, 41713, 13, 50414, 50414, 639, 307, 439, 294, 452, 1378, 11, 558, 30, 50464, 50464, 1436, 341, 2316, 11, 341, 2128, 2316, 307, 294, 452, 1378, 13, 50614, 50614, 467, 311, 294, 452, 34647, 33312, 13, 50764, 50764, 407, 286, 478, 406, 767, 884, 341, 294, 264, 1002, 11, 1030, 11458, 11, 558, 30, 51114, 51114, 407, 286, 393, 12266, 341, 337, 257, 1326, 565, 4439, 13, 51364, 51364, 3950, 565, 4439, 393, 312, 34184, 498, 286, 1969, 9530, 13, 51514, 51514, 814, 393, 312, 3949, 498, 286, 1969, 1090, 1496, 5909, 13, 51714, 51714, 814, 393, 312, 2496, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08856481926463475, "compression_ratio": 1.7666666666666666, "no_speech_prob": 2.076243436022196e-05}, {"id": 560, "seek": 303206, "start": 3034.06, "end": 3037.06, "text": " Because this model, this forward model is in my head.", "tokens": [50364, 639, 307, 439, 41713, 13, 50414, 50414, 639, 307, 439, 294, 452, 1378, 11, 558, 30, 50464, 50464, 1436, 341, 2316, 11, 341, 2128, 2316, 307, 294, 452, 1378, 13, 50614, 50614, 467, 311, 294, 452, 34647, 33312, 13, 50764, 50764, 407, 286, 478, 406, 767, 884, 341, 294, 264, 1002, 11, 1030, 11458, 11, 558, 30, 51114, 51114, 407, 286, 393, 12266, 341, 337, 257, 1326, 565, 4439, 13, 51364, 51364, 3950, 565, 4439, 393, 312, 34184, 498, 286, 1969, 9530, 13, 51514, 51514, 814, 393, 312, 3949, 498, 286, 1969, 1090, 1496, 5909, 13, 51714, 51714, 814, 393, 312, 2496, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08856481926463475, "compression_ratio": 1.7666666666666666, "no_speech_prob": 2.076243436022196e-05}, {"id": 561, "seek": 303206, "start": 3037.06, "end": 3040.06, "text": " It's in my frontal cortex.", "tokens": [50364, 639, 307, 439, 41713, 13, 50414, 50414, 639, 307, 439, 294, 452, 1378, 11, 558, 30, 50464, 50464, 1436, 341, 2316, 11, 341, 2128, 2316, 307, 294, 452, 1378, 13, 50614, 50614, 467, 311, 294, 452, 34647, 33312, 13, 50764, 50764, 407, 286, 478, 406, 767, 884, 341, 294, 264, 1002, 11, 1030, 11458, 11, 558, 30, 51114, 51114, 407, 286, 393, 12266, 341, 337, 257, 1326, 565, 4439, 13, 51364, 51364, 3950, 565, 4439, 393, 312, 34184, 498, 286, 1969, 9530, 13, 51514, 51514, 814, 393, 312, 3949, 498, 286, 1969, 1090, 1496, 5909, 13, 51714, 51714, 814, 393, 312, 2496, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08856481926463475, "compression_ratio": 1.7666666666666666, "no_speech_prob": 2.076243436022196e-05}, {"id": 562, "seek": 303206, "start": 3040.06, "end": 3047.06, "text": " So I'm not actually doing this in the world, et cetera, right?", "tokens": [50364, 639, 307, 439, 41713, 13, 50414, 50414, 639, 307, 439, 294, 452, 1378, 11, 558, 30, 50464, 50464, 1436, 341, 2316, 11, 341, 2128, 2316, 307, 294, 452, 1378, 13, 50614, 50614, 467, 311, 294, 452, 34647, 33312, 13, 50764, 50764, 407, 286, 478, 406, 767, 884, 341, 294, 264, 1002, 11, 1030, 11458, 11, 558, 30, 51114, 51114, 407, 286, 393, 12266, 341, 337, 257, 1326, 565, 4439, 13, 51364, 51364, 3950, 565, 4439, 393, 312, 34184, 498, 286, 1969, 9530, 13, 51514, 51514, 814, 393, 312, 3949, 498, 286, 1969, 1090, 1496, 5909, 13, 51714, 51714, 814, 393, 312, 2496, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08856481926463475, "compression_ratio": 1.7666666666666666, "no_speech_prob": 2.076243436022196e-05}, {"id": 563, "seek": 303206, "start": 3047.06, "end": 3052.06, "text": " So I can enroll this for a few time steps.", "tokens": [50364, 639, 307, 439, 41713, 13, 50414, 50414, 639, 307, 439, 294, 452, 1378, 11, 558, 30, 50464, 50464, 1436, 341, 2316, 11, 341, 2128, 2316, 307, 294, 452, 1378, 13, 50614, 50614, 467, 311, 294, 452, 34647, 33312, 13, 50764, 50764, 407, 286, 478, 406, 767, 884, 341, 294, 264, 1002, 11, 1030, 11458, 11, 558, 30, 51114, 51114, 407, 286, 393, 12266, 341, 337, 257, 1326, 565, 4439, 13, 51364, 51364, 3950, 565, 4439, 393, 312, 34184, 498, 286, 1969, 9530, 13, 51514, 51514, 814, 393, 312, 3949, 498, 286, 1969, 1090, 1496, 5909, 13, 51714, 51714, 814, 393, 312, 2496, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08856481926463475, "compression_ratio": 1.7666666666666666, "no_speech_prob": 2.076243436022196e-05}, {"id": 564, "seek": 303206, "start": 3052.06, "end": 3055.06, "text": " Those time steps can be milliseconds if I control muscles.", "tokens": [50364, 639, 307, 439, 41713, 13, 50414, 50414, 639, 307, 439, 294, 452, 1378, 11, 558, 30, 50464, 50464, 1436, 341, 2316, 11, 341, 2128, 2316, 307, 294, 452, 1378, 13, 50614, 50614, 467, 311, 294, 452, 34647, 33312, 13, 50764, 50764, 407, 286, 478, 406, 767, 884, 341, 294, 264, 1002, 11, 1030, 11458, 11, 558, 30, 51114, 51114, 407, 286, 393, 12266, 341, 337, 257, 1326, 565, 4439, 13, 51364, 51364, 3950, 565, 4439, 393, 312, 34184, 498, 286, 1969, 9530, 13, 51514, 51514, 814, 393, 312, 3949, 498, 286, 1969, 1090, 1496, 5909, 13, 51714, 51714, 814, 393, 312, 2496, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08856481926463475, "compression_ratio": 1.7666666666666666, "no_speech_prob": 2.076243436022196e-05}, {"id": 565, "seek": 303206, "start": 3055.06, "end": 3059.06, "text": " They can be seconds if I control high level actions.", "tokens": [50364, 639, 307, 439, 41713, 13, 50414, 50414, 639, 307, 439, 294, 452, 1378, 11, 558, 30, 50464, 50464, 1436, 341, 2316, 11, 341, 2128, 2316, 307, 294, 452, 1378, 13, 50614, 50614, 467, 311, 294, 452, 34647, 33312, 13, 50764, 50764, 407, 286, 478, 406, 767, 884, 341, 294, 264, 1002, 11, 1030, 11458, 11, 558, 30, 51114, 51114, 407, 286, 393, 12266, 341, 337, 257, 1326, 565, 4439, 13, 51364, 51364, 3950, 565, 4439, 393, 312, 34184, 498, 286, 1969, 9530, 13, 51514, 51514, 814, 393, 312, 3949, 498, 286, 1969, 1090, 1496, 5909, 13, 51714, 51714, 814, 393, 312, 2496, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08856481926463475, "compression_ratio": 1.7666666666666666, "no_speech_prob": 2.076243436022196e-05}, {"id": 566, "seek": 303206, "start": 3059.06, "end": 3060.06, "text": " They can be hours.", "tokens": [50364, 639, 307, 439, 41713, 13, 50414, 50414, 639, 307, 439, 294, 452, 1378, 11, 558, 30, 50464, 50464, 1436, 341, 2316, 11, 341, 2128, 2316, 307, 294, 452, 1378, 13, 50614, 50614, 467, 311, 294, 452, 34647, 33312, 13, 50764, 50764, 407, 286, 478, 406, 767, 884, 341, 294, 264, 1002, 11, 1030, 11458, 11, 558, 30, 51114, 51114, 407, 286, 393, 12266, 341, 337, 257, 1326, 565, 4439, 13, 51364, 51364, 3950, 565, 4439, 393, 312, 34184, 498, 286, 1969, 9530, 13, 51514, 51514, 814, 393, 312, 3949, 498, 286, 1969, 1090, 1496, 5909, 13, 51714, 51714, 814, 393, 312, 2496, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08856481926463475, "compression_ratio": 1.7666666666666666, "no_speech_prob": 2.076243436022196e-05}, {"id": 567, "seek": 306006, "start": 3060.06, "end": 3070.06, "text": " So if I want to plan how to, I don't know, go to San Francisco, I need to get to the airports and then catch a plane.", "tokens": [50364, 407, 498, 286, 528, 281, 1393, 577, 281, 11, 286, 500, 380, 458, 11, 352, 281, 5271, 12279, 11, 286, 643, 281, 483, 281, 264, 36561, 293, 550, 3745, 257, 5720, 13, 50864, 50864, 400, 550, 562, 286, 8881, 456, 11, 3745, 257, 18984, 420, 746, 11, 1030, 11458, 13, 51214, 51214, 2264, 11, 370, 341, 307, 6695, 295, 264, 1496, 295, 3855, 295, 264, 551, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09507291417726328, "compression_ratio": 1.5119047619047619, "no_speech_prob": 3.812185605056584e-05}, {"id": 568, "seek": 306006, "start": 3070.06, "end": 3077.06, "text": " And then when I arrive there, catch a taxi or something, et cetera.", "tokens": [50364, 407, 498, 286, 528, 281, 1393, 577, 281, 11, 286, 500, 380, 458, 11, 352, 281, 5271, 12279, 11, 286, 643, 281, 483, 281, 264, 36561, 293, 550, 3745, 257, 5720, 13, 50864, 50864, 400, 550, 562, 286, 8881, 456, 11, 3745, 257, 18984, 420, 746, 11, 1030, 11458, 13, 51214, 51214, 2264, 11, 370, 341, 307, 6695, 295, 264, 1496, 295, 3855, 295, 264, 551, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09507291417726328, "compression_ratio": 1.5119047619047619, "no_speech_prob": 3.812185605056584e-05}, {"id": 569, "seek": 306006, "start": 3077.06, "end": 3084.06, "text": " OK, so this is independent of the level of description of the thing.", "tokens": [50364, 407, 498, 286, 528, 281, 1393, 577, 281, 11, 286, 500, 380, 458, 11, 352, 281, 5271, 12279, 11, 286, 643, 281, 483, 281, 264, 36561, 293, 550, 3745, 257, 5720, 13, 50864, 50864, 400, 550, 562, 286, 8881, 456, 11, 3745, 257, 18984, 420, 746, 11, 1030, 11458, 13, 51214, 51214, 2264, 11, 370, 341, 307, 6695, 295, 264, 1496, 295, 3855, 295, 264, 551, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09507291417726328, "compression_ratio": 1.5119047619047619, "no_speech_prob": 3.812185605056584e-05}, {"id": 570, "seek": 308406, "start": 3084.06, "end": 3096.06, "text": " OK, so what I can do with this is I can do a very classical method called model predictive control.", "tokens": [50364, 2264, 11, 370, 437, 286, 393, 360, 365, 341, 307, 286, 393, 360, 257, 588, 13735, 3170, 1219, 2316, 35521, 1969, 13, 50964, 50964, 407, 309, 311, 257, 13735, 3170, 295, 16252, 1969, 11, 597, 307, 257, 1379, 13635, 300, 575, 668, 926, 1670, 264, 2625, 82, 11, 498, 406, 3071, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.041896168674741475, "compression_ratio": 1.4487179487179487, "no_speech_prob": 4.8844609409570694e-05}, {"id": 571, "seek": 308406, "start": 3096.06, "end": 3110.06, "text": " So it's a classical method of optimal control, which is a whole discipline that has been around since the 50s, if not earlier.", "tokens": [50364, 2264, 11, 370, 437, 286, 393, 360, 365, 341, 307, 286, 393, 360, 257, 588, 13735, 3170, 1219, 2316, 35521, 1969, 13, 50964, 50964, 407, 309, 311, 257, 13735, 3170, 295, 16252, 1969, 11, 597, 307, 257, 1379, 13635, 300, 575, 668, 926, 1670, 264, 2625, 82, 11, 498, 406, 3071, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.041896168674741475, "compression_ratio": 1.4487179487179487, "no_speech_prob": 4.8844609409570694e-05}, {"id": 572, "seek": 311006, "start": 3110.06, "end": 3114.06, "text": " And some of the methods or method predictive controls go back to the 1960s.", "tokens": [50364, 400, 512, 295, 264, 7150, 420, 3170, 35521, 9003, 352, 646, 281, 264, 16157, 82, 13, 50564, 50564, 821, 307, 746, 1219, 264, 12345, 12, 33, 470, 3953, 9284, 13, 50764, 50764, 286, 519, 309, 311, 12345, 365, 364, 462, 11, 286, 478, 406, 988, 13, 51114, 51114, 407, 341, 307, 257, 3170, 588, 2531, 281, 264, 472, 286, 478, 16141, 412, 264, 1623, 13, 51464, 51464, 400, 341, 390, 1143, 10029, 538, 12077, 11, 718, 311, 584, 11, 281, 14722, 18257, 2083, 337, 28361, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1125232908460829, "compression_ratio": 1.4649122807017543, "no_speech_prob": 1.5153075764828827e-05}, {"id": 573, "seek": 311006, "start": 3114.06, "end": 3118.06, "text": " There is something called the Kelly-Brierson algorithm.", "tokens": [50364, 400, 512, 295, 264, 7150, 420, 3170, 35521, 9003, 352, 646, 281, 264, 16157, 82, 13, 50564, 50564, 821, 307, 746, 1219, 264, 12345, 12, 33, 470, 3953, 9284, 13, 50764, 50764, 286, 519, 309, 311, 12345, 365, 364, 462, 11, 286, 478, 406, 988, 13, 51114, 51114, 407, 341, 307, 257, 3170, 588, 2531, 281, 264, 472, 286, 478, 16141, 412, 264, 1623, 13, 51464, 51464, 400, 341, 390, 1143, 10029, 538, 12077, 11, 718, 311, 584, 11, 281, 14722, 18257, 2083, 337, 28361, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1125232908460829, "compression_ratio": 1.4649122807017543, "no_speech_prob": 1.5153075764828827e-05}, {"id": 574, "seek": 311006, "start": 3118.06, "end": 3125.06, "text": " I think it's Kelly with an E, I'm not sure.", "tokens": [50364, 400, 512, 295, 264, 7150, 420, 3170, 35521, 9003, 352, 646, 281, 264, 16157, 82, 13, 50564, 50564, 821, 307, 746, 1219, 264, 12345, 12, 33, 470, 3953, 9284, 13, 50764, 50764, 286, 519, 309, 311, 12345, 365, 364, 462, 11, 286, 478, 406, 988, 13, 51114, 51114, 407, 341, 307, 257, 3170, 588, 2531, 281, 264, 472, 286, 478, 16141, 412, 264, 1623, 13, 51464, 51464, 400, 341, 390, 1143, 10029, 538, 12077, 11, 718, 311, 584, 11, 281, 14722, 18257, 2083, 337, 28361, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1125232908460829, "compression_ratio": 1.4649122807017543, "no_speech_prob": 1.5153075764828827e-05}, {"id": 575, "seek": 311006, "start": 3125.06, "end": 3132.06, "text": " So this is a method very similar to the one I'm describing at the moment.", "tokens": [50364, 400, 512, 295, 264, 7150, 420, 3170, 35521, 9003, 352, 646, 281, 264, 16157, 82, 13, 50564, 50564, 821, 307, 746, 1219, 264, 12345, 12, 33, 470, 3953, 9284, 13, 50764, 50764, 286, 519, 309, 311, 12345, 365, 364, 462, 11, 286, 478, 406, 988, 13, 51114, 51114, 407, 341, 307, 257, 3170, 588, 2531, 281, 264, 472, 286, 478, 16141, 412, 264, 1623, 13, 51464, 51464, 400, 341, 390, 1143, 10029, 538, 12077, 11, 718, 311, 584, 11, 281, 14722, 18257, 2083, 337, 28361, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1125232908460829, "compression_ratio": 1.4649122807017543, "no_speech_prob": 1.5153075764828827e-05}, {"id": 576, "seek": 311006, "start": 3132.06, "end": 3138.06, "text": " And this was used primarily by NASA, let's say, to compute trajectories for rockets.", "tokens": [50364, 400, 512, 295, 264, 7150, 420, 3170, 35521, 9003, 352, 646, 281, 264, 16157, 82, 13, 50564, 50564, 821, 307, 746, 1219, 264, 12345, 12, 33, 470, 3953, 9284, 13, 50764, 50764, 286, 519, 309, 311, 12345, 365, 364, 462, 11, 286, 478, 406, 988, 13, 51114, 51114, 407, 341, 307, 257, 3170, 588, 2531, 281, 264, 472, 286, 478, 16141, 412, 264, 1623, 13, 51464, 51464, 400, 341, 390, 1143, 10029, 538, 12077, 11, 718, 311, 584, 11, 281, 14722, 18257, 2083, 337, 28361, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1125232908460829, "compression_ratio": 1.4649122807017543, "no_speech_prob": 1.5153075764828827e-05}, {"id": 577, "seek": 313806, "start": 3138.06, "end": 3148.06, "text": " So when they started having computers in the 60s at NASA, they started computing trajectories with computers and they were basically using things like this.", "tokens": [50364, 407, 562, 436, 1409, 1419, 10807, 294, 264, 4060, 82, 412, 12077, 11, 436, 1409, 15866, 18257, 2083, 365, 10807, 293, 436, 645, 1936, 1228, 721, 411, 341, 13, 50864, 50864, 4546, 300, 11, 436, 632, 281, 360, 309, 538, 1011, 13, 51064, 51064, 400, 498, 291, 2378, 380, 1612, 264, 3169, 41156, 22443, 1303, 11, 309, 15626, 577, 561, 645, 15866, 341, 538, 1011, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07021898542131697, "compression_ratio": 1.5904255319148937, "no_speech_prob": 3.6980214645154774e-05}, {"id": 578, "seek": 313806, "start": 3148.06, "end": 3152.06, "text": " Before that, they had to do it by hand.", "tokens": [50364, 407, 562, 436, 1409, 1419, 10807, 294, 264, 4060, 82, 412, 12077, 11, 436, 1409, 15866, 18257, 2083, 365, 10807, 293, 436, 645, 1936, 1228, 721, 411, 341, 13, 50864, 50864, 4546, 300, 11, 436, 632, 281, 360, 309, 538, 1011, 13, 51064, 51064, 400, 498, 291, 2378, 380, 1612, 264, 3169, 41156, 22443, 1303, 11, 309, 15626, 577, 561, 645, 15866, 341, 538, 1011, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07021898542131697, "compression_ratio": 1.5904255319148937, "no_speech_prob": 3.6980214645154774e-05}, {"id": 579, "seek": 313806, "start": 3152.06, "end": 3159.06, "text": " And if you haven't seen the movie Hidden Figures, it describes how people were computing this by hand.", "tokens": [50364, 407, 562, 436, 1409, 1419, 10807, 294, 264, 4060, 82, 412, 12077, 11, 436, 1409, 15866, 18257, 2083, 365, 10807, 293, 436, 645, 1936, 1228, 721, 411, 341, 13, 50864, 50864, 4546, 300, 11, 436, 632, 281, 360, 309, 538, 1011, 13, 51064, 51064, 400, 498, 291, 2378, 380, 1612, 264, 3169, 41156, 22443, 1303, 11, 309, 15626, 577, 561, 645, 15866, 341, 538, 1011, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07021898542131697, "compression_ratio": 1.5904255319148937, "no_speech_prob": 3.6980214645154774e-05}, {"id": 580, "seek": 315906, "start": 3159.06, "end": 3169.06, "text": " This was mostly done by black women, black mathematicians, women mathematicians, who also ended up kind of programming those computers.", "tokens": [50364, 639, 390, 5240, 1096, 538, 2211, 2266, 11, 2211, 32811, 2567, 11, 2266, 32811, 2567, 11, 567, 611, 4590, 493, 733, 295, 9410, 729, 10807, 13, 50864, 50864, 7277, 300, 3169, 11, 309, 311, 534, 869, 13, 51014, 51014, 2264, 11, 370, 510, 307, 257, 3875, 1558, 510, 13, 51164, 51164, 639, 1542, 588, 709, 411, 257, 18680, 1753, 2533, 11, 570, 428, 2128, 2316, 307, 1936, 264, 912, 3209, 46365, 670, 565, 13, 51514, 51514, 400, 341, 307, 411, 364, 517, 13217, 18680, 1753, 3209, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0894047349363893, "compression_ratio": 1.6025641025641026, "no_speech_prob": 3.82197649742011e-05}, {"id": 581, "seek": 315906, "start": 3169.06, "end": 3172.06, "text": " Watch that movie, it's really great.", "tokens": [50364, 639, 390, 5240, 1096, 538, 2211, 2266, 11, 2211, 32811, 2567, 11, 2266, 32811, 2567, 11, 567, 611, 4590, 493, 733, 295, 9410, 729, 10807, 13, 50864, 50864, 7277, 300, 3169, 11, 309, 311, 534, 869, 13, 51014, 51014, 2264, 11, 370, 510, 307, 257, 3875, 1558, 510, 13, 51164, 51164, 639, 1542, 588, 709, 411, 257, 18680, 1753, 2533, 11, 570, 428, 2128, 2316, 307, 1936, 264, 912, 3209, 46365, 670, 565, 13, 51514, 51514, 400, 341, 307, 411, 364, 517, 13217, 18680, 1753, 3209, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0894047349363893, "compression_ratio": 1.6025641025641026, "no_speech_prob": 3.82197649742011e-05}, {"id": 582, "seek": 315906, "start": 3172.06, "end": 3175.06, "text": " OK, so here is a basic idea here.", "tokens": [50364, 639, 390, 5240, 1096, 538, 2211, 2266, 11, 2211, 32811, 2567, 11, 2266, 32811, 2567, 11, 567, 611, 4590, 493, 733, 295, 9410, 729, 10807, 13, 50864, 50864, 7277, 300, 3169, 11, 309, 311, 534, 869, 13, 51014, 51014, 2264, 11, 370, 510, 307, 257, 3875, 1558, 510, 13, 51164, 51164, 639, 1542, 588, 709, 411, 257, 18680, 1753, 2533, 11, 570, 428, 2128, 2316, 307, 1936, 264, 912, 3209, 46365, 670, 565, 13, 51514, 51514, 400, 341, 307, 411, 364, 517, 13217, 18680, 1753, 3209, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0894047349363893, "compression_ratio": 1.6025641025641026, "no_speech_prob": 3.82197649742011e-05}, {"id": 583, "seek": 315906, "start": 3175.06, "end": 3182.06, "text": " This looks very much like a recurrent net, because your forward model is basically the same network replicated over time.", "tokens": [50364, 639, 390, 5240, 1096, 538, 2211, 2266, 11, 2211, 32811, 2567, 11, 2266, 32811, 2567, 11, 567, 611, 4590, 493, 733, 295, 9410, 729, 10807, 13, 50864, 50864, 7277, 300, 3169, 11, 309, 311, 534, 869, 13, 51014, 51014, 2264, 11, 370, 510, 307, 257, 3875, 1558, 510, 13, 51164, 51164, 639, 1542, 588, 709, 411, 257, 18680, 1753, 2533, 11, 570, 428, 2128, 2316, 307, 1936, 264, 912, 3209, 46365, 670, 565, 13, 51514, 51514, 400, 341, 307, 411, 364, 517, 13217, 18680, 1753, 3209, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0894047349363893, "compression_ratio": 1.6025641025641026, "no_speech_prob": 3.82197649742011e-05}, {"id": 584, "seek": 315906, "start": 3182.06, "end": 3185.06, "text": " And this is like an unworld recurrent network.", "tokens": [50364, 639, 390, 5240, 1096, 538, 2211, 2266, 11, 2211, 32811, 2567, 11, 2266, 32811, 2567, 11, 567, 611, 4590, 493, 733, 295, 9410, 729, 10807, 13, 50864, 50864, 7277, 300, 3169, 11, 309, 311, 534, 869, 13, 51014, 51014, 2264, 11, 370, 510, 307, 257, 3875, 1558, 510, 13, 51164, 51164, 639, 1542, 588, 709, 411, 257, 18680, 1753, 2533, 11, 570, 428, 2128, 2316, 307, 1936, 264, 912, 3209, 46365, 670, 565, 13, 51514, 51514, 400, 341, 307, 411, 364, 517, 13217, 18680, 1753, 3209, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0894047349363893, "compression_ratio": 1.6025641025641026, "no_speech_prob": 3.82197649742011e-05}, {"id": 585, "seek": 318506, "start": 3185.06, "end": 3196.06, "text": " And so what you do here is you back propagate the value of the cost through this entire network all the way to the actions.", "tokens": [50364, 400, 370, 437, 291, 360, 510, 307, 291, 646, 48256, 264, 2158, 295, 264, 2063, 807, 341, 2302, 3209, 439, 264, 636, 281, 264, 5909, 13, 50914, 50914, 400, 291, 500, 380, 764, 341, 337, 3097, 11, 291, 764, 341, 337, 38253, 13, 51114, 51114, 509, 519, 295, 264, 5909, 382, 48994, 9102, 13, 51264, 51264, 400, 291, 1936, 538, 16235, 23475, 420, 512, 661, 19618, 3170, 11, 291, 915, 257, 8310, 295, 5909, 300, 486, 17522, 264, 2408, 295, 264, 2063, 670, 264, 21512, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08097731802198622, "compression_ratio": 1.7008547008547008, "no_speech_prob": 1.8619342881720513e-05}, {"id": 586, "seek": 318506, "start": 3196.06, "end": 3200.06, "text": " And you don't use this for training, you use this for inference.", "tokens": [50364, 400, 370, 437, 291, 360, 510, 307, 291, 646, 48256, 264, 2158, 295, 264, 2063, 807, 341, 2302, 3209, 439, 264, 636, 281, 264, 5909, 13, 50914, 50914, 400, 291, 500, 380, 764, 341, 337, 3097, 11, 291, 764, 341, 337, 38253, 13, 51114, 51114, 509, 519, 295, 264, 5909, 382, 48994, 9102, 13, 51264, 51264, 400, 291, 1936, 538, 16235, 23475, 420, 512, 661, 19618, 3170, 11, 291, 915, 257, 8310, 295, 5909, 300, 486, 17522, 264, 2408, 295, 264, 2063, 670, 264, 21512, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08097731802198622, "compression_ratio": 1.7008547008547008, "no_speech_prob": 1.8619342881720513e-05}, {"id": 587, "seek": 318506, "start": 3200.06, "end": 3203.06, "text": " You think of the actions as latent variables.", "tokens": [50364, 400, 370, 437, 291, 360, 510, 307, 291, 646, 48256, 264, 2158, 295, 264, 2063, 807, 341, 2302, 3209, 439, 264, 636, 281, 264, 5909, 13, 50914, 50914, 400, 291, 500, 380, 764, 341, 337, 3097, 11, 291, 764, 341, 337, 38253, 13, 51114, 51114, 509, 519, 295, 264, 5909, 382, 48994, 9102, 13, 51264, 51264, 400, 291, 1936, 538, 16235, 23475, 420, 512, 661, 19618, 3170, 11, 291, 915, 257, 8310, 295, 5909, 300, 486, 17522, 264, 2408, 295, 264, 2063, 670, 264, 21512, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08097731802198622, "compression_ratio": 1.7008547008547008, "no_speech_prob": 1.8619342881720513e-05}, {"id": 588, "seek": 318506, "start": 3203.06, "end": 3212.06, "text": " And you basically by gradient descent or some other optimization method, you find a sequence of actions that will minimize the sum of the cost over the trajectory.", "tokens": [50364, 400, 370, 437, 291, 360, 510, 307, 291, 646, 48256, 264, 2158, 295, 264, 2063, 807, 341, 2302, 3209, 439, 264, 636, 281, 264, 5909, 13, 50914, 50914, 400, 291, 500, 380, 764, 341, 337, 3097, 11, 291, 764, 341, 337, 38253, 13, 51114, 51114, 509, 519, 295, 264, 5909, 382, 48994, 9102, 13, 51264, 51264, 400, 291, 1936, 538, 16235, 23475, 420, 512, 661, 19618, 3170, 11, 291, 915, 257, 8310, 295, 5909, 300, 486, 17522, 264, 2408, 295, 264, 2063, 670, 264, 21512, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08097731802198622, "compression_ratio": 1.7008547008547008, "no_speech_prob": 1.8619342881720513e-05}, {"id": 589, "seek": 321206, "start": 3212.06, "end": 3224.06, "text": " OK, so basically you have an overall cost.", "tokens": [50364, 2264, 11, 370, 1936, 291, 362, 364, 4787, 2063, 13, 50964, 50964, 286, 478, 516, 281, 818, 309, 955, 383, 11, 293, 300, 311, 516, 281, 312, 264, 2408, 670, 565, 4439, 295, 264, 707, 269, 295, 262, 256, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.23448200659318405, "compression_ratio": 1.2702702702702702, "no_speech_prob": 5.253401013760595e-06}, {"id": 590, "seek": 321206, "start": 3224.06, "end": 3233.06, "text": " I'm going to call it big C, and that's going to be the sum over time steps of the little c of s t.", "tokens": [50364, 2264, 11, 370, 1936, 291, 362, 364, 4787, 2063, 13, 50964, 50964, 286, 478, 516, 281, 818, 309, 955, 383, 11, 293, 300, 311, 516, 281, 312, 264, 2408, 670, 565, 4439, 295, 264, 707, 269, 295, 262, 256, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.23448200659318405, "compression_ratio": 1.2702702702702702, "no_speech_prob": 5.253401013760595e-06}, {"id": 591, "seek": 323306, "start": 3233.06, "end": 3252.06, "text": " OK, and what you're going to do is big A, which is the sequence of A, is going to be replaced by its own value minus some step size times the gradient of big C with respect to A.", "tokens": [50364, 2264, 11, 293, 437, 291, 434, 516, 281, 360, 307, 955, 316, 11, 597, 307, 264, 8310, 295, 316, 11, 307, 516, 281, 312, 10772, 538, 1080, 1065, 2158, 3175, 512, 1823, 2744, 1413, 264, 16235, 295, 955, 383, 365, 3104, 281, 316, 13, 51314, 51314, 2264, 11, 370, 382, 938, 382, 291, 393, 14722, 264, 16235, 295, 264, 2408, 295, 729, 5497, 670, 264, 21512, 365, 3104, 281, 439, 295, 264, 6677, 295, 316, 11, 597, 1355, 264, 18257, 2083, 295, 316, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09882989796725186, "compression_ratio": 1.7156862745098038, "no_speech_prob": 2.0764013243024237e-05}, {"id": 592, "seek": 323306, "start": 3252.06, "end": 3261.06, "text": " OK, so as long as you can compute the gradient of the sum of those costs over the trajectory with respect to all of the components of A, which means the trajectories of A,", "tokens": [50364, 2264, 11, 293, 437, 291, 434, 516, 281, 360, 307, 955, 316, 11, 597, 307, 264, 8310, 295, 316, 11, 307, 516, 281, 312, 10772, 538, 1080, 1065, 2158, 3175, 512, 1823, 2744, 1413, 264, 16235, 295, 955, 383, 365, 3104, 281, 316, 13, 51314, 51314, 2264, 11, 370, 382, 938, 382, 291, 393, 14722, 264, 16235, 295, 264, 2408, 295, 729, 5497, 670, 264, 21512, 365, 3104, 281, 439, 295, 264, 6677, 295, 316, 11, 597, 1355, 264, 18257, 2083, 295, 316, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09882989796725186, "compression_ratio": 1.7156862745098038, "no_speech_prob": 2.0764013243024237e-05}, {"id": 593, "seek": 326106, "start": 3261.06, "end": 3265.06, "text": " you can do this optimization. You don't have to do it necessarily through gradient descent.", "tokens": [50364, 291, 393, 360, 341, 19618, 13, 509, 500, 380, 362, 281, 360, 309, 4725, 807, 16235, 23475, 13, 50564, 50564, 682, 512, 3331, 11, 456, 366, 544, 7148, 2098, 281, 360, 341, 19618, 1228, 8546, 9410, 13, 50814, 50814, 1171, 1365, 11, 498, 316, 307, 27706, 11, 300, 1062, 312, 544, 7148, 13, 51014, 51014, 583, 498, 316, 307, 10957, 293, 1090, 18795, 11, 291, 1936, 362, 572, 3922, 457, 281, 764, 16235, 2361, 7150, 13, 51264, 51264, 2264, 11, 370, 341, 307, 38253, 13, 821, 311, 572, 2539, 1939, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.10722350070351049, "compression_ratio": 1.6126482213438735, "no_speech_prob": 1.2377330676827114e-05}, {"id": 594, "seek": 326106, "start": 3265.06, "end": 3270.06, "text": " In some cases, there are more efficient ways to do this optimization using dynamic programming.", "tokens": [50364, 291, 393, 360, 341, 19618, 13, 509, 500, 380, 362, 281, 360, 309, 4725, 807, 16235, 23475, 13, 50564, 50564, 682, 512, 3331, 11, 456, 366, 544, 7148, 2098, 281, 360, 341, 19618, 1228, 8546, 9410, 13, 50814, 50814, 1171, 1365, 11, 498, 316, 307, 27706, 11, 300, 1062, 312, 544, 7148, 13, 51014, 51014, 583, 498, 316, 307, 10957, 293, 1090, 18795, 11, 291, 1936, 362, 572, 3922, 457, 281, 764, 16235, 2361, 7150, 13, 51264, 51264, 2264, 11, 370, 341, 307, 38253, 13, 821, 311, 572, 2539, 1939, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.10722350070351049, "compression_ratio": 1.6126482213438735, "no_speech_prob": 1.2377330676827114e-05}, {"id": 595, "seek": 326106, "start": 3270.06, "end": 3274.06, "text": " For example, if A is discrete, that might be more efficient.", "tokens": [50364, 291, 393, 360, 341, 19618, 13, 509, 500, 380, 362, 281, 360, 309, 4725, 807, 16235, 23475, 13, 50564, 50564, 682, 512, 3331, 11, 456, 366, 544, 7148, 2098, 281, 360, 341, 19618, 1228, 8546, 9410, 13, 50814, 50814, 1171, 1365, 11, 498, 316, 307, 27706, 11, 300, 1062, 312, 544, 7148, 13, 51014, 51014, 583, 498, 316, 307, 10957, 293, 1090, 18795, 11, 291, 1936, 362, 572, 3922, 457, 281, 764, 16235, 2361, 7150, 13, 51264, 51264, 2264, 11, 370, 341, 307, 38253, 13, 821, 311, 572, 2539, 1939, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.10722350070351049, "compression_ratio": 1.6126482213438735, "no_speech_prob": 1.2377330676827114e-05}, {"id": 596, "seek": 326106, "start": 3274.06, "end": 3279.06, "text": " But if A is continuous and high dimensional, you basically have no choice but to use gradient based methods.", "tokens": [50364, 291, 393, 360, 341, 19618, 13, 509, 500, 380, 362, 281, 360, 309, 4725, 807, 16235, 23475, 13, 50564, 50564, 682, 512, 3331, 11, 456, 366, 544, 7148, 2098, 281, 360, 341, 19618, 1228, 8546, 9410, 13, 50814, 50814, 1171, 1365, 11, 498, 316, 307, 27706, 11, 300, 1062, 312, 544, 7148, 13, 51014, 51014, 583, 498, 316, 307, 10957, 293, 1090, 18795, 11, 291, 1936, 362, 572, 3922, 457, 281, 764, 16235, 2361, 7150, 13, 51264, 51264, 2264, 11, 370, 341, 307, 38253, 13, 821, 311, 572, 2539, 1939, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.10722350070351049, "compression_ratio": 1.6126482213438735, "no_speech_prob": 1.2377330676827114e-05}, {"id": 597, "seek": 326106, "start": 3279.06, "end": 3282.06, "text": " OK, so this is inference. There's no learning yet.", "tokens": [50364, 291, 393, 360, 341, 19618, 13, 509, 500, 380, 362, 281, 360, 309, 4725, 807, 16235, 23475, 13, 50564, 50564, 682, 512, 3331, 11, 456, 366, 544, 7148, 2098, 281, 360, 341, 19618, 1228, 8546, 9410, 13, 50814, 50814, 1171, 1365, 11, 498, 316, 307, 27706, 11, 300, 1062, 312, 544, 7148, 13, 51014, 51014, 583, 498, 316, 307, 10957, 293, 1090, 18795, 11, 291, 1936, 362, 572, 3922, 457, 281, 764, 16235, 2361, 7150, 13, 51264, 51264, 2264, 11, 370, 341, 307, 38253, 13, 821, 311, 572, 2539, 1939, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.10722350070351049, "compression_ratio": 1.6126482213438735, "no_speech_prob": 1.2377330676827114e-05}, {"id": 598, "seek": 328206, "start": 3282.06, "end": 3291.06, "text": " What is A? Big A is the sequence A1, A2, A3, et cetera.", "tokens": [50364, 708, 307, 316, 30, 5429, 316, 307, 264, 8310, 316, 16, 11, 316, 17, 11, 316, 18, 11, 1030, 11458, 13, 50814, 50814, 2264, 11, 370, 291, 362, 257, 819, 9364, 10024, 2445, 293, 291, 393, 17522, 309, 365, 3104, 281, 264, 9102, 291, 434, 3102, 294, 13, 51164, 51164, 407, 437, 360, 291, 483, 484, 295, 341, 30, 821, 366, 572, 17443, 294, 316, 13, 316, 307, 257, 8062, 11, 558, 30, 51364, 51364, 407, 316, 307, 257, 8062, 11, 1338, 13, 865, 11, 370, 300, 390, 767, 570, 321, 1128, 764, 321, 1128, 17522, 18875, 370, 1400, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13036482150738055, "compression_ratio": 1.5879828326180256, "no_speech_prob": 4.831216574530117e-05}, {"id": 599, "seek": 328206, "start": 3291.06, "end": 3298.06, "text": " OK, so you have a differentiable objective function and you can minimize it with respect to the variables you're interested in.", "tokens": [50364, 708, 307, 316, 30, 5429, 316, 307, 264, 8310, 316, 16, 11, 316, 17, 11, 316, 18, 11, 1030, 11458, 13, 50814, 50814, 2264, 11, 370, 291, 362, 257, 819, 9364, 10024, 2445, 293, 291, 393, 17522, 309, 365, 3104, 281, 264, 9102, 291, 434, 3102, 294, 13, 51164, 51164, 407, 437, 360, 291, 483, 484, 295, 341, 30, 821, 366, 572, 17443, 294, 316, 13, 316, 307, 257, 8062, 11, 558, 30, 51364, 51364, 407, 316, 307, 257, 8062, 11, 1338, 13, 865, 11, 370, 300, 390, 767, 570, 321, 1128, 764, 321, 1128, 17522, 18875, 370, 1400, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13036482150738055, "compression_ratio": 1.5879828326180256, "no_speech_prob": 4.831216574530117e-05}, {"id": 600, "seek": 328206, "start": 3298.06, "end": 3302.06, "text": " So what do you get out of this? There are no weights in A. A is a vector, right?", "tokens": [50364, 708, 307, 316, 30, 5429, 316, 307, 264, 8310, 316, 16, 11, 316, 17, 11, 316, 18, 11, 1030, 11458, 13, 50814, 50814, 2264, 11, 370, 291, 362, 257, 819, 9364, 10024, 2445, 293, 291, 393, 17522, 309, 365, 3104, 281, 264, 9102, 291, 434, 3102, 294, 13, 51164, 51164, 407, 437, 360, 291, 483, 484, 295, 341, 30, 821, 366, 572, 17443, 294, 316, 13, 316, 307, 257, 8062, 11, 558, 30, 51364, 51364, 407, 316, 307, 257, 8062, 11, 1338, 13, 865, 11, 370, 300, 390, 767, 570, 321, 1128, 764, 321, 1128, 17522, 18875, 370, 1400, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13036482150738055, "compression_ratio": 1.5879828326180256, "no_speech_prob": 4.831216574530117e-05}, {"id": 601, "seek": 328206, "start": 3302.06, "end": 3308.06, "text": " So A is a vector, yeah. Yeah, so that was actually because we never use we never minimize vectors so far.", "tokens": [50364, 708, 307, 316, 30, 5429, 316, 307, 264, 8310, 316, 16, 11, 316, 17, 11, 316, 18, 11, 1030, 11458, 13, 50814, 50814, 2264, 11, 370, 291, 362, 257, 819, 9364, 10024, 2445, 293, 291, 393, 17522, 309, 365, 3104, 281, 264, 9102, 291, 434, 3102, 294, 13, 51164, 51164, 407, 437, 360, 291, 483, 484, 295, 341, 30, 821, 366, 572, 17443, 294, 316, 13, 316, 307, 257, 8062, 11, 558, 30, 51364, 51364, 407, 316, 307, 257, 8062, 11, 1338, 13, 865, 11, 370, 300, 390, 767, 570, 321, 1128, 764, 321, 1128, 17522, 18875, 370, 1400, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13036482150738055, "compression_ratio": 1.5879828326180256, "no_speech_prob": 4.831216574530117e-05}, {"id": 602, "seek": 330806, "start": 3308.06, "end": 3312.06, "text": " We've always been optimizing weights so people are confused.", "tokens": [50364, 492, 600, 1009, 668, 40425, 17443, 370, 561, 366, 9019, 13, 50564, 50564, 876, 11, 321, 362, 13, 1171, 48994, 9102, 411, 264, 710, 9102, 11, 264, 48994, 9102, 295, 264, 2281, 2361, 5245, 11, 50864, 50864, 264, 48994, 9102, 11, 321, 360, 17522, 264, 2281, 365, 3104, 281, 710, 13, 407, 341, 307, 264, 912, 1154, 510, 321, 434, 12606, 13, 51114, 51114, 286, 519, 11, 1338, 11, 286, 519, 406, 1518, 7320, 300, 264, 48994, 9102, 366, 767, 15743, 13, 51364, 51364, 407, 300, 390, 11, 286, 519, 11, 411, 257, 29227, 365, 264, 1168, 321, 632, 322, 430, 654, 26786, 466, 3097, 613, 48994, 7006, 5245, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.13887758422316165, "compression_ratio": 1.8518518518518519, "no_speech_prob": 0.00014639426080975682}, {"id": 603, "seek": 330806, "start": 3312.06, "end": 3318.06, "text": " Oh, we have. For latent variables like the z variables, the latent variables of the energy based models,", "tokens": [50364, 492, 600, 1009, 668, 40425, 17443, 370, 561, 366, 9019, 13, 50564, 50564, 876, 11, 321, 362, 13, 1171, 48994, 9102, 411, 264, 710, 9102, 11, 264, 48994, 9102, 295, 264, 2281, 2361, 5245, 11, 50864, 50864, 264, 48994, 9102, 11, 321, 360, 17522, 264, 2281, 365, 3104, 281, 710, 13, 407, 341, 307, 264, 912, 1154, 510, 321, 434, 12606, 13, 51114, 51114, 286, 519, 11, 1338, 11, 286, 519, 406, 1518, 7320, 300, 264, 48994, 9102, 366, 767, 15743, 13, 51364, 51364, 407, 300, 390, 11, 286, 519, 11, 411, 257, 29227, 365, 264, 1168, 321, 632, 322, 430, 654, 26786, 466, 3097, 613, 48994, 7006, 5245, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.13887758422316165, "compression_ratio": 1.8518518518518519, "no_speech_prob": 0.00014639426080975682}, {"id": 604, "seek": 330806, "start": 3318.06, "end": 3323.06, "text": " the latent variables, we do minimize the energy with respect to z. So this is the same problem here we're solving.", "tokens": [50364, 492, 600, 1009, 668, 40425, 17443, 370, 561, 366, 9019, 13, 50564, 50564, 876, 11, 321, 362, 13, 1171, 48994, 9102, 411, 264, 710, 9102, 11, 264, 48994, 9102, 295, 264, 2281, 2361, 5245, 11, 50864, 50864, 264, 48994, 9102, 11, 321, 360, 17522, 264, 2281, 365, 3104, 281, 710, 13, 407, 341, 307, 264, 912, 1154, 510, 321, 434, 12606, 13, 51114, 51114, 286, 519, 11, 1338, 11, 286, 519, 406, 1518, 7320, 300, 264, 48994, 9102, 366, 767, 15743, 13, 51364, 51364, 407, 300, 390, 11, 286, 519, 11, 411, 257, 29227, 365, 264, 1168, 321, 632, 322, 430, 654, 26786, 466, 3097, 613, 48994, 7006, 5245, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.13887758422316165, "compression_ratio": 1.8518518518518519, "no_speech_prob": 0.00014639426080975682}, {"id": 605, "seek": 330806, "start": 3323.06, "end": 3328.06, "text": " I think, yeah, I think not everyone understood that the latent variables are actually inputs.", "tokens": [50364, 492, 600, 1009, 668, 40425, 17443, 370, 561, 366, 9019, 13, 50564, 50564, 876, 11, 321, 362, 13, 1171, 48994, 9102, 411, 264, 710, 9102, 11, 264, 48994, 9102, 295, 264, 2281, 2361, 5245, 11, 50864, 50864, 264, 48994, 9102, 11, 321, 360, 17522, 264, 2281, 365, 3104, 281, 710, 13, 407, 341, 307, 264, 912, 1154, 510, 321, 434, 12606, 13, 51114, 51114, 286, 519, 11, 1338, 11, 286, 519, 406, 1518, 7320, 300, 264, 48994, 9102, 366, 767, 15743, 13, 51364, 51364, 407, 300, 390, 11, 286, 519, 11, 411, 257, 29227, 365, 264, 1168, 321, 632, 322, 430, 654, 26786, 466, 3097, 613, 48994, 7006, 5245, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.13887758422316165, "compression_ratio": 1.8518518518518519, "no_speech_prob": 0.00014639426080975682}, {"id": 606, "seek": 330806, "start": 3328.06, "end": 3337.06, "text": " So that was, I think, like a misunderstanding with the question we had on Piazza about training these latent variable models.", "tokens": [50364, 492, 600, 1009, 668, 40425, 17443, 370, 561, 366, 9019, 13, 50564, 50564, 876, 11, 321, 362, 13, 1171, 48994, 9102, 411, 264, 710, 9102, 11, 264, 48994, 9102, 295, 264, 2281, 2361, 5245, 11, 50864, 50864, 264, 48994, 9102, 11, 321, 360, 17522, 264, 2281, 365, 3104, 281, 710, 13, 407, 341, 307, 264, 912, 1154, 510, 321, 434, 12606, 13, 51114, 51114, 286, 519, 11, 1338, 11, 286, 519, 406, 1518, 7320, 300, 264, 48994, 9102, 366, 767, 15743, 13, 51364, 51364, 407, 300, 390, 11, 286, 519, 11, 411, 257, 29227, 365, 264, 1168, 321, 632, 322, 430, 654, 26786, 466, 3097, 613, 48994, 7006, 5245, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.13887758422316165, "compression_ratio": 1.8518518518518519, "no_speech_prob": 0.00014639426080975682}, {"id": 607, "seek": 333706, "start": 3337.06, "end": 3345.06, "text": " You don't want to use the word training for latent variables or for things like this because you want to use inference.", "tokens": [50364, 509, 500, 380, 528, 281, 764, 264, 1349, 3097, 337, 48994, 9102, 420, 337, 721, 411, 341, 570, 291, 528, 281, 764, 38253, 13, 50764, 50764, 2264, 11, 291, 500, 380, 764, 264, 1349, 281, 13596, 420, 406, 281, 3847, 13, 286, 528, 281, 764, 264, 1349, 38253, 11, 406, 3097, 13, 51114, 51114, 708, 311, 264, 2649, 1296, 38253, 293, 3097, 30, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.10125185838386194, "compression_ratio": 1.7908496732026145, "no_speech_prob": 4.425161478138762e-06}, {"id": 608, "seek": 333706, "start": 3345.06, "end": 3352.06, "text": " OK, you don't use the word to infer or not to train. I want to use the word inference, not training.", "tokens": [50364, 509, 500, 380, 528, 281, 764, 264, 1349, 3097, 337, 48994, 9102, 420, 337, 721, 411, 341, 570, 291, 528, 281, 764, 38253, 13, 50764, 50764, 2264, 11, 291, 500, 380, 764, 264, 1349, 281, 13596, 420, 406, 281, 3847, 13, 286, 528, 281, 764, 264, 1349, 38253, 11, 406, 3097, 13, 51114, 51114, 708, 311, 264, 2649, 1296, 38253, 293, 3097, 30, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.10125185838386194, "compression_ratio": 1.7908496732026145, "no_speech_prob": 4.425161478138762e-06}, {"id": 609, "seek": 333706, "start": 3352.06, "end": 3355.06, "text": " What's the difference between inference and training?", "tokens": [50364, 509, 500, 380, 528, 281, 764, 264, 1349, 3097, 337, 48994, 9102, 420, 337, 721, 411, 341, 570, 291, 528, 281, 764, 38253, 13, 50764, 50764, 2264, 11, 291, 500, 380, 764, 264, 1349, 281, 13596, 420, 406, 281, 3847, 13, 286, 528, 281, 764, 264, 1349, 38253, 11, 406, 3097, 13, 51114, 51114, 708, 311, 264, 2649, 1296, 38253, 293, 3097, 30, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.10125185838386194, "compression_ratio": 1.7908496732026145, "no_speech_prob": 4.425161478138762e-06}, {"id": 610, "seek": 335506, "start": 3355.06, "end": 3367.06, "text": " Training, with training, you learn a parameter that is the same for a large number of samples.", "tokens": [50364, 20620, 11, 365, 3097, 11, 291, 1466, 257, 13075, 300, 307, 264, 912, 337, 257, 2416, 1230, 295, 10938, 13, 50964, 50964, 1171, 38253, 11, 291, 915, 264, 2158, 295, 512, 7006, 11, 257, 48994, 7006, 11, 316, 294, 341, 1389, 11, 710, 294, 264, 1389, 295, 257, 48994, 7006, 2281, 2361, 2316, 51514, 51514, 300, 307, 2685, 281, 472, 6889, 13, 509, 1319, 264, 6889, 11, 264, 48994, 7006, 2962, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09740939265803288, "compression_ratio": 1.7431693989071038, "no_speech_prob": 3.6322704545455053e-05}, {"id": 611, "seek": 335506, "start": 3367.06, "end": 3378.06, "text": " For inference, you find the value of some variable, a latent variable, A in this case, z in the case of a latent variable energy based model", "tokens": [50364, 20620, 11, 365, 3097, 11, 291, 1466, 257, 13075, 300, 307, 264, 912, 337, 257, 2416, 1230, 295, 10938, 13, 50964, 50964, 1171, 38253, 11, 291, 915, 264, 2158, 295, 512, 7006, 11, 257, 48994, 7006, 11, 316, 294, 341, 1389, 11, 710, 294, 264, 1389, 295, 257, 48994, 7006, 2281, 2361, 2316, 51514, 51514, 300, 307, 2685, 281, 472, 6889, 13, 509, 1319, 264, 6889, 11, 264, 48994, 7006, 2962, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09740939265803288, "compression_ratio": 1.7431693989071038, "no_speech_prob": 3.6322704545455053e-05}, {"id": 612, "seek": 335506, "start": 3378.06, "end": 3383.06, "text": " that is specific to one sample. You change the sample, the latent variable changes.", "tokens": [50364, 20620, 11, 365, 3097, 11, 291, 1466, 257, 13075, 300, 307, 264, 912, 337, 257, 2416, 1230, 295, 10938, 13, 50964, 50964, 1171, 38253, 11, 291, 915, 264, 2158, 295, 512, 7006, 11, 257, 48994, 7006, 11, 316, 294, 341, 1389, 11, 710, 294, 264, 1389, 295, 257, 48994, 7006, 2281, 2361, 2316, 51514, 51514, 300, 307, 2685, 281, 472, 6889, 13, 509, 1319, 264, 6889, 11, 264, 48994, 7006, 2962, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09740939265803288, "compression_ratio": 1.7431693989071038, "no_speech_prob": 3.6322704545455053e-05}, {"id": 613, "seek": 338306, "start": 3383.06, "end": 3388.06, "text": " So you don't learn it because you don't remember it from one time to the next.", "tokens": [50364, 407, 291, 500, 380, 1466, 309, 570, 291, 500, 380, 1604, 309, 490, 472, 565, 281, 264, 958, 13, 50614, 50614, 821, 311, 572, 4675, 337, 309, 11, 558, 30, 50914, 50914, 407, 300, 311, 264, 2649, 13, 47482, 671, 11, 291, 434, 884, 264, 912, 733, 295, 6916, 689, 291, 360, 2539, 293, 38253, 13, 51164, 51164, 400, 370, 412, 512, 1496, 295, 37765, 11, 436, 366, 264, 912, 13, 583, 38253, 11, 291, 360, 309, 680, 6889, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11192844027564638, "compression_ratio": 1.6130653266331658, "no_speech_prob": 1.5188275028776843e-05}, {"id": 614, "seek": 338306, "start": 3388.06, "end": 3394.06, "text": " There's no memory for it, right?", "tokens": [50364, 407, 291, 500, 380, 1466, 309, 570, 291, 500, 380, 1604, 309, 490, 472, 565, 281, 264, 958, 13, 50614, 50614, 821, 311, 572, 4675, 337, 309, 11, 558, 30, 50914, 50914, 407, 300, 311, 264, 2649, 13, 47482, 671, 11, 291, 434, 884, 264, 912, 733, 295, 6916, 689, 291, 360, 2539, 293, 38253, 13, 51164, 51164, 400, 370, 412, 512, 1496, 295, 37765, 11, 436, 366, 264, 912, 13, 583, 38253, 11, 291, 360, 309, 680, 6889, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11192844027564638, "compression_ratio": 1.6130653266331658, "no_speech_prob": 1.5188275028776843e-05}, {"id": 615, "seek": 338306, "start": 3394.06, "end": 3399.06, "text": " So that's the difference. Conceptually, you're doing the same kind of operation where you do learning and inference.", "tokens": [50364, 407, 291, 500, 380, 1466, 309, 570, 291, 500, 380, 1604, 309, 490, 472, 565, 281, 264, 958, 13, 50614, 50614, 821, 311, 572, 4675, 337, 309, 11, 558, 30, 50914, 50914, 407, 300, 311, 264, 2649, 13, 47482, 671, 11, 291, 434, 884, 264, 912, 733, 295, 6916, 689, 291, 360, 2539, 293, 38253, 13, 51164, 51164, 400, 370, 412, 512, 1496, 295, 37765, 11, 436, 366, 264, 912, 13, 583, 38253, 11, 291, 360, 309, 680, 6889, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11192844027564638, "compression_ratio": 1.6130653266331658, "no_speech_prob": 1.5188275028776843e-05}, {"id": 616, "seek": 338306, "start": 3399.06, "end": 3406.06, "text": " And so at some level of abstraction, they are the same. But inference, you do it per sample.", "tokens": [50364, 407, 291, 500, 380, 1466, 309, 570, 291, 500, 380, 1604, 309, 490, 472, 565, 281, 264, 958, 13, 50614, 50614, 821, 311, 572, 4675, 337, 309, 11, 558, 30, 50914, 50914, 407, 300, 311, 264, 2649, 13, 47482, 671, 11, 291, 434, 884, 264, 912, 733, 295, 6916, 689, 291, 360, 2539, 293, 38253, 13, 51164, 51164, 400, 370, 412, 512, 1496, 295, 37765, 11, 436, 366, 264, 912, 13, 583, 38253, 11, 291, 360, 309, 680, 6889, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11192844027564638, "compression_ratio": 1.6130653266331658, "no_speech_prob": 1.5188275028776843e-05}, {"id": 617, "seek": 340606, "start": 3406.06, "end": 3414.06, "text": " Learning, you do it over a bunch of samples and the parameter is shared across the samples.", "tokens": [50364, 15205, 11, 291, 360, 309, 670, 257, 3840, 295, 10938, 293, 264, 13075, 307, 5507, 2108, 264, 10938, 13, 50764, 50764, 1133, 321, 362, 364, 2281, 2361, 2316, 293, 321, 1116, 411, 281, 360, 38253, 11, 321, 920, 362, 257, 4464, 2144, 281, 360, 412, 633, 565, 321, 2042, 341, 11, 321, 764, 309, 11, 558, 30, 51264, 51264, 407, 300, 390, 257, 955, 2649, 1296, 485, 51364, 51364, 2381, 291, 600, 8895, 264, 2316, 11, 562, 291, 764, 309, 11, 291, 920, 362, 281, 360, 4464, 2144, 365, 3104, 281, 264, 48994, 9102, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11562245301526002, "compression_ratio": 1.6611570247933884, "no_speech_prob": 1.4498736163659487e-05}, {"id": 618, "seek": 340606, "start": 3414.06, "end": 3424.06, "text": " When we have an energy based model and we'd like to do inference, we still have a minimization to do at every time we perform this, we use it, right?", "tokens": [50364, 15205, 11, 291, 360, 309, 670, 257, 3840, 295, 10938, 293, 264, 13075, 307, 5507, 2108, 264, 10938, 13, 50764, 50764, 1133, 321, 362, 364, 2281, 2361, 2316, 293, 321, 1116, 411, 281, 360, 38253, 11, 321, 920, 362, 257, 4464, 2144, 281, 360, 412, 633, 565, 321, 2042, 341, 11, 321, 764, 309, 11, 558, 30, 51264, 51264, 407, 300, 390, 257, 955, 2649, 1296, 485, 51364, 51364, 2381, 291, 600, 8895, 264, 2316, 11, 562, 291, 764, 309, 11, 291, 920, 362, 281, 360, 4464, 2144, 365, 3104, 281, 264, 48994, 9102, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11562245301526002, "compression_ratio": 1.6611570247933884, "no_speech_prob": 1.4498736163659487e-05}, {"id": 619, "seek": 340606, "start": 3424.06, "end": 3426.06, "text": " So that was a big difference between...", "tokens": [50364, 15205, 11, 291, 360, 309, 670, 257, 3840, 295, 10938, 293, 264, 13075, 307, 5507, 2108, 264, 10938, 13, 50764, 50764, 1133, 321, 362, 364, 2281, 2361, 2316, 293, 321, 1116, 411, 281, 360, 38253, 11, 321, 920, 362, 257, 4464, 2144, 281, 360, 412, 633, 565, 321, 2042, 341, 11, 321, 764, 309, 11, 558, 30, 51264, 51264, 407, 300, 390, 257, 955, 2649, 1296, 485, 51364, 51364, 2381, 291, 600, 8895, 264, 2316, 11, 562, 291, 764, 309, 11, 291, 920, 362, 281, 360, 4464, 2144, 365, 3104, 281, 264, 48994, 9102, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11562245301526002, "compression_ratio": 1.6611570247933884, "no_speech_prob": 1.4498736163659487e-05}, {"id": 620, "seek": 340606, "start": 3426.06, "end": 3433.06, "text": " After you've trained the model, when you use it, you still have to do minimization with respect to the latent variables.", "tokens": [50364, 15205, 11, 291, 360, 309, 670, 257, 3840, 295, 10938, 293, 264, 13075, 307, 5507, 2108, 264, 10938, 13, 50764, 50764, 1133, 321, 362, 364, 2281, 2361, 2316, 293, 321, 1116, 411, 281, 360, 38253, 11, 321, 920, 362, 257, 4464, 2144, 281, 360, 412, 633, 565, 321, 2042, 341, 11, 321, 764, 309, 11, 558, 30, 51264, 51264, 407, 300, 390, 257, 955, 2649, 1296, 485, 51364, 51364, 2381, 291, 600, 8895, 264, 2316, 11, 562, 291, 764, 309, 11, 291, 920, 362, 281, 360, 4464, 2144, 365, 3104, 281, 264, 48994, 9102, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11562245301526002, "compression_ratio": 1.6611570247933884, "no_speech_prob": 1.4498736163659487e-05}, {"id": 621, "seek": 343306, "start": 3433.06, "end": 3436.06, "text": " That's the big difference. Same here.", "tokens": [50364, 663, 311, 264, 955, 2649, 13, 10635, 510, 13, 50514, 50514, 1692, 11, 456, 815, 420, 815, 406, 312, 604, 3097, 13, 2260, 2128, 2316, 815, 312, 3094, 538, 1011, 420, 815, 312, 8895, 13, 50764, 50764, 583, 538, 264, 565, 321, 366, 510, 11, 309, 311, 8895, 13, 492, 434, 406, 3097, 1340, 510, 13, 492, 434, 445, 884, 38253, 13, 51014, 51014, 492, 434, 15213, 484, 437, 307, 264, 16252, 2158, 295, 264, 8310, 295, 316, 311, 300, 486, 17522, 527, 2063, 11, 4787, 2063, 13, 51364, 51364, 400, 341, 307, 364, 38253, 1154, 11, 445, 411, 2281, 2361, 5245, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11017730748542001, "compression_ratio": 1.6975806451612903, "no_speech_prob": 8.800398063613102e-06}, {"id": 622, "seek": 343306, "start": 3436.06, "end": 3441.06, "text": " Here, there may or may not be any training. Your forward model may be built by hand or may be trained.", "tokens": [50364, 663, 311, 264, 955, 2649, 13, 10635, 510, 13, 50514, 50514, 1692, 11, 456, 815, 420, 815, 406, 312, 604, 3097, 13, 2260, 2128, 2316, 815, 312, 3094, 538, 1011, 420, 815, 312, 8895, 13, 50764, 50764, 583, 538, 264, 565, 321, 366, 510, 11, 309, 311, 8895, 13, 492, 434, 406, 3097, 1340, 510, 13, 492, 434, 445, 884, 38253, 13, 51014, 51014, 492, 434, 15213, 484, 437, 307, 264, 16252, 2158, 295, 264, 8310, 295, 316, 311, 300, 486, 17522, 527, 2063, 11, 4787, 2063, 13, 51364, 51364, 400, 341, 307, 364, 38253, 1154, 11, 445, 411, 2281, 2361, 5245, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11017730748542001, "compression_ratio": 1.6975806451612903, "no_speech_prob": 8.800398063613102e-06}, {"id": 623, "seek": 343306, "start": 3441.06, "end": 3446.06, "text": " But by the time we are here, it's trained. We're not training anything here. We're just doing inference.", "tokens": [50364, 663, 311, 264, 955, 2649, 13, 10635, 510, 13, 50514, 50514, 1692, 11, 456, 815, 420, 815, 406, 312, 604, 3097, 13, 2260, 2128, 2316, 815, 312, 3094, 538, 1011, 420, 815, 312, 8895, 13, 50764, 50764, 583, 538, 264, 565, 321, 366, 510, 11, 309, 311, 8895, 13, 492, 434, 406, 3097, 1340, 510, 13, 492, 434, 445, 884, 38253, 13, 51014, 51014, 492, 434, 15213, 484, 437, 307, 264, 16252, 2158, 295, 264, 8310, 295, 316, 311, 300, 486, 17522, 527, 2063, 11, 4787, 2063, 13, 51364, 51364, 400, 341, 307, 364, 38253, 1154, 11, 445, 411, 2281, 2361, 5245, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11017730748542001, "compression_ratio": 1.6975806451612903, "no_speech_prob": 8.800398063613102e-06}, {"id": 624, "seek": 343306, "start": 3446.06, "end": 3453.06, "text": " We're figuring out what is the optimal value of the sequence of A's that will minimize our cost, overall cost.", "tokens": [50364, 663, 311, 264, 955, 2649, 13, 10635, 510, 13, 50514, 50514, 1692, 11, 456, 815, 420, 815, 406, 312, 604, 3097, 13, 2260, 2128, 2316, 815, 312, 3094, 538, 1011, 420, 815, 312, 8895, 13, 50764, 50764, 583, 538, 264, 565, 321, 366, 510, 11, 309, 311, 8895, 13, 492, 434, 406, 3097, 1340, 510, 13, 492, 434, 445, 884, 38253, 13, 51014, 51014, 492, 434, 15213, 484, 437, 307, 264, 16252, 2158, 295, 264, 8310, 295, 316, 311, 300, 486, 17522, 527, 2063, 11, 4787, 2063, 13, 51364, 51364, 400, 341, 307, 364, 38253, 1154, 11, 445, 411, 2281, 2361, 5245, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11017730748542001, "compression_ratio": 1.6975806451612903, "no_speech_prob": 8.800398063613102e-06}, {"id": 625, "seek": 343306, "start": 3453.06, "end": 3456.06, "text": " And this is an inference problem, just like energy based models.", "tokens": [50364, 663, 311, 264, 955, 2649, 13, 10635, 510, 13, 50514, 50514, 1692, 11, 456, 815, 420, 815, 406, 312, 604, 3097, 13, 2260, 2128, 2316, 815, 312, 3094, 538, 1011, 420, 815, 312, 8895, 13, 50764, 50764, 583, 538, 264, 565, 321, 366, 510, 11, 309, 311, 8895, 13, 492, 434, 406, 3097, 1340, 510, 13, 492, 434, 445, 884, 38253, 13, 51014, 51014, 492, 434, 15213, 484, 437, 307, 264, 16252, 2158, 295, 264, 8310, 295, 316, 311, 300, 486, 17522, 527, 2063, 11, 4787, 2063, 13, 51364, 51364, 400, 341, 307, 364, 38253, 1154, 11, 445, 411, 2281, 2361, 5245, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11017730748542001, "compression_ratio": 1.6975806451612903, "no_speech_prob": 8.800398063613102e-06}, {"id": 626, "seek": 345606, "start": 3456.06, "end": 3463.06, "text": " For example, the FM, the forward model can be just one line of equation of physics, right? It can be just a deterministic equation.", "tokens": [50364, 1171, 1365, 11, 264, 29614, 11, 264, 2128, 2316, 393, 312, 445, 472, 1622, 295, 5367, 295, 10649, 11, 558, 30, 467, 393, 312, 445, 257, 15957, 3142, 5367, 13, 50714, 50714, 407, 3811, 264, 2128, 2316, 307, 264, 1326, 11787, 300, 6786, 264, 10649, 295, 257, 13012, 13, 51064, 51064, 400, 316, 307, 1936, 264, 3069, 322, 264, 14823, 11, 577, 291, 8579, 264, 572, 4313, 904, 293, 550, 264, 24030, 13, 51514, 51514, 407, 300, 576, 312, 264, 5765, 295, 316, 11, 576, 312, 264, 5765, 295, 729, 9102, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11675785978635152, "compression_ratio": 1.7882882882882882, "no_speech_prob": 9.816043530008756e-06}, {"id": 627, "seek": 345606, "start": 3463.06, "end": 3470.06, "text": " So imagine the forward model is the few equations that describe the physics of a rocket.", "tokens": [50364, 1171, 1365, 11, 264, 29614, 11, 264, 2128, 2316, 393, 312, 445, 472, 1622, 295, 5367, 295, 10649, 11, 558, 30, 467, 393, 312, 445, 257, 15957, 3142, 5367, 13, 50714, 50714, 407, 3811, 264, 2128, 2316, 307, 264, 1326, 11787, 300, 6786, 264, 10649, 295, 257, 13012, 13, 51064, 51064, 400, 316, 307, 1936, 264, 3069, 322, 264, 14823, 11, 577, 291, 8579, 264, 572, 4313, 904, 293, 550, 264, 24030, 13, 51514, 51514, 407, 300, 576, 312, 264, 5765, 295, 316, 11, 576, 312, 264, 5765, 295, 729, 9102, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11675785978635152, "compression_ratio": 1.7882882882882882, "no_speech_prob": 9.816043530008756e-06}, {"id": 628, "seek": 345606, "start": 3470.06, "end": 3479.06, "text": " And A is basically the action on the steering, how you orient the nozzles and then the thrust.", "tokens": [50364, 1171, 1365, 11, 264, 29614, 11, 264, 2128, 2316, 393, 312, 445, 472, 1622, 295, 5367, 295, 10649, 11, 558, 30, 467, 393, 312, 445, 257, 15957, 3142, 5367, 13, 50714, 50714, 407, 3811, 264, 2128, 2316, 307, 264, 1326, 11787, 300, 6786, 264, 10649, 295, 257, 13012, 13, 51064, 51064, 400, 316, 307, 1936, 264, 3069, 322, 264, 14823, 11, 577, 291, 8579, 264, 572, 4313, 904, 293, 550, 264, 24030, 13, 51514, 51514, 407, 300, 576, 312, 264, 5765, 295, 316, 11, 576, 312, 264, 5765, 295, 729, 9102, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11675785978635152, "compression_ratio": 1.7882882882882882, "no_speech_prob": 9.816043530008756e-06}, {"id": 629, "seek": 345606, "start": 3479.06, "end": 3482.06, "text": " So that would be the collection of A, would be the collection of those variables.", "tokens": [50364, 1171, 1365, 11, 264, 29614, 11, 264, 2128, 2316, 393, 312, 445, 472, 1622, 295, 5367, 295, 10649, 11, 558, 30, 467, 393, 312, 445, 257, 15957, 3142, 5367, 13, 50714, 50714, 407, 3811, 264, 2128, 2316, 307, 264, 1326, 11787, 300, 6786, 264, 10649, 295, 257, 13012, 13, 51064, 51064, 400, 316, 307, 1936, 264, 3069, 322, 264, 14823, 11, 577, 291, 8579, 264, 572, 4313, 904, 293, 550, 264, 24030, 13, 51514, 51514, 407, 300, 576, 312, 264, 5765, 295, 316, 11, 576, 312, 264, 5765, 295, 729, 9102, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11675785978635152, "compression_ratio": 1.7882882882882882, "no_speech_prob": 9.816043530008756e-06}, {"id": 630, "seek": 348206, "start": 3482.06, "end": 3487.06, "text": " And then there is very simple physics, Newtonian physics, basically. You can write the equations.", "tokens": [50364, 400, 550, 456, 307, 588, 2199, 10649, 11, 19541, 952, 10649, 11, 1936, 13, 509, 393, 2464, 264, 11787, 13, 50614, 50614, 467, 486, 976, 291, 264, 1785, 295, 264, 13012, 412, 264, 958, 565, 1823, 382, 257, 2445, 295, 1785, 295, 264, 13012, 412, 264, 3894, 565, 1823, 293, 264, 5909, 291, 434, 1940, 13, 51014, 51014, 663, 311, 577, 291, 360, 35138, 11, 300, 311, 577, 633, 32974, 1985, 13, 51264, 51264, 400, 550, 428, 2063, 2445, 11, 498, 291, 528, 281, 3076, 257, 13012, 11, 576, 312, 1310, 257, 6562, 295, 732, 721, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09069647647366666, "compression_ratio": 1.7698744769874477, "no_speech_prob": 4.9061953177442774e-05}, {"id": 631, "seek": 348206, "start": 3487.06, "end": 3495.06, "text": " It will give you the state of the rocket at the next time step as a function of state of the rocket at the previous time step and the actions you're taking.", "tokens": [50364, 400, 550, 456, 307, 588, 2199, 10649, 11, 19541, 952, 10649, 11, 1936, 13, 509, 393, 2464, 264, 11787, 13, 50614, 50614, 467, 486, 976, 291, 264, 1785, 295, 264, 13012, 412, 264, 958, 565, 1823, 382, 257, 2445, 295, 1785, 295, 264, 13012, 412, 264, 3894, 565, 1823, 293, 264, 5909, 291, 434, 1940, 13, 51014, 51014, 663, 311, 577, 291, 360, 35138, 11, 300, 311, 577, 633, 32974, 1985, 13, 51264, 51264, 400, 550, 428, 2063, 2445, 11, 498, 291, 528, 281, 3076, 257, 13012, 11, 576, 312, 1310, 257, 6562, 295, 732, 721, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09069647647366666, "compression_ratio": 1.7698744769874477, "no_speech_prob": 4.9061953177442774e-05}, {"id": 632, "seek": 348206, "start": 3495.06, "end": 3500.06, "text": " That's how you do simulations, that's how every simulator works.", "tokens": [50364, 400, 550, 456, 307, 588, 2199, 10649, 11, 19541, 952, 10649, 11, 1936, 13, 509, 393, 2464, 264, 11787, 13, 50614, 50614, 467, 486, 976, 291, 264, 1785, 295, 264, 13012, 412, 264, 958, 565, 1823, 382, 257, 2445, 295, 1785, 295, 264, 13012, 412, 264, 3894, 565, 1823, 293, 264, 5909, 291, 434, 1940, 13, 51014, 51014, 663, 311, 577, 291, 360, 35138, 11, 300, 311, 577, 633, 32974, 1985, 13, 51264, 51264, 400, 550, 428, 2063, 2445, 11, 498, 291, 528, 281, 3076, 257, 13012, 11, 576, 312, 1310, 257, 6562, 295, 732, 721, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09069647647366666, "compression_ratio": 1.7698744769874477, "no_speech_prob": 4.9061953177442774e-05}, {"id": 633, "seek": 348206, "start": 3500.06, "end": 3505.06, "text": " And then your cost function, if you want to shoot a rocket, would be maybe a combination of two things.", "tokens": [50364, 400, 550, 456, 307, 588, 2199, 10649, 11, 19541, 952, 10649, 11, 1936, 13, 509, 393, 2464, 264, 11787, 13, 50614, 50614, 467, 486, 976, 291, 264, 1785, 295, 264, 13012, 412, 264, 958, 565, 1823, 382, 257, 2445, 295, 1785, 295, 264, 13012, 412, 264, 3894, 565, 1823, 293, 264, 5909, 291, 434, 1940, 13, 51014, 51014, 663, 311, 577, 291, 360, 35138, 11, 300, 311, 577, 633, 32974, 1985, 13, 51264, 51264, 400, 550, 428, 2063, 2445, 11, 498, 291, 528, 281, 3076, 257, 13012, 11, 576, 312, 1310, 257, 6562, 295, 732, 721, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09069647647366666, "compression_ratio": 1.7698744769874477, "no_speech_prob": 4.9061953177442774e-05}, {"id": 634, "seek": 350506, "start": 3505.06, "end": 3514.06, "text": " One would be the energy spent during that time step, the amount of fuel you spent, something like that.", "tokens": [50364, 1485, 576, 312, 264, 2281, 4418, 1830, 300, 565, 1823, 11, 264, 2372, 295, 6616, 291, 4418, 11, 746, 411, 300, 13, 50814, 50814, 400, 264, 1150, 1433, 1062, 312, 264, 4560, 281, 257, 3779, 291, 528, 281, 2524, 13, 50964, 50964, 2704, 291, 528, 281, 40026, 16514, 365, 257, 1901, 5214, 13, 51114, 51114, 400, 264, 1150, 1433, 294, 264, 2063, 576, 312, 264, 4560, 281, 264, 1901, 5214, 11, 3732, 295, 4560, 281, 264, 1901, 5214, 13, 51514, 51514, 759, 291, 3481, 264, 2408, 670, 264, 2302, 21512, 295, 264, 4560, 281, 264, 1901, 5214, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0760657927569221, "compression_ratio": 2.028169014084507, "no_speech_prob": 2.3549446268589236e-05}, {"id": 635, "seek": 350506, "start": 3514.06, "end": 3517.06, "text": " And the second term might be the distance to a target you want to reach.", "tokens": [50364, 1485, 576, 312, 264, 2281, 4418, 1830, 300, 565, 1823, 11, 264, 2372, 295, 6616, 291, 4418, 11, 746, 411, 300, 13, 50814, 50814, 400, 264, 1150, 1433, 1062, 312, 264, 4560, 281, 257, 3779, 291, 528, 281, 2524, 13, 50964, 50964, 2704, 291, 528, 281, 40026, 16514, 365, 257, 1901, 5214, 13, 51114, 51114, 400, 264, 1150, 1433, 294, 264, 2063, 576, 312, 264, 4560, 281, 264, 1901, 5214, 11, 3732, 295, 4560, 281, 264, 1901, 5214, 13, 51514, 51514, 759, 291, 3481, 264, 2408, 670, 264, 2302, 21512, 295, 264, 4560, 281, 264, 1901, 5214, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0760657927569221, "compression_ratio": 2.028169014084507, "no_speech_prob": 2.3549446268589236e-05}, {"id": 636, "seek": 350506, "start": 3517.06, "end": 3520.06, "text": " Maybe you want to rendezvous with a space station.", "tokens": [50364, 1485, 576, 312, 264, 2281, 4418, 1830, 300, 565, 1823, 11, 264, 2372, 295, 6616, 291, 4418, 11, 746, 411, 300, 13, 50814, 50814, 400, 264, 1150, 1433, 1062, 312, 264, 4560, 281, 257, 3779, 291, 528, 281, 2524, 13, 50964, 50964, 2704, 291, 528, 281, 40026, 16514, 365, 257, 1901, 5214, 13, 51114, 51114, 400, 264, 1150, 1433, 294, 264, 2063, 576, 312, 264, 4560, 281, 264, 1901, 5214, 11, 3732, 295, 4560, 281, 264, 1901, 5214, 13, 51514, 51514, 759, 291, 3481, 264, 2408, 670, 264, 2302, 21512, 295, 264, 4560, 281, 264, 1901, 5214, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0760657927569221, "compression_ratio": 2.028169014084507, "no_speech_prob": 2.3549446268589236e-05}, {"id": 637, "seek": 350506, "start": 3520.06, "end": 3528.06, "text": " And the second term in the cost would be the distance to the space station, square of distance to the space station.", "tokens": [50364, 1485, 576, 312, 264, 2281, 4418, 1830, 300, 565, 1823, 11, 264, 2372, 295, 6616, 291, 4418, 11, 746, 411, 300, 13, 50814, 50814, 400, 264, 1150, 1433, 1062, 312, 264, 4560, 281, 257, 3779, 291, 528, 281, 2524, 13, 50964, 50964, 2704, 291, 528, 281, 40026, 16514, 365, 257, 1901, 5214, 13, 51114, 51114, 400, 264, 1150, 1433, 294, 264, 2063, 576, 312, 264, 4560, 281, 264, 1901, 5214, 11, 3732, 295, 4560, 281, 264, 1901, 5214, 13, 51514, 51514, 759, 291, 3481, 264, 2408, 670, 264, 2302, 21512, 295, 264, 4560, 281, 264, 1901, 5214, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0760657927569221, "compression_ratio": 2.028169014084507, "no_speech_prob": 2.3549446268589236e-05}, {"id": 638, "seek": 350506, "start": 3528.06, "end": 3534.06, "text": " If you measure the sum over the entire trajectory of the distance to the space station,", "tokens": [50364, 1485, 576, 312, 264, 2281, 4418, 1830, 300, 565, 1823, 11, 264, 2372, 295, 6616, 291, 4418, 11, 746, 411, 300, 13, 50814, 50814, 400, 264, 1150, 1433, 1062, 312, 264, 4560, 281, 257, 3779, 291, 528, 281, 2524, 13, 50964, 50964, 2704, 291, 528, 281, 40026, 16514, 365, 257, 1901, 5214, 13, 51114, 51114, 400, 264, 1150, 1433, 294, 264, 2063, 576, 312, 264, 4560, 281, 264, 1901, 5214, 11, 3732, 295, 4560, 281, 264, 1901, 5214, 13, 51514, 51514, 759, 291, 3481, 264, 2408, 670, 264, 2302, 21512, 295, 264, 4560, 281, 264, 1901, 5214, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0760657927569221, "compression_ratio": 2.028169014084507, "no_speech_prob": 2.3549446268589236e-05}, {"id": 639, "seek": 353406, "start": 3534.06, "end": 3543.06, "text": " the system will try to minimize the time it will take to get to the space station because it won't want to minimize the sum of the square of the distances to the space station over the trajectory.", "tokens": [50364, 264, 1185, 486, 853, 281, 17522, 264, 565, 309, 486, 747, 281, 483, 281, 264, 1901, 5214, 570, 309, 1582, 380, 528, 281, 17522, 264, 2408, 295, 264, 3732, 295, 264, 22182, 281, 264, 1901, 5214, 670, 264, 21512, 13, 50814, 50814, 583, 412, 264, 912, 565, 11, 309, 2738, 281, 17522, 6616, 13, 407, 291, 362, 281, 4772, 729, 732, 2115, 13, 51014, 51014, 407, 300, 311, 257, 13735, 636, 295, 884, 16252, 1969, 13, 400, 300, 311, 1219, 2316, 35521, 1969, 13, 51264, 51264, 1119, 12655, 1601, 30822, 472, 2010, 295, 2316, 35521, 1969, 30, 51464, 51464, 883, 11, 12655, 1601, 30822, 307, 257, 1729, 2128, 2316, 11, 498, 291, 528, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08729385925551593, "compression_ratio": 1.9018867924528302, "no_speech_prob": 2.144100290024653e-05}, {"id": 640, "seek": 353406, "start": 3543.06, "end": 3547.06, "text": " But at the same time, it wants to minimize fuel. So you have to balance those two terms.", "tokens": [50364, 264, 1185, 486, 853, 281, 17522, 264, 565, 309, 486, 747, 281, 483, 281, 264, 1901, 5214, 570, 309, 1582, 380, 528, 281, 17522, 264, 2408, 295, 264, 3732, 295, 264, 22182, 281, 264, 1901, 5214, 670, 264, 21512, 13, 50814, 50814, 583, 412, 264, 912, 565, 11, 309, 2738, 281, 17522, 6616, 13, 407, 291, 362, 281, 4772, 729, 732, 2115, 13, 51014, 51014, 407, 300, 311, 257, 13735, 636, 295, 884, 16252, 1969, 13, 400, 300, 311, 1219, 2316, 35521, 1969, 13, 51264, 51264, 1119, 12655, 1601, 30822, 472, 2010, 295, 2316, 35521, 1969, 30, 51464, 51464, 883, 11, 12655, 1601, 30822, 307, 257, 1729, 2128, 2316, 11, 498, 291, 528, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08729385925551593, "compression_ratio": 1.9018867924528302, "no_speech_prob": 2.144100290024653e-05}, {"id": 641, "seek": 353406, "start": 3547.06, "end": 3552.06, "text": " So that's a classical way of doing optimal control. And that's called model predictive control.", "tokens": [50364, 264, 1185, 486, 853, 281, 17522, 264, 565, 309, 486, 747, 281, 483, 281, 264, 1901, 5214, 570, 309, 1582, 380, 528, 281, 17522, 264, 2408, 295, 264, 3732, 295, 264, 22182, 281, 264, 1901, 5214, 670, 264, 21512, 13, 50814, 50814, 583, 412, 264, 912, 565, 11, 309, 2738, 281, 17522, 6616, 13, 407, 291, 362, 281, 4772, 729, 732, 2115, 13, 51014, 51014, 407, 300, 311, 257, 13735, 636, 295, 884, 16252, 1969, 13, 400, 300, 311, 1219, 2316, 35521, 1969, 13, 51264, 51264, 1119, 12655, 1601, 30822, 472, 2010, 295, 2316, 35521, 1969, 30, 51464, 51464, 883, 11, 12655, 1601, 30822, 307, 257, 1729, 2128, 2316, 11, 498, 291, 528, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08729385925551593, "compression_ratio": 1.9018867924528302, "no_speech_prob": 2.144100290024653e-05}, {"id": 642, "seek": 353406, "start": 3552.06, "end": 3556.06, "text": " Is Kalman filtering one type of model predictive control?", "tokens": [50364, 264, 1185, 486, 853, 281, 17522, 264, 565, 309, 486, 747, 281, 483, 281, 264, 1901, 5214, 570, 309, 1582, 380, 528, 281, 17522, 264, 2408, 295, 264, 3732, 295, 264, 22182, 281, 264, 1901, 5214, 670, 264, 21512, 13, 50814, 50814, 583, 412, 264, 912, 565, 11, 309, 2738, 281, 17522, 6616, 13, 407, 291, 362, 281, 4772, 729, 732, 2115, 13, 51014, 51014, 407, 300, 311, 257, 13735, 636, 295, 884, 16252, 1969, 13, 400, 300, 311, 1219, 2316, 35521, 1969, 13, 51264, 51264, 1119, 12655, 1601, 30822, 472, 2010, 295, 2316, 35521, 1969, 30, 51464, 51464, 883, 11, 12655, 1601, 30822, 307, 257, 1729, 2128, 2316, 11, 498, 291, 528, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08729385925551593, "compression_ratio": 1.9018867924528302, "no_speech_prob": 2.144100290024653e-05}, {"id": 643, "seek": 353406, "start": 3556.06, "end": 3560.06, "text": " No, Kalman filtering is a particular forward model, if you want.", "tokens": [50364, 264, 1185, 486, 853, 281, 17522, 264, 565, 309, 486, 747, 281, 483, 281, 264, 1901, 5214, 570, 309, 1582, 380, 528, 281, 17522, 264, 2408, 295, 264, 3732, 295, 264, 22182, 281, 264, 1901, 5214, 670, 264, 21512, 13, 50814, 50814, 583, 412, 264, 912, 565, 11, 309, 2738, 281, 17522, 6616, 13, 407, 291, 362, 281, 4772, 729, 732, 2115, 13, 51014, 51014, 407, 300, 311, 257, 13735, 636, 295, 884, 16252, 1969, 13, 400, 300, 311, 1219, 2316, 35521, 1969, 13, 51264, 51264, 1119, 12655, 1601, 30822, 472, 2010, 295, 2316, 35521, 1969, 30, 51464, 51464, 883, 11, 12655, 1601, 30822, 307, 257, 1729, 2128, 2316, 11, 498, 291, 528, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08729385925551593, "compression_ratio": 1.9018867924528302, "no_speech_prob": 2.144100290024653e-05}, {"id": 644, "seek": 356006, "start": 3560.06, "end": 3564.06, "text": " It's a way of estimating the state of the world.", "tokens": [50364, 467, 311, 257, 636, 295, 8017, 990, 264, 1785, 295, 264, 1002, 13, 50564, 50564, 583, 309, 311, 1936, 2212, 428, 14816, 295, 264, 1785, 295, 264, 1002, 807, 257, 12860, 1185, 11, 456, 311, 516, 281, 312, 512, 15697, 466, 264, 1785, 295, 264, 1002, 13, 51114, 51114, 400, 264, 12655, 1601, 6608, 1936, 37808, 257, 39148, 7316, 322, 341, 15697, 13, 51364, 51364, 400, 586, 562, 291, 1190, 807, 428, 2128, 2316, 11, 291, 434, 516, 281, 362, 257, 16505, 15697, 466, 264, 1785, 295, 264, 1002, 412, 264, 958, 565, 1823, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08329453131165167, "compression_ratio": 1.9515418502202644, "no_speech_prob": 1.593410524947103e-05}, {"id": 645, "seek": 356006, "start": 3564.06, "end": 3575.06, "text": " But it's basically given your observation of the state of the world through a perception system, there's going to be some uncertainty about the state of the world.", "tokens": [50364, 467, 311, 257, 636, 295, 8017, 990, 264, 1785, 295, 264, 1002, 13, 50564, 50564, 583, 309, 311, 1936, 2212, 428, 14816, 295, 264, 1785, 295, 264, 1002, 807, 257, 12860, 1185, 11, 456, 311, 516, 281, 312, 512, 15697, 466, 264, 1785, 295, 264, 1002, 13, 51114, 51114, 400, 264, 12655, 1601, 6608, 1936, 37808, 257, 39148, 7316, 322, 341, 15697, 13, 51364, 51364, 400, 586, 562, 291, 1190, 807, 428, 2128, 2316, 11, 291, 434, 516, 281, 362, 257, 16505, 15697, 466, 264, 1785, 295, 264, 1002, 412, 264, 958, 565, 1823, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08329453131165167, "compression_ratio": 1.9515418502202644, "no_speech_prob": 1.593410524947103e-05}, {"id": 646, "seek": 356006, "start": 3575.06, "end": 3580.06, "text": " And the Kalman filter basically assumes a Gaussian distribution on this uncertainty.", "tokens": [50364, 467, 311, 257, 636, 295, 8017, 990, 264, 1785, 295, 264, 1002, 13, 50564, 50564, 583, 309, 311, 1936, 2212, 428, 14816, 295, 264, 1785, 295, 264, 1002, 807, 257, 12860, 1185, 11, 456, 311, 516, 281, 312, 512, 15697, 466, 264, 1785, 295, 264, 1002, 13, 51114, 51114, 400, 264, 12655, 1601, 6608, 1936, 37808, 257, 39148, 7316, 322, 341, 15697, 13, 51364, 51364, 400, 586, 562, 291, 1190, 807, 428, 2128, 2316, 11, 291, 434, 516, 281, 362, 257, 16505, 15697, 466, 264, 1785, 295, 264, 1002, 412, 264, 958, 565, 1823, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08329453131165167, "compression_ratio": 1.9515418502202644, "no_speech_prob": 1.593410524947103e-05}, {"id": 647, "seek": 356006, "start": 3580.06, "end": 3589.06, "text": " And now when you run through your forward model, you're going to have a resulting uncertainty about the state of the world at the next time step.", "tokens": [50364, 467, 311, 257, 636, 295, 8017, 990, 264, 1785, 295, 264, 1002, 13, 50564, 50564, 583, 309, 311, 1936, 2212, 428, 14816, 295, 264, 1785, 295, 264, 1002, 807, 257, 12860, 1185, 11, 456, 311, 516, 281, 312, 512, 15697, 466, 264, 1785, 295, 264, 1002, 13, 51114, 51114, 400, 264, 12655, 1601, 6608, 1936, 37808, 257, 39148, 7316, 322, 341, 15697, 13, 51364, 51364, 400, 586, 562, 291, 1190, 807, 428, 2128, 2316, 11, 291, 434, 516, 281, 362, 257, 16505, 15697, 466, 264, 1785, 295, 264, 1002, 412, 264, 958, 565, 1823, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08329453131165167, "compression_ratio": 1.9515418502202644, "no_speech_prob": 1.593410524947103e-05}, {"id": 648, "seek": 358906, "start": 3589.06, "end": 3593.06, "text": " Because it was uncertain to start with.", "tokens": [50364, 1436, 309, 390, 11308, 281, 722, 365, 13, 50564, 50564, 407, 2212, 264, 15697, 562, 291, 1409, 490, 11, 437, 311, 264, 15697, 934, 472, 1823, 295, 10649, 11, 498, 291, 528, 30, 51014, 51014, 400, 498, 291, 6552, 8213, 507, 295, 439, 729, 4439, 293, 39148, 507, 295, 264, 15697, 11, 300, 311, 437, 257, 12655, 1601, 6608, 307, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10662446388831505, "compression_ratio": 1.6158536585365855, "no_speech_prob": 1.919872738653794e-05}, {"id": 649, "seek": 358906, "start": 3593.06, "end": 3602.06, "text": " So given the uncertainty when you started from, what's the uncertainty after one step of physics, if you want?", "tokens": [50364, 1436, 309, 390, 11308, 281, 722, 365, 13, 50564, 50564, 407, 2212, 264, 15697, 562, 291, 1409, 490, 11, 437, 311, 264, 15697, 934, 472, 1823, 295, 10649, 11, 498, 291, 528, 30, 51014, 51014, 400, 498, 291, 6552, 8213, 507, 295, 439, 729, 4439, 293, 39148, 507, 295, 264, 15697, 11, 300, 311, 437, 257, 12655, 1601, 6608, 307, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10662446388831505, "compression_ratio": 1.6158536585365855, "no_speech_prob": 1.919872738653794e-05}, {"id": 650, "seek": 358906, "start": 3602.06, "end": 3614.06, "text": " And if you assume linearity of all those steps and Gaussianity of the uncertainty, that's what a Kalman filter is.", "tokens": [50364, 1436, 309, 390, 11308, 281, 722, 365, 13, 50564, 50564, 407, 2212, 264, 15697, 562, 291, 1409, 490, 11, 437, 311, 264, 15697, 934, 472, 1823, 295, 10649, 11, 498, 291, 528, 30, 51014, 51014, 400, 498, 291, 6552, 8213, 507, 295, 439, 729, 4439, 293, 39148, 507, 295, 264, 15697, 11, 300, 311, 437, 257, 12655, 1601, 6608, 307, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10662446388831505, "compression_ratio": 1.6158536585365855, "no_speech_prob": 1.919872738653794e-05}, {"id": 651, "seek": 361406, "start": 3614.06, "end": 3619.06, "text": " Most of the uncertainty comes from, OK, so now your forward model produces a prediction.", "tokens": [50364, 4534, 295, 264, 15697, 1487, 490, 11, 2264, 11, 370, 586, 428, 2128, 2316, 14725, 257, 17630, 13, 50614, 50614, 400, 412, 264, 958, 565, 1823, 11, 291, 1062, 483, 1071, 3760, 295, 264, 1785, 295, 264, 1002, 570, 428, 14840, 366, 920, 1364, 13, 50914, 50914, 407, 586, 291, 362, 732, 10384, 2023, 2567, 13, 1485, 307, 428, 777, 12860, 295, 264, 1002, 5112, 291, 11, 510, 307, 689, 286, 519, 264, 1785, 295, 264, 1002, 307, 13, 51264, 51264, 400, 428, 2128, 2316, 611, 19147, 11, 510, 311, 689, 286, 519, 309, 307, 13, 51414, 51414, 400, 291, 362, 281, 10432, 729, 732, 13, 663, 311, 689, 264, 14024, 295, 12655, 1601, 30822, 1487, 294, 11, 597, 307, 286, 600, 658, 732, 39148, 21264, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09971855250933698, "compression_ratio": 1.8958333333333333, "no_speech_prob": 2.2117095795692876e-05}, {"id": 652, "seek": 361406, "start": 3619.06, "end": 3625.06, "text": " And at the next time step, you might get another reading of the state of the world because your sensors are still working.", "tokens": [50364, 4534, 295, 264, 15697, 1487, 490, 11, 2264, 11, 370, 586, 428, 2128, 2316, 14725, 257, 17630, 13, 50614, 50614, 400, 412, 264, 958, 565, 1823, 11, 291, 1062, 483, 1071, 3760, 295, 264, 1785, 295, 264, 1002, 570, 428, 14840, 366, 920, 1364, 13, 50914, 50914, 407, 586, 291, 362, 732, 10384, 2023, 2567, 13, 1485, 307, 428, 777, 12860, 295, 264, 1002, 5112, 291, 11, 510, 307, 689, 286, 519, 264, 1785, 295, 264, 1002, 307, 13, 51264, 51264, 400, 428, 2128, 2316, 611, 19147, 11, 510, 311, 689, 286, 519, 309, 307, 13, 51414, 51414, 400, 291, 362, 281, 10432, 729, 732, 13, 663, 311, 689, 264, 14024, 295, 12655, 1601, 30822, 1487, 294, 11, 597, 307, 286, 600, 658, 732, 39148, 21264, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09971855250933698, "compression_ratio": 1.8958333333333333, "no_speech_prob": 2.2117095795692876e-05}, {"id": 653, "seek": 361406, "start": 3625.06, "end": 3632.06, "text": " So now you have two Gaussians. One is your new perception of the world tells you, here is where I think the state of the world is.", "tokens": [50364, 4534, 295, 264, 15697, 1487, 490, 11, 2264, 11, 370, 586, 428, 2128, 2316, 14725, 257, 17630, 13, 50614, 50614, 400, 412, 264, 958, 565, 1823, 11, 291, 1062, 483, 1071, 3760, 295, 264, 1785, 295, 264, 1002, 570, 428, 14840, 366, 920, 1364, 13, 50914, 50914, 407, 586, 291, 362, 732, 10384, 2023, 2567, 13, 1485, 307, 428, 777, 12860, 295, 264, 1002, 5112, 291, 11, 510, 307, 689, 286, 519, 264, 1785, 295, 264, 1002, 307, 13, 51264, 51264, 400, 428, 2128, 2316, 611, 19147, 11, 510, 311, 689, 286, 519, 309, 307, 13, 51414, 51414, 400, 291, 362, 281, 10432, 729, 732, 13, 663, 311, 689, 264, 14024, 295, 12655, 1601, 30822, 1487, 294, 11, 597, 307, 286, 600, 658, 732, 39148, 21264, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09971855250933698, "compression_ratio": 1.8958333333333333, "no_speech_prob": 2.2117095795692876e-05}, {"id": 654, "seek": 361406, "start": 3632.06, "end": 3635.06, "text": " And your forward model also predicted, here's where I think it is.", "tokens": [50364, 4534, 295, 264, 15697, 1487, 490, 11, 2264, 11, 370, 586, 428, 2128, 2316, 14725, 257, 17630, 13, 50614, 50614, 400, 412, 264, 958, 565, 1823, 11, 291, 1062, 483, 1071, 3760, 295, 264, 1785, 295, 264, 1002, 570, 428, 14840, 366, 920, 1364, 13, 50914, 50914, 407, 586, 291, 362, 732, 10384, 2023, 2567, 13, 1485, 307, 428, 777, 12860, 295, 264, 1002, 5112, 291, 11, 510, 307, 689, 286, 519, 264, 1785, 295, 264, 1002, 307, 13, 51264, 51264, 400, 428, 2128, 2316, 611, 19147, 11, 510, 311, 689, 286, 519, 309, 307, 13, 51414, 51414, 400, 291, 362, 281, 10432, 729, 732, 13, 663, 311, 689, 264, 14024, 295, 12655, 1601, 30822, 1487, 294, 11, 597, 307, 286, 600, 658, 732, 39148, 21264, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09971855250933698, "compression_ratio": 1.8958333333333333, "no_speech_prob": 2.2117095795692876e-05}, {"id": 655, "seek": 361406, "start": 3635.06, "end": 3643.06, "text": " And you have to combine those two. That's where the complexity of Kalman filtering comes in, which is I've got two Gaussian predictions.", "tokens": [50364, 4534, 295, 264, 15697, 1487, 490, 11, 2264, 11, 370, 586, 428, 2128, 2316, 14725, 257, 17630, 13, 50614, 50614, 400, 412, 264, 958, 565, 1823, 11, 291, 1062, 483, 1071, 3760, 295, 264, 1785, 295, 264, 1002, 570, 428, 14840, 366, 920, 1364, 13, 50914, 50914, 407, 586, 291, 362, 732, 10384, 2023, 2567, 13, 1485, 307, 428, 777, 12860, 295, 264, 1002, 5112, 291, 11, 510, 307, 689, 286, 519, 264, 1785, 295, 264, 1002, 307, 13, 51264, 51264, 400, 428, 2128, 2316, 611, 19147, 11, 510, 311, 689, 286, 519, 309, 307, 13, 51414, 51414, 400, 291, 362, 281, 10432, 729, 732, 13, 663, 311, 689, 264, 14024, 295, 12655, 1601, 30822, 1487, 294, 11, 597, 307, 286, 600, 658, 732, 39148, 21264, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09971855250933698, "compression_ratio": 1.8958333333333333, "no_speech_prob": 2.2117095795692876e-05}, {"id": 656, "seek": 364306, "start": 3643.06, "end": 3649.06, "text": " So the resulting probability distribution is also a Gaussian if you compute the covariance matrix and et cetera.", "tokens": [50364, 407, 264, 16505, 8482, 7316, 307, 611, 257, 39148, 498, 291, 14722, 264, 49851, 719, 8141, 293, 1030, 11458, 13, 50664, 50664, 400, 300, 311, 689, 264, 30546, 337, 12655, 1601, 15995, 808, 490, 13, 50964, 50964, 2264, 11, 370, 12655, 1601, 6608, 307, 257, 636, 281, 2028, 365, 264, 15697, 294, 264, 3760, 428, 12860, 295, 264, 1002, 293, 294, 264, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13448670416167288, "compression_ratio": 1.5132275132275133, "no_speech_prob": 9.803994544199668e-06}, {"id": 657, "seek": 364306, "start": 3649.06, "end": 3655.06, "text": " And that's where the formulas for Kalman filters come from.", "tokens": [50364, 407, 264, 16505, 8482, 7316, 307, 611, 257, 39148, 498, 291, 14722, 264, 49851, 719, 8141, 293, 1030, 11458, 13, 50664, 50664, 400, 300, 311, 689, 264, 30546, 337, 12655, 1601, 15995, 808, 490, 13, 50964, 50964, 2264, 11, 370, 12655, 1601, 6608, 307, 257, 636, 281, 2028, 365, 264, 15697, 294, 264, 3760, 428, 12860, 295, 264, 1002, 293, 294, 264, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13448670416167288, "compression_ratio": 1.5132275132275133, "no_speech_prob": 9.803994544199668e-06}, {"id": 658, "seek": 364306, "start": 3655.06, "end": 3666.06, "text": " OK, so Kalman filter is a way to deal with the uncertainty in the reading your perception of the world and in the", "tokens": [50364, 407, 264, 16505, 8482, 7316, 307, 611, 257, 39148, 498, 291, 14722, 264, 49851, 719, 8141, 293, 1030, 11458, 13, 50664, 50664, 400, 300, 311, 689, 264, 30546, 337, 12655, 1601, 15995, 808, 490, 13, 50964, 50964, 2264, 11, 370, 12655, 1601, 6608, 307, 257, 636, 281, 2028, 365, 264, 15697, 294, 264, 3760, 428, 12860, 295, 264, 1002, 293, 294, 264, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13448670416167288, "compression_ratio": 1.5132275132275133, "no_speech_prob": 9.803994544199668e-06}, {"id": 659, "seek": 366606, "start": 3666.06, "end": 3674.06, "text": " when you propagate this uncertainty in your forward model.", "tokens": [50364, 562, 291, 48256, 341, 15697, 294, 428, 2128, 2316, 13, 50764, 50764, 286, 519, 456, 390, 920, 257, 2135, 2649, 13, 286, 519, 291, 1415, 281, 2985, 264, 935, 300, 341, 307, 819, 490, 497, 43, 13, 51064, 51064, 2264, 11, 370, 437, 307, 497, 43, 294, 300, 4319, 30, 2264, 11, 370, 2264, 11, 286, 643, 472, 544, 1823, 949, 286, 751, 466, 497, 43, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11923303738446303, "compression_ratio": 1.5260115606936415, "no_speech_prob": 2.3533764760941267e-05}, {"id": 660, "seek": 366606, "start": 3674.06, "end": 3680.06, "text": " I think there was still a main difference. I think you wanted to address the point that this is different from RL.", "tokens": [50364, 562, 291, 48256, 341, 15697, 294, 428, 2128, 2316, 13, 50764, 50764, 286, 519, 456, 390, 920, 257, 2135, 2649, 13, 286, 519, 291, 1415, 281, 2985, 264, 935, 300, 341, 307, 819, 490, 497, 43, 13, 51064, 51064, 2264, 11, 370, 437, 307, 497, 43, 294, 300, 4319, 30, 2264, 11, 370, 2264, 11, 286, 643, 472, 544, 1823, 949, 286, 751, 466, 497, 43, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11923303738446303, "compression_ratio": 1.5260115606936415, "no_speech_prob": 2.3533764760941267e-05}, {"id": 661, "seek": 366606, "start": 3680.06, "end": 3688.06, "text": " OK, so what is RL in that context? OK, so OK, I need one more step before I talk about RL.", "tokens": [50364, 562, 291, 48256, 341, 15697, 294, 428, 2128, 2316, 13, 50764, 50764, 286, 519, 456, 390, 920, 257, 2135, 2649, 13, 286, 519, 291, 1415, 281, 2985, 264, 935, 300, 341, 307, 819, 490, 497, 43, 13, 51064, 51064, 2264, 11, 370, 437, 307, 497, 43, 294, 300, 4319, 30, 2264, 11, 370, 2264, 11, 286, 643, 472, 544, 1823, 949, 286, 751, 466, 497, 43, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11923303738446303, "compression_ratio": 1.5260115606936415, "no_speech_prob": 2.3533764760941267e-05}, {"id": 662, "seek": 368806, "start": 3688.06, "end": 3696.06, "text": " OK, and here is that step.", "tokens": [50364, 2264, 11, 293, 510, 307, 300, 1823, 13, 50764, 50764, 2264, 11, 370, 437, 321, 632, 445, 257, 3456, 2057, 390, 257, 2128, 2316, 300, 311, 517, 28850, 294, 565, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.08972420011247907, "compression_ratio": 1.1612903225806452, "no_speech_prob": 4.783868007507408e-06}, {"id": 663, "seek": 368806, "start": 3696.06, "end": 3708.06, "text": " OK, so what we had just a minute ago was a forward model that's unrolled in time.", "tokens": [50364, 2264, 11, 293, 510, 307, 300, 1823, 13, 50764, 50764, 2264, 11, 370, 437, 321, 632, 445, 257, 3456, 2057, 390, 257, 2128, 2316, 300, 311, 517, 28850, 294, 565, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.08972420011247907, "compression_ratio": 1.1612903225806452, "no_speech_prob": 4.783868007507408e-06}, {"id": 664, "seek": 370806, "start": 3708.06, "end": 3727.06, "text": " And the system has takes a sequence of actions. A1, A2, A3, S1, S2.", "tokens": [50364, 400, 264, 1185, 575, 2516, 257, 8310, 295, 5909, 13, 316, 16, 11, 316, 17, 11, 316, 18, 11, 318, 16, 11, 318, 17, 13, 51314, 51314, 400, 550, 321, 362, 264, 2063, 2445, 510, 1348, 493, 13, 51614, 51614, 2264, 11, 293, 341, 727, 352, 322, 11, 558, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1387790662271005, "compression_ratio": 1.217741935483871, "no_speech_prob": 2.8122856292611687e-06}, {"id": 665, "seek": 370806, "start": 3727.06, "end": 3733.06, "text": " And then we have the cost function here coming up.", "tokens": [50364, 400, 264, 1185, 575, 2516, 257, 8310, 295, 5909, 13, 316, 16, 11, 316, 17, 11, 316, 18, 11, 318, 16, 11, 318, 17, 13, 51314, 51314, 400, 550, 321, 362, 264, 2063, 2445, 510, 1348, 493, 13, 51614, 51614, 2264, 11, 293, 341, 727, 352, 322, 11, 558, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1387790662271005, "compression_ratio": 1.217741935483871, "no_speech_prob": 2.8122856292611687e-06}, {"id": 666, "seek": 370806, "start": 3733.06, "end": 3736.06, "text": " OK, and this could go on, right?", "tokens": [50364, 400, 264, 1185, 575, 2516, 257, 8310, 295, 5909, 13, 316, 16, 11, 316, 17, 11, 316, 18, 11, 318, 16, 11, 318, 17, 13, 51314, 51314, 400, 550, 321, 362, 264, 2063, 2445, 510, 1348, 493, 13, 51614, 51614, 2264, 11, 293, 341, 727, 352, 322, 11, 558, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1387790662271005, "compression_ratio": 1.217741935483871, "no_speech_prob": 2.8122856292611687e-06}, {"id": 667, "seek": 373606, "start": 3736.06, "end": 3745.06, "text": " Now, what we'd like to be able to do is not have to do this optimization with respect to A1, A2, A3, A4 every time.", "tokens": [50364, 823, 11, 437, 321, 1116, 411, 281, 312, 1075, 281, 360, 307, 406, 362, 281, 360, 341, 19618, 365, 3104, 281, 316, 16, 11, 316, 17, 11, 316, 18, 11, 316, 19, 633, 565, 13, 50814, 50814, 2048, 565, 321, 643, 281, 360, 257, 5038, 11, 321, 500, 380, 528, 281, 362, 281, 360, 264, 281, 352, 807, 341, 3997, 1399, 295, 646, 12425, 990, 16235, 51164, 51164, 807, 341, 2302, 1185, 281, 360, 2316, 35521, 1969, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11112620190876286, "compression_ratio": 1.566326530612245, "no_speech_prob": 1.1297969649604056e-05}, {"id": 668, "seek": 373606, "start": 3745.06, "end": 3752.06, "text": " Every time we need to do a planning, we don't want to have to do the to go through this complex process of back propagating gradient", "tokens": [50364, 823, 11, 437, 321, 1116, 411, 281, 312, 1075, 281, 360, 307, 406, 362, 281, 360, 341, 19618, 365, 3104, 281, 316, 16, 11, 316, 17, 11, 316, 18, 11, 316, 19, 633, 565, 13, 50814, 50814, 2048, 565, 321, 643, 281, 360, 257, 5038, 11, 321, 500, 380, 528, 281, 362, 281, 360, 264, 281, 352, 807, 341, 3997, 1399, 295, 646, 12425, 990, 16235, 51164, 51164, 807, 341, 2302, 1185, 281, 360, 2316, 35521, 1969, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11112620190876286, "compression_ratio": 1.566326530612245, "no_speech_prob": 1.1297969649604056e-05}, {"id": 669, "seek": 373606, "start": 3752.06, "end": 3759.06, "text": " through this entire system to do model predictive control.", "tokens": [50364, 823, 11, 437, 321, 1116, 411, 281, 312, 1075, 281, 360, 307, 406, 362, 281, 360, 341, 19618, 365, 3104, 281, 316, 16, 11, 316, 17, 11, 316, 18, 11, 316, 19, 633, 565, 13, 50814, 50814, 2048, 565, 321, 643, 281, 360, 257, 5038, 11, 321, 500, 380, 528, 281, 362, 281, 360, 264, 281, 352, 807, 341, 3997, 1399, 295, 646, 12425, 990, 16235, 51164, 51164, 807, 341, 2302, 1185, 281, 360, 2316, 35521, 1969, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11112620190876286, "compression_ratio": 1.566326530612245, "no_speech_prob": 1.1297969649604056e-05}, {"id": 670, "seek": 375906, "start": 3759.06, "end": 3767.06, "text": " And so a simple way to get rid of that step is the same trick that we use in autoencoders versus sparse coding.", "tokens": [50364, 400, 370, 257, 2199, 636, 281, 483, 3973, 295, 300, 1823, 307, 264, 912, 4282, 300, 321, 764, 294, 8399, 22660, 378, 433, 5717, 637, 11668, 17720, 13, 50764, 50764, 407, 1604, 11, 309, 311, 637, 11668, 17720, 13, 492, 1415, 281, 31499, 11, 457, 550, 321, 632, 281, 360, 38253, 365, 3104, 281, 264, 48994, 7006, 538, 19618, 13, 51114, 51114, 400, 300, 3574, 484, 281, 312, 5124, 13, 51264, 51264, 407, 437, 321, 2825, 466, 1036, 1243, 390, 264, 1558, 295, 1228, 364, 2058, 19866, 300, 321, 3847, 281, 6069, 264, 16252, 2158, 3838, 13, 51564, 51564, 2264, 11, 293, 321, 434, 516, 281, 360, 264, 912, 510, 13, 663, 18753, 294, 264, 1558, 295, 637, 11668, 8399, 22660, 19866, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08456418645663524, "compression_ratio": 1.725085910652921, "no_speech_prob": 3.844659659080207e-06}, {"id": 671, "seek": 375906, "start": 3767.06, "end": 3774.06, "text": " So remember, it's sparse coding. We wanted to reconstruct, but then we had to do inference with respect to the latent variable by optimization.", "tokens": [50364, 400, 370, 257, 2199, 636, 281, 483, 3973, 295, 300, 1823, 307, 264, 912, 4282, 300, 321, 764, 294, 8399, 22660, 378, 433, 5717, 637, 11668, 17720, 13, 50764, 50764, 407, 1604, 11, 309, 311, 637, 11668, 17720, 13, 492, 1415, 281, 31499, 11, 457, 550, 321, 632, 281, 360, 38253, 365, 3104, 281, 264, 48994, 7006, 538, 19618, 13, 51114, 51114, 400, 300, 3574, 484, 281, 312, 5124, 13, 51264, 51264, 407, 437, 321, 2825, 466, 1036, 1243, 390, 264, 1558, 295, 1228, 364, 2058, 19866, 300, 321, 3847, 281, 6069, 264, 16252, 2158, 3838, 13, 51564, 51564, 2264, 11, 293, 321, 434, 516, 281, 360, 264, 912, 510, 13, 663, 18753, 294, 264, 1558, 295, 637, 11668, 8399, 22660, 19866, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08456418645663524, "compression_ratio": 1.725085910652921, "no_speech_prob": 3.844659659080207e-06}, {"id": 672, "seek": 375906, "start": 3774.06, "end": 3777.06, "text": " And that turned out to be expensive.", "tokens": [50364, 400, 370, 257, 2199, 636, 281, 483, 3973, 295, 300, 1823, 307, 264, 912, 4282, 300, 321, 764, 294, 8399, 22660, 378, 433, 5717, 637, 11668, 17720, 13, 50764, 50764, 407, 1604, 11, 309, 311, 637, 11668, 17720, 13, 492, 1415, 281, 31499, 11, 457, 550, 321, 632, 281, 360, 38253, 365, 3104, 281, 264, 48994, 7006, 538, 19618, 13, 51114, 51114, 400, 300, 3574, 484, 281, 312, 5124, 13, 51264, 51264, 407, 437, 321, 2825, 466, 1036, 1243, 390, 264, 1558, 295, 1228, 364, 2058, 19866, 300, 321, 3847, 281, 6069, 264, 16252, 2158, 3838, 13, 51564, 51564, 2264, 11, 293, 321, 434, 516, 281, 360, 264, 912, 510, 13, 663, 18753, 294, 264, 1558, 295, 637, 11668, 8399, 22660, 19866, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08456418645663524, "compression_ratio": 1.725085910652921, "no_speech_prob": 3.844659659080207e-06}, {"id": 673, "seek": 375906, "start": 3777.06, "end": 3783.06, "text": " So what we talked about last week was the idea of using an encoder that we train to predict the optimal value directly.", "tokens": [50364, 400, 370, 257, 2199, 636, 281, 483, 3973, 295, 300, 1823, 307, 264, 912, 4282, 300, 321, 764, 294, 8399, 22660, 378, 433, 5717, 637, 11668, 17720, 13, 50764, 50764, 407, 1604, 11, 309, 311, 637, 11668, 17720, 13, 492, 1415, 281, 31499, 11, 457, 550, 321, 632, 281, 360, 38253, 365, 3104, 281, 264, 48994, 7006, 538, 19618, 13, 51114, 51114, 400, 300, 3574, 484, 281, 312, 5124, 13, 51264, 51264, 407, 437, 321, 2825, 466, 1036, 1243, 390, 264, 1558, 295, 1228, 364, 2058, 19866, 300, 321, 3847, 281, 6069, 264, 16252, 2158, 3838, 13, 51564, 51564, 2264, 11, 293, 321, 434, 516, 281, 360, 264, 912, 510, 13, 663, 18753, 294, 264, 1558, 295, 637, 11668, 8399, 22660, 19866, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08456418645663524, "compression_ratio": 1.725085910652921, "no_speech_prob": 3.844659659080207e-06}, {"id": 674, "seek": 375906, "start": 3783.06, "end": 3787.06, "text": " OK, and we're going to do the same here. That resulted in the idea of sparse autoencoder.", "tokens": [50364, 400, 370, 257, 2199, 636, 281, 483, 3973, 295, 300, 1823, 307, 264, 912, 4282, 300, 321, 764, 294, 8399, 22660, 378, 433, 5717, 637, 11668, 17720, 13, 50764, 50764, 407, 1604, 11, 309, 311, 637, 11668, 17720, 13, 492, 1415, 281, 31499, 11, 457, 550, 321, 632, 281, 360, 38253, 365, 3104, 281, 264, 48994, 7006, 538, 19618, 13, 51114, 51114, 400, 300, 3574, 484, 281, 312, 5124, 13, 51264, 51264, 407, 437, 321, 2825, 466, 1036, 1243, 390, 264, 1558, 295, 1228, 364, 2058, 19866, 300, 321, 3847, 281, 6069, 264, 16252, 2158, 3838, 13, 51564, 51564, 2264, 11, 293, 321, 434, 516, 281, 360, 264, 912, 510, 13, 663, 18753, 294, 264, 1558, 295, 637, 11668, 8399, 22660, 19866, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08456418645663524, "compression_ratio": 1.725085910652921, "no_speech_prob": 3.844659659080207e-06}, {"id": 675, "seek": 378706, "start": 3787.06, "end": 3793.06, "text": " We're going to do the same here. We're going to train a network to take the state", "tokens": [50364, 492, 434, 516, 281, 360, 264, 912, 510, 13, 492, 434, 516, 281, 3847, 257, 3209, 281, 747, 264, 1785, 50664, 50664, 293, 3838, 6069, 437, 264, 16252, 2158, 295, 264, 3069, 307, 13, 50914, 50914, 400, 341, 3209, 11, 295, 1164, 11, 321, 434, 516, 281, 3079, 633, 565, 1823, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06087001732417515, "compression_ratio": 1.532846715328467, "no_speech_prob": 9.814071745495312e-06}, {"id": 676, "seek": 378706, "start": 3793.06, "end": 3798.06, "text": " and directly predict what the optimal value of the action is.", "tokens": [50364, 492, 434, 516, 281, 360, 264, 912, 510, 13, 492, 434, 516, 281, 3847, 257, 3209, 281, 747, 264, 1785, 50664, 50664, 293, 3838, 6069, 437, 264, 16252, 2158, 295, 264, 3069, 307, 13, 50914, 50914, 400, 341, 3209, 11, 295, 1164, 11, 321, 434, 516, 281, 3079, 633, 565, 1823, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06087001732417515, "compression_ratio": 1.532846715328467, "no_speech_prob": 9.814071745495312e-06}, {"id": 677, "seek": 378706, "start": 3798.06, "end": 3810.06, "text": " And this network, of course, we're going to apply every time step.", "tokens": [50364, 492, 434, 516, 281, 360, 264, 912, 510, 13, 492, 434, 516, 281, 3847, 257, 3209, 281, 747, 264, 1785, 50664, 50664, 293, 3838, 6069, 437, 264, 16252, 2158, 295, 264, 3069, 307, 13, 50914, 50914, 400, 341, 3209, 11, 295, 1164, 11, 321, 434, 516, 281, 3079, 633, 565, 1823, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06087001732417515, "compression_ratio": 1.532846715328467, "no_speech_prob": 9.814071745495312e-06}, {"id": 678, "seek": 381006, "start": 3810.06, "end": 3817.06, "text": " And this is going to be called a policy network.", "tokens": [50364, 400, 341, 307, 516, 281, 312, 1219, 257, 3897, 3209, 13, 50714, 50714, 2264, 11, 370, 264, 3897, 3209, 2516, 264, 1785, 293, 14725, 257, 2041, 466, 264, 1151, 3069, 281, 747, 412, 341, 565, 13, 51214, 51214, 407, 382, 281, 17522, 264, 4787, 2063, 13, 51414, 51414, 400, 341, 307, 516, 281, 312, 257, 3847, 712, 18161, 2533, 420, 2035, 2316, 13075, 1602, 2316, 300, 321, 528, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06692105123441514, "compression_ratio": 1.6201117318435754, "no_speech_prob": 5.255129963188665e-06}, {"id": 679, "seek": 381006, "start": 3817.06, "end": 3827.06, "text": " OK, so the policy network takes the state and produces a guess about the best action to take at this time.", "tokens": [50364, 400, 341, 307, 516, 281, 312, 1219, 257, 3897, 3209, 13, 50714, 50714, 2264, 11, 370, 264, 3897, 3209, 2516, 264, 1785, 293, 14725, 257, 2041, 466, 264, 1151, 3069, 281, 747, 412, 341, 565, 13, 51214, 51214, 407, 382, 281, 17522, 264, 4787, 2063, 13, 51414, 51414, 400, 341, 307, 516, 281, 312, 257, 3847, 712, 18161, 2533, 420, 2035, 2316, 13075, 1602, 2316, 300, 321, 528, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06692105123441514, "compression_ratio": 1.6201117318435754, "no_speech_prob": 5.255129963188665e-06}, {"id": 680, "seek": 381006, "start": 3827.06, "end": 3831.06, "text": " So as to minimize the overall cost.", "tokens": [50364, 400, 341, 307, 516, 281, 312, 1219, 257, 3897, 3209, 13, 50714, 50714, 2264, 11, 370, 264, 3897, 3209, 2516, 264, 1785, 293, 14725, 257, 2041, 466, 264, 1151, 3069, 281, 747, 412, 341, 565, 13, 51214, 51214, 407, 382, 281, 17522, 264, 4787, 2063, 13, 51414, 51414, 400, 341, 307, 516, 281, 312, 257, 3847, 712, 18161, 2533, 420, 2035, 2316, 13075, 1602, 2316, 300, 321, 528, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06692105123441514, "compression_ratio": 1.6201117318435754, "no_speech_prob": 5.255129963188665e-06}, {"id": 681, "seek": 381006, "start": 3831.06, "end": 3836.06, "text": " And this is going to be a trainable neural net or whatever model parameterized model that we want.", "tokens": [50364, 400, 341, 307, 516, 281, 312, 1219, 257, 3897, 3209, 13, 50714, 50714, 2264, 11, 370, 264, 3897, 3209, 2516, 264, 1785, 293, 14725, 257, 2041, 466, 264, 1151, 3069, 281, 747, 412, 341, 565, 13, 51214, 51214, 407, 382, 281, 17522, 264, 4787, 2063, 13, 51414, 51414, 400, 341, 307, 516, 281, 312, 257, 3847, 712, 18161, 2533, 420, 2035, 2316, 13075, 1602, 2316, 300, 321, 528, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06692105123441514, "compression_ratio": 1.6201117318435754, "no_speech_prob": 5.255129963188665e-06}, {"id": 682, "seek": 383606, "start": 3836.06, "end": 3840.06, "text": " The way we're going to train this model is basically just by propagation.", "tokens": [50364, 440, 636, 321, 434, 516, 281, 3847, 341, 2316, 307, 1936, 445, 538, 38377, 13, 50564, 50564, 407, 321, 434, 516, 281, 1228, 527, 12860, 10088, 13, 50764, 50764, 639, 307, 264, 1002, 510, 13, 50914, 50914, 400, 321, 434, 1237, 412, 264, 1002, 365, 257, 2799, 13, 51064, 51064, 400, 456, 307, 257, 12860, 10088, 300, 2709, 505, 257, 2041, 382, 281, 437, 264, 1785, 295, 264, 1002, 307, 13, 51314, 51314, 2264, 11, 341, 307, 12860, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11100385275231786, "compression_ratio": 1.6868131868131868, "no_speech_prob": 7.766551789245568e-06}, {"id": 683, "seek": 383606, "start": 3840.06, "end": 3844.06, "text": " So we're going to using our perception module.", "tokens": [50364, 440, 636, 321, 434, 516, 281, 3847, 341, 2316, 307, 1936, 445, 538, 38377, 13, 50564, 50564, 407, 321, 434, 516, 281, 1228, 527, 12860, 10088, 13, 50764, 50764, 639, 307, 264, 1002, 510, 13, 50914, 50914, 400, 321, 434, 1237, 412, 264, 1002, 365, 257, 2799, 13, 51064, 51064, 400, 456, 307, 257, 12860, 10088, 300, 2709, 505, 257, 2041, 382, 281, 437, 264, 1785, 295, 264, 1002, 307, 13, 51314, 51314, 2264, 11, 341, 307, 12860, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11100385275231786, "compression_ratio": 1.6868131868131868, "no_speech_prob": 7.766551789245568e-06}, {"id": 684, "seek": 383606, "start": 3844.06, "end": 3847.06, "text": " This is the world here.", "tokens": [50364, 440, 636, 321, 434, 516, 281, 3847, 341, 2316, 307, 1936, 445, 538, 38377, 13, 50564, 50564, 407, 321, 434, 516, 281, 1228, 527, 12860, 10088, 13, 50764, 50764, 639, 307, 264, 1002, 510, 13, 50914, 50914, 400, 321, 434, 1237, 412, 264, 1002, 365, 257, 2799, 13, 51064, 51064, 400, 456, 307, 257, 12860, 10088, 300, 2709, 505, 257, 2041, 382, 281, 437, 264, 1785, 295, 264, 1002, 307, 13, 51314, 51314, 2264, 11, 341, 307, 12860, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11100385275231786, "compression_ratio": 1.6868131868131868, "no_speech_prob": 7.766551789245568e-06}, {"id": 685, "seek": 383606, "start": 3847.06, "end": 3850.06, "text": " And we're looking at the world with a camera.", "tokens": [50364, 440, 636, 321, 434, 516, 281, 3847, 341, 2316, 307, 1936, 445, 538, 38377, 13, 50564, 50564, 407, 321, 434, 516, 281, 1228, 527, 12860, 10088, 13, 50764, 50764, 639, 307, 264, 1002, 510, 13, 50914, 50914, 400, 321, 434, 1237, 412, 264, 1002, 365, 257, 2799, 13, 51064, 51064, 400, 456, 307, 257, 12860, 10088, 300, 2709, 505, 257, 2041, 382, 281, 437, 264, 1785, 295, 264, 1002, 307, 13, 51314, 51314, 2264, 11, 341, 307, 12860, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11100385275231786, "compression_ratio": 1.6868131868131868, "no_speech_prob": 7.766551789245568e-06}, {"id": 686, "seek": 383606, "start": 3850.06, "end": 3855.06, "text": " And there is a perception module that gives us a guess as to what the state of the world is.", "tokens": [50364, 440, 636, 321, 434, 516, 281, 3847, 341, 2316, 307, 1936, 445, 538, 38377, 13, 50564, 50564, 407, 321, 434, 516, 281, 1228, 527, 12860, 10088, 13, 50764, 50764, 639, 307, 264, 1002, 510, 13, 50914, 50914, 400, 321, 434, 1237, 412, 264, 1002, 365, 257, 2799, 13, 51064, 51064, 400, 456, 307, 257, 12860, 10088, 300, 2709, 505, 257, 2041, 382, 281, 437, 264, 1785, 295, 264, 1002, 307, 13, 51314, 51314, 2264, 11, 341, 307, 12860, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11100385275231786, "compression_ratio": 1.6868131868131868, "no_speech_prob": 7.766551789245568e-06}, {"id": 687, "seek": 383606, "start": 3855.06, "end": 3859.06, "text": " OK, this is perception.", "tokens": [50364, 440, 636, 321, 434, 516, 281, 3847, 341, 2316, 307, 1936, 445, 538, 38377, 13, 50564, 50564, 407, 321, 434, 516, 281, 1228, 527, 12860, 10088, 13, 50764, 50764, 639, 307, 264, 1002, 510, 13, 50914, 50914, 400, 321, 434, 1237, 412, 264, 1002, 365, 257, 2799, 13, 51064, 51064, 400, 456, 307, 257, 12860, 10088, 300, 2709, 505, 257, 2041, 382, 281, 437, 264, 1785, 295, 264, 1002, 307, 13, 51314, 51314, 2264, 11, 341, 307, 12860, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11100385275231786, "compression_ratio": 1.6868131868131868, "no_speech_prob": 7.766551789245568e-06}, {"id": 688, "seek": 385906, "start": 3859.06, "end": 3867.06, "text": " And this is a forward model applied multiple time steps.", "tokens": [50364, 400, 341, 307, 257, 2128, 2316, 6456, 3866, 565, 4439, 13, 50764, 50764, 400, 341, 307, 257, 2063, 13, 50914, 50914, 2264, 11, 370, 437, 321, 393, 360, 307, 1190, 264, 1185, 13, 51264, 51264, 400, 281, 1190, 264, 1185, 11, 321, 700, 1190, 807, 264, 12860, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10784758054293118, "compression_ratio": 1.462809917355372, "no_speech_prob": 2.9934178655821597e-06}, {"id": 689, "seek": 385906, "start": 3867.06, "end": 3870.06, "text": " And this is a cost.", "tokens": [50364, 400, 341, 307, 257, 2128, 2316, 6456, 3866, 565, 4439, 13, 50764, 50764, 400, 341, 307, 257, 2063, 13, 50914, 50914, 2264, 11, 370, 437, 321, 393, 360, 307, 1190, 264, 1185, 13, 51264, 51264, 400, 281, 1190, 264, 1185, 11, 321, 700, 1190, 807, 264, 12860, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10784758054293118, "compression_ratio": 1.462809917355372, "no_speech_prob": 2.9934178655821597e-06}, {"id": 690, "seek": 385906, "start": 3870.06, "end": 3877.06, "text": " OK, so what we can do is run the system.", "tokens": [50364, 400, 341, 307, 257, 2128, 2316, 6456, 3866, 565, 4439, 13, 50764, 50764, 400, 341, 307, 257, 2063, 13, 50914, 50914, 2264, 11, 370, 437, 321, 393, 360, 307, 1190, 264, 1185, 13, 51264, 51264, 400, 281, 1190, 264, 1185, 11, 321, 700, 1190, 807, 264, 12860, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10784758054293118, "compression_ratio": 1.462809917355372, "no_speech_prob": 2.9934178655821597e-06}, {"id": 691, "seek": 385906, "start": 3877.06, "end": 3885.06, "text": " And to run the system, we first run through the perception.", "tokens": [50364, 400, 341, 307, 257, 2128, 2316, 6456, 3866, 565, 4439, 13, 50764, 50764, 400, 341, 307, 257, 2063, 13, 50914, 50914, 2264, 11, 370, 437, 321, 393, 360, 307, 1190, 264, 1185, 13, 51264, 51264, 400, 281, 1190, 264, 1185, 11, 321, 700, 1190, 807, 264, 12860, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10784758054293118, "compression_ratio": 1.462809917355372, "no_speech_prob": 2.9934178655821597e-06}, {"id": 692, "seek": 388506, "start": 3885.06, "end": 3889.06, "text": " We compute an action, we run this action through the forward model.", "tokens": [50364, 492, 14722, 364, 3069, 11, 321, 1190, 341, 3069, 807, 264, 2128, 2316, 13, 50564, 50564, 639, 2128, 2316, 2709, 505, 510, 307, 264, 958, 1785, 321, 434, 516, 281, 312, 294, 13, 50714, 50714, 6620, 1169, 264, 2063, 293, 550, 1066, 516, 13, 50914, 50914, 5527, 884, 341, 445, 2128, 2365, 807, 341, 2302, 1185, 11, 597, 307, 534, 733, 295, 364, 517, 13217, 18680, 1753, 2533, 11, 498, 291, 528, 13, 51314, 51314, 400, 1564, 291, 434, 1096, 11, 291, 646, 48256, 2771, 2448, 490, 439, 264, 2115, 294, 264, 2063, 2445, 439, 264, 636, 807, 264, 3209, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11238519577752976, "compression_ratio": 1.728744939271255, "no_speech_prob": 1.1477929547254462e-05}, {"id": 693, "seek": 388506, "start": 3889.06, "end": 3892.06, "text": " This forward model gives us here is the next state we're going to be in.", "tokens": [50364, 492, 14722, 364, 3069, 11, 321, 1190, 341, 3069, 807, 264, 2128, 2316, 13, 50564, 50564, 639, 2128, 2316, 2709, 505, 510, 307, 264, 958, 1785, 321, 434, 516, 281, 312, 294, 13, 50714, 50714, 6620, 1169, 264, 2063, 293, 550, 1066, 516, 13, 50914, 50914, 5527, 884, 341, 445, 2128, 2365, 807, 341, 2302, 1185, 11, 597, 307, 534, 733, 295, 364, 517, 13217, 18680, 1753, 2533, 11, 498, 291, 528, 13, 51314, 51314, 400, 1564, 291, 434, 1096, 11, 291, 646, 48256, 2771, 2448, 490, 439, 264, 2115, 294, 264, 2063, 2445, 439, 264, 636, 807, 264, 3209, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11238519577752976, "compression_ratio": 1.728744939271255, "no_speech_prob": 1.1477929547254462e-05}, {"id": 694, "seek": 388506, "start": 3892.06, "end": 3896.06, "text": " Compute the cost and then keep going.", "tokens": [50364, 492, 14722, 364, 3069, 11, 321, 1190, 341, 3069, 807, 264, 2128, 2316, 13, 50564, 50564, 639, 2128, 2316, 2709, 505, 510, 307, 264, 958, 1785, 321, 434, 516, 281, 312, 294, 13, 50714, 50714, 6620, 1169, 264, 2063, 293, 550, 1066, 516, 13, 50914, 50914, 5527, 884, 341, 445, 2128, 2365, 807, 341, 2302, 1185, 11, 597, 307, 534, 733, 295, 364, 517, 13217, 18680, 1753, 2533, 11, 498, 291, 528, 13, 51314, 51314, 400, 1564, 291, 434, 1096, 11, 291, 646, 48256, 2771, 2448, 490, 439, 264, 2115, 294, 264, 2063, 2445, 439, 264, 636, 807, 264, 3209, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11238519577752976, "compression_ratio": 1.728744939271255, "no_speech_prob": 1.1477929547254462e-05}, {"id": 695, "seek": 388506, "start": 3896.06, "end": 3904.06, "text": " Keep doing this just forward prop through this entire system, which is really kind of an unworld recurrent net, if you want.", "tokens": [50364, 492, 14722, 364, 3069, 11, 321, 1190, 341, 3069, 807, 264, 2128, 2316, 13, 50564, 50564, 639, 2128, 2316, 2709, 505, 510, 307, 264, 958, 1785, 321, 434, 516, 281, 312, 294, 13, 50714, 50714, 6620, 1169, 264, 2063, 293, 550, 1066, 516, 13, 50914, 50914, 5527, 884, 341, 445, 2128, 2365, 807, 341, 2302, 1185, 11, 597, 307, 534, 733, 295, 364, 517, 13217, 18680, 1753, 2533, 11, 498, 291, 528, 13, 51314, 51314, 400, 1564, 291, 434, 1096, 11, 291, 646, 48256, 2771, 2448, 490, 439, 264, 2115, 294, 264, 2063, 2445, 439, 264, 636, 807, 264, 3209, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11238519577752976, "compression_ratio": 1.728744939271255, "no_speech_prob": 1.1477929547254462e-05}, {"id": 696, "seek": 388506, "start": 3904.06, "end": 3913.06, "text": " And once you're done, you back propagate gradients from all the terms in the cost function all the way through the network,", "tokens": [50364, 492, 14722, 364, 3069, 11, 321, 1190, 341, 3069, 807, 264, 2128, 2316, 13, 50564, 50564, 639, 2128, 2316, 2709, 505, 510, 307, 264, 958, 1785, 321, 434, 516, 281, 312, 294, 13, 50714, 50714, 6620, 1169, 264, 2063, 293, 550, 1066, 516, 13, 50914, 50914, 5527, 884, 341, 445, 2128, 2365, 807, 341, 2302, 1185, 11, 597, 307, 534, 733, 295, 364, 517, 13217, 18680, 1753, 2533, 11, 498, 291, 528, 13, 51314, 51314, 400, 1564, 291, 434, 1096, 11, 291, 646, 48256, 2771, 2448, 490, 439, 264, 2115, 294, 264, 2063, 2445, 439, 264, 636, 807, 264, 3209, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11238519577752976, "compression_ratio": 1.728744939271255, "no_speech_prob": 1.1477929547254462e-05}, {"id": 697, "seek": 391306, "start": 3913.06, "end": 3921.06, "text": " all the way through the parameters of that policy network.", "tokens": [50364, 439, 264, 636, 807, 264, 9834, 295, 300, 3897, 3209, 13, 50764, 50764, 407, 1936, 291, 14722, 264, 955, 383, 13, 50964, 50964, 407, 955, 383, 11, 1604, 11, 307, 264, 2408, 295, 439, 264, 383, 311, 294, 257, 938, 565, 365, 3104, 281, 45318, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.14929804801940919, "compression_ratio": 1.3257575757575757, "no_speech_prob": 6.436930107156513e-06}, {"id": 698, "seek": 391306, "start": 3921.06, "end": 3925.06, "text": " So basically you compute the big C.", "tokens": [50364, 439, 264, 636, 807, 264, 9834, 295, 300, 3897, 3209, 13, 50764, 50764, 407, 1936, 291, 14722, 264, 955, 383, 13, 50964, 50964, 407, 955, 383, 11, 1604, 11, 307, 264, 2408, 295, 439, 264, 383, 311, 294, 257, 938, 565, 365, 3104, 281, 45318, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.14929804801940919, "compression_ratio": 1.3257575757575757, "no_speech_prob": 6.436930107156513e-06}, {"id": 699, "seek": 391306, "start": 3925.06, "end": 3933.06, "text": " So big C, remember, is the sum of all the C's in a long time with respect to DW.", "tokens": [50364, 439, 264, 636, 807, 264, 9834, 295, 300, 3897, 3209, 13, 50764, 50764, 407, 1936, 291, 14722, 264, 955, 383, 13, 50964, 50964, 407, 955, 383, 11, 1604, 11, 307, 264, 2408, 295, 439, 264, 383, 311, 294, 257, 938, 565, 365, 3104, 281, 45318, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.14929804801940919, "compression_ratio": 1.3257575757575757, "no_speech_prob": 6.436930107156513e-06}, {"id": 700, "seek": 393306, "start": 3933.06, "end": 3944.06, "text": " OK, and that's just going to be the sum over time of the big C over DW.", "tokens": [50364, 2264, 11, 293, 300, 311, 445, 516, 281, 312, 264, 2408, 670, 565, 295, 264, 955, 383, 670, 45318, 13, 50914, 50914, 4919, 11, 1338, 11, 955, 383, 670, 413, 2218, 13, 51214, 51214, 413, 2218, 670, 45318, 13, 51364, 51364, 2264, 11, 286, 445, 6456, 5021, 4978, 11, 558, 30, 51464, 51464, 583, 286, 500, 380, 643, 281, 11, 558, 30, 51514, 51514, 759, 286, 445, 6964, 341, 2445, 294, 25878, 284, 339, 293, 286, 445, 360, 646, 79, 1513, 11, 309, 603, 445, 360, 264, 558, 551, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.17675594573325298, "compression_ratio": 1.5081967213114753, "no_speech_prob": 4.703665126726264e-06}, {"id": 701, "seek": 393306, "start": 3944.06, "end": 3950.06, "text": " Sorry, yeah, big C over DAT.", "tokens": [50364, 2264, 11, 293, 300, 311, 445, 516, 281, 312, 264, 2408, 670, 565, 295, 264, 955, 383, 670, 45318, 13, 50914, 50914, 4919, 11, 1338, 11, 955, 383, 670, 413, 2218, 13, 51214, 51214, 413, 2218, 670, 45318, 13, 51364, 51364, 2264, 11, 286, 445, 6456, 5021, 4978, 11, 558, 30, 51464, 51464, 583, 286, 500, 380, 643, 281, 11, 558, 30, 51514, 51514, 759, 286, 445, 6964, 341, 2445, 294, 25878, 284, 339, 293, 286, 445, 360, 646, 79, 1513, 11, 309, 603, 445, 360, 264, 558, 551, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.17675594573325298, "compression_ratio": 1.5081967213114753, "no_speech_prob": 4.703665126726264e-06}, {"id": 702, "seek": 393306, "start": 3950.06, "end": 3953.06, "text": " DAT over DW.", "tokens": [50364, 2264, 11, 293, 300, 311, 445, 516, 281, 312, 264, 2408, 670, 565, 295, 264, 955, 383, 670, 45318, 13, 50914, 50914, 4919, 11, 1338, 11, 955, 383, 670, 413, 2218, 13, 51214, 51214, 413, 2218, 670, 45318, 13, 51364, 51364, 2264, 11, 286, 445, 6456, 5021, 4978, 11, 558, 30, 51464, 51464, 583, 286, 500, 380, 643, 281, 11, 558, 30, 51514, 51514, 759, 286, 445, 6964, 341, 2445, 294, 25878, 284, 339, 293, 286, 445, 360, 646, 79, 1513, 11, 309, 603, 445, 360, 264, 558, 551, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.17675594573325298, "compression_ratio": 1.5081967213114753, "no_speech_prob": 4.703665126726264e-06}, {"id": 703, "seek": 393306, "start": 3953.06, "end": 3955.06, "text": " OK, I just applied chain rule, right?", "tokens": [50364, 2264, 11, 293, 300, 311, 445, 516, 281, 312, 264, 2408, 670, 565, 295, 264, 955, 383, 670, 45318, 13, 50914, 50914, 4919, 11, 1338, 11, 955, 383, 670, 413, 2218, 13, 51214, 51214, 413, 2218, 670, 45318, 13, 51364, 51364, 2264, 11, 286, 445, 6456, 5021, 4978, 11, 558, 30, 51464, 51464, 583, 286, 500, 380, 643, 281, 11, 558, 30, 51514, 51514, 759, 286, 445, 6964, 341, 2445, 294, 25878, 284, 339, 293, 286, 445, 360, 646, 79, 1513, 11, 309, 603, 445, 360, 264, 558, 551, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.17675594573325298, "compression_ratio": 1.5081967213114753, "no_speech_prob": 4.703665126726264e-06}, {"id": 704, "seek": 393306, "start": 3955.06, "end": 3956.06, "text": " But I don't need to, right?", "tokens": [50364, 2264, 11, 293, 300, 311, 445, 516, 281, 312, 264, 2408, 670, 565, 295, 264, 955, 383, 670, 45318, 13, 50914, 50914, 4919, 11, 1338, 11, 955, 383, 670, 413, 2218, 13, 51214, 51214, 413, 2218, 670, 45318, 13, 51364, 51364, 2264, 11, 286, 445, 6456, 5021, 4978, 11, 558, 30, 51464, 51464, 583, 286, 500, 380, 643, 281, 11, 558, 30, 51514, 51514, 759, 286, 445, 6964, 341, 2445, 294, 25878, 284, 339, 293, 286, 445, 360, 646, 79, 1513, 11, 309, 603, 445, 360, 264, 558, 551, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.17675594573325298, "compression_ratio": 1.5081967213114753, "no_speech_prob": 4.703665126726264e-06}, {"id": 705, "seek": 393306, "start": 3956.06, "end": 3962.06, "text": " If I just define this function in pytorch and I just do backprop, it'll just do the right thing.", "tokens": [50364, 2264, 11, 293, 300, 311, 445, 516, 281, 312, 264, 2408, 670, 565, 295, 264, 955, 383, 670, 45318, 13, 50914, 50914, 4919, 11, 1338, 11, 955, 383, 670, 413, 2218, 13, 51214, 51214, 413, 2218, 670, 45318, 13, 51364, 51364, 2264, 11, 286, 445, 6456, 5021, 4978, 11, 558, 30, 51464, 51464, 583, 286, 500, 380, 643, 281, 11, 558, 30, 51514, 51514, 759, 286, 445, 6964, 341, 2445, 294, 25878, 284, 339, 293, 286, 445, 360, 646, 79, 1513, 11, 309, 603, 445, 360, 264, 558, 551, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.17675594573325298, "compression_ratio": 1.5081967213114753, "no_speech_prob": 4.703665126726264e-06}, {"id": 706, "seek": 396206, "start": 3962.06, "end": 3969.06, "text": " So I can compute the gradient of the overall cost with respect to the parameters of that policy network.", "tokens": [50364, 407, 286, 393, 14722, 264, 16235, 295, 264, 4787, 2063, 365, 3104, 281, 264, 9834, 295, 300, 3897, 3209, 13, 50714, 50714, 400, 370, 498, 286, 3847, 341, 670, 31868, 867, 10938, 11, 498, 452, 2128, 2316, 307, 3006, 11, 498, 452, 2063, 2445, 775, 437, 286, 528, 11, 51064, 51064, 550, 452, 3897, 3209, 307, 516, 281, 1466, 257, 665, 3897, 300, 445, 1237, 412, 264, 1785, 486, 17522, 264, 5176, 2063, 670, 257, 21512, 13, 51514, 51514, 2264, 11, 264, 4274, 2063, 670, 257, 21512, 13, 51664, 51664, 821, 311, 572, 29280, 2539, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07703280684971574, "compression_ratio": 1.7153846153846153, "no_speech_prob": 8.396937118959613e-06}, {"id": 707, "seek": 396206, "start": 3969.06, "end": 3976.06, "text": " And so if I train this over sufficiently many samples, if my forward model is correct, if my cost function does what I want,", "tokens": [50364, 407, 286, 393, 14722, 264, 16235, 295, 264, 4787, 2063, 365, 3104, 281, 264, 9834, 295, 300, 3897, 3209, 13, 50714, 50714, 400, 370, 498, 286, 3847, 341, 670, 31868, 867, 10938, 11, 498, 452, 2128, 2316, 307, 3006, 11, 498, 452, 2063, 2445, 775, 437, 286, 528, 11, 51064, 51064, 550, 452, 3897, 3209, 307, 516, 281, 1466, 257, 665, 3897, 300, 445, 1237, 412, 264, 1785, 486, 17522, 264, 5176, 2063, 670, 257, 21512, 13, 51514, 51514, 2264, 11, 264, 4274, 2063, 670, 257, 21512, 13, 51664, 51664, 821, 311, 572, 29280, 2539, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07703280684971574, "compression_ratio": 1.7153846153846153, "no_speech_prob": 8.396937118959613e-06}, {"id": 708, "seek": 396206, "start": 3976.06, "end": 3985.06, "text": " then my policy network is going to learn a good policy that just looking at the state will minimize the expected cost over a trajectory.", "tokens": [50364, 407, 286, 393, 14722, 264, 16235, 295, 264, 4787, 2063, 365, 3104, 281, 264, 9834, 295, 300, 3897, 3209, 13, 50714, 50714, 400, 370, 498, 286, 3847, 341, 670, 31868, 867, 10938, 11, 498, 452, 2128, 2316, 307, 3006, 11, 498, 452, 2063, 2445, 775, 437, 286, 528, 11, 51064, 51064, 550, 452, 3897, 3209, 307, 516, 281, 1466, 257, 665, 3897, 300, 445, 1237, 412, 264, 1785, 486, 17522, 264, 5176, 2063, 670, 257, 21512, 13, 51514, 51514, 2264, 11, 264, 4274, 2063, 670, 257, 21512, 13, 51664, 51664, 821, 311, 572, 29280, 2539, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07703280684971574, "compression_ratio": 1.7153846153846153, "no_speech_prob": 8.396937118959613e-06}, {"id": 709, "seek": 396206, "start": 3985.06, "end": 3988.06, "text": " OK, the average cost over a trajectory.", "tokens": [50364, 407, 286, 393, 14722, 264, 16235, 295, 264, 4787, 2063, 365, 3104, 281, 264, 9834, 295, 300, 3897, 3209, 13, 50714, 50714, 400, 370, 498, 286, 3847, 341, 670, 31868, 867, 10938, 11, 498, 452, 2128, 2316, 307, 3006, 11, 498, 452, 2063, 2445, 775, 437, 286, 528, 11, 51064, 51064, 550, 452, 3897, 3209, 307, 516, 281, 1466, 257, 665, 3897, 300, 445, 1237, 412, 264, 1785, 486, 17522, 264, 5176, 2063, 670, 257, 21512, 13, 51514, 51514, 2264, 11, 264, 4274, 2063, 670, 257, 21512, 13, 51664, 51664, 821, 311, 572, 29280, 2539, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07703280684971574, "compression_ratio": 1.7153846153846153, "no_speech_prob": 8.396937118959613e-06}, {"id": 710, "seek": 396206, "start": 3988.06, "end": 3989.06, "text": " There's no reinforcement learning here.", "tokens": [50364, 407, 286, 393, 14722, 264, 16235, 295, 264, 4787, 2063, 365, 3104, 281, 264, 9834, 295, 300, 3897, 3209, 13, 50714, 50714, 400, 370, 498, 286, 3847, 341, 670, 31868, 867, 10938, 11, 498, 452, 2128, 2316, 307, 3006, 11, 498, 452, 2063, 2445, 775, 437, 286, 528, 11, 51064, 51064, 550, 452, 3897, 3209, 307, 516, 281, 1466, 257, 665, 3897, 300, 445, 1237, 412, 264, 1785, 486, 17522, 264, 5176, 2063, 670, 257, 21512, 13, 51514, 51514, 2264, 11, 264, 4274, 2063, 670, 257, 21512, 13, 51664, 51664, 821, 311, 572, 29280, 2539, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07703280684971574, "compression_ratio": 1.7153846153846153, "no_speech_prob": 8.396937118959613e-06}, {"id": 711, "seek": 398906, "start": 3989.06, "end": 3992.06, "text": " This is all backdrop.", "tokens": [50364, 639, 307, 439, 32697, 13, 50514, 50514, 2264, 11, 586, 321, 393, 751, 466, 264, 2649, 365, 29280, 2539, 13, 50714, 50714, 440, 2135, 2649, 365, 29280, 2539, 510, 307, 732, 18353, 13, 51014, 51014, 440, 700, 472, 307, 294, 29280, 2539, 11, 294, 881, 29280, 2539, 15077, 412, 1935, 11, 264, 383, 2445, 307, 257, 2211, 2424, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10749782834734235, "compression_ratio": 1.8193548387096774, "no_speech_prob": 6.043361736374209e-06}, {"id": 712, "seek": 398906, "start": 3992.06, "end": 3996.06, "text": " OK, now we can talk about the difference with reinforcement learning.", "tokens": [50364, 639, 307, 439, 32697, 13, 50514, 50514, 2264, 11, 586, 321, 393, 751, 466, 264, 2649, 365, 29280, 2539, 13, 50714, 50714, 440, 2135, 2649, 365, 29280, 2539, 510, 307, 732, 18353, 13, 51014, 51014, 440, 700, 472, 307, 294, 29280, 2539, 11, 294, 881, 29280, 2539, 15077, 412, 1935, 11, 264, 383, 2445, 307, 257, 2211, 2424, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10749782834734235, "compression_ratio": 1.8193548387096774, "no_speech_prob": 6.043361736374209e-06}, {"id": 713, "seek": 398906, "start": 3996.06, "end": 4002.06, "text": " The main difference with reinforcement learning here is twofold.", "tokens": [50364, 639, 307, 439, 32697, 13, 50514, 50514, 2264, 11, 586, 321, 393, 751, 466, 264, 2649, 365, 29280, 2539, 13, 50714, 50714, 440, 2135, 2649, 365, 29280, 2539, 510, 307, 732, 18353, 13, 51014, 51014, 440, 700, 472, 307, 294, 29280, 2539, 11, 294, 881, 29280, 2539, 15077, 412, 1935, 11, 264, 383, 2445, 307, 257, 2211, 2424, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10749782834734235, "compression_ratio": 1.8193548387096774, "no_speech_prob": 6.043361736374209e-06}, {"id": 714, "seek": 398906, "start": 4002.06, "end": 4015.06, "text": " The first one is in reinforcement learning, in most reinforcement learning scenarios at least, the C function is a black box.", "tokens": [50364, 639, 307, 439, 32697, 13, 50514, 50514, 2264, 11, 586, 321, 393, 751, 466, 264, 2649, 365, 29280, 2539, 13, 50714, 50714, 440, 2135, 2649, 365, 29280, 2539, 510, 307, 732, 18353, 13, 51014, 51014, 440, 700, 472, 307, 294, 29280, 2539, 11, 294, 881, 29280, 2539, 15077, 412, 1935, 11, 264, 383, 2445, 307, 257, 2211, 2424, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10749782834734235, "compression_ratio": 1.8193548387096774, "no_speech_prob": 6.043361736374209e-06}, {"id": 715, "seek": 401506, "start": 4015.06, "end": 4027.06, "text": " Well, it's a black box, not a red box.", "tokens": [50364, 1042, 11, 309, 311, 257, 2211, 2424, 11, 406, 257, 2182, 2424, 13, 50964, 50964, 2264, 11, 300, 311, 264, 700, 2649, 13, 51064, 51064, 440, 1150, 2649, 307, 300, 341, 307, 406, 257, 2128, 2316, 295, 264, 1002, 13, 51214, 51214, 639, 307, 264, 957, 1002, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.03966390628081102, "compression_ratio": 1.3916666666666666, "no_speech_prob": 3.0233044526539743e-05}, {"id": 716, "seek": 401506, "start": 4027.06, "end": 4029.06, "text": " OK, that's the first difference.", "tokens": [50364, 1042, 11, 309, 311, 257, 2211, 2424, 11, 406, 257, 2182, 2424, 13, 50964, 50964, 2264, 11, 300, 311, 264, 700, 2649, 13, 51064, 51064, 440, 1150, 2649, 307, 300, 341, 307, 406, 257, 2128, 2316, 295, 264, 1002, 13, 51214, 51214, 639, 307, 264, 957, 1002, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.03966390628081102, "compression_ratio": 1.3916666666666666, "no_speech_prob": 3.0233044526539743e-05}, {"id": 717, "seek": 401506, "start": 4029.06, "end": 4032.06, "text": " The second difference is that this is not a forward model of the world.", "tokens": [50364, 1042, 11, 309, 311, 257, 2211, 2424, 11, 406, 257, 2182, 2424, 13, 50964, 50964, 2264, 11, 300, 311, 264, 700, 2649, 13, 51064, 51064, 440, 1150, 2649, 307, 300, 341, 307, 406, 257, 2128, 2316, 295, 264, 1002, 13, 51214, 51214, 639, 307, 264, 957, 1002, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.03966390628081102, "compression_ratio": 1.3916666666666666, "no_speech_prob": 3.0233044526539743e-05}, {"id": 718, "seek": 401506, "start": 4032.06, "end": 4043.06, "text": " This is the real world.", "tokens": [50364, 1042, 11, 309, 311, 257, 2211, 2424, 11, 406, 257, 2182, 2424, 13, 50964, 50964, 2264, 11, 300, 311, 264, 700, 2649, 13, 51064, 51064, 440, 1150, 2649, 307, 300, 341, 307, 406, 257, 2128, 2316, 295, 264, 1002, 13, 51214, 51214, 639, 307, 264, 957, 1002, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.03966390628081102, "compression_ratio": 1.3916666666666666, "no_speech_prob": 3.0233044526539743e-05}, {"id": 719, "seek": 404306, "start": 4043.06, "end": 4046.06, "text": " And your measure of the state of the world is imperfect.", "tokens": [50364, 400, 428, 3481, 295, 264, 1785, 295, 264, 1002, 307, 26714, 13, 50514, 50514, 407, 1854, 295, 341, 3897, 3209, 11, 291, 1062, 362, 257, 12860, 3209, 510, 300, 20561, 264, 1785, 295, 264, 1002, 13, 50964, 50964, 407, 291, 362, 572, 1969, 670, 264, 957, 1002, 293, 428, 2063, 2445, 307, 406, 2570, 13, 51314, 51314, 509, 393, 445, 483, 264, 5598, 295, 264, 2063, 2445, 538, 445, 1382, 746, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.05793154867071854, "compression_ratio": 1.7473118279569892, "no_speech_prob": 5.421331934485352e-06}, {"id": 720, "seek": 404306, "start": 4046.06, "end": 4055.06, "text": " So inside of this policy network, you might have a perception network here that estimates the state of the world.", "tokens": [50364, 400, 428, 3481, 295, 264, 1785, 295, 264, 1002, 307, 26714, 13, 50514, 50514, 407, 1854, 295, 341, 3897, 3209, 11, 291, 1062, 362, 257, 12860, 3209, 510, 300, 20561, 264, 1785, 295, 264, 1002, 13, 50964, 50964, 407, 291, 362, 572, 1969, 670, 264, 957, 1002, 293, 428, 2063, 2445, 307, 406, 2570, 13, 51314, 51314, 509, 393, 445, 483, 264, 5598, 295, 264, 2063, 2445, 538, 445, 1382, 746, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.05793154867071854, "compression_ratio": 1.7473118279569892, "no_speech_prob": 5.421331934485352e-06}, {"id": 721, "seek": 404306, "start": 4055.06, "end": 4062.06, "text": " So you have no control over the real world and your cost function is not known.", "tokens": [50364, 400, 428, 3481, 295, 264, 1785, 295, 264, 1002, 307, 26714, 13, 50514, 50514, 407, 1854, 295, 341, 3897, 3209, 11, 291, 1062, 362, 257, 12860, 3209, 510, 300, 20561, 264, 1785, 295, 264, 1002, 13, 50964, 50964, 407, 291, 362, 572, 1969, 670, 264, 957, 1002, 293, 428, 2063, 2445, 307, 406, 2570, 13, 51314, 51314, 509, 393, 445, 483, 264, 5598, 295, 264, 2063, 2445, 538, 445, 1382, 746, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.05793154867071854, "compression_ratio": 1.7473118279569892, "no_speech_prob": 5.421331934485352e-06}, {"id": 722, "seek": 404306, "start": 4062.06, "end": 4065.06, "text": " You can just get the output of the cost function by just trying something.", "tokens": [50364, 400, 428, 3481, 295, 264, 1785, 295, 264, 1002, 307, 26714, 13, 50514, 50514, 407, 1854, 295, 341, 3897, 3209, 11, 291, 1062, 362, 257, 12860, 3209, 510, 300, 20561, 264, 1785, 295, 264, 1002, 13, 50964, 50964, 407, 291, 362, 572, 1969, 670, 264, 957, 1002, 293, 428, 2063, 2445, 307, 406, 2570, 13, 51314, 51314, 509, 393, 445, 483, 264, 5598, 295, 264, 2063, 2445, 538, 445, 1382, 746, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.05793154867071854, "compression_ratio": 1.7473118279569892, "no_speech_prob": 5.421331934485352e-06}, {"id": 723, "seek": 406506, "start": 4065.06, "end": 4075.06, "text": " You take an action, you see the effect on the world, and that gives you what reinforcement learning people call a reward, but it's just a negative cost.", "tokens": [50364, 509, 747, 364, 3069, 11, 291, 536, 264, 1802, 322, 264, 1002, 11, 293, 300, 2709, 291, 437, 29280, 2539, 561, 818, 257, 7782, 11, 457, 309, 311, 445, 257, 3671, 2063, 13, 50864, 50864, 467, 311, 264, 3671, 2158, 295, 428, 2063, 13, 51064, 51064, 583, 264, 2063, 307, 406, 819, 9364, 13, 51164, 51164, 509, 500, 380, 458, 264, 2445, 295, 264, 2063, 13, 51214, 51214, 509, 362, 281, 352, 807, 264, 1002, 281, 2573, 484, 264, 2158, 295, 264, 2063, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06779589978131381, "compression_ratio": 1.6766169154228856, "no_speech_prob": 5.861650606675539e-06}, {"id": 724, "seek": 406506, "start": 4075.06, "end": 4079.06, "text": " It's the negative value of your cost.", "tokens": [50364, 509, 747, 364, 3069, 11, 291, 536, 264, 1802, 322, 264, 1002, 11, 293, 300, 2709, 291, 437, 29280, 2539, 561, 818, 257, 7782, 11, 457, 309, 311, 445, 257, 3671, 2063, 13, 50864, 50864, 467, 311, 264, 3671, 2158, 295, 428, 2063, 13, 51064, 51064, 583, 264, 2063, 307, 406, 819, 9364, 13, 51164, 51164, 509, 500, 380, 458, 264, 2445, 295, 264, 2063, 13, 51214, 51214, 509, 362, 281, 352, 807, 264, 1002, 281, 2573, 484, 264, 2158, 295, 264, 2063, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06779589978131381, "compression_ratio": 1.6766169154228856, "no_speech_prob": 5.861650606675539e-06}, {"id": 725, "seek": 406506, "start": 4079.06, "end": 4081.06, "text": " But the cost is not differentiable.", "tokens": [50364, 509, 747, 364, 3069, 11, 291, 536, 264, 1802, 322, 264, 1002, 11, 293, 300, 2709, 291, 437, 29280, 2539, 561, 818, 257, 7782, 11, 457, 309, 311, 445, 257, 3671, 2063, 13, 50864, 50864, 467, 311, 264, 3671, 2158, 295, 428, 2063, 13, 51064, 51064, 583, 264, 2063, 307, 406, 819, 9364, 13, 51164, 51164, 509, 500, 380, 458, 264, 2445, 295, 264, 2063, 13, 51214, 51214, 509, 362, 281, 352, 807, 264, 1002, 281, 2573, 484, 264, 2158, 295, 264, 2063, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06779589978131381, "compression_ratio": 1.6766169154228856, "no_speech_prob": 5.861650606675539e-06}, {"id": 726, "seek": 406506, "start": 4081.06, "end": 4082.06, "text": " You don't know the function of the cost.", "tokens": [50364, 509, 747, 364, 3069, 11, 291, 536, 264, 1802, 322, 264, 1002, 11, 293, 300, 2709, 291, 437, 29280, 2539, 561, 818, 257, 7782, 11, 457, 309, 311, 445, 257, 3671, 2063, 13, 50864, 50864, 467, 311, 264, 3671, 2158, 295, 428, 2063, 13, 51064, 51064, 583, 264, 2063, 307, 406, 819, 9364, 13, 51164, 51164, 509, 500, 380, 458, 264, 2445, 295, 264, 2063, 13, 51214, 51214, 509, 362, 281, 352, 807, 264, 1002, 281, 2573, 484, 264, 2158, 295, 264, 2063, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06779589978131381, "compression_ratio": 1.6766169154228856, "no_speech_prob": 5.861650606675539e-06}, {"id": 727, "seek": 406506, "start": 4082.06, "end": 4089.06, "text": " You have to go through the world to figure out the value of the cost.", "tokens": [50364, 509, 747, 364, 3069, 11, 291, 536, 264, 1802, 322, 264, 1002, 11, 293, 300, 2709, 291, 437, 29280, 2539, 561, 818, 257, 7782, 11, 457, 309, 311, 445, 257, 3671, 2063, 13, 50864, 50864, 467, 311, 264, 3671, 2158, 295, 428, 2063, 13, 51064, 51064, 583, 264, 2063, 307, 406, 819, 9364, 13, 51164, 51164, 509, 500, 380, 458, 264, 2445, 295, 264, 2063, 13, 51214, 51214, 509, 362, 281, 352, 807, 264, 1002, 281, 2573, 484, 264, 2158, 295, 264, 2063, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06779589978131381, "compression_ratio": 1.6766169154228856, "no_speech_prob": 5.861650606675539e-06}, {"id": 728, "seek": 408906, "start": 4089.06, "end": 4097.0599999999995, "text": " And that's the main issue with reinforcement learning, which is that the cost function is not differentiable.", "tokens": [50364, 400, 300, 311, 264, 2135, 2734, 365, 29280, 2539, 11, 597, 307, 300, 264, 2063, 2445, 307, 406, 819, 9364, 13, 50764, 50764, 467, 311, 9841, 13, 50814, 50814, 440, 787, 636, 281, 12539, 309, 307, 538, 1382, 746, 293, 550, 22107, 264, 2158, 11, 597, 307, 437, 264, 7782, 307, 11, 534, 13, 51114, 51114, 467, 311, 257, 3671, 13, 51164, 51164, 440, 3671, 295, 264, 7782, 307, 1936, 428, 2063, 13, 51414, 51414, 407, 294, 300, 2590, 11, 1670, 291, 2644, 13059, 2771, 2448, 11, 281, 17522, 428, 2063, 11, 291, 362, 281, 853, 3866, 721, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06957374035733417, "compression_ratio": 1.7028112449799198, "no_speech_prob": 1.3209659300628118e-05}, {"id": 729, "seek": 408906, "start": 4097.0599999999995, "end": 4098.0599999999995, "text": " It's unknown.", "tokens": [50364, 400, 300, 311, 264, 2135, 2734, 365, 29280, 2539, 11, 597, 307, 300, 264, 2063, 2445, 307, 406, 819, 9364, 13, 50764, 50764, 467, 311, 9841, 13, 50814, 50814, 440, 787, 636, 281, 12539, 309, 307, 538, 1382, 746, 293, 550, 22107, 264, 2158, 11, 597, 307, 437, 264, 7782, 307, 11, 534, 13, 51114, 51114, 467, 311, 257, 3671, 13, 51164, 51164, 440, 3671, 295, 264, 7782, 307, 1936, 428, 2063, 13, 51414, 51414, 407, 294, 300, 2590, 11, 1670, 291, 2644, 13059, 2771, 2448, 11, 281, 17522, 428, 2063, 11, 291, 362, 281, 853, 3866, 721, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06957374035733417, "compression_ratio": 1.7028112449799198, "no_speech_prob": 1.3209659300628118e-05}, {"id": 730, "seek": 408906, "start": 4098.0599999999995, "end": 4104.0599999999995, "text": " The only way to estimate it is by trying something and then observing the value, which is what the reward is, really.", "tokens": [50364, 400, 300, 311, 264, 2135, 2734, 365, 29280, 2539, 11, 597, 307, 300, 264, 2063, 2445, 307, 406, 819, 9364, 13, 50764, 50764, 467, 311, 9841, 13, 50814, 50814, 440, 787, 636, 281, 12539, 309, 307, 538, 1382, 746, 293, 550, 22107, 264, 2158, 11, 597, 307, 437, 264, 7782, 307, 11, 534, 13, 51114, 51114, 467, 311, 257, 3671, 13, 51164, 51164, 440, 3671, 295, 264, 7782, 307, 1936, 428, 2063, 13, 51414, 51414, 407, 294, 300, 2590, 11, 1670, 291, 2644, 13059, 2771, 2448, 11, 281, 17522, 428, 2063, 11, 291, 362, 281, 853, 3866, 721, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06957374035733417, "compression_ratio": 1.7028112449799198, "no_speech_prob": 1.3209659300628118e-05}, {"id": 731, "seek": 408906, "start": 4104.0599999999995, "end": 4105.0599999999995, "text": " It's a negative.", "tokens": [50364, 400, 300, 311, 264, 2135, 2734, 365, 29280, 2539, 11, 597, 307, 300, 264, 2063, 2445, 307, 406, 819, 9364, 13, 50764, 50764, 467, 311, 9841, 13, 50814, 50814, 440, 787, 636, 281, 12539, 309, 307, 538, 1382, 746, 293, 550, 22107, 264, 2158, 11, 597, 307, 437, 264, 7782, 307, 11, 534, 13, 51114, 51114, 467, 311, 257, 3671, 13, 51164, 51164, 440, 3671, 295, 264, 7782, 307, 1936, 428, 2063, 13, 51414, 51414, 407, 294, 300, 2590, 11, 1670, 291, 2644, 13059, 2771, 2448, 11, 281, 17522, 428, 2063, 11, 291, 362, 281, 853, 3866, 721, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06957374035733417, "compression_ratio": 1.7028112449799198, "no_speech_prob": 1.3209659300628118e-05}, {"id": 732, "seek": 408906, "start": 4105.0599999999995, "end": 4110.0599999999995, "text": " The negative of the reward is basically your cost.", "tokens": [50364, 400, 300, 311, 264, 2135, 2734, 365, 29280, 2539, 11, 597, 307, 300, 264, 2063, 2445, 307, 406, 819, 9364, 13, 50764, 50764, 467, 311, 9841, 13, 50814, 50814, 440, 787, 636, 281, 12539, 309, 307, 538, 1382, 746, 293, 550, 22107, 264, 2158, 11, 597, 307, 437, 264, 7782, 307, 11, 534, 13, 51114, 51114, 467, 311, 257, 3671, 13, 51164, 51164, 440, 3671, 295, 264, 7782, 307, 1936, 428, 2063, 13, 51414, 51414, 407, 294, 300, 2590, 11, 1670, 291, 2644, 13059, 2771, 2448, 11, 281, 17522, 428, 2063, 11, 291, 362, 281, 853, 3866, 721, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06957374035733417, "compression_ratio": 1.7028112449799198, "no_speech_prob": 1.3209659300628118e-05}, {"id": 733, "seek": 408906, "start": 4110.0599999999995, "end": 4117.0599999999995, "text": " So in that situation, since you cannot evaluate gradients, to minimize your cost, you have to try multiple things.", "tokens": [50364, 400, 300, 311, 264, 2135, 2734, 365, 29280, 2539, 11, 597, 307, 300, 264, 2063, 2445, 307, 406, 819, 9364, 13, 50764, 50764, 467, 311, 9841, 13, 50814, 50814, 440, 787, 636, 281, 12539, 309, 307, 538, 1382, 746, 293, 550, 22107, 264, 2158, 11, 597, 307, 437, 264, 7782, 307, 11, 534, 13, 51114, 51114, 467, 311, 257, 3671, 13, 51164, 51164, 440, 3671, 295, 264, 7782, 307, 1936, 428, 2063, 13, 51414, 51414, 407, 294, 300, 2590, 11, 1670, 291, 2644, 13059, 2771, 2448, 11, 281, 17522, 428, 2063, 11, 291, 362, 281, 853, 3866, 721, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06957374035733417, "compression_ratio": 1.7028112449799198, "no_speech_prob": 1.3209659300628118e-05}, {"id": 734, "seek": 411706, "start": 4117.06, "end": 4126.06, "text": " You have to try an action, see the result, and then try another action, see if the result is better, and then try another action, see if the result is better.", "tokens": [50364, 509, 362, 281, 853, 364, 3069, 11, 536, 264, 1874, 11, 293, 550, 853, 1071, 3069, 11, 536, 498, 264, 1874, 307, 1101, 11, 293, 550, 853, 1071, 3069, 11, 536, 498, 264, 1874, 307, 1101, 13, 50814, 50814, 400, 498, 428, 2063, 2445, 307, 588, 4962, 11, 291, 362, 281, 853, 867, 11, 867, 721, 949, 291, 483, 257, 2107, 12, 32226, 7782, 420, 257, 2107, 12, 21454, 2063, 13, 51264, 51264, 400, 370, 300, 311, 689, 264, 14024, 1709, 13, 51414, 51414, 821, 307, 264, 4497, 1154, 295, 16197, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.05176381766796112, "compression_ratio": 1.917948717948718, "no_speech_prob": 1.3628595297632273e-05}, {"id": 735, "seek": 411706, "start": 4126.06, "end": 4135.06, "text": " And if your cost function is very flat, you have to try many, many things before you get a non-zero reward or a non-high cost.", "tokens": [50364, 509, 362, 281, 853, 364, 3069, 11, 536, 264, 1874, 11, 293, 550, 853, 1071, 3069, 11, 536, 498, 264, 1874, 307, 1101, 11, 293, 550, 853, 1071, 3069, 11, 536, 498, 264, 1874, 307, 1101, 13, 50814, 50814, 400, 498, 428, 2063, 2445, 307, 588, 4962, 11, 291, 362, 281, 853, 867, 11, 867, 721, 949, 291, 483, 257, 2107, 12, 32226, 7782, 420, 257, 2107, 12, 21454, 2063, 13, 51264, 51264, 400, 370, 300, 311, 689, 264, 14024, 1709, 13, 51414, 51414, 821, 307, 264, 4497, 1154, 295, 16197, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.05176381766796112, "compression_ratio": 1.917948717948718, "no_speech_prob": 1.3628595297632273e-05}, {"id": 736, "seek": 411706, "start": 4135.06, "end": 4138.06, "text": " And so that's where the complexity goes.", "tokens": [50364, 509, 362, 281, 853, 364, 3069, 11, 536, 264, 1874, 11, 293, 550, 853, 1071, 3069, 11, 536, 498, 264, 1874, 307, 1101, 11, 293, 550, 853, 1071, 3069, 11, 536, 498, 264, 1874, 307, 1101, 13, 50814, 50814, 400, 498, 428, 2063, 2445, 307, 588, 4962, 11, 291, 362, 281, 853, 867, 11, 867, 721, 949, 291, 483, 257, 2107, 12, 32226, 7782, 420, 257, 2107, 12, 21454, 2063, 13, 51264, 51264, 400, 370, 300, 311, 689, 264, 14024, 1709, 13, 51414, 51414, 821, 307, 264, 4497, 1154, 295, 16197, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.05176381766796112, "compression_ratio": 1.917948717948718, "no_speech_prob": 1.3628595297632273e-05}, {"id": 737, "seek": 411706, "start": 4138.06, "end": 4141.06, "text": " There is the additional problem of exploration.", "tokens": [50364, 509, 362, 281, 853, 364, 3069, 11, 536, 264, 1874, 11, 293, 550, 853, 1071, 3069, 11, 536, 498, 264, 1874, 307, 1101, 11, 293, 550, 853, 1071, 3069, 11, 536, 498, 264, 1874, 307, 1101, 13, 50814, 50814, 400, 498, 428, 2063, 2445, 307, 588, 4962, 11, 291, 362, 281, 853, 867, 11, 867, 721, 949, 291, 483, 257, 2107, 12, 32226, 7782, 420, 257, 2107, 12, 21454, 2063, 13, 51264, 51264, 400, 370, 300, 311, 689, 264, 14024, 1709, 13, 51414, 51414, 821, 307, 264, 4497, 1154, 295, 16197, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.05176381766796112, "compression_ratio": 1.917948717948718, "no_speech_prob": 1.3628595297632273e-05}, {"id": 738, "seek": 414106, "start": 4141.06, "end": 4159.06, "text": " So because you don't know the form of the cost and because it's non-differentiable, you might need to kind of try many actions in kind of a smart way to figure out in which part of the space to go to be able to sort of figure out how can I improve my performance.", "tokens": [50364, 407, 570, 291, 500, 380, 458, 264, 1254, 295, 264, 2063, 293, 570, 309, 311, 2107, 12, 67, 15790, 9364, 11, 291, 1062, 643, 281, 733, 295, 853, 867, 5909, 294, 733, 295, 257, 4069, 636, 281, 2573, 484, 294, 597, 644, 295, 264, 1901, 281, 352, 281, 312, 1075, 281, 1333, 295, 2573, 484, 577, 393, 286, 3470, 452, 3389, 13, 51264, 51264, 2264, 11, 370, 300, 311, 264, 2135, 2734, 295, 16197, 13, 51564, 51564, 400, 550, 456, 307, 264, 2734, 295, 16197, 5717, 33122, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06884395557901134, "compression_ratio": 1.6986301369863013, "no_speech_prob": 6.853784725535661e-06}, {"id": 739, "seek": 414106, "start": 4159.06, "end": 4165.06, "text": " OK, so that's the main issue of exploration.", "tokens": [50364, 407, 570, 291, 500, 380, 458, 264, 1254, 295, 264, 2063, 293, 570, 309, 311, 2107, 12, 67, 15790, 9364, 11, 291, 1062, 643, 281, 733, 295, 853, 867, 5909, 294, 733, 295, 257, 4069, 636, 281, 2573, 484, 294, 597, 644, 295, 264, 1901, 281, 352, 281, 312, 1075, 281, 1333, 295, 2573, 484, 577, 393, 286, 3470, 452, 3389, 13, 51264, 51264, 2264, 11, 370, 300, 311, 264, 2135, 2734, 295, 16197, 13, 51564, 51564, 400, 550, 456, 307, 264, 2734, 295, 16197, 5717, 33122, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06884395557901134, "compression_ratio": 1.6986301369863013, "no_speech_prob": 6.853784725535661e-06}, {"id": 740, "seek": 414106, "start": 4165.06, "end": 4169.06, "text": " And then there is the issue of exploration versus exploitation.", "tokens": [50364, 407, 570, 291, 500, 380, 458, 264, 1254, 295, 264, 2063, 293, 570, 309, 311, 2107, 12, 67, 15790, 9364, 11, 291, 1062, 643, 281, 733, 295, 853, 867, 5909, 294, 733, 295, 257, 4069, 636, 281, 2573, 484, 294, 597, 644, 295, 264, 1901, 281, 352, 281, 312, 1075, 281, 1333, 295, 2573, 484, 577, 393, 286, 3470, 452, 3389, 13, 51264, 51264, 2264, 11, 370, 300, 311, 264, 2135, 2734, 295, 16197, 13, 51564, 51564, 400, 550, 456, 307, 264, 2734, 295, 16197, 5717, 33122, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06884395557901134, "compression_ratio": 1.6986301369863013, "no_speech_prob": 6.853784725535661e-06}, {"id": 741, "seek": 416906, "start": 4169.06, "end": 4177.06, "text": " So the fact that when you're on a situation, you don't want to take completely random actions because they're likely to not result in anything interesting.", "tokens": [50364, 407, 264, 1186, 300, 562, 291, 434, 322, 257, 2590, 11, 291, 500, 380, 528, 281, 747, 2584, 4974, 5909, 570, 436, 434, 3700, 281, 406, 1874, 294, 1340, 1880, 13, 50764, 50764, 407, 291, 528, 281, 747, 5909, 300, 366, 733, 295, 1998, 281, 437, 291, 519, 1062, 589, 293, 1333, 295, 16895, 733, 295, 853, 746, 1646, 1339, 291, 434, 2539, 293, 1466, 428, 3897, 382, 291, 352, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.07707753499348959, "compression_ratio": 1.6764705882352942, "no_speech_prob": 1.3208565178501885e-05}, {"id": 742, "seek": 416906, "start": 4177.06, "end": 4189.06, "text": " So you want to take actions that are kind of close to what you think might work and sort of occasionally kind of try something else while you're learning and learn your policy as you go.", "tokens": [50364, 407, 264, 1186, 300, 562, 291, 434, 322, 257, 2590, 11, 291, 500, 380, 528, 281, 747, 2584, 4974, 5909, 570, 436, 434, 3700, 281, 406, 1874, 294, 1340, 1880, 13, 50764, 50764, 407, 291, 528, 281, 747, 5909, 300, 366, 733, 295, 1998, 281, 437, 291, 519, 1062, 589, 293, 1333, 295, 16895, 733, 295, 853, 746, 1646, 1339, 291, 434, 2539, 293, 1466, 428, 3897, 382, 291, 352, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.07707753499348959, "compression_ratio": 1.6764705882352942, "no_speech_prob": 1.3208565178501885e-05}, {"id": 743, "seek": 418906, "start": 4189.06, "end": 4205.06, "text": " What I'm describing, what I was describing just before, is a situation where you can do all of this in your head because you have a model of the world and you can optimize your sequence of action very efficiently because you have a differentiable cost function.", "tokens": [50364, 708, 286, 478, 16141, 11, 437, 286, 390, 16141, 445, 949, 11, 307, 257, 2590, 689, 291, 393, 360, 439, 295, 341, 294, 428, 1378, 570, 291, 362, 257, 2316, 295, 264, 1002, 293, 291, 393, 19719, 428, 8310, 295, 3069, 588, 19621, 570, 291, 362, 257, 819, 9364, 2063, 2445, 13, 51164, 51164, 2260, 2063, 2445, 307, 40610, 538, 428, 1065, 3567, 11, 498, 291, 528, 11, 1854, 295, 428, 9461, 13, 51414, 51414, 509, 393, 980, 498, 291, 4444, 264, 3435, 11, 291, 393, 980, 264, 4560, 1296, 428, 1011, 293, 264, 3435, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05686873912811279, "compression_ratio": 1.8057851239669422, "no_speech_prob": 8.529617844033055e-06}, {"id": 744, "seek": 418906, "start": 4205.06, "end": 4210.06, "text": " Your cost function is computed by your own brain, if you want, inside of your agent.", "tokens": [50364, 708, 286, 478, 16141, 11, 437, 286, 390, 16141, 445, 949, 11, 307, 257, 2590, 689, 291, 393, 360, 439, 295, 341, 294, 428, 1378, 570, 291, 362, 257, 2316, 295, 264, 1002, 293, 291, 393, 19719, 428, 8310, 295, 3069, 588, 19621, 570, 291, 362, 257, 819, 9364, 2063, 2445, 13, 51164, 51164, 2260, 2063, 2445, 307, 40610, 538, 428, 1065, 3567, 11, 498, 291, 528, 11, 1854, 295, 428, 9461, 13, 51414, 51414, 509, 393, 980, 498, 291, 4444, 264, 3435, 11, 291, 393, 980, 264, 4560, 1296, 428, 1011, 293, 264, 3435, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05686873912811279, "compression_ratio": 1.8057851239669422, "no_speech_prob": 8.529617844033055e-06}, {"id": 745, "seek": 418906, "start": 4210.06, "end": 4216.06, "text": " You can tell if you grab the pen, you can tell the distance between your hand and the pen.", "tokens": [50364, 708, 286, 478, 16141, 11, 437, 286, 390, 16141, 445, 949, 11, 307, 257, 2590, 689, 291, 393, 360, 439, 295, 341, 294, 428, 1378, 570, 291, 362, 257, 2316, 295, 264, 1002, 293, 291, 393, 19719, 428, 8310, 295, 3069, 588, 19621, 570, 291, 362, 257, 819, 9364, 2063, 2445, 13, 51164, 51164, 2260, 2063, 2445, 307, 40610, 538, 428, 1065, 3567, 11, 498, 291, 528, 11, 1854, 295, 428, 9461, 13, 51414, 51414, 509, 393, 980, 498, 291, 4444, 264, 3435, 11, 291, 393, 980, 264, 4560, 1296, 428, 1011, 293, 264, 3435, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05686873912811279, "compression_ratio": 1.8057851239669422, "no_speech_prob": 8.529617844033055e-06}, {"id": 746, "seek": 421606, "start": 4216.06, "end": 4222.06, "text": " So you can compute your own cost function and it is kind of in your internal world model is differentiable.", "tokens": [50364, 407, 291, 393, 14722, 428, 1065, 2063, 2445, 293, 309, 307, 733, 295, 294, 428, 6920, 1002, 2316, 307, 819, 9364, 13, 50664, 50664, 682, 264, 957, 1002, 11, 309, 311, 406, 13, 682, 264, 957, 1002, 11, 291, 500, 380, 458, 264, 13760, 295, 264, 4560, 295, 428, 1011, 281, 264, 3435, 5969, 291, 362, 512, 2316, 295, 300, 294, 428, 1378, 13, 51114, 51114, 583, 538, 7576, 11, 291, 500, 380, 13, 583, 570, 1203, 307, 294, 428, 1378, 11, 1203, 307, 819, 9364, 13, 51314, 51314, 5471, 307, 12270, 538, 18161, 2533, 293, 1203, 291, 393, 646, 48256, 16235, 807, 1203, 13, 51564, 51564, 407, 300, 311, 264, 955, 5002, 295, 341, 733, 295, 3109, 5717, 29280, 2539, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0889249150715177, "compression_ratio": 1.9581881533101044, "no_speech_prob": 1.280230480915634e-05}, {"id": 747, "seek": 421606, "start": 4222.06, "end": 4231.06, "text": " In the real world, it's not. In the real world, you don't know the derivative of the distance of your hand to the pen unless you have some model of that in your head.", "tokens": [50364, 407, 291, 393, 14722, 428, 1065, 2063, 2445, 293, 309, 307, 733, 295, 294, 428, 6920, 1002, 2316, 307, 819, 9364, 13, 50664, 50664, 682, 264, 957, 1002, 11, 309, 311, 406, 13, 682, 264, 957, 1002, 11, 291, 500, 380, 458, 264, 13760, 295, 264, 4560, 295, 428, 1011, 281, 264, 3435, 5969, 291, 362, 512, 2316, 295, 300, 294, 428, 1378, 13, 51114, 51114, 583, 538, 7576, 11, 291, 500, 380, 13, 583, 570, 1203, 307, 294, 428, 1378, 11, 1203, 307, 819, 9364, 13, 51314, 51314, 5471, 307, 12270, 538, 18161, 2533, 293, 1203, 291, 393, 646, 48256, 16235, 807, 1203, 13, 51564, 51564, 407, 300, 311, 264, 955, 5002, 295, 341, 733, 295, 3109, 5717, 29280, 2539, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0889249150715177, "compression_ratio": 1.9581881533101044, "no_speech_prob": 1.280230480915634e-05}, {"id": 748, "seek": 421606, "start": 4231.06, "end": 4235.06, "text": " But by default, you don't. But because everything is in your head, everything is differentiable.", "tokens": [50364, 407, 291, 393, 14722, 428, 1065, 2063, 2445, 293, 309, 307, 733, 295, 294, 428, 6920, 1002, 2316, 307, 819, 9364, 13, 50664, 50664, 682, 264, 957, 1002, 11, 309, 311, 406, 13, 682, 264, 957, 1002, 11, 291, 500, 380, 458, 264, 13760, 295, 264, 4560, 295, 428, 1011, 281, 264, 3435, 5969, 291, 362, 512, 2316, 295, 300, 294, 428, 1378, 13, 51114, 51114, 583, 538, 7576, 11, 291, 500, 380, 13, 583, 570, 1203, 307, 294, 428, 1378, 11, 1203, 307, 819, 9364, 13, 51314, 51314, 5471, 307, 12270, 538, 18161, 2533, 293, 1203, 291, 393, 646, 48256, 16235, 807, 1203, 13, 51564, 51564, 407, 300, 311, 264, 955, 5002, 295, 341, 733, 295, 3109, 5717, 29280, 2539, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0889249150715177, "compression_ratio": 1.9581881533101044, "no_speech_prob": 1.280230480915634e-05}, {"id": 749, "seek": 421606, "start": 4235.06, "end": 4240.06, "text": " Everything is implemented by neural net and everything you can back propagate gradient through everything.", "tokens": [50364, 407, 291, 393, 14722, 428, 1065, 2063, 2445, 293, 309, 307, 733, 295, 294, 428, 6920, 1002, 2316, 307, 819, 9364, 13, 50664, 50664, 682, 264, 957, 1002, 11, 309, 311, 406, 13, 682, 264, 957, 1002, 11, 291, 500, 380, 458, 264, 13760, 295, 264, 4560, 295, 428, 1011, 281, 264, 3435, 5969, 291, 362, 512, 2316, 295, 300, 294, 428, 1378, 13, 51114, 51114, 583, 538, 7576, 11, 291, 500, 380, 13, 583, 570, 1203, 307, 294, 428, 1378, 11, 1203, 307, 819, 9364, 13, 51314, 51314, 5471, 307, 12270, 538, 18161, 2533, 293, 1203, 291, 393, 646, 48256, 16235, 807, 1203, 13, 51564, 51564, 407, 300, 311, 264, 955, 5002, 295, 341, 733, 295, 3109, 5717, 29280, 2539, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0889249150715177, "compression_ratio": 1.9581881533101044, "no_speech_prob": 1.280230480915634e-05}, {"id": 750, "seek": 421606, "start": 4240.06, "end": 4244.06, "text": " So that's the big advantage of this kind of approach versus reinforcement learning.", "tokens": [50364, 407, 291, 393, 14722, 428, 1065, 2063, 2445, 293, 309, 307, 733, 295, 294, 428, 6920, 1002, 2316, 307, 819, 9364, 13, 50664, 50664, 682, 264, 957, 1002, 11, 309, 311, 406, 13, 682, 264, 957, 1002, 11, 291, 500, 380, 458, 264, 13760, 295, 264, 4560, 295, 428, 1011, 281, 264, 3435, 5969, 291, 362, 512, 2316, 295, 300, 294, 428, 1378, 13, 51114, 51114, 583, 538, 7576, 11, 291, 500, 380, 13, 583, 570, 1203, 307, 294, 428, 1378, 11, 1203, 307, 819, 9364, 13, 51314, 51314, 5471, 307, 12270, 538, 18161, 2533, 293, 1203, 291, 393, 646, 48256, 16235, 807, 1203, 13, 51564, 51564, 407, 300, 311, 264, 955, 5002, 295, 341, 733, 295, 3109, 5717, 29280, 2539, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0889249150715177, "compression_ratio": 1.9581881533101044, "no_speech_prob": 1.280230480915634e-05}, {"id": 751, "seek": 424406, "start": 4244.06, "end": 4248.06, "text": " Make everything differentiable. So there's two problems with the world.", "tokens": [50364, 4387, 1203, 819, 9364, 13, 407, 456, 311, 732, 2740, 365, 264, 1002, 13, 50564, 50564, 407, 456, 311, 472, 955, 5002, 294, 341, 733, 295, 9005, 11, 597, 307, 291, 393, 1190, 341, 4663, 813, 957, 565, 570, 428, 2128, 2316, 1854, 295, 428, 9461, 393, 1190, 382, 2370, 382, 291, 528, 13, 51164, 51164, 509, 500, 380, 643, 281, 1190, 807, 264, 1002, 13, 663, 311, 472, 5002, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.0813922627766927, "compression_ratio": 1.6526315789473685, "no_speech_prob": 2.2123005692264996e-05}, {"id": 752, "seek": 424406, "start": 4248.06, "end": 4260.06, "text": " So there's one big advantage in this kind of scenario, which is you can run this faster than real time because your forward model inside of your agent can run as fast as you want.", "tokens": [50364, 4387, 1203, 819, 9364, 13, 407, 456, 311, 732, 2740, 365, 264, 1002, 13, 50564, 50564, 407, 456, 311, 472, 955, 5002, 294, 341, 733, 295, 9005, 11, 597, 307, 291, 393, 1190, 341, 4663, 813, 957, 565, 570, 428, 2128, 2316, 1854, 295, 428, 9461, 393, 1190, 382, 2370, 382, 291, 528, 13, 51164, 51164, 509, 500, 380, 643, 281, 1190, 807, 264, 1002, 13, 663, 311, 472, 5002, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.0813922627766927, "compression_ratio": 1.6526315789473685, "no_speech_prob": 2.2123005692264996e-05}, {"id": 753, "seek": 424406, "start": 4260.06, "end": 4264.06, "text": " You don't need to run through the world. That's one advantage.", "tokens": [50364, 4387, 1203, 819, 9364, 13, 407, 456, 311, 732, 2740, 365, 264, 1002, 13, 50564, 50564, 407, 456, 311, 472, 955, 5002, 294, 341, 733, 295, 9005, 11, 597, 307, 291, 393, 1190, 341, 4663, 813, 957, 565, 570, 428, 2128, 2316, 1854, 295, 428, 9461, 393, 1190, 382, 2370, 382, 291, 528, 13, 51164, 51164, 509, 500, 380, 643, 281, 1190, 807, 264, 1002, 13, 663, 311, 472, 5002, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.0813922627766927, "compression_ratio": 1.6526315789473685, "no_speech_prob": 2.2123005692264996e-05}, {"id": 754, "seek": 426406, "start": 4264.06, "end": 4277.06, "text": " Second advantage is the actions you're taking will not kill you because you can predict using your forward model. Maybe you'll predict that the action will kill you, but you're not going to take it in the real world.", "tokens": [50364, 5736, 5002, 307, 264, 5909, 291, 434, 1940, 486, 406, 1961, 291, 570, 291, 393, 6069, 1228, 428, 2128, 2316, 13, 2704, 291, 603, 6069, 300, 264, 3069, 486, 1961, 291, 11, 457, 291, 434, 406, 516, 281, 747, 309, 294, 264, 957, 1002, 13, 51014, 51014, 407, 309, 1582, 380, 1961, 291, 498, 291, 362, 364, 8559, 2128, 2316, 13, 51314, 51314, 12548, 5002, 11, 570, 1203, 2516, 1081, 294, 428, 1378, 11, 1203, 307, 294, 18161, 2533, 11, 1203, 307, 819, 9364, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0851322024056081, "compression_ratio": 1.837962962962963, "no_speech_prob": 2.260035444123787e-06}, {"id": 755, "seek": 426406, "start": 4277.06, "end": 4283.06, "text": " So it won't kill you if you have an accurate forward model.", "tokens": [50364, 5736, 5002, 307, 264, 5909, 291, 434, 1940, 486, 406, 1961, 291, 570, 291, 393, 6069, 1228, 428, 2128, 2316, 13, 2704, 291, 603, 6069, 300, 264, 3069, 486, 1961, 291, 11, 457, 291, 434, 406, 516, 281, 747, 309, 294, 264, 957, 1002, 13, 51014, 51014, 407, 309, 1582, 380, 1961, 291, 498, 291, 362, 364, 8559, 2128, 2316, 13, 51314, 51314, 12548, 5002, 11, 570, 1203, 2516, 1081, 294, 428, 1378, 11, 1203, 307, 294, 18161, 2533, 11, 1203, 307, 819, 9364, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0851322024056081, "compression_ratio": 1.837962962962963, "no_speech_prob": 2.260035444123787e-06}, {"id": 756, "seek": 426406, "start": 4283.06, "end": 4288.06, "text": " Third advantage, because everything takes place in your head, everything is in neural net, everything is differentiable.", "tokens": [50364, 5736, 5002, 307, 264, 5909, 291, 434, 1940, 486, 406, 1961, 291, 570, 291, 393, 6069, 1228, 428, 2128, 2316, 13, 2704, 291, 603, 6069, 300, 264, 3069, 486, 1961, 291, 11, 457, 291, 434, 406, 516, 281, 747, 309, 294, 264, 957, 1002, 13, 51014, 51014, 407, 309, 1582, 380, 1961, 291, 498, 291, 362, 364, 8559, 2128, 2316, 13, 51314, 51314, 12548, 5002, 11, 570, 1203, 2516, 1081, 294, 428, 1378, 11, 1203, 307, 294, 18161, 2533, 11, 1203, 307, 819, 9364, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0851322024056081, "compression_ratio": 1.837962962962963, "no_speech_prob": 2.260035444123787e-06}, {"id": 757, "seek": 428806, "start": 4288.06, "end": 4298.06, "text": " You can use all kinds of efficient learning or inference algorithms to figure out a good course of actions.", "tokens": [50364, 509, 393, 764, 439, 3685, 295, 7148, 2539, 420, 38253, 14642, 281, 2573, 484, 257, 665, 1164, 295, 5909, 13, 50864, 50864, 2264, 11, 370, 300, 311, 264, 2649, 365, 29280, 2539, 13, 51064, 51064, 682, 29280, 2539, 11, 291, 434, 3585, 1803, 11, 286, 362, 281, 352, 807, 264, 957, 1002, 13, 51464, 51464, 286, 500, 380, 362, 257, 2316, 295, 264, 957, 1002, 13, 286, 500, 380, 458, 577, 281, 14722, 264, 2063, 2445, 294, 257, 819, 9364, 636, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07751564092414323, "compression_ratio": 1.721698113207547, "no_speech_prob": 3.7251957110129297e-06}, {"id": 758, "seek": 428806, "start": 4298.06, "end": 4302.06, "text": " OK, so that's the difference with reinforcement learning.", "tokens": [50364, 509, 393, 764, 439, 3685, 295, 7148, 2539, 420, 38253, 14642, 281, 2573, 484, 257, 665, 1164, 295, 5909, 13, 50864, 50864, 2264, 11, 370, 300, 311, 264, 2649, 365, 29280, 2539, 13, 51064, 51064, 682, 29280, 2539, 11, 291, 434, 3585, 1803, 11, 286, 362, 281, 352, 807, 264, 957, 1002, 13, 51464, 51464, 286, 500, 380, 362, 257, 2316, 295, 264, 957, 1002, 13, 286, 500, 380, 458, 577, 281, 14722, 264, 2063, 2445, 294, 257, 819, 9364, 636, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07751564092414323, "compression_ratio": 1.721698113207547, "no_speech_prob": 3.7251957110129297e-06}, {"id": 759, "seek": 428806, "start": 4302.06, "end": 4310.06, "text": " In reinforcement learning, you're telling yourself, I have to go through the real world.", "tokens": [50364, 509, 393, 764, 439, 3685, 295, 7148, 2539, 420, 38253, 14642, 281, 2573, 484, 257, 665, 1164, 295, 5909, 13, 50864, 50864, 2264, 11, 370, 300, 311, 264, 2649, 365, 29280, 2539, 13, 51064, 51064, 682, 29280, 2539, 11, 291, 434, 3585, 1803, 11, 286, 362, 281, 352, 807, 264, 957, 1002, 13, 51464, 51464, 286, 500, 380, 362, 257, 2316, 295, 264, 957, 1002, 13, 286, 500, 380, 458, 577, 281, 14722, 264, 2063, 2445, 294, 257, 819, 9364, 636, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07751564092414323, "compression_ratio": 1.721698113207547, "no_speech_prob": 3.7251957110129297e-06}, {"id": 760, "seek": 428806, "start": 4310.06, "end": 4315.06, "text": " I don't have a model of the real world. I don't know how to compute the cost function in a differentiable way.", "tokens": [50364, 509, 393, 764, 439, 3685, 295, 7148, 2539, 420, 38253, 14642, 281, 2573, 484, 257, 665, 1164, 295, 5909, 13, 50864, 50864, 2264, 11, 370, 300, 311, 264, 2649, 365, 29280, 2539, 13, 51064, 51064, 682, 29280, 2539, 11, 291, 434, 3585, 1803, 11, 286, 362, 281, 352, 807, 264, 957, 1002, 13, 51464, 51464, 286, 500, 380, 362, 257, 2316, 295, 264, 957, 1002, 13, 286, 500, 380, 458, 577, 281, 14722, 264, 2063, 2445, 294, 257, 819, 9364, 636, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07751564092414323, "compression_ratio": 1.721698113207547, "no_speech_prob": 3.7251957110129297e-06}, {"id": 761, "seek": 431506, "start": 4315.06, "end": 4322.06, "text": " That said, a lot of reinforcement learning methods actually work by training a model of the cost function.", "tokens": [50364, 663, 848, 11, 257, 688, 295, 29280, 2539, 7150, 767, 589, 538, 3097, 257, 2316, 295, 264, 2063, 2445, 13, 50714, 50714, 2264, 11, 370, 8747, 7850, 7150, 13, 50864, 50864, 8537, 11, 264, 3090, 295, 264, 7850, 307, 281, 1466, 281, 13059, 11, 281, 733, 295, 6069, 264, 2158, 295, 264, 4787, 10024, 2445, 11, 264, 5176, 2158, 295, 264, 10024, 2445, 13, 51464, 51464, 400, 570, 309, 311, 257, 18161, 2533, 300, 291, 434, 516, 281, 3847, 11, 291, 393, 646, 48256, 16235, 281, 309, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09835866223210873, "compression_ratio": 1.7136752136752136, "no_speech_prob": 3.089285655732965e-06}, {"id": 762, "seek": 431506, "start": 4322.06, "end": 4325.06, "text": " OK, so actor critic methods.", "tokens": [50364, 663, 848, 11, 257, 688, 295, 29280, 2539, 7150, 767, 589, 538, 3097, 257, 2316, 295, 264, 2063, 2445, 13, 50714, 50714, 2264, 11, 370, 8747, 7850, 7150, 13, 50864, 50864, 8537, 11, 264, 3090, 295, 264, 7850, 307, 281, 1466, 281, 13059, 11, 281, 733, 295, 6069, 264, 2158, 295, 264, 4787, 10024, 2445, 11, 264, 5176, 2158, 295, 264, 10024, 2445, 13, 51464, 51464, 400, 570, 309, 311, 257, 18161, 2533, 300, 291, 434, 516, 281, 3847, 11, 291, 393, 646, 48256, 16235, 281, 309, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09835866223210873, "compression_ratio": 1.7136752136752136, "no_speech_prob": 3.089285655732965e-06}, {"id": 763, "seek": 431506, "start": 4325.06, "end": 4337.06, "text": " Basically, the role of the critic is to learn to evaluate, to kind of predict the value of the overall objective function, the expected value of the objective function.", "tokens": [50364, 663, 848, 11, 257, 688, 295, 29280, 2539, 7150, 767, 589, 538, 3097, 257, 2316, 295, 264, 2063, 2445, 13, 50714, 50714, 2264, 11, 370, 8747, 7850, 7150, 13, 50864, 50864, 8537, 11, 264, 3090, 295, 264, 7850, 307, 281, 1466, 281, 13059, 11, 281, 733, 295, 6069, 264, 2158, 295, 264, 4787, 10024, 2445, 11, 264, 5176, 2158, 295, 264, 10024, 2445, 13, 51464, 51464, 400, 570, 309, 311, 257, 18161, 2533, 300, 291, 434, 516, 281, 3847, 11, 291, 393, 646, 48256, 16235, 281, 309, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09835866223210873, "compression_ratio": 1.7136752136752136, "no_speech_prob": 3.089285655732965e-06}, {"id": 764, "seek": 431506, "start": 4337.06, "end": 4342.06, "text": " And because it's a neural net that you're going to train, you can back propagate gradient to it.", "tokens": [50364, 663, 848, 11, 257, 688, 295, 29280, 2539, 7150, 767, 589, 538, 3097, 257, 2316, 295, 264, 2063, 2445, 13, 50714, 50714, 2264, 11, 370, 8747, 7850, 7150, 13, 50864, 50864, 8537, 11, 264, 3090, 295, 264, 7850, 307, 281, 1466, 281, 13059, 11, 281, 733, 295, 6069, 264, 2158, 295, 264, 4787, 10024, 2445, 11, 264, 5176, 2158, 295, 264, 10024, 2445, 13, 51464, 51464, 400, 570, 309, 311, 257, 18161, 2533, 300, 291, 434, 516, 281, 3847, 11, 291, 393, 646, 48256, 16235, 281, 309, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09835866223210873, "compression_ratio": 1.7136752136752136, "no_speech_prob": 3.089285655732965e-06}, {"id": 765, "seek": 434206, "start": 4342.06, "end": 4349.06, "text": " So you're basically learning an approximation of the cost function of the real world using a neural net.", "tokens": [50364, 407, 291, 434, 1936, 2539, 364, 28023, 295, 264, 2063, 2445, 295, 264, 957, 1002, 1228, 257, 18161, 2533, 13, 50714, 50714, 663, 311, 264, 3090, 295, 257, 7850, 13, 50864, 50864, 2264, 11, 983, 307, 309, 370, 665, 281, 362, 5245, 562, 291, 434, 2539, 257, 5389, 11, 411, 2539, 281, 3332, 11, 337, 1365, 30, 51464, 51464, 467, 311, 1936, 437, 4045, 291, 281, 1466, 2661, 293, 281, 1466, 1553, 8011, 1803, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0520977068550979, "compression_ratio": 1.603960396039604, "no_speech_prob": 6.43170096736867e-06}, {"id": 766, "seek": 434206, "start": 4349.06, "end": 4352.06, "text": " That's the role of a critic.", "tokens": [50364, 407, 291, 434, 1936, 2539, 364, 28023, 295, 264, 2063, 2445, 295, 264, 957, 1002, 1228, 257, 18161, 2533, 13, 50714, 50714, 663, 311, 264, 3090, 295, 257, 7850, 13, 50864, 50864, 2264, 11, 983, 307, 309, 370, 665, 281, 362, 5245, 562, 291, 434, 2539, 257, 5389, 11, 411, 2539, 281, 3332, 11, 337, 1365, 30, 51464, 51464, 467, 311, 1936, 437, 4045, 291, 281, 1466, 2661, 293, 281, 1466, 1553, 8011, 1803, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0520977068550979, "compression_ratio": 1.603960396039604, "no_speech_prob": 6.43170096736867e-06}, {"id": 767, "seek": 434206, "start": 4352.06, "end": 4364.06, "text": " OK, why is it so good to have models when you're learning a skill, like learning to drive, for example?", "tokens": [50364, 407, 291, 434, 1936, 2539, 364, 28023, 295, 264, 2063, 2445, 295, 264, 957, 1002, 1228, 257, 18161, 2533, 13, 50714, 50714, 663, 311, 264, 3090, 295, 257, 7850, 13, 50864, 50864, 2264, 11, 983, 307, 309, 370, 665, 281, 362, 5245, 562, 291, 434, 2539, 257, 5389, 11, 411, 2539, 281, 3332, 11, 337, 1365, 30, 51464, 51464, 467, 311, 1936, 437, 4045, 291, 281, 1466, 2661, 293, 281, 1466, 1553, 8011, 1803, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0520977068550979, "compression_ratio": 1.603960396039604, "no_speech_prob": 6.43170096736867e-06}, {"id": 768, "seek": 434206, "start": 4364.06, "end": 4369.06, "text": " It's basically what allows you to learn quickly and to learn without killing yourself.", "tokens": [50364, 407, 291, 434, 1936, 2539, 364, 28023, 295, 264, 2063, 2445, 295, 264, 957, 1002, 1228, 257, 18161, 2533, 13, 50714, 50714, 663, 311, 264, 3090, 295, 257, 7850, 13, 50864, 50864, 2264, 11, 983, 307, 309, 370, 665, 281, 362, 5245, 562, 291, 434, 2539, 257, 5389, 11, 411, 2539, 281, 3332, 11, 337, 1365, 30, 51464, 51464, 467, 311, 1936, 437, 4045, 291, 281, 1466, 2661, 293, 281, 1466, 1553, 8011, 1803, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0520977068550979, "compression_ratio": 1.603960396039604, "no_speech_prob": 6.43170096736867e-06}, {"id": 769, "seek": 436906, "start": 4369.06, "end": 4376.06, "text": " So if you don't have a good model of the world, you don't know about gravity, you don't know about the dynamics of objects, you don't know anything.", "tokens": [50364, 407, 498, 291, 500, 380, 362, 257, 665, 2316, 295, 264, 1002, 11, 291, 500, 380, 458, 466, 12110, 11, 291, 500, 380, 458, 466, 264, 15679, 295, 6565, 11, 291, 500, 380, 458, 1340, 13, 50714, 50714, 400, 291, 829, 364, 9461, 412, 264, 5589, 295, 257, 1032, 13, 440, 9461, 575, 572, 1558, 437, 264, 10649, 295, 257, 1032, 307, 13, 51064, 51064, 2264, 11, 293, 291, 829, 264, 1032, 958, 281, 257, 22316, 13, 440, 1032, 307, 4840, 412, 2217, 6193, 364, 1773, 958, 281, 257, 22316, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08874587008827611, "compression_ratio": 1.7947368421052632, "no_speech_prob": 4.004561924375594e-05}, {"id": 770, "seek": 436906, "start": 4376.06, "end": 4383.06, "text": " And you put an agent at the wheel of a car. The agent has no idea what the physics of a car is.", "tokens": [50364, 407, 498, 291, 500, 380, 362, 257, 665, 2316, 295, 264, 1002, 11, 291, 500, 380, 458, 466, 12110, 11, 291, 500, 380, 458, 466, 264, 15679, 295, 6565, 11, 291, 500, 380, 458, 1340, 13, 50714, 50714, 400, 291, 829, 364, 9461, 412, 264, 5589, 295, 257, 1032, 13, 440, 9461, 575, 572, 1558, 437, 264, 10649, 295, 257, 1032, 307, 13, 51064, 51064, 2264, 11, 293, 291, 829, 264, 1032, 958, 281, 257, 22316, 13, 440, 1032, 307, 4840, 412, 2217, 6193, 364, 1773, 958, 281, 257, 22316, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08874587008827611, "compression_ratio": 1.7947368421052632, "no_speech_prob": 4.004561924375594e-05}, {"id": 771, "seek": 436906, "start": 4383.06, "end": 4390.06, "text": " OK, and you put the car next to a cliff. The car is driving at 30 miles an hour next to a cliff.", "tokens": [50364, 407, 498, 291, 500, 380, 362, 257, 665, 2316, 295, 264, 1002, 11, 291, 500, 380, 458, 466, 12110, 11, 291, 500, 380, 458, 466, 264, 15679, 295, 6565, 11, 291, 500, 380, 458, 1340, 13, 50714, 50714, 400, 291, 829, 364, 9461, 412, 264, 5589, 295, 257, 1032, 13, 440, 9461, 575, 572, 1558, 437, 264, 10649, 295, 257, 1032, 307, 13, 51064, 51064, 2264, 11, 293, 291, 829, 264, 1032, 958, 281, 257, 22316, 13, 440, 1032, 307, 4840, 412, 2217, 6193, 364, 1773, 958, 281, 257, 22316, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08874587008827611, "compression_ratio": 1.7947368421052632, "no_speech_prob": 4.004561924375594e-05}, {"id": 772, "seek": 439006, "start": 4390.06, "end": 4400.06, "text": " The agent doesn't have a model of the world. It has no idea that by turning the wheel to the right, the car will run off a cliff and will fall into the ravine.", "tokens": [50364, 440, 9461, 1177, 380, 362, 257, 2316, 295, 264, 1002, 13, 467, 575, 572, 1558, 300, 538, 6246, 264, 5589, 281, 264, 558, 11, 264, 1032, 486, 1190, 766, 257, 22316, 293, 486, 2100, 666, 264, 32987, 533, 13, 50864, 50864, 467, 575, 281, 767, 853, 309, 281, 2573, 309, 484, 13, 467, 575, 281, 2100, 666, 264, 32987, 533, 281, 2573, 484, 300, 341, 307, 257, 1578, 1558, 13, 51214, 51214, 2264, 11, 293, 1310, 445, 490, 472, 6889, 11, 309, 311, 406, 516, 281, 312, 1075, 281, 1466, 309, 13, 51364, 51364, 407, 309, 311, 516, 281, 362, 281, 1190, 666, 264, 32987, 533, 411, 5383, 295, 1413, 949, 309, 9624, 484, 264, 2316, 295, 264, 1002, 300, 700, 11, 6246, 264, 5589, 281, 264, 558, 1669, 264, 1032, 352, 281, 264, 558, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07381364958626883, "compression_ratio": 2.0226415094339623, "no_speech_prob": 4.6376840145967435e-06}, {"id": 773, "seek": 439006, "start": 4400.06, "end": 4407.06, "text": " It has to actually try it to figure it out. It has to fall into the ravine to figure out that this is a bad idea.", "tokens": [50364, 440, 9461, 1177, 380, 362, 257, 2316, 295, 264, 1002, 13, 467, 575, 572, 1558, 300, 538, 6246, 264, 5589, 281, 264, 558, 11, 264, 1032, 486, 1190, 766, 257, 22316, 293, 486, 2100, 666, 264, 32987, 533, 13, 50864, 50864, 467, 575, 281, 767, 853, 309, 281, 2573, 309, 484, 13, 467, 575, 281, 2100, 666, 264, 32987, 533, 281, 2573, 484, 300, 341, 307, 257, 1578, 1558, 13, 51214, 51214, 2264, 11, 293, 1310, 445, 490, 472, 6889, 11, 309, 311, 406, 516, 281, 312, 1075, 281, 1466, 309, 13, 51364, 51364, 407, 309, 311, 516, 281, 362, 281, 1190, 666, 264, 32987, 533, 411, 5383, 295, 1413, 949, 309, 9624, 484, 264, 2316, 295, 264, 1002, 300, 700, 11, 6246, 264, 5589, 281, 264, 558, 1669, 264, 1032, 352, 281, 264, 558, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07381364958626883, "compression_ratio": 2.0226415094339623, "no_speech_prob": 4.6376840145967435e-06}, {"id": 774, "seek": 439006, "start": 4407.06, "end": 4410.06, "text": " OK, and maybe just from one sample, it's not going to be able to learn it.", "tokens": [50364, 440, 9461, 1177, 380, 362, 257, 2316, 295, 264, 1002, 13, 467, 575, 572, 1558, 300, 538, 6246, 264, 5589, 281, 264, 558, 11, 264, 1032, 486, 1190, 766, 257, 22316, 293, 486, 2100, 666, 264, 32987, 533, 13, 50864, 50864, 467, 575, 281, 767, 853, 309, 281, 2573, 309, 484, 13, 467, 575, 281, 2100, 666, 264, 32987, 533, 281, 2573, 484, 300, 341, 307, 257, 1578, 1558, 13, 51214, 51214, 2264, 11, 293, 1310, 445, 490, 472, 6889, 11, 309, 311, 406, 516, 281, 312, 1075, 281, 1466, 309, 13, 51364, 51364, 407, 309, 311, 516, 281, 362, 281, 1190, 666, 264, 32987, 533, 411, 5383, 295, 1413, 949, 309, 9624, 484, 264, 2316, 295, 264, 1002, 300, 700, 11, 6246, 264, 5589, 281, 264, 558, 1669, 264, 1032, 352, 281, 264, 558, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07381364958626883, "compression_ratio": 2.0226415094339623, "no_speech_prob": 4.6376840145967435e-06}, {"id": 775, "seek": 439006, "start": 4410.06, "end": 4418.06, "text": " So it's going to have to run into the ravine like thousands of times before it figures out the model of the world that first, turning the wheel to the right makes the car go to the right.", "tokens": [50364, 440, 9461, 1177, 380, 362, 257, 2316, 295, 264, 1002, 13, 467, 575, 572, 1558, 300, 538, 6246, 264, 5589, 281, 264, 558, 11, 264, 1032, 486, 1190, 766, 257, 22316, 293, 486, 2100, 666, 264, 32987, 533, 13, 50864, 50864, 467, 575, 281, 767, 853, 309, 281, 2573, 309, 484, 13, 467, 575, 281, 2100, 666, 264, 32987, 533, 281, 2573, 484, 300, 341, 307, 257, 1578, 1558, 13, 51214, 51214, 2264, 11, 293, 1310, 445, 490, 472, 6889, 11, 309, 311, 406, 516, 281, 312, 1075, 281, 1466, 309, 13, 51364, 51364, 407, 309, 311, 516, 281, 362, 281, 1190, 666, 264, 32987, 533, 411, 5383, 295, 1413, 949, 309, 9624, 484, 264, 2316, 295, 264, 1002, 300, 700, 11, 6246, 264, 5589, 281, 264, 558, 1669, 264, 1032, 352, 281, 264, 558, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07381364958626883, "compression_ratio": 2.0226415094339623, "no_speech_prob": 4.6376840145967435e-06}, {"id": 776, "seek": 441806, "start": 4418.06, "end": 4424.06, "text": " And second, that when the car goes above a ravine, it falls into a ravine and destroys itself.", "tokens": [50364, 400, 1150, 11, 300, 562, 264, 1032, 1709, 3673, 257, 32987, 533, 11, 309, 8804, 666, 257, 32987, 533, 293, 36714, 2564, 13, 50664, 50664, 2264, 11, 498, 291, 362, 257, 2316, 295, 264, 1002, 300, 15146, 466, 12110, 293, 721, 411, 341, 11, 550, 291, 458, 300, 6246, 264, 5589, 281, 264, 558, 307, 516, 281, 652, 264, 1032, 5766, 281, 264, 32987, 533, 13, 51064, 51064, 400, 291, 500, 380, 360, 309, 570, 291, 458, 309, 311, 516, 281, 1961, 291, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.06373934312300249, "compression_ratio": 1.6782178217821782, "no_speech_prob": 1.7227292119059712e-05}, {"id": 777, "seek": 441806, "start": 4424.06, "end": 4432.06, "text": " OK, if you have a model of the world that understands about gravity and things like this, then you know that turning the wheel to the right is going to make the car via to the ravine.", "tokens": [50364, 400, 1150, 11, 300, 562, 264, 1032, 1709, 3673, 257, 32987, 533, 11, 309, 8804, 666, 257, 32987, 533, 293, 36714, 2564, 13, 50664, 50664, 2264, 11, 498, 291, 362, 257, 2316, 295, 264, 1002, 300, 15146, 466, 12110, 293, 721, 411, 341, 11, 550, 291, 458, 300, 6246, 264, 5589, 281, 264, 558, 307, 516, 281, 652, 264, 1032, 5766, 281, 264, 32987, 533, 13, 51064, 51064, 400, 291, 500, 380, 360, 309, 570, 291, 458, 309, 311, 516, 281, 1961, 291, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.06373934312300249, "compression_ratio": 1.6782178217821782, "no_speech_prob": 1.7227292119059712e-05}, {"id": 778, "seek": 441806, "start": 4432.06, "end": 4434.06, "text": " And you don't do it because you know it's going to kill you.", "tokens": [50364, 400, 1150, 11, 300, 562, 264, 1032, 1709, 3673, 257, 32987, 533, 11, 309, 8804, 666, 257, 32987, 533, 293, 36714, 2564, 13, 50664, 50664, 2264, 11, 498, 291, 362, 257, 2316, 295, 264, 1002, 300, 15146, 466, 12110, 293, 721, 411, 341, 11, 550, 291, 458, 300, 6246, 264, 5589, 281, 264, 558, 307, 516, 281, 652, 264, 1032, 5766, 281, 264, 32987, 533, 13, 51064, 51064, 400, 291, 500, 380, 360, 309, 570, 291, 458, 309, 311, 516, 281, 1961, 291, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.06373934312300249, "compression_ratio": 1.6782178217821782, "no_speech_prob": 1.7227292119059712e-05}, {"id": 779, "seek": 443406, "start": 4434.06, "end": 4450.06, "text": " OK, so what allows humans and animals to learn quickly, much, much quicker than any model free reinforcement learning methods that has ever been devised is the fact that we have very, very good word models in our head.", "tokens": [50364, 2264, 11, 370, 437, 4045, 6255, 293, 4882, 281, 1466, 2661, 11, 709, 11, 709, 16255, 813, 604, 2316, 1737, 29280, 2539, 7150, 300, 575, 1562, 668, 1905, 2640, 307, 264, 1186, 300, 321, 362, 588, 11, 588, 665, 1349, 5245, 294, 527, 1378, 13, 51164, 51164, 2264, 11, 586, 11, 437, 775, 300, 980, 505, 30, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11496878451988345, "compression_ratio": 1.5120481927710843, "no_speech_prob": 2.6680188966565765e-05}, {"id": 780, "seek": 443406, "start": 4450.06, "end": 4456.06, "text": " OK, now, what does that tell us?", "tokens": [50364, 2264, 11, 370, 437, 4045, 6255, 293, 4882, 281, 1466, 2661, 11, 709, 11, 709, 16255, 813, 604, 2316, 1737, 29280, 2539, 7150, 300, 575, 1562, 668, 1905, 2640, 307, 264, 1186, 300, 321, 362, 588, 11, 588, 665, 1349, 5245, 294, 527, 1378, 13, 51164, 51164, 2264, 11, 586, 11, 437, 775, 300, 980, 505, 30, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11496878451988345, "compression_ratio": 1.5120481927710843, "no_speech_prob": 2.6680188966565765e-05}, {"id": 781, "seek": 445606, "start": 4456.06, "end": 4467.06, "text": " OK, so here is the problem with the world. The world is not deterministic or if it is deterministic, it's so complex that it equally well could be non deterministic.", "tokens": [50364, 2264, 11, 370, 510, 307, 264, 1154, 365, 264, 1002, 13, 440, 1002, 307, 406, 15957, 3142, 420, 498, 309, 307, 15957, 3142, 11, 309, 311, 370, 3997, 300, 309, 12309, 731, 727, 312, 2107, 15957, 3142, 13, 50914, 50914, 467, 1177, 380, 652, 604, 2649, 337, 505, 13, 51114, 51114, 821, 311, 732, 2740, 365, 32884, 264, 958, 1785, 295, 264, 1002, 13, 51264, 51264, 440, 700, 1154, 307, 300, 264, 1002, 307, 406, 7696, 27737, 293, 309, 727, 312, 406, 7696, 27737, 337, 732, 4112, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09170794486999512, "compression_ratio": 1.8578199052132702, "no_speech_prob": 2.748865335888695e-05}, {"id": 782, "seek": 445606, "start": 4467.06, "end": 4471.06, "text": " It doesn't make any difference for us.", "tokens": [50364, 2264, 11, 370, 510, 307, 264, 1154, 365, 264, 1002, 13, 440, 1002, 307, 406, 15957, 3142, 420, 498, 309, 307, 15957, 3142, 11, 309, 311, 370, 3997, 300, 309, 12309, 731, 727, 312, 2107, 15957, 3142, 13, 50914, 50914, 467, 1177, 380, 652, 604, 2649, 337, 505, 13, 51114, 51114, 821, 311, 732, 2740, 365, 32884, 264, 958, 1785, 295, 264, 1002, 13, 51264, 51264, 440, 700, 1154, 307, 300, 264, 1002, 307, 406, 7696, 27737, 293, 309, 727, 312, 406, 7696, 27737, 337, 732, 4112, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09170794486999512, "compression_ratio": 1.8578199052132702, "no_speech_prob": 2.748865335888695e-05}, {"id": 783, "seek": 445606, "start": 4471.06, "end": 4474.06, "text": " There's two problems with predicting the next state of the world.", "tokens": [50364, 2264, 11, 370, 510, 307, 264, 1154, 365, 264, 1002, 13, 440, 1002, 307, 406, 15957, 3142, 420, 498, 309, 307, 15957, 3142, 11, 309, 311, 370, 3997, 300, 309, 12309, 731, 727, 312, 2107, 15957, 3142, 13, 50914, 50914, 467, 1177, 380, 652, 604, 2649, 337, 505, 13, 51114, 51114, 821, 311, 732, 2740, 365, 32884, 264, 958, 1785, 295, 264, 1002, 13, 51264, 51264, 440, 700, 1154, 307, 300, 264, 1002, 307, 406, 7696, 27737, 293, 309, 727, 312, 406, 7696, 27737, 337, 732, 4112, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09170794486999512, "compression_ratio": 1.8578199052132702, "no_speech_prob": 2.748865335888695e-05}, {"id": 784, "seek": 445606, "start": 4474.06, "end": 4481.06, "text": " The first problem is that the world is not entirely predictable and it could be not entirely predictable for two reasons.", "tokens": [50364, 2264, 11, 370, 510, 307, 264, 1154, 365, 264, 1002, 13, 440, 1002, 307, 406, 15957, 3142, 420, 498, 309, 307, 15957, 3142, 11, 309, 311, 370, 3997, 300, 309, 12309, 731, 727, 312, 2107, 15957, 3142, 13, 50914, 50914, 467, 1177, 380, 652, 604, 2649, 337, 505, 13, 51114, 51114, 821, 311, 732, 2740, 365, 32884, 264, 958, 1785, 295, 264, 1002, 13, 51264, 51264, 440, 700, 1154, 307, 300, 264, 1002, 307, 406, 7696, 27737, 293, 309, 727, 312, 406, 7696, 27737, 337, 732, 4112, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09170794486999512, "compression_ratio": 1.8578199052132702, "no_speech_prob": 2.748865335888695e-05}, {"id": 785, "seek": 448106, "start": 4481.06, "end": 4486.06, "text": " Those are called aleatoric uncertainty and epistemic uncertainty.", "tokens": [50364, 3950, 366, 1219, 6775, 1639, 299, 15697, 293, 2388, 468, 3438, 15697, 13, 50614, 50614, 9366, 1639, 299, 15697, 307, 3462, 281, 264, 1186, 300, 264, 1002, 307, 28621, 984, 31160, 420, 264, 1186, 300, 321, 500, 380, 362, 1577, 1589, 466, 264, 1785, 295, 264, 1002, 13, 51064, 51064, 407, 321, 2644, 6069, 2293, 437, 311, 516, 281, 1051, 958, 13, 51164, 51164, 407, 291, 434, 1237, 412, 385, 558, 586, 13, 509, 434, 257, 1238, 665, 2316, 295, 264, 11629, 2823, 295, 385, 13, 51464, 51464, 2264, 11, 457, 291, 2644, 2293, 6069, 294, 597, 636, 286, 478, 516, 281, 1286, 452, 1378, 958, 570, 291, 500, 380, 362, 364, 8559, 2316, 295, 437, 311, 1854, 452, 11743, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07328873443603516, "compression_ratio": 1.819112627986348, "no_speech_prob": 3.0707997211720794e-05}, {"id": 786, "seek": 448106, "start": 4486.06, "end": 4495.06, "text": " Aleatoric uncertainty is due to the fact that the world is intrinsically unpredictable or the fact that we don't have full information about the state of the world.", "tokens": [50364, 3950, 366, 1219, 6775, 1639, 299, 15697, 293, 2388, 468, 3438, 15697, 13, 50614, 50614, 9366, 1639, 299, 15697, 307, 3462, 281, 264, 1186, 300, 264, 1002, 307, 28621, 984, 31160, 420, 264, 1186, 300, 321, 500, 380, 362, 1577, 1589, 466, 264, 1785, 295, 264, 1002, 13, 51064, 51064, 407, 321, 2644, 6069, 2293, 437, 311, 516, 281, 1051, 958, 13, 51164, 51164, 407, 291, 434, 1237, 412, 385, 558, 586, 13, 509, 434, 257, 1238, 665, 2316, 295, 264, 11629, 2823, 295, 385, 13, 51464, 51464, 2264, 11, 457, 291, 2644, 2293, 6069, 294, 597, 636, 286, 478, 516, 281, 1286, 452, 1378, 958, 570, 291, 500, 380, 362, 364, 8559, 2316, 295, 437, 311, 1854, 452, 11743, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07328873443603516, "compression_ratio": 1.819112627986348, "no_speech_prob": 3.0707997211720794e-05}, {"id": 787, "seek": 448106, "start": 4495.06, "end": 4497.06, "text": " So we cannot predict exactly what's going to happen next.", "tokens": [50364, 3950, 366, 1219, 6775, 1639, 299, 15697, 293, 2388, 468, 3438, 15697, 13, 50614, 50614, 9366, 1639, 299, 15697, 307, 3462, 281, 264, 1186, 300, 264, 1002, 307, 28621, 984, 31160, 420, 264, 1186, 300, 321, 500, 380, 362, 1577, 1589, 466, 264, 1785, 295, 264, 1002, 13, 51064, 51064, 407, 321, 2644, 6069, 2293, 437, 311, 516, 281, 1051, 958, 13, 51164, 51164, 407, 291, 434, 1237, 412, 385, 558, 586, 13, 509, 434, 257, 1238, 665, 2316, 295, 264, 11629, 2823, 295, 385, 13, 51464, 51464, 2264, 11, 457, 291, 2644, 2293, 6069, 294, 597, 636, 286, 478, 516, 281, 1286, 452, 1378, 958, 570, 291, 500, 380, 362, 364, 8559, 2316, 295, 437, 311, 1854, 452, 11743, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07328873443603516, "compression_ratio": 1.819112627986348, "no_speech_prob": 3.0707997211720794e-05}, {"id": 788, "seek": 448106, "start": 4497.06, "end": 4503.06, "text": " So you're looking at me right now. You're a pretty good model of the immediate environment of me.", "tokens": [50364, 3950, 366, 1219, 6775, 1639, 299, 15697, 293, 2388, 468, 3438, 15697, 13, 50614, 50614, 9366, 1639, 299, 15697, 307, 3462, 281, 264, 1186, 300, 264, 1002, 307, 28621, 984, 31160, 420, 264, 1186, 300, 321, 500, 380, 362, 1577, 1589, 466, 264, 1785, 295, 264, 1002, 13, 51064, 51064, 407, 321, 2644, 6069, 2293, 437, 311, 516, 281, 1051, 958, 13, 51164, 51164, 407, 291, 434, 1237, 412, 385, 558, 586, 13, 509, 434, 257, 1238, 665, 2316, 295, 264, 11629, 2823, 295, 385, 13, 51464, 51464, 2264, 11, 457, 291, 2644, 2293, 6069, 294, 597, 636, 286, 478, 516, 281, 1286, 452, 1378, 958, 570, 291, 500, 380, 362, 364, 8559, 2316, 295, 437, 311, 1854, 452, 11743, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07328873443603516, "compression_ratio": 1.819112627986348, "no_speech_prob": 3.0707997211720794e-05}, {"id": 789, "seek": 448106, "start": 4503.06, "end": 4510.06, "text": " OK, but you cannot exactly predict in which way I'm going to move my head next because you don't have an accurate model of what's inside my skull.", "tokens": [50364, 3950, 366, 1219, 6775, 1639, 299, 15697, 293, 2388, 468, 3438, 15697, 13, 50614, 50614, 9366, 1639, 299, 15697, 307, 3462, 281, 264, 1186, 300, 264, 1002, 307, 28621, 984, 31160, 420, 264, 1186, 300, 321, 500, 380, 362, 1577, 1589, 466, 264, 1785, 295, 264, 1002, 13, 51064, 51064, 407, 321, 2644, 6069, 2293, 437, 311, 516, 281, 1051, 958, 13, 51164, 51164, 407, 291, 434, 1237, 412, 385, 558, 586, 13, 509, 434, 257, 1238, 665, 2316, 295, 264, 11629, 2823, 295, 385, 13, 51464, 51464, 2264, 11, 457, 291, 2644, 2293, 6069, 294, 597, 636, 286, 478, 516, 281, 1286, 452, 1378, 958, 570, 291, 500, 380, 362, 364, 8559, 2316, 295, 437, 311, 1854, 452, 11743, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07328873443603516, "compression_ratio": 1.819112627986348, "no_speech_prob": 3.0707997211720794e-05}, {"id": 790, "seek": 451006, "start": 4510.06, "end": 4520.06, "text": " OK, your perceptual system does not give you a full model of how my brain functions, unfortunately.", "tokens": [50364, 2264, 11, 428, 43276, 901, 1185, 775, 406, 976, 291, 257, 1577, 2316, 295, 577, 452, 3567, 6828, 11, 7015, 13, 50864, 50864, 407, 291, 2644, 2293, 6069, 437, 286, 478, 516, 281, 360, 958, 11, 437, 286, 478, 516, 281, 584, 11, 577, 286, 478, 516, 281, 1286, 452, 1378, 11, 1030, 11458, 13, 51314, 51314, 407, 300, 311, 6775, 1639, 299, 15697, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08481461068858272, "compression_ratio": 1.4941176470588236, "no_speech_prob": 3.852958616334945e-05}, {"id": 791, "seek": 451006, "start": 4520.06, "end": 4529.06, "text": " So you cannot exactly predict what I'm going to do next, what I'm going to say, how I'm going to move my head, et cetera.", "tokens": [50364, 2264, 11, 428, 43276, 901, 1185, 775, 406, 976, 291, 257, 1577, 2316, 295, 577, 452, 3567, 6828, 11, 7015, 13, 50864, 50864, 407, 291, 2644, 2293, 6069, 437, 286, 478, 516, 281, 360, 958, 11, 437, 286, 478, 516, 281, 584, 11, 577, 286, 478, 516, 281, 1286, 452, 1378, 11, 1030, 11458, 13, 51314, 51314, 407, 300, 311, 6775, 1639, 299, 15697, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08481461068858272, "compression_ratio": 1.4941176470588236, "no_speech_prob": 3.852958616334945e-05}, {"id": 792, "seek": 451006, "start": 4529.06, "end": 4533.06, "text": " So that's aleatoric uncertainty.", "tokens": [50364, 2264, 11, 428, 43276, 901, 1185, 775, 406, 976, 291, 257, 1577, 2316, 295, 577, 452, 3567, 6828, 11, 7015, 13, 50864, 50864, 407, 291, 2644, 2293, 6069, 437, 286, 478, 516, 281, 360, 958, 11, 437, 286, 478, 516, 281, 584, 11, 577, 286, 478, 516, 281, 1286, 452, 1378, 11, 1030, 11458, 13, 51314, 51314, 407, 300, 311, 6775, 1639, 299, 15697, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08481461068858272, "compression_ratio": 1.4941176470588236, "no_speech_prob": 3.852958616334945e-05}, {"id": 793, "seek": 453306, "start": 4533.06, "end": 4545.06, "text": " There is also epistemic uncertainty. Epistemic uncertainty is the fact that you can't completely predict the next state of the world because the amount of training data you've had was not enough.", "tokens": [50364, 821, 307, 611, 2388, 468, 3438, 15697, 13, 9970, 468, 3438, 15697, 307, 264, 1186, 300, 291, 393, 380, 2584, 6069, 264, 958, 1785, 295, 264, 1002, 570, 264, 2372, 295, 3097, 1412, 291, 600, 632, 390, 406, 1547, 13, 50964, 50964, 2260, 2316, 6132, 380, 668, 8895, 1547, 281, 534, 733, 295, 2573, 309, 484, 13, 51114, 51114, 2264, 11, 300, 311, 733, 295, 257, 819, 2010, 295, 15697, 13, 51364, 51364, 407, 264, 955, 1168, 586, 307, 577, 360, 321, 3847, 5245, 295, 264, 1002, 833, 257, 27022, 30, 286, 976, 291, 364, 4904, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08840838517292891, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.7505049729370512e-05}, {"id": 794, "seek": 453306, "start": 4545.06, "end": 4548.06, "text": " Your model hasn't been trained enough to really kind of figure it out.", "tokens": [50364, 821, 307, 611, 2388, 468, 3438, 15697, 13, 9970, 468, 3438, 15697, 307, 264, 1186, 300, 291, 393, 380, 2584, 6069, 264, 958, 1785, 295, 264, 1002, 570, 264, 2372, 295, 3097, 1412, 291, 600, 632, 390, 406, 1547, 13, 50964, 50964, 2260, 2316, 6132, 380, 668, 8895, 1547, 281, 534, 733, 295, 2573, 309, 484, 13, 51114, 51114, 2264, 11, 300, 311, 733, 295, 257, 819, 2010, 295, 15697, 13, 51364, 51364, 407, 264, 955, 1168, 586, 307, 577, 360, 321, 3847, 5245, 295, 264, 1002, 833, 257, 27022, 30, 286, 976, 291, 364, 4904, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08840838517292891, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.7505049729370512e-05}, {"id": 795, "seek": 453306, "start": 4548.06, "end": 4553.06, "text": " OK, that's kind of a different type of uncertainty.", "tokens": [50364, 821, 307, 611, 2388, 468, 3438, 15697, 13, 9970, 468, 3438, 15697, 307, 264, 1186, 300, 291, 393, 380, 2584, 6069, 264, 958, 1785, 295, 264, 1002, 570, 264, 2372, 295, 3097, 1412, 291, 600, 632, 390, 406, 1547, 13, 50964, 50964, 2260, 2316, 6132, 380, 668, 8895, 1547, 281, 534, 733, 295, 2573, 309, 484, 13, 51114, 51114, 2264, 11, 300, 311, 733, 295, 257, 819, 2010, 295, 15697, 13, 51364, 51364, 407, 264, 955, 1168, 586, 307, 577, 360, 321, 3847, 5245, 295, 264, 1002, 833, 257, 27022, 30, 286, 976, 291, 364, 4904, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08840838517292891, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.7505049729370512e-05}, {"id": 796, "seek": 453306, "start": 4553.06, "end": 4559.06, "text": " So the big question now is how do we train models of the world under a certainty? I give you an ST.", "tokens": [50364, 821, 307, 611, 2388, 468, 3438, 15697, 13, 9970, 468, 3438, 15697, 307, 264, 1186, 300, 291, 393, 380, 2584, 6069, 264, 958, 1785, 295, 264, 1002, 570, 264, 2372, 295, 3097, 1412, 291, 600, 632, 390, 406, 1547, 13, 50964, 50964, 2260, 2316, 6132, 380, 668, 8895, 1547, 281, 534, 733, 295, 2573, 309, 484, 13, 51114, 51114, 2264, 11, 300, 311, 733, 295, 257, 819, 2010, 295, 15697, 13, 51364, 51364, 407, 264, 955, 1168, 586, 307, 577, 360, 321, 3847, 5245, 295, 264, 1002, 833, 257, 27022, 30, 286, 976, 291, 364, 4904, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08840838517292891, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.7505049729370512e-05}, {"id": 797, "seek": 455906, "start": 4559.06, "end": 4566.06, "text": " Can you predict ST plus 1? And it's the same problem we encountered before with our supervised learning. I give you an X.", "tokens": [50364, 1664, 291, 6069, 4904, 1804, 502, 30, 400, 309, 311, 264, 912, 1154, 321, 20381, 949, 365, 527, 46533, 2539, 13, 286, 976, 291, 364, 1783, 13, 50714, 50714, 1664, 291, 6069, 398, 30, 583, 264, 1154, 307, 300, 456, 366, 586, 3866, 398, 82, 300, 366, 18218, 365, 1783, 13, 821, 366, 3866, 4904, 1804, 502, 82, 300, 366, 18218, 365, 318, 11, 754, 337, 257, 2212, 3069, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.1369970553630107, "compression_ratio": 1.681564245810056, "no_speech_prob": 4.8891080950852484e-05}, {"id": 798, "seek": 455906, "start": 4566.06, "end": 4578.06, "text": " Can you predict Y? But the problem is that there are now multiple Ys that are compatible with X. There are multiple ST plus 1s that are compatible with S, even for a given action.", "tokens": [50364, 1664, 291, 6069, 4904, 1804, 502, 30, 400, 309, 311, 264, 912, 1154, 321, 20381, 949, 365, 527, 46533, 2539, 13, 286, 976, 291, 364, 1783, 13, 50714, 50714, 1664, 291, 6069, 398, 30, 583, 264, 1154, 307, 300, 456, 366, 586, 3866, 398, 82, 300, 366, 18218, 365, 1783, 13, 821, 366, 3866, 4904, 1804, 502, 82, 300, 366, 18218, 365, 318, 11, 754, 337, 257, 2212, 3069, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.1369970553630107, "compression_ratio": 1.681564245810056, "no_speech_prob": 4.8891080950852484e-05}, {"id": 799, "seek": 457806, "start": 4578.06, "end": 4594.06, "text": " So what does that mean? That means that our model here, our forward model, may take the state of the world and an action.", "tokens": [50364, 407, 437, 775, 300, 914, 30, 663, 1355, 300, 527, 2316, 510, 11, 527, 2128, 2316, 11, 815, 747, 264, 1785, 295, 264, 1002, 293, 364, 3069, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.12690085172653198, "compression_ratio": 1.3010752688172043, "no_speech_prob": 1.094642993848538e-05}, {"id": 800, "seek": 459406, "start": 4594.06, "end": 4608.06, "text": " But it will also have to take a latent variable, which we don't know the value of, to predict the next state.", "tokens": [50364, 583, 309, 486, 611, 362, 281, 747, 257, 48994, 7006, 11, 597, 321, 500, 380, 458, 264, 2158, 295, 11, 281, 6069, 264, 958, 1785, 13, 51064, 51064, 400, 341, 390, 588, 709, 411, 437, 321, 2825, 466, 3071, 11, 689, 321, 632, 286, 478, 516, 281, 2642, 341, 294, 257, 819, 1192, 1793, 11, 457, 309, 311, 264, 912, 1558, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08495709390351266, "compression_ratio": 1.44, "no_speech_prob": 1.6179188605747186e-05}, {"id": 801, "seek": 459406, "start": 4608.06, "end": 4617.06, "text": " And this was very much like what we talked about earlier, where we had I'm going to draw this in a different topology, but it's the same idea.", "tokens": [50364, 583, 309, 486, 611, 362, 281, 747, 257, 48994, 7006, 11, 597, 321, 500, 380, 458, 264, 2158, 295, 11, 281, 6069, 264, 958, 1785, 13, 51064, 51064, 400, 341, 390, 588, 709, 411, 437, 321, 2825, 466, 3071, 11, 689, 321, 632, 286, 478, 516, 281, 2642, 341, 294, 257, 819, 1192, 1793, 11, 457, 309, 311, 264, 912, 1558, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08495709390351266, "compression_ratio": 1.44, "no_speech_prob": 1.6179188605747186e-05}, {"id": 802, "seek": 461706, "start": 4617.06, "end": 4627.06, "text": " So we had X and it was going through a predictor, computing H.", "tokens": [50364, 407, 321, 632, 1783, 293, 309, 390, 516, 807, 257, 6069, 284, 11, 15866, 389, 13, 50864, 50864, 400, 550, 300, 390, 516, 807, 257, 979, 19866, 300, 486, 747, 666, 2696, 257, 48994, 7006, 281, 6069, 398, 2159, 13, 51564, 51564, 400, 550, 321, 11441, 398, 13, 2264, 11, 341, 307, 257, 17630, 337, 318, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09604425899318007, "compression_ratio": 1.5170068027210883, "no_speech_prob": 5.42180305274087e-06}, {"id": 803, "seek": 461706, "start": 4627.06, "end": 4641.06, "text": " And then that was going through a decoder that will take into account a latent variable to predict Y bar.", "tokens": [50364, 407, 321, 632, 1783, 293, 309, 390, 516, 807, 257, 6069, 284, 11, 15866, 389, 13, 50864, 50864, 400, 550, 300, 390, 516, 807, 257, 979, 19866, 300, 486, 747, 666, 2696, 257, 48994, 7006, 281, 6069, 398, 2159, 13, 51564, 51564, 400, 550, 321, 11441, 398, 13, 2264, 11, 341, 307, 257, 17630, 337, 318, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09604425899318007, "compression_ratio": 1.5170068027210883, "no_speech_prob": 5.42180305274087e-06}, {"id": 804, "seek": 461706, "start": 4641.06, "end": 4646.06, "text": " And then we observe Y. OK, this is a prediction for S.", "tokens": [50364, 407, 321, 632, 1783, 293, 309, 390, 516, 807, 257, 6069, 284, 11, 15866, 389, 13, 50864, 50864, 400, 550, 300, 390, 516, 807, 257, 979, 19866, 300, 486, 747, 666, 2696, 257, 48994, 7006, 281, 6069, 398, 2159, 13, 51564, 51564, 400, 550, 321, 11441, 398, 13, 2264, 11, 341, 307, 257, 17630, 337, 318, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09604425899318007, "compression_ratio": 1.5170068027210883, "no_speech_prob": 5.42180305274087e-06}, {"id": 805, "seek": 464606, "start": 4646.06, "end": 4653.06, "text": " And maybe at some time, we might be able to actually take the action and observe the next state of the world.", "tokens": [50364, 400, 1310, 412, 512, 565, 11, 321, 1062, 312, 1075, 281, 767, 747, 264, 3069, 293, 11441, 264, 958, 1785, 295, 264, 1002, 13, 50714, 50714, 3987, 321, 366, 3097, 527, 2316, 11, 321, 603, 767, 312, 22107, 264, 958, 1785, 295, 264, 1002, 11, 314, 1804, 502, 13, 51164, 51164, 2264, 11, 370, 281, 3847, 527, 2128, 2316, 510, 11, 321, 747, 264, 1785, 4904, 13, 492, 747, 364, 3069, 498, 321, 362, 364, 3069, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09418547006300938, "compression_ratio": 1.7277777777777779, "no_speech_prob": 1.5927973436191678e-05}, {"id": 806, "seek": 464606, "start": 4653.06, "end": 4662.06, "text": " While we are training our model, we'll actually be observing the next state of the world, T plus 1.", "tokens": [50364, 400, 1310, 412, 512, 565, 11, 321, 1062, 312, 1075, 281, 767, 747, 264, 3069, 293, 11441, 264, 958, 1785, 295, 264, 1002, 13, 50714, 50714, 3987, 321, 366, 3097, 527, 2316, 11, 321, 603, 767, 312, 22107, 264, 958, 1785, 295, 264, 1002, 11, 314, 1804, 502, 13, 51164, 51164, 2264, 11, 370, 281, 3847, 527, 2128, 2316, 510, 11, 321, 747, 264, 1785, 4904, 13, 492, 747, 364, 3069, 498, 321, 362, 364, 3069, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09418547006300938, "compression_ratio": 1.7277777777777779, "no_speech_prob": 1.5927973436191678e-05}, {"id": 807, "seek": 464606, "start": 4662.06, "end": 4670.06, "text": " OK, so to train our forward model here, we take the state ST. We take an action if we have an action.", "tokens": [50364, 400, 1310, 412, 512, 565, 11, 321, 1062, 312, 1075, 281, 767, 747, 264, 3069, 293, 11441, 264, 958, 1785, 295, 264, 1002, 13, 50714, 50714, 3987, 321, 366, 3097, 527, 2316, 11, 321, 603, 767, 312, 22107, 264, 958, 1785, 295, 264, 1002, 11, 314, 1804, 502, 13, 51164, 51164, 2264, 11, 370, 281, 3847, 527, 2128, 2316, 510, 11, 321, 747, 264, 1785, 4904, 13, 492, 747, 364, 3069, 498, 321, 362, 364, 3069, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09418547006300938, "compression_ratio": 1.7277777777777779, "no_speech_prob": 1.5927973436191678e-05}, {"id": 808, "seek": 467006, "start": 4670.06, "end": 4684.06, "text": " We have a latent variable and our prediction goes into a cost function. That diagram is exactly identical to the one on the right.", "tokens": [50364, 492, 362, 257, 48994, 7006, 293, 527, 17630, 1709, 666, 257, 2063, 2445, 13, 663, 10686, 307, 2293, 14800, 281, 264, 472, 322, 264, 558, 13, 51064, 51064, 1779, 11, 309, 311, 264, 912, 13, 467, 311, 2293, 264, 912, 10686, 11, 3993, 286, 7472, 264, 29614, 666, 732, 16679, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12336320010098543, "compression_ratio": 1.457516339869281, "no_speech_prob": 1.1122116120532155e-05}, {"id": 809, "seek": 467006, "start": 4684.06, "end": 4693.06, "text": " Right, it's the same. It's exactly the same diagram, except I split the FM into two modules.", "tokens": [50364, 492, 362, 257, 48994, 7006, 293, 527, 17630, 1709, 666, 257, 2063, 2445, 13, 663, 10686, 307, 2293, 14800, 281, 264, 472, 322, 264, 558, 13, 51064, 51064, 1779, 11, 309, 311, 264, 912, 13, 467, 311, 2293, 264, 912, 10686, 11, 3993, 286, 7472, 264, 29614, 666, 732, 16679, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12336320010098543, "compression_ratio": 1.457516339869281, "no_speech_prob": 1.1122116120532155e-05}, {"id": 810, "seek": 469306, "start": 4693.06, "end": 4702.06, "text": " I've given it a particular architecture. In fact, I could make this more explicit.", "tokens": [50364, 286, 600, 2212, 309, 257, 1729, 9482, 13, 682, 1186, 11, 286, 727, 652, 341, 544, 13691, 13, 50814, 50814, 286, 519, 291, 362, 264, 1687, 5060, 15247, 8209, 13, 50964, 50964, 286, 360, 11, 2086, 13, 509, 500, 380, 411, 300, 11, 7020, 30, 51214, 51214, 407, 341, 576, 312, 452, 2128, 2316, 13, 51364, 51364, 2264, 11, 370, 300, 311, 437, 311, 1854, 341, 2424, 13, 15123, 264, 2128, 2316, 2424, 510, 307, 341, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09770126695986148, "compression_ratio": 1.5, "no_speech_prob": 1.669013909122441e-05}, {"id": 811, "seek": 469306, "start": 4702.06, "end": 4705.06, "text": " I think you have the super thick marker selected.", "tokens": [50364, 286, 600, 2212, 309, 257, 1729, 9482, 13, 682, 1186, 11, 286, 727, 652, 341, 544, 13691, 13, 50814, 50814, 286, 519, 291, 362, 264, 1687, 5060, 15247, 8209, 13, 50964, 50964, 286, 360, 11, 2086, 13, 509, 500, 380, 411, 300, 11, 7020, 30, 51214, 51214, 407, 341, 576, 312, 452, 2128, 2316, 13, 51364, 51364, 2264, 11, 370, 300, 311, 437, 311, 1854, 341, 2424, 13, 15123, 264, 2128, 2316, 2424, 510, 307, 341, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09770126695986148, "compression_ratio": 1.5, "no_speech_prob": 1.669013909122441e-05}, {"id": 812, "seek": 469306, "start": 4705.06, "end": 4710.06, "text": " I do, yes. You don't like that, huh?", "tokens": [50364, 286, 600, 2212, 309, 257, 1729, 9482, 13, 682, 1186, 11, 286, 727, 652, 341, 544, 13691, 13, 50814, 50814, 286, 519, 291, 362, 264, 1687, 5060, 15247, 8209, 13, 50964, 50964, 286, 360, 11, 2086, 13, 509, 500, 380, 411, 300, 11, 7020, 30, 51214, 51214, 407, 341, 576, 312, 452, 2128, 2316, 13, 51364, 51364, 2264, 11, 370, 300, 311, 437, 311, 1854, 341, 2424, 13, 15123, 264, 2128, 2316, 2424, 510, 307, 341, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09770126695986148, "compression_ratio": 1.5, "no_speech_prob": 1.669013909122441e-05}, {"id": 813, "seek": 469306, "start": 4710.06, "end": 4713.06, "text": " So this would be my forward model.", "tokens": [50364, 286, 600, 2212, 309, 257, 1729, 9482, 13, 682, 1186, 11, 286, 727, 652, 341, 544, 13691, 13, 50814, 50814, 286, 519, 291, 362, 264, 1687, 5060, 15247, 8209, 13, 50964, 50964, 286, 360, 11, 2086, 13, 509, 500, 380, 411, 300, 11, 7020, 30, 51214, 51214, 407, 341, 576, 312, 452, 2128, 2316, 13, 51364, 51364, 2264, 11, 370, 300, 311, 437, 311, 1854, 341, 2424, 13, 15123, 264, 2128, 2316, 2424, 510, 307, 341, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09770126695986148, "compression_ratio": 1.5, "no_speech_prob": 1.669013909122441e-05}, {"id": 814, "seek": 469306, "start": 4713.06, "end": 4722.06, "text": " OK, so that's what's inside this box. Inside the forward model box here is this.", "tokens": [50364, 286, 600, 2212, 309, 257, 1729, 9482, 13, 682, 1186, 11, 286, 727, 652, 341, 544, 13691, 13, 50814, 50814, 286, 519, 291, 362, 264, 1687, 5060, 15247, 8209, 13, 50964, 50964, 286, 360, 11, 2086, 13, 509, 500, 380, 411, 300, 11, 7020, 30, 51214, 51214, 407, 341, 576, 312, 452, 2128, 2316, 13, 51364, 51364, 2264, 11, 370, 300, 311, 437, 311, 1854, 341, 2424, 13, 15123, 264, 2128, 2316, 2424, 510, 307, 341, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09770126695986148, "compression_ratio": 1.5, "no_speech_prob": 1.669013909122441e-05}, {"id": 815, "seek": 472206, "start": 4722.06, "end": 4731.06, "text": " And you know, I renamed ST is now called X and ST plus 1 is now called Y bar.", "tokens": [50364, 400, 291, 458, 11, 286, 40949, 4904, 307, 586, 1219, 1783, 293, 4904, 1804, 502, 307, 586, 1219, 398, 2159, 13, 50814, 50814, 286, 914, 11, 309, 311, 586, 1219, 398, 11, 457, 309, 311, 264, 912, 551, 5911, 13, 50964, 50964, 407, 309, 311, 264, 912, 9005, 300, 321, 2825, 466, 949, 294, 48994, 7006, 2281, 2361, 5245, 11, 4476, 13, 51264, 51264, 583, 586, 321, 434, 516, 281, 764, 341, 281, 3847, 257, 2128, 2316, 281, 6069, 437, 311, 516, 281, 1051, 294, 264, 1002, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13486112718996796, "compression_ratio": 1.6401869158878504, "no_speech_prob": 1.721546141197905e-05}, {"id": 816, "seek": 472206, "start": 4731.06, "end": 4734.06, "text": " I mean, it's now called Y, but it's the same thing otherwise.", "tokens": [50364, 400, 291, 458, 11, 286, 40949, 4904, 307, 586, 1219, 1783, 293, 4904, 1804, 502, 307, 586, 1219, 398, 2159, 13, 50814, 50814, 286, 914, 11, 309, 311, 586, 1219, 398, 11, 457, 309, 311, 264, 912, 551, 5911, 13, 50964, 50964, 407, 309, 311, 264, 912, 9005, 300, 321, 2825, 466, 949, 294, 48994, 7006, 2281, 2361, 5245, 11, 4476, 13, 51264, 51264, 583, 586, 321, 434, 516, 281, 764, 341, 281, 3847, 257, 2128, 2316, 281, 6069, 437, 311, 516, 281, 1051, 294, 264, 1002, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13486112718996796, "compression_ratio": 1.6401869158878504, "no_speech_prob": 1.721546141197905e-05}, {"id": 817, "seek": 472206, "start": 4734.06, "end": 4740.06, "text": " So it's the same scenario that we talked about before in latent variable energy based models, essentially.", "tokens": [50364, 400, 291, 458, 11, 286, 40949, 4904, 307, 586, 1219, 1783, 293, 4904, 1804, 502, 307, 586, 1219, 398, 2159, 13, 50814, 50814, 286, 914, 11, 309, 311, 586, 1219, 398, 11, 457, 309, 311, 264, 912, 551, 5911, 13, 50964, 50964, 407, 309, 311, 264, 912, 9005, 300, 321, 2825, 466, 949, 294, 48994, 7006, 2281, 2361, 5245, 11, 4476, 13, 51264, 51264, 583, 586, 321, 434, 516, 281, 764, 341, 281, 3847, 257, 2128, 2316, 281, 6069, 437, 311, 516, 281, 1051, 294, 264, 1002, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13486112718996796, "compression_ratio": 1.6401869158878504, "no_speech_prob": 1.721546141197905e-05}, {"id": 818, "seek": 472206, "start": 4740.06, "end": 4747.06, "text": " But now we're going to use this to train a forward model to predict what's going to happen in the world.", "tokens": [50364, 400, 291, 458, 11, 286, 40949, 4904, 307, 586, 1219, 1783, 293, 4904, 1804, 502, 307, 586, 1219, 398, 2159, 13, 50814, 50814, 286, 914, 11, 309, 311, 586, 1219, 398, 11, 457, 309, 311, 264, 912, 551, 5911, 13, 50964, 50964, 407, 309, 311, 264, 912, 9005, 300, 321, 2825, 466, 949, 294, 48994, 7006, 2281, 2361, 5245, 11, 4476, 13, 51264, 51264, 583, 586, 321, 434, 516, 281, 764, 341, 281, 3847, 257, 2128, 2316, 281, 6069, 437, 311, 516, 281, 1051, 294, 264, 1002, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13486112718996796, "compression_ratio": 1.6401869158878504, "no_speech_prob": 1.721546141197905e-05}, {"id": 819, "seek": 474706, "start": 4747.06, "end": 4759.06, "text": " So we may have to play the same tricks that we played, that we talked about last week,", "tokens": [50364, 407, 321, 815, 362, 281, 862, 264, 912, 11733, 300, 321, 3737, 11, 300, 321, 2825, 466, 1036, 1243, 11, 50964, 50964], "temperature": 0.0, "avg_logprob": -0.13502190510431925, "compression_ratio": 1.1466666666666667, "no_speech_prob": 8.937958227761555e-06}, {"id": 820, "seek": 475906, "start": 4759.06, "end": 4784.06, "text": " which is that last week what we explained was that we can take, OK, the way I drew this last week was slightly different.", "tokens": [50364, 597, 307, 300, 1036, 1243, 437, 321, 8825, 390, 300, 321, 393, 747, 11, 2264, 11, 264, 636, 286, 12804, 341, 1036, 1243, 390, 4748, 819, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12708862366214876, "compression_ratio": 1.2604166666666667, "no_speech_prob": 4.537666609394364e-05}, {"id": 821, "seek": 478406, "start": 4784.06, "end": 4793.06, "text": " What I explained last week is that we can, if we have while we are training our forward model, we have a pair X and Y.", "tokens": [50364, 708, 286, 8825, 1036, 1243, 307, 300, 321, 393, 11, 498, 321, 362, 1339, 321, 366, 3097, 527, 2128, 2316, 11, 321, 362, 257, 6119, 1783, 293, 398, 13, 50814, 50814, 400, 264, 636, 321, 915, 264, 2158, 295, 1176, 307, 538, 46608, 264, 2281, 365, 3104, 281, 1176, 11, 558, 30, 51064, 51064], "temperature": 0.0, "avg_logprob": -0.10580627407346453, "compression_ratio": 1.38, "no_speech_prob": 4.42398095401586e-06}, {"id": 822, "seek": 478406, "start": 4793.06, "end": 4798.06, "text": " And the way we find the value of Z is by minimizing the energy with respect to Z, right?", "tokens": [50364, 708, 286, 8825, 1036, 1243, 307, 300, 321, 393, 11, 498, 321, 362, 1339, 321, 366, 3097, 527, 2128, 2316, 11, 321, 362, 257, 6119, 1783, 293, 398, 13, 50814, 50814, 400, 264, 636, 321, 915, 264, 2158, 295, 1176, 307, 538, 46608, 264, 2281, 365, 3104, 281, 1176, 11, 558, 30, 51064, 51064], "temperature": 0.0, "avg_logprob": -0.10580627407346453, "compression_ratio": 1.38, "no_speech_prob": 4.42398095401586e-06}, {"id": 823, "seek": 479806, "start": 4798.06, "end": 4815.06, "text": " So we basically find the star, which is the argmin of C of Y and Y bar, Y bar being the output of our predictor of our system.", "tokens": [50364, 407, 321, 1936, 915, 264, 3543, 11, 597, 307, 264, 3882, 2367, 295, 383, 295, 398, 293, 398, 2159, 11, 398, 2159, 885, 264, 5598, 295, 527, 6069, 284, 295, 527, 1185, 13, 51214, 51214, 2264, 11, 293, 550, 321, 360, 472, 1823, 295, 16235, 23475, 13, 51414, 51414, 407, 321, 1319, 264, 9834, 295, 527, 2302, 1185, 4650, 281, 264, 16235, 295, 300, 2063, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10092592920575823, "compression_ratio": 1.5621301775147929, "no_speech_prob": 1.0611635843815748e-05}, {"id": 824, "seek": 479806, "start": 4815.06, "end": 4819.06, "text": " OK, and then we do one step of gradient descent.", "tokens": [50364, 407, 321, 1936, 915, 264, 3543, 11, 597, 307, 264, 3882, 2367, 295, 383, 295, 398, 293, 398, 2159, 11, 398, 2159, 885, 264, 5598, 295, 527, 6069, 284, 295, 527, 1185, 13, 51214, 51214, 2264, 11, 293, 550, 321, 360, 472, 1823, 295, 16235, 23475, 13, 51414, 51414, 407, 321, 1319, 264, 9834, 295, 527, 2302, 1185, 4650, 281, 264, 16235, 295, 300, 2063, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10092592920575823, "compression_ratio": 1.5621301775147929, "no_speech_prob": 1.0611635843815748e-05}, {"id": 825, "seek": 479806, "start": 4819.06, "end": 4826.06, "text": " So we change the parameters of our entire system according to the gradient of that cost.", "tokens": [50364, 407, 321, 1936, 915, 264, 3543, 11, 597, 307, 264, 3882, 2367, 295, 383, 295, 398, 293, 398, 2159, 11, 398, 2159, 885, 264, 5598, 295, 527, 6069, 284, 295, 527, 1185, 13, 51214, 51214, 2264, 11, 293, 550, 321, 360, 472, 1823, 295, 16235, 23475, 13, 51414, 51414, 407, 321, 1319, 264, 9834, 295, 527, 2302, 1185, 4650, 281, 264, 16235, 295, 300, 2063, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10092592920575823, "compression_ratio": 1.5621301775147929, "no_speech_prob": 1.0611635843815748e-05}, {"id": 826, "seek": 482606, "start": 4826.06, "end": 4834.06, "text": " But for this to work, we had to regularize Z, limit its information content.", "tokens": [50364, 583, 337, 341, 281, 589, 11, 321, 632, 281, 3890, 1125, 1176, 11, 4948, 1080, 1589, 2701, 13, 50764, 50764, 400, 321, 362, 281, 360, 264, 912, 510, 13, 1545, 307, 300, 30, 51014, 51014, 1042, 11, 510, 321, 434, 1382, 281, 5039, 257, 17630, 1154, 13, 51264, 51264, 583, 3811, 11, 293, 321, 2825, 466, 341, 257, 1916, 295, 3259, 2057, 11, 286, 976, 291, 364, 1783, 293, 257, 398, 11, 51564, 51564, 293, 291, 915, 264, 1176, 300, 4464, 5660, 264, 4787, 2281, 293, 264, 1176, 307, 406, 3890, 1602, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09091179641251712, "compression_ratio": 1.5446428571428572, "no_speech_prob": 9.665492143540177e-06}, {"id": 827, "seek": 482606, "start": 4834.06, "end": 4839.06, "text": " And we have to do the same here. Why is that?", "tokens": [50364, 583, 337, 341, 281, 589, 11, 321, 632, 281, 3890, 1125, 1176, 11, 4948, 1080, 1589, 2701, 13, 50764, 50764, 400, 321, 362, 281, 360, 264, 912, 510, 13, 1545, 307, 300, 30, 51014, 51014, 1042, 11, 510, 321, 434, 1382, 281, 5039, 257, 17630, 1154, 13, 51264, 51264, 583, 3811, 11, 293, 321, 2825, 466, 341, 257, 1916, 295, 3259, 2057, 11, 286, 976, 291, 364, 1783, 293, 257, 398, 11, 51564, 51564, 293, 291, 915, 264, 1176, 300, 4464, 5660, 264, 4787, 2281, 293, 264, 1176, 307, 406, 3890, 1602, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09091179641251712, "compression_ratio": 1.5446428571428572, "no_speech_prob": 9.665492143540177e-06}, {"id": 828, "seek": 482606, "start": 4839.06, "end": 4844.06, "text": " Well, here we're trying to solve a prediction problem.", "tokens": [50364, 583, 337, 341, 281, 589, 11, 321, 632, 281, 3890, 1125, 1176, 11, 4948, 1080, 1589, 2701, 13, 50764, 50764, 400, 321, 362, 281, 360, 264, 912, 510, 13, 1545, 307, 300, 30, 51014, 51014, 1042, 11, 510, 321, 434, 1382, 281, 5039, 257, 17630, 1154, 13, 51264, 51264, 583, 3811, 11, 293, 321, 2825, 466, 341, 257, 1916, 295, 3259, 2057, 11, 286, 976, 291, 364, 1783, 293, 257, 398, 11, 51564, 51564, 293, 291, 915, 264, 1176, 300, 4464, 5660, 264, 4787, 2281, 293, 264, 1176, 307, 406, 3890, 1602, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09091179641251712, "compression_ratio": 1.5446428571428572, "no_speech_prob": 9.665492143540177e-06}, {"id": 829, "seek": 482606, "start": 4844.06, "end": 4850.06, "text": " But imagine, and we talked about this a couple of weeks ago, I give you an X and a Y,", "tokens": [50364, 583, 337, 341, 281, 589, 11, 321, 632, 281, 3890, 1125, 1176, 11, 4948, 1080, 1589, 2701, 13, 50764, 50764, 400, 321, 362, 281, 360, 264, 912, 510, 13, 1545, 307, 300, 30, 51014, 51014, 1042, 11, 510, 321, 434, 1382, 281, 5039, 257, 17630, 1154, 13, 51264, 51264, 583, 3811, 11, 293, 321, 2825, 466, 341, 257, 1916, 295, 3259, 2057, 11, 286, 976, 291, 364, 1783, 293, 257, 398, 11, 51564, 51564, 293, 291, 915, 264, 1176, 300, 4464, 5660, 264, 4787, 2281, 293, 264, 1176, 307, 406, 3890, 1602, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09091179641251712, "compression_ratio": 1.5446428571428572, "no_speech_prob": 9.665492143540177e-06}, {"id": 830, "seek": 482606, "start": 4850.06, "end": 4855.06, "text": " and you find the Z that minimizes the overall energy and the Z is not regularized.", "tokens": [50364, 583, 337, 341, 281, 589, 11, 321, 632, 281, 3890, 1125, 1176, 11, 4948, 1080, 1589, 2701, 13, 50764, 50764, 400, 321, 362, 281, 360, 264, 912, 510, 13, 1545, 307, 300, 30, 51014, 51014, 1042, 11, 510, 321, 434, 1382, 281, 5039, 257, 17630, 1154, 13, 51264, 51264, 583, 3811, 11, 293, 321, 2825, 466, 341, 257, 1916, 295, 3259, 2057, 11, 286, 976, 291, 364, 1783, 293, 257, 398, 11, 51564, 51564, 293, 291, 915, 264, 1176, 300, 4464, 5660, 264, 4787, 2281, 293, 264, 1176, 307, 406, 3890, 1602, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09091179641251712, "compression_ratio": 1.5446428571428572, "no_speech_prob": 9.665492143540177e-06}, {"id": 831, "seek": 485506, "start": 4855.06, "end": 4864.06, "text": " If Z is the same dimension as Y, there's probably going to be a Z for any Y that makes the cost function zero, right?", "tokens": [50364, 759, 1176, 307, 264, 912, 10139, 382, 398, 11, 456, 311, 1391, 516, 281, 312, 257, 1176, 337, 604, 398, 300, 1669, 264, 2063, 2445, 4018, 11, 558, 30, 50814, 50814, 759, 456, 311, 1547, 6042, 294, 1176, 11, 456, 311, 1009, 516, 281, 312, 257, 2158, 295, 1176, 300, 1669, 264, 2063, 2445, 4018, 13, 51214, 51214, 400, 300, 311, 1578, 570, 300, 1355, 452, 2281, 2445, 307, 516, 281, 312, 2584, 4962, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07285766360125964, "compression_ratio": 1.7318435754189945, "no_speech_prob": 3.683791874209419e-05}, {"id": 832, "seek": 485506, "start": 4864.06, "end": 4872.06, "text": " If there's enough capacity in Z, there's always going to be a value of Z that makes the cost function zero.", "tokens": [50364, 759, 1176, 307, 264, 912, 10139, 382, 398, 11, 456, 311, 1391, 516, 281, 312, 257, 1176, 337, 604, 398, 300, 1669, 264, 2063, 2445, 4018, 11, 558, 30, 50814, 50814, 759, 456, 311, 1547, 6042, 294, 1176, 11, 456, 311, 1009, 516, 281, 312, 257, 2158, 295, 1176, 300, 1669, 264, 2063, 2445, 4018, 13, 51214, 51214, 400, 300, 311, 1578, 570, 300, 1355, 452, 2281, 2445, 307, 516, 281, 312, 2584, 4962, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07285766360125964, "compression_ratio": 1.7318435754189945, "no_speech_prob": 3.683791874209419e-05}, {"id": 833, "seek": 485506, "start": 4872.06, "end": 4876.06, "text": " And that's bad because that means my energy function is going to be completely flat.", "tokens": [50364, 759, 1176, 307, 264, 912, 10139, 382, 398, 11, 456, 311, 1391, 516, 281, 312, 257, 1176, 337, 604, 398, 300, 1669, 264, 2063, 2445, 4018, 11, 558, 30, 50814, 50814, 759, 456, 311, 1547, 6042, 294, 1176, 11, 456, 311, 1009, 516, 281, 312, 257, 2158, 295, 1176, 300, 1669, 264, 2063, 2445, 4018, 13, 51214, 51214, 400, 300, 311, 1578, 570, 300, 1355, 452, 2281, 2445, 307, 516, 281, 312, 2584, 4962, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07285766360125964, "compression_ratio": 1.7318435754189945, "no_speech_prob": 3.683791874209419e-05}, {"id": 834, "seek": 487606, "start": 4876.06, "end": 4886.06, "text": " It's going to be zero everywhere. And I need it to be small on the training samples and high outside of the region of high data density.", "tokens": [50364, 467, 311, 516, 281, 312, 4018, 5315, 13, 400, 286, 643, 309, 281, 312, 1359, 322, 264, 3097, 10938, 293, 1090, 2380, 295, 264, 4458, 295, 1090, 1412, 10305, 13, 50864, 50864, 400, 437, 321, 1866, 294, 264, 1036, 1916, 295, 3259, 307, 300, 538, 3890, 3319, 1176, 11, 22083, 1080, 6042, 11, 51114, 51114, 2139, 538, 1455, 309, 637, 11668, 11, 337, 1365, 11, 420, 1455, 309, 27706, 420, 538, 1455, 309, 24518, 11, 550, 321, 393, 4948, 1080, 6042, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05926150499388229, "compression_ratio": 1.6650717703349283, "no_speech_prob": 2.6004081519204192e-05}, {"id": 835, "seek": 487606, "start": 4886.06, "end": 4891.06, "text": " And what we saw in the last couple of weeks is that by regularizing Z, limiting its capacity,", "tokens": [50364, 467, 311, 516, 281, 312, 4018, 5315, 13, 400, 286, 643, 309, 281, 312, 1359, 322, 264, 3097, 10938, 293, 1090, 2380, 295, 264, 4458, 295, 1090, 1412, 10305, 13, 50864, 50864, 400, 437, 321, 1866, 294, 264, 1036, 1916, 295, 3259, 307, 300, 538, 3890, 3319, 1176, 11, 22083, 1080, 6042, 11, 51114, 51114, 2139, 538, 1455, 309, 637, 11668, 11, 337, 1365, 11, 420, 1455, 309, 27706, 420, 538, 1455, 309, 24518, 11, 550, 321, 393, 4948, 1080, 6042, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05926150499388229, "compression_ratio": 1.6650717703349283, "no_speech_prob": 2.6004081519204192e-05}, {"id": 836, "seek": 487606, "start": 4891.06, "end": 4903.06, "text": " either by making it sparse, for example, or making it discrete or by making it noisy, then we can limit its capacity.", "tokens": [50364, 467, 311, 516, 281, 312, 4018, 5315, 13, 400, 286, 643, 309, 281, 312, 1359, 322, 264, 3097, 10938, 293, 1090, 2380, 295, 264, 4458, 295, 1090, 1412, 10305, 13, 50864, 50864, 400, 437, 321, 1866, 294, 264, 1036, 1916, 295, 3259, 307, 300, 538, 3890, 3319, 1176, 11, 22083, 1080, 6042, 11, 51114, 51114, 2139, 538, 1455, 309, 637, 11668, 11, 337, 1365, 11, 420, 1455, 309, 27706, 420, 538, 1455, 309, 24518, 11, 550, 321, 393, 4948, 1080, 6042, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05926150499388229, "compression_ratio": 1.6650717703349283, "no_speech_prob": 2.6004081519204192e-05}, {"id": 837, "seek": 490306, "start": 4903.06, "end": 4907.06, "text": " Why do we need ZT if you already have AT?", "tokens": [50364, 1545, 360, 321, 643, 1176, 51, 498, 291, 1217, 362, 8872, 30, 50564, 50564, 1042, 11, 370, 8872, 307, 264, 3069, 291, 747, 11, 558, 30, 50814, 50814, 2264, 11, 286, 478, 516, 281, 980, 291, 286, 478, 516, 281, 718, 341, 551, 352, 13, 51164, 51164, 2264, 11, 457, 291, 500, 380, 458, 294, 597, 3513, 309, 311, 516, 281, 352, 11, 558, 30, 51364, 51364, 407, 718, 311, 584, 309, 1709, 341, 636, 13, 583, 286, 362, 281, 6069, 294, 7295, 597, 636, 309, 311, 516, 281, 352, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12413761741236637, "compression_ratio": 1.601063829787234, "no_speech_prob": 0.00012692282325588167}, {"id": 838, "seek": 490306, "start": 4907.06, "end": 4912.06, "text": " Well, so AT is the action you take, right?", "tokens": [50364, 1545, 360, 321, 643, 1176, 51, 498, 291, 1217, 362, 8872, 30, 50564, 50564, 1042, 11, 370, 8872, 307, 264, 3069, 291, 747, 11, 558, 30, 50814, 50814, 2264, 11, 286, 478, 516, 281, 980, 291, 286, 478, 516, 281, 718, 341, 551, 352, 13, 51164, 51164, 2264, 11, 457, 291, 500, 380, 458, 294, 597, 3513, 309, 311, 516, 281, 352, 11, 558, 30, 51364, 51364, 407, 718, 311, 584, 309, 1709, 341, 636, 13, 583, 286, 362, 281, 6069, 294, 7295, 597, 636, 309, 311, 516, 281, 352, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12413761741236637, "compression_ratio": 1.601063829787234, "no_speech_prob": 0.00012692282325588167}, {"id": 839, "seek": 490306, "start": 4912.06, "end": 4919.06, "text": " OK, I'm going to tell you I'm going to let this thing go.", "tokens": [50364, 1545, 360, 321, 643, 1176, 51, 498, 291, 1217, 362, 8872, 30, 50564, 50564, 1042, 11, 370, 8872, 307, 264, 3069, 291, 747, 11, 558, 30, 50814, 50814, 2264, 11, 286, 478, 516, 281, 980, 291, 286, 478, 516, 281, 718, 341, 551, 352, 13, 51164, 51164, 2264, 11, 457, 291, 500, 380, 458, 294, 597, 3513, 309, 311, 516, 281, 352, 11, 558, 30, 51364, 51364, 407, 718, 311, 584, 309, 1709, 341, 636, 13, 583, 286, 362, 281, 6069, 294, 7295, 597, 636, 309, 311, 516, 281, 352, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12413761741236637, "compression_ratio": 1.601063829787234, "no_speech_prob": 0.00012692282325588167}, {"id": 840, "seek": 490306, "start": 4919.06, "end": 4923.06, "text": " OK, but you don't know in which direction it's going to go, right?", "tokens": [50364, 1545, 360, 321, 643, 1176, 51, 498, 291, 1217, 362, 8872, 30, 50564, 50564, 1042, 11, 370, 8872, 307, 264, 3069, 291, 747, 11, 558, 30, 50814, 50814, 2264, 11, 286, 478, 516, 281, 980, 291, 286, 478, 516, 281, 718, 341, 551, 352, 13, 51164, 51164, 2264, 11, 457, 291, 500, 380, 458, 294, 597, 3513, 309, 311, 516, 281, 352, 11, 558, 30, 51364, 51364, 407, 718, 311, 584, 309, 1709, 341, 636, 13, 583, 286, 362, 281, 6069, 294, 7295, 597, 636, 309, 311, 516, 281, 352, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12413761741236637, "compression_ratio": 1.601063829787234, "no_speech_prob": 0.00012692282325588167}, {"id": 841, "seek": 490306, "start": 4923.06, "end": 4928.06, "text": " So let's say it goes this way. But I have to predict in advance which way it's going to go.", "tokens": [50364, 1545, 360, 321, 643, 1176, 51, 498, 291, 1217, 362, 8872, 30, 50564, 50564, 1042, 11, 370, 8872, 307, 264, 3069, 291, 747, 11, 558, 30, 50814, 50814, 2264, 11, 286, 478, 516, 281, 980, 291, 286, 478, 516, 281, 718, 341, 551, 352, 13, 51164, 51164, 2264, 11, 457, 291, 500, 380, 458, 294, 597, 3513, 309, 311, 516, 281, 352, 11, 558, 30, 51364, 51364, 407, 718, 311, 584, 309, 1709, 341, 636, 13, 583, 286, 362, 281, 6069, 294, 7295, 597, 636, 309, 311, 516, 281, 352, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12413761741236637, "compression_ratio": 1.601063829787234, "no_speech_prob": 0.00012692282325588167}, {"id": 842, "seek": 492806, "start": 4928.06, "end": 4937.06, "text": " It's like, OK, here's a better situation. You are goalie playing soccer.", "tokens": [50364, 467, 311, 411, 11, 2264, 11, 510, 311, 257, 1101, 2590, 13, 509, 366, 3387, 414, 2433, 15469, 13, 50814, 50814, 2264, 11, 293, 309, 311, 257, 16263, 4437, 13, 407, 291, 434, 294, 1868, 295, 264, 4437, 260, 294, 1868, 295, 291, 13, 51164, 51164, 400, 264, 2146, 307, 516, 281, 4437, 264, 2594, 293, 291, 434, 516, 281, 362, 281, 3012, 472, 636, 420, 264, 661, 13, 51364, 51364, 400, 291, 362, 281, 652, 257, 3922, 466, 11233, 1411, 420, 558, 13, 51514, 51514, 400, 291, 362, 281, 652, 300, 3537, 2361, 322, 437, 291, 11441, 490, 264, 954, 13, 51714, 51714, 583, 291, 500, 380, 458, 2293, 437, 264, 2594, 307, 516, 281, 360, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08985788314068904, "compression_ratio": 1.7590361445783131, "no_speech_prob": 6.012661469867453e-05}, {"id": 843, "seek": 492806, "start": 4937.06, "end": 4944.06, "text": " OK, and it's a penalty kick. So you're in front of the kicker in front of you.", "tokens": [50364, 467, 311, 411, 11, 2264, 11, 510, 311, 257, 1101, 2590, 13, 509, 366, 3387, 414, 2433, 15469, 13, 50814, 50814, 2264, 11, 293, 309, 311, 257, 16263, 4437, 13, 407, 291, 434, 294, 1868, 295, 264, 4437, 260, 294, 1868, 295, 291, 13, 51164, 51164, 400, 264, 2146, 307, 516, 281, 4437, 264, 2594, 293, 291, 434, 516, 281, 362, 281, 3012, 472, 636, 420, 264, 661, 13, 51364, 51364, 400, 291, 362, 281, 652, 257, 3922, 466, 11233, 1411, 420, 558, 13, 51514, 51514, 400, 291, 362, 281, 652, 300, 3537, 2361, 322, 437, 291, 11441, 490, 264, 954, 13, 51714, 51714, 583, 291, 500, 380, 458, 2293, 437, 264, 2594, 307, 516, 281, 360, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08985788314068904, "compression_ratio": 1.7590361445783131, "no_speech_prob": 6.012661469867453e-05}, {"id": 844, "seek": 492806, "start": 4944.06, "end": 4948.06, "text": " And the guy is going to kick the ball and you're going to have to jump one way or the other.", "tokens": [50364, 467, 311, 411, 11, 2264, 11, 510, 311, 257, 1101, 2590, 13, 509, 366, 3387, 414, 2433, 15469, 13, 50814, 50814, 2264, 11, 293, 309, 311, 257, 16263, 4437, 13, 407, 291, 434, 294, 1868, 295, 264, 4437, 260, 294, 1868, 295, 291, 13, 51164, 51164, 400, 264, 2146, 307, 516, 281, 4437, 264, 2594, 293, 291, 434, 516, 281, 362, 281, 3012, 472, 636, 420, 264, 661, 13, 51364, 51364, 400, 291, 362, 281, 652, 257, 3922, 466, 11233, 1411, 420, 558, 13, 51514, 51514, 400, 291, 362, 281, 652, 300, 3537, 2361, 322, 437, 291, 11441, 490, 264, 954, 13, 51714, 51714, 583, 291, 500, 380, 458, 2293, 437, 264, 2594, 307, 516, 281, 360, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08985788314068904, "compression_ratio": 1.7590361445783131, "no_speech_prob": 6.012661469867453e-05}, {"id": 845, "seek": 492806, "start": 4948.06, "end": 4951.06, "text": " And you have to make a choice about jumping left or right.", "tokens": [50364, 467, 311, 411, 11, 2264, 11, 510, 311, 257, 1101, 2590, 13, 509, 366, 3387, 414, 2433, 15469, 13, 50814, 50814, 2264, 11, 293, 309, 311, 257, 16263, 4437, 13, 407, 291, 434, 294, 1868, 295, 264, 4437, 260, 294, 1868, 295, 291, 13, 51164, 51164, 400, 264, 2146, 307, 516, 281, 4437, 264, 2594, 293, 291, 434, 516, 281, 362, 281, 3012, 472, 636, 420, 264, 661, 13, 51364, 51364, 400, 291, 362, 281, 652, 257, 3922, 466, 11233, 1411, 420, 558, 13, 51514, 51514, 400, 291, 362, 281, 652, 300, 3537, 2361, 322, 437, 291, 11441, 490, 264, 954, 13, 51714, 51714, 583, 291, 500, 380, 458, 2293, 437, 264, 2594, 307, 516, 281, 360, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08985788314068904, "compression_ratio": 1.7590361445783131, "no_speech_prob": 6.012661469867453e-05}, {"id": 846, "seek": 492806, "start": 4951.06, "end": 4955.06, "text": " And you have to make that decision based on what you observe from the person.", "tokens": [50364, 467, 311, 411, 11, 2264, 11, 510, 311, 257, 1101, 2590, 13, 509, 366, 3387, 414, 2433, 15469, 13, 50814, 50814, 2264, 11, 293, 309, 311, 257, 16263, 4437, 13, 407, 291, 434, 294, 1868, 295, 264, 4437, 260, 294, 1868, 295, 291, 13, 51164, 51164, 400, 264, 2146, 307, 516, 281, 4437, 264, 2594, 293, 291, 434, 516, 281, 362, 281, 3012, 472, 636, 420, 264, 661, 13, 51364, 51364, 400, 291, 362, 281, 652, 257, 3922, 466, 11233, 1411, 420, 558, 13, 51514, 51514, 400, 291, 362, 281, 652, 300, 3537, 2361, 322, 437, 291, 11441, 490, 264, 954, 13, 51714, 51714, 583, 291, 500, 380, 458, 2293, 437, 264, 2594, 307, 516, 281, 360, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08985788314068904, "compression_ratio": 1.7590361445783131, "no_speech_prob": 6.012661469867453e-05}, {"id": 847, "seek": 492806, "start": 4955.06, "end": 4957.06, "text": " But you don't know exactly what the ball is going to do.", "tokens": [50364, 467, 311, 411, 11, 2264, 11, 510, 311, 257, 1101, 2590, 13, 509, 366, 3387, 414, 2433, 15469, 13, 50814, 50814, 2264, 11, 293, 309, 311, 257, 16263, 4437, 13, 407, 291, 434, 294, 1868, 295, 264, 4437, 260, 294, 1868, 295, 291, 13, 51164, 51164, 400, 264, 2146, 307, 516, 281, 4437, 264, 2594, 293, 291, 434, 516, 281, 362, 281, 3012, 472, 636, 420, 264, 661, 13, 51364, 51364, 400, 291, 362, 281, 652, 257, 3922, 466, 11233, 1411, 420, 558, 13, 51514, 51514, 400, 291, 362, 281, 652, 300, 3537, 2361, 322, 437, 291, 11441, 490, 264, 954, 13, 51714, 51714, 583, 291, 500, 380, 458, 2293, 437, 264, 2594, 307, 516, 281, 360, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08985788314068904, "compression_ratio": 1.7590361445783131, "no_speech_prob": 6.012661469867453e-05}, {"id": 848, "seek": 495706, "start": 4957.06, "end": 4962.06, "text": " A is which direction you jump in. It's basically how you jump.", "tokens": [50364, 316, 307, 597, 3513, 291, 3012, 294, 13, 467, 311, 1936, 577, 291, 3012, 13, 50614, 50614, 1176, 307, 437, 291, 500, 380, 458, 466, 264, 4256, 294, 1868, 295, 291, 884, 13, 50864, 50864, 509, 500, 380, 458, 264, 1785, 295, 264, 1002, 13, 509, 500, 380, 458, 264, 1785, 295, 264, 3567, 295, 341, 2146, 13, 51014, 51014, 400, 370, 291, 500, 380, 458, 498, 415, 311, 516, 281, 3076, 1411, 420, 558, 420, 493, 420, 760, 13, 51264, 51264, 2264, 11, 300, 311, 264, 2649, 13, 51414, 51414, 1176, 307, 437, 291, 2644, 458, 466, 264, 1002, 300, 307, 4818, 281, 652, 264, 17630, 295, 264, 958, 1785, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0660977815759593, "compression_ratio": 1.883408071748879, "no_speech_prob": 3.644305616035126e-05}, {"id": 849, "seek": 495706, "start": 4962.06, "end": 4967.06, "text": " Z is what you don't know about the player in front of you doing.", "tokens": [50364, 316, 307, 597, 3513, 291, 3012, 294, 13, 467, 311, 1936, 577, 291, 3012, 13, 50614, 50614, 1176, 307, 437, 291, 500, 380, 458, 466, 264, 4256, 294, 1868, 295, 291, 884, 13, 50864, 50864, 509, 500, 380, 458, 264, 1785, 295, 264, 1002, 13, 509, 500, 380, 458, 264, 1785, 295, 264, 3567, 295, 341, 2146, 13, 51014, 51014, 400, 370, 291, 500, 380, 458, 498, 415, 311, 516, 281, 3076, 1411, 420, 558, 420, 493, 420, 760, 13, 51264, 51264, 2264, 11, 300, 311, 264, 2649, 13, 51414, 51414, 1176, 307, 437, 291, 2644, 458, 466, 264, 1002, 300, 307, 4818, 281, 652, 264, 17630, 295, 264, 958, 1785, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0660977815759593, "compression_ratio": 1.883408071748879, "no_speech_prob": 3.644305616035126e-05}, {"id": 850, "seek": 495706, "start": 4967.06, "end": 4970.06, "text": " You don't know the state of the world. You don't know the state of the brain of this guy.", "tokens": [50364, 316, 307, 597, 3513, 291, 3012, 294, 13, 467, 311, 1936, 577, 291, 3012, 13, 50614, 50614, 1176, 307, 437, 291, 500, 380, 458, 466, 264, 4256, 294, 1868, 295, 291, 884, 13, 50864, 50864, 509, 500, 380, 458, 264, 1785, 295, 264, 1002, 13, 509, 500, 380, 458, 264, 1785, 295, 264, 3567, 295, 341, 2146, 13, 51014, 51014, 400, 370, 291, 500, 380, 458, 498, 415, 311, 516, 281, 3076, 1411, 420, 558, 420, 493, 420, 760, 13, 51264, 51264, 2264, 11, 300, 311, 264, 2649, 13, 51414, 51414, 1176, 307, 437, 291, 2644, 458, 466, 264, 1002, 300, 307, 4818, 281, 652, 264, 17630, 295, 264, 958, 1785, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0660977815759593, "compression_ratio": 1.883408071748879, "no_speech_prob": 3.644305616035126e-05}, {"id": 851, "seek": 495706, "start": 4970.06, "end": 4975.06, "text": " And so you don't know if he's going to shoot left or right or up or down.", "tokens": [50364, 316, 307, 597, 3513, 291, 3012, 294, 13, 467, 311, 1936, 577, 291, 3012, 13, 50614, 50614, 1176, 307, 437, 291, 500, 380, 458, 466, 264, 4256, 294, 1868, 295, 291, 884, 13, 50864, 50864, 509, 500, 380, 458, 264, 1785, 295, 264, 1002, 13, 509, 500, 380, 458, 264, 1785, 295, 264, 3567, 295, 341, 2146, 13, 51014, 51014, 400, 370, 291, 500, 380, 458, 498, 415, 311, 516, 281, 3076, 1411, 420, 558, 420, 493, 420, 760, 13, 51264, 51264, 2264, 11, 300, 311, 264, 2649, 13, 51414, 51414, 1176, 307, 437, 291, 2644, 458, 466, 264, 1002, 300, 307, 4818, 281, 652, 264, 17630, 295, 264, 958, 1785, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0660977815759593, "compression_ratio": 1.883408071748879, "no_speech_prob": 3.644305616035126e-05}, {"id": 852, "seek": 495706, "start": 4975.06, "end": 4978.06, "text": " OK, that's the difference.", "tokens": [50364, 316, 307, 597, 3513, 291, 3012, 294, 13, 467, 311, 1936, 577, 291, 3012, 13, 50614, 50614, 1176, 307, 437, 291, 500, 380, 458, 466, 264, 4256, 294, 1868, 295, 291, 884, 13, 50864, 50864, 509, 500, 380, 458, 264, 1785, 295, 264, 1002, 13, 509, 500, 380, 458, 264, 1785, 295, 264, 3567, 295, 341, 2146, 13, 51014, 51014, 400, 370, 291, 500, 380, 458, 498, 415, 311, 516, 281, 3076, 1411, 420, 558, 420, 493, 420, 760, 13, 51264, 51264, 2264, 11, 300, 311, 264, 2649, 13, 51414, 51414, 1176, 307, 437, 291, 2644, 458, 466, 264, 1002, 300, 307, 4818, 281, 652, 264, 17630, 295, 264, 958, 1785, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0660977815759593, "compression_ratio": 1.883408071748879, "no_speech_prob": 3.644305616035126e-05}, {"id": 853, "seek": 495706, "start": 4978.06, "end": 4986.06, "text": " Z is what you cannot know about the world that is necessary to make the prediction of the next state.", "tokens": [50364, 316, 307, 597, 3513, 291, 3012, 294, 13, 467, 311, 1936, 577, 291, 3012, 13, 50614, 50614, 1176, 307, 437, 291, 500, 380, 458, 466, 264, 4256, 294, 1868, 295, 291, 884, 13, 50864, 50864, 509, 500, 380, 458, 264, 1785, 295, 264, 1002, 13, 509, 500, 380, 458, 264, 1785, 295, 264, 3567, 295, 341, 2146, 13, 51014, 51014, 400, 370, 291, 500, 380, 458, 498, 415, 311, 516, 281, 3076, 1411, 420, 558, 420, 493, 420, 760, 13, 51264, 51264, 2264, 11, 300, 311, 264, 2649, 13, 51414, 51414, 1176, 307, 437, 291, 2644, 458, 466, 264, 1002, 300, 307, 4818, 281, 652, 264, 17630, 295, 264, 958, 1785, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0660977815759593, "compression_ratio": 1.883408071748879, "no_speech_prob": 3.644305616035126e-05}, {"id": 854, "seek": 498606, "start": 4986.06, "end": 4992.06, "text": " A is the action you take, which in this case has very little influence on the immediate state of the world.", "tokens": [50364, 316, 307, 264, 3069, 291, 747, 11, 597, 294, 341, 1389, 575, 588, 707, 6503, 322, 264, 11629, 1785, 295, 264, 1002, 13, 50664, 50664, 865, 11, 309, 2544, 281, 312, 1850, 586, 13, 50814, 50814, 1779, 13, 407, 291, 643, 281, 3890, 1125, 1176, 13, 400, 550, 472, 295, 264, 11733, 321, 7619, 13, 51264, 51264, 407, 472, 295, 264, 721, 321, 7619, 281, 3890, 1125, 1176, 390, 637, 685, 507, 13, 3996, 472, 390, 5127, 5658, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10093116760253906, "compression_ratio": 1.6230366492146597, "no_speech_prob": 4.603699198924005e-05}, {"id": 855, "seek": 498606, "start": 4992.06, "end": 4995.06, "text": " Yeah, it seems to be clear now.", "tokens": [50364, 316, 307, 264, 3069, 291, 747, 11, 597, 294, 341, 1389, 575, 588, 707, 6503, 322, 264, 11629, 1785, 295, 264, 1002, 13, 50664, 50664, 865, 11, 309, 2544, 281, 312, 1850, 586, 13, 50814, 50814, 1779, 13, 407, 291, 643, 281, 3890, 1125, 1176, 13, 400, 550, 472, 295, 264, 11733, 321, 7619, 13, 51264, 51264, 407, 472, 295, 264, 721, 321, 7619, 281, 3890, 1125, 1176, 390, 637, 685, 507, 13, 3996, 472, 390, 5127, 5658, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10093116760253906, "compression_ratio": 1.6230366492146597, "no_speech_prob": 4.603699198924005e-05}, {"id": 856, "seek": 498606, "start": 4995.06, "end": 5004.06, "text": " Right. So you need to regularize Z. And then one of the tricks we described.", "tokens": [50364, 316, 307, 264, 3069, 291, 747, 11, 597, 294, 341, 1389, 575, 588, 707, 6503, 322, 264, 11629, 1785, 295, 264, 1002, 13, 50664, 50664, 865, 11, 309, 2544, 281, 312, 1850, 586, 13, 50814, 50814, 1779, 13, 407, 291, 643, 281, 3890, 1125, 1176, 13, 400, 550, 472, 295, 264, 11733, 321, 7619, 13, 51264, 51264, 407, 472, 295, 264, 721, 321, 7619, 281, 3890, 1125, 1176, 390, 637, 685, 507, 13, 3996, 472, 390, 5127, 5658, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10093116760253906, "compression_ratio": 1.6230366492146597, "no_speech_prob": 4.603699198924005e-05}, {"id": 857, "seek": 498606, "start": 5004.06, "end": 5015.06, "text": " So one of the things we described to regularize Z was sparsity. Another one was adding noise.", "tokens": [50364, 316, 307, 264, 3069, 291, 747, 11, 597, 294, 341, 1389, 575, 588, 707, 6503, 322, 264, 11629, 1785, 295, 264, 1002, 13, 50664, 50664, 865, 11, 309, 2544, 281, 312, 1850, 586, 13, 50814, 50814, 1779, 13, 407, 291, 643, 281, 3890, 1125, 1176, 13, 400, 550, 472, 295, 264, 11733, 321, 7619, 13, 51264, 51264, 407, 472, 295, 264, 721, 321, 7619, 281, 3890, 1125, 1176, 390, 637, 685, 507, 13, 3996, 472, 390, 5127, 5658, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10093116760253906, "compression_ratio": 1.6230366492146597, "no_speech_prob": 4.603699198924005e-05}, {"id": 858, "seek": 501506, "start": 5015.06, "end": 5018.06, "text": " But the other trick we described is this idea of having an encoder.", "tokens": [50364, 583, 264, 661, 4282, 321, 7619, 307, 341, 1558, 295, 1419, 364, 2058, 19866, 13, 50514, 50514, 407, 291, 362, 1783, 420, 4904, 1190, 807, 264, 6069, 284, 13, 50864, 50864, 440, 6069, 284, 1709, 666, 264, 979, 19866, 11, 597, 1669, 257, 17630, 466, 398, 13, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.12308416179582185, "compression_ratio": 1.3909774436090225, "no_speech_prob": 6.338160346786026e-06}, {"id": 859, "seek": 501506, "start": 5018.06, "end": 5025.06, "text": " So you have X or ST run through the predictor.", "tokens": [50364, 583, 264, 661, 4282, 321, 7619, 307, 341, 1558, 295, 1419, 364, 2058, 19866, 13, 50514, 50514, 407, 291, 362, 1783, 420, 4904, 1190, 807, 264, 6069, 284, 13, 50864, 50864, 440, 6069, 284, 1709, 666, 264, 979, 19866, 11, 597, 1669, 257, 17630, 466, 398, 13, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.12308416179582185, "compression_ratio": 1.3909774436090225, "no_speech_prob": 6.338160346786026e-06}, {"id": 860, "seek": 501506, "start": 5025.06, "end": 5032.06, "text": " The predictor goes into the decoder, which makes a prediction about Y.", "tokens": [50364, 583, 264, 661, 4282, 321, 7619, 307, 341, 1558, 295, 1419, 364, 2058, 19866, 13, 50514, 50514, 407, 291, 362, 1783, 420, 4904, 1190, 807, 264, 6069, 284, 13, 50864, 50864, 440, 6069, 284, 1709, 666, 264, 979, 19866, 11, 597, 1669, 257, 17630, 466, 398, 13, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.12308416179582185, "compression_ratio": 1.3909774436090225, "no_speech_prob": 6.338160346786026e-06}, {"id": 861, "seek": 503206, "start": 5032.06, "end": 5047.06, "text": " Let's call it Y bar. And you compare Y bar to Y.", "tokens": [50364, 961, 311, 818, 309, 398, 2159, 13, 400, 291, 6794, 398, 2159, 281, 398, 13, 51114, 51114, 400, 510, 291, 362, 1176, 13, 400, 437, 321, 2825, 466, 307, 264, 1558, 295, 1228, 364, 2058, 19866, 510, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11394985710702292, "compression_ratio": 1.2476190476190476, "no_speech_prob": 2.8127046789450105e-06}, {"id": 862, "seek": 503206, "start": 5047.06, "end": 5055.06, "text": " And here you have Z. And what we talked about is the idea of using an encoder here", "tokens": [50364, 961, 311, 818, 309, 398, 2159, 13, 400, 291, 6794, 398, 2159, 281, 398, 13, 51114, 51114, 400, 510, 291, 362, 1176, 13, 400, 437, 321, 2825, 466, 307, 264, 1558, 295, 1228, 364, 2058, 19866, 510, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11394985710702292, "compression_ratio": 1.2476190476190476, "no_speech_prob": 2.8127046789450105e-06}, {"id": 863, "seek": 505506, "start": 5055.06, "end": 5062.06, "text": " to predict the optimal value of Z and then basically having a cost function that", "tokens": [50364, 281, 6069, 264, 16252, 2158, 295, 1176, 293, 550, 1936, 1419, 257, 2063, 2445, 300, 50714, 50714, 307, 257, 1433, 294, 264, 2281, 300, 8000, 264, 2983, 265, 6040, 1344, 1296, 264, 2158, 295, 1176, 291, 767, 764, 293, 264, 2158, 295, 1176, 19147, 538, 264, 2058, 19866, 13, 51064, 51064, 400, 4317, 341, 307, 3890, 1602, 294, 512, 636, 13, 51464, 51464, 400, 264, 6069, 284, 611, 575, 281, 6503, 264, 2058, 19866, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.049726462062401104, "compression_ratio": 1.7043010752688172, "no_speech_prob": 1.1475176506792195e-05}, {"id": 864, "seek": 505506, "start": 5062.06, "end": 5069.06, "text": " is a term in the energy that measures the discrepancy between the value of Z you actually use and the value of Z predicted by the encoder.", "tokens": [50364, 281, 6069, 264, 16252, 2158, 295, 1176, 293, 550, 1936, 1419, 257, 2063, 2445, 300, 50714, 50714, 307, 257, 1433, 294, 264, 2281, 300, 8000, 264, 2983, 265, 6040, 1344, 1296, 264, 2158, 295, 1176, 291, 767, 764, 293, 264, 2158, 295, 1176, 19147, 538, 264, 2058, 19866, 13, 51064, 51064, 400, 4317, 341, 307, 3890, 1602, 294, 512, 636, 13, 51464, 51464, 400, 264, 6069, 284, 611, 575, 281, 6503, 264, 2058, 19866, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.049726462062401104, "compression_ratio": 1.7043010752688172, "no_speech_prob": 1.1475176506792195e-05}, {"id": 865, "seek": 505506, "start": 5069.06, "end": 5077.06, "text": " And perhaps this is regularized in some way.", "tokens": [50364, 281, 6069, 264, 16252, 2158, 295, 1176, 293, 550, 1936, 1419, 257, 2063, 2445, 300, 50714, 50714, 307, 257, 1433, 294, 264, 2281, 300, 8000, 264, 2983, 265, 6040, 1344, 1296, 264, 2158, 295, 1176, 291, 767, 764, 293, 264, 2158, 295, 1176, 19147, 538, 264, 2058, 19866, 13, 51064, 51064, 400, 4317, 341, 307, 3890, 1602, 294, 512, 636, 13, 51464, 51464, 400, 264, 6069, 284, 611, 575, 281, 6503, 264, 2058, 19866, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.049726462062401104, "compression_ratio": 1.7043010752688172, "no_speech_prob": 1.1475176506792195e-05}, {"id": 866, "seek": 505506, "start": 5077.06, "end": 5082.06, "text": " And the predictor also has to influence the encoder.", "tokens": [50364, 281, 6069, 264, 16252, 2158, 295, 1176, 293, 550, 1936, 1419, 257, 2063, 2445, 300, 50714, 50714, 307, 257, 1433, 294, 264, 2281, 300, 8000, 264, 2983, 265, 6040, 1344, 1296, 264, 2158, 295, 1176, 291, 767, 764, 293, 264, 2158, 295, 1176, 19147, 538, 264, 2058, 19866, 13, 51064, 51064, 400, 4317, 341, 307, 3890, 1602, 294, 512, 636, 13, 51464, 51464, 400, 264, 6069, 284, 611, 575, 281, 6503, 264, 2058, 19866, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.049726462062401104, "compression_ratio": 1.7043010752688172, "no_speech_prob": 1.1475176506792195e-05}, {"id": 867, "seek": 508206, "start": 5082.06, "end": 5089.06, "text": " So it's pretty clear that you need an information bottleneck between the encoder and the decoder.", "tokens": [50364, 407, 309, 311, 1238, 1850, 300, 291, 643, 364, 1589, 44641, 547, 1296, 264, 2058, 19866, 293, 264, 979, 19866, 13, 50714, 50714, 10328, 11, 264, 1185, 486, 17470, 13, 467, 486, 2584, 11200, 1783, 13, 50864, 50864, 467, 486, 312, 1075, 281, 6069, 398, 2293, 538, 445, 18309, 11, 538, 1237, 412, 264, 2158, 295, 398, 11, 2614, 309, 807, 264, 2058, 19866, 293, 550, 2614, 309, 807, 264, 979, 19866, 293, 550, 32884, 398, 13, 51264, 51264, 1779, 13, 663, 311, 445, 257, 588, 2199, 8399, 22660, 19866, 13, 51364, 51364, 407, 5969, 291, 7694, 264, 6042, 295, 1176, 11, 264, 1185, 486, 445, 17470, 293, 406, 767, 3847, 2564, 281, 6069, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10149615952948562, "compression_ratio": 1.837037037037037, "no_speech_prob": 2.6236037228954956e-05}, {"id": 868, "seek": 508206, "start": 5089.06, "end": 5092.06, "text": " Otherwise, the system will cheat. It will completely ignore X.", "tokens": [50364, 407, 309, 311, 1238, 1850, 300, 291, 643, 364, 1589, 44641, 547, 1296, 264, 2058, 19866, 293, 264, 979, 19866, 13, 50714, 50714, 10328, 11, 264, 1185, 486, 17470, 13, 467, 486, 2584, 11200, 1783, 13, 50864, 50864, 467, 486, 312, 1075, 281, 6069, 398, 2293, 538, 445, 18309, 11, 538, 1237, 412, 264, 2158, 295, 398, 11, 2614, 309, 807, 264, 2058, 19866, 293, 550, 2614, 309, 807, 264, 979, 19866, 293, 550, 32884, 398, 13, 51264, 51264, 1779, 13, 663, 311, 445, 257, 588, 2199, 8399, 22660, 19866, 13, 51364, 51364, 407, 5969, 291, 7694, 264, 6042, 295, 1176, 11, 264, 1185, 486, 445, 17470, 293, 406, 767, 3847, 2564, 281, 6069, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10149615952948562, "compression_ratio": 1.837037037037037, "no_speech_prob": 2.6236037228954956e-05}, {"id": 869, "seek": 508206, "start": 5092.06, "end": 5100.06, "text": " It will be able to predict Y exactly by just cheating, by looking at the value of Y, running it through the encoder and then running it through the decoder and then predicting Y.", "tokens": [50364, 407, 309, 311, 1238, 1850, 300, 291, 643, 364, 1589, 44641, 547, 1296, 264, 2058, 19866, 293, 264, 979, 19866, 13, 50714, 50714, 10328, 11, 264, 1185, 486, 17470, 13, 467, 486, 2584, 11200, 1783, 13, 50864, 50864, 467, 486, 312, 1075, 281, 6069, 398, 2293, 538, 445, 18309, 11, 538, 1237, 412, 264, 2158, 295, 398, 11, 2614, 309, 807, 264, 2058, 19866, 293, 550, 2614, 309, 807, 264, 979, 19866, 293, 550, 32884, 398, 13, 51264, 51264, 1779, 13, 663, 311, 445, 257, 588, 2199, 8399, 22660, 19866, 13, 51364, 51364, 407, 5969, 291, 7694, 264, 6042, 295, 1176, 11, 264, 1185, 486, 445, 17470, 293, 406, 767, 3847, 2564, 281, 6069, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10149615952948562, "compression_ratio": 1.837037037037037, "no_speech_prob": 2.6236037228954956e-05}, {"id": 870, "seek": 508206, "start": 5100.06, "end": 5102.06, "text": " Right. That's just a very simple autoencoder.", "tokens": [50364, 407, 309, 311, 1238, 1850, 300, 291, 643, 364, 1589, 44641, 547, 1296, 264, 2058, 19866, 293, 264, 979, 19866, 13, 50714, 50714, 10328, 11, 264, 1185, 486, 17470, 13, 467, 486, 2584, 11200, 1783, 13, 50864, 50864, 467, 486, 312, 1075, 281, 6069, 398, 2293, 538, 445, 18309, 11, 538, 1237, 412, 264, 2158, 295, 398, 11, 2614, 309, 807, 264, 2058, 19866, 293, 550, 2614, 309, 807, 264, 979, 19866, 293, 550, 32884, 398, 13, 51264, 51264, 1779, 13, 663, 311, 445, 257, 588, 2199, 8399, 22660, 19866, 13, 51364, 51364, 407, 5969, 291, 7694, 264, 6042, 295, 1176, 11, 264, 1185, 486, 445, 17470, 293, 406, 767, 3847, 2564, 281, 6069, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10149615952948562, "compression_ratio": 1.837037037037037, "no_speech_prob": 2.6236037228954956e-05}, {"id": 871, "seek": 508206, "start": 5102.06, "end": 5109.06, "text": " So unless you restrict the capacity of Z, the system will just cheat and not actually train itself to predict.", "tokens": [50364, 407, 309, 311, 1238, 1850, 300, 291, 643, 364, 1589, 44641, 547, 1296, 264, 2058, 19866, 293, 264, 979, 19866, 13, 50714, 50714, 10328, 11, 264, 1185, 486, 17470, 13, 467, 486, 2584, 11200, 1783, 13, 50864, 50864, 467, 486, 312, 1075, 281, 6069, 398, 2293, 538, 445, 18309, 11, 538, 1237, 412, 264, 2158, 295, 398, 11, 2614, 309, 807, 264, 2058, 19866, 293, 550, 2614, 309, 807, 264, 979, 19866, 293, 550, 32884, 398, 13, 51264, 51264, 1779, 13, 663, 311, 445, 257, 588, 2199, 8399, 22660, 19866, 13, 51364, 51364, 407, 5969, 291, 7694, 264, 6042, 295, 1176, 11, 264, 1185, 486, 445, 17470, 293, 406, 767, 3847, 2564, 281, 6069, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10149615952948562, "compression_ratio": 1.837037037037037, "no_speech_prob": 2.6236037228954956e-05}, {"id": 872, "seek": 510906, "start": 5109.06, "end": 5121.06, "text": " You have to push down on the information content of Z so as to force the system to use the information from X.", "tokens": [50364, 509, 362, 281, 2944, 760, 322, 264, 1589, 2701, 295, 1176, 370, 382, 281, 3464, 264, 1185, 281, 764, 264, 1589, 490, 1783, 13, 50964, 50964, 2264, 11, 281, 652, 264, 1151, 17630, 13, 51264, 51264, 2264, 11, 586, 321, 393, 764, 300, 4282, 281, 3847, 527, 2128, 2316, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14411885650069625, "compression_ratio": 1.4779411764705883, "no_speech_prob": 2.9295048079802655e-05}, {"id": 873, "seek": 510906, "start": 5121.06, "end": 5127.06, "text": " OK, to make the best prediction.", "tokens": [50364, 509, 362, 281, 2944, 760, 322, 264, 1589, 2701, 295, 1176, 370, 382, 281, 3464, 264, 1185, 281, 764, 264, 1589, 490, 1783, 13, 50964, 50964, 2264, 11, 281, 652, 264, 1151, 17630, 13, 51264, 51264, 2264, 11, 586, 321, 393, 764, 300, 4282, 281, 3847, 527, 2128, 2316, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14411885650069625, "compression_ratio": 1.4779411764705883, "no_speech_prob": 2.9295048079802655e-05}, {"id": 874, "seek": 510906, "start": 5127.06, "end": 5135.06, "text": " OK, now we can use that trick to train our forward model.", "tokens": [50364, 509, 362, 281, 2944, 760, 322, 264, 1589, 2701, 295, 1176, 370, 382, 281, 3464, 264, 1185, 281, 764, 264, 1589, 490, 1783, 13, 50964, 50964, 2264, 11, 281, 652, 264, 1151, 17630, 13, 51264, 51264, 2264, 11, 586, 321, 393, 764, 300, 4282, 281, 3847, 527, 2128, 2316, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14411885650069625, "compression_ratio": 1.4779411764705883, "no_speech_prob": 2.9295048079802655e-05}, {"id": 875, "seek": 513506, "start": 5135.06, "end": 5140.06, "text": " Because again, the forward model is basically just an instance of this.", "tokens": [50364, 1436, 797, 11, 264, 2128, 2316, 307, 1936, 445, 364, 5197, 295, 341, 13, 50614, 50614, 400, 341, 307, 264, 1716, 337, 23797, 4840, 300, 257, 5819, 3107, 16380, 48909, 682, 706, 2732, 322, 13, 51114, 51114, 400, 28327, 78, 575, 2732, 322, 341, 293, 307, 920, 1364, 322, 341, 1716, 13, 51364, 51364, 400, 370, 510, 291, 434, 1382, 281, 3847, 257, 1032, 281, 3332, 2564, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19952735635969374, "compression_ratio": 1.5561497326203209, "no_speech_prob": 8.798720955383033e-06}, {"id": 876, "seek": 513506, "start": 5140.06, "end": 5150.06, "text": " And this is the project for autonomous driving that a former student Mikhail Inav worked on.", "tokens": [50364, 1436, 797, 11, 264, 2128, 2316, 307, 1936, 445, 364, 5197, 295, 341, 13, 50614, 50614, 400, 341, 307, 264, 1716, 337, 23797, 4840, 300, 257, 5819, 3107, 16380, 48909, 682, 706, 2732, 322, 13, 51114, 51114, 400, 28327, 78, 575, 2732, 322, 341, 293, 307, 920, 1364, 322, 341, 1716, 13, 51364, 51364, 400, 370, 510, 291, 434, 1382, 281, 3847, 257, 1032, 281, 3332, 2564, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19952735635969374, "compression_ratio": 1.5561497326203209, "no_speech_prob": 8.798720955383033e-06}, {"id": 877, "seek": 513506, "start": 5150.06, "end": 5155.06, "text": " And Alfredo has worked on this and is still working on this project.", "tokens": [50364, 1436, 797, 11, 264, 2128, 2316, 307, 1936, 445, 364, 5197, 295, 341, 13, 50614, 50614, 400, 341, 307, 264, 1716, 337, 23797, 4840, 300, 257, 5819, 3107, 16380, 48909, 682, 706, 2732, 322, 13, 51114, 51114, 400, 28327, 78, 575, 2732, 322, 341, 293, 307, 920, 1364, 322, 341, 1716, 13, 51364, 51364, 400, 370, 510, 291, 434, 1382, 281, 3847, 257, 1032, 281, 3332, 2564, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19952735635969374, "compression_ratio": 1.5561497326203209, "no_speech_prob": 8.798720955383033e-06}, {"id": 878, "seek": 513506, "start": 5155.06, "end": 5162.06, "text": " And so here you're trying to train a car to drive itself.", "tokens": [50364, 1436, 797, 11, 264, 2128, 2316, 307, 1936, 445, 364, 5197, 295, 341, 13, 50614, 50614, 400, 341, 307, 264, 1716, 337, 23797, 4840, 300, 257, 5819, 3107, 16380, 48909, 682, 706, 2732, 322, 13, 51114, 51114, 400, 28327, 78, 575, 2732, 322, 341, 293, 307, 920, 1364, 322, 341, 1716, 13, 51364, 51364, 400, 370, 510, 291, 434, 1382, 281, 3847, 257, 1032, 281, 3332, 2564, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19952735635969374, "compression_ratio": 1.5561497326203209, "no_speech_prob": 8.798720955383033e-06}, {"id": 879, "seek": 516206, "start": 5162.06, "end": 5168.06, "text": " And what's difficult to predict is what the car around you are going to do.", "tokens": [50364, 400, 437, 311, 2252, 281, 6069, 307, 437, 264, 1032, 926, 291, 366, 516, 281, 360, 13, 50664, 50664, 407, 291, 1081, 257, 2799, 3673, 264, 17205, 293, 291, 1159, 264, 5163, 733, 295, 352, 538, 13, 51014, 51014, 400, 291, 393, 2837, 633, 1032, 293, 550, 8947, 264, 11629, 7630, 295, 264, 1032, 13, 51264, 51264, 8537, 11, 257, 707, 21930, 926, 633, 1032, 300, 16203, 689, 264, 661, 5163, 366, 4972, 281, 428, 1032, 13, 51614, 51614, 400, 341, 307, 437, 390, 10379, 412, 264, 2767, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06768321990966797, "compression_ratio": 1.7056277056277056, "no_speech_prob": 7.138275395845994e-05}, {"id": 880, "seek": 516206, "start": 5168.06, "end": 5175.06, "text": " So you place a camera above the highway and you watch the cars kind of go by.", "tokens": [50364, 400, 437, 311, 2252, 281, 6069, 307, 437, 264, 1032, 926, 291, 366, 516, 281, 360, 13, 50664, 50664, 407, 291, 1081, 257, 2799, 3673, 264, 17205, 293, 291, 1159, 264, 5163, 733, 295, 352, 538, 13, 51014, 51014, 400, 291, 393, 2837, 633, 1032, 293, 550, 8947, 264, 11629, 7630, 295, 264, 1032, 13, 51264, 51264, 8537, 11, 257, 707, 21930, 926, 633, 1032, 300, 16203, 689, 264, 661, 5163, 366, 4972, 281, 428, 1032, 13, 51614, 51614, 400, 341, 307, 437, 390, 10379, 412, 264, 2767, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06768321990966797, "compression_ratio": 1.7056277056277056, "no_speech_prob": 7.138275395845994e-05}, {"id": 881, "seek": 516206, "start": 5175.06, "end": 5180.06, "text": " And you can track every car and then extract the immediate neighborhood of the car.", "tokens": [50364, 400, 437, 311, 2252, 281, 6069, 307, 437, 264, 1032, 926, 291, 366, 516, 281, 360, 13, 50664, 50664, 407, 291, 1081, 257, 2799, 3673, 264, 17205, 293, 291, 1159, 264, 5163, 733, 295, 352, 538, 13, 51014, 51014, 400, 291, 393, 2837, 633, 1032, 293, 550, 8947, 264, 11629, 7630, 295, 264, 1032, 13, 51264, 51264, 8537, 11, 257, 707, 21930, 926, 633, 1032, 300, 16203, 689, 264, 661, 5163, 366, 4972, 281, 428, 1032, 13, 51614, 51614, 400, 341, 307, 437, 390, 10379, 412, 264, 2767, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06768321990966797, "compression_ratio": 1.7056277056277056, "no_speech_prob": 7.138275395845994e-05}, {"id": 882, "seek": 516206, "start": 5180.06, "end": 5187.06, "text": " Basically, a little rectangle around every car that indicates where the other cars are relative to your car.", "tokens": [50364, 400, 437, 311, 2252, 281, 6069, 307, 437, 264, 1032, 926, 291, 366, 516, 281, 360, 13, 50664, 50664, 407, 291, 1081, 257, 2799, 3673, 264, 17205, 293, 291, 1159, 264, 5163, 733, 295, 352, 538, 13, 51014, 51014, 400, 291, 393, 2837, 633, 1032, 293, 550, 8947, 264, 11629, 7630, 295, 264, 1032, 13, 51264, 51264, 8537, 11, 257, 707, 21930, 926, 633, 1032, 300, 16203, 689, 264, 661, 5163, 366, 4972, 281, 428, 1032, 13, 51614, 51614, 400, 341, 307, 437, 390, 10379, 412, 264, 2767, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06768321990966797, "compression_ratio": 1.7056277056277056, "no_speech_prob": 7.138275395845994e-05}, {"id": 883, "seek": 516206, "start": 5187.06, "end": 5190.06, "text": " And this is what was represented at the bottom.", "tokens": [50364, 400, 437, 311, 2252, 281, 6069, 307, 437, 264, 1032, 926, 291, 366, 516, 281, 360, 13, 50664, 50664, 407, 291, 1081, 257, 2799, 3673, 264, 17205, 293, 291, 1159, 264, 5163, 733, 295, 352, 538, 13, 51014, 51014, 400, 291, 393, 2837, 633, 1032, 293, 550, 8947, 264, 11629, 7630, 295, 264, 1032, 13, 51264, 51264, 8537, 11, 257, 707, 21930, 926, 633, 1032, 300, 16203, 689, 264, 661, 5163, 366, 4972, 281, 428, 1032, 13, 51614, 51614, 400, 341, 307, 437, 390, 10379, 412, 264, 2767, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06768321990966797, "compression_ratio": 1.7056277056277056, "no_speech_prob": 7.138275395845994e-05}, {"id": 884, "seek": 519006, "start": 5190.06, "end": 5195.06, "text": " So at the bottom, you have a little rectangle that's centered around a given car.", "tokens": [50364, 407, 412, 264, 2767, 11, 291, 362, 257, 707, 21930, 300, 311, 18988, 926, 257, 2212, 1032, 13, 50614, 50614, 400, 550, 439, 264, 5163, 926, 366, 264, 707, 21930, 18988, 322, 300, 1032, 689, 264, 1032, 307, 294, 257, 31677, 4914, 294, 264, 2808, 295, 300, 21930, 13, 51164, 51164, 509, 360, 341, 337, 633, 1032, 13, 51264, 51264, 708, 309, 2709, 291, 307, 337, 633, 1032, 257, 8310, 295, 437, 264, 5163, 926, 309, 366, 516, 281, 360, 13, 51514, 51514, 400, 321, 393, 764, 341, 281, 3847, 257, 2128, 2316, 300, 486, 6069, 437, 264, 5163, 926, 505, 366, 516, 281, 360, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08732913624156605, "compression_ratio": 1.956896551724138, "no_speech_prob": 6.707174907205626e-05}, {"id": 885, "seek": 519006, "start": 5195.06, "end": 5206.06, "text": " And then all the cars around are the little rectangle centered on that car where the car is in a standardized location in the middle of that rectangle.", "tokens": [50364, 407, 412, 264, 2767, 11, 291, 362, 257, 707, 21930, 300, 311, 18988, 926, 257, 2212, 1032, 13, 50614, 50614, 400, 550, 439, 264, 5163, 926, 366, 264, 707, 21930, 18988, 322, 300, 1032, 689, 264, 1032, 307, 294, 257, 31677, 4914, 294, 264, 2808, 295, 300, 21930, 13, 51164, 51164, 509, 360, 341, 337, 633, 1032, 13, 51264, 51264, 708, 309, 2709, 291, 307, 337, 633, 1032, 257, 8310, 295, 437, 264, 5163, 926, 309, 366, 516, 281, 360, 13, 51514, 51514, 400, 321, 393, 764, 341, 281, 3847, 257, 2128, 2316, 300, 486, 6069, 437, 264, 5163, 926, 505, 366, 516, 281, 360, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08732913624156605, "compression_ratio": 1.956896551724138, "no_speech_prob": 6.707174907205626e-05}, {"id": 886, "seek": 519006, "start": 5206.06, "end": 5208.06, "text": " You do this for every car.", "tokens": [50364, 407, 412, 264, 2767, 11, 291, 362, 257, 707, 21930, 300, 311, 18988, 926, 257, 2212, 1032, 13, 50614, 50614, 400, 550, 439, 264, 5163, 926, 366, 264, 707, 21930, 18988, 322, 300, 1032, 689, 264, 1032, 307, 294, 257, 31677, 4914, 294, 264, 2808, 295, 300, 21930, 13, 51164, 51164, 509, 360, 341, 337, 633, 1032, 13, 51264, 51264, 708, 309, 2709, 291, 307, 337, 633, 1032, 257, 8310, 295, 437, 264, 5163, 926, 309, 366, 516, 281, 360, 13, 51514, 51514, 400, 321, 393, 764, 341, 281, 3847, 257, 2128, 2316, 300, 486, 6069, 437, 264, 5163, 926, 505, 366, 516, 281, 360, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08732913624156605, "compression_ratio": 1.956896551724138, "no_speech_prob": 6.707174907205626e-05}, {"id": 887, "seek": 519006, "start": 5208.06, "end": 5213.06, "text": " What it gives you is for every car a sequence of what the cars around it are going to do.", "tokens": [50364, 407, 412, 264, 2767, 11, 291, 362, 257, 707, 21930, 300, 311, 18988, 926, 257, 2212, 1032, 13, 50614, 50614, 400, 550, 439, 264, 5163, 926, 366, 264, 707, 21930, 18988, 322, 300, 1032, 689, 264, 1032, 307, 294, 257, 31677, 4914, 294, 264, 2808, 295, 300, 21930, 13, 51164, 51164, 509, 360, 341, 337, 633, 1032, 13, 51264, 51264, 708, 309, 2709, 291, 307, 337, 633, 1032, 257, 8310, 295, 437, 264, 5163, 926, 309, 366, 516, 281, 360, 13, 51514, 51514, 400, 321, 393, 764, 341, 281, 3847, 257, 2128, 2316, 300, 486, 6069, 437, 264, 5163, 926, 505, 366, 516, 281, 360, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08732913624156605, "compression_ratio": 1.956896551724138, "no_speech_prob": 6.707174907205626e-05}, {"id": 888, "seek": 519006, "start": 5213.06, "end": 5219.06, "text": " And we can use this to train a forward model that will predict what the cars around us are going to do.", "tokens": [50364, 407, 412, 264, 2767, 11, 291, 362, 257, 707, 21930, 300, 311, 18988, 926, 257, 2212, 1032, 13, 50614, 50614, 400, 550, 439, 264, 5163, 926, 366, 264, 707, 21930, 18988, 322, 300, 1032, 689, 264, 1032, 307, 294, 257, 31677, 4914, 294, 264, 2808, 295, 300, 21930, 13, 51164, 51164, 509, 360, 341, 337, 633, 1032, 13, 51264, 51264, 708, 309, 2709, 291, 307, 337, 633, 1032, 257, 8310, 295, 437, 264, 5163, 926, 309, 366, 516, 281, 360, 13, 51514, 51514, 400, 321, 393, 764, 341, 281, 3847, 257, 2128, 2316, 300, 486, 6069, 437, 264, 5163, 926, 505, 366, 516, 281, 360, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08732913624156605, "compression_ratio": 1.956896551724138, "no_speech_prob": 6.707174907205626e-05}, {"id": 889, "seek": 521906, "start": 5219.06, "end": 5226.06, "text": " The question is if these forward models predict in all possible futures irrespective of the action taken.", "tokens": [50364, 440, 1168, 307, 498, 613, 2128, 5245, 6069, 294, 439, 1944, 26071, 3418, 19575, 488, 295, 264, 3069, 2726, 13, 50714, 50714, 865, 13, 50814, 50814, 2305, 321, 6069, 257, 992, 295, 26071, 13, 50964, 50964, 407, 2212, 472, 3069, 293, 2212, 472, 5883, 1785, 11, 472, 3069, 11, 293, 472, 1729, 2158, 295, 264, 48994, 7006, 11, 309, 486, 652, 257, 2167, 17630, 13, 51514, 51514, 400, 550, 291, 393, 10559, 264, 48994, 7006, 293, 309, 486, 652, 3866, 21264, 13, 51664, 51664, 509, 393, 1319, 264, 3069, 11, 295, 1164, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16255250911122746, "compression_ratio": 1.8506787330316743, "no_speech_prob": 1.8270280634169467e-05}, {"id": 890, "seek": 521906, "start": 5226.06, "end": 5228.06, "text": " Yeah.", "tokens": [50364, 440, 1168, 307, 498, 613, 2128, 5245, 6069, 294, 439, 1944, 26071, 3418, 19575, 488, 295, 264, 3069, 2726, 13, 50714, 50714, 865, 13, 50814, 50814, 2305, 321, 6069, 257, 992, 295, 26071, 13, 50964, 50964, 407, 2212, 472, 3069, 293, 2212, 472, 5883, 1785, 11, 472, 3069, 11, 293, 472, 1729, 2158, 295, 264, 48994, 7006, 11, 309, 486, 652, 257, 2167, 17630, 13, 51514, 51514, 400, 550, 291, 393, 10559, 264, 48994, 7006, 293, 309, 486, 652, 3866, 21264, 13, 51664, 51664, 509, 393, 1319, 264, 3069, 11, 295, 1164, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16255250911122746, "compression_ratio": 1.8506787330316743, "no_speech_prob": 1.8270280634169467e-05}, {"id": 891, "seek": 521906, "start": 5228.06, "end": 5231.06, "text": " Where we predict a set of futures.", "tokens": [50364, 440, 1168, 307, 498, 613, 2128, 5245, 6069, 294, 439, 1944, 26071, 3418, 19575, 488, 295, 264, 3069, 2726, 13, 50714, 50714, 865, 13, 50814, 50814, 2305, 321, 6069, 257, 992, 295, 26071, 13, 50964, 50964, 407, 2212, 472, 3069, 293, 2212, 472, 5883, 1785, 11, 472, 3069, 11, 293, 472, 1729, 2158, 295, 264, 48994, 7006, 11, 309, 486, 652, 257, 2167, 17630, 13, 51514, 51514, 400, 550, 291, 393, 10559, 264, 48994, 7006, 293, 309, 486, 652, 3866, 21264, 13, 51664, 51664, 509, 393, 1319, 264, 3069, 11, 295, 1164, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16255250911122746, "compression_ratio": 1.8506787330316743, "no_speech_prob": 1.8270280634169467e-05}, {"id": 892, "seek": 521906, "start": 5231.06, "end": 5242.06, "text": " So given one action and given one initial state, one action, and one particular value of the latent variable, it will make a single prediction.", "tokens": [50364, 440, 1168, 307, 498, 613, 2128, 5245, 6069, 294, 439, 1944, 26071, 3418, 19575, 488, 295, 264, 3069, 2726, 13, 50714, 50714, 865, 13, 50814, 50814, 2305, 321, 6069, 257, 992, 295, 26071, 13, 50964, 50964, 407, 2212, 472, 3069, 293, 2212, 472, 5883, 1785, 11, 472, 3069, 11, 293, 472, 1729, 2158, 295, 264, 48994, 7006, 11, 309, 486, 652, 257, 2167, 17630, 13, 51514, 51514, 400, 550, 291, 393, 10559, 264, 48994, 7006, 293, 309, 486, 652, 3866, 21264, 13, 51664, 51664, 509, 393, 1319, 264, 3069, 11, 295, 1164, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16255250911122746, "compression_ratio": 1.8506787330316743, "no_speech_prob": 1.8270280634169467e-05}, {"id": 893, "seek": 521906, "start": 5242.06, "end": 5245.06, "text": " And then you can vary the latent variable and it will make multiple predictions.", "tokens": [50364, 440, 1168, 307, 498, 613, 2128, 5245, 6069, 294, 439, 1944, 26071, 3418, 19575, 488, 295, 264, 3069, 2726, 13, 50714, 50714, 865, 13, 50814, 50814, 2305, 321, 6069, 257, 992, 295, 26071, 13, 50964, 50964, 407, 2212, 472, 3069, 293, 2212, 472, 5883, 1785, 11, 472, 3069, 11, 293, 472, 1729, 2158, 295, 264, 48994, 7006, 11, 309, 486, 652, 257, 2167, 17630, 13, 51514, 51514, 400, 550, 291, 393, 10559, 264, 48994, 7006, 293, 309, 486, 652, 3866, 21264, 13, 51664, 51664, 509, 393, 1319, 264, 3069, 11, 295, 1164, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16255250911122746, "compression_ratio": 1.8506787330316743, "no_speech_prob": 1.8270280634169467e-05}, {"id": 894, "seek": 521906, "start": 5245.06, "end": 5247.06, "text": " You can change the action, of course.", "tokens": [50364, 440, 1168, 307, 498, 613, 2128, 5245, 6069, 294, 439, 1944, 26071, 3418, 19575, 488, 295, 264, 3069, 2726, 13, 50714, 50714, 865, 13, 50814, 50814, 2305, 321, 6069, 257, 992, 295, 26071, 13, 50964, 50964, 407, 2212, 472, 3069, 293, 2212, 472, 5883, 1785, 11, 472, 3069, 11, 293, 472, 1729, 2158, 295, 264, 48994, 7006, 11, 309, 486, 652, 257, 2167, 17630, 13, 51514, 51514, 400, 550, 291, 393, 10559, 264, 48994, 7006, 293, 309, 486, 652, 3866, 21264, 13, 51664, 51664, 509, 393, 1319, 264, 3069, 11, 295, 1164, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16255250911122746, "compression_ratio": 1.8506787330316743, "no_speech_prob": 1.8270280634169467e-05}, {"id": 895, "seek": 524706, "start": 5247.06, "end": 5252.06, "text": " So I've redrawn the little diagram I drew previously here.", "tokens": [50364, 407, 286, 600, 2182, 29603, 264, 707, 10686, 286, 12804, 8046, 510, 13, 50614, 50614, 1692, 264, 1785, 1936, 307, 257, 8310, 295, 1045, 12083, 490, 341, 960, 13, 50914, 50914, 821, 311, 572, 12649, 1785, 510, 13, 50964, 50964, 467, 311, 445, 264, 3036, 2564, 13, 51114, 51114, 440, 3344, 1032, 307, 527, 1032, 293, 264, 3092, 5163, 366, 264, 661, 5163, 13, 51314, 51314, 407, 291, 747, 733, 295, 1045, 12083, 490, 264, 1791, 11, 1190, 341, 807, 341, 18161, 2533, 11, 597, 15257, 281, 6069, 264, 958, 3920, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10315093398094177, "compression_ratio": 1.676991150442478, "no_speech_prob": 1.5205323506961577e-05}, {"id": 896, "seek": 524706, "start": 5252.06, "end": 5258.06, "text": " Here the state basically is a sequence of three frames from this video.", "tokens": [50364, 407, 286, 600, 2182, 29603, 264, 707, 10686, 286, 12804, 8046, 510, 13, 50614, 50614, 1692, 264, 1785, 1936, 307, 257, 8310, 295, 1045, 12083, 490, 341, 960, 13, 50914, 50914, 821, 311, 572, 12649, 1785, 510, 13, 50964, 50964, 467, 311, 445, 264, 3036, 2564, 13, 51114, 51114, 440, 3344, 1032, 307, 527, 1032, 293, 264, 3092, 5163, 366, 264, 661, 5163, 13, 51314, 51314, 407, 291, 747, 733, 295, 1045, 12083, 490, 264, 1791, 11, 1190, 341, 807, 341, 18161, 2533, 11, 597, 15257, 281, 6069, 264, 958, 3920, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10315093398094177, "compression_ratio": 1.676991150442478, "no_speech_prob": 1.5205323506961577e-05}, {"id": 897, "seek": 524706, "start": 5258.06, "end": 5259.06, "text": " There's no abstract state here.", "tokens": [50364, 407, 286, 600, 2182, 29603, 264, 707, 10686, 286, 12804, 8046, 510, 13, 50614, 50614, 1692, 264, 1785, 1936, 307, 257, 8310, 295, 1045, 12083, 490, 341, 960, 13, 50914, 50914, 821, 311, 572, 12649, 1785, 510, 13, 50964, 50964, 467, 311, 445, 264, 3036, 2564, 13, 51114, 51114, 440, 3344, 1032, 307, 527, 1032, 293, 264, 3092, 5163, 366, 264, 661, 5163, 13, 51314, 51314, 407, 291, 747, 733, 295, 1045, 12083, 490, 264, 1791, 11, 1190, 341, 807, 341, 18161, 2533, 11, 597, 15257, 281, 6069, 264, 958, 3920, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10315093398094177, "compression_ratio": 1.676991150442478, "no_speech_prob": 1.5205323506961577e-05}, {"id": 898, "seek": 524706, "start": 5259.06, "end": 5262.06, "text": " It's just the picture itself.", "tokens": [50364, 407, 286, 600, 2182, 29603, 264, 707, 10686, 286, 12804, 8046, 510, 13, 50614, 50614, 1692, 264, 1785, 1936, 307, 257, 8310, 295, 1045, 12083, 490, 341, 960, 13, 50914, 50914, 821, 311, 572, 12649, 1785, 510, 13, 50964, 50964, 467, 311, 445, 264, 3036, 2564, 13, 51114, 51114, 440, 3344, 1032, 307, 527, 1032, 293, 264, 3092, 5163, 366, 264, 661, 5163, 13, 51314, 51314, 407, 291, 747, 733, 295, 1045, 12083, 490, 264, 1791, 11, 1190, 341, 807, 341, 18161, 2533, 11, 597, 15257, 281, 6069, 264, 958, 3920, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10315093398094177, "compression_ratio": 1.676991150442478, "no_speech_prob": 1.5205323506961577e-05}, {"id": 899, "seek": 524706, "start": 5262.06, "end": 5266.06, "text": " The blue car is our car and the green cars are the other cars.", "tokens": [50364, 407, 286, 600, 2182, 29603, 264, 707, 10686, 286, 12804, 8046, 510, 13, 50614, 50614, 1692, 264, 1785, 1936, 307, 257, 8310, 295, 1045, 12083, 490, 341, 960, 13, 50914, 50914, 821, 311, 572, 12649, 1785, 510, 13, 50964, 50964, 467, 311, 445, 264, 3036, 2564, 13, 51114, 51114, 440, 3344, 1032, 307, 527, 1032, 293, 264, 3092, 5163, 366, 264, 661, 5163, 13, 51314, 51314, 407, 291, 747, 733, 295, 1045, 12083, 490, 264, 1791, 11, 1190, 341, 807, 341, 18161, 2533, 11, 597, 15257, 281, 6069, 264, 958, 3920, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10315093398094177, "compression_ratio": 1.676991150442478, "no_speech_prob": 1.5205323506961577e-05}, {"id": 900, "seek": 524706, "start": 5266.06, "end": 5275.06, "text": " So you take kind of three frames from the past, run this through this neural net, which attempts to predict the next frame.", "tokens": [50364, 407, 286, 600, 2182, 29603, 264, 707, 10686, 286, 12804, 8046, 510, 13, 50614, 50614, 1692, 264, 1785, 1936, 307, 257, 8310, 295, 1045, 12083, 490, 341, 960, 13, 50914, 50914, 821, 311, 572, 12649, 1785, 510, 13, 50964, 50964, 467, 311, 445, 264, 3036, 2564, 13, 51114, 51114, 440, 3344, 1032, 307, 527, 1032, 293, 264, 3092, 5163, 366, 264, 661, 5163, 13, 51314, 51314, 407, 291, 747, 733, 295, 1045, 12083, 490, 264, 1791, 11, 1190, 341, 807, 341, 18161, 2533, 11, 597, 15257, 281, 6069, 264, 958, 3920, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10315093398094177, "compression_ratio": 1.676991150442478, "no_speech_prob": 1.5205323506961577e-05}, {"id": 901, "seek": 527506, "start": 5275.06, "end": 5284.06, "text": " Okay. Using basically a big convolutional net as a predictor and a big convolutional net as a decoder.", "tokens": [50364, 1033, 13, 11142, 1936, 257, 955, 45216, 304, 2533, 382, 257, 6069, 284, 293, 257, 955, 45216, 304, 2533, 382, 257, 979, 19866, 13, 50814, 50814, 821, 311, 257, 48994, 7006, 510, 13, 50864, 50864, 821, 311, 611, 364, 3069, 510, 597, 307, 406, 10117, 300, 2170, 666, 341, 13, 51164, 51164, 400, 264, 1185, 611, 575, 364, 2058, 19866, 13, 51364, 51364, 407, 309, 1542, 544, 411, 341, 13, 51514, 51514, 400, 797, 11, 264, 3069, 510, 307, 406, 10379, 11, 457, 3811, 456, 307, 472, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1072353694749915, "compression_ratio": 1.7474226804123711, "no_speech_prob": 2.8128929443482775e-06}, {"id": 902, "seek": 527506, "start": 5284.06, "end": 5285.06, "text": " There's a latent variable here.", "tokens": [50364, 1033, 13, 11142, 1936, 257, 955, 45216, 304, 2533, 382, 257, 6069, 284, 293, 257, 955, 45216, 304, 2533, 382, 257, 979, 19866, 13, 50814, 50814, 821, 311, 257, 48994, 7006, 510, 13, 50864, 50864, 821, 311, 611, 364, 3069, 510, 597, 307, 406, 10117, 300, 2170, 666, 341, 13, 51164, 51164, 400, 264, 1185, 611, 575, 364, 2058, 19866, 13, 51364, 51364, 407, 309, 1542, 544, 411, 341, 13, 51514, 51514, 400, 797, 11, 264, 3069, 510, 307, 406, 10379, 11, 457, 3811, 456, 307, 472, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1072353694749915, "compression_ratio": 1.7474226804123711, "no_speech_prob": 2.8128929443482775e-06}, {"id": 903, "seek": 527506, "start": 5285.06, "end": 5291.06, "text": " There's also an action here which is not drawn that gets into this.", "tokens": [50364, 1033, 13, 11142, 1936, 257, 955, 45216, 304, 2533, 382, 257, 6069, 284, 293, 257, 955, 45216, 304, 2533, 382, 257, 979, 19866, 13, 50814, 50814, 821, 311, 257, 48994, 7006, 510, 13, 50864, 50864, 821, 311, 611, 364, 3069, 510, 597, 307, 406, 10117, 300, 2170, 666, 341, 13, 51164, 51164, 400, 264, 1185, 611, 575, 364, 2058, 19866, 13, 51364, 51364, 407, 309, 1542, 544, 411, 341, 13, 51514, 51514, 400, 797, 11, 264, 3069, 510, 307, 406, 10379, 11, 457, 3811, 456, 307, 472, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1072353694749915, "compression_ratio": 1.7474226804123711, "no_speech_prob": 2.8128929443482775e-06}, {"id": 904, "seek": 527506, "start": 5291.06, "end": 5295.06, "text": " And the system also has an encoder.", "tokens": [50364, 1033, 13, 11142, 1936, 257, 955, 45216, 304, 2533, 382, 257, 6069, 284, 293, 257, 955, 45216, 304, 2533, 382, 257, 979, 19866, 13, 50814, 50814, 821, 311, 257, 48994, 7006, 510, 13, 50864, 50864, 821, 311, 611, 364, 3069, 510, 597, 307, 406, 10117, 300, 2170, 666, 341, 13, 51164, 51164, 400, 264, 1185, 611, 575, 364, 2058, 19866, 13, 51364, 51364, 407, 309, 1542, 544, 411, 341, 13, 51514, 51514, 400, 797, 11, 264, 3069, 510, 307, 406, 10379, 11, 457, 3811, 456, 307, 472, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1072353694749915, "compression_ratio": 1.7474226804123711, "no_speech_prob": 2.8128929443482775e-06}, {"id": 905, "seek": 527506, "start": 5295.06, "end": 5298.06, "text": " So it looks more like this.", "tokens": [50364, 1033, 13, 11142, 1936, 257, 955, 45216, 304, 2533, 382, 257, 6069, 284, 293, 257, 955, 45216, 304, 2533, 382, 257, 979, 19866, 13, 50814, 50814, 821, 311, 257, 48994, 7006, 510, 13, 50864, 50864, 821, 311, 611, 364, 3069, 510, 597, 307, 406, 10117, 300, 2170, 666, 341, 13, 51164, 51164, 400, 264, 1185, 611, 575, 364, 2058, 19866, 13, 51364, 51364, 407, 309, 1542, 544, 411, 341, 13, 51514, 51514, 400, 797, 11, 264, 3069, 510, 307, 406, 10379, 11, 457, 3811, 456, 307, 472, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1072353694749915, "compression_ratio": 1.7474226804123711, "no_speech_prob": 2.8128929443482775e-06}, {"id": 906, "seek": 527506, "start": 5298.06, "end": 5303.06, "text": " And again, the action here is not represented, but imagine there is one.", "tokens": [50364, 1033, 13, 11142, 1936, 257, 955, 45216, 304, 2533, 382, 257, 6069, 284, 293, 257, 955, 45216, 304, 2533, 382, 257, 979, 19866, 13, 50814, 50814, 821, 311, 257, 48994, 7006, 510, 13, 50864, 50864, 821, 311, 611, 364, 3069, 510, 597, 307, 406, 10117, 300, 2170, 666, 341, 13, 51164, 51164, 400, 264, 1185, 611, 575, 364, 2058, 19866, 13, 51364, 51364, 407, 309, 1542, 544, 411, 341, 13, 51514, 51514, 400, 797, 11, 264, 3069, 510, 307, 406, 10379, 11, 457, 3811, 456, 307, 472, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1072353694749915, "compression_ratio": 1.7474226804123711, "no_speech_prob": 2.8128929443482775e-06}, {"id": 907, "seek": 530306, "start": 5303.06, "end": 5305.06, "text": " So X is the past frames.", "tokens": [50364, 407, 1783, 307, 264, 1791, 12083, 13, 50464, 50464, 467, 1709, 807, 257, 6069, 284, 300, 6069, 82, 257, 10290, 295, 264, 4846, 13, 50664, 50664, 400, 550, 300, 10290, 1709, 666, 257, 45216, 304, 2533, 300, 264, 979, 19866, 300, 6069, 82, 309, 1936, 307, 9354, 45558, 356, 365, 257, 48994, 7006, 13, 51264, 51264, 407, 309, 311, 3869, 281, 257, 48994, 7006, 949, 516, 666, 257, 979, 19866, 300, 1669, 257, 17630, 337, 264, 958, 1785, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08718431426818113, "compression_ratio": 1.7969543147208122, "no_speech_prob": 1.2677983249886893e-06}, {"id": 908, "seek": 530306, "start": 5305.06, "end": 5309.06, "text": " It goes through a predictor that predicts a representation of the input.", "tokens": [50364, 407, 1783, 307, 264, 1791, 12083, 13, 50464, 50464, 467, 1709, 807, 257, 6069, 284, 300, 6069, 82, 257, 10290, 295, 264, 4846, 13, 50664, 50664, 400, 550, 300, 10290, 1709, 666, 257, 45216, 304, 2533, 300, 264, 979, 19866, 300, 6069, 82, 309, 1936, 307, 9354, 45558, 356, 365, 257, 48994, 7006, 13, 51264, 51264, 407, 309, 311, 3869, 281, 257, 48994, 7006, 949, 516, 666, 257, 979, 19866, 300, 1669, 257, 17630, 337, 264, 958, 1785, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08718431426818113, "compression_ratio": 1.7969543147208122, "no_speech_prob": 1.2677983249886893e-06}, {"id": 909, "seek": 530306, "start": 5309.06, "end": 5321.06, "text": " And then that representation goes into a convolutional net that the decoder that predicts it basically is combined additively with a latent variable.", "tokens": [50364, 407, 1783, 307, 264, 1791, 12083, 13, 50464, 50464, 467, 1709, 807, 257, 6069, 284, 300, 6069, 82, 257, 10290, 295, 264, 4846, 13, 50664, 50664, 400, 550, 300, 10290, 1709, 666, 257, 45216, 304, 2533, 300, 264, 979, 19866, 300, 6069, 82, 309, 1936, 307, 9354, 45558, 356, 365, 257, 48994, 7006, 13, 51264, 51264, 407, 309, 311, 3869, 281, 257, 48994, 7006, 949, 516, 666, 257, 979, 19866, 300, 1669, 257, 17630, 337, 264, 958, 1785, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08718431426818113, "compression_ratio": 1.7969543147208122, "no_speech_prob": 1.2677983249886893e-06}, {"id": 910, "seek": 530306, "start": 5321.06, "end": 5329.06, "text": " So it's added to a latent variable before going into a decoder that makes a prediction for the next state.", "tokens": [50364, 407, 1783, 307, 264, 1791, 12083, 13, 50464, 50464, 467, 1709, 807, 257, 6069, 284, 300, 6069, 82, 257, 10290, 295, 264, 4846, 13, 50664, 50664, 400, 550, 300, 10290, 1709, 666, 257, 45216, 304, 2533, 300, 264, 979, 19866, 300, 6069, 82, 309, 1936, 307, 9354, 45558, 356, 365, 257, 48994, 7006, 13, 51264, 51264, 407, 309, 311, 3869, 281, 257, 48994, 7006, 949, 516, 666, 257, 979, 19866, 300, 1669, 257, 17630, 337, 264, 958, 1785, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08718431426818113, "compression_ratio": 1.7969543147208122, "no_speech_prob": 1.2677983249886893e-06}, {"id": 911, "seek": 532906, "start": 5329.06, "end": 5339.06, "text": " And the latent variable itself is a latent variable, but is being predicted by an encoder, which itself is also a convolutional net.", "tokens": [50364, 400, 264, 48994, 7006, 2564, 307, 257, 48994, 7006, 11, 457, 307, 885, 19147, 538, 364, 2058, 19866, 11, 597, 2564, 307, 611, 257, 45216, 304, 2533, 13, 50864, 50864, 467, 2516, 264, 1791, 293, 264, 2027, 293, 9898, 281, 6069, 264, 7157, 2158, 295, 264, 48994, 7006, 13, 51164, 51164, 823, 11, 295, 1164, 11, 291, 362, 281, 7694, 264, 1589, 2701, 510, 13, 51314, 51314, 400, 341, 307, 1096, 294, 341, 1729, 1716, 1228, 1333, 295, 257, 18527, 36, 12, 4092, 3109, 689, 264, 485, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07826536828344995, "compression_ratio": 1.7066666666666668, "no_speech_prob": 2.4058053895714693e-06}, {"id": 912, "seek": 532906, "start": 5339.06, "end": 5345.06, "text": " It takes the past and the future and tries to predict the ideal value of the latent variable.", "tokens": [50364, 400, 264, 48994, 7006, 2564, 307, 257, 48994, 7006, 11, 457, 307, 885, 19147, 538, 364, 2058, 19866, 11, 597, 2564, 307, 611, 257, 45216, 304, 2533, 13, 50864, 50864, 467, 2516, 264, 1791, 293, 264, 2027, 293, 9898, 281, 6069, 264, 7157, 2158, 295, 264, 48994, 7006, 13, 51164, 51164, 823, 11, 295, 1164, 11, 291, 362, 281, 7694, 264, 1589, 2701, 510, 13, 51314, 51314, 400, 341, 307, 1096, 294, 341, 1729, 1716, 1228, 1333, 295, 257, 18527, 36, 12, 4092, 3109, 689, 264, 485, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07826536828344995, "compression_ratio": 1.7066666666666668, "no_speech_prob": 2.4058053895714693e-06}, {"id": 913, "seek": 532906, "start": 5345.06, "end": 5348.06, "text": " Now, of course, you have to restrict the information content here.", "tokens": [50364, 400, 264, 48994, 7006, 2564, 307, 257, 48994, 7006, 11, 457, 307, 885, 19147, 538, 364, 2058, 19866, 11, 597, 2564, 307, 611, 257, 45216, 304, 2533, 13, 50864, 50864, 467, 2516, 264, 1791, 293, 264, 2027, 293, 9898, 281, 6069, 264, 7157, 2158, 295, 264, 48994, 7006, 13, 51164, 51164, 823, 11, 295, 1164, 11, 291, 362, 281, 7694, 264, 1589, 2701, 510, 13, 51314, 51314, 400, 341, 307, 1096, 294, 341, 1729, 1716, 1228, 1333, 295, 257, 18527, 36, 12, 4092, 3109, 689, 264, 485, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07826536828344995, "compression_ratio": 1.7066666666666668, "no_speech_prob": 2.4058053895714693e-06}, {"id": 914, "seek": 532906, "start": 5348.06, "end": 5355.06, "text": " And this is done in this particular project using sort of a VAE-like approach where the...", "tokens": [50364, 400, 264, 48994, 7006, 2564, 307, 257, 48994, 7006, 11, 457, 307, 885, 19147, 538, 364, 2058, 19866, 11, 597, 2564, 307, 611, 257, 45216, 304, 2533, 13, 50864, 50864, 467, 2516, 264, 1791, 293, 264, 2027, 293, 9898, 281, 6069, 264, 7157, 2158, 295, 264, 48994, 7006, 13, 51164, 51164, 823, 11, 295, 1164, 11, 291, 362, 281, 7694, 264, 1589, 2701, 510, 13, 51314, 51314, 400, 341, 307, 1096, 294, 341, 1729, 1716, 1228, 1333, 295, 257, 18527, 36, 12, 4092, 3109, 689, 264, 485, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07826536828344995, "compression_ratio": 1.7066666666666668, "no_speech_prob": 2.4058053895714693e-06}, {"id": 915, "seek": 535506, "start": 5355.06, "end": 5360.06, "text": " I mean, it's basically a VAE with a few tricks.", "tokens": [50364, 286, 914, 11, 309, 311, 1936, 257, 18527, 36, 365, 257, 1326, 11733, 13, 50614, 50614, 407, 1176, 307, 3247, 15551, 490, 257, 7316, 300, 307, 14879, 490, 264, 5598, 295, 264, 2058, 19866, 13, 50864, 50864, 440, 5598, 295, 264, 2058, 19866, 23930, 257, 17630, 337, 1176, 2159, 382, 731, 382, 17630, 337, 1374, 21518, 13, 51114, 51114, 400, 1176, 307, 3247, 15551, 490, 300, 7316, 13, 407, 309, 311, 406, 26941, 11, 309, 311, 3247, 15551, 13, 51414, 51414, 583, 456, 311, 611, 257, 1433, 300, 9898, 281, 733, 295, 17522, 264, 2408, 295, 264, 19368, 295, 264, 1176, 311, 670, 565, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07595471723364033, "compression_ratio": 1.7797356828193833, "no_speech_prob": 6.9573638938891236e-06}, {"id": 916, "seek": 535506, "start": 5360.06, "end": 5365.06, "text": " So Z is sampled from a distribution that is obtained from the output of the encoder.", "tokens": [50364, 286, 914, 11, 309, 311, 1936, 257, 18527, 36, 365, 257, 1326, 11733, 13, 50614, 50614, 407, 1176, 307, 3247, 15551, 490, 257, 7316, 300, 307, 14879, 490, 264, 5598, 295, 264, 2058, 19866, 13, 50864, 50864, 440, 5598, 295, 264, 2058, 19866, 23930, 257, 17630, 337, 1176, 2159, 382, 731, 382, 17630, 337, 1374, 21518, 13, 51114, 51114, 400, 1176, 307, 3247, 15551, 490, 300, 7316, 13, 407, 309, 311, 406, 26941, 11, 309, 311, 3247, 15551, 13, 51414, 51414, 583, 456, 311, 611, 257, 1433, 300, 9898, 281, 733, 295, 17522, 264, 2408, 295, 264, 19368, 295, 264, 1176, 311, 670, 565, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07595471723364033, "compression_ratio": 1.7797356828193833, "no_speech_prob": 6.9573638938891236e-06}, {"id": 917, "seek": 535506, "start": 5365.06, "end": 5370.06, "text": " The output of the encoder outputs a prediction for Z bar as well as prediction for variances.", "tokens": [50364, 286, 914, 11, 309, 311, 1936, 257, 18527, 36, 365, 257, 1326, 11733, 13, 50614, 50614, 407, 1176, 307, 3247, 15551, 490, 257, 7316, 300, 307, 14879, 490, 264, 5598, 295, 264, 2058, 19866, 13, 50864, 50864, 440, 5598, 295, 264, 2058, 19866, 23930, 257, 17630, 337, 1176, 2159, 382, 731, 382, 17630, 337, 1374, 21518, 13, 51114, 51114, 400, 1176, 307, 3247, 15551, 490, 300, 7316, 13, 407, 309, 311, 406, 26941, 11, 309, 311, 3247, 15551, 13, 51414, 51414, 583, 456, 311, 611, 257, 1433, 300, 9898, 281, 733, 295, 17522, 264, 2408, 295, 264, 19368, 295, 264, 1176, 311, 670, 565, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07595471723364033, "compression_ratio": 1.7797356828193833, "no_speech_prob": 6.9573638938891236e-06}, {"id": 918, "seek": 535506, "start": 5370.06, "end": 5376.06, "text": " And Z is sampled from that distribution. So it's not optimized, it's sampled.", "tokens": [50364, 286, 914, 11, 309, 311, 1936, 257, 18527, 36, 365, 257, 1326, 11733, 13, 50614, 50614, 407, 1176, 307, 3247, 15551, 490, 257, 7316, 300, 307, 14879, 490, 264, 5598, 295, 264, 2058, 19866, 13, 50864, 50864, 440, 5598, 295, 264, 2058, 19866, 23930, 257, 17630, 337, 1176, 2159, 382, 731, 382, 17630, 337, 1374, 21518, 13, 51114, 51114, 400, 1176, 307, 3247, 15551, 490, 300, 7316, 13, 407, 309, 311, 406, 26941, 11, 309, 311, 3247, 15551, 13, 51414, 51414, 583, 456, 311, 611, 257, 1433, 300, 9898, 281, 733, 295, 17522, 264, 2408, 295, 264, 19368, 295, 264, 1176, 311, 670, 565, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07595471723364033, "compression_ratio": 1.7797356828193833, "no_speech_prob": 6.9573638938891236e-06}, {"id": 919, "seek": 535506, "start": 5376.06, "end": 5382.06, "text": " But there's also a term that tries to kind of minimize the sum of the squares of the Z's over time,", "tokens": [50364, 286, 914, 11, 309, 311, 1936, 257, 18527, 36, 365, 257, 1326, 11733, 13, 50614, 50614, 407, 1176, 307, 3247, 15551, 490, 257, 7316, 300, 307, 14879, 490, 264, 5598, 295, 264, 2058, 19866, 13, 50864, 50864, 440, 5598, 295, 264, 2058, 19866, 23930, 257, 17630, 337, 1176, 2159, 382, 731, 382, 17630, 337, 1374, 21518, 13, 51114, 51114, 400, 1176, 307, 3247, 15551, 490, 300, 7316, 13, 407, 309, 311, 406, 26941, 11, 309, 311, 3247, 15551, 13, 51414, 51414, 583, 456, 311, 611, 257, 1433, 300, 9898, 281, 733, 295, 17522, 264, 2408, 295, 264, 19368, 295, 264, 1176, 311, 670, 565, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07595471723364033, "compression_ratio": 1.7797356828193833, "no_speech_prob": 6.9573638938891236e-06}, {"id": 920, "seek": 538206, "start": 5382.06, "end": 5388.06, "text": " which is the standard technique for VAE. And that goes into the decoder.", "tokens": [50364, 597, 307, 264, 3832, 6532, 337, 18527, 36, 13, 400, 300, 1709, 666, 264, 979, 19866, 13, 50664, 50664, 400, 370, 341, 307, 8895, 382, 257, 27708, 8399, 22660, 19866, 11, 1936, 13, 50864, 50864, 821, 311, 1071, 4282, 300, 311, 3869, 281, 341, 11, 597, 307, 300, 1922, 264, 565, 1176, 307, 2935, 992, 281, 4018, 13, 51164, 51164, 407, 1922, 264, 565, 264, 1185, 307, 1907, 291, 434, 406, 4350, 281, 764, 1176, 13, 51364, 51364, 1449, 652, 428, 1151, 2041, 382, 281, 17630, 1553, 257, 1176, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06980777801351344, "compression_ratio": 1.5874439461883407, "no_speech_prob": 1.568232983117923e-05}, {"id": 921, "seek": 538206, "start": 5388.06, "end": 5392.06, "text": " And so this is trained as a conditional autoencoder, basically.", "tokens": [50364, 597, 307, 264, 3832, 6532, 337, 18527, 36, 13, 400, 300, 1709, 666, 264, 979, 19866, 13, 50664, 50664, 400, 370, 341, 307, 8895, 382, 257, 27708, 8399, 22660, 19866, 11, 1936, 13, 50864, 50864, 821, 311, 1071, 4282, 300, 311, 3869, 281, 341, 11, 597, 307, 300, 1922, 264, 565, 1176, 307, 2935, 992, 281, 4018, 13, 51164, 51164, 407, 1922, 264, 565, 264, 1185, 307, 1907, 291, 434, 406, 4350, 281, 764, 1176, 13, 51364, 51364, 1449, 652, 428, 1151, 2041, 382, 281, 17630, 1553, 257, 1176, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06980777801351344, "compression_ratio": 1.5874439461883407, "no_speech_prob": 1.568232983117923e-05}, {"id": 922, "seek": 538206, "start": 5392.06, "end": 5398.06, "text": " There's another trick that's added to this, which is that half the time Z is simply set to zero.", "tokens": [50364, 597, 307, 264, 3832, 6532, 337, 18527, 36, 13, 400, 300, 1709, 666, 264, 979, 19866, 13, 50664, 50664, 400, 370, 341, 307, 8895, 382, 257, 27708, 8399, 22660, 19866, 11, 1936, 13, 50864, 50864, 821, 311, 1071, 4282, 300, 311, 3869, 281, 341, 11, 597, 307, 300, 1922, 264, 565, 1176, 307, 2935, 992, 281, 4018, 13, 51164, 51164, 407, 1922, 264, 565, 264, 1185, 307, 1907, 291, 434, 406, 4350, 281, 764, 1176, 13, 51364, 51364, 1449, 652, 428, 1151, 2041, 382, 281, 17630, 1553, 257, 1176, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06980777801351344, "compression_ratio": 1.5874439461883407, "no_speech_prob": 1.568232983117923e-05}, {"id": 923, "seek": 538206, "start": 5398.06, "end": 5402.06, "text": " So half the time the system is told you're not allowed to use Z.", "tokens": [50364, 597, 307, 264, 3832, 6532, 337, 18527, 36, 13, 400, 300, 1709, 666, 264, 979, 19866, 13, 50664, 50664, 400, 370, 341, 307, 8895, 382, 257, 27708, 8399, 22660, 19866, 11, 1936, 13, 50864, 50864, 821, 311, 1071, 4282, 300, 311, 3869, 281, 341, 11, 597, 307, 300, 1922, 264, 565, 1176, 307, 2935, 992, 281, 4018, 13, 51164, 51164, 407, 1922, 264, 565, 264, 1185, 307, 1907, 291, 434, 406, 4350, 281, 764, 1176, 13, 51364, 51364, 1449, 652, 428, 1151, 2041, 382, 281, 17630, 1553, 257, 1176, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06980777801351344, "compression_ratio": 1.5874439461883407, "no_speech_prob": 1.568232983117923e-05}, {"id": 924, "seek": 538206, "start": 5402.06, "end": 5406.06, "text": " Just make your best guess as to prediction without a Z.", "tokens": [50364, 597, 307, 264, 3832, 6532, 337, 18527, 36, 13, 400, 300, 1709, 666, 264, 979, 19866, 13, 50664, 50664, 400, 370, 341, 307, 8895, 382, 257, 27708, 8399, 22660, 19866, 11, 1936, 13, 50864, 50864, 821, 311, 1071, 4282, 300, 311, 3869, 281, 341, 11, 597, 307, 300, 1922, 264, 565, 1176, 307, 2935, 992, 281, 4018, 13, 51164, 51164, 407, 1922, 264, 565, 264, 1185, 307, 1907, 291, 434, 406, 4350, 281, 764, 1176, 13, 51364, 51364, 1449, 652, 428, 1151, 2041, 382, 281, 17630, 1553, 257, 1176, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06980777801351344, "compression_ratio": 1.5874439461883407, "no_speech_prob": 1.568232983117923e-05}, {"id": 925, "seek": 540606, "start": 5406.06, "end": 5414.06, "text": " And that drives the system to really use the past in a bigger way than if you just have a noisy Z.", "tokens": [50364, 400, 300, 11754, 264, 1185, 281, 534, 764, 264, 1791, 294, 257, 3801, 636, 813, 498, 291, 445, 362, 257, 24518, 1176, 13, 50764, 50764, 759, 291, 445, 764, 264, 3832, 18527, 36, 12, 20467, 3097, 11, 264, 1185, 1936, 5335, 2706, 264, 1791, 13, 51014, 51014, 467, 445, 947, 1720, 13, 467, 1542, 412, 264, 1867, 983, 13, 51214, 51214, 286, 486, 2060, 264, 1472, 294, 5044, 2607, 294, 257, 2715, 11, 294, 257, 2027, 2715, 13, 51414, 51414, 10517, 291, 528, 281, 584, 746, 466, 264, 460, 1770, 82, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08426341414451599, "compression_ratio": 1.5727272727272728, "no_speech_prob": 5.771121777797816e-06}, {"id": 926, "seek": 540606, "start": 5414.06, "end": 5419.06, "text": " If you just use the standard VAE-type training, the system basically ignores the past.", "tokens": [50364, 400, 300, 11754, 264, 1185, 281, 534, 764, 264, 1791, 294, 257, 3801, 636, 813, 498, 291, 445, 362, 257, 24518, 1176, 13, 50764, 50764, 759, 291, 445, 764, 264, 3832, 18527, 36, 12, 20467, 3097, 11, 264, 1185, 1936, 5335, 2706, 264, 1791, 13, 51014, 51014, 467, 445, 947, 1720, 13, 467, 1542, 412, 264, 1867, 983, 13, 51214, 51214, 286, 486, 2060, 264, 1472, 294, 5044, 2607, 294, 257, 2715, 11, 294, 257, 2027, 2715, 13, 51414, 51414, 10517, 291, 528, 281, 584, 746, 466, 264, 460, 1770, 82, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08426341414451599, "compression_ratio": 1.5727272727272728, "no_speech_prob": 5.771121777797816e-06}, {"id": 927, "seek": 540606, "start": 5419.06, "end": 5423.06, "text": " It just cheats. It looks at the answer why.", "tokens": [50364, 400, 300, 11754, 264, 1185, 281, 534, 764, 264, 1791, 294, 257, 3801, 636, 813, 498, 291, 445, 362, 257, 24518, 1176, 13, 50764, 50764, 759, 291, 445, 764, 264, 3832, 18527, 36, 12, 20467, 3097, 11, 264, 1185, 1936, 5335, 2706, 264, 1791, 13, 51014, 51014, 467, 445, 947, 1720, 13, 467, 1542, 412, 264, 1867, 983, 13, 51214, 51214, 286, 486, 2060, 264, 1472, 294, 5044, 2607, 294, 257, 2715, 11, 294, 257, 2027, 2715, 13, 51414, 51414, 10517, 291, 528, 281, 584, 746, 466, 264, 460, 1770, 82, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08426341414451599, "compression_ratio": 1.5727272727272728, "no_speech_prob": 5.771121777797816e-06}, {"id": 928, "seek": 540606, "start": 5423.06, "end": 5427.06, "text": " I will cover the rest in greater detail in a lab, in a future lab.", "tokens": [50364, 400, 300, 11754, 264, 1185, 281, 534, 764, 264, 1791, 294, 257, 3801, 636, 813, 498, 291, 445, 362, 257, 24518, 1176, 13, 50764, 50764, 759, 291, 445, 764, 264, 3832, 18527, 36, 12, 20467, 3097, 11, 264, 1185, 1936, 5335, 2706, 264, 1791, 13, 51014, 51014, 467, 445, 947, 1720, 13, 467, 1542, 412, 264, 1867, 983, 13, 51214, 51214, 286, 486, 2060, 264, 1472, 294, 5044, 2607, 294, 257, 2715, 11, 294, 257, 2027, 2715, 13, 51414, 51414, 10517, 291, 528, 281, 584, 746, 466, 264, 460, 1770, 82, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08426341414451599, "compression_ratio": 1.5727272727272728, "no_speech_prob": 5.771121777797816e-06}, {"id": 929, "seek": 540606, "start": 5427.06, "end": 5431.06, "text": " Perhaps you want to say something about the GANs?", "tokens": [50364, 400, 300, 11754, 264, 1185, 281, 534, 764, 264, 1791, 294, 257, 3801, 636, 813, 498, 291, 445, 362, 257, 24518, 1176, 13, 50764, 50764, 759, 291, 445, 764, 264, 3832, 18527, 36, 12, 20467, 3097, 11, 264, 1185, 1936, 5335, 2706, 264, 1791, 13, 51014, 51014, 467, 445, 947, 1720, 13, 467, 1542, 412, 264, 1867, 983, 13, 51214, 51214, 286, 486, 2060, 264, 1472, 294, 5044, 2607, 294, 257, 2715, 11, 294, 257, 2027, 2715, 13, 51414, 51414, 10517, 291, 528, 281, 584, 746, 466, 264, 460, 1770, 82, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08426341414451599, "compression_ratio": 1.5727272727272728, "no_speech_prob": 5.771121777797816e-06}, {"id": 930, "seek": 543106, "start": 5431.06, "end": 5436.06, "text": " I want to say something about GANs.", "tokens": [50364, 286, 528, 281, 584, 746, 466, 460, 1770, 82, 13, 50614, 50614, 407, 460, 1770, 82, 366, 257, 1729, 1254, 295, 8712, 488, 2539, 13, 50914, 50914, 5459, 300, 562, 321, 2825, 466, 2281, 12, 6032, 2539, 11, 321, 362, 1412, 2793, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1910050640935483, "compression_ratio": 1.2923076923076924, "no_speech_prob": 4.3566546992224175e-06}, {"id": 931, "seek": 543106, "start": 5436.06, "end": 5442.06, "text": " So GANs are a particular form of contrastive learning.", "tokens": [50364, 286, 528, 281, 584, 746, 466, 460, 1770, 82, 13, 50614, 50614, 407, 460, 1770, 82, 366, 257, 1729, 1254, 295, 8712, 488, 2539, 13, 50914, 50914, 5459, 300, 562, 321, 2825, 466, 2281, 12, 6032, 2539, 11, 321, 362, 1412, 2793, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1910050640935483, "compression_ratio": 1.2923076923076924, "no_speech_prob": 4.3566546992224175e-06}, {"id": 932, "seek": 543106, "start": 5442.06, "end": 5455.06, "text": " Remember that when we talked about energy-based learning, we have data points", "tokens": [50364, 286, 528, 281, 584, 746, 466, 460, 1770, 82, 13, 50614, 50614, 407, 460, 1770, 82, 366, 257, 1729, 1254, 295, 8712, 488, 2539, 13, 50914, 50914, 5459, 300, 562, 321, 2825, 466, 2281, 12, 6032, 2539, 11, 321, 362, 1412, 2793, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1910050640935483, "compression_ratio": 1.2923076923076924, "no_speech_prob": 4.3566546992224175e-06}, {"id": 933, "seek": 545506, "start": 5455.06, "end": 5473.06, "text": " and a model, which I'm going to draw like this, with a cost function.", "tokens": [50364, 293, 257, 2316, 11, 597, 286, 478, 516, 281, 2642, 411, 341, 11, 365, 257, 2063, 2445, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.23956014893271707, "compression_ratio": 0.971830985915493, "no_speech_prob": 8.662430445838254e-06}, {"id": 934, "seek": 547306, "start": 5473.06, "end": 5488.06, "text": " It could have any kind of structure, but I'm just going to draw it like this.", "tokens": [50364, 467, 727, 362, 604, 733, 295, 3877, 11, 457, 286, 478, 445, 516, 281, 2642, 309, 411, 341, 13, 51114, 51114, 407, 341, 576, 312, 1333, 295, 257, 31565, 12, 20467, 2316, 13, 51364, 51364, 407, 3811, 300, 264, 2316, 510, 307, 364, 8399, 22660, 19866, 420, 746, 411, 341, 13, 51564, 51564, 583, 291, 393, 3811, 445, 466, 1340, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14586849212646485, "compression_ratio": 1.4939024390243902, "no_speech_prob": 2.2600784177484456e-06}, {"id": 935, "seek": 547306, "start": 5488.06, "end": 5493.06, "text": " So this would be sort of a reconstruction-type model.", "tokens": [50364, 467, 727, 362, 604, 733, 295, 3877, 11, 457, 286, 478, 445, 516, 281, 2642, 309, 411, 341, 13, 51114, 51114, 407, 341, 576, 312, 1333, 295, 257, 31565, 12, 20467, 2316, 13, 51364, 51364, 407, 3811, 300, 264, 2316, 510, 307, 364, 8399, 22660, 19866, 420, 746, 411, 341, 13, 51564, 51564, 583, 291, 393, 3811, 445, 466, 1340, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14586849212646485, "compression_ratio": 1.4939024390243902, "no_speech_prob": 2.2600784177484456e-06}, {"id": 936, "seek": 547306, "start": 5493.06, "end": 5497.06, "text": " So imagine that the model here is an autoencoder or something like this.", "tokens": [50364, 467, 727, 362, 604, 733, 295, 3877, 11, 457, 286, 478, 445, 516, 281, 2642, 309, 411, 341, 13, 51114, 51114, 407, 341, 576, 312, 1333, 295, 257, 31565, 12, 20467, 2316, 13, 51364, 51364, 407, 3811, 300, 264, 2316, 510, 307, 364, 8399, 22660, 19866, 420, 746, 411, 341, 13, 51564, 51564, 583, 291, 393, 3811, 445, 466, 1340, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14586849212646485, "compression_ratio": 1.4939024390243902, "no_speech_prob": 2.2600784177484456e-06}, {"id": 937, "seek": 547306, "start": 5497.06, "end": 5501.06, "text": " But you can imagine just about anything.", "tokens": [50364, 467, 727, 362, 604, 733, 295, 3877, 11, 457, 286, 478, 445, 516, 281, 2642, 309, 411, 341, 13, 51114, 51114, 407, 341, 576, 312, 1333, 295, 257, 31565, 12, 20467, 2316, 13, 51364, 51364, 407, 3811, 300, 264, 2316, 510, 307, 364, 8399, 22660, 19866, 420, 746, 411, 341, 13, 51564, 51564, 583, 291, 393, 3811, 445, 466, 1340, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14586849212646485, "compression_ratio": 1.4939024390243902, "no_speech_prob": 2.2600784177484456e-06}, {"id": 938, "seek": 550106, "start": 5501.06, "end": 5507.06, "text": " A simplified version, I mean a more general version of this, would be just Y goes into a cost function", "tokens": [50364, 316, 26335, 3037, 11, 286, 914, 257, 544, 2674, 3037, 295, 341, 11, 576, 312, 445, 398, 1709, 666, 257, 2063, 2445, 50664, 50664, 293, 286, 478, 406, 1608, 5489, 437, 264, 2063, 2445, 1542, 411, 13, 51064, 51064, 407, 437, 264, 2063, 2445, 715, 1819, 307, 294, 264, 1901, 295, 398, 13, 51314, 51314, 407, 718, 311, 584, 398, 307, 732, 12, 18759, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15929987810660098, "compression_ratio": 1.5460122699386503, "no_speech_prob": 5.789537317468785e-05}, {"id": 939, "seek": 550106, "start": 5507.06, "end": 5515.06, "text": " and I'm not specifying what the cost function looks like.", "tokens": [50364, 316, 26335, 3037, 11, 286, 914, 257, 544, 2674, 3037, 295, 341, 11, 576, 312, 445, 398, 1709, 666, 257, 2063, 2445, 50664, 50664, 293, 286, 478, 406, 1608, 5489, 437, 264, 2063, 2445, 1542, 411, 13, 51064, 51064, 407, 437, 264, 2063, 2445, 715, 1819, 307, 294, 264, 1901, 295, 398, 13, 51314, 51314, 407, 718, 311, 584, 398, 307, 732, 12, 18759, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15929987810660098, "compression_ratio": 1.5460122699386503, "no_speech_prob": 5.789537317468785e-05}, {"id": 940, "seek": 550106, "start": 5515.06, "end": 5520.06, "text": " So what the cost function computes is in the space of Y.", "tokens": [50364, 316, 26335, 3037, 11, 286, 914, 257, 544, 2674, 3037, 295, 341, 11, 576, 312, 445, 398, 1709, 666, 257, 2063, 2445, 50664, 50664, 293, 286, 478, 406, 1608, 5489, 437, 264, 2063, 2445, 1542, 411, 13, 51064, 51064, 407, 437, 264, 2063, 2445, 715, 1819, 307, 294, 264, 1901, 295, 398, 13, 51314, 51314, 407, 718, 311, 584, 398, 307, 732, 12, 18759, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15929987810660098, "compression_ratio": 1.5460122699386503, "no_speech_prob": 5.789537317468785e-05}, {"id": 941, "seek": 550106, "start": 5520.06, "end": 5528.06, "text": " So let's say Y is two-dimensional.", "tokens": [50364, 316, 26335, 3037, 11, 286, 914, 257, 544, 2674, 3037, 295, 341, 11, 576, 312, 445, 398, 1709, 666, 257, 2063, 2445, 50664, 50664, 293, 286, 478, 406, 1608, 5489, 437, 264, 2063, 2445, 1542, 411, 13, 51064, 51064, 407, 437, 264, 2063, 2445, 715, 1819, 307, 294, 264, 1901, 295, 398, 13, 51314, 51314, 407, 718, 311, 584, 398, 307, 732, 12, 18759, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15929987810660098, "compression_ratio": 1.5460122699386503, "no_speech_prob": 5.789537317468785e-05}, {"id": 942, "seek": 552806, "start": 5528.06, "end": 5537.06, "text": " So Y is an energy that we want to be low on the data and high outside the data.", "tokens": [50364, 407, 398, 307, 364, 2281, 300, 321, 528, 281, 312, 2295, 322, 264, 1412, 293, 1090, 2380, 264, 1412, 13, 50814, 50814, 400, 510, 286, 23506, 12804, 257, 1578, 2281, 2445, 13, 50964, 50964, 407, 341, 2281, 2445, 307, 1578, 570, 309, 820, 312, 2295, 926, 341, 4458, 689, 321, 362, 1412, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.10362412636740166, "compression_ratio": 1.5804195804195804, "no_speech_prob": 1.5201068890746683e-05}, {"id": 943, "seek": 552806, "start": 5537.06, "end": 5540.06, "text": " And here I deliberately drew a bad energy function.", "tokens": [50364, 407, 398, 307, 364, 2281, 300, 321, 528, 281, 312, 2295, 322, 264, 1412, 293, 1090, 2380, 264, 1412, 13, 50814, 50814, 400, 510, 286, 23506, 12804, 257, 1578, 2281, 2445, 13, 50964, 50964, 407, 341, 2281, 2445, 307, 1578, 570, 309, 820, 312, 2295, 926, 341, 4458, 689, 321, 362, 1412, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.10362412636740166, "compression_ratio": 1.5804195804195804, "no_speech_prob": 1.5201068890746683e-05}, {"id": 944, "seek": 552806, "start": 5540.06, "end": 5549.06, "text": " So this energy function is bad because it should be low around this region where we have data.", "tokens": [50364, 407, 398, 307, 364, 2281, 300, 321, 528, 281, 312, 2295, 322, 264, 1412, 293, 1090, 2380, 264, 1412, 13, 50814, 50814, 400, 510, 286, 23506, 12804, 257, 1578, 2281, 2445, 13, 50964, 50964, 407, 341, 2281, 2445, 307, 1578, 570, 309, 820, 312, 2295, 926, 341, 4458, 689, 321, 362, 1412, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.10362412636740166, "compression_ratio": 1.5804195804195804, "no_speech_prob": 1.5201068890746683e-05}, {"id": 945, "seek": 554906, "start": 5549.06, "end": 5558.06, "text": " And it should be higher outside, and right now it's pretty low in this region right here.", "tokens": [50364, 400, 309, 820, 312, 2946, 2380, 11, 293, 558, 586, 309, 311, 1238, 2295, 294, 341, 4458, 558, 510, 13, 50814, 50814, 407, 321, 2825, 466, 8712, 488, 7150, 11, 293, 8712, 488, 7150, 4603, 294, 1940, 257, 6889, 51214, 51214, 293, 7380, 760, 322, 1080, 2281, 293, 550, 1940, 257, 8712, 488, 6889, 11, 597, 286, 478, 516, 281, 2642, 294, 9656, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08030557632446289, "compression_ratio": 1.6511627906976745, "no_speech_prob": 7.646034646313637e-06}, {"id": 946, "seek": 554906, "start": 5558.06, "end": 5566.06, "text": " So we talked about contrastive methods, and contrastive methods consist in taking a sample", "tokens": [50364, 400, 309, 820, 312, 2946, 2380, 11, 293, 558, 586, 309, 311, 1238, 2295, 294, 341, 4458, 558, 510, 13, 50814, 50814, 407, 321, 2825, 466, 8712, 488, 7150, 11, 293, 8712, 488, 7150, 4603, 294, 1940, 257, 6889, 51214, 51214, 293, 7380, 760, 322, 1080, 2281, 293, 550, 1940, 257, 8712, 488, 6889, 11, 597, 286, 478, 516, 281, 2642, 294, 9656, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08030557632446289, "compression_ratio": 1.6511627906976745, "no_speech_prob": 7.646034646313637e-06}, {"id": 947, "seek": 554906, "start": 5566.06, "end": 5576.06, "text": " and pushing down on its energy and then taking a contrastive sample, which I'm going to draw in purple.", "tokens": [50364, 400, 309, 820, 312, 2946, 2380, 11, 293, 558, 586, 309, 311, 1238, 2295, 294, 341, 4458, 558, 510, 13, 50814, 50814, 407, 321, 2825, 466, 8712, 488, 7150, 11, 293, 8712, 488, 7150, 4603, 294, 1940, 257, 6889, 51214, 51214, 293, 7380, 760, 322, 1080, 2281, 293, 550, 1940, 257, 8712, 488, 6889, 11, 597, 286, 478, 516, 281, 2642, 294, 9656, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08030557632446289, "compression_ratio": 1.6511627906976745, "no_speech_prob": 7.646034646313637e-06}, {"id": 948, "seek": 557606, "start": 5576.06, "end": 5583.06, "text": " So contrastive samples should be a sample that our model already gives low energy to, but should not give low energy to.", "tokens": [50364, 407, 8712, 488, 10938, 820, 312, 257, 6889, 300, 527, 2316, 1217, 2709, 2295, 2281, 281, 11, 457, 820, 406, 976, 2295, 2281, 281, 13, 50714, 50714, 492, 434, 516, 281, 2944, 300, 493, 13, 50864, 50864, 407, 2944, 493, 322, 264, 2281, 295, 341, 2146, 11, 2944, 760, 322, 264, 2281, 295, 300, 2146, 13, 51164, 51164, 400, 498, 291, 1066, 8867, 729, 10938, 293, 729, 8712, 488, 10938, 731, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09480243607571251, "compression_ratio": 1.7738095238095237, "no_speech_prob": 2.4296570700244047e-05}, {"id": 949, "seek": 557606, "start": 5583.06, "end": 5586.06, "text": " We're going to push that up.", "tokens": [50364, 407, 8712, 488, 10938, 820, 312, 257, 6889, 300, 527, 2316, 1217, 2709, 2295, 2281, 281, 11, 457, 820, 406, 976, 2295, 2281, 281, 13, 50714, 50714, 492, 434, 516, 281, 2944, 300, 493, 13, 50864, 50864, 407, 2944, 493, 322, 264, 2281, 295, 341, 2146, 11, 2944, 760, 322, 264, 2281, 295, 300, 2146, 13, 51164, 51164, 400, 498, 291, 1066, 8867, 729, 10938, 293, 729, 8712, 488, 10938, 731, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09480243607571251, "compression_ratio": 1.7738095238095237, "no_speech_prob": 2.4296570700244047e-05}, {"id": 950, "seek": 557606, "start": 5586.06, "end": 5592.06, "text": " So push up on the energy of this guy, push down on the energy of that guy.", "tokens": [50364, 407, 8712, 488, 10938, 820, 312, 257, 6889, 300, 527, 2316, 1217, 2709, 2295, 2281, 281, 11, 457, 820, 406, 976, 2295, 2281, 281, 13, 50714, 50714, 492, 434, 516, 281, 2944, 300, 493, 13, 50864, 50864, 407, 2944, 493, 322, 264, 2281, 295, 341, 2146, 11, 2944, 760, 322, 264, 2281, 295, 300, 2146, 13, 51164, 51164, 400, 498, 291, 1066, 8867, 729, 10938, 293, 729, 8712, 488, 10938, 731, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09480243607571251, "compression_ratio": 1.7738095238095237, "no_speech_prob": 2.4296570700244047e-05}, {"id": 951, "seek": 557606, "start": 5592.06, "end": 5598.06, "text": " And if you keep picking those samples and those contrastive samples well,", "tokens": [50364, 407, 8712, 488, 10938, 820, 312, 257, 6889, 300, 527, 2316, 1217, 2709, 2295, 2281, 281, 11, 457, 820, 406, 976, 2295, 2281, 281, 13, 50714, 50714, 492, 434, 516, 281, 2944, 300, 493, 13, 50864, 50864, 407, 2944, 493, 322, 264, 2281, 295, 341, 2146, 11, 2944, 760, 322, 264, 2281, 295, 300, 2146, 13, 51164, 51164, 400, 498, 291, 1066, 8867, 729, 10938, 293, 729, 8712, 488, 10938, 731, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09480243607571251, "compression_ratio": 1.7738095238095237, "no_speech_prob": 2.4296570700244047e-05}, {"id": 952, "seek": 559806, "start": 5598.06, "end": 5606.06, "text": " by minimizing some objective function that wants to make the energy of the blue point small and the energy of the pink points high,", "tokens": [50364, 538, 46608, 512, 10024, 2445, 300, 2738, 281, 652, 264, 2281, 295, 264, 3344, 935, 1359, 293, 264, 2281, 295, 264, 7022, 2793, 1090, 11, 50764, 50764, 550, 264, 1185, 486, 1466, 6108, 13, 51014, 51014, 407, 321, 600, 1612, 2940, 2098, 295, 17746, 8712, 488, 10938, 13, 51164, 51164, 440, 1558, 295, 1441, 78, 3436, 8399, 22660, 19866, 11, 597, 307, 281, 747, 257, 6889, 293, 1936, 17366, 309, 294, 512, 636, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.07381629332517967, "compression_ratio": 1.5817307692307692, "no_speech_prob": 1.8923063180409372e-05}, {"id": 953, "seek": 559806, "start": 5606.06, "end": 5611.06, "text": " then the system will learn properly.", "tokens": [50364, 538, 46608, 512, 10024, 2445, 300, 2738, 281, 652, 264, 2281, 295, 264, 3344, 935, 1359, 293, 264, 2281, 295, 264, 7022, 2793, 1090, 11, 50764, 50764, 550, 264, 1185, 486, 1466, 6108, 13, 51014, 51014, 407, 321, 600, 1612, 2940, 2098, 295, 17746, 8712, 488, 10938, 13, 51164, 51164, 440, 1558, 295, 1441, 78, 3436, 8399, 22660, 19866, 11, 597, 307, 281, 747, 257, 6889, 293, 1936, 17366, 309, 294, 512, 636, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.07381629332517967, "compression_ratio": 1.5817307692307692, "no_speech_prob": 1.8923063180409372e-05}, {"id": 954, "seek": 559806, "start": 5611.06, "end": 5614.06, "text": " So we've seen several ways of generating contrastive samples.", "tokens": [50364, 538, 46608, 512, 10024, 2445, 300, 2738, 281, 652, 264, 2281, 295, 264, 3344, 935, 1359, 293, 264, 2281, 295, 264, 7022, 2793, 1090, 11, 50764, 50764, 550, 264, 1185, 486, 1466, 6108, 13, 51014, 51014, 407, 321, 600, 1612, 2940, 2098, 295, 17746, 8712, 488, 10938, 13, 51164, 51164, 440, 1558, 295, 1441, 78, 3436, 8399, 22660, 19866, 11, 597, 307, 281, 747, 257, 6889, 293, 1936, 17366, 309, 294, 512, 636, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.07381629332517967, "compression_ratio": 1.5817307692307692, "no_speech_prob": 1.8923063180409372e-05}, {"id": 955, "seek": 559806, "start": 5614.06, "end": 5620.06, "text": " The idea of denoising autoencoder, which is to take a sample and basically corrupt it in some way.", "tokens": [50364, 538, 46608, 512, 10024, 2445, 300, 2738, 281, 652, 264, 2281, 295, 264, 3344, 935, 1359, 293, 264, 2281, 295, 264, 7022, 2793, 1090, 11, 50764, 50764, 550, 264, 1185, 486, 1466, 6108, 13, 51014, 51014, 407, 321, 600, 1612, 2940, 2098, 295, 17746, 8712, 488, 10938, 13, 51164, 51164, 440, 1558, 295, 1441, 78, 3436, 8399, 22660, 19866, 11, 597, 307, 281, 747, 257, 6889, 293, 1936, 17366, 309, 294, 512, 636, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.07381629332517967, "compression_ratio": 1.5817307692307692, "no_speech_prob": 1.8923063180409372e-05}, {"id": 956, "seek": 562006, "start": 5620.06, "end": 5629.06, "text": " We've seen the idea of contrastive divergence, which takes a sample and then you go down the energy with some noise,", "tokens": [50364, 492, 600, 1612, 264, 1558, 295, 8712, 488, 47387, 11, 597, 2516, 257, 6889, 293, 550, 291, 352, 760, 264, 2281, 365, 512, 5658, 11, 50814, 50814, 293, 300, 2709, 291, 257, 8712, 488, 6889, 281, 2944, 493, 13, 51064, 51064, 400, 321, 600, 1612, 257, 1230, 295, 661, 7150, 300, 366, 2361, 322, 4059, 3601, 466, 32194, 1296, 10938, 13, 51414, 51414, 583, 510, 307, 1071, 1558, 13, 51514, 51514, 440, 661, 1558, 307, 281, 3847, 257, 18161, 2533, 281, 5258, 729, 8712, 488, 10938, 5613, 2276, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0724684961380497, "compression_ratio": 1.70995670995671, "no_speech_prob": 1.1657779396045953e-05}, {"id": 957, "seek": 562006, "start": 5629.06, "end": 5634.06, "text": " and that gives you a contrastive sample to push up.", "tokens": [50364, 492, 600, 1612, 264, 1558, 295, 8712, 488, 47387, 11, 597, 2516, 257, 6889, 293, 550, 291, 352, 760, 264, 2281, 365, 512, 5658, 11, 50814, 50814, 293, 300, 2709, 291, 257, 8712, 488, 6889, 281, 2944, 493, 13, 51064, 51064, 400, 321, 600, 1612, 257, 1230, 295, 661, 7150, 300, 366, 2361, 322, 4059, 3601, 466, 32194, 1296, 10938, 13, 51414, 51414, 583, 510, 307, 1071, 1558, 13, 51514, 51514, 440, 661, 1558, 307, 281, 3847, 257, 18161, 2533, 281, 5258, 729, 8712, 488, 10938, 5613, 2276, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0724684961380497, "compression_ratio": 1.70995670995671, "no_speech_prob": 1.1657779396045953e-05}, {"id": 958, "seek": 562006, "start": 5634.06, "end": 5641.06, "text": " And we've seen a number of other methods that are based on prior knowledge about similarity between samples.", "tokens": [50364, 492, 600, 1612, 264, 1558, 295, 8712, 488, 47387, 11, 597, 2516, 257, 6889, 293, 550, 291, 352, 760, 264, 2281, 365, 512, 5658, 11, 50814, 50814, 293, 300, 2709, 291, 257, 8712, 488, 6889, 281, 2944, 493, 13, 51064, 51064, 400, 321, 600, 1612, 257, 1230, 295, 661, 7150, 300, 366, 2361, 322, 4059, 3601, 466, 32194, 1296, 10938, 13, 51414, 51414, 583, 510, 307, 1071, 1558, 13, 51514, 51514, 440, 661, 1558, 307, 281, 3847, 257, 18161, 2533, 281, 5258, 729, 8712, 488, 10938, 5613, 2276, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0724684961380497, "compression_ratio": 1.70995670995671, "no_speech_prob": 1.1657779396045953e-05}, {"id": 959, "seek": 562006, "start": 5641.06, "end": 5643.06, "text": " But here is another idea.", "tokens": [50364, 492, 600, 1612, 264, 1558, 295, 8712, 488, 47387, 11, 597, 2516, 257, 6889, 293, 550, 291, 352, 760, 264, 2281, 365, 512, 5658, 11, 50814, 50814, 293, 300, 2709, 291, 257, 8712, 488, 6889, 281, 2944, 493, 13, 51064, 51064, 400, 321, 600, 1612, 257, 1230, 295, 661, 7150, 300, 366, 2361, 322, 4059, 3601, 466, 32194, 1296, 10938, 13, 51414, 51414, 583, 510, 307, 1071, 1558, 13, 51514, 51514, 440, 661, 1558, 307, 281, 3847, 257, 18161, 2533, 281, 5258, 729, 8712, 488, 10938, 5613, 2276, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0724684961380497, "compression_ratio": 1.70995670995671, "no_speech_prob": 1.1657779396045953e-05}, {"id": 960, "seek": 562006, "start": 5643.06, "end": 5648.06, "text": " The other idea is to train a neural net to produce those contrastive samples intelligently.", "tokens": [50364, 492, 600, 1612, 264, 1558, 295, 8712, 488, 47387, 11, 597, 2516, 257, 6889, 293, 550, 291, 352, 760, 264, 2281, 365, 512, 5658, 11, 50814, 50814, 293, 300, 2709, 291, 257, 8712, 488, 6889, 281, 2944, 493, 13, 51064, 51064, 400, 321, 600, 1612, 257, 1230, 295, 661, 7150, 300, 366, 2361, 322, 4059, 3601, 466, 32194, 1296, 10938, 13, 51414, 51414, 583, 510, 307, 1071, 1558, 13, 51514, 51514, 440, 661, 1558, 307, 281, 3847, 257, 18161, 2533, 281, 5258, 729, 8712, 488, 10938, 5613, 2276, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0724684961380497, "compression_ratio": 1.70995670995671, "no_speech_prob": 1.1657779396045953e-05}, {"id": 961, "seek": 564806, "start": 5648.06, "end": 5654.06, "text": " And that's the basic idea of GANs, at least in a form of GANs that would be called energy-based GANs.", "tokens": [50364, 400, 300, 311, 264, 3875, 1558, 295, 460, 1770, 82, 11, 412, 1935, 294, 257, 1254, 295, 460, 1770, 82, 300, 576, 312, 1219, 2281, 12, 6032, 460, 1770, 82, 13, 50664, 50664, 821, 366, 2940, 1254, 4136, 295, 460, 1770, 82, 13, 50764, 50764, 682, 1186, 11, 456, 307, 364, 2302, 19811, 1329, 295, 3683, 3467, 295, 460, 1770, 82, 13, 50964, 50964, 583, 264, 3875, 1558, 295, 460, 1770, 82, 307, 300, 291, 3847, 428, 2281, 2316, 13, 51214, 51214, 407, 264, 2281, 2316, 294, 264, 4319, 295, 460, 1770, 307, 1219, 257, 20828, 1639, 420, 2171, 257, 7850, 11, 51414, 51414, 457, 309, 311, 1936, 445, 588, 2531, 281, 364, 2281, 2316, 13, 51614, 51614, 400, 291, 3847, 309, 281, 747, 2295, 2281, 322, 264, 1412, 2793, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07300114101833767, "compression_ratio": 1.7962264150943397, "no_speech_prob": 1.95242500922177e-05}, {"id": 962, "seek": 564806, "start": 5654.06, "end": 5656.06, "text": " There are several formulations of GANs.", "tokens": [50364, 400, 300, 311, 264, 3875, 1558, 295, 460, 1770, 82, 11, 412, 1935, 294, 257, 1254, 295, 460, 1770, 82, 300, 576, 312, 1219, 2281, 12, 6032, 460, 1770, 82, 13, 50664, 50664, 821, 366, 2940, 1254, 4136, 295, 460, 1770, 82, 13, 50764, 50764, 682, 1186, 11, 456, 307, 364, 2302, 19811, 1329, 295, 3683, 3467, 295, 460, 1770, 82, 13, 50964, 50964, 583, 264, 3875, 1558, 295, 460, 1770, 82, 307, 300, 291, 3847, 428, 2281, 2316, 13, 51214, 51214, 407, 264, 2281, 2316, 294, 264, 4319, 295, 460, 1770, 307, 1219, 257, 20828, 1639, 420, 2171, 257, 7850, 11, 51414, 51414, 457, 309, 311, 1936, 445, 588, 2531, 281, 364, 2281, 2316, 13, 51614, 51614, 400, 291, 3847, 309, 281, 747, 2295, 2281, 322, 264, 1412, 2793, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07300114101833767, "compression_ratio": 1.7962264150943397, "no_speech_prob": 1.95242500922177e-05}, {"id": 963, "seek": 564806, "start": 5656.06, "end": 5660.06, "text": " In fact, there is an entire laundry list of various types of GANs.", "tokens": [50364, 400, 300, 311, 264, 3875, 1558, 295, 460, 1770, 82, 11, 412, 1935, 294, 257, 1254, 295, 460, 1770, 82, 300, 576, 312, 1219, 2281, 12, 6032, 460, 1770, 82, 13, 50664, 50664, 821, 366, 2940, 1254, 4136, 295, 460, 1770, 82, 13, 50764, 50764, 682, 1186, 11, 456, 307, 364, 2302, 19811, 1329, 295, 3683, 3467, 295, 460, 1770, 82, 13, 50964, 50964, 583, 264, 3875, 1558, 295, 460, 1770, 82, 307, 300, 291, 3847, 428, 2281, 2316, 13, 51214, 51214, 407, 264, 2281, 2316, 294, 264, 4319, 295, 460, 1770, 307, 1219, 257, 20828, 1639, 420, 2171, 257, 7850, 11, 51414, 51414, 457, 309, 311, 1936, 445, 588, 2531, 281, 364, 2281, 2316, 13, 51614, 51614, 400, 291, 3847, 309, 281, 747, 2295, 2281, 322, 264, 1412, 2793, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07300114101833767, "compression_ratio": 1.7962264150943397, "no_speech_prob": 1.95242500922177e-05}, {"id": 964, "seek": 564806, "start": 5660.06, "end": 5665.06, "text": " But the basic idea of GANs is that you train your energy model.", "tokens": [50364, 400, 300, 311, 264, 3875, 1558, 295, 460, 1770, 82, 11, 412, 1935, 294, 257, 1254, 295, 460, 1770, 82, 300, 576, 312, 1219, 2281, 12, 6032, 460, 1770, 82, 13, 50664, 50664, 821, 366, 2940, 1254, 4136, 295, 460, 1770, 82, 13, 50764, 50764, 682, 1186, 11, 456, 307, 364, 2302, 19811, 1329, 295, 3683, 3467, 295, 460, 1770, 82, 13, 50964, 50964, 583, 264, 3875, 1558, 295, 460, 1770, 82, 307, 300, 291, 3847, 428, 2281, 2316, 13, 51214, 51214, 407, 264, 2281, 2316, 294, 264, 4319, 295, 460, 1770, 307, 1219, 257, 20828, 1639, 420, 2171, 257, 7850, 11, 51414, 51414, 457, 309, 311, 1936, 445, 588, 2531, 281, 364, 2281, 2316, 13, 51614, 51614, 400, 291, 3847, 309, 281, 747, 2295, 2281, 322, 264, 1412, 2793, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07300114101833767, "compression_ratio": 1.7962264150943397, "no_speech_prob": 1.95242500922177e-05}, {"id": 965, "seek": 564806, "start": 5665.06, "end": 5669.06, "text": " So the energy model in the context of GAN is called a discriminator or sometimes a critic,", "tokens": [50364, 400, 300, 311, 264, 3875, 1558, 295, 460, 1770, 82, 11, 412, 1935, 294, 257, 1254, 295, 460, 1770, 82, 300, 576, 312, 1219, 2281, 12, 6032, 460, 1770, 82, 13, 50664, 50664, 821, 366, 2940, 1254, 4136, 295, 460, 1770, 82, 13, 50764, 50764, 682, 1186, 11, 456, 307, 364, 2302, 19811, 1329, 295, 3683, 3467, 295, 460, 1770, 82, 13, 50964, 50964, 583, 264, 3875, 1558, 295, 460, 1770, 82, 307, 300, 291, 3847, 428, 2281, 2316, 13, 51214, 51214, 407, 264, 2281, 2316, 294, 264, 4319, 295, 460, 1770, 307, 1219, 257, 20828, 1639, 420, 2171, 257, 7850, 11, 51414, 51414, 457, 309, 311, 1936, 445, 588, 2531, 281, 364, 2281, 2316, 13, 51614, 51614, 400, 291, 3847, 309, 281, 747, 2295, 2281, 322, 264, 1412, 2793, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07300114101833767, "compression_ratio": 1.7962264150943397, "no_speech_prob": 1.95242500922177e-05}, {"id": 966, "seek": 564806, "start": 5669.06, "end": 5673.06, "text": " but it's basically just very similar to an energy model.", "tokens": [50364, 400, 300, 311, 264, 3875, 1558, 295, 460, 1770, 82, 11, 412, 1935, 294, 257, 1254, 295, 460, 1770, 82, 300, 576, 312, 1219, 2281, 12, 6032, 460, 1770, 82, 13, 50664, 50664, 821, 366, 2940, 1254, 4136, 295, 460, 1770, 82, 13, 50764, 50764, 682, 1186, 11, 456, 307, 364, 2302, 19811, 1329, 295, 3683, 3467, 295, 460, 1770, 82, 13, 50964, 50964, 583, 264, 3875, 1558, 295, 460, 1770, 82, 307, 300, 291, 3847, 428, 2281, 2316, 13, 51214, 51214, 407, 264, 2281, 2316, 294, 264, 4319, 295, 460, 1770, 307, 1219, 257, 20828, 1639, 420, 2171, 257, 7850, 11, 51414, 51414, 457, 309, 311, 1936, 445, 588, 2531, 281, 364, 2281, 2316, 13, 51614, 51614, 400, 291, 3847, 309, 281, 747, 2295, 2281, 322, 264, 1412, 2793, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07300114101833767, "compression_ratio": 1.7962264150943397, "no_speech_prob": 1.95242500922177e-05}, {"id": 967, "seek": 564806, "start": 5673.06, "end": 5676.06, "text": " And you train it to take low energy on the data points.", "tokens": [50364, 400, 300, 311, 264, 3875, 1558, 295, 460, 1770, 82, 11, 412, 1935, 294, 257, 1254, 295, 460, 1770, 82, 300, 576, 312, 1219, 2281, 12, 6032, 460, 1770, 82, 13, 50664, 50664, 821, 366, 2940, 1254, 4136, 295, 460, 1770, 82, 13, 50764, 50764, 682, 1186, 11, 456, 307, 364, 2302, 19811, 1329, 295, 3683, 3467, 295, 460, 1770, 82, 13, 50964, 50964, 583, 264, 3875, 1558, 295, 460, 1770, 82, 307, 300, 291, 3847, 428, 2281, 2316, 13, 51214, 51214, 407, 264, 2281, 2316, 294, 264, 4319, 295, 460, 1770, 307, 1219, 257, 20828, 1639, 420, 2171, 257, 7850, 11, 51414, 51414, 457, 309, 311, 1936, 445, 588, 2531, 281, 364, 2281, 2316, 13, 51614, 51614, 400, 291, 3847, 309, 281, 747, 2295, 2281, 322, 264, 1412, 2793, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07300114101833767, "compression_ratio": 1.7962264150943397, "no_speech_prob": 1.95242500922177e-05}, {"id": 968, "seek": 567606, "start": 5676.06, "end": 5685.06, "text": " And then you train another neural net to generate contrastive data points, and you move their energy up.", "tokens": [50364, 400, 550, 291, 3847, 1071, 18161, 2533, 281, 8460, 8712, 488, 1412, 2793, 11, 293, 291, 1286, 641, 2281, 493, 13, 50814, 50814, 407, 264, 4787, 10686, 307, 746, 411, 341, 13, 51214, 51214, 509, 362, 257, 20828, 1639, 11, 293, 257, 20828, 1639, 534, 820, 312, 406, 10117, 341, 636, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11725706713540214, "compression_ratio": 1.5, "no_speech_prob": 1.2604466064658482e-05}, {"id": 969, "seek": 567606, "start": 5685.06, "end": 5693.06, "text": " So the overall diagram is something like this.", "tokens": [50364, 400, 550, 291, 3847, 1071, 18161, 2533, 281, 8460, 8712, 488, 1412, 2793, 11, 293, 291, 1286, 641, 2281, 493, 13, 50814, 50814, 407, 264, 4787, 10686, 307, 746, 411, 341, 13, 51214, 51214, 509, 362, 257, 20828, 1639, 11, 293, 257, 20828, 1639, 534, 820, 312, 406, 10117, 341, 636, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11725706713540214, "compression_ratio": 1.5, "no_speech_prob": 1.2604466064658482e-05}, {"id": 970, "seek": 567606, "start": 5693.06, "end": 5699.06, "text": " You have a discriminator, and a discriminator really should be not drawn this way.", "tokens": [50364, 400, 550, 291, 3847, 1071, 18161, 2533, 281, 8460, 8712, 488, 1412, 2793, 11, 293, 291, 1286, 641, 2281, 493, 13, 50814, 50814, 407, 264, 4787, 10686, 307, 746, 411, 341, 13, 51214, 51214, 509, 362, 257, 20828, 1639, 11, 293, 257, 20828, 1639, 534, 820, 312, 406, 10117, 341, 636, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11725706713540214, "compression_ratio": 1.5, "no_speech_prob": 1.2604466064658482e-05}, {"id": 971, "seek": 569906, "start": 5699.06, "end": 5716.06, "text": " It could be a large neural net, but in the end, it's just a cost function.", "tokens": [50364, 467, 727, 312, 257, 2416, 18161, 2533, 11, 457, 294, 264, 917, 11, 309, 311, 445, 257, 2063, 2445, 13, 51214, 51214, 407, 309, 2516, 257, 7006, 288, 11, 293, 309, 5112, 291, 309, 311, 665, 420, 1578, 13, 51564, 51564, 17078, 2281, 498, 309, 311, 665, 11, 1090, 2281, 498, 309, 311, 1578, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09213881573434603, "compression_ratio": 1.4230769230769231, "no_speech_prob": 6.8329031819303054e-06}, {"id": 972, "seek": 569906, "start": 5716.06, "end": 5723.06, "text": " So it takes a variable y, and it tells you it's good or bad.", "tokens": [50364, 467, 727, 312, 257, 2416, 18161, 2533, 11, 457, 294, 264, 917, 11, 309, 311, 445, 257, 2063, 2445, 13, 51214, 51214, 407, 309, 2516, 257, 7006, 288, 11, 293, 309, 5112, 291, 309, 311, 665, 420, 1578, 13, 51564, 51564, 17078, 2281, 498, 309, 311, 665, 11, 1090, 2281, 498, 309, 311, 1578, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09213881573434603, "compression_ratio": 1.4230769230769231, "no_speech_prob": 6.8329031819303054e-06}, {"id": 973, "seek": 569906, "start": 5723.06, "end": 5727.06, "text": " Low energy if it's good, high energy if it's bad.", "tokens": [50364, 467, 727, 312, 257, 2416, 18161, 2533, 11, 457, 294, 264, 917, 11, 309, 311, 445, 257, 2063, 2445, 13, 51214, 51214, 407, 309, 2516, 257, 7006, 288, 11, 293, 309, 5112, 291, 309, 311, 665, 420, 1578, 13, 51564, 51564, 17078, 2281, 498, 309, 311, 665, 11, 1090, 2281, 498, 309, 311, 1578, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09213881573434603, "compression_ratio": 1.4230769230769231, "no_speech_prob": 6.8329031819303054e-06}, {"id": 974, "seek": 572706, "start": 5727.06, "end": 5738.06, "text": " So in one phase, you collect a piece of data from your data set and just give it to your discriminator.", "tokens": [50364, 407, 294, 472, 5574, 11, 291, 2500, 257, 2522, 295, 1412, 490, 428, 1412, 992, 293, 445, 976, 309, 281, 428, 20828, 1639, 13, 50914, 50914, 407, 341, 307, 257, 957, 288, 1348, 490, 1412, 13, 51264, 51264, 663, 311, 257, 3097, 6889, 13, 51364, 51364, 400, 291, 584, 264, 5598, 295, 300, 820, 352, 760, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07914876937866211, "compression_ratio": 1.4172185430463575, "no_speech_prob": 5.09322262587375e-06}, {"id": 975, "seek": 572706, "start": 5738.06, "end": 5745.06, "text": " So this is a real y coming from data.", "tokens": [50364, 407, 294, 472, 5574, 11, 291, 2500, 257, 2522, 295, 1412, 490, 428, 1412, 992, 293, 445, 976, 309, 281, 428, 20828, 1639, 13, 50914, 50914, 407, 341, 307, 257, 957, 288, 1348, 490, 1412, 13, 51264, 51264, 663, 311, 257, 3097, 6889, 13, 51364, 51364, 400, 291, 584, 264, 5598, 295, 300, 820, 352, 760, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07914876937866211, "compression_ratio": 1.4172185430463575, "no_speech_prob": 5.09322262587375e-06}, {"id": 976, "seek": 572706, "start": 5745.06, "end": 5747.06, "text": " That's a training sample.", "tokens": [50364, 407, 294, 472, 5574, 11, 291, 2500, 257, 2522, 295, 1412, 490, 428, 1412, 992, 293, 445, 976, 309, 281, 428, 20828, 1639, 13, 50914, 50914, 407, 341, 307, 257, 957, 288, 1348, 490, 1412, 13, 51264, 51264, 663, 311, 257, 3097, 6889, 13, 51364, 51364, 400, 291, 584, 264, 5598, 295, 300, 820, 352, 760, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07914876937866211, "compression_ratio": 1.4172185430463575, "no_speech_prob": 5.09322262587375e-06}, {"id": 977, "seek": 572706, "start": 5747.06, "end": 5752.06, "text": " And you say the output of that should go down.", "tokens": [50364, 407, 294, 472, 5574, 11, 291, 2500, 257, 2522, 295, 1412, 490, 428, 1412, 992, 293, 445, 976, 309, 281, 428, 20828, 1639, 13, 50914, 50914, 407, 341, 307, 257, 957, 288, 1348, 490, 1412, 13, 51264, 51264, 663, 311, 257, 3097, 6889, 13, 51364, 51364, 400, 291, 584, 264, 5598, 295, 300, 820, 352, 760, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07914876937866211, "compression_ratio": 1.4172185430463575, "no_speech_prob": 5.09322262587375e-06}, {"id": 978, "seek": 575206, "start": 5752.06, "end": 5766.06, "text": " I should really write this as f, because after all, it's just an energy function.", "tokens": [50364, 286, 820, 534, 2464, 341, 382, 283, 11, 570, 934, 439, 11, 309, 311, 445, 364, 2281, 2445, 13, 51064, 51064, 407, 652, 283, 295, 288, 352, 760, 11, 295, 1164, 11, 538, 4473, 264, 9834, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1356824549233041, "compression_ratio": 1.2307692307692308, "no_speech_prob": 2.4409125671809306e-06}, {"id": 979, "seek": 575206, "start": 5766.06, "end": 5775.06, "text": " So make f of y go down, of course, by changing the parameters.", "tokens": [50364, 286, 820, 534, 2464, 341, 382, 283, 11, 570, 934, 439, 11, 309, 311, 445, 364, 2281, 2445, 13, 51064, 51064, 407, 652, 283, 295, 288, 352, 760, 11, 295, 1164, 11, 538, 4473, 264, 9834, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1356824549233041, "compression_ratio": 1.2307692307692308, "no_speech_prob": 2.4409125671809306e-06}, {"id": 980, "seek": 577506, "start": 5775.06, "end": 5782.06, "text": " So you do w, replaced by w minus eta df.", "tokens": [50364, 407, 291, 360, 261, 11, 10772, 538, 261, 3175, 32415, 274, 69, 13, 50714, 50714, 407, 283, 307, 257, 18161, 2533, 13, 50814, 50814, 283, 307, 257, 18161, 2533, 13, 50914, 50914, 2188, 13075, 1602, 2445, 11, 457, 1391, 257, 1238, 6179, 18161, 2533, 13, 51214, 51214, 663, 311, 264, 700, 551, 13, 51364, 51364, 400, 300, 486, 652, 264, 2281, 295, 1412, 2793, 1359, 13, 51564, 51564, 823, 11, 456, 311, 257, 1254, 295, 341, 300, 311, 27708, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1435240109761556, "compression_ratio": 1.4973262032085561, "no_speech_prob": 3.480466330074705e-05}, {"id": 981, "seek": 577506, "start": 5782.06, "end": 5784.06, "text": " So f is a neural net.", "tokens": [50364, 407, 291, 360, 261, 11, 10772, 538, 261, 3175, 32415, 274, 69, 13, 50714, 50714, 407, 283, 307, 257, 18161, 2533, 13, 50814, 50814, 283, 307, 257, 18161, 2533, 13, 50914, 50914, 2188, 13075, 1602, 2445, 11, 457, 1391, 257, 1238, 6179, 18161, 2533, 13, 51214, 51214, 663, 311, 264, 700, 551, 13, 51364, 51364, 400, 300, 486, 652, 264, 2281, 295, 1412, 2793, 1359, 13, 51564, 51564, 823, 11, 456, 311, 257, 1254, 295, 341, 300, 311, 27708, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1435240109761556, "compression_ratio": 1.4973262032085561, "no_speech_prob": 3.480466330074705e-05}, {"id": 982, "seek": 577506, "start": 5784.06, "end": 5786.06, "text": " f is a neural net.", "tokens": [50364, 407, 291, 360, 261, 11, 10772, 538, 261, 3175, 32415, 274, 69, 13, 50714, 50714, 407, 283, 307, 257, 18161, 2533, 13, 50814, 50814, 283, 307, 257, 18161, 2533, 13, 50914, 50914, 2188, 13075, 1602, 2445, 11, 457, 1391, 257, 1238, 6179, 18161, 2533, 13, 51214, 51214, 663, 311, 264, 700, 551, 13, 51364, 51364, 400, 300, 486, 652, 264, 2281, 295, 1412, 2793, 1359, 13, 51564, 51564, 823, 11, 456, 311, 257, 1254, 295, 341, 300, 311, 27708, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1435240109761556, "compression_ratio": 1.4973262032085561, "no_speech_prob": 3.480466330074705e-05}, {"id": 983, "seek": 577506, "start": 5786.06, "end": 5792.06, "text": " Some parameterized function, but probably a pretty complicated neural net.", "tokens": [50364, 407, 291, 360, 261, 11, 10772, 538, 261, 3175, 32415, 274, 69, 13, 50714, 50714, 407, 283, 307, 257, 18161, 2533, 13, 50814, 50814, 283, 307, 257, 18161, 2533, 13, 50914, 50914, 2188, 13075, 1602, 2445, 11, 457, 1391, 257, 1238, 6179, 18161, 2533, 13, 51214, 51214, 663, 311, 264, 700, 551, 13, 51364, 51364, 400, 300, 486, 652, 264, 2281, 295, 1412, 2793, 1359, 13, 51564, 51564, 823, 11, 456, 311, 257, 1254, 295, 341, 300, 311, 27708, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1435240109761556, "compression_ratio": 1.4973262032085561, "no_speech_prob": 3.480466330074705e-05}, {"id": 984, "seek": 577506, "start": 5792.06, "end": 5795.06, "text": " That's the first thing.", "tokens": [50364, 407, 291, 360, 261, 11, 10772, 538, 261, 3175, 32415, 274, 69, 13, 50714, 50714, 407, 283, 307, 257, 18161, 2533, 13, 50814, 50814, 283, 307, 257, 18161, 2533, 13, 50914, 50914, 2188, 13075, 1602, 2445, 11, 457, 1391, 257, 1238, 6179, 18161, 2533, 13, 51214, 51214, 663, 311, 264, 700, 551, 13, 51364, 51364, 400, 300, 486, 652, 264, 2281, 295, 1412, 2793, 1359, 13, 51564, 51564, 823, 11, 456, 311, 257, 1254, 295, 341, 300, 311, 27708, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1435240109761556, "compression_ratio": 1.4973262032085561, "no_speech_prob": 3.480466330074705e-05}, {"id": 985, "seek": 577506, "start": 5795.06, "end": 5799.06, "text": " And that will make the energy of data points small.", "tokens": [50364, 407, 291, 360, 261, 11, 10772, 538, 261, 3175, 32415, 274, 69, 13, 50714, 50714, 407, 283, 307, 257, 18161, 2533, 13, 50814, 50814, 283, 307, 257, 18161, 2533, 13, 50914, 50914, 2188, 13075, 1602, 2445, 11, 457, 1391, 257, 1238, 6179, 18161, 2533, 13, 51214, 51214, 663, 311, 264, 700, 551, 13, 51364, 51364, 400, 300, 486, 652, 264, 2281, 295, 1412, 2793, 1359, 13, 51564, 51564, 823, 11, 456, 311, 257, 1254, 295, 341, 300, 311, 27708, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1435240109761556, "compression_ratio": 1.4973262032085561, "no_speech_prob": 3.480466330074705e-05}, {"id": 986, "seek": 577506, "start": 5799.06, "end": 5801.06, "text": " Now, there's a form of this that's conditional.", "tokens": [50364, 407, 291, 360, 261, 11, 10772, 538, 261, 3175, 32415, 274, 69, 13, 50714, 50714, 407, 283, 307, 257, 18161, 2533, 13, 50814, 50814, 283, 307, 257, 18161, 2533, 13, 50914, 50914, 2188, 13075, 1602, 2445, 11, 457, 1391, 257, 1238, 6179, 18161, 2533, 13, 51214, 51214, 663, 311, 264, 700, 551, 13, 51364, 51364, 400, 300, 486, 652, 264, 2281, 295, 1412, 2793, 1359, 13, 51564, 51564, 823, 11, 456, 311, 257, 1254, 295, 341, 300, 311, 27708, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1435240109761556, "compression_ratio": 1.4973262032085561, "no_speech_prob": 3.480466330074705e-05}, {"id": 987, "seek": 580106, "start": 5801.06, "end": 5807.06, "text": " So the form of this is conditional, you have an extra input here, which is an observation.", "tokens": [50364, 407, 264, 1254, 295, 341, 307, 27708, 11, 291, 362, 364, 2857, 4846, 510, 11, 597, 307, 364, 14816, 13, 50664, 50664, 583, 291, 393, 362, 341, 420, 406, 13, 50764, 50764, 663, 311, 1219, 27708, 797, 13, 50814, 50814, 467, 1177, 380, 1871, 13, 50914, 50914, 2264, 11, 1150, 5574, 11, 337, 8712, 488, 10938, 11, 291, 362, 257, 48994, 7006, 710, 300, 291, 6889, 490, 512, 7316, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1351049010818069, "compression_ratio": 1.518716577540107, "no_speech_prob": 1.0450695299368817e-05}, {"id": 988, "seek": 580106, "start": 5807.06, "end": 5809.06, "text": " But you can have this or not.", "tokens": [50364, 407, 264, 1254, 295, 341, 307, 27708, 11, 291, 362, 364, 2857, 4846, 510, 11, 597, 307, 364, 14816, 13, 50664, 50664, 583, 291, 393, 362, 341, 420, 406, 13, 50764, 50764, 663, 311, 1219, 27708, 797, 13, 50814, 50814, 467, 1177, 380, 1871, 13, 50914, 50914, 2264, 11, 1150, 5574, 11, 337, 8712, 488, 10938, 11, 291, 362, 257, 48994, 7006, 710, 300, 291, 6889, 490, 512, 7316, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1351049010818069, "compression_ratio": 1.518716577540107, "no_speech_prob": 1.0450695299368817e-05}, {"id": 989, "seek": 580106, "start": 5809.06, "end": 5810.06, "text": " That's called conditional again.", "tokens": [50364, 407, 264, 1254, 295, 341, 307, 27708, 11, 291, 362, 364, 2857, 4846, 510, 11, 597, 307, 364, 14816, 13, 50664, 50664, 583, 291, 393, 362, 341, 420, 406, 13, 50764, 50764, 663, 311, 1219, 27708, 797, 13, 50814, 50814, 467, 1177, 380, 1871, 13, 50914, 50914, 2264, 11, 1150, 5574, 11, 337, 8712, 488, 10938, 11, 291, 362, 257, 48994, 7006, 710, 300, 291, 6889, 490, 512, 7316, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1351049010818069, "compression_ratio": 1.518716577540107, "no_speech_prob": 1.0450695299368817e-05}, {"id": 990, "seek": 580106, "start": 5810.06, "end": 5812.06, "text": " It doesn't matter.", "tokens": [50364, 407, 264, 1254, 295, 341, 307, 27708, 11, 291, 362, 364, 2857, 4846, 510, 11, 597, 307, 364, 14816, 13, 50664, 50664, 583, 291, 393, 362, 341, 420, 406, 13, 50764, 50764, 663, 311, 1219, 27708, 797, 13, 50814, 50814, 467, 1177, 380, 1871, 13, 50914, 50914, 2264, 11, 1150, 5574, 11, 337, 8712, 488, 10938, 11, 291, 362, 257, 48994, 7006, 710, 300, 291, 6889, 490, 512, 7316, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1351049010818069, "compression_ratio": 1.518716577540107, "no_speech_prob": 1.0450695299368817e-05}, {"id": 991, "seek": 580106, "start": 5812.06, "end": 5823.06, "text": " OK, second phase, for contrastive samples, you have a latent variable z that you sample from some distribution,", "tokens": [50364, 407, 264, 1254, 295, 341, 307, 27708, 11, 291, 362, 364, 2857, 4846, 510, 11, 597, 307, 364, 14816, 13, 50664, 50664, 583, 291, 393, 362, 341, 420, 406, 13, 50764, 50764, 663, 311, 1219, 27708, 797, 13, 50814, 50814, 467, 1177, 380, 1871, 13, 50914, 50914, 2264, 11, 1150, 5574, 11, 337, 8712, 488, 10938, 11, 291, 362, 257, 48994, 7006, 710, 300, 291, 6889, 490, 512, 7316, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.1351049010818069, "compression_ratio": 1.518716577540107, "no_speech_prob": 1.0450695299368817e-05}, {"id": 992, "seek": 582306, "start": 5823.06, "end": 5831.06, "text": " a distribution that's easy to sample from, let's say Gaussian, multivariate Gaussian, or uniform or something.", "tokens": [50364, 257, 7316, 300, 311, 1858, 281, 6889, 490, 11, 718, 311, 584, 39148, 11, 2120, 592, 3504, 473, 39148, 11, 420, 9452, 420, 746, 13, 50764, 50764, 509, 1190, 341, 807, 437, 311, 1219, 257, 19265, 13, 51014, 51014, 407, 341, 307, 257, 18161, 2533, 13, 51064, 51064, 400, 300, 18161, 2533, 14725, 746, 2531, 281, 288, 13, 51364, 51364, 467, 445, 14725, 364, 3256, 11, 718, 311, 584, 11, 337, 16486, 5267, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10039089887570112, "compression_ratio": 1.6145251396648044, "no_speech_prob": 4.153129339101724e-06}, {"id": 993, "seek": 582306, "start": 5831.06, "end": 5836.06, "text": " You run this through what's called a generator.", "tokens": [50364, 257, 7316, 300, 311, 1858, 281, 6889, 490, 11, 718, 311, 584, 39148, 11, 2120, 592, 3504, 473, 39148, 11, 420, 9452, 420, 746, 13, 50764, 50764, 509, 1190, 341, 807, 437, 311, 1219, 257, 19265, 13, 51014, 51014, 407, 341, 307, 257, 18161, 2533, 13, 51064, 51064, 400, 300, 18161, 2533, 14725, 746, 2531, 281, 288, 13, 51364, 51364, 467, 445, 14725, 364, 3256, 11, 718, 311, 584, 11, 337, 16486, 5267, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10039089887570112, "compression_ratio": 1.6145251396648044, "no_speech_prob": 4.153129339101724e-06}, {"id": 994, "seek": 582306, "start": 5836.06, "end": 5837.06, "text": " So this is a neural net.", "tokens": [50364, 257, 7316, 300, 311, 1858, 281, 6889, 490, 11, 718, 311, 584, 39148, 11, 2120, 592, 3504, 473, 39148, 11, 420, 9452, 420, 746, 13, 50764, 50764, 509, 1190, 341, 807, 437, 311, 1219, 257, 19265, 13, 51014, 51014, 407, 341, 307, 257, 18161, 2533, 13, 51064, 51064, 400, 300, 18161, 2533, 14725, 746, 2531, 281, 288, 13, 51364, 51364, 467, 445, 14725, 364, 3256, 11, 718, 311, 584, 11, 337, 16486, 5267, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10039089887570112, "compression_ratio": 1.6145251396648044, "no_speech_prob": 4.153129339101724e-06}, {"id": 995, "seek": 582306, "start": 5837.06, "end": 5843.06, "text": " And that neural net produces something similar to y.", "tokens": [50364, 257, 7316, 300, 311, 1858, 281, 6889, 490, 11, 718, 311, 584, 39148, 11, 2120, 592, 3504, 473, 39148, 11, 420, 9452, 420, 746, 13, 50764, 50764, 509, 1190, 341, 807, 437, 311, 1219, 257, 19265, 13, 51014, 51014, 407, 341, 307, 257, 18161, 2533, 13, 51064, 51064, 400, 300, 18161, 2533, 14725, 746, 2531, 281, 288, 13, 51364, 51364, 467, 445, 14725, 364, 3256, 11, 718, 311, 584, 11, 337, 16486, 5267, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10039089887570112, "compression_ratio": 1.6145251396648044, "no_speech_prob": 4.153129339101724e-06}, {"id": 996, "seek": 582306, "start": 5843.06, "end": 5849.06, "text": " It just produces an image, let's say, for IR images.", "tokens": [50364, 257, 7316, 300, 311, 1858, 281, 6889, 490, 11, 718, 311, 584, 39148, 11, 2120, 592, 3504, 473, 39148, 11, 420, 9452, 420, 746, 13, 50764, 50764, 509, 1190, 341, 807, 437, 311, 1219, 257, 19265, 13, 51014, 51014, 407, 341, 307, 257, 18161, 2533, 13, 51064, 51064, 400, 300, 18161, 2533, 14725, 746, 2531, 281, 288, 13, 51364, 51364, 467, 445, 14725, 364, 3256, 11, 718, 311, 584, 11, 337, 16486, 5267, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10039089887570112, "compression_ratio": 1.6145251396648044, "no_speech_prob": 4.153129339101724e-06}, {"id": 997, "seek": 584906, "start": 5849.06, "end": 5857.06, "text": " And again, you run this through your discriminator.", "tokens": [50364, 400, 797, 11, 291, 1190, 341, 807, 428, 20828, 1639, 13, 50764, 50764, 583, 586, 291, 528, 281, 652, 300, 2416, 13, 51014, 51014, 2264, 11, 370, 294, 1186, 11, 437, 286, 1907, 291, 949, 307, 257, 4544, 13, 51314, 51314, 509, 500, 380, 360, 341, 5623, 411, 300, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08584361606174046, "compression_ratio": 1.300751879699248, "no_speech_prob": 1.4733344869455323e-05}, {"id": 998, "seek": 584906, "start": 5857.06, "end": 5862.06, "text": " But now you want to make that large.", "tokens": [50364, 400, 797, 11, 291, 1190, 341, 807, 428, 20828, 1639, 13, 50764, 50764, 583, 586, 291, 528, 281, 652, 300, 2416, 13, 51014, 51014, 2264, 11, 370, 294, 1186, 11, 437, 286, 1907, 291, 949, 307, 257, 4544, 13, 51314, 51314, 509, 500, 380, 360, 341, 5623, 411, 300, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08584361606174046, "compression_ratio": 1.300751879699248, "no_speech_prob": 1.4733344869455323e-05}, {"id": 999, "seek": 584906, "start": 5862.06, "end": 5868.06, "text": " OK, so in fact, what I told you before is a lie.", "tokens": [50364, 400, 797, 11, 291, 1190, 341, 807, 428, 20828, 1639, 13, 50764, 50764, 583, 586, 291, 528, 281, 652, 300, 2416, 13, 51014, 51014, 2264, 11, 370, 294, 1186, 11, 437, 286, 1907, 291, 949, 307, 257, 4544, 13, 51314, 51314, 509, 500, 380, 360, 341, 5623, 411, 300, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08584361606174046, "compression_ratio": 1.300751879699248, "no_speech_prob": 1.4733344869455323e-05}, {"id": 1000, "seek": 584906, "start": 5868.06, "end": 5874.06, "text": " You don't do this update like that.", "tokens": [50364, 400, 797, 11, 291, 1190, 341, 807, 428, 20828, 1639, 13, 50764, 50764, 583, 586, 291, 528, 281, 652, 300, 2416, 13, 51014, 51014, 2264, 11, 370, 294, 1186, 11, 437, 286, 1907, 291, 949, 307, 257, 4544, 13, 51314, 51314, 509, 500, 380, 360, 341, 5623, 411, 300, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08584361606174046, "compression_ratio": 1.300751879699248, "no_speech_prob": 1.4733344869455323e-05}, {"id": 1001, "seek": 587406, "start": 5874.06, "end": 5884.06, "text": " OK, but here what you want is you want to make FW of this y bar high.", "tokens": [50364, 2264, 11, 457, 510, 437, 291, 528, 307, 291, 528, 281, 652, 479, 54, 295, 341, 288, 2159, 1090, 13, 50864, 50864, 2264, 13, 51114, 51114, 400, 437, 291, 434, 516, 281, 360, 586, 307, 3847, 264, 20828, 1639, 293, 264, 19265, 16561, 13, 51464, 51464, 407, 291, 434, 516, 281, 700, 362, 281, 808, 493, 365, 257, 2063, 2445, 11, 257, 4470, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08562305008155713, "compression_ratio": 1.50920245398773, "no_speech_prob": 2.026026322710095e-06}, {"id": 1002, "seek": 587406, "start": 5884.06, "end": 5889.06, "text": " OK.", "tokens": [50364, 2264, 11, 457, 510, 437, 291, 528, 307, 291, 528, 281, 652, 479, 54, 295, 341, 288, 2159, 1090, 13, 50864, 50864, 2264, 13, 51114, 51114, 400, 437, 291, 434, 516, 281, 360, 586, 307, 3847, 264, 20828, 1639, 293, 264, 19265, 16561, 13, 51464, 51464, 407, 291, 434, 516, 281, 700, 362, 281, 808, 493, 365, 257, 2063, 2445, 11, 257, 4470, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08562305008155713, "compression_ratio": 1.50920245398773, "no_speech_prob": 2.026026322710095e-06}, {"id": 1003, "seek": 587406, "start": 5889.06, "end": 5896.06, "text": " And what you're going to do now is train the discriminator and the generator simultaneously.", "tokens": [50364, 2264, 11, 457, 510, 437, 291, 528, 307, 291, 528, 281, 652, 479, 54, 295, 341, 288, 2159, 1090, 13, 50864, 50864, 2264, 13, 51114, 51114, 400, 437, 291, 434, 516, 281, 360, 586, 307, 3847, 264, 20828, 1639, 293, 264, 19265, 16561, 13, 51464, 51464, 407, 291, 434, 516, 281, 700, 362, 281, 808, 493, 365, 257, 2063, 2445, 11, 257, 4470, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08562305008155713, "compression_ratio": 1.50920245398773, "no_speech_prob": 2.026026322710095e-06}, {"id": 1004, "seek": 587406, "start": 5896.06, "end": 5902.06, "text": " So you're going to first have to come up with a cost function, a loss function.", "tokens": [50364, 2264, 11, 457, 510, 437, 291, 528, 307, 291, 528, 281, 652, 479, 54, 295, 341, 288, 2159, 1090, 13, 50864, 50864, 2264, 13, 51114, 51114, 400, 437, 291, 434, 516, 281, 360, 586, 307, 3847, 264, 20828, 1639, 293, 264, 19265, 16561, 13, 51464, 51464, 407, 291, 434, 516, 281, 700, 362, 281, 808, 493, 365, 257, 2063, 2445, 11, 257, 4470, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08562305008155713, "compression_ratio": 1.50920245398773, "no_speech_prob": 2.026026322710095e-06}, {"id": 1005, "seek": 590206, "start": 5902.06, "end": 5927.06, "text": " And this loss function is going to be some over sample of a per sample loss function that basically is a function of F of y and F of y bar.", "tokens": [50364, 400, 341, 4470, 2445, 307, 516, 281, 312, 512, 670, 6889, 295, 257, 680, 6889, 4470, 2445, 300, 1936, 307, 257, 2445, 295, 479, 295, 288, 293, 479, 295, 288, 2159, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1712631516986423, "compression_ratio": 1.4479166666666667, "no_speech_prob": 3.7262445857777493e-06}, {"id": 1006, "seek": 592706, "start": 5927.06, "end": 5932.06, "text": " Y bar, of course, is generated from the randomly sample latent variable z.", "tokens": [50364, 398, 2159, 11, 295, 1164, 11, 307, 10833, 490, 264, 16979, 6889, 48994, 7006, 710, 13, 50614, 50614, 823, 11, 341, 2063, 2445, 2203, 281, 312, 257, 23223, 2445, 295, 479, 295, 288, 293, 364, 5662, 2445, 295, 479, 295, 288, 2159, 13, 51064, 51064, 2264, 11, 291, 393, 764, 445, 466, 604, 2063, 2445, 291, 528, 382, 938, 382, 309, 1669, 479, 295, 288, 11514, 293, 309, 1669, 479, 295, 288, 2159, 3488, 13, 51464, 51464, 1610, 382, 938, 382, 309, 1669, 257, 2649, 11514, 11, 479, 295, 288, 3175, 479, 295, 288, 2159, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09565228462219239, "compression_ratio": 1.895, "no_speech_prob": 2.5609895146772033e-06}, {"id": 1007, "seek": 592706, "start": 5932.06, "end": 5941.06, "text": " Now, this cost function needs to be a decreasing function of F of y and an increasing function of F of y bar.", "tokens": [50364, 398, 2159, 11, 295, 1164, 11, 307, 10833, 490, 264, 16979, 6889, 48994, 7006, 710, 13, 50614, 50614, 823, 11, 341, 2063, 2445, 2203, 281, 312, 257, 23223, 2445, 295, 479, 295, 288, 293, 364, 5662, 2445, 295, 479, 295, 288, 2159, 13, 51064, 51064, 2264, 11, 291, 393, 764, 445, 466, 604, 2063, 2445, 291, 528, 382, 938, 382, 309, 1669, 479, 295, 288, 11514, 293, 309, 1669, 479, 295, 288, 2159, 3488, 13, 51464, 51464, 1610, 382, 938, 382, 309, 1669, 257, 2649, 11514, 11, 479, 295, 288, 3175, 479, 295, 288, 2159, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09565228462219239, "compression_ratio": 1.895, "no_speech_prob": 2.5609895146772033e-06}, {"id": 1008, "seek": 592706, "start": 5941.06, "end": 5949.06, "text": " OK, you can use just about any cost function you want as long as it makes F of y decrease and it makes F of y bar increase.", "tokens": [50364, 398, 2159, 11, 295, 1164, 11, 307, 10833, 490, 264, 16979, 6889, 48994, 7006, 710, 13, 50614, 50614, 823, 11, 341, 2063, 2445, 2203, 281, 312, 257, 23223, 2445, 295, 479, 295, 288, 293, 364, 5662, 2445, 295, 479, 295, 288, 2159, 13, 51064, 51064, 2264, 11, 291, 393, 764, 445, 466, 604, 2063, 2445, 291, 528, 382, 938, 382, 309, 1669, 479, 295, 288, 11514, 293, 309, 1669, 479, 295, 288, 2159, 3488, 13, 51464, 51464, 1610, 382, 938, 382, 309, 1669, 257, 2649, 11514, 11, 479, 295, 288, 3175, 479, 295, 288, 2159, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09565228462219239, "compression_ratio": 1.895, "no_speech_prob": 2.5609895146772033e-06}, {"id": 1009, "seek": 592706, "start": 5949.06, "end": 5955.06, "text": " Or as long as it makes a difference decrease, F of y minus F of y bar.", "tokens": [50364, 398, 2159, 11, 295, 1164, 11, 307, 10833, 490, 264, 16979, 6889, 48994, 7006, 710, 13, 50614, 50614, 823, 11, 341, 2063, 2445, 2203, 281, 312, 257, 23223, 2445, 295, 479, 295, 288, 293, 364, 5662, 2445, 295, 479, 295, 288, 2159, 13, 51064, 51064, 2264, 11, 291, 393, 764, 445, 466, 604, 2063, 2445, 291, 528, 382, 938, 382, 309, 1669, 479, 295, 288, 11514, 293, 309, 1669, 479, 295, 288, 2159, 3488, 13, 51464, 51464, 1610, 382, 938, 382, 309, 1669, 257, 2649, 11514, 11, 479, 295, 288, 3175, 479, 295, 288, 2159, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09565228462219239, "compression_ratio": 1.895, "no_speech_prob": 2.5609895146772033e-06}, {"id": 1010, "seek": 595506, "start": 5955.06, "end": 5960.06, "text": " Good example of this would be kind of a hinge loss, for example.", "tokens": [50364, 2205, 1365, 295, 341, 576, 312, 733, 295, 257, 28822, 4470, 11, 337, 1365, 13, 50614, 50614, 2264, 11, 370, 746, 300, 1619, 452, 4470, 2445, 307, 516, 281, 312, 479, 295, 288, 1804, 512, 10270, 3175, 479, 295, 288, 2159, 3353, 644, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10716324051221211, "compression_ratio": 1.376923076923077, "no_speech_prob": 5.421876721811714e-06}, {"id": 1011, "seek": 595506, "start": 5960.06, "end": 5981.06, "text": " OK, so something that says my loss function is going to be F of y plus some margin minus F of y bar positive part.", "tokens": [50364, 2205, 1365, 295, 341, 576, 312, 733, 295, 257, 28822, 4470, 11, 337, 1365, 13, 50614, 50614, 2264, 11, 370, 746, 300, 1619, 452, 4470, 2445, 307, 516, 281, 312, 479, 295, 288, 1804, 512, 10270, 3175, 479, 295, 288, 2159, 3353, 644, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10716324051221211, "compression_ratio": 1.376923076923077, "no_speech_prob": 5.421876721811714e-06}, {"id": 1012, "seek": 598106, "start": 5981.06, "end": 5986.06, "text": " OK, so this is a hinge.", "tokens": [50364, 2264, 11, 370, 341, 307, 257, 28822, 13, 50614, 50614, 400, 309, 1619, 286, 528, 281, 652, 479, 295, 288, 2159, 4356, 813, 275, 13, 51064, 51064, 5358, 813, 300, 11, 286, 500, 380, 1127, 13, 51264, 51264, 5429, 1321, 813, 275, 11, 286, 478, 2597, 13, 286, 12804, 341, 12204, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14007574319839478, "compression_ratio": 1.256, "no_speech_prob": 2.9473847007466247e-06}, {"id": 1013, "seek": 598106, "start": 5986.06, "end": 5995.06, "text": " And it says I want to make F of y bar smaller than m.", "tokens": [50364, 2264, 11, 370, 341, 307, 257, 28822, 13, 50614, 50614, 400, 309, 1619, 286, 528, 281, 652, 479, 295, 288, 2159, 4356, 813, 275, 13, 51064, 51064, 5358, 813, 300, 11, 286, 500, 380, 1127, 13, 51264, 51264, 5429, 1321, 813, 275, 11, 286, 478, 2597, 13, 286, 12804, 341, 12204, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14007574319839478, "compression_ratio": 1.256, "no_speech_prob": 2.9473847007466247e-06}, {"id": 1014, "seek": 598106, "start": 5995.06, "end": 5999.06, "text": " Other than that, I don't care.", "tokens": [50364, 2264, 11, 370, 341, 307, 257, 28822, 13, 50614, 50614, 400, 309, 1619, 286, 528, 281, 652, 479, 295, 288, 2159, 4356, 813, 275, 13, 51064, 51064, 5358, 813, 300, 11, 286, 500, 380, 1127, 13, 51264, 51264, 5429, 1321, 813, 275, 11, 286, 478, 2597, 13, 286, 12804, 341, 12204, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14007574319839478, "compression_ratio": 1.256, "no_speech_prob": 2.9473847007466247e-06}, {"id": 1015, "seek": 598106, "start": 5999.06, "end": 6009.06, "text": " Bigger than m, I'm sorry. I drew this backwards.", "tokens": [50364, 2264, 11, 370, 341, 307, 257, 28822, 13, 50614, 50614, 400, 309, 1619, 286, 528, 281, 652, 479, 295, 288, 2159, 4356, 813, 275, 13, 51064, 51064, 5358, 813, 300, 11, 286, 500, 380, 1127, 13, 51264, 51264, 5429, 1321, 813, 275, 11, 286, 478, 2597, 13, 286, 12804, 341, 12204, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14007574319839478, "compression_ratio": 1.256, "no_speech_prob": 2.9473847007466247e-06}, {"id": 1016, "seek": 600906, "start": 6009.06, "end": 6016.06, "text": " So overall, as a function of F of y bar, this function looks like this.", "tokens": [50364, 407, 4787, 11, 382, 257, 2445, 295, 479, 295, 288, 2159, 11, 341, 2445, 1542, 411, 341, 13, 50714, 50714, 407, 309, 2738, 281, 652, 479, 295, 288, 2159, 4833, 813, 275, 13, 51014, 51014, 2264, 11, 370, 300, 311, 364, 1365, 13, 51114, 51114, 440, 3539, 2063, 2445, 300, 881, 264, 3380, 37642, 295, 460, 1770, 82, 1143, 1936, 33899, 1184, 295, 729, 2115, 666, 257, 4556, 3280, 327, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0816497802734375, "compression_ratio": 1.467032967032967, "no_speech_prob": 9.817039426707197e-06}, {"id": 1017, "seek": 600906, "start": 6016.06, "end": 6022.06, "text": " So it wants to make F of y bar larger than m.", "tokens": [50364, 407, 4787, 11, 382, 257, 2445, 295, 479, 295, 288, 2159, 11, 341, 2445, 1542, 411, 341, 13, 50714, 50714, 407, 309, 2738, 281, 652, 479, 295, 288, 2159, 4833, 813, 275, 13, 51014, 51014, 2264, 11, 370, 300, 311, 364, 1365, 13, 51114, 51114, 440, 3539, 2063, 2445, 300, 881, 264, 3380, 37642, 295, 460, 1770, 82, 1143, 1936, 33899, 1184, 295, 729, 2115, 666, 257, 4556, 3280, 327, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0816497802734375, "compression_ratio": 1.467032967032967, "no_speech_prob": 9.817039426707197e-06}, {"id": 1018, "seek": 600906, "start": 6022.06, "end": 6024.06, "text": " OK, so that's an example.", "tokens": [50364, 407, 4787, 11, 382, 257, 2445, 295, 479, 295, 288, 2159, 11, 341, 2445, 1542, 411, 341, 13, 50714, 50714, 407, 309, 2738, 281, 652, 479, 295, 288, 2159, 4833, 813, 275, 13, 51014, 51014, 2264, 11, 370, 300, 311, 364, 1365, 13, 51114, 51114, 440, 3539, 2063, 2445, 300, 881, 264, 3380, 37642, 295, 460, 1770, 82, 1143, 1936, 33899, 1184, 295, 729, 2115, 666, 257, 4556, 3280, 327, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0816497802734375, "compression_ratio": 1.467032967032967, "no_speech_prob": 9.817039426707197e-06}, {"id": 1019, "seek": 600906, "start": 6024.06, "end": 6032.06, "text": " The actual cost function that most the original formulation of GANs used basically plugs each of those terms into a sigmoid", "tokens": [50364, 407, 4787, 11, 382, 257, 2445, 295, 479, 295, 288, 2159, 11, 341, 2445, 1542, 411, 341, 13, 50714, 50714, 407, 309, 2738, 281, 652, 479, 295, 288, 2159, 4833, 813, 275, 13, 51014, 51014, 2264, 11, 370, 300, 311, 364, 1365, 13, 51114, 51114, 440, 3539, 2063, 2445, 300, 881, 264, 3380, 37642, 295, 460, 1770, 82, 1143, 1936, 33899, 1184, 295, 729, 2115, 666, 257, 4556, 3280, 327, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0816497802734375, "compression_ratio": 1.467032967032967, "no_speech_prob": 9.817039426707197e-06}, {"id": 1020, "seek": 603206, "start": 6032.06, "end": 6042.06, "text": " and tries to make the sigmoid apply to F of y as close to 1 as possible and the sigmoid apply to F of y bar as close to 0 as possible.", "tokens": [50364, 293, 9898, 281, 652, 264, 4556, 3280, 327, 3079, 281, 479, 295, 288, 382, 1998, 281, 502, 382, 1944, 293, 264, 4556, 3280, 327, 3079, 281, 479, 295, 288, 2159, 382, 1998, 281, 1958, 382, 1944, 13, 50864, 50864, 467, 311, 1936, 300, 13, 6693, 544, 813, 300, 13, 51014, 51014, 407, 309, 311, 4556, 3280, 327, 295, 479, 295, 288, 1804, 502, 3175, 4556, 3280, 327, 295, 479, 295, 288, 2159, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07672869075428355, "compression_ratio": 1.7463768115942029, "no_speech_prob": 1.5205600902845617e-05}, {"id": 1021, "seek": 603206, "start": 6042.06, "end": 6045.06, "text": " It's basically that. Nothing more than that.", "tokens": [50364, 293, 9898, 281, 652, 264, 4556, 3280, 327, 3079, 281, 479, 295, 288, 382, 1998, 281, 502, 382, 1944, 293, 264, 4556, 3280, 327, 3079, 281, 479, 295, 288, 2159, 382, 1998, 281, 1958, 382, 1944, 13, 50864, 50864, 467, 311, 1936, 300, 13, 6693, 544, 813, 300, 13, 51014, 51014, 407, 309, 311, 4556, 3280, 327, 295, 479, 295, 288, 1804, 502, 3175, 4556, 3280, 327, 295, 479, 295, 288, 2159, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07672869075428355, "compression_ratio": 1.7463768115942029, "no_speech_prob": 1.5205600902845617e-05}, {"id": 1022, "seek": 603206, "start": 6045.06, "end": 6056.06, "text": " So it's sigmoid of F of y plus 1 minus sigmoid of F of y bar.", "tokens": [50364, 293, 9898, 281, 652, 264, 4556, 3280, 327, 3079, 281, 479, 295, 288, 382, 1998, 281, 502, 382, 1944, 293, 264, 4556, 3280, 327, 3079, 281, 479, 295, 288, 2159, 382, 1998, 281, 1958, 382, 1944, 13, 50864, 50864, 467, 311, 1936, 300, 13, 6693, 544, 813, 300, 13, 51014, 51014, 407, 309, 311, 4556, 3280, 327, 295, 479, 295, 288, 1804, 502, 3175, 4556, 3280, 327, 295, 479, 295, 288, 2159, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07672869075428355, "compression_ratio": 1.7463768115942029, "no_speech_prob": 1.5205600902845617e-05}, {"id": 1023, "seek": 605606, "start": 6056.06, "end": 6065.06, "text": " And you take logs because this is not the last function. This is kind of what goes before the last function.", "tokens": [50364, 400, 291, 747, 20820, 570, 341, 307, 406, 264, 1036, 2445, 13, 639, 307, 733, 295, 437, 1709, 949, 264, 1036, 2445, 13, 50814, 50814, 407, 341, 307, 733, 295, 411, 257, 3278, 30867, 13, 583, 291, 362, 3278, 30867, 300, 311, 3353, 337, 264, 3353, 5574, 13, 51164, 51164, 400, 370, 264, 3779, 307, 3671, 337, 264, 3671, 5574, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13222044431246244, "compression_ratio": 1.7763157894736843, "no_speech_prob": 9.305423191108275e-06}, {"id": 1024, "seek": 605606, "start": 6065.06, "end": 6072.06, "text": " So this is kind of like a cross entropy. But you have cross entropy that's positive for the positive phase.", "tokens": [50364, 400, 291, 747, 20820, 570, 341, 307, 406, 264, 1036, 2445, 13, 639, 307, 733, 295, 437, 1709, 949, 264, 1036, 2445, 13, 50814, 50814, 407, 341, 307, 733, 295, 411, 257, 3278, 30867, 13, 583, 291, 362, 3278, 30867, 300, 311, 3353, 337, 264, 3353, 5574, 13, 51164, 51164, 400, 370, 264, 3779, 307, 3671, 337, 264, 3671, 5574, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13222044431246244, "compression_ratio": 1.7763157894736843, "no_speech_prob": 9.305423191108275e-06}, {"id": 1025, "seek": 605606, "start": 6072.06, "end": 6081.06, "text": " And so the target is negative for the negative phase.", "tokens": [50364, 400, 291, 747, 20820, 570, 341, 307, 406, 264, 1036, 2445, 13, 639, 307, 733, 295, 437, 1709, 949, 264, 1036, 2445, 13, 50814, 50814, 407, 341, 307, 733, 295, 411, 257, 3278, 30867, 13, 583, 291, 362, 3278, 30867, 300, 311, 3353, 337, 264, 3353, 5574, 13, 51164, 51164, 400, 370, 264, 3779, 307, 3671, 337, 264, 3671, 5574, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13222044431246244, "compression_ratio": 1.7763157894736843, "no_speech_prob": 9.305423191108275e-06}, {"id": 1026, "seek": 608106, "start": 6081.06, "end": 6087.06, "text": " Yeah, I shouldn't write it this way. This is wrong, actually. Sorry about that.", "tokens": [50364, 865, 11, 286, 4659, 380, 2464, 309, 341, 636, 13, 639, 307, 2085, 11, 767, 13, 4919, 466, 300, 13, 50664, 50664, 583, 291, 829, 309, 294, 264, 3565, 3142, 4470, 337, 1184, 295, 729, 13, 50914, 50914, 407, 309, 311, 12120, 3565, 295, 502, 1804, 21510, 479, 295, 288, 337, 264, 3006, 472, 293, 3175, 3052, 300, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1466442138429672, "compression_ratio": 1.4085365853658536, "no_speech_prob": 7.410380931105465e-06}, {"id": 1027, "seek": 608106, "start": 6087.06, "end": 6092.06, "text": " But you put it in the logistic loss for each of those.", "tokens": [50364, 865, 11, 286, 4659, 380, 2464, 309, 341, 636, 13, 639, 307, 2085, 11, 767, 13, 4919, 466, 300, 13, 50664, 50664, 583, 291, 829, 309, 294, 264, 3565, 3142, 4470, 337, 1184, 295, 729, 13, 50914, 50914, 407, 309, 311, 12120, 3565, 295, 502, 1804, 21510, 479, 295, 288, 337, 264, 3006, 472, 293, 3175, 3052, 300, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1466442138429672, "compression_ratio": 1.4085365853658536, "no_speech_prob": 7.410380931105465e-06}, {"id": 1028, "seek": 608106, "start": 6092.06, "end": 6107.06, "text": " So it's technically log of 1 plus exponential F of y for the correct one and minus further that.", "tokens": [50364, 865, 11, 286, 4659, 380, 2464, 309, 341, 636, 13, 639, 307, 2085, 11, 767, 13, 4919, 466, 300, 13, 50664, 50664, 583, 291, 829, 309, 294, 264, 3565, 3142, 4470, 337, 1184, 295, 729, 13, 50914, 50914, 407, 309, 311, 12120, 3565, 295, 502, 1804, 21510, 479, 295, 288, 337, 264, 3006, 472, 293, 3175, 3052, 300, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1466442138429672, "compression_ratio": 1.4085365853658536, "no_speech_prob": 7.410380931105465e-06}, {"id": 1029, "seek": 610706, "start": 6107.06, "end": 6126.06, "text": " Log of 1 plus e to the F of y plus log 1 plus e to the minus F of y bar.", "tokens": [50364, 10824, 295, 502, 1804, 308, 281, 264, 479, 295, 288, 1804, 3565, 502, 1804, 308, 281, 264, 3175, 479, 295, 288, 2159, 13, 51314, 51314, 583, 291, 727, 3811, 257, 2416, 1230, 295, 10024, 6828, 295, 341, 2010, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09867987521859102, "compression_ratio": 1.3394495412844036, "no_speech_prob": 4.936423465551343e-06}, {"id": 1030, "seek": 610706, "start": 6126.06, "end": 6135.06, "text": " But you could imagine a large number of objective functions of this type.", "tokens": [50364, 10824, 295, 502, 1804, 308, 281, 264, 479, 295, 288, 1804, 3565, 502, 1804, 308, 281, 264, 3175, 479, 295, 288, 2159, 13, 51314, 51314, 583, 291, 727, 3811, 257, 2416, 1230, 295, 10024, 6828, 295, 341, 2010, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09867987521859102, "compression_ratio": 1.3394495412844036, "no_speech_prob": 4.936423465551343e-06}, {"id": 1031, "seek": 613506, "start": 6135.06, "end": 6142.06, "text": " OK, so this is the last function you're going to use to train the discriminator.", "tokens": [50364, 2264, 11, 370, 341, 307, 264, 1036, 2445, 291, 434, 516, 281, 764, 281, 3847, 264, 20828, 1639, 13, 50714, 50714, 583, 264, 19265, 11, 341, 307, 337, 264, 20828, 1639, 13, 50964, 50964, 583, 309, 311, 516, 281, 312, 257, 1036, 2445, 337, 264, 19265, 11, 293, 300, 311, 257, 819, 1036, 2445, 13, 51214, 51214, 400, 291, 434, 516, 281, 19719, 729, 732, 1036, 6828, 264, 912, 636, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.09085004806518554, "compression_ratio": 1.8838709677419354, "no_speech_prob": 4.425444330991013e-06}, {"id": 1032, "seek": 613506, "start": 6142.06, "end": 6147.06, "text": " But the generator, this is for the discriminator.", "tokens": [50364, 2264, 11, 370, 341, 307, 264, 1036, 2445, 291, 434, 516, 281, 764, 281, 3847, 264, 20828, 1639, 13, 50714, 50714, 583, 264, 19265, 11, 341, 307, 337, 264, 20828, 1639, 13, 50964, 50964, 583, 309, 311, 516, 281, 312, 257, 1036, 2445, 337, 264, 19265, 11, 293, 300, 311, 257, 819, 1036, 2445, 13, 51214, 51214, 400, 291, 434, 516, 281, 19719, 729, 732, 1036, 6828, 264, 912, 636, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.09085004806518554, "compression_ratio": 1.8838709677419354, "no_speech_prob": 4.425444330991013e-06}, {"id": 1033, "seek": 613506, "start": 6147.06, "end": 6152.06, "text": " But it's going to be a last function for the generator, and that's a different last function.", "tokens": [50364, 2264, 11, 370, 341, 307, 264, 1036, 2445, 291, 434, 516, 281, 764, 281, 3847, 264, 20828, 1639, 13, 50714, 50714, 583, 264, 19265, 11, 341, 307, 337, 264, 20828, 1639, 13, 50964, 50964, 583, 309, 311, 516, 281, 312, 257, 1036, 2445, 337, 264, 19265, 11, 293, 300, 311, 257, 819, 1036, 2445, 13, 51214, 51214, 400, 291, 434, 516, 281, 19719, 729, 732, 1036, 6828, 264, 912, 636, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.09085004806518554, "compression_ratio": 1.8838709677419354, "no_speech_prob": 4.425444330991013e-06}, {"id": 1034, "seek": 613506, "start": 6152.06, "end": 6156.06, "text": " And you're going to optimize those two last functions the same way.", "tokens": [50364, 2264, 11, 370, 341, 307, 264, 1036, 2445, 291, 434, 516, 281, 764, 281, 3847, 264, 20828, 1639, 13, 50714, 50714, 583, 264, 19265, 11, 341, 307, 337, 264, 20828, 1639, 13, 50964, 50964, 583, 309, 311, 516, 281, 312, 257, 1036, 2445, 337, 264, 19265, 11, 293, 300, 311, 257, 819, 1036, 2445, 13, 51214, 51214, 400, 291, 434, 516, 281, 19719, 729, 732, 1036, 6828, 264, 912, 636, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.09085004806518554, "compression_ratio": 1.8838709677419354, "no_speech_prob": 4.425444330991013e-06}, {"id": 1035, "seek": 615606, "start": 6156.06, "end": 6171.06, "text": " So the generator is one that basically wants to make the generator produce outputs that the discriminator thinks are good, but they're not.", "tokens": [50364, 407, 264, 19265, 307, 472, 300, 1936, 2738, 281, 652, 264, 19265, 5258, 23930, 300, 264, 20828, 1639, 7309, 366, 665, 11, 457, 436, 434, 406, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.06713678759913291, "compression_ratio": 1.3495145631067962, "no_speech_prob": 3.1875090371613624e-06}, {"id": 1036, "seek": 617106, "start": 6171.06, "end": 6195.06, "text": " So basically the generator wants to adapt its weight so that the output that it produces, y bar, produces a low energy for F of y.", "tokens": [50364, 407, 1936, 264, 19265, 2738, 281, 6231, 1080, 3364, 370, 300, 264, 5598, 300, 309, 14725, 11, 288, 2159, 11, 14725, 257, 2295, 2281, 337, 479, 295, 288, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08213220943104137, "compression_ratio": 1.2871287128712872, "no_speech_prob": 3.7265510854922468e-06}, {"id": 1037, "seek": 619506, "start": 6195.06, "end": 6204.06, "text": " So you sample a random variable z, you run it through the generator, it produces a y bar, you run through the discriminator, the F of y, you get some value.", "tokens": [50364, 407, 291, 6889, 257, 4974, 7006, 710, 11, 291, 1190, 309, 807, 264, 19265, 11, 309, 14725, 257, 288, 2159, 11, 291, 1190, 807, 264, 20828, 1639, 11, 264, 479, 295, 288, 11, 291, 483, 512, 2158, 13, 50814, 50814, 400, 550, 291, 646, 48256, 264, 2158, 807, 264, 19265, 293, 6231, 264, 17443, 295, 264, 19265, 370, 300, 341, 2281, 1709, 760, 13, 51314, 51314, 407, 1936, 264, 19265, 307, 1382, 281, 915, 257, 288, 2159, 300, 575, 2295, 2281, 11, 382, 2295, 382, 1944, 13, 51614, 51614, 400, 309, 16329, 2564, 281, 733, 295, 5258, 288, 311, 300, 362, 2295, 2281, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09035995271470812, "compression_ratio": 1.8818565400843883, "no_speech_prob": 9.368176506541204e-06}, {"id": 1038, "seek": 619506, "start": 6204.06, "end": 6214.06, "text": " And then you back propagate the value through the generator and adapt the weights of the generator so that this energy goes down.", "tokens": [50364, 407, 291, 6889, 257, 4974, 7006, 710, 11, 291, 1190, 309, 807, 264, 19265, 11, 309, 14725, 257, 288, 2159, 11, 291, 1190, 807, 264, 20828, 1639, 11, 264, 479, 295, 288, 11, 291, 483, 512, 2158, 13, 50814, 50814, 400, 550, 291, 646, 48256, 264, 2158, 807, 264, 19265, 293, 6231, 264, 17443, 295, 264, 19265, 370, 300, 341, 2281, 1709, 760, 13, 51314, 51314, 407, 1936, 264, 19265, 307, 1382, 281, 915, 257, 288, 2159, 300, 575, 2295, 2281, 11, 382, 2295, 382, 1944, 13, 51614, 51614, 400, 309, 16329, 2564, 281, 733, 295, 5258, 288, 311, 300, 362, 2295, 2281, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09035995271470812, "compression_ratio": 1.8818565400843883, "no_speech_prob": 9.368176506541204e-06}, {"id": 1039, "seek": 619506, "start": 6214.06, "end": 6220.06, "text": " So basically the generator is trying to find a y bar that has low energy, as low as possible.", "tokens": [50364, 407, 291, 6889, 257, 4974, 7006, 710, 11, 291, 1190, 309, 807, 264, 19265, 11, 309, 14725, 257, 288, 2159, 11, 291, 1190, 807, 264, 20828, 1639, 11, 264, 479, 295, 288, 11, 291, 483, 512, 2158, 13, 50814, 50814, 400, 550, 291, 646, 48256, 264, 2158, 807, 264, 19265, 293, 6231, 264, 17443, 295, 264, 19265, 370, 300, 341, 2281, 1709, 760, 13, 51314, 51314, 407, 1936, 264, 19265, 307, 1382, 281, 915, 257, 288, 2159, 300, 575, 2295, 2281, 11, 382, 2295, 382, 1944, 13, 51614, 51614, 400, 309, 16329, 2564, 281, 733, 295, 5258, 288, 311, 300, 362, 2295, 2281, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09035995271470812, "compression_ratio": 1.8818565400843883, "no_speech_prob": 9.368176506541204e-06}, {"id": 1040, "seek": 619506, "start": 6220.06, "end": 6224.06, "text": " And it trains itself to kind of produce y's that have low energy.", "tokens": [50364, 407, 291, 6889, 257, 4974, 7006, 710, 11, 291, 1190, 309, 807, 264, 19265, 11, 309, 14725, 257, 288, 2159, 11, 291, 1190, 807, 264, 20828, 1639, 11, 264, 479, 295, 288, 11, 291, 483, 512, 2158, 13, 50814, 50814, 400, 550, 291, 646, 48256, 264, 2158, 807, 264, 19265, 293, 6231, 264, 17443, 295, 264, 19265, 370, 300, 341, 2281, 1709, 760, 13, 51314, 51314, 407, 1936, 264, 19265, 307, 1382, 281, 915, 257, 288, 2159, 300, 575, 2295, 2281, 11, 382, 2295, 382, 1944, 13, 51614, 51614, 400, 309, 16329, 2564, 281, 733, 295, 5258, 288, 311, 300, 362, 2295, 2281, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09035995271470812, "compression_ratio": 1.8818565400843883, "no_speech_prob": 9.368176506541204e-06}, {"id": 1041, "seek": 622406, "start": 6224.06, "end": 6241.06, "text": " Again, if we're talking about conditional GANs, there's going to be an x variable that's going to enter those two modules, but that makes no difference in the end.", "tokens": [50364, 3764, 11, 498, 321, 434, 1417, 466, 27708, 460, 1770, 82, 11, 456, 311, 516, 281, 312, 364, 2031, 7006, 300, 311, 516, 281, 3242, 729, 732, 16679, 11, 457, 300, 1669, 572, 2649, 294, 264, 917, 13, 51214, 51214, 407, 287, 70, 307, 1310, 2935, 364, 5662, 2445, 295, 479, 295, 288, 2159, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06364678124249992, "compression_ratio": 1.39375, "no_speech_prob": 7.410986199829495e-06}, {"id": 1042, "seek": 622406, "start": 6241.06, "end": 6251.06, "text": " So lg is maybe simply an increasing function of F of y bar.", "tokens": [50364, 3764, 11, 498, 321, 434, 1417, 466, 27708, 460, 1770, 82, 11, 456, 311, 516, 281, 312, 364, 2031, 7006, 300, 311, 516, 281, 3242, 729, 732, 16679, 11, 457, 300, 1669, 572, 2649, 294, 264, 917, 13, 51214, 51214, 407, 287, 70, 307, 1310, 2935, 364, 5662, 2445, 295, 479, 295, 288, 2159, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06364678124249992, "compression_ratio": 1.39375, "no_speech_prob": 7.410986199829495e-06}, {"id": 1043, "seek": 625106, "start": 6251.06, "end": 6263.06, "text": " I think we are kind of running out of time. We are. We have run out of time.", "tokens": [50364, 286, 519, 321, 366, 733, 295, 2614, 484, 295, 565, 13, 492, 366, 13, 492, 362, 1190, 484, 295, 565, 13, 50964, 50964, 407, 341, 576, 312, 512, 10024, 2445, 295, 479, 295, 290, 498, 290, 307, 264, 19265, 295, 710, 689, 710, 307, 6889, 16979, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11313868503944546, "compression_ratio": 1.4, "no_speech_prob": 1.6187017536140047e-05}, {"id": 1044, "seek": 625106, "start": 6263.06, "end": 6276.06, "text": " So this would be some objective function of F of g if g is the generator of z where z is sample randomly.", "tokens": [50364, 286, 519, 321, 366, 733, 295, 2614, 484, 295, 565, 13, 492, 366, 13, 492, 362, 1190, 484, 295, 565, 13, 50964, 50964, 407, 341, 576, 312, 512, 10024, 2445, 295, 479, 295, 290, 498, 290, 307, 264, 19265, 295, 710, 689, 710, 307, 6889, 16979, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11313868503944546, "compression_ratio": 1.4, "no_speech_prob": 1.6187017536140047e-05}, {"id": 1045, "seek": 627606, "start": 6276.06, "end": 6285.06, "text": " So you just do back prop through this and you change the parameters of g, let's call them u, so that this goes down.", "tokens": [50364, 407, 291, 445, 360, 646, 2365, 807, 341, 293, 291, 1319, 264, 9834, 295, 290, 11, 718, 311, 818, 552, 344, 11, 370, 300, 341, 1709, 760, 13, 50814, 50814, 823, 341, 307, 1219, 257, 1216, 294, 264, 2020, 300, 291, 362, 732, 10024, 6828, 300, 291, 643, 281, 17522, 16561, 293, 436, 366, 40393, 267, 964, 365, 1184, 661, 13, 51214, 51214, 400, 370, 309, 311, 406, 257, 16235, 23475, 1154, 13, 509, 362, 281, 915, 437, 311, 1219, 257, 25012, 15625, 1296, 729, 732, 6828, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12119007110595703, "compression_ratio": 1.639344262295082, "no_speech_prob": 8.664410415804014e-06}, {"id": 1046, "seek": 627606, "start": 6285.06, "end": 6293.06, "text": " Now this is called a game in the sense that you have two objective functions that you need to minimize simultaneously and they are incompatible with each other.", "tokens": [50364, 407, 291, 445, 360, 646, 2365, 807, 341, 293, 291, 1319, 264, 9834, 295, 290, 11, 718, 311, 818, 552, 344, 11, 370, 300, 341, 1709, 760, 13, 50814, 50814, 823, 341, 307, 1219, 257, 1216, 294, 264, 2020, 300, 291, 362, 732, 10024, 6828, 300, 291, 643, 281, 17522, 16561, 293, 436, 366, 40393, 267, 964, 365, 1184, 661, 13, 51214, 51214, 400, 370, 309, 311, 406, 257, 16235, 23475, 1154, 13, 509, 362, 281, 915, 437, 311, 1219, 257, 25012, 15625, 1296, 729, 732, 6828, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12119007110595703, "compression_ratio": 1.639344262295082, "no_speech_prob": 8.664410415804014e-06}, {"id": 1047, "seek": 627606, "start": 6293.06, "end": 6300.06, "text": " And so it's not a gradient descent problem. You have to find what's called a Nash equilibrium between those two functions.", "tokens": [50364, 407, 291, 445, 360, 646, 2365, 807, 341, 293, 291, 1319, 264, 9834, 295, 290, 11, 718, 311, 818, 552, 344, 11, 370, 300, 341, 1709, 760, 13, 50814, 50814, 823, 341, 307, 1219, 257, 1216, 294, 264, 2020, 300, 291, 362, 732, 10024, 6828, 300, 291, 643, 281, 17522, 16561, 293, 436, 366, 40393, 267, 964, 365, 1184, 661, 13, 51214, 51214, 400, 370, 309, 311, 406, 257, 16235, 23475, 1154, 13, 509, 362, 281, 915, 437, 311, 1219, 257, 25012, 15625, 1296, 729, 732, 6828, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12119007110595703, "compression_ratio": 1.639344262295082, "no_speech_prob": 8.664410415804014e-06}, {"id": 1048, "seek": 630006, "start": 6300.06, "end": 6307.06, "text": " And gradient descent will not do it by default. So that leads to instabilities.", "tokens": [50364, 400, 16235, 23475, 486, 406, 360, 309, 538, 7576, 13, 407, 300, 6689, 281, 1058, 6167, 13, 50714, 50714, 400, 456, 307, 9131, 295, 10577, 322, 577, 281, 652, 460, 1770, 82, 767, 589, 13, 663, 311, 733, 295, 257, 6179, 644, 13, 583, 28327, 78, 486, 980, 291, 439, 466, 341, 4153, 13, 51164, 51164, 2704, 291, 611, 291, 528, 281, 2152, 264, 472, 365, 264, 4556, 3280, 327, 300, 7829, 512, 2663, 498, 321, 362, 411, 10938, 300, 366, 1998, 281, 264, 2074, 47138, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11210593548449842, "compression_ratio": 1.5454545454545454, "no_speech_prob": 2.178077556891367e-05}, {"id": 1049, "seek": 630006, "start": 6307.06, "end": 6316.06, "text": " And there is tons of papers on how to make GANs actually work. That's kind of a complicated part. But Alfredo will tell you all about this tomorrow.", "tokens": [50364, 400, 16235, 23475, 486, 406, 360, 309, 538, 7576, 13, 407, 300, 6689, 281, 1058, 6167, 13, 50714, 50714, 400, 456, 307, 9131, 295, 10577, 322, 577, 281, 652, 460, 1770, 82, 767, 589, 13, 663, 311, 733, 295, 257, 6179, 644, 13, 583, 28327, 78, 486, 980, 291, 439, 466, 341, 4153, 13, 51164, 51164, 2704, 291, 611, 291, 528, 281, 2152, 264, 472, 365, 264, 4556, 3280, 327, 300, 7829, 512, 2663, 498, 321, 362, 411, 10938, 300, 366, 1998, 281, 264, 2074, 47138, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11210593548449842, "compression_ratio": 1.5454545454545454, "no_speech_prob": 2.178077556891367e-05}, {"id": 1050, "seek": 630006, "start": 6316.06, "end": 6325.06, "text": " Maybe you also you want to mention the one with the sigmoid that creates some issues if we have like samples that are close to the true manifold.", "tokens": [50364, 400, 16235, 23475, 486, 406, 360, 309, 538, 7576, 13, 407, 300, 6689, 281, 1058, 6167, 13, 50714, 50714, 400, 456, 307, 9131, 295, 10577, 322, 577, 281, 652, 460, 1770, 82, 767, 589, 13, 663, 311, 733, 295, 257, 6179, 644, 13, 583, 28327, 78, 486, 980, 291, 439, 466, 341, 4153, 13, 51164, 51164, 2704, 291, 611, 291, 528, 281, 2152, 264, 472, 365, 264, 4556, 3280, 327, 300, 7829, 512, 2663, 498, 321, 362, 411, 10938, 300, 366, 1998, 281, 264, 2074, 47138, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11210593548449842, "compression_ratio": 1.5454545454545454, "no_speech_prob": 2.178077556891367e-05}, {"id": 1051, "seek": 632506, "start": 6325.06, "end": 6337.06, "text": " Yes. And then I think we can close the closed. OK, so let me mention that. So let's imagine that your data.", "tokens": [50364, 1079, 13, 400, 550, 286, 519, 321, 393, 1998, 264, 5395, 13, 2264, 11, 370, 718, 385, 2152, 300, 13, 407, 718, 311, 3811, 300, 428, 1412, 13, 50964, 50964, 407, 797, 11, 2281, 2361, 8388, 13, 2260, 1412, 307, 926, 370, 47138, 11, 457, 309, 311, 257, 5862, 47138, 13, 407, 309, 311, 364, 36227, 5862, 7316, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.22033094981360057, "compression_ratio": 1.4876543209876543, "no_speech_prob": 5.306710227159783e-05}, {"id": 1052, "seek": 632506, "start": 6337.06, "end": 6353.06, "text": " So again, energy based framework. Your data is around so manifold, but it's a thin manifold. So it's an infinitely thin distribution.", "tokens": [50364, 1079, 13, 400, 550, 286, 519, 321, 393, 1998, 264, 5395, 13, 2264, 11, 370, 718, 385, 2152, 300, 13, 407, 718, 311, 3811, 300, 428, 1412, 13, 50964, 50964, 407, 797, 11, 2281, 2361, 8388, 13, 2260, 1412, 307, 926, 370, 47138, 11, 457, 309, 311, 257, 5862, 47138, 13, 407, 309, 311, 364, 36227, 5862, 7316, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.22033094981360057, "compression_ratio": 1.4876543209876543, "no_speech_prob": 5.306710227159783e-05}, {"id": 1053, "seek": 635306, "start": 6353.06, "end": 6368.06, "text": " OK. In the original formulation of GAN, the GAN, the discriminator would need to produce zero probability outside of this.", "tokens": [50364, 2264, 13, 682, 264, 3380, 37642, 295, 460, 1770, 11, 264, 460, 1770, 11, 264, 20828, 1639, 576, 643, 281, 5258, 4018, 8482, 2380, 295, 341, 13, 51114, 51114, 2264, 11, 370, 309, 2203, 281, 5258, 4018, 8482, 510, 13, 400, 309, 2203, 281, 5258, 322, 264, 47138, 11, 309, 2203, 281, 5258, 13785, 8482, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1359126885732015, "compression_ratio": 1.7832167832167831, "no_speech_prob": 1.3209930330049247e-05}, {"id": 1054, "seek": 635306, "start": 6368.06, "end": 6379.06, "text": " OK, so it needs to produce zero probability here. And it needs to produce on the manifold, it needs to produce infinite probability.", "tokens": [50364, 2264, 13, 682, 264, 3380, 37642, 295, 460, 1770, 11, 264, 460, 1770, 11, 264, 20828, 1639, 576, 643, 281, 5258, 4018, 8482, 2380, 295, 341, 13, 51114, 51114, 2264, 11, 370, 309, 2203, 281, 5258, 4018, 8482, 510, 13, 400, 309, 2203, 281, 5258, 322, 264, 47138, 11, 309, 2203, 281, 5258, 13785, 8482, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1359126885732015, "compression_ratio": 1.7832167832167831, "no_speech_prob": 1.3209930330049247e-05}, {"id": 1055, "seek": 637906, "start": 6379.06, "end": 6388.06, "text": " In such a way that the integral, if this is really a density estimation, in such a way that the integral of the density over the entire space is one.", "tokens": [50364, 682, 1270, 257, 636, 300, 264, 11573, 11, 498, 341, 307, 534, 257, 10305, 35701, 11, 294, 1270, 257, 636, 300, 264, 11573, 295, 264, 10305, 670, 264, 2302, 1901, 307, 472, 13, 50814, 50814, 400, 341, 307, 11, 295, 1164, 11, 588, 1152, 13, 407, 460, 1770, 82, 1936, 9072, 264, 1558, 295, 767, 2539, 257, 7316, 13, 51214, 51214, 708, 436, 528, 281, 360, 307, 5258, 4018, 11, 264, 3380, 37642, 11, 5258, 4018, 2380, 264, 47138, 295, 1412, 293, 5258, 472, 510, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07260530789693197, "compression_ratio": 1.7219730941704037, "no_speech_prob": 1.1299138350295834e-05}, {"id": 1056, "seek": 637906, "start": 6388.06, "end": 6396.06, "text": " And this is, of course, very hard. So GANs basically abandon the idea of actually learning a distribution.", "tokens": [50364, 682, 1270, 257, 636, 300, 264, 11573, 11, 498, 341, 307, 534, 257, 10305, 35701, 11, 294, 1270, 257, 636, 300, 264, 11573, 295, 264, 10305, 670, 264, 2302, 1901, 307, 472, 13, 50814, 50814, 400, 341, 307, 11, 295, 1164, 11, 588, 1152, 13, 407, 460, 1770, 82, 1936, 9072, 264, 1558, 295, 767, 2539, 257, 7316, 13, 51214, 51214, 708, 436, 528, 281, 360, 307, 5258, 4018, 11, 264, 3380, 37642, 11, 5258, 4018, 2380, 264, 47138, 295, 1412, 293, 5258, 472, 510, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07260530789693197, "compression_ratio": 1.7219730941704037, "no_speech_prob": 1.1299138350295834e-05}, {"id": 1057, "seek": 637906, "start": 6396.06, "end": 6404.06, "text": " What they want to do is produce zero, the original formulation, produce zero outside the manifold of data and produce one here.", "tokens": [50364, 682, 1270, 257, 636, 300, 264, 11573, 11, 498, 341, 307, 534, 257, 10305, 35701, 11, 294, 1270, 257, 636, 300, 264, 11573, 295, 264, 10305, 670, 264, 2302, 1901, 307, 472, 13, 50814, 50814, 400, 341, 307, 11, 295, 1164, 11, 588, 1152, 13, 407, 460, 1770, 82, 1936, 9072, 264, 1558, 295, 767, 2539, 257, 7316, 13, 51214, 51214, 708, 436, 528, 281, 360, 307, 5258, 4018, 11, 264, 3380, 37642, 11, 5258, 4018, 2380, 264, 47138, 295, 1412, 293, 5258, 472, 510, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07260530789693197, "compression_ratio": 1.7219730941704037, "no_speech_prob": 1.1299138350295834e-05}, {"id": 1058, "seek": 640406, "start": 6404.06, "end": 6410.06, "text": " It's the output of a sigmoid that needs to be one, which means the weighted sum going into that sigmoid needs to be infinite, essentially.", "tokens": [50364, 467, 311, 264, 5598, 295, 257, 4556, 3280, 327, 300, 2203, 281, 312, 472, 11, 597, 1355, 264, 32807, 2408, 516, 666, 300, 4556, 3280, 327, 2203, 281, 312, 13785, 11, 4476, 13, 50664, 50664, 407, 309, 311, 406, 300, 819, 13, 400, 264, 1154, 365, 341, 307, 300, 498, 291, 3847, 264, 1185, 10727, 13, 51164, 51164, 400, 291, 483, 264, 2281, 2445, 11, 597, 307, 4018, 2380, 264, 47138, 293, 472, 322, 264, 47138, 11, 428, 2281, 2445, 307, 2584, 14115, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09605543179945512, "compression_ratio": 1.7201834862385321, "no_speech_prob": 1.6185287677217275e-05}, {"id": 1059, "seek": 640406, "start": 6410.06, "end": 6420.06, "text": " So it's not that different. And the problem with this is that if you train the system successfully.", "tokens": [50364, 467, 311, 264, 5598, 295, 257, 4556, 3280, 327, 300, 2203, 281, 312, 472, 11, 597, 1355, 264, 32807, 2408, 516, 666, 300, 4556, 3280, 327, 2203, 281, 312, 13785, 11, 4476, 13, 50664, 50664, 407, 309, 311, 406, 300, 819, 13, 400, 264, 1154, 365, 341, 307, 300, 498, 291, 3847, 264, 1185, 10727, 13, 51164, 51164, 400, 291, 483, 264, 2281, 2445, 11, 597, 307, 4018, 2380, 264, 47138, 293, 472, 322, 264, 47138, 11, 428, 2281, 2445, 307, 2584, 14115, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09605543179945512, "compression_ratio": 1.7201834862385321, "no_speech_prob": 1.6185287677217275e-05}, {"id": 1060, "seek": 640406, "start": 6420.06, "end": 6428.06, "text": " And you get the energy function, which is zero outside the manifold and one on the manifold, your energy function is completely useless.", "tokens": [50364, 467, 311, 264, 5598, 295, 257, 4556, 3280, 327, 300, 2203, 281, 312, 472, 11, 597, 1355, 264, 32807, 2408, 516, 666, 300, 4556, 3280, 327, 2203, 281, 312, 13785, 11, 4476, 13, 50664, 50664, 407, 309, 311, 406, 300, 819, 13, 400, 264, 1154, 365, 341, 307, 300, 498, 291, 3847, 264, 1185, 10727, 13, 51164, 51164, 400, 291, 483, 264, 2281, 2445, 11, 597, 307, 4018, 2380, 264, 47138, 293, 472, 322, 264, 47138, 11, 428, 2281, 2445, 307, 2584, 14115, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09605543179945512, "compression_ratio": 1.7201834862385321, "no_speech_prob": 1.6185287677217275e-05}, {"id": 1061, "seek": 642806, "start": 6428.06, "end": 6437.06, "text": " It's useless because it's a golf course, right? It's flat. So the energy function basically that corresponds to this would be the negative log of that.", "tokens": [50364, 467, 311, 14115, 570, 309, 311, 257, 12880, 1164, 11, 558, 30, 467, 311, 4962, 13, 407, 264, 2281, 2445, 1936, 300, 23249, 281, 341, 576, 312, 264, 3671, 3565, 295, 300, 13, 50814, 50814, 407, 309, 576, 312, 13202, 510, 13, 400, 264, 7285, 2158, 295, 428, 2063, 2445, 322, 264, 47138, 11, 597, 11, 337, 1365, 11, 727, 312, 4018, 13, 51364, 51364, 759, 309, 311, 364, 8399, 22660, 19866, 11, 264, 2281, 393, 312, 4356, 813, 4018, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12458736195283777, "compression_ratio": 1.5794392523364487, "no_speech_prob": 1.5203181646938901e-05}, {"id": 1062, "seek": 642806, "start": 6437.06, "end": 6448.06, "text": " So it would be infinity here. And the minimum value of your cost function on the manifold, which, for example, could be zero.", "tokens": [50364, 467, 311, 14115, 570, 309, 311, 257, 12880, 1164, 11, 558, 30, 467, 311, 4962, 13, 407, 264, 2281, 2445, 1936, 300, 23249, 281, 341, 576, 312, 264, 3671, 3565, 295, 300, 13, 50814, 50814, 407, 309, 576, 312, 13202, 510, 13, 400, 264, 7285, 2158, 295, 428, 2063, 2445, 322, 264, 47138, 11, 597, 11, 337, 1365, 11, 727, 312, 4018, 13, 51364, 51364, 759, 309, 311, 364, 8399, 22660, 19866, 11, 264, 2281, 393, 312, 4356, 813, 4018, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12458736195283777, "compression_ratio": 1.5794392523364487, "no_speech_prob": 1.5203181646938901e-05}, {"id": 1063, "seek": 642806, "start": 6448.06, "end": 6454.06, "text": " If it's an autoencoder, the energy can be smaller than zero.", "tokens": [50364, 467, 311, 14115, 570, 309, 311, 257, 12880, 1164, 11, 558, 30, 467, 311, 4962, 13, 407, 264, 2281, 2445, 1936, 300, 23249, 281, 341, 576, 312, 264, 3671, 3565, 295, 300, 13, 50814, 50814, 407, 309, 576, 312, 13202, 510, 13, 400, 264, 7285, 2158, 295, 428, 2063, 2445, 322, 264, 47138, 11, 597, 11, 337, 1365, 11, 727, 312, 4018, 13, 51364, 51364, 759, 309, 311, 364, 8399, 22660, 19866, 11, 264, 2281, 393, 312, 4356, 813, 4018, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12458736195283777, "compression_ratio": 1.5794392523364487, "no_speech_prob": 1.5203181646938901e-05}, {"id": 1064, "seek": 645406, "start": 6454.06, "end": 6461.06, "text": " And so it's a golf course of infinite altitude, which is really not that useful.", "tokens": [50364, 400, 370, 309, 311, 257, 12880, 1164, 295, 13785, 24003, 11, 597, 307, 534, 406, 300, 4420, 13, 50714, 50714, 708, 291, 528, 11, 382, 286, 848, 949, 11, 337, 633, 2281, 12, 6032, 2316, 11, 498, 291, 528, 364, 2281, 12, 6032, 2316, 281, 312, 4420, 11, 291, 528, 264, 2281, 2445, 281, 312, 5508, 13, 51164, 51164, 509, 500, 380, 528, 309, 281, 352, 281, 13202, 294, 1333, 295, 588, 1359, 4439, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.060607771330241915, "compression_ratio": 1.6162162162162161, "no_speech_prob": 3.02361004287377e-05}, {"id": 1065, "seek": 645406, "start": 6461.06, "end": 6470.06, "text": " What you want, as I said before, for every energy-based model, if you want an energy-based model to be useful, you want the energy function to be smooth.", "tokens": [50364, 400, 370, 309, 311, 257, 12880, 1164, 295, 13785, 24003, 11, 597, 307, 534, 406, 300, 4420, 13, 50714, 50714, 708, 291, 528, 11, 382, 286, 848, 949, 11, 337, 633, 2281, 12, 6032, 2316, 11, 498, 291, 528, 364, 2281, 12, 6032, 2316, 281, 312, 4420, 11, 291, 528, 264, 2281, 2445, 281, 312, 5508, 13, 51164, 51164, 509, 500, 380, 528, 309, 281, 352, 281, 13202, 294, 1333, 295, 588, 1359, 4439, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.060607771330241915, "compression_ratio": 1.6162162162162161, "no_speech_prob": 3.02361004287377e-05}, {"id": 1066, "seek": 645406, "start": 6470.06, "end": 6475.06, "text": " You don't want it to go to infinity in sort of very small steps.", "tokens": [50364, 400, 370, 309, 311, 257, 12880, 1164, 295, 13785, 24003, 11, 597, 307, 534, 406, 300, 4420, 13, 50714, 50714, 708, 291, 528, 11, 382, 286, 848, 949, 11, 337, 633, 2281, 12, 6032, 2316, 11, 498, 291, 528, 364, 2281, 12, 6032, 2316, 281, 312, 4420, 11, 291, 528, 264, 2281, 2445, 281, 312, 5508, 13, 51164, 51164, 509, 500, 380, 528, 309, 281, 352, 281, 13202, 294, 1333, 295, 588, 1359, 4439, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.060607771330241915, "compression_ratio": 1.6162162162162161, "no_speech_prob": 3.02361004287377e-05}, {"id": 1067, "seek": 647506, "start": 6475.06, "end": 6485.06, "text": " You want it to be smooth so that you can do inference. So that if you start from a point here, it's easy to find a point on the manifold that's nearby using gradient descent, for example.", "tokens": [50364, 509, 528, 309, 281, 312, 5508, 370, 300, 291, 393, 360, 38253, 13, 407, 300, 498, 291, 722, 490, 257, 935, 510, 11, 309, 311, 1858, 281, 915, 257, 935, 322, 264, 47138, 300, 311, 11184, 1228, 16235, 23475, 11, 337, 1365, 13, 50864, 50864, 407, 264, 3380, 37642, 295, 10384, 45, 6689, 281, 11, 700, 295, 439, 11, 13785, 17443, 294, 264, 20828, 1639, 11, 1058, 6167, 11, 746, 1219, 4391, 15584, 11, 597, 28327, 78, 486, 980, 291, 466, 13, 51514, 51514, 400, 294, 264, 917, 11, 257, 8712, 2445, 11, 364, 2281, 2445, 300, 311, 4476, 14115, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09690385546003069, "compression_ratio": 1.6340579710144927, "no_speech_prob": 9.970112841983791e-06}, {"id": 1068, "seek": 647506, "start": 6485.06, "end": 6498.06, "text": " So the original formulation of GaN leads to, first of all, infinite weights in the discriminator, instabilities, something called mode collapse, which Alfredo will tell you about.", "tokens": [50364, 509, 528, 309, 281, 312, 5508, 370, 300, 291, 393, 360, 38253, 13, 407, 300, 498, 291, 722, 490, 257, 935, 510, 11, 309, 311, 1858, 281, 915, 257, 935, 322, 264, 47138, 300, 311, 11184, 1228, 16235, 23475, 11, 337, 1365, 13, 50864, 50864, 407, 264, 3380, 37642, 295, 10384, 45, 6689, 281, 11, 700, 295, 439, 11, 13785, 17443, 294, 264, 20828, 1639, 11, 1058, 6167, 11, 746, 1219, 4391, 15584, 11, 597, 28327, 78, 486, 980, 291, 466, 13, 51514, 51514, 400, 294, 264, 917, 11, 257, 8712, 2445, 11, 364, 2281, 2445, 300, 311, 4476, 14115, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09690385546003069, "compression_ratio": 1.6340579710144927, "no_speech_prob": 9.970112841983791e-06}, {"id": 1069, "seek": 647506, "start": 6498.06, "end": 6504.06, "text": " And in the end, a contrast function, an energy function that's essentially useless.", "tokens": [50364, 509, 528, 309, 281, 312, 5508, 370, 300, 291, 393, 360, 38253, 13, 407, 300, 498, 291, 722, 490, 257, 935, 510, 11, 309, 311, 1858, 281, 915, 257, 935, 322, 264, 47138, 300, 311, 11184, 1228, 16235, 23475, 11, 337, 1365, 13, 50864, 50864, 407, 264, 3380, 37642, 295, 10384, 45, 6689, 281, 11, 700, 295, 439, 11, 13785, 17443, 294, 264, 20828, 1639, 11, 1058, 6167, 11, 746, 1219, 4391, 15584, 11, 597, 28327, 78, 486, 980, 291, 466, 13, 51514, 51514, 400, 294, 264, 917, 11, 257, 8712, 2445, 11, 364, 2281, 2445, 300, 311, 4476, 14115, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09690385546003069, "compression_ratio": 1.6340579710144927, "no_speech_prob": 9.970112841983791e-06}, {"id": 1070, "seek": 650406, "start": 6504.06, "end": 6514.06, "text": " So it's not ideally formulated. So people have proposed ways to fix it by regularizing the energy function, basically forcing it to be smooth.", "tokens": [50364, 407, 309, 311, 406, 22915, 48936, 13, 407, 561, 362, 10348, 2098, 281, 3191, 309, 538, 3890, 3319, 264, 2281, 2445, 11, 1936, 19030, 309, 281, 312, 5508, 13, 50864, 50864, 407, 472, 665, 1365, 295, 341, 307, 746, 1219, 17351, 9089, 10384, 45, 82, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.10947478294372559, "compression_ratio": 1.3594771241830066, "no_speech_prob": 3.488653601380065e-05}, {"id": 1071, "seek": 650406, "start": 6514.06, "end": 6523.06, "text": " So one good example of this is something called Wasserstein GaNs.", "tokens": [50364, 407, 309, 311, 406, 22915, 48936, 13, 407, 561, 362, 10348, 2098, 281, 3191, 309, 538, 3890, 3319, 264, 2281, 2445, 11, 1936, 19030, 309, 281, 312, 5508, 13, 50864, 50864, 407, 472, 665, 1365, 295, 341, 307, 746, 1219, 17351, 9089, 10384, 45, 82, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.10947478294372559, "compression_ratio": 1.3594771241830066, "no_speech_prob": 3.488653601380065e-05}, {"id": 1072, "seek": 652306, "start": 6523.06, "end": 6538.06, "text": " Proposed by Martin Narzowski, who just graduated from NYU, and Leon Boutroux, and a few other people.", "tokens": [50364, 21944, 1744, 538, 9184, 13512, 89, 21866, 11, 567, 445, 13693, 490, 42682, 11, 293, 13244, 363, 346, 13122, 87, 11, 293, 257, 1326, 661, 561, 13, 51114, 51114, 400, 264, 1558, 295, 300, 307, 281, 1936, 4948, 264, 2744, 295, 264, 17443, 295, 264, 20828, 1639, 370, 300, 264, 2445, 307, 5508, 13, 51414, 51414, 400, 456, 307, 3683, 18894, 12869, 294, 31959, 3142, 8388, 11, 457, 300, 311, 264, 3875, 1558, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14566906904562926, "compression_ratio": 1.509433962264151, "no_speech_prob": 3.21181578328833e-05}, {"id": 1073, "seek": 652306, "start": 6538.06, "end": 6544.06, "text": " And the idea of that is to basically limit the size of the weights of the discriminator so that the function is smooth.", "tokens": [50364, 21944, 1744, 538, 9184, 13512, 89, 21866, 11, 567, 445, 13693, 490, 42682, 11, 293, 13244, 363, 346, 13122, 87, 11, 293, 257, 1326, 661, 561, 13, 51114, 51114, 400, 264, 1558, 295, 300, 307, 281, 1936, 4948, 264, 2744, 295, 264, 17443, 295, 264, 20828, 1639, 370, 300, 264, 2445, 307, 5508, 13, 51414, 51414, 400, 456, 307, 3683, 18894, 12869, 294, 31959, 3142, 8388, 11, 457, 300, 311, 264, 3875, 1558, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14566906904562926, "compression_ratio": 1.509433962264151, "no_speech_prob": 3.21181578328833e-05}, {"id": 1074, "seek": 652306, "start": 6544.06, "end": 6549.06, "text": " And there is various mathematical arguments in probabilistic framework, but that's the basic idea.", "tokens": [50364, 21944, 1744, 538, 9184, 13512, 89, 21866, 11, 567, 445, 13693, 490, 42682, 11, 293, 13244, 363, 346, 13122, 87, 11, 293, 257, 1326, 661, 561, 13, 51114, 51114, 400, 264, 1558, 295, 300, 307, 281, 1936, 4948, 264, 2744, 295, 264, 17443, 295, 264, 20828, 1639, 370, 300, 264, 2445, 307, 5508, 13, 51414, 51414, 400, 456, 307, 3683, 18894, 12869, 294, 31959, 3142, 8388, 11, 457, 300, 311, 264, 3875, 1558, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14566906904562926, "compression_ratio": 1.509433962264151, "no_speech_prob": 3.21181578328833e-05}, {"id": 1075, "seek": 654906, "start": 6549.06, "end": 6554.06, "text": " And there's lots of variations of this also. Questions about today class?", "tokens": [50364, 400, 456, 311, 3195, 295, 17840, 295, 341, 611, 13, 27738, 466, 965, 1508, 30, 50614, 50614, 467, 390, 18011, 11, 457, 412, 1935, 321, 645, 13430, 633, 1168, 309, 390, 1348, 807, 13, 50914, 50914, 407, 286, 519, 321, 1524, 2051, 965, 13, 51114, 51114, 286, 2067, 380, 988, 498, 1310, 291, 8825, 309, 294, 257, 819, 1254, 293, 286, 994, 380, 4325, 309, 311, 264, 912, 551, 11, 457, 286, 390, 257, 707, 2731, 322, 437, 264, 3897, 3209, 307, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09131103274465978, "compression_ratio": 1.51528384279476, "no_speech_prob": 4.682397047872655e-05}, {"id": 1076, "seek": 654906, "start": 6554.06, "end": 6560.06, "text": " It was dense, but at least we were answering every question it was coming through.", "tokens": [50364, 400, 456, 311, 3195, 295, 17840, 295, 341, 611, 13, 27738, 466, 965, 1508, 30, 50614, 50614, 467, 390, 18011, 11, 457, 412, 1935, 321, 645, 13430, 633, 1168, 309, 390, 1348, 807, 13, 50914, 50914, 407, 286, 519, 321, 1524, 2051, 965, 13, 51114, 51114, 286, 2067, 380, 988, 498, 1310, 291, 8825, 309, 294, 257, 819, 1254, 293, 286, 994, 380, 4325, 309, 311, 264, 912, 551, 11, 457, 286, 390, 257, 707, 2731, 322, 437, 264, 3897, 3209, 307, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09131103274465978, "compression_ratio": 1.51528384279476, "no_speech_prob": 4.682397047872655e-05}, {"id": 1077, "seek": 654906, "start": 6560.06, "end": 6564.06, "text": " So I think we follow along today.", "tokens": [50364, 400, 456, 311, 3195, 295, 17840, 295, 341, 611, 13, 27738, 466, 965, 1508, 30, 50614, 50614, 467, 390, 18011, 11, 457, 412, 1935, 321, 645, 13430, 633, 1168, 309, 390, 1348, 807, 13, 50914, 50914, 407, 286, 519, 321, 1524, 2051, 965, 13, 51114, 51114, 286, 2067, 380, 988, 498, 1310, 291, 8825, 309, 294, 257, 819, 1254, 293, 286, 994, 380, 4325, 309, 311, 264, 912, 551, 11, 457, 286, 390, 257, 707, 2731, 322, 437, 264, 3897, 3209, 307, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09131103274465978, "compression_ratio": 1.51528384279476, "no_speech_prob": 4.682397047872655e-05}, {"id": 1078, "seek": 654906, "start": 6564.06, "end": 6577.06, "text": " I wasn't sure if maybe you explained it in a different form and I didn't realize it's the same thing, but I was a little lost on what the policy network is.", "tokens": [50364, 400, 456, 311, 3195, 295, 17840, 295, 341, 611, 13, 27738, 466, 965, 1508, 30, 50614, 50614, 467, 390, 18011, 11, 457, 412, 1935, 321, 645, 13430, 633, 1168, 309, 390, 1348, 807, 13, 50914, 50914, 407, 286, 519, 321, 1524, 2051, 965, 13, 51114, 51114, 286, 2067, 380, 988, 498, 1310, 291, 8825, 309, 294, 257, 819, 1254, 293, 286, 994, 380, 4325, 309, 311, 264, 912, 551, 11, 457, 286, 390, 257, 707, 2731, 322, 437, 264, 3897, 3209, 307, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09131103274465978, "compression_ratio": 1.51528384279476, "no_speech_prob": 4.682397047872655e-05}, {"id": 1079, "seek": 657706, "start": 6577.06, "end": 6580.06, "text": " Okay. What that does.", "tokens": [50364, 1033, 13, 708, 300, 775, 13, 50514, 50514, 407, 264, 3897, 3209, 2516, 264, 35701, 295, 264, 1785, 295, 264, 1002, 293, 14725, 364, 3069, 13, 50814, 50814, 400, 309, 311, 8895, 281, 17522, 264, 5176, 2063, 295, 264, 1785, 670, 264, 21512, 11, 457, 309, 2516, 445, 472, 3069, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13333615389737216, "compression_ratio": 1.530612244897959, "no_speech_prob": 4.0053339034784585e-05}, {"id": 1080, "seek": 657706, "start": 6580.06, "end": 6586.06, "text": " So the policy network takes the estimation of the state of the world and produces an action.", "tokens": [50364, 1033, 13, 708, 300, 775, 13, 50514, 50514, 407, 264, 3897, 3209, 2516, 264, 35701, 295, 264, 1785, 295, 264, 1002, 293, 14725, 364, 3069, 13, 50814, 50814, 400, 309, 311, 8895, 281, 17522, 264, 5176, 2063, 295, 264, 1785, 670, 264, 21512, 11, 457, 309, 2516, 445, 472, 3069, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13333615389737216, "compression_ratio": 1.530612244897959, "no_speech_prob": 4.0053339034784585e-05}, {"id": 1081, "seek": 657706, "start": 6586.06, "end": 6600.06, "text": " And it's trained to minimize the expected cost of the state over the trajectory, but it takes just one action.", "tokens": [50364, 1033, 13, 708, 300, 775, 13, 50514, 50514, 407, 264, 3897, 3209, 2516, 264, 35701, 295, 264, 1785, 295, 264, 1002, 293, 14725, 364, 3069, 13, 50814, 50814, 400, 309, 311, 8895, 281, 17522, 264, 5176, 2063, 295, 264, 1785, 670, 264, 21512, 11, 457, 309, 2516, 445, 472, 3069, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13333615389737216, "compression_ratio": 1.530612244897959, "no_speech_prob": 4.0053339034784585e-05}, {"id": 1082, "seek": 660006, "start": 6600.06, "end": 6608.06, "text": " Right. And there was a part towards the end where I guess you drew a new connection.", "tokens": [50364, 1779, 13, 400, 456, 390, 257, 644, 3030, 264, 917, 689, 286, 2041, 291, 12804, 257, 777, 4984, 13, 50764, 50764, 1779, 13, 663, 490, 411, 318, 281, 689, 309, 1709, 760, 11, 411, 4582, 807, 512, 10088, 281, 316, 13, 407, 437, 307, 2737, 456, 30, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.18596215341605393, "compression_ratio": 1.408450704225352, "no_speech_prob": 9.367558050143998e-06}, {"id": 1083, "seek": 660006, "start": 6608.06, "end": 6623.06, "text": " Right. That from like S to where it goes down, like connected through some module to A. So what is happening there?", "tokens": [50364, 1779, 13, 400, 456, 390, 257, 644, 3030, 264, 917, 689, 286, 2041, 291, 12804, 257, 777, 4984, 13, 50764, 50764, 1779, 13, 663, 490, 411, 318, 281, 689, 309, 1709, 760, 11, 411, 4582, 807, 512, 10088, 281, 316, 13, 407, 437, 307, 2737, 456, 30, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.18596215341605393, "compression_ratio": 1.408450704225352, "no_speech_prob": 9.367558050143998e-06}, {"id": 1084, "seek": 662306, "start": 6623.06, "end": 6636.06, "text": " So the policy network is indicated by pi here on the screen. So it takes S, the state, and it produces an action.", "tokens": [50364, 407, 264, 3897, 3209, 307, 16176, 538, 3895, 510, 322, 264, 2568, 13, 407, 309, 2516, 318, 11, 264, 1785, 11, 293, 309, 14725, 364, 3069, 13, 51014, 51014, 1033, 13, 1033, 13, 663, 311, 437, 257, 3897, 307, 11, 558, 30, 509, 11441, 264, 1785, 295, 264, 1002, 293, 291, 747, 364, 3069, 13, 51364, 51364, 286, 536, 13, 1033, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.14930980855768378, "compression_ratio": 1.506578947368421, "no_speech_prob": 1.4737654055352323e-05}, {"id": 1085, "seek": 662306, "start": 6636.06, "end": 6643.06, "text": " Okay. Okay. That's what a policy is, right? You observe the state of the world and you take an action.", "tokens": [50364, 407, 264, 3897, 3209, 307, 16176, 538, 3895, 510, 322, 264, 2568, 13, 407, 309, 2516, 318, 11, 264, 1785, 11, 293, 309, 14725, 364, 3069, 13, 51014, 51014, 1033, 13, 1033, 13, 663, 311, 437, 257, 3897, 307, 11, 558, 30, 509, 11441, 264, 1785, 295, 264, 1002, 293, 291, 747, 364, 3069, 13, 51364, 51364, 286, 536, 13, 1033, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.14930980855768378, "compression_ratio": 1.506578947368421, "no_speech_prob": 1.4737654055352323e-05}, {"id": 1086, "seek": 662306, "start": 6643.06, "end": 6645.06, "text": " I see. Okay.", "tokens": [50364, 407, 264, 3897, 3209, 307, 16176, 538, 3895, 510, 322, 264, 2568, 13, 407, 309, 2516, 318, 11, 264, 1785, 11, 293, 309, 14725, 364, 3069, 13, 51014, 51014, 1033, 13, 1033, 13, 663, 311, 437, 257, 3897, 307, 11, 558, 30, 509, 11441, 264, 1785, 295, 264, 1002, 293, 291, 747, 364, 3069, 13, 51364, 51364, 286, 536, 13, 1033, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.14930980855768378, "compression_ratio": 1.506578947368421, "no_speech_prob": 1.4737654055352323e-05}, {"id": 1087, "seek": 664506, "start": 6645.06, "end": 6653.06, "text": " In fact, a probabilistic policy is you don't take an action. You give a distribution of actions and then you pick the action in some way from the distribution.", "tokens": [50364, 682, 1186, 11, 257, 31959, 3142, 3897, 307, 291, 500, 380, 747, 364, 3069, 13, 509, 976, 257, 7316, 295, 5909, 293, 550, 291, 1888, 264, 3069, 294, 512, 636, 490, 264, 7316, 13, 50764, 50764, 583, 510, 11, 291, 458, 11, 291, 445, 362, 281, 747, 364, 3069, 13, 50964, 50964, 759, 264, 1230, 295, 5909, 307, 27706, 11, 550, 341, 3895, 3209, 307, 341, 3897, 3209, 307, 1936, 257, 1508, 9902, 13, 51314, 51314, 400, 309, 14725, 257, 3840, 295, 13444, 337, 1184, 1944, 3069, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11540980961011804, "compression_ratio": 1.7239819004524888, "no_speech_prob": 6.4388113969471306e-06}, {"id": 1088, "seek": 664506, "start": 6653.06, "end": 6657.06, "text": " But here, you know, you just have to take an action.", "tokens": [50364, 682, 1186, 11, 257, 31959, 3142, 3897, 307, 291, 500, 380, 747, 364, 3069, 13, 509, 976, 257, 7316, 295, 5909, 293, 550, 291, 1888, 264, 3069, 294, 512, 636, 490, 264, 7316, 13, 50764, 50764, 583, 510, 11, 291, 458, 11, 291, 445, 362, 281, 747, 364, 3069, 13, 50964, 50964, 759, 264, 1230, 295, 5909, 307, 27706, 11, 550, 341, 3895, 3209, 307, 341, 3897, 3209, 307, 1936, 257, 1508, 9902, 13, 51314, 51314, 400, 309, 14725, 257, 3840, 295, 13444, 337, 1184, 1944, 3069, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11540980961011804, "compression_ratio": 1.7239819004524888, "no_speech_prob": 6.4388113969471306e-06}, {"id": 1089, "seek": 664506, "start": 6657.06, "end": 6664.06, "text": " If the number of actions is discrete, then this pi network is this policy network is basically a classifier.", "tokens": [50364, 682, 1186, 11, 257, 31959, 3142, 3897, 307, 291, 500, 380, 747, 364, 3069, 13, 509, 976, 257, 7316, 295, 5909, 293, 550, 291, 1888, 264, 3069, 294, 512, 636, 490, 264, 7316, 13, 50764, 50764, 583, 510, 11, 291, 458, 11, 291, 445, 362, 281, 747, 364, 3069, 13, 50964, 50964, 759, 264, 1230, 295, 5909, 307, 27706, 11, 550, 341, 3895, 3209, 307, 341, 3897, 3209, 307, 1936, 257, 1508, 9902, 13, 51314, 51314, 400, 309, 14725, 257, 3840, 295, 13444, 337, 1184, 1944, 3069, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11540980961011804, "compression_ratio": 1.7239819004524888, "no_speech_prob": 6.4388113969471306e-06}, {"id": 1090, "seek": 664506, "start": 6664.06, "end": 6668.06, "text": " And it produces a bunch of scores for each possible action.", "tokens": [50364, 682, 1186, 11, 257, 31959, 3142, 3897, 307, 291, 500, 380, 747, 364, 3069, 13, 509, 976, 257, 7316, 295, 5909, 293, 550, 291, 1888, 264, 3069, 294, 512, 636, 490, 264, 7316, 13, 50764, 50764, 583, 510, 11, 291, 458, 11, 291, 445, 362, 281, 747, 364, 3069, 13, 50964, 50964, 759, 264, 1230, 295, 5909, 307, 27706, 11, 550, 341, 3895, 3209, 307, 341, 3897, 3209, 307, 1936, 257, 1508, 9902, 13, 51314, 51314, 400, 309, 14725, 257, 3840, 295, 13444, 337, 1184, 1944, 3069, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11540980961011804, "compression_ratio": 1.7239819004524888, "no_speech_prob": 6.4388113969471306e-06}, {"id": 1091, "seek": 666806, "start": 6668.06, "end": 6676.06, "text": " And then you take one of the actions probabilistically or deterministically. Deterministically, you just take the action with the highest score.", "tokens": [50364, 400, 550, 291, 747, 472, 295, 264, 5909, 31959, 20458, 420, 15957, 20458, 13, 4237, 966, 259, 20458, 11, 291, 445, 747, 264, 3069, 365, 264, 6343, 6175, 13, 50764, 50764, 8736, 5177, 20458, 11, 291, 393, 6889, 4650, 281, 264, 6175, 13, 400, 550, 291, 1190, 807, 428, 1868, 2316, 293, 291, 1066, 516, 13, 51164, 51164, 1033, 13, 407, 1553, 264, 3897, 4984, 11, 550, 264, 3069, 307, 445, 733, 295, 485, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.13167664943597254, "compression_ratio": 1.7564766839378239, "no_speech_prob": 5.594156391452998e-06}, {"id": 1092, "seek": 666806, "start": 6676.06, "end": 6684.06, "text": " Probabilistically, you can sample according to the score. And then you run through your front model and you keep going.", "tokens": [50364, 400, 550, 291, 747, 472, 295, 264, 5909, 31959, 20458, 420, 15957, 20458, 13, 4237, 966, 259, 20458, 11, 291, 445, 747, 264, 3069, 365, 264, 6343, 6175, 13, 50764, 50764, 8736, 5177, 20458, 11, 291, 393, 6889, 4650, 281, 264, 6175, 13, 400, 550, 291, 1190, 807, 428, 1868, 2316, 293, 291, 1066, 516, 13, 51164, 51164, 1033, 13, 407, 1553, 264, 3897, 4984, 11, 550, 264, 3069, 307, 445, 733, 295, 485, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.13167664943597254, "compression_ratio": 1.7564766839378239, "no_speech_prob": 5.594156391452998e-06}, {"id": 1093, "seek": 666806, "start": 6684.06, "end": 6690.06, "text": " Okay. So without the policy connection, then the action is just kind of...", "tokens": [50364, 400, 550, 291, 747, 472, 295, 264, 5909, 31959, 20458, 420, 15957, 20458, 13, 4237, 966, 259, 20458, 11, 291, 445, 747, 264, 3069, 365, 264, 6343, 6175, 13, 50764, 50764, 8736, 5177, 20458, 11, 291, 393, 6889, 4650, 281, 264, 6175, 13, 400, 550, 291, 1190, 807, 428, 1868, 2316, 293, 291, 1066, 516, 13, 51164, 51164, 1033, 13, 407, 1553, 264, 3897, 4984, 11, 550, 264, 3069, 307, 445, 733, 295, 485, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.13167664943597254, "compression_ratio": 1.7564766839378239, "no_speech_prob": 5.594156391452998e-06}, {"id": 1094, "seek": 669006, "start": 6690.06, "end": 6705.06, "text": " Is a latent variable. So you have to optimize with respect to the latent variable to find its optimal value. So you have this kind of diagram now where the actions are not produced by neural net.", "tokens": [50364, 1119, 257, 48994, 7006, 13, 407, 291, 362, 281, 19719, 365, 3104, 281, 264, 48994, 7006, 281, 915, 1080, 16252, 2158, 13, 407, 291, 362, 341, 733, 295, 10686, 586, 689, 264, 5909, 366, 406, 7126, 538, 18161, 2533, 13, 51114, 51114, 821, 366, 48994, 9102, 300, 291, 362, 281, 2573, 484, 337, 633, 565, 291, 1190, 428, 2316, 13, 51414, 51414, 509, 362, 281, 2573, 484, 437, 311, 264, 1151, 8310, 295, 3069, 281, 17522, 452, 2063, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11443349539515484, "compression_ratio": 1.801980198019802, "no_speech_prob": 6.438681793952128e-06}, {"id": 1095, "seek": 669006, "start": 6705.06, "end": 6711.06, "text": " There are latent variables that you have to figure out for every time you run your model.", "tokens": [50364, 1119, 257, 48994, 7006, 13, 407, 291, 362, 281, 19719, 365, 3104, 281, 264, 48994, 7006, 281, 915, 1080, 16252, 2158, 13, 407, 291, 362, 341, 733, 295, 10686, 586, 689, 264, 5909, 366, 406, 7126, 538, 18161, 2533, 13, 51114, 51114, 821, 366, 48994, 9102, 300, 291, 362, 281, 2573, 484, 337, 633, 565, 291, 1190, 428, 2316, 13, 51414, 51414, 509, 362, 281, 2573, 484, 437, 311, 264, 1151, 8310, 295, 3069, 281, 17522, 452, 2063, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11443349539515484, "compression_ratio": 1.801980198019802, "no_speech_prob": 6.438681793952128e-06}, {"id": 1096, "seek": 669006, "start": 6711.06, "end": 6715.06, "text": " You have to figure out what's the best sequence of action to minimize my cost.", "tokens": [50364, 1119, 257, 48994, 7006, 13, 407, 291, 362, 281, 19719, 365, 3104, 281, 264, 48994, 7006, 281, 915, 1080, 16252, 2158, 13, 407, 291, 362, 341, 733, 295, 10686, 586, 689, 264, 5909, 366, 406, 7126, 538, 18161, 2533, 13, 51114, 51114, 821, 366, 48994, 9102, 300, 291, 362, 281, 2573, 484, 337, 633, 565, 291, 1190, 428, 2316, 13, 51414, 51414, 509, 362, 281, 2573, 484, 437, 311, 264, 1151, 8310, 295, 3069, 281, 17522, 452, 2063, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11443349539515484, "compression_ratio": 1.801980198019802, "no_speech_prob": 6.438681793952128e-06}, {"id": 1097, "seek": 671506, "start": 6715.06, "end": 6725.06, "text": " You have to basically do this, for example, by gradient descent, figuring out the sequence of A that will minimize the sum of the C's over the trajectory.", "tokens": [50364, 509, 362, 281, 1936, 360, 341, 11, 337, 1365, 11, 538, 16235, 23475, 11, 15213, 484, 264, 8310, 295, 316, 300, 486, 17522, 264, 2408, 295, 264, 383, 311, 670, 264, 21512, 13, 50864, 50864, 663, 311, 1219, 2316, 35521, 1969, 13, 400, 550, 264, 472, 365, 264, 3897, 3209, 307, 1219, 2047, 1969, 11, 4476, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12199275220026735, "compression_ratio": 1.4574468085106382, "no_speech_prob": 1.8055558030027896e-05}, {"id": 1098, "seek": 671506, "start": 6725.06, "end": 6739.06, "text": " That's called model predictive control. And then the one with the policy network is called direct control, essentially.", "tokens": [50364, 509, 362, 281, 1936, 360, 341, 11, 337, 1365, 11, 538, 16235, 23475, 11, 15213, 484, 264, 8310, 295, 316, 300, 486, 17522, 264, 2408, 295, 264, 383, 311, 670, 264, 21512, 13, 50864, 50864, 663, 311, 1219, 2316, 35521, 1969, 13, 400, 550, 264, 472, 365, 264, 3897, 3209, 307, 1219, 2047, 1969, 11, 4476, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12199275220026735, "compression_ratio": 1.4574468085106382, "no_speech_prob": 1.8055558030027896e-05}, {"id": 1099, "seek": 673906, "start": 6739.06, "end": 6750.06, "text": " Professor, you say that during inference, we need to minimize the energy to get the final value.", "tokens": [50364, 8419, 11, 291, 584, 300, 1830, 38253, 11, 321, 643, 281, 17522, 264, 2281, 281, 483, 264, 2572, 2158, 13, 50914, 50914, 583, 1392, 11, 456, 366, 732, 1651, 13, 1485, 11, 1582, 380, 309, 747, 886, 709, 565, 1830, 38253, 293, 576, 309, 312, 4420, 337, 957, 565, 3652, 30, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.15446154854514382, "compression_ratio": 1.41875, "no_speech_prob": 1.5203454495349433e-05}, {"id": 1100, "seek": 673906, "start": 6750.06, "end": 6759.06, "text": " But okay, there are two questions. One, won't it take too much time during inference and would it be useful for real time systems?", "tokens": [50364, 8419, 11, 291, 584, 300, 1830, 38253, 11, 321, 643, 281, 17522, 264, 2281, 281, 483, 264, 2572, 2158, 13, 50914, 50914, 583, 1392, 11, 456, 366, 732, 1651, 13, 1485, 11, 1582, 380, 309, 747, 886, 709, 565, 1830, 38253, 293, 576, 309, 312, 4420, 337, 957, 565, 3652, 30, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.15446154854514382, "compression_ratio": 1.41875, "no_speech_prob": 1.5203454495349433e-05}, {"id": 1101, "seek": 675906, "start": 6759.06, "end": 6773.06, "text": " And the second one is, since it's unrolled and you have to back propagate all the way through the beginning, it would have all the problems that we face in recurrent neural networks.", "tokens": [50364, 400, 264, 1150, 472, 307, 11, 1670, 309, 311, 517, 28850, 293, 291, 362, 281, 646, 48256, 439, 264, 636, 807, 264, 2863, 11, 309, 576, 362, 439, 264, 2740, 300, 321, 1851, 294, 18680, 1753, 18161, 9590, 13, 51064, 51064, 7587, 13, 407, 26742, 291, 434, 406, 516, 281, 483, 264, 912, 2740, 382, 291, 362, 365, 18680, 1753, 36170, 570, 428, 2128, 2316, 11, 26742, 704, 17988, 264, 15679, 295, 512, 957, 1185, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12600826025009154, "compression_ratio": 1.635135135135135, "no_speech_prob": 2.4824271349643823e-06}, {"id": 1102, "seek": 675906, "start": 6773.06, "end": 6783.06, "text": " Exactly. So presumably you're not going to get the same problems as you have with recurrent nets because your forward model, presumably implements the dynamics of some real system.", "tokens": [50364, 400, 264, 1150, 472, 307, 11, 1670, 309, 311, 517, 28850, 293, 291, 362, 281, 646, 48256, 439, 264, 636, 807, 264, 2863, 11, 309, 576, 362, 439, 264, 2740, 300, 321, 1851, 294, 18680, 1753, 18161, 9590, 13, 51064, 51064, 7587, 13, 407, 26742, 291, 434, 406, 516, 281, 483, 264, 912, 2740, 382, 291, 362, 365, 18680, 1753, 36170, 570, 428, 2128, 2316, 11, 26742, 704, 17988, 264, 15679, 295, 512, 957, 1185, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12600826025009154, "compression_ratio": 1.635135135135135, "no_speech_prob": 2.4824271349643823e-06}, {"id": 1103, "seek": 678306, "start": 6783.06, "end": 6791.06, "text": " So it might not have the issues of sort of non-invertibility that you have. If it's a physical system, it's probably going to be reversible.", "tokens": [50364, 407, 309, 1062, 406, 362, 264, 2663, 295, 1333, 295, 2107, 12, 259, 3281, 2841, 300, 291, 362, 13, 759, 309, 311, 257, 4001, 1185, 11, 309, 311, 1391, 516, 281, 312, 44788, 13, 50764, 50764, 407, 291, 815, 406, 362, 264, 912, 2734, 382, 365, 3890, 18680, 1753, 36170, 13, 583, 1338, 11, 291, 434, 7170, 264, 912, 2740, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.06331806182861328, "compression_ratio": 1.4970059880239521, "no_speech_prob": 6.338676485029282e-06}, {"id": 1104, "seek": 678306, "start": 6791.06, "end": 6798.06, "text": " So you may not have the same issue as with regular recurrent nets. But yeah, you're facing the same problems.", "tokens": [50364, 407, 309, 1062, 406, 362, 264, 2663, 295, 1333, 295, 2107, 12, 259, 3281, 2841, 300, 291, 362, 13, 759, 309, 311, 257, 4001, 1185, 11, 309, 311, 1391, 516, 281, 312, 44788, 13, 50764, 50764, 407, 291, 815, 406, 362, 264, 912, 2734, 382, 365, 3890, 18680, 1753, 36170, 13, 583, 1338, 11, 291, 434, 7170, 264, 912, 2740, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.06331806182861328, "compression_ratio": 1.4970059880239521, "no_speech_prob": 6.338676485029282e-06}, {"id": 1105, "seek": 679806, "start": 6798.06, "end": 6818.06, "text": " Now, in real time situations, you use a form of this called receding horizon planning.", "tokens": [50364, 823, 11, 294, 957, 565, 6851, 11, 291, 764, 257, 1254, 295, 341, 1219, 850, 9794, 18046, 5038, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.0857226796772169, "compression_ratio": 1.0617283950617284, "no_speech_prob": 5.954383595963009e-06}, {"id": 1106, "seek": 681806, "start": 6818.06, "end": 6830.06, "text": " So receding horizon planning is when you are in a real time situation, your system will run its forward model for a few steps in the future.", "tokens": [50364, 407, 850, 9794, 18046, 5038, 307, 562, 291, 366, 294, 257, 957, 565, 2590, 11, 428, 1185, 486, 1190, 1080, 2128, 2316, 337, 257, 1326, 4439, 294, 264, 2027, 13, 50964, 50964, 286, 500, 380, 458, 11, 718, 311, 584, 257, 1326, 3949, 13, 1033, 13, 318, 30664, 356, 867, 4439, 281, 6069, 337, 257, 1326, 3949, 13, 663, 311, 428, 18046, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.13319570626785507, "compression_ratio": 1.4942528735632183, "no_speech_prob": 1.0285053576808423e-05}, {"id": 1107, "seek": 681806, "start": 6830.06, "end": 6838.06, "text": " I don't know, let's say a few seconds. Okay. Sufficiently many steps to predict for a few seconds. That's your horizon.", "tokens": [50364, 407, 850, 9794, 18046, 5038, 307, 562, 291, 366, 294, 257, 957, 565, 2590, 11, 428, 1185, 486, 1190, 1080, 2128, 2316, 337, 257, 1326, 4439, 294, 264, 2027, 13, 50964, 50964, 286, 500, 380, 458, 11, 718, 311, 584, 257, 1326, 3949, 13, 1033, 13, 318, 30664, 356, 867, 4439, 281, 6069, 337, 257, 1326, 3949, 13, 663, 311, 428, 18046, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.13319570626785507, "compression_ratio": 1.4942528735632183, "no_speech_prob": 1.0285053576808423e-05}, {"id": 1108, "seek": 683806, "start": 6838.06, "end": 6848.06, "text": " Then you do this model predictive control by optimizing, finding the optimal A that minimizes your cost, your estimated cost, according to your model.", "tokens": [50364, 1396, 291, 360, 341, 2316, 35521, 1969, 538, 40425, 11, 5006, 264, 16252, 316, 300, 4464, 5660, 428, 2063, 11, 428, 14109, 2063, 11, 4650, 281, 428, 2316, 13, 50864, 50864, 509, 2378, 380, 2726, 364, 3069, 1939, 13, 509, 445, 1190, 428, 6920, 2316, 281, 652, 300, 17630, 13, 51164, 51164, 407, 807, 19618, 365, 3104, 281, 316, 11, 291, 915, 264, 8310, 295, 316, 300, 5028, 5660, 428, 2063, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10280766612605045, "compression_ratio": 1.702020202020202, "no_speech_prob": 1.1840325896628201e-05}, {"id": 1109, "seek": 683806, "start": 6848.06, "end": 6854.06, "text": " You haven't taken an action yet. You just run your internal model to make that prediction.", "tokens": [50364, 1396, 291, 360, 341, 2316, 35521, 1969, 538, 40425, 11, 5006, 264, 16252, 316, 300, 4464, 5660, 428, 2063, 11, 428, 14109, 2063, 11, 4650, 281, 428, 2316, 13, 50864, 50864, 509, 2378, 380, 2726, 364, 3069, 1939, 13, 509, 445, 1190, 428, 6920, 2316, 281, 652, 300, 17630, 13, 51164, 51164, 407, 807, 19618, 365, 3104, 281, 316, 11, 291, 915, 264, 8310, 295, 316, 300, 5028, 5660, 428, 2063, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10280766612605045, "compression_ratio": 1.702020202020202, "no_speech_prob": 1.1840325896628201e-05}, {"id": 1110, "seek": 683806, "start": 6854.06, "end": 6860.06, "text": " So through optimization with respect to A, you find the sequence of A that optimizes your cost.", "tokens": [50364, 1396, 291, 360, 341, 2316, 35521, 1969, 538, 40425, 11, 5006, 264, 16252, 316, 300, 4464, 5660, 428, 2063, 11, 428, 14109, 2063, 11, 4650, 281, 428, 2316, 13, 50864, 50864, 509, 2378, 380, 2726, 364, 3069, 1939, 13, 509, 445, 1190, 428, 6920, 2316, 281, 652, 300, 17630, 13, 51164, 51164, 407, 807, 19618, 365, 3104, 281, 316, 11, 291, 915, 264, 8310, 295, 316, 300, 5028, 5660, 428, 2063, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10280766612605045, "compression_ratio": 1.702020202020202, "no_speech_prob": 1.1840325896628201e-05}, {"id": 1111, "seek": 686006, "start": 6860.06, "end": 6869.06, "text": " And then you take the first action in that A and then you do it again. Okay. So with the A you took, observe the state of the world now.", "tokens": [50364, 400, 550, 291, 747, 264, 700, 3069, 294, 300, 316, 293, 550, 291, 360, 309, 797, 13, 1033, 13, 407, 365, 264, 316, 291, 1890, 11, 11441, 264, 1785, 295, 264, 1002, 586, 13, 50814, 50814, 509, 362, 257, 777, 1785, 11, 597, 291, 11441, 490, 428, 14840, 13, 400, 586, 7149, 264, 1399, 13, 51164, 51164, 8950, 428, 2128, 2316, 11, 257, 1230, 295, 4439, 294, 264, 2027, 13, 35013, 1125, 264, 8310, 295, 5909, 281, 17522, 428, 2063, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12512899286606732, "compression_ratio": 1.6231884057971016, "no_speech_prob": 5.862239049747586e-06}, {"id": 1112, "seek": 686006, "start": 6869.06, "end": 6876.06, "text": " You have a new state, which you observe from your sensors. And now repeat the process.", "tokens": [50364, 400, 550, 291, 747, 264, 700, 3069, 294, 300, 316, 293, 550, 291, 360, 309, 797, 13, 1033, 13, 407, 365, 264, 316, 291, 1890, 11, 11441, 264, 1785, 295, 264, 1002, 586, 13, 50814, 50814, 509, 362, 257, 777, 1785, 11, 597, 291, 11441, 490, 428, 14840, 13, 400, 586, 7149, 264, 1399, 13, 51164, 51164, 8950, 428, 2128, 2316, 11, 257, 1230, 295, 4439, 294, 264, 2027, 13, 35013, 1125, 264, 8310, 295, 5909, 281, 17522, 428, 2063, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12512899286606732, "compression_ratio": 1.6231884057971016, "no_speech_prob": 5.862239049747586e-06}, {"id": 1113, "seek": 686006, "start": 6876.06, "end": 6883.06, "text": " Run your forward model, a number of steps in the future. Optimize the sequence of actions to minimize your cost.", "tokens": [50364, 400, 550, 291, 747, 264, 700, 3069, 294, 300, 316, 293, 550, 291, 360, 309, 797, 13, 1033, 13, 407, 365, 264, 316, 291, 1890, 11, 11441, 264, 1785, 295, 264, 1002, 586, 13, 50814, 50814, 509, 362, 257, 777, 1785, 11, 597, 291, 11441, 490, 428, 14840, 13, 400, 586, 7149, 264, 1399, 13, 51164, 51164, 8950, 428, 2128, 2316, 11, 257, 1230, 295, 4439, 294, 264, 2027, 13, 35013, 1125, 264, 8310, 295, 5909, 281, 17522, 428, 2063, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12512899286606732, "compression_ratio": 1.6231884057971016, "no_speech_prob": 5.862239049747586e-06}, {"id": 1114, "seek": 688306, "start": 6883.06, "end": 6893.06, "text": " Take the first action and do it again. So it can be expensive if your horizon is long, if your forward model is complicated.", "tokens": [50364, 3664, 264, 700, 3069, 293, 360, 309, 797, 13, 407, 309, 393, 312, 5124, 498, 428, 18046, 307, 938, 11, 498, 428, 2128, 2316, 307, 6179, 13, 50864, 50864, 400, 370, 300, 311, 562, 291, 643, 257, 2128, 2316, 11, 257, 3897, 3209, 13, 51114, 51114, 407, 264, 3897, 3209, 1936, 715, 4680, 341, 1379, 1399, 666, 257, 18161, 2533, 300, 3838, 14725, 264, 1151, 3069, 490, 264, 1785, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06699824977565456, "compression_ratio": 1.6326530612244898, "no_speech_prob": 1.2410669114615303e-05}, {"id": 1115, "seek": 688306, "start": 6893.06, "end": 6898.06, "text": " And so that's when you need a forward model, a policy network.", "tokens": [50364, 3664, 264, 700, 3069, 293, 360, 309, 797, 13, 407, 309, 393, 312, 5124, 498, 428, 18046, 307, 938, 11, 498, 428, 2128, 2316, 307, 6179, 13, 50864, 50864, 400, 370, 300, 311, 562, 291, 643, 257, 2128, 2316, 11, 257, 3897, 3209, 13, 51114, 51114, 407, 264, 3897, 3209, 1936, 715, 4680, 341, 1379, 1399, 666, 257, 18161, 2533, 300, 3838, 14725, 264, 1151, 3069, 490, 264, 1785, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06699824977565456, "compression_ratio": 1.6326530612244898, "no_speech_prob": 1.2410669114615303e-05}, {"id": 1116, "seek": 688306, "start": 6898.06, "end": 6906.06, "text": " So the policy network basically compiles this whole process into a neural net that directly produces the best action from the state.", "tokens": [50364, 3664, 264, 700, 3069, 293, 360, 309, 797, 13, 407, 309, 393, 312, 5124, 498, 428, 18046, 307, 938, 11, 498, 428, 2128, 2316, 307, 6179, 13, 50864, 50864, 400, 370, 300, 311, 562, 291, 643, 257, 2128, 2316, 11, 257, 3897, 3209, 13, 51114, 51114, 407, 264, 3897, 3209, 1936, 715, 4680, 341, 1379, 1399, 666, 257, 18161, 2533, 300, 3838, 14725, 264, 1151, 3069, 490, 264, 1785, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06699824977565456, "compression_ratio": 1.6326530612244898, "no_speech_prob": 1.2410669114615303e-05}, {"id": 1117, "seek": 690606, "start": 6906.06, "end": 6914.06, "text": " Okay. Which may or may not be possible, but it gives you a good guess. Now, to give you a concrete example,", "tokens": [50364, 1033, 13, 3013, 815, 420, 815, 406, 312, 1944, 11, 457, 309, 2709, 291, 257, 665, 2041, 13, 823, 11, 281, 976, 291, 257, 9859, 1365, 11, 50764, 50764, 456, 311, 364, 1880, 2638, 295, 3642, 538, 257, 24611, 22604, 8224, 36696, 300, 307, 294, 1873, 3609, 1219, 16682, 591, 12140, 15023, 13, 51114, 51114, 400, 415, 6686, 466, 732, 3652, 294, 264, 1952, 1575, 1219, 1185, 472, 293, 1185, 732, 13, 51414, 51414, 407, 1185, 472, 307, 264, 1399, 538, 597, 291, 747, 364, 3069, 1553, 1953, 13, 1033, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08104317313746402, "compression_ratio": 1.564516129032258, "no_speech_prob": 1.8339200323680416e-05}, {"id": 1118, "seek": 690606, "start": 6914.06, "end": 6921.06, "text": " there's an interesting series of books by a Nobel Prize winning economist that is in New York called Danny Kahneman.", "tokens": [50364, 1033, 13, 3013, 815, 420, 815, 406, 312, 1944, 11, 457, 309, 2709, 291, 257, 665, 2041, 13, 823, 11, 281, 976, 291, 257, 9859, 1365, 11, 50764, 50764, 456, 311, 364, 1880, 2638, 295, 3642, 538, 257, 24611, 22604, 8224, 36696, 300, 307, 294, 1873, 3609, 1219, 16682, 591, 12140, 15023, 13, 51114, 51114, 400, 415, 6686, 466, 732, 3652, 294, 264, 1952, 1575, 1219, 1185, 472, 293, 1185, 732, 13, 51414, 51414, 407, 1185, 472, 307, 264, 1399, 538, 597, 291, 747, 364, 3069, 1553, 1953, 13, 1033, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08104317313746402, "compression_ratio": 1.564516129032258, "no_speech_prob": 1.8339200323680416e-05}, {"id": 1119, "seek": 690606, "start": 6921.06, "end": 6927.06, "text": " And he talks about two systems in the human mind called system one and system two.", "tokens": [50364, 1033, 13, 3013, 815, 420, 815, 406, 312, 1944, 11, 457, 309, 2709, 291, 257, 665, 2041, 13, 823, 11, 281, 976, 291, 257, 9859, 1365, 11, 50764, 50764, 456, 311, 364, 1880, 2638, 295, 3642, 538, 257, 24611, 22604, 8224, 36696, 300, 307, 294, 1873, 3609, 1219, 16682, 591, 12140, 15023, 13, 51114, 51114, 400, 415, 6686, 466, 732, 3652, 294, 264, 1952, 1575, 1219, 1185, 472, 293, 1185, 732, 13, 51414, 51414, 407, 1185, 472, 307, 264, 1399, 538, 597, 291, 747, 364, 3069, 1553, 1953, 13, 1033, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08104317313746402, "compression_ratio": 1.564516129032258, "no_speech_prob": 1.8339200323680416e-05}, {"id": 1120, "seek": 690606, "start": 6927.06, "end": 6935.06, "text": " So system one is the process by which you take an action without thinking. Okay.", "tokens": [50364, 1033, 13, 3013, 815, 420, 815, 406, 312, 1944, 11, 457, 309, 2709, 291, 257, 665, 2041, 13, 823, 11, 281, 976, 291, 257, 9859, 1365, 11, 50764, 50764, 456, 311, 364, 1880, 2638, 295, 3642, 538, 257, 24611, 22604, 8224, 36696, 300, 307, 294, 1873, 3609, 1219, 16682, 591, 12140, 15023, 13, 51114, 51114, 400, 415, 6686, 466, 732, 3652, 294, 264, 1952, 1575, 1219, 1185, 472, 293, 1185, 732, 13, 51414, 51414, 407, 1185, 472, 307, 264, 1399, 538, 597, 291, 747, 364, 3069, 1553, 1953, 13, 1033, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08104317313746402, "compression_ratio": 1.564516129032258, "no_speech_prob": 1.8339200323680416e-05}, {"id": 1121, "seek": 693506, "start": 6935.06, "end": 6941.06, "text": " You're a very experienced driver and you can drive your car without even paying attention by talking to someone next to you.", "tokens": [50364, 509, 434, 257, 588, 6751, 6787, 293, 291, 393, 3332, 428, 1032, 1553, 754, 6229, 3202, 538, 1417, 281, 1580, 958, 281, 291, 13, 50664, 50664, 509, 500, 380, 767, 643, 281, 519, 466, 309, 13, 1033, 13, 8910, 732, 307, 544, 1333, 295, 30515, 5038, 13, 51014, 51014, 407, 1185, 732, 307, 562, 291, 764, 428, 6920, 2316, 295, 264, 1002, 281, 733, 295, 6069, 294, 7295, 437, 311, 516, 281, 1051, 2286, 11, 51364, 51364, 1333, 295, 38736, 437, 311, 516, 281, 1051, 293, 550, 747, 257, 30515, 3069, 300, 291, 519, 307, 516, 281, 312, 264, 558, 472, 4650, 281, 428, 2316, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05370453054254705, "compression_ratio": 1.786764705882353, "no_speech_prob": 6.438507625716738e-06}, {"id": 1122, "seek": 693506, "start": 6941.06, "end": 6948.06, "text": " You don't actually need to think about it. Okay. System two is more sort of deliberate planning.", "tokens": [50364, 509, 434, 257, 588, 6751, 6787, 293, 291, 393, 3332, 428, 1032, 1553, 754, 6229, 3202, 538, 1417, 281, 1580, 958, 281, 291, 13, 50664, 50664, 509, 500, 380, 767, 643, 281, 519, 466, 309, 13, 1033, 13, 8910, 732, 307, 544, 1333, 295, 30515, 5038, 13, 51014, 51014, 407, 1185, 732, 307, 562, 291, 764, 428, 6920, 2316, 295, 264, 1002, 281, 733, 295, 6069, 294, 7295, 437, 311, 516, 281, 1051, 2286, 11, 51364, 51364, 1333, 295, 38736, 437, 311, 516, 281, 1051, 293, 550, 747, 257, 30515, 3069, 300, 291, 519, 307, 516, 281, 312, 264, 558, 472, 4650, 281, 428, 2316, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05370453054254705, "compression_ratio": 1.786764705882353, "no_speech_prob": 6.438507625716738e-06}, {"id": 1123, "seek": 693506, "start": 6948.06, "end": 6955.06, "text": " So system two is when you use your internal model of the world to kind of predict in advance what's going to happen ahead,", "tokens": [50364, 509, 434, 257, 588, 6751, 6787, 293, 291, 393, 3332, 428, 1032, 1553, 754, 6229, 3202, 538, 1417, 281, 1580, 958, 281, 291, 13, 50664, 50664, 509, 500, 380, 767, 643, 281, 519, 466, 309, 13, 1033, 13, 8910, 732, 307, 544, 1333, 295, 30515, 5038, 13, 51014, 51014, 407, 1185, 732, 307, 562, 291, 764, 428, 6920, 2316, 295, 264, 1002, 281, 733, 295, 6069, 294, 7295, 437, 311, 516, 281, 1051, 2286, 11, 51364, 51364, 1333, 295, 38736, 437, 311, 516, 281, 1051, 293, 550, 747, 257, 30515, 3069, 300, 291, 519, 307, 516, 281, 312, 264, 558, 472, 4650, 281, 428, 2316, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05370453054254705, "compression_ratio": 1.786764705882353, "no_speech_prob": 6.438507625716738e-06}, {"id": 1124, "seek": 693506, "start": 6955.06, "end": 6963.06, "text": " sort of foresee what's going to happen and then take a deliberate action that you think is going to be the right one according to your model.", "tokens": [50364, 509, 434, 257, 588, 6751, 6787, 293, 291, 393, 3332, 428, 1032, 1553, 754, 6229, 3202, 538, 1417, 281, 1580, 958, 281, 291, 13, 50664, 50664, 509, 500, 380, 767, 643, 281, 519, 466, 309, 13, 1033, 13, 8910, 732, 307, 544, 1333, 295, 30515, 5038, 13, 51014, 51014, 407, 1185, 732, 307, 562, 291, 764, 428, 6920, 2316, 295, 264, 1002, 281, 733, 295, 6069, 294, 7295, 437, 311, 516, 281, 1051, 2286, 11, 51364, 51364, 1333, 295, 38736, 437, 311, 516, 281, 1051, 293, 550, 747, 257, 30515, 3069, 300, 291, 519, 307, 516, 281, 312, 264, 558, 472, 4650, 281, 428, 2316, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05370453054254705, "compression_ratio": 1.786764705882353, "no_speech_prob": 6.438507625716738e-06}, {"id": 1125, "seek": 696306, "start": 6963.06, "end": 6972.06, "text": " So it's more like reasoning. Okay. You can think of this, you know, optimization with respect to actions to minimize an objective as a form of reasoning.", "tokens": [50364, 407, 309, 311, 544, 411, 21577, 13, 1033, 13, 509, 393, 519, 295, 341, 11, 291, 458, 11, 19618, 365, 3104, 281, 5909, 281, 17522, 364, 10024, 382, 257, 1254, 295, 21577, 13, 50814, 50814, 400, 321, 2825, 466, 341, 949, 13, 407, 1936, 2316, 35521, 1969, 307, 562, 291, 500, 380, 362, 257, 3897, 11, 291, 2378, 380, 3264, 264, 5389, 11, 51264, 51264, 291, 458, 437, 428, 2063, 2445, 307, 11, 291, 362, 257, 1238, 665, 2316, 295, 264, 1002, 11, 457, 291, 500, 380, 458, 577, 281, 4515, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05898641546567281, "compression_ratio": 1.6434426229508197, "no_speech_prob": 4.4943135435460135e-06}, {"id": 1126, "seek": 696306, "start": 6972.06, "end": 6981.06, "text": " And we talked about this before. So basically model predictive control is when you don't have a policy, you haven't learned the skill,", "tokens": [50364, 407, 309, 311, 544, 411, 21577, 13, 1033, 13, 509, 393, 519, 295, 341, 11, 291, 458, 11, 19618, 365, 3104, 281, 5909, 281, 17522, 364, 10024, 382, 257, 1254, 295, 21577, 13, 50814, 50814, 400, 321, 2825, 466, 341, 949, 13, 407, 1936, 2316, 35521, 1969, 307, 562, 291, 500, 380, 362, 257, 3897, 11, 291, 2378, 380, 3264, 264, 5389, 11, 51264, 51264, 291, 458, 437, 428, 2063, 2445, 307, 11, 291, 362, 257, 1238, 665, 2316, 295, 264, 1002, 11, 457, 291, 500, 380, 458, 577, 281, 4515, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05898641546567281, "compression_ratio": 1.6434426229508197, "no_speech_prob": 4.4943135435460135e-06}, {"id": 1127, "seek": 696306, "start": 6981.06, "end": 6986.06, "text": " you know what your cost function is, you have a pretty good model of the world, but you don't know how to react.", "tokens": [50364, 407, 309, 311, 544, 411, 21577, 13, 1033, 13, 509, 393, 519, 295, 341, 11, 291, 458, 11, 19618, 365, 3104, 281, 5909, 281, 17522, 364, 10024, 382, 257, 1254, 295, 21577, 13, 50814, 50814, 400, 321, 2825, 466, 341, 949, 13, 407, 1936, 2316, 35521, 1969, 307, 562, 291, 500, 380, 362, 257, 3897, 11, 291, 2378, 380, 3264, 264, 5389, 11, 51264, 51264, 291, 458, 437, 428, 2063, 2445, 307, 11, 291, 362, 257, 1238, 665, 2316, 295, 264, 1002, 11, 457, 291, 500, 380, 458, 577, 281, 4515, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05898641546567281, "compression_ratio": 1.6434426229508197, "no_speech_prob": 4.4943135435460135e-06}, {"id": 1128, "seek": 698606, "start": 6986.06, "end": 6996.06, "text": " Okay. So a beginner chess player would be like that. You look at the chess game and you have to think about all possibilities before you play", "tokens": [50364, 1033, 13, 407, 257, 22080, 24122, 4256, 576, 312, 411, 300, 13, 509, 574, 412, 264, 24122, 1216, 293, 291, 362, 281, 519, 466, 439, 12178, 949, 291, 862, 50864, 50864, 570, 11, 291, 458, 11, 291, 500, 380, 458, 689, 281, 862, 13, 407, 291, 362, 281, 733, 295, 3811, 439, 264, 12178, 13, 51164, 51164, 759, 291, 434, 364, 5844, 4256, 293, 291, 862, 1970, 257, 22080, 11, 291, 458, 4258, 437, 281, 862, 13, 509, 500, 380, 362, 281, 519, 466, 309, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06427299711439345, "compression_ratio": 1.7932692307692308, "no_speech_prob": 1.7775899323169142e-05}, {"id": 1129, "seek": 698606, "start": 6996.06, "end": 7002.06, "text": " because, you know, you don't know where to play. So you have to kind of imagine all the possibilities.", "tokens": [50364, 1033, 13, 407, 257, 22080, 24122, 4256, 576, 312, 411, 300, 13, 509, 574, 412, 264, 24122, 1216, 293, 291, 362, 281, 519, 466, 439, 12178, 949, 291, 862, 50864, 50864, 570, 11, 291, 458, 11, 291, 500, 380, 458, 689, 281, 862, 13, 407, 291, 362, 281, 733, 295, 3811, 439, 264, 12178, 13, 51164, 51164, 759, 291, 434, 364, 5844, 4256, 293, 291, 862, 1970, 257, 22080, 11, 291, 458, 4258, 437, 281, 862, 13, 509, 500, 380, 362, 281, 519, 466, 309, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06427299711439345, "compression_ratio": 1.7932692307692308, "no_speech_prob": 1.7775899323169142e-05}, {"id": 1130, "seek": 698606, "start": 7002.06, "end": 7010.06, "text": " If you're an expert player and you play against a beginner, you know immediately what to play. You don't have to think about it.", "tokens": [50364, 1033, 13, 407, 257, 22080, 24122, 4256, 576, 312, 411, 300, 13, 509, 574, 412, 264, 24122, 1216, 293, 291, 362, 281, 519, 466, 439, 12178, 949, 291, 862, 50864, 50864, 570, 11, 291, 458, 11, 291, 500, 380, 458, 689, 281, 862, 13, 407, 291, 362, 281, 733, 295, 3811, 439, 264, 12178, 13, 51164, 51164, 759, 291, 434, 364, 5844, 4256, 293, 291, 862, 1970, 257, 22080, 11, 291, 458, 4258, 437, 281, 862, 13, 509, 500, 380, 362, 281, 519, 466, 309, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06427299711439345, "compression_ratio": 1.7932692307692308, "no_speech_prob": 1.7775899323169142e-05}, {"id": 1131, "seek": 701006, "start": 7010.06, "end": 7016.06, "text": " I don't know if you've played simultaneous games against a master or grandmaster at chess.", "tokens": [50364, 286, 500, 380, 458, 498, 291, 600, 3737, 46218, 2813, 1970, 257, 4505, 420, 2697, 21640, 412, 24122, 13, 50664, 50664, 316, 2697, 21640, 393, 862, 1970, 2625, 561, 293, 4224, 552, 294, 257, 1326, 2077, 570, 264, 4256, 393, 352, 490, 257, 24122, 11, 291, 458, 11, 472, 10620, 281, 1071, 51264, 51264, 293, 445, 4258, 862, 13, 467, 311, 2584, 28897, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.15753036386826458, "compression_ratio": 1.5372340425531914, "no_speech_prob": 1.0288817975379061e-05}, {"id": 1132, "seek": 701006, "start": 7016.06, "end": 7028.06, "text": " A grandmaster can play against 50 people and beat them in a few minutes because the player can go from a chess, you know, one opponent to another", "tokens": [50364, 286, 500, 380, 458, 498, 291, 600, 3737, 46218, 2813, 1970, 257, 4505, 420, 2697, 21640, 412, 24122, 13, 50664, 50664, 316, 2697, 21640, 393, 862, 1970, 2625, 561, 293, 4224, 552, 294, 257, 1326, 2077, 570, 264, 4256, 393, 352, 490, 257, 24122, 11, 291, 458, 11, 472, 10620, 281, 1071, 51264, 51264, 293, 445, 4258, 862, 13, 467, 311, 2584, 28897, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.15753036386826458, "compression_ratio": 1.5372340425531914, "no_speech_prob": 1.0288817975379061e-05}, {"id": 1133, "seek": 701006, "start": 7028.06, "end": 7032.06, "text": " and just immediately play. It's completely reactive.", "tokens": [50364, 286, 500, 380, 458, 498, 291, 600, 3737, 46218, 2813, 1970, 257, 4505, 420, 2697, 21640, 412, 24122, 13, 50664, 50664, 316, 2697, 21640, 393, 862, 1970, 2625, 561, 293, 4224, 552, 294, 257, 1326, 2077, 570, 264, 4256, 393, 352, 490, 257, 24122, 11, 291, 458, 11, 472, 10620, 281, 1071, 51264, 51264, 293, 445, 4258, 862, 13, 467, 311, 2584, 28897, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.15753036386826458, "compression_ratio": 1.5372340425531914, "no_speech_prob": 1.0288817975379061e-05}, {"id": 1134, "seek": 703206, "start": 7032.06, "end": 7040.06, "text": " You actually don't need to think because, you know, they've kind of compiled that if you want in their knowledge of chess,", "tokens": [50364, 509, 767, 500, 380, 643, 281, 519, 570, 11, 291, 458, 11, 436, 600, 733, 295, 36548, 300, 498, 291, 528, 294, 641, 3601, 295, 24122, 11, 50764, 50764, 300, 436, 500, 380, 643, 281, 519, 562, 291, 536, 341, 733, 295, 2010, 295, 1858, 2590, 13, 51014, 51014, 407, 300, 311, 516, 490, 1185, 732, 281, 1185, 472, 13, 400, 562, 291, 1466, 257, 5389, 11, 412, 700, 291, 434, 36290, 293, 291, 362, 281, 519, 466, 309, 13, 51364, 51364, 509, 458, 11, 291, 434, 36290, 562, 291, 3332, 11, 291, 3332, 5692, 293, 291, 574, 412, 1203, 293, 291, 1689, 3202, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11077573758746506, "compression_ratio": 1.8771186440677967, "no_speech_prob": 7.646172889508307e-06}, {"id": 1135, "seek": 703206, "start": 7040.06, "end": 7045.06, "text": " that they don't need to think when you see this kind of type of easy situation.", "tokens": [50364, 509, 767, 500, 380, 643, 281, 519, 570, 11, 291, 458, 11, 436, 600, 733, 295, 36548, 300, 498, 291, 528, 294, 641, 3601, 295, 24122, 11, 50764, 50764, 300, 436, 500, 380, 643, 281, 519, 562, 291, 536, 341, 733, 295, 2010, 295, 1858, 2590, 13, 51014, 51014, 407, 300, 311, 516, 490, 1185, 732, 281, 1185, 472, 13, 400, 562, 291, 1466, 257, 5389, 11, 412, 700, 291, 434, 36290, 293, 291, 362, 281, 519, 466, 309, 13, 51364, 51364, 509, 458, 11, 291, 434, 36290, 562, 291, 3332, 11, 291, 3332, 5692, 293, 291, 574, 412, 1203, 293, 291, 1689, 3202, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11077573758746506, "compression_ratio": 1.8771186440677967, "no_speech_prob": 7.646172889508307e-06}, {"id": 1136, "seek": 703206, "start": 7045.06, "end": 7052.06, "text": " So that's going from system two to system one. And when you learn a skill, at first you're hesitant and you have to think about it.", "tokens": [50364, 509, 767, 500, 380, 643, 281, 519, 570, 11, 291, 458, 11, 436, 600, 733, 295, 36548, 300, 498, 291, 528, 294, 641, 3601, 295, 24122, 11, 50764, 50764, 300, 436, 500, 380, 643, 281, 519, 562, 291, 536, 341, 733, 295, 2010, 295, 1858, 2590, 13, 51014, 51014, 407, 300, 311, 516, 490, 1185, 732, 281, 1185, 472, 13, 400, 562, 291, 1466, 257, 5389, 11, 412, 700, 291, 434, 36290, 293, 291, 362, 281, 519, 466, 309, 13, 51364, 51364, 509, 458, 11, 291, 434, 36290, 562, 291, 3332, 11, 291, 3332, 5692, 293, 291, 574, 412, 1203, 293, 291, 1689, 3202, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11077573758746506, "compression_ratio": 1.8771186440677967, "no_speech_prob": 7.646172889508307e-06}, {"id": 1137, "seek": 703206, "start": 7052.06, "end": 7056.06, "text": " You know, you're hesitant when you drive, you drive slowly and you look at everything and you pay attention.", "tokens": [50364, 509, 767, 500, 380, 643, 281, 519, 570, 11, 291, 458, 11, 436, 600, 733, 295, 36548, 300, 498, 291, 528, 294, 641, 3601, 295, 24122, 11, 50764, 50764, 300, 436, 500, 380, 643, 281, 519, 562, 291, 536, 341, 733, 295, 2010, 295, 1858, 2590, 13, 51014, 51014, 407, 300, 311, 516, 490, 1185, 732, 281, 1185, 472, 13, 400, 562, 291, 1466, 257, 5389, 11, 412, 700, 291, 434, 36290, 293, 291, 362, 281, 519, 466, 309, 13, 51364, 51364, 509, 458, 11, 291, 434, 36290, 562, 291, 3332, 11, 291, 3332, 5692, 293, 291, 574, 412, 1203, 293, 291, 1689, 3202, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11077573758746506, "compression_ratio": 1.8771186440677967, "no_speech_prob": 7.646172889508307e-06}, {"id": 1138, "seek": 705606, "start": 7056.06, "end": 7062.06, "text": " And then when you're experimented, you can just react really quickly.", "tokens": [50364, 400, 550, 562, 291, 434, 5120, 292, 11, 291, 393, 445, 4515, 534, 2661, 13, 50664, 50664, 8537, 11, 291, 600, 2780, 490, 2316, 35521, 1969, 281, 1936, 3097, 428, 1065, 3897, 3209, 11, 498, 291, 486, 13, 51064, 51064], "temperature": 0.0, "avg_logprob": -0.0749315307253883, "compression_ratio": 1.3582089552238805, "no_speech_prob": 9.079808478418272e-06}, {"id": 1139, "seek": 705606, "start": 7062.06, "end": 7070.06, "text": " Basically, you've gone from model predictive control to basically training your own policy network, if you will.", "tokens": [50364, 400, 550, 562, 291, 434, 5120, 292, 11, 291, 393, 445, 4515, 534, 2661, 13, 50664, 50664, 8537, 11, 291, 600, 2780, 490, 2316, 35521, 1969, 281, 1936, 3097, 428, 1065, 3897, 3209, 11, 498, 291, 486, 13, 51064, 51064], "temperature": 0.0, "avg_logprob": -0.0749315307253883, "compression_ratio": 1.3582089552238805, "no_speech_prob": 9.079808478418272e-06}, {"id": 1140, "seek": 707006, "start": 7070.06, "end": 7086.06, "text": " And in the process of doing this, your skill has gone from a sort of deliberate planned conscious decision mechanism to a sort of subconscious automatic decision mechanism.", "tokens": [50364, 400, 294, 264, 1399, 295, 884, 341, 11, 428, 5389, 575, 2780, 490, 257, 1333, 295, 30515, 8589, 6648, 3537, 7513, 281, 257, 1333, 295, 27389, 12509, 3537, 7513, 13, 51164, 51164, 663, 311, 437, 37374, 11769, 775, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.09268882662750953, "compression_ratio": 1.4788732394366197, "no_speech_prob": 8.139007150020916e-06}, {"id": 1141, "seek": 707006, "start": 7086.06, "end": 7091.06, "text": " That's what acquiring expertise does.", "tokens": [50364, 400, 294, 264, 1399, 295, 884, 341, 11, 428, 5389, 575, 2780, 490, 257, 1333, 295, 30515, 8589, 6648, 3537, 7513, 281, 257, 1333, 295, 27389, 12509, 3537, 7513, 13, 51164, 51164, 663, 311, 437, 37374, 11769, 775, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.09268882662750953, "compression_ratio": 1.4788732394366197, "no_speech_prob": 8.139007150020916e-06}, {"id": 1142, "seek": 709106, "start": 7091.06, "end": 7100.06, "text": " And that's that's how you go from this diagram to that diagram where you have a policy that directly predicts the action without having to plan.", "tokens": [50364, 400, 300, 311, 300, 311, 577, 291, 352, 490, 341, 10686, 281, 300, 10686, 689, 291, 362, 257, 3897, 300, 3838, 6069, 82, 264, 3069, 1553, 1419, 281, 1393, 13, 50814, 50814, 2264, 11, 658, 309, 13, 2561, 13, 50964], "temperature": 0.0, "avg_logprob": -0.111917325428554, "compression_ratio": 1.3333333333333333, "no_speech_prob": 2.8816606572945602e-05}, {"id": 1143, "seek": 710006, "start": 7100.06, "end": 7125.06, "text": " OK, got it. Thanks.", "tokens": [50364, 2264, 11, 658, 309, 13, 2561, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2802703619003296, "compression_ratio": 0.7037037037037037, "no_speech_prob": 0.0009510601521469653}], "language": "en", "video_id": "Pgct8PKV7iw", "entity": "Yann LeCun"}}