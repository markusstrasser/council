{"video_id": "wBx3NZ0ucgc", "title": "3.7 Regularization to Reduce Overfitting | The problem of Overfitting -[Machine Learning|Andrew Ng]", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 712, "views": 213, "publish_date": "11/04/2022", "timestamp": 1661299200, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " Now you've seen a couple of different learning algorithms, linear regression and logistic regression. They work well for many tasks, but sometimes in an application the algorithm could run into a problem called overfitting, which can cause it to perform poorly. What I'd like to do in this video is to show you what is overfitting, as well as a closely related almost opposite problem called underfitting. And in the next videos after this, I'll share with you some techniques for addressing overfitting. In particular, there's a method called regularization. Very useful technique, I use it all the time, but regularization will help you minimize this overfitting problem and get your learning algorithms to work much better. So let's take a look at what is overfitting. To help us understand what is overfitting, let's take a look at a few examples. Let's go back to our original example of predicting housing prices with linear regression, where you want to predict the price as a function of the size of a house. To help us understand what is overfitting, let's take a look at a linear regression example. And I'm going to go back to our original running example of predicting housing prices with linear regression. Suppose your data set looks like this, with the input feature x being the size of the house and the value y they're trying to predict the price of the house. One thing you could do is fit a linear function to this data. And if you do that, you get a straight line fit to the data that maybe looks like this. But this isn't a very good model. Looking at the data, it seems pretty clear that as the size of the house increases, the housing prices kind of flatten out. So this algorithm does not fit the training data very well. The technical term for this is the model is underfitting the training data. Another term is the algorithm has high bias. You may have read in the news about some learning algorithms, really unfortunately, demonstrating bias against certain ethnicities or certain genders. In machine learning, the term bias has multiple meanings. Checking learning algorithms for bias based on characteristics such as gender or ethnicity is absolutely critical. But the term bias has a second technical meaning as well, which is the one I'm using here, which is if the algorithm has underfit the data, meaning that it's just not even able to fit the training set that well, that there's a clear pattern in the training data that the algorithm is just unable to capture. Another way to think of this form of bias is as if the learning algorithm has a very strong preconception or we say a very strong bias that the housing prices are going to be a completely linear function of the size, despite data to the contrary. So this preconception that the data is linear causes it to fit a straight line that fits the data poorly, leading it to underfit the data. Now let's look at a second variation of a model, which is if you instead fit a quadratic function to the data with two features, x and x squared, then when you fit the parameters w1 and w2, you can get a curve that fits the data somewhat better. Maybe it looks like this. So if you were to get a new house that's not in this set of five training examples, this model would probably do quite well on that new house. So if you're a real estate agent, the idea that you want your learning algorithm to do well, even on examples that are not on the training set, that's called generalization. Technically, we say that you want your learning algorithm to generalize well, which means to make good predictions, even on brand new examples that it has never seen before. So this quadratic model seems to fit the training set not perfectly, but pretty well. And I think it will generalize well to new examples. Now let's look at the other extreme. What if you were to fit a fourth order polynomial to the data? So you have x, x squared, x cubed, and x to the fourth all as features. With this fourth order polynomial, you can actually fit the curve that passes through all five of the training examples exactly. And you might get a curve that looks like this. This on one hand seems to do an extremely good job fitting the training data, because it passes through all of the training data perfectly. In fact, you'll be able to choose parameters that will result in the cost function being exactly equal to zero, because the errors are zero on all five training examples. But this is a very wiggly curve. It's going up and down all over the place. And if you have this house size right here, the model would predict that this house is cheaper than houses that are smaller than it. So we don't think that this is a particularly good model for predicting housing prices. The technical term is that we'll say this model has overfit the data, or this model has an overfitting problem. Because even though it fits the training set very well, it has fit the data almost too well, hence is overfit. And it does not look like this model will generalize to new examples that has never seen before. Another term for this is that the algorithm has high variance. In machine learning, many people will use the terms overfit and high variance almost interchangeably, and we use the terms underfit and high bias almost interchangeably. The intuition behind overfitting or high variance is that the algorithm is trying very, very hard to fit every single training example. And it turns out that if your training set were just even a little bit different, say one house was priced just a little bit more, a little bit less, then the function that the algorithm fits could end up being totally different. So if two different machine learning engineers were to fit this fourth order polynomial model to just slightly different data sets, they could end up with totally different predictions or highly variable predictions. And that's why we say the algorithm has high variance. Using this rightmost model with the one in the middle for the same house, it seems the middle model gives the much more reasonable prediction for price. There isn't really a name for this case in the middle, but I'm just going to call this just right because it is neither underfit nor overfit. So we can say that the goal of machine learning is to find a model that hopefully is neither underfitting nor overfitting. In other words, hopefully a model that has neither high bias nor high variance. When I think about underfitting and overfitting, high bias and high variance, I'm sometimes reminded of the children's story of Goldilocks and the three bears. In this children's tale, a girl called Goldilocks visits the home of a bear family. There's a bowl of porridge that's too cold to taste and so that's no good. There's also a bowl of porridge that's too hot to eat. So that's no good either. But there's a bowl of porridge that is neither too cold nor too hot. The temperature is in the middle, which is just right to eat. So to recap, if you have too many features, like the full water polynomial on the right, then the model may fit the training set well, but almost too well or overfit in the high variance. On the flip side, if you have too few features, then in this example, like the one on the left, it underfits and has high bias. And in this example, using quadratic features x and x squared, that seems to be just right. So far, we've looked at underfitting and overfitting for linear regression model. Similarly, overfitting applies to classification as well. Here's a classification example with two features x1 and x2, where x1 is maybe the tumor size and x2 is the age of patient. And we're trying to classify if a tumor is malignant or benign, as denoted by these crosses and circles. One thing you can do is fit a logistic regression model, just a simple model like this, where as usual, g is the sigmoid function, and this term here inside is z. So if you do that, you end up with a straight line as the decision boundary. This is the line where z is equal to zero that separates the positive and negative examples. This straight line doesn't look terrible. It looks kind of okay, but it doesn't look like a very good fit to the data either. So this is an example of underfitting or of high bias. Let's look at another example. If you were to add to your features these quadratic terms, then z becomes this new term in the middle and the decision boundary, that is where z equals zero, can look more like this, more like an ellipse or part of an ellipse. And this is a pretty good fit to the data, even though it does not perfectly classify every single training sample in the training set. Notice how some of these crosses get classified among the circles. But this model looks pretty good. I'm going to call it just right, and it looks like this will generalize pretty well to new patients. And finally, at the other extreme, if you were to fit a very high order polynomial with many, many features like these, then the model may try really hard and can't hold or twist itself to find a decision boundary that fits your training data perfectly. Having all these higher order polynomial features allows the algorithm to choose this really overly complex decision boundary. If the features are tumor size and age, and you're trying to classify tumors as malignant or benign, then this doesn't really look like a very good model for making predictions. So once again, this is an instance of overfitting and high variance, because this model, despite doing very well in the training set, doesn't look like it will generalize well to new examples. So now you've seen how an algorithm can underfit or have high bias or overfit and have high variance. You may want to know how you can get a model that is just right. In the next video, we'll look at some ways you can address the issue of overfitting. We'll also touch on some ideas relevant for accessing underfitting. Let's go on to the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.28, "text": " Now you've seen a couple of different learning algorithms, linear regression and logistic", "tokens": [50364, 823, 291, 600, 1612, 257, 1916, 295, 819, 2539, 14642, 11, 8213, 24590, 293, 3565, 3142, 50728, 50728, 24590, 13, 50778, 50778, 814, 589, 731, 337, 867, 9608, 11, 457, 2171, 294, 364, 3861, 264, 9284, 727, 1190, 51058, 51058, 666, 257, 1154, 1219, 670, 69, 2414, 11, 597, 393, 3082, 309, 281, 2042, 22271, 13, 51344, 51344, 708, 286, 1116, 411, 281, 360, 294, 341, 960, 307, 281, 855, 291, 437, 307, 670, 69, 2414, 11, 382, 731, 382, 257, 8185, 51606, 51606, 4077, 1920, 6182, 1154, 1219, 833, 69, 2414, 13, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.13501112239876972, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.006095339544117451}, {"id": 1, "seek": 0, "start": 7.28, "end": 8.28, "text": " regression.", "tokens": [50364, 823, 291, 600, 1612, 257, 1916, 295, 819, 2539, 14642, 11, 8213, 24590, 293, 3565, 3142, 50728, 50728, 24590, 13, 50778, 50778, 814, 589, 731, 337, 867, 9608, 11, 457, 2171, 294, 364, 3861, 264, 9284, 727, 1190, 51058, 51058, 666, 257, 1154, 1219, 670, 69, 2414, 11, 597, 393, 3082, 309, 281, 2042, 22271, 13, 51344, 51344, 708, 286, 1116, 411, 281, 360, 294, 341, 960, 307, 281, 855, 291, 437, 307, 670, 69, 2414, 11, 382, 731, 382, 257, 8185, 51606, 51606, 4077, 1920, 6182, 1154, 1219, 833, 69, 2414, 13, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.13501112239876972, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.006095339544117451}, {"id": 2, "seek": 0, "start": 8.28, "end": 13.88, "text": " They work well for many tasks, but sometimes in an application the algorithm could run", "tokens": [50364, 823, 291, 600, 1612, 257, 1916, 295, 819, 2539, 14642, 11, 8213, 24590, 293, 3565, 3142, 50728, 50728, 24590, 13, 50778, 50778, 814, 589, 731, 337, 867, 9608, 11, 457, 2171, 294, 364, 3861, 264, 9284, 727, 1190, 51058, 51058, 666, 257, 1154, 1219, 670, 69, 2414, 11, 597, 393, 3082, 309, 281, 2042, 22271, 13, 51344, 51344, 708, 286, 1116, 411, 281, 360, 294, 341, 960, 307, 281, 855, 291, 437, 307, 670, 69, 2414, 11, 382, 731, 382, 257, 8185, 51606, 51606, 4077, 1920, 6182, 1154, 1219, 833, 69, 2414, 13, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.13501112239876972, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.006095339544117451}, {"id": 3, "seek": 0, "start": 13.88, "end": 19.6, "text": " into a problem called overfitting, which can cause it to perform poorly.", "tokens": [50364, 823, 291, 600, 1612, 257, 1916, 295, 819, 2539, 14642, 11, 8213, 24590, 293, 3565, 3142, 50728, 50728, 24590, 13, 50778, 50778, 814, 589, 731, 337, 867, 9608, 11, 457, 2171, 294, 364, 3861, 264, 9284, 727, 1190, 51058, 51058, 666, 257, 1154, 1219, 670, 69, 2414, 11, 597, 393, 3082, 309, 281, 2042, 22271, 13, 51344, 51344, 708, 286, 1116, 411, 281, 360, 294, 341, 960, 307, 281, 855, 291, 437, 307, 670, 69, 2414, 11, 382, 731, 382, 257, 8185, 51606, 51606, 4077, 1920, 6182, 1154, 1219, 833, 69, 2414, 13, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.13501112239876972, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.006095339544117451}, {"id": 4, "seek": 0, "start": 19.6, "end": 24.84, "text": " What I'd like to do in this video is to show you what is overfitting, as well as a closely", "tokens": [50364, 823, 291, 600, 1612, 257, 1916, 295, 819, 2539, 14642, 11, 8213, 24590, 293, 3565, 3142, 50728, 50728, 24590, 13, 50778, 50778, 814, 589, 731, 337, 867, 9608, 11, 457, 2171, 294, 364, 3861, 264, 9284, 727, 1190, 51058, 51058, 666, 257, 1154, 1219, 670, 69, 2414, 11, 597, 393, 3082, 309, 281, 2042, 22271, 13, 51344, 51344, 708, 286, 1116, 411, 281, 360, 294, 341, 960, 307, 281, 855, 291, 437, 307, 670, 69, 2414, 11, 382, 731, 382, 257, 8185, 51606, 51606, 4077, 1920, 6182, 1154, 1219, 833, 69, 2414, 13, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.13501112239876972, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.006095339544117451}, {"id": 5, "seek": 0, "start": 24.84, "end": 28.88, "text": " related almost opposite problem called underfitting.", "tokens": [50364, 823, 291, 600, 1612, 257, 1916, 295, 819, 2539, 14642, 11, 8213, 24590, 293, 3565, 3142, 50728, 50728, 24590, 13, 50778, 50778, 814, 589, 731, 337, 867, 9608, 11, 457, 2171, 294, 364, 3861, 264, 9284, 727, 1190, 51058, 51058, 666, 257, 1154, 1219, 670, 69, 2414, 11, 597, 393, 3082, 309, 281, 2042, 22271, 13, 51344, 51344, 708, 286, 1116, 411, 281, 360, 294, 341, 960, 307, 281, 855, 291, 437, 307, 670, 69, 2414, 11, 382, 731, 382, 257, 8185, 51606, 51606, 4077, 1920, 6182, 1154, 1219, 833, 69, 2414, 13, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.13501112239876972, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.006095339544117451}, {"id": 6, "seek": 2888, "start": 28.88, "end": 34.879999999999995, "text": " And in the next videos after this, I'll share with you some techniques for addressing overfitting.", "tokens": [50364, 400, 294, 264, 958, 2145, 934, 341, 11, 286, 603, 2073, 365, 291, 512, 7512, 337, 14329, 670, 69, 2414, 13, 50664, 50664, 682, 1729, 11, 456, 311, 257, 3170, 1219, 3890, 2144, 13, 50812, 50812, 4372, 4420, 6532, 11, 286, 764, 309, 439, 264, 565, 11, 457, 3890, 2144, 486, 854, 291, 17522, 51052, 51052, 341, 670, 69, 2414, 1154, 293, 483, 428, 2539, 14642, 281, 589, 709, 1101, 13, 51290, 51290, 407, 718, 311, 747, 257, 574, 412, 437, 307, 670, 69, 2414, 13, 51514, 51514, 1407, 854, 505, 1223, 437, 307, 670, 69, 2414, 11, 718, 311, 747, 257, 574, 412, 257, 1326, 5110, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1096205200467791, "compression_ratio": 1.7450980392156863, "no_speech_prob": 6.0135578678455204e-05}, {"id": 7, "seek": 2888, "start": 34.879999999999995, "end": 37.84, "text": " In particular, there's a method called regularization.", "tokens": [50364, 400, 294, 264, 958, 2145, 934, 341, 11, 286, 603, 2073, 365, 291, 512, 7512, 337, 14329, 670, 69, 2414, 13, 50664, 50664, 682, 1729, 11, 456, 311, 257, 3170, 1219, 3890, 2144, 13, 50812, 50812, 4372, 4420, 6532, 11, 286, 764, 309, 439, 264, 565, 11, 457, 3890, 2144, 486, 854, 291, 17522, 51052, 51052, 341, 670, 69, 2414, 1154, 293, 483, 428, 2539, 14642, 281, 589, 709, 1101, 13, 51290, 51290, 407, 718, 311, 747, 257, 574, 412, 437, 307, 670, 69, 2414, 13, 51514, 51514, 1407, 854, 505, 1223, 437, 307, 670, 69, 2414, 11, 718, 311, 747, 257, 574, 412, 257, 1326, 5110, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1096205200467791, "compression_ratio": 1.7450980392156863, "no_speech_prob": 6.0135578678455204e-05}, {"id": 8, "seek": 2888, "start": 37.84, "end": 42.64, "text": " Very useful technique, I use it all the time, but regularization will help you minimize", "tokens": [50364, 400, 294, 264, 958, 2145, 934, 341, 11, 286, 603, 2073, 365, 291, 512, 7512, 337, 14329, 670, 69, 2414, 13, 50664, 50664, 682, 1729, 11, 456, 311, 257, 3170, 1219, 3890, 2144, 13, 50812, 50812, 4372, 4420, 6532, 11, 286, 764, 309, 439, 264, 565, 11, 457, 3890, 2144, 486, 854, 291, 17522, 51052, 51052, 341, 670, 69, 2414, 1154, 293, 483, 428, 2539, 14642, 281, 589, 709, 1101, 13, 51290, 51290, 407, 718, 311, 747, 257, 574, 412, 437, 307, 670, 69, 2414, 13, 51514, 51514, 1407, 854, 505, 1223, 437, 307, 670, 69, 2414, 11, 718, 311, 747, 257, 574, 412, 257, 1326, 5110, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1096205200467791, "compression_ratio": 1.7450980392156863, "no_speech_prob": 6.0135578678455204e-05}, {"id": 9, "seek": 2888, "start": 42.64, "end": 47.4, "text": " this overfitting problem and get your learning algorithms to work much better.", "tokens": [50364, 400, 294, 264, 958, 2145, 934, 341, 11, 286, 603, 2073, 365, 291, 512, 7512, 337, 14329, 670, 69, 2414, 13, 50664, 50664, 682, 1729, 11, 456, 311, 257, 3170, 1219, 3890, 2144, 13, 50812, 50812, 4372, 4420, 6532, 11, 286, 764, 309, 439, 264, 565, 11, 457, 3890, 2144, 486, 854, 291, 17522, 51052, 51052, 341, 670, 69, 2414, 1154, 293, 483, 428, 2539, 14642, 281, 589, 709, 1101, 13, 51290, 51290, 407, 718, 311, 747, 257, 574, 412, 437, 307, 670, 69, 2414, 13, 51514, 51514, 1407, 854, 505, 1223, 437, 307, 670, 69, 2414, 11, 718, 311, 747, 257, 574, 412, 257, 1326, 5110, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1096205200467791, "compression_ratio": 1.7450980392156863, "no_speech_prob": 6.0135578678455204e-05}, {"id": 10, "seek": 2888, "start": 47.4, "end": 51.879999999999995, "text": " So let's take a look at what is overfitting.", "tokens": [50364, 400, 294, 264, 958, 2145, 934, 341, 11, 286, 603, 2073, 365, 291, 512, 7512, 337, 14329, 670, 69, 2414, 13, 50664, 50664, 682, 1729, 11, 456, 311, 257, 3170, 1219, 3890, 2144, 13, 50812, 50812, 4372, 4420, 6532, 11, 286, 764, 309, 439, 264, 565, 11, 457, 3890, 2144, 486, 854, 291, 17522, 51052, 51052, 341, 670, 69, 2414, 1154, 293, 483, 428, 2539, 14642, 281, 589, 709, 1101, 13, 51290, 51290, 407, 718, 311, 747, 257, 574, 412, 437, 307, 670, 69, 2414, 13, 51514, 51514, 1407, 854, 505, 1223, 437, 307, 670, 69, 2414, 11, 718, 311, 747, 257, 574, 412, 257, 1326, 5110, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1096205200467791, "compression_ratio": 1.7450980392156863, "no_speech_prob": 6.0135578678455204e-05}, {"id": 11, "seek": 2888, "start": 51.879999999999995, "end": 58.84, "text": " To help us understand what is overfitting, let's take a look at a few examples.", "tokens": [50364, 400, 294, 264, 958, 2145, 934, 341, 11, 286, 603, 2073, 365, 291, 512, 7512, 337, 14329, 670, 69, 2414, 13, 50664, 50664, 682, 1729, 11, 456, 311, 257, 3170, 1219, 3890, 2144, 13, 50812, 50812, 4372, 4420, 6532, 11, 286, 764, 309, 439, 264, 565, 11, 457, 3890, 2144, 486, 854, 291, 17522, 51052, 51052, 341, 670, 69, 2414, 1154, 293, 483, 428, 2539, 14642, 281, 589, 709, 1101, 13, 51290, 51290, 407, 718, 311, 747, 257, 574, 412, 437, 307, 670, 69, 2414, 13, 51514, 51514, 1407, 854, 505, 1223, 437, 307, 670, 69, 2414, 11, 718, 311, 747, 257, 574, 412, 257, 1326, 5110, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1096205200467791, "compression_ratio": 1.7450980392156863, "no_speech_prob": 6.0135578678455204e-05}, {"id": 12, "seek": 5884, "start": 58.84, "end": 65.24000000000001, "text": " Let's go back to our original example of predicting housing prices with linear regression, where", "tokens": [50364, 961, 311, 352, 646, 281, 527, 3380, 1365, 295, 32884, 6849, 7901, 365, 8213, 24590, 11, 689, 50684, 50684, 291, 528, 281, 6069, 264, 3218, 382, 257, 2445, 295, 264, 2744, 295, 257, 1782, 13, 50930, 50930, 1407, 854, 505, 1223, 437, 307, 670, 69, 2414, 11, 718, 311, 747, 257, 574, 412, 257, 8213, 24590, 1365, 13, 51336, 51336, 400, 286, 478, 516, 281, 352, 646, 281, 527, 3380, 2614, 1365, 295, 32884, 6849, 7901, 365, 51562, 51562, 8213, 24590, 13, 51697, 51697], "temperature": 0.0, "avg_logprob": -0.08094491515048714, "compression_ratio": 1.9114583333333333, "no_speech_prob": 6.854228558950126e-06}, {"id": 13, "seek": 5884, "start": 65.24000000000001, "end": 70.16, "text": " you want to predict the price as a function of the size of a house.", "tokens": [50364, 961, 311, 352, 646, 281, 527, 3380, 1365, 295, 32884, 6849, 7901, 365, 8213, 24590, 11, 689, 50684, 50684, 291, 528, 281, 6069, 264, 3218, 382, 257, 2445, 295, 264, 2744, 295, 257, 1782, 13, 50930, 50930, 1407, 854, 505, 1223, 437, 307, 670, 69, 2414, 11, 718, 311, 747, 257, 574, 412, 257, 8213, 24590, 1365, 13, 51336, 51336, 400, 286, 478, 516, 281, 352, 646, 281, 527, 3380, 2614, 1365, 295, 32884, 6849, 7901, 365, 51562, 51562, 8213, 24590, 13, 51697, 51697], "temperature": 0.0, "avg_logprob": -0.08094491515048714, "compression_ratio": 1.9114583333333333, "no_speech_prob": 6.854228558950126e-06}, {"id": 14, "seek": 5884, "start": 70.16, "end": 78.28, "text": " To help us understand what is overfitting, let's take a look at a linear regression example.", "tokens": [50364, 961, 311, 352, 646, 281, 527, 3380, 1365, 295, 32884, 6849, 7901, 365, 8213, 24590, 11, 689, 50684, 50684, 291, 528, 281, 6069, 264, 3218, 382, 257, 2445, 295, 264, 2744, 295, 257, 1782, 13, 50930, 50930, 1407, 854, 505, 1223, 437, 307, 670, 69, 2414, 11, 718, 311, 747, 257, 574, 412, 257, 8213, 24590, 1365, 13, 51336, 51336, 400, 286, 478, 516, 281, 352, 646, 281, 527, 3380, 2614, 1365, 295, 32884, 6849, 7901, 365, 51562, 51562, 8213, 24590, 13, 51697, 51697], "temperature": 0.0, "avg_logprob": -0.08094491515048714, "compression_ratio": 1.9114583333333333, "no_speech_prob": 6.854228558950126e-06}, {"id": 15, "seek": 5884, "start": 78.28, "end": 82.80000000000001, "text": " And I'm going to go back to our original running example of predicting housing prices with", "tokens": [50364, 961, 311, 352, 646, 281, 527, 3380, 1365, 295, 32884, 6849, 7901, 365, 8213, 24590, 11, 689, 50684, 50684, 291, 528, 281, 6069, 264, 3218, 382, 257, 2445, 295, 264, 2744, 295, 257, 1782, 13, 50930, 50930, 1407, 854, 505, 1223, 437, 307, 670, 69, 2414, 11, 718, 311, 747, 257, 574, 412, 257, 8213, 24590, 1365, 13, 51336, 51336, 400, 286, 478, 516, 281, 352, 646, 281, 527, 3380, 2614, 1365, 295, 32884, 6849, 7901, 365, 51562, 51562, 8213, 24590, 13, 51697, 51697], "temperature": 0.0, "avg_logprob": -0.08094491515048714, "compression_ratio": 1.9114583333333333, "no_speech_prob": 6.854228558950126e-06}, {"id": 16, "seek": 5884, "start": 82.80000000000001, "end": 85.5, "text": " linear regression.", "tokens": [50364, 961, 311, 352, 646, 281, 527, 3380, 1365, 295, 32884, 6849, 7901, 365, 8213, 24590, 11, 689, 50684, 50684, 291, 528, 281, 6069, 264, 3218, 382, 257, 2445, 295, 264, 2744, 295, 257, 1782, 13, 50930, 50930, 1407, 854, 505, 1223, 437, 307, 670, 69, 2414, 11, 718, 311, 747, 257, 574, 412, 257, 8213, 24590, 1365, 13, 51336, 51336, 400, 286, 478, 516, 281, 352, 646, 281, 527, 3380, 2614, 1365, 295, 32884, 6849, 7901, 365, 51562, 51562, 8213, 24590, 13, 51697, 51697], "temperature": 0.0, "avg_logprob": -0.08094491515048714, "compression_ratio": 1.9114583333333333, "no_speech_prob": 6.854228558950126e-06}, {"id": 17, "seek": 8550, "start": 85.5, "end": 91.12, "text": " Suppose your data set looks like this, with the input feature x being the size of the", "tokens": [50364, 21360, 428, 1412, 992, 1542, 411, 341, 11, 365, 264, 4846, 4111, 2031, 885, 264, 2744, 295, 264, 50645, 50645, 1782, 293, 264, 2158, 288, 436, 434, 1382, 281, 6069, 264, 3218, 295, 264, 1782, 13, 50917, 50917, 1485, 551, 291, 727, 360, 307, 3318, 257, 8213, 2445, 281, 341, 1412, 13, 51163, 51163, 400, 498, 291, 360, 300, 11, 291, 483, 257, 2997, 1622, 3318, 281, 264, 1412, 300, 1310, 1542, 411, 341, 13, 51471, 51471, 583, 341, 1943, 380, 257, 588, 665, 2316, 13, 51607, 51607], "temperature": 0.0, "avg_logprob": -0.10358670552571615, "compression_ratio": 1.6715686274509804, "no_speech_prob": 1.5445813914993778e-05}, {"id": 18, "seek": 8550, "start": 91.12, "end": 96.56, "text": " house and the value y they're trying to predict the price of the house.", "tokens": [50364, 21360, 428, 1412, 992, 1542, 411, 341, 11, 365, 264, 4846, 4111, 2031, 885, 264, 2744, 295, 264, 50645, 50645, 1782, 293, 264, 2158, 288, 436, 434, 1382, 281, 6069, 264, 3218, 295, 264, 1782, 13, 50917, 50917, 1485, 551, 291, 727, 360, 307, 3318, 257, 8213, 2445, 281, 341, 1412, 13, 51163, 51163, 400, 498, 291, 360, 300, 11, 291, 483, 257, 2997, 1622, 3318, 281, 264, 1412, 300, 1310, 1542, 411, 341, 13, 51471, 51471, 583, 341, 1943, 380, 257, 588, 665, 2316, 13, 51607, 51607], "temperature": 0.0, "avg_logprob": -0.10358670552571615, "compression_ratio": 1.6715686274509804, "no_speech_prob": 1.5445813914993778e-05}, {"id": 19, "seek": 8550, "start": 96.56, "end": 101.48, "text": " One thing you could do is fit a linear function to this data.", "tokens": [50364, 21360, 428, 1412, 992, 1542, 411, 341, 11, 365, 264, 4846, 4111, 2031, 885, 264, 2744, 295, 264, 50645, 50645, 1782, 293, 264, 2158, 288, 436, 434, 1382, 281, 6069, 264, 3218, 295, 264, 1782, 13, 50917, 50917, 1485, 551, 291, 727, 360, 307, 3318, 257, 8213, 2445, 281, 341, 1412, 13, 51163, 51163, 400, 498, 291, 360, 300, 11, 291, 483, 257, 2997, 1622, 3318, 281, 264, 1412, 300, 1310, 1542, 411, 341, 13, 51471, 51471, 583, 341, 1943, 380, 257, 588, 665, 2316, 13, 51607, 51607], "temperature": 0.0, "avg_logprob": -0.10358670552571615, "compression_ratio": 1.6715686274509804, "no_speech_prob": 1.5445813914993778e-05}, {"id": 20, "seek": 8550, "start": 101.48, "end": 107.64, "text": " And if you do that, you get a straight line fit to the data that maybe looks like this.", "tokens": [50364, 21360, 428, 1412, 992, 1542, 411, 341, 11, 365, 264, 4846, 4111, 2031, 885, 264, 2744, 295, 264, 50645, 50645, 1782, 293, 264, 2158, 288, 436, 434, 1382, 281, 6069, 264, 3218, 295, 264, 1782, 13, 50917, 50917, 1485, 551, 291, 727, 360, 307, 3318, 257, 8213, 2445, 281, 341, 1412, 13, 51163, 51163, 400, 498, 291, 360, 300, 11, 291, 483, 257, 2997, 1622, 3318, 281, 264, 1412, 300, 1310, 1542, 411, 341, 13, 51471, 51471, 583, 341, 1943, 380, 257, 588, 665, 2316, 13, 51607, 51607], "temperature": 0.0, "avg_logprob": -0.10358670552571615, "compression_ratio": 1.6715686274509804, "no_speech_prob": 1.5445813914993778e-05}, {"id": 21, "seek": 8550, "start": 107.64, "end": 110.36, "text": " But this isn't a very good model.", "tokens": [50364, 21360, 428, 1412, 992, 1542, 411, 341, 11, 365, 264, 4846, 4111, 2031, 885, 264, 2744, 295, 264, 50645, 50645, 1782, 293, 264, 2158, 288, 436, 434, 1382, 281, 6069, 264, 3218, 295, 264, 1782, 13, 50917, 50917, 1485, 551, 291, 727, 360, 307, 3318, 257, 8213, 2445, 281, 341, 1412, 13, 51163, 51163, 400, 498, 291, 360, 300, 11, 291, 483, 257, 2997, 1622, 3318, 281, 264, 1412, 300, 1310, 1542, 411, 341, 13, 51471, 51471, 583, 341, 1943, 380, 257, 588, 665, 2316, 13, 51607, 51607], "temperature": 0.0, "avg_logprob": -0.10358670552571615, "compression_ratio": 1.6715686274509804, "no_speech_prob": 1.5445813914993778e-05}, {"id": 22, "seek": 11036, "start": 110.36, "end": 115.76, "text": " Looking at the data, it seems pretty clear that as the size of the house increases, the", "tokens": [50364, 11053, 412, 264, 1412, 11, 309, 2544, 1238, 1850, 300, 382, 264, 2744, 295, 264, 1782, 8637, 11, 264, 50634, 50634, 6849, 7901, 733, 295, 24183, 484, 13, 50800, 50800, 407, 341, 9284, 775, 406, 3318, 264, 3097, 1412, 588, 731, 13, 51056, 51056, 440, 6191, 1433, 337, 341, 307, 264, 2316, 307, 833, 69, 2414, 264, 3097, 1412, 13, 51416, 51416, 3996, 1433, 307, 264, 9284, 575, 1090, 12577, 13, 51675, 51675], "temperature": 0.0, "avg_logprob": -0.08138367335001627, "compression_ratio": 1.6344086021505377, "no_speech_prob": 7.1830518209026195e-06}, {"id": 23, "seek": 11036, "start": 115.76, "end": 119.08, "text": " housing prices kind of flatten out.", "tokens": [50364, 11053, 412, 264, 1412, 11, 309, 2544, 1238, 1850, 300, 382, 264, 2744, 295, 264, 1782, 8637, 11, 264, 50634, 50634, 6849, 7901, 733, 295, 24183, 484, 13, 50800, 50800, 407, 341, 9284, 775, 406, 3318, 264, 3097, 1412, 588, 731, 13, 51056, 51056, 440, 6191, 1433, 337, 341, 307, 264, 2316, 307, 833, 69, 2414, 264, 3097, 1412, 13, 51416, 51416, 3996, 1433, 307, 264, 9284, 575, 1090, 12577, 13, 51675, 51675], "temperature": 0.0, "avg_logprob": -0.08138367335001627, "compression_ratio": 1.6344086021505377, "no_speech_prob": 7.1830518209026195e-06}, {"id": 24, "seek": 11036, "start": 119.08, "end": 124.2, "text": " So this algorithm does not fit the training data very well.", "tokens": [50364, 11053, 412, 264, 1412, 11, 309, 2544, 1238, 1850, 300, 382, 264, 2744, 295, 264, 1782, 8637, 11, 264, 50634, 50634, 6849, 7901, 733, 295, 24183, 484, 13, 50800, 50800, 407, 341, 9284, 775, 406, 3318, 264, 3097, 1412, 588, 731, 13, 51056, 51056, 440, 6191, 1433, 337, 341, 307, 264, 2316, 307, 833, 69, 2414, 264, 3097, 1412, 13, 51416, 51416, 3996, 1433, 307, 264, 9284, 575, 1090, 12577, 13, 51675, 51675], "temperature": 0.0, "avg_logprob": -0.08138367335001627, "compression_ratio": 1.6344086021505377, "no_speech_prob": 7.1830518209026195e-06}, {"id": 25, "seek": 11036, "start": 124.2, "end": 131.4, "text": " The technical term for this is the model is underfitting the training data.", "tokens": [50364, 11053, 412, 264, 1412, 11, 309, 2544, 1238, 1850, 300, 382, 264, 2744, 295, 264, 1782, 8637, 11, 264, 50634, 50634, 6849, 7901, 733, 295, 24183, 484, 13, 50800, 50800, 407, 341, 9284, 775, 406, 3318, 264, 3097, 1412, 588, 731, 13, 51056, 51056, 440, 6191, 1433, 337, 341, 307, 264, 2316, 307, 833, 69, 2414, 264, 3097, 1412, 13, 51416, 51416, 3996, 1433, 307, 264, 9284, 575, 1090, 12577, 13, 51675, 51675], "temperature": 0.0, "avg_logprob": -0.08138367335001627, "compression_ratio": 1.6344086021505377, "no_speech_prob": 7.1830518209026195e-06}, {"id": 26, "seek": 11036, "start": 131.4, "end": 136.57999999999998, "text": " Another term is the algorithm has high bias.", "tokens": [50364, 11053, 412, 264, 1412, 11, 309, 2544, 1238, 1850, 300, 382, 264, 2744, 295, 264, 1782, 8637, 11, 264, 50634, 50634, 6849, 7901, 733, 295, 24183, 484, 13, 50800, 50800, 407, 341, 9284, 775, 406, 3318, 264, 3097, 1412, 588, 731, 13, 51056, 51056, 440, 6191, 1433, 337, 341, 307, 264, 2316, 307, 833, 69, 2414, 264, 3097, 1412, 13, 51416, 51416, 3996, 1433, 307, 264, 9284, 575, 1090, 12577, 13, 51675, 51675], "temperature": 0.0, "avg_logprob": -0.08138367335001627, "compression_ratio": 1.6344086021505377, "no_speech_prob": 7.1830518209026195e-06}, {"id": 27, "seek": 13658, "start": 136.58, "end": 142.52, "text": " You may have read in the news about some learning algorithms, really unfortunately, demonstrating", "tokens": [50364, 509, 815, 362, 1401, 294, 264, 2583, 466, 512, 2539, 14642, 11, 534, 7015, 11, 29889, 50661, 50661, 12577, 1970, 1629, 14363, 1088, 420, 1629, 290, 16292, 13, 50857, 50857, 682, 3479, 2539, 11, 264, 1433, 12577, 575, 3866, 28138, 13, 51139, 51139, 6881, 278, 2539, 14642, 337, 12577, 2361, 322, 10891, 1270, 382, 7898, 420, 33774, 51455, 51455, 307, 3122, 4924, 13, 51595, 51595], "temperature": 0.0, "avg_logprob": -0.1496691917305562, "compression_ratio": 1.6313131313131313, "no_speech_prob": 9.080271411221474e-06}, {"id": 28, "seek": 13658, "start": 142.52, "end": 146.44, "text": " bias against certain ethnicities or certain genders.", "tokens": [50364, 509, 815, 362, 1401, 294, 264, 2583, 466, 512, 2539, 14642, 11, 534, 7015, 11, 29889, 50661, 50661, 12577, 1970, 1629, 14363, 1088, 420, 1629, 290, 16292, 13, 50857, 50857, 682, 3479, 2539, 11, 264, 1433, 12577, 575, 3866, 28138, 13, 51139, 51139, 6881, 278, 2539, 14642, 337, 12577, 2361, 322, 10891, 1270, 382, 7898, 420, 33774, 51455, 51455, 307, 3122, 4924, 13, 51595, 51595], "temperature": 0.0, "avg_logprob": -0.1496691917305562, "compression_ratio": 1.6313131313131313, "no_speech_prob": 9.080271411221474e-06}, {"id": 29, "seek": 13658, "start": 146.44, "end": 152.08, "text": " In machine learning, the term bias has multiple meanings.", "tokens": [50364, 509, 815, 362, 1401, 294, 264, 2583, 466, 512, 2539, 14642, 11, 534, 7015, 11, 29889, 50661, 50661, 12577, 1970, 1629, 14363, 1088, 420, 1629, 290, 16292, 13, 50857, 50857, 682, 3479, 2539, 11, 264, 1433, 12577, 575, 3866, 28138, 13, 51139, 51139, 6881, 278, 2539, 14642, 337, 12577, 2361, 322, 10891, 1270, 382, 7898, 420, 33774, 51455, 51455, 307, 3122, 4924, 13, 51595, 51595], "temperature": 0.0, "avg_logprob": -0.1496691917305562, "compression_ratio": 1.6313131313131313, "no_speech_prob": 9.080271411221474e-06}, {"id": 30, "seek": 13658, "start": 152.08, "end": 158.4, "text": " Checking learning algorithms for bias based on characteristics such as gender or ethnicity", "tokens": [50364, 509, 815, 362, 1401, 294, 264, 2583, 466, 512, 2539, 14642, 11, 534, 7015, 11, 29889, 50661, 50661, 12577, 1970, 1629, 14363, 1088, 420, 1629, 290, 16292, 13, 50857, 50857, 682, 3479, 2539, 11, 264, 1433, 12577, 575, 3866, 28138, 13, 51139, 51139, 6881, 278, 2539, 14642, 337, 12577, 2361, 322, 10891, 1270, 382, 7898, 420, 33774, 51455, 51455, 307, 3122, 4924, 13, 51595, 51595], "temperature": 0.0, "avg_logprob": -0.1496691917305562, "compression_ratio": 1.6313131313131313, "no_speech_prob": 9.080271411221474e-06}, {"id": 31, "seek": 13658, "start": 158.4, "end": 161.20000000000002, "text": " is absolutely critical.", "tokens": [50364, 509, 815, 362, 1401, 294, 264, 2583, 466, 512, 2539, 14642, 11, 534, 7015, 11, 29889, 50661, 50661, 12577, 1970, 1629, 14363, 1088, 420, 1629, 290, 16292, 13, 50857, 50857, 682, 3479, 2539, 11, 264, 1433, 12577, 575, 3866, 28138, 13, 51139, 51139, 6881, 278, 2539, 14642, 337, 12577, 2361, 322, 10891, 1270, 382, 7898, 420, 33774, 51455, 51455, 307, 3122, 4924, 13, 51595, 51595], "temperature": 0.0, "avg_logprob": -0.1496691917305562, "compression_ratio": 1.6313131313131313, "no_speech_prob": 9.080271411221474e-06}, {"id": 32, "seek": 16120, "start": 161.2, "end": 167.56, "text": " But the term bias has a second technical meaning as well, which is the one I'm using here,", "tokens": [50364, 583, 264, 1433, 12577, 575, 257, 1150, 6191, 3620, 382, 731, 11, 597, 307, 264, 472, 286, 478, 1228, 510, 11, 50682, 50682, 597, 307, 498, 264, 9284, 575, 833, 6845, 264, 1412, 11, 3620, 300, 309, 311, 445, 406, 754, 1075, 50918, 50918, 281, 3318, 264, 3097, 992, 300, 731, 11, 300, 456, 311, 257, 1850, 5102, 294, 264, 3097, 1412, 300, 51210, 51210, 264, 9284, 307, 445, 11299, 281, 7983, 13, 51394, 51394, 3996, 636, 281, 519, 295, 341, 1254, 295, 12577, 307, 382, 498, 264, 2539, 9284, 575, 257, 588, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.08251335694617831, "compression_ratio": 1.84037558685446, "no_speech_prob": 3.0893952498445287e-06}, {"id": 33, "seek": 16120, "start": 167.56, "end": 172.28, "text": " which is if the algorithm has underfit the data, meaning that it's just not even able", "tokens": [50364, 583, 264, 1433, 12577, 575, 257, 1150, 6191, 3620, 382, 731, 11, 597, 307, 264, 472, 286, 478, 1228, 510, 11, 50682, 50682, 597, 307, 498, 264, 9284, 575, 833, 6845, 264, 1412, 11, 3620, 300, 309, 311, 445, 406, 754, 1075, 50918, 50918, 281, 3318, 264, 3097, 992, 300, 731, 11, 300, 456, 311, 257, 1850, 5102, 294, 264, 3097, 1412, 300, 51210, 51210, 264, 9284, 307, 445, 11299, 281, 7983, 13, 51394, 51394, 3996, 636, 281, 519, 295, 341, 1254, 295, 12577, 307, 382, 498, 264, 2539, 9284, 575, 257, 588, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.08251335694617831, "compression_ratio": 1.84037558685446, "no_speech_prob": 3.0893952498445287e-06}, {"id": 34, "seek": 16120, "start": 172.28, "end": 178.12, "text": " to fit the training set that well, that there's a clear pattern in the training data that", "tokens": [50364, 583, 264, 1433, 12577, 575, 257, 1150, 6191, 3620, 382, 731, 11, 597, 307, 264, 472, 286, 478, 1228, 510, 11, 50682, 50682, 597, 307, 498, 264, 9284, 575, 833, 6845, 264, 1412, 11, 3620, 300, 309, 311, 445, 406, 754, 1075, 50918, 50918, 281, 3318, 264, 3097, 992, 300, 731, 11, 300, 456, 311, 257, 1850, 5102, 294, 264, 3097, 1412, 300, 51210, 51210, 264, 9284, 307, 445, 11299, 281, 7983, 13, 51394, 51394, 3996, 636, 281, 519, 295, 341, 1254, 295, 12577, 307, 382, 498, 264, 2539, 9284, 575, 257, 588, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.08251335694617831, "compression_ratio": 1.84037558685446, "no_speech_prob": 3.0893952498445287e-06}, {"id": 35, "seek": 16120, "start": 178.12, "end": 181.79999999999998, "text": " the algorithm is just unable to capture.", "tokens": [50364, 583, 264, 1433, 12577, 575, 257, 1150, 6191, 3620, 382, 731, 11, 597, 307, 264, 472, 286, 478, 1228, 510, 11, 50682, 50682, 597, 307, 498, 264, 9284, 575, 833, 6845, 264, 1412, 11, 3620, 300, 309, 311, 445, 406, 754, 1075, 50918, 50918, 281, 3318, 264, 3097, 992, 300, 731, 11, 300, 456, 311, 257, 1850, 5102, 294, 264, 3097, 1412, 300, 51210, 51210, 264, 9284, 307, 445, 11299, 281, 7983, 13, 51394, 51394, 3996, 636, 281, 519, 295, 341, 1254, 295, 12577, 307, 382, 498, 264, 2539, 9284, 575, 257, 588, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.08251335694617831, "compression_ratio": 1.84037558685446, "no_speech_prob": 3.0893952498445287e-06}, {"id": 36, "seek": 16120, "start": 181.79999999999998, "end": 187.32, "text": " Another way to think of this form of bias is as if the learning algorithm has a very", "tokens": [50364, 583, 264, 1433, 12577, 575, 257, 1150, 6191, 3620, 382, 731, 11, 597, 307, 264, 472, 286, 478, 1228, 510, 11, 50682, 50682, 597, 307, 498, 264, 9284, 575, 833, 6845, 264, 1412, 11, 3620, 300, 309, 311, 445, 406, 754, 1075, 50918, 50918, 281, 3318, 264, 3097, 992, 300, 731, 11, 300, 456, 311, 257, 1850, 5102, 294, 264, 3097, 1412, 300, 51210, 51210, 264, 9284, 307, 445, 11299, 281, 7983, 13, 51394, 51394, 3996, 636, 281, 519, 295, 341, 1254, 295, 12577, 307, 382, 498, 264, 2539, 9284, 575, 257, 588, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.08251335694617831, "compression_ratio": 1.84037558685446, "no_speech_prob": 3.0893952498445287e-06}, {"id": 37, "seek": 18732, "start": 187.32, "end": 192.64, "text": " strong preconception or we say a very strong bias that the housing prices are going to", "tokens": [50364, 2068, 47473, 7311, 420, 321, 584, 257, 588, 2068, 12577, 300, 264, 6849, 7901, 366, 516, 281, 50630, 50630, 312, 257, 2584, 8213, 2445, 295, 264, 2744, 11, 7228, 1412, 281, 264, 19506, 13, 50995, 50995, 407, 341, 47473, 7311, 300, 264, 1412, 307, 8213, 7700, 309, 281, 3318, 257, 2997, 1622, 300, 9001, 51310, 51310, 264, 1412, 22271, 11, 5775, 309, 281, 833, 6845, 264, 1412, 13, 51544, 51544], "temperature": 0.0, "avg_logprob": -0.10582646396425036, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.2218534720886964e-05}, {"id": 38, "seek": 18732, "start": 192.64, "end": 199.94, "text": " be a completely linear function of the size, despite data to the contrary.", "tokens": [50364, 2068, 47473, 7311, 420, 321, 584, 257, 588, 2068, 12577, 300, 264, 6849, 7901, 366, 516, 281, 50630, 50630, 312, 257, 2584, 8213, 2445, 295, 264, 2744, 11, 7228, 1412, 281, 264, 19506, 13, 50995, 50995, 407, 341, 47473, 7311, 300, 264, 1412, 307, 8213, 7700, 309, 281, 3318, 257, 2997, 1622, 300, 9001, 51310, 51310, 264, 1412, 22271, 11, 5775, 309, 281, 833, 6845, 264, 1412, 13, 51544, 51544], "temperature": 0.0, "avg_logprob": -0.10582646396425036, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.2218534720886964e-05}, {"id": 39, "seek": 18732, "start": 199.94, "end": 206.24, "text": " So this preconception that the data is linear causes it to fit a straight line that fits", "tokens": [50364, 2068, 47473, 7311, 420, 321, 584, 257, 588, 2068, 12577, 300, 264, 6849, 7901, 366, 516, 281, 50630, 50630, 312, 257, 2584, 8213, 2445, 295, 264, 2744, 11, 7228, 1412, 281, 264, 19506, 13, 50995, 50995, 407, 341, 47473, 7311, 300, 264, 1412, 307, 8213, 7700, 309, 281, 3318, 257, 2997, 1622, 300, 9001, 51310, 51310, 264, 1412, 22271, 11, 5775, 309, 281, 833, 6845, 264, 1412, 13, 51544, 51544], "temperature": 0.0, "avg_logprob": -0.10582646396425036, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.2218534720886964e-05}, {"id": 40, "seek": 18732, "start": 206.24, "end": 210.92, "text": " the data poorly, leading it to underfit the data.", "tokens": [50364, 2068, 47473, 7311, 420, 321, 584, 257, 588, 2068, 12577, 300, 264, 6849, 7901, 366, 516, 281, 50630, 50630, 312, 257, 2584, 8213, 2445, 295, 264, 2744, 11, 7228, 1412, 281, 264, 19506, 13, 50995, 50995, 407, 341, 47473, 7311, 300, 264, 1412, 307, 8213, 7700, 309, 281, 3318, 257, 2997, 1622, 300, 9001, 51310, 51310, 264, 1412, 22271, 11, 5775, 309, 281, 833, 6845, 264, 1412, 13, 51544, 51544], "temperature": 0.0, "avg_logprob": -0.10582646396425036, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.2218534720886964e-05}, {"id": 41, "seek": 21092, "start": 210.92, "end": 219.32, "text": " Now let's look at a second variation of a model, which is if you instead fit a quadratic", "tokens": [50364, 823, 718, 311, 574, 412, 257, 1150, 12990, 295, 257, 2316, 11, 597, 307, 498, 291, 2602, 3318, 257, 37262, 50784, 50784, 2445, 281, 264, 1412, 365, 732, 4122, 11, 2031, 293, 2031, 8889, 11, 550, 562, 291, 3318, 264, 9834, 51098, 51098, 261, 16, 293, 261, 17, 11, 291, 393, 483, 257, 7605, 300, 9001, 264, 1412, 8344, 1101, 13, 51468, 51468, 2704, 309, 1542, 411, 341, 13, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.12076512428179179, "compression_ratio": 1.489010989010989, "no_speech_prob": 5.422030881163664e-06}, {"id": 42, "seek": 21092, "start": 219.32, "end": 225.6, "text": " function to the data with two features, x and x squared, then when you fit the parameters", "tokens": [50364, 823, 718, 311, 574, 412, 257, 1150, 12990, 295, 257, 2316, 11, 597, 307, 498, 291, 2602, 3318, 257, 37262, 50784, 50784, 2445, 281, 264, 1412, 365, 732, 4122, 11, 2031, 293, 2031, 8889, 11, 550, 562, 291, 3318, 264, 9834, 51098, 51098, 261, 16, 293, 261, 17, 11, 291, 393, 483, 257, 7605, 300, 9001, 264, 1412, 8344, 1101, 13, 51468, 51468, 2704, 309, 1542, 411, 341, 13, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.12076512428179179, "compression_ratio": 1.489010989010989, "no_speech_prob": 5.422030881163664e-06}, {"id": 43, "seek": 21092, "start": 225.6, "end": 233.0, "text": " w1 and w2, you can get a curve that fits the data somewhat better.", "tokens": [50364, 823, 718, 311, 574, 412, 257, 1150, 12990, 295, 257, 2316, 11, 597, 307, 498, 291, 2602, 3318, 257, 37262, 50784, 50784, 2445, 281, 264, 1412, 365, 732, 4122, 11, 2031, 293, 2031, 8889, 11, 550, 562, 291, 3318, 264, 9834, 51098, 51098, 261, 16, 293, 261, 17, 11, 291, 393, 483, 257, 7605, 300, 9001, 264, 1412, 8344, 1101, 13, 51468, 51468, 2704, 309, 1542, 411, 341, 13, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.12076512428179179, "compression_ratio": 1.489010989010989, "no_speech_prob": 5.422030881163664e-06}, {"id": 44, "seek": 21092, "start": 233.0, "end": 235.88, "text": " Maybe it looks like this.", "tokens": [50364, 823, 718, 311, 574, 412, 257, 1150, 12990, 295, 257, 2316, 11, 597, 307, 498, 291, 2602, 3318, 257, 37262, 50784, 50784, 2445, 281, 264, 1412, 365, 732, 4122, 11, 2031, 293, 2031, 8889, 11, 550, 562, 291, 3318, 264, 9834, 51098, 51098, 261, 16, 293, 261, 17, 11, 291, 393, 483, 257, 7605, 300, 9001, 264, 1412, 8344, 1101, 13, 51468, 51468, 2704, 309, 1542, 411, 341, 13, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.12076512428179179, "compression_ratio": 1.489010989010989, "no_speech_prob": 5.422030881163664e-06}, {"id": 45, "seek": 23588, "start": 235.88, "end": 242.57999999999998, "text": " So if you were to get a new house that's not in this set of five training examples, this", "tokens": [50364, 407, 498, 291, 645, 281, 483, 257, 777, 1782, 300, 311, 406, 294, 341, 992, 295, 1732, 3097, 5110, 11, 341, 50699, 50699, 2316, 576, 1391, 360, 1596, 731, 322, 300, 777, 1782, 13, 50923, 50923, 407, 498, 291, 434, 257, 957, 9749, 9461, 11, 264, 1558, 300, 291, 528, 428, 2539, 9284, 281, 360, 51157, 51157, 731, 11, 754, 322, 5110, 300, 366, 406, 322, 264, 3097, 992, 11, 300, 311, 1219, 2674, 2144, 13, 51476, 51476, 42494, 11, 321, 584, 300, 291, 528, 428, 2539, 9284, 281, 2674, 1125, 731, 11, 597, 1355, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.10773072098240707, "compression_ratio": 1.8623853211009174, "no_speech_prob": 2.9479813292709878e-06}, {"id": 46, "seek": 23588, "start": 242.57999999999998, "end": 247.06, "text": " model would probably do quite well on that new house.", "tokens": [50364, 407, 498, 291, 645, 281, 483, 257, 777, 1782, 300, 311, 406, 294, 341, 992, 295, 1732, 3097, 5110, 11, 341, 50699, 50699, 2316, 576, 1391, 360, 1596, 731, 322, 300, 777, 1782, 13, 50923, 50923, 407, 498, 291, 434, 257, 957, 9749, 9461, 11, 264, 1558, 300, 291, 528, 428, 2539, 9284, 281, 360, 51157, 51157, 731, 11, 754, 322, 5110, 300, 366, 406, 322, 264, 3097, 992, 11, 300, 311, 1219, 2674, 2144, 13, 51476, 51476, 42494, 11, 321, 584, 300, 291, 528, 428, 2539, 9284, 281, 2674, 1125, 731, 11, 597, 1355, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.10773072098240707, "compression_ratio": 1.8623853211009174, "no_speech_prob": 2.9479813292709878e-06}, {"id": 47, "seek": 23588, "start": 247.06, "end": 251.74, "text": " So if you're a real estate agent, the idea that you want your learning algorithm to do", "tokens": [50364, 407, 498, 291, 645, 281, 483, 257, 777, 1782, 300, 311, 406, 294, 341, 992, 295, 1732, 3097, 5110, 11, 341, 50699, 50699, 2316, 576, 1391, 360, 1596, 731, 322, 300, 777, 1782, 13, 50923, 50923, 407, 498, 291, 434, 257, 957, 9749, 9461, 11, 264, 1558, 300, 291, 528, 428, 2539, 9284, 281, 360, 51157, 51157, 731, 11, 754, 322, 5110, 300, 366, 406, 322, 264, 3097, 992, 11, 300, 311, 1219, 2674, 2144, 13, 51476, 51476, 42494, 11, 321, 584, 300, 291, 528, 428, 2539, 9284, 281, 2674, 1125, 731, 11, 597, 1355, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.10773072098240707, "compression_ratio": 1.8623853211009174, "no_speech_prob": 2.9479813292709878e-06}, {"id": 48, "seek": 23588, "start": 251.74, "end": 258.12, "text": " well, even on examples that are not on the training set, that's called generalization.", "tokens": [50364, 407, 498, 291, 645, 281, 483, 257, 777, 1782, 300, 311, 406, 294, 341, 992, 295, 1732, 3097, 5110, 11, 341, 50699, 50699, 2316, 576, 1391, 360, 1596, 731, 322, 300, 777, 1782, 13, 50923, 50923, 407, 498, 291, 434, 257, 957, 9749, 9461, 11, 264, 1558, 300, 291, 528, 428, 2539, 9284, 281, 360, 51157, 51157, 731, 11, 754, 322, 5110, 300, 366, 406, 322, 264, 3097, 992, 11, 300, 311, 1219, 2674, 2144, 13, 51476, 51476, 42494, 11, 321, 584, 300, 291, 528, 428, 2539, 9284, 281, 2674, 1125, 731, 11, 597, 1355, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.10773072098240707, "compression_ratio": 1.8623853211009174, "no_speech_prob": 2.9479813292709878e-06}, {"id": 49, "seek": 23588, "start": 258.12, "end": 263.6, "text": " Technically, we say that you want your learning algorithm to generalize well, which means", "tokens": [50364, 407, 498, 291, 645, 281, 483, 257, 777, 1782, 300, 311, 406, 294, 341, 992, 295, 1732, 3097, 5110, 11, 341, 50699, 50699, 2316, 576, 1391, 360, 1596, 731, 322, 300, 777, 1782, 13, 50923, 50923, 407, 498, 291, 434, 257, 957, 9749, 9461, 11, 264, 1558, 300, 291, 528, 428, 2539, 9284, 281, 360, 51157, 51157, 731, 11, 754, 322, 5110, 300, 366, 406, 322, 264, 3097, 992, 11, 300, 311, 1219, 2674, 2144, 13, 51476, 51476, 42494, 11, 321, 584, 300, 291, 528, 428, 2539, 9284, 281, 2674, 1125, 731, 11, 597, 1355, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.10773072098240707, "compression_ratio": 1.8623853211009174, "no_speech_prob": 2.9479813292709878e-06}, {"id": 50, "seek": 26360, "start": 263.6, "end": 269.04, "text": " to make good predictions, even on brand new examples that it has never seen before.", "tokens": [50364, 281, 652, 665, 21264, 11, 754, 322, 3360, 777, 5110, 300, 309, 575, 1128, 1612, 949, 13, 50636, 50636, 407, 341, 37262, 2316, 2544, 281, 3318, 264, 3097, 992, 406, 6239, 11, 457, 1238, 731, 13, 50922, 50922, 400, 286, 519, 309, 486, 2674, 1125, 731, 281, 777, 5110, 13, 51130, 51130, 823, 718, 311, 574, 412, 264, 661, 8084, 13, 51282, 51282, 708, 498, 291, 645, 281, 3318, 257, 6409, 1668, 26110, 281, 264, 1412, 30, 51486, 51486, 407, 291, 362, 2031, 11, 2031, 8889, 11, 2031, 36510, 11, 293, 2031, 281, 264, 6409, 439, 382, 4122, 13, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.11637832123099022, "compression_ratio": 1.576, "no_speech_prob": 1.7603177866476472e-06}, {"id": 51, "seek": 26360, "start": 269.04, "end": 274.76000000000005, "text": " So this quadratic model seems to fit the training set not perfectly, but pretty well.", "tokens": [50364, 281, 652, 665, 21264, 11, 754, 322, 3360, 777, 5110, 300, 309, 575, 1128, 1612, 949, 13, 50636, 50636, 407, 341, 37262, 2316, 2544, 281, 3318, 264, 3097, 992, 406, 6239, 11, 457, 1238, 731, 13, 50922, 50922, 400, 286, 519, 309, 486, 2674, 1125, 731, 281, 777, 5110, 13, 51130, 51130, 823, 718, 311, 574, 412, 264, 661, 8084, 13, 51282, 51282, 708, 498, 291, 645, 281, 3318, 257, 6409, 1668, 26110, 281, 264, 1412, 30, 51486, 51486, 407, 291, 362, 2031, 11, 2031, 8889, 11, 2031, 36510, 11, 293, 2031, 281, 264, 6409, 439, 382, 4122, 13, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.11637832123099022, "compression_ratio": 1.576, "no_speech_prob": 1.7603177866476472e-06}, {"id": 52, "seek": 26360, "start": 274.76000000000005, "end": 278.92, "text": " And I think it will generalize well to new examples.", "tokens": [50364, 281, 652, 665, 21264, 11, 754, 322, 3360, 777, 5110, 300, 309, 575, 1128, 1612, 949, 13, 50636, 50636, 407, 341, 37262, 2316, 2544, 281, 3318, 264, 3097, 992, 406, 6239, 11, 457, 1238, 731, 13, 50922, 50922, 400, 286, 519, 309, 486, 2674, 1125, 731, 281, 777, 5110, 13, 51130, 51130, 823, 718, 311, 574, 412, 264, 661, 8084, 13, 51282, 51282, 708, 498, 291, 645, 281, 3318, 257, 6409, 1668, 26110, 281, 264, 1412, 30, 51486, 51486, 407, 291, 362, 2031, 11, 2031, 8889, 11, 2031, 36510, 11, 293, 2031, 281, 264, 6409, 439, 382, 4122, 13, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.11637832123099022, "compression_ratio": 1.576, "no_speech_prob": 1.7603177866476472e-06}, {"id": 53, "seek": 26360, "start": 278.92, "end": 281.96000000000004, "text": " Now let's look at the other extreme.", "tokens": [50364, 281, 652, 665, 21264, 11, 754, 322, 3360, 777, 5110, 300, 309, 575, 1128, 1612, 949, 13, 50636, 50636, 407, 341, 37262, 2316, 2544, 281, 3318, 264, 3097, 992, 406, 6239, 11, 457, 1238, 731, 13, 50922, 50922, 400, 286, 519, 309, 486, 2674, 1125, 731, 281, 777, 5110, 13, 51130, 51130, 823, 718, 311, 574, 412, 264, 661, 8084, 13, 51282, 51282, 708, 498, 291, 645, 281, 3318, 257, 6409, 1668, 26110, 281, 264, 1412, 30, 51486, 51486, 407, 291, 362, 2031, 11, 2031, 8889, 11, 2031, 36510, 11, 293, 2031, 281, 264, 6409, 439, 382, 4122, 13, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.11637832123099022, "compression_ratio": 1.576, "no_speech_prob": 1.7603177866476472e-06}, {"id": 54, "seek": 26360, "start": 281.96000000000004, "end": 286.04, "text": " What if you were to fit a fourth order polynomial to the data?", "tokens": [50364, 281, 652, 665, 21264, 11, 754, 322, 3360, 777, 5110, 300, 309, 575, 1128, 1612, 949, 13, 50636, 50636, 407, 341, 37262, 2316, 2544, 281, 3318, 264, 3097, 992, 406, 6239, 11, 457, 1238, 731, 13, 50922, 50922, 400, 286, 519, 309, 486, 2674, 1125, 731, 281, 777, 5110, 13, 51130, 51130, 823, 718, 311, 574, 412, 264, 661, 8084, 13, 51282, 51282, 708, 498, 291, 645, 281, 3318, 257, 6409, 1668, 26110, 281, 264, 1412, 30, 51486, 51486, 407, 291, 362, 2031, 11, 2031, 8889, 11, 2031, 36510, 11, 293, 2031, 281, 264, 6409, 439, 382, 4122, 13, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.11637832123099022, "compression_ratio": 1.576, "no_speech_prob": 1.7603177866476472e-06}, {"id": 55, "seek": 26360, "start": 286.04, "end": 292.44, "text": " So you have x, x squared, x cubed, and x to the fourth all as features.", "tokens": [50364, 281, 652, 665, 21264, 11, 754, 322, 3360, 777, 5110, 300, 309, 575, 1128, 1612, 949, 13, 50636, 50636, 407, 341, 37262, 2316, 2544, 281, 3318, 264, 3097, 992, 406, 6239, 11, 457, 1238, 731, 13, 50922, 50922, 400, 286, 519, 309, 486, 2674, 1125, 731, 281, 777, 5110, 13, 51130, 51130, 823, 718, 311, 574, 412, 264, 661, 8084, 13, 51282, 51282, 708, 498, 291, 645, 281, 3318, 257, 6409, 1668, 26110, 281, 264, 1412, 30, 51486, 51486, 407, 291, 362, 2031, 11, 2031, 8889, 11, 2031, 36510, 11, 293, 2031, 281, 264, 6409, 439, 382, 4122, 13, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.11637832123099022, "compression_ratio": 1.576, "no_speech_prob": 1.7603177866476472e-06}, {"id": 56, "seek": 29244, "start": 292.44, "end": 296.16, "text": " With this fourth order polynomial, you can actually fit the curve that passes through", "tokens": [50364, 2022, 341, 6409, 1668, 26110, 11, 291, 393, 767, 3318, 264, 7605, 300, 11335, 807, 50550, 50550, 439, 1732, 295, 264, 3097, 5110, 2293, 13, 50724, 50724, 400, 291, 1062, 483, 257, 7605, 300, 1542, 411, 341, 13, 50923, 50923, 639, 322, 472, 1011, 2544, 281, 360, 364, 4664, 665, 1691, 15669, 264, 3097, 1412, 11, 570, 51190, 51190, 309, 11335, 807, 439, 295, 264, 3097, 1412, 6239, 13, 51384, 51384, 682, 1186, 11, 291, 603, 312, 1075, 281, 2826, 9834, 300, 486, 1874, 294, 264, 2063, 2445, 885, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.12251918546615108, "compression_ratio": 1.7016806722689075, "no_speech_prob": 1.3925345001553069e-06}, {"id": 57, "seek": 29244, "start": 296.16, "end": 299.64, "text": " all five of the training examples exactly.", "tokens": [50364, 2022, 341, 6409, 1668, 26110, 11, 291, 393, 767, 3318, 264, 7605, 300, 11335, 807, 50550, 50550, 439, 1732, 295, 264, 3097, 5110, 2293, 13, 50724, 50724, 400, 291, 1062, 483, 257, 7605, 300, 1542, 411, 341, 13, 50923, 50923, 639, 322, 472, 1011, 2544, 281, 360, 364, 4664, 665, 1691, 15669, 264, 3097, 1412, 11, 570, 51190, 51190, 309, 11335, 807, 439, 295, 264, 3097, 1412, 6239, 13, 51384, 51384, 682, 1186, 11, 291, 603, 312, 1075, 281, 2826, 9834, 300, 486, 1874, 294, 264, 2063, 2445, 885, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.12251918546615108, "compression_ratio": 1.7016806722689075, "no_speech_prob": 1.3925345001553069e-06}, {"id": 58, "seek": 29244, "start": 299.64, "end": 303.62, "text": " And you might get a curve that looks like this.", "tokens": [50364, 2022, 341, 6409, 1668, 26110, 11, 291, 393, 767, 3318, 264, 7605, 300, 11335, 807, 50550, 50550, 439, 1732, 295, 264, 3097, 5110, 2293, 13, 50724, 50724, 400, 291, 1062, 483, 257, 7605, 300, 1542, 411, 341, 13, 50923, 50923, 639, 322, 472, 1011, 2544, 281, 360, 364, 4664, 665, 1691, 15669, 264, 3097, 1412, 11, 570, 51190, 51190, 309, 11335, 807, 439, 295, 264, 3097, 1412, 6239, 13, 51384, 51384, 682, 1186, 11, 291, 603, 312, 1075, 281, 2826, 9834, 300, 486, 1874, 294, 264, 2063, 2445, 885, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.12251918546615108, "compression_ratio": 1.7016806722689075, "no_speech_prob": 1.3925345001553069e-06}, {"id": 59, "seek": 29244, "start": 303.62, "end": 308.96, "text": " This on one hand seems to do an extremely good job fitting the training data, because", "tokens": [50364, 2022, 341, 6409, 1668, 26110, 11, 291, 393, 767, 3318, 264, 7605, 300, 11335, 807, 50550, 50550, 439, 1732, 295, 264, 3097, 5110, 2293, 13, 50724, 50724, 400, 291, 1062, 483, 257, 7605, 300, 1542, 411, 341, 13, 50923, 50923, 639, 322, 472, 1011, 2544, 281, 360, 364, 4664, 665, 1691, 15669, 264, 3097, 1412, 11, 570, 51190, 51190, 309, 11335, 807, 439, 295, 264, 3097, 1412, 6239, 13, 51384, 51384, 682, 1186, 11, 291, 603, 312, 1075, 281, 2826, 9834, 300, 486, 1874, 294, 264, 2063, 2445, 885, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.12251918546615108, "compression_ratio": 1.7016806722689075, "no_speech_prob": 1.3925345001553069e-06}, {"id": 60, "seek": 29244, "start": 308.96, "end": 312.84, "text": " it passes through all of the training data perfectly.", "tokens": [50364, 2022, 341, 6409, 1668, 26110, 11, 291, 393, 767, 3318, 264, 7605, 300, 11335, 807, 50550, 50550, 439, 1732, 295, 264, 3097, 5110, 2293, 13, 50724, 50724, 400, 291, 1062, 483, 257, 7605, 300, 1542, 411, 341, 13, 50923, 50923, 639, 322, 472, 1011, 2544, 281, 360, 364, 4664, 665, 1691, 15669, 264, 3097, 1412, 11, 570, 51190, 51190, 309, 11335, 807, 439, 295, 264, 3097, 1412, 6239, 13, 51384, 51384, 682, 1186, 11, 291, 603, 312, 1075, 281, 2826, 9834, 300, 486, 1874, 294, 264, 2063, 2445, 885, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.12251918546615108, "compression_ratio": 1.7016806722689075, "no_speech_prob": 1.3925345001553069e-06}, {"id": 61, "seek": 29244, "start": 312.84, "end": 317.48, "text": " In fact, you'll be able to choose parameters that will result in the cost function being", "tokens": [50364, 2022, 341, 6409, 1668, 26110, 11, 291, 393, 767, 3318, 264, 7605, 300, 11335, 807, 50550, 50550, 439, 1732, 295, 264, 3097, 5110, 2293, 13, 50724, 50724, 400, 291, 1062, 483, 257, 7605, 300, 1542, 411, 341, 13, 50923, 50923, 639, 322, 472, 1011, 2544, 281, 360, 364, 4664, 665, 1691, 15669, 264, 3097, 1412, 11, 570, 51190, 51190, 309, 11335, 807, 439, 295, 264, 3097, 1412, 6239, 13, 51384, 51384, 682, 1186, 11, 291, 603, 312, 1075, 281, 2826, 9834, 300, 486, 1874, 294, 264, 2063, 2445, 885, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.12251918546615108, "compression_ratio": 1.7016806722689075, "no_speech_prob": 1.3925345001553069e-06}, {"id": 62, "seek": 31748, "start": 317.48, "end": 324.40000000000003, "text": " exactly equal to zero, because the errors are zero on all five training examples.", "tokens": [50364, 2293, 2681, 281, 4018, 11, 570, 264, 13603, 366, 4018, 322, 439, 1732, 3097, 5110, 13, 50710, 50710, 583, 341, 307, 257, 588, 261, 46737, 7605, 13, 50838, 50838, 467, 311, 516, 493, 293, 760, 439, 670, 264, 1081, 13, 50984, 50984, 400, 498, 291, 362, 341, 1782, 2744, 558, 510, 11, 264, 2316, 576, 6069, 300, 341, 1782, 307, 51250, 51250, 12284, 813, 8078, 300, 366, 4356, 813, 309, 13, 51462, 51462, 407, 321, 500, 380, 519, 300, 341, 307, 257, 4098, 665, 2316, 337, 32884, 6849, 7901, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1026337501850534, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.24826271985512e-06}, {"id": 63, "seek": 31748, "start": 324.40000000000003, "end": 326.96000000000004, "text": " But this is a very wiggly curve.", "tokens": [50364, 2293, 2681, 281, 4018, 11, 570, 264, 13603, 366, 4018, 322, 439, 1732, 3097, 5110, 13, 50710, 50710, 583, 341, 307, 257, 588, 261, 46737, 7605, 13, 50838, 50838, 467, 311, 516, 493, 293, 760, 439, 670, 264, 1081, 13, 50984, 50984, 400, 498, 291, 362, 341, 1782, 2744, 558, 510, 11, 264, 2316, 576, 6069, 300, 341, 1782, 307, 51250, 51250, 12284, 813, 8078, 300, 366, 4356, 813, 309, 13, 51462, 51462, 407, 321, 500, 380, 519, 300, 341, 307, 257, 4098, 665, 2316, 337, 32884, 6849, 7901, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1026337501850534, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.24826271985512e-06}, {"id": 64, "seek": 31748, "start": 326.96000000000004, "end": 329.88, "text": " It's going up and down all over the place.", "tokens": [50364, 2293, 2681, 281, 4018, 11, 570, 264, 13603, 366, 4018, 322, 439, 1732, 3097, 5110, 13, 50710, 50710, 583, 341, 307, 257, 588, 261, 46737, 7605, 13, 50838, 50838, 467, 311, 516, 493, 293, 760, 439, 670, 264, 1081, 13, 50984, 50984, 400, 498, 291, 362, 341, 1782, 2744, 558, 510, 11, 264, 2316, 576, 6069, 300, 341, 1782, 307, 51250, 51250, 12284, 813, 8078, 300, 366, 4356, 813, 309, 13, 51462, 51462, 407, 321, 500, 380, 519, 300, 341, 307, 257, 4098, 665, 2316, 337, 32884, 6849, 7901, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1026337501850534, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.24826271985512e-06}, {"id": 65, "seek": 31748, "start": 329.88, "end": 335.20000000000005, "text": " And if you have this house size right here, the model would predict that this house is", "tokens": [50364, 2293, 2681, 281, 4018, 11, 570, 264, 13603, 366, 4018, 322, 439, 1732, 3097, 5110, 13, 50710, 50710, 583, 341, 307, 257, 588, 261, 46737, 7605, 13, 50838, 50838, 467, 311, 516, 493, 293, 760, 439, 670, 264, 1081, 13, 50984, 50984, 400, 498, 291, 362, 341, 1782, 2744, 558, 510, 11, 264, 2316, 576, 6069, 300, 341, 1782, 307, 51250, 51250, 12284, 813, 8078, 300, 366, 4356, 813, 309, 13, 51462, 51462, 407, 321, 500, 380, 519, 300, 341, 307, 257, 4098, 665, 2316, 337, 32884, 6849, 7901, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1026337501850534, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.24826271985512e-06}, {"id": 66, "seek": 31748, "start": 335.20000000000005, "end": 339.44, "text": " cheaper than houses that are smaller than it.", "tokens": [50364, 2293, 2681, 281, 4018, 11, 570, 264, 13603, 366, 4018, 322, 439, 1732, 3097, 5110, 13, 50710, 50710, 583, 341, 307, 257, 588, 261, 46737, 7605, 13, 50838, 50838, 467, 311, 516, 493, 293, 760, 439, 670, 264, 1081, 13, 50984, 50984, 400, 498, 291, 362, 341, 1782, 2744, 558, 510, 11, 264, 2316, 576, 6069, 300, 341, 1782, 307, 51250, 51250, 12284, 813, 8078, 300, 366, 4356, 813, 309, 13, 51462, 51462, 407, 321, 500, 380, 519, 300, 341, 307, 257, 4098, 665, 2316, 337, 32884, 6849, 7901, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1026337501850534, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.24826271985512e-06}, {"id": 67, "seek": 31748, "start": 339.44, "end": 346.24, "text": " So we don't think that this is a particularly good model for predicting housing prices.", "tokens": [50364, 2293, 2681, 281, 4018, 11, 570, 264, 13603, 366, 4018, 322, 439, 1732, 3097, 5110, 13, 50710, 50710, 583, 341, 307, 257, 588, 261, 46737, 7605, 13, 50838, 50838, 467, 311, 516, 493, 293, 760, 439, 670, 264, 1081, 13, 50984, 50984, 400, 498, 291, 362, 341, 1782, 2744, 558, 510, 11, 264, 2316, 576, 6069, 300, 341, 1782, 307, 51250, 51250, 12284, 813, 8078, 300, 366, 4356, 813, 309, 13, 51462, 51462, 407, 321, 500, 380, 519, 300, 341, 307, 257, 4098, 665, 2316, 337, 32884, 6849, 7901, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1026337501850534, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.24826271985512e-06}, {"id": 68, "seek": 34624, "start": 346.24, "end": 352.52, "text": " The technical term is that we'll say this model has overfit the data, or this model", "tokens": [50364, 440, 6191, 1433, 307, 300, 321, 603, 584, 341, 2316, 575, 670, 6845, 264, 1412, 11, 420, 341, 2316, 50678, 50678, 575, 364, 670, 69, 2414, 1154, 13, 50806, 50806, 1436, 754, 1673, 309, 9001, 264, 3097, 992, 588, 731, 11, 309, 575, 3318, 264, 1412, 1920, 886, 51034, 51034, 731, 11, 16678, 307, 670, 6845, 13, 51158, 51158, 400, 309, 775, 406, 574, 411, 341, 2316, 486, 2674, 1125, 281, 777, 5110, 300, 575, 1128, 51381, 51381, 1612, 949, 13, 51482, 51482, 3996, 1433, 337, 341, 307, 300, 264, 9284, 575, 1090, 21977, 13, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.1253864114934748, "compression_ratio": 1.7130044843049328, "no_speech_prob": 1.3419646165857557e-05}, {"id": 69, "seek": 34624, "start": 352.52, "end": 355.08, "text": " has an overfitting problem.", "tokens": [50364, 440, 6191, 1433, 307, 300, 321, 603, 584, 341, 2316, 575, 670, 6845, 264, 1412, 11, 420, 341, 2316, 50678, 50678, 575, 364, 670, 69, 2414, 1154, 13, 50806, 50806, 1436, 754, 1673, 309, 9001, 264, 3097, 992, 588, 731, 11, 309, 575, 3318, 264, 1412, 1920, 886, 51034, 51034, 731, 11, 16678, 307, 670, 6845, 13, 51158, 51158, 400, 309, 775, 406, 574, 411, 341, 2316, 486, 2674, 1125, 281, 777, 5110, 300, 575, 1128, 51381, 51381, 1612, 949, 13, 51482, 51482, 3996, 1433, 337, 341, 307, 300, 264, 9284, 575, 1090, 21977, 13, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.1253864114934748, "compression_ratio": 1.7130044843049328, "no_speech_prob": 1.3419646165857557e-05}, {"id": 70, "seek": 34624, "start": 355.08, "end": 359.64, "text": " Because even though it fits the training set very well, it has fit the data almost too", "tokens": [50364, 440, 6191, 1433, 307, 300, 321, 603, 584, 341, 2316, 575, 670, 6845, 264, 1412, 11, 420, 341, 2316, 50678, 50678, 575, 364, 670, 69, 2414, 1154, 13, 50806, 50806, 1436, 754, 1673, 309, 9001, 264, 3097, 992, 588, 731, 11, 309, 575, 3318, 264, 1412, 1920, 886, 51034, 51034, 731, 11, 16678, 307, 670, 6845, 13, 51158, 51158, 400, 309, 775, 406, 574, 411, 341, 2316, 486, 2674, 1125, 281, 777, 5110, 300, 575, 1128, 51381, 51381, 1612, 949, 13, 51482, 51482, 3996, 1433, 337, 341, 307, 300, 264, 9284, 575, 1090, 21977, 13, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.1253864114934748, "compression_ratio": 1.7130044843049328, "no_speech_prob": 1.3419646165857557e-05}, {"id": 71, "seek": 34624, "start": 359.64, "end": 362.12, "text": " well, hence is overfit.", "tokens": [50364, 440, 6191, 1433, 307, 300, 321, 603, 584, 341, 2316, 575, 670, 6845, 264, 1412, 11, 420, 341, 2316, 50678, 50678, 575, 364, 670, 69, 2414, 1154, 13, 50806, 50806, 1436, 754, 1673, 309, 9001, 264, 3097, 992, 588, 731, 11, 309, 575, 3318, 264, 1412, 1920, 886, 51034, 51034, 731, 11, 16678, 307, 670, 6845, 13, 51158, 51158, 400, 309, 775, 406, 574, 411, 341, 2316, 486, 2674, 1125, 281, 777, 5110, 300, 575, 1128, 51381, 51381, 1612, 949, 13, 51482, 51482, 3996, 1433, 337, 341, 307, 300, 264, 9284, 575, 1090, 21977, 13, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.1253864114934748, "compression_ratio": 1.7130044843049328, "no_speech_prob": 1.3419646165857557e-05}, {"id": 72, "seek": 34624, "start": 362.12, "end": 366.58, "text": " And it does not look like this model will generalize to new examples that has never", "tokens": [50364, 440, 6191, 1433, 307, 300, 321, 603, 584, 341, 2316, 575, 670, 6845, 264, 1412, 11, 420, 341, 2316, 50678, 50678, 575, 364, 670, 69, 2414, 1154, 13, 50806, 50806, 1436, 754, 1673, 309, 9001, 264, 3097, 992, 588, 731, 11, 309, 575, 3318, 264, 1412, 1920, 886, 51034, 51034, 731, 11, 16678, 307, 670, 6845, 13, 51158, 51158, 400, 309, 775, 406, 574, 411, 341, 2316, 486, 2674, 1125, 281, 777, 5110, 300, 575, 1128, 51381, 51381, 1612, 949, 13, 51482, 51482, 3996, 1433, 337, 341, 307, 300, 264, 9284, 575, 1090, 21977, 13, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.1253864114934748, "compression_ratio": 1.7130044843049328, "no_speech_prob": 1.3419646165857557e-05}, {"id": 73, "seek": 34624, "start": 366.58, "end": 368.6, "text": " seen before.", "tokens": [50364, 440, 6191, 1433, 307, 300, 321, 603, 584, 341, 2316, 575, 670, 6845, 264, 1412, 11, 420, 341, 2316, 50678, 50678, 575, 364, 670, 69, 2414, 1154, 13, 50806, 50806, 1436, 754, 1673, 309, 9001, 264, 3097, 992, 588, 731, 11, 309, 575, 3318, 264, 1412, 1920, 886, 51034, 51034, 731, 11, 16678, 307, 670, 6845, 13, 51158, 51158, 400, 309, 775, 406, 574, 411, 341, 2316, 486, 2674, 1125, 281, 777, 5110, 300, 575, 1128, 51381, 51381, 1612, 949, 13, 51482, 51482, 3996, 1433, 337, 341, 307, 300, 264, 9284, 575, 1090, 21977, 13, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.1253864114934748, "compression_ratio": 1.7130044843049328, "no_speech_prob": 1.3419646165857557e-05}, {"id": 74, "seek": 34624, "start": 368.6, "end": 374.64, "text": " Another term for this is that the algorithm has high variance.", "tokens": [50364, 440, 6191, 1433, 307, 300, 321, 603, 584, 341, 2316, 575, 670, 6845, 264, 1412, 11, 420, 341, 2316, 50678, 50678, 575, 364, 670, 69, 2414, 1154, 13, 50806, 50806, 1436, 754, 1673, 309, 9001, 264, 3097, 992, 588, 731, 11, 309, 575, 3318, 264, 1412, 1920, 886, 51034, 51034, 731, 11, 16678, 307, 670, 6845, 13, 51158, 51158, 400, 309, 775, 406, 574, 411, 341, 2316, 486, 2674, 1125, 281, 777, 5110, 300, 575, 1128, 51381, 51381, 1612, 949, 13, 51482, 51482, 3996, 1433, 337, 341, 307, 300, 264, 9284, 575, 1090, 21977, 13, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.1253864114934748, "compression_ratio": 1.7130044843049328, "no_speech_prob": 1.3419646165857557e-05}, {"id": 75, "seek": 37464, "start": 374.64, "end": 379.64, "text": " In machine learning, many people will use the terms overfit and high variance almost", "tokens": [50364, 682, 3479, 2539, 11, 867, 561, 486, 764, 264, 2115, 670, 6845, 293, 1090, 21977, 1920, 50614, 50614, 30358, 1188, 11, 293, 321, 764, 264, 2115, 833, 6845, 293, 1090, 12577, 1920, 30358, 1188, 13, 50946, 50946, 440, 24002, 2261, 670, 69, 2414, 420, 1090, 21977, 307, 300, 264, 9284, 307, 1382, 588, 11, 588, 51176, 51176, 1152, 281, 3318, 633, 2167, 3097, 1365, 13, 51364, 51364, 400, 309, 4523, 484, 300, 498, 428, 3097, 992, 645, 445, 754, 257, 707, 857, 819, 11, 584, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12661354193526708, "compression_ratio": 1.7853881278538812, "no_speech_prob": 4.495108896662714e-06}, {"id": 76, "seek": 37464, "start": 379.64, "end": 386.28, "text": " interchangeably, and we use the terms underfit and high bias almost interchangeably.", "tokens": [50364, 682, 3479, 2539, 11, 867, 561, 486, 764, 264, 2115, 670, 6845, 293, 1090, 21977, 1920, 50614, 50614, 30358, 1188, 11, 293, 321, 764, 264, 2115, 833, 6845, 293, 1090, 12577, 1920, 30358, 1188, 13, 50946, 50946, 440, 24002, 2261, 670, 69, 2414, 420, 1090, 21977, 307, 300, 264, 9284, 307, 1382, 588, 11, 588, 51176, 51176, 1152, 281, 3318, 633, 2167, 3097, 1365, 13, 51364, 51364, 400, 309, 4523, 484, 300, 498, 428, 3097, 992, 645, 445, 754, 257, 707, 857, 819, 11, 584, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12661354193526708, "compression_ratio": 1.7853881278538812, "no_speech_prob": 4.495108896662714e-06}, {"id": 77, "seek": 37464, "start": 386.28, "end": 390.88, "text": " The intuition behind overfitting or high variance is that the algorithm is trying very, very", "tokens": [50364, 682, 3479, 2539, 11, 867, 561, 486, 764, 264, 2115, 670, 6845, 293, 1090, 21977, 1920, 50614, 50614, 30358, 1188, 11, 293, 321, 764, 264, 2115, 833, 6845, 293, 1090, 12577, 1920, 30358, 1188, 13, 50946, 50946, 440, 24002, 2261, 670, 69, 2414, 420, 1090, 21977, 307, 300, 264, 9284, 307, 1382, 588, 11, 588, 51176, 51176, 1152, 281, 3318, 633, 2167, 3097, 1365, 13, 51364, 51364, 400, 309, 4523, 484, 300, 498, 428, 3097, 992, 645, 445, 754, 257, 707, 857, 819, 11, 584, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12661354193526708, "compression_ratio": 1.7853881278538812, "no_speech_prob": 4.495108896662714e-06}, {"id": 78, "seek": 37464, "start": 390.88, "end": 394.64, "text": " hard to fit every single training example.", "tokens": [50364, 682, 3479, 2539, 11, 867, 561, 486, 764, 264, 2115, 670, 6845, 293, 1090, 21977, 1920, 50614, 50614, 30358, 1188, 11, 293, 321, 764, 264, 2115, 833, 6845, 293, 1090, 12577, 1920, 30358, 1188, 13, 50946, 50946, 440, 24002, 2261, 670, 69, 2414, 420, 1090, 21977, 307, 300, 264, 9284, 307, 1382, 588, 11, 588, 51176, 51176, 1152, 281, 3318, 633, 2167, 3097, 1365, 13, 51364, 51364, 400, 309, 4523, 484, 300, 498, 428, 3097, 992, 645, 445, 754, 257, 707, 857, 819, 11, 584, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12661354193526708, "compression_ratio": 1.7853881278538812, "no_speech_prob": 4.495108896662714e-06}, {"id": 79, "seek": 37464, "start": 394.64, "end": 399.64, "text": " And it turns out that if your training set were just even a little bit different, say", "tokens": [50364, 682, 3479, 2539, 11, 867, 561, 486, 764, 264, 2115, 670, 6845, 293, 1090, 21977, 1920, 50614, 50614, 30358, 1188, 11, 293, 321, 764, 264, 2115, 833, 6845, 293, 1090, 12577, 1920, 30358, 1188, 13, 50946, 50946, 440, 24002, 2261, 670, 69, 2414, 420, 1090, 21977, 307, 300, 264, 9284, 307, 1382, 588, 11, 588, 51176, 51176, 1152, 281, 3318, 633, 2167, 3097, 1365, 13, 51364, 51364, 400, 309, 4523, 484, 300, 498, 428, 3097, 992, 645, 445, 754, 257, 707, 857, 819, 11, 584, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12661354193526708, "compression_ratio": 1.7853881278538812, "no_speech_prob": 4.495108896662714e-06}, {"id": 80, "seek": 39964, "start": 399.64, "end": 404.91999999999996, "text": " one house was priced just a little bit more, a little bit less, then the function that", "tokens": [50364, 472, 1782, 390, 30349, 445, 257, 707, 857, 544, 11, 257, 707, 857, 1570, 11, 550, 264, 2445, 300, 50628, 50628, 264, 9284, 9001, 727, 917, 493, 885, 3879, 819, 13, 50842, 50842, 407, 498, 732, 819, 3479, 2539, 11955, 645, 281, 3318, 341, 6409, 1668, 26110, 2316, 51184, 51184, 281, 445, 4748, 819, 1412, 6352, 11, 436, 727, 917, 493, 365, 3879, 819, 21264, 51400, 51400, 420, 5405, 7006, 21264, 13, 51540, 51540, 400, 300, 311, 983, 321, 584, 264, 9284, 575, 1090, 21977, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.09728189044528537, "compression_ratio": 1.8043478260869565, "no_speech_prob": 8.39765380078461e-06}, {"id": 81, "seek": 39964, "start": 404.91999999999996, "end": 409.2, "text": " the algorithm fits could end up being totally different.", "tokens": [50364, 472, 1782, 390, 30349, 445, 257, 707, 857, 544, 11, 257, 707, 857, 1570, 11, 550, 264, 2445, 300, 50628, 50628, 264, 9284, 9001, 727, 917, 493, 885, 3879, 819, 13, 50842, 50842, 407, 498, 732, 819, 3479, 2539, 11955, 645, 281, 3318, 341, 6409, 1668, 26110, 2316, 51184, 51184, 281, 445, 4748, 819, 1412, 6352, 11, 436, 727, 917, 493, 365, 3879, 819, 21264, 51400, 51400, 420, 5405, 7006, 21264, 13, 51540, 51540, 400, 300, 311, 983, 321, 584, 264, 9284, 575, 1090, 21977, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.09728189044528537, "compression_ratio": 1.8043478260869565, "no_speech_prob": 8.39765380078461e-06}, {"id": 82, "seek": 39964, "start": 409.2, "end": 416.03999999999996, "text": " So if two different machine learning engineers were to fit this fourth order polynomial model", "tokens": [50364, 472, 1782, 390, 30349, 445, 257, 707, 857, 544, 11, 257, 707, 857, 1570, 11, 550, 264, 2445, 300, 50628, 50628, 264, 9284, 9001, 727, 917, 493, 885, 3879, 819, 13, 50842, 50842, 407, 498, 732, 819, 3479, 2539, 11955, 645, 281, 3318, 341, 6409, 1668, 26110, 2316, 51184, 51184, 281, 445, 4748, 819, 1412, 6352, 11, 436, 727, 917, 493, 365, 3879, 819, 21264, 51400, 51400, 420, 5405, 7006, 21264, 13, 51540, 51540, 400, 300, 311, 983, 321, 584, 264, 9284, 575, 1090, 21977, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.09728189044528537, "compression_ratio": 1.8043478260869565, "no_speech_prob": 8.39765380078461e-06}, {"id": 83, "seek": 39964, "start": 416.03999999999996, "end": 420.36, "text": " to just slightly different data sets, they could end up with totally different predictions", "tokens": [50364, 472, 1782, 390, 30349, 445, 257, 707, 857, 544, 11, 257, 707, 857, 1570, 11, 550, 264, 2445, 300, 50628, 50628, 264, 9284, 9001, 727, 917, 493, 885, 3879, 819, 13, 50842, 50842, 407, 498, 732, 819, 3479, 2539, 11955, 645, 281, 3318, 341, 6409, 1668, 26110, 2316, 51184, 51184, 281, 445, 4748, 819, 1412, 6352, 11, 436, 727, 917, 493, 365, 3879, 819, 21264, 51400, 51400, 420, 5405, 7006, 21264, 13, 51540, 51540, 400, 300, 311, 983, 321, 584, 264, 9284, 575, 1090, 21977, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.09728189044528537, "compression_ratio": 1.8043478260869565, "no_speech_prob": 8.39765380078461e-06}, {"id": 84, "seek": 39964, "start": 420.36, "end": 423.15999999999997, "text": " or highly variable predictions.", "tokens": [50364, 472, 1782, 390, 30349, 445, 257, 707, 857, 544, 11, 257, 707, 857, 1570, 11, 550, 264, 2445, 300, 50628, 50628, 264, 9284, 9001, 727, 917, 493, 885, 3879, 819, 13, 50842, 50842, 407, 498, 732, 819, 3479, 2539, 11955, 645, 281, 3318, 341, 6409, 1668, 26110, 2316, 51184, 51184, 281, 445, 4748, 819, 1412, 6352, 11, 436, 727, 917, 493, 365, 3879, 819, 21264, 51400, 51400, 420, 5405, 7006, 21264, 13, 51540, 51540, 400, 300, 311, 983, 321, 584, 264, 9284, 575, 1090, 21977, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.09728189044528537, "compression_ratio": 1.8043478260869565, "no_speech_prob": 8.39765380078461e-06}, {"id": 85, "seek": 39964, "start": 423.15999999999997, "end": 428.76, "text": " And that's why we say the algorithm has high variance.", "tokens": [50364, 472, 1782, 390, 30349, 445, 257, 707, 857, 544, 11, 257, 707, 857, 1570, 11, 550, 264, 2445, 300, 50628, 50628, 264, 9284, 9001, 727, 917, 493, 885, 3879, 819, 13, 50842, 50842, 407, 498, 732, 819, 3479, 2539, 11955, 645, 281, 3318, 341, 6409, 1668, 26110, 2316, 51184, 51184, 281, 445, 4748, 819, 1412, 6352, 11, 436, 727, 917, 493, 365, 3879, 819, 21264, 51400, 51400, 420, 5405, 7006, 21264, 13, 51540, 51540, 400, 300, 311, 983, 321, 584, 264, 9284, 575, 1090, 21977, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.09728189044528537, "compression_ratio": 1.8043478260869565, "no_speech_prob": 8.39765380078461e-06}, {"id": 86, "seek": 42876, "start": 428.76, "end": 434.36, "text": " Using this rightmost model with the one in the middle for the same house, it seems the", "tokens": [50364, 11142, 341, 558, 1761, 2316, 365, 264, 472, 294, 264, 2808, 337, 264, 912, 1782, 11, 309, 2544, 264, 50644, 50644, 2808, 2316, 2709, 264, 709, 544, 10585, 17630, 337, 3218, 13, 50884, 50884, 821, 1943, 380, 534, 257, 1315, 337, 341, 1389, 294, 264, 2808, 11, 457, 286, 478, 445, 516, 281, 818, 341, 51098, 51098, 445, 558, 570, 309, 307, 9662, 833, 6845, 6051, 670, 6845, 13, 51350, 51350, 407, 321, 393, 584, 300, 264, 3387, 295, 3479, 2539, 307, 281, 915, 257, 2316, 300, 4696, 307, 9662, 51604, 51604, 833, 69, 2414, 6051, 670, 69, 2414, 13, 51718, 51718], "temperature": 0.0, "avg_logprob": -0.09915143709916335, "compression_ratio": 1.774468085106383, "no_speech_prob": 1.5445499229826964e-05}, {"id": 87, "seek": 42876, "start": 434.36, "end": 439.15999999999997, "text": " middle model gives the much more reasonable prediction for price.", "tokens": [50364, 11142, 341, 558, 1761, 2316, 365, 264, 472, 294, 264, 2808, 337, 264, 912, 1782, 11, 309, 2544, 264, 50644, 50644, 2808, 2316, 2709, 264, 709, 544, 10585, 17630, 337, 3218, 13, 50884, 50884, 821, 1943, 380, 534, 257, 1315, 337, 341, 1389, 294, 264, 2808, 11, 457, 286, 478, 445, 516, 281, 818, 341, 51098, 51098, 445, 558, 570, 309, 307, 9662, 833, 6845, 6051, 670, 6845, 13, 51350, 51350, 407, 321, 393, 584, 300, 264, 3387, 295, 3479, 2539, 307, 281, 915, 257, 2316, 300, 4696, 307, 9662, 51604, 51604, 833, 69, 2414, 6051, 670, 69, 2414, 13, 51718, 51718], "temperature": 0.0, "avg_logprob": -0.09915143709916335, "compression_ratio": 1.774468085106383, "no_speech_prob": 1.5445499229826964e-05}, {"id": 88, "seek": 42876, "start": 439.15999999999997, "end": 443.44, "text": " There isn't really a name for this case in the middle, but I'm just going to call this", "tokens": [50364, 11142, 341, 558, 1761, 2316, 365, 264, 472, 294, 264, 2808, 337, 264, 912, 1782, 11, 309, 2544, 264, 50644, 50644, 2808, 2316, 2709, 264, 709, 544, 10585, 17630, 337, 3218, 13, 50884, 50884, 821, 1943, 380, 534, 257, 1315, 337, 341, 1389, 294, 264, 2808, 11, 457, 286, 478, 445, 516, 281, 818, 341, 51098, 51098, 445, 558, 570, 309, 307, 9662, 833, 6845, 6051, 670, 6845, 13, 51350, 51350, 407, 321, 393, 584, 300, 264, 3387, 295, 3479, 2539, 307, 281, 915, 257, 2316, 300, 4696, 307, 9662, 51604, 51604, 833, 69, 2414, 6051, 670, 69, 2414, 13, 51718, 51718], "temperature": 0.0, "avg_logprob": -0.09915143709916335, "compression_ratio": 1.774468085106383, "no_speech_prob": 1.5445499229826964e-05}, {"id": 89, "seek": 42876, "start": 443.44, "end": 448.48, "text": " just right because it is neither underfit nor overfit.", "tokens": [50364, 11142, 341, 558, 1761, 2316, 365, 264, 472, 294, 264, 2808, 337, 264, 912, 1782, 11, 309, 2544, 264, 50644, 50644, 2808, 2316, 2709, 264, 709, 544, 10585, 17630, 337, 3218, 13, 50884, 50884, 821, 1943, 380, 534, 257, 1315, 337, 341, 1389, 294, 264, 2808, 11, 457, 286, 478, 445, 516, 281, 818, 341, 51098, 51098, 445, 558, 570, 309, 307, 9662, 833, 6845, 6051, 670, 6845, 13, 51350, 51350, 407, 321, 393, 584, 300, 264, 3387, 295, 3479, 2539, 307, 281, 915, 257, 2316, 300, 4696, 307, 9662, 51604, 51604, 833, 69, 2414, 6051, 670, 69, 2414, 13, 51718, 51718], "temperature": 0.0, "avg_logprob": -0.09915143709916335, "compression_ratio": 1.774468085106383, "no_speech_prob": 1.5445499229826964e-05}, {"id": 90, "seek": 42876, "start": 448.48, "end": 453.56, "text": " So we can say that the goal of machine learning is to find a model that hopefully is neither", "tokens": [50364, 11142, 341, 558, 1761, 2316, 365, 264, 472, 294, 264, 2808, 337, 264, 912, 1782, 11, 309, 2544, 264, 50644, 50644, 2808, 2316, 2709, 264, 709, 544, 10585, 17630, 337, 3218, 13, 50884, 50884, 821, 1943, 380, 534, 257, 1315, 337, 341, 1389, 294, 264, 2808, 11, 457, 286, 478, 445, 516, 281, 818, 341, 51098, 51098, 445, 558, 570, 309, 307, 9662, 833, 6845, 6051, 670, 6845, 13, 51350, 51350, 407, 321, 393, 584, 300, 264, 3387, 295, 3479, 2539, 307, 281, 915, 257, 2316, 300, 4696, 307, 9662, 51604, 51604, 833, 69, 2414, 6051, 670, 69, 2414, 13, 51718, 51718], "temperature": 0.0, "avg_logprob": -0.09915143709916335, "compression_ratio": 1.774468085106383, "no_speech_prob": 1.5445499229826964e-05}, {"id": 91, "seek": 42876, "start": 453.56, "end": 455.84, "text": " underfitting nor overfitting.", "tokens": [50364, 11142, 341, 558, 1761, 2316, 365, 264, 472, 294, 264, 2808, 337, 264, 912, 1782, 11, 309, 2544, 264, 50644, 50644, 2808, 2316, 2709, 264, 709, 544, 10585, 17630, 337, 3218, 13, 50884, 50884, 821, 1943, 380, 534, 257, 1315, 337, 341, 1389, 294, 264, 2808, 11, 457, 286, 478, 445, 516, 281, 818, 341, 51098, 51098, 445, 558, 570, 309, 307, 9662, 833, 6845, 6051, 670, 6845, 13, 51350, 51350, 407, 321, 393, 584, 300, 264, 3387, 295, 3479, 2539, 307, 281, 915, 257, 2316, 300, 4696, 307, 9662, 51604, 51604, 833, 69, 2414, 6051, 670, 69, 2414, 13, 51718, 51718], "temperature": 0.0, "avg_logprob": -0.09915143709916335, "compression_ratio": 1.774468085106383, "no_speech_prob": 1.5445499229826964e-05}, {"id": 92, "seek": 45584, "start": 455.84, "end": 462.91999999999996, "text": " In other words, hopefully a model that has neither high bias nor high variance.", "tokens": [50364, 682, 661, 2283, 11, 4696, 257, 2316, 300, 575, 9662, 1090, 12577, 6051, 1090, 21977, 13, 50718, 50718, 1133, 286, 519, 466, 833, 69, 2414, 293, 670, 69, 2414, 11, 1090, 12577, 293, 1090, 21977, 11, 286, 478, 2171, 50988, 50988, 15920, 295, 264, 2227, 311, 1657, 295, 6731, 388, 35507, 293, 264, 1045, 17276, 13, 51286, 51286, 682, 341, 2227, 311, 17172, 11, 257, 2013, 1219, 6731, 388, 35507, 17753, 264, 1280, 295, 257, 6155, 1605, 13, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.08029781899801115, "compression_ratio": 1.6564102564102565, "no_speech_prob": 5.1738134061452e-06}, {"id": 93, "seek": 45584, "start": 462.91999999999996, "end": 468.32, "text": " When I think about underfitting and overfitting, high bias and high variance, I'm sometimes", "tokens": [50364, 682, 661, 2283, 11, 4696, 257, 2316, 300, 575, 9662, 1090, 12577, 6051, 1090, 21977, 13, 50718, 50718, 1133, 286, 519, 466, 833, 69, 2414, 293, 670, 69, 2414, 11, 1090, 12577, 293, 1090, 21977, 11, 286, 478, 2171, 50988, 50988, 15920, 295, 264, 2227, 311, 1657, 295, 6731, 388, 35507, 293, 264, 1045, 17276, 13, 51286, 51286, 682, 341, 2227, 311, 17172, 11, 257, 2013, 1219, 6731, 388, 35507, 17753, 264, 1280, 295, 257, 6155, 1605, 13, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.08029781899801115, "compression_ratio": 1.6564102564102565, "no_speech_prob": 5.1738134061452e-06}, {"id": 94, "seek": 45584, "start": 468.32, "end": 474.28, "text": " reminded of the children's story of Goldilocks and the three bears.", "tokens": [50364, 682, 661, 2283, 11, 4696, 257, 2316, 300, 575, 9662, 1090, 12577, 6051, 1090, 21977, 13, 50718, 50718, 1133, 286, 519, 466, 833, 69, 2414, 293, 670, 69, 2414, 11, 1090, 12577, 293, 1090, 21977, 11, 286, 478, 2171, 50988, 50988, 15920, 295, 264, 2227, 311, 1657, 295, 6731, 388, 35507, 293, 264, 1045, 17276, 13, 51286, 51286, 682, 341, 2227, 311, 17172, 11, 257, 2013, 1219, 6731, 388, 35507, 17753, 264, 1280, 295, 257, 6155, 1605, 13, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.08029781899801115, "compression_ratio": 1.6564102564102565, "no_speech_prob": 5.1738134061452e-06}, {"id": 95, "seek": 45584, "start": 474.28, "end": 481.0, "text": " In this children's tale, a girl called Goldilocks visits the home of a bear family.", "tokens": [50364, 682, 661, 2283, 11, 4696, 257, 2316, 300, 575, 9662, 1090, 12577, 6051, 1090, 21977, 13, 50718, 50718, 1133, 286, 519, 466, 833, 69, 2414, 293, 670, 69, 2414, 11, 1090, 12577, 293, 1090, 21977, 11, 286, 478, 2171, 50988, 50988, 15920, 295, 264, 2227, 311, 1657, 295, 6731, 388, 35507, 293, 264, 1045, 17276, 13, 51286, 51286, 682, 341, 2227, 311, 17172, 11, 257, 2013, 1219, 6731, 388, 35507, 17753, 264, 1280, 295, 257, 6155, 1605, 13, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.08029781899801115, "compression_ratio": 1.6564102564102565, "no_speech_prob": 5.1738134061452e-06}, {"id": 96, "seek": 48100, "start": 481.0, "end": 486.84, "text": " There's a bowl of porridge that's too cold to taste and so that's no good.", "tokens": [50364, 821, 311, 257, 6571, 295, 38872, 300, 311, 886, 3554, 281, 3939, 293, 370, 300, 311, 572, 665, 13, 50656, 50656, 821, 311, 611, 257, 6571, 295, 38872, 300, 311, 886, 2368, 281, 1862, 13, 50840, 50840, 407, 300, 311, 572, 665, 2139, 13, 50928, 50928, 583, 456, 311, 257, 6571, 295, 38872, 300, 307, 9662, 886, 3554, 6051, 886, 2368, 13, 51114, 51114, 440, 4292, 307, 294, 264, 2808, 11, 597, 307, 445, 558, 281, 1862, 13, 51352, 51352, 407, 281, 20928, 11, 498, 291, 362, 886, 867, 4122, 11, 411, 264, 1577, 1281, 26110, 322, 264, 558, 11, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.10741440149453971, "compression_ratio": 1.963350785340314, "no_speech_prob": 3.6476700188359246e-05}, {"id": 97, "seek": 48100, "start": 486.84, "end": 490.52, "text": " There's also a bowl of porridge that's too hot to eat.", "tokens": [50364, 821, 311, 257, 6571, 295, 38872, 300, 311, 886, 3554, 281, 3939, 293, 370, 300, 311, 572, 665, 13, 50656, 50656, 821, 311, 611, 257, 6571, 295, 38872, 300, 311, 886, 2368, 281, 1862, 13, 50840, 50840, 407, 300, 311, 572, 665, 2139, 13, 50928, 50928, 583, 456, 311, 257, 6571, 295, 38872, 300, 307, 9662, 886, 3554, 6051, 886, 2368, 13, 51114, 51114, 440, 4292, 307, 294, 264, 2808, 11, 597, 307, 445, 558, 281, 1862, 13, 51352, 51352, 407, 281, 20928, 11, 498, 291, 362, 886, 867, 4122, 11, 411, 264, 1577, 1281, 26110, 322, 264, 558, 11, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.10741440149453971, "compression_ratio": 1.963350785340314, "no_speech_prob": 3.6476700188359246e-05}, {"id": 98, "seek": 48100, "start": 490.52, "end": 492.28, "text": " So that's no good either.", "tokens": [50364, 821, 311, 257, 6571, 295, 38872, 300, 311, 886, 3554, 281, 3939, 293, 370, 300, 311, 572, 665, 13, 50656, 50656, 821, 311, 611, 257, 6571, 295, 38872, 300, 311, 886, 2368, 281, 1862, 13, 50840, 50840, 407, 300, 311, 572, 665, 2139, 13, 50928, 50928, 583, 456, 311, 257, 6571, 295, 38872, 300, 307, 9662, 886, 3554, 6051, 886, 2368, 13, 51114, 51114, 440, 4292, 307, 294, 264, 2808, 11, 597, 307, 445, 558, 281, 1862, 13, 51352, 51352, 407, 281, 20928, 11, 498, 291, 362, 886, 867, 4122, 11, 411, 264, 1577, 1281, 26110, 322, 264, 558, 11, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.10741440149453971, "compression_ratio": 1.963350785340314, "no_speech_prob": 3.6476700188359246e-05}, {"id": 99, "seek": 48100, "start": 492.28, "end": 496.0, "text": " But there's a bowl of porridge that is neither too cold nor too hot.", "tokens": [50364, 821, 311, 257, 6571, 295, 38872, 300, 311, 886, 3554, 281, 3939, 293, 370, 300, 311, 572, 665, 13, 50656, 50656, 821, 311, 611, 257, 6571, 295, 38872, 300, 311, 886, 2368, 281, 1862, 13, 50840, 50840, 407, 300, 311, 572, 665, 2139, 13, 50928, 50928, 583, 456, 311, 257, 6571, 295, 38872, 300, 307, 9662, 886, 3554, 6051, 886, 2368, 13, 51114, 51114, 440, 4292, 307, 294, 264, 2808, 11, 597, 307, 445, 558, 281, 1862, 13, 51352, 51352, 407, 281, 20928, 11, 498, 291, 362, 886, 867, 4122, 11, 411, 264, 1577, 1281, 26110, 322, 264, 558, 11, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.10741440149453971, "compression_ratio": 1.963350785340314, "no_speech_prob": 3.6476700188359246e-05}, {"id": 100, "seek": 48100, "start": 496.0, "end": 500.76, "text": " The temperature is in the middle, which is just right to eat.", "tokens": [50364, 821, 311, 257, 6571, 295, 38872, 300, 311, 886, 3554, 281, 3939, 293, 370, 300, 311, 572, 665, 13, 50656, 50656, 821, 311, 611, 257, 6571, 295, 38872, 300, 311, 886, 2368, 281, 1862, 13, 50840, 50840, 407, 300, 311, 572, 665, 2139, 13, 50928, 50928, 583, 456, 311, 257, 6571, 295, 38872, 300, 307, 9662, 886, 3554, 6051, 886, 2368, 13, 51114, 51114, 440, 4292, 307, 294, 264, 2808, 11, 597, 307, 445, 558, 281, 1862, 13, 51352, 51352, 407, 281, 20928, 11, 498, 291, 362, 886, 867, 4122, 11, 411, 264, 1577, 1281, 26110, 322, 264, 558, 11, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.10741440149453971, "compression_ratio": 1.963350785340314, "no_speech_prob": 3.6476700188359246e-05}, {"id": 101, "seek": 48100, "start": 500.76, "end": 506.12, "text": " So to recap, if you have too many features, like the full water polynomial on the right,", "tokens": [50364, 821, 311, 257, 6571, 295, 38872, 300, 311, 886, 3554, 281, 3939, 293, 370, 300, 311, 572, 665, 13, 50656, 50656, 821, 311, 611, 257, 6571, 295, 38872, 300, 311, 886, 2368, 281, 1862, 13, 50840, 50840, 407, 300, 311, 572, 665, 2139, 13, 50928, 50928, 583, 456, 311, 257, 6571, 295, 38872, 300, 307, 9662, 886, 3554, 6051, 886, 2368, 13, 51114, 51114, 440, 4292, 307, 294, 264, 2808, 11, 597, 307, 445, 558, 281, 1862, 13, 51352, 51352, 407, 281, 20928, 11, 498, 291, 362, 886, 867, 4122, 11, 411, 264, 1577, 1281, 26110, 322, 264, 558, 11, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.10741440149453971, "compression_ratio": 1.963350785340314, "no_speech_prob": 3.6476700188359246e-05}, {"id": 102, "seek": 50612, "start": 506.12, "end": 511.32, "text": " then the model may fit the training set well, but almost too well or overfit in the high", "tokens": [50364, 550, 264, 2316, 815, 3318, 264, 3097, 992, 731, 11, 457, 1920, 886, 731, 420, 670, 6845, 294, 264, 1090, 50624, 50624, 21977, 13, 50694, 50694, 1282, 264, 7929, 1252, 11, 498, 291, 362, 886, 1326, 4122, 11, 550, 294, 341, 1365, 11, 411, 264, 472, 322, 264, 50920, 50920, 1411, 11, 309, 833, 13979, 293, 575, 1090, 12577, 13, 51092, 51092, 400, 294, 341, 1365, 11, 1228, 37262, 4122, 2031, 293, 2031, 8889, 11, 300, 2544, 281, 312, 445, 558, 13, 51447, 51447, 407, 1400, 11, 321, 600, 2956, 412, 833, 69, 2414, 293, 670, 69, 2414, 337, 8213, 24590, 2316, 13, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.1404832233892423, "compression_ratio": 1.6736401673640167, "no_speech_prob": 3.785178250836907e-06}, {"id": 103, "seek": 50612, "start": 511.32, "end": 512.72, "text": " variance.", "tokens": [50364, 550, 264, 2316, 815, 3318, 264, 3097, 992, 731, 11, 457, 1920, 886, 731, 420, 670, 6845, 294, 264, 1090, 50624, 50624, 21977, 13, 50694, 50694, 1282, 264, 7929, 1252, 11, 498, 291, 362, 886, 1326, 4122, 11, 550, 294, 341, 1365, 11, 411, 264, 472, 322, 264, 50920, 50920, 1411, 11, 309, 833, 13979, 293, 575, 1090, 12577, 13, 51092, 51092, 400, 294, 341, 1365, 11, 1228, 37262, 4122, 2031, 293, 2031, 8889, 11, 300, 2544, 281, 312, 445, 558, 13, 51447, 51447, 407, 1400, 11, 321, 600, 2956, 412, 833, 69, 2414, 293, 670, 69, 2414, 337, 8213, 24590, 2316, 13, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.1404832233892423, "compression_ratio": 1.6736401673640167, "no_speech_prob": 3.785178250836907e-06}, {"id": 104, "seek": 50612, "start": 512.72, "end": 517.24, "text": " On the flip side, if you have too few features, then in this example, like the one on the", "tokens": [50364, 550, 264, 2316, 815, 3318, 264, 3097, 992, 731, 11, 457, 1920, 886, 731, 420, 670, 6845, 294, 264, 1090, 50624, 50624, 21977, 13, 50694, 50694, 1282, 264, 7929, 1252, 11, 498, 291, 362, 886, 1326, 4122, 11, 550, 294, 341, 1365, 11, 411, 264, 472, 322, 264, 50920, 50920, 1411, 11, 309, 833, 13979, 293, 575, 1090, 12577, 13, 51092, 51092, 400, 294, 341, 1365, 11, 1228, 37262, 4122, 2031, 293, 2031, 8889, 11, 300, 2544, 281, 312, 445, 558, 13, 51447, 51447, 407, 1400, 11, 321, 600, 2956, 412, 833, 69, 2414, 293, 670, 69, 2414, 337, 8213, 24590, 2316, 13, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.1404832233892423, "compression_ratio": 1.6736401673640167, "no_speech_prob": 3.785178250836907e-06}, {"id": 105, "seek": 50612, "start": 517.24, "end": 520.68, "text": " left, it underfits and has high bias.", "tokens": [50364, 550, 264, 2316, 815, 3318, 264, 3097, 992, 731, 11, 457, 1920, 886, 731, 420, 670, 6845, 294, 264, 1090, 50624, 50624, 21977, 13, 50694, 50694, 1282, 264, 7929, 1252, 11, 498, 291, 362, 886, 1326, 4122, 11, 550, 294, 341, 1365, 11, 411, 264, 472, 322, 264, 50920, 50920, 1411, 11, 309, 833, 13979, 293, 575, 1090, 12577, 13, 51092, 51092, 400, 294, 341, 1365, 11, 1228, 37262, 4122, 2031, 293, 2031, 8889, 11, 300, 2544, 281, 312, 445, 558, 13, 51447, 51447, 407, 1400, 11, 321, 600, 2956, 412, 833, 69, 2414, 293, 670, 69, 2414, 337, 8213, 24590, 2316, 13, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.1404832233892423, "compression_ratio": 1.6736401673640167, "no_speech_prob": 3.785178250836907e-06}, {"id": 106, "seek": 50612, "start": 520.68, "end": 527.78, "text": " And in this example, using quadratic features x and x squared, that seems to be just right.", "tokens": [50364, 550, 264, 2316, 815, 3318, 264, 3097, 992, 731, 11, 457, 1920, 886, 731, 420, 670, 6845, 294, 264, 1090, 50624, 50624, 21977, 13, 50694, 50694, 1282, 264, 7929, 1252, 11, 498, 291, 362, 886, 1326, 4122, 11, 550, 294, 341, 1365, 11, 411, 264, 472, 322, 264, 50920, 50920, 1411, 11, 309, 833, 13979, 293, 575, 1090, 12577, 13, 51092, 51092, 400, 294, 341, 1365, 11, 1228, 37262, 4122, 2031, 293, 2031, 8889, 11, 300, 2544, 281, 312, 445, 558, 13, 51447, 51447, 407, 1400, 11, 321, 600, 2956, 412, 833, 69, 2414, 293, 670, 69, 2414, 337, 8213, 24590, 2316, 13, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.1404832233892423, "compression_ratio": 1.6736401673640167, "no_speech_prob": 3.785178250836907e-06}, {"id": 107, "seek": 50612, "start": 527.78, "end": 532.84, "text": " So far, we've looked at underfitting and overfitting for linear regression model.", "tokens": [50364, 550, 264, 2316, 815, 3318, 264, 3097, 992, 731, 11, 457, 1920, 886, 731, 420, 670, 6845, 294, 264, 1090, 50624, 50624, 21977, 13, 50694, 50694, 1282, 264, 7929, 1252, 11, 498, 291, 362, 886, 1326, 4122, 11, 550, 294, 341, 1365, 11, 411, 264, 472, 322, 264, 50920, 50920, 1411, 11, 309, 833, 13979, 293, 575, 1090, 12577, 13, 51092, 51092, 400, 294, 341, 1365, 11, 1228, 37262, 4122, 2031, 293, 2031, 8889, 11, 300, 2544, 281, 312, 445, 558, 13, 51447, 51447, 407, 1400, 11, 321, 600, 2956, 412, 833, 69, 2414, 293, 670, 69, 2414, 337, 8213, 24590, 2316, 13, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.1404832233892423, "compression_ratio": 1.6736401673640167, "no_speech_prob": 3.785178250836907e-06}, {"id": 108, "seek": 53284, "start": 532.84, "end": 537.0, "text": " Similarly, overfitting applies to classification as well.", "tokens": [50364, 13157, 11, 670, 69, 2414, 13165, 281, 21538, 382, 731, 13, 50572, 50572, 1692, 311, 257, 21538, 1365, 365, 732, 4122, 2031, 16, 293, 2031, 17, 11, 689, 2031, 16, 307, 1310, 264, 22512, 2744, 50886, 50886, 293, 2031, 17, 307, 264, 3205, 295, 4537, 13, 51041, 51041, 400, 321, 434, 1382, 281, 33872, 498, 257, 22512, 307, 2806, 36818, 420, 3271, 788, 11, 382, 1441, 23325, 538, 613, 28467, 51338, 51338, 293, 13040, 13, 51436, 51436, 1485, 551, 291, 393, 360, 307, 3318, 257, 3565, 3142, 24590, 2316, 11, 445, 257, 2199, 2316, 411, 341, 11, 689, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.11324078428979013, "compression_ratio": 1.5899581589958158, "no_speech_prob": 3.393106680960045e-06}, {"id": 109, "seek": 53284, "start": 537.0, "end": 543.2800000000001, "text": " Here's a classification example with two features x1 and x2, where x1 is maybe the tumor size", "tokens": [50364, 13157, 11, 670, 69, 2414, 13165, 281, 21538, 382, 731, 13, 50572, 50572, 1692, 311, 257, 21538, 1365, 365, 732, 4122, 2031, 16, 293, 2031, 17, 11, 689, 2031, 16, 307, 1310, 264, 22512, 2744, 50886, 50886, 293, 2031, 17, 307, 264, 3205, 295, 4537, 13, 51041, 51041, 400, 321, 434, 1382, 281, 33872, 498, 257, 22512, 307, 2806, 36818, 420, 3271, 788, 11, 382, 1441, 23325, 538, 613, 28467, 51338, 51338, 293, 13040, 13, 51436, 51436, 1485, 551, 291, 393, 360, 307, 3318, 257, 3565, 3142, 24590, 2316, 11, 445, 257, 2199, 2316, 411, 341, 11, 689, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.11324078428979013, "compression_ratio": 1.5899581589958158, "no_speech_prob": 3.393106680960045e-06}, {"id": 110, "seek": 53284, "start": 543.2800000000001, "end": 546.38, "text": " and x2 is the age of patient.", "tokens": [50364, 13157, 11, 670, 69, 2414, 13165, 281, 21538, 382, 731, 13, 50572, 50572, 1692, 311, 257, 21538, 1365, 365, 732, 4122, 2031, 16, 293, 2031, 17, 11, 689, 2031, 16, 307, 1310, 264, 22512, 2744, 50886, 50886, 293, 2031, 17, 307, 264, 3205, 295, 4537, 13, 51041, 51041, 400, 321, 434, 1382, 281, 33872, 498, 257, 22512, 307, 2806, 36818, 420, 3271, 788, 11, 382, 1441, 23325, 538, 613, 28467, 51338, 51338, 293, 13040, 13, 51436, 51436, 1485, 551, 291, 393, 360, 307, 3318, 257, 3565, 3142, 24590, 2316, 11, 445, 257, 2199, 2316, 411, 341, 11, 689, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.11324078428979013, "compression_ratio": 1.5899581589958158, "no_speech_prob": 3.393106680960045e-06}, {"id": 111, "seek": 53284, "start": 546.38, "end": 552.32, "text": " And we're trying to classify if a tumor is malignant or benign, as denoted by these crosses", "tokens": [50364, 13157, 11, 670, 69, 2414, 13165, 281, 21538, 382, 731, 13, 50572, 50572, 1692, 311, 257, 21538, 1365, 365, 732, 4122, 2031, 16, 293, 2031, 17, 11, 689, 2031, 16, 307, 1310, 264, 22512, 2744, 50886, 50886, 293, 2031, 17, 307, 264, 3205, 295, 4537, 13, 51041, 51041, 400, 321, 434, 1382, 281, 33872, 498, 257, 22512, 307, 2806, 36818, 420, 3271, 788, 11, 382, 1441, 23325, 538, 613, 28467, 51338, 51338, 293, 13040, 13, 51436, 51436, 1485, 551, 291, 393, 360, 307, 3318, 257, 3565, 3142, 24590, 2316, 11, 445, 257, 2199, 2316, 411, 341, 11, 689, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.11324078428979013, "compression_ratio": 1.5899581589958158, "no_speech_prob": 3.393106680960045e-06}, {"id": 112, "seek": 53284, "start": 552.32, "end": 554.2800000000001, "text": " and circles.", "tokens": [50364, 13157, 11, 670, 69, 2414, 13165, 281, 21538, 382, 731, 13, 50572, 50572, 1692, 311, 257, 21538, 1365, 365, 732, 4122, 2031, 16, 293, 2031, 17, 11, 689, 2031, 16, 307, 1310, 264, 22512, 2744, 50886, 50886, 293, 2031, 17, 307, 264, 3205, 295, 4537, 13, 51041, 51041, 400, 321, 434, 1382, 281, 33872, 498, 257, 22512, 307, 2806, 36818, 420, 3271, 788, 11, 382, 1441, 23325, 538, 613, 28467, 51338, 51338, 293, 13040, 13, 51436, 51436, 1485, 551, 291, 393, 360, 307, 3318, 257, 3565, 3142, 24590, 2316, 11, 445, 257, 2199, 2316, 411, 341, 11, 689, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.11324078428979013, "compression_ratio": 1.5899581589958158, "no_speech_prob": 3.393106680960045e-06}, {"id": 113, "seek": 53284, "start": 554.2800000000001, "end": 561.0, "text": " One thing you can do is fit a logistic regression model, just a simple model like this, where", "tokens": [50364, 13157, 11, 670, 69, 2414, 13165, 281, 21538, 382, 731, 13, 50572, 50572, 1692, 311, 257, 21538, 1365, 365, 732, 4122, 2031, 16, 293, 2031, 17, 11, 689, 2031, 16, 307, 1310, 264, 22512, 2744, 50886, 50886, 293, 2031, 17, 307, 264, 3205, 295, 4537, 13, 51041, 51041, 400, 321, 434, 1382, 281, 33872, 498, 257, 22512, 307, 2806, 36818, 420, 3271, 788, 11, 382, 1441, 23325, 538, 613, 28467, 51338, 51338, 293, 13040, 13, 51436, 51436, 1485, 551, 291, 393, 360, 307, 3318, 257, 3565, 3142, 24590, 2316, 11, 445, 257, 2199, 2316, 411, 341, 11, 689, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.11324078428979013, "compression_ratio": 1.5899581589958158, "no_speech_prob": 3.393106680960045e-06}, {"id": 114, "seek": 56100, "start": 561.0, "end": 569.76, "text": " as usual, g is the sigmoid function, and this term here inside is z.", "tokens": [50364, 382, 7713, 11, 290, 307, 264, 4556, 3280, 327, 2445, 11, 293, 341, 1433, 510, 1854, 307, 710, 13, 50802, 50802, 407, 498, 291, 360, 300, 11, 291, 917, 493, 365, 257, 2997, 1622, 382, 264, 3537, 12866, 13, 51141, 51141, 639, 307, 264, 1622, 689, 710, 307, 2681, 281, 4018, 300, 34149, 264, 3353, 293, 3671, 5110, 13, 51439, 51439, 639, 2997, 1622, 1177, 380, 574, 6237, 13, 51528, 51528, 467, 1542, 733, 295, 1392, 11, 457, 309, 1177, 380, 574, 411, 257, 588, 665, 3318, 281, 264, 1412, 2139, 13, 51775, 51775], "temperature": 0.0, "avg_logprob": -0.10800764958063762, "compression_ratio": 1.6851851851851851, "no_speech_prob": 2.1233679490251234e-06}, {"id": 115, "seek": 56100, "start": 569.76, "end": 576.54, "text": " So if you do that, you end up with a straight line as the decision boundary.", "tokens": [50364, 382, 7713, 11, 290, 307, 264, 4556, 3280, 327, 2445, 11, 293, 341, 1433, 510, 1854, 307, 710, 13, 50802, 50802, 407, 498, 291, 360, 300, 11, 291, 917, 493, 365, 257, 2997, 1622, 382, 264, 3537, 12866, 13, 51141, 51141, 639, 307, 264, 1622, 689, 710, 307, 2681, 281, 4018, 300, 34149, 264, 3353, 293, 3671, 5110, 13, 51439, 51439, 639, 2997, 1622, 1177, 380, 574, 6237, 13, 51528, 51528, 467, 1542, 733, 295, 1392, 11, 457, 309, 1177, 380, 574, 411, 257, 588, 665, 3318, 281, 264, 1412, 2139, 13, 51775, 51775], "temperature": 0.0, "avg_logprob": -0.10800764958063762, "compression_ratio": 1.6851851851851851, "no_speech_prob": 2.1233679490251234e-06}, {"id": 116, "seek": 56100, "start": 576.54, "end": 582.5, "text": " This is the line where z is equal to zero that separates the positive and negative examples.", "tokens": [50364, 382, 7713, 11, 290, 307, 264, 4556, 3280, 327, 2445, 11, 293, 341, 1433, 510, 1854, 307, 710, 13, 50802, 50802, 407, 498, 291, 360, 300, 11, 291, 917, 493, 365, 257, 2997, 1622, 382, 264, 3537, 12866, 13, 51141, 51141, 639, 307, 264, 1622, 689, 710, 307, 2681, 281, 4018, 300, 34149, 264, 3353, 293, 3671, 5110, 13, 51439, 51439, 639, 2997, 1622, 1177, 380, 574, 6237, 13, 51528, 51528, 467, 1542, 733, 295, 1392, 11, 457, 309, 1177, 380, 574, 411, 257, 588, 665, 3318, 281, 264, 1412, 2139, 13, 51775, 51775], "temperature": 0.0, "avg_logprob": -0.10800764958063762, "compression_ratio": 1.6851851851851851, "no_speech_prob": 2.1233679490251234e-06}, {"id": 117, "seek": 56100, "start": 582.5, "end": 584.28, "text": " This straight line doesn't look terrible.", "tokens": [50364, 382, 7713, 11, 290, 307, 264, 4556, 3280, 327, 2445, 11, 293, 341, 1433, 510, 1854, 307, 710, 13, 50802, 50802, 407, 498, 291, 360, 300, 11, 291, 917, 493, 365, 257, 2997, 1622, 382, 264, 3537, 12866, 13, 51141, 51141, 639, 307, 264, 1622, 689, 710, 307, 2681, 281, 4018, 300, 34149, 264, 3353, 293, 3671, 5110, 13, 51439, 51439, 639, 2997, 1622, 1177, 380, 574, 6237, 13, 51528, 51528, 467, 1542, 733, 295, 1392, 11, 457, 309, 1177, 380, 574, 411, 257, 588, 665, 3318, 281, 264, 1412, 2139, 13, 51775, 51775], "temperature": 0.0, "avg_logprob": -0.10800764958063762, "compression_ratio": 1.6851851851851851, "no_speech_prob": 2.1233679490251234e-06}, {"id": 118, "seek": 56100, "start": 584.28, "end": 589.22, "text": " It looks kind of okay, but it doesn't look like a very good fit to the data either.", "tokens": [50364, 382, 7713, 11, 290, 307, 264, 4556, 3280, 327, 2445, 11, 293, 341, 1433, 510, 1854, 307, 710, 13, 50802, 50802, 407, 498, 291, 360, 300, 11, 291, 917, 493, 365, 257, 2997, 1622, 382, 264, 3537, 12866, 13, 51141, 51141, 639, 307, 264, 1622, 689, 710, 307, 2681, 281, 4018, 300, 34149, 264, 3353, 293, 3671, 5110, 13, 51439, 51439, 639, 2997, 1622, 1177, 380, 574, 6237, 13, 51528, 51528, 467, 1542, 733, 295, 1392, 11, 457, 309, 1177, 380, 574, 411, 257, 588, 665, 3318, 281, 264, 1412, 2139, 13, 51775, 51775], "temperature": 0.0, "avg_logprob": -0.10800764958063762, "compression_ratio": 1.6851851851851851, "no_speech_prob": 2.1233679490251234e-06}, {"id": 119, "seek": 58922, "start": 589.22, "end": 595.0, "text": " So this is an example of underfitting or of high bias.", "tokens": [50364, 407, 341, 307, 364, 1365, 295, 833, 69, 2414, 420, 295, 1090, 12577, 13, 50653, 50653, 961, 311, 574, 412, 1071, 1365, 13, 50759, 50759, 759, 291, 645, 281, 909, 281, 428, 4122, 613, 37262, 2115, 11, 550, 710, 3643, 341, 777, 1433, 51057, 51057, 294, 264, 2808, 293, 264, 3537, 12866, 11, 300, 307, 689, 710, 6915, 4018, 11, 393, 574, 544, 411, 51363, 51363, 341, 11, 544, 411, 364, 8284, 48041, 420, 644, 295, 364, 8284, 48041, 13, 51579, 51579, 400, 341, 307, 257, 1238, 665, 3318, 281, 264, 1412, 11, 754, 1673, 309, 775, 406, 6239, 33872, 51819, 51819], "temperature": 0.0, "avg_logprob": -0.09664824375739464, "compression_ratio": 1.6048387096774193, "no_speech_prob": 4.936897767038317e-06}, {"id": 120, "seek": 58922, "start": 595.0, "end": 597.12, "text": " Let's look at another example.", "tokens": [50364, 407, 341, 307, 364, 1365, 295, 833, 69, 2414, 420, 295, 1090, 12577, 13, 50653, 50653, 961, 311, 574, 412, 1071, 1365, 13, 50759, 50759, 759, 291, 645, 281, 909, 281, 428, 4122, 613, 37262, 2115, 11, 550, 710, 3643, 341, 777, 1433, 51057, 51057, 294, 264, 2808, 293, 264, 3537, 12866, 11, 300, 307, 689, 710, 6915, 4018, 11, 393, 574, 544, 411, 51363, 51363, 341, 11, 544, 411, 364, 8284, 48041, 420, 644, 295, 364, 8284, 48041, 13, 51579, 51579, 400, 341, 307, 257, 1238, 665, 3318, 281, 264, 1412, 11, 754, 1673, 309, 775, 406, 6239, 33872, 51819, 51819], "temperature": 0.0, "avg_logprob": -0.09664824375739464, "compression_ratio": 1.6048387096774193, "no_speech_prob": 4.936897767038317e-06}, {"id": 121, "seek": 58922, "start": 597.12, "end": 603.08, "text": " If you were to add to your features these quadratic terms, then z becomes this new term", "tokens": [50364, 407, 341, 307, 364, 1365, 295, 833, 69, 2414, 420, 295, 1090, 12577, 13, 50653, 50653, 961, 311, 574, 412, 1071, 1365, 13, 50759, 50759, 759, 291, 645, 281, 909, 281, 428, 4122, 613, 37262, 2115, 11, 550, 710, 3643, 341, 777, 1433, 51057, 51057, 294, 264, 2808, 293, 264, 3537, 12866, 11, 300, 307, 689, 710, 6915, 4018, 11, 393, 574, 544, 411, 51363, 51363, 341, 11, 544, 411, 364, 8284, 48041, 420, 644, 295, 364, 8284, 48041, 13, 51579, 51579, 400, 341, 307, 257, 1238, 665, 3318, 281, 264, 1412, 11, 754, 1673, 309, 775, 406, 6239, 33872, 51819, 51819], "temperature": 0.0, "avg_logprob": -0.09664824375739464, "compression_ratio": 1.6048387096774193, "no_speech_prob": 4.936897767038317e-06}, {"id": 122, "seek": 58922, "start": 603.08, "end": 609.2, "text": " in the middle and the decision boundary, that is where z equals zero, can look more like", "tokens": [50364, 407, 341, 307, 364, 1365, 295, 833, 69, 2414, 420, 295, 1090, 12577, 13, 50653, 50653, 961, 311, 574, 412, 1071, 1365, 13, 50759, 50759, 759, 291, 645, 281, 909, 281, 428, 4122, 613, 37262, 2115, 11, 550, 710, 3643, 341, 777, 1433, 51057, 51057, 294, 264, 2808, 293, 264, 3537, 12866, 11, 300, 307, 689, 710, 6915, 4018, 11, 393, 574, 544, 411, 51363, 51363, 341, 11, 544, 411, 364, 8284, 48041, 420, 644, 295, 364, 8284, 48041, 13, 51579, 51579, 400, 341, 307, 257, 1238, 665, 3318, 281, 264, 1412, 11, 754, 1673, 309, 775, 406, 6239, 33872, 51819, 51819], "temperature": 0.0, "avg_logprob": -0.09664824375739464, "compression_ratio": 1.6048387096774193, "no_speech_prob": 4.936897767038317e-06}, {"id": 123, "seek": 58922, "start": 609.2, "end": 613.52, "text": " this, more like an ellipse or part of an ellipse.", "tokens": [50364, 407, 341, 307, 364, 1365, 295, 833, 69, 2414, 420, 295, 1090, 12577, 13, 50653, 50653, 961, 311, 574, 412, 1071, 1365, 13, 50759, 50759, 759, 291, 645, 281, 909, 281, 428, 4122, 613, 37262, 2115, 11, 550, 710, 3643, 341, 777, 1433, 51057, 51057, 294, 264, 2808, 293, 264, 3537, 12866, 11, 300, 307, 689, 710, 6915, 4018, 11, 393, 574, 544, 411, 51363, 51363, 341, 11, 544, 411, 364, 8284, 48041, 420, 644, 295, 364, 8284, 48041, 13, 51579, 51579, 400, 341, 307, 257, 1238, 665, 3318, 281, 264, 1412, 11, 754, 1673, 309, 775, 406, 6239, 33872, 51819, 51819], "temperature": 0.0, "avg_logprob": -0.09664824375739464, "compression_ratio": 1.6048387096774193, "no_speech_prob": 4.936897767038317e-06}, {"id": 124, "seek": 58922, "start": 613.52, "end": 618.32, "text": " And this is a pretty good fit to the data, even though it does not perfectly classify", "tokens": [50364, 407, 341, 307, 364, 1365, 295, 833, 69, 2414, 420, 295, 1090, 12577, 13, 50653, 50653, 961, 311, 574, 412, 1071, 1365, 13, 50759, 50759, 759, 291, 645, 281, 909, 281, 428, 4122, 613, 37262, 2115, 11, 550, 710, 3643, 341, 777, 1433, 51057, 51057, 294, 264, 2808, 293, 264, 3537, 12866, 11, 300, 307, 689, 710, 6915, 4018, 11, 393, 574, 544, 411, 51363, 51363, 341, 11, 544, 411, 364, 8284, 48041, 420, 644, 295, 364, 8284, 48041, 13, 51579, 51579, 400, 341, 307, 257, 1238, 665, 3318, 281, 264, 1412, 11, 754, 1673, 309, 775, 406, 6239, 33872, 51819, 51819], "temperature": 0.0, "avg_logprob": -0.09664824375739464, "compression_ratio": 1.6048387096774193, "no_speech_prob": 4.936897767038317e-06}, {"id": 125, "seek": 61832, "start": 618.32, "end": 621.6, "text": " every single training sample in the training set.", "tokens": [50364, 633, 2167, 3097, 6889, 294, 264, 3097, 992, 13, 50528, 50528, 13428, 577, 512, 295, 613, 28467, 483, 20627, 3654, 264, 13040, 13, 50750, 50750, 583, 341, 2316, 1542, 1238, 665, 13, 50842, 50842, 286, 478, 516, 281, 818, 309, 445, 558, 11, 293, 309, 1542, 411, 341, 486, 2674, 1125, 1238, 731, 281, 777, 51066, 51066, 4209, 13, 51176, 51176, 400, 2721, 11, 412, 264, 661, 8084, 11, 498, 291, 645, 281, 3318, 257, 588, 1090, 1668, 26110, 365, 51484, 51484, 867, 11, 867, 4122, 411, 613, 11, 550, 264, 2316, 815, 853, 534, 1152, 293, 393, 380, 1797, 420, 8203, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.15210912812430905, "compression_ratio": 1.6349809885931559, "no_speech_prob": 1.0782931894937064e-05}, {"id": 126, "seek": 61832, "start": 621.6, "end": 626.0400000000001, "text": " Notice how some of these crosses get classified among the circles.", "tokens": [50364, 633, 2167, 3097, 6889, 294, 264, 3097, 992, 13, 50528, 50528, 13428, 577, 512, 295, 613, 28467, 483, 20627, 3654, 264, 13040, 13, 50750, 50750, 583, 341, 2316, 1542, 1238, 665, 13, 50842, 50842, 286, 478, 516, 281, 818, 309, 445, 558, 11, 293, 309, 1542, 411, 341, 486, 2674, 1125, 1238, 731, 281, 777, 51066, 51066, 4209, 13, 51176, 51176, 400, 2721, 11, 412, 264, 661, 8084, 11, 498, 291, 645, 281, 3318, 257, 588, 1090, 1668, 26110, 365, 51484, 51484, 867, 11, 867, 4122, 411, 613, 11, 550, 264, 2316, 815, 853, 534, 1152, 293, 393, 380, 1797, 420, 8203, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.15210912812430905, "compression_ratio": 1.6349809885931559, "no_speech_prob": 1.0782931894937064e-05}, {"id": 127, "seek": 61832, "start": 626.0400000000001, "end": 627.88, "text": " But this model looks pretty good.", "tokens": [50364, 633, 2167, 3097, 6889, 294, 264, 3097, 992, 13, 50528, 50528, 13428, 577, 512, 295, 613, 28467, 483, 20627, 3654, 264, 13040, 13, 50750, 50750, 583, 341, 2316, 1542, 1238, 665, 13, 50842, 50842, 286, 478, 516, 281, 818, 309, 445, 558, 11, 293, 309, 1542, 411, 341, 486, 2674, 1125, 1238, 731, 281, 777, 51066, 51066, 4209, 13, 51176, 51176, 400, 2721, 11, 412, 264, 661, 8084, 11, 498, 291, 645, 281, 3318, 257, 588, 1090, 1668, 26110, 365, 51484, 51484, 867, 11, 867, 4122, 411, 613, 11, 550, 264, 2316, 815, 853, 534, 1152, 293, 393, 380, 1797, 420, 8203, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.15210912812430905, "compression_ratio": 1.6349809885931559, "no_speech_prob": 1.0782931894937064e-05}, {"id": 128, "seek": 61832, "start": 627.88, "end": 632.36, "text": " I'm going to call it just right, and it looks like this will generalize pretty well to new", "tokens": [50364, 633, 2167, 3097, 6889, 294, 264, 3097, 992, 13, 50528, 50528, 13428, 577, 512, 295, 613, 28467, 483, 20627, 3654, 264, 13040, 13, 50750, 50750, 583, 341, 2316, 1542, 1238, 665, 13, 50842, 50842, 286, 478, 516, 281, 818, 309, 445, 558, 11, 293, 309, 1542, 411, 341, 486, 2674, 1125, 1238, 731, 281, 777, 51066, 51066, 4209, 13, 51176, 51176, 400, 2721, 11, 412, 264, 661, 8084, 11, 498, 291, 645, 281, 3318, 257, 588, 1090, 1668, 26110, 365, 51484, 51484, 867, 11, 867, 4122, 411, 613, 11, 550, 264, 2316, 815, 853, 534, 1152, 293, 393, 380, 1797, 420, 8203, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.15210912812430905, "compression_ratio": 1.6349809885931559, "no_speech_prob": 1.0782931894937064e-05}, {"id": 129, "seek": 61832, "start": 632.36, "end": 634.5600000000001, "text": " patients.", "tokens": [50364, 633, 2167, 3097, 6889, 294, 264, 3097, 992, 13, 50528, 50528, 13428, 577, 512, 295, 613, 28467, 483, 20627, 3654, 264, 13040, 13, 50750, 50750, 583, 341, 2316, 1542, 1238, 665, 13, 50842, 50842, 286, 478, 516, 281, 818, 309, 445, 558, 11, 293, 309, 1542, 411, 341, 486, 2674, 1125, 1238, 731, 281, 777, 51066, 51066, 4209, 13, 51176, 51176, 400, 2721, 11, 412, 264, 661, 8084, 11, 498, 291, 645, 281, 3318, 257, 588, 1090, 1668, 26110, 365, 51484, 51484, 867, 11, 867, 4122, 411, 613, 11, 550, 264, 2316, 815, 853, 534, 1152, 293, 393, 380, 1797, 420, 8203, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.15210912812430905, "compression_ratio": 1.6349809885931559, "no_speech_prob": 1.0782931894937064e-05}, {"id": 130, "seek": 61832, "start": 634.5600000000001, "end": 640.72, "text": " And finally, at the other extreme, if you were to fit a very high order polynomial with", "tokens": [50364, 633, 2167, 3097, 6889, 294, 264, 3097, 992, 13, 50528, 50528, 13428, 577, 512, 295, 613, 28467, 483, 20627, 3654, 264, 13040, 13, 50750, 50750, 583, 341, 2316, 1542, 1238, 665, 13, 50842, 50842, 286, 478, 516, 281, 818, 309, 445, 558, 11, 293, 309, 1542, 411, 341, 486, 2674, 1125, 1238, 731, 281, 777, 51066, 51066, 4209, 13, 51176, 51176, 400, 2721, 11, 412, 264, 661, 8084, 11, 498, 291, 645, 281, 3318, 257, 588, 1090, 1668, 26110, 365, 51484, 51484, 867, 11, 867, 4122, 411, 613, 11, 550, 264, 2316, 815, 853, 534, 1152, 293, 393, 380, 1797, 420, 8203, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.15210912812430905, "compression_ratio": 1.6349809885931559, "no_speech_prob": 1.0782931894937064e-05}, {"id": 131, "seek": 61832, "start": 640.72, "end": 647.8800000000001, "text": " many, many features like these, then the model may try really hard and can't hold or twist", "tokens": [50364, 633, 2167, 3097, 6889, 294, 264, 3097, 992, 13, 50528, 50528, 13428, 577, 512, 295, 613, 28467, 483, 20627, 3654, 264, 13040, 13, 50750, 50750, 583, 341, 2316, 1542, 1238, 665, 13, 50842, 50842, 286, 478, 516, 281, 818, 309, 445, 558, 11, 293, 309, 1542, 411, 341, 486, 2674, 1125, 1238, 731, 281, 777, 51066, 51066, 4209, 13, 51176, 51176, 400, 2721, 11, 412, 264, 661, 8084, 11, 498, 291, 645, 281, 3318, 257, 588, 1090, 1668, 26110, 365, 51484, 51484, 867, 11, 867, 4122, 411, 613, 11, 550, 264, 2316, 815, 853, 534, 1152, 293, 393, 380, 1797, 420, 8203, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.15210912812430905, "compression_ratio": 1.6349809885931559, "no_speech_prob": 1.0782931894937064e-05}, {"id": 132, "seek": 64788, "start": 647.88, "end": 654.04, "text": " itself to find a decision boundary that fits your training data perfectly.", "tokens": [50364, 2564, 281, 915, 257, 3537, 12866, 300, 9001, 428, 3097, 1412, 6239, 13, 50672, 50672, 10222, 439, 613, 2946, 1668, 26110, 4122, 4045, 264, 9284, 281, 2826, 341, 534, 50972, 50972, 24324, 3997, 3537, 12866, 13, 51172, 51172, 759, 264, 4122, 366, 22512, 2744, 293, 3205, 11, 293, 291, 434, 1382, 281, 33872, 38466, 382, 2806, 36818, 51444, 51444, 420, 3271, 788, 11, 550, 341, 1177, 380, 534, 574, 411, 257, 588, 665, 2316, 337, 1455, 21264, 13, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.07933203185476907, "compression_ratio": 1.6406926406926408, "no_speech_prob": 5.421992227638839e-06}, {"id": 133, "seek": 64788, "start": 654.04, "end": 660.04, "text": " Having all these higher order polynomial features allows the algorithm to choose this really", "tokens": [50364, 2564, 281, 915, 257, 3537, 12866, 300, 9001, 428, 3097, 1412, 6239, 13, 50672, 50672, 10222, 439, 613, 2946, 1668, 26110, 4122, 4045, 264, 9284, 281, 2826, 341, 534, 50972, 50972, 24324, 3997, 3537, 12866, 13, 51172, 51172, 759, 264, 4122, 366, 22512, 2744, 293, 3205, 11, 293, 291, 434, 1382, 281, 33872, 38466, 382, 2806, 36818, 51444, 51444, 420, 3271, 788, 11, 550, 341, 1177, 380, 534, 574, 411, 257, 588, 665, 2316, 337, 1455, 21264, 13, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.07933203185476907, "compression_ratio": 1.6406926406926408, "no_speech_prob": 5.421992227638839e-06}, {"id": 134, "seek": 64788, "start": 660.04, "end": 664.04, "text": " overly complex decision boundary.", "tokens": [50364, 2564, 281, 915, 257, 3537, 12866, 300, 9001, 428, 3097, 1412, 6239, 13, 50672, 50672, 10222, 439, 613, 2946, 1668, 26110, 4122, 4045, 264, 9284, 281, 2826, 341, 534, 50972, 50972, 24324, 3997, 3537, 12866, 13, 51172, 51172, 759, 264, 4122, 366, 22512, 2744, 293, 3205, 11, 293, 291, 434, 1382, 281, 33872, 38466, 382, 2806, 36818, 51444, 51444, 420, 3271, 788, 11, 550, 341, 1177, 380, 534, 574, 411, 257, 588, 665, 2316, 337, 1455, 21264, 13, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.07933203185476907, "compression_ratio": 1.6406926406926408, "no_speech_prob": 5.421992227638839e-06}, {"id": 135, "seek": 64788, "start": 664.04, "end": 669.48, "text": " If the features are tumor size and age, and you're trying to classify tumors as malignant", "tokens": [50364, 2564, 281, 915, 257, 3537, 12866, 300, 9001, 428, 3097, 1412, 6239, 13, 50672, 50672, 10222, 439, 613, 2946, 1668, 26110, 4122, 4045, 264, 9284, 281, 2826, 341, 534, 50972, 50972, 24324, 3997, 3537, 12866, 13, 51172, 51172, 759, 264, 4122, 366, 22512, 2744, 293, 3205, 11, 293, 291, 434, 1382, 281, 33872, 38466, 382, 2806, 36818, 51444, 51444, 420, 3271, 788, 11, 550, 341, 1177, 380, 534, 574, 411, 257, 588, 665, 2316, 337, 1455, 21264, 13, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.07933203185476907, "compression_ratio": 1.6406926406926408, "no_speech_prob": 5.421992227638839e-06}, {"id": 136, "seek": 64788, "start": 669.48, "end": 675.64, "text": " or benign, then this doesn't really look like a very good model for making predictions.", "tokens": [50364, 2564, 281, 915, 257, 3537, 12866, 300, 9001, 428, 3097, 1412, 6239, 13, 50672, 50672, 10222, 439, 613, 2946, 1668, 26110, 4122, 4045, 264, 9284, 281, 2826, 341, 534, 50972, 50972, 24324, 3997, 3537, 12866, 13, 51172, 51172, 759, 264, 4122, 366, 22512, 2744, 293, 3205, 11, 293, 291, 434, 1382, 281, 33872, 38466, 382, 2806, 36818, 51444, 51444, 420, 3271, 788, 11, 550, 341, 1177, 380, 534, 574, 411, 257, 588, 665, 2316, 337, 1455, 21264, 13, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.07933203185476907, "compression_ratio": 1.6406926406926408, "no_speech_prob": 5.421992227638839e-06}, {"id": 137, "seek": 67564, "start": 675.64, "end": 681.6, "text": " So once again, this is an instance of overfitting and high variance, because this model, despite", "tokens": [50364, 407, 1564, 797, 11, 341, 307, 364, 5197, 295, 670, 69, 2414, 293, 1090, 21977, 11, 570, 341, 2316, 11, 7228, 50662, 50662, 884, 588, 731, 294, 264, 3097, 992, 11, 1177, 380, 574, 411, 309, 486, 2674, 1125, 731, 281, 777, 5110, 13, 50981, 50981, 407, 586, 291, 600, 1612, 577, 364, 9284, 393, 833, 6845, 420, 362, 1090, 12577, 420, 670, 6845, 293, 362, 1090, 51258, 51258, 21977, 13, 51346, 51346, 509, 815, 528, 281, 458, 577, 291, 393, 483, 257, 2316, 300, 307, 445, 558, 13, 51552, 51552, 682, 264, 958, 960, 11, 321, 603, 574, 412, 512, 2098, 291, 393, 2985, 264, 2734, 295, 670, 69, 2414, 13, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.08949858566810345, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.682270511897514e-06}, {"id": 138, "seek": 67564, "start": 681.6, "end": 687.98, "text": " doing very well in the training set, doesn't look like it will generalize well to new examples.", "tokens": [50364, 407, 1564, 797, 11, 341, 307, 364, 5197, 295, 670, 69, 2414, 293, 1090, 21977, 11, 570, 341, 2316, 11, 7228, 50662, 50662, 884, 588, 731, 294, 264, 3097, 992, 11, 1177, 380, 574, 411, 309, 486, 2674, 1125, 731, 281, 777, 5110, 13, 50981, 50981, 407, 586, 291, 600, 1612, 577, 364, 9284, 393, 833, 6845, 420, 362, 1090, 12577, 420, 670, 6845, 293, 362, 1090, 51258, 51258, 21977, 13, 51346, 51346, 509, 815, 528, 281, 458, 577, 291, 393, 483, 257, 2316, 300, 307, 445, 558, 13, 51552, 51552, 682, 264, 958, 960, 11, 321, 603, 574, 412, 512, 2098, 291, 393, 2985, 264, 2734, 295, 670, 69, 2414, 13, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.08949858566810345, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.682270511897514e-06}, {"id": 139, "seek": 67564, "start": 687.98, "end": 693.52, "text": " So now you've seen how an algorithm can underfit or have high bias or overfit and have high", "tokens": [50364, 407, 1564, 797, 11, 341, 307, 364, 5197, 295, 670, 69, 2414, 293, 1090, 21977, 11, 570, 341, 2316, 11, 7228, 50662, 50662, 884, 588, 731, 294, 264, 3097, 992, 11, 1177, 380, 574, 411, 309, 486, 2674, 1125, 731, 281, 777, 5110, 13, 50981, 50981, 407, 586, 291, 600, 1612, 577, 364, 9284, 393, 833, 6845, 420, 362, 1090, 12577, 420, 670, 6845, 293, 362, 1090, 51258, 51258, 21977, 13, 51346, 51346, 509, 815, 528, 281, 458, 577, 291, 393, 483, 257, 2316, 300, 307, 445, 558, 13, 51552, 51552, 682, 264, 958, 960, 11, 321, 603, 574, 412, 512, 2098, 291, 393, 2985, 264, 2734, 295, 670, 69, 2414, 13, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.08949858566810345, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.682270511897514e-06}, {"id": 140, "seek": 67564, "start": 693.52, "end": 695.28, "text": " variance.", "tokens": [50364, 407, 1564, 797, 11, 341, 307, 364, 5197, 295, 670, 69, 2414, 293, 1090, 21977, 11, 570, 341, 2316, 11, 7228, 50662, 50662, 884, 588, 731, 294, 264, 3097, 992, 11, 1177, 380, 574, 411, 309, 486, 2674, 1125, 731, 281, 777, 5110, 13, 50981, 50981, 407, 586, 291, 600, 1612, 577, 364, 9284, 393, 833, 6845, 420, 362, 1090, 12577, 420, 670, 6845, 293, 362, 1090, 51258, 51258, 21977, 13, 51346, 51346, 509, 815, 528, 281, 458, 577, 291, 393, 483, 257, 2316, 300, 307, 445, 558, 13, 51552, 51552, 682, 264, 958, 960, 11, 321, 603, 574, 412, 512, 2098, 291, 393, 2985, 264, 2734, 295, 670, 69, 2414, 13, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.08949858566810345, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.682270511897514e-06}, {"id": 141, "seek": 67564, "start": 695.28, "end": 699.4, "text": " You may want to know how you can get a model that is just right.", "tokens": [50364, 407, 1564, 797, 11, 341, 307, 364, 5197, 295, 670, 69, 2414, 293, 1090, 21977, 11, 570, 341, 2316, 11, 7228, 50662, 50662, 884, 588, 731, 294, 264, 3097, 992, 11, 1177, 380, 574, 411, 309, 486, 2674, 1125, 731, 281, 777, 5110, 13, 50981, 50981, 407, 586, 291, 600, 1612, 577, 364, 9284, 393, 833, 6845, 420, 362, 1090, 12577, 420, 670, 6845, 293, 362, 1090, 51258, 51258, 21977, 13, 51346, 51346, 509, 815, 528, 281, 458, 577, 291, 393, 483, 257, 2316, 300, 307, 445, 558, 13, 51552, 51552, 682, 264, 958, 960, 11, 321, 603, 574, 412, 512, 2098, 291, 393, 2985, 264, 2734, 295, 670, 69, 2414, 13, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.08949858566810345, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.682270511897514e-06}, {"id": 142, "seek": 67564, "start": 699.4, "end": 704.84, "text": " In the next video, we'll look at some ways you can address the issue of overfitting.", "tokens": [50364, 407, 1564, 797, 11, 341, 307, 364, 5197, 295, 670, 69, 2414, 293, 1090, 21977, 11, 570, 341, 2316, 11, 7228, 50662, 50662, 884, 588, 731, 294, 264, 3097, 992, 11, 1177, 380, 574, 411, 309, 486, 2674, 1125, 731, 281, 777, 5110, 13, 50981, 50981, 407, 586, 291, 600, 1612, 577, 364, 9284, 393, 833, 6845, 420, 362, 1090, 12577, 420, 670, 6845, 293, 362, 1090, 51258, 51258, 21977, 13, 51346, 51346, 509, 815, 528, 281, 458, 577, 291, 393, 483, 257, 2316, 300, 307, 445, 558, 13, 51552, 51552, 682, 264, 958, 960, 11, 321, 603, 574, 412, 512, 2098, 291, 393, 2985, 264, 2734, 295, 670, 69, 2414, 13, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.08949858566810345, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.682270511897514e-06}, {"id": 143, "seek": 70484, "start": 704.84, "end": 709.32, "text": " We'll also touch on some ideas relevant for accessing underfitting.", "tokens": [50364, 492, 603, 611, 2557, 322, 512, 3487, 7340, 337, 26440, 833, 69, 2414, 13, 50588, 50588, 961, 311, 352, 322, 281, 264, 958, 960, 13, 50660], "temperature": 0.0, "avg_logprob": -0.20315798691340856, "compression_ratio": 1.1136363636363635, "no_speech_prob": 4.020700362161733e-05}, {"id": 144, "seek": 70932, "start": 709.32, "end": 736.32, "text": " Let's go on to the next video.", "tokens": [50364, 961, 311, 352, 322, 281, 264, 958, 960, 13, 51714], "temperature": 0.0, "avg_logprob": -0.3802918990453084, "compression_ratio": 0.7894736842105263, "no_speech_prob": 0.0009782282868400216}], "language": "en", "video_id": "wBx3NZ0ucgc", "entity": "ML Specialization, Andrew Ng (2022)"}}