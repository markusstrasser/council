{"video_id": "rGvoO8U2Ozc", "title": "1.17 Machine Learning Overview | Gradient descent intuition  --[Machine Learning | Andrew Ng]", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 422, "views": 372, "publish_date": "11/04/2022", "timestamp": 1660953600, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " Now, let's dive more deeply into gradient descent to gain better intuition about what it's doing and why it might make sense. Here's the gradient descent algorithm that you saw in the previous video. And as a reminder, this variable, this Greek symbol alpha is the learning rate. And the learning rate controls how big of a step you take when updating the model's parameters w and b. So this term here, this d over dw, this is a derivative term. And by convention in math, this d is written with this funny font here. And in case anyone watching this has a PhD in math or is an expert in multivariate calculus, they may be wondering, that's not the derivative, that's the partial derivative. And yes, they'd be right. But for the purposes of implementing a machine learning algorithm, I'm just going to call it derivative, and don't worry about these little distinctions. So what we're going to focus on now is get more intuition about what this learning rate and what this derivative are doing, and why, when multiplied together like this, it results in updates to parameters w and b that make sense. In order to do this, let's use a slightly simpler example, where we work on minimizing just one parameter. So let's say that you have a cost function j of just one parameter w, where w is a number. This means that gradient descent now looks like this. w is updated to w minus the learning rate alpha times d over dw of j of w. And you're trying to minimize the cost by adjusting the parameter w. So this is like our previous example, where we had temporarily set b equal to zero. With one parameter w instead of two, you can look at two dimensional graphs of the cost function j instead of three dimensional graphs. Let's look at what gradient descent does on this function j of w. Here on the horizontal axis is parameter w, and on the vertical axis is the cost j of w. Now, let's initialize gradient descent with some starting value for w. Let's initialize it at this location. So imagine that you start off at this point right here on the function j. What gradient descent will do is it will update w to be w minus learning rate alpha times d over dw of j of w. Let's look at what this derivative term here means. A way to think about the derivative at this point on the line is to draw a tangent line, which is a straight line that touches this curve at that point. In math, the slope of this line is the derivative of the function j at this point. And to get the slope, you can draw a little triangle like this. And if you compute the height divided by the width of this triangle, that is the slope. So for example, the slope might be, you know, 2 over 1, for instance. And when the tangent line is pointing up into the right, the slope is positive, which means that this derivative is a positive number, so it's greater than zero. And so the updated w is going to be w minus the learning rate times some positive number. The learning rate is always a positive number. So if you take w minus a positive number, you end up with a new value for w that is smaller. So on the graph, we are moving to the left, we are decreasing the value of w. And you may notice that this is the right thing to do if your goal is to decrease the cost j, because when we move toward the left on this curve, the cost j decreases and you're getting closer to the minimum for j, which is over here. So so far, gradient descent seems to be doing the right thing. Now, let's look at another example. Let's take the same function j of w as above. And now let's say that you initialize gradient descent at a different location, say by choosing a starting value for w that's over here on the left. So that's this point of the function j. Now the derivative term, remember is d over dw of j of w. And when we look at the tangent line at this point over here, the slope of this line is the derivative of j at this point. But this tangent line is sloping down into the right. And so this line sloping down into the right has a negative slope. In other words, the derivative j at this point is a negative number. For instance, if you draw a triangle, then the height like this is negative two and the width is one. So the slope is negative two divided by one, which is negative two, which is a negative number. So when you update w, you get w minus the learning rate times a negative number. And so this means you subtract from w a negative number. But subtracting a negative number means adding a positive number. And so you end up increasing w, because subtracting a negative number is the same as adding a positive number to w. So this step of gradient descent causes w to increase, which means you're moving to the right of the graph, and your cos j has decreased down to here. And again, it looks like gradient descent is doing something reasonable, it's getting you closer to the minimum. So hopefully, these last two examples show some of the intuition behind what the derivative term is doing, and why this helps gradient descent change w to get you closer to the minimum. I hope this video gave you some sense for why the derivative term in gradient descent makes sense. One other key quantity in the gradient descent algorithm is the learning rate alpha. How do you choose alpha? What happens if it's too small? What happens if it's too big? In the next video, let's take a deeper look at the parameter alpha to help build intuitions about what it does, as well as how to make a good choice for a good value of alpha for your implementation of gradient descent.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.88, "text": " Now, let's dive more deeply into gradient descent to gain better intuition about what", "tokens": [50364, 823, 11, 718, 311, 9192, 544, 8760, 666, 16235, 23475, 281, 6052, 1101, 24002, 466, 437, 50758, 50758, 309, 311, 884, 293, 983, 309, 1062, 652, 2020, 13, 50906, 50906, 1692, 311, 264, 16235, 23475, 9284, 300, 291, 1866, 294, 264, 3894, 960, 13, 51140, 51140, 400, 382, 257, 13548, 11, 341, 7006, 11, 341, 10281, 5986, 8961, 307, 264, 2539, 3314, 13, 51442, 51442, 400, 264, 2539, 3314, 9003, 577, 955, 295, 257, 1823, 291, 747, 562, 25113, 264, 2316, 311, 51656, 51656, 9834, 261, 293, 272, 13, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.15021960966048703, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.00884319469332695}, {"id": 1, "seek": 0, "start": 7.88, "end": 10.84, "text": " it's doing and why it might make sense.", "tokens": [50364, 823, 11, 718, 311, 9192, 544, 8760, 666, 16235, 23475, 281, 6052, 1101, 24002, 466, 437, 50758, 50758, 309, 311, 884, 293, 983, 309, 1062, 652, 2020, 13, 50906, 50906, 1692, 311, 264, 16235, 23475, 9284, 300, 291, 1866, 294, 264, 3894, 960, 13, 51140, 51140, 400, 382, 257, 13548, 11, 341, 7006, 11, 341, 10281, 5986, 8961, 307, 264, 2539, 3314, 13, 51442, 51442, 400, 264, 2539, 3314, 9003, 577, 955, 295, 257, 1823, 291, 747, 562, 25113, 264, 2316, 311, 51656, 51656, 9834, 261, 293, 272, 13, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.15021960966048703, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.00884319469332695}, {"id": 2, "seek": 0, "start": 10.84, "end": 15.52, "text": " Here's the gradient descent algorithm that you saw in the previous video.", "tokens": [50364, 823, 11, 718, 311, 9192, 544, 8760, 666, 16235, 23475, 281, 6052, 1101, 24002, 466, 437, 50758, 50758, 309, 311, 884, 293, 983, 309, 1062, 652, 2020, 13, 50906, 50906, 1692, 311, 264, 16235, 23475, 9284, 300, 291, 1866, 294, 264, 3894, 960, 13, 51140, 51140, 400, 382, 257, 13548, 11, 341, 7006, 11, 341, 10281, 5986, 8961, 307, 264, 2539, 3314, 13, 51442, 51442, 400, 264, 2539, 3314, 9003, 577, 955, 295, 257, 1823, 291, 747, 562, 25113, 264, 2316, 311, 51656, 51656, 9834, 261, 293, 272, 13, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.15021960966048703, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.00884319469332695}, {"id": 3, "seek": 0, "start": 15.52, "end": 21.56, "text": " And as a reminder, this variable, this Greek symbol alpha is the learning rate.", "tokens": [50364, 823, 11, 718, 311, 9192, 544, 8760, 666, 16235, 23475, 281, 6052, 1101, 24002, 466, 437, 50758, 50758, 309, 311, 884, 293, 983, 309, 1062, 652, 2020, 13, 50906, 50906, 1692, 311, 264, 16235, 23475, 9284, 300, 291, 1866, 294, 264, 3894, 960, 13, 51140, 51140, 400, 382, 257, 13548, 11, 341, 7006, 11, 341, 10281, 5986, 8961, 307, 264, 2539, 3314, 13, 51442, 51442, 400, 264, 2539, 3314, 9003, 577, 955, 295, 257, 1823, 291, 747, 562, 25113, 264, 2316, 311, 51656, 51656, 9834, 261, 293, 272, 13, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.15021960966048703, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.00884319469332695}, {"id": 4, "seek": 0, "start": 21.56, "end": 25.84, "text": " And the learning rate controls how big of a step you take when updating the model's", "tokens": [50364, 823, 11, 718, 311, 9192, 544, 8760, 666, 16235, 23475, 281, 6052, 1101, 24002, 466, 437, 50758, 50758, 309, 311, 884, 293, 983, 309, 1062, 652, 2020, 13, 50906, 50906, 1692, 311, 264, 16235, 23475, 9284, 300, 291, 1866, 294, 264, 3894, 960, 13, 51140, 51140, 400, 382, 257, 13548, 11, 341, 7006, 11, 341, 10281, 5986, 8961, 307, 264, 2539, 3314, 13, 51442, 51442, 400, 264, 2539, 3314, 9003, 577, 955, 295, 257, 1823, 291, 747, 562, 25113, 264, 2316, 311, 51656, 51656, 9834, 261, 293, 272, 13, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.15021960966048703, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.00884319469332695}, {"id": 5, "seek": 0, "start": 25.84, "end": 28.8, "text": " parameters w and b.", "tokens": [50364, 823, 11, 718, 311, 9192, 544, 8760, 666, 16235, 23475, 281, 6052, 1101, 24002, 466, 437, 50758, 50758, 309, 311, 884, 293, 983, 309, 1062, 652, 2020, 13, 50906, 50906, 1692, 311, 264, 16235, 23475, 9284, 300, 291, 1866, 294, 264, 3894, 960, 13, 51140, 51140, 400, 382, 257, 13548, 11, 341, 7006, 11, 341, 10281, 5986, 8961, 307, 264, 2539, 3314, 13, 51442, 51442, 400, 264, 2539, 3314, 9003, 577, 955, 295, 257, 1823, 291, 747, 562, 25113, 264, 2316, 311, 51656, 51656, 9834, 261, 293, 272, 13, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.15021960966048703, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.00884319469332695}, {"id": 6, "seek": 2880, "start": 28.8, "end": 35.0, "text": " So this term here, this d over dw, this is a derivative term.", "tokens": [50364, 407, 341, 1433, 510, 11, 341, 274, 670, 27379, 11, 341, 307, 257, 13760, 1433, 13, 50674, 50674, 400, 538, 10286, 294, 5221, 11, 341, 274, 307, 3720, 365, 341, 4074, 10703, 510, 13, 50960, 50960, 400, 294, 1389, 2878, 1976, 341, 575, 257, 14476, 294, 5221, 420, 307, 364, 5844, 294, 2120, 592, 3504, 473, 33400, 11, 51200, 51200, 436, 815, 312, 6359, 11, 300, 311, 406, 264, 13760, 11, 300, 311, 264, 14641, 13760, 13, 51394, 51394, 400, 2086, 11, 436, 1116, 312, 558, 13, 51468, 51468, 583, 337, 264, 9932, 295, 18114, 257, 3479, 2539, 9284, 11, 286, 478, 445, 516, 281, 818, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.1463791066950018, "compression_ratio": 1.7125506072874495, "no_speech_prob": 1.1124877346446738e-05}, {"id": 7, "seek": 2880, "start": 35.0, "end": 40.72, "text": " And by convention in math, this d is written with this funny font here.", "tokens": [50364, 407, 341, 1433, 510, 11, 341, 274, 670, 27379, 11, 341, 307, 257, 13760, 1433, 13, 50674, 50674, 400, 538, 10286, 294, 5221, 11, 341, 274, 307, 3720, 365, 341, 4074, 10703, 510, 13, 50960, 50960, 400, 294, 1389, 2878, 1976, 341, 575, 257, 14476, 294, 5221, 420, 307, 364, 5844, 294, 2120, 592, 3504, 473, 33400, 11, 51200, 51200, 436, 815, 312, 6359, 11, 300, 311, 406, 264, 13760, 11, 300, 311, 264, 14641, 13760, 13, 51394, 51394, 400, 2086, 11, 436, 1116, 312, 558, 13, 51468, 51468, 583, 337, 264, 9932, 295, 18114, 257, 3479, 2539, 9284, 11, 286, 478, 445, 516, 281, 818, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.1463791066950018, "compression_ratio": 1.7125506072874495, "no_speech_prob": 1.1124877346446738e-05}, {"id": 8, "seek": 2880, "start": 40.72, "end": 45.519999999999996, "text": " And in case anyone watching this has a PhD in math or is an expert in multivariate calculus,", "tokens": [50364, 407, 341, 1433, 510, 11, 341, 274, 670, 27379, 11, 341, 307, 257, 13760, 1433, 13, 50674, 50674, 400, 538, 10286, 294, 5221, 11, 341, 274, 307, 3720, 365, 341, 4074, 10703, 510, 13, 50960, 50960, 400, 294, 1389, 2878, 1976, 341, 575, 257, 14476, 294, 5221, 420, 307, 364, 5844, 294, 2120, 592, 3504, 473, 33400, 11, 51200, 51200, 436, 815, 312, 6359, 11, 300, 311, 406, 264, 13760, 11, 300, 311, 264, 14641, 13760, 13, 51394, 51394, 400, 2086, 11, 436, 1116, 312, 558, 13, 51468, 51468, 583, 337, 264, 9932, 295, 18114, 257, 3479, 2539, 9284, 11, 286, 478, 445, 516, 281, 818, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.1463791066950018, "compression_ratio": 1.7125506072874495, "no_speech_prob": 1.1124877346446738e-05}, {"id": 9, "seek": 2880, "start": 45.519999999999996, "end": 49.400000000000006, "text": " they may be wondering, that's not the derivative, that's the partial derivative.", "tokens": [50364, 407, 341, 1433, 510, 11, 341, 274, 670, 27379, 11, 341, 307, 257, 13760, 1433, 13, 50674, 50674, 400, 538, 10286, 294, 5221, 11, 341, 274, 307, 3720, 365, 341, 4074, 10703, 510, 13, 50960, 50960, 400, 294, 1389, 2878, 1976, 341, 575, 257, 14476, 294, 5221, 420, 307, 364, 5844, 294, 2120, 592, 3504, 473, 33400, 11, 51200, 51200, 436, 815, 312, 6359, 11, 300, 311, 406, 264, 13760, 11, 300, 311, 264, 14641, 13760, 13, 51394, 51394, 400, 2086, 11, 436, 1116, 312, 558, 13, 51468, 51468, 583, 337, 264, 9932, 295, 18114, 257, 3479, 2539, 9284, 11, 286, 478, 445, 516, 281, 818, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.1463791066950018, "compression_ratio": 1.7125506072874495, "no_speech_prob": 1.1124877346446738e-05}, {"id": 10, "seek": 2880, "start": 49.400000000000006, "end": 50.88, "text": " And yes, they'd be right.", "tokens": [50364, 407, 341, 1433, 510, 11, 341, 274, 670, 27379, 11, 341, 307, 257, 13760, 1433, 13, 50674, 50674, 400, 538, 10286, 294, 5221, 11, 341, 274, 307, 3720, 365, 341, 4074, 10703, 510, 13, 50960, 50960, 400, 294, 1389, 2878, 1976, 341, 575, 257, 14476, 294, 5221, 420, 307, 364, 5844, 294, 2120, 592, 3504, 473, 33400, 11, 51200, 51200, 436, 815, 312, 6359, 11, 300, 311, 406, 264, 13760, 11, 300, 311, 264, 14641, 13760, 13, 51394, 51394, 400, 2086, 11, 436, 1116, 312, 558, 13, 51468, 51468, 583, 337, 264, 9932, 295, 18114, 257, 3479, 2539, 9284, 11, 286, 478, 445, 516, 281, 818, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.1463791066950018, "compression_ratio": 1.7125506072874495, "no_speech_prob": 1.1124877346446738e-05}, {"id": 11, "seek": 2880, "start": 50.88, "end": 55.040000000000006, "text": " But for the purposes of implementing a machine learning algorithm, I'm just going to call", "tokens": [50364, 407, 341, 1433, 510, 11, 341, 274, 670, 27379, 11, 341, 307, 257, 13760, 1433, 13, 50674, 50674, 400, 538, 10286, 294, 5221, 11, 341, 274, 307, 3720, 365, 341, 4074, 10703, 510, 13, 50960, 50960, 400, 294, 1389, 2878, 1976, 341, 575, 257, 14476, 294, 5221, 420, 307, 364, 5844, 294, 2120, 592, 3504, 473, 33400, 11, 51200, 51200, 436, 815, 312, 6359, 11, 300, 311, 406, 264, 13760, 11, 300, 311, 264, 14641, 13760, 13, 51394, 51394, 400, 2086, 11, 436, 1116, 312, 558, 13, 51468, 51468, 583, 337, 264, 9932, 295, 18114, 257, 3479, 2539, 9284, 11, 286, 478, 445, 516, 281, 818, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.1463791066950018, "compression_ratio": 1.7125506072874495, "no_speech_prob": 1.1124877346446738e-05}, {"id": 12, "seek": 5504, "start": 55.04, "end": 60.2, "text": " it derivative, and don't worry about these little distinctions.", "tokens": [50364, 309, 13760, 11, 293, 500, 380, 3292, 466, 613, 707, 1483, 49798, 13, 50622, 50622, 407, 437, 321, 434, 516, 281, 1879, 322, 586, 307, 483, 544, 24002, 466, 437, 341, 2539, 3314, 50920, 50920, 293, 437, 341, 13760, 366, 884, 11, 293, 983, 11, 562, 17207, 1214, 411, 341, 11, 309, 3542, 51246, 51246, 294, 9205, 281, 9834, 261, 293, 272, 300, 652, 2020, 13, 51498, 51498, 682, 1668, 281, 360, 341, 11, 718, 311, 764, 257, 4748, 18587, 1365, 11, 689, 321, 589, 322, 46608, 51801, 51801], "temperature": 0.0, "avg_logprob": -0.09813092829106929, "compression_ratio": 1.6379310344827587, "no_speech_prob": 1.1842795174743515e-05}, {"id": 13, "seek": 5504, "start": 60.2, "end": 66.16, "text": " So what we're going to focus on now is get more intuition about what this learning rate", "tokens": [50364, 309, 13760, 11, 293, 500, 380, 3292, 466, 613, 707, 1483, 49798, 13, 50622, 50622, 407, 437, 321, 434, 516, 281, 1879, 322, 586, 307, 483, 544, 24002, 466, 437, 341, 2539, 3314, 50920, 50920, 293, 437, 341, 13760, 366, 884, 11, 293, 983, 11, 562, 17207, 1214, 411, 341, 11, 309, 3542, 51246, 51246, 294, 9205, 281, 9834, 261, 293, 272, 300, 652, 2020, 13, 51498, 51498, 682, 1668, 281, 360, 341, 11, 718, 311, 764, 257, 4748, 18587, 1365, 11, 689, 321, 589, 322, 46608, 51801, 51801], "temperature": 0.0, "avg_logprob": -0.09813092829106929, "compression_ratio": 1.6379310344827587, "no_speech_prob": 1.1842795174743515e-05}, {"id": 14, "seek": 5504, "start": 66.16, "end": 72.68, "text": " and what this derivative are doing, and why, when multiplied together like this, it results", "tokens": [50364, 309, 13760, 11, 293, 500, 380, 3292, 466, 613, 707, 1483, 49798, 13, 50622, 50622, 407, 437, 321, 434, 516, 281, 1879, 322, 586, 307, 483, 544, 24002, 466, 437, 341, 2539, 3314, 50920, 50920, 293, 437, 341, 13760, 366, 884, 11, 293, 983, 11, 562, 17207, 1214, 411, 341, 11, 309, 3542, 51246, 51246, 294, 9205, 281, 9834, 261, 293, 272, 300, 652, 2020, 13, 51498, 51498, 682, 1668, 281, 360, 341, 11, 718, 311, 764, 257, 4748, 18587, 1365, 11, 689, 321, 589, 322, 46608, 51801, 51801], "temperature": 0.0, "avg_logprob": -0.09813092829106929, "compression_ratio": 1.6379310344827587, "no_speech_prob": 1.1842795174743515e-05}, {"id": 15, "seek": 5504, "start": 72.68, "end": 77.72, "text": " in updates to parameters w and b that make sense.", "tokens": [50364, 309, 13760, 11, 293, 500, 380, 3292, 466, 613, 707, 1483, 49798, 13, 50622, 50622, 407, 437, 321, 434, 516, 281, 1879, 322, 586, 307, 483, 544, 24002, 466, 437, 341, 2539, 3314, 50920, 50920, 293, 437, 341, 13760, 366, 884, 11, 293, 983, 11, 562, 17207, 1214, 411, 341, 11, 309, 3542, 51246, 51246, 294, 9205, 281, 9834, 261, 293, 272, 300, 652, 2020, 13, 51498, 51498, 682, 1668, 281, 360, 341, 11, 718, 311, 764, 257, 4748, 18587, 1365, 11, 689, 321, 589, 322, 46608, 51801, 51801], "temperature": 0.0, "avg_logprob": -0.09813092829106929, "compression_ratio": 1.6379310344827587, "no_speech_prob": 1.1842795174743515e-05}, {"id": 16, "seek": 5504, "start": 77.72, "end": 83.78, "text": " In order to do this, let's use a slightly simpler example, where we work on minimizing", "tokens": [50364, 309, 13760, 11, 293, 500, 380, 3292, 466, 613, 707, 1483, 49798, 13, 50622, 50622, 407, 437, 321, 434, 516, 281, 1879, 322, 586, 307, 483, 544, 24002, 466, 437, 341, 2539, 3314, 50920, 50920, 293, 437, 341, 13760, 366, 884, 11, 293, 983, 11, 562, 17207, 1214, 411, 341, 11, 309, 3542, 51246, 51246, 294, 9205, 281, 9834, 261, 293, 272, 300, 652, 2020, 13, 51498, 51498, 682, 1668, 281, 360, 341, 11, 718, 311, 764, 257, 4748, 18587, 1365, 11, 689, 321, 589, 322, 46608, 51801, 51801], "temperature": 0.0, "avg_logprob": -0.09813092829106929, "compression_ratio": 1.6379310344827587, "no_speech_prob": 1.1842795174743515e-05}, {"id": 17, "seek": 8378, "start": 83.78, "end": 86.56, "text": " just one parameter.", "tokens": [50364, 445, 472, 13075, 13, 50503, 50503, 407, 718, 311, 584, 300, 291, 362, 257, 2063, 2445, 361, 295, 445, 472, 13075, 261, 11, 689, 261, 307, 257, 1230, 13, 50885, 50885, 639, 1355, 300, 16235, 23475, 586, 1542, 411, 341, 13, 51069, 51069, 261, 307, 10588, 281, 261, 3175, 264, 2539, 3314, 8961, 1413, 274, 670, 27379, 295, 361, 295, 261, 13, 51533, 51533, 400, 291, 434, 1382, 281, 17522, 264, 2063, 538, 23559, 264, 13075, 261, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10308129612992449, "compression_ratio": 1.5794871794871794, "no_speech_prob": 2.9944021662231535e-06}, {"id": 18, "seek": 8378, "start": 86.56, "end": 94.2, "text": " So let's say that you have a cost function j of just one parameter w, where w is a number.", "tokens": [50364, 445, 472, 13075, 13, 50503, 50503, 407, 718, 311, 584, 300, 291, 362, 257, 2063, 2445, 361, 295, 445, 472, 13075, 261, 11, 689, 261, 307, 257, 1230, 13, 50885, 50885, 639, 1355, 300, 16235, 23475, 586, 1542, 411, 341, 13, 51069, 51069, 261, 307, 10588, 281, 261, 3175, 264, 2539, 3314, 8961, 1413, 274, 670, 27379, 295, 361, 295, 261, 13, 51533, 51533, 400, 291, 434, 1382, 281, 17522, 264, 2063, 538, 23559, 264, 13075, 261, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10308129612992449, "compression_ratio": 1.5794871794871794, "no_speech_prob": 2.9944021662231535e-06}, {"id": 19, "seek": 8378, "start": 94.2, "end": 97.88, "text": " This means that gradient descent now looks like this.", "tokens": [50364, 445, 472, 13075, 13, 50503, 50503, 407, 718, 311, 584, 300, 291, 362, 257, 2063, 2445, 361, 295, 445, 472, 13075, 261, 11, 689, 261, 307, 257, 1230, 13, 50885, 50885, 639, 1355, 300, 16235, 23475, 586, 1542, 411, 341, 13, 51069, 51069, 261, 307, 10588, 281, 261, 3175, 264, 2539, 3314, 8961, 1413, 274, 670, 27379, 295, 361, 295, 261, 13, 51533, 51533, 400, 291, 434, 1382, 281, 17522, 264, 2063, 538, 23559, 264, 13075, 261, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10308129612992449, "compression_ratio": 1.5794871794871794, "no_speech_prob": 2.9944021662231535e-06}, {"id": 20, "seek": 8378, "start": 97.88, "end": 107.16, "text": " w is updated to w minus the learning rate alpha times d over dw of j of w.", "tokens": [50364, 445, 472, 13075, 13, 50503, 50503, 407, 718, 311, 584, 300, 291, 362, 257, 2063, 2445, 361, 295, 445, 472, 13075, 261, 11, 689, 261, 307, 257, 1230, 13, 50885, 50885, 639, 1355, 300, 16235, 23475, 586, 1542, 411, 341, 13, 51069, 51069, 261, 307, 10588, 281, 261, 3175, 264, 2539, 3314, 8961, 1413, 274, 670, 27379, 295, 361, 295, 261, 13, 51533, 51533, 400, 291, 434, 1382, 281, 17522, 264, 2063, 538, 23559, 264, 13075, 261, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10308129612992449, "compression_ratio": 1.5794871794871794, "no_speech_prob": 2.9944021662231535e-06}, {"id": 21, "seek": 8378, "start": 107.16, "end": 112.74000000000001, "text": " And you're trying to minimize the cost by adjusting the parameter w.", "tokens": [50364, 445, 472, 13075, 13, 50503, 50503, 407, 718, 311, 584, 300, 291, 362, 257, 2063, 2445, 361, 295, 445, 472, 13075, 261, 11, 689, 261, 307, 257, 1230, 13, 50885, 50885, 639, 1355, 300, 16235, 23475, 586, 1542, 411, 341, 13, 51069, 51069, 261, 307, 10588, 281, 261, 3175, 264, 2539, 3314, 8961, 1413, 274, 670, 27379, 295, 361, 295, 261, 13, 51533, 51533, 400, 291, 434, 1382, 281, 17522, 264, 2063, 538, 23559, 264, 13075, 261, 13, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10308129612992449, "compression_ratio": 1.5794871794871794, "no_speech_prob": 2.9944021662231535e-06}, {"id": 22, "seek": 11274, "start": 112.74, "end": 119.44, "text": " So this is like our previous example, where we had temporarily set b equal to zero.", "tokens": [50364, 407, 341, 307, 411, 527, 3894, 1365, 11, 689, 321, 632, 23750, 992, 272, 2681, 281, 4018, 13, 50699, 50699, 2022, 472, 13075, 261, 2602, 295, 732, 11, 291, 393, 574, 412, 732, 18795, 24877, 295, 264, 2063, 50957, 50957, 2445, 361, 2602, 295, 1045, 18795, 24877, 13, 51147, 51147, 961, 311, 574, 412, 437, 16235, 23475, 775, 322, 341, 2445, 361, 295, 261, 13, 51497, 51497, 1692, 322, 264, 12750, 10298, 307, 13075, 261, 11, 293, 322, 264, 9429, 10298, 307, 264, 2063, 361, 295, 51847, 51847], "temperature": 0.0, "avg_logprob": -0.11613654030693901, "compression_ratio": 1.733644859813084, "no_speech_prob": 8.186306104107643e-07}, {"id": 23, "seek": 11274, "start": 119.44, "end": 124.6, "text": " With one parameter w instead of two, you can look at two dimensional graphs of the cost", "tokens": [50364, 407, 341, 307, 411, 527, 3894, 1365, 11, 689, 321, 632, 23750, 992, 272, 2681, 281, 4018, 13, 50699, 50699, 2022, 472, 13075, 261, 2602, 295, 732, 11, 291, 393, 574, 412, 732, 18795, 24877, 295, 264, 2063, 50957, 50957, 2445, 361, 2602, 295, 1045, 18795, 24877, 13, 51147, 51147, 961, 311, 574, 412, 437, 16235, 23475, 775, 322, 341, 2445, 361, 295, 261, 13, 51497, 51497, 1692, 322, 264, 12750, 10298, 307, 13075, 261, 11, 293, 322, 264, 9429, 10298, 307, 264, 2063, 361, 295, 51847, 51847], "temperature": 0.0, "avg_logprob": -0.11613654030693901, "compression_ratio": 1.733644859813084, "no_speech_prob": 8.186306104107643e-07}, {"id": 24, "seek": 11274, "start": 124.6, "end": 128.4, "text": " function j instead of three dimensional graphs.", "tokens": [50364, 407, 341, 307, 411, 527, 3894, 1365, 11, 689, 321, 632, 23750, 992, 272, 2681, 281, 4018, 13, 50699, 50699, 2022, 472, 13075, 261, 2602, 295, 732, 11, 291, 393, 574, 412, 732, 18795, 24877, 295, 264, 2063, 50957, 50957, 2445, 361, 2602, 295, 1045, 18795, 24877, 13, 51147, 51147, 961, 311, 574, 412, 437, 16235, 23475, 775, 322, 341, 2445, 361, 295, 261, 13, 51497, 51497, 1692, 322, 264, 12750, 10298, 307, 13075, 261, 11, 293, 322, 264, 9429, 10298, 307, 264, 2063, 361, 295, 51847, 51847], "temperature": 0.0, "avg_logprob": -0.11613654030693901, "compression_ratio": 1.733644859813084, "no_speech_prob": 8.186306104107643e-07}, {"id": 25, "seek": 11274, "start": 128.4, "end": 135.4, "text": " Let's look at what gradient descent does on this function j of w.", "tokens": [50364, 407, 341, 307, 411, 527, 3894, 1365, 11, 689, 321, 632, 23750, 992, 272, 2681, 281, 4018, 13, 50699, 50699, 2022, 472, 13075, 261, 2602, 295, 732, 11, 291, 393, 574, 412, 732, 18795, 24877, 295, 264, 2063, 50957, 50957, 2445, 361, 2602, 295, 1045, 18795, 24877, 13, 51147, 51147, 961, 311, 574, 412, 437, 16235, 23475, 775, 322, 341, 2445, 361, 295, 261, 13, 51497, 51497, 1692, 322, 264, 12750, 10298, 307, 13075, 261, 11, 293, 322, 264, 9429, 10298, 307, 264, 2063, 361, 295, 51847, 51847], "temperature": 0.0, "avg_logprob": -0.11613654030693901, "compression_ratio": 1.733644859813084, "no_speech_prob": 8.186306104107643e-07}, {"id": 26, "seek": 11274, "start": 135.4, "end": 142.4, "text": " Here on the horizontal axis is parameter w, and on the vertical axis is the cost j of", "tokens": [50364, 407, 341, 307, 411, 527, 3894, 1365, 11, 689, 321, 632, 23750, 992, 272, 2681, 281, 4018, 13, 50699, 50699, 2022, 472, 13075, 261, 2602, 295, 732, 11, 291, 393, 574, 412, 732, 18795, 24877, 295, 264, 2063, 50957, 50957, 2445, 361, 2602, 295, 1045, 18795, 24877, 13, 51147, 51147, 961, 311, 574, 412, 437, 16235, 23475, 775, 322, 341, 2445, 361, 295, 261, 13, 51497, 51497, 1692, 322, 264, 12750, 10298, 307, 13075, 261, 11, 293, 322, 264, 9429, 10298, 307, 264, 2063, 361, 295, 51847, 51847], "temperature": 0.0, "avg_logprob": -0.11613654030693901, "compression_ratio": 1.733644859813084, "no_speech_prob": 8.186306104107643e-07}, {"id": 27, "seek": 14240, "start": 142.4, "end": 143.4, "text": " w.", "tokens": [50364, 261, 13, 50414, 50414, 823, 11, 718, 311, 5883, 1125, 16235, 23475, 365, 512, 2891, 2158, 337, 261, 13, 50646, 50646, 961, 311, 5883, 1125, 309, 412, 341, 4914, 13, 50796, 50796, 407, 3811, 300, 291, 722, 766, 412, 341, 935, 558, 510, 322, 264, 2445, 361, 13, 51072, 51072, 708, 16235, 23475, 486, 360, 307, 309, 486, 5623, 261, 281, 312, 261, 3175, 2539, 3314, 8961, 1413, 51484, 51484, 274, 670, 27379, 295, 361, 295, 261, 13, 51660, 51660, 961, 311, 574, 412, 437, 341, 13760, 1433, 510, 1355, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1099821592632093, "compression_ratio": 1.6571428571428573, "no_speech_prob": 5.955071173957549e-06}, {"id": 28, "seek": 14240, "start": 143.4, "end": 148.04, "text": " Now, let's initialize gradient descent with some starting value for w.", "tokens": [50364, 261, 13, 50414, 50414, 823, 11, 718, 311, 5883, 1125, 16235, 23475, 365, 512, 2891, 2158, 337, 261, 13, 50646, 50646, 961, 311, 5883, 1125, 309, 412, 341, 4914, 13, 50796, 50796, 407, 3811, 300, 291, 722, 766, 412, 341, 935, 558, 510, 322, 264, 2445, 361, 13, 51072, 51072, 708, 16235, 23475, 486, 360, 307, 309, 486, 5623, 261, 281, 312, 261, 3175, 2539, 3314, 8961, 1413, 51484, 51484, 274, 670, 27379, 295, 361, 295, 261, 13, 51660, 51660, 961, 311, 574, 412, 437, 341, 13760, 1433, 510, 1355, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1099821592632093, "compression_ratio": 1.6571428571428573, "no_speech_prob": 5.955071173957549e-06}, {"id": 29, "seek": 14240, "start": 148.04, "end": 151.04000000000002, "text": " Let's initialize it at this location.", "tokens": [50364, 261, 13, 50414, 50414, 823, 11, 718, 311, 5883, 1125, 16235, 23475, 365, 512, 2891, 2158, 337, 261, 13, 50646, 50646, 961, 311, 5883, 1125, 309, 412, 341, 4914, 13, 50796, 50796, 407, 3811, 300, 291, 722, 766, 412, 341, 935, 558, 510, 322, 264, 2445, 361, 13, 51072, 51072, 708, 16235, 23475, 486, 360, 307, 309, 486, 5623, 261, 281, 312, 261, 3175, 2539, 3314, 8961, 1413, 51484, 51484, 274, 670, 27379, 295, 361, 295, 261, 13, 51660, 51660, 961, 311, 574, 412, 437, 341, 13760, 1433, 510, 1355, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1099821592632093, "compression_ratio": 1.6571428571428573, "no_speech_prob": 5.955071173957549e-06}, {"id": 30, "seek": 14240, "start": 151.04000000000002, "end": 156.56, "text": " So imagine that you start off at this point right here on the function j.", "tokens": [50364, 261, 13, 50414, 50414, 823, 11, 718, 311, 5883, 1125, 16235, 23475, 365, 512, 2891, 2158, 337, 261, 13, 50646, 50646, 961, 311, 5883, 1125, 309, 412, 341, 4914, 13, 50796, 50796, 407, 3811, 300, 291, 722, 766, 412, 341, 935, 558, 510, 322, 264, 2445, 361, 13, 51072, 51072, 708, 16235, 23475, 486, 360, 307, 309, 486, 5623, 261, 281, 312, 261, 3175, 2539, 3314, 8961, 1413, 51484, 51484, 274, 670, 27379, 295, 361, 295, 261, 13, 51660, 51660, 961, 311, 574, 412, 437, 341, 13760, 1433, 510, 1355, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1099821592632093, "compression_ratio": 1.6571428571428573, "no_speech_prob": 5.955071173957549e-06}, {"id": 31, "seek": 14240, "start": 156.56, "end": 164.8, "text": " What gradient descent will do is it will update w to be w minus learning rate alpha times", "tokens": [50364, 261, 13, 50414, 50414, 823, 11, 718, 311, 5883, 1125, 16235, 23475, 365, 512, 2891, 2158, 337, 261, 13, 50646, 50646, 961, 311, 5883, 1125, 309, 412, 341, 4914, 13, 50796, 50796, 407, 3811, 300, 291, 722, 766, 412, 341, 935, 558, 510, 322, 264, 2445, 361, 13, 51072, 51072, 708, 16235, 23475, 486, 360, 307, 309, 486, 5623, 261, 281, 312, 261, 3175, 2539, 3314, 8961, 1413, 51484, 51484, 274, 670, 27379, 295, 361, 295, 261, 13, 51660, 51660, 961, 311, 574, 412, 437, 341, 13760, 1433, 510, 1355, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1099821592632093, "compression_ratio": 1.6571428571428573, "no_speech_prob": 5.955071173957549e-06}, {"id": 32, "seek": 14240, "start": 164.8, "end": 168.32, "text": " d over dw of j of w.", "tokens": [50364, 261, 13, 50414, 50414, 823, 11, 718, 311, 5883, 1125, 16235, 23475, 365, 512, 2891, 2158, 337, 261, 13, 50646, 50646, 961, 311, 5883, 1125, 309, 412, 341, 4914, 13, 50796, 50796, 407, 3811, 300, 291, 722, 766, 412, 341, 935, 558, 510, 322, 264, 2445, 361, 13, 51072, 51072, 708, 16235, 23475, 486, 360, 307, 309, 486, 5623, 261, 281, 312, 261, 3175, 2539, 3314, 8961, 1413, 51484, 51484, 274, 670, 27379, 295, 361, 295, 261, 13, 51660, 51660, 961, 311, 574, 412, 437, 341, 13760, 1433, 510, 1355, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1099821592632093, "compression_ratio": 1.6571428571428573, "no_speech_prob": 5.955071173957549e-06}, {"id": 33, "seek": 14240, "start": 168.32, "end": 172.08, "text": " Let's look at what this derivative term here means.", "tokens": [50364, 261, 13, 50414, 50414, 823, 11, 718, 311, 5883, 1125, 16235, 23475, 365, 512, 2891, 2158, 337, 261, 13, 50646, 50646, 961, 311, 5883, 1125, 309, 412, 341, 4914, 13, 50796, 50796, 407, 3811, 300, 291, 722, 766, 412, 341, 935, 558, 510, 322, 264, 2445, 361, 13, 51072, 51072, 708, 16235, 23475, 486, 360, 307, 309, 486, 5623, 261, 281, 312, 261, 3175, 2539, 3314, 8961, 1413, 51484, 51484, 274, 670, 27379, 295, 361, 295, 261, 13, 51660, 51660, 961, 311, 574, 412, 437, 341, 13760, 1433, 510, 1355, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1099821592632093, "compression_ratio": 1.6571428571428573, "no_speech_prob": 5.955071173957549e-06}, {"id": 34, "seek": 17208, "start": 172.08, "end": 179.8, "text": " A way to think about the derivative at this point on the line is to draw a tangent line,", "tokens": [50364, 316, 636, 281, 519, 466, 264, 13760, 412, 341, 935, 322, 264, 1622, 307, 281, 2642, 257, 27747, 1622, 11, 50750, 50750, 597, 307, 257, 2997, 1622, 300, 17431, 341, 7605, 412, 300, 935, 13, 50968, 50968, 682, 5221, 11, 264, 13525, 295, 341, 1622, 307, 264, 13760, 295, 264, 2445, 361, 412, 341, 935, 13, 51292, 51292, 400, 281, 483, 264, 13525, 11, 291, 393, 2642, 257, 707, 13369, 411, 341, 13, 51514, 51514, 400, 498, 291, 14722, 264, 6681, 6666, 538, 264, 11402, 295, 341, 13369, 11, 300, 307, 264, 13525, 13, 51833, 51833], "temperature": 0.0, "avg_logprob": -0.0869767325265067, "compression_ratio": 1.8695652173913044, "no_speech_prob": 1.7502425180282444e-05}, {"id": 35, "seek": 17208, "start": 179.8, "end": 184.16000000000003, "text": " which is a straight line that touches this curve at that point.", "tokens": [50364, 316, 636, 281, 519, 466, 264, 13760, 412, 341, 935, 322, 264, 1622, 307, 281, 2642, 257, 27747, 1622, 11, 50750, 50750, 597, 307, 257, 2997, 1622, 300, 17431, 341, 7605, 412, 300, 935, 13, 50968, 50968, 682, 5221, 11, 264, 13525, 295, 341, 1622, 307, 264, 13760, 295, 264, 2445, 361, 412, 341, 935, 13, 51292, 51292, 400, 281, 483, 264, 13525, 11, 291, 393, 2642, 257, 707, 13369, 411, 341, 13, 51514, 51514, 400, 498, 291, 14722, 264, 6681, 6666, 538, 264, 11402, 295, 341, 13369, 11, 300, 307, 264, 13525, 13, 51833, 51833], "temperature": 0.0, "avg_logprob": -0.0869767325265067, "compression_ratio": 1.8695652173913044, "no_speech_prob": 1.7502425180282444e-05}, {"id": 36, "seek": 17208, "start": 184.16000000000003, "end": 190.64000000000001, "text": " In math, the slope of this line is the derivative of the function j at this point.", "tokens": [50364, 316, 636, 281, 519, 466, 264, 13760, 412, 341, 935, 322, 264, 1622, 307, 281, 2642, 257, 27747, 1622, 11, 50750, 50750, 597, 307, 257, 2997, 1622, 300, 17431, 341, 7605, 412, 300, 935, 13, 50968, 50968, 682, 5221, 11, 264, 13525, 295, 341, 1622, 307, 264, 13760, 295, 264, 2445, 361, 412, 341, 935, 13, 51292, 51292, 400, 281, 483, 264, 13525, 11, 291, 393, 2642, 257, 707, 13369, 411, 341, 13, 51514, 51514, 400, 498, 291, 14722, 264, 6681, 6666, 538, 264, 11402, 295, 341, 13369, 11, 300, 307, 264, 13525, 13, 51833, 51833], "temperature": 0.0, "avg_logprob": -0.0869767325265067, "compression_ratio": 1.8695652173913044, "no_speech_prob": 1.7502425180282444e-05}, {"id": 37, "seek": 17208, "start": 190.64000000000001, "end": 195.08, "text": " And to get the slope, you can draw a little triangle like this.", "tokens": [50364, 316, 636, 281, 519, 466, 264, 13760, 412, 341, 935, 322, 264, 1622, 307, 281, 2642, 257, 27747, 1622, 11, 50750, 50750, 597, 307, 257, 2997, 1622, 300, 17431, 341, 7605, 412, 300, 935, 13, 50968, 50968, 682, 5221, 11, 264, 13525, 295, 341, 1622, 307, 264, 13760, 295, 264, 2445, 361, 412, 341, 935, 13, 51292, 51292, 400, 281, 483, 264, 13525, 11, 291, 393, 2642, 257, 707, 13369, 411, 341, 13, 51514, 51514, 400, 498, 291, 14722, 264, 6681, 6666, 538, 264, 11402, 295, 341, 13369, 11, 300, 307, 264, 13525, 13, 51833, 51833], "temperature": 0.0, "avg_logprob": -0.0869767325265067, "compression_ratio": 1.8695652173913044, "no_speech_prob": 1.7502425180282444e-05}, {"id": 38, "seek": 17208, "start": 195.08, "end": 201.46, "text": " And if you compute the height divided by the width of this triangle, that is the slope.", "tokens": [50364, 316, 636, 281, 519, 466, 264, 13760, 412, 341, 935, 322, 264, 1622, 307, 281, 2642, 257, 27747, 1622, 11, 50750, 50750, 597, 307, 257, 2997, 1622, 300, 17431, 341, 7605, 412, 300, 935, 13, 50968, 50968, 682, 5221, 11, 264, 13525, 295, 341, 1622, 307, 264, 13760, 295, 264, 2445, 361, 412, 341, 935, 13, 51292, 51292, 400, 281, 483, 264, 13525, 11, 291, 393, 2642, 257, 707, 13369, 411, 341, 13, 51514, 51514, 400, 498, 291, 14722, 264, 6681, 6666, 538, 264, 11402, 295, 341, 13369, 11, 300, 307, 264, 13525, 13, 51833, 51833], "temperature": 0.0, "avg_logprob": -0.0869767325265067, "compression_ratio": 1.8695652173913044, "no_speech_prob": 1.7502425180282444e-05}, {"id": 39, "seek": 20146, "start": 201.46, "end": 207.56, "text": " So for example, the slope might be, you know, 2 over 1, for instance.", "tokens": [50364, 407, 337, 1365, 11, 264, 13525, 1062, 312, 11, 291, 458, 11, 568, 670, 502, 11, 337, 5197, 13, 50669, 50669, 400, 562, 264, 27747, 1622, 307, 12166, 493, 666, 264, 558, 11, 264, 13525, 307, 3353, 11, 597, 1355, 50939, 50939, 300, 341, 13760, 307, 257, 3353, 1230, 11, 370, 309, 311, 5044, 813, 4018, 13, 51265, 51265, 400, 370, 264, 10588, 261, 307, 516, 281, 312, 261, 3175, 264, 2539, 3314, 1413, 512, 3353, 1230, 13, 51663, 51663, 440, 2539, 3314, 307, 1009, 257, 3353, 1230, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.12144497902162614, "compression_ratio": 1.7440758293838863, "no_speech_prob": 5.014687303628307e-06}, {"id": 40, "seek": 20146, "start": 207.56, "end": 212.96, "text": " And when the tangent line is pointing up into the right, the slope is positive, which means", "tokens": [50364, 407, 337, 1365, 11, 264, 13525, 1062, 312, 11, 291, 458, 11, 568, 670, 502, 11, 337, 5197, 13, 50669, 50669, 400, 562, 264, 27747, 1622, 307, 12166, 493, 666, 264, 558, 11, 264, 13525, 307, 3353, 11, 597, 1355, 50939, 50939, 300, 341, 13760, 307, 257, 3353, 1230, 11, 370, 309, 311, 5044, 813, 4018, 13, 51265, 51265, 400, 370, 264, 10588, 261, 307, 516, 281, 312, 261, 3175, 264, 2539, 3314, 1413, 512, 3353, 1230, 13, 51663, 51663, 440, 2539, 3314, 307, 1009, 257, 3353, 1230, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.12144497902162614, "compression_ratio": 1.7440758293838863, "no_speech_prob": 5.014687303628307e-06}, {"id": 41, "seek": 20146, "start": 212.96, "end": 219.48000000000002, "text": " that this derivative is a positive number, so it's greater than zero.", "tokens": [50364, 407, 337, 1365, 11, 264, 13525, 1062, 312, 11, 291, 458, 11, 568, 670, 502, 11, 337, 5197, 13, 50669, 50669, 400, 562, 264, 27747, 1622, 307, 12166, 493, 666, 264, 558, 11, 264, 13525, 307, 3353, 11, 597, 1355, 50939, 50939, 300, 341, 13760, 307, 257, 3353, 1230, 11, 370, 309, 311, 5044, 813, 4018, 13, 51265, 51265, 400, 370, 264, 10588, 261, 307, 516, 281, 312, 261, 3175, 264, 2539, 3314, 1413, 512, 3353, 1230, 13, 51663, 51663, 440, 2539, 3314, 307, 1009, 257, 3353, 1230, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.12144497902162614, "compression_ratio": 1.7440758293838863, "no_speech_prob": 5.014687303628307e-06}, {"id": 42, "seek": 20146, "start": 219.48000000000002, "end": 227.44, "text": " And so the updated w is going to be w minus the learning rate times some positive number.", "tokens": [50364, 407, 337, 1365, 11, 264, 13525, 1062, 312, 11, 291, 458, 11, 568, 670, 502, 11, 337, 5197, 13, 50669, 50669, 400, 562, 264, 27747, 1622, 307, 12166, 493, 666, 264, 558, 11, 264, 13525, 307, 3353, 11, 597, 1355, 50939, 50939, 300, 341, 13760, 307, 257, 3353, 1230, 11, 370, 309, 311, 5044, 813, 4018, 13, 51265, 51265, 400, 370, 264, 10588, 261, 307, 516, 281, 312, 261, 3175, 264, 2539, 3314, 1413, 512, 3353, 1230, 13, 51663, 51663, 440, 2539, 3314, 307, 1009, 257, 3353, 1230, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.12144497902162614, "compression_ratio": 1.7440758293838863, "no_speech_prob": 5.014687303628307e-06}, {"id": 43, "seek": 20146, "start": 227.44, "end": 230.54000000000002, "text": " The learning rate is always a positive number.", "tokens": [50364, 407, 337, 1365, 11, 264, 13525, 1062, 312, 11, 291, 458, 11, 568, 670, 502, 11, 337, 5197, 13, 50669, 50669, 400, 562, 264, 27747, 1622, 307, 12166, 493, 666, 264, 558, 11, 264, 13525, 307, 3353, 11, 597, 1355, 50939, 50939, 300, 341, 13760, 307, 257, 3353, 1230, 11, 370, 309, 311, 5044, 813, 4018, 13, 51265, 51265, 400, 370, 264, 10588, 261, 307, 516, 281, 312, 261, 3175, 264, 2539, 3314, 1413, 512, 3353, 1230, 13, 51663, 51663, 440, 2539, 3314, 307, 1009, 257, 3353, 1230, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.12144497902162614, "compression_ratio": 1.7440758293838863, "no_speech_prob": 5.014687303628307e-06}, {"id": 44, "seek": 23054, "start": 230.54, "end": 237.04, "text": " So if you take w minus a positive number, you end up with a new value for w that is", "tokens": [50364, 407, 498, 291, 747, 261, 3175, 257, 3353, 1230, 11, 291, 917, 493, 365, 257, 777, 2158, 337, 261, 300, 307, 50689, 50689, 4356, 13, 50772, 50772, 407, 322, 264, 4295, 11, 321, 366, 2684, 281, 264, 1411, 11, 321, 366, 23223, 264, 2158, 295, 261, 13, 51129, 51129, 400, 291, 815, 3449, 300, 341, 307, 264, 558, 551, 281, 360, 498, 428, 3387, 307, 281, 11514, 264, 51327, 51327, 2063, 361, 11, 570, 562, 321, 1286, 7361, 264, 1411, 322, 341, 7605, 11, 264, 2063, 361, 24108, 293, 291, 434, 51639, 51639], "temperature": 0.0, "avg_logprob": -0.1355379907708419, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.5779456816744641e-06}, {"id": 45, "seek": 23054, "start": 237.04, "end": 238.7, "text": " smaller.", "tokens": [50364, 407, 498, 291, 747, 261, 3175, 257, 3353, 1230, 11, 291, 917, 493, 365, 257, 777, 2158, 337, 261, 300, 307, 50689, 50689, 4356, 13, 50772, 50772, 407, 322, 264, 4295, 11, 321, 366, 2684, 281, 264, 1411, 11, 321, 366, 23223, 264, 2158, 295, 261, 13, 51129, 51129, 400, 291, 815, 3449, 300, 341, 307, 264, 558, 551, 281, 360, 498, 428, 3387, 307, 281, 11514, 264, 51327, 51327, 2063, 361, 11, 570, 562, 321, 1286, 7361, 264, 1411, 322, 341, 7605, 11, 264, 2063, 361, 24108, 293, 291, 434, 51639, 51639], "temperature": 0.0, "avg_logprob": -0.1355379907708419, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.5779456816744641e-06}, {"id": 46, "seek": 23054, "start": 238.7, "end": 245.84, "text": " So on the graph, we are moving to the left, we are decreasing the value of w.", "tokens": [50364, 407, 498, 291, 747, 261, 3175, 257, 3353, 1230, 11, 291, 917, 493, 365, 257, 777, 2158, 337, 261, 300, 307, 50689, 50689, 4356, 13, 50772, 50772, 407, 322, 264, 4295, 11, 321, 366, 2684, 281, 264, 1411, 11, 321, 366, 23223, 264, 2158, 295, 261, 13, 51129, 51129, 400, 291, 815, 3449, 300, 341, 307, 264, 558, 551, 281, 360, 498, 428, 3387, 307, 281, 11514, 264, 51327, 51327, 2063, 361, 11, 570, 562, 321, 1286, 7361, 264, 1411, 322, 341, 7605, 11, 264, 2063, 361, 24108, 293, 291, 434, 51639, 51639], "temperature": 0.0, "avg_logprob": -0.1355379907708419, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.5779456816744641e-06}, {"id": 47, "seek": 23054, "start": 245.84, "end": 249.79999999999998, "text": " And you may notice that this is the right thing to do if your goal is to decrease the", "tokens": [50364, 407, 498, 291, 747, 261, 3175, 257, 3353, 1230, 11, 291, 917, 493, 365, 257, 777, 2158, 337, 261, 300, 307, 50689, 50689, 4356, 13, 50772, 50772, 407, 322, 264, 4295, 11, 321, 366, 2684, 281, 264, 1411, 11, 321, 366, 23223, 264, 2158, 295, 261, 13, 51129, 51129, 400, 291, 815, 3449, 300, 341, 307, 264, 558, 551, 281, 360, 498, 428, 3387, 307, 281, 11514, 264, 51327, 51327, 2063, 361, 11, 570, 562, 321, 1286, 7361, 264, 1411, 322, 341, 7605, 11, 264, 2063, 361, 24108, 293, 291, 434, 51639, 51639], "temperature": 0.0, "avg_logprob": -0.1355379907708419, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.5779456816744641e-06}, {"id": 48, "seek": 23054, "start": 249.79999999999998, "end": 256.03999999999996, "text": " cost j, because when we move toward the left on this curve, the cost j decreases and you're", "tokens": [50364, 407, 498, 291, 747, 261, 3175, 257, 3353, 1230, 11, 291, 917, 493, 365, 257, 777, 2158, 337, 261, 300, 307, 50689, 50689, 4356, 13, 50772, 50772, 407, 322, 264, 4295, 11, 321, 366, 2684, 281, 264, 1411, 11, 321, 366, 23223, 264, 2158, 295, 261, 13, 51129, 51129, 400, 291, 815, 3449, 300, 341, 307, 264, 558, 551, 281, 360, 498, 428, 3387, 307, 281, 11514, 264, 51327, 51327, 2063, 361, 11, 570, 562, 321, 1286, 7361, 264, 1411, 322, 341, 7605, 11, 264, 2063, 361, 24108, 293, 291, 434, 51639, 51639], "temperature": 0.0, "avg_logprob": -0.1355379907708419, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.5779456816744641e-06}, {"id": 49, "seek": 25604, "start": 256.04, "end": 261.04, "text": " getting closer to the minimum for j, which is over here.", "tokens": [50364, 1242, 4966, 281, 264, 7285, 337, 361, 11, 597, 307, 670, 510, 13, 50614, 50614, 407, 370, 1400, 11, 16235, 23475, 2544, 281, 312, 884, 264, 558, 551, 13, 50828, 50828, 823, 11, 718, 311, 574, 412, 1071, 1365, 13, 51010, 51010, 961, 311, 747, 264, 912, 2445, 361, 295, 261, 382, 3673, 13, 51157, 51157, 400, 586, 718, 311, 584, 300, 291, 5883, 1125, 16235, 23475, 412, 257, 819, 4914, 11, 584, 538, 10875, 51448, 51448, 257, 2891, 2158, 337, 261, 300, 311, 670, 510, 322, 264, 1411, 13, 51686, 51686, 407, 300, 311, 341, 935, 295, 264, 2445, 361, 13, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.13535229664928508, "compression_ratio": 1.6455696202531647, "no_speech_prob": 1.9333467662363546e-06}, {"id": 50, "seek": 25604, "start": 261.04, "end": 265.32, "text": " So so far, gradient descent seems to be doing the right thing.", "tokens": [50364, 1242, 4966, 281, 264, 7285, 337, 361, 11, 597, 307, 670, 510, 13, 50614, 50614, 407, 370, 1400, 11, 16235, 23475, 2544, 281, 312, 884, 264, 558, 551, 13, 50828, 50828, 823, 11, 718, 311, 574, 412, 1071, 1365, 13, 51010, 51010, 961, 311, 747, 264, 912, 2445, 361, 295, 261, 382, 3673, 13, 51157, 51157, 400, 586, 718, 311, 584, 300, 291, 5883, 1125, 16235, 23475, 412, 257, 819, 4914, 11, 584, 538, 10875, 51448, 51448, 257, 2891, 2158, 337, 261, 300, 311, 670, 510, 322, 264, 1411, 13, 51686, 51686, 407, 300, 311, 341, 935, 295, 264, 2445, 361, 13, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.13535229664928508, "compression_ratio": 1.6455696202531647, "no_speech_prob": 1.9333467662363546e-06}, {"id": 51, "seek": 25604, "start": 265.32, "end": 268.96000000000004, "text": " Now, let's look at another example.", "tokens": [50364, 1242, 4966, 281, 264, 7285, 337, 361, 11, 597, 307, 670, 510, 13, 50614, 50614, 407, 370, 1400, 11, 16235, 23475, 2544, 281, 312, 884, 264, 558, 551, 13, 50828, 50828, 823, 11, 718, 311, 574, 412, 1071, 1365, 13, 51010, 51010, 961, 311, 747, 264, 912, 2445, 361, 295, 261, 382, 3673, 13, 51157, 51157, 400, 586, 718, 311, 584, 300, 291, 5883, 1125, 16235, 23475, 412, 257, 819, 4914, 11, 584, 538, 10875, 51448, 51448, 257, 2891, 2158, 337, 261, 300, 311, 670, 510, 322, 264, 1411, 13, 51686, 51686, 407, 300, 311, 341, 935, 295, 264, 2445, 361, 13, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.13535229664928508, "compression_ratio": 1.6455696202531647, "no_speech_prob": 1.9333467662363546e-06}, {"id": 52, "seek": 25604, "start": 268.96000000000004, "end": 271.90000000000003, "text": " Let's take the same function j of w as above.", "tokens": [50364, 1242, 4966, 281, 264, 7285, 337, 361, 11, 597, 307, 670, 510, 13, 50614, 50614, 407, 370, 1400, 11, 16235, 23475, 2544, 281, 312, 884, 264, 558, 551, 13, 50828, 50828, 823, 11, 718, 311, 574, 412, 1071, 1365, 13, 51010, 51010, 961, 311, 747, 264, 912, 2445, 361, 295, 261, 382, 3673, 13, 51157, 51157, 400, 586, 718, 311, 584, 300, 291, 5883, 1125, 16235, 23475, 412, 257, 819, 4914, 11, 584, 538, 10875, 51448, 51448, 257, 2891, 2158, 337, 261, 300, 311, 670, 510, 322, 264, 1411, 13, 51686, 51686, 407, 300, 311, 341, 935, 295, 264, 2445, 361, 13, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.13535229664928508, "compression_ratio": 1.6455696202531647, "no_speech_prob": 1.9333467662363546e-06}, {"id": 53, "seek": 25604, "start": 271.90000000000003, "end": 277.72, "text": " And now let's say that you initialize gradient descent at a different location, say by choosing", "tokens": [50364, 1242, 4966, 281, 264, 7285, 337, 361, 11, 597, 307, 670, 510, 13, 50614, 50614, 407, 370, 1400, 11, 16235, 23475, 2544, 281, 312, 884, 264, 558, 551, 13, 50828, 50828, 823, 11, 718, 311, 574, 412, 1071, 1365, 13, 51010, 51010, 961, 311, 747, 264, 912, 2445, 361, 295, 261, 382, 3673, 13, 51157, 51157, 400, 586, 718, 311, 584, 300, 291, 5883, 1125, 16235, 23475, 412, 257, 819, 4914, 11, 584, 538, 10875, 51448, 51448, 257, 2891, 2158, 337, 261, 300, 311, 670, 510, 322, 264, 1411, 13, 51686, 51686, 407, 300, 311, 341, 935, 295, 264, 2445, 361, 13, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.13535229664928508, "compression_ratio": 1.6455696202531647, "no_speech_prob": 1.9333467662363546e-06}, {"id": 54, "seek": 25604, "start": 277.72, "end": 282.48, "text": " a starting value for w that's over here on the left.", "tokens": [50364, 1242, 4966, 281, 264, 7285, 337, 361, 11, 597, 307, 670, 510, 13, 50614, 50614, 407, 370, 1400, 11, 16235, 23475, 2544, 281, 312, 884, 264, 558, 551, 13, 50828, 50828, 823, 11, 718, 311, 574, 412, 1071, 1365, 13, 51010, 51010, 961, 311, 747, 264, 912, 2445, 361, 295, 261, 382, 3673, 13, 51157, 51157, 400, 586, 718, 311, 584, 300, 291, 5883, 1125, 16235, 23475, 412, 257, 819, 4914, 11, 584, 538, 10875, 51448, 51448, 257, 2891, 2158, 337, 261, 300, 311, 670, 510, 322, 264, 1411, 13, 51686, 51686, 407, 300, 311, 341, 935, 295, 264, 2445, 361, 13, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.13535229664928508, "compression_ratio": 1.6455696202531647, "no_speech_prob": 1.9333467662363546e-06}, {"id": 55, "seek": 25604, "start": 282.48, "end": 285.76, "text": " So that's this point of the function j.", "tokens": [50364, 1242, 4966, 281, 264, 7285, 337, 361, 11, 597, 307, 670, 510, 13, 50614, 50614, 407, 370, 1400, 11, 16235, 23475, 2544, 281, 312, 884, 264, 558, 551, 13, 50828, 50828, 823, 11, 718, 311, 574, 412, 1071, 1365, 13, 51010, 51010, 961, 311, 747, 264, 912, 2445, 361, 295, 261, 382, 3673, 13, 51157, 51157, 400, 586, 718, 311, 584, 300, 291, 5883, 1125, 16235, 23475, 412, 257, 819, 4914, 11, 584, 538, 10875, 51448, 51448, 257, 2891, 2158, 337, 261, 300, 311, 670, 510, 322, 264, 1411, 13, 51686, 51686, 407, 300, 311, 341, 935, 295, 264, 2445, 361, 13, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.13535229664928508, "compression_ratio": 1.6455696202531647, "no_speech_prob": 1.9333467662363546e-06}, {"id": 56, "seek": 28576, "start": 285.76, "end": 293.76, "text": " Now the derivative term, remember is d over dw of j of w.", "tokens": [50364, 823, 264, 13760, 1433, 11, 1604, 307, 274, 670, 27379, 295, 361, 295, 261, 13, 50764, 50764, 400, 562, 321, 574, 412, 264, 27747, 1622, 412, 341, 935, 670, 510, 11, 264, 13525, 295, 341, 1622, 307, 50998, 50998, 264, 13760, 295, 361, 412, 341, 935, 13, 51128, 51128, 583, 341, 27747, 1622, 307, 21254, 278, 760, 666, 264, 558, 13, 51331, 51331, 400, 370, 341, 1622, 21254, 278, 760, 666, 264, 558, 575, 257, 3671, 13525, 13, 51567, 51567, 682, 661, 2283, 11, 264, 13760, 361, 412, 341, 935, 307, 257, 3671, 1230, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.12787917166045218, "compression_ratio": 2.0670391061452515, "no_speech_prob": 7.4112026595685165e-06}, {"id": 57, "seek": 28576, "start": 293.76, "end": 298.44, "text": " And when we look at the tangent line at this point over here, the slope of this line is", "tokens": [50364, 823, 264, 13760, 1433, 11, 1604, 307, 274, 670, 27379, 295, 361, 295, 261, 13, 50764, 50764, 400, 562, 321, 574, 412, 264, 27747, 1622, 412, 341, 935, 670, 510, 11, 264, 13525, 295, 341, 1622, 307, 50998, 50998, 264, 13760, 295, 361, 412, 341, 935, 13, 51128, 51128, 583, 341, 27747, 1622, 307, 21254, 278, 760, 666, 264, 558, 13, 51331, 51331, 400, 370, 341, 1622, 21254, 278, 760, 666, 264, 558, 575, 257, 3671, 13525, 13, 51567, 51567, 682, 661, 2283, 11, 264, 13760, 361, 412, 341, 935, 307, 257, 3671, 1230, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.12787917166045218, "compression_ratio": 2.0670391061452515, "no_speech_prob": 7.4112026595685165e-06}, {"id": 58, "seek": 28576, "start": 298.44, "end": 301.03999999999996, "text": " the derivative of j at this point.", "tokens": [50364, 823, 264, 13760, 1433, 11, 1604, 307, 274, 670, 27379, 295, 361, 295, 261, 13, 50764, 50764, 400, 562, 321, 574, 412, 264, 27747, 1622, 412, 341, 935, 670, 510, 11, 264, 13525, 295, 341, 1622, 307, 50998, 50998, 264, 13760, 295, 361, 412, 341, 935, 13, 51128, 51128, 583, 341, 27747, 1622, 307, 21254, 278, 760, 666, 264, 558, 13, 51331, 51331, 400, 370, 341, 1622, 21254, 278, 760, 666, 264, 558, 575, 257, 3671, 13525, 13, 51567, 51567, 682, 661, 2283, 11, 264, 13760, 361, 412, 341, 935, 307, 257, 3671, 1230, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.12787917166045218, "compression_ratio": 2.0670391061452515, "no_speech_prob": 7.4112026595685165e-06}, {"id": 59, "seek": 28576, "start": 301.03999999999996, "end": 305.09999999999997, "text": " But this tangent line is sloping down into the right.", "tokens": [50364, 823, 264, 13760, 1433, 11, 1604, 307, 274, 670, 27379, 295, 361, 295, 261, 13, 50764, 50764, 400, 562, 321, 574, 412, 264, 27747, 1622, 412, 341, 935, 670, 510, 11, 264, 13525, 295, 341, 1622, 307, 50998, 50998, 264, 13760, 295, 361, 412, 341, 935, 13, 51128, 51128, 583, 341, 27747, 1622, 307, 21254, 278, 760, 666, 264, 558, 13, 51331, 51331, 400, 370, 341, 1622, 21254, 278, 760, 666, 264, 558, 575, 257, 3671, 13525, 13, 51567, 51567, 682, 661, 2283, 11, 264, 13760, 361, 412, 341, 935, 307, 257, 3671, 1230, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.12787917166045218, "compression_ratio": 2.0670391061452515, "no_speech_prob": 7.4112026595685165e-06}, {"id": 60, "seek": 28576, "start": 305.09999999999997, "end": 309.82, "text": " And so this line sloping down into the right has a negative slope.", "tokens": [50364, 823, 264, 13760, 1433, 11, 1604, 307, 274, 670, 27379, 295, 361, 295, 261, 13, 50764, 50764, 400, 562, 321, 574, 412, 264, 27747, 1622, 412, 341, 935, 670, 510, 11, 264, 13525, 295, 341, 1622, 307, 50998, 50998, 264, 13760, 295, 361, 412, 341, 935, 13, 51128, 51128, 583, 341, 27747, 1622, 307, 21254, 278, 760, 666, 264, 558, 13, 51331, 51331, 400, 370, 341, 1622, 21254, 278, 760, 666, 264, 558, 575, 257, 3671, 13525, 13, 51567, 51567, 682, 661, 2283, 11, 264, 13760, 361, 412, 341, 935, 307, 257, 3671, 1230, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.12787917166045218, "compression_ratio": 2.0670391061452515, "no_speech_prob": 7.4112026595685165e-06}, {"id": 61, "seek": 28576, "start": 309.82, "end": 314.52, "text": " In other words, the derivative j at this point is a negative number.", "tokens": [50364, 823, 264, 13760, 1433, 11, 1604, 307, 274, 670, 27379, 295, 361, 295, 261, 13, 50764, 50764, 400, 562, 321, 574, 412, 264, 27747, 1622, 412, 341, 935, 670, 510, 11, 264, 13525, 295, 341, 1622, 307, 50998, 50998, 264, 13760, 295, 361, 412, 341, 935, 13, 51128, 51128, 583, 341, 27747, 1622, 307, 21254, 278, 760, 666, 264, 558, 13, 51331, 51331, 400, 370, 341, 1622, 21254, 278, 760, 666, 264, 558, 575, 257, 3671, 13525, 13, 51567, 51567, 682, 661, 2283, 11, 264, 13760, 361, 412, 341, 935, 307, 257, 3671, 1230, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.12787917166045218, "compression_ratio": 2.0670391061452515, "no_speech_prob": 7.4112026595685165e-06}, {"id": 62, "seek": 31452, "start": 314.52, "end": 320.24, "text": " For instance, if you draw a triangle, then the height like this is negative two and the", "tokens": [50364, 1171, 5197, 11, 498, 291, 2642, 257, 13369, 11, 550, 264, 6681, 411, 341, 307, 3671, 732, 293, 264, 50650, 50650, 11402, 307, 472, 13, 50740, 50740, 407, 264, 13525, 307, 3671, 732, 6666, 538, 472, 11, 597, 307, 3671, 732, 11, 597, 307, 257, 3671, 51030, 51030, 1230, 13, 51120, 51120, 407, 562, 291, 5623, 261, 11, 291, 483, 261, 3175, 264, 2539, 3314, 1413, 257, 3671, 1230, 13, 51462, 51462, 400, 370, 341, 1355, 291, 16390, 490, 261, 257, 3671, 1230, 13, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.11085395379499956, "compression_ratio": 1.8611111111111112, "no_speech_prob": 8.664551387482788e-06}, {"id": 63, "seek": 31452, "start": 320.24, "end": 322.03999999999996, "text": " width is one.", "tokens": [50364, 1171, 5197, 11, 498, 291, 2642, 257, 13369, 11, 550, 264, 6681, 411, 341, 307, 3671, 732, 293, 264, 50650, 50650, 11402, 307, 472, 13, 50740, 50740, 407, 264, 13525, 307, 3671, 732, 6666, 538, 472, 11, 597, 307, 3671, 732, 11, 597, 307, 257, 3671, 51030, 51030, 1230, 13, 51120, 51120, 407, 562, 291, 5623, 261, 11, 291, 483, 261, 3175, 264, 2539, 3314, 1413, 257, 3671, 1230, 13, 51462, 51462, 400, 370, 341, 1355, 291, 16390, 490, 261, 257, 3671, 1230, 13, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.11085395379499956, "compression_ratio": 1.8611111111111112, "no_speech_prob": 8.664551387482788e-06}, {"id": 64, "seek": 31452, "start": 322.03999999999996, "end": 327.84, "text": " So the slope is negative two divided by one, which is negative two, which is a negative", "tokens": [50364, 1171, 5197, 11, 498, 291, 2642, 257, 13369, 11, 550, 264, 6681, 411, 341, 307, 3671, 732, 293, 264, 50650, 50650, 11402, 307, 472, 13, 50740, 50740, 407, 264, 13525, 307, 3671, 732, 6666, 538, 472, 11, 597, 307, 3671, 732, 11, 597, 307, 257, 3671, 51030, 51030, 1230, 13, 51120, 51120, 407, 562, 291, 5623, 261, 11, 291, 483, 261, 3175, 264, 2539, 3314, 1413, 257, 3671, 1230, 13, 51462, 51462, 400, 370, 341, 1355, 291, 16390, 490, 261, 257, 3671, 1230, 13, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.11085395379499956, "compression_ratio": 1.8611111111111112, "no_speech_prob": 8.664551387482788e-06}, {"id": 65, "seek": 31452, "start": 327.84, "end": 329.64, "text": " number.", "tokens": [50364, 1171, 5197, 11, 498, 291, 2642, 257, 13369, 11, 550, 264, 6681, 411, 341, 307, 3671, 732, 293, 264, 50650, 50650, 11402, 307, 472, 13, 50740, 50740, 407, 264, 13525, 307, 3671, 732, 6666, 538, 472, 11, 597, 307, 3671, 732, 11, 597, 307, 257, 3671, 51030, 51030, 1230, 13, 51120, 51120, 407, 562, 291, 5623, 261, 11, 291, 483, 261, 3175, 264, 2539, 3314, 1413, 257, 3671, 1230, 13, 51462, 51462, 400, 370, 341, 1355, 291, 16390, 490, 261, 257, 3671, 1230, 13, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.11085395379499956, "compression_ratio": 1.8611111111111112, "no_speech_prob": 8.664551387482788e-06}, {"id": 66, "seek": 31452, "start": 329.64, "end": 336.47999999999996, "text": " So when you update w, you get w minus the learning rate times a negative number.", "tokens": [50364, 1171, 5197, 11, 498, 291, 2642, 257, 13369, 11, 550, 264, 6681, 411, 341, 307, 3671, 732, 293, 264, 50650, 50650, 11402, 307, 472, 13, 50740, 50740, 407, 264, 13525, 307, 3671, 732, 6666, 538, 472, 11, 597, 307, 3671, 732, 11, 597, 307, 257, 3671, 51030, 51030, 1230, 13, 51120, 51120, 407, 562, 291, 5623, 261, 11, 291, 483, 261, 3175, 264, 2539, 3314, 1413, 257, 3671, 1230, 13, 51462, 51462, 400, 370, 341, 1355, 291, 16390, 490, 261, 257, 3671, 1230, 13, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.11085395379499956, "compression_ratio": 1.8611111111111112, "no_speech_prob": 8.664551387482788e-06}, {"id": 67, "seek": 31452, "start": 336.47999999999996, "end": 341.68, "text": " And so this means you subtract from w a negative number.", "tokens": [50364, 1171, 5197, 11, 498, 291, 2642, 257, 13369, 11, 550, 264, 6681, 411, 341, 307, 3671, 732, 293, 264, 50650, 50650, 11402, 307, 472, 13, 50740, 50740, 407, 264, 13525, 307, 3671, 732, 6666, 538, 472, 11, 597, 307, 3671, 732, 11, 597, 307, 257, 3671, 51030, 51030, 1230, 13, 51120, 51120, 407, 562, 291, 5623, 261, 11, 291, 483, 261, 3175, 264, 2539, 3314, 1413, 257, 3671, 1230, 13, 51462, 51462, 400, 370, 341, 1355, 291, 16390, 490, 261, 257, 3671, 1230, 13, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.11085395379499956, "compression_ratio": 1.8611111111111112, "no_speech_prob": 8.664551387482788e-06}, {"id": 68, "seek": 34168, "start": 341.68, "end": 347.44, "text": " But subtracting a negative number means adding a positive number.", "tokens": [50364, 583, 16390, 278, 257, 3671, 1230, 1355, 5127, 257, 3353, 1230, 13, 50652, 50652, 400, 370, 291, 917, 493, 5662, 261, 11, 570, 16390, 278, 257, 3671, 1230, 307, 264, 912, 382, 5127, 257, 51014, 51014, 3353, 1230, 281, 261, 13, 51175, 51175, 407, 341, 1823, 295, 16235, 23475, 7700, 261, 281, 3488, 11, 597, 1355, 291, 434, 2684, 281, 51464, 51464, 264, 558, 295, 264, 4295, 11, 293, 428, 3792, 361, 575, 24436, 760, 281, 510, 13, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.13946927466043613, "compression_ratio": 1.7945945945945947, "no_speech_prob": 1.0783096513478085e-05}, {"id": 69, "seek": 34168, "start": 347.44, "end": 354.68, "text": " And so you end up increasing w, because subtracting a negative number is the same as adding a", "tokens": [50364, 583, 16390, 278, 257, 3671, 1230, 1355, 5127, 257, 3353, 1230, 13, 50652, 50652, 400, 370, 291, 917, 493, 5662, 261, 11, 570, 16390, 278, 257, 3671, 1230, 307, 264, 912, 382, 5127, 257, 51014, 51014, 3353, 1230, 281, 261, 13, 51175, 51175, 407, 341, 1823, 295, 16235, 23475, 7700, 261, 281, 3488, 11, 597, 1355, 291, 434, 2684, 281, 51464, 51464, 264, 558, 295, 264, 4295, 11, 293, 428, 3792, 361, 575, 24436, 760, 281, 510, 13, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.13946927466043613, "compression_ratio": 1.7945945945945947, "no_speech_prob": 1.0783096513478085e-05}, {"id": 70, "seek": 34168, "start": 354.68, "end": 357.9, "text": " positive number to w.", "tokens": [50364, 583, 16390, 278, 257, 3671, 1230, 1355, 5127, 257, 3353, 1230, 13, 50652, 50652, 400, 370, 291, 917, 493, 5662, 261, 11, 570, 16390, 278, 257, 3671, 1230, 307, 264, 912, 382, 5127, 257, 51014, 51014, 3353, 1230, 281, 261, 13, 51175, 51175, 407, 341, 1823, 295, 16235, 23475, 7700, 261, 281, 3488, 11, 597, 1355, 291, 434, 2684, 281, 51464, 51464, 264, 558, 295, 264, 4295, 11, 293, 428, 3792, 361, 575, 24436, 760, 281, 510, 13, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.13946927466043613, "compression_ratio": 1.7945945945945947, "no_speech_prob": 1.0783096513478085e-05}, {"id": 71, "seek": 34168, "start": 357.9, "end": 363.68, "text": " So this step of gradient descent causes w to increase, which means you're moving to", "tokens": [50364, 583, 16390, 278, 257, 3671, 1230, 1355, 5127, 257, 3353, 1230, 13, 50652, 50652, 400, 370, 291, 917, 493, 5662, 261, 11, 570, 16390, 278, 257, 3671, 1230, 307, 264, 912, 382, 5127, 257, 51014, 51014, 3353, 1230, 281, 261, 13, 51175, 51175, 407, 341, 1823, 295, 16235, 23475, 7700, 261, 281, 3488, 11, 597, 1355, 291, 434, 2684, 281, 51464, 51464, 264, 558, 295, 264, 4295, 11, 293, 428, 3792, 361, 575, 24436, 760, 281, 510, 13, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.13946927466043613, "compression_ratio": 1.7945945945945947, "no_speech_prob": 1.0783096513478085e-05}, {"id": 72, "seek": 34168, "start": 363.68, "end": 369.24, "text": " the right of the graph, and your cos j has decreased down to here.", "tokens": [50364, 583, 16390, 278, 257, 3671, 1230, 1355, 5127, 257, 3353, 1230, 13, 50652, 50652, 400, 370, 291, 917, 493, 5662, 261, 11, 570, 16390, 278, 257, 3671, 1230, 307, 264, 912, 382, 5127, 257, 51014, 51014, 3353, 1230, 281, 261, 13, 51175, 51175, 407, 341, 1823, 295, 16235, 23475, 7700, 261, 281, 3488, 11, 597, 1355, 291, 434, 2684, 281, 51464, 51464, 264, 558, 295, 264, 4295, 11, 293, 428, 3792, 361, 575, 24436, 760, 281, 510, 13, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.13946927466043613, "compression_ratio": 1.7945945945945947, "no_speech_prob": 1.0783096513478085e-05}, {"id": 73, "seek": 36924, "start": 369.24, "end": 374.56, "text": " And again, it looks like gradient descent is doing something reasonable, it's getting", "tokens": [50364, 400, 797, 11, 309, 1542, 411, 16235, 23475, 307, 884, 746, 10585, 11, 309, 311, 1242, 50630, 50630, 291, 4966, 281, 264, 7285, 13, 50768, 50768, 407, 4696, 11, 613, 1036, 732, 5110, 855, 512, 295, 264, 24002, 2261, 437, 264, 13760, 51086, 51086, 1433, 307, 884, 11, 293, 983, 341, 3665, 16235, 23475, 1319, 261, 281, 483, 291, 4966, 281, 264, 51418, 51418, 7285, 13, 51468, 51468, 286, 1454, 341, 960, 2729, 291, 512, 2020, 337, 983, 264, 13760, 1433, 294, 16235, 23475, 51754, 51754, 1669, 2020, 13, 51829, 51829], "temperature": 0.0, "avg_logprob": -0.10083524642452117, "compression_ratio": 1.8465116279069766, "no_speech_prob": 1.2805023288819939e-05}, {"id": 74, "seek": 36924, "start": 374.56, "end": 377.32, "text": " you closer to the minimum.", "tokens": [50364, 400, 797, 11, 309, 1542, 411, 16235, 23475, 307, 884, 746, 10585, 11, 309, 311, 1242, 50630, 50630, 291, 4966, 281, 264, 7285, 13, 50768, 50768, 407, 4696, 11, 613, 1036, 732, 5110, 855, 512, 295, 264, 24002, 2261, 437, 264, 13760, 51086, 51086, 1433, 307, 884, 11, 293, 983, 341, 3665, 16235, 23475, 1319, 261, 281, 483, 291, 4966, 281, 264, 51418, 51418, 7285, 13, 51468, 51468, 286, 1454, 341, 960, 2729, 291, 512, 2020, 337, 983, 264, 13760, 1433, 294, 16235, 23475, 51754, 51754, 1669, 2020, 13, 51829, 51829], "temperature": 0.0, "avg_logprob": -0.10083524642452117, "compression_ratio": 1.8465116279069766, "no_speech_prob": 1.2805023288819939e-05}, {"id": 75, "seek": 36924, "start": 377.32, "end": 383.68, "text": " So hopefully, these last two examples show some of the intuition behind what the derivative", "tokens": [50364, 400, 797, 11, 309, 1542, 411, 16235, 23475, 307, 884, 746, 10585, 11, 309, 311, 1242, 50630, 50630, 291, 4966, 281, 264, 7285, 13, 50768, 50768, 407, 4696, 11, 613, 1036, 732, 5110, 855, 512, 295, 264, 24002, 2261, 437, 264, 13760, 51086, 51086, 1433, 307, 884, 11, 293, 983, 341, 3665, 16235, 23475, 1319, 261, 281, 483, 291, 4966, 281, 264, 51418, 51418, 7285, 13, 51468, 51468, 286, 1454, 341, 960, 2729, 291, 512, 2020, 337, 983, 264, 13760, 1433, 294, 16235, 23475, 51754, 51754, 1669, 2020, 13, 51829, 51829], "temperature": 0.0, "avg_logprob": -0.10083524642452117, "compression_ratio": 1.8465116279069766, "no_speech_prob": 1.2805023288819939e-05}, {"id": 76, "seek": 36924, "start": 383.68, "end": 390.32, "text": " term is doing, and why this helps gradient descent change w to get you closer to the", "tokens": [50364, 400, 797, 11, 309, 1542, 411, 16235, 23475, 307, 884, 746, 10585, 11, 309, 311, 1242, 50630, 50630, 291, 4966, 281, 264, 7285, 13, 50768, 50768, 407, 4696, 11, 613, 1036, 732, 5110, 855, 512, 295, 264, 24002, 2261, 437, 264, 13760, 51086, 51086, 1433, 307, 884, 11, 293, 983, 341, 3665, 16235, 23475, 1319, 261, 281, 483, 291, 4966, 281, 264, 51418, 51418, 7285, 13, 51468, 51468, 286, 1454, 341, 960, 2729, 291, 512, 2020, 337, 983, 264, 13760, 1433, 294, 16235, 23475, 51754, 51754, 1669, 2020, 13, 51829, 51829], "temperature": 0.0, "avg_logprob": -0.10083524642452117, "compression_ratio": 1.8465116279069766, "no_speech_prob": 1.2805023288819939e-05}, {"id": 77, "seek": 36924, "start": 390.32, "end": 391.32, "text": " minimum.", "tokens": [50364, 400, 797, 11, 309, 1542, 411, 16235, 23475, 307, 884, 746, 10585, 11, 309, 311, 1242, 50630, 50630, 291, 4966, 281, 264, 7285, 13, 50768, 50768, 407, 4696, 11, 613, 1036, 732, 5110, 855, 512, 295, 264, 24002, 2261, 437, 264, 13760, 51086, 51086, 1433, 307, 884, 11, 293, 983, 341, 3665, 16235, 23475, 1319, 261, 281, 483, 291, 4966, 281, 264, 51418, 51418, 7285, 13, 51468, 51468, 286, 1454, 341, 960, 2729, 291, 512, 2020, 337, 983, 264, 13760, 1433, 294, 16235, 23475, 51754, 51754, 1669, 2020, 13, 51829, 51829], "temperature": 0.0, "avg_logprob": -0.10083524642452117, "compression_ratio": 1.8465116279069766, "no_speech_prob": 1.2805023288819939e-05}, {"id": 78, "seek": 36924, "start": 391.32, "end": 397.04, "text": " I hope this video gave you some sense for why the derivative term in gradient descent", "tokens": [50364, 400, 797, 11, 309, 1542, 411, 16235, 23475, 307, 884, 746, 10585, 11, 309, 311, 1242, 50630, 50630, 291, 4966, 281, 264, 7285, 13, 50768, 50768, 407, 4696, 11, 613, 1036, 732, 5110, 855, 512, 295, 264, 24002, 2261, 437, 264, 13760, 51086, 51086, 1433, 307, 884, 11, 293, 983, 341, 3665, 16235, 23475, 1319, 261, 281, 483, 291, 4966, 281, 264, 51418, 51418, 7285, 13, 51468, 51468, 286, 1454, 341, 960, 2729, 291, 512, 2020, 337, 983, 264, 13760, 1433, 294, 16235, 23475, 51754, 51754, 1669, 2020, 13, 51829, 51829], "temperature": 0.0, "avg_logprob": -0.10083524642452117, "compression_ratio": 1.8465116279069766, "no_speech_prob": 1.2805023288819939e-05}, {"id": 79, "seek": 36924, "start": 397.04, "end": 398.54, "text": " makes sense.", "tokens": [50364, 400, 797, 11, 309, 1542, 411, 16235, 23475, 307, 884, 746, 10585, 11, 309, 311, 1242, 50630, 50630, 291, 4966, 281, 264, 7285, 13, 50768, 50768, 407, 4696, 11, 613, 1036, 732, 5110, 855, 512, 295, 264, 24002, 2261, 437, 264, 13760, 51086, 51086, 1433, 307, 884, 11, 293, 983, 341, 3665, 16235, 23475, 1319, 261, 281, 483, 291, 4966, 281, 264, 51418, 51418, 7285, 13, 51468, 51468, 286, 1454, 341, 960, 2729, 291, 512, 2020, 337, 983, 264, 13760, 1433, 294, 16235, 23475, 51754, 51754, 1669, 2020, 13, 51829, 51829], "temperature": 0.0, "avg_logprob": -0.10083524642452117, "compression_ratio": 1.8465116279069766, "no_speech_prob": 1.2805023288819939e-05}, {"id": 80, "seek": 39854, "start": 398.54, "end": 403.52000000000004, "text": " One other key quantity in the gradient descent algorithm is the learning rate alpha.", "tokens": [50364, 1485, 661, 2141, 11275, 294, 264, 16235, 23475, 9284, 307, 264, 2539, 3314, 8961, 13, 50613, 50613, 1012, 360, 291, 2826, 8961, 30, 50671, 50671, 708, 2314, 498, 309, 311, 886, 1359, 30, 50745, 50745, 708, 2314, 498, 309, 311, 886, 955, 30, 50831, 50831, 682, 264, 958, 960, 11, 718, 311, 747, 257, 7731, 574, 412, 264, 13075, 8961, 281, 854, 1322, 16224, 626, 51098, 51098, 466, 437, 309, 775, 11, 382, 731, 382, 577, 281, 652, 257, 665, 3922, 337, 257, 665, 2158, 295, 8961, 337, 51351, 51351, 428, 11420, 295, 16235, 23475, 13, 51463], "temperature": 0.0, "avg_logprob": -0.13436807767309325, "compression_ratio": 1.6853448275862069, "no_speech_prob": 9.274761396227404e-05}, {"id": 81, "seek": 39854, "start": 403.52000000000004, "end": 404.68, "text": " How do you choose alpha?", "tokens": [50364, 1485, 661, 2141, 11275, 294, 264, 16235, 23475, 9284, 307, 264, 2539, 3314, 8961, 13, 50613, 50613, 1012, 360, 291, 2826, 8961, 30, 50671, 50671, 708, 2314, 498, 309, 311, 886, 1359, 30, 50745, 50745, 708, 2314, 498, 309, 311, 886, 955, 30, 50831, 50831, 682, 264, 958, 960, 11, 718, 311, 747, 257, 7731, 574, 412, 264, 13075, 8961, 281, 854, 1322, 16224, 626, 51098, 51098, 466, 437, 309, 775, 11, 382, 731, 382, 577, 281, 652, 257, 665, 3922, 337, 257, 665, 2158, 295, 8961, 337, 51351, 51351, 428, 11420, 295, 16235, 23475, 13, 51463], "temperature": 0.0, "avg_logprob": -0.13436807767309325, "compression_ratio": 1.6853448275862069, "no_speech_prob": 9.274761396227404e-05}, {"id": 82, "seek": 39854, "start": 404.68, "end": 406.16, "text": " What happens if it's too small?", "tokens": [50364, 1485, 661, 2141, 11275, 294, 264, 16235, 23475, 9284, 307, 264, 2539, 3314, 8961, 13, 50613, 50613, 1012, 360, 291, 2826, 8961, 30, 50671, 50671, 708, 2314, 498, 309, 311, 886, 1359, 30, 50745, 50745, 708, 2314, 498, 309, 311, 886, 955, 30, 50831, 50831, 682, 264, 958, 960, 11, 718, 311, 747, 257, 7731, 574, 412, 264, 13075, 8961, 281, 854, 1322, 16224, 626, 51098, 51098, 466, 437, 309, 775, 11, 382, 731, 382, 577, 281, 652, 257, 665, 3922, 337, 257, 665, 2158, 295, 8961, 337, 51351, 51351, 428, 11420, 295, 16235, 23475, 13, 51463], "temperature": 0.0, "avg_logprob": -0.13436807767309325, "compression_ratio": 1.6853448275862069, "no_speech_prob": 9.274761396227404e-05}, {"id": 83, "seek": 39854, "start": 406.16, "end": 407.88, "text": " What happens if it's too big?", "tokens": [50364, 1485, 661, 2141, 11275, 294, 264, 16235, 23475, 9284, 307, 264, 2539, 3314, 8961, 13, 50613, 50613, 1012, 360, 291, 2826, 8961, 30, 50671, 50671, 708, 2314, 498, 309, 311, 886, 1359, 30, 50745, 50745, 708, 2314, 498, 309, 311, 886, 955, 30, 50831, 50831, 682, 264, 958, 960, 11, 718, 311, 747, 257, 7731, 574, 412, 264, 13075, 8961, 281, 854, 1322, 16224, 626, 51098, 51098, 466, 437, 309, 775, 11, 382, 731, 382, 577, 281, 652, 257, 665, 3922, 337, 257, 665, 2158, 295, 8961, 337, 51351, 51351, 428, 11420, 295, 16235, 23475, 13, 51463], "temperature": 0.0, "avg_logprob": -0.13436807767309325, "compression_ratio": 1.6853448275862069, "no_speech_prob": 9.274761396227404e-05}, {"id": 84, "seek": 39854, "start": 407.88, "end": 413.22, "text": " In the next video, let's take a deeper look at the parameter alpha to help build intuitions", "tokens": [50364, 1485, 661, 2141, 11275, 294, 264, 16235, 23475, 9284, 307, 264, 2539, 3314, 8961, 13, 50613, 50613, 1012, 360, 291, 2826, 8961, 30, 50671, 50671, 708, 2314, 498, 309, 311, 886, 1359, 30, 50745, 50745, 708, 2314, 498, 309, 311, 886, 955, 30, 50831, 50831, 682, 264, 958, 960, 11, 718, 311, 747, 257, 7731, 574, 412, 264, 13075, 8961, 281, 854, 1322, 16224, 626, 51098, 51098, 466, 437, 309, 775, 11, 382, 731, 382, 577, 281, 652, 257, 665, 3922, 337, 257, 665, 2158, 295, 8961, 337, 51351, 51351, 428, 11420, 295, 16235, 23475, 13, 51463], "temperature": 0.0, "avg_logprob": -0.13436807767309325, "compression_ratio": 1.6853448275862069, "no_speech_prob": 9.274761396227404e-05}, {"id": 85, "seek": 39854, "start": 413.22, "end": 418.28000000000003, "text": " about what it does, as well as how to make a good choice for a good value of alpha for", "tokens": [50364, 1485, 661, 2141, 11275, 294, 264, 16235, 23475, 9284, 307, 264, 2539, 3314, 8961, 13, 50613, 50613, 1012, 360, 291, 2826, 8961, 30, 50671, 50671, 708, 2314, 498, 309, 311, 886, 1359, 30, 50745, 50745, 708, 2314, 498, 309, 311, 886, 955, 30, 50831, 50831, 682, 264, 958, 960, 11, 718, 311, 747, 257, 7731, 574, 412, 264, 13075, 8961, 281, 854, 1322, 16224, 626, 51098, 51098, 466, 437, 309, 775, 11, 382, 731, 382, 577, 281, 652, 257, 665, 3922, 337, 257, 665, 2158, 295, 8961, 337, 51351, 51351, 428, 11420, 295, 16235, 23475, 13, 51463], "temperature": 0.0, "avg_logprob": -0.13436807767309325, "compression_ratio": 1.6853448275862069, "no_speech_prob": 9.274761396227404e-05}, {"id": 86, "seek": 41828, "start": 418.28, "end": 428.96, "text": " your implementation of gradient descent.", "tokens": [50364, 428, 11420, 295, 16235, 23475, 13, 50898], "temperature": 0.0, "avg_logprob": -0.5230323473612467, "compression_ratio": 0.8888888888888888, "no_speech_prob": 2.09716017707251e-05}], "language": "en", "video_id": "rGvoO8U2Ozc", "entity": "ML Specialization, Andrew Ng (2022)"}}