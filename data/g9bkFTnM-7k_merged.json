{"video_id": "g9bkFTnM-7k", "title": "2.6 Practical Tips for Linear Regression | Feature scaling part 2-- [Machine Learning | Andrew Ng]", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 454, "views": 273, "publish_date": "11/04/2022", "timestamp": 1661040000, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " Let's look at how you can implement feature scaling to take features that take on very different ranges of values and scale them to have comparable ranges of value to each other. So how do you actually scale features? Well, if X1 ranges from 3 to 2000, one way to get a scale version of X1 is to take each original X1 value and divide by 2000 the maximum of the range. So the scale X1 will range from 0.15 up to 1. Similarly, since X2 ranges from 0 to 5, you can calculate a scale version of X2 by taking each original X2 and dividing by 5, which is again the maximum. So the scale X2 will now range from 0 to 1. So if you plot the scaled X1 and X2 on a graph, it might look like this. In addition to dividing by the maximum, you can also do what's called mean normalization. So what this looks like is, you start with the original features and then you rescale them so that both of them are centered around 0. So whereas before they only had values greater than 0, now they have both negative and positive values, but maybe usually between negative 1 and plus 1. So to calculate the mean normalization of X1, first find the average, also called the mean of X1 on your training set, and let's call this mean mu1, with this being the Greek alphabet mu. For example, you may find that the average of feature 1, mu1, is 600 square feet. So let's take each X1, subtract the mean mu1, and then let's divide by the difference 2000 minus 300, where 2000 is the maximum and 300 the minimum. And if you do this, you get the normalized X1 to range from negative 0.18 to 0.82. Similarly to mean normalize X2, you can calculate the average of feature 2, and for instance mu2 may be 2.3. Then you can take each X2, subtract mu2, and divide by 5 minus 0, again the max 5 minus the min which is 0. The mean normalized X2 now ranges from negative 0.46 to 0.54, so if you plot the training data using the mean normalized X1 and X2, it might look like this. There's one last common rescaling method called z-score normalization. To implement z-score normalization, you need to calculate something called the standard deviation of each feature. If you don't know what a standard deviation is, don't worry about it, you won't need to know it for this class. Or if you've heard of the normal distribution or the bell-shaped curve, sometimes also called the Gaussian distribution, this is what the standard deviation for the normal distribution looks like. But if you haven't heard of this, you don't need to worry about that either. But if you do know what is the standard deviation, then to implement a z-score normalization, you first calculate the mean mu as well as the standard deviation, which is often denoted by the lowercase Greek alphabet sigma of each feature. So for instance, maybe feature 1 has a standard deviation of 450 and mean 600, then to z-score normalize x1, take each x1, subtract mu 1, and then divide by the standard deviation, which I'm going to denote as sigma 1. And what you might find is that the z-score normalize x1 now ranges from negative 0.67 to 3.1. Similarly, if you calculate the second feature's standard deviation to be 1.4 and mean to be 2.3, then you can compute x2 minus mu 2 divided by sigma 2. And in this case, the z-score normalize by x2 might now range from negative 1.6 to 1.9. So if you plot the training data on the normalized x1 and x2 on a graph, it might look like this. As a rule of thumb, when performing feature scaling, you might want to aim for getting the features to range from maybe anywhere around negative 1 to somewhere around plus 1 for each feature x. But these values, negative 1 and plus 1 can be a little bit loose. So if the features range from negative 3 to plus 3, or negative 0.3 to plus 0.3, all of these are completely okay. So if you have a feature x1 that winds up being between 0 and 3, that's not a problem. And you can rescale it if you want, but if you don't rescale it, it should work okay too. Or if you have a different feature, x2, whose values are between negative 2 and plus 0.5, again, that's okay. No harm rescaling it, but it might be okay if you leave it alone as well. But if another feature like x3 here ranges from negative 100 to plus 100, then this takes on a very different range of values than something from around negative 1 to plus 1. So you're probably better off rescaling this feature x3 so that it ranges from something closer to negative 1 to plus 1. Similarly, if you have a feature x4 that takes on really small values, say between negative 0.001 and plus 0.001, then these values are so small, that means you may want to rescale it as well. Finally, what if your feature x5, such as measurements of a hospital patient's body temperature, ranges from 98.6 to 105 degrees Fahrenheit? In this case, these values are around 100, which is actually pretty large compared to other scale features, and this will actually cause gradient descent to run more slowly. So in this case, feature rescaling will likely help. There's almost never any harm to carrying out feature rescaling, so when in doubt, I encourage you to just carry it out. And that's it for feature scaling. With this little technique, you'll often be able to get gradient descent to run much faster. So that's feature scaling. And with or without feature scaling, when you run gradient descent, how can you know, how can you check if gradient descent is really working, if it is finding you the global minimum or something close to it? In the next video, let's take a look at how to recognize if gradient descent is converging. And then in the video after that, this will lead to discussion of how to choose a good learning rate for gradient descent.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.6000000000000005, "text": " Let's look at how you can implement feature scaling to take features that take on very", "tokens": [50364, 961, 311, 574, 412, 577, 291, 393, 4445, 4111, 21589, 281, 747, 4122, 300, 747, 322, 588, 50694, 50694, 819, 22526, 295, 4190, 293, 4373, 552, 281, 362, 25323, 22526, 295, 2158, 281, 1184, 50928, 50928, 661, 13, 50978, 50978, 407, 577, 360, 291, 767, 4373, 4122, 30, 51092, 51092, 1042, 11, 498, 1783, 16, 22526, 490, 805, 281, 8132, 11, 472, 636, 281, 483, 257, 4373, 3037, 295, 1783, 16, 307, 281, 747, 1184, 51502, 51502, 3380, 1783, 16, 2158, 293, 9845, 538, 8132, 264, 6674, 295, 264, 3613, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.13554289968390215, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.006288329139351845}, {"id": 1, "seek": 0, "start": 6.6000000000000005, "end": 11.28, "text": " different ranges of values and scale them to have comparable ranges of value to each", "tokens": [50364, 961, 311, 574, 412, 577, 291, 393, 4445, 4111, 21589, 281, 747, 4122, 300, 747, 322, 588, 50694, 50694, 819, 22526, 295, 4190, 293, 4373, 552, 281, 362, 25323, 22526, 295, 2158, 281, 1184, 50928, 50928, 661, 13, 50978, 50978, 407, 577, 360, 291, 767, 4373, 4122, 30, 51092, 51092, 1042, 11, 498, 1783, 16, 22526, 490, 805, 281, 8132, 11, 472, 636, 281, 483, 257, 4373, 3037, 295, 1783, 16, 307, 281, 747, 1184, 51502, 51502, 3380, 1783, 16, 2158, 293, 9845, 538, 8132, 264, 6674, 295, 264, 3613, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.13554289968390215, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.006288329139351845}, {"id": 2, "seek": 0, "start": 11.28, "end": 12.280000000000001, "text": " other.", "tokens": [50364, 961, 311, 574, 412, 577, 291, 393, 4445, 4111, 21589, 281, 747, 4122, 300, 747, 322, 588, 50694, 50694, 819, 22526, 295, 4190, 293, 4373, 552, 281, 362, 25323, 22526, 295, 2158, 281, 1184, 50928, 50928, 661, 13, 50978, 50978, 407, 577, 360, 291, 767, 4373, 4122, 30, 51092, 51092, 1042, 11, 498, 1783, 16, 22526, 490, 805, 281, 8132, 11, 472, 636, 281, 483, 257, 4373, 3037, 295, 1783, 16, 307, 281, 747, 1184, 51502, 51502, 3380, 1783, 16, 2158, 293, 9845, 538, 8132, 264, 6674, 295, 264, 3613, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.13554289968390215, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.006288329139351845}, {"id": 3, "seek": 0, "start": 12.280000000000001, "end": 14.56, "text": " So how do you actually scale features?", "tokens": [50364, 961, 311, 574, 412, 577, 291, 393, 4445, 4111, 21589, 281, 747, 4122, 300, 747, 322, 588, 50694, 50694, 819, 22526, 295, 4190, 293, 4373, 552, 281, 362, 25323, 22526, 295, 2158, 281, 1184, 50928, 50928, 661, 13, 50978, 50978, 407, 577, 360, 291, 767, 4373, 4122, 30, 51092, 51092, 1042, 11, 498, 1783, 16, 22526, 490, 805, 281, 8132, 11, 472, 636, 281, 483, 257, 4373, 3037, 295, 1783, 16, 307, 281, 747, 1184, 51502, 51502, 3380, 1783, 16, 2158, 293, 9845, 538, 8132, 264, 6674, 295, 264, 3613, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.13554289968390215, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.006288329139351845}, {"id": 4, "seek": 0, "start": 14.56, "end": 22.76, "text": " Well, if X1 ranges from 3 to 2000, one way to get a scale version of X1 is to take each", "tokens": [50364, 961, 311, 574, 412, 577, 291, 393, 4445, 4111, 21589, 281, 747, 4122, 300, 747, 322, 588, 50694, 50694, 819, 22526, 295, 4190, 293, 4373, 552, 281, 362, 25323, 22526, 295, 2158, 281, 1184, 50928, 50928, 661, 13, 50978, 50978, 407, 577, 360, 291, 767, 4373, 4122, 30, 51092, 51092, 1042, 11, 498, 1783, 16, 22526, 490, 805, 281, 8132, 11, 472, 636, 281, 483, 257, 4373, 3037, 295, 1783, 16, 307, 281, 747, 1184, 51502, 51502, 3380, 1783, 16, 2158, 293, 9845, 538, 8132, 264, 6674, 295, 264, 3613, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.13554289968390215, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.006288329139351845}, {"id": 5, "seek": 0, "start": 22.76, "end": 28.92, "text": " original X1 value and divide by 2000 the maximum of the range.", "tokens": [50364, 961, 311, 574, 412, 577, 291, 393, 4445, 4111, 21589, 281, 747, 4122, 300, 747, 322, 588, 50694, 50694, 819, 22526, 295, 4190, 293, 4373, 552, 281, 362, 25323, 22526, 295, 2158, 281, 1184, 50928, 50928, 661, 13, 50978, 50978, 407, 577, 360, 291, 767, 4373, 4122, 30, 51092, 51092, 1042, 11, 498, 1783, 16, 22526, 490, 805, 281, 8132, 11, 472, 636, 281, 483, 257, 4373, 3037, 295, 1783, 16, 307, 281, 747, 1184, 51502, 51502, 3380, 1783, 16, 2158, 293, 9845, 538, 8132, 264, 6674, 295, 264, 3613, 13, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.13554289968390215, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.006288329139351845}, {"id": 6, "seek": 2892, "start": 28.92, "end": 34.440000000000005, "text": " So the scale X1 will range from 0.15 up to 1.", "tokens": [50364, 407, 264, 4373, 1783, 16, 486, 3613, 490, 1958, 13, 5211, 493, 281, 502, 13, 50640, 50640, 13157, 11, 1670, 1783, 17, 22526, 490, 1958, 281, 1025, 11, 291, 393, 8873, 257, 4373, 3037, 295, 1783, 17, 538, 1940, 51006, 51006, 1184, 3380, 1783, 17, 293, 26764, 538, 1025, 11, 597, 307, 797, 264, 6674, 13, 51278, 51278, 407, 264, 4373, 1783, 17, 486, 586, 3613, 490, 1958, 281, 502, 13, 51546, 51546], "temperature": 0.0, "avg_logprob": -0.10382996241251628, "compression_ratio": 1.528301886792453, "no_speech_prob": 1.9333260752318893e-06}, {"id": 7, "seek": 2892, "start": 34.440000000000005, "end": 41.760000000000005, "text": " Similarly, since X2 ranges from 0 to 5, you can calculate a scale version of X2 by taking", "tokens": [50364, 407, 264, 4373, 1783, 16, 486, 3613, 490, 1958, 13, 5211, 493, 281, 502, 13, 50640, 50640, 13157, 11, 1670, 1783, 17, 22526, 490, 1958, 281, 1025, 11, 291, 393, 8873, 257, 4373, 3037, 295, 1783, 17, 538, 1940, 51006, 51006, 1184, 3380, 1783, 17, 293, 26764, 538, 1025, 11, 597, 307, 797, 264, 6674, 13, 51278, 51278, 407, 264, 4373, 1783, 17, 486, 586, 3613, 490, 1958, 281, 502, 13, 51546, 51546], "temperature": 0.0, "avg_logprob": -0.10382996241251628, "compression_ratio": 1.528301886792453, "no_speech_prob": 1.9333260752318893e-06}, {"id": 8, "seek": 2892, "start": 41.760000000000005, "end": 47.2, "text": " each original X2 and dividing by 5, which is again the maximum.", "tokens": [50364, 407, 264, 4373, 1783, 16, 486, 3613, 490, 1958, 13, 5211, 493, 281, 502, 13, 50640, 50640, 13157, 11, 1670, 1783, 17, 22526, 490, 1958, 281, 1025, 11, 291, 393, 8873, 257, 4373, 3037, 295, 1783, 17, 538, 1940, 51006, 51006, 1184, 3380, 1783, 17, 293, 26764, 538, 1025, 11, 597, 307, 797, 264, 6674, 13, 51278, 51278, 407, 264, 4373, 1783, 17, 486, 586, 3613, 490, 1958, 281, 502, 13, 51546, 51546], "temperature": 0.0, "avg_logprob": -0.10382996241251628, "compression_ratio": 1.528301886792453, "no_speech_prob": 1.9333260752318893e-06}, {"id": 9, "seek": 2892, "start": 47.2, "end": 52.56, "text": " So the scale X2 will now range from 0 to 1.", "tokens": [50364, 407, 264, 4373, 1783, 16, 486, 3613, 490, 1958, 13, 5211, 493, 281, 502, 13, 50640, 50640, 13157, 11, 1670, 1783, 17, 22526, 490, 1958, 281, 1025, 11, 291, 393, 8873, 257, 4373, 3037, 295, 1783, 17, 538, 1940, 51006, 51006, 1184, 3380, 1783, 17, 293, 26764, 538, 1025, 11, 597, 307, 797, 264, 6674, 13, 51278, 51278, 407, 264, 4373, 1783, 17, 486, 586, 3613, 490, 1958, 281, 502, 13, 51546, 51546], "temperature": 0.0, "avg_logprob": -0.10382996241251628, "compression_ratio": 1.528301886792453, "no_speech_prob": 1.9333260752318893e-06}, {"id": 10, "seek": 5256, "start": 52.56, "end": 59.0, "text": " So if you plot the scaled X1 and X2 on a graph, it might look like this.", "tokens": [50364, 407, 498, 291, 7542, 264, 36039, 1783, 16, 293, 1783, 17, 322, 257, 4295, 11, 309, 1062, 574, 411, 341, 13, 50686, 50686, 682, 4500, 281, 26764, 538, 264, 6674, 11, 291, 393, 611, 360, 437, 311, 1219, 914, 2710, 2144, 13, 50992, 50992, 407, 437, 341, 1542, 411, 307, 11, 291, 722, 365, 264, 3380, 4122, 293, 550, 291, 9610, 1220, 51222, 51222, 552, 370, 300, 1293, 295, 552, 366, 18988, 926, 1958, 13, 51412, 51412, 407, 9735, 949, 436, 787, 632, 4190, 5044, 813, 1958, 11, 586, 436, 362, 1293, 3671, 293, 3353, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.07550089768689087, "compression_ratio": 1.6307053941908713, "no_speech_prob": 4.092790277354652e-06}, {"id": 11, "seek": 5256, "start": 59.0, "end": 65.12, "text": " In addition to dividing by the maximum, you can also do what's called mean normalization.", "tokens": [50364, 407, 498, 291, 7542, 264, 36039, 1783, 16, 293, 1783, 17, 322, 257, 4295, 11, 309, 1062, 574, 411, 341, 13, 50686, 50686, 682, 4500, 281, 26764, 538, 264, 6674, 11, 291, 393, 611, 360, 437, 311, 1219, 914, 2710, 2144, 13, 50992, 50992, 407, 437, 341, 1542, 411, 307, 11, 291, 722, 365, 264, 3380, 4122, 293, 550, 291, 9610, 1220, 51222, 51222, 552, 370, 300, 1293, 295, 552, 366, 18988, 926, 1958, 13, 51412, 51412, 407, 9735, 949, 436, 787, 632, 4190, 5044, 813, 1958, 11, 586, 436, 362, 1293, 3671, 293, 3353, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.07550089768689087, "compression_ratio": 1.6307053941908713, "no_speech_prob": 4.092790277354652e-06}, {"id": 12, "seek": 5256, "start": 65.12, "end": 69.72, "text": " So what this looks like is, you start with the original features and then you rescale", "tokens": [50364, 407, 498, 291, 7542, 264, 36039, 1783, 16, 293, 1783, 17, 322, 257, 4295, 11, 309, 1062, 574, 411, 341, 13, 50686, 50686, 682, 4500, 281, 26764, 538, 264, 6674, 11, 291, 393, 611, 360, 437, 311, 1219, 914, 2710, 2144, 13, 50992, 50992, 407, 437, 341, 1542, 411, 307, 11, 291, 722, 365, 264, 3380, 4122, 293, 550, 291, 9610, 1220, 51222, 51222, 552, 370, 300, 1293, 295, 552, 366, 18988, 926, 1958, 13, 51412, 51412, 407, 9735, 949, 436, 787, 632, 4190, 5044, 813, 1958, 11, 586, 436, 362, 1293, 3671, 293, 3353, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.07550089768689087, "compression_ratio": 1.6307053941908713, "no_speech_prob": 4.092790277354652e-06}, {"id": 13, "seek": 5256, "start": 69.72, "end": 73.52000000000001, "text": " them so that both of them are centered around 0.", "tokens": [50364, 407, 498, 291, 7542, 264, 36039, 1783, 16, 293, 1783, 17, 322, 257, 4295, 11, 309, 1062, 574, 411, 341, 13, 50686, 50686, 682, 4500, 281, 26764, 538, 264, 6674, 11, 291, 393, 611, 360, 437, 311, 1219, 914, 2710, 2144, 13, 50992, 50992, 407, 437, 341, 1542, 411, 307, 11, 291, 722, 365, 264, 3380, 4122, 293, 550, 291, 9610, 1220, 51222, 51222, 552, 370, 300, 1293, 295, 552, 366, 18988, 926, 1958, 13, 51412, 51412, 407, 9735, 949, 436, 787, 632, 4190, 5044, 813, 1958, 11, 586, 436, 362, 1293, 3671, 293, 3353, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.07550089768689087, "compression_ratio": 1.6307053941908713, "no_speech_prob": 4.092790277354652e-06}, {"id": 14, "seek": 5256, "start": 73.52000000000001, "end": 79.24000000000001, "text": " So whereas before they only had values greater than 0, now they have both negative and positive", "tokens": [50364, 407, 498, 291, 7542, 264, 36039, 1783, 16, 293, 1783, 17, 322, 257, 4295, 11, 309, 1062, 574, 411, 341, 13, 50686, 50686, 682, 4500, 281, 26764, 538, 264, 6674, 11, 291, 393, 611, 360, 437, 311, 1219, 914, 2710, 2144, 13, 50992, 50992, 407, 437, 341, 1542, 411, 307, 11, 291, 722, 365, 264, 3380, 4122, 293, 550, 291, 9610, 1220, 51222, 51222, 552, 370, 300, 1293, 295, 552, 366, 18988, 926, 1958, 13, 51412, 51412, 407, 9735, 949, 436, 787, 632, 4190, 5044, 813, 1958, 11, 586, 436, 362, 1293, 3671, 293, 3353, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.07550089768689087, "compression_ratio": 1.6307053941908713, "no_speech_prob": 4.092790277354652e-06}, {"id": 15, "seek": 7924, "start": 79.24, "end": 85.67999999999999, "text": " values, but maybe usually between negative 1 and plus 1.", "tokens": [50364, 4190, 11, 457, 1310, 2673, 1296, 3671, 502, 293, 1804, 502, 13, 50686, 50686, 407, 281, 8873, 264, 914, 2710, 2144, 295, 1783, 16, 11, 700, 915, 264, 4274, 11, 611, 1219, 264, 50952, 50952, 914, 295, 1783, 16, 322, 428, 3097, 992, 11, 293, 718, 311, 818, 341, 914, 2992, 16, 11, 365, 341, 885, 264, 10281, 51302, 51302, 23339, 2992, 13, 51392, 51392, 1171, 1365, 11, 291, 815, 915, 300, 264, 4274, 295, 4111, 502, 11, 2992, 16, 11, 307, 11849, 3732, 3521, 13, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.08695514467027453, "compression_ratio": 1.5450236966824644, "no_speech_prob": 4.710852863354376e-06}, {"id": 16, "seek": 7924, "start": 85.67999999999999, "end": 91.0, "text": " So to calculate the mean normalization of X1, first find the average, also called the", "tokens": [50364, 4190, 11, 457, 1310, 2673, 1296, 3671, 502, 293, 1804, 502, 13, 50686, 50686, 407, 281, 8873, 264, 914, 2710, 2144, 295, 1783, 16, 11, 700, 915, 264, 4274, 11, 611, 1219, 264, 50952, 50952, 914, 295, 1783, 16, 322, 428, 3097, 992, 11, 293, 718, 311, 818, 341, 914, 2992, 16, 11, 365, 341, 885, 264, 10281, 51302, 51302, 23339, 2992, 13, 51392, 51392, 1171, 1365, 11, 291, 815, 915, 300, 264, 4274, 295, 4111, 502, 11, 2992, 16, 11, 307, 11849, 3732, 3521, 13, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.08695514467027453, "compression_ratio": 1.5450236966824644, "no_speech_prob": 4.710852863354376e-06}, {"id": 17, "seek": 7924, "start": 91.0, "end": 98.0, "text": " mean of X1 on your training set, and let's call this mean mu1, with this being the Greek", "tokens": [50364, 4190, 11, 457, 1310, 2673, 1296, 3671, 502, 293, 1804, 502, 13, 50686, 50686, 407, 281, 8873, 264, 914, 2710, 2144, 295, 1783, 16, 11, 700, 915, 264, 4274, 11, 611, 1219, 264, 50952, 50952, 914, 295, 1783, 16, 322, 428, 3097, 992, 11, 293, 718, 311, 818, 341, 914, 2992, 16, 11, 365, 341, 885, 264, 10281, 51302, 51302, 23339, 2992, 13, 51392, 51392, 1171, 1365, 11, 291, 815, 915, 300, 264, 4274, 295, 4111, 502, 11, 2992, 16, 11, 307, 11849, 3732, 3521, 13, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.08695514467027453, "compression_ratio": 1.5450236966824644, "no_speech_prob": 4.710852863354376e-06}, {"id": 18, "seek": 7924, "start": 98.0, "end": 99.8, "text": " alphabet mu.", "tokens": [50364, 4190, 11, 457, 1310, 2673, 1296, 3671, 502, 293, 1804, 502, 13, 50686, 50686, 407, 281, 8873, 264, 914, 2710, 2144, 295, 1783, 16, 11, 700, 915, 264, 4274, 11, 611, 1219, 264, 50952, 50952, 914, 295, 1783, 16, 322, 428, 3097, 992, 11, 293, 718, 311, 818, 341, 914, 2992, 16, 11, 365, 341, 885, 264, 10281, 51302, 51302, 23339, 2992, 13, 51392, 51392, 1171, 1365, 11, 291, 815, 915, 300, 264, 4274, 295, 4111, 502, 11, 2992, 16, 11, 307, 11849, 3732, 3521, 13, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.08695514467027453, "compression_ratio": 1.5450236966824644, "no_speech_prob": 4.710852863354376e-06}, {"id": 19, "seek": 7924, "start": 99.8, "end": 106.74, "text": " For example, you may find that the average of feature 1, mu1, is 600 square feet.", "tokens": [50364, 4190, 11, 457, 1310, 2673, 1296, 3671, 502, 293, 1804, 502, 13, 50686, 50686, 407, 281, 8873, 264, 914, 2710, 2144, 295, 1783, 16, 11, 700, 915, 264, 4274, 11, 611, 1219, 264, 50952, 50952, 914, 295, 1783, 16, 322, 428, 3097, 992, 11, 293, 718, 311, 818, 341, 914, 2992, 16, 11, 365, 341, 885, 264, 10281, 51302, 51302, 23339, 2992, 13, 51392, 51392, 1171, 1365, 11, 291, 815, 915, 300, 264, 4274, 295, 4111, 502, 11, 2992, 16, 11, 307, 11849, 3732, 3521, 13, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.08695514467027453, "compression_ratio": 1.5450236966824644, "no_speech_prob": 4.710852863354376e-06}, {"id": 20, "seek": 10674, "start": 106.74, "end": 115.24, "text": " So let's take each X1, subtract the mean mu1, and then let's divide by the difference 2000", "tokens": [50364, 407, 718, 311, 747, 1184, 1783, 16, 11, 16390, 264, 914, 2992, 16, 11, 293, 550, 718, 311, 9845, 538, 264, 2649, 8132, 50789, 50789, 3175, 6641, 11, 689, 8132, 307, 264, 6674, 293, 6641, 264, 7285, 13, 51131, 51131, 400, 498, 291, 360, 341, 11, 291, 483, 264, 48704, 1783, 16, 281, 3613, 490, 3671, 1958, 13, 6494, 281, 1958, 13, 32848, 13, 51597, 51597], "temperature": 0.0, "avg_logprob": -0.13858981693492217, "compression_ratio": 1.4171779141104295, "no_speech_prob": 2.1907524114794796e-06}, {"id": 21, "seek": 10674, "start": 115.24, "end": 122.08, "text": " minus 300, where 2000 is the maximum and 300 the minimum.", "tokens": [50364, 407, 718, 311, 747, 1184, 1783, 16, 11, 16390, 264, 914, 2992, 16, 11, 293, 550, 718, 311, 9845, 538, 264, 2649, 8132, 50789, 50789, 3175, 6641, 11, 689, 8132, 307, 264, 6674, 293, 6641, 264, 7285, 13, 51131, 51131, 400, 498, 291, 360, 341, 11, 291, 483, 264, 48704, 1783, 16, 281, 3613, 490, 3671, 1958, 13, 6494, 281, 1958, 13, 32848, 13, 51597, 51597], "temperature": 0.0, "avg_logprob": -0.13858981693492217, "compression_ratio": 1.4171779141104295, "no_speech_prob": 2.1907524114794796e-06}, {"id": 22, "seek": 10674, "start": 122.08, "end": 131.4, "text": " And if you do this, you get the normalized X1 to range from negative 0.18 to 0.82.", "tokens": [50364, 407, 718, 311, 747, 1184, 1783, 16, 11, 16390, 264, 914, 2992, 16, 11, 293, 550, 718, 311, 9845, 538, 264, 2649, 8132, 50789, 50789, 3175, 6641, 11, 689, 8132, 307, 264, 6674, 293, 6641, 264, 7285, 13, 51131, 51131, 400, 498, 291, 360, 341, 11, 291, 483, 264, 48704, 1783, 16, 281, 3613, 490, 3671, 1958, 13, 6494, 281, 1958, 13, 32848, 13, 51597, 51597], "temperature": 0.0, "avg_logprob": -0.13858981693492217, "compression_ratio": 1.4171779141104295, "no_speech_prob": 2.1907524114794796e-06}, {"id": 23, "seek": 13140, "start": 131.4, "end": 137.96, "text": " Similarly to mean normalize X2, you can calculate the average of feature 2, and for instance", "tokens": [50364, 13157, 281, 914, 2710, 1125, 1783, 17, 11, 291, 393, 8873, 264, 4274, 295, 4111, 568, 11, 293, 337, 5197, 50692, 50692, 2992, 17, 815, 312, 568, 13, 18, 13, 50850, 50850, 1396, 291, 393, 747, 1184, 1783, 17, 11, 16390, 2992, 17, 11, 293, 9845, 538, 1025, 3175, 1958, 11, 797, 264, 11469, 1025, 3175, 51314, 51314, 264, 923, 597, 307, 1958, 13, 51444, 51444], "temperature": 0.0, "avg_logprob": -0.1379065092872171, "compression_ratio": 1.411764705882353, "no_speech_prob": 1.873867518042971e-06}, {"id": 24, "seek": 13140, "start": 137.96, "end": 141.12, "text": " mu2 may be 2.3.", "tokens": [50364, 13157, 281, 914, 2710, 1125, 1783, 17, 11, 291, 393, 8873, 264, 4274, 295, 4111, 568, 11, 293, 337, 5197, 50692, 50692, 2992, 17, 815, 312, 568, 13, 18, 13, 50850, 50850, 1396, 291, 393, 747, 1184, 1783, 17, 11, 16390, 2992, 17, 11, 293, 9845, 538, 1025, 3175, 1958, 11, 797, 264, 11469, 1025, 3175, 51314, 51314, 264, 923, 597, 307, 1958, 13, 51444, 51444], "temperature": 0.0, "avg_logprob": -0.1379065092872171, "compression_ratio": 1.411764705882353, "no_speech_prob": 1.873867518042971e-06}, {"id": 25, "seek": 13140, "start": 141.12, "end": 150.4, "text": " Then you can take each X2, subtract mu2, and divide by 5 minus 0, again the max 5 minus", "tokens": [50364, 13157, 281, 914, 2710, 1125, 1783, 17, 11, 291, 393, 8873, 264, 4274, 295, 4111, 568, 11, 293, 337, 5197, 50692, 50692, 2992, 17, 815, 312, 568, 13, 18, 13, 50850, 50850, 1396, 291, 393, 747, 1184, 1783, 17, 11, 16390, 2992, 17, 11, 293, 9845, 538, 1025, 3175, 1958, 11, 797, 264, 11469, 1025, 3175, 51314, 51314, 264, 923, 597, 307, 1958, 13, 51444, 51444], "temperature": 0.0, "avg_logprob": -0.1379065092872171, "compression_ratio": 1.411764705882353, "no_speech_prob": 1.873867518042971e-06}, {"id": 26, "seek": 13140, "start": 150.4, "end": 153.0, "text": " the min which is 0.", "tokens": [50364, 13157, 281, 914, 2710, 1125, 1783, 17, 11, 291, 393, 8873, 264, 4274, 295, 4111, 568, 11, 293, 337, 5197, 50692, 50692, 2992, 17, 815, 312, 568, 13, 18, 13, 50850, 50850, 1396, 291, 393, 747, 1184, 1783, 17, 11, 16390, 2992, 17, 11, 293, 9845, 538, 1025, 3175, 1958, 11, 797, 264, 11469, 1025, 3175, 51314, 51314, 264, 923, 597, 307, 1958, 13, 51444, 51444], "temperature": 0.0, "avg_logprob": -0.1379065092872171, "compression_ratio": 1.411764705882353, "no_speech_prob": 1.873867518042971e-06}, {"id": 27, "seek": 15300, "start": 153.0, "end": 162.76, "text": " The mean normalized X2 now ranges from negative 0.46 to 0.54, so if you plot the training", "tokens": [50364, 440, 914, 48704, 1783, 17, 586, 22526, 490, 3671, 1958, 13, 16169, 281, 1958, 13, 19563, 11, 370, 498, 291, 7542, 264, 3097, 50852, 50852, 1412, 1228, 264, 914, 48704, 1783, 16, 293, 1783, 17, 11, 309, 1062, 574, 411, 341, 13, 51168, 51168, 821, 311, 472, 1036, 2689, 9610, 4270, 3170, 1219, 710, 12, 4417, 418, 2710, 2144, 13, 51440, 51440, 1407, 4445, 710, 12, 4417, 418, 2710, 2144, 11, 291, 643, 281, 8873, 746, 1219, 264, 3832, 51654, 51654, 25163, 295, 1184, 4111, 13, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.09190404680040147, "compression_ratio": 1.6208530805687205, "no_speech_prob": 1.844812345552782e-06}, {"id": 28, "seek": 15300, "start": 162.76, "end": 169.08, "text": " data using the mean normalized X1 and X2, it might look like this.", "tokens": [50364, 440, 914, 48704, 1783, 17, 586, 22526, 490, 3671, 1958, 13, 16169, 281, 1958, 13, 19563, 11, 370, 498, 291, 7542, 264, 3097, 50852, 50852, 1412, 1228, 264, 914, 48704, 1783, 16, 293, 1783, 17, 11, 309, 1062, 574, 411, 341, 13, 51168, 51168, 821, 311, 472, 1036, 2689, 9610, 4270, 3170, 1219, 710, 12, 4417, 418, 2710, 2144, 13, 51440, 51440, 1407, 4445, 710, 12, 4417, 418, 2710, 2144, 11, 291, 643, 281, 8873, 746, 1219, 264, 3832, 51654, 51654, 25163, 295, 1184, 4111, 13, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.09190404680040147, "compression_ratio": 1.6208530805687205, "no_speech_prob": 1.844812345552782e-06}, {"id": 29, "seek": 15300, "start": 169.08, "end": 174.52, "text": " There's one last common rescaling method called z-score normalization.", "tokens": [50364, 440, 914, 48704, 1783, 17, 586, 22526, 490, 3671, 1958, 13, 16169, 281, 1958, 13, 19563, 11, 370, 498, 291, 7542, 264, 3097, 50852, 50852, 1412, 1228, 264, 914, 48704, 1783, 16, 293, 1783, 17, 11, 309, 1062, 574, 411, 341, 13, 51168, 51168, 821, 311, 472, 1036, 2689, 9610, 4270, 3170, 1219, 710, 12, 4417, 418, 2710, 2144, 13, 51440, 51440, 1407, 4445, 710, 12, 4417, 418, 2710, 2144, 11, 291, 643, 281, 8873, 746, 1219, 264, 3832, 51654, 51654, 25163, 295, 1184, 4111, 13, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.09190404680040147, "compression_ratio": 1.6208530805687205, "no_speech_prob": 1.844812345552782e-06}, {"id": 30, "seek": 15300, "start": 174.52, "end": 178.8, "text": " To implement z-score normalization, you need to calculate something called the standard", "tokens": [50364, 440, 914, 48704, 1783, 17, 586, 22526, 490, 3671, 1958, 13, 16169, 281, 1958, 13, 19563, 11, 370, 498, 291, 7542, 264, 3097, 50852, 50852, 1412, 1228, 264, 914, 48704, 1783, 16, 293, 1783, 17, 11, 309, 1062, 574, 411, 341, 13, 51168, 51168, 821, 311, 472, 1036, 2689, 9610, 4270, 3170, 1219, 710, 12, 4417, 418, 2710, 2144, 13, 51440, 51440, 1407, 4445, 710, 12, 4417, 418, 2710, 2144, 11, 291, 643, 281, 8873, 746, 1219, 264, 3832, 51654, 51654, 25163, 295, 1184, 4111, 13, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.09190404680040147, "compression_ratio": 1.6208530805687205, "no_speech_prob": 1.844812345552782e-06}, {"id": 31, "seek": 15300, "start": 178.8, "end": 181.04, "text": " deviation of each feature.", "tokens": [50364, 440, 914, 48704, 1783, 17, 586, 22526, 490, 3671, 1958, 13, 16169, 281, 1958, 13, 19563, 11, 370, 498, 291, 7542, 264, 3097, 50852, 50852, 1412, 1228, 264, 914, 48704, 1783, 16, 293, 1783, 17, 11, 309, 1062, 574, 411, 341, 13, 51168, 51168, 821, 311, 472, 1036, 2689, 9610, 4270, 3170, 1219, 710, 12, 4417, 418, 2710, 2144, 13, 51440, 51440, 1407, 4445, 710, 12, 4417, 418, 2710, 2144, 11, 291, 643, 281, 8873, 746, 1219, 264, 3832, 51654, 51654, 25163, 295, 1184, 4111, 13, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.09190404680040147, "compression_ratio": 1.6208530805687205, "no_speech_prob": 1.844812345552782e-06}, {"id": 32, "seek": 18104, "start": 181.04, "end": 184.6, "text": " If you don't know what a standard deviation is, don't worry about it, you won't need to", "tokens": [50364, 759, 291, 500, 380, 458, 437, 257, 3832, 25163, 307, 11, 500, 380, 3292, 466, 309, 11, 291, 1582, 380, 643, 281, 50542, 50542, 458, 309, 337, 341, 1508, 13, 50644, 50644, 1610, 498, 291, 600, 2198, 295, 264, 2710, 7316, 420, 264, 4549, 12, 23103, 7605, 11, 2171, 611, 1219, 50868, 50868, 264, 39148, 7316, 11, 341, 307, 437, 264, 3832, 25163, 337, 264, 2710, 7316, 51148, 51148, 1542, 411, 13, 51198, 51198, 583, 498, 291, 2378, 380, 2198, 295, 341, 11, 291, 500, 380, 643, 281, 3292, 466, 300, 2139, 13, 51376, 51376, 583, 498, 291, 360, 458, 437, 307, 264, 3832, 25163, 11, 550, 281, 4445, 257, 710, 12, 4417, 418, 2710, 2144, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09471727008661948, "compression_ratio": 1.987551867219917, "no_speech_prob": 6.240842594706919e-06}, {"id": 33, "seek": 18104, "start": 184.6, "end": 186.64, "text": " know it for this class.", "tokens": [50364, 759, 291, 500, 380, 458, 437, 257, 3832, 25163, 307, 11, 500, 380, 3292, 466, 309, 11, 291, 1582, 380, 643, 281, 50542, 50542, 458, 309, 337, 341, 1508, 13, 50644, 50644, 1610, 498, 291, 600, 2198, 295, 264, 2710, 7316, 420, 264, 4549, 12, 23103, 7605, 11, 2171, 611, 1219, 50868, 50868, 264, 39148, 7316, 11, 341, 307, 437, 264, 3832, 25163, 337, 264, 2710, 7316, 51148, 51148, 1542, 411, 13, 51198, 51198, 583, 498, 291, 2378, 380, 2198, 295, 341, 11, 291, 500, 380, 643, 281, 3292, 466, 300, 2139, 13, 51376, 51376, 583, 498, 291, 360, 458, 437, 307, 264, 3832, 25163, 11, 550, 281, 4445, 257, 710, 12, 4417, 418, 2710, 2144, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09471727008661948, "compression_ratio": 1.987551867219917, "no_speech_prob": 6.240842594706919e-06}, {"id": 34, "seek": 18104, "start": 186.64, "end": 191.12, "text": " Or if you've heard of the normal distribution or the bell-shaped curve, sometimes also called", "tokens": [50364, 759, 291, 500, 380, 458, 437, 257, 3832, 25163, 307, 11, 500, 380, 3292, 466, 309, 11, 291, 1582, 380, 643, 281, 50542, 50542, 458, 309, 337, 341, 1508, 13, 50644, 50644, 1610, 498, 291, 600, 2198, 295, 264, 2710, 7316, 420, 264, 4549, 12, 23103, 7605, 11, 2171, 611, 1219, 50868, 50868, 264, 39148, 7316, 11, 341, 307, 437, 264, 3832, 25163, 337, 264, 2710, 7316, 51148, 51148, 1542, 411, 13, 51198, 51198, 583, 498, 291, 2378, 380, 2198, 295, 341, 11, 291, 500, 380, 643, 281, 3292, 466, 300, 2139, 13, 51376, 51376, 583, 498, 291, 360, 458, 437, 307, 264, 3832, 25163, 11, 550, 281, 4445, 257, 710, 12, 4417, 418, 2710, 2144, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09471727008661948, "compression_ratio": 1.987551867219917, "no_speech_prob": 6.240842594706919e-06}, {"id": 35, "seek": 18104, "start": 191.12, "end": 196.72, "text": " the Gaussian distribution, this is what the standard deviation for the normal distribution", "tokens": [50364, 759, 291, 500, 380, 458, 437, 257, 3832, 25163, 307, 11, 500, 380, 3292, 466, 309, 11, 291, 1582, 380, 643, 281, 50542, 50542, 458, 309, 337, 341, 1508, 13, 50644, 50644, 1610, 498, 291, 600, 2198, 295, 264, 2710, 7316, 420, 264, 4549, 12, 23103, 7605, 11, 2171, 611, 1219, 50868, 50868, 264, 39148, 7316, 11, 341, 307, 437, 264, 3832, 25163, 337, 264, 2710, 7316, 51148, 51148, 1542, 411, 13, 51198, 51198, 583, 498, 291, 2378, 380, 2198, 295, 341, 11, 291, 500, 380, 643, 281, 3292, 466, 300, 2139, 13, 51376, 51376, 583, 498, 291, 360, 458, 437, 307, 264, 3832, 25163, 11, 550, 281, 4445, 257, 710, 12, 4417, 418, 2710, 2144, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09471727008661948, "compression_ratio": 1.987551867219917, "no_speech_prob": 6.240842594706919e-06}, {"id": 36, "seek": 18104, "start": 196.72, "end": 197.72, "text": " looks like.", "tokens": [50364, 759, 291, 500, 380, 458, 437, 257, 3832, 25163, 307, 11, 500, 380, 3292, 466, 309, 11, 291, 1582, 380, 643, 281, 50542, 50542, 458, 309, 337, 341, 1508, 13, 50644, 50644, 1610, 498, 291, 600, 2198, 295, 264, 2710, 7316, 420, 264, 4549, 12, 23103, 7605, 11, 2171, 611, 1219, 50868, 50868, 264, 39148, 7316, 11, 341, 307, 437, 264, 3832, 25163, 337, 264, 2710, 7316, 51148, 51148, 1542, 411, 13, 51198, 51198, 583, 498, 291, 2378, 380, 2198, 295, 341, 11, 291, 500, 380, 643, 281, 3292, 466, 300, 2139, 13, 51376, 51376, 583, 498, 291, 360, 458, 437, 307, 264, 3832, 25163, 11, 550, 281, 4445, 257, 710, 12, 4417, 418, 2710, 2144, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09471727008661948, "compression_ratio": 1.987551867219917, "no_speech_prob": 6.240842594706919e-06}, {"id": 37, "seek": 18104, "start": 197.72, "end": 201.28, "text": " But if you haven't heard of this, you don't need to worry about that either.", "tokens": [50364, 759, 291, 500, 380, 458, 437, 257, 3832, 25163, 307, 11, 500, 380, 3292, 466, 309, 11, 291, 1582, 380, 643, 281, 50542, 50542, 458, 309, 337, 341, 1508, 13, 50644, 50644, 1610, 498, 291, 600, 2198, 295, 264, 2710, 7316, 420, 264, 4549, 12, 23103, 7605, 11, 2171, 611, 1219, 50868, 50868, 264, 39148, 7316, 11, 341, 307, 437, 264, 3832, 25163, 337, 264, 2710, 7316, 51148, 51148, 1542, 411, 13, 51198, 51198, 583, 498, 291, 2378, 380, 2198, 295, 341, 11, 291, 500, 380, 643, 281, 3292, 466, 300, 2139, 13, 51376, 51376, 583, 498, 291, 360, 458, 437, 307, 264, 3832, 25163, 11, 550, 281, 4445, 257, 710, 12, 4417, 418, 2710, 2144, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09471727008661948, "compression_ratio": 1.987551867219917, "no_speech_prob": 6.240842594706919e-06}, {"id": 38, "seek": 18104, "start": 201.28, "end": 207.04, "text": " But if you do know what is the standard deviation, then to implement a z-score normalization,", "tokens": [50364, 759, 291, 500, 380, 458, 437, 257, 3832, 25163, 307, 11, 500, 380, 3292, 466, 309, 11, 291, 1582, 380, 643, 281, 50542, 50542, 458, 309, 337, 341, 1508, 13, 50644, 50644, 1610, 498, 291, 600, 2198, 295, 264, 2710, 7316, 420, 264, 4549, 12, 23103, 7605, 11, 2171, 611, 1219, 50868, 50868, 264, 39148, 7316, 11, 341, 307, 437, 264, 3832, 25163, 337, 264, 2710, 7316, 51148, 51148, 1542, 411, 13, 51198, 51198, 583, 498, 291, 2378, 380, 2198, 295, 341, 11, 291, 500, 380, 643, 281, 3292, 466, 300, 2139, 13, 51376, 51376, 583, 498, 291, 360, 458, 437, 307, 264, 3832, 25163, 11, 550, 281, 4445, 257, 710, 12, 4417, 418, 2710, 2144, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09471727008661948, "compression_ratio": 1.987551867219917, "no_speech_prob": 6.240842594706919e-06}, {"id": 39, "seek": 20704, "start": 207.04, "end": 213.48, "text": " you first calculate the mean mu as well as the standard deviation, which is often denoted", "tokens": [50364, 291, 700, 8873, 264, 914, 2992, 382, 731, 382, 264, 3832, 25163, 11, 597, 307, 2049, 1441, 23325, 50686, 50686, 538, 264, 3126, 9765, 10281, 23339, 12771, 295, 1184, 4111, 13, 50946, 50946, 407, 337, 5197, 11, 1310, 4111, 502, 575, 257, 3832, 25163, 295, 26034, 293, 914, 11849, 11, 550, 281, 710, 12, 4417, 418, 51430, 51430], "temperature": 0.0, "avg_logprob": -0.09079878330230713, "compression_ratio": 1.4662576687116564, "no_speech_prob": 5.338091341400286e-06}, {"id": 40, "seek": 20704, "start": 213.48, "end": 218.68, "text": " by the lowercase Greek alphabet sigma of each feature.", "tokens": [50364, 291, 700, 8873, 264, 914, 2992, 382, 731, 382, 264, 3832, 25163, 11, 597, 307, 2049, 1441, 23325, 50686, 50686, 538, 264, 3126, 9765, 10281, 23339, 12771, 295, 1184, 4111, 13, 50946, 50946, 407, 337, 5197, 11, 1310, 4111, 502, 575, 257, 3832, 25163, 295, 26034, 293, 914, 11849, 11, 550, 281, 710, 12, 4417, 418, 51430, 51430], "temperature": 0.0, "avg_logprob": -0.09079878330230713, "compression_ratio": 1.4662576687116564, "no_speech_prob": 5.338091341400286e-06}, {"id": 41, "seek": 20704, "start": 218.68, "end": 228.35999999999999, "text": " So for instance, maybe feature 1 has a standard deviation of 450 and mean 600, then to z-score", "tokens": [50364, 291, 700, 8873, 264, 914, 2992, 382, 731, 382, 264, 3832, 25163, 11, 597, 307, 2049, 1441, 23325, 50686, 50686, 538, 264, 3126, 9765, 10281, 23339, 12771, 295, 1184, 4111, 13, 50946, 50946, 407, 337, 5197, 11, 1310, 4111, 502, 575, 257, 3832, 25163, 295, 26034, 293, 914, 11849, 11, 550, 281, 710, 12, 4417, 418, 51430, 51430], "temperature": 0.0, "avg_logprob": -0.09079878330230713, "compression_ratio": 1.4662576687116564, "no_speech_prob": 5.338091341400286e-06}, {"id": 42, "seek": 22836, "start": 228.36, "end": 236.96, "text": " normalize x1, take each x1, subtract mu 1, and then divide by the standard deviation,", "tokens": [50364, 2710, 1125, 2031, 16, 11, 747, 1184, 2031, 16, 11, 16390, 2992, 502, 11, 293, 550, 9845, 538, 264, 3832, 25163, 11, 50794, 50794, 597, 286, 478, 516, 281, 45708, 382, 12771, 502, 13, 50956, 50956, 400, 437, 291, 1062, 915, 307, 300, 264, 710, 12, 4417, 418, 2710, 1125, 2031, 16, 586, 22526, 490, 3671, 1958, 13, 22452, 51342, 51342, 281, 805, 13, 16, 13, 51492, 51492], "temperature": 0.0, "avg_logprob": -0.14952684129987445, "compression_ratio": 1.371069182389937, "no_speech_prob": 1.788051804396673e-06}, {"id": 43, "seek": 22836, "start": 236.96, "end": 240.20000000000002, "text": " which I'm going to denote as sigma 1.", "tokens": [50364, 2710, 1125, 2031, 16, 11, 747, 1184, 2031, 16, 11, 16390, 2992, 502, 11, 293, 550, 9845, 538, 264, 3832, 25163, 11, 50794, 50794, 597, 286, 478, 516, 281, 45708, 382, 12771, 502, 13, 50956, 50956, 400, 437, 291, 1062, 915, 307, 300, 264, 710, 12, 4417, 418, 2710, 1125, 2031, 16, 586, 22526, 490, 3671, 1958, 13, 22452, 51342, 51342, 281, 805, 13, 16, 13, 51492, 51492], "temperature": 0.0, "avg_logprob": -0.14952684129987445, "compression_ratio": 1.371069182389937, "no_speech_prob": 1.788051804396673e-06}, {"id": 44, "seek": 22836, "start": 240.20000000000002, "end": 247.92000000000002, "text": " And what you might find is that the z-score normalize x1 now ranges from negative 0.67", "tokens": [50364, 2710, 1125, 2031, 16, 11, 747, 1184, 2031, 16, 11, 16390, 2992, 502, 11, 293, 550, 9845, 538, 264, 3832, 25163, 11, 50794, 50794, 597, 286, 478, 516, 281, 45708, 382, 12771, 502, 13, 50956, 50956, 400, 437, 291, 1062, 915, 307, 300, 264, 710, 12, 4417, 418, 2710, 1125, 2031, 16, 586, 22526, 490, 3671, 1958, 13, 22452, 51342, 51342, 281, 805, 13, 16, 13, 51492, 51492], "temperature": 0.0, "avg_logprob": -0.14952684129987445, "compression_ratio": 1.371069182389937, "no_speech_prob": 1.788051804396673e-06}, {"id": 45, "seek": 22836, "start": 247.92000000000002, "end": 250.92000000000002, "text": " to 3.1.", "tokens": [50364, 2710, 1125, 2031, 16, 11, 747, 1184, 2031, 16, 11, 16390, 2992, 502, 11, 293, 550, 9845, 538, 264, 3832, 25163, 11, 50794, 50794, 597, 286, 478, 516, 281, 45708, 382, 12771, 502, 13, 50956, 50956, 400, 437, 291, 1062, 915, 307, 300, 264, 710, 12, 4417, 418, 2710, 1125, 2031, 16, 586, 22526, 490, 3671, 1958, 13, 22452, 51342, 51342, 281, 805, 13, 16, 13, 51492, 51492], "temperature": 0.0, "avg_logprob": -0.14952684129987445, "compression_ratio": 1.371069182389937, "no_speech_prob": 1.788051804396673e-06}, {"id": 46, "seek": 25092, "start": 250.92, "end": 258.52, "text": " Similarly, if you calculate the second feature's standard deviation to be 1.4 and mean to be", "tokens": [50364, 13157, 11, 498, 291, 8873, 264, 1150, 4111, 311, 3832, 25163, 281, 312, 502, 13, 19, 293, 914, 281, 312, 50744, 50744, 568, 13, 18, 11, 550, 291, 393, 14722, 2031, 17, 3175, 2992, 568, 6666, 538, 12771, 568, 13, 51126, 51126, 400, 294, 341, 1389, 11, 264, 710, 12, 4417, 418, 2710, 1125, 538, 2031, 17, 1062, 586, 3613, 490, 3671, 502, 13, 21, 281, 502, 13, 24, 13, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.10742504532272751, "compression_ratio": 1.3793103448275863, "no_speech_prob": 1.653676804380666e-06}, {"id": 47, "seek": 25092, "start": 258.52, "end": 266.15999999999997, "text": " 2.3, then you can compute x2 minus mu 2 divided by sigma 2.", "tokens": [50364, 13157, 11, 498, 291, 8873, 264, 1150, 4111, 311, 3832, 25163, 281, 312, 502, 13, 19, 293, 914, 281, 312, 50744, 50744, 568, 13, 18, 11, 550, 291, 393, 14722, 2031, 17, 3175, 2992, 568, 6666, 538, 12771, 568, 13, 51126, 51126, 400, 294, 341, 1389, 11, 264, 710, 12, 4417, 418, 2710, 1125, 538, 2031, 17, 1062, 586, 3613, 490, 3671, 502, 13, 21, 281, 502, 13, 24, 13, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.10742504532272751, "compression_ratio": 1.3793103448275863, "no_speech_prob": 1.653676804380666e-06}, {"id": 48, "seek": 25092, "start": 266.15999999999997, "end": 275.88, "text": " And in this case, the z-score normalize by x2 might now range from negative 1.6 to 1.9.", "tokens": [50364, 13157, 11, 498, 291, 8873, 264, 1150, 4111, 311, 3832, 25163, 281, 312, 502, 13, 19, 293, 914, 281, 312, 50744, 50744, 568, 13, 18, 11, 550, 291, 393, 14722, 2031, 17, 3175, 2992, 568, 6666, 538, 12771, 568, 13, 51126, 51126, 400, 294, 341, 1389, 11, 264, 710, 12, 4417, 418, 2710, 1125, 538, 2031, 17, 1062, 586, 3613, 490, 3671, 502, 13, 21, 281, 502, 13, 24, 13, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.10742504532272751, "compression_ratio": 1.3793103448275863, "no_speech_prob": 1.653676804380666e-06}, {"id": 49, "seek": 27588, "start": 275.88, "end": 283.84, "text": " So if you plot the training data on the normalized x1 and x2 on a graph, it might look like this.", "tokens": [50364, 407, 498, 291, 7542, 264, 3097, 1412, 322, 264, 48704, 2031, 16, 293, 2031, 17, 322, 257, 4295, 11, 309, 1062, 574, 411, 341, 13, 50762, 50762, 1018, 257, 4978, 295, 9298, 11, 562, 10205, 4111, 21589, 11, 291, 1062, 528, 281, 5939, 337, 1242, 51014, 51014, 264, 4122, 281, 3613, 490, 1310, 4992, 926, 3671, 502, 281, 4079, 926, 1804, 51320, 51320, 502, 337, 1184, 4111, 2031, 13, 51482, 51482, 583, 613, 4190, 11, 3671, 502, 293, 1804, 502, 393, 312, 257, 707, 857, 9612, 13, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.08641708290183937, "compression_ratio": 1.6272727272727272, "no_speech_prob": 9.276286618842278e-07}, {"id": 50, "seek": 27588, "start": 283.84, "end": 288.88, "text": " As a rule of thumb, when performing feature scaling, you might want to aim for getting", "tokens": [50364, 407, 498, 291, 7542, 264, 3097, 1412, 322, 264, 48704, 2031, 16, 293, 2031, 17, 322, 257, 4295, 11, 309, 1062, 574, 411, 341, 13, 50762, 50762, 1018, 257, 4978, 295, 9298, 11, 562, 10205, 4111, 21589, 11, 291, 1062, 528, 281, 5939, 337, 1242, 51014, 51014, 264, 4122, 281, 3613, 490, 1310, 4992, 926, 3671, 502, 281, 4079, 926, 1804, 51320, 51320, 502, 337, 1184, 4111, 2031, 13, 51482, 51482, 583, 613, 4190, 11, 3671, 502, 293, 1804, 502, 393, 312, 257, 707, 857, 9612, 13, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.08641708290183937, "compression_ratio": 1.6272727272727272, "no_speech_prob": 9.276286618842278e-07}, {"id": 51, "seek": 27588, "start": 288.88, "end": 295.0, "text": " the features to range from maybe anywhere around negative 1 to somewhere around plus", "tokens": [50364, 407, 498, 291, 7542, 264, 3097, 1412, 322, 264, 48704, 2031, 16, 293, 2031, 17, 322, 257, 4295, 11, 309, 1062, 574, 411, 341, 13, 50762, 50762, 1018, 257, 4978, 295, 9298, 11, 562, 10205, 4111, 21589, 11, 291, 1062, 528, 281, 5939, 337, 1242, 51014, 51014, 264, 4122, 281, 3613, 490, 1310, 4992, 926, 3671, 502, 281, 4079, 926, 1804, 51320, 51320, 502, 337, 1184, 4111, 2031, 13, 51482, 51482, 583, 613, 4190, 11, 3671, 502, 293, 1804, 502, 393, 312, 257, 707, 857, 9612, 13, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.08641708290183937, "compression_ratio": 1.6272727272727272, "no_speech_prob": 9.276286618842278e-07}, {"id": 52, "seek": 27588, "start": 295.0, "end": 298.24, "text": " 1 for each feature x.", "tokens": [50364, 407, 498, 291, 7542, 264, 3097, 1412, 322, 264, 48704, 2031, 16, 293, 2031, 17, 322, 257, 4295, 11, 309, 1062, 574, 411, 341, 13, 50762, 50762, 1018, 257, 4978, 295, 9298, 11, 562, 10205, 4111, 21589, 11, 291, 1062, 528, 281, 5939, 337, 1242, 51014, 51014, 264, 4122, 281, 3613, 490, 1310, 4992, 926, 3671, 502, 281, 4079, 926, 1804, 51320, 51320, 502, 337, 1184, 4111, 2031, 13, 51482, 51482, 583, 613, 4190, 11, 3671, 502, 293, 1804, 502, 393, 312, 257, 707, 857, 9612, 13, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.08641708290183937, "compression_ratio": 1.6272727272727272, "no_speech_prob": 9.276286618842278e-07}, {"id": 53, "seek": 27588, "start": 298.24, "end": 303.28, "text": " But these values, negative 1 and plus 1 can be a little bit loose.", "tokens": [50364, 407, 498, 291, 7542, 264, 3097, 1412, 322, 264, 48704, 2031, 16, 293, 2031, 17, 322, 257, 4295, 11, 309, 1062, 574, 411, 341, 13, 50762, 50762, 1018, 257, 4978, 295, 9298, 11, 562, 10205, 4111, 21589, 11, 291, 1062, 528, 281, 5939, 337, 1242, 51014, 51014, 264, 4122, 281, 3613, 490, 1310, 4992, 926, 3671, 502, 281, 4079, 926, 1804, 51320, 51320, 502, 337, 1184, 4111, 2031, 13, 51482, 51482, 583, 613, 4190, 11, 3671, 502, 293, 1804, 502, 393, 312, 257, 707, 857, 9612, 13, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.08641708290183937, "compression_ratio": 1.6272727272727272, "no_speech_prob": 9.276286618842278e-07}, {"id": 54, "seek": 30328, "start": 303.28, "end": 311.11999999999995, "text": " So if the features range from negative 3 to plus 3, or negative 0.3 to plus 0.3, all of", "tokens": [50364, 407, 498, 264, 4122, 3613, 490, 3671, 805, 281, 1804, 805, 11, 420, 3671, 1958, 13, 18, 281, 1804, 1958, 13, 18, 11, 439, 295, 50756, 50756, 613, 366, 2584, 1392, 13, 50852, 50852, 407, 498, 291, 362, 257, 4111, 2031, 16, 300, 17765, 493, 885, 1296, 1958, 293, 805, 11, 300, 311, 406, 257, 1154, 13, 51164, 51164, 400, 291, 393, 9610, 1220, 309, 498, 291, 528, 11, 457, 498, 291, 500, 380, 9610, 1220, 309, 11, 309, 820, 589, 1392, 51386, 51386, 886, 13, 51457, 51457, 1610, 498, 291, 362, 257, 819, 4111, 11, 2031, 17, 11, 6104, 4190, 366, 1296, 3671, 568, 293, 1804, 1958, 13, 20, 11, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.11170487611190133, "compression_ratio": 1.6858407079646018, "no_speech_prob": 1.5056870097396313e-06}, {"id": 55, "seek": 30328, "start": 311.11999999999995, "end": 313.03999999999996, "text": " these are completely okay.", "tokens": [50364, 407, 498, 264, 4122, 3613, 490, 3671, 805, 281, 1804, 805, 11, 420, 3671, 1958, 13, 18, 281, 1804, 1958, 13, 18, 11, 439, 295, 50756, 50756, 613, 366, 2584, 1392, 13, 50852, 50852, 407, 498, 291, 362, 257, 4111, 2031, 16, 300, 17765, 493, 885, 1296, 1958, 293, 805, 11, 300, 311, 406, 257, 1154, 13, 51164, 51164, 400, 291, 393, 9610, 1220, 309, 498, 291, 528, 11, 457, 498, 291, 500, 380, 9610, 1220, 309, 11, 309, 820, 589, 1392, 51386, 51386, 886, 13, 51457, 51457, 1610, 498, 291, 362, 257, 819, 4111, 11, 2031, 17, 11, 6104, 4190, 366, 1296, 3671, 568, 293, 1804, 1958, 13, 20, 11, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.11170487611190133, "compression_ratio": 1.6858407079646018, "no_speech_prob": 1.5056870097396313e-06}, {"id": 56, "seek": 30328, "start": 313.03999999999996, "end": 319.28, "text": " So if you have a feature x1 that winds up being between 0 and 3, that's not a problem.", "tokens": [50364, 407, 498, 264, 4122, 3613, 490, 3671, 805, 281, 1804, 805, 11, 420, 3671, 1958, 13, 18, 281, 1804, 1958, 13, 18, 11, 439, 295, 50756, 50756, 613, 366, 2584, 1392, 13, 50852, 50852, 407, 498, 291, 362, 257, 4111, 2031, 16, 300, 17765, 493, 885, 1296, 1958, 293, 805, 11, 300, 311, 406, 257, 1154, 13, 51164, 51164, 400, 291, 393, 9610, 1220, 309, 498, 291, 528, 11, 457, 498, 291, 500, 380, 9610, 1220, 309, 11, 309, 820, 589, 1392, 51386, 51386, 886, 13, 51457, 51457, 1610, 498, 291, 362, 257, 819, 4111, 11, 2031, 17, 11, 6104, 4190, 366, 1296, 3671, 568, 293, 1804, 1958, 13, 20, 11, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.11170487611190133, "compression_ratio": 1.6858407079646018, "no_speech_prob": 1.5056870097396313e-06}, {"id": 57, "seek": 30328, "start": 319.28, "end": 323.71999999999997, "text": " And you can rescale it if you want, but if you don't rescale it, it should work okay", "tokens": [50364, 407, 498, 264, 4122, 3613, 490, 3671, 805, 281, 1804, 805, 11, 420, 3671, 1958, 13, 18, 281, 1804, 1958, 13, 18, 11, 439, 295, 50756, 50756, 613, 366, 2584, 1392, 13, 50852, 50852, 407, 498, 291, 362, 257, 4111, 2031, 16, 300, 17765, 493, 885, 1296, 1958, 293, 805, 11, 300, 311, 406, 257, 1154, 13, 51164, 51164, 400, 291, 393, 9610, 1220, 309, 498, 291, 528, 11, 457, 498, 291, 500, 380, 9610, 1220, 309, 11, 309, 820, 589, 1392, 51386, 51386, 886, 13, 51457, 51457, 1610, 498, 291, 362, 257, 819, 4111, 11, 2031, 17, 11, 6104, 4190, 366, 1296, 3671, 568, 293, 1804, 1958, 13, 20, 11, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.11170487611190133, "compression_ratio": 1.6858407079646018, "no_speech_prob": 1.5056870097396313e-06}, {"id": 58, "seek": 30328, "start": 323.71999999999997, "end": 325.14, "text": " too.", "tokens": [50364, 407, 498, 264, 4122, 3613, 490, 3671, 805, 281, 1804, 805, 11, 420, 3671, 1958, 13, 18, 281, 1804, 1958, 13, 18, 11, 439, 295, 50756, 50756, 613, 366, 2584, 1392, 13, 50852, 50852, 407, 498, 291, 362, 257, 4111, 2031, 16, 300, 17765, 493, 885, 1296, 1958, 293, 805, 11, 300, 311, 406, 257, 1154, 13, 51164, 51164, 400, 291, 393, 9610, 1220, 309, 498, 291, 528, 11, 457, 498, 291, 500, 380, 9610, 1220, 309, 11, 309, 820, 589, 1392, 51386, 51386, 886, 13, 51457, 51457, 1610, 498, 291, 362, 257, 819, 4111, 11, 2031, 17, 11, 6104, 4190, 366, 1296, 3671, 568, 293, 1804, 1958, 13, 20, 11, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.11170487611190133, "compression_ratio": 1.6858407079646018, "no_speech_prob": 1.5056870097396313e-06}, {"id": 59, "seek": 30328, "start": 325.14, "end": 331.52, "text": " Or if you have a different feature, x2, whose values are between negative 2 and plus 0.5,", "tokens": [50364, 407, 498, 264, 4122, 3613, 490, 3671, 805, 281, 1804, 805, 11, 420, 3671, 1958, 13, 18, 281, 1804, 1958, 13, 18, 11, 439, 295, 50756, 50756, 613, 366, 2584, 1392, 13, 50852, 50852, 407, 498, 291, 362, 257, 4111, 2031, 16, 300, 17765, 493, 885, 1296, 1958, 293, 805, 11, 300, 311, 406, 257, 1154, 13, 51164, 51164, 400, 291, 393, 9610, 1220, 309, 498, 291, 528, 11, 457, 498, 291, 500, 380, 9610, 1220, 309, 11, 309, 820, 589, 1392, 51386, 51386, 886, 13, 51457, 51457, 1610, 498, 291, 362, 257, 819, 4111, 11, 2031, 17, 11, 6104, 4190, 366, 1296, 3671, 568, 293, 1804, 1958, 13, 20, 11, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.11170487611190133, "compression_ratio": 1.6858407079646018, "no_speech_prob": 1.5056870097396313e-06}, {"id": 60, "seek": 33152, "start": 331.52, "end": 333.4, "text": " again, that's okay.", "tokens": [50364, 797, 11, 300, 311, 1392, 13, 50458, 50458, 883, 6491, 9610, 4270, 309, 11, 457, 309, 1062, 312, 1392, 498, 291, 1856, 309, 3312, 382, 731, 13, 50752, 50752, 583, 498, 1071, 4111, 411, 2031, 18, 510, 22526, 490, 3671, 2319, 281, 1804, 2319, 11, 550, 341, 2516, 51128, 51128, 322, 257, 588, 819, 3613, 295, 4190, 813, 746, 490, 926, 3671, 502, 281, 1804, 502, 13, 51400, 51400, 407, 291, 434, 1391, 1101, 766, 9610, 4270, 341, 4111, 2031, 18, 370, 300, 309, 22526, 490, 746, 51679, 51679], "temperature": 0.0, "avg_logprob": -0.0732999738756117, "compression_ratio": 1.6227272727272728, "no_speech_prob": 4.1163426089951827e-07}, {"id": 61, "seek": 33152, "start": 333.4, "end": 339.28, "text": " No harm rescaling it, but it might be okay if you leave it alone as well.", "tokens": [50364, 797, 11, 300, 311, 1392, 13, 50458, 50458, 883, 6491, 9610, 4270, 309, 11, 457, 309, 1062, 312, 1392, 498, 291, 1856, 309, 3312, 382, 731, 13, 50752, 50752, 583, 498, 1071, 4111, 411, 2031, 18, 510, 22526, 490, 3671, 2319, 281, 1804, 2319, 11, 550, 341, 2516, 51128, 51128, 322, 257, 588, 819, 3613, 295, 4190, 813, 746, 490, 926, 3671, 502, 281, 1804, 502, 13, 51400, 51400, 407, 291, 434, 1391, 1101, 766, 9610, 4270, 341, 4111, 2031, 18, 370, 300, 309, 22526, 490, 746, 51679, 51679], "temperature": 0.0, "avg_logprob": -0.0732999738756117, "compression_ratio": 1.6227272727272728, "no_speech_prob": 4.1163426089951827e-07}, {"id": 62, "seek": 33152, "start": 339.28, "end": 346.79999999999995, "text": " But if another feature like x3 here ranges from negative 100 to plus 100, then this takes", "tokens": [50364, 797, 11, 300, 311, 1392, 13, 50458, 50458, 883, 6491, 9610, 4270, 309, 11, 457, 309, 1062, 312, 1392, 498, 291, 1856, 309, 3312, 382, 731, 13, 50752, 50752, 583, 498, 1071, 4111, 411, 2031, 18, 510, 22526, 490, 3671, 2319, 281, 1804, 2319, 11, 550, 341, 2516, 51128, 51128, 322, 257, 588, 819, 3613, 295, 4190, 813, 746, 490, 926, 3671, 502, 281, 1804, 502, 13, 51400, 51400, 407, 291, 434, 1391, 1101, 766, 9610, 4270, 341, 4111, 2031, 18, 370, 300, 309, 22526, 490, 746, 51679, 51679], "temperature": 0.0, "avg_logprob": -0.0732999738756117, "compression_ratio": 1.6227272727272728, "no_speech_prob": 4.1163426089951827e-07}, {"id": 63, "seek": 33152, "start": 346.79999999999995, "end": 352.24, "text": " on a very different range of values than something from around negative 1 to plus 1.", "tokens": [50364, 797, 11, 300, 311, 1392, 13, 50458, 50458, 883, 6491, 9610, 4270, 309, 11, 457, 309, 1062, 312, 1392, 498, 291, 1856, 309, 3312, 382, 731, 13, 50752, 50752, 583, 498, 1071, 4111, 411, 2031, 18, 510, 22526, 490, 3671, 2319, 281, 1804, 2319, 11, 550, 341, 2516, 51128, 51128, 322, 257, 588, 819, 3613, 295, 4190, 813, 746, 490, 926, 3671, 502, 281, 1804, 502, 13, 51400, 51400, 407, 291, 434, 1391, 1101, 766, 9610, 4270, 341, 4111, 2031, 18, 370, 300, 309, 22526, 490, 746, 51679, 51679], "temperature": 0.0, "avg_logprob": -0.0732999738756117, "compression_ratio": 1.6227272727272728, "no_speech_prob": 4.1163426089951827e-07}, {"id": 64, "seek": 33152, "start": 352.24, "end": 357.82, "text": " So you're probably better off rescaling this feature x3 so that it ranges from something", "tokens": [50364, 797, 11, 300, 311, 1392, 13, 50458, 50458, 883, 6491, 9610, 4270, 309, 11, 457, 309, 1062, 312, 1392, 498, 291, 1856, 309, 3312, 382, 731, 13, 50752, 50752, 583, 498, 1071, 4111, 411, 2031, 18, 510, 22526, 490, 3671, 2319, 281, 1804, 2319, 11, 550, 341, 2516, 51128, 51128, 322, 257, 588, 819, 3613, 295, 4190, 813, 746, 490, 926, 3671, 502, 281, 1804, 502, 13, 51400, 51400, 407, 291, 434, 1391, 1101, 766, 9610, 4270, 341, 4111, 2031, 18, 370, 300, 309, 22526, 490, 746, 51679, 51679], "temperature": 0.0, "avg_logprob": -0.0732999738756117, "compression_ratio": 1.6227272727272728, "no_speech_prob": 4.1163426089951827e-07}, {"id": 65, "seek": 35782, "start": 357.82, "end": 361.71999999999997, "text": " closer to negative 1 to plus 1.", "tokens": [50364, 4966, 281, 3671, 502, 281, 1804, 502, 13, 50559, 50559, 13157, 11, 498, 291, 362, 257, 4111, 2031, 19, 300, 2516, 322, 534, 1359, 4190, 11, 584, 1296, 3671, 50893, 50893, 1958, 13, 628, 16, 293, 1804, 1958, 13, 628, 16, 11, 550, 613, 4190, 366, 370, 1359, 11, 300, 1355, 291, 815, 528, 281, 9610, 1220, 51315, 51315, 309, 382, 731, 13, 51397, 51397, 6288, 11, 437, 498, 428, 4111, 2031, 20, 11, 1270, 382, 15383, 295, 257, 4530, 4537, 311, 1772, 51743, 51743], "temperature": 0.0, "avg_logprob": -0.12007781280868354, "compression_ratio": 1.54, "no_speech_prob": 4.965262974110374e-07}, {"id": 66, "seek": 35782, "start": 361.71999999999997, "end": 368.4, "text": " Similarly, if you have a feature x4 that takes on really small values, say between negative", "tokens": [50364, 4966, 281, 3671, 502, 281, 1804, 502, 13, 50559, 50559, 13157, 11, 498, 291, 362, 257, 4111, 2031, 19, 300, 2516, 322, 534, 1359, 4190, 11, 584, 1296, 3671, 50893, 50893, 1958, 13, 628, 16, 293, 1804, 1958, 13, 628, 16, 11, 550, 613, 4190, 366, 370, 1359, 11, 300, 1355, 291, 815, 528, 281, 9610, 1220, 51315, 51315, 309, 382, 731, 13, 51397, 51397, 6288, 11, 437, 498, 428, 4111, 2031, 20, 11, 1270, 382, 15383, 295, 257, 4530, 4537, 311, 1772, 51743, 51743], "temperature": 0.0, "avg_logprob": -0.12007781280868354, "compression_ratio": 1.54, "no_speech_prob": 4.965262974110374e-07}, {"id": 67, "seek": 35782, "start": 368.4, "end": 376.84, "text": " 0.001 and plus 0.001, then these values are so small, that means you may want to rescale", "tokens": [50364, 4966, 281, 3671, 502, 281, 1804, 502, 13, 50559, 50559, 13157, 11, 498, 291, 362, 257, 4111, 2031, 19, 300, 2516, 322, 534, 1359, 4190, 11, 584, 1296, 3671, 50893, 50893, 1958, 13, 628, 16, 293, 1804, 1958, 13, 628, 16, 11, 550, 613, 4190, 366, 370, 1359, 11, 300, 1355, 291, 815, 528, 281, 9610, 1220, 51315, 51315, 309, 382, 731, 13, 51397, 51397, 6288, 11, 437, 498, 428, 4111, 2031, 20, 11, 1270, 382, 15383, 295, 257, 4530, 4537, 311, 1772, 51743, 51743], "temperature": 0.0, "avg_logprob": -0.12007781280868354, "compression_ratio": 1.54, "no_speech_prob": 4.965262974110374e-07}, {"id": 68, "seek": 35782, "start": 376.84, "end": 378.48, "text": " it as well.", "tokens": [50364, 4966, 281, 3671, 502, 281, 1804, 502, 13, 50559, 50559, 13157, 11, 498, 291, 362, 257, 4111, 2031, 19, 300, 2516, 322, 534, 1359, 4190, 11, 584, 1296, 3671, 50893, 50893, 1958, 13, 628, 16, 293, 1804, 1958, 13, 628, 16, 11, 550, 613, 4190, 366, 370, 1359, 11, 300, 1355, 291, 815, 528, 281, 9610, 1220, 51315, 51315, 309, 382, 731, 13, 51397, 51397, 6288, 11, 437, 498, 428, 4111, 2031, 20, 11, 1270, 382, 15383, 295, 257, 4530, 4537, 311, 1772, 51743, 51743], "temperature": 0.0, "avg_logprob": -0.12007781280868354, "compression_ratio": 1.54, "no_speech_prob": 4.965262974110374e-07}, {"id": 69, "seek": 35782, "start": 378.48, "end": 385.4, "text": " Finally, what if your feature x5, such as measurements of a hospital patient's body", "tokens": [50364, 4966, 281, 3671, 502, 281, 1804, 502, 13, 50559, 50559, 13157, 11, 498, 291, 362, 257, 4111, 2031, 19, 300, 2516, 322, 534, 1359, 4190, 11, 584, 1296, 3671, 50893, 50893, 1958, 13, 628, 16, 293, 1804, 1958, 13, 628, 16, 11, 550, 613, 4190, 366, 370, 1359, 11, 300, 1355, 291, 815, 528, 281, 9610, 1220, 51315, 51315, 309, 382, 731, 13, 51397, 51397, 6288, 11, 437, 498, 428, 4111, 2031, 20, 11, 1270, 382, 15383, 295, 257, 4530, 4537, 311, 1772, 51743, 51743], "temperature": 0.0, "avg_logprob": -0.12007781280868354, "compression_ratio": 1.54, "no_speech_prob": 4.965262974110374e-07}, {"id": 70, "seek": 38540, "start": 385.4, "end": 392.52, "text": " temperature, ranges from 98.6 to 105 degrees Fahrenheit?", "tokens": [50364, 4292, 11, 22526, 490, 20860, 13, 21, 281, 33705, 5310, 31199, 30, 50720, 50720, 682, 341, 1389, 11, 613, 4190, 366, 926, 2319, 11, 597, 307, 767, 1238, 2416, 5347, 281, 51024, 51024, 661, 4373, 4122, 11, 293, 341, 486, 767, 3082, 16235, 23475, 281, 1190, 544, 5692, 13, 51316, 51316, 407, 294, 341, 1389, 11, 4111, 9610, 4270, 486, 3700, 854, 13, 51528, 51528, 821, 311, 1920, 1128, 604, 6491, 281, 9792, 484, 4111, 9610, 4270, 11, 370, 562, 294, 6385, 11, 286, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.10510681975971568, "compression_ratio": 1.5659574468085107, "no_speech_prob": 3.187531319781556e-06}, {"id": 71, "seek": 38540, "start": 392.52, "end": 398.59999999999997, "text": " In this case, these values are around 100, which is actually pretty large compared to", "tokens": [50364, 4292, 11, 22526, 490, 20860, 13, 21, 281, 33705, 5310, 31199, 30, 50720, 50720, 682, 341, 1389, 11, 613, 4190, 366, 926, 2319, 11, 597, 307, 767, 1238, 2416, 5347, 281, 51024, 51024, 661, 4373, 4122, 11, 293, 341, 486, 767, 3082, 16235, 23475, 281, 1190, 544, 5692, 13, 51316, 51316, 407, 294, 341, 1389, 11, 4111, 9610, 4270, 486, 3700, 854, 13, 51528, 51528, 821, 311, 1920, 1128, 604, 6491, 281, 9792, 484, 4111, 9610, 4270, 11, 370, 562, 294, 6385, 11, 286, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.10510681975971568, "compression_ratio": 1.5659574468085107, "no_speech_prob": 3.187531319781556e-06}, {"id": 72, "seek": 38540, "start": 398.59999999999997, "end": 404.44, "text": " other scale features, and this will actually cause gradient descent to run more slowly.", "tokens": [50364, 4292, 11, 22526, 490, 20860, 13, 21, 281, 33705, 5310, 31199, 30, 50720, 50720, 682, 341, 1389, 11, 613, 4190, 366, 926, 2319, 11, 597, 307, 767, 1238, 2416, 5347, 281, 51024, 51024, 661, 4373, 4122, 11, 293, 341, 486, 767, 3082, 16235, 23475, 281, 1190, 544, 5692, 13, 51316, 51316, 407, 294, 341, 1389, 11, 4111, 9610, 4270, 486, 3700, 854, 13, 51528, 51528, 821, 311, 1920, 1128, 604, 6491, 281, 9792, 484, 4111, 9610, 4270, 11, 370, 562, 294, 6385, 11, 286, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.10510681975971568, "compression_ratio": 1.5659574468085107, "no_speech_prob": 3.187531319781556e-06}, {"id": 73, "seek": 38540, "start": 404.44, "end": 408.67999999999995, "text": " So in this case, feature rescaling will likely help.", "tokens": [50364, 4292, 11, 22526, 490, 20860, 13, 21, 281, 33705, 5310, 31199, 30, 50720, 50720, 682, 341, 1389, 11, 613, 4190, 366, 926, 2319, 11, 597, 307, 767, 1238, 2416, 5347, 281, 51024, 51024, 661, 4373, 4122, 11, 293, 341, 486, 767, 3082, 16235, 23475, 281, 1190, 544, 5692, 13, 51316, 51316, 407, 294, 341, 1389, 11, 4111, 9610, 4270, 486, 3700, 854, 13, 51528, 51528, 821, 311, 1920, 1128, 604, 6491, 281, 9792, 484, 4111, 9610, 4270, 11, 370, 562, 294, 6385, 11, 286, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.10510681975971568, "compression_ratio": 1.5659574468085107, "no_speech_prob": 3.187531319781556e-06}, {"id": 74, "seek": 38540, "start": 408.67999999999995, "end": 413.88, "text": " There's almost never any harm to carrying out feature rescaling, so when in doubt, I", "tokens": [50364, 4292, 11, 22526, 490, 20860, 13, 21, 281, 33705, 5310, 31199, 30, 50720, 50720, 682, 341, 1389, 11, 613, 4190, 366, 926, 2319, 11, 597, 307, 767, 1238, 2416, 5347, 281, 51024, 51024, 661, 4373, 4122, 11, 293, 341, 486, 767, 3082, 16235, 23475, 281, 1190, 544, 5692, 13, 51316, 51316, 407, 294, 341, 1389, 11, 4111, 9610, 4270, 486, 3700, 854, 13, 51528, 51528, 821, 311, 1920, 1128, 604, 6491, 281, 9792, 484, 4111, 9610, 4270, 11, 370, 562, 294, 6385, 11, 286, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.10510681975971568, "compression_ratio": 1.5659574468085107, "no_speech_prob": 3.187531319781556e-06}, {"id": 75, "seek": 41388, "start": 413.88, "end": 416.71999999999997, "text": " encourage you to just carry it out.", "tokens": [50364, 5373, 291, 281, 445, 3985, 309, 484, 13, 50506, 50506, 400, 300, 311, 309, 337, 4111, 21589, 13, 50618, 50618, 2022, 341, 707, 6532, 11, 291, 603, 2049, 312, 1075, 281, 483, 16235, 23475, 281, 1190, 709, 4663, 13, 50943, 50943, 407, 300, 311, 4111, 21589, 13, 51064, 51064, 400, 365, 420, 1553, 4111, 21589, 11, 562, 291, 1190, 16235, 23475, 11, 577, 393, 291, 458, 11, 51308, 51308, 577, 393, 291, 1520, 498, 16235, 23475, 307, 534, 1364, 11, 498, 309, 307, 5006, 291, 264, 4338, 7285, 51594, 51594, 420, 746, 1998, 281, 309, 30, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.10999698638916015, "compression_ratio": 1.78125, "no_speech_prob": 2.8572810606419807e-06}, {"id": 76, "seek": 41388, "start": 416.71999999999997, "end": 418.96, "text": " And that's it for feature scaling.", "tokens": [50364, 5373, 291, 281, 445, 3985, 309, 484, 13, 50506, 50506, 400, 300, 311, 309, 337, 4111, 21589, 13, 50618, 50618, 2022, 341, 707, 6532, 11, 291, 603, 2049, 312, 1075, 281, 483, 16235, 23475, 281, 1190, 709, 4663, 13, 50943, 50943, 407, 300, 311, 4111, 21589, 13, 51064, 51064, 400, 365, 420, 1553, 4111, 21589, 11, 562, 291, 1190, 16235, 23475, 11, 577, 393, 291, 458, 11, 51308, 51308, 577, 393, 291, 1520, 498, 16235, 23475, 307, 534, 1364, 11, 498, 309, 307, 5006, 291, 264, 4338, 7285, 51594, 51594, 420, 746, 1998, 281, 309, 30, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.10999698638916015, "compression_ratio": 1.78125, "no_speech_prob": 2.8572810606419807e-06}, {"id": 77, "seek": 41388, "start": 418.96, "end": 425.46, "text": " With this little technique, you'll often be able to get gradient descent to run much faster.", "tokens": [50364, 5373, 291, 281, 445, 3985, 309, 484, 13, 50506, 50506, 400, 300, 311, 309, 337, 4111, 21589, 13, 50618, 50618, 2022, 341, 707, 6532, 11, 291, 603, 2049, 312, 1075, 281, 483, 16235, 23475, 281, 1190, 709, 4663, 13, 50943, 50943, 407, 300, 311, 4111, 21589, 13, 51064, 51064, 400, 365, 420, 1553, 4111, 21589, 11, 562, 291, 1190, 16235, 23475, 11, 577, 393, 291, 458, 11, 51308, 51308, 577, 393, 291, 1520, 498, 16235, 23475, 307, 534, 1364, 11, 498, 309, 307, 5006, 291, 264, 4338, 7285, 51594, 51594, 420, 746, 1998, 281, 309, 30, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.10999698638916015, "compression_ratio": 1.78125, "no_speech_prob": 2.8572810606419807e-06}, {"id": 78, "seek": 41388, "start": 425.46, "end": 427.88, "text": " So that's feature scaling.", "tokens": [50364, 5373, 291, 281, 445, 3985, 309, 484, 13, 50506, 50506, 400, 300, 311, 309, 337, 4111, 21589, 13, 50618, 50618, 2022, 341, 707, 6532, 11, 291, 603, 2049, 312, 1075, 281, 483, 16235, 23475, 281, 1190, 709, 4663, 13, 50943, 50943, 407, 300, 311, 4111, 21589, 13, 51064, 51064, 400, 365, 420, 1553, 4111, 21589, 11, 562, 291, 1190, 16235, 23475, 11, 577, 393, 291, 458, 11, 51308, 51308, 577, 393, 291, 1520, 498, 16235, 23475, 307, 534, 1364, 11, 498, 309, 307, 5006, 291, 264, 4338, 7285, 51594, 51594, 420, 746, 1998, 281, 309, 30, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.10999698638916015, "compression_ratio": 1.78125, "no_speech_prob": 2.8572810606419807e-06}, {"id": 79, "seek": 41388, "start": 427.88, "end": 432.76, "text": " And with or without feature scaling, when you run gradient descent, how can you know,", "tokens": [50364, 5373, 291, 281, 445, 3985, 309, 484, 13, 50506, 50506, 400, 300, 311, 309, 337, 4111, 21589, 13, 50618, 50618, 2022, 341, 707, 6532, 11, 291, 603, 2049, 312, 1075, 281, 483, 16235, 23475, 281, 1190, 709, 4663, 13, 50943, 50943, 407, 300, 311, 4111, 21589, 13, 51064, 51064, 400, 365, 420, 1553, 4111, 21589, 11, 562, 291, 1190, 16235, 23475, 11, 577, 393, 291, 458, 11, 51308, 51308, 577, 393, 291, 1520, 498, 16235, 23475, 307, 534, 1364, 11, 498, 309, 307, 5006, 291, 264, 4338, 7285, 51594, 51594, 420, 746, 1998, 281, 309, 30, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.10999698638916015, "compression_ratio": 1.78125, "no_speech_prob": 2.8572810606419807e-06}, {"id": 80, "seek": 41388, "start": 432.76, "end": 438.48, "text": " how can you check if gradient descent is really working, if it is finding you the global minimum", "tokens": [50364, 5373, 291, 281, 445, 3985, 309, 484, 13, 50506, 50506, 400, 300, 311, 309, 337, 4111, 21589, 13, 50618, 50618, 2022, 341, 707, 6532, 11, 291, 603, 2049, 312, 1075, 281, 483, 16235, 23475, 281, 1190, 709, 4663, 13, 50943, 50943, 407, 300, 311, 4111, 21589, 13, 51064, 51064, 400, 365, 420, 1553, 4111, 21589, 11, 562, 291, 1190, 16235, 23475, 11, 577, 393, 291, 458, 11, 51308, 51308, 577, 393, 291, 1520, 498, 16235, 23475, 307, 534, 1364, 11, 498, 309, 307, 5006, 291, 264, 4338, 7285, 51594, 51594, 420, 746, 1998, 281, 309, 30, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.10999698638916015, "compression_ratio": 1.78125, "no_speech_prob": 2.8572810606419807e-06}, {"id": 81, "seek": 41388, "start": 438.48, "end": 440.4, "text": " or something close to it?", "tokens": [50364, 5373, 291, 281, 445, 3985, 309, 484, 13, 50506, 50506, 400, 300, 311, 309, 337, 4111, 21589, 13, 50618, 50618, 2022, 341, 707, 6532, 11, 291, 603, 2049, 312, 1075, 281, 483, 16235, 23475, 281, 1190, 709, 4663, 13, 50943, 50943, 407, 300, 311, 4111, 21589, 13, 51064, 51064, 400, 365, 420, 1553, 4111, 21589, 11, 562, 291, 1190, 16235, 23475, 11, 577, 393, 291, 458, 11, 51308, 51308, 577, 393, 291, 1520, 498, 16235, 23475, 307, 534, 1364, 11, 498, 309, 307, 5006, 291, 264, 4338, 7285, 51594, 51594, 420, 746, 1998, 281, 309, 30, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.10999698638916015, "compression_ratio": 1.78125, "no_speech_prob": 2.8572810606419807e-06}, {"id": 82, "seek": 44040, "start": 440.4, "end": 446.67999999999995, "text": " In the next video, let's take a look at how to recognize if gradient descent is converging.", "tokens": [50364, 682, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 577, 281, 5521, 498, 16235, 23475, 307, 9652, 3249, 13, 50678, 50678, 400, 550, 294, 264, 960, 934, 300, 11, 341, 486, 1477, 281, 5017, 295, 577, 281, 2826, 257, 665, 50902, 50902, 2539, 3314, 337, 16235, 23475, 13, 51018], "temperature": 0.0, "avg_logprob": -0.11408015017239552, "compression_ratio": 1.4758620689655173, "no_speech_prob": 5.820220394525677e-05}, {"id": 83, "seek": 44040, "start": 446.67999999999995, "end": 451.15999999999997, "text": " And then in the video after that, this will lead to discussion of how to choose a good", "tokens": [50364, 682, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 577, 281, 5521, 498, 16235, 23475, 307, 9652, 3249, 13, 50678, 50678, 400, 550, 294, 264, 960, 934, 300, 11, 341, 486, 1477, 281, 5017, 295, 577, 281, 2826, 257, 665, 50902, 50902, 2539, 3314, 337, 16235, 23475, 13, 51018], "temperature": 0.0, "avg_logprob": -0.11408015017239552, "compression_ratio": 1.4758620689655173, "no_speech_prob": 5.820220394525677e-05}, {"id": 84, "seek": 45116, "start": 451.16, "end": 471.16, "text": " learning rate for gradient descent.", "tokens": [50364, 2539, 3314, 337, 16235, 23475, 13, 51364], "temperature": 0.0, "avg_logprob": -0.5568161010742188, "compression_ratio": 0.8536585365853658, "no_speech_prob": 2.971834510390181e-05}], "language": "en", "video_id": "g9bkFTnM-7k", "entity": "ML Specialization, Andrew Ng (2022)"}}