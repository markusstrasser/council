{"video_id": "uNNx1Czrt1w", "title": "6.2 Evaluating and choosing models | Evaluating a model  -[Machine Learning | Andrew Ng]", "description": "Second Course:\nAdvanced Learning Algorithms.\n\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 625, "views": 116, "publish_date": "11/04/2022", "timestamp": 1661817600, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " Let's say you've trained a machine learning model. How do you evaluate that model's performance? You find that having a systematic way to evaluate performance will also help paint a clearer path for how to then improve this performance. So let's take a look at how to evaluate a model. Let's take the example of learning to predict housing prices as a function of the size. Let's say you've trained a model to predict housing prices as a function of the size x. And for the model, that is a fourth-order polynomial. So it features x, x squared, x cubed, and x to the fourth. Because we fit a fourth-order polynomial to a training set with five data points, this fits the training data really well. But we don't like this model very much, because even though the model fits the training data well, we think it will fail to generalize to new examples that aren't in the training set. So when you are predicting prices just a single feature the size of the house, you could plot the model like this, and we could see that the curve is very wiggly. So we know this probably isn't a good model. But if you were fitting this model with even more features, say we had x1 the size of the house, number of bedrooms, the number of floors of the house, also the age of the home and years, then it becomes much harder to plot f, because f is now a function of x1 through x4. And how do you plot a four-dimensional function? So in order to tell if your model is doing well, especially for applications where you have more than one or two features, which makes it difficult to plot f of x, we need some more systematic way to evaluate how well your model is doing. Here's a technique that you can use. If you have a training set, and this is a small training set with just 10 examples listed here, rather than taking all your data to train the parameters w and b of the model, you can instead split the training set into two subsets. I'm going to draw a line here. And let's put 70% of the data into the first part. And I'm going to call that the training set. And the second part of the data, let's say 30% of the data, I'm going to put into a test set. And what we're going to do is train the model's parameters on the training set on this first 70% or so of the data, and then we'll test this performance on this test set. In notation, I'm going to use x1, y1, same as before, to denote the training examples through xm, ym, except that now to make explicit. So in this little example, we would have seven training examples. And to introduce one new piece of notation, I'm going to use m subscript train. m train is a number of training examples, which in this small data set is seven. So the subscript train just emphasizes if we're looking at the training set portion of the data. And for the test sets, I'm going to use the notation x1 subscript test, y1 subscript test to denote the first test example. And this goes all the way to xm test, subscript test, ym test, subscript test. And m test is the number of test examples, which in this case is three. And it's not uncommon to split your data set according to maybe a 70-30 split or 80-20 split with most of your data going into the training set and then a smaller fraction going into the test set. So in order to train a model and evaluate it, this is what it would look like if you're using linear regression with a squared error cost. Start off by fitting the parameters by minimizing the cost function j of wb. So this is a usual cost function. Minimize over wb of this squared error cost plus regularization term, longer over 2m times sum of the wj squared. And then to tell how well this model is doing, you would compute j test of wb, which is equal to the average error on the test set. And that's just equal to 1 over 2 times m test. That's the number of test examples. And then of sum over all the examples from i equals 1 to the number of test examples of the squared error on each of the test examples like so. So it's a prediction on the i-th test example input minus the actual price of the house on the i-th test example squared. And notice that the test error formula j test, it does not include that regularization term. And this will give you a sense of how well your learning algorithm is doing. One other quantity that's often useful to compute as well is the training error, which is a measure of how well your learning algorithm is doing on the training set. So let me define j train of wb to be equal to the average over the training sets 1 over 2m or 1 over 2m subscript train of sum over your training sets of this squared error term. And once again, this does not include the regularization term, unlike the cost function that you were minimizing to fit the parameters. So in a model like what we saw earlier in this video, j train of wb will be low because the average error on your training examples will be zero or very close to zero. So j train will be very close to zero. But if you had a few additional examples in your test set that the algorithm had not trained on then those test examples might look like these. And there's a large gap between what the algorithm is predicting as the estimated housing price and the actual value of those housing prices. And so j tests will be high. So seeing that j test is high on this model gives you a way to realize that even though it does great on the training set is actually not so good at generalizing to new examples to new data points that were not in the training set. So that was regression with squared error costs. Now let's take a look at how you'd apply this procedure to a classification problem. For example, if you were classifying between handwritten digits that are either zero or one. So same as before, you fit the parameters by minimizing the cost function to find the parameters wb. For example, if you were training logistic regression, then this would be the cost function j of wb where this is the usual logistic loss function and then plus also the regularization term and to compute the test error. J test is then the average over your test examples. That's that 30% of the data that wasn't in the training set of the logistic loss on your test set. And the training error you could also compute using this formula is the average logistic loss on your training data that the algorithm was using to minimize the cost function j of wb. Well what I describe here will work okay for figuring out if your learning algorithm is doing well. I've seen how it was doing in terms of test error. When applying machine learning to classification problems, there's actually one other definition of j test and j train that is maybe even more commonly used, which is instead of using the logistic loss to compute the test error and the training error to instead measure what's the fraction of the test set and the fraction of the training set that the algorithm has misclassified. So specifically on the test set, you can have the algorithm make a prediction one or zero on every test example. So recall y hat, we would predict as one if f of x is greater than or equal to 0.5 and zero if it's less than 0.5. And you can then count up in the test set the fraction of examples where y hat is not equal to the actual ground truth label y in the test set. So concretely, if you were classifying handwritten digits zero or one by new classification toss, then j test would be the fraction of that test set where zero was classified as one or one classified as zero. And similarly, j train is a fraction of the training set that has been misclassified. Taking a data set and splitting it into a training set and a separate test set gives you a way to systematically evaluate how well your learning algorithm is doing. By computing both j test and j train, you can now measure how it's doing on the test set and on the training set. This procedure is one step to what you being able to automatically choose what model to use for a given machine learning application. For example, if you're trying to predict housing prices, should you fit a straight line to your data or fit a second order polynomial or third order or fourth order polynomial? It turns out that with one further refinement to the idea you saw in this video, you'll be able to have an algorithm help you to automatically make that type of decision well. Let's take a look at how to do that in the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.48, "text": " Let's say you've trained a machine learning model.", "tokens": [50364, 961, 311, 584, 291, 600, 8895, 257, 3479, 2539, 2316, 13, 50588, 50588, 1012, 360, 291, 13059, 300, 2316, 311, 3389, 30, 50708, 50708, 509, 915, 300, 1419, 257, 27249, 636, 281, 13059, 3389, 486, 611, 854, 4225, 257, 26131, 50966, 50966, 3100, 337, 577, 281, 550, 3470, 341, 3389, 13, 51106, 51106, 407, 718, 311, 747, 257, 574, 412, 577, 281, 13059, 257, 2316, 13, 51260, 51260, 961, 311, 747, 264, 1365, 295, 2539, 281, 6069, 6849, 7901, 382, 257, 2445, 295, 264, 2744, 13, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.13406271404690212, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.011680986732244492}, {"id": 1, "seek": 0, "start": 4.48, "end": 6.88, "text": " How do you evaluate that model's performance?", "tokens": [50364, 961, 311, 584, 291, 600, 8895, 257, 3479, 2539, 2316, 13, 50588, 50588, 1012, 360, 291, 13059, 300, 2316, 311, 3389, 30, 50708, 50708, 509, 915, 300, 1419, 257, 27249, 636, 281, 13059, 3389, 486, 611, 854, 4225, 257, 26131, 50966, 50966, 3100, 337, 577, 281, 550, 3470, 341, 3389, 13, 51106, 51106, 407, 718, 311, 747, 257, 574, 412, 577, 281, 13059, 257, 2316, 13, 51260, 51260, 961, 311, 747, 264, 1365, 295, 2539, 281, 6069, 6849, 7901, 382, 257, 2445, 295, 264, 2744, 13, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.13406271404690212, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.011680986732244492}, {"id": 2, "seek": 0, "start": 6.88, "end": 12.040000000000001, "text": " You find that having a systematic way to evaluate performance will also help paint a clearer", "tokens": [50364, 961, 311, 584, 291, 600, 8895, 257, 3479, 2539, 2316, 13, 50588, 50588, 1012, 360, 291, 13059, 300, 2316, 311, 3389, 30, 50708, 50708, 509, 915, 300, 1419, 257, 27249, 636, 281, 13059, 3389, 486, 611, 854, 4225, 257, 26131, 50966, 50966, 3100, 337, 577, 281, 550, 3470, 341, 3389, 13, 51106, 51106, 407, 718, 311, 747, 257, 574, 412, 577, 281, 13059, 257, 2316, 13, 51260, 51260, 961, 311, 747, 264, 1365, 295, 2539, 281, 6069, 6849, 7901, 382, 257, 2445, 295, 264, 2744, 13, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.13406271404690212, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.011680986732244492}, {"id": 3, "seek": 0, "start": 12.040000000000001, "end": 14.84, "text": " path for how to then improve this performance.", "tokens": [50364, 961, 311, 584, 291, 600, 8895, 257, 3479, 2539, 2316, 13, 50588, 50588, 1012, 360, 291, 13059, 300, 2316, 311, 3389, 30, 50708, 50708, 509, 915, 300, 1419, 257, 27249, 636, 281, 13059, 3389, 486, 611, 854, 4225, 257, 26131, 50966, 50966, 3100, 337, 577, 281, 550, 3470, 341, 3389, 13, 51106, 51106, 407, 718, 311, 747, 257, 574, 412, 577, 281, 13059, 257, 2316, 13, 51260, 51260, 961, 311, 747, 264, 1365, 295, 2539, 281, 6069, 6849, 7901, 382, 257, 2445, 295, 264, 2744, 13, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.13406271404690212, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.011680986732244492}, {"id": 4, "seek": 0, "start": 14.84, "end": 17.92, "text": " So let's take a look at how to evaluate a model.", "tokens": [50364, 961, 311, 584, 291, 600, 8895, 257, 3479, 2539, 2316, 13, 50588, 50588, 1012, 360, 291, 13059, 300, 2316, 311, 3389, 30, 50708, 50708, 509, 915, 300, 1419, 257, 27249, 636, 281, 13059, 3389, 486, 611, 854, 4225, 257, 26131, 50966, 50966, 3100, 337, 577, 281, 550, 3470, 341, 3389, 13, 51106, 51106, 407, 718, 311, 747, 257, 574, 412, 577, 281, 13059, 257, 2316, 13, 51260, 51260, 961, 311, 747, 264, 1365, 295, 2539, 281, 6069, 6849, 7901, 382, 257, 2445, 295, 264, 2744, 13, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.13406271404690212, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.011680986732244492}, {"id": 5, "seek": 0, "start": 17.92, "end": 24.96, "text": " Let's take the example of learning to predict housing prices as a function of the size.", "tokens": [50364, 961, 311, 584, 291, 600, 8895, 257, 3479, 2539, 2316, 13, 50588, 50588, 1012, 360, 291, 13059, 300, 2316, 311, 3389, 30, 50708, 50708, 509, 915, 300, 1419, 257, 27249, 636, 281, 13059, 3389, 486, 611, 854, 4225, 257, 26131, 50966, 50966, 3100, 337, 577, 281, 550, 3470, 341, 3389, 13, 51106, 51106, 407, 718, 311, 747, 257, 574, 412, 577, 281, 13059, 257, 2316, 13, 51260, 51260, 961, 311, 747, 264, 1365, 295, 2539, 281, 6069, 6849, 7901, 382, 257, 2445, 295, 264, 2744, 13, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.13406271404690212, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.011680986732244492}, {"id": 6, "seek": 2496, "start": 24.96, "end": 31.04, "text": " Let's say you've trained a model to predict housing prices as a function of the size x.", "tokens": [50364, 961, 311, 584, 291, 600, 8895, 257, 2316, 281, 6069, 6849, 7901, 382, 257, 2445, 295, 264, 2744, 2031, 13, 50668, 50668, 400, 337, 264, 2316, 11, 300, 307, 257, 6409, 12, 4687, 26110, 13, 50926, 50926, 407, 309, 4122, 2031, 11, 2031, 8889, 11, 2031, 36510, 11, 293, 2031, 281, 264, 6409, 13, 51142, 51142, 1436, 321, 3318, 257, 6409, 12, 4687, 26110, 281, 257, 3097, 992, 365, 1732, 1412, 2793, 11, 341, 51450, 51450, 9001, 264, 3097, 1412, 534, 731, 13, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.14772214560673155, "compression_ratio": 1.6069651741293531, "no_speech_prob": 3.071484388783574e-05}, {"id": 7, "seek": 2496, "start": 31.04, "end": 36.2, "text": " And for the model, that is a fourth-order polynomial.", "tokens": [50364, 961, 311, 584, 291, 600, 8895, 257, 2316, 281, 6069, 6849, 7901, 382, 257, 2445, 295, 264, 2744, 2031, 13, 50668, 50668, 400, 337, 264, 2316, 11, 300, 307, 257, 6409, 12, 4687, 26110, 13, 50926, 50926, 407, 309, 4122, 2031, 11, 2031, 8889, 11, 2031, 36510, 11, 293, 2031, 281, 264, 6409, 13, 51142, 51142, 1436, 321, 3318, 257, 6409, 12, 4687, 26110, 281, 257, 3097, 992, 365, 1732, 1412, 2793, 11, 341, 51450, 51450, 9001, 264, 3097, 1412, 534, 731, 13, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.14772214560673155, "compression_ratio": 1.6069651741293531, "no_speech_prob": 3.071484388783574e-05}, {"id": 8, "seek": 2496, "start": 36.2, "end": 40.52, "text": " So it features x, x squared, x cubed, and x to the fourth.", "tokens": [50364, 961, 311, 584, 291, 600, 8895, 257, 2316, 281, 6069, 6849, 7901, 382, 257, 2445, 295, 264, 2744, 2031, 13, 50668, 50668, 400, 337, 264, 2316, 11, 300, 307, 257, 6409, 12, 4687, 26110, 13, 50926, 50926, 407, 309, 4122, 2031, 11, 2031, 8889, 11, 2031, 36510, 11, 293, 2031, 281, 264, 6409, 13, 51142, 51142, 1436, 321, 3318, 257, 6409, 12, 4687, 26110, 281, 257, 3097, 992, 365, 1732, 1412, 2793, 11, 341, 51450, 51450, 9001, 264, 3097, 1412, 534, 731, 13, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.14772214560673155, "compression_ratio": 1.6069651741293531, "no_speech_prob": 3.071484388783574e-05}, {"id": 9, "seek": 2496, "start": 40.52, "end": 46.68, "text": " Because we fit a fourth-order polynomial to a training set with five data points, this", "tokens": [50364, 961, 311, 584, 291, 600, 8895, 257, 2316, 281, 6069, 6849, 7901, 382, 257, 2445, 295, 264, 2744, 2031, 13, 50668, 50668, 400, 337, 264, 2316, 11, 300, 307, 257, 6409, 12, 4687, 26110, 13, 50926, 50926, 407, 309, 4122, 2031, 11, 2031, 8889, 11, 2031, 36510, 11, 293, 2031, 281, 264, 6409, 13, 51142, 51142, 1436, 321, 3318, 257, 6409, 12, 4687, 26110, 281, 257, 3097, 992, 365, 1732, 1412, 2793, 11, 341, 51450, 51450, 9001, 264, 3097, 1412, 534, 731, 13, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.14772214560673155, "compression_ratio": 1.6069651741293531, "no_speech_prob": 3.071484388783574e-05}, {"id": 10, "seek": 2496, "start": 46.68, "end": 50.36, "text": " fits the training data really well.", "tokens": [50364, 961, 311, 584, 291, 600, 8895, 257, 2316, 281, 6069, 6849, 7901, 382, 257, 2445, 295, 264, 2744, 2031, 13, 50668, 50668, 400, 337, 264, 2316, 11, 300, 307, 257, 6409, 12, 4687, 26110, 13, 50926, 50926, 407, 309, 4122, 2031, 11, 2031, 8889, 11, 2031, 36510, 11, 293, 2031, 281, 264, 6409, 13, 51142, 51142, 1436, 321, 3318, 257, 6409, 12, 4687, 26110, 281, 257, 3097, 992, 365, 1732, 1412, 2793, 11, 341, 51450, 51450, 9001, 264, 3097, 1412, 534, 731, 13, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.14772214560673155, "compression_ratio": 1.6069651741293531, "no_speech_prob": 3.071484388783574e-05}, {"id": 11, "seek": 5036, "start": 50.36, "end": 57.32, "text": " But we don't like this model very much, because even though the model fits the training data", "tokens": [50364, 583, 321, 500, 380, 411, 341, 2316, 588, 709, 11, 570, 754, 1673, 264, 2316, 9001, 264, 3097, 1412, 50712, 50712, 731, 11, 321, 519, 309, 486, 3061, 281, 2674, 1125, 281, 777, 5110, 300, 3212, 380, 294, 264, 3097, 50940, 50940, 992, 13, 51044, 51044, 407, 562, 291, 366, 32884, 7901, 445, 257, 2167, 4111, 264, 2744, 295, 264, 1782, 11, 291, 727, 7542, 51340, 51340, 264, 2316, 411, 341, 11, 293, 321, 727, 536, 300, 264, 7605, 307, 588, 261, 46737, 13, 51574, 51574, 407, 321, 458, 341, 1391, 1943, 380, 257, 665, 2316, 13, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.11946375063150236, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.1478229680506047e-05}, {"id": 12, "seek": 5036, "start": 57.32, "end": 61.879999999999995, "text": " well, we think it will fail to generalize to new examples that aren't in the training", "tokens": [50364, 583, 321, 500, 380, 411, 341, 2316, 588, 709, 11, 570, 754, 1673, 264, 2316, 9001, 264, 3097, 1412, 50712, 50712, 731, 11, 321, 519, 309, 486, 3061, 281, 2674, 1125, 281, 777, 5110, 300, 3212, 380, 294, 264, 3097, 50940, 50940, 992, 13, 51044, 51044, 407, 562, 291, 366, 32884, 7901, 445, 257, 2167, 4111, 264, 2744, 295, 264, 1782, 11, 291, 727, 7542, 51340, 51340, 264, 2316, 411, 341, 11, 293, 321, 727, 536, 300, 264, 7605, 307, 588, 261, 46737, 13, 51574, 51574, 407, 321, 458, 341, 1391, 1943, 380, 257, 665, 2316, 13, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.11946375063150236, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.1478229680506047e-05}, {"id": 13, "seek": 5036, "start": 61.879999999999995, "end": 63.96, "text": " set.", "tokens": [50364, 583, 321, 500, 380, 411, 341, 2316, 588, 709, 11, 570, 754, 1673, 264, 2316, 9001, 264, 3097, 1412, 50712, 50712, 731, 11, 321, 519, 309, 486, 3061, 281, 2674, 1125, 281, 777, 5110, 300, 3212, 380, 294, 264, 3097, 50940, 50940, 992, 13, 51044, 51044, 407, 562, 291, 366, 32884, 7901, 445, 257, 2167, 4111, 264, 2744, 295, 264, 1782, 11, 291, 727, 7542, 51340, 51340, 264, 2316, 411, 341, 11, 293, 321, 727, 536, 300, 264, 7605, 307, 588, 261, 46737, 13, 51574, 51574, 407, 321, 458, 341, 1391, 1943, 380, 257, 665, 2316, 13, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.11946375063150236, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.1478229680506047e-05}, {"id": 14, "seek": 5036, "start": 63.96, "end": 69.88, "text": " So when you are predicting prices just a single feature the size of the house, you could plot", "tokens": [50364, 583, 321, 500, 380, 411, 341, 2316, 588, 709, 11, 570, 754, 1673, 264, 2316, 9001, 264, 3097, 1412, 50712, 50712, 731, 11, 321, 519, 309, 486, 3061, 281, 2674, 1125, 281, 777, 5110, 300, 3212, 380, 294, 264, 3097, 50940, 50940, 992, 13, 51044, 51044, 407, 562, 291, 366, 32884, 7901, 445, 257, 2167, 4111, 264, 2744, 295, 264, 1782, 11, 291, 727, 7542, 51340, 51340, 264, 2316, 411, 341, 11, 293, 321, 727, 536, 300, 264, 7605, 307, 588, 261, 46737, 13, 51574, 51574, 407, 321, 458, 341, 1391, 1943, 380, 257, 665, 2316, 13, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.11946375063150236, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.1478229680506047e-05}, {"id": 15, "seek": 5036, "start": 69.88, "end": 74.56, "text": " the model like this, and we could see that the curve is very wiggly.", "tokens": [50364, 583, 321, 500, 380, 411, 341, 2316, 588, 709, 11, 570, 754, 1673, 264, 2316, 9001, 264, 3097, 1412, 50712, 50712, 731, 11, 321, 519, 309, 486, 3061, 281, 2674, 1125, 281, 777, 5110, 300, 3212, 380, 294, 264, 3097, 50940, 50940, 992, 13, 51044, 51044, 407, 562, 291, 366, 32884, 7901, 445, 257, 2167, 4111, 264, 2744, 295, 264, 1782, 11, 291, 727, 7542, 51340, 51340, 264, 2316, 411, 341, 11, 293, 321, 727, 536, 300, 264, 7605, 307, 588, 261, 46737, 13, 51574, 51574, 407, 321, 458, 341, 1391, 1943, 380, 257, 665, 2316, 13, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.11946375063150236, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.1478229680506047e-05}, {"id": 16, "seek": 5036, "start": 74.56, "end": 77.84, "text": " So we know this probably isn't a good model.", "tokens": [50364, 583, 321, 500, 380, 411, 341, 2316, 588, 709, 11, 570, 754, 1673, 264, 2316, 9001, 264, 3097, 1412, 50712, 50712, 731, 11, 321, 519, 309, 486, 3061, 281, 2674, 1125, 281, 777, 5110, 300, 3212, 380, 294, 264, 3097, 50940, 50940, 992, 13, 51044, 51044, 407, 562, 291, 366, 32884, 7901, 445, 257, 2167, 4111, 264, 2744, 295, 264, 1782, 11, 291, 727, 7542, 51340, 51340, 264, 2316, 411, 341, 11, 293, 321, 727, 536, 300, 264, 7605, 307, 588, 261, 46737, 13, 51574, 51574, 407, 321, 458, 341, 1391, 1943, 380, 257, 665, 2316, 13, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.11946375063150236, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.1478229680506047e-05}, {"id": 17, "seek": 7784, "start": 77.84, "end": 82.84, "text": " But if you were fitting this model with even more features, say we had x1 the size of the", "tokens": [50364, 583, 498, 291, 645, 15669, 341, 2316, 365, 754, 544, 4122, 11, 584, 321, 632, 2031, 16, 264, 2744, 295, 264, 50614, 50614, 1782, 11, 1230, 295, 39955, 11, 264, 1230, 295, 21008, 295, 264, 1782, 11, 611, 264, 3205, 295, 264, 1280, 293, 50838, 50838, 924, 11, 550, 309, 3643, 709, 6081, 281, 7542, 283, 11, 570, 283, 307, 586, 257, 2445, 295, 2031, 16, 807, 51262, 51262, 2031, 19, 13, 51326, 51326, 400, 577, 360, 291, 7542, 257, 1451, 12, 18759, 2445, 30, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.17221209708224522, "compression_ratio": 1.6294416243654823, "no_speech_prob": 5.5942705330380704e-06}, {"id": 18, "seek": 7784, "start": 82.84, "end": 87.32000000000001, "text": " house, number of bedrooms, the number of floors of the house, also the age of the home and", "tokens": [50364, 583, 498, 291, 645, 15669, 341, 2316, 365, 754, 544, 4122, 11, 584, 321, 632, 2031, 16, 264, 2744, 295, 264, 50614, 50614, 1782, 11, 1230, 295, 39955, 11, 264, 1230, 295, 21008, 295, 264, 1782, 11, 611, 264, 3205, 295, 264, 1280, 293, 50838, 50838, 924, 11, 550, 309, 3643, 709, 6081, 281, 7542, 283, 11, 570, 283, 307, 586, 257, 2445, 295, 2031, 16, 807, 51262, 51262, 2031, 19, 13, 51326, 51326, 400, 577, 360, 291, 7542, 257, 1451, 12, 18759, 2445, 30, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.17221209708224522, "compression_ratio": 1.6294416243654823, "no_speech_prob": 5.5942705330380704e-06}, {"id": 19, "seek": 7784, "start": 87.32000000000001, "end": 95.80000000000001, "text": " years, then it becomes much harder to plot f, because f is now a function of x1 through", "tokens": [50364, 583, 498, 291, 645, 15669, 341, 2316, 365, 754, 544, 4122, 11, 584, 321, 632, 2031, 16, 264, 2744, 295, 264, 50614, 50614, 1782, 11, 1230, 295, 39955, 11, 264, 1230, 295, 21008, 295, 264, 1782, 11, 611, 264, 3205, 295, 264, 1280, 293, 50838, 50838, 924, 11, 550, 309, 3643, 709, 6081, 281, 7542, 283, 11, 570, 283, 307, 586, 257, 2445, 295, 2031, 16, 807, 51262, 51262, 2031, 19, 13, 51326, 51326, 400, 577, 360, 291, 7542, 257, 1451, 12, 18759, 2445, 30, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.17221209708224522, "compression_ratio": 1.6294416243654823, "no_speech_prob": 5.5942705330380704e-06}, {"id": 20, "seek": 7784, "start": 95.80000000000001, "end": 97.08000000000001, "text": " x4.", "tokens": [50364, 583, 498, 291, 645, 15669, 341, 2316, 365, 754, 544, 4122, 11, 584, 321, 632, 2031, 16, 264, 2744, 295, 264, 50614, 50614, 1782, 11, 1230, 295, 39955, 11, 264, 1230, 295, 21008, 295, 264, 1782, 11, 611, 264, 3205, 295, 264, 1280, 293, 50838, 50838, 924, 11, 550, 309, 3643, 709, 6081, 281, 7542, 283, 11, 570, 283, 307, 586, 257, 2445, 295, 2031, 16, 807, 51262, 51262, 2031, 19, 13, 51326, 51326, 400, 577, 360, 291, 7542, 257, 1451, 12, 18759, 2445, 30, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.17221209708224522, "compression_ratio": 1.6294416243654823, "no_speech_prob": 5.5942705330380704e-06}, {"id": 21, "seek": 7784, "start": 97.08000000000001, "end": 102.96000000000001, "text": " And how do you plot a four-dimensional function?", "tokens": [50364, 583, 498, 291, 645, 15669, 341, 2316, 365, 754, 544, 4122, 11, 584, 321, 632, 2031, 16, 264, 2744, 295, 264, 50614, 50614, 1782, 11, 1230, 295, 39955, 11, 264, 1230, 295, 21008, 295, 264, 1782, 11, 611, 264, 3205, 295, 264, 1280, 293, 50838, 50838, 924, 11, 550, 309, 3643, 709, 6081, 281, 7542, 283, 11, 570, 283, 307, 586, 257, 2445, 295, 2031, 16, 807, 51262, 51262, 2031, 19, 13, 51326, 51326, 400, 577, 360, 291, 7542, 257, 1451, 12, 18759, 2445, 30, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.17221209708224522, "compression_ratio": 1.6294416243654823, "no_speech_prob": 5.5942705330380704e-06}, {"id": 22, "seek": 10296, "start": 102.96, "end": 108.03999999999999, "text": " So in order to tell if your model is doing well, especially for applications where you", "tokens": [50364, 407, 294, 1668, 281, 980, 498, 428, 2316, 307, 884, 731, 11, 2318, 337, 5821, 689, 291, 50618, 50618, 362, 544, 813, 472, 420, 732, 4122, 11, 597, 1669, 309, 2252, 281, 7542, 283, 295, 2031, 11, 321, 643, 50902, 50902, 512, 544, 27249, 636, 281, 13059, 577, 731, 428, 2316, 307, 884, 13, 51146, 51146, 1692, 311, 257, 6532, 300, 291, 393, 764, 13, 51294, 51294, 759, 291, 362, 257, 3097, 992, 11, 293, 341, 307, 257, 1359, 3097, 992, 365, 445, 1266, 5110, 10052, 51512, 51512, 510, 11, 2831, 813, 1940, 439, 428, 1412, 281, 3847, 264, 9834, 261, 293, 272, 295, 264, 2316, 11, 51843, 51843], "temperature": 0.0, "avg_logprob": -0.0903464566479932, "compression_ratio": 1.6917293233082706, "no_speech_prob": 3.0894511837686878e-06}, {"id": 23, "seek": 10296, "start": 108.03999999999999, "end": 113.72, "text": " have more than one or two features, which makes it difficult to plot f of x, we need", "tokens": [50364, 407, 294, 1668, 281, 980, 498, 428, 2316, 307, 884, 731, 11, 2318, 337, 5821, 689, 291, 50618, 50618, 362, 544, 813, 472, 420, 732, 4122, 11, 597, 1669, 309, 2252, 281, 7542, 283, 295, 2031, 11, 321, 643, 50902, 50902, 512, 544, 27249, 636, 281, 13059, 577, 731, 428, 2316, 307, 884, 13, 51146, 51146, 1692, 311, 257, 6532, 300, 291, 393, 764, 13, 51294, 51294, 759, 291, 362, 257, 3097, 992, 11, 293, 341, 307, 257, 1359, 3097, 992, 365, 445, 1266, 5110, 10052, 51512, 51512, 510, 11, 2831, 813, 1940, 439, 428, 1412, 281, 3847, 264, 9834, 261, 293, 272, 295, 264, 2316, 11, 51843, 51843], "temperature": 0.0, "avg_logprob": -0.0903464566479932, "compression_ratio": 1.6917293233082706, "no_speech_prob": 3.0894511837686878e-06}, {"id": 24, "seek": 10296, "start": 113.72, "end": 118.6, "text": " some more systematic way to evaluate how well your model is doing.", "tokens": [50364, 407, 294, 1668, 281, 980, 498, 428, 2316, 307, 884, 731, 11, 2318, 337, 5821, 689, 291, 50618, 50618, 362, 544, 813, 472, 420, 732, 4122, 11, 597, 1669, 309, 2252, 281, 7542, 283, 295, 2031, 11, 321, 643, 50902, 50902, 512, 544, 27249, 636, 281, 13059, 577, 731, 428, 2316, 307, 884, 13, 51146, 51146, 1692, 311, 257, 6532, 300, 291, 393, 764, 13, 51294, 51294, 759, 291, 362, 257, 3097, 992, 11, 293, 341, 307, 257, 1359, 3097, 992, 365, 445, 1266, 5110, 10052, 51512, 51512, 510, 11, 2831, 813, 1940, 439, 428, 1412, 281, 3847, 264, 9834, 261, 293, 272, 295, 264, 2316, 11, 51843, 51843], "temperature": 0.0, "avg_logprob": -0.0903464566479932, "compression_ratio": 1.6917293233082706, "no_speech_prob": 3.0894511837686878e-06}, {"id": 25, "seek": 10296, "start": 118.6, "end": 121.56, "text": " Here's a technique that you can use.", "tokens": [50364, 407, 294, 1668, 281, 980, 498, 428, 2316, 307, 884, 731, 11, 2318, 337, 5821, 689, 291, 50618, 50618, 362, 544, 813, 472, 420, 732, 4122, 11, 597, 1669, 309, 2252, 281, 7542, 283, 295, 2031, 11, 321, 643, 50902, 50902, 512, 544, 27249, 636, 281, 13059, 577, 731, 428, 2316, 307, 884, 13, 51146, 51146, 1692, 311, 257, 6532, 300, 291, 393, 764, 13, 51294, 51294, 759, 291, 362, 257, 3097, 992, 11, 293, 341, 307, 257, 1359, 3097, 992, 365, 445, 1266, 5110, 10052, 51512, 51512, 510, 11, 2831, 813, 1940, 439, 428, 1412, 281, 3847, 264, 9834, 261, 293, 272, 295, 264, 2316, 11, 51843, 51843], "temperature": 0.0, "avg_logprob": -0.0903464566479932, "compression_ratio": 1.6917293233082706, "no_speech_prob": 3.0894511837686878e-06}, {"id": 26, "seek": 10296, "start": 121.56, "end": 125.91999999999999, "text": " If you have a training set, and this is a small training set with just 10 examples listed", "tokens": [50364, 407, 294, 1668, 281, 980, 498, 428, 2316, 307, 884, 731, 11, 2318, 337, 5821, 689, 291, 50618, 50618, 362, 544, 813, 472, 420, 732, 4122, 11, 597, 1669, 309, 2252, 281, 7542, 283, 295, 2031, 11, 321, 643, 50902, 50902, 512, 544, 27249, 636, 281, 13059, 577, 731, 428, 2316, 307, 884, 13, 51146, 51146, 1692, 311, 257, 6532, 300, 291, 393, 764, 13, 51294, 51294, 759, 291, 362, 257, 3097, 992, 11, 293, 341, 307, 257, 1359, 3097, 992, 365, 445, 1266, 5110, 10052, 51512, 51512, 510, 11, 2831, 813, 1940, 439, 428, 1412, 281, 3847, 264, 9834, 261, 293, 272, 295, 264, 2316, 11, 51843, 51843], "temperature": 0.0, "avg_logprob": -0.0903464566479932, "compression_ratio": 1.6917293233082706, "no_speech_prob": 3.0894511837686878e-06}, {"id": 27, "seek": 10296, "start": 125.91999999999999, "end": 132.54, "text": " here, rather than taking all your data to train the parameters w and b of the model,", "tokens": [50364, 407, 294, 1668, 281, 980, 498, 428, 2316, 307, 884, 731, 11, 2318, 337, 5821, 689, 291, 50618, 50618, 362, 544, 813, 472, 420, 732, 4122, 11, 597, 1669, 309, 2252, 281, 7542, 283, 295, 2031, 11, 321, 643, 50902, 50902, 512, 544, 27249, 636, 281, 13059, 577, 731, 428, 2316, 307, 884, 13, 51146, 51146, 1692, 311, 257, 6532, 300, 291, 393, 764, 13, 51294, 51294, 759, 291, 362, 257, 3097, 992, 11, 293, 341, 307, 257, 1359, 3097, 992, 365, 445, 1266, 5110, 10052, 51512, 51512, 510, 11, 2831, 813, 1940, 439, 428, 1412, 281, 3847, 264, 9834, 261, 293, 272, 295, 264, 2316, 11, 51843, 51843], "temperature": 0.0, "avg_logprob": -0.0903464566479932, "compression_ratio": 1.6917293233082706, "no_speech_prob": 3.0894511837686878e-06}, {"id": 28, "seek": 13254, "start": 132.54, "end": 136.07999999999998, "text": " you can instead split the training set into two subsets.", "tokens": [50364, 291, 393, 2602, 7472, 264, 3097, 992, 666, 732, 2090, 1385, 13, 50541, 50541, 286, 478, 516, 281, 2642, 257, 1622, 510, 13, 50655, 50655, 400, 718, 311, 829, 5285, 4, 295, 264, 1412, 666, 264, 700, 644, 13, 50961, 50961, 400, 286, 478, 516, 281, 818, 300, 264, 3097, 992, 13, 51125, 51125, 400, 264, 1150, 644, 295, 264, 1412, 11, 718, 311, 584, 2217, 4, 295, 264, 1412, 11, 286, 478, 516, 281, 829, 666, 257, 1500, 51431, 51431, 992, 13, 51521, 51521, 400, 437, 321, 434, 516, 281, 360, 307, 3847, 264, 2316, 311, 9834, 322, 264, 3097, 992, 322, 341, 700, 51783, 51783], "temperature": 0.0, "avg_logprob": -0.11067532180646143, "compression_ratio": 1.9270833333333333, "no_speech_prob": 8.267578778031748e-06}, {"id": 29, "seek": 13254, "start": 136.07999999999998, "end": 138.35999999999999, "text": " I'm going to draw a line here.", "tokens": [50364, 291, 393, 2602, 7472, 264, 3097, 992, 666, 732, 2090, 1385, 13, 50541, 50541, 286, 478, 516, 281, 2642, 257, 1622, 510, 13, 50655, 50655, 400, 718, 311, 829, 5285, 4, 295, 264, 1412, 666, 264, 700, 644, 13, 50961, 50961, 400, 286, 478, 516, 281, 818, 300, 264, 3097, 992, 13, 51125, 51125, 400, 264, 1150, 644, 295, 264, 1412, 11, 718, 311, 584, 2217, 4, 295, 264, 1412, 11, 286, 478, 516, 281, 829, 666, 257, 1500, 51431, 51431, 992, 13, 51521, 51521, 400, 437, 321, 434, 516, 281, 360, 307, 3847, 264, 2316, 311, 9834, 322, 264, 3097, 992, 322, 341, 700, 51783, 51783], "temperature": 0.0, "avg_logprob": -0.11067532180646143, "compression_ratio": 1.9270833333333333, "no_speech_prob": 8.267578778031748e-06}, {"id": 30, "seek": 13254, "start": 138.35999999999999, "end": 144.48, "text": " And let's put 70% of the data into the first part.", "tokens": [50364, 291, 393, 2602, 7472, 264, 3097, 992, 666, 732, 2090, 1385, 13, 50541, 50541, 286, 478, 516, 281, 2642, 257, 1622, 510, 13, 50655, 50655, 400, 718, 311, 829, 5285, 4, 295, 264, 1412, 666, 264, 700, 644, 13, 50961, 50961, 400, 286, 478, 516, 281, 818, 300, 264, 3097, 992, 13, 51125, 51125, 400, 264, 1150, 644, 295, 264, 1412, 11, 718, 311, 584, 2217, 4, 295, 264, 1412, 11, 286, 478, 516, 281, 829, 666, 257, 1500, 51431, 51431, 992, 13, 51521, 51521, 400, 437, 321, 434, 516, 281, 360, 307, 3847, 264, 2316, 311, 9834, 322, 264, 3097, 992, 322, 341, 700, 51783, 51783], "temperature": 0.0, "avg_logprob": -0.11067532180646143, "compression_ratio": 1.9270833333333333, "no_speech_prob": 8.267578778031748e-06}, {"id": 31, "seek": 13254, "start": 144.48, "end": 147.76, "text": " And I'm going to call that the training set.", "tokens": [50364, 291, 393, 2602, 7472, 264, 3097, 992, 666, 732, 2090, 1385, 13, 50541, 50541, 286, 478, 516, 281, 2642, 257, 1622, 510, 13, 50655, 50655, 400, 718, 311, 829, 5285, 4, 295, 264, 1412, 666, 264, 700, 644, 13, 50961, 50961, 400, 286, 478, 516, 281, 818, 300, 264, 3097, 992, 13, 51125, 51125, 400, 264, 1150, 644, 295, 264, 1412, 11, 718, 311, 584, 2217, 4, 295, 264, 1412, 11, 286, 478, 516, 281, 829, 666, 257, 1500, 51431, 51431, 992, 13, 51521, 51521, 400, 437, 321, 434, 516, 281, 360, 307, 3847, 264, 2316, 311, 9834, 322, 264, 3097, 992, 322, 341, 700, 51783, 51783], "temperature": 0.0, "avg_logprob": -0.11067532180646143, "compression_ratio": 1.9270833333333333, "no_speech_prob": 8.267578778031748e-06}, {"id": 32, "seek": 13254, "start": 147.76, "end": 153.88, "text": " And the second part of the data, let's say 30% of the data, I'm going to put into a test", "tokens": [50364, 291, 393, 2602, 7472, 264, 3097, 992, 666, 732, 2090, 1385, 13, 50541, 50541, 286, 478, 516, 281, 2642, 257, 1622, 510, 13, 50655, 50655, 400, 718, 311, 829, 5285, 4, 295, 264, 1412, 666, 264, 700, 644, 13, 50961, 50961, 400, 286, 478, 516, 281, 818, 300, 264, 3097, 992, 13, 51125, 51125, 400, 264, 1150, 644, 295, 264, 1412, 11, 718, 311, 584, 2217, 4, 295, 264, 1412, 11, 286, 478, 516, 281, 829, 666, 257, 1500, 51431, 51431, 992, 13, 51521, 51521, 400, 437, 321, 434, 516, 281, 360, 307, 3847, 264, 2316, 311, 9834, 322, 264, 3097, 992, 322, 341, 700, 51783, 51783], "temperature": 0.0, "avg_logprob": -0.11067532180646143, "compression_ratio": 1.9270833333333333, "no_speech_prob": 8.267578778031748e-06}, {"id": 33, "seek": 13254, "start": 153.88, "end": 155.68, "text": " set.", "tokens": [50364, 291, 393, 2602, 7472, 264, 3097, 992, 666, 732, 2090, 1385, 13, 50541, 50541, 286, 478, 516, 281, 2642, 257, 1622, 510, 13, 50655, 50655, 400, 718, 311, 829, 5285, 4, 295, 264, 1412, 666, 264, 700, 644, 13, 50961, 50961, 400, 286, 478, 516, 281, 818, 300, 264, 3097, 992, 13, 51125, 51125, 400, 264, 1150, 644, 295, 264, 1412, 11, 718, 311, 584, 2217, 4, 295, 264, 1412, 11, 286, 478, 516, 281, 829, 666, 257, 1500, 51431, 51431, 992, 13, 51521, 51521, 400, 437, 321, 434, 516, 281, 360, 307, 3847, 264, 2316, 311, 9834, 322, 264, 3097, 992, 322, 341, 700, 51783, 51783], "temperature": 0.0, "avg_logprob": -0.11067532180646143, "compression_ratio": 1.9270833333333333, "no_speech_prob": 8.267578778031748e-06}, {"id": 34, "seek": 13254, "start": 155.68, "end": 160.92, "text": " And what we're going to do is train the model's parameters on the training set on this first", "tokens": [50364, 291, 393, 2602, 7472, 264, 3097, 992, 666, 732, 2090, 1385, 13, 50541, 50541, 286, 478, 516, 281, 2642, 257, 1622, 510, 13, 50655, 50655, 400, 718, 311, 829, 5285, 4, 295, 264, 1412, 666, 264, 700, 644, 13, 50961, 50961, 400, 286, 478, 516, 281, 818, 300, 264, 3097, 992, 13, 51125, 51125, 400, 264, 1150, 644, 295, 264, 1412, 11, 718, 311, 584, 2217, 4, 295, 264, 1412, 11, 286, 478, 516, 281, 829, 666, 257, 1500, 51431, 51431, 992, 13, 51521, 51521, 400, 437, 321, 434, 516, 281, 360, 307, 3847, 264, 2316, 311, 9834, 322, 264, 3097, 992, 322, 341, 700, 51783, 51783], "temperature": 0.0, "avg_logprob": -0.11067532180646143, "compression_ratio": 1.9270833333333333, "no_speech_prob": 8.267578778031748e-06}, {"id": 35, "seek": 16092, "start": 160.92, "end": 168.07999999999998, "text": " 70% or so of the data, and then we'll test this performance on this test set.", "tokens": [50364, 5285, 4, 420, 370, 295, 264, 1412, 11, 293, 550, 321, 603, 1500, 341, 3389, 322, 341, 1500, 992, 13, 50722, 50722, 682, 24657, 11, 286, 478, 516, 281, 764, 2031, 16, 11, 288, 16, 11, 912, 382, 949, 11, 281, 45708, 264, 3097, 5110, 51176, 51176, 807, 2031, 76, 11, 288, 76, 11, 3993, 300, 586, 281, 652, 13691, 13, 51642, 51642], "temperature": 0.0, "avg_logprob": -0.15982459141657904, "compression_ratio": 1.3831168831168832, "no_speech_prob": 1.7330337414023234e-06}, {"id": 36, "seek": 16092, "start": 168.07999999999998, "end": 177.16, "text": " In notation, I'm going to use x1, y1, same as before, to denote the training examples", "tokens": [50364, 5285, 4, 420, 370, 295, 264, 1412, 11, 293, 550, 321, 603, 1500, 341, 3389, 322, 341, 1500, 992, 13, 50722, 50722, 682, 24657, 11, 286, 478, 516, 281, 764, 2031, 16, 11, 288, 16, 11, 912, 382, 949, 11, 281, 45708, 264, 3097, 5110, 51176, 51176, 807, 2031, 76, 11, 288, 76, 11, 3993, 300, 586, 281, 652, 13691, 13, 51642, 51642], "temperature": 0.0, "avg_logprob": -0.15982459141657904, "compression_ratio": 1.3831168831168832, "no_speech_prob": 1.7330337414023234e-06}, {"id": 37, "seek": 16092, "start": 177.16, "end": 186.48, "text": " through xm, ym, except that now to make explicit.", "tokens": [50364, 5285, 4, 420, 370, 295, 264, 1412, 11, 293, 550, 321, 603, 1500, 341, 3389, 322, 341, 1500, 992, 13, 50722, 50722, 682, 24657, 11, 286, 478, 516, 281, 764, 2031, 16, 11, 288, 16, 11, 912, 382, 949, 11, 281, 45708, 264, 3097, 5110, 51176, 51176, 807, 2031, 76, 11, 288, 76, 11, 3993, 300, 586, 281, 652, 13691, 13, 51642, 51642], "temperature": 0.0, "avg_logprob": -0.15982459141657904, "compression_ratio": 1.3831168831168832, "no_speech_prob": 1.7330337414023234e-06}, {"id": 38, "seek": 18648, "start": 186.48, "end": 190.88, "text": " So in this little example, we would have seven training examples.", "tokens": [50364, 407, 294, 341, 707, 1365, 11, 321, 576, 362, 3407, 3097, 5110, 13, 50584, 50584, 400, 281, 5366, 472, 777, 2522, 295, 24657, 11, 286, 478, 516, 281, 764, 275, 2325, 662, 3847, 13, 50908, 50908, 275, 3847, 307, 257, 1230, 295, 3097, 5110, 11, 597, 294, 341, 1359, 1412, 992, 307, 3407, 13, 51201, 51201, 407, 264, 2325, 662, 3847, 445, 48856, 498, 321, 434, 1237, 412, 264, 3097, 992, 8044, 51448, 51448, 295, 264, 1412, 13, 51538, 51538], "temperature": 0.0, "avg_logprob": -0.1380987632565382, "compression_ratio": 1.6649484536082475, "no_speech_prob": 2.812925004036515e-06}, {"id": 39, "seek": 18648, "start": 190.88, "end": 197.35999999999999, "text": " And to introduce one new piece of notation, I'm going to use m subscript train.", "tokens": [50364, 407, 294, 341, 707, 1365, 11, 321, 576, 362, 3407, 3097, 5110, 13, 50584, 50584, 400, 281, 5366, 472, 777, 2522, 295, 24657, 11, 286, 478, 516, 281, 764, 275, 2325, 662, 3847, 13, 50908, 50908, 275, 3847, 307, 257, 1230, 295, 3097, 5110, 11, 597, 294, 341, 1359, 1412, 992, 307, 3407, 13, 51201, 51201, 407, 264, 2325, 662, 3847, 445, 48856, 498, 321, 434, 1237, 412, 264, 3097, 992, 8044, 51448, 51448, 295, 264, 1412, 13, 51538, 51538], "temperature": 0.0, "avg_logprob": -0.1380987632565382, "compression_ratio": 1.6649484536082475, "no_speech_prob": 2.812925004036515e-06}, {"id": 40, "seek": 18648, "start": 197.35999999999999, "end": 203.22, "text": " m train is a number of training examples, which in this small data set is seven.", "tokens": [50364, 407, 294, 341, 707, 1365, 11, 321, 576, 362, 3407, 3097, 5110, 13, 50584, 50584, 400, 281, 5366, 472, 777, 2522, 295, 24657, 11, 286, 478, 516, 281, 764, 275, 2325, 662, 3847, 13, 50908, 50908, 275, 3847, 307, 257, 1230, 295, 3097, 5110, 11, 597, 294, 341, 1359, 1412, 992, 307, 3407, 13, 51201, 51201, 407, 264, 2325, 662, 3847, 445, 48856, 498, 321, 434, 1237, 412, 264, 3097, 992, 8044, 51448, 51448, 295, 264, 1412, 13, 51538, 51538], "temperature": 0.0, "avg_logprob": -0.1380987632565382, "compression_ratio": 1.6649484536082475, "no_speech_prob": 2.812925004036515e-06}, {"id": 41, "seek": 18648, "start": 203.22, "end": 208.16, "text": " So the subscript train just emphasizes if we're looking at the training set portion", "tokens": [50364, 407, 294, 341, 707, 1365, 11, 321, 576, 362, 3407, 3097, 5110, 13, 50584, 50584, 400, 281, 5366, 472, 777, 2522, 295, 24657, 11, 286, 478, 516, 281, 764, 275, 2325, 662, 3847, 13, 50908, 50908, 275, 3847, 307, 257, 1230, 295, 3097, 5110, 11, 597, 294, 341, 1359, 1412, 992, 307, 3407, 13, 51201, 51201, 407, 264, 2325, 662, 3847, 445, 48856, 498, 321, 434, 1237, 412, 264, 3097, 992, 8044, 51448, 51448, 295, 264, 1412, 13, 51538, 51538], "temperature": 0.0, "avg_logprob": -0.1380987632565382, "compression_ratio": 1.6649484536082475, "no_speech_prob": 2.812925004036515e-06}, {"id": 42, "seek": 18648, "start": 208.16, "end": 209.95999999999998, "text": " of the data.", "tokens": [50364, 407, 294, 341, 707, 1365, 11, 321, 576, 362, 3407, 3097, 5110, 13, 50584, 50584, 400, 281, 5366, 472, 777, 2522, 295, 24657, 11, 286, 478, 516, 281, 764, 275, 2325, 662, 3847, 13, 50908, 50908, 275, 3847, 307, 257, 1230, 295, 3097, 5110, 11, 597, 294, 341, 1359, 1412, 992, 307, 3407, 13, 51201, 51201, 407, 264, 2325, 662, 3847, 445, 48856, 498, 321, 434, 1237, 412, 264, 3097, 992, 8044, 51448, 51448, 295, 264, 1412, 13, 51538, 51538], "temperature": 0.0, "avg_logprob": -0.1380987632565382, "compression_ratio": 1.6649484536082475, "no_speech_prob": 2.812925004036515e-06}, {"id": 43, "seek": 20996, "start": 209.96, "end": 218.32, "text": " And for the test sets, I'm going to use the notation x1 subscript test, y1 subscript test", "tokens": [50364, 400, 337, 264, 1500, 6352, 11, 286, 478, 516, 281, 764, 264, 24657, 2031, 16, 2325, 662, 1500, 11, 288, 16, 2325, 662, 1500, 50782, 50782, 281, 45708, 264, 700, 1500, 1365, 13, 50914, 50914, 400, 341, 1709, 439, 264, 636, 281, 2031, 76, 1500, 11, 2325, 662, 1500, 11, 288, 76, 1500, 11, 2325, 662, 1500, 13, 51414, 51414, 400, 275, 1500, 307, 264, 1230, 295, 1500, 5110, 11, 597, 294, 341, 1389, 307, 1045, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.12195066758144049, "compression_ratio": 1.8266666666666667, "no_speech_prob": 3.5007399219466606e-06}, {"id": 44, "seek": 20996, "start": 218.32, "end": 220.96, "text": " to denote the first test example.", "tokens": [50364, 400, 337, 264, 1500, 6352, 11, 286, 478, 516, 281, 764, 264, 24657, 2031, 16, 2325, 662, 1500, 11, 288, 16, 2325, 662, 1500, 50782, 50782, 281, 45708, 264, 700, 1500, 1365, 13, 50914, 50914, 400, 341, 1709, 439, 264, 636, 281, 2031, 76, 1500, 11, 2325, 662, 1500, 11, 288, 76, 1500, 11, 2325, 662, 1500, 13, 51414, 51414, 400, 275, 1500, 307, 264, 1230, 295, 1500, 5110, 11, 597, 294, 341, 1389, 307, 1045, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.12195066758144049, "compression_ratio": 1.8266666666666667, "no_speech_prob": 3.5007399219466606e-06}, {"id": 45, "seek": 20996, "start": 220.96, "end": 230.96, "text": " And this goes all the way to xm test, subscript test, ym test, subscript test.", "tokens": [50364, 400, 337, 264, 1500, 6352, 11, 286, 478, 516, 281, 764, 264, 24657, 2031, 16, 2325, 662, 1500, 11, 288, 16, 2325, 662, 1500, 50782, 50782, 281, 45708, 264, 700, 1500, 1365, 13, 50914, 50914, 400, 341, 1709, 439, 264, 636, 281, 2031, 76, 1500, 11, 2325, 662, 1500, 11, 288, 76, 1500, 11, 2325, 662, 1500, 13, 51414, 51414, 400, 275, 1500, 307, 264, 1230, 295, 1500, 5110, 11, 597, 294, 341, 1389, 307, 1045, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.12195066758144049, "compression_ratio": 1.8266666666666667, "no_speech_prob": 3.5007399219466606e-06}, {"id": 46, "seek": 20996, "start": 230.96, "end": 236.20000000000002, "text": " And m test is the number of test examples, which in this case is three.", "tokens": [50364, 400, 337, 264, 1500, 6352, 11, 286, 478, 516, 281, 764, 264, 24657, 2031, 16, 2325, 662, 1500, 11, 288, 16, 2325, 662, 1500, 50782, 50782, 281, 45708, 264, 700, 1500, 1365, 13, 50914, 50914, 400, 341, 1709, 439, 264, 636, 281, 2031, 76, 1500, 11, 2325, 662, 1500, 11, 288, 76, 1500, 11, 2325, 662, 1500, 13, 51414, 51414, 400, 275, 1500, 307, 264, 1230, 295, 1500, 5110, 11, 597, 294, 341, 1389, 307, 1045, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.12195066758144049, "compression_ratio": 1.8266666666666667, "no_speech_prob": 3.5007399219466606e-06}, {"id": 47, "seek": 23620, "start": 236.2, "end": 243.6, "text": " And it's not uncommon to split your data set according to maybe a 70-30 split or 80-20", "tokens": [50364, 400, 309, 311, 406, 29289, 281, 7472, 428, 1412, 992, 4650, 281, 1310, 257, 5285, 12, 3446, 7472, 420, 4688, 12, 2009, 50734, 50734, 7472, 365, 881, 295, 428, 1412, 516, 666, 264, 3097, 992, 293, 550, 257, 4356, 14135, 516, 50974, 50974, 666, 264, 1500, 992, 13, 51088, 51088, 407, 294, 1668, 281, 3847, 257, 2316, 293, 13059, 309, 11, 341, 307, 437, 309, 576, 574, 411, 498, 291, 434, 51395, 51395, 1228, 8213, 24590, 365, 257, 8889, 6713, 2063, 13, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.13770722233971885, "compression_ratio": 1.5727699530516432, "no_speech_prob": 6.786676749470644e-07}, {"id": 48, "seek": 23620, "start": 243.6, "end": 248.39999999999998, "text": " split with most of your data going into the training set and then a smaller fraction going", "tokens": [50364, 400, 309, 311, 406, 29289, 281, 7472, 428, 1412, 992, 4650, 281, 1310, 257, 5285, 12, 3446, 7472, 420, 4688, 12, 2009, 50734, 50734, 7472, 365, 881, 295, 428, 1412, 516, 666, 264, 3097, 992, 293, 550, 257, 4356, 14135, 516, 50974, 50974, 666, 264, 1500, 992, 13, 51088, 51088, 407, 294, 1668, 281, 3847, 257, 2316, 293, 13059, 309, 11, 341, 307, 437, 309, 576, 574, 411, 498, 291, 434, 51395, 51395, 1228, 8213, 24590, 365, 257, 8889, 6713, 2063, 13, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.13770722233971885, "compression_ratio": 1.5727699530516432, "no_speech_prob": 6.786676749470644e-07}, {"id": 49, "seek": 23620, "start": 248.39999999999998, "end": 250.67999999999998, "text": " into the test set.", "tokens": [50364, 400, 309, 311, 406, 29289, 281, 7472, 428, 1412, 992, 4650, 281, 1310, 257, 5285, 12, 3446, 7472, 420, 4688, 12, 2009, 50734, 50734, 7472, 365, 881, 295, 428, 1412, 516, 666, 264, 3097, 992, 293, 550, 257, 4356, 14135, 516, 50974, 50974, 666, 264, 1500, 992, 13, 51088, 51088, 407, 294, 1668, 281, 3847, 257, 2316, 293, 13059, 309, 11, 341, 307, 437, 309, 576, 574, 411, 498, 291, 434, 51395, 51395, 1228, 8213, 24590, 365, 257, 8889, 6713, 2063, 13, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.13770722233971885, "compression_ratio": 1.5727699530516432, "no_speech_prob": 6.786676749470644e-07}, {"id": 50, "seek": 23620, "start": 250.67999999999998, "end": 256.82, "text": " So in order to train a model and evaluate it, this is what it would look like if you're", "tokens": [50364, 400, 309, 311, 406, 29289, 281, 7472, 428, 1412, 992, 4650, 281, 1310, 257, 5285, 12, 3446, 7472, 420, 4688, 12, 2009, 50734, 50734, 7472, 365, 881, 295, 428, 1412, 516, 666, 264, 3097, 992, 293, 550, 257, 4356, 14135, 516, 50974, 50974, 666, 264, 1500, 992, 13, 51088, 51088, 407, 294, 1668, 281, 3847, 257, 2316, 293, 13059, 309, 11, 341, 307, 437, 309, 576, 574, 411, 498, 291, 434, 51395, 51395, 1228, 8213, 24590, 365, 257, 8889, 6713, 2063, 13, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.13770722233971885, "compression_ratio": 1.5727699530516432, "no_speech_prob": 6.786676749470644e-07}, {"id": 51, "seek": 23620, "start": 256.82, "end": 261.4, "text": " using linear regression with a squared error cost.", "tokens": [50364, 400, 309, 311, 406, 29289, 281, 7472, 428, 1412, 992, 4650, 281, 1310, 257, 5285, 12, 3446, 7472, 420, 4688, 12, 2009, 50734, 50734, 7472, 365, 881, 295, 428, 1412, 516, 666, 264, 3097, 992, 293, 550, 257, 4356, 14135, 516, 50974, 50974, 666, 264, 1500, 992, 13, 51088, 51088, 407, 294, 1668, 281, 3847, 257, 2316, 293, 13059, 309, 11, 341, 307, 437, 309, 576, 574, 411, 498, 291, 434, 51395, 51395, 1228, 8213, 24590, 365, 257, 8889, 6713, 2063, 13, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.13770722233971885, "compression_ratio": 1.5727699530516432, "no_speech_prob": 6.786676749470644e-07}, {"id": 52, "seek": 26140, "start": 261.4, "end": 266.44, "text": " Start off by fitting the parameters by minimizing the cost function j of wb.", "tokens": [50364, 6481, 766, 538, 15669, 264, 9834, 538, 46608, 264, 2063, 2445, 361, 295, 261, 65, 13, 50616, 50616, 407, 341, 307, 257, 7713, 2063, 2445, 13, 50756, 50756, 2829, 43890, 670, 261, 65, 295, 341, 8889, 6713, 2063, 1804, 3890, 2144, 1433, 11, 2854, 670, 568, 76, 1413, 51146, 51146, 2408, 295, 264, 261, 73, 8889, 13, 51358, 51358, 400, 550, 281, 980, 577, 731, 341, 2316, 307, 884, 11, 291, 576, 14722, 361, 1500, 295, 261, 65, 11, 597, 307, 2681, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.10869582863741142, "compression_ratio": 1.5665024630541873, "no_speech_prob": 2.6425741452840157e-06}, {"id": 53, "seek": 26140, "start": 266.44, "end": 269.23999999999995, "text": " So this is a usual cost function.", "tokens": [50364, 6481, 766, 538, 15669, 264, 9834, 538, 46608, 264, 2063, 2445, 361, 295, 261, 65, 13, 50616, 50616, 407, 341, 307, 257, 7713, 2063, 2445, 13, 50756, 50756, 2829, 43890, 670, 261, 65, 295, 341, 8889, 6713, 2063, 1804, 3890, 2144, 1433, 11, 2854, 670, 568, 76, 1413, 51146, 51146, 2408, 295, 264, 261, 73, 8889, 13, 51358, 51358, 400, 550, 281, 980, 577, 731, 341, 2316, 307, 884, 11, 291, 576, 14722, 361, 1500, 295, 261, 65, 11, 597, 307, 2681, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.10869582863741142, "compression_ratio": 1.5665024630541873, "no_speech_prob": 2.6425741452840157e-06}, {"id": 54, "seek": 26140, "start": 269.23999999999995, "end": 277.03999999999996, "text": " Minimize over wb of this squared error cost plus regularization term, longer over 2m times", "tokens": [50364, 6481, 766, 538, 15669, 264, 9834, 538, 46608, 264, 2063, 2445, 361, 295, 261, 65, 13, 50616, 50616, 407, 341, 307, 257, 7713, 2063, 2445, 13, 50756, 50756, 2829, 43890, 670, 261, 65, 295, 341, 8889, 6713, 2063, 1804, 3890, 2144, 1433, 11, 2854, 670, 568, 76, 1413, 51146, 51146, 2408, 295, 264, 261, 73, 8889, 13, 51358, 51358, 400, 550, 281, 980, 577, 731, 341, 2316, 307, 884, 11, 291, 576, 14722, 361, 1500, 295, 261, 65, 11, 597, 307, 2681, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.10869582863741142, "compression_ratio": 1.5665024630541873, "no_speech_prob": 2.6425741452840157e-06}, {"id": 55, "seek": 26140, "start": 277.03999999999996, "end": 281.28, "text": " sum of the wj squared.", "tokens": [50364, 6481, 766, 538, 15669, 264, 9834, 538, 46608, 264, 2063, 2445, 361, 295, 261, 65, 13, 50616, 50616, 407, 341, 307, 257, 7713, 2063, 2445, 13, 50756, 50756, 2829, 43890, 670, 261, 65, 295, 341, 8889, 6713, 2063, 1804, 3890, 2144, 1433, 11, 2854, 670, 568, 76, 1413, 51146, 51146, 2408, 295, 264, 261, 73, 8889, 13, 51358, 51358, 400, 550, 281, 980, 577, 731, 341, 2316, 307, 884, 11, 291, 576, 14722, 361, 1500, 295, 261, 65, 11, 597, 307, 2681, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.10869582863741142, "compression_ratio": 1.5665024630541873, "no_speech_prob": 2.6425741452840157e-06}, {"id": 56, "seek": 26140, "start": 281.28, "end": 289.79999999999995, "text": " And then to tell how well this model is doing, you would compute j test of wb, which is equal", "tokens": [50364, 6481, 766, 538, 15669, 264, 9834, 538, 46608, 264, 2063, 2445, 361, 295, 261, 65, 13, 50616, 50616, 407, 341, 307, 257, 7713, 2063, 2445, 13, 50756, 50756, 2829, 43890, 670, 261, 65, 295, 341, 8889, 6713, 2063, 1804, 3890, 2144, 1433, 11, 2854, 670, 568, 76, 1413, 51146, 51146, 2408, 295, 264, 261, 73, 8889, 13, 51358, 51358, 400, 550, 281, 980, 577, 731, 341, 2316, 307, 884, 11, 291, 576, 14722, 361, 1500, 295, 261, 65, 11, 597, 307, 2681, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.10869582863741142, "compression_ratio": 1.5665024630541873, "no_speech_prob": 2.6425741452840157e-06}, {"id": 57, "seek": 28980, "start": 289.8, "end": 293.16, "text": " to the average error on the test set.", "tokens": [50364, 281, 264, 4274, 6713, 322, 264, 1500, 992, 13, 50532, 50532, 400, 300, 311, 445, 2681, 281, 502, 670, 568, 1413, 275, 1500, 13, 50756, 50756, 663, 311, 264, 1230, 295, 1500, 5110, 13, 50866, 50866, 400, 550, 295, 2408, 670, 439, 264, 5110, 490, 741, 6915, 502, 281, 264, 1230, 295, 1500, 5110, 51158, 51158, 295, 264, 8889, 6713, 322, 1184, 295, 264, 1500, 5110, 411, 370, 13, 51394, 51394, 407, 309, 311, 257, 17630, 322, 264, 741, 12, 392, 1500, 1365, 4846, 3175, 264, 3539, 3218, 295, 264, 1782, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.14402808139198706, "compression_ratio": 1.877659574468085, "no_speech_prob": 6.33908075542422e-06}, {"id": 58, "seek": 28980, "start": 293.16, "end": 297.64, "text": " And that's just equal to 1 over 2 times m test.", "tokens": [50364, 281, 264, 4274, 6713, 322, 264, 1500, 992, 13, 50532, 50532, 400, 300, 311, 445, 2681, 281, 502, 670, 568, 1413, 275, 1500, 13, 50756, 50756, 663, 311, 264, 1230, 295, 1500, 5110, 13, 50866, 50866, 400, 550, 295, 2408, 670, 439, 264, 5110, 490, 741, 6915, 502, 281, 264, 1230, 295, 1500, 5110, 51158, 51158, 295, 264, 8889, 6713, 322, 1184, 295, 264, 1500, 5110, 411, 370, 13, 51394, 51394, 407, 309, 311, 257, 17630, 322, 264, 741, 12, 392, 1500, 1365, 4846, 3175, 264, 3539, 3218, 295, 264, 1782, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.14402808139198706, "compression_ratio": 1.877659574468085, "no_speech_prob": 6.33908075542422e-06}, {"id": 59, "seek": 28980, "start": 297.64, "end": 299.84000000000003, "text": " That's the number of test examples.", "tokens": [50364, 281, 264, 4274, 6713, 322, 264, 1500, 992, 13, 50532, 50532, 400, 300, 311, 445, 2681, 281, 502, 670, 568, 1413, 275, 1500, 13, 50756, 50756, 663, 311, 264, 1230, 295, 1500, 5110, 13, 50866, 50866, 400, 550, 295, 2408, 670, 439, 264, 5110, 490, 741, 6915, 502, 281, 264, 1230, 295, 1500, 5110, 51158, 51158, 295, 264, 8889, 6713, 322, 1184, 295, 264, 1500, 5110, 411, 370, 13, 51394, 51394, 407, 309, 311, 257, 17630, 322, 264, 741, 12, 392, 1500, 1365, 4846, 3175, 264, 3539, 3218, 295, 264, 1782, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.14402808139198706, "compression_ratio": 1.877659574468085, "no_speech_prob": 6.33908075542422e-06}, {"id": 60, "seek": 28980, "start": 299.84000000000003, "end": 305.68, "text": " And then of sum over all the examples from i equals 1 to the number of test examples", "tokens": [50364, 281, 264, 4274, 6713, 322, 264, 1500, 992, 13, 50532, 50532, 400, 300, 311, 445, 2681, 281, 502, 670, 568, 1413, 275, 1500, 13, 50756, 50756, 663, 311, 264, 1230, 295, 1500, 5110, 13, 50866, 50866, 400, 550, 295, 2408, 670, 439, 264, 5110, 490, 741, 6915, 502, 281, 264, 1230, 295, 1500, 5110, 51158, 51158, 295, 264, 8889, 6713, 322, 1184, 295, 264, 1500, 5110, 411, 370, 13, 51394, 51394, 407, 309, 311, 257, 17630, 322, 264, 741, 12, 392, 1500, 1365, 4846, 3175, 264, 3539, 3218, 295, 264, 1782, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.14402808139198706, "compression_ratio": 1.877659574468085, "no_speech_prob": 6.33908075542422e-06}, {"id": 61, "seek": 28980, "start": 305.68, "end": 310.40000000000003, "text": " of the squared error on each of the test examples like so.", "tokens": [50364, 281, 264, 4274, 6713, 322, 264, 1500, 992, 13, 50532, 50532, 400, 300, 311, 445, 2681, 281, 502, 670, 568, 1413, 275, 1500, 13, 50756, 50756, 663, 311, 264, 1230, 295, 1500, 5110, 13, 50866, 50866, 400, 550, 295, 2408, 670, 439, 264, 5110, 490, 741, 6915, 502, 281, 264, 1230, 295, 1500, 5110, 51158, 51158, 295, 264, 8889, 6713, 322, 1184, 295, 264, 1500, 5110, 411, 370, 13, 51394, 51394, 407, 309, 311, 257, 17630, 322, 264, 741, 12, 392, 1500, 1365, 4846, 3175, 264, 3539, 3218, 295, 264, 1782, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.14402808139198706, "compression_ratio": 1.877659574468085, "no_speech_prob": 6.33908075542422e-06}, {"id": 62, "seek": 28980, "start": 310.40000000000003, "end": 318.72, "text": " So it's a prediction on the i-th test example input minus the actual price of the house", "tokens": [50364, 281, 264, 4274, 6713, 322, 264, 1500, 992, 13, 50532, 50532, 400, 300, 311, 445, 2681, 281, 502, 670, 568, 1413, 275, 1500, 13, 50756, 50756, 663, 311, 264, 1230, 295, 1500, 5110, 13, 50866, 50866, 400, 550, 295, 2408, 670, 439, 264, 5110, 490, 741, 6915, 502, 281, 264, 1230, 295, 1500, 5110, 51158, 51158, 295, 264, 8889, 6713, 322, 1184, 295, 264, 1500, 5110, 411, 370, 13, 51394, 51394, 407, 309, 311, 257, 17630, 322, 264, 741, 12, 392, 1500, 1365, 4846, 3175, 264, 3539, 3218, 295, 264, 1782, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.14402808139198706, "compression_ratio": 1.877659574468085, "no_speech_prob": 6.33908075542422e-06}, {"id": 63, "seek": 31872, "start": 318.72, "end": 323.32000000000005, "text": " on the i-th test example squared.", "tokens": [50364, 322, 264, 741, 12, 392, 1500, 1365, 8889, 13, 50594, 50594, 400, 3449, 300, 264, 1500, 6713, 8513, 361, 1500, 11, 309, 775, 406, 4090, 300, 3890, 2144, 1433, 13, 50974, 50974, 400, 341, 486, 976, 291, 257, 2020, 295, 577, 731, 428, 2539, 9284, 307, 884, 13, 51218, 51218, 1485, 661, 11275, 300, 311, 2049, 4420, 281, 14722, 382, 731, 307, 264, 3097, 6713, 11, 597, 51474, 51474, 307, 257, 3481, 295, 577, 731, 428, 2539, 9284, 307, 884, 322, 264, 3097, 992, 13, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.09229106045840832, "compression_ratio": 1.8, "no_speech_prob": 2.7264541131444275e-06}, {"id": 64, "seek": 31872, "start": 323.32000000000005, "end": 330.92, "text": " And notice that the test error formula j test, it does not include that regularization term.", "tokens": [50364, 322, 264, 741, 12, 392, 1500, 1365, 8889, 13, 50594, 50594, 400, 3449, 300, 264, 1500, 6713, 8513, 361, 1500, 11, 309, 775, 406, 4090, 300, 3890, 2144, 1433, 13, 50974, 50974, 400, 341, 486, 976, 291, 257, 2020, 295, 577, 731, 428, 2539, 9284, 307, 884, 13, 51218, 51218, 1485, 661, 11275, 300, 311, 2049, 4420, 281, 14722, 382, 731, 307, 264, 3097, 6713, 11, 597, 51474, 51474, 307, 257, 3481, 295, 577, 731, 428, 2539, 9284, 307, 884, 322, 264, 3097, 992, 13, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.09229106045840832, "compression_ratio": 1.8, "no_speech_prob": 2.7264541131444275e-06}, {"id": 65, "seek": 31872, "start": 330.92, "end": 335.8, "text": " And this will give you a sense of how well your learning algorithm is doing.", "tokens": [50364, 322, 264, 741, 12, 392, 1500, 1365, 8889, 13, 50594, 50594, 400, 3449, 300, 264, 1500, 6713, 8513, 361, 1500, 11, 309, 775, 406, 4090, 300, 3890, 2144, 1433, 13, 50974, 50974, 400, 341, 486, 976, 291, 257, 2020, 295, 577, 731, 428, 2539, 9284, 307, 884, 13, 51218, 51218, 1485, 661, 11275, 300, 311, 2049, 4420, 281, 14722, 382, 731, 307, 264, 3097, 6713, 11, 597, 51474, 51474, 307, 257, 3481, 295, 577, 731, 428, 2539, 9284, 307, 884, 322, 264, 3097, 992, 13, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.09229106045840832, "compression_ratio": 1.8, "no_speech_prob": 2.7264541131444275e-06}, {"id": 66, "seek": 31872, "start": 335.8, "end": 340.92, "text": " One other quantity that's often useful to compute as well is the training error, which", "tokens": [50364, 322, 264, 741, 12, 392, 1500, 1365, 8889, 13, 50594, 50594, 400, 3449, 300, 264, 1500, 6713, 8513, 361, 1500, 11, 309, 775, 406, 4090, 300, 3890, 2144, 1433, 13, 50974, 50974, 400, 341, 486, 976, 291, 257, 2020, 295, 577, 731, 428, 2539, 9284, 307, 884, 13, 51218, 51218, 1485, 661, 11275, 300, 311, 2049, 4420, 281, 14722, 382, 731, 307, 264, 3097, 6713, 11, 597, 51474, 51474, 307, 257, 3481, 295, 577, 731, 428, 2539, 9284, 307, 884, 322, 264, 3097, 992, 13, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.09229106045840832, "compression_ratio": 1.8, "no_speech_prob": 2.7264541131444275e-06}, {"id": 67, "seek": 31872, "start": 340.92, "end": 346.32000000000005, "text": " is a measure of how well your learning algorithm is doing on the training set.", "tokens": [50364, 322, 264, 741, 12, 392, 1500, 1365, 8889, 13, 50594, 50594, 400, 3449, 300, 264, 1500, 6713, 8513, 361, 1500, 11, 309, 775, 406, 4090, 300, 3890, 2144, 1433, 13, 50974, 50974, 400, 341, 486, 976, 291, 257, 2020, 295, 577, 731, 428, 2539, 9284, 307, 884, 13, 51218, 51218, 1485, 661, 11275, 300, 311, 2049, 4420, 281, 14722, 382, 731, 307, 264, 3097, 6713, 11, 597, 51474, 51474, 307, 257, 3481, 295, 577, 731, 428, 2539, 9284, 307, 884, 322, 264, 3097, 992, 13, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.09229106045840832, "compression_ratio": 1.8, "no_speech_prob": 2.7264541131444275e-06}, {"id": 68, "seek": 34632, "start": 346.32, "end": 352.84, "text": " So let me define j train of wb to be equal to the average over the training sets 1 over", "tokens": [50364, 407, 718, 385, 6964, 361, 3847, 295, 261, 65, 281, 312, 2681, 281, 264, 4274, 670, 264, 3097, 6352, 502, 670, 50690, 50690, 568, 76, 420, 502, 670, 568, 76, 2325, 662, 3847, 295, 2408, 670, 428, 3097, 6352, 295, 341, 8889, 6713, 1433, 13, 51084, 51084, 400, 1564, 797, 11, 341, 775, 406, 4090, 264, 3890, 2144, 1433, 11, 8343, 264, 2063, 2445, 51342, 51342, 300, 291, 645, 46608, 281, 3318, 264, 9834, 13, 51508, 51508], "temperature": 0.0, "avg_logprob": -0.12514387203168265, "compression_ratio": 1.6269430051813472, "no_speech_prob": 4.88828447942069e-07}, {"id": 69, "seek": 34632, "start": 352.84, "end": 360.71999999999997, "text": " 2m or 1 over 2m subscript train of sum over your training sets of this squared error term.", "tokens": [50364, 407, 718, 385, 6964, 361, 3847, 295, 261, 65, 281, 312, 2681, 281, 264, 4274, 670, 264, 3097, 6352, 502, 670, 50690, 50690, 568, 76, 420, 502, 670, 568, 76, 2325, 662, 3847, 295, 2408, 670, 428, 3097, 6352, 295, 341, 8889, 6713, 1433, 13, 51084, 51084, 400, 1564, 797, 11, 341, 775, 406, 4090, 264, 3890, 2144, 1433, 11, 8343, 264, 2063, 2445, 51342, 51342, 300, 291, 645, 46608, 281, 3318, 264, 9834, 13, 51508, 51508], "temperature": 0.0, "avg_logprob": -0.12514387203168265, "compression_ratio": 1.6269430051813472, "no_speech_prob": 4.88828447942069e-07}, {"id": 70, "seek": 34632, "start": 360.71999999999997, "end": 365.88, "text": " And once again, this does not include the regularization term, unlike the cost function", "tokens": [50364, 407, 718, 385, 6964, 361, 3847, 295, 261, 65, 281, 312, 2681, 281, 264, 4274, 670, 264, 3097, 6352, 502, 670, 50690, 50690, 568, 76, 420, 502, 670, 568, 76, 2325, 662, 3847, 295, 2408, 670, 428, 3097, 6352, 295, 341, 8889, 6713, 1433, 13, 51084, 51084, 400, 1564, 797, 11, 341, 775, 406, 4090, 264, 3890, 2144, 1433, 11, 8343, 264, 2063, 2445, 51342, 51342, 300, 291, 645, 46608, 281, 3318, 264, 9834, 13, 51508, 51508], "temperature": 0.0, "avg_logprob": -0.12514387203168265, "compression_ratio": 1.6269430051813472, "no_speech_prob": 4.88828447942069e-07}, {"id": 71, "seek": 34632, "start": 365.88, "end": 369.2, "text": " that you were minimizing to fit the parameters.", "tokens": [50364, 407, 718, 385, 6964, 361, 3847, 295, 261, 65, 281, 312, 2681, 281, 264, 4274, 670, 264, 3097, 6352, 502, 670, 50690, 50690, 568, 76, 420, 502, 670, 568, 76, 2325, 662, 3847, 295, 2408, 670, 428, 3097, 6352, 295, 341, 8889, 6713, 1433, 13, 51084, 51084, 400, 1564, 797, 11, 341, 775, 406, 4090, 264, 3890, 2144, 1433, 11, 8343, 264, 2063, 2445, 51342, 51342, 300, 291, 645, 46608, 281, 3318, 264, 9834, 13, 51508, 51508], "temperature": 0.0, "avg_logprob": -0.12514387203168265, "compression_ratio": 1.6269430051813472, "no_speech_prob": 4.88828447942069e-07}, {"id": 72, "seek": 36920, "start": 369.2, "end": 377.64, "text": " So in a model like what we saw earlier in this video, j train of wb will be low because", "tokens": [50364, 407, 294, 257, 2316, 411, 437, 321, 1866, 3071, 294, 341, 960, 11, 361, 3847, 295, 261, 65, 486, 312, 2295, 570, 50786, 50786, 264, 4274, 6713, 322, 428, 3097, 5110, 486, 312, 4018, 420, 588, 1998, 281, 4018, 13, 51128, 51128, 407, 361, 3847, 486, 312, 588, 1998, 281, 4018, 13, 51272, 51272, 583, 498, 291, 632, 257, 1326, 4497, 5110, 294, 428, 1500, 992, 300, 264, 9284, 632, 406, 8895, 51502, 51502, 322, 550, 729, 1500, 5110, 1062, 574, 411, 613, 13, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.10018790851939809, "compression_ratio": 1.75, "no_speech_prob": 1.1015897598554147e-06}, {"id": 73, "seek": 36920, "start": 377.64, "end": 384.48, "text": " the average error on your training examples will be zero or very close to zero.", "tokens": [50364, 407, 294, 257, 2316, 411, 437, 321, 1866, 3071, 294, 341, 960, 11, 361, 3847, 295, 261, 65, 486, 312, 2295, 570, 50786, 50786, 264, 4274, 6713, 322, 428, 3097, 5110, 486, 312, 4018, 420, 588, 1998, 281, 4018, 13, 51128, 51128, 407, 361, 3847, 486, 312, 588, 1998, 281, 4018, 13, 51272, 51272, 583, 498, 291, 632, 257, 1326, 4497, 5110, 294, 428, 1500, 992, 300, 264, 9284, 632, 406, 8895, 51502, 51502, 322, 550, 729, 1500, 5110, 1062, 574, 411, 613, 13, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.10018790851939809, "compression_ratio": 1.75, "no_speech_prob": 1.1015897598554147e-06}, {"id": 74, "seek": 36920, "start": 384.48, "end": 387.36, "text": " So j train will be very close to zero.", "tokens": [50364, 407, 294, 257, 2316, 411, 437, 321, 1866, 3071, 294, 341, 960, 11, 361, 3847, 295, 261, 65, 486, 312, 2295, 570, 50786, 50786, 264, 4274, 6713, 322, 428, 3097, 5110, 486, 312, 4018, 420, 588, 1998, 281, 4018, 13, 51128, 51128, 407, 361, 3847, 486, 312, 588, 1998, 281, 4018, 13, 51272, 51272, 583, 498, 291, 632, 257, 1326, 4497, 5110, 294, 428, 1500, 992, 300, 264, 9284, 632, 406, 8895, 51502, 51502, 322, 550, 729, 1500, 5110, 1062, 574, 411, 613, 13, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.10018790851939809, "compression_ratio": 1.75, "no_speech_prob": 1.1015897598554147e-06}, {"id": 75, "seek": 36920, "start": 387.36, "end": 391.96, "text": " But if you had a few additional examples in your test set that the algorithm had not trained", "tokens": [50364, 407, 294, 257, 2316, 411, 437, 321, 1866, 3071, 294, 341, 960, 11, 361, 3847, 295, 261, 65, 486, 312, 2295, 570, 50786, 50786, 264, 4274, 6713, 322, 428, 3097, 5110, 486, 312, 4018, 420, 588, 1998, 281, 4018, 13, 51128, 51128, 407, 361, 3847, 486, 312, 588, 1998, 281, 4018, 13, 51272, 51272, 583, 498, 291, 632, 257, 1326, 4497, 5110, 294, 428, 1500, 992, 300, 264, 9284, 632, 406, 8895, 51502, 51502, 322, 550, 729, 1500, 5110, 1062, 574, 411, 613, 13, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.10018790851939809, "compression_ratio": 1.75, "no_speech_prob": 1.1015897598554147e-06}, {"id": 76, "seek": 36920, "start": 391.96, "end": 396.96, "text": " on then those test examples might look like these.", "tokens": [50364, 407, 294, 257, 2316, 411, 437, 321, 1866, 3071, 294, 341, 960, 11, 361, 3847, 295, 261, 65, 486, 312, 2295, 570, 50786, 50786, 264, 4274, 6713, 322, 428, 3097, 5110, 486, 312, 4018, 420, 588, 1998, 281, 4018, 13, 51128, 51128, 407, 361, 3847, 486, 312, 588, 1998, 281, 4018, 13, 51272, 51272, 583, 498, 291, 632, 257, 1326, 4497, 5110, 294, 428, 1500, 992, 300, 264, 9284, 632, 406, 8895, 51502, 51502, 322, 550, 729, 1500, 5110, 1062, 574, 411, 613, 13, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.10018790851939809, "compression_ratio": 1.75, "no_speech_prob": 1.1015897598554147e-06}, {"id": 77, "seek": 39696, "start": 396.96, "end": 401.91999999999996, "text": " And there's a large gap between what the algorithm is predicting as the estimated housing price", "tokens": [50364, 400, 456, 311, 257, 2416, 7417, 1296, 437, 264, 9284, 307, 32884, 382, 264, 14109, 6849, 3218, 50612, 50612, 293, 264, 3539, 2158, 295, 729, 6849, 7901, 13, 50780, 50780, 400, 370, 361, 6921, 486, 312, 1090, 13, 50933, 50933, 407, 2577, 300, 361, 1500, 307, 1090, 322, 341, 2316, 2709, 291, 257, 636, 281, 4325, 300, 754, 1673, 51282, 51282, 309, 775, 869, 322, 264, 3097, 992, 307, 767, 406, 370, 665, 412, 2674, 3319, 281, 777, 5110, 51561, 51561, 281, 777, 1412, 2793, 300, 645, 406, 294, 264, 3097, 992, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07946622494569759, "compression_ratio": 1.755458515283843, "no_speech_prob": 5.507197784027085e-06}, {"id": 78, "seek": 39696, "start": 401.91999999999996, "end": 405.28, "text": " and the actual value of those housing prices.", "tokens": [50364, 400, 456, 311, 257, 2416, 7417, 1296, 437, 264, 9284, 307, 32884, 382, 264, 14109, 6849, 3218, 50612, 50612, 293, 264, 3539, 2158, 295, 729, 6849, 7901, 13, 50780, 50780, 400, 370, 361, 6921, 486, 312, 1090, 13, 50933, 50933, 407, 2577, 300, 361, 1500, 307, 1090, 322, 341, 2316, 2709, 291, 257, 636, 281, 4325, 300, 754, 1673, 51282, 51282, 309, 775, 869, 322, 264, 3097, 992, 307, 767, 406, 370, 665, 412, 2674, 3319, 281, 777, 5110, 51561, 51561, 281, 777, 1412, 2793, 300, 645, 406, 294, 264, 3097, 992, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07946622494569759, "compression_ratio": 1.755458515283843, "no_speech_prob": 5.507197784027085e-06}, {"id": 79, "seek": 39696, "start": 405.28, "end": 408.34, "text": " And so j tests will be high.", "tokens": [50364, 400, 456, 311, 257, 2416, 7417, 1296, 437, 264, 9284, 307, 32884, 382, 264, 14109, 6849, 3218, 50612, 50612, 293, 264, 3539, 2158, 295, 729, 6849, 7901, 13, 50780, 50780, 400, 370, 361, 6921, 486, 312, 1090, 13, 50933, 50933, 407, 2577, 300, 361, 1500, 307, 1090, 322, 341, 2316, 2709, 291, 257, 636, 281, 4325, 300, 754, 1673, 51282, 51282, 309, 775, 869, 322, 264, 3097, 992, 307, 767, 406, 370, 665, 412, 2674, 3319, 281, 777, 5110, 51561, 51561, 281, 777, 1412, 2793, 300, 645, 406, 294, 264, 3097, 992, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07946622494569759, "compression_ratio": 1.755458515283843, "no_speech_prob": 5.507197784027085e-06}, {"id": 80, "seek": 39696, "start": 408.34, "end": 415.32, "text": " So seeing that j test is high on this model gives you a way to realize that even though", "tokens": [50364, 400, 456, 311, 257, 2416, 7417, 1296, 437, 264, 9284, 307, 32884, 382, 264, 14109, 6849, 3218, 50612, 50612, 293, 264, 3539, 2158, 295, 729, 6849, 7901, 13, 50780, 50780, 400, 370, 361, 6921, 486, 312, 1090, 13, 50933, 50933, 407, 2577, 300, 361, 1500, 307, 1090, 322, 341, 2316, 2709, 291, 257, 636, 281, 4325, 300, 754, 1673, 51282, 51282, 309, 775, 869, 322, 264, 3097, 992, 307, 767, 406, 370, 665, 412, 2674, 3319, 281, 777, 5110, 51561, 51561, 281, 777, 1412, 2793, 300, 645, 406, 294, 264, 3097, 992, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07946622494569759, "compression_ratio": 1.755458515283843, "no_speech_prob": 5.507197784027085e-06}, {"id": 81, "seek": 39696, "start": 415.32, "end": 420.9, "text": " it does great on the training set is actually not so good at generalizing to new examples", "tokens": [50364, 400, 456, 311, 257, 2416, 7417, 1296, 437, 264, 9284, 307, 32884, 382, 264, 14109, 6849, 3218, 50612, 50612, 293, 264, 3539, 2158, 295, 729, 6849, 7901, 13, 50780, 50780, 400, 370, 361, 6921, 486, 312, 1090, 13, 50933, 50933, 407, 2577, 300, 361, 1500, 307, 1090, 322, 341, 2316, 2709, 291, 257, 636, 281, 4325, 300, 754, 1673, 51282, 51282, 309, 775, 869, 322, 264, 3097, 992, 307, 767, 406, 370, 665, 412, 2674, 3319, 281, 777, 5110, 51561, 51561, 281, 777, 1412, 2793, 300, 645, 406, 294, 264, 3097, 992, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07946622494569759, "compression_ratio": 1.755458515283843, "no_speech_prob": 5.507197784027085e-06}, {"id": 82, "seek": 39696, "start": 420.9, "end": 424.96, "text": " to new data points that were not in the training set.", "tokens": [50364, 400, 456, 311, 257, 2416, 7417, 1296, 437, 264, 9284, 307, 32884, 382, 264, 14109, 6849, 3218, 50612, 50612, 293, 264, 3539, 2158, 295, 729, 6849, 7901, 13, 50780, 50780, 400, 370, 361, 6921, 486, 312, 1090, 13, 50933, 50933, 407, 2577, 300, 361, 1500, 307, 1090, 322, 341, 2316, 2709, 291, 257, 636, 281, 4325, 300, 754, 1673, 51282, 51282, 309, 775, 869, 322, 264, 3097, 992, 307, 767, 406, 370, 665, 412, 2674, 3319, 281, 777, 5110, 51561, 51561, 281, 777, 1412, 2793, 300, 645, 406, 294, 264, 3097, 992, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07946622494569759, "compression_ratio": 1.755458515283843, "no_speech_prob": 5.507197784027085e-06}, {"id": 83, "seek": 42496, "start": 424.96, "end": 428.71999999999997, "text": " So that was regression with squared error costs.", "tokens": [50364, 407, 300, 390, 24590, 365, 8889, 6713, 5497, 13, 50552, 50552, 823, 718, 311, 747, 257, 574, 412, 577, 291, 1116, 3079, 341, 10747, 281, 257, 21538, 1154, 13, 50792, 50792, 1171, 1365, 11, 498, 291, 645, 1508, 5489, 1296, 1011, 26859, 27011, 300, 366, 2139, 4018, 420, 51040, 51040, 472, 13, 51106, 51106, 407, 912, 382, 949, 11, 291, 3318, 264, 9834, 538, 46608, 264, 2063, 2445, 281, 915, 264, 51332, 51332, 9834, 261, 65, 13, 51422, 51422, 1171, 1365, 11, 498, 291, 645, 3097, 3565, 3142, 24590, 11, 550, 341, 576, 312, 264, 2063, 2445, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.12818702848831026, "compression_ratio": 1.7314049586776858, "no_speech_prob": 4.289237949706148e-06}, {"id": 84, "seek": 42496, "start": 428.71999999999997, "end": 433.52, "text": " Now let's take a look at how you'd apply this procedure to a classification problem.", "tokens": [50364, 407, 300, 390, 24590, 365, 8889, 6713, 5497, 13, 50552, 50552, 823, 718, 311, 747, 257, 574, 412, 577, 291, 1116, 3079, 341, 10747, 281, 257, 21538, 1154, 13, 50792, 50792, 1171, 1365, 11, 498, 291, 645, 1508, 5489, 1296, 1011, 26859, 27011, 300, 366, 2139, 4018, 420, 51040, 51040, 472, 13, 51106, 51106, 407, 912, 382, 949, 11, 291, 3318, 264, 9834, 538, 46608, 264, 2063, 2445, 281, 915, 264, 51332, 51332, 9834, 261, 65, 13, 51422, 51422, 1171, 1365, 11, 498, 291, 645, 3097, 3565, 3142, 24590, 11, 550, 341, 576, 312, 264, 2063, 2445, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.12818702848831026, "compression_ratio": 1.7314049586776858, "no_speech_prob": 4.289237949706148e-06}, {"id": 85, "seek": 42496, "start": 433.52, "end": 438.47999999999996, "text": " For example, if you were classifying between handwritten digits that are either zero or", "tokens": [50364, 407, 300, 390, 24590, 365, 8889, 6713, 5497, 13, 50552, 50552, 823, 718, 311, 747, 257, 574, 412, 577, 291, 1116, 3079, 341, 10747, 281, 257, 21538, 1154, 13, 50792, 50792, 1171, 1365, 11, 498, 291, 645, 1508, 5489, 1296, 1011, 26859, 27011, 300, 366, 2139, 4018, 420, 51040, 51040, 472, 13, 51106, 51106, 407, 912, 382, 949, 11, 291, 3318, 264, 9834, 538, 46608, 264, 2063, 2445, 281, 915, 264, 51332, 51332, 9834, 261, 65, 13, 51422, 51422, 1171, 1365, 11, 498, 291, 645, 3097, 3565, 3142, 24590, 11, 550, 341, 576, 312, 264, 2063, 2445, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.12818702848831026, "compression_ratio": 1.7314049586776858, "no_speech_prob": 4.289237949706148e-06}, {"id": 86, "seek": 42496, "start": 438.47999999999996, "end": 439.79999999999995, "text": " one.", "tokens": [50364, 407, 300, 390, 24590, 365, 8889, 6713, 5497, 13, 50552, 50552, 823, 718, 311, 747, 257, 574, 412, 577, 291, 1116, 3079, 341, 10747, 281, 257, 21538, 1154, 13, 50792, 50792, 1171, 1365, 11, 498, 291, 645, 1508, 5489, 1296, 1011, 26859, 27011, 300, 366, 2139, 4018, 420, 51040, 51040, 472, 13, 51106, 51106, 407, 912, 382, 949, 11, 291, 3318, 264, 9834, 538, 46608, 264, 2063, 2445, 281, 915, 264, 51332, 51332, 9834, 261, 65, 13, 51422, 51422, 1171, 1365, 11, 498, 291, 645, 3097, 3565, 3142, 24590, 11, 550, 341, 576, 312, 264, 2063, 2445, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.12818702848831026, "compression_ratio": 1.7314049586776858, "no_speech_prob": 4.289237949706148e-06}, {"id": 87, "seek": 42496, "start": 439.79999999999995, "end": 444.32, "text": " So same as before, you fit the parameters by minimizing the cost function to find the", "tokens": [50364, 407, 300, 390, 24590, 365, 8889, 6713, 5497, 13, 50552, 50552, 823, 718, 311, 747, 257, 574, 412, 577, 291, 1116, 3079, 341, 10747, 281, 257, 21538, 1154, 13, 50792, 50792, 1171, 1365, 11, 498, 291, 645, 1508, 5489, 1296, 1011, 26859, 27011, 300, 366, 2139, 4018, 420, 51040, 51040, 472, 13, 51106, 51106, 407, 912, 382, 949, 11, 291, 3318, 264, 9834, 538, 46608, 264, 2063, 2445, 281, 915, 264, 51332, 51332, 9834, 261, 65, 13, 51422, 51422, 1171, 1365, 11, 498, 291, 645, 3097, 3565, 3142, 24590, 11, 550, 341, 576, 312, 264, 2063, 2445, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.12818702848831026, "compression_ratio": 1.7314049586776858, "no_speech_prob": 4.289237949706148e-06}, {"id": 88, "seek": 42496, "start": 444.32, "end": 446.12, "text": " parameters wb.", "tokens": [50364, 407, 300, 390, 24590, 365, 8889, 6713, 5497, 13, 50552, 50552, 823, 718, 311, 747, 257, 574, 412, 577, 291, 1116, 3079, 341, 10747, 281, 257, 21538, 1154, 13, 50792, 50792, 1171, 1365, 11, 498, 291, 645, 1508, 5489, 1296, 1011, 26859, 27011, 300, 366, 2139, 4018, 420, 51040, 51040, 472, 13, 51106, 51106, 407, 912, 382, 949, 11, 291, 3318, 264, 9834, 538, 46608, 264, 2063, 2445, 281, 915, 264, 51332, 51332, 9834, 261, 65, 13, 51422, 51422, 1171, 1365, 11, 498, 291, 645, 3097, 3565, 3142, 24590, 11, 550, 341, 576, 312, 264, 2063, 2445, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.12818702848831026, "compression_ratio": 1.7314049586776858, "no_speech_prob": 4.289237949706148e-06}, {"id": 89, "seek": 42496, "start": 446.12, "end": 451.08, "text": " For example, if you were training logistic regression, then this would be the cost function", "tokens": [50364, 407, 300, 390, 24590, 365, 8889, 6713, 5497, 13, 50552, 50552, 823, 718, 311, 747, 257, 574, 412, 577, 291, 1116, 3079, 341, 10747, 281, 257, 21538, 1154, 13, 50792, 50792, 1171, 1365, 11, 498, 291, 645, 1508, 5489, 1296, 1011, 26859, 27011, 300, 366, 2139, 4018, 420, 51040, 51040, 472, 13, 51106, 51106, 407, 912, 382, 949, 11, 291, 3318, 264, 9834, 538, 46608, 264, 2063, 2445, 281, 915, 264, 51332, 51332, 9834, 261, 65, 13, 51422, 51422, 1171, 1365, 11, 498, 291, 645, 3097, 3565, 3142, 24590, 11, 550, 341, 576, 312, 264, 2063, 2445, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.12818702848831026, "compression_ratio": 1.7314049586776858, "no_speech_prob": 4.289237949706148e-06}, {"id": 90, "seek": 45108, "start": 451.08, "end": 460.44, "text": " j of wb where this is the usual logistic loss function and then plus also the regularization", "tokens": [50364, 361, 295, 261, 65, 689, 341, 307, 264, 7713, 3565, 3142, 4470, 2445, 293, 550, 1804, 611, 264, 3890, 2144, 50832, 50832, 1433, 293, 281, 14722, 264, 1500, 6713, 13, 51032, 51032, 508, 1500, 307, 550, 264, 4274, 670, 428, 1500, 5110, 13, 51346, 51346, 663, 311, 300, 2217, 4, 295, 264, 1412, 300, 2067, 380, 294, 264, 3097, 992, 295, 264, 3565, 3142, 4470, 322, 428, 51661, 51661, 1500, 992, 13, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.20050297285381116, "compression_ratio": 1.6127167630057804, "no_speech_prob": 2.8572533210535767e-06}, {"id": 91, "seek": 45108, "start": 460.44, "end": 464.44, "text": " term and to compute the test error.", "tokens": [50364, 361, 295, 261, 65, 689, 341, 307, 264, 7713, 3565, 3142, 4470, 2445, 293, 550, 1804, 611, 264, 3890, 2144, 50832, 50832, 1433, 293, 281, 14722, 264, 1500, 6713, 13, 51032, 51032, 508, 1500, 307, 550, 264, 4274, 670, 428, 1500, 5110, 13, 51346, 51346, 663, 311, 300, 2217, 4, 295, 264, 1412, 300, 2067, 380, 294, 264, 3097, 992, 295, 264, 3565, 3142, 4470, 322, 428, 51661, 51661, 1500, 992, 13, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.20050297285381116, "compression_ratio": 1.6127167630057804, "no_speech_prob": 2.8572533210535767e-06}, {"id": 92, "seek": 45108, "start": 464.44, "end": 470.71999999999997, "text": " J test is then the average over your test examples.", "tokens": [50364, 361, 295, 261, 65, 689, 341, 307, 264, 7713, 3565, 3142, 4470, 2445, 293, 550, 1804, 611, 264, 3890, 2144, 50832, 50832, 1433, 293, 281, 14722, 264, 1500, 6713, 13, 51032, 51032, 508, 1500, 307, 550, 264, 4274, 670, 428, 1500, 5110, 13, 51346, 51346, 663, 311, 300, 2217, 4, 295, 264, 1412, 300, 2067, 380, 294, 264, 3097, 992, 295, 264, 3565, 3142, 4470, 322, 428, 51661, 51661, 1500, 992, 13, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.20050297285381116, "compression_ratio": 1.6127167630057804, "no_speech_prob": 2.8572533210535767e-06}, {"id": 93, "seek": 45108, "start": 470.71999999999997, "end": 477.02, "text": " That's that 30% of the data that wasn't in the training set of the logistic loss on your", "tokens": [50364, 361, 295, 261, 65, 689, 341, 307, 264, 7713, 3565, 3142, 4470, 2445, 293, 550, 1804, 611, 264, 3890, 2144, 50832, 50832, 1433, 293, 281, 14722, 264, 1500, 6713, 13, 51032, 51032, 508, 1500, 307, 550, 264, 4274, 670, 428, 1500, 5110, 13, 51346, 51346, 663, 311, 300, 2217, 4, 295, 264, 1412, 300, 2067, 380, 294, 264, 3097, 992, 295, 264, 3565, 3142, 4470, 322, 428, 51661, 51661, 1500, 992, 13, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.20050297285381116, "compression_ratio": 1.6127167630057804, "no_speech_prob": 2.8572533210535767e-06}, {"id": 94, "seek": 45108, "start": 477.02, "end": 479.12, "text": " test set.", "tokens": [50364, 361, 295, 261, 65, 689, 341, 307, 264, 7713, 3565, 3142, 4470, 2445, 293, 550, 1804, 611, 264, 3890, 2144, 50832, 50832, 1433, 293, 281, 14722, 264, 1500, 6713, 13, 51032, 51032, 508, 1500, 307, 550, 264, 4274, 670, 428, 1500, 5110, 13, 51346, 51346, 663, 311, 300, 2217, 4, 295, 264, 1412, 300, 2067, 380, 294, 264, 3097, 992, 295, 264, 3565, 3142, 4470, 322, 428, 51661, 51661, 1500, 992, 13, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.20050297285381116, "compression_ratio": 1.6127167630057804, "no_speech_prob": 2.8572533210535767e-06}, {"id": 95, "seek": 47912, "start": 479.12, "end": 484.48, "text": " And the training error you could also compute using this formula is the average logistic", "tokens": [50364, 400, 264, 3097, 6713, 291, 727, 611, 14722, 1228, 341, 8513, 307, 264, 4274, 3565, 3142, 50632, 50632, 4470, 322, 428, 3097, 1412, 300, 264, 9284, 390, 1228, 281, 17522, 264, 2063, 2445, 361, 50906, 50906, 295, 261, 65, 13, 51006, 51006, 1042, 437, 286, 6786, 510, 486, 589, 1392, 337, 15213, 484, 498, 428, 2539, 9284, 307, 51278, 51278, 884, 731, 13, 51328, 51328, 286, 600, 1612, 577, 309, 390, 884, 294, 2115, 295, 1500, 6713, 13, 51492, 51492, 1133, 9275, 3479, 2539, 281, 21538, 2740, 11, 456, 311, 767, 472, 661, 7123, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.1711205852274992, "compression_ratio": 1.664092664092664, "no_speech_prob": 1.1726294815161964e-06}, {"id": 96, "seek": 47912, "start": 484.48, "end": 489.96, "text": " loss on your training data that the algorithm was using to minimize the cost function j", "tokens": [50364, 400, 264, 3097, 6713, 291, 727, 611, 14722, 1228, 341, 8513, 307, 264, 4274, 3565, 3142, 50632, 50632, 4470, 322, 428, 3097, 1412, 300, 264, 9284, 390, 1228, 281, 17522, 264, 2063, 2445, 361, 50906, 50906, 295, 261, 65, 13, 51006, 51006, 1042, 437, 286, 6786, 510, 486, 589, 1392, 337, 15213, 484, 498, 428, 2539, 9284, 307, 51278, 51278, 884, 731, 13, 51328, 51328, 286, 600, 1612, 577, 309, 390, 884, 294, 2115, 295, 1500, 6713, 13, 51492, 51492, 1133, 9275, 3479, 2539, 281, 21538, 2740, 11, 456, 311, 767, 472, 661, 7123, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.1711205852274992, "compression_ratio": 1.664092664092664, "no_speech_prob": 1.1726294815161964e-06}, {"id": 97, "seek": 47912, "start": 489.96, "end": 491.96, "text": " of wb.", "tokens": [50364, 400, 264, 3097, 6713, 291, 727, 611, 14722, 1228, 341, 8513, 307, 264, 4274, 3565, 3142, 50632, 50632, 4470, 322, 428, 3097, 1412, 300, 264, 9284, 390, 1228, 281, 17522, 264, 2063, 2445, 361, 50906, 50906, 295, 261, 65, 13, 51006, 51006, 1042, 437, 286, 6786, 510, 486, 589, 1392, 337, 15213, 484, 498, 428, 2539, 9284, 307, 51278, 51278, 884, 731, 13, 51328, 51328, 286, 600, 1612, 577, 309, 390, 884, 294, 2115, 295, 1500, 6713, 13, 51492, 51492, 1133, 9275, 3479, 2539, 281, 21538, 2740, 11, 456, 311, 767, 472, 661, 7123, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.1711205852274992, "compression_ratio": 1.664092664092664, "no_speech_prob": 1.1726294815161964e-06}, {"id": 98, "seek": 47912, "start": 491.96, "end": 497.4, "text": " Well what I describe here will work okay for figuring out if your learning algorithm is", "tokens": [50364, 400, 264, 3097, 6713, 291, 727, 611, 14722, 1228, 341, 8513, 307, 264, 4274, 3565, 3142, 50632, 50632, 4470, 322, 428, 3097, 1412, 300, 264, 9284, 390, 1228, 281, 17522, 264, 2063, 2445, 361, 50906, 50906, 295, 261, 65, 13, 51006, 51006, 1042, 437, 286, 6786, 510, 486, 589, 1392, 337, 15213, 484, 498, 428, 2539, 9284, 307, 51278, 51278, 884, 731, 13, 51328, 51328, 286, 600, 1612, 577, 309, 390, 884, 294, 2115, 295, 1500, 6713, 13, 51492, 51492, 1133, 9275, 3479, 2539, 281, 21538, 2740, 11, 456, 311, 767, 472, 661, 7123, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.1711205852274992, "compression_ratio": 1.664092664092664, "no_speech_prob": 1.1726294815161964e-06}, {"id": 99, "seek": 47912, "start": 497.4, "end": 498.4, "text": " doing well.", "tokens": [50364, 400, 264, 3097, 6713, 291, 727, 611, 14722, 1228, 341, 8513, 307, 264, 4274, 3565, 3142, 50632, 50632, 4470, 322, 428, 3097, 1412, 300, 264, 9284, 390, 1228, 281, 17522, 264, 2063, 2445, 361, 50906, 50906, 295, 261, 65, 13, 51006, 51006, 1042, 437, 286, 6786, 510, 486, 589, 1392, 337, 15213, 484, 498, 428, 2539, 9284, 307, 51278, 51278, 884, 731, 13, 51328, 51328, 286, 600, 1612, 577, 309, 390, 884, 294, 2115, 295, 1500, 6713, 13, 51492, 51492, 1133, 9275, 3479, 2539, 281, 21538, 2740, 11, 456, 311, 767, 472, 661, 7123, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.1711205852274992, "compression_ratio": 1.664092664092664, "no_speech_prob": 1.1726294815161964e-06}, {"id": 100, "seek": 47912, "start": 498.4, "end": 501.68, "text": " I've seen how it was doing in terms of test error.", "tokens": [50364, 400, 264, 3097, 6713, 291, 727, 611, 14722, 1228, 341, 8513, 307, 264, 4274, 3565, 3142, 50632, 50632, 4470, 322, 428, 3097, 1412, 300, 264, 9284, 390, 1228, 281, 17522, 264, 2063, 2445, 361, 50906, 50906, 295, 261, 65, 13, 51006, 51006, 1042, 437, 286, 6786, 510, 486, 589, 1392, 337, 15213, 484, 498, 428, 2539, 9284, 307, 51278, 51278, 884, 731, 13, 51328, 51328, 286, 600, 1612, 577, 309, 390, 884, 294, 2115, 295, 1500, 6713, 13, 51492, 51492, 1133, 9275, 3479, 2539, 281, 21538, 2740, 11, 456, 311, 767, 472, 661, 7123, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.1711205852274992, "compression_ratio": 1.664092664092664, "no_speech_prob": 1.1726294815161964e-06}, {"id": 101, "seek": 47912, "start": 501.68, "end": 507.0, "text": " When applying machine learning to classification problems, there's actually one other definition", "tokens": [50364, 400, 264, 3097, 6713, 291, 727, 611, 14722, 1228, 341, 8513, 307, 264, 4274, 3565, 3142, 50632, 50632, 4470, 322, 428, 3097, 1412, 300, 264, 9284, 390, 1228, 281, 17522, 264, 2063, 2445, 361, 50906, 50906, 295, 261, 65, 13, 51006, 51006, 1042, 437, 286, 6786, 510, 486, 589, 1392, 337, 15213, 484, 498, 428, 2539, 9284, 307, 51278, 51278, 884, 731, 13, 51328, 51328, 286, 600, 1612, 577, 309, 390, 884, 294, 2115, 295, 1500, 6713, 13, 51492, 51492, 1133, 9275, 3479, 2539, 281, 21538, 2740, 11, 456, 311, 767, 472, 661, 7123, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.1711205852274992, "compression_ratio": 1.664092664092664, "no_speech_prob": 1.1726294815161964e-06}, {"id": 102, "seek": 50700, "start": 507.0, "end": 513.68, "text": " of j test and j train that is maybe even more commonly used, which is instead of using the", "tokens": [50364, 295, 361, 1500, 293, 361, 3847, 300, 307, 1310, 754, 544, 12719, 1143, 11, 597, 307, 2602, 295, 1228, 264, 50698, 50698, 3565, 3142, 4470, 281, 14722, 264, 1500, 6713, 293, 264, 3097, 6713, 281, 2602, 3481, 437, 311, 50986, 50986, 264, 14135, 295, 264, 1500, 992, 293, 264, 14135, 295, 264, 3097, 992, 300, 264, 9284, 575, 51258, 51258, 3346, 11665, 2587, 13, 51348, 51348, 407, 4682, 322, 264, 1500, 992, 11, 291, 393, 362, 264, 9284, 652, 257, 17630, 472, 420, 4018, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.11930953372608531, "compression_ratio": 1.8195121951219513, "no_speech_prob": 8.939221515902318e-06}, {"id": 103, "seek": 50700, "start": 513.68, "end": 519.44, "text": " logistic loss to compute the test error and the training error to instead measure what's", "tokens": [50364, 295, 361, 1500, 293, 361, 3847, 300, 307, 1310, 754, 544, 12719, 1143, 11, 597, 307, 2602, 295, 1228, 264, 50698, 50698, 3565, 3142, 4470, 281, 14722, 264, 1500, 6713, 293, 264, 3097, 6713, 281, 2602, 3481, 437, 311, 50986, 50986, 264, 14135, 295, 264, 1500, 992, 293, 264, 14135, 295, 264, 3097, 992, 300, 264, 9284, 575, 51258, 51258, 3346, 11665, 2587, 13, 51348, 51348, 407, 4682, 322, 264, 1500, 992, 11, 291, 393, 362, 264, 9284, 652, 257, 17630, 472, 420, 4018, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.11930953372608531, "compression_ratio": 1.8195121951219513, "no_speech_prob": 8.939221515902318e-06}, {"id": 104, "seek": 50700, "start": 519.44, "end": 524.88, "text": " the fraction of the test set and the fraction of the training set that the algorithm has", "tokens": [50364, 295, 361, 1500, 293, 361, 3847, 300, 307, 1310, 754, 544, 12719, 1143, 11, 597, 307, 2602, 295, 1228, 264, 50698, 50698, 3565, 3142, 4470, 281, 14722, 264, 1500, 6713, 293, 264, 3097, 6713, 281, 2602, 3481, 437, 311, 50986, 50986, 264, 14135, 295, 264, 1500, 992, 293, 264, 14135, 295, 264, 3097, 992, 300, 264, 9284, 575, 51258, 51258, 3346, 11665, 2587, 13, 51348, 51348, 407, 4682, 322, 264, 1500, 992, 11, 291, 393, 362, 264, 9284, 652, 257, 17630, 472, 420, 4018, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.11930953372608531, "compression_ratio": 1.8195121951219513, "no_speech_prob": 8.939221515902318e-06}, {"id": 105, "seek": 50700, "start": 524.88, "end": 526.68, "text": " misclassified.", "tokens": [50364, 295, 361, 1500, 293, 361, 3847, 300, 307, 1310, 754, 544, 12719, 1143, 11, 597, 307, 2602, 295, 1228, 264, 50698, 50698, 3565, 3142, 4470, 281, 14722, 264, 1500, 6713, 293, 264, 3097, 6713, 281, 2602, 3481, 437, 311, 50986, 50986, 264, 14135, 295, 264, 1500, 992, 293, 264, 14135, 295, 264, 3097, 992, 300, 264, 9284, 575, 51258, 51258, 3346, 11665, 2587, 13, 51348, 51348, 407, 4682, 322, 264, 1500, 992, 11, 291, 393, 362, 264, 9284, 652, 257, 17630, 472, 420, 4018, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.11930953372608531, "compression_ratio": 1.8195121951219513, "no_speech_prob": 8.939221515902318e-06}, {"id": 106, "seek": 50700, "start": 526.68, "end": 535.92, "text": " So specifically on the test set, you can have the algorithm make a prediction one or zero", "tokens": [50364, 295, 361, 1500, 293, 361, 3847, 300, 307, 1310, 754, 544, 12719, 1143, 11, 597, 307, 2602, 295, 1228, 264, 50698, 50698, 3565, 3142, 4470, 281, 14722, 264, 1500, 6713, 293, 264, 3097, 6713, 281, 2602, 3481, 437, 311, 50986, 50986, 264, 14135, 295, 264, 1500, 992, 293, 264, 14135, 295, 264, 3097, 992, 300, 264, 9284, 575, 51258, 51258, 3346, 11665, 2587, 13, 51348, 51348, 407, 4682, 322, 264, 1500, 992, 11, 291, 393, 362, 264, 9284, 652, 257, 17630, 472, 420, 4018, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.11930953372608531, "compression_ratio": 1.8195121951219513, "no_speech_prob": 8.939221515902318e-06}, {"id": 107, "seek": 53592, "start": 535.92, "end": 537.68, "text": " on every test example.", "tokens": [50364, 322, 633, 1500, 1365, 13, 50452, 50452, 407, 9901, 288, 2385, 11, 321, 576, 6069, 382, 472, 498, 283, 295, 2031, 307, 5044, 813, 420, 2681, 281, 1958, 13, 20, 293, 50788, 50788, 4018, 498, 309, 311, 1570, 813, 1958, 13, 20, 13, 50946, 50946, 400, 291, 393, 550, 1207, 493, 294, 264, 1500, 992, 264, 14135, 295, 5110, 689, 288, 2385, 307, 406, 51252, 51252, 2681, 281, 264, 3539, 2727, 3494, 7645, 288, 294, 264, 1500, 992, 13, 51494, 51494, 407, 39481, 736, 11, 498, 291, 645, 1508, 5489, 1011, 26859, 27011, 4018, 420, 472, 538, 777, 21538, 14432, 11, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.16324252174014137, "compression_ratio": 1.6478260869565218, "no_speech_prob": 5.0146727517130785e-06}, {"id": 108, "seek": 53592, "start": 537.68, "end": 544.4, "text": " So recall y hat, we would predict as one if f of x is greater than or equal to 0.5 and", "tokens": [50364, 322, 633, 1500, 1365, 13, 50452, 50452, 407, 9901, 288, 2385, 11, 321, 576, 6069, 382, 472, 498, 283, 295, 2031, 307, 5044, 813, 420, 2681, 281, 1958, 13, 20, 293, 50788, 50788, 4018, 498, 309, 311, 1570, 813, 1958, 13, 20, 13, 50946, 50946, 400, 291, 393, 550, 1207, 493, 294, 264, 1500, 992, 264, 14135, 295, 5110, 689, 288, 2385, 307, 406, 51252, 51252, 2681, 281, 264, 3539, 2727, 3494, 7645, 288, 294, 264, 1500, 992, 13, 51494, 51494, 407, 39481, 736, 11, 498, 291, 645, 1508, 5489, 1011, 26859, 27011, 4018, 420, 472, 538, 777, 21538, 14432, 11, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.16324252174014137, "compression_ratio": 1.6478260869565218, "no_speech_prob": 5.0146727517130785e-06}, {"id": 109, "seek": 53592, "start": 544.4, "end": 547.56, "text": " zero if it's less than 0.5.", "tokens": [50364, 322, 633, 1500, 1365, 13, 50452, 50452, 407, 9901, 288, 2385, 11, 321, 576, 6069, 382, 472, 498, 283, 295, 2031, 307, 5044, 813, 420, 2681, 281, 1958, 13, 20, 293, 50788, 50788, 4018, 498, 309, 311, 1570, 813, 1958, 13, 20, 13, 50946, 50946, 400, 291, 393, 550, 1207, 493, 294, 264, 1500, 992, 264, 14135, 295, 5110, 689, 288, 2385, 307, 406, 51252, 51252, 2681, 281, 264, 3539, 2727, 3494, 7645, 288, 294, 264, 1500, 992, 13, 51494, 51494, 407, 39481, 736, 11, 498, 291, 645, 1508, 5489, 1011, 26859, 27011, 4018, 420, 472, 538, 777, 21538, 14432, 11, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.16324252174014137, "compression_ratio": 1.6478260869565218, "no_speech_prob": 5.0146727517130785e-06}, {"id": 110, "seek": 53592, "start": 547.56, "end": 553.68, "text": " And you can then count up in the test set the fraction of examples where y hat is not", "tokens": [50364, 322, 633, 1500, 1365, 13, 50452, 50452, 407, 9901, 288, 2385, 11, 321, 576, 6069, 382, 472, 498, 283, 295, 2031, 307, 5044, 813, 420, 2681, 281, 1958, 13, 20, 293, 50788, 50788, 4018, 498, 309, 311, 1570, 813, 1958, 13, 20, 13, 50946, 50946, 400, 291, 393, 550, 1207, 493, 294, 264, 1500, 992, 264, 14135, 295, 5110, 689, 288, 2385, 307, 406, 51252, 51252, 2681, 281, 264, 3539, 2727, 3494, 7645, 288, 294, 264, 1500, 992, 13, 51494, 51494, 407, 39481, 736, 11, 498, 291, 645, 1508, 5489, 1011, 26859, 27011, 4018, 420, 472, 538, 777, 21538, 14432, 11, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.16324252174014137, "compression_ratio": 1.6478260869565218, "no_speech_prob": 5.0146727517130785e-06}, {"id": 111, "seek": 53592, "start": 553.68, "end": 558.52, "text": " equal to the actual ground truth label y in the test set.", "tokens": [50364, 322, 633, 1500, 1365, 13, 50452, 50452, 407, 9901, 288, 2385, 11, 321, 576, 6069, 382, 472, 498, 283, 295, 2031, 307, 5044, 813, 420, 2681, 281, 1958, 13, 20, 293, 50788, 50788, 4018, 498, 309, 311, 1570, 813, 1958, 13, 20, 13, 50946, 50946, 400, 291, 393, 550, 1207, 493, 294, 264, 1500, 992, 264, 14135, 295, 5110, 689, 288, 2385, 307, 406, 51252, 51252, 2681, 281, 264, 3539, 2727, 3494, 7645, 288, 294, 264, 1500, 992, 13, 51494, 51494, 407, 39481, 736, 11, 498, 291, 645, 1508, 5489, 1011, 26859, 27011, 4018, 420, 472, 538, 777, 21538, 14432, 11, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.16324252174014137, "compression_ratio": 1.6478260869565218, "no_speech_prob": 5.0146727517130785e-06}, {"id": 112, "seek": 53592, "start": 558.52, "end": 565.64, "text": " So concretely, if you were classifying handwritten digits zero or one by new classification toss,", "tokens": [50364, 322, 633, 1500, 1365, 13, 50452, 50452, 407, 9901, 288, 2385, 11, 321, 576, 6069, 382, 472, 498, 283, 295, 2031, 307, 5044, 813, 420, 2681, 281, 1958, 13, 20, 293, 50788, 50788, 4018, 498, 309, 311, 1570, 813, 1958, 13, 20, 13, 50946, 50946, 400, 291, 393, 550, 1207, 493, 294, 264, 1500, 992, 264, 14135, 295, 5110, 689, 288, 2385, 307, 406, 51252, 51252, 2681, 281, 264, 3539, 2727, 3494, 7645, 288, 294, 264, 1500, 992, 13, 51494, 51494, 407, 39481, 736, 11, 498, 291, 645, 1508, 5489, 1011, 26859, 27011, 4018, 420, 472, 538, 777, 21538, 14432, 11, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.16324252174014137, "compression_ratio": 1.6478260869565218, "no_speech_prob": 5.0146727517130785e-06}, {"id": 113, "seek": 56564, "start": 565.64, "end": 570.88, "text": " then j test would be the fraction of that test set where zero was classified as one", "tokens": [50364, 550, 361, 1500, 576, 312, 264, 14135, 295, 300, 1500, 992, 689, 4018, 390, 20627, 382, 472, 50626, 50626, 420, 472, 20627, 382, 4018, 13, 50728, 50728, 400, 14138, 11, 361, 3847, 307, 257, 14135, 295, 264, 3097, 992, 300, 575, 668, 3346, 11665, 2587, 13, 51022, 51022, 17837, 257, 1412, 992, 293, 30348, 309, 666, 257, 3097, 992, 293, 257, 4994, 1500, 992, 2709, 51262, 51262, 291, 257, 636, 281, 39531, 13059, 577, 731, 428, 2539, 9284, 307, 884, 13, 51472, 51472, 3146, 15866, 1293, 361, 1500, 293, 361, 3847, 11, 291, 393, 586, 3481, 577, 309, 311, 884, 322, 264, 1500, 51728, 51728, 992, 293, 322, 264, 3097, 992, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1273825004183013, "compression_ratio": 1.8700787401574803, "no_speech_prob": 1.4823223182247602e-06}, {"id": 114, "seek": 56564, "start": 570.88, "end": 572.92, "text": " or one classified as zero.", "tokens": [50364, 550, 361, 1500, 576, 312, 264, 14135, 295, 300, 1500, 992, 689, 4018, 390, 20627, 382, 472, 50626, 50626, 420, 472, 20627, 382, 4018, 13, 50728, 50728, 400, 14138, 11, 361, 3847, 307, 257, 14135, 295, 264, 3097, 992, 300, 575, 668, 3346, 11665, 2587, 13, 51022, 51022, 17837, 257, 1412, 992, 293, 30348, 309, 666, 257, 3097, 992, 293, 257, 4994, 1500, 992, 2709, 51262, 51262, 291, 257, 636, 281, 39531, 13059, 577, 731, 428, 2539, 9284, 307, 884, 13, 51472, 51472, 3146, 15866, 1293, 361, 1500, 293, 361, 3847, 11, 291, 393, 586, 3481, 577, 309, 311, 884, 322, 264, 1500, 51728, 51728, 992, 293, 322, 264, 3097, 992, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1273825004183013, "compression_ratio": 1.8700787401574803, "no_speech_prob": 1.4823223182247602e-06}, {"id": 115, "seek": 56564, "start": 572.92, "end": 578.8, "text": " And similarly, j train is a fraction of the training set that has been misclassified.", "tokens": [50364, 550, 361, 1500, 576, 312, 264, 14135, 295, 300, 1500, 992, 689, 4018, 390, 20627, 382, 472, 50626, 50626, 420, 472, 20627, 382, 4018, 13, 50728, 50728, 400, 14138, 11, 361, 3847, 307, 257, 14135, 295, 264, 3097, 992, 300, 575, 668, 3346, 11665, 2587, 13, 51022, 51022, 17837, 257, 1412, 992, 293, 30348, 309, 666, 257, 3097, 992, 293, 257, 4994, 1500, 992, 2709, 51262, 51262, 291, 257, 636, 281, 39531, 13059, 577, 731, 428, 2539, 9284, 307, 884, 13, 51472, 51472, 3146, 15866, 1293, 361, 1500, 293, 361, 3847, 11, 291, 393, 586, 3481, 577, 309, 311, 884, 322, 264, 1500, 51728, 51728, 992, 293, 322, 264, 3097, 992, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1273825004183013, "compression_ratio": 1.8700787401574803, "no_speech_prob": 1.4823223182247602e-06}, {"id": 116, "seek": 56564, "start": 578.8, "end": 583.6, "text": " Taking a data set and splitting it into a training set and a separate test set gives", "tokens": [50364, 550, 361, 1500, 576, 312, 264, 14135, 295, 300, 1500, 992, 689, 4018, 390, 20627, 382, 472, 50626, 50626, 420, 472, 20627, 382, 4018, 13, 50728, 50728, 400, 14138, 11, 361, 3847, 307, 257, 14135, 295, 264, 3097, 992, 300, 575, 668, 3346, 11665, 2587, 13, 51022, 51022, 17837, 257, 1412, 992, 293, 30348, 309, 666, 257, 3097, 992, 293, 257, 4994, 1500, 992, 2709, 51262, 51262, 291, 257, 636, 281, 39531, 13059, 577, 731, 428, 2539, 9284, 307, 884, 13, 51472, 51472, 3146, 15866, 1293, 361, 1500, 293, 361, 3847, 11, 291, 393, 586, 3481, 577, 309, 311, 884, 322, 264, 1500, 51728, 51728, 992, 293, 322, 264, 3097, 992, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1273825004183013, "compression_ratio": 1.8700787401574803, "no_speech_prob": 1.4823223182247602e-06}, {"id": 117, "seek": 56564, "start": 583.6, "end": 587.8, "text": " you a way to systematically evaluate how well your learning algorithm is doing.", "tokens": [50364, 550, 361, 1500, 576, 312, 264, 14135, 295, 300, 1500, 992, 689, 4018, 390, 20627, 382, 472, 50626, 50626, 420, 472, 20627, 382, 4018, 13, 50728, 50728, 400, 14138, 11, 361, 3847, 307, 257, 14135, 295, 264, 3097, 992, 300, 575, 668, 3346, 11665, 2587, 13, 51022, 51022, 17837, 257, 1412, 992, 293, 30348, 309, 666, 257, 3097, 992, 293, 257, 4994, 1500, 992, 2709, 51262, 51262, 291, 257, 636, 281, 39531, 13059, 577, 731, 428, 2539, 9284, 307, 884, 13, 51472, 51472, 3146, 15866, 1293, 361, 1500, 293, 361, 3847, 11, 291, 393, 586, 3481, 577, 309, 311, 884, 322, 264, 1500, 51728, 51728, 992, 293, 322, 264, 3097, 992, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1273825004183013, "compression_ratio": 1.8700787401574803, "no_speech_prob": 1.4823223182247602e-06}, {"id": 118, "seek": 56564, "start": 587.8, "end": 592.92, "text": " By computing both j test and j train, you can now measure how it's doing on the test", "tokens": [50364, 550, 361, 1500, 576, 312, 264, 14135, 295, 300, 1500, 992, 689, 4018, 390, 20627, 382, 472, 50626, 50626, 420, 472, 20627, 382, 4018, 13, 50728, 50728, 400, 14138, 11, 361, 3847, 307, 257, 14135, 295, 264, 3097, 992, 300, 575, 668, 3346, 11665, 2587, 13, 51022, 51022, 17837, 257, 1412, 992, 293, 30348, 309, 666, 257, 3097, 992, 293, 257, 4994, 1500, 992, 2709, 51262, 51262, 291, 257, 636, 281, 39531, 13059, 577, 731, 428, 2539, 9284, 307, 884, 13, 51472, 51472, 3146, 15866, 1293, 361, 1500, 293, 361, 3847, 11, 291, 393, 586, 3481, 577, 309, 311, 884, 322, 264, 1500, 51728, 51728, 992, 293, 322, 264, 3097, 992, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1273825004183013, "compression_ratio": 1.8700787401574803, "no_speech_prob": 1.4823223182247602e-06}, {"id": 119, "seek": 56564, "start": 592.92, "end": 595.3199999999999, "text": " set and on the training set.", "tokens": [50364, 550, 361, 1500, 576, 312, 264, 14135, 295, 300, 1500, 992, 689, 4018, 390, 20627, 382, 472, 50626, 50626, 420, 472, 20627, 382, 4018, 13, 50728, 50728, 400, 14138, 11, 361, 3847, 307, 257, 14135, 295, 264, 3097, 992, 300, 575, 668, 3346, 11665, 2587, 13, 51022, 51022, 17837, 257, 1412, 992, 293, 30348, 309, 666, 257, 3097, 992, 293, 257, 4994, 1500, 992, 2709, 51262, 51262, 291, 257, 636, 281, 39531, 13059, 577, 731, 428, 2539, 9284, 307, 884, 13, 51472, 51472, 3146, 15866, 1293, 361, 1500, 293, 361, 3847, 11, 291, 393, 586, 3481, 577, 309, 311, 884, 322, 264, 1500, 51728, 51728, 992, 293, 322, 264, 3097, 992, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1273825004183013, "compression_ratio": 1.8700787401574803, "no_speech_prob": 1.4823223182247602e-06}, {"id": 120, "seek": 59532, "start": 595.32, "end": 600.5600000000001, "text": " This procedure is one step to what you being able to automatically choose what model to", "tokens": [50364, 639, 10747, 307, 472, 1823, 281, 437, 291, 885, 1075, 281, 6772, 2826, 437, 2316, 281, 50626, 50626, 764, 337, 257, 2212, 3479, 2539, 3861, 13, 50788, 50788, 1171, 1365, 11, 498, 291, 434, 1382, 281, 6069, 6849, 7901, 11, 820, 291, 3318, 257, 2997, 1622, 281, 51018, 51018, 428, 1412, 420, 3318, 257, 1150, 1668, 26110, 420, 2636, 1668, 420, 6409, 1668, 26110, 30, 51260, 51260, 467, 4523, 484, 300, 365, 472, 3052, 1895, 30229, 281, 264, 1558, 291, 1866, 294, 341, 960, 11, 291, 603, 51472, 51472, 312, 1075, 281, 362, 364, 9284, 854, 291, 281, 6772, 652, 300, 2010, 295, 3537, 731, 13, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.12204346223310991, "compression_ratio": 1.7234042553191489, "no_speech_prob": 1.544562474009581e-05}, {"id": 121, "seek": 59532, "start": 600.5600000000001, "end": 603.8000000000001, "text": " use for a given machine learning application.", "tokens": [50364, 639, 10747, 307, 472, 1823, 281, 437, 291, 885, 1075, 281, 6772, 2826, 437, 2316, 281, 50626, 50626, 764, 337, 257, 2212, 3479, 2539, 3861, 13, 50788, 50788, 1171, 1365, 11, 498, 291, 434, 1382, 281, 6069, 6849, 7901, 11, 820, 291, 3318, 257, 2997, 1622, 281, 51018, 51018, 428, 1412, 420, 3318, 257, 1150, 1668, 26110, 420, 2636, 1668, 420, 6409, 1668, 26110, 30, 51260, 51260, 467, 4523, 484, 300, 365, 472, 3052, 1895, 30229, 281, 264, 1558, 291, 1866, 294, 341, 960, 11, 291, 603, 51472, 51472, 312, 1075, 281, 362, 364, 9284, 854, 291, 281, 6772, 652, 300, 2010, 295, 3537, 731, 13, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.12204346223310991, "compression_ratio": 1.7234042553191489, "no_speech_prob": 1.544562474009581e-05}, {"id": 122, "seek": 59532, "start": 603.8000000000001, "end": 608.4000000000001, "text": " For example, if you're trying to predict housing prices, should you fit a straight line to", "tokens": [50364, 639, 10747, 307, 472, 1823, 281, 437, 291, 885, 1075, 281, 6772, 2826, 437, 2316, 281, 50626, 50626, 764, 337, 257, 2212, 3479, 2539, 3861, 13, 50788, 50788, 1171, 1365, 11, 498, 291, 434, 1382, 281, 6069, 6849, 7901, 11, 820, 291, 3318, 257, 2997, 1622, 281, 51018, 51018, 428, 1412, 420, 3318, 257, 1150, 1668, 26110, 420, 2636, 1668, 420, 6409, 1668, 26110, 30, 51260, 51260, 467, 4523, 484, 300, 365, 472, 3052, 1895, 30229, 281, 264, 1558, 291, 1866, 294, 341, 960, 11, 291, 603, 51472, 51472, 312, 1075, 281, 362, 364, 9284, 854, 291, 281, 6772, 652, 300, 2010, 295, 3537, 731, 13, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.12204346223310991, "compression_ratio": 1.7234042553191489, "no_speech_prob": 1.544562474009581e-05}, {"id": 123, "seek": 59532, "start": 608.4000000000001, "end": 613.24, "text": " your data or fit a second order polynomial or third order or fourth order polynomial?", "tokens": [50364, 639, 10747, 307, 472, 1823, 281, 437, 291, 885, 1075, 281, 6772, 2826, 437, 2316, 281, 50626, 50626, 764, 337, 257, 2212, 3479, 2539, 3861, 13, 50788, 50788, 1171, 1365, 11, 498, 291, 434, 1382, 281, 6069, 6849, 7901, 11, 820, 291, 3318, 257, 2997, 1622, 281, 51018, 51018, 428, 1412, 420, 3318, 257, 1150, 1668, 26110, 420, 2636, 1668, 420, 6409, 1668, 26110, 30, 51260, 51260, 467, 4523, 484, 300, 365, 472, 3052, 1895, 30229, 281, 264, 1558, 291, 1866, 294, 341, 960, 11, 291, 603, 51472, 51472, 312, 1075, 281, 362, 364, 9284, 854, 291, 281, 6772, 652, 300, 2010, 295, 3537, 731, 13, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.12204346223310991, "compression_ratio": 1.7234042553191489, "no_speech_prob": 1.544562474009581e-05}, {"id": 124, "seek": 59532, "start": 613.24, "end": 617.48, "text": " It turns out that with one further refinement to the idea you saw in this video, you'll", "tokens": [50364, 639, 10747, 307, 472, 1823, 281, 437, 291, 885, 1075, 281, 6772, 2826, 437, 2316, 281, 50626, 50626, 764, 337, 257, 2212, 3479, 2539, 3861, 13, 50788, 50788, 1171, 1365, 11, 498, 291, 434, 1382, 281, 6069, 6849, 7901, 11, 820, 291, 3318, 257, 2997, 1622, 281, 51018, 51018, 428, 1412, 420, 3318, 257, 1150, 1668, 26110, 420, 2636, 1668, 420, 6409, 1668, 26110, 30, 51260, 51260, 467, 4523, 484, 300, 365, 472, 3052, 1895, 30229, 281, 264, 1558, 291, 1866, 294, 341, 960, 11, 291, 603, 51472, 51472, 312, 1075, 281, 362, 364, 9284, 854, 291, 281, 6772, 652, 300, 2010, 295, 3537, 731, 13, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.12204346223310991, "compression_ratio": 1.7234042553191489, "no_speech_prob": 1.544562474009581e-05}, {"id": 125, "seek": 59532, "start": 617.48, "end": 622.08, "text": " be able to have an algorithm help you to automatically make that type of decision well.", "tokens": [50364, 639, 10747, 307, 472, 1823, 281, 437, 291, 885, 1075, 281, 6772, 2826, 437, 2316, 281, 50626, 50626, 764, 337, 257, 2212, 3479, 2539, 3861, 13, 50788, 50788, 1171, 1365, 11, 498, 291, 434, 1382, 281, 6069, 6849, 7901, 11, 820, 291, 3318, 257, 2997, 1622, 281, 51018, 51018, 428, 1412, 420, 3318, 257, 1150, 1668, 26110, 420, 2636, 1668, 420, 6409, 1668, 26110, 30, 51260, 51260, 467, 4523, 484, 300, 365, 472, 3052, 1895, 30229, 281, 264, 1558, 291, 1866, 294, 341, 960, 11, 291, 603, 51472, 51472, 312, 1075, 281, 362, 364, 9284, 854, 291, 281, 6772, 652, 300, 2010, 295, 3537, 731, 13, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.12204346223310991, "compression_ratio": 1.7234042553191489, "no_speech_prob": 1.544562474009581e-05}, {"id": 126, "seek": 62208, "start": 622.08, "end": 626.0, "text": " Let's take a look at how to do that in the next video.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 577, 281, 360, 300, 294, 264, 958, 960, 13, 50560], "temperature": 0.0, "avg_logprob": -0.22245224316914877, "compression_ratio": 0.9152542372881356, "no_speech_prob": 6.102163024479523e-05}], "language": "en", "video_id": "uNNx1Czrt1w", "entity": "ML Specialization, Andrew Ng (2022)"}}