{"video_id": "orElYjWScBw", "title": "5.4 Activation Functions | Choosing activation functions--[Machine Learning | Andrew Ng]", "description": "Second Course:\nAdvanced Learning Algorithms.\n\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 504, "views": 83, "publish_date": "11/04/2022", "timestamp": 1661472000, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " Let's take a look at how you can choose the activation function for different neurons in your neural network. We'll start with some guidance for how to choose it for the output layer. It turns out that depending on what the target label or the ground truth label Y is, there will be one fairly natural choice for the activation function for the output layer. And we'll then go and look at the choice of the activation function also for the hidden layers of your neural network. Let's take a look. You can choose different activation functions for different neurons in your neural network. And when considering the activation function for the output layer, it turns out that there will often be one fairly natural choice depending on what is the target or the ground truth label Y. Specifically, if you are working on a classification problem where Y is either 0 or 1, so a binary classification problem, then the sigmoid activation function will almost always be the most natural choice because then the neural network learns to predict the probability that Y is equal to 1, just like we had for logistic regression. So my recommendation is if you're working on a binary classification problem, use sigmoid at the output layer. Alternatively, if you're solving a regression problem, then you might choose a different activation function. For example, if you're trying to predict how tomorrow's stock price will change compared to today's stock price, well, it can go up or down. And so in this case, Y would be a number that can be either positive or negative. And in that case, I would recommend you use the linear activation function. Why is that? Well, that's because then the output of your neural network, f of x, which is equal to a3 in the example above, would be g applied to z3. And with the linear activation function, g of z can take on either positive or negative values. So Y can be positive or negative, use the linear activation function. And finally, if Y can only take on non-negative values, such as if you're predicting the price of a house, that can never be negative. Then the most natural choice would be the ReLU activation function. Because as you see here, this activation function only takes on non-negative values, either zero or positive values. So when choosing the activation function to use for your output layer, usually depending on what is the label, why you're trying to predict, there'll be one fairly natural choice. And in fact, the guidance on this slide is how I pretty much always choose my activation function as well for the output layer of a neural network. How about the hidden layers of a neural network? It turns out that the ReLU activation function is by far the most common choice in how neural networks are trained by many, many practitioners today. Even though we had initially described neural networks using the sigmoid activation function, and in fact, in the early history of the development of neural networks, people use sigmoid activation functions in many places, the field has evolved to use ReLU much more often and sigmoids hardly ever. With the one exception that you do use a sigmoid activation function in the output layer if you have a binary classification problem. So why is that? Well, there are a few reasons. First, if you compare the ReLU and the sigmoid activation functions, the ReLU is a bit faster to compute because it just requires computing max of 0, z, whereas the sigmoid requires taking an exponentiation and an inverse and so on, and so it's a little bit less efficient. But the second reason, which turns out to be even more important, is that the ReLU function kind of goes flat only in one part of the graph, here on the left, it's completely flat. Whereas the sigmoid activation function, it kind of goes flat in two places. It goes flat to the left of the graph and it goes flat to the right of the graph. And if you're using gradient descent to train a neural network, then when you have a function that is flat in a lot of places, gradient descent will be really slow. I know that gradient descent optimizes the cost function J of WB rather than optimizes the activation function, but the activation function is a piece of what goes into computing, and that results in more places in the cost function J of WB that are flat as well and with a smaller gradient and it slows down learning. I know that that was just an intuitive explanation, but researchers have found that using the ReLU activation function can cause your neural network to learn a bit faster as well, which is why for most practitioners, if you're trying to decide what activation function to use for the hidden layer, the ReLU activation function has become now by far the most common choice. In fact, if I'm building a neural network, this is how I choose activation functions for the hidden layers as well. So to summarize, here's what I recommend in terms of how you choose the activation functions for your neural network. For the output layer, use a sigmoid if you have a binary classification problem, linear if Y is a number that can take on positive or negative values, or use ReLU if Y can take on only positive values or zero positive values or non-negative values. Then for the hidden layers, I would recommend just using ReLU as a default activation function. And in TensorFlow, this is how you would implement it. Rather than saying activation equals sigmoid as we had previously, you can then for the hidden layers, that's the first hidden layer, the second hidden layer, ask TensorFlow to use the ReLU activation function. And then for the output layer, in this example, I've asked it to use the sigmoid activation function. But if you want it to use the linear activation function instead, that's the syntax for it. Or if you wanted to use the ReLU activation function, that shows the syntax for it. With this richer set of activation functions, you'd be well positioned to build much more powerful neural networks than just once using only the sigmoid activation function. By the way, if you look at the research literature, you sometimes hear of authors using even other activation functions, such as the tanh activation function or the leaky ReLU activation function or the swish activation function. Every few years, researchers sometimes come up with another interesting activation function, and sometimes they do work a little bit better. For example, I've used the leaky ReLU activation function a few times in my work, and sometimes it works a little bit better than the ReLU activation function you learned about in this video. But I think for the most part, and for the vast majority of applications, what you learned about in this video would be good enough. Of course, if you want to learn more about other activation functions, feel free to look on the internet, and there are just a small handful of cases where these other activation functions could be even more powerful as well. With that, I hope you also enjoy practicing these ideas, these activation functions, in the optional labs and in the practice labs. But this raises yet another question. Why do we even need activation functions at all? Why don't we just use the linear activation function, or use no activation function anywhere? It turns out this does not work at all. And in the next video, let's take a look at why that's the case, and why activation functions are so important for getting your neural networks to work.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.3, "text": " Let's take a look at how you can choose the activation function for different neurons", "tokens": [50364, 961, 311, 747, 257, 574, 412, 577, 291, 393, 2826, 264, 24433, 2445, 337, 819, 22027, 50679, 50679, 294, 428, 18161, 3209, 13, 50756, 50756, 492, 603, 722, 365, 512, 10056, 337, 577, 281, 2826, 309, 337, 264, 5598, 4583, 13, 50985, 50985, 467, 4523, 484, 300, 5413, 322, 437, 264, 3779, 7645, 420, 264, 2727, 3494, 7645, 398, 307, 11, 456, 51258, 51258, 486, 312, 472, 6457, 3303, 3922, 337, 264, 24433, 2445, 337, 264, 5598, 4583, 13, 51571, 51571, 400, 321, 603, 550, 352, 293, 574, 412, 264, 3922, 295, 264, 24433, 2445, 611, 337, 264, 7633, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.14019307812440743, "compression_ratio": 1.8739495798319328, "no_speech_prob": 0.00926437508314848}, {"id": 1, "seek": 0, "start": 6.3, "end": 7.84, "text": " in your neural network.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 577, 291, 393, 2826, 264, 24433, 2445, 337, 819, 22027, 50679, 50679, 294, 428, 18161, 3209, 13, 50756, 50756, 492, 603, 722, 365, 512, 10056, 337, 577, 281, 2826, 309, 337, 264, 5598, 4583, 13, 50985, 50985, 467, 4523, 484, 300, 5413, 322, 437, 264, 3779, 7645, 420, 264, 2727, 3494, 7645, 398, 307, 11, 456, 51258, 51258, 486, 312, 472, 6457, 3303, 3922, 337, 264, 24433, 2445, 337, 264, 5598, 4583, 13, 51571, 51571, 400, 321, 603, 550, 352, 293, 574, 412, 264, 3922, 295, 264, 24433, 2445, 611, 337, 264, 7633, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.14019307812440743, "compression_ratio": 1.8739495798319328, "no_speech_prob": 0.00926437508314848}, {"id": 2, "seek": 0, "start": 7.84, "end": 12.42, "text": " We'll start with some guidance for how to choose it for the output layer.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 577, 291, 393, 2826, 264, 24433, 2445, 337, 819, 22027, 50679, 50679, 294, 428, 18161, 3209, 13, 50756, 50756, 492, 603, 722, 365, 512, 10056, 337, 577, 281, 2826, 309, 337, 264, 5598, 4583, 13, 50985, 50985, 467, 4523, 484, 300, 5413, 322, 437, 264, 3779, 7645, 420, 264, 2727, 3494, 7645, 398, 307, 11, 456, 51258, 51258, 486, 312, 472, 6457, 3303, 3922, 337, 264, 24433, 2445, 337, 264, 5598, 4583, 13, 51571, 51571, 400, 321, 603, 550, 352, 293, 574, 412, 264, 3922, 295, 264, 24433, 2445, 611, 337, 264, 7633, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.14019307812440743, "compression_ratio": 1.8739495798319328, "no_speech_prob": 0.00926437508314848}, {"id": 3, "seek": 0, "start": 12.42, "end": 17.88, "text": " It turns out that depending on what the target label or the ground truth label Y is, there", "tokens": [50364, 961, 311, 747, 257, 574, 412, 577, 291, 393, 2826, 264, 24433, 2445, 337, 819, 22027, 50679, 50679, 294, 428, 18161, 3209, 13, 50756, 50756, 492, 603, 722, 365, 512, 10056, 337, 577, 281, 2826, 309, 337, 264, 5598, 4583, 13, 50985, 50985, 467, 4523, 484, 300, 5413, 322, 437, 264, 3779, 7645, 420, 264, 2727, 3494, 7645, 398, 307, 11, 456, 51258, 51258, 486, 312, 472, 6457, 3303, 3922, 337, 264, 24433, 2445, 337, 264, 5598, 4583, 13, 51571, 51571, 400, 321, 603, 550, 352, 293, 574, 412, 264, 3922, 295, 264, 24433, 2445, 611, 337, 264, 7633, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.14019307812440743, "compression_ratio": 1.8739495798319328, "no_speech_prob": 0.00926437508314848}, {"id": 4, "seek": 0, "start": 17.88, "end": 24.14, "text": " will be one fairly natural choice for the activation function for the output layer.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 577, 291, 393, 2826, 264, 24433, 2445, 337, 819, 22027, 50679, 50679, 294, 428, 18161, 3209, 13, 50756, 50756, 492, 603, 722, 365, 512, 10056, 337, 577, 281, 2826, 309, 337, 264, 5598, 4583, 13, 50985, 50985, 467, 4523, 484, 300, 5413, 322, 437, 264, 3779, 7645, 420, 264, 2727, 3494, 7645, 398, 307, 11, 456, 51258, 51258, 486, 312, 472, 6457, 3303, 3922, 337, 264, 24433, 2445, 337, 264, 5598, 4583, 13, 51571, 51571, 400, 321, 603, 550, 352, 293, 574, 412, 264, 3922, 295, 264, 24433, 2445, 611, 337, 264, 7633, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.14019307812440743, "compression_ratio": 1.8739495798319328, "no_speech_prob": 0.00926437508314848}, {"id": 5, "seek": 0, "start": 24.14, "end": 28.48, "text": " And we'll then go and look at the choice of the activation function also for the hidden", "tokens": [50364, 961, 311, 747, 257, 574, 412, 577, 291, 393, 2826, 264, 24433, 2445, 337, 819, 22027, 50679, 50679, 294, 428, 18161, 3209, 13, 50756, 50756, 492, 603, 722, 365, 512, 10056, 337, 577, 281, 2826, 309, 337, 264, 5598, 4583, 13, 50985, 50985, 467, 4523, 484, 300, 5413, 322, 437, 264, 3779, 7645, 420, 264, 2727, 3494, 7645, 398, 307, 11, 456, 51258, 51258, 486, 312, 472, 6457, 3303, 3922, 337, 264, 24433, 2445, 337, 264, 5598, 4583, 13, 51571, 51571, 400, 321, 603, 550, 352, 293, 574, 412, 264, 3922, 295, 264, 24433, 2445, 611, 337, 264, 7633, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.14019307812440743, "compression_ratio": 1.8739495798319328, "no_speech_prob": 0.00926437508314848}, {"id": 6, "seek": 2848, "start": 28.48, "end": 30.28, "text": " layers of your neural network.", "tokens": [50364, 7914, 295, 428, 18161, 3209, 13, 50454, 50454, 961, 311, 747, 257, 574, 13, 50526, 50526, 509, 393, 2826, 819, 24433, 6828, 337, 819, 22027, 294, 428, 18161, 3209, 13, 50838, 50838, 400, 562, 8079, 264, 24433, 2445, 337, 264, 5598, 4583, 11, 309, 4523, 484, 300, 456, 51136, 51136, 486, 2049, 312, 472, 6457, 3303, 3922, 5413, 322, 437, 307, 264, 3779, 420, 264, 2727, 3494, 51482, 51482, 7645, 398, 13, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.13041065868578458, "compression_ratio": 1.6989795918367347, "no_speech_prob": 6.643255801463965e-06}, {"id": 7, "seek": 2848, "start": 30.28, "end": 31.72, "text": " Let's take a look.", "tokens": [50364, 7914, 295, 428, 18161, 3209, 13, 50454, 50454, 961, 311, 747, 257, 574, 13, 50526, 50526, 509, 393, 2826, 819, 24433, 6828, 337, 819, 22027, 294, 428, 18161, 3209, 13, 50838, 50838, 400, 562, 8079, 264, 24433, 2445, 337, 264, 5598, 4583, 11, 309, 4523, 484, 300, 456, 51136, 51136, 486, 2049, 312, 472, 6457, 3303, 3922, 5413, 322, 437, 307, 264, 3779, 420, 264, 2727, 3494, 51482, 51482, 7645, 398, 13, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.13041065868578458, "compression_ratio": 1.6989795918367347, "no_speech_prob": 6.643255801463965e-06}, {"id": 8, "seek": 2848, "start": 31.72, "end": 37.96, "text": " You can choose different activation functions for different neurons in your neural network.", "tokens": [50364, 7914, 295, 428, 18161, 3209, 13, 50454, 50454, 961, 311, 747, 257, 574, 13, 50526, 50526, 509, 393, 2826, 819, 24433, 6828, 337, 819, 22027, 294, 428, 18161, 3209, 13, 50838, 50838, 400, 562, 8079, 264, 24433, 2445, 337, 264, 5598, 4583, 11, 309, 4523, 484, 300, 456, 51136, 51136, 486, 2049, 312, 472, 6457, 3303, 3922, 5413, 322, 437, 307, 264, 3779, 420, 264, 2727, 3494, 51482, 51482, 7645, 398, 13, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.13041065868578458, "compression_ratio": 1.6989795918367347, "no_speech_prob": 6.643255801463965e-06}, {"id": 9, "seek": 2848, "start": 37.96, "end": 43.92, "text": " And when considering the activation function for the output layer, it turns out that there", "tokens": [50364, 7914, 295, 428, 18161, 3209, 13, 50454, 50454, 961, 311, 747, 257, 574, 13, 50526, 50526, 509, 393, 2826, 819, 24433, 6828, 337, 819, 22027, 294, 428, 18161, 3209, 13, 50838, 50838, 400, 562, 8079, 264, 24433, 2445, 337, 264, 5598, 4583, 11, 309, 4523, 484, 300, 456, 51136, 51136, 486, 2049, 312, 472, 6457, 3303, 3922, 5413, 322, 437, 307, 264, 3779, 420, 264, 2727, 3494, 51482, 51482, 7645, 398, 13, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.13041065868578458, "compression_ratio": 1.6989795918367347, "no_speech_prob": 6.643255801463965e-06}, {"id": 10, "seek": 2848, "start": 43.92, "end": 50.84, "text": " will often be one fairly natural choice depending on what is the target or the ground truth", "tokens": [50364, 7914, 295, 428, 18161, 3209, 13, 50454, 50454, 961, 311, 747, 257, 574, 13, 50526, 50526, 509, 393, 2826, 819, 24433, 6828, 337, 819, 22027, 294, 428, 18161, 3209, 13, 50838, 50838, 400, 562, 8079, 264, 24433, 2445, 337, 264, 5598, 4583, 11, 309, 4523, 484, 300, 456, 51136, 51136, 486, 2049, 312, 472, 6457, 3303, 3922, 5413, 322, 437, 307, 264, 3779, 420, 264, 2727, 3494, 51482, 51482, 7645, 398, 13, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.13041065868578458, "compression_ratio": 1.6989795918367347, "no_speech_prob": 6.643255801463965e-06}, {"id": 11, "seek": 2848, "start": 50.84, "end": 51.84, "text": " label Y.", "tokens": [50364, 7914, 295, 428, 18161, 3209, 13, 50454, 50454, 961, 311, 747, 257, 574, 13, 50526, 50526, 509, 393, 2826, 819, 24433, 6828, 337, 819, 22027, 294, 428, 18161, 3209, 13, 50838, 50838, 400, 562, 8079, 264, 24433, 2445, 337, 264, 5598, 4583, 11, 309, 4523, 484, 300, 456, 51136, 51136, 486, 2049, 312, 472, 6457, 3303, 3922, 5413, 322, 437, 307, 264, 3779, 420, 264, 2727, 3494, 51482, 51482, 7645, 398, 13, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.13041065868578458, "compression_ratio": 1.6989795918367347, "no_speech_prob": 6.643255801463965e-06}, {"id": 12, "seek": 5184, "start": 51.84, "end": 59.64, "text": " Specifically, if you are working on a classification problem where Y is either 0 or 1, so a binary", "tokens": [50364, 26058, 11, 498, 291, 366, 1364, 322, 257, 21538, 1154, 689, 398, 307, 2139, 1958, 420, 502, 11, 370, 257, 17434, 50754, 50754, 21538, 1154, 11, 550, 264, 4556, 3280, 327, 24433, 2445, 486, 1920, 1009, 312, 264, 881, 3303, 51080, 51080, 3922, 570, 550, 264, 18161, 3209, 27152, 281, 6069, 264, 8482, 300, 398, 307, 2681, 51416, 51416, 281, 502, 11, 445, 411, 321, 632, 337, 3565, 3142, 24590, 13, 51586, 51586, 407, 452, 11879, 307, 498, 291, 434, 1364, 322, 257, 17434, 21538, 1154, 11, 764, 4556, 3280, 327, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.09067936947471217, "compression_ratio": 1.8162393162393162, "no_speech_prob": 2.2473321223515086e-05}, {"id": 13, "seek": 5184, "start": 59.64, "end": 66.16, "text": " classification problem, then the sigmoid activation function will almost always be the most natural", "tokens": [50364, 26058, 11, 498, 291, 366, 1364, 322, 257, 21538, 1154, 689, 398, 307, 2139, 1958, 420, 502, 11, 370, 257, 17434, 50754, 50754, 21538, 1154, 11, 550, 264, 4556, 3280, 327, 24433, 2445, 486, 1920, 1009, 312, 264, 881, 3303, 51080, 51080, 3922, 570, 550, 264, 18161, 3209, 27152, 281, 6069, 264, 8482, 300, 398, 307, 2681, 51416, 51416, 281, 502, 11, 445, 411, 321, 632, 337, 3565, 3142, 24590, 13, 51586, 51586, 407, 452, 11879, 307, 498, 291, 434, 1364, 322, 257, 17434, 21538, 1154, 11, 764, 4556, 3280, 327, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.09067936947471217, "compression_ratio": 1.8162393162393162, "no_speech_prob": 2.2473321223515086e-05}, {"id": 14, "seek": 5184, "start": 66.16, "end": 72.88, "text": " choice because then the neural network learns to predict the probability that Y is equal", "tokens": [50364, 26058, 11, 498, 291, 366, 1364, 322, 257, 21538, 1154, 689, 398, 307, 2139, 1958, 420, 502, 11, 370, 257, 17434, 50754, 50754, 21538, 1154, 11, 550, 264, 4556, 3280, 327, 24433, 2445, 486, 1920, 1009, 312, 264, 881, 3303, 51080, 51080, 3922, 570, 550, 264, 18161, 3209, 27152, 281, 6069, 264, 8482, 300, 398, 307, 2681, 51416, 51416, 281, 502, 11, 445, 411, 321, 632, 337, 3565, 3142, 24590, 13, 51586, 51586, 407, 452, 11879, 307, 498, 291, 434, 1364, 322, 257, 17434, 21538, 1154, 11, 764, 4556, 3280, 327, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.09067936947471217, "compression_ratio": 1.8162393162393162, "no_speech_prob": 2.2473321223515086e-05}, {"id": 15, "seek": 5184, "start": 72.88, "end": 76.28, "text": " to 1, just like we had for logistic regression.", "tokens": [50364, 26058, 11, 498, 291, 366, 1364, 322, 257, 21538, 1154, 689, 398, 307, 2139, 1958, 420, 502, 11, 370, 257, 17434, 50754, 50754, 21538, 1154, 11, 550, 264, 4556, 3280, 327, 24433, 2445, 486, 1920, 1009, 312, 264, 881, 3303, 51080, 51080, 3922, 570, 550, 264, 18161, 3209, 27152, 281, 6069, 264, 8482, 300, 398, 307, 2681, 51416, 51416, 281, 502, 11, 445, 411, 321, 632, 337, 3565, 3142, 24590, 13, 51586, 51586, 407, 452, 11879, 307, 498, 291, 434, 1364, 322, 257, 17434, 21538, 1154, 11, 764, 4556, 3280, 327, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.09067936947471217, "compression_ratio": 1.8162393162393162, "no_speech_prob": 2.2473321223515086e-05}, {"id": 16, "seek": 5184, "start": 76.28, "end": 81.60000000000001, "text": " So my recommendation is if you're working on a binary classification problem, use sigmoid", "tokens": [50364, 26058, 11, 498, 291, 366, 1364, 322, 257, 21538, 1154, 689, 398, 307, 2139, 1958, 420, 502, 11, 370, 257, 17434, 50754, 50754, 21538, 1154, 11, 550, 264, 4556, 3280, 327, 24433, 2445, 486, 1920, 1009, 312, 264, 881, 3303, 51080, 51080, 3922, 570, 550, 264, 18161, 3209, 27152, 281, 6069, 264, 8482, 300, 398, 307, 2681, 51416, 51416, 281, 502, 11, 445, 411, 321, 632, 337, 3565, 3142, 24590, 13, 51586, 51586, 407, 452, 11879, 307, 498, 291, 434, 1364, 322, 257, 17434, 21538, 1154, 11, 764, 4556, 3280, 327, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.09067936947471217, "compression_ratio": 1.8162393162393162, "no_speech_prob": 2.2473321223515086e-05}, {"id": 17, "seek": 8160, "start": 81.6, "end": 83.36, "text": " at the output layer.", "tokens": [50364, 412, 264, 5598, 4583, 13, 50452, 50452, 46167, 11, 498, 291, 434, 12606, 257, 24590, 1154, 11, 550, 291, 1062, 2826, 257, 819, 50706, 50706, 24433, 2445, 13, 50798, 50798, 1171, 1365, 11, 498, 291, 434, 1382, 281, 6069, 577, 4153, 311, 4127, 3218, 486, 1319, 5347, 51126, 51126, 281, 965, 311, 4127, 3218, 11, 731, 11, 309, 393, 352, 493, 420, 760, 13, 51284, 51284, 400, 370, 294, 341, 1389, 11, 398, 576, 312, 257, 1230, 300, 393, 312, 2139, 3353, 420, 3671, 13, 51534, 51534, 400, 294, 300, 1389, 11, 286, 576, 2748, 291, 764, 264, 8213, 24433, 2445, 13, 51800, 51800, 1545, 307, 300, 30, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1002911925315857, "compression_ratio": 1.655430711610487, "no_speech_prob": 1.428519044566201e-05}, {"id": 18, "seek": 8160, "start": 83.36, "end": 88.44, "text": " Alternatively, if you're solving a regression problem, then you might choose a different", "tokens": [50364, 412, 264, 5598, 4583, 13, 50452, 50452, 46167, 11, 498, 291, 434, 12606, 257, 24590, 1154, 11, 550, 291, 1062, 2826, 257, 819, 50706, 50706, 24433, 2445, 13, 50798, 50798, 1171, 1365, 11, 498, 291, 434, 1382, 281, 6069, 577, 4153, 311, 4127, 3218, 486, 1319, 5347, 51126, 51126, 281, 965, 311, 4127, 3218, 11, 731, 11, 309, 393, 352, 493, 420, 760, 13, 51284, 51284, 400, 370, 294, 341, 1389, 11, 398, 576, 312, 257, 1230, 300, 393, 312, 2139, 3353, 420, 3671, 13, 51534, 51534, 400, 294, 300, 1389, 11, 286, 576, 2748, 291, 764, 264, 8213, 24433, 2445, 13, 51800, 51800, 1545, 307, 300, 30, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1002911925315857, "compression_ratio": 1.655430711610487, "no_speech_prob": 1.428519044566201e-05}, {"id": 19, "seek": 8160, "start": 88.44, "end": 90.28, "text": " activation function.", "tokens": [50364, 412, 264, 5598, 4583, 13, 50452, 50452, 46167, 11, 498, 291, 434, 12606, 257, 24590, 1154, 11, 550, 291, 1062, 2826, 257, 819, 50706, 50706, 24433, 2445, 13, 50798, 50798, 1171, 1365, 11, 498, 291, 434, 1382, 281, 6069, 577, 4153, 311, 4127, 3218, 486, 1319, 5347, 51126, 51126, 281, 965, 311, 4127, 3218, 11, 731, 11, 309, 393, 352, 493, 420, 760, 13, 51284, 51284, 400, 370, 294, 341, 1389, 11, 398, 576, 312, 257, 1230, 300, 393, 312, 2139, 3353, 420, 3671, 13, 51534, 51534, 400, 294, 300, 1389, 11, 286, 576, 2748, 291, 764, 264, 8213, 24433, 2445, 13, 51800, 51800, 1545, 307, 300, 30, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1002911925315857, "compression_ratio": 1.655430711610487, "no_speech_prob": 1.428519044566201e-05}, {"id": 20, "seek": 8160, "start": 90.28, "end": 96.83999999999999, "text": " For example, if you're trying to predict how tomorrow's stock price will change compared", "tokens": [50364, 412, 264, 5598, 4583, 13, 50452, 50452, 46167, 11, 498, 291, 434, 12606, 257, 24590, 1154, 11, 550, 291, 1062, 2826, 257, 819, 50706, 50706, 24433, 2445, 13, 50798, 50798, 1171, 1365, 11, 498, 291, 434, 1382, 281, 6069, 577, 4153, 311, 4127, 3218, 486, 1319, 5347, 51126, 51126, 281, 965, 311, 4127, 3218, 11, 731, 11, 309, 393, 352, 493, 420, 760, 13, 51284, 51284, 400, 370, 294, 341, 1389, 11, 398, 576, 312, 257, 1230, 300, 393, 312, 2139, 3353, 420, 3671, 13, 51534, 51534, 400, 294, 300, 1389, 11, 286, 576, 2748, 291, 764, 264, 8213, 24433, 2445, 13, 51800, 51800, 1545, 307, 300, 30, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1002911925315857, "compression_ratio": 1.655430711610487, "no_speech_prob": 1.428519044566201e-05}, {"id": 21, "seek": 8160, "start": 96.83999999999999, "end": 100.0, "text": " to today's stock price, well, it can go up or down.", "tokens": [50364, 412, 264, 5598, 4583, 13, 50452, 50452, 46167, 11, 498, 291, 434, 12606, 257, 24590, 1154, 11, 550, 291, 1062, 2826, 257, 819, 50706, 50706, 24433, 2445, 13, 50798, 50798, 1171, 1365, 11, 498, 291, 434, 1382, 281, 6069, 577, 4153, 311, 4127, 3218, 486, 1319, 5347, 51126, 51126, 281, 965, 311, 4127, 3218, 11, 731, 11, 309, 393, 352, 493, 420, 760, 13, 51284, 51284, 400, 370, 294, 341, 1389, 11, 398, 576, 312, 257, 1230, 300, 393, 312, 2139, 3353, 420, 3671, 13, 51534, 51534, 400, 294, 300, 1389, 11, 286, 576, 2748, 291, 764, 264, 8213, 24433, 2445, 13, 51800, 51800, 1545, 307, 300, 30, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1002911925315857, "compression_ratio": 1.655430711610487, "no_speech_prob": 1.428519044566201e-05}, {"id": 22, "seek": 8160, "start": 100.0, "end": 105.0, "text": " And so in this case, Y would be a number that can be either positive or negative.", "tokens": [50364, 412, 264, 5598, 4583, 13, 50452, 50452, 46167, 11, 498, 291, 434, 12606, 257, 24590, 1154, 11, 550, 291, 1062, 2826, 257, 819, 50706, 50706, 24433, 2445, 13, 50798, 50798, 1171, 1365, 11, 498, 291, 434, 1382, 281, 6069, 577, 4153, 311, 4127, 3218, 486, 1319, 5347, 51126, 51126, 281, 965, 311, 4127, 3218, 11, 731, 11, 309, 393, 352, 493, 420, 760, 13, 51284, 51284, 400, 370, 294, 341, 1389, 11, 398, 576, 312, 257, 1230, 300, 393, 312, 2139, 3353, 420, 3671, 13, 51534, 51534, 400, 294, 300, 1389, 11, 286, 576, 2748, 291, 764, 264, 8213, 24433, 2445, 13, 51800, 51800, 1545, 307, 300, 30, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1002911925315857, "compression_ratio": 1.655430711610487, "no_speech_prob": 1.428519044566201e-05}, {"id": 23, "seek": 8160, "start": 105.0, "end": 110.32, "text": " And in that case, I would recommend you use the linear activation function.", "tokens": [50364, 412, 264, 5598, 4583, 13, 50452, 50452, 46167, 11, 498, 291, 434, 12606, 257, 24590, 1154, 11, 550, 291, 1062, 2826, 257, 819, 50706, 50706, 24433, 2445, 13, 50798, 50798, 1171, 1365, 11, 498, 291, 434, 1382, 281, 6069, 577, 4153, 311, 4127, 3218, 486, 1319, 5347, 51126, 51126, 281, 965, 311, 4127, 3218, 11, 731, 11, 309, 393, 352, 493, 420, 760, 13, 51284, 51284, 400, 370, 294, 341, 1389, 11, 398, 576, 312, 257, 1230, 300, 393, 312, 2139, 3353, 420, 3671, 13, 51534, 51534, 400, 294, 300, 1389, 11, 286, 576, 2748, 291, 764, 264, 8213, 24433, 2445, 13, 51800, 51800, 1545, 307, 300, 30, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1002911925315857, "compression_ratio": 1.655430711610487, "no_speech_prob": 1.428519044566201e-05}, {"id": 24, "seek": 8160, "start": 110.32, "end": 111.32, "text": " Why is that?", "tokens": [50364, 412, 264, 5598, 4583, 13, 50452, 50452, 46167, 11, 498, 291, 434, 12606, 257, 24590, 1154, 11, 550, 291, 1062, 2826, 257, 819, 50706, 50706, 24433, 2445, 13, 50798, 50798, 1171, 1365, 11, 498, 291, 434, 1382, 281, 6069, 577, 4153, 311, 4127, 3218, 486, 1319, 5347, 51126, 51126, 281, 965, 311, 4127, 3218, 11, 731, 11, 309, 393, 352, 493, 420, 760, 13, 51284, 51284, 400, 370, 294, 341, 1389, 11, 398, 576, 312, 257, 1230, 300, 393, 312, 2139, 3353, 420, 3671, 13, 51534, 51534, 400, 294, 300, 1389, 11, 286, 576, 2748, 291, 764, 264, 8213, 24433, 2445, 13, 51800, 51800, 1545, 307, 300, 30, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1002911925315857, "compression_ratio": 1.655430711610487, "no_speech_prob": 1.428519044566201e-05}, {"id": 25, "seek": 11132, "start": 111.32, "end": 115.55999999999999, "text": " Well, that's because then the output of your neural network, f of x, which is equal to", "tokens": [50364, 1042, 11, 300, 311, 570, 550, 264, 5598, 295, 428, 18161, 3209, 11, 283, 295, 2031, 11, 597, 307, 2681, 281, 50576, 50576, 257, 18, 294, 264, 1365, 3673, 11, 576, 312, 290, 6456, 281, 710, 18, 13, 50860, 50860, 400, 365, 264, 8213, 24433, 2445, 11, 290, 295, 710, 393, 747, 322, 2139, 3353, 420, 3671, 51142, 51142, 4190, 13, 51192, 51192, 407, 398, 393, 312, 3353, 420, 3671, 11, 764, 264, 8213, 24433, 2445, 13, 51398, 51398, 400, 2721, 11, 498, 398, 393, 787, 747, 322, 2107, 12, 28561, 1166, 4190, 11, 1270, 382, 498, 291, 434, 32884, 264, 3218, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.12882851654628538, "compression_ratio": 1.7155172413793103, "no_speech_prob": 5.771736141468864e-06}, {"id": 26, "seek": 11132, "start": 115.55999999999999, "end": 121.24, "text": " a3 in the example above, would be g applied to z3.", "tokens": [50364, 1042, 11, 300, 311, 570, 550, 264, 5598, 295, 428, 18161, 3209, 11, 283, 295, 2031, 11, 597, 307, 2681, 281, 50576, 50576, 257, 18, 294, 264, 1365, 3673, 11, 576, 312, 290, 6456, 281, 710, 18, 13, 50860, 50860, 400, 365, 264, 8213, 24433, 2445, 11, 290, 295, 710, 393, 747, 322, 2139, 3353, 420, 3671, 51142, 51142, 4190, 13, 51192, 51192, 407, 398, 393, 312, 3353, 420, 3671, 11, 764, 264, 8213, 24433, 2445, 13, 51398, 51398, 400, 2721, 11, 498, 398, 393, 787, 747, 322, 2107, 12, 28561, 1166, 4190, 11, 1270, 382, 498, 291, 434, 32884, 264, 3218, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.12882851654628538, "compression_ratio": 1.7155172413793103, "no_speech_prob": 5.771736141468864e-06}, {"id": 27, "seek": 11132, "start": 121.24, "end": 126.88, "text": " And with the linear activation function, g of z can take on either positive or negative", "tokens": [50364, 1042, 11, 300, 311, 570, 550, 264, 5598, 295, 428, 18161, 3209, 11, 283, 295, 2031, 11, 597, 307, 2681, 281, 50576, 50576, 257, 18, 294, 264, 1365, 3673, 11, 576, 312, 290, 6456, 281, 710, 18, 13, 50860, 50860, 400, 365, 264, 8213, 24433, 2445, 11, 290, 295, 710, 393, 747, 322, 2139, 3353, 420, 3671, 51142, 51142, 4190, 13, 51192, 51192, 407, 398, 393, 312, 3353, 420, 3671, 11, 764, 264, 8213, 24433, 2445, 13, 51398, 51398, 400, 2721, 11, 498, 398, 393, 787, 747, 322, 2107, 12, 28561, 1166, 4190, 11, 1270, 382, 498, 291, 434, 32884, 264, 3218, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.12882851654628538, "compression_ratio": 1.7155172413793103, "no_speech_prob": 5.771736141468864e-06}, {"id": 28, "seek": 11132, "start": 126.88, "end": 127.88, "text": " values.", "tokens": [50364, 1042, 11, 300, 311, 570, 550, 264, 5598, 295, 428, 18161, 3209, 11, 283, 295, 2031, 11, 597, 307, 2681, 281, 50576, 50576, 257, 18, 294, 264, 1365, 3673, 11, 576, 312, 290, 6456, 281, 710, 18, 13, 50860, 50860, 400, 365, 264, 8213, 24433, 2445, 11, 290, 295, 710, 393, 747, 322, 2139, 3353, 420, 3671, 51142, 51142, 4190, 13, 51192, 51192, 407, 398, 393, 312, 3353, 420, 3671, 11, 764, 264, 8213, 24433, 2445, 13, 51398, 51398, 400, 2721, 11, 498, 398, 393, 787, 747, 322, 2107, 12, 28561, 1166, 4190, 11, 1270, 382, 498, 291, 434, 32884, 264, 3218, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.12882851654628538, "compression_ratio": 1.7155172413793103, "no_speech_prob": 5.771736141468864e-06}, {"id": 29, "seek": 11132, "start": 127.88, "end": 132.0, "text": " So Y can be positive or negative, use the linear activation function.", "tokens": [50364, 1042, 11, 300, 311, 570, 550, 264, 5598, 295, 428, 18161, 3209, 11, 283, 295, 2031, 11, 597, 307, 2681, 281, 50576, 50576, 257, 18, 294, 264, 1365, 3673, 11, 576, 312, 290, 6456, 281, 710, 18, 13, 50860, 50860, 400, 365, 264, 8213, 24433, 2445, 11, 290, 295, 710, 393, 747, 322, 2139, 3353, 420, 3671, 51142, 51142, 4190, 13, 51192, 51192, 407, 398, 393, 312, 3353, 420, 3671, 11, 764, 264, 8213, 24433, 2445, 13, 51398, 51398, 400, 2721, 11, 498, 398, 393, 787, 747, 322, 2107, 12, 28561, 1166, 4190, 11, 1270, 382, 498, 291, 434, 32884, 264, 3218, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.12882851654628538, "compression_ratio": 1.7155172413793103, "no_speech_prob": 5.771736141468864e-06}, {"id": 30, "seek": 11132, "start": 132.0, "end": 139.04, "text": " And finally, if Y can only take on non-negative values, such as if you're predicting the price", "tokens": [50364, 1042, 11, 300, 311, 570, 550, 264, 5598, 295, 428, 18161, 3209, 11, 283, 295, 2031, 11, 597, 307, 2681, 281, 50576, 50576, 257, 18, 294, 264, 1365, 3673, 11, 576, 312, 290, 6456, 281, 710, 18, 13, 50860, 50860, 400, 365, 264, 8213, 24433, 2445, 11, 290, 295, 710, 393, 747, 322, 2139, 3353, 420, 3671, 51142, 51142, 4190, 13, 51192, 51192, 407, 398, 393, 312, 3353, 420, 3671, 11, 764, 264, 8213, 24433, 2445, 13, 51398, 51398, 400, 2721, 11, 498, 398, 393, 787, 747, 322, 2107, 12, 28561, 1166, 4190, 11, 1270, 382, 498, 291, 434, 32884, 264, 3218, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.12882851654628538, "compression_ratio": 1.7155172413793103, "no_speech_prob": 5.771736141468864e-06}, {"id": 31, "seek": 13904, "start": 139.04, "end": 142.16, "text": " of a house, that can never be negative.", "tokens": [50364, 295, 257, 1782, 11, 300, 393, 1128, 312, 3671, 13, 50520, 50520, 1396, 264, 881, 3303, 3922, 576, 312, 264, 1300, 43, 52, 24433, 2445, 13, 50726, 50726, 1436, 382, 291, 536, 510, 11, 341, 24433, 2445, 787, 2516, 322, 2107, 12, 28561, 1166, 4190, 11, 2139, 50992, 50992, 4018, 420, 3353, 4190, 13, 51097, 51097, 407, 562, 10875, 264, 24433, 2445, 281, 764, 337, 428, 5598, 4583, 11, 2673, 5413, 51368, 51368, 322, 437, 307, 264, 7645, 11, 983, 291, 434, 1382, 281, 6069, 11, 456, 603, 312, 472, 6457, 3303, 3922, 13, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.13629695347377233, "compression_ratio": 1.7118644067796611, "no_speech_prob": 1.577940452079929e-06}, {"id": 32, "seek": 13904, "start": 142.16, "end": 146.28, "text": " Then the most natural choice would be the ReLU activation function.", "tokens": [50364, 295, 257, 1782, 11, 300, 393, 1128, 312, 3671, 13, 50520, 50520, 1396, 264, 881, 3303, 3922, 576, 312, 264, 1300, 43, 52, 24433, 2445, 13, 50726, 50726, 1436, 382, 291, 536, 510, 11, 341, 24433, 2445, 787, 2516, 322, 2107, 12, 28561, 1166, 4190, 11, 2139, 50992, 50992, 4018, 420, 3353, 4190, 13, 51097, 51097, 407, 562, 10875, 264, 24433, 2445, 281, 764, 337, 428, 5598, 4583, 11, 2673, 5413, 51368, 51368, 322, 437, 307, 264, 7645, 11, 983, 291, 434, 1382, 281, 6069, 11, 456, 603, 312, 472, 6457, 3303, 3922, 13, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.13629695347377233, "compression_ratio": 1.7118644067796611, "no_speech_prob": 1.577940452079929e-06}, {"id": 33, "seek": 13904, "start": 146.28, "end": 151.6, "text": " Because as you see here, this activation function only takes on non-negative values, either", "tokens": [50364, 295, 257, 1782, 11, 300, 393, 1128, 312, 3671, 13, 50520, 50520, 1396, 264, 881, 3303, 3922, 576, 312, 264, 1300, 43, 52, 24433, 2445, 13, 50726, 50726, 1436, 382, 291, 536, 510, 11, 341, 24433, 2445, 787, 2516, 322, 2107, 12, 28561, 1166, 4190, 11, 2139, 50992, 50992, 4018, 420, 3353, 4190, 13, 51097, 51097, 407, 562, 10875, 264, 24433, 2445, 281, 764, 337, 428, 5598, 4583, 11, 2673, 5413, 51368, 51368, 322, 437, 307, 264, 7645, 11, 983, 291, 434, 1382, 281, 6069, 11, 456, 603, 312, 472, 6457, 3303, 3922, 13, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.13629695347377233, "compression_ratio": 1.7118644067796611, "no_speech_prob": 1.577940452079929e-06}, {"id": 34, "seek": 13904, "start": 151.6, "end": 153.7, "text": " zero or positive values.", "tokens": [50364, 295, 257, 1782, 11, 300, 393, 1128, 312, 3671, 13, 50520, 50520, 1396, 264, 881, 3303, 3922, 576, 312, 264, 1300, 43, 52, 24433, 2445, 13, 50726, 50726, 1436, 382, 291, 536, 510, 11, 341, 24433, 2445, 787, 2516, 322, 2107, 12, 28561, 1166, 4190, 11, 2139, 50992, 50992, 4018, 420, 3353, 4190, 13, 51097, 51097, 407, 562, 10875, 264, 24433, 2445, 281, 764, 337, 428, 5598, 4583, 11, 2673, 5413, 51368, 51368, 322, 437, 307, 264, 7645, 11, 983, 291, 434, 1382, 281, 6069, 11, 456, 603, 312, 472, 6457, 3303, 3922, 13, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.13629695347377233, "compression_ratio": 1.7118644067796611, "no_speech_prob": 1.577940452079929e-06}, {"id": 35, "seek": 13904, "start": 153.7, "end": 159.12, "text": " So when choosing the activation function to use for your output layer, usually depending", "tokens": [50364, 295, 257, 1782, 11, 300, 393, 1128, 312, 3671, 13, 50520, 50520, 1396, 264, 881, 3303, 3922, 576, 312, 264, 1300, 43, 52, 24433, 2445, 13, 50726, 50726, 1436, 382, 291, 536, 510, 11, 341, 24433, 2445, 787, 2516, 322, 2107, 12, 28561, 1166, 4190, 11, 2139, 50992, 50992, 4018, 420, 3353, 4190, 13, 51097, 51097, 407, 562, 10875, 264, 24433, 2445, 281, 764, 337, 428, 5598, 4583, 11, 2673, 5413, 51368, 51368, 322, 437, 307, 264, 7645, 11, 983, 291, 434, 1382, 281, 6069, 11, 456, 603, 312, 472, 6457, 3303, 3922, 13, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.13629695347377233, "compression_ratio": 1.7118644067796611, "no_speech_prob": 1.577940452079929e-06}, {"id": 36, "seek": 13904, "start": 159.12, "end": 164.84, "text": " on what is the label, why you're trying to predict, there'll be one fairly natural choice.", "tokens": [50364, 295, 257, 1782, 11, 300, 393, 1128, 312, 3671, 13, 50520, 50520, 1396, 264, 881, 3303, 3922, 576, 312, 264, 1300, 43, 52, 24433, 2445, 13, 50726, 50726, 1436, 382, 291, 536, 510, 11, 341, 24433, 2445, 787, 2516, 322, 2107, 12, 28561, 1166, 4190, 11, 2139, 50992, 50992, 4018, 420, 3353, 4190, 13, 51097, 51097, 407, 562, 10875, 264, 24433, 2445, 281, 764, 337, 428, 5598, 4583, 11, 2673, 5413, 51368, 51368, 322, 437, 307, 264, 7645, 11, 983, 291, 434, 1382, 281, 6069, 11, 456, 603, 312, 472, 6457, 3303, 3922, 13, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.13629695347377233, "compression_ratio": 1.7118644067796611, "no_speech_prob": 1.577940452079929e-06}, {"id": 37, "seek": 16484, "start": 164.84, "end": 171.48000000000002, "text": " And in fact, the guidance on this slide is how I pretty much always choose my activation", "tokens": [50364, 400, 294, 1186, 11, 264, 10056, 322, 341, 4137, 307, 577, 286, 1238, 709, 1009, 2826, 452, 24433, 50696, 50696, 2445, 382, 731, 337, 264, 5598, 4583, 295, 257, 18161, 3209, 13, 50860, 50860, 1012, 466, 264, 7633, 7914, 295, 257, 18161, 3209, 30, 51040, 51040, 467, 4523, 484, 300, 264, 1300, 43, 52, 24433, 2445, 307, 538, 1400, 264, 881, 2689, 3922, 294, 577, 18161, 51416, 51416, 9590, 366, 8895, 538, 867, 11, 867, 25742, 965, 13, 51655, 51655], "temperature": 0.0, "avg_logprob": -0.08417282453397425, "compression_ratio": 1.696078431372549, "no_speech_prob": 2.726414777498576e-06}, {"id": 38, "seek": 16484, "start": 171.48000000000002, "end": 174.76, "text": " function as well for the output layer of a neural network.", "tokens": [50364, 400, 294, 1186, 11, 264, 10056, 322, 341, 4137, 307, 577, 286, 1238, 709, 1009, 2826, 452, 24433, 50696, 50696, 2445, 382, 731, 337, 264, 5598, 4583, 295, 257, 18161, 3209, 13, 50860, 50860, 1012, 466, 264, 7633, 7914, 295, 257, 18161, 3209, 30, 51040, 51040, 467, 4523, 484, 300, 264, 1300, 43, 52, 24433, 2445, 307, 538, 1400, 264, 881, 2689, 3922, 294, 577, 18161, 51416, 51416, 9590, 366, 8895, 538, 867, 11, 867, 25742, 965, 13, 51655, 51655], "temperature": 0.0, "avg_logprob": -0.08417282453397425, "compression_ratio": 1.696078431372549, "no_speech_prob": 2.726414777498576e-06}, {"id": 39, "seek": 16484, "start": 174.76, "end": 178.36, "text": " How about the hidden layers of a neural network?", "tokens": [50364, 400, 294, 1186, 11, 264, 10056, 322, 341, 4137, 307, 577, 286, 1238, 709, 1009, 2826, 452, 24433, 50696, 50696, 2445, 382, 731, 337, 264, 5598, 4583, 295, 257, 18161, 3209, 13, 50860, 50860, 1012, 466, 264, 7633, 7914, 295, 257, 18161, 3209, 30, 51040, 51040, 467, 4523, 484, 300, 264, 1300, 43, 52, 24433, 2445, 307, 538, 1400, 264, 881, 2689, 3922, 294, 577, 18161, 51416, 51416, 9590, 366, 8895, 538, 867, 11, 867, 25742, 965, 13, 51655, 51655], "temperature": 0.0, "avg_logprob": -0.08417282453397425, "compression_ratio": 1.696078431372549, "no_speech_prob": 2.726414777498576e-06}, {"id": 40, "seek": 16484, "start": 178.36, "end": 185.88, "text": " It turns out that the ReLU activation function is by far the most common choice in how neural", "tokens": [50364, 400, 294, 1186, 11, 264, 10056, 322, 341, 4137, 307, 577, 286, 1238, 709, 1009, 2826, 452, 24433, 50696, 50696, 2445, 382, 731, 337, 264, 5598, 4583, 295, 257, 18161, 3209, 13, 50860, 50860, 1012, 466, 264, 7633, 7914, 295, 257, 18161, 3209, 30, 51040, 51040, 467, 4523, 484, 300, 264, 1300, 43, 52, 24433, 2445, 307, 538, 1400, 264, 881, 2689, 3922, 294, 577, 18161, 51416, 51416, 9590, 366, 8895, 538, 867, 11, 867, 25742, 965, 13, 51655, 51655], "temperature": 0.0, "avg_logprob": -0.08417282453397425, "compression_ratio": 1.696078431372549, "no_speech_prob": 2.726414777498576e-06}, {"id": 41, "seek": 16484, "start": 185.88, "end": 190.66, "text": " networks are trained by many, many practitioners today.", "tokens": [50364, 400, 294, 1186, 11, 264, 10056, 322, 341, 4137, 307, 577, 286, 1238, 709, 1009, 2826, 452, 24433, 50696, 50696, 2445, 382, 731, 337, 264, 5598, 4583, 295, 257, 18161, 3209, 13, 50860, 50860, 1012, 466, 264, 7633, 7914, 295, 257, 18161, 3209, 30, 51040, 51040, 467, 4523, 484, 300, 264, 1300, 43, 52, 24433, 2445, 307, 538, 1400, 264, 881, 2689, 3922, 294, 577, 18161, 51416, 51416, 9590, 366, 8895, 538, 867, 11, 867, 25742, 965, 13, 51655, 51655], "temperature": 0.0, "avg_logprob": -0.08417282453397425, "compression_ratio": 1.696078431372549, "no_speech_prob": 2.726414777498576e-06}, {"id": 42, "seek": 19066, "start": 190.66, "end": 196.96, "text": " Even though we had initially described neural networks using the sigmoid activation function,", "tokens": [50364, 2754, 1673, 321, 632, 9105, 7619, 18161, 9590, 1228, 264, 4556, 3280, 327, 24433, 2445, 11, 50679, 50679, 293, 294, 1186, 11, 294, 264, 2440, 2503, 295, 264, 3250, 295, 18161, 9590, 11, 561, 764, 4556, 3280, 327, 24433, 50951, 50951, 6828, 294, 867, 3190, 11, 264, 2519, 575, 14178, 281, 764, 1300, 43, 52, 709, 544, 2049, 293, 4556, 3280, 3742, 13572, 51367, 51367, 1562, 13, 51417, 51417, 2022, 264, 472, 11183, 300, 291, 360, 764, 257, 4556, 3280, 327, 24433, 2445, 294, 264, 5598, 4583, 498, 51642, 51642, 291, 362, 257, 17434, 21538, 1154, 13, 51775, 51775, 407, 983, 307, 300, 30, 51825, 51825], "temperature": 0.0, "avg_logprob": -0.11519274888215242, "compression_ratio": 1.7707509881422925, "no_speech_prob": 1.4823169749433873e-06}, {"id": 43, "seek": 19066, "start": 196.96, "end": 202.4, "text": " and in fact, in the early history of the development of neural networks, people use sigmoid activation", "tokens": [50364, 2754, 1673, 321, 632, 9105, 7619, 18161, 9590, 1228, 264, 4556, 3280, 327, 24433, 2445, 11, 50679, 50679, 293, 294, 1186, 11, 294, 264, 2440, 2503, 295, 264, 3250, 295, 18161, 9590, 11, 561, 764, 4556, 3280, 327, 24433, 50951, 50951, 6828, 294, 867, 3190, 11, 264, 2519, 575, 14178, 281, 764, 1300, 43, 52, 709, 544, 2049, 293, 4556, 3280, 3742, 13572, 51367, 51367, 1562, 13, 51417, 51417, 2022, 264, 472, 11183, 300, 291, 360, 764, 257, 4556, 3280, 327, 24433, 2445, 294, 264, 5598, 4583, 498, 51642, 51642, 291, 362, 257, 17434, 21538, 1154, 13, 51775, 51775, 407, 983, 307, 300, 30, 51825, 51825], "temperature": 0.0, "avg_logprob": -0.11519274888215242, "compression_ratio": 1.7707509881422925, "no_speech_prob": 1.4823169749433873e-06}, {"id": 44, "seek": 19066, "start": 202.4, "end": 210.72, "text": " functions in many places, the field has evolved to use ReLU much more often and sigmoids hardly", "tokens": [50364, 2754, 1673, 321, 632, 9105, 7619, 18161, 9590, 1228, 264, 4556, 3280, 327, 24433, 2445, 11, 50679, 50679, 293, 294, 1186, 11, 294, 264, 2440, 2503, 295, 264, 3250, 295, 18161, 9590, 11, 561, 764, 4556, 3280, 327, 24433, 50951, 50951, 6828, 294, 867, 3190, 11, 264, 2519, 575, 14178, 281, 764, 1300, 43, 52, 709, 544, 2049, 293, 4556, 3280, 3742, 13572, 51367, 51367, 1562, 13, 51417, 51417, 2022, 264, 472, 11183, 300, 291, 360, 764, 257, 4556, 3280, 327, 24433, 2445, 294, 264, 5598, 4583, 498, 51642, 51642, 291, 362, 257, 17434, 21538, 1154, 13, 51775, 51775, 407, 983, 307, 300, 30, 51825, 51825], "temperature": 0.0, "avg_logprob": -0.11519274888215242, "compression_ratio": 1.7707509881422925, "no_speech_prob": 1.4823169749433873e-06}, {"id": 45, "seek": 19066, "start": 210.72, "end": 211.72, "text": " ever.", "tokens": [50364, 2754, 1673, 321, 632, 9105, 7619, 18161, 9590, 1228, 264, 4556, 3280, 327, 24433, 2445, 11, 50679, 50679, 293, 294, 1186, 11, 294, 264, 2440, 2503, 295, 264, 3250, 295, 18161, 9590, 11, 561, 764, 4556, 3280, 327, 24433, 50951, 50951, 6828, 294, 867, 3190, 11, 264, 2519, 575, 14178, 281, 764, 1300, 43, 52, 709, 544, 2049, 293, 4556, 3280, 3742, 13572, 51367, 51367, 1562, 13, 51417, 51417, 2022, 264, 472, 11183, 300, 291, 360, 764, 257, 4556, 3280, 327, 24433, 2445, 294, 264, 5598, 4583, 498, 51642, 51642, 291, 362, 257, 17434, 21538, 1154, 13, 51775, 51775, 407, 983, 307, 300, 30, 51825, 51825], "temperature": 0.0, "avg_logprob": -0.11519274888215242, "compression_ratio": 1.7707509881422925, "no_speech_prob": 1.4823169749433873e-06}, {"id": 46, "seek": 19066, "start": 211.72, "end": 216.22, "text": " With the one exception that you do use a sigmoid activation function in the output layer if", "tokens": [50364, 2754, 1673, 321, 632, 9105, 7619, 18161, 9590, 1228, 264, 4556, 3280, 327, 24433, 2445, 11, 50679, 50679, 293, 294, 1186, 11, 294, 264, 2440, 2503, 295, 264, 3250, 295, 18161, 9590, 11, 561, 764, 4556, 3280, 327, 24433, 50951, 50951, 6828, 294, 867, 3190, 11, 264, 2519, 575, 14178, 281, 764, 1300, 43, 52, 709, 544, 2049, 293, 4556, 3280, 3742, 13572, 51367, 51367, 1562, 13, 51417, 51417, 2022, 264, 472, 11183, 300, 291, 360, 764, 257, 4556, 3280, 327, 24433, 2445, 294, 264, 5598, 4583, 498, 51642, 51642, 291, 362, 257, 17434, 21538, 1154, 13, 51775, 51775, 407, 983, 307, 300, 30, 51825, 51825], "temperature": 0.0, "avg_logprob": -0.11519274888215242, "compression_ratio": 1.7707509881422925, "no_speech_prob": 1.4823169749433873e-06}, {"id": 47, "seek": 19066, "start": 216.22, "end": 218.88, "text": " you have a binary classification problem.", "tokens": [50364, 2754, 1673, 321, 632, 9105, 7619, 18161, 9590, 1228, 264, 4556, 3280, 327, 24433, 2445, 11, 50679, 50679, 293, 294, 1186, 11, 294, 264, 2440, 2503, 295, 264, 3250, 295, 18161, 9590, 11, 561, 764, 4556, 3280, 327, 24433, 50951, 50951, 6828, 294, 867, 3190, 11, 264, 2519, 575, 14178, 281, 764, 1300, 43, 52, 709, 544, 2049, 293, 4556, 3280, 3742, 13572, 51367, 51367, 1562, 13, 51417, 51417, 2022, 264, 472, 11183, 300, 291, 360, 764, 257, 4556, 3280, 327, 24433, 2445, 294, 264, 5598, 4583, 498, 51642, 51642, 291, 362, 257, 17434, 21538, 1154, 13, 51775, 51775, 407, 983, 307, 300, 30, 51825, 51825], "temperature": 0.0, "avg_logprob": -0.11519274888215242, "compression_ratio": 1.7707509881422925, "no_speech_prob": 1.4823169749433873e-06}, {"id": 48, "seek": 19066, "start": 218.88, "end": 219.88, "text": " So why is that?", "tokens": [50364, 2754, 1673, 321, 632, 9105, 7619, 18161, 9590, 1228, 264, 4556, 3280, 327, 24433, 2445, 11, 50679, 50679, 293, 294, 1186, 11, 294, 264, 2440, 2503, 295, 264, 3250, 295, 18161, 9590, 11, 561, 764, 4556, 3280, 327, 24433, 50951, 50951, 6828, 294, 867, 3190, 11, 264, 2519, 575, 14178, 281, 764, 1300, 43, 52, 709, 544, 2049, 293, 4556, 3280, 3742, 13572, 51367, 51367, 1562, 13, 51417, 51417, 2022, 264, 472, 11183, 300, 291, 360, 764, 257, 4556, 3280, 327, 24433, 2445, 294, 264, 5598, 4583, 498, 51642, 51642, 291, 362, 257, 17434, 21538, 1154, 13, 51775, 51775, 407, 983, 307, 300, 30, 51825, 51825], "temperature": 0.0, "avg_logprob": -0.11519274888215242, "compression_ratio": 1.7707509881422925, "no_speech_prob": 1.4823169749433873e-06}, {"id": 49, "seek": 21988, "start": 219.88, "end": 221.84, "text": " Well, there are a few reasons.", "tokens": [50364, 1042, 11, 456, 366, 257, 1326, 4112, 13, 50462, 50462, 2386, 11, 498, 291, 6794, 264, 1300, 43, 52, 293, 264, 4556, 3280, 327, 24433, 6828, 11, 264, 1300, 43, 52, 307, 257, 857, 4663, 50814, 50814, 281, 14722, 570, 309, 445, 7029, 15866, 11469, 295, 1958, 11, 710, 11, 9735, 264, 4556, 3280, 327, 7029, 51110, 51110, 1940, 364, 37871, 6642, 293, 364, 17340, 293, 370, 322, 11, 293, 370, 309, 311, 257, 707, 857, 1570, 7148, 13, 51408, 51408, 583, 264, 1150, 1778, 11, 597, 4523, 484, 281, 312, 754, 544, 1021, 11, 307, 300, 264, 1300, 43, 52, 2445, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.14299711191429282, "compression_ratio": 1.6638655462184875, "no_speech_prob": 4.289206117391586e-06}, {"id": 50, "seek": 21988, "start": 221.84, "end": 228.88, "text": " First, if you compare the ReLU and the sigmoid activation functions, the ReLU is a bit faster", "tokens": [50364, 1042, 11, 456, 366, 257, 1326, 4112, 13, 50462, 50462, 2386, 11, 498, 291, 6794, 264, 1300, 43, 52, 293, 264, 4556, 3280, 327, 24433, 6828, 11, 264, 1300, 43, 52, 307, 257, 857, 4663, 50814, 50814, 281, 14722, 570, 309, 445, 7029, 15866, 11469, 295, 1958, 11, 710, 11, 9735, 264, 4556, 3280, 327, 7029, 51110, 51110, 1940, 364, 37871, 6642, 293, 364, 17340, 293, 370, 322, 11, 293, 370, 309, 311, 257, 707, 857, 1570, 7148, 13, 51408, 51408, 583, 264, 1150, 1778, 11, 597, 4523, 484, 281, 312, 754, 544, 1021, 11, 307, 300, 264, 1300, 43, 52, 2445, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.14299711191429282, "compression_ratio": 1.6638655462184875, "no_speech_prob": 4.289206117391586e-06}, {"id": 51, "seek": 21988, "start": 228.88, "end": 234.79999999999998, "text": " to compute because it just requires computing max of 0, z, whereas the sigmoid requires", "tokens": [50364, 1042, 11, 456, 366, 257, 1326, 4112, 13, 50462, 50462, 2386, 11, 498, 291, 6794, 264, 1300, 43, 52, 293, 264, 4556, 3280, 327, 24433, 6828, 11, 264, 1300, 43, 52, 307, 257, 857, 4663, 50814, 50814, 281, 14722, 570, 309, 445, 7029, 15866, 11469, 295, 1958, 11, 710, 11, 9735, 264, 4556, 3280, 327, 7029, 51110, 51110, 1940, 364, 37871, 6642, 293, 364, 17340, 293, 370, 322, 11, 293, 370, 309, 311, 257, 707, 857, 1570, 7148, 13, 51408, 51408, 583, 264, 1150, 1778, 11, 597, 4523, 484, 281, 312, 754, 544, 1021, 11, 307, 300, 264, 1300, 43, 52, 2445, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.14299711191429282, "compression_ratio": 1.6638655462184875, "no_speech_prob": 4.289206117391586e-06}, {"id": 52, "seek": 21988, "start": 234.79999999999998, "end": 240.76, "text": " taking an exponentiation and an inverse and so on, and so it's a little bit less efficient.", "tokens": [50364, 1042, 11, 456, 366, 257, 1326, 4112, 13, 50462, 50462, 2386, 11, 498, 291, 6794, 264, 1300, 43, 52, 293, 264, 4556, 3280, 327, 24433, 6828, 11, 264, 1300, 43, 52, 307, 257, 857, 4663, 50814, 50814, 281, 14722, 570, 309, 445, 7029, 15866, 11469, 295, 1958, 11, 710, 11, 9735, 264, 4556, 3280, 327, 7029, 51110, 51110, 1940, 364, 37871, 6642, 293, 364, 17340, 293, 370, 322, 11, 293, 370, 309, 311, 257, 707, 857, 1570, 7148, 13, 51408, 51408, 583, 264, 1150, 1778, 11, 597, 4523, 484, 281, 312, 754, 544, 1021, 11, 307, 300, 264, 1300, 43, 52, 2445, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.14299711191429282, "compression_ratio": 1.6638655462184875, "no_speech_prob": 4.289206117391586e-06}, {"id": 53, "seek": 21988, "start": 240.76, "end": 246.64, "text": " But the second reason, which turns out to be even more important, is that the ReLU function", "tokens": [50364, 1042, 11, 456, 366, 257, 1326, 4112, 13, 50462, 50462, 2386, 11, 498, 291, 6794, 264, 1300, 43, 52, 293, 264, 4556, 3280, 327, 24433, 6828, 11, 264, 1300, 43, 52, 307, 257, 857, 4663, 50814, 50814, 281, 14722, 570, 309, 445, 7029, 15866, 11469, 295, 1958, 11, 710, 11, 9735, 264, 4556, 3280, 327, 7029, 51110, 51110, 1940, 364, 37871, 6642, 293, 364, 17340, 293, 370, 322, 11, 293, 370, 309, 311, 257, 707, 857, 1570, 7148, 13, 51408, 51408, 583, 264, 1150, 1778, 11, 597, 4523, 484, 281, 312, 754, 544, 1021, 11, 307, 300, 264, 1300, 43, 52, 2445, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.14299711191429282, "compression_ratio": 1.6638655462184875, "no_speech_prob": 4.289206117391586e-06}, {"id": 54, "seek": 24664, "start": 246.64, "end": 254.0, "text": " kind of goes flat only in one part of the graph, here on the left, it's completely flat.", "tokens": [50364, 733, 295, 1709, 4962, 787, 294, 472, 644, 295, 264, 4295, 11, 510, 322, 264, 1411, 11, 309, 311, 2584, 4962, 13, 50732, 50732, 13813, 264, 4556, 3280, 327, 24433, 2445, 11, 309, 733, 295, 1709, 4962, 294, 732, 3190, 13, 50948, 50948, 467, 1709, 4962, 281, 264, 1411, 295, 264, 4295, 293, 309, 1709, 4962, 281, 264, 558, 295, 264, 4295, 13, 51348, 51348, 400, 498, 291, 434, 1228, 16235, 23475, 281, 3847, 257, 18161, 3209, 11, 550, 562, 291, 362, 257, 2445, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.12392809174277565, "compression_ratio": 1.8138297872340425, "no_speech_prob": 1.1842839739983901e-05}, {"id": 55, "seek": 24664, "start": 254.0, "end": 258.32, "text": " Whereas the sigmoid activation function, it kind of goes flat in two places.", "tokens": [50364, 733, 295, 1709, 4962, 787, 294, 472, 644, 295, 264, 4295, 11, 510, 322, 264, 1411, 11, 309, 311, 2584, 4962, 13, 50732, 50732, 13813, 264, 4556, 3280, 327, 24433, 2445, 11, 309, 733, 295, 1709, 4962, 294, 732, 3190, 13, 50948, 50948, 467, 1709, 4962, 281, 264, 1411, 295, 264, 4295, 293, 309, 1709, 4962, 281, 264, 558, 295, 264, 4295, 13, 51348, 51348, 400, 498, 291, 434, 1228, 16235, 23475, 281, 3847, 257, 18161, 3209, 11, 550, 562, 291, 362, 257, 2445, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.12392809174277565, "compression_ratio": 1.8138297872340425, "no_speech_prob": 1.1842839739983901e-05}, {"id": 56, "seek": 24664, "start": 258.32, "end": 266.32, "text": " It goes flat to the left of the graph and it goes flat to the right of the graph.", "tokens": [50364, 733, 295, 1709, 4962, 787, 294, 472, 644, 295, 264, 4295, 11, 510, 322, 264, 1411, 11, 309, 311, 2584, 4962, 13, 50732, 50732, 13813, 264, 4556, 3280, 327, 24433, 2445, 11, 309, 733, 295, 1709, 4962, 294, 732, 3190, 13, 50948, 50948, 467, 1709, 4962, 281, 264, 1411, 295, 264, 4295, 293, 309, 1709, 4962, 281, 264, 558, 295, 264, 4295, 13, 51348, 51348, 400, 498, 291, 434, 1228, 16235, 23475, 281, 3847, 257, 18161, 3209, 11, 550, 562, 291, 362, 257, 2445, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.12392809174277565, "compression_ratio": 1.8138297872340425, "no_speech_prob": 1.1842839739983901e-05}, {"id": 57, "seek": 24664, "start": 266.32, "end": 272.03999999999996, "text": " And if you're using gradient descent to train a neural network, then when you have a function", "tokens": [50364, 733, 295, 1709, 4962, 787, 294, 472, 644, 295, 264, 4295, 11, 510, 322, 264, 1411, 11, 309, 311, 2584, 4962, 13, 50732, 50732, 13813, 264, 4556, 3280, 327, 24433, 2445, 11, 309, 733, 295, 1709, 4962, 294, 732, 3190, 13, 50948, 50948, 467, 1709, 4962, 281, 264, 1411, 295, 264, 4295, 293, 309, 1709, 4962, 281, 264, 558, 295, 264, 4295, 13, 51348, 51348, 400, 498, 291, 434, 1228, 16235, 23475, 281, 3847, 257, 18161, 3209, 11, 550, 562, 291, 362, 257, 2445, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.12392809174277565, "compression_ratio": 1.8138297872340425, "no_speech_prob": 1.1842839739983901e-05}, {"id": 58, "seek": 27204, "start": 272.04, "end": 278.24, "text": " that is flat in a lot of places, gradient descent will be really slow.", "tokens": [50364, 300, 307, 4962, 294, 257, 688, 295, 3190, 11, 16235, 23475, 486, 312, 534, 2964, 13, 50674, 50674, 286, 458, 300, 16235, 23475, 5028, 5660, 264, 2063, 2445, 508, 295, 343, 33, 2831, 813, 5028, 5660, 51019, 51019, 264, 24433, 2445, 11, 457, 264, 24433, 2445, 307, 257, 2522, 295, 437, 1709, 666, 15866, 11, 51330, 51330, 293, 300, 3542, 294, 544, 3190, 294, 264, 2063, 2445, 508, 295, 343, 33, 300, 366, 4962, 382, 731, 293, 51642, 51642, 365, 257, 4356, 16235, 293, 309, 35789, 760, 2539, 13, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.13199530365646525, "compression_ratio": 1.8883495145631068, "no_speech_prob": 1.0782699973788112e-05}, {"id": 59, "seek": 27204, "start": 278.24, "end": 285.14000000000004, "text": " I know that gradient descent optimizes the cost function J of WB rather than optimizes", "tokens": [50364, 300, 307, 4962, 294, 257, 688, 295, 3190, 11, 16235, 23475, 486, 312, 534, 2964, 13, 50674, 50674, 286, 458, 300, 16235, 23475, 5028, 5660, 264, 2063, 2445, 508, 295, 343, 33, 2831, 813, 5028, 5660, 51019, 51019, 264, 24433, 2445, 11, 457, 264, 24433, 2445, 307, 257, 2522, 295, 437, 1709, 666, 15866, 11, 51330, 51330, 293, 300, 3542, 294, 544, 3190, 294, 264, 2063, 2445, 508, 295, 343, 33, 300, 366, 4962, 382, 731, 293, 51642, 51642, 365, 257, 4356, 16235, 293, 309, 35789, 760, 2539, 13, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.13199530365646525, "compression_ratio": 1.8883495145631068, "no_speech_prob": 1.0782699973788112e-05}, {"id": 60, "seek": 27204, "start": 285.14000000000004, "end": 291.36, "text": " the activation function, but the activation function is a piece of what goes into computing,", "tokens": [50364, 300, 307, 4962, 294, 257, 688, 295, 3190, 11, 16235, 23475, 486, 312, 534, 2964, 13, 50674, 50674, 286, 458, 300, 16235, 23475, 5028, 5660, 264, 2063, 2445, 508, 295, 343, 33, 2831, 813, 5028, 5660, 51019, 51019, 264, 24433, 2445, 11, 457, 264, 24433, 2445, 307, 257, 2522, 295, 437, 1709, 666, 15866, 11, 51330, 51330, 293, 300, 3542, 294, 544, 3190, 294, 264, 2063, 2445, 508, 295, 343, 33, 300, 366, 4962, 382, 731, 293, 51642, 51642, 365, 257, 4356, 16235, 293, 309, 35789, 760, 2539, 13, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.13199530365646525, "compression_ratio": 1.8883495145631068, "no_speech_prob": 1.0782699973788112e-05}, {"id": 61, "seek": 27204, "start": 291.36, "end": 297.6, "text": " and that results in more places in the cost function J of WB that are flat as well and", "tokens": [50364, 300, 307, 4962, 294, 257, 688, 295, 3190, 11, 16235, 23475, 486, 312, 534, 2964, 13, 50674, 50674, 286, 458, 300, 16235, 23475, 5028, 5660, 264, 2063, 2445, 508, 295, 343, 33, 2831, 813, 5028, 5660, 51019, 51019, 264, 24433, 2445, 11, 457, 264, 24433, 2445, 307, 257, 2522, 295, 437, 1709, 666, 15866, 11, 51330, 51330, 293, 300, 3542, 294, 544, 3190, 294, 264, 2063, 2445, 508, 295, 343, 33, 300, 366, 4962, 382, 731, 293, 51642, 51642, 365, 257, 4356, 16235, 293, 309, 35789, 760, 2539, 13, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.13199530365646525, "compression_ratio": 1.8883495145631068, "no_speech_prob": 1.0782699973788112e-05}, {"id": 62, "seek": 27204, "start": 297.6, "end": 301.64000000000004, "text": " with a smaller gradient and it slows down learning.", "tokens": [50364, 300, 307, 4962, 294, 257, 688, 295, 3190, 11, 16235, 23475, 486, 312, 534, 2964, 13, 50674, 50674, 286, 458, 300, 16235, 23475, 5028, 5660, 264, 2063, 2445, 508, 295, 343, 33, 2831, 813, 5028, 5660, 51019, 51019, 264, 24433, 2445, 11, 457, 264, 24433, 2445, 307, 257, 2522, 295, 437, 1709, 666, 15866, 11, 51330, 51330, 293, 300, 3542, 294, 544, 3190, 294, 264, 2063, 2445, 508, 295, 343, 33, 300, 366, 4962, 382, 731, 293, 51642, 51642, 365, 257, 4356, 16235, 293, 309, 35789, 760, 2539, 13, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.13199530365646525, "compression_ratio": 1.8883495145631068, "no_speech_prob": 1.0782699973788112e-05}, {"id": 63, "seek": 30164, "start": 301.64, "end": 306.56, "text": " I know that that was just an intuitive explanation, but researchers have found that using the", "tokens": [50364, 286, 458, 300, 300, 390, 445, 364, 21769, 10835, 11, 457, 10309, 362, 1352, 300, 1228, 264, 50610, 50610, 1300, 43, 52, 24433, 2445, 393, 3082, 428, 18161, 3209, 281, 1466, 257, 857, 4663, 382, 731, 11, 597, 50894, 50894, 307, 983, 337, 881, 25742, 11, 498, 291, 434, 1382, 281, 4536, 437, 24433, 2445, 281, 764, 51098, 51098, 337, 264, 7633, 4583, 11, 264, 1300, 43, 52, 24433, 2445, 575, 1813, 586, 538, 1400, 264, 881, 2689, 51384, 51384, 3922, 13, 51434, 51434, 682, 1186, 11, 498, 286, 478, 2390, 257, 18161, 3209, 11, 341, 307, 577, 286, 2826, 24433, 6828, 51686, 51686, 337, 264, 7633, 7914, 382, 731, 13, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.1015433933423913, "compression_ratio": 1.90625, "no_speech_prob": 2.7693042738974327e-06}, {"id": 64, "seek": 30164, "start": 306.56, "end": 312.24, "text": " ReLU activation function can cause your neural network to learn a bit faster as well, which", "tokens": [50364, 286, 458, 300, 300, 390, 445, 364, 21769, 10835, 11, 457, 10309, 362, 1352, 300, 1228, 264, 50610, 50610, 1300, 43, 52, 24433, 2445, 393, 3082, 428, 18161, 3209, 281, 1466, 257, 857, 4663, 382, 731, 11, 597, 50894, 50894, 307, 983, 337, 881, 25742, 11, 498, 291, 434, 1382, 281, 4536, 437, 24433, 2445, 281, 764, 51098, 51098, 337, 264, 7633, 4583, 11, 264, 1300, 43, 52, 24433, 2445, 575, 1813, 586, 538, 1400, 264, 881, 2689, 51384, 51384, 3922, 13, 51434, 51434, 682, 1186, 11, 498, 286, 478, 2390, 257, 18161, 3209, 11, 341, 307, 577, 286, 2826, 24433, 6828, 51686, 51686, 337, 264, 7633, 7914, 382, 731, 13, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.1015433933423913, "compression_ratio": 1.90625, "no_speech_prob": 2.7693042738974327e-06}, {"id": 65, "seek": 30164, "start": 312.24, "end": 316.32, "text": " is why for most practitioners, if you're trying to decide what activation function to use", "tokens": [50364, 286, 458, 300, 300, 390, 445, 364, 21769, 10835, 11, 457, 10309, 362, 1352, 300, 1228, 264, 50610, 50610, 1300, 43, 52, 24433, 2445, 393, 3082, 428, 18161, 3209, 281, 1466, 257, 857, 4663, 382, 731, 11, 597, 50894, 50894, 307, 983, 337, 881, 25742, 11, 498, 291, 434, 1382, 281, 4536, 437, 24433, 2445, 281, 764, 51098, 51098, 337, 264, 7633, 4583, 11, 264, 1300, 43, 52, 24433, 2445, 575, 1813, 586, 538, 1400, 264, 881, 2689, 51384, 51384, 3922, 13, 51434, 51434, 682, 1186, 11, 498, 286, 478, 2390, 257, 18161, 3209, 11, 341, 307, 577, 286, 2826, 24433, 6828, 51686, 51686, 337, 264, 7633, 7914, 382, 731, 13, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.1015433933423913, "compression_ratio": 1.90625, "no_speech_prob": 2.7693042738974327e-06}, {"id": 66, "seek": 30164, "start": 316.32, "end": 322.03999999999996, "text": " for the hidden layer, the ReLU activation function has become now by far the most common", "tokens": [50364, 286, 458, 300, 300, 390, 445, 364, 21769, 10835, 11, 457, 10309, 362, 1352, 300, 1228, 264, 50610, 50610, 1300, 43, 52, 24433, 2445, 393, 3082, 428, 18161, 3209, 281, 1466, 257, 857, 4663, 382, 731, 11, 597, 50894, 50894, 307, 983, 337, 881, 25742, 11, 498, 291, 434, 1382, 281, 4536, 437, 24433, 2445, 281, 764, 51098, 51098, 337, 264, 7633, 4583, 11, 264, 1300, 43, 52, 24433, 2445, 575, 1813, 586, 538, 1400, 264, 881, 2689, 51384, 51384, 3922, 13, 51434, 51434, 682, 1186, 11, 498, 286, 478, 2390, 257, 18161, 3209, 11, 341, 307, 577, 286, 2826, 24433, 6828, 51686, 51686, 337, 264, 7633, 7914, 382, 731, 13, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.1015433933423913, "compression_ratio": 1.90625, "no_speech_prob": 2.7693042738974327e-06}, {"id": 67, "seek": 30164, "start": 322.03999999999996, "end": 323.03999999999996, "text": " choice.", "tokens": [50364, 286, 458, 300, 300, 390, 445, 364, 21769, 10835, 11, 457, 10309, 362, 1352, 300, 1228, 264, 50610, 50610, 1300, 43, 52, 24433, 2445, 393, 3082, 428, 18161, 3209, 281, 1466, 257, 857, 4663, 382, 731, 11, 597, 50894, 50894, 307, 983, 337, 881, 25742, 11, 498, 291, 434, 1382, 281, 4536, 437, 24433, 2445, 281, 764, 51098, 51098, 337, 264, 7633, 4583, 11, 264, 1300, 43, 52, 24433, 2445, 575, 1813, 586, 538, 1400, 264, 881, 2689, 51384, 51384, 3922, 13, 51434, 51434, 682, 1186, 11, 498, 286, 478, 2390, 257, 18161, 3209, 11, 341, 307, 577, 286, 2826, 24433, 6828, 51686, 51686, 337, 264, 7633, 7914, 382, 731, 13, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.1015433933423913, "compression_ratio": 1.90625, "no_speech_prob": 2.7693042738974327e-06}, {"id": 68, "seek": 30164, "start": 323.03999999999996, "end": 328.08, "text": " In fact, if I'm building a neural network, this is how I choose activation functions", "tokens": [50364, 286, 458, 300, 300, 390, 445, 364, 21769, 10835, 11, 457, 10309, 362, 1352, 300, 1228, 264, 50610, 50610, 1300, 43, 52, 24433, 2445, 393, 3082, 428, 18161, 3209, 281, 1466, 257, 857, 4663, 382, 731, 11, 597, 50894, 50894, 307, 983, 337, 881, 25742, 11, 498, 291, 434, 1382, 281, 4536, 437, 24433, 2445, 281, 764, 51098, 51098, 337, 264, 7633, 4583, 11, 264, 1300, 43, 52, 24433, 2445, 575, 1813, 586, 538, 1400, 264, 881, 2689, 51384, 51384, 3922, 13, 51434, 51434, 682, 1186, 11, 498, 286, 478, 2390, 257, 18161, 3209, 11, 341, 307, 577, 286, 2826, 24433, 6828, 51686, 51686, 337, 264, 7633, 7914, 382, 731, 13, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.1015433933423913, "compression_ratio": 1.90625, "no_speech_prob": 2.7693042738974327e-06}, {"id": 69, "seek": 30164, "start": 328.08, "end": 330.44, "text": " for the hidden layers as well.", "tokens": [50364, 286, 458, 300, 300, 390, 445, 364, 21769, 10835, 11, 457, 10309, 362, 1352, 300, 1228, 264, 50610, 50610, 1300, 43, 52, 24433, 2445, 393, 3082, 428, 18161, 3209, 281, 1466, 257, 857, 4663, 382, 731, 11, 597, 50894, 50894, 307, 983, 337, 881, 25742, 11, 498, 291, 434, 1382, 281, 4536, 437, 24433, 2445, 281, 764, 51098, 51098, 337, 264, 7633, 4583, 11, 264, 1300, 43, 52, 24433, 2445, 575, 1813, 586, 538, 1400, 264, 881, 2689, 51384, 51384, 3922, 13, 51434, 51434, 682, 1186, 11, 498, 286, 478, 2390, 257, 18161, 3209, 11, 341, 307, 577, 286, 2826, 24433, 6828, 51686, 51686, 337, 264, 7633, 7914, 382, 731, 13, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.1015433933423913, "compression_ratio": 1.90625, "no_speech_prob": 2.7693042738974327e-06}, {"id": 70, "seek": 33044, "start": 330.44, "end": 337.04, "text": " So to summarize, here's what I recommend in terms of how you choose the activation functions", "tokens": [50364, 407, 281, 20858, 11, 510, 311, 437, 286, 2748, 294, 2115, 295, 577, 291, 2826, 264, 24433, 6828, 50694, 50694, 337, 428, 18161, 3209, 13, 50784, 50784, 1171, 264, 5598, 4583, 11, 764, 257, 4556, 3280, 327, 498, 291, 362, 257, 17434, 21538, 1154, 11, 8213, 51094, 51094, 498, 398, 307, 257, 1230, 300, 393, 747, 322, 3353, 420, 3671, 4190, 11, 420, 764, 1300, 43, 52, 498, 398, 393, 747, 51418, 51418, 322, 787, 3353, 4190, 420, 4018, 3353, 4190, 420, 2107, 12, 28561, 1166, 4190, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.09155571979025136, "compression_ratio": 1.6486486486486487, "no_speech_prob": 1.7603307469471474e-06}, {"id": 71, "seek": 33044, "start": 337.04, "end": 338.84, "text": " for your neural network.", "tokens": [50364, 407, 281, 20858, 11, 510, 311, 437, 286, 2748, 294, 2115, 295, 577, 291, 2826, 264, 24433, 6828, 50694, 50694, 337, 428, 18161, 3209, 13, 50784, 50784, 1171, 264, 5598, 4583, 11, 764, 257, 4556, 3280, 327, 498, 291, 362, 257, 17434, 21538, 1154, 11, 8213, 51094, 51094, 498, 398, 307, 257, 1230, 300, 393, 747, 322, 3353, 420, 3671, 4190, 11, 420, 764, 1300, 43, 52, 498, 398, 393, 747, 51418, 51418, 322, 787, 3353, 4190, 420, 4018, 3353, 4190, 420, 2107, 12, 28561, 1166, 4190, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.09155571979025136, "compression_ratio": 1.6486486486486487, "no_speech_prob": 1.7603307469471474e-06}, {"id": 72, "seek": 33044, "start": 338.84, "end": 345.04, "text": " For the output layer, use a sigmoid if you have a binary classification problem, linear", "tokens": [50364, 407, 281, 20858, 11, 510, 311, 437, 286, 2748, 294, 2115, 295, 577, 291, 2826, 264, 24433, 6828, 50694, 50694, 337, 428, 18161, 3209, 13, 50784, 50784, 1171, 264, 5598, 4583, 11, 764, 257, 4556, 3280, 327, 498, 291, 362, 257, 17434, 21538, 1154, 11, 8213, 51094, 51094, 498, 398, 307, 257, 1230, 300, 393, 747, 322, 3353, 420, 3671, 4190, 11, 420, 764, 1300, 43, 52, 498, 398, 393, 747, 51418, 51418, 322, 787, 3353, 4190, 420, 4018, 3353, 4190, 420, 2107, 12, 28561, 1166, 4190, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.09155571979025136, "compression_ratio": 1.6486486486486487, "no_speech_prob": 1.7603307469471474e-06}, {"id": 73, "seek": 33044, "start": 345.04, "end": 351.52, "text": " if Y is a number that can take on positive or negative values, or use ReLU if Y can take", "tokens": [50364, 407, 281, 20858, 11, 510, 311, 437, 286, 2748, 294, 2115, 295, 577, 291, 2826, 264, 24433, 6828, 50694, 50694, 337, 428, 18161, 3209, 13, 50784, 50784, 1171, 264, 5598, 4583, 11, 764, 257, 4556, 3280, 327, 498, 291, 362, 257, 17434, 21538, 1154, 11, 8213, 51094, 51094, 498, 398, 307, 257, 1230, 300, 393, 747, 322, 3353, 420, 3671, 4190, 11, 420, 764, 1300, 43, 52, 498, 398, 393, 747, 51418, 51418, 322, 787, 3353, 4190, 420, 4018, 3353, 4190, 420, 2107, 12, 28561, 1166, 4190, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.09155571979025136, "compression_ratio": 1.6486486486486487, "no_speech_prob": 1.7603307469471474e-06}, {"id": 74, "seek": 33044, "start": 351.52, "end": 356.56, "text": " on only positive values or zero positive values or non-negative values.", "tokens": [50364, 407, 281, 20858, 11, 510, 311, 437, 286, 2748, 294, 2115, 295, 577, 291, 2826, 264, 24433, 6828, 50694, 50694, 337, 428, 18161, 3209, 13, 50784, 50784, 1171, 264, 5598, 4583, 11, 764, 257, 4556, 3280, 327, 498, 291, 362, 257, 17434, 21538, 1154, 11, 8213, 51094, 51094, 498, 398, 307, 257, 1230, 300, 393, 747, 322, 3353, 420, 3671, 4190, 11, 420, 764, 1300, 43, 52, 498, 398, 393, 747, 51418, 51418, 322, 787, 3353, 4190, 420, 4018, 3353, 4190, 420, 2107, 12, 28561, 1166, 4190, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.09155571979025136, "compression_ratio": 1.6486486486486487, "no_speech_prob": 1.7603307469471474e-06}, {"id": 75, "seek": 35656, "start": 356.56, "end": 365.12, "text": " Then for the hidden layers, I would recommend just using ReLU as a default activation function.", "tokens": [50364, 1396, 337, 264, 7633, 7914, 11, 286, 576, 2748, 445, 1228, 1300, 43, 52, 382, 257, 7576, 24433, 2445, 13, 50792, 50792, 400, 294, 37624, 11, 341, 307, 577, 291, 576, 4445, 309, 13, 50978, 50978, 16571, 813, 1566, 24433, 6915, 4556, 3280, 327, 382, 321, 632, 8046, 11, 291, 393, 550, 337, 264, 51274, 51274, 7633, 7914, 11, 300, 311, 264, 700, 7633, 4583, 11, 264, 1150, 7633, 4583, 11, 1029, 37624, 281, 51536, 51536, 764, 264, 1300, 43, 52, 24433, 2445, 13, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.12517927993427624, "compression_ratio": 1.7439613526570048, "no_speech_prob": 6.9621428337995894e-06}, {"id": 76, "seek": 35656, "start": 365.12, "end": 368.84000000000003, "text": " And in TensorFlow, this is how you would implement it.", "tokens": [50364, 1396, 337, 264, 7633, 7914, 11, 286, 576, 2748, 445, 1228, 1300, 43, 52, 382, 257, 7576, 24433, 2445, 13, 50792, 50792, 400, 294, 37624, 11, 341, 307, 577, 291, 576, 4445, 309, 13, 50978, 50978, 16571, 813, 1566, 24433, 6915, 4556, 3280, 327, 382, 321, 632, 8046, 11, 291, 393, 550, 337, 264, 51274, 51274, 7633, 7914, 11, 300, 311, 264, 700, 7633, 4583, 11, 264, 1150, 7633, 4583, 11, 1029, 37624, 281, 51536, 51536, 764, 264, 1300, 43, 52, 24433, 2445, 13, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.12517927993427624, "compression_ratio": 1.7439613526570048, "no_speech_prob": 6.9621428337995894e-06}, {"id": 77, "seek": 35656, "start": 368.84000000000003, "end": 374.76, "text": " Rather than saying activation equals sigmoid as we had previously, you can then for the", "tokens": [50364, 1396, 337, 264, 7633, 7914, 11, 286, 576, 2748, 445, 1228, 1300, 43, 52, 382, 257, 7576, 24433, 2445, 13, 50792, 50792, 400, 294, 37624, 11, 341, 307, 577, 291, 576, 4445, 309, 13, 50978, 50978, 16571, 813, 1566, 24433, 6915, 4556, 3280, 327, 382, 321, 632, 8046, 11, 291, 393, 550, 337, 264, 51274, 51274, 7633, 7914, 11, 300, 311, 264, 700, 7633, 4583, 11, 264, 1150, 7633, 4583, 11, 1029, 37624, 281, 51536, 51536, 764, 264, 1300, 43, 52, 24433, 2445, 13, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.12517927993427624, "compression_ratio": 1.7439613526570048, "no_speech_prob": 6.9621428337995894e-06}, {"id": 78, "seek": 35656, "start": 374.76, "end": 380.0, "text": " hidden layers, that's the first hidden layer, the second hidden layer, ask TensorFlow to", "tokens": [50364, 1396, 337, 264, 7633, 7914, 11, 286, 576, 2748, 445, 1228, 1300, 43, 52, 382, 257, 7576, 24433, 2445, 13, 50792, 50792, 400, 294, 37624, 11, 341, 307, 577, 291, 576, 4445, 309, 13, 50978, 50978, 16571, 813, 1566, 24433, 6915, 4556, 3280, 327, 382, 321, 632, 8046, 11, 291, 393, 550, 337, 264, 51274, 51274, 7633, 7914, 11, 300, 311, 264, 700, 7633, 4583, 11, 264, 1150, 7633, 4583, 11, 1029, 37624, 281, 51536, 51536, 764, 264, 1300, 43, 52, 24433, 2445, 13, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.12517927993427624, "compression_ratio": 1.7439613526570048, "no_speech_prob": 6.9621428337995894e-06}, {"id": 79, "seek": 35656, "start": 380.0, "end": 383.08, "text": " use the ReLU activation function.", "tokens": [50364, 1396, 337, 264, 7633, 7914, 11, 286, 576, 2748, 445, 1228, 1300, 43, 52, 382, 257, 7576, 24433, 2445, 13, 50792, 50792, 400, 294, 37624, 11, 341, 307, 577, 291, 576, 4445, 309, 13, 50978, 50978, 16571, 813, 1566, 24433, 6915, 4556, 3280, 327, 382, 321, 632, 8046, 11, 291, 393, 550, 337, 264, 51274, 51274, 7633, 7914, 11, 300, 311, 264, 700, 7633, 4583, 11, 264, 1150, 7633, 4583, 11, 1029, 37624, 281, 51536, 51536, 764, 264, 1300, 43, 52, 24433, 2445, 13, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.12517927993427624, "compression_ratio": 1.7439613526570048, "no_speech_prob": 6.9621428337995894e-06}, {"id": 80, "seek": 38308, "start": 383.08, "end": 389.35999999999996, "text": " And then for the output layer, in this example, I've asked it to use the sigmoid activation", "tokens": [50364, 400, 550, 337, 264, 5598, 4583, 11, 294, 341, 1365, 11, 286, 600, 2351, 309, 281, 764, 264, 4556, 3280, 327, 24433, 50678, 50678, 2445, 13, 50728, 50728, 583, 498, 291, 528, 309, 281, 764, 264, 8213, 24433, 2445, 2602, 11, 300, 311, 264, 28431, 337, 309, 13, 51016, 51016, 1610, 498, 291, 1415, 281, 764, 264, 1300, 43, 52, 24433, 2445, 11, 300, 3110, 264, 28431, 337, 309, 13, 51323, 51323, 2022, 341, 29021, 992, 295, 24433, 6828, 11, 291, 1116, 312, 731, 24889, 281, 1322, 709, 544, 51578, 51578], "temperature": 0.0, "avg_logprob": -0.10578239604990969, "compression_ratio": 1.8724489795918366, "no_speech_prob": 1.3081586303087533e-06}, {"id": 81, "seek": 38308, "start": 389.35999999999996, "end": 390.35999999999996, "text": " function.", "tokens": [50364, 400, 550, 337, 264, 5598, 4583, 11, 294, 341, 1365, 11, 286, 600, 2351, 309, 281, 764, 264, 4556, 3280, 327, 24433, 50678, 50678, 2445, 13, 50728, 50728, 583, 498, 291, 528, 309, 281, 764, 264, 8213, 24433, 2445, 2602, 11, 300, 311, 264, 28431, 337, 309, 13, 51016, 51016, 1610, 498, 291, 1415, 281, 764, 264, 1300, 43, 52, 24433, 2445, 11, 300, 3110, 264, 28431, 337, 309, 13, 51323, 51323, 2022, 341, 29021, 992, 295, 24433, 6828, 11, 291, 1116, 312, 731, 24889, 281, 1322, 709, 544, 51578, 51578], "temperature": 0.0, "avg_logprob": -0.10578239604990969, "compression_ratio": 1.8724489795918366, "no_speech_prob": 1.3081586303087533e-06}, {"id": 82, "seek": 38308, "start": 390.35999999999996, "end": 396.12, "text": " But if you want it to use the linear activation function instead, that's the syntax for it.", "tokens": [50364, 400, 550, 337, 264, 5598, 4583, 11, 294, 341, 1365, 11, 286, 600, 2351, 309, 281, 764, 264, 4556, 3280, 327, 24433, 50678, 50678, 2445, 13, 50728, 50728, 583, 498, 291, 528, 309, 281, 764, 264, 8213, 24433, 2445, 2602, 11, 300, 311, 264, 28431, 337, 309, 13, 51016, 51016, 1610, 498, 291, 1415, 281, 764, 264, 1300, 43, 52, 24433, 2445, 11, 300, 3110, 264, 28431, 337, 309, 13, 51323, 51323, 2022, 341, 29021, 992, 295, 24433, 6828, 11, 291, 1116, 312, 731, 24889, 281, 1322, 709, 544, 51578, 51578], "temperature": 0.0, "avg_logprob": -0.10578239604990969, "compression_ratio": 1.8724489795918366, "no_speech_prob": 1.3081586303087533e-06}, {"id": 83, "seek": 38308, "start": 396.12, "end": 402.26, "text": " Or if you wanted to use the ReLU activation function, that shows the syntax for it.", "tokens": [50364, 400, 550, 337, 264, 5598, 4583, 11, 294, 341, 1365, 11, 286, 600, 2351, 309, 281, 764, 264, 4556, 3280, 327, 24433, 50678, 50678, 2445, 13, 50728, 50728, 583, 498, 291, 528, 309, 281, 764, 264, 8213, 24433, 2445, 2602, 11, 300, 311, 264, 28431, 337, 309, 13, 51016, 51016, 1610, 498, 291, 1415, 281, 764, 264, 1300, 43, 52, 24433, 2445, 11, 300, 3110, 264, 28431, 337, 309, 13, 51323, 51323, 2022, 341, 29021, 992, 295, 24433, 6828, 11, 291, 1116, 312, 731, 24889, 281, 1322, 709, 544, 51578, 51578], "temperature": 0.0, "avg_logprob": -0.10578239604990969, "compression_ratio": 1.8724489795918366, "no_speech_prob": 1.3081586303087533e-06}, {"id": 84, "seek": 38308, "start": 402.26, "end": 407.36, "text": " With this richer set of activation functions, you'd be well positioned to build much more", "tokens": [50364, 400, 550, 337, 264, 5598, 4583, 11, 294, 341, 1365, 11, 286, 600, 2351, 309, 281, 764, 264, 4556, 3280, 327, 24433, 50678, 50678, 2445, 13, 50728, 50728, 583, 498, 291, 528, 309, 281, 764, 264, 8213, 24433, 2445, 2602, 11, 300, 311, 264, 28431, 337, 309, 13, 51016, 51016, 1610, 498, 291, 1415, 281, 764, 264, 1300, 43, 52, 24433, 2445, 11, 300, 3110, 264, 28431, 337, 309, 13, 51323, 51323, 2022, 341, 29021, 992, 295, 24433, 6828, 11, 291, 1116, 312, 731, 24889, 281, 1322, 709, 544, 51578, 51578], "temperature": 0.0, "avg_logprob": -0.10578239604990969, "compression_ratio": 1.8724489795918366, "no_speech_prob": 1.3081586303087533e-06}, {"id": 85, "seek": 40736, "start": 407.36, "end": 413.72, "text": " powerful neural networks than just once using only the sigmoid activation function.", "tokens": [50364, 4005, 18161, 9590, 813, 445, 1564, 1228, 787, 264, 4556, 3280, 327, 24433, 2445, 13, 50682, 50682, 3146, 264, 636, 11, 498, 291, 574, 412, 264, 2132, 10394, 11, 291, 2171, 1568, 295, 16552, 1228, 754, 661, 50974, 50974, 24433, 6828, 11, 1270, 382, 264, 7603, 71, 24433, 2445, 420, 264, 476, 15681, 1300, 43, 52, 24433, 2445, 51324, 51324, 420, 264, 1693, 742, 24433, 2445, 13, 51480, 51480, 2048, 1326, 924, 11, 10309, 2171, 808, 493, 365, 1071, 1880, 24433, 2445, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.12550122710480088, "compression_ratio": 1.9519230769230769, "no_speech_prob": 3.4465408589312574e-06}, {"id": 86, "seek": 40736, "start": 413.72, "end": 419.56, "text": " By the way, if you look at the research literature, you sometimes hear of authors using even other", "tokens": [50364, 4005, 18161, 9590, 813, 445, 1564, 1228, 787, 264, 4556, 3280, 327, 24433, 2445, 13, 50682, 50682, 3146, 264, 636, 11, 498, 291, 574, 412, 264, 2132, 10394, 11, 291, 2171, 1568, 295, 16552, 1228, 754, 661, 50974, 50974, 24433, 6828, 11, 1270, 382, 264, 7603, 71, 24433, 2445, 420, 264, 476, 15681, 1300, 43, 52, 24433, 2445, 51324, 51324, 420, 264, 1693, 742, 24433, 2445, 13, 51480, 51480, 2048, 1326, 924, 11, 10309, 2171, 808, 493, 365, 1071, 1880, 24433, 2445, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.12550122710480088, "compression_ratio": 1.9519230769230769, "no_speech_prob": 3.4465408589312574e-06}, {"id": 87, "seek": 40736, "start": 419.56, "end": 426.56, "text": " activation functions, such as the tanh activation function or the leaky ReLU activation function", "tokens": [50364, 4005, 18161, 9590, 813, 445, 1564, 1228, 787, 264, 4556, 3280, 327, 24433, 2445, 13, 50682, 50682, 3146, 264, 636, 11, 498, 291, 574, 412, 264, 2132, 10394, 11, 291, 2171, 1568, 295, 16552, 1228, 754, 661, 50974, 50974, 24433, 6828, 11, 1270, 382, 264, 7603, 71, 24433, 2445, 420, 264, 476, 15681, 1300, 43, 52, 24433, 2445, 51324, 51324, 420, 264, 1693, 742, 24433, 2445, 13, 51480, 51480, 2048, 1326, 924, 11, 10309, 2171, 808, 493, 365, 1071, 1880, 24433, 2445, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.12550122710480088, "compression_ratio": 1.9519230769230769, "no_speech_prob": 3.4465408589312574e-06}, {"id": 88, "seek": 40736, "start": 426.56, "end": 429.68, "text": " or the swish activation function.", "tokens": [50364, 4005, 18161, 9590, 813, 445, 1564, 1228, 787, 264, 4556, 3280, 327, 24433, 2445, 13, 50682, 50682, 3146, 264, 636, 11, 498, 291, 574, 412, 264, 2132, 10394, 11, 291, 2171, 1568, 295, 16552, 1228, 754, 661, 50974, 50974, 24433, 6828, 11, 1270, 382, 264, 7603, 71, 24433, 2445, 420, 264, 476, 15681, 1300, 43, 52, 24433, 2445, 51324, 51324, 420, 264, 1693, 742, 24433, 2445, 13, 51480, 51480, 2048, 1326, 924, 11, 10309, 2171, 808, 493, 365, 1071, 1880, 24433, 2445, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.12550122710480088, "compression_ratio": 1.9519230769230769, "no_speech_prob": 3.4465408589312574e-06}, {"id": 89, "seek": 40736, "start": 429.68, "end": 434.64, "text": " Every few years, researchers sometimes come up with another interesting activation function,", "tokens": [50364, 4005, 18161, 9590, 813, 445, 1564, 1228, 787, 264, 4556, 3280, 327, 24433, 2445, 13, 50682, 50682, 3146, 264, 636, 11, 498, 291, 574, 412, 264, 2132, 10394, 11, 291, 2171, 1568, 295, 16552, 1228, 754, 661, 50974, 50974, 24433, 6828, 11, 1270, 382, 264, 7603, 71, 24433, 2445, 420, 264, 476, 15681, 1300, 43, 52, 24433, 2445, 51324, 51324, 420, 264, 1693, 742, 24433, 2445, 13, 51480, 51480, 2048, 1326, 924, 11, 10309, 2171, 808, 493, 365, 1071, 1880, 24433, 2445, 11, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.12550122710480088, "compression_ratio": 1.9519230769230769, "no_speech_prob": 3.4465408589312574e-06}, {"id": 90, "seek": 43464, "start": 434.64, "end": 437.56, "text": " and sometimes they do work a little bit better.", "tokens": [50364, 293, 2171, 436, 360, 589, 257, 707, 857, 1101, 13, 50510, 50510, 1171, 1365, 11, 286, 600, 1143, 264, 476, 15681, 1300, 43, 52, 24433, 2445, 257, 1326, 1413, 294, 452, 589, 11, 293, 2171, 50794, 50794, 309, 1985, 257, 707, 857, 1101, 813, 264, 1300, 43, 52, 24433, 2445, 291, 3264, 466, 294, 341, 51038, 51038, 960, 13, 51088, 51088, 583, 286, 519, 337, 264, 881, 644, 11, 293, 337, 264, 8369, 6286, 295, 5821, 11, 437, 291, 3264, 51322, 51322, 466, 294, 341, 960, 576, 312, 665, 1547, 13, 51470, 51470, 2720, 1164, 11, 498, 291, 528, 281, 1466, 544, 466, 661, 24433, 6828, 11, 841, 1737, 281, 574, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.07150344848632813, "compression_ratio": 1.889344262295082, "no_speech_prob": 5.682264145434601e-06}, {"id": 91, "seek": 43464, "start": 437.56, "end": 443.24, "text": " For example, I've used the leaky ReLU activation function a few times in my work, and sometimes", "tokens": [50364, 293, 2171, 436, 360, 589, 257, 707, 857, 1101, 13, 50510, 50510, 1171, 1365, 11, 286, 600, 1143, 264, 476, 15681, 1300, 43, 52, 24433, 2445, 257, 1326, 1413, 294, 452, 589, 11, 293, 2171, 50794, 50794, 309, 1985, 257, 707, 857, 1101, 813, 264, 1300, 43, 52, 24433, 2445, 291, 3264, 466, 294, 341, 51038, 51038, 960, 13, 51088, 51088, 583, 286, 519, 337, 264, 881, 644, 11, 293, 337, 264, 8369, 6286, 295, 5821, 11, 437, 291, 3264, 51322, 51322, 466, 294, 341, 960, 576, 312, 665, 1547, 13, 51470, 51470, 2720, 1164, 11, 498, 291, 528, 281, 1466, 544, 466, 661, 24433, 6828, 11, 841, 1737, 281, 574, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.07150344848632813, "compression_ratio": 1.889344262295082, "no_speech_prob": 5.682264145434601e-06}, {"id": 92, "seek": 43464, "start": 443.24, "end": 448.12, "text": " it works a little bit better than the ReLU activation function you learned about in this", "tokens": [50364, 293, 2171, 436, 360, 589, 257, 707, 857, 1101, 13, 50510, 50510, 1171, 1365, 11, 286, 600, 1143, 264, 476, 15681, 1300, 43, 52, 24433, 2445, 257, 1326, 1413, 294, 452, 589, 11, 293, 2171, 50794, 50794, 309, 1985, 257, 707, 857, 1101, 813, 264, 1300, 43, 52, 24433, 2445, 291, 3264, 466, 294, 341, 51038, 51038, 960, 13, 51088, 51088, 583, 286, 519, 337, 264, 881, 644, 11, 293, 337, 264, 8369, 6286, 295, 5821, 11, 437, 291, 3264, 51322, 51322, 466, 294, 341, 960, 576, 312, 665, 1547, 13, 51470, 51470, 2720, 1164, 11, 498, 291, 528, 281, 1466, 544, 466, 661, 24433, 6828, 11, 841, 1737, 281, 574, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.07150344848632813, "compression_ratio": 1.889344262295082, "no_speech_prob": 5.682264145434601e-06}, {"id": 93, "seek": 43464, "start": 448.12, "end": 449.12, "text": " video.", "tokens": [50364, 293, 2171, 436, 360, 589, 257, 707, 857, 1101, 13, 50510, 50510, 1171, 1365, 11, 286, 600, 1143, 264, 476, 15681, 1300, 43, 52, 24433, 2445, 257, 1326, 1413, 294, 452, 589, 11, 293, 2171, 50794, 50794, 309, 1985, 257, 707, 857, 1101, 813, 264, 1300, 43, 52, 24433, 2445, 291, 3264, 466, 294, 341, 51038, 51038, 960, 13, 51088, 51088, 583, 286, 519, 337, 264, 881, 644, 11, 293, 337, 264, 8369, 6286, 295, 5821, 11, 437, 291, 3264, 51322, 51322, 466, 294, 341, 960, 576, 312, 665, 1547, 13, 51470, 51470, 2720, 1164, 11, 498, 291, 528, 281, 1466, 544, 466, 661, 24433, 6828, 11, 841, 1737, 281, 574, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.07150344848632813, "compression_ratio": 1.889344262295082, "no_speech_prob": 5.682264145434601e-06}, {"id": 94, "seek": 43464, "start": 449.12, "end": 453.8, "text": " But I think for the most part, and for the vast majority of applications, what you learned", "tokens": [50364, 293, 2171, 436, 360, 589, 257, 707, 857, 1101, 13, 50510, 50510, 1171, 1365, 11, 286, 600, 1143, 264, 476, 15681, 1300, 43, 52, 24433, 2445, 257, 1326, 1413, 294, 452, 589, 11, 293, 2171, 50794, 50794, 309, 1985, 257, 707, 857, 1101, 813, 264, 1300, 43, 52, 24433, 2445, 291, 3264, 466, 294, 341, 51038, 51038, 960, 13, 51088, 51088, 583, 286, 519, 337, 264, 881, 644, 11, 293, 337, 264, 8369, 6286, 295, 5821, 11, 437, 291, 3264, 51322, 51322, 466, 294, 341, 960, 576, 312, 665, 1547, 13, 51470, 51470, 2720, 1164, 11, 498, 291, 528, 281, 1466, 544, 466, 661, 24433, 6828, 11, 841, 1737, 281, 574, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.07150344848632813, "compression_ratio": 1.889344262295082, "no_speech_prob": 5.682264145434601e-06}, {"id": 95, "seek": 43464, "start": 453.8, "end": 456.76, "text": " about in this video would be good enough.", "tokens": [50364, 293, 2171, 436, 360, 589, 257, 707, 857, 1101, 13, 50510, 50510, 1171, 1365, 11, 286, 600, 1143, 264, 476, 15681, 1300, 43, 52, 24433, 2445, 257, 1326, 1413, 294, 452, 589, 11, 293, 2171, 50794, 50794, 309, 1985, 257, 707, 857, 1101, 813, 264, 1300, 43, 52, 24433, 2445, 291, 3264, 466, 294, 341, 51038, 51038, 960, 13, 51088, 51088, 583, 286, 519, 337, 264, 881, 644, 11, 293, 337, 264, 8369, 6286, 295, 5821, 11, 437, 291, 3264, 51322, 51322, 466, 294, 341, 960, 576, 312, 665, 1547, 13, 51470, 51470, 2720, 1164, 11, 498, 291, 528, 281, 1466, 544, 466, 661, 24433, 6828, 11, 841, 1737, 281, 574, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.07150344848632813, "compression_ratio": 1.889344262295082, "no_speech_prob": 5.682264145434601e-06}, {"id": 96, "seek": 43464, "start": 456.76, "end": 461.84, "text": " Of course, if you want to learn more about other activation functions, feel free to look", "tokens": [50364, 293, 2171, 436, 360, 589, 257, 707, 857, 1101, 13, 50510, 50510, 1171, 1365, 11, 286, 600, 1143, 264, 476, 15681, 1300, 43, 52, 24433, 2445, 257, 1326, 1413, 294, 452, 589, 11, 293, 2171, 50794, 50794, 309, 1985, 257, 707, 857, 1101, 813, 264, 1300, 43, 52, 24433, 2445, 291, 3264, 466, 294, 341, 51038, 51038, 960, 13, 51088, 51088, 583, 286, 519, 337, 264, 881, 644, 11, 293, 337, 264, 8369, 6286, 295, 5821, 11, 437, 291, 3264, 51322, 51322, 466, 294, 341, 960, 576, 312, 665, 1547, 13, 51470, 51470, 2720, 1164, 11, 498, 291, 528, 281, 1466, 544, 466, 661, 24433, 6828, 11, 841, 1737, 281, 574, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.07150344848632813, "compression_ratio": 1.889344262295082, "no_speech_prob": 5.682264145434601e-06}, {"id": 97, "seek": 46184, "start": 461.84, "end": 467.4, "text": " on the internet, and there are just a small handful of cases where these other activation", "tokens": [50364, 322, 264, 4705, 11, 293, 456, 366, 445, 257, 1359, 16458, 295, 3331, 689, 613, 661, 24433, 50642, 50642, 6828, 727, 312, 754, 544, 4005, 382, 731, 13, 50850, 50850, 2022, 300, 11, 286, 1454, 291, 611, 2103, 11350, 613, 3487, 11, 613, 24433, 6828, 11, 294, 51152, 51152, 264, 17312, 20339, 293, 294, 264, 3124, 20339, 13, 51320, 51320, 583, 341, 19658, 1939, 1071, 1168, 13, 51460, 51460, 1545, 360, 321, 754, 643, 24433, 6828, 412, 439, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10993198027093727, "compression_ratio": 1.7401960784313726, "no_speech_prob": 5.224226333666593e-05}, {"id": 98, "seek": 46184, "start": 467.4, "end": 471.56, "text": " functions could be even more powerful as well.", "tokens": [50364, 322, 264, 4705, 11, 293, 456, 366, 445, 257, 1359, 16458, 295, 3331, 689, 613, 661, 24433, 50642, 50642, 6828, 727, 312, 754, 544, 4005, 382, 731, 13, 50850, 50850, 2022, 300, 11, 286, 1454, 291, 611, 2103, 11350, 613, 3487, 11, 613, 24433, 6828, 11, 294, 51152, 51152, 264, 17312, 20339, 293, 294, 264, 3124, 20339, 13, 51320, 51320, 583, 341, 19658, 1939, 1071, 1168, 13, 51460, 51460, 1545, 360, 321, 754, 643, 24433, 6828, 412, 439, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10993198027093727, "compression_ratio": 1.7401960784313726, "no_speech_prob": 5.224226333666593e-05}, {"id": 99, "seek": 46184, "start": 471.56, "end": 477.59999999999997, "text": " With that, I hope you also enjoy practicing these ideas, these activation functions, in", "tokens": [50364, 322, 264, 4705, 11, 293, 456, 366, 445, 257, 1359, 16458, 295, 3331, 689, 613, 661, 24433, 50642, 50642, 6828, 727, 312, 754, 544, 4005, 382, 731, 13, 50850, 50850, 2022, 300, 11, 286, 1454, 291, 611, 2103, 11350, 613, 3487, 11, 613, 24433, 6828, 11, 294, 51152, 51152, 264, 17312, 20339, 293, 294, 264, 3124, 20339, 13, 51320, 51320, 583, 341, 19658, 1939, 1071, 1168, 13, 51460, 51460, 1545, 360, 321, 754, 643, 24433, 6828, 412, 439, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10993198027093727, "compression_ratio": 1.7401960784313726, "no_speech_prob": 5.224226333666593e-05}, {"id": 100, "seek": 46184, "start": 477.59999999999997, "end": 480.96, "text": " the optional labs and in the practice labs.", "tokens": [50364, 322, 264, 4705, 11, 293, 456, 366, 445, 257, 1359, 16458, 295, 3331, 689, 613, 661, 24433, 50642, 50642, 6828, 727, 312, 754, 544, 4005, 382, 731, 13, 50850, 50850, 2022, 300, 11, 286, 1454, 291, 611, 2103, 11350, 613, 3487, 11, 613, 24433, 6828, 11, 294, 51152, 51152, 264, 17312, 20339, 293, 294, 264, 3124, 20339, 13, 51320, 51320, 583, 341, 19658, 1939, 1071, 1168, 13, 51460, 51460, 1545, 360, 321, 754, 643, 24433, 6828, 412, 439, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10993198027093727, "compression_ratio": 1.7401960784313726, "no_speech_prob": 5.224226333666593e-05}, {"id": 101, "seek": 46184, "start": 480.96, "end": 483.76, "text": " But this raises yet another question.", "tokens": [50364, 322, 264, 4705, 11, 293, 456, 366, 445, 257, 1359, 16458, 295, 3331, 689, 613, 661, 24433, 50642, 50642, 6828, 727, 312, 754, 544, 4005, 382, 731, 13, 50850, 50850, 2022, 300, 11, 286, 1454, 291, 611, 2103, 11350, 613, 3487, 11, 613, 24433, 6828, 11, 294, 51152, 51152, 264, 17312, 20339, 293, 294, 264, 3124, 20339, 13, 51320, 51320, 583, 341, 19658, 1939, 1071, 1168, 13, 51460, 51460, 1545, 360, 321, 754, 643, 24433, 6828, 412, 439, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10993198027093727, "compression_ratio": 1.7401960784313726, "no_speech_prob": 5.224226333666593e-05}, {"id": 102, "seek": 46184, "start": 483.76, "end": 486.84, "text": " Why do we even need activation functions at all?", "tokens": [50364, 322, 264, 4705, 11, 293, 456, 366, 445, 257, 1359, 16458, 295, 3331, 689, 613, 661, 24433, 50642, 50642, 6828, 727, 312, 754, 544, 4005, 382, 731, 13, 50850, 50850, 2022, 300, 11, 286, 1454, 291, 611, 2103, 11350, 613, 3487, 11, 613, 24433, 6828, 11, 294, 51152, 51152, 264, 17312, 20339, 293, 294, 264, 3124, 20339, 13, 51320, 51320, 583, 341, 19658, 1939, 1071, 1168, 13, 51460, 51460, 1545, 360, 321, 754, 643, 24433, 6828, 412, 439, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10993198027093727, "compression_ratio": 1.7401960784313726, "no_speech_prob": 5.224226333666593e-05}, {"id": 103, "seek": 48684, "start": 486.84, "end": 492.71999999999997, "text": " Why don't we just use the linear activation function, or use no activation function anywhere?", "tokens": [50364, 1545, 500, 380, 321, 445, 764, 264, 8213, 24433, 2445, 11, 420, 764, 572, 24433, 2445, 4992, 30, 50658, 50658, 467, 4523, 484, 341, 775, 406, 589, 412, 439, 13, 50810, 50810, 400, 294, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 983, 300, 311, 264, 1389, 11, 293, 983, 24433, 6828, 51045, 51045, 366, 370, 1021, 337, 1242, 428, 18161, 9590, 281, 589, 13, 51212], "temperature": 0.0, "avg_logprob": -0.1341562407357352, "compression_ratio": 1.6436781609195403, "no_speech_prob": 3.299187301308848e-05}, {"id": 104, "seek": 48684, "start": 492.71999999999997, "end": 495.76, "text": " It turns out this does not work at all.", "tokens": [50364, 1545, 500, 380, 321, 445, 764, 264, 8213, 24433, 2445, 11, 420, 764, 572, 24433, 2445, 4992, 30, 50658, 50658, 467, 4523, 484, 341, 775, 406, 589, 412, 439, 13, 50810, 50810, 400, 294, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 983, 300, 311, 264, 1389, 11, 293, 983, 24433, 6828, 51045, 51045, 366, 370, 1021, 337, 1242, 428, 18161, 9590, 281, 589, 13, 51212], "temperature": 0.0, "avg_logprob": -0.1341562407357352, "compression_ratio": 1.6436781609195403, "no_speech_prob": 3.299187301308848e-05}, {"id": 105, "seek": 48684, "start": 495.76, "end": 500.46, "text": " And in the next video, let's take a look at why that's the case, and why activation functions", "tokens": [50364, 1545, 500, 380, 321, 445, 764, 264, 8213, 24433, 2445, 11, 420, 764, 572, 24433, 2445, 4992, 30, 50658, 50658, 467, 4523, 484, 341, 775, 406, 589, 412, 439, 13, 50810, 50810, 400, 294, 264, 958, 960, 11, 718, 311, 747, 257, 574, 412, 983, 300, 311, 264, 1389, 11, 293, 983, 24433, 6828, 51045, 51045, 366, 370, 1021, 337, 1242, 428, 18161, 9590, 281, 589, 13, 51212], "temperature": 0.0, "avg_logprob": -0.1341562407357352, "compression_ratio": 1.6436781609195403, "no_speech_prob": 3.299187301308848e-05}, {"id": 106, "seek": 50046, "start": 500.46, "end": 517.16, "text": " are so important for getting your neural networks to work.", "tokens": [50364, 366, 370, 1021, 337, 1242, 428, 18161, 9590, 281, 589, 13, 51199], "temperature": 0.0, "avg_logprob": -0.32116719654628206, "compression_ratio": 0.9666666666666667, "no_speech_prob": 5.93625509281992e-06}], "language": "en", "video_id": "orElYjWScBw", "entity": "ML Specialization, Andrew Ng (2022)"}}