{"video_id": "7Rb4s9wNOmc", "title": "Week 8 \u2013 Practicum: Variational autoencoders", "description": "Course website: http://bit.ly/pDL-home\nPlaylist: http://bit.ly/pDL-YouTube\nSpeaker: Alfredo Canziani\nWeek 8: http://bit.ly/pDL-en-08\n\n0:00:00 \u2013 Week 8 \u2013 Practicum\n\nPRACTICUM: http://bit.ly/pDL-en-08-3\nIn this section, we discussed a specific type of generative model called Variational Autoencoders and compared their functionalities and advantages over Classic Autoencoders. We explored the objective function of VAE in detail, understanding how it enforced some structure in the latent space. Finally, we implemented and trained a VAE on the MNIST dataset and used it to generate new samples.\n0:02:35 \u2013 Autoencoders (AEs) vs. variational autoencoders (VAEs)\n0:16:37 \u2013 Understanding the VAE objective function\n0:31:33 \u2013 Notebook example for variational autoencoder", "author": "Alfredo Canziani", "keywords": ["Deep Learning", "Yann LeCun", "autoencoder", "over-complete", "generative", "variational autoencoder", "posterior", "prior", "KL divergence", "relative entropy", "PyTorch"], "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw", "length": 3485, "views": 18382, "publish_date": "11/02/2022", "timestamp": 1589932800, "entity": "Yann LeCun", "transcript": {"text": " Okay. Oh, 97. Yes, almost 100. Come on, three more, please. I should invite my mom too. She hacked this morning conversation. That was funny. How the heck she managed to hack so only God knows. Yeah, don't join with the two devices, just increase the number. Yeah, 100. Yeah, 101. Okay. It's like the dogs. All right. So let's get back to the auto encoders that we have started. Oh, well, not auto encoders, generative models, right? And so let's restart by having a quick review about the auto encoders, right? So again, we have an input at the bottom in pink as now you can see the colors. Then you have the rotation, the affine transformation, and then you get the hidden layer. Again, another rotation, and then you get the final output, which we are going to be trying to enforce to be close to be similar to the input. Again, you have a parallel kind of diagram where each transformation is represented with a box, right? And so in this case, people call this network a two layer neural net because there are two transformations. But what I actually, you know, advocate is that this is a three layer neural net because for me, the layers are the activations. That's kind of what is usually the definition. And then yeah, now uses that new kind of symbols that look like a box with a round top. Okay. All right. So we have two different diagrams here because we can switch back and forth between the representations. Sometimes it's easier to use the left one when we want to talk about the single neurons, but then sometimes we prefer to use the other one, which can also like, you know, account for multiple layers. So each like block here, like the encoder and the decoder can be several layers as well. So again, these are two macro modules, I guess. And so the input gets goes inside an encoder, which gives us a code. So H, which was before the hidden representation of a neural net. When we talk about, you know, auto encoders, H is called code. And therefore we have an encoder, which is encoding the input into this code. And then we have a decoder, which is decoding the code into whatever representation in this case is, is the similar, is the same representation as the input. Okay. So on the right hand side, you have an auto encoder on the left hand side, you're going to be seen what is a variational auto encoder. All right. So there you go. Variational auto encoder. Okay. It looks the same. So what's the difference? Nothing. I was missing something. So the first difference is here that instead of having the hidden layer H, now we have the code is actually made of two things. It's made of one thing that is this kind of a capital E of Z and V of Z. And they're going to be representing soon the mean and the variance of this latent variable Z. Then we are going to be sampling from this distribution that has been parameterized by the encoder and we get to Z and Z is my latent variable, my latent representation. And then this latent representation goes inside the decoder. So the parameters that I sample from like I have a normal distribution, which has some parameters E and V, E and V are deterministically determined by the input X, but then Z is not deterministic. Z is a random variable, which you get sample from a distribution, which is parameterized by the encoder. Okay. So let's say H was of size D. Now the code here on the left hand side is going to be of size two times D, right? Because we have to represent the, all the means and then all the variances. In this case, we assume that we have, you know, D means and D variance. So each of those components are independent. Okay. All right. So we can also think about the classic autoencoder as just encoding the means. And so if you encode the mean and you have basically zero variance, you're going to get a, again, a deterministic autoencoder. So H might be in this case D and therefore on the left hand side, E and V will be total two D. Since we have D means, so does that mean we're sampling D distributions? It's going to be one multivariate Gaussian that is orthogonal. And so you have all those components that are independent from each other. And therefore Z is going to be a D dimensional vector. But then to sample a D dimensional vector from a Gaussian, you will need D means and then in this case, D variances, because we assume in that all the other components in the covariance metrics are all zeros. You only have the diagonal where you have all the variances. Okay. So here, just to make a recap, you have the encoder that is mapping this kind of input distribution into like the input set of samples into this R2D. And so we can think in this case that we map from X to the hidden representation. And then the decoder instead maps the Z space into Rn, which is back to the original space of the X. And therefore we go from lowercase Z into X hat. Someone asked if E of Z and the V of Z, is that the output of the encoder? Yeah. E of Z and V of Z are just parameters that are deterministically output by the encoder. So the encoder is a deterministic, you know, it's just the classical rotation and squashing and then another fine transformation. So it's just a piece of a neural network, which is outputting some parameters. Okay. So this is the encoder, which is giving me these parameters E and V, given my input X. Right. So this is deterministic part. Then given that we have these parameters, these parameters are, you know, giving me a Gaussian distribution with specific means and specific variances. And from these variance, from these Gaussian distribution, we sample one sample Z. Okay. And then we decode, which means, we're going to see what means this in a second. But basically you're going to be encoding the mean, and then you're going to be adding some additional, some noise, okay, to that encoding. In the noisy not encoder, we were getting our input. You were adding noise to the input and then you were trying to reconstruct the input without noise. In here, the only thing that is changed is the fact that the noise is added to the hidden representation rather than to the input. Does it make sense? Yeah, that makes a lot more sense. Thank you. So I noticed that the notation itself kind of looks like expected value. Are we generating just a normal mean from Z or are we actually computing like kind of a weighted average? No, no, there is no. Okay. So my X instead of outputting the, is outputting, let's say D is going to be 10. That is the hidden representation. Now instead of having 10 values representing the mean, we're going to have 20 values. 10 values are representing the mean and 10 values are representing the variances. Okay. So we just output a vector H here, given my X, the first half of the vector represents the means of a Gaussian distribution and the other half of the vector represents the variances for the same Gaussian distribution. Okay. So the component H, the first component H1 is going to be the mean of the first Gaussian and then the component H, let's say, okay, let's call it H2. In this case, it's going to be the variance. Then you have H3 is going to be another mean. H4 is going to be another variance and so on. Okay. So does that make, would that make Z like a 10 dimensional vector that's sampled from those? Yeah. Yeah. So Z here is going to be half of this size here, right? So the encoder gives me twice the dimension of Z and then because you get half of the dimensions, like one set of these are for the means and one set of these are the variances. Then we sample from a Gaussian that has these values. So the network simply gives me not just the means as for the classical autoencoder, but also gives me some, what is the range that I can pick things from, right? Before, when we were using the classical autoencoder here, we only have the means and then you simply decode the means. In this case, you not only have the means, but also you can have some variance, some variations across those means. Okay. So autoencoder, normal autoencoder is deterministic. The output is deterministic input function of the input. A variational autoencoder, the output is not longer deterministic. It's no longer a deterministic function of the input. It's going to be a distribution given the input, right? So it's a conditional distribution given the input. So in this case, we see that we saw a similar diagram last time where we were going from a specific point on the left hand side to the right hand side. In this case, we start here, like a point, and then we get through the encoder, you're going to get some position here, but then there is a addition of noise, right? If you only have the mean, you would get just one Z. But then given that there is some additional noise that is due to the fact that we don't have a zero variance, that final point, that final Z, it's not going to be just one point. It's going to be like a fuzzy point. Okay. So instead of having one point, now one X is going to be mapped into one region of points. Okay. So it's going to be actually taking some space. And then how do we train the system? We train the system by sending this latent variable Z back to the decoder in order to get this X, a hat. And of course, it's not going to be getting it exactly to the original point because perhaps we haven't yet trained. So we have to reconstruct the original input. And to do that, we're going to be trying to minimize what is the square distance between the reconstruction and the original input. And then we had the problem before, like to go from the latent to the input space, we need to know more the latent distribution or to enforce some distribution. Last time we were seeing that we were doing something similar when we are using the classical, the standard auto encoder. But we were going from one point X to one point Z and then back to X. Right now instead, we are going to be enforcing a distribution over these points in the latent space. Before we were going to one point, one point, one point. And then you don't know what's happening if you move around in the latent space. Remember? So if you have on the left hand side, 10 samples, you're going to have automatically on the other side, 10 latent variables. But then you don't know how to go between these inputs, between these, you don't know how to travel in this latent space because we don't know how the space behaves. Variational auto encoders enforce some structure. And they do this by adding a penalty of being different or far from a normal distribution. So if you have a latent distribution, which is not really resembling a Gaussian, then this term here will be very strong, very high. And when we train a variational auto encoder, we're going to be training it by minimizing both this term over here and this term over here. So the term on the left hand side, make sure that we can get back to the original position. The term on the right hand side enforce some structure in the latent space because otherwise we wouldn't be able to sample from there when we'd like to use this decoder as a generative model. Okay. This is maybe not too clear, but let me give you a little bit more things to think about. So how do we actually create these latent variables of z? So my z is simply going to be my mean E of z plus some noise epsilon, which is a sample from a normal distribution, which is like a normal multivariate Gaussian distribution with zero mean and identity metrics as the covariance metrics, which has each component multiplied by the standard deviation. So you should be familiar with this equation here on the top right. This is how you rescale a random variable epsilon, which again is a normal, you have to use this kind of reparameterization in order to get a Gaussian that has a specific mean and a specific variance. Okay. The noise in the latent variables is just the encoded version of the noise introduced in the input. So there is no noise in the input. You put the input inside the encoder and then the encoder gives you two parameters, E and variance. When you sample from this distribution, you basically get z and what you get here, it's simply, you can write the sampling part as this one. So the problem with sampling is that we don't know how to perform back propagation through a sampling module. Actually, there is no way to perform back propagation through sampling because this one is just generating a new z. So how do we get gradients through this module in order to train the encoder? And so this can be done if you use this trick, which is called the reparameterization trick. The reparameterization trick allows you to express your sampling in terms of additions and multiplication, which we can differentiate through. The epsilon is simply an additional input that is coming from whatever, wherever. Well, we don't have any need to send gradients through this input. The gradients will be going through the multiplication and through the addition. So whenever you have gradients for training the system, the gradient comes down. And then here we can replace the sampling module with a addition between E plus the epsilon multiplied by the square root of the variance. Such that now you have addition, you know how to back prop through an addition. Therefore, you get gradients for the encoder, here a output gradient, and then you can compute the partial derivatives of the final cost with respect to the parameters in this module. So just in an intuition part, this KL here allows me to enforce a structure in the latent space. That's what we think about. That's how I'd like you to think about this KL term. And so let's actually figure out how this stuff works. So we have two terms in my pair sample loss. We have the first one, which is the reconstruction loss, and then there is the second term, which is going to be this KL, this relative entropy term. So we have some z's in this case, which are spheres, bubbles in this case. Why there are bubbles? Because if we add some additional noise, we had the means, and the means are basically the center of these points. So you have one mean here, one mean over here, one mean over here, one mean over here. And then what the reconstruction term is going to be doing is the following. So if these bubbles overlap, what does it happen? So if you have one mean here and another mean, like one bubble here and another bubble that is overlapping, and there is a region where there is intersection, how can you reconstruct these two points later on? You can't. Are you following so far? If you have a bubble here and then you have another bubble here, all points on this bubble here will be reconstructed to the original input here. So you start from an original point, you go to the latent space over here, and then you add some noise. You actually have a volume here. Then you take another point, and this other point, it gets reconstructed here. Right now, if these two guys overlap, how can you reconstruct the points over here? So if the points are in this bubble, I'd like to go back to the original point here. If the points are in this bubble, I'd like to go to the other point. But if points are overlapped, sorry, if the bubbles are overlapped, then you can't really figure out where to go back. So then the reconstruction term will just do this. The reconstruction term will try to get all those bubbles as far as possible such that they don't overlap. Because if they overlap, then the reconstruction is not going to be good. And so now we have to fix this. So there are a few ways to fix this. How can we fix this overlapping issue? Why didn't we have this overlapping issue with the normal out encoder? Because there was no variance. And so what does it mean? Can you translate what not having a variance mean? The spheres are not spheres, but they're points. Right. So if you have just points, points will never overlap. They have to be the exact same point. But you have the exact same point only if the encoder is dead. Or you have the same input, I think. Well, it's unlikely that two points overlap. If now instead of having points you have actually volumes, well, volume can overlap because there are many infinite points in that volume. So one option is going to be kill the variance. And so you have points. And now this defeats the whole variational thing. Without this spacey thing, by killing the variance, now you don't know anymore what's happening between the points. Because if you have space, if they take volume, you can walk around in the latent space. You can always figure out where to go back. If these are points, as soon as you leave this position here, you have no whatsoever idea where to go. Anyhow, first point, we can kill the variance. Other option, well, the one I show you here, right? The other option is going to get these bubbles as far as possible. So if they are as far as possible, what's going to happen in your Python script? So if these means go very, very far, then they will increase a lot, a lot, a lot. And then the problem is that you're going to get infinite. This stuff is going to explode because all these values are trying to go as far as possible such that they don't overlap. And then that's not good. Okay. All right. So let's figure out how variational out encoder fixes this problem. Oh, could you just clarify what you mean by pushing the points apart? Like, are you putting them in a higher dimensional space to push them apart? So as they are here, so each, if you don't have the variance, all those circles here, all those bubbles here are just points. Given that we have some variance, they will take some space. Now, if this space taken by two bubbles overlaps with another bubble, the reconstruction error will increase because you have no idea how to go back to the original point that generated that sphere. And so the network, the encoder has two options in order to reduce this reconstruction error. One option is going to be to kill the variance such that you get points. The other option is going to be to send all those points in any direction such that they don't overlap. Okay. Okay. Yeah, that makes sense. Okay. Cool. So reconstruction error gets this stuff to fly around, but then let's introduce the second term. So I would really recommend you to compute these relative entropy between the Gaussian and a normal distribution such that you can practice maybe for next week. But then if you compute that relative entropy, you get this stuff, right? You get several, four terms basically. And everyone should understand how this looks. No, okay. I'm just joking. I'm going to be actually explaining that. Okay. So we have this expression. Let's try to analyze a little bit in more detail what these terms represent. So the first term, you have these variance minus log variance minus one. So if we graph it, it looks like this. You have a linear function, right? After two on the X axis. And then on the other terms, you subtract a logarithm, which goes to plus infinity. If you sum a minus logarithm, it goes to plus infinity at zero. And then otherwise it's going to be just decay. So if you sum the two and subtract one, you get this kind of cute function. And if you minimize this function, you get just one. And therefore this shows you how this term enforces those spheres here to have a radius of one in each direction. Because if it tries to be smaller than one, this stuff goes up as crazy. And if it increases here, it doesn't go as up as crazy. So they are slightly, roughly always at least one or half, but they won't be as much smaller because this stuff increases a lot. So in this case, we have enforced the network not to collapse these bubbles nor to make it grow them too much, right? Because otherwise they still get penalized here. So then we have another term here, this E of Z everything squared and that's classical parabola, which has a minimum over there. And so this term here basically says that all the means should be condensed towards zero. And so basically you get like this additional force here by this purple side. And then you get that all those bubbles get squashed together into this bigger bubble. So here you get the bubbles of bubbles representation of a variational autoencoder. How cute is this? Very cute, right? How can you pack more bubbles? So what is the only parameter here, which is telling you the strength of your variational autoencoder? It's going to be simply the dimension D because given a dimension, you always know how many bubbles you can pack in a larger bubble, right? So it's just a function of the dimension you pick and you choose for your hidden layer. So is the reconstruction last the first term, the yellow term? Is that the one that actually pushes the bubbles further apart and then the rest of it is what kind of keeps them from doing that? So the reconstruction would push things around because we have these additional taking volumes thing, right? So if we wouldn't be taking volumes, the reconstruction term wouldn't be pushing anything away because they don't overlap. Given that we actually have some variants, the variants will have these points actually taking some volume and therefore this reconstruction will try to get those points away. So if you check again those few animations I show you. So we had at the beginning, those were the points with additional noise. Now you get the reconstruction that is pushing everything away. Then you get the variance that is assuring you that those little bubbles don't collapse. And then you have the final term, which is the spring term because it's the quadratic term in the loss, which is basically adding this additional pressure such that all the little guys get packed towards zero. But they don't overlap because there is the reconstruction term. So no overlap due to the reconstruction. Size not going to small than one because of the first part of the relative entropy. And then all these guys are packed again for the quadratic part, which is the spring force. Is the beta term something that needs to be tuned like a hyperparameter kind of thing? So the beta is the actual, in the original version of this variational encoder, there was no beta. And then there is a paper which is the beta variational encoder. Just to say that you can use a hyperparameter to change how much these two terms contribute for the final loss. This loss, the second loss term with the beta, that's the KL divergence. Yeah. Between the Z and the normal distribution, right? Yeah, between the Z, which is coming from a Gaussian of mean E and variance V. And then the second term is going to be this normal distribution. And so this term tries to get Z to be as close as possible to a normal distribution in the space, the dimensional space. Okay. And this formula that you're working down, that's a generic KL language formula. So I would recommend you to take a paper and pen and then try to write the relative entropy between a Gaussian and a normal distribution. And then you should get all these terms. The relative entropy. So yeah, this LKL is the relative entropy. Yeah. So just look up the formula for the relative entropy, which is telling you basically how far two distributions are. And the first distribution is going to be a multivariate Gaussian. And the second one is going to be a normal distribution, right? Are the normal distributions not the same thing? The Gaussian has a mean vector and the covariance matrix. The normal has zero mean and identity matrix for the covariance matrix. We said earlier though that the Z should not have covariance. It should be diagonal, right? Yeah. So it's going to be diagonal, but the values on the diagonal are those V variances. It's an off-centered big normal versus a centered small normal? So it's off-center and then each direction is scaled by the standard deviation of that dimension. So if you have a large standard deviation in one dimension, it means that in that direction it's very, very spread. Make sense? But there is a line. It's a line on the D axis, right? Because again, all the components are independent. Yeah. Is the reconstruction loss the pixel-wise distance between the final out and the original image? The reconstruction loss, we saw that last week and we have two options for the reconstruction loss. One was the binary for binary data and we have the binary cross entropy. And the other one is going to be instead the real value one. So it's such that you can use the half or well, the MSC, right? So these are the reconstruction losses we can use. For example, you talk more with me than with Jan. Good. No, well, not good. You should talk as well with Jan, but we should be going over the notebook such that we can see how to code the stuff and also play with the distributions. Because before, again, the main point was that before we were mapping points to points and back to points, right? Right now instead you're going to map points to space and then space to points. But then also all the space now is going to be all covered by these bubbles because of several factors, right? If you have some space between these bubbles, then you have no idea how to go from this region here back to the input space, right? Instead of variation of the encoder gets you to this very well behaved coverage, you know, this nice coverage of the latent space. Okay. Good. I can't see you. I miss you guys. Okay. So code or are there questions so far? I hope you can see stuff. Just give feedback. Can you see stuff? Yep. Yes. Yep. All right. So work, get the PDL, conda, activate PDL, Jupyter notebook. Boom. Okay. So I'm going to be covering now the VAE. And so now I'm going to just execute everything such that this stuff starts training and then I'm going to be explaining things. All right. So at the beginning, I'm going to be just import in our random sheet as usual. Then I have a display routine. We don't care. Don't add it to the notes. I have some default values for the random sheets such that you're going to get the same numbers I get. Then here I just use the MNIST data set, the modify NIST from YARN device. I set the CPU or GPU. In theory, I could have used GPU because my Mac here actually has a GPU. And then I have my variation out encoder. Okay. So my variation out encoder has two parts, has an encoder here. Let me turn on the line numbers. So my encoder goes from 784, which is the size of the input to D square, for example. And D in this case is 20, so 400. And then from D square, I go to two times T, which is going to be half of my means and half is going to be for my sigma squares for my variances. The other case, the decoder instead picks only D, right? You can see D here. We go from D to D square and then from D square to 784 such that we match the input dimensionality. And then finally, I have a sigmoid. Why do I have a sigmoid? Because my input is going to be limited from zero to one. There are images from zero to one. Then there is a module here, which is called reparameterize. And if we are training, we use these reparameterization part. Sorry, could you just say again why you use the sigmoid in the decoder? Yeah, because my data is living between zero and one. So I have those digits from the MNIST and they are values like the values of the digits are going to be from zero to one. So I like to have my network, this module here outputs things that goes from minus infinity to plus infinity. If I send it through a sigmoid, this stuff sends things through like zero to one. When you say the values of the digits, you mean the deactivations, right? So I use the MNIST dataset and this is going to be both my input and also my targets, right? The images and the values of these images will be ranging between zero to one. Like it's a real value. Each pixel can be between zero and one. Yeah. I think actually the inputs are binary. So the inputs are all zero or one. But my network will be outputting a real range between zero and one. So reparameterization, we have the, what do we do here? So reparameterization given a mu and a log variance, I'll explain later why we use log variance. If you are in training, we compute standard deviation as going to be log variance multiplied by one half. And then I take the exponential. And so I get the standard deviation from the log variance. And then I get my epsilon, which is simply sampled from a normal distribution, which with whatever size I have here, right? So standard deviation, I get the size, I create a new tensor and I fill it with a normal distribution data. Then I return the epsilon, which applies by the standard deviation and I add the mu, which is what I showed you before. If I am not training, I don't have to add noise, right? So I can simply return my mu. So I use this network in a deterministic way. The forward mode is the following. So here we have that the encoder gets the input, which is going to be reshaped into these things, such that basically I unroll the images into a vector. Then the encoder is going to be outputting something. And then I reshape that one such that I have batch size two and then D, where D is the dimension of the mean and the dimension of the variances. Then I have mu, which is the mean, simply the first part, right, of these guys, of this D. And then the log variance is going to be the other guy. And then I have my z, which is going to be my latent variable. It's going to be this reparameterization given my mu and the log var. Why do I use a log var? You tell me. Why do I use a log var? The output of networks can be negative, so you need positive. Right, right. So given that the variances are only positive, if I compute the log, it allows you to output the full real range for the encoder, right? So you can use the whole real range. And then I define my model as this VAE and I send it to the device. Here I define the optimization optimizer. And then I define my loss function, which is the sum of two parts, the binary cross-entropy between the input and the reconstruction, which is linear. So I have the x hat and then the x. And then I try, I sum all of them. And then the KL divergence. So we have the var, which is the linear. Then you have the minus log var, which is the logarithmic flip down, and then minus one. And then we have the mu. And then we try to minimize this stuff, right? All right. So training scripts. It's very simple, right? So you have the model, which is outputting the prediction x hat. Let's see here, right? Forward outputs the output of the decoder, the mu and the log var. So here you get the model. You feed the input. You get x hat, mu, log var. You can compute the loss using the x hat, x mu, and log var. X being the input, but also the target. And then we add the item to the loss. We clean up the gradients from the previous steps, perform computation, compute the partial derivatives, and then you step. And then here I just do the testing and do some caching for later on. So we started with initial error of 500, roughly 540. This is before training. And then it goes immediately down to 200 and then goes down to 100. And so now I'm going to be showing you a few of the results. This is the input I feed to the network. And the untrained network reconstructions, of course, look like shit, right? But okay, that's fine. So we can keep going. And that's going to be the first epoch, right? Cool. Second epoch, third, fourth, and so on. And they look better and better, of course. So what can we do right now? A few things we can do. For example, now, we can simply sample z from a normal distribution. And then I decode this random stuff, right? So this doesn't come from an encoder. And I show you now what the decoder does whenever you sample from the distribution that the latent variable should have been following. And so these are a few examples of how sampling from the latent distribution, you know, gets decoded into something. We got a nine here, we got a zero, we got some five. So some of the regions are very well defined, nine, two. But then other regions like this thing here, or this thing here, or the number 14 here, they don't really look like well, like digits. This is because why? What's the problem here? We haven't really covered the whole space. I just trained for one minute. If I train for 10 minutes, it's going to be just working perfectly. Okay. So here, those bubbles don't yet fill the whole space, right? And that's the same problem which you would have with a normal autoencoder without this variational thing, right? With a normal autoencoder, you don't have any kind of structure, any kind of defined behavior in the regions between different points. With a variational autoencoder, we actually take the space and enforce that the reconstruction of the all these regions actually makes sense. Okay. So let's do some cute stuff. And then I am done. Here, I just show you a few digits. And so let's pick two of them. For example, let's pick three and eight, which is going to be, let me show you here. So we'd like to find an interpolation now between a five and a four. Okay. And this is my five reconstructed and our four reconstructed. So if I perform a linear interpolation in the latent space and I then send it to the decoder, we get this one. So the five gets morphed into a four. Can you see? Slowly. But it looks like crap. Let's try to get something that stays on the manifold. So let's get, for example, these three. So it's going to be number one. Number one. And then let's say maybe these 14 here. So I do interpolation of these guys here. You can see my autoencoder actually fixed those kind of issues here. And then you can see now how the three gets those little edges closed to look like an eight. Right? And so all of them look like kind of legit, you know, this is kind of a three, kind of a three, a three that became an eight. Right. And so you can see how now by walking in the latent space, we get to reconstruct things that look legit in the input space. Right. This would have never worked with a normal autoencoder. Finally, I'm going to show you a few nice representation of the embeddings of the means for this train autoencoder. So here I just show you a collection of the embeddings of, you know, the test dataset. And then I perform a dimensionality reduction. And then I show you how the encoder clusters all the means in different regions in the latent space. And so here is what you get when you train this variational autoencoder. So this is the beginning when the network is not trained. You can still see, you know, clusters of digits. But then as you keep training, well, at least, you know, after five epochs, you get these groups to be, you know, separated. And then I think if you keep training more, you should have like more separation. Okay. So here I'm basically doing the testing part. I get all the means. So my model outputs X hat, mu and log bar. And so my means, I append them, I append all my mu's into this mean list. I append all the log bars in this log bars list. And I append all the y's to this labels list during the testing part. So this is testing. And so I have like a list here of codes, which is I have the mu, log bar, and then the y's. So here later on, I put those lists inside my dictionary. And then later here below, I compute a dimensionality reduction for epoch zero, epoch five, and epoch 10. So I use this TSNI, which is a technique for reducing the dimensions of the codes, which are 20. Right now the dimensionality is 20. So I fit, I get my X is going to be, let's say the first thousand components, first thousand samples of the means. And then I get these E's, which are basically a 2D projection somehow of these 20 dimensional mu's. And then I show you in this chart here, how these 2D projections, they look at epoch zero before training the network, because this one is before the first training epoch, and then at epoch five. And you can see how the network gets all this mass here to be kind of more nicely put. Here, I didn't visualize the variances. I'm thinking whether I can, if I'm able to do that as well, I'm not sure. So each of these points represent the location of the mean after training the variation of the encoder. I haven't represented the area that these means are actually taking. Okay. Aren't the means supposed to be random at epoch zero? The randomness is in the encoder, right? But then you still feed to the encoder and those input digits. So the input digits, all the ones are kind of similar, right? So if you perform a random transformation of those similarly looking initial vectors, you're going to have similarly looking transformed versions. But then they are not necessarily grouped altogether. Like most of them are, for example, let's say these are ones. Let me turn on the color bar so we can see what this stuff is. So let's say these are the zeros, these over here. So all zeros look like, they all look similar. Therefore, even a random projection of those zeros will all be kind of together. What you can see instead is going to be this purple is all spread around, right? So it means the force, there are very many ways of drawing a four. Someone closes the top, someone doesn't. So if you see on the right hand side instead, all the fours are almost all here, right? There is just a little cluster here next to the nine because you can think about if you write a four like that, it's very similar to write a nine, right? And so you have these fours here that are very close to the nines just because of how people drew this specific force. Nevertheless, they are still clustered. Over here, you get all these things are spread around. So this is very bad. Nevertheless, they tell you this diagram here shows you that there is very little variance across the drawing of a zero. So it shows you somehow there is a specific mode that is very concentrated here but it's really not concentrated for these guys. So I'm just curious, what are some other motivations or usages of variational autoencoder? Like why would that be useful? So the main point was that whenever I show you in class two weeks ago, a generative model, you cannot have a generative model with a classical autoencoder. In this case here, again, I didn't train this stuff a lot. If you train it longer, you can have better performance. Here the point is that my input, z, comes from just this random distribution. And then by sending this random number here, a number coming from a normal distribution, you send it inside this decoder. If this decoder is actually a powerful decoder, then this stuff will actually draw very nice shapes or numbers. Like, for example, those two images I show you of the two faces in the first part of the class, the last time, those are simply you take a number from a random distribution, you feed it to a decoder, and the decoder is going to be drawing you this very beautiful picture of whatever you train this decoder on. And you cannot use a standard autoencoder to get these kind of properties because again, here we enforce the decoder to reconstruct meaningful or good looking reconstruction when they are sampled from this normal distribution. Therefore, later on, we can sample from this normal distribution, feed things to the decoder, and the decoder will generate stuff that looks like legit. If you didn't train the decoder in order to perform a good reconstruction when you sample from this normal distribution, you wouldn't be able to actually get anything meaningful. That's the big takeaway here. Next time, we're going to be seeing generative adversarial networks and how they are very similar to this stuff we have seen today. Hi, Alfredo. I have a question for the yellow bubble. Yellow bubble, yeah. Yeah, so each yellow bubble comes from one input example. So if we have 1,000, I don't know, images or 1,000 inputs, that means we have 1,000 exactly yellow bubbles. And each yellow bubble, it comes from the easy distribution together with the noise added to latent variable. So the bubble comes from here. Let me show you. Should I show you this one? It's okay? Or should I show you the slides? Yeah, yeah, it's okay. Okay. So here you get this X and this X goes inside the model, right? Whenever you send this X through the model, it goes inside forward. So X goes inside here and then it goes inside the encoder, right? And then from the encoder gives me this mu logvar term from which I just extract the mu and logvar, okay? So far everything is like a normal autoencoder. The bubble comes here. So my Z now comes out from these self-reparameterized. And this self-reparameterized is going to be working in a different way if we are in the training loop or we are not in the training loop. So if we are not in the training loop, I just return the mean. So there is no bubble when I use the testing part, okay? So I get the best value the encoder can give me. If I am training instead, this is what happens. So I compute the standard deviation from this logvar. So I get the logvar, I divide it by two and then I take the exponential, right? So I have e to the one half logvar such that you get the standard deviation. And then the epsilon is going to be simply a d-dimensional vector sampled from a normal distribution. And so this one is one sample coming from this normal distribution. And the normal distribution is like a sphere in d dimensions, right? A sphere with the radius, which is going to be square root of d. But then, so here at the end, you simply resize that thing. The point is that every time you call this reparameterization, reparameterize function, you're going to get a different epsilon because epsilon is sampled from a normal distribution, right? So given a mu and given a logvar, you're going to be getting every time different epsilons. And therefore, this stuff here, if you call it 100 times, is going to give you 100 different points, all of them clustered in mu with a radius of roughly standard deviation. And so this is the line which returns you every time just one sample. But if you call this in a for loop, you're going to get a cloud of points, all of them centered in mu, which has a specific radius. And so this is where we get these bubbles. Come from the sampling of this thing, right? But I have to run it 100 times. If you want 100 samples, you have to run it 100 times. This reparameterization gives you every time a different point, which is parameterized by this location and this kind of volume. Yeah, and this comes from the mu and log variance comes from one sample, one input example. Yeah, yeah, yeah. So my one input x here gives me one mu and gives me one logvar. And this one mu and one logvar gives me z, which is one sample from the whole distribution. If you run this function here 1000 times, you're going to get 1000 z's, which all of them will take this volume, right? Okay. Got it. Got it. Thank you. Of course. I had a question about encoders and decodes in general. It looks like in this implementation, it's fairly straightforward in terms of it just has a couple linear layers with a ReLU and a sigmoid. I've previously seen encoders where they're using attention and all this stuff. Is this something as basic as this? It seems like it's pretty satisfactory. Are they usually this basic or more complex? Okay, okay. That was, I think, softball for me. So everything we see in class is things that I've tried, it works, and it's fairly representative of what is sufficient to get this stuff to run. So I'm running on my laptop on the MNIST dataset. You can run several of these kind of tests and play. And so today we have seen how you can encode, how can you code up a variation autoencoder. And all you need is three lines, four lines of code, which are like, what are the differences between the plain autoencoder, right? And so the difference is you have the reparameterization, reparameterized module method here. And then just these three lines over here, right? So you have six lines plus the relative entropy. The architecture, that's completely different. So it's completely orthogonal, right? One thing is going to be the architecture, which is based on the current input. You can use a convolutional net, you can use a recurrent net, you can use anything you want. And the other thing is the fact that you convert some deterministic network into a network that allows you to sample and then generate samples from a distribution. So we never had to talk about distributions before. We didn't know how to generate distributions. Now with generative model, you can actually generate data, which are basically a, you know, how do you say, like a bending, a rotation or a transformation of whatever is original Gaussian, right? So we have this multivariate Gaussian and then the decoder takes this ball and then it shapes it to make it look like the input. The input may be like something curved. You have this bubble here, this big bubble of bubbles. And then you, the decoder gets it back to whatever it looks like, how the input looks like. So all you need depends on the specific data you're using. For MNIST, this is sufficient. If you're using a convolutional version, it's going to be working much better. The point is that this class was about variational autencoder, know how to get crazy stuff. All the crazy stuff is simply, you know, adding several of these things I've been teaching you so far. But the bit about variational autencoder, I think it was covered mostly here. Okay. Okay. Thanks. Other questions? No. Okay. Okay. That was it. Okay. Thank you so much for joining us. Okay. Everyone almost left 70%. See you next week. Bye. Bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.040000000000001, "text": " Okay. Oh, 97. Yes, almost 100. Come on, three more, please. I should invite my mom too.", "tokens": [50364, 1033, 13, 876, 11, 23399, 13, 1079, 11, 1920, 2319, 13, 2492, 322, 11, 1045, 544, 11, 1767, 13, 286, 820, 7980, 452, 1225, 886, 13, 50916, 50916, 1240, 36218, 341, 2446, 3761, 13, 663, 390, 4074, 13, 1012, 264, 12872, 750, 6453, 281, 10339, 51194, 51194, 370, 787, 1265, 3255, 13, 865, 11, 500, 380, 3917, 365, 264, 732, 5759, 11, 445, 3488, 264, 1230, 13, 865, 11, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.27199844464863815, "compression_ratio": 1.3678756476683938, "no_speech_prob": 0.30984610319137573}, {"id": 1, "seek": 0, "start": 11.040000000000001, "end": 16.6, "text": " She hacked this morning conversation. That was funny. How the heck she managed to hack", "tokens": [50364, 1033, 13, 876, 11, 23399, 13, 1079, 11, 1920, 2319, 13, 2492, 322, 11, 1045, 544, 11, 1767, 13, 286, 820, 7980, 452, 1225, 886, 13, 50916, 50916, 1240, 36218, 341, 2446, 3761, 13, 663, 390, 4074, 13, 1012, 264, 12872, 750, 6453, 281, 10339, 51194, 51194, 370, 787, 1265, 3255, 13, 865, 11, 500, 380, 3917, 365, 264, 732, 5759, 11, 445, 3488, 264, 1230, 13, 865, 11, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.27199844464863815, "compression_ratio": 1.3678756476683938, "no_speech_prob": 0.30984610319137573}, {"id": 2, "seek": 0, "start": 16.6, "end": 24.400000000000002, "text": " so only God knows. Yeah, don't join with the two devices, just increase the number. Yeah,", "tokens": [50364, 1033, 13, 876, 11, 23399, 13, 1079, 11, 1920, 2319, 13, 2492, 322, 11, 1045, 544, 11, 1767, 13, 286, 820, 7980, 452, 1225, 886, 13, 50916, 50916, 1240, 36218, 341, 2446, 3761, 13, 663, 390, 4074, 13, 1012, 264, 12872, 750, 6453, 281, 10339, 51194, 51194, 370, 787, 1265, 3255, 13, 865, 11, 500, 380, 3917, 365, 264, 732, 5759, 11, 445, 3488, 264, 1230, 13, 865, 11, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.27199844464863815, "compression_ratio": 1.3678756476683938, "no_speech_prob": 0.30984610319137573}, {"id": 3, "seek": 2440, "start": 24.4, "end": 33.839999999999996, "text": " 100. Yeah, 101. Okay. It's like the dogs. All right. So let's get back to the auto encoders", "tokens": [50364, 2319, 13, 865, 11, 21055, 13, 1033, 13, 467, 311, 411, 264, 7197, 13, 1057, 558, 13, 407, 718, 311, 483, 646, 281, 264, 8399, 2058, 378, 433, 50836, 50836, 300, 321, 362, 1409, 13, 876, 11, 731, 11, 406, 8399, 2058, 378, 433, 11, 1337, 1166, 5245, 11, 558, 30, 400, 370, 718, 311, 21022, 51190, 51190, 538, 1419, 257, 1702, 3131, 466, 264, 8399, 2058, 378, 433, 11, 558, 30, 407, 797, 11, 321, 362, 364, 4846, 412, 264, 2767, 51492, 51492, 294, 7022, 382, 586, 291, 393, 536, 264, 4577, 13, 1396, 291, 362, 264, 12447, 11, 264, 2096, 533, 9887, 11, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.18075414535102494, "compression_ratio": 1.6422413793103448, "no_speech_prob": 5.303006764734164e-05}, {"id": 4, "seek": 2440, "start": 33.839999999999996, "end": 40.92, "text": " that we have started. Oh, well, not auto encoders, generative models, right? And so let's restart", "tokens": [50364, 2319, 13, 865, 11, 21055, 13, 1033, 13, 467, 311, 411, 264, 7197, 13, 1057, 558, 13, 407, 718, 311, 483, 646, 281, 264, 8399, 2058, 378, 433, 50836, 50836, 300, 321, 362, 1409, 13, 876, 11, 731, 11, 406, 8399, 2058, 378, 433, 11, 1337, 1166, 5245, 11, 558, 30, 400, 370, 718, 311, 21022, 51190, 51190, 538, 1419, 257, 1702, 3131, 466, 264, 8399, 2058, 378, 433, 11, 558, 30, 407, 797, 11, 321, 362, 364, 4846, 412, 264, 2767, 51492, 51492, 294, 7022, 382, 586, 291, 393, 536, 264, 4577, 13, 1396, 291, 362, 264, 12447, 11, 264, 2096, 533, 9887, 11, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.18075414535102494, "compression_ratio": 1.6422413793103448, "no_speech_prob": 5.303006764734164e-05}, {"id": 5, "seek": 2440, "start": 40.92, "end": 46.959999999999994, "text": " by having a quick review about the auto encoders, right? So again, we have an input at the bottom", "tokens": [50364, 2319, 13, 865, 11, 21055, 13, 1033, 13, 467, 311, 411, 264, 7197, 13, 1057, 558, 13, 407, 718, 311, 483, 646, 281, 264, 8399, 2058, 378, 433, 50836, 50836, 300, 321, 362, 1409, 13, 876, 11, 731, 11, 406, 8399, 2058, 378, 433, 11, 1337, 1166, 5245, 11, 558, 30, 400, 370, 718, 311, 21022, 51190, 51190, 538, 1419, 257, 1702, 3131, 466, 264, 8399, 2058, 378, 433, 11, 558, 30, 407, 797, 11, 321, 362, 364, 4846, 412, 264, 2767, 51492, 51492, 294, 7022, 382, 586, 291, 393, 536, 264, 4577, 13, 1396, 291, 362, 264, 12447, 11, 264, 2096, 533, 9887, 11, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.18075414535102494, "compression_ratio": 1.6422413793103448, "no_speech_prob": 5.303006764734164e-05}, {"id": 6, "seek": 2440, "start": 46.959999999999994, "end": 52.12, "text": " in pink as now you can see the colors. Then you have the rotation, the affine transformation,", "tokens": [50364, 2319, 13, 865, 11, 21055, 13, 1033, 13, 467, 311, 411, 264, 7197, 13, 1057, 558, 13, 407, 718, 311, 483, 646, 281, 264, 8399, 2058, 378, 433, 50836, 50836, 300, 321, 362, 1409, 13, 876, 11, 731, 11, 406, 8399, 2058, 378, 433, 11, 1337, 1166, 5245, 11, 558, 30, 400, 370, 718, 311, 21022, 51190, 51190, 538, 1419, 257, 1702, 3131, 466, 264, 8399, 2058, 378, 433, 11, 558, 30, 407, 797, 11, 321, 362, 364, 4846, 412, 264, 2767, 51492, 51492, 294, 7022, 382, 586, 291, 393, 536, 264, 4577, 13, 1396, 291, 362, 264, 12447, 11, 264, 2096, 533, 9887, 11, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.18075414535102494, "compression_ratio": 1.6422413793103448, "no_speech_prob": 5.303006764734164e-05}, {"id": 7, "seek": 5212, "start": 52.12, "end": 57.4, "text": " and then you get the hidden layer. Again, another rotation, and then you get the final", "tokens": [50364, 293, 550, 291, 483, 264, 7633, 4583, 13, 3764, 11, 1071, 12447, 11, 293, 550, 291, 483, 264, 2572, 50628, 50628, 5598, 11, 597, 321, 366, 516, 281, 312, 1382, 281, 24825, 281, 312, 1998, 281, 312, 2531, 281, 264, 4846, 13, 51026, 51026, 3764, 11, 291, 362, 257, 8952, 733, 295, 10686, 689, 1184, 9887, 307, 10379, 365, 51334, 51334, 257, 2424, 11, 558, 30, 400, 370, 294, 341, 1389, 11, 561, 818, 341, 3209, 257, 732, 4583, 18161, 2533, 570, 51570, 51570], "temperature": 0.0, "avg_logprob": -0.1674624487411144, "compression_ratio": 1.6682242990654206, "no_speech_prob": 1.6435467841802165e-05}, {"id": 8, "seek": 5212, "start": 57.4, "end": 65.36, "text": " output, which we are going to be trying to enforce to be close to be similar to the input.", "tokens": [50364, 293, 550, 291, 483, 264, 7633, 4583, 13, 3764, 11, 1071, 12447, 11, 293, 550, 291, 483, 264, 2572, 50628, 50628, 5598, 11, 597, 321, 366, 516, 281, 312, 1382, 281, 24825, 281, 312, 1998, 281, 312, 2531, 281, 264, 4846, 13, 51026, 51026, 3764, 11, 291, 362, 257, 8952, 733, 295, 10686, 689, 1184, 9887, 307, 10379, 365, 51334, 51334, 257, 2424, 11, 558, 30, 400, 370, 294, 341, 1389, 11, 561, 818, 341, 3209, 257, 732, 4583, 18161, 2533, 570, 51570, 51570], "temperature": 0.0, "avg_logprob": -0.1674624487411144, "compression_ratio": 1.6682242990654206, "no_speech_prob": 1.6435467841802165e-05}, {"id": 9, "seek": 5212, "start": 65.36, "end": 71.52, "text": " Again, you have a parallel kind of diagram where each transformation is represented with", "tokens": [50364, 293, 550, 291, 483, 264, 7633, 4583, 13, 3764, 11, 1071, 12447, 11, 293, 550, 291, 483, 264, 2572, 50628, 50628, 5598, 11, 597, 321, 366, 516, 281, 312, 1382, 281, 24825, 281, 312, 1998, 281, 312, 2531, 281, 264, 4846, 13, 51026, 51026, 3764, 11, 291, 362, 257, 8952, 733, 295, 10686, 689, 1184, 9887, 307, 10379, 365, 51334, 51334, 257, 2424, 11, 558, 30, 400, 370, 294, 341, 1389, 11, 561, 818, 341, 3209, 257, 732, 4583, 18161, 2533, 570, 51570, 51570], "temperature": 0.0, "avg_logprob": -0.1674624487411144, "compression_ratio": 1.6682242990654206, "no_speech_prob": 1.6435467841802165e-05}, {"id": 10, "seek": 5212, "start": 71.52, "end": 76.24, "text": " a box, right? And so in this case, people call this network a two layer neural net because", "tokens": [50364, 293, 550, 291, 483, 264, 7633, 4583, 13, 3764, 11, 1071, 12447, 11, 293, 550, 291, 483, 264, 2572, 50628, 50628, 5598, 11, 597, 321, 366, 516, 281, 312, 1382, 281, 24825, 281, 312, 1998, 281, 312, 2531, 281, 264, 4846, 13, 51026, 51026, 3764, 11, 291, 362, 257, 8952, 733, 295, 10686, 689, 1184, 9887, 307, 10379, 365, 51334, 51334, 257, 2424, 11, 558, 30, 400, 370, 294, 341, 1389, 11, 561, 818, 341, 3209, 257, 732, 4583, 18161, 2533, 570, 51570, 51570], "temperature": 0.0, "avg_logprob": -0.1674624487411144, "compression_ratio": 1.6682242990654206, "no_speech_prob": 1.6435467841802165e-05}, {"id": 11, "seek": 7624, "start": 76.24, "end": 82.16, "text": " there are two transformations. But what I actually, you know, advocate is that this is a three", "tokens": [50364, 456, 366, 732, 34852, 13, 583, 437, 286, 767, 11, 291, 458, 11, 14608, 307, 300, 341, 307, 257, 1045, 50660, 50660, 4583, 18161, 2533, 570, 337, 385, 11, 264, 7914, 366, 264, 2430, 763, 13, 663, 311, 733, 295, 437, 307, 50936, 50936, 2673, 264, 7123, 13, 400, 550, 1338, 11, 586, 4960, 300, 777, 733, 295, 16944, 300, 574, 411, 51278, 51278, 257, 2424, 365, 257, 3098, 1192, 13, 1033, 13, 1057, 558, 13, 407, 321, 362, 732, 819, 36709, 510, 570, 51600, 51600, 321, 393, 3679, 646, 293, 5220, 1296, 264, 33358, 13, 4803, 309, 311, 3571, 281, 764, 264, 1411, 472, 562, 51825, 51825], "temperature": 0.0, "avg_logprob": -0.1597122018987482, "compression_ratio": 1.6654676258992807, "no_speech_prob": 6.238549303816399e-06}, {"id": 12, "seek": 7624, "start": 82.16, "end": 87.67999999999999, "text": " layer neural net because for me, the layers are the activations. That's kind of what is", "tokens": [50364, 456, 366, 732, 34852, 13, 583, 437, 286, 767, 11, 291, 458, 11, 14608, 307, 300, 341, 307, 257, 1045, 50660, 50660, 4583, 18161, 2533, 570, 337, 385, 11, 264, 7914, 366, 264, 2430, 763, 13, 663, 311, 733, 295, 437, 307, 50936, 50936, 2673, 264, 7123, 13, 400, 550, 1338, 11, 586, 4960, 300, 777, 733, 295, 16944, 300, 574, 411, 51278, 51278, 257, 2424, 365, 257, 3098, 1192, 13, 1033, 13, 1057, 558, 13, 407, 321, 362, 732, 819, 36709, 510, 570, 51600, 51600, 321, 393, 3679, 646, 293, 5220, 1296, 264, 33358, 13, 4803, 309, 311, 3571, 281, 764, 264, 1411, 472, 562, 51825, 51825], "temperature": 0.0, "avg_logprob": -0.1597122018987482, "compression_ratio": 1.6654676258992807, "no_speech_prob": 6.238549303816399e-06}, {"id": 13, "seek": 7624, "start": 87.67999999999999, "end": 94.52, "text": " usually the definition. And then yeah, now uses that new kind of symbols that look like", "tokens": [50364, 456, 366, 732, 34852, 13, 583, 437, 286, 767, 11, 291, 458, 11, 14608, 307, 300, 341, 307, 257, 1045, 50660, 50660, 4583, 18161, 2533, 570, 337, 385, 11, 264, 7914, 366, 264, 2430, 763, 13, 663, 311, 733, 295, 437, 307, 50936, 50936, 2673, 264, 7123, 13, 400, 550, 1338, 11, 586, 4960, 300, 777, 733, 295, 16944, 300, 574, 411, 51278, 51278, 257, 2424, 365, 257, 3098, 1192, 13, 1033, 13, 1057, 558, 13, 407, 321, 362, 732, 819, 36709, 510, 570, 51600, 51600, 321, 393, 3679, 646, 293, 5220, 1296, 264, 33358, 13, 4803, 309, 311, 3571, 281, 764, 264, 1411, 472, 562, 51825, 51825], "temperature": 0.0, "avg_logprob": -0.1597122018987482, "compression_ratio": 1.6654676258992807, "no_speech_prob": 6.238549303816399e-06}, {"id": 14, "seek": 7624, "start": 94.52, "end": 100.96, "text": " a box with a round top. Okay. All right. So we have two different diagrams here because", "tokens": [50364, 456, 366, 732, 34852, 13, 583, 437, 286, 767, 11, 291, 458, 11, 14608, 307, 300, 341, 307, 257, 1045, 50660, 50660, 4583, 18161, 2533, 570, 337, 385, 11, 264, 7914, 366, 264, 2430, 763, 13, 663, 311, 733, 295, 437, 307, 50936, 50936, 2673, 264, 7123, 13, 400, 550, 1338, 11, 586, 4960, 300, 777, 733, 295, 16944, 300, 574, 411, 51278, 51278, 257, 2424, 365, 257, 3098, 1192, 13, 1033, 13, 1057, 558, 13, 407, 321, 362, 732, 819, 36709, 510, 570, 51600, 51600, 321, 393, 3679, 646, 293, 5220, 1296, 264, 33358, 13, 4803, 309, 311, 3571, 281, 764, 264, 1411, 472, 562, 51825, 51825], "temperature": 0.0, "avg_logprob": -0.1597122018987482, "compression_ratio": 1.6654676258992807, "no_speech_prob": 6.238549303816399e-06}, {"id": 15, "seek": 7624, "start": 100.96, "end": 105.46, "text": " we can switch back and forth between the representations. Sometimes it's easier to use the left one when", "tokens": [50364, 456, 366, 732, 34852, 13, 583, 437, 286, 767, 11, 291, 458, 11, 14608, 307, 300, 341, 307, 257, 1045, 50660, 50660, 4583, 18161, 2533, 570, 337, 385, 11, 264, 7914, 366, 264, 2430, 763, 13, 663, 311, 733, 295, 437, 307, 50936, 50936, 2673, 264, 7123, 13, 400, 550, 1338, 11, 586, 4960, 300, 777, 733, 295, 16944, 300, 574, 411, 51278, 51278, 257, 2424, 365, 257, 3098, 1192, 13, 1033, 13, 1057, 558, 13, 407, 321, 362, 732, 819, 36709, 510, 570, 51600, 51600, 321, 393, 3679, 646, 293, 5220, 1296, 264, 33358, 13, 4803, 309, 311, 3571, 281, 764, 264, 1411, 472, 562, 51825, 51825], "temperature": 0.0, "avg_logprob": -0.1597122018987482, "compression_ratio": 1.6654676258992807, "no_speech_prob": 6.238549303816399e-06}, {"id": 16, "seek": 10546, "start": 105.46, "end": 111.11999999999999, "text": " we want to talk about the single neurons, but then sometimes we prefer to use the other", "tokens": [50364, 321, 528, 281, 751, 466, 264, 2167, 22027, 11, 457, 550, 2171, 321, 4382, 281, 764, 264, 661, 50647, 50647, 472, 11, 597, 393, 611, 411, 11, 291, 458, 11, 2696, 337, 3866, 7914, 13, 407, 1184, 411, 3461, 510, 11, 50997, 50997, 411, 264, 2058, 19866, 293, 264, 979, 19866, 393, 312, 2940, 7914, 382, 731, 13, 407, 797, 11, 613, 366, 732, 18887, 51351, 51351, 16679, 11, 286, 2041, 13, 400, 370, 264, 4846, 2170, 1709, 1854, 364, 2058, 19866, 11, 597, 2709, 505, 257, 3089, 13, 51683, 51683], "temperature": 0.0, "avg_logprob": -0.13711689364525578, "compression_ratio": 1.6053811659192825, "no_speech_prob": 1.0611372090352233e-05}, {"id": 17, "seek": 10546, "start": 111.11999999999999, "end": 118.11999999999999, "text": " one, which can also like, you know, account for multiple layers. So each like block here,", "tokens": [50364, 321, 528, 281, 751, 466, 264, 2167, 22027, 11, 457, 550, 2171, 321, 4382, 281, 764, 264, 661, 50647, 50647, 472, 11, 597, 393, 611, 411, 11, 291, 458, 11, 2696, 337, 3866, 7914, 13, 407, 1184, 411, 3461, 510, 11, 50997, 50997, 411, 264, 2058, 19866, 293, 264, 979, 19866, 393, 312, 2940, 7914, 382, 731, 13, 407, 797, 11, 613, 366, 732, 18887, 51351, 51351, 16679, 11, 286, 2041, 13, 400, 370, 264, 4846, 2170, 1709, 1854, 364, 2058, 19866, 11, 597, 2709, 505, 257, 3089, 13, 51683, 51683], "temperature": 0.0, "avg_logprob": -0.13711689364525578, "compression_ratio": 1.6053811659192825, "no_speech_prob": 1.0611372090352233e-05}, {"id": 18, "seek": 10546, "start": 118.11999999999999, "end": 125.19999999999999, "text": " like the encoder and the decoder can be several layers as well. So again, these are two macro", "tokens": [50364, 321, 528, 281, 751, 466, 264, 2167, 22027, 11, 457, 550, 2171, 321, 4382, 281, 764, 264, 661, 50647, 50647, 472, 11, 597, 393, 611, 411, 11, 291, 458, 11, 2696, 337, 3866, 7914, 13, 407, 1184, 411, 3461, 510, 11, 50997, 50997, 411, 264, 2058, 19866, 293, 264, 979, 19866, 393, 312, 2940, 7914, 382, 731, 13, 407, 797, 11, 613, 366, 732, 18887, 51351, 51351, 16679, 11, 286, 2041, 13, 400, 370, 264, 4846, 2170, 1709, 1854, 364, 2058, 19866, 11, 597, 2709, 505, 257, 3089, 13, 51683, 51683], "temperature": 0.0, "avg_logprob": -0.13711689364525578, "compression_ratio": 1.6053811659192825, "no_speech_prob": 1.0611372090352233e-05}, {"id": 19, "seek": 10546, "start": 125.19999999999999, "end": 131.84, "text": " modules, I guess. And so the input gets goes inside an encoder, which gives us a code.", "tokens": [50364, 321, 528, 281, 751, 466, 264, 2167, 22027, 11, 457, 550, 2171, 321, 4382, 281, 764, 264, 661, 50647, 50647, 472, 11, 597, 393, 611, 411, 11, 291, 458, 11, 2696, 337, 3866, 7914, 13, 407, 1184, 411, 3461, 510, 11, 50997, 50997, 411, 264, 2058, 19866, 293, 264, 979, 19866, 393, 312, 2940, 7914, 382, 731, 13, 407, 797, 11, 613, 366, 732, 18887, 51351, 51351, 16679, 11, 286, 2041, 13, 400, 370, 264, 4846, 2170, 1709, 1854, 364, 2058, 19866, 11, 597, 2709, 505, 257, 3089, 13, 51683, 51683], "temperature": 0.0, "avg_logprob": -0.13711689364525578, "compression_ratio": 1.6053811659192825, "no_speech_prob": 1.0611372090352233e-05}, {"id": 20, "seek": 13184, "start": 131.84, "end": 138.12, "text": " So H, which was before the hidden representation of a neural net. When we talk about, you know,", "tokens": [50364, 407, 389, 11, 597, 390, 949, 264, 7633, 10290, 295, 257, 18161, 2533, 13, 1133, 321, 751, 466, 11, 291, 458, 11, 50678, 50678, 8399, 2058, 378, 433, 11, 389, 307, 1219, 3089, 13, 400, 4412, 321, 362, 364, 2058, 19866, 11, 597, 307, 43430, 264, 4846, 50962, 50962, 666, 341, 3089, 13, 400, 550, 321, 362, 257, 979, 19866, 11, 597, 307, 979, 8616, 264, 3089, 666, 2035, 10290, 51304, 51304, 294, 341, 1389, 307, 11, 307, 264, 912, 10290, 382, 264, 4846, 13, 51506], "temperature": 0.0, "avg_logprob": -0.20531905781139026, "compression_ratio": 1.86096256684492, "no_speech_prob": 3.1373319870908745e-06}, {"id": 21, "seek": 13184, "start": 138.12, "end": 143.8, "text": " auto encoders, H is called code. And therefore we have an encoder, which is encoding the input", "tokens": [50364, 407, 389, 11, 597, 390, 949, 264, 7633, 10290, 295, 257, 18161, 2533, 13, 1133, 321, 751, 466, 11, 291, 458, 11, 50678, 50678, 8399, 2058, 378, 433, 11, 389, 307, 1219, 3089, 13, 400, 4412, 321, 362, 364, 2058, 19866, 11, 597, 307, 43430, 264, 4846, 50962, 50962, 666, 341, 3089, 13, 400, 550, 321, 362, 257, 979, 19866, 11, 597, 307, 979, 8616, 264, 3089, 666, 2035, 10290, 51304, 51304, 294, 341, 1389, 307, 11, 307, 264, 912, 10290, 382, 264, 4846, 13, 51506], "temperature": 0.0, "avg_logprob": -0.20531905781139026, "compression_ratio": 1.86096256684492, "no_speech_prob": 3.1373319870908745e-06}, {"id": 22, "seek": 13184, "start": 143.8, "end": 150.64000000000001, "text": " into this code. And then we have a decoder, which is decoding the code into whatever representation", "tokens": [50364, 407, 389, 11, 597, 390, 949, 264, 7633, 10290, 295, 257, 18161, 2533, 13, 1133, 321, 751, 466, 11, 291, 458, 11, 50678, 50678, 8399, 2058, 378, 433, 11, 389, 307, 1219, 3089, 13, 400, 4412, 321, 362, 364, 2058, 19866, 11, 597, 307, 43430, 264, 4846, 50962, 50962, 666, 341, 3089, 13, 400, 550, 321, 362, 257, 979, 19866, 11, 597, 307, 979, 8616, 264, 3089, 666, 2035, 10290, 51304, 51304, 294, 341, 1389, 307, 11, 307, 264, 912, 10290, 382, 264, 4846, 13, 51506], "temperature": 0.0, "avg_logprob": -0.20531905781139026, "compression_ratio": 1.86096256684492, "no_speech_prob": 3.1373319870908745e-06}, {"id": 23, "seek": 15064, "start": 150.64, "end": 164.0, "text": " in this case is, is the similar, is the same representation as the input. Okay. So on the", "tokens": [50364, 294, 341, 1389, 307, 11, 307, 264, 2531, 11, 307, 264, 912, 10290, 382, 264, 4846, 13, 1033, 13, 407, 322, 264, 51032, 51032, 558, 1011, 1252, 11, 291, 362, 364, 8399, 2058, 19866, 322, 264, 1411, 1011, 1252, 11, 291, 434, 516, 281, 312, 51260, 51260, 1612, 437, 307, 257, 3034, 1478, 8399, 2058, 19866, 13, 1057, 558, 13, 407, 456, 291, 352, 13, 32511, 1478, 8399, 2058, 19866, 13, 51600, 51600, 1033, 13, 467, 1542, 264, 912, 13, 407, 437, 311, 264, 2649, 30, 6693, 13, 286, 390, 5361, 746, 13, 407, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.1956005674419981, "compression_ratio": 1.791044776119403, "no_speech_prob": 4.93654533784138e-06}, {"id": 24, "seek": 15064, "start": 164.0, "end": 168.56, "text": " right hand side, you have an auto encoder on the left hand side, you're going to be", "tokens": [50364, 294, 341, 1389, 307, 11, 307, 264, 2531, 11, 307, 264, 912, 10290, 382, 264, 4846, 13, 1033, 13, 407, 322, 264, 51032, 51032, 558, 1011, 1252, 11, 291, 362, 364, 8399, 2058, 19866, 322, 264, 1411, 1011, 1252, 11, 291, 434, 516, 281, 312, 51260, 51260, 1612, 437, 307, 257, 3034, 1478, 8399, 2058, 19866, 13, 1057, 558, 13, 407, 456, 291, 352, 13, 32511, 1478, 8399, 2058, 19866, 13, 51600, 51600, 1033, 13, 467, 1542, 264, 912, 13, 407, 437, 311, 264, 2649, 30, 6693, 13, 286, 390, 5361, 746, 13, 407, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.1956005674419981, "compression_ratio": 1.791044776119403, "no_speech_prob": 4.93654533784138e-06}, {"id": 25, "seek": 15064, "start": 168.56, "end": 175.35999999999999, "text": " seen what is a variational auto encoder. All right. So there you go. Variational auto encoder.", "tokens": [50364, 294, 341, 1389, 307, 11, 307, 264, 2531, 11, 307, 264, 912, 10290, 382, 264, 4846, 13, 1033, 13, 407, 322, 264, 51032, 51032, 558, 1011, 1252, 11, 291, 362, 364, 8399, 2058, 19866, 322, 264, 1411, 1011, 1252, 11, 291, 434, 516, 281, 312, 51260, 51260, 1612, 437, 307, 257, 3034, 1478, 8399, 2058, 19866, 13, 1057, 558, 13, 407, 456, 291, 352, 13, 32511, 1478, 8399, 2058, 19866, 13, 51600, 51600, 1033, 13, 467, 1542, 264, 912, 13, 407, 437, 311, 264, 2649, 30, 6693, 13, 286, 390, 5361, 746, 13, 407, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.1956005674419981, "compression_ratio": 1.791044776119403, "no_speech_prob": 4.93654533784138e-06}, {"id": 26, "seek": 15064, "start": 175.35999999999999, "end": 180.51999999999998, "text": " Okay. It looks the same. So what's the difference? Nothing. I was missing something. So the", "tokens": [50364, 294, 341, 1389, 307, 11, 307, 264, 2531, 11, 307, 264, 912, 10290, 382, 264, 4846, 13, 1033, 13, 407, 322, 264, 51032, 51032, 558, 1011, 1252, 11, 291, 362, 364, 8399, 2058, 19866, 322, 264, 1411, 1011, 1252, 11, 291, 434, 516, 281, 312, 51260, 51260, 1612, 437, 307, 257, 3034, 1478, 8399, 2058, 19866, 13, 1057, 558, 13, 407, 456, 291, 352, 13, 32511, 1478, 8399, 2058, 19866, 13, 51600, 51600, 1033, 13, 467, 1542, 264, 912, 13, 407, 437, 311, 264, 2649, 30, 6693, 13, 286, 390, 5361, 746, 13, 407, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.1956005674419981, "compression_ratio": 1.791044776119403, "no_speech_prob": 4.93654533784138e-06}, {"id": 27, "seek": 18052, "start": 180.52, "end": 186.68, "text": " first difference is here that instead of having the hidden layer H, now we have the code is", "tokens": [50364, 700, 2649, 307, 510, 300, 2602, 295, 1419, 264, 7633, 4583, 389, 11, 586, 321, 362, 264, 3089, 307, 50672, 50672, 767, 1027, 295, 732, 721, 13, 467, 311, 1027, 295, 472, 551, 300, 307, 341, 733, 295, 257, 4238, 462, 50967, 50967, 295, 1176, 293, 691, 295, 1176, 13, 400, 436, 434, 516, 281, 312, 13460, 2321, 264, 914, 293, 264, 21977, 295, 341, 48994, 51380, 51380, 7006, 1176, 13, 1396, 321, 366, 516, 281, 312, 21179, 490, 341, 7316, 300, 575, 668, 13075, 1602, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.17467435201009116, "compression_ratio": 1.6607142857142858, "no_speech_prob": 1.4280738469096832e-05}, {"id": 28, "seek": 18052, "start": 186.68, "end": 192.58, "text": " actually made of two things. It's made of one thing that is this kind of a capital E", "tokens": [50364, 700, 2649, 307, 510, 300, 2602, 295, 1419, 264, 7633, 4583, 389, 11, 586, 321, 362, 264, 3089, 307, 50672, 50672, 767, 1027, 295, 732, 721, 13, 467, 311, 1027, 295, 472, 551, 300, 307, 341, 733, 295, 257, 4238, 462, 50967, 50967, 295, 1176, 293, 691, 295, 1176, 13, 400, 436, 434, 516, 281, 312, 13460, 2321, 264, 914, 293, 264, 21977, 295, 341, 48994, 51380, 51380, 7006, 1176, 13, 1396, 321, 366, 516, 281, 312, 21179, 490, 341, 7316, 300, 575, 668, 13075, 1602, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.17467435201009116, "compression_ratio": 1.6607142857142858, "no_speech_prob": 1.4280738469096832e-05}, {"id": 29, "seek": 18052, "start": 192.58, "end": 200.84, "text": " of Z and V of Z. And they're going to be representing soon the mean and the variance of this latent", "tokens": [50364, 700, 2649, 307, 510, 300, 2602, 295, 1419, 264, 7633, 4583, 389, 11, 586, 321, 362, 264, 3089, 307, 50672, 50672, 767, 1027, 295, 732, 721, 13, 467, 311, 1027, 295, 472, 551, 300, 307, 341, 733, 295, 257, 4238, 462, 50967, 50967, 295, 1176, 293, 691, 295, 1176, 13, 400, 436, 434, 516, 281, 312, 13460, 2321, 264, 914, 293, 264, 21977, 295, 341, 48994, 51380, 51380, 7006, 1176, 13, 1396, 321, 366, 516, 281, 312, 21179, 490, 341, 7316, 300, 575, 668, 13075, 1602, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.17467435201009116, "compression_ratio": 1.6607142857142858, "no_speech_prob": 1.4280738469096832e-05}, {"id": 30, "seek": 18052, "start": 200.84, "end": 208.92000000000002, "text": " variable Z. Then we are going to be sampling from this distribution that has been parameterized", "tokens": [50364, 700, 2649, 307, 510, 300, 2602, 295, 1419, 264, 7633, 4583, 389, 11, 586, 321, 362, 264, 3089, 307, 50672, 50672, 767, 1027, 295, 732, 721, 13, 467, 311, 1027, 295, 472, 551, 300, 307, 341, 733, 295, 257, 4238, 462, 50967, 50967, 295, 1176, 293, 691, 295, 1176, 13, 400, 436, 434, 516, 281, 312, 13460, 2321, 264, 914, 293, 264, 21977, 295, 341, 48994, 51380, 51380, 7006, 1176, 13, 1396, 321, 366, 516, 281, 312, 21179, 490, 341, 7316, 300, 575, 668, 13075, 1602, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.17467435201009116, "compression_ratio": 1.6607142857142858, "no_speech_prob": 1.4280738469096832e-05}, {"id": 31, "seek": 20892, "start": 208.92, "end": 216.28, "text": " by the encoder and we get to Z and Z is my latent variable, my latent representation.", "tokens": [50364, 538, 264, 2058, 19866, 293, 321, 483, 281, 1176, 293, 1176, 307, 452, 48994, 7006, 11, 452, 48994, 10290, 13, 50732, 50732, 400, 550, 341, 48994, 10290, 1709, 1854, 264, 979, 19866, 13, 407, 264, 9834, 300, 286, 6889, 51122, 51122, 490, 411, 286, 362, 257, 2710, 7316, 11, 597, 575, 512, 9834, 462, 293, 691, 11, 462, 293, 691, 366, 15957, 20458, 51420, 51420, 9540, 538, 264, 4846, 1783, 11, 457, 550, 1176, 307, 406, 15957, 3142, 13, 1176, 307, 257, 4974, 7006, 11, 597, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.14683078130086263, "compression_ratio": 1.759433962264151, "no_speech_prob": 3.2883474432310322e-06}, {"id": 32, "seek": 20892, "start": 216.28, "end": 224.07999999999998, "text": " And then this latent representation goes inside the decoder. So the parameters that I sample", "tokens": [50364, 538, 264, 2058, 19866, 293, 321, 483, 281, 1176, 293, 1176, 307, 452, 48994, 7006, 11, 452, 48994, 10290, 13, 50732, 50732, 400, 550, 341, 48994, 10290, 1709, 1854, 264, 979, 19866, 13, 407, 264, 9834, 300, 286, 6889, 51122, 51122, 490, 411, 286, 362, 257, 2710, 7316, 11, 597, 575, 512, 9834, 462, 293, 691, 11, 462, 293, 691, 366, 15957, 20458, 51420, 51420, 9540, 538, 264, 4846, 1783, 11, 457, 550, 1176, 307, 406, 15957, 3142, 13, 1176, 307, 257, 4974, 7006, 11, 597, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.14683078130086263, "compression_ratio": 1.759433962264151, "no_speech_prob": 3.2883474432310322e-06}, {"id": 33, "seek": 20892, "start": 224.07999999999998, "end": 230.04, "text": " from like I have a normal distribution, which has some parameters E and V, E and V are deterministically", "tokens": [50364, 538, 264, 2058, 19866, 293, 321, 483, 281, 1176, 293, 1176, 307, 452, 48994, 7006, 11, 452, 48994, 10290, 13, 50732, 50732, 400, 550, 341, 48994, 10290, 1709, 1854, 264, 979, 19866, 13, 407, 264, 9834, 300, 286, 6889, 51122, 51122, 490, 411, 286, 362, 257, 2710, 7316, 11, 597, 575, 512, 9834, 462, 293, 691, 11, 462, 293, 691, 366, 15957, 20458, 51420, 51420, 9540, 538, 264, 4846, 1783, 11, 457, 550, 1176, 307, 406, 15957, 3142, 13, 1176, 307, 257, 4974, 7006, 11, 597, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.14683078130086263, "compression_ratio": 1.759433962264151, "no_speech_prob": 3.2883474432310322e-06}, {"id": 34, "seek": 20892, "start": 230.04, "end": 236.95999999999998, "text": " determined by the input X, but then Z is not deterministic. Z is a random variable, which", "tokens": [50364, 538, 264, 2058, 19866, 293, 321, 483, 281, 1176, 293, 1176, 307, 452, 48994, 7006, 11, 452, 48994, 10290, 13, 50732, 50732, 400, 550, 341, 48994, 10290, 1709, 1854, 264, 979, 19866, 13, 407, 264, 9834, 300, 286, 6889, 51122, 51122, 490, 411, 286, 362, 257, 2710, 7316, 11, 597, 575, 512, 9834, 462, 293, 691, 11, 462, 293, 691, 366, 15957, 20458, 51420, 51420, 9540, 538, 264, 4846, 1783, 11, 457, 550, 1176, 307, 406, 15957, 3142, 13, 1176, 307, 257, 4974, 7006, 11, 597, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.14683078130086263, "compression_ratio": 1.759433962264151, "no_speech_prob": 3.2883474432310322e-06}, {"id": 35, "seek": 23696, "start": 236.96, "end": 243.12, "text": " you get sample from a distribution, which is parameterized by the encoder. Okay. So", "tokens": [50364, 291, 483, 6889, 490, 257, 7316, 11, 597, 307, 13075, 1602, 538, 264, 2058, 19866, 13, 1033, 13, 407, 50672, 50672, 718, 311, 584, 389, 390, 295, 2744, 413, 13, 823, 264, 3089, 510, 322, 264, 1411, 1011, 1252, 307, 516, 281, 312, 295, 2744, 51238, 51238, 732, 1413, 413, 11, 558, 30, 1436, 321, 362, 281, 2906, 264, 11, 439, 264, 1355, 293, 550, 439, 264, 1374, 21518, 13, 51544, 51544], "temperature": 0.0, "avg_logprob": -0.15650491456727725, "compression_ratio": 1.446236559139785, "no_speech_prob": 3.6666187952505425e-06}, {"id": 36, "seek": 23696, "start": 243.12, "end": 254.44, "text": " let's say H was of size D. Now the code here on the left hand side is going to be of size", "tokens": [50364, 291, 483, 6889, 490, 257, 7316, 11, 597, 307, 13075, 1602, 538, 264, 2058, 19866, 13, 1033, 13, 407, 50672, 50672, 718, 311, 584, 389, 390, 295, 2744, 413, 13, 823, 264, 3089, 510, 322, 264, 1411, 1011, 1252, 307, 516, 281, 312, 295, 2744, 51238, 51238, 732, 1413, 413, 11, 558, 30, 1436, 321, 362, 281, 2906, 264, 11, 439, 264, 1355, 293, 550, 439, 264, 1374, 21518, 13, 51544, 51544], "temperature": 0.0, "avg_logprob": -0.15650491456727725, "compression_ratio": 1.446236559139785, "no_speech_prob": 3.6666187952505425e-06}, {"id": 37, "seek": 23696, "start": 254.44, "end": 260.56, "text": " two times D, right? Because we have to represent the, all the means and then all the variances.", "tokens": [50364, 291, 483, 6889, 490, 257, 7316, 11, 597, 307, 13075, 1602, 538, 264, 2058, 19866, 13, 1033, 13, 407, 50672, 50672, 718, 311, 584, 389, 390, 295, 2744, 413, 13, 823, 264, 3089, 510, 322, 264, 1411, 1011, 1252, 307, 516, 281, 312, 295, 2744, 51238, 51238, 732, 1413, 413, 11, 558, 30, 1436, 321, 362, 281, 2906, 264, 11, 439, 264, 1355, 293, 550, 439, 264, 1374, 21518, 13, 51544, 51544], "temperature": 0.0, "avg_logprob": -0.15650491456727725, "compression_ratio": 1.446236559139785, "no_speech_prob": 3.6666187952505425e-06}, {"id": 38, "seek": 26056, "start": 260.56, "end": 267.36, "text": " In this case, we assume that we have, you know, D means and D variance. So each of those", "tokens": [50364, 682, 341, 1389, 11, 321, 6552, 300, 321, 362, 11, 291, 458, 11, 413, 1355, 293, 413, 21977, 13, 407, 1184, 295, 729, 50704, 50704, 6677, 366, 6695, 13, 1033, 13, 1057, 558, 13, 407, 321, 393, 611, 519, 466, 264, 7230, 8399, 22660, 19866, 51252, 51252, 382, 445, 43430, 264, 1355, 13, 400, 370, 498, 291, 2058, 1429, 264, 914, 293, 291, 362, 1936, 4018, 21977, 11, 51539, 51539], "temperature": 0.0, "avg_logprob": -0.156284769376119, "compression_ratio": 1.5135135135135136, "no_speech_prob": 1.7993726942222565e-05}, {"id": 39, "seek": 26056, "start": 267.36, "end": 278.32, "text": " components are independent. Okay. All right. So we can also think about the classic autoencoder", "tokens": [50364, 682, 341, 1389, 11, 321, 6552, 300, 321, 362, 11, 291, 458, 11, 413, 1355, 293, 413, 21977, 13, 407, 1184, 295, 729, 50704, 50704, 6677, 366, 6695, 13, 1033, 13, 1057, 558, 13, 407, 321, 393, 611, 519, 466, 264, 7230, 8399, 22660, 19866, 51252, 51252, 382, 445, 43430, 264, 1355, 13, 400, 370, 498, 291, 2058, 1429, 264, 914, 293, 291, 362, 1936, 4018, 21977, 11, 51539, 51539], "temperature": 0.0, "avg_logprob": -0.156284769376119, "compression_ratio": 1.5135135135135136, "no_speech_prob": 1.7993726942222565e-05}, {"id": 40, "seek": 26056, "start": 278.32, "end": 284.06, "text": " as just encoding the means. And so if you encode the mean and you have basically zero variance,", "tokens": [50364, 682, 341, 1389, 11, 321, 6552, 300, 321, 362, 11, 291, 458, 11, 413, 1355, 293, 413, 21977, 13, 407, 1184, 295, 729, 50704, 50704, 6677, 366, 6695, 13, 1033, 13, 1057, 558, 13, 407, 321, 393, 611, 519, 466, 264, 7230, 8399, 22660, 19866, 51252, 51252, 382, 445, 43430, 264, 1355, 13, 400, 370, 498, 291, 2058, 1429, 264, 914, 293, 291, 362, 1936, 4018, 21977, 11, 51539, 51539], "temperature": 0.0, "avg_logprob": -0.156284769376119, "compression_ratio": 1.5135135135135136, "no_speech_prob": 1.7993726942222565e-05}, {"id": 41, "seek": 28406, "start": 284.06, "end": 291.4, "text": " you're going to get a, again, a deterministic autoencoder. So H might be in this case D", "tokens": [50364, 291, 434, 516, 281, 483, 257, 11, 797, 11, 257, 15957, 3142, 8399, 22660, 19866, 13, 407, 389, 1062, 312, 294, 341, 1389, 413, 50731, 50731, 293, 4412, 322, 264, 1411, 1011, 1252, 11, 462, 293, 691, 486, 312, 3217, 732, 413, 13, 50961, 50961, 4162, 321, 362, 413, 1355, 11, 370, 775, 300, 914, 321, 434, 21179, 413, 37870, 30, 51225, 51225, 467, 311, 516, 281, 312, 472, 2120, 592, 3504, 473, 39148, 300, 307, 41488, 13, 400, 370, 291, 362, 439, 729, 51623, 51623], "temperature": 0.0, "avg_logprob": -0.1638300202109597, "compression_ratio": 1.4928909952606635, "no_speech_prob": 1.831729241530411e-05}, {"id": 42, "seek": 28406, "start": 291.4, "end": 296.0, "text": " and therefore on the left hand side, E and V will be total two D.", "tokens": [50364, 291, 434, 516, 281, 483, 257, 11, 797, 11, 257, 15957, 3142, 8399, 22660, 19866, 13, 407, 389, 1062, 312, 294, 341, 1389, 413, 50731, 50731, 293, 4412, 322, 264, 1411, 1011, 1252, 11, 462, 293, 691, 486, 312, 3217, 732, 413, 13, 50961, 50961, 4162, 321, 362, 413, 1355, 11, 370, 775, 300, 914, 321, 434, 21179, 413, 37870, 30, 51225, 51225, 467, 311, 516, 281, 312, 472, 2120, 592, 3504, 473, 39148, 300, 307, 41488, 13, 400, 370, 291, 362, 439, 729, 51623, 51623], "temperature": 0.0, "avg_logprob": -0.1638300202109597, "compression_ratio": 1.4928909952606635, "no_speech_prob": 1.831729241530411e-05}, {"id": 43, "seek": 28406, "start": 296.0, "end": 301.28, "text": " Since we have D means, so does that mean we're sampling D distributions?", "tokens": [50364, 291, 434, 516, 281, 483, 257, 11, 797, 11, 257, 15957, 3142, 8399, 22660, 19866, 13, 407, 389, 1062, 312, 294, 341, 1389, 413, 50731, 50731, 293, 4412, 322, 264, 1411, 1011, 1252, 11, 462, 293, 691, 486, 312, 3217, 732, 413, 13, 50961, 50961, 4162, 321, 362, 413, 1355, 11, 370, 775, 300, 914, 321, 434, 21179, 413, 37870, 30, 51225, 51225, 467, 311, 516, 281, 312, 472, 2120, 592, 3504, 473, 39148, 300, 307, 41488, 13, 400, 370, 291, 362, 439, 729, 51623, 51623], "temperature": 0.0, "avg_logprob": -0.1638300202109597, "compression_ratio": 1.4928909952606635, "no_speech_prob": 1.831729241530411e-05}, {"id": 44, "seek": 28406, "start": 301.28, "end": 309.24, "text": " It's going to be one multivariate Gaussian that is orthogonal. And so you have all those", "tokens": [50364, 291, 434, 516, 281, 483, 257, 11, 797, 11, 257, 15957, 3142, 8399, 22660, 19866, 13, 407, 389, 1062, 312, 294, 341, 1389, 413, 50731, 50731, 293, 4412, 322, 264, 1411, 1011, 1252, 11, 462, 293, 691, 486, 312, 3217, 732, 413, 13, 50961, 50961, 4162, 321, 362, 413, 1355, 11, 370, 775, 300, 914, 321, 434, 21179, 413, 37870, 30, 51225, 51225, 467, 311, 516, 281, 312, 472, 2120, 592, 3504, 473, 39148, 300, 307, 41488, 13, 400, 370, 291, 362, 439, 729, 51623, 51623], "temperature": 0.0, "avg_logprob": -0.1638300202109597, "compression_ratio": 1.4928909952606635, "no_speech_prob": 1.831729241530411e-05}, {"id": 45, "seek": 30924, "start": 309.24, "end": 314.8, "text": " components that are independent from each other. And therefore Z is going to be a D", "tokens": [50364, 6677, 300, 366, 6695, 490, 1184, 661, 13, 400, 4412, 1176, 307, 516, 281, 312, 257, 413, 50642, 50642, 18795, 8062, 13, 583, 550, 281, 6889, 257, 413, 18795, 8062, 490, 257, 39148, 11, 291, 486, 51008, 51008, 643, 413, 1355, 293, 550, 294, 341, 1389, 11, 413, 1374, 21518, 11, 570, 321, 6552, 294, 300, 439, 264, 661, 6677, 51366, 51366, 294, 264, 49851, 719, 16367, 366, 439, 35193, 13, 509, 787, 362, 264, 21539, 689, 291, 362, 439, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.16430434726533436, "compression_ratio": 1.6728971962616823, "no_speech_prob": 1.1452568287495524e-05}, {"id": 46, "seek": 30924, "start": 314.8, "end": 322.12, "text": " dimensional vector. But then to sample a D dimensional vector from a Gaussian, you will", "tokens": [50364, 6677, 300, 366, 6695, 490, 1184, 661, 13, 400, 4412, 1176, 307, 516, 281, 312, 257, 413, 50642, 50642, 18795, 8062, 13, 583, 550, 281, 6889, 257, 413, 18795, 8062, 490, 257, 39148, 11, 291, 486, 51008, 51008, 643, 413, 1355, 293, 550, 294, 341, 1389, 11, 413, 1374, 21518, 11, 570, 321, 6552, 294, 300, 439, 264, 661, 6677, 51366, 51366, 294, 264, 49851, 719, 16367, 366, 439, 35193, 13, 509, 787, 362, 264, 21539, 689, 291, 362, 439, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.16430434726533436, "compression_ratio": 1.6728971962616823, "no_speech_prob": 1.1452568287495524e-05}, {"id": 47, "seek": 30924, "start": 322.12, "end": 329.28000000000003, "text": " need D means and then in this case, D variances, because we assume in that all the other components", "tokens": [50364, 6677, 300, 366, 6695, 490, 1184, 661, 13, 400, 4412, 1176, 307, 516, 281, 312, 257, 413, 50642, 50642, 18795, 8062, 13, 583, 550, 281, 6889, 257, 413, 18795, 8062, 490, 257, 39148, 11, 291, 486, 51008, 51008, 643, 413, 1355, 293, 550, 294, 341, 1389, 11, 413, 1374, 21518, 11, 570, 321, 6552, 294, 300, 439, 264, 661, 6677, 51366, 51366, 294, 264, 49851, 719, 16367, 366, 439, 35193, 13, 509, 787, 362, 264, 21539, 689, 291, 362, 439, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.16430434726533436, "compression_ratio": 1.6728971962616823, "no_speech_prob": 1.1452568287495524e-05}, {"id": 48, "seek": 30924, "start": 329.28000000000003, "end": 334.40000000000003, "text": " in the covariance metrics are all zeros. You only have the diagonal where you have all", "tokens": [50364, 6677, 300, 366, 6695, 490, 1184, 661, 13, 400, 4412, 1176, 307, 516, 281, 312, 257, 413, 50642, 50642, 18795, 8062, 13, 583, 550, 281, 6889, 257, 413, 18795, 8062, 490, 257, 39148, 11, 291, 486, 51008, 51008, 643, 413, 1355, 293, 550, 294, 341, 1389, 11, 413, 1374, 21518, 11, 570, 321, 6552, 294, 300, 439, 264, 661, 6677, 51366, 51366, 294, 264, 49851, 719, 16367, 366, 439, 35193, 13, 509, 787, 362, 264, 21539, 689, 291, 362, 439, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.16430434726533436, "compression_ratio": 1.6728971962616823, "no_speech_prob": 1.1452568287495524e-05}, {"id": 49, "seek": 33440, "start": 334.4, "end": 340.47999999999996, "text": " the variances. Okay. So here, just to make a recap, you have the encoder that is mapping", "tokens": [50364, 264, 1374, 21518, 13, 1033, 13, 407, 510, 11, 445, 281, 652, 257, 20928, 11, 291, 362, 264, 2058, 19866, 300, 307, 18350, 50668, 50668, 341, 733, 295, 4846, 7316, 666, 411, 264, 4846, 992, 295, 10938, 666, 341, 497, 17, 35, 13, 400, 51000, 51000, 370, 321, 393, 519, 294, 341, 1389, 300, 321, 4471, 490, 1783, 281, 264, 7633, 10290, 13, 400, 550, 264, 51272, 51272, 979, 19866, 2602, 11317, 264, 1176, 1901, 666, 497, 77, 11, 597, 307, 646, 281, 264, 3380, 1901, 295, 264, 51624, 51624, 1783, 13, 400, 4412, 321, 352, 490, 3126, 9765, 1176, 666, 1783, 2385, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.16146391409414787, "compression_ratio": 1.6475409836065573, "no_speech_prob": 2.9757027732557617e-05}, {"id": 50, "seek": 33440, "start": 340.47999999999996, "end": 347.12, "text": " this kind of input distribution into like the input set of samples into this R2D. And", "tokens": [50364, 264, 1374, 21518, 13, 1033, 13, 407, 510, 11, 445, 281, 652, 257, 20928, 11, 291, 362, 264, 2058, 19866, 300, 307, 18350, 50668, 50668, 341, 733, 295, 4846, 7316, 666, 411, 264, 4846, 992, 295, 10938, 666, 341, 497, 17, 35, 13, 400, 51000, 51000, 370, 321, 393, 519, 294, 341, 1389, 300, 321, 4471, 490, 1783, 281, 264, 7633, 10290, 13, 400, 550, 264, 51272, 51272, 979, 19866, 2602, 11317, 264, 1176, 1901, 666, 497, 77, 11, 597, 307, 646, 281, 264, 3380, 1901, 295, 264, 51624, 51624, 1783, 13, 400, 4412, 321, 352, 490, 3126, 9765, 1176, 666, 1783, 2385, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.16146391409414787, "compression_ratio": 1.6475409836065573, "no_speech_prob": 2.9757027732557617e-05}, {"id": 51, "seek": 33440, "start": 347.12, "end": 352.56, "text": " so we can think in this case that we map from X to the hidden representation. And then the", "tokens": [50364, 264, 1374, 21518, 13, 1033, 13, 407, 510, 11, 445, 281, 652, 257, 20928, 11, 291, 362, 264, 2058, 19866, 300, 307, 18350, 50668, 50668, 341, 733, 295, 4846, 7316, 666, 411, 264, 4846, 992, 295, 10938, 666, 341, 497, 17, 35, 13, 400, 51000, 51000, 370, 321, 393, 519, 294, 341, 1389, 300, 321, 4471, 490, 1783, 281, 264, 7633, 10290, 13, 400, 550, 264, 51272, 51272, 979, 19866, 2602, 11317, 264, 1176, 1901, 666, 497, 77, 11, 597, 307, 646, 281, 264, 3380, 1901, 295, 264, 51624, 51624, 1783, 13, 400, 4412, 321, 352, 490, 3126, 9765, 1176, 666, 1783, 2385, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.16146391409414787, "compression_ratio": 1.6475409836065573, "no_speech_prob": 2.9757027732557617e-05}, {"id": 52, "seek": 33440, "start": 352.56, "end": 359.59999999999997, "text": " decoder instead maps the Z space into Rn, which is back to the original space of the", "tokens": [50364, 264, 1374, 21518, 13, 1033, 13, 407, 510, 11, 445, 281, 652, 257, 20928, 11, 291, 362, 264, 2058, 19866, 300, 307, 18350, 50668, 50668, 341, 733, 295, 4846, 7316, 666, 411, 264, 4846, 992, 295, 10938, 666, 341, 497, 17, 35, 13, 400, 51000, 51000, 370, 321, 393, 519, 294, 341, 1389, 300, 321, 4471, 490, 1783, 281, 264, 7633, 10290, 13, 400, 550, 264, 51272, 51272, 979, 19866, 2602, 11317, 264, 1176, 1901, 666, 497, 77, 11, 597, 307, 646, 281, 264, 3380, 1901, 295, 264, 51624, 51624, 1783, 13, 400, 4412, 321, 352, 490, 3126, 9765, 1176, 666, 1783, 2385, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.16146391409414787, "compression_ratio": 1.6475409836065573, "no_speech_prob": 2.9757027732557617e-05}, {"id": 53, "seek": 33440, "start": 359.59999999999997, "end": 364.35999999999996, "text": " X. And therefore we go from lowercase Z into X hat.", "tokens": [50364, 264, 1374, 21518, 13, 1033, 13, 407, 510, 11, 445, 281, 652, 257, 20928, 11, 291, 362, 264, 2058, 19866, 300, 307, 18350, 50668, 50668, 341, 733, 295, 4846, 7316, 666, 411, 264, 4846, 992, 295, 10938, 666, 341, 497, 17, 35, 13, 400, 51000, 51000, 370, 321, 393, 519, 294, 341, 1389, 300, 321, 4471, 490, 1783, 281, 264, 7633, 10290, 13, 400, 550, 264, 51272, 51272, 979, 19866, 2602, 11317, 264, 1176, 1901, 666, 497, 77, 11, 597, 307, 646, 281, 264, 3380, 1901, 295, 264, 51624, 51624, 1783, 13, 400, 4412, 321, 352, 490, 3126, 9765, 1176, 666, 1783, 2385, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.16146391409414787, "compression_ratio": 1.6475409836065573, "no_speech_prob": 2.9757027732557617e-05}, {"id": 54, "seek": 36436, "start": 364.36, "end": 370.12, "text": " Someone asked if E of Z and the V of Z, is that the output of the encoder?", "tokens": [50364, 8734, 2351, 498, 462, 295, 1176, 293, 264, 691, 295, 1176, 11, 307, 300, 264, 5598, 295, 264, 2058, 19866, 30, 50652, 50652, 865, 13, 462, 295, 1176, 293, 691, 295, 1176, 366, 445, 9834, 300, 366, 15957, 20458, 5598, 538, 264, 2058, 19866, 13, 51018, 51018, 407, 264, 2058, 19866, 307, 257, 15957, 3142, 11, 291, 458, 11, 309, 311, 445, 264, 13735, 12447, 293, 2339, 11077, 51386, 51386, 293, 550, 1071, 2489, 9887, 13, 407, 309, 311, 445, 257, 2522, 295, 257, 18161, 3209, 11, 597, 307, 51594, 51594, 5598, 783, 512, 9834, 13, 1033, 13, 407, 341, 307, 264, 2058, 19866, 11, 597, 307, 2902, 385, 613, 9834, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1489337921142578, "compression_ratio": 1.893162393162393, "no_speech_prob": 3.023876342922449e-05}, {"id": 55, "seek": 36436, "start": 370.12, "end": 377.44, "text": " Yeah. E of Z and V of Z are just parameters that are deterministically output by the encoder.", "tokens": [50364, 8734, 2351, 498, 462, 295, 1176, 293, 264, 691, 295, 1176, 11, 307, 300, 264, 5598, 295, 264, 2058, 19866, 30, 50652, 50652, 865, 13, 462, 295, 1176, 293, 691, 295, 1176, 366, 445, 9834, 300, 366, 15957, 20458, 5598, 538, 264, 2058, 19866, 13, 51018, 51018, 407, 264, 2058, 19866, 307, 257, 15957, 3142, 11, 291, 458, 11, 309, 311, 445, 264, 13735, 12447, 293, 2339, 11077, 51386, 51386, 293, 550, 1071, 2489, 9887, 13, 407, 309, 311, 445, 257, 2522, 295, 257, 18161, 3209, 11, 597, 307, 51594, 51594, 5598, 783, 512, 9834, 13, 1033, 13, 407, 341, 307, 264, 2058, 19866, 11, 597, 307, 2902, 385, 613, 9834, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1489337921142578, "compression_ratio": 1.893162393162393, "no_speech_prob": 3.023876342922449e-05}, {"id": 56, "seek": 36436, "start": 377.44, "end": 384.8, "text": " So the encoder is a deterministic, you know, it's just the classical rotation and squashing", "tokens": [50364, 8734, 2351, 498, 462, 295, 1176, 293, 264, 691, 295, 1176, 11, 307, 300, 264, 5598, 295, 264, 2058, 19866, 30, 50652, 50652, 865, 13, 462, 295, 1176, 293, 691, 295, 1176, 366, 445, 9834, 300, 366, 15957, 20458, 5598, 538, 264, 2058, 19866, 13, 51018, 51018, 407, 264, 2058, 19866, 307, 257, 15957, 3142, 11, 291, 458, 11, 309, 311, 445, 264, 13735, 12447, 293, 2339, 11077, 51386, 51386, 293, 550, 1071, 2489, 9887, 13, 407, 309, 311, 445, 257, 2522, 295, 257, 18161, 3209, 11, 597, 307, 51594, 51594, 5598, 783, 512, 9834, 13, 1033, 13, 407, 341, 307, 264, 2058, 19866, 11, 597, 307, 2902, 385, 613, 9834, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1489337921142578, "compression_ratio": 1.893162393162393, "no_speech_prob": 3.023876342922449e-05}, {"id": 57, "seek": 36436, "start": 384.8, "end": 388.96000000000004, "text": " and then another fine transformation. So it's just a piece of a neural network, which is", "tokens": [50364, 8734, 2351, 498, 462, 295, 1176, 293, 264, 691, 295, 1176, 11, 307, 300, 264, 5598, 295, 264, 2058, 19866, 30, 50652, 50652, 865, 13, 462, 295, 1176, 293, 691, 295, 1176, 366, 445, 9834, 300, 366, 15957, 20458, 5598, 538, 264, 2058, 19866, 13, 51018, 51018, 407, 264, 2058, 19866, 307, 257, 15957, 3142, 11, 291, 458, 11, 309, 311, 445, 264, 13735, 12447, 293, 2339, 11077, 51386, 51386, 293, 550, 1071, 2489, 9887, 13, 407, 309, 311, 445, 257, 2522, 295, 257, 18161, 3209, 11, 597, 307, 51594, 51594, 5598, 783, 512, 9834, 13, 1033, 13, 407, 341, 307, 264, 2058, 19866, 11, 597, 307, 2902, 385, 613, 9834, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1489337921142578, "compression_ratio": 1.893162393162393, "no_speech_prob": 3.023876342922449e-05}, {"id": 58, "seek": 36436, "start": 388.96000000000004, "end": 394.2, "text": " outputting some parameters. Okay. So this is the encoder, which is giving me these parameters", "tokens": [50364, 8734, 2351, 498, 462, 295, 1176, 293, 264, 691, 295, 1176, 11, 307, 300, 264, 5598, 295, 264, 2058, 19866, 30, 50652, 50652, 865, 13, 462, 295, 1176, 293, 691, 295, 1176, 366, 445, 9834, 300, 366, 15957, 20458, 5598, 538, 264, 2058, 19866, 13, 51018, 51018, 407, 264, 2058, 19866, 307, 257, 15957, 3142, 11, 291, 458, 11, 309, 311, 445, 264, 13735, 12447, 293, 2339, 11077, 51386, 51386, 293, 550, 1071, 2489, 9887, 13, 407, 309, 311, 445, 257, 2522, 295, 257, 18161, 3209, 11, 597, 307, 51594, 51594, 5598, 783, 512, 9834, 13, 1033, 13, 407, 341, 307, 264, 2058, 19866, 11, 597, 307, 2902, 385, 613, 9834, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1489337921142578, "compression_ratio": 1.893162393162393, "no_speech_prob": 3.023876342922449e-05}, {"id": 59, "seek": 39420, "start": 394.2, "end": 401.32, "text": " E and V, given my input X. Right. So this is deterministic part. Then given that we", "tokens": [50364, 462, 293, 691, 11, 2212, 452, 4846, 1783, 13, 1779, 13, 407, 341, 307, 15957, 3142, 644, 13, 1396, 2212, 300, 321, 50720, 50720, 362, 613, 9834, 11, 613, 9834, 366, 11, 291, 458, 11, 2902, 385, 257, 39148, 7316, 51106, 51106, 365, 2685, 1355, 293, 2685, 1374, 21518, 13, 400, 490, 613, 21977, 11, 490, 613, 39148, 51380, 51380, 7316, 11, 321, 6889, 472, 6889, 1176, 13, 1033, 13, 400, 550, 321, 979, 1429, 11, 597, 1355, 11, 321, 434, 516, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.21229566529739735, "compression_ratio": 1.6826923076923077, "no_speech_prob": 1.1837624697363935e-05}, {"id": 60, "seek": 39420, "start": 401.32, "end": 409.03999999999996, "text": " have these parameters, these parameters are, you know, giving me a Gaussian distribution", "tokens": [50364, 462, 293, 691, 11, 2212, 452, 4846, 1783, 13, 1779, 13, 407, 341, 307, 15957, 3142, 644, 13, 1396, 2212, 300, 321, 50720, 50720, 362, 613, 9834, 11, 613, 9834, 366, 11, 291, 458, 11, 2902, 385, 257, 39148, 7316, 51106, 51106, 365, 2685, 1355, 293, 2685, 1374, 21518, 13, 400, 490, 613, 21977, 11, 490, 613, 39148, 51380, 51380, 7316, 11, 321, 6889, 472, 6889, 1176, 13, 1033, 13, 400, 550, 321, 979, 1429, 11, 597, 1355, 11, 321, 434, 516, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.21229566529739735, "compression_ratio": 1.6826923076923077, "no_speech_prob": 1.1837624697363935e-05}, {"id": 61, "seek": 39420, "start": 409.03999999999996, "end": 414.52, "text": " with specific means and specific variances. And from these variance, from these Gaussian", "tokens": [50364, 462, 293, 691, 11, 2212, 452, 4846, 1783, 13, 1779, 13, 407, 341, 307, 15957, 3142, 644, 13, 1396, 2212, 300, 321, 50720, 50720, 362, 613, 9834, 11, 613, 9834, 366, 11, 291, 458, 11, 2902, 385, 257, 39148, 7316, 51106, 51106, 365, 2685, 1355, 293, 2685, 1374, 21518, 13, 400, 490, 613, 21977, 11, 490, 613, 39148, 51380, 51380, 7316, 11, 321, 6889, 472, 6889, 1176, 13, 1033, 13, 400, 550, 321, 979, 1429, 11, 597, 1355, 11, 321, 434, 516, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.21229566529739735, "compression_ratio": 1.6826923076923077, "no_speech_prob": 1.1837624697363935e-05}, {"id": 62, "seek": 39420, "start": 414.52, "end": 422.52, "text": " distribution, we sample one sample Z. Okay. And then we decode, which means, we're going", "tokens": [50364, 462, 293, 691, 11, 2212, 452, 4846, 1783, 13, 1779, 13, 407, 341, 307, 15957, 3142, 644, 13, 1396, 2212, 300, 321, 50720, 50720, 362, 613, 9834, 11, 613, 9834, 366, 11, 291, 458, 11, 2902, 385, 257, 39148, 7316, 51106, 51106, 365, 2685, 1355, 293, 2685, 1374, 21518, 13, 400, 490, 613, 21977, 11, 490, 613, 39148, 51380, 51380, 7316, 11, 321, 6889, 472, 6889, 1176, 13, 1033, 13, 400, 550, 321, 979, 1429, 11, 597, 1355, 11, 321, 434, 516, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.21229566529739735, "compression_ratio": 1.6826923076923077, "no_speech_prob": 1.1837624697363935e-05}, {"id": 63, "seek": 42252, "start": 422.52, "end": 430.03999999999996, "text": " to see what means this in a second. But basically you're going to be encoding the mean, and", "tokens": [50364, 281, 536, 437, 1355, 341, 294, 257, 1150, 13, 583, 1936, 291, 434, 516, 281, 312, 43430, 264, 914, 11, 293, 50740, 50740, 550, 291, 434, 516, 281, 312, 5127, 512, 4497, 11, 512, 5658, 11, 1392, 11, 281, 300, 43430, 13, 682, 264, 51094, 51094, 24518, 406, 2058, 19866, 11, 321, 645, 1242, 527, 4846, 13, 509, 645, 5127, 5658, 281, 264, 4846, 293, 550, 51338, 51338, 291, 645, 1382, 281, 31499, 264, 4846, 1553, 5658, 13, 682, 510, 11, 264, 787, 551, 300, 307, 3105, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.1801088144491007, "compression_ratio": 1.8308457711442787, "no_speech_prob": 3.217241101083346e-05}, {"id": 64, "seek": 42252, "start": 430.03999999999996, "end": 437.12, "text": " then you're going to be adding some additional, some noise, okay, to that encoding. In the", "tokens": [50364, 281, 536, 437, 1355, 341, 294, 257, 1150, 13, 583, 1936, 291, 434, 516, 281, 312, 43430, 264, 914, 11, 293, 50740, 50740, 550, 291, 434, 516, 281, 312, 5127, 512, 4497, 11, 512, 5658, 11, 1392, 11, 281, 300, 43430, 13, 682, 264, 51094, 51094, 24518, 406, 2058, 19866, 11, 321, 645, 1242, 527, 4846, 13, 509, 645, 5127, 5658, 281, 264, 4846, 293, 550, 51338, 51338, 291, 645, 1382, 281, 31499, 264, 4846, 1553, 5658, 13, 682, 510, 11, 264, 787, 551, 300, 307, 3105, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.1801088144491007, "compression_ratio": 1.8308457711442787, "no_speech_prob": 3.217241101083346e-05}, {"id": 65, "seek": 42252, "start": 437.12, "end": 442.0, "text": " noisy not encoder, we were getting our input. You were adding noise to the input and then", "tokens": [50364, 281, 536, 437, 1355, 341, 294, 257, 1150, 13, 583, 1936, 291, 434, 516, 281, 312, 43430, 264, 914, 11, 293, 50740, 50740, 550, 291, 434, 516, 281, 312, 5127, 512, 4497, 11, 512, 5658, 11, 1392, 11, 281, 300, 43430, 13, 682, 264, 51094, 51094, 24518, 406, 2058, 19866, 11, 321, 645, 1242, 527, 4846, 13, 509, 645, 5127, 5658, 281, 264, 4846, 293, 550, 51338, 51338, 291, 645, 1382, 281, 31499, 264, 4846, 1553, 5658, 13, 682, 510, 11, 264, 787, 551, 300, 307, 3105, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.1801088144491007, "compression_ratio": 1.8308457711442787, "no_speech_prob": 3.217241101083346e-05}, {"id": 66, "seek": 42252, "start": 442.0, "end": 447.88, "text": " you were trying to reconstruct the input without noise. In here, the only thing that is changed", "tokens": [50364, 281, 536, 437, 1355, 341, 294, 257, 1150, 13, 583, 1936, 291, 434, 516, 281, 312, 43430, 264, 914, 11, 293, 50740, 50740, 550, 291, 434, 516, 281, 312, 5127, 512, 4497, 11, 512, 5658, 11, 1392, 11, 281, 300, 43430, 13, 682, 264, 51094, 51094, 24518, 406, 2058, 19866, 11, 321, 645, 1242, 527, 4846, 13, 509, 645, 5127, 5658, 281, 264, 4846, 293, 550, 51338, 51338, 291, 645, 1382, 281, 31499, 264, 4846, 1553, 5658, 13, 682, 510, 11, 264, 787, 551, 300, 307, 3105, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.1801088144491007, "compression_ratio": 1.8308457711442787, "no_speech_prob": 3.217241101083346e-05}, {"id": 67, "seek": 44788, "start": 447.88, "end": 454.52, "text": " is the fact that the noise is added to the hidden representation rather than to the input.", "tokens": [50364, 307, 264, 1186, 300, 264, 5658, 307, 3869, 281, 264, 7633, 10290, 2831, 813, 281, 264, 4846, 13, 50696, 50696, 4402, 309, 652, 2020, 30, 865, 11, 300, 1669, 257, 688, 544, 2020, 13, 1044, 291, 13, 407, 286, 5694, 300, 264, 50970, 50970, 24657, 2564, 733, 295, 1542, 411, 5176, 2158, 13, 2014, 321, 17746, 445, 257, 2710, 914, 51282, 51282, 490, 1176, 420, 366, 321, 767, 15866, 411, 733, 295, 257, 32807, 4274, 30, 883, 11, 572, 11, 456, 307, 572, 13, 51568, 51568], "temperature": 0.0, "avg_logprob": -0.15861183946782892, "compression_ratio": 1.5964125560538116, "no_speech_prob": 8.397892997891176e-06}, {"id": 68, "seek": 44788, "start": 454.52, "end": 460.0, "text": " Does it make sense? Yeah, that makes a lot more sense. Thank you. So I noticed that the", "tokens": [50364, 307, 264, 1186, 300, 264, 5658, 307, 3869, 281, 264, 7633, 10290, 2831, 813, 281, 264, 4846, 13, 50696, 50696, 4402, 309, 652, 2020, 30, 865, 11, 300, 1669, 257, 688, 544, 2020, 13, 1044, 291, 13, 407, 286, 5694, 300, 264, 50970, 50970, 24657, 2564, 733, 295, 1542, 411, 5176, 2158, 13, 2014, 321, 17746, 445, 257, 2710, 914, 51282, 51282, 490, 1176, 420, 366, 321, 767, 15866, 411, 733, 295, 257, 32807, 4274, 30, 883, 11, 572, 11, 456, 307, 572, 13, 51568, 51568], "temperature": 0.0, "avg_logprob": -0.15861183946782892, "compression_ratio": 1.5964125560538116, "no_speech_prob": 8.397892997891176e-06}, {"id": 69, "seek": 44788, "start": 460.0, "end": 466.24, "text": " notation itself kind of looks like expected value. Are we generating just a normal mean", "tokens": [50364, 307, 264, 1186, 300, 264, 5658, 307, 3869, 281, 264, 7633, 10290, 2831, 813, 281, 264, 4846, 13, 50696, 50696, 4402, 309, 652, 2020, 30, 865, 11, 300, 1669, 257, 688, 544, 2020, 13, 1044, 291, 13, 407, 286, 5694, 300, 264, 50970, 50970, 24657, 2564, 733, 295, 1542, 411, 5176, 2158, 13, 2014, 321, 17746, 445, 257, 2710, 914, 51282, 51282, 490, 1176, 420, 366, 321, 767, 15866, 411, 733, 295, 257, 32807, 4274, 30, 883, 11, 572, 11, 456, 307, 572, 13, 51568, 51568], "temperature": 0.0, "avg_logprob": -0.15861183946782892, "compression_ratio": 1.5964125560538116, "no_speech_prob": 8.397892997891176e-06}, {"id": 70, "seek": 44788, "start": 466.24, "end": 471.96, "text": " from Z or are we actually computing like kind of a weighted average? No, no, there is no.", "tokens": [50364, 307, 264, 1186, 300, 264, 5658, 307, 3869, 281, 264, 7633, 10290, 2831, 813, 281, 264, 4846, 13, 50696, 50696, 4402, 309, 652, 2020, 30, 865, 11, 300, 1669, 257, 688, 544, 2020, 13, 1044, 291, 13, 407, 286, 5694, 300, 264, 50970, 50970, 24657, 2564, 733, 295, 1542, 411, 5176, 2158, 13, 2014, 321, 17746, 445, 257, 2710, 914, 51282, 51282, 490, 1176, 420, 366, 321, 767, 15866, 411, 733, 295, 257, 32807, 4274, 30, 883, 11, 572, 11, 456, 307, 572, 13, 51568, 51568], "temperature": 0.0, "avg_logprob": -0.15861183946782892, "compression_ratio": 1.5964125560538116, "no_speech_prob": 8.397892997891176e-06}, {"id": 71, "seek": 47196, "start": 471.96, "end": 479.44, "text": " Okay. So my X instead of outputting the, is outputting, let's say D is going to be 10.", "tokens": [50364, 1033, 13, 407, 452, 1783, 2602, 295, 5598, 783, 264, 11, 307, 5598, 783, 11, 718, 311, 584, 413, 307, 516, 281, 312, 1266, 13, 50738, 50738, 663, 307, 264, 7633, 10290, 13, 823, 2602, 295, 1419, 1266, 4190, 13460, 264, 914, 11, 51006, 51006, 321, 434, 516, 281, 362, 945, 4190, 13, 1266, 4190, 366, 13460, 264, 914, 293, 1266, 4190, 366, 13460, 51248, 51248, 264, 1374, 21518, 13, 1033, 13, 407, 321, 445, 5598, 257, 8062, 389, 510, 11, 2212, 452, 1783, 11, 264, 700, 1922, 295, 264, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.14031145420480282, "compression_ratio": 1.8383838383838385, "no_speech_prob": 1.3844681234331802e-05}, {"id": 72, "seek": 47196, "start": 479.44, "end": 484.79999999999995, "text": " That is the hidden representation. Now instead of having 10 values representing the mean,", "tokens": [50364, 1033, 13, 407, 452, 1783, 2602, 295, 5598, 783, 264, 11, 307, 5598, 783, 11, 718, 311, 584, 413, 307, 516, 281, 312, 1266, 13, 50738, 50738, 663, 307, 264, 7633, 10290, 13, 823, 2602, 295, 1419, 1266, 4190, 13460, 264, 914, 11, 51006, 51006, 321, 434, 516, 281, 362, 945, 4190, 13, 1266, 4190, 366, 13460, 264, 914, 293, 1266, 4190, 366, 13460, 51248, 51248, 264, 1374, 21518, 13, 1033, 13, 407, 321, 445, 5598, 257, 8062, 389, 510, 11, 2212, 452, 1783, 11, 264, 700, 1922, 295, 264, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.14031145420480282, "compression_ratio": 1.8383838383838385, "no_speech_prob": 1.3844681234331802e-05}, {"id": 73, "seek": 47196, "start": 484.79999999999995, "end": 489.64, "text": " we're going to have 20 values. 10 values are representing the mean and 10 values are representing", "tokens": [50364, 1033, 13, 407, 452, 1783, 2602, 295, 5598, 783, 264, 11, 307, 5598, 783, 11, 718, 311, 584, 413, 307, 516, 281, 312, 1266, 13, 50738, 50738, 663, 307, 264, 7633, 10290, 13, 823, 2602, 295, 1419, 1266, 4190, 13460, 264, 914, 11, 51006, 51006, 321, 434, 516, 281, 362, 945, 4190, 13, 1266, 4190, 366, 13460, 264, 914, 293, 1266, 4190, 366, 13460, 51248, 51248, 264, 1374, 21518, 13, 1033, 13, 407, 321, 445, 5598, 257, 8062, 389, 510, 11, 2212, 452, 1783, 11, 264, 700, 1922, 295, 264, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.14031145420480282, "compression_ratio": 1.8383838383838385, "no_speech_prob": 1.3844681234331802e-05}, {"id": 74, "seek": 47196, "start": 489.64, "end": 498.08, "text": " the variances. Okay. So we just output a vector H here, given my X, the first half of the", "tokens": [50364, 1033, 13, 407, 452, 1783, 2602, 295, 5598, 783, 264, 11, 307, 5598, 783, 11, 718, 311, 584, 413, 307, 516, 281, 312, 1266, 13, 50738, 50738, 663, 307, 264, 7633, 10290, 13, 823, 2602, 295, 1419, 1266, 4190, 13460, 264, 914, 11, 51006, 51006, 321, 434, 516, 281, 362, 945, 4190, 13, 1266, 4190, 366, 13460, 264, 914, 293, 1266, 4190, 366, 13460, 51248, 51248, 264, 1374, 21518, 13, 1033, 13, 407, 321, 445, 5598, 257, 8062, 389, 510, 11, 2212, 452, 1783, 11, 264, 700, 1922, 295, 264, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.14031145420480282, "compression_ratio": 1.8383838383838385, "no_speech_prob": 1.3844681234331802e-05}, {"id": 75, "seek": 49808, "start": 498.08, "end": 504.91999999999996, "text": " vector represents the means of a Gaussian distribution and the other half of the vector", "tokens": [50364, 8062, 8855, 264, 1355, 295, 257, 39148, 7316, 293, 264, 661, 1922, 295, 264, 8062, 50706, 50706, 8855, 264, 1374, 21518, 337, 264, 912, 39148, 7316, 13, 1033, 13, 407, 264, 6542, 389, 11, 264, 51000, 51000, 700, 6542, 389, 16, 307, 516, 281, 312, 264, 914, 295, 264, 700, 39148, 293, 550, 264, 6542, 51310, 51310, 389, 11, 718, 311, 584, 11, 1392, 11, 718, 311, 818, 309, 389, 17, 13, 682, 341, 1389, 11, 309, 311, 516, 281, 312, 264, 21977, 13, 1396, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.1646650828672259, "compression_ratio": 1.9344262295081966, "no_speech_prob": 1.3630628927785438e-05}, {"id": 76, "seek": 49808, "start": 504.91999999999996, "end": 510.8, "text": " represents the variances for the same Gaussian distribution. Okay. So the component H, the", "tokens": [50364, 8062, 8855, 264, 1355, 295, 257, 39148, 7316, 293, 264, 661, 1922, 295, 264, 8062, 50706, 50706, 8855, 264, 1374, 21518, 337, 264, 912, 39148, 7316, 13, 1033, 13, 407, 264, 6542, 389, 11, 264, 51000, 51000, 700, 6542, 389, 16, 307, 516, 281, 312, 264, 914, 295, 264, 700, 39148, 293, 550, 264, 6542, 51310, 51310, 389, 11, 718, 311, 584, 11, 1392, 11, 718, 311, 818, 309, 389, 17, 13, 682, 341, 1389, 11, 309, 311, 516, 281, 312, 264, 21977, 13, 1396, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.1646650828672259, "compression_ratio": 1.9344262295081966, "no_speech_prob": 1.3630628927785438e-05}, {"id": 77, "seek": 49808, "start": 510.8, "end": 517.0, "text": " first component H1 is going to be the mean of the first Gaussian and then the component", "tokens": [50364, 8062, 8855, 264, 1355, 295, 257, 39148, 7316, 293, 264, 661, 1922, 295, 264, 8062, 50706, 50706, 8855, 264, 1374, 21518, 337, 264, 912, 39148, 7316, 13, 1033, 13, 407, 264, 6542, 389, 11, 264, 51000, 51000, 700, 6542, 389, 16, 307, 516, 281, 312, 264, 914, 295, 264, 700, 39148, 293, 550, 264, 6542, 51310, 51310, 389, 11, 718, 311, 584, 11, 1392, 11, 718, 311, 818, 309, 389, 17, 13, 682, 341, 1389, 11, 309, 311, 516, 281, 312, 264, 21977, 13, 1396, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.1646650828672259, "compression_ratio": 1.9344262295081966, "no_speech_prob": 1.3630628927785438e-05}, {"id": 78, "seek": 49808, "start": 517.0, "end": 522.96, "text": " H, let's say, okay, let's call it H2. In this case, it's going to be the variance. Then", "tokens": [50364, 8062, 8855, 264, 1355, 295, 257, 39148, 7316, 293, 264, 661, 1922, 295, 264, 8062, 50706, 50706, 8855, 264, 1374, 21518, 337, 264, 912, 39148, 7316, 13, 1033, 13, 407, 264, 6542, 389, 11, 264, 51000, 51000, 700, 6542, 389, 16, 307, 516, 281, 312, 264, 914, 295, 264, 700, 39148, 293, 550, 264, 6542, 51310, 51310, 389, 11, 718, 311, 584, 11, 1392, 11, 718, 311, 818, 309, 389, 17, 13, 682, 341, 1389, 11, 309, 311, 516, 281, 312, 264, 21977, 13, 1396, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.1646650828672259, "compression_ratio": 1.9344262295081966, "no_speech_prob": 1.3630628927785438e-05}, {"id": 79, "seek": 52296, "start": 522.96, "end": 529.36, "text": " you have H3 is going to be another mean. H4 is going to be another variance and so on.", "tokens": [50364, 291, 362, 389, 18, 307, 516, 281, 312, 1071, 914, 13, 389, 19, 307, 516, 281, 312, 1071, 21977, 293, 370, 322, 13, 50684, 50684, 1033, 13, 407, 775, 300, 652, 11, 576, 300, 652, 1176, 411, 257, 1266, 18795, 8062, 300, 311, 3247, 15551, 51012, 51012, 490, 729, 30, 865, 13, 865, 13, 407, 1176, 510, 307, 516, 281, 312, 1922, 295, 341, 2744, 510, 11, 558, 30, 407, 264, 51320, 51320, 2058, 19866, 2709, 385, 6091, 264, 10139, 295, 1176, 293, 550, 570, 291, 483, 1922, 295, 264, 12819, 11, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.1691298484802246, "compression_ratio": 1.6714285714285715, "no_speech_prob": 7.766741873638239e-06}, {"id": 80, "seek": 52296, "start": 529.36, "end": 535.9200000000001, "text": " Okay. So does that make, would that make Z like a 10 dimensional vector that's sampled", "tokens": [50364, 291, 362, 389, 18, 307, 516, 281, 312, 1071, 914, 13, 389, 19, 307, 516, 281, 312, 1071, 21977, 293, 370, 322, 13, 50684, 50684, 1033, 13, 407, 775, 300, 652, 11, 576, 300, 652, 1176, 411, 257, 1266, 18795, 8062, 300, 311, 3247, 15551, 51012, 51012, 490, 729, 30, 865, 13, 865, 13, 407, 1176, 510, 307, 516, 281, 312, 1922, 295, 341, 2744, 510, 11, 558, 30, 407, 264, 51320, 51320, 2058, 19866, 2709, 385, 6091, 264, 10139, 295, 1176, 293, 550, 570, 291, 483, 1922, 295, 264, 12819, 11, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.1691298484802246, "compression_ratio": 1.6714285714285715, "no_speech_prob": 7.766741873638239e-06}, {"id": 81, "seek": 52296, "start": 535.9200000000001, "end": 542.08, "text": " from those? Yeah. Yeah. So Z here is going to be half of this size here, right? So the", "tokens": [50364, 291, 362, 389, 18, 307, 516, 281, 312, 1071, 914, 13, 389, 19, 307, 516, 281, 312, 1071, 21977, 293, 370, 322, 13, 50684, 50684, 1033, 13, 407, 775, 300, 652, 11, 576, 300, 652, 1176, 411, 257, 1266, 18795, 8062, 300, 311, 3247, 15551, 51012, 51012, 490, 729, 30, 865, 13, 865, 13, 407, 1176, 510, 307, 516, 281, 312, 1922, 295, 341, 2744, 510, 11, 558, 30, 407, 264, 51320, 51320, 2058, 19866, 2709, 385, 6091, 264, 10139, 295, 1176, 293, 550, 570, 291, 483, 1922, 295, 264, 12819, 11, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.1691298484802246, "compression_ratio": 1.6714285714285715, "no_speech_prob": 7.766741873638239e-06}, {"id": 82, "seek": 52296, "start": 542.08, "end": 549.8000000000001, "text": " encoder gives me twice the dimension of Z and then because you get half of the dimensions,", "tokens": [50364, 291, 362, 389, 18, 307, 516, 281, 312, 1071, 914, 13, 389, 19, 307, 516, 281, 312, 1071, 21977, 293, 370, 322, 13, 50684, 50684, 1033, 13, 407, 775, 300, 652, 11, 576, 300, 652, 1176, 411, 257, 1266, 18795, 8062, 300, 311, 3247, 15551, 51012, 51012, 490, 729, 30, 865, 13, 865, 13, 407, 1176, 510, 307, 516, 281, 312, 1922, 295, 341, 2744, 510, 11, 558, 30, 407, 264, 51320, 51320, 2058, 19866, 2709, 385, 6091, 264, 10139, 295, 1176, 293, 550, 570, 291, 483, 1922, 295, 264, 12819, 11, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.1691298484802246, "compression_ratio": 1.6714285714285715, "no_speech_prob": 7.766741873638239e-06}, {"id": 83, "seek": 54980, "start": 549.8, "end": 555.3199999999999, "text": " like one set of these are for the means and one set of these are the variances. Then we", "tokens": [50364, 411, 472, 992, 295, 613, 366, 337, 264, 1355, 293, 472, 992, 295, 613, 366, 264, 1374, 21518, 13, 1396, 321, 50640, 50640, 6889, 490, 257, 39148, 300, 575, 613, 4190, 13, 407, 264, 3209, 2935, 2709, 385, 406, 445, 264, 50950, 50950, 1355, 382, 337, 264, 13735, 8399, 22660, 19866, 11, 457, 611, 2709, 385, 512, 11, 437, 307, 264, 3613, 300, 51398, 51398, 286, 393, 1888, 721, 490, 11, 558, 30, 4546, 11, 562, 321, 645, 1228, 264, 13735, 8399, 22660, 19866, 510, 11, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.11390808953179253, "compression_ratio": 1.7611940298507462, "no_speech_prob": 6.745803602825617e-06}, {"id": 84, "seek": 54980, "start": 555.3199999999999, "end": 561.52, "text": " sample from a Gaussian that has these values. So the network simply gives me not just the", "tokens": [50364, 411, 472, 992, 295, 613, 366, 337, 264, 1355, 293, 472, 992, 295, 613, 366, 264, 1374, 21518, 13, 1396, 321, 50640, 50640, 6889, 490, 257, 39148, 300, 575, 613, 4190, 13, 407, 264, 3209, 2935, 2709, 385, 406, 445, 264, 50950, 50950, 1355, 382, 337, 264, 13735, 8399, 22660, 19866, 11, 457, 611, 2709, 385, 512, 11, 437, 307, 264, 3613, 300, 51398, 51398, 286, 393, 1888, 721, 490, 11, 558, 30, 4546, 11, 562, 321, 645, 1228, 264, 13735, 8399, 22660, 19866, 510, 11, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.11390808953179253, "compression_ratio": 1.7611940298507462, "no_speech_prob": 6.745803602825617e-06}, {"id": 85, "seek": 54980, "start": 561.52, "end": 570.4799999999999, "text": " means as for the classical autoencoder, but also gives me some, what is the range that", "tokens": [50364, 411, 472, 992, 295, 613, 366, 337, 264, 1355, 293, 472, 992, 295, 613, 366, 264, 1374, 21518, 13, 1396, 321, 50640, 50640, 6889, 490, 257, 39148, 300, 575, 613, 4190, 13, 407, 264, 3209, 2935, 2709, 385, 406, 445, 264, 50950, 50950, 1355, 382, 337, 264, 13735, 8399, 22660, 19866, 11, 457, 611, 2709, 385, 512, 11, 437, 307, 264, 3613, 300, 51398, 51398, 286, 393, 1888, 721, 490, 11, 558, 30, 4546, 11, 562, 321, 645, 1228, 264, 13735, 8399, 22660, 19866, 510, 11, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.11390808953179253, "compression_ratio": 1.7611940298507462, "no_speech_prob": 6.745803602825617e-06}, {"id": 86, "seek": 54980, "start": 570.4799999999999, "end": 576.38, "text": " I can pick things from, right? Before, when we were using the classical autoencoder here,", "tokens": [50364, 411, 472, 992, 295, 613, 366, 337, 264, 1355, 293, 472, 992, 295, 613, 366, 264, 1374, 21518, 13, 1396, 321, 50640, 50640, 6889, 490, 257, 39148, 300, 575, 613, 4190, 13, 407, 264, 3209, 2935, 2709, 385, 406, 445, 264, 50950, 50950, 1355, 382, 337, 264, 13735, 8399, 22660, 19866, 11, 457, 611, 2709, 385, 512, 11, 437, 307, 264, 3613, 300, 51398, 51398, 286, 393, 1888, 721, 490, 11, 558, 30, 4546, 11, 562, 321, 645, 1228, 264, 13735, 8399, 22660, 19866, 510, 11, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.11390808953179253, "compression_ratio": 1.7611940298507462, "no_speech_prob": 6.745803602825617e-06}, {"id": 87, "seek": 57638, "start": 576.38, "end": 583.16, "text": " we only have the means and then you simply decode the means. In this case, you not only", "tokens": [50364, 321, 787, 362, 264, 1355, 293, 550, 291, 2935, 979, 1429, 264, 1355, 13, 682, 341, 1389, 11, 291, 406, 787, 50703, 50703, 362, 264, 1355, 11, 457, 611, 291, 393, 362, 512, 21977, 11, 512, 17840, 2108, 729, 1355, 13, 51061, 51061, 1033, 13, 407, 8399, 22660, 19866, 11, 2710, 8399, 22660, 19866, 307, 15957, 3142, 13, 440, 5598, 307, 15957, 3142, 51404, 51404, 4846, 2445, 295, 264, 4846, 13, 316, 3034, 1478, 8399, 22660, 19866, 11, 264, 5598, 307, 406, 2854, 15957, 3142, 13, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.16886987686157226, "compression_ratio": 1.951086956521739, "no_speech_prob": 1.0122231287823524e-05}, {"id": 88, "seek": 57638, "start": 583.16, "end": 590.32, "text": " have the means, but also you can have some variance, some variations across those means.", "tokens": [50364, 321, 787, 362, 264, 1355, 293, 550, 291, 2935, 979, 1429, 264, 1355, 13, 682, 341, 1389, 11, 291, 406, 787, 50703, 50703, 362, 264, 1355, 11, 457, 611, 291, 393, 362, 512, 21977, 11, 512, 17840, 2108, 729, 1355, 13, 51061, 51061, 1033, 13, 407, 8399, 22660, 19866, 11, 2710, 8399, 22660, 19866, 307, 15957, 3142, 13, 440, 5598, 307, 15957, 3142, 51404, 51404, 4846, 2445, 295, 264, 4846, 13, 316, 3034, 1478, 8399, 22660, 19866, 11, 264, 5598, 307, 406, 2854, 15957, 3142, 13, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.16886987686157226, "compression_ratio": 1.951086956521739, "no_speech_prob": 1.0122231287823524e-05}, {"id": 89, "seek": 57638, "start": 590.32, "end": 597.18, "text": " Okay. So autoencoder, normal autoencoder is deterministic. The output is deterministic", "tokens": [50364, 321, 787, 362, 264, 1355, 293, 550, 291, 2935, 979, 1429, 264, 1355, 13, 682, 341, 1389, 11, 291, 406, 787, 50703, 50703, 362, 264, 1355, 11, 457, 611, 291, 393, 362, 512, 21977, 11, 512, 17840, 2108, 729, 1355, 13, 51061, 51061, 1033, 13, 407, 8399, 22660, 19866, 11, 2710, 8399, 22660, 19866, 307, 15957, 3142, 13, 440, 5598, 307, 15957, 3142, 51404, 51404, 4846, 2445, 295, 264, 4846, 13, 316, 3034, 1478, 8399, 22660, 19866, 11, 264, 5598, 307, 406, 2854, 15957, 3142, 13, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.16886987686157226, "compression_ratio": 1.951086956521739, "no_speech_prob": 1.0122231287823524e-05}, {"id": 90, "seek": 57638, "start": 597.18, "end": 605.58, "text": " input function of the input. A variational autoencoder, the output is not longer deterministic.", "tokens": [50364, 321, 787, 362, 264, 1355, 293, 550, 291, 2935, 979, 1429, 264, 1355, 13, 682, 341, 1389, 11, 291, 406, 787, 50703, 50703, 362, 264, 1355, 11, 457, 611, 291, 393, 362, 512, 21977, 11, 512, 17840, 2108, 729, 1355, 13, 51061, 51061, 1033, 13, 407, 8399, 22660, 19866, 11, 2710, 8399, 22660, 19866, 307, 15957, 3142, 13, 440, 5598, 307, 15957, 3142, 51404, 51404, 4846, 2445, 295, 264, 4846, 13, 316, 3034, 1478, 8399, 22660, 19866, 11, 264, 5598, 307, 406, 2854, 15957, 3142, 13, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.16886987686157226, "compression_ratio": 1.951086956521739, "no_speech_prob": 1.0122231287823524e-05}, {"id": 91, "seek": 60558, "start": 605.58, "end": 611.6, "text": " It's no longer a deterministic function of the input. It's going to be a distribution", "tokens": [50364, 467, 311, 572, 2854, 257, 15957, 3142, 2445, 295, 264, 4846, 13, 467, 311, 516, 281, 312, 257, 7316, 50665, 50665, 2212, 264, 4846, 11, 558, 30, 407, 309, 311, 257, 27708, 7316, 2212, 264, 4846, 13, 407, 294, 341, 1389, 11, 50997, 50997, 321, 536, 300, 321, 1866, 257, 2531, 10686, 1036, 565, 689, 321, 645, 516, 490, 257, 2685, 935, 51309, 51309, 322, 264, 1411, 1011, 1252, 281, 264, 558, 1011, 1252, 13, 682, 341, 1389, 11, 321, 722, 510, 11, 411, 257, 935, 11, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.11477921035263565, "compression_ratio": 1.771144278606965, "no_speech_prob": 8.938147402659524e-06}, {"id": 92, "seek": 60558, "start": 611.6, "end": 618.24, "text": " given the input, right? So it's a conditional distribution given the input. So in this case,", "tokens": [50364, 467, 311, 572, 2854, 257, 15957, 3142, 2445, 295, 264, 4846, 13, 467, 311, 516, 281, 312, 257, 7316, 50665, 50665, 2212, 264, 4846, 11, 558, 30, 407, 309, 311, 257, 27708, 7316, 2212, 264, 4846, 13, 407, 294, 341, 1389, 11, 50997, 50997, 321, 536, 300, 321, 1866, 257, 2531, 10686, 1036, 565, 689, 321, 645, 516, 490, 257, 2685, 935, 51309, 51309, 322, 264, 1411, 1011, 1252, 281, 264, 558, 1011, 1252, 13, 682, 341, 1389, 11, 321, 722, 510, 11, 411, 257, 935, 11, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.11477921035263565, "compression_ratio": 1.771144278606965, "no_speech_prob": 8.938147402659524e-06}, {"id": 93, "seek": 60558, "start": 618.24, "end": 624.48, "text": " we see that we saw a similar diagram last time where we were going from a specific point", "tokens": [50364, 467, 311, 572, 2854, 257, 15957, 3142, 2445, 295, 264, 4846, 13, 467, 311, 516, 281, 312, 257, 7316, 50665, 50665, 2212, 264, 4846, 11, 558, 30, 407, 309, 311, 257, 27708, 7316, 2212, 264, 4846, 13, 407, 294, 341, 1389, 11, 50997, 50997, 321, 536, 300, 321, 1866, 257, 2531, 10686, 1036, 565, 689, 321, 645, 516, 490, 257, 2685, 935, 51309, 51309, 322, 264, 1411, 1011, 1252, 281, 264, 558, 1011, 1252, 13, 682, 341, 1389, 11, 321, 722, 510, 11, 411, 257, 935, 11, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.11477921035263565, "compression_ratio": 1.771144278606965, "no_speech_prob": 8.938147402659524e-06}, {"id": 94, "seek": 60558, "start": 624.48, "end": 630.62, "text": " on the left hand side to the right hand side. In this case, we start here, like a point,", "tokens": [50364, 467, 311, 572, 2854, 257, 15957, 3142, 2445, 295, 264, 4846, 13, 467, 311, 516, 281, 312, 257, 7316, 50665, 50665, 2212, 264, 4846, 11, 558, 30, 407, 309, 311, 257, 27708, 7316, 2212, 264, 4846, 13, 407, 294, 341, 1389, 11, 50997, 50997, 321, 536, 300, 321, 1866, 257, 2531, 10686, 1036, 565, 689, 321, 645, 516, 490, 257, 2685, 935, 51309, 51309, 322, 264, 1411, 1011, 1252, 281, 264, 558, 1011, 1252, 13, 682, 341, 1389, 11, 321, 722, 510, 11, 411, 257, 935, 11, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.11477921035263565, "compression_ratio": 1.771144278606965, "no_speech_prob": 8.938147402659524e-06}, {"id": 95, "seek": 63062, "start": 630.62, "end": 636.2, "text": " and then we get through the encoder, you're going to get some position here, but then", "tokens": [50364, 293, 550, 321, 483, 807, 264, 2058, 19866, 11, 291, 434, 516, 281, 483, 512, 2535, 510, 11, 457, 550, 50643, 50643, 456, 307, 257, 4500, 295, 5658, 11, 558, 30, 759, 291, 787, 362, 264, 914, 11, 291, 576, 483, 445, 472, 50901, 50901, 1176, 13, 583, 550, 2212, 300, 456, 307, 512, 4497, 5658, 300, 307, 3462, 281, 264, 1186, 300, 321, 500, 380, 51205, 51205, 362, 257, 4018, 21977, 11, 300, 2572, 935, 11, 300, 2572, 1176, 11, 309, 311, 406, 516, 281, 312, 445, 472, 935, 13, 51469, 51469, 467, 311, 516, 281, 312, 411, 257, 34710, 935, 13, 1033, 13, 407, 2602, 295, 1419, 472, 935, 11, 586, 472, 1783, 51783, 51783], "temperature": 0.0, "avg_logprob": -0.1411151405142135, "compression_ratio": 1.7959183673469388, "no_speech_prob": 3.424833994358778e-05}, {"id": 96, "seek": 63062, "start": 636.2, "end": 641.36, "text": " there is a addition of noise, right? If you only have the mean, you would get just one", "tokens": [50364, 293, 550, 321, 483, 807, 264, 2058, 19866, 11, 291, 434, 516, 281, 483, 512, 2535, 510, 11, 457, 550, 50643, 50643, 456, 307, 257, 4500, 295, 5658, 11, 558, 30, 759, 291, 787, 362, 264, 914, 11, 291, 576, 483, 445, 472, 50901, 50901, 1176, 13, 583, 550, 2212, 300, 456, 307, 512, 4497, 5658, 300, 307, 3462, 281, 264, 1186, 300, 321, 500, 380, 51205, 51205, 362, 257, 4018, 21977, 11, 300, 2572, 935, 11, 300, 2572, 1176, 11, 309, 311, 406, 516, 281, 312, 445, 472, 935, 13, 51469, 51469, 467, 311, 516, 281, 312, 411, 257, 34710, 935, 13, 1033, 13, 407, 2602, 295, 1419, 472, 935, 11, 586, 472, 1783, 51783, 51783], "temperature": 0.0, "avg_logprob": -0.1411151405142135, "compression_ratio": 1.7959183673469388, "no_speech_prob": 3.424833994358778e-05}, {"id": 97, "seek": 63062, "start": 641.36, "end": 647.44, "text": " Z. But then given that there is some additional noise that is due to the fact that we don't", "tokens": [50364, 293, 550, 321, 483, 807, 264, 2058, 19866, 11, 291, 434, 516, 281, 483, 512, 2535, 510, 11, 457, 550, 50643, 50643, 456, 307, 257, 4500, 295, 5658, 11, 558, 30, 759, 291, 787, 362, 264, 914, 11, 291, 576, 483, 445, 472, 50901, 50901, 1176, 13, 583, 550, 2212, 300, 456, 307, 512, 4497, 5658, 300, 307, 3462, 281, 264, 1186, 300, 321, 500, 380, 51205, 51205, 362, 257, 4018, 21977, 11, 300, 2572, 935, 11, 300, 2572, 1176, 11, 309, 311, 406, 516, 281, 312, 445, 472, 935, 13, 51469, 51469, 467, 311, 516, 281, 312, 411, 257, 34710, 935, 13, 1033, 13, 407, 2602, 295, 1419, 472, 935, 11, 586, 472, 1783, 51783, 51783], "temperature": 0.0, "avg_logprob": -0.1411151405142135, "compression_ratio": 1.7959183673469388, "no_speech_prob": 3.424833994358778e-05}, {"id": 98, "seek": 63062, "start": 647.44, "end": 652.72, "text": " have a zero variance, that final point, that final Z, it's not going to be just one point.", "tokens": [50364, 293, 550, 321, 483, 807, 264, 2058, 19866, 11, 291, 434, 516, 281, 483, 512, 2535, 510, 11, 457, 550, 50643, 50643, 456, 307, 257, 4500, 295, 5658, 11, 558, 30, 759, 291, 787, 362, 264, 914, 11, 291, 576, 483, 445, 472, 50901, 50901, 1176, 13, 583, 550, 2212, 300, 456, 307, 512, 4497, 5658, 300, 307, 3462, 281, 264, 1186, 300, 321, 500, 380, 51205, 51205, 362, 257, 4018, 21977, 11, 300, 2572, 935, 11, 300, 2572, 1176, 11, 309, 311, 406, 516, 281, 312, 445, 472, 935, 13, 51469, 51469, 467, 311, 516, 281, 312, 411, 257, 34710, 935, 13, 1033, 13, 407, 2602, 295, 1419, 472, 935, 11, 586, 472, 1783, 51783, 51783], "temperature": 0.0, "avg_logprob": -0.1411151405142135, "compression_ratio": 1.7959183673469388, "no_speech_prob": 3.424833994358778e-05}, {"id": 99, "seek": 63062, "start": 652.72, "end": 659.0, "text": " It's going to be like a fuzzy point. Okay. So instead of having one point, now one X", "tokens": [50364, 293, 550, 321, 483, 807, 264, 2058, 19866, 11, 291, 434, 516, 281, 483, 512, 2535, 510, 11, 457, 550, 50643, 50643, 456, 307, 257, 4500, 295, 5658, 11, 558, 30, 759, 291, 787, 362, 264, 914, 11, 291, 576, 483, 445, 472, 50901, 50901, 1176, 13, 583, 550, 2212, 300, 456, 307, 512, 4497, 5658, 300, 307, 3462, 281, 264, 1186, 300, 321, 500, 380, 51205, 51205, 362, 257, 4018, 21977, 11, 300, 2572, 935, 11, 300, 2572, 1176, 11, 309, 311, 406, 516, 281, 312, 445, 472, 935, 13, 51469, 51469, 467, 311, 516, 281, 312, 411, 257, 34710, 935, 13, 1033, 13, 407, 2602, 295, 1419, 472, 935, 11, 586, 472, 1783, 51783, 51783], "temperature": 0.0, "avg_logprob": -0.1411151405142135, "compression_ratio": 1.7959183673469388, "no_speech_prob": 3.424833994358778e-05}, {"id": 100, "seek": 65900, "start": 659.0, "end": 662.72, "text": " is going to be mapped into one region of points. Okay. So it's going to be actually taking", "tokens": [50364, 307, 516, 281, 312, 33318, 666, 472, 4458, 295, 2793, 13, 1033, 13, 407, 309, 311, 516, 281, 312, 767, 1940, 50550, 50550, 512, 1901, 13, 400, 550, 577, 360, 321, 3847, 264, 1185, 30, 492, 3847, 264, 1185, 538, 7750, 341, 48994, 50862, 50862, 7006, 1176, 646, 281, 264, 979, 19866, 294, 1668, 281, 483, 341, 1783, 11, 257, 2385, 13, 400, 295, 1164, 11, 309, 311, 406, 51286, 51286, 516, 281, 312, 1242, 309, 2293, 281, 264, 3380, 935, 570, 4317, 321, 2378, 380, 1939, 8895, 13, 51560, 51560, 407, 321, 362, 281, 31499, 264, 3380, 4846, 13, 400, 281, 360, 300, 11, 321, 434, 516, 281, 312, 1382, 281, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.10755868615775273, "compression_ratio": 1.796812749003984, "no_speech_prob": 1.3173621482565068e-05}, {"id": 101, "seek": 65900, "start": 662.72, "end": 668.96, "text": " some space. And then how do we train the system? We train the system by sending this latent", "tokens": [50364, 307, 516, 281, 312, 33318, 666, 472, 4458, 295, 2793, 13, 1033, 13, 407, 309, 311, 516, 281, 312, 767, 1940, 50550, 50550, 512, 1901, 13, 400, 550, 577, 360, 321, 3847, 264, 1185, 30, 492, 3847, 264, 1185, 538, 7750, 341, 48994, 50862, 50862, 7006, 1176, 646, 281, 264, 979, 19866, 294, 1668, 281, 483, 341, 1783, 11, 257, 2385, 13, 400, 295, 1164, 11, 309, 311, 406, 51286, 51286, 516, 281, 312, 1242, 309, 2293, 281, 264, 3380, 935, 570, 4317, 321, 2378, 380, 1939, 8895, 13, 51560, 51560, 407, 321, 362, 281, 31499, 264, 3380, 4846, 13, 400, 281, 360, 300, 11, 321, 434, 516, 281, 312, 1382, 281, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.10755868615775273, "compression_ratio": 1.796812749003984, "no_speech_prob": 1.3173621482565068e-05}, {"id": 102, "seek": 65900, "start": 668.96, "end": 677.44, "text": " variable Z back to the decoder in order to get this X, a hat. And of course, it's not", "tokens": [50364, 307, 516, 281, 312, 33318, 666, 472, 4458, 295, 2793, 13, 1033, 13, 407, 309, 311, 516, 281, 312, 767, 1940, 50550, 50550, 512, 1901, 13, 400, 550, 577, 360, 321, 3847, 264, 1185, 30, 492, 3847, 264, 1185, 538, 7750, 341, 48994, 50862, 50862, 7006, 1176, 646, 281, 264, 979, 19866, 294, 1668, 281, 483, 341, 1783, 11, 257, 2385, 13, 400, 295, 1164, 11, 309, 311, 406, 51286, 51286, 516, 281, 312, 1242, 309, 2293, 281, 264, 3380, 935, 570, 4317, 321, 2378, 380, 1939, 8895, 13, 51560, 51560, 407, 321, 362, 281, 31499, 264, 3380, 4846, 13, 400, 281, 360, 300, 11, 321, 434, 516, 281, 312, 1382, 281, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.10755868615775273, "compression_ratio": 1.796812749003984, "no_speech_prob": 1.3173621482565068e-05}, {"id": 103, "seek": 65900, "start": 677.44, "end": 682.92, "text": " going to be getting it exactly to the original point because perhaps we haven't yet trained.", "tokens": [50364, 307, 516, 281, 312, 33318, 666, 472, 4458, 295, 2793, 13, 1033, 13, 407, 309, 311, 516, 281, 312, 767, 1940, 50550, 50550, 512, 1901, 13, 400, 550, 577, 360, 321, 3847, 264, 1185, 30, 492, 3847, 264, 1185, 538, 7750, 341, 48994, 50862, 50862, 7006, 1176, 646, 281, 264, 979, 19866, 294, 1668, 281, 483, 341, 1783, 11, 257, 2385, 13, 400, 295, 1164, 11, 309, 311, 406, 51286, 51286, 516, 281, 312, 1242, 309, 2293, 281, 264, 3380, 935, 570, 4317, 321, 2378, 380, 1939, 8895, 13, 51560, 51560, 407, 321, 362, 281, 31499, 264, 3380, 4846, 13, 400, 281, 360, 300, 11, 321, 434, 516, 281, 312, 1382, 281, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.10755868615775273, "compression_ratio": 1.796812749003984, "no_speech_prob": 1.3173621482565068e-05}, {"id": 104, "seek": 65900, "start": 682.92, "end": 688.56, "text": " So we have to reconstruct the original input. And to do that, we're going to be trying to", "tokens": [50364, 307, 516, 281, 312, 33318, 666, 472, 4458, 295, 2793, 13, 1033, 13, 407, 309, 311, 516, 281, 312, 767, 1940, 50550, 50550, 512, 1901, 13, 400, 550, 577, 360, 321, 3847, 264, 1185, 30, 492, 3847, 264, 1185, 538, 7750, 341, 48994, 50862, 50862, 7006, 1176, 646, 281, 264, 979, 19866, 294, 1668, 281, 483, 341, 1783, 11, 257, 2385, 13, 400, 295, 1164, 11, 309, 311, 406, 51286, 51286, 516, 281, 312, 1242, 309, 2293, 281, 264, 3380, 935, 570, 4317, 321, 2378, 380, 1939, 8895, 13, 51560, 51560, 407, 321, 362, 281, 31499, 264, 3380, 4846, 13, 400, 281, 360, 300, 11, 321, 434, 516, 281, 312, 1382, 281, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.10755868615775273, "compression_ratio": 1.796812749003984, "no_speech_prob": 1.3173621482565068e-05}, {"id": 105, "seek": 68856, "start": 688.56, "end": 694.9599999999999, "text": " minimize what is the square distance between the reconstruction and the original input.", "tokens": [50364, 17522, 437, 307, 264, 3732, 4560, 1296, 264, 31565, 293, 264, 3380, 4846, 13, 50684, 50684, 400, 550, 321, 632, 264, 1154, 949, 11, 411, 281, 352, 490, 264, 48994, 281, 264, 4846, 1901, 11, 321, 51040, 51040, 643, 281, 458, 544, 264, 48994, 7316, 420, 281, 24825, 512, 7316, 13, 5264, 565, 51322, 51322, 321, 645, 2577, 300, 321, 645, 884, 746, 2531, 562, 321, 366, 1228, 264, 13735, 11, 264, 51628, 51628], "temperature": 0.0, "avg_logprob": -0.13563026879963122, "compression_ratio": 1.6926829268292682, "no_speech_prob": 2.2087722754804417e-05}, {"id": 106, "seek": 68856, "start": 694.9599999999999, "end": 702.0799999999999, "text": " And then we had the problem before, like to go from the latent to the input space, we", "tokens": [50364, 17522, 437, 307, 264, 3732, 4560, 1296, 264, 31565, 293, 264, 3380, 4846, 13, 50684, 50684, 400, 550, 321, 632, 264, 1154, 949, 11, 411, 281, 352, 490, 264, 48994, 281, 264, 4846, 1901, 11, 321, 51040, 51040, 643, 281, 458, 544, 264, 48994, 7316, 420, 281, 24825, 512, 7316, 13, 5264, 565, 51322, 51322, 321, 645, 2577, 300, 321, 645, 884, 746, 2531, 562, 321, 366, 1228, 264, 13735, 11, 264, 51628, 51628], "temperature": 0.0, "avg_logprob": -0.13563026879963122, "compression_ratio": 1.6926829268292682, "no_speech_prob": 2.2087722754804417e-05}, {"id": 107, "seek": 68856, "start": 702.0799999999999, "end": 707.7199999999999, "text": " need to know more the latent distribution or to enforce some distribution. Last time", "tokens": [50364, 17522, 437, 307, 264, 3732, 4560, 1296, 264, 31565, 293, 264, 3380, 4846, 13, 50684, 50684, 400, 550, 321, 632, 264, 1154, 949, 11, 411, 281, 352, 490, 264, 48994, 281, 264, 4846, 1901, 11, 321, 51040, 51040, 643, 281, 458, 544, 264, 48994, 7316, 420, 281, 24825, 512, 7316, 13, 5264, 565, 51322, 51322, 321, 645, 2577, 300, 321, 645, 884, 746, 2531, 562, 321, 366, 1228, 264, 13735, 11, 264, 51628, 51628], "temperature": 0.0, "avg_logprob": -0.13563026879963122, "compression_ratio": 1.6926829268292682, "no_speech_prob": 2.2087722754804417e-05}, {"id": 108, "seek": 68856, "start": 707.7199999999999, "end": 713.8399999999999, "text": " we were seeing that we were doing something similar when we are using the classical, the", "tokens": [50364, 17522, 437, 307, 264, 3732, 4560, 1296, 264, 31565, 293, 264, 3380, 4846, 13, 50684, 50684, 400, 550, 321, 632, 264, 1154, 949, 11, 411, 281, 352, 490, 264, 48994, 281, 264, 4846, 1901, 11, 321, 51040, 51040, 643, 281, 458, 544, 264, 48994, 7316, 420, 281, 24825, 512, 7316, 13, 5264, 565, 51322, 51322, 321, 645, 2577, 300, 321, 645, 884, 746, 2531, 562, 321, 366, 1228, 264, 13735, 11, 264, 51628, 51628], "temperature": 0.0, "avg_logprob": -0.13563026879963122, "compression_ratio": 1.6926829268292682, "no_speech_prob": 2.2087722754804417e-05}, {"id": 109, "seek": 71384, "start": 713.84, "end": 720.88, "text": " standard auto encoder. But we were going from one point X to one point Z and then back to", "tokens": [50364, 3832, 8399, 2058, 19866, 13, 583, 321, 645, 516, 490, 472, 935, 1783, 281, 472, 935, 1176, 293, 550, 646, 281, 50716, 50716, 1783, 13, 1779, 586, 2602, 11, 321, 366, 516, 281, 312, 25495, 2175, 257, 7316, 670, 613, 2793, 294, 264, 48994, 51058, 51058, 1901, 13, 4546, 321, 645, 516, 281, 472, 935, 11, 472, 935, 11, 472, 935, 13, 400, 550, 291, 500, 380, 458, 51306, 51306, 437, 311, 2737, 498, 291, 1286, 926, 294, 264, 48994, 1901, 13, 5459, 30, 407, 498, 291, 362, 322, 51532, 51532, 264, 1411, 1011, 1252, 11, 1266, 10938, 11, 291, 434, 516, 281, 362, 6772, 322, 264, 661, 1252, 11, 1266, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.16459718787151834, "compression_ratio": 1.8553719008264462, "no_speech_prob": 4.4244047785468865e-06}, {"id": 110, "seek": 71384, "start": 720.88, "end": 727.72, "text": " X. Right now instead, we are going to be enforcing a distribution over these points in the latent", "tokens": [50364, 3832, 8399, 2058, 19866, 13, 583, 321, 645, 516, 490, 472, 935, 1783, 281, 472, 935, 1176, 293, 550, 646, 281, 50716, 50716, 1783, 13, 1779, 586, 2602, 11, 321, 366, 516, 281, 312, 25495, 2175, 257, 7316, 670, 613, 2793, 294, 264, 48994, 51058, 51058, 1901, 13, 4546, 321, 645, 516, 281, 472, 935, 11, 472, 935, 11, 472, 935, 13, 400, 550, 291, 500, 380, 458, 51306, 51306, 437, 311, 2737, 498, 291, 1286, 926, 294, 264, 48994, 1901, 13, 5459, 30, 407, 498, 291, 362, 322, 51532, 51532, 264, 1411, 1011, 1252, 11, 1266, 10938, 11, 291, 434, 516, 281, 362, 6772, 322, 264, 661, 1252, 11, 1266, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.16459718787151834, "compression_ratio": 1.8553719008264462, "no_speech_prob": 4.4244047785468865e-06}, {"id": 111, "seek": 71384, "start": 727.72, "end": 732.6800000000001, "text": " space. Before we were going to one point, one point, one point. And then you don't know", "tokens": [50364, 3832, 8399, 2058, 19866, 13, 583, 321, 645, 516, 490, 472, 935, 1783, 281, 472, 935, 1176, 293, 550, 646, 281, 50716, 50716, 1783, 13, 1779, 586, 2602, 11, 321, 366, 516, 281, 312, 25495, 2175, 257, 7316, 670, 613, 2793, 294, 264, 48994, 51058, 51058, 1901, 13, 4546, 321, 645, 516, 281, 472, 935, 11, 472, 935, 11, 472, 935, 13, 400, 550, 291, 500, 380, 458, 51306, 51306, 437, 311, 2737, 498, 291, 1286, 926, 294, 264, 48994, 1901, 13, 5459, 30, 407, 498, 291, 362, 322, 51532, 51532, 264, 1411, 1011, 1252, 11, 1266, 10938, 11, 291, 434, 516, 281, 362, 6772, 322, 264, 661, 1252, 11, 1266, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.16459718787151834, "compression_ratio": 1.8553719008264462, "no_speech_prob": 4.4244047785468865e-06}, {"id": 112, "seek": 71384, "start": 732.6800000000001, "end": 737.2, "text": " what's happening if you move around in the latent space. Remember? So if you have on", "tokens": [50364, 3832, 8399, 2058, 19866, 13, 583, 321, 645, 516, 490, 472, 935, 1783, 281, 472, 935, 1176, 293, 550, 646, 281, 50716, 50716, 1783, 13, 1779, 586, 2602, 11, 321, 366, 516, 281, 312, 25495, 2175, 257, 7316, 670, 613, 2793, 294, 264, 48994, 51058, 51058, 1901, 13, 4546, 321, 645, 516, 281, 472, 935, 11, 472, 935, 11, 472, 935, 13, 400, 550, 291, 500, 380, 458, 51306, 51306, 437, 311, 2737, 498, 291, 1286, 926, 294, 264, 48994, 1901, 13, 5459, 30, 407, 498, 291, 362, 322, 51532, 51532, 264, 1411, 1011, 1252, 11, 1266, 10938, 11, 291, 434, 516, 281, 362, 6772, 322, 264, 661, 1252, 11, 1266, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.16459718787151834, "compression_ratio": 1.8553719008264462, "no_speech_prob": 4.4244047785468865e-06}, {"id": 113, "seek": 71384, "start": 737.2, "end": 741.96, "text": " the left hand side, 10 samples, you're going to have automatically on the other side, 10", "tokens": [50364, 3832, 8399, 2058, 19866, 13, 583, 321, 645, 516, 490, 472, 935, 1783, 281, 472, 935, 1176, 293, 550, 646, 281, 50716, 50716, 1783, 13, 1779, 586, 2602, 11, 321, 366, 516, 281, 312, 25495, 2175, 257, 7316, 670, 613, 2793, 294, 264, 48994, 51058, 51058, 1901, 13, 4546, 321, 645, 516, 281, 472, 935, 11, 472, 935, 11, 472, 935, 13, 400, 550, 291, 500, 380, 458, 51306, 51306, 437, 311, 2737, 498, 291, 1286, 926, 294, 264, 48994, 1901, 13, 5459, 30, 407, 498, 291, 362, 322, 51532, 51532, 264, 1411, 1011, 1252, 11, 1266, 10938, 11, 291, 434, 516, 281, 362, 6772, 322, 264, 661, 1252, 11, 1266, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.16459718787151834, "compression_ratio": 1.8553719008264462, "no_speech_prob": 4.4244047785468865e-06}, {"id": 114, "seek": 74196, "start": 741.96, "end": 747.96, "text": " latent variables. But then you don't know how to go between these inputs, between these,", "tokens": [50364, 48994, 9102, 13, 583, 550, 291, 500, 380, 458, 577, 281, 352, 1296, 613, 15743, 11, 1296, 613, 11, 50664, 50664, 291, 500, 380, 458, 577, 281, 3147, 294, 341, 48994, 1901, 570, 321, 500, 380, 458, 577, 264, 1901, 50966, 50966, 36896, 13, 32511, 1478, 8399, 2058, 378, 433, 24825, 512, 3877, 13, 400, 436, 360, 341, 538, 5127, 51368, 51368, 257, 16263, 295, 885, 819, 420, 1400, 490, 257, 2710, 7316, 13, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.1775831809410682, "compression_ratio": 1.6875, "no_speech_prob": 3.2060306693892926e-05}, {"id": 115, "seek": 74196, "start": 747.96, "end": 754.0, "text": " you don't know how to travel in this latent space because we don't know how the space", "tokens": [50364, 48994, 9102, 13, 583, 550, 291, 500, 380, 458, 577, 281, 352, 1296, 613, 15743, 11, 1296, 613, 11, 50664, 50664, 291, 500, 380, 458, 577, 281, 3147, 294, 341, 48994, 1901, 570, 321, 500, 380, 458, 577, 264, 1901, 50966, 50966, 36896, 13, 32511, 1478, 8399, 2058, 378, 433, 24825, 512, 3877, 13, 400, 436, 360, 341, 538, 5127, 51368, 51368, 257, 16263, 295, 885, 819, 420, 1400, 490, 257, 2710, 7316, 13, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.1775831809410682, "compression_ratio": 1.6875, "no_speech_prob": 3.2060306693892926e-05}, {"id": 116, "seek": 74196, "start": 754.0, "end": 762.0400000000001, "text": " behaves. Variational auto encoders enforce some structure. And they do this by adding", "tokens": [50364, 48994, 9102, 13, 583, 550, 291, 500, 380, 458, 577, 281, 352, 1296, 613, 15743, 11, 1296, 613, 11, 50664, 50664, 291, 500, 380, 458, 577, 281, 3147, 294, 341, 48994, 1901, 570, 321, 500, 380, 458, 577, 264, 1901, 50966, 50966, 36896, 13, 32511, 1478, 8399, 2058, 378, 433, 24825, 512, 3877, 13, 400, 436, 360, 341, 538, 5127, 51368, 51368, 257, 16263, 295, 885, 819, 420, 1400, 490, 257, 2710, 7316, 13, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.1775831809410682, "compression_ratio": 1.6875, "no_speech_prob": 3.2060306693892926e-05}, {"id": 117, "seek": 74196, "start": 762.0400000000001, "end": 768.0, "text": " a penalty of being different or far from a normal distribution.", "tokens": [50364, 48994, 9102, 13, 583, 550, 291, 500, 380, 458, 577, 281, 352, 1296, 613, 15743, 11, 1296, 613, 11, 50664, 50664, 291, 500, 380, 458, 577, 281, 3147, 294, 341, 48994, 1901, 570, 321, 500, 380, 458, 577, 264, 1901, 50966, 50966, 36896, 13, 32511, 1478, 8399, 2058, 378, 433, 24825, 512, 3877, 13, 400, 436, 360, 341, 538, 5127, 51368, 51368, 257, 16263, 295, 885, 819, 420, 1400, 490, 257, 2710, 7316, 13, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.1775831809410682, "compression_ratio": 1.6875, "no_speech_prob": 3.2060306693892926e-05}, {"id": 118, "seek": 76800, "start": 768.0, "end": 776.16, "text": " So if you have a latent distribution, which is not really resembling a Gaussian, then", "tokens": [50364, 407, 498, 291, 362, 257, 48994, 7316, 11, 597, 307, 406, 534, 20695, 1688, 257, 39148, 11, 550, 50772, 50772, 341, 1433, 510, 486, 312, 588, 2068, 11, 588, 1090, 13, 400, 562, 321, 3847, 257, 3034, 1478, 8399, 2058, 19866, 11, 51038, 51038, 321, 434, 516, 281, 312, 3097, 309, 538, 46608, 1293, 341, 1433, 670, 510, 293, 341, 1433, 670, 51346, 51346, 510, 13, 407, 264, 1433, 322, 264, 1411, 1011, 1252, 11, 652, 988, 300, 321, 393, 483, 646, 281, 264, 3380, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.10898955484454552, "compression_ratio": 1.6238532110091743, "no_speech_prob": 1.7763435607776046e-05}, {"id": 119, "seek": 76800, "start": 776.16, "end": 781.48, "text": " this term here will be very strong, very high. And when we train a variational auto encoder,", "tokens": [50364, 407, 498, 291, 362, 257, 48994, 7316, 11, 597, 307, 406, 534, 20695, 1688, 257, 39148, 11, 550, 50772, 50772, 341, 1433, 510, 486, 312, 588, 2068, 11, 588, 1090, 13, 400, 562, 321, 3847, 257, 3034, 1478, 8399, 2058, 19866, 11, 51038, 51038, 321, 434, 516, 281, 312, 3097, 309, 538, 46608, 1293, 341, 1433, 670, 510, 293, 341, 1433, 670, 51346, 51346, 510, 13, 407, 264, 1433, 322, 264, 1411, 1011, 1252, 11, 652, 988, 300, 321, 393, 483, 646, 281, 264, 3380, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.10898955484454552, "compression_ratio": 1.6238532110091743, "no_speech_prob": 1.7763435607776046e-05}, {"id": 120, "seek": 76800, "start": 781.48, "end": 787.64, "text": " we're going to be training it by minimizing both this term over here and this term over", "tokens": [50364, 407, 498, 291, 362, 257, 48994, 7316, 11, 597, 307, 406, 534, 20695, 1688, 257, 39148, 11, 550, 50772, 50772, 341, 1433, 510, 486, 312, 588, 2068, 11, 588, 1090, 13, 400, 562, 321, 3847, 257, 3034, 1478, 8399, 2058, 19866, 11, 51038, 51038, 321, 434, 516, 281, 312, 3097, 309, 538, 46608, 1293, 341, 1433, 670, 510, 293, 341, 1433, 670, 51346, 51346, 510, 13, 407, 264, 1433, 322, 264, 1411, 1011, 1252, 11, 652, 988, 300, 321, 393, 483, 646, 281, 264, 3380, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.10898955484454552, "compression_ratio": 1.6238532110091743, "no_speech_prob": 1.7763435607776046e-05}, {"id": 121, "seek": 76800, "start": 787.64, "end": 793.12, "text": " here. So the term on the left hand side, make sure that we can get back to the original", "tokens": [50364, 407, 498, 291, 362, 257, 48994, 7316, 11, 597, 307, 406, 534, 20695, 1688, 257, 39148, 11, 550, 50772, 50772, 341, 1433, 510, 486, 312, 588, 2068, 11, 588, 1090, 13, 400, 562, 321, 3847, 257, 3034, 1478, 8399, 2058, 19866, 11, 51038, 51038, 321, 434, 516, 281, 312, 3097, 309, 538, 46608, 1293, 341, 1433, 670, 510, 293, 341, 1433, 670, 51346, 51346, 510, 13, 407, 264, 1433, 322, 264, 1411, 1011, 1252, 11, 652, 988, 300, 321, 393, 483, 646, 281, 264, 3380, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.10898955484454552, "compression_ratio": 1.6238532110091743, "no_speech_prob": 1.7763435607776046e-05}, {"id": 122, "seek": 79312, "start": 793.12, "end": 799.5600000000001, "text": " position. The term on the right hand side enforce some structure in the latent space", "tokens": [50364, 2535, 13, 440, 1433, 322, 264, 558, 1011, 1252, 24825, 512, 3877, 294, 264, 48994, 1901, 50686, 50686, 570, 5911, 321, 2759, 380, 312, 1075, 281, 6889, 490, 456, 562, 321, 1116, 411, 281, 764, 341, 979, 19866, 51046, 51046, 382, 257, 1337, 1166, 2316, 13, 1033, 13, 639, 307, 1310, 406, 886, 1850, 11, 457, 718, 385, 976, 291, 257, 707, 51314, 51314, 857, 544, 721, 281, 519, 466, 13, 407, 577, 360, 321, 767, 1884, 613, 48994, 9102, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.13517001696995326, "compression_ratio": 1.5580357142857142, "no_speech_prob": 1.3196827239880804e-05}, {"id": 123, "seek": 79312, "start": 799.5600000000001, "end": 806.76, "text": " because otherwise we wouldn't be able to sample from there when we'd like to use this decoder", "tokens": [50364, 2535, 13, 440, 1433, 322, 264, 558, 1011, 1252, 24825, 512, 3877, 294, 264, 48994, 1901, 50686, 50686, 570, 5911, 321, 2759, 380, 312, 1075, 281, 6889, 490, 456, 562, 321, 1116, 411, 281, 764, 341, 979, 19866, 51046, 51046, 382, 257, 1337, 1166, 2316, 13, 1033, 13, 639, 307, 1310, 406, 886, 1850, 11, 457, 718, 385, 976, 291, 257, 707, 51314, 51314, 857, 544, 721, 281, 519, 466, 13, 407, 577, 360, 321, 767, 1884, 613, 48994, 9102, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.13517001696995326, "compression_ratio": 1.5580357142857142, "no_speech_prob": 1.3196827239880804e-05}, {"id": 124, "seek": 79312, "start": 806.76, "end": 812.12, "text": " as a generative model. Okay. This is maybe not too clear, but let me give you a little", "tokens": [50364, 2535, 13, 440, 1433, 322, 264, 558, 1011, 1252, 24825, 512, 3877, 294, 264, 48994, 1901, 50686, 50686, 570, 5911, 321, 2759, 380, 312, 1075, 281, 6889, 490, 456, 562, 321, 1116, 411, 281, 764, 341, 979, 19866, 51046, 51046, 382, 257, 1337, 1166, 2316, 13, 1033, 13, 639, 307, 1310, 406, 886, 1850, 11, 457, 718, 385, 976, 291, 257, 707, 51314, 51314, 857, 544, 721, 281, 519, 466, 13, 407, 577, 360, 321, 767, 1884, 613, 48994, 9102, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.13517001696995326, "compression_ratio": 1.5580357142857142, "no_speech_prob": 1.3196827239880804e-05}, {"id": 125, "seek": 79312, "start": 812.12, "end": 819.6, "text": " bit more things to think about. So how do we actually create these latent variables", "tokens": [50364, 2535, 13, 440, 1433, 322, 264, 558, 1011, 1252, 24825, 512, 3877, 294, 264, 48994, 1901, 50686, 50686, 570, 5911, 321, 2759, 380, 312, 1075, 281, 6889, 490, 456, 562, 321, 1116, 411, 281, 764, 341, 979, 19866, 51046, 51046, 382, 257, 1337, 1166, 2316, 13, 1033, 13, 639, 307, 1310, 406, 886, 1850, 11, 457, 718, 385, 976, 291, 257, 707, 51314, 51314, 857, 544, 721, 281, 519, 466, 13, 407, 577, 360, 321, 767, 1884, 613, 48994, 9102, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.13517001696995326, "compression_ratio": 1.5580357142857142, "no_speech_prob": 1.3196827239880804e-05}, {"id": 126, "seek": 81960, "start": 819.6, "end": 830.32, "text": " of z? So my z is simply going to be my mean E of z plus some noise epsilon, which is a", "tokens": [50364, 295, 710, 30, 407, 452, 710, 307, 2935, 516, 281, 312, 452, 914, 462, 295, 710, 1804, 512, 5658, 17889, 11, 597, 307, 257, 50900, 50900, 6889, 490, 257, 2710, 7316, 11, 597, 307, 411, 257, 2710, 2120, 592, 3504, 473, 39148, 7316, 51226, 51226, 365, 4018, 914, 293, 6575, 16367, 382, 264, 49851, 719, 16367, 11, 597, 575, 1184, 6542, 51610, 51610], "temperature": 0.0, "avg_logprob": -0.17517220423771784, "compression_ratio": 1.5798816568047338, "no_speech_prob": 3.4774264349834993e-05}, {"id": 127, "seek": 81960, "start": 830.32, "end": 836.84, "text": " sample from a normal distribution, which is like a normal multivariate Gaussian distribution", "tokens": [50364, 295, 710, 30, 407, 452, 710, 307, 2935, 516, 281, 312, 452, 914, 462, 295, 710, 1804, 512, 5658, 17889, 11, 597, 307, 257, 50900, 50900, 6889, 490, 257, 2710, 7316, 11, 597, 307, 411, 257, 2710, 2120, 592, 3504, 473, 39148, 7316, 51226, 51226, 365, 4018, 914, 293, 6575, 16367, 382, 264, 49851, 719, 16367, 11, 597, 575, 1184, 6542, 51610, 51610], "temperature": 0.0, "avg_logprob": -0.17517220423771784, "compression_ratio": 1.5798816568047338, "no_speech_prob": 3.4774264349834993e-05}, {"id": 128, "seek": 81960, "start": 836.84, "end": 844.52, "text": " with zero mean and identity metrics as the covariance metrics, which has each component", "tokens": [50364, 295, 710, 30, 407, 452, 710, 307, 2935, 516, 281, 312, 452, 914, 462, 295, 710, 1804, 512, 5658, 17889, 11, 597, 307, 257, 50900, 50900, 6889, 490, 257, 2710, 7316, 11, 597, 307, 411, 257, 2710, 2120, 592, 3504, 473, 39148, 7316, 51226, 51226, 365, 4018, 914, 293, 6575, 16367, 382, 264, 49851, 719, 16367, 11, 597, 575, 1184, 6542, 51610, 51610], "temperature": 0.0, "avg_logprob": -0.17517220423771784, "compression_ratio": 1.5798816568047338, "no_speech_prob": 3.4774264349834993e-05}, {"id": 129, "seek": 84452, "start": 844.52, "end": 850.56, "text": " multiplied by the standard deviation. So you should be familiar with this equation here", "tokens": [50364, 17207, 538, 264, 3832, 25163, 13, 407, 291, 820, 312, 4963, 365, 341, 5367, 510, 50666, 50666, 322, 264, 1192, 558, 13, 639, 307, 577, 291, 9610, 1220, 257, 4974, 7006, 17889, 11, 597, 797, 307, 50958, 50958, 257, 2710, 11, 291, 362, 281, 764, 341, 733, 295, 1085, 12835, 2398, 2144, 294, 1668, 281, 483, 257, 39148, 300, 575, 257, 2685, 51344, 51344, 914, 293, 257, 2685, 21977, 13, 1033, 13, 440, 5658, 294, 264, 48994, 9102, 307, 445, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.1651416506086077, "compression_ratio": 1.5874439461883407, "no_speech_prob": 1.3416058209259063e-05}, {"id": 130, "seek": 84452, "start": 850.56, "end": 856.4, "text": " on the top right. This is how you rescale a random variable epsilon, which again is", "tokens": [50364, 17207, 538, 264, 3832, 25163, 13, 407, 291, 820, 312, 4963, 365, 341, 5367, 510, 50666, 50666, 322, 264, 1192, 558, 13, 639, 307, 577, 291, 9610, 1220, 257, 4974, 7006, 17889, 11, 597, 797, 307, 50958, 50958, 257, 2710, 11, 291, 362, 281, 764, 341, 733, 295, 1085, 12835, 2398, 2144, 294, 1668, 281, 483, 257, 39148, 300, 575, 257, 2685, 51344, 51344, 914, 293, 257, 2685, 21977, 13, 1033, 13, 440, 5658, 294, 264, 48994, 9102, 307, 445, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.1651416506086077, "compression_ratio": 1.5874439461883407, "no_speech_prob": 1.3416058209259063e-05}, {"id": 131, "seek": 84452, "start": 856.4, "end": 864.12, "text": " a normal, you have to use this kind of reparameterization in order to get a Gaussian that has a specific", "tokens": [50364, 17207, 538, 264, 3832, 25163, 13, 407, 291, 820, 312, 4963, 365, 341, 5367, 510, 50666, 50666, 322, 264, 1192, 558, 13, 639, 307, 577, 291, 9610, 1220, 257, 4974, 7006, 17889, 11, 597, 797, 307, 50958, 50958, 257, 2710, 11, 291, 362, 281, 764, 341, 733, 295, 1085, 12835, 2398, 2144, 294, 1668, 281, 483, 257, 39148, 300, 575, 257, 2685, 51344, 51344, 914, 293, 257, 2685, 21977, 13, 1033, 13, 440, 5658, 294, 264, 48994, 9102, 307, 445, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.1651416506086077, "compression_ratio": 1.5874439461883407, "no_speech_prob": 1.3416058209259063e-05}, {"id": 132, "seek": 84452, "start": 864.12, "end": 870.16, "text": " mean and a specific variance. Okay. The noise in the latent variables is just", "tokens": [50364, 17207, 538, 264, 3832, 25163, 13, 407, 291, 820, 312, 4963, 365, 341, 5367, 510, 50666, 50666, 322, 264, 1192, 558, 13, 639, 307, 577, 291, 9610, 1220, 257, 4974, 7006, 17889, 11, 597, 797, 307, 50958, 50958, 257, 2710, 11, 291, 362, 281, 764, 341, 733, 295, 1085, 12835, 2398, 2144, 294, 1668, 281, 483, 257, 39148, 300, 575, 257, 2685, 51344, 51344, 914, 293, 257, 2685, 21977, 13, 1033, 13, 440, 5658, 294, 264, 48994, 9102, 307, 445, 51646, 51646], "temperature": 0.0, "avg_logprob": -0.1651416506086077, "compression_ratio": 1.5874439461883407, "no_speech_prob": 1.3416058209259063e-05}, {"id": 133, "seek": 87016, "start": 870.16, "end": 876.16, "text": " the encoded version of the noise introduced in the input. So there is no noise in the", "tokens": [50364, 264, 2058, 12340, 3037, 295, 264, 5658, 7268, 294, 264, 4846, 13, 407, 456, 307, 572, 5658, 294, 264, 50664, 50664, 4846, 13, 509, 829, 264, 4846, 1854, 264, 2058, 19866, 293, 550, 264, 2058, 19866, 2709, 291, 732, 9834, 11, 50978, 50978, 462, 293, 21977, 13, 1133, 291, 6889, 490, 341, 7316, 11, 291, 1936, 483, 710, 293, 437, 51356, 51356, 291, 483, 510, 11, 309, 311, 2935, 11, 291, 393, 2464, 264, 21179, 644, 382, 341, 472, 13, 407, 264, 1154, 51682, 51682], "temperature": 0.0, "avg_logprob": -0.15441770663206605, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.4730400835105684e-05}, {"id": 134, "seek": 87016, "start": 876.16, "end": 882.4399999999999, "text": " input. You put the input inside the encoder and then the encoder gives you two parameters,", "tokens": [50364, 264, 2058, 12340, 3037, 295, 264, 5658, 7268, 294, 264, 4846, 13, 407, 456, 307, 572, 5658, 294, 264, 50664, 50664, 4846, 13, 509, 829, 264, 4846, 1854, 264, 2058, 19866, 293, 550, 264, 2058, 19866, 2709, 291, 732, 9834, 11, 50978, 50978, 462, 293, 21977, 13, 1133, 291, 6889, 490, 341, 7316, 11, 291, 1936, 483, 710, 293, 437, 51356, 51356, 291, 483, 510, 11, 309, 311, 2935, 11, 291, 393, 2464, 264, 21179, 644, 382, 341, 472, 13, 407, 264, 1154, 51682, 51682], "temperature": 0.0, "avg_logprob": -0.15441770663206605, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.4730400835105684e-05}, {"id": 135, "seek": 87016, "start": 882.4399999999999, "end": 890.0, "text": " E and variance. When you sample from this distribution, you basically get z and what", "tokens": [50364, 264, 2058, 12340, 3037, 295, 264, 5658, 7268, 294, 264, 4846, 13, 407, 456, 307, 572, 5658, 294, 264, 50664, 50664, 4846, 13, 509, 829, 264, 4846, 1854, 264, 2058, 19866, 293, 550, 264, 2058, 19866, 2709, 291, 732, 9834, 11, 50978, 50978, 462, 293, 21977, 13, 1133, 291, 6889, 490, 341, 7316, 11, 291, 1936, 483, 710, 293, 437, 51356, 51356, 291, 483, 510, 11, 309, 311, 2935, 11, 291, 393, 2464, 264, 21179, 644, 382, 341, 472, 13, 407, 264, 1154, 51682, 51682], "temperature": 0.0, "avg_logprob": -0.15441770663206605, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.4730400835105684e-05}, {"id": 136, "seek": 87016, "start": 890.0, "end": 896.52, "text": " you get here, it's simply, you can write the sampling part as this one. So the problem", "tokens": [50364, 264, 2058, 12340, 3037, 295, 264, 5658, 7268, 294, 264, 4846, 13, 407, 456, 307, 572, 5658, 294, 264, 50664, 50664, 4846, 13, 509, 829, 264, 4846, 1854, 264, 2058, 19866, 293, 550, 264, 2058, 19866, 2709, 291, 732, 9834, 11, 50978, 50978, 462, 293, 21977, 13, 1133, 291, 6889, 490, 341, 7316, 11, 291, 1936, 483, 710, 293, 437, 51356, 51356, 291, 483, 510, 11, 309, 311, 2935, 11, 291, 393, 2464, 264, 21179, 644, 382, 341, 472, 13, 407, 264, 1154, 51682, 51682], "temperature": 0.0, "avg_logprob": -0.15441770663206605, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.4730400835105684e-05}, {"id": 137, "seek": 89652, "start": 896.52, "end": 901.52, "text": " with sampling is that we don't know how to perform back propagation through a sampling", "tokens": [50364, 365, 21179, 307, 300, 321, 500, 380, 458, 577, 281, 2042, 646, 38377, 807, 257, 21179, 50614, 50614, 10088, 13, 5135, 11, 456, 307, 572, 636, 281, 2042, 646, 38377, 807, 21179, 570, 50890, 50890, 341, 472, 307, 445, 17746, 257, 777, 710, 13, 407, 577, 360, 321, 483, 2771, 2448, 807, 341, 10088, 294, 51186, 51186, 1668, 281, 3847, 264, 2058, 19866, 30, 400, 370, 341, 393, 312, 1096, 498, 291, 764, 341, 4282, 11, 597, 307, 1219, 51477, 51477, 264, 1085, 12835, 2398, 2144, 4282, 13, 440, 1085, 12835, 2398, 2144, 4282, 4045, 291, 281, 5109, 428, 21179, 51839, 51839], "temperature": 0.0, "avg_logprob": -0.11235785484313965, "compression_ratio": 1.8739495798319328, "no_speech_prob": 3.802828359766863e-05}, {"id": 138, "seek": 89652, "start": 901.52, "end": 907.04, "text": " module. Actually, there is no way to perform back propagation through sampling because", "tokens": [50364, 365, 21179, 307, 300, 321, 500, 380, 458, 577, 281, 2042, 646, 38377, 807, 257, 21179, 50614, 50614, 10088, 13, 5135, 11, 456, 307, 572, 636, 281, 2042, 646, 38377, 807, 21179, 570, 50890, 50890, 341, 472, 307, 445, 17746, 257, 777, 710, 13, 407, 577, 360, 321, 483, 2771, 2448, 807, 341, 10088, 294, 51186, 51186, 1668, 281, 3847, 264, 2058, 19866, 30, 400, 370, 341, 393, 312, 1096, 498, 291, 764, 341, 4282, 11, 597, 307, 1219, 51477, 51477, 264, 1085, 12835, 2398, 2144, 4282, 13, 440, 1085, 12835, 2398, 2144, 4282, 4045, 291, 281, 5109, 428, 21179, 51839, 51839], "temperature": 0.0, "avg_logprob": -0.11235785484313965, "compression_ratio": 1.8739495798319328, "no_speech_prob": 3.802828359766863e-05}, {"id": 139, "seek": 89652, "start": 907.04, "end": 912.96, "text": " this one is just generating a new z. So how do we get gradients through this module in", "tokens": [50364, 365, 21179, 307, 300, 321, 500, 380, 458, 577, 281, 2042, 646, 38377, 807, 257, 21179, 50614, 50614, 10088, 13, 5135, 11, 456, 307, 572, 636, 281, 2042, 646, 38377, 807, 21179, 570, 50890, 50890, 341, 472, 307, 445, 17746, 257, 777, 710, 13, 407, 577, 360, 321, 483, 2771, 2448, 807, 341, 10088, 294, 51186, 51186, 1668, 281, 3847, 264, 2058, 19866, 30, 400, 370, 341, 393, 312, 1096, 498, 291, 764, 341, 4282, 11, 597, 307, 1219, 51477, 51477, 264, 1085, 12835, 2398, 2144, 4282, 13, 440, 1085, 12835, 2398, 2144, 4282, 4045, 291, 281, 5109, 428, 21179, 51839, 51839], "temperature": 0.0, "avg_logprob": -0.11235785484313965, "compression_ratio": 1.8739495798319328, "no_speech_prob": 3.802828359766863e-05}, {"id": 140, "seek": 89652, "start": 912.96, "end": 918.78, "text": " order to train the encoder? And so this can be done if you use this trick, which is called", "tokens": [50364, 365, 21179, 307, 300, 321, 500, 380, 458, 577, 281, 2042, 646, 38377, 807, 257, 21179, 50614, 50614, 10088, 13, 5135, 11, 456, 307, 572, 636, 281, 2042, 646, 38377, 807, 21179, 570, 50890, 50890, 341, 472, 307, 445, 17746, 257, 777, 710, 13, 407, 577, 360, 321, 483, 2771, 2448, 807, 341, 10088, 294, 51186, 51186, 1668, 281, 3847, 264, 2058, 19866, 30, 400, 370, 341, 393, 312, 1096, 498, 291, 764, 341, 4282, 11, 597, 307, 1219, 51477, 51477, 264, 1085, 12835, 2398, 2144, 4282, 13, 440, 1085, 12835, 2398, 2144, 4282, 4045, 291, 281, 5109, 428, 21179, 51839, 51839], "temperature": 0.0, "avg_logprob": -0.11235785484313965, "compression_ratio": 1.8739495798319328, "no_speech_prob": 3.802828359766863e-05}, {"id": 141, "seek": 89652, "start": 918.78, "end": 926.02, "text": " the reparameterization trick. The reparameterization trick allows you to express your sampling", "tokens": [50364, 365, 21179, 307, 300, 321, 500, 380, 458, 577, 281, 2042, 646, 38377, 807, 257, 21179, 50614, 50614, 10088, 13, 5135, 11, 456, 307, 572, 636, 281, 2042, 646, 38377, 807, 21179, 570, 50890, 50890, 341, 472, 307, 445, 17746, 257, 777, 710, 13, 407, 577, 360, 321, 483, 2771, 2448, 807, 341, 10088, 294, 51186, 51186, 1668, 281, 3847, 264, 2058, 19866, 30, 400, 370, 341, 393, 312, 1096, 498, 291, 764, 341, 4282, 11, 597, 307, 1219, 51477, 51477, 264, 1085, 12835, 2398, 2144, 4282, 13, 440, 1085, 12835, 2398, 2144, 4282, 4045, 291, 281, 5109, 428, 21179, 51839, 51839], "temperature": 0.0, "avg_logprob": -0.11235785484313965, "compression_ratio": 1.8739495798319328, "no_speech_prob": 3.802828359766863e-05}, {"id": 142, "seek": 92602, "start": 926.02, "end": 933.56, "text": " in terms of additions and multiplication, which we can differentiate through. The epsilon", "tokens": [50364, 294, 2115, 295, 35113, 293, 27290, 11, 597, 321, 393, 23203, 807, 13, 440, 17889, 50741, 50741, 307, 2935, 364, 4497, 4846, 300, 307, 1348, 490, 2035, 11, 8660, 13, 1042, 11, 321, 500, 380, 362, 51101, 51101, 604, 643, 281, 2845, 2771, 2448, 807, 341, 4846, 13, 440, 2771, 2448, 486, 312, 516, 807, 264, 27290, 51393, 51393, 293, 807, 264, 4500, 13, 407, 5699, 291, 362, 2771, 2448, 337, 3097, 264, 1185, 11, 264, 51715, 51715], "temperature": 0.0, "avg_logprob": -0.13572715520858764, "compression_ratio": 1.755980861244019, "no_speech_prob": 2.5430519599467516e-05}, {"id": 143, "seek": 92602, "start": 933.56, "end": 940.76, "text": " is simply an additional input that is coming from whatever, wherever. Well, we don't have", "tokens": [50364, 294, 2115, 295, 35113, 293, 27290, 11, 597, 321, 393, 23203, 807, 13, 440, 17889, 50741, 50741, 307, 2935, 364, 4497, 4846, 300, 307, 1348, 490, 2035, 11, 8660, 13, 1042, 11, 321, 500, 380, 362, 51101, 51101, 604, 643, 281, 2845, 2771, 2448, 807, 341, 4846, 13, 440, 2771, 2448, 486, 312, 516, 807, 264, 27290, 51393, 51393, 293, 807, 264, 4500, 13, 407, 5699, 291, 362, 2771, 2448, 337, 3097, 264, 1185, 11, 264, 51715, 51715], "temperature": 0.0, "avg_logprob": -0.13572715520858764, "compression_ratio": 1.755980861244019, "no_speech_prob": 2.5430519599467516e-05}, {"id": 144, "seek": 92602, "start": 940.76, "end": 946.6, "text": " any need to send gradients through this input. The gradients will be going through the multiplication", "tokens": [50364, 294, 2115, 295, 35113, 293, 27290, 11, 597, 321, 393, 23203, 807, 13, 440, 17889, 50741, 50741, 307, 2935, 364, 4497, 4846, 300, 307, 1348, 490, 2035, 11, 8660, 13, 1042, 11, 321, 500, 380, 362, 51101, 51101, 604, 643, 281, 2845, 2771, 2448, 807, 341, 4846, 13, 440, 2771, 2448, 486, 312, 516, 807, 264, 27290, 51393, 51393, 293, 807, 264, 4500, 13, 407, 5699, 291, 362, 2771, 2448, 337, 3097, 264, 1185, 11, 264, 51715, 51715], "temperature": 0.0, "avg_logprob": -0.13572715520858764, "compression_ratio": 1.755980861244019, "no_speech_prob": 2.5430519599467516e-05}, {"id": 145, "seek": 92602, "start": 946.6, "end": 953.04, "text": " and through the addition. So whenever you have gradients for training the system, the", "tokens": [50364, 294, 2115, 295, 35113, 293, 27290, 11, 597, 321, 393, 23203, 807, 13, 440, 17889, 50741, 50741, 307, 2935, 364, 4497, 4846, 300, 307, 1348, 490, 2035, 11, 8660, 13, 1042, 11, 321, 500, 380, 362, 51101, 51101, 604, 643, 281, 2845, 2771, 2448, 807, 341, 4846, 13, 440, 2771, 2448, 486, 312, 516, 807, 264, 27290, 51393, 51393, 293, 807, 264, 4500, 13, 407, 5699, 291, 362, 2771, 2448, 337, 3097, 264, 1185, 11, 264, 51715, 51715], "temperature": 0.0, "avg_logprob": -0.13572715520858764, "compression_ratio": 1.755980861244019, "no_speech_prob": 2.5430519599467516e-05}, {"id": 146, "seek": 95304, "start": 953.04, "end": 960.16, "text": " gradient comes down. And then here we can replace the sampling module with a addition", "tokens": [50364, 16235, 1487, 760, 13, 400, 550, 510, 321, 393, 7406, 264, 21179, 10088, 365, 257, 4500, 50720, 50720, 1296, 462, 1804, 264, 17889, 17207, 538, 264, 3732, 5593, 295, 264, 21977, 13, 9653, 300, 586, 51036, 51036, 291, 362, 4500, 11, 291, 458, 577, 281, 646, 2365, 807, 364, 4500, 13, 7504, 11, 291, 483, 2771, 2448, 51310, 51310, 337, 264, 2058, 19866, 11, 510, 257, 5598, 16235, 11, 293, 550, 291, 393, 14722, 264, 14641, 33733, 51566, 51566], "temperature": 0.0, "avg_logprob": -0.1752057252106843, "compression_ratio": 1.634703196347032, "no_speech_prob": 1.9796923879766837e-05}, {"id": 147, "seek": 95304, "start": 960.16, "end": 966.48, "text": " between E plus the epsilon multiplied by the square root of the variance. Such that now", "tokens": [50364, 16235, 1487, 760, 13, 400, 550, 510, 321, 393, 7406, 264, 21179, 10088, 365, 257, 4500, 50720, 50720, 1296, 462, 1804, 264, 17889, 17207, 538, 264, 3732, 5593, 295, 264, 21977, 13, 9653, 300, 586, 51036, 51036, 291, 362, 4500, 11, 291, 458, 577, 281, 646, 2365, 807, 364, 4500, 13, 7504, 11, 291, 483, 2771, 2448, 51310, 51310, 337, 264, 2058, 19866, 11, 510, 257, 5598, 16235, 11, 293, 550, 291, 393, 14722, 264, 14641, 33733, 51566, 51566], "temperature": 0.0, "avg_logprob": -0.1752057252106843, "compression_ratio": 1.634703196347032, "no_speech_prob": 1.9796923879766837e-05}, {"id": 148, "seek": 95304, "start": 966.48, "end": 971.9599999999999, "text": " you have addition, you know how to back prop through an addition. Therefore, you get gradients", "tokens": [50364, 16235, 1487, 760, 13, 400, 550, 510, 321, 393, 7406, 264, 21179, 10088, 365, 257, 4500, 50720, 50720, 1296, 462, 1804, 264, 17889, 17207, 538, 264, 3732, 5593, 295, 264, 21977, 13, 9653, 300, 586, 51036, 51036, 291, 362, 4500, 11, 291, 458, 577, 281, 646, 2365, 807, 364, 4500, 13, 7504, 11, 291, 483, 2771, 2448, 51310, 51310, 337, 264, 2058, 19866, 11, 510, 257, 5598, 16235, 11, 293, 550, 291, 393, 14722, 264, 14641, 33733, 51566, 51566], "temperature": 0.0, "avg_logprob": -0.1752057252106843, "compression_ratio": 1.634703196347032, "no_speech_prob": 1.9796923879766837e-05}, {"id": 149, "seek": 95304, "start": 971.9599999999999, "end": 977.0799999999999, "text": " for the encoder, here a output gradient, and then you can compute the partial derivatives", "tokens": [50364, 16235, 1487, 760, 13, 400, 550, 510, 321, 393, 7406, 264, 21179, 10088, 365, 257, 4500, 50720, 50720, 1296, 462, 1804, 264, 17889, 17207, 538, 264, 3732, 5593, 295, 264, 21977, 13, 9653, 300, 586, 51036, 51036, 291, 362, 4500, 11, 291, 458, 577, 281, 646, 2365, 807, 364, 4500, 13, 7504, 11, 291, 483, 2771, 2448, 51310, 51310, 337, 264, 2058, 19866, 11, 510, 257, 5598, 16235, 11, 293, 550, 291, 393, 14722, 264, 14641, 33733, 51566, 51566], "temperature": 0.0, "avg_logprob": -0.1752057252106843, "compression_ratio": 1.634703196347032, "no_speech_prob": 1.9796923879766837e-05}, {"id": 150, "seek": 97708, "start": 977.08, "end": 987.4000000000001, "text": " of the final cost with respect to the parameters in this module. So just in an intuition part,", "tokens": [50364, 295, 264, 2572, 2063, 365, 3104, 281, 264, 9834, 294, 341, 10088, 13, 407, 445, 294, 364, 24002, 644, 11, 50880, 50880, 341, 47991, 510, 4045, 385, 281, 24825, 257, 3877, 294, 264, 48994, 1901, 13, 663, 311, 437, 321, 519, 51246, 51246, 466, 13, 663, 311, 577, 286, 1116, 411, 291, 281, 519, 466, 341, 47991, 1433, 13, 400, 370, 718, 311, 767, 2573, 51520, 51520], "temperature": 0.0, "avg_logprob": -0.11583644756372424, "compression_ratio": 1.5139664804469273, "no_speech_prob": 6.23361484031193e-06}, {"id": 151, "seek": 97708, "start": 987.4000000000001, "end": 994.72, "text": " this KL here allows me to enforce a structure in the latent space. That's what we think", "tokens": [50364, 295, 264, 2572, 2063, 365, 3104, 281, 264, 9834, 294, 341, 10088, 13, 407, 445, 294, 364, 24002, 644, 11, 50880, 50880, 341, 47991, 510, 4045, 385, 281, 24825, 257, 3877, 294, 264, 48994, 1901, 13, 663, 311, 437, 321, 519, 51246, 51246, 466, 13, 663, 311, 577, 286, 1116, 411, 291, 281, 519, 466, 341, 47991, 1433, 13, 400, 370, 718, 311, 767, 2573, 51520, 51520], "temperature": 0.0, "avg_logprob": -0.11583644756372424, "compression_ratio": 1.5139664804469273, "no_speech_prob": 6.23361484031193e-06}, {"id": 152, "seek": 97708, "start": 994.72, "end": 1000.2, "text": " about. That's how I'd like you to think about this KL term. And so let's actually figure", "tokens": [50364, 295, 264, 2572, 2063, 365, 3104, 281, 264, 9834, 294, 341, 10088, 13, 407, 445, 294, 364, 24002, 644, 11, 50880, 50880, 341, 47991, 510, 4045, 385, 281, 24825, 257, 3877, 294, 264, 48994, 1901, 13, 663, 311, 437, 321, 519, 51246, 51246, 466, 13, 663, 311, 577, 286, 1116, 411, 291, 281, 519, 466, 341, 47991, 1433, 13, 400, 370, 718, 311, 767, 2573, 51520, 51520], "temperature": 0.0, "avg_logprob": -0.11583644756372424, "compression_ratio": 1.5139664804469273, "no_speech_prob": 6.23361484031193e-06}, {"id": 153, "seek": 100020, "start": 1000.2, "end": 1009.0, "text": " out how this stuff works. So we have two terms in my pair sample loss. We have the first", "tokens": [50364, 484, 577, 341, 1507, 1985, 13, 407, 321, 362, 732, 2115, 294, 452, 6119, 6889, 4470, 13, 492, 362, 264, 700, 50804, 50804, 472, 11, 597, 307, 264, 31565, 4470, 11, 293, 550, 456, 307, 264, 1150, 1433, 11, 597, 307, 516, 51020, 51020, 281, 312, 341, 47991, 11, 341, 4972, 30867, 1433, 13, 407, 321, 362, 512, 710, 311, 294, 341, 1389, 11, 597, 366, 51402, 51402, 41225, 11, 16295, 294, 341, 1389, 13, 1545, 456, 366, 16295, 30, 1436, 498, 321, 909, 512, 4497, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.15760922961764864, "compression_ratio": 1.7004830917874396, "no_speech_prob": 6.236465196707286e-06}, {"id": 154, "seek": 100020, "start": 1009.0, "end": 1013.32, "text": " one, which is the reconstruction loss, and then there is the second term, which is going", "tokens": [50364, 484, 577, 341, 1507, 1985, 13, 407, 321, 362, 732, 2115, 294, 452, 6119, 6889, 4470, 13, 492, 362, 264, 700, 50804, 50804, 472, 11, 597, 307, 264, 31565, 4470, 11, 293, 550, 456, 307, 264, 1150, 1433, 11, 597, 307, 516, 51020, 51020, 281, 312, 341, 47991, 11, 341, 4972, 30867, 1433, 13, 407, 321, 362, 512, 710, 311, 294, 341, 1389, 11, 597, 366, 51402, 51402, 41225, 11, 16295, 294, 341, 1389, 13, 1545, 456, 366, 16295, 30, 1436, 498, 321, 909, 512, 4497, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.15760922961764864, "compression_ratio": 1.7004830917874396, "no_speech_prob": 6.236465196707286e-06}, {"id": 155, "seek": 100020, "start": 1013.32, "end": 1020.96, "text": " to be this KL, this relative entropy term. So we have some z's in this case, which are", "tokens": [50364, 484, 577, 341, 1507, 1985, 13, 407, 321, 362, 732, 2115, 294, 452, 6119, 6889, 4470, 13, 492, 362, 264, 700, 50804, 50804, 472, 11, 597, 307, 264, 31565, 4470, 11, 293, 550, 456, 307, 264, 1150, 1433, 11, 597, 307, 516, 51020, 51020, 281, 312, 341, 47991, 11, 341, 4972, 30867, 1433, 13, 407, 321, 362, 512, 710, 311, 294, 341, 1389, 11, 597, 366, 51402, 51402, 41225, 11, 16295, 294, 341, 1389, 13, 1545, 456, 366, 16295, 30, 1436, 498, 321, 909, 512, 4497, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.15760922961764864, "compression_ratio": 1.7004830917874396, "no_speech_prob": 6.236465196707286e-06}, {"id": 156, "seek": 100020, "start": 1020.96, "end": 1028.24, "text": " spheres, bubbles in this case. Why there are bubbles? Because if we add some additional", "tokens": [50364, 484, 577, 341, 1507, 1985, 13, 407, 321, 362, 732, 2115, 294, 452, 6119, 6889, 4470, 13, 492, 362, 264, 700, 50804, 50804, 472, 11, 597, 307, 264, 31565, 4470, 11, 293, 550, 456, 307, 264, 1150, 1433, 11, 597, 307, 516, 51020, 51020, 281, 312, 341, 47991, 11, 341, 4972, 30867, 1433, 13, 407, 321, 362, 512, 710, 311, 294, 341, 1389, 11, 597, 366, 51402, 51402, 41225, 11, 16295, 294, 341, 1389, 13, 1545, 456, 366, 16295, 30, 1436, 498, 321, 909, 512, 4497, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.15760922961764864, "compression_ratio": 1.7004830917874396, "no_speech_prob": 6.236465196707286e-06}, {"id": 157, "seek": 102824, "start": 1028.24, "end": 1034.64, "text": " noise, we had the means, and the means are basically the center of these points. So you", "tokens": [50364, 5658, 11, 321, 632, 264, 1355, 11, 293, 264, 1355, 366, 1936, 264, 3056, 295, 613, 2793, 13, 407, 291, 50684, 50684, 362, 472, 914, 510, 11, 472, 914, 670, 510, 11, 472, 914, 670, 510, 11, 472, 914, 670, 510, 13, 400, 550, 51174, 51174, 437, 264, 31565, 1433, 307, 516, 281, 312, 884, 307, 264, 3480, 13, 407, 498, 613, 16295, 51468, 51468, 19959, 11, 437, 775, 309, 1051, 30, 407, 498, 291, 362, 472, 914, 510, 293, 1071, 914, 11, 411, 472, 12212, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.12662067413330078, "compression_ratio": 1.940217391304348, "no_speech_prob": 1.7501088223070838e-05}, {"id": 158, "seek": 102824, "start": 1034.64, "end": 1044.44, "text": " have one mean here, one mean over here, one mean over here, one mean over here. And then", "tokens": [50364, 5658, 11, 321, 632, 264, 1355, 11, 293, 264, 1355, 366, 1936, 264, 3056, 295, 613, 2793, 13, 407, 291, 50684, 50684, 362, 472, 914, 510, 11, 472, 914, 670, 510, 11, 472, 914, 670, 510, 11, 472, 914, 670, 510, 13, 400, 550, 51174, 51174, 437, 264, 31565, 1433, 307, 516, 281, 312, 884, 307, 264, 3480, 13, 407, 498, 613, 16295, 51468, 51468, 19959, 11, 437, 775, 309, 1051, 30, 407, 498, 291, 362, 472, 914, 510, 293, 1071, 914, 11, 411, 472, 12212, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.12662067413330078, "compression_ratio": 1.940217391304348, "no_speech_prob": 1.7501088223070838e-05}, {"id": 159, "seek": 102824, "start": 1044.44, "end": 1050.32, "text": " what the reconstruction term is going to be doing is the following. So if these bubbles", "tokens": [50364, 5658, 11, 321, 632, 264, 1355, 11, 293, 264, 1355, 366, 1936, 264, 3056, 295, 613, 2793, 13, 407, 291, 50684, 50684, 362, 472, 914, 510, 11, 472, 914, 670, 510, 11, 472, 914, 670, 510, 11, 472, 914, 670, 510, 13, 400, 550, 51174, 51174, 437, 264, 31565, 1433, 307, 516, 281, 312, 884, 307, 264, 3480, 13, 407, 498, 613, 16295, 51468, 51468, 19959, 11, 437, 775, 309, 1051, 30, 407, 498, 291, 362, 472, 914, 510, 293, 1071, 914, 11, 411, 472, 12212, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.12662067413330078, "compression_ratio": 1.940217391304348, "no_speech_prob": 1.7501088223070838e-05}, {"id": 160, "seek": 102824, "start": 1050.32, "end": 1057.08, "text": " overlap, what does it happen? So if you have one mean here and another mean, like one bubble", "tokens": [50364, 5658, 11, 321, 632, 264, 1355, 11, 293, 264, 1355, 366, 1936, 264, 3056, 295, 613, 2793, 13, 407, 291, 50684, 50684, 362, 472, 914, 510, 11, 472, 914, 670, 510, 11, 472, 914, 670, 510, 11, 472, 914, 670, 510, 13, 400, 550, 51174, 51174, 437, 264, 31565, 1433, 307, 516, 281, 312, 884, 307, 264, 3480, 13, 407, 498, 613, 16295, 51468, 51468, 19959, 11, 437, 775, 309, 1051, 30, 407, 498, 291, 362, 472, 914, 510, 293, 1071, 914, 11, 411, 472, 12212, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.12662067413330078, "compression_ratio": 1.940217391304348, "no_speech_prob": 1.7501088223070838e-05}, {"id": 161, "seek": 105708, "start": 1057.08, "end": 1063.76, "text": " here and another bubble that is overlapping, and there is a region where there is intersection,", "tokens": [50364, 510, 293, 1071, 12212, 300, 307, 33535, 11, 293, 456, 307, 257, 4458, 689, 456, 307, 15236, 11, 50698, 50698, 577, 393, 291, 31499, 613, 732, 2793, 1780, 322, 30, 509, 393, 380, 13, 2014, 291, 3480, 370, 1400, 30, 759, 51084, 51084, 291, 362, 257, 12212, 510, 293, 550, 291, 362, 1071, 12212, 510, 11, 439, 2793, 322, 341, 12212, 510, 51376, 51376, 486, 312, 31499, 292, 281, 264, 3380, 4846, 510, 13, 407, 291, 722, 490, 364, 3380, 935, 11, 51612, 51612, 291, 352, 281, 264, 48994, 1901, 670, 510, 11, 293, 550, 291, 909, 512, 5658, 13, 509, 767, 362, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.10491484347904954, "compression_ratio": 1.887029288702929, "no_speech_prob": 6.960858172533335e-06}, {"id": 162, "seek": 105708, "start": 1063.76, "end": 1071.48, "text": " how can you reconstruct these two points later on? You can't. Are you following so far? If", "tokens": [50364, 510, 293, 1071, 12212, 300, 307, 33535, 11, 293, 456, 307, 257, 4458, 689, 456, 307, 15236, 11, 50698, 50698, 577, 393, 291, 31499, 613, 732, 2793, 1780, 322, 30, 509, 393, 380, 13, 2014, 291, 3480, 370, 1400, 30, 759, 51084, 51084, 291, 362, 257, 12212, 510, 293, 550, 291, 362, 1071, 12212, 510, 11, 439, 2793, 322, 341, 12212, 510, 51376, 51376, 486, 312, 31499, 292, 281, 264, 3380, 4846, 510, 13, 407, 291, 722, 490, 364, 3380, 935, 11, 51612, 51612, 291, 352, 281, 264, 48994, 1901, 670, 510, 11, 293, 550, 291, 909, 512, 5658, 13, 509, 767, 362, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.10491484347904954, "compression_ratio": 1.887029288702929, "no_speech_prob": 6.960858172533335e-06}, {"id": 163, "seek": 105708, "start": 1071.48, "end": 1077.32, "text": " you have a bubble here and then you have another bubble here, all points on this bubble here", "tokens": [50364, 510, 293, 1071, 12212, 300, 307, 33535, 11, 293, 456, 307, 257, 4458, 689, 456, 307, 15236, 11, 50698, 50698, 577, 393, 291, 31499, 613, 732, 2793, 1780, 322, 30, 509, 393, 380, 13, 2014, 291, 3480, 370, 1400, 30, 759, 51084, 51084, 291, 362, 257, 12212, 510, 293, 550, 291, 362, 1071, 12212, 510, 11, 439, 2793, 322, 341, 12212, 510, 51376, 51376, 486, 312, 31499, 292, 281, 264, 3380, 4846, 510, 13, 407, 291, 722, 490, 364, 3380, 935, 11, 51612, 51612, 291, 352, 281, 264, 48994, 1901, 670, 510, 11, 293, 550, 291, 909, 512, 5658, 13, 509, 767, 362, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.10491484347904954, "compression_ratio": 1.887029288702929, "no_speech_prob": 6.960858172533335e-06}, {"id": 164, "seek": 105708, "start": 1077.32, "end": 1082.04, "text": " will be reconstructed to the original input here. So you start from an original point,", "tokens": [50364, 510, 293, 1071, 12212, 300, 307, 33535, 11, 293, 456, 307, 257, 4458, 689, 456, 307, 15236, 11, 50698, 50698, 577, 393, 291, 31499, 613, 732, 2793, 1780, 322, 30, 509, 393, 380, 13, 2014, 291, 3480, 370, 1400, 30, 759, 51084, 51084, 291, 362, 257, 12212, 510, 293, 550, 291, 362, 1071, 12212, 510, 11, 439, 2793, 322, 341, 12212, 510, 51376, 51376, 486, 312, 31499, 292, 281, 264, 3380, 4846, 510, 13, 407, 291, 722, 490, 364, 3380, 935, 11, 51612, 51612, 291, 352, 281, 264, 48994, 1901, 670, 510, 11, 293, 550, 291, 909, 512, 5658, 13, 509, 767, 362, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.10491484347904954, "compression_ratio": 1.887029288702929, "no_speech_prob": 6.960858172533335e-06}, {"id": 165, "seek": 105708, "start": 1082.04, "end": 1086.12, "text": " you go to the latent space over here, and then you add some noise. You actually have", "tokens": [50364, 510, 293, 1071, 12212, 300, 307, 33535, 11, 293, 456, 307, 257, 4458, 689, 456, 307, 15236, 11, 50698, 50698, 577, 393, 291, 31499, 613, 732, 2793, 1780, 322, 30, 509, 393, 380, 13, 2014, 291, 3480, 370, 1400, 30, 759, 51084, 51084, 291, 362, 257, 12212, 510, 293, 550, 291, 362, 1071, 12212, 510, 11, 439, 2793, 322, 341, 12212, 510, 51376, 51376, 486, 312, 31499, 292, 281, 264, 3380, 4846, 510, 13, 407, 291, 722, 490, 364, 3380, 935, 11, 51612, 51612, 291, 352, 281, 264, 48994, 1901, 670, 510, 11, 293, 550, 291, 909, 512, 5658, 13, 509, 767, 362, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.10491484347904954, "compression_ratio": 1.887029288702929, "no_speech_prob": 6.960858172533335e-06}, {"id": 166, "seek": 108612, "start": 1086.12, "end": 1092.1599999999999, "text": " a volume here. Then you take another point, and this other point, it gets reconstructed", "tokens": [50364, 257, 5523, 510, 13, 1396, 291, 747, 1071, 935, 11, 293, 341, 661, 935, 11, 309, 2170, 31499, 292, 50666, 50666, 510, 13, 1779, 586, 11, 498, 613, 732, 1074, 19959, 11, 577, 393, 291, 31499, 264, 2793, 670, 510, 30, 51108, 51108, 407, 498, 264, 2793, 366, 294, 341, 12212, 11, 286, 1116, 411, 281, 352, 646, 281, 264, 3380, 935, 510, 13, 759, 51336, 51336, 264, 2793, 366, 294, 341, 12212, 11, 286, 1116, 411, 281, 352, 281, 264, 661, 935, 13, 583, 498, 2793, 366, 670, 875, 3320, 11, 51582, 51582], "temperature": 0.0, "avg_logprob": -0.11271690328915913, "compression_ratio": 1.9408602150537635, "no_speech_prob": 4.1978000808740035e-05}, {"id": 167, "seek": 108612, "start": 1092.1599999999999, "end": 1101.0, "text": " here. Right now, if these two guys overlap, how can you reconstruct the points over here?", "tokens": [50364, 257, 5523, 510, 13, 1396, 291, 747, 1071, 935, 11, 293, 341, 661, 935, 11, 309, 2170, 31499, 292, 50666, 50666, 510, 13, 1779, 586, 11, 498, 613, 732, 1074, 19959, 11, 577, 393, 291, 31499, 264, 2793, 670, 510, 30, 51108, 51108, 407, 498, 264, 2793, 366, 294, 341, 12212, 11, 286, 1116, 411, 281, 352, 646, 281, 264, 3380, 935, 510, 13, 759, 51336, 51336, 264, 2793, 366, 294, 341, 12212, 11, 286, 1116, 411, 281, 352, 281, 264, 661, 935, 13, 583, 498, 2793, 366, 670, 875, 3320, 11, 51582, 51582], "temperature": 0.0, "avg_logprob": -0.11271690328915913, "compression_ratio": 1.9408602150537635, "no_speech_prob": 4.1978000808740035e-05}, {"id": 168, "seek": 108612, "start": 1101.0, "end": 1105.56, "text": " So if the points are in this bubble, I'd like to go back to the original point here. If", "tokens": [50364, 257, 5523, 510, 13, 1396, 291, 747, 1071, 935, 11, 293, 341, 661, 935, 11, 309, 2170, 31499, 292, 50666, 50666, 510, 13, 1779, 586, 11, 498, 613, 732, 1074, 19959, 11, 577, 393, 291, 31499, 264, 2793, 670, 510, 30, 51108, 51108, 407, 498, 264, 2793, 366, 294, 341, 12212, 11, 286, 1116, 411, 281, 352, 646, 281, 264, 3380, 935, 510, 13, 759, 51336, 51336, 264, 2793, 366, 294, 341, 12212, 11, 286, 1116, 411, 281, 352, 281, 264, 661, 935, 13, 583, 498, 2793, 366, 670, 875, 3320, 11, 51582, 51582], "temperature": 0.0, "avg_logprob": -0.11271690328915913, "compression_ratio": 1.9408602150537635, "no_speech_prob": 4.1978000808740035e-05}, {"id": 169, "seek": 108612, "start": 1105.56, "end": 1110.4799999999998, "text": " the points are in this bubble, I'd like to go to the other point. But if points are overlapped,", "tokens": [50364, 257, 5523, 510, 13, 1396, 291, 747, 1071, 935, 11, 293, 341, 661, 935, 11, 309, 2170, 31499, 292, 50666, 50666, 510, 13, 1779, 586, 11, 498, 613, 732, 1074, 19959, 11, 577, 393, 291, 31499, 264, 2793, 670, 510, 30, 51108, 51108, 407, 498, 264, 2793, 366, 294, 341, 12212, 11, 286, 1116, 411, 281, 352, 646, 281, 264, 3380, 935, 510, 13, 759, 51336, 51336, 264, 2793, 366, 294, 341, 12212, 11, 286, 1116, 411, 281, 352, 281, 264, 661, 935, 13, 583, 498, 2793, 366, 670, 875, 3320, 11, 51582, 51582], "temperature": 0.0, "avg_logprob": -0.11271690328915913, "compression_ratio": 1.9408602150537635, "no_speech_prob": 4.1978000808740035e-05}, {"id": 170, "seek": 111048, "start": 1110.48, "end": 1116.72, "text": " sorry, if the bubbles are overlapped, then you can't really figure out where to go back.", "tokens": [50364, 2597, 11, 498, 264, 16295, 366, 670, 875, 3320, 11, 550, 291, 393, 380, 534, 2573, 484, 689, 281, 352, 646, 13, 50676, 50676, 407, 550, 264, 31565, 1433, 486, 445, 360, 341, 13, 440, 31565, 1433, 486, 853, 50950, 50950, 281, 483, 439, 729, 16295, 382, 1400, 382, 1944, 1270, 300, 436, 500, 380, 19959, 13, 1436, 498, 436, 51202, 51202, 19959, 11, 550, 264, 31565, 307, 406, 516, 281, 312, 665, 13, 400, 370, 586, 321, 362, 281, 3191, 341, 13, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.09918923213564117, "compression_ratio": 1.76, "no_speech_prob": 3.94358103221748e-05}, {"id": 171, "seek": 111048, "start": 1116.72, "end": 1122.2, "text": " So then the reconstruction term will just do this. The reconstruction term will try", "tokens": [50364, 2597, 11, 498, 264, 16295, 366, 670, 875, 3320, 11, 550, 291, 393, 380, 534, 2573, 484, 689, 281, 352, 646, 13, 50676, 50676, 407, 550, 264, 31565, 1433, 486, 445, 360, 341, 13, 440, 31565, 1433, 486, 853, 50950, 50950, 281, 483, 439, 729, 16295, 382, 1400, 382, 1944, 1270, 300, 436, 500, 380, 19959, 13, 1436, 498, 436, 51202, 51202, 19959, 11, 550, 264, 31565, 307, 406, 516, 281, 312, 665, 13, 400, 370, 586, 321, 362, 281, 3191, 341, 13, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.09918923213564117, "compression_ratio": 1.76, "no_speech_prob": 3.94358103221748e-05}, {"id": 172, "seek": 111048, "start": 1122.2, "end": 1127.24, "text": " to get all those bubbles as far as possible such that they don't overlap. Because if they", "tokens": [50364, 2597, 11, 498, 264, 16295, 366, 670, 875, 3320, 11, 550, 291, 393, 380, 534, 2573, 484, 689, 281, 352, 646, 13, 50676, 50676, 407, 550, 264, 31565, 1433, 486, 445, 360, 341, 13, 440, 31565, 1433, 486, 853, 50950, 50950, 281, 483, 439, 729, 16295, 382, 1400, 382, 1944, 1270, 300, 436, 500, 380, 19959, 13, 1436, 498, 436, 51202, 51202, 19959, 11, 550, 264, 31565, 307, 406, 516, 281, 312, 665, 13, 400, 370, 586, 321, 362, 281, 3191, 341, 13, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.09918923213564117, "compression_ratio": 1.76, "no_speech_prob": 3.94358103221748e-05}, {"id": 173, "seek": 111048, "start": 1127.24, "end": 1135.92, "text": " overlap, then the reconstruction is not going to be good. And so now we have to fix this.", "tokens": [50364, 2597, 11, 498, 264, 16295, 366, 670, 875, 3320, 11, 550, 291, 393, 380, 534, 2573, 484, 689, 281, 352, 646, 13, 50676, 50676, 407, 550, 264, 31565, 1433, 486, 445, 360, 341, 13, 440, 31565, 1433, 486, 853, 50950, 50950, 281, 483, 439, 729, 16295, 382, 1400, 382, 1944, 1270, 300, 436, 500, 380, 19959, 13, 1436, 498, 436, 51202, 51202, 19959, 11, 550, 264, 31565, 307, 406, 516, 281, 312, 665, 13, 400, 370, 586, 321, 362, 281, 3191, 341, 13, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.09918923213564117, "compression_ratio": 1.76, "no_speech_prob": 3.94358103221748e-05}, {"id": 174, "seek": 113592, "start": 1135.92, "end": 1145.52, "text": " So there are a few ways to fix this. How can we fix this overlapping issue? Why didn't", "tokens": [50364, 407, 456, 366, 257, 1326, 2098, 281, 3191, 341, 13, 1012, 393, 321, 3191, 341, 33535, 2734, 30, 1545, 994, 380, 50844, 50844, 321, 362, 341, 33535, 2734, 365, 264, 2710, 484, 2058, 19866, 30, 51108, 51108, 1436, 456, 390, 572, 21977, 13, 51212, 51212, 400, 370, 437, 775, 309, 914, 30, 1664, 291, 13799, 437, 406, 1419, 257, 21977, 914, 30, 51558, 51558, 440, 41225, 366, 406, 41225, 11, 457, 436, 434, 2793, 13, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.17577107345001608, "compression_ratio": 1.5947368421052632, "no_speech_prob": 4.4652890210272744e-05}, {"id": 175, "seek": 113592, "start": 1145.52, "end": 1150.8000000000002, "text": " we have this overlapping issue with the normal out encoder?", "tokens": [50364, 407, 456, 366, 257, 1326, 2098, 281, 3191, 341, 13, 1012, 393, 321, 3191, 341, 33535, 2734, 30, 1545, 994, 380, 50844, 50844, 321, 362, 341, 33535, 2734, 365, 264, 2710, 484, 2058, 19866, 30, 51108, 51108, 1436, 456, 390, 572, 21977, 13, 51212, 51212, 400, 370, 437, 775, 309, 914, 30, 1664, 291, 13799, 437, 406, 1419, 257, 21977, 914, 30, 51558, 51558, 440, 41225, 366, 406, 41225, 11, 457, 436, 434, 2793, 13, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.17577107345001608, "compression_ratio": 1.5947368421052632, "no_speech_prob": 4.4652890210272744e-05}, {"id": 176, "seek": 113592, "start": 1150.8000000000002, "end": 1152.88, "text": " Because there was no variance.", "tokens": [50364, 407, 456, 366, 257, 1326, 2098, 281, 3191, 341, 13, 1012, 393, 321, 3191, 341, 33535, 2734, 30, 1545, 994, 380, 50844, 50844, 321, 362, 341, 33535, 2734, 365, 264, 2710, 484, 2058, 19866, 30, 51108, 51108, 1436, 456, 390, 572, 21977, 13, 51212, 51212, 400, 370, 437, 775, 309, 914, 30, 1664, 291, 13799, 437, 406, 1419, 257, 21977, 914, 30, 51558, 51558, 440, 41225, 366, 406, 41225, 11, 457, 436, 434, 2793, 13, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.17577107345001608, "compression_ratio": 1.5947368421052632, "no_speech_prob": 4.4652890210272744e-05}, {"id": 177, "seek": 113592, "start": 1152.88, "end": 1159.8000000000002, "text": " And so what does it mean? Can you translate what not having a variance mean?", "tokens": [50364, 407, 456, 366, 257, 1326, 2098, 281, 3191, 341, 13, 1012, 393, 321, 3191, 341, 33535, 2734, 30, 1545, 994, 380, 50844, 50844, 321, 362, 341, 33535, 2734, 365, 264, 2710, 484, 2058, 19866, 30, 51108, 51108, 1436, 456, 390, 572, 21977, 13, 51212, 51212, 400, 370, 437, 775, 309, 914, 30, 1664, 291, 13799, 437, 406, 1419, 257, 21977, 914, 30, 51558, 51558, 440, 41225, 366, 406, 41225, 11, 457, 436, 434, 2793, 13, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.17577107345001608, "compression_ratio": 1.5947368421052632, "no_speech_prob": 4.4652890210272744e-05}, {"id": 178, "seek": 113592, "start": 1159.8000000000002, "end": 1162.1200000000001, "text": " The spheres are not spheres, but they're points.", "tokens": [50364, 407, 456, 366, 257, 1326, 2098, 281, 3191, 341, 13, 1012, 393, 321, 3191, 341, 33535, 2734, 30, 1545, 994, 380, 50844, 50844, 321, 362, 341, 33535, 2734, 365, 264, 2710, 484, 2058, 19866, 30, 51108, 51108, 1436, 456, 390, 572, 21977, 13, 51212, 51212, 400, 370, 437, 775, 309, 914, 30, 1664, 291, 13799, 437, 406, 1419, 257, 21977, 914, 30, 51558, 51558, 440, 41225, 366, 406, 41225, 11, 457, 436, 434, 2793, 13, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.17577107345001608, "compression_ratio": 1.5947368421052632, "no_speech_prob": 4.4652890210272744e-05}, {"id": 179, "seek": 116212, "start": 1162.12, "end": 1169.36, "text": " Right. So if you have just points, points will never overlap. They have to be the exact", "tokens": [50364, 1779, 13, 407, 498, 291, 362, 445, 2793, 11, 2793, 486, 1128, 19959, 13, 814, 362, 281, 312, 264, 1900, 50726, 50726, 912, 935, 13, 583, 291, 362, 264, 1900, 912, 935, 787, 498, 264, 2058, 19866, 307, 3116, 13, 1610, 291, 362, 264, 51024, 51024, 912, 4846, 11, 286, 519, 13, 1042, 11, 309, 311, 17518, 300, 732, 2793, 19959, 13, 759, 586, 2602, 295, 1419, 51346, 51346, 2793, 291, 362, 767, 22219, 11, 731, 11, 5523, 393, 19959, 570, 456, 366, 867, 13785, 51710, 51710, 2793, 294, 300, 5523, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.1742337147394816, "compression_ratio": 1.7568807339449541, "no_speech_prob": 2.353648596908897e-05}, {"id": 180, "seek": 116212, "start": 1169.36, "end": 1175.32, "text": " same point. But you have the exact same point only if the encoder is dead. Or you have the", "tokens": [50364, 1779, 13, 407, 498, 291, 362, 445, 2793, 11, 2793, 486, 1128, 19959, 13, 814, 362, 281, 312, 264, 1900, 50726, 50726, 912, 935, 13, 583, 291, 362, 264, 1900, 912, 935, 787, 498, 264, 2058, 19866, 307, 3116, 13, 1610, 291, 362, 264, 51024, 51024, 912, 4846, 11, 286, 519, 13, 1042, 11, 309, 311, 17518, 300, 732, 2793, 19959, 13, 759, 586, 2602, 295, 1419, 51346, 51346, 2793, 291, 362, 767, 22219, 11, 731, 11, 5523, 393, 19959, 570, 456, 366, 867, 13785, 51710, 51710, 2793, 294, 300, 5523, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.1742337147394816, "compression_ratio": 1.7568807339449541, "no_speech_prob": 2.353648596908897e-05}, {"id": 181, "seek": 116212, "start": 1175.32, "end": 1181.76, "text": " same input, I think. Well, it's unlikely that two points overlap. If now instead of having", "tokens": [50364, 1779, 13, 407, 498, 291, 362, 445, 2793, 11, 2793, 486, 1128, 19959, 13, 814, 362, 281, 312, 264, 1900, 50726, 50726, 912, 935, 13, 583, 291, 362, 264, 1900, 912, 935, 787, 498, 264, 2058, 19866, 307, 3116, 13, 1610, 291, 362, 264, 51024, 51024, 912, 4846, 11, 286, 519, 13, 1042, 11, 309, 311, 17518, 300, 732, 2793, 19959, 13, 759, 586, 2602, 295, 1419, 51346, 51346, 2793, 291, 362, 767, 22219, 11, 731, 11, 5523, 393, 19959, 570, 456, 366, 867, 13785, 51710, 51710, 2793, 294, 300, 5523, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.1742337147394816, "compression_ratio": 1.7568807339449541, "no_speech_prob": 2.353648596908897e-05}, {"id": 182, "seek": 116212, "start": 1181.76, "end": 1189.04, "text": " points you have actually volumes, well, volume can overlap because there are many infinite", "tokens": [50364, 1779, 13, 407, 498, 291, 362, 445, 2793, 11, 2793, 486, 1128, 19959, 13, 814, 362, 281, 312, 264, 1900, 50726, 50726, 912, 935, 13, 583, 291, 362, 264, 1900, 912, 935, 787, 498, 264, 2058, 19866, 307, 3116, 13, 1610, 291, 362, 264, 51024, 51024, 912, 4846, 11, 286, 519, 13, 1042, 11, 309, 311, 17518, 300, 732, 2793, 19959, 13, 759, 586, 2602, 295, 1419, 51346, 51346, 2793, 291, 362, 767, 22219, 11, 731, 11, 5523, 393, 19959, 570, 456, 366, 867, 13785, 51710, 51710, 2793, 294, 300, 5523, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.1742337147394816, "compression_ratio": 1.7568807339449541, "no_speech_prob": 2.353648596908897e-05}, {"id": 183, "seek": 116212, "start": 1189.04, "end": 1191.9199999999998, "text": " points in that volume.", "tokens": [50364, 1779, 13, 407, 498, 291, 362, 445, 2793, 11, 2793, 486, 1128, 19959, 13, 814, 362, 281, 312, 264, 1900, 50726, 50726, 912, 935, 13, 583, 291, 362, 264, 1900, 912, 935, 787, 498, 264, 2058, 19866, 307, 3116, 13, 1610, 291, 362, 264, 51024, 51024, 912, 4846, 11, 286, 519, 13, 1042, 11, 309, 311, 17518, 300, 732, 2793, 19959, 13, 759, 586, 2602, 295, 1419, 51346, 51346, 2793, 291, 362, 767, 22219, 11, 731, 11, 5523, 393, 19959, 570, 456, 366, 867, 13785, 51710, 51710, 2793, 294, 300, 5523, 13, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.1742337147394816, "compression_ratio": 1.7568807339449541, "no_speech_prob": 2.353648596908897e-05}, {"id": 184, "seek": 119192, "start": 1191.92, "end": 1198.0, "text": " So one option is going to be kill the variance. And so you have points. And now this defeats", "tokens": [50364, 407, 472, 3614, 307, 516, 281, 312, 1961, 264, 21977, 13, 400, 370, 291, 362, 2793, 13, 400, 586, 341, 7486, 1720, 50668, 50668, 264, 1379, 3034, 1478, 551, 13, 9129, 341, 1901, 88, 551, 11, 538, 8011, 264, 21977, 11, 586, 51064, 51064, 291, 500, 380, 458, 3602, 437, 311, 2737, 1296, 264, 2793, 13, 1436, 498, 291, 362, 1901, 11, 498, 51360, 51360, 436, 747, 5523, 11, 291, 393, 1792, 926, 294, 264, 48994, 1901, 13, 509, 393, 1009, 2573, 484, 689, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14797284386374734, "compression_ratio": 1.6651162790697673, "no_speech_prob": 2.3540345864603296e-05}, {"id": 185, "seek": 119192, "start": 1198.0, "end": 1205.92, "text": " the whole variational thing. Without this spacey thing, by killing the variance, now", "tokens": [50364, 407, 472, 3614, 307, 516, 281, 312, 1961, 264, 21977, 13, 400, 370, 291, 362, 2793, 13, 400, 586, 341, 7486, 1720, 50668, 50668, 264, 1379, 3034, 1478, 551, 13, 9129, 341, 1901, 88, 551, 11, 538, 8011, 264, 21977, 11, 586, 51064, 51064, 291, 500, 380, 458, 3602, 437, 311, 2737, 1296, 264, 2793, 13, 1436, 498, 291, 362, 1901, 11, 498, 51360, 51360, 436, 747, 5523, 11, 291, 393, 1792, 926, 294, 264, 48994, 1901, 13, 509, 393, 1009, 2573, 484, 689, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14797284386374734, "compression_ratio": 1.6651162790697673, "no_speech_prob": 2.3540345864603296e-05}, {"id": 186, "seek": 119192, "start": 1205.92, "end": 1211.8400000000001, "text": " you don't know anymore what's happening between the points. Because if you have space, if", "tokens": [50364, 407, 472, 3614, 307, 516, 281, 312, 1961, 264, 21977, 13, 400, 370, 291, 362, 2793, 13, 400, 586, 341, 7486, 1720, 50668, 50668, 264, 1379, 3034, 1478, 551, 13, 9129, 341, 1901, 88, 551, 11, 538, 8011, 264, 21977, 11, 586, 51064, 51064, 291, 500, 380, 458, 3602, 437, 311, 2737, 1296, 264, 2793, 13, 1436, 498, 291, 362, 1901, 11, 498, 51360, 51360, 436, 747, 5523, 11, 291, 393, 1792, 926, 294, 264, 48994, 1901, 13, 509, 393, 1009, 2573, 484, 689, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14797284386374734, "compression_ratio": 1.6651162790697673, "no_speech_prob": 2.3540345864603296e-05}, {"id": 187, "seek": 119192, "start": 1211.8400000000001, "end": 1217.92, "text": " they take volume, you can walk around in the latent space. You can always figure out where", "tokens": [50364, 407, 472, 3614, 307, 516, 281, 312, 1961, 264, 21977, 13, 400, 370, 291, 362, 2793, 13, 400, 586, 341, 7486, 1720, 50668, 50668, 264, 1379, 3034, 1478, 551, 13, 9129, 341, 1901, 88, 551, 11, 538, 8011, 264, 21977, 11, 586, 51064, 51064, 291, 500, 380, 458, 3602, 437, 311, 2737, 1296, 264, 2793, 13, 1436, 498, 291, 362, 1901, 11, 498, 51360, 51360, 436, 747, 5523, 11, 291, 393, 1792, 926, 294, 264, 48994, 1901, 13, 509, 393, 1009, 2573, 484, 689, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14797284386374734, "compression_ratio": 1.6651162790697673, "no_speech_prob": 2.3540345864603296e-05}, {"id": 188, "seek": 121792, "start": 1217.92, "end": 1224.3200000000002, "text": " to go back. If these are points, as soon as you leave this position here, you have no", "tokens": [50364, 281, 352, 646, 13, 759, 613, 366, 2793, 11, 382, 2321, 382, 291, 1856, 341, 2535, 510, 11, 291, 362, 572, 50684, 50684, 17076, 1558, 689, 281, 352, 13, 50904, 50904, 2639, 4286, 11, 700, 935, 11, 321, 393, 1961, 264, 21977, 13, 5358, 3614, 11, 731, 11, 264, 472, 286, 855, 291, 510, 11, 51276, 51276, 558, 30, 440, 661, 3614, 307, 516, 281, 483, 613, 16295, 382, 1400, 382, 1944, 13, 407, 498, 436, 366, 51558, 51558], "temperature": 0.0, "avg_logprob": -0.18640511124222367, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.2025527212244924e-05}, {"id": 189, "seek": 121792, "start": 1224.3200000000002, "end": 1228.72, "text": " whatsoever idea where to go.", "tokens": [50364, 281, 352, 646, 13, 759, 613, 366, 2793, 11, 382, 2321, 382, 291, 1856, 341, 2535, 510, 11, 291, 362, 572, 50684, 50684, 17076, 1558, 689, 281, 352, 13, 50904, 50904, 2639, 4286, 11, 700, 935, 11, 321, 393, 1961, 264, 21977, 13, 5358, 3614, 11, 731, 11, 264, 472, 286, 855, 291, 510, 11, 51276, 51276, 558, 30, 440, 661, 3614, 307, 516, 281, 483, 613, 16295, 382, 1400, 382, 1944, 13, 407, 498, 436, 366, 51558, 51558], "temperature": 0.0, "avg_logprob": -0.18640511124222367, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.2025527212244924e-05}, {"id": 190, "seek": 121792, "start": 1228.72, "end": 1236.16, "text": " Anyhow, first point, we can kill the variance. Other option, well, the one I show you here,", "tokens": [50364, 281, 352, 646, 13, 759, 613, 366, 2793, 11, 382, 2321, 382, 291, 1856, 341, 2535, 510, 11, 291, 362, 572, 50684, 50684, 17076, 1558, 689, 281, 352, 13, 50904, 50904, 2639, 4286, 11, 700, 935, 11, 321, 393, 1961, 264, 21977, 13, 5358, 3614, 11, 731, 11, 264, 472, 286, 855, 291, 510, 11, 51276, 51276, 558, 30, 440, 661, 3614, 307, 516, 281, 483, 613, 16295, 382, 1400, 382, 1944, 13, 407, 498, 436, 366, 51558, 51558], "temperature": 0.0, "avg_logprob": -0.18640511124222367, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.2025527212244924e-05}, {"id": 191, "seek": 121792, "start": 1236.16, "end": 1241.8000000000002, "text": " right? The other option is going to get these bubbles as far as possible. So if they are", "tokens": [50364, 281, 352, 646, 13, 759, 613, 366, 2793, 11, 382, 2321, 382, 291, 1856, 341, 2535, 510, 11, 291, 362, 572, 50684, 50684, 17076, 1558, 689, 281, 352, 13, 50904, 50904, 2639, 4286, 11, 700, 935, 11, 321, 393, 1961, 264, 21977, 13, 5358, 3614, 11, 731, 11, 264, 472, 286, 855, 291, 510, 11, 51276, 51276, 558, 30, 440, 661, 3614, 307, 516, 281, 483, 613, 16295, 382, 1400, 382, 1944, 13, 407, 498, 436, 366, 51558, 51558], "temperature": 0.0, "avg_logprob": -0.18640511124222367, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.2025527212244924e-05}, {"id": 192, "seek": 124180, "start": 1241.8, "end": 1252.1599999999999, "text": " as far as possible, what's going to happen in your Python script? So if these means go", "tokens": [50364, 382, 1400, 382, 1944, 11, 437, 311, 516, 281, 1051, 294, 428, 15329, 5755, 30, 407, 498, 613, 1355, 352, 50882, 50882, 588, 11, 588, 1400, 11, 550, 436, 486, 3488, 257, 688, 11, 257, 688, 11, 257, 688, 13, 400, 550, 264, 1154, 307, 51360, 51360, 300, 291, 434, 516, 281, 483, 13785, 13, 639, 1507, 307, 516, 281, 21411, 570, 439, 613, 4190, 51572, 51572, 366, 1382, 281, 352, 382, 1400, 382, 1944, 1270, 300, 436, 500, 380, 19959, 13, 400, 550, 300, 311, 406, 665, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.14727478642617503, "compression_ratio": 1.6857142857142857, "no_speech_prob": 1.8610213373904116e-05}, {"id": 193, "seek": 124180, "start": 1252.1599999999999, "end": 1261.72, "text": " very, very far, then they will increase a lot, a lot, a lot. And then the problem is", "tokens": [50364, 382, 1400, 382, 1944, 11, 437, 311, 516, 281, 1051, 294, 428, 15329, 5755, 30, 407, 498, 613, 1355, 352, 50882, 50882, 588, 11, 588, 1400, 11, 550, 436, 486, 3488, 257, 688, 11, 257, 688, 11, 257, 688, 13, 400, 550, 264, 1154, 307, 51360, 51360, 300, 291, 434, 516, 281, 483, 13785, 13, 639, 1507, 307, 516, 281, 21411, 570, 439, 613, 4190, 51572, 51572, 366, 1382, 281, 352, 382, 1400, 382, 1944, 1270, 300, 436, 500, 380, 19959, 13, 400, 550, 300, 311, 406, 665, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.14727478642617503, "compression_ratio": 1.6857142857142857, "no_speech_prob": 1.8610213373904116e-05}, {"id": 194, "seek": 124180, "start": 1261.72, "end": 1265.96, "text": " that you're going to get infinite. This stuff is going to explode because all these values", "tokens": [50364, 382, 1400, 382, 1944, 11, 437, 311, 516, 281, 1051, 294, 428, 15329, 5755, 30, 407, 498, 613, 1355, 352, 50882, 50882, 588, 11, 588, 1400, 11, 550, 436, 486, 3488, 257, 688, 11, 257, 688, 11, 257, 688, 13, 400, 550, 264, 1154, 307, 51360, 51360, 300, 291, 434, 516, 281, 483, 13785, 13, 639, 1507, 307, 516, 281, 21411, 570, 439, 613, 4190, 51572, 51572, 366, 1382, 281, 352, 382, 1400, 382, 1944, 1270, 300, 436, 500, 380, 19959, 13, 400, 550, 300, 311, 406, 665, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.14727478642617503, "compression_ratio": 1.6857142857142857, "no_speech_prob": 1.8610213373904116e-05}, {"id": 195, "seek": 124180, "start": 1265.96, "end": 1271.72, "text": " are trying to go as far as possible such that they don't overlap. And then that's not good.", "tokens": [50364, 382, 1400, 382, 1944, 11, 437, 311, 516, 281, 1051, 294, 428, 15329, 5755, 30, 407, 498, 613, 1355, 352, 50882, 50882, 588, 11, 588, 1400, 11, 550, 436, 486, 3488, 257, 688, 11, 257, 688, 11, 257, 688, 13, 400, 550, 264, 1154, 307, 51360, 51360, 300, 291, 434, 516, 281, 483, 13785, 13, 639, 1507, 307, 516, 281, 21411, 570, 439, 613, 4190, 51572, 51572, 366, 1382, 281, 352, 382, 1400, 382, 1944, 1270, 300, 436, 500, 380, 19959, 13, 400, 550, 300, 311, 406, 665, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.14727478642617503, "compression_ratio": 1.6857142857142857, "no_speech_prob": 1.8610213373904116e-05}, {"id": 196, "seek": 127172, "start": 1271.72, "end": 1277.6000000000001, "text": " Okay. All right. So let's figure out how variational out encoder fixes this problem.", "tokens": [50364, 1033, 13, 1057, 558, 13, 407, 718, 311, 2573, 484, 577, 3034, 1478, 484, 2058, 19866, 32539, 341, 1154, 13, 50658, 50658, 876, 11, 727, 291, 445, 17594, 437, 291, 914, 538, 7380, 264, 2793, 4936, 30, 1743, 11, 366, 291, 3372, 50902, 50902, 552, 294, 257, 2946, 18795, 1901, 281, 2944, 552, 4936, 30, 51042, 51042, 407, 382, 436, 366, 510, 11, 370, 1184, 11, 498, 291, 500, 380, 362, 264, 21977, 11, 439, 729, 13040, 510, 11, 51328, 51328, 439, 729, 16295, 510, 366, 445, 2793, 13, 18600, 300, 321, 362, 512, 21977, 11, 436, 486, 747, 51587, 51587], "temperature": 0.0, "avg_logprob": -0.2108674095672311, "compression_ratio": 1.6504065040650406, "no_speech_prob": 2.3919470550026745e-05}, {"id": 197, "seek": 127172, "start": 1277.6000000000001, "end": 1282.48, "text": " Oh, could you just clarify what you mean by pushing the points apart? Like, are you putting", "tokens": [50364, 1033, 13, 1057, 558, 13, 407, 718, 311, 2573, 484, 577, 3034, 1478, 484, 2058, 19866, 32539, 341, 1154, 13, 50658, 50658, 876, 11, 727, 291, 445, 17594, 437, 291, 914, 538, 7380, 264, 2793, 4936, 30, 1743, 11, 366, 291, 3372, 50902, 50902, 552, 294, 257, 2946, 18795, 1901, 281, 2944, 552, 4936, 30, 51042, 51042, 407, 382, 436, 366, 510, 11, 370, 1184, 11, 498, 291, 500, 380, 362, 264, 21977, 11, 439, 729, 13040, 510, 11, 51328, 51328, 439, 729, 16295, 510, 366, 445, 2793, 13, 18600, 300, 321, 362, 512, 21977, 11, 436, 486, 747, 51587, 51587], "temperature": 0.0, "avg_logprob": -0.2108674095672311, "compression_ratio": 1.6504065040650406, "no_speech_prob": 2.3919470550026745e-05}, {"id": 198, "seek": 127172, "start": 1282.48, "end": 1285.28, "text": " them in a higher dimensional space to push them apart?", "tokens": [50364, 1033, 13, 1057, 558, 13, 407, 718, 311, 2573, 484, 577, 3034, 1478, 484, 2058, 19866, 32539, 341, 1154, 13, 50658, 50658, 876, 11, 727, 291, 445, 17594, 437, 291, 914, 538, 7380, 264, 2793, 4936, 30, 1743, 11, 366, 291, 3372, 50902, 50902, 552, 294, 257, 2946, 18795, 1901, 281, 2944, 552, 4936, 30, 51042, 51042, 407, 382, 436, 366, 510, 11, 370, 1184, 11, 498, 291, 500, 380, 362, 264, 21977, 11, 439, 729, 13040, 510, 11, 51328, 51328, 439, 729, 16295, 510, 366, 445, 2793, 13, 18600, 300, 321, 362, 512, 21977, 11, 436, 486, 747, 51587, 51587], "temperature": 0.0, "avg_logprob": -0.2108674095672311, "compression_ratio": 1.6504065040650406, "no_speech_prob": 2.3919470550026745e-05}, {"id": 199, "seek": 127172, "start": 1285.28, "end": 1291.0, "text": " So as they are here, so each, if you don't have the variance, all those circles here,", "tokens": [50364, 1033, 13, 1057, 558, 13, 407, 718, 311, 2573, 484, 577, 3034, 1478, 484, 2058, 19866, 32539, 341, 1154, 13, 50658, 50658, 876, 11, 727, 291, 445, 17594, 437, 291, 914, 538, 7380, 264, 2793, 4936, 30, 1743, 11, 366, 291, 3372, 50902, 50902, 552, 294, 257, 2946, 18795, 1901, 281, 2944, 552, 4936, 30, 51042, 51042, 407, 382, 436, 366, 510, 11, 370, 1184, 11, 498, 291, 500, 380, 362, 264, 21977, 11, 439, 729, 13040, 510, 11, 51328, 51328, 439, 729, 16295, 510, 366, 445, 2793, 13, 18600, 300, 321, 362, 512, 21977, 11, 436, 486, 747, 51587, 51587], "temperature": 0.0, "avg_logprob": -0.2108674095672311, "compression_ratio": 1.6504065040650406, "no_speech_prob": 2.3919470550026745e-05}, {"id": 200, "seek": 127172, "start": 1291.0, "end": 1296.18, "text": " all those bubbles here are just points. Given that we have some variance, they will take", "tokens": [50364, 1033, 13, 1057, 558, 13, 407, 718, 311, 2573, 484, 577, 3034, 1478, 484, 2058, 19866, 32539, 341, 1154, 13, 50658, 50658, 876, 11, 727, 291, 445, 17594, 437, 291, 914, 538, 7380, 264, 2793, 4936, 30, 1743, 11, 366, 291, 3372, 50902, 50902, 552, 294, 257, 2946, 18795, 1901, 281, 2944, 552, 4936, 30, 51042, 51042, 407, 382, 436, 366, 510, 11, 370, 1184, 11, 498, 291, 500, 380, 362, 264, 21977, 11, 439, 729, 13040, 510, 11, 51328, 51328, 439, 729, 16295, 510, 366, 445, 2793, 13, 18600, 300, 321, 362, 512, 21977, 11, 436, 486, 747, 51587, 51587], "temperature": 0.0, "avg_logprob": -0.2108674095672311, "compression_ratio": 1.6504065040650406, "no_speech_prob": 2.3919470550026745e-05}, {"id": 201, "seek": 129618, "start": 1296.18, "end": 1303.68, "text": " some space. Now, if this space taken by two bubbles overlaps with another bubble, the", "tokens": [50364, 512, 1901, 13, 823, 11, 498, 341, 1901, 2726, 538, 732, 16295, 15986, 2382, 365, 1071, 12212, 11, 264, 50739, 50739, 31565, 6713, 486, 3488, 570, 291, 362, 572, 1558, 577, 281, 352, 646, 281, 264, 3380, 50981, 50981, 935, 300, 10833, 300, 16687, 13, 400, 370, 264, 3209, 11, 264, 2058, 19866, 575, 732, 3956, 294, 1668, 51273, 51273, 281, 5407, 341, 31565, 6713, 13, 1485, 3614, 307, 516, 281, 312, 281, 1961, 264, 21977, 1270, 300, 51581, 51581, 291, 483, 2793, 13, 440, 661, 3614, 307, 516, 281, 312, 281, 2845, 439, 729, 2793, 294, 604, 3513, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.08835151820506865, "compression_ratio": 1.7896825396825398, "no_speech_prob": 2.0726552975247614e-05}, {"id": 202, "seek": 129618, "start": 1303.68, "end": 1308.52, "text": " reconstruction error will increase because you have no idea how to go back to the original", "tokens": [50364, 512, 1901, 13, 823, 11, 498, 341, 1901, 2726, 538, 732, 16295, 15986, 2382, 365, 1071, 12212, 11, 264, 50739, 50739, 31565, 6713, 486, 3488, 570, 291, 362, 572, 1558, 577, 281, 352, 646, 281, 264, 3380, 50981, 50981, 935, 300, 10833, 300, 16687, 13, 400, 370, 264, 3209, 11, 264, 2058, 19866, 575, 732, 3956, 294, 1668, 51273, 51273, 281, 5407, 341, 31565, 6713, 13, 1485, 3614, 307, 516, 281, 312, 281, 1961, 264, 21977, 1270, 300, 51581, 51581, 291, 483, 2793, 13, 440, 661, 3614, 307, 516, 281, 312, 281, 2845, 439, 729, 2793, 294, 604, 3513, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.08835151820506865, "compression_ratio": 1.7896825396825398, "no_speech_prob": 2.0726552975247614e-05}, {"id": 203, "seek": 129618, "start": 1308.52, "end": 1314.3600000000001, "text": " point that generated that sphere. And so the network, the encoder has two options in order", "tokens": [50364, 512, 1901, 13, 823, 11, 498, 341, 1901, 2726, 538, 732, 16295, 15986, 2382, 365, 1071, 12212, 11, 264, 50739, 50739, 31565, 6713, 486, 3488, 570, 291, 362, 572, 1558, 577, 281, 352, 646, 281, 264, 3380, 50981, 50981, 935, 300, 10833, 300, 16687, 13, 400, 370, 264, 3209, 11, 264, 2058, 19866, 575, 732, 3956, 294, 1668, 51273, 51273, 281, 5407, 341, 31565, 6713, 13, 1485, 3614, 307, 516, 281, 312, 281, 1961, 264, 21977, 1270, 300, 51581, 51581, 291, 483, 2793, 13, 440, 661, 3614, 307, 516, 281, 312, 281, 2845, 439, 729, 2793, 294, 604, 3513, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.08835151820506865, "compression_ratio": 1.7896825396825398, "no_speech_prob": 2.0726552975247614e-05}, {"id": 204, "seek": 129618, "start": 1314.3600000000001, "end": 1320.52, "text": " to reduce this reconstruction error. One option is going to be to kill the variance such that", "tokens": [50364, 512, 1901, 13, 823, 11, 498, 341, 1901, 2726, 538, 732, 16295, 15986, 2382, 365, 1071, 12212, 11, 264, 50739, 50739, 31565, 6713, 486, 3488, 570, 291, 362, 572, 1558, 577, 281, 352, 646, 281, 264, 3380, 50981, 50981, 935, 300, 10833, 300, 16687, 13, 400, 370, 264, 3209, 11, 264, 2058, 19866, 575, 732, 3956, 294, 1668, 51273, 51273, 281, 5407, 341, 31565, 6713, 13, 1485, 3614, 307, 516, 281, 312, 281, 1961, 264, 21977, 1270, 300, 51581, 51581, 291, 483, 2793, 13, 440, 661, 3614, 307, 516, 281, 312, 281, 2845, 439, 729, 2793, 294, 604, 3513, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.08835151820506865, "compression_ratio": 1.7896825396825398, "no_speech_prob": 2.0726552975247614e-05}, {"id": 205, "seek": 129618, "start": 1320.52, "end": 1325.6200000000001, "text": " you get points. The other option is going to be to send all those points in any direction", "tokens": [50364, 512, 1901, 13, 823, 11, 498, 341, 1901, 2726, 538, 732, 16295, 15986, 2382, 365, 1071, 12212, 11, 264, 50739, 50739, 31565, 6713, 486, 3488, 570, 291, 362, 572, 1558, 577, 281, 352, 646, 281, 264, 3380, 50981, 50981, 935, 300, 10833, 300, 16687, 13, 400, 370, 264, 3209, 11, 264, 2058, 19866, 575, 732, 3956, 294, 1668, 51273, 51273, 281, 5407, 341, 31565, 6713, 13, 1485, 3614, 307, 516, 281, 312, 281, 1961, 264, 21977, 1270, 300, 51581, 51581, 291, 483, 2793, 13, 440, 661, 3614, 307, 516, 281, 312, 281, 2845, 439, 729, 2793, 294, 604, 3513, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.08835151820506865, "compression_ratio": 1.7896825396825398, "no_speech_prob": 2.0726552975247614e-05}, {"id": 206, "seek": 132562, "start": 1325.62, "end": 1331.0, "text": " such that they don't overlap. Okay. Okay. Yeah, that makes sense. Okay. Cool. So reconstruction", "tokens": [50364, 1270, 300, 436, 500, 380, 19959, 13, 1033, 13, 1033, 13, 865, 11, 300, 1669, 2020, 13, 1033, 13, 8561, 13, 407, 31565, 50633, 50633, 6713, 2170, 341, 1507, 281, 3603, 926, 11, 457, 550, 718, 311, 5366, 264, 1150, 1433, 13, 407, 286, 576, 51047, 51047, 534, 2748, 291, 281, 14722, 613, 4972, 30867, 1296, 264, 39148, 293, 257, 2710, 51331, 51331, 7316, 1270, 300, 291, 393, 3124, 1310, 337, 958, 1243, 13, 583, 550, 498, 291, 14722, 300, 51667, 51667], "temperature": 0.0, "avg_logprob": -0.14814614114307223, "compression_ratio": 1.5826086956521739, "no_speech_prob": 3.0162222174112685e-05}, {"id": 207, "seek": 132562, "start": 1331.0, "end": 1339.28, "text": " error gets this stuff to fly around, but then let's introduce the second term. So I would", "tokens": [50364, 1270, 300, 436, 500, 380, 19959, 13, 1033, 13, 1033, 13, 865, 11, 300, 1669, 2020, 13, 1033, 13, 8561, 13, 407, 31565, 50633, 50633, 6713, 2170, 341, 1507, 281, 3603, 926, 11, 457, 550, 718, 311, 5366, 264, 1150, 1433, 13, 407, 286, 576, 51047, 51047, 534, 2748, 291, 281, 14722, 613, 4972, 30867, 1296, 264, 39148, 293, 257, 2710, 51331, 51331, 7316, 1270, 300, 291, 393, 3124, 1310, 337, 958, 1243, 13, 583, 550, 498, 291, 14722, 300, 51667, 51667], "temperature": 0.0, "avg_logprob": -0.14814614114307223, "compression_ratio": 1.5826086956521739, "no_speech_prob": 3.0162222174112685e-05}, {"id": 208, "seek": 132562, "start": 1339.28, "end": 1344.9599999999998, "text": " really recommend you to compute these relative entropy between the Gaussian and a normal", "tokens": [50364, 1270, 300, 436, 500, 380, 19959, 13, 1033, 13, 1033, 13, 865, 11, 300, 1669, 2020, 13, 1033, 13, 8561, 13, 407, 31565, 50633, 50633, 6713, 2170, 341, 1507, 281, 3603, 926, 11, 457, 550, 718, 311, 5366, 264, 1150, 1433, 13, 407, 286, 576, 51047, 51047, 534, 2748, 291, 281, 14722, 613, 4972, 30867, 1296, 264, 39148, 293, 257, 2710, 51331, 51331, 7316, 1270, 300, 291, 393, 3124, 1310, 337, 958, 1243, 13, 583, 550, 498, 291, 14722, 300, 51667, 51667], "temperature": 0.0, "avg_logprob": -0.14814614114307223, "compression_ratio": 1.5826086956521739, "no_speech_prob": 3.0162222174112685e-05}, {"id": 209, "seek": 132562, "start": 1344.9599999999998, "end": 1351.6799999999998, "text": " distribution such that you can practice maybe for next week. But then if you compute that", "tokens": [50364, 1270, 300, 436, 500, 380, 19959, 13, 1033, 13, 1033, 13, 865, 11, 300, 1669, 2020, 13, 1033, 13, 8561, 13, 407, 31565, 50633, 50633, 6713, 2170, 341, 1507, 281, 3603, 926, 11, 457, 550, 718, 311, 5366, 264, 1150, 1433, 13, 407, 286, 576, 51047, 51047, 534, 2748, 291, 281, 14722, 613, 4972, 30867, 1296, 264, 39148, 293, 257, 2710, 51331, 51331, 7316, 1270, 300, 291, 393, 3124, 1310, 337, 958, 1243, 13, 583, 550, 498, 291, 14722, 300, 51667, 51667], "temperature": 0.0, "avg_logprob": -0.14814614114307223, "compression_ratio": 1.5826086956521739, "no_speech_prob": 3.0162222174112685e-05}, {"id": 210, "seek": 135168, "start": 1351.68, "end": 1358.48, "text": " relative entropy, you get this stuff, right? You get several, four terms basically. And", "tokens": [50364, 4972, 30867, 11, 291, 483, 341, 1507, 11, 558, 30, 509, 483, 2940, 11, 1451, 2115, 1936, 13, 400, 50704, 50704, 1518, 820, 1223, 577, 341, 1542, 13, 883, 11, 1392, 13, 286, 478, 445, 17396, 13, 286, 478, 516, 281, 312, 50930, 50930, 767, 13468, 300, 13, 1033, 13, 407, 321, 362, 341, 6114, 13, 961, 311, 853, 281, 12477, 257, 707, 51128, 51128, 857, 294, 544, 2607, 437, 613, 2115, 2906, 13, 407, 264, 700, 1433, 11, 291, 362, 613, 21977, 51396, 51396, 3175, 3565, 21977, 3175, 472, 13, 407, 498, 321, 4295, 309, 11, 309, 1542, 411, 341, 13, 509, 362, 257, 8213, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.14272436662153765, "compression_ratio": 1.6356877323420074, "no_speech_prob": 2.389937435509637e-05}, {"id": 211, "seek": 135168, "start": 1358.48, "end": 1363.0, "text": " everyone should understand how this looks. No, okay. I'm just joking. I'm going to be", "tokens": [50364, 4972, 30867, 11, 291, 483, 341, 1507, 11, 558, 30, 509, 483, 2940, 11, 1451, 2115, 1936, 13, 400, 50704, 50704, 1518, 820, 1223, 577, 341, 1542, 13, 883, 11, 1392, 13, 286, 478, 445, 17396, 13, 286, 478, 516, 281, 312, 50930, 50930, 767, 13468, 300, 13, 1033, 13, 407, 321, 362, 341, 6114, 13, 961, 311, 853, 281, 12477, 257, 707, 51128, 51128, 857, 294, 544, 2607, 437, 613, 2115, 2906, 13, 407, 264, 700, 1433, 11, 291, 362, 613, 21977, 51396, 51396, 3175, 3565, 21977, 3175, 472, 13, 407, 498, 321, 4295, 309, 11, 309, 1542, 411, 341, 13, 509, 362, 257, 8213, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.14272436662153765, "compression_ratio": 1.6356877323420074, "no_speech_prob": 2.389937435509637e-05}, {"id": 212, "seek": 135168, "start": 1363.0, "end": 1366.96, "text": " actually explaining that. Okay. So we have this expression. Let's try to analyze a little", "tokens": [50364, 4972, 30867, 11, 291, 483, 341, 1507, 11, 558, 30, 509, 483, 2940, 11, 1451, 2115, 1936, 13, 400, 50704, 50704, 1518, 820, 1223, 577, 341, 1542, 13, 883, 11, 1392, 13, 286, 478, 445, 17396, 13, 286, 478, 516, 281, 312, 50930, 50930, 767, 13468, 300, 13, 1033, 13, 407, 321, 362, 341, 6114, 13, 961, 311, 853, 281, 12477, 257, 707, 51128, 51128, 857, 294, 544, 2607, 437, 613, 2115, 2906, 13, 407, 264, 700, 1433, 11, 291, 362, 613, 21977, 51396, 51396, 3175, 3565, 21977, 3175, 472, 13, 407, 498, 321, 4295, 309, 11, 309, 1542, 411, 341, 13, 509, 362, 257, 8213, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.14272436662153765, "compression_ratio": 1.6356877323420074, "no_speech_prob": 2.389937435509637e-05}, {"id": 213, "seek": 135168, "start": 1366.96, "end": 1372.3200000000002, "text": " bit in more detail what these terms represent. So the first term, you have these variance", "tokens": [50364, 4972, 30867, 11, 291, 483, 341, 1507, 11, 558, 30, 509, 483, 2940, 11, 1451, 2115, 1936, 13, 400, 50704, 50704, 1518, 820, 1223, 577, 341, 1542, 13, 883, 11, 1392, 13, 286, 478, 445, 17396, 13, 286, 478, 516, 281, 312, 50930, 50930, 767, 13468, 300, 13, 1033, 13, 407, 321, 362, 341, 6114, 13, 961, 311, 853, 281, 12477, 257, 707, 51128, 51128, 857, 294, 544, 2607, 437, 613, 2115, 2906, 13, 407, 264, 700, 1433, 11, 291, 362, 613, 21977, 51396, 51396, 3175, 3565, 21977, 3175, 472, 13, 407, 498, 321, 4295, 309, 11, 309, 1542, 411, 341, 13, 509, 362, 257, 8213, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.14272436662153765, "compression_ratio": 1.6356877323420074, "no_speech_prob": 2.389937435509637e-05}, {"id": 214, "seek": 135168, "start": 1372.3200000000002, "end": 1378.28, "text": " minus log variance minus one. So if we graph it, it looks like this. You have a linear", "tokens": [50364, 4972, 30867, 11, 291, 483, 341, 1507, 11, 558, 30, 509, 483, 2940, 11, 1451, 2115, 1936, 13, 400, 50704, 50704, 1518, 820, 1223, 577, 341, 1542, 13, 883, 11, 1392, 13, 286, 478, 445, 17396, 13, 286, 478, 516, 281, 312, 50930, 50930, 767, 13468, 300, 13, 1033, 13, 407, 321, 362, 341, 6114, 13, 961, 311, 853, 281, 12477, 257, 707, 51128, 51128, 857, 294, 544, 2607, 437, 613, 2115, 2906, 13, 407, 264, 700, 1433, 11, 291, 362, 613, 21977, 51396, 51396, 3175, 3565, 21977, 3175, 472, 13, 407, 498, 321, 4295, 309, 11, 309, 1542, 411, 341, 13, 509, 362, 257, 8213, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.14272436662153765, "compression_ratio": 1.6356877323420074, "no_speech_prob": 2.389937435509637e-05}, {"id": 215, "seek": 137828, "start": 1378.28, "end": 1388.32, "text": " function, right? After two on the X axis. And then on the other terms, you subtract", "tokens": [50364, 2445, 11, 558, 30, 2381, 732, 322, 264, 1783, 10298, 13, 400, 550, 322, 264, 661, 2115, 11, 291, 16390, 50866, 50866, 257, 41473, 32674, 11, 597, 1709, 281, 1804, 13202, 13, 759, 291, 2408, 257, 3175, 41473, 32674, 11, 309, 1709, 281, 1804, 51112, 51112, 13202, 412, 4018, 13, 400, 550, 5911, 309, 311, 516, 281, 312, 445, 21039, 13, 407, 498, 291, 2408, 264, 51456, 51456, 732, 293, 16390, 472, 11, 291, 483, 341, 733, 295, 4052, 2445, 13, 400, 498, 291, 17522, 341, 2445, 11, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.11429279783497685, "compression_ratio": 1.74, "no_speech_prob": 1.5204695955617353e-05}, {"id": 216, "seek": 137828, "start": 1388.32, "end": 1393.24, "text": " a logarithm, which goes to plus infinity. If you sum a minus logarithm, it goes to plus", "tokens": [50364, 2445, 11, 558, 30, 2381, 732, 322, 264, 1783, 10298, 13, 400, 550, 322, 264, 661, 2115, 11, 291, 16390, 50866, 50866, 257, 41473, 32674, 11, 597, 1709, 281, 1804, 13202, 13, 759, 291, 2408, 257, 3175, 41473, 32674, 11, 309, 1709, 281, 1804, 51112, 51112, 13202, 412, 4018, 13, 400, 550, 5911, 309, 311, 516, 281, 312, 445, 21039, 13, 407, 498, 291, 2408, 264, 51456, 51456, 732, 293, 16390, 472, 11, 291, 483, 341, 733, 295, 4052, 2445, 13, 400, 498, 291, 17522, 341, 2445, 11, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.11429279783497685, "compression_ratio": 1.74, "no_speech_prob": 1.5204695955617353e-05}, {"id": 217, "seek": 137828, "start": 1393.24, "end": 1400.12, "text": " infinity at zero. And then otherwise it's going to be just decay. So if you sum the", "tokens": [50364, 2445, 11, 558, 30, 2381, 732, 322, 264, 1783, 10298, 13, 400, 550, 322, 264, 661, 2115, 11, 291, 16390, 50866, 50866, 257, 41473, 32674, 11, 597, 1709, 281, 1804, 13202, 13, 759, 291, 2408, 257, 3175, 41473, 32674, 11, 309, 1709, 281, 1804, 51112, 51112, 13202, 412, 4018, 13, 400, 550, 5911, 309, 311, 516, 281, 312, 445, 21039, 13, 407, 498, 291, 2408, 264, 51456, 51456, 732, 293, 16390, 472, 11, 291, 483, 341, 733, 295, 4052, 2445, 13, 400, 498, 291, 17522, 341, 2445, 11, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.11429279783497685, "compression_ratio": 1.74, "no_speech_prob": 1.5204695955617353e-05}, {"id": 218, "seek": 137828, "start": 1400.12, "end": 1405.32, "text": " two and subtract one, you get this kind of cute function. And if you minimize this function,", "tokens": [50364, 2445, 11, 558, 30, 2381, 732, 322, 264, 1783, 10298, 13, 400, 550, 322, 264, 661, 2115, 11, 291, 16390, 50866, 50866, 257, 41473, 32674, 11, 597, 1709, 281, 1804, 13202, 13, 759, 291, 2408, 257, 3175, 41473, 32674, 11, 309, 1709, 281, 1804, 51112, 51112, 13202, 412, 4018, 13, 400, 550, 5911, 309, 311, 516, 281, 312, 445, 21039, 13, 407, 498, 291, 2408, 264, 51456, 51456, 732, 293, 16390, 472, 11, 291, 483, 341, 733, 295, 4052, 2445, 13, 400, 498, 291, 17522, 341, 2445, 11, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.11429279783497685, "compression_ratio": 1.74, "no_speech_prob": 1.5204695955617353e-05}, {"id": 219, "seek": 140532, "start": 1405.32, "end": 1414.6, "text": " you get just one. And therefore this shows you how this term enforces those spheres here", "tokens": [50364, 291, 483, 445, 472, 13, 400, 4412, 341, 3110, 291, 577, 341, 1433, 25495, 887, 729, 41225, 510, 50828, 50828, 281, 362, 257, 15845, 295, 472, 294, 1184, 3513, 13, 1436, 498, 309, 9898, 281, 312, 4356, 813, 472, 11, 51174, 51174, 341, 1507, 1709, 493, 382, 3219, 13, 400, 498, 309, 8637, 510, 11, 309, 1177, 380, 352, 382, 493, 382, 3219, 13, 407, 436, 51482, 51482, 366, 4748, 11, 9810, 1009, 412, 1935, 472, 420, 1922, 11, 457, 436, 1582, 380, 312, 382, 709, 4356, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11260317708109761, "compression_ratio": 1.6267281105990783, "no_speech_prob": 1.2605505617102608e-05}, {"id": 220, "seek": 140532, "start": 1414.6, "end": 1421.52, "text": " to have a radius of one in each direction. Because if it tries to be smaller than one,", "tokens": [50364, 291, 483, 445, 472, 13, 400, 4412, 341, 3110, 291, 577, 341, 1433, 25495, 887, 729, 41225, 510, 50828, 50828, 281, 362, 257, 15845, 295, 472, 294, 1184, 3513, 13, 1436, 498, 309, 9898, 281, 312, 4356, 813, 472, 11, 51174, 51174, 341, 1507, 1709, 493, 382, 3219, 13, 400, 498, 309, 8637, 510, 11, 309, 1177, 380, 352, 382, 493, 382, 3219, 13, 407, 436, 51482, 51482, 366, 4748, 11, 9810, 1009, 412, 1935, 472, 420, 1922, 11, 457, 436, 1582, 380, 312, 382, 709, 4356, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11260317708109761, "compression_ratio": 1.6267281105990783, "no_speech_prob": 1.2605505617102608e-05}, {"id": 221, "seek": 140532, "start": 1421.52, "end": 1427.6799999999998, "text": " this stuff goes up as crazy. And if it increases here, it doesn't go as up as crazy. So they", "tokens": [50364, 291, 483, 445, 472, 13, 400, 4412, 341, 3110, 291, 577, 341, 1433, 25495, 887, 729, 41225, 510, 50828, 50828, 281, 362, 257, 15845, 295, 472, 294, 1184, 3513, 13, 1436, 498, 309, 9898, 281, 312, 4356, 813, 472, 11, 51174, 51174, 341, 1507, 1709, 493, 382, 3219, 13, 400, 498, 309, 8637, 510, 11, 309, 1177, 380, 352, 382, 493, 382, 3219, 13, 407, 436, 51482, 51482, 366, 4748, 11, 9810, 1009, 412, 1935, 472, 420, 1922, 11, 457, 436, 1582, 380, 312, 382, 709, 4356, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11260317708109761, "compression_ratio": 1.6267281105990783, "no_speech_prob": 1.2605505617102608e-05}, {"id": 222, "seek": 140532, "start": 1427.6799999999998, "end": 1435.2, "text": " are slightly, roughly always at least one or half, but they won't be as much smaller", "tokens": [50364, 291, 483, 445, 472, 13, 400, 4412, 341, 3110, 291, 577, 341, 1433, 25495, 887, 729, 41225, 510, 50828, 50828, 281, 362, 257, 15845, 295, 472, 294, 1184, 3513, 13, 1436, 498, 309, 9898, 281, 312, 4356, 813, 472, 11, 51174, 51174, 341, 1507, 1709, 493, 382, 3219, 13, 400, 498, 309, 8637, 510, 11, 309, 1177, 380, 352, 382, 493, 382, 3219, 13, 407, 436, 51482, 51482, 366, 4748, 11, 9810, 1009, 412, 1935, 472, 420, 1922, 11, 457, 436, 1582, 380, 312, 382, 709, 4356, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11260317708109761, "compression_ratio": 1.6267281105990783, "no_speech_prob": 1.2605505617102608e-05}, {"id": 223, "seek": 143520, "start": 1435.2, "end": 1443.68, "text": " because this stuff increases a lot. So in this case, we have enforced the network not", "tokens": [50364, 570, 341, 1507, 8637, 257, 688, 13, 407, 294, 341, 1389, 11, 321, 362, 40953, 264, 3209, 406, 50788, 50788, 281, 15584, 613, 16295, 6051, 281, 652, 309, 1852, 552, 886, 709, 11, 558, 30, 1436, 5911, 436, 51120, 51120, 920, 483, 13661, 1602, 510, 13, 407, 550, 321, 362, 1071, 1433, 510, 11, 341, 462, 295, 1176, 1203, 51512, 51512, 8889, 293, 300, 311, 13735, 45729, 4711, 11, 597, 575, 257, 7285, 670, 456, 13, 400, 370, 341, 1433, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.14111126036871047, "compression_ratio": 1.5605381165919283, "no_speech_prob": 1.129182601289358e-05}, {"id": 224, "seek": 143520, "start": 1443.68, "end": 1450.32, "text": " to collapse these bubbles nor to make it grow them too much, right? Because otherwise they", "tokens": [50364, 570, 341, 1507, 8637, 257, 688, 13, 407, 294, 341, 1389, 11, 321, 362, 40953, 264, 3209, 406, 50788, 50788, 281, 15584, 613, 16295, 6051, 281, 652, 309, 1852, 552, 886, 709, 11, 558, 30, 1436, 5911, 436, 51120, 51120, 920, 483, 13661, 1602, 510, 13, 407, 550, 321, 362, 1071, 1433, 510, 11, 341, 462, 295, 1176, 1203, 51512, 51512, 8889, 293, 300, 311, 13735, 45729, 4711, 11, 597, 575, 257, 7285, 670, 456, 13, 400, 370, 341, 1433, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.14111126036871047, "compression_ratio": 1.5605381165919283, "no_speech_prob": 1.129182601289358e-05}, {"id": 225, "seek": 143520, "start": 1450.32, "end": 1458.16, "text": " still get penalized here. So then we have another term here, this E of Z everything", "tokens": [50364, 570, 341, 1507, 8637, 257, 688, 13, 407, 294, 341, 1389, 11, 321, 362, 40953, 264, 3209, 406, 50788, 50788, 281, 15584, 613, 16295, 6051, 281, 652, 309, 1852, 552, 886, 709, 11, 558, 30, 1436, 5911, 436, 51120, 51120, 920, 483, 13661, 1602, 510, 13, 407, 550, 321, 362, 1071, 1433, 510, 11, 341, 462, 295, 1176, 1203, 51512, 51512, 8889, 293, 300, 311, 13735, 45729, 4711, 11, 597, 575, 257, 7285, 670, 456, 13, 400, 370, 341, 1433, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.14111126036871047, "compression_ratio": 1.5605381165919283, "no_speech_prob": 1.129182601289358e-05}, {"id": 226, "seek": 143520, "start": 1458.16, "end": 1464.04, "text": " squared and that's classical parabola, which has a minimum over there. And so this term", "tokens": [50364, 570, 341, 1507, 8637, 257, 688, 13, 407, 294, 341, 1389, 11, 321, 362, 40953, 264, 3209, 406, 50788, 50788, 281, 15584, 613, 16295, 6051, 281, 652, 309, 1852, 552, 886, 709, 11, 558, 30, 1436, 5911, 436, 51120, 51120, 920, 483, 13661, 1602, 510, 13, 407, 550, 321, 362, 1071, 1433, 510, 11, 341, 462, 295, 1176, 1203, 51512, 51512, 8889, 293, 300, 311, 13735, 45729, 4711, 11, 597, 575, 257, 7285, 670, 456, 13, 400, 370, 341, 1433, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.14111126036871047, "compression_ratio": 1.5605381165919283, "no_speech_prob": 1.129182601289358e-05}, {"id": 227, "seek": 146404, "start": 1464.04, "end": 1470.52, "text": " here basically says that all the means should be condensed towards zero. And so basically", "tokens": [50364, 510, 1936, 1619, 300, 439, 264, 1355, 820, 312, 36398, 3030, 4018, 13, 400, 370, 1936, 50688, 50688, 291, 483, 411, 341, 4497, 3464, 510, 538, 341, 9656, 1252, 13, 400, 550, 291, 483, 300, 439, 51044, 51044, 729, 16295, 483, 2339, 12219, 1214, 666, 341, 3801, 12212, 13, 407, 510, 291, 483, 264, 16295, 51384, 51384, 295, 16295, 10290, 295, 257, 3034, 1478, 8399, 22660, 19866, 13, 1012, 4052, 307, 341, 30, 4372, 4052, 11, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.1613946318626404, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.694784441497177e-05}, {"id": 228, "seek": 146404, "start": 1470.52, "end": 1477.6399999999999, "text": " you get like this additional force here by this purple side. And then you get that all", "tokens": [50364, 510, 1936, 1619, 300, 439, 264, 1355, 820, 312, 36398, 3030, 4018, 13, 400, 370, 1936, 50688, 50688, 291, 483, 411, 341, 4497, 3464, 510, 538, 341, 9656, 1252, 13, 400, 550, 291, 483, 300, 439, 51044, 51044, 729, 16295, 483, 2339, 12219, 1214, 666, 341, 3801, 12212, 13, 407, 510, 291, 483, 264, 16295, 51384, 51384, 295, 16295, 10290, 295, 257, 3034, 1478, 8399, 22660, 19866, 13, 1012, 4052, 307, 341, 30, 4372, 4052, 11, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.1613946318626404, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.694784441497177e-05}, {"id": 229, "seek": 146404, "start": 1477.6399999999999, "end": 1484.44, "text": " those bubbles get squashed together into this bigger bubble. So here you get the bubbles", "tokens": [50364, 510, 1936, 1619, 300, 439, 264, 1355, 820, 312, 36398, 3030, 4018, 13, 400, 370, 1936, 50688, 50688, 291, 483, 411, 341, 4497, 3464, 510, 538, 341, 9656, 1252, 13, 400, 550, 291, 483, 300, 439, 51044, 51044, 729, 16295, 483, 2339, 12219, 1214, 666, 341, 3801, 12212, 13, 407, 510, 291, 483, 264, 16295, 51384, 51384, 295, 16295, 10290, 295, 257, 3034, 1478, 8399, 22660, 19866, 13, 1012, 4052, 307, 341, 30, 4372, 4052, 11, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.1613946318626404, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.694784441497177e-05}, {"id": 230, "seek": 146404, "start": 1484.44, "end": 1493.08, "text": " of bubbles representation of a variational autoencoder. How cute is this? Very cute,", "tokens": [50364, 510, 1936, 1619, 300, 439, 264, 1355, 820, 312, 36398, 3030, 4018, 13, 400, 370, 1936, 50688, 50688, 291, 483, 411, 341, 4497, 3464, 510, 538, 341, 9656, 1252, 13, 400, 550, 291, 483, 300, 439, 51044, 51044, 729, 16295, 483, 2339, 12219, 1214, 666, 341, 3801, 12212, 13, 407, 510, 291, 483, 264, 16295, 51384, 51384, 295, 16295, 10290, 295, 257, 3034, 1478, 8399, 22660, 19866, 13, 1012, 4052, 307, 341, 30, 4372, 4052, 11, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.1613946318626404, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.694784441497177e-05}, {"id": 231, "seek": 149308, "start": 1493.08, "end": 1503.76, "text": " right? How can you pack more bubbles? So what is the only parameter here, which is telling", "tokens": [50364, 558, 30, 1012, 393, 291, 2844, 544, 16295, 30, 407, 437, 307, 264, 787, 13075, 510, 11, 597, 307, 3585, 50898, 50898, 291, 264, 3800, 295, 428, 3034, 1478, 8399, 22660, 19866, 30, 467, 311, 516, 281, 312, 2935, 264, 10139, 413, 570, 51252, 51252, 2212, 257, 10139, 11, 291, 1009, 458, 577, 867, 16295, 291, 393, 2844, 294, 257, 4833, 12212, 11, 558, 30, 51568, 51568, 407, 309, 311, 445, 257, 2445, 295, 264, 10139, 291, 1888, 293, 291, 2826, 337, 428, 7633, 4583, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.14221151140001084, "compression_ratio": 1.6069868995633187, "no_speech_prob": 8.138940756907687e-06}, {"id": 232, "seek": 149308, "start": 1503.76, "end": 1510.84, "text": " you the strength of your variational autoencoder? It's going to be simply the dimension D because", "tokens": [50364, 558, 30, 1012, 393, 291, 2844, 544, 16295, 30, 407, 437, 307, 264, 787, 13075, 510, 11, 597, 307, 3585, 50898, 50898, 291, 264, 3800, 295, 428, 3034, 1478, 8399, 22660, 19866, 30, 467, 311, 516, 281, 312, 2935, 264, 10139, 413, 570, 51252, 51252, 2212, 257, 10139, 11, 291, 1009, 458, 577, 867, 16295, 291, 393, 2844, 294, 257, 4833, 12212, 11, 558, 30, 51568, 51568, 407, 309, 311, 445, 257, 2445, 295, 264, 10139, 291, 1888, 293, 291, 2826, 337, 428, 7633, 4583, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.14221151140001084, "compression_ratio": 1.6069868995633187, "no_speech_prob": 8.138940756907687e-06}, {"id": 233, "seek": 149308, "start": 1510.84, "end": 1517.1599999999999, "text": " given a dimension, you always know how many bubbles you can pack in a larger bubble, right?", "tokens": [50364, 558, 30, 1012, 393, 291, 2844, 544, 16295, 30, 407, 437, 307, 264, 787, 13075, 510, 11, 597, 307, 3585, 50898, 50898, 291, 264, 3800, 295, 428, 3034, 1478, 8399, 22660, 19866, 30, 467, 311, 516, 281, 312, 2935, 264, 10139, 413, 570, 51252, 51252, 2212, 257, 10139, 11, 291, 1009, 458, 577, 867, 16295, 291, 393, 2844, 294, 257, 4833, 12212, 11, 558, 30, 51568, 51568, 407, 309, 311, 445, 257, 2445, 295, 264, 10139, 291, 1888, 293, 291, 2826, 337, 428, 7633, 4583, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.14221151140001084, "compression_ratio": 1.6069868995633187, "no_speech_prob": 8.138940756907687e-06}, {"id": 234, "seek": 149308, "start": 1517.1599999999999, "end": 1522.1599999999999, "text": " So it's just a function of the dimension you pick and you choose for your hidden layer.", "tokens": [50364, 558, 30, 1012, 393, 291, 2844, 544, 16295, 30, 407, 437, 307, 264, 787, 13075, 510, 11, 597, 307, 3585, 50898, 50898, 291, 264, 3800, 295, 428, 3034, 1478, 8399, 22660, 19866, 30, 467, 311, 516, 281, 312, 2935, 264, 10139, 413, 570, 51252, 51252, 2212, 257, 10139, 11, 291, 1009, 458, 577, 867, 16295, 291, 393, 2844, 294, 257, 4833, 12212, 11, 558, 30, 51568, 51568, 407, 309, 311, 445, 257, 2445, 295, 264, 10139, 291, 1888, 293, 291, 2826, 337, 428, 7633, 4583, 13, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.14221151140001084, "compression_ratio": 1.6069868995633187, "no_speech_prob": 8.138940756907687e-06}, {"id": 235, "seek": 152216, "start": 1522.16, "end": 1528.1200000000001, "text": " So is the reconstruction last the first term, the yellow term? Is that the one that actually", "tokens": [50364, 407, 307, 264, 31565, 1036, 264, 700, 1433, 11, 264, 5566, 1433, 30, 1119, 300, 264, 472, 300, 767, 50662, 50662, 21020, 264, 16295, 3052, 4936, 293, 550, 264, 1472, 295, 309, 307, 437, 733, 295, 5965, 552, 50998, 50998, 490, 884, 300, 30, 407, 264, 31565, 576, 2944, 721, 926, 570, 321, 362, 613, 4497, 51512, 51512, 1940, 22219, 551, 11, 558, 30, 407, 498, 321, 2759, 380, 312, 1940, 22219, 11, 264, 31565, 1433, 51754, 51754], "temperature": 0.0, "avg_logprob": -0.13575528860092162, "compression_ratio": 1.8059701492537314, "no_speech_prob": 1.6186671928153373e-05}, {"id": 236, "seek": 152216, "start": 1528.1200000000001, "end": 1534.8400000000001, "text": " pushes the bubbles further apart and then the rest of it is what kind of keeps them", "tokens": [50364, 407, 307, 264, 31565, 1036, 264, 700, 1433, 11, 264, 5566, 1433, 30, 1119, 300, 264, 472, 300, 767, 50662, 50662, 21020, 264, 16295, 3052, 4936, 293, 550, 264, 1472, 295, 309, 307, 437, 733, 295, 5965, 552, 50998, 50998, 490, 884, 300, 30, 407, 264, 31565, 576, 2944, 721, 926, 570, 321, 362, 613, 4497, 51512, 51512, 1940, 22219, 551, 11, 558, 30, 407, 498, 321, 2759, 380, 312, 1940, 22219, 11, 264, 31565, 1433, 51754, 51754], "temperature": 0.0, "avg_logprob": -0.13575528860092162, "compression_ratio": 1.8059701492537314, "no_speech_prob": 1.6186671928153373e-05}, {"id": 237, "seek": 152216, "start": 1534.8400000000001, "end": 1545.1200000000001, "text": " from doing that? So the reconstruction would push things around because we have these additional", "tokens": [50364, 407, 307, 264, 31565, 1036, 264, 700, 1433, 11, 264, 5566, 1433, 30, 1119, 300, 264, 472, 300, 767, 50662, 50662, 21020, 264, 16295, 3052, 4936, 293, 550, 264, 1472, 295, 309, 307, 437, 733, 295, 5965, 552, 50998, 50998, 490, 884, 300, 30, 407, 264, 31565, 576, 2944, 721, 926, 570, 321, 362, 613, 4497, 51512, 51512, 1940, 22219, 551, 11, 558, 30, 407, 498, 321, 2759, 380, 312, 1940, 22219, 11, 264, 31565, 1433, 51754, 51754], "temperature": 0.0, "avg_logprob": -0.13575528860092162, "compression_ratio": 1.8059701492537314, "no_speech_prob": 1.6186671928153373e-05}, {"id": 238, "seek": 152216, "start": 1545.1200000000001, "end": 1549.96, "text": " taking volumes thing, right? So if we wouldn't be taking volumes, the reconstruction term", "tokens": [50364, 407, 307, 264, 31565, 1036, 264, 700, 1433, 11, 264, 5566, 1433, 30, 1119, 300, 264, 472, 300, 767, 50662, 50662, 21020, 264, 16295, 3052, 4936, 293, 550, 264, 1472, 295, 309, 307, 437, 733, 295, 5965, 552, 50998, 50998, 490, 884, 300, 30, 407, 264, 31565, 576, 2944, 721, 926, 570, 321, 362, 613, 4497, 51512, 51512, 1940, 22219, 551, 11, 558, 30, 407, 498, 321, 2759, 380, 312, 1940, 22219, 11, 264, 31565, 1433, 51754, 51754], "temperature": 0.0, "avg_logprob": -0.13575528860092162, "compression_ratio": 1.8059701492537314, "no_speech_prob": 1.6186671928153373e-05}, {"id": 239, "seek": 154996, "start": 1549.96, "end": 1555.16, "text": " wouldn't be pushing anything away because they don't overlap. Given that we actually", "tokens": [50364, 2759, 380, 312, 7380, 1340, 1314, 570, 436, 500, 380, 19959, 13, 18600, 300, 321, 767, 50624, 50624, 362, 512, 21669, 11, 264, 21669, 486, 362, 613, 2793, 767, 1940, 512, 5523, 293, 50930, 50930, 4412, 341, 31565, 486, 853, 281, 483, 729, 2793, 1314, 13, 407, 498, 291, 1520, 797, 51228, 51228, 729, 1326, 22868, 286, 855, 291, 13, 407, 321, 632, 412, 264, 2863, 11, 729, 645, 264, 2793, 365, 51550, 51550, 4497, 5658, 13, 823, 291, 483, 264, 31565, 300, 307, 7380, 1203, 1314, 13, 1396, 291, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.13131189346313477, "compression_ratio": 1.7732793522267207, "no_speech_prob": 1.8051734514301643e-05}, {"id": 240, "seek": 154996, "start": 1555.16, "end": 1561.28, "text": " have some variants, the variants will have these points actually taking some volume and", "tokens": [50364, 2759, 380, 312, 7380, 1340, 1314, 570, 436, 500, 380, 19959, 13, 18600, 300, 321, 767, 50624, 50624, 362, 512, 21669, 11, 264, 21669, 486, 362, 613, 2793, 767, 1940, 512, 5523, 293, 50930, 50930, 4412, 341, 31565, 486, 853, 281, 483, 729, 2793, 1314, 13, 407, 498, 291, 1520, 797, 51228, 51228, 729, 1326, 22868, 286, 855, 291, 13, 407, 321, 632, 412, 264, 2863, 11, 729, 645, 264, 2793, 365, 51550, 51550, 4497, 5658, 13, 823, 291, 483, 264, 31565, 300, 307, 7380, 1203, 1314, 13, 1396, 291, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.13131189346313477, "compression_ratio": 1.7732793522267207, "no_speech_prob": 1.8051734514301643e-05}, {"id": 241, "seek": 154996, "start": 1561.28, "end": 1567.24, "text": " therefore this reconstruction will try to get those points away. So if you check again", "tokens": [50364, 2759, 380, 312, 7380, 1340, 1314, 570, 436, 500, 380, 19959, 13, 18600, 300, 321, 767, 50624, 50624, 362, 512, 21669, 11, 264, 21669, 486, 362, 613, 2793, 767, 1940, 512, 5523, 293, 50930, 50930, 4412, 341, 31565, 486, 853, 281, 483, 729, 2793, 1314, 13, 407, 498, 291, 1520, 797, 51228, 51228, 729, 1326, 22868, 286, 855, 291, 13, 407, 321, 632, 412, 264, 2863, 11, 729, 645, 264, 2793, 365, 51550, 51550, 4497, 5658, 13, 823, 291, 483, 264, 31565, 300, 307, 7380, 1203, 1314, 13, 1396, 291, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.13131189346313477, "compression_ratio": 1.7732793522267207, "no_speech_prob": 1.8051734514301643e-05}, {"id": 242, "seek": 154996, "start": 1567.24, "end": 1573.68, "text": " those few animations I show you. So we had at the beginning, those were the points with", "tokens": [50364, 2759, 380, 312, 7380, 1340, 1314, 570, 436, 500, 380, 19959, 13, 18600, 300, 321, 767, 50624, 50624, 362, 512, 21669, 11, 264, 21669, 486, 362, 613, 2793, 767, 1940, 512, 5523, 293, 50930, 50930, 4412, 341, 31565, 486, 853, 281, 483, 729, 2793, 1314, 13, 407, 498, 291, 1520, 797, 51228, 51228, 729, 1326, 22868, 286, 855, 291, 13, 407, 321, 632, 412, 264, 2863, 11, 729, 645, 264, 2793, 365, 51550, 51550, 4497, 5658, 13, 823, 291, 483, 264, 31565, 300, 307, 7380, 1203, 1314, 13, 1396, 291, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.13131189346313477, "compression_ratio": 1.7732793522267207, "no_speech_prob": 1.8051734514301643e-05}, {"id": 243, "seek": 154996, "start": 1573.68, "end": 1579.72, "text": " additional noise. Now you get the reconstruction that is pushing everything away. Then you", "tokens": [50364, 2759, 380, 312, 7380, 1340, 1314, 570, 436, 500, 380, 19959, 13, 18600, 300, 321, 767, 50624, 50624, 362, 512, 21669, 11, 264, 21669, 486, 362, 613, 2793, 767, 1940, 512, 5523, 293, 50930, 50930, 4412, 341, 31565, 486, 853, 281, 483, 729, 2793, 1314, 13, 407, 498, 291, 1520, 797, 51228, 51228, 729, 1326, 22868, 286, 855, 291, 13, 407, 321, 632, 412, 264, 2863, 11, 729, 645, 264, 2793, 365, 51550, 51550, 4497, 5658, 13, 823, 291, 483, 264, 31565, 300, 307, 7380, 1203, 1314, 13, 1396, 291, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.13131189346313477, "compression_ratio": 1.7732793522267207, "no_speech_prob": 1.8051734514301643e-05}, {"id": 244, "seek": 157972, "start": 1579.72, "end": 1586.16, "text": " get the variance that is assuring you that those little bubbles don't collapse. And then", "tokens": [50364, 483, 264, 21977, 300, 307, 1256, 1345, 291, 300, 729, 707, 16295, 500, 380, 15584, 13, 400, 550, 50686, 50686, 291, 362, 264, 2572, 1433, 11, 597, 307, 264, 5587, 1433, 570, 309, 311, 264, 37262, 1433, 294, 264, 50942, 50942, 4470, 11, 597, 307, 1936, 5127, 341, 4497, 3321, 1270, 300, 439, 264, 707, 1074, 483, 51338, 51338, 13265, 3030, 4018, 13, 583, 436, 500, 380, 19959, 570, 456, 307, 264, 31565, 1433, 13, 51585, 51585], "temperature": 0.0, "avg_logprob": -0.13872838322120376, "compression_ratio": 1.669811320754717, "no_speech_prob": 6.000783832860179e-05}, {"id": 245, "seek": 157972, "start": 1586.16, "end": 1591.28, "text": " you have the final term, which is the spring term because it's the quadratic term in the", "tokens": [50364, 483, 264, 21977, 300, 307, 1256, 1345, 291, 300, 729, 707, 16295, 500, 380, 15584, 13, 400, 550, 50686, 50686, 291, 362, 264, 2572, 1433, 11, 597, 307, 264, 5587, 1433, 570, 309, 311, 264, 37262, 1433, 294, 264, 50942, 50942, 4470, 11, 597, 307, 1936, 5127, 341, 4497, 3321, 1270, 300, 439, 264, 707, 1074, 483, 51338, 51338, 13265, 3030, 4018, 13, 583, 436, 500, 380, 19959, 570, 456, 307, 264, 31565, 1433, 13, 51585, 51585], "temperature": 0.0, "avg_logprob": -0.13872838322120376, "compression_ratio": 1.669811320754717, "no_speech_prob": 6.000783832860179e-05}, {"id": 246, "seek": 157972, "start": 1591.28, "end": 1599.2, "text": " loss, which is basically adding this additional pressure such that all the little guys get", "tokens": [50364, 483, 264, 21977, 300, 307, 1256, 1345, 291, 300, 729, 707, 16295, 500, 380, 15584, 13, 400, 550, 50686, 50686, 291, 362, 264, 2572, 1433, 11, 597, 307, 264, 5587, 1433, 570, 309, 311, 264, 37262, 1433, 294, 264, 50942, 50942, 4470, 11, 597, 307, 1936, 5127, 341, 4497, 3321, 1270, 300, 439, 264, 707, 1074, 483, 51338, 51338, 13265, 3030, 4018, 13, 583, 436, 500, 380, 19959, 570, 456, 307, 264, 31565, 1433, 13, 51585, 51585], "temperature": 0.0, "avg_logprob": -0.13872838322120376, "compression_ratio": 1.669811320754717, "no_speech_prob": 6.000783832860179e-05}, {"id": 247, "seek": 157972, "start": 1599.2, "end": 1604.14, "text": " packed towards zero. But they don't overlap because there is the reconstruction term.", "tokens": [50364, 483, 264, 21977, 300, 307, 1256, 1345, 291, 300, 729, 707, 16295, 500, 380, 15584, 13, 400, 550, 50686, 50686, 291, 362, 264, 2572, 1433, 11, 597, 307, 264, 5587, 1433, 570, 309, 311, 264, 37262, 1433, 294, 264, 50942, 50942, 4470, 11, 597, 307, 1936, 5127, 341, 4497, 3321, 1270, 300, 439, 264, 707, 1074, 483, 51338, 51338, 13265, 3030, 4018, 13, 583, 436, 500, 380, 19959, 570, 456, 307, 264, 31565, 1433, 13, 51585, 51585], "temperature": 0.0, "avg_logprob": -0.13872838322120376, "compression_ratio": 1.669811320754717, "no_speech_prob": 6.000783832860179e-05}, {"id": 248, "seek": 160414, "start": 1604.14, "end": 1612.64, "text": " So no overlap due to the reconstruction. Size not going to small than one because of the", "tokens": [50364, 407, 572, 19959, 3462, 281, 264, 31565, 13, 35818, 406, 516, 281, 1359, 813, 472, 570, 295, 264, 50789, 50789, 700, 644, 295, 264, 4972, 30867, 13, 400, 550, 439, 613, 1074, 366, 13265, 797, 337, 264, 37262, 51059, 51059, 644, 11, 597, 307, 264, 5587, 3464, 13, 51227, 51227, 1119, 264, 9861, 1433, 746, 300, 2203, 281, 312, 10870, 411, 257, 9848, 2181, 335, 2398, 733, 295, 551, 30, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.20721521893063108, "compression_ratio": 1.5226130653266332, "no_speech_prob": 2.4537686840631068e-05}, {"id": 249, "seek": 160414, "start": 1612.64, "end": 1618.0400000000002, "text": " first part of the relative entropy. And then all these guys are packed again for the quadratic", "tokens": [50364, 407, 572, 19959, 3462, 281, 264, 31565, 13, 35818, 406, 516, 281, 1359, 813, 472, 570, 295, 264, 50789, 50789, 700, 644, 295, 264, 4972, 30867, 13, 400, 550, 439, 613, 1074, 366, 13265, 797, 337, 264, 37262, 51059, 51059, 644, 11, 597, 307, 264, 5587, 3464, 13, 51227, 51227, 1119, 264, 9861, 1433, 746, 300, 2203, 281, 312, 10870, 411, 257, 9848, 2181, 335, 2398, 733, 295, 551, 30, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.20721521893063108, "compression_ratio": 1.5226130653266332, "no_speech_prob": 2.4537686840631068e-05}, {"id": 250, "seek": 160414, "start": 1618.0400000000002, "end": 1621.4, "text": " part, which is the spring force.", "tokens": [50364, 407, 572, 19959, 3462, 281, 264, 31565, 13, 35818, 406, 516, 281, 1359, 813, 472, 570, 295, 264, 50789, 50789, 700, 644, 295, 264, 4972, 30867, 13, 400, 550, 439, 613, 1074, 366, 13265, 797, 337, 264, 37262, 51059, 51059, 644, 11, 597, 307, 264, 5587, 3464, 13, 51227, 51227, 1119, 264, 9861, 1433, 746, 300, 2203, 281, 312, 10870, 411, 257, 9848, 2181, 335, 2398, 733, 295, 551, 30, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.20721521893063108, "compression_ratio": 1.5226130653266332, "no_speech_prob": 2.4537686840631068e-05}, {"id": 251, "seek": 160414, "start": 1621.4, "end": 1628.7, "text": " Is the beta term something that needs to be tuned like a hyperparameter kind of thing?", "tokens": [50364, 407, 572, 19959, 3462, 281, 264, 31565, 13, 35818, 406, 516, 281, 1359, 813, 472, 570, 295, 264, 50789, 50789, 700, 644, 295, 264, 4972, 30867, 13, 400, 550, 439, 613, 1074, 366, 13265, 797, 337, 264, 37262, 51059, 51059, 644, 11, 597, 307, 264, 5587, 3464, 13, 51227, 51227, 1119, 264, 9861, 1433, 746, 300, 2203, 281, 312, 10870, 411, 257, 9848, 2181, 335, 2398, 733, 295, 551, 30, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.20721521893063108, "compression_ratio": 1.5226130653266332, "no_speech_prob": 2.4537686840631068e-05}, {"id": 252, "seek": 162870, "start": 1628.7, "end": 1634.96, "text": " So the beta is the actual, in the original version of this variational encoder, there", "tokens": [50364, 407, 264, 9861, 307, 264, 3539, 11, 294, 264, 3380, 3037, 295, 341, 3034, 1478, 2058, 19866, 11, 456, 50677, 50677, 390, 572, 9861, 13, 400, 550, 456, 307, 257, 3035, 597, 307, 264, 9861, 3034, 1478, 2058, 19866, 13, 1449, 281, 584, 51019, 51019, 300, 291, 393, 764, 257, 9848, 2181, 335, 2398, 281, 1319, 577, 709, 613, 732, 2115, 10586, 337, 264, 51369, 51369, 2572, 4470, 13, 51477, 51477, 639, 4470, 11, 264, 1150, 4470, 1433, 365, 264, 9861, 11, 300, 311, 264, 47991, 47387, 13, 51745, 51745, 865, 13, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.18743425607681274, "compression_ratio": 1.6542056074766356, "no_speech_prob": 8.265571523224935e-06}, {"id": 253, "seek": 162870, "start": 1634.96, "end": 1641.8, "text": " was no beta. And then there is a paper which is the beta variational encoder. Just to say", "tokens": [50364, 407, 264, 9861, 307, 264, 3539, 11, 294, 264, 3380, 3037, 295, 341, 3034, 1478, 2058, 19866, 11, 456, 50677, 50677, 390, 572, 9861, 13, 400, 550, 456, 307, 257, 3035, 597, 307, 264, 9861, 3034, 1478, 2058, 19866, 13, 1449, 281, 584, 51019, 51019, 300, 291, 393, 764, 257, 9848, 2181, 335, 2398, 281, 1319, 577, 709, 613, 732, 2115, 10586, 337, 264, 51369, 51369, 2572, 4470, 13, 51477, 51477, 639, 4470, 11, 264, 1150, 4470, 1433, 365, 264, 9861, 11, 300, 311, 264, 47991, 47387, 13, 51745, 51745, 865, 13, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.18743425607681274, "compression_ratio": 1.6542056074766356, "no_speech_prob": 8.265571523224935e-06}, {"id": 254, "seek": 162870, "start": 1641.8, "end": 1648.8, "text": " that you can use a hyperparameter to change how much these two terms contribute for the", "tokens": [50364, 407, 264, 9861, 307, 264, 3539, 11, 294, 264, 3380, 3037, 295, 341, 3034, 1478, 2058, 19866, 11, 456, 50677, 50677, 390, 572, 9861, 13, 400, 550, 456, 307, 257, 3035, 597, 307, 264, 9861, 3034, 1478, 2058, 19866, 13, 1449, 281, 584, 51019, 51019, 300, 291, 393, 764, 257, 9848, 2181, 335, 2398, 281, 1319, 577, 709, 613, 732, 2115, 10586, 337, 264, 51369, 51369, 2572, 4470, 13, 51477, 51477, 639, 4470, 11, 264, 1150, 4470, 1433, 365, 264, 9861, 11, 300, 311, 264, 47991, 47387, 13, 51745, 51745, 865, 13, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.18743425607681274, "compression_ratio": 1.6542056074766356, "no_speech_prob": 8.265571523224935e-06}, {"id": 255, "seek": 162870, "start": 1648.8, "end": 1650.96, "text": " final loss.", "tokens": [50364, 407, 264, 9861, 307, 264, 3539, 11, 294, 264, 3380, 3037, 295, 341, 3034, 1478, 2058, 19866, 11, 456, 50677, 50677, 390, 572, 9861, 13, 400, 550, 456, 307, 257, 3035, 597, 307, 264, 9861, 3034, 1478, 2058, 19866, 13, 1449, 281, 584, 51019, 51019, 300, 291, 393, 764, 257, 9848, 2181, 335, 2398, 281, 1319, 577, 709, 613, 732, 2115, 10586, 337, 264, 51369, 51369, 2572, 4470, 13, 51477, 51477, 639, 4470, 11, 264, 1150, 4470, 1433, 365, 264, 9861, 11, 300, 311, 264, 47991, 47387, 13, 51745, 51745, 865, 13, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.18743425607681274, "compression_ratio": 1.6542056074766356, "no_speech_prob": 8.265571523224935e-06}, {"id": 256, "seek": 162870, "start": 1650.96, "end": 1656.32, "text": " This loss, the second loss term with the beta, that's the KL divergence.", "tokens": [50364, 407, 264, 9861, 307, 264, 3539, 11, 294, 264, 3380, 3037, 295, 341, 3034, 1478, 2058, 19866, 11, 456, 50677, 50677, 390, 572, 9861, 13, 400, 550, 456, 307, 257, 3035, 597, 307, 264, 9861, 3034, 1478, 2058, 19866, 13, 1449, 281, 584, 51019, 51019, 300, 291, 393, 764, 257, 9848, 2181, 335, 2398, 281, 1319, 577, 709, 613, 732, 2115, 10586, 337, 264, 51369, 51369, 2572, 4470, 13, 51477, 51477, 639, 4470, 11, 264, 1150, 4470, 1433, 365, 264, 9861, 11, 300, 311, 264, 47991, 47387, 13, 51745, 51745, 865, 13, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.18743425607681274, "compression_ratio": 1.6542056074766356, "no_speech_prob": 8.265571523224935e-06}, {"id": 257, "seek": 162870, "start": 1656.32, "end": 1657.32, "text": " Yeah.", "tokens": [50364, 407, 264, 9861, 307, 264, 3539, 11, 294, 264, 3380, 3037, 295, 341, 3034, 1478, 2058, 19866, 11, 456, 50677, 50677, 390, 572, 9861, 13, 400, 550, 456, 307, 257, 3035, 597, 307, 264, 9861, 3034, 1478, 2058, 19866, 13, 1449, 281, 584, 51019, 51019, 300, 291, 393, 764, 257, 9848, 2181, 335, 2398, 281, 1319, 577, 709, 613, 732, 2115, 10586, 337, 264, 51369, 51369, 2572, 4470, 13, 51477, 51477, 639, 4470, 11, 264, 1150, 4470, 1433, 365, 264, 9861, 11, 300, 311, 264, 47991, 47387, 13, 51745, 51745, 865, 13, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.18743425607681274, "compression_ratio": 1.6542056074766356, "no_speech_prob": 8.265571523224935e-06}, {"id": 258, "seek": 165732, "start": 1657.32, "end": 1659.3999999999999, "text": " Between the Z and the normal distribution, right?", "tokens": [50364, 18967, 264, 1176, 293, 264, 2710, 7316, 11, 558, 30, 50468, 50468, 865, 11, 1296, 264, 1176, 11, 597, 307, 1348, 490, 257, 39148, 295, 914, 462, 293, 21977, 691, 13, 400, 550, 50834, 50834, 264, 1150, 1433, 307, 516, 281, 312, 341, 2710, 7316, 13, 400, 370, 341, 1433, 9898, 281, 483, 51146, 51146, 1176, 281, 312, 382, 1998, 382, 1944, 281, 257, 2710, 7316, 294, 264, 1901, 11, 264, 18795, 1901, 13, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.2162770735911834, "compression_ratio": 1.7597765363128492, "no_speech_prob": 1.5679648640798405e-05}, {"id": 259, "seek": 165732, "start": 1659.3999999999999, "end": 1666.72, "text": " Yeah, between the Z, which is coming from a Gaussian of mean E and variance V. And then", "tokens": [50364, 18967, 264, 1176, 293, 264, 2710, 7316, 11, 558, 30, 50468, 50468, 865, 11, 1296, 264, 1176, 11, 597, 307, 1348, 490, 257, 39148, 295, 914, 462, 293, 21977, 691, 13, 400, 550, 50834, 50834, 264, 1150, 1433, 307, 516, 281, 312, 341, 2710, 7316, 13, 400, 370, 341, 1433, 9898, 281, 483, 51146, 51146, 1176, 281, 312, 382, 1998, 382, 1944, 281, 257, 2710, 7316, 294, 264, 1901, 11, 264, 18795, 1901, 13, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.2162770735911834, "compression_ratio": 1.7597765363128492, "no_speech_prob": 1.5679648640798405e-05}, {"id": 260, "seek": 165732, "start": 1666.72, "end": 1672.96, "text": " the second term is going to be this normal distribution. And so this term tries to get", "tokens": [50364, 18967, 264, 1176, 293, 264, 2710, 7316, 11, 558, 30, 50468, 50468, 865, 11, 1296, 264, 1176, 11, 597, 307, 1348, 490, 257, 39148, 295, 914, 462, 293, 21977, 691, 13, 400, 550, 50834, 50834, 264, 1150, 1433, 307, 516, 281, 312, 341, 2710, 7316, 13, 400, 370, 341, 1433, 9898, 281, 483, 51146, 51146, 1176, 281, 312, 382, 1998, 382, 1944, 281, 257, 2710, 7316, 294, 264, 1901, 11, 264, 18795, 1901, 13, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.2162770735911834, "compression_ratio": 1.7597765363128492, "no_speech_prob": 1.5679648640798405e-05}, {"id": 261, "seek": 165732, "start": 1672.96, "end": 1681.96, "text": " Z to be as close as possible to a normal distribution in the space, the dimensional space.", "tokens": [50364, 18967, 264, 1176, 293, 264, 2710, 7316, 11, 558, 30, 50468, 50468, 865, 11, 1296, 264, 1176, 11, 597, 307, 1348, 490, 257, 39148, 295, 914, 462, 293, 21977, 691, 13, 400, 550, 50834, 50834, 264, 1150, 1433, 307, 516, 281, 312, 341, 2710, 7316, 13, 400, 370, 341, 1433, 9898, 281, 483, 51146, 51146, 1176, 281, 312, 382, 1998, 382, 1944, 281, 257, 2710, 7316, 294, 264, 1901, 11, 264, 18795, 1901, 13, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.2162770735911834, "compression_ratio": 1.7597765363128492, "no_speech_prob": 1.5679648640798405e-05}, {"id": 262, "seek": 168196, "start": 1681.96, "end": 1691.92, "text": " Okay. And this formula that you're working down, that's a generic KL language formula.", "tokens": [50364, 1033, 13, 400, 341, 8513, 300, 291, 434, 1364, 760, 11, 300, 311, 257, 19577, 47991, 2856, 8513, 13, 50862, 50862, 407, 286, 576, 2748, 291, 281, 747, 257, 3035, 293, 3435, 293, 550, 853, 281, 2464, 264, 4972, 30867, 51216, 51216, 1296, 257, 39148, 293, 257, 2710, 7316, 13, 400, 550, 291, 820, 483, 439, 613, 2115, 13, 51530, 51530, 440, 4972, 30867, 13, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.2292741554370825, "compression_ratio": 1.5105263157894737, "no_speech_prob": 1.0446965461596847e-05}, {"id": 263, "seek": 168196, "start": 1691.92, "end": 1699.0, "text": " So I would recommend you to take a paper and pen and then try to write the relative entropy", "tokens": [50364, 1033, 13, 400, 341, 8513, 300, 291, 434, 1364, 760, 11, 300, 311, 257, 19577, 47991, 2856, 8513, 13, 50862, 50862, 407, 286, 576, 2748, 291, 281, 747, 257, 3035, 293, 3435, 293, 550, 853, 281, 2464, 264, 4972, 30867, 51216, 51216, 1296, 257, 39148, 293, 257, 2710, 7316, 13, 400, 550, 291, 820, 483, 439, 613, 2115, 13, 51530, 51530, 440, 4972, 30867, 13, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.2292741554370825, "compression_ratio": 1.5105263157894737, "no_speech_prob": 1.0446965461596847e-05}, {"id": 264, "seek": 168196, "start": 1699.0, "end": 1705.28, "text": " between a Gaussian and a normal distribution. And then you should get all these terms.", "tokens": [50364, 1033, 13, 400, 341, 8513, 300, 291, 434, 1364, 760, 11, 300, 311, 257, 19577, 47991, 2856, 8513, 13, 50862, 50862, 407, 286, 576, 2748, 291, 281, 747, 257, 3035, 293, 3435, 293, 550, 853, 281, 2464, 264, 4972, 30867, 51216, 51216, 1296, 257, 39148, 293, 257, 2710, 7316, 13, 400, 550, 291, 820, 483, 439, 613, 2115, 13, 51530, 51530, 440, 4972, 30867, 13, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.2292741554370825, "compression_ratio": 1.5105263157894737, "no_speech_prob": 1.0446965461596847e-05}, {"id": 265, "seek": 168196, "start": 1705.28, "end": 1707.1200000000001, "text": " The relative entropy.", "tokens": [50364, 1033, 13, 400, 341, 8513, 300, 291, 434, 1364, 760, 11, 300, 311, 257, 19577, 47991, 2856, 8513, 13, 50862, 50862, 407, 286, 576, 2748, 291, 281, 747, 257, 3035, 293, 3435, 293, 550, 853, 281, 2464, 264, 4972, 30867, 51216, 51216, 1296, 257, 39148, 293, 257, 2710, 7316, 13, 400, 550, 291, 820, 483, 439, 613, 2115, 13, 51530, 51530, 440, 4972, 30867, 13, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.2292741554370825, "compression_ratio": 1.5105263157894737, "no_speech_prob": 1.0446965461596847e-05}, {"id": 266, "seek": 170712, "start": 1707.12, "end": 1715.32, "text": " So yeah, this LKL is the relative entropy. Yeah. So just look up the formula for the", "tokens": [50364, 407, 1338, 11, 341, 441, 42, 43, 307, 264, 4972, 30867, 13, 865, 13, 407, 445, 574, 493, 264, 8513, 337, 264, 50774, 50774, 4972, 30867, 11, 597, 307, 3585, 291, 1936, 577, 1400, 732, 37870, 366, 13, 400, 264, 700, 51152, 51152, 7316, 307, 516, 281, 312, 257, 2120, 592, 3504, 473, 39148, 13, 400, 264, 1150, 472, 307, 516, 281, 312, 51400, 51400, 257, 2710, 7316, 11, 558, 30, 51508, 51508, 2014, 264, 2710, 37870, 406, 264, 912, 551, 30, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.1394816664762275, "compression_ratio": 1.7731958762886597, "no_speech_prob": 6.438815034925938e-06}, {"id": 267, "seek": 170712, "start": 1715.32, "end": 1722.8799999999999, "text": " relative entropy, which is telling you basically how far two distributions are. And the first", "tokens": [50364, 407, 1338, 11, 341, 441, 42, 43, 307, 264, 4972, 30867, 13, 865, 13, 407, 445, 574, 493, 264, 8513, 337, 264, 50774, 50774, 4972, 30867, 11, 597, 307, 3585, 291, 1936, 577, 1400, 732, 37870, 366, 13, 400, 264, 700, 51152, 51152, 7316, 307, 516, 281, 312, 257, 2120, 592, 3504, 473, 39148, 13, 400, 264, 1150, 472, 307, 516, 281, 312, 51400, 51400, 257, 2710, 7316, 11, 558, 30, 51508, 51508, 2014, 264, 2710, 37870, 406, 264, 912, 551, 30, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.1394816664762275, "compression_ratio": 1.7731958762886597, "no_speech_prob": 6.438815034925938e-06}, {"id": 268, "seek": 170712, "start": 1722.8799999999999, "end": 1727.84, "text": " distribution is going to be a multivariate Gaussian. And the second one is going to be", "tokens": [50364, 407, 1338, 11, 341, 441, 42, 43, 307, 264, 4972, 30867, 13, 865, 13, 407, 445, 574, 493, 264, 8513, 337, 264, 50774, 50774, 4972, 30867, 11, 597, 307, 3585, 291, 1936, 577, 1400, 732, 37870, 366, 13, 400, 264, 700, 51152, 51152, 7316, 307, 516, 281, 312, 257, 2120, 592, 3504, 473, 39148, 13, 400, 264, 1150, 472, 307, 516, 281, 312, 51400, 51400, 257, 2710, 7316, 11, 558, 30, 51508, 51508, 2014, 264, 2710, 37870, 406, 264, 912, 551, 30, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.1394816664762275, "compression_ratio": 1.7731958762886597, "no_speech_prob": 6.438815034925938e-06}, {"id": 269, "seek": 170712, "start": 1727.84, "end": 1730.0, "text": " a normal distribution, right?", "tokens": [50364, 407, 1338, 11, 341, 441, 42, 43, 307, 264, 4972, 30867, 13, 865, 13, 407, 445, 574, 493, 264, 8513, 337, 264, 50774, 50774, 4972, 30867, 11, 597, 307, 3585, 291, 1936, 577, 1400, 732, 37870, 366, 13, 400, 264, 700, 51152, 51152, 7316, 307, 516, 281, 312, 257, 2120, 592, 3504, 473, 39148, 13, 400, 264, 1150, 472, 307, 516, 281, 312, 51400, 51400, 257, 2710, 7316, 11, 558, 30, 51508, 51508, 2014, 264, 2710, 37870, 406, 264, 912, 551, 30, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.1394816664762275, "compression_ratio": 1.7731958762886597, "no_speech_prob": 6.438815034925938e-06}, {"id": 270, "seek": 170712, "start": 1730.0, "end": 1733.8799999999999, "text": " Are the normal distributions not the same thing?", "tokens": [50364, 407, 1338, 11, 341, 441, 42, 43, 307, 264, 4972, 30867, 13, 865, 13, 407, 445, 574, 493, 264, 8513, 337, 264, 50774, 50774, 4972, 30867, 11, 597, 307, 3585, 291, 1936, 577, 1400, 732, 37870, 366, 13, 400, 264, 700, 51152, 51152, 7316, 307, 516, 281, 312, 257, 2120, 592, 3504, 473, 39148, 13, 400, 264, 1150, 472, 307, 516, 281, 312, 51400, 51400, 257, 2710, 7316, 11, 558, 30, 51508, 51508, 2014, 264, 2710, 37870, 406, 264, 912, 551, 30, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.1394816664762275, "compression_ratio": 1.7731958762886597, "no_speech_prob": 6.438815034925938e-06}, {"id": 271, "seek": 173388, "start": 1733.88, "end": 1742.5600000000002, "text": " The Gaussian has a mean vector and the covariance matrix. The normal has zero mean and identity", "tokens": [50364, 440, 39148, 575, 257, 914, 8062, 293, 264, 49851, 719, 8141, 13, 440, 2710, 575, 4018, 914, 293, 6575, 50798, 50798, 8141, 337, 264, 49851, 719, 8141, 13, 50928, 50928, 492, 848, 3071, 1673, 300, 264, 1176, 820, 406, 362, 49851, 719, 13, 467, 820, 312, 21539, 11, 558, 30, 51142, 51142, 865, 13, 407, 309, 311, 516, 281, 312, 21539, 11, 457, 264, 4190, 322, 264, 21539, 366, 729, 691, 1374, 21518, 13, 51498, 51498, 467, 311, 364, 766, 12, 36814, 955, 2710, 5717, 257, 18988, 1359, 2710, 30, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.18651507763152428, "compression_ratio": 1.7688679245283019, "no_speech_prob": 0.00012505342601798475}, {"id": 272, "seek": 173388, "start": 1742.5600000000002, "end": 1745.16, "text": " matrix for the covariance matrix.", "tokens": [50364, 440, 39148, 575, 257, 914, 8062, 293, 264, 49851, 719, 8141, 13, 440, 2710, 575, 4018, 914, 293, 6575, 50798, 50798, 8141, 337, 264, 49851, 719, 8141, 13, 50928, 50928, 492, 848, 3071, 1673, 300, 264, 1176, 820, 406, 362, 49851, 719, 13, 467, 820, 312, 21539, 11, 558, 30, 51142, 51142, 865, 13, 407, 309, 311, 516, 281, 312, 21539, 11, 457, 264, 4190, 322, 264, 21539, 366, 729, 691, 1374, 21518, 13, 51498, 51498, 467, 311, 364, 766, 12, 36814, 955, 2710, 5717, 257, 18988, 1359, 2710, 30, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.18651507763152428, "compression_ratio": 1.7688679245283019, "no_speech_prob": 0.00012505342601798475}, {"id": 273, "seek": 173388, "start": 1745.16, "end": 1749.44, "text": " We said earlier though that the Z should not have covariance. It should be diagonal, right?", "tokens": [50364, 440, 39148, 575, 257, 914, 8062, 293, 264, 49851, 719, 8141, 13, 440, 2710, 575, 4018, 914, 293, 6575, 50798, 50798, 8141, 337, 264, 49851, 719, 8141, 13, 50928, 50928, 492, 848, 3071, 1673, 300, 264, 1176, 820, 406, 362, 49851, 719, 13, 467, 820, 312, 21539, 11, 558, 30, 51142, 51142, 865, 13, 407, 309, 311, 516, 281, 312, 21539, 11, 457, 264, 4190, 322, 264, 21539, 366, 729, 691, 1374, 21518, 13, 51498, 51498, 467, 311, 364, 766, 12, 36814, 955, 2710, 5717, 257, 18988, 1359, 2710, 30, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.18651507763152428, "compression_ratio": 1.7688679245283019, "no_speech_prob": 0.00012505342601798475}, {"id": 274, "seek": 173388, "start": 1749.44, "end": 1756.5600000000002, "text": " Yeah. So it's going to be diagonal, but the values on the diagonal are those V variances.", "tokens": [50364, 440, 39148, 575, 257, 914, 8062, 293, 264, 49851, 719, 8141, 13, 440, 2710, 575, 4018, 914, 293, 6575, 50798, 50798, 8141, 337, 264, 49851, 719, 8141, 13, 50928, 50928, 492, 848, 3071, 1673, 300, 264, 1176, 820, 406, 362, 49851, 719, 13, 467, 820, 312, 21539, 11, 558, 30, 51142, 51142, 865, 13, 407, 309, 311, 516, 281, 312, 21539, 11, 457, 264, 4190, 322, 264, 21539, 366, 729, 691, 1374, 21518, 13, 51498, 51498, 467, 311, 364, 766, 12, 36814, 955, 2710, 5717, 257, 18988, 1359, 2710, 30, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.18651507763152428, "compression_ratio": 1.7688679245283019, "no_speech_prob": 0.00012505342601798475}, {"id": 275, "seek": 173388, "start": 1756.5600000000002, "end": 1761.72, "text": " It's an off-centered big normal versus a centered small normal?", "tokens": [50364, 440, 39148, 575, 257, 914, 8062, 293, 264, 49851, 719, 8141, 13, 440, 2710, 575, 4018, 914, 293, 6575, 50798, 50798, 8141, 337, 264, 49851, 719, 8141, 13, 50928, 50928, 492, 848, 3071, 1673, 300, 264, 1176, 820, 406, 362, 49851, 719, 13, 467, 820, 312, 21539, 11, 558, 30, 51142, 51142, 865, 13, 407, 309, 311, 516, 281, 312, 21539, 11, 457, 264, 4190, 322, 264, 21539, 366, 729, 691, 1374, 21518, 13, 51498, 51498, 467, 311, 364, 766, 12, 36814, 955, 2710, 5717, 257, 18988, 1359, 2710, 30, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.18651507763152428, "compression_ratio": 1.7688679245283019, "no_speech_prob": 0.00012505342601798475}, {"id": 276, "seek": 176172, "start": 1761.72, "end": 1772.08, "text": " So it's off-center and then each direction is scaled by the standard deviation of that", "tokens": [50364, 407, 309, 311, 766, 12, 50085, 293, 550, 1184, 3513, 307, 36039, 538, 264, 3832, 25163, 295, 300, 50882, 50882, 10139, 13, 407, 498, 291, 362, 257, 2416, 3832, 25163, 294, 472, 10139, 11, 309, 1355, 300, 51174, 51174, 294, 300, 3513, 309, 311, 588, 11, 588, 3974, 13, 4387, 2020, 30, 583, 456, 307, 257, 1622, 13, 467, 311, 257, 1622, 51576, 51576, 322, 264, 413, 10298, 11, 558, 30, 1436, 797, 11, 439, 264, 6677, 366, 6695, 13, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.18375258218674434, "compression_ratio": 1.6067961165048543, "no_speech_prob": 5.953342224529479e-06}, {"id": 277, "seek": 176172, "start": 1772.08, "end": 1777.92, "text": " dimension. So if you have a large standard deviation in one dimension, it means that", "tokens": [50364, 407, 309, 311, 766, 12, 50085, 293, 550, 1184, 3513, 307, 36039, 538, 264, 3832, 25163, 295, 300, 50882, 50882, 10139, 13, 407, 498, 291, 362, 257, 2416, 3832, 25163, 294, 472, 10139, 11, 309, 1355, 300, 51174, 51174, 294, 300, 3513, 309, 311, 588, 11, 588, 3974, 13, 4387, 2020, 30, 583, 456, 307, 257, 1622, 13, 467, 311, 257, 1622, 51576, 51576, 322, 264, 413, 10298, 11, 558, 30, 1436, 797, 11, 439, 264, 6677, 366, 6695, 13, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.18375258218674434, "compression_ratio": 1.6067961165048543, "no_speech_prob": 5.953342224529479e-06}, {"id": 278, "seek": 176172, "start": 1777.92, "end": 1785.96, "text": " in that direction it's very, very spread. Make sense? But there is a line. It's a line", "tokens": [50364, 407, 309, 311, 766, 12, 50085, 293, 550, 1184, 3513, 307, 36039, 538, 264, 3832, 25163, 295, 300, 50882, 50882, 10139, 13, 407, 498, 291, 362, 257, 2416, 3832, 25163, 294, 472, 10139, 11, 309, 1355, 300, 51174, 51174, 294, 300, 3513, 309, 311, 588, 11, 588, 3974, 13, 4387, 2020, 30, 583, 456, 307, 257, 1622, 13, 467, 311, 257, 1622, 51576, 51576, 322, 264, 413, 10298, 11, 558, 30, 1436, 797, 11, 439, 264, 6677, 366, 6695, 13, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.18375258218674434, "compression_ratio": 1.6067961165048543, "no_speech_prob": 5.953342224529479e-06}, {"id": 279, "seek": 176172, "start": 1785.96, "end": 1791.1200000000001, "text": " on the D axis, right? Because again, all the components are independent.", "tokens": [50364, 407, 309, 311, 766, 12, 50085, 293, 550, 1184, 3513, 307, 36039, 538, 264, 3832, 25163, 295, 300, 50882, 50882, 10139, 13, 407, 498, 291, 362, 257, 2416, 3832, 25163, 294, 472, 10139, 11, 309, 1355, 300, 51174, 51174, 294, 300, 3513, 309, 311, 588, 11, 588, 3974, 13, 4387, 2020, 30, 583, 456, 307, 257, 1622, 13, 467, 311, 257, 1622, 51576, 51576, 322, 264, 413, 10298, 11, 558, 30, 1436, 797, 11, 439, 264, 6677, 366, 6695, 13, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.18375258218674434, "compression_ratio": 1.6067961165048543, "no_speech_prob": 5.953342224529479e-06}, {"id": 280, "seek": 179112, "start": 1791.12, "end": 1797.6399999999999, "text": " Yeah. Is the reconstruction loss the pixel-wise distance between the final out and the original", "tokens": [50364, 865, 13, 1119, 264, 31565, 4470, 264, 19261, 12, 3711, 4560, 1296, 264, 2572, 484, 293, 264, 3380, 50690, 50690, 3256, 30, 50740, 50740, 440, 31565, 4470, 11, 321, 1866, 300, 1036, 1243, 293, 321, 362, 732, 3956, 337, 264, 31565, 51042, 51042, 4470, 13, 1485, 390, 264, 17434, 337, 17434, 1412, 293, 321, 362, 264, 17434, 3278, 30867, 13, 400, 264, 51442, 51442, 661, 472, 307, 516, 281, 312, 2602, 264, 957, 2158, 472, 13, 407, 309, 311, 1270, 300, 291, 393, 764, 264, 51726, 51726, 1922, 420, 731, 11, 264, 7395, 34, 11, 558, 30, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.23093657918495708, "compression_ratio": 1.7467248908296944, "no_speech_prob": 4.9071182729676366e-05}, {"id": 281, "seek": 179112, "start": 1797.6399999999999, "end": 1798.6399999999999, "text": " image?", "tokens": [50364, 865, 13, 1119, 264, 31565, 4470, 264, 19261, 12, 3711, 4560, 1296, 264, 2572, 484, 293, 264, 3380, 50690, 50690, 3256, 30, 50740, 50740, 440, 31565, 4470, 11, 321, 1866, 300, 1036, 1243, 293, 321, 362, 732, 3956, 337, 264, 31565, 51042, 51042, 4470, 13, 1485, 390, 264, 17434, 337, 17434, 1412, 293, 321, 362, 264, 17434, 3278, 30867, 13, 400, 264, 51442, 51442, 661, 472, 307, 516, 281, 312, 2602, 264, 957, 2158, 472, 13, 407, 309, 311, 1270, 300, 291, 393, 764, 264, 51726, 51726, 1922, 420, 731, 11, 264, 7395, 34, 11, 558, 30, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.23093657918495708, "compression_ratio": 1.7467248908296944, "no_speech_prob": 4.9071182729676366e-05}, {"id": 282, "seek": 179112, "start": 1798.6399999999999, "end": 1804.6799999999998, "text": " The reconstruction loss, we saw that last week and we have two options for the reconstruction", "tokens": [50364, 865, 13, 1119, 264, 31565, 4470, 264, 19261, 12, 3711, 4560, 1296, 264, 2572, 484, 293, 264, 3380, 50690, 50690, 3256, 30, 50740, 50740, 440, 31565, 4470, 11, 321, 1866, 300, 1036, 1243, 293, 321, 362, 732, 3956, 337, 264, 31565, 51042, 51042, 4470, 13, 1485, 390, 264, 17434, 337, 17434, 1412, 293, 321, 362, 264, 17434, 3278, 30867, 13, 400, 264, 51442, 51442, 661, 472, 307, 516, 281, 312, 2602, 264, 957, 2158, 472, 13, 407, 309, 311, 1270, 300, 291, 393, 764, 264, 51726, 51726, 1922, 420, 731, 11, 264, 7395, 34, 11, 558, 30, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.23093657918495708, "compression_ratio": 1.7467248908296944, "no_speech_prob": 4.9071182729676366e-05}, {"id": 283, "seek": 179112, "start": 1804.6799999999998, "end": 1812.6799999999998, "text": " loss. One was the binary for binary data and we have the binary cross entropy. And the", "tokens": [50364, 865, 13, 1119, 264, 31565, 4470, 264, 19261, 12, 3711, 4560, 1296, 264, 2572, 484, 293, 264, 3380, 50690, 50690, 3256, 30, 50740, 50740, 440, 31565, 4470, 11, 321, 1866, 300, 1036, 1243, 293, 321, 362, 732, 3956, 337, 264, 31565, 51042, 51042, 4470, 13, 1485, 390, 264, 17434, 337, 17434, 1412, 293, 321, 362, 264, 17434, 3278, 30867, 13, 400, 264, 51442, 51442, 661, 472, 307, 516, 281, 312, 2602, 264, 957, 2158, 472, 13, 407, 309, 311, 1270, 300, 291, 393, 764, 264, 51726, 51726, 1922, 420, 731, 11, 264, 7395, 34, 11, 558, 30, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.23093657918495708, "compression_ratio": 1.7467248908296944, "no_speech_prob": 4.9071182729676366e-05}, {"id": 284, "seek": 179112, "start": 1812.6799999999998, "end": 1818.36, "text": " other one is going to be instead the real value one. So it's such that you can use the", "tokens": [50364, 865, 13, 1119, 264, 31565, 4470, 264, 19261, 12, 3711, 4560, 1296, 264, 2572, 484, 293, 264, 3380, 50690, 50690, 3256, 30, 50740, 50740, 440, 31565, 4470, 11, 321, 1866, 300, 1036, 1243, 293, 321, 362, 732, 3956, 337, 264, 31565, 51042, 51042, 4470, 13, 1485, 390, 264, 17434, 337, 17434, 1412, 293, 321, 362, 264, 17434, 3278, 30867, 13, 400, 264, 51442, 51442, 661, 472, 307, 516, 281, 312, 2602, 264, 957, 2158, 472, 13, 407, 309, 311, 1270, 300, 291, 393, 764, 264, 51726, 51726, 1922, 420, 731, 11, 264, 7395, 34, 11, 558, 30, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.23093657918495708, "compression_ratio": 1.7467248908296944, "no_speech_prob": 4.9071182729676366e-05}, {"id": 285, "seek": 179112, "start": 1818.36, "end": 1821.04, "text": " half or well, the MSC, right?", "tokens": [50364, 865, 13, 1119, 264, 31565, 4470, 264, 19261, 12, 3711, 4560, 1296, 264, 2572, 484, 293, 264, 3380, 50690, 50690, 3256, 30, 50740, 50740, 440, 31565, 4470, 11, 321, 1866, 300, 1036, 1243, 293, 321, 362, 732, 3956, 337, 264, 31565, 51042, 51042, 4470, 13, 1485, 390, 264, 17434, 337, 17434, 1412, 293, 321, 362, 264, 17434, 3278, 30867, 13, 400, 264, 51442, 51442, 661, 472, 307, 516, 281, 312, 2602, 264, 957, 2158, 472, 13, 407, 309, 311, 1270, 300, 291, 393, 764, 264, 51726, 51726, 1922, 420, 731, 11, 264, 7395, 34, 11, 558, 30, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.23093657918495708, "compression_ratio": 1.7467248908296944, "no_speech_prob": 4.9071182729676366e-05}, {"id": 286, "seek": 182104, "start": 1821.04, "end": 1827.8, "text": " So these are the reconstruction losses we can use. For example, you talk more with me", "tokens": [50364, 407, 613, 366, 264, 31565, 15352, 321, 393, 764, 13, 1171, 1365, 11, 291, 751, 544, 365, 385, 50702, 50702, 813, 365, 4956, 13, 2205, 13, 883, 11, 731, 11, 406, 665, 13, 509, 820, 751, 382, 731, 365, 4956, 11, 457, 321, 820, 51074, 51074, 312, 516, 670, 264, 21060, 1270, 300, 321, 393, 536, 577, 281, 3089, 264, 1507, 293, 611, 862, 365, 51394, 51394, 264, 37870, 13, 1436, 949, 11, 797, 11, 264, 2135, 935, 390, 300, 949, 321, 645, 18350, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.15744286233728583, "compression_ratio": 1.592760180995475, "no_speech_prob": 7.810169336153194e-05}, {"id": 287, "seek": 182104, "start": 1827.8, "end": 1835.24, "text": " than with Jan. Good. No, well, not good. You should talk as well with Jan, but we should", "tokens": [50364, 407, 613, 366, 264, 31565, 15352, 321, 393, 764, 13, 1171, 1365, 11, 291, 751, 544, 365, 385, 50702, 50702, 813, 365, 4956, 13, 2205, 13, 883, 11, 731, 11, 406, 665, 13, 509, 820, 751, 382, 731, 365, 4956, 11, 457, 321, 820, 51074, 51074, 312, 516, 670, 264, 21060, 1270, 300, 321, 393, 536, 577, 281, 3089, 264, 1507, 293, 611, 862, 365, 51394, 51394, 264, 37870, 13, 1436, 949, 11, 797, 11, 264, 2135, 935, 390, 300, 949, 321, 645, 18350, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.15744286233728583, "compression_ratio": 1.592760180995475, "no_speech_prob": 7.810169336153194e-05}, {"id": 288, "seek": 182104, "start": 1835.24, "end": 1841.6399999999999, "text": " be going over the notebook such that we can see how to code the stuff and also play with", "tokens": [50364, 407, 613, 366, 264, 31565, 15352, 321, 393, 764, 13, 1171, 1365, 11, 291, 751, 544, 365, 385, 50702, 50702, 813, 365, 4956, 13, 2205, 13, 883, 11, 731, 11, 406, 665, 13, 509, 820, 751, 382, 731, 365, 4956, 11, 457, 321, 820, 51074, 51074, 312, 516, 670, 264, 21060, 1270, 300, 321, 393, 536, 577, 281, 3089, 264, 1507, 293, 611, 862, 365, 51394, 51394, 264, 37870, 13, 1436, 949, 11, 797, 11, 264, 2135, 935, 390, 300, 949, 321, 645, 18350, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.15744286233728583, "compression_ratio": 1.592760180995475, "no_speech_prob": 7.810169336153194e-05}, {"id": 289, "seek": 182104, "start": 1841.6399999999999, "end": 1847.28, "text": " the distributions. Because before, again, the main point was that before we were mapping", "tokens": [50364, 407, 613, 366, 264, 31565, 15352, 321, 393, 764, 13, 1171, 1365, 11, 291, 751, 544, 365, 385, 50702, 50702, 813, 365, 4956, 13, 2205, 13, 883, 11, 731, 11, 406, 665, 13, 509, 820, 751, 382, 731, 365, 4956, 11, 457, 321, 820, 51074, 51074, 312, 516, 670, 264, 21060, 1270, 300, 321, 393, 536, 577, 281, 3089, 264, 1507, 293, 611, 862, 365, 51394, 51394, 264, 37870, 13, 1436, 949, 11, 797, 11, 264, 2135, 935, 390, 300, 949, 321, 645, 18350, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.15744286233728583, "compression_ratio": 1.592760180995475, "no_speech_prob": 7.810169336153194e-05}, {"id": 290, "seek": 184728, "start": 1847.28, "end": 1852.28, "text": " points to points and back to points, right? Right now instead you're going to map points", "tokens": [50364, 2793, 281, 2793, 293, 646, 281, 2793, 11, 558, 30, 1779, 586, 2602, 291, 434, 516, 281, 4471, 2793, 50614, 50614, 281, 1901, 293, 550, 1901, 281, 2793, 13, 583, 550, 611, 439, 264, 1901, 586, 307, 516, 281, 312, 439, 50972, 50972, 5343, 538, 613, 16295, 570, 295, 2940, 6771, 11, 558, 30, 759, 291, 362, 512, 1901, 1296, 51300, 51300, 613, 16295, 11, 550, 291, 362, 572, 1558, 577, 281, 352, 490, 341, 4458, 510, 646, 281, 264, 4846, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.13785402634564567, "compression_ratio": 1.7777777777777777, "no_speech_prob": 3.8800575566710904e-05}, {"id": 291, "seek": 184728, "start": 1852.28, "end": 1859.44, "text": " to space and then space to points. But then also all the space now is going to be all", "tokens": [50364, 2793, 281, 2793, 293, 646, 281, 2793, 11, 558, 30, 1779, 586, 2602, 291, 434, 516, 281, 4471, 2793, 50614, 50614, 281, 1901, 293, 550, 1901, 281, 2793, 13, 583, 550, 611, 439, 264, 1901, 586, 307, 516, 281, 312, 439, 50972, 50972, 5343, 538, 613, 16295, 570, 295, 2940, 6771, 11, 558, 30, 759, 291, 362, 512, 1901, 1296, 51300, 51300, 613, 16295, 11, 550, 291, 362, 572, 1558, 577, 281, 352, 490, 341, 4458, 510, 646, 281, 264, 4846, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.13785402634564567, "compression_ratio": 1.7777777777777777, "no_speech_prob": 3.8800575566710904e-05}, {"id": 292, "seek": 184728, "start": 1859.44, "end": 1866.0, "text": " covered by these bubbles because of several factors, right? If you have some space between", "tokens": [50364, 2793, 281, 2793, 293, 646, 281, 2793, 11, 558, 30, 1779, 586, 2602, 291, 434, 516, 281, 4471, 2793, 50614, 50614, 281, 1901, 293, 550, 1901, 281, 2793, 13, 583, 550, 611, 439, 264, 1901, 586, 307, 516, 281, 312, 439, 50972, 50972, 5343, 538, 613, 16295, 570, 295, 2940, 6771, 11, 558, 30, 759, 291, 362, 512, 1901, 1296, 51300, 51300, 613, 16295, 11, 550, 291, 362, 572, 1558, 577, 281, 352, 490, 341, 4458, 510, 646, 281, 264, 4846, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.13785402634564567, "compression_ratio": 1.7777777777777777, "no_speech_prob": 3.8800575566710904e-05}, {"id": 293, "seek": 184728, "start": 1866.0, "end": 1871.52, "text": " these bubbles, then you have no idea how to go from this region here back to the input", "tokens": [50364, 2793, 281, 2793, 293, 646, 281, 2793, 11, 558, 30, 1779, 586, 2602, 291, 434, 516, 281, 4471, 2793, 50614, 50614, 281, 1901, 293, 550, 1901, 281, 2793, 13, 583, 550, 611, 439, 264, 1901, 586, 307, 516, 281, 312, 439, 50972, 50972, 5343, 538, 613, 16295, 570, 295, 2940, 6771, 11, 558, 30, 759, 291, 362, 512, 1901, 1296, 51300, 51300, 613, 16295, 11, 550, 291, 362, 572, 1558, 577, 281, 352, 490, 341, 4458, 510, 646, 281, 264, 4846, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.13785402634564567, "compression_ratio": 1.7777777777777777, "no_speech_prob": 3.8800575566710904e-05}, {"id": 294, "seek": 187152, "start": 1871.52, "end": 1877.4, "text": " space, right? Instead of variation of the encoder gets you to this very well behaved", "tokens": [50364, 1901, 11, 558, 30, 7156, 295, 12990, 295, 264, 2058, 19866, 2170, 291, 281, 341, 588, 731, 48249, 50658, 50658, 9645, 11, 291, 458, 11, 341, 1481, 9645, 295, 264, 48994, 1901, 13, 1033, 13, 2205, 13, 286, 393, 380, 536, 51002, 51002, 291, 13, 286, 1713, 291, 1074, 13, 1033, 13, 407, 3089, 420, 366, 456, 1651, 370, 1400, 30, 286, 1454, 291, 393, 536, 51506, 51506, 1507, 13, 1449, 976, 5824, 13, 1664, 291, 536, 1507, 30, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.2702460691153285, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.236373802588787e-05}, {"id": 295, "seek": 187152, "start": 1877.4, "end": 1884.28, "text": " coverage, you know, this nice coverage of the latent space. Okay. Good. I can't see", "tokens": [50364, 1901, 11, 558, 30, 7156, 295, 12990, 295, 264, 2058, 19866, 2170, 291, 281, 341, 588, 731, 48249, 50658, 50658, 9645, 11, 291, 458, 11, 341, 1481, 9645, 295, 264, 48994, 1901, 13, 1033, 13, 2205, 13, 286, 393, 380, 536, 51002, 51002, 291, 13, 286, 1713, 291, 1074, 13, 1033, 13, 407, 3089, 420, 366, 456, 1651, 370, 1400, 30, 286, 1454, 291, 393, 536, 51506, 51506, 1507, 13, 1449, 976, 5824, 13, 1664, 291, 536, 1507, 30, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.2702460691153285, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.236373802588787e-05}, {"id": 296, "seek": 187152, "start": 1884.28, "end": 1894.36, "text": " you. I miss you guys. Okay. So code or are there questions so far? I hope you can see", "tokens": [50364, 1901, 11, 558, 30, 7156, 295, 12990, 295, 264, 2058, 19866, 2170, 291, 281, 341, 588, 731, 48249, 50658, 50658, 9645, 11, 291, 458, 11, 341, 1481, 9645, 295, 264, 48994, 1901, 13, 1033, 13, 2205, 13, 286, 393, 380, 536, 51002, 51002, 291, 13, 286, 1713, 291, 1074, 13, 1033, 13, 407, 3089, 420, 366, 456, 1651, 370, 1400, 30, 286, 1454, 291, 393, 536, 51506, 51506, 1507, 13, 1449, 976, 5824, 13, 1664, 291, 536, 1507, 30, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.2702460691153285, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.236373802588787e-05}, {"id": 297, "seek": 187152, "start": 1894.36, "end": 1899.08, "text": " stuff. Just give feedback. Can you see stuff?", "tokens": [50364, 1901, 11, 558, 30, 7156, 295, 12990, 295, 264, 2058, 19866, 2170, 291, 281, 341, 588, 731, 48249, 50658, 50658, 9645, 11, 291, 458, 11, 341, 1481, 9645, 295, 264, 48994, 1901, 13, 1033, 13, 2205, 13, 286, 393, 380, 536, 51002, 51002, 291, 13, 286, 1713, 291, 1074, 13, 1033, 13, 407, 3089, 420, 366, 456, 1651, 370, 1400, 30, 286, 1454, 291, 393, 536, 51506, 51506, 1507, 13, 1449, 976, 5824, 13, 1664, 291, 536, 1507, 30, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.2702460691153285, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.236373802588787e-05}, {"id": 298, "seek": 189908, "start": 1899.08, "end": 1919.6799999999998, "text": " Yep. Yes. Yep. All right. So work, get the PDL, conda, activate PDL, Jupyter notebook.", "tokens": [50364, 7010, 13, 1079, 13, 7010, 13, 1057, 558, 13, 407, 589, 11, 483, 264, 10464, 43, 11, 2224, 64, 11, 13615, 10464, 43, 11, 22125, 88, 391, 21060, 13, 51394, 51394, 15523, 13, 1033, 13, 407, 286, 478, 516, 281, 312, 10322, 586, 264, 18527, 36, 13, 400, 370, 586, 286, 478, 516, 281, 445, 14483, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.3116941769917806, "compression_ratio": 1.2661870503597121, "no_speech_prob": 5.1336603064555675e-05}, {"id": 299, "seek": 189908, "start": 1919.6799999999998, "end": 1928.04, "text": " Boom. Okay. So I'm going to be covering now the VAE. And so now I'm going to just execute", "tokens": [50364, 7010, 13, 1079, 13, 7010, 13, 1057, 558, 13, 407, 589, 11, 483, 264, 10464, 43, 11, 2224, 64, 11, 13615, 10464, 43, 11, 22125, 88, 391, 21060, 13, 51394, 51394, 15523, 13, 1033, 13, 407, 286, 478, 516, 281, 312, 10322, 586, 264, 18527, 36, 13, 400, 370, 586, 286, 478, 516, 281, 445, 14483, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.3116941769917806, "compression_ratio": 1.2661870503597121, "no_speech_prob": 5.1336603064555675e-05}, {"id": 300, "seek": 192804, "start": 1928.04, "end": 1933.0, "text": " everything such that this stuff starts training and then I'm going to be explaining things.", "tokens": [50364, 1203, 1270, 300, 341, 1507, 3719, 3097, 293, 550, 286, 478, 516, 281, 312, 13468, 721, 13, 50612, 50612, 1057, 558, 13, 407, 412, 264, 2863, 11, 286, 478, 516, 281, 312, 445, 974, 294, 527, 4974, 8193, 382, 7713, 13, 50906, 50906, 1396, 286, 362, 257, 4674, 9927, 13, 492, 500, 380, 1127, 13, 1468, 380, 909, 309, 281, 264, 5570, 13, 286, 362, 512, 7576, 51238, 51238, 4190, 337, 264, 4974, 15421, 1270, 300, 291, 434, 516, 281, 483, 264, 912, 3547, 286, 483, 13, 1396, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.1922323807426121, "compression_ratio": 1.6605504587155964, "no_speech_prob": 7.47913945815526e-05}, {"id": 301, "seek": 192804, "start": 1933.0, "end": 1938.8799999999999, "text": " All right. So at the beginning, I'm going to be just import in our random sheet as usual.", "tokens": [50364, 1203, 1270, 300, 341, 1507, 3719, 3097, 293, 550, 286, 478, 516, 281, 312, 13468, 721, 13, 50612, 50612, 1057, 558, 13, 407, 412, 264, 2863, 11, 286, 478, 516, 281, 312, 445, 974, 294, 527, 4974, 8193, 382, 7713, 13, 50906, 50906, 1396, 286, 362, 257, 4674, 9927, 13, 492, 500, 380, 1127, 13, 1468, 380, 909, 309, 281, 264, 5570, 13, 286, 362, 512, 7576, 51238, 51238, 4190, 337, 264, 4974, 15421, 1270, 300, 291, 434, 516, 281, 483, 264, 912, 3547, 286, 483, 13, 1396, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.1922323807426121, "compression_ratio": 1.6605504587155964, "no_speech_prob": 7.47913945815526e-05}, {"id": 302, "seek": 192804, "start": 1938.8799999999999, "end": 1945.52, "text": " Then I have a display routine. We don't care. Don't add it to the notes. I have some default", "tokens": [50364, 1203, 1270, 300, 341, 1507, 3719, 3097, 293, 550, 286, 478, 516, 281, 312, 13468, 721, 13, 50612, 50612, 1057, 558, 13, 407, 412, 264, 2863, 11, 286, 478, 516, 281, 312, 445, 974, 294, 527, 4974, 8193, 382, 7713, 13, 50906, 50906, 1396, 286, 362, 257, 4674, 9927, 13, 492, 500, 380, 1127, 13, 1468, 380, 909, 309, 281, 264, 5570, 13, 286, 362, 512, 7576, 51238, 51238, 4190, 337, 264, 4974, 15421, 1270, 300, 291, 434, 516, 281, 483, 264, 912, 3547, 286, 483, 13, 1396, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.1922323807426121, "compression_ratio": 1.6605504587155964, "no_speech_prob": 7.47913945815526e-05}, {"id": 303, "seek": 192804, "start": 1945.52, "end": 1951.36, "text": " values for the random sheets such that you're going to get the same numbers I get. Then", "tokens": [50364, 1203, 1270, 300, 341, 1507, 3719, 3097, 293, 550, 286, 478, 516, 281, 312, 13468, 721, 13, 50612, 50612, 1057, 558, 13, 407, 412, 264, 2863, 11, 286, 478, 516, 281, 312, 445, 974, 294, 527, 4974, 8193, 382, 7713, 13, 50906, 50906, 1396, 286, 362, 257, 4674, 9927, 13, 492, 500, 380, 1127, 13, 1468, 380, 909, 309, 281, 264, 5570, 13, 286, 362, 512, 7576, 51238, 51238, 4190, 337, 264, 4974, 15421, 1270, 300, 291, 434, 516, 281, 483, 264, 912, 3547, 286, 483, 13, 1396, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.1922323807426121, "compression_ratio": 1.6605504587155964, "no_speech_prob": 7.47913945815526e-05}, {"id": 304, "seek": 195136, "start": 1951.36, "end": 1959.6399999999999, "text": " here I just use the MNIST data set, the modify NIST from YARN device. I set the CPU or GPU.", "tokens": [50364, 510, 286, 445, 764, 264, 376, 45, 19756, 1412, 992, 11, 264, 16927, 426, 19756, 490, 398, 1899, 45, 4302, 13, 286, 992, 264, 13199, 420, 18407, 13, 50778, 50778, 682, 5261, 11, 286, 727, 362, 1143, 18407, 570, 452, 5707, 510, 767, 575, 257, 18407, 13, 400, 550, 286, 362, 51044, 51044, 452, 12990, 484, 2058, 19866, 13, 1033, 13, 407, 452, 12990, 484, 2058, 19866, 575, 732, 3166, 11, 575, 364, 2058, 19866, 51336, 51336], "temperature": 0.0, "avg_logprob": -0.2099478274961061, "compression_ratio": 1.5340909090909092, "no_speech_prob": 4.784549673786387e-06}, {"id": 305, "seek": 195136, "start": 1959.6399999999999, "end": 1964.9599999999998, "text": " In theory, I could have used GPU because my Mac here actually has a GPU. And then I have", "tokens": [50364, 510, 286, 445, 764, 264, 376, 45, 19756, 1412, 992, 11, 264, 16927, 426, 19756, 490, 398, 1899, 45, 4302, 13, 286, 992, 264, 13199, 420, 18407, 13, 50778, 50778, 682, 5261, 11, 286, 727, 362, 1143, 18407, 570, 452, 5707, 510, 767, 575, 257, 18407, 13, 400, 550, 286, 362, 51044, 51044, 452, 12990, 484, 2058, 19866, 13, 1033, 13, 407, 452, 12990, 484, 2058, 19866, 575, 732, 3166, 11, 575, 364, 2058, 19866, 51336, 51336], "temperature": 0.0, "avg_logprob": -0.2099478274961061, "compression_ratio": 1.5340909090909092, "no_speech_prob": 4.784549673786387e-06}, {"id": 306, "seek": 195136, "start": 1964.9599999999998, "end": 1970.8, "text": " my variation out encoder. Okay. So my variation out encoder has two parts, has an encoder", "tokens": [50364, 510, 286, 445, 764, 264, 376, 45, 19756, 1412, 992, 11, 264, 16927, 426, 19756, 490, 398, 1899, 45, 4302, 13, 286, 992, 264, 13199, 420, 18407, 13, 50778, 50778, 682, 5261, 11, 286, 727, 362, 1143, 18407, 570, 452, 5707, 510, 767, 575, 257, 18407, 13, 400, 550, 286, 362, 51044, 51044, 452, 12990, 484, 2058, 19866, 13, 1033, 13, 407, 452, 12990, 484, 2058, 19866, 575, 732, 3166, 11, 575, 364, 2058, 19866, 51336, 51336], "temperature": 0.0, "avg_logprob": -0.2099478274961061, "compression_ratio": 1.5340909090909092, "no_speech_prob": 4.784549673786387e-06}, {"id": 307, "seek": 197080, "start": 1970.8, "end": 1983.08, "text": " here. Let me turn on the line numbers. So my encoder goes from 784, which is the size", "tokens": [50364, 510, 13, 961, 385, 1261, 322, 264, 1622, 3547, 13, 407, 452, 2058, 19866, 1709, 490, 1614, 25494, 11, 597, 307, 264, 2744, 50978, 50978, 295, 264, 4846, 281, 413, 3732, 11, 337, 1365, 13, 400, 413, 294, 341, 1389, 307, 945, 11, 370, 8423, 13, 400, 550, 490, 51424, 51424, 413, 3732, 11, 286, 352, 281, 732, 1413, 314, 11, 597, 307, 516, 281, 312, 1922, 295, 452, 1355, 293, 1922, 307, 516, 281, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.12039185777495179, "compression_ratio": 1.4719101123595506, "no_speech_prob": 1.4508813364955131e-05}, {"id": 308, "seek": 197080, "start": 1983.08, "end": 1992.0, "text": " of the input to D square, for example. And D in this case is 20, so 400. And then from", "tokens": [50364, 510, 13, 961, 385, 1261, 322, 264, 1622, 3547, 13, 407, 452, 2058, 19866, 1709, 490, 1614, 25494, 11, 597, 307, 264, 2744, 50978, 50978, 295, 264, 4846, 281, 413, 3732, 11, 337, 1365, 13, 400, 413, 294, 341, 1389, 307, 945, 11, 370, 8423, 13, 400, 550, 490, 51424, 51424, 413, 3732, 11, 286, 352, 281, 732, 1413, 314, 11, 597, 307, 516, 281, 312, 1922, 295, 452, 1355, 293, 1922, 307, 516, 281, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.12039185777495179, "compression_ratio": 1.4719101123595506, "no_speech_prob": 1.4508813364955131e-05}, {"id": 309, "seek": 197080, "start": 1992.0, "end": 1998.32, "text": " D square, I go to two times T, which is going to be half of my means and half is going to", "tokens": [50364, 510, 13, 961, 385, 1261, 322, 264, 1622, 3547, 13, 407, 452, 2058, 19866, 1709, 490, 1614, 25494, 11, 597, 307, 264, 2744, 50978, 50978, 295, 264, 4846, 281, 413, 3732, 11, 337, 1365, 13, 400, 413, 294, 341, 1389, 307, 945, 11, 370, 8423, 13, 400, 550, 490, 51424, 51424, 413, 3732, 11, 286, 352, 281, 732, 1413, 314, 11, 597, 307, 516, 281, 312, 1922, 295, 452, 1355, 293, 1922, 307, 516, 281, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.12039185777495179, "compression_ratio": 1.4719101123595506, "no_speech_prob": 1.4508813364955131e-05}, {"id": 310, "seek": 199832, "start": 1998.32, "end": 2005.96, "text": " be for my sigma squares for my variances. The other case, the decoder instead picks", "tokens": [50364, 312, 337, 452, 12771, 19368, 337, 452, 1374, 21518, 13, 440, 661, 1389, 11, 264, 979, 19866, 2602, 16137, 50746, 50746, 787, 413, 11, 558, 30, 509, 393, 536, 413, 510, 13, 492, 352, 490, 413, 281, 413, 3732, 293, 550, 490, 413, 3732, 281, 1614, 25494, 51084, 51084, 1270, 300, 321, 2995, 264, 4846, 10139, 1860, 13, 400, 550, 2721, 11, 286, 362, 257, 4556, 3280, 327, 13, 1545, 360, 51292, 51292, 286, 362, 257, 4556, 3280, 327, 30, 1436, 452, 4846, 307, 516, 281, 312, 5567, 490, 4018, 281, 472, 13, 821, 366, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1658925913801097, "compression_ratio": 1.563063063063063, "no_speech_prob": 4.488198101171292e-06}, {"id": 311, "seek": 199832, "start": 2005.96, "end": 2012.72, "text": " only D, right? You can see D here. We go from D to D square and then from D square to 784", "tokens": [50364, 312, 337, 452, 12771, 19368, 337, 452, 1374, 21518, 13, 440, 661, 1389, 11, 264, 979, 19866, 2602, 16137, 50746, 50746, 787, 413, 11, 558, 30, 509, 393, 536, 413, 510, 13, 492, 352, 490, 413, 281, 413, 3732, 293, 550, 490, 413, 3732, 281, 1614, 25494, 51084, 51084, 1270, 300, 321, 2995, 264, 4846, 10139, 1860, 13, 400, 550, 2721, 11, 286, 362, 257, 4556, 3280, 327, 13, 1545, 360, 51292, 51292, 286, 362, 257, 4556, 3280, 327, 30, 1436, 452, 4846, 307, 516, 281, 312, 5567, 490, 4018, 281, 472, 13, 821, 366, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1658925913801097, "compression_ratio": 1.563063063063063, "no_speech_prob": 4.488198101171292e-06}, {"id": 312, "seek": 199832, "start": 2012.72, "end": 2016.8799999999999, "text": " such that we match the input dimensionality. And then finally, I have a sigmoid. Why do", "tokens": [50364, 312, 337, 452, 12771, 19368, 337, 452, 1374, 21518, 13, 440, 661, 1389, 11, 264, 979, 19866, 2602, 16137, 50746, 50746, 787, 413, 11, 558, 30, 509, 393, 536, 413, 510, 13, 492, 352, 490, 413, 281, 413, 3732, 293, 550, 490, 413, 3732, 281, 1614, 25494, 51084, 51084, 1270, 300, 321, 2995, 264, 4846, 10139, 1860, 13, 400, 550, 2721, 11, 286, 362, 257, 4556, 3280, 327, 13, 1545, 360, 51292, 51292, 286, 362, 257, 4556, 3280, 327, 30, 1436, 452, 4846, 307, 516, 281, 312, 5567, 490, 4018, 281, 472, 13, 821, 366, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1658925913801097, "compression_ratio": 1.563063063063063, "no_speech_prob": 4.488198101171292e-06}, {"id": 313, "seek": 199832, "start": 2016.8799999999999, "end": 2023.32, "text": " I have a sigmoid? Because my input is going to be limited from zero to one. There are", "tokens": [50364, 312, 337, 452, 12771, 19368, 337, 452, 1374, 21518, 13, 440, 661, 1389, 11, 264, 979, 19866, 2602, 16137, 50746, 50746, 787, 413, 11, 558, 30, 509, 393, 536, 413, 510, 13, 492, 352, 490, 413, 281, 413, 3732, 293, 550, 490, 413, 3732, 281, 1614, 25494, 51084, 51084, 1270, 300, 321, 2995, 264, 4846, 10139, 1860, 13, 400, 550, 2721, 11, 286, 362, 257, 4556, 3280, 327, 13, 1545, 360, 51292, 51292, 286, 362, 257, 4556, 3280, 327, 30, 1436, 452, 4846, 307, 516, 281, 312, 5567, 490, 4018, 281, 472, 13, 821, 366, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1658925913801097, "compression_ratio": 1.563063063063063, "no_speech_prob": 4.488198101171292e-06}, {"id": 314, "seek": 202332, "start": 2023.32, "end": 2030.4399999999998, "text": " images from zero to one. Then there is a module here, which is called reparameterize. And", "tokens": [50364, 5267, 490, 4018, 281, 472, 13, 1396, 456, 307, 257, 10088, 510, 11, 597, 307, 1219, 1085, 12835, 2398, 1125, 13, 400, 50720, 50720, 498, 321, 366, 3097, 11, 321, 764, 613, 1085, 12835, 2398, 2144, 644, 13, 50928, 50928, 4919, 11, 727, 291, 445, 584, 797, 983, 291, 764, 264, 4556, 3280, 327, 294, 264, 979, 19866, 30, 51164, 51164, 865, 11, 570, 452, 1412, 307, 2647, 1296, 4018, 293, 472, 13, 407, 286, 362, 729, 27011, 490, 264, 376, 45, 19756, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.19565477042362608, "compression_ratio": 1.502415458937198, "no_speech_prob": 2.5833360268734396e-05}, {"id": 315, "seek": 202332, "start": 2030.4399999999998, "end": 2034.6, "text": " if we are training, we use these reparameterization part.", "tokens": [50364, 5267, 490, 4018, 281, 472, 13, 1396, 456, 307, 257, 10088, 510, 11, 597, 307, 1219, 1085, 12835, 2398, 1125, 13, 400, 50720, 50720, 498, 321, 366, 3097, 11, 321, 764, 613, 1085, 12835, 2398, 2144, 644, 13, 50928, 50928, 4919, 11, 727, 291, 445, 584, 797, 983, 291, 764, 264, 4556, 3280, 327, 294, 264, 979, 19866, 30, 51164, 51164, 865, 11, 570, 452, 1412, 307, 2647, 1296, 4018, 293, 472, 13, 407, 286, 362, 729, 27011, 490, 264, 376, 45, 19756, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.19565477042362608, "compression_ratio": 1.502415458937198, "no_speech_prob": 2.5833360268734396e-05}, {"id": 316, "seek": 202332, "start": 2034.6, "end": 2039.32, "text": " Sorry, could you just say again why you use the sigmoid in the decoder?", "tokens": [50364, 5267, 490, 4018, 281, 472, 13, 1396, 456, 307, 257, 10088, 510, 11, 597, 307, 1219, 1085, 12835, 2398, 1125, 13, 400, 50720, 50720, 498, 321, 366, 3097, 11, 321, 764, 613, 1085, 12835, 2398, 2144, 644, 13, 50928, 50928, 4919, 11, 727, 291, 445, 584, 797, 983, 291, 764, 264, 4556, 3280, 327, 294, 264, 979, 19866, 30, 51164, 51164, 865, 11, 570, 452, 1412, 307, 2647, 1296, 4018, 293, 472, 13, 407, 286, 362, 729, 27011, 490, 264, 376, 45, 19756, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.19565477042362608, "compression_ratio": 1.502415458937198, "no_speech_prob": 2.5833360268734396e-05}, {"id": 317, "seek": 202332, "start": 2039.32, "end": 2046.76, "text": " Yeah, because my data is living between zero and one. So I have those digits from the MNIST", "tokens": [50364, 5267, 490, 4018, 281, 472, 13, 1396, 456, 307, 257, 10088, 510, 11, 597, 307, 1219, 1085, 12835, 2398, 1125, 13, 400, 50720, 50720, 498, 321, 366, 3097, 11, 321, 764, 613, 1085, 12835, 2398, 2144, 644, 13, 50928, 50928, 4919, 11, 727, 291, 445, 584, 797, 983, 291, 764, 264, 4556, 3280, 327, 294, 264, 979, 19866, 30, 51164, 51164, 865, 11, 570, 452, 1412, 307, 2647, 1296, 4018, 293, 472, 13, 407, 286, 362, 729, 27011, 490, 264, 376, 45, 19756, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.19565477042362608, "compression_ratio": 1.502415458937198, "no_speech_prob": 2.5833360268734396e-05}, {"id": 318, "seek": 204676, "start": 2046.76, "end": 2054.04, "text": " and they are values like the values of the digits are going to be from zero to one. So", "tokens": [50364, 293, 436, 366, 4190, 411, 264, 4190, 295, 264, 27011, 366, 516, 281, 312, 490, 4018, 281, 472, 13, 407, 50728, 50728, 286, 411, 281, 362, 452, 3209, 11, 341, 10088, 510, 23930, 721, 300, 1709, 490, 3175, 13202, 51022, 51022, 281, 1804, 13202, 13, 759, 286, 2845, 309, 807, 257, 4556, 3280, 327, 11, 341, 1507, 14790, 721, 807, 411, 4018, 51314, 51314, 281, 472, 13, 51482, 51482, 1133, 291, 584, 264, 4190, 295, 264, 27011, 11, 291, 914, 264, 45428, 763, 11, 558, 30, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.12492341995239258, "compression_ratio": 1.745, "no_speech_prob": 1.3004947504668962e-05}, {"id": 319, "seek": 204676, "start": 2054.04, "end": 2059.92, "text": " I like to have my network, this module here outputs things that goes from minus infinity", "tokens": [50364, 293, 436, 366, 4190, 411, 264, 4190, 295, 264, 27011, 366, 516, 281, 312, 490, 4018, 281, 472, 13, 407, 50728, 50728, 286, 411, 281, 362, 452, 3209, 11, 341, 10088, 510, 23930, 721, 300, 1709, 490, 3175, 13202, 51022, 51022, 281, 1804, 13202, 13, 759, 286, 2845, 309, 807, 257, 4556, 3280, 327, 11, 341, 1507, 14790, 721, 807, 411, 4018, 51314, 51314, 281, 472, 13, 51482, 51482, 1133, 291, 584, 264, 4190, 295, 264, 27011, 11, 291, 914, 264, 45428, 763, 11, 558, 30, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.12492341995239258, "compression_ratio": 1.745, "no_speech_prob": 1.3004947504668962e-05}, {"id": 320, "seek": 204676, "start": 2059.92, "end": 2065.76, "text": " to plus infinity. If I send it through a sigmoid, this stuff sends things through like zero", "tokens": [50364, 293, 436, 366, 4190, 411, 264, 4190, 295, 264, 27011, 366, 516, 281, 312, 490, 4018, 281, 472, 13, 407, 50728, 50728, 286, 411, 281, 362, 452, 3209, 11, 341, 10088, 510, 23930, 721, 300, 1709, 490, 3175, 13202, 51022, 51022, 281, 1804, 13202, 13, 759, 286, 2845, 309, 807, 257, 4556, 3280, 327, 11, 341, 1507, 14790, 721, 807, 411, 4018, 51314, 51314, 281, 472, 13, 51482, 51482, 1133, 291, 584, 264, 4190, 295, 264, 27011, 11, 291, 914, 264, 45428, 763, 11, 558, 30, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.12492341995239258, "compression_ratio": 1.745, "no_speech_prob": 1.3004947504668962e-05}, {"id": 321, "seek": 204676, "start": 2065.76, "end": 2069.12, "text": " to one.", "tokens": [50364, 293, 436, 366, 4190, 411, 264, 4190, 295, 264, 27011, 366, 516, 281, 312, 490, 4018, 281, 472, 13, 407, 50728, 50728, 286, 411, 281, 362, 452, 3209, 11, 341, 10088, 510, 23930, 721, 300, 1709, 490, 3175, 13202, 51022, 51022, 281, 1804, 13202, 13, 759, 286, 2845, 309, 807, 257, 4556, 3280, 327, 11, 341, 1507, 14790, 721, 807, 411, 4018, 51314, 51314, 281, 472, 13, 51482, 51482, 1133, 291, 584, 264, 4190, 295, 264, 27011, 11, 291, 914, 264, 45428, 763, 11, 558, 30, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.12492341995239258, "compression_ratio": 1.745, "no_speech_prob": 1.3004947504668962e-05}, {"id": 322, "seek": 204676, "start": 2069.12, "end": 2074.04, "text": " When you say the values of the digits, you mean the deactivations, right?", "tokens": [50364, 293, 436, 366, 4190, 411, 264, 4190, 295, 264, 27011, 366, 516, 281, 312, 490, 4018, 281, 472, 13, 407, 50728, 50728, 286, 411, 281, 362, 452, 3209, 11, 341, 10088, 510, 23930, 721, 300, 1709, 490, 3175, 13202, 51022, 51022, 281, 1804, 13202, 13, 759, 286, 2845, 309, 807, 257, 4556, 3280, 327, 11, 341, 1507, 14790, 721, 807, 411, 4018, 51314, 51314, 281, 472, 13, 51482, 51482, 1133, 291, 584, 264, 4190, 295, 264, 27011, 11, 291, 914, 264, 45428, 763, 11, 558, 30, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.12492341995239258, "compression_ratio": 1.745, "no_speech_prob": 1.3004947504668962e-05}, {"id": 323, "seek": 207404, "start": 2074.04, "end": 2080.96, "text": " So I use the MNIST dataset and this is going to be both my input and also my targets, right?", "tokens": [50364, 407, 286, 764, 264, 376, 45, 19756, 28872, 293, 341, 307, 516, 281, 312, 1293, 452, 4846, 293, 611, 452, 12911, 11, 558, 30, 50710, 50710, 440, 5267, 293, 264, 4190, 295, 613, 5267, 486, 312, 25532, 1296, 4018, 281, 472, 13, 1743, 51022, 51022, 309, 311, 257, 957, 2158, 13, 6947, 19261, 393, 312, 1296, 4018, 293, 472, 13, 51268, 51268, 865, 13, 51318, 51318, 286, 519, 767, 264, 15743, 366, 17434, 13, 407, 264, 15743, 366, 439, 4018, 420, 472, 13, 583, 452, 3209, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.17246251636081272, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.748545193753671e-05}, {"id": 324, "seek": 207404, "start": 2080.96, "end": 2087.2, "text": " The images and the values of these images will be ranging between zero to one. Like", "tokens": [50364, 407, 286, 764, 264, 376, 45, 19756, 28872, 293, 341, 307, 516, 281, 312, 1293, 452, 4846, 293, 611, 452, 12911, 11, 558, 30, 50710, 50710, 440, 5267, 293, 264, 4190, 295, 613, 5267, 486, 312, 25532, 1296, 4018, 281, 472, 13, 1743, 51022, 51022, 309, 311, 257, 957, 2158, 13, 6947, 19261, 393, 312, 1296, 4018, 293, 472, 13, 51268, 51268, 865, 13, 51318, 51318, 286, 519, 767, 264, 15743, 366, 17434, 13, 407, 264, 15743, 366, 439, 4018, 420, 472, 13, 583, 452, 3209, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.17246251636081272, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.748545193753671e-05}, {"id": 325, "seek": 207404, "start": 2087.2, "end": 2092.12, "text": " it's a real value. Each pixel can be between zero and one.", "tokens": [50364, 407, 286, 764, 264, 376, 45, 19756, 28872, 293, 341, 307, 516, 281, 312, 1293, 452, 4846, 293, 611, 452, 12911, 11, 558, 30, 50710, 50710, 440, 5267, 293, 264, 4190, 295, 613, 5267, 486, 312, 25532, 1296, 4018, 281, 472, 13, 1743, 51022, 51022, 309, 311, 257, 957, 2158, 13, 6947, 19261, 393, 312, 1296, 4018, 293, 472, 13, 51268, 51268, 865, 13, 51318, 51318, 286, 519, 767, 264, 15743, 366, 17434, 13, 407, 264, 15743, 366, 439, 4018, 420, 472, 13, 583, 452, 3209, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.17246251636081272, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.748545193753671e-05}, {"id": 326, "seek": 207404, "start": 2092.12, "end": 2093.12, "text": " Yeah.", "tokens": [50364, 407, 286, 764, 264, 376, 45, 19756, 28872, 293, 341, 307, 516, 281, 312, 1293, 452, 4846, 293, 611, 452, 12911, 11, 558, 30, 50710, 50710, 440, 5267, 293, 264, 4190, 295, 613, 5267, 486, 312, 25532, 1296, 4018, 281, 472, 13, 1743, 51022, 51022, 309, 311, 257, 957, 2158, 13, 6947, 19261, 393, 312, 1296, 4018, 293, 472, 13, 51268, 51268, 865, 13, 51318, 51318, 286, 519, 767, 264, 15743, 366, 17434, 13, 407, 264, 15743, 366, 439, 4018, 420, 472, 13, 583, 452, 3209, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.17246251636081272, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.748545193753671e-05}, {"id": 327, "seek": 207404, "start": 2093.12, "end": 2098.92, "text": " I think actually the inputs are binary. So the inputs are all zero or one. But my network", "tokens": [50364, 407, 286, 764, 264, 376, 45, 19756, 28872, 293, 341, 307, 516, 281, 312, 1293, 452, 4846, 293, 611, 452, 12911, 11, 558, 30, 50710, 50710, 440, 5267, 293, 264, 4190, 295, 613, 5267, 486, 312, 25532, 1296, 4018, 281, 472, 13, 1743, 51022, 51022, 309, 311, 257, 957, 2158, 13, 6947, 19261, 393, 312, 1296, 4018, 293, 472, 13, 51268, 51268, 865, 13, 51318, 51318, 286, 519, 767, 264, 15743, 366, 17434, 13, 407, 264, 15743, 366, 439, 4018, 420, 472, 13, 583, 452, 3209, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.17246251636081272, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.748545193753671e-05}, {"id": 328, "seek": 209892, "start": 2098.92, "end": 2106.44, "text": " will be outputting a real range between zero and one. So reparameterization, we have the,", "tokens": [50364, 486, 312, 5598, 783, 257, 957, 3613, 1296, 4018, 293, 472, 13, 407, 1085, 12835, 2398, 2144, 11, 321, 362, 264, 11, 50740, 50740, 437, 360, 321, 360, 510, 30, 407, 1085, 12835, 2398, 2144, 2212, 257, 2992, 293, 257, 3565, 21977, 11, 286, 603, 2903, 51246, 51246, 1780, 983, 321, 764, 3565, 21977, 13, 759, 291, 366, 294, 3097, 11, 321, 14722, 3832, 25163, 51572, 51572, 382, 516, 281, 312, 3565, 21977, 17207, 538, 472, 1922, 13, 400, 550, 286, 747, 264, 21510, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.17182185140888342, "compression_ratio": 1.619718309859155, "no_speech_prob": 2.1677871700376272e-05}, {"id": 329, "seek": 209892, "start": 2106.44, "end": 2116.56, "text": " what do we do here? So reparameterization given a mu and a log variance, I'll explain", "tokens": [50364, 486, 312, 5598, 783, 257, 957, 3613, 1296, 4018, 293, 472, 13, 407, 1085, 12835, 2398, 2144, 11, 321, 362, 264, 11, 50740, 50740, 437, 360, 321, 360, 510, 30, 407, 1085, 12835, 2398, 2144, 2212, 257, 2992, 293, 257, 3565, 21977, 11, 286, 603, 2903, 51246, 51246, 1780, 983, 321, 764, 3565, 21977, 13, 759, 291, 366, 294, 3097, 11, 321, 14722, 3832, 25163, 51572, 51572, 382, 516, 281, 312, 3565, 21977, 17207, 538, 472, 1922, 13, 400, 550, 286, 747, 264, 21510, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.17182185140888342, "compression_ratio": 1.619718309859155, "no_speech_prob": 2.1677871700376272e-05}, {"id": 330, "seek": 209892, "start": 2116.56, "end": 2123.08, "text": " later why we use log variance. If you are in training, we compute standard deviation", "tokens": [50364, 486, 312, 5598, 783, 257, 957, 3613, 1296, 4018, 293, 472, 13, 407, 1085, 12835, 2398, 2144, 11, 321, 362, 264, 11, 50740, 50740, 437, 360, 321, 360, 510, 30, 407, 1085, 12835, 2398, 2144, 2212, 257, 2992, 293, 257, 3565, 21977, 11, 286, 603, 2903, 51246, 51246, 1780, 983, 321, 764, 3565, 21977, 13, 759, 291, 366, 294, 3097, 11, 321, 14722, 3832, 25163, 51572, 51572, 382, 516, 281, 312, 3565, 21977, 17207, 538, 472, 1922, 13, 400, 550, 286, 747, 264, 21510, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.17182185140888342, "compression_ratio": 1.619718309859155, "no_speech_prob": 2.1677871700376272e-05}, {"id": 331, "seek": 209892, "start": 2123.08, "end": 2128.04, "text": " as going to be log variance multiplied by one half. And then I take the exponential.", "tokens": [50364, 486, 312, 5598, 783, 257, 957, 3613, 1296, 4018, 293, 472, 13, 407, 1085, 12835, 2398, 2144, 11, 321, 362, 264, 11, 50740, 50740, 437, 360, 321, 360, 510, 30, 407, 1085, 12835, 2398, 2144, 2212, 257, 2992, 293, 257, 3565, 21977, 11, 286, 603, 2903, 51246, 51246, 1780, 983, 321, 764, 3565, 21977, 13, 759, 291, 366, 294, 3097, 11, 321, 14722, 3832, 25163, 51572, 51572, 382, 516, 281, 312, 3565, 21977, 17207, 538, 472, 1922, 13, 400, 550, 286, 747, 264, 21510, 13, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.17182185140888342, "compression_ratio": 1.619718309859155, "no_speech_prob": 2.1677871700376272e-05}, {"id": 332, "seek": 212804, "start": 2128.04, "end": 2133.08, "text": " And so I get the standard deviation from the log variance. And then I get my epsilon, which", "tokens": [50364, 400, 370, 286, 483, 264, 3832, 25163, 490, 264, 3565, 21977, 13, 400, 550, 286, 483, 452, 17889, 11, 597, 50616, 50616, 307, 2935, 3247, 15551, 490, 257, 2710, 7316, 11, 597, 365, 2035, 2744, 286, 362, 510, 11, 558, 30, 50946, 50946, 407, 3832, 25163, 11, 286, 483, 264, 2744, 11, 286, 1884, 257, 777, 40863, 293, 286, 2836, 309, 365, 257, 2710, 7316, 51258, 51258, 1412, 13, 51344, 51344, 1396, 286, 2736, 264, 17889, 11, 597, 13165, 538, 264, 3832, 25163, 293, 286, 909, 264, 2992, 11, 597, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.17548481961514087, "compression_ratio": 1.8585365853658538, "no_speech_prob": 1.0285314601787832e-05}, {"id": 333, "seek": 212804, "start": 2133.08, "end": 2139.68, "text": " is simply sampled from a normal distribution, which with whatever size I have here, right?", "tokens": [50364, 400, 370, 286, 483, 264, 3832, 25163, 490, 264, 3565, 21977, 13, 400, 550, 286, 483, 452, 17889, 11, 597, 50616, 50616, 307, 2935, 3247, 15551, 490, 257, 2710, 7316, 11, 597, 365, 2035, 2744, 286, 362, 510, 11, 558, 30, 50946, 50946, 407, 3832, 25163, 11, 286, 483, 264, 2744, 11, 286, 1884, 257, 777, 40863, 293, 286, 2836, 309, 365, 257, 2710, 7316, 51258, 51258, 1412, 13, 51344, 51344, 1396, 286, 2736, 264, 17889, 11, 597, 13165, 538, 264, 3832, 25163, 293, 286, 909, 264, 2992, 11, 597, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.17548481961514087, "compression_ratio": 1.8585365853658538, "no_speech_prob": 1.0285314601787832e-05}, {"id": 334, "seek": 212804, "start": 2139.68, "end": 2145.92, "text": " So standard deviation, I get the size, I create a new tensor and I fill it with a normal distribution", "tokens": [50364, 400, 370, 286, 483, 264, 3832, 25163, 490, 264, 3565, 21977, 13, 400, 550, 286, 483, 452, 17889, 11, 597, 50616, 50616, 307, 2935, 3247, 15551, 490, 257, 2710, 7316, 11, 597, 365, 2035, 2744, 286, 362, 510, 11, 558, 30, 50946, 50946, 407, 3832, 25163, 11, 286, 483, 264, 2744, 11, 286, 1884, 257, 777, 40863, 293, 286, 2836, 309, 365, 257, 2710, 7316, 51258, 51258, 1412, 13, 51344, 51344, 1396, 286, 2736, 264, 17889, 11, 597, 13165, 538, 264, 3832, 25163, 293, 286, 909, 264, 2992, 11, 597, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.17548481961514087, "compression_ratio": 1.8585365853658538, "no_speech_prob": 1.0285314601787832e-05}, {"id": 335, "seek": 212804, "start": 2145.92, "end": 2147.64, "text": " data.", "tokens": [50364, 400, 370, 286, 483, 264, 3832, 25163, 490, 264, 3565, 21977, 13, 400, 550, 286, 483, 452, 17889, 11, 597, 50616, 50616, 307, 2935, 3247, 15551, 490, 257, 2710, 7316, 11, 597, 365, 2035, 2744, 286, 362, 510, 11, 558, 30, 50946, 50946, 407, 3832, 25163, 11, 286, 483, 264, 2744, 11, 286, 1884, 257, 777, 40863, 293, 286, 2836, 309, 365, 257, 2710, 7316, 51258, 51258, 1412, 13, 51344, 51344, 1396, 286, 2736, 264, 17889, 11, 597, 13165, 538, 264, 3832, 25163, 293, 286, 909, 264, 2992, 11, 597, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.17548481961514087, "compression_ratio": 1.8585365853658538, "no_speech_prob": 1.0285314601787832e-05}, {"id": 336, "seek": 212804, "start": 2147.64, "end": 2152.2799999999997, "text": " Then I return the epsilon, which applies by the standard deviation and I add the mu, which", "tokens": [50364, 400, 370, 286, 483, 264, 3832, 25163, 490, 264, 3565, 21977, 13, 400, 550, 286, 483, 452, 17889, 11, 597, 50616, 50616, 307, 2935, 3247, 15551, 490, 257, 2710, 7316, 11, 597, 365, 2035, 2744, 286, 362, 510, 11, 558, 30, 50946, 50946, 407, 3832, 25163, 11, 286, 483, 264, 2744, 11, 286, 1884, 257, 777, 40863, 293, 286, 2836, 309, 365, 257, 2710, 7316, 51258, 51258, 1412, 13, 51344, 51344, 1396, 286, 2736, 264, 17889, 11, 597, 13165, 538, 264, 3832, 25163, 293, 286, 909, 264, 2992, 11, 597, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.17548481961514087, "compression_ratio": 1.8585365853658538, "no_speech_prob": 1.0285314601787832e-05}, {"id": 337, "seek": 215228, "start": 2152.28, "end": 2158.4, "text": " is what I showed you before. If I am not training, I don't have to add noise, right? So I can", "tokens": [50364, 307, 437, 286, 4712, 291, 949, 13, 759, 286, 669, 406, 3097, 11, 286, 500, 380, 362, 281, 909, 5658, 11, 558, 30, 407, 286, 393, 50670, 50670, 2935, 2736, 452, 2992, 13, 407, 286, 764, 341, 3209, 294, 257, 15957, 3142, 636, 13, 440, 2128, 4391, 307, 51002, 51002, 264, 3480, 13, 407, 510, 321, 362, 300, 264, 2058, 19866, 2170, 264, 4846, 11, 597, 307, 516, 281, 312, 725, 71, 18653, 51352, 51352, 666, 613, 721, 11, 1270, 300, 1936, 286, 517, 3970, 264, 5267, 666, 257, 8062, 13, 1396, 264, 2058, 19866, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.10886223147613834, "compression_ratio": 1.6008771929824561, "no_speech_prob": 1.202662770083407e-05}, {"id": 338, "seek": 215228, "start": 2158.4, "end": 2165.0400000000004, "text": " simply return my mu. So I use this network in a deterministic way. The forward mode is", "tokens": [50364, 307, 437, 286, 4712, 291, 949, 13, 759, 286, 669, 406, 3097, 11, 286, 500, 380, 362, 281, 909, 5658, 11, 558, 30, 407, 286, 393, 50670, 50670, 2935, 2736, 452, 2992, 13, 407, 286, 764, 341, 3209, 294, 257, 15957, 3142, 636, 13, 440, 2128, 4391, 307, 51002, 51002, 264, 3480, 13, 407, 510, 321, 362, 300, 264, 2058, 19866, 2170, 264, 4846, 11, 597, 307, 516, 281, 312, 725, 71, 18653, 51352, 51352, 666, 613, 721, 11, 1270, 300, 1936, 286, 517, 3970, 264, 5267, 666, 257, 8062, 13, 1396, 264, 2058, 19866, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.10886223147613834, "compression_ratio": 1.6008771929824561, "no_speech_prob": 1.202662770083407e-05}, {"id": 339, "seek": 215228, "start": 2165.0400000000004, "end": 2172.0400000000004, "text": " the following. So here we have that the encoder gets the input, which is going to be reshaped", "tokens": [50364, 307, 437, 286, 4712, 291, 949, 13, 759, 286, 669, 406, 3097, 11, 286, 500, 380, 362, 281, 909, 5658, 11, 558, 30, 407, 286, 393, 50670, 50670, 2935, 2736, 452, 2992, 13, 407, 286, 764, 341, 3209, 294, 257, 15957, 3142, 636, 13, 440, 2128, 4391, 307, 51002, 51002, 264, 3480, 13, 407, 510, 321, 362, 300, 264, 2058, 19866, 2170, 264, 4846, 11, 597, 307, 516, 281, 312, 725, 71, 18653, 51352, 51352, 666, 613, 721, 11, 1270, 300, 1936, 286, 517, 3970, 264, 5267, 666, 257, 8062, 13, 1396, 264, 2058, 19866, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.10886223147613834, "compression_ratio": 1.6008771929824561, "no_speech_prob": 1.202662770083407e-05}, {"id": 340, "seek": 215228, "start": 2172.0400000000004, "end": 2179.92, "text": " into these things, such that basically I unroll the images into a vector. Then the encoder", "tokens": [50364, 307, 437, 286, 4712, 291, 949, 13, 759, 286, 669, 406, 3097, 11, 286, 500, 380, 362, 281, 909, 5658, 11, 558, 30, 407, 286, 393, 50670, 50670, 2935, 2736, 452, 2992, 13, 407, 286, 764, 341, 3209, 294, 257, 15957, 3142, 636, 13, 440, 2128, 4391, 307, 51002, 51002, 264, 3480, 13, 407, 510, 321, 362, 300, 264, 2058, 19866, 2170, 264, 4846, 11, 597, 307, 516, 281, 312, 725, 71, 18653, 51352, 51352, 666, 613, 721, 11, 1270, 300, 1936, 286, 517, 3970, 264, 5267, 666, 257, 8062, 13, 1396, 264, 2058, 19866, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.10886223147613834, "compression_ratio": 1.6008771929824561, "no_speech_prob": 1.202662770083407e-05}, {"id": 341, "seek": 217992, "start": 2179.92, "end": 2185.12, "text": " is going to be outputting something. And then I reshape that one such that I have batch", "tokens": [50364, 307, 516, 281, 312, 5598, 783, 746, 13, 400, 550, 286, 725, 42406, 300, 472, 1270, 300, 286, 362, 15245, 50624, 50624, 2744, 732, 293, 550, 413, 11, 689, 413, 307, 264, 10139, 295, 264, 914, 293, 264, 10139, 295, 264, 1374, 21518, 13, 50990, 50990, 1396, 286, 362, 2992, 11, 597, 307, 264, 914, 11, 2935, 264, 700, 644, 11, 558, 11, 295, 613, 1074, 11, 295, 341, 51326, 51326, 413, 13, 400, 550, 264, 3565, 21977, 307, 516, 281, 312, 264, 661, 2146, 13, 400, 550, 286, 362, 452, 710, 11, 597, 51596, 51596, 307, 516, 281, 312, 452, 48994, 7006, 13, 467, 311, 516, 281, 312, 341, 1085, 12835, 2398, 2144, 2212, 452, 2992, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.14234161376953125, "compression_ratio": 1.905579399141631, "no_speech_prob": 5.376992339733988e-05}, {"id": 342, "seek": 217992, "start": 2185.12, "end": 2192.44, "text": " size two and then D, where D is the dimension of the mean and the dimension of the variances.", "tokens": [50364, 307, 516, 281, 312, 5598, 783, 746, 13, 400, 550, 286, 725, 42406, 300, 472, 1270, 300, 286, 362, 15245, 50624, 50624, 2744, 732, 293, 550, 413, 11, 689, 413, 307, 264, 10139, 295, 264, 914, 293, 264, 10139, 295, 264, 1374, 21518, 13, 50990, 50990, 1396, 286, 362, 2992, 11, 597, 307, 264, 914, 11, 2935, 264, 700, 644, 11, 558, 11, 295, 613, 1074, 11, 295, 341, 51326, 51326, 413, 13, 400, 550, 264, 3565, 21977, 307, 516, 281, 312, 264, 661, 2146, 13, 400, 550, 286, 362, 452, 710, 11, 597, 51596, 51596, 307, 516, 281, 312, 452, 48994, 7006, 13, 467, 311, 516, 281, 312, 341, 1085, 12835, 2398, 2144, 2212, 452, 2992, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.14234161376953125, "compression_ratio": 1.905579399141631, "no_speech_prob": 5.376992339733988e-05}, {"id": 343, "seek": 217992, "start": 2192.44, "end": 2199.16, "text": " Then I have mu, which is the mean, simply the first part, right, of these guys, of this", "tokens": [50364, 307, 516, 281, 312, 5598, 783, 746, 13, 400, 550, 286, 725, 42406, 300, 472, 1270, 300, 286, 362, 15245, 50624, 50624, 2744, 732, 293, 550, 413, 11, 689, 413, 307, 264, 10139, 295, 264, 914, 293, 264, 10139, 295, 264, 1374, 21518, 13, 50990, 50990, 1396, 286, 362, 2992, 11, 597, 307, 264, 914, 11, 2935, 264, 700, 644, 11, 558, 11, 295, 613, 1074, 11, 295, 341, 51326, 51326, 413, 13, 400, 550, 264, 3565, 21977, 307, 516, 281, 312, 264, 661, 2146, 13, 400, 550, 286, 362, 452, 710, 11, 597, 51596, 51596, 307, 516, 281, 312, 452, 48994, 7006, 13, 467, 311, 516, 281, 312, 341, 1085, 12835, 2398, 2144, 2212, 452, 2992, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.14234161376953125, "compression_ratio": 1.905579399141631, "no_speech_prob": 5.376992339733988e-05}, {"id": 344, "seek": 217992, "start": 2199.16, "end": 2204.56, "text": " D. And then the log variance is going to be the other guy. And then I have my z, which", "tokens": [50364, 307, 516, 281, 312, 5598, 783, 746, 13, 400, 550, 286, 725, 42406, 300, 472, 1270, 300, 286, 362, 15245, 50624, 50624, 2744, 732, 293, 550, 413, 11, 689, 413, 307, 264, 10139, 295, 264, 914, 293, 264, 10139, 295, 264, 1374, 21518, 13, 50990, 50990, 1396, 286, 362, 2992, 11, 597, 307, 264, 914, 11, 2935, 264, 700, 644, 11, 558, 11, 295, 613, 1074, 11, 295, 341, 51326, 51326, 413, 13, 400, 550, 264, 3565, 21977, 307, 516, 281, 312, 264, 661, 2146, 13, 400, 550, 286, 362, 452, 710, 11, 597, 51596, 51596, 307, 516, 281, 312, 452, 48994, 7006, 13, 467, 311, 516, 281, 312, 341, 1085, 12835, 2398, 2144, 2212, 452, 2992, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.14234161376953125, "compression_ratio": 1.905579399141631, "no_speech_prob": 5.376992339733988e-05}, {"id": 345, "seek": 217992, "start": 2204.56, "end": 2209.32, "text": " is going to be my latent variable. It's going to be this reparameterization given my mu", "tokens": [50364, 307, 516, 281, 312, 5598, 783, 746, 13, 400, 550, 286, 725, 42406, 300, 472, 1270, 300, 286, 362, 15245, 50624, 50624, 2744, 732, 293, 550, 413, 11, 689, 413, 307, 264, 10139, 295, 264, 914, 293, 264, 10139, 295, 264, 1374, 21518, 13, 50990, 50990, 1396, 286, 362, 2992, 11, 597, 307, 264, 914, 11, 2935, 264, 700, 644, 11, 558, 11, 295, 613, 1074, 11, 295, 341, 51326, 51326, 413, 13, 400, 550, 264, 3565, 21977, 307, 516, 281, 312, 264, 661, 2146, 13, 400, 550, 286, 362, 452, 710, 11, 597, 51596, 51596, 307, 516, 281, 312, 452, 48994, 7006, 13, 467, 311, 516, 281, 312, 341, 1085, 12835, 2398, 2144, 2212, 452, 2992, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.14234161376953125, "compression_ratio": 1.905579399141631, "no_speech_prob": 5.376992339733988e-05}, {"id": 346, "seek": 220932, "start": 2209.32, "end": 2216.96, "text": " and the log var. Why do I use a log var? You tell me. Why do I use a log var?", "tokens": [50364, 293, 264, 3565, 1374, 13, 1545, 360, 286, 764, 257, 3565, 1374, 30, 509, 980, 385, 13, 1545, 360, 286, 764, 257, 3565, 1374, 30, 50746, 50746, 440, 5598, 295, 9590, 393, 312, 3671, 11, 370, 291, 643, 3353, 13, 50964, 50964, 1779, 11, 558, 13, 407, 2212, 300, 264, 1374, 21518, 366, 787, 3353, 11, 498, 286, 14722, 264, 3565, 11, 309, 51226, 51226, 4045, 291, 281, 5598, 264, 1577, 957, 3613, 337, 264, 2058, 19866, 11, 558, 30, 407, 291, 393, 764, 264, 1379, 51558, 51558], "temperature": 0.0, "avg_logprob": -0.23592760297987198, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.0453862816793844e-05}, {"id": 347, "seek": 220932, "start": 2216.96, "end": 2221.32, "text": " The output of networks can be negative, so you need positive.", "tokens": [50364, 293, 264, 3565, 1374, 13, 1545, 360, 286, 764, 257, 3565, 1374, 30, 509, 980, 385, 13, 1545, 360, 286, 764, 257, 3565, 1374, 30, 50746, 50746, 440, 5598, 295, 9590, 393, 312, 3671, 11, 370, 291, 643, 3353, 13, 50964, 50964, 1779, 11, 558, 13, 407, 2212, 300, 264, 1374, 21518, 366, 787, 3353, 11, 498, 286, 14722, 264, 3565, 11, 309, 51226, 51226, 4045, 291, 281, 5598, 264, 1577, 957, 3613, 337, 264, 2058, 19866, 11, 558, 30, 407, 291, 393, 764, 264, 1379, 51558, 51558], "temperature": 0.0, "avg_logprob": -0.23592760297987198, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.0453862816793844e-05}, {"id": 348, "seek": 220932, "start": 2221.32, "end": 2226.56, "text": " Right, right. So given that the variances are only positive, if I compute the log, it", "tokens": [50364, 293, 264, 3565, 1374, 13, 1545, 360, 286, 764, 257, 3565, 1374, 30, 509, 980, 385, 13, 1545, 360, 286, 764, 257, 3565, 1374, 30, 50746, 50746, 440, 5598, 295, 9590, 393, 312, 3671, 11, 370, 291, 643, 3353, 13, 50964, 50964, 1779, 11, 558, 13, 407, 2212, 300, 264, 1374, 21518, 366, 787, 3353, 11, 498, 286, 14722, 264, 3565, 11, 309, 51226, 51226, 4045, 291, 281, 5598, 264, 1577, 957, 3613, 337, 264, 2058, 19866, 11, 558, 30, 407, 291, 393, 764, 264, 1379, 51558, 51558], "temperature": 0.0, "avg_logprob": -0.23592760297987198, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.0453862816793844e-05}, {"id": 349, "seek": 220932, "start": 2226.56, "end": 2233.2000000000003, "text": " allows you to output the full real range for the encoder, right? So you can use the whole", "tokens": [50364, 293, 264, 3565, 1374, 13, 1545, 360, 286, 764, 257, 3565, 1374, 30, 509, 980, 385, 13, 1545, 360, 286, 764, 257, 3565, 1374, 30, 50746, 50746, 440, 5598, 295, 9590, 393, 312, 3671, 11, 370, 291, 643, 3353, 13, 50964, 50964, 1779, 11, 558, 13, 407, 2212, 300, 264, 1374, 21518, 366, 787, 3353, 11, 498, 286, 14722, 264, 3565, 11, 309, 51226, 51226, 4045, 291, 281, 5598, 264, 1577, 957, 3613, 337, 264, 2058, 19866, 11, 558, 30, 407, 291, 393, 764, 264, 1379, 51558, 51558], "temperature": 0.0, "avg_logprob": -0.23592760297987198, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.0453862816793844e-05}, {"id": 350, "seek": 223320, "start": 2233.2, "end": 2239.72, "text": " real range. And then I define my model as this VAE and I send it to the device. Here", "tokens": [50364, 957, 3613, 13, 400, 550, 286, 6964, 452, 2316, 382, 341, 18527, 36, 293, 286, 2845, 309, 281, 264, 4302, 13, 1692, 50690, 50690, 286, 6964, 264, 19618, 5028, 6545, 13, 400, 550, 286, 6964, 452, 4470, 2445, 11, 597, 307, 264, 2408, 51054, 51054, 295, 732, 3166, 11, 264, 17434, 3278, 12, 317, 27514, 1296, 264, 4846, 293, 264, 31565, 11, 597, 307, 51412, 51412], "temperature": 0.0, "avg_logprob": -0.10811186538023107, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.6427429727627896e-05}, {"id": 351, "seek": 223320, "start": 2239.72, "end": 2247.0, "text": " I define the optimization optimizer. And then I define my loss function, which is the sum", "tokens": [50364, 957, 3613, 13, 400, 550, 286, 6964, 452, 2316, 382, 341, 18527, 36, 293, 286, 2845, 309, 281, 264, 4302, 13, 1692, 50690, 50690, 286, 6964, 264, 19618, 5028, 6545, 13, 400, 550, 286, 6964, 452, 4470, 2445, 11, 597, 307, 264, 2408, 51054, 51054, 295, 732, 3166, 11, 264, 17434, 3278, 12, 317, 27514, 1296, 264, 4846, 293, 264, 31565, 11, 597, 307, 51412, 51412], "temperature": 0.0, "avg_logprob": -0.10811186538023107, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.6427429727627896e-05}, {"id": 352, "seek": 223320, "start": 2247.0, "end": 2254.16, "text": " of two parts, the binary cross-entropy between the input and the reconstruction, which is", "tokens": [50364, 957, 3613, 13, 400, 550, 286, 6964, 452, 2316, 382, 341, 18527, 36, 293, 286, 2845, 309, 281, 264, 4302, 13, 1692, 50690, 50690, 286, 6964, 264, 19618, 5028, 6545, 13, 400, 550, 286, 6964, 452, 4470, 2445, 11, 597, 307, 264, 2408, 51054, 51054, 295, 732, 3166, 11, 264, 17434, 3278, 12, 317, 27514, 1296, 264, 4846, 293, 264, 31565, 11, 597, 307, 51412, 51412], "temperature": 0.0, "avg_logprob": -0.10811186538023107, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.6427429727627896e-05}, {"id": 353, "seek": 225416, "start": 2254.16, "end": 2264.04, "text": " linear. So I have the x hat and then the x. And then I try, I sum all of them. And then", "tokens": [50364, 8213, 13, 407, 286, 362, 264, 2031, 2385, 293, 550, 264, 2031, 13, 400, 550, 286, 853, 11, 286, 2408, 439, 295, 552, 13, 400, 550, 50858, 50858, 264, 47991, 47387, 13, 407, 321, 362, 264, 1374, 11, 597, 307, 264, 8213, 13, 1396, 291, 362, 264, 3175, 3565, 51250, 51250, 1374, 11, 597, 307, 264, 41473, 355, 13195, 7929, 760, 11, 293, 550, 3175, 472, 13, 400, 550, 321, 362, 264, 2992, 13, 400, 51550, 51550, 550, 321, 853, 281, 17522, 341, 1507, 11, 558, 30, 1057, 558, 13, 407, 3097, 23294, 13, 467, 311, 588, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.18647067381603882, "compression_ratio": 1.8134715025906736, "no_speech_prob": 1.406123374181334e-05}, {"id": 354, "seek": 225416, "start": 2264.04, "end": 2271.8799999999997, "text": " the KL divergence. So we have the var, which is the linear. Then you have the minus log", "tokens": [50364, 8213, 13, 407, 286, 362, 264, 2031, 2385, 293, 550, 264, 2031, 13, 400, 550, 286, 853, 11, 286, 2408, 439, 295, 552, 13, 400, 550, 50858, 50858, 264, 47991, 47387, 13, 407, 321, 362, 264, 1374, 11, 597, 307, 264, 8213, 13, 1396, 291, 362, 264, 3175, 3565, 51250, 51250, 1374, 11, 597, 307, 264, 41473, 355, 13195, 7929, 760, 11, 293, 550, 3175, 472, 13, 400, 550, 321, 362, 264, 2992, 13, 400, 51550, 51550, 550, 321, 853, 281, 17522, 341, 1507, 11, 558, 30, 1057, 558, 13, 407, 3097, 23294, 13, 467, 311, 588, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.18647067381603882, "compression_ratio": 1.8134715025906736, "no_speech_prob": 1.406123374181334e-05}, {"id": 355, "seek": 225416, "start": 2271.8799999999997, "end": 2277.8799999999997, "text": " var, which is the logarithmic flip down, and then minus one. And then we have the mu. And", "tokens": [50364, 8213, 13, 407, 286, 362, 264, 2031, 2385, 293, 550, 264, 2031, 13, 400, 550, 286, 853, 11, 286, 2408, 439, 295, 552, 13, 400, 550, 50858, 50858, 264, 47991, 47387, 13, 407, 321, 362, 264, 1374, 11, 597, 307, 264, 8213, 13, 1396, 291, 362, 264, 3175, 3565, 51250, 51250, 1374, 11, 597, 307, 264, 41473, 355, 13195, 7929, 760, 11, 293, 550, 3175, 472, 13, 400, 550, 321, 362, 264, 2992, 13, 400, 51550, 51550, 550, 321, 853, 281, 17522, 341, 1507, 11, 558, 30, 1057, 558, 13, 407, 3097, 23294, 13, 467, 311, 588, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.18647067381603882, "compression_ratio": 1.8134715025906736, "no_speech_prob": 1.406123374181334e-05}, {"id": 356, "seek": 225416, "start": 2277.8799999999997, "end": 2283.0, "text": " then we try to minimize this stuff, right? All right. So training scripts. It's very", "tokens": [50364, 8213, 13, 407, 286, 362, 264, 2031, 2385, 293, 550, 264, 2031, 13, 400, 550, 286, 853, 11, 286, 2408, 439, 295, 552, 13, 400, 550, 50858, 50858, 264, 47991, 47387, 13, 407, 321, 362, 264, 1374, 11, 597, 307, 264, 8213, 13, 1396, 291, 362, 264, 3175, 3565, 51250, 51250, 1374, 11, 597, 307, 264, 41473, 355, 13195, 7929, 760, 11, 293, 550, 3175, 472, 13, 400, 550, 321, 362, 264, 2992, 13, 400, 51550, 51550, 550, 321, 853, 281, 17522, 341, 1507, 11, 558, 30, 1057, 558, 13, 407, 3097, 23294, 13, 467, 311, 588, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.18647067381603882, "compression_ratio": 1.8134715025906736, "no_speech_prob": 1.406123374181334e-05}, {"id": 357, "seek": 228300, "start": 2283.0, "end": 2289.0, "text": " simple, right? So you have the model, which is outputting the prediction x hat. Let's", "tokens": [50364, 2199, 11, 558, 30, 407, 291, 362, 264, 2316, 11, 597, 307, 5598, 783, 264, 17630, 2031, 2385, 13, 961, 311, 50664, 50664, 536, 510, 11, 558, 30, 35524, 23930, 264, 5598, 295, 264, 979, 19866, 11, 264, 2992, 293, 264, 3565, 1374, 13, 407, 50998, 50998, 510, 291, 483, 264, 2316, 13, 509, 3154, 264, 4846, 13, 509, 483, 2031, 2385, 11, 2992, 11, 3565, 1374, 13, 509, 393, 14722, 51268, 51268, 264, 4470, 1228, 264, 2031, 2385, 11, 2031, 2992, 11, 293, 3565, 1374, 13, 1783, 885, 264, 4846, 11, 457, 611, 264, 3779, 13, 400, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.17238553365071616, "compression_ratio": 1.745, "no_speech_prob": 2.3434029571944848e-05}, {"id": 358, "seek": 228300, "start": 2289.0, "end": 2295.68, "text": " see here, right? Forward outputs the output of the decoder, the mu and the log var. So", "tokens": [50364, 2199, 11, 558, 30, 407, 291, 362, 264, 2316, 11, 597, 307, 5598, 783, 264, 17630, 2031, 2385, 13, 961, 311, 50664, 50664, 536, 510, 11, 558, 30, 35524, 23930, 264, 5598, 295, 264, 979, 19866, 11, 264, 2992, 293, 264, 3565, 1374, 13, 407, 50998, 50998, 510, 291, 483, 264, 2316, 13, 509, 3154, 264, 4846, 13, 509, 483, 2031, 2385, 11, 2992, 11, 3565, 1374, 13, 509, 393, 14722, 51268, 51268, 264, 4470, 1228, 264, 2031, 2385, 11, 2031, 2992, 11, 293, 3565, 1374, 13, 1783, 885, 264, 4846, 11, 457, 611, 264, 3779, 13, 400, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.17238553365071616, "compression_ratio": 1.745, "no_speech_prob": 2.3434029571944848e-05}, {"id": 359, "seek": 228300, "start": 2295.68, "end": 2301.08, "text": " here you get the model. You feed the input. You get x hat, mu, log var. You can compute", "tokens": [50364, 2199, 11, 558, 30, 407, 291, 362, 264, 2316, 11, 597, 307, 5598, 783, 264, 17630, 2031, 2385, 13, 961, 311, 50664, 50664, 536, 510, 11, 558, 30, 35524, 23930, 264, 5598, 295, 264, 979, 19866, 11, 264, 2992, 293, 264, 3565, 1374, 13, 407, 50998, 50998, 510, 291, 483, 264, 2316, 13, 509, 3154, 264, 4846, 13, 509, 483, 2031, 2385, 11, 2992, 11, 3565, 1374, 13, 509, 393, 14722, 51268, 51268, 264, 4470, 1228, 264, 2031, 2385, 11, 2031, 2992, 11, 293, 3565, 1374, 13, 1783, 885, 264, 4846, 11, 457, 611, 264, 3779, 13, 400, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.17238553365071616, "compression_ratio": 1.745, "no_speech_prob": 2.3434029571944848e-05}, {"id": 360, "seek": 228300, "start": 2301.08, "end": 2309.16, "text": " the loss using the x hat, x mu, and log var. X being the input, but also the target. And", "tokens": [50364, 2199, 11, 558, 30, 407, 291, 362, 264, 2316, 11, 597, 307, 5598, 783, 264, 17630, 2031, 2385, 13, 961, 311, 50664, 50664, 536, 510, 11, 558, 30, 35524, 23930, 264, 5598, 295, 264, 979, 19866, 11, 264, 2992, 293, 264, 3565, 1374, 13, 407, 50998, 50998, 510, 291, 483, 264, 2316, 13, 509, 3154, 264, 4846, 13, 509, 483, 2031, 2385, 11, 2992, 11, 3565, 1374, 13, 509, 393, 14722, 51268, 51268, 264, 4470, 1228, 264, 2031, 2385, 11, 2031, 2992, 11, 293, 3565, 1374, 13, 1783, 885, 264, 4846, 11, 457, 611, 264, 3779, 13, 400, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.17238553365071616, "compression_ratio": 1.745, "no_speech_prob": 2.3434029571944848e-05}, {"id": 361, "seek": 230916, "start": 2309.16, "end": 2317.6, "text": " then we add the item to the loss. We clean up the gradients from the previous steps,", "tokens": [50364, 550, 321, 909, 264, 3174, 281, 264, 4470, 13, 492, 2541, 493, 264, 2771, 2448, 490, 264, 3894, 4439, 11, 50786, 50786, 2042, 24903, 11, 14722, 264, 14641, 33733, 11, 293, 550, 291, 1823, 13, 400, 550, 510, 286, 445, 360, 51138, 51138, 264, 4997, 293, 360, 512, 269, 2834, 337, 1780, 322, 13, 407, 321, 1409, 365, 5883, 6713, 295, 5923, 11, 51476, 51476], "temperature": 0.0, "avg_logprob": -0.19863535752936975, "compression_ratio": 1.4972067039106145, "no_speech_prob": 2.5051012926269323e-05}, {"id": 362, "seek": 230916, "start": 2317.6, "end": 2324.64, "text": " perform computation, compute the partial derivatives, and then you step. And then here I just do", "tokens": [50364, 550, 321, 909, 264, 3174, 281, 264, 4470, 13, 492, 2541, 493, 264, 2771, 2448, 490, 264, 3894, 4439, 11, 50786, 50786, 2042, 24903, 11, 14722, 264, 14641, 33733, 11, 293, 550, 291, 1823, 13, 400, 550, 510, 286, 445, 360, 51138, 51138, 264, 4997, 293, 360, 512, 269, 2834, 337, 1780, 322, 13, 407, 321, 1409, 365, 5883, 6713, 295, 5923, 11, 51476, 51476], "temperature": 0.0, "avg_logprob": -0.19863535752936975, "compression_ratio": 1.4972067039106145, "no_speech_prob": 2.5051012926269323e-05}, {"id": 363, "seek": 230916, "start": 2324.64, "end": 2331.3999999999996, "text": " the testing and do some caching for later on. So we started with initial error of 500,", "tokens": [50364, 550, 321, 909, 264, 3174, 281, 264, 4470, 13, 492, 2541, 493, 264, 2771, 2448, 490, 264, 3894, 4439, 11, 50786, 50786, 2042, 24903, 11, 14722, 264, 14641, 33733, 11, 293, 550, 291, 1823, 13, 400, 550, 510, 286, 445, 360, 51138, 51138, 264, 4997, 293, 360, 512, 269, 2834, 337, 1780, 322, 13, 407, 321, 1409, 365, 5883, 6713, 295, 5923, 11, 51476, 51476], "temperature": 0.0, "avg_logprob": -0.19863535752936975, "compression_ratio": 1.4972067039106145, "no_speech_prob": 2.5051012926269323e-05}, {"id": 364, "seek": 233140, "start": 2331.4, "end": 2339.92, "text": " roughly 540. This is before training. And then it goes immediately down to 200 and then", "tokens": [50364, 9810, 1025, 5254, 13, 639, 307, 949, 3097, 13, 400, 550, 309, 1709, 4258, 760, 281, 2331, 293, 550, 50790, 50790, 1709, 760, 281, 2319, 13, 400, 370, 586, 286, 478, 516, 281, 312, 4099, 291, 257, 1326, 295, 264, 3542, 13, 639, 51160, 51160, 307, 264, 4846, 286, 3154, 281, 264, 3209, 13, 400, 264, 1701, 31774, 3209, 31499, 626, 11, 295, 1164, 11, 51500, 51500, 574, 411, 4611, 11, 558, 30, 583, 1392, 11, 300, 311, 2489, 13, 407, 321, 393, 1066, 516, 13, 400, 300, 311, 516, 281, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.132297094244706, "compression_ratio": 1.5936073059360731, "no_speech_prob": 9.51406400417909e-06}, {"id": 365, "seek": 233140, "start": 2339.92, "end": 2347.32, "text": " goes down to 100. And so now I'm going to be showing you a few of the results. This", "tokens": [50364, 9810, 1025, 5254, 13, 639, 307, 949, 3097, 13, 400, 550, 309, 1709, 4258, 760, 281, 2331, 293, 550, 50790, 50790, 1709, 760, 281, 2319, 13, 400, 370, 586, 286, 478, 516, 281, 312, 4099, 291, 257, 1326, 295, 264, 3542, 13, 639, 51160, 51160, 307, 264, 4846, 286, 3154, 281, 264, 3209, 13, 400, 264, 1701, 31774, 3209, 31499, 626, 11, 295, 1164, 11, 51500, 51500, 574, 411, 4611, 11, 558, 30, 583, 1392, 11, 300, 311, 2489, 13, 407, 321, 393, 1066, 516, 13, 400, 300, 311, 516, 281, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.132297094244706, "compression_ratio": 1.5936073059360731, "no_speech_prob": 9.51406400417909e-06}, {"id": 366, "seek": 233140, "start": 2347.32, "end": 2354.12, "text": " is the input I feed to the network. And the untrained network reconstructions, of course,", "tokens": [50364, 9810, 1025, 5254, 13, 639, 307, 949, 3097, 13, 400, 550, 309, 1709, 4258, 760, 281, 2331, 293, 550, 50790, 50790, 1709, 760, 281, 2319, 13, 400, 370, 586, 286, 478, 516, 281, 312, 4099, 291, 257, 1326, 295, 264, 3542, 13, 639, 51160, 51160, 307, 264, 4846, 286, 3154, 281, 264, 3209, 13, 400, 264, 1701, 31774, 3209, 31499, 626, 11, 295, 1164, 11, 51500, 51500, 574, 411, 4611, 11, 558, 30, 583, 1392, 11, 300, 311, 2489, 13, 407, 321, 393, 1066, 516, 13, 400, 300, 311, 516, 281, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.132297094244706, "compression_ratio": 1.5936073059360731, "no_speech_prob": 9.51406400417909e-06}, {"id": 367, "seek": 233140, "start": 2354.12, "end": 2360.28, "text": " look like shit, right? But okay, that's fine. So we can keep going. And that's going to", "tokens": [50364, 9810, 1025, 5254, 13, 639, 307, 949, 3097, 13, 400, 550, 309, 1709, 4258, 760, 281, 2331, 293, 550, 50790, 50790, 1709, 760, 281, 2319, 13, 400, 370, 586, 286, 478, 516, 281, 312, 4099, 291, 257, 1326, 295, 264, 3542, 13, 639, 51160, 51160, 307, 264, 4846, 286, 3154, 281, 264, 3209, 13, 400, 264, 1701, 31774, 3209, 31499, 626, 11, 295, 1164, 11, 51500, 51500, 574, 411, 4611, 11, 558, 30, 583, 1392, 11, 300, 311, 2489, 13, 407, 321, 393, 1066, 516, 13, 400, 300, 311, 516, 281, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.132297094244706, "compression_ratio": 1.5936073059360731, "no_speech_prob": 9.51406400417909e-06}, {"id": 368, "seek": 236028, "start": 2360.28, "end": 2367.6000000000004, "text": " be the first epoch, right? Cool. Second epoch, third, fourth, and so on. And they look better", "tokens": [50364, 312, 264, 700, 30992, 339, 11, 558, 30, 8561, 13, 5736, 30992, 339, 11, 2636, 11, 6409, 11, 293, 370, 322, 13, 400, 436, 574, 1101, 50730, 50730, 293, 1101, 11, 295, 1164, 13, 407, 437, 393, 321, 360, 558, 586, 30, 316, 1326, 721, 321, 393, 360, 13, 1171, 1365, 11, 51154, 51154, 586, 11, 321, 393, 2935, 6889, 710, 490, 257, 2710, 7316, 13, 400, 550, 286, 979, 1429, 341, 4974, 51574, 51574, 1507, 11, 558, 30, 407, 341, 1177, 380, 808, 490, 364, 2058, 19866, 13, 400, 286, 855, 291, 586, 437, 264, 979, 19866, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.14240397659002565, "compression_ratio": 1.6255707762557077, "no_speech_prob": 1.8050339349429123e-05}, {"id": 369, "seek": 236028, "start": 2367.6000000000004, "end": 2376.0800000000004, "text": " and better, of course. So what can we do right now? A few things we can do. For example,", "tokens": [50364, 312, 264, 700, 30992, 339, 11, 558, 30, 8561, 13, 5736, 30992, 339, 11, 2636, 11, 6409, 11, 293, 370, 322, 13, 400, 436, 574, 1101, 50730, 50730, 293, 1101, 11, 295, 1164, 13, 407, 437, 393, 321, 360, 558, 586, 30, 316, 1326, 721, 321, 393, 360, 13, 1171, 1365, 11, 51154, 51154, 586, 11, 321, 393, 2935, 6889, 710, 490, 257, 2710, 7316, 13, 400, 550, 286, 979, 1429, 341, 4974, 51574, 51574, 1507, 11, 558, 30, 407, 341, 1177, 380, 808, 490, 364, 2058, 19866, 13, 400, 286, 855, 291, 586, 437, 264, 979, 19866, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.14240397659002565, "compression_ratio": 1.6255707762557077, "no_speech_prob": 1.8050339349429123e-05}, {"id": 370, "seek": 236028, "start": 2376.0800000000004, "end": 2384.48, "text": " now, we can simply sample z from a normal distribution. And then I decode this random", "tokens": [50364, 312, 264, 700, 30992, 339, 11, 558, 30, 8561, 13, 5736, 30992, 339, 11, 2636, 11, 6409, 11, 293, 370, 322, 13, 400, 436, 574, 1101, 50730, 50730, 293, 1101, 11, 295, 1164, 13, 407, 437, 393, 321, 360, 558, 586, 30, 316, 1326, 721, 321, 393, 360, 13, 1171, 1365, 11, 51154, 51154, 586, 11, 321, 393, 2935, 6889, 710, 490, 257, 2710, 7316, 13, 400, 550, 286, 979, 1429, 341, 4974, 51574, 51574, 1507, 11, 558, 30, 407, 341, 1177, 380, 808, 490, 364, 2058, 19866, 13, 400, 286, 855, 291, 586, 437, 264, 979, 19866, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.14240397659002565, "compression_ratio": 1.6255707762557077, "no_speech_prob": 1.8050339349429123e-05}, {"id": 371, "seek": 236028, "start": 2384.48, "end": 2389.6800000000003, "text": " stuff, right? So this doesn't come from an encoder. And I show you now what the decoder", "tokens": [50364, 312, 264, 700, 30992, 339, 11, 558, 30, 8561, 13, 5736, 30992, 339, 11, 2636, 11, 6409, 11, 293, 370, 322, 13, 400, 436, 574, 1101, 50730, 50730, 293, 1101, 11, 295, 1164, 13, 407, 437, 393, 321, 360, 558, 586, 30, 316, 1326, 721, 321, 393, 360, 13, 1171, 1365, 11, 51154, 51154, 586, 11, 321, 393, 2935, 6889, 710, 490, 257, 2710, 7316, 13, 400, 550, 286, 979, 1429, 341, 4974, 51574, 51574, 1507, 11, 558, 30, 407, 341, 1177, 380, 808, 490, 364, 2058, 19866, 13, 400, 286, 855, 291, 586, 437, 264, 979, 19866, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.14240397659002565, "compression_ratio": 1.6255707762557077, "no_speech_prob": 1.8050339349429123e-05}, {"id": 372, "seek": 238968, "start": 2389.68, "end": 2394.7599999999998, "text": " does whenever you sample from the distribution that the latent variable should have been", "tokens": [50364, 775, 5699, 291, 6889, 490, 264, 7316, 300, 264, 48994, 7006, 820, 362, 668, 50618, 50618, 3480, 13, 400, 370, 613, 366, 257, 1326, 5110, 295, 577, 21179, 490, 264, 48994, 7316, 11, 51118, 51118, 291, 458, 11, 2170, 979, 12340, 666, 746, 13, 492, 658, 257, 4949, 510, 11, 321, 658, 257, 4018, 11, 321, 658, 512, 51384, 51384, 1732, 13, 407, 512, 295, 264, 10682, 366, 588, 731, 7642, 11, 4949, 11, 732, 13, 583, 550, 661, 10682, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.13429569062732516, "compression_ratio": 1.6698564593301435, "no_speech_prob": 3.480059604044072e-05}, {"id": 373, "seek": 238968, "start": 2394.7599999999998, "end": 2404.7599999999998, "text": " following. And so these are a few examples of how sampling from the latent distribution,", "tokens": [50364, 775, 5699, 291, 6889, 490, 264, 7316, 300, 264, 48994, 7006, 820, 362, 668, 50618, 50618, 3480, 13, 400, 370, 613, 366, 257, 1326, 5110, 295, 577, 21179, 490, 264, 48994, 7316, 11, 51118, 51118, 291, 458, 11, 2170, 979, 12340, 666, 746, 13, 492, 658, 257, 4949, 510, 11, 321, 658, 257, 4018, 11, 321, 658, 512, 51384, 51384, 1732, 13, 407, 512, 295, 264, 10682, 366, 588, 731, 7642, 11, 4949, 11, 732, 13, 583, 550, 661, 10682, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.13429569062732516, "compression_ratio": 1.6698564593301435, "no_speech_prob": 3.480059604044072e-05}, {"id": 374, "seek": 238968, "start": 2404.7599999999998, "end": 2410.08, "text": " you know, gets decoded into something. We got a nine here, we got a zero, we got some", "tokens": [50364, 775, 5699, 291, 6889, 490, 264, 7316, 300, 264, 48994, 7006, 820, 362, 668, 50618, 50618, 3480, 13, 400, 370, 613, 366, 257, 1326, 5110, 295, 577, 21179, 490, 264, 48994, 7316, 11, 51118, 51118, 291, 458, 11, 2170, 979, 12340, 666, 746, 13, 492, 658, 257, 4949, 510, 11, 321, 658, 257, 4018, 11, 321, 658, 512, 51384, 51384, 1732, 13, 407, 512, 295, 264, 10682, 366, 588, 731, 7642, 11, 4949, 11, 732, 13, 583, 550, 661, 10682, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.13429569062732516, "compression_ratio": 1.6698564593301435, "no_speech_prob": 3.480059604044072e-05}, {"id": 375, "seek": 238968, "start": 2410.08, "end": 2416.0, "text": " five. So some of the regions are very well defined, nine, two. But then other regions", "tokens": [50364, 775, 5699, 291, 6889, 490, 264, 7316, 300, 264, 48994, 7006, 820, 362, 668, 50618, 50618, 3480, 13, 400, 370, 613, 366, 257, 1326, 5110, 295, 577, 21179, 490, 264, 48994, 7316, 11, 51118, 51118, 291, 458, 11, 2170, 979, 12340, 666, 746, 13, 492, 658, 257, 4949, 510, 11, 321, 658, 257, 4018, 11, 321, 658, 512, 51384, 51384, 1732, 13, 407, 512, 295, 264, 10682, 366, 588, 731, 7642, 11, 4949, 11, 732, 13, 583, 550, 661, 10682, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.13429569062732516, "compression_ratio": 1.6698564593301435, "no_speech_prob": 3.480059604044072e-05}, {"id": 376, "seek": 241600, "start": 2416.0, "end": 2422.36, "text": " like this thing here, or this thing here, or the number 14 here, they don't really look", "tokens": [50364, 411, 341, 551, 510, 11, 420, 341, 551, 510, 11, 420, 264, 1230, 3499, 510, 11, 436, 500, 380, 534, 574, 50682, 50682, 411, 731, 11, 411, 27011, 13, 639, 307, 570, 983, 30, 708, 311, 264, 1154, 510, 30, 492, 2378, 380, 534, 51034, 51034, 5343, 264, 1379, 1901, 13, 286, 445, 8895, 337, 472, 3456, 13, 759, 286, 3847, 337, 1266, 2077, 11, 309, 311, 51308, 51308, 516, 281, 312, 445, 1364, 6239, 13, 1033, 13, 407, 510, 11, 729, 16295, 500, 380, 1939, 2836, 264, 51544, 51544], "temperature": 0.0, "avg_logprob": -0.1354513168334961, "compression_ratio": 1.6214953271028036, "no_speech_prob": 2.2818530851509422e-05}, {"id": 377, "seek": 241600, "start": 2422.36, "end": 2429.4, "text": " like well, like digits. This is because why? What's the problem here? We haven't really", "tokens": [50364, 411, 341, 551, 510, 11, 420, 341, 551, 510, 11, 420, 264, 1230, 3499, 510, 11, 436, 500, 380, 534, 574, 50682, 50682, 411, 731, 11, 411, 27011, 13, 639, 307, 570, 983, 30, 708, 311, 264, 1154, 510, 30, 492, 2378, 380, 534, 51034, 51034, 5343, 264, 1379, 1901, 13, 286, 445, 8895, 337, 472, 3456, 13, 759, 286, 3847, 337, 1266, 2077, 11, 309, 311, 51308, 51308, 516, 281, 312, 445, 1364, 6239, 13, 1033, 13, 407, 510, 11, 729, 16295, 500, 380, 1939, 2836, 264, 51544, 51544], "temperature": 0.0, "avg_logprob": -0.1354513168334961, "compression_ratio": 1.6214953271028036, "no_speech_prob": 2.2818530851509422e-05}, {"id": 378, "seek": 241600, "start": 2429.4, "end": 2434.88, "text": " covered the whole space. I just trained for one minute. If I train for 10 minutes, it's", "tokens": [50364, 411, 341, 551, 510, 11, 420, 341, 551, 510, 11, 420, 264, 1230, 3499, 510, 11, 436, 500, 380, 534, 574, 50682, 50682, 411, 731, 11, 411, 27011, 13, 639, 307, 570, 983, 30, 708, 311, 264, 1154, 510, 30, 492, 2378, 380, 534, 51034, 51034, 5343, 264, 1379, 1901, 13, 286, 445, 8895, 337, 472, 3456, 13, 759, 286, 3847, 337, 1266, 2077, 11, 309, 311, 51308, 51308, 516, 281, 312, 445, 1364, 6239, 13, 1033, 13, 407, 510, 11, 729, 16295, 500, 380, 1939, 2836, 264, 51544, 51544], "temperature": 0.0, "avg_logprob": -0.1354513168334961, "compression_ratio": 1.6214953271028036, "no_speech_prob": 2.2818530851509422e-05}, {"id": 379, "seek": 241600, "start": 2434.88, "end": 2439.6, "text": " going to be just working perfectly. Okay. So here, those bubbles don't yet fill the", "tokens": [50364, 411, 341, 551, 510, 11, 420, 341, 551, 510, 11, 420, 264, 1230, 3499, 510, 11, 436, 500, 380, 534, 574, 50682, 50682, 411, 731, 11, 411, 27011, 13, 639, 307, 570, 983, 30, 708, 311, 264, 1154, 510, 30, 492, 2378, 380, 534, 51034, 51034, 5343, 264, 1379, 1901, 13, 286, 445, 8895, 337, 472, 3456, 13, 759, 286, 3847, 337, 1266, 2077, 11, 309, 311, 51308, 51308, 516, 281, 312, 445, 1364, 6239, 13, 1033, 13, 407, 510, 11, 729, 16295, 500, 380, 1939, 2836, 264, 51544, 51544], "temperature": 0.0, "avg_logprob": -0.1354513168334961, "compression_ratio": 1.6214953271028036, "no_speech_prob": 2.2818530851509422e-05}, {"id": 380, "seek": 243960, "start": 2439.6, "end": 2446.04, "text": " whole space, right? And that's the same problem which you would have with a normal autoencoder", "tokens": [50364, 1379, 1901, 11, 558, 30, 400, 300, 311, 264, 912, 1154, 597, 291, 576, 362, 365, 257, 2710, 8399, 22660, 19866, 50686, 50686, 1553, 341, 3034, 1478, 551, 11, 558, 30, 2022, 257, 2710, 8399, 22660, 19866, 11, 291, 500, 380, 362, 604, 733, 50976, 50976, 295, 3877, 11, 604, 733, 295, 7642, 5223, 294, 264, 10682, 1296, 819, 2793, 13, 2022, 51270, 51270, 257, 3034, 1478, 8399, 22660, 19866, 11, 321, 767, 747, 264, 1901, 293, 24825, 300, 264, 31565, 51512, 51512], "temperature": 0.0, "avg_logprob": -0.1441148421343635, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.4060732610232662e-05}, {"id": 381, "seek": 243960, "start": 2446.04, "end": 2451.8399999999997, "text": " without this variational thing, right? With a normal autoencoder, you don't have any kind", "tokens": [50364, 1379, 1901, 11, 558, 30, 400, 300, 311, 264, 912, 1154, 597, 291, 576, 362, 365, 257, 2710, 8399, 22660, 19866, 50686, 50686, 1553, 341, 3034, 1478, 551, 11, 558, 30, 2022, 257, 2710, 8399, 22660, 19866, 11, 291, 500, 380, 362, 604, 733, 50976, 50976, 295, 3877, 11, 604, 733, 295, 7642, 5223, 294, 264, 10682, 1296, 819, 2793, 13, 2022, 51270, 51270, 257, 3034, 1478, 8399, 22660, 19866, 11, 321, 767, 747, 264, 1901, 293, 24825, 300, 264, 31565, 51512, 51512], "temperature": 0.0, "avg_logprob": -0.1441148421343635, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.4060732610232662e-05}, {"id": 382, "seek": 243960, "start": 2451.8399999999997, "end": 2457.72, "text": " of structure, any kind of defined behavior in the regions between different points. With", "tokens": [50364, 1379, 1901, 11, 558, 30, 400, 300, 311, 264, 912, 1154, 597, 291, 576, 362, 365, 257, 2710, 8399, 22660, 19866, 50686, 50686, 1553, 341, 3034, 1478, 551, 11, 558, 30, 2022, 257, 2710, 8399, 22660, 19866, 11, 291, 500, 380, 362, 604, 733, 50976, 50976, 295, 3877, 11, 604, 733, 295, 7642, 5223, 294, 264, 10682, 1296, 819, 2793, 13, 2022, 51270, 51270, 257, 3034, 1478, 8399, 22660, 19866, 11, 321, 767, 747, 264, 1901, 293, 24825, 300, 264, 31565, 51512, 51512], "temperature": 0.0, "avg_logprob": -0.1441148421343635, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.4060732610232662e-05}, {"id": 383, "seek": 243960, "start": 2457.72, "end": 2462.56, "text": " a variational autoencoder, we actually take the space and enforce that the reconstruction", "tokens": [50364, 1379, 1901, 11, 558, 30, 400, 300, 311, 264, 912, 1154, 597, 291, 576, 362, 365, 257, 2710, 8399, 22660, 19866, 50686, 50686, 1553, 341, 3034, 1478, 551, 11, 558, 30, 2022, 257, 2710, 8399, 22660, 19866, 11, 291, 500, 380, 362, 604, 733, 50976, 50976, 295, 3877, 11, 604, 733, 295, 7642, 5223, 294, 264, 10682, 1296, 819, 2793, 13, 2022, 51270, 51270, 257, 3034, 1478, 8399, 22660, 19866, 11, 321, 767, 747, 264, 1901, 293, 24825, 300, 264, 31565, 51512, 51512], "temperature": 0.0, "avg_logprob": -0.1441148421343635, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.4060732610232662e-05}, {"id": 384, "seek": 246256, "start": 2462.56, "end": 2471.96, "text": " of the all these regions actually makes sense. Okay. So let's do some cute stuff. And then", "tokens": [50364, 295, 264, 439, 613, 10682, 767, 1669, 2020, 13, 1033, 13, 407, 718, 311, 360, 512, 4052, 1507, 13, 400, 550, 50834, 50834, 286, 669, 1096, 13, 1692, 11, 286, 445, 855, 291, 257, 1326, 27011, 13, 400, 370, 718, 311, 1888, 732, 295, 552, 13, 1171, 1365, 11, 51398, 51398, 718, 311, 1888, 1045, 293, 3180, 11, 597, 307, 516, 281, 312, 11, 718, 385, 855, 291, 510, 13, 407, 321, 1116, 411, 51737, 51737], "temperature": 0.0, "avg_logprob": -0.1443340228154109, "compression_ratio": 1.4615384615384615, "no_speech_prob": 3.759131141123362e-05}, {"id": 385, "seek": 246256, "start": 2471.96, "end": 2483.24, "text": " I am done. Here, I just show you a few digits. And so let's pick two of them. For example,", "tokens": [50364, 295, 264, 439, 613, 10682, 767, 1669, 2020, 13, 1033, 13, 407, 718, 311, 360, 512, 4052, 1507, 13, 400, 550, 50834, 50834, 286, 669, 1096, 13, 1692, 11, 286, 445, 855, 291, 257, 1326, 27011, 13, 400, 370, 718, 311, 1888, 732, 295, 552, 13, 1171, 1365, 11, 51398, 51398, 718, 311, 1888, 1045, 293, 3180, 11, 597, 307, 516, 281, 312, 11, 718, 385, 855, 291, 510, 13, 407, 321, 1116, 411, 51737, 51737], "temperature": 0.0, "avg_logprob": -0.1443340228154109, "compression_ratio": 1.4615384615384615, "no_speech_prob": 3.759131141123362e-05}, {"id": 386, "seek": 246256, "start": 2483.24, "end": 2490.02, "text": " let's pick three and eight, which is going to be, let me show you here. So we'd like", "tokens": [50364, 295, 264, 439, 613, 10682, 767, 1669, 2020, 13, 1033, 13, 407, 718, 311, 360, 512, 4052, 1507, 13, 400, 550, 50834, 50834, 286, 669, 1096, 13, 1692, 11, 286, 445, 855, 291, 257, 1326, 27011, 13, 400, 370, 718, 311, 1888, 732, 295, 552, 13, 1171, 1365, 11, 51398, 51398, 718, 311, 1888, 1045, 293, 3180, 11, 597, 307, 516, 281, 312, 11, 718, 385, 855, 291, 510, 13, 407, 321, 1116, 411, 51737, 51737], "temperature": 0.0, "avg_logprob": -0.1443340228154109, "compression_ratio": 1.4615384615384615, "no_speech_prob": 3.759131141123362e-05}, {"id": 387, "seek": 249002, "start": 2490.02, "end": 2497.88, "text": " to find an interpolation now between a five and a four. Okay. And this is my five reconstructed", "tokens": [50364, 281, 915, 364, 44902, 399, 586, 1296, 257, 1732, 293, 257, 1451, 13, 1033, 13, 400, 341, 307, 452, 1732, 31499, 292, 50757, 50757, 293, 527, 1451, 31499, 292, 13, 407, 498, 286, 2042, 257, 8213, 44902, 399, 294, 264, 48994, 1901, 51063, 51063, 293, 286, 550, 2845, 309, 281, 264, 979, 19866, 11, 321, 483, 341, 472, 13, 407, 264, 1732, 2170, 25778, 292, 666, 257, 51387, 51387, 1451, 13, 1664, 291, 536, 30, 29674, 13, 583, 309, 1542, 411, 12426, 13, 961, 311, 853, 281, 483, 746, 300, 10834, 51677, 51677], "temperature": 0.0, "avg_logprob": -0.0917153509039628, "compression_ratio": 1.6063348416289593, "no_speech_prob": 3.479464794509113e-05}, {"id": 388, "seek": 249002, "start": 2497.88, "end": 2504.0, "text": " and our four reconstructed. So if I perform a linear interpolation in the latent space", "tokens": [50364, 281, 915, 364, 44902, 399, 586, 1296, 257, 1732, 293, 257, 1451, 13, 1033, 13, 400, 341, 307, 452, 1732, 31499, 292, 50757, 50757, 293, 527, 1451, 31499, 292, 13, 407, 498, 286, 2042, 257, 8213, 44902, 399, 294, 264, 48994, 1901, 51063, 51063, 293, 286, 550, 2845, 309, 281, 264, 979, 19866, 11, 321, 483, 341, 472, 13, 407, 264, 1732, 2170, 25778, 292, 666, 257, 51387, 51387, 1451, 13, 1664, 291, 536, 30, 29674, 13, 583, 309, 1542, 411, 12426, 13, 961, 311, 853, 281, 483, 746, 300, 10834, 51677, 51677], "temperature": 0.0, "avg_logprob": -0.0917153509039628, "compression_ratio": 1.6063348416289593, "no_speech_prob": 3.479464794509113e-05}, {"id": 389, "seek": 249002, "start": 2504.0, "end": 2510.48, "text": " and I then send it to the decoder, we get this one. So the five gets morphed into a", "tokens": [50364, 281, 915, 364, 44902, 399, 586, 1296, 257, 1732, 293, 257, 1451, 13, 1033, 13, 400, 341, 307, 452, 1732, 31499, 292, 50757, 50757, 293, 527, 1451, 31499, 292, 13, 407, 498, 286, 2042, 257, 8213, 44902, 399, 294, 264, 48994, 1901, 51063, 51063, 293, 286, 550, 2845, 309, 281, 264, 979, 19866, 11, 321, 483, 341, 472, 13, 407, 264, 1732, 2170, 25778, 292, 666, 257, 51387, 51387, 1451, 13, 1664, 291, 536, 30, 29674, 13, 583, 309, 1542, 411, 12426, 13, 961, 311, 853, 281, 483, 746, 300, 10834, 51677, 51677], "temperature": 0.0, "avg_logprob": -0.0917153509039628, "compression_ratio": 1.6063348416289593, "no_speech_prob": 3.479464794509113e-05}, {"id": 390, "seek": 249002, "start": 2510.48, "end": 2516.28, "text": " four. Can you see? Slowly. But it looks like crap. Let's try to get something that stays", "tokens": [50364, 281, 915, 364, 44902, 399, 586, 1296, 257, 1732, 293, 257, 1451, 13, 1033, 13, 400, 341, 307, 452, 1732, 31499, 292, 50757, 50757, 293, 527, 1451, 31499, 292, 13, 407, 498, 286, 2042, 257, 8213, 44902, 399, 294, 264, 48994, 1901, 51063, 51063, 293, 286, 550, 2845, 309, 281, 264, 979, 19866, 11, 321, 483, 341, 472, 13, 407, 264, 1732, 2170, 25778, 292, 666, 257, 51387, 51387, 1451, 13, 1664, 291, 536, 30, 29674, 13, 583, 309, 1542, 411, 12426, 13, 961, 311, 853, 281, 483, 746, 300, 10834, 51677, 51677], "temperature": 0.0, "avg_logprob": -0.0917153509039628, "compression_ratio": 1.6063348416289593, "no_speech_prob": 3.479464794509113e-05}, {"id": 391, "seek": 251628, "start": 2516.28, "end": 2527.28, "text": " on the manifold. So let's get, for example, these three. So it's going to be number one.", "tokens": [50364, 322, 264, 47138, 13, 407, 718, 311, 483, 11, 337, 1365, 11, 613, 1045, 13, 407, 309, 311, 516, 281, 312, 1230, 472, 13, 50914, 50914, 5118, 472, 13, 400, 550, 718, 311, 584, 1310, 613, 3499, 510, 13, 407, 286, 360, 44902, 399, 295, 613, 1074, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.09669111289230048, "compression_ratio": 1.375, "no_speech_prob": 2.7275718821329065e-05}, {"id": 392, "seek": 251628, "start": 2527.28, "end": 2539.48, "text": " Number one. And then let's say maybe these 14 here. So I do interpolation of these guys", "tokens": [50364, 322, 264, 47138, 13, 407, 718, 311, 483, 11, 337, 1365, 11, 613, 1045, 13, 407, 309, 311, 516, 281, 312, 1230, 472, 13, 50914, 50914, 5118, 472, 13, 400, 550, 718, 311, 584, 1310, 613, 3499, 510, 13, 407, 286, 360, 44902, 399, 295, 613, 1074, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.09669111289230048, "compression_ratio": 1.375, "no_speech_prob": 2.7275718821329065e-05}, {"id": 393, "seek": 253948, "start": 2539.48, "end": 2547.48, "text": " here. You can see my autoencoder actually fixed those kind of issues here. And then", "tokens": [50364, 510, 13, 509, 393, 536, 452, 8399, 22660, 19866, 767, 6806, 729, 733, 295, 2663, 510, 13, 400, 550, 50764, 50764, 291, 393, 536, 586, 577, 264, 1045, 2170, 729, 707, 8819, 5395, 281, 574, 411, 364, 3180, 13, 1779, 30, 51140, 51140, 400, 370, 439, 295, 552, 574, 411, 733, 295, 10275, 11, 291, 458, 11, 341, 307, 733, 295, 257, 1045, 11, 733, 295, 51314, 51314, 257, 1045, 11, 257, 1045, 300, 3062, 364, 3180, 13, 1779, 13, 400, 370, 291, 393, 536, 577, 586, 538, 4494, 294, 264, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.17247547350431744, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.7043770134914666e-05}, {"id": 394, "seek": 253948, "start": 2547.48, "end": 2555.0, "text": " you can see now how the three gets those little edges closed to look like an eight. Right?", "tokens": [50364, 510, 13, 509, 393, 536, 452, 8399, 22660, 19866, 767, 6806, 729, 733, 295, 2663, 510, 13, 400, 550, 50764, 50764, 291, 393, 536, 586, 577, 264, 1045, 2170, 729, 707, 8819, 5395, 281, 574, 411, 364, 3180, 13, 1779, 30, 51140, 51140, 400, 370, 439, 295, 552, 574, 411, 733, 295, 10275, 11, 291, 458, 11, 341, 307, 733, 295, 257, 1045, 11, 733, 295, 51314, 51314, 257, 1045, 11, 257, 1045, 300, 3062, 364, 3180, 13, 1779, 13, 400, 370, 291, 393, 536, 577, 586, 538, 4494, 294, 264, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.17247547350431744, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.7043770134914666e-05}, {"id": 395, "seek": 253948, "start": 2555.0, "end": 2558.48, "text": " And so all of them look like kind of legit, you know, this is kind of a three, kind of", "tokens": [50364, 510, 13, 509, 393, 536, 452, 8399, 22660, 19866, 767, 6806, 729, 733, 295, 2663, 510, 13, 400, 550, 50764, 50764, 291, 393, 536, 586, 577, 264, 1045, 2170, 729, 707, 8819, 5395, 281, 574, 411, 364, 3180, 13, 1779, 30, 51140, 51140, 400, 370, 439, 295, 552, 574, 411, 733, 295, 10275, 11, 291, 458, 11, 341, 307, 733, 295, 257, 1045, 11, 733, 295, 51314, 51314, 257, 1045, 11, 257, 1045, 300, 3062, 364, 3180, 13, 1779, 13, 400, 370, 291, 393, 536, 577, 586, 538, 4494, 294, 264, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.17247547350431744, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.7043770134914666e-05}, {"id": 396, "seek": 253948, "start": 2558.48, "end": 2563.88, "text": " a three, a three that became an eight. Right. And so you can see how now by walking in the", "tokens": [50364, 510, 13, 509, 393, 536, 452, 8399, 22660, 19866, 767, 6806, 729, 733, 295, 2663, 510, 13, 400, 550, 50764, 50764, 291, 393, 536, 586, 577, 264, 1045, 2170, 729, 707, 8819, 5395, 281, 574, 411, 364, 3180, 13, 1779, 30, 51140, 51140, 400, 370, 439, 295, 552, 574, 411, 733, 295, 10275, 11, 291, 458, 11, 341, 307, 733, 295, 257, 1045, 11, 733, 295, 51314, 51314, 257, 1045, 11, 257, 1045, 300, 3062, 364, 3180, 13, 1779, 13, 400, 370, 291, 393, 536, 577, 586, 538, 4494, 294, 264, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.17247547350431744, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.7043770134914666e-05}, {"id": 397, "seek": 256388, "start": 2563.88, "end": 2570.44, "text": " latent space, we get to reconstruct things that look legit in the input space. Right.", "tokens": [50364, 48994, 1901, 11, 321, 483, 281, 31499, 721, 300, 574, 10275, 294, 264, 4846, 1901, 13, 1779, 13, 50692, 50692, 639, 576, 362, 1128, 2732, 365, 257, 2710, 8399, 22660, 19866, 13, 6288, 11, 286, 478, 516, 281, 855, 291, 51022, 51022, 257, 1326, 1481, 10290, 295, 264, 12240, 29432, 295, 264, 1355, 337, 341, 3847, 8399, 22660, 19866, 13, 407, 51482, 51482, 510, 286, 445, 855, 291, 257, 5765, 295, 264, 12240, 29432, 295, 11, 291, 458, 11, 264, 1500, 28872, 13, 400, 550, 286, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.10988998413085938, "compression_ratio": 1.628440366972477, "no_speech_prob": 2.5213528260792373e-06}, {"id": 398, "seek": 256388, "start": 2570.44, "end": 2577.04, "text": " This would have never worked with a normal autoencoder. Finally, I'm going to show you", "tokens": [50364, 48994, 1901, 11, 321, 483, 281, 31499, 721, 300, 574, 10275, 294, 264, 4846, 1901, 13, 1779, 13, 50692, 50692, 639, 576, 362, 1128, 2732, 365, 257, 2710, 8399, 22660, 19866, 13, 6288, 11, 286, 478, 516, 281, 855, 291, 51022, 51022, 257, 1326, 1481, 10290, 295, 264, 12240, 29432, 295, 264, 1355, 337, 341, 3847, 8399, 22660, 19866, 13, 407, 51482, 51482, 510, 286, 445, 855, 291, 257, 5765, 295, 264, 12240, 29432, 295, 11, 291, 458, 11, 264, 1500, 28872, 13, 400, 550, 286, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.10988998413085938, "compression_ratio": 1.628440366972477, "no_speech_prob": 2.5213528260792373e-06}, {"id": 399, "seek": 256388, "start": 2577.04, "end": 2586.2400000000002, "text": " a few nice representation of the embeddings of the means for this train autoencoder. So", "tokens": [50364, 48994, 1901, 11, 321, 483, 281, 31499, 721, 300, 574, 10275, 294, 264, 4846, 1901, 13, 1779, 13, 50692, 50692, 639, 576, 362, 1128, 2732, 365, 257, 2710, 8399, 22660, 19866, 13, 6288, 11, 286, 478, 516, 281, 855, 291, 51022, 51022, 257, 1326, 1481, 10290, 295, 264, 12240, 29432, 295, 264, 1355, 337, 341, 3847, 8399, 22660, 19866, 13, 407, 51482, 51482, 510, 286, 445, 855, 291, 257, 5765, 295, 264, 12240, 29432, 295, 11, 291, 458, 11, 264, 1500, 28872, 13, 400, 550, 286, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.10988998413085938, "compression_ratio": 1.628440366972477, "no_speech_prob": 2.5213528260792373e-06}, {"id": 400, "seek": 256388, "start": 2586.2400000000002, "end": 2593.2400000000002, "text": " here I just show you a collection of the embeddings of, you know, the test dataset. And then I", "tokens": [50364, 48994, 1901, 11, 321, 483, 281, 31499, 721, 300, 574, 10275, 294, 264, 4846, 1901, 13, 1779, 13, 50692, 50692, 639, 576, 362, 1128, 2732, 365, 257, 2710, 8399, 22660, 19866, 13, 6288, 11, 286, 478, 516, 281, 855, 291, 51022, 51022, 257, 1326, 1481, 10290, 295, 264, 12240, 29432, 295, 264, 1355, 337, 341, 3847, 8399, 22660, 19866, 13, 407, 51482, 51482, 510, 286, 445, 855, 291, 257, 5765, 295, 264, 12240, 29432, 295, 11, 291, 458, 11, 264, 1500, 28872, 13, 400, 550, 286, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.10988998413085938, "compression_ratio": 1.628440366972477, "no_speech_prob": 2.5213528260792373e-06}, {"id": 401, "seek": 259324, "start": 2593.24, "end": 2599.9599999999996, "text": " perform a dimensionality reduction. And then I show you how the encoder clusters all the", "tokens": [50364, 2042, 257, 10139, 1860, 11004, 13, 400, 550, 286, 855, 291, 577, 264, 2058, 19866, 23313, 439, 264, 50700, 50700, 1355, 294, 819, 10682, 294, 264, 48994, 1901, 13, 400, 370, 510, 307, 437, 291, 483, 562, 291, 3847, 51090, 51090, 341, 3034, 1478, 8399, 22660, 19866, 13, 407, 341, 307, 264, 2863, 562, 264, 3209, 307, 406, 8895, 13, 51332, 51332, 509, 393, 920, 536, 11, 291, 458, 11, 23313, 295, 27011, 13, 583, 550, 382, 291, 1066, 3097, 11, 731, 11, 412, 1935, 11, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.11786923938327365, "compression_ratio": 1.6425339366515836, "no_speech_prob": 6.438240234274417e-06}, {"id": 402, "seek": 259324, "start": 2599.9599999999996, "end": 2607.7599999999998, "text": " means in different regions in the latent space. And so here is what you get when you train", "tokens": [50364, 2042, 257, 10139, 1860, 11004, 13, 400, 550, 286, 855, 291, 577, 264, 2058, 19866, 23313, 439, 264, 50700, 50700, 1355, 294, 819, 10682, 294, 264, 48994, 1901, 13, 400, 370, 510, 307, 437, 291, 483, 562, 291, 3847, 51090, 51090, 341, 3034, 1478, 8399, 22660, 19866, 13, 407, 341, 307, 264, 2863, 562, 264, 3209, 307, 406, 8895, 13, 51332, 51332, 509, 393, 920, 536, 11, 291, 458, 11, 23313, 295, 27011, 13, 583, 550, 382, 291, 1066, 3097, 11, 731, 11, 412, 1935, 11, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.11786923938327365, "compression_ratio": 1.6425339366515836, "no_speech_prob": 6.438240234274417e-06}, {"id": 403, "seek": 259324, "start": 2607.7599999999998, "end": 2612.6, "text": " this variational autoencoder. So this is the beginning when the network is not trained.", "tokens": [50364, 2042, 257, 10139, 1860, 11004, 13, 400, 550, 286, 855, 291, 577, 264, 2058, 19866, 23313, 439, 264, 50700, 50700, 1355, 294, 819, 10682, 294, 264, 48994, 1901, 13, 400, 370, 510, 307, 437, 291, 483, 562, 291, 3847, 51090, 51090, 341, 3034, 1478, 8399, 22660, 19866, 13, 407, 341, 307, 264, 2863, 562, 264, 3209, 307, 406, 8895, 13, 51332, 51332, 509, 393, 920, 536, 11, 291, 458, 11, 23313, 295, 27011, 13, 583, 550, 382, 291, 1066, 3097, 11, 731, 11, 412, 1935, 11, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.11786923938327365, "compression_ratio": 1.6425339366515836, "no_speech_prob": 6.438240234274417e-06}, {"id": 404, "seek": 259324, "start": 2612.6, "end": 2618.64, "text": " You can still see, you know, clusters of digits. But then as you keep training, well, at least,", "tokens": [50364, 2042, 257, 10139, 1860, 11004, 13, 400, 550, 286, 855, 291, 577, 264, 2058, 19866, 23313, 439, 264, 50700, 50700, 1355, 294, 819, 10682, 294, 264, 48994, 1901, 13, 400, 370, 510, 307, 437, 291, 483, 562, 291, 3847, 51090, 51090, 341, 3034, 1478, 8399, 22660, 19866, 13, 407, 341, 307, 264, 2863, 562, 264, 3209, 307, 406, 8895, 13, 51332, 51332, 509, 393, 920, 536, 11, 291, 458, 11, 23313, 295, 27011, 13, 583, 550, 382, 291, 1066, 3097, 11, 731, 11, 412, 1935, 11, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.11786923938327365, "compression_ratio": 1.6425339366515836, "no_speech_prob": 6.438240234274417e-06}, {"id": 405, "seek": 261864, "start": 2618.64, "end": 2624.72, "text": " you know, after five epochs, you get these groups to be, you know, separated. And then", "tokens": [50364, 291, 458, 11, 934, 1732, 30992, 28346, 11, 291, 483, 613, 3935, 281, 312, 11, 291, 458, 11, 12005, 13, 400, 550, 50668, 50668, 286, 519, 498, 291, 1066, 3097, 544, 11, 291, 820, 362, 411, 544, 14634, 13, 1033, 13, 407, 510, 286, 478, 51096, 51096, 1936, 884, 264, 4997, 644, 13, 286, 483, 439, 264, 1355, 13, 407, 452, 2316, 23930, 1783, 2385, 11, 2992, 293, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.14351125558217367, "compression_ratio": 1.4777777777777779, "no_speech_prob": 4.461814387468621e-05}, {"id": 406, "seek": 261864, "start": 2624.72, "end": 2633.2799999999997, "text": " I think if you keep training more, you should have like more separation. Okay. So here I'm", "tokens": [50364, 291, 458, 11, 934, 1732, 30992, 28346, 11, 291, 483, 613, 3935, 281, 312, 11, 291, 458, 11, 12005, 13, 400, 550, 50668, 50668, 286, 519, 498, 291, 1066, 3097, 544, 11, 291, 820, 362, 411, 544, 14634, 13, 1033, 13, 407, 510, 286, 478, 51096, 51096, 1936, 884, 264, 4997, 644, 13, 286, 483, 439, 264, 1355, 13, 407, 452, 2316, 23930, 1783, 2385, 11, 2992, 293, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.14351125558217367, "compression_ratio": 1.4777777777777779, "no_speech_prob": 4.461814387468621e-05}, {"id": 407, "seek": 261864, "start": 2633.2799999999997, "end": 2641.8399999999997, "text": " basically doing the testing part. I get all the means. So my model outputs X hat, mu and", "tokens": [50364, 291, 458, 11, 934, 1732, 30992, 28346, 11, 291, 483, 613, 3935, 281, 312, 11, 291, 458, 11, 12005, 13, 400, 550, 50668, 50668, 286, 519, 498, 291, 1066, 3097, 544, 11, 291, 820, 362, 411, 544, 14634, 13, 1033, 13, 407, 510, 286, 478, 51096, 51096, 1936, 884, 264, 4997, 644, 13, 286, 483, 439, 264, 1355, 13, 407, 452, 2316, 23930, 1783, 2385, 11, 2992, 293, 51524, 51524], "temperature": 0.0, "avg_logprob": -0.14351125558217367, "compression_ratio": 1.4777777777777779, "no_speech_prob": 4.461814387468621e-05}, {"id": 408, "seek": 264184, "start": 2641.84, "end": 2649.44, "text": " log bar. And so my means, I append them, I append all my mu's into this mean list. I", "tokens": [50364, 3565, 2159, 13, 400, 370, 452, 1355, 11, 286, 34116, 552, 11, 286, 34116, 439, 452, 2992, 311, 666, 341, 914, 1329, 13, 286, 50744, 50744, 34116, 439, 264, 3565, 10228, 294, 341, 3565, 10228, 1329, 13, 400, 286, 34116, 439, 264, 288, 311, 281, 341, 16949, 1329, 51080, 51080, 1830, 264, 4997, 644, 13, 407, 341, 307, 4997, 13, 400, 370, 286, 362, 411, 257, 1329, 510, 295, 14211, 11, 597, 51452, 51452], "temperature": 0.0, "avg_logprob": -0.18234417313023618, "compression_ratio": 1.763157894736842, "no_speech_prob": 4.74287080578506e-05}, {"id": 409, "seek": 264184, "start": 2649.44, "end": 2656.1600000000003, "text": " append all the log bars in this log bars list. And I append all the y's to this labels list", "tokens": [50364, 3565, 2159, 13, 400, 370, 452, 1355, 11, 286, 34116, 552, 11, 286, 34116, 439, 452, 2992, 311, 666, 341, 914, 1329, 13, 286, 50744, 50744, 34116, 439, 264, 3565, 10228, 294, 341, 3565, 10228, 1329, 13, 400, 286, 34116, 439, 264, 288, 311, 281, 341, 16949, 1329, 51080, 51080, 1830, 264, 4997, 644, 13, 407, 341, 307, 4997, 13, 400, 370, 286, 362, 411, 257, 1329, 510, 295, 14211, 11, 597, 51452, 51452], "temperature": 0.0, "avg_logprob": -0.18234417313023618, "compression_ratio": 1.763157894736842, "no_speech_prob": 4.74287080578506e-05}, {"id": 410, "seek": 264184, "start": 2656.1600000000003, "end": 2663.6000000000004, "text": " during the testing part. So this is testing. And so I have like a list here of codes, which", "tokens": [50364, 3565, 2159, 13, 400, 370, 452, 1355, 11, 286, 34116, 552, 11, 286, 34116, 439, 452, 2992, 311, 666, 341, 914, 1329, 13, 286, 50744, 50744, 34116, 439, 264, 3565, 10228, 294, 341, 3565, 10228, 1329, 13, 400, 286, 34116, 439, 264, 288, 311, 281, 341, 16949, 1329, 51080, 51080, 1830, 264, 4997, 644, 13, 407, 341, 307, 4997, 13, 400, 370, 286, 362, 411, 257, 1329, 510, 295, 14211, 11, 597, 51452, 51452], "temperature": 0.0, "avg_logprob": -0.18234417313023618, "compression_ratio": 1.763157894736842, "no_speech_prob": 4.74287080578506e-05}, {"id": 411, "seek": 266360, "start": 2663.6, "end": 2672.2, "text": " is I have the mu, log bar, and then the y's. So here later on, I put those lists inside", "tokens": [50364, 307, 286, 362, 264, 2992, 11, 3565, 2159, 11, 293, 550, 264, 288, 311, 13, 407, 510, 1780, 322, 11, 286, 829, 729, 14511, 1854, 50794, 50794, 452, 25890, 13, 400, 550, 1780, 510, 2507, 11, 286, 14722, 257, 10139, 1860, 11004, 337, 30992, 339, 51330, 51330, 4018, 11, 30992, 339, 1732, 11, 293, 30992, 339, 1266, 13, 407, 286, 764, 341, 314, 32481, 40, 11, 597, 307, 257, 6532, 337, 12245, 264, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.18697904611562755, "compression_ratio": 1.4943820224719102, "no_speech_prob": 2.8395992558216676e-05}, {"id": 412, "seek": 266360, "start": 2672.2, "end": 2682.92, "text": " my dictionary. And then later here below, I compute a dimensionality reduction for epoch", "tokens": [50364, 307, 286, 362, 264, 2992, 11, 3565, 2159, 11, 293, 550, 264, 288, 311, 13, 407, 510, 1780, 322, 11, 286, 829, 729, 14511, 1854, 50794, 50794, 452, 25890, 13, 400, 550, 1780, 510, 2507, 11, 286, 14722, 257, 10139, 1860, 11004, 337, 30992, 339, 51330, 51330, 4018, 11, 30992, 339, 1732, 11, 293, 30992, 339, 1266, 13, 407, 286, 764, 341, 314, 32481, 40, 11, 597, 307, 257, 6532, 337, 12245, 264, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.18697904611562755, "compression_ratio": 1.4943820224719102, "no_speech_prob": 2.8395992558216676e-05}, {"id": 413, "seek": 266360, "start": 2682.92, "end": 2691.0, "text": " zero, epoch five, and epoch 10. So I use this TSNI, which is a technique for reducing the", "tokens": [50364, 307, 286, 362, 264, 2992, 11, 3565, 2159, 11, 293, 550, 264, 288, 311, 13, 407, 510, 1780, 322, 11, 286, 829, 729, 14511, 1854, 50794, 50794, 452, 25890, 13, 400, 550, 1780, 510, 2507, 11, 286, 14722, 257, 10139, 1860, 11004, 337, 30992, 339, 51330, 51330, 4018, 11, 30992, 339, 1732, 11, 293, 30992, 339, 1266, 13, 407, 286, 764, 341, 314, 32481, 40, 11, 597, 307, 257, 6532, 337, 12245, 264, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.18697904611562755, "compression_ratio": 1.4943820224719102, "no_speech_prob": 2.8395992558216676e-05}, {"id": 414, "seek": 269100, "start": 2691.0, "end": 2700.28, "text": " dimensions of the codes, which are 20. Right now the dimensionality is 20. So I fit, I", "tokens": [50364, 12819, 295, 264, 14211, 11, 597, 366, 945, 13, 1779, 586, 264, 10139, 1860, 307, 945, 13, 407, 286, 3318, 11, 286, 50828, 50828, 483, 452, 1783, 307, 516, 281, 312, 11, 718, 311, 584, 264, 700, 4714, 6677, 11, 700, 4714, 10938, 51080, 51080, 295, 264, 1355, 13, 400, 550, 286, 483, 613, 462, 311, 11, 597, 366, 1936, 257, 568, 35, 22743, 6063, 295, 613, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.19928182682520906, "compression_ratio": 1.5227272727272727, "no_speech_prob": 3.7624551623594016e-05}, {"id": 415, "seek": 269100, "start": 2700.28, "end": 2705.32, "text": " get my X is going to be, let's say the first thousand components, first thousand samples", "tokens": [50364, 12819, 295, 264, 14211, 11, 597, 366, 945, 13, 1779, 586, 264, 10139, 1860, 307, 945, 13, 407, 286, 3318, 11, 286, 50828, 50828, 483, 452, 1783, 307, 516, 281, 312, 11, 718, 311, 584, 264, 700, 4714, 6677, 11, 700, 4714, 10938, 51080, 51080, 295, 264, 1355, 13, 400, 550, 286, 483, 613, 462, 311, 11, 597, 366, 1936, 257, 568, 35, 22743, 6063, 295, 613, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.19928182682520906, "compression_ratio": 1.5227272727272727, "no_speech_prob": 3.7624551623594016e-05}, {"id": 416, "seek": 269100, "start": 2705.32, "end": 2714.32, "text": " of the means. And then I get these E's, which are basically a 2D projection somehow of these", "tokens": [50364, 12819, 295, 264, 14211, 11, 597, 366, 945, 13, 1779, 586, 264, 10139, 1860, 307, 945, 13, 407, 286, 3318, 11, 286, 50828, 50828, 483, 452, 1783, 307, 516, 281, 312, 11, 718, 311, 584, 264, 700, 4714, 6677, 11, 700, 4714, 10938, 51080, 51080, 295, 264, 1355, 13, 400, 550, 286, 483, 613, 462, 311, 11, 597, 366, 1936, 257, 568, 35, 22743, 6063, 295, 613, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.19928182682520906, "compression_ratio": 1.5227272727272727, "no_speech_prob": 3.7624551623594016e-05}, {"id": 417, "seek": 271432, "start": 2714.32, "end": 2723.36, "text": " 20 dimensional mu's. And then I show you in this chart here, how these 2D projections,", "tokens": [50364, 945, 18795, 2992, 311, 13, 400, 550, 286, 855, 291, 294, 341, 6927, 510, 11, 577, 613, 568, 35, 32371, 11, 50816, 50816, 436, 574, 412, 30992, 339, 4018, 949, 3097, 264, 3209, 11, 570, 341, 472, 307, 949, 264, 700, 51104, 51104, 3097, 30992, 339, 11, 293, 550, 412, 30992, 339, 1732, 13, 400, 291, 393, 536, 577, 264, 3209, 2170, 439, 341, 51458, 51458], "temperature": 0.0, "avg_logprob": -0.12265930456273696, "compression_ratio": 1.5783132530120483, "no_speech_prob": 1.6959651475190185e-05}, {"id": 418, "seek": 271432, "start": 2723.36, "end": 2729.1200000000003, "text": " they look at epoch zero before training the network, because this one is before the first", "tokens": [50364, 945, 18795, 2992, 311, 13, 400, 550, 286, 855, 291, 294, 341, 6927, 510, 11, 577, 613, 568, 35, 32371, 11, 50816, 50816, 436, 574, 412, 30992, 339, 4018, 949, 3097, 264, 3209, 11, 570, 341, 472, 307, 949, 264, 700, 51104, 51104, 3097, 30992, 339, 11, 293, 550, 412, 30992, 339, 1732, 13, 400, 291, 393, 536, 577, 264, 3209, 2170, 439, 341, 51458, 51458], "temperature": 0.0, "avg_logprob": -0.12265930456273696, "compression_ratio": 1.5783132530120483, "no_speech_prob": 1.6959651475190185e-05}, {"id": 419, "seek": 271432, "start": 2729.1200000000003, "end": 2736.2000000000003, "text": " training epoch, and then at epoch five. And you can see how the network gets all this", "tokens": [50364, 945, 18795, 2992, 311, 13, 400, 550, 286, 855, 291, 294, 341, 6927, 510, 11, 577, 613, 568, 35, 32371, 11, 50816, 50816, 436, 574, 412, 30992, 339, 4018, 949, 3097, 264, 3209, 11, 570, 341, 472, 307, 949, 264, 700, 51104, 51104, 3097, 30992, 339, 11, 293, 550, 412, 30992, 339, 1732, 13, 400, 291, 393, 536, 577, 264, 3209, 2170, 439, 341, 51458, 51458], "temperature": 0.0, "avg_logprob": -0.12265930456273696, "compression_ratio": 1.5783132530120483, "no_speech_prob": 1.6959651475190185e-05}, {"id": 420, "seek": 273620, "start": 2736.2, "end": 2745.3199999999997, "text": " mass here to be kind of more nicely put. Here, I didn't visualize the variances. I'm thinking", "tokens": [50364, 2758, 510, 281, 312, 733, 295, 544, 9594, 829, 13, 1692, 11, 286, 994, 380, 23273, 264, 1374, 21518, 13, 286, 478, 1953, 50820, 50820, 1968, 286, 393, 11, 498, 286, 478, 1075, 281, 360, 300, 382, 731, 11, 286, 478, 406, 988, 13, 407, 1184, 295, 613, 2793, 2906, 51148, 51148, 264, 4914, 295, 264, 914, 934, 3097, 264, 12990, 295, 264, 2058, 19866, 13, 286, 2378, 380, 10379, 51432, 51432, 264, 1859, 300, 613, 1355, 366, 767, 1940, 13, 1033, 13, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.16041102354553924, "compression_ratio": 1.608695652173913, "no_speech_prob": 2.7531830710358918e-05}, {"id": 421, "seek": 273620, "start": 2745.3199999999997, "end": 2751.8799999999997, "text": " whether I can, if I'm able to do that as well, I'm not sure. So each of these points represent", "tokens": [50364, 2758, 510, 281, 312, 733, 295, 544, 9594, 829, 13, 1692, 11, 286, 994, 380, 23273, 264, 1374, 21518, 13, 286, 478, 1953, 50820, 50820, 1968, 286, 393, 11, 498, 286, 478, 1075, 281, 360, 300, 382, 731, 11, 286, 478, 406, 988, 13, 407, 1184, 295, 613, 2793, 2906, 51148, 51148, 264, 4914, 295, 264, 914, 934, 3097, 264, 12990, 295, 264, 2058, 19866, 13, 286, 2378, 380, 10379, 51432, 51432, 264, 1859, 300, 613, 1355, 366, 767, 1940, 13, 1033, 13, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.16041102354553924, "compression_ratio": 1.608695652173913, "no_speech_prob": 2.7531830710358918e-05}, {"id": 422, "seek": 273620, "start": 2751.8799999999997, "end": 2757.56, "text": " the location of the mean after training the variation of the encoder. I haven't represented", "tokens": [50364, 2758, 510, 281, 312, 733, 295, 544, 9594, 829, 13, 1692, 11, 286, 994, 380, 23273, 264, 1374, 21518, 13, 286, 478, 1953, 50820, 50820, 1968, 286, 393, 11, 498, 286, 478, 1075, 281, 360, 300, 382, 731, 11, 286, 478, 406, 988, 13, 407, 1184, 295, 613, 2793, 2906, 51148, 51148, 264, 4914, 295, 264, 914, 934, 3097, 264, 12990, 295, 264, 2058, 19866, 13, 286, 2378, 380, 10379, 51432, 51432, 264, 1859, 300, 613, 1355, 366, 767, 1940, 13, 1033, 13, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.16041102354553924, "compression_ratio": 1.608695652173913, "no_speech_prob": 2.7531830710358918e-05}, {"id": 423, "seek": 273620, "start": 2757.56, "end": 2762.8799999999997, "text": " the area that these means are actually taking. Okay.", "tokens": [50364, 2758, 510, 281, 312, 733, 295, 544, 9594, 829, 13, 1692, 11, 286, 994, 380, 23273, 264, 1374, 21518, 13, 286, 478, 1953, 50820, 50820, 1968, 286, 393, 11, 498, 286, 478, 1075, 281, 360, 300, 382, 731, 11, 286, 478, 406, 988, 13, 407, 1184, 295, 613, 2793, 2906, 51148, 51148, 264, 4914, 295, 264, 914, 934, 3097, 264, 12990, 295, 264, 2058, 19866, 13, 286, 2378, 380, 10379, 51432, 51432, 264, 1859, 300, 613, 1355, 366, 767, 1940, 13, 1033, 13, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.16041102354553924, "compression_ratio": 1.608695652173913, "no_speech_prob": 2.7531830710358918e-05}, {"id": 424, "seek": 276288, "start": 2762.88, "end": 2768.76, "text": " Aren't the means supposed to be random at epoch zero?", "tokens": [50364, 15464, 380, 264, 1355, 3442, 281, 312, 4974, 412, 30992, 339, 4018, 30, 50658, 50658, 440, 4974, 1287, 307, 294, 264, 2058, 19866, 11, 558, 30, 583, 550, 291, 920, 3154, 281, 264, 2058, 19866, 293, 729, 51070, 51070, 4846, 27011, 13, 407, 264, 4846, 27011, 11, 439, 264, 2306, 366, 733, 295, 2531, 11, 558, 30, 407, 498, 291, 51418, 51418, 2042, 257, 4974, 9887, 295, 729, 14138, 1237, 5883, 18875, 11, 291, 434, 516, 281, 362, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.10894776553642459, "compression_ratio": 1.6548223350253808, "no_speech_prob": 9.815916200750507e-06}, {"id": 425, "seek": 276288, "start": 2768.76, "end": 2777.0, "text": " The randomness is in the encoder, right? But then you still feed to the encoder and those", "tokens": [50364, 15464, 380, 264, 1355, 3442, 281, 312, 4974, 412, 30992, 339, 4018, 30, 50658, 50658, 440, 4974, 1287, 307, 294, 264, 2058, 19866, 11, 558, 30, 583, 550, 291, 920, 3154, 281, 264, 2058, 19866, 293, 729, 51070, 51070, 4846, 27011, 13, 407, 264, 4846, 27011, 11, 439, 264, 2306, 366, 733, 295, 2531, 11, 558, 30, 407, 498, 291, 51418, 51418, 2042, 257, 4974, 9887, 295, 729, 14138, 1237, 5883, 18875, 11, 291, 434, 516, 281, 362, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.10894776553642459, "compression_ratio": 1.6548223350253808, "no_speech_prob": 9.815916200750507e-06}, {"id": 426, "seek": 276288, "start": 2777.0, "end": 2783.96, "text": " input digits. So the input digits, all the ones are kind of similar, right? So if you", "tokens": [50364, 15464, 380, 264, 1355, 3442, 281, 312, 4974, 412, 30992, 339, 4018, 30, 50658, 50658, 440, 4974, 1287, 307, 294, 264, 2058, 19866, 11, 558, 30, 583, 550, 291, 920, 3154, 281, 264, 2058, 19866, 293, 729, 51070, 51070, 4846, 27011, 13, 407, 264, 4846, 27011, 11, 439, 264, 2306, 366, 733, 295, 2531, 11, 558, 30, 407, 498, 291, 51418, 51418, 2042, 257, 4974, 9887, 295, 729, 14138, 1237, 5883, 18875, 11, 291, 434, 516, 281, 362, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.10894776553642459, "compression_ratio": 1.6548223350253808, "no_speech_prob": 9.815916200750507e-06}, {"id": 427, "seek": 276288, "start": 2783.96, "end": 2790.28, "text": " perform a random transformation of those similarly looking initial vectors, you're going to have", "tokens": [50364, 15464, 380, 264, 1355, 3442, 281, 312, 4974, 412, 30992, 339, 4018, 30, 50658, 50658, 440, 4974, 1287, 307, 294, 264, 2058, 19866, 11, 558, 30, 583, 550, 291, 920, 3154, 281, 264, 2058, 19866, 293, 729, 51070, 51070, 4846, 27011, 13, 407, 264, 4846, 27011, 11, 439, 264, 2306, 366, 733, 295, 2531, 11, 558, 30, 407, 498, 291, 51418, 51418, 2042, 257, 4974, 9887, 295, 729, 14138, 1237, 5883, 18875, 11, 291, 434, 516, 281, 362, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.10894776553642459, "compression_ratio": 1.6548223350253808, "no_speech_prob": 9.815916200750507e-06}, {"id": 428, "seek": 279028, "start": 2790.28, "end": 2799.32, "text": " similarly looking transformed versions. But then they are not necessarily grouped altogether.", "tokens": [50364, 14138, 1237, 16894, 9606, 13, 583, 550, 436, 366, 406, 4725, 41877, 19051, 13, 50816, 50816, 1743, 881, 295, 552, 366, 11, 337, 1365, 11, 718, 311, 584, 613, 366, 2306, 13, 961, 385, 1261, 322, 264, 2017, 51110, 51110, 2159, 370, 321, 393, 536, 437, 341, 1507, 307, 13, 407, 718, 311, 584, 613, 366, 264, 35193, 11, 613, 670, 510, 13, 51431, 51431, 407, 439, 35193, 574, 411, 11, 436, 439, 574, 2531, 13, 7504, 11, 754, 257, 4974, 22743, 295, 729, 51759, 51759], "temperature": 0.0, "avg_logprob": -0.13305630467154764, "compression_ratio": 1.6409090909090909, "no_speech_prob": 1.3623754966829438e-05}, {"id": 429, "seek": 279028, "start": 2799.32, "end": 2805.2000000000003, "text": " Like most of them are, for example, let's say these are ones. Let me turn on the color", "tokens": [50364, 14138, 1237, 16894, 9606, 13, 583, 550, 436, 366, 406, 4725, 41877, 19051, 13, 50816, 50816, 1743, 881, 295, 552, 366, 11, 337, 1365, 11, 718, 311, 584, 613, 366, 2306, 13, 961, 385, 1261, 322, 264, 2017, 51110, 51110, 2159, 370, 321, 393, 536, 437, 341, 1507, 307, 13, 407, 718, 311, 584, 613, 366, 264, 35193, 11, 613, 670, 510, 13, 51431, 51431, 407, 439, 35193, 574, 411, 11, 436, 439, 574, 2531, 13, 7504, 11, 754, 257, 4974, 22743, 295, 729, 51759, 51759], "temperature": 0.0, "avg_logprob": -0.13305630467154764, "compression_ratio": 1.6409090909090909, "no_speech_prob": 1.3623754966829438e-05}, {"id": 430, "seek": 279028, "start": 2805.2000000000003, "end": 2811.6200000000003, "text": " bar so we can see what this stuff is. So let's say these are the zeros, these over here.", "tokens": [50364, 14138, 1237, 16894, 9606, 13, 583, 550, 436, 366, 406, 4725, 41877, 19051, 13, 50816, 50816, 1743, 881, 295, 552, 366, 11, 337, 1365, 11, 718, 311, 584, 613, 366, 2306, 13, 961, 385, 1261, 322, 264, 2017, 51110, 51110, 2159, 370, 321, 393, 536, 437, 341, 1507, 307, 13, 407, 718, 311, 584, 613, 366, 264, 35193, 11, 613, 670, 510, 13, 51431, 51431, 407, 439, 35193, 574, 411, 11, 436, 439, 574, 2531, 13, 7504, 11, 754, 257, 4974, 22743, 295, 729, 51759, 51759], "temperature": 0.0, "avg_logprob": -0.13305630467154764, "compression_ratio": 1.6409090909090909, "no_speech_prob": 1.3623754966829438e-05}, {"id": 431, "seek": 279028, "start": 2811.6200000000003, "end": 2818.1800000000003, "text": " So all zeros look like, they all look similar. Therefore, even a random projection of those", "tokens": [50364, 14138, 1237, 16894, 9606, 13, 583, 550, 436, 366, 406, 4725, 41877, 19051, 13, 50816, 50816, 1743, 881, 295, 552, 366, 11, 337, 1365, 11, 718, 311, 584, 613, 366, 2306, 13, 961, 385, 1261, 322, 264, 2017, 51110, 51110, 2159, 370, 321, 393, 536, 437, 341, 1507, 307, 13, 407, 718, 311, 584, 613, 366, 264, 35193, 11, 613, 670, 510, 13, 51431, 51431, 407, 439, 35193, 574, 411, 11, 436, 439, 574, 2531, 13, 7504, 11, 754, 257, 4974, 22743, 295, 729, 51759, 51759], "temperature": 0.0, "avg_logprob": -0.13305630467154764, "compression_ratio": 1.6409090909090909, "no_speech_prob": 1.3623754966829438e-05}, {"id": 432, "seek": 281818, "start": 2818.18, "end": 2824.2, "text": " zeros will all be kind of together. What you can see instead is going to be this purple", "tokens": [50364, 35193, 486, 439, 312, 733, 295, 1214, 13, 708, 291, 393, 536, 2602, 307, 516, 281, 312, 341, 9656, 50665, 50665, 307, 439, 3974, 926, 11, 558, 30, 407, 309, 1355, 264, 3464, 11, 456, 366, 588, 867, 2098, 295, 6316, 50925, 50925, 257, 1451, 13, 8734, 24157, 264, 1192, 11, 1580, 1177, 380, 13, 407, 498, 291, 536, 322, 264, 558, 1011, 1252, 2602, 11, 51319, 51319, 439, 264, 1451, 82, 366, 1920, 439, 510, 11, 558, 30, 821, 307, 445, 257, 707, 13630, 510, 958, 281, 51523, 51523], "temperature": 0.0, "avg_logprob": -0.1751939110133959, "compression_ratio": 1.6255707762557077, "no_speech_prob": 2.4296396077261306e-05}, {"id": 433, "seek": 281818, "start": 2824.2, "end": 2829.3999999999996, "text": " is all spread around, right? So it means the force, there are very many ways of drawing", "tokens": [50364, 35193, 486, 439, 312, 733, 295, 1214, 13, 708, 291, 393, 536, 2602, 307, 516, 281, 312, 341, 9656, 50665, 50665, 307, 439, 3974, 926, 11, 558, 30, 407, 309, 1355, 264, 3464, 11, 456, 366, 588, 867, 2098, 295, 6316, 50925, 50925, 257, 1451, 13, 8734, 24157, 264, 1192, 11, 1580, 1177, 380, 13, 407, 498, 291, 536, 322, 264, 558, 1011, 1252, 2602, 11, 51319, 51319, 439, 264, 1451, 82, 366, 1920, 439, 510, 11, 558, 30, 821, 307, 445, 257, 707, 13630, 510, 958, 281, 51523, 51523], "temperature": 0.0, "avg_logprob": -0.1751939110133959, "compression_ratio": 1.6255707762557077, "no_speech_prob": 2.4296396077261306e-05}, {"id": 434, "seek": 281818, "start": 2829.3999999999996, "end": 2837.2799999999997, "text": " a four. Someone closes the top, someone doesn't. So if you see on the right hand side instead,", "tokens": [50364, 35193, 486, 439, 312, 733, 295, 1214, 13, 708, 291, 393, 536, 2602, 307, 516, 281, 312, 341, 9656, 50665, 50665, 307, 439, 3974, 926, 11, 558, 30, 407, 309, 1355, 264, 3464, 11, 456, 366, 588, 867, 2098, 295, 6316, 50925, 50925, 257, 1451, 13, 8734, 24157, 264, 1192, 11, 1580, 1177, 380, 13, 407, 498, 291, 536, 322, 264, 558, 1011, 1252, 2602, 11, 51319, 51319, 439, 264, 1451, 82, 366, 1920, 439, 510, 11, 558, 30, 821, 307, 445, 257, 707, 13630, 510, 958, 281, 51523, 51523], "temperature": 0.0, "avg_logprob": -0.1751939110133959, "compression_ratio": 1.6255707762557077, "no_speech_prob": 2.4296396077261306e-05}, {"id": 435, "seek": 281818, "start": 2837.2799999999997, "end": 2841.3599999999997, "text": " all the fours are almost all here, right? There is just a little cluster here next to", "tokens": [50364, 35193, 486, 439, 312, 733, 295, 1214, 13, 708, 291, 393, 536, 2602, 307, 516, 281, 312, 341, 9656, 50665, 50665, 307, 439, 3974, 926, 11, 558, 30, 407, 309, 1355, 264, 3464, 11, 456, 366, 588, 867, 2098, 295, 6316, 50925, 50925, 257, 1451, 13, 8734, 24157, 264, 1192, 11, 1580, 1177, 380, 13, 407, 498, 291, 536, 322, 264, 558, 1011, 1252, 2602, 11, 51319, 51319, 439, 264, 1451, 82, 366, 1920, 439, 510, 11, 558, 30, 821, 307, 445, 257, 707, 13630, 510, 958, 281, 51523, 51523], "temperature": 0.0, "avg_logprob": -0.1751939110133959, "compression_ratio": 1.6255707762557077, "no_speech_prob": 2.4296396077261306e-05}, {"id": 436, "seek": 284136, "start": 2841.36, "end": 2848.36, "text": " the nine because you can think about if you write a four like that, it's very similar", "tokens": [50364, 264, 4949, 570, 291, 393, 519, 466, 498, 291, 2464, 257, 1451, 411, 300, 11, 309, 311, 588, 2531, 50714, 50714, 281, 2464, 257, 4949, 11, 558, 30, 400, 370, 291, 362, 613, 1451, 82, 510, 300, 366, 588, 1998, 281, 264, 297, 1652, 51042, 51042, 445, 570, 295, 577, 561, 12804, 341, 2685, 3464, 13, 26554, 11, 436, 366, 920, 596, 38624, 13, 51368, 51368, 4886, 510, 11, 291, 483, 439, 613, 721, 366, 3974, 926, 13, 407, 341, 307, 588, 1578, 13, 26554, 11, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.1385229640536838, "compression_ratio": 1.6574074074074074, "no_speech_prob": 4.132671165280044e-05}, {"id": 437, "seek": 284136, "start": 2848.36, "end": 2854.92, "text": " to write a nine, right? And so you have these fours here that are very close to the nines", "tokens": [50364, 264, 4949, 570, 291, 393, 519, 466, 498, 291, 2464, 257, 1451, 411, 300, 11, 309, 311, 588, 2531, 50714, 50714, 281, 2464, 257, 4949, 11, 558, 30, 400, 370, 291, 362, 613, 1451, 82, 510, 300, 366, 588, 1998, 281, 264, 297, 1652, 51042, 51042, 445, 570, 295, 577, 561, 12804, 341, 2685, 3464, 13, 26554, 11, 436, 366, 920, 596, 38624, 13, 51368, 51368, 4886, 510, 11, 291, 483, 439, 613, 721, 366, 3974, 926, 13, 407, 341, 307, 588, 1578, 13, 26554, 11, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.1385229640536838, "compression_ratio": 1.6574074074074074, "no_speech_prob": 4.132671165280044e-05}, {"id": 438, "seek": 284136, "start": 2854.92, "end": 2861.44, "text": " just because of how people drew this specific force. Nevertheless, they are still clustered.", "tokens": [50364, 264, 4949, 570, 291, 393, 519, 466, 498, 291, 2464, 257, 1451, 411, 300, 11, 309, 311, 588, 2531, 50714, 50714, 281, 2464, 257, 4949, 11, 558, 30, 400, 370, 291, 362, 613, 1451, 82, 510, 300, 366, 588, 1998, 281, 264, 297, 1652, 51042, 51042, 445, 570, 295, 577, 561, 12804, 341, 2685, 3464, 13, 26554, 11, 436, 366, 920, 596, 38624, 13, 51368, 51368, 4886, 510, 11, 291, 483, 439, 613, 721, 366, 3974, 926, 13, 407, 341, 307, 588, 1578, 13, 26554, 11, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.1385229640536838, "compression_ratio": 1.6574074074074074, "no_speech_prob": 4.132671165280044e-05}, {"id": 439, "seek": 284136, "start": 2861.44, "end": 2868.52, "text": " Over here, you get all these things are spread around. So this is very bad. Nevertheless,", "tokens": [50364, 264, 4949, 570, 291, 393, 519, 466, 498, 291, 2464, 257, 1451, 411, 300, 11, 309, 311, 588, 2531, 50714, 50714, 281, 2464, 257, 4949, 11, 558, 30, 400, 370, 291, 362, 613, 1451, 82, 510, 300, 366, 588, 1998, 281, 264, 297, 1652, 51042, 51042, 445, 570, 295, 577, 561, 12804, 341, 2685, 3464, 13, 26554, 11, 436, 366, 920, 596, 38624, 13, 51368, 51368, 4886, 510, 11, 291, 483, 439, 613, 721, 366, 3974, 926, 13, 407, 341, 307, 588, 1578, 13, 26554, 11, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.1385229640536838, "compression_ratio": 1.6574074074074074, "no_speech_prob": 4.132671165280044e-05}, {"id": 440, "seek": 286852, "start": 2868.52, "end": 2873.1, "text": " they tell you this diagram here shows you that there is very little variance across", "tokens": [50364, 436, 980, 291, 341, 10686, 510, 3110, 291, 300, 456, 307, 588, 707, 21977, 2108, 50593, 50593, 264, 6316, 295, 257, 4018, 13, 407, 309, 3110, 291, 6063, 456, 307, 257, 2685, 4391, 300, 307, 588, 21321, 51018, 51018, 510, 457, 309, 311, 534, 406, 21321, 337, 613, 1074, 13, 51264, 51264, 407, 286, 478, 445, 6369, 11, 437, 366, 512, 661, 39034, 420, 505, 1660, 295, 3034, 1478, 8399, 22660, 19866, 30, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.19577950316590148, "compression_ratio": 1.63, "no_speech_prob": 1.9523495211615227e-05}, {"id": 441, "seek": 286852, "start": 2873.1, "end": 2881.6, "text": " the drawing of a zero. So it shows you somehow there is a specific mode that is very concentrated", "tokens": [50364, 436, 980, 291, 341, 10686, 510, 3110, 291, 300, 456, 307, 588, 707, 21977, 2108, 50593, 50593, 264, 6316, 295, 257, 4018, 13, 407, 309, 3110, 291, 6063, 456, 307, 257, 2685, 4391, 300, 307, 588, 21321, 51018, 51018, 510, 457, 309, 311, 534, 406, 21321, 337, 613, 1074, 13, 51264, 51264, 407, 286, 478, 445, 6369, 11, 437, 366, 512, 661, 39034, 420, 505, 1660, 295, 3034, 1478, 8399, 22660, 19866, 30, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.19577950316590148, "compression_ratio": 1.63, "no_speech_prob": 1.9523495211615227e-05}, {"id": 442, "seek": 286852, "start": 2881.6, "end": 2886.52, "text": " here but it's really not concentrated for these guys.", "tokens": [50364, 436, 980, 291, 341, 10686, 510, 3110, 291, 300, 456, 307, 588, 707, 21977, 2108, 50593, 50593, 264, 6316, 295, 257, 4018, 13, 407, 309, 3110, 291, 6063, 456, 307, 257, 2685, 4391, 300, 307, 588, 21321, 51018, 51018, 510, 457, 309, 311, 534, 406, 21321, 337, 613, 1074, 13, 51264, 51264, 407, 286, 478, 445, 6369, 11, 437, 366, 512, 661, 39034, 420, 505, 1660, 295, 3034, 1478, 8399, 22660, 19866, 30, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.19577950316590148, "compression_ratio": 1.63, "no_speech_prob": 1.9523495211615227e-05}, {"id": 443, "seek": 286852, "start": 2886.52, "end": 2897.44, "text": " So I'm just curious, what are some other motivations or usages of variational autoencoder?", "tokens": [50364, 436, 980, 291, 341, 10686, 510, 3110, 291, 300, 456, 307, 588, 707, 21977, 2108, 50593, 50593, 264, 6316, 295, 257, 4018, 13, 407, 309, 3110, 291, 6063, 456, 307, 257, 2685, 4391, 300, 307, 588, 21321, 51018, 51018, 510, 457, 309, 311, 534, 406, 21321, 337, 613, 1074, 13, 51264, 51264, 407, 286, 478, 445, 6369, 11, 437, 366, 512, 661, 39034, 420, 505, 1660, 295, 3034, 1478, 8399, 22660, 19866, 30, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.19577950316590148, "compression_ratio": 1.63, "no_speech_prob": 1.9523495211615227e-05}, {"id": 444, "seek": 289744, "start": 2897.44, "end": 2901.44, "text": " Like why would that be useful?", "tokens": [50364, 1743, 983, 576, 300, 312, 4420, 30, 50564, 50564, 407, 264, 2135, 935, 390, 300, 5699, 286, 855, 291, 294, 1508, 732, 3259, 2057, 11, 257, 1337, 1166, 2316, 11, 50982, 50982, 291, 2644, 362, 257, 1337, 1166, 2316, 365, 257, 13735, 8399, 22660, 19866, 13, 682, 341, 1389, 510, 11, 51184, 51184, 797, 11, 286, 994, 380, 3847, 341, 1507, 257, 688, 13, 759, 291, 3847, 309, 2854, 11, 291, 393, 362, 1101, 3389, 13, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.17168946266174318, "compression_ratio": 1.5647668393782384, "no_speech_prob": 3.216984259779565e-05}, {"id": 445, "seek": 289744, "start": 2901.44, "end": 2909.8, "text": " So the main point was that whenever I show you in class two weeks ago, a generative model,", "tokens": [50364, 1743, 983, 576, 300, 312, 4420, 30, 50564, 50564, 407, 264, 2135, 935, 390, 300, 5699, 286, 855, 291, 294, 1508, 732, 3259, 2057, 11, 257, 1337, 1166, 2316, 11, 50982, 50982, 291, 2644, 362, 257, 1337, 1166, 2316, 365, 257, 13735, 8399, 22660, 19866, 13, 682, 341, 1389, 510, 11, 51184, 51184, 797, 11, 286, 994, 380, 3847, 341, 1507, 257, 688, 13, 759, 291, 3847, 309, 2854, 11, 291, 393, 362, 1101, 3389, 13, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.17168946266174318, "compression_ratio": 1.5647668393782384, "no_speech_prob": 3.216984259779565e-05}, {"id": 446, "seek": 289744, "start": 2909.8, "end": 2913.84, "text": " you cannot have a generative model with a classical autoencoder. In this case here,", "tokens": [50364, 1743, 983, 576, 300, 312, 4420, 30, 50564, 50564, 407, 264, 2135, 935, 390, 300, 5699, 286, 855, 291, 294, 1508, 732, 3259, 2057, 11, 257, 1337, 1166, 2316, 11, 50982, 50982, 291, 2644, 362, 257, 1337, 1166, 2316, 365, 257, 13735, 8399, 22660, 19866, 13, 682, 341, 1389, 510, 11, 51184, 51184, 797, 11, 286, 994, 380, 3847, 341, 1507, 257, 688, 13, 759, 291, 3847, 309, 2854, 11, 291, 393, 362, 1101, 3389, 13, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.17168946266174318, "compression_ratio": 1.5647668393782384, "no_speech_prob": 3.216984259779565e-05}, {"id": 447, "seek": 289744, "start": 2913.84, "end": 2921.12, "text": " again, I didn't train this stuff a lot. If you train it longer, you can have better performance.", "tokens": [50364, 1743, 983, 576, 300, 312, 4420, 30, 50564, 50564, 407, 264, 2135, 935, 390, 300, 5699, 286, 855, 291, 294, 1508, 732, 3259, 2057, 11, 257, 1337, 1166, 2316, 11, 50982, 50982, 291, 2644, 362, 257, 1337, 1166, 2316, 365, 257, 13735, 8399, 22660, 19866, 13, 682, 341, 1389, 510, 11, 51184, 51184, 797, 11, 286, 994, 380, 3847, 341, 1507, 257, 688, 13, 759, 291, 3847, 309, 2854, 11, 291, 393, 362, 1101, 3389, 13, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.17168946266174318, "compression_ratio": 1.5647668393782384, "no_speech_prob": 3.216984259779565e-05}, {"id": 448, "seek": 292112, "start": 2921.12, "end": 2929.0, "text": " Here the point is that my input, z, comes from just this random distribution. And then", "tokens": [50364, 1692, 264, 935, 307, 300, 452, 4846, 11, 710, 11, 1487, 490, 445, 341, 4974, 7316, 13, 400, 550, 50758, 50758, 538, 7750, 341, 4974, 1230, 510, 11, 257, 1230, 1348, 490, 257, 2710, 7316, 11, 291, 2845, 51044, 51044, 309, 1854, 341, 979, 19866, 13, 759, 341, 979, 19866, 307, 767, 257, 4005, 979, 19866, 11, 550, 341, 1507, 51370, 51370, 486, 767, 2642, 588, 1481, 10854, 420, 3547, 13, 1743, 11, 337, 1365, 11, 729, 732, 5267, 286, 855, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.152118480906767, "compression_ratio": 1.669811320754717, "no_speech_prob": 2.545853567426093e-05}, {"id": 449, "seek": 292112, "start": 2929.0, "end": 2934.72, "text": " by sending this random number here, a number coming from a normal distribution, you send", "tokens": [50364, 1692, 264, 935, 307, 300, 452, 4846, 11, 710, 11, 1487, 490, 445, 341, 4974, 7316, 13, 400, 550, 50758, 50758, 538, 7750, 341, 4974, 1230, 510, 11, 257, 1230, 1348, 490, 257, 2710, 7316, 11, 291, 2845, 51044, 51044, 309, 1854, 341, 979, 19866, 13, 759, 341, 979, 19866, 307, 767, 257, 4005, 979, 19866, 11, 550, 341, 1507, 51370, 51370, 486, 767, 2642, 588, 1481, 10854, 420, 3547, 13, 1743, 11, 337, 1365, 11, 729, 732, 5267, 286, 855, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.152118480906767, "compression_ratio": 1.669811320754717, "no_speech_prob": 2.545853567426093e-05}, {"id": 450, "seek": 292112, "start": 2934.72, "end": 2941.24, "text": " it inside this decoder. If this decoder is actually a powerful decoder, then this stuff", "tokens": [50364, 1692, 264, 935, 307, 300, 452, 4846, 11, 710, 11, 1487, 490, 445, 341, 4974, 7316, 13, 400, 550, 50758, 50758, 538, 7750, 341, 4974, 1230, 510, 11, 257, 1230, 1348, 490, 257, 2710, 7316, 11, 291, 2845, 51044, 51044, 309, 1854, 341, 979, 19866, 13, 759, 341, 979, 19866, 307, 767, 257, 4005, 979, 19866, 11, 550, 341, 1507, 51370, 51370, 486, 767, 2642, 588, 1481, 10854, 420, 3547, 13, 1743, 11, 337, 1365, 11, 729, 732, 5267, 286, 855, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.152118480906767, "compression_ratio": 1.669811320754717, "no_speech_prob": 2.545853567426093e-05}, {"id": 451, "seek": 292112, "start": 2941.24, "end": 2948.0, "text": " will actually draw very nice shapes or numbers. Like, for example, those two images I show", "tokens": [50364, 1692, 264, 935, 307, 300, 452, 4846, 11, 710, 11, 1487, 490, 445, 341, 4974, 7316, 13, 400, 550, 50758, 50758, 538, 7750, 341, 4974, 1230, 510, 11, 257, 1230, 1348, 490, 257, 2710, 7316, 11, 291, 2845, 51044, 51044, 309, 1854, 341, 979, 19866, 13, 759, 341, 979, 19866, 307, 767, 257, 4005, 979, 19866, 11, 550, 341, 1507, 51370, 51370, 486, 767, 2642, 588, 1481, 10854, 420, 3547, 13, 1743, 11, 337, 1365, 11, 729, 732, 5267, 286, 855, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.152118480906767, "compression_ratio": 1.669811320754717, "no_speech_prob": 2.545853567426093e-05}, {"id": 452, "seek": 294800, "start": 2948.0, "end": 2954.52, "text": " you of the two faces in the first part of the class, the last time, those are simply", "tokens": [50364, 291, 295, 264, 732, 8475, 294, 264, 700, 644, 295, 264, 1508, 11, 264, 1036, 565, 11, 729, 366, 2935, 50690, 50690, 291, 747, 257, 1230, 490, 257, 4974, 7316, 11, 291, 3154, 309, 281, 257, 979, 19866, 11, 293, 264, 979, 19866, 50962, 50962, 307, 516, 281, 312, 6316, 291, 341, 588, 2238, 3036, 295, 2035, 291, 3847, 341, 979, 19866, 51230, 51230, 322, 13, 400, 291, 2644, 764, 257, 3832, 8399, 22660, 19866, 281, 483, 613, 733, 295, 7221, 570, 797, 11, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.15030666914853183, "compression_ratio": 1.6824644549763033, "no_speech_prob": 3.318729068269022e-05}, {"id": 453, "seek": 294800, "start": 2954.52, "end": 2959.96, "text": " you take a number from a random distribution, you feed it to a decoder, and the decoder", "tokens": [50364, 291, 295, 264, 732, 8475, 294, 264, 700, 644, 295, 264, 1508, 11, 264, 1036, 565, 11, 729, 366, 2935, 50690, 50690, 291, 747, 257, 1230, 490, 257, 4974, 7316, 11, 291, 3154, 309, 281, 257, 979, 19866, 11, 293, 264, 979, 19866, 50962, 50962, 307, 516, 281, 312, 6316, 291, 341, 588, 2238, 3036, 295, 2035, 291, 3847, 341, 979, 19866, 51230, 51230, 322, 13, 400, 291, 2644, 764, 257, 3832, 8399, 22660, 19866, 281, 483, 613, 733, 295, 7221, 570, 797, 11, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.15030666914853183, "compression_ratio": 1.6824644549763033, "no_speech_prob": 3.318729068269022e-05}, {"id": 454, "seek": 294800, "start": 2959.96, "end": 2965.32, "text": " is going to be drawing you this very beautiful picture of whatever you train this decoder", "tokens": [50364, 291, 295, 264, 732, 8475, 294, 264, 700, 644, 295, 264, 1508, 11, 264, 1036, 565, 11, 729, 366, 2935, 50690, 50690, 291, 747, 257, 1230, 490, 257, 4974, 7316, 11, 291, 3154, 309, 281, 257, 979, 19866, 11, 293, 264, 979, 19866, 50962, 50962, 307, 516, 281, 312, 6316, 291, 341, 588, 2238, 3036, 295, 2035, 291, 3847, 341, 979, 19866, 51230, 51230, 322, 13, 400, 291, 2644, 764, 257, 3832, 8399, 22660, 19866, 281, 483, 613, 733, 295, 7221, 570, 797, 11, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.15030666914853183, "compression_ratio": 1.6824644549763033, "no_speech_prob": 3.318729068269022e-05}, {"id": 455, "seek": 294800, "start": 2965.32, "end": 2972.2, "text": " on. And you cannot use a standard autoencoder to get these kind of properties because again,", "tokens": [50364, 291, 295, 264, 732, 8475, 294, 264, 700, 644, 295, 264, 1508, 11, 264, 1036, 565, 11, 729, 366, 2935, 50690, 50690, 291, 747, 257, 1230, 490, 257, 4974, 7316, 11, 291, 3154, 309, 281, 257, 979, 19866, 11, 293, 264, 979, 19866, 50962, 50962, 307, 516, 281, 312, 6316, 291, 341, 588, 2238, 3036, 295, 2035, 291, 3847, 341, 979, 19866, 51230, 51230, 322, 13, 400, 291, 2644, 764, 257, 3832, 8399, 22660, 19866, 281, 483, 613, 733, 295, 7221, 570, 797, 11, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.15030666914853183, "compression_ratio": 1.6824644549763033, "no_speech_prob": 3.318729068269022e-05}, {"id": 456, "seek": 297220, "start": 2972.2, "end": 2979.4399999999996, "text": " here we enforce the decoder to reconstruct meaningful or good looking reconstruction", "tokens": [50364, 510, 321, 24825, 264, 979, 19866, 281, 31499, 10995, 420, 665, 1237, 31565, 50726, 50726, 562, 436, 366, 3247, 15551, 490, 341, 2710, 7316, 13, 7504, 11, 1780, 322, 11, 321, 393, 6889, 490, 341, 51034, 51034, 2710, 7316, 11, 3154, 721, 281, 264, 979, 19866, 11, 293, 264, 979, 19866, 486, 8460, 1507, 300, 1542, 51242, 51242, 411, 10275, 13, 759, 291, 994, 380, 3847, 264, 979, 19866, 294, 1668, 281, 2042, 257, 665, 31565, 51618, 51618], "temperature": 0.0, "avg_logprob": -0.10574007034301758, "compression_ratio": 1.8477157360406091, "no_speech_prob": 2.7515225156093948e-05}, {"id": 457, "seek": 297220, "start": 2979.4399999999996, "end": 2985.6, "text": " when they are sampled from this normal distribution. Therefore, later on, we can sample from this", "tokens": [50364, 510, 321, 24825, 264, 979, 19866, 281, 31499, 10995, 420, 665, 1237, 31565, 50726, 50726, 562, 436, 366, 3247, 15551, 490, 341, 2710, 7316, 13, 7504, 11, 1780, 322, 11, 321, 393, 6889, 490, 341, 51034, 51034, 2710, 7316, 11, 3154, 721, 281, 264, 979, 19866, 11, 293, 264, 979, 19866, 486, 8460, 1507, 300, 1542, 51242, 51242, 411, 10275, 13, 759, 291, 994, 380, 3847, 264, 979, 19866, 294, 1668, 281, 2042, 257, 665, 31565, 51618, 51618], "temperature": 0.0, "avg_logprob": -0.10574007034301758, "compression_ratio": 1.8477157360406091, "no_speech_prob": 2.7515225156093948e-05}, {"id": 458, "seek": 297220, "start": 2985.6, "end": 2989.7599999999998, "text": " normal distribution, feed things to the decoder, and the decoder will generate stuff that looks", "tokens": [50364, 510, 321, 24825, 264, 979, 19866, 281, 31499, 10995, 420, 665, 1237, 31565, 50726, 50726, 562, 436, 366, 3247, 15551, 490, 341, 2710, 7316, 13, 7504, 11, 1780, 322, 11, 321, 393, 6889, 490, 341, 51034, 51034, 2710, 7316, 11, 3154, 721, 281, 264, 979, 19866, 11, 293, 264, 979, 19866, 486, 8460, 1507, 300, 1542, 51242, 51242, 411, 10275, 13, 759, 291, 994, 380, 3847, 264, 979, 19866, 294, 1668, 281, 2042, 257, 665, 31565, 51618, 51618], "temperature": 0.0, "avg_logprob": -0.10574007034301758, "compression_ratio": 1.8477157360406091, "no_speech_prob": 2.7515225156093948e-05}, {"id": 459, "seek": 297220, "start": 2989.7599999999998, "end": 2997.2799999999997, "text": " like legit. If you didn't train the decoder in order to perform a good reconstruction", "tokens": [50364, 510, 321, 24825, 264, 979, 19866, 281, 31499, 10995, 420, 665, 1237, 31565, 50726, 50726, 562, 436, 366, 3247, 15551, 490, 341, 2710, 7316, 13, 7504, 11, 1780, 322, 11, 321, 393, 6889, 490, 341, 51034, 51034, 2710, 7316, 11, 3154, 721, 281, 264, 979, 19866, 11, 293, 264, 979, 19866, 486, 8460, 1507, 300, 1542, 51242, 51242, 411, 10275, 13, 759, 291, 994, 380, 3847, 264, 979, 19866, 294, 1668, 281, 2042, 257, 665, 31565, 51618, 51618], "temperature": 0.0, "avg_logprob": -0.10574007034301758, "compression_ratio": 1.8477157360406091, "no_speech_prob": 2.7515225156093948e-05}, {"id": 460, "seek": 299728, "start": 2997.28, "end": 3002.48, "text": " when you sample from this normal distribution, you wouldn't be able to actually get anything", "tokens": [50364, 562, 291, 6889, 490, 341, 2710, 7316, 11, 291, 2759, 380, 312, 1075, 281, 767, 483, 1340, 50624, 50624, 10995, 13, 663, 311, 264, 955, 30681, 510, 13, 3087, 565, 11, 321, 434, 516, 281, 312, 2577, 1337, 1166, 51000, 51000, 17641, 44745, 9590, 293, 577, 436, 366, 588, 2531, 281, 341, 1507, 321, 362, 1612, 965, 13, 51360, 51360, 2421, 11, 28327, 78, 13, 286, 362, 257, 1168, 337, 264, 5566, 12212, 13, 51554, 51554, 17550, 12212, 11, 1338, 13, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.2221221923828125, "compression_ratio": 1.4956140350877194, "no_speech_prob": 3.320444739074446e-05}, {"id": 461, "seek": 299728, "start": 3002.48, "end": 3010.0, "text": " meaningful. That's the big takeaway here. Next time, we're going to be seeing generative", "tokens": [50364, 562, 291, 6889, 490, 341, 2710, 7316, 11, 291, 2759, 380, 312, 1075, 281, 767, 483, 1340, 50624, 50624, 10995, 13, 663, 311, 264, 955, 30681, 510, 13, 3087, 565, 11, 321, 434, 516, 281, 312, 2577, 1337, 1166, 51000, 51000, 17641, 44745, 9590, 293, 577, 436, 366, 588, 2531, 281, 341, 1507, 321, 362, 1612, 965, 13, 51360, 51360, 2421, 11, 28327, 78, 13, 286, 362, 257, 1168, 337, 264, 5566, 12212, 13, 51554, 51554, 17550, 12212, 11, 1338, 13, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.2221221923828125, "compression_ratio": 1.4956140350877194, "no_speech_prob": 3.320444739074446e-05}, {"id": 462, "seek": 299728, "start": 3010.0, "end": 3017.2000000000003, "text": " adversarial networks and how they are very similar to this stuff we have seen today.", "tokens": [50364, 562, 291, 6889, 490, 341, 2710, 7316, 11, 291, 2759, 380, 312, 1075, 281, 767, 483, 1340, 50624, 50624, 10995, 13, 663, 311, 264, 955, 30681, 510, 13, 3087, 565, 11, 321, 434, 516, 281, 312, 2577, 1337, 1166, 51000, 51000, 17641, 44745, 9590, 293, 577, 436, 366, 588, 2531, 281, 341, 1507, 321, 362, 1612, 965, 13, 51360, 51360, 2421, 11, 28327, 78, 13, 286, 362, 257, 1168, 337, 264, 5566, 12212, 13, 51554, 51554, 17550, 12212, 11, 1338, 13, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.2221221923828125, "compression_ratio": 1.4956140350877194, "no_speech_prob": 3.320444739074446e-05}, {"id": 463, "seek": 299728, "start": 3017.2000000000003, "end": 3021.0800000000004, "text": " Hi, Alfredo. I have a question for the yellow bubble.", "tokens": [50364, 562, 291, 6889, 490, 341, 2710, 7316, 11, 291, 2759, 380, 312, 1075, 281, 767, 483, 1340, 50624, 50624, 10995, 13, 663, 311, 264, 955, 30681, 510, 13, 3087, 565, 11, 321, 434, 516, 281, 312, 2577, 1337, 1166, 51000, 51000, 17641, 44745, 9590, 293, 577, 436, 366, 588, 2531, 281, 341, 1507, 321, 362, 1612, 965, 13, 51360, 51360, 2421, 11, 28327, 78, 13, 286, 362, 257, 1168, 337, 264, 5566, 12212, 13, 51554, 51554, 17550, 12212, 11, 1338, 13, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.2221221923828125, "compression_ratio": 1.4956140350877194, "no_speech_prob": 3.320444739074446e-05}, {"id": 464, "seek": 299728, "start": 3021.0800000000004, "end": 3022.4, "text": " Yellow bubble, yeah.", "tokens": [50364, 562, 291, 6889, 490, 341, 2710, 7316, 11, 291, 2759, 380, 312, 1075, 281, 767, 483, 1340, 50624, 50624, 10995, 13, 663, 311, 264, 955, 30681, 510, 13, 3087, 565, 11, 321, 434, 516, 281, 312, 2577, 1337, 1166, 51000, 51000, 17641, 44745, 9590, 293, 577, 436, 366, 588, 2531, 281, 341, 1507, 321, 362, 1612, 965, 13, 51360, 51360, 2421, 11, 28327, 78, 13, 286, 362, 257, 1168, 337, 264, 5566, 12212, 13, 51554, 51554, 17550, 12212, 11, 1338, 13, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.2221221923828125, "compression_ratio": 1.4956140350877194, "no_speech_prob": 3.320444739074446e-05}, {"id": 465, "seek": 302240, "start": 3022.4, "end": 3030.2400000000002, "text": " Yeah, so each yellow bubble comes from one input example. So if we have 1,000, I don't", "tokens": [50364, 865, 11, 370, 1184, 5566, 12212, 1487, 490, 472, 4846, 1365, 13, 407, 498, 321, 362, 502, 11, 1360, 11, 286, 500, 380, 50756, 50756, 458, 11, 5267, 420, 502, 11, 1360, 15743, 11, 300, 1355, 321, 362, 502, 11, 1360, 2293, 5566, 16295, 13, 400, 1184, 51100, 51100, 5566, 12212, 11, 309, 1487, 490, 264, 1858, 7316, 1214, 365, 264, 5658, 3869, 281, 48994, 7006, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.15765726062613475, "compression_ratio": 1.5895953757225434, "no_speech_prob": 7.250794442370534e-05}, {"id": 466, "seek": 302240, "start": 3030.2400000000002, "end": 3037.12, "text": " know, images or 1,000 inputs, that means we have 1,000 exactly yellow bubbles. And each", "tokens": [50364, 865, 11, 370, 1184, 5566, 12212, 1487, 490, 472, 4846, 1365, 13, 407, 498, 321, 362, 502, 11, 1360, 11, 286, 500, 380, 50756, 50756, 458, 11, 5267, 420, 502, 11, 1360, 15743, 11, 300, 1355, 321, 362, 502, 11, 1360, 2293, 5566, 16295, 13, 400, 1184, 51100, 51100, 5566, 12212, 11, 309, 1487, 490, 264, 1858, 7316, 1214, 365, 264, 5658, 3869, 281, 48994, 7006, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.15765726062613475, "compression_ratio": 1.5895953757225434, "no_speech_prob": 7.250794442370534e-05}, {"id": 467, "seek": 302240, "start": 3037.12, "end": 3048.1600000000003, "text": " yellow bubble, it comes from the easy distribution together with the noise added to latent variable.", "tokens": [50364, 865, 11, 370, 1184, 5566, 12212, 1487, 490, 472, 4846, 1365, 13, 407, 498, 321, 362, 502, 11, 1360, 11, 286, 500, 380, 50756, 50756, 458, 11, 5267, 420, 502, 11, 1360, 15743, 11, 300, 1355, 321, 362, 502, 11, 1360, 2293, 5566, 16295, 13, 400, 1184, 51100, 51100, 5566, 12212, 11, 309, 1487, 490, 264, 1858, 7316, 1214, 365, 264, 5658, 3869, 281, 48994, 7006, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.15765726062613475, "compression_ratio": 1.5895953757225434, "no_speech_prob": 7.250794442370534e-05}, {"id": 468, "seek": 304816, "start": 3048.16, "end": 3053.52, "text": " So the bubble comes from here. Let me show you. Should I show you this one? It's okay?", "tokens": [50364, 407, 264, 12212, 1487, 490, 510, 13, 961, 385, 855, 291, 13, 6454, 286, 855, 291, 341, 472, 30, 467, 311, 1392, 30, 50632, 50632, 1610, 820, 286, 855, 291, 264, 9788, 30, 50702, 50702, 865, 11, 1338, 11, 309, 311, 1392, 13, 50752, 50752, 1033, 13, 407, 510, 291, 483, 341, 1783, 293, 341, 1783, 1709, 1854, 264, 2316, 11, 558, 30, 14159, 291, 2845, 51002, 51002, 341, 1783, 807, 264, 2316, 11, 309, 1709, 1854, 2128, 13, 407, 1783, 1709, 1854, 510, 293, 550, 309, 1709, 1854, 51306, 51306, 264, 2058, 19866, 11, 558, 30, 51464, 51464, 400, 550, 490, 264, 2058, 19866, 2709, 385, 341, 2992, 3565, 8517, 1433, 490, 597, 286, 445, 8947, 264, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.19611804211725953, "compression_ratio": 1.873913043478261, "no_speech_prob": 0.00022313487716019154}, {"id": 469, "seek": 304816, "start": 3053.52, "end": 3054.92, "text": " Or should I show you the slides?", "tokens": [50364, 407, 264, 12212, 1487, 490, 510, 13, 961, 385, 855, 291, 13, 6454, 286, 855, 291, 341, 472, 30, 467, 311, 1392, 30, 50632, 50632, 1610, 820, 286, 855, 291, 264, 9788, 30, 50702, 50702, 865, 11, 1338, 11, 309, 311, 1392, 13, 50752, 50752, 1033, 13, 407, 510, 291, 483, 341, 1783, 293, 341, 1783, 1709, 1854, 264, 2316, 11, 558, 30, 14159, 291, 2845, 51002, 51002, 341, 1783, 807, 264, 2316, 11, 309, 1709, 1854, 2128, 13, 407, 1783, 1709, 1854, 510, 293, 550, 309, 1709, 1854, 51306, 51306, 264, 2058, 19866, 11, 558, 30, 51464, 51464, 400, 550, 490, 264, 2058, 19866, 2709, 385, 341, 2992, 3565, 8517, 1433, 490, 597, 286, 445, 8947, 264, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.19611804211725953, "compression_ratio": 1.873913043478261, "no_speech_prob": 0.00022313487716019154}, {"id": 470, "seek": 304816, "start": 3054.92, "end": 3055.92, "text": " Yeah, yeah, it's okay.", "tokens": [50364, 407, 264, 12212, 1487, 490, 510, 13, 961, 385, 855, 291, 13, 6454, 286, 855, 291, 341, 472, 30, 467, 311, 1392, 30, 50632, 50632, 1610, 820, 286, 855, 291, 264, 9788, 30, 50702, 50702, 865, 11, 1338, 11, 309, 311, 1392, 13, 50752, 50752, 1033, 13, 407, 510, 291, 483, 341, 1783, 293, 341, 1783, 1709, 1854, 264, 2316, 11, 558, 30, 14159, 291, 2845, 51002, 51002, 341, 1783, 807, 264, 2316, 11, 309, 1709, 1854, 2128, 13, 407, 1783, 1709, 1854, 510, 293, 550, 309, 1709, 1854, 51306, 51306, 264, 2058, 19866, 11, 558, 30, 51464, 51464, 400, 550, 490, 264, 2058, 19866, 2709, 385, 341, 2992, 3565, 8517, 1433, 490, 597, 286, 445, 8947, 264, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.19611804211725953, "compression_ratio": 1.873913043478261, "no_speech_prob": 0.00022313487716019154}, {"id": 471, "seek": 304816, "start": 3055.92, "end": 3060.92, "text": " Okay. So here you get this X and this X goes inside the model, right? Whenever you send", "tokens": [50364, 407, 264, 12212, 1487, 490, 510, 13, 961, 385, 855, 291, 13, 6454, 286, 855, 291, 341, 472, 30, 467, 311, 1392, 30, 50632, 50632, 1610, 820, 286, 855, 291, 264, 9788, 30, 50702, 50702, 865, 11, 1338, 11, 309, 311, 1392, 13, 50752, 50752, 1033, 13, 407, 510, 291, 483, 341, 1783, 293, 341, 1783, 1709, 1854, 264, 2316, 11, 558, 30, 14159, 291, 2845, 51002, 51002, 341, 1783, 807, 264, 2316, 11, 309, 1709, 1854, 2128, 13, 407, 1783, 1709, 1854, 510, 293, 550, 309, 1709, 1854, 51306, 51306, 264, 2058, 19866, 11, 558, 30, 51464, 51464, 400, 550, 490, 264, 2058, 19866, 2709, 385, 341, 2992, 3565, 8517, 1433, 490, 597, 286, 445, 8947, 264, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.19611804211725953, "compression_ratio": 1.873913043478261, "no_speech_prob": 0.00022313487716019154}, {"id": 472, "seek": 304816, "start": 3060.92, "end": 3067.0, "text": " this X through the model, it goes inside forward. So X goes inside here and then it goes inside", "tokens": [50364, 407, 264, 12212, 1487, 490, 510, 13, 961, 385, 855, 291, 13, 6454, 286, 855, 291, 341, 472, 30, 467, 311, 1392, 30, 50632, 50632, 1610, 820, 286, 855, 291, 264, 9788, 30, 50702, 50702, 865, 11, 1338, 11, 309, 311, 1392, 13, 50752, 50752, 1033, 13, 407, 510, 291, 483, 341, 1783, 293, 341, 1783, 1709, 1854, 264, 2316, 11, 558, 30, 14159, 291, 2845, 51002, 51002, 341, 1783, 807, 264, 2316, 11, 309, 1709, 1854, 2128, 13, 407, 1783, 1709, 1854, 510, 293, 550, 309, 1709, 1854, 51306, 51306, 264, 2058, 19866, 11, 558, 30, 51464, 51464, 400, 550, 490, 264, 2058, 19866, 2709, 385, 341, 2992, 3565, 8517, 1433, 490, 597, 286, 445, 8947, 264, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.19611804211725953, "compression_ratio": 1.873913043478261, "no_speech_prob": 0.00022313487716019154}, {"id": 473, "seek": 304816, "start": 3067.0, "end": 3070.16, "text": " the encoder, right?", "tokens": [50364, 407, 264, 12212, 1487, 490, 510, 13, 961, 385, 855, 291, 13, 6454, 286, 855, 291, 341, 472, 30, 467, 311, 1392, 30, 50632, 50632, 1610, 820, 286, 855, 291, 264, 9788, 30, 50702, 50702, 865, 11, 1338, 11, 309, 311, 1392, 13, 50752, 50752, 1033, 13, 407, 510, 291, 483, 341, 1783, 293, 341, 1783, 1709, 1854, 264, 2316, 11, 558, 30, 14159, 291, 2845, 51002, 51002, 341, 1783, 807, 264, 2316, 11, 309, 1709, 1854, 2128, 13, 407, 1783, 1709, 1854, 510, 293, 550, 309, 1709, 1854, 51306, 51306, 264, 2058, 19866, 11, 558, 30, 51464, 51464, 400, 550, 490, 264, 2058, 19866, 2709, 385, 341, 2992, 3565, 8517, 1433, 490, 597, 286, 445, 8947, 264, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.19611804211725953, "compression_ratio": 1.873913043478261, "no_speech_prob": 0.00022313487716019154}, {"id": 474, "seek": 304816, "start": 3070.16, "end": 3076.0, "text": " And then from the encoder gives me this mu logvar term from which I just extract the", "tokens": [50364, 407, 264, 12212, 1487, 490, 510, 13, 961, 385, 855, 291, 13, 6454, 286, 855, 291, 341, 472, 30, 467, 311, 1392, 30, 50632, 50632, 1610, 820, 286, 855, 291, 264, 9788, 30, 50702, 50702, 865, 11, 1338, 11, 309, 311, 1392, 13, 50752, 50752, 1033, 13, 407, 510, 291, 483, 341, 1783, 293, 341, 1783, 1709, 1854, 264, 2316, 11, 558, 30, 14159, 291, 2845, 51002, 51002, 341, 1783, 807, 264, 2316, 11, 309, 1709, 1854, 2128, 13, 407, 1783, 1709, 1854, 510, 293, 550, 309, 1709, 1854, 51306, 51306, 264, 2058, 19866, 11, 558, 30, 51464, 51464, 400, 550, 490, 264, 2058, 19866, 2709, 385, 341, 2992, 3565, 8517, 1433, 490, 597, 286, 445, 8947, 264, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.19611804211725953, "compression_ratio": 1.873913043478261, "no_speech_prob": 0.00022313487716019154}, {"id": 475, "seek": 307600, "start": 3076.0, "end": 3082.24, "text": " mu and logvar, okay? So far everything is like a normal autoencoder.", "tokens": [50364, 2992, 293, 3565, 8517, 11, 1392, 30, 407, 1400, 1203, 307, 411, 257, 2710, 8399, 22660, 19866, 13, 50676, 50676, 440, 12212, 1487, 510, 13, 407, 452, 1176, 586, 1487, 484, 490, 613, 2698, 12, 265, 2181, 335, 2398, 1602, 13, 400, 341, 51104, 51104, 2698, 12, 265, 2181, 335, 2398, 1602, 307, 516, 281, 312, 1364, 294, 257, 819, 636, 498, 321, 366, 294, 264, 3097, 51426, 51426, 6367, 420, 321, 366, 406, 294, 264, 3097, 6367, 13, 407, 498, 321, 366, 406, 294, 264, 3097, 6367, 11, 286, 445, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.1475496994821649, "compression_ratio": 1.7239583333333333, "no_speech_prob": 1.1840750630653929e-05}, {"id": 476, "seek": 307600, "start": 3082.24, "end": 3090.8, "text": " The bubble comes here. So my Z now comes out from these self-reparameterized. And this", "tokens": [50364, 2992, 293, 3565, 8517, 11, 1392, 30, 407, 1400, 1203, 307, 411, 257, 2710, 8399, 22660, 19866, 13, 50676, 50676, 440, 12212, 1487, 510, 13, 407, 452, 1176, 586, 1487, 484, 490, 613, 2698, 12, 265, 2181, 335, 2398, 1602, 13, 400, 341, 51104, 51104, 2698, 12, 265, 2181, 335, 2398, 1602, 307, 516, 281, 312, 1364, 294, 257, 819, 636, 498, 321, 366, 294, 264, 3097, 51426, 51426, 6367, 420, 321, 366, 406, 294, 264, 3097, 6367, 13, 407, 498, 321, 366, 406, 294, 264, 3097, 6367, 11, 286, 445, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.1475496994821649, "compression_ratio": 1.7239583333333333, "no_speech_prob": 1.1840750630653929e-05}, {"id": 477, "seek": 307600, "start": 3090.8, "end": 3097.24, "text": " self-reparameterized is going to be working in a different way if we are in the training", "tokens": [50364, 2992, 293, 3565, 8517, 11, 1392, 30, 407, 1400, 1203, 307, 411, 257, 2710, 8399, 22660, 19866, 13, 50676, 50676, 440, 12212, 1487, 510, 13, 407, 452, 1176, 586, 1487, 484, 490, 613, 2698, 12, 265, 2181, 335, 2398, 1602, 13, 400, 341, 51104, 51104, 2698, 12, 265, 2181, 335, 2398, 1602, 307, 516, 281, 312, 1364, 294, 257, 819, 636, 498, 321, 366, 294, 264, 3097, 51426, 51426, 6367, 420, 321, 366, 406, 294, 264, 3097, 6367, 13, 407, 498, 321, 366, 406, 294, 264, 3097, 6367, 11, 286, 445, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.1475496994821649, "compression_ratio": 1.7239583333333333, "no_speech_prob": 1.1840750630653929e-05}, {"id": 478, "seek": 307600, "start": 3097.24, "end": 3102.72, "text": " loop or we are not in the training loop. So if we are not in the training loop, I just", "tokens": [50364, 2992, 293, 3565, 8517, 11, 1392, 30, 407, 1400, 1203, 307, 411, 257, 2710, 8399, 22660, 19866, 13, 50676, 50676, 440, 12212, 1487, 510, 13, 407, 452, 1176, 586, 1487, 484, 490, 613, 2698, 12, 265, 2181, 335, 2398, 1602, 13, 400, 341, 51104, 51104, 2698, 12, 265, 2181, 335, 2398, 1602, 307, 516, 281, 312, 1364, 294, 257, 819, 636, 498, 321, 366, 294, 264, 3097, 51426, 51426, 6367, 420, 321, 366, 406, 294, 264, 3097, 6367, 13, 407, 498, 321, 366, 406, 294, 264, 3097, 6367, 11, 286, 445, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.1475496994821649, "compression_ratio": 1.7239583333333333, "no_speech_prob": 1.1840750630653929e-05}, {"id": 479, "seek": 310272, "start": 3102.72, "end": 3108.3599999999997, "text": " return the mean. So there is no bubble when I use the testing part, okay? So I get the", "tokens": [50364, 2736, 264, 914, 13, 407, 456, 307, 572, 12212, 562, 286, 764, 264, 4997, 644, 11, 1392, 30, 407, 286, 483, 264, 50646, 50646, 1151, 2158, 264, 2058, 19866, 393, 976, 385, 13, 759, 286, 669, 3097, 2602, 11, 341, 307, 437, 2314, 13, 407, 51108, 51108, 286, 14722, 264, 3832, 25163, 490, 341, 3565, 8517, 13, 407, 286, 483, 264, 3565, 8517, 11, 286, 9845, 309, 538, 51356, 51356, 732, 293, 550, 286, 747, 264, 21510, 11, 558, 30, 407, 286, 362, 308, 281, 264, 472, 1922, 3565, 8517, 1270, 300, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.1423805058002472, "compression_ratio": 1.6232558139534883, "no_speech_prob": 1.776954741217196e-05}, {"id": 480, "seek": 310272, "start": 3108.3599999999997, "end": 3117.6, "text": " best value the encoder can give me. If I am training instead, this is what happens. So", "tokens": [50364, 2736, 264, 914, 13, 407, 456, 307, 572, 12212, 562, 286, 764, 264, 4997, 644, 11, 1392, 30, 407, 286, 483, 264, 50646, 50646, 1151, 2158, 264, 2058, 19866, 393, 976, 385, 13, 759, 286, 669, 3097, 2602, 11, 341, 307, 437, 2314, 13, 407, 51108, 51108, 286, 14722, 264, 3832, 25163, 490, 341, 3565, 8517, 13, 407, 286, 483, 264, 3565, 8517, 11, 286, 9845, 309, 538, 51356, 51356, 732, 293, 550, 286, 747, 264, 21510, 11, 558, 30, 407, 286, 362, 308, 281, 264, 472, 1922, 3565, 8517, 1270, 300, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.1423805058002472, "compression_ratio": 1.6232558139534883, "no_speech_prob": 1.776954741217196e-05}, {"id": 481, "seek": 310272, "start": 3117.6, "end": 3122.56, "text": " I compute the standard deviation from this logvar. So I get the logvar, I divide it by", "tokens": [50364, 2736, 264, 914, 13, 407, 456, 307, 572, 12212, 562, 286, 764, 264, 4997, 644, 11, 1392, 30, 407, 286, 483, 264, 50646, 50646, 1151, 2158, 264, 2058, 19866, 393, 976, 385, 13, 759, 286, 669, 3097, 2602, 11, 341, 307, 437, 2314, 13, 407, 51108, 51108, 286, 14722, 264, 3832, 25163, 490, 341, 3565, 8517, 13, 407, 286, 483, 264, 3565, 8517, 11, 286, 9845, 309, 538, 51356, 51356, 732, 293, 550, 286, 747, 264, 21510, 11, 558, 30, 407, 286, 362, 308, 281, 264, 472, 1922, 3565, 8517, 1270, 300, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.1423805058002472, "compression_ratio": 1.6232558139534883, "no_speech_prob": 1.776954741217196e-05}, {"id": 482, "seek": 310272, "start": 3122.56, "end": 3130.16, "text": " two and then I take the exponential, right? So I have e to the one half logvar such that", "tokens": [50364, 2736, 264, 914, 13, 407, 456, 307, 572, 12212, 562, 286, 764, 264, 4997, 644, 11, 1392, 30, 407, 286, 483, 264, 50646, 50646, 1151, 2158, 264, 2058, 19866, 393, 976, 385, 13, 759, 286, 669, 3097, 2602, 11, 341, 307, 437, 2314, 13, 407, 51108, 51108, 286, 14722, 264, 3832, 25163, 490, 341, 3565, 8517, 13, 407, 286, 483, 264, 3565, 8517, 11, 286, 9845, 309, 538, 51356, 51356, 732, 293, 550, 286, 747, 264, 21510, 11, 558, 30, 407, 286, 362, 308, 281, 264, 472, 1922, 3565, 8517, 1270, 300, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.1423805058002472, "compression_ratio": 1.6232558139534883, "no_speech_prob": 1.776954741217196e-05}, {"id": 483, "seek": 313016, "start": 3130.16, "end": 3137.8799999999997, "text": " you get the standard deviation. And then the epsilon is going to be simply a d-dimensional", "tokens": [50364, 291, 483, 264, 3832, 25163, 13, 400, 550, 264, 17889, 307, 516, 281, 312, 2935, 257, 274, 12, 18759, 50750, 50750, 8062, 3247, 15551, 490, 257, 2710, 7316, 13, 400, 370, 341, 472, 307, 472, 6889, 1348, 490, 51120, 51120, 341, 2710, 7316, 13, 400, 264, 2710, 7316, 307, 411, 257, 16687, 294, 274, 12819, 11, 558, 30, 316, 51544, 51544, 16687, 365, 264, 15845, 11, 597, 307, 516, 281, 312, 3732, 5593, 295, 274, 13, 583, 550, 11, 370, 510, 412, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.18045220264168674, "compression_ratio": 1.8256410256410256, "no_speech_prob": 6.699613004457206e-05}, {"id": 484, "seek": 313016, "start": 3137.8799999999997, "end": 3145.2799999999997, "text": " vector sampled from a normal distribution. And so this one is one sample coming from", "tokens": [50364, 291, 483, 264, 3832, 25163, 13, 400, 550, 264, 17889, 307, 516, 281, 312, 2935, 257, 274, 12, 18759, 50750, 50750, 8062, 3247, 15551, 490, 257, 2710, 7316, 13, 400, 370, 341, 472, 307, 472, 6889, 1348, 490, 51120, 51120, 341, 2710, 7316, 13, 400, 264, 2710, 7316, 307, 411, 257, 16687, 294, 274, 12819, 11, 558, 30, 316, 51544, 51544, 16687, 365, 264, 15845, 11, 597, 307, 516, 281, 312, 3732, 5593, 295, 274, 13, 583, 550, 11, 370, 510, 412, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.18045220264168674, "compression_ratio": 1.8256410256410256, "no_speech_prob": 6.699613004457206e-05}, {"id": 485, "seek": 313016, "start": 3145.2799999999997, "end": 3153.7599999999998, "text": " this normal distribution. And the normal distribution is like a sphere in d dimensions, right? A", "tokens": [50364, 291, 483, 264, 3832, 25163, 13, 400, 550, 264, 17889, 307, 516, 281, 312, 2935, 257, 274, 12, 18759, 50750, 50750, 8062, 3247, 15551, 490, 257, 2710, 7316, 13, 400, 370, 341, 472, 307, 472, 6889, 1348, 490, 51120, 51120, 341, 2710, 7316, 13, 400, 264, 2710, 7316, 307, 411, 257, 16687, 294, 274, 12819, 11, 558, 30, 316, 51544, 51544, 16687, 365, 264, 15845, 11, 597, 307, 516, 281, 312, 3732, 5593, 295, 274, 13, 583, 550, 11, 370, 510, 412, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.18045220264168674, "compression_ratio": 1.8256410256410256, "no_speech_prob": 6.699613004457206e-05}, {"id": 486, "seek": 313016, "start": 3153.7599999999998, "end": 3158.24, "text": " sphere with the radius, which is going to be square root of d. But then, so here at", "tokens": [50364, 291, 483, 264, 3832, 25163, 13, 400, 550, 264, 17889, 307, 516, 281, 312, 2935, 257, 274, 12, 18759, 50750, 50750, 8062, 3247, 15551, 490, 257, 2710, 7316, 13, 400, 370, 341, 472, 307, 472, 6889, 1348, 490, 51120, 51120, 341, 2710, 7316, 13, 400, 264, 2710, 7316, 307, 411, 257, 16687, 294, 274, 12819, 11, 558, 30, 316, 51544, 51544, 16687, 365, 264, 15845, 11, 597, 307, 516, 281, 312, 3732, 5593, 295, 274, 13, 583, 550, 11, 370, 510, 412, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.18045220264168674, "compression_ratio": 1.8256410256410256, "no_speech_prob": 6.699613004457206e-05}, {"id": 487, "seek": 315824, "start": 3158.24, "end": 3165.4399999999996, "text": " the end, you simply resize that thing. The point is that every time you call this reparameterization,", "tokens": [50364, 264, 917, 11, 291, 2935, 50069, 300, 551, 13, 440, 935, 307, 300, 633, 565, 291, 818, 341, 1085, 12835, 2398, 2144, 11, 50724, 50724, 1085, 12835, 2398, 1125, 2445, 11, 291, 434, 516, 281, 483, 257, 819, 17889, 570, 17889, 307, 3247, 15551, 50974, 50974, 490, 257, 2710, 7316, 11, 558, 30, 407, 2212, 257, 2992, 293, 2212, 257, 3565, 8517, 11, 291, 434, 516, 281, 312, 51372, 51372, 1242, 633, 565, 819, 308, 1878, 388, 892, 13, 400, 4412, 11, 341, 1507, 510, 11, 498, 291, 818, 309, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.15992418248602686, "compression_ratio": 1.7729468599033817, "no_speech_prob": 1.669883931754157e-05}, {"id": 488, "seek": 315824, "start": 3165.4399999999996, "end": 3170.4399999999996, "text": " reparameterize function, you're going to get a different epsilon because epsilon is sampled", "tokens": [50364, 264, 917, 11, 291, 2935, 50069, 300, 551, 13, 440, 935, 307, 300, 633, 565, 291, 818, 341, 1085, 12835, 2398, 2144, 11, 50724, 50724, 1085, 12835, 2398, 1125, 2445, 11, 291, 434, 516, 281, 483, 257, 819, 17889, 570, 17889, 307, 3247, 15551, 50974, 50974, 490, 257, 2710, 7316, 11, 558, 30, 407, 2212, 257, 2992, 293, 2212, 257, 3565, 8517, 11, 291, 434, 516, 281, 312, 51372, 51372, 1242, 633, 565, 819, 308, 1878, 388, 892, 13, 400, 4412, 11, 341, 1507, 510, 11, 498, 291, 818, 309, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.15992418248602686, "compression_ratio": 1.7729468599033817, "no_speech_prob": 1.669883931754157e-05}, {"id": 489, "seek": 315824, "start": 3170.4399999999996, "end": 3178.3999999999996, "text": " from a normal distribution, right? So given a mu and given a logvar, you're going to be", "tokens": [50364, 264, 917, 11, 291, 2935, 50069, 300, 551, 13, 440, 935, 307, 300, 633, 565, 291, 818, 341, 1085, 12835, 2398, 2144, 11, 50724, 50724, 1085, 12835, 2398, 1125, 2445, 11, 291, 434, 516, 281, 483, 257, 819, 17889, 570, 17889, 307, 3247, 15551, 50974, 50974, 490, 257, 2710, 7316, 11, 558, 30, 407, 2212, 257, 2992, 293, 2212, 257, 3565, 8517, 11, 291, 434, 516, 281, 312, 51372, 51372, 1242, 633, 565, 819, 308, 1878, 388, 892, 13, 400, 4412, 11, 341, 1507, 510, 11, 498, 291, 818, 309, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.15992418248602686, "compression_ratio": 1.7729468599033817, "no_speech_prob": 1.669883931754157e-05}, {"id": 490, "seek": 315824, "start": 3178.3999999999996, "end": 3183.72, "text": " getting every time different epsilons. And therefore, this stuff here, if you call it", "tokens": [50364, 264, 917, 11, 291, 2935, 50069, 300, 551, 13, 440, 935, 307, 300, 633, 565, 291, 818, 341, 1085, 12835, 2398, 2144, 11, 50724, 50724, 1085, 12835, 2398, 1125, 2445, 11, 291, 434, 516, 281, 483, 257, 819, 17889, 570, 17889, 307, 3247, 15551, 50974, 50974, 490, 257, 2710, 7316, 11, 558, 30, 407, 2212, 257, 2992, 293, 2212, 257, 3565, 8517, 11, 291, 434, 516, 281, 312, 51372, 51372, 1242, 633, 565, 819, 308, 1878, 388, 892, 13, 400, 4412, 11, 341, 1507, 510, 11, 498, 291, 818, 309, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.15992418248602686, "compression_ratio": 1.7729468599033817, "no_speech_prob": 1.669883931754157e-05}, {"id": 491, "seek": 318372, "start": 3183.72, "end": 3192.24, "text": " 100 times, is going to give you 100 different points, all of them clustered in mu with a", "tokens": [50364, 2319, 1413, 11, 307, 516, 281, 976, 291, 2319, 819, 2793, 11, 439, 295, 552, 596, 38624, 294, 2992, 365, 257, 50790, 50790, 15845, 295, 9810, 3832, 25163, 13, 400, 370, 341, 307, 264, 1622, 597, 11247, 291, 633, 51140, 51140, 565, 445, 472, 6889, 13, 583, 498, 291, 818, 341, 294, 257, 337, 6367, 11, 291, 434, 516, 281, 483, 257, 4588, 51372, 51372, 295, 2793, 11, 439, 295, 552, 18988, 294, 2992, 11, 597, 575, 257, 2685, 15845, 13, 400, 370, 341, 307, 689, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.14275454415215386, "compression_ratio": 1.6941747572815533, "no_speech_prob": 2.7928223062190227e-05}, {"id": 492, "seek": 318372, "start": 3192.24, "end": 3199.24, "text": " radius of roughly standard deviation. And so this is the line which returns you every", "tokens": [50364, 2319, 1413, 11, 307, 516, 281, 976, 291, 2319, 819, 2793, 11, 439, 295, 552, 596, 38624, 294, 2992, 365, 257, 50790, 50790, 15845, 295, 9810, 3832, 25163, 13, 400, 370, 341, 307, 264, 1622, 597, 11247, 291, 633, 51140, 51140, 565, 445, 472, 6889, 13, 583, 498, 291, 818, 341, 294, 257, 337, 6367, 11, 291, 434, 516, 281, 483, 257, 4588, 51372, 51372, 295, 2793, 11, 439, 295, 552, 18988, 294, 2992, 11, 597, 575, 257, 2685, 15845, 13, 400, 370, 341, 307, 689, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.14275454415215386, "compression_ratio": 1.6941747572815533, "no_speech_prob": 2.7928223062190227e-05}, {"id": 493, "seek": 318372, "start": 3199.24, "end": 3203.8799999999997, "text": " time just one sample. But if you call this in a for loop, you're going to get a cloud", "tokens": [50364, 2319, 1413, 11, 307, 516, 281, 976, 291, 2319, 819, 2793, 11, 439, 295, 552, 596, 38624, 294, 2992, 365, 257, 50790, 50790, 15845, 295, 9810, 3832, 25163, 13, 400, 370, 341, 307, 264, 1622, 597, 11247, 291, 633, 51140, 51140, 565, 445, 472, 6889, 13, 583, 498, 291, 818, 341, 294, 257, 337, 6367, 11, 291, 434, 516, 281, 483, 257, 4588, 51372, 51372, 295, 2793, 11, 439, 295, 552, 18988, 294, 2992, 11, 597, 575, 257, 2685, 15845, 13, 400, 370, 341, 307, 689, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.14275454415215386, "compression_ratio": 1.6941747572815533, "no_speech_prob": 2.7928223062190227e-05}, {"id": 494, "seek": 318372, "start": 3203.8799999999997, "end": 3210.48, "text": " of points, all of them centered in mu, which has a specific radius. And so this is where", "tokens": [50364, 2319, 1413, 11, 307, 516, 281, 976, 291, 2319, 819, 2793, 11, 439, 295, 552, 596, 38624, 294, 2992, 365, 257, 50790, 50790, 15845, 295, 9810, 3832, 25163, 13, 400, 370, 341, 307, 264, 1622, 597, 11247, 291, 633, 51140, 51140, 565, 445, 472, 6889, 13, 583, 498, 291, 818, 341, 294, 257, 337, 6367, 11, 291, 434, 516, 281, 483, 257, 4588, 51372, 51372, 295, 2793, 11, 439, 295, 552, 18988, 294, 2992, 11, 597, 575, 257, 2685, 15845, 13, 400, 370, 341, 307, 689, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.14275454415215386, "compression_ratio": 1.6941747572815533, "no_speech_prob": 2.7928223062190227e-05}, {"id": 495, "seek": 321048, "start": 3210.48, "end": 3217.56, "text": " we get these bubbles. Come from the sampling of this thing, right?", "tokens": [50364, 321, 483, 613, 16295, 13, 2492, 490, 264, 21179, 295, 341, 551, 11, 558, 30, 50718, 50718, 583, 286, 362, 281, 1190, 309, 2319, 1413, 13, 50932, 50932, 759, 291, 528, 2319, 10938, 11, 291, 362, 281, 1190, 309, 2319, 1413, 13, 639, 1085, 12835, 2398, 2144, 2709, 291, 51344, 51344, 633, 565, 257, 819, 935, 11, 597, 307, 13075, 1602, 538, 341, 4914, 293, 341, 733, 295, 5523, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.2209899747693861, "compression_ratio": 1.558011049723757, "no_speech_prob": 2.4290004148497246e-05}, {"id": 496, "seek": 321048, "start": 3217.56, "end": 3221.84, "text": " But I have to run it 100 times.", "tokens": [50364, 321, 483, 613, 16295, 13, 2492, 490, 264, 21179, 295, 341, 551, 11, 558, 30, 50718, 50718, 583, 286, 362, 281, 1190, 309, 2319, 1413, 13, 50932, 50932, 759, 291, 528, 2319, 10938, 11, 291, 362, 281, 1190, 309, 2319, 1413, 13, 639, 1085, 12835, 2398, 2144, 2709, 291, 51344, 51344, 633, 565, 257, 819, 935, 11, 597, 307, 13075, 1602, 538, 341, 4914, 293, 341, 733, 295, 5523, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.2209899747693861, "compression_ratio": 1.558011049723757, "no_speech_prob": 2.4290004148497246e-05}, {"id": 497, "seek": 321048, "start": 3221.84, "end": 3230.08, "text": " If you want 100 samples, you have to run it 100 times. This reparameterization gives you", "tokens": [50364, 321, 483, 613, 16295, 13, 2492, 490, 264, 21179, 295, 341, 551, 11, 558, 30, 50718, 50718, 583, 286, 362, 281, 1190, 309, 2319, 1413, 13, 50932, 50932, 759, 291, 528, 2319, 10938, 11, 291, 362, 281, 1190, 309, 2319, 1413, 13, 639, 1085, 12835, 2398, 2144, 2709, 291, 51344, 51344, 633, 565, 257, 819, 935, 11, 597, 307, 13075, 1602, 538, 341, 4914, 293, 341, 733, 295, 5523, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.2209899747693861, "compression_ratio": 1.558011049723757, "no_speech_prob": 2.4290004148497246e-05}, {"id": 498, "seek": 321048, "start": 3230.08, "end": 3238.84, "text": " every time a different point, which is parameterized by this location and this kind of volume.", "tokens": [50364, 321, 483, 613, 16295, 13, 2492, 490, 264, 21179, 295, 341, 551, 11, 558, 30, 50718, 50718, 583, 286, 362, 281, 1190, 309, 2319, 1413, 13, 50932, 50932, 759, 291, 528, 2319, 10938, 11, 291, 362, 281, 1190, 309, 2319, 1413, 13, 639, 1085, 12835, 2398, 2144, 2709, 291, 51344, 51344, 633, 565, 257, 819, 935, 11, 597, 307, 13075, 1602, 538, 341, 4914, 293, 341, 733, 295, 5523, 13, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.2209899747693861, "compression_ratio": 1.558011049723757, "no_speech_prob": 2.4290004148497246e-05}, {"id": 499, "seek": 323884, "start": 3238.84, "end": 3246.84, "text": " Yeah, and this comes from the mu and log variance comes from one sample, one input example.", "tokens": [50364, 865, 11, 293, 341, 1487, 490, 264, 2992, 293, 3565, 21977, 1487, 490, 472, 6889, 11, 472, 4846, 1365, 13, 50764, 50764, 865, 11, 1338, 11, 1338, 13, 407, 452, 472, 4846, 2031, 510, 2709, 385, 472, 2992, 293, 2709, 385, 472, 3565, 8517, 13, 400, 341, 51112, 51112, 472, 2992, 293, 472, 3565, 8517, 2709, 385, 710, 11, 597, 307, 472, 6889, 490, 264, 1379, 7316, 13, 759, 51498, 51498, 291, 1190, 341, 2445, 510, 9714, 1413, 11, 291, 434, 516, 281, 483, 9714, 710, 311, 11, 597, 439, 295, 552, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.18203669786453247, "compression_ratio": 1.8298969072164948, "no_speech_prob": 6.107932858867571e-05}, {"id": 500, "seek": 323884, "start": 3246.84, "end": 3253.8, "text": " Yeah, yeah, yeah. So my one input x here gives me one mu and gives me one logvar. And this", "tokens": [50364, 865, 11, 293, 341, 1487, 490, 264, 2992, 293, 3565, 21977, 1487, 490, 472, 6889, 11, 472, 4846, 1365, 13, 50764, 50764, 865, 11, 1338, 11, 1338, 13, 407, 452, 472, 4846, 2031, 510, 2709, 385, 472, 2992, 293, 2709, 385, 472, 3565, 8517, 13, 400, 341, 51112, 51112, 472, 2992, 293, 472, 3565, 8517, 2709, 385, 710, 11, 597, 307, 472, 6889, 490, 264, 1379, 7316, 13, 759, 51498, 51498, 291, 1190, 341, 2445, 510, 9714, 1413, 11, 291, 434, 516, 281, 483, 9714, 710, 311, 11, 597, 439, 295, 552, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.18203669786453247, "compression_ratio": 1.8298969072164948, "no_speech_prob": 6.107932858867571e-05}, {"id": 501, "seek": 323884, "start": 3253.8, "end": 3261.52, "text": " one mu and one logvar gives me z, which is one sample from the whole distribution. If", "tokens": [50364, 865, 11, 293, 341, 1487, 490, 264, 2992, 293, 3565, 21977, 1487, 490, 472, 6889, 11, 472, 4846, 1365, 13, 50764, 50764, 865, 11, 1338, 11, 1338, 13, 407, 452, 472, 4846, 2031, 510, 2709, 385, 472, 2992, 293, 2709, 385, 472, 3565, 8517, 13, 400, 341, 51112, 51112, 472, 2992, 293, 472, 3565, 8517, 2709, 385, 710, 11, 597, 307, 472, 6889, 490, 264, 1379, 7316, 13, 759, 51498, 51498, 291, 1190, 341, 2445, 510, 9714, 1413, 11, 291, 434, 516, 281, 483, 9714, 710, 311, 11, 597, 439, 295, 552, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.18203669786453247, "compression_ratio": 1.8298969072164948, "no_speech_prob": 6.107932858867571e-05}, {"id": 502, "seek": 323884, "start": 3261.52, "end": 3268.4, "text": " you run this function here 1000 times, you're going to get 1000 z's, which all of them", "tokens": [50364, 865, 11, 293, 341, 1487, 490, 264, 2992, 293, 3565, 21977, 1487, 490, 472, 6889, 11, 472, 4846, 1365, 13, 50764, 50764, 865, 11, 1338, 11, 1338, 13, 407, 452, 472, 4846, 2031, 510, 2709, 385, 472, 2992, 293, 2709, 385, 472, 3565, 8517, 13, 400, 341, 51112, 51112, 472, 2992, 293, 472, 3565, 8517, 2709, 385, 710, 11, 597, 307, 472, 6889, 490, 264, 1379, 7316, 13, 759, 51498, 51498, 291, 1190, 341, 2445, 510, 9714, 1413, 11, 291, 434, 516, 281, 483, 9714, 710, 311, 11, 597, 439, 295, 552, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.18203669786453247, "compression_ratio": 1.8298969072164948, "no_speech_prob": 6.107932858867571e-05}, {"id": 503, "seek": 326840, "start": 3268.4, "end": 3271.96, "text": " will take this volume, right?", "tokens": [50364, 486, 747, 341, 5523, 11, 558, 30, 50542, 50542, 1033, 13, 5803, 309, 13, 5803, 309, 13, 1044, 291, 13, 50736, 50736, 2720, 1164, 13, 50786, 50786, 286, 632, 257, 1168, 466, 2058, 378, 433, 293, 979, 4789, 294, 2674, 13, 467, 1542, 411, 294, 341, 11420, 11, 51142, 51142, 309, 311, 6457, 15325, 294, 2115, 295, 309, 445, 575, 257, 1916, 8213, 7914, 365, 257, 1300, 43, 52, 51384, 51384, 293, 257, 4556, 3280, 327, 13, 286, 600, 8046, 1612, 2058, 378, 433, 689, 436, 434, 1228, 3202, 293, 439, 341, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.25268534819285077, "compression_ratio": 1.5, "no_speech_prob": 4.832266495213844e-05}, {"id": 504, "seek": 326840, "start": 3271.96, "end": 3275.84, "text": " Okay. Got it. Got it. Thank you.", "tokens": [50364, 486, 747, 341, 5523, 11, 558, 30, 50542, 50542, 1033, 13, 5803, 309, 13, 5803, 309, 13, 1044, 291, 13, 50736, 50736, 2720, 1164, 13, 50786, 50786, 286, 632, 257, 1168, 466, 2058, 378, 433, 293, 979, 4789, 294, 2674, 13, 467, 1542, 411, 294, 341, 11420, 11, 51142, 51142, 309, 311, 6457, 15325, 294, 2115, 295, 309, 445, 575, 257, 1916, 8213, 7914, 365, 257, 1300, 43, 52, 51384, 51384, 293, 257, 4556, 3280, 327, 13, 286, 600, 8046, 1612, 2058, 378, 433, 689, 436, 434, 1228, 3202, 293, 439, 341, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.25268534819285077, "compression_ratio": 1.5, "no_speech_prob": 4.832266495213844e-05}, {"id": 505, "seek": 326840, "start": 3275.84, "end": 3276.84, "text": " Of course.", "tokens": [50364, 486, 747, 341, 5523, 11, 558, 30, 50542, 50542, 1033, 13, 5803, 309, 13, 5803, 309, 13, 1044, 291, 13, 50736, 50736, 2720, 1164, 13, 50786, 50786, 286, 632, 257, 1168, 466, 2058, 378, 433, 293, 979, 4789, 294, 2674, 13, 467, 1542, 411, 294, 341, 11420, 11, 51142, 51142, 309, 311, 6457, 15325, 294, 2115, 295, 309, 445, 575, 257, 1916, 8213, 7914, 365, 257, 1300, 43, 52, 51384, 51384, 293, 257, 4556, 3280, 327, 13, 286, 600, 8046, 1612, 2058, 378, 433, 689, 436, 434, 1228, 3202, 293, 439, 341, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.25268534819285077, "compression_ratio": 1.5, "no_speech_prob": 4.832266495213844e-05}, {"id": 506, "seek": 326840, "start": 3276.84, "end": 3283.96, "text": " I had a question about encoders and decodes in general. It looks like in this implementation,", "tokens": [50364, 486, 747, 341, 5523, 11, 558, 30, 50542, 50542, 1033, 13, 5803, 309, 13, 5803, 309, 13, 1044, 291, 13, 50736, 50736, 2720, 1164, 13, 50786, 50786, 286, 632, 257, 1168, 466, 2058, 378, 433, 293, 979, 4789, 294, 2674, 13, 467, 1542, 411, 294, 341, 11420, 11, 51142, 51142, 309, 311, 6457, 15325, 294, 2115, 295, 309, 445, 575, 257, 1916, 8213, 7914, 365, 257, 1300, 43, 52, 51384, 51384, 293, 257, 4556, 3280, 327, 13, 286, 600, 8046, 1612, 2058, 378, 433, 689, 436, 434, 1228, 3202, 293, 439, 341, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.25268534819285077, "compression_ratio": 1.5, "no_speech_prob": 4.832266495213844e-05}, {"id": 507, "seek": 326840, "start": 3283.96, "end": 3288.8, "text": " it's fairly straightforward in terms of it just has a couple linear layers with a ReLU", "tokens": [50364, 486, 747, 341, 5523, 11, 558, 30, 50542, 50542, 1033, 13, 5803, 309, 13, 5803, 309, 13, 1044, 291, 13, 50736, 50736, 2720, 1164, 13, 50786, 50786, 286, 632, 257, 1168, 466, 2058, 378, 433, 293, 979, 4789, 294, 2674, 13, 467, 1542, 411, 294, 341, 11420, 11, 51142, 51142, 309, 311, 6457, 15325, 294, 2115, 295, 309, 445, 575, 257, 1916, 8213, 7914, 365, 257, 1300, 43, 52, 51384, 51384, 293, 257, 4556, 3280, 327, 13, 286, 600, 8046, 1612, 2058, 378, 433, 689, 436, 434, 1228, 3202, 293, 439, 341, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.25268534819285077, "compression_ratio": 1.5, "no_speech_prob": 4.832266495213844e-05}, {"id": 508, "seek": 326840, "start": 3288.8, "end": 3296.56, "text": " and a sigmoid. I've previously seen encoders where they're using attention and all this", "tokens": [50364, 486, 747, 341, 5523, 11, 558, 30, 50542, 50542, 1033, 13, 5803, 309, 13, 5803, 309, 13, 1044, 291, 13, 50736, 50736, 2720, 1164, 13, 50786, 50786, 286, 632, 257, 1168, 466, 2058, 378, 433, 293, 979, 4789, 294, 2674, 13, 467, 1542, 411, 294, 341, 11420, 11, 51142, 51142, 309, 311, 6457, 15325, 294, 2115, 295, 309, 445, 575, 257, 1916, 8213, 7914, 365, 257, 1300, 43, 52, 51384, 51384, 293, 257, 4556, 3280, 327, 13, 286, 600, 8046, 1612, 2058, 378, 433, 689, 436, 434, 1228, 3202, 293, 439, 341, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.25268534819285077, "compression_ratio": 1.5, "no_speech_prob": 4.832266495213844e-05}, {"id": 509, "seek": 329656, "start": 3296.56, "end": 3305.2799999999997, "text": " stuff. Is this something as basic as this? It seems like it's pretty satisfactory. Are", "tokens": [50364, 1507, 13, 1119, 341, 746, 382, 3875, 382, 341, 30, 467, 2544, 411, 309, 311, 1238, 48614, 13, 2014, 50800, 50800, 436, 2673, 341, 3875, 420, 544, 3997, 30, 50926, 50926, 1033, 11, 1392, 13, 663, 390, 11, 286, 519, 11, 2787, 3129, 337, 385, 13, 407, 1203, 321, 536, 294, 1508, 307, 721, 51376, 51376, 300, 286, 600, 3031, 11, 309, 1985, 11, 293, 309, 311, 6457, 12424, 295, 437, 307, 11563, 281, 483, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.22116289259512215, "compression_ratio": 1.473170731707317, "no_speech_prob": 5.1412207540124655e-05}, {"id": 510, "seek": 329656, "start": 3305.2799999999997, "end": 3307.7999999999997, "text": " they usually this basic or more complex?", "tokens": [50364, 1507, 13, 1119, 341, 746, 382, 3875, 382, 341, 30, 467, 2544, 411, 309, 311, 1238, 48614, 13, 2014, 50800, 50800, 436, 2673, 341, 3875, 420, 544, 3997, 30, 50926, 50926, 1033, 11, 1392, 13, 663, 390, 11, 286, 519, 11, 2787, 3129, 337, 385, 13, 407, 1203, 321, 536, 294, 1508, 307, 721, 51376, 51376, 300, 286, 600, 3031, 11, 309, 1985, 11, 293, 309, 311, 6457, 12424, 295, 437, 307, 11563, 281, 483, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.22116289259512215, "compression_ratio": 1.473170731707317, "no_speech_prob": 5.1412207540124655e-05}, {"id": 511, "seek": 329656, "start": 3307.7999999999997, "end": 3316.7999999999997, "text": " Okay, okay. That was, I think, softball for me. So everything we see in class is things", "tokens": [50364, 1507, 13, 1119, 341, 746, 382, 3875, 382, 341, 30, 467, 2544, 411, 309, 311, 1238, 48614, 13, 2014, 50800, 50800, 436, 2673, 341, 3875, 420, 544, 3997, 30, 50926, 50926, 1033, 11, 1392, 13, 663, 390, 11, 286, 519, 11, 2787, 3129, 337, 385, 13, 407, 1203, 321, 536, 294, 1508, 307, 721, 51376, 51376, 300, 286, 600, 3031, 11, 309, 1985, 11, 293, 309, 311, 6457, 12424, 295, 437, 307, 11563, 281, 483, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.22116289259512215, "compression_ratio": 1.473170731707317, "no_speech_prob": 5.1412207540124655e-05}, {"id": 512, "seek": 329656, "start": 3316.7999999999997, "end": 3323.68, "text": " that I've tried, it works, and it's fairly representative of what is sufficient to get", "tokens": [50364, 1507, 13, 1119, 341, 746, 382, 3875, 382, 341, 30, 467, 2544, 411, 309, 311, 1238, 48614, 13, 2014, 50800, 50800, 436, 2673, 341, 3875, 420, 544, 3997, 30, 50926, 50926, 1033, 11, 1392, 13, 663, 390, 11, 286, 519, 11, 2787, 3129, 337, 385, 13, 407, 1203, 321, 536, 294, 1508, 307, 721, 51376, 51376, 300, 286, 600, 3031, 11, 309, 1985, 11, 293, 309, 311, 6457, 12424, 295, 437, 307, 11563, 281, 483, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.22116289259512215, "compression_ratio": 1.473170731707317, "no_speech_prob": 5.1412207540124655e-05}, {"id": 513, "seek": 332368, "start": 3323.68, "end": 3330.3999999999996, "text": " this stuff to run. So I'm running on my laptop on the MNIST dataset. You can run several", "tokens": [50364, 341, 1507, 281, 1190, 13, 407, 286, 478, 2614, 322, 452, 10732, 322, 264, 376, 45, 19756, 28872, 13, 509, 393, 1190, 2940, 50700, 50700, 295, 613, 733, 295, 6921, 293, 862, 13, 400, 370, 965, 321, 362, 1612, 577, 291, 393, 2058, 1429, 11, 577, 393, 291, 51048, 51048, 3089, 493, 257, 12990, 8399, 22660, 19866, 13, 400, 439, 291, 643, 307, 1045, 3876, 11, 1451, 3876, 295, 3089, 11, 597, 51344, 51344, 366, 411, 11, 437, 366, 264, 7300, 1296, 264, 11121, 8399, 22660, 19866, 11, 558, 30, 400, 370, 264, 2649, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.16005880978642678, "compression_ratio": 1.6712328767123288, "no_speech_prob": 6.746039161953377e-06}, {"id": 514, "seek": 332368, "start": 3330.3999999999996, "end": 3337.3599999999997, "text": " of these kind of tests and play. And so today we have seen how you can encode, how can you", "tokens": [50364, 341, 1507, 281, 1190, 13, 407, 286, 478, 2614, 322, 452, 10732, 322, 264, 376, 45, 19756, 28872, 13, 509, 393, 1190, 2940, 50700, 50700, 295, 613, 733, 295, 6921, 293, 862, 13, 400, 370, 965, 321, 362, 1612, 577, 291, 393, 2058, 1429, 11, 577, 393, 291, 51048, 51048, 3089, 493, 257, 12990, 8399, 22660, 19866, 13, 400, 439, 291, 643, 307, 1045, 3876, 11, 1451, 3876, 295, 3089, 11, 597, 51344, 51344, 366, 411, 11, 437, 366, 264, 7300, 1296, 264, 11121, 8399, 22660, 19866, 11, 558, 30, 400, 370, 264, 2649, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.16005880978642678, "compression_ratio": 1.6712328767123288, "no_speech_prob": 6.746039161953377e-06}, {"id": 515, "seek": 332368, "start": 3337.3599999999997, "end": 3343.2799999999997, "text": " code up a variation autoencoder. And all you need is three lines, four lines of code, which", "tokens": [50364, 341, 1507, 281, 1190, 13, 407, 286, 478, 2614, 322, 452, 10732, 322, 264, 376, 45, 19756, 28872, 13, 509, 393, 1190, 2940, 50700, 50700, 295, 613, 733, 295, 6921, 293, 862, 13, 400, 370, 965, 321, 362, 1612, 577, 291, 393, 2058, 1429, 11, 577, 393, 291, 51048, 51048, 3089, 493, 257, 12990, 8399, 22660, 19866, 13, 400, 439, 291, 643, 307, 1045, 3876, 11, 1451, 3876, 295, 3089, 11, 597, 51344, 51344, 366, 411, 11, 437, 366, 264, 7300, 1296, 264, 11121, 8399, 22660, 19866, 11, 558, 30, 400, 370, 264, 2649, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.16005880978642678, "compression_ratio": 1.6712328767123288, "no_speech_prob": 6.746039161953377e-06}, {"id": 516, "seek": 332368, "start": 3343.2799999999997, "end": 3348.44, "text": " are like, what are the differences between the plain autoencoder, right? And so the difference", "tokens": [50364, 341, 1507, 281, 1190, 13, 407, 286, 478, 2614, 322, 452, 10732, 322, 264, 376, 45, 19756, 28872, 13, 509, 393, 1190, 2940, 50700, 50700, 295, 613, 733, 295, 6921, 293, 862, 13, 400, 370, 965, 321, 362, 1612, 577, 291, 393, 2058, 1429, 11, 577, 393, 291, 51048, 51048, 3089, 493, 257, 12990, 8399, 22660, 19866, 13, 400, 439, 291, 643, 307, 1045, 3876, 11, 1451, 3876, 295, 3089, 11, 597, 51344, 51344, 366, 411, 11, 437, 366, 264, 7300, 1296, 264, 11121, 8399, 22660, 19866, 11, 558, 30, 400, 370, 264, 2649, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.16005880978642678, "compression_ratio": 1.6712328767123288, "no_speech_prob": 6.746039161953377e-06}, {"id": 517, "seek": 334844, "start": 3348.44, "end": 3358.08, "text": " is you have the reparameterization, reparameterized module method here. And then just these three", "tokens": [50364, 307, 291, 362, 264, 1085, 12835, 2398, 2144, 11, 1085, 12835, 2398, 1602, 10088, 3170, 510, 13, 400, 550, 445, 613, 1045, 50846, 50846, 3876, 670, 510, 11, 558, 30, 407, 291, 362, 2309, 3876, 1804, 264, 4972, 30867, 13, 440, 9482, 11, 51258, 51258, 300, 311, 2584, 819, 13, 407, 309, 311, 2584, 41488, 11, 558, 30, 1485, 551, 307, 516, 281, 312, 51490, 51490, 264, 9482, 11, 597, 307, 2361, 322, 264, 2190, 4846, 13, 509, 393, 764, 257, 45216, 304, 2533, 11, 291, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.17057556576199, "compression_ratio": 1.6756756756756757, "no_speech_prob": 1.14769745778176e-05}, {"id": 518, "seek": 334844, "start": 3358.08, "end": 3366.32, "text": " lines over here, right? So you have six lines plus the relative entropy. The architecture,", "tokens": [50364, 307, 291, 362, 264, 1085, 12835, 2398, 2144, 11, 1085, 12835, 2398, 1602, 10088, 3170, 510, 13, 400, 550, 445, 613, 1045, 50846, 50846, 3876, 670, 510, 11, 558, 30, 407, 291, 362, 2309, 3876, 1804, 264, 4972, 30867, 13, 440, 9482, 11, 51258, 51258, 300, 311, 2584, 819, 13, 407, 309, 311, 2584, 41488, 11, 558, 30, 1485, 551, 307, 516, 281, 312, 51490, 51490, 264, 9482, 11, 597, 307, 2361, 322, 264, 2190, 4846, 13, 509, 393, 764, 257, 45216, 304, 2533, 11, 291, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.17057556576199, "compression_ratio": 1.6756756756756757, "no_speech_prob": 1.14769745778176e-05}, {"id": 519, "seek": 334844, "start": 3366.32, "end": 3370.96, "text": " that's completely different. So it's completely orthogonal, right? One thing is going to be", "tokens": [50364, 307, 291, 362, 264, 1085, 12835, 2398, 2144, 11, 1085, 12835, 2398, 1602, 10088, 3170, 510, 13, 400, 550, 445, 613, 1045, 50846, 50846, 3876, 670, 510, 11, 558, 30, 407, 291, 362, 2309, 3876, 1804, 264, 4972, 30867, 13, 440, 9482, 11, 51258, 51258, 300, 311, 2584, 819, 13, 407, 309, 311, 2584, 41488, 11, 558, 30, 1485, 551, 307, 516, 281, 312, 51490, 51490, 264, 9482, 11, 597, 307, 2361, 322, 264, 2190, 4846, 13, 509, 393, 764, 257, 45216, 304, 2533, 11, 291, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.17057556576199, "compression_ratio": 1.6756756756756757, "no_speech_prob": 1.14769745778176e-05}, {"id": 520, "seek": 334844, "start": 3370.96, "end": 3374.7200000000003, "text": " the architecture, which is based on the current input. You can use a convolutional net, you", "tokens": [50364, 307, 291, 362, 264, 1085, 12835, 2398, 2144, 11, 1085, 12835, 2398, 1602, 10088, 3170, 510, 13, 400, 550, 445, 613, 1045, 50846, 50846, 3876, 670, 510, 11, 558, 30, 407, 291, 362, 2309, 3876, 1804, 264, 4972, 30867, 13, 440, 9482, 11, 51258, 51258, 300, 311, 2584, 819, 13, 407, 309, 311, 2584, 41488, 11, 558, 30, 1485, 551, 307, 516, 281, 312, 51490, 51490, 264, 9482, 11, 597, 307, 2361, 322, 264, 2190, 4846, 13, 509, 393, 764, 257, 45216, 304, 2533, 11, 291, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.17057556576199, "compression_ratio": 1.6756756756756757, "no_speech_prob": 1.14769745778176e-05}, {"id": 521, "seek": 337472, "start": 3374.72, "end": 3378.3999999999996, "text": " can use a recurrent net, you can use anything you want. And the other thing is the fact", "tokens": [50364, 393, 764, 257, 18680, 1753, 2533, 11, 291, 393, 764, 1340, 291, 528, 13, 400, 264, 661, 551, 307, 264, 1186, 50548, 50548, 300, 291, 7620, 512, 15957, 3142, 3209, 666, 257, 3209, 300, 4045, 291, 281, 6889, 293, 50938, 50938, 550, 8460, 10938, 490, 257, 7316, 13, 407, 321, 1128, 632, 281, 751, 466, 37870, 51272, 51272, 949, 13, 492, 994, 380, 458, 577, 281, 8460, 37870, 13, 823, 365, 1337, 1166, 2316, 11, 291, 393, 767, 51546, 51546], "temperature": 0.0, "avg_logprob": -0.10874676122898008, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.0951204785669688e-05}, {"id": 522, "seek": 337472, "start": 3378.3999999999996, "end": 3386.2, "text": " that you convert some deterministic network into a network that allows you to sample and", "tokens": [50364, 393, 764, 257, 18680, 1753, 2533, 11, 291, 393, 764, 1340, 291, 528, 13, 400, 264, 661, 551, 307, 264, 1186, 50548, 50548, 300, 291, 7620, 512, 15957, 3142, 3209, 666, 257, 3209, 300, 4045, 291, 281, 6889, 293, 50938, 50938, 550, 8460, 10938, 490, 257, 7316, 13, 407, 321, 1128, 632, 281, 751, 466, 37870, 51272, 51272, 949, 13, 492, 994, 380, 458, 577, 281, 8460, 37870, 13, 823, 365, 1337, 1166, 2316, 11, 291, 393, 767, 51546, 51546], "temperature": 0.0, "avg_logprob": -0.10874676122898008, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.0951204785669688e-05}, {"id": 523, "seek": 337472, "start": 3386.2, "end": 3392.8799999999997, "text": " then generate samples from a distribution. So we never had to talk about distributions", "tokens": [50364, 393, 764, 257, 18680, 1753, 2533, 11, 291, 393, 764, 1340, 291, 528, 13, 400, 264, 661, 551, 307, 264, 1186, 50548, 50548, 300, 291, 7620, 512, 15957, 3142, 3209, 666, 257, 3209, 300, 4045, 291, 281, 6889, 293, 50938, 50938, 550, 8460, 10938, 490, 257, 7316, 13, 407, 321, 1128, 632, 281, 751, 466, 37870, 51272, 51272, 949, 13, 492, 994, 380, 458, 577, 281, 8460, 37870, 13, 823, 365, 1337, 1166, 2316, 11, 291, 393, 767, 51546, 51546], "temperature": 0.0, "avg_logprob": -0.10874676122898008, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.0951204785669688e-05}, {"id": 524, "seek": 337472, "start": 3392.8799999999997, "end": 3398.3599999999997, "text": " before. We didn't know how to generate distributions. Now with generative model, you can actually", "tokens": [50364, 393, 764, 257, 18680, 1753, 2533, 11, 291, 393, 764, 1340, 291, 528, 13, 400, 264, 661, 551, 307, 264, 1186, 50548, 50548, 300, 291, 7620, 512, 15957, 3142, 3209, 666, 257, 3209, 300, 4045, 291, 281, 6889, 293, 50938, 50938, 550, 8460, 10938, 490, 257, 7316, 13, 407, 321, 1128, 632, 281, 751, 466, 37870, 51272, 51272, 949, 13, 492, 994, 380, 458, 577, 281, 8460, 37870, 13, 823, 365, 1337, 1166, 2316, 11, 291, 393, 767, 51546, 51546], "temperature": 0.0, "avg_logprob": -0.10874676122898008, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.0951204785669688e-05}, {"id": 525, "seek": 339836, "start": 3398.36, "end": 3406.92, "text": " generate data, which are basically a, you know, how do you say, like a bending, a rotation", "tokens": [50364, 8460, 1412, 11, 597, 366, 1936, 257, 11, 291, 458, 11, 577, 360, 291, 584, 11, 411, 257, 22487, 11, 257, 12447, 50792, 50792, 420, 257, 9887, 295, 2035, 307, 3380, 39148, 11, 558, 30, 407, 321, 362, 341, 2120, 592, 3504, 473, 51106, 51106, 39148, 293, 550, 264, 979, 19866, 2516, 341, 2594, 293, 550, 309, 10854, 309, 281, 652, 309, 574, 411, 51410, 51410, 264, 4846, 13, 440, 4846, 815, 312, 411, 746, 24991, 13, 509, 362, 341, 12212, 510, 11, 341, 955, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.17834773760163383, "compression_ratio": 1.6108597285067874, "no_speech_prob": 2.1437546820379794e-05}, {"id": 526, "seek": 339836, "start": 3406.92, "end": 3413.2000000000003, "text": " or a transformation of whatever is original Gaussian, right? So we have this multivariate", "tokens": [50364, 8460, 1412, 11, 597, 366, 1936, 257, 11, 291, 458, 11, 577, 360, 291, 584, 11, 411, 257, 22487, 11, 257, 12447, 50792, 50792, 420, 257, 9887, 295, 2035, 307, 3380, 39148, 11, 558, 30, 407, 321, 362, 341, 2120, 592, 3504, 473, 51106, 51106, 39148, 293, 550, 264, 979, 19866, 2516, 341, 2594, 293, 550, 309, 10854, 309, 281, 652, 309, 574, 411, 51410, 51410, 264, 4846, 13, 440, 4846, 815, 312, 411, 746, 24991, 13, 509, 362, 341, 12212, 510, 11, 341, 955, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.17834773760163383, "compression_ratio": 1.6108597285067874, "no_speech_prob": 2.1437546820379794e-05}, {"id": 527, "seek": 339836, "start": 3413.2000000000003, "end": 3419.28, "text": " Gaussian and then the decoder takes this ball and then it shapes it to make it look like", "tokens": [50364, 8460, 1412, 11, 597, 366, 1936, 257, 11, 291, 458, 11, 577, 360, 291, 584, 11, 411, 257, 22487, 11, 257, 12447, 50792, 50792, 420, 257, 9887, 295, 2035, 307, 3380, 39148, 11, 558, 30, 407, 321, 362, 341, 2120, 592, 3504, 473, 51106, 51106, 39148, 293, 550, 264, 979, 19866, 2516, 341, 2594, 293, 550, 309, 10854, 309, 281, 652, 309, 574, 411, 51410, 51410, 264, 4846, 13, 440, 4846, 815, 312, 411, 746, 24991, 13, 509, 362, 341, 12212, 510, 11, 341, 955, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.17834773760163383, "compression_ratio": 1.6108597285067874, "no_speech_prob": 2.1437546820379794e-05}, {"id": 528, "seek": 339836, "start": 3419.28, "end": 3424.6, "text": " the input. The input may be like something curved. You have this bubble here, this big", "tokens": [50364, 8460, 1412, 11, 597, 366, 1936, 257, 11, 291, 458, 11, 577, 360, 291, 584, 11, 411, 257, 22487, 11, 257, 12447, 50792, 50792, 420, 257, 9887, 295, 2035, 307, 3380, 39148, 11, 558, 30, 407, 321, 362, 341, 2120, 592, 3504, 473, 51106, 51106, 39148, 293, 550, 264, 979, 19866, 2516, 341, 2594, 293, 550, 309, 10854, 309, 281, 652, 309, 574, 411, 51410, 51410, 264, 4846, 13, 440, 4846, 815, 312, 411, 746, 24991, 13, 509, 362, 341, 12212, 510, 11, 341, 955, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.17834773760163383, "compression_ratio": 1.6108597285067874, "no_speech_prob": 2.1437546820379794e-05}, {"id": 529, "seek": 342460, "start": 3424.6, "end": 3429.48, "text": " bubble of bubbles. And then you, the decoder gets it back to whatever it looks like, how", "tokens": [50364, 12212, 295, 16295, 13, 400, 550, 291, 11, 264, 979, 19866, 2170, 309, 646, 281, 2035, 309, 1542, 411, 11, 577, 50608, 50608, 264, 4846, 1542, 411, 13, 407, 439, 291, 643, 5946, 322, 264, 2685, 1412, 291, 434, 1228, 13, 1171, 376, 45, 19756, 11, 51046, 51046, 341, 307, 11563, 13, 759, 291, 434, 1228, 257, 45216, 304, 3037, 11, 309, 311, 516, 281, 312, 1364, 709, 1101, 13, 51366, 51366, 440, 935, 307, 300, 341, 1508, 390, 466, 3034, 1478, 1476, 22660, 19866, 11, 458, 577, 281, 483, 3219, 1507, 13, 1057, 51640, 51640, 264, 3219, 1507, 307, 2935, 11, 291, 458, 11, 5127, 2940, 295, 613, 721, 286, 600, 668, 4571, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.18171743619239936, "compression_ratio": 1.6325088339222615, "no_speech_prob": 1.536842501081992e-05}, {"id": 530, "seek": 342460, "start": 3429.48, "end": 3438.24, "text": " the input looks like. So all you need depends on the specific data you're using. For MNIST,", "tokens": [50364, 12212, 295, 16295, 13, 400, 550, 291, 11, 264, 979, 19866, 2170, 309, 646, 281, 2035, 309, 1542, 411, 11, 577, 50608, 50608, 264, 4846, 1542, 411, 13, 407, 439, 291, 643, 5946, 322, 264, 2685, 1412, 291, 434, 1228, 13, 1171, 376, 45, 19756, 11, 51046, 51046, 341, 307, 11563, 13, 759, 291, 434, 1228, 257, 45216, 304, 3037, 11, 309, 311, 516, 281, 312, 1364, 709, 1101, 13, 51366, 51366, 440, 935, 307, 300, 341, 1508, 390, 466, 3034, 1478, 1476, 22660, 19866, 11, 458, 577, 281, 483, 3219, 1507, 13, 1057, 51640, 51640, 264, 3219, 1507, 307, 2935, 11, 291, 458, 11, 5127, 2940, 295, 613, 721, 286, 600, 668, 4571, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.18171743619239936, "compression_ratio": 1.6325088339222615, "no_speech_prob": 1.536842501081992e-05}, {"id": 531, "seek": 342460, "start": 3438.24, "end": 3444.64, "text": " this is sufficient. If you're using a convolutional version, it's going to be working much better.", "tokens": [50364, 12212, 295, 16295, 13, 400, 550, 291, 11, 264, 979, 19866, 2170, 309, 646, 281, 2035, 309, 1542, 411, 11, 577, 50608, 50608, 264, 4846, 1542, 411, 13, 407, 439, 291, 643, 5946, 322, 264, 2685, 1412, 291, 434, 1228, 13, 1171, 376, 45, 19756, 11, 51046, 51046, 341, 307, 11563, 13, 759, 291, 434, 1228, 257, 45216, 304, 3037, 11, 309, 311, 516, 281, 312, 1364, 709, 1101, 13, 51366, 51366, 440, 935, 307, 300, 341, 1508, 390, 466, 3034, 1478, 1476, 22660, 19866, 11, 458, 577, 281, 483, 3219, 1507, 13, 1057, 51640, 51640, 264, 3219, 1507, 307, 2935, 11, 291, 458, 11, 5127, 2940, 295, 613, 721, 286, 600, 668, 4571, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.18171743619239936, "compression_ratio": 1.6325088339222615, "no_speech_prob": 1.536842501081992e-05}, {"id": 532, "seek": 342460, "start": 3444.64, "end": 3450.12, "text": " The point is that this class was about variational autencoder, know how to get crazy stuff. All", "tokens": [50364, 12212, 295, 16295, 13, 400, 550, 291, 11, 264, 979, 19866, 2170, 309, 646, 281, 2035, 309, 1542, 411, 11, 577, 50608, 50608, 264, 4846, 1542, 411, 13, 407, 439, 291, 643, 5946, 322, 264, 2685, 1412, 291, 434, 1228, 13, 1171, 376, 45, 19756, 11, 51046, 51046, 341, 307, 11563, 13, 759, 291, 434, 1228, 257, 45216, 304, 3037, 11, 309, 311, 516, 281, 312, 1364, 709, 1101, 13, 51366, 51366, 440, 935, 307, 300, 341, 1508, 390, 466, 3034, 1478, 1476, 22660, 19866, 11, 458, 577, 281, 483, 3219, 1507, 13, 1057, 51640, 51640, 264, 3219, 1507, 307, 2935, 11, 291, 458, 11, 5127, 2940, 295, 613, 721, 286, 600, 668, 4571, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.18171743619239936, "compression_ratio": 1.6325088339222615, "no_speech_prob": 1.536842501081992e-05}, {"id": 533, "seek": 342460, "start": 3450.12, "end": 3453.7599999999998, "text": " the crazy stuff is simply, you know, adding several of these things I've been teaching", "tokens": [50364, 12212, 295, 16295, 13, 400, 550, 291, 11, 264, 979, 19866, 2170, 309, 646, 281, 2035, 309, 1542, 411, 11, 577, 50608, 50608, 264, 4846, 1542, 411, 13, 407, 439, 291, 643, 5946, 322, 264, 2685, 1412, 291, 434, 1228, 13, 1171, 376, 45, 19756, 11, 51046, 51046, 341, 307, 11563, 13, 759, 291, 434, 1228, 257, 45216, 304, 3037, 11, 309, 311, 516, 281, 312, 1364, 709, 1101, 13, 51366, 51366, 440, 935, 307, 300, 341, 1508, 390, 466, 3034, 1478, 1476, 22660, 19866, 11, 458, 577, 281, 483, 3219, 1507, 13, 1057, 51640, 51640, 264, 3219, 1507, 307, 2935, 11, 291, 458, 11, 5127, 2940, 295, 613, 721, 286, 600, 668, 4571, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.18171743619239936, "compression_ratio": 1.6325088339222615, "no_speech_prob": 1.536842501081992e-05}, {"id": 534, "seek": 345376, "start": 3453.76, "end": 3460.6800000000003, "text": " you so far. But the bit about variational autencoder, I think it was covered mostly", "tokens": [50364, 291, 370, 1400, 13, 583, 264, 857, 466, 3034, 1478, 1476, 22660, 19866, 11, 286, 519, 309, 390, 5343, 5240, 50710, 50710, 510, 13, 1033, 13, 1033, 13, 2561, 13, 5358, 1651, 30, 883, 13, 1033, 13, 1033, 13, 663, 390, 309, 13, 1033, 13, 1044, 291, 51218, 51218, 370, 709, 337, 5549, 505, 13, 1033, 13, 5198, 1920, 1411, 5285, 6856, 3008, 291, 958, 1243, 13, 4621, 13, 4621, 13, 51842], "temperature": 0.0, "avg_logprob": -0.16522314741804794, "compression_ratio": 1.5058823529411764, "no_speech_prob": 2.5863486371235922e-05}, {"id": 535, "seek": 345376, "start": 3460.6800000000003, "end": 3470.84, "text": " here. Okay. Okay. Thanks. Other questions? No. Okay. Okay. That was it. Okay. Thank you", "tokens": [50364, 291, 370, 1400, 13, 583, 264, 857, 466, 3034, 1478, 1476, 22660, 19866, 11, 286, 519, 309, 390, 5343, 5240, 50710, 50710, 510, 13, 1033, 13, 1033, 13, 2561, 13, 5358, 1651, 30, 883, 13, 1033, 13, 1033, 13, 663, 390, 309, 13, 1033, 13, 1044, 291, 51218, 51218, 370, 709, 337, 5549, 505, 13, 1033, 13, 5198, 1920, 1411, 5285, 6856, 3008, 291, 958, 1243, 13, 4621, 13, 4621, 13, 51842], "temperature": 0.0, "avg_logprob": -0.16522314741804794, "compression_ratio": 1.5058823529411764, "no_speech_prob": 2.5863486371235922e-05}, {"id": 536, "seek": 347084, "start": 3470.84, "end": 3483.32, "text": " so much for joining us. Okay. Everyone almost left 70%. See you next week. Bye. Bye.", "tokens": [50364, 370, 709, 337, 5549, 505, 13, 1033, 13, 5198, 1920, 1411, 5285, 6856, 3008, 291, 958, 1243, 13, 4621, 13, 4621, 13, 50988], "temperature": 0.0, "avg_logprob": -0.22535287857055664, "compression_ratio": 1.0, "no_speech_prob": 4.535759944701567e-05}], "language": "en", "video_id": "7Rb4s9wNOmc", "entity": "Yann LeCun"}}