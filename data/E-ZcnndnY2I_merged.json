{"video_id": "E-ZcnndnY2I", "title": "3.4 Cost function | Cost function for logistic regression  --[Machine Learning | Andrew Ng]", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 719, "views": 272, "publish_date": "11/04/2022", "timestamp": 1661126400, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " Remember that the cost function gives you a way to measure how well a specific set of parameters fits the training data, and it thereby gives you a way to try to choose better parameters. In this video, we'll look at how the squared error cost function is not an ideal cost function for logistic regression, and we'll take a look at a different cost function that can help us choose better parameters for logistic regression. Notice what the training set for a logistic regression model might look like, where here each row might correspond to a patient that was paying a visit to the doctor and wound up with some sort of diagnosis. As before, we'll use m to denote the number of training examples. Each training example has one or more features, such as the tumor size, the patient's age, and so on, for a total of n features. And so let's call the features x1 through xn. And since this is a binary classification task, the target label y takes on only two values, either 0 or 1. And finally, the logistic regression model is defined by this equation. Okay, so the question you want to answer is, given this training set, how can you choose parameters w and b? Recall for linear regression, this is the squared error cost function. The only thing I've changed is that I put the one half inside the summation instead of outside the summation. And you might remember that in the case of linear regression, where f of x is the linear function w dot x plus b, the cost function looks like this is a convex function or a bow shape or a hammock shape. And so gradient descent will look like this, where you take one step, one step, one step, and so on to converge at the global minimum. Now, you could try to use the same cost function for logistic regression, but it turns out that if I were to write f of x equals 1 over 1 plus e to the negative w x plus b, and plot the cost function using this value of f of x, then the cost will look like this. This becomes what's called a non-convex cost function, it's not convex. And what this means is that if you were to try to use gradient descent, there are lots of local minima that you can get stuck in. So it turns out that for logistic regression, this squared error cost function is not a good choice. Instead, there will be a different cost function that can make the cost function convex again, so that gradient descent can be guaranteed to converge to the global minimum. The only thing I've changed is that I put the one half inside the summation instead of outside the summation. This will make the math you see later on the slide a little bit simpler. In order to build a new cost function, one that we'll use for logistic regression, I'm going to change a little bit the definition of the cost function j of w and b. In particular, if you look inside the summation, let's call this term inside the loss on a single training example. And I'm going to denote the loss via this capital L, and is a function of the prediction of the learning algorithm f of x, as well as of the true label y. And so the loss, given the predicted f of x and the true label y, is equal in this case to one half of the squared difference. We'll see shortly that by choosing a different form for this loss function, we'll be able to keep the overall cost function, which is one over m times the sum of these loss functions, to be a convex function. Now the loss function inputs f of x and the true label y, and tells us how well we're doing on that example. I'm going to just write down here the definition of the loss function we'll use for logistic regression. If the label y is equal to one, then the loss is negative log of f of x. And if the label y is equal to zero, then the loss is negative log of one minus f of x. Let's take a look at why this loss function hopefully makes sense. Let's first consider the case of y equals one, and plot what this function looks like to gain some intuition about what this loss function is doing. And remember, the loss function measures how well you're doing on one training example, and is by summing up the losses on all of the training examples that you then get the cost function, which measures how well you're doing on the entire training set. So if you plot log of f, it looks like this curve here, where f here is on the horizontal axis. And so a plot of negative of the log of f looks like this, where we just flip the curve along the horizontal axis. Notice that it intersects the horizontal axis at f equals one, and continues downward from there. Now f is the output of logistic regression. Thus, f is always between zero and one, because the output of logistic regression is always between zero and one. The only part of the function that's relevant is therefore this part over here, corresponding to f between zero and one. So let's zoom in and take a closer look at this part of the graph. If the algorithm predicts a probability close to one, and the true label is one, then the loss is very small, it's pretty much zero, because you're very close to the right answer. Now continuing with the example of the true label y being one, so say it really is a malignant tumor, if the algorithm predicts 0.5, then the loss is at this point here, which is a bit higher, but not that high. Whereas in contrast, if the algorithm were to have output 0.1, if it thinks that there's only a 10% chance of the tumor being malignant, but y really is one, it really is malignant, then the loss is this much higher value over here. So when y is equal to one, the loss function incentivizes, or nudges, or helps push the algorithm to make more accurate predictions, because the loss is lowest when it predicts values close to one. Now on this slide, we've been looking at what the loss is when y is equal to one. On this slide, let's look at the second part of the loss function corresponding to when y is equal to zero. In this case, the loss is negative log of one minus f of x. When this function is plotted, it actually looks like this. The range of f is limited to zero to one, because logistic regression only outputs values between zero and one, and if we zoom in, this is what it looks like. So in this plot, corresponding to y equals zero, the vertical axis shows the value of the loss for different values of f of x. So when f is zero, or very close to zero, the loss is also going to be very small, which means that if the true label is zero and the model's prediction is very close to zero, well, you nearly got it right. So the loss is appropriately very close to zero. And the larger the value of f of x gets, the bigger the loss, because the prediction is further from the true label zero. And in fact, as that prediction approaches one, the loss actually approaches infinity. Going back to the tumor prediction example, this is if a model predicts that the patient's tumor is almost certain to be malignant, say 99.9% chance of malignancy, but it turns out to actually not be malignant. So y equals zero, then we penalize the model with a very high loss. So in this case of y equals zero, similar to the case of y equals one on the previous slide, the further the prediction f of x is away from the true value of y, the higher the loss. And in fact, if f of x approaches zero, the loss here actually goes really, really large and in fact approaches infinity. So when the true label is one, the algorithm is strongly incentivized not to predict something too close to zero. So in this video, you saw why the squared error cost function doesn't work well for logistic regression. We also defined the loss for a single training example and came up with a new definition for the loss function for logistic regression. It turns out that with this choice of loss function, the overall cost function will be convex and thus you can reliably use gradient descent to take you to the global minimum. Proving that this function is convex is beyond the scope of this course. You may remember that the cost function is a function of the entire training set and is therefore the average or one over m times the sum of the loss function on the individual training examples. So the cost on a certain set of parameters w and b is equal to one over m times the sum over all the training examples of the loss on the training examples. And if you can find the value of the parameters w and b that minimizes this, then you'd have a pretty good set of values for the parameters w and b for logistic regression. In the upcoming optional lab, you get to take a look at how the squared error cost function doesn't work very well for classification because you see that the surface plot results in a very wiggly cost surface with many local minima. Then you take a look at the new logistic loss function and as you can see here, this produces a nice and smooth convex surface plot that does not have all those local minima. So please take a look at the code and the plots after this video. Alright so we've seen a lot in this video. In the next video, let's go back and take the loss function for a single training example and use that to define the overall cost function for the entire training set. And we'll also figure out a simpler way to write out the cost function, which will then later allow us to run gradient descent to find good parameters for logistic regression. Let's go on to the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.96, "text": " Remember that the cost function gives you a way to measure how well a specific set of", "tokens": [50364, 5459, 300, 264, 2063, 2445, 2709, 291, 257, 636, 281, 3481, 577, 731, 257, 2685, 992, 295, 50712, 50712, 9834, 9001, 264, 3097, 1412, 11, 293, 309, 28281, 2709, 291, 257, 636, 281, 853, 281, 2826, 1101, 51010, 51010, 9834, 13, 51060, 51060, 682, 341, 960, 11, 321, 603, 574, 412, 577, 264, 8889, 6713, 2063, 2445, 307, 406, 364, 7157, 2063, 2445, 51390, 51390, 337, 3565, 3142, 24590, 11, 293, 321, 603, 747, 257, 574, 412, 257, 819, 2063, 2445, 300, 393, 51622, 51622, 854, 505, 2826, 1101, 9834, 337, 3565, 3142, 24590, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.14381353782885004, "compression_ratio": 1.9495412844036697, "no_speech_prob": 0.01064786221832037}, {"id": 1, "seek": 0, "start": 6.96, "end": 12.92, "text": " parameters fits the training data, and it thereby gives you a way to try to choose better", "tokens": [50364, 5459, 300, 264, 2063, 2445, 2709, 291, 257, 636, 281, 3481, 577, 731, 257, 2685, 992, 295, 50712, 50712, 9834, 9001, 264, 3097, 1412, 11, 293, 309, 28281, 2709, 291, 257, 636, 281, 853, 281, 2826, 1101, 51010, 51010, 9834, 13, 51060, 51060, 682, 341, 960, 11, 321, 603, 574, 412, 577, 264, 8889, 6713, 2063, 2445, 307, 406, 364, 7157, 2063, 2445, 51390, 51390, 337, 3565, 3142, 24590, 11, 293, 321, 603, 747, 257, 574, 412, 257, 819, 2063, 2445, 300, 393, 51622, 51622, 854, 505, 2826, 1101, 9834, 337, 3565, 3142, 24590, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.14381353782885004, "compression_ratio": 1.9495412844036697, "no_speech_prob": 0.01064786221832037}, {"id": 2, "seek": 0, "start": 12.92, "end": 13.92, "text": " parameters.", "tokens": [50364, 5459, 300, 264, 2063, 2445, 2709, 291, 257, 636, 281, 3481, 577, 731, 257, 2685, 992, 295, 50712, 50712, 9834, 9001, 264, 3097, 1412, 11, 293, 309, 28281, 2709, 291, 257, 636, 281, 853, 281, 2826, 1101, 51010, 51010, 9834, 13, 51060, 51060, 682, 341, 960, 11, 321, 603, 574, 412, 577, 264, 8889, 6713, 2063, 2445, 307, 406, 364, 7157, 2063, 2445, 51390, 51390, 337, 3565, 3142, 24590, 11, 293, 321, 603, 747, 257, 574, 412, 257, 819, 2063, 2445, 300, 393, 51622, 51622, 854, 505, 2826, 1101, 9834, 337, 3565, 3142, 24590, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.14381353782885004, "compression_ratio": 1.9495412844036697, "no_speech_prob": 0.01064786221832037}, {"id": 3, "seek": 0, "start": 13.92, "end": 20.52, "text": " In this video, we'll look at how the squared error cost function is not an ideal cost function", "tokens": [50364, 5459, 300, 264, 2063, 2445, 2709, 291, 257, 636, 281, 3481, 577, 731, 257, 2685, 992, 295, 50712, 50712, 9834, 9001, 264, 3097, 1412, 11, 293, 309, 28281, 2709, 291, 257, 636, 281, 853, 281, 2826, 1101, 51010, 51010, 9834, 13, 51060, 51060, 682, 341, 960, 11, 321, 603, 574, 412, 577, 264, 8889, 6713, 2063, 2445, 307, 406, 364, 7157, 2063, 2445, 51390, 51390, 337, 3565, 3142, 24590, 11, 293, 321, 603, 747, 257, 574, 412, 257, 819, 2063, 2445, 300, 393, 51622, 51622, 854, 505, 2826, 1101, 9834, 337, 3565, 3142, 24590, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.14381353782885004, "compression_ratio": 1.9495412844036697, "no_speech_prob": 0.01064786221832037}, {"id": 4, "seek": 0, "start": 20.52, "end": 25.16, "text": " for logistic regression, and we'll take a look at a different cost function that can", "tokens": [50364, 5459, 300, 264, 2063, 2445, 2709, 291, 257, 636, 281, 3481, 577, 731, 257, 2685, 992, 295, 50712, 50712, 9834, 9001, 264, 3097, 1412, 11, 293, 309, 28281, 2709, 291, 257, 636, 281, 853, 281, 2826, 1101, 51010, 51010, 9834, 13, 51060, 51060, 682, 341, 960, 11, 321, 603, 574, 412, 577, 264, 8889, 6713, 2063, 2445, 307, 406, 364, 7157, 2063, 2445, 51390, 51390, 337, 3565, 3142, 24590, 11, 293, 321, 603, 747, 257, 574, 412, 257, 819, 2063, 2445, 300, 393, 51622, 51622, 854, 505, 2826, 1101, 9834, 337, 3565, 3142, 24590, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.14381353782885004, "compression_ratio": 1.9495412844036697, "no_speech_prob": 0.01064786221832037}, {"id": 5, "seek": 0, "start": 25.16, "end": 29.28, "text": " help us choose better parameters for logistic regression.", "tokens": [50364, 5459, 300, 264, 2063, 2445, 2709, 291, 257, 636, 281, 3481, 577, 731, 257, 2685, 992, 295, 50712, 50712, 9834, 9001, 264, 3097, 1412, 11, 293, 309, 28281, 2709, 291, 257, 636, 281, 853, 281, 2826, 1101, 51010, 51010, 9834, 13, 51060, 51060, 682, 341, 960, 11, 321, 603, 574, 412, 577, 264, 8889, 6713, 2063, 2445, 307, 406, 364, 7157, 2063, 2445, 51390, 51390, 337, 3565, 3142, 24590, 11, 293, 321, 603, 747, 257, 574, 412, 257, 819, 2063, 2445, 300, 393, 51622, 51622, 854, 505, 2826, 1101, 9834, 337, 3565, 3142, 24590, 13, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.14381353782885004, "compression_ratio": 1.9495412844036697, "no_speech_prob": 0.01064786221832037}, {"id": 6, "seek": 2928, "start": 29.28, "end": 35.1, "text": " Notice what the training set for a logistic regression model might look like, where here", "tokens": [50364, 13428, 437, 264, 3097, 992, 337, 257, 3565, 3142, 24590, 2316, 1062, 574, 411, 11, 689, 510, 50655, 50655, 1184, 5386, 1062, 6805, 281, 257, 4537, 300, 390, 6229, 257, 3441, 281, 264, 4631, 293, 10999, 50972, 50972, 493, 365, 512, 1333, 295, 15217, 13, 51142, 51142, 1018, 949, 11, 321, 603, 764, 275, 281, 45708, 264, 1230, 295, 3097, 5110, 13, 51466, 51466, 6947, 3097, 1365, 575, 472, 420, 544, 4122, 11, 1270, 382, 264, 22512, 2744, 11, 264, 4537, 311, 3205, 11, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.11094946211034601, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.705042399815284e-05}, {"id": 7, "seek": 2928, "start": 35.1, "end": 41.44, "text": " each row might correspond to a patient that was paying a visit to the doctor and wound", "tokens": [50364, 13428, 437, 264, 3097, 992, 337, 257, 3565, 3142, 24590, 2316, 1062, 574, 411, 11, 689, 510, 50655, 50655, 1184, 5386, 1062, 6805, 281, 257, 4537, 300, 390, 6229, 257, 3441, 281, 264, 4631, 293, 10999, 50972, 50972, 493, 365, 512, 1333, 295, 15217, 13, 51142, 51142, 1018, 949, 11, 321, 603, 764, 275, 281, 45708, 264, 1230, 295, 3097, 5110, 13, 51466, 51466, 6947, 3097, 1365, 575, 472, 420, 544, 4122, 11, 1270, 382, 264, 22512, 2744, 11, 264, 4537, 311, 3205, 11, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.11094946211034601, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.705042399815284e-05}, {"id": 8, "seek": 2928, "start": 41.44, "end": 44.84, "text": " up with some sort of diagnosis.", "tokens": [50364, 13428, 437, 264, 3097, 992, 337, 257, 3565, 3142, 24590, 2316, 1062, 574, 411, 11, 689, 510, 50655, 50655, 1184, 5386, 1062, 6805, 281, 257, 4537, 300, 390, 6229, 257, 3441, 281, 264, 4631, 293, 10999, 50972, 50972, 493, 365, 512, 1333, 295, 15217, 13, 51142, 51142, 1018, 949, 11, 321, 603, 764, 275, 281, 45708, 264, 1230, 295, 3097, 5110, 13, 51466, 51466, 6947, 3097, 1365, 575, 472, 420, 544, 4122, 11, 1270, 382, 264, 22512, 2744, 11, 264, 4537, 311, 3205, 11, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.11094946211034601, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.705042399815284e-05}, {"id": 9, "seek": 2928, "start": 44.84, "end": 51.32, "text": " As before, we'll use m to denote the number of training examples.", "tokens": [50364, 13428, 437, 264, 3097, 992, 337, 257, 3565, 3142, 24590, 2316, 1062, 574, 411, 11, 689, 510, 50655, 50655, 1184, 5386, 1062, 6805, 281, 257, 4537, 300, 390, 6229, 257, 3441, 281, 264, 4631, 293, 10999, 50972, 50972, 493, 365, 512, 1333, 295, 15217, 13, 51142, 51142, 1018, 949, 11, 321, 603, 764, 275, 281, 45708, 264, 1230, 295, 3097, 5110, 13, 51466, 51466, 6947, 3097, 1365, 575, 472, 420, 544, 4122, 11, 1270, 382, 264, 22512, 2744, 11, 264, 4537, 311, 3205, 11, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.11094946211034601, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.705042399815284e-05}, {"id": 10, "seek": 2928, "start": 51.32, "end": 57.64, "text": " Each training example has one or more features, such as the tumor size, the patient's age,", "tokens": [50364, 13428, 437, 264, 3097, 992, 337, 257, 3565, 3142, 24590, 2316, 1062, 574, 411, 11, 689, 510, 50655, 50655, 1184, 5386, 1062, 6805, 281, 257, 4537, 300, 390, 6229, 257, 3441, 281, 264, 4631, 293, 10999, 50972, 50972, 493, 365, 512, 1333, 295, 15217, 13, 51142, 51142, 1018, 949, 11, 321, 603, 764, 275, 281, 45708, 264, 1230, 295, 3097, 5110, 13, 51466, 51466, 6947, 3097, 1365, 575, 472, 420, 544, 4122, 11, 1270, 382, 264, 22512, 2744, 11, 264, 4537, 311, 3205, 11, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.11094946211034601, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.705042399815284e-05}, {"id": 11, "seek": 5764, "start": 57.64, "end": 62.0, "text": " and so on, for a total of n features.", "tokens": [50364, 293, 370, 322, 11, 337, 257, 3217, 295, 297, 4122, 13, 50582, 50582, 400, 370, 718, 311, 818, 264, 4122, 2031, 16, 807, 2031, 77, 13, 50818, 50818, 400, 1670, 341, 307, 257, 17434, 21538, 5633, 11, 264, 3779, 7645, 288, 2516, 322, 787, 732, 4190, 11, 51154, 51154, 2139, 1958, 420, 502, 13, 51280, 51280, 400, 2721, 11, 264, 3565, 3142, 24590, 2316, 307, 7642, 538, 341, 5367, 13, 51546, 51546, 1033, 11, 370, 264, 1168, 291, 528, 281, 1867, 307, 11, 2212, 341, 3097, 992, 11, 577, 393, 291, 2826, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.14066610733668009, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.187534048265661e-06}, {"id": 12, "seek": 5764, "start": 62.0, "end": 66.72, "text": " And so let's call the features x1 through xn.", "tokens": [50364, 293, 370, 322, 11, 337, 257, 3217, 295, 297, 4122, 13, 50582, 50582, 400, 370, 718, 311, 818, 264, 4122, 2031, 16, 807, 2031, 77, 13, 50818, 50818, 400, 1670, 341, 307, 257, 17434, 21538, 5633, 11, 264, 3779, 7645, 288, 2516, 322, 787, 732, 4190, 11, 51154, 51154, 2139, 1958, 420, 502, 13, 51280, 51280, 400, 2721, 11, 264, 3565, 3142, 24590, 2316, 307, 7642, 538, 341, 5367, 13, 51546, 51546, 1033, 11, 370, 264, 1168, 291, 528, 281, 1867, 307, 11, 2212, 341, 3097, 992, 11, 577, 393, 291, 2826, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.14066610733668009, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.187534048265661e-06}, {"id": 13, "seek": 5764, "start": 66.72, "end": 73.44, "text": " And since this is a binary classification task, the target label y takes on only two values,", "tokens": [50364, 293, 370, 322, 11, 337, 257, 3217, 295, 297, 4122, 13, 50582, 50582, 400, 370, 718, 311, 818, 264, 4122, 2031, 16, 807, 2031, 77, 13, 50818, 50818, 400, 1670, 341, 307, 257, 17434, 21538, 5633, 11, 264, 3779, 7645, 288, 2516, 322, 787, 732, 4190, 11, 51154, 51154, 2139, 1958, 420, 502, 13, 51280, 51280, 400, 2721, 11, 264, 3565, 3142, 24590, 2316, 307, 7642, 538, 341, 5367, 13, 51546, 51546, 1033, 11, 370, 264, 1168, 291, 528, 281, 1867, 307, 11, 2212, 341, 3097, 992, 11, 577, 393, 291, 2826, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.14066610733668009, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.187534048265661e-06}, {"id": 14, "seek": 5764, "start": 73.44, "end": 75.96000000000001, "text": " either 0 or 1.", "tokens": [50364, 293, 370, 322, 11, 337, 257, 3217, 295, 297, 4122, 13, 50582, 50582, 400, 370, 718, 311, 818, 264, 4122, 2031, 16, 807, 2031, 77, 13, 50818, 50818, 400, 1670, 341, 307, 257, 17434, 21538, 5633, 11, 264, 3779, 7645, 288, 2516, 322, 787, 732, 4190, 11, 51154, 51154, 2139, 1958, 420, 502, 13, 51280, 51280, 400, 2721, 11, 264, 3565, 3142, 24590, 2316, 307, 7642, 538, 341, 5367, 13, 51546, 51546, 1033, 11, 370, 264, 1168, 291, 528, 281, 1867, 307, 11, 2212, 341, 3097, 992, 11, 577, 393, 291, 2826, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.14066610733668009, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.187534048265661e-06}, {"id": 15, "seek": 5764, "start": 75.96000000000001, "end": 81.28, "text": " And finally, the logistic regression model is defined by this equation.", "tokens": [50364, 293, 370, 322, 11, 337, 257, 3217, 295, 297, 4122, 13, 50582, 50582, 400, 370, 718, 311, 818, 264, 4122, 2031, 16, 807, 2031, 77, 13, 50818, 50818, 400, 1670, 341, 307, 257, 17434, 21538, 5633, 11, 264, 3779, 7645, 288, 2516, 322, 787, 732, 4190, 11, 51154, 51154, 2139, 1958, 420, 502, 13, 51280, 51280, 400, 2721, 11, 264, 3565, 3142, 24590, 2316, 307, 7642, 538, 341, 5367, 13, 51546, 51546, 1033, 11, 370, 264, 1168, 291, 528, 281, 1867, 307, 11, 2212, 341, 3097, 992, 11, 577, 393, 291, 2826, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.14066610733668009, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.187534048265661e-06}, {"id": 16, "seek": 5764, "start": 81.28, "end": 87.04, "text": " Okay, so the question you want to answer is, given this training set, how can you choose", "tokens": [50364, 293, 370, 322, 11, 337, 257, 3217, 295, 297, 4122, 13, 50582, 50582, 400, 370, 718, 311, 818, 264, 4122, 2031, 16, 807, 2031, 77, 13, 50818, 50818, 400, 1670, 341, 307, 257, 17434, 21538, 5633, 11, 264, 3779, 7645, 288, 2516, 322, 787, 732, 4190, 11, 51154, 51154, 2139, 1958, 420, 502, 13, 51280, 51280, 400, 2721, 11, 264, 3565, 3142, 24590, 2316, 307, 7642, 538, 341, 5367, 13, 51546, 51546, 1033, 11, 370, 264, 1168, 291, 528, 281, 1867, 307, 11, 2212, 341, 3097, 992, 11, 577, 393, 291, 2826, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.14066610733668009, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.187534048265661e-06}, {"id": 17, "seek": 8704, "start": 87.04, "end": 90.72000000000001, "text": " parameters w and b?", "tokens": [50364, 9834, 261, 293, 272, 30, 50548, 50548, 9647, 336, 337, 8213, 24590, 11, 341, 307, 264, 8889, 6713, 2063, 2445, 13, 50790, 50790, 440, 787, 551, 286, 600, 3105, 307, 300, 286, 829, 264, 472, 1922, 1854, 264, 28811, 2602, 51038, 51038, 295, 2380, 264, 28811, 13, 51146, 51146, 400, 291, 1062, 1604, 300, 294, 264, 1389, 295, 8213, 24590, 11, 689, 283, 295, 2031, 307, 264, 8213, 51382, 51382, 2445, 261, 5893, 2031, 1804, 272, 11, 264, 2063, 2445, 1542, 411, 341, 307, 257, 42432, 2445, 420, 257, 51746, 51746, 4503, 3909, 420, 257, 36600, 1560, 3909, 13, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1374133767433537, "compression_ratio": 1.7413793103448276, "no_speech_prob": 1.7778274923330173e-05}, {"id": 18, "seek": 8704, "start": 90.72000000000001, "end": 95.56, "text": " Recall for linear regression, this is the squared error cost function.", "tokens": [50364, 9834, 261, 293, 272, 30, 50548, 50548, 9647, 336, 337, 8213, 24590, 11, 341, 307, 264, 8889, 6713, 2063, 2445, 13, 50790, 50790, 440, 787, 551, 286, 600, 3105, 307, 300, 286, 829, 264, 472, 1922, 1854, 264, 28811, 2602, 51038, 51038, 295, 2380, 264, 28811, 13, 51146, 51146, 400, 291, 1062, 1604, 300, 294, 264, 1389, 295, 8213, 24590, 11, 689, 283, 295, 2031, 307, 264, 8213, 51382, 51382, 2445, 261, 5893, 2031, 1804, 272, 11, 264, 2063, 2445, 1542, 411, 341, 307, 257, 42432, 2445, 420, 257, 51746, 51746, 4503, 3909, 420, 257, 36600, 1560, 3909, 13, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1374133767433537, "compression_ratio": 1.7413793103448276, "no_speech_prob": 1.7778274923330173e-05}, {"id": 19, "seek": 8704, "start": 95.56, "end": 100.52000000000001, "text": " The only thing I've changed is that I put the one half inside the summation instead", "tokens": [50364, 9834, 261, 293, 272, 30, 50548, 50548, 9647, 336, 337, 8213, 24590, 11, 341, 307, 264, 8889, 6713, 2063, 2445, 13, 50790, 50790, 440, 787, 551, 286, 600, 3105, 307, 300, 286, 829, 264, 472, 1922, 1854, 264, 28811, 2602, 51038, 51038, 295, 2380, 264, 28811, 13, 51146, 51146, 400, 291, 1062, 1604, 300, 294, 264, 1389, 295, 8213, 24590, 11, 689, 283, 295, 2031, 307, 264, 8213, 51382, 51382, 2445, 261, 5893, 2031, 1804, 272, 11, 264, 2063, 2445, 1542, 411, 341, 307, 257, 42432, 2445, 420, 257, 51746, 51746, 4503, 3909, 420, 257, 36600, 1560, 3909, 13, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1374133767433537, "compression_ratio": 1.7413793103448276, "no_speech_prob": 1.7778274923330173e-05}, {"id": 20, "seek": 8704, "start": 100.52000000000001, "end": 102.68, "text": " of outside the summation.", "tokens": [50364, 9834, 261, 293, 272, 30, 50548, 50548, 9647, 336, 337, 8213, 24590, 11, 341, 307, 264, 8889, 6713, 2063, 2445, 13, 50790, 50790, 440, 787, 551, 286, 600, 3105, 307, 300, 286, 829, 264, 472, 1922, 1854, 264, 28811, 2602, 51038, 51038, 295, 2380, 264, 28811, 13, 51146, 51146, 400, 291, 1062, 1604, 300, 294, 264, 1389, 295, 8213, 24590, 11, 689, 283, 295, 2031, 307, 264, 8213, 51382, 51382, 2445, 261, 5893, 2031, 1804, 272, 11, 264, 2063, 2445, 1542, 411, 341, 307, 257, 42432, 2445, 420, 257, 51746, 51746, 4503, 3909, 420, 257, 36600, 1560, 3909, 13, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1374133767433537, "compression_ratio": 1.7413793103448276, "no_speech_prob": 1.7778274923330173e-05}, {"id": 21, "seek": 8704, "start": 102.68, "end": 107.4, "text": " And you might remember that in the case of linear regression, where f of x is the linear", "tokens": [50364, 9834, 261, 293, 272, 30, 50548, 50548, 9647, 336, 337, 8213, 24590, 11, 341, 307, 264, 8889, 6713, 2063, 2445, 13, 50790, 50790, 440, 787, 551, 286, 600, 3105, 307, 300, 286, 829, 264, 472, 1922, 1854, 264, 28811, 2602, 51038, 51038, 295, 2380, 264, 28811, 13, 51146, 51146, 400, 291, 1062, 1604, 300, 294, 264, 1389, 295, 8213, 24590, 11, 689, 283, 295, 2031, 307, 264, 8213, 51382, 51382, 2445, 261, 5893, 2031, 1804, 272, 11, 264, 2063, 2445, 1542, 411, 341, 307, 257, 42432, 2445, 420, 257, 51746, 51746, 4503, 3909, 420, 257, 36600, 1560, 3909, 13, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1374133767433537, "compression_ratio": 1.7413793103448276, "no_speech_prob": 1.7778274923330173e-05}, {"id": 22, "seek": 8704, "start": 107.4, "end": 114.68, "text": " function w dot x plus b, the cost function looks like this is a convex function or a", "tokens": [50364, 9834, 261, 293, 272, 30, 50548, 50548, 9647, 336, 337, 8213, 24590, 11, 341, 307, 264, 8889, 6713, 2063, 2445, 13, 50790, 50790, 440, 787, 551, 286, 600, 3105, 307, 300, 286, 829, 264, 472, 1922, 1854, 264, 28811, 2602, 51038, 51038, 295, 2380, 264, 28811, 13, 51146, 51146, 400, 291, 1062, 1604, 300, 294, 264, 1389, 295, 8213, 24590, 11, 689, 283, 295, 2031, 307, 264, 8213, 51382, 51382, 2445, 261, 5893, 2031, 1804, 272, 11, 264, 2063, 2445, 1542, 411, 341, 307, 257, 42432, 2445, 420, 257, 51746, 51746, 4503, 3909, 420, 257, 36600, 1560, 3909, 13, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1374133767433537, "compression_ratio": 1.7413793103448276, "no_speech_prob": 1.7778274923330173e-05}, {"id": 23, "seek": 8704, "start": 114.68, "end": 116.88000000000001, "text": " bow shape or a hammock shape.", "tokens": [50364, 9834, 261, 293, 272, 30, 50548, 50548, 9647, 336, 337, 8213, 24590, 11, 341, 307, 264, 8889, 6713, 2063, 2445, 13, 50790, 50790, 440, 787, 551, 286, 600, 3105, 307, 300, 286, 829, 264, 472, 1922, 1854, 264, 28811, 2602, 51038, 51038, 295, 2380, 264, 28811, 13, 51146, 51146, 400, 291, 1062, 1604, 300, 294, 264, 1389, 295, 8213, 24590, 11, 689, 283, 295, 2031, 307, 264, 8213, 51382, 51382, 2445, 261, 5893, 2031, 1804, 272, 11, 264, 2063, 2445, 1542, 411, 341, 307, 257, 42432, 2445, 420, 257, 51746, 51746, 4503, 3909, 420, 257, 36600, 1560, 3909, 13, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1374133767433537, "compression_ratio": 1.7413793103448276, "no_speech_prob": 1.7778274923330173e-05}, {"id": 24, "seek": 11688, "start": 116.88, "end": 122.19999999999999, "text": " And so gradient descent will look like this, where you take one step, one step, one step,", "tokens": [50364, 400, 370, 16235, 23475, 486, 574, 411, 341, 11, 689, 291, 747, 472, 1823, 11, 472, 1823, 11, 472, 1823, 11, 50630, 50630, 293, 370, 322, 281, 41881, 412, 264, 4338, 7285, 13, 50792, 50792, 823, 11, 291, 727, 853, 281, 764, 264, 912, 2063, 2445, 337, 3565, 3142, 24590, 11, 457, 309, 4523, 484, 51104, 51104, 300, 498, 286, 645, 281, 2464, 283, 295, 2031, 6915, 502, 670, 502, 1804, 308, 281, 264, 3671, 261, 2031, 1804, 272, 11, 293, 7542, 51506, 51506, 264, 2063, 2445, 1228, 341, 2158, 295, 283, 295, 2031, 11, 550, 264, 2063, 486, 574, 411, 341, 13, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.08676086853597766, "compression_ratio": 1.7412280701754386, "no_speech_prob": 1.2804873222194146e-05}, {"id": 25, "seek": 11688, "start": 122.19999999999999, "end": 125.44, "text": " and so on to converge at the global minimum.", "tokens": [50364, 400, 370, 16235, 23475, 486, 574, 411, 341, 11, 689, 291, 747, 472, 1823, 11, 472, 1823, 11, 472, 1823, 11, 50630, 50630, 293, 370, 322, 281, 41881, 412, 264, 4338, 7285, 13, 50792, 50792, 823, 11, 291, 727, 853, 281, 764, 264, 912, 2063, 2445, 337, 3565, 3142, 24590, 11, 457, 309, 4523, 484, 51104, 51104, 300, 498, 286, 645, 281, 2464, 283, 295, 2031, 6915, 502, 670, 502, 1804, 308, 281, 264, 3671, 261, 2031, 1804, 272, 11, 293, 7542, 51506, 51506, 264, 2063, 2445, 1228, 341, 2158, 295, 283, 295, 2031, 11, 550, 264, 2063, 486, 574, 411, 341, 13, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.08676086853597766, "compression_ratio": 1.7412280701754386, "no_speech_prob": 1.2804873222194146e-05}, {"id": 26, "seek": 11688, "start": 125.44, "end": 131.68, "text": " Now, you could try to use the same cost function for logistic regression, but it turns out", "tokens": [50364, 400, 370, 16235, 23475, 486, 574, 411, 341, 11, 689, 291, 747, 472, 1823, 11, 472, 1823, 11, 472, 1823, 11, 50630, 50630, 293, 370, 322, 281, 41881, 412, 264, 4338, 7285, 13, 50792, 50792, 823, 11, 291, 727, 853, 281, 764, 264, 912, 2063, 2445, 337, 3565, 3142, 24590, 11, 457, 309, 4523, 484, 51104, 51104, 300, 498, 286, 645, 281, 2464, 283, 295, 2031, 6915, 502, 670, 502, 1804, 308, 281, 264, 3671, 261, 2031, 1804, 272, 11, 293, 7542, 51506, 51506, 264, 2063, 2445, 1228, 341, 2158, 295, 283, 295, 2031, 11, 550, 264, 2063, 486, 574, 411, 341, 13, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.08676086853597766, "compression_ratio": 1.7412280701754386, "no_speech_prob": 1.2804873222194146e-05}, {"id": 27, "seek": 11688, "start": 131.68, "end": 139.72, "text": " that if I were to write f of x equals 1 over 1 plus e to the negative w x plus b, and plot", "tokens": [50364, 400, 370, 16235, 23475, 486, 574, 411, 341, 11, 689, 291, 747, 472, 1823, 11, 472, 1823, 11, 472, 1823, 11, 50630, 50630, 293, 370, 322, 281, 41881, 412, 264, 4338, 7285, 13, 50792, 50792, 823, 11, 291, 727, 853, 281, 764, 264, 912, 2063, 2445, 337, 3565, 3142, 24590, 11, 457, 309, 4523, 484, 51104, 51104, 300, 498, 286, 645, 281, 2464, 283, 295, 2031, 6915, 502, 670, 502, 1804, 308, 281, 264, 3671, 261, 2031, 1804, 272, 11, 293, 7542, 51506, 51506, 264, 2063, 2445, 1228, 341, 2158, 295, 283, 295, 2031, 11, 550, 264, 2063, 486, 574, 411, 341, 13, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.08676086853597766, "compression_ratio": 1.7412280701754386, "no_speech_prob": 1.2804873222194146e-05}, {"id": 28, "seek": 11688, "start": 139.72, "end": 146.64, "text": " the cost function using this value of f of x, then the cost will look like this.", "tokens": [50364, 400, 370, 16235, 23475, 486, 574, 411, 341, 11, 689, 291, 747, 472, 1823, 11, 472, 1823, 11, 472, 1823, 11, 50630, 50630, 293, 370, 322, 281, 41881, 412, 264, 4338, 7285, 13, 50792, 50792, 823, 11, 291, 727, 853, 281, 764, 264, 912, 2063, 2445, 337, 3565, 3142, 24590, 11, 457, 309, 4523, 484, 51104, 51104, 300, 498, 286, 645, 281, 2464, 283, 295, 2031, 6915, 502, 670, 502, 1804, 308, 281, 264, 3671, 261, 2031, 1804, 272, 11, 293, 7542, 51506, 51506, 264, 2063, 2445, 1228, 341, 2158, 295, 283, 295, 2031, 11, 550, 264, 2063, 486, 574, 411, 341, 13, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.08676086853597766, "compression_ratio": 1.7412280701754386, "no_speech_prob": 1.2804873222194146e-05}, {"id": 29, "seek": 14664, "start": 146.64, "end": 152.51999999999998, "text": " This becomes what's called a non-convex cost function, it's not convex.", "tokens": [50364, 639, 3643, 437, 311, 1219, 257, 2107, 12, 1671, 303, 87, 2063, 2445, 11, 309, 311, 406, 42432, 13, 50658, 50658, 400, 437, 341, 1355, 307, 300, 498, 291, 645, 281, 853, 281, 764, 16235, 23475, 11, 456, 366, 3195, 50944, 50944, 295, 2654, 4464, 64, 300, 291, 393, 483, 5541, 294, 13, 51124, 51124, 407, 309, 4523, 484, 300, 337, 3565, 3142, 24590, 11, 341, 8889, 6713, 2063, 2445, 307, 406, 257, 51422, 51422, 665, 3922, 13, 51472, 51472, 7156, 11, 456, 486, 312, 257, 819, 2063, 2445, 300, 393, 652, 264, 2063, 2445, 42432, 797, 11, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.08083989573459999, "compression_ratio": 1.7112068965517242, "no_speech_prob": 6.8541639848263e-06}, {"id": 30, "seek": 14664, "start": 152.51999999999998, "end": 158.23999999999998, "text": " And what this means is that if you were to try to use gradient descent, there are lots", "tokens": [50364, 639, 3643, 437, 311, 1219, 257, 2107, 12, 1671, 303, 87, 2063, 2445, 11, 309, 311, 406, 42432, 13, 50658, 50658, 400, 437, 341, 1355, 307, 300, 498, 291, 645, 281, 853, 281, 764, 16235, 23475, 11, 456, 366, 3195, 50944, 50944, 295, 2654, 4464, 64, 300, 291, 393, 483, 5541, 294, 13, 51124, 51124, 407, 309, 4523, 484, 300, 337, 3565, 3142, 24590, 11, 341, 8889, 6713, 2063, 2445, 307, 406, 257, 51422, 51422, 665, 3922, 13, 51472, 51472, 7156, 11, 456, 486, 312, 257, 819, 2063, 2445, 300, 393, 652, 264, 2063, 2445, 42432, 797, 11, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.08083989573459999, "compression_ratio": 1.7112068965517242, "no_speech_prob": 6.8541639848263e-06}, {"id": 31, "seek": 14664, "start": 158.23999999999998, "end": 161.83999999999997, "text": " of local minima that you can get stuck in.", "tokens": [50364, 639, 3643, 437, 311, 1219, 257, 2107, 12, 1671, 303, 87, 2063, 2445, 11, 309, 311, 406, 42432, 13, 50658, 50658, 400, 437, 341, 1355, 307, 300, 498, 291, 645, 281, 853, 281, 764, 16235, 23475, 11, 456, 366, 3195, 50944, 50944, 295, 2654, 4464, 64, 300, 291, 393, 483, 5541, 294, 13, 51124, 51124, 407, 309, 4523, 484, 300, 337, 3565, 3142, 24590, 11, 341, 8889, 6713, 2063, 2445, 307, 406, 257, 51422, 51422, 665, 3922, 13, 51472, 51472, 7156, 11, 456, 486, 312, 257, 819, 2063, 2445, 300, 393, 652, 264, 2063, 2445, 42432, 797, 11, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.08083989573459999, "compression_ratio": 1.7112068965517242, "no_speech_prob": 6.8541639848263e-06}, {"id": 32, "seek": 14664, "start": 161.83999999999997, "end": 167.79999999999998, "text": " So it turns out that for logistic regression, this squared error cost function is not a", "tokens": [50364, 639, 3643, 437, 311, 1219, 257, 2107, 12, 1671, 303, 87, 2063, 2445, 11, 309, 311, 406, 42432, 13, 50658, 50658, 400, 437, 341, 1355, 307, 300, 498, 291, 645, 281, 853, 281, 764, 16235, 23475, 11, 456, 366, 3195, 50944, 50944, 295, 2654, 4464, 64, 300, 291, 393, 483, 5541, 294, 13, 51124, 51124, 407, 309, 4523, 484, 300, 337, 3565, 3142, 24590, 11, 341, 8889, 6713, 2063, 2445, 307, 406, 257, 51422, 51422, 665, 3922, 13, 51472, 51472, 7156, 11, 456, 486, 312, 257, 819, 2063, 2445, 300, 393, 652, 264, 2063, 2445, 42432, 797, 11, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.08083989573459999, "compression_ratio": 1.7112068965517242, "no_speech_prob": 6.8541639848263e-06}, {"id": 33, "seek": 14664, "start": 167.79999999999998, "end": 168.79999999999998, "text": " good choice.", "tokens": [50364, 639, 3643, 437, 311, 1219, 257, 2107, 12, 1671, 303, 87, 2063, 2445, 11, 309, 311, 406, 42432, 13, 50658, 50658, 400, 437, 341, 1355, 307, 300, 498, 291, 645, 281, 853, 281, 764, 16235, 23475, 11, 456, 366, 3195, 50944, 50944, 295, 2654, 4464, 64, 300, 291, 393, 483, 5541, 294, 13, 51124, 51124, 407, 309, 4523, 484, 300, 337, 3565, 3142, 24590, 11, 341, 8889, 6713, 2063, 2445, 307, 406, 257, 51422, 51422, 665, 3922, 13, 51472, 51472, 7156, 11, 456, 486, 312, 257, 819, 2063, 2445, 300, 393, 652, 264, 2063, 2445, 42432, 797, 11, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.08083989573459999, "compression_ratio": 1.7112068965517242, "no_speech_prob": 6.8541639848263e-06}, {"id": 34, "seek": 14664, "start": 168.79999999999998, "end": 175.72, "text": " Instead, there will be a different cost function that can make the cost function convex again,", "tokens": [50364, 639, 3643, 437, 311, 1219, 257, 2107, 12, 1671, 303, 87, 2063, 2445, 11, 309, 311, 406, 42432, 13, 50658, 50658, 400, 437, 341, 1355, 307, 300, 498, 291, 645, 281, 853, 281, 764, 16235, 23475, 11, 456, 366, 3195, 50944, 50944, 295, 2654, 4464, 64, 300, 291, 393, 483, 5541, 294, 13, 51124, 51124, 407, 309, 4523, 484, 300, 337, 3565, 3142, 24590, 11, 341, 8889, 6713, 2063, 2445, 307, 406, 257, 51422, 51422, 665, 3922, 13, 51472, 51472, 7156, 11, 456, 486, 312, 257, 819, 2063, 2445, 300, 393, 652, 264, 2063, 2445, 42432, 797, 11, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.08083989573459999, "compression_ratio": 1.7112068965517242, "no_speech_prob": 6.8541639848263e-06}, {"id": 35, "seek": 17572, "start": 175.72, "end": 180.44, "text": " so that gradient descent can be guaranteed to converge to the global minimum.", "tokens": [50364, 370, 300, 16235, 23475, 393, 312, 18031, 281, 41881, 281, 264, 4338, 7285, 13, 50600, 50600, 440, 787, 551, 286, 600, 3105, 307, 300, 286, 829, 264, 472, 1922, 1854, 264, 28811, 2602, 50808, 50808, 295, 2380, 264, 28811, 13, 50914, 50914, 639, 486, 652, 264, 5221, 291, 536, 1780, 322, 264, 4137, 257, 707, 857, 18587, 13, 51172, 51172, 682, 1668, 281, 1322, 257, 777, 2063, 2445, 11, 472, 300, 321, 603, 764, 337, 3565, 3142, 24590, 11, 286, 478, 51414, 51414, 516, 281, 1319, 257, 707, 857, 264, 7123, 295, 264, 2063, 2445, 361, 295, 261, 293, 272, 13, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.10295652662004744, "compression_ratio": 1.7317073170731707, "no_speech_prob": 9.818049875320867e-06}, {"id": 36, "seek": 17572, "start": 180.44, "end": 184.6, "text": " The only thing I've changed is that I put the one half inside the summation instead", "tokens": [50364, 370, 300, 16235, 23475, 393, 312, 18031, 281, 41881, 281, 264, 4338, 7285, 13, 50600, 50600, 440, 787, 551, 286, 600, 3105, 307, 300, 286, 829, 264, 472, 1922, 1854, 264, 28811, 2602, 50808, 50808, 295, 2380, 264, 28811, 13, 50914, 50914, 639, 486, 652, 264, 5221, 291, 536, 1780, 322, 264, 4137, 257, 707, 857, 18587, 13, 51172, 51172, 682, 1668, 281, 1322, 257, 777, 2063, 2445, 11, 472, 300, 321, 603, 764, 337, 3565, 3142, 24590, 11, 286, 478, 51414, 51414, 516, 281, 1319, 257, 707, 857, 264, 7123, 295, 264, 2063, 2445, 361, 295, 261, 293, 272, 13, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.10295652662004744, "compression_ratio": 1.7317073170731707, "no_speech_prob": 9.818049875320867e-06}, {"id": 37, "seek": 17572, "start": 184.6, "end": 186.72, "text": " of outside the summation.", "tokens": [50364, 370, 300, 16235, 23475, 393, 312, 18031, 281, 41881, 281, 264, 4338, 7285, 13, 50600, 50600, 440, 787, 551, 286, 600, 3105, 307, 300, 286, 829, 264, 472, 1922, 1854, 264, 28811, 2602, 50808, 50808, 295, 2380, 264, 28811, 13, 50914, 50914, 639, 486, 652, 264, 5221, 291, 536, 1780, 322, 264, 4137, 257, 707, 857, 18587, 13, 51172, 51172, 682, 1668, 281, 1322, 257, 777, 2063, 2445, 11, 472, 300, 321, 603, 764, 337, 3565, 3142, 24590, 11, 286, 478, 51414, 51414, 516, 281, 1319, 257, 707, 857, 264, 7123, 295, 264, 2063, 2445, 361, 295, 261, 293, 272, 13, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.10295652662004744, "compression_ratio": 1.7317073170731707, "no_speech_prob": 9.818049875320867e-06}, {"id": 38, "seek": 17572, "start": 186.72, "end": 191.88, "text": " This will make the math you see later on the slide a little bit simpler.", "tokens": [50364, 370, 300, 16235, 23475, 393, 312, 18031, 281, 41881, 281, 264, 4338, 7285, 13, 50600, 50600, 440, 787, 551, 286, 600, 3105, 307, 300, 286, 829, 264, 472, 1922, 1854, 264, 28811, 2602, 50808, 50808, 295, 2380, 264, 28811, 13, 50914, 50914, 639, 486, 652, 264, 5221, 291, 536, 1780, 322, 264, 4137, 257, 707, 857, 18587, 13, 51172, 51172, 682, 1668, 281, 1322, 257, 777, 2063, 2445, 11, 472, 300, 321, 603, 764, 337, 3565, 3142, 24590, 11, 286, 478, 51414, 51414, 516, 281, 1319, 257, 707, 857, 264, 7123, 295, 264, 2063, 2445, 361, 295, 261, 293, 272, 13, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.10295652662004744, "compression_ratio": 1.7317073170731707, "no_speech_prob": 9.818049875320867e-06}, {"id": 39, "seek": 17572, "start": 191.88, "end": 196.72, "text": " In order to build a new cost function, one that we'll use for logistic regression, I'm", "tokens": [50364, 370, 300, 16235, 23475, 393, 312, 18031, 281, 41881, 281, 264, 4338, 7285, 13, 50600, 50600, 440, 787, 551, 286, 600, 3105, 307, 300, 286, 829, 264, 472, 1922, 1854, 264, 28811, 2602, 50808, 50808, 295, 2380, 264, 28811, 13, 50914, 50914, 639, 486, 652, 264, 5221, 291, 536, 1780, 322, 264, 4137, 257, 707, 857, 18587, 13, 51172, 51172, 682, 1668, 281, 1322, 257, 777, 2063, 2445, 11, 472, 300, 321, 603, 764, 337, 3565, 3142, 24590, 11, 286, 478, 51414, 51414, 516, 281, 1319, 257, 707, 857, 264, 7123, 295, 264, 2063, 2445, 361, 295, 261, 293, 272, 13, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.10295652662004744, "compression_ratio": 1.7317073170731707, "no_speech_prob": 9.818049875320867e-06}, {"id": 40, "seek": 17572, "start": 196.72, "end": 203.0, "text": " going to change a little bit the definition of the cost function j of w and b.", "tokens": [50364, 370, 300, 16235, 23475, 393, 312, 18031, 281, 41881, 281, 264, 4338, 7285, 13, 50600, 50600, 440, 787, 551, 286, 600, 3105, 307, 300, 286, 829, 264, 472, 1922, 1854, 264, 28811, 2602, 50808, 50808, 295, 2380, 264, 28811, 13, 50914, 50914, 639, 486, 652, 264, 5221, 291, 536, 1780, 322, 264, 4137, 257, 707, 857, 18587, 13, 51172, 51172, 682, 1668, 281, 1322, 257, 777, 2063, 2445, 11, 472, 300, 321, 603, 764, 337, 3565, 3142, 24590, 11, 286, 478, 51414, 51414, 516, 281, 1319, 257, 707, 857, 264, 7123, 295, 264, 2063, 2445, 361, 295, 261, 293, 272, 13, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.10295652662004744, "compression_ratio": 1.7317073170731707, "no_speech_prob": 9.818049875320867e-06}, {"id": 41, "seek": 20300, "start": 203.0, "end": 209.48, "text": " In particular, if you look inside the summation, let's call this term inside the loss on a", "tokens": [50364, 682, 1729, 11, 498, 291, 574, 1854, 264, 28811, 11, 718, 311, 818, 341, 1433, 1854, 264, 4470, 322, 257, 50688, 50688, 2167, 3097, 1365, 13, 50856, 50856, 400, 286, 478, 516, 281, 45708, 264, 4470, 5766, 341, 4238, 441, 11, 293, 307, 257, 2445, 295, 264, 17630, 51253, 51253, 295, 264, 2539, 9284, 283, 295, 2031, 11, 382, 731, 382, 295, 264, 2074, 7645, 288, 13, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.09722281845522598, "compression_ratio": 1.569767441860465, "no_speech_prob": 1.0129819202120416e-05}, {"id": 42, "seek": 20300, "start": 209.48, "end": 212.84, "text": " single training example.", "tokens": [50364, 682, 1729, 11, 498, 291, 574, 1854, 264, 28811, 11, 718, 311, 818, 341, 1433, 1854, 264, 4470, 322, 257, 50688, 50688, 2167, 3097, 1365, 13, 50856, 50856, 400, 286, 478, 516, 281, 45708, 264, 4470, 5766, 341, 4238, 441, 11, 293, 307, 257, 2445, 295, 264, 17630, 51253, 51253, 295, 264, 2539, 9284, 283, 295, 2031, 11, 382, 731, 382, 295, 264, 2074, 7645, 288, 13, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.09722281845522598, "compression_ratio": 1.569767441860465, "no_speech_prob": 1.0129819202120416e-05}, {"id": 43, "seek": 20300, "start": 212.84, "end": 220.78, "text": " And I'm going to denote the loss via this capital L, and is a function of the prediction", "tokens": [50364, 682, 1729, 11, 498, 291, 574, 1854, 264, 28811, 11, 718, 311, 818, 341, 1433, 1854, 264, 4470, 322, 257, 50688, 50688, 2167, 3097, 1365, 13, 50856, 50856, 400, 286, 478, 516, 281, 45708, 264, 4470, 5766, 341, 4238, 441, 11, 293, 307, 257, 2445, 295, 264, 17630, 51253, 51253, 295, 264, 2539, 9284, 283, 295, 2031, 11, 382, 731, 382, 295, 264, 2074, 7645, 288, 13, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.09722281845522598, "compression_ratio": 1.569767441860465, "no_speech_prob": 1.0129819202120416e-05}, {"id": 44, "seek": 20300, "start": 220.78, "end": 228.04, "text": " of the learning algorithm f of x, as well as of the true label y.", "tokens": [50364, 682, 1729, 11, 498, 291, 574, 1854, 264, 28811, 11, 718, 311, 818, 341, 1433, 1854, 264, 4470, 322, 257, 50688, 50688, 2167, 3097, 1365, 13, 50856, 50856, 400, 286, 478, 516, 281, 45708, 264, 4470, 5766, 341, 4238, 441, 11, 293, 307, 257, 2445, 295, 264, 17630, 51253, 51253, 295, 264, 2539, 9284, 283, 295, 2031, 11, 382, 731, 382, 295, 264, 2074, 7645, 288, 13, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.09722281845522598, "compression_ratio": 1.569767441860465, "no_speech_prob": 1.0129819202120416e-05}, {"id": 45, "seek": 22804, "start": 228.04, "end": 235.44, "text": " And so the loss, given the predicted f of x and the true label y, is equal in this case", "tokens": [50364, 400, 370, 264, 4470, 11, 2212, 264, 19147, 283, 295, 2031, 293, 264, 2074, 7645, 288, 11, 307, 2681, 294, 341, 1389, 50734, 50734, 281, 472, 1922, 295, 264, 8889, 2649, 13, 50930, 50930, 492, 603, 536, 13392, 300, 538, 10875, 257, 819, 1254, 337, 341, 4470, 2445, 11, 321, 603, 312, 1075, 51176, 51176, 281, 1066, 264, 4787, 2063, 2445, 11, 597, 307, 472, 670, 275, 1413, 264, 2408, 295, 613, 4470, 6828, 11, 51516, 51516, 281, 312, 257, 42432, 2445, 13, 51626, 51626], "temperature": 0.0, "avg_logprob": -0.12627453639589506, "compression_ratio": 1.6834170854271358, "no_speech_prob": 1.8738545577434707e-06}, {"id": 46, "seek": 22804, "start": 235.44, "end": 239.35999999999999, "text": " to one half of the squared difference.", "tokens": [50364, 400, 370, 264, 4470, 11, 2212, 264, 19147, 283, 295, 2031, 293, 264, 2074, 7645, 288, 11, 307, 2681, 294, 341, 1389, 50734, 50734, 281, 472, 1922, 295, 264, 8889, 2649, 13, 50930, 50930, 492, 603, 536, 13392, 300, 538, 10875, 257, 819, 1254, 337, 341, 4470, 2445, 11, 321, 603, 312, 1075, 51176, 51176, 281, 1066, 264, 4787, 2063, 2445, 11, 597, 307, 472, 670, 275, 1413, 264, 2408, 295, 613, 4470, 6828, 11, 51516, 51516, 281, 312, 257, 42432, 2445, 13, 51626, 51626], "temperature": 0.0, "avg_logprob": -0.12627453639589506, "compression_ratio": 1.6834170854271358, "no_speech_prob": 1.8738545577434707e-06}, {"id": 47, "seek": 22804, "start": 239.35999999999999, "end": 244.28, "text": " We'll see shortly that by choosing a different form for this loss function, we'll be able", "tokens": [50364, 400, 370, 264, 4470, 11, 2212, 264, 19147, 283, 295, 2031, 293, 264, 2074, 7645, 288, 11, 307, 2681, 294, 341, 1389, 50734, 50734, 281, 472, 1922, 295, 264, 8889, 2649, 13, 50930, 50930, 492, 603, 536, 13392, 300, 538, 10875, 257, 819, 1254, 337, 341, 4470, 2445, 11, 321, 603, 312, 1075, 51176, 51176, 281, 1066, 264, 4787, 2063, 2445, 11, 597, 307, 472, 670, 275, 1413, 264, 2408, 295, 613, 4470, 6828, 11, 51516, 51516, 281, 312, 257, 42432, 2445, 13, 51626, 51626], "temperature": 0.0, "avg_logprob": -0.12627453639589506, "compression_ratio": 1.6834170854271358, "no_speech_prob": 1.8738545577434707e-06}, {"id": 48, "seek": 22804, "start": 244.28, "end": 251.07999999999998, "text": " to keep the overall cost function, which is one over m times the sum of these loss functions,", "tokens": [50364, 400, 370, 264, 4470, 11, 2212, 264, 19147, 283, 295, 2031, 293, 264, 2074, 7645, 288, 11, 307, 2681, 294, 341, 1389, 50734, 50734, 281, 472, 1922, 295, 264, 8889, 2649, 13, 50930, 50930, 492, 603, 536, 13392, 300, 538, 10875, 257, 819, 1254, 337, 341, 4470, 2445, 11, 321, 603, 312, 1075, 51176, 51176, 281, 1066, 264, 4787, 2063, 2445, 11, 597, 307, 472, 670, 275, 1413, 264, 2408, 295, 613, 4470, 6828, 11, 51516, 51516, 281, 312, 257, 42432, 2445, 13, 51626, 51626], "temperature": 0.0, "avg_logprob": -0.12627453639589506, "compression_ratio": 1.6834170854271358, "no_speech_prob": 1.8738545577434707e-06}, {"id": 49, "seek": 22804, "start": 251.07999999999998, "end": 253.28, "text": " to be a convex function.", "tokens": [50364, 400, 370, 264, 4470, 11, 2212, 264, 19147, 283, 295, 2031, 293, 264, 2074, 7645, 288, 11, 307, 2681, 294, 341, 1389, 50734, 50734, 281, 472, 1922, 295, 264, 8889, 2649, 13, 50930, 50930, 492, 603, 536, 13392, 300, 538, 10875, 257, 819, 1254, 337, 341, 4470, 2445, 11, 321, 603, 312, 1075, 51176, 51176, 281, 1066, 264, 4787, 2063, 2445, 11, 597, 307, 472, 670, 275, 1413, 264, 2408, 295, 613, 4470, 6828, 11, 51516, 51516, 281, 312, 257, 42432, 2445, 13, 51626, 51626], "temperature": 0.0, "avg_logprob": -0.12627453639589506, "compression_ratio": 1.6834170854271358, "no_speech_prob": 1.8738545577434707e-06}, {"id": 50, "seek": 25328, "start": 253.28, "end": 261.28, "text": " Now the loss function inputs f of x and the true label y, and tells us how well we're", "tokens": [50364, 823, 264, 4470, 2445, 15743, 283, 295, 2031, 293, 264, 2074, 7645, 288, 11, 293, 5112, 505, 577, 731, 321, 434, 50764, 50764, 884, 322, 300, 1365, 13, 50914, 50914, 286, 478, 516, 281, 445, 2464, 760, 510, 264, 7123, 295, 264, 4470, 2445, 321, 603, 764, 337, 3565, 3142, 51176, 51176, 24590, 13, 51270, 51270, 759, 264, 7645, 288, 307, 2681, 281, 472, 11, 550, 264, 4470, 307, 3671, 3565, 295, 283, 295, 2031, 13, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.10061153173446655, "compression_ratio": 1.615819209039548, "no_speech_prob": 4.09287622460397e-06}, {"id": 51, "seek": 25328, "start": 261.28, "end": 264.28, "text": " doing on that example.", "tokens": [50364, 823, 264, 4470, 2445, 15743, 283, 295, 2031, 293, 264, 2074, 7645, 288, 11, 293, 5112, 505, 577, 731, 321, 434, 50764, 50764, 884, 322, 300, 1365, 13, 50914, 50914, 286, 478, 516, 281, 445, 2464, 760, 510, 264, 7123, 295, 264, 4470, 2445, 321, 603, 764, 337, 3565, 3142, 51176, 51176, 24590, 13, 51270, 51270, 759, 264, 7645, 288, 307, 2681, 281, 472, 11, 550, 264, 4470, 307, 3671, 3565, 295, 283, 295, 2031, 13, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.10061153173446655, "compression_ratio": 1.615819209039548, "no_speech_prob": 4.09287622460397e-06}, {"id": 52, "seek": 25328, "start": 264.28, "end": 269.52, "text": " I'm going to just write down here the definition of the loss function we'll use for logistic", "tokens": [50364, 823, 264, 4470, 2445, 15743, 283, 295, 2031, 293, 264, 2074, 7645, 288, 11, 293, 5112, 505, 577, 731, 321, 434, 50764, 50764, 884, 322, 300, 1365, 13, 50914, 50914, 286, 478, 516, 281, 445, 2464, 760, 510, 264, 7123, 295, 264, 4470, 2445, 321, 603, 764, 337, 3565, 3142, 51176, 51176, 24590, 13, 51270, 51270, 759, 264, 7645, 288, 307, 2681, 281, 472, 11, 550, 264, 4470, 307, 3671, 3565, 295, 283, 295, 2031, 13, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.10061153173446655, "compression_ratio": 1.615819209039548, "no_speech_prob": 4.09287622460397e-06}, {"id": 53, "seek": 25328, "start": 269.52, "end": 271.4, "text": " regression.", "tokens": [50364, 823, 264, 4470, 2445, 15743, 283, 295, 2031, 293, 264, 2074, 7645, 288, 11, 293, 5112, 505, 577, 731, 321, 434, 50764, 50764, 884, 322, 300, 1365, 13, 50914, 50914, 286, 478, 516, 281, 445, 2464, 760, 510, 264, 7123, 295, 264, 4470, 2445, 321, 603, 764, 337, 3565, 3142, 51176, 51176, 24590, 13, 51270, 51270, 759, 264, 7645, 288, 307, 2681, 281, 472, 11, 550, 264, 4470, 307, 3671, 3565, 295, 283, 295, 2031, 13, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.10061153173446655, "compression_ratio": 1.615819209039548, "no_speech_prob": 4.09287622460397e-06}, {"id": 54, "seek": 25328, "start": 271.4, "end": 279.2, "text": " If the label y is equal to one, then the loss is negative log of f of x.", "tokens": [50364, 823, 264, 4470, 2445, 15743, 283, 295, 2031, 293, 264, 2074, 7645, 288, 11, 293, 5112, 505, 577, 731, 321, 434, 50764, 50764, 884, 322, 300, 1365, 13, 50914, 50914, 286, 478, 516, 281, 445, 2464, 760, 510, 264, 7123, 295, 264, 4470, 2445, 321, 603, 764, 337, 3565, 3142, 51176, 51176, 24590, 13, 51270, 51270, 759, 264, 7645, 288, 307, 2681, 281, 472, 11, 550, 264, 4470, 307, 3671, 3565, 295, 283, 295, 2031, 13, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.10061153173446655, "compression_ratio": 1.615819209039548, "no_speech_prob": 4.09287622460397e-06}, {"id": 55, "seek": 27920, "start": 279.2, "end": 287.15999999999997, "text": " And if the label y is equal to zero, then the loss is negative log of one minus f of", "tokens": [50364, 400, 498, 264, 7645, 288, 307, 2681, 281, 4018, 11, 550, 264, 4470, 307, 3671, 3565, 295, 472, 3175, 283, 295, 50762, 50762, 2031, 13, 50874, 50874, 961, 311, 747, 257, 574, 412, 983, 341, 4470, 2445, 4696, 1669, 2020, 13, 51202, 51202, 961, 311, 700, 1949, 264, 1389, 295, 288, 6915, 472, 11, 293, 7542, 437, 341, 2445, 1542, 411, 51502, 51502, 281, 6052, 512, 24002, 466, 437, 341, 4470, 2445, 307, 884, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.09736648994156077, "compression_ratio": 1.6378378378378378, "no_speech_prob": 1.414465373272833e-06}, {"id": 56, "seek": 27920, "start": 287.15999999999997, "end": 289.4, "text": " x.", "tokens": [50364, 400, 498, 264, 7645, 288, 307, 2681, 281, 4018, 11, 550, 264, 4470, 307, 3671, 3565, 295, 472, 3175, 283, 295, 50762, 50762, 2031, 13, 50874, 50874, 961, 311, 747, 257, 574, 412, 983, 341, 4470, 2445, 4696, 1669, 2020, 13, 51202, 51202, 961, 311, 700, 1949, 264, 1389, 295, 288, 6915, 472, 11, 293, 7542, 437, 341, 2445, 1542, 411, 51502, 51502, 281, 6052, 512, 24002, 466, 437, 341, 4470, 2445, 307, 884, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.09736648994156077, "compression_ratio": 1.6378378378378378, "no_speech_prob": 1.414465373272833e-06}, {"id": 57, "seek": 27920, "start": 289.4, "end": 295.96, "text": " Let's take a look at why this loss function hopefully makes sense.", "tokens": [50364, 400, 498, 264, 7645, 288, 307, 2681, 281, 4018, 11, 550, 264, 4470, 307, 3671, 3565, 295, 472, 3175, 283, 295, 50762, 50762, 2031, 13, 50874, 50874, 961, 311, 747, 257, 574, 412, 983, 341, 4470, 2445, 4696, 1669, 2020, 13, 51202, 51202, 961, 311, 700, 1949, 264, 1389, 295, 288, 6915, 472, 11, 293, 7542, 437, 341, 2445, 1542, 411, 51502, 51502, 281, 6052, 512, 24002, 466, 437, 341, 4470, 2445, 307, 884, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.09736648994156077, "compression_ratio": 1.6378378378378378, "no_speech_prob": 1.414465373272833e-06}, {"id": 58, "seek": 27920, "start": 295.96, "end": 301.96, "text": " Let's first consider the case of y equals one, and plot what this function looks like", "tokens": [50364, 400, 498, 264, 7645, 288, 307, 2681, 281, 4018, 11, 550, 264, 4470, 307, 3671, 3565, 295, 472, 3175, 283, 295, 50762, 50762, 2031, 13, 50874, 50874, 961, 311, 747, 257, 574, 412, 983, 341, 4470, 2445, 4696, 1669, 2020, 13, 51202, 51202, 961, 311, 700, 1949, 264, 1389, 295, 288, 6915, 472, 11, 293, 7542, 437, 341, 2445, 1542, 411, 51502, 51502, 281, 6052, 512, 24002, 466, 437, 341, 4470, 2445, 307, 884, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.09736648994156077, "compression_ratio": 1.6378378378378378, "no_speech_prob": 1.414465373272833e-06}, {"id": 59, "seek": 27920, "start": 301.96, "end": 306.64, "text": " to gain some intuition about what this loss function is doing.", "tokens": [50364, 400, 498, 264, 7645, 288, 307, 2681, 281, 4018, 11, 550, 264, 4470, 307, 3671, 3565, 295, 472, 3175, 283, 295, 50762, 50762, 2031, 13, 50874, 50874, 961, 311, 747, 257, 574, 412, 983, 341, 4470, 2445, 4696, 1669, 2020, 13, 51202, 51202, 961, 311, 700, 1949, 264, 1389, 295, 288, 6915, 472, 11, 293, 7542, 437, 341, 2445, 1542, 411, 51502, 51502, 281, 6052, 512, 24002, 466, 437, 341, 4470, 2445, 307, 884, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.09736648994156077, "compression_ratio": 1.6378378378378378, "no_speech_prob": 1.414465373272833e-06}, {"id": 60, "seek": 30664, "start": 306.64, "end": 312.2, "text": " And remember, the loss function measures how well you're doing on one training example,", "tokens": [50364, 400, 1604, 11, 264, 4470, 2445, 8000, 577, 731, 291, 434, 884, 322, 472, 3097, 1365, 11, 50642, 50642, 293, 307, 538, 2408, 2810, 493, 264, 15352, 322, 439, 295, 264, 3097, 5110, 300, 291, 550, 483, 264, 50866, 50866, 2063, 2445, 11, 597, 8000, 577, 731, 291, 434, 884, 322, 264, 2302, 3097, 992, 13, 51168, 51168, 407, 498, 291, 7542, 3565, 295, 283, 11, 309, 1542, 411, 341, 7605, 510, 11, 689, 283, 510, 307, 322, 264, 12750, 51604, 51604, 10298, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.11481627550992099, "compression_ratio": 1.780612244897959, "no_speech_prob": 2.7264566142548574e-06}, {"id": 61, "seek": 30664, "start": 312.2, "end": 316.68, "text": " and is by summing up the losses on all of the training examples that you then get the", "tokens": [50364, 400, 1604, 11, 264, 4470, 2445, 8000, 577, 731, 291, 434, 884, 322, 472, 3097, 1365, 11, 50642, 50642, 293, 307, 538, 2408, 2810, 493, 264, 15352, 322, 439, 295, 264, 3097, 5110, 300, 291, 550, 483, 264, 50866, 50866, 2063, 2445, 11, 597, 8000, 577, 731, 291, 434, 884, 322, 264, 2302, 3097, 992, 13, 51168, 51168, 407, 498, 291, 7542, 3565, 295, 283, 11, 309, 1542, 411, 341, 7605, 510, 11, 689, 283, 510, 307, 322, 264, 12750, 51604, 51604, 10298, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.11481627550992099, "compression_ratio": 1.780612244897959, "no_speech_prob": 2.7264566142548574e-06}, {"id": 62, "seek": 30664, "start": 316.68, "end": 322.71999999999997, "text": " cost function, which measures how well you're doing on the entire training set.", "tokens": [50364, 400, 1604, 11, 264, 4470, 2445, 8000, 577, 731, 291, 434, 884, 322, 472, 3097, 1365, 11, 50642, 50642, 293, 307, 538, 2408, 2810, 493, 264, 15352, 322, 439, 295, 264, 3097, 5110, 300, 291, 550, 483, 264, 50866, 50866, 2063, 2445, 11, 597, 8000, 577, 731, 291, 434, 884, 322, 264, 2302, 3097, 992, 13, 51168, 51168, 407, 498, 291, 7542, 3565, 295, 283, 11, 309, 1542, 411, 341, 7605, 510, 11, 689, 283, 510, 307, 322, 264, 12750, 51604, 51604, 10298, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.11481627550992099, "compression_ratio": 1.780612244897959, "no_speech_prob": 2.7264566142548574e-06}, {"id": 63, "seek": 30664, "start": 322.71999999999997, "end": 331.44, "text": " So if you plot log of f, it looks like this curve here, where f here is on the horizontal", "tokens": [50364, 400, 1604, 11, 264, 4470, 2445, 8000, 577, 731, 291, 434, 884, 322, 472, 3097, 1365, 11, 50642, 50642, 293, 307, 538, 2408, 2810, 493, 264, 15352, 322, 439, 295, 264, 3097, 5110, 300, 291, 550, 483, 264, 50866, 50866, 2063, 2445, 11, 597, 8000, 577, 731, 291, 434, 884, 322, 264, 2302, 3097, 992, 13, 51168, 51168, 407, 498, 291, 7542, 3565, 295, 283, 11, 309, 1542, 411, 341, 7605, 510, 11, 689, 283, 510, 307, 322, 264, 12750, 51604, 51604, 10298, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.11481627550992099, "compression_ratio": 1.780612244897959, "no_speech_prob": 2.7264566142548574e-06}, {"id": 64, "seek": 30664, "start": 331.44, "end": 332.76, "text": " axis.", "tokens": [50364, 400, 1604, 11, 264, 4470, 2445, 8000, 577, 731, 291, 434, 884, 322, 472, 3097, 1365, 11, 50642, 50642, 293, 307, 538, 2408, 2810, 493, 264, 15352, 322, 439, 295, 264, 3097, 5110, 300, 291, 550, 483, 264, 50866, 50866, 2063, 2445, 11, 597, 8000, 577, 731, 291, 434, 884, 322, 264, 2302, 3097, 992, 13, 51168, 51168, 407, 498, 291, 7542, 3565, 295, 283, 11, 309, 1542, 411, 341, 7605, 510, 11, 689, 283, 510, 307, 322, 264, 12750, 51604, 51604, 10298, 13, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.11481627550992099, "compression_ratio": 1.780612244897959, "no_speech_prob": 2.7264566142548574e-06}, {"id": 65, "seek": 33276, "start": 332.76, "end": 339.56, "text": " And so a plot of negative of the log of f looks like this, where we just flip the curve", "tokens": [50364, 400, 370, 257, 7542, 295, 3671, 295, 264, 3565, 295, 283, 1542, 411, 341, 11, 689, 321, 445, 7929, 264, 7605, 50704, 50704, 2051, 264, 12750, 10298, 13, 50844, 50844, 13428, 300, 309, 27815, 82, 264, 12750, 10298, 412, 283, 6915, 472, 11, 293, 6515, 24805, 490, 51162, 51162, 456, 13, 51284, 51284, 823, 283, 307, 264, 5598, 295, 3565, 3142, 24590, 13, 51436, 51436, 13827, 11, 283, 307, 1009, 1296, 4018, 293, 472, 11, 570, 264, 5598, 295, 3565, 3142, 24590, 307, 1009, 51749, 51749], "temperature": 0.0, "avg_logprob": -0.13581861149181018, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.2878851975983707e-06}, {"id": 66, "seek": 33276, "start": 339.56, "end": 342.36, "text": " along the horizontal axis.", "tokens": [50364, 400, 370, 257, 7542, 295, 3671, 295, 264, 3565, 295, 283, 1542, 411, 341, 11, 689, 321, 445, 7929, 264, 7605, 50704, 50704, 2051, 264, 12750, 10298, 13, 50844, 50844, 13428, 300, 309, 27815, 82, 264, 12750, 10298, 412, 283, 6915, 472, 11, 293, 6515, 24805, 490, 51162, 51162, 456, 13, 51284, 51284, 823, 283, 307, 264, 5598, 295, 3565, 3142, 24590, 13, 51436, 51436, 13827, 11, 283, 307, 1009, 1296, 4018, 293, 472, 11, 570, 264, 5598, 295, 3565, 3142, 24590, 307, 1009, 51749, 51749], "temperature": 0.0, "avg_logprob": -0.13581861149181018, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.2878851975983707e-06}, {"id": 67, "seek": 33276, "start": 342.36, "end": 348.71999999999997, "text": " Notice that it intersects the horizontal axis at f equals one, and continues downward from", "tokens": [50364, 400, 370, 257, 7542, 295, 3671, 295, 264, 3565, 295, 283, 1542, 411, 341, 11, 689, 321, 445, 7929, 264, 7605, 50704, 50704, 2051, 264, 12750, 10298, 13, 50844, 50844, 13428, 300, 309, 27815, 82, 264, 12750, 10298, 412, 283, 6915, 472, 11, 293, 6515, 24805, 490, 51162, 51162, 456, 13, 51284, 51284, 823, 283, 307, 264, 5598, 295, 3565, 3142, 24590, 13, 51436, 51436, 13827, 11, 283, 307, 1009, 1296, 4018, 293, 472, 11, 570, 264, 5598, 295, 3565, 3142, 24590, 307, 1009, 51749, 51749], "temperature": 0.0, "avg_logprob": -0.13581861149181018, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.2878851975983707e-06}, {"id": 68, "seek": 33276, "start": 348.71999999999997, "end": 351.15999999999997, "text": " there.", "tokens": [50364, 400, 370, 257, 7542, 295, 3671, 295, 264, 3565, 295, 283, 1542, 411, 341, 11, 689, 321, 445, 7929, 264, 7605, 50704, 50704, 2051, 264, 12750, 10298, 13, 50844, 50844, 13428, 300, 309, 27815, 82, 264, 12750, 10298, 412, 283, 6915, 472, 11, 293, 6515, 24805, 490, 51162, 51162, 456, 13, 51284, 51284, 823, 283, 307, 264, 5598, 295, 3565, 3142, 24590, 13, 51436, 51436, 13827, 11, 283, 307, 1009, 1296, 4018, 293, 472, 11, 570, 264, 5598, 295, 3565, 3142, 24590, 307, 1009, 51749, 51749], "temperature": 0.0, "avg_logprob": -0.13581861149181018, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.2878851975983707e-06}, {"id": 69, "seek": 33276, "start": 351.15999999999997, "end": 354.2, "text": " Now f is the output of logistic regression.", "tokens": [50364, 400, 370, 257, 7542, 295, 3671, 295, 264, 3565, 295, 283, 1542, 411, 341, 11, 689, 321, 445, 7929, 264, 7605, 50704, 50704, 2051, 264, 12750, 10298, 13, 50844, 50844, 13428, 300, 309, 27815, 82, 264, 12750, 10298, 412, 283, 6915, 472, 11, 293, 6515, 24805, 490, 51162, 51162, 456, 13, 51284, 51284, 823, 283, 307, 264, 5598, 295, 3565, 3142, 24590, 13, 51436, 51436, 13827, 11, 283, 307, 1009, 1296, 4018, 293, 472, 11, 570, 264, 5598, 295, 3565, 3142, 24590, 307, 1009, 51749, 51749], "temperature": 0.0, "avg_logprob": -0.13581861149181018, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.2878851975983707e-06}, {"id": 70, "seek": 33276, "start": 354.2, "end": 360.46, "text": " Thus, f is always between zero and one, because the output of logistic regression is always", "tokens": [50364, 400, 370, 257, 7542, 295, 3671, 295, 264, 3565, 295, 283, 1542, 411, 341, 11, 689, 321, 445, 7929, 264, 7605, 50704, 50704, 2051, 264, 12750, 10298, 13, 50844, 50844, 13428, 300, 309, 27815, 82, 264, 12750, 10298, 412, 283, 6915, 472, 11, 293, 6515, 24805, 490, 51162, 51162, 456, 13, 51284, 51284, 823, 283, 307, 264, 5598, 295, 3565, 3142, 24590, 13, 51436, 51436, 13827, 11, 283, 307, 1009, 1296, 4018, 293, 472, 11, 570, 264, 5598, 295, 3565, 3142, 24590, 307, 1009, 51749, 51749], "temperature": 0.0, "avg_logprob": -0.13581861149181018, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.2878851975983707e-06}, {"id": 71, "seek": 36046, "start": 360.46, "end": 363.26, "text": " between zero and one.", "tokens": [50364, 1296, 4018, 293, 472, 13, 50504, 50504, 440, 787, 644, 295, 264, 2445, 300, 311, 7340, 307, 4412, 341, 644, 670, 510, 11, 11760, 50864, 50864, 281, 283, 1296, 4018, 293, 472, 13, 51012, 51012, 407, 718, 311, 8863, 294, 293, 747, 257, 4966, 574, 412, 341, 644, 295, 264, 4295, 13, 51277, 51277, 759, 264, 9284, 6069, 82, 257, 8482, 1998, 281, 472, 11, 293, 264, 2074, 7645, 307, 472, 11, 550, 264, 51659, 51659], "temperature": 0.0, "avg_logprob": -0.0906366140414507, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.994418764501461e-06}, {"id": 72, "seek": 36046, "start": 363.26, "end": 370.46, "text": " The only part of the function that's relevant is therefore this part over here, corresponding", "tokens": [50364, 1296, 4018, 293, 472, 13, 50504, 50504, 440, 787, 644, 295, 264, 2445, 300, 311, 7340, 307, 4412, 341, 644, 670, 510, 11, 11760, 50864, 50864, 281, 283, 1296, 4018, 293, 472, 13, 51012, 51012, 407, 718, 311, 8863, 294, 293, 747, 257, 4966, 574, 412, 341, 644, 295, 264, 4295, 13, 51277, 51277, 759, 264, 9284, 6069, 82, 257, 8482, 1998, 281, 472, 11, 293, 264, 2074, 7645, 307, 472, 11, 550, 264, 51659, 51659], "temperature": 0.0, "avg_logprob": -0.0906366140414507, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.994418764501461e-06}, {"id": 73, "seek": 36046, "start": 370.46, "end": 373.41999999999996, "text": " to f between zero and one.", "tokens": [50364, 1296, 4018, 293, 472, 13, 50504, 50504, 440, 787, 644, 295, 264, 2445, 300, 311, 7340, 307, 4412, 341, 644, 670, 510, 11, 11760, 50864, 50864, 281, 283, 1296, 4018, 293, 472, 13, 51012, 51012, 407, 718, 311, 8863, 294, 293, 747, 257, 4966, 574, 412, 341, 644, 295, 264, 4295, 13, 51277, 51277, 759, 264, 9284, 6069, 82, 257, 8482, 1998, 281, 472, 11, 293, 264, 2074, 7645, 307, 472, 11, 550, 264, 51659, 51659], "temperature": 0.0, "avg_logprob": -0.0906366140414507, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.994418764501461e-06}, {"id": 74, "seek": 36046, "start": 373.41999999999996, "end": 378.71999999999997, "text": " So let's zoom in and take a closer look at this part of the graph.", "tokens": [50364, 1296, 4018, 293, 472, 13, 50504, 50504, 440, 787, 644, 295, 264, 2445, 300, 311, 7340, 307, 4412, 341, 644, 670, 510, 11, 11760, 50864, 50864, 281, 283, 1296, 4018, 293, 472, 13, 51012, 51012, 407, 718, 311, 8863, 294, 293, 747, 257, 4966, 574, 412, 341, 644, 295, 264, 4295, 13, 51277, 51277, 759, 264, 9284, 6069, 82, 257, 8482, 1998, 281, 472, 11, 293, 264, 2074, 7645, 307, 472, 11, 550, 264, 51659, 51659], "temperature": 0.0, "avg_logprob": -0.0906366140414507, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.994418764501461e-06}, {"id": 75, "seek": 36046, "start": 378.71999999999997, "end": 386.35999999999996, "text": " If the algorithm predicts a probability close to one, and the true label is one, then the", "tokens": [50364, 1296, 4018, 293, 472, 13, 50504, 50504, 440, 787, 644, 295, 264, 2445, 300, 311, 7340, 307, 4412, 341, 644, 670, 510, 11, 11760, 50864, 50864, 281, 283, 1296, 4018, 293, 472, 13, 51012, 51012, 407, 718, 311, 8863, 294, 293, 747, 257, 4966, 574, 412, 341, 644, 295, 264, 4295, 13, 51277, 51277, 759, 264, 9284, 6069, 82, 257, 8482, 1998, 281, 472, 11, 293, 264, 2074, 7645, 307, 472, 11, 550, 264, 51659, 51659], "temperature": 0.0, "avg_logprob": -0.0906366140414507, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.994418764501461e-06}, {"id": 76, "seek": 38636, "start": 386.36, "end": 394.36, "text": " loss is very small, it's pretty much zero, because you're very close to the right answer.", "tokens": [50364, 4470, 307, 588, 1359, 11, 309, 311, 1238, 709, 4018, 11, 570, 291, 434, 588, 1998, 281, 264, 558, 1867, 13, 50764, 50764, 823, 9289, 365, 264, 1365, 295, 264, 2074, 7645, 288, 885, 472, 11, 370, 584, 309, 534, 307, 257, 2806, 36818, 51054, 51054, 22512, 11, 498, 264, 9284, 6069, 82, 1958, 13, 20, 11, 550, 264, 4470, 307, 412, 341, 935, 510, 11, 597, 307, 257, 51520, 51520, 857, 2946, 11, 457, 406, 300, 1090, 13, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.150397679891931, "compression_ratio": 1.5125628140703518, "no_speech_prob": 4.49513254352496e-06}, {"id": 77, "seek": 38636, "start": 394.36, "end": 400.16, "text": " Now continuing with the example of the true label y being one, so say it really is a malignant", "tokens": [50364, 4470, 307, 588, 1359, 11, 309, 311, 1238, 709, 4018, 11, 570, 291, 434, 588, 1998, 281, 264, 558, 1867, 13, 50764, 50764, 823, 9289, 365, 264, 1365, 295, 264, 2074, 7645, 288, 885, 472, 11, 370, 584, 309, 534, 307, 257, 2806, 36818, 51054, 51054, 22512, 11, 498, 264, 9284, 6069, 82, 1958, 13, 20, 11, 550, 264, 4470, 307, 412, 341, 935, 510, 11, 597, 307, 257, 51520, 51520, 857, 2946, 11, 457, 406, 300, 1090, 13, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.150397679891931, "compression_ratio": 1.5125628140703518, "no_speech_prob": 4.49513254352496e-06}, {"id": 78, "seek": 38636, "start": 400.16, "end": 409.48, "text": " tumor, if the algorithm predicts 0.5, then the loss is at this point here, which is a", "tokens": [50364, 4470, 307, 588, 1359, 11, 309, 311, 1238, 709, 4018, 11, 570, 291, 434, 588, 1998, 281, 264, 558, 1867, 13, 50764, 50764, 823, 9289, 365, 264, 1365, 295, 264, 2074, 7645, 288, 885, 472, 11, 370, 584, 309, 534, 307, 257, 2806, 36818, 51054, 51054, 22512, 11, 498, 264, 9284, 6069, 82, 1958, 13, 20, 11, 550, 264, 4470, 307, 412, 341, 935, 510, 11, 597, 307, 257, 51520, 51520, 857, 2946, 11, 457, 406, 300, 1090, 13, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.150397679891931, "compression_ratio": 1.5125628140703518, "no_speech_prob": 4.49513254352496e-06}, {"id": 79, "seek": 38636, "start": 409.48, "end": 412.16, "text": " bit higher, but not that high.", "tokens": [50364, 4470, 307, 588, 1359, 11, 309, 311, 1238, 709, 4018, 11, 570, 291, 434, 588, 1998, 281, 264, 558, 1867, 13, 50764, 50764, 823, 9289, 365, 264, 1365, 295, 264, 2074, 7645, 288, 885, 472, 11, 370, 584, 309, 534, 307, 257, 2806, 36818, 51054, 51054, 22512, 11, 498, 264, 9284, 6069, 82, 1958, 13, 20, 11, 550, 264, 4470, 307, 412, 341, 935, 510, 11, 597, 307, 257, 51520, 51520, 857, 2946, 11, 457, 406, 300, 1090, 13, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.150397679891931, "compression_ratio": 1.5125628140703518, "no_speech_prob": 4.49513254352496e-06}, {"id": 80, "seek": 41216, "start": 412.16, "end": 419.0, "text": " Whereas in contrast, if the algorithm were to have output 0.1, if it thinks that there's", "tokens": [50364, 13813, 294, 8712, 11, 498, 264, 9284, 645, 281, 362, 5598, 1958, 13, 16, 11, 498, 309, 7309, 300, 456, 311, 50706, 50706, 787, 257, 1266, 4, 2931, 295, 264, 22512, 885, 2806, 36818, 11, 457, 288, 534, 307, 472, 11, 309, 534, 307, 2806, 36818, 11, 51048, 51048, 550, 264, 4470, 307, 341, 709, 2946, 2158, 670, 510, 13, 51318, 51318, 407, 562, 288, 307, 2681, 281, 472, 11, 264, 4470, 2445, 35328, 5660, 11, 420, 40045, 2880, 11, 420, 3665, 2944, 264, 51607, 51607], "temperature": 0.0, "avg_logprob": -0.11395364457910712, "compression_ratio": 1.5686274509803921, "no_speech_prob": 2.225257958343718e-06}, {"id": 81, "seek": 41216, "start": 419.0, "end": 425.84000000000003, "text": " only a 10% chance of the tumor being malignant, but y really is one, it really is malignant,", "tokens": [50364, 13813, 294, 8712, 11, 498, 264, 9284, 645, 281, 362, 5598, 1958, 13, 16, 11, 498, 309, 7309, 300, 456, 311, 50706, 50706, 787, 257, 1266, 4, 2931, 295, 264, 22512, 885, 2806, 36818, 11, 457, 288, 534, 307, 472, 11, 309, 534, 307, 2806, 36818, 11, 51048, 51048, 550, 264, 4470, 307, 341, 709, 2946, 2158, 670, 510, 13, 51318, 51318, 407, 562, 288, 307, 2681, 281, 472, 11, 264, 4470, 2445, 35328, 5660, 11, 420, 40045, 2880, 11, 420, 3665, 2944, 264, 51607, 51607], "temperature": 0.0, "avg_logprob": -0.11395364457910712, "compression_ratio": 1.5686274509803921, "no_speech_prob": 2.225257958343718e-06}, {"id": 82, "seek": 41216, "start": 425.84000000000003, "end": 431.24, "text": " then the loss is this much higher value over here.", "tokens": [50364, 13813, 294, 8712, 11, 498, 264, 9284, 645, 281, 362, 5598, 1958, 13, 16, 11, 498, 309, 7309, 300, 456, 311, 50706, 50706, 787, 257, 1266, 4, 2931, 295, 264, 22512, 885, 2806, 36818, 11, 457, 288, 534, 307, 472, 11, 309, 534, 307, 2806, 36818, 11, 51048, 51048, 550, 264, 4470, 307, 341, 709, 2946, 2158, 670, 510, 13, 51318, 51318, 407, 562, 288, 307, 2681, 281, 472, 11, 264, 4470, 2445, 35328, 5660, 11, 420, 40045, 2880, 11, 420, 3665, 2944, 264, 51607, 51607], "temperature": 0.0, "avg_logprob": -0.11395364457910712, "compression_ratio": 1.5686274509803921, "no_speech_prob": 2.225257958343718e-06}, {"id": 83, "seek": 41216, "start": 431.24, "end": 437.02000000000004, "text": " So when y is equal to one, the loss function incentivizes, or nudges, or helps push the", "tokens": [50364, 13813, 294, 8712, 11, 498, 264, 9284, 645, 281, 362, 5598, 1958, 13, 16, 11, 498, 309, 7309, 300, 456, 311, 50706, 50706, 787, 257, 1266, 4, 2931, 295, 264, 22512, 885, 2806, 36818, 11, 457, 288, 534, 307, 472, 11, 309, 534, 307, 2806, 36818, 11, 51048, 51048, 550, 264, 4470, 307, 341, 709, 2946, 2158, 670, 510, 13, 51318, 51318, 407, 562, 288, 307, 2681, 281, 472, 11, 264, 4470, 2445, 35328, 5660, 11, 420, 40045, 2880, 11, 420, 3665, 2944, 264, 51607, 51607], "temperature": 0.0, "avg_logprob": -0.11395364457910712, "compression_ratio": 1.5686274509803921, "no_speech_prob": 2.225257958343718e-06}, {"id": 84, "seek": 43702, "start": 437.02, "end": 442.62, "text": " algorithm to make more accurate predictions, because the loss is lowest when it predicts", "tokens": [50364, 9284, 281, 652, 544, 8559, 21264, 11, 570, 264, 4470, 307, 12437, 562, 309, 6069, 82, 50644, 50644, 4190, 1998, 281, 472, 13, 50775, 50775, 823, 322, 341, 4137, 11, 321, 600, 668, 1237, 412, 437, 264, 4470, 307, 562, 288, 307, 2681, 281, 472, 13, 51045, 51045, 1282, 341, 4137, 11, 718, 311, 574, 412, 264, 1150, 644, 295, 264, 4470, 2445, 11760, 281, 562, 51267, 51267, 288, 307, 2681, 281, 4018, 13, 51383, 51383, 682, 341, 1389, 11, 264, 4470, 307, 3671, 3565, 295, 472, 3175, 283, 295, 2031, 13, 51691, 51691], "temperature": 0.0, "avg_logprob": -0.07918944458166759, "compression_ratio": 1.7598039215686274, "no_speech_prob": 8.939604413171764e-06}, {"id": 85, "seek": 43702, "start": 442.62, "end": 445.24, "text": " values close to one.", "tokens": [50364, 9284, 281, 652, 544, 8559, 21264, 11, 570, 264, 4470, 307, 12437, 562, 309, 6069, 82, 50644, 50644, 4190, 1998, 281, 472, 13, 50775, 50775, 823, 322, 341, 4137, 11, 321, 600, 668, 1237, 412, 437, 264, 4470, 307, 562, 288, 307, 2681, 281, 472, 13, 51045, 51045, 1282, 341, 4137, 11, 718, 311, 574, 412, 264, 1150, 644, 295, 264, 4470, 2445, 11760, 281, 562, 51267, 51267, 288, 307, 2681, 281, 4018, 13, 51383, 51383, 682, 341, 1389, 11, 264, 4470, 307, 3671, 3565, 295, 472, 3175, 283, 295, 2031, 13, 51691, 51691], "temperature": 0.0, "avg_logprob": -0.07918944458166759, "compression_ratio": 1.7598039215686274, "no_speech_prob": 8.939604413171764e-06}, {"id": 86, "seek": 43702, "start": 445.24, "end": 450.64, "text": " Now on this slide, we've been looking at what the loss is when y is equal to one.", "tokens": [50364, 9284, 281, 652, 544, 8559, 21264, 11, 570, 264, 4470, 307, 12437, 562, 309, 6069, 82, 50644, 50644, 4190, 1998, 281, 472, 13, 50775, 50775, 823, 322, 341, 4137, 11, 321, 600, 668, 1237, 412, 437, 264, 4470, 307, 562, 288, 307, 2681, 281, 472, 13, 51045, 51045, 1282, 341, 4137, 11, 718, 311, 574, 412, 264, 1150, 644, 295, 264, 4470, 2445, 11760, 281, 562, 51267, 51267, 288, 307, 2681, 281, 4018, 13, 51383, 51383, 682, 341, 1389, 11, 264, 4470, 307, 3671, 3565, 295, 472, 3175, 283, 295, 2031, 13, 51691, 51691], "temperature": 0.0, "avg_logprob": -0.07918944458166759, "compression_ratio": 1.7598039215686274, "no_speech_prob": 8.939604413171764e-06}, {"id": 87, "seek": 43702, "start": 450.64, "end": 455.08, "text": " On this slide, let's look at the second part of the loss function corresponding to when", "tokens": [50364, 9284, 281, 652, 544, 8559, 21264, 11, 570, 264, 4470, 307, 12437, 562, 309, 6069, 82, 50644, 50644, 4190, 1998, 281, 472, 13, 50775, 50775, 823, 322, 341, 4137, 11, 321, 600, 668, 1237, 412, 437, 264, 4470, 307, 562, 288, 307, 2681, 281, 472, 13, 51045, 51045, 1282, 341, 4137, 11, 718, 311, 574, 412, 264, 1150, 644, 295, 264, 4470, 2445, 11760, 281, 562, 51267, 51267, 288, 307, 2681, 281, 4018, 13, 51383, 51383, 682, 341, 1389, 11, 264, 4470, 307, 3671, 3565, 295, 472, 3175, 283, 295, 2031, 13, 51691, 51691], "temperature": 0.0, "avg_logprob": -0.07918944458166759, "compression_ratio": 1.7598039215686274, "no_speech_prob": 8.939604413171764e-06}, {"id": 88, "seek": 43702, "start": 455.08, "end": 457.4, "text": " y is equal to zero.", "tokens": [50364, 9284, 281, 652, 544, 8559, 21264, 11, 570, 264, 4470, 307, 12437, 562, 309, 6069, 82, 50644, 50644, 4190, 1998, 281, 472, 13, 50775, 50775, 823, 322, 341, 4137, 11, 321, 600, 668, 1237, 412, 437, 264, 4470, 307, 562, 288, 307, 2681, 281, 472, 13, 51045, 51045, 1282, 341, 4137, 11, 718, 311, 574, 412, 264, 1150, 644, 295, 264, 4470, 2445, 11760, 281, 562, 51267, 51267, 288, 307, 2681, 281, 4018, 13, 51383, 51383, 682, 341, 1389, 11, 264, 4470, 307, 3671, 3565, 295, 472, 3175, 283, 295, 2031, 13, 51691, 51691], "temperature": 0.0, "avg_logprob": -0.07918944458166759, "compression_ratio": 1.7598039215686274, "no_speech_prob": 8.939604413171764e-06}, {"id": 89, "seek": 43702, "start": 457.4, "end": 463.56, "text": " In this case, the loss is negative log of one minus f of x.", "tokens": [50364, 9284, 281, 652, 544, 8559, 21264, 11, 570, 264, 4470, 307, 12437, 562, 309, 6069, 82, 50644, 50644, 4190, 1998, 281, 472, 13, 50775, 50775, 823, 322, 341, 4137, 11, 321, 600, 668, 1237, 412, 437, 264, 4470, 307, 562, 288, 307, 2681, 281, 472, 13, 51045, 51045, 1282, 341, 4137, 11, 718, 311, 574, 412, 264, 1150, 644, 295, 264, 4470, 2445, 11760, 281, 562, 51267, 51267, 288, 307, 2681, 281, 4018, 13, 51383, 51383, 682, 341, 1389, 11, 264, 4470, 307, 3671, 3565, 295, 472, 3175, 283, 295, 2031, 13, 51691, 51691], "temperature": 0.0, "avg_logprob": -0.07918944458166759, "compression_ratio": 1.7598039215686274, "no_speech_prob": 8.939604413171764e-06}, {"id": 90, "seek": 46356, "start": 463.56, "end": 468.16, "text": " When this function is plotted, it actually looks like this.", "tokens": [50364, 1133, 341, 2445, 307, 43288, 11, 309, 767, 1542, 411, 341, 13, 50594, 50594, 440, 3613, 295, 283, 307, 5567, 281, 4018, 281, 472, 11, 570, 3565, 3142, 24590, 787, 23930, 4190, 50922, 50922, 1296, 4018, 293, 472, 11, 293, 498, 321, 8863, 294, 11, 341, 307, 437, 309, 1542, 411, 13, 51233, 51233, 407, 294, 341, 7542, 11, 11760, 281, 288, 6915, 4018, 11, 264, 9429, 10298, 3110, 264, 2158, 295, 51594, 51594, 264, 4470, 337, 819, 4190, 295, 283, 295, 2031, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.0810900330543518, "compression_ratio": 1.5972222222222223, "no_speech_prob": 3.393110773686203e-06}, {"id": 91, "seek": 46356, "start": 468.16, "end": 474.72, "text": " The range of f is limited to zero to one, because logistic regression only outputs values", "tokens": [50364, 1133, 341, 2445, 307, 43288, 11, 309, 767, 1542, 411, 341, 13, 50594, 50594, 440, 3613, 295, 283, 307, 5567, 281, 4018, 281, 472, 11, 570, 3565, 3142, 24590, 787, 23930, 4190, 50922, 50922, 1296, 4018, 293, 472, 11, 293, 498, 321, 8863, 294, 11, 341, 307, 437, 309, 1542, 411, 13, 51233, 51233, 407, 294, 341, 7542, 11, 11760, 281, 288, 6915, 4018, 11, 264, 9429, 10298, 3110, 264, 2158, 295, 51594, 51594, 264, 4470, 337, 819, 4190, 295, 283, 295, 2031, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.0810900330543518, "compression_ratio": 1.5972222222222223, "no_speech_prob": 3.393110773686203e-06}, {"id": 92, "seek": 46356, "start": 474.72, "end": 480.94, "text": " between zero and one, and if we zoom in, this is what it looks like.", "tokens": [50364, 1133, 341, 2445, 307, 43288, 11, 309, 767, 1542, 411, 341, 13, 50594, 50594, 440, 3613, 295, 283, 307, 5567, 281, 4018, 281, 472, 11, 570, 3565, 3142, 24590, 787, 23930, 4190, 50922, 50922, 1296, 4018, 293, 472, 11, 293, 498, 321, 8863, 294, 11, 341, 307, 437, 309, 1542, 411, 13, 51233, 51233, 407, 294, 341, 7542, 11, 11760, 281, 288, 6915, 4018, 11, 264, 9429, 10298, 3110, 264, 2158, 295, 51594, 51594, 264, 4470, 337, 819, 4190, 295, 283, 295, 2031, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.0810900330543518, "compression_ratio": 1.5972222222222223, "no_speech_prob": 3.393110773686203e-06}, {"id": 93, "seek": 46356, "start": 480.94, "end": 488.16, "text": " So in this plot, corresponding to y equals zero, the vertical axis shows the value of", "tokens": [50364, 1133, 341, 2445, 307, 43288, 11, 309, 767, 1542, 411, 341, 13, 50594, 50594, 440, 3613, 295, 283, 307, 5567, 281, 4018, 281, 472, 11, 570, 3565, 3142, 24590, 787, 23930, 4190, 50922, 50922, 1296, 4018, 293, 472, 11, 293, 498, 321, 8863, 294, 11, 341, 307, 437, 309, 1542, 411, 13, 51233, 51233, 407, 294, 341, 7542, 11, 11760, 281, 288, 6915, 4018, 11, 264, 9429, 10298, 3110, 264, 2158, 295, 51594, 51594, 264, 4470, 337, 819, 4190, 295, 283, 295, 2031, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.0810900330543518, "compression_ratio": 1.5972222222222223, "no_speech_prob": 3.393110773686203e-06}, {"id": 94, "seek": 46356, "start": 488.16, "end": 493.48, "text": " the loss for different values of f of x.", "tokens": [50364, 1133, 341, 2445, 307, 43288, 11, 309, 767, 1542, 411, 341, 13, 50594, 50594, 440, 3613, 295, 283, 307, 5567, 281, 4018, 281, 472, 11, 570, 3565, 3142, 24590, 787, 23930, 4190, 50922, 50922, 1296, 4018, 293, 472, 11, 293, 498, 321, 8863, 294, 11, 341, 307, 437, 309, 1542, 411, 13, 51233, 51233, 407, 294, 341, 7542, 11, 11760, 281, 288, 6915, 4018, 11, 264, 9429, 10298, 3110, 264, 2158, 295, 51594, 51594, 264, 4470, 337, 819, 4190, 295, 283, 295, 2031, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.0810900330543518, "compression_ratio": 1.5972222222222223, "no_speech_prob": 3.393110773686203e-06}, {"id": 95, "seek": 49348, "start": 493.48, "end": 500.44, "text": " So when f is zero, or very close to zero, the loss is also going to be very small, which", "tokens": [50364, 407, 562, 283, 307, 4018, 11, 420, 588, 1998, 281, 4018, 11, 264, 4470, 307, 611, 516, 281, 312, 588, 1359, 11, 597, 50712, 50712, 1355, 300, 498, 264, 2074, 7645, 307, 4018, 293, 264, 2316, 311, 17630, 307, 588, 1998, 281, 4018, 11, 50946, 50946, 731, 11, 291, 6217, 658, 309, 558, 13, 51068, 51068, 407, 264, 4470, 307, 23505, 588, 1998, 281, 4018, 13, 51298, 51298, 400, 264, 4833, 264, 2158, 295, 283, 295, 2031, 2170, 11, 264, 3801, 264, 4470, 11, 570, 264, 17630, 307, 51551, 51551, 3052, 490, 264, 2074, 7645, 4018, 13, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.10766040688694113, "compression_ratio": 1.8480392156862746, "no_speech_prob": 3.966943950217683e-06}, {"id": 96, "seek": 49348, "start": 500.44, "end": 505.12, "text": " means that if the true label is zero and the model's prediction is very close to zero,", "tokens": [50364, 407, 562, 283, 307, 4018, 11, 420, 588, 1998, 281, 4018, 11, 264, 4470, 307, 611, 516, 281, 312, 588, 1359, 11, 597, 50712, 50712, 1355, 300, 498, 264, 2074, 7645, 307, 4018, 293, 264, 2316, 311, 17630, 307, 588, 1998, 281, 4018, 11, 50946, 50946, 731, 11, 291, 6217, 658, 309, 558, 13, 51068, 51068, 407, 264, 4470, 307, 23505, 588, 1998, 281, 4018, 13, 51298, 51298, 400, 264, 4833, 264, 2158, 295, 283, 295, 2031, 2170, 11, 264, 3801, 264, 4470, 11, 570, 264, 17630, 307, 51551, 51551, 3052, 490, 264, 2074, 7645, 4018, 13, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.10766040688694113, "compression_ratio": 1.8480392156862746, "no_speech_prob": 3.966943950217683e-06}, {"id": 97, "seek": 49348, "start": 505.12, "end": 507.56, "text": " well, you nearly got it right.", "tokens": [50364, 407, 562, 283, 307, 4018, 11, 420, 588, 1998, 281, 4018, 11, 264, 4470, 307, 611, 516, 281, 312, 588, 1359, 11, 597, 50712, 50712, 1355, 300, 498, 264, 2074, 7645, 307, 4018, 293, 264, 2316, 311, 17630, 307, 588, 1998, 281, 4018, 11, 50946, 50946, 731, 11, 291, 6217, 658, 309, 558, 13, 51068, 51068, 407, 264, 4470, 307, 23505, 588, 1998, 281, 4018, 13, 51298, 51298, 400, 264, 4833, 264, 2158, 295, 283, 295, 2031, 2170, 11, 264, 3801, 264, 4470, 11, 570, 264, 17630, 307, 51551, 51551, 3052, 490, 264, 2074, 7645, 4018, 13, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.10766040688694113, "compression_ratio": 1.8480392156862746, "no_speech_prob": 3.966943950217683e-06}, {"id": 98, "seek": 49348, "start": 507.56, "end": 512.16, "text": " So the loss is appropriately very close to zero.", "tokens": [50364, 407, 562, 283, 307, 4018, 11, 420, 588, 1998, 281, 4018, 11, 264, 4470, 307, 611, 516, 281, 312, 588, 1359, 11, 597, 50712, 50712, 1355, 300, 498, 264, 2074, 7645, 307, 4018, 293, 264, 2316, 311, 17630, 307, 588, 1998, 281, 4018, 11, 50946, 50946, 731, 11, 291, 6217, 658, 309, 558, 13, 51068, 51068, 407, 264, 4470, 307, 23505, 588, 1998, 281, 4018, 13, 51298, 51298, 400, 264, 4833, 264, 2158, 295, 283, 295, 2031, 2170, 11, 264, 3801, 264, 4470, 11, 570, 264, 17630, 307, 51551, 51551, 3052, 490, 264, 2074, 7645, 4018, 13, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.10766040688694113, "compression_ratio": 1.8480392156862746, "no_speech_prob": 3.966943950217683e-06}, {"id": 99, "seek": 49348, "start": 512.16, "end": 517.22, "text": " And the larger the value of f of x gets, the bigger the loss, because the prediction is", "tokens": [50364, 407, 562, 283, 307, 4018, 11, 420, 588, 1998, 281, 4018, 11, 264, 4470, 307, 611, 516, 281, 312, 588, 1359, 11, 597, 50712, 50712, 1355, 300, 498, 264, 2074, 7645, 307, 4018, 293, 264, 2316, 311, 17630, 307, 588, 1998, 281, 4018, 11, 50946, 50946, 731, 11, 291, 6217, 658, 309, 558, 13, 51068, 51068, 407, 264, 4470, 307, 23505, 588, 1998, 281, 4018, 13, 51298, 51298, 400, 264, 4833, 264, 2158, 295, 283, 295, 2031, 2170, 11, 264, 3801, 264, 4470, 11, 570, 264, 17630, 307, 51551, 51551, 3052, 490, 264, 2074, 7645, 4018, 13, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.10766040688694113, "compression_ratio": 1.8480392156862746, "no_speech_prob": 3.966943950217683e-06}, {"id": 100, "seek": 49348, "start": 517.22, "end": 520.76, "text": " further from the true label zero.", "tokens": [50364, 407, 562, 283, 307, 4018, 11, 420, 588, 1998, 281, 4018, 11, 264, 4470, 307, 611, 516, 281, 312, 588, 1359, 11, 597, 50712, 50712, 1355, 300, 498, 264, 2074, 7645, 307, 4018, 293, 264, 2316, 311, 17630, 307, 588, 1998, 281, 4018, 11, 50946, 50946, 731, 11, 291, 6217, 658, 309, 558, 13, 51068, 51068, 407, 264, 4470, 307, 23505, 588, 1998, 281, 4018, 13, 51298, 51298, 400, 264, 4833, 264, 2158, 295, 283, 295, 2031, 2170, 11, 264, 3801, 264, 4470, 11, 570, 264, 17630, 307, 51551, 51551, 3052, 490, 264, 2074, 7645, 4018, 13, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.10766040688694113, "compression_ratio": 1.8480392156862746, "no_speech_prob": 3.966943950217683e-06}, {"id": 101, "seek": 52076, "start": 520.76, "end": 527.36, "text": " And in fact, as that prediction approaches one, the loss actually approaches infinity.", "tokens": [50364, 400, 294, 1186, 11, 382, 300, 17630, 11587, 472, 11, 264, 4470, 767, 11587, 13202, 13, 50694, 50694, 10963, 646, 281, 264, 22512, 17630, 1365, 11, 341, 307, 498, 257, 2316, 6069, 82, 300, 264, 4537, 311, 50960, 50960, 22512, 307, 1920, 1629, 281, 312, 2806, 36818, 11, 584, 11803, 13, 24, 4, 2931, 295, 2806, 788, 6717, 11, 457, 309, 4523, 484, 51294, 51294, 281, 767, 406, 312, 2806, 36818, 13, 51382, 51382, 407, 288, 6915, 4018, 11, 550, 321, 13661, 1125, 264, 2316, 365, 257, 588, 1090, 4470, 13, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.08104033721120733, "compression_ratio": 1.6441441441441442, "no_speech_prob": 4.222798906994285e-06}, {"id": 102, "seek": 52076, "start": 527.36, "end": 532.68, "text": " Going back to the tumor prediction example, this is if a model predicts that the patient's", "tokens": [50364, 400, 294, 1186, 11, 382, 300, 17630, 11587, 472, 11, 264, 4470, 767, 11587, 13202, 13, 50694, 50694, 10963, 646, 281, 264, 22512, 17630, 1365, 11, 341, 307, 498, 257, 2316, 6069, 82, 300, 264, 4537, 311, 50960, 50960, 22512, 307, 1920, 1629, 281, 312, 2806, 36818, 11, 584, 11803, 13, 24, 4, 2931, 295, 2806, 788, 6717, 11, 457, 309, 4523, 484, 51294, 51294, 281, 767, 406, 312, 2806, 36818, 13, 51382, 51382, 407, 288, 6915, 4018, 11, 550, 321, 13661, 1125, 264, 2316, 365, 257, 588, 1090, 4470, 13, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.08104033721120733, "compression_ratio": 1.6441441441441442, "no_speech_prob": 4.222798906994285e-06}, {"id": 103, "seek": 52076, "start": 532.68, "end": 539.36, "text": " tumor is almost certain to be malignant, say 99.9% chance of malignancy, but it turns out", "tokens": [50364, 400, 294, 1186, 11, 382, 300, 17630, 11587, 472, 11, 264, 4470, 767, 11587, 13202, 13, 50694, 50694, 10963, 646, 281, 264, 22512, 17630, 1365, 11, 341, 307, 498, 257, 2316, 6069, 82, 300, 264, 4537, 311, 50960, 50960, 22512, 307, 1920, 1629, 281, 312, 2806, 36818, 11, 584, 11803, 13, 24, 4, 2931, 295, 2806, 788, 6717, 11, 457, 309, 4523, 484, 51294, 51294, 281, 767, 406, 312, 2806, 36818, 13, 51382, 51382, 407, 288, 6915, 4018, 11, 550, 321, 13661, 1125, 264, 2316, 365, 257, 588, 1090, 4470, 13, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.08104033721120733, "compression_ratio": 1.6441441441441442, "no_speech_prob": 4.222798906994285e-06}, {"id": 104, "seek": 52076, "start": 539.36, "end": 541.12, "text": " to actually not be malignant.", "tokens": [50364, 400, 294, 1186, 11, 382, 300, 17630, 11587, 472, 11, 264, 4470, 767, 11587, 13202, 13, 50694, 50694, 10963, 646, 281, 264, 22512, 17630, 1365, 11, 341, 307, 498, 257, 2316, 6069, 82, 300, 264, 4537, 311, 50960, 50960, 22512, 307, 1920, 1629, 281, 312, 2806, 36818, 11, 584, 11803, 13, 24, 4, 2931, 295, 2806, 788, 6717, 11, 457, 309, 4523, 484, 51294, 51294, 281, 767, 406, 312, 2806, 36818, 13, 51382, 51382, 407, 288, 6915, 4018, 11, 550, 321, 13661, 1125, 264, 2316, 365, 257, 588, 1090, 4470, 13, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.08104033721120733, "compression_ratio": 1.6441441441441442, "no_speech_prob": 4.222798906994285e-06}, {"id": 105, "seek": 52076, "start": 541.12, "end": 547.24, "text": " So y equals zero, then we penalize the model with a very high loss.", "tokens": [50364, 400, 294, 1186, 11, 382, 300, 17630, 11587, 472, 11, 264, 4470, 767, 11587, 13202, 13, 50694, 50694, 10963, 646, 281, 264, 22512, 17630, 1365, 11, 341, 307, 498, 257, 2316, 6069, 82, 300, 264, 4537, 311, 50960, 50960, 22512, 307, 1920, 1629, 281, 312, 2806, 36818, 11, 584, 11803, 13, 24, 4, 2931, 295, 2806, 788, 6717, 11, 457, 309, 4523, 484, 51294, 51294, 281, 767, 406, 312, 2806, 36818, 13, 51382, 51382, 407, 288, 6915, 4018, 11, 550, 321, 13661, 1125, 264, 2316, 365, 257, 588, 1090, 4470, 13, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.08104033721120733, "compression_ratio": 1.6441441441441442, "no_speech_prob": 4.222798906994285e-06}, {"id": 106, "seek": 54724, "start": 547.24, "end": 552.36, "text": " So in this case of y equals zero, similar to the case of y equals one on the previous", "tokens": [50364, 407, 294, 341, 1389, 295, 288, 6915, 4018, 11, 2531, 281, 264, 1389, 295, 288, 6915, 472, 322, 264, 3894, 50620, 50620, 4137, 11, 264, 3052, 264, 17630, 283, 295, 2031, 307, 1314, 490, 264, 2074, 2158, 295, 288, 11, 264, 2946, 50886, 50886, 264, 4470, 13, 50936, 50936, 400, 294, 1186, 11, 498, 283, 295, 2031, 11587, 4018, 11, 264, 4470, 510, 767, 1709, 534, 11, 534, 2416, 51342, 51342, 293, 294, 1186, 11587, 13202, 13, 51494, 51494, 407, 562, 264, 2074, 7645, 307, 472, 11, 264, 9284, 307, 10613, 35328, 1602, 406, 281, 6069, 746, 51815, 51815], "temperature": 0.0, "avg_logprob": -0.09290645146133876, "compression_ratio": 1.7847533632286996, "no_speech_prob": 9.13251483325439e-07}, {"id": 107, "seek": 54724, "start": 552.36, "end": 557.6800000000001, "text": " slide, the further the prediction f of x is away from the true value of y, the higher", "tokens": [50364, 407, 294, 341, 1389, 295, 288, 6915, 4018, 11, 2531, 281, 264, 1389, 295, 288, 6915, 472, 322, 264, 3894, 50620, 50620, 4137, 11, 264, 3052, 264, 17630, 283, 295, 2031, 307, 1314, 490, 264, 2074, 2158, 295, 288, 11, 264, 2946, 50886, 50886, 264, 4470, 13, 50936, 50936, 400, 294, 1186, 11, 498, 283, 295, 2031, 11587, 4018, 11, 264, 4470, 510, 767, 1709, 534, 11, 534, 2416, 51342, 51342, 293, 294, 1186, 11587, 13202, 13, 51494, 51494, 407, 562, 264, 2074, 7645, 307, 472, 11, 264, 9284, 307, 10613, 35328, 1602, 406, 281, 6069, 746, 51815, 51815], "temperature": 0.0, "avg_logprob": -0.09290645146133876, "compression_ratio": 1.7847533632286996, "no_speech_prob": 9.13251483325439e-07}, {"id": 108, "seek": 54724, "start": 557.6800000000001, "end": 558.6800000000001, "text": " the loss.", "tokens": [50364, 407, 294, 341, 1389, 295, 288, 6915, 4018, 11, 2531, 281, 264, 1389, 295, 288, 6915, 472, 322, 264, 3894, 50620, 50620, 4137, 11, 264, 3052, 264, 17630, 283, 295, 2031, 307, 1314, 490, 264, 2074, 2158, 295, 288, 11, 264, 2946, 50886, 50886, 264, 4470, 13, 50936, 50936, 400, 294, 1186, 11, 498, 283, 295, 2031, 11587, 4018, 11, 264, 4470, 510, 767, 1709, 534, 11, 534, 2416, 51342, 51342, 293, 294, 1186, 11587, 13202, 13, 51494, 51494, 407, 562, 264, 2074, 7645, 307, 472, 11, 264, 9284, 307, 10613, 35328, 1602, 406, 281, 6069, 746, 51815, 51815], "temperature": 0.0, "avg_logprob": -0.09290645146133876, "compression_ratio": 1.7847533632286996, "no_speech_prob": 9.13251483325439e-07}, {"id": 109, "seek": 54724, "start": 558.6800000000001, "end": 566.8, "text": " And in fact, if f of x approaches zero, the loss here actually goes really, really large", "tokens": [50364, 407, 294, 341, 1389, 295, 288, 6915, 4018, 11, 2531, 281, 264, 1389, 295, 288, 6915, 472, 322, 264, 3894, 50620, 50620, 4137, 11, 264, 3052, 264, 17630, 283, 295, 2031, 307, 1314, 490, 264, 2074, 2158, 295, 288, 11, 264, 2946, 50886, 50886, 264, 4470, 13, 50936, 50936, 400, 294, 1186, 11, 498, 283, 295, 2031, 11587, 4018, 11, 264, 4470, 510, 767, 1709, 534, 11, 534, 2416, 51342, 51342, 293, 294, 1186, 11587, 13202, 13, 51494, 51494, 407, 562, 264, 2074, 7645, 307, 472, 11, 264, 9284, 307, 10613, 35328, 1602, 406, 281, 6069, 746, 51815, 51815], "temperature": 0.0, "avg_logprob": -0.09290645146133876, "compression_ratio": 1.7847533632286996, "no_speech_prob": 9.13251483325439e-07}, {"id": 110, "seek": 54724, "start": 566.8, "end": 569.84, "text": " and in fact approaches infinity.", "tokens": [50364, 407, 294, 341, 1389, 295, 288, 6915, 4018, 11, 2531, 281, 264, 1389, 295, 288, 6915, 472, 322, 264, 3894, 50620, 50620, 4137, 11, 264, 3052, 264, 17630, 283, 295, 2031, 307, 1314, 490, 264, 2074, 2158, 295, 288, 11, 264, 2946, 50886, 50886, 264, 4470, 13, 50936, 50936, 400, 294, 1186, 11, 498, 283, 295, 2031, 11587, 4018, 11, 264, 4470, 510, 767, 1709, 534, 11, 534, 2416, 51342, 51342, 293, 294, 1186, 11587, 13202, 13, 51494, 51494, 407, 562, 264, 2074, 7645, 307, 472, 11, 264, 9284, 307, 10613, 35328, 1602, 406, 281, 6069, 746, 51815, 51815], "temperature": 0.0, "avg_logprob": -0.09290645146133876, "compression_ratio": 1.7847533632286996, "no_speech_prob": 9.13251483325439e-07}, {"id": 111, "seek": 54724, "start": 569.84, "end": 576.26, "text": " So when the true label is one, the algorithm is strongly incentivized not to predict something", "tokens": [50364, 407, 294, 341, 1389, 295, 288, 6915, 4018, 11, 2531, 281, 264, 1389, 295, 288, 6915, 472, 322, 264, 3894, 50620, 50620, 4137, 11, 264, 3052, 264, 17630, 283, 295, 2031, 307, 1314, 490, 264, 2074, 2158, 295, 288, 11, 264, 2946, 50886, 50886, 264, 4470, 13, 50936, 50936, 400, 294, 1186, 11, 498, 283, 295, 2031, 11587, 4018, 11, 264, 4470, 510, 767, 1709, 534, 11, 534, 2416, 51342, 51342, 293, 294, 1186, 11587, 13202, 13, 51494, 51494, 407, 562, 264, 2074, 7645, 307, 472, 11, 264, 9284, 307, 10613, 35328, 1602, 406, 281, 6069, 746, 51815, 51815], "temperature": 0.0, "avg_logprob": -0.09290645146133876, "compression_ratio": 1.7847533632286996, "no_speech_prob": 9.13251483325439e-07}, {"id": 112, "seek": 57626, "start": 576.26, "end": 578.88, "text": " too close to zero.", "tokens": [50364, 886, 1998, 281, 4018, 13, 50495, 50495, 407, 294, 341, 960, 11, 291, 1866, 983, 264, 8889, 6713, 2063, 2445, 1177, 380, 589, 731, 337, 50695, 50695, 3565, 3142, 24590, 13, 50819, 50819, 492, 611, 7642, 264, 4470, 337, 257, 2167, 3097, 1365, 293, 1361, 493, 365, 257, 777, 7123, 51207, 51207, 337, 264, 4470, 2445, 337, 3565, 3142, 24590, 13, 51449, 51449, 467, 4523, 484, 300, 365, 341, 3922, 295, 4470, 2445, 11, 264, 4787, 2063, 2445, 486, 312, 51721, 51721], "temperature": 0.0, "avg_logprob": -0.07094566027323405, "compression_ratio": 1.7213930348258706, "no_speech_prob": 4.56592533737421e-06}, {"id": 113, "seek": 57626, "start": 578.88, "end": 582.88, "text": " So in this video, you saw why the squared error cost function doesn't work well for", "tokens": [50364, 886, 1998, 281, 4018, 13, 50495, 50495, 407, 294, 341, 960, 11, 291, 1866, 983, 264, 8889, 6713, 2063, 2445, 1177, 380, 589, 731, 337, 50695, 50695, 3565, 3142, 24590, 13, 50819, 50819, 492, 611, 7642, 264, 4470, 337, 257, 2167, 3097, 1365, 293, 1361, 493, 365, 257, 777, 7123, 51207, 51207, 337, 264, 4470, 2445, 337, 3565, 3142, 24590, 13, 51449, 51449, 467, 4523, 484, 300, 365, 341, 3922, 295, 4470, 2445, 11, 264, 4787, 2063, 2445, 486, 312, 51721, 51721], "temperature": 0.0, "avg_logprob": -0.07094566027323405, "compression_ratio": 1.7213930348258706, "no_speech_prob": 4.56592533737421e-06}, {"id": 114, "seek": 57626, "start": 582.88, "end": 585.36, "text": " logistic regression.", "tokens": [50364, 886, 1998, 281, 4018, 13, 50495, 50495, 407, 294, 341, 960, 11, 291, 1866, 983, 264, 8889, 6713, 2063, 2445, 1177, 380, 589, 731, 337, 50695, 50695, 3565, 3142, 24590, 13, 50819, 50819, 492, 611, 7642, 264, 4470, 337, 257, 2167, 3097, 1365, 293, 1361, 493, 365, 257, 777, 7123, 51207, 51207, 337, 264, 4470, 2445, 337, 3565, 3142, 24590, 13, 51449, 51449, 467, 4523, 484, 300, 365, 341, 3922, 295, 4470, 2445, 11, 264, 4787, 2063, 2445, 486, 312, 51721, 51721], "temperature": 0.0, "avg_logprob": -0.07094566027323405, "compression_ratio": 1.7213930348258706, "no_speech_prob": 4.56592533737421e-06}, {"id": 115, "seek": 57626, "start": 585.36, "end": 593.12, "text": " We also defined the loss for a single training example and came up with a new definition", "tokens": [50364, 886, 1998, 281, 4018, 13, 50495, 50495, 407, 294, 341, 960, 11, 291, 1866, 983, 264, 8889, 6713, 2063, 2445, 1177, 380, 589, 731, 337, 50695, 50695, 3565, 3142, 24590, 13, 50819, 50819, 492, 611, 7642, 264, 4470, 337, 257, 2167, 3097, 1365, 293, 1361, 493, 365, 257, 777, 7123, 51207, 51207, 337, 264, 4470, 2445, 337, 3565, 3142, 24590, 13, 51449, 51449, 467, 4523, 484, 300, 365, 341, 3922, 295, 4470, 2445, 11, 264, 4787, 2063, 2445, 486, 312, 51721, 51721], "temperature": 0.0, "avg_logprob": -0.07094566027323405, "compression_ratio": 1.7213930348258706, "no_speech_prob": 4.56592533737421e-06}, {"id": 116, "seek": 57626, "start": 593.12, "end": 597.96, "text": " for the loss function for logistic regression.", "tokens": [50364, 886, 1998, 281, 4018, 13, 50495, 50495, 407, 294, 341, 960, 11, 291, 1866, 983, 264, 8889, 6713, 2063, 2445, 1177, 380, 589, 731, 337, 50695, 50695, 3565, 3142, 24590, 13, 50819, 50819, 492, 611, 7642, 264, 4470, 337, 257, 2167, 3097, 1365, 293, 1361, 493, 365, 257, 777, 7123, 51207, 51207, 337, 264, 4470, 2445, 337, 3565, 3142, 24590, 13, 51449, 51449, 467, 4523, 484, 300, 365, 341, 3922, 295, 4470, 2445, 11, 264, 4787, 2063, 2445, 486, 312, 51721, 51721], "temperature": 0.0, "avg_logprob": -0.07094566027323405, "compression_ratio": 1.7213930348258706, "no_speech_prob": 4.56592533737421e-06}, {"id": 117, "seek": 57626, "start": 597.96, "end": 603.4, "text": " It turns out that with this choice of loss function, the overall cost function will be", "tokens": [50364, 886, 1998, 281, 4018, 13, 50495, 50495, 407, 294, 341, 960, 11, 291, 1866, 983, 264, 8889, 6713, 2063, 2445, 1177, 380, 589, 731, 337, 50695, 50695, 3565, 3142, 24590, 13, 50819, 50819, 492, 611, 7642, 264, 4470, 337, 257, 2167, 3097, 1365, 293, 1361, 493, 365, 257, 777, 7123, 51207, 51207, 337, 264, 4470, 2445, 337, 3565, 3142, 24590, 13, 51449, 51449, 467, 4523, 484, 300, 365, 341, 3922, 295, 4470, 2445, 11, 264, 4787, 2063, 2445, 486, 312, 51721, 51721], "temperature": 0.0, "avg_logprob": -0.07094566027323405, "compression_ratio": 1.7213930348258706, "no_speech_prob": 4.56592533737421e-06}, {"id": 118, "seek": 60340, "start": 603.4, "end": 610.0799999999999, "text": " convex and thus you can reliably use gradient descent to take you to the global minimum.", "tokens": [50364, 42432, 293, 8807, 291, 393, 49927, 764, 16235, 23475, 281, 747, 291, 281, 264, 4338, 7285, 13, 50698, 50698, 1705, 798, 300, 341, 2445, 307, 42432, 307, 4399, 264, 11923, 295, 341, 1164, 13, 50960, 50960, 509, 815, 1604, 300, 264, 2063, 2445, 307, 257, 2445, 295, 264, 2302, 3097, 992, 293, 51260, 51260, 307, 4412, 264, 4274, 420, 472, 670, 275, 1413, 264, 2408, 295, 264, 4470, 2445, 322, 264, 2609, 51606, 51606, 3097, 5110, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.10551915345368562, "compression_ratio": 1.7163461538461537, "no_speech_prob": 2.190753548347857e-06}, {"id": 119, "seek": 60340, "start": 610.0799999999999, "end": 615.3199999999999, "text": " Proving that this function is convex is beyond the scope of this course.", "tokens": [50364, 42432, 293, 8807, 291, 393, 49927, 764, 16235, 23475, 281, 747, 291, 281, 264, 4338, 7285, 13, 50698, 50698, 1705, 798, 300, 341, 2445, 307, 42432, 307, 4399, 264, 11923, 295, 341, 1164, 13, 50960, 50960, 509, 815, 1604, 300, 264, 2063, 2445, 307, 257, 2445, 295, 264, 2302, 3097, 992, 293, 51260, 51260, 307, 4412, 264, 4274, 420, 472, 670, 275, 1413, 264, 2408, 295, 264, 4470, 2445, 322, 264, 2609, 51606, 51606, 3097, 5110, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.10551915345368562, "compression_ratio": 1.7163461538461537, "no_speech_prob": 2.190753548347857e-06}, {"id": 120, "seek": 60340, "start": 615.3199999999999, "end": 621.3199999999999, "text": " You may remember that the cost function is a function of the entire training set and", "tokens": [50364, 42432, 293, 8807, 291, 393, 49927, 764, 16235, 23475, 281, 747, 291, 281, 264, 4338, 7285, 13, 50698, 50698, 1705, 798, 300, 341, 2445, 307, 42432, 307, 4399, 264, 11923, 295, 341, 1164, 13, 50960, 50960, 509, 815, 1604, 300, 264, 2063, 2445, 307, 257, 2445, 295, 264, 2302, 3097, 992, 293, 51260, 51260, 307, 4412, 264, 4274, 420, 472, 670, 275, 1413, 264, 2408, 295, 264, 4470, 2445, 322, 264, 2609, 51606, 51606, 3097, 5110, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.10551915345368562, "compression_ratio": 1.7163461538461537, "no_speech_prob": 2.190753548347857e-06}, {"id": 121, "seek": 60340, "start": 621.3199999999999, "end": 628.24, "text": " is therefore the average or one over m times the sum of the loss function on the individual", "tokens": [50364, 42432, 293, 8807, 291, 393, 49927, 764, 16235, 23475, 281, 747, 291, 281, 264, 4338, 7285, 13, 50698, 50698, 1705, 798, 300, 341, 2445, 307, 42432, 307, 4399, 264, 11923, 295, 341, 1164, 13, 50960, 50960, 509, 815, 1604, 300, 264, 2063, 2445, 307, 257, 2445, 295, 264, 2302, 3097, 992, 293, 51260, 51260, 307, 4412, 264, 4274, 420, 472, 670, 275, 1413, 264, 2408, 295, 264, 4470, 2445, 322, 264, 2609, 51606, 51606, 3097, 5110, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.10551915345368562, "compression_ratio": 1.7163461538461537, "no_speech_prob": 2.190753548347857e-06}, {"id": 122, "seek": 60340, "start": 628.24, "end": 630.36, "text": " training examples.", "tokens": [50364, 42432, 293, 8807, 291, 393, 49927, 764, 16235, 23475, 281, 747, 291, 281, 264, 4338, 7285, 13, 50698, 50698, 1705, 798, 300, 341, 2445, 307, 42432, 307, 4399, 264, 11923, 295, 341, 1164, 13, 50960, 50960, 509, 815, 1604, 300, 264, 2063, 2445, 307, 257, 2445, 295, 264, 2302, 3097, 992, 293, 51260, 51260, 307, 4412, 264, 4274, 420, 472, 670, 275, 1413, 264, 2408, 295, 264, 4470, 2445, 322, 264, 2609, 51606, 51606, 3097, 5110, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.10551915345368562, "compression_ratio": 1.7163461538461537, "no_speech_prob": 2.190753548347857e-06}, {"id": 123, "seek": 63036, "start": 630.36, "end": 639.2, "text": " So the cost on a certain set of parameters w and b is equal to one over m times the sum", "tokens": [50364, 407, 264, 2063, 322, 257, 1629, 992, 295, 9834, 261, 293, 272, 307, 2681, 281, 472, 670, 275, 1413, 264, 2408, 50806, 50806, 670, 439, 264, 3097, 5110, 295, 264, 4470, 322, 264, 3097, 5110, 13, 51070, 51070, 400, 498, 291, 393, 915, 264, 2158, 295, 264, 9834, 261, 293, 272, 300, 4464, 5660, 341, 11, 550, 291, 1116, 362, 51408, 51408, 257, 1238, 665, 992, 295, 4190, 337, 264, 9834, 261, 293, 272, 337, 3565, 3142, 24590, 13, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.06856559845338385, "compression_ratio": 1.7978142076502732, "no_speech_prob": 3.905416633642744e-06}, {"id": 124, "seek": 63036, "start": 639.2, "end": 644.48, "text": " over all the training examples of the loss on the training examples.", "tokens": [50364, 407, 264, 2063, 322, 257, 1629, 992, 295, 9834, 261, 293, 272, 307, 2681, 281, 472, 670, 275, 1413, 264, 2408, 50806, 50806, 670, 439, 264, 3097, 5110, 295, 264, 4470, 322, 264, 3097, 5110, 13, 51070, 51070, 400, 498, 291, 393, 915, 264, 2158, 295, 264, 9834, 261, 293, 272, 300, 4464, 5660, 341, 11, 550, 291, 1116, 362, 51408, 51408, 257, 1238, 665, 992, 295, 4190, 337, 264, 9834, 261, 293, 272, 337, 3565, 3142, 24590, 13, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.06856559845338385, "compression_ratio": 1.7978142076502732, "no_speech_prob": 3.905416633642744e-06}, {"id": 125, "seek": 63036, "start": 644.48, "end": 651.24, "text": " And if you can find the value of the parameters w and b that minimizes this, then you'd have", "tokens": [50364, 407, 264, 2063, 322, 257, 1629, 992, 295, 9834, 261, 293, 272, 307, 2681, 281, 472, 670, 275, 1413, 264, 2408, 50806, 50806, 670, 439, 264, 3097, 5110, 295, 264, 4470, 322, 264, 3097, 5110, 13, 51070, 51070, 400, 498, 291, 393, 915, 264, 2158, 295, 264, 9834, 261, 293, 272, 300, 4464, 5660, 341, 11, 550, 291, 1116, 362, 51408, 51408, 257, 1238, 665, 992, 295, 4190, 337, 264, 9834, 261, 293, 272, 337, 3565, 3142, 24590, 13, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.06856559845338385, "compression_ratio": 1.7978142076502732, "no_speech_prob": 3.905416633642744e-06}, {"id": 126, "seek": 63036, "start": 651.24, "end": 657.08, "text": " a pretty good set of values for the parameters w and b for logistic regression.", "tokens": [50364, 407, 264, 2063, 322, 257, 1629, 992, 295, 9834, 261, 293, 272, 307, 2681, 281, 472, 670, 275, 1413, 264, 2408, 50806, 50806, 670, 439, 264, 3097, 5110, 295, 264, 4470, 322, 264, 3097, 5110, 13, 51070, 51070, 400, 498, 291, 393, 915, 264, 2158, 295, 264, 9834, 261, 293, 272, 300, 4464, 5660, 341, 11, 550, 291, 1116, 362, 51408, 51408, 257, 1238, 665, 992, 295, 4190, 337, 264, 9834, 261, 293, 272, 337, 3565, 3142, 24590, 13, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.06856559845338385, "compression_ratio": 1.7978142076502732, "no_speech_prob": 3.905416633642744e-06}, {"id": 127, "seek": 65708, "start": 657.08, "end": 662.5, "text": " In the upcoming optional lab, you get to take a look at how the squared error cost function", "tokens": [50364, 682, 264, 11500, 17312, 2715, 11, 291, 483, 281, 747, 257, 574, 412, 577, 264, 8889, 6713, 2063, 2445, 50635, 50635, 1177, 380, 589, 588, 731, 337, 21538, 570, 291, 536, 300, 264, 3753, 7542, 3542, 50899, 50899, 294, 257, 588, 261, 46737, 2063, 3753, 365, 867, 2654, 4464, 64, 13, 51152, 51152, 1396, 291, 747, 257, 574, 412, 264, 777, 3565, 3142, 4470, 2445, 293, 382, 291, 393, 536, 510, 11, 341, 14725, 51528, 51528, 257, 1481, 293, 5508, 42432, 3753, 7542, 300, 775, 406, 362, 439, 729, 2654, 4464, 64, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09088321567810688, "compression_ratio": 1.728813559322034, "no_speech_prob": 2.190738314311602e-06}, {"id": 128, "seek": 65708, "start": 662.5, "end": 667.7800000000001, "text": " doesn't work very well for classification because you see that the surface plot results", "tokens": [50364, 682, 264, 11500, 17312, 2715, 11, 291, 483, 281, 747, 257, 574, 412, 577, 264, 8889, 6713, 2063, 2445, 50635, 50635, 1177, 380, 589, 588, 731, 337, 21538, 570, 291, 536, 300, 264, 3753, 7542, 3542, 50899, 50899, 294, 257, 588, 261, 46737, 2063, 3753, 365, 867, 2654, 4464, 64, 13, 51152, 51152, 1396, 291, 747, 257, 574, 412, 264, 777, 3565, 3142, 4470, 2445, 293, 382, 291, 393, 536, 510, 11, 341, 14725, 51528, 51528, 257, 1481, 293, 5508, 42432, 3753, 7542, 300, 775, 406, 362, 439, 729, 2654, 4464, 64, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09088321567810688, "compression_ratio": 1.728813559322034, "no_speech_prob": 2.190738314311602e-06}, {"id": 129, "seek": 65708, "start": 667.7800000000001, "end": 672.84, "text": " in a very wiggly cost surface with many local minima.", "tokens": [50364, 682, 264, 11500, 17312, 2715, 11, 291, 483, 281, 747, 257, 574, 412, 577, 264, 8889, 6713, 2063, 2445, 50635, 50635, 1177, 380, 589, 588, 731, 337, 21538, 570, 291, 536, 300, 264, 3753, 7542, 3542, 50899, 50899, 294, 257, 588, 261, 46737, 2063, 3753, 365, 867, 2654, 4464, 64, 13, 51152, 51152, 1396, 291, 747, 257, 574, 412, 264, 777, 3565, 3142, 4470, 2445, 293, 382, 291, 393, 536, 510, 11, 341, 14725, 51528, 51528, 257, 1481, 293, 5508, 42432, 3753, 7542, 300, 775, 406, 362, 439, 729, 2654, 4464, 64, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09088321567810688, "compression_ratio": 1.728813559322034, "no_speech_prob": 2.190738314311602e-06}, {"id": 130, "seek": 65708, "start": 672.84, "end": 680.36, "text": " Then you take a look at the new logistic loss function and as you can see here, this produces", "tokens": [50364, 682, 264, 11500, 17312, 2715, 11, 291, 483, 281, 747, 257, 574, 412, 577, 264, 8889, 6713, 2063, 2445, 50635, 50635, 1177, 380, 589, 588, 731, 337, 21538, 570, 291, 536, 300, 264, 3753, 7542, 3542, 50899, 50899, 294, 257, 588, 261, 46737, 2063, 3753, 365, 867, 2654, 4464, 64, 13, 51152, 51152, 1396, 291, 747, 257, 574, 412, 264, 777, 3565, 3142, 4470, 2445, 293, 382, 291, 393, 536, 510, 11, 341, 14725, 51528, 51528, 257, 1481, 293, 5508, 42432, 3753, 7542, 300, 775, 406, 362, 439, 729, 2654, 4464, 64, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09088321567810688, "compression_ratio": 1.728813559322034, "no_speech_prob": 2.190738314311602e-06}, {"id": 131, "seek": 65708, "start": 680.36, "end": 687.0, "text": " a nice and smooth convex surface plot that does not have all those local minima.", "tokens": [50364, 682, 264, 11500, 17312, 2715, 11, 291, 483, 281, 747, 257, 574, 412, 577, 264, 8889, 6713, 2063, 2445, 50635, 50635, 1177, 380, 589, 588, 731, 337, 21538, 570, 291, 536, 300, 264, 3753, 7542, 3542, 50899, 50899, 294, 257, 588, 261, 46737, 2063, 3753, 365, 867, 2654, 4464, 64, 13, 51152, 51152, 1396, 291, 747, 257, 574, 412, 264, 777, 3565, 3142, 4470, 2445, 293, 382, 291, 393, 536, 510, 11, 341, 14725, 51528, 51528, 257, 1481, 293, 5508, 42432, 3753, 7542, 300, 775, 406, 362, 439, 729, 2654, 4464, 64, 13, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09088321567810688, "compression_ratio": 1.728813559322034, "no_speech_prob": 2.190738314311602e-06}, {"id": 132, "seek": 68700, "start": 687.0, "end": 692.2, "text": " So please take a look at the code and the plots after this video.", "tokens": [50364, 407, 1767, 747, 257, 574, 412, 264, 3089, 293, 264, 28609, 934, 341, 960, 13, 50624, 50624, 2798, 370, 321, 600, 1612, 257, 688, 294, 341, 960, 13, 50796, 50796, 682, 264, 958, 960, 11, 718, 311, 352, 646, 293, 747, 264, 4470, 2445, 337, 257, 2167, 3097, 1365, 51048, 51048, 293, 764, 300, 281, 6964, 264, 4787, 2063, 2445, 337, 264, 2302, 3097, 992, 13, 51314, 51314, 400, 321, 603, 611, 2573, 484, 257, 18587, 636, 281, 2464, 484, 264, 2063, 2445, 11, 597, 486, 550, 51558, 51558, 1780, 2089, 505, 281, 1190, 16235, 23475, 281, 915, 665, 9834, 337, 3565, 3142, 24590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.10973875238261091, "compression_ratio": 1.751937984496124, "no_speech_prob": 8.013293154363055e-06}, {"id": 133, "seek": 68700, "start": 692.2, "end": 695.64, "text": " Alright so we've seen a lot in this video.", "tokens": [50364, 407, 1767, 747, 257, 574, 412, 264, 3089, 293, 264, 28609, 934, 341, 960, 13, 50624, 50624, 2798, 370, 321, 600, 1612, 257, 688, 294, 341, 960, 13, 50796, 50796, 682, 264, 958, 960, 11, 718, 311, 352, 646, 293, 747, 264, 4470, 2445, 337, 257, 2167, 3097, 1365, 51048, 51048, 293, 764, 300, 281, 6964, 264, 4787, 2063, 2445, 337, 264, 2302, 3097, 992, 13, 51314, 51314, 400, 321, 603, 611, 2573, 484, 257, 18587, 636, 281, 2464, 484, 264, 2063, 2445, 11, 597, 486, 550, 51558, 51558, 1780, 2089, 505, 281, 1190, 16235, 23475, 281, 915, 665, 9834, 337, 3565, 3142, 24590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.10973875238261091, "compression_ratio": 1.751937984496124, "no_speech_prob": 8.013293154363055e-06}, {"id": 134, "seek": 68700, "start": 695.64, "end": 700.68, "text": " In the next video, let's go back and take the loss function for a single training example", "tokens": [50364, 407, 1767, 747, 257, 574, 412, 264, 3089, 293, 264, 28609, 934, 341, 960, 13, 50624, 50624, 2798, 370, 321, 600, 1612, 257, 688, 294, 341, 960, 13, 50796, 50796, 682, 264, 958, 960, 11, 718, 311, 352, 646, 293, 747, 264, 4470, 2445, 337, 257, 2167, 3097, 1365, 51048, 51048, 293, 764, 300, 281, 6964, 264, 4787, 2063, 2445, 337, 264, 2302, 3097, 992, 13, 51314, 51314, 400, 321, 603, 611, 2573, 484, 257, 18587, 636, 281, 2464, 484, 264, 2063, 2445, 11, 597, 486, 550, 51558, 51558, 1780, 2089, 505, 281, 1190, 16235, 23475, 281, 915, 665, 9834, 337, 3565, 3142, 24590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.10973875238261091, "compression_ratio": 1.751937984496124, "no_speech_prob": 8.013293154363055e-06}, {"id": 135, "seek": 68700, "start": 700.68, "end": 706.0, "text": " and use that to define the overall cost function for the entire training set.", "tokens": [50364, 407, 1767, 747, 257, 574, 412, 264, 3089, 293, 264, 28609, 934, 341, 960, 13, 50624, 50624, 2798, 370, 321, 600, 1612, 257, 688, 294, 341, 960, 13, 50796, 50796, 682, 264, 958, 960, 11, 718, 311, 352, 646, 293, 747, 264, 4470, 2445, 337, 257, 2167, 3097, 1365, 51048, 51048, 293, 764, 300, 281, 6964, 264, 4787, 2063, 2445, 337, 264, 2302, 3097, 992, 13, 51314, 51314, 400, 321, 603, 611, 2573, 484, 257, 18587, 636, 281, 2464, 484, 264, 2063, 2445, 11, 597, 486, 550, 51558, 51558, 1780, 2089, 505, 281, 1190, 16235, 23475, 281, 915, 665, 9834, 337, 3565, 3142, 24590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.10973875238261091, "compression_ratio": 1.751937984496124, "no_speech_prob": 8.013293154363055e-06}, {"id": 136, "seek": 68700, "start": 706.0, "end": 710.88, "text": " And we'll also figure out a simpler way to write out the cost function, which will then", "tokens": [50364, 407, 1767, 747, 257, 574, 412, 264, 3089, 293, 264, 28609, 934, 341, 960, 13, 50624, 50624, 2798, 370, 321, 600, 1612, 257, 688, 294, 341, 960, 13, 50796, 50796, 682, 264, 958, 960, 11, 718, 311, 352, 646, 293, 747, 264, 4470, 2445, 337, 257, 2167, 3097, 1365, 51048, 51048, 293, 764, 300, 281, 6964, 264, 4787, 2063, 2445, 337, 264, 2302, 3097, 992, 13, 51314, 51314, 400, 321, 603, 611, 2573, 484, 257, 18587, 636, 281, 2464, 484, 264, 2063, 2445, 11, 597, 486, 550, 51558, 51558, 1780, 2089, 505, 281, 1190, 16235, 23475, 281, 915, 665, 9834, 337, 3565, 3142, 24590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.10973875238261091, "compression_ratio": 1.751937984496124, "no_speech_prob": 8.013293154363055e-06}, {"id": 137, "seek": 68700, "start": 710.88, "end": 716.64, "text": " later allow us to run gradient descent to find good parameters for logistic regression.", "tokens": [50364, 407, 1767, 747, 257, 574, 412, 264, 3089, 293, 264, 28609, 934, 341, 960, 13, 50624, 50624, 2798, 370, 321, 600, 1612, 257, 688, 294, 341, 960, 13, 50796, 50796, 682, 264, 958, 960, 11, 718, 311, 352, 646, 293, 747, 264, 4470, 2445, 337, 257, 2167, 3097, 1365, 51048, 51048, 293, 764, 300, 281, 6964, 264, 4787, 2063, 2445, 337, 264, 2302, 3097, 992, 13, 51314, 51314, 400, 321, 603, 611, 2573, 484, 257, 18587, 636, 281, 2464, 484, 264, 2063, 2445, 11, 597, 486, 550, 51558, 51558, 1780, 2089, 505, 281, 1190, 16235, 23475, 281, 915, 665, 9834, 337, 3565, 3142, 24590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.10973875238261091, "compression_ratio": 1.751937984496124, "no_speech_prob": 8.013293154363055e-06}, {"id": 138, "seek": 71664, "start": 716.64, "end": 718.24, "text": " Let's go on to the next video.", "tokens": [50364, 961, 311, 352, 322, 281, 264, 958, 960, 13, 50444], "temperature": 0.0, "avg_logprob": -0.1425479253133138, "compression_ratio": 0.7894736842105263, "no_speech_prob": 0.00044052035082131624}], "language": "en", "video_id": "E-ZcnndnY2I", "entity": "ML Specialization, Andrew Ng (2022)"}}