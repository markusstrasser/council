{"video_id": "odAhNw-e4o0", "title": "2.4 Linear Regression with Multiple Variables|Gradient descent for multiple linear regression ---ML", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 466, "views": 370, "publish_date": "11/04/2022", "timestamp": 1661040000, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " So, you've learned about gradient descent, about multiple linear regression, and also vectorization. Let's put it all together to implement gradient descent for multiple linear regression with vectorization. This would be cool. Let's quickly review what multiple linear regression looks like. Using our previous notation, let's see how you can write it more succinctly using vector notation. We have parameters w1 to wn, as well as b, but instead of thinking of w1 to wn as separate numbers, that is, separate parameters, let's instead collect all of the w's into a vector w, so that now w is a vector of length n. So, we're just going to think of the parameters of this model as a vector w, as well as b, where b is still a number, same as before. For us before, we had defined multiple linear regression like this. Now using vector notation, we can write the model as f sub wb of x equals the vector w dot product with the vector x plus b. And remember that this dot here means dot product. Our cost function can be defined as j of w1 through wn comma b, but instead of just thinking of j as a function of these n different parameters wj as well as b, we're going to write j as a function of parameter vector w and the number b. So this w1 through wn is replaced by this vector w, and j now takes this input, a vector w and a number b, and returns a number. Here's what gradient descent looks like. We're going to repeatedly update each parameter wj to be wj minus alpha times the derivative of the cost j, where j has parameters w1 through wn and b, and once again we just write this as j of vector w and number b. Let's see what this will look like when you implement gradient descent, and in particular, let's take a look at the derivative term. You'll see that gradient descent becomes just a little bit different with multiple features compared to just one feature. Here's what we had when we had gradient descent with one feature. We had an update rule for w and a separate update rule for b. So hopefully these look familiar to you. And this term here is the derivative of the cost function j with respect to the parameter w. And similarly, we have an update rule for parameter b with univariate regression, we had only one feature. We call that feature xi without any subscript. Now here's the new notation for when we have n features where n is two or more. We get this update rule for gradient descent. We have a w1 to be w1 minus alpha times this expression here. And this formula is actually the derivative of the cost j with respect to w1. The formula for the derivative of j with respect to w1 on the right looks very similar to the case of one feature on the left. The error term still takes a prediction f of x minus the target y. One difference is that w and x are now vectors. And just as w on the left has now become w1 here on the right, xi here on the left is now instead xi subscript 1 here on the right. And this is just for j equals 1. For multiple linear regression, we have j ranging from 1 through n, and so we'll update the parameters w1, w2 all the way up to wn. And then as before, we'll update b. And if you implement this, you get gradient descent for multiple regression. So that's it for gradient descent for multiple regression. Before moving on from this video, I want to make a quick aside or a quick side note on an alternative way for finding w and b for linear regression. And this method is called the normal equation. Whereas it turns out gradient descent is a great method for minimizing the cost function j to find w and b. There is one other algorithm that works only for linear regression and pretty much none of the other algorithms you see in this specialization for solving for w and b. And this other method does not need an iterative gradient descent algorithm. Called the normal equation method, it turns out to be possible to use an advanced linear algebra library to just solve for w and b all in one go without iterations. Some disadvantages of the normal equation method are, first, unlike gradient descent, this does not generalize to other learning algorithms, such as the logistic regression algorithm that you learn about next week, or the neural networks or other algorithms you see later in this specialization. The normal equation method is also quite slow if the number of features n is large. Almost no machine learning practitioners should implement the normal equation method themselves. But if you're using a mature machine learning library and call linear regression, there's a chance that on the backend, it will be using this to solve for w and b. So if you're ever in a job interview and hear the term normal equation, that's what this refers to. Don't worry about the details of how the normal equation works. Just be aware that some machine learning libraries may use this complicated method in the backend to solve for w and b. But for most learning algorithms, including how you implement linear regression yourself, gradient descent is often a better way to get the job done. In the optional lab that follows this video, you see how to define a multiple regression model in code, and also how to calculate the prediction f of x. You also see how to calculate the cost and implement gradient descent for multiple linear regression model. This will be using Python's NumPy library, so if any of the code looks very new, that's okay. But you should feel free also to take a look at the previous optional lab that introduces NumPy and vectorization for a refresher of NumPy functions and how to implement those in code. So that's it. You now know multiple linear regression. This is probably the single most widely used learning algorithm in the world today. But there's more. With just a few tricks such as picking and scaling features appropriately, and also choosing the learning rate alpha appropriately, you will be able to make this work much better. So just a few more videos to go for this week. Let's go on to the next video to see those little tricks that will help you make multiple linear regression work much better.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.2, "text": " So, you've learned about gradient descent, about multiple linear regression, and also", "tokens": [50364, 407, 11, 291, 600, 3264, 466, 16235, 23475, 11, 466, 3866, 8213, 24590, 11, 293, 611, 50724, 50724, 8062, 2144, 13, 50774, 50774, 961, 311, 829, 309, 439, 1214, 281, 4445, 16235, 23475, 337, 3866, 8213, 24590, 365, 51022, 51022, 8062, 2144, 13, 51086, 51086, 639, 576, 312, 1627, 13, 51136, 51136, 961, 311, 2661, 3131, 437, 3866, 8213, 24590, 1542, 411, 13, 51340, 51340, 11142, 527, 3894, 24657, 11, 718, 311, 536, 577, 291, 393, 2464, 309, 544, 21578, 5460, 356, 1228, 8062, 51566, 51566, 24657, 13, 51623, 51623], "temperature": 0.0, "avg_logprob": -0.11949920654296875, "compression_ratio": 1.8271028037383177, "no_speech_prob": 0.010647728107869625}, {"id": 1, "seek": 0, "start": 7.2, "end": 8.2, "text": " vectorization.", "tokens": [50364, 407, 11, 291, 600, 3264, 466, 16235, 23475, 11, 466, 3866, 8213, 24590, 11, 293, 611, 50724, 50724, 8062, 2144, 13, 50774, 50774, 961, 311, 829, 309, 439, 1214, 281, 4445, 16235, 23475, 337, 3866, 8213, 24590, 365, 51022, 51022, 8062, 2144, 13, 51086, 51086, 639, 576, 312, 1627, 13, 51136, 51136, 961, 311, 2661, 3131, 437, 3866, 8213, 24590, 1542, 411, 13, 51340, 51340, 11142, 527, 3894, 24657, 11, 718, 311, 536, 577, 291, 393, 2464, 309, 544, 21578, 5460, 356, 1228, 8062, 51566, 51566, 24657, 13, 51623, 51623], "temperature": 0.0, "avg_logprob": -0.11949920654296875, "compression_ratio": 1.8271028037383177, "no_speech_prob": 0.010647728107869625}, {"id": 2, "seek": 0, "start": 8.2, "end": 13.16, "text": " Let's put it all together to implement gradient descent for multiple linear regression with", "tokens": [50364, 407, 11, 291, 600, 3264, 466, 16235, 23475, 11, 466, 3866, 8213, 24590, 11, 293, 611, 50724, 50724, 8062, 2144, 13, 50774, 50774, 961, 311, 829, 309, 439, 1214, 281, 4445, 16235, 23475, 337, 3866, 8213, 24590, 365, 51022, 51022, 8062, 2144, 13, 51086, 51086, 639, 576, 312, 1627, 13, 51136, 51136, 961, 311, 2661, 3131, 437, 3866, 8213, 24590, 1542, 411, 13, 51340, 51340, 11142, 527, 3894, 24657, 11, 718, 311, 536, 577, 291, 393, 2464, 309, 544, 21578, 5460, 356, 1228, 8062, 51566, 51566, 24657, 13, 51623, 51623], "temperature": 0.0, "avg_logprob": -0.11949920654296875, "compression_ratio": 1.8271028037383177, "no_speech_prob": 0.010647728107869625}, {"id": 3, "seek": 0, "start": 13.16, "end": 14.44, "text": " vectorization.", "tokens": [50364, 407, 11, 291, 600, 3264, 466, 16235, 23475, 11, 466, 3866, 8213, 24590, 11, 293, 611, 50724, 50724, 8062, 2144, 13, 50774, 50774, 961, 311, 829, 309, 439, 1214, 281, 4445, 16235, 23475, 337, 3866, 8213, 24590, 365, 51022, 51022, 8062, 2144, 13, 51086, 51086, 639, 576, 312, 1627, 13, 51136, 51136, 961, 311, 2661, 3131, 437, 3866, 8213, 24590, 1542, 411, 13, 51340, 51340, 11142, 527, 3894, 24657, 11, 718, 311, 536, 577, 291, 393, 2464, 309, 544, 21578, 5460, 356, 1228, 8062, 51566, 51566, 24657, 13, 51623, 51623], "temperature": 0.0, "avg_logprob": -0.11949920654296875, "compression_ratio": 1.8271028037383177, "no_speech_prob": 0.010647728107869625}, {"id": 4, "seek": 0, "start": 14.44, "end": 15.44, "text": " This would be cool.", "tokens": [50364, 407, 11, 291, 600, 3264, 466, 16235, 23475, 11, 466, 3866, 8213, 24590, 11, 293, 611, 50724, 50724, 8062, 2144, 13, 50774, 50774, 961, 311, 829, 309, 439, 1214, 281, 4445, 16235, 23475, 337, 3866, 8213, 24590, 365, 51022, 51022, 8062, 2144, 13, 51086, 51086, 639, 576, 312, 1627, 13, 51136, 51136, 961, 311, 2661, 3131, 437, 3866, 8213, 24590, 1542, 411, 13, 51340, 51340, 11142, 527, 3894, 24657, 11, 718, 311, 536, 577, 291, 393, 2464, 309, 544, 21578, 5460, 356, 1228, 8062, 51566, 51566, 24657, 13, 51623, 51623], "temperature": 0.0, "avg_logprob": -0.11949920654296875, "compression_ratio": 1.8271028037383177, "no_speech_prob": 0.010647728107869625}, {"id": 5, "seek": 0, "start": 15.44, "end": 19.52, "text": " Let's quickly review what multiple linear regression looks like.", "tokens": [50364, 407, 11, 291, 600, 3264, 466, 16235, 23475, 11, 466, 3866, 8213, 24590, 11, 293, 611, 50724, 50724, 8062, 2144, 13, 50774, 50774, 961, 311, 829, 309, 439, 1214, 281, 4445, 16235, 23475, 337, 3866, 8213, 24590, 365, 51022, 51022, 8062, 2144, 13, 51086, 51086, 639, 576, 312, 1627, 13, 51136, 51136, 961, 311, 2661, 3131, 437, 3866, 8213, 24590, 1542, 411, 13, 51340, 51340, 11142, 527, 3894, 24657, 11, 718, 311, 536, 577, 291, 393, 2464, 309, 544, 21578, 5460, 356, 1228, 8062, 51566, 51566, 24657, 13, 51623, 51623], "temperature": 0.0, "avg_logprob": -0.11949920654296875, "compression_ratio": 1.8271028037383177, "no_speech_prob": 0.010647728107869625}, {"id": 6, "seek": 0, "start": 19.52, "end": 24.04, "text": " Using our previous notation, let's see how you can write it more succinctly using vector", "tokens": [50364, 407, 11, 291, 600, 3264, 466, 16235, 23475, 11, 466, 3866, 8213, 24590, 11, 293, 611, 50724, 50724, 8062, 2144, 13, 50774, 50774, 961, 311, 829, 309, 439, 1214, 281, 4445, 16235, 23475, 337, 3866, 8213, 24590, 365, 51022, 51022, 8062, 2144, 13, 51086, 51086, 639, 576, 312, 1627, 13, 51136, 51136, 961, 311, 2661, 3131, 437, 3866, 8213, 24590, 1542, 411, 13, 51340, 51340, 11142, 527, 3894, 24657, 11, 718, 311, 536, 577, 291, 393, 2464, 309, 544, 21578, 5460, 356, 1228, 8062, 51566, 51566, 24657, 13, 51623, 51623], "temperature": 0.0, "avg_logprob": -0.11949920654296875, "compression_ratio": 1.8271028037383177, "no_speech_prob": 0.010647728107869625}, {"id": 7, "seek": 0, "start": 24.04, "end": 25.18, "text": " notation.", "tokens": [50364, 407, 11, 291, 600, 3264, 466, 16235, 23475, 11, 466, 3866, 8213, 24590, 11, 293, 611, 50724, 50724, 8062, 2144, 13, 50774, 50774, 961, 311, 829, 309, 439, 1214, 281, 4445, 16235, 23475, 337, 3866, 8213, 24590, 365, 51022, 51022, 8062, 2144, 13, 51086, 51086, 639, 576, 312, 1627, 13, 51136, 51136, 961, 311, 2661, 3131, 437, 3866, 8213, 24590, 1542, 411, 13, 51340, 51340, 11142, 527, 3894, 24657, 11, 718, 311, 536, 577, 291, 393, 2464, 309, 544, 21578, 5460, 356, 1228, 8062, 51566, 51566, 24657, 13, 51623, 51623], "temperature": 0.0, "avg_logprob": -0.11949920654296875, "compression_ratio": 1.8271028037383177, "no_speech_prob": 0.010647728107869625}, {"id": 8, "seek": 2518, "start": 25.18, "end": 33.24, "text": " We have parameters w1 to wn, as well as b, but instead of thinking of w1 to wn as separate", "tokens": [50364, 492, 362, 9834, 261, 16, 281, 45368, 11, 382, 731, 382, 272, 11, 457, 2602, 295, 1953, 295, 261, 16, 281, 45368, 382, 4994, 50767, 50767, 3547, 11, 300, 307, 11, 4994, 9834, 11, 718, 311, 2602, 2500, 439, 295, 264, 261, 311, 666, 257, 8062, 51057, 51057, 261, 11, 370, 300, 586, 261, 307, 257, 8062, 295, 4641, 297, 13, 51293, 51293, 407, 11, 321, 434, 445, 516, 281, 519, 295, 264, 9834, 295, 341, 2316, 382, 257, 8062, 261, 11, 382, 731, 382, 272, 11, 51667, 51667, 689, 272, 307, 920, 257, 1230, 11, 912, 382, 949, 13, 51833, 51833], "temperature": 0.0, "avg_logprob": -0.14626468144930327, "compression_ratio": 1.8445595854922279, "no_speech_prob": 1.6963525922619738e-05}, {"id": 9, "seek": 2518, "start": 33.24, "end": 39.04, "text": " numbers, that is, separate parameters, let's instead collect all of the w's into a vector", "tokens": [50364, 492, 362, 9834, 261, 16, 281, 45368, 11, 382, 731, 382, 272, 11, 457, 2602, 295, 1953, 295, 261, 16, 281, 45368, 382, 4994, 50767, 50767, 3547, 11, 300, 307, 11, 4994, 9834, 11, 718, 311, 2602, 2500, 439, 295, 264, 261, 311, 666, 257, 8062, 51057, 51057, 261, 11, 370, 300, 586, 261, 307, 257, 8062, 295, 4641, 297, 13, 51293, 51293, 407, 11, 321, 434, 445, 516, 281, 519, 295, 264, 9834, 295, 341, 2316, 382, 257, 8062, 261, 11, 382, 731, 382, 272, 11, 51667, 51667, 689, 272, 307, 920, 257, 1230, 11, 912, 382, 949, 13, 51833, 51833], "temperature": 0.0, "avg_logprob": -0.14626468144930327, "compression_ratio": 1.8445595854922279, "no_speech_prob": 1.6963525922619738e-05}, {"id": 10, "seek": 2518, "start": 39.04, "end": 43.760000000000005, "text": " w, so that now w is a vector of length n.", "tokens": [50364, 492, 362, 9834, 261, 16, 281, 45368, 11, 382, 731, 382, 272, 11, 457, 2602, 295, 1953, 295, 261, 16, 281, 45368, 382, 4994, 50767, 50767, 3547, 11, 300, 307, 11, 4994, 9834, 11, 718, 311, 2602, 2500, 439, 295, 264, 261, 311, 666, 257, 8062, 51057, 51057, 261, 11, 370, 300, 586, 261, 307, 257, 8062, 295, 4641, 297, 13, 51293, 51293, 407, 11, 321, 434, 445, 516, 281, 519, 295, 264, 9834, 295, 341, 2316, 382, 257, 8062, 261, 11, 382, 731, 382, 272, 11, 51667, 51667, 689, 272, 307, 920, 257, 1230, 11, 912, 382, 949, 13, 51833, 51833], "temperature": 0.0, "avg_logprob": -0.14626468144930327, "compression_ratio": 1.8445595854922279, "no_speech_prob": 1.6963525922619738e-05}, {"id": 11, "seek": 2518, "start": 43.760000000000005, "end": 51.24, "text": " So, we're just going to think of the parameters of this model as a vector w, as well as b,", "tokens": [50364, 492, 362, 9834, 261, 16, 281, 45368, 11, 382, 731, 382, 272, 11, 457, 2602, 295, 1953, 295, 261, 16, 281, 45368, 382, 4994, 50767, 50767, 3547, 11, 300, 307, 11, 4994, 9834, 11, 718, 311, 2602, 2500, 439, 295, 264, 261, 311, 666, 257, 8062, 51057, 51057, 261, 11, 370, 300, 586, 261, 307, 257, 8062, 295, 4641, 297, 13, 51293, 51293, 407, 11, 321, 434, 445, 516, 281, 519, 295, 264, 9834, 295, 341, 2316, 382, 257, 8062, 261, 11, 382, 731, 382, 272, 11, 51667, 51667, 689, 272, 307, 920, 257, 1230, 11, 912, 382, 949, 13, 51833, 51833], "temperature": 0.0, "avg_logprob": -0.14626468144930327, "compression_ratio": 1.8445595854922279, "no_speech_prob": 1.6963525922619738e-05}, {"id": 12, "seek": 2518, "start": 51.24, "end": 54.56, "text": " where b is still a number, same as before.", "tokens": [50364, 492, 362, 9834, 261, 16, 281, 45368, 11, 382, 731, 382, 272, 11, 457, 2602, 295, 1953, 295, 261, 16, 281, 45368, 382, 4994, 50767, 50767, 3547, 11, 300, 307, 11, 4994, 9834, 11, 718, 311, 2602, 2500, 439, 295, 264, 261, 311, 666, 257, 8062, 51057, 51057, 261, 11, 370, 300, 586, 261, 307, 257, 8062, 295, 4641, 297, 13, 51293, 51293, 407, 11, 321, 434, 445, 516, 281, 519, 295, 264, 9834, 295, 341, 2316, 382, 257, 8062, 261, 11, 382, 731, 382, 272, 11, 51667, 51667, 689, 272, 307, 920, 257, 1230, 11, 912, 382, 949, 13, 51833, 51833], "temperature": 0.0, "avg_logprob": -0.14626468144930327, "compression_ratio": 1.8445595854922279, "no_speech_prob": 1.6963525922619738e-05}, {"id": 13, "seek": 5456, "start": 54.56, "end": 59.080000000000005, "text": " For us before, we had defined multiple linear regression like this.", "tokens": [50364, 1171, 505, 949, 11, 321, 632, 7642, 3866, 8213, 24590, 411, 341, 13, 50590, 50590, 823, 1228, 8062, 24657, 11, 321, 393, 2464, 264, 2316, 382, 283, 1422, 261, 65, 295, 2031, 6915, 264, 8062, 261, 51010, 51010, 5893, 1674, 365, 264, 8062, 2031, 1804, 272, 13, 51196, 51196, 400, 1604, 300, 341, 5893, 510, 1355, 5893, 1674, 13, 51406, 51406, 2621, 2063, 2445, 393, 312, 7642, 382, 361, 295, 261, 16, 807, 45368, 22117, 272, 11, 457, 2602, 295, 445, 1953, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.15340669765028841, "compression_ratio": 1.5849056603773586, "no_speech_prob": 1.6027959190978436e-06}, {"id": 14, "seek": 5456, "start": 59.080000000000005, "end": 67.48, "text": " Now using vector notation, we can write the model as f sub wb of x equals the vector w", "tokens": [50364, 1171, 505, 949, 11, 321, 632, 7642, 3866, 8213, 24590, 411, 341, 13, 50590, 50590, 823, 1228, 8062, 24657, 11, 321, 393, 2464, 264, 2316, 382, 283, 1422, 261, 65, 295, 2031, 6915, 264, 8062, 261, 51010, 51010, 5893, 1674, 365, 264, 8062, 2031, 1804, 272, 13, 51196, 51196, 400, 1604, 300, 341, 5893, 510, 1355, 5893, 1674, 13, 51406, 51406, 2621, 2063, 2445, 393, 312, 7642, 382, 361, 295, 261, 16, 807, 45368, 22117, 272, 11, 457, 2602, 295, 445, 1953, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.15340669765028841, "compression_ratio": 1.5849056603773586, "no_speech_prob": 1.6027959190978436e-06}, {"id": 15, "seek": 5456, "start": 67.48, "end": 71.2, "text": " dot product with the vector x plus b.", "tokens": [50364, 1171, 505, 949, 11, 321, 632, 7642, 3866, 8213, 24590, 411, 341, 13, 50590, 50590, 823, 1228, 8062, 24657, 11, 321, 393, 2464, 264, 2316, 382, 283, 1422, 261, 65, 295, 2031, 6915, 264, 8062, 261, 51010, 51010, 5893, 1674, 365, 264, 8062, 2031, 1804, 272, 13, 51196, 51196, 400, 1604, 300, 341, 5893, 510, 1355, 5893, 1674, 13, 51406, 51406, 2621, 2063, 2445, 393, 312, 7642, 382, 361, 295, 261, 16, 807, 45368, 22117, 272, 11, 457, 2602, 295, 445, 1953, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.15340669765028841, "compression_ratio": 1.5849056603773586, "no_speech_prob": 1.6027959190978436e-06}, {"id": 16, "seek": 5456, "start": 71.2, "end": 75.4, "text": " And remember that this dot here means dot product.", "tokens": [50364, 1171, 505, 949, 11, 321, 632, 7642, 3866, 8213, 24590, 411, 341, 13, 50590, 50590, 823, 1228, 8062, 24657, 11, 321, 393, 2464, 264, 2316, 382, 283, 1422, 261, 65, 295, 2031, 6915, 264, 8062, 261, 51010, 51010, 5893, 1674, 365, 264, 8062, 2031, 1804, 272, 13, 51196, 51196, 400, 1604, 300, 341, 5893, 510, 1355, 5893, 1674, 13, 51406, 51406, 2621, 2063, 2445, 393, 312, 7642, 382, 361, 295, 261, 16, 807, 45368, 22117, 272, 11, 457, 2602, 295, 445, 1953, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.15340669765028841, "compression_ratio": 1.5849056603773586, "no_speech_prob": 1.6027959190978436e-06}, {"id": 17, "seek": 5456, "start": 75.4, "end": 82.76, "text": " Our cost function can be defined as j of w1 through wn comma b, but instead of just thinking", "tokens": [50364, 1171, 505, 949, 11, 321, 632, 7642, 3866, 8213, 24590, 411, 341, 13, 50590, 50590, 823, 1228, 8062, 24657, 11, 321, 393, 2464, 264, 2316, 382, 283, 1422, 261, 65, 295, 2031, 6915, 264, 8062, 261, 51010, 51010, 5893, 1674, 365, 264, 8062, 2031, 1804, 272, 13, 51196, 51196, 400, 1604, 300, 341, 5893, 510, 1355, 5893, 1674, 13, 51406, 51406, 2621, 2063, 2445, 393, 312, 7642, 382, 361, 295, 261, 16, 807, 45368, 22117, 272, 11, 457, 2602, 295, 445, 1953, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.15340669765028841, "compression_ratio": 1.5849056603773586, "no_speech_prob": 1.6027959190978436e-06}, {"id": 18, "seek": 8276, "start": 82.76, "end": 90.92, "text": " of j as a function of these n different parameters wj as well as b, we're going to write j as", "tokens": [50364, 295, 361, 382, 257, 2445, 295, 613, 297, 819, 9834, 261, 73, 382, 731, 382, 272, 11, 321, 434, 516, 281, 2464, 361, 382, 50772, 50772, 257, 2445, 295, 13075, 8062, 261, 293, 264, 1230, 272, 13, 51058, 51058, 407, 341, 261, 16, 807, 45368, 307, 10772, 538, 341, 8062, 261, 11, 293, 361, 586, 2516, 341, 4846, 11, 257, 8062, 51522, 51522, 261, 293, 257, 1230, 272, 11, 293, 11247, 257, 1230, 13, 51752, 51752, 1692, 311, 437, 16235, 23475, 1542, 411, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1776219823143699, "compression_ratio": 1.6881720430107527, "no_speech_prob": 2.9479492695827503e-06}, {"id": 19, "seek": 8276, "start": 90.92, "end": 96.64, "text": " a function of parameter vector w and the number b.", "tokens": [50364, 295, 361, 382, 257, 2445, 295, 613, 297, 819, 9834, 261, 73, 382, 731, 382, 272, 11, 321, 434, 516, 281, 2464, 361, 382, 50772, 50772, 257, 2445, 295, 13075, 8062, 261, 293, 264, 1230, 272, 13, 51058, 51058, 407, 341, 261, 16, 807, 45368, 307, 10772, 538, 341, 8062, 261, 11, 293, 361, 586, 2516, 341, 4846, 11, 257, 8062, 51522, 51522, 261, 293, 257, 1230, 272, 11, 293, 11247, 257, 1230, 13, 51752, 51752, 1692, 311, 437, 16235, 23475, 1542, 411, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1776219823143699, "compression_ratio": 1.6881720430107527, "no_speech_prob": 2.9479492695827503e-06}, {"id": 20, "seek": 8276, "start": 96.64, "end": 105.92, "text": " So this w1 through wn is replaced by this vector w, and j now takes this input, a vector", "tokens": [50364, 295, 361, 382, 257, 2445, 295, 613, 297, 819, 9834, 261, 73, 382, 731, 382, 272, 11, 321, 434, 516, 281, 2464, 361, 382, 50772, 50772, 257, 2445, 295, 13075, 8062, 261, 293, 264, 1230, 272, 13, 51058, 51058, 407, 341, 261, 16, 807, 45368, 307, 10772, 538, 341, 8062, 261, 11, 293, 361, 586, 2516, 341, 4846, 11, 257, 8062, 51522, 51522, 261, 293, 257, 1230, 272, 11, 293, 11247, 257, 1230, 13, 51752, 51752, 1692, 311, 437, 16235, 23475, 1542, 411, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1776219823143699, "compression_ratio": 1.6881720430107527, "no_speech_prob": 2.9479492695827503e-06}, {"id": 21, "seek": 8276, "start": 105.92, "end": 110.52000000000001, "text": " w and a number b, and returns a number.", "tokens": [50364, 295, 361, 382, 257, 2445, 295, 613, 297, 819, 9834, 261, 73, 382, 731, 382, 272, 11, 321, 434, 516, 281, 2464, 361, 382, 50772, 50772, 257, 2445, 295, 13075, 8062, 261, 293, 264, 1230, 272, 13, 51058, 51058, 407, 341, 261, 16, 807, 45368, 307, 10772, 538, 341, 8062, 261, 11, 293, 361, 586, 2516, 341, 4846, 11, 257, 8062, 51522, 51522, 261, 293, 257, 1230, 272, 11, 293, 11247, 257, 1230, 13, 51752, 51752, 1692, 311, 437, 16235, 23475, 1542, 411, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1776219823143699, "compression_ratio": 1.6881720430107527, "no_speech_prob": 2.9479492695827503e-06}, {"id": 22, "seek": 8276, "start": 110.52000000000001, "end": 112.72, "text": " Here's what gradient descent looks like.", "tokens": [50364, 295, 361, 382, 257, 2445, 295, 613, 297, 819, 9834, 261, 73, 382, 731, 382, 272, 11, 321, 434, 516, 281, 2464, 361, 382, 50772, 50772, 257, 2445, 295, 13075, 8062, 261, 293, 264, 1230, 272, 13, 51058, 51058, 407, 341, 261, 16, 807, 45368, 307, 10772, 538, 341, 8062, 261, 11, 293, 361, 586, 2516, 341, 4846, 11, 257, 8062, 51522, 51522, 261, 293, 257, 1230, 272, 11, 293, 11247, 257, 1230, 13, 51752, 51752, 1692, 311, 437, 16235, 23475, 1542, 411, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.1776219823143699, "compression_ratio": 1.6881720430107527, "no_speech_prob": 2.9479492695827503e-06}, {"id": 23, "seek": 11272, "start": 112.72, "end": 119.67999999999999, "text": " We're going to repeatedly update each parameter wj to be wj minus alpha times the derivative", "tokens": [50364, 492, 434, 516, 281, 18227, 5623, 1184, 13075, 261, 73, 281, 312, 261, 73, 3175, 8961, 1413, 264, 13760, 50712, 50712, 295, 264, 2063, 361, 11, 689, 361, 575, 9834, 261, 16, 807, 45368, 293, 272, 11, 293, 1564, 797, 321, 445, 2464, 341, 51144, 51144, 382, 361, 295, 8062, 261, 293, 1230, 272, 13, 51428, 51428, 961, 311, 536, 437, 341, 486, 574, 411, 562, 291, 4445, 16235, 23475, 11, 293, 294, 1729, 11, 51650, 51650, 718, 311, 747, 257, 574, 412, 264, 13760, 1433, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0823457896054446, "compression_ratio": 1.5936073059360731, "no_speech_prob": 1.9637705008790363e-06}, {"id": 24, "seek": 11272, "start": 119.67999999999999, "end": 128.32, "text": " of the cost j, where j has parameters w1 through wn and b, and once again we just write this", "tokens": [50364, 492, 434, 516, 281, 18227, 5623, 1184, 13075, 261, 73, 281, 312, 261, 73, 3175, 8961, 1413, 264, 13760, 50712, 50712, 295, 264, 2063, 361, 11, 689, 361, 575, 9834, 261, 16, 807, 45368, 293, 272, 11, 293, 1564, 797, 321, 445, 2464, 341, 51144, 51144, 382, 361, 295, 8062, 261, 293, 1230, 272, 13, 51428, 51428, 961, 311, 536, 437, 341, 486, 574, 411, 562, 291, 4445, 16235, 23475, 11, 293, 294, 1729, 11, 51650, 51650, 718, 311, 747, 257, 574, 412, 264, 13760, 1433, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0823457896054446, "compression_ratio": 1.5936073059360731, "no_speech_prob": 1.9637705008790363e-06}, {"id": 25, "seek": 11272, "start": 128.32, "end": 134.0, "text": " as j of vector w and number b.", "tokens": [50364, 492, 434, 516, 281, 18227, 5623, 1184, 13075, 261, 73, 281, 312, 261, 73, 3175, 8961, 1413, 264, 13760, 50712, 50712, 295, 264, 2063, 361, 11, 689, 361, 575, 9834, 261, 16, 807, 45368, 293, 272, 11, 293, 1564, 797, 321, 445, 2464, 341, 51144, 51144, 382, 361, 295, 8062, 261, 293, 1230, 272, 13, 51428, 51428, 961, 311, 536, 437, 341, 486, 574, 411, 562, 291, 4445, 16235, 23475, 11, 293, 294, 1729, 11, 51650, 51650, 718, 311, 747, 257, 574, 412, 264, 13760, 1433, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0823457896054446, "compression_ratio": 1.5936073059360731, "no_speech_prob": 1.9637705008790363e-06}, {"id": 26, "seek": 11272, "start": 134.0, "end": 138.44, "text": " Let's see what this will look like when you implement gradient descent, and in particular,", "tokens": [50364, 492, 434, 516, 281, 18227, 5623, 1184, 13075, 261, 73, 281, 312, 261, 73, 3175, 8961, 1413, 264, 13760, 50712, 50712, 295, 264, 2063, 361, 11, 689, 361, 575, 9834, 261, 16, 807, 45368, 293, 272, 11, 293, 1564, 797, 321, 445, 2464, 341, 51144, 51144, 382, 361, 295, 8062, 261, 293, 1230, 272, 13, 51428, 51428, 961, 311, 536, 437, 341, 486, 574, 411, 562, 291, 4445, 16235, 23475, 11, 293, 294, 1729, 11, 51650, 51650, 718, 311, 747, 257, 574, 412, 264, 13760, 1433, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0823457896054446, "compression_ratio": 1.5936073059360731, "no_speech_prob": 1.9637705008790363e-06}, {"id": 27, "seek": 11272, "start": 138.44, "end": 141.72, "text": " let's take a look at the derivative term.", "tokens": [50364, 492, 434, 516, 281, 18227, 5623, 1184, 13075, 261, 73, 281, 312, 261, 73, 3175, 8961, 1413, 264, 13760, 50712, 50712, 295, 264, 2063, 361, 11, 689, 361, 575, 9834, 261, 16, 807, 45368, 293, 272, 11, 293, 1564, 797, 321, 445, 2464, 341, 51144, 51144, 382, 361, 295, 8062, 261, 293, 1230, 272, 13, 51428, 51428, 961, 311, 536, 437, 341, 486, 574, 411, 562, 291, 4445, 16235, 23475, 11, 293, 294, 1729, 11, 51650, 51650, 718, 311, 747, 257, 574, 412, 264, 13760, 1433, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0823457896054446, "compression_ratio": 1.5936073059360731, "no_speech_prob": 1.9637705008790363e-06}, {"id": 28, "seek": 14172, "start": 141.72, "end": 146.64, "text": " You'll see that gradient descent becomes just a little bit different with multiple features", "tokens": [50364, 509, 603, 536, 300, 16235, 23475, 3643, 445, 257, 707, 857, 819, 365, 3866, 4122, 50610, 50610, 5347, 281, 445, 472, 4111, 13, 50750, 50750, 1692, 311, 437, 321, 632, 562, 321, 632, 16235, 23475, 365, 472, 4111, 13, 50964, 50964, 492, 632, 364, 5623, 4978, 337, 261, 293, 257, 4994, 5623, 4978, 337, 272, 13, 51214, 51214, 407, 4696, 613, 574, 4963, 281, 291, 13, 51390, 51390, 400, 341, 1433, 510, 307, 264, 13760, 295, 264, 2063, 2445, 361, 365, 3104, 281, 264, 13075, 51672, 51672, 261, 13, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.1156594471264911, "compression_ratio": 1.7098214285714286, "no_speech_prob": 3.1381050575873815e-06}, {"id": 29, "seek": 14172, "start": 146.64, "end": 149.44, "text": " compared to just one feature.", "tokens": [50364, 509, 603, 536, 300, 16235, 23475, 3643, 445, 257, 707, 857, 819, 365, 3866, 4122, 50610, 50610, 5347, 281, 445, 472, 4111, 13, 50750, 50750, 1692, 311, 437, 321, 632, 562, 321, 632, 16235, 23475, 365, 472, 4111, 13, 50964, 50964, 492, 632, 364, 5623, 4978, 337, 261, 293, 257, 4994, 5623, 4978, 337, 272, 13, 51214, 51214, 407, 4696, 613, 574, 4963, 281, 291, 13, 51390, 51390, 400, 341, 1433, 510, 307, 264, 13760, 295, 264, 2063, 2445, 361, 365, 3104, 281, 264, 13075, 51672, 51672, 261, 13, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.1156594471264911, "compression_ratio": 1.7098214285714286, "no_speech_prob": 3.1381050575873815e-06}, {"id": 30, "seek": 14172, "start": 149.44, "end": 153.72, "text": " Here's what we had when we had gradient descent with one feature.", "tokens": [50364, 509, 603, 536, 300, 16235, 23475, 3643, 445, 257, 707, 857, 819, 365, 3866, 4122, 50610, 50610, 5347, 281, 445, 472, 4111, 13, 50750, 50750, 1692, 311, 437, 321, 632, 562, 321, 632, 16235, 23475, 365, 472, 4111, 13, 50964, 50964, 492, 632, 364, 5623, 4978, 337, 261, 293, 257, 4994, 5623, 4978, 337, 272, 13, 51214, 51214, 407, 4696, 613, 574, 4963, 281, 291, 13, 51390, 51390, 400, 341, 1433, 510, 307, 264, 13760, 295, 264, 2063, 2445, 361, 365, 3104, 281, 264, 13075, 51672, 51672, 261, 13, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.1156594471264911, "compression_ratio": 1.7098214285714286, "no_speech_prob": 3.1381050575873815e-06}, {"id": 31, "seek": 14172, "start": 153.72, "end": 158.72, "text": " We had an update rule for w and a separate update rule for b.", "tokens": [50364, 509, 603, 536, 300, 16235, 23475, 3643, 445, 257, 707, 857, 819, 365, 3866, 4122, 50610, 50610, 5347, 281, 445, 472, 4111, 13, 50750, 50750, 1692, 311, 437, 321, 632, 562, 321, 632, 16235, 23475, 365, 472, 4111, 13, 50964, 50964, 492, 632, 364, 5623, 4978, 337, 261, 293, 257, 4994, 5623, 4978, 337, 272, 13, 51214, 51214, 407, 4696, 613, 574, 4963, 281, 291, 13, 51390, 51390, 400, 341, 1433, 510, 307, 264, 13760, 295, 264, 2063, 2445, 361, 365, 3104, 281, 264, 13075, 51672, 51672, 261, 13, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.1156594471264911, "compression_ratio": 1.7098214285714286, "no_speech_prob": 3.1381050575873815e-06}, {"id": 32, "seek": 14172, "start": 158.72, "end": 162.24, "text": " So hopefully these look familiar to you.", "tokens": [50364, 509, 603, 536, 300, 16235, 23475, 3643, 445, 257, 707, 857, 819, 365, 3866, 4122, 50610, 50610, 5347, 281, 445, 472, 4111, 13, 50750, 50750, 1692, 311, 437, 321, 632, 562, 321, 632, 16235, 23475, 365, 472, 4111, 13, 50964, 50964, 492, 632, 364, 5623, 4978, 337, 261, 293, 257, 4994, 5623, 4978, 337, 272, 13, 51214, 51214, 407, 4696, 613, 574, 4963, 281, 291, 13, 51390, 51390, 400, 341, 1433, 510, 307, 264, 13760, 295, 264, 2063, 2445, 361, 365, 3104, 281, 264, 13075, 51672, 51672, 261, 13, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.1156594471264911, "compression_ratio": 1.7098214285714286, "no_speech_prob": 3.1381050575873815e-06}, {"id": 33, "seek": 14172, "start": 162.24, "end": 167.88, "text": " And this term here is the derivative of the cost function j with respect to the parameter", "tokens": [50364, 509, 603, 536, 300, 16235, 23475, 3643, 445, 257, 707, 857, 819, 365, 3866, 4122, 50610, 50610, 5347, 281, 445, 472, 4111, 13, 50750, 50750, 1692, 311, 437, 321, 632, 562, 321, 632, 16235, 23475, 365, 472, 4111, 13, 50964, 50964, 492, 632, 364, 5623, 4978, 337, 261, 293, 257, 4994, 5623, 4978, 337, 272, 13, 51214, 51214, 407, 4696, 613, 574, 4963, 281, 291, 13, 51390, 51390, 400, 341, 1433, 510, 307, 264, 13760, 295, 264, 2063, 2445, 361, 365, 3104, 281, 264, 13075, 51672, 51672, 261, 13, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.1156594471264911, "compression_ratio": 1.7098214285714286, "no_speech_prob": 3.1381050575873815e-06}, {"id": 34, "seek": 14172, "start": 167.88, "end": 168.88, "text": " w.", "tokens": [50364, 509, 603, 536, 300, 16235, 23475, 3643, 445, 257, 707, 857, 819, 365, 3866, 4122, 50610, 50610, 5347, 281, 445, 472, 4111, 13, 50750, 50750, 1692, 311, 437, 321, 632, 562, 321, 632, 16235, 23475, 365, 472, 4111, 13, 50964, 50964, 492, 632, 364, 5623, 4978, 337, 261, 293, 257, 4994, 5623, 4978, 337, 272, 13, 51214, 51214, 407, 4696, 613, 574, 4963, 281, 291, 13, 51390, 51390, 400, 341, 1433, 510, 307, 264, 13760, 295, 264, 2063, 2445, 361, 365, 3104, 281, 264, 13075, 51672, 51672, 261, 13, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.1156594471264911, "compression_ratio": 1.7098214285714286, "no_speech_prob": 3.1381050575873815e-06}, {"id": 35, "seek": 16888, "start": 168.88, "end": 175.79999999999998, "text": " And similarly, we have an update rule for parameter b with univariate regression, we", "tokens": [50364, 400, 14138, 11, 321, 362, 364, 5623, 4978, 337, 13075, 272, 365, 517, 592, 3504, 473, 24590, 11, 321, 50710, 50710, 632, 787, 472, 4111, 13, 50832, 50832, 492, 818, 300, 4111, 36800, 1553, 604, 2325, 662, 13, 51062, 51062, 823, 510, 311, 264, 777, 24657, 337, 562, 321, 362, 297, 4122, 689, 297, 307, 732, 420, 544, 13, 51386, 51386, 492, 483, 341, 5623, 4978, 337, 16235, 23475, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.15941842826637062, "compression_ratio": 1.5852272727272727, "no_speech_prob": 5.338058599591022e-06}, {"id": 36, "seek": 16888, "start": 175.79999999999998, "end": 178.24, "text": " had only one feature.", "tokens": [50364, 400, 14138, 11, 321, 362, 364, 5623, 4978, 337, 13075, 272, 365, 517, 592, 3504, 473, 24590, 11, 321, 50710, 50710, 632, 787, 472, 4111, 13, 50832, 50832, 492, 818, 300, 4111, 36800, 1553, 604, 2325, 662, 13, 51062, 51062, 823, 510, 311, 264, 777, 24657, 337, 562, 321, 362, 297, 4122, 689, 297, 307, 732, 420, 544, 13, 51386, 51386, 492, 483, 341, 5623, 4978, 337, 16235, 23475, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.15941842826637062, "compression_ratio": 1.5852272727272727, "no_speech_prob": 5.338058599591022e-06}, {"id": 37, "seek": 16888, "start": 178.24, "end": 182.84, "text": " We call that feature xi without any subscript.", "tokens": [50364, 400, 14138, 11, 321, 362, 364, 5623, 4978, 337, 13075, 272, 365, 517, 592, 3504, 473, 24590, 11, 321, 50710, 50710, 632, 787, 472, 4111, 13, 50832, 50832, 492, 818, 300, 4111, 36800, 1553, 604, 2325, 662, 13, 51062, 51062, 823, 510, 311, 264, 777, 24657, 337, 562, 321, 362, 297, 4122, 689, 297, 307, 732, 420, 544, 13, 51386, 51386, 492, 483, 341, 5623, 4978, 337, 16235, 23475, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.15941842826637062, "compression_ratio": 1.5852272727272727, "no_speech_prob": 5.338058599591022e-06}, {"id": 38, "seek": 16888, "start": 182.84, "end": 189.32, "text": " Now here's the new notation for when we have n features where n is two or more.", "tokens": [50364, 400, 14138, 11, 321, 362, 364, 5623, 4978, 337, 13075, 272, 365, 517, 592, 3504, 473, 24590, 11, 321, 50710, 50710, 632, 787, 472, 4111, 13, 50832, 50832, 492, 818, 300, 4111, 36800, 1553, 604, 2325, 662, 13, 51062, 51062, 823, 510, 311, 264, 777, 24657, 337, 562, 321, 362, 297, 4122, 689, 297, 307, 732, 420, 544, 13, 51386, 51386, 492, 483, 341, 5623, 4978, 337, 16235, 23475, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.15941842826637062, "compression_ratio": 1.5852272727272727, "no_speech_prob": 5.338058599591022e-06}, {"id": 39, "seek": 16888, "start": 189.32, "end": 192.88, "text": " We get this update rule for gradient descent.", "tokens": [50364, 400, 14138, 11, 321, 362, 364, 5623, 4978, 337, 13075, 272, 365, 517, 592, 3504, 473, 24590, 11, 321, 50710, 50710, 632, 787, 472, 4111, 13, 50832, 50832, 492, 818, 300, 4111, 36800, 1553, 604, 2325, 662, 13, 51062, 51062, 823, 510, 311, 264, 777, 24657, 337, 562, 321, 362, 297, 4122, 689, 297, 307, 732, 420, 544, 13, 51386, 51386, 492, 483, 341, 5623, 4978, 337, 16235, 23475, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.15941842826637062, "compression_ratio": 1.5852272727272727, "no_speech_prob": 5.338058599591022e-06}, {"id": 40, "seek": 19288, "start": 192.88, "end": 199.48, "text": " We have a w1 to be w1 minus alpha times this expression here.", "tokens": [50364, 492, 362, 257, 261, 16, 281, 312, 261, 16, 3175, 8961, 1413, 341, 6114, 510, 13, 50694, 50694, 400, 341, 8513, 307, 767, 264, 13760, 295, 264, 2063, 361, 365, 3104, 281, 261, 16, 13, 51088, 51088, 440, 8513, 337, 264, 13760, 295, 361, 365, 3104, 281, 261, 16, 322, 264, 558, 1542, 588, 2531, 281, 264, 51376, 51376, 1389, 295, 472, 4111, 322, 264, 1411, 13, 51538, 51538, 440, 6713, 1433, 920, 2516, 257, 17630, 283, 295, 2031, 3175, 264, 3779, 288, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1451246738433838, "compression_ratio": 1.7076923076923076, "no_speech_prob": 8.939527106122114e-06}, {"id": 41, "seek": 19288, "start": 199.48, "end": 207.35999999999999, "text": " And this formula is actually the derivative of the cost j with respect to w1.", "tokens": [50364, 492, 362, 257, 261, 16, 281, 312, 261, 16, 3175, 8961, 1413, 341, 6114, 510, 13, 50694, 50694, 400, 341, 8513, 307, 767, 264, 13760, 295, 264, 2063, 361, 365, 3104, 281, 261, 16, 13, 51088, 51088, 440, 8513, 337, 264, 13760, 295, 361, 365, 3104, 281, 261, 16, 322, 264, 558, 1542, 588, 2531, 281, 264, 51376, 51376, 1389, 295, 472, 4111, 322, 264, 1411, 13, 51538, 51538, 440, 6713, 1433, 920, 2516, 257, 17630, 283, 295, 2031, 3175, 264, 3779, 288, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1451246738433838, "compression_ratio": 1.7076923076923076, "no_speech_prob": 8.939527106122114e-06}, {"id": 42, "seek": 19288, "start": 207.35999999999999, "end": 213.12, "text": " The formula for the derivative of j with respect to w1 on the right looks very similar to the", "tokens": [50364, 492, 362, 257, 261, 16, 281, 312, 261, 16, 3175, 8961, 1413, 341, 6114, 510, 13, 50694, 50694, 400, 341, 8513, 307, 767, 264, 13760, 295, 264, 2063, 361, 365, 3104, 281, 261, 16, 13, 51088, 51088, 440, 8513, 337, 264, 13760, 295, 361, 365, 3104, 281, 261, 16, 322, 264, 558, 1542, 588, 2531, 281, 264, 51376, 51376, 1389, 295, 472, 4111, 322, 264, 1411, 13, 51538, 51538, 440, 6713, 1433, 920, 2516, 257, 17630, 283, 295, 2031, 3175, 264, 3779, 288, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1451246738433838, "compression_ratio": 1.7076923076923076, "no_speech_prob": 8.939527106122114e-06}, {"id": 43, "seek": 19288, "start": 213.12, "end": 216.35999999999999, "text": " case of one feature on the left.", "tokens": [50364, 492, 362, 257, 261, 16, 281, 312, 261, 16, 3175, 8961, 1413, 341, 6114, 510, 13, 50694, 50694, 400, 341, 8513, 307, 767, 264, 13760, 295, 264, 2063, 361, 365, 3104, 281, 261, 16, 13, 51088, 51088, 440, 8513, 337, 264, 13760, 295, 361, 365, 3104, 281, 261, 16, 322, 264, 558, 1542, 588, 2531, 281, 264, 51376, 51376, 1389, 295, 472, 4111, 322, 264, 1411, 13, 51538, 51538, 440, 6713, 1433, 920, 2516, 257, 17630, 283, 295, 2031, 3175, 264, 3779, 288, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1451246738433838, "compression_ratio": 1.7076923076923076, "no_speech_prob": 8.939527106122114e-06}, {"id": 44, "seek": 19288, "start": 216.35999999999999, "end": 222.04, "text": " The error term still takes a prediction f of x minus the target y.", "tokens": [50364, 492, 362, 257, 261, 16, 281, 312, 261, 16, 3175, 8961, 1413, 341, 6114, 510, 13, 50694, 50694, 400, 341, 8513, 307, 767, 264, 13760, 295, 264, 2063, 361, 365, 3104, 281, 261, 16, 13, 51088, 51088, 440, 8513, 337, 264, 13760, 295, 361, 365, 3104, 281, 261, 16, 322, 264, 558, 1542, 588, 2531, 281, 264, 51376, 51376, 1389, 295, 472, 4111, 322, 264, 1411, 13, 51538, 51538, 440, 6713, 1433, 920, 2516, 257, 17630, 283, 295, 2031, 3175, 264, 3779, 288, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.1451246738433838, "compression_ratio": 1.7076923076923076, "no_speech_prob": 8.939527106122114e-06}, {"id": 45, "seek": 22204, "start": 222.04, "end": 226.88, "text": " One difference is that w and x are now vectors.", "tokens": [50364, 1485, 2649, 307, 300, 261, 293, 2031, 366, 586, 18875, 13, 50606, 50606, 400, 445, 382, 261, 322, 264, 1411, 575, 586, 1813, 261, 16, 510, 322, 264, 558, 11, 36800, 510, 322, 264, 1411, 307, 50996, 50996, 586, 2602, 36800, 2325, 662, 502, 510, 322, 264, 558, 13, 51240, 51240, 400, 341, 307, 445, 337, 361, 6915, 502, 13, 51516, 51516], "temperature": 0.0, "avg_logprob": -0.11586463451385498, "compression_ratio": 1.536231884057971, "no_speech_prob": 6.962159659451572e-06}, {"id": 46, "seek": 22204, "start": 226.88, "end": 234.68, "text": " And just as w on the left has now become w1 here on the right, xi here on the left is", "tokens": [50364, 1485, 2649, 307, 300, 261, 293, 2031, 366, 586, 18875, 13, 50606, 50606, 400, 445, 382, 261, 322, 264, 1411, 575, 586, 1813, 261, 16, 510, 322, 264, 558, 11, 36800, 510, 322, 264, 1411, 307, 50996, 50996, 586, 2602, 36800, 2325, 662, 502, 510, 322, 264, 558, 13, 51240, 51240, 400, 341, 307, 445, 337, 361, 6915, 502, 13, 51516, 51516], "temperature": 0.0, "avg_logprob": -0.11586463451385498, "compression_ratio": 1.536231884057971, "no_speech_prob": 6.962159659451572e-06}, {"id": 47, "seek": 22204, "start": 234.68, "end": 239.56, "text": " now instead xi subscript 1 here on the right.", "tokens": [50364, 1485, 2649, 307, 300, 261, 293, 2031, 366, 586, 18875, 13, 50606, 50606, 400, 445, 382, 261, 322, 264, 1411, 575, 586, 1813, 261, 16, 510, 322, 264, 558, 11, 36800, 510, 322, 264, 1411, 307, 50996, 50996, 586, 2602, 36800, 2325, 662, 502, 510, 322, 264, 558, 13, 51240, 51240, 400, 341, 307, 445, 337, 361, 6915, 502, 13, 51516, 51516], "temperature": 0.0, "avg_logprob": -0.11586463451385498, "compression_ratio": 1.536231884057971, "no_speech_prob": 6.962159659451572e-06}, {"id": 48, "seek": 22204, "start": 239.56, "end": 245.07999999999998, "text": " And this is just for j equals 1.", "tokens": [50364, 1485, 2649, 307, 300, 261, 293, 2031, 366, 586, 18875, 13, 50606, 50606, 400, 445, 382, 261, 322, 264, 1411, 575, 586, 1813, 261, 16, 510, 322, 264, 558, 11, 36800, 510, 322, 264, 1411, 307, 50996, 50996, 586, 2602, 36800, 2325, 662, 502, 510, 322, 264, 558, 13, 51240, 51240, 400, 341, 307, 445, 337, 361, 6915, 502, 13, 51516, 51516], "temperature": 0.0, "avg_logprob": -0.11586463451385498, "compression_ratio": 1.536231884057971, "no_speech_prob": 6.962159659451572e-06}, {"id": 49, "seek": 24508, "start": 245.08, "end": 252.52, "text": " For multiple linear regression, we have j ranging from 1 through n, and so we'll update", "tokens": [50364, 1171, 3866, 8213, 24590, 11, 321, 362, 361, 25532, 490, 502, 807, 297, 11, 293, 370, 321, 603, 5623, 50736, 50736, 264, 9834, 261, 16, 11, 261, 17, 439, 264, 636, 493, 281, 261, 77, 13, 51130, 51130, 400, 550, 382, 949, 11, 321, 603, 5623, 272, 13, 51328, 51328, 400, 498, 291, 4445, 341, 11, 291, 483, 16235, 23475, 337, 3866, 24590, 13, 51594, 51594, 407, 300, 311, 309, 337, 16235, 23475, 337, 3866, 24590, 13, 51843, 51843], "temperature": 0.0, "avg_logprob": -0.13036030898859471, "compression_ratio": 1.7314285714285715, "no_speech_prob": 3.2377092793467455e-06}, {"id": 50, "seek": 24508, "start": 252.52, "end": 260.40000000000003, "text": " the parameters w1, w2 all the way up to wn.", "tokens": [50364, 1171, 3866, 8213, 24590, 11, 321, 362, 361, 25532, 490, 502, 807, 297, 11, 293, 370, 321, 603, 5623, 50736, 50736, 264, 9834, 261, 16, 11, 261, 17, 439, 264, 636, 493, 281, 261, 77, 13, 51130, 51130, 400, 550, 382, 949, 11, 321, 603, 5623, 272, 13, 51328, 51328, 400, 498, 291, 4445, 341, 11, 291, 483, 16235, 23475, 337, 3866, 24590, 13, 51594, 51594, 407, 300, 311, 309, 337, 16235, 23475, 337, 3866, 24590, 13, 51843, 51843], "temperature": 0.0, "avg_logprob": -0.13036030898859471, "compression_ratio": 1.7314285714285715, "no_speech_prob": 3.2377092793467455e-06}, {"id": 51, "seek": 24508, "start": 260.40000000000003, "end": 264.36, "text": " And then as before, we'll update b.", "tokens": [50364, 1171, 3866, 8213, 24590, 11, 321, 362, 361, 25532, 490, 502, 807, 297, 11, 293, 370, 321, 603, 5623, 50736, 50736, 264, 9834, 261, 16, 11, 261, 17, 439, 264, 636, 493, 281, 261, 77, 13, 51130, 51130, 400, 550, 382, 949, 11, 321, 603, 5623, 272, 13, 51328, 51328, 400, 498, 291, 4445, 341, 11, 291, 483, 16235, 23475, 337, 3866, 24590, 13, 51594, 51594, 407, 300, 311, 309, 337, 16235, 23475, 337, 3866, 24590, 13, 51843, 51843], "temperature": 0.0, "avg_logprob": -0.13036030898859471, "compression_ratio": 1.7314285714285715, "no_speech_prob": 3.2377092793467455e-06}, {"id": 52, "seek": 24508, "start": 264.36, "end": 269.68, "text": " And if you implement this, you get gradient descent for multiple regression.", "tokens": [50364, 1171, 3866, 8213, 24590, 11, 321, 362, 361, 25532, 490, 502, 807, 297, 11, 293, 370, 321, 603, 5623, 50736, 50736, 264, 9834, 261, 16, 11, 261, 17, 439, 264, 636, 493, 281, 261, 77, 13, 51130, 51130, 400, 550, 382, 949, 11, 321, 603, 5623, 272, 13, 51328, 51328, 400, 498, 291, 4445, 341, 11, 291, 483, 16235, 23475, 337, 3866, 24590, 13, 51594, 51594, 407, 300, 311, 309, 337, 16235, 23475, 337, 3866, 24590, 13, 51843, 51843], "temperature": 0.0, "avg_logprob": -0.13036030898859471, "compression_ratio": 1.7314285714285715, "no_speech_prob": 3.2377092793467455e-06}, {"id": 53, "seek": 24508, "start": 269.68, "end": 274.66, "text": " So that's it for gradient descent for multiple regression.", "tokens": [50364, 1171, 3866, 8213, 24590, 11, 321, 362, 361, 25532, 490, 502, 807, 297, 11, 293, 370, 321, 603, 5623, 50736, 50736, 264, 9834, 261, 16, 11, 261, 17, 439, 264, 636, 493, 281, 261, 77, 13, 51130, 51130, 400, 550, 382, 949, 11, 321, 603, 5623, 272, 13, 51328, 51328, 400, 498, 291, 4445, 341, 11, 291, 483, 16235, 23475, 337, 3866, 24590, 13, 51594, 51594, 407, 300, 311, 309, 337, 16235, 23475, 337, 3866, 24590, 13, 51843, 51843], "temperature": 0.0, "avg_logprob": -0.13036030898859471, "compression_ratio": 1.7314285714285715, "no_speech_prob": 3.2377092793467455e-06}, {"id": 54, "seek": 27466, "start": 274.66, "end": 281.28000000000003, "text": " Before moving on from this video, I want to make a quick aside or a quick side note on", "tokens": [50364, 4546, 2684, 322, 490, 341, 960, 11, 286, 528, 281, 652, 257, 1702, 7359, 420, 257, 1702, 1252, 3637, 322, 50695, 50695, 364, 8535, 636, 337, 5006, 261, 293, 272, 337, 8213, 24590, 13, 50999, 50999, 400, 341, 3170, 307, 1219, 264, 2710, 5367, 13, 51203, 51203, 13813, 309, 4523, 484, 16235, 23475, 307, 257, 869, 3170, 337, 46608, 264, 2063, 2445, 51441, 51441, 361, 281, 915, 261, 293, 272, 13, 51597, 51597, 821, 307, 472, 661, 9284, 300, 1985, 787, 337, 8213, 24590, 293, 1238, 709, 6022, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1029387038687001, "compression_ratio": 1.6638297872340426, "no_speech_prob": 2.4299051801790483e-05}, {"id": 55, "seek": 27466, "start": 281.28000000000003, "end": 287.36, "text": " an alternative way for finding w and b for linear regression.", "tokens": [50364, 4546, 2684, 322, 490, 341, 960, 11, 286, 528, 281, 652, 257, 1702, 7359, 420, 257, 1702, 1252, 3637, 322, 50695, 50695, 364, 8535, 636, 337, 5006, 261, 293, 272, 337, 8213, 24590, 13, 50999, 50999, 400, 341, 3170, 307, 1219, 264, 2710, 5367, 13, 51203, 51203, 13813, 309, 4523, 484, 16235, 23475, 307, 257, 869, 3170, 337, 46608, 264, 2063, 2445, 51441, 51441, 361, 281, 915, 261, 293, 272, 13, 51597, 51597, 821, 307, 472, 661, 9284, 300, 1985, 787, 337, 8213, 24590, 293, 1238, 709, 6022, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1029387038687001, "compression_ratio": 1.6638297872340426, "no_speech_prob": 2.4299051801790483e-05}, {"id": 56, "seek": 27466, "start": 287.36, "end": 291.44000000000005, "text": " And this method is called the normal equation.", "tokens": [50364, 4546, 2684, 322, 490, 341, 960, 11, 286, 528, 281, 652, 257, 1702, 7359, 420, 257, 1702, 1252, 3637, 322, 50695, 50695, 364, 8535, 636, 337, 5006, 261, 293, 272, 337, 8213, 24590, 13, 50999, 50999, 400, 341, 3170, 307, 1219, 264, 2710, 5367, 13, 51203, 51203, 13813, 309, 4523, 484, 16235, 23475, 307, 257, 869, 3170, 337, 46608, 264, 2063, 2445, 51441, 51441, 361, 281, 915, 261, 293, 272, 13, 51597, 51597, 821, 307, 472, 661, 9284, 300, 1985, 787, 337, 8213, 24590, 293, 1238, 709, 6022, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1029387038687001, "compression_ratio": 1.6638297872340426, "no_speech_prob": 2.4299051801790483e-05}, {"id": 57, "seek": 27466, "start": 291.44000000000005, "end": 296.20000000000005, "text": " Whereas it turns out gradient descent is a great method for minimizing the cost function", "tokens": [50364, 4546, 2684, 322, 490, 341, 960, 11, 286, 528, 281, 652, 257, 1702, 7359, 420, 257, 1702, 1252, 3637, 322, 50695, 50695, 364, 8535, 636, 337, 5006, 261, 293, 272, 337, 8213, 24590, 13, 50999, 50999, 400, 341, 3170, 307, 1219, 264, 2710, 5367, 13, 51203, 51203, 13813, 309, 4523, 484, 16235, 23475, 307, 257, 869, 3170, 337, 46608, 264, 2063, 2445, 51441, 51441, 361, 281, 915, 261, 293, 272, 13, 51597, 51597, 821, 307, 472, 661, 9284, 300, 1985, 787, 337, 8213, 24590, 293, 1238, 709, 6022, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1029387038687001, "compression_ratio": 1.6638297872340426, "no_speech_prob": 2.4299051801790483e-05}, {"id": 58, "seek": 27466, "start": 296.20000000000005, "end": 299.32000000000005, "text": " j to find w and b.", "tokens": [50364, 4546, 2684, 322, 490, 341, 960, 11, 286, 528, 281, 652, 257, 1702, 7359, 420, 257, 1702, 1252, 3637, 322, 50695, 50695, 364, 8535, 636, 337, 5006, 261, 293, 272, 337, 8213, 24590, 13, 50999, 50999, 400, 341, 3170, 307, 1219, 264, 2710, 5367, 13, 51203, 51203, 13813, 309, 4523, 484, 16235, 23475, 307, 257, 869, 3170, 337, 46608, 264, 2063, 2445, 51441, 51441, 361, 281, 915, 261, 293, 272, 13, 51597, 51597, 821, 307, 472, 661, 9284, 300, 1985, 787, 337, 8213, 24590, 293, 1238, 709, 6022, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1029387038687001, "compression_ratio": 1.6638297872340426, "no_speech_prob": 2.4299051801790483e-05}, {"id": 59, "seek": 27466, "start": 299.32000000000005, "end": 304.38, "text": " There is one other algorithm that works only for linear regression and pretty much none", "tokens": [50364, 4546, 2684, 322, 490, 341, 960, 11, 286, 528, 281, 652, 257, 1702, 7359, 420, 257, 1702, 1252, 3637, 322, 50695, 50695, 364, 8535, 636, 337, 5006, 261, 293, 272, 337, 8213, 24590, 13, 50999, 50999, 400, 341, 3170, 307, 1219, 264, 2710, 5367, 13, 51203, 51203, 13813, 309, 4523, 484, 16235, 23475, 307, 257, 869, 3170, 337, 46608, 264, 2063, 2445, 51441, 51441, 361, 281, 915, 261, 293, 272, 13, 51597, 51597, 821, 307, 472, 661, 9284, 300, 1985, 787, 337, 8213, 24590, 293, 1238, 709, 6022, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.1029387038687001, "compression_ratio": 1.6638297872340426, "no_speech_prob": 2.4299051801790483e-05}, {"id": 60, "seek": 30438, "start": 304.38, "end": 310.08, "text": " of the other algorithms you see in this specialization for solving for w and b.", "tokens": [50364, 295, 264, 661, 14642, 291, 536, 294, 341, 2121, 2144, 337, 12606, 337, 261, 293, 272, 13, 50649, 50649, 400, 341, 661, 3170, 775, 406, 643, 364, 17138, 1166, 16235, 23475, 9284, 13, 50945, 50945, 45001, 264, 2710, 5367, 3170, 11, 309, 4523, 484, 281, 312, 1944, 281, 764, 364, 7339, 8213, 51187, 51187, 21989, 6405, 281, 445, 5039, 337, 261, 293, 272, 439, 294, 472, 352, 1553, 36540, 13, 51511, 51511, 2188, 37431, 295, 264, 2710, 5367, 3170, 366, 11, 700, 11, 8343, 16235, 23475, 11, 51779, 51779], "temperature": 0.0, "avg_logprob": -0.08968582782116565, "compression_ratio": 1.777292576419214, "no_speech_prob": 3.966904841945507e-06}, {"id": 61, "seek": 30438, "start": 310.08, "end": 316.0, "text": " And this other method does not need an iterative gradient descent algorithm.", "tokens": [50364, 295, 264, 661, 14642, 291, 536, 294, 341, 2121, 2144, 337, 12606, 337, 261, 293, 272, 13, 50649, 50649, 400, 341, 661, 3170, 775, 406, 643, 364, 17138, 1166, 16235, 23475, 9284, 13, 50945, 50945, 45001, 264, 2710, 5367, 3170, 11, 309, 4523, 484, 281, 312, 1944, 281, 764, 364, 7339, 8213, 51187, 51187, 21989, 6405, 281, 445, 5039, 337, 261, 293, 272, 439, 294, 472, 352, 1553, 36540, 13, 51511, 51511, 2188, 37431, 295, 264, 2710, 5367, 3170, 366, 11, 700, 11, 8343, 16235, 23475, 11, 51779, 51779], "temperature": 0.0, "avg_logprob": -0.08968582782116565, "compression_ratio": 1.777292576419214, "no_speech_prob": 3.966904841945507e-06}, {"id": 62, "seek": 30438, "start": 316.0, "end": 320.84, "text": " Called the normal equation method, it turns out to be possible to use an advanced linear", "tokens": [50364, 295, 264, 661, 14642, 291, 536, 294, 341, 2121, 2144, 337, 12606, 337, 261, 293, 272, 13, 50649, 50649, 400, 341, 661, 3170, 775, 406, 643, 364, 17138, 1166, 16235, 23475, 9284, 13, 50945, 50945, 45001, 264, 2710, 5367, 3170, 11, 309, 4523, 484, 281, 312, 1944, 281, 764, 364, 7339, 8213, 51187, 51187, 21989, 6405, 281, 445, 5039, 337, 261, 293, 272, 439, 294, 472, 352, 1553, 36540, 13, 51511, 51511, 2188, 37431, 295, 264, 2710, 5367, 3170, 366, 11, 700, 11, 8343, 16235, 23475, 11, 51779, 51779], "temperature": 0.0, "avg_logprob": -0.08968582782116565, "compression_ratio": 1.777292576419214, "no_speech_prob": 3.966904841945507e-06}, {"id": 63, "seek": 30438, "start": 320.84, "end": 327.32, "text": " algebra library to just solve for w and b all in one go without iterations.", "tokens": [50364, 295, 264, 661, 14642, 291, 536, 294, 341, 2121, 2144, 337, 12606, 337, 261, 293, 272, 13, 50649, 50649, 400, 341, 661, 3170, 775, 406, 643, 364, 17138, 1166, 16235, 23475, 9284, 13, 50945, 50945, 45001, 264, 2710, 5367, 3170, 11, 309, 4523, 484, 281, 312, 1944, 281, 764, 364, 7339, 8213, 51187, 51187, 21989, 6405, 281, 445, 5039, 337, 261, 293, 272, 439, 294, 472, 352, 1553, 36540, 13, 51511, 51511, 2188, 37431, 295, 264, 2710, 5367, 3170, 366, 11, 700, 11, 8343, 16235, 23475, 11, 51779, 51779], "temperature": 0.0, "avg_logprob": -0.08968582782116565, "compression_ratio": 1.777292576419214, "no_speech_prob": 3.966904841945507e-06}, {"id": 64, "seek": 30438, "start": 327.32, "end": 332.68, "text": " Some disadvantages of the normal equation method are, first, unlike gradient descent,", "tokens": [50364, 295, 264, 661, 14642, 291, 536, 294, 341, 2121, 2144, 337, 12606, 337, 261, 293, 272, 13, 50649, 50649, 400, 341, 661, 3170, 775, 406, 643, 364, 17138, 1166, 16235, 23475, 9284, 13, 50945, 50945, 45001, 264, 2710, 5367, 3170, 11, 309, 4523, 484, 281, 312, 1944, 281, 764, 364, 7339, 8213, 51187, 51187, 21989, 6405, 281, 445, 5039, 337, 261, 293, 272, 439, 294, 472, 352, 1553, 36540, 13, 51511, 51511, 2188, 37431, 295, 264, 2710, 5367, 3170, 366, 11, 700, 11, 8343, 16235, 23475, 11, 51779, 51779], "temperature": 0.0, "avg_logprob": -0.08968582782116565, "compression_ratio": 1.777292576419214, "no_speech_prob": 3.966904841945507e-06}, {"id": 65, "seek": 33268, "start": 332.68, "end": 337.28000000000003, "text": " this does not generalize to other learning algorithms, such as the logistic regression", "tokens": [50364, 341, 775, 406, 2674, 1125, 281, 661, 2539, 14642, 11, 1270, 382, 264, 3565, 3142, 24590, 50594, 50594, 9284, 300, 291, 1466, 466, 958, 1243, 11, 420, 264, 18161, 9590, 420, 661, 14642, 50828, 50828, 291, 536, 1780, 294, 341, 2121, 2144, 13, 50958, 50958, 440, 2710, 5367, 3170, 307, 611, 1596, 2964, 498, 264, 1230, 295, 4122, 297, 307, 2416, 13, 51280, 51280, 12627, 572, 3479, 2539, 25742, 820, 4445, 264, 2710, 5367, 3170, 2969, 13, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.11205324714566454, "compression_ratio": 1.7180616740088106, "no_speech_prob": 3.5558089166443096e-06}, {"id": 66, "seek": 33268, "start": 337.28000000000003, "end": 341.96, "text": " algorithm that you learn about next week, or the neural networks or other algorithms", "tokens": [50364, 341, 775, 406, 2674, 1125, 281, 661, 2539, 14642, 11, 1270, 382, 264, 3565, 3142, 24590, 50594, 50594, 9284, 300, 291, 1466, 466, 958, 1243, 11, 420, 264, 18161, 9590, 420, 661, 14642, 50828, 50828, 291, 536, 1780, 294, 341, 2121, 2144, 13, 50958, 50958, 440, 2710, 5367, 3170, 307, 611, 1596, 2964, 498, 264, 1230, 295, 4122, 297, 307, 2416, 13, 51280, 51280, 12627, 572, 3479, 2539, 25742, 820, 4445, 264, 2710, 5367, 3170, 2969, 13, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.11205324714566454, "compression_ratio": 1.7180616740088106, "no_speech_prob": 3.5558089166443096e-06}, {"id": 67, "seek": 33268, "start": 341.96, "end": 344.56, "text": " you see later in this specialization.", "tokens": [50364, 341, 775, 406, 2674, 1125, 281, 661, 2539, 14642, 11, 1270, 382, 264, 3565, 3142, 24590, 50594, 50594, 9284, 300, 291, 1466, 466, 958, 1243, 11, 420, 264, 18161, 9590, 420, 661, 14642, 50828, 50828, 291, 536, 1780, 294, 341, 2121, 2144, 13, 50958, 50958, 440, 2710, 5367, 3170, 307, 611, 1596, 2964, 498, 264, 1230, 295, 4122, 297, 307, 2416, 13, 51280, 51280, 12627, 572, 3479, 2539, 25742, 820, 4445, 264, 2710, 5367, 3170, 2969, 13, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.11205324714566454, "compression_ratio": 1.7180616740088106, "no_speech_prob": 3.5558089166443096e-06}, {"id": 68, "seek": 33268, "start": 344.56, "end": 351.0, "text": " The normal equation method is also quite slow if the number of features n is large.", "tokens": [50364, 341, 775, 406, 2674, 1125, 281, 661, 2539, 14642, 11, 1270, 382, 264, 3565, 3142, 24590, 50594, 50594, 9284, 300, 291, 1466, 466, 958, 1243, 11, 420, 264, 18161, 9590, 420, 661, 14642, 50828, 50828, 291, 536, 1780, 294, 341, 2121, 2144, 13, 50958, 50958, 440, 2710, 5367, 3170, 307, 611, 1596, 2964, 498, 264, 1230, 295, 4122, 297, 307, 2416, 13, 51280, 51280, 12627, 572, 3479, 2539, 25742, 820, 4445, 264, 2710, 5367, 3170, 2969, 13, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.11205324714566454, "compression_ratio": 1.7180616740088106, "no_speech_prob": 3.5558089166443096e-06}, {"id": 69, "seek": 33268, "start": 351.0, "end": 356.84000000000003, "text": " Almost no machine learning practitioners should implement the normal equation method themselves.", "tokens": [50364, 341, 775, 406, 2674, 1125, 281, 661, 2539, 14642, 11, 1270, 382, 264, 3565, 3142, 24590, 50594, 50594, 9284, 300, 291, 1466, 466, 958, 1243, 11, 420, 264, 18161, 9590, 420, 661, 14642, 50828, 50828, 291, 536, 1780, 294, 341, 2121, 2144, 13, 50958, 50958, 440, 2710, 5367, 3170, 307, 611, 1596, 2964, 498, 264, 1230, 295, 4122, 297, 307, 2416, 13, 51280, 51280, 12627, 572, 3479, 2539, 25742, 820, 4445, 264, 2710, 5367, 3170, 2969, 13, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.11205324714566454, "compression_ratio": 1.7180616740088106, "no_speech_prob": 3.5558089166443096e-06}, {"id": 70, "seek": 35684, "start": 356.84, "end": 363.44, "text": " But if you're using a mature machine learning library and call linear regression, there's", "tokens": [50364, 583, 498, 291, 434, 1228, 257, 14442, 3479, 2539, 6405, 293, 818, 8213, 24590, 11, 456, 311, 50694, 50694, 257, 2931, 300, 322, 264, 38087, 11, 309, 486, 312, 1228, 341, 281, 5039, 337, 261, 293, 272, 13, 50954, 50954, 407, 498, 291, 434, 1562, 294, 257, 1691, 4049, 293, 1568, 264, 1433, 2710, 5367, 11, 300, 311, 437, 341, 51181, 51181, 14942, 281, 13, 51290, 51290, 1468, 380, 3292, 466, 264, 4365, 295, 577, 264, 2710, 5367, 1985, 13, 51456, 51456, 1449, 312, 3650, 300, 512, 3479, 2539, 15148, 815, 764, 341, 6179, 3170, 294, 264, 38087, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.1020668347676595, "compression_ratio": 1.7418032786885247, "no_speech_prob": 3.7852462355658645e-06}, {"id": 71, "seek": 35684, "start": 363.44, "end": 368.64, "text": " a chance that on the backend, it will be using this to solve for w and b.", "tokens": [50364, 583, 498, 291, 434, 1228, 257, 14442, 3479, 2539, 6405, 293, 818, 8213, 24590, 11, 456, 311, 50694, 50694, 257, 2931, 300, 322, 264, 38087, 11, 309, 486, 312, 1228, 341, 281, 5039, 337, 261, 293, 272, 13, 50954, 50954, 407, 498, 291, 434, 1562, 294, 257, 1691, 4049, 293, 1568, 264, 1433, 2710, 5367, 11, 300, 311, 437, 341, 51181, 51181, 14942, 281, 13, 51290, 51290, 1468, 380, 3292, 466, 264, 4365, 295, 577, 264, 2710, 5367, 1985, 13, 51456, 51456, 1449, 312, 3650, 300, 512, 3479, 2539, 15148, 815, 764, 341, 6179, 3170, 294, 264, 38087, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.1020668347676595, "compression_ratio": 1.7418032786885247, "no_speech_prob": 3.7852462355658645e-06}, {"id": 72, "seek": 35684, "start": 368.64, "end": 373.17999999999995, "text": " So if you're ever in a job interview and hear the term normal equation, that's what this", "tokens": [50364, 583, 498, 291, 434, 1228, 257, 14442, 3479, 2539, 6405, 293, 818, 8213, 24590, 11, 456, 311, 50694, 50694, 257, 2931, 300, 322, 264, 38087, 11, 309, 486, 312, 1228, 341, 281, 5039, 337, 261, 293, 272, 13, 50954, 50954, 407, 498, 291, 434, 1562, 294, 257, 1691, 4049, 293, 1568, 264, 1433, 2710, 5367, 11, 300, 311, 437, 341, 51181, 51181, 14942, 281, 13, 51290, 51290, 1468, 380, 3292, 466, 264, 4365, 295, 577, 264, 2710, 5367, 1985, 13, 51456, 51456, 1449, 312, 3650, 300, 512, 3479, 2539, 15148, 815, 764, 341, 6179, 3170, 294, 264, 38087, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.1020668347676595, "compression_ratio": 1.7418032786885247, "no_speech_prob": 3.7852462355658645e-06}, {"id": 73, "seek": 35684, "start": 373.17999999999995, "end": 375.35999999999996, "text": " refers to.", "tokens": [50364, 583, 498, 291, 434, 1228, 257, 14442, 3479, 2539, 6405, 293, 818, 8213, 24590, 11, 456, 311, 50694, 50694, 257, 2931, 300, 322, 264, 38087, 11, 309, 486, 312, 1228, 341, 281, 5039, 337, 261, 293, 272, 13, 50954, 50954, 407, 498, 291, 434, 1562, 294, 257, 1691, 4049, 293, 1568, 264, 1433, 2710, 5367, 11, 300, 311, 437, 341, 51181, 51181, 14942, 281, 13, 51290, 51290, 1468, 380, 3292, 466, 264, 4365, 295, 577, 264, 2710, 5367, 1985, 13, 51456, 51456, 1449, 312, 3650, 300, 512, 3479, 2539, 15148, 815, 764, 341, 6179, 3170, 294, 264, 38087, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.1020668347676595, "compression_ratio": 1.7418032786885247, "no_speech_prob": 3.7852462355658645e-06}, {"id": 74, "seek": 35684, "start": 375.35999999999996, "end": 378.67999999999995, "text": " Don't worry about the details of how the normal equation works.", "tokens": [50364, 583, 498, 291, 434, 1228, 257, 14442, 3479, 2539, 6405, 293, 818, 8213, 24590, 11, 456, 311, 50694, 50694, 257, 2931, 300, 322, 264, 38087, 11, 309, 486, 312, 1228, 341, 281, 5039, 337, 261, 293, 272, 13, 50954, 50954, 407, 498, 291, 434, 1562, 294, 257, 1691, 4049, 293, 1568, 264, 1433, 2710, 5367, 11, 300, 311, 437, 341, 51181, 51181, 14942, 281, 13, 51290, 51290, 1468, 380, 3292, 466, 264, 4365, 295, 577, 264, 2710, 5367, 1985, 13, 51456, 51456, 1449, 312, 3650, 300, 512, 3479, 2539, 15148, 815, 764, 341, 6179, 3170, 294, 264, 38087, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.1020668347676595, "compression_ratio": 1.7418032786885247, "no_speech_prob": 3.7852462355658645e-06}, {"id": 75, "seek": 35684, "start": 378.67999999999995, "end": 384.91999999999996, "text": " Just be aware that some machine learning libraries may use this complicated method in the backend", "tokens": [50364, 583, 498, 291, 434, 1228, 257, 14442, 3479, 2539, 6405, 293, 818, 8213, 24590, 11, 456, 311, 50694, 50694, 257, 2931, 300, 322, 264, 38087, 11, 309, 486, 312, 1228, 341, 281, 5039, 337, 261, 293, 272, 13, 50954, 50954, 407, 498, 291, 434, 1562, 294, 257, 1691, 4049, 293, 1568, 264, 1433, 2710, 5367, 11, 300, 311, 437, 341, 51181, 51181, 14942, 281, 13, 51290, 51290, 1468, 380, 3292, 466, 264, 4365, 295, 577, 264, 2710, 5367, 1985, 13, 51456, 51456, 1449, 312, 3650, 300, 512, 3479, 2539, 15148, 815, 764, 341, 6179, 3170, 294, 264, 38087, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.1020668347676595, "compression_ratio": 1.7418032786885247, "no_speech_prob": 3.7852462355658645e-06}, {"id": 76, "seek": 38492, "start": 384.92, "end": 386.76, "text": " to solve for w and b.", "tokens": [50364, 281, 5039, 337, 261, 293, 272, 13, 50456, 50456, 583, 337, 881, 2539, 14642, 11, 3009, 577, 291, 4445, 8213, 24590, 1803, 11, 50738, 50738, 16235, 23475, 307, 2049, 257, 1101, 636, 281, 483, 264, 1691, 1096, 13, 50936, 50936, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 536, 577, 281, 6964, 257, 3866, 24590, 51198, 51198, 2316, 294, 3089, 11, 293, 611, 577, 281, 8873, 264, 17630, 283, 295, 2031, 13, 51528, 51528, 509, 611, 536, 577, 281, 8873, 264, 2063, 293, 4445, 16235, 23475, 337, 3866, 8213, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.11510014784963507, "compression_ratio": 1.7692307692307692, "no_speech_prob": 6.8542112785507925e-06}, {"id": 77, "seek": 38492, "start": 386.76, "end": 392.40000000000003, "text": " But for most learning algorithms, including how you implement linear regression yourself,", "tokens": [50364, 281, 5039, 337, 261, 293, 272, 13, 50456, 50456, 583, 337, 881, 2539, 14642, 11, 3009, 577, 291, 4445, 8213, 24590, 1803, 11, 50738, 50738, 16235, 23475, 307, 2049, 257, 1101, 636, 281, 483, 264, 1691, 1096, 13, 50936, 50936, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 536, 577, 281, 6964, 257, 3866, 24590, 51198, 51198, 2316, 294, 3089, 11, 293, 611, 577, 281, 8873, 264, 17630, 283, 295, 2031, 13, 51528, 51528, 509, 611, 536, 577, 281, 8873, 264, 2063, 293, 4445, 16235, 23475, 337, 3866, 8213, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.11510014784963507, "compression_ratio": 1.7692307692307692, "no_speech_prob": 6.8542112785507925e-06}, {"id": 78, "seek": 38492, "start": 392.40000000000003, "end": 396.36, "text": " gradient descent is often a better way to get the job done.", "tokens": [50364, 281, 5039, 337, 261, 293, 272, 13, 50456, 50456, 583, 337, 881, 2539, 14642, 11, 3009, 577, 291, 4445, 8213, 24590, 1803, 11, 50738, 50738, 16235, 23475, 307, 2049, 257, 1101, 636, 281, 483, 264, 1691, 1096, 13, 50936, 50936, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 536, 577, 281, 6964, 257, 3866, 24590, 51198, 51198, 2316, 294, 3089, 11, 293, 611, 577, 281, 8873, 264, 17630, 283, 295, 2031, 13, 51528, 51528, 509, 611, 536, 577, 281, 8873, 264, 2063, 293, 4445, 16235, 23475, 337, 3866, 8213, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.11510014784963507, "compression_ratio": 1.7692307692307692, "no_speech_prob": 6.8542112785507925e-06}, {"id": 79, "seek": 38492, "start": 396.36, "end": 401.6, "text": " In the optional lab that follows this video, you see how to define a multiple regression", "tokens": [50364, 281, 5039, 337, 261, 293, 272, 13, 50456, 50456, 583, 337, 881, 2539, 14642, 11, 3009, 577, 291, 4445, 8213, 24590, 1803, 11, 50738, 50738, 16235, 23475, 307, 2049, 257, 1101, 636, 281, 483, 264, 1691, 1096, 13, 50936, 50936, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 536, 577, 281, 6964, 257, 3866, 24590, 51198, 51198, 2316, 294, 3089, 11, 293, 611, 577, 281, 8873, 264, 17630, 283, 295, 2031, 13, 51528, 51528, 509, 611, 536, 577, 281, 8873, 264, 2063, 293, 4445, 16235, 23475, 337, 3866, 8213, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.11510014784963507, "compression_ratio": 1.7692307692307692, "no_speech_prob": 6.8542112785507925e-06}, {"id": 80, "seek": 38492, "start": 401.6, "end": 408.20000000000005, "text": " model in code, and also how to calculate the prediction f of x.", "tokens": [50364, 281, 5039, 337, 261, 293, 272, 13, 50456, 50456, 583, 337, 881, 2539, 14642, 11, 3009, 577, 291, 4445, 8213, 24590, 1803, 11, 50738, 50738, 16235, 23475, 307, 2049, 257, 1101, 636, 281, 483, 264, 1691, 1096, 13, 50936, 50936, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 536, 577, 281, 6964, 257, 3866, 24590, 51198, 51198, 2316, 294, 3089, 11, 293, 611, 577, 281, 8873, 264, 17630, 283, 295, 2031, 13, 51528, 51528, 509, 611, 536, 577, 281, 8873, 264, 2063, 293, 4445, 16235, 23475, 337, 3866, 8213, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.11510014784963507, "compression_ratio": 1.7692307692307692, "no_speech_prob": 6.8542112785507925e-06}, {"id": 81, "seek": 38492, "start": 408.20000000000005, "end": 414.04, "text": " You also see how to calculate the cost and implement gradient descent for multiple linear", "tokens": [50364, 281, 5039, 337, 261, 293, 272, 13, 50456, 50456, 583, 337, 881, 2539, 14642, 11, 3009, 577, 291, 4445, 8213, 24590, 1803, 11, 50738, 50738, 16235, 23475, 307, 2049, 257, 1101, 636, 281, 483, 264, 1691, 1096, 13, 50936, 50936, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 536, 577, 281, 6964, 257, 3866, 24590, 51198, 51198, 2316, 294, 3089, 11, 293, 611, 577, 281, 8873, 264, 17630, 283, 295, 2031, 13, 51528, 51528, 509, 611, 536, 577, 281, 8873, 264, 2063, 293, 4445, 16235, 23475, 337, 3866, 8213, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.11510014784963507, "compression_ratio": 1.7692307692307692, "no_speech_prob": 6.8542112785507925e-06}, {"id": 82, "seek": 41404, "start": 414.04, "end": 415.68, "text": " regression model.", "tokens": [50364, 24590, 2316, 13, 50446, 50446, 639, 486, 312, 1228, 15329, 311, 22592, 47, 88, 6405, 11, 370, 498, 604, 295, 264, 3089, 1542, 588, 777, 11, 300, 311, 50732, 50732, 1392, 13, 50790, 50790, 583, 291, 820, 841, 1737, 611, 281, 747, 257, 574, 412, 264, 3894, 17312, 2715, 300, 31472, 51080, 51080, 22592, 47, 88, 293, 8062, 2144, 337, 257, 17368, 511, 295, 22592, 47, 88, 6828, 293, 577, 281, 4445, 729, 51374, 51374, 294, 3089, 13, 51462, 51462, 407, 300, 311, 309, 13, 51520, 51520, 509, 586, 458, 3866, 8213, 24590, 13, 51653, 51653], "temperature": 0.0, "avg_logprob": -0.13944678403893296, "compression_ratio": 1.5260869565217392, "no_speech_prob": 5.337911716196686e-06}, {"id": 83, "seek": 41404, "start": 415.68, "end": 421.40000000000003, "text": " This will be using Python's NumPy library, so if any of the code looks very new, that's", "tokens": [50364, 24590, 2316, 13, 50446, 50446, 639, 486, 312, 1228, 15329, 311, 22592, 47, 88, 6405, 11, 370, 498, 604, 295, 264, 3089, 1542, 588, 777, 11, 300, 311, 50732, 50732, 1392, 13, 50790, 50790, 583, 291, 820, 841, 1737, 611, 281, 747, 257, 574, 412, 264, 3894, 17312, 2715, 300, 31472, 51080, 51080, 22592, 47, 88, 293, 8062, 2144, 337, 257, 17368, 511, 295, 22592, 47, 88, 6828, 293, 577, 281, 4445, 729, 51374, 51374, 294, 3089, 13, 51462, 51462, 407, 300, 311, 309, 13, 51520, 51520, 509, 586, 458, 3866, 8213, 24590, 13, 51653, 51653], "temperature": 0.0, "avg_logprob": -0.13944678403893296, "compression_ratio": 1.5260869565217392, "no_speech_prob": 5.337911716196686e-06}, {"id": 84, "seek": 41404, "start": 421.40000000000003, "end": 422.56, "text": " okay.", "tokens": [50364, 24590, 2316, 13, 50446, 50446, 639, 486, 312, 1228, 15329, 311, 22592, 47, 88, 6405, 11, 370, 498, 604, 295, 264, 3089, 1542, 588, 777, 11, 300, 311, 50732, 50732, 1392, 13, 50790, 50790, 583, 291, 820, 841, 1737, 611, 281, 747, 257, 574, 412, 264, 3894, 17312, 2715, 300, 31472, 51080, 51080, 22592, 47, 88, 293, 8062, 2144, 337, 257, 17368, 511, 295, 22592, 47, 88, 6828, 293, 577, 281, 4445, 729, 51374, 51374, 294, 3089, 13, 51462, 51462, 407, 300, 311, 309, 13, 51520, 51520, 509, 586, 458, 3866, 8213, 24590, 13, 51653, 51653], "temperature": 0.0, "avg_logprob": -0.13944678403893296, "compression_ratio": 1.5260869565217392, "no_speech_prob": 5.337911716196686e-06}, {"id": 85, "seek": 41404, "start": 422.56, "end": 428.36, "text": " But you should feel free also to take a look at the previous optional lab that introduces", "tokens": [50364, 24590, 2316, 13, 50446, 50446, 639, 486, 312, 1228, 15329, 311, 22592, 47, 88, 6405, 11, 370, 498, 604, 295, 264, 3089, 1542, 588, 777, 11, 300, 311, 50732, 50732, 1392, 13, 50790, 50790, 583, 291, 820, 841, 1737, 611, 281, 747, 257, 574, 412, 264, 3894, 17312, 2715, 300, 31472, 51080, 51080, 22592, 47, 88, 293, 8062, 2144, 337, 257, 17368, 511, 295, 22592, 47, 88, 6828, 293, 577, 281, 4445, 729, 51374, 51374, 294, 3089, 13, 51462, 51462, 407, 300, 311, 309, 13, 51520, 51520, 509, 586, 458, 3866, 8213, 24590, 13, 51653, 51653], "temperature": 0.0, "avg_logprob": -0.13944678403893296, "compression_ratio": 1.5260869565217392, "no_speech_prob": 5.337911716196686e-06}, {"id": 86, "seek": 41404, "start": 428.36, "end": 434.24, "text": " NumPy and vectorization for a refresher of NumPy functions and how to implement those", "tokens": [50364, 24590, 2316, 13, 50446, 50446, 639, 486, 312, 1228, 15329, 311, 22592, 47, 88, 6405, 11, 370, 498, 604, 295, 264, 3089, 1542, 588, 777, 11, 300, 311, 50732, 50732, 1392, 13, 50790, 50790, 583, 291, 820, 841, 1737, 611, 281, 747, 257, 574, 412, 264, 3894, 17312, 2715, 300, 31472, 51080, 51080, 22592, 47, 88, 293, 8062, 2144, 337, 257, 17368, 511, 295, 22592, 47, 88, 6828, 293, 577, 281, 4445, 729, 51374, 51374, 294, 3089, 13, 51462, 51462, 407, 300, 311, 309, 13, 51520, 51520, 509, 586, 458, 3866, 8213, 24590, 13, 51653, 51653], "temperature": 0.0, "avg_logprob": -0.13944678403893296, "compression_ratio": 1.5260869565217392, "no_speech_prob": 5.337911716196686e-06}, {"id": 87, "seek": 41404, "start": 434.24, "end": 436.0, "text": " in code.", "tokens": [50364, 24590, 2316, 13, 50446, 50446, 639, 486, 312, 1228, 15329, 311, 22592, 47, 88, 6405, 11, 370, 498, 604, 295, 264, 3089, 1542, 588, 777, 11, 300, 311, 50732, 50732, 1392, 13, 50790, 50790, 583, 291, 820, 841, 1737, 611, 281, 747, 257, 574, 412, 264, 3894, 17312, 2715, 300, 31472, 51080, 51080, 22592, 47, 88, 293, 8062, 2144, 337, 257, 17368, 511, 295, 22592, 47, 88, 6828, 293, 577, 281, 4445, 729, 51374, 51374, 294, 3089, 13, 51462, 51462, 407, 300, 311, 309, 13, 51520, 51520, 509, 586, 458, 3866, 8213, 24590, 13, 51653, 51653], "temperature": 0.0, "avg_logprob": -0.13944678403893296, "compression_ratio": 1.5260869565217392, "no_speech_prob": 5.337911716196686e-06}, {"id": 88, "seek": 41404, "start": 436.0, "end": 437.16, "text": " So that's it.", "tokens": [50364, 24590, 2316, 13, 50446, 50446, 639, 486, 312, 1228, 15329, 311, 22592, 47, 88, 6405, 11, 370, 498, 604, 295, 264, 3089, 1542, 588, 777, 11, 300, 311, 50732, 50732, 1392, 13, 50790, 50790, 583, 291, 820, 841, 1737, 611, 281, 747, 257, 574, 412, 264, 3894, 17312, 2715, 300, 31472, 51080, 51080, 22592, 47, 88, 293, 8062, 2144, 337, 257, 17368, 511, 295, 22592, 47, 88, 6828, 293, 577, 281, 4445, 729, 51374, 51374, 294, 3089, 13, 51462, 51462, 407, 300, 311, 309, 13, 51520, 51520, 509, 586, 458, 3866, 8213, 24590, 13, 51653, 51653], "temperature": 0.0, "avg_logprob": -0.13944678403893296, "compression_ratio": 1.5260869565217392, "no_speech_prob": 5.337911716196686e-06}, {"id": 89, "seek": 41404, "start": 437.16, "end": 439.82000000000005, "text": " You now know multiple linear regression.", "tokens": [50364, 24590, 2316, 13, 50446, 50446, 639, 486, 312, 1228, 15329, 311, 22592, 47, 88, 6405, 11, 370, 498, 604, 295, 264, 3089, 1542, 588, 777, 11, 300, 311, 50732, 50732, 1392, 13, 50790, 50790, 583, 291, 820, 841, 1737, 611, 281, 747, 257, 574, 412, 264, 3894, 17312, 2715, 300, 31472, 51080, 51080, 22592, 47, 88, 293, 8062, 2144, 337, 257, 17368, 511, 295, 22592, 47, 88, 6828, 293, 577, 281, 4445, 729, 51374, 51374, 294, 3089, 13, 51462, 51462, 407, 300, 311, 309, 13, 51520, 51520, 509, 586, 458, 3866, 8213, 24590, 13, 51653, 51653], "temperature": 0.0, "avg_logprob": -0.13944678403893296, "compression_ratio": 1.5260869565217392, "no_speech_prob": 5.337911716196686e-06}, {"id": 90, "seek": 43982, "start": 439.82, "end": 444.71999999999997, "text": " This is probably the single most widely used learning algorithm in the world today.", "tokens": [50364, 639, 307, 1391, 264, 2167, 881, 13371, 1143, 2539, 9284, 294, 264, 1002, 965, 13, 50609, 50609, 583, 456, 311, 544, 13, 50677, 50677, 2022, 445, 257, 1326, 11733, 1270, 382, 8867, 293, 21589, 4122, 23505, 11, 293, 611, 10875, 50919, 50919, 264, 2539, 3314, 8961, 23505, 11, 291, 486, 312, 1075, 281, 652, 341, 589, 709, 1101, 13, 51167, 51167, 407, 445, 257, 1326, 544, 2145, 281, 352, 337, 341, 1243, 13, 51295, 51295, 961, 311, 352, 322, 281, 264, 958, 960, 281, 536, 729, 707, 11733, 300, 486, 854, 291, 652, 3866, 51519, 51519, 8213, 24590, 589, 709, 1101, 13, 51617], "temperature": 0.0, "avg_logprob": -0.1304495584397089, "compression_ratio": 1.7328244274809161, "no_speech_prob": 3.477303107501939e-05}, {"id": 91, "seek": 43982, "start": 444.71999999999997, "end": 446.08, "text": " But there's more.", "tokens": [50364, 639, 307, 1391, 264, 2167, 881, 13371, 1143, 2539, 9284, 294, 264, 1002, 965, 13, 50609, 50609, 583, 456, 311, 544, 13, 50677, 50677, 2022, 445, 257, 1326, 11733, 1270, 382, 8867, 293, 21589, 4122, 23505, 11, 293, 611, 10875, 50919, 50919, 264, 2539, 3314, 8961, 23505, 11, 291, 486, 312, 1075, 281, 652, 341, 589, 709, 1101, 13, 51167, 51167, 407, 445, 257, 1326, 544, 2145, 281, 352, 337, 341, 1243, 13, 51295, 51295, 961, 311, 352, 322, 281, 264, 958, 960, 281, 536, 729, 707, 11733, 300, 486, 854, 291, 652, 3866, 51519, 51519, 8213, 24590, 589, 709, 1101, 13, 51617], "temperature": 0.0, "avg_logprob": -0.1304495584397089, "compression_ratio": 1.7328244274809161, "no_speech_prob": 3.477303107501939e-05}, {"id": 92, "seek": 43982, "start": 446.08, "end": 450.92, "text": " With just a few tricks such as picking and scaling features appropriately, and also choosing", "tokens": [50364, 639, 307, 1391, 264, 2167, 881, 13371, 1143, 2539, 9284, 294, 264, 1002, 965, 13, 50609, 50609, 583, 456, 311, 544, 13, 50677, 50677, 2022, 445, 257, 1326, 11733, 1270, 382, 8867, 293, 21589, 4122, 23505, 11, 293, 611, 10875, 50919, 50919, 264, 2539, 3314, 8961, 23505, 11, 291, 486, 312, 1075, 281, 652, 341, 589, 709, 1101, 13, 51167, 51167, 407, 445, 257, 1326, 544, 2145, 281, 352, 337, 341, 1243, 13, 51295, 51295, 961, 311, 352, 322, 281, 264, 958, 960, 281, 536, 729, 707, 11733, 300, 486, 854, 291, 652, 3866, 51519, 51519, 8213, 24590, 589, 709, 1101, 13, 51617], "temperature": 0.0, "avg_logprob": -0.1304495584397089, "compression_ratio": 1.7328244274809161, "no_speech_prob": 3.477303107501939e-05}, {"id": 93, "seek": 43982, "start": 450.92, "end": 455.88, "text": " the learning rate alpha appropriately, you will be able to make this work much better.", "tokens": [50364, 639, 307, 1391, 264, 2167, 881, 13371, 1143, 2539, 9284, 294, 264, 1002, 965, 13, 50609, 50609, 583, 456, 311, 544, 13, 50677, 50677, 2022, 445, 257, 1326, 11733, 1270, 382, 8867, 293, 21589, 4122, 23505, 11, 293, 611, 10875, 50919, 50919, 264, 2539, 3314, 8961, 23505, 11, 291, 486, 312, 1075, 281, 652, 341, 589, 709, 1101, 13, 51167, 51167, 407, 445, 257, 1326, 544, 2145, 281, 352, 337, 341, 1243, 13, 51295, 51295, 961, 311, 352, 322, 281, 264, 958, 960, 281, 536, 729, 707, 11733, 300, 486, 854, 291, 652, 3866, 51519, 51519, 8213, 24590, 589, 709, 1101, 13, 51617], "temperature": 0.0, "avg_logprob": -0.1304495584397089, "compression_ratio": 1.7328244274809161, "no_speech_prob": 3.477303107501939e-05}, {"id": 94, "seek": 43982, "start": 455.88, "end": 458.44, "text": " So just a few more videos to go for this week.", "tokens": [50364, 639, 307, 1391, 264, 2167, 881, 13371, 1143, 2539, 9284, 294, 264, 1002, 965, 13, 50609, 50609, 583, 456, 311, 544, 13, 50677, 50677, 2022, 445, 257, 1326, 11733, 1270, 382, 8867, 293, 21589, 4122, 23505, 11, 293, 611, 10875, 50919, 50919, 264, 2539, 3314, 8961, 23505, 11, 291, 486, 312, 1075, 281, 652, 341, 589, 709, 1101, 13, 51167, 51167, 407, 445, 257, 1326, 544, 2145, 281, 352, 337, 341, 1243, 13, 51295, 51295, 961, 311, 352, 322, 281, 264, 958, 960, 281, 536, 729, 707, 11733, 300, 486, 854, 291, 652, 3866, 51519, 51519, 8213, 24590, 589, 709, 1101, 13, 51617], "temperature": 0.0, "avg_logprob": -0.1304495584397089, "compression_ratio": 1.7328244274809161, "no_speech_prob": 3.477303107501939e-05}, {"id": 95, "seek": 43982, "start": 458.44, "end": 462.92, "text": " Let's go on to the next video to see those little tricks that will help you make multiple", "tokens": [50364, 639, 307, 1391, 264, 2167, 881, 13371, 1143, 2539, 9284, 294, 264, 1002, 965, 13, 50609, 50609, 583, 456, 311, 544, 13, 50677, 50677, 2022, 445, 257, 1326, 11733, 1270, 382, 8867, 293, 21589, 4122, 23505, 11, 293, 611, 10875, 50919, 50919, 264, 2539, 3314, 8961, 23505, 11, 291, 486, 312, 1075, 281, 652, 341, 589, 709, 1101, 13, 51167, 51167, 407, 445, 257, 1326, 544, 2145, 281, 352, 337, 341, 1243, 13, 51295, 51295, 961, 311, 352, 322, 281, 264, 958, 960, 281, 536, 729, 707, 11733, 300, 486, 854, 291, 652, 3866, 51519, 51519, 8213, 24590, 589, 709, 1101, 13, 51617], "temperature": 0.0, "avg_logprob": -0.1304495584397089, "compression_ratio": 1.7328244274809161, "no_speech_prob": 3.477303107501939e-05}, {"id": 96, "seek": 46292, "start": 462.92, "end": 469.92, "text": " linear regression work much better.", "tokens": [50364, 8213, 24590, 589, 709, 1101, 13, 50714], "temperature": 0.0, "avg_logprob": -0.5540156364440918, "compression_ratio": 0.813953488372093, "no_speech_prob": 0.00014531565830111504}], "language": "en", "video_id": "odAhNw-e4o0", "entity": "ML Specialization, Andrew Ng (2022)"}}